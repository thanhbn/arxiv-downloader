# 2008.09396.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/byte-level/2008.09396.pdf
# Kích thước file: 278259 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Dịch máy thần kinh không có embedding
Uri Shaham}Omer Levy}
}Trường Khoa học Máy tính Blavatnik, Đại học Tel Aviv
Facebook AI Research
Tóm tắt
Nhiều mô hình NLP hoạt động trên các chuỗi
token từ con được tạo ra bởi các quy tắc tokenization
thủ công và các thuật toán quy nạp từ con theo kinh
nghiệm. Một giải pháp thay thế phổ quát đơn giản
là biểu diễn mọi văn bản máy tính thành một chuỗi
byte thông qua UTF-8, loại bỏ nhu cầu về lớp
embedding vì có ít loại token hơn (256) so với
số chiều. Một cách đáng ngạc nhiên, việc thay thế
lớp embedding phổ biến bằng biểu diễn one-hot
của mỗi byte không làm tổn hại hiệu suất; các thí
nghiệm về dịch máy byte-to-byte từ tiếng Anh sang
10 ngôn ngữ khác nhau cho thấy sự cải thiện nhất
quán trong BLEU, cạnh tranh với các mô hình ở
mức ký tự và thậm chí cả mức từ con tiêu chuẩn.
Một cuộc điều tra sâu hơn tiết lộ rằng sự kết hợp
của các mô hình không có embedding với decoder-input
dropout tương đương với token dropout, điều này
đặc biệt có lợi cho các mô hình byte-to-byte.1
1 Giới thiệu
Các mô hình NLP thần kinh thường hoạt động ở
mức từ con, điều này đòi hỏi các tokenizer đặc thù
cho ngôn ngữ (Koehn et al., 2007; Adler and Elhadad, 2006) và
các thuật toán quy nạp từ con, chẳng hạn như BPE (Sennrich et al., 2016; Kudo, 2018). Thay vào đó, làm việc
ở mức byte bằng cách biểu diễn mỗi ký tự dưới dạng
một số byte Unicode (UTF-8) biến đổi, không
yêu cầu bất kỳ hình thức tiền xử lý nào, cho phép
mô hình đọc và dự đoán mọi văn bản máy tính
sử dụng một từ vựng duy nhất gồm 256 loại. Trong khi
nghiên cứu trước đây phát hiện rằng các mô hình mức byte
có xu hướng kém hiệu quả hơn các mô hình dựa trên token từ con
(Wang et al., 2019), các mô hình dựa trên byte thể hiện một
tính chất thú vị: từ vựng của chúng nhỏ hơn
số chiều tiềm ẩn (256 < d).
Trong nghiên cứu này, chúng tôi chứng minh rằng tính chất này
cho phép chúng tôi loại bỏ các lớp embedding đầu vào và đầu ra
từ các mô hình dịch byte-to-byte,
1Mã nguồn của chúng tôi được công khai tại: https://github.
com/UriSha/EmbeddinglessNMTvà khi làm như vậy, cải thiện hiệu suất của mô hình
một cách nhất quán.
Chúng tôi thay thế ma trận embedding dense có thể huấn luyện
bằng một mã hóa one-hot cố định của từ vựng
làm lớp đầu tiên và cuối cùng của một mô hình transformer
tiêu chuẩn. Các thí nghiệm dịch máy trên 10 cặp
ngôn ngữ cho thấy rằng các mô hình byte-to-byte không có
lớp embedding đạt được điểm BLEU cao hơn
so với các mô hình dựa trên byte với embedding có tham số
(+0.5 trung bình), do đó thu hẹp khoảng cách
hiệu suất với các mô hình từ con và ký tự. Chúng tôi
quan sát kết quả này một cách nhất quán trong nhiều
ngôn ngữ đích và hệ thống chữ viết đa dạng.
Việc loại bỏ tham số lại cải thiện hiệu
suất là điều phản trực giác, đặc biệt khi xem xét
xu hướng gần đây trong machine learning ủng hộ các
mạng ngày càng lớn hơn. Chúng tôi tiếp tục điều tra
tại sao các mô hình không có embedding mang lại kết quả
tốt hơn và tìm thấy token dropout ngầm (thường
được gọi là "word dropout") là nguồn chính
của việc tăng cường đó. Trong khi nghiên cứu trước cho thấy rằng việc
che giấu token ngẫu nhiên từ đầu vào decoder có thể
cải thiện hiệu suất của các mô hình tạo ngôn ngữ
(Bowman et al., 2016), chúng tôi thấy rằng hiệu ứng này
được khuếch đại khi hoạt động ở mức byte.
Nhìn chung, kết quả của chúng tôi gợi ý rằng, ngay cả khi không có
tham số bổ sung, các mô hình dựa trên byte có thể cạnh tranh
và có khả năng vượt trội hơn các mô hình từ con, nhưng
chúng có thể yêu cầu các kỹ thuật tối ưu hóa thay thế
để đạt được mục tiêu đó.
2 Tokenization Byte
Phần mềm hiện đại thường biểu diễn văn bản bằng
cách sử dụng chuỗi Unicode (UTF-8), cho phép mã hóa
hầu như bất kỳ hệ thống chữ viết nào sử dụng một số
byte biến đổi mỗi token; các ký tự tiếng Anh thường
được biểu diễn bằng một byte duy nhất, với các
hệ thống chữ viết khác lấy hai (ví dụ: tiếng Ả Rập), ba (ví dụ:
tiếng Trung), hoặc bốn (ví dụ: emoji) byte mỗi ký tự.
Bằng cách xử lý mỗi byte như một token riêng biệt, chúng ta có thể
mã hóa bất kỳ văn bản ngôn ngữ tự nhiên nào sử dụng một từ vựng uni-arXiv:2008.09396v2  [cs.CL]  12 Apr 2021

--- TRANG 2 ---
Văn bản gốc Будь здоров.
Từ con (BPE) Бу@ дь здо@ ров .
Ký tự Б у д ь з д о р о в .
Byte (UTF-8) D091D183D0B4D18C20D0B7D0B4D0BED180D0BED0B22E
Hình 1: Token từ con (BPE), ký tự, và byte của chuỗi " Будь здоров ." UTF-8 sử dụng hai byte để
biểu diễn mỗi ký tự trong chữ Cyrillic, làm cho chuỗi byte dài hơn số ký tự.
versal chỉ với 256 loại token. Hơn
nữa, tokenization byte loại bỏ nhu cầu về
bất kỳ tiền xử lý theo kinh nghiệm nào, chẳng hạn như
tách khoảng trắng, dấu câu, và từ viết tắt. Hình 1 minh họa
tokenization từ con, ký tự, và byte.
3 Mô hình không có Embedding
Mô hình của chúng tôi dựa trên transformer encoder-decoder
ban đầu (Vaswani et al., 2017) với một
điểm khác biệt chính: chúng tôi loại bỏ các lớp embedding
token đầu vào và đầu ra. Các lớp này thường sử dụng
một ma trận tham số chung E∈R|V|×d chứa
một vector embedding d-chiều cho mỗi
mục từ vựng nguồn và đích trong V.2
Thay vào đó, chúng tôi sử dụng một biểu diễn one-hot
cố định của từ vựng byte của chúng tôi. Ví dụ, ký tự
"R" có thể được biểu diễn như một vector với 1 tại
chiều 82 và 0 ở những nơi khác. Vì đó là thực hành
tiêu chuẩn để sử dụng biểu diễn hơn 256
chiều, mọi byte có thể được biểu diễn
bằng các vector one-hot như vậy. Để dự đoán token tiếp theo
cho một đầu vào decoder của n token, chúng tôi lấy đầu ra
của lớp decoder transformer cuối cùng, Y∈Rn×d,
và áp dụng một softmax trên các chiều của mỗi vector.
Các biểu thức chính thức của đầu vào và đầu ra
của mô hình chúng tôi được trình bày chi tiết trong Hình 2.
Bỏ qua lớp embedding giảm số
tham số theo hệ số O(|V|d).3Chúng tôi
thêm tổng cộng 3 tham số để chia tỷ lệ đầu vào (one-hot)
của encoder và decoder và đầu ra của decoder
(trước softmax). Chúng tôi khởi tạo cả ba với√d, tương tự như hệ số chia tỷ lệ hằng số thường
được áp dụng cho lớp embedding đầu vào trong
transformer. Mặc dù giảm kích thước mô hình, tiêu thụ bộ nhớ
2Người ta có thể lập luận rằng lớp đầu tiên của mỗi stack transformer
(các ma trận key, query, và value) đủ điều kiện như một
hình thức lớp embedding đa đầu đa mục đích, nơi
mỗi loại token được biểu diễn hiệu quả bởi 3h vector khác nhau
(h là số đầu attention) trong encoder
và 3h vector bổ sung trong decoder. Điều này rất khác
so với khái niệm embedding tiêu chuẩn, nơi mỗi loại
token có một biểu diễn phổ quát có thể được chia sẻ qua
đầu vào encoder, đầu vào decoder, và đầu ra decoder.
3Đối với tokenization từ con, điều này chiếm một phần
đáng kể của ngân sách tham số, nhưng đối với các mô hình dựa trên byte
chi phí tham số bổ sung là không đáng kể.Ban đầu Không có Embedding
Đầu vào XE + Pn X + Pn
Đầu ra softmax|V|(YE⊤) softmaxd(Y)
Hình 2: Những khác biệt chính giữa mô hình
encoder-decoder ban đầu và mô hình không có embedding mới.
X∈Rn×|V| là biểu diễn one-hot của n
token đầu vào (byte); Pn là các embedding vị trí
đến độ dài n.
tăng khi làm việc trên các chuỗi dài hơn, vì độ phức tạp
không gian của transformer là O(n2 + nd). Trong trường hợp của chúng tôi, d (512) thường
lớn hơn n (xem Bảng 1), dẫn đến sự tăng
tiêu thụ bộ nhớ gần như tuyến tính theo
độ dài chuỗi n, và một sự giảm tương tự trong
tốc độ xử lý khi so sánh với các mô hình ký tự và
từ con.
Ngoài việc thay thế các lớp embedding,
chúng tôi cũng loại bỏ các lớp dropout trên đầu vào encoder
và đầu ra decoder, vì việc đặt các mục của vector one-hot
về 0 tương đương với việc che giấu ngẫu nhiên
các token đầu vào hoặc xóa các phần quan trọng
của phân phối dự đoán của mô hình. Dropout trên
đầu vào decoder (tiền tố của đích được cung cấp với
teacher forcing) vẫn còn nguyên tại thời điểm này và được
áp dụng trong suốt các thí nghiệm chính của chúng tôi. Phân tích
tiếp theo cho thấy rằng decoder input dropout thực sự là
một nguồn quan trọng của việc cải thiện hiệu suất, mà
chúng tôi điều tra thêm trong Phần 6.
4 Thí nghiệm
Chúng tôi huấn luyện các mô hình không có embedding được tokenized theo byte cho
dịch máy và so sánh chúng với các mô hình
dựa trên byte, ký tự, và từ con tiêu chuẩn trên một
tập hợp ngôn ngữ đa dạng. Chúng tôi áp dụng một thiết lập thí nghiệm
tiêu chuẩn được thiết kế và điều chỉnh cho
baseline từ con và giới hạn việc điều chỉnh siêu tham số
của chúng tôi với các xác suất dropout.
Tập dữ liệu Chúng tôi sử dụng các tập dữ liệu IWSLT4 của các buổi nói
TED tiếng Anh được dịch sang các ngôn ngữ khác (Cettolo
4Tất cả các ngôn ngữ sử dụng dữ liệu IWSLT2014 ngoại trừ
tiếng Việt (IWSLT2015) và tiếng Nhật (IWSLT2017).

--- TRANG 3 ---
ID Ngôn ngữ #Câu Độ dài trung bình
BPE Char Byte
Trung Quốc zh 166k 20.9 32.4 90.1
Tây Ban Nha es 167k 25.4 100.2 100.2
Ả Rập ar 166k 24.4 79.3 142.2
Nga ru 164k 26.3 93.9 169.7
Đức de 159k 26.6 106.5 107.9
Nhật Bản ja 215k 20.9 42.4 115.3
Thổ Nhĩ Kỳ tr 143k 24.1 93.6 102.0
Việt Nam vi 124k 26.9 99.8 132.5
Ba Tư fa 100k 27.4 93.1 165.9
Do Thái he 171k 23.0 72.8 129.2
Tiếng Anh en - 25.6 97.0 97.1
Bảng 1: Các ngôn ngữ từ tập dữ liệu IWSLT, cùng với
số cặp câu trong tập huấn luyện và
độ dài chuỗi trung bình trên mỗi phương pháp tokenization.
et al., 2014), chọn 10 ngôn ngữ bổ sung với
các đặc điểm khác nhau5 (xem Bảng 1). Đối với mỗi ngôn ngữ,
chúng tôi huấn luyện các mô hình dịch từ tiếng Anh sang ngôn ngữ
đích (hướng dịch ban đầu), và cũng theo
hướng ngược lại để đảm bảo tính đầy đủ. Chúng tôi làm sạch dữ liệu huấn luyện
cho mỗi cặp ngôn ngữ bằng cách đầu tiên loại bỏ các câu
dài hơn 800 byte, và sau đó các câu có tỷ lệ
độ dài byte lớn nhất giữa nguồn và đích sao cho chúng tôi
loại bỏ tổng cộng 5% ví dụ huấn luyện.
Baseline Ngoài transformer dựa trên byte không có embedding,
chúng tôi huấn luyện các mô hình transformer encoder-decoder tiêu chuẩn
làm baseline, mỗi mô hình sử dụng
một sơ đồ tokenization khác nhau: từ con, ký
tự, và byte. Đối với tokenization từ con, chúng tôi áp dụng
tokenizer Moses (Koehn et al., 2007) theo sau
bởi BPE (Sennrich et al., 2016). Cả tokenization ký tự và
byte đều không áp dụng bất kỳ tiền xử lý bổ sung
nào và bao gồm khoảng trắng như các token hợp lệ.
Siêu tham số Mã cho mô hình và
baseline của chúng tôi dựa trên việc triển khai Fairseq (Ott et al., 2019)
của mô hình transformer encoder-decoder. Trong quá trình tiền xử lý, chúng tôi sử dụng 10,000 bước
hợp nhất khi xây dựng từ vựng BPE cho
mỗi cặp ngôn ngữ. Các từ vựng và embedding
luôn được chia sẻ giữa ngôn ngữ nguồn và đích.
Trong mỗi transformer, chúng tôi sử dụng 6 lớp encoder
và decoder, 4 đầu attention, một chiều ẩn
512, và một chiều feed-forward
1024. Chúng tôi tối ưu hóa với Adam (Kingma and Ba,
2014), sử dụng bộ lập lịch tốc độ học nghịch đảo căn bậc hai
với 4000 bước khởi động và tốc độ học đỉnh
5Trong khi trong nghiên cứu này chúng tôi ưu tiên sự đa dạng ngôn ngữ và
hệ thống chữ viết, có chỗ để kiểm tra các mô hình không có embedding
trên các tập dữ liệu lớn hơn trong nghiên cứu tương lai.Benchmark Mô hình dựa trên Embedding Không có Embed
Src Tgt Từ con Ký tự Byte Byte
en zh 19.9 20.8 20.2 21.0
en es 36.8 36.3 36.3 36.8
en ar 12.5 12.5 12.3 12.9
en ru 18.1 17.6 17.4 18.2
en de 29.4 28.6 28.7 29.1
en ja 12.0 12.5 12.5 13.1
en tr 13.6 13.7 13.8 14.1
en vi 29.7 28.2 28.0 28.7
en fa 11.5 11.7 12.0 12.1
en he 26.1 26.9 26.4 26.7
zh en 16.8 16.6 15.6 16.1
es en 39.6 38.5 38.4 38.8
ar en 31.5 30.2 30.3 30.8
ru en 22.7 21.9 22.0 22.0
de en 35.4 34.0 34.1 34.5
ja en 13.1 12.6 11.4 12.2
tr en 23.3 22.5 22.3 23.3
vi en 26.8 25.0 24.7 25.3
fa en 23.5 22.4 22.1 22.6
he en 37.8 36.9 37.0 37.4
Bảng 2: Điểm BLEU kiểm tra của các mô hình baseline và
không có embedding trên tập dữ liệu IWSLT.
5×10−4, làm mượt nhãn 0.1, và
weight decay 1×10−4. Chúng tôi huấn luyện mỗi mô hình
trong 50k bước và lấy trung bình 5 checkpoint hàng đầu
theo validation loss. Chúng tôi điều chỉnh dropout
(0.2 hoặc 0.3) trên tập validation. Chúng tôi đặt kích thước batch
theo tối đa 64,000 byte mỗi batch, điều này
kiểm soát số batch mỗi epoch qua các
phương pháp tokenization khác nhau.
Đánh giá Chúng tôi đánh giá các mô hình của mình bằng SacreBLEU, phân biệt chữ hoa chữ thường, với tokenizer 13a cho
tất cả các ngôn ngữ ngoại trừ tiếng Trung (tokenizer ZH) và
tiếng Nhật (tokenizer MeCab). Chúng tôi sử dụng văn bản thô
làm tham chiếu cho tất cả các thí nghiệm của mình, thay vì
sử dụng phiên bản tokenized-detokenized mặc định,
điều này chuẩn hóa văn bản và mang lại lợi thế
giả tạo cho văn bản được xử lý với Moses.
5 Kết quả
Bảng 2 cho thấy kết quả thí nghiệm của chúng tôi. Mỗi hàng
mô tả điểm BLEU kiểm tra của mô hình chúng tôi và
ba baseline được huấn luyện trên một cặp ngôn ngữ khác nhau.
Chúng tôi thảo luận về ý nghĩa của những kết quả này dưới đây.
Embedding có cần thiết không? Kết quả cho thấy
rằng thực sự có thể huấn luyện các mô hình dịch máy không có embedding
hoạt động cạnh tranh. Khoảng cách hiệu suất giữa các mô hình với
các sơ đồ tokenization khác nhau là tương đối nhỏ.
Ngoại trừ tiếng Việt, sự khác biệt giữa

--- TRANG 4 ---
mô hình không có embedding và mô hình dựa trên embedding
tốt nhất luôn dưới 1 BLEU.
Trong thiết lập được kiểm soát nhất, nơi chúng tôi so sánh
các mô hình dựa trên byte có và không có embedding
có thể học, các mô hình không có embedding nhất quán
đạt điểm BLEU cao hơn trong 19 trên 20 trường hợp (và
điểm bằng nhau cho ru-en), với sự tăng cường khoảng
0.5 BLEU trung bình. Khi so sánh với các mô hình
dựa trên embedding ký tự, phương pháp byte-to-byte không có embedding
mang lại điểm BLEU cao hơn trong 17 trên 20 trường hợp,
mặc dù sự khác biệt trung bình khá nhỏ trong thực tế (0.3 BLEU).
Tokenization từ con có vượt trội hơn byte hoặc
ký tự không? Nghiên cứu trước đây trong dịch máy
cho thấy rằng các mô hình từ con nhất quán vượt trội
hơn các mô hình dựa trên ký tự hoặc byte (Gupta
et al., 2019; Wang et al., 2019; Gao et al., 2020).
Tuy nhiên, kết quả của chúng tôi chỉ ra rằng điều này không
nhất thiết đúng. Khi dịch từ tiếng Anh
sang ngôn ngữ nước ngoài, hướng ban đầu của
tập dữ liệu IWSLT, các mô hình byte-to-byte không có embedding
đạt hiệu suất bằng hoặc tốt hơn
các mô hình embedding từ con trong 8 trên 10 trường hợp.
Chúng tôi quan sát một xu hướng khác khi dịch sang
tiếng Anh, nơi các mô hình từ con vượt trội hơn các
mô hình khác cho mọi ngôn ngữ nguồn; việc Moses
là một tokenizer đặc biệt tốt cho tiếng Anh – và
ít hơn cho các ngôn ngữ khác – có lẽ liên quan đến
hiện tượng này. Trong khi nghiên cứu trước đây đề xuất
thu hẹp khoảng cách hiệu suất bằng cách thêm lớp vào
kiến trúc cơ bản, dưới giả định rằng
các mô hình dựa trên ký tự thiếu khả năng hoặc tính biểu cảm,
kết quả của chúng tôi cho thấy rằng thực tế việc loại bỏ
một thành phần khỏi mô hình có thể cải thiện hiệu
suất trong một số điều kiện nhất định. Có thể rằng
các mô hình transformer dựa trên ký tự và byte gặp phải
một vấn đề tối ưu hóa thay vì một vấn đề về
khả năng hoặc tính biểu cảm.
6 Phân tích
Tại sao việc loại bỏ ma trận embedding lại cải thiện
hiệu suất của các mô hình dựa trên byte? Như
đã đề cập trong Phần 3, các mô hình không có embedding không
sử dụng dropout trên đầu vào encoder và đầu ra
decoder, nhưng áp dụng dropout trên đầu vào decoder
trong khi huấn luyện. Vì đầu vào của decoder không có embedding
là các vector one-hot cố định, việc sử dụng dropout
ẩn ý loại bỏ hoàn toàn các token. Trong nghiên cứu trước,
token dropout ("word dropout") đã được chứng minh
có hiệu ứng tích cực nhất quán (Bowman et al.,
2016). Do đó, chúng tôi chạy lại các thí nghiệm của mình khi

Mô hình dựa trên Embedding Không có Embed
Từ con Ký tự Byte Byte
en→xx +0.33 +0.53 +0.42 +0.62
xx→en +0.69 +0.67 +0.92 +0.83
Bảng 3: Sự cải thiện hiệu suất tập validation của token
dropout (0.2), trung bình qua các ngôn ngữ và giá trị
dropout mô hình.
kiểm soát token dropout (p = 0.2) để xác định
hiệu ứng của nó trên kết quả của chúng tôi.
Bảng 3 cho thấy rằng token dropout phía decoder
cải thiện hiệu suất của tất cả các mô hình, với
tác động lớn hơn trên các mô hình dựa trên byte và các
mô hình không có embedding đặc biệt. Hiệu ứng này chủ yếu
nhất quán, với chỉ 7 trên 160 trường hợp mà
token dropout làm giảm hiệu suất trên tập
validation. Chúng tôi nghi ngờ rằng việc loại bỏ các token đích
làm mềm hiệu ứng của exposure bias bằng cách tiêm
nhiễu vào tiền tố ground-truth.
Với lợi ích của token dropout trên các
mô hình baseline, chúng tôi đánh giá lại kết quả từ Phần 5, trong khi cho phép token dropout như một
siêu tham số tiềm năng. Bảng 4 cho thấy rằng, khi dịch
từ văn bản tiếng Anh ban đầu sang ngôn ngữ nước ngoài,
các mô hình khác nhau hoạt động gần như ngang bằng,
không có phương pháp tokenization nào thống trị
các phương pháp khác. Hơn nữa, các mô hình mức byte
có và không có embedding đạt kết quả gần như
giống hệt nhau. Ngược lại, khi dịch theo
hướng ngược lại, các mô hình từ con nhất quán
vượt trội hơn các phương pháp khác với khoảng cách trung bình
0.76 BLEU so với mô hình tốt thứ hai. Ngoài ra,
việc loại bỏ embedding từ các mô hình dựa trên byte
làm giảm hiệu suất trung bình 0.45 BLEU khi tạo
tiếng Anh. Sự khác biệt này có thể bắt nguồn từ các
tạo tác của dịch ngược, hoặc có lẽ từ bản chất
tập trung vào tiếng Anh của tokenization từ con, được dựa trên
tiền xử lý Moses và BPE. Nhìn chung, những kết quả này
gợi ý rằng mặc dù có số lượng tham số lớn hơn trong
các mô hình từ con, các mô hình ký tự và byte có thể hoạt động
cạnh tranh, nhưng có thể yêu cầu các kỹ thuật tối ưu hóa
hơi khác để làm như vậy.
7 Nghiên cứu liên quan
Có nghiên cứu trước đây về việc thay thế các tokenizer
đặc thù cho ngôn ngữ bằng các phương pháp tokenization
phổ quát hơn. Schütze (2017) cho thấy cách các embedding
n-gram ký tự có thể được huấn luyện hiệu quả bằng
cách phân đoạn văn bản sử dụng một quá trình ngẫu nhiên. Sen-

--- TRANG 5 ---
Benchmark Mô hình dựa trên Embedding Không có Embed
Src Tgt Từ con Ký tự Byte Byte
en zh 20.3 21.2 20.8 21.0
en es 36.7 36.8 36.8 36.8
en ar 12.7 13.1 12.7 12.9
en ru 18.5 18.2 17.7 18.2
en de 29.8 29.3 29.2 29.1
en ja 12.4 13.1 12.5 13.1
en tr 13.9 14.3 14.4 14.1
en vi 30.0 29.1 28.9 28.7
en fa 11.5 12.2 12.1 12.1
en he 26.8 27.1 27.1 26.7
zh en 17.3 17.2 16.3 16.1
es en 40.0 39.1 39.1 38.8
ar en 32.0 31.1 31.2 30.8
ru en 22.9 22.4 22.5 22.0
de en 35.6 34.9 35.0 34.5
ja en 13.5 12.8 12.3 11.2
tr en 24.3 23.3 23.7 23.3
vi en 27.4 25.9 25.9 25.3
fa en 24.5 23.2 23.3 22.6
he en 38.2 37.8 37.4 37.4
Bảng 4: Điểm BLEU kiểm tra của các mô hình baseline và
không có embedding trên tập dữ liệu IWSLT, khi decoder-side token dropout được xem xét như một thiết lập siêu tham số
tiềm năng.
tencePiece (Kudo and Richardson, 2018) tokenize
các chuỗi Unicode thô thành từ con sử dụng BPE (Sennrich et al., 2016) hoặc unigram LM (Kudo, 2018).
Byte BPE (Wang et al., 2019) mở rộng SentencePiece để hoạt động ở mức byte. Trong khi phương pháp này
thực sự bất khả tri ngôn ngữ hơn so với
các tokenizer theo kinh nghiệm, nó vẫn bị suy giảm hiệu suất
khi không áp dụng tiền tokenization (ví dụ
tách bằng khoảng trắng).6Hơn nữa, giả định rằng các đơn vị từ con phải là các
đoạn liền kề không đúng cho các ngôn ngữ có
hình thái học không liên kết như tiếng Ả Rập và
tiếng Do Thái.
Các mô hình ngôn ngữ dựa trên ký tự và byte (Lee
et al., 2017; Al-Rfou et al., 2019) xử lý văn bản thô
như một chuỗi token (ký tự hoặc byte)
và không yêu cầu bất kỳ hình thức tiền xử lý hoặc
tokenization từ nào, và Choe et al. (2019) thậm chí
chứng minh rằng các mô hình ngôn ngữ dựa trên byte có thể
hoạt động tương đương với các mô hình ngôn ngữ dựa trên từ
trên benchmark tỷ từ (Chelba et al.,
2013). Mặc dù kết quả sớm hơn trên các mô hình dịch máy
dựa trên LSTM cho thấy rằng tokenization ký tự
có thể vượt trội hơn tokenization từ con
(Cherry et al., 2018), tài liệu gần đây cho thấy rằng
6https://github.com/google/
sentencepiece/blob/master/doc/
experiments.mdđiều tương tự không đúng cho transformer (Gupta
et al., 2019; Wang et al., 2019; Gao et al., 2020). Để
thu hẹp khoảng cách, nghiên cứu gần đây đề xuất sử dụng các mô hình sâu hơn (Gupta et al., 2019) hoặc kiến trúc chuyên biệt (Gao et al., 2020). Nghiên cứu của chúng tôi khác với
xu hướng này bằng cách loại bỏ các lớp để cải thiện mô hình.
Quan sát này thách thức giả thuyết hàng đầu trong
tài liệu hiện tại – rằng khoảng cách hiệu suất
xuất phát từ việc giảm khả năng mô hình – và gợi ý
rằng vấn đề có thể là một vấn đề tối ưu hóa.
8 Kết luận
Nghiên cứu này thách thức hai giả định chính trong các
mô hình dịch máy thần kinh: sự cần thiết của các lớp embedding, và sự vượt trội của tokenization từ con. Các thí nghiệm trên 10 ngôn ngữ khác nhau
cho thấy rằng, mặc dù việc sử dụng phổ biến của chúng, các mô hình cạnh tranh có thể được huấn luyện mà không có bất kỳ embedding nào bằng cách xử lý văn bản như một chuỗi byte. Cuộc điều tra của chúng tôi gợi ý rằng các phương pháp tokenization khác nhau có thể yêu cầu xem xét lại các kỹ thuật tối ưu hóa tiêu chuẩn được sử dụng với transformer, chủ yếu được thiết kế cho các chuỗi từ con tiếng Anh.
Lời cảm ơn
Nghiên cứu này được hỗ trợ một phần bởi Len Blavatnik và quỹ gia đình Blavatnik, học bổng Alon, và Trung tâm Khoa học Dữ liệu Đại học Tel Aviv.
Tài liệu tham khảo
Meni Adler and Michael Elhadad. 2006. An unsupervised morpheme-based HMM for Hebrew morphological disambiguation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 665–672, Sydney, Australia. Association for Computational Linguistics.
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. Proceedings of the AAAI Conference on Artificial Intelligence, 33:3159–3166.
Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 10–21, Berlin, Germany. Association for Computational Linguistics.

--- TRANG 6 ---
Mauro Cettolo, Jan Niehues, Sebastian Stüker, Luisa Bentivogli, and Marcello Federico. 2014. Report on the 11th iwslt evaluation campaign, iwslt 2014. In Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam, volume 57.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling.
Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. 2018. Revisiting character-based neural machine translation with capacity and compression. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4295–4305, Brussels, Belgium. Association for Computational Linguistics.
Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, and Noah Constant. 2019. Bridging the gap for tokenizer-free language models.
Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, and Richard H.R. Hahnloser. 2020. Character-level translation with self-attention. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1591–1604, Online. Association for Computational Linguistics.
Rohit Gupta, Laurent Besacier, Marc Dymetman, and Matthias Gallé. 2019. Character-based nmt with transformer. arXiv preprint arXiv:1911.04997.
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.
Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66–75, Melbourne, Australia. Association for Computational Linguistics.
Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully character-level neural machine translation without explicit segmentation. Transactions of the Association for Computational Linguistics, 5:365–378.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.
Hinrich Schütze. 2017. Nonsymbolic text representation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 785–796, Valencia, Spain. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.
Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2019. Neural machine translation with byte-level subwords.
