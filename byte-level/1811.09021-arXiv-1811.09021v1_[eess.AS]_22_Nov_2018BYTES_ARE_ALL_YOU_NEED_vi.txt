# 1811.09021.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/byte-level/1811.09021.pdf
# Kích thước tệp: 128243 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:1811.09021v1  [eess.AS]  22 Nov 2018BYTE LÀ TẤT CẢ NHỮNG GÌ BẠN CẦN:
NHẬN DẠNG GIỌNG NÓI VÀ TỔNG HỢP ĐA NGÔN NGỮ ĐẦU CUỐI ĐẾN CUỐI VỚI BYTE
Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan
Google
{boboli,ngyuzh,tsainath,yonghui,williamchan }@google.com
TÓM TẮT
Chúng tôi trình bày hai mô hình đầu cuối đến cuối: Audio-to-Byte (A2B) và Byte-
to-Audio (B2A), cho nhận dạng giọng nói và tổng hợp đa ngôn ngữ.
Các nghiên cứu trước đây chủ yếu sử dụng ký tự, từ con hoặc từ
làm đơn vị lựa chọn để mô hình hóa văn bản. Những đơn vị này khó mở rộng
cho các ngôn ngữ có từ vựng lớn, đặc biệt trong trường hợp xử lý đa
ngôn ngữ. Trong nghiên cứu này, chúng tôi mô hình hóa văn bản thông qua một chuỗi
byte Unicode, cụ thể là chuỗi byte có độ dài thay đổi UTF-8
cho mỗi ký tự. Byte cho phép chúng tôi tránh các softmax lớn
trong các ngôn ngữ có từ vựng lớn, và chia sẻ biểu diễn trong
các mô hình đa ngôn ngữ. Chúng tôi chỉ ra rằng byte vượt trội hơn ký tự
grapheme trên nhiều loại ngôn ngữ khác nhau trong nhận dạng giọng nói
đơn ngôn ngữ đầu cuối đến cuối. Ngoài ra, mô hình byte đa ngôn ngữ của chúng tôi
vượt trội hơn từng baseline đơn ngôn ngữ tương ứng trung bình
4.4% tương đối. Trong giọng nói chuyển mã Nhật-Anh, mô hình byte đa ngôn ngữ
của chúng tôi vượt trội hơn baseline đơn ngôn ngữ 38.6% tương đối. Cuối cùng,
chúng tôi trình bày một mô hình tổng hợp giọng nói đa ngôn ngữ đầu cuối đến cuối
sử dụng biểu diễn byte có hiệu suất phù hợp với các baseline đơn ngôn ngữ
của chúng tôi.
Từ khóa chỉ mục —đa ngôn ngữ, nhận dạng giọng nói đầu cuối đến cuối,
tổng hợp giọng nói đầu cuối đến cuối
1. GIỚI THIỆU
Việc mở rộng phạm vi bao phủ các ngôn ngữ trên thế giới trong các hệ thống
Nhận dạng Giọng nói Tự động (ASR) và Chuyển văn bản thành giọng nói (TTS) đã
thu hút nhiều sự quan tâm trong cả học thuật và công nghiệp [1, 2].
Các hệ thống xử lý giọng nói dựa trên ngữ âm truyền thống yêu cầu
từ điển phát âm ánh xạ các đơn vị ngữ âm thành từ. Việc xây dựng
các tài nguyên như vậy đòi hỏi kiến thức chuyên môn cho từng ngôn ngữ.
Ngay cả với nỗ lực của con người tốn kém, nhiều ngôn ngữ không
có đủ tài nguyên ngôn ngữ học sẵn có để xây dựng các
từ điển như vậy. Ngoài ra, sự không nhất quán trong các hệ thống
ngữ âm cũng thách thức trong việc giải quyết [3] khi hợp nhất các ngôn ngữ khác nhau.
Grapheme đã được sử dụng như một đơn vị mô hình hóa thay thế cho
phoneme đối với xử lý giọng nói [4–7]. Đối với các hệ thống này, một
từ điển chính tả thay vì từ điển phát âm được sử dụng để
cung cấp danh sách từ vựng. Với những tiến bộ gần đây trong mô hình hóa
đầu cuối đến cuối (E2E), grapheme đã trở thành lựa chọn phổ biến. Ví dụ,
[8] đã xây dựng một mô hình Phân loại Thời gian Kết nối (CTC) để
trực tiếp xuất ra grapheme, trong khi [9–11] sử dụng grapheme trong các mô hình
chuỗi đến chuỗi (seq2seq). Các đơn vị từ con được sử dụng trong
[12–14] và các mô hình RNNT [15], và các đơn vị từ được sử dụng bởi [16,
17]. Tương tự, grapheme cũng thường được sử dụng để xây dựng các hệ thống
TTS đầu cuối đến cuối [18–20].
Việc sử dụng grapheme mang lại sự đơn giản của mô hình và cho phép
tối ưu hóa đầu cuối đến cuối, điều này đã được chứng minh mang lại hiệu suất
tốt hơn so với các mô hình dựa trên phoneme [21]. Tuy nhiên, không như
phoneme, kích thước của từ vựng grapheme thay đổi rất nhiều giữa các
ngôn ngữ. Ví dụ, nhiều ngôn ngữ phương Đông, như Trung Quốc,
Nhật Bản và Hàn Quốc, có hàng chục nghìn grapheme. Với
lượng dữ liệu huấn luyện hạn chế, nhiều grapheme có thể có
ít hoặc không có phạm vi bao phủ. Vấn đề thưa thớt nhãn trở nên nghiêm trọng hơn
đối với các mô hình đa ngôn ngữ, nơi cần phải gộp tất cả
các grapheme riêng biệt từ tất cả các ngôn ngữ lại với nhau dẫn đến
từ vựng rất lớn thường có các grapheme đuôi dài với phạm vi bao phủ
rất kém.
Để giải quyết những vấn đề này, [3] đã khám phá việc sử dụng các đặc trưng từ
mô tả ký tự Unicode để xây dựng cây quyết định cho việc phân cụm
grapheme. Tuy nhiên, khi mô hình thay đổi để hỗ trợ
một ngôn ngữ mới, cây quyết định cần được cập nhật. Gần đây,
đã có công trình khám phá việc sử dụng byte Unicode để biểu diễn
văn bản. [22] đã trình bày một mô hình byte-to-span đa ngôn ngữ dựa trên LSTM.
Mô hình tiêu thụ văn bản đầu vào từng byte một và xuất ra
các chú thích span. Các byte Unicode độc lập với ngôn ngữ và
do đó một mô hình duy nhất có thể được sử dụng cho nhiều ngôn ngữ. Kích thước
từ vựng của byte Unicode luôn là 256 và không tăng
khi gộp thêm nhiều ngôn ngữ, điều này ưu tiên hơn
grapheme cho các ứng dụng đa ngôn ngữ.
Trong nghiên cứu này, chúng tôi điều tra tiềm năng của việc biểu diễn văn bản
sử dụng các chuỗi byte được giới thiệu trong [22] cho xử lý giọng nói. Đối với
ASR, chúng tôi áp dụng mô hình Listen, Attend and Spell (LAS) [9] để chuyển đổi
giọng nói đầu vào thành các chuỗi byte Unicode tương ứng với
mã hóa UTF-8 của các văn bản đích. Mô hình này được gọi là
mô hình Audio-to-Byte (A2B). Đối với TTS, mô hình của chúng tôi dựa trên
kiến trúc Tacotron 2 [20], và tạo ra tín hiệu giọng nói từ
một chuỗi byte đầu vào. Mô hình này được gọi là mô hình
Byte-To-Audio (B2A). Vì cả mô hình A2B và mô hình B2A
đều hoạt động trực tiếp trên byte Unicode, chúng có thể xử lý bất kỳ số lượng
ngôn ngữ nào được viết bằng Unicode mà không cần bất kỳ sửa đổi nào đối với
xử lý đầu vào. Do kích thước từ vựng nhỏ được sử dụng, 256
trong trường hợp này, các mô hình của chúng tôi có thể rất nhỏ gọn và rất phù hợp cho
các ứng dụng trên thiết bị.
Chúng tôi báo cáo kết quả nhận dạng cho mô hình A2B trên 4 ngôn ngữ
khác nhau – tiếng Anh, tiếng Nhật, tiếng Tây Ban Nha và tiếng Hàn. Đầu tiên, đối với
từng ngôn ngữ riêng lẻ, chúng tôi so sánh các mô hình A2B với các mô hình Audio-
to-Char (A2C) xuất ra đầu ra grapheme. Đối với tiếng Anh
và tiếng Tây Ban Nha nơi các grapheme là ký tự một byte, A2B
có hiệu suất chính xác như A2C như mong đợi. Tuy nhiên, đối với
các ngôn ngữ có từ vựng grapheme lớn, như tiếng Nhật
và tiếng Hàn, vấn đề thưa thớt nhãn làm tổn hại hiệu suất của các mô hình
A2C, trong khi mô hình A2B chia sẻ byte giữa các grapheme và
hoạt động tốt hơn các mô hình A2C. Hưởng lợi từ biểu diễn độc lập
ngôn ngữ của byte Unicode, chúng tôi thấy có thể
tiến từng bước thêm hỗ trợ cho các ngôn ngữ mới khi xây dựng một mô hình
A2B đa ngôn ngữ. Cụ thể, chúng tôi bắt đầu với một mô hình A2B được huấn luyện

--- TRANG 2 ---
trên tiếng Anh và tiếng Nhật và thêm vào một ngôn ngữ mới sau khi hội tụ.
Khi thêm một ngôn ngữ mới, chúng tôi thường đảm bảo ngôn ngữ mới
có tỷ lệ trộn cao nhất nhưng đồng thời giữ một phần nhỏ
cho mỗi ngôn ngữ hiện có để tránh quên các ngôn ngữ cũ. Chúng tôi thử nghiệm
thêm tiếng Tây Ban Nha và tiếng Hàn từng cái một. Bằng cách này, chúng tôi có thể
tái sử dụng mô hình đã xây dựng trước đó và mở rộng phạm vi bao phủ
ngôn ngữ mà không cần sửa đổi cấu trúc mô hình. Đối với ASR
đa ngôn ngữ, chúng tôi thấy rằng A2B được huấn luyện theo cách này tốt hơn
việc huấn luyện từ đầu. Ngoài ra, bằng cách thêm một vector ngôn ngữ
1-hot vào hệ thống A2B, điều này đã được chứng minh là tăng cường hiệu suất
hệ thống đa phương ngữ [23] và đa ngôn ngữ [24], chúng tôi thấy rằng hệ thống
A2B đa ngôn ngữ vượt trội hơn tất cả các hệ thống phụ thuộc ngôn ngữ.
Chúng tôi đánh giá mô hình B2A trên 3 ngôn ngữ khác nhau, bao gồm
tiếng Anh, tiếng Quan Thoại và tiếng Tây Ban Nha. Một lần nữa, chúng tôi so sánh
các mô hình B2A với những mô hình lấy grapheme làm đầu vào. Đối với cả ba
ngôn ngữ, B2A có hiệu suất tương tự trong các đánh giá chủ quan định lượng
như grapheme được huấn luyện trên các ngôn ngữ đơn lẻ, điều này cung cấp một
mô hình TTS đa ngôn ngữ nhỏ gọn hơn.
2. AUDIO-TO-BYTE ĐA NGÔN NGỮ (A2B)
2.1. Cấu trúc Mô hình
Mô hình Audio-to-Byte (A2B) dựa trên mô hình Listen, Attend
and Spell (LAS) [9], với đích đầu ra được thay đổi từ
grapheme thành byte Unicode. Mạng encoder bao gồm 5
lớp Long Short-Term Memory (LSTM) [25] một chiều, với
mỗi lớp có 1.400 đơn vị ẩn. Mạng decoder bao gồm
2 lớp LSTM một chiều với 1.024 đơn vị ẩn. Attention
dựa trên nội dung cộng [26] với 4 đầu attention được sử dụng để học
sự căn chỉnh giữa các đặc trưng âm thanh đầu vào và các đơn vị
đích đầu ra. Lớp đầu ra là một softmax 256 chiều, tương ứng
với 256 giá trị byte có thể.
Front-end của chúng tôi bao gồm các đặc trưng log-mel 80 chiều, được
tính toán với cửa sổ 25ms và dịch chuyển mỗi 10ms. Tương tự như [27,
28], tại mỗi khung hiện tại, các đặc trưng này được xếp chồng với 3 khung
liên tiếp bên trái và sau đó được lấy mẫu xuống với tốc độ khung 30ms.
Lượng dữ liệu huấn luyện thường thay đổi giữa các ngôn ngữ. Ví dụ,
đối với tiếng Anh chúng tôi có khoảng 3.5 lần lượng dữ liệu
so với các ngôn ngữ khác. Thêm chi tiết về dữ liệu có thể
được tìm thấy trong Mục 4. Trong nghiên cứu này, chúng tôi điều chỉnh tỷ lệ lấy mẫu
dữ liệu của các ngôn ngữ khác nhau để giúp giải quyết sự mất cân bằng dữ liệu.
Chúng tôi chọn tỷ lệ lấy mẫu dựa trên trực giác và quan sát thực nghiệm.
Cụ thể, chúng tôi bắt đầu với việc trộn các ngôn ngữ đều nhau và tăng
tỷ lệ cho một ngôn ngữ nơi hiệu suất cần cải thiện nhiều hơn.
Ngoài ra, một vector ID ngôn ngữ 1-hot đơn giản đã được tìm thấy
hiệu quả trong việc cải thiện các hệ thống đa ngôn ngữ [23, 24]. Chúng tôi cũng
áp dụng vector ID ngôn ngữ 1-hot này làm đầu vào bổ sung được truyền vào
các mô hình A2B, và nối nó với tất cả các lớp bao gồm cả
các lớp encoder và decoder.
2.2. Đơn vị Đầu ra
Các mô hình nhận dạng giọng nói đầu cuối đến cuối thường sử dụng ký tự
[9], từ con [12], mảnh từ [15] hoặc từ [16] làm đơn vị
đầu ra lựa chọn. Các đơn vị dựa trên từ khó mở rộng cho
các ngôn ngữ có từ vựng lớn, điều này làm cho softmax trở nên
cấm đoán lớn, đặc biệt trong các mô hình đa ngôn ngữ. Một giải pháp
là sử dụng các mô hình mảnh từ dựa trên dữ liệu. Các mảnh từ học từ
dữ liệu có thể được huấn luyện để có kích thước từ vựng cố định. Nhưng nó yêu cầu
xây dựng một mô hình mảnh từ mới khi một ngôn ngữ mới hoặc dữ liệu mới
được thêm vào. Ngoài ra, việc xây dựng một mô hình mảnh từ đa ngôn ngữ
thách thức do phân phối grapheme không cân bằng. Các đơn vị grapheme
cho kích thước từ vựng nhỏ nhất trong số các đơn vị này; tuy nhiên,
một số ngôn ngữ vẫn có từ vựng rất lớn. Ví dụ, từ vựng
tiếng Nhật của chúng tôi có hơn 4.8k ký tự. Trong nghiên cứu này, chúng tôi
khám phá việc phân tách grapheme thành một chuỗi byte Unicode.
Mô hình A2B của chúng tôi tạo ra chuỗi văn bản từng byte Unicode
một lúc. Chúng tôi biểu diễn văn bản như một chuỗi byte UTF-8
có độ dài thay đổi. Đối với các ngôn ngữ có ký tự một byte (ví dụ, tiếng Anh), việc
sử dụng đầu ra byte tương đương với đầu ra ký tự grapheme.
Tuy nhiên, đối với các ngôn ngữ có ký tự nhiều byte, như tiếng Nhật
và tiếng Hàn, mô hình A2B cần tạo ra một chuỗi byte đúng
để phát ra một token grapheme. Điều này yêu cầu mô hình
học cả các phụ thuộc byte ngắn hạn trong grapheme, và
các phụ thuộc dài hạn giữa grapheme hoặc thậm chí giữa từ/cụm từ,
điều này sẽ là một nhiệm vụ khó hơn so với hệ thống dựa trên grapheme.
Lợi thế chính của biểu diễn byte là tính độc lập ngôn ngữ của nó.
Bất kỳ chữ viết nào của bất kỳ ngôn ngữ nào có thể biểu diễn bằng Unicode đều có thể
được biểu diễn bằng một chuỗi byte, và không cần thay đổi
cấu trúc mô hình hiện có. Tuy nhiên, đối với các mô hình grapheme, bất cứ khi nào
có một ký hiệu mới được thêm vào, cần thay đổi lớp
softmax đầu ra. Tính độc lập ngôn ngữ này làm cho nó ưu tiên hơn
để mô hình hóa nhiều ngôn ngữ và cũng cho giọng nói chuyển mã [29]
trong một mô hình duy nhất.
3. BYTE-TO-AUDIO ĐA NGÔN NGỮ (B2A)
3.1. Cấu trúc Mô hình
Mô hình Byte-to-Audio (B2A) dựa trên mô hình Tacotron 2 [20].
Embedding chuỗi byte đầu vào được mã hóa bởi ba lớp tích chập,
chứa 512 bộ lọc với hình dạng 5×1, theo sau bởi
một lớp bộ nhớ dài ngắn hạn hai chiều (LSTM) của 256 đơn vị
cho mỗi hướng. Các mã hóa văn bản kết quả được truy cập bởi
decoder thông qua một cơ chế attention nhạy cảm vị trí, lấy
lịch sử attention vào tài khoản khi tính toán một vector
trọng số chuẩn hóa để tổng hợp.
Mạng decoder tự hồi quy lấy đầu vào là mã hóa
byte tổng hợp, và được điều kiện trên một embedding loa cố định
cho mỗi loa, về cơ bản là ID ngôn ngữ vì dữ liệu huấn luyện
của chúng tôi chỉ có một loa mỗi ngôn ngữ. Tương tự như Tacotron 2,
chúng tôi huấn luyện riêng một WaveRNN [30] để nghịch đảo mel spectrogram
thành dạng sóng miền thời gian.
4. KẾT QUẢ
4.1. Byte cho ASR
4.1.1. Dữ liệu
Các thí nghiệm nhận dạng giọng nói của chúng tôi được tiến hành trên một bộ dữ liệu
huấn luyện có giám sát được phiên âm bởi con người bao gồm giọng nói từ 4 ngôn ngữ
khác nhau, cụ thể là tiếng Anh (EN), tiếng Nhật (JA), tiếng Tây Ban Nha (ES)
và tiếng Hàn (KO). Tổng lượng dữ liệu là khoảng 76.000 giờ và
thông tin cụ thể theo ngôn ngữ có thể được tìm thấy trong Bảng 2. Các
phát ngôn huấn luyện này được ẩn danh và phiên âm thủ công, và
đại diện cho lưu lượng tìm kiếm bằng giọng nói và chính tả của Google. Các
phát ngôn này được làm hỏng nhân tạo thêm bằng cách sử dụng một trình mô phỏng phòng
[31], thêm các mức độ nhiễu và vang khác nhau sao cho
SNR tổng thể nằm giữa 0dB và 30dB, với SNR trung bình là
12dB. Các nguồn nhiễu từ YouTube và các bản ghi âm môi trường ồn ào
hàng ngày. Đối với mỗi phát ngôn, chúng tôi tạo ra 10 phiên bản
nhiễu khác nhau để huấn luyện. Đối với đánh giá, chúng tôi báo cáo kết quả trên
các bộ kiểm tra cụ thể theo ngôn ngữ, mỗi bộ chứa khoảng 15K phát ngôn
được ẩn danh, phiên âm thủ công từ lưu lượng tìm kiếm bằng giọng nói của Google mà
không trùng lặp với dữ liệu huấn luyện. Điều này tương đương với khoảng 20

--- TRANG 3 ---
Bảng 1 : Hiệu suất nhận dạng giọng nói của các mô hình đơn ngôn ngữ và đa ngôn ngữ với Audio-to-Byte (A2B) hoặc Audio-to-Char (A2C).
Mô hình ExpId Cấu hìnhNgôn ngữ Huấn luyện Tiếng Anh Tiếng Nhật Tiếng Tây Ban Nha Tiếng Hàn
WER(%) TER(%) WER(%) WER(%)
Đơn
ngôn ngữA1 A2CEN/JA/ES/KO6.9 13.8 11.2 26.5
A2 A2B 6.9 13.2 11.2 25.8
Đa
ngôn ngữB1 A2CEN+JA9.5 13.9 - -
B2 A2B 8.9 13.3 - -
C1 A2B, Khởi tạo NgẫunhiênEN+JA+ES9.7 13.6 11.1 -
C2 A2B, Khởi tạo Từ B2 8.6 13.2 11.0 -
D1 A2B, Khởi tạo Từ C2 EN+JA+ES+KO 8.4 13.4 11.3 26.0
B3 A2B, Mô hình LớnhơnEN+JA8.8 13.6 - -
B4 A2B, Mô hình Lớnhơn, LangVec 7.5 13.3 - -
C3 A2B, Khởi tạo Từ B4 EN+JA+ES 7.5 12.9 10.8 -
D2 A2B, Mô hình Lớnhơn, LangVec
EN+JA+ES+KO8.6 13.5 11.2 25.4
D3 A2B, Khởi tạo Từ C3 7.0 12.8 10.8 25.0
D4 A2B, Khởi tạo Từ D3 6.6 12.6 10.7 24.7
Bảng 2 : Thống kê của dữ liệu huấn luyện và kiểm tra được sử dụng trong các thí nghiệm
của chúng tôi. "utts" biểu thị tổng số phát ngôn trong mỗi bộ và
"time" là tổng thời lượng âm thanh cho mỗi bộ.
Ngôn ngữHuấn luyện Kiểm tra
utts (M) time (Kh) utts (K) time (h)
Tiếng Anh (EN) 35.0 27.5 15.4 20.0
Tiếng Nhật (JA) 9.9 16.5 17.6 22.2
Tiếng Tây Ban Nha (ES) 8.9 16.3 16.6 22.3
Tiếng Hàn (KO) 9.6 16.1 12.6 15.0
giờ dữ liệu kiểm tra mỗi ngôn ngữ. Chi tiết của mỗi bộ kiểm tra phụ thuộc ngôn ngữ
có thể được tìm thấy trong Bảng 2. Chúng tôi sử dụng tỷ lệ lỗi từ (WER)
làm tiêu chí đánh giá cho tất cả các ngôn ngữ ngoại trừ tiếng Nhật,
nơi tỷ lệ lỗi token (TER) được sử dụng để loại trừ sự mơ hồ của
phân đoạn từ.
4.1.2. Hệ thống Phụ thuộc Ngôn ngữ
Chúng tôi đầu tiên xây dựng các mô hình A2B phụ thuộc ngôn ngữ để điều tra
hiệu suất của biểu diễn ngôn ngữ dựa trên byte cho ASR. Để so sánh,
chúng tôi cũng xây dựng các mô hình Audio-to-Char (A2C) tương ứng
có cùng cấu trúc mô hình nhưng xuất ra grapheme. Đối với cả
bốn ngôn ngữ, mô hình xuất ra byte luôn có lớp đầu ra
softmax 256 chiều. Tuy nhiên, đối với các mô hình grapheme, các từ vựng
grapheme khác nhau phải được sử dụng cho các ngôn ngữ khác nhau.
Bộ grapheme hoàn chỉnh cho tiếng Anh và tiếng Tây Ban Nha vì nó chứa
tất cả các chữ cái có thể có trong mỗi ngôn ngữ. Tuy nhiên, đối với tiếng Nhật và tiếng Hàn,
chúng tôi sử dụng từ vựng dữ liệu huấn luyện là 4.8K và 2.7K tương ứng.
Tỷ lệ OOV grapheme bộ kiểm tra tương ứng là 2.1% và 1.0%. Trong khi với
đầu ra byte, chúng tôi không có vấn đề OOV cho bất kỳ ngôn ngữ nào.
Kết quả thí nghiệm được trình bày là A1 cho các mô hình A2C
và A2 cho các mô hình A2B trong Bảng 1. Sự khác biệt giữa
biểu diễn grapheme và byte chủ yếu nằm ở các ngôn ngữ sử dụng
ký tự nhiều byte, như tiếng Nhật và tiếng Hàn. So sánh
A1 với A2, đầu ra byte cho kết quả tốt hơn cho tiếng Nhật và tiếng Hàn.
Trong khi đối với các ngôn ngữ có ký tự một byte, cụ thể là tiếng Anh
và tiếng Tây Ban Nha, chúng có hiệu suất chính xác như mong đợi.
Đầu ra byte yêu cầu mô hình học cả các phụ thuộc byte ngắn hạn
trong grapheme và các phụ thuộc dài hạn giữa grapheme hoặc
thậm chí giữa từ/cụm từ; nó có thể là một nhiệm vụ khó hơn
so với các hệ thống dựa trên grapheme. Tuy nhiên, mô hình A2B mang lại
giảm WER tương đối 4.0% trên tiếng Nhật và 2.6% trên tiếng Hàn
so với các hệ thống grapheme. Thật thú vị khi thấy rằng ngay cả với
cùng cấu trúc mô hình, chúng tôi có thể có hiệu suất tốt hơn với
biểu diễn byte.
4.1.3. Hệ thống ASR Đa ngôn ngữ
Trong thí nghiệm này, chúng tôi chứng minh tính hiệu quả của các mô hình dựa trên byte
so với grapheme cho nhận dạng giọng nói đa ngôn ngữ. Chúng tôi đầu tiên xây dựng
một mô hình tiếng Anh và tiếng Nhật kết hợp bằng cách trộn đều
dữ liệu huấn luyện. Đối với hệ thống grapheme, chúng tôi kết hợp từ vựng grapheme
của tiếng Anh và tiếng Nhật dẫn đến một lớp softmax lớn. Cùng
cấu trúc mô hình ngoại trừ lớp softmax, nơi một softmax
256 chiều được sử dụng, được sử dụng để xây dựng mô hình A2B. Mặc dù
mô hình bây giờ cần nhận dạng hai ngôn ngữ, chúng tôi giữ kích thước
mô hình giống với các mô hình phụ thuộc ngôn ngữ. Từ Bảng 1, hệ thống
byte đa ngôn ngữ (B2) tốt hơn hệ thống grapheme (B1) trên cả bộ kiểm tra
tiếng Anh và tiếng Nhật. Tuy nhiên, hiệu suất của nó tệ hơn so với
các mô hình phụ thuộc ngôn ngữ, điều mà chúng tôi sẽ giải quyết sau trong nghiên cứu này.
Đối với các thí nghiệm sau, chúng tôi tiếp tục chỉ với các mô hình A2B vì chúng
tốt hơn các mô hình A2C.
Để tăng phạm vi bao phủ ngôn ngữ của mô hình, ví dụ, tiếng Tây Ban Nha,
một cách là bắt đầu từ khởi tạo ngẫu nhiên và huấn luyện trên tất cả
dữ liệu huấn luyện. Chúng tôi trộn đều dữ liệu từ ba ngôn ngữ này để
huấn luyện. Kết quả được trình bày là C1 trong Bảng 1. Do tính
độc lập ngôn ngữ của biểu diễn byte, chúng tôi, thay vào đó, có thể
thêm một ngôn ngữ mới bằng cách đơn giản huấn luyện trên dữ liệu mới. Do đó, chúng tôi
tái sử dụng mô hình B2 để tiếp tục huấn luyện với dữ liệu tiếng Tây Ban Nha. Để tránh
mô hình quên các ngôn ngữ trước đó, cụ thể là tiếng Anh và tiếng Nhật,
chúng tôi cũng trộn vào các ngôn ngữ đó nhưng với tỷ lệ trộn thấp hơn một chút
là 3:3:4 cho tiếng Anh, tiếng Nhật và tiếng Tây Ban Nha. Kết quả
được trình bày là C2 trong Bảng 1. Với phương pháp này, mô hình byte
không chỉ huấn luyện nhanh hơn mà còn đạt hiệu suất tốt hơn C1.
Quan trọng nhất, C2 khớp với hiệu suất của các mô hình phụ thuộc
ngôn ngữ trên tiếng Nhật và thậm chí tốt hơn một chút cho tiếng Tây Ban Nha.
Để thêm hỗ trợ cho tiếng Hàn, chúng tôi đơn giản tiếp tục
huấn luyện của C2 với hỗn hợp dữ liệu huấn luyện mới. Chúng tôi sử dụng tỷ lệ
0.23:0.23:0.23:0.31, dựa trên phỏng đoán để cân bằng
các ngôn ngữ hiện có và sử dụng tỷ lệ cao hơn cho các ngôn ngữ mới.
Chúng tôi không điều chỉnh cụ thể tỷ lệ trộn. Kết quả (D1 trong Bảng 1)
cho thấy chúng tôi có thể đến gần hơn với các mô hình phụ thuộc ngôn ngữ
ngoại trừ tiếng Anh. Mặc dù tệ hơn mô hình chỉ tiếng Anh,
D1 cho hiệu suất đa ngôn ngữ tốt nhất trên tiếng Anh cho đến nay.

--- TRANG 4 ---
Bảng 3 : Kết quả trên các mô hình A2B và A2C trên dữ liệu chuyển mã
tiếng Anh-tiếng Nhật.
Mô hình ExpId Cấu hình TER(%)
Đơn
ngôn ngữA1 A2C 36.5
A2 A2B 22.4
Đa
ngôn ngữB1 A2C 21.4
B2 A2B 20.5
D4 A2B Mô hình Lớnhơn, LangVec 21.3
Để cải thiện hiệu suất của các hệ thống đa ngôn ngữ, chúng tôi đầu tiên
tăng số lớp decoder từ 2 lên 6 để xem xét
sự gia tăng biến thiên trong các chuỗi byte khi trộn thêm
ngôn ngữ. Tuy nhiên, kết quả thí nghiệm cho thấy mô hình lớn hơn
cải thiện hiệu suất trên tiếng Anh nhưng giảm trên tiếng Nhật do
khả năng overfitting (so sánh B3 với B2). Để giải quyết vấn đề này,
chúng tôi đưa vào vector ID ngôn ngữ 1-hot cho tất cả các lớp trong
mô hình A2B. Điều này cho phép học các ma trận trọng số độc lập ngôn ngữ
cùng với các bias phụ thuộc ngôn ngữ để đáp ứng nhu cầu
cụ thể cho mỗi ngôn ngữ. Thí nghiệm B4 cho thấy giảm lỗi
đáng kể với vector 1-hot đơn giản này so với B3.
Tương tự, để hỗ trợ nhận dạng tiếng Tây Ban Nha, chúng tôi tiếp tục
huấn luyện của B4 bằng cách trộn các ngôn ngữ với tỷ lệ 3:3:4 nơi
trọng số nhiều hơn được dành cho ngôn ngữ mới. Điều này cho chúng tôi mô hình
C3 vượt trội hơn các mô hình phụ thuộc ngôn ngữ trên cả tiếng Nhật
và tiếng Tây Ban Nha. Hơn nữa, chúng tôi thêm tiếng Hàn theo cách tương tự với
tỷ lệ 0.3:0.15:0.15:0.4. Lần này trong khi đảm bảo tỷ lệ cho
ngôn ngữ mới, tiếng Hàn, là cao nhất, chúng tôi cũng tăng
tỷ lệ cho tiếng Anh vì chúng tôi có nhiều dữ liệu huấn luyện tiếng Anh hơn. Mô hình D3
thắng so với các mô hình phụ thuộc ngôn ngữ ngoại trừ tiếng Anh. Một
giả định cho sự suy giảm trên tiếng Anh là khi trộn vào các
ngôn ngữ khác, mô hình đa ngôn ngữ thấy ít dữ liệu hơn từ mỗi
ngôn ngữ so với các mô hình đơn ngôn ngữ. Để chứng minh điều này, chúng tôi tiếp tục
huấn luyện của D3 với tỷ lệ hiện diện dữ liệu tiếng Anh tăng trong
hỗn hợp, cụ thể chúng tôi sử dụng tỷ lệ 2:1:1:1. Chúng tôi không
điều chỉnh cụ thể các tỷ lệ trộn này được sử dụng để huấn luyện. Mô hình cuối cùng
D4 thắng so với tất cả các mô hình phụ thuộc ngôn ngữ trung bình 4.4%
tương đối. Để so sánh, chúng tôi bao gồm kết quả cho một mô hình
khởi tạo ngẫu nhiên với tỷ lệ trộn dữ liệu huấn luyện đều D2, tệ hơn nhiều.
4.1.4. Phân tích lỗi
Để hiểu rõ hơn về lợi ích của việc sử dụng byte so với grapheme như
biểu diễn ngôn ngữ, chúng tôi lấy tiếng Nhật cho nghiên cứu này và
so sánh các giả thuyết giải mã giữa A1 và A2. Thú vị thay,
mô hình A2B thắng so với các mô hình A2C chủ yếu trên các từ tiếng Anh trong
các phát ngôn có tiếng Anh và tiếng Nhật trộn lẫn. Bộ kiểm tra tiếng Nhật
không được tạo ra đặc biệt để bao gồm các phát ngôn chuyển mã.
Kiểm tra các từ tiếng Anh xuất hiện trong bộ kiểm tra tiếng Nhật, chúng chủ yếu
là danh từ riêng như "Google", "wi-fi", "LAN" v.v. Một
ví dụ về các trường hợp như vậy là A2B tạo ra giả thuyết đúng
"wi-fiオン" trong khi A2C xuất ra "i-iオン". Một ví dụ khác là
"google音声認識" nơi A2B nhận dạng chính xác, nhưng
mô hình A2C bỏ "g" đầu tiên và cho "oogle音声認識".
Một trong những lợi ích tiềm năng của việc sử dụng các mô hình dựa trên byte là cho
giọng nói chuyển mã. Việc thu thập dữ liệu như vậy thách thức. Chất lượng
của giọng nói được nối nhân tạo cách xa thực tế. Trong nghiên cứu này,
chúng tôi sử dụng dữ liệu được lọc từ bộ kiểm tra tiếng Nhật, nơi các
phát ngôn có bản transcript chứa 5 hoặc nhiều ký tự tiếng Anh liên tiếp
được giữ lại. Các phát ngôn này chủ yếu chỉ chứa một từ tiếng Anh
trong văn bản tiếng Nhật. Trong số 17.6K phát ngôn, chúng tôi có
Bảng 4 : Điểm Ý kiến Trung bình (MOS) về tự nhiên của giọng nói với khoảng
tin cậy 95% qua các ngôn ngữ và hệ thống khác nhau.
Ngôn ngữ EN CN ES
Đơn ngôn ngữ C2A 4.24±0.12 3.48±0.11 4.21±0.11
Đa ngôn ngữ B2A 4.23±0.14 3.42±0.12 4.23±0.10
476 câu chuyển mã và chúng tôi báo cáo TER trên tập con này
trong Bảng 3. Với các mô hình đơn ngôn ngữ tiếng Nhật (A1 và A2),
mô hình A2B của chúng tôi vượt trội hơn mô hình A2C 38.6% tương đối. Với
các mô hình đa ngôn ngữ tiếng Anh và tiếng Nhật (B1 và B2), mô hình A2B
của chúng tôi thắng so với mô hình A2C 4.2% tương đối. Chúng tôi cũng kiểm tra
hệ thống D4 trên dữ liệu chuyển mã này. Tuy nhiên, do vector 1-hot
ngôn ngữ được sử dụng trong D4 là cấp độ phát ngôn, hiệu suất tệ hơn
B2. Việc sử dụng thông tin ngôn ngữ cấp độ khung/đoạn có thể
giải quyết vấn đề này, điều sẽ được khám phá trong tương lai.
4.2. Byte cho TTS
4.2.1. Dữ liệu
Các mô hình chuyển văn bản thành giọng nói được huấn luyện trên (1) 44 giờ giọng nói
tiếng Anh Bắc Mỹ được ghi bởi một loa nữ; (2) 37 giờ
giọng nói tiếng Quan Thoại bởi một loa nữ; (3) 44 giờ giọng nói
tiếng Tây Ban Nha Bắc Mỹ bởi một loa nữ. Đối với tất cả các mô hình
so sánh, chúng tôi tổng hợp âm thanh thô ở 24 kHz trong định dạng 16-bit. Chúng tôi
dựa vào các đánh giá Điểm Ý kiến Trung bình (MOS) crowdsourced dựa trên
các bài kiểm tra nghe chủ quan. Tất cả các đánh giá MOS của chúng tôi được căn chỉnh với
thang đo Đánh giá Danh mục Tuyệt đối [32], với điểm số xếp hạng từ 1 đến 5
theo bước tăng 0.5 điểm.
4.2.2. Hệ thống TTS Đa ngôn ngữ
Bảng 4 so sánh MOS tự nhiên chủ quan của mô hình đề xuất
với baseline sử dụng grapheme cho tiếng Anh, tiếng Quan Thoại và tiếng Tây Ban Nha
tương ứng. Cả hai kết quả đều chỉ ra rằng mô hình B2A đa ngôn ngữ
đề xuất có thể so sánh được với mô hình đơn ngôn ngữ tiên tiến¹. Hơn nữa,
chúng tôi quan sát thấy rằng mô hình B2A có thể đọc văn bản chuyển mã.
Tuy nhiên, chúng tôi không có chỉ số tốt để đánh giá chất lượng chuyển mã
cho TTS, ví dụ giọng nói trôi chảy nhưng loa thay đổi cho các ngôn ngữ khác nhau.
Nghiên cứu tương lai có thể khám phá cách đánh giá TTS trong tình huống chuyển mã
và cách tách biệt ngôn ngữ và loa với nhiều dữ liệu huấn luyện hơn.
5. KẾT LUẬN
Trong bài báo này, chúng tôi đã điều tra việc sử dụng byte Unicode như một
biểu diễn ngôn ngữ mới cho cả ASR và TTS. Chúng tôi đề xuất Audio-
to-Byte (A2B) và Byte-to-Audio (B2A) như các mô hình ASR và
TTS đa ngôn ngữ đầu cuối đến cuối. Việc sử dụng byte cho phép chúng tôi xây dựng
một mô hình duy nhất cho nhiều ngôn ngữ mà không cần sửa đổi cấu trúc
mô hình cho các ngôn ngữ mới. Điều này mang lại việc chia sẻ biểu diễn qua
các grapheme, và rất quan trọng cho các ngôn ngữ có từ vựng grapheme lớn,
đặc biệt trong xử lý đa ngôn ngữ. Các thí nghiệm của chúng tôi cho thấy
các mô hình byte vượt trội hơn các mô hình grapheme trong cả mô hình đa ngôn ngữ
và đơn ngôn ngữ. Hơn nữa, mô hình A2B đa ngôn ngữ của chúng tôi vượt trội hơn
các baseline đơn ngôn ngữ trung bình 4.4% tương đối. Tính độc lập ngôn ngữ
của các mô hình byte cung cấp một góc nhìn mới cho vấn đề chuyển mã,
nơi mô hình A2B đa ngôn ngữ của chúng tôi đạt được cải thiện tương đối 38.6%
so với các baseline đơn ngôn ngữ. Cuối cùng, chúng tôi cũng cho thấy các mô hình B2A
đa ngôn ngữ của chúng tôi khớp với hiệu suất của các baseline đơn ngôn ngữ trong TTS.
¹MOS tệ hơn [20] vì chúng tôi có OOV trong bộ kiểm tra.

--- TRANG 5 ---
6. TÀI LIỆU THAM KHẢO
[1] Tanja Schultz và Katrin Kirchhoff, Xử lý giọng nói đa ngôn ngữ,
Elsevier, 2006.
[2] Hervé Bourlard, John Dines, Mathew Magimai-Doss, Philip N
Garner, David Imseng, Petr Motlicek, Hui Liang, Lakshmi Sa-
heer, và Fabio Valente, "Xu hướng hiện tại trong xử lý giọng nói
đa ngôn ngữ," Sadhana, tập 36, số 5, trang 885–915, 2011.
[3] Mark JF Gales, Kate M Knill, và Anton Ragni, "Các hệ thống
graphemic dựa trên Unicode cho các ngôn ngữ tài nguyên hạn chế,"
trong ICASSP, 2015.
[4] Stephan Kanthak và Hermann Ney, "Mô hình âm học phụ thuộc
ngữ cảnh sử dụng grapheme cho nhận dạng giọng nói từ vựng lớn,"
trong ICASSP, 2002.
[5] Mirjam Killer, Sebastian Stuker, và Tanja Schultz,
"Nhận dạng giọng nói dựa trên grapheme," trong Hội nghị
Châu Âu thứ tám về Truyền thông Giọng nói và Công nghệ, 2003.
[6] Sebastian Stüker và Tanja Schultz, "Một hệ thống nhận dạng
giọng nói dựa trên grapheme cho tiếng Nga," trong Hội nghị thứ 9
Giọng nói và Máy tính, 2004.
[7] Willem D Basson và Marelie H Davel, "So sánh
nhận dạng giọng nói dựa trên grapheme và phoneme cho
tiếng Afrikaans," trong PRASA, 2012.
[8] Alex Graves và Navdeep Jaitly, "Hướng tới nhận dạng giọng nói
đầu cuối đến cuối với mạng nơ-ron hồi quy," trong ICML, 2014.
[9] William Chan, Navdeep Jaitly, Quoc Le, và Oriol Vinyals,
"Listen, Attend and Spell: Một Mạng Nơ-ron cho Nhận dạng
Giọng nói Hội thoại Từ vựng Lớn," trong ICASSP, 2016.
[10] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon
Brakel, và Yoshua Bengio, "Nhận dạng giọng nói từ vựng lớn
dựa trên attention đầu cuối đến cuối," trong ICASSP, 2016.
[11] William Chan và Ian Lane, "Về Nhận dạng Giọng nói dựa trên
Attention Trực tuyến và Huấn luyện Kết hợp Ký tự-Pinyin
tiếng Quan Thoại," trong INTERSPEECH, 2016.
[12] William Chan, Yu Zhang, Quoc Le, và Navdeep Jaitly, "Phân
tách Chuỗi Ẩn," trong ICLR, 2017.
[13] Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit
Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kan-
nan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep
Jaitly, Bo Li, Jan Chorowsk, và Michiel Bacchiani, "Nhận dạng
Giọng nói Tiên tiến với Các Mô hình Chuỗi-đến-Chuỗi," trong
ICASSP, 2018.
[14] Albert Zeyer, Kazuki Irie, Ralf Schluter, và Hermann Ney,
"Cải thiện huấn luyện các mô hình attention đầu cuối đến cuối cho
nhận dạng giọng nói," trong INTERSPEECH, 2018.
[15] K. Rao, R. Prabhavalkar, và H. Sak, "Khám phá Kiến trúc,
Dữ liệu và Đơn vị cho Nhận dạng Giọng nói Đầu cuối đến cuối
Streaming với RNN-Transducer," trong ASRU, 2017.
[16] Hagen Soltau, Hank Liao, và Hasim Sak, "Bộ Nhận dạng Giọng nói
Nơ-ron: Mô hình LSTM Acoustic-to-Word cho Nhận dạng Giọng nói
Từ vựng Lớn," trong arXiv:1610.09975, 2016.
[17] Jinyu Li, Guoli Ye, Amit Das, Rui Zhao, và Yifan Gong, "Thúc
đẩy Mô hình CTC Acoustic-to-Word," trong ICASSP, 2018.
[18] Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe
Santos, Kyle Kastner, Aaron Courville, và Yoshua Bengio,
"Char2wav: Tổng hợp giọng nói đầu cuối đến cuối," trong ICLR:
Workshop, 2017.
[19] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu,
Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao,
Zhifeng Chen, Samy Bengio, và cộng sự, "Tacotron: Một mô hình
tổng hợp text-to-speech hoàn toàn đầu cuối đến cuối," arXiv preprint, 2017.
[20] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, RJ Skerry-Ryan, và cộng sự, "Tổng hợp TTS tự nhiên
bằng cách điều kiện wavenet trên dự đoán mel spectrogram," trong
ICASSP, 2018.
[21] Rohit Prabhavalkar, Kanishka Rao, Tara N Sainath, Bo Li, Leif
Johnson, và Navdeep Jaitly, "Một so sánh các mô hình chuỗi-đến-
chuỗi cho nhận dạng giọng nói," trong INTERSPEECH,
2017, trang 939–943.
[22] Dan Gillick, Cliff Brunk, Oriol Vinyals, và Amarnag Sub-
ramanya, "Xử lý ngôn ngữ đa ngôn ngữ từ byte," trong
NAACL, 2016.
[23] Bo Li, Tara N Sainath, Khe Chai Sim, Michiel Bacchiani, Eu-
gene Weinstein, Patrick Nguyen, Zhifeng Chen, Yonghui Wu,
và Kanishka Rao, "Nhận dạng giọng nói đa phương ngữ với
một mô hình sequence-to-sequence duy nhất," trong ICASSP, 2018.
[24] Shubham Toshniwal, Tara N Sainath, Ron J Weiss, Bo Li, Pe-
dro Moreno, Eugene Weinstein, và Kanishka Rao, "Nhận dạng
giọng nói đa ngôn ngữ với một mô hình đầu cuối đến cuối duy nhất," trong
ICASSP, 2018.
[25] Sepp Hochreiter và Jürgen Schmidhuber, "Bộ nhớ ngắn hạn
dài," Neural computation, tập 9, số 8, trang 1735–1780,
1997.
[26] Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio,
"Dịch máy nơ-ron bằng cách học đồng thời căn chỉnh và
dịch," 2015.
[27] Golan Pundak và Tara N Sainath, "Các Mô hình Âm học Mạng
Nơ-ron Tốc độ Khung Thấp hơn," trong INTERSPEECH, 2016.
[28] Haşim Sak, Andrew Senior, Kanishka Rao, và Françoise
Beaufays, "Các mô hình âm học mạng nơ-ron hồi quy nhanh và
chính xác cho nhận dạng giọng nói," arXiv preprint
arXiv:1507.06947, 2015.
[29] Peter Auer, Code-switching trong hội thoại: Ngôn ngữ, tương tác
và bản sắc, Routledge, 2013.
[30] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb
Noury, Norman Casagrande, Edward Lockhart, Florian Stim-
berg, Aäron van den Oord, Sander Dieleman, và Koray
Kavukcuoglu, "Tổng hợp âm thanh nơ-ron hiệu quả," trong ICML,
2018.
[31] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. N.
Sainath, và M. Bacchiani, "Tạo ra Các Phát ngôn Mô phỏng
Quy mô Lớn trong Phòng Ảo để Huấn luyện Mạng Nơ-ron Sâu
cho Nhận dạng Giọng nói Trường xa trong Google Home," trong
INTERSPEECH, 2017.
[32] ITUT Rec, "P. 800: Phương pháp xác định chủ quan chất lượng
truyền," Liên minh Viễn thông Quốc tế, Geneva, 1996.
