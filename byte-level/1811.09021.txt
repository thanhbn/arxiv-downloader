# 1811.09021.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/byte-level/1811.09021.pdf
# File size: 128243 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:1811.09021v1  [eess.AS]  22 Nov 2018BYTES ARE ALL YOU NEED:
END-TO-END MULTILINGUAL SPEECH RECOGNITION AND SYNTHESIS WITH BYTES
Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, William Chan
Google
{boboli,ngyuzh,tsainath,yonghui,williamchan }@google.com
ABSTRACT
We present two end-to-end models: Audio-to-Byte (A2B) and B yte-
to-Audio (B2A), for multilingual speech recognition and sy nthesis.
Prior work has predominantly used characters, sub-words or words
as the unit of choice to model text. These units are difﬁcult t o scale
to languages with large vocabularies, particularly in the c ase of mul-
tilingual processing. In this work, we model text via a seque nce
of Unicode bytes, speciﬁcally, the UTF-8 variable length by te se-
quence for each character. Bytes allow us to avoid large soft maxes
in languages with large vocabularies, and share representa tions in
multilingual models. We show that bytes are superior to grap heme
characters over a wide variety of languages in monolingual e nd-to-
end speech recognition. Additionally, our multilingual by te model
outperform each respective single language baseline on ave rage by
4.4% relatively. In Japanese-English code-switching spee ch, our
multilingual byte model outperform our monolingual baseli ne by
38.6% relatively. Finally, we present an end-to-end multil ingual
speech synthesis model using byte representations which ma tches
the performance of our monolingual baselines.
Index Terms —multilingual, end-to-end speech recognition,
end-to-end speech synthesis
1. INTRODUCTION
Expanding the coverage of the world’s languages in Automati c
Speech Recognition (ASR) and Text-to-Speech (TTS) systems have
been attracting much interest in both academia and industry [1, 2].
Conventional phonetically-based speech processing syste ms require
pronunciation dictionaries that map phonetic units to word s. Build-
ing such resources require expert knowledge for each langua ge.
Even with the costly human effort involved, many languages d o
not have sufﬁcient linguistic resources available for buil ding such
dictionaries. Additionally, the inconsistency in the phon etic systems
is also challenging to resolve [3] when merging different la nguages.
Graphemes have been used as an alternative modeling unit to
phonemes for speech processing [4–7]. For these systems, an or-
thographic lexicon instead of a pronunciation dictionary i s used to
provide a vocabulary list. With recent advances in end-to-e nd (E2E)
modeling, graphemes have become a popular choice. For examp le,
[8] built a Connectionist Temporal Classiﬁcation (CTC) mod el to di-
rectly output graphemes, while [9–11] used graphemes in seq uence-
to-sequence (seq2seq) models. Sub-word units were used in s eq2seq
[12–14] and RNNT [15] models, and word units were used by [16,
17]. Similarly, graphemes are also commonly used to build en d-to-
end TTS systems [18–20].
The use of graphemes bring model simplicity and enables
end-to-end optimization, which has been shown to yield bett er
performance than phoneme-based models [21]. However, unli kephonemes, the size of the grapheme vocabulary varies greatl y across
languages. For example, many eastern languages, such as Chi nese,
Japanese and Korean, have tens of thousands of graphemes. Wi th
limited amounts of training data, many graphemes may have li t-
tle or no coverage. The label sparsity issue becomes even mor e
severe for multilingual models, where one needs to pool all t he
distinct graphemes from all languages together resulting i n a very
large vocabulary that often has long tail graphemes with ver y poor
coverage.
To address these problems, [3] explored the use of features f rom
Unicode character descriptions to construct decision tree s for clus-
tering graphemes. However, when the model changes to suppor t
a new language, the decision tree needs to be updated. Recent ly,
there has been work on exploring the use of Unicode bytes to re pre-
sent text. [22] presented an LSTM-based multilingual byte- to-span
model. The model consumes the input text byte-by-byte and ou tputs
span annotations. The Unicode bytes are language independe nt and
hence a single model can be used for many languages. The vocab -
ulary size of Unicode bytes is always 256 and it does not incre ase
when pooling more languages together, which is more prefera ble to
graphemes for multilingual applications.
In this work, we investigate the potential of representing t ext
using byte sequences introduced in [22] for speech processi ng. For
ASR, we adopt the Listen, Attend and Spell (LAS) [9] model to c on-
vert input speech into sequences of Unicode bytes which corr espond
to the UTF-8 encoding of the target texts. This model is refer red to
as the Audio-to-Byte (A2B) model. For TTS, our model is based on
the Tacotron 2 architecture [20], and generates speech sign als from
an input byte sequence. This model is referred to as the the By te-
To-Audio (B2A) model. Since both the A2B model and the B2A
model operate directly on Unicode bytes, they can handle any num-
ber of languages written in Unicode without any modiﬁcation to the
input processing. Due to the small vocabulary size being use d, 256
in this case, our models can be very compact and very suitable for
on-device applications.
We report recognition results for the A2B model on 4 differ-
ent languages – English, Japanese, Spanish and Korean. Firs t, for
each individual language, we compare our A2B models to Audio -
to-Char (A2C) models which emit grapheme outputs. For Engli sh
and Spanish where the graphemes are single-byte characters , A2B
has the exact same performance as A2C as expected. However, f or
languages that have a large grapheme vocabulary, such as Jap anese
and Korean, the label sparsity issue hurts the performance o f A2C
models, whereas the A2B model shares bytes across graphemes and
performs better than A2C models. Beneﬁting from the languag e in-
dependence representation of Unicode bytes, we ﬁnd it is pos sible to
progressively add support for new languages when building a multi-
lingual A2B model. Speciﬁcally, we start with an A2B model tr ained

--- PAGE 2 ---
on English and Japanese and add in a new language after conver -
gence. When adding a new language we usually make sure the new
language has the highest mixing ratio but meanwhile keeping small
portion for each of the existing languages to avoid forgetti ng older
ones. We experiment adding Spanish and Korean one at a time. I n
this way, we can reuse the previously built model and expand t he
language coverage without modifying the model structure. F or mul-
tilingual ASR, we ﬁnd that the A2B trained in this way is bette r than
training from scratch. In addition, by adding a 1-hot langua ge vector
to the A2B system, which has been shown to boost multi-dialec t [23]
and multilingual [24] system performance, we ﬁnd that the mu ltilin-
gual A2B system outperforms all the language dependent ones .
We evaluate the B2A model on 3 different languages, which
include English, Mandarin and Spanish. Again, we compare B2 A
models with those take graphemes as input. For all three lang uages,
B2A has similar performance on quantitative subjective eva luations
as graphemes trained on single languages, this providing a m ore
compact multilingual TTS model.
2. MULTILINGUAL AUDIO-TO-BYTE (A2B)
2.1. Model Structure
The Audio-to-Byte (A2B) model is based on the Listen, Attend
and Spell (LAS) [9] model, with the output target changed fro m
graphemes to Unicode bytes. The encoder network consists of 5
unidirectional Long Short-Term Memory (LSTMs) [25] layers , with
each layer having 1,400hidden units. The decoder network consists
of 2 unidirectional LSTM layers with 1,024hidden units. Additive
content-based attention [26] with 4 attention heads are use d to learn
the alignment between the input audio features and the outpu t target
units. The output layer is a 256 dimensional softmax, corres ponding
to the 256 possible byte values.
Our front-end consists of 80-dimensional log-mel features , com-
puted with a 25ms window and shifted every 10ms. Similar to [2 7,
28], at each current frame, these features are stacked with 3 consec-
utive frames to the left and then down-sampled to a 30ms frame rate.
The amount of training data usually varies across languages . For
example, for English we have around 3.5 times the amount of da ta
compared to the other languages. More details about data can be
found in Section 4. In this work, we adjust the data sampling r atio of
the different languages to help tackle the data imbalance. W e choose
the sampling ratio based on intuition and empirical observa tions.
Specially, we start with mixing the language equally and inc rease
the ratio for a language where the performance needs more imp rove-
ment. In addition, a simple 1-hot language ID vector has been found
to be effective improving multilingual systems [23, 24]. We also
adopt this 1-hot language ID vector as additional input pass ed into
the A2B models, and concatenate it to all the layers includin g both
the encoder and decoder layers.
2.2. Output Unit
End-to-end speech recognition models have typically used c har-
acters [9], sub-words [12], word-pieces [15] or words [16] a s the
output unit of choice. Word-based units are difﬁcult to scal e for
languages with large vocabularies, which makes the softmax pro-
hibitively large, especially in multilingual models. One s olution is
to use data-driven word-piece models. Word-pieces learned from
data can be trained to have a ﬁxed vocabulary size. But it requ ires
building a new word-piece model when a new language or new dat a
is added. Additionally, building a multilingual word-piec e model is
challenging due to the unbalanced grapheme distribution. G raphemeunits give the smallest vocabulary size among these units; h owever,
some languages still have very large vocabularies. For exam ple our
Japanese vocabulary has over 4.8k characters. In this work, we
explore decomposing graphemes into a sequence of Unicode by tes.
Our A2B model generates the text sequence one Unicode byte
at a time. We represent text as a sequence of variable length U TF-8
bytes. For languages with single-byte characters (e.g., En glish), the
use of byte output is equivalent to the grapheme character ou tput.
However, for languages with multi-byte characters, such as Japanese
and Korean, the A2B model needs to generate a sequence of cor-
rect bytes to emit one grapheme token. This requires the mode l to
learn both the short-term within-grapheme byte dependenci es, and
the long-term inter-grapheme or even inter-word/phrase de penden-
cies, which would be a harder task than grapheme based system .
The main advantage of byte representation is its language in de-
pendence. Any script of any language representable by Unico de can
be represented by a byte sequence, and there is no need to chan ge the
existing model structure. However, for grapheme models, wh enever
there is a new symbol added, there is a need to change the outpu t
softmax layer. This language independence makes it more pre fer-
able for modeling multiple languages and also code-switchi ng [29]
speech within a single model.
3. MULTILINGUAL BYTE-TO-AUDIO (B2A)
3.1. Model Structure
The Byte-to-Audio (B2A) model is based on Tacotron 2 [20] mod el.
The input byte sequence embedding is encoded by three convol u-
tional layers, which contain 512 ﬁlters with shape 5×1, followed by
a bidirectional long short-term memory (LSTM) layer of 256 u nits
for each direction. The resulting text encodings are access ed by
the decoder through a location sensitive attention mechani sm, which
takes attention history into account when computing a norma lized
weight vector for aggregation.
The autoregressive decoder network takes as input the aggre -
gated byte encoding, and conditioned on a ﬁxed speaker embed ding
for each speaker, which is essentially the language ID since our train-
ing data has only one speaker per language. Similar to Tacotr on 2,
we separately train a WaveRNN [30] to invert mel spectrogram s to a
time-domain waveform.
4. RESULTS
4.1. Byte for ASR
4.1.1. Data
Our speech recognition experiments are conducted on a human tran-
scribed supervised training set consisting speech from 4 di fferent
languages, namely English (EN), Japanese (JA), Spanish (ES ) and
Korean (KO). The total amount of data is around 76,000 hours a nd
the language-speciﬁc information can be found in Table 2. Th ese
training utterances are anonymized and hand-transcribed, and are
representative of Google’s voice search and dictation traf ﬁc. These
utterances are further artiﬁcially corrupted using a room s imulator
[31], adding varying degrees of noise and reverberation suc h that
the overall SNR is between 0dB and 30dB, with an average SNR of
12dB. The noise sources are from YouTube and daily life noisy en-
vironmental recordings. For each utterance, we generated 1 0 differ-
ent noisy versions for training. For evaluation, we report r esults on
language-speciﬁc test sets, each contains roughly 15K anon ymized,
hand-transcribed utterances from Google’s voice search tr afﬁc with-
out overlapping with the training data. This amounts to roug hly 20

--- PAGE 3 ---
Table 1 : Speech recognition performance of monolingual and multil ingual with Audio-to-Byte (A2B) or Audio-to-Char (A2C) mod els.
Model ExpId ConﬁgurationTraining English Japanese Spanish Korean
Languages WER(%) TER(%) WER(%) WER(%)
Mono-
lingualA1 A2CEN/JA/ES/KO6.9 13.8 11.2 26.5
A2 A2B 6.9 13.2 11.2 25.8
Multi-
lingualB1 A2CEN+JA9.5 13.9 - -
B2 A2B 8.9 13.3 - -
C1 A2B, Random InitEN+JA+ES9.7 13.6 11.1 -
C2 A2B, Init From B2 8.6 13.2 11.0 -
D1 A2B, Init From C2 EN+JA+ES+KO 8.4 13.4 11.3 26.0
B3 A2B, Larger ModelEN+JA8.8 13.6 - -
B4 A2B, Larger Model, LangVec 7.5 13.3 - -
C3 A2B, Init From B4 EN+JA+ES 7.5 12.9 10.8 -
D2 A2B, Larger Model, LangVec
EN+JA+ES+KO8.6 13.5 11.2 25.4
D3 A2B, Init From C3 7.0 12.8 10.8 25.0
D4 A2B, Init From D3 6.6 12.6 10.7 24.7
Table 2 : Statistics of the training and testing data used in our expe r-
iments. “utts” denotes the total number of utterances in eac h set and
“time” is the total duration of audio for each set.
LanguagesTrain Test
utts (M) time (Kh) utts (K) time (h)
English (EN) 35.0 27.5 15.4 20.0
Japanese (JA) 9.9 16.5 17.6 22.2
Spanish (ES) 8.9 16.3 16.6 22.3
Korean (KO) 9.6 16.1 12.6 15.0
hours of test data per language. Details of each language dep endent
test set can be found in Table 2. We use word error rates (WERs)
as the evaluation criterion for all the languages except for Japanese,
where token error rates (TERs) are used to exclude the ambigu ity of
word segmentation.
4.1.2. Language Dependent Systems
We ﬁrst build language dependent A2B models to investigate t he per-
formance of byte-based language representations for ASR. F or com-
parison, we also build corresponding Audio-to-Char (A2C) m odels
that have the same model structure but output graphemes. For all
the four languages, the model which outputs byte always has a 256-
dimensional softmax output layer. However, for the graphem e mod-
els, different grapheme vocabularies have to be used for dif ferent
languages. The grapheme set is complete for English and Span ish
as it contains all possible letters in each of the languages. How-
ever, for Japanese and Korean, we use the training data vocab ularies
which are 4.8K and2.7K respectively. The corresponding test set
grapheme OOV rates are 2.1% and 1.0%. Whereas with byte out-
puts, we do not have OOV problem for any language.
Experimental results are presented as A1for the A2C models
andA2for the A2B models in Table 1. The difference between
grapheme and byte representations mainly lies in languages which
use multi-byte characters, such as Japanese and Korean. Com paring
A1toA2, byte outputs give better results for Japanese and Korean.
While for languages with single-byte characters, namely En glish
and Spanish, they have exactly the same performance as expec ted.
Byte output requires the model to learn both the short-term w ithin-
grapheme byte dependencies and the long-term inter-graphe me or
even inter-word/phrase dependencies; it would possibly be a harder
task than grapheme based systems. However, the A2B model yie ldsa 4.0% relative WER reduction on Japanese and 2.6% on Korean
over the grapheme systems. It is interesting to see that even with the
same model structure, we are able to get better performance w ith the
byte representation.
4.1.3. Multilingual ASR Systems
In this experiment, we justify the effectiveness of byte bas ed models
over graphemes for multilingual speech recognition. We ﬁrs t build
a joint English and Japanese model by equally mixing the trai ning
data. For grapheme system, we combine the grapheme vocab of E n-
glish and Japanese which leads to a large softmax layer. The s ame
model structure except for the softmax layer, where a 256 dim en-
sional softmax is used, is used to build the A2B model. Althou gh
the model now needs to recognize two languages, we keep the mo del
size the same as those language dependent ones. From Table 1, the
multilingual byte system ( B2) is better than the grapheme system
(B1) on both English and Japanese test sets. However, its perfor -
mance is worse than those language dependent ones, which we w ill
address later in this work. For the following experiments, w e con-
tinue with only the A2B models as they are better than A2C mode ls.
To increase the model’s language coverage, e.g., Spanish, o ne
way is to start from a random initialization and train on all t he train-
ing data. We equally mix the data from these three languages f or
training. The results are presented as C1in Table 1. Due to the lan-
guage independence of the byte representation, we, alterna tively, can
add a new language by simply training on new data. Hence, we re use
theB2model to continue training with Spanish data. To avoid the
model forgetting previous languages, namely English and Ja panese,
we also mix in those languages but with a slightly lower mixin g ra-
tio which is 3:3:4 for English, Japanese and Spanish. The res ults
are presented as C2in Table 1. With this method, the byte model
not only trains faster but also achieves better performance thanC1.
Most importantly, C2matches the performance of language depen-
dent models on Japanese and is even slightly better for Spani sh.
To add support for Korean, we simply continued the train-
ing ofC2with the new training data mixture. We use a ratio of
0.23:0.23:0.23:0.31, which is based on heuristics to balan ce the
existing languages and use a higher ratio for the new languag es. We
did not speciﬁcally tune the mixing ratio. The results ( D1in Table 1)
show that we are able to get closer to the language dependent m odels
except for English. Even though worse than the English only m odel,
D1gives the best multilingual performance on English so far.

--- PAGE 4 ---
Table 3 : Results on A2B and A2C models on English-Japanese
code-switching data.
Model ExpId Conﬁguration TER(%)
Mono-
lingualA1 A2C 36.5
A2 A2B 22.4
Multi-
lingualB1 A2C 21.4
B2 A2B 20.5
D4 A2B Larger Model, LangVec 21.3
To improve the performance of the multilingual systems, we ﬁ rst
increase the number of decoder layers from 2 to 6 in considera tion
of the increased variations in byte sequences when mixing mo re lan-
guages. However, experimental results show that the larger model
improves performance on English but degrades on Japanese du e to
potential over-ﬁtting (comparing B3toB2). To address this prob-
lem, we brings in the 1-hot language ID vector to all the layer s in
the A2B model. This enables the learning of language indepen dent
weight matrices together with language dependent biases to cater the
speciﬁc needs for each language. Experiment B4shows dramatic er-
ror reduction with this simple 1-hot vector comparing to B3.
Similarly, to support the recognition of Spanish, we contin ue the
training of B4by mixing the languages at the ratio of 3:3:4 where
more weight is given to the new language. This gives us the mod el
C3which outperforms language dependent ones on both Japanese
and Spanish. Furthermore, we add Korean in a similar way with the
ratio of 0.3:0.15:0.15:0.4. This time while making sure the ratio for
the new language, Korean, is the highest, we also increase th e ratio
for English as we have more English training data. The model D3
wins over language dependent models except for English. One as-
sumption for the degradation on English is that when mixing i n other
languages, the multilingual model sees less data from each l anguage
than those single language models. To justify this, we conti nue the
training of D3with an increased English data presence ratio in the
mixture, speciﬁcally we use the ratio of 2:1:1:1. We didn’t s pecif-
ically tune these mixing ratios used for training. The ﬁnal m odel
D4wins over all the language dependent ones on average by 4.4%
relatively. For comparison, we include the results for a ran domly
initialized model with equal training data mixing ratio D2, which is
much worse.
4.1.4. Error analysis
To further understand the gains of using bytes versus graphe mes as
language representations, we take Japanese for this study a nd com-
pare the decoding hypotheses between A1andA2. Interestingly, the
A2B model wins over the A2C models mainly on English words in
utterances with mixed English and Japanese. The Japanese te st set
was not particularly created to include code-switching utt erances.
Examining the English words appeared in Japanese test set, t hey are
mostly proper nouns such as “ Google ”, “wi-ﬁ”, “LAN” etc. One
example of such cases is the A2B generates the correct hypoth esis
“wi-ﬁオン ” while the A2C outputs “ i-iオン ”. Another example is
“google音声認識” where the A2B recognizes it correctly, but the
A2C model drops the initial “ g” and gives “ oogle音声認識”.
One of the potential beneﬁts of using byte-based models is fo r
code-switching speech. Collecting such data is challengin g. The
quality of artiﬁcially concatenated speech is far from real . In this
study we use data ﬁltered from the Japanese test set, where ut ter-
ances having transcript that contains 5 or more consecutive English
characters are kept. These utterances mostly contain only a single
English word in Japanese texts. Out of the 17.6K utterances, we getTable 4 : Speech naturalness Mean Opinion Score (MOS) with 95%
conﬁdence intervals across different language and systems .
Languages EN CN ES
Monolingual C2A 4.24±0.12 3.48±0.11 4.21±0.11
Multilingual B2A 4.23±0.14 3.42±0.12 4.23±0.10
476 code-switching sentences and we report the TERs on this s ubset
in Table 3. With Japanese monolingual models ( A1andA2), our
A2B model outperforms the A2C model by 38.6% relatively. Wit h
English and Japanese multilingual models ( B1andB2), our A2B
model wins over the A2C model by 4.2% relatively. We also test
systemD4on these code-switch data. However, due to the language
1-hot vector used in D4is utterance-level, the performance is worse
thanB2. Using frame/segment level language information may ad-
dress this problem, which will be explored in future.
4.2. Byte for TTS
4.2.1. Data
Text-to-speech models were trained on (1) 44 hours of North A mer-
ican English speech recorded by a female speaker; (2) 37 hour s of
Mandarin speech by a female speaker; (3) 44 hours of North Ame r-
ican Spanish speech by a female speaker. For all compared mod -
els, we synthesize raw audio at 24 kHz in 16-bit format. We rel y
on crowdsourced Mean Opinion Score (MOS) evaluations based on
subjective listening tests. All our MOS evaluations are ali gned to the
Absolute Category Rating scale [32], with rating scores from 1 to 5
in 0.5 point increments.
4.2.2. Multilingual TTS System
Table 4 compares subjective naturalness MOS of the proposed model
to the baseline using graphemes for English, Mandarin and Sp an-
ish respectively. Both results indicate that the proposed m ultilin-
gual B2A model is comparable as the state-of-the-art monoli ngual
model1. Moreover, we observed that the B2A model was able to
read code-switching text. However, we don’t have good metri c to
evaluate the quality of code-switching for TTS, e.g. the spe ech is
ﬂuent but the speaker is changed for different language. Fut ure work
may explore how to evaluate TTS on code-switching scenario a nd
how to disentangle language and speaker given more training data.
5. CONCLUSIONS
In this paper, we investigated the use of Unicode bytes as a ne w lan-
guage representation for both ASR and TTS. We proposed Audio -
to-Byte (A2B) and Byte-to-Audio (B2A) as multilingual ASR a nd
TTS end-to-end models. The use of bytes allows us to build a si ngle
model for many languages without modifying the model struct ure
for new ones. This brings representation sharing across gra phemes,
and is crucial for languages with large grapheme vocabulari es, es-
pecially in multilingual processing. Our experiments show that
byte models outperform grapheme models in both multilingua l and
monolingual models. Moreover, our multilingual A2B model o ut-
performs our monolingual baselines by 4.4% relatively on average.
The language independence of byte models provides a new pers pec-
tive to the code-switching problem, where our multilingual A2B
model achieves 38.6% relative improvement over our monolingual
baselines. Finally, we also show our multilingual B2A model s match
the performance of our monolingual baselines in TTS.
1MOS is worse than [20] because we have OOV in the test set.

--- PAGE 5 ---
6. REFERENCES
[1] Tanja Schultz and Katrin Kirchhoff, Multilingual speech pro-
cessing , Elsevier, 2006.
[2] Herv´ e Bourlard, John Dines, Mathew Magimai-Doss, Phil ip N
Garner, David Imseng, Petr Motlicek, Hui Liang, Lakshmi Sa-
heer, and Fabio Valente, “Current trends in multilingual sp eech
processing,” Sadhana , vol. 36, no. 5, pp. 885–915, 2011.
[3] Mark JF Gales, Kate M Knill, and Anton Ragni, “Unicode-
based graphemic systems for limited resource languages,” i n
ICASSP , 2015.
[4] Stephan Kanthak and Hermann Ney, “Context-dependent
acoustic modeling using graphemes for large vocabulary
speech recognition,” in ICASSP , 2002.
[5] Mirjam Killer, Sebastian Stuker, and Tanja Schultz,
“Grapheme based speech recognition,” in Eighth Euro-
pean Conference on Speech Communication and Technology ,
2003.
[6] Sebastian St¨ uker and Tanja Schultz, “A grapheme based s peech
recognition system for russian,” in 9th Conference Speech and
Computer , 2004.
[7] Willem D Basson and Marelie H Davel, “Comparing
grapheme-based and phoneme-based speech recognition for
afrikaans,” in PRASA , 2012.
[8] Alex Graves and Navdeep Jaitly, “Towards end-to-end spe ech
recognition with recurrent neural networks,” in ICML , 2014.
[9] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals ,
“Listen, Attend and Spell: A Neural Network for Large V ocab-
ulary Conversational Speech Recognition ,” in ICASSP , 2016.
[10] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Phi le-
mon Brakel, and Yoshua Bengio, “End-to-end attention-base d
large vocabulary speech recognition,” in ICASSP , 2016.
[11] William Chan and Ian Lane, “On Online Attention-based
Speech Recognition and Joint Mandarin Character-Pinyin
Training,” in INTERSPEECH , 2016.
[12] William Chan, Yu Zhang, Quoc Le, and Navdeep Jaitly, “La -
tent Sequence Decompositions,” in ICLR , 2017.
[13] Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit
Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kan-
nan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep
Jaitly, Bo Li, Jan Chorowsk, and Michiel Bacchiani, “State- of-
the-art Speech Recognition With Sequence-to-Sequence Mod -
els,” in ICASSP , 2018.
[14] Albert Zeyer, Kazuki Irie, Ralf Schluter, and Hermann N ey,
“Improved training of end-to-end attention models for spee ch
recognition,” in INTERSPEECH , 2018.
[15] K. Rao, R. Prabhavalkar, and H. Sak, “Exploring Archite c-
tures, Data and Units for Streaming End-to-End Speech Recog -
nition with RNN-Transducer,” in ASRU , 2017.
[16] Hagen Soltau, Hank Liao, and Hasim Sak, “Neural Speech
Recognizer: Acoustic-to-Word LSTM Model for Large V ocab-
ulary Speech Recognition,” in arXiv:1610.09975 , 2016.
[17] Jinyu Li, Guoli Ye, Amit Das, Rui Zhao, and Yifan Gong, “A d-
vancing Acoustic-to-Word CTC Model,” in ICASSP , 2018.
[18] Jose Sotelo, Soroush Mehri, Kundan Kumar, Joao Felipe
Santos, Kyle Kastner, Aaron Courville, and Yoshua Bengio,
“Char2wav: End-to-end speech synthesis,” in ICLR: Work-
shop, 2017.[19] Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu ,
Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao,
Zhifeng Chen, Samy Bengio, et al., “Tacotron: A fully end-
to-end text-to-speech synthesis model,” arXiv preprint , 2017.
[20] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster ,
Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,
Yuxuan Wang, RJ Skerry-Ryan, et al., “Natural TTS synthesis
by conditioning wavenet on mel spectrogram predictions,” i n
ICASSP , 2018.
[21] Rohit Prabhavalkar, Kanishka Rao, Tara N Sainath, Bo Li , Leif
Johnson, and Navdeep Jaitly, “A comparison of sequence-to-
sequence models for speech recognition,” in INTERSPEECH ,
2017, pp. 939–943.
[22] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Su b-
ramanya, “Multilingual language processing from bytes,” i n
NAACL , 2016.
[23] Bo Li, Tara N Sainath, Khe Chai Sim, Michiel Bacchiani, E u-
gene Weinstein, Patrick Nguyen, Zhifeng Chen, Yonghui Wu,
and Kanishka Rao, “Multi-dialect speech recognition with a
single sequence-to-sequence model,” in ICASSP , 2018.
[24] Shubham Toshniwal, Tara N Sainath, Ron J Weiss, Bo Li, Pe -
dro Moreno, Eugene Weinstein, and Kanishka Rao, “Multilin-
gual speech recognition with a single end-to-end model,” in
ICASSP , 2018.
[25] Sepp Hochreiter and J¨ urgen Schmidhuber, “Long short- term
memory,” Neural computation , vol. 9, no. 8, pp. 1735–1780,
1997.
[26] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,
“Neural Machine Translation by Jointly Learning to Align an d
Translate,” 2015.
[27] Golan Pundak and Tara N Sainath, “Lower Frame Rate Neura l
Network Acoustic Models,” in INTERSPEECH , 2016.
[28] Has ¸im Sak, Andrew Senior, Kanishka Rao, and Franc ¸ois e
Beaufays, “Fast and accurate recurrent neural network
acoustic models for speech recognition,” arXiv preprint
arXiv:1507.06947 , 2015.
[29] Peter Auer, Code-switching in conversation: Language, inter-
action and identity , Routledge, 2013.
[30] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb
Noury, Norman Casagrande, Edward Lockhart, Florian Stim-
berg, A¨ aron van den Oord, Sander Dieleman, and Koray
Kavukcuoglu, “Efﬁcient neural audio synthesis,” in ICML ,
2018.
[31] C. Kim, A. Misra, K. Chin, T. Hughes, A. Narayanan, T. N.
Sainath, and M. Bacchiani, “Generated of Large-scale Simu-
lated Utterances in Virtual Rooms to Train Deep-Neural Net-
works for Far-ﬁeld Speech Recognition in Google Home,” in
INTERSPEECH , 2017.
[32] ITUT Rec, “P. 800: Methods for subjective determinatio n
of transmission quality,” International Telecommunication
Union, Geneva , 1996.
