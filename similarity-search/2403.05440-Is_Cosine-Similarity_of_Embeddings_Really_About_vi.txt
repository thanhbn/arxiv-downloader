# 2403.05440.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/similarity-search/2403.05440.pdf
# Kích thước tệp: 1676072 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Độ Tương Đồng Cosine của Embeddings Có Thật Sự Về
Sự Tương Đồng Không?
Harald Steck
hsteck@netflix.com
Netflix Inc.
Los Gatos, CA, USAChaitanya Ekanadham
cekanadham@netflix.com
Netflix Inc.
Los Angeles, CA, USA
Nathan Kallus
nkallus@netflix.com
Netflix Inc. & Cornell University
New York, NY, USA
Ngày 11 tháng 3, 2024
Tóm tắt
Độ tương đồng cosine là cosine của góc giữa hai vectơ, hoặc tương đương
là tích vô hướng giữa các chuẩn hóa của chúng. Một ứng dụng phổ biến
là định lượng độ tương đồng ngữ nghĩa giữa các đối tượng chiều cao bằng
cách áp dụng độ tương đồng cosine cho một embedding đặc trưng chiều
thấp đã học được. Điều này có thể hoạt động tốt hơn nhưng đôi khi cũng
tệ hơn so với tích vô hướng không chuẩn hóa giữa các vectơ nhúng trong
thực tế. Để có cái nhìn sâu sắc về quan sát thực nghiệm này, chúng tôi
nghiên cứu các embedding được rút ra từ các mô hình tuyến tính được điều
chuẩn, nơi các nghiệm dạng đóng tạo điều kiện cho những hiểu biết phân
tích. Chúng tôi rút ra một cách phân tích cách độ tương đồng cosine có thể
cho ra những 'độ tương đồng' tùy ý và do đó vô nghĩa. Đối với một số mô
hình tuyến tính, các độ tương đồng thậm chí không duy nhất, trong khi đối
với những mô hình khác chúng được kiểm soát ngầm định bởi điều chuẩn.
Chúng tôi thảo luận về các tác động vượt ra ngoài các mô hình tuyến tính:
một sự kết hợp của các điều chuẩn khác nhau được sử dụng khi học các mô
hình sâu; những điều này có tác động ngầm định và không có chủ ý khi lấy
độ tương đồng cosine của các embedding kết quả, làm cho kết quả trở nên
mờ đục và có thể tùy ý. Dựa trên những hiểu biết này, chúng tôi cảnh báo
không nên sử dụng mù quáng độ tương đồng cosine và nêu ra các lựa chọn
thay thế.
1 Giới thiệu
Các thực thể rời rạc thường được nhúng thông qua một ánh xạ học được tới các
vectơ giá trị thực dày đặc trong nhiều lĩnh vực khác nhau. Ví dụ, các từ được
nhúng dựa trên ngữ cảnh xung quanh chúng trong một mô hình ngôn ngữ lớn
(LLM), trong khi các hệ thống gợi ý thường học một embedding của các mục
(và người dùng) dựa trên cách chúng được tiêu thụ bởi người dùng. Lợi ích của
các embedding như vậy là đa dạng. Đặc biệt, chúng có thể được sử dụng trực
tiếp như các đầu vào (đông lạnh hoặc tinh chỉnh) cho các mô hình khác, và/hoặc
chúng có thể cung cấp một khái niệm dựa trên dữ liệu về độ tương đồng (ngữ
nghĩa) giữa các thực thể trước đây là nguyên tử và rời rạc.
1arXiv:2403.05440v1  [cs.IR]  8 Mar 2024

--- TRANG 2 ---
Trong khi độ tương đồng trong 'độ tương đồng cosine' đề cập đến thực tế rằng
các giá trị lớn hơn (trái ngược với các giá trị nhỏ hơn trong các chỉ số khoảng
cách) chỉ ra sự gần gũi gần hơn, tuy nhiên, nó cũng đã trở thành một thước đo
rất phổ biến về độ tương đồng ngữ nghĩa giữa các thực thể quan tâm, với động
lực là chuẩn của các vectơ embedding đã học không quan trọng bằng sự sắp
xếp định hướng giữa các vectơ embedding. Mặc dù có vô số bài báo báo cáo về
việc sử dụng thành công độ tương đồng cosine trong các ứng dụng thực tế, tuy
nhiên, nó cũng được phát hiện là không hoạt động tốt bằng các phương pháp
khác, như tích vô hướng (không chuẩn hóa) giữa các embedding đã học, ví dụ,
xem [3, 4, 8].

Trong bài báo này, chúng tôi cố gắng làm sáng tỏ những quan sát thực nghiệm
không nhất quán này. Chúng tôi chỉ ra rằng độ tương đồng cosine của các
embedding đã học thực tế có thể cho ra kết quả tùy ý. Chúng tôi phát hiện rằng
lý do cơ bản không phải là bản thân độ tương đồng cosine, mà là thực tế rằng
các embedding đã học có một bậc tự do có thể tạo ra các độ tương đồng cosine
tùy ý mặc dù các tích vô hướng (không chuẩn hóa) của chúng được định nghĩa
rõ ràng và duy nhất. Để có được những hiểu biết có tính tổng quát hơn, chúng
tôi rút ra các nghiệm phân tích, điều này có thể thực hiện được đối với các mô
hình Phân tích Ma trận (MF) tuyến tính–điều này được nêu chi tiết trong Phần
tiếp theo. Trong Phần 3, chúng tôi đề xuất các biện pháp khắc phục có thể.
Các thí nghiệm trong Phần 4 minh họa những phát hiện của chúng tôi được rút
ra trong bài báo này.

2 Các Mô hình Phân tích Ma trận

Trong bài báo này, chúng tôi tập trung vào các mô hình tuyến tính vì chúng cho
phép có các nghiệm dạng đóng, và do đó có sự hiểu biết lý thuyết về các giới
hạn của chỉ số độ tương đồng cosine áp dụng cho các embedding đã học. Chúng
ta được cho một ma trận X∈Rn×p chứa n điểm dữ liệu và p đặc trưng (ví dụ,
người dùng và mục tương ứng, trong trường hợp hệ thống gợi ý). Mục tiêu trong
các mô hình phân tích ma trận (MF), hoặc tương đương trong các bộ tự mã hóa
tuyến tính, là ước lượng một ma trận thứ hạng thấp AB⊤∈Rp×p, trong đó
A, B∈Rp×k với k≤p, sao cho tích XAB^T là một xấp xỉ tốt của X:¹X≈XAB⊤.
Khi X đã cho là một ma trận người dùng-mục, các hàng ⃗bi của B thường được
gọi là các embedding mục (k-chiều), trong khi các hàng của XA, ký hiệu là
⃗xu·A, có thể được hiểu là các embedding người dùng, trong đó embedding của
người dùng u là tổng của các embedding mục ⃗aj mà người dùng đã tiêu thụ.

Lưu ý rằng mô hình này được định nghĩa theo tích vô hướng (không chuẩn hóa)
¹Lưu ý rằng chúng tôi đã bỏ qua các số hạng bias (độ lệch hằng số) ở đây để rõ ràng ký
hiệu–chúng có thể được đưa vào đơn giản trong bước tiền xử lý bằng cách trừ chúng khỏi mỗi
cột hoặc hàng của X. Cho rằng các số hạng bias như vậy có thể giảm bias phổ biến của các
embedding đã học ở một mức độ nào đó, chúng có thể có một số tác động liên quan đến các
độ tương đồng đã học, nhưng cuối cùng nó bị hạn chế.
2

--- TRANG 3 ---
giữa các embedding người dùng và mục (XAB⊤)u,i=⟨⃗xu·A,⃗bi⟩. Tuy nhiên,
một khi các embedding đã được học, thông thường người ta cũng xem xét độ
tương đồng cosine của chúng, giữa hai mục cosSim(⃗bi,⃗bi′), hai người dùng
cosSim(⃗xu·A, ⃗xu′·A), hoặc một người dùng và một mục cosSim(⃗xu·A,⃗bi).
Trong phần sau, chúng tôi chỉ ra rằng điều này có thể dẫn đến kết quả tùy ý, và
chúng thậm chí có thể không duy nhất.

2.1 Huấn luyện

Một yếu tố chính ảnh hưởng đến tính hữu ích của chỉ số độ tương đồng cosine
là điều chuẩn được sử dụng khi học các embedding trong A, B, như được nêu
trong phần sau. Xem xét hai lược đồ điều chuẩn sau đây, thường được sử dụng
(cả hai đều có nghiệm dạng đóng, xem Phần 2.2 và 2.3:

min
A,B||X−XAB⊤||²F+λ||AB⊤||²F (1)

min
A,B||X−XAB⊤||²F+λ(||XA||²F+||B||²F) (2)

Hai mục tiêu huấn luyện rõ ràng khác nhau trong điều chuẩn chuẩn L2 của chúng:

•Trong mục tiêu đầu tiên, ||AB⊤||²F áp dụng cho tích của chúng. Trong các
mô hình tuyến tính, loại điều chuẩn chuẩn L2 này có thể được chỉ ra là
tương đương với việc học với khử nhiễu, tức là drop-out trong lớp đầu vào,
ví dụ, xem [6]. Hơn nữa, độ chính xác dự đoán kết quả trên dữ liệu thử
nghiệm giữ lại được thấy bằng thực nghiệm là vượt trội hơn so với mục tiêu
thứ hai [2]. Không chỉ trong các mô hình MF, mà còn trong học sâu, người
ta thường quan sát thấy rằng khử nhiễu hoặc drop-out (mục tiêu này) dẫn
đến kết quả tốt hơn trên dữ liệu thử nghiệm giữ lại so với suy giảm trọng
số (mục tiêu thứ hai).

•Mục tiêu thứ hai tương đương với mục tiêu phân tích ma trận thông thường
min W||X−PQ⊤||²F+λ(||P||²F+||Q||²F), trong đó X được phân tích như
PQ⊤, và P=XA và Q=B. Sự tương đương này được nêu, ví dụ, trong
[2]. Ở đây, điều quan trọng là mỗi ma trận P và Q được điều chuẩn riêng
biệt, tương tự như suy giảm trọng số trong học sâu.

Nếu ÂA và ÂB là nghiệm của một trong hai mục tiêu, thì cũng được biết rõ rằng
ÂAR và ÂBR với một ma trận xoay tùy ý R∈Rk×k, cũng là nghiệm. Trong khi
độ tương đồng cosine bất biến dưới các phép xoay R như vậy, một trong những
hiểu biết chính trong bài báo này là mục tiêu đầu tiên (nhưng không phải mục
tiêu thứ hai) cũng bất biến với việc tái tỷ lệ các cột của A và B (tức là, các
chiều tiềm ẩn khác nhau của các embedding): nếu ÂAÂ B⊤ là một nghiệm của
mục tiêu đầu tiên, thì ÂAD D⁻¹ÂB⊤ cũng vậy, trong đó D∈Rk×k là một ma
trận đường chéo tùy ý. Do đó chúng ta có thể định nghĩa một nghiệm mới (như
một hàm của D) như sau:

ÂA(D):= ÂAD và
ÂB(D):= ÂBD⁻¹. (3)

3

--- TRANG 4 ---
Đổi lại, ma trận đường chéo D này ảnh hưởng đến sự chuẩn hóa của các
embedding người dùng và mục đã học (tức là, các hàng):

(XÂA(D))(chuẩn hóa) = ΩAXÂA(D)= ΩAXÂAD và
ÂB(D)(chuẩn hóa)= ΩBÂB(D)= ΩBÂBD⁻¹, (4)

trong đó ΩA và ΩB là các ma trận đường chéo thích hợp để chuẩn hóa mỗi
embedding đã học (hàng) thành chuẩn Euclidean đơn vị. Lưu ý rằng nói chung
các ma trận không giao hoán, và do đó một lựa chọn khác cho D không thể
(chính xác) được bù trừ bởi các ma trận chuẩn hóa ΩA và ΩB. Vì chúng phụ
thuộc vào D, chúng tôi làm cho điều này rõ ràng bằng ΩA(D) và ΩB(D). Do
đó, các độ tương đồng cosine của các embedding cũng phụ thuộc vào ma trận
tùy ý D này.

Vì có thể xem xét độ tương đồng cosine giữa hai mục, hai người dùng, hoặc một
người dùng và một mục, ba sự kết hợp đọc

•mục – mục:
cosSim(ÂB(D),ÂB(D)) = ΩB(D)·ÂB·D⁻²·ÂB⊤·ΩB(D)

•người dùng – người dùng:
cosSim(XÂAA(D), XÂAA(D)) = ΩA(D)·XÂA·D²·(XÂA)⊤·ΩA(D)

•người dùng – mục:
cosSim(XÂAA(D),ÂB(D)) = ΩA(D)·XÂA·ÂB⊤·ΩB(D)

Rõ ràng rằng độ tương đồng cosine trong cả ba sự kết hợp đều phụ thuộc vào
ma trận đường chéo tùy ý D: trong khi tất cả chúng gián tiếp phụ thuộc vào D
do tác động của nó lên các ma trận chuẩn hóa ΩA(D) và ΩB(D), lưu ý rằng độ
tương đồng cosine mục-mục (đặc biệt phổ biến) (dòng đầu tiên) ngoài ra còn
phụ thuộc trực tiếp vào D (và độ tương đồng cosine người dùng-người dùng
cũng vậy, xem mục thứ hai).

2.2 Chi tiết về Mục tiêu Đầu tiên (Phương trình 1)

Nghiệm dạng đóng của mục tiêu huấn luyện trong Phương trình 1 được rút ra
trong [2] và đọc ÂA(1)ÂB⊤(1)=Vk·dMat(...,1/(1+λ/σ²i),...)k·V⊤k, trong đó
X=:UΣV⊤ là phân rã giá trị kỳ dị (SVD) của ma trận dữ liệu đã cho X, trong
đó Σ = dMat(..., σi, ...) ký hiệu ma trận đường chéo của các giá trị kỳ dị, trong
khi U, V chứa các vectơ kỳ dị trái và phải, tương ứng. Liên quan đến k giá trị
riêng lớn nhất σi, chúng ta ký hiệu các ma trận cắt ngắn thứ hạng k là Uk, Vk
và (...)k. Chúng ta có thể định nghĩa²

ÂA(1)=ÂB(1):=Vk·dMat(...,1/(1+λ/σ²i),...)^(1/2)k. (5)

²Vì D là tùy ý, chúng tôi chọn gán dMat(...,1/(1+λ/σ²i),...)^(1/2)k cho mỗi ÂA,ÂB mà
không mất tính tổng quát.
4

--- TRANG 5 ---
Tính tùy ý của độ tương đồng cosine trở nên đặc biệt nổi bật ở đây khi chúng ta
xem xét trường hợp đặc biệt của một mô hình MF thứ hạng đầy đủ, tức là khi
k=p. Điều này được minh họa bởi hai trường hợp sau:

•nếu chúng ta chọn D= dMat(...,1/(1+λ/σ²i),...)^(1/2), thì chúng ta có
ÂA(D)(1)=ÂA(1)·D= V·dMat(...,1/(1+λ/σ²i),...) và ÂB(D)(1)=ÂB(1)·D⁻¹=V.
Cho rằng ma trận thứ hạng đầy đủ của các vectơ kỳ dị V đã được chuẩn
hóa (liên quan đến cả cột và hàng), sự chuẩn hóa ΩB=I do đó bằng ma
trận đơn vị I. Do đó chúng ta có được liên quan đến các độ tương đồng
cosine mục-mục:

cosSim(ÂB(D)(1),ÂB(D)(1)) =VV⊤=I,

đây là một kết quả khá kỳ dị, vì nó nói rằng độ tương đồng cosine giữa
bất kỳ cặp embedding mục (khác nhau) nào đều bằng không, tức là một
mục chỉ tương đồng với chính nó, nhưng không với bất kỳ mục nào khác!

Một kết quả đáng chú ý khác được có được cho độ tương đồng cosine người
dùng-mục:

cosSim(XÂA(D)(1),ÂB(D)(1)) = ΩA·X·V·dMat(...,1/(1+λ/σ²i),...)·V⊤
= ΩA·X·ÂA(1)ÂB⊤(1),

vì sự khác biệt duy nhất so với tích vô hướng (không chuẩn hóa) là do ma
trận ΩA, chuẩn hóa các hàng—do đó, khi chúng ta xem xét thứ hạng của
các mục cho một người dùng cho trước dựa trên các điểm dự đoán, độ
tương đồng cosine và tích vô hướng (không chuẩn hóa) dẫn đến chính xác
cùng một thứ hạng của các mục vì chuẩn hóa hàng chỉ là một hằng số
không liên quan trong trường hợp này.

•nếu chúng ta chọn D= dMat(...,1/(1+λ/σ²i),...)^(-1/2), thì chúng ta có
tương tự như trường hợp trước: ÂB(D)(1)=V·dMat(...,1/(1+λ/σ²i),...),
và ÂA(D)(1)=V là trực chuẩn. Bây giờ chúng ta có được liên quan đến các
độ tương đồng cosine người dùng-người dùng:

cosSim(XÂA(D)(1), XÂA(D)(1)) = ΩA·X·X⊤·ΩA,

tức là bây giờ các độ tương đồng người dùng đơn giản dựa trên ma trận
dữ liệu thô X, tức là không có bất kỳ sự làm mượt nào do các embedding
đã học. Liên quan đến các độ tương đồng cosine người dùng-mục, bây giờ
chúng ta có được

cosSim(XÂA(D)(1),ÂB(D)(1)) = ΩA·X·ÂA(1)·ÂB⊤(1)·ΩB,

tức là bây giờ ΩB chuẩn hóa các hàng của B, điều mà chúng ta không có
trong lựa chọn D trước đó.

Tương tự, các độ tương đồng cosine mục-mục

cosSim(ÂB(D)(1),ÂB(D)(1)) = ΩB·V·dMat(...,1/(1+λ/σ²i),...)²·V⊤·ΩB

rất khác với kết quả kỳ dị mà chúng ta có được trong lựa chọn D trước
đó.

5

--- TRANG 6 ---
Nhìn chung, hai trường hợp này cho thấy rằng các lựa chọn khác nhau cho D
dẫn đến các độ tương đồng cosine khác nhau, mặc dù mô hình đã học
ÂA(D)(1)ÂB(D)⊤(1)=ÂA(1)ÂB⊤(1) bất biến với D. Nói cách khác, kết quả của độ
tương đồng cosine là tùy ý và không duy nhất cho mô hình này.

2.3 Chi tiết về Mục tiêu Thứ hai (Phương trình 2)

Nghiệm của mục tiêu huấn luyện trong Phương trình 2 được rút ra trong [7] và
đọc

ÂA(2)=Vk·dMat(...,√(1/σi·(1-λ/σi)+),...)k và
ÂB(2)=Vk·dMat(...,√(σi·(1-λ/σi)+),...)k (6)

trong đó (y)+= max(0, y), và một lần nữa X=:UΣV⊤ là SVD của dữ liệu huấn
luyện X, và Σ = dMat(..., σi, ...). Lưu ý rằng, nếu chúng ta sử dụng ký hiệu
thông thường của MF trong đó P=XA và Q=B, chúng ta có được
ÂP=XÂA(2)=Uk·dMat(...,√(σi·(1-λ/σi)+),...)k, trong đó chúng ta có thể thấy
rằng ở đây ma trận đường chéo dMat(...,√(σi·(1-λ/σi)+),...)k là giống nhau
cho các embedding người dùng và embedding mục trong Phương trình 6, như
mong đợi do tính đối xứng trong điều chuẩn chuẩn L2 ||P||²F+||Q||²F trong mục
tiêu huấn luyện trong Phương trình 2.

Sự khác biệt chính so với mục tiêu huấn luyện đầu tiên (xem Phương trình 1)
là ở đây điều chuẩn chuẩn L2 ||P||²F+||Q||²F được áp dụng cho mỗi ma trận
riêng lẻ, do đó nghiệm này là duy nhất (cho đến các phép xoay không liên quan,
như đã đề cập ở trên), tức là trong trường hợp này không có cách nào để đưa
một ma trận đường chéo tùy ý D vào nghiệm của mục tiêu thứ hai. Do đó, độ
tương đồng cosine áp dụng cho các embedding đã học của biến thể MF này cho
ra kết quả duy nhất.

Mặc dù nghiệm này là duy nhất, vẫn còn là một câu hỏi mở liệu ma trận đường
chéo duy nhất này dMat(...,√(σi·(1-λ/σi)+),...)k liên quan đến các embedding
người dùng và mục có cho ra các độ tương đồng ngữ nghĩa tốt nhất có thể trong
thực tế hay không. Tuy nhiên, nếu chúng ta tin rằng điều chuẩn này làm cho độ
tương đồng cosine hữu ích liên quan đến độ tương đồng ngữ nghĩa, chúng ta có
thể so sánh các dạng của các ma trận đường chéo trong cả hai biến thể, tức là
so sánh Phương trình 6 với Phương trình 5 gợi ý rằng ma trận đường chéo tùy
ý D trong biến thể đầu tiên (xem phần trên) tương tự có thể được chọn là
D= dMat(...,√(1/σi),...)k.

3 Biện pháp Khắc phục và Lựa chọn Thay thế cho Độ Tương đồng Cosine

Như chúng ta đã chỉ ra một cách phân tích ở trên, khi một mô hình được huấn
luyện đối với tích vô hướng, tác động của nó lên độ tương đồng cosine có thể
mờ đục và đôi khi thậm chí không duy nhất. Một giải pháp rõ ràng là huấn
luyện mô hình đối với độ tương đồng cosine, mà chuẩn hóa lớp [1] có thể tạo
điều kiện thuận lợi. Một phương pháp khác là tránh không gian embedding, điều
đã gây ra các vấn đề được nêu ở trên ngay từ đầu,

6

--- TRANG 7 ---
Hình 1: Minh họa sự biến thiên lớn của các độ tương đồng cosine mục-mục
cosSim(B, B) trên cùng dữ liệu do các lựa chọn mô hình hóa khác nhau. Trái:
các cụm thực tế (các mục được sắp xếp theo gán cụm, và trong mỗi cụm theo
độ phổ biến cơ sở giảm dần). Sau khi huấn luyện đối với Phương trình 1, cho
phép tái tỷ lệ tùy ý của các vectơ kỳ dị trong Vk, ba biểu đồ ở giữa cho thấy ba
lựa chọn cụ thể của tái tỷ lệ, như được chỉ ra phía trên mỗi biểu đồ. Phải: dựa
trên B (duy nhất) thu được khi huấn luyện đối với Phương trình 2.

và chiếu nó trở lại vào không gian gốc, nơi độ tương đồng cosine sau đó có thể
được áp dụng. Ví dụ, sử dụng các mô hình ở trên, và cho dữ liệu thô X, người
ta có thể xem XÂAÂB⊤ như phiên bản làm mượt của nó, và các hàng của
XÂAÂB⊤ như các embedding của người dùng trong không gian gốc, nơi độ
tương đồng cosine sau đó có thể được áp dụng.

Ngoài ra, cũng quan trọng để lưu ý rằng, trong độ tương đồng cosine, chuẩn
hóa chỉ được áp dụng sau khi các embedding đã được học. Điều này có thể giảm
đáng kể các độ tương đồng (ngữ nghĩa) kết quả so với việc áp dụng một số
chuẩn hóa, hoặc giảm bias phổ biến, trước hoặc trong quá trình học. Điều này
có thể được thực hiện bằng nhiều cách. Ví dụ, một phương pháp mặc định trong
thống kê là chuẩn hóa dữ liệu X (để mỗi cột có trung bình bằng không và
phương sai đơn vị). Các phương pháp thông thường trong học sâu bao gồm việc
sử dụng lấy mẫu âm hoặc tỷ lệ khuynh hướng nghịch đảo (IPS) để tính đến các
độ phổ biến mục khác nhau (và mức độ hoạt động của người dùng). Ví dụ,
trong word2vec [5], một mô hình phân tích ma trận được huấn luyện bằng cách
lấy mẫu âm với xác suất tỷ lệ thuận với tần suất (độ phổ biến) của chúng trong
dữ liệu huấn luyện được nâng lên lũy thừa β= 3/4, điều này dẫn đến các độ
tương đồng từ ấn tượng vào thời điểm đó.

4 Thí nghiệm

Mặc dù chúng tôi đã thảo luận về mô hình thứ hạng đầy đủ ở trên, vì nó có thể
tiếp cận được với những hiểu biết phân tích, bây giờ chúng tôi minh họa những
phát hiện này bằng thực nghiệm cho các embedding thứ hạng thấp. Chúng tôi
không biết về một chỉ số tốt cho độ tương đồng ngữ nghĩa, điều này thúc đẩy
chúng tôi tiến hành thí nghiệm trên dữ liệu mô phỏng, để các độ tương đồng
ngữ nghĩa thực tế được biết. Để làm điều này, chúng tôi mô phỏng dữ liệu nơi
các mục được nhóm thành các cụm, và người dùng tương tác với các mục dựa
trên sở thích cụm của họ. Sau đó chúng tôi kiểm tra mức độ mà các độ tương
đồng cosine áp dụng cho các embedding đã học có thể khôi phục cấu trúc cụm
mục.

Chi tiết, chúng tôi tạo ra các tương tác giữa n= 20.000 người dùng và p=

7

--- TRANG 8 ---
1.000 mục được gán ngẫu nhiên vào C= 5 cụm với xác suất pc cho c= 1, ..., C.
Sau đó chúng tôi lấy mẫu số mũ luật lũy thừa cho mỗi cụm c, βc∼Unif(β(item)min,
β(item)max) trong đó chúng tôi chọn β(item)min = 0.25 và β(item)max = 1.5, và sau đó
gán một độ phổ biến cơ sở cho mỗi mục i theo luật lũy thừa pi= PowerLaw(βc).
Sau đó chúng tôi tạo ra các mục mà mỗi người dùng u đã tương tác: đầu tiên,
chúng tôi lấy mẫu ngẫu nhiên sở thích cụm người dùng puc, và sau đó tính toán
xác suất người dùng-mục: pui=pucipi/∑ipucipi. Chúng tôi lấy mẫu số lượng
mục cho người dùng này, ku∼PowerLaw(β(user)), trong đó chúng tôi sử dụng
β(user)= 0.5, và sau đó lấy mẫu ku mục (không thay thế) sử dụng xác suất pui.

Sau đó chúng tôi học các ma trận A, B theo Phương trình 1 và cả Phương trình
2 (với λ= 10.000 và λ= 100, tương ứng) từ dữ liệu mô phỏng. Chúng tôi sử
dụng một ràng buộc thứ hạng thấp k= 50≪p= 1.000 để bổ sung cho các kết
quả phân tích cho trường hợp thứ hạng đầy đủ ở trên.

Hình 1 cho thấy các độ tương đồng mục-mục "thực" như được định nghĩa bởi
các cụm mục ở phía bên trái, trong khi bốn biểu đồ còn lại cho thấy các độ
tương đồng cosine mục-mục thu được cho bốn kịch bản sau: sau khi huấn luyện
đối với Phương trình 1, cho phép tái tỷ lệ tùy ý của các vectơ kỳ dị trong Vk
(như được nêu trong Phần 2.2), ba độ tương đồng cosine ở giữa được thu được
cho ba lựa chọn tái tỷ lệ. Biểu đồ cuối cùng trong hàng này được thu được từ
huấn luyện đối với Phương trình 2, dẫn đến một nghiệm duy nhất cho các độ
tương đồng cosine. Một lần nữa, mục đích chính ở đây là minh họa các độ
tương đồng cosine kết quả có thể khác nhau đến mức nào ngay cả đối với các
lựa chọn tái tỷ lệ hợp lý khi huấn luyện đối với Phương trình 1 (lưu ý rằng
chúng tôi không sử dụng bất kỳ lựa chọn cực đoan nào cho tái tỷ lệ ở đây, như
tương quan âm với các giá trị kỳ dị, mặc dù điều này cũng được cho phép), và
cũng cho nghiệm duy nhất khi huấn luyện đối với Phương trình 2.

Kết luận

Thông thường người ta sử dụng độ tương đồng cosine giữa các embedding
người dùng và/hoặc mục đã học như một thước đo độ tương đồng ngữ nghĩa
giữa các thực thể này. Chúng tôi nghiên cứu các độ tương đồng cosine trong
bối cảnh của các mô hình phân tích ma trận tuyến tính, cho phép các rút ra phân
tích, và chỉ ra rằng các độ tương đồng cosine phụ thuộc rất nhiều vào phương
pháp và kỹ thuật điều chuẩn, và trong một số trường hợp có thể được tạo ra
thậm chí vô nghĩa. Các rút ra phân tích của chúng tôi được bổ sung bằng thực
nghiệm bằng cách kiểm tra định tính đầu ra của các mô hình này áp dụng cho
dữ liệu mô phỏng nơi chúng ta có độ tương đồng mục-mục thực tế. Dựa trên
những hiểu biết này, chúng tôi cảnh báo không nên sử dụng mù quáng độ tương
đồng cosine, và đề xuất một vài phương pháp để giảm thiểu vấn đề này. Mặc
dù bài báo ngắn này giới hạn ở các mô hình tuyến tính cho phép hiểu biết dựa
trên các rút ra phân tích, chúng tôi hy vọng độ tương đồng cosine của các
embedding đã học trong các mô hình sâu sẽ bị ảnh hưởng bởi các vấn đề tương
tự, nếu không muốn nói là lớn hơn, vì một sự kết hợp của các phương pháp
điều chuẩn khác nhau thường được áp dụng ở đó, và các lớp khác nhau trong
mô hình có thể chịu điều chuẩn khác nhau—điều này ngầm định xác định một
tỷ lệ cụ thể (tương tự như ma trận D ở trên) của các chiều tiềm ẩn khác nhau
trong các embedding đã học trong mỗi lớp của mô hình sâu, và do đó tác động
của nó lên các độ tương đồng cosine kết quả có thể trở nên thậm chí mờ đục
hơn ở đó.

8

--- TRANG 9 ---
Tài liệu tham khảo

[1] J. L. Ba, J. R. Kiros, và G. E. Hinton. Layer normalization, 2016.
arXiv:1607.06450.

[2] R. Jin, D. Li, J. Gao, Z. Liu, L. Chen, và Y. Zhou. Towards a better
understanding of linear models for recommendation. Trong ACM Conference
on Knowledge Discovery and Data Mining (KDD), 2021.

[3] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, và
W. Yih. Dense passage retrieval for open-domain question answering, 2020.
arXiv:2004.04906v3.

[4] O. Khattab và M. Zaharia. ColBERT: Efficient and effective passage search
via contextualized late interaction over BERT, 2020. arXiv:2004.12832v2.

[5] T. Mikolov, K. Chen, G. Corrado, và J. Dean. Efficient estimation of word
representations in vector space, 2013.

[6] H. Steck. Autoencoders that don't overfit towards the identity. Trong Advances
in Neural Information Processing Systems (NeurIPS), 2020.

[7] S. Zheng, C. Ding, và F. Nie. Regularized singular value decomposition
and application to recommender system, 2018. arXiv:1804.05090.

[8] K. Zhou, K. Ethayarajh, D. Card, và D. Jurafsky. Problems with cosine as
a measure of embedding similarity for high frequency words. Trong 60th Annual
Meeting of the Association for Computational Linguistics, 2022.

9
