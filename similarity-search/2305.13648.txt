# 2305.13648.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/similarity-search/2305.13648.pdf
# File size: 515986 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Non-parametric, Nearest-neighbor-assisted Fine-tuning for
Neural Machine Translation
Jiayi Wang∗1, Ke Wang∗2, Yuqi Zhang2, Yu Zhao2, Pontus Stenetorp1
1University College London
2Alibaba DAMO Academy
ucabj45@ucl.ac.uk,{wk258730,chenwei.zyq}@alibaba-inc.com,
kongyu@taobao.com,p.stenetorp@cs.ucl.ac.uk
Abstract
Non-parametric, k-nearest-neighbor algo-
rithms have recently made inroads to assist
generative models such as language models
and machine translation decoders. We explore
whether such non-parametric models can
improve machine translation models at the
fine-tuning stage by incorporating statistics
from the kNN predictions to inform the
gradient updates for a baseline translation
model. There are multiple methods which
could be used to incorporate kNN statistics
and we investigate gradient scaling by a
gating mechanism, the kNN’s ground truth
probability, and reinforcement learning. For
four standard in-domain machine translation
datasets, compared with classic fine-tuning,
we report consistent improvements of all of
the three methods by as much as 1.45BLEU
and 1.28BLEU for German-English and
English-German translations respectively.
Through qualitative analysis, we found
particular improvements when it comes to
translating grammatical relations or function
words, which results in increased fluency of
our model.
1 Introduction
Non-parametric nearest neighbor models have been
seen recent success for generative natural language
processing tasks such as language modeling (Khan-
delwal et al., 2020) and machine translation (Khan-
delwal et al., 2021). Not only because explicitly
memorizing the training data helps generalization,
generative natural language models can scale to
larger text collections without the added cost of
training. Khandelwal et al. (2020) introduced k-
nearest-neighbor machine translation ( kNN-MT):
a simple non-parametric method for machine trans-
lation (MT) via nearest-neighbor retrievals was pro-
posed and has been verified its effectiveness – im-
∗Equal Contribution.proving BLEU scores by roughly 3 for translating
from English into German and Chinese.
To easily adapt to multi domains, during in-
ference, kNN-MT interpolates the softmax distri-
bution for the target token from the neural ma-
chine translation (NMT) model with the distribu-
tion of the retrieved set generated by the k-nearest-
neighbor ( kNN) search on a datastore of cached
examples. The datastore is constructed from key-
value pairs of parallel training data, where the key
is the latent contextual representation of the target
prefix tokens obtained via the NMT’s stochastic
forward-pass computing, and the value is the corre-
sponding ground-truth target token.
During preliminary investigations, we observed
that the kNN search is able to memorize content
words with lexical meanings of in-domain contexts.
However, when it comes to translate grammati-
cal relations, such as function word translations,
querying the datastore for knearest neighbors is
insufficient, which has a negative impact on the
fluency of the final translation result.
Moreover, although kNN-MT has the advan-
tage that it does not require additional fine-tuning,
our experiments show that kNN-MT cannot out-
perform or even achieve comparable performance
to classic fine-tuning (Mou et al., 2016) when in-
domain data is accessible. This is due to the ma-
chine translation model not having been optimized
on the in-domain data, and thus it limits the transla-
tion model’s capability to utilize the kNN search. A
simple way to observe this fact is to apply the kNN-
MT algorithm on a fine-tuned translation model,
but not a baseline translation model trained on out-
of-domain data. As shown in Table 1 and Table 8
in Appendix A, the performance of the algorithm
can be largely improved when the kNN datastore
is constructed with fine-tuned contextual represen-
tations and their corresponding keys. Therefore,
fine-tuning is still necessary and it benefits the non-
parametric kNN search algorithm.arXiv:2305.13648v1  [cs.CL]  23 May 2023

--- PAGE 2 ---
De-En En-De
Model IT Medical Law Koran Avg. IT Medical Law Koran Avg.
Base MT 38.35 40.14 45.63 16.29 35.10 29.74 35.56 40.85 13.97 30.03
kNN-MT 46.12 54.41 61.70 21.14 45.84 36.44 49.74 55.73 25.87 41.95
Fine-tuned MT 47.14 57.19 61.28 22.98 47.15 39.70 52.50 57.16 32.45 45.45
kNN-FT-MT 49.33 57.46 63.63 22.95 48.34 40.68 53.28 58.91 32.61 46.37
Table 1: Performances of Base NMT and the fine-tuned NMT with and without the integration with kNN search
during inference in German-English (De-En) and English-German (En-De) multi-domain translations respectively.
Results are reported with the metric SacreBLEU (Post, 2018). kNN-MT represents the Base NMT with integration
withkNN search during inference, while kNN-FT-MT represents the fine-tuned NMT with integration with kNN
search during inference.
Given that there are both advantages and disad-
vantages stemming from the kNN-MT algorithm,
we proceed to maximize the use of the results from
thekNN search to enhance the performance of
translation models. We hypothesize that the fine-
tuning procedure of a neural translation model can
be improved with the assistance of statistics from
thekNN predictions. Furthermore, we also explore
gradient scaling for the original neural translation
model with (1) a gate mechanism applied on the dis-
tribution of kNN predictions, (2) the kNN ground
truth probability and (3) reinforcement learning
based on the statistics of kNN predictions.
Based on these observations, we propose train-
able-kNN-MT to alleviate the problems of the the
original kNN-MT (Khandelwal et al., 2021). Our
trainable -kNN-MT is able to learn translations
conditioned on the retrieved k-nearest-neighbors.
The statistics of the retrieved set are incorporated
into model fine-tuning via three ways aforemen-
tioned to dynamically scale up the gradient for
back-propagation. In addition, the kNN datastore
for retrieving is jointly updated with model fine-
tuning so that the kNN search can secure more
accurate knearest neighbors.
There are two main contributions in this paper:
(1) The trainable -kNN-MT generates better objec-
tive contextual representations of relevant exam-
ples, which improves the retrieved sets of top-k
nearest neighbors. (2) The trainable -kNN-MT sig-
nificantly outperforms both of the original kNN-
MT algorithm and the classic fine-tuning, making
it a novel fine-tuning method for neural machine
translation. In addition, the fluency of the transla-
tion from trainable -kNN-MT is qualitatively im-
proved, while staying more faithful to the original
language.2 Methodology
In this section, we will introduce the trainable -
kNN-MT model, which is able to (1) generate
better objective contextual representations, (2) im-
prove the performances of both of the vanilla fine-
tuning and kNN-MT algorithm via gradient scaling
with the assistance of statistics from kNN predic-
tions.
2.1 Preliminary Method
Formerly, for the prediction of each target token,
given the source sentence xand the target prefix to-
kensy1:i−1, the NMT model predicts the next target
token yiwith the probability PNMT(yi|x, y 1:i−1)
from the softmax distribution over the vocabulary.
InkNN-MT (Khandelwal et al., 2021), with a
sequence of source tokens and a sequence of target
prefix tokens (s, t1:i−1)from the in-domain data D,
the pre-trained base NMT model outputs the hidden
representations fkNN(s, t1:i−1)of the i-th target to-
kentito construct a datastore. The definition of
the datastore is as follows:
(K,V) =
{(fkNN(s, t1:i−1), ti),∀ti∈t|(s, t)∈ D} ,
where Krepresents all of the keys, while Vrepre-
sents all of the corresponding values.
During inference, given a sequence of the source
text which needs to be translated and its gener-
ated target prefix tokens, the kNN-MT will first
retrieve top-k relevant neighbors from the above
datatore based on the Euclidean distances (Daniels-
son, 1980) between their contextual hidden rep-
resentation in the decoder and all the keys based
onfkNN(x, y 1:i−1). The retrieved set is then con-
verted into the distribution PkNN(yi|x, y 1,i−1)over
the vocabulary by,

--- PAGE 3 ---
kNearest Neighbors DistanceTarget10command30instruction……𝑷𝒌𝑵𝑵ProbabilityTarget0.5 (𝑀!"")command0.2instruction……𝑷𝑵𝑴𝑻ProbabilityTarget0.3command0.4instruction……Trainable-kNN-MTTraining ExamplesSourceTargetDelicious-Bibliotheksdaten...Import Delicious ...Um ein Feld aus der Liste …To remove a field ………Der watchgnupg…The …DatastoreRepresentationTargetTokenImportDelicious……informationMake a query
Translation ContextBefehl im eingebetteten Terminal-Emulator ausführen.Run theSource Text 𝒙Ground truth target 𝒚!𝒊𝒍𝒐𝒔𝒔𝒕𝒓𝒂𝒊𝒏𝒂𝒃𝒍𝒆*𝒌𝑵𝑵*𝑴𝑻
Datastore ConstructionModel Fine-tuning
Jointly UpdatedAssist to scale up gradientRetrieveFigure 1: The schematic representation of the trainable -kNN-MT. Arrows with broken lines illustrate the workflow
of learning translations with the assistance of statistics from kNN predictions.
PkNN(yi|x, y 1:i−1)∝ (1)
X
(ki,vi)1yj=vjexp(−d(kj, fkNN(x, y 1:i−1)
T)
where j∈[1, k], andkj, vjare the key and value of
the retrieved neighbors respectively. Trepresents
the temperature.
Finally, the prediction of the next token yirelies
on the interpolation of the predictions from the
NMT model and the kNN search as follows,
Pcomb(yi|x, y 1:i−1) = (2)
λPkNN(yi|x, y 1:i−1) + (1 −λ)PNMT(yi|x, y 1:i−1)
where λis a hyper-parameter for merging the two
different distributions.
2.2 The trainable -kNN-MT
As mentioned in the introduction, we found that
kNN-MT does not achieve comparable perfor-
mances with the classic fine-tuning as illustrated
in Table 1. We hypothesize that the base NMT
model trained on out-of-domain data might gener-
ate inappropriate contextual representations used
by in-domain datastore construction and the kNN
search algorithm. Conclusively, fine-tuning thebase NMT model with the in-domain data would
be still necessary when in-domain data is achiev-
able.
Inspired by both of the advantages and disadvan-
tages of the non-parametric kNN-MT, we propose
trainable -kNN-MT which involves the statistics
from the kNN search as an assistance to scale up
gradient into the NMT fine-tuning procedure. It can
not only bridge the gap between the NMT model
and the kNN search at a further step during infer-
ence, but enhance vanilla fine-tuning performance
as well. Specific details of the trainable -kNN-MT
are displayed in Figure 1. It basically contains two
parts: the datastore construction (left) and the NMT
model fine-tuning (right).
At the fine-tuning stage, the datastore is con-
structed on the in-domain training data with pa-
rameters of the NMT model, and it is jointly up-
dated with the NMT fine-tuning. After each cer-
tain number of fine-tuning steps, the datastore is
re-constructed with updated weights of the NMT
model. At each fine-tuning step, given a source
sentence x and the ground-truth target prefix to-
kensy1:i−1, the trainable -kNN-MT retrieves top- k
nearest neighbors just as what it does in the kNN-
MT, and the retrieved set of the kNN predictions
is converted to into a distribution by Equation 1,
and its statistics assists the NMT model how to do

--- PAGE 4 ---
back-propagation with gradient scaling.
Originally, training a NMT model optimizes the
parameters θvia minimizing the cross entropy loss
on the in-domain training dataset Das follows,
L=1
|D|X
(x,y)∈D−logPNMT(yi|x, y 1,i−1;θ).(3)
Instead, for the trainable -kNN-MT, we generally
define a function gkNN(·), which generates the gra-
dient scaling coefficient conditioned on the distri-
bution of the kNN predictions. Then, our proposed
loss for trainable -kNN-MT will be as follows,
L=1
|D|X
(x,y)∈D−loggkNNPNMT(yi|x, y 1,i−1;θ),
(4)
which is translated into loss trainable -kNN-MT in Fig-
ure 1.
In the next subsections, we will explicitly de-
scribe three ways to specify the function gkNN(·):
(1) a gate mechanism applied on the distribution of
kNN predictions, (2) the kNN ground truth proba-
bility and (3) reinforcement learning based on the
statistics of kNN predictions.
2.2.1 Gate Mechanism
In our preliminary observations, when the next tar-
get token is a content word with lexical meanings,
the distribution of the kNN predictions is usually
skewed with a remarkable highest probability mass.
However, such a phenomenon is not obvious in
the translations of grammatical relations, such as
function word translations, which results in a flat
distribution of the kNN outputs. An example is
shown in Table 8. The probability of the correct
next token "you" is 0.201 + 0 .028 = 0 .229, which
is not remarkable in the distribution of kNN out-
puts. Motivated by such cases, we hypothesize that
the NMT model should learn at a greater extent to
improve fluency or styling of the translation.
We utilize the maximum probability in the
kNN distribution, notated as MkNN, to design
gkNN(·). When the distribution of the kNN predic-
tions is flat, and MkNNis less than some thresh-
old,gkNN(·)can be specified to be a constant
in between 0 and 1. Since gkNNis in (0,1),
−loggkNNPNMT(yi|x, y 1,i−1;θ)in Equation 4 is
larger than the original one in vanilla NMT fine-
tuning, which leads to greater gradients for updat-
ing the weights of the NMT model. On the contrary,
we would keep the original gradient calculationsfrom the NMT fine-tuning. In details, gkNN(·)is
defined as follows,
gkNN=(
cMkNN< τ
1MkNN≥τ,(5)
where cis a constant in between 0 and 1, and
the hyper-parameter τrepresents the threshold for
MkNN, which plays the role of a gate in controlling
when to push the NMT model to learn translations
more greatly. An intuitive way to assign the value
ofccan be λfrom the original kNN-MT.
2.2.2 The kNN Ground Truth Probability
One disadvantage of the gate mechanism is that
it would be challenging for us to evaluate how
the setting of the threshold τwould affect the per-
formance of the trainable -kNN-MT, and investi-
gations via ablation studies are definitely needed
when it comes to new domains or new languages.
To overcome such a problem, we need to fig-
ure out solutions that does not contain any hyper-
parameters strongly bound to any specific statistics
ofkNN predictions.
Inspired by the original cross entropy loss
(Zhang and Sabuncu, 2018), one potential solution
can be utilizing the probability of the ground truth
target token from the distribution of kNN predic-
tions. If the probability of the ground truth target
word from the distribution of kNN predictions is
low, it is suitable to enhance the learning of the
NMT model, regardless of whether the next target
word is a content or function word.
Then, we dynamically set gkNNto be the proba-
bility of the ground truth as follows,
gkNN=PkNN(yi|x, y 1:i−1). (6)
However, this method does not work if
PkNN(yi|x, y 1:i−1)is zero, which means the ground
truth target word is not retrieved by the kNN search
for some reason. In such a scenario, we must set a
minimum of gkNNto avoid training crash. As we
know, the most extreme case for the distribution of
kNN predictions would be a uniform distribution
in which all predictions are equally likely with a
probability of 1/k. Therefore, it is reasonable to
setgkNNequal to 1/kwhen the PkNN(yi|x, y 1:i−1)
is zero.
2.2.3 Reinforcement Learning
The success of non-parametric kNN methods in
generative models relies on its explicit capability

--- PAGE 5 ---
SourceIst diese Einstellung aktiv, werden Benachrichtigungen wie zum Beispiel Sperren des
Bildschirms oder Änderungen des Profils durch ein passives Meldungsfenster angezeigt.
ReferenceIf checked, you will be notified through a passive popup whenever PowerDevil has to
notify something, such as screen locking or profile change.
Subword T@@ tab you noti@@ hin@@ you promp@@ de@@
Probability 0.344 0.236 0.201 0.096 0.054 0.028 0.022 0.019
Table 2: An example of the kNN predictions with k= 8 in the German-English IT validation set. The target prefix
tokens generated are "If checked,", and the correct next token should be "you". The sub-word candidates in Byte
Pair Encoding (Sennrich et al., 2016) are retrieved via kNN search with corresponding probabilities.
of memorizing the training data which enhances
generalizations for domain adaption (Khandelwal
et al., 2020, 2021) without extra training. It out-
performs the base NMT model in terms of quality
as well as effectiveness. In our explorations, it
may maintain the ability to equip the NMT model
to know when to learn more greatly for accurate
predictions about unseen contexts of different do-
mains.
In addition, considering that non-parametric
kNN search is based on the datastore constructed
on the golden labeled training data, it can be re-
garded as a supervised model of translation pre-
diction. It is essential to try to leverage the gap
between the kNN search and the predictions from
the NMT model by directly optimizing the eval-
uation measures based the kNN search, which is
very in line with the spirit of reinforcement learn-
ing for structured predictions in generative natural
language models (Paulus et al., 2017; Sutton and
Barto, 2018; Wu et al., 2018).
As stated in Wu et al. (2018), the NMT model
can be viewed as an agent , which interacts with
theenvironment with the previous words y1:i−1
and the corresponding contextual representations
at each training step. The parameters of the
agent define a policy, a conditional probability
PNMT(yi|x, y 1:i−1), and the agent will pick an ac-
tion, that is a candidate word out from the vocabu-
lary, according to the policy.
Different from the setting of reward as BLEU
(Papineni et al., 2002) in Wu et al. (2018), in our
trainable -kNN-MT, the reward for the NMT model
is the corresponding probability from the distri-
bution of kNN predictions, denoted as R( ˆyi, yi),
which is defined by comparing the generate ˆyiwith
the ground-truth sentence yiin terms of their corre-
sponding probabilities in the distribution of kNN
predictions. Note that the reward R( ˆyi, yi)is now atoken-level reward, a scalar for the generated token
ˆyi, which makes another difference compared with
Wu et al. (2018).
Therefore, the goal of fine-tuning in such a rein-
forcement learning framework is to minimize the
expected reward as follows,
L= (7)
1
|D|X
(x,y)∈D−R( ˆyi, yi) logPNMT(yi|x, y 1:i−1),
where R( ˆyi, yi)is defined as,
R( ˆyi,yi) = (8)
|PkNN( ˆyi|x, y 1:i−1)−PkNN(yi|x, y 1:i−1)|.
When R( ˆyi, yi)is zero, it leads training crash
in the current design. It means that either ˆyiis
correct or both of ˆyiand the ground truth yihave
not been retrieved from the kNN search. If it is in
the first situation, we will keep the loss calculation
from the vanilla NMT fine-tuning for the generated
token. Otherwise, R( ˆyi, yi)will be set to 1/kwith
the similar reason stated in the method of the kNN
ground truth probability.
3 Experiments
In this section, we will describe our experimental
design and report and discuss experimental results
and findings.
3.1 Data
We conduct experiments of the trainable -kNN-
MT on German-English translation tasks, keeping
on the same track as the kNN-MT (Khandelwal
et al., 2021) does, which include the IT, Medical,
Law, and Koran domains. We also conduct ex-
periments on English-German translation tasks to

--- PAGE 6 ---
Domain IT Medical Law Koran
Train 177,792 206,804 447,696 14,979
Validation 2,000 2,000 2,000 2,000
Test 2,000 2,000 2,000 2,000
De-En Datastore size 3.10M 5.70M 18.38M 0.45M
En-De Datastore size 3.33M 6.13M 18.77M 0.48M
Table 3: The datastore size and the number of the parallel sentences in the training, validation, test sets of each
domain, separately for German-English (De-En) and English-German (En-De) datasets.
Model IT Medical Law Koran Avg.
Base MT 38.35 40.14 45.63 16.29 35.10
Fine-tuned MT 47.14 57.19 61.28 22.98 47.15
Base trainable -kNN-MTGate Mechanism 48.63 57.81 61.42 22.77 47.66
Ground Truth Prob. 48.14 57.94 62.45 22.87 47.85
RL 47.88 57.33 62.49 23.39 47.77
FTtrainable -kNN-MTGate Mechanism 48.98 58.20 62.06 22.97 48.05
Ground Truth Prob. 49.31 58.28 63.41 22.90 48.48
RL 49.51 58.50 63.31 23.09 48.60
Table 4: Performances of the trainable -kNN-MT in vanilla fine-tuning, which means there is no integration of kNN
search during inference on German-English multi-domain test sets. The SacreBLEU scores are averaged along
domains for overall comparison. Compared with classic fine-tuning, the overall performance can be improved as
much as 1.45 BLEU by the trainable -kNN-MT.
evaluation the performance of the trainable -kNN-
MT on from-English translation task. We use the
same datasets as German-English translation tasks,
but switching the source and target side.
In order to pre-process the data, we perform
maximum length filtering with 250 on all of the
in-domain training data to ensure data quality. The
statistics of the in-domain datasets are shown in
Table 3.
3.2 Experimental Setup
Pre-trained NMT Models In our experimental
design, we use the pre-trained winner systems of
WMT 2019 German-English and English-German
news translation tasks (Ng et al., 2019) as the base
NMT models for the trainable -kNN-MT, which
are implemented with the Fairseq toolkit (Ott et al.,
2019) based on the big Transformer architecture
(Vaswani et al., 2017).
Given the base NMT model pre-trained on the
out-of-domain data, we apply the trainable -kNN-
MT algorithm for fine-tuning and compare its per-
formance with the classic vanilla fine-tuning. In
addition, when the "pre-trained" NMT model is afine-tuned model, it is also worth evaluating the per-
formance of trainable -kNN-MT to see if the NMT
model can be continuously enhanced by the train-
able-kNN-MT algorithm, even though the NMT
model has been trained and optimized on the in-
domain datasets. For either of the above cases, we
conduct experiments of trainable -kNN-MT with
three different ways of gradient scaling and report
SacreBLEU results with and without the integra-
tion of kNN search during inference.
Model Setting Thetrainable -kNN-MT is initial-
ized with the pre-trained NMT model and fine-
tuned with the Adam algorithm (Kingma and Ba,
2015). The learning rate is set to 5e-04 or 7e-05
for fine-tuning based on the base NMT model or
continuously fine-tuning based on the fine-tuned
model respectively. All experiments are run on
a single Tesla V-100 GPU card with a batch size
of 2048 tokens and a gradient accumulation of 32
batches. The datastore is re-constructed with the
updated weights of NMT model after each epoch
training, and this procedure repeats until the NMT
model converges.

--- PAGE 7 ---
Model IT Medical Law Koran Avg.
Base MT 38.35 40.14 45.63 16.29 35.10
kNN-MT (Khandelwal et al., 2021) 46.12 54.41 61.70 21.14 45.84
Adaptive kNN-MT (Zheng et al., 2021) 47.20 55.71 62.64 19.39 46.24
CKMT (Wang et al., 2022) 47.94 56.92 62.98 19.92 46.94
Robust- kNN-MT (Jiang et al., 2022) 48.90 57.28 64.07 20.71 47.74
kNN-KD (Yang et al., 2022) — 56.5 61.89 24.86 —
kNN-FT-MT 49.33 57.46 63.63 22.95 48.34
Base trainable -kNN-MTGate Mechanism 49.23 58.00 64.10 23.74 48.77
Ground Truth Prob. 49.49 58.15 64.48 23.59 48.93
RL 49.14 57.40 64.44 23.68 48.67
FTtrainable -kNN-MTGate Mechanism 49.96 58.34 64.67 23.81 49.20
Ground Truth Prob. 49.97 58.39 64.78 23.84 49.25
RL 49.84 58.60 64.99 23.78 49.30
Table 5: Performances of the trainable -kNN-MT with the integration of kNN search during inference on German-
English multi-domain test sets. The FT trainable -kNN-MT significantly outperforms all of the baseline systems
with the kNN search algorithm and yields an rough improvement of 1 compared with kNN-FT-MT.
Model IT Medical Law Koran Avg.
Base MT 29.74 35.56 40.85 13.97 30.03
Fine-tuned MT 39.70 52.5 57.16 32.45 45.45
Base trainable -kNN-MTGround Truth Prob. 41.17 53.38 57.75 32.45 46.19
RL 40.99 53.33 57.64 32.62 46.15
FTtrainable -kNN-MTGround Truth Prob. 41.65 54.10 58.41 32.74 46.73
RL 41.53 54.36 58.34 32.70 46.73
Table 6: Performances of the trainable -kNN-MT in vanilla fine-tuning on English-German multi-domain test
sets. Compared with classic fine-tuning, the overall performance can be improved as much as 1.28 BLEU by the
trainable -kNN-MT.
The hyper-parameters k,λandTare tuned on
the validation set of each domain, shown in table
9 and 10 of appendix A. We empirically set τin
Equation 5 to 0.6for each domain based on the
ablation studies.
Efficiency of kNN Search In terms of time and
storage efficiency, we follow Khandelwal et al.
(2021) to use FAISS (Johnson et al., 2021) index to
represent the domain-specific datastore and search
for nearest neighbors, with which the keys can be
stored in clusters to speed up search and be quan-
tized to 64-bytes for space efficiency, and the index
can be constructed offline via a single forward pass
over every example in the given in-domain datasets.Evaluation We evaluate the performance of
trainable -kNN-MT in the cases with two different
pre-trained NMT models as aforementioned and
compare them with classic fine-tuning and other
competitive models involving non-parametric kNN
search. The final results are evaluated with Sacre-
BLEU (Post, 2018) in a case-sensitive detokenized
setting1.
Among these, (1) Base MT represents the base
NMT model, the winner system of WMT 2019
news translation (Ng et al., 2019) trained with out-
of-domain data; (2) Fine-tuned MT represents
fine-tuning Base MT with the in-domain dataset;
1We use the exact same evaluation process as the kNN-MT
does.

--- PAGE 8 ---
Model IT Medical Law Koran Avg.
Base MT 29.74 35.56 40.85 13.97 30.03
kNN-MT (Khandelwal et al., 2021) 36.44 49.74 55.73 25.87 41.95
kNN-FT-MT 40.68 53.28 58.91 32.61 46.37
Base trainable -kNN-MTGround Truth Prob. 41.22 53.53 59.03 33.53 46.83
RL 41.10 53.40 58.97 33.30 46.69
FTtrainable -kNN-MTGround Truth Prob. 41.73 54.50 59.21 32.99 47.11
RL 41.73 54.63 59.22 32.88 47.12
Table 7: Performances of the trainable -kNN-MT with the integration of kNN search during inference on English-
German multi-domain test sets. The FT trainable -kNN-MT significantly outperforms all of the baseline systems
with the kNN search algorithm and yields an improvement of 0.75 BLEU compared with kNN-FT-MT.
(3)kNN-MT stands for the original kNN-MT algo-
rithm (Khandelwal et al., 2021) applied to the Base
MT; (4) kNN-FT-MT means the original kNN-
MT algorithm applied on Fine-tuned MT, where
the datastore is constructed with Fine-tuned MT;
(5)Base trainable -kNN-MT stands for fine-tuning
with our trainable -kNN-MT from the Base MT;
(6)FTtrainable -kNN-MT means continuously
fine-tuning with our trainable -kNN-MT from Fine-
tuned MT. All other competitive systems listed in
the tables of results are cited accordingly.
3.3 Experimental Results
Results without integration of kNN search dur-
ing inference. Our experimental results without
the integration of kNN search during inference
are summarized in Table 4 and Table 6. For both
of German-English and English-German transla-
tions, the trainable -kNN-MT is significantly supe-
rior over the classic fine-tuning in both of the Base
trainable -kNN-MT and the FT trainable -kNN-MT.
For German-English translations, the SacreBLEU
scores are generally improved by as much as 0.7
and1.45separately, while for English-German
translations, the SacreBLEU scores are improved
by0.74and1.28separately.
Moreover, even though the gate mechanism per-
forms comparably with the kNN ground truth prob-
ability in both of the the Base trainable -kNN-MT
and the FT trainable -kNN-MT, it is not an econom-
ical method, since ablation studies on the setting
of hyper-parameter tauin Equation 5 are always
needed once it comes to a new domain or language.
Due to this reason, we performed ablation stud-
ies on taufor German-English translations, but
we didn’t construct such experiments in English-German translations.
Results with integration of kNN search during
inference. With the integration of kNN search
during inference, the trainable -kNN-MT shows its
advantages in multiple domains, which is shown in
Table 5 and Table 7. It can be observed that both of
Base trainable -kNN-MT and FT trainable -kNN-
MT significantly outperforms all of the baseline
systems involving kNN search algorithms.
Base trainable -kNN-MT considerably outper-
forms the original kNN-MT and Fine-tuned MT
by as much as 3.09BLEU and 1.78BLEU respec-
tively for German-English translations and 4.88
BLEU and 1.38BLEU respectively for English-
German translations. It also outperforms the kNN-
FT-MT in all of the four domains by an average of
0.61BLEU and 0.46BLEU for German-English
and English-German translations separately. .
Moreover, FT trainable -kNN-MT generally out-
performs the original kNN-MT and Fine-tuned MT
by as much as 3.46BLEU and 2.15BLEU respec-
tively for German-English translations and 5.17
BLEU and 1.67BLEU respectively for English-
German translations. It also outperforms the kNN-
FT-MT in all of the four domains by an average
of1BLEU and 0.75BLEU for German-English
and English-German translations separately, which
prominently verifies that even if the NMT model
has been fine-tuned on the in-domain data, the train-
able-kNN-MT algorithm continues to improve the
translation performance consistently. Compared
with Base trainable -kNN-MT, the training of FT
trainable -kNN-MT is more efficient, economical
and practical.

--- PAGE 9 ---
Comparisons among methods of gradient scal-
ing. Among the three methods of gradient scal-
ing, reinforcement learning framework achieves
the best in the FT trainable -kNN-MT for both of
German-English and English-German translations,
while it performs comparably with the kNN ground
truth probability in the Base trainable -kNN-MT. In
the reinforcement learning framework, it is reason-
able because the kNN search is treated as a super-
vised reward model. Through preliminary observa-
tions, we have found that in retrieving translation
candidates, the kNN search from the datastore con-
structed with fine-tuned model weights can achieve
higher accuracy than that constructed with base
model weights. The better the reward model, the
better the effect of reinforcement learning.
We also perform qualitative analysis on the trans-
lations from the FT trainable -kNN-MT compared
withkNN-FT-MT, since kNN-FT-MT was the best
model we could get before we proposed the train-
able-kNN-MT. Interestingly, we found that when
translating grammatical relations, FT trainable -
kNN-MT with any of the three gradient scaling
methods performs better than kNN-FT-MT, which
was unexpected. Examples are displayed in Table
11 of Appendix A.
4 Conclusion
In this paper, we propose the trainable -kNN-MT
to learn translations with the assistance of statis-
tics from the non-parametric kNN predictions. It
utilizes a gate mechanism, the kNN ground truth
probability, and reinforcement learning to make
full use of the respective advantages and disadvan-
tages of the non-parametric kNN search algorithm.
Experimental results show that the trainable -kNN-
MT significantly outperforms the original kNN-
MT and the classic fine-tuning method, making it a
novel fine-tuning method for various domains and
translation tasks.
References
Per-Erik Danielsson. 1980. Euclidean distance map-
ping. Computer Graphics and image processing ,
14(3):227–248.
Hui Jiang, Ziyao Lu, Fandong Meng, Chulun Zhou,
Jie Zhou, Degen Huang, and Jinsong Su. 2022. To-
wards robust k-nearest-neighbor machine translation.
InProceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing , pages
5468–5477, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021.
Billion-scale similarity search with gpus. IEEE
Transactions on Big Data , 7(3):535–547.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2021. Nearest neigh-
bor machine translation. In International Conference
on Learning Representations .
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In International Conference on Learning
Representations .
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu,
Lu Zhang, and Zhi Jin. 2016. How transferable are
neural networks in NLP applications? In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing , pages 479–489,
Austin, Texas. Association for Computational Lin-
guistics.
Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,
Michael Auli, and Sergey Edunov. 2019. Facebook
FAIR’s WMT19 news translation task submission.
InProceedings of the Fourth Conference on Machine
Translation (Volume 2: Shared Task Papers, Day
1), pages 314–319, Florence, Italy. Association for
Computational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,
Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. 2019. fairseq: A fast, extensible toolkit for
sequence modeling. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics (Demonstrations) ,
pages 48–53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive sum-
marization. arXiv preprint arXiv:1705.04304 .
Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers , pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.

--- PAGE 10 ---
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 1715–1725,
Berlin, Germany. Association for Computational Lin-
guistics.
Richard S Sutton and Andrew G Barto. 2018. Reinforce-
ment learning: An introduction . MIT press.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Dexin Wang, Kai Fan, Boxing Chen, and Deyi Xiong.
2022. Efficient cluster-based k-nearest-neighbor ma-
chine translation. arXiv preprint arXiv:2204.06175 .
Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-
Yan Liu. 2018. A study of reinforcement learn-
ing for neural machine translation. arXiv preprint
arXiv:1808.08866 .
Zhixian Yang, Renliang Sun, and Xiaojun Wan. 2022.
Nearest neighbor knowledge distillation for neural
machine translation. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 5546–5556, Seattle,
United States. Association for Computational Lin-
guistics.
Zhilu Zhang and Mert Sabuncu. 2018. Generalized
cross entropy loss for training deep neural networks
with noisy labels. Advances in neural information
processing systems , 31.
Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian Huang,
Boxing Chen, Weihua Luo, and Jiajun Chen. 2021.
Adaptive nearest neighbor machine translation. arXiv
preprint arXiv:2105.13022 .
A Appendix

--- PAGE 11 ---
SourceSie können Writer-Textrahmen so miteinander verketten, dass ihr Inhalt automatisch
von einem Rahmen in den nächsten fließt.
ReferenceYou can link Writer text frames so that their contents automatically flow from one
frame to another.
Target Prefix You can
kNN-MTSub. join link chain link use link connect comb@@
Prob. 0.379 0.242 0.103 0.093 0.068 0.041 0.037 0.032
kNN-FT-MTSub. link chain nest nest link link link link
Prob. 0.698 0.243 0.029 0.012 0.008 0.003 0.002 0.001
SourceDie Quell- und Zielansicht ist der Hauptarbeitsbereich von & kompare;. Hier werden
der Inhalt und die hervorgehobenen Abweichungen der aktuell ausgewählten Quell-
und Zieldatei mit den Zeilennummern angezeigt.
ReferenceThe source and destination view is the main workspace of & kompare;. The contents
and highlighted differences of the currently selected source and destination file are
displayed here with line numbers.
Target Prefix The source and destination
kNN-MTSub. p@@ ann@@ brow@@ view view view view view
Prob. 0.679 0.110 0.049 0.039 0.035 0.034 0.025 0.025
kNN-FT-MTSub. view View view View View View view view
Prob. 0.361 0.193 0.159 0.073 0.073 0.048 0.047 0.043
Table 8: Examples of the kNN predictions in German-English IT domain translation task, where kNN-FT-MT
means applying kNN-MT algorithm on the fine-tuned NMT model, Sub. and Prob. represent kNN retrieved tokens
and corresponding probabilities in the kNN distribution and Target Prefix stands for prefix generated target tokens.
ThekNN search algorithm secures more accurate prediction candidates when the NMT model has been fine-tuned
with the IT training data.
Model IT Medical Law Koran
kNN-MT & Base trainable -kNN-MTk 8 16 16 8
λ0.6 0.8 0.8 0.6
T 5 5 5 100
kNN-FT-MT & FT trainable -kNN-MTk 8 16 8 8
λ0.4 0.4 0.6 0.4
T 10 10 10 100
Table 9: The hyper-parameters used in the kNN based models of German-English translations.
Model IT Medical Law Koran
kNN-MT & Base trainable -kNN-MTk 8 8 16 16
λ0.6 0.8 0.8 0.8
T 10 10 5 10
kNN-FT-MT & FT trainable -kNN-MTk 4 4 8 16
λ0.4 0.4 0.4 0.2
T 10 100 5 5
Table 10: The hyper-parameters used in the kNN based models of English-German translations.

--- PAGE 12 ---
SourceSollte Seine Peinigung über euch nachts oder am Tage hereinbrechen,
was wollen denn die schwer Verfehlenden davon beschleunigen?
Reference If His chastisement comes upon you by night or day, what part of it will
the sinners seek to hasten?
kNN-FT-MT If His punishment befalls you at night or in the day, what would the
sinners do to despatch it?
Gate Mechanism (Ours) If His punishment comes upon youby night or by day , how will the
sinners hasten it?
Ground Truth Prob. (Ours) If His punishment comes upon you at night or in the day, how will the
sinners hasten it?
RL (Ours) If His punishment comes upon you at night or in the day, what will the
sinners do to hasten it?
SourceUnd sie sagen: "Wir glauben daran." Aber wie könnten sie (den Glauben)
von einem fernen Ort aus erlangen,
Reference and they say, ‘We believe in it’; but how can they reach from a place far
away,
kNN-FT-MT They say: "We believe in it;" but how could they reach it from a place of
no return?
Gate Mechanism (Ours) And they say , "We believe in it"; so how can they reach it from a place
far away?
Ground Truth Prob. (Ours) And they say , "We believe in it"; so how can they reach it from a place of
no return?
RL (Ours) And they say , "We believe in it"; so how can they reach it from a place of
no return?
Source Und haltet sie an; denn sie sollen befragt werden.
Reference And detain them, for they will be questioned.
kNN-FT-MT Surely they are to be interrogated.
Gate Mechanism (Ours) And test them, and they will be questioned.
Ground Truth Prob. (Ours) Persevere with them, and they will be questioned.
RL (Ours) Persevere with them, and they will be questioned.
Table 11: Examples of translations from the trainable -kNN-MT with three ways to gradient scaling compared with
kNN-FT-MT in the Koran domain. Boldfaced words indicate their differences. The FTtrainable -kNN-MT can
translate correctly in the case of grammatical relation translations, while the kNN-FT-MT can’t.
