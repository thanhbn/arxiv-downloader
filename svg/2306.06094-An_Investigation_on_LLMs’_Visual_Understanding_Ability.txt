# 2306.06094.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/svg/2306.06094.pdf
# File size: 1751328 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
An Investigation on LLMs’ Visual Understanding Ability
Using SVG for Image-Text Bridging
Mu Cai∗1, Zeyi Huang∗1, Yuheng Li1, Utkarsh Ojha1, Haohan Wang2, Yong Jae Lee1
1University of Wisconsin–Madison2University of Illinois Urbana-Champaign
Abstract
Large language models (LLMs) have made significant advancements in natural lan-
guage understanding. However, through that enormous semantic representation
that the LLM has learnt, is it somehow possible for it to understand images as well?
This work investigates this question. To enable the LLM to process images, we con-
vert them into a representation given by Scalable Vector Graphics (SVG). To study
what the LLM can do with this XML-based textual description of images, we test
the LLM on three broad computer vision tasks: (i) visual reasoning and question an-
swering, (ii) image classification under distribution shift, few-shot learning, and (iii)
generating new images using visual prompting. Even though we do not naturally
associate LLMs with any visual understanding capabilities, our results indicate
that the LLM can often do a decent job in many of these tasks, potentially opening
new avenues for research into LLMs’ ability to understand image data. Our code,
data, and models can be found here https://github.com/mu-cai/svg-llm .
<?xmlversion="1.0"encoding="iso-8859-1"?><svgversion="1.1"id="Capa_1"xmlns="http://www.w3.org/2000/svg"xmlns:xlink="http://www.w3.org/1999/xlink"x="0px"y="0px"viewBox=“0 0 60 60”style=“enable-background:new0 0 54 54;”xml:space=“preserve”></svg>
<rectid="Rectangle"data-name="Rectangle"width="58"height="58"rx="2"transform="translate(1 1)"fill="white"stroke="skyblue"stroke-miterlimit="10"stroke-width="1"/><ellipsestyle="fill:#88C057;"cx="30"cy="40"rx="27"ry="11.5"/><ellipsestyle="fill:#659C35;"cx="22"cy="40.833"rx="18"ry="7.667"/><ellipsestyle="fill:#38454F;"cx="24.071"cy="39.222"rx="6.071"ry="4.722"/><polygonstyle="fill:#E64C3C;"points="21,3.5 40,8.5 21,14.5 "/>
<circlestyle="fill:#FFFFFF;"cx="43"cy="42.5"r="3"/><linestyle="fill:none;stroke:#ECF0F1;stroke-width:2;stroke-linecap:round;stroke-miterlimit:10;"x1="22"y1="38.5"x2="22"y2="3.5"/>File header
Rectangle  Polygon Line 
Image BorderflagCircle flagpole
golfgrassSet of ellipses (a)(b)
Figure 1: (a) An SVG representation illustrating a golf course. Each geometric shape represents a
distinct object. (b) LLMs can understand and generate shapes, colors, and relationships between
different elements in an interactive manner.
∗Equal Contribution. Order determined by random dice rolling.
1arXiv:2306.06094v2  [cs.CV]  11 Jul 2024

--- PAGE 2 ---
1 Introduction
Large-scale data and enormous compute: the effect of these two ingredients has been on display in
recent years in the significantly increased capability of machine learning systems. Models operating
on the two most popular forms of data - image and text - have particularly felt that effect the most.
From the side dealing with textual data, we have seen the emergence of large language models
(LLMs) such as ChatGPT (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b). Similarly, on the vision side,
large vision models (LVMs) have also shown impressive accomplishments (Dosovitskiy et al., 2020;
Liu et al., 2022; Dehghani et al., 2023).
When we compare these two realms, the abilities of LLMs stand out in a distinct way because of
their remarkable abilities in reasoning, in-context learning, and open-ended tasks (Bubeck et al.,
2023). These analytical capabilities are something that the vision models, despite their significant
advances, have not yet mirrored to the same depth (Dehghani et al., 2023; Alayrac et al., 2022).
This distinction can be attributed to the inherent nature of their respective data: LLMs thrive on
the diverse and sequential structure of textual data, which is conducive to understanding intricate
relationships and producing contextually relevant responses. In contrast, the continuous and varied
nature of visual data complicates the discernment of nuanced relationships, potentially hindering
the depth of analysis LVMs can achieve (Bar et al., 2022; Diwan et al., 2022; Lake et al., 2017).
Moreover, there is an ongoing debate on whether LLMs, trained on internet-scale text data, can
learn world models that could lead to AGI capabilities, or that they are fundamentally limited due
to their lack of grounding on physical interaction and visual perception in the real world (LeCun,
2022).
The disparity between LLMs and LVMs, and the debate on the necessity of physical interaction
and perceptual grounding, intrigue us to a question: Can LLMs, which have never seen visual data,
understand and reason about images? Answering this question will bring us closer to understanding
the capabilities of LLMs beyond the textual domain, their fundamental limitations, and whether
they possess world models. As such, our study takes a small but important step toward this goal.
To enable an off-the-shelf pre-trained LLM to “read” images, we use the Scalable Vector Graphics
(SVG) (Ferraiolo et al., 2000) representation to convert images into readable text. Unlike traditional
pixel-based images, SVGs are described in XML, offering a text-based portrayal of mid-level shapes,
curves, lines, and colors, as shown in Figure 1. The textual nature of SVG provides a data modality
that LLMs excel at, acting as a bridge to apply their analytical strengths to the visual domain. While
the Sparks of AGI paper (Bubeck et al., 2023) showed some initial qualitative results on the image
understanding capabilities of LLMs using a similar idea, we provide a deeper, comprehensive study
that includes both qualitative and quantitative analyses on a variety of visual understanding and
reasoning tasks.
Specifically, we evaluate whether an LLM can perform both discriminative and generative visual
understanding tasks. For discriminative tasks, we study their capabilities in visual reasoning, visual
question answering, few-shot learning for image classification, and their robustness to distribution
shift in visual data. Surprisingly, despite never having seen dense visual data, LLMs perform much
better than chance and are often robust to distribution shifts. For generative tasks, we study LLMs’
image generation ability through the task of visual prompting. We find that LLMs can identify
transformations related to color, shape, style, and content from provided SVG examples, generating
credible output that is consistent with those examples.
2

--- PAGE 3 ---
2 Related Work
2.1 Leveraging LLMs for Visual Tasks
Upon observing the powerful reasoning capabilities of LLM, researchers began to harness its
potential for visual tasks. Presently, there are three primary works to utilizing LLM for these
purposes. The first work involves using LLM to produce textual guidelines. Vision models then
rely on these instructions to execute a range of visual tasks. Examples include Visual ChatGPT (Wu
et al., 2023), visual programming as seen in Gupta & Kembhavi (2023), and ViperGPT (Sur ´ıs et al.,
2023). The second work, as illustrated by LLaVA (Liu et al., 2023) and MiniGPT4 (Zhu et al.,
2023), incorporates the pretrained vision encoder model, along with a trainable projector. This
allows for feeding visual features directly to LLMs, demonstrating remarkable reasoning abilities.
VisionLLM (Wang et al., 2023) presents bounding boxes and segmentation masks in text format (as
polygons), enabling LLMs to address more intricate perception challenges. The third work seeks to
represent images directly in a text-based format, bypassing the use of visual encoders. The goal here
is to allow LLMs to interpret these text-based representations. For instance, LIFT (Dinh et al., 2022)
represents images using their raw pixel values in textual form and then fine-tunes the language
model on them for visual tasks. Another study (Bubeck et al., 2023) explores image generation by
expressing the image in text formats, like TiKZ or SVG.
Our novelty: Our study aligns with the third direction. Unlike LIFT (Dinh et al., 2022), we represent
images using SVG, a format that inherently encodes more structural information than raw pixel
values. This could enable LLMs to better grasp intricate relationships and yield contextually
relevant responses. Distinct from the investigation presented in Bubeck et al. (2023), we conduct
a comprehensive study of how LLMs process images via textual representations, including both
discriminative and generative tasks.
2.2 Scalable Vector Graphics
Vector graphics describe images as collections of parameterized shape primitives such as polygons,
circles, and rectangles, rather than a regular raster grid of pixel values (Peng & Zhang, 2004).
Primitives are usually characterized by a set of coordinates delineating their contour and the
associated color. This leads to a compact and infinitely scalable representation where the appearance
can be easily modified by adjusting stroke or color parameters. Consequently, vector graphics
are the preferred choice among graphic artists and designers, as images maintain their sharpness
regardless of the zoom level. Encapsulated PostScript (EPS) and Scalable Vector Graphics (SVG) are
two notable vector-based formats (Ferraiolo et al., 2000). SVG format stores images as XML-based
text files that define geometrical objects and their properties (Ferraiolo et al., 2000), as shown in
Figure 1. This enables easy editing, manipulation, and embedding, which makes SVG particularly
versatile for web applications and graphic design tasks (Badros et al., 2001). EPS is another vector
format for high-quality graphics that can be resized without losing quality (Gruber et al., 2008). In
this paper, we systematically evaluate LLMs’ abilities to generate and understand an assortment of
visual concepts.
2.3 Large Language Models
Large Language Models (LLMs) have attracted much attention in recent years due to their remark-
able performance across numerous natural language processing tasks. GPT-3 (Brown et al., 2020a),
developed by OpenAI, is a prime example of this category, boasting an immense scale of 175 billion
parameters and human-like text generation capabilities. In a similar vein, BERT (Devlin et al.,
3

--- PAGE 4 ---
Table 1: Visual reasoning performance of Sort-of-Clevr dataset under in-distribution test set and
out-of-distribution test set with shape and color distribution shift.
Distribution shift i.i.d. o.o.d. Shape o.o.d. Color
Question type GPT4-brief GPT4-CoT CNN Rel. Net. GPT4-CoT CNN Rel. Net. GPT4-CoT CNN Rel. Net.
Image format SVG SVG PNG PNG SVG PNG PNG SVG PNG PNG
Unary 0.50 0.90 0.65 0.89 0.95 0.58 0.82 0.95 0.56 0.83
Binary 0.90 0.95 0.75 0.80 0.95 0.36 0.44 0.95 0.57 0.66
Ternary 0.10 0.88 0.55 0.55 0.63 0.52 0.56 0.71 0.47 0.54
Average 0.50 0.89 0.65 0.75 0.84 0.49 0.61 0.87 0.53 0.67
2019) (Bidirectional Encoder Representations from Transformers), introduced by Google, takes
advantage of the transformer architecture and has substantially enhanced the state-of-the-art across
various tasks by learning deep bidirectional representations. ChatGPT (OpenAI, 2023a), another
noteworthy model, is a GPT variant specifically designed for human-like conversational abilities.
The most recent iteration, GPT-4 (OpenAI, 2023b), succeeds GPT-3 (Brown et al., 2020b) and carries
on the LLM advancements in terms of scale and performance. These models lay the groundwork
for our research, enabling us to investigate their potential in more complex tasks such as image
understanding and generation. Our work illustrates the applicability of LLMs to SVG-based image
understanding and generation, paving the way for novel applications and research directions in
the visual domain.
3 Tasks and Experimental Results
There are many different kinds of visual understanding problems, and often times, models need
to have different kinds of abilities to solve them. In this section, we wish to investigate if LLMs
indeed have those required abilities. However, for them to solve a visual understanding problem,
there needs to be a way for them to seean image. SVG can be that bridge, where an image is
converted into a structured XML code (see Fig. 1 for an example). And like any other code, LLMs
can potentially read the code and perform some task of interest. To test what is possible using this
form of image representation, we consider three broad categories of visual understanding tasks.
In Sec. 3.1, we first study the problem of visual reasoning, where the model is asked certain kinds
of questions about the contents of an input image (e.g., How many objects have the same shape as the
red object? ). Next, in Sec 3.2, we study how LLMs fare in image classification tasks; especially under
non-trivial settings like distribution shifts and few-shot learning. Then, in Sec 3.2, we demonstrate
SVGs’ robustness to permutations. Furthermore, in Sec 3.3, we explore the understanding of SVG
by LLMs in visual question answering tasks. After studying their abilities in discriminative tasks,
in Sec 3.4, we then test the generative modeling capabilities of LLMs through the task of visual
prompting, which asks the model to generate a new image following the pattern depicted by three
images - A : B, C : ?. Finally, due to page constraints, we delve into additional generative tasks such
as style and content extrapolation, generation with human feedback, and referring segmentation in
the supplementary materials.
Unless otherwise mentioned, all the experiments use GPT-4 OpenAI (2023b) as the LLM model.
The common theme across all these different experiments is that the LLM processes the images
converted in the form of SVG.
4

--- PAGE 5 ---
(a) In-distribution test set(c) Out-of-distribution test set on shape(d) Out-of-distribution test set on colorUnary: Q: What is the shape of the green object? A: RectangleBinary:Q: How many objects have the same shape as the red object?A: 4Ternary:Q: Is there an object lies on the line connecting the blue and gray objects?A: Yes(b) Sample questions from in-distribution test setFigure 2: Illustration of In-Distribution and Out-of-Distribution Test Sets: (a) Images from the
in-distribution test set, showcasing random sampling of object color, shape, and location. (b)
Accompanying each image are questions assessing unary, binary, and ternary reasoning capabilities.
(c) Expansion of shape variety to include ellipses alongside rectangles and circles. (d) Introduction
of additional colors, including magenta, black, and purple, to the sampling palette.
3.1 Visual reasoning
With all the successes in the traditional perception tasks like image classification/segmentation Liu
et al. (2022); Kirillov et al. (2023), visual reasoning still remains a pivotal challenge for many modern
computer vision systems Zhang et al. (2019); Barrett et al. (2018); Santoro et al. (2017). It typically
refers to the ability of a model to answer questions about different constituents of an image. To
better understand what entails that task, we first discuss the dataset we use for this experiment.
Dataset: We use the Sort-of-Clevr dataset Santoro et al. (2017). As shown in Figure 2, each image
in the dataset is composed of 6 objects, each with a unique color, having a randomly chosen shape
between rectangle and circle. For each such image, the dataset contains many {question, answer }
pairs. There are three categories of questions crafted to test three levels of reasoning capabilities:
unary, binary, and ternary relationships. An example question testing binary reasoning ability is -
What is the shape of the object closest to the gray object? Please refer to Fig. 2 for more examples of the
questions. Each question is, ultimately, a classification problem. The methods are evaluated based
on their top-1 accuracy on questions from the test set of the dataset. Please refer to appendix for the
details about the test sets used for evaluation.
Methods: Broadly speaking, we show the results on two kinds of methods - (i) LLMs which are
nottrained for this task, and are only prompted during inference, and (ii) methods which are trained
for this task on the training set of Sort-of-Clevr dataset. Among the first category, we analyze (a)
GPT4-brief (OpenAI, 2023b), (b) GPT-CoT (Chain of thought), and (c) GPT-4V OpenAI (2023b)
and open source multimodal models including LLaVA (Liu et al., 2023), InstructBLIP Dai et al.
(2023), BLIP2 Li et al. (2023), mPLUG owl Ye et al. (2023), and MiniGPT4 Zhu et al. (2023). Among
the second category, we study two methods: (a) CNN+MLP and (b) Relation Networks (Santoro
et al., 2017). The reason we choose to evaluate on this category of trained models is to have an
idea of what the upper bound could be; i.e., how difficult the task really is. To evaluate any of
the LLMs during inference, we transform the original images into geometric primitives into their
SVG format. Then we query the LLM using the following prompt: e.g., ”Given the following SVG
image <svg>...</svg> , what is the shape of the red object?” The difference between GPT4-brief
and GPT4-CoT is the way we ask the final question: in GPT4-brief, as the name suggests, our final
question asks the model to provide the answer briefly, whereas in GPT4-CoT, we explicitly ask the
5

--- PAGE 6 ---
Table 2: Image classification results with both vision and language models. We utilize the Mini-
MNIST dataset to evaluate GPT4’s ability to understand SVG through both zero-shot and one-shot
in-context learning. To evaluate the model’s robustness against distribution shift, vision model
ConvNeXt and language model Vicuna are finetuned on the MNIST training set and evaluated
on the MNIST test set, CMNIST-A, and CMNIST-B respectively. ICL denotes in-context learning.
Additionally, Vicuna’s performance with PNG inputs is compared.
Method ConvNeXt(fine-tuning) Vicuna(fine-tuning) GPT4(Zero-Shot) GPT4(One-Shot ICL)
Image Format PNG SVG PNG SVG SVG
MNIST 99.5% 99.1% 99.4% 20% 24%
CMNIST-(A) 79.5% 95.7% 42.9% 16% 19%
CMNIST-(B) 32.6% 92.9% 24.8% 13% 20%
model to break down its reasoning before arriving at the answer (please see the appendix for the
exact prompt used to elicit this behavior). By querying an LLM, we obtain answers which are then
summarized post human evaluation to determine the final accuracy. Due to costs associated with
probing GPT4 models, our evaluation is restricted to 120 examples.
Table 1 shows the top-1 accuracy of different methods (left: zero-shot inference of LLMs, right:
methods trained for the task). Note that the results of multimodal models such as LLaVA are shown
in the appendix. When looking at the two LLM models that process images in the SVG format,
GPT4-brief, and GPT-CoT, we can first see that their performance is much higher than chance
(many questions in the test set have 5-6 correct answers, thereby reducing the chance performance
accuracy; please see the appendix). Furthermore, the performance of GPT-CoT even surpasses
the performance of a model explicitly trained for this task. If we take a step back and think once
more about the nature of SVG representation (Fig. 1), the best-case scenario might be when images
from the Sort-of-Clevr dataset have the locations of certain shapes embedded in their XML code.
But even if such a nicely structured code is often available to the LLM, to properly be able to
reason about questions like the ones described in Fig. 2, the LLM needs to precisely perform many
mathematical relational operations. From that perspective, the results depict that LLMs might
possess much complex models already.
Distribution shift evaluation: Furthermore, to study an even more difficult version of the problem,
we test the performance of models under distribution shifts. Specifically, we evaluate under both
shape and color distribution shifts. As for the color, we replace 3 colors from the original 6 colors
with the new colors. For shape, we randomly enlarge the options to further include the ellipse and
triangle, as shown in Figure 2 (c) (d). As a result, each object can sample the shape uniformly from
the 4 choices. Importantly, we make sure that all visual reasoning questions can be answered using
the original one-hot choices for vision models like CNN-MLP and relation networks.
As shown in Table 1, the LLM model (GPT4-CoT) using the SVG format to process images does not
suffer much by any of the newly added complications in the test images (e.g., more shapes added
under the shape distribution shift), maintaining its ability to perform the reasoning tasks. Here
is what this means in simple words, as understood through an example: if in the original image
(before distribution shift), there was a red circle immediately to the left of a blue rectangle, even
after introducing other shapes (e.g., triangles), the LLM can still detect the red circle to the left of
that rectangle. This is not trivial, because the models which were explicitly trained on the Sort-of-
Clevr dataset do suffer a non-trivial loss in performance; both in color and shape distribution shift.
Overall, these results indicate that the internal model used by the LLM is surprisingly effective at
tasks that we wouldn’t have naturally thought of it being good at.
6

--- PAGE 7 ---
Figure 3: Out-of-Distribution generalization tasks, where we wish to train a model on the standard
gray-scale MNIST and test on variants of colored MNIST.
3.2 Out-of-distribution Generalization
To DNNs, innocuous transformations can completely change predictions. This has been reported
in various cases such as shifting the image by a few pixels (Azulay & Weiss, 2018), adding a bit of
random noise (Hendrycks & Dietterich, 2019) or changing the background, color, or texture (He
et al., 2021; Arjovsky et al., 2019; Geirhos et al., 2018) while keeping the shape intact. In this section,
we aim to investigate if representing images as SVG could mitigate these issues and result in a
model focusing more on recognizing the target shapes.
Datasets : We construct two variants of the Colored-MNIST dataset to assess model robustness
against color and background variations. The first version, termed Colored-MNIST-A, assigns
a color of either red or green to the foreground, with each color being selected randomly at an
equal likelihood of 50%. In the more challenging second version, dubbed Colored-MNIST-B, both
the background and foreground are selected from a color palette that includes black, white, red,
blue, and green. The background and foreground colors are always distinct, yielding 20 unique
color combinations. Visualization of these Colored-MNIST datasets can be viewed in Figure 3.
Furthermore, we utilize the curve tracing algorithm to convert MNIST images into the SVG format.
More details can be found in the appendix.
Task and experimental setting : In the first setup, we fine-tune the ImageNet pre-trained vision
model ConvNeXt (Liu et al., 2022) using PNG images and the pre-trained language model Vi-
cuna Vicuna (2023) using both PNG images and SVG-converted images on MNIST. Subsequent
testing is carried out on both Colored-MNIST variants (A) and (B). This setup seeks to examine
whether the model can prioritize shape over other features for its predictions. In the second setup,
our objective is to explore the potential of harnessing the potent in-context capabilities of Large
Language Models (LLMs) to enhance image classification using SVG. So, we employ GPT-4 (Ope-
nAI, 2023b) to conduct both zero-shot and in-context learning on MNIST variants. More details on
ConvNeXt and Vicuna fine-tuning, prompting for in context learning can be found in the appendix.
Results and discussion : In Table 2, fine-tuning Vicuna with SVG representation shows promising
results on the CMNIST-A and CMNIST-B benchmarks, achieving accuracies of 95.7% and 92.9%
respectively. This suggests some level of robustness against color and background perturbations.
On the other hand, ConvNeXt and Vicuna under PNG format are more susceptible to these
perturbations, with a noticeable decline in performance on both benchmarks compared to i.i.d
results. We hypothesize that SVG might offer a representation more biased towards shape, given
its explicit textual encoding of object shapes, allowing for disentanglement of shape from color
information. Further, as illustrated in Table 2, there’s a notable 4% accuracy boost when using a
single in-context sample, as compared to a zero-shot classification approach. This demonstrates the
capability of LLM to grasp visual concepts contextually.
7

--- PAGE 8 ---
Table 3: Synthetic data study results (color-aware mIOU) on the six tasks (Bar et al., 2022). GPT4 is
better able to understand and reason about different transformations using SVG representation.
Method Color Shape Size Color Shape Color Size Shape Size
VQGAN (Esser et al., 2021) 7.0 19.1 16.2 7.4 2.2 18.4
BEiT (Bao et al., 2022) 40.9 31.4 7.1 33.1 21.2 13.0
MAE (He et al., 2022) 70.2 44.0 34.7 19.3 19.0 46.0
MAE-VQGAN (Bar et al., 2022) 40.4 46.5 42.0 20.4 18.3 40.3
SVG with GPT4 100.0 92.6 100.0 92.6 100.0 86.5
Table 4: Results of GPT-4, GPT3.5, and Mixtral under each type of question task: Color, Category,
and Usage. The results showcase that current LLMs have a certain level of SVG understanding
capabilities in object’s color, category, and usage.
GPT-3.5 Mixtral-8x7B GPT-4
Color 32.7 49.8 55.7
Category 25.7 27.6 41.7
Usage 37.5 38.6 49.1
Average 32.0 38.7 48.8
3.3 Visual Question Answering
To quantitatively and comprehensively how well large language models can understand SVGs, we
collect a large-scale SVG question-answering dataset with 2318 QA pairs under three hierarchical
levels including color, category, and usage.
Q: What type of equipment is represented in this SVG image?A: ExcavatorB: ForkliftC: BulldozerD: Crane
Q: What is the primary color of the folder in the image?A: BlueB: YellowC: GreenD: RedQ: Which type of application is this icon most likely to represent?A: A currency exchange platformB: A music creationC: An email clientD: A social media app(a) Color(b) Category(c) Usage
Figure 4: The samples SVG question-answering dataset, which belongs to the Color, Category, and
Usage task, respectively.
Datasets : We collect the dataset following 2 steps. First, we leverage GPT-4V to automatically
generate the question, four options along with a solution. This process involves feeding the
rasterized SVG image, task instruction, and two in-context examples into GPT-4V . The detailed
prompts are shown in the appendix. After this step, we collect 1000 questions for each type.
Next, we ask human annotators to filter out any QAs that contain either wrong or unreasonable
questions/options/answers, where we finally get 671, 869, and 778 high-quality SVG QAs. The
examples are shown in Figure 4.
Task and experimental setting : In this task, LLMs are tasked with answering various questions
regarding the attributes of objects, including their color, category, and usage. We query repre-
sentative LLMs, including GPT-3.5 OpenAI (2023a), GPT-4 OpenAI (2023b), and the recent SoTA
open-sourced LLM Mixtral-8x7B Jiang et al. (2024). After getting their responses, we leverage GPT-4
to rate whether the LLM prediction is correct again using in-context examples.
8

--- PAGE 9 ---
Figure 5: Understanding SVG transformations through the lens of GPT-4. GPT4 clearly understands
simple shape, color, and size transformations by analyzing the SVG code without any pixel-level
information. The generation results of our method are annotated with a red square.
Results and discussion : As shown in Table 4, GPT-4 shows a nontrivial average accuracy of 48.
8%, which means that GPT-4 can understand both semantics and low-level information of SVGs
to a reasonable extent. For less capable models like GPT-3.5, the performance is less desirable
but still higher than the random chance (25%). Surprisingly, open-sourced LLM Mixtral-8x7B
shows stronger SVG understanding capability than GPT-3.5 across all aspects, specifically for color
understanding. Our benchmark demonstrates that both open-sourced and closed-sourced LLMs
have certain level of the SVG understanding ability, where they excel in color recognition, while
showing lower performance in category (semantic) understanding.
3.4 Visual Prompting
The previous sections discussed the emergent abilities of LLMs in discriminative tasks. Now, we
turn our attention towards the generative side to see if LLMs can understand and generate logically
coherent images as well. We consider the task of visual prompting, where, given a series of images,
the goal is to learn the transformation and fill in the remaining spot with an appropriate image.
Dataset and evaluation. We follow Bar et al. (2022) to create a set of three tasks of filling in the
remaining spot (Fig. 5), and three of their combinations, and evaluate each model on 100 examples
per task. Every pair in our example set includes an SVG showcasing a colored shape along with
a corresponding SVG with specific transformations. The transformations consist of color, size, or
a combination of these aspects. We explain the descriptions of each task in the appendix. For
evaluation purposes, we adopt the method from Bar et al. (2022), for measuring and reporting the
color-aware mean Intersection over Union (mIOU). Given two example pairs and a query SVG,
we structure the text prompt in the same fashion for all tasks. The prompt is designed to figure
out the common transformation in the two examples first and then transform that query into the
corresponding key SVG code. We include the prompt details in the appendix.
Qualitative and quantitative results. The results are presented in Table 3. See Figure 5 for our
generated results. GPT4 clearly understands simple shape, color, and size transformations by
analyzing the SVG code without any pixel-level information.
9

--- PAGE 10 ---
4 Conclusion
This paper studied whether LLMs can “see” and understand images through the Scalable Vector
Graphics (SVG) format. By converting raster images into SVG representations, we provided LLMs
with a textual representation of visual data, enabling them to engage in a variety of computer vision
tasks. Our exploration across three major domains—visual reasoning and question answering,
image classification in the face of distribution shifts and with limited examples, and the generation
of new images through visual prompting. Despite their primary design for natural language
understanding, LLMs demonstrate a promising capacity to understand and generate visual content
when it is represented in an SVG format. Additionally, we have collected a SVG question-answering
dataset for future research aimed at assessing the visual understanding capabilities of LLMs. We
believe that our work provides an exploratory step for understanding the capabilities of LLMs
beyond the textual domain, whether they possess world models, and what their potential is.
5 Limitations
While the structured nature of the SVG representation does make them effective for certain visual
tasks, that format also abstracts away the finer details of the natural images (e.g., the furry texture
of a dog). To understand the negative effect of this abstraction, we consider the image classification
task; specifically, 10-way classification on Imagenette Howard dataset. We convert the images from
the dataset into their corresponding SVG format and use the Vicuna-7B model to get the predicted
class. This prompting method gives a top-1 accuracy of 68.14%. We also train a ResNet50 model
on the original images, which achieves an accuracy of 90.62%. These results show that the loss of
information during the SVG conversion can lead to a loss of performance for certain tasks.
References
Adobe Inc. Adobe illustrator. https://adobe.com/products/illustrator , 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. NeurIPS , 35:23716–23736, 2022.
Martin Arjovsky, L ´eon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893 , 2019.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? arXiv preprint arXiv:1805.12177 , 2018.
Greg J Badros, Jojada J Tirtowidjojo, Kim Marriott, Bernd Meyer, Will Portnoy, and Alan Borning. A
constraint extension to scalable vector graphics. In WWW , pp. 489–498, 2001.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers.
InInternational Conference on Learning Representations , 2022. URL https://openreview.net/
forum?id=p-BhZSz59o4 .
Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting
via image inpainting. NeurIPS , 35:25005–25017, 2022.
David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract
reasoning in neural networks. In International conference on machine learning , pp. 511–520. PMLR,
2018.
10

--- PAGE 11 ---
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In NeurIPS , 2020a.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. NeurIPS , 33:1877–1901, 2020b.
S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:
Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning. In NeurIPS , 2023.
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision
transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442 , 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT , 2019.
Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong
Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for
non-language machine learning tasks. Advances in Neural Information Processing Systems , 35:
11763–11784, 2022.
Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground
hard? investigating failures in visuolinguistic compositionality. arXiv preprint arXiv:2211.00768 ,
2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image
is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 ,
2020.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.
12873–12883, 2021.
Jon Ferraiolo, Fujisawa Jun, and Dean Jackson. Scalable vector graphics (SVG) 1.0 specification .
iuniverse Bloomington, 2000.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias
improves accuracy and robustness. arXiv preprint arXiv:1811.12231 , 2018.
Andreas R Gruber, Ronny Lorenz, Stephan H Bernhart, Richard Neub ¨ock, and Ivo L Hofacker. The
vienna rna websuite. Nucleic acids research , 36(suppl 2):W70–W74, 2008.
11

--- PAGE 12 ---
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reason-
ing without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 14953–14962, 2023.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 16000–16009, 2022.
Yue He, Zheyan Shen, and Peng Cui. Towards non-iid image classification: A dataset and baselines.
Pattern Recognition , 110:107383, 2021.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261 , 2019.
Jeremy Howard. imagenette. URL https://github.com/fastai/imagenette/ .
Inkscape Project. Inkscape. https://inkscape.org , 2020.
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
et al. Mixtral of experts. arXiv preprint arXiv:2401.04088 , 2024.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint
arXiv:2304.02643 , 2023.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and brain sciences , 40:e253, 2017.
Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open
Review , 62, 2022.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. 2023.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
arXiv:2304.08485 , 2023.
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 11976–11986, 2022.
OpenAI. Chatgpt. https://openai.com/blog/chatgpt/ , 2023a.
OpenAI. Gpt-4 technical report. 2023b.
Zhong-Ren Peng and Chuanrong Zhang. The roles of geography markup language (gml), scalable
vector graphics (svg), and web feature service (wfs) specifications in the development of internet
geographic information systems (gis). Journal of Geographical Systems , 6:95–116, 2004.
Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter
Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning.
Advances in neural information processing systems , 30, 2017.
12

--- PAGE 13 ---
D´ıdac Sur ´ıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for
reasoning. arXiv preprint arXiv:2303.08128 , 2023.
Vicuna. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https:
//vicuna.lmsys.org/ , 2023.
VTracer. Vtracer. https://www.visioncortex.org/vtracer-docs , 2020.
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong
Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for
vision-centric tasks. arXiv preprint arXiv:2305.11175 , 2023.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-
sual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint
arXiv:2303.04671 , 2023.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,
Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models
with multimodality. arXiv preprint arXiv:2304.14178 , 2023.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational
and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pp. 5317–5327, 2019.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhanc-
ing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 , 2023.
Appendix
A Qualatative Chat Results
Image Recognition and Manipulation. In this section, we provide examples for chat-based
image recognition and manipulation using GPT4 (OpenAI, 2023b). The qualitative results are
shown in Figure 6 and 7: (a) SVG representation empowers robust in-context digit recognition
capability given different background and foreground colors, (b) GPT4 can recognize and depict the
details of a dog with the prompt: "a stylized bear or a similar mammal with a round face
and ears." Furthermore, GPT-4 can identify the location of the dog’s left eye and remove it. (c)
GPT4 is capable of recognizing a natural image from the CIFAR-10 dataset.
Referring Segmentation The objective of the task is to label pixels in an image or video that
correspond to an object instance referred by a linguistic expression. SVG representation has
two advantages. First, language instruction is naturally embedded within the prompt, thus a
separate design of the image segmentation model is not needed. Second, a large corpus of text and
programming languages including XML are seen during pretraining, benefiting the vision-language
understanding ability.
SVG is typically composed of several colored polygons, where each of them can correspond to a
part of the object. Therefore, we can use the referring segmentation instructions to guide the LLM
in finding the corresponding SVG code. Shown in Figure 6 (b) and (d), LLM can localize the object
decently well. In (b), the majority of the airplane is selected as foreground, while in (d), not only is
the lettuce recognized, but also the two pieces of cheese are localized and subsequently removed.
13

--- PAGE 14 ---
Generate SVG letter “D” with same style as “B” and “C”?
Keep font size consistent with “B” and “C”
What’s the object?
Segment the airplane
Airplane.
What’s the food?
Change lettuce to tomato
Sure! Here is the SVG drawing:
Great! Please remove cheese
Sure! Here is the SVG drawing:
The SVG drawing represents a hamburger.
(a)(d)(b)
What digit does this SVG represent?
No,it is8. What about this? 
The SVG drawing represents the digit 2.
Apologies for the error. Current SVG represents digit 6.
No, it is 0. What about this?
1.
Great! What about this?
5.
Great! Thank you!You're welcome! If you have any more questions or need further assistance, feel free to ask!
(c)Figure 6: In-context learning and image generation capabilities of SVG with LLMs. (a) With human
feedback, LLM gradually performs better on digit classification. (b) LLM powers SVG with the
capability of image recognition and referring segmentation. (c) With human feedback, the content
generation performance becomes better. (d) LLM can recognize and manipulate specific parts of
the hamburger, such as removing or replacing them.
14

--- PAGE 15 ---
What object does this SVG represent? 
What kind of animal is it?
It's difficult to determine the exact animal based on the SVG, but it resembles a stylized bear or a similar mammal with a round face and ears.This SVG represents a cute, symmetric face with ears, eyes, and a mouth.
This SVG represents a bird.
(a)(b)
What digit does this SVG image represent?
No, this is digit 3. What about this? 
This SVG image represents the digit 0.
This SVG image represents the digit 6.
Great! What about this?
This SVG image represents the digit 1.(c)
Removethe left eye
What animaldoes this SVG represent? 
Great, thank you!
You're welcome! I'm glad I could help. If you have any more questions, feel free to ask.Figure 7: More qualitative results of chat-based image recognition and manipulation. (a) In-context
digit recognition in Colored-MNIST-(B). (b) GPT can explain and manipulate the dog SVG image.
(c) GPT4 can also recognize the bird from a CIFAR-10 example.
15

--- PAGE 16 ---
Figure 8: More qualitative results of style extrapolation. The generation results of our method are
annotated with a red square.
16

--- PAGE 17 ---
B Robustness to Permutations
Next, to evaluate the robustness of LLMs against variations in SVG data, we conduct three distinct
experiments, where we: (i) shuffle the order of paths, (ii) randomize path coordinate replacement,
and (iii) randomize string replacement. Each experiment is designed to mimic real scenarios
of imperfections in SVG data, providing insights into using SVG with LLMs under challenging
conditions.
Path Shuffle Experiment. SVG data consists of multiple path elements, each representing an object
or line in the image. In this experiment, we test the model’s ability to interpret hand-drawn SVG
data from the MNIST dataset when the sequence of path elements is shuffled. Note that, by the very
nature of SVG data, this alteration will not change the final image; and hence, it is important to test
if the LLM can remain invariant to this change, even though we have not explicitly provided it with
this knowledge. The goal is to assess the model’s ability to correctly interpret the digit, regardless
of the order of path elements. The results in Table 5 indicate that LLMs are robust to path shuffle at
test time, maintaining a comparable performance regardless of the path ordering.
Table 5: Model performance with and without shuffled SVG path elements.
Without Shuffle With Shuffle
Accuracy (%) 99.10 98.74
Random Path Coordinate Replacement. We further introduce a subtle form of noise by randomly
altering the coordinates in the SVG path data. Each numerical value within the path commands
is randomly translated within a specific range. For example, for, ... <path d="M0 0 C18 0
17..."> might become ... <path d="M1 0 C18 1 18..."> . We test several variations: (i) a
minor adjustment within a 1/28th range (reflecting the 28x28 resolution of MNIST) and (ii) a
more significant alteration within a 5/28th range. This simulates real-world scenarios of minor
inaccuracies in SVG data, such as those resulting from conversion errors or imprecise digitization.
The accuracy under different noise scales is presented in Table 6. Results indicate that LLMs are
decently robust to the perturbation of the coordinate values.
Table 6: Model performance under different noise scales.
0/28 1/28 2/28 5/28
Accuracy (%) 99.10 98.97 97.91 87.56
Random String Replacement. Finally, we design the most aggressive test of robustness, where
we replace random characters in the SVG strings with any English alphabet letter, digit, or spe-
cial symbol, regardless of whether they are numerical values or part of SVG commands (like
transform="translate(0,0)" ). The experiment is conducted with varying probabilities for char-
acter replacement, as shown in Table 7. Surprisingly, even after replacing 20% of a string with
random characters, LLMs maintains a high accuracy rate of 90.79%. This suggests that LLMs can
handle a wide range of perturbations in SVG data.
In conclusion, the results from these experiments demonstrate that despite the introduction of
perturbations in the SVG data, LLMs can still perform well under challenging conditions.
Table 7: Accuracy (%) with different probabilities of random string replacement.
0% 1% 5% 10% 20% 50%
Acc 99.10 99.06 98.40 97.40 90.79 39.38
17

--- PAGE 18 ---
C Style and Content Extrapolation
In this section, we assess if LLMs can extrapolate SVG codes with more challenging transformations,
such as content and style.
Style generation: We present LLMs with sample SVG letters. The first task is to figure out the style
in the given examples. Then, given a new test query, the second task is to transform this given query
so that it adheres to the same stylistic conventions as the example letters. The qualitative results
can be found in the appendix. We observe that GPT-4 is capable of replicating styles by analyzing
the correlation between given example SVG letter pairs and using this analysis to generate the
corresponding test key letter.
Figure 9: Understanding SVG content through the lens of GPT-4: GPT-4 demonstrates its ability
to generate accurate content by analyzing the correlation between provided example number
pairs, and subsequently applying this relationship to ascertain the corresponding test key number.
Remarkably, in scenarios where the relationship exhibits ambiguity, GPT-4 can identify multiple
possible interpretations.The generation results of our method are annotated with a red square.
Content generation: LLMs are shown two examples of SVG code pairs. Each pair consists of a
query and key pair (both are numbers), where the query describes an SVG code of a number, and
the key describes the SVG code of another number with an introduced mathematical operation. The
operation can consist of add, subtract, multiply, and divide. The mathematical operation should
be held in both example pairs. The first task is to figure out the mathematical operation in the
two examples. Then, given a new test query SVG number, the second task is to identify what
number it is and follow the mathematical operation discovered to generate the corresponding test
key number. GPT-4 showcases its capability in content generation by analyzing correlations in
example SVG number pairs and applying these relationships to identify corresponding test key
numbers. Impressively, in cases of ambiguity, GPT-4 can discern multiple potential interpretations.
We include qualitative results in Figure 9. The prompt details can be found in the appendix.
D Visual Reasoning Results of More Large Multimodal Models
Here we evaluate GPT-4V OpenAI (2023b) and recent open-sourced multi-modal large language
models, including LLaVA (Liu et al., 2023), InstructBLIP Dai et al. (2023), BLIP2 Li et al. (2023),
mPLUG owl Ye et al. (2023), and MiniGPT4 Zhu et al. (2023). As the result in Table 8 shows, all
current open-sourced multimodal models struggles at this fundamental reasoning task. Besides,
we observe that LLaVA frequently defaults to ’yes’ for yes/no queries and often resorts to random
guessing for counting tasks. This behavior underscores the limitations of current large multimodal
models in structured and sophisticated reasoning.
18

--- PAGE 19 ---
Table 8: Catogori-wise accuracy on the Sort-of-Clevr dataset.
Question type GPT4-brief GPT-CoT GPT-4V LLaVA CNN+MLP Relation Networks InstructBLIP BLIP2 mPLUG owl MiniGPT4
Format SVG SVG PNG PNG PNG PNG PNG PNG PNG PNG
Unary 0.50 0.90 0.75 0.60 0.65 0.89 0.53 0.50 0.38 0.53
Binary 0.90 0.95 0.74 0.60 0.75 0.80 0.53 0.53 0.63 0.55
Ternary 0.10 0.88 0.28 0.10 0.55 0.55 0.10 0.30 0.30 0.30
Average 0.50 0.89 0.59 0.43 0.65 0.75 0.38 0.44 0.43 0.46
Style Extrapolation: LLMs are provided with five example pairs and are tasked with deciphering
the stylistic attributes inherent in these examples. Following this, a new test query is presented
to the LLMs. Their objective is to modify this query into the corresponding key, ensuring that it
aligns with the same stylistic principles showcased in the example pairs. The qualitative results
are shown in figure 8. The specific prompt utilized for this purpose is detailed below: ‘‘Please
perform the following task carefully. In this task, you will be shown five
examples of Scalable Vector Graphics (SVG) code pairs. Each pair consists of a
query and key pair (both are English letter), where the query describes the SVG
code of the original image, and the key describes the SVG code of the transformed
image. Each will be named \Example Query #" and \Example Key #" respectively.
Your first task is to figure out the common transformation in the five examples.
The transformation can consist of color, shape, size, style, font, and background
changes, or any combination thereof. Even though you cannot see the images, and
only their SVG codes, you need to discover the transformations that are happening
at the image level and not just at the code level. Be detailed, and try to
discover every change, and the most important change is that the paths in the
SVG code between each query and key is different due to the common transformation
but the shapes of the letters that query and key are representing remain the same.
Then, given a new test query SVG code (named \Test Query"), your second task is
to transform that query into the corresponding key SVG code (named \Test Key"),
following the common transformation that you discovered in the five example pairs.
To help you better understand the transformation, I will also inform you of what
letter each query and key represent. You need to find the shape of each query
and key by analyzing their path. Here are the five example query and key pairs:
Example Query 1 (letter B):; Example Key 1 (letter B):<SVG code here>; Example
Query 2 (letter R):<SVG code here>; Example Key 2 (letter R):<SVG code here>;
Example Query 3 (letter Z):<SVG code here>; Example Key 3 (letter Z):<SVG code
here>; Example Query 4 (letter E):<SVG code here>; Example Key 4 (letter E):<SVG
code here>; Example Query 5 (letter N):<SVG code here>; Example Key 5 (letter
N):<SVG code here>; Here is the test query and key pair: Test Query (letter #):;
Test Key: ’’
E Experiment Details
E.1 Dataset
Human Designed SVG Dataset We collect a dataset from the public collection of SVG images.1
Specifically, we collect the digits and icons to demonstrate image recognition and generation
capabilities. Examples are shown in Figure 10 (a) and (b).
1https://www.svgrepo.com/ ,https://www.kaggle.com/datasets/victorcondino/svgicons
19

--- PAGE 20 ---
Convert Raster Images to SVG 1) Directly convert using curve tracing. Given the rich set of natural
images in raster format, we utilize the curve tracing algorithm to convert RGB images into the SVG
format.2Specifically, we convert MNIST (LeCun et al., 2010) to SVG format using this approach,
shown in Figure 10 (c).
(a)(b)(c)(d)
Figure 10: Visualization of our datasets. (a) and (b) are human-designed SVG vectors and icons. (c)
and (d) are converted from raster images. Specifically, (c) is generated using curve tracing from
MNIST (LeCun et al., 2010), while (d) is generated using SAM (Kirillov et al., 2023) and curve
tracing sequentially.
E.2 Raster Images to SVG Conversion
One of the most fundamental pieces of information for visual perception is object shape. Our
method can be conceptualized as selectively diminishing details from an image, prioritizing the
extraction of less significant shapes. This guided process of reduction offers a quantitative way to
manage the amount of visual data present within an image. Within this framework, we perceive
segmentation as an example of extreme simplification, whereas vectorization serves as a more
moderate form of the same. Here we introduce how we use such two approaches to convert the
raster images to SVG.
Image Vectorization. The vector tracing algorithm operates in a sequential three-step process.
Initially, pixels are transformed into a defined path. Subsequently, this path is condensed into
a simplified polygonal representation. Lastly, the polygon is refined and approximated using a
curve-fitting (tracing) technique, which enhances its smoothness.
There are several online tools to convert the raster images (jpg and png) into vector graphics (SVG),
such as Adobe Illustrator (Adobe Inc., 2019), Inkscape (Inkscape Project, 2020), and VTracer (VTracer,
2020). We experiment with all of them and found that VTracer leads to the best balance between
SVG complexity (code length) and rich semantic representation.
In MNIST (LeCun et al., 2010), we use the default hyperparameters during conversion. Specifically,
we (i) first binarize the MNIST pixel value from the continuous range [0, 255] to the binary set
{0, 255}using the threshold 127.5, (ii) set the foreground to black, and the background to white,
and (iii) apply the vector tracing algorithm VTracer.
Segmentation Prior. As mentioned earlier, segmentation can provide a strong prior for object
shape. We want a generalist model that can segment any image, i.e., not trained and thus biased
2https://github.com/visioncortex/vtracer
20

--- PAGE 21 ---
towards a certain dataset. The Segment Anything (SA) (Kirillov et al., 2023) project introduces
such an image segmentation model, the Segment Anything Model (SAM), and a large-scale dataset,
SA-1B, with the aim of achieving powerful generalization and zero-shot transfer across diverse
segmentation tasks, demonstrating competitive results often surpassing prior fully supervised
methods. We use the default hyper-parameters of SAM to segment the whole image into a set of
masks without class labels, where the color of each mask is represented by the average value of
the pixels within the mask. Specifically, we sample 32 query points per side (1024 points overall)
to generate the image mask. Then we select the top 20 masks with the highest area as the final
representation for an image.
We then use VTracer to transform the mask into SVG format. Note that, to reduce the final SVG, we
adjust several settings: we set the number of significant bits to use in an RGB channel to 0; we set
the minimum angle displacement degree to splice a spline to 90; we set the color difference between
gradient layers to be 35; we consider a corner to have a minimum momentary angle of 0 degrees;
we discard patches smaller than 16 pixels in size; and we perform iterative subdivide smoothing
until all segments are shorter than 10 pixels.
E.3 Fine-tuning Dataset for Vicuna
We use the same JSON format in Vicuna (Vicuna, 2023) to construct the fine-tuning dataset. We
use all the training samples in MNIST, translating to 60,000 SVG images. For each sample, we
construct one round of conversation: (i) From human: ‘‘Which digit does the following SVG
reflect? <SVG code here>’’ , and (ii) From GPT: ‘‘<label>’’ . Here <label> denotes the digit
label, which ranges from 0 to 9. Then we use this dataset to fine-tune Vicuna using the default
hyper-parameters3for 3 epochs.
E.4 Prompt Engineering
In this section, we provide the details of prompt engineering for each task. The prompt is designed
to figure out the common transformation in the SVG example pairs first (each example pair consists
of a query and a key) and then transform the query into the corresponding key SVG by following
the discovered common transformation.
In-context Image Classification. In this task, in-context examples are aimed to provide
more context information using several image-label pairs, thus facilitating the final classifi-
cation. The specific prompt utilized for this purpose using 3 in-context examples is detailed
below: ‘‘Instruction: please predict the digit number for each of the following
SVG images. Please think step by step, and closely look at the key identifying
image characteristics. Please just tell me the image class, no other information
is needed. Q: What digit does this SVG image represent? <SVG code here> A: This
SVG image represents digit <label> Q: What digit does this SVG image represent?
<SVG code here> A: This SVG image represents digit <label> Q: What digit does
this SVG image represent? <SVG code here> A: This SVG image represents digit
<label> Q: What digit does this SVG image represent? <SVG code here> A: This SVG
image represents digit .
Synthetic Data Study: In this task, the objective is to conduct an analytical evaluation of the
provided two example pairs, examining changes that occur in aspects such as color, shape,
3https://github.com/lm-sys/FastChat
21

--- PAGE 22 ---
and size. The insight gathered from this analysis will then be used to adapt the given query
into its corresponding key. The specific prompt utilized for this purpose is detailed below:
‘‘Please perform the following task carefully. In this task, you will be shown
two examples of Scalable Vector Graphics (SVG) code pairs. Each pair consists
of a query and key pair, where the query describes the SVG code of the original
image, and the key describes the SVG code of the transformed image. Each will be
named ‘‘Example Query #" and ‘‘Example Key #" respectively. Your first task is
to figure out the common transformation in the two examples. The transformation
can consist of color, shape, size, or any combination thereof. Then, given a
new test query SVG code (named \Test Query"), your second task is to transform
that query into the corresponding key SVG code (named \Test Key"), following the
common transformation that you discovered in the two example pairs. Here are the
two example query and key pairs: Example Query 1: <SVG code here>; Example Key
1:<SVG code here>; Example Query 2:<SVG code here>; Example Key 2:<SVG code here>;
Here are the test query and key pair: Test Query:<SVG code here>; Test Key:’’
Content Extrapolation: In this task, LLMs are presented with SVG code pairs, each con-
taining a query-key set that depicts numbers. The key introduces a consistent mathematical
operation (addition, subtraction, multiplication, or division) to the query number. The tasks
are to identify this operation in the examples and apply it to new test queries to generate
corresponding test keys. To facilitate a more comprehensive understanding of SVG number
codes for the LLM, we initially present the SVG codes for numbers 0 through 9 to the LLM
prior to posing specific queries. The specific prompt utilized for this purpose is detailed below:
‘‘Please perform the following task carefully. In this task, you will be shown
two examples of Scalable Vector Graphics (SVG) code pairs. Each pair consists of
a query and key pair, where the query describes an SVG code of an integer number,
and the key describes the SVG code of another integer number with an introduced
mathematical operation. Each will be named \Example Query #" and \Example Key
#" respectively. In addition to the example pairs, you will be shown a new test
query SVG code (named \Test Query"). Your first task is to identify which number
each example query, example key, and test query represents. Your second task
is to figure out all the possible mathematical operations that are held for all
given example pairs. The operation could be add, subtract, multiply, and divide
(the subtract or multiply factor could be a fraction). Then, according to the
numbers of example pairs and test query you identified, your third task is to
predict the corresponding test key number (named \Test Key"), following all the
mathematical operations that you discovered in the given example pairs. Finally,
you need to generate the corresponding SVG code of the test key number. Here are
the two example query and key pairs: Example Query 1: <SVG code here>; Example
Key 1:<SVG code here>; Example Query 2:<SVG code here>; Example Key 2:<SVG code
here>; Here are the test query and key pair: Test Query: <SVG code here>; Test
Key: (Note: think about four operations one by one, and the operation should be
consistent for all given example pairs)’’
F Prompt Engineering for SVG QA Dataset Curation
We use the following prompt to curate the SVG QA pairs by leveraging GPT-4V:
Generate a JSON object containing a quiz question based on an image derived from
an SVG file, such as icons or emojis. The image filename is also provided to
22

--- PAGE 23 ---
help you better curate the question, but note that this filename is not leaked
to the observer. If the filename does not clearly correspond to the image, just
discard that filename, never over-rely on the filename. The question should be
designed to test the observer’s perception to the image by making the correct
answer evident only upon seeing the image. Include four answer options, ensuring
that the correct answer is straightforward to identify for someone who actually
view this image. The question should relate to the image’s category, distinctive
features, or its usuage. Provide the JSON structure with fields for the question,
the four options (labeled A, B, C, D), and the correct answer indicated. Below
are two examples of how to structure the question and answers within the JSON
format.
{"question": "Which category does this SVG icon best represent?", "options":
{"A": "Technology", "B": "Nature", "C": "Sports", "D": "Food" },
"correct answer": "A" }
{"question": "The SVG icon does not use which of the following geometric
shapes?", "options": {"A": "Circles", "B": "Squares", "C": "Triangles", "D":
"Hexagons" }, "correct answer": "D" }
Given this image and its filename file path, your JSON:
G Limitations (Extended)
Our focus was to demonstrate whether LLMs can understand images, and we used the SVG
representation as a bridge to enable our studies. If one were to develop an approach out of this
pipeline, then there are several limitations. One major limitation of SVG representation is the loss of
fine details: Though converting raster images into SVG format and leveraging XML-based textual
descriptions allows for efficient processing of crisp graphics and designs, it is not as effective in
handling photographic content. As a result, fine-grained details, such as image textures, may be
lost during conversion.
Conversely, when the SVG code incorporates an excessive level of detail, its sequence length can
become prohibitively long, which can pose challenges for the training and inference of current
Transformer-based LLMs. Developing hybrid representations that can retain the advantages of
both discrete and continuous data, while preserving finer details, is a potential area for future
exploration. For example, in LLMs, the processing unit is the token, which can correspond to one or
several words. However, in SVG, we would prefer to have a specific embedding module for each
geometric primitive in SVG, such as circles, polygons, and so on.
Furthermore, we empirically observed that LLMs can not handle low-level image manipulation
tasks such as rotating the overall image by a certain angle and scaling it by a ratio. For example,
we prompt GPT4 10 SVG images to conduct the following tasks: (1) enlarge the width and height
by one time, (2) shrink the width and height to half, (3) rotate clock-close by 90 degrees, (4) rotate
90 degrees. Results indicate that none of the trials succeeded. Conducting such low-level image
manipulation tasks needs to update the majority content of SVG code, where current LLMs are not
capable of handling. Additionally, our empirical tests highlighted certain areas where LLMs fall
short, particularly in handling low-level image manipulation tasks. For instance, when prompted
to manipulate SVG images in tasks like enlarging dimensions, shrinking dimensions, or rotations,
LLMs like GPT-4 displayed inadequate proficiency. Such operations, which mandate considerable
updates to the SVG code, currently lie outside the proficiency range of these models.
23

--- PAGE 24 ---
In summary, while LLMs do present limitations, it offers promising initial results for the integration
of LLMs and SVG for visual tasks. Addressing these limitations could lead to more powerful
image representation algorithms and pave the way for more versatile and comprehensive artificial
intelligence systems.
24
