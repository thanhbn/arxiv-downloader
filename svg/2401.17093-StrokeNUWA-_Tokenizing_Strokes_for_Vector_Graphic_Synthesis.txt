# 2401.17093.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/svg/2401.17093.pdf
# File size: 3490055 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis
Zecheng Tang* 1 2Chenfei Wu* 2Zekai Zhang2Mingheng Ni2Shengming Yin2Yu Liu2Zhengyuan Yang3
Lijuan Wang3Zicheng Liu3Juntao Li1Nan Duan2
Abstract
To leverage LLMs for visual synthesis, traditional
methods convert raster image information into
discrete grid tokens through specialized visual
modules, while disrupting the model‚Äôs ability to
capture the true semantic representation of visual
scenes. This paper posits that an alternative rep-
resentation of images, vector graphics, can ef-
fectively surmount this limitation by enabling a
more natural and semantically coherent segmen-
tation of the image information. Thus, we intro-
duce StrokeNUWA, a pioneering work exploring
a better visual representation ‚Äî ‚Äústroke tokens‚Äù
on vector graphics, which is inherently visual se-
mantics rich, naturally compatible with LLMs,
and highly compressed. Equipped with stroke to-
kens, StrokeNUWA can significantly surpass tra-
ditional LLM-based and optimization-based meth-
ods across various metrics in the vector graphic
generation task. Besides, StrokeNUWA achieves
up to a 94√óspeedup in inference over the speed
of prior methods with an exceptional SVG code
compression ratio of 6.9%.
1. Introduction
In recent years, Large transformer-based Language Models,
commonly referred as LLMs, have made significant strides,
particularly in the domain of Natural Language Process-
ing (NLP) (Brown et al., 2020; Chowdhery et al., 2022;
Touvron et al., 2023; Anil et al., 2023). Concurrently, LLMs
are gradually expanding their capabilities to other modali-
ties, such as audio (Ghosal et al., 2023), medical (Singhal
et al., 2023) and robotics (Brohan et al., 2023).
Current methodologies (Reddy et al., 2021; Wu et al., 2022;
Chang et al., 2022; Kondratyuk et al., 2023) enable LLMs to
generate visual information by transforming the continuous
visual pixels into to discrete grid tokens via specialized vi-
*Equal contribution, during Zecheng‚Äôs internship under the men-
torship of Chenfei at MSRA1Soochow University2Microsoft Re-
search Asia3Microsoft Azure AI. Correspondence to: Nan Duan
<nanduan@microsoft.com >.
21
(2)(1)(3)(4)
‚Ä¶‚Ä¶
‚Ä¶Grid Tokens:
StrokeTokens:(1)(2)(3)3456789
VQ-StrokeVQ-GAN‚Ä¶
Large Language Model
Large Language ModelRaster Image
Vector Graphic
<svgviewBox="0.0 0.0 200.0 200.0‚Äùheight-="200px"width="200px"> <pathfill=‚Äùred"fill-opacity="1.0"filling="0"d="M4.0 104.0 ‚Ä¶Visual Tokenizer
Code TokenizerFigure 1. Comparison between the visual representation of ‚Äúgrid‚Äù
token and our proposed ‚Äústroke‚Äù token. Instead of tokenizing
pixels from raster images, we explore a novel visual representation
by tokenizing codes, from another image format‚ÄîScalable Vector
Graphic (SVG). ‚ÄúStroke‚Äù tokens have the following advantages:
(1) inherently contain visual semantics, (2) naturally compatible
with LLMs, and (3) highly compressed.
sual modules such as VQ-V AE (Van Den Oord et al., 2017)
and VQ-GAN (Esser et al., 2021). Subsequently, these trans-
formed grid tokens are processed by the LLM in a manner
akin to textual word handling, which facilitates LLMs‚Äô gen-
erative modeling process. However, when compared with
diffusion models (Rombach et al., 2022), LLMs still fall be-
hind (Lee et al., 2022; Sun et al., 2023). The shortcomings
of LLMs in visual tasks primarily arise from two reasons:
First, the transformation process relies on specific visual
modules, which inherently possess limitations. For instance,
advanced visual modules like VQ-GAN (Esser et al., 2021)
can lead to the generation of images with artifact (Yu et al.,
2023); Second, the use of grid tokens can disrupt the vi-
sual semantics, as the grids are artificially designed and
not inherently semantic-aware. This artificial discretization
imposes constraints on the model‚Äôs ability to capture the
true semantic representation of visual scenes.
Is there a visual representation that preserves the semantic
integrity of visual information while being conducive to pro-
1arXiv:2401.17093v1  [cs.CV]  30 Jan 2024

--- PAGE 2 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
Short Hair Woman           Pizza              Rain, Weather        Frozen, Snow          Quick bread              Volleyball 
Fireworks             Seashell                Sneakers           Owl, Wildlife     Furniture, Chair    Clock, NightMicrochip           Strawberry            Dollar, Coin            Horse Knight    Safety, Productivity          Bird, Fly   Robot                 Light Bulb                 Diamond               Emotion, Joy         Cycling, Sport           Car, Vehicle
Figure 2. SVG generated by StrokeNUWA. For each image, we provide partial keywords for clarity.
cessing by LLMs? Finding such a representation within the
framework of grid tokens is non-trivial, as the arrangement
of grid tokens is typically regular and uniform, whereas the
semantic structure within images is often irregular and com-
plex. As illustrated in Fig. 1, the dolphin‚Äôs body is arbitrarily
segmented into different grid tokens. Although there have
been efforts to improve the VQ-V AE method (Esser et al.,
2021; Yu et al., 2023), enhancing the visual representation
quality, they are fundamentally constrained by the limita-
tions inherent to raster image formats, leading to bottlenecks
in semantic preservation. In light of these challenges, we
propose a novel approach that fundamentally retains the se-
mantic concepts of images by utilizing an alternative image
format: vector graphics. Different from pixel-based formats,
vector graphics intrinsically reveal the construction of ob-
jects, naturally encapsulating the semantic concepts of the
image. For example, our proposed ‚Äústroke‚Äù tokens segment
the dolphin into sequentially connected strokes, where each
stroke unit contains complete semantic information, such as
the dolphin‚Äôs fin (stroke ‚ë†) and back (stroke ‚ë°).
It is worth mentioning that our intention is not to claim that
vector graphics are superior to raster images, but rather tointroduce a fresh perspective on visual representation. The
advantages of our ‚Äústroke‚Äù token concept include: (1) In-
herently contains visual semantics: each stroke token intrin-
sically contains visual semantics, offering a more intuitive
semantic segmentation of the image content; (2) Naturally
compatible with LLMs: the creation process of vector graph-
ics is naturally sequential and interconnected, which mirrors
the way LLMs process information. In other words, Each
stroke is created in relation to the ones before and after it,
establishing a contiguous and coherent sequence that LLMs
can process more naturally; (3) Highly compressed: strokes
in vector graphics can be highly compressed, allowing each
stroke token to encapsulate a rich, compressed representa-
tion of the visual information, significantly reducing the
data size while maintaining quality and semantic integrity.
Based on the above analysis, we introduce StrokeNUWA,
a model that crafts vector graphics without the reliance on
the visual module. StrokeNUWA consists of a VQ-Stroke
module and an Encoder-Decoder model. The VQ-Stroke,
based on the residual quantizer model architecture (Mar-
tinez et al., 2014), can compress serialized vector graphic
information into several SVG tokens. The Encoder-Decoder
2

--- PAGE 3 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
model primarily utilizes the capabilities of a pre-trained
LLM to generate SVG tokens guided by text prompts.
We compare StrokeNUWA with optimization-based meth-
ods in the text-guided Scalable Vector Graphic (SVG) gener-
ation task. Our approach achieves higher CLIPScore (Hessel
et al., 2021) metrics, suggesting that utilizing stroke tokens
can yield content with richer visual semantics. When bench-
marked against LLM-based baselines, our method surpasses
them across all metrics, indicating that stroke tokens can
integrate effectively with LLMs. Finally, due to the com-
pression capabilities inherent in vector graphics, our model
demonstrates significant efficiency in generation, achieving
speed improvements of up to 94 times.
In a nutshell, our contributions can be outlined as follows:
‚Ä¢We introduce StrokeNUWA, the pioneering study ex-
ploring a better visual representation‚Äîstroke token,
to synthesize vector graphics solely through LLMs
without relying on specialized visual modules.
‚Ä¢We propose VQ-Stroke, a specialized Vector Quantized
Variational Autoencoder (VQ-V AE) designed to com-
press vector graphics into stroke tokens, providing an
exceptional compression ratio of 6.9%.
‚Ä¢We conduct detailed experiments that demonstrate the
significant potential of stroke tokens in the text-guided
vector graphic synthesis task.
2. Related Work
2.1. Visual Representation
In the realm of computer graphics, two predominant
image formats prevail: raster images, characterized by
pixel matrices; and vector images, a.k.a, Scalable Vector
Graphic (SVG), characterized by a series of code language
commands (Zhang et al., 2023). Recent developments in
visual synthesis have predominantly centered on the gener-
ation of raster images. The basic idea is to transform the
continuous image pixels into discrete grid tokens via special-
ized visual modules such as VQ-V AE (Van Den Oord et al.,
2017) and VQ-GAN (Esser et al., 2021), and then leverage
LLMs to generate these tokens (Reddy et al., 2021; Wu
et al., 2022; Kondratyuk et al., 2023). Most recently, some
works have tried to improve ‚Äúgrid‚Äù tokens by designing ad-
vanced architectures such as Lookup-Free Quantization (Yu
et al., 2023) and Efficient VQ-GAN (Cao et al., 2023). How-
ever, these ‚Äúgrid‚Äù token representations can disrupt visual
semantics as the grids are artificially designed, which lacks
inherent semantic awareness, and are easily subject to the
visual module‚Äôs intrinsic limitations like disturbances and
tampering (Hu et al., 2023). Conversely, our study is a pi-
oneering effort exploring a better visual representation by
proposing the concept of the ‚Äústroke‚Äù token. Different from
the ‚Äògrid‚Äù tokens, the ‚Äústroke‚Äù token is inherently definedName Symbol Argument Example
Move
ToM(x0,y0)
(x1,y1)
Line
ToL(x0,y0)
(x1,y1)
Cubic
B¬¥ezierC(x0,y0)
(x1,y1)
(cx
0,cy
0)
(cx
1,cy
1)
Table 1. Overview of basic SVG commands, including M,L, andC,
where each command contains one beginning point (x0, y0)and
one end point (x1, y1). For Cubic B ¬¥ezier command, it contains
two extra control points (cx
0, cy
0)and(cx
1, cy
1).
by contextually associated coded language commands that
offer strong semantic integrity, potentially mitigating the
aforementioned issues.
2.2. SVG Generation
SVG generation employs a method of structured code gen-
eration for producing graphics, which offers better inter-
pretability, flexibility, and scalability in image representa-
tion. The current mainstream approach of SVG generation
is optimization-based methods (Su et al., 2023; Jain et al.,
2023; Xing et al., 2023), which share a similarity with tradi-
tional raster image generation, involving iteratively refining
randomly initialized SVG paths to fit a target raster image
with a differentiable rasterizer (Li et al., 2020). However, the
optimization process is both time-consuming and computa-
tionally intensive, e.g., creating an SVG graphic comprised
of 24 SVG paths can exceed 20 minutes1. Alternatively,
some recent approaches have begun to adopt auto-regressive
models to directly generate code for SVG synthesis (Wang
et al., 2022; Wu et al., 2023a). However, due to the inherent
extensive length nature of SVGs and a lack of effective SVG
representation, these methods constrain LLMs to generate
complex SVGs. To address these challenges, we introduce
VQ-Stroke and present the concept of ‚Äústroke‚Äù tokens. By
transforming SVGs into stroke tokens, our approach en-
ables LLMs to produce intricate SVGs with significantly
improved inference speed.
3. Methodology
3.1. Problem Formulation
SVG code provides a suite of command and syntax
rules, e.g., the ‚Äú <rect> ‚Äù command defines a rectangle
shape with its position, width, and height, which can
1We test with LIVE (Ma et al., 2022) and VectorFusion (Jain
et al., 2023) on one NVIDIA V100 GPU.
3

--- PAGE 4 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
be written as <rect x="10" y="20" width="50"
height="80"/> . However, considering the multitude
of SVG command types, creating such a system not only
requires a complex data structure, but without a massive
dataset, LLMs would struggle to model the diverse range of
commands effectively. Therefore, as shown in Tab. 1, we
can simplify each SVG using just three basic commands:
‚ÄúMove To‚Äù, ‚ÄúLine To‚Äù, and ‚ÄúCubic B ¬¥ezier‚Äù by following
Iconshop (Wu et al., 2023a) and DeepSVG (Carlier et al.,
2020). For instance, intricate commands like ‚Äú <rect> ‚Äù
can be constructed by those three basic commands. Af-
ter simplification, an SVG G={Pi}N
i=1can be described
withNSVG paths, with each SVG path Piconsists of Mi
basic commands: Pi={Cj
i}Mi
j=1, where Cj
iis the j-th com-
mand in the i-th path. Eventually, each basic command
C= (T,V)is consist of command type T‚àà {M,L,C}, and
the corresponding position argument V.
3.2. StrokeNUWA
StrokeNUWA contains three core components: a Vector
Quantized-Stroke (VQ-Stroke) for SVG compression, an
Encoder-Decoder-based LLM (EDM) for SVG generation,
and an SVG Fixer (SF) for post-processing. Firstly, VQ-
Stroke compresses the SVG into stroke tokens, which en-
ables a transformation between the SVG code and the dis-
crete stroke tokens. Then, EDM utilizes the stroke tokens
produced from VQ-Stroke to generate SVG code. Finally,
SF is a post-processing module designed to refine the qual-
ity of the generated SVGs, given that the output generated
from the EDM or VQ-Stroke may not always conform to
the stringent syntactical rules of SVG code. Below, we will
introduce the details of each component.
3.2.1. V ECTOR QUANTIZED -STROKE
VQ-Stroke encompasses two main stages: ‚ÄúCode to Ma-
trix‚Äù stage that transforms SVG code into the matrix format
suitable for model input, and ‚ÄúMatrix to Token‚Äù stage that
transforms the matrix data into stroke tokens.
Code to Matrix As depicted in Fig. 3, we first transform
the simplified SVG code (Sec. 3.1) into SVG matrix format
by converting each basic command Cj
ito the individual
vector Kj
i‚ààR9with rules f:
Kj
i=f(Cj
i) = (T, x 0, y0, cx
0, cy
0, cx
1, cy
1, x1, y1)j
i,(1)
where Tdenotes the basic command type, (x0, y0)and
(x1, y1)represent the beginning and the end points, with
(cx
0, cy
0)and(cx
1, cy
1)as the control points of each basic com-
mand. Then, to establish interconnections among the ad-
jacent commands, we set the end point of j-th command
(x1, y1)j
iequal to the beginning point (x0, y0)j+1
iof the
subsequent (j+ 1) -th command in each individual path.
Raw SVG Code<svgviewBox=‚Äú0.0 0.0 200.0 200.0‚Äùheight=‚Äú200px‚Äùwidth=‚Äú200px‚Äù> <pathfill=‚Äúblack‚Äùfill-opacity=‚Äú1.0‚Äùfilling=‚Äú0‚Äùd=‚ÄúM4.0 54.0 L60.0 130.0 C54.0 92.0 25.0 28.0 97.0 93.0 ‚Ä¶ L4.0104.0></path>‚Ä¶<pathfill=‚Äùblack‚Äú fill-opacity=‚Äù1.0‚Äú filling=‚Äù0‚Äú d=‚ÄùM101.0 45.0 L99.0 49.0 ‚Ä¶‚Äú></path></svg>SVG MatrixDiffVG& RULEsDown-SampleBlocksCompressNormalization‚Ñì!"#$%&'()=‚Ñì*'++,%+)-%	+‚Ñì*'/)0''(	+‚Ñì1)2'-3%&42%,'-Stroke Codebook
M Command0000000454145400006013024549225121289793
‚Ä¶‚Ä¶‚Ä¶18272 00003390L CommandC CommandL Command‚Ä¶Construction ProcessUp-SampleBlocksDe-Normalization‚Ä¶Compressed Latent !‚Ä¶De-Compress‚Ä¶Stroke Tokens !"LookupQuantizeVQ-StrokeFigure 3. Overview of VQ-Stroke.
We then decompose all the paths within the SVG Ginto
distinct basic commands and combine their corresponding
vectors into a matrix form:
f(G) = (f(Pi))N
i=1=
f(Cj
i)Mi
j=1N
i=1
=Ô£´
Ô£¨Ô£≠(K1
1;K2
1;¬∑¬∑¬∑;KM1
1)
............
(K1
N;K2
N;¬∑¬∑¬∑;KMN
N)Ô£∂
Ô£∑Ô£∏,(2)
where ‚Äú;‚Äù denotes the stack operation, and each matrix row
represents an individual command. Thus, we can obtain a
structured SVG matrix f(G)‚ààR(PN
i=1Mi)√ó9to represent
an SVG that containsPN
i=1Miindividual basic commands.
Matrix to Stroke After obtaining the SVG matrix f(G),
we aim to compress the matrix into discrete stroke tokens
via latent representation, with which one can reconstruct
thef(G). As shown in Fig. 3, the VQ-Stroke model is
composed of Down-Sample blocks, a Stroke Codebook B,
and Up-Sample blocks. The SVG matrix f(G)is first en-
coded by the Down-Sample blocks to obtain the compressed
representations, which entails increasing the number of rep-
resentation channels (column of f(G)) while concurrently
compressing the spatial dimensions (row of f(G)) to yield a
more compact representation, i.e. compressing the number
of commands into Ts.t.T <PN
i=1Mi. Then, the Code-
bookBsimultaneously conducts dlevels of compression
with residual vector quantization (Martinez et al., 2014),
enabling VQ-Stroke to better model the compressed rep-
4

--- PAGE 5 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
4√ó4Conv1d, Stride=2
2√ó2Conv1d, Stride=1‚Ñù("/$)√ó'3√ó3Conv1d, Stride=13√ó3Conv1d, Stride=1‚Ñù("/$)√ó'‚Ñù("/$)√ó'Input: ‚Ñù"√ó'√óùëÅ4√ó4ConvTranspose1d, Stride=2
2√ó2Conv1d, Stride=1‚Ñù$"√ó'3√ó3Conv1d, Stride=13√ó3Conv1d, Stride=1‚Ñù$"√ó'‚Ñù$"√ó'Input: ‚Ñù"√ó'√óùëÅ
Down-Sample BlocksUp-Sample BlocksResNet1dResNet1d
Figure 4. Architecture of Down-Sample and Up-Sample Blocks.
resentations. We depict the detailed architecture of Down-
Sample blocks and Up-Sample blocks in Fig 4, wherein both
blocks first utilize a Conv1d or ConvTranspose1d model to
compress or expand the features, succeeded by a ResNet1d
module and an additional Conv1d module for feature ex-
traction. It is worth mentioning that a low compression rate
allows the VQ-Stroke to learn the fine details of SVGs (the
first and second columns), while more aggressive compres-
sion (the third column) enables the VQ-Stroke to capture
the overall contours of the SVGs, As illustrated in Fig.5,
a low compression rate allows the VQ-Stroke to learn the
fine details of SVGs (the first and second columns), while
more aggressive compression (the third column) enables
the VQ-Stroke to capture the overall contours of the SVGs.
We have more discussion in Sec. 4.2. Finally, the Down-
Sample blocks reconstruct the SVG latent representation
output from the Codebook B.
To train such a network, we follow Dhariwal et al. to calcu-
late the commitment loss, codebook loss, and reconstruction
loss to jointly update the VQ-Stroke in Equ. 3:
‚ÑìV Q‚àíStroke =Œ±(‚Ñìcodebook +‚Ñìcommit ) +‚Ñìrecon
=Œ±
|| Z ‚àí sg[ÀúZ]||2
2+||sg[Z]‚àíÀúZ ||2
2
+ MSE(]f(G), f(G)),(3)
where Œ±is the hyper-parameter, Zis the compressed latent
output from down-sample blocks, ÀúZis the latent looked
up from codebook B, and sg[¬∑]is the gradient clipping op-
eration. Besides, we pre-normalize the input data into the
[‚àí1,1]range to stabilize the training process.
3.2.2. E NCODER -DECODER -BASED LLM
We employ an Encoder-Decoder LLM (EDM) to predict
the stroke tokens obtained from the codebook B. Con-
sidering LLM‚Äôs inherent textual instruction capability, we
freeze the EDM encoder to leverage its inherited textual
knowledge. Subsequently, we fine-tune the EDM decoder
to learn the stroke token prediction task. Due to the dis-
crepancy between the vocabulary of stroke tokens and the
Golden 
SVGCompression
Rate = 2 (PC)Compression
Rate = 2 (PI)Compression
Rate = 4 (PI)SVG
Details
Stroke Token
Number
Figure 5. Analysis of SVG reconstruction, where Cis a constant
representing the number of inserted <M> command in PI setting.
To facilitate clear observation of the SVG composition, we repre-
sent each basic command with a distinct color.
original LLM‚Äôs vocabulary, we extend EDM with an ad-
ditional stroke embedding layer and a stroke predictor.
Consequently, given the trainable model parameters Œ∏and
the textual prompt K, we maximize the log probability
argmaxŒ∏QT
i=1P(ti|t<i,K)with the cross-entropy loss.
3.2.3. SVG F IXER
A critical issue arises in the generation results from both
SDM and EDM, as they fail to guarantee Equ. 1 due to the
discrepancies of the interconnection points among adjacent
commands in each individual SVG path, i.e., (x1, y1)j
iÃ∏=
(x0, y0)j+1
iini-th path. To address this issue, we intro-
duce the SVG Fixer (SF) as a post-processing module for
the generated results. It encompasses two strategies: Path
Clipping (PC) and Path Interpolation (PI). Specifically, PC
involves the direct substitution of each SVG command‚Äôs be-
ginning point with the endpoint of adjacent SVG commands:
(x0, y0)j+1
i:= (x1, y1)j
i. On the other hand, PI entails the
addition of Mcommands between each pair of adjacent, yet
non-interconnected SVG commands to bridge the discrep-
ancy, i.e., if (x1, y1)j
iÃ∏= (x0, y0)j+1
i=‚áíadding an extra
command
M,(x1, y1)j
i,0,0,0,0,(x0, y0)j+1
i
to force the
previous command‚Äôs end point to move to the beginning
point of the next adjacent command. As shown in Fig. 5,
PC can streamline the overall paths of SVGs, making them
more succinct, but may lead to some inaccuracies in the
details. On the other hand, PI tends to reveal more gener-
ated stokes‚Äô details, but it may introduce more curves. Each
strategy has its own applicable scenarios.
5

--- PAGE 6 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
4. Experiment
4.1. Experimental Settings
Dataset We construct the training and evaluation data
with FIGR-8-SVG dataset (Clou ÀÜatre & Demers, 2019),
which consists of massive monochromatic (black-and-white)
SVG icons. We pre-process the SVG data by transform-
ing each SVG sample into standardized representations,
eliminating the redundant SVG paths, dropping the outer
black box, and filtering the data by applying a thresh-
old of 1,024 basic commands in length. We filter the in-
stance with less than two annotated discrete keywords and
apply a template ‚Äú Generating SVG according to
keywords: {¬∑¬∑¬∑} ‚Äù to build the text prompt. After pre-
processing, we sample 2,000 instances with varying SVG
code lengths as a testing set, 8,000 samples for validation,
and apply the remaining 740K samples for training.
Evaluation Metrics We evaluate the quality of the gener-
ated SVG of VQ-Stroke and StrokeNUWA from various as-
pects. For VQ-Stroke, we primarily consider the reconstruc-
tion quality and the compression effectiveness. We evaluate
the reconstruction quality with the Fr ¬¥echet Inception Dis-
tance (FID)2(Heusel et al., 2017) and the CLIPScore (Rad-
ford et al., 2021). Given that the generated SVG graph-
ics consist solely of lines, we set the background color to
white to mitigate the potential biases for FID and CLIPScore
brought by the background (Wu et al., 2023a). Addition-
ally, we calculate the Edit Score (EDIT) between the recon-
structed SVG code and the Golden SVG code to reflect the
fidelity of the reconstructed SVG graphics in replicating fine
details. To reflect the practical compression effectiveness of
VQ-Stroke, we calculate the Compression Ratio (CR) score
between the tokenized SVG code and the stroke tokens, i.e.,
CR = Len(tokenized SVG code) /Len(stroke tokens) .
For StokeNUWA, apart from utilizing the metrics mentioned
above, we supplement evaluation with Human Preference
Score (HPS) (Wu et al., 2023b) and Recall Score3to re-
flect the quality of the generated SVG graphics and their
degree of overlap with the Golden SVG Code. Additionally,
we also report the time required to generate each SVG and
conduct the qualitative evaluation.
Tasks and Baselines We evaluate VQ-Stroke and
SVGNUWA with the SVG reconstruction and the text-
guided SVG generation tasks, respectively. For VQ-Stroke,
considering the absence of works in the field of SVG repre-
sentation, we focus on comparing the performance of two
SF methods, i.e., PI and PC. Additionally, we evaluate the re-
construction performance of two different compression rates,
2Specifically, we obtain the image features of rendered SVG
graphics with the CLIP image encoder (Radford et al., 2021)
3We convert all SVGs into the stroke token format, subse-
quently computing the recall rate.
CR-2PICR-2PCCR-4PIGTComplexityEasyHardMore SVG PathsLess SVG PathsFigure 6. Reconstruction cases generated by VQ-Stroke, diffi-
culty (reflected by path numbers) increases from left to right.
i.e., compression rates of 2 and 4. For SVGNUWA, we com-
pare with the optimization-based methods, including Vector
Fusion (Jain et al., 2023) and the Stable Diffusion (Rom-
bach et al., 2022) combined with LIVE method (Li et al.,
2020). Given that optimization-based methods are notably
time-intensive, i.e., requiring more than 20 minutes to gener-
ate a single SVG on one NVIDIA V100 GPU, we randomly
sample 500 instances from the testing set for evaluation to
ensure a feasible timeframe. Additionally, we also compare
with the LLM-based method Iconshop (Wu et al., 2023a).
We re-implement Iconshop with the same Flan-T5 back-
bone as in StrokeNUWA and use a T5 tokenizer to encode
the numerical values built in Iconshop. Notably, the pri-
mary distinction between Iconshop and StrokeNUWA lies
in their approaches to handling visual representation. While
Iconshop directly treats SVG code as visual tokens, Stro-
keNUWA converts SVG code into stroke visual tokens with
VQ-Stroke. We set the maximum model length to 1,500 for
IconShop to ensure the completeness of the SVG code.
Implementation Details For VQ-Stroke, we set the depth
of the residual vector quantization dto 2, corresponding to
compression rates of 2 and 4. Then, we set the codebook
size| B | as 4096, with each code corresponding to a latent
representation of 512 dimensions. We set Œ±= 1in Equ. 3
during the training process. For EDM, we utilize the 3B
Flan-T5 model (Chung et al., 2022) as the backbone. We
utilize DeepSpeed Library (Rajbhandari et al., 2020) to
implement models on 64 NVIDIA V100 GPUs and set the
maximum model length as 512.
4.2. Quantitative Evaluation
VQ-Stroke We report the reconstruction quality of VQ-
Stroke in Tab. 3. without SF, VQ-Stroke fails to generate
results that conform to SVG syntax. After equipping VQ-
6

--- PAGE 7 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
MethodsVisual Performance SVG Code Quality Generation
Speed (‚Üì)
(perSVG) FID (‚Üì) CLIPScore ( ‚Üë) HPS ( ‚Üë)Recall ( ‚Üë)
(Stoke Token)EDIT (‚Üì)Optim / Pred
Length ( Avg)
SD & LIVE 14.236 12.908 11.210 0.028 - 160 (32 Path) ‚âà28.0min
VectorFusion 7.754 17.539 15.901 0.079 - 2,048 (128 Path) ‚âà30.0min
Iconshop 17.828 8.402 8.234 0.114 24,792.476 993.244 ‚âà63.743 sec
SVGNUWA (PC) 6.607 17.852 16.134 0.239 9,092.476 271.420 ‚âà19.128 sec
SVGNUWA (PI) 6.513 17.994 16.801 0.207 12,249.091 271.420 ‚âà19.128 sec
Table 2. Performance of StrokeNUWA, where ‚ÄúOptim/Pred Length‚Äù denotes the actual predicted or optimized number of paths.
Methods FID (‚Üì) CLIPScore ( ‚Üë) EDIT ( ‚Üì) CR (‚Üì)
SQM (C-2) - - 1,114.791 8.549%
SQM (C-2) + SF (PC) 3.751 19.861 1,096.313 8.786%
SQM (C-2) + SF (PI) 3.518 20.290 1,315.137 13.780%
SQM (C-4) + SF (PI) 4.943 17.192 2,100.671 6.890%
Golden SVG - - - 100%
Table 3. Performance of VQ-Stroke on SVG reconstruction task,
where C-2 and C-4 denote the Compression Rate 2 and 4.
2048/5124096/10244096/5128192/512More Precise DetailsPoorGood
Figure 7. Reconstruction performance of difference VQ-Strokes.
Stroke with SF, PI facilitates a more faithful approxima-
tion of the original SVG graphics by achieving the lowest
FID score and demonstrating a higher concordance with
the given text prompts, as evidenced by the lowest CLIP
score. In contrast, the PC method yields better alignment
results with the original SVG code as it achieves the lowest
EDIT score. Utilizing compression level 2 (C-4), VQ-Stroke
attains a notable Compression Ratio (CR) of 6.9%, main-
taining performance on par with that of C-2 as evidenced
by comparable CLIPScore and FID. This suggests that VQ-
Stroke preserves the semantic integrity of the original SVG
graphics despite the substantial path compression.
StrokeNUWA As illustrated in Table 2, StrokeNUWA
outperforms other methods by achieving superior results.
Specifically, in terms of visual performance, StrokeNUWA
is capable of generating graphics that more closely re-
semble the Golden SVG‚Äîevidenced by the lowest FID
score (6.513) and the highest HPS (16.801). This indicates
that our Stroke Tokens offer greater compatibility with the
LLMs than the vanilla approach (Iconshop). Moreover,
StrokeNUWA has attained the highest CLIPScore (17.994),
surpassing even Optimization-based methods. This suggests
that StrokeTokens encapsulates visual semantics effectively.
In terms of the quality of the SVG Code and the efficiency
of generation, the Stroke Token not only aligns closely with
Electrical Chip         Clock               Cloud            Family Car
Stable Diffusion (First Row) + LIVE (Second Row)IconshopStrokeNUWARaster ImageTime Cost:2 secondCLIPScore:25.266SVG by LIVETime Cost:28 minuteCLIPScore:23.113IconshopTime Cost:63.743 secondCLIPScore:21.834StrokeNUWATime Cost:19.128 secondCLIPScore:24.682Metrics(Per Sample)
(a) Comparison between StrokeNUWA and other baselines.
GPT-4-TurboGPT-4-TurboTime Cost:*CLIPScore:16.323
(b) Cases generated by GPT-4-Turbo with same keywords. As
GPT-4 is not open-source, we cannot get the generation time.
Figure 8. Sampled cases from different models in SVG generation
task, where the CLIPScore is the average score calculated across
four generated cases for each method.
the Golden standard but also markedly enhances the gener-
ation speed, i.e., around 19 seconds of StrokeNUWA V .S.
around 30 minutes of Optimization-based method LIVE.
This underscores the impressive compressive capabilities of
the Stroke token on the original SVG Code, demonstrating
both its efficiency and the quality of compression.
4.3. Qualitative Evaluation
Case Study We show the reconstruction results of VQ-
Stroke with varying complexity levels in Fig. 7 and present
a qualitative comparison between StrokeNUWA and other
baselines in Fig. 8(a). It is impressive that VQ-Stroke can
7

--- PAGE 8 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
Prompt 
Alignment
Overall 
Quality
Graphic
Details
Figure 9. Human evaluation between StrokeNUWA and LLM-
based method‚ÄîIconshop.
reconstruct complex SVGs with a limit of only 4,096 code-
book size. Then, at the Compression Rate of 2 (CR-2),
VQ-Stroke successfully outlines the edge of objects within
the graphics, demonstrating that stroke tokens can be highly
compressed with a dense representation and inherently in-
corporate semantic segmentation, which is essential for
retaining visual semantics. Regarding the comparison of
StrokeNUWA, we note that employing LLM-based gener-
ation methods can result in incomplete SVGs (Iconshop).
This is attributed to the excessive SVG code lengths and
LLMs struggling to capture the key information embedded
within SVG graphics. However, the use of stroke tokens
can mitigate these issues by compressing the paths and be-
ing compatible with LLMs. Furthermore, we find that the
performance of the optimization-based method heavily re-
lies on the outputs generated by the stable diffusion model,
which is subject to the limitations of grid tokens mentioned
in Sec. 1, e.g., it is hard to capture the visual semantics and
tends to generate extra visual information that is not aligned
with the text prompt. Besides, the optimization process is
extremely slow. In contrast, StrokeNUWA, which utilizes
stroke tokens, inherently contains visual semantic segmen-
tation. As a result, the content generated is more aligned
with the textual semantics, providing a more coherent and
semantically accurate graphic.
Human Evaluation Furthermore, we conduct a human
evaluation to compare the generated SVG outputs from Stro-
keNUWA with those produced by the LLM-based method,
Iconshop. We select 50 different textual prompts and guide
the model to generate corresponding SVGs for evaluation.
As depicted in Figure 9, our comparison is founded on three
criteria: Prompt Alignment (consistency between the gener-
ated result and the text prompt), Overall Quality (the gen-
eral caliber of SVGs), and Graphic Details (intricacies such
as curves). We observe that StrokeNUWA, compared to
Iconshop, which regards code as visual representation, not
only yields more complete content (better Overall Quality)
but produces results more closely aligned with the textual
prompts (better Prompt Alignment)4. Given that stroke
tokens compress the details of SVG, it is natural that Stro-
keNUWA excels in generating Graphic Details.
4The main reason for low Prompt Alignment in Iconshop is
also due to the incompleteness of the generated SVGs.SettingsFID (‚Üì) CLIPScore ( ‚Üë) EDIT ( ‚Üì)
| B | Dim
2048 512 5.702 19.365 2,323.810
4096 512 3.518 20.290 1,315.137
4096 1024 3.901 20.159 1,793.008
8192 512 2.639 21.014 907.106
Table 4. Comparison among different VQ-Stroke Settings.
5. Ablation Study
5.1. Analysis of VQ-Stroke Model Architecture
To investigate the impact of VQ-Stroke architecture config-
urations on the stroke token performance, we experiment
with different codebook sizes | B | and codebook dimension
Dim. As shown in Tab. 4, we can observe that by increas-
ing the codebook size while simultaneously reducing the
dimension of each stroke token, the VQ-Stroke achieves
superior performance across multiple metrics. We sample a
set of reconstruction cases to showcase the trend of changes
in Fig. 7, which indicates that, with a larger codebook size
and smaller dimension, the VQ-Stroke can delineate details
with greater accuracy, e.g., straighter lines.
5.2. Comparison with GPT-4
We compare the generation results with GPT-4 (Achiam
et al., 2023) by employing the following template to
guide GPT-4 in producing the corresponding SVG code:
Generate SVG codes in icon style based
on keywords: {KEYWORDS }. We show the rendered
SVGs in Fig. 8(b), where we can observe that GPT-4
can only generate simple SVGs, which is consistent with
LLM-based methods. Moreover, GPT-4 often yields SVGs
that are incongruent with the associated text.
6. Conclusion and Future Work
This paper presents StrokeNUWA, a pioneering study that
explores a superior visual representation‚Äî‚Äústroke‚Äù tokens,
as an alternative method for expressing images through
vector graphics. Stroke tokens not only preserve the se-
mantic integrity of the images but are also conducive to
processing by LLMs. Moreover, strokes in vector graph-
ics can be highly compressed. Experiments indicate that,
equipped with stroke tokens, LLMs can achieve superior
results across various metrics in the SVG synthesis task.
This paper showcases the tremendous potential of stroke
token representation in the field of vector graphic synthesis.
Moving forward, we aim to continue improving the quality
of stroke tokens through advanced visual tokenization meth-
ods tailored for LLMs. In addition, we intend to generalize
stroke token utilization to a broader range of tasks (SVG
Understanding), domains (3D), and the generation of SVGs
for images sourced from the real world.
8

--- PAGE 9 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
Impact Statement
The implications of this work are manifold, potentially rev-
olutionizing the visual synthesis from another format of
image, vector graphics. As stroke tokens refine the inter-
play between visual representation and LLMs, future ad-
vancements in visual tokenization techniques designed for
LLMs are anticipated. Moving forward, the community
can extend stroke token application into wider tasks and
domains, including SVG comprehension and open-domain
SVG synthesis for images from the real world. As we pi-
oneer this nascent field, we are conscious of the profound
societal impact that such advancements in machine learn-
ing and graphical representations hold. The capabilities for
automated graphic design, scalable vector graphics produc-
tion, and enhanced digital artistry foreshadow considerable
shifts in industries reliant on visual content. By forging
new pathways for artistic expression and visual communica-
tion, our work stands to not only contribute to the scientific
community but also to catalyze transformations in creative,
technological, and educational sectors. We recognize the
importance of our work and our responsibility to ensure
that our contributions to the field are conducted ethically,
aiming to benefit society as a whole, democratize the visual
landscape, and enrich it through responsible and judicious
innovation.
References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
arXiv:2303.08774 , 2023.
Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
Z., et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y ., Chen,
X., Choromanski, K., Ding, T., Driess, D., Dubey, A.,
Finn, C., et al. Rt-2: Vision-language-action models
transfer web knowledge to robotic control. arXiv preprint
arXiv:2307.15818 , 2023.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877‚Äì1901, 2020.
Cao, S., Yin, Y ., Huang, L., Liu, Y ., Zhao, X., Zhao, D.,
and Huang, K. Efficient-vqgan: Towards high-resolution
image generation with efficient vision transformers. In
Proceedings of the IEEE/CVF International Conference
on Computer Vision , pp. 7368‚Äì7377, 2023.Carlier, A., Danelljan, M., Alahi, A., and Timofte, R.
Deepsvg: A hierarchical generative network for vector
graphics animation. Advances in Neural Information
Processing Systems , 33:16351‚Äì16361, 2020.
Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman,
W. T. Maskgit: Masked generative image transformer. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 11315‚Äì11325, 2022.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022.
Clou ÀÜatre, L. and Demers, M. Figr: Few-shot image gen-
eration with reptile. arXiv preprint arXiv:1901.02199 ,
2019.
Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,
and Sutskever, I. Jukebox: A generative model for music.
arXiv preprint arXiv:2005.00341 , 2020.
Esser, P., Rombach, R., and Ommer, B. Taming transformers
for high-resolution image synthesis. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pp. 12873‚Äì12883, 2021.
Ghosal, D., Majumder, N., Mehrish, A., and Poria, S. Text-
to-audio generation using instruction-tuned llm and latent
diffusion model. arXiv preprint arXiv:2304.13731 , 2023.
Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,
Y . Clipscore: A reference-free evaluation metric for im-
age captioning. arXiv preprint arXiv:2104.08718 , 2021.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and
Hochreiter, S. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. Advances in
neural information processing systems , 30, 2017.
Hu, Q., Zhang, G., Qin, Z., Cai, Y ., Yu, G., and Li, G. Y .
Robust semantic communications with masked vq-vae
enabled codebook. IEEE Transactions on Wireless Com-
munications , 2023.
Jain, A., Xie, A., and Abbeel, P. Vectorfusion: Text-to-svg
by abstracting pixel-based diffusion models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 1911‚Äì1920, 2023.
Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J.,
Hornung, R., Adam, H., Akbari, H., Alon, Y ., Birodkar,
9

--- PAGE 10 ---
StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis (Preprint)
V ., et al. Videopoet: A large language model for zero-
shot video generation. arXiv preprint arXiv:2312.14125 ,
2023.
Lee, D., Kim, C., Kim, S., Cho, M., and HAN, W. S. Draft-
and-revise: Effective image generation with contextual rq-
transformer. Advances in Neural Information Processing
Systems , 35:30127‚Äì30138, 2022.
Li, T.-M., Luk ¬¥aÀác, M., Gharbi, M., and Ragan-Kelley, J.
Differentiable vector graphics rasterization for editing
and learning. ACM Transactions on Graphics (TOG) , 39
(6):1‚Äì15, 2020.
Ma, X., Zhou, Y ., Xu, X., Sun, B., Filev, V ., Orlov, N., Fu, Y .,
and Shi, H. Towards layer-wise image vectorization. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 16314‚Äì16323, 2022.
Martinez, J., Hoos, H. H., and Little, J. J. Stacked quantizers
for compositional vector compression. arXiv preprint
arXiv:1411.2173 , 2014.
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. In International conference on
machine learning , pp. 8748‚Äì8763. PMLR, 2021.
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:
Memory optimizations toward training trillion parameter
models. In SC20: International Conference for High Per-
formance Computing, Networking, Storage and Analysis ,
pp. 1‚Äì16. IEEE, 2020.
Reddy, M. D. M., Basha, M. S. M., Hari, M. M. C., and
Penchalaiah, M. N. Dall-e: Creating images from text.
UGC Care Group I Journal , 8(14):71‚Äì75, 2021.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pp.
10684‚Äì10695, 2022.
Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E.,
Hou, L., Clark, K., Pfohl, S., Cole-Lewis, H., Neal,
D., et al. Towards expert-level medical question an-
swering with large language models. arXiv preprint
arXiv:2305.09617 , 2023.
Su, H., Liu, X., Niu, J., Cui, J., Wan, J., Wu, X., and Wang,
N. Marvel: Raster gray-level manga vectorization via
primitive-wise deep reinforcement learning. IEEE Trans-
actions on Circuits and Systems for Video Technology ,
2023.Sun, Q., Yu, Q., Cui, Y ., Zhang, F., Zhang, X., Wang,
Y ., Gao, H., Liu, J., Huang, T., and Wang, X. Gen-
erative pretraining in multimodality. arXiv preprint
arXiv:2307.05222 , 2023.
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,
Azhar, F., et al. Llama: Open and efficient foundation lan-
guage models. arXiv preprint arXiv:2302.13971 , 2023.
Van Den Oord, A., Vinyals, O., et al. Neural discrete rep-
resentation learning. Advances in neural information
processing systems , 30, 2017.
Wang, Y ., Pu, G., Luo, W., Wang, Y ., Xiong, P., Kang, H.,
and Lian, Z. Aesthetic text logo synthesis via content-
aware layout inferring. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pp. 2436‚Äì2445, 2022.
Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y ., Jiang, D.,
and Duan, N. N ¬®uwa: Visual synthesis pre-training for
neural visual world creation. In European conference on
computer vision , pp. 720‚Äì736. Springer, 2022.
Wu, R., Su, W., Ma, K., and Liao, J. Iconshop: Text-
guided vector icon synthesis with autoregressive trans-
formers. ACM Transactions on Graphics (TOG) , 42(6):
1‚Äì14, 2023a.
Wu, X., Sun, K., Zhu, F., Zhao, R., and Li, H. Human
preference score: Better aligning text-to-image models
with human preference. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 2096‚Äì
2105, 2023b.
Xing, X., Zhou, H., Wang, C., Zhang, J., Xu, D., and Yu, Q.
Svgdreamer: Text guided svg generation with diffusion
model. arXiv preprint arXiv:2312.16476 , 2023.
Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn,
K., Minnen, D., Cheng, Y ., Gupta, A., Gu, X., Haupt-
mann, A. G., et al. Language model beats diffusion‚Äì
tokenizer is key to visual generation. arXiv preprint
arXiv:2310.05737 , 2023.
Zhang, T., Liu, H., Zhang, P., Cheng, Y ., and Wang, H.
Beyond pixels: Exploring human-readable svg generation
for simple images with vision language models. arXiv
preprint arXiv:2311.15543 , 2023.
10
