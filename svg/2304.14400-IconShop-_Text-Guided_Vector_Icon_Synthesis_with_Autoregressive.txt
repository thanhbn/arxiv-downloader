# 2304.14400.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/svg/2304.14400.pdf
# File size: 1550934 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive
Transformers
RONGHUAN WU, City University of Hong Kong
WANCHAO SU, City University of Hong Kong
KEDE MA, City University of Hong Kong
JING LIAOâˆ—,City University of Hong Kong
A calendar with 
a mountain in 
the middle.
hot-air-balloonThe night sky has a 
half-moon and a star.A mop used to 
clean the floor.phone -book,
agenda , contactAn icon of a black 
summer T -shirt.
airplane, plane, 
travel, flight
tree, garden, 
nature, ecologyAn image of a pair of 
fashion eyeglasses.kite, fly,
game, festival phone, device,
sun, weather 
A bridge built 
over the river .
An image of an 
air-conditioner.umbrella, cloud
An SVG depicts a 
flask used in a 
medical laboratory.
rabbit, bunny, 
animal, Easter
A bulb is producing 
light.car, drive, vehicle,
transportation
Fig. 1. Vector icons generated with text prompts. The proposed IconShop supports vector icon synthesis from keywords (left panel) and natural phrases and
sentences (right panel).
Scalable Vector Graphics (SVG) is a popular vector image format that of-
fers good support for interactivity and animation. Despite its appealing
characteristics, creating custom SVG content can be challenging for users
due to the steep learning curve required to understand SVG grammars or
get familiar with professional editing software. Recent advancements in
text-to-image generation have inspired researchers to explore vector graph-
ics synthesis using either image-based methods (i.e., text â†’raster image
â†’vector graphics) combining text-to-image generation models with im-
age vectorization, or language-based methods (i.e., text â†’vector graphics
script) through pretrained large language models. Nevertheless, these meth-
ods suffer from limitations in terms of generation quality, diversity, and
flexibility. In this paper, we introduce IconShop, a text-guided vector icon
*Corresponding Author.
Authorsâ€™ addresses: Ronghuan Wu, City University of Hong Kong, ronghwu2-c@
my.cityu.edu.hk; Wanchao Su, City University of Hong Kong, wanchao_su@outlook.
com; Kede Ma, City University of Hong Kong, kede.ma@cityu.edu.hk; Jing Liaoâˆ—, City
University of Hong Kong, jingliao@cityu.edu.hk.synthesis method using autoregressive transformers. The key to success
of our approach is to sequentialize and tokenize SVG paths (and textual
descriptions as guidance) into a uniquely decodable token sequence. With
that, we are able to exploit the sequence learning power of autoregressive
transformers, while enabling both unconditional and text-conditioned icon
synthesis. Through standard training to predict the next token on a large-
scale vector icon dataset accompanied by textural descriptions, the proposed
IconShop consistently exhibits better icon synthesis capability than exist-
ing image-based and language-based methods both quantitatively (using
the FID and CLIP scores) and qualitatively (through formal subjective user
studies). Meanwhile, we observe a dramatic improvement in generation
diversity, which is validated by the objective Uniqueness and Novelty mea-
sures. More importantly, we demonstrate the flexibility of IconShop with
multiple novel icon synthesis tasks, including icon editing, icon interpo-
lation, icon semantic combination, and icon design auto-suggestion. The
project page is https://icon-shop.github.io/.arXiv:2304.14400v4  [cs.CV]  7 Jun 2023

--- PAGE 2 ---
2 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
Additional Key Words and Phrases: SVG, Icon Generation, Vector Graphics
Synthesis, Text-Guided Generation, Autoregressive Transformers.
1 INTRODUCTION
As a form of computer graphics, vector graphics represents visual
content based directly on geometric shapes (via command lines
and arguments), and is widely used in scientific and artistic appli-
cations, including architecture, surveying, 3D rendering, typogra-
phy, and graphic design. Compared with raster graphics, vector
graphics is preferred when a high degree of geometric precision
is required across arbitrary scales, among which Scalable Vector
Graphics (SVG) is a popular vector graphics file format, particularly
in creative industries. Generally, creating SVG content is difficult
for non-professional users. It is tedious and time-consuming to gain
adequate knowledge of SVG grammars and/or master professional
editing software such as Adobe Illustrator. Recently, there have
been impressive successes in generating raster images from text,
providing a convenient and efficient means of fulfilling usersâ€™ image
design intents. Thus, it is highly desirable to build a computational
system that can accomplish something similar in the field of SVG,
allowing accurate and flexible SVG content synthesis guided by
intuitive textual descriptions.
One straightforward approach is adapting raster image genera-
tion to vector synthesis by converting the image outputs of text-
to-image generation models into vector graphics using image vec-
torization methods. While such image-based methods (i.e., text â†’
raster imageâ†’vector graphics) directly incorporate recent ad-
vances in text-to-image generation (e.g., Stable Diffusion [Rombach
et al.2022]) into SVG generation, their results are often unsatisfac-
tory. This is because text-to-image models are mostly trained to
generate complex natural geometric shapes and color appearances,
and are less probable to reproduce SVG styles with simple geometric
primitives and flat colors. Furthermore, to fit such complex raster im-
ages, vectorization methods often use jagged paths with unwanted
corners and crossovers, resulting in visible and annoying artifacts.
As SVG is based on Extensible Markup Language (XML), one
research avenue for synthesizing SVG content from text is to train
Sequence-To-Sequence (seq2seq) models that take text prompts as
input and directly produce SVG scripts as output. We refer to these as
language-based methods (i.e., text â†’vector graphics script). Despite
the conceptual simplicity, SVG involves complex grammars, and
naive tokenization of SVG as a form of natural language may result
in complex and lengthy token sequences, which complicate subse-
quent probabilistic modeling. Preliminary experiments [Bubeck et al .
2023] show that Large Language Models (LLMs) like GPT-4 [Ope-
nAI 2023] tend to combine basic geometric shapes, such as Circle
andEllipse , to convey semantic information with relatively good
text-SVG alignment. However, the synthesis results show limited
complexity and diversity (see Section 4.3), which is inadequate for
real-world SVG applications.
In this paper, we develop an autoregressive transformer-based
method that supports accurate and flexible SVG content synthesis
guided by textual descriptions. We demonstrate the feasibility of
our method in the context of SVG icons, giving rise to the name,
IconShop. The key to success of IconShop is to exploit the sequentialnature of SVG: An SVG script is composed of a sequence of paths,
which in turn consists of a sequence of drawing commands (e.g.,
lines and curves, see Section 3.2). We thus concatenate all SVG
paths in a uniquely decodable way to form a command sequence.
Since the text prompt is sequential in nature as well, it can be
straightforwardly prepended to the command sequence. We thus
tokenize and mask the combined sequence in a way [Aghajanyan
et al.2022; Bavarian et al .2022; Fried et al .2023] that admits standard
training of IconShop (to predict the next token autoregressively),
while enabling conditional SVG icon synthesis on the bidirectional
context (also known as the filling-in-the-middle task). This can be
effectively achieved by incorporating the right context into the left
context (separated by a special token), which conforms to causal
(i.e., autoregressive) masking.
We train IconShop on a large-scale vector icon dataset FIGR-8-
SVG [ClouÃ¢tre and Demers 2019], consisting of monochromatic (i.e.,
black-and-white) SVG icons. We conduct comprehensive evalua-
tions of IconShop in terms of generation quality and diversity under
different synthesis settings. Our experimental results show that
IconShop is superior to existing image-based and language-based
methods in these two aspects both quantitatively and qualitatively.
The synthesized SVG icons also show reasonable faithfulness to the
corresponding text prompts. Moreover, we demonstrate the flexibil-
ity of IconShop with multiple novel icon synthesis tasks, including
icon editing, icon interpolation, icon semantic combination, and
icon design auto-suggestion.
2 RELATED WORK
Our work is related to text-to-image generation (Section 2.1), vector
graphics generation (Section 2.2), and generative transformers (Sec-
tion 2.3). Here we only provide a concise review of previous work
that is closely related to ours, and a comprehensive treatment of the
above areas is beyond the scope of this paper.
2.1 Text-to-Image Generation
Generating images from text is a challenging task that has gained
substantial attention in recent years, and has gone through three
stages of development: Generative Adversarial Networks (GANs) [Kang
et al.2023; Qiao et al .2019; Reed et al .2016; Xu et al .2018; Zhang
et al.2017, 2018], seq2seq models based on transformers [Chang et al .
2023; Ding et al .2021, 2022; Ramesh et al .2021; Yu et al .2022], and
diffusion models [Nichol et al .2021; Rombach et al .2022; Saharia
et al.2022]. Specifically, a GAN [Goodfellow et al .2014] involves
training two neural networks - a generator and a discriminator -
to play a minmax zero-sum game. The system learns to generate
new images by respecting the training data distribution and text
conditions. Text-conditioned GANs [Kang et al .2023; Qiao et al .
2019; Reed et al .2016; Xu et al .2018; Zhang et al .2017, 2018] are typ-
ically limited to modeling single and multiple object classes. Scaling
them up to handle complex image datasets remains very challenging
due to the instability occurred in the training procedure, until very
recently [Kang et al .2023]. Seq2seq models based on transform-
ers operate by converting (and concatenating) the input text (and
image) into a sequence of tokens for predicting another sequence
of tokens that corresponds to the target image [Chang et al .2023;

--- PAGE 3 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢3
Ding et al .2021, 2022; Ramesh et al .2021; Yu et al .2022]. Text-only
and image-only self-attention and text-image cross-attention are
canonical computational mechanisms in seq2seq models to capture
intricate dependencies between text and image tokens. Recently,
diffusion models have emerged to be the new standard in text-to-
image generation overnight. Typically, an unconditional diffusion
model [Ho et al .2020] initiates its process with Gaussian noise,
and iteratively eliminates it to yield a natural image. Text-guided
diffusion models [Nichol et al .2021; Rombach et al .2022; Saharia
et al.2022] leverage the text embedding either as input or through
cross-attention. Although previous work tackles text-guided visual
content generation like ours, they focus primarily on raster im-
ages with fixed resolution. In contrast, we aim for a different goal -
text-guided vector icon synthesis with arbitrary scaling.
2.2 Vector Graphics Generation
In the early 2000s, SVG content can be created using PERL [Probets
et al.2001] with plentiful drawing commands, but requires extensive
human intervention. Bergen and Ross [2012] automated the deter-
mination of the number and type of drawing commands by evolu-
tionary computation to match a target raster image. These work can
be seen as ancestors of vector graphics generation methods based
on deep neural networks for learning editable SVG representations.
SketchRNN [Ha and Eck 2017] is a pioneering deep representation
learning model for vector sketches based on a seq2seq Variational
Auto-Encoder (VAE) [Kingma and Welling 2013]. The encoder and
the decoder were implemented by a bidirectional recurrent neu-
ral network (RNN) and an autoregressive RNN, respectively. The
sketches were parameterized by polylines - a sequence of points
with line segments drawn between consecutive points. Lopes et al.
[2019] incorporated a VAE-learned raster image representation to
aid SVG font synthesis. The feasibility of the method is demon-
strated only on glyphs with a maximum of 50commands. Modeling
the layered structure of SVG, DeepSVG [Carlier et al .2020] trained
two transformer-based encoders in tandem to map SVG icons from
commands to path-level representations, and then to a global la-
tent representation. Two decoders were paired up for SVG icon
reconstruction. Although the reconstructed shapes look reasonable,
DeepSVG fails to reproduce simple geometric relationships like
perpendicularity and parallelism. Inspired by DeepSVG, Aoki and
Aizawa [2022] made full use of the global latent representation in
every stage of the decoder for synthesizing Chinese SVG characters.
Despite demonstrated success, the above-mentioned methods fail
to support text-guided SVG generation.
One straightforward approach of text-guided SVG content syn-
thesis is to first generate a raster image with a trained text-to-image
generation model (e.g., DALL Â·E [Ramesh et al .2021] and Stable
Diffusion [Rombach et al .2022]), and then vectorize it using off-the-
shelf image vectorization techniques (e.g., Potrace [Selinger 2003]
and LIVE [Ma et al .2022]). Another emerging line of research is to
directly optimize SVG parameters for a pretrained vision-language
model as the loss function. For instance, CLIPDraw [Frans et al .
2021] adopted the CLIP [Radford et al .2021] model as the opti-
mization objective to measure the embedding distance between
the vector-to-raster image and the input textual description. Apartfrom the CLIP distance, VectorFusion [Jain et al .2022] leveraged the
Score Distillation Sampling (SDS) loss [Poole et al .2022] based on a
pixel-space text-to-image diffusion model. Despite different design
philosophies, the vectorization-based and optimization-based meth-
ods suffer from similar limitations. First, the vision-language models
as the key enablers are pretrained on raster images of complex nat-
ural scenes, and thus can hardly provide guidance in synthesizing
SVG-style images with simple geometric primitives and flat colors.
Second, the generated paths are often jagged and messy, failing
to reproduce accurate geometric relations such as parallelism and
perpendicularity. Third, the per-SVG optimization can be painfully
slow, making it impractical for real-time applications. In contrast,
our proposed system, IconShop, does not suffer from any of the
above-mentioned problems. Once trained, IconShop can perform
text-guided vector icon synthesis automatically and efficiently.
2.3 Transformers as Generative Models
Owing to its inherent capability to capture long-term dependen-
cies and support parallel computing, Transformers [Vaswani et al .
2017] have emerged as a powerful class of generative models for
producing a wide variety of outputs, ranging from natural lan-
guages [Brown et al .2020; Radford et al .2019; Raffel et al .2020], au-
dios [Huang et al .2018; Li et al .2019; Valle et al .2020], and raster im-
ages [Chen et al .2020; Esser et al .2021]. Transformers can be made
non-autoregressive and autoregressive. The non-autoregressive in-
stantiation [Chang et al .2023, 2022; Ding et al .2022; Zhang et al .
2021] suggests to leverage the bidirectional context using BERT-
like [Devlin et al .2018] bidirectional Transformers for its sampling
efficiency. The autoregressive instantiation [Ding et al .2021; Ramesh
et al.2021; Yu et al .2022] emphasizes the importance of learning
to predict the next token in a causal way. Together with scaling, it
unlocks the emerging abilities of LLMs. Inspired by [Aghajanyan
et al.2022; Bavarian et al .2022], we unify non-autoregressive and
autoregressive modeling of vector icons for various synthesis tasks.
3 ICONSHOP
In this section, we first briefly introduce autoregressive models (Sec-
tion 3.1), and describe the SVG commands of vector icons, followed
by our tokenization strategy (Section 3.2). We then describe the
"causal" masking strategy, which enables our autoregressive model
to perform the filling-in-the-middle task (Section 3.3). We next elab-
orate our model architecture (Section 3.4) and, finally, present the
training objectives (Section 3.5).
3.1 Preliminaries on Autoregressive Models
An autoregressive model specifies that the current state depends
only on its previous states. In probabilistic terms, this corresponds
to the chain rule of probability:
ğ‘(ğ‘†)=ğ‘Ã–
ğ‘›=1ğ‘(ğ‘†ğ‘›|ğ‘†1,...,ğ‘†ğ‘›âˆ’1), (1)
in which we factorize the joint probability of a sequence of random
variablesğ‘†=(ğ‘†1,...,ğ‘†ğ‘)into a product of conditional probabil-
ities. At the ğ‘›-th instance, autoregressive models take the values
of previous ğ‘›âˆ’1random variables (or the most recent ones if a

--- PAGE 4 ---
4 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
Name Symbol Argument Explanation Example
Move To M ğ‘¥,ğ‘¦Move the cursor to the specified point
(ğ‘¥,ğ‘¦).M20,10
(20,10)
Line To L ğ‘¥,ğ‘¦Draw a line segment from the current
point to the specified point (ğ‘¥,ğ‘¦).L20,10
(20, 10)
Cubic BÃ©zier Cğ‘¥1,ğ‘¦1
ğ‘¥2,ğ‘¦2
ğ‘¥,ğ‘¦Draw a curved path from the current
point to the specified point (ğ‘¥,ğ‘¦)using
two control points (ğ‘¥1,ğ‘¦1)and(ğ‘¥2,ğ‘¦2).C13,27,820,10
(20, 10)(13, 2)
(7, 8)
Table 1. Overview of SVG commands. In our implementation, we employ three simple yet expressive commands (namely, M,L,
andC) to represent vector icons. In the Example column, we assume the current point is located at (0,0). For a sequence of
commands, the starting position of each command is the ending position of the previous command.
Markov window is applied) as input, and compute the conditional
probability distribution of ğ‘†ğ‘›, from which we are able to draw a
sample as its prediction. Here we resort to autoregressive models
for SVG icon synthesis because it fits naturally in the sequential
nature of SVG and textural descriptions.
3.2 SVG Representation and Tokenization
SVG offers a range of features and syntax options, allowing users
to create their original work with great flexibility. For example,
theRect command creates a rectangular shape controlled by the
starting point, width, and height arguments, like <Rect x="80"
y="90" width="100" height="100"/> . The Transform attribute
applies an affine transformation to an existing shape, like <Rect
Transform="rotate (-10 50 100)" x="80" y="90" width="100"
height="100"/> . If we try to come up with a universal data struc-
ture to represent all possible SVG commands and attributes, such
a data structure would become highly complex, which may hinder
the probabilistic modeling of SVG icons. To bypass this issue, we
choose to limit the number of commands and attributes, while main-
taining their expressiveness to capture the essence of SVG icons. In
other words, we seek a compact SVG representation that makes its
probabilistic modeling easier.
Inspired by DeepSVG [Carlier et al .2020], we simplify every SVG
icon by removing all attributes and using only three basic commands:
Move To, Line To, and Cubic BÃ©zier (see Table 1 for explanations
and examples). Other complex commands (e.g., Rect ,Circle , and
Ellipse ) can be approximated by combinations of these basic com-
mands with negligible visual differences. For example, we can use
four line segments to construct a Rect , and concatenate four BÃ©zier
curves to form a Circle . An SVG text script ğº, after the simplifica-
tion, contains ğ‘€paths ,ğº={ğ‘ƒğ‘–}ğ‘€
ğ‘–=1, whereğ‘ƒğ‘–is theğ‘–-th path, and
each pathğ‘ƒğ‘–in turn consists of ğ‘ğ‘–commands ,ğ‘ƒğ‘–={ğ¶ğ‘—
ğ‘–}ğ‘ğ‘–
ğ‘—=1, where
ğ¶ğ‘—
ğ‘–is theğ‘—-th command in the ğ‘–-th path. A command, ğ¶ğ‘—
ğ‘–=(ğ‘ˆğ‘—
ğ‘–,ğ‘‰ğ‘—
ğ‘–),
contains its type ğ‘ˆğ‘—
ğ‘–âˆˆ {M,L,C}and the corresponding location
argumentğ‘‰ğ‘—
ğ‘–.
The next step is to convert the SVG script into a discrete sequence
of tokens, which are subject to autoregressive modeling. We intro-
duce an intuitive SVG tokenization approach, which is composed offour major steps. First, flatten the layered structure of the SVG script
by concatenating commands from different paths to form a single
command sequence. To uniquely decode the flattened command
sequence back to the layered representation, we prepend a special
token, <BOP> (i.e., begin-of-path ), before the first command of each
path. Second, assign distinct tokens to each command type (i.e., M,
L,C). Third, map the 2D location argument to 1D using row-major
order, which roughly halves the length of the token sequence. For
example, suppose the default width of an SVG image is ğ‘¤, we trans-
form a 2D location argument (ğ‘¥,ğ‘¦)to a 1D argument using the
formulağ‘¥Ã—ğ‘¤+ğ‘¦. Fourth, append a special token, <EOS> (i.e., end-
of-SVG ), at the end of the sequence that indicates the completion of
an SVG icon sequence. An example of a sequence created using the
above steps is shown in Figure 2.
3.3 Masking Scheme
Autoregressive models have been shown to be effective in generating
token sequences from scratch, but they are constrained to doing
so in a causal direction (i.e., from left to right). This constraint
hinders SVG synthesis performance in tasks such as icon editing,
where we need to fill in missing content based on the bidirectional
context. Some methods have been developed to expand the ability of
autoregressive models to perform the filling-in-the-middle task by
massaging training data without modifying the model architecture.
For example, CM3 [Aghajanyan et al .2022] and InCoder [Fried
et al.2023] used a "causal" masking strategy that randomly selects
several chunks in the input sequence, and moves them to the end.
In [Bavarian et al .2022], a similar strategy was implemented to learn
to fill in the middle without sacrificing the original causal generation
capability. In training IconShop, we incorporate a similar "casual"
masking strategy to unify non-autoregressive and autoregressive
modeling of SVG token sequences.
For a given input sequence ğ‘†(0), we first select a random chunk
called span, based on which we split the sequence into three parts,
[Left :Span :Right], where " :" represents concatenation. We
replace the span with a special <Mask> token to obtain a new se-
quenceğ‘†(1)=[Left :<Mask> :Right]. Next, we add the same
<Mask> token to the beginning of the span and add a <EOM> token
(i.e., end-of-mask ) to the end of the span to create a new sequence

--- PAGE 5 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢5
6
7
82
43
51
6
7
8
Path 1
Command
SVG
Cmd 1 Cmd 2 Cmd 3 Cmd 4 Cmd 5Path 1 Path 2
SVG
MMoveÂ To LLineÂ To CCubicÂ BÃ©zier1DÂ coordinate
EOSL
L1
2
3 45
Command
Path 2M
C
C
C
C
M
Token
Token
M C C C C M L LCmd 6Cmd 7Cmd 8â€œclock, timeâ€Tokenization
Tokenization
Autoregressive
TransformerTextCmd 1
Cmd 2
Cmd 3
Cmd 4
Cmd 5
Cmd 6
Cmd 7
Cmd 8
M L LÂ·Â·Â·CLSSEPSOS
OutputÂ Sequence
SOS StartÂ ofÂ Sequence
Tokenizer
Concatenate
SVG
BOP
SVGÂ Embedding
TextÂ Embedding+
+
Fig. 2. The system diagram of IconShop. In this example, the clock icon has two paths, each comprising three types of basic commands (refer to Table 1). For
Path 1, there is one M(Move To) command and four BÃ©zier curves. For Path 2, there is one Mcommand and two line segments. To tokenize these paths, we first
concatenate the commands of the two paths to form a single sequence. We then convert each commandâ€™s 2D location argument (ğ‘¥,ğ‘¦)into 1D using the
formulağ‘¥Ã—ğ‘¤+ğ‘¦, whereğ‘¤is the default SVG image width. We use a pretrained text encoder to tokenize and embed the text "clock, time", where <CLS> and
<SEP> mark the beginning and the end of the text input. We then concatenate the embedded text and SVG sequences, and add an <SOS> at the beginning.
This concatenated sequence is sent to an autoregressive transformer for joint probability modeling.
ğ‘†(2)=[<Mask> :Span :<EOM>]. Finally, we concatenate ğ‘†(1)and
ğ‘†(2)to form
ğ‘†=[Left :<Mask> :Right :<Mask> :Span :<EOM>], (2)
which is sent to the model for probabilistic modeling. The masked
sequenceğ‘†conveys the following information: 1) the first <Mask>
token indicates the original position of the span, 2) the second
<Mask> token denotes the beginning of the span, and 3) the <EOM>
token marks the end of the span. During training, we randomly apply
this masking strategy to 50%of the training data, while leaving the
remaining 50%unchanged. We exclude the <Mask> token from the
cross-entropy loss calculation to discourage its generation during
inference.
We now explain how this masking technique enables autoregres-
sive models to perform the filling-in-the-middle (i.e., non-autoregressive)
generation without modifying the architecture. Suppose we have a
token sequence[Left :Right], and want to fill in the middle chunk
between Left andRight . As in Eq. equation 2, we add two <Mask>
tokens to the sequence to create [Left :<Mask> :Right :<Mask>],
and send it to the model for seq2seq generation until the <EOM>
token occurs, giving rise to the sequence [Left :<Mask> :Right :
<Mask> :Span :<EOM>]. After that, we move the predicted Spanback to its original position, i.e., the position of the first <Mask>
token, to obtain the final output [Left :Span :Right]. Since the
model leverages both Left andRight contexts to fill in the mid-
dleSpan chunk, we achieve non-autoregressive modeling through
autoregressive prediction, and thus unify these two.
3.4 Model Architecture
We employ the Transformer decoder [Vaswani et al .2017] to im-
plement our autoregressive model, as it effectively captures the
long-range interdependencies among various tokens that constitute
a vector icon sequence. Specifically, the model consists of three
modules: an SVG embedding module to encode the SVG sequence,
a text embedding module to encode the text sequence, and a trans-
former (decoder) module to exploit text-SVG correlations, and learn
the joint probability distribution of the combined token sequence,
sampling from which produces novel sequences that are not present
in the training set.
SVG Embedding Module . As discussed previously, an SVG se-
quence is represented by six distinct categories of tokens: 1) Com-
mand type, 2) 1D location argument, 3) Begin-of-path token <BOP> ,
4) End-of-SVG token <EOS> , 5) Mask token <Mask> , and 6) End-of-
mask token <EOM> . By default, each icon is constrained within a

--- PAGE 6 ---
6 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
100Ã—100bounding box, resulting in 1002possible values for the 1D
location argument. Thus, a one-hot vector ğ‘’with a dimension of
10,007(=3+1002+1+1+1+1)suffices to represent all possible token
cases. We then use a learnable embedding matrix ğ‘ŠâˆˆRğ·Ã—10007to
transform the one-hot vector into an embedding vector of size ğ·.
We incorporate two extra learnable matrices ğ‘Šğ‘¥,ğ‘Šğ‘¦âˆˆRğ·Ã—100to
augment the location information as suggested in [Xu et al. 2022]:
ğ‘£ğ‘–â†ğ‘Šğ‘’ğ‘–+ğ‘Šğ‘¥ğ‘’ğ‘¥
ğ‘–+ğ‘Šğ‘¦ğ‘’ğ‘¦
ğ‘–, (3)
whereğ‘’ğ‘–âˆˆR10007Ã—1is the one-hot vector of the ğ‘–-th token, and
ğ‘’ğ‘¥
ğ‘–,ğ‘’ğ‘¦
ğ‘–âˆˆR100Ã—1are one-hot encoding of the 2D coordinates, respec-
tively.
Text Embedding Module . LLMs trained on a large corpus of tex-
tual data have the ability to capture intricate word interrelationships,
including synonymy and antonymy, as suggested in [Saharia et al .
2022]. Here we make use of the tokenization and word embedding
layers from a pretrained BERT [Turc et al .2019] model, and fix them
to tokenize and embed textual inputs. The tokenizer adds a <CLS>
token to the beginning of the text, and appends a <SEP> token at
the end of the text, indicating the start and end of the text sequence,
respectively.
Transformer Module . Our autoregressive transformer model con-
sists of a stack of 12identical layers. Each layer is a standard trans-
former decoder block, comprising (masked) multi-head attention,
layer normalization, and feed-forward layers, all interconnected
via residual connections. The autoregressive transformer ultimately
produces ağ·-dimensional vector at the ğ‘›-th token position, which
is conditioned on its preceding ğ‘›âˆ’1tokens. A linear layer followed
by the softmax function is applied to obtain the probabilities of all
possible tokens at the ğ‘›-th position.
3.5 Training Objective
Since the text descriptions and SVG scripts in the training set have
varying lengths, we pad both the text token sequence ğ‘†textand the
icon token sequence ğ‘†iconwith zeros to a fixed length ( 50for the text
and512for the icon in our implementation). We then concatenate
them to obtain the target sequence ğ‘†=[ğ‘†text:ğ‘†icon].
The autoregressive transformer is trained to predict the next to-
ken based on previous tokens. To prepare the input sequence ğ‘†in,
we remove the last token of ğ‘†(i.e., <EOS> ), and add an <SOS> token
at the start. This essentially shifts ğ‘†to the right by one position,
which enables initial autoregressive prediction with an empty con-
text. The transformer outputs a token sequence Ë†ğ‘†=[Ë†ğ‘†text:Ë†ğ‘†icon]
sequentially. Our objective is to minimize the cross-entropy loss
between the target and output tokens at each position, and then
combine text and icon losses with a weighted sum [Ramesh et al .
2021]:
â„“text=CE(ğ‘†text,Ë†ğ‘†text),
â„“icon=CE(ğ‘†icon,Ë†ğ‘†icon),
â„“total=â„“text+ğœ†â„“icon,(4)
where CE()is the standard cross-entropy function, and ğœ†=7.0is
the weighting to control the relative importance between the text
and icon reconstruction.
24 hour
 card/password
 play/music
 cat/face
Fig. 3. Monochromatic icon samples from the FIGR-8-SVG dataset (1st
row). Each icon is associated with several discrete keywords as textual
descriptions. We remove the black box to improve the visual quality (2nd
row).
4 EXPERIMENTS
In this section, we first present in detail the data processing pro-
cedure for the icon dataset with textual descriptions (Section 4.1).
We then introduce an ablation study to validate the efficacy of the
model architecture under both conditional and unconditional gen-
eration settings (Section 4.2). Lastly, we compare the proposed Icon-
Shop with alternative solutions, demonstrating that IconShop yields
higher-quality results (Section 4.3).
4.1 Data Preparation
SVG Dataset . We use the FIGR-8-SVG dataset [ClouÃ¢tre and Demers
2019] that consists of 1.5million monochromatic (black-and-white)
vector icons. Typically, the first step for SVG data processing is
transforming icons with varied grammars into standardized repre-
sentations. Fortunately, in the FIGR-8-SVG dataset, all icons have
been converted to a uniform representation with discretized argu-
ments. We show several icon examples from the dataset in the first
row of Figure 3. We further enhance the visual attractiveness of each
icon by removing the outer black box. The corresponding enhanced
icons are shown in the second row. After the SVG tokenization
described in Section 3.2, we set the maximum length of an icon se-
quence to 512, filtering out those with longer lengths. This results in
about 1.1million samples, among which, we select 300,000samples
for model training and experimentation at the current stage. We
partition these samples into 90%for training, 5%for validation, and
5%for testing.
Text Input . In the FIGR-8-SVG dataset, every vector icon is anno-
tated with discrete keywords, such as " cat/face ". Training Icon-
Shop only with keywords would constrain its capacity to generate
icons from natural language phrases and sentences. Inspired by In-
structPix2Pix [Brooks et al .2022], which fine-tunes GPT-3 [Brown
et al.2020] to produce editing instructions and captions, we use
LLMs (ChatGPTâˆ—in particular) to expand these keywords into natu-
ral language phrases and sentences. The prompt given to ChatGPT is
"Write the simplest sentence from keywords: #{keywords}.
Do not add additional facts ".
Implementation Details . We implement IconShop using PyTorch .
The training process employs the Adam optimizer [Kingma and
âˆ—https://openai.com/blog/chatgpt

--- PAGE 7 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢7
0.9883 0.98000.9863 0.9760Generated
0.9604
0.97460.9920 0.9775 0.9595DatasetÂ Samples
CosineÂ Similarity0.52300.5320
0.5845
Fig. 4. We use the CLIP image encoder to extract image features, and
calculate the cosine similarity between the generated icons and the samples
in the dataset. As visually distinct icons may have high cosine similarity
scores due to the simple black strokes against the white background, we
set a relatively high threshold of 0.98to determine whether two icons are
identical when computing the "Uniqueness" and "Novelty" scores.
Ba 2014] with a learning rate of 0.0006 , along with linear warm-
up, decay, and gradient clipping. The dropout rate is set to 0.1.
Shuffled discrete keywords, natural language phrases and sequences
(generated by ChatGPT), and blank text are, respectively, trained
with ratios of 60%,30%, and 10%, with a total minibatch size of 192.
We train IconShop for 300epochs. We use an NVIDIA A100 GPU
in the following experiments to test IconShop, which takes 1.38
seconds to generate an SVG icon sequence on average.
Metrics . To evaluate the quality of the generated SVG icons, we use
the FrÃ©chet Inception Distance (FID) [Heusel et al .2017], which mea-
sures the distance between the image features of synthesized and
"ground-truth" SVG icons from FIGR-8-SVG . Specifically, we obtain
the image features of rendered and rasterized SVG icons using the
CLIP image encoder [Radford et al .2021]. We also compute the CLIP
score to measure text-SVG alignment (i.e., the semantic similarity
between the text input and the visual icon output). Additionally,
following SkexGen [Xu et al .2022], we calculate the "Novelty" and
the "Uniqueness" scores, which refer to the proportion of generated
data absent from a target set (i.e., the training set from FiGR-8-SVG )
and occurring only once among all generated samples, respectively.
We generate 20,000icons unconditionally and 7,000icons with
textual guidance. The cosine similarity between the CLIP features
is used to determine whether the two icons are identical with a
threshold of 0.98. Figure 4 gives an intuitive visual comparison of
different similarity values.
4.2 Ablation on Different Network Architecture
In this subsection, we conduct two ablation studies to illustrate
the effects of two key components of IconShop on the synthesis
performance. The first study shows the importance of exploiting the
sequential nature of the SVG icons in generation by comparing our
seq2seq model to a GAN model. The second study demonstrates the
necessity of our autoregressive model augmented with the "causal"masking scheme by comparing it to a non-autoregressive training
strategy (i.e., BERT [Devlin et al. 2018]).
Seq2seq versus Layered Modeling . As previously mentioned, an
SVG file is layered in structure, consisting of higher-layer paths and
lower-layer commands. Here we use DeepSVG+GAN as a baseline
to testify our choice of the seq2seq generation mechanism.
The original DeepSVG reconstructs an SVG icon by first obtaining
path-level representations and then aggregating them to a global
representation. To make a fair comparison between IconShop and
DeepSVG, we need to enable the original DeepSVG to generate new
icons and support text-guided generation. To achieve these goals,
we retrain DeepSVG for SVG icon reconstruction on our dataset.
We then train a conditional GAN model, which takes the textual
feature as input and predicts the latent representation precomputed
from the encoder of DeepSVG as output, which can be straightfor-
wardly decoded into an SVG icon by passing through the decoder
of DeepSVG. In addition to text-guided generation, we also enable
the GAN to generate icons unconditionally by replacing the text
feature with random noise 10%of the time during training.
We compare both random generation andtext-guided generation
performance. For random generation, we produce 20,000icons for
each method. For text-guided generation, we choose both discrete
keywords and natural phrases/sentences commonly used in design
scenarios as text input and generate 7,000icons for each model. As
shown in Figures 5 and 6, IconShop is capable of generating visu-
ally recognizable and eye-catching icons while preserving salient
geometric relationships, such as perpendicularity, parallelism, and
symmetry. We believe such consistent high-quality generation re-
sults arise because we prioritize the sequential modeling of SVG
icons. In contrast, DeepSVG+GAN presents visually worse results
than ours, which we attribute to its averaging operation over com-
mands and paths, resulting in a loss of geometric details in the
generated icons. Please refer to the project page for more qualitative
results.
We also quantitatively evaluate the random generation and text-
guided generation results in Tables 2a and 2b. We find that Icon-
Shop synthesizes results with significantly lower FID scores in both
random and text-guided generation tasks, which provides a strong
indication of the superior synthesis capability of IconShop. Regard-
ing Uniqueness and Novelty as measures of generation diversity,
IconShop performs comparably well to DeepSVG+GAN. Neverthe-
less, it is important to note that the high Uniqueness and Novelty
values achieved by DeepSVG+GAN are largely attributed to the
noticeable visual distortions (e.g., jittering curves) it suffers from
rather than novel instantiations of the same object/concept, as ev-
ident in Figure 5. We regard this as "fake diversity" if it is solely
interpreted without being conditioned on acceptable generation
quality. Moreover, IconShop obtains the highest CLIP score in the
text-guided generation task, signaling the outstanding ability to
produce icons that accurately reflect the text semantics.
Autoregressive versus Non-Autoregressive Modeling . Recent
research [Chang et al .2023, 2022] shows that a non-autoregressive
training paradigm for generating raster images is feasible. These
approaches use bidirectional Transformers (i.e., BERT) as the funda-
mental model architecture. The departure from the autoregressive to
non-autoregressive modeling allows parallel prediction of multiple

--- PAGE 8 ---
8 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
IconShop DeepSVG+GAN BERT
Fig. 5. Icons randomly generated by IconShop, DeepSVG+GAN, and BERT, respectively. Our approach creates icons with form consistency, high-precision
of recognizability, geometry simplicity, and good composition. Icons produced by DeepSVG+GAN do not meet such desired properties, while BERT only
synthesizes basic geometric shapes with essentially no semantics.
FIDâ†“ Uniqueness %â†‘ Novelty %â†‘
DeepSVG+GAN 11.95 98.72 99 .22
BERT 43.61 2.06 19 .90
IconShop 6.08 78.77 85 .10
(a) Random Generation
FIDâ†“ Uniqueness %â†‘ Novelty %â†‘ CLIP Scoreâ†‘
DeepSVG+GAN 12.01 97.59 99 .01 21.78
BERT 35.10 14 .41 50 .30 22 .03
IconShop 4.65 68.29 68 .60 25.74
(b) Text-Guided Generation
Table 2. We evaluate IconShop through random generation and text-guided
generation tasks. We use the FID score, with features extracted from the
CLIP image encoder, to assess generation quality. We also compute the
percentage of unique and novel icons as measures of generation diversity.
For text-guided generation, we employ the CLIP score to show the semantic
alignment between the text input and the generated icons.
tokens, and opens the door to diverse image editing tasks such as
inpainting. We explore the possibility of using BERT to produce SVG
token sequences, as an alternative to our "causal" masking scheme.
It is noteworthy that, unlike raster image generation which is often
fixed in length, SVG icon generation expects output sequences of
varying lengths as a manifestation of generation diversity. This
makes it a more difficult task because in SVG icon generation, BERT
needs to not only model the likelihood of next tokens but also de-
termine where to terminate the sequence.
After training the BERT model on the same dataset as ours, we
assess the generation quality and diversity of its outputs. Quan-
titative evaluations are conducted similarly to DeepSVG+GAN inboth random (Table 2a) and text-guided generation (Table 2b). BERT
produces worse results in both tasks than ours and DeepSVG+GAN.
The qualitative results in Figure 5 also suggest that BERT can only
generate simple geometric shapes such as circles and rectangles
that are relatively insipid. A closer inspection of the filled tokens
reveals that the end-of-SVG token, <EOS> , may frequently appear in
multiple positions due to parallel prediction, which results in early
termination when reconstructing SVG icons. Therefore, despite the
impressive ability of the non-autoregressive BERT in sequence edit-
ing, it is inferior to the autoregressive counterpart (as in IconShop)
in modeling and producing sequences of varying lengths.
4.3 Comparison to the State-of-the-Art
We compare the proposed IconShop to two types of text-to-SVG
generation schemes: image-based and language-based methods. For
the former, we use Stable Diffusion [Rombach et al .2022] to create
raster images, which are transformed into SVG images using the
LIVE [Ma et al .2022] program. To encourage icon-style results,
we include keywords (like "monochrome" and "line art") in the
text prompts. For the latter, we employ GPT-4, arguably the best-
performing LLM so far, to generate SVG scripts directly. To initiate
the conversation, we provide a text prompt at the system level asking
it to act as an SVG code generator.
Figure 6 shows icons synthesized by the competing methods. We
find that the outcomes of the Stable Diffusion model often fall short
of expectations. After all, it is trained on raster images, which as
expected struggles to produce icon-style images even with some
prompt engineering. After applying LIVE, the resulting SVG icons

--- PAGE 9 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢9
IconShopStableÂ Diffusion
+Â LIVE,Â 32Â pathsDeepSVG
+Â GAN
â€œspider,Â bug,Â 
nature,Â insectâ€
â€œcake,Â dessert,
sweet,Â foodâ€
â€œAÂ pumpkin,Â aÂ kindÂ ofÂ 
fallÂ vegetable.â€
â€œAÂ beautifulÂ 
snowflake.â€
â€œA microphoneÂ usedÂ 
forÂ recordingÂ sound.â€
â€œAÂ pictureÂ showsÂ aÂ 
seaÂ creature,Â aÂ crab.â€InputÂ TextÂ Prompt BERT GPTÂ­4
Fig. 6. We evaluate the text-guided icon synthesis of IconShop against several other methods, including DeepSVG+GAN, BERT, the image-based method
(Stable Diffusion+LIVE), and the language-based method (GPT-4). The results show that Stable Diffusion+LIVE provides poor vectorization results, in terms of
geometrical and semantic precision of icon forms, while GPT-4 is only able to combine basic geometric shapes with limited recognizability. Our IconShop clearly
outperforms the competing methods in terms of icon generation quality.
usually present unsmooth and inconsistent structures, many of
which are not semantically recognizable. In addition, Stable Diffu-
sion is inefficient in generation due to the iterative optimization-
based vectorization by LIVE. With respect to the language-based
method, GPT-4 has relatively strong capabilities in generating SVG
icon scripts purely from text prompts, with moderate text-SVG
alignment. Nevertheless, the results are manifested as simple combi-
nations of primitive shapes with no complex overlaying, which are
inadequate for graphics design, and also suffer from the recogniz-
ability problem (see the third and fifth rows). In stark contrast, our
IconShop produces results with the highest visual quality in terms
of form consistency, high precision of recognizability and text-SVG
alignment, and geometry simplicity. Reasonable generation diver-
sity is also well observed from IconShop. More visual results of the
text-guided generation can be found in our project page.
4.4 Subjective User Study
To formally validate the perceptual gains by IconShop, we conduct
a subjective user study, consisting of three tasks to visually assess 1)
User Selection%â†‘
DeepSVG
+ GANStable Diffusion
+ LIVEGPT-4 IconShop Dataset
Quality (random) 54.09 15 .95 2 .95 82.11 83.71
Quality (text) 51.90 49 .49 2 .15 96.33 \
Alignment (text) 29.24 72 .78 1 .77 96.20 \
Table 3. Subjective user study results. In each task, we report the average
percentage of selected (i.e., high-quality) icons by the users.random generation quality, 2) text-guided generation quality, and 3)
text-SVG alignment. In the first task, we first familiarize users with
high-quality icons from the training set. We then present them a
total of 15Ã—5icons in random order, one-fifth of which are generated
by (or sampled from) DeepSVG+GAN, Stable Diffusion+LIVE, GPT-
4, IconShop, and FIGR-8-SVG , respectively. Users are forced to give a
binary decision of whether each presented icon is of high quality. In
the second task, we randomly select ten text prompts, and generate
the corresponding icons by the four competing methods. Users are
then asked to select two icons that they believe are of the best visual
quality. The setup of the third task is identical to that of the second
task. The difference is that this time users need to pick two icons
that best match the corresponding text prompt.
We conduct the user study via an online questionnaire, with 79
users participating in the study. For the first task, we obtain 79
(users)Ã—15(icons)Ã—5(methods) = 5,925human judgments. We
calculate the average percentage of high-quality icons identified by
users for each method, and list the results in the first row of Table 3.
We see that IconShop approaches the performance to that of the
"Dataset" as the upper bound, and is clearly better than the other
three methods. This shows that IconShop is able to produce high-
quality icons that consistently "fool" the subjects in the random
(unconditional) generation setting.
For the second and third tasks, we, respectively, collect 79(users)
Ã—10(text prompts)Ã—2(selections) = 1,580human judgments for
each task. We report the average percentage of user-selected icons
by each method in the last two rows of Table 3. It is clear that icons
synthesized by IconShop are most frequently selected, indicating the

--- PAGE 10 ---
10 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
RandomÂ Editing Text-GuidedÂ Editing
folder calendar display smileÂ andÂ â€¦
cloudÂ andÂ â€¦ star snow raintigerÂ andÂ â€¦ happy sleep oneÂ eye
Fig. 7. IconShop enables both random and text-guided icon editing. We use the green color to highlight the paths to be edited.
snowflake
flowerspider
crabumbrella
treecalendar
photoğ›¼= 1.0ğ›¼= 0.0
ğ›¼= 0.5
ğ›¼= 1.0ğ›¼= 0.0
ğ›¼= 0.5
ğ›¼= 1.0ğ›¼= 0.0
ğ›¼= 0.5
ğ›¼= 1.0ğ›¼= 0.0
ğ›¼= 0.5
Fig. 8. We linearly interpolate two text embedding vectors using the formula ğ‘£=(1âˆ’ğ›¼)Â·ğ‘£(1)+ğ›¼Â·ğ‘£(2), whereğ›¼âˆˆ[0,1]. Subsequently, we generate icons
corresponding to each interpolated vector ğ‘£. The results show that IconShop learns a smooth mapping between the text and SVG spaces.
highest quality and the best text-SVG alignment in the text-guided
generation task. We also perform one-way ANalysis Of VAriance
(ANOVA) tests, and the ğ‘-values of the three tasks are all less than
0.001, suggesting that the perceptual gains by IconShop are statisti-
cally significant. In summary, the proposed IconShop demonstrates
the highest quality for both random and text-guided generation,
with a strong text-SVG alignment.
5 APPLICATION
In this section, we explore four practical applications of IconShop:
icon editing, icon interpolation, icon semantic combination, and icon
design auto-suggestion. These applications streamline the process ofvector icon synthesis, enhancing user productivity and experience
significantly.
5.1 Icon Editing
Thanks to the unification of non-autoregressive and autoregres-
sive modeling through the "causal" masking strategy outlined in
Section 3.3, the proposed IconShop facilitates icon editing as exem-
plified in Figure 7. IconShop is capable of filling in missing content
based on the bidirectional context, in either random or text-guided
generation scenario. This leads to precise, consistent, and diverse
restoration of the missing paths in icons.

--- PAGE 11 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢11
phone
+Â flask +Â star +Â sun
computer
+Â cake +Â clock +Â eye
calendar
+Â rain +Â apple +Â mountain
cloud
+Â umbrella +Â apple +Â tree
Fig. 9. IconShop learns to produce creative icons by combining semantics
of different text prompts.
5.2 Icon Interpolation
Text-to-image models demonstrate an impressive ability to combine
textual inputs, encouraging generation of novel concepts that do not
exist in the training data, such as " avocado chair " generated by
DALL Â·E [Ramesh et al .2021]. In our experiments, we find that Icon-
Shop also learns to create innovative and reasonable combinations,
as shown in Figure 9.
5.3 Icon Semantic Combination
Text-to-image models demonstrate an impressive ability to combine
textual inputs, allowing them to generate novel concepts that do
not exist in the training data, such as â€œ avocado chair â€ generated
by DALL Â·E [Ramesh et al .2021]. In our experiments, we find that
our model can also produce innovative and creative combinations,
as shown in Figure 9.
5.4 Icon Design Auto-Suggestion
One advantage of automated icon generation is to support both
designers and non-specialists in expressing their creative ideas. A
desired feature of such an automated system is the ability to suggest
possible placements for subsequent paths on a canvas, which would
significantly improve work efficiency and productivity. Relying
on the autoregressive transformer, our trained IconShop is able to
predict the next path that users may choose in their icon creation
processes (see Figure 10). Please refer to the project page for the
video demonstrations of our auto-suggestion system for SVG icon
design.
6 CONCLUSION
We have introduced IconShop, an autoregressive transformer-based
method proficient at generating vector icons from textual descrip-
tions. IconShop stands out from both image-based methods that
Fig. 10. IconShop is able to suggest subsequent paths for the users to design
icons, with significantly boosted efficiency. We highlight paths suggested
by IconShop with the green color and paths drawn by the users with the
blue color. Even if users diverge from the suggested route, IconShop still
predicts subsequent paths that are compatible with the usesâ€™ chosen paths.
combine text-to-image generation and image vectorization, and
language-based techniques that treat SVG scripts as natural lan-
guages. Comprehensive experiments showcase the effectiveness
and flexibility of IconShop in terms of generation quality, diversity,
text-icon alignment, and wide applicability.
IconShop exhibits impressive SVG icon synthesis performance
by exploiting the sequential nature of SVG scripts and unifying
non-autoregressive and autoregressive modeling, but it is not with-
out limitations (see Figure 11). First, natural language phrases and
sentences generated by ChatGPT may inadvertently result in text-
SVG mismatches. This problem can be mitigated by using a higher-
quality SVG dataset with accurate text annotations or by human
filtering. Second, the semantic icon combination performance by
IconShop may not be as remarkable as text-to-image generation,
because most icons in the FIGR-8-SVG dataset contain a single ob-
ject situated at the center, occupying a significant portion of the
space. We believe proper data augmentation such as scaling and
merging SVG data to create new icons has the potential to no-
tably improve the combination performance. Finally, IconShop is
AÂ swimmer inÂ 
aÂ seaÂ withÂ fish.fish/sea/ swim phone clock
Mismatch
SemanticÂ Combination
ChatGPT
phone/clock
Fig. 11. Limitations contain text-SVG mismatches (left panel) and subopti-
mal semantic icon combination (right panel).

--- PAGE 12 ---
12 â€¢Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liaoâˆ—
restricted to black-and-white icon generation, but it could poten-
tially be expanded to yield multicolored icons or more general SVG
content (e.g., clip art) with proper modifications.
REFERENCES
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Na-
man Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke
Zettlemoyer. 2022. CM3: A Causal Masked Multimodal Model of the Internet. arXiv
preprint arXiv:2201.07520 .
Haruka Aoki and Kiyoharu Aizawa. 2022. SVG Vector Font Generation for Chinese
Characters with Transformer. In IEEE International Conference on Image Processing .
646â€“650.
Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey,
Jerry Tworek, and Mark Chen. 2022. Efficient Training of Language Models to Fill
in the Middle. arXiv preprint arXiv:2207.14255 .
Steven Bergen and Brian J. Ross. 2012. Automatic and Interactive Evolution of Vector
Graphics Images with Genetic Algorithms. The Visual Computer 28, 1, 35â€“45.
Tim Brooks, Aleksander Holynski, and Alexei A. Efros. 2022. InstructPix2Pix: Learning
to Follow Image Editing Instructions. arXiv preprint arXiv:2211.09800 .
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-
fulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural
Information Processing Systems . 1877â€“1901.
SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,
Ece Kamar, Peter Lee, Yin-Tat Lee, Yuanzhi Lee, Scott Lundberg, Harsha Nori,
Hamid Palangi, Macro T. Ribeiro, and Yi Zhang. 2023. Sparks of Artificial General
Intelligence: Early Experiments with GPT-4. arXiv preprint arXiv:2303.12712 .
Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. 2020.
DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation.
InAdvances in Neural Information Processing Systems . 16351â€“16361.
Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu
Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein,
Yuanzhen Li, and Dilip Krishnan. 2023. Muse: Text-To-Image Generation via Masked
Generative Transformers. arXiv preprint arXiv:2301.00704 .
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. 2022. MaskGIT:
Masked Generative Image Transformer. In IEEE Conference on Computer Vision and
Pattern Recognition . 11315â€“11325.
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya
Sutskever. 2020. Generative Pretraining from Pixels. In International Conference on
Machine Learning . 1691â€“1703.
Louis ClouÃ¢tre and Marc Demers. 2019. FIGR: Few-shot Image Generation with Reptile.
arXiv preprint arXiv:1901.02199 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv
preprint arXiv:1810.04805 .
Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang
Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. CogView: Mastering
Text-to-Image Generation via Transformers. In Advances in Neural Information
Processing Systems . 19822â€“19835.
Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. 2022. CogView2: Faster and
Better Text-to-Image Generation via Hierarchical Transformers. arXiv preprint
arXiv:2204.14217 .
Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming Transformers for High-
Resolution Image Synthesis. In IEEE Conference on Computer Vision and Pattern
Recognition . 12873â€“12883.
Kevin Frans, Lisa B. Soros, and Olaf Witkowski. 2021. CLIPDraw: Exploring
Text-to-Drawing Synthesis through Language-Image Encoders. arXiv preprint
arXiv:2106.14843 .
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi
Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative
Model for Code Infilling and Synthesis. In International Conference on Learning
Representations .
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In
Advances in Neural Information Processing Systems . 2672â€“2680.
David Ha and Douglas Eck. 2017. A Neural Representation of Sketch Drawings. arXiv
preprint arXiv:1704.03477 .
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to
a Local Nash Equilibrium. In Advances in Neural Information Processing Systems .
6629â€“6640.Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic
Models. In Advances in Neural Information Processing Systems . 6840â€“6851.
Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon,
Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and
Douglas Eck. 2018. Music Transformer. arXiv preprint arXiv:1809.04281 .
Ajay Jain, Amber Xie, and Pieter Abbeel. 2022. VectorFusion: Text-to-SVG by Abstract-
ing Pixel-Based Diffusion Models. arXiv preprint arXiv:2211.11319 .
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris,
and Taesung Park. 2023. Scaling up GANs for Text-to-Image Synthesis. arXiv
preprint arXiv:2303.05511 .
Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization.
arXiv preprint arXiv:1412.6980 .
Diederik P. Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes. arXiv
preprint arXiv:1312.6114 .
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neural Speech
Synthesis with Transformer Network. In AAAI Conference on Artificial Intelligence .
6706â€“6713.
Raphael G. Lopes, David Ha, Douglas Eck, and Jonathon Shlens. 2019. A Learned
Representation for Scalable Vector Graphics. In IEEE International Conference on
Computer Vision . 7930â€“7939.
Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and
Humphrey Shi. 2022. Towards Layer-wise Image Vectorization. In IEEE Conference
on Computer Vision and Pattern Recognition . 16314â€“16323.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob
McGrew, Ilya Sutskever, and Mark Chen. 2021. GLIDE: Towards Photorealistic
Image Generation and Editing with Text-Guided Diffusion Models. arXiv preprint
arXiv:2112.10741 .
OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 .
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. Dreamfusion:
Text-to-3D using 2D Diffusion. arXiv preprint arXiv:2209.14988 .
Steve Probets, Julius Mong, David Evans, and David Brailsford. 2001. Vector Graphics:
From PostScript and Flash to SVG. In ACM Symposium on Document Engineering .
135â€“143.
Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao. 2019. MirrorGAN: Learning
Text-to-image Generation by Redescription. In IEEE Conference on Computer Vision
and Pattern Recognition . 1505â€“1514.
Alec Radford, Jong-Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From
Natural Language Supervision. In International Conference on Machine Learning .
8748â€“8763.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
2019. Language Models are Unsupervised Multitask Learners. OpenAI blog 1, 8, 9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer
Learning with a Unified Text-to-Text Transformer. The Journal of Machine Learning
Research 21, 1, 5485â€“5551.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford,
Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. In
International Conference on Machine Learning . 8821â€“8831.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and
Honglak Lee. 2016. Generative Adversarial Text to Image Synthesis. In International
Conference on Machine Learning . 1060â€“1069.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
2022. High-Resolution Image Synthesis with Latent Diffusion Models. In IEEE
Conference on Computer Vision and Pattern Recognition . 10684â€“10695.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton,
Seyed K. S. Ghasemipour, Burcu-Karagol Ayan, S. Sara Mahdavi, Rapha G. Lopes, Tim
Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic
Text-to-Image Diffusion Models with Deep Language Understanding. In Advances
in Neural Information Processing Systems . 36479â€“36494.
Peter Selinger. 2003. Potrace: A polygon-based tracing algorithm.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-Read
Students Learn Better: On the Importance of Pre-training Compact Models. arXiv
preprint arXiv:1908.08962v2 .
Rafael Valle, Kevin Shih, Ryan Prenger, and Bryan Catanzaro. 2020. Flowtron: An
Autoregressive Flow-based Generative Network for Text-to-Speech Synthesis. arXiv
preprint arXiv:2005.05957 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.
Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In
Advances in Neural Information Processing Systems . 6000â€“6010.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang,
and Xiaodong He. 2018. AttnGAN: Fine-Grained Text to Image Generation with
Attentional Generative Adversarial Networks. In IEEE Conference on Computer Vision
and Pattern Recognition . 1316â€“1324.

--- PAGE 13 ---
IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers â€¢13
Xiang Xu, Karl D. D. Willis, Joseph G. Lambourne, Chin-Yi Cheng, Pradeep K. Jayara-
man, and Yasutaka Furukawa. 2022. SkexGen: Autoregressive Generation of CAD
Construction Sequences with Disentangled Codebooks. In International Conference
on Machine Learning . 24698â€“24724.
Jiahui Yu, Yuanzhong Xu, Jing-Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay
Vasudevan, Alexander Ku, Yinfei Yang, Burcu-Karagol Ayan, Ben Hutchinson, Wei
Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. 2022.
Scaling Autoregressive Models for Content-Rich Text-to-Image Generation. arXiv
preprint arXiv:2206.10789 .
Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang,
and Dimitris N. Metaxas. 2017. StackGAN: Text to Photo-realistic Image Synthesiswith Stacked Generative Adversarial Networks. In IEEE International Conference on
Computer Vision . 5907â€“5915.
Zhu Zhang, Jianxin Ma, Chang Zhou, Rui Men, Zhikang Li, Ming Ding, Jie Tang,
Jingren Zhou, and Hongxia Yang. 2021. M6-UFC: Unifying Multi-Modal Controls
for Conditional Image Synthesis via Non-Autoregressive Generative Transformers.
arXiv preprint arXiv:2105.14211 .
Zizhao Zhang, Yuanpu Xie, and Lin Yang. 2018. Photographic Text-to-Image Synthesis
with a Hierarchically-nested Adversarial Network. In IEEE Conference on Computer
Vision and Pattern Recognition . 6199â€“6208.
