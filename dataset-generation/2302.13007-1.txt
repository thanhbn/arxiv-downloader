# 2302.13007.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/dataset-generation/2302.13007.pdf
# File size: 1068294 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
AugGPT: Leveraging ChatGPT for Text Data
Augmentation
Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin Zhao,
Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li,
Dinggang Shen, Tianming Liu, and Xiang Li
Abstract ‚ÄîText data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language
processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain
is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data
augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods
either can‚Äôt ensure the correct labeling of the generated data (lacking faithfulness) or can‚Äôt ensure sufÔ¨Åcient diversity in the generated
data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT,
which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on
ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically
different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning
text classiÔ¨Åcation tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation
methods in terms of testing accuracy and distribution of the augmented samples.
Index Terms ‚ÄîLarge language model, few-shot learning, natural language processing, data augmentation.
F
1 I NTRODUCTION
THEeffectiveness of natural language processing (NLP)
heavily relies on the quality and quantity of the train-
ing data. With limited training data available, which is a
common issue in practice due to privacy concerns or the
cost of annotations, it can be challenging to train an accu-
rate NLP model that generalizes well to unseen samples.
The challenge of training data insufÔ¨Åciency is especially
  These authors contributed equally to this paper.
Haixing Dai, Zhengliang Liu, Zihao Wu, Lin Zhao, Shaochen Xu, Ning-
hao Liu, and Tianming Liu are with the School of Computing, University
of Georgia, Athens, GA, USA. (e-mail: fhd54134, zl18864 ,zw63397,
lin.zhao, shaochen.xu25, ninghao.liu, tliu g@uga.edu).
Wenxiong Liao, Xiaoke Huang, Hongmin Cai are with the
School of Computer Science and Engineering, South China Uni-
versity of Technology, China. (e-mail: fcswxliao@mail.scut.edu.cn,
csxkhuang@mail.scut.edu.cn, hmcai@scut.edu.cn).
Yihan Cao and Lichao Sun are with the Department of Computer Sci-
ence and Engineering, Lehigh University, Bethlehem, P A, USA. (e-mail:
fyihanc@andrew.cmu.edu, lis221@lehigh.edu).
Yihan Cao is also with the Heinz College of Information Systems and
Public Policy, Carnegie Mellon University, Pittsburgh, P A, USA (e-mail:
femail yihanc@andrew.cmu.edu).
Wei Liu is with the Department of Radiation Oncology, Mayo Clinic,
Phoenix, AZ, USA. (e-mail: liu.wei@mayo.edu)
Sheng Li is with the School of Data Science, University of Virginia,
Charlottesville, V A, USA. (email: shengli@virginia.edu)
Dajiang Zhu is with the Department of Computer Science and Engineer-
ing, The University of Texas at Arlington, Arlington, TX, USA. (e-mail:
dajiang.zhu@uta.edu)
Quanzheng Li and Xiang Li are with the Department of Radiol-
ogy, Massachusetts General Hospital and Harvard Medical School,
Boston, MA, USA. (e-mail: li.quanzheng@mgh.harvard.edu, xian-
gli.shaun@gmail.com)
Dinggang Shen is with the School of Biomedical Engineering, Shang-
haiTech University, Shanghai 201210, China. He is also with Shanghai
United Imaging Intelligence Co., Ltd., Shanghai 200230, China, and
Shanghai Clinical Research and Trial Center, Shanghai, 201210, China.
(e-mail: Dinggang.Shen@gmail.com)prominent in few-shot learning (FSL) scenarios, where the
model trained on the original (source) domain data is
expected to generalize from only a few examples in the
new (target) domain [1]. Many FSL methods have shown
promising results in overcoming this challenge in various
tasks. Existing FSL methods mainly focus on improving
the learning and generalization capability of the model via
better architectural design [2], [3], leveraging pre-trained
language models as the basis and then Ô¨Åne-tuning it using
limited samples [4] with meta-learning [2], [5] or prompt-
based methods [6], [7], [8], [9]. However, the performance of
these methods is still intrinsically limited by the data quality
and quantity in both the source and target domains.
Besides model development, text data augmentation can
also overcome the sample size limit and work together with
other FSL methods in NLP [10], [11]. Data augmentation
is usually model-agnostic and involves no change to the
underlying model architecture, which makes this approach
particularly practical and applicable to a wide range of
tasks. In NLP , there are several types of data augmentation
methods. Traditional text-level data augmentation methods
rely on direct operations on the existing sample base. Some
frequently used techniques include synonym replacement,
random deletion, and random insertion [12]. More recent
methods utilize language models to generate reliable sam-
ples for more effective data augmentation, including back-
translation [13] and word vector interpolation in the latent
space [14]. However, existing data augmentation methods
are limited in the accuracy and diversity of the generated
text data, and human annotation is still mandatory in many
application scenarios [12], [15], [16].
The advent of (very) large language models (LLMs) such
as the GPT family [6], [17] brings new opportunities for gen-arXiv:2302.13007v3  [cs.CL]  20 Mar 2023

--- PAGE 2 ---
2
erating text samples that resemble human-labeled data [18],
which signiÔ¨Åcantly alleviates the burden of human anno-
tators [19]. LLMs are trained in self-supervised manners,
which scale up with the amount of text corpus available in
the open domains. The large parameter space of LLMs also
allows them to store a large amount of knowledge, while
large-scale pre-training (e.g., the autoregressive objective in
training GPTs) enables LLMs to encode rich factual knowl-
edge for language generation even in very speciÔ¨Åc domains
[20]. Furthermore, the training of ChatGPT follows that
of Instruct-GPT [21], which utilizes reinforcement learning
with human feedback (RLHF), thus enabling it to produce
more informative and impartial responses to input.
Inspired by the success of language models in text
generation, we propose a new data augmentation method
named AugGPT, which leverages ChatGPT to generate aux-
iliary samples for few-shot text classiÔ¨Åcation. We test the
performance of AugGPT via experiments on both general
domain and medical domain datasets. Performance compar-
ison of the proposed AugGPT approach with existing data
augmentation methods shows double-digit improvements
in sentence classiÔ¨Åcation accuracy. Further investigation
into the faithfulness and compactness of the generated text
samples reveals that AugGPT can generate more diversiÔ¨Åed
augmented samples while simultaneously maintaining their
accuracy (i.e., semantic similarity to the original labels). We
envision that the development of LLMs will lead to human-
level annotation performance, thus revolutionizing the Ô¨Åeld
of few-shot learning and other tasks in NLP .
2 R ELATED WORK
2.1 Data Augmentation
Data augmentation, the artiÔ¨Åcial generation of new text
through transformations, is widely used to improve model
training in text classiÔ¨Åcation. In NLP , existing data aug-
mentation methods work at different granularity levels:
characters, words, sentences, and documents.
Data augmentation at the character level refers to the
randomly inserting, exchanging, replacing, or deleting of
characters in the text [22], which improves the robustness
of the NLP model against noises. Another method called
optical character recognition (OCR) data augmentation gen-
erates new text by simulating the errors that occur when
using OCR tools to recognize text from pictures. Spelling
augmentation [23] deliberately misspells some frequently
misspelled words. Keyboard augmentation [22] simulates
random typo errors by replacing a selected key with another
key close to it on the QWERTY layout keyboard.
Data augmentation also works at the word level. Ran-
dom swap augmentation randomly exchanges two words
in the text, and random deletion augmentation randomly
deletes some words [24]. Synonym augmentation uses syn-
onym databases such as PPDB [25] to replace randomly
selected words [26]. WordNet [27] is also widely used as a
reference for synonym augmentation. These methods main-
tain semantic consistency and are suitable for text classiÔ¨Å-
cation tasks. Wang et al. [28] proposed a data augmentation
method based on word embeddings, replacing words with
their top-nsimilar words to create a new sentence. Differentpre-trained word embeddings are considered (e.g., Google-
News Lexical Embeddings [29]). This method is based on
the principle that words close to each other in the embed-
ding space often appear in similar contexts, which might
help with maintaining grammatical consistency.
However, a serious limitation of word embedding-based
methods is that close words in the embedding space are
not necessarily semantically similar, yet semantic changes
can affect the classiÔ¨Åcation results. For example, ‚Äúhot‚Äù and
‚Äúcold‚Äù usually appear in similar contexts, so their word em-
beddings are close, but they have exactly opposite seman-
tic meanings. The counter-Ô¨Åtting embedding augmentation
[30], [31] solves this problem by using a synonym dictio-
nary and an antonym dictionary to adjust the initial word
embeddings. SpeciÔ¨Åcally, the distance between embeddings
of synonyms will be shortened, and the distance between
embeddings of antonyms will become enlarged.
Contextual augmentation [32], [33] is another word-
level data augmentation method, which uses masked lan-
guage models (MLMs) such as BERT [34], [35], DistilBERT
[36] and RoBERTA [37] to generate new text based on the
context. SpeciÔ¨Åcally, they insert < mask > tokens in some
positions of the text, or replace some words in the text
with< mask > tokens, and then let the MLM predict
what words should be put in these masked positions. Since
MLMs are pre-trained on a large number of texts, contextual
augmentation can usually generate meaningful new texts.
Some text data augmentation methods work at the
sentence and document level. For example, back transla-
tion [38] uses translation models for data augmentation.
SpeciÔ¨Åcally, the language model Ô¨Årst translates the text into
another language and then translates it back to the original
language. Due to the randomness of the translation process,
the augmented text is different from the original text, but
semantic consistency is maintained. At the document level,
Gangal et al. [39] proposed a method to paraphrase the
entire document to preserve document-level consistency.
In general, regardless of the granularity level or the text
generation backbone (i.e., rule-based or language models),
the goal of data augmentation is to produce sensible and
diverse new samples that maintain semantic consistency.
2.2 Few-shot Learning
Deep learning has achieved remarkable success in various
data-intensive applications. However, the performance of
deep models could be affected if the dataset size is small
in the downstream tasks. Few-shot Learning is a branch
of science that focuses on developing solutions to address
the challenge of small sample sizes [1], [40]. FSL research
aims to leverage prior knowledge to rapidly generalize to
new tasks that contain only a few labeled samples. A classic
application scenario for few-shot learning is when obtaining
supervised examples is difÔ¨Åcult or not possible due to
privacy, safety, or ethical considerations. The development
of few-shot learning enables practitioners to improve the
efÔ¨Åciency and accuracy of text classiÔ¨Åcation in various sce-
narios and deploy practical applications.
Recent advances in few-shot learning have shown
promising results in overcoming the challenges of limited
training data for text classiÔ¨Åcation. For example, a common

--- PAGE 3 ---
3
Fig. 1. The framework of AugGPT. a (top panel): First, we apply ChatGPT for data augmentation. We input samples of all classes into ChatGPT
and prompt ChatGPT to generate samples that preserves semantic consistency with existing labelled instance. b (bottom panel): In the next
step, we train a BERT -based sentence classiÔ¨Åer on the few-shot samples and the generated data samples and evaluate the model‚Äôs classiÔ¨Åcation
performance.
approach in NLP is to use a pre-trained language model
such as BERT [4] as a starting point and then Ô¨Åne-tune it
with limited samples. Some of the most recent methodolog-
ical developments [2], [41] approaches that have gained
traction include prompt-tuning [6], [7], [8], [9] and meta-
learning [2], [5]. In general, existing FSL methods target
either architectural design [2], [3], data augmentation [10],
[11] or the training process [42].
Despite the recent development of prompt-tuning and
meta-learning methods, they suffer from some major limi-
tations. For example, prompt engineering is a cumbersome
art that requires extensive experience and manual trial-and-
errors [43]. Meta-learning, on the other hand, suffers from
problems such as training instability [44], [45], [46] and sen-
sitivity to hyper-parameters [44], [45]. In addition, all these
FSL pipelines demand deep machine learning expertise and
acquaintance with complex model architectures and train-
ing strategies, which are not attainable by common prac-
titioners and general developers. As discussed in section
2.1, data augmentation is an effective solution for FSL and
can be combined with other FSL models. Thus, the AugGPT
method proposed in this paper, which has demonstrated the
capability to generate accurate and comprehensive training
samples, can overcome the issues of current FSL methods
and potentially change the landscape of few-shot learning
in NLP .
2.3 Very Large Language Models
Pre-trained language models (PLMs) based on the trans-
former architecture, such as the BERT [4] and GPT [47]model families, have revolutionized natural language pro-
cessing. Compared to previous methods, they deliver state-
of-the-art performance on a wide range of downstream tasks
and contribute to the rising popularity and democratization
of language models. In general, there are three classes of pre-
trained language models: autoregressive language models
(e.g., the decoder-based GPT), masked language models
(e.g., the encoder-based BERT), and encoder-decoder mod-
els(e.g., BART [48] and T5 [49]). These models typically
contain between 100M and 1B parameters [17].
In recent years, NLP communities have witnessed the
rise of very large language models such as GPT-3 (175B
parameters) [6], PaLM (540B parameters) [50], Bloom (176B
parameters) [51], OPT (up to 175B parameters) [52], and
the FLAN series (FLAN has 137B parameters) [53]. At their
core, these large language models are transformer models
inspired by BERT and GPT, albeit at a much larger scale.
Large language models aim to learn accurate latent fea-
ture representations of input text. These representations are
often context-dependent and domain-dependent. For exam-
ple, the vector representation of the word ‚Äùtreat‚Äù might be
vastly different between medical domains and the general
domain. For smaller pre-trained language models, it is often
necessary to continuously pre-train and Ô¨Åne-tune such mod-
els to attain acceptable performance [54]. However, very
large language models can potentially eliminate the need for
Ô¨Åne-tuning while maintaining competitive performance [6],
[18], [55].
Existing studies indicate that pre-trained language mod-
els can help augment a dataset with new samples with

--- PAGE 4 ---
4
TABLE 1
Generated Samples From ChatGPT on PubMed20K
Original Text Altered Text
The follow-up rate after 5 years was 85%.1. The study showed a follow-up rate of 85% after 5 years.
2. The 5-year follow-up rate was found to be signiÔ¨Åcantly high at 85%.
3. The participants had an impressive follow-up rate of 85% after 5 years.
4. The data indicated a follow-up rate of 85% 5 years post-study.
5. The 5-year follow-up rate was recorded as 85%, according to the Ô¨Åndings.
6. The results of the study showed a follow-up rate of 85% after a 5-year period.
This means that the maturation of calluses
was 27% faster in the LIPUS group.1. The LIPUS group showed a 27% acceleration in callus maturation.
2. The results revealed that the callus maturation process was 27% faster in the LIPUS group.
3. The study found that the callus maturation rate in the LIPUS group was 27% quicker.
4. Callus maturation was 27% more rapid in the LIPUS group compared to others.
5. The callus maturation process in the LIPUS group was 27% more efÔ¨Åcient, according to the
results.
Test Registration: IRCT.ir
IRCT2012071010230N1.1. The trial was registered with the IRCT under the identiÔ¨Åer IRCT2012071010230N1.
2. The study was ofÔ¨Åcially registered with IRCT.ir, with the registration number
IRCT2012071010230N1.
3. The trial was documented with IRCT.ir with the identiÔ¨Åcation code IRCT2012071010230N1.
4. IRCT.ir served as the ofÔ¨Åcial registrar for the trial, with the registration number
IRCT2012071010230N1.
5. The study was recorded with IRCT under the registration number IRCT2012071010230N1.
6. IRCT.ir recorded the trial with the identiÔ¨Åer IRCT2012071010230N1.
While behavioral and technological
interventions can lead to some modest
improvements in glycemic control,
these interventions have not performed
much better than conventional prevention
in achieving glycemic control.1. The study found that although behavioral and technological interventions led to some
slight improvements in glycemic control, they were not signiÔ¨Åcantly more effective
than typical care.
2. Despite the modest improvement in glycemic control through behavioral and
technological interventions, they did not perform better than the standard care.
3. The results showed that while behavioral and technological interventions resulted
in some minimal gains in glycemic control, they did not surpass the usual care in
achieving glycemic control.
4. Although behavioral and technological interventions showed some improvement
in glycemic control, they were not found to be signiÔ¨Åcantly superior to the usual care.
5. The study showed that the usual care was not outperformed by behavioral and
technological interventions in terms of achieving glycemic control, despite some
small improvements.
similar semantic meaning [12], [16], which is of signiÔ¨Åcant
practical value to real-world applications. In this study,
we aim to use ChatGPT, a popular LLM to conduct data
augmentation. ChatGPT is based on GPT-3 [6], which was
trained on massive web data with diverse and rich infor-
mation. Furthermore, ChatGPT was trained through Rein-
forcement learning from Human Feedback (RLHF). During
RLHF, human feedback is incorporated into the process of
generating and selecting the best results. More speciÔ¨Åcally,
a reward model is trained based on human annotators‚Äô
ranking or generated results. In turn, this reward model
rewards model outputs that are most aligned with human
preference and human values. We believe these innovations
make ChatGPT the best candidate for generating human-
level quality data samples.
2.4 ChatGPT: Present and Future
ChatGPT is a game changer in natural language processing.
For the Ô¨Årst time in human history, the power of large
language models is accessible to the general public through
a user-friendly chatbot interface. In turn, this common acces-
sibility contributes to ChatGPT‚Äôs unprecedented popularity.
ChatGPT has emerged as a general-purpose problem solver
for many NLP applications [56]. Qin et al. [56] evaluated
ChatGPT on a comprehensive set of NLP tasks, includ-
ing common benchmarks in natural language inference,
arithmetic reasoning, named entity recognition, sentiment
analysis, question answering, dialogue and summarization.They conclude that ChatGPT excels in most tasks, except for
tasks that focus on speciÔ¨Åc details (e.g., sequence tagging).
ChatGPT is also a valuable solution for multilingual
tasks. A recent empirical study [57] reports that ChatGPT
excels at tasks involving high-resource languages (various
European languages and Chinese) and is comparable with
Google Translate, DeepL Translate and Tencent TranSmart.
Nonetheless, ChatGPT performs poorly on low-resource
languages and faces extra challenges handling distant lan-
guage translation (i.e., English-German translation is con-
sidered to be less ‚Äùdistant‚Äù, compared to English-Hindi
translation). A later study [58] conÔ¨Årms that ChatGPT strug-
gles with low-resource languages, although the authors
observe that ChatGPT does better in understanding non-
Latin scripts than generating them.
In addition, it is also possible to use the purely text-
based ChatGPT to interact with multi-modal data. A group
of researchers [58] use HTML Canvas and Python Turtle
graphics as media for text-to-image generation. ChatGPT
can faithfully generate HTML and Python code, which
can be then used to generate desired images. The authors
designed a Ô¨Çag drawing task that required ChatGPT to
generate code that can generate country Ô¨Çags. It was found
that ChatGPT could generate better Ô¨Çags when the prompt
for code was preceded by a prompt that queries ChatGPT
for the Ô¨Çag‚Äôs description. In other words, descriptive text
prompts could improve multimodal task performance.
Beyond computer science, ChatGPT can be readily ap-
plied to medical report generation and comprehension [59],

--- PAGE 5 ---
5
[60], education [61], [62], [63], rigorous math research [64]
and Ô¨Ånance [65]. Overall, ChatGPT is a versatile tool that
promotes general AI usage.
However, researchers are also cautious about the possi-
ble negative impact of ChatGPT. Some of the more promi-
nent concerns are related to bias [66], [67], ethics [68], [69],
plagiarism [70], [71] and job replacement en masse [72], [73].
In response, a commentary published in Nature advocates
for urgent attention to accountability, open-source large
language models and societal embrace of AI [66].
3 D ATASET
We Ô¨Årst use an open domain dataset Amazon to verify the
effectiveness of our method. Then, we use clinical natural
language processing (clinical NLP) as the task and carry out
our experiments on two popular public benchmarks. Data
augmentation is particularly in demand in clinical NLP ,
because the signiÔ¨Åcant burden of expert annotation and
stringent privacy regulations make large-scale data labeling
infeasible. We will describe these datasets in detail in the
following sections.
3.1 Amazon dataset
Amazon [74], [75], [76] contains customer reviews from 24
product categories. The task is to classify reviews into their
respective product categories. Since the original Amazon
product dataset is proverbially large, we sample a subset
of 300 samples from each category.
3.2 Symptoms Dataset
This dataset is published on Kaggle1. It contains the audio
data of common medical symptom descriptions over 8
hours. We use the text transcripts corresponding to the au-
dio data and perform sample de-duplication, and use them
as model input. The dataset after preprocessing includes 231
samples of 7 symptom categories. Every example represents
a sentence describing the provided symptoms, and the task
is to classify the sentence into the corresponding symptoms.
3.3 PubMed20k Dataset
The PubMed20K dataset is an extensively utilized resource
in NLP and text mining research, comprising around 20,000
annotated scientiÔ¨Åc abstracts from the biomedical Ô¨Åeld.
These annotations encompass named entities, relationships
between entities, and various semantic roles, making the
dataset valuable for diverse NLP tasks such as named entity
recognition, relation extraction, and text classiÔ¨Åcation. The
dataset originates from the PubMed database, which spans
a wide array of biomedical subjects. Owing to its substantial
size, variety, and high-quality annotations, PubMed20K has
emerged as a popular benchmark dataset for assessing the
performance of machine learning models in the realm of
biomedical NLP . The abstracts in the PubMed 20K dataset
undergo preprocessing and segmentation into individual
sentences. Each sentence is labeled with one of the following
Ô¨Åve categories: background, objective, method, result, or
conclusion. The task is to map the input sentences to their
corresponding categories.
1. https://www.kaggle.com/datasets/paultimothymooney/medical-
speech-transcription-and-intentAlgorithm 1 The framework of AugGPT for few-shot text
classiÔ¨Åcation.
Input : base dataset Dband novel dataset Dn
Initialize : Initialized pre-trained BERT model
DeÔ¨Ånition :D0is the dataset with the base dataset Db
and augmented dataset Daug
n, and chatGPT augis the data
augmentation method based on ChatGPT
Parameters : Fine-tuning epochs of base dataset epochb, Ô¨Åne-
tuning epochs of FSL epochf
forepoch inepochbdo
train(model ,Db)
end for
Daug
n=chatGPT aug(Dn)
forepoch inepochfdo
train(model ,Daug
n)
end for
4 M ETHOD
4.1 Overall Framework
Given a base dataset Db=f(xi;yi)gNb
i=1with a label
spaceyi2Yb, a novel dataset Dn=f(xj;yj)gNn
j=1with
a label space yj2Yn, andYb\Yn=;. In the few-shot
classiÔ¨Åcation scenario, the base dataset Dbhas a relatively
larger set of labeled samples, while the novel dataset Dn
has only a few labeled samples. The performance of few-
shot learning is evaluated on the novel dataset. Our goal is
to train a model with both base and limited novel datasets,
while achieving satisfying generalizability on the novel
dataset.
The overall framework of AugGPT is shown in Fig 1,
and the training steps are shown in Algorithm 1. First of
all, we Ô¨Åne-tune BERT on Db. Then, the Daug
nis generated
by data augmentation with ChatGPT. Finally, we Ô¨Åne-tune
BERT withDaug
n.
4.2 Data Augmentation with ChatGPT
Similar to GPT [47], GPT-2 [77], and GPT-3 [6], ChatGPT
belongs to the family of autoregressive language models and
uses transformer decoder blocks [78] as the model backbone.
During pre-training, ChatGPT is regarded as an un-
supervised distribution estimation from a set of samples
X=fx1;x2;:::;xng, and sample xicomposed of mtokens
is deÔ¨Åned as xi= (s1;s2;:::;sm). The objective of pre-
training is to maximize the following likelihood:
L(xi) =mX
i=1logP(sijs1;:::;si 1;) (1)
whererepresents the trainable parameters of ChatGPT.
The tokens are represented by token embedding and posi-
tion embedding:
h0=xiWe+Wp (2)
whereWeis the token embedding matrix and Wpis the
position embedding matrix. Then Ntransformer blocks are
used to extract the features of the sample:
hn=transformer blocks (hn 1) (3)
wheren2[1;N].

--- PAGE 6 ---
6
Finally, the target token is predicted:
si=softmax (hNWT
e) (4)
wherehNis the output of top transformer blocks.
After pre-training, the developers of ChatGPT apply
Reinforcement Learning from Human Feedback (RLHF) [21]
to Ô¨Åne-tune the pre-trained language model. The RLHF
aligns language models with user intent on a wide range
of tasks by Ô¨Åne-tuning them according to human feedback.
The RLHF of ChatGPT contains three steps:
Supervised Fine-tuning (SFT) : Unlike GPT, GPT-2, and
GPT-3, ChatGPT uses labeled data for further training. The
AI trainers play as users and AI assistants to build the
answers based on prompts. The answers with prompts are
used as supervised data for further training of the pre-
trained model. After further pre-training, a SFT model can
be obtained.
Reward Modeling (RM) : Based on the SFT method, a
reward model is trained to take in a pair of prompt and
response, and output a scalar reward. Human labelers rank
the outputs from best to worst to build a ranking dataset.
The loss function between two outputs is deÔ¨Åned as follows:
loss(r) =E(x;yw;yl)Dc[log ((rr(x;yw) rr(x;yl)))]
(5)
whereris the parameters of reward model; xis the prompt,
ywis the preferred completion out of the pair of ywandyl;
Dcis the dataset of human comparisons.
Reinforcement Learning (RL) : By using reward models,
ChatGPT can be Ô¨Åne-tuned using Proximal Policy Optimiza-
tion (PPO) [79]. In order to Ô¨Åx the performance degradation
on public NLP datasets, the RLHF mixes the pretraining
gradients into the PPO gradients, which is also known as
PPO-ptx:
objective() =ExDpretrain
log 
RL
(x)
+
E(x;y)DRL

rr(x;y) log 
RL
(yjx)=SFT(yjx)
(6)
whereRL
is the learned RL policy, SFT is the supervised
trained model, and Dpretrain is the pretraining distribu-
tion. Theis the pre-training loss coefÔ¨Åcient that controls
the strength of pre-training gradients, and the is the
KL (Kullback-Leibler) reward coefÔ¨Åcient that controls the
strength of the KL penalty.
Compared to previous data augmentation methods,
ChatGPT is more suitable for data augmentation for the
following reasons:
ChatGPT is pre-trained on large-scale corpora, so it
has a broader semantic expression space, and is help-
ful to enhance the diversity of data augmentation.
Since the Ô¨Åne-tuning stage of ChatGPT introduces
a large number of manual annotation samples, the
language generated by ChatGPT is more in line with
human expression habits.
Through reinforcement learning, ChatGPT can com-
pare the advantages and disadvantages of different
expressions and ensure that the generated data are
of high quality.
Under the BERT framework, we introduce ChatGPT as
the data augmentation tool for few-shot text classiÔ¨Åcation.SpeciÔ¨Åcally, ChatGPT is applied to rephrase each input
sentence into six additional sentences, thereby augmenting
the few-shot samples.
4.3 Few-shot Text ClassiÔ¨Åcation
We apply BERT [80] to train a few-shot text classiÔ¨Åcation
model. The output features hof the top layer of BERT can
be written as:
z= [zc;z1;z2;:::;zn]; (7)
wherezcis the representation of the class-speciÔ¨Åc token
CLS. For text classiÔ¨Åcation, zcis usually fed into a task-
speciÔ¨Åc classiÔ¨Åer header for Ô¨Ånal prediction. However, in
the FSL scenario, it is difÔ¨Åcult to achieve satisfactory perfor-
mance through BERT Ô¨Åne-tuning because the small scale of
few-shot samples will easily lead to over-Ô¨Åtting and lack of
generalization ability.
To effectively address the challenge of few-shot text
classiÔ¨Åcation, many approaches have been proposed. Gen-
erally, there are four categories of methods for few-shot
text classiÔ¨Åcation based on large language models: meta-
learning, prompt-tuning, model design, and data augmen-
tation. meta-learning refers to the process of learning to learn
with tasks that update meta-parameters [2], [5]. Prompt-
based methods guide large language models to predict
correct results by designing templates [6], [7], [8], [9]. Model
design methods guide the model to learn from few-shot
samples by changing the structure of the model [81]. Data
augmentation uses similar characters [22], similar word
semantics [30], [31], or knowledge base [55], [82] to expand
samples. Our method directly data augmentation through
the language capabilities of large language models, which is
a simple and efÔ¨Åcient data augmentation method.
Objective Function : Our objective function of few-shot
learning consists of two parts: cross entropy and contrastive
learning loss. We feed zcinto a fully connected layer, the
classiÔ¨Åer for the Ô¨Ånal prediction:
^y=WT
czc+bc; (8)
whereWcandbcare trainable parameters, and take cross-
entropy as one of the objective functions:
LCE= X
d2D0CX
c=1ydcln ^ydc; (9)
whereCis the output dimension, which is equal to the
union of label spaces of the base dataset and novel dataset,
andydis the ground truth.
Then, to make full use of the prior knowledge in the
base dataset to guide the learning of the novel dataset, we
introduce the contrastive loss function to make the sample
representation of the same category more compact and the
sample representation of different categories more separate.
The contrastive loss between pairs of samples in the same
batch is deÔ¨Åned as follows:
LCL= logPecos(vi;vi0)
Pecos(vi;vi0)+Pecos(vi;vj); (10)
whereviandv0
iare thezcof samples that belong to the same
category;viandvjare thezcof samples belong to different
categories;cos(;)is the cosine similarity.

--- PAGE 7 ---
7
In the BERT Ô¨Åne-tuning stage on the base dataset, we
only use cross entropy as the objective function. In the
few-shot learning stage, we combine cross entropy and
contrastive learning loss as the objective function:
L=LCE+LCL: (11)
4.4 Baseline Methods
In the experiment section, we compare our method with
other popular data augmentation methods. For these meth-
ods, we use the implementation in open-source libraries
including, nlpaug [83] and textattack [84].
InsertCharAugmentation . This method inserts ran-
dom characters at random locations in text, which
improves the generalization ability of the model by
injecting noise into the data.
SubstituteCharAugmentation . This method ran-
domly replaces selected characters with other ones.
SwapCharAugmentation [22]. This method ran-
domly exchanges two characters.
DeleteCharAugmentation . This method randomly
deletes characters.
OCRAugmentation . OCRAugmentation simulates
possible errors during OCR recognition. For exam-
ple, OCR tool may wrongly identify ‚Äú0‚Äù as ‚Äúo‚Äù, and
wrongly identify ‚ÄúI‚Äù as ‚Äúl‚Äù.
SpellingAugmentation [23]. It creates new text by
deliberately misspelling some words. The method
uses a list of English words that are most likely to
be misspelled provided by Oxford Dictionary, for
example, misspelling ‚Äúbecause‚Äù as ‚Äúbecouse‚Äù.
KeyboardAugmentation [22]. It simulates typo error
by replacing randomly selected characters with the
adjacent characters in the QWERTY layout keyboard.
For example, replacing ‚Äòg‚Äô with ‚Äòr‚Äô, ‚Äòt‚Äô, ‚Äòy‚Äô, ‚Äòf‚Äô, ‚Äòh‚Äô, ‚Äòv‚Äô,
‚Äòb‚Äô or ‚Äòn‚Äô.
SwapWordAug [24]. It randomly exchanges words
in text. This method is a submethod of Easy Data
Augmentation (EDA) proposed by Wei et al.
DeleteWordAug . DeleteWordAug randomly deletes
words in the text, which is also a submethod of EDA.
PPDBSynonymAug [26]. It replaces words with their
synonym in PPDB thesaurus. Synonym replacement
can ensure semantic consistency and is suitable for
classiÔ¨Åcation tasks.
WordNetSynonymAug . It replaces words with their
synonym in WordNet thesaurus.
SubstituteWordByGoogleNewsEmbeddings [28]. It
replaces words with their top- nsimilar words in the
embedding space. The word embeddings used are
pre-trained with GoogleNews corpus.
InsertWordByGoogleNewsEmbeddings [83]. It ran-
domly selects word from vocabulary of GoogleNews
corpus and inserts it the random position of the text.
CounterFittedEmbeddingAug [30], [31]. It replaces
words with their neighbors in counter-Ô¨Åtting em-
bedding space. Compared with GoogleNews word
vectors used by SubstituteWordByGoogleNewsEm-
beddings, counter-Ô¨Åtting embedding introduces the
constraint of synonyms and antonyms, that is, theembedding between synonyms will be pulled closer,
and vice versa.
ContextualWordAugUsingBert(Insert) [32], [33].
This method uses BERT to insert words based on
context, that is, add < mask > token at random
position of the input text, and then let BERT predict
the token at that position.
ContextualWordAugUsingDistilBERT(Insert) . This
method uses DistilBERT to replace BERT for predic-
tion, and the rest is the same as ContextualWordAu-
gUsingBert(Insert).
ContextualWordAugUsingRoBERTA(Insert) .
This method uses RoBERTA to replace BERT
for prediction, and the rest is the same as
ContextualWordAugUsingBert(Insert).
ContextualWordAugUsingBert(Substitute) . This
method [32], [33] uses BERT to replace words based
on context, that is, replace randomly selected words
in text with < mask > token, and then let BERT
predict the token at that position.
ContextualWordAugUsingDistilBERT(Substitute) .
This method uses DistilBERT to replace BERT
for prediction, and the rest is the same as
ContextualWordAugUsingBert(Substitute).
ContextualWordAugUsingRoBERTA(Substitute) .
This method uses RoBERTA to replace BERT
for prediction, and the rest is the same as
ContextualWordAugUsingBert(Substitute).
BackTranslationAug . The method [38] translates the
text into German and then into English, resulting
in a new text that is different from the original but
has the same semantics. We use wmt19-en-de and
facebook/wmt19-de-en language translation models
[85] developed by Facebook for translation.
4.5 Prompt Design
We have designed prompts for single-turn dialogue and
multi-turn dialogues. The prompts are shown in Fig 2. The
Amazon dataset use the multi-turn dialogues prompt for
data augmentation. The Symptoms and PubMed20K use the
single-turn dialogue prompt for data augmentation.
4.6 Evaluation Metrics
We employed cosine similarity and TransRate [86] as metrics
to assess the faithfulness (i.e., whether the generated data
samples are close to the original samples) and compactness
(i.e., whether samples of each class are compact enough for
good discrimination) of the augmented data.
4.6.1 Embedding Similarity
To evaluate the semantic similarity between the samples
generated by data augmentation methods and actual sam-
ples, we adopt embedding similarity between the generated
samples and the actual samples of the test dataset. Some
of the most common similarity metrics include Euclidean
distance, cosine similarity and dot product similarity. In this
study, we select cosine similarity to capture the distance re-
lationship in the latent space. The cosine similarity measures
the cosine value of the angle between two vectors. This
value increases when two vectors are more similar, and is

--- PAGE 8 ---
8
TABLE 2
Data Augmentation and Ablation Study. The BERT + C indicates BERT with contrastive loss.
Data AugmentationAmazon Symptoms PubMed20K
BERT BERT + C BERT BERT + C BERT BERT + C
Raw 0.734 0.745 0.636 0.606 0.792 0.798
BackTranslationAug 0.757 0.748 0.778 0.747 0.812 0.83
ContextualWordAugUsingBert(Insert) 0.761 0.750 0.697 0.677 0.802 0.811
ContextualWordAugUsingBert(Substitute) 0.770 0.757 0.626 0.667 0.815 0.830
ContextualWordAugUsingDistilBERT(Insert) 0.759 0.762 0.707 0.747 0.796 0.796
ContextualWordAugUsingDistilBERT(Substitute) 0.787 0.766 0.667 0.646 0.797 0.800
ContextualWordAugUsingRoBERTA(Insert) 0.775 0.768 0.758 0.707 0.815 0.814
ContextualWordAugUsingRoBERTA(Substitute) 0.745 0.730 0.727 0.667 0.782 0.782
CounterFittedEmbeddingAug 0.754 0.741 0.667 0.626 0.805 0.805
InsertCharAugmentation 0.771 0.775 0.404 0.475 0.826 0.831
InsertWordByGoogleNewsEmbeddings 0.816 0.794 0.636 0.677 0.786 0.784
KeyboardAugmentation 0.764 0.766 0.545 0.505 0.809 0.815
OCRAugmentation 0.775 0.782 0.768 0.778 0.789 0.789
PPDBSynonymAug 0.691 0.690 0.697 0.758 0.795 0.829
SpellingAugmentation 0.727 0.736 0.697 0.707 0.808 0.811
SubstituteCharAugmentation 0.762 0.768 0.535 0.586 0.816 0.821
SubstituteWordByGoogleNewsEmbeddings 0.729 0.741 0.727 0.727 0.807 0.822
SwapCharAugmentation 0.762 0.766 0.475 0.485 0.797 0.801
SwapWordAug 0.771 0.766 0.687 0.727 0.798 0.794
WordNetSynonymAug 0.805 0.798 0.616 0.758 0.761 0.757
ChatGPT (2-shot) 0.753 0.980 0.748
AugGPT 0.816 0.826 0.889 0.899 0.835 0.835
Single- turn dialogue Multi-turn dialogues
Please rephrase the following sentence: {text}You are a helpful assistant that rephrase text and make sentence smooth.
Iwill give you a sample, please rephrase it, then give me 6 rephrased answers.
Sure, please provide the sentence you would like me to rephrase.
{text}
System User Assistant
Fig. 2. Single-turn dialogue and multi-turn dialogues prompt
bounded by a range between 0 and 1. Since the pre-trained
language models without Ô¨Åne-tunning poorly to capture
semantic meaning, we Ô¨Åne-tunning the pre-trained BERT on
base dataset by BERT-Ô¨Çow [87] method, and Ô¨Ånally apply
the Ô¨Åne-tunned BERT to get smaple embedding. The cosine
similarity metric is commonly used in NLP [88] and we
follow this convention.
cos() =AB
kAk2kBk2; (12)
where A and B denote the two embedding vectors in com-
parison, respectively.
4.6.2 TransRate
TransRate is a metric that quantiÔ¨Åes transferability based
on the mutual information between the features extracted
by a pre-trained model and their labels, with a single pass
through the target data. The metric achieves a minimum
value when the data covariance matrices of all classes areidentical, making it impossible to distinguish between the
data from different classes and preventing any classiÔ¨Åer
from achieving better than random guessing. Thus, a higher
TransRate could indicate better learnability of the data. More
speciÔ¨Åcally, knowledge transfer from a source task Tsto a
target taskTtis measured as shown below:
TrRTs!Tt(g) =H(Z) H(ZjY); (13)
where Y represents the labels of augmented examples, and
Zdenotes the latency embedding features extracted by the
pre-trained feature extractor g.TrR means the TransRate
value.H()denotes the Shannon entropy [89].
4.7 Direct ClassiÔ¨Åcation Performance by ChatGPT
An interesting and important question about the utilization
of ChatGPT for text data augmentation would be how Chat-
GPT will perform when directly applied to FSL downstream
tasks. Thus, we developed tailored prompts for ChatGPT
to perform the classiÔ¨Åcation tasks with integrated the API

--- PAGE 9 ---
9
(a) Symptoms (b) PubMed20K
 (c) Amazon
Fig. 3. We employed two evaluation metrics to assess the faithfulness and compactness of our newly augmented data. The top left plot displays
the cosine similarity metric and Ô¨Ånal accuracy of all data augmentation methods on the Symptoms dataset, and the bottom left plot shows the
TransRate metric and Ô¨Ånal accuracy of all data augmentation methods on the Symptoms dataset. In the middle and bottom panels, we plotted the
cosine similarity and TransRate values of all data augmentation methods on the Amazon and PubMed20K datasets, respectively. On the right side
of the picture, we listed all the augmented methods with different colors and shapes.
for prompting. For the Symptoms dataset, we employed
the following prompt instruction: ‚ÄùGiven a person‚Äôs health
description or symptom, predict the corresponding illness
from the following categories: CLASSES.‚Äù Additionally, we
used ‚ÄùDescription: DESCRIPTION. Typically, this symptom
corresponds to CLASS‚Äù as the prompt for each example in
the dataset. In this way, We can include few-shot examples
(in this work, we used two) to facilitate the model‚Äôs adap-
tation to downstream tasks. We used similarly-designed
prompt instructions for the other two tasks and the corre-
sponding example prompt to implement the few-shot in-
context learning by ChatGPT.
5 E XPERIMENT RESULTS
In our experiments, we use BERT as the base model.
Firstly, we train our model on the base dataset to produce
the pre-trained model. Then we Ô¨Åne-tune the model with
the combination of few-shot samples and the augmented
samples generated from various data augmentation meth-
ods. SpeciÔ¨Åcally, in all three FSL tasks, we perform 2-shot
learning, i.e., there would be two real samples used for
each class in the target domain. Afterward, We use those
samples to Ô¨Åne-tune the pre-trained models. To evaluate
the effectiveness of different data augmentation methods,
we apply two different settings. The Ô¨Årst one is the vanilla
BERT model. In the second setting, we add a contrastive loss
to the training objective function. In our experiments on the
Symptoms dataset, we use a batch size of 8 for 150 epochs,
set the maximum sequence length to 25, as 1, and use a
learning rate of 4e-5. In our experiments on the PubMed20K
dataset, we adopt the same training conÔ¨Åguration, with the
maximum sequence length set to 40. For all three tasks, we
will generate six augmented samples per class. Examples
of the augmented samples generated by AugGPT and other
selected baseline methods can be found in the appendix.5.1 ClassiÔ¨Åcation Performance Comparison
Table 2 shows the accuracy of different data augmenta-
tion methods. As shown in Table 2, AugGPT achieves the
highest accuracy for Amazon, Symptoms and PubMed20K
datasets. For the Amazon dataset, AugGPT and InsertWord-
ByGoogleNewsEmbeddings achieve the best performance
for BERT, and AugGPT achieve the best performance for
BERT with contrastive loss. In the PubMed20K dataset, Aug-
GPT achieves 83.5% accuracy for both BERT and BERT with
contrastive loss, whereas without data augmentation, the
accuracy values are only 79.2% and 79.8%, respectively. For
the Symptoms dataset, the accuracy for BERT downstream
augmentation is only 63.6%, and 60.6% with contrastive loss.
However, our AugGPT approach signiÔ¨Åcantly improves the
accuracy to 88.9% and 89.9%, respectively. These results
suggest that data augmentation using ChatGPT is more
effective in enhancing the performance of machine learning
models in various applications.
5.2 Evaluation of Augmented Datasets
In addition to the classiÔ¨Åcation accuracy, we evaluate the
augmented data in the latent space and visualize the results
in Fig 3. Latent embeddings are evaluated using cosine
similarity and the TransRate metric (see section 4.6 for more
details). The horizontal axis represents the cosine similarity
values and Transrate values, and the vertical axis describes
the classiÔ¨Åcation accuracy. Since embedded similarity mea-
sures the similarity between the generated data and the test
dataset, high similarity means that the generated data are
close to real input data and with higher faithfulness and
compactness. Higher TransRate indicates better learnability
of the data. Therefore, a higher TransRate score indicates
that the augmented data are of higher quality. The most
ideal candidate method should be positioned at the top-
right of the visualization. As shown in Fig 3, AugGPT pro-

--- PAGE 10 ---
10
duces high-quality samples in terms of both faithfulness and
compactness on the Symptoms dataset and the PubMed20K
dataset. On the open-domain Amazon dataset, AugGPT also
produces high-quality samples with a higher TransRate.
5.3 Performance Comparison with ChatGPT
Furthermore, we used ChatGPT to directly perform the
downstream text data classiÔ¨Åcation tasks under a 5-shot
learning scheme. We used in-house designed instructions
with few-shot in-context examples to prompt ChatGPT as
described in 4.7. The performance of ChatGPT for the down-
stream tasks is listed in Table 2. The result reveals that state-
of-the-art large language models such as ChatGPT tend
to perform better on relatively easier tasks, for example,
identifying symptoms according to a one-sentence descrip-
tion. However, when it comes to complicated tasks such
like PubMed, model Ô¨Åne-tuning is still needed and could
achieve better performance compared to few-shot prompts.
6 C ONCLUSION AND DISCUSSION
In this paper, we proposed a novel data augmentation
approach for few-shot classiÔ¨Åcation. Unlike other methods,
our model expands the limited data at the semantic level
to enhance data consistency and robustness, which results
in a better performance than most of the current text data
augmentation methods. With the advancement of LLM and
its nature of a multi-task learner [77], we envision that a
series of tasks in NLP can be enhanced or even replaced in
a similar fashion.
Although AugGPT has shown promising results in data
augmentation, it has certain limitations. For example, when
recognizing and augmenting medical texts, AugGPT may
produce incorrect augmentation results due to the lack of
domain knowledge of ChatGPT. In future works, we will
investigate adapting the general-domain LLMs, such as
ChatGPT, to domain-speciÔ¨Åc data, such as medical texts, via
model Ô¨Åne-tuning, in-context learning (prompt engineer-
ing), knowledge distillation, style transfer, etc.
AugGPT has demonstrated that the augmentation re-
sults can effectively improve the performance of the down-
stream classiÔ¨Åcation task. A promising direction for future
research is to investigate AugGPT against a wider range of
downstream tasks. For example, given the strong ability of
ChatGPT to extract key points and understand sentences,
it can be utilized in tasks such as text summarization.
SpeciÔ¨Åcally, ChatGPT might be valuable for domain-speciÔ¨Åc
science paper summarization [90] and clinical report sum-
marization [91]. Publicly available domain-speciÔ¨Åc science
paper summarization datasets and clinical report datasets
are rare and often provided at small scales due to privacy
concerns and the need for expert knowledge to generate
annotated summaries. However, ChatGPT could address
this challenge by generating diverse augmented summa-
rization samples in different representation styles. The data
generated from ChatGPT are typically concise, which can be
valuable for further enhancing the generalization capabili-
ties of the trained model.
The dramatic rise of generative image models such as
DALLE2 [92] and Stable Diffusion [93] provides oppor-
tunities for applying AugGPT to few-shot learning tasksin computer vision. For example, accurate language de-
scriptions may be used to guide the generative model to
generate images from text or to generate new images based
on existing images as a data augmentation method for few-
shot learning tasks, especially when combined with efÔ¨Åcient
Ô¨Åne-tuning methods [94], [95] such as LoRA for Stable
Diffusion. Thus, prior knowledge from a large language
model can facilitate faster domain adaptation and better
few-shot learning of generative models in computer vision.
Recent research shows that large language models
(LLMs), such as GPT-3 and ChatGPT, are capable of solv-
ing Theory of Mind (ToM) tasks, which were previously
thought to be unique to humans [96]. While the ToM-like
capabilities of LLMs may be an unintended byproduct of
improved performance, the underlying connection between
cognitive science and the human brain is an area ripe for
exploration. Advancements in cognitive and brain science
can also be used to inspire and optimize the design of
LLMs. For example, it has been suggested that the activation
patterns of the neurons in the BERT model and those in
the human brain networks may share similarities and could
be coupled together [97]. This presents a promising new
direction for developing LLMs by utilizing prior knowledge
from brain science. As researchers continue to investigate
the connections between LLMs and the human brain, we
may discover new means to enhance the performance and
capabilities of AI systems, leading to exciting breakthroughs
in the Ô¨Åeld.
REFERENCES
[1] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, ‚ÄúGeneralizing from a
few examples: A survey on few-shot learning,‚Äù ACM computing
surveys (csur) , vol. 53, no. 3, pp. 1‚Äì34, 2020.
[2] W. Yin, ‚ÄúMeta-learning for few-shot natural language processing:
A survey,‚Äù arXiv preprint arXiv:2007.09604 , 2020.
[3] C. Wang, J. Wang, M. Qiu, J. Huang, and M. Gao, ‚ÄúTransprompt:
Towards an automatic transferable prompting framework for few-
shot text classiÔ¨Åcation,‚Äù in Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing , 2021, pp. 2792‚Äì
2802.
[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-
training of deep bidirectional transformers for language under-
standing,‚Äù arXiv preprint arXiv:1810.04805 , 2018.
[5] H.-y. Lee, S.-W. Li, and N. T. Vu, ‚ÄúMeta learning for natural
language processing: A survey,‚Äù arXiv preprint arXiv:2205.01500 ,
2022.
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P . Dhari-
wal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell et al. , ‚ÄúLan-
guage models are few-shot learners,‚Äù Advances in neural informa-
tion processing systems , vol. 33, pp. 1877‚Äì1901, 2020.
[7] B. Lester, R. Al-Rfou, and N. Constant, ‚ÄúThe power of scale
for parameter-efÔ¨Åcient prompt tuning,‚Äù in Proceedings of the 2021
Conference on Empirical Methods in Natural Language Processing ,
2021, pp. 3045‚Äì3059.
[8] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun, ‚ÄúPtr: Prompt tuning
with rules for text classiÔ¨Åcation,‚Äù AI Open , vol. 3, pp. 182‚Äì192,
2022.
[9] J. Wang, C. Wang, F. Luo, C. Tan, M. Qiu, F. Yang, Q. Shi, S. Huang,
and M. Gao, ‚ÄúTowards uniÔ¨Åed prompt tuning for few-shot text
classiÔ¨Åcation,‚Äù arXiv preprint arXiv:2205.05313 , 2022.
[10] J. Wei and K. Zou, ‚ÄúEda: Easy data augmentation techniques for
boosting performance on text classiÔ¨Åcation tasks,‚Äù arXiv preprint
arXiv:1901.11196 , 2019.
[11] V . Kumar, H. Glaude, C. de Lichy, and W. Campbell, ‚ÄúA closer look
at feature space data augmentation for few-shot intent classiÔ¨Åca-
tion,‚Äù in Proceedings of the 2nd Workshop on Deep Learning Approaches
for Low-Resource NLP (DeepLo 2019) , 2019, pp. 1‚Äì10.

--- PAGE 11 ---
11
[12] S. Y. Feng, V . Gangal, J. Wei, S. Chandar, S. Vosoughi, T. Mitamura,
and E. Hovy, ‚ÄúA survey of data augmentation approaches for nlp,‚Äù
arXiv preprint arXiv:2105.03075 , 2021.
[13] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúImproving neural ma-
chine translation models with monolingual data,‚Äù arXiv preprint
arXiv:1511.06709 , 2015.
[14] A. Jindal, A. G. Chowdhury, A. Didolkar, D. Jin, R. Sawhney,
and R. Shah, ‚ÄúAugmenting nlp models using latent feature in-
terpolations,‚Äù in Proceedings of the 28th International Conference on
Computational Linguistics , 2020, pp. 6931‚Äì6936.
[15] C. Shorten, T. M. Khoshgoftaar, and B. Furht, ‚ÄúText data augmen-
tation for deep learning,‚Äù Journal of big Data , vol. 8, pp. 1‚Äì34, 2021.
[16] M. Bayer, M.-A. Kaufhold, and C. Reuter, ‚ÄúA survey on data
augmentation for text classiÔ¨Åcation,‚Äù ACM Computing Surveys ,
vol. 55, no. 7, pp. 1‚Äì39, 2022.
[17] B. Min, H. Ross, E. Sulem, A. P . B. Veyseh, T. H. Nguyen, O. Sainz,
E. Agirre, I. Heinz, and D. Roth, ‚ÄúRecent advances in natural
language processing via large pre-trained language models: A
survey,‚Äù arXiv preprint arXiv:2111.01243 , 2021.
[18] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji,
Q. Yan, L. He et al. , ‚ÄúA comprehensive survey on pretrained
foundation models: A history from bert to chatgpt,‚Äù arXiv preprint
arXiv:2302.09419 , 2023.
[19] Z. Liu, M. He, Z. Jiang, Z. Wu, H. Dai, L. Zhang, S. Luo, T. Han,
X. Li, X. Jiang et al. , ‚ÄúSurvey on natural language processing in
medical image analysis.‚Äù Zhong nan da xue xue bao. Yi xue ban=
Journal of Central South University. Medical Sciences , vol. 47, no. 8,
pp. 981‚Äì993, 2022.
[20] S. Wang, Z. Zhao, X. Ouyang, Q. Wang, and D. Shen, ‚ÄúChatcad:
Interactive computer-aided diagnosis on medical image using
large language models,‚Äù arXiv preprint arXiv:2302.07257 , 2023.
[21] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Gray et al. , ‚ÄúTraining language
models to follow instructions with human feedback,‚Äù in Advances
in Neural Information Processing Systems , 2022.
[22] Y. Belinkov and Y. Bisk, ‚ÄúSynthetic and natural noise both break
neural machine translation,‚Äù arXiv preprint arXiv:1711.02173 , 2017.
[23] C. Coulombe, ‚ÄúText Data Augmentation Made Simple By Lever-
aging NLP Cloud APIs,‚Äù Dec. 2018.
[24] J. Wei and K. Zou, ‚ÄúEDA: Easy Data Augmentation Techniques for
Boosting Performance on Text ClassiÔ¨Åcation Tasks,‚Äù in Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) . Hong Kong, China: Association for
Computational Linguistics, Nov. 2019, pp. 6382‚Äì6388.
[25] E. Pavlick, P . Rastogi, J. Ganitkevitch, B. Van Durme, and
C. Callison-Burch, ‚ÄúPpdb 2.0: Better paraphrase ranking, Ô¨Åne-
grained entailment relations, word embeddings, and style classiÔ¨Å-
cation,‚Äù in Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference
on Natural Language Processing (Volume 2: Short Papers) , 2015, pp.
425‚Äì430.
[26] T. Niu and M. Bansal, ‚ÄúAdversarial Over-Sensitivity and Over-
Stability Strategies for Dialogue Models,‚Äù in Proceedings of the 22nd
Conference on Computational Natural Language Learning . Brussels,
Belgium: Association for Computational Linguistics, 2018, pp.
486‚Äì496.
[27] G. A. Miller, ‚ÄúWordnet: a lexical database for english,‚Äù Communi-
cations of the ACM , vol. 38, no. 11, pp. 39‚Äì41, 1995.
[28] W. Y. Wang and D. Yang, ‚ÄúThat‚Äôs so annoying!!!: A lexical and
frame-semantic embedding based data augmentation approach to
automatic categorization of annoying behaviors using# petpeeve
tweets,‚Äù in Proceedings of the 2015 conference on empirical methods in
natural language processing , 2015, pp. 2557‚Äì2563.
[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
‚ÄúDistributed representations of words and phrases and their
compositionality,‚Äù Advances in neural information processing systems ,
vol. 26, 2013.
[30] N. Mrk Àási¬¥c, D. ¬¥O S¬¥eaghdha, B. Thomson, M. Ga Àási¬¥c, L. M. Rojas-
Barahona, P .-H. Su, D. Vandyke, T.-H. Wen, and S. Young,
‚ÄúCounter-Ô¨Åtting Word Vectors to Linguistic Constraints,‚Äù in Pro-
ceedings of the 2016 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Tech-
nologies . San Diego, California: Association for Computational
Linguistics, Jun. 2016, pp. 142‚Äì148.
[31] M. Alzantot, Y. Sharma, A. Elgohary, B.-J. Ho, M. Srivastava, and
K.-W. Chang, ‚ÄúGenerating Natural Language Adversarial Exam-ples,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing . Brussels, Belgium: Association for
Computational Linguistics, 2018, pp. 2890‚Äì2896.
[32] S. Kobayashi, ‚ÄúContextual Augmentation: Data Augmentation by
Words with Paradigmatic Relations,‚Äù in Proceedings of the 2018
Conference of the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies, Volume 2 (Short
Papers) . New Orleans, Louisiana: Association for Computational
Linguistics, Jun. 2018, pp. 452‚Äì457.
[33] V . Kumar, A. Choudhary, and E. Cho, ‚ÄúData Augmentation Using
Pre-trained Transformer Models,‚Äù arXiv preprint arXiv:2003.02245 ,
2020.
[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-
training of Deep Bidirectional Transformers for Language Under-
standing,‚Äù May 2019.
[35] L. Sun, C. Xia, W. Yin, T. Liang, P . S. Yu, and L. He, ‚ÄúMixup-
transformer: dynamic data augmentation for nlp tasks,‚Äù arXiv
preprint arXiv:2010.02394 , 2020.
[36] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, ‚ÄúDistilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,‚Äù ArXiv , vol.
abs/1910.01108, 2019.
[37] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,
M. Lewis, L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A ro-
bustly optimized bert pretraining approach,‚Äù arXiv preprint
arXiv:1907.11692 , 2019.
[38] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúImproving Neural Ma-
chine Translation Models with Monolingual Data,‚Äù in Proceedings
of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) . Berlin, Germany: Association
for Computational Linguistics, Aug. 2016, pp. 86‚Äì96.
[39] V . Gangal, S. Y. Feng, M. Alikhani, T. Mitamura, and E. Hovy,
‚ÄúNareor: The narrative reordering problem,‚Äù in Proceedings of the
AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 36, no. 10, 2022, pp.
10 645‚Äì10 653.
[40] L. Fei-Fei, R. Fergus, and P . Perona, ‚ÄúOne-shot learning of object
categories,‚Äù IEEE transactions on pattern analysis and machine intelli-
gence , vol. 28, no. 4, pp. 594‚Äì611, 2006.
[41] Y. Ge, Y. Guo, Y.-C. Yang, M. A. Al-Garadi, and A. Sarker, ‚ÄúFew-
shot learning for medical text: A systematic review,‚Äù arXiv preprint
arXiv:2204.14081 , 2022.
[42] J. Wei, C. Huang, S. Vosoughi, Y. Cheng, and S. Xu, ‚ÄúFew-shot
text classiÔ¨Åcation with triplet networks, data augmentation, and
curriculum learning,‚Äù in Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies , 2021, pp. 5493‚Äì5500.
[43] T. Gao, A. Fisch, and D. Chen, ‚ÄúMaking pre-trained language
models better few-shot learners,‚Äù in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , 2021, pp. 3816‚Äì3830.
[44] A. Antoniou, H. Edwards, and A. Storkey, ‚ÄúHow to train your
maml,‚Äù arXiv preprint arXiv:1810.09502 , 2018.
[45] C. Finn, P . Abbeel, and S. Levine, ‚ÄúModel-agnostic meta-learning
for fast adaptation of deep networks,‚Äù in International conference on
machine learning . PMLR, 2017, pp. 1126‚Äì1135.
[46] X. Yao, J. Zhu, G. Huo, N. Xu, X. Liu, and C. Zhang, ‚ÄúModel-
agnostic multi-stage loss optimization meta learning,‚Äù Interna-
tional Journal of Machine Learning and Cybernetics , vol. 12, no. 8,
pp. 2349‚Äì2363, 2021.
[47] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. ,
‚ÄúImproving language understanding by generative pre-training,‚Äù
2018.
[48] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V . Stoyanov, and L. Zettlemoyer, ‚ÄúBart:
Denoising sequence-to-sequence pre-training for natural lan-
guage generation, translation, and comprehension,‚Äù arXiv preprint
arXiv:1910.13461 , 2019.
[49] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
Y. Zhou, W. Li, and P . J. Liu, ‚ÄúExploring the limits of transfer
learning with a uniÔ¨Åed text-to-text transformer,‚Äù The Journal of
Machine Learning Research , vol. 21, no. 1, pp. 5485‚Äì5551, 2020.
[50] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
A. Roberts, P . Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. ,
‚ÄúPalm: Scaling language modeling with pathways,‚Äù arXiv preprint
arXiv:2204.02311 , 2022.
[51] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow,
R. Castagn ¬¥e, A. S. Luccioni, F. Yvon, M. Gall ¬¥eet al. , ‚ÄúBloom: A

--- PAGE 12 ---
12
176b-parameter open-access multilingual language model,‚Äù arXiv
preprint arXiv:2211.05100 , 2022.
[52] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,
C. Dewan, M. Diab, X. Li, X. V . Lin et al. , ‚ÄúOpt: Open pre-trained
transformer language models,‚Äù arXiv preprint arXiv:2205.01068 ,
2022.
[53] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay,
D. Zhou, Q. V . Le, B. Zoph, J. Wei et al. , ‚ÄúThe Ô¨Çan collection:
Designing data and methods for effective instruction tuning,‚Äù
arXiv preprint arXiv:2301.13688 , 2023.
[54] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Nau-
mann, J. Gao, and H. Poon, ‚ÄúDomain-speciÔ¨Åc language model
pretraining for biomedical natural language processing,‚Äù ACM
Transactions on Computing for Healthcare (HEALTH) , vol. 3, no. 1,
pp. 1‚Äì23, 2021.
[55] S. Rezayi, H. Dai, Z. Liu, Z. Wu, A. Hebbar, A. H. Burns, L. Zhao,
D. Zhu, Q. Li, W. Liu et al. , ‚ÄúClinicalradiobert: Knowledge-infused
few shot learning for clinical notes named entity recognition,‚Äù in
Machine Learning in Medical Imaging: 13th International Workshop,
MLMI 2022, Held in Conjunction with MICCAI 2022, Singapore,
September 18, 2022, Proceedings . Springer, 2022, pp. 269‚Äì278.
[56] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang,
‚ÄúIs chatgpt a general-purpose natural language processing task
solver?‚Äù arXiv preprint arXiv:2302.06476 , 2023.
[57] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, ‚ÄúIs chat-
gpt a good translator? a preliminary study,‚Äù arXiv preprint
arXiv:2301.08745 , 2023.
[58] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Love-
nia, Z. Ji, T. Yu, W. Chung et al. , ‚ÄúA multitask, multilingual,
multimodal evaluation of chatgpt on reasoning, hallucination, and
interactivity,‚Äù arXiv preprint arXiv:2302.04023 , 2023.
[59] Y. Shen, L. Heacock, J. Elias, K. D. Hentel, B. Reig, G. Shih, and
L. Moy, ‚ÄúChatgpt and other large language models are double-
edged swords,‚Äù p. 230163, 2023.
[60] F. Antaki, S. Touma, D. Milad, J. El-Khoury, and R. Duval, ‚ÄúEval-
uating the performance of chatgpt in ophthalmology: An analysis
of its successes and shortcomings,‚Äù medRxiv , pp. 2023‚Äì01, 2023.
[61] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,
C. Elepa Àúno, M. Madriaga, R. Aggabao, G. Diaz-Candido,
J. Maningo et al. , ‚ÄúPerformance of chatgpt on usmle: Potential for
ai-assisted medical education using large language models,‚Äù PLOS
Digital Health , vol. 2, no. 2, p. e0000198, 2023.
[62] J. V . Pavlik, ‚ÄúCollaborating with chatgpt: Considering the im-
plications of generative artiÔ¨Åcial intelligence for journalism and
media education,‚Äù Journalism & Mass Communication Educator , p.
10776958221149577, 2023.
[63] D. Baidoo-Anu and L. Owusu Ansah, ‚ÄúEducation in the era of
generative artiÔ¨Åcial intelligence (ai): Understanding the potential
beneÔ¨Åts of chatgpt in promoting teaching and learning,‚Äù Available
at SSRN 4337484 , 2023.
[64] S. Frieder, L. Pinchetti, R.-R. GrifÔ¨Åths, T. Salvatori, T. Lukasiewicz,
P . C. Petersen, A. Chevalier, and J. Berner, ‚ÄúMathematical capabil-
ities of chatgpt,‚Äù arXiv preprint arXiv:2301.13867 , 2023.
[65] M. Dowling and B. Lucey, ‚ÄúChatgpt for (Ô¨Ånance) research: The
bananarama conjecture,‚Äù Finance Research Letters , p. 103662, 2023.
[66] E. A. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L.
Bockting, ‚ÄúChatgpt: Ô¨Åve priorities for research,‚Äù Nature , vol. 614,
no. 7947, pp. 224‚Äì226, 2023.
[67] R. W. McGee, ‚ÄúIs chat gpt biased against conservatives? an empir-
ical study,‚Äù An Empirical Study (February 15, 2023) , 2023.
[68] A. Blum, ‚ÄúBreaking chatgpt with dangerous questions under-
standing how chatgpt prioritizes safety, context, and obedience,‚Äù
2022.
[69] H. Y. Jabotinsky and R. Sarel, ‚ÄúCo-authoring with an ai? ethical
dilemmas and artiÔ¨Åcial intelligence,‚Äù Ethical Dilemmas and ArtiÔ¨Åcial
Intelligence (December 15, 2022) , 2022.
[70] T. Susnjak, ‚ÄúChatgpt: The end of online exam integrity?‚Äù arXiv
preprint arXiv:2212.09292 , 2022.
[71] M. Khalil and E. Er, ‚ÄúWill chatgpt get you caught? rethinking of
plagiarism detection,‚Äù arXiv preprint arXiv:2302.04335 , 2023.
[72] D. Castelvecchi, ‚ÄúAre chatgpt and alphacode going to replace
programmers?‚Äù Nature , 2022.
[73] A. Zarifhonarvar, ‚ÄúEconomics of chatgpt: A labor market view
on the occupational impact of artiÔ¨Åcial intelligence,‚Äù Available at
SSRN 4350925 , 2023.
[74] R. He and J. McAuley, ‚ÄúUps and downs: Modeling the visual
evolution of fashion trends with one-class collaborative Ô¨Åltering,‚Äùinproceedings of the 25th international conference on world wide web ,
2016, pp. 507‚Äì517.
[75] Y. Bao, M. Wu, S. Chang, and R. Barzilay, ‚ÄúFew-shot text
classiÔ¨Åcation with distributional signatures,‚Äù arXiv preprint
arXiv:1908.06039 , 2019.
[76] S. Wang, X. Liu, B. Liu, and D. Dong, ‚ÄúSentence-aware adversarial
meta-learning for few-shot text classiÔ¨Åcation,‚Äù in Proceedings of the
29th International Conference on Computational Linguistics , 2022, pp.
4844‚Äì4852.
[77] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI
blog, vol. 1, no. 8, p. 9, 2019.
[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
Advances in neural information processing systems , vol. 30, 2017.
[79] J. Schulman, F. Wolski, P . Dhariwal, A. Radford, and O. Klimov,
‚ÄúProximal policy optimization algorithms,‚Äù arXiv preprint
arXiv:1707.06347 , 2017.
[80] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-
training of deep bidirectional transformers for language under-
standing,‚Äù in Proceedings of NAACL-HLT , 2019, pp. 4171‚Äì4186.
[81] W. Liao, Z. Liu, H. Dai, Z. Wu, Y. Zhang, X. Huang, Y. Chen,
X. Jiang, D. Zhu, T. Liu, S. Li, X. Li, and H. Cai, ‚ÄúMask-guided bert
for few shot text classiÔ¨Åcation,‚Äù arXiv preprint arXiv:2302.10447 ,
2023.
[82] S. Rezayi, Z. Liu, Z. Wu, C. Dhakal, B. Ge, C. Zhen, T. Liu, and
S. Li, ‚ÄúAgribert: Knowledge-infused agricultural language models
for matching food and nutrition,‚Äù International Joint Conference on
ArtiÔ¨Åcial Intelligence, July 23-29, 2022, Vienna, Austria , 2022.
[83] E. Ma, ‚ÄúNlp augmentation,‚Äù https://github.com/makcedward/nlpaug,
2019.
[84] J. Morris, E. LiÔ¨Çand, J. Y. Yoo, J. Grigsby, D. Jin, and Y. Qi,
‚ÄúTextattack: A framework for adversarial attacks, data augmen-
tation, and adversarial training in nlp,‚Äù in Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing:
System Demonstrations , 2020, pp. 119‚Äì126.
[85] N. Ng, K. Yee, A. Baevski, M. Ott, M. Auli, and S. Edunov,
‚ÄúFacebook fair‚Äôs wmt19 news translation task submission,‚Äù in
Proc. of WMT , 2020.
[86] L.-K. Huang, J. Huang, Y. Rong, Q. Yang, and Y. Wei, ‚ÄúFrustrat-
ingly easy transferability estimation,‚Äù in International Conference
on Machine Learning . PMLR, 2022, pp. 9201‚Äì9225.
[87] B. Li, H. Zhou, J. He, M. Wang, Y. Yang, and L. Li, ‚ÄúOn the sentence
embeddings from pre-trained language models,‚Äù in Proceedings
of the 2020 Conference on Empirical Methods in Natural Language
Processing (EMNLP) , 2020, pp. 9119‚Äì9130.
[88] J. Wang and Y. Dong, ‚ÄúMeasurement of text similarity: a survey,‚Äù
Information , vol. 11, no. 9, p. 421, 2020.
[89] T. M. Cover, Elements of information theory . John Wiley & Sons,
1999.
[90] X. Cai, S. Liu, J. Han, L. Yang, Z. Liu, and T. Liu, ‚ÄúChestxraybert:
A pretrained language model for chest radiology report summa-
rization,‚Äù IEEE Transactions on Multimedia , pp. 1‚Äì1, 2021.
[91] X. Cai, S. Liu, L. Yang, Y. Lu, J. Zhao, D. Shen, and T. Liu, ‚ÄúCovid-
sum: A linguistically enriched scibert-based summarization model
for covid-19 scientiÔ¨Åc papers,‚Äù Journal of Biomedical Informatics , vol.
127, p. 103999, 2022.
[92] A. Ramesh, P . Dhariwal, A. Nichol, C. Chu, and M. Chen, ‚ÄúHier-
archical text-conditional image generation with clip latents,‚Äù arXiv
preprint arXiv:2204.06125 , 2022.
[93] R. Rombach, A. Blattmann, D. Lorenz, P . Esser, and B. Ommer,
‚ÄúHigh-resolution image synthesis with latent diffusion models,‚Äù
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 10 684‚Äì10 695.
[94] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
and W. Chen, ‚ÄúLora: Low-rank adaptation of large language
models,‚Äù arXiv preprint arXiv:2106.09685 , 2021.
[95] N. Ruiz, Y. Li, V . Jampani, Y. Pritch, M. Rubinstein, and K. Aber-
man, ‚ÄúDreambooth: Fine tuning text-to-image diffusion models
for subject-driven generation,‚Äù arXiv preprint arXiv:2208.12242 ,
2022.
[96] M. Kosinski, ‚ÄúTheory of mind may have spontaneously emerged
in large language models,‚Äù arXiv preprint arXiv:2302.02083 , 2023.
[97] X. Liu, M. Zhou, G. Shi, Y. Du, L. Zhao, Z. Wu, D. Liu, T. Liu, and
X. Hu, ‚ÄúCoupling artiÔ¨Åcial neurons in bert and biological neurons
in the human brain,‚Äù in Proceedings of the 37th AAAI Conference on
ArtiÔ¨Åcial Intelligence, AAAI , 2023.
