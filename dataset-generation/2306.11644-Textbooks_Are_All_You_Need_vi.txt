Looking at this text, I need to translate a research paper about phi-1, a large language model for code generation. Let me translate this into Vietnamese:

# Sách giáo khoa là tất cả những gì bạn cần

Suriya Gunasekar Yi Zhang Jyoti Aneja Caio César Teodoro Mendes
Allie Del Giorno Sivakanth Gopi Mojan Javaheripi Piero Kauffmann
Gustavo de Rosa Olli Saarikivi Adil Salim Shital Shah Harkirat Singh Behl
Xin Wang Sébastien Bubeck Ronen Eldan Adam Tauman Kalai Yin Tat Lee
Yuanzhi Li
Microsoft Research

## Tóm tắt

Chúng tôi giới thiệu phi-1, một mô hình ngôn ngữ lớn mới cho code, với kích thước nhỏ hơn đáng kể so với các mô hình cạnh tranh: phi-1 là một mô hình dựa trên Transformer với 1.3B tham số, được huấn luyện trong 4 ngày trên 8 A100, sử dụng lựa chọn dữ liệu "chất lượng sách giáo khoa" từ web (6B token) và sách giáo khoa cũng như bài tập được tạo tổng hợp bằng GPT-3.5 (1B token). Mặc dù quy mô nhỏ, phi-1 đạt được độ chính xác pass@1 50.6% trên HumanEval và 55.5% trên MBPP. Nó cũng thể hiện các thuộc tính nổi lên đáng ngạc nhiên so với phi-1-base, mô hình của chúng tôi trước giai đoạn tinh chỉnh trên tập dữ liệu bài tập lập trình, và phi-1-small, một mô hình nhỏ hơn với 350M tham số được huấn luyện với cùng pipeline như phi-1 vẫn đạt 45% trên HumanEval.

## 1 Giới thiệu

Nghệ thuật huấn luyện các mạng nơ-ron nhân tạo lớn đã có tiến bộ phi thường trong thập kỷ qua, đặc biệt sau khi khám phá ra kiến trúc Transformer [VSP+17], tuy nhiên khoa học đằng sau thành công này vẫn còn hạn chế. Giữa một loạt kết quả rộng lớn và khó hiểu, một sự trật tự xuất hiện cùng thời điểm với việc giới thiệu Transformer, cụ thể là hiệu suất cải thiện một cách có thể dự đoán khi tăng quy mô lượng tính toán hoặc kích thước mạng [HNA+17], một hiện tượng hiện được gọi là quy luật tỷ lệ [KMH+20]. Việc khám phá quy mô trong deep learning sau đó được hướng dẫn bởi các quy luật tỷ lệ này [BMR+20], và việc khám phá các biến thể của những quy luật này dẫn đến bước nhảy nhanh chóng trong hiệu suất [HBM+22]. Trong công trình này, theo bước chân của Eldan và Li [EL23], chúng tôi khám phá sự cải thiện có thể đạt được theo một trục khác: chất lượng dữ liệu. Từ lâu đã biết rằng dữ liệu chất lượng cao dẫn đến kết quả tốt hơn, ví dụ, làm sạch dữ liệu là một phần quan trọng của việc tạo tập dữ liệu hiện đại [RSR+20], và nó có thể mang lại các lợi ích phụ khác như tập dữ liệu nhỏ hơn [LYR+23, YGK+23] hoặc cho phép nhiều lượt xử lý dữ liệu hơn [MRB+23]. Công trình gần đây của Eldan và Li về TinyStories (một tập dữ liệu chất lượng cao được tạo tổng hợp để dạy tiếng Anh cho mạng nơ-ron) đã cho thấy rằng tác động của dữ liệu chất lượng cao thực sự vượt xa điều này: cải thiện chất lượng dữ liệu có thể thay đổi đáng kể hình dạng của các quy luật tỷ lệ, có khả năng cho phép đạt được hiệu suất của các mô hình quy mô lớn với huấn luyện/mô hình tinh gọn hơn nhiều. Trong công trình này, chúng tôi vượt xa bước đầu của Eldan và Li để chỉ ra rằng dữ liệu chất lượng cao thậm chí có thể cải thiện SOTA của các mô hình ngôn ngữ lớn (LLM), đồng thời giảm đáng kể kích thước tập dữ liệu và tính toán huấn luyện. Quan trọng là, các mô hình nhỏ hơn yêu cầu ít huấn luyện hơn có thể giảm đáng kể chi phí môi trường của LLM [BGMMS21].

Chúng tôi tập trung vào LLM được huấn luyện cho code, và cụ thể là viết các hàm Python đơn giản từ docstring của chúng như trong [CTJ+21]. Benchmark đánh giá được đề xuất trong công trình sau, HumanEval, đã được áp dụng rộng rãi để so sánh hiệu suất của LLM trên code. Chúng tôi chứng minh sức mạnh của dữ liệu chất lượng cao trong việc phá vỡ các quy luật tỷ lệ hiện tại bằng cách huấn luyện một mô hình 1.3B tham số, mà chúng tôi gọi là phi-1, trong khoảng 8 lượt qua 7B token (hơi trên 50B tổng token đã thấy) sau đó tinh chỉnh trên ít hơn 200M token. Nói một cách đại khái, chúng tôi pre-train trên dữ liệu "chất lượng sách giáo khoa", cả được tạo tổng hợp (với GPT-3.5) và được lọc từ các nguồn web, và chúng tôi tinh chỉnh trên dữ liệu "giống như bài tập sách giáo khoa". Mặc dù nhỏ hơn vài bậc độ so với các mô hình cạnh tranh, cả về kích thước tập dữ liệu và mô hình (xem Bảng 1), chúng tôi đạt được độ chính xác pass@1 50.6% trên HumanEval và 55.5% pass@1 trên MBPP (Mostly Basic Python Programs), đây là một trong những con số tự báo cáo tốt nhất chỉ sử dụng một lần sinh LLM. Trong Phần 2, chúng tôi đưa ra một số chi tiết về quy trình huấn luyện của chúng tôi, và chúng tôi thảo luận bằng chứng về tầm quan trọng của quy trình lựa chọn dữ liệu trong việc đạt được kết quả này. Hơn nữa, mặc dù được huấn luyện trên ít token hơn nhiều so với các mô hình hiện tại, phi-1 vẫn thể hiện các thuộc tính nổi lên. Trong Phần 3, chúng tôi thảo luận các thuộc tính nổi lên này, và đặc biệt chúng tôi xác nhận giả thuyết rằng số lượng tham số đóng vai trò chính trong sự nổi lên (xem ví dụ [WTB+22]), bằng cách so sánh đầu ra của phi-1 với phi-1-small, một mô hình được huấn luyện với cùng pipeline nhưng chỉ có 350M tham số.

Phương pháp được sử dụng trong phần này gợi nhớ đến bài báo Sparks of AGI [BCE+23] đã lập luận cho việc chuyển khỏi các benchmark tĩnh để kiểm tra hiệu suất LLM. Cuối cùng trong Phần 4, chúng tôi thảo luận các benchmark thay thế để đánh giá mô hình và trong Phần 5, chúng tôi nghiên cứu khả năng nhiễm dữ liệu huấn luyện của chúng tôi đối với HumanEval. Chúng tôi phát hành mô hình để sử dụng và đánh giá bởi cộng đồng rộng lớn hơn, nhưng bỏ qua một số chi tiết về việc tạo dữ liệu tổng hợp, vì lý do sở hữu.

**Các công trình liên quan khác** Công trình của chúng tôi là một phần của chương trình gần đây sử dụng LLM cho tổng hợp chương trình, xem [CTJ+21, NPH+22] để có thêm tài liệu tham khảo về điều này. Cách tiếp cận của chúng tôi cũng là một phần của xu hướng mới nổi sử dụng LLM hiện tại để tổng hợp dữ liệu cho việc huấn luyện thế hệ LLM mới [WKM+22, TGZ+23, MMJ+23, LGK+23, JWJ+23]. Có một cuộc tranh luận đang diễn ra về việc liệu "huấn luyện đệ quy" như vậy có thể dẫn đến phạm vi hẹp hơn cho LLM kết quả [SSZ+23, GWS+23], xem [MMJ+23] để có quan điểm phản bác. Lưu ý rằng trong bài báo này, chúng tôi tập trung vào một nhiệm vụ hẹp, tương tự như [JWJ+23], trong trường hợp đó, có vẻ hợp lý để đạt được hiệu suất tốt hơn LLM giáo viên trên nhiệm vụ cụ thể đó (như được lập luận trong bài báo sau).

## 2 Chi tiết huấn luyện và tầm quan trọng của dữ liệu chất lượng cao

Như đã ám chỉ trong tiêu đề của bài báo, thành phần trung tâm mà mô hình của chúng tôi dựa vào là dữ liệu huấn luyện chất lượng sách giáo khoa. Không giống như công trình trước đây sử dụng các nguồn dữ liệu văn bản tiêu chuẩn để tạo code, như The Stack [KLA+22] (chứa mã nguồn từ các kho có giấy phép cho phép) và các tập dữ liệu dựa trên web khác (ví dụ: StackOverflow và CodeContest [LCC+22]), chúng tôi lập luận rằng những nguồn này không tối ưu để dạy mô hình cách lý luận và lập kế hoạch thuật toán. Mặt khác, kiến trúc mô hình và phương pháp huấn luyện của chúng tôi khá thông thường (Phần 2.3), vì vậy chúng tôi dành phần này chủ yếu để giải thích cách chúng tôi tuyển chọn dữ liệu.

Các tập dữ liệu code tiêu chuẩn [KLA+22, LCC+22] tạo thành một kho tài liệu lớn và đa dạng bao gồm phạm vi rộng các chủ đề và trường hợp sử dụng. Tuy nhiên, dựa trên việc kiểm tra thủ công các mẫu ngẫu nhiên, chúng tôi quan sát thấy rằng nhiều đoạn code này không rất hữu ích để học những kiến thức cơ bản về lập trình, và gặp phải một số nhược điểm:

• Nhiều mẫu không tự hoàn chỉnh, có nghĩa là chúng phụ thuộc vào các module hoặc file khác bên ngoài đoạn code, khiến chúng khó hiểu mà không có ngữ cảnh bổ sung.

• Các ví dụ điển hình không liên quan đến tính toán có ý nghĩa nào, mà thay vào đó bao gồm code tầm thường hoặc boilerplate, như định nghĩa hằng số, thiết lập tham số, hoặc cấu hình các thành phần GUI.

• Các mẫu có chứa logic thuật toán thường bị chôn vùi bên trong các hàm phức tạp hoặc được tài liệu hóa kém, khiến chúng khó theo dõi hoặc học hỏi.

• Các ví dụ thiên về các chủ đề hoặc trường hợp sử dụng nhất định, dẫn đến phân phối không cân bằng các khái niệm và kỹ năng lập trình trong toàn bộ tập dữ liệu.

Người ta chỉ có thể tưởng tượng việc một học viên con người cố gắng thu thập kỹ năng lập trình từ những tập dữ liệu này sẽ thất vọng và không hiệu quả như thế nào, vì họ sẽ phải đối phó với rất nhiều nhiễu, mơ hồ và không hoàn chỉnh trong dữ liệu. Chúng tôi đưa ra giả thuyết rằng những vấn đề này cũng ảnh hưởng đến hiệu suất của các mô hình ngôn ngữ, vì chúng làm giảm chất lượng và số lượng tín hiệu ánh xạ ngôn ngữ tự nhiên thành code. Chúng tôi phỏng đoán rằng các mô hình ngôn ngữ sẽ được hưởng lợi từ một tập huấn luyện có những đặc tính giống như một "sách giáo khoa" tốt: nó nên rõ ràng, tự hoàn chỉnh, hướng dẫn và cân bằng.

Trong công trình này, chúng tôi giải quyết thách thức này một cách trực tiếp và chỉ ra rằng bằng cách cố ý lựa chọn và tạo ra dữ liệu chất lượng cao, chúng tôi có thể đạt được kết quả tiên tiến nhất trên các nhiệm vụ tạo code với một mô hình nhỏ hơn nhiều và ít tính toán hơn so với các cách tiếp cận hiện tại. Việc huấn luyện của chúng tôi dựa vào ba tập dữ liệu chính:

• Một tập dữ liệu code-ngôn ngữ được lọc, là một tập con của The Stack và StackOverflow, thu được bằng cách sử dụng bộ phân loại dựa trên mô hình ngôn ngữ (bao gồm khoảng 6B token).

• Một tập dữ liệu sách giáo khoa tổng hợp bao gồm <1B token của sách giáo khoa Python được tạo bằng GPT-3.5.

• Một tập dữ liệu bài tập tổng hợp nhỏ bao gồm ~180M token bài tập Python và lời giải.

Chúng tôi mô tả những tập dữ liệu đó chi tiết hơn trong các phần tiếp theo. Tổng cộng, các tập dữ liệu trên chứa ít hơn 7B token. Chúng tôi gọi sự kết hợp của tập dữ liệu code-ngôn ngữ được lọc và sách giáo khoa tổng hợp là "CodeTextbook" và sử dụng nó trong giai đoạn pre-training để có được mô hình cơ sở phi-1-base - mô hình này đã đạt được hiệu suất HumanEval cạnh tranh là 29%. Sau đó chúng tôi sử dụng tập dữ liệu bài tập tổng hợp 180M token, được gọi là "CodeExercises", để tinh chỉnh mô hình phi-1-base để có được phi-1. Mặc dù kích thước nhỏ của tập dữ liệu "CodeExercises", việc tinh chỉnh với tập dữ liệu này rất quan trọng không chỉ cho những cải thiện lớn trong việc tạo hàm Python đơn giản như thể hiện trong Hình 2.1, mà còn rộng hơn để mở khóa nhiều khả năng nổi lên thú vị trong mô hình phi-1 của chúng tôi mà không được quan sát thấy trong phi-1-base (xem Phần 3).

### 2.1 Lọc các tập dữ liệu code hiện tại bằng bộ phân loại dựa trên transformer

Chúng tôi bắt đầu với các tập dữ liệu code Python có sẵn công khai: chúng tôi sử dụng tập con Python của phiên bản deduplicated của The Stack và StackOverflow, cùng nhau chứa hơn 35 triệu file/mẫu, tổng cộng hơn 35B token. Chúng tôi chú thích chất lượng của một tập con nhỏ của những file này (khoảng 100k mẫu) bằng GPT-4: cho một đoạn code, mô hình được nhắc "xác định giá trị giáo dục của nó cho một học sinh có mục tiêu học các khái niệm lập trình cơ bản".

Sau đó chúng tôi sử dụng tập dữ liệu đã chú thích này để huấn luyện một bộ phân loại random forest dự đoán chất lượng của một file/mẫu bằng cách sử dụng embedding đầu ra từ một mô hình codegen được pre-train làm đặc trưng. Chúng tôi lưu ý rằng không giống như GPT-3.5, mà chúng tôi sử dụng rộng rãi để tạo nội dung tổng hợp (thảo luận bên dưới), chúng tôi sử dụng GPT-4 tối thiểu chỉ cho chú thích về chất lượng của một tập con nhỏ The Stack và các mẫu StackOverflow. Do đó chúng tôi xem việc sử dụng GPT-4 của chúng tôi chỉ đơn thuần là một cách để tránh những nỗ lực chú thích thủ công tẻ nhạt [DLT+23].

Phương pháp lọc của chúng tôi tăng hiệu suất mô hình đáng kể ngay cả khi không có các tập dữ liệu tổng hợp được thảo luận bên dưới: đối với các mô hình 350M tham số được huấn luyện trên Stack chưa lọc (python deduplicated) và StackOverflow, hiệu suất HumanEval bão hòa ở 12.19% ngay cả sau khi huấn luyện trong 96k bước (~200B token), trong khi huấn luyện trên tập con đã lọc đạt 17.68% trên HumanEval sau 36k bước. Chúng tôi cải thiện thêm điều này lên 20.12% (báo cáo trong Hình 2.1) bằng cách huấn luyện trên sự kết hợp của tập dữ liệu đã lọc và tập dữ liệu sách giáo khoa tổng hợp được thảo luận bên dưới.

### 2.2 Tạo các tập dữ liệu tổng hợp chất lượng sách giáo khoa

Một trong những thách thức chính trong việc tạo tập dữ liệu chất lượng cao cho việc tạo code là đảm bảo rằng các ví dụ đa dạng và không lặp lại. Bằng đa dạng, chúng tôi có nghĩa là các ví dụ nên bao gồm phạm vi rộng các khái niệm, kỹ năng và tình huống lập trình, và chúng nên thay đổi về mức độ khó, phức tạp và phong cách. Đa dạng quan trọng vì một số lý do: nó cho phép mô hình ngôn ngữ tiếp xúc với các cách khác nhau để biểu đạt và giải quyết vấn đề trong code, nó giảm nguy cơ overfitting hoặc ghi nhớ các mẫu hoặc giải pháp cụ thể, và nó tăng khả năng tổng quát hóa và tính mạnh mẽ của mô hình đối với các nhiệm vụ chưa thấy hoặc mới lạ. Tuy nhiên, việc đạt được đa dạng không đơn giản, đặc biệt khi sử dụng dữ liệu tổng hợp được tạo bởi một mô hình ngôn ngữ khác. Đơn giản là nhắc mô hình tạo ra một sách giáo khoa lập trình hoặc một tập bài tập, ngay cả với một số biến thể trong hướng dẫn hoặc tham số, có khả năng sẽ dẫn đến một tập dữ liệu rất đồng nhất và dư thừa, nơi các khái niệm và giải pháp giống nhau được lặp lại nhiều lần với những thay đổi nhỏ. Điều này là do các mô hình ngôn ngữ có xu hướng theo những con đường có khả năng xảy ra nhiều nhất hoặc phổ biến nhất dựa trên dữ liệu huấn luyện và tiên nghiệm của chúng, và chúng thiếu sự sáng tạo hoặc động lực để khám phá các cách thay thế hoặc mới lạ để tạo code. Do đó, người ta cần tìm "thủ thuật" phù hợp sẽ khiến mô hình ngôn ngữ sáng tạo và đa dạng hơn trong đầu ra, đồng thời vẫn duy trì chất lượng và tính nhất quán của các ví dụ. Lấy cảm hứng từ [EL23], nơi một tập đa dạng các câu chuyện ngắn được tạo ra bằng cách bao gồm một tập con ngẫu nhiên các từ được chọn từ một số từ vựng cố định trong prompt và yêu cầu chúng được kết hợp bằng cách nào đó trong văn bản được tạo, chúng tôi tìm cách để đưa tính ngẫu nhiên vào prompt theo cách tạo ra việc tạo tập dữ liệu đa dạng.

**Tập dữ liệu sách giáo khoa tổng hợp**

Tập dữ liệu này bao gồm ít hơn 1B token của sách giáo khoa Python được tạo bằng GPT-3.5, được tổng hợp để cung cấp nguồn văn bản ngôn ngữ tự nhiên nặng chất lượng cao xen kẽ với các đoạn code liên quan. Chúng tôi còn hướng nội dung của những sách giáo khoa này để bao gồm các chủ đề thúc đẩy lý luận và kỹ năng thuật toán cơ bản. Ở đây, đa dạng được có được bằng cách cung cấp các ràng buộc về chủ đề và đối tượng mục tiêu của sách giáo khoa được tạo. Sau đây là một ví dụ văn bản từ sách giáo khoa tổng hợp:

Để bắt đầu, hãy định nghĩa ma trận đơn lẻ và không đơn lẻ. Một ma trận được gọi là đơn lẻ nếu định thức của nó bằng không. Mặt khác, một ma trận được gọi là không đơn lẻ nếu định thức của nó khác không. Bây giờ, hãy khám phá những khái niệm này qua các ví dụ.

Ví dụ 1: Xét ma trận A = np.array([[1, 2], [2, 4]]). Chúng ta có thể kiểm tra xem ma trận này đơn lẻ hay không đơn lẻ bằng hàm định thức. Chúng ta có thể định nghĩa một hàm Python, `is_singular(A)`, trả về true nếu định thức của A bằng không, và false nếu ngược lại.

```python
import numpy as np
def is_singular(A):
    det = np.linalg.det(A)
    if det == 0:
        return True
    else:
        return False

A = np.array([[1, 2], [2, 4]])
print(is_singular(A)) # True
```

**Tập dữ liệu CodeExercises**

Đây là một tập dữ liệu bài tập tổng hợp nhỏ bao gồm ít hơn 180M token bài tập Python và lời giải. Mỗi bài tập là một docstring của một hàm cần được hoàn thành. Mục tiêu của tập dữ liệu này là căn chỉnh mô hình để thực hiện các nhiệm vụ hoàn thành hàm dựa trên hướng dẫn ngôn ngữ tự nhiên. Tập dữ liệu này cũng được tạo bởi GPT-3.5, nơi phương tiện chính để tạo ra đa dạng là bằng cách ràng buộc tên hàm. Đối với tập dữ liệu này cụ thể, chúng tôi tiến hành khử nhiễm rõ ràng và đánh giá thay thế trong các phần sau để đảm bảo rằng các vấn đề tương tự như những vấn đề từ benchmark HumanEval không được thấy trong quá trình tinh chỉnh. Đoạn code sau minh họa một bài tập được tạo tổng hợp.

```python
def valid_guessing_letters(word: str, guesses: List[str]) -> List[str]:
    """
    Trả về danh sách các chữ cái đoán hợp lệ, là những chữ cái chưa được đoán và 
    có mặt trong từ.
    
    Tham số:
    word (str): Từ cần đoán.
    guesses (List[str]): Danh sách các chữ cái đã được đoán.
    
    Trả về:
    List[str]: Danh sách các chữ cái đoán hợp lệ.
    """
    valid_letters = []
    for letter in word:
        if letter not in guesses and letter not in valid_letters:
            valid_letters.append(letter)
    return valid_letters
```

### 2.3 Kiến trúc mô hình và huấn luyện

Chúng tôi sử dụng mô hình transformer chỉ decoder [VSP+17] sử dụng triển khai FlashAttention của multi-head attention (MHA) [DFE+22]. Chúng tôi cũng sử dụng các lớp MHA và MLP trong cấu hình song song theo một số mô hình gần đây như CodeGen [NPH+22], PaLM [CND+22], và GPT-NeoX [BBH+22]. Kiến trúc cho mô hình phi-1 1.3B tham số của chúng tôi bao gồm 24 lớp, chiều ẩn 2048, chiều MLP-inner 8192, và 32 attention head với chiều 64 mỗi cái. Mô hình phi-1-small 350M tham số nhỏ hơn bao gồm 20 lớp, chiều ẩn 1024, chiều MLP-inner 4096, và 16 attention head với chiều 64 mỗi cái. Chúng tôi cũng sử dụng rotary position embedding [SLP+21] với chiều rotary 32. Những lựa chọn kiến trúc này được áp dụng từ [NPH+22]. Chúng tôi cũng sử dụng cùng tokenizer như codegen-350M-mono [NPH+22]. Ngoài FlashAttention, các mô hình của chúng tôi không sử dụng các kỹ thuật khác như Fill-In-the-Middle (FIM) [BJT+22], hoặc Multi-Query-Attention (MQA) [RSR+20] có thể tăng hiệu suất và hiệu quả hơn nữa [LAZ+23].

Đối với cả pre-training và fine-tuning, chúng tôi nối các tập dữ liệu tương ứng thành một mảng một chiều với token "⟨∣endoftext∣⟩" được sử dụng để phân tách các file. Chúng tôi huấn luyện các mô hình trên độ dài chuỗi 2048 được cắt từ mảng tập dữ liệu với loss dự đoán token tiếp theo. Chúng tôi sử dụng huấn luyện fp16 với optimizer AdamW, lịch học tỷ lệ linear-warmup-linear-decay, và attention và residual dropout 0.1. Chúng tôi huấn luyện trên 8 GPU Nvidia-A100 sử dụng deepspeed. Mô hình cơ sở pre-trained phi-1-base được có được trong vòng chưa đến 4 ngày huấn luyện. Fine-tuning để có được phi-1 sử dụng thêm 7 giờ trên cùng phần cứng.

**Pre-training.** phi-1-base được huấn luyện trên tập dữ liệu CodeTextbook (corpus code-ngôn ngữ đã lọc và sách giáo khoa tổng hợp). Chúng tôi sử dụng kích thước batch hiệu quả 1024 (bao gồm song song hóa dữ liệu và tích lũy gradient), tỷ lệ học tối đa 1e-3 với warmup qua 750 bước, và weight decay 0.1, tổng cộng 36,000 bước. Chúng tôi sử dụng checkpoint ở 24,000 bước làm phi-1-base của chúng tôi - điều này tương đương với ~8 epoch trên tập dữ liệu CodeTextbook cho tổng cộng hơi trên 50B tổng token huấn luyện. Mặc dù kích thước và tính toán nhỏ, mô hình này đã đạt được độ chính xác 29% trên HumanEval.

**Fine-tuning.** phi-1 được có được bằng cách fine-tune phi-1-base trên tập dữ liệu CodeExercises. Đối với fine-tuning, chúng tôi sử dụng cùng thiết lập như pre-training, nhưng siêu tham số khác: chúng tôi sử dụng kích thước batch hiệu quả 256, tỷ lệ học tối đa 1e-4 với 50 bước warmup, và weight decay 0.01. Chúng tôi huấn luyện tổng cộng 6,000 bước và chọn checkpoint tốt nhất (lưu mỗi 1000 bước).

## 3 Những bước nhảy về khả năng mô hình sau khi fine-tune trên CodeExercises

Hình 2.1 cho thấy rằng cải thiện lớn nhất trong HumanEval là kết quả từ việc fine-tuning trên tập dữ liệu CodeExercises nhỏ (<200M token). CodeExercises chỉ bao gồm các nhiệm vụ Python ngắn sử dụng chỉ các thư viện Python cơ bản. Trong phần này, chúng tôi chứng minh rằng, khá đáng chú ý là mô hình sau fine-tuning cũng thể hiện sự cải thiện đáng kể trong việc thực hiện các nhiệm vụ không có trong tập dữ liệu fine-tuning. Điều này bao gồm quản lý các nhiệm vụ thuật toán phức tạp và sử dụng các thư viện bên ngoài. Điều này gợi ý rằng quy trình fine-tuning của chúng tôi có thể đã giúp mô hình tổ chức lại và củng cố kiến thức thu được trong quá trình pre-training, ngay cả khi kiến thức đó không có rõ ràng trong tập dữ liệu CodeExercises của chúng tôi. Trong phần này, chúng tôi sẽ tập trung vào việc so sánh và đối chiếu định tính các khả năng của mô hình fine-tuned phi-1 và mô hình cơ sở pre-trained 1.3B tham số phi-1-base.

### 3.1 Fine-tuning cải thiện khả năng hiểu của mô hình

Sử dụng một hàm Python đơn giản mà chúng tôi tự tạo, chúng tôi quan sát thấy rằng mô hình cho thấy mức độ hiểu biết và tuân thủ hướng dẫn cao hơn nhiều sau khi fine-tuning. Cụ thể, chúng tôi thấy rằng phi-1-base gặp khó khăn với các mối quan hệ logic trong prompt, trong khi phi-1 có thể diễn giải câu hỏi và tạo ra câu trả lời chính xác. Trong ví dụ này, ngay cả mô hình phi-1-small 350M của chúng tôi cũng cho thấy một mức độ hiểu biết về vấn đề mặc dù giải pháp kết quả sai. Chúng tôi thấy những xu hướng như vậy một cách nhất quán trong các tương tác của chúng tôi, xem Phụ lục A để có ví dụ khác.

**Prompt:**
```
""" Alice, Bob và Charles đang chơi game. Alice trước tiên chọn một số ngẫu nhiên từ 333 
đến 678, Bob sau đó chọn một số bắt đầu từ số của Alice đến 888, Charles sau đó chọn một 
số bắt đầu từ 123 đến số của Bob. Alice nhận một điểm nếu số của Alice trừ số của Charles 
chia hết cho floor của căn bậc hai của số Bob, nếu không Bob nhận một điểm. Mô phỏng điểm 
của Alice và Bob trong 30 lần lặp.
"""
```

**phi-1:**
```python
points = [0, 0]
for i in range(30):
    alice_num = random.randint(333, 678)
    bob_num = random.randint(alice_num, 888)
    charles_num = random.randint(123, bob_num)
    if (alice_num - charles_num) % int(bob_num **0.5) == 0:
        points[0] += 1
    else:
        points[1] += 1
return f"Alice: {points[0]}, Bob: {points[1]}"
```

**phi-1-base:**
```python
self.points = 0
self.alice = 333
self.bob = 678
self.charles = 888
self.bob_points = 0
self.alice_points = 0
self.iterations = 30
self.alice_turn = True
self.bob_turn = True
self.alice_turn_points = 0
self.bob_turn_points = 0
self.alice_turn_points_history = []
self.bob_turn_points_history = []
self.alice_turn_points_history.append(self.alice_points)
self.bob_turn_points_history.append(self.bob_points)
```

**phi-1-small:**
```python
indices = []
import random
alice_points = 0
bob_points = 0
for i in range(30):
    alice_num = random.randint(333, 678)
    bob_num = random.randint(123, bob_num)
    if (alice_num - bob_num) % math.floor(math.sqrt(bob_num)) == 0:
        alice_points += 1
    if (i % 2) == 0:
        bob_points += 1
print(f"Alice gets {alice_points} points, while Bob gets {bob_points} points.")
```

### 3.2 Fine-tuning cải thiện khả năng sử dụng thư viện bên ngoài của mô hình

Chúng tôi chứng minh ở đây rằng fine-tuning trên CodeExercises bất ngờ cải thiện khả năng sử dụng các thư viện bên ngoài như Pygame và Tkinter của mô hình, mặc dù các bài tập của chúng tôi không chứa những thư viện này. Điều này gợi ý rằng fine-tuning của chúng tôi không chỉ cải thiện các nhiệm vụ chúng tôi nhắm tới, mà còn làm cho các nhiệm vụ không liên quan dễ dàng trích xuất từ pre-training hơn. Để tham khảo, Hình 3.1 cho thấy phân phối import package trong tập dữ liệu CodeExercises của chúng tôi.

**Ví dụ PyGame.** Chúng tôi bắt đầu với một PyGame yêu cầu mô hình tạo code để di chuyển một quả bóng:

**Prompt:**
```
""" Viết một PyGame. Có một quả bóng. Ở mỗi lần lặp, (1). Trục x của quả bóng tăng 
bằng một số nguyên ngẫu nhiên giữa (-10, 10), (2). Trục y của quả bóng tăng bằng một 
số nguyên ngẫu nhiên giữa (-20, 20). Trục x của quả bóng nên ở trong 0-400, và trục y 
của quả bóng nên ở trong 0-600. Khi người dùng nhấn 'space', đặt trục x của quả bóng 
thành 200 và trục y của quả bóng thành 400.
"""
```

**phi-1:**
```python
...
screen = pygame.display.set_mode((400, 600))
x, y = 0, 0
while True:
    for event in pygame.event.get():
        ...
        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_SPACE:
                x = 200
                y = 400
    x += random.randint(-10, 10)
    y += random.randint(-20, 20)
    if x < 0:
        x = 0
    elif x > 400:
        x = 400
    if y < 0:
        y = 0
    elif y > 600:
        y = 600
    screen.fill((255, 255, 255))
    pygame.draw.circle(screen, (255, 0, 0), (x, y), 10)
    pygame.display.update()
```

**phi-1-base:**
```python
...
ball_x = random.randint(-10, 10)
ball_y = random.randint(-20, 20)
ball_vx = random.randint(-5, 5)
ball_vy = random.randint(-5, 5)
...
screen = pygame.display.set_mode((800, 600))
...
while not done:
    for event in pygame.event.get():
        ...
        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_SPACE:
                ball_x = 200
                ball_y = 400
    ball_x += ball_vx
    ball_y += ball_vy
    ball_vx += ball
    pygame.display.flip()
```

**phi-1-small:**
```python
...
x, y = 0, 0
dx, dy = random.randint(-10, 10), random.randint(-20, 20)
while True:
    screen.fill((0, 0, 0))
    pygame.draw.rect(screen, (255, 255, 255), (x, y, 400, 600))
    pygame.display.update()
    x += dx
    y += dy
    if x + dx > 400:
        dx = -dx
    elif x + dx < 0:
        dx = dx
    if y + dy > 600:
        dy = -dy
    elif y + dy < 0:
        dy = dy
    pygame.time.wait(50)
```

Đoạn code trên cho thấy vòng lặp chính của một chương trình PyGame đơn giản làm cho quả bóng nảy trên màn hình. phi-1 áp dụng đúng các hàm PyGame để cập nhật và vẽ quả bóng, như được hướng dẫn bởi prompt. phi-1-base và phi-1-small tạo ra các lời gọi hàm đúng về mặt cú pháp nhưng không liên quan về mặt ngữ nghĩa. Chúng ta có thể thấy rằng phi-1-base cho thấy một số khả năng sử dụng các lời gọi API thích hợp, nhưng nó thất bại trong việc theo logic của nhiệm vụ, trong khi phi-1-small sau fine-tuning hiểu logic nhưng không có đủ khả năng để học các lời gọi hàm đúng.

**Ví dụ TKinter.** Ví dụ thứ hai của chúng tôi là một ứng dụng TKinter nơi chúng tôi yêu cầu mô hình cập nhật một textfield tương ứng khi người dùng nhấp chuột vào các nút.

**Prompt:**
```
""" Viết một ứng dụng tkinter, có một textfield ở trên, một nút submit ở dưới và một 
nút recover bên cạnh nó. Khi nhấn nút submit, lấy văn bản từ textfield và lưu nó vào 
một chuỗi toàn cục s. Nếu văn bản chứa từ "pew" cập nhật văn bản textfield thành 
"pewpewpew?", nếu không, thêm "no pew..." vào cuối. Khi nhấn nút recover, thay thế 
văn bản trong textfield bằng s.
"""
```

**phi-1:**
```python
...
global s
s = ""

def submit():
    global s
    text = textfield.get()
    if "pew" in text:
        textfield.delete(0, tk.END)
        textfield.insert(0, "pewpewpew...")
    else:
        textfield.delete(0, tk.END)
        textfield.insert(0, text + "no pew...")
    s = text

def recover():
    global s
    textfield.delete(0, tk.END)
    textfield.insert(0, s)

textfield = tk.Entry(root)
textfield.pack()
submit_button = tk.Button(root, text="Submit", command=submit)
submit_button.pack()
recover_button = tk.Button(root, text="Recover", command=recover)
recover_button.pack()
root.mainloop()
```

**phi-1-base:**
```python
global s
s = s.replace("pew", "pewpewpew")
if "pew" in s:
    s = s.replace("pew", "pewpewpew")
else:
    s = s + "no pew..."
text.delete(0.0, END)
text.insert(0.0, s)

def main():
    global s
    s = "pewpewpew"
    text.delete(0.0, END)
    text.insert(0.0, s)
    text.tag_add("button", "<Button-1>", "<Button-3>")
    text.tag_add("button", "<Button-2>", "<Button-4>")
    # Mô hình tiếp tục thêm các nút từ đây.
```

**phi-1-small:**
```python
import tkinter as tk
root = tk.Tk()
global s
text = ""
while True:
    event, values = tk.popup()
    if event == "submit":
        text += values.split()[0] + " "
    elif event == "textfield":
        s += values.split()[0] + " "
    elif event == "submit-button-text":
        text = values.split()[0] + " "
    elif event == "text-button-text":
        s = text
    if "pew" in text:
        text = text.replace("pew", "pewpewpew")
    if "no pew" in text:
        break
tk.destroy()
```

Ba mô hình hoàn thành cho thấy khoảng cách lớn trong việc hiểu prompt của chúng. Cả phi-1-base và phi-1-small đều thất bại trong việc sử dụng API Tkinter đúng và tạo ra các lời gọi hàm vô nghĩa. Mặt khác, phi-1 triển khai GUI và tất cả các hàm một cách chính xác (ngoại trừ việc nó không sao chép "pewpewpew?" một cách chính xác). Chúng tôi trình bày hai ví dụ bổ sung trong Phụ lục A cho pytorch và pyplot.

**Ví dụ chế độ Chat.** Cuối cùng, chúng tôi cho thấy rằng phi-1 có khả năng chat tốt hơn phi-1-base mặc dù dữ liệu chat chỉ có trong pre-training, không có trong fine-tuning.

**Prompt:**
```
Student: Tôi có một Python pyplot, tôi muốn tăng độ phân giải và xoay nó, tôi nên làm gì? TA:
```

**phi-1:**
```
1. Đặt tham số 'dpi' thành độ phân giải mong muốn (ví dụ 300 cho 300 DPI)
2. Sử dụng hàm 'rotate' từ module 'matplotlib.pyplot' để xoay biểu đồ 90 độ theo chiều kim đồng hồ

Đây là một ví dụ:
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot([1, 2, 3], [4, 5, 6])
# Đặt độ phân giải thành 300 DPI
ax.set_dpi(300)
# Xoay biểu đồ 90 độ theo chiều kim đồng hồ
ax.rotate(90)
```

**phi-1-base:**
```
Bạn có thể sử dụng hàm `setp()` để đặt độ phân giải và hàm `set_rotation()` để xoay biểu đồ. <|endoftext|>
```

**phi-1-small:**
```
Tôi là một Python pyplot, và tôi muốn tăng độ phân giải và xoay nó, tôi nên làm gì?
SECOND:
Tôi là một Python pyplot, và tôi muốn tăng độ phân giải và xoay nó, tôi nên làm gì?
THIRD:...
```

## 4 Đánh giá trên các vấn đề không thông thường với chấm điểm LLM

Một mối quan tâm tiềm tàng với hiệu suất tốt đáng ngạc nhiên của phi-1 trên HumanEval (xem Bảng 1 và Hình 2.1) là có thể có sự ghi nhớ xuất phát từ nhiễm của tập dữ liệu CodeExercises tổng hợp. Chúng tôi nghiên cứu khả năng nhiễm tiềm tàng này trực tiếp trong Phần 5, trong khi phần này giải quyết mối quan tâm với một đánh giá mới được thiết kế để đủ không thông thường để không có khả năng xuất hiện trong tập dữ liệu huấn luyện của chúng tôi.

Để giảm thiểu bias và rò rỉ, các vấn đề đánh giá mới được tạo ra bởi một nhóm chuyên dụng không truy cập vào tập dữ liệu CodeExercises hoặc mô hình cuối cùng. Họ tạo ra 50 vấn đề mới trong cùng định dạng như HumanEval với hướng dẫn thiết kế các vấn đề có khả năng không xuất hiện trong các cơ sở code thực tế hoặc như bài tập lập trình. Đây là một ví dụ về vấn đề như vậy:

```python
def sort_concat_square_deduplicate(list1, list2, my_threshold):
    """
    Hàm này nhận hai danh sách số nguyên, sắp xếp mỗi danh sách theo thứ tự tăng dần,
    nối chúng lại, bình phương các mục ở chỉ số chẵn, lọc ra các mục
    nhỏ hơn my_threshold và sau đó loại bỏ các bản sao. Danh sách kết quả được
    trả về.
    """
```

Một trong những thách thức của việc đánh giá các mô hình ngôn ngữ trên các nhiệm vụ lập trình là đầu ra của mô hình thường là nhị phân: hoặc code vượt qua tất cả các unit test hoặc nó thất bại. Tuy nhiên, điều này không nắm bắt được những tinh tế của hiệu suất mô hình, vì nó có thể đã tạo ra code gần như chính xác nhưng có lỗi nhỏ, hoặc code hoàn toàn sai nhưng tình cờ vượt qua một số test. Có thể nói, một cách đánh giá kỹ năng lập trình của mô hình có thông tin hơn là so sánh đầu ra của nó với giải pháp đúng và chấm điểm dựa trên mức độ phù hợp với logic mong đợi. Điều này tương tự như cách con người được đánh giá trong các cuộc phỏng vấn lập trình, nơi người phỏng vấn không chỉ chạy code mà còn kiểm tra lý luận và chất lượng của giải pháp.

Để đánh giá các giải pháp ứng viên, do đó chúng tôi áp dụng cách tiếp cận sử dụng GPT-4 để chấm điểm giải pháp (như trong [EL23]). Cách tiếp cận này có hai lợi thế riêng biệt: (1) bằng cách sử dụng GPT-4 làm người chấm điểm, chúng tôi có thể tận dụng kiến thức và khả năng sinh của nó để có được tín hiệu chi tiết và có ý nghĩa hơn về khả năng lập trình của mô hình học sinh, và (2) nó loại bỏ nhu cầu về các test. Prompt của chúng tôi hướng dẫn LLM đánh giá giải pháp của học sinh trước tiên trong một đánh giá bằng lời ngắn gọn tiếp theo bằng điểm từ 0 đến 10.

Xem Bảng 2 cho kết quả của chúng tôi với phi-1 và các mô hình cạnh tranh. Các điểm trên các vấn đề không thông thường mới của chúng tôi cho cùng thứ hạng như HumanEval (xem Bảng 1). phi-1 một lần nữa đạt được điểm số cao hơn đáng kể so với StarCoder, như nó đã làm trên HumanEval. Cho rằng các vấn đề mới không có cơ hội nhiễm dữ liệu huấn luyện và, hơn nữa, được thiết kế để nằm ngoài phân phối huấn luyện, những kết quả này tăng đáng kể sự tin tưởng của chúng tôi vào tính hợp lệ của hiệu suất phi-1.

## 5 Tỉa dữ liệu để đánh giá hiệu suất không thiên vị

Trong Hình 2.1, chúng ta thấy rằng huấn luyện trên CodeExercises dẫn đến sự tăng đáng kể trong hiệu suất của mô hình trên benchmark HumanEval. Để điều tra sự tăng này, chúng tôi đề xuất tỉa tập dữ liệu CodeExercises bằng cách loại bỏ các file "tương tự" với những file trong HumanEval. Quy trình này có thể được xem như một "dạng mạnh" của khử nhiễm dữ liệu. Sau đó chúng tôi huấn luyện lại mô hình trên dữ liệu đã tỉa như vậy, và vẫn quan sát hiệu suất mạnh trên HumanEval. Cụ thể, ngay cả sau khi tỉa một cách tích cực hơn 40% tập dữ liệu CodeExercises (điều này thậm chí tỉa các file chỉ mơ hồ tương tự với HumanEval, xem Phụ lục C), phi-1 được huấn luyện lại vẫn vượt trội StarCoder.

Chúng tôi tin rằng thí nghiệm tỉa dữ liệu như vậy là một cách công bằng để đánh giá hiệu suất, và có thông tin hơn so với các nghiên cứu "nhiễm" tiêu chuẩn trong tài liệu thường dựa trên các thước đo chồng chéo giữa dữ liệu huấn luyện và test (ví dụ, Phần 4.8 của [AON+21]). Để hoàn chỉnh, chúng tôi bắt đầu phần này bằng cách tiến hành một thí nghiệm nhiễm tiêu chuẩn, cho thấy rằng CodeExercises không bị nhiễm bởi HumanEval theo nghĩa tiêu chuẩn này.

### 5.1 Chồng chéo N-gram

N-gram đo độ tương tự của các đoạn văn bản dựa trên các chuỗi n-từ được chia sẻ. Chúng tôi tính toán chồng chéo n-gram giữa docstring của mỗi câu hỏi humaneval và mỗi bài tập trong tập dữ liệu CodeExercises được tạo ra. Chúng tôi tìm thấy 4 câu hỏi humaneval với chồng chéo 13-gram với ít nhất một mục trong tập dữ liệu của chúng tôi. Sau khi điều tra thêm, chúng tôi phát hiện ra rằng tất cả 4 trường hợp chồng chéo trong 13-gram đều là dương tính giả như ví dụ bên dưới. Phân tích chồng chéo n-gram của chúng tôi cho thấy rằng tập dữ liệu của chúng tôi có chồng chéo ký tự-với-ký tự tối thiểu với HumanEval.

**HumanEval:**
```
Bạn được cho một danh sách không rỗng các số nguyên dương. Trả về số nguyên lớn nhất
mà lớn hơn không, và có tần suất lớn hơn hoặc bằng giá trị của chính số nguyên đó.
Tần suất của một số nguyên là số lần nó xuất hiện trong danh sách.
```

**CodeExercises:**
```
Tính tổng phân tích tần suất công suất của một danh sách số nguyên. Tổng phân tích
tần suất công suất được tính bằng cách lấy tổng bình phương của tần suất của mỗi số
nguyên duy nhất trong danh sách. Tần suất của một số nguyên là số lần nó xuất hiện
trong danh sách.
```

### 5.2 Phân tích độ tương tự dựa trên embedding và cú pháp

Như chúng ta vừa thấy, phân tích n-gram không đủ tinh tế để tìm các đoạn code tương tự giữa HumanEval và CodeExercises. Thay vào đó, chúng tôi sử dụng sự kết hợp của khoảng cách embedding và dựa trên cú pháp. Đối với khoảng cách embedding, chúng tôi tính toán khoảng cách L2 giữa embedding của các đoạn code nơi embedding được rút ra từ mô hình CodeGen-Mono 350M được pre-train [NPH+23]. Chúng tôi quan sát thấy rằng khoảng cách embedding thành công trong việc nắm bắt các cặp code nơi ngữ nghĩa code tổng thể tương tự, có thể được suy ra qua Python Docstring, tên hàm/lớp, cũng như cấu trúc code. Đối với khoảng cách dựa trên cú pháp, chúng tôi tính toán khoảng cách chỉnh sửa (chuỗi) giữa các cây cú pháp trừu tượng (AST) của hai đoạn code đã cho. Khoảng cách AST thành công xác định các phần chồng chéo giữa các cặp code trong khi bất khả tri với văn bản không cú pháp như đặt tên biến/hàm, bình luận và Python Docstring. Để tỉa CodeExercises của chúng tôi, chúng tôi cố định một ngưỡng cho khoảng cách embedding, và chúng tôi thử nghiệm một số tỷ lệ khớp τ cho khoảng cách AST. Xem Phụ lục C để có ví dụ về các cặp code được nắm bắt với khoảng cách embedding và các tỷ lệ khớp AST khác nhau τ. Chúng tôi thay đổi τ giữa 0.95 và 0.8, tương ứng với việc loại bỏ từ 42.5K đến 354K trong tổng số 879.5K vấn đề trong CodeExercises.

Bảng 3 tóm tắt hiệu suất của phi-1 được huấn luyện lại trên các tập dữ liệu đã tỉa (với τ=0.95,0.9,0.85 và 0.8) so với phi-1 gốc được huấn luyện trên CodeExercises đầy đủ và StarCoder-prompted 15.5B tham số. Chúng tôi chia các vấn đề HumanEval thành hai tập con ("tương tự" và "không tương tự") dựa trên việc chúng có ít nhất một khớp gần (cho τ đã cho này) bên trong tập dữ liệu CodeExercises gốc hay không. Sau đó chúng tôi báo cáo độ chính xác của các mô hình trên mỗi tập con HumanEval riêng biệt. Như có thể thấy, ngay cả sau khi tỉa nặng tập dữ liệu của chúng tôi, phi-1 vẫn vượt trội StarCoder-Prompted với một khoảng cách lớn, điều này xác nhận rằng sự tăng hiệu suất của chúng tôi không phải do "nhiễm" tập dữ liệu, ngay cả khi thuật ngữ sau được hiểu một cách lỏng lẻo. Cũng lưu ý rằng độ chính xác của tất cả các mô hình thấp hơn trên tập con HumanEval không tương tự so với tập tương tự.

## 6 Kết luận

Cũng như một sách giáo khoa toàn diện, được tạo tác khéo léo có thể cung cấp cho học sinh kiến thức cần thiết để thành thạo một chủ đề mới, công trình của chúng tôi chứng minh tác động đáng chú ý của dữ liệu chất lượng cao trong việc mài giũa khả năng thành thạo của mô hình ngôn ngữ trong các nhiệm vụ tạo code. Bằng cách tạo dữ liệu "chất lượng sách giáo khoa", chúng tôi đã có thể huấn luyện một mô hình vượt trội hầu hết các mô hình mã nguồn mở trên các benchmark lập trình như HumanEval và MBPP mặc dù nhỏ hơn 10 lần về kích thước mô hình và nhỏ hơn 100 lần về kích thước tập dữ liệu. Chúng tôi đưa ra giả thuyết rằng dữ liệu chất lượng cao như vậy cải thiện đáng kể hiệu quả học của các mô hình ngôn ngữ cho code vì chúng cung cấp các ví dụ rõ ràng, tự hoàn chỉnh, hướng dẫn và cân bằng về các khái niệm và kỹ năng lập trình.

Vẫn còn một số hạn chế của mô hình chúng tôi so với các mô hình lớn hơn cho code. Thứ nhất, phi-1 chuyên về lập trình Python, điều này hạn chế tính linh hoạt của nó so với các mô hình đa ngôn ngữ. Thứ hai, phi-1 thiếu kiến thức chuyên ngành của các mô hình lớn hơn như lập trình với các API cụ thể hoặc sử dụng các package ít phổ biến. Cuối cùng, do tính chất có cấu trúc của các tập dữ liệu và thiếu đa dạng về ngôn ngữ và phong cách, phi-1 ít mạnh mẽ hơn đối với các biến thể phong cách hoặc lỗi trong prompt (ví dụ, hiệu suất của nó giảm đáng kể khi có lỗi ngữ pháp trong prompt). Chúng tôi mở rộng về những hạn chế này và đưa ra ví dụ về các chế độ thất bại của phi-1 trong Phụ lục B.

Không có hạn chế nào trong số này có vẻ cơ bản, và với nhiều công việc hơn, cách tiếp cận của chúng tôi có thể được sử dụng để giải quyết từng hạn chế, mặc dù không rõ việc mở rộng nào có thể cần thiết để vượt qua chúng (cả cho kích thước mô hình và kích thước tập dữ liệu). Chúng tôi cũng tin rằng những lợi ích đáng kể có thể đạt được bằng cách sử dụng GPT-4 để tạo dữ liệu tổng hợp thay vì GPT-3.5, vì chúng tôi nhận thấy rằng dữ liệu GPT-3.5 có tỷ lệ lỗi cao. Thật thú vị là phi-1 có thể đạt được khả năng lập trình cao như vậy mặc dù những lỗi đó (một hiện tượng tương tự được quan sát trong [AZL23] nơi một mô hình ngôn ngữ có thể được huấn luyện trên dữ liệu với tỷ lệ lỗi 100% và vẫn tạo ra câu trả lời đúng tại thời điểm test).

Tổng quát hơn, công trình của chúng tôi cung cấp bằng chứng rằng việc phát triển phương pháp tốt để tạo các tập dữ liệu chất lượng cao là một hướng nghiên cứu trung tâm để thúc đẩy xử lý ngôn ngữ tự nhiên và các lĩnh vực liên quan (xem thêm [JWJ+23] để có bằng chứng thêm). Tuy nhiên, việc tạo các tập dữ liệu chất lượng cao không phải là nhiệm vụ đơn giản, và nó đặt ra một số thách thức cần được giải quyết. Một thách thức là đảm bảo rằng tập dữ liệu bao gồm tất cả nội dung và khái niệm liên quan mà người ta muốn mô hình học, và làm điều đó một cách cân bằng và đại diện. Một thách thức khác là đảm bảo rằng tập dữ liệu thực sự đa dạng và không lặp lại, để mô hình không đơn giản overfit với dữ liệu hoặc ghi nhớ các mẫu hoặc giải pháp cụ thể. Điều này đòi hỏi việc tìm cách để đưa tính ngẫu nhiên và sáng tạo vào quá trình tạo dữ liệu, đồng thời vẫn duy trì chất lượng và tính nhất quán của các ví dụ. Hơn nữa, ngay cả sau khi tạo ra các tập dữ liệu như vậy, chúng ta thiếu phương pháp tốt để đo lường và đánh giá lượng đa dạng và dư thừa trong dữ liệu. Ví dụ, nếu chúng ta có một tập dữ liệu với các bài tập lập trình, rất khó để xác định có bao nhiều biến thể khác nhau của mỗi bài tập tồn tại, và chúng được phân phối như thế nào trong tập dữ liệu. Cuối cùng, khi chính các mô hình ngôn ngữ sẽ được sử dụng để tuyển chọn dữ liệu cho các mô hình ngôn ngữ tương lai, nó càng tăng tính cấp bách về các tác động đạo đức và xã hội của việc huấn luyện các mô hình như vậy, như trách nhiệm, tính minh bạch và bias của dữ liệu và các mô hình có liên quan trong quá trình này.
