# 2404.05875.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-generation/2404.05875.pdf
# Kích thước tệp: 4505253 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CodecLM: Căn chỉnh các Mô hình Ngôn ngữ với Dữ liệu Tổng hợp Được Tùy chỉnh
Zifeng Wang†, Chun-Liang Li†, Vincent Perot∗, Long T. Le†,
Jin Miao‡, Zizhao Zhang‡, Chen-Yu Lee†, Tomas Pfister†
†Google Cloud AI Research,‡Google Cloud AI,∗Google Research
{zifengw, chunliang, vperot, longtle,
jinmiao, zizhaoz, chenyulee, tpfister}@google.com
Tóm tắt
Điều chỉnh hướng dẫn đã nổi lên như yếu tố quan trọng trong việc căn chỉnh các mô hình ngôn ngữ lớn (LLM) với các hướng dẫn nhiệm vụ cụ thể, từ đó giảm thiểu sự khác biệt giữa mục tiêu dự đoán token tiếp theo và các mục tiêu thực tế của người dùng. Để giảm chi phí lao động và thời gian thu thập hoặc chú thích dữ liệu bởi con người, các nhà nghiên cứu bắt đầu khám phá việc sử dụng LLM để tạo ra dữ liệu tổng hợp được căn chỉnh theo hướng dẫn. Các nghiên cứu gần đây tập trung vào việc tạo ra các hướng dẫn đa dạng và áp dụng LLM để tăng độ phức tạp của hướng dẫn, thường bỏ qua các trường hợp sử dụng downstream. Vẫn chưa rõ làm thế nào để tùy chỉnh dữ liệu chất lượng cao nhằm khai thác khả năng tuân theo hướng dẫn tốt hơn trong các phân phối hướng dẫn mục tiêu khác nhau và LLM.
Để giải quyết vấn đề này, chúng tôi giới thiệu CodecLM, một khung tổng quát để tạo ra dữ liệu tổng hợp chất lượng cao một cách thích ứng cho việc căn chỉnh LLM với các phân phối hướng dẫn downstream và LLM khác nhau. Dựa trên các nguyên tắc Mã hóa-Giải mã, chúng tôi sử dụng LLM như các codec để hướng dẫn quá trình tạo ra dữ liệu. Trước tiên chúng tôi mã hóa các hướng dẫn gốc thành siêu dữ liệu, là những từ khóa ngắn gọn được tạo ra tức thì để nắm bắt phân phối hướng dẫn mục tiêu, sau đó giải mã siêu dữ liệu để tạo ra các hướng dẫn được tùy chỉnh. Chúng tôi cũng giới thiệu Self-Rubrics và Contrastive Filtering trong quá trình giải mã để tùy chỉnh các mẫu hiệu quả về dữ liệu. Các thử nghiệm rộng rãi trên bốn benchmark tuân theo hướng dẫn miền mở xác nhận hiệu quả của CodecLM so với các phương pháp tiên tiến hiện tại.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng đáng chú ý trên một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023a; Anil et al., 2023). Đặc biệt, LLM có thể được huấn luyện để cải thiện việc tuân theo hướng dẫn thông qua các phương pháp khác nhau, bao gồm fine-tuning trên dữ liệu được chú thích bởi con người (Touvron et al., 2023; Bai et al., 2022) hoặc trích xuất kiến thức từ các LLM mạnh hơn (Wang et al., 2022; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023). Tiến bộ gần đây trong lĩnh vực này nhấn mạnh vai trò quan trọng của dữ liệu chất lượng cao trong việc nâng cao khả năng tuân theo hướng dẫn của LLM (Zhou et al., 2023a; Köpf et al., 2023; Chen et al., 2023b). Tuy nhiên, việc thu thập dữ liệu như vậy thông qua chú thích của con người vẫn tốn kém và khó mở rộng quy mô, cản trở tiến bộ tiếp theo.

Như một giải pháp thay thế cho chú thích của con người, các nghiên cứu gần đây khám phá việc tạo ra các cặp hướng dẫn-phản hồi cho việc căn chỉnh LLM bằng cách nhắc nhở chúng với dữ liệu mẫu hoặc prompts và lặp đi lặp lại việc tinh chỉnh kết quả (Honovich et al., 2022; Wang et al., 2022; Li et al., 2023; Xu et al., 2023). Mặc dù các phương pháp này hiệu quả trong việc tạo ra các hướng dẫn đa dạng và phức tạp cho việc căn chỉnh LLM nói chung, các ứng dụng thực tế thường ưu tiên việc tùy chỉnh LLM cho các nhiệm vụ downstream cụ thể như các ứng dụng doanh nghiệp riêng lẻ hoặc các agent trợ lý cá nhân (OpenAI, 2023b), điều này thường liên quan đến các phân phối hướng dẫn khác nhau.

[Hình ảnh được mô tả: Tổng quan về CodecLM. Trước tiên chúng tôi mã hóa các hướng dẫn gốc thành siêu dữ liệu để nắm bắt phân phối cơ bản của các hướng dẫn. Siêu dữ liệu này sau đó được giải mã thông qua Self-Rubrics và Contrastive Filtering để tùy chỉnh các hướng dẫn tổng hợp chất lượng cao được căn chỉnh với phân phối hướng dẫn mục tiêu. Các hướng dẫn và phản hồi trung gian được bỏ qua trong hình để rõ ràng hơn.]

--- TRANG 2 ---
thường liên quan đến các phân phối hướng dẫn khác nhau. Nhu cầu về căn chỉnh cụ thể theo nhiệm vụ này đưa chúng ta đến một câu hỏi cốt lõi cho việc tổng hợp dữ liệu: làm thế nào chúng ta có thể tùy chỉnh dữ liệu tổng hợp để căn chỉnh LLM cho các nhiệm vụ tuân theo hướng dẫn khác nhau?

Cụ thể, các phương pháp tổng hợp dữ liệu hiện tại không cung cấp giải pháp hiệu quả cho việc căn chỉnh LLM cụ thể theo nhiệm vụ. Trong khi các công trình trước đây của Wang et al. (2022) và Xu et al. (2023) nhấn mạnh tính đa dạng và độ phức tạp như dấu hiệu của dữ liệu chất lượng cao, những phương pháp này gặp khó khăn khi đối mặt với các nhiệm vụ downstream khác nhau có thể liên quan đến các phân phối hướng dẫn cụ thể. Một tập dữ liệu đa dạng cho một nhiệm vụ có thể không bao phủ hiệu quả phân phối hướng dẫn cho nhiệm vụ khác. Hơn nữa, định nghĩa về các hướng dẫn "phức tạp" có thể mang tính chủ quan và khác nhau giữa các nhiệm vụ. Để làm phức tạp thêm vấn đề, một LLM có thể xuất sắc với một số hướng dẫn có vẻ phức tạp trong khi gặp khó khăn với những hướng dẫn khác có vẻ đơn giản theo tiêu chí do con người xây dựng. Những hạn chế này nhấn mạnh nhu cầu về một khung tổng hợp dữ liệu thống nhất có thể tạo ra dữ liệu được tùy chỉnh để căn chỉnh LLM trên các nhiệm vụ downstream cụ thể.

Trong công trình này, chúng tôi trình bày một khung mới, CodecLM, tạo ra dữ liệu chất lượng cao được tùy chỉnh một cách có hệ thống để căn chỉnh LLM cho các nhiệm vụ downstream khác nhau. Tổng quan cấp cao về CodecLM được thể hiện trong Hình 1. Lấy cảm hứng từ các nguyên tắc của quá trình Mã hóa-Giải mã (Kramer, 1991; Kingma và Welling, 2013), chúng tôi tận dụng một LLM mạnh như một codec để "mã hóa" các hướng dẫn gốc từ nhiệm vụ mục tiêu của chúng tôi thành siêu dữ liệu hướng dẫn và sau đó "giải mã" siêu dữ liệu thành các hướng dẫn tổng hợp được tùy chỉnh. Siêu dữ liệu phục vụ như một sự trừu tượng hóa ở mức từ của phân phối hướng dẫn đầu vào, bao gồm trường hợp sử dụng và kỹ năng để tuân theo hướng dẫn hiệu quả. Nó có thể được tạo ra tự động bằng cách mã hóa các hướng dẫn gốc, hoặc được cung cấp trực tiếp bởi người dùng với kỳ vọng cấp cao về nhiệm vụ downstream.

Một khi siêu dữ liệu được trích xuất, chúng tôi sau đó "giải mã" chúng để tạo ra các hướng dẫn được tùy chỉnh. Chúng tôi bắt đầu bằng cách nhắc nhở một LLM với siêu dữ liệu như các ràng buộc, tạo ra các hướng dẫn cơ bản. Để nâng cao chất lượng hướng dẫn, chúng tôi giới thiệu Self-Rubrics. Nó lấy mẫu các hành động phù hợp từ các LLM mạnh để làm cho hướng dẫn cơ bản trở nên phức tạp hoặc thách thức hơn dựa trên các tiêu chí nó tạo ra cho các siêu dữ liệu khác nhau. Một cách trực quan, một hướng dẫn QA kiến thức tổng quát về toán học sẽ khác về tiêu chí độ phức tạp so với một hướng dẫn về viết sáng tạo trong thể thao. Với các tiêu chí và hành động tự tạo dựa trên siêu dữ liệu, LLM mạnh tạo ra các hướng dẫn căn chỉnh LLM mục tiêu tốt hơn với kiến thức cụ thể cần thiết cho nhiệm vụ downstream. Chúng tôi có thể chạy Self-Rubrics một cách lặp đi lặp lại để kiểm soát độ phức tạp của hướng dẫn, tương tự như Xu et al. (2023), và cuối cùng tạo ra các phản hồi tương ứng.

Chúng tôi cũng giới thiệu Contrastive Filtering trong quá trình giải mã để xác định thêm các cặp hướng dẫn-phản hồi hiệu quả nhất bằng cách tận dụng sự khác biệt về chất lượng giữa LLM mục tiêu và LLM mạnh hơn. Chiến lược này xác định hai tập hợp hướng dẫn chính: (a) những hướng dẫn mà LLM mục tiêu gặp khó khăn, thúc đẩy nó cải thiện trong các lĩnh vực yếu để có được lợi ích đáng kể hơn, và (b) những hướng dẫn mà LLM mục tiêu xuất sắc, đưa chúng trở lại quy trình Self-Rubrics để cải thiện hiệu quả dữ liệu. Contrastive Filtering phục vụ như một sự tương tự ở mức phản hồi của contrastive decoding (Li et al., 2022).

CodecLM thiết lập một kỷ lục tiên tiến mới trên bốn benchmark tuân theo hướng dẫn miền mở với các lựa chọn LLM khác nhau, chứng minh hiệu quả của nó trong việc căn chỉnh LLM cho các phân phối hướng dẫn đa dạng.

2 Công trình liên quan
Điều chỉnh Hướng dẫn cho Căn chỉnh LLM. Điều chỉnh LLM để tuân theo hướng dẫn một cách trung thực và căn chỉnh với các sở thích đa dạng của con người vẫn là một thách thức đáng kể (Efrat và Levy, 2020). Nghiên cứu ban đầu chủ yếu tập trung vào khái quát hóa cross-task, nơi các mô hình được fine-tune trên nhiều tập dữ liệu NLP công khai khác nhau để cải thiện hiệu suất trên các nhiệm vụ đa dạng (Raffel et al., 2020; Wei et al., 2021; Aribandi et al., 2021; Victor et al., 2022; Chung et al., 2022). Gần đây hơn, các nhà nghiên cứu đã mở rộng điều chỉnh hướng dẫn sang các miền mở, được đặc trưng bởi một phạm vi rộng hơn các định dạng và loại nhiệm vụ. Sự chuyển đổi này được thúc đẩy bởi việc crowdsourcing các cặp hướng dẫn-phản hồi do con người tạo ra (Ouyang et al., 2022; Köpf et al., 2023; Zhou et al., 2023a) và dữ liệu do LLM tạo ra (Taori et al., 2023; Chiang et al., 2023). Khác với các công trình trước đây, CodecLM trình bày một cách tiếp cận độc đáo để tùy chỉnh dữ liệu tổng hợp cho các nhiệm vụ downstream cụ thể mà không cần chú thích của con người, sử dụng khái niệm siêu dữ liệu hướng dẫn.

Tạo Dữ liệu cho Điều chỉnh Hướng dẫn. Để giải quyết chi phí cao của chú thích con người cho các cặp hướng dẫn-phản hồi chất lượng cao, một số nghiên cứu ủng hộ việc tự động hóa quy trình tạo dữ liệu (Schick và Schütze, 2021; Liu et al., 2022; Meng et al., 2023). Tận dụng khả năng học trong ngữ cảnh (Brown et al., 2020) của LLM, Wang

--- TRANG 3 ---
et al. (2022); Honovich et al. (2022) nhắc nhở LLM với các hướng dẫn gốc để tạo ra những hướng dẫn tổng hợp. Những hướng dẫn này sau đó được đưa vào các LLM mạnh hơn, ví dụ ChatGPT, để tạo ra phản hồi cho việc huấn luyện LLM mục tiêu (thường nhỏ hơn) (Taori et al., 2023). Như một công trình đại diện, WizardLM (Xu et al., 2023), thiết kế một tập hợp cố định các thao tác do con người tạo ra để tăng độ phức tạp của hướng dẫn và kiểm soát độ khó của dữ liệu được tạo ra. Zhao et al. (2023); Zhou et al. (2023a) tiếp tục xác nhận tầm quan trọng của độ phức tạp hướng dẫn đối với việc căn chỉnh LLM thông qua các nghiên cứu thực nghiệm. Khác với những công trình này dựa vào các quy tắc được định nghĩa trước mà không xem xét các nhiệm vụ downstream, CodecLM cho phép tự động tùy chỉnh hướng dẫn cho các nhiệm vụ downstream và LLM mục tiêu khác nhau. Chúng tôi cũng giới thiệu Self-Rubrics và Contrastive Filtering để xác định thêm các cặp hướng dẫn-phản hồi hiệu quả nhất.

Chưng cất. Cách khác, điều chỉnh LLM mục tiêu với các phản hồi được tạo ra từ LLM khác có thể được xem như chưng cất kiến thức (Hinton et al., 2015; Beyer et al., 2022). Tuy nhiên, trọng tâm của chúng tôi vẫn là việc tạo ra hướng dẫn, trong khi vẫn linh hoạt để dễ dàng tích hợp với các kỹ thuật chưng cất hiện có (Hsieh et al., 2023; Liang et al., 2023).

Cuối cùng, chúng tôi thảo luận về một số công trình gần đây có liên quan nhất. AttrPrompt (Yu et al., 2023) tận dụng LLM như một trình tạo dữ liệu có thuộc tính bằng cách trích xuất các thuộc tính trong hướng dẫn. Tuy nhiên, nó chỉ tập trung vào các nhiệm vụ phân loại và yêu cầu can thiệp của con người để lựa chọn thuộc tính. Ngược lại, công trình của chúng tôi tập trung vào bối cảnh rộng hơn của việc căn chỉnh LLM để tuân theo các hướng dẫn miền mở, loại bỏ nhu cầu nỗ lực của con người. MSP (Chen et al., 2023a) sử dụng các soft prompt có thể huấn luyện để kiểm soát việc tạo ra, nhưng yêu cầu truy cập gradient vào LLM. Phương pháp của chúng tôi, mặt khác, dễ dàng tương thích với các LLM hộp đen chỉ cung cấp truy cập API cho việc tạo dữ liệu chất lượng cao. SteerLM (Dong et al., 2023) phân tích các khía cạnh liên quan đến chất lượng của phản hồi, thay vì hướng dẫn, để nắm bắt sở thích của con người. Do đó, SteerLM có thể được sử dụng cùng với CodecLM như một cách tiếp cận song song để nâng cao chất lượng phản hồi.

3 Phát biểu Vấn đề
Chúng tôi nghiên cứu vấn đề tuân theo hướng dẫn miền mở (Wang et al., 2022; Taori et al., 2023; Xu et al., 2023), nơi các hướng dẫn khác nhau về định dạng đầu vào và nhiệm vụ. Cụ thể, chúng tôi xem xét hai tình huống thực tế: (1) Bắt đầu với một tập hợp cho trước gồm n hướng dẫn gốc Ds={Ii}^n_{i=1}, mỗi hướng dẫn được rút ra từ một phân phối cơ bản nào đó PI. Đối với các thử nghiệm của chúng tôi, chúng tôi tạo ra một tập hợp các hướng dẫn gốc sử dụng một tập validation được giữ lại. Trên thực tế, những hướng dẫn như vậy có thể được thu thập từ lưu lượng sử dụng của người dùng. (2) Trong trường hợp không có hướng dẫn gốc, nhưng với kiến thức trước về các nhiệm vụ downstream, chúng tôi trực tiếp bắt đầu với một tập hợp cho trước các siêu dữ liệu hướng dẫn M (xem Phần 4.1 để biết định nghĩa). Tình huống sau đặc biệt hữu ích cho người dùng cuối thiếu dữ liệu hướng dẫn hiện có nhưng muốn khởi động LLM được tùy chỉnh cho các ứng dụng cụ thể, tương tự như khái niệm GPTs (OpenAI, 2023b).

Chúng tôi tập trung vào tình huống đầu tiên để rõ ràng, mặc dù tình huống thứ hai có thể được suy ra tương tự bằng cách tận dụng LLM như bộ mã hóa (Phần 4.1). Mục tiêu của chúng tôi là tạo ra một tập hợp các cặp hướng dẫn-phản hồi chất lượng cao Dg={(I'_j, R'_j)}^m_{j=1}, sử dụng một LLM mạnh fs, và sau đó sử dụng Dg để fine-tune LLM mục tiêu ft. Chúng tôi đánh giá hiệu suất của LLM ft đã được fine-tune trên các hướng dẫn kiểm tra từ phân phối mục tiêu PI, mà chúng tôi đang căn chỉnh.

4 CodecLM
Chúng tôi đề xuất CodecLM, một khung tổng quát để tạo ra các cặp hướng dẫn-phản hồi chất lượng cao được tùy chỉnh cho các nhiệm vụ downstream và LLM khác nhau, loại bỏ nhu cầu chú thích của con người. Xem Hình 2 để có tổng quan về phương pháp.

4.1 LLM như Codec cho Hướng dẫn
Trong phần này, chúng tôi giới thiệu khái niệm sử dụng một LLM mạnh như một codec, tức là cả bộ mã hóa và bộ giải mã, để tạo ra hướng dẫn.

LLM như Bộ mã hóa với Siêu dữ liệu Hướng dẫn.
Chúng tôi bắt đầu bằng việc mã hóa các hướng dẫn gốc cho trước Ds={Ii}^n_{i=1} thành siêu dữ liệu hướng dẫn M, tức là các từ khóa nắm bắt phân phối hướng dẫn mục tiêu cơ bản. Lấy cảm hứng từ task pool của Wang et al. (2022) và phân tích hậu hoc về phân phối kỹ năng của Xu et al. (2023), chúng tôi định nghĩa siêu dữ liệu bao gồm hai khía cạnh chính: trường hợp sử dụng và kỹ năng. Trường hợp sử dụng mô tả nhiệm vụ dự định (ví dụ, trả lời câu hỏi hoặc viết sáng tạo), trong khi Kỹ năng là kiến thức mà LLM cần có để phản hồi thành công cho hướng dẫn đã cho (ví dụ, thuật toán hoặc giao tiếp). Kỹ năng thường có thể khái quát hóa cho các trường hợp sử dụng khác nhau. Do đó, mỗi hướng dẫn có một trường hợp sử dụng duy nhất và có thể liên quan đến nhiều kỹ năng. Để trích xuất siêu dữ liệu này, chúng tôi tận dụng LLM mạnh fs theo mẫu prompt trong Hình 7, Phụ lục A.9.

[Hình ảnh 2 được mô tả: Tổng quan về CodecLM được đề xuất. Đầu tiên, LLM mạnh fs mã hóa hướng dẫn gốc thành siêu dữ liệu hướng dẫn, chỉ định trường hợp sử dụng và kỹ năng cần thiết cho phản hồi. Tiếp theo, fs giải mã siêu dữ liệu thành các hướng dẫn cơ bản. Trong khi đó, Self-Rubrics tận dụng fs để tạo ra tiêu chí và hành động nhằm cải thiện hướng dẫn cơ bản, tùy chỉnh chúng cho nhiệm vụ downstream. Cuối cùng, Contrastive Filtering sử dụng hàm tính điểm S để so sánh phản hồi của fs và ft. Các cặp hiệu quả nhất được chọn để căn chỉnh LLM, trong khi các hướng dẫn kém hiệu quả được gửi để cải thiện thêm. Trong hình này, phản hồi của LLM mạnh thắng so với phản hồi của mục tiêu, vì vậy chúng tôi chọn cặp tương ứng để điều chỉnh hướng dẫn cho LLM mục tiêu.]

--- TRANG 4 ---
Mặc dù các định nghĩa phong phú hơn có thể dựa trên các chỉ số tuân theo hướng dẫn chi tiết hơn (Zhou et al., 2023b), chúng tôi ưu tiên trường hợp sử dụng và kỹ năng vì khả năng áp dụng rộng rãi của chúng trên các phân phối hướng dẫn đa dạng. Các nghiên cứu trong tương lai có thể khám phá việc mở rộng siêu dữ liệu này hơn nữa.

Đối với mỗi hướng dẫn Ii, chúng tôi trích xuất trường hợp sử dụng tương ứng ui và tập hợp kỹ năng si. Sau đó chúng tôi có tập hợp siêu dữ liệu là M={(ui,si)}^n_{i=1}. Các hướng dẫn có thể chia sẻ hoặc chồng chéo một phần trong ui và si của chúng, phản ánh phân phối của các nhiệm vụ và khả năng trong các hướng dẫn gốc. Các trường hợp sử dụng và kỹ năng được tạo ra tức thì, không giới hạn ở một số tập hợp được định nghĩa trước, cho phép khả năng áp dụng rộng hơn. Tuy nhiên, chúng ta luôn có thể cung cấp những ràng buộc như vậy với kiến thức trước của chúng ta, hoặc thậm chí trực tiếp viết ra siêu dữ liệu mà không cần bất kỳ hướng dẫn gốc nào.

LLM như Bộ giải mã để Tạo Hướng dẫn.
Với siêu dữ liệu M, chúng tôi giải mã siêu dữ liệu thành các hướng dẫn tổng hợp, theo một mô hình tạo ra và tùy chỉnh. Đối với mỗi cặp trường hợp sử dụng và kỹ năng trong M, chúng tôi liệt kê chúng như các ràng buộc để nhắc nhở LLM mạnh fs tạo ra nhiều hướng dẫn. Do đó, các hướng dẫn được tạo ra dành cho trường hợp sử dụng đã cho, và yêu cầu các kỹ năng đã cho để được phản hồi. Hơn nữa, để ngăn LLM tạo ra các hướng dẫn lặp lại, chúng tôi khuyến khích việc tạo ra của nó phải đa dạng trong prompt, và không cung cấp bất kỳ minh họa nào mà LLM có thể sao chép. Mẫu prompt ví dụ để tạo ra các hướng dẫn cơ bản có trong Hình 8, Phụ lục A.9. Tiếp tục quá trình giải mã, chúng tôi sau đó tùy chỉnh các hướng dẫn cơ bản để căn chỉnh hiệu quả hơn thông qua Self-Rubrics (Phần 4.2) và Contrastive Filtering (Phần 4.3).

4.2 Tùy chỉnh Hướng dẫn thông qua Self-Rubrics
Các hướng dẫn được điều kiện hóa bởi siêu dữ liệu đặt nền tảng cho việc căn chỉnh LLM mục tiêu với các nhiệm vụ mong muốn. Các nghiên cứu gợi ý rằng các hướng dẫn phức tạp hơn có thể cải thiện hiệu suất căn chỉnh (Xu et al., 2023; Zhao et al., 2023). Một thực hành phổ biến là liên quan đến các chuyên gia con người tạo ra hướng dẫn tổng quát để làm phức tạp hướng dẫn, chẳng hạn như thêm các bước lý luận hoặc ràng buộc. Tuy nhiên, chiến lược một-kích-cỡ-cho-tất-cả này không phù hợp với các hướng dẫn đa dạng. Việc tùy chỉnh hướng dẫn cho các nhiệm vụ khác nhau, như giải quyết bài toán tính tích phân so với viết bài báo, yêu cầu các cách tiếp cận riêng biệt.

Do đó, chúng tôi giới thiệu Self-Rubrics, tận dụng LLM mạnh để tùy chỉnh hướng dẫn bằng cách điều chỉnh độ phức tạp của chúng theo siêu dữ liệu được trích xuất. Self-Rubrics trước tiên hướng dẫn LLM tạo ra các tiêu chí cụ thể theo siêu dữ liệu để đánh giá độ phức tạp của hướng dẫn. Sau đó, được thông báo bởi những tiêu chí này, LLM tạo ra một tập hợp hành động tương ứng để nâng cao độ phức tạp của hướng dẫn. Đối với siêu dữ liệu (ui,si), tập hợp hành động được tạo ra tương ứng là ai. Các hành động được tạo ra của chúng tôi cụ thể hơn theo miền, và rõ ràng hơn so với các quy tắc chung do con người tạo ra, làm cho các hướng dẫn phức tạp được tùy chỉnh tốt hơn theo phân phối mục tiêu được nắm bắt bởi siêu dữ liệu. Ví dụ, đối với trường hợp sử dụng "phát triển kế hoạch kinh doanh" và kỹ năng "nghiên cứu thị trường và lập kế hoạch", các quy tắc chung như "thêm các bước lý luận" là mơ hồ và không phù hợp. Ngược lại, Self-Rubrics có thể tạo ra các hành động như "thêm phân tích SWOT" và "bao gồm so sánh với các đối thủ cạnh tranh trên thị trường" (xem Phụ lục A.8 để biết chi tiết đầy đủ) để làm phức tạp hướng dẫn. Mẫu prompt để tạo ra tiêu chí và hành động cho việc cải thiện hướng dẫn được thể hiện trong Hình 9, Phụ lục A.9.

Với các hành động thu được {ai}^n_{i=1}, chúng tôi có thể lặp đi lặp lại nhắc nhở fs để làm phức tạp các hướng dẫn cơ bản, theo mẫu prompt trong Hình 10. Chúng tôi ngẫu nhiên lấy mẫu một hành động ai từ nhiều hành động được tạo ra cho một cặp trường hợp sử dụng và kỹ năng. Lựa chọn thiết kế này không chỉ cho phép độ phức tạp được kiểm soát (Xu et al., 2023), mà còn ngăn ngừa sự nhầm lẫn tiềm năng giữa các hành động khác nhau cho LLM.

4.3 Lựa chọn Hướng dẫn thông qua Contrastive Filtering
Mặc dù Self-Rubrics tùy chỉnh các hướng dẫn phức tạp dựa trên siêu dữ liệu hướng dẫn, nhưng không phải tất cả hướng dẫn đều hiệu quả như nhau cho việc điều chỉnh hướng dẫn, bất kể độ phức tạp của chúng (Chen et al., 2023b; Zhou et al., 2023a). Một cách trực quan, việc cho LLM mục tiêu tiếp xúc với các hướng dẫn mà nó cảm thấy thách thức có thể xác định hiệu quả các lĩnh vực cần cải thiện. Do đó, việc lựa chọn các hướng dẫn có tác động mạnh nhất để căn chỉnh LLM mục tiêu là rất quan trọng.

Do đó chúng tôi giới thiệu Contrastive Filtering, một phương pháp để lựa chọn các hướng dẫn có thể nâng cao hiệu quả LLM mục tiêu ft. Để rõ ràng, chúng tôi định nghĩa không gian của tất cả chuỗi ngôn ngữ tự nhiên là N. Chúng tôi có LLM mạnh fs:N → N, LLM mục tiêu ft:N → N, và một hàm tính điểm S:N → R để đánh giá chất lượng phản hồi. Trên thực tế, S được thu được bằng cách tái sử dụng LLM mạnh fs với mẫu prompt (Hình 11, Phụ lục A.9) được điều chỉnh từ mẫu đánh giá cặp đôi Vicuna (Taori et al., 2023; Chiang et al., 2023). Để giảm thiểu thiên lệch vị trí tiềm năng, chúng tôi tính trung bình các điểm số thu được bằng cách trao đổi vị trí của hai phản hồi (Chiang et al., 2023). Chúng tôi quan sát thấy việc sử dụng fs để tính điểm hoạt động khá tốt trong thực tế, vì vậy chúng tôi ưu tiên tùy chọn này vì tính đơn giản. Với một hướng dẫn đầu vào I∈N, chúng tôi thu được phản hồi từ cả hai LLM là fs(I) và ft(I), tương ứng. Sau đó chúng tôi định nghĩa khoảng cách chất lượng G:N → R giữa các phản hồi này để ước tính hiệu quả của hướng dẫn: G(I) = S(fs(I)) - S(ft(I)).

Chỉ số khoảng cách chất lượng G phản ánh mức độ LLM mục tiêu hưởng lợi từ LLM mạnh cho mỗi hướng dẫn I. Như được chứng minh trong Hình 2, đây là hai trường hợp có thể: (1) |G(I)| > θ, nơi θ∈R là một ngưỡng nhất định. Điều này chỉ ra rằng: Hoặc là LLM mạnh có phản hồi tốt hơn nhiều so với LLM mục tiêu, chúng tôi thêm (I, fs(I)) vào pool cặp hướng dẫn-phản hồi chất lượng cao Dg để lấp đầy khoảng cách; Hoặc hiếm khi, LLM mục tiêu đưa ra phản hồi tốt hơn nhiều so với LLM mạnh, chúng tôi thêm (I, ft(I)) vào Dg như một điều chỉnh ngầm để giữ hành vi mong muốn của LLM mục tiêu đối với một số hướng dẫn nhất định. (2) |G(I)| ≤ θ, nơi chất lượng phản hồi từ cả hai LLM tương tự nhau, vì vậy việc học từ I không dẫn đến nhiều lợi ích. Sau đó chúng tôi gửi I đến lần lặp Self-Rubrics tiếp theo để cải thiện thêm.

Contrastive Filtering bổ sung cho Self-Rubrics để lựa chọn các cặp hướng dẫn-phản hồi hiệu quả bằng cách hiệu chỉnh khả năng tuân theo hướng dẫn của LLM mục tiêu với LLM mạnh. Tương tự như Contrastive Decoding (Li et al., 2022) ở mức phản hồi, Contrastive Filtering cũng có thể được coi là phản hồi LLM (Madaan et al., 2023) với sự tương tác của hai LLM. Mặc dù chúng tôi áp dụng LLM mạnh như hàm tính điểm để đo khoảng cách chất lượng, khung của chúng tôi có thể tương thích và có thể hưởng lợi từ những tiến bộ trong các hệ thống tính điểm và phản hồi đáng tin cậy và toàn diện hơn (Lee et al., 2023), và chúng tôi để lại điều này như một công việc tương lai đầy hứa hẹn.

5 Thử nghiệm
Chúng tôi tiến hành các thử nghiệm toàn diện để đánh giá CodecLM sử dụng các LLM khác nhau trên nhiều benchmark đại diện, tuân thủ chặt chẽ các cài đặt đánh giá được thiết lập tốt cho việc tuân theo hướng dẫn miền mở trong các công trình trước đây (Xu et al., 2023; Chen et al., 2023b). Chúng tôi cũng tiến hành một nghiên cứu điển hình trong Phụ lục A.8 để minh họa cách CodecLM tùy chỉnh một hướng dẫn từng bước.

5.1 Benchmark Đánh giá
Chúng tôi đánh giá CodecLM trên bốn benchmark tuân theo hướng dẫn miền mở được sử dụng rộng rãi với các phân phối hướng dẫn đa dạng để giảm thiểu thiên lệch đánh giá. Các benchmark kiểm tra của chúng tôi bao gồm Evol-Instruct (Xu et al., 2023), Vicuna (Chiang et al., 2023), Self-Instruct (Wang et al., 2022) và Koala (Geng et al., 2023). Để bổ sung cho việc đánh giá, chúng tôi cũng đánh giá trên hai benchmark NLP tiêu chuẩn MMLU (Hendrycks et al., 2020) và BBH (Suzgun et al., 2022) trong Phụ lục A.7. Vui lòng tham khảo Phụ lục A.1 để biết chi tiết benchmark.

5.2 Phương pháp Baseline
Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp tạo dữ liệu tiên tiến cho việc điều chỉnh hướng dẫn. Để so sánh công bằng, chúng tôi cung cấp cho tất cả các phương pháp cùng backbone LLM khi có thể. Hơn nữa, chúng tôi kiểm soát số lượng cặp hướng dẫn-phản hồi giống nhau cho tất cả các phương pháp để ablate tác động của số lượng dữ liệu. Các phương pháp baseline bao gồm Self-Instruct (Wang et al., 2022), Alpagasus (Chen et al., 2023b), Tree-Instruct, WizardLM (Xu et al., 2023), và WizardLM+, một phiên bản nâng cao của WizardLM sử dụng cùng các hướng dẫn cơ bản được tạo ra từ CodecLM như các hướng dẫn gốc. Chi tiết baseline được trình bày trong Phụ lục A.2.

5.3 Chi tiết Thử nghiệm và Đánh giá
Backbone LLM. Chúng tôi áp dụng các LLM dựa trên LLaMA (Touvron et al., 2023) và dựa trên PaLM (Anil et al., 2023) như LLM mục tiêu trong các thử nghiệm của chúng tôi. Đối với LLM mục tiêu dựa trên LLaMA, chúng tôi sử dụng Gemini-Pro (Team et al., 2023) như LLM mạnh, và LLaMA-7B, -13B như LLM mục tiêu. Đối với LLM mục tiêu dựa trên PaLM, chúng tôi sử dụng text-unicorn như LLM mạnh, và text-bison như LLM mục tiêu. Các mô hình dựa trên PaLM và Gemini-Pro có thể truy cập thông qua Google Cloud API.

Chi tiết Triển khai của CodecLM. Chúng tôi chia tất cả benchmark thành 20% tập validation và 80% tập đánh giá. Chúng tôi trích xuất siêu dữ liệu hướng dẫn từ tập validation, xem Phụ lục A.3 để biết thêm chi tiết. Tùy thuộc vào kích thước dữ liệu tổng được chỉ định, chúng tôi nhắc nhở LLM mạnh tạo ra số lượng hướng dẫn cơ bản bằng nhau cho mỗi siêu dữ liệu. Chúng tôi tạo ra 500-8000 dữ liệu tổng hợp trong suốt các thử nghiệm. Chúng tôi tạo ra 4 tiêu chí và hành động tương ứng. Tại mỗi lần lặp, chúng tôi ngẫu nhiên chọn 1 hành động để cải thiện hướng dẫn. Chúng tôi chạy Self-Rubrics tối đa 4 lần lặp. Đối với Contrastive Filtering, chúng tôi đặt thang điểm là 10 và ngưỡng lọc là 3 cho tất cả thử nghiệm. Chúng tôi căn chỉnh các cấu hình này với Xu et al. (2023) và để lại lý do chi tiết hơn về các cấu hình này, cài đặt siêu tham số bổ sung và chi tiết huấn luyện trong Phụ lục A.3-A.4.

Đánh giá. Việc đánh giá mức độ LLM tuân theo hướng dẫn là phức tạp, phát sinh từ thực tế rằng một hướng dẫn có nhiều phản hồi hợp lệ khác nhau, và thách thức trong việc sao chép đánh giá của con người. Những tiến bộ gần đây trong đánh giá tự động về việc tuân theo hướng dẫn (Dubois et al., 2023; Zheng et al., 2023) chứng minh rằng các đánh giá viên dựa trên LLM có thể mở rộng, có thể giải thích và nhất quán với các đánh giá của con người. Do đó, chúng tôi áp dụng đánh giá viên cặp đôi Vicuna được sử dụng rộng rãi (Chiang et al., 2023) dựa trên ChatGPT để so sánh chất lượng phản hồi từ hai LLM vì khả năng tiếp cận về giá cả và hiệu quả. Mẫu prompt đánh giá có trong Hình 12, Phụ lục A.9. Chúng tôi bao gồm kết quả đánh giá dựa trên GPT-4 trong Phụ lục A.6 để chứng minh tính nhất quán của các đánh giá viên dựa trên LLM. Để giảm thiểu thiên lệch vị trí mà đánh giá viên LLM có thể có, chúng tôi tiến hành mọi đánh giá hai lần bằng cách trao đổi thứ tự phản hồi. Một phản hồi được coi là tốt hơn chỉ khi nó thắng hai lần. Theo (Chen et al., 2023b), chúng tôi đặt temperature thành 0.0 để giảm tính ngẫu nhiên trong đánh giá, và để các tham số khác là mặc định. Tương tự như các công trình trước đây (Xu et al., 2023; Zhao et al., 2023), chúng tôi tính tỷ lệ tổng thắng và hòa của LLM mục tiêu so với LLM mạnh, để chỉ ra mức độ khả năng mô hình mà LLM mục tiêu phục hồi từ LLM mạnh (thường được coi là người thực hiện giới hạn trên). CRR đơn giản hóa các so sánh cặp đôi tổ hợp giữa tất cả LLM mục tiêu. Chúng tôi đặt tên cho chỉ số là Tỷ lệ Phục hồi Khả năng (CRR), nơi CRR = (thắng + hòa) / tổng số so sánh. Trong thử nghiệm, chúng tôi quan sát thấy rằng số lần hòa thường chiếm ưu thế so với số lần thắng, vì LLM mạnh có khả năng cao hơn nhiều so với mô hình mục tiêu. Vì vậy chúng tôi không đặt trọng số bổ sung cho các lần thắng trong tính toán. Để chứng minh CRR phản ánh trung thực hiệu suất mô hình, chúng tôi hiển thị số chính xác các lần thắng, hòa và thua trong Phụ lục A.5 trên Evol-Instruct. Chúng tôi muốn nhấn mạnh trọng tâm của chúng tôi vào khoảng cách trong CRR giữa các phương pháp khác nhau thay vì giá trị tuyệt đối, vì giá trị tuyệt đối có thể dựa trên đánh giá viên LLM cụ thể mà chúng tôi chọn.

5.4 Kết quả Tuân theo Hướng dẫn Miền Mở
Kết quả với LLM Mục tiêu dựa trên LLaMA. Bảng 1 tóm tắt hiệu suất của CodecLM và các baseline so sánh với 2000 dữ liệu tổng hợp cho việc điều chỉnh hướng dẫn. Tất cả các phương pháp được huấn luyện trên LLaMA-7B hoặc -13B như LLM mục tiêu và so sánh với Gemini-Pro, LLM mạnh tạo ra dữ liệu. CodecLM vượt trội so với các phương pháp so sánh một cách nhất quán trên tất cả benchmark, với hai LLM mục tiêu có kích thước khác nhau. Hiệu suất vượt trội nhất quán của CodecLM nhấn mạnh khả năng tổng quát hóa của nó đối với các phân phối hướng dẫn downstream khác nhau và LLM mục tiêu. Cả Tree-Instruct và các biến thể của WizardLM đều tập trung vào tầm quan trọng của độ phức tạp hướng dẫn, tuy nhiên, hiệu suất của chúng không phải lúc nào cũng tốt hơn Alpagasus với các hướng dẫn đơn giản, đặc biệt với LLM mục tiêu lớn hơn. Quan sát này chỉ ra rằng hiệu quả của dữ liệu không thể được xác định chỉ bởi độ phức tạp hướng dẫn, và xác nhận động lực thiết kế Self-Rubrics và Contrastive Filtering của chúng tôi. Hơn nữa, chiến thắng của WizardLM+ so với WizardLM xác nhận hiệu quả của việc khớp phân phối hướng dẫn thông qua siêu dữ liệu hướng dẫn. Khi chuyển LLM mục tiêu từ LLaMA-7B sang -13B, tất cả các phương pháp đều có được sự gia tăng hiệu suất đáng kể, phù hợp với các khám phá trước đây về việc mở rộng kích thước mô hình (Wei et al., 2021).

--- TRANG 5 ---
Kết quả với Mô hình dựa trên PaLM. Bảng 2 tóm tắt kết quả của CodecLM và các baseline có hiệu suất tốt nhất trong các thử nghiệm dựa trên LLaMA. Chúng tôi tạo ra 1000 dữ liệu tổng hợp do ngân sách tính toán. Vì text-bison là một mô hình độc quyền đã được căn chỉnh với nhiều kỹ thuật khác nhau bao gồm điều chỉnh hướng dẫn, chúng tôi cũng bao gồm nó như một phương pháp baseline. Thật thú vị, text-bison đạt được hiệu suất mạnh trên các benchmark khác nhau. Cả Alpagasus và WizardLM+ đều kém hiệu suất so với text-bison, gợi ý rằng việc cải thiện liên tục một LLM được điều chỉnh tốt là không tầm thường. Ngược lại, CodecLM vượt trội so với text-bison trong hầu hết các trường hợp, nhờ vào các thiết kế cốt lõi của chúng tôi mà tùy chỉnh thích ứng các cặp dữ liệu chất lượng cao để cải thiện LLM mục tiêu.

5.5 Nghiên cứu Ablation
Trong phần này, chúng tôi tiến hành các nghiên cứu ablation toàn diện để khám phá thực nghiệm hiệu quả của CodecLM. Chúng tôi chủ yếu tiến hành các thử nghiệm với mô hình LLaMA-7B như LLM mục tiêu, Gemini-Pro như LLM mạnh, và báo cáo CRR trên benchmark Evol-Instruct.

Hiệu quả của Các Thiết kế Cốt lõi. Chúng tôi hiển thị những đóng góp theo từng thành phần trong khung của chúng tôi trong Bảng 3. Hàng 1 có kết quả từ Self-Instruct như baseline; Trong hàng 2, chúng tôi chỉ căn chỉnh LLM với các hướng dẫn cơ bản từ siêu dữ liệu hướng dẫn; Chúng tôi dần dần thêm Self-Rubrics và Contrastive Filtering trong hàng 3 và 4, tương ứng. Chúng tôi quan sát rõ ràng rằng mọi thành phần đều đóng góp vào hiệu suất cuối cùng. Thú vị, hiệu suất của việc sử dụng các hướng dẫn cơ bản từ siêu dữ liệu thậm chí còn ngang bằng với WizardLM+ trong Bảng 1. Quan sát này chỉ ra rằng các chiến lược do con người tạo ra để làm phức tạp hướng dẫn có thể không phù hợp với các loại hướng dẫn khác nhau. Ngược lại, Self-Rubrics tạo ra thích ứng các hành động cải thiện hướng dẫn dựa trên siêu dữ liệu khác nhau, dẫn đến các hướng dẫn được tùy chỉnh tốt hơn cho LLM mục tiêu. Những cải thiện tiếp theo từ Contrastive Filtering chứng minh rằng dữ liệu được chọn thực sự hiệu quả hơn cho việc căn chỉnh.

Tác động của Số lần Lặp. Chúng tôi chứng minh tác động của số lần lặp CodecLM trong Hình 3. Cụ thể, chúng tôi đếm tỷ lệ dữ liệu từ mỗi lần lặp trong tất cả dữ liệu tổng hợp Dg và hiển thị nó trong biểu đồ cột màu xanh với trục y bên trái. Chúng tôi cũng vẽ hiệu suất mô hình mục tiêu trong CRR sau khi huấn luyện trên dữ liệu tổng hợp cho đến lần lặp hiện tại trong biểu đồ đường màu vàng với trục y bên phải. Từ biểu đồ cột tỷ lệ dữ liệu, chúng tôi quan sát thấy hơn 70% dữ liệu đến từ lần lặp đầu tiên. Điều này chỉ ra Contrastive Filtering thành công thu thập các hướng dẫn ít phức tạp nhưng thách thức, là quan trọng để xây dựng khả năng tuân theo hướng dẫn của LLM mục tiêu. Bắt đầu từ lần lặp thứ hai, tỷ lệ dữ liệu ngày càng nhỏ. Tuy nhiên, tương tự như quan sát ít hơn là nhiều hơn cho việc căn chỉnh (Zhou et al., 2023a), các hướng dẫn chất lượng cao và phức tạp hơn thực sự đóng góp vào hiệu suất cuối cùng mặc dù ít về số lượng.

Khám phá về Khớp Phân phối. Như được hiển thị bởi các kết quả trước đây, việc tạo ra siêu dữ liệu được trích xuất từ phân phối hướng dẫn downstream thực sự giúp ích. Tuy nhiên, trong thực tế, siêu dữ liệu được trích xuất hoặc viết bởi con người có thể không thể đặc trưng chính xác phân phối hướng dẫn. Do đó, cần thiết phải khám phá hiệu suất của CodecLM khi phân phối được đại diện bởi siêu dữ liệu hướng dẫn không khớp hoàn toàn với phân phối kiểm tra. Vì phân phối kiểm tra thực sự phức tạp và không được biết trước, chúng tôi xấp xỉ các mức độ khớp phân phối khác nhau bằng cách lấy mẫu ngẫu nhiên từ tập siêu dữ liệu M. Để kiểm soát tác động của số lượng dữ liệu, chúng tôi giữ tổng số cặp hướng dẫn-phản hồi giống nhau cho mỗi trường hợp. Ví dụ, khi lấy mẫu phụ 20% của M, chúng tôi nhắc nhở LLM mạnh tạo ra gấp 5 lần hướng dẫn cho mỗi siêu dữ liệu tương ứng. Kết quả được hiển thị trong phần trên của Hình 4, và chúng tôi thực sự quan sát xu hướng rằng siêu dữ liệu hướng dẫn nắm bắt phân phối cơ bản càng tốt thì hiệu suất LLM mục tiêu có thể đạt được càng tốt. Hơn nữa, khi tỷ lệ khớp siêu dữ liệu bằng hoặc lớn hơn 60%, chúng tôi đạt được hiệu suất gần như kết quả khớp hoàn toàn. Quan sát này nhấn mạnh tính mạnh mẽ của CodecLM dưới sự không khớp siêu dữ liệu hướng dẫn tiềm năng.

Mở rộng với Kích thước Mô hình và Số lượng Dữ liệu. Để khám phá cách phương pháp của chúng tôi mở rộng với các số lượng dữ liệu tổng hợp khác nhau và kích thước mô hình, chúng tôi tiến hành các thử nghiệm bằng cách so sánh CodecLM với WizardLM+, baseline cạnh tranh nhất. Kết quả thử nghiệm trên Evol-Instruct với LLaMA-7B và -13B như LLM mục tiêu được trình bày trong Hình 5. Cả hai phương pháp đều đạt được hiệu suất ngày càng tốt hơn với nhiều dữ liệu tổng hợp hơn và mô hình mục tiêu lớn hơn. CodecLM liên tục vượt trội so với WizardLM+ trong tất cả các trường hợp, chứng minh hiệu quả dữ liệu và khả năng mở rộng tuyệt vời của nó. Chúng tôi mong đợi lợi ích sẽ dần giảm sau khi chúng tôi tạo ra hơn 8k dữ liệu tổng hợp, do khoảng cách khả năng nội tại giữa các mô hình mục tiêu và LLM mạnh.

6 Kết luận
Trong công trình này, chúng tôi đề xuất CodecLM để tùy chỉnh dữ liệu tổng hợp cho việc căn chỉnh LLM với các phân phối hướng dẫn mục tiêu khác nhau và LLM. Chúng tôi cho thấy rằng CodecLM hiệu quả nắm bắt phân phối hướng dẫn cơ bản thông qua siêu dữ liệu hướng dẫn, và tiếp tục tùy chỉnh các cặp hướng dẫn-phản hồi hiệu quả nhất thông qua Self-Rubrics và Contrastive Filtering. CodecLM cung cấp một giải pháp mạnh mẽ hướng tới việc thích ứng LLM cho các sử dụng tùy chỉnh, mà không cần thiết chú thích của con người. Chúng tôi tin rằng CodecLM phục vụ như một khung tổng quát cho việc căn chỉnh LLM có mục tiêu, mở ra cánh cửa cho nhiều hướng nghiên cứu đầy hứa hẹn trong khung, chẳng hạn như định nghĩa siêu dữ liệu phong phú hơn, thiết kế prompt tốt hơn, và hệ thống tính điểm dựa trên LLM đáng tin cậy hơn. CodecLM cũng có thể hưởng lợi từ các lĩnh vực nghiên cứu trực giao, và chúng tôi tiếp tục thảo luận trong các phần Cân nhắc Đạo đức và Hạn chế.

--- TRANG 6 ---
Cân nhắc Đạo đức
Mặc dù CodecLM phục vụ như một khung tổng hợp dữ liệu hiệu quả cho việc căn chỉnh LLM, chúng ta cũng nên suy ngẫm về tác động đạo đức của công trình của chúng tôi. Phương pháp của chúng tôi tận dụng LLM để tạo ra các cặp hướng dẫn-phản hồi. Tương tự như các chú thích viên con người có thể mắc sai lầm vô thức trong quá trình chú thích dữ liệu, LLM đôi khi cũng tạo ra các hướng dẫn và phản hồi không đạo đức, độc hại hoặc gây hiểu lầm (Bender et al., 2021). Hơn nữa, khi chúng tôi huấn luyện LLM mục tiêu sử dụng dữ liệu được tạo ra, LLM được điều chỉnh hướng dẫn kết quả cũng có thể mang theo các vấn đề thiên lệ và công bằng (Gallegos et al., 2023) từ mô hình gốc. Mặc dù chúng tôi đã tiến hành kiểm tra thủ công như được chỉ định trong Phụ lục A.3, trong thực tế, chúng ta nên áp dụng các kỹ thuật hiện có (Hanu và Unitary team, 2020; Thakur et al., 2023) để giải độc và giảm thiểu thiên lệ từ LLM được sử dụng trong CodecLM, và thiết kế các quy tắc kiểm tra và lọc nghiêm ngặt hơn để dọn dẹp dữ liệu được tạo ra. Do tính linh hoạt của khung của chúng tôi, chúng tôi dự đoán tiến bộ trong tương lai trong lĩnh vực giảm thiểu thiên lệ và các vấn đề công bằng có thể bổ sung cho CodecLM.

Hạn chế
Chúng tôi thừa nhận các hạn chế của CodecLM từ các khía cạnh sau để truyền cảm hứng cho các cơ hội nghiên cứu trong tương lai trong lĩnh vực căn chỉnh LLM.

Trước hết, như đã thảo luận trong Cân nhắc Đạo đức, phương pháp của chúng tôi yêu cầu một LLM mạnh để tạo ra dữ liệu, vì vậy hiệu suất của phương pháp chúng tôi phụ thuộc vào chất lượng của LLM và có thể kế thừa các vấn đề thiên lệ và công bằng từ nó. Mặt khác, CodecLM có thể hưởng lợi từ các LLM mạnh hơn được cải thiện với các phương pháp nâng cao giảm thiên lệ và tăng cường công bằng.

Thứ hai, như một hướng trực giao, phương pháp của chúng tôi không khám phá tính mạnh mẽ của mô hình được điều chỉnh hướng dẫn đối với các cuộc tấn công đối kháng như tiêm prompt (Liu et al., 2023) và jailbreaking (Zou et al., 2023). Trong thực tế, chúng ta nên áp dụng các kỹ thuật phòng thủ đối kháng (Jain et al., 2023) tương ứng cho LLM được điều chỉnh hướng dẫn từ phương pháp của chúng tôi.

Hơn nữa, chúng tôi chủ yếu sử dụng các phương pháp đánh giá tự động dựa trên LLM theo các công trình gần đây trong tổng hợp dữ liệu cho việc căn chỉnh. Mặc dù các nghiên cứu gần đây (Chiang et al., 2023; Dubois et al., 2023) chứng minh đánh giá dựa trên LLM phần lớn nhất quán với đánh giá của con người, khả năng mở rộng và độ tin cậy của các đánh giá viên dựa trên LLM vẫn có chỗ để cải thiện. Mặc dù chúng tôi bao gồm một số kết quả benchmark tiêu chuẩn trong Phụ lục A.7 để bổ sung cho kết quả đánh giá dựa trên LLM, chúng tôi vẫn tin rằng tiến bộ trong việc đánh giá LLM tốt hơn có thể dẫn đến một minh chứng đáng tin cậy hơn về hiệu quả của phương pháp chúng tôi.

Cuối cùng, như được hiển thị trong Phần 5.5, mặc dù CodecLM mạnh mẽ đối với sự không khớp phân phối vừa phải, hiệu suất của nó vẫn phụ thuộc vào mức độ siêu dữ liệu nắm bắt phân phối hướng dẫn cơ bản. Trong thực tế, hướng dẫn gốc mà chúng tôi thu thập có thể khác với các hướng dẫn kiểm tra thực tế. Hoặc trong trường hợp chúng tôi trực tiếp tạo siêu dữ liệu từ đặc tả của người dùng, người dùng có thể thay đổi ý định tại thời điểm kiểm tra để gửi các hướng dẫn ngoài phân phối cho mô hình vượt ra ngoài siêu dữ liệu ban đầu. Kết quả là, CodecLM có thể gặp suy giảm hiệu suất dưới sự không khớp phân phối. Như một biện pháp khắc phục, chúng ta có thể liên tục thu thập lưu lượng hướng dẫn của người dùng hoặc phản hồi của người dùng để cập nhật dữ liệu được tạo ra từ CodecLM, và liên tục cập nhật LLM mục tiêu.

Chúng tôi hy vọng các công trình trong tương lai có thể tận dụng CodecLM như một khung tổng hợp dữ liệu linh hoạt cho việc căn chỉnh LLM, để những tiến bộ trong lĩnh vực có thể được tích hợp vào CodecLM để giảm các hạn chế hiện tại của nó.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài với nhiều nghiên cứu về LLM, instruction tuning, và các phương pháp liên quan - bao gồm các tác giả như Rohan Anil, Andrew M Dai, Tom Brown, và nhiều người khác từ năm 2020-2023]

--- TRANG 7-16 ---
[Các bảng kết quả thử nghiệm, biểu đồ phân tích, và các phụ lục chi tiết về implementation, prompt templates, và các nghiên cứu case study của CodecLM]
