# 2403.12468.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/dataset-generation/2403.12468.pdf
# File size: 988410 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CrossTune: Black-Box Few-Shot Classification with Label
Enhancement
Danqing Luo1⋆, Chen Zhang1⋆, Yan Zhang1, Haizhou Li1,2†
1National University of Singapore
2The Chinese University of Hong Kong (Shenzhen), China
chen_zhang@u.nus.edu, {danqing, eleyanz, haizhou.li}@nus.edu.sg
Abstract
Training or finetuning large-scale language models (LLMs) requires substantial computation resources, motivating
recent efforts to explore parameter-efficient adaptation to downstream tasks. One approach is to treat these models
as black boxes and use forward passes (Inference APIs) to interact with them. Current research focuses on adapting
these black-box models to downstream tasks using gradient-free prompt optimization, but this often involves an
expensive process of searching task-specific prompts. Therefore, we are motivated to study black-box language
model adaptation without prompt search. Specifically, we introduce a label-enhanced cross-attention network
called CrossTune, which models the semantic relatedness between the input text sequence and task-specific label
descriptions. Its effectiveness is examined in the context of few-shot text classification. To improve the generalization
of CrossTune, we utilize ChatGPT to generate additional training data through in-context learning. A switch
mechanism is implemented to exclude low-quality ChatGPT-generated data. Through extensive experiments on
seven benchmark text classification datasets, we demonstrate that our proposed approach outperforms the previous
state-of-the-art gradient-free black-box tuning method by 5.7% on average. Even without using ChatGPT-augmented
data, CrossTune performs better or comparably than previous black-box tuning methods, suggesting the effectiveness
of our approach.
Keywords: Black-Box Tuning, Few-shot Text Classification, Large Language Model
1. Introduction
In the past few years, significant progress has
been made in research on large-scale language
models (LLMs) (Devlin et al., 2019; Liu et al.,
2019; Ouyang and et al., 2022; Chowdhery et al.,
2022). Scaling up language models has been
demonstrated to boost performance and sam-
ple efficiency on a great variety of downstream
tasks (Raffel et al., 2020; Brown et al., 2020b, inter
alia). However, training such LLMs is not practi-
cal with typical research hardware. Even finetun-
ing them on task-specific data is extremely chal-
lenging. Many research efforts have been de-
voted to more parameter-efficient adaptation ap-
proaches, including (1) parameter-efficient fine-
tuning (PEFT) (Lester et al., 2021; Li and Liang,
2021; Houlsby et al., 2019; Hu et al., 2022), which
optimizes a small portion of task-specific parame-
ters, while keeping the language model intact; (2)
prompt-basedlearning, where acarefully-designed
task-specific sequence, known as a prompt, is
added to the input text sequence of a pre-trained
language model (LM). The LM is repurposed to
adapt to the downstream tasks without additional
training.
Due to commercial reasons, powerful LLMs are
provided as a service in the cloud, and end users
can only interact with them through inference APIs.
⋆Equal contribution.
†Corresponding author.This setup is referred to as Language-Model-as-a-
Service(LMaaS)Sunetal.(2022b). PopularPEFT
approaches are impractical in this context since
theyrequireaccesstomodelgradients. Toaddress
this challenge, an emerging line of prompt-based
learning research focuses on gradient-free prompt
optimization techniques (Brown et al., 2020b; Sun
et al., 2022b,a; Deng et al., 2022; Prasad et al.,
2023; Hou et al., 2023). However, these meth-
ods are also problematic because (1) prompt opti-
mization is highly sensitive to the template design
and demonstration selection (Gao et al., 2021a;
Zhao et al., 2021) leading to unstable performance
and poor generalization. (2) The prompt search
process, either manual or automatic, is also time-
consuming. For example, the covariance matrix
adaptation evolution strategy (CMA-ES) adopted
by Sun et al. (2022b) requires tens of thousands
of forward passes through the LLMs to achieve
satisfactory performance even in few-shot text clas-
sification scenarios.
To this end, we propose CrossTune, a label-
enhanced black-box few-shot learner for the adap-
tation of the black-box LMs without prompt search.
Following existing works, we assume the inference
APIs provide forward-pass LM outputs and study
our approach in the context of few-shot text clas-
sification. In CrossTune, the black-box model is
treated as a feature extractor where hidden states
of the input text sequence are derived. Besides,
the original label words are expanded to long textarXiv:2403.12468v1  [cs.CL]  19 Mar 2024

--- PAGE 2 ---
descriptions. A cross-attention network is trained
to align the input text sequence with its associated
label. In this way, we can steer the model to fo-
cus on specific aspects of the input text data that
are semantically related to the label descriptions,
which act as a form of contextual input and provide
additional semantic guidance to the model about
what each label means.
In the few-shot scenarios, the model can easily
overfit the training data resulting in poor general-
ization to unseen test data. Existing works mainly
rely on semi-supervised and weakly-supervised
methodstoboostthegeneralizationofthetextclas-
sifiers. Both assume the presence of abundant
in-distribution unlabeled data (Schick and Schütze,
2021; Chen et al., 2021; Fei et al., 2022; Du et al.,
2021; Cho et al., 2023)1. Contrary to prior works,
we do not make such an assumption. Instead, we
harness the strong instruction-following capability
of ChatGPT2to generate data conditioned on the
labels through in-context learning (Brown et al.,
2020b). To filter out low-quality generation, we
implement an additional switch mechanism as de-
scribed in §3.4.
In summary, our contributions are as follows:
•We introduce CrossTune, a new approach
for the few-shot adaptation of black-box lan-
guage models. Different from existing meth-
ods, CrossTune does not rely on the expen-
sive prompt search process. Additionally,
CrossTune leverages the rich semantic infor-
mationinlabeldescriptionstoperformtheclas-
sification task.
•Instead of relying on in-distribution unlabeled
training data, which are rarely available in
real-life scenarios, we harness the power of
a strong instruction-following text generator,
ChatGPT, to generate data conditioned on the
labels through in-context learning. A pipeline
is designed to generate and clean the data.
Our experiments demonstrate that the quality
of data generated by ChatGPT is on par with
the original training data.
•Extensiveexperimentsareperformedon7few-
shottextclassificationdatasetsandCrossTune
significantly outperforms previous the state-of-
the-art gradient-free prompt optimization ap-
proach with an absolute improvement of 5.7%
on average.
1The unlabeled data are either the original training
set with their ground-truth labels removed or retrieved
sentencesfromasentencebankbasedontheirsimilarity
to the few-shot training examples.
2https://openai.com/chatgpt2. Related Work
Gradient-Free Black-Box Tuning The success
of prompt-based learning with GPT-3 (Brown et al.,
2020b) has inspired fruitful research in NLP com-
munity. A typical line is to optimize the prompts
for downstream tasks based on the gradients of
pretrained language models such that the output
canaligncloselywiththedesiredresults(Gaoetal.,
2021a; Chen et al., 2021; Li and Liang, 2021; Liu
et al., 2021). However, many practical applications
involve models where internal parameters or gradi-
ents are obscured or inaccessible, leading to a so-
called "black-box" tuning setting (Sun et al., 2022b;
Diao et al., 2023).
Several studies have ventured into black-box
tuning challenges. BBT (Sun et al., 2022b) and
BBTv2 (Sun et al., 2022a) utilize the CMA evo-
lution strategy to optimize prompts but face chal-
lenges in efficiency and flexibility. RLPrompt (Deng
et al., 2022) and Black-box Discrete Prompt Learn-
ing (BDPL) (Diao et al., 2023) both use reinforce-
ment learning to fine-tune discrete prompts, with
BDPL featuring a streamlined search approach.
TEMPERA (Zhang et al., 2023) expands optimiza-
tion components, while GrIPS (Prasad et al., 2023)
focuses on phrase-level editing. However, many
of these black-box tuning methods suffer from effi-
ciencyissuesandmaynotalwaysdeliveroptimalre-
sults. Recently, PromptBoosting (Hou et al., 2023)
adapts the ensembling idea of AdaBoost to black-
box tuning and achieves state-of-art performance
in multiple black-box few-shot classification tasks.
Different from the existing approaches, CrossTune
does not require the expensive prompt search and
offers a much simpler and more effective adaption
of the black-box language models.
Few-shot Text Classification with Augmented
DataPopular research directions for enhancing
the generalization of few-shot text classifier in-
clude semi-supervised learning (Xie et al., 2020;
Sohn et al., 2020; Zoph et al., 2020) and weakly-
supervisedlearning(Mengetal.,2020;Zhangetal.,
2021; Fei et al., 2022; Cho et al., 2023). Both
line of works assume the presence of a substantial
amount of unlabeled data. Common techniques to
obtain unlabeled text data include (1) removing the
goldlabelsoftheoriginalfulltrainingdataforaspe-
cific task (Chen et al., 2021; Schick and Schütze,
2021), (2) applying a retriever to retrieve sentences
from a large-scale sentence bank that are semanti-
cally similar to the few-shot training data (Du et al.,
2021), and (3) text data augmentation, such as
paraphrasing and back-translation (Bayer et al.,
2022). However, these techniques have several
limitations: Using the full training data as an un-
labeled source is often impractical because sub-

--- PAGE 3 ---
   0: description
   1: entity
   2: abbreviation
   3: human
   4: number
   5: location
(a) Template For Question T ype Classification
(b) T emplate For Natural Language Inference 0: Entailment
 1: Neutral
 2: Contradiction
<s> A female spins in a white dress.  ?               , 
A female is wearing a dress.  </s>[MASK]<s>              question: Who are the presidents of
 Mexico and Indonesia?  </s>[MASK]Figure1: Inputtemplateexamples. Theblueboxes
contain the labels for the corresponding classifica-
tion tasks.
stantial in-distribution unlabeled data is not always
available in real-life scenarios. Moreover, retrieval
and text augmentation tend to produce similar un-
labeled data to the few-shot training set, limiting
the diversity of the augmented data. Furthermore,
both semi- and weakly-supervised learning rely on
potentially inaccurate pseudo-labeling of the unla-
beled data.
Motivated by the recent imitation learning re-
search on distilling high-quality training data from
strongLLMs,likeChatGPTandGPT-4(Wangetal.,
2023; Xu et al., 2023; Mukherjee et al., 2023), we
tackle the above limitations by prompting Chat-
GPT to generate high-quality training data through
in-context learning. With its strong instruction-
followingandtext-generationcapabilities,ChatGPT
serves as a powerful tool for text data augmenta-
tion.
3. Methodology
3.1. Problem Formulation
In a few-shot text classification task Twith a label
space Y, we assume there are Klabeled training
examples per class in the training set, DT
train. The
training data size, |DT
train|=K× |Y|. We also as-
sume an development set, DT
dev, which is of equal
data size as DT
train. Both DT
trainandDT
devconsist
of data instances (Xi, yi)where yi∈ YandXi
denotes the input text sequence, which contains n
tokens,i.e., Xi={x1
i, x2
i, . . . , xn
i}. Assumethatwe
have task-specific template mapping function FT,
which maps Xito a specific input format FT(Xi).
Figure 1 shows two examples of FT(Xi). The un-
derlined texts in the boxes are the original input
texts, Xi. Note that no additional prompt token is
prepended to the transformed input. Moreover, as-
sume a black-box language model denoted as M,
whichisforinferenceonly. ThroughitsAPI,wecan
obtain the logits of “[MASK]" tokens and the hidden
states of the input text sequences. Our goal is to
developamodelthatgeneralizeswelltoanunseentest set DT
test.
3.2. CrossTune Architecture
Figure 2 presents the model architecture of
CrossTune. Using the frozen black-box language
model M,wederiveasequenceofhiddenstatesaf-
tereachlayer lwithrespecttothereformattedinput
textFT(Xi). Asweareinterestedinthehiddenvec-
tors of the “[MASK]" token that is {hmask
i,l∈Rd}L
l=1,
weperformmaxpoolingon {hmask
i,l}L
l=L−3toderive
a single vector representation, hmask
i∈Rd. This
operation is motivated by previous works on sen-
tence representation learning (Ethayarajh, 2019; Li
et al., 2020; Hosseini et al., 2023) which state that
combining embeddings from multiple layers leads
to better semantic representation than using only
the last-layer embedding.
Furthermore, each label in the space Yis con-
verted into its corresponding label text description,
which is either the definitions specified in the orig-
inal datasets or from Wikipedia. Using the same
model M, we obtain the single-vector label embed-
dings hyiforeach yiinY. The hyiembeddingsare
obtained by applying the same max pooling proce-
dure described above on the hidden states of the
label description and then followed by a token-level
mean pooling operation.
A multi-head cross-attention module is imple-
mented such that hmask
Xican attend to each hyi
inY. More specifically, hmask
Xiand all label embed-
dings, HY, are first linearly transformed into query
vector and key matrix for each head:
qk=Wk
Qhmask
Xi
Kk=Wk
KHY
where kdenotes the k-th head. Wk
Q∈Rd×d, Wk
K∈
Rd×darethek-thheadweightmatricesforthequery
and key respectively. The cross attention is then
computed as:
CrossAttnk(qk,Kk) =softmaxqk(Kk)T
√
d
wheredisthedimensionalityoftheweightmatrices.
To obtain the final attention scores, we average
the scores from each head. These resulting atten-
tion scores indicate the significance of each label
description in relation to the input text sequence.
Finally, cross entropy loss is chosen as the training
objective:
L=X
(Xi,yi)∈DT
train−yilogˆyi
where yiis converted to one-hot vector while ˆyi
is the final attention score vector, i.e., probability
distribution across the labels in the label space Y.

--- PAGE 4 ---
Black -boxdefinition of something, description of something..[label descriptions]
[input][embeddings]
[attention score]
P1,  P2, P3 , P4 , P5, P6<description> 
<entity> 
<abbr > 
<number> 
<human> 
<location> 
lossmulti -head attentionanimal, organ of body, color , Invention, book …
a shortened form of a word or phrase that is used ..
number of something, Date, Distance, Price, …
individual, title of a person, description of person …
City, Country, Mountain, State, Other location …
[MASK] question: Why do leaves 
change color  in autumn?[MASK] Figure 2: System Overview of CrossTune.
### Instruction:
{Label} is defined as {Label Definition}.
Follow the below examples and generate 10 di-
verse questions of {Label} type and output one
question at a line.
### Examples:
{Here are the in-context exemplars}
### Your Output:
{Here are the ChatGPT-generated texts}
(a) Question Type Classification### Instruction:
{Label} is defined as {Label Definition}.
Generate 10 diverse {Label} sentence pairs in
the following format: [Premise |Hypothesis ]and
output one pair at a line.
### Examples:
{Here are the in-context exemplars}
### Your Output:
{ChatGPT Generated Text}
(b) Natural Language Inference
Table 1: Example instruction templates for prompting ChatGPT to generate task-specific data conditioned
on a specific label. In “Label Definition", we provide the meaning of the label. For instance, “Entailment
is defined as when the hypothesis can be logically inferred or implied from the premise" in the case of
natural language inference.
3.3. ChatGPT for Data Generation
We propose to generate task-specific data with
ChatGPT (gpt-3.5-turbo). Task-specific instruction
templates are designed to prompt ChatGPT to gen-
eraterelevanttextdatabelongingtoaspecificclass.
ChatGPT offers richer data variations, and through
in-contextlearning,itcanbepromptedfortask-and
context-specifictextgeneration, ensuringmorepre-
cise and natural outputs. Table 1 presents two
examples of how we prompt ChatGPT to generate
trainingdata. Thein-contextexemplarsarethetask-
andseed-specificfew-shottrainingdataassociated
with a particular class for which we aim to perform
dataaugmentationwithChatGPT.Forselectingthe
in-context exemplars, we follow the most common
setup, which is random sampling, i.e., to generate
samples of a particular class, we random sample
8 training samples of that class. When calling the
inference API of ChatGPT, we set the temperature,
top_p, frequency_penalty, and presence_penalty
to 0.8, 0.95, 0.8, and 1.4 respectively. For each
class, we iteratively call the inference API until a
sufficient amount of training data is obtained.3.4. The Switch Mechanism
Even though ChatGPT is a strong instruction-
following text generator, it does not always guar-
antee the production of high-quality labeled data.
Therefore,weutilizethetext-understandingcapabil-
ity of another teacher model. We select DeBERTa-
base as the teacher model due to its manageable
size (suitable for a standard research GPU) and
its superior performance on popular text classifica-
tion benchmarks (Wang et al., 2019) compared
to similar sized models, such as BERT (Devlin
etal.,2019),RoBERTa(Liuetal.,2019),andELEC-
TRA (Clark et al., 2020). A switch mechanism is
introduced such that both ChatGPT and DeBERTa
teachers can complement each other and collabo-
rativelydeterminethelabelsoftheaugmenteddata.
LetAchagptandAdebertadenote the ChatGPT and
DeBERTa teachers respectively. Motivated by the
findingsinpreviousworks(Gaoetal.,2021a;Chen
et al., 2021) that prompt-based finetuning of the
language model with demonstrations can drasti-
cally outperform standard fine-tuning procedures
in the low resource setting, we apply prompt-based
finetuning for adapting Adebertato task T. The pa-

--- PAGE 5 ---
rametersizeof Adebertaissignificantlysmallerthan
black-box LM, thus it can be viewed as a readily ac-
cessible auxiliary model designed to enhance the
quality of the augmented data. Our experimental
results reveal that incorporating a switch mecha-
nism with Adebertaenhances the performance of
CrossTune.
    <s> a comedy that swings and jostles to the rhythms of life. It was                . </s> 
    The worst film a man has made. It was terrible . </s> 
     A poem to the enduring strengths of women. It was great  . </s>[MASK]MLM Head of    great  ( label:positive )
   terrible  (label:negative)
Verbalizer
Figure 3: Prompt-based finetuning of Adeberta.
The underlined text is the prompt template. In
the bottom box, the first, second, and third lines
are the input text sequence, the demonstration
for label:negative, and the demonstration for la-
bel:positive respectively. The verbalizer maps the
labels to the corresponding words.
Prompt-basedFinetuningof AdebertaFigure3il-
lustrateshowwefinetune Adeberta. Given (Xi, yi)∈
DT
train, the Xiis first transformed into FT(Xi)ac-
cording to the task-specific templates3. The verbal-
izer converts yito the corresponding word in the
vocabulary of Adeberta. To fill in the “[MASK]" posi-
tion in FT(Xi),Adebertalearns to assign a higher
probability to the word mapped to yithan other
label words. For example, Adebertashould pre-
dict a higher probability of “great" than “terrible"
for the example input in Figure 3. To further en-
hance the prompt-based finetuning process, we
append demonstrations after FT(Xi). A demon-
strationisaninputtextexample. Foreachcategory,
one demonstration is added. Adebertais finetuned
with the standard MLM loss on DT
train. In addition,
for model selection, we perform the grid search
procedure on different training hyperparameters.
The checkpoint with the best performance on DT
dev
is used as the teacher model.
Switch Rule The data generated by Achagptis
equipped with pseudo labels that it deems correct.
To validate these labels, we implement a rule to
decide if Adebertashould annotate the data. The
decisionisbasedontheclassificationperformance
of both AchagptandAdebertaonDT
dev. IfAchagpt
outperforms Adeberta, we retain the pseudo labels.
Otherwise, we employ Adebertafor further annota-
tions, discarding any data on which Adebertaand
3In our experiments, we use the same set of task-
specific manual templates for both prompt-based finetun-
ing of Adebertaand the training of CrossTune.Achagptdisagreeandkeepingthose Adebertaiscon-
fident about.
4. Experiment Setup
Datasets CrossTuneisevaluatedon7textclassi-
ficationdatasets,including3single-sentenceand4
sentence-pairclassificationdatasets. Amongthem,
AGNews (Zhang et al., 2015) is for topic classifi-
cation. SST-2 (Wang et al., 2019) is for sentiment
analysis. TREC (Hovy et al., 2001) is for ques-
tion classification. MRPC (Wang et al., 2019) and
QQP (Wang et al., 2019) are paraphrasing tasks.
QNLI(Wangetal.,2019)andMNLI(Bowmanetal.,
2015) are for natural language inference. Follow-
ing Sun et al. (2022a), K samples are randomly
drawn from the original training set for each class
to construct the training set and another K samples
from the original training set for the development
set. For the test sets, we use the original develop-
ment set if it exists, otherwise, the original test set
is used. K is set to be 16 across all datasets.
Baselines We compare our approach with full-
model fine-tuning methods and state-of-the-art
black-box tuning methods described as follows: (1)
Finetuning , the standard way of finetuning a lan-
guage model for few-shot text classification. (2)
prompt-based fine-tuning as implemented by Gao
et al.(2021). The approach is referred to as LM-
BFF. Both (1) and (2) require updating the weights
of the LLM. Hence, they can be seen as white-box
methods. (3) ICL-RoBERTa , which applies the in-
context learning approach proposed in Brown et al.
(2020) (Brown et al., 2020b) with RoBERTa-large.
(4)Black-Box Tuning (BBT) (Sun et al., 2022b).
(5)BBTv2(Sun et al., 2022a). (4) and (5) are
derivative-freeoptimizationmethodsthatarebased
on the covariance matrix adaptation evolution strat-
egy to optimize the continuous prompt (Hansen
andOstermeier,2001). (6) RLPrompt (Dengetal.,
2022), which optimizes the discrete prompts with
reinforcement learning and adopts Q-learning to
find the best prompt. (7) Promptboosting (Hou
et al., 2023), which searches the verbalizer and
ensemble hundreds of verbalizers via AdaBoost to
weight different training samples. (8) To validate
the effectiveness of CrossTune, we consider an-
other feature-based variant, which is implemented
as an MLP classifier. Specifically, the MASK token
embedding is extracted from the frozen black-box
model and fed to a 2-layer MLP for classification.
We name the baseline MLP-Classifier.
Implementation Details To align with previous
studies on black-box tuning, we employ RoBERTa-
Large(Liuetal.,2019)(with354millionparameters)
as our large-scale black-box language model. It is

--- PAGE 6 ---
important to note that our methodology is model-
agnostic. This means that the black-box LLMs can
be any encoder-only or encoder-decoder models,
even those with billions of parameters.
Task Name Template
TREC [MASK] question: <X>
AGNews [MASK] News: <X>
SST-2 <X>. It was [MASK] .
MRPC <X 1>? [MASK] , <X 2>
QQP <X 1>? [MASK] , <X 2>
QNLI <X 1>? [MASK] , <X 2>
MNLI <X 1>? [MASK] , <X 2>
Table 2: Task-specific prompt templates and label
words.
For training the teacher model Adeberta, we set
the training batch size, the maximum sequence
length, and the maximum number of training steps
as 2, 128, and 2000 respectively. We perform grid
search on the learning rate of (1e-5, 2e-5) and gra-
dient accumulation steps (1, 2) respectively. The
DeBERTafinetuningisconductedonaNvidia1080
card, utilizing 5GB of GPU memory. The time cost
is quite light. The average hyperparameter search
time for a seed is about 30 minutes. When filtering
the ChatGPT-augmented data with the DeBERTa
teacher, we set the confidence threshold of the out-
put probability to 0.9 according to its distribution,
preserving up to M samples for each class. Empiri-
cally, 1000 <=M <= 1500. Table 3 presents the
statistics of the data used in our experiment. To en-
sureafaircomparison, thebaselineMLP-Classifier
model is trained on the same data as CrossTune.
For training CrossTune, we set the train batch
size, the learning rate, the total number of train-
ing epochs, and the maximum sequence length as
32, 4e-5, 100, and 512 respectively. Grid search
is performed on the number of attention heads (1,
2, 4, 8). The model is evaluated with the devel-
opment set at the end of each epoch and if the
validation performance does not improve for con-
secutive 5 epochs, we early stop the training pro-
cess. Table 2 describes the templates we use for
training CrossTune. It is worth noting that we do
not need a verbalizer in our approach and no ad-
ditional prompt is prepended to the template. In
Figure2,wepresentthelabeldescriptionsofTREC
andthoseoftheremainingdatasetswillbeincluded
in the Appendix of the final version.
5. Results & Analysis
5.1. Main Results
Table 4 summarizes the main results. We can
observe that on average, CrossTune outperformsTask Name #classes #augmented data #filtered data
TREC 6 8400 5500
AGNews 4 7000 4540
SST-2 2 3700 2800
MRPC 2 4000 2000
QQP 2 2800 1900
QNLI 2 3000 2000
MNLI 3 10000 2500
Table3: Theamountofaugmenteddataandfiltered
data. The data quantity in the table represents the
total count across all categories.
BBTv2by9.4%onaverage. Italsomatchestheper-
formance of LM-BFF, which is a strong white-box
adaptation method employing prompt-based tun-
ing. ComparedtothecurrentSoTAblack-boxtuning
approach, PromptBoosting, CrossTune achieves
significantly better results on MRPC, QNLI, and
MNLI. It outperforms PromptBoosting by an abso-
lute margin of 5.7% on average.
ComparedtoMLP-Classifier,whichalsodoesnot
rely on the expensive prompt search process and
istrainedonthesameaugmenteddata,CrossTune
achieves an improvement of 1.8% on average
across the seven datasets, underscoring that our
proposed label cross-attention network is more ef-
fective than using an MLP classifier. Furthermore,
CrossTune is more lightweight and efficient than
MLP-Classifierastheirnumbersoftrainableparam-
eters are 2.10M and 3.15M respectively.
Additionally, we can see that the performance
of CrossTune is more consistent with a smaller
standard deviation across different data splits com-
paredtoprompt-basedblack-boxmethods,suchas
BBTv2 and RLPrompt, suggesting that CrossTune
is less likely to overfit to specific data splits and
exhibits better generalization.
Impact of Augmented Data Amount We study
howtheperformanceofCrossTunevariesw.r.t. the
amount of augmented data. The results of MLP-
Classifier are also included in Table 5 for compari-
son. Specifically, we compare the cases when the
amount of augmented data for each class is 0, 300,
andfullrespectively. Fullamountreferstothesame
setting shown in Table 3.
When the quantity of augmented data is 0, i.e.,
only the original K-shot data is used, the perfor-
mance of both CrossTune and MLP-Classifier dras-
tically declines by around 10% on average. The
most pronounced performance drop is evident on
TREC and QNLI, which contain test cases with
diverse semantic and syntactic variations. This ob-
servation highlights the importance of boosting the
generalization of feature-based black-box tuning
approacheswithdataaugmentation. Notably, even
without data augmentation, CrossTune performs
comparably or better than the prompt-based black-

--- PAGE 7 ---
TREC AGNews SST-2 MRPC QQP QNLI MNLI Average
acc acc acc f1 f1 acc acc
Finetuning † 88.8 (2.1) 86.2 (1.4) 81.4 (3.8) 76.6 (2.5) 60.7 (4.3) 56.3 (1.5) 45.8 (6.4) 70.8
LM-BFF † 83.4 (2.7) 87.1 (1.2) 92.3 (1.5) 77.8 (2.0) 69.8 (1.8) 64.4 (4.6) 68.7 (2.0) 77.6
ICL-RoBERTa ‡26.2 (2.4) 62.2 (13.5) 85.9 (0.7) 45.8 (6.7) 36.1 (5.2) 53.8 (0.4) 52.0 (0.7) 51.7
BBT‡ 39.3 (5.2) 81.2 (2.7) 88.2 (1.7) 61.6 (4.3) 48.6 (8.3) 56.8 (2.0) 42.3 (2.8) 59.7
BBTv2 ‡ 42.0 (4.5) 85.3 (0.5) 83.8 (0.8) 77.0 (4.7) 56.3 (3.9) 66.3 (2.3) 51.4 (3.3) 66.0
RLPrompt ‡ 37.3 (3.5) 76.2 (2.7) 90.5(1.2) 68.9 (2.1) 53.7 (2.2) 52.1 (2.9) 40.7 (4.7) 59.9
PromptBoosting ‡81.6(4.0) 85.2 (0.9) 87.6 (3.0) 70.5 (2.9) 64.8 (3.7) 58.0 (3.3) 52.5 (1.5) 71.5
MLP-Classifier ‡80.8 (0.2) 85.9 (0.5) 89.1 (2.3) 80.4 (0.5) 64.8 (2.1) 70.4 (1.3) 56.7 (1.5) 75.4
CrossTune ‡ 85.0(1.8)86.6(1.1) 90.2 (2.5)82.3(0.6)66.1(1.8)71.4(0.8)58.5(1.8) 77.2
Table4: Mainexperimentresults. †referstowhite-boxmethodswhile ‡referstoblack-boxmethods. Inthe
black-box category, the best score for each task is highlighted in bold and the second best is underlined.
#data modelTREC AGNews SST-2 MRPC QQP QNLI MNLI Average
acc acc acc f1 f1 acc acc
0MLP-Classifier 46.1 82.3 88.9 76.2 64.8 54.0 53.6 66.6
CrossTune 46.4 82.5 88.2 79.5 63.8 58.8 52.5 67.4
300MLP-Classifier 73.9 85.8 88.9 78.9 67.0 67.3 54.2 73.7
CrossTune 78.6 85.1 88.6 81.8 65.7 69.1 54.7 74.8
fullMLP-Classifier 80.8 85.9 89.1 80.4 64.8 70.4 56.7 75.4
CrossTune 85.0 86.6 90.2 82.3 66.1 71.4 58.5 77.2
Table 5: Impact analysis of the augmented data amount on the performance of MLP-Classifier and
CorssTune. “Full" refers to the same amount of data as that presented in Table 3.
box tuning methods on most datasets (Table 4)
while requiring no expensive prompt or verbalizer
search process.
After increasing the number of augmented data
to 300 per class, the performance on TREC and
QNLI improves drastically. The average perfor-
mance of both MLP-Classifier and CrossTune be-
comes almost on par with their respective vari-
ants trained on the full data, surpassing all the
prompt-based black-box methods like BBTv2 and
RLPrompt. This suggests that feature-based black-
box tuning methods exhibit high data efficiency.
Finally, regardless of the amount of augmented
data used, CrossTune consistently outperforms
MLP-Classifier. This further emphasizes the ef-
ficacy of utilizing the rich semantics of label de-
scriptions with a cross-attention network.
5.2. Ablation Analysis
In-Distribution vs ChatGPT-Augmented Data
To examine whether ChatGPT is effective in pro-
viding augmented data for enhancing the few-shot
learners, we compare the performance of learn-
ing with the in-distribution augmented data against
learning with our augmented data using ChatGPT.
The in-distribution augmented data are the orig-
inal task-specific training data with their ground-
truth labels removed and then pseudo-labeled with
the DeBERTa teacher model. In the case of us-ing ChatGPT-augmented data, we also apply the
same DeBERTa teacher to pseudo-label and filter
the augmented data. Note that we keep the maxi-
mumamountoffiltereddatathesameforbothdata
sources to ensure a fair comparison. Table 6 show-
cases the results of the MLP-Classifier trained on
the two data sources across six different datasets.
Except for QQP, training on ChatGPT-Augmented
data yields better or comparable results than when
trained on in-distribution augmented data. The ob-
servation implies that ChatGPT is capable of pro-
ducing high-quality task-specific data. In practical
scenarios, we often have access to limited labeled
data and lack in-distribution training data. In these
situations, using ChatGPT for data augmentation
is a viable option to improve the performance of
few-shot learners.
We further analyze the data distributions of
ChatGPT-augmented data, the original training
data, and the test data. We first encode the text
to high-dimensional embeddings with the SimCSE
sentence embedder4(Gao et al., 2021b) and then
apply the T-SNE transformation. Figure 4 shows
the plots of TREC, QQP, and AGNews. We can
observe that for TREC and AGnews, ChatGPT-
augmented data is distributed relatively evenly
across the space of the in-distribution training data
and resemble a large portion of the test data.
4https://huggingface.co/princeton-nlp/

--- PAGE 8 ---
Figure 4: T-SNE Plots of embeddings w.r.t. original training, test, and ChatGPT-augmented training data.
Notethatwerandomlysamplethesameamountofin-distributiontrainingdataastheChatGPT-augmented
data from the original training set.
0102030405060708090100
dev test
1) TREC82.58383.58484.58585.58686.5
dev test
2) AGNews808284868890929496
dev test
3) SST -2
0102030405060708090
dev test
4) MRPC0102030405060708090
dev test
5) QQP01020304050607080
dev test
6) QNLI01020304050607080
dev test
7) MNLIChatGPT classification
DeBERTa teacher classification
ChatGPT only
ChatGPT + DeBERTa filter
Figure5: TheperformanceofChatGPTvsDeBERTaonthedevelopmentset,whichhelpsdeterminewhen
to filter the ChatGPT-augmented data. A positive correlation can be observed between the performance
of the teacher model on the development set and that of CrossTune on the test set across most datasets.
Data SourceTREC AGNews SST-2 MRPC QQP QNLI Avg
acc acc acc f1 f1 acc
I.I.D 78.4 86.1 88.5 75.3 77.8 66.6 78.8
ChatGPT 80.8 85.9 89.1 80.4 64.8 70.4 78.6
Table 6: Performance of MLP-Classifier trained on
in-distribution vs ChatGPT-Augmented Data.
However, for QQP, the distribution of ChatGPT-
augmenteddatadoesnotoverlapwellwiththeorig-
inal training data. Besides, because the amount
of test data in QQP is much greater than that of
the augmented data ( 40400 ≫1900), most of the
test data are not covered. The observations are
in line with results in Table 6 that MLP-Classifier
trained on ChatGPT-augmented data performs on
par with that trained on the original training data
in TREC and AGNews, but worse in the case of
QQP.Apossiblesolutionistooptimizetheprompts
input to ChatGPT such that more diverse data can
be generated. We leave such prompt engineering
efforts to future work.
Effectiveness of the Switch As introduced
in §3.4, a switch mechanism is implemented to de-
termine whether to filter the ChatGPT-augmenteddata with an additional DeBERTa teacher. As de-
picted in Figure 5, there is a consistent positive cor-
relation between the teachers’ performance on the
few-shot development set and the final test perfor-
mance of CrossTune across all the tasks except for
SST-2. Thatis,whentheswitchisactivated(indicat-
ing the DeBERTa teacher outperforms ChatGPT),
CrossTune, which is trained on data filtered by the
DeBERTa teacher, surpasses its variant trained on
the unfiltered augmented data, and vice versa. For
example, on TREC, AGNews, MRPC, QQP, and
MNLI,thetestperformanceofCrossTuneimproves
with DeBERTa filter (as the DeBERTa teacher ex-
hibitssuperiorperformancetoChatGPTonthefew-
shot development set) while on QNLI, the perfor-
manceofCrossTuneisbetterwithouttheDeBERTa
filter, given that the DeBERTa teacher underper-
forms compared to ChatGPT. These observations
confirm that our proposed switch mechanism is
reasonable.
Impact of label descriptions we further study
the effect of using different label descriptions. In
Table 7, we compare the performance of using
long and informative vs short and non-informative

--- PAGE 9 ---
description typeTREC AGNews QNLI
acc acc acc
Short 83.6 85.3 70.9
informative 85.0 86.6 71.4
Table 7: Effect of using informative vs non-
informative descriptions.
label descriptions. When the non-informative de-
scriptions are employed, CrossTune still works but
performs slightly worse than when using the long
and informative label descriptions. Hence, we can
conclude that informative label descriptions help to
improve CrossTune’s text classification capability.
The details of different label descriptions will be
presented in appendix in the final version.
Additionally, we examine whether label descrip-
tions also help improve other approaches. Ex-
periments are conducted on MRPC and SST-2
with the MLP-Classifier baseline. Specifically,
the input to the model is the concatenation of
{desc 1, desc 2, . . . , desc c, xi}where desc jis the la-
bel description of the j-th class and xiis the text
sequence to classify. We notice that the perfor-
mance of the MLP-classifier drops from 89.1% to
74.65% on SST-2 and 80.4% to 75.92% on MRPC,
suggesting a negative impact of label descriptions
on the MLP-classifier.
Impact of Augmented Data on Other Baselines
We further perform experiments with the base-
lines, BBTv2 and RLprompt on SST2, with the
same ChatGPT-augmented data as that used on
CrossTune. No significant improvement is ob-
served compared to training with the original 16-
shot data. BBTv2 achieves 83.8% vs 82.8% accu-
racy while RLPrompt achieves 90.5% vs 91.0% ac-
curacy before and after data augmentation respec-
tively. It shows these prompt-optimization-based
methods do not utilize the augmented data as ef-
fectively as CrossTune.
ImpactofOtherAugmentationTechniques Be-
sides ChatGPT augmentation, we explore whether
traditional data augmentation techniques, also en-
hanceCrossTune. Experimentsareconductedwith
CrossTune on MRPC and SST2, using data aug-
mented from the EDA techniques (Wei and Zou,
2019), including random swap, deletion, and inser-
tion of the input text. Our findings indicate that with
300 EDA-augmented data points, CrossTune’s per-
formance matches models trained with ChatGPT-
augmented data. However, as we increase the
data augmentation to 2000, the performance us-
ing EDA augmentation deteriorates compared to
using no augmentation at all. This decline could be
because a significant volume of EDA-augmenteddata introduces excessive noise into the language
model. The deletion, insertion, and swapping oper-
ations risk altering the original sentence’s semantic
meaning. Compared to EDA, ChatGPT-based aug-
mentation emerges as a more reliable method.
6. Conclusion
In summary, we propose CrossTune for few-shot
text classification under the black-box setting.
CrossTune treats the black-box LM as a feature
extractor and leverages label descriptions as ad-
ditional input semantic context. To boost the gen-
eralization of CrossTune, we avoid relying on in-
distribution unlabeled data, instead utilizing Chat-
GPT to generate pseudo-labeled training samples.
A switch mechanism is implemented to ensure the
qualityofthegenerateddata. Ourextensiveempiri-
calassessmentsacrosssevenbenchmarkdatasets
reveal CrossTune’s effectiveness in black-box tun-
ing,outperformingexistingstate-of-the-artbyanim-
pressive5.7%scoreonaverage. Evenwithoutdata
augmentation, CrossTune performs better or com-
parably than previous methods on most datasets.
Acknowledgement
Wethanktheanonymousreviewersfortheirinsight-
ful comments. This work is supported by the Na-
tional Natural Science Foundation of China (Grant
No. 62271432), Shenzhen Science and Tech-
nology Research Fund (Fundamental Research
Key Project Grant No. JCYJ20220818103001002),
and the Internal Project Fund from Shenzhen
Research Institute of Big Data under Grant No.
T00120220002.
Bibliographical References
Markus Bayer, Marc-André Kaufhold, and Christian
Reuter.2022. Asurveyondataaugmentationfor
text classification. ACM Comput. Surv. , 55(7).
Samuel R. Bowman, Gabor Angeli, Christopher
Potts,andChristopherD.Manning.2015. Alarge
annotated corpus for learning natural language
inference. In Proceedings of the 2015 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 632–642, Lisbon, Portugal.
Association for Computational Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,etal.
2020a. Language models are few-shot learners.
InAdvances in Neural Information Processing
Systems , volume 33, pages 1877–1901. Curran
Associates, Inc.

--- PAGE 10 ---
TomBrownetal.2020b. Languagemodelsarefew-
shot learners. In Advances in Neural Information
Processing Systems , volume 33, pages 1877–
1901. Curran Associates, Inc.
Yiming Chen, Yan Zhang, Chen Zhang, Grandee
Lee, Ran Cheng, and Haizhou Li. 2021. Revisit-
ingself-trainingforfew-shotlearningoflanguage
model. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 9125–9135, Online and Punta
Cana, DominicanRepublic.AssociationforCom-
putational Linguistics.
Hyunsoo Cho, Youna Kim, and Sang-goo Lee.
2023. CELDA: Leveraging black-box language
model as enhanced classifier without labels.
InProceedings of the 61st Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 4364–4379,
Toronto, Canada. Association for Computational
Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob
Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, et al. 2022. PaLM: Scaling language
modeling with pathways.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and
Christopher D. Manning. 2020. ELECTRA: Pre-
training text encoders as discriminators rather
than generators. In International Conference on
Learning Representations .
MingkaiDeng,JianyuWang,Cheng-PingHsieh,Yi-
han Wang, Han Guo, Tianmin Shu, Meng Song,
Eric Xing, and Zhiting Hu. 2022. RLPrompt: Op-
timizing discrete text prompts with reinforcement
learning. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 3369–3391, Abu Dhabi, United
Arab Emirates. Association for Computational
Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and
Short Papers) , pages 4171–4186, Minneapolis,
Minnesota. Association for Computational Lin-
guistics.
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun
Li, LIN Yong, Xiao Zhou, and Tong Zhang. 2023.
Black-box prompt learning for pre-trained lan-
guage models. Transactions on Machine Learn-
ing Research .Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav
Chaudhary, Onur Celebi, Michael Auli, Veselin
Stoyanov, and Alexis Conneau. 2021. Self-
training improves pre-training for natural lan-
guage understanding. In Proceedings of the
2021 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies , pages 5408–
5418, Online. Association for Computational Lin-
guistics.
Kawin Ethayarajh. 2019. How contextual are con-
textualized word representations? Comparing
the geometry of BERT, ELMo, and GPT-2 em-
beddings. In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP) , pages 55–65, Hong Kong, China. As-
sociation for Computational Linguistics.
Yu Fei, Zhao Meng, Ping Nie, Roger Wattenhofer,
and Mrinmaya Sachan. 2022. Beyond prompt-
ing: Making pre-trained language models better
zero-shot learners by clustering representations.
InProceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 8560–8579, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021a.
Making pre-trained language models better few-
shot learners. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume
1: Long Papers) , pages 3816–3830, Online. As-
sociation for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
2021b. SimCSE: Simple contrastive learning
of sentence embeddings. In Proceedings of the
2021 Conference on Empirical Methods in Nat-
ural Language Processing , pages 6894–6910,
Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Nikolaus Hansen and Andreas Ostermeier. 2001.
Completely Derandomized Self-Adaptation in
Evolution Strategies. Evolutionary Computation ,
9(2):159–195.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. DEBERTA: Decoding-
enhanced BERT with disentangled attention. In
International Conference on Learning Represen-
tations.
MohammadSaleh Hosseini, Munawara Munia, and
Latifur Khan. 2023. BERT has more to offer:
BERT layers combination yields better sentence

--- PAGE 11 ---
embeddings. In Findings of the Association for
Computational Linguistics: EMNLP 2023 , pages
15419–15431, Singapore. Association for Com-
putational Linguistics.
Bairu Hou, Joe O’Connor, Jacob Andreas, Shiyu
Chang, and Yang Zhang. 2023. PromptBoost-
ing: Black-box text classification with ten forward
passes. In Proceedings of the 40th International
Conference on Machine Learning , volume 202
ofProceedings of Machine Learning Research ,
pages 13309–13324. PMLR.
NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efficient transfer learning for
NLP. In Proceedings of the 36th International
Conference on Machine Learning , volume 97
ofProceedings of Machine Learning Research ,
pages 2790–2799. PMLR.
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. To-
ward semantics-based answer pinpointing. In
Proceedings of the First International Conference
on Human Language Technology Research .
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2022. LoRA: Low-rank adap-
tation of large language models. In International
Conference on Learning Representations .
Jacob Kahn, Ann Lee, and Awni Hannun. 2020.
Self-training for end-to-end speech recognition.
InICASSP 2020 - 2020 IEEE International Con-
ference on Acoustics, Speech and Signal Pro-
cessing (ICASSP) , pages 7084–7088.
Brian Lester, Rami Al-Rfou, and Noah Constant.
2021. The power of scale for parameter-efficient
prompt tuning. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Lan-
guage Processing , pages 3045–3059, Online
and Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang,
Yiming Yang, and Lei Li. 2020. On the sentence
embeddings from pre-trained language models.
InProceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP) , pages 9119–9130, Online. Associa-
tion for Computational Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics andthe 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Pa-
pers), pages 4582–4597, Online. Association for
Computational Linguistics.
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
YujieQian,ZhilinYang,andJieTang.2021. GPT
understands, too.
Yinhan Liu et al. 2019. RoBERTa: A robustly opti-
mizedBERTpretrainingapproach. arXiv preprint
arXiv: Arxiv-1907.11692 .
Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan
Xiong, Heng Ji, Chao Zhang, and Jiawei Han.
2020. Text classification using label names only:
A language model self-training approach. In
Proceedings of the 2020 Conference on Empir-
ical Methods in Natural Language Processing
(EMNLP) , pages 9006–9017, Online. Associa-
tion for Computational Linguistics.
Subhabrata Mukherjee, Arindam Mitra, Ganesh
Jawahar, Sahaj Agarwal, Hamid Palangi, and
Ahmed Awadallah. 2023. Orca: Progressive
learningfromcomplexexplanationtracesofGPT-
4.arXiv preprint arXiv: 2306.02707 .
OpenAI. 2023. GPT-4 technical report.
Long Ouyang and et al. 2022. Training language
models to follow instructions with human feed-
back. In Advances in neural information process-
ing systems .
ArchikiPrasad, PeterHase, XiangZhou, andMohit
Bansal. 2023. GrIPS: Gradient-free, edit-based
instruction search for prompting large language
models. In Proceedings of the 17th Conference
of the European Chapter of the Association for
Computational Linguistics , pages 3845–3864,
Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
Colin Raffel et al. 2020. Exploring the limits of
transfer learning with a unified text-to-text trans-
former. Journal of Machine Learning Research ,
21(140):1–67.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for few-shot text classification
and natural language inference. In Proceedings
of the 16th Conference of the European Chapter
of the Association for Computational Linguistics:
Main Volume , pages 255–269, Online. Associa-
tion for Computational Linguistics.
Kihyuk Sohn, David Berthelot, Nicholas Carlini,
Zizhao Zhang, Han Zhang, Colin A Raffel,

--- PAGE 12 ---
Ekin Dogus Cubuk, Alexey Kurakin, and Chun-
Liang Li. 2020. Fixmatch: Simplifying semi-
supervised learning with consistency and confi-
dence. Advances in neural information process-
ing systems , 33:596–608.
Tianxiang Sun, Zhengfu He, Hong Qian, Yunhua
Zhou, Xuanjing Huang, and Xipeng Qiu. 2022a.
BBTv2: Towardsagradient-freefuturewithlarge
language models. In Proceedings of the 2022
Conference on Empirical Methods in Natural
Language Processing , pages 3916–3930, Abu
Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Tianxiang Sun, Yunfan Shao, Hong Qian, Xuan-
jing Huang, and Xipeng Qiu. 2022b. Black-box
tuning for language-model-as-a-service. In Pro-
ceedings of the 39th International Conference on
Machine Learning , volume 162 of Proceedings
of Machine Learning Research , pages 20841–
20855. PMLR.
AlexWang, AmanpreetSingh, JulianMichael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE:Amulti-taskbenchmarkandanalysisplat-
form for natural language understanding. In In-
ternational Conference on Learning Representa-
tions.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A. Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. 2023. Self-instruct: Align-
ing language models with self-generated instruc-
tions. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 13484–13508,
Toronto, Canada. Association for Computational
Linguistics.
JasonWeiandKaiZou.2019. EDA:Easydataaug-
mentation techniques for boosting performance
ontextclassificationtasks. In Proceedings of the
2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 6382–6388,
Hong Kong, China. Association for Computa-
tional Linguistics.
QizheXie, ZihangDai, EduardHovy, ThangLuong,
and Quoc Le. 2020. Unsupervised data aug-
mentation for consistency training. In Advances
in Neural Information Processing Systems , vol-
ume 33, pages 6256–6268. Curran Associates,
Inc.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and
DaxinJiang.2023. WizardLM:Empoweringlarge
language models to follow complex instructions.
arXiv preprint arXiv: 2304.12244 .Lu Zhang, Jiandong Ding, Yi Xu, Yingyao Liu, and
Shuigeng Zhou. 2021. Weakly-supervised text
classification based on keyword graph. In Pro-
ceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing ,pages
2803–2813, Online and Punta Cana, Dominican
Republic. Association for Computational Linguis-
tics.
Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale
Schuurmans, and Joseph E. Gonzalez. 2023.
TEMPERA: Test-time prompt editing via rein-
forcementlearning. In The Eleventh International
Conference on Learning Representations .
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text
classification. In Advances in Neural Information
Processing Systems , volume 28. Curran Asso-
ciates, Inc.
ZihaoZhao,EricWallace,ShiFeng,DanKlein,and
Sameer Singh. 2021. Calibrate before use: Im-
proving few-shot performance of language mod-
els. In Proceedings of the 38th International Con-
ference on Machine Learning ,volume139of Pro-
ceedings of Machine Learning Research , pages
12697–12706. PMLR.
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui,
Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.
2020. Rethinking pre-training and self-training.
Advances in neural information processing sys-
tems, 33:3833–3845.
A. Addtional Dataset Details
Dataset Label Description
TRECdescription Answer to the question is a description.
entity Answer to the question is an entity.
abbreviation Answer to the question is an abbreviation.
number Answer to the question is a number.
human Answer to the question is a human.
location Answer to the question is a location.
AGNewstech It is a technology news.
world It is a world news.
sports It is a sports news.
business It is a business news.
QNLIentailment The statement contains the answer to the question.
non_entailment The statement contains no answer to the question.
Table 8: Short and Non-informative Label Descrip-
tions
Table 8 depicts the short and non-informative
label descriptions used in our ablation study (§5.2)
where we compare the effects of using informative
label descriptions against using non-informative
ones. Table 9 shows the label descriptions we use
in our main experiments.

--- PAGE 13 ---
Dataset Label Description
TRECdescription Definition of something, description of something, manner of an action, reason.
entityAnimal, Organ of body, Color, Invention, book and other creative piece, Currency name, Disease and medicine,
Event, Food, Musical instrument, Language, Letter like a-z, Other entity, Plant, Product, Religion, Sport, Element
and substance, Symbols and sign, Techniques and method, Equivalent term, Vehicle, Word with a special property.
abbreviation A shortened form of a word or phrase that is used to represent the full meaning.
numberNumber of something, Date, Distance, Price, Order, rank, Lasting time, Percent,
fraction, Speed, Temperature, Size, area and volume, Weight, Postcode or other code.
human Individual, Title of a person, Description of a person, Group or organization of persons.
location City, Country, Mountain, State, Other location.
AGNewstechThe Sci/Tech category is designed to encompass articles related to science and technology. It might include news about
scientific discoveries or research breakthroughs, technology product launches, technology company updates, coverage of
scientific and technology conferences, interviews with scientists or tech leaders, articles on new theories or models
in various scientific disciplines, advancements in medical technology, and many more.
worldIt’s a news article about international affairs, geopolitics, global events, or any topic that has a worldwide or international scope.
Examples may include news on international diplomacy, major global events like the United Nations General Assembly,
international conflicts or wars, significant elections or political events in different countries, global environmental issues, and more.
sportsArticles related to various sporting events, news, and updates. the Sports category could encompass
a wide range of topics such as game results, player transfers, injuries, interviews with athletes, coverage of international
sporting events like the Olympics, football (soccer) world cup, tennis grand slams, and more.
businessThe Business category typically cover topics related to commerce, economics, and finance on a local, national, or international
scale. It may include news about company mergers, financial reports, stock market updates, changes in economic
policies, interviews with business leaders, innovation in business models, trends in various industry sectors, and so on.
QNLIentailmentThe given statement logically contains the answer to the associated question.
If the truth of the statement provides the answer to the question, it’s considered an entailment.
non_entaimentThe given statement does not logically contain the answer to the associated question.
Even if the statement is true, it does not provide a valid answer to the question.
MNLIentailment The hypothesis can be logically inferred or implied from the premise.
neutral The premise and the hypothesis do not have a clear logical relationship.
contradiction The hypothesis contradicts or conflicts with the information presented in the premise.
MRPCequivalent Two sentences in the pair are semantically equivalent - they express the same, or very similar, meaning.
non_equivalent Two sentences in the pair are not semantically equivalent - they do not convey the same meaning.
QQPequivalent That’s to say,
non_equivalent Another different questions is,
SST-2positivesentences from movie reviews that express favorable, complimentary, or praiseworthy viewpoints about a movie.
The concept of positive sentiment in this context typically includes feelings of enjoyment, admiration, appreciation,
or satisfaction with elements of a movie such as its plot, acting, direction, cinematography, or other aspects of its production.
negativeSentences that express unfavorable, critical, or disparaging viewpoints about a movie. The concept of negative
sentiment here typically includes feelings of disappointment, dissatisfaction, frustration, or displeasure with elements
of a movie such as its plot, acting, direction, cinematography, or other aspects of its production.
Table 9: Label Descriptions used in main experiments
