# 2310.17876.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/dataset-generation/2310.17876.pdf
# Kích thước tệp: 3626614 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại COLM 2024
TarGEN: Tạo Dữ liệu Có Mục tiêu với Các Mô hình Ngôn ngữ Lớn
Himanshu Gupta1‡Kevin Scaria1♢‡Ujjwala Anantheswaran1♢∗Shreyas Verma2
Mihir Parmar1Saurabh Arjun Sawant1Chitta Baral1Swaroop Mishra1†
1Đại học Bang Arizona2Viện Công nghệ Georgia
{hgupta35, kscaria}@asu.edu
Tóm tắt
Chúng tôi trình bày TarGEN, một chiến lược nhắc nhiều bước để tạo ra các bộ dữ liệu tổng hợp chất lượng cao bằng cách sử dụng LLM. Một ưu điểm của TarGEN là bản chất không cần hạt giống của nó; nó không yêu cầu các thực thể tác vụ cụ thể, mở rộng khả năng áp dụng của nó vượt ra ngoài việc sao chép tác vụ. Điều này phân biệt nó với các kỹ thuật tạo dữ liệu khác, vì nó có thể được tận dụng cho các tác vụ mới hoặc cực kỳ đặc thù về miền mà không có dữ liệu thực thể hiện có. Chúng tôi tăng cường TarGEN với một mô-đun tự sửa lỗi cho phép LLM sửa chữa các thực thể được gán nhãn không chính xác trong quá trình tạo bộ dữ liệu, đảm bảo nhãn đáng tin cậy.

Để đánh giá hiệu quả của kỹ thuật của chúng tôi so với các đường cơ sở hiện có, chúng tôi mô phỏng tám tác vụ từ điểm chuẩn SuperGLUE để tạo ra một phiên bản "tổng hợp" và tinh chỉnh các mô hình ngôn ngữ khác nhau trên cả bộ huấn luyện tổng hợp và gốc. Đánh giá trên bộ kiểm tra gốc cho thấy các mô hình được huấn luyện trên các bộ dữ liệu tổng hợp hoạt động cao hơn ~1-3% điểm so với những mô hình được huấn luyện trên các bộ dữ liệu gốc. Cuối cùng, khi được tiền tinh chỉnh trên bộ dữ liệu SuperGLUE "tổng hợp" của chúng tôi, Llama2 (7B) mang lại kết quả ấn tượng trên bảng xếp hạng OpenLLM, vượt trội hơn mô hình được huấn luyện trên bộ dữ liệu Self-Instruct 2.62% điểm. Phân tích của chúng tôi cho thấy dữ liệu tổng hợp được tạo bởi TarGEN không chỉ cải thiện việc học của mô hình, mà còn có mức độ phức tạp, đa dạng tương đương hoặc cao hơn, và mức độ thiên lệch tương tự so với dữ liệu gốc1.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) như ChatGPT, Llama (Touvron et al., 2023a;c), và Mistral (Jiang et al., 2023) đã thể hiện kết quả ấn tượng trên rất nhiều tác vụ (OpenAI, 2023; Brown et al., 2020). Khi khả năng của LLM phát triển, các công cụ để kiểm tra mức độ của những khả năng này trở nên không đủ (Liu et al., 2022b; He et al., 2023; Valmeekam et al., 2022; Chen et al., 2021). Điều này đặc biệt đúng đối với các bộ dữ liệu đặc thù về miền, vì việc tạo ra các điểm chuẩn đánh giá được quản lý bởi chuyên gia tốn thời gian và công sức (Clark et al., 2018; Suzgun et al., 2022; Wang et al., 2022).

Một số phương pháp tạo bộ dữ liệu tổng hợp như Self-Instruct (Wang et al., 2023), AttrPrompt (Yu et al., 2023) và ZeroGen (Ye et al., 2022a) đã được đề xuất chủ yếu cho các tác vụ phân loại văn bản. Những cách tiếp cận này sử dụng học trong ngữ cảnh để tạo ra các điểm dữ liệu tổng hợp giống với các mẫu trong nhắc của chúng, do đó vốn dĩ hạn chế khả năng tạo ra các ví dụ đa dạng. Ngoài ra, những cách tiếp cận này thường được thiết kế riêng cho một số tác vụ hoặc bộ dữ liệu hạ nguồn nhất định, giảm khả năng thích ứng của chúng. Do đó, chúng có thể không phù hợp để tổng hợp các tác vụ/điểm chuẩn đặc thù về miền đánh giá các tác vụ mới nơi thiếu mẫu dữ liệu đủ để sử dụng làm mẫu nhắc.

Để giảm thiểu những vấn đề này, chúng tôi giới thiệu TarGEN, một chiến lược nhắc nhiều bước (Hình 1). Cách tiếp cận này bao gồm bốn bước. Đầu tiên, chúng tôi khởi tạo một tập hợp các ngữ cảnh để tiêm đa dạng ngữ nghĩa, tiếp theo là việc tạo ra các yếu tố đặc thù tác vụ mà chúng tôi gọi là "hạt giống thực thể" - các yếu tố tạo thành cơ sở độc đáo của mỗi thực thể. Những hạt giống này có thể là câu, đoạn văn, hoặc các yếu tố nguyên tử hơn nhưng không phải là mẫu đầu vào. Sau đó, đối với mỗi "hạt giống thực thể", chúng tôi xây dựng một ràng buộc nhãn sử dụng những hạt giống này để tạo ra một thực thể dữ liệu có thể quy cho nhãn bị ràng buộc.

1Cơ sở mã: https://github.com/kevinscaria/TarGEN ∗Hiện tại ở Microsoft †Hiện tại ở Google DeepMind ‡Hiện tại ở Amazon (Công việc được thực hiện trước khi gia nhập Amazon) ♢Đóng góp bằng nhau
1arXiv:2310.17876v3 [cs.CL] 8 Aug 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 1: Tổng quan về việc sử dụng TarGEN để tạo thực thể cho tác vụ WiC. Chúng tôi đầu tiên tạo một tập hợp các nhắc (hộp 1, 2 trong hình) để tạo hạt giống thực thể, hoặc các thành phần ngôn ngữ độc đáo cho mỗi thực thể tác vụ. Tiếp theo, chúng tôi tạo các nhắc đặc thù nhãn (hộp 3) tạo ra các thực thể dựa trên hạt giống thực thể và mối quan hệ được ngụ ý bởi nhãn cho tác vụ này. Chúng tôi sử dụng suy luận LLM zero-shot để tạo ra một tập hợp ban đầu các thực thể tổng hợp. Các thực thể sau đó được chuyển đến mô-đun tự sửa lỗi của chúng tôi bao gồm một meta-nhắc duy nhất cho phép chúng tôi gán lại nhãn cho các thực thể dữ liệu bị gán nhãn sai, giúp chúng tôi giảm nhiễu. Do đó, dựa trên mô tả tác vụ, chúng tôi có được các thực thể tổng hợp chất lượng cao để đánh giá một tác vụ.

Cuối cùng, chúng tôi tận dụng mô hình đánh giá của chúng tôi để tự sửa lỗi trên các thực thể được tạo, và gán lại nhãn cho chúng khi cần thiết (§D). Mô-đun tự sửa lỗi được tích hợp vào quy trình góp phần giảm nhiễu và nâng cao chất lượng tổng thể của nội dung được tạo. Sử dụng TarGEN, chúng tôi có thể tạo ra các bộ dữ liệu đa dạng chất lượng cao với nhãn chính xác. Phương pháp này cung cấp lợi ích không cần thiết các thực thể tác vụ có sẵn làm điểm bắt đầu để tạo. Do đó, nó phù hợp cho các tác vụ đặc thù về miền và việc tạo ra các điểm chuẩn mới, chỉ dựa vào các mô tả tác vụ được cung cấp.

Chúng tôi sử dụng TarGEN kết hợp với ChatGPT để tạo dữ liệu cho các tác vụ ngôn ngữ tự nhiên (suy luận văn bản, hỏi đáp) được sử dụng trong các điểm chuẩn như SNLI, MNLI, và SciTail. Để đảm bảo đánh giá công bằng so với các đường cơ sở hiện có, chúng tôi chọn các tác vụ từ điểm chuẩn SuperGLUE toàn diện và được công nhận rộng rãi và tạo ra các thực thể tổng hợp cho chúng dựa hoàn toàn trên mô tả tác vụ. Chúng tôi xử lý mỗi tác vụ như một thách thức hiểu ngôn ngữ độc lập, không tham chiếu trực tiếp đến tên điểm chuẩn hoặc bộ dữ liệu trong quá trình tạo. Chúng tôi huấn luyện một số mô hình từ các họ khác nhau (chỉ mã hóa, mã hóa-giải mã, và chỉ giải mã) trên bộ huấn luyện được tạo tổng hợp và bộ huấn luyện SuperGLUE gốc và đánh giá những mô hình này trên bộ kiểm tra gốc. Chúng tôi thấy rằng các mô hình được huấn luyện trên bộ huấn luyện tổng hợp hoạt động ngang bằng so với các mô hình được huấn luyện trên bộ huấn luyện gốc. Tinh chỉnh hướng dẫn dẫn đến tăng 3.42% cho các mô hình Flan T5 và cải thiện 3.24% cho các mô hình Pythia GPT. Chúng tôi cũng quan sát thấy rằng huấn luyện trên dữ liệu tổng hợp theo cách đa tác vụ mang lại cải thiện trung bình 4.73%, 3.21%, và 2.94% tương ứng cho T5-3B, Llama2-7B, và Mistral-7B, so với huấn luyện đa tác vụ trên dữ liệu gốc (§4). Mô-đun tự sửa lỗi đóng vai trò quan trọng trong việc cải thiện hiệu suất; huấn luyện đa tác vụ với T5-3B trên dữ liệu tổng hợp được tự sửa lỗi cải thiện hiệu suất so với dữ liệu tổng hợp không được tự sửa lỗi trên tất cả các tác vụ trung bình 5.9%.

Chúng tôi cũng tiến hành một nghiên cứu so sánh giữa TarGEN và Self-Instruct (Wang et al., 2023) bằng cách tiền tinh chỉnh Llama2 (7B) (Touvron et al., 2023c) trên cả hai bộ dữ liệu và đánh giá chúng trên điểm chuẩn OpenLLM. Các phát hiện của chúng tôi chỉ ra rằng Llama2 được tiền tinh chỉnh trên bộ dữ liệu của chúng tôi vượt trội hơn Llama2 được huấn luyện bằng Self-Instruct 2.62% điểm.

2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Một đánh giá con người có hệ thống đã được tiến hành để đánh giá chất lượng của các thực thể được tạo. Chúng tôi cũng thực hiện phân tích kỹ lưỡng về các bộ dữ liệu cho thấy tính bền vững của bộ dữ liệu SuperGLUE được tạo tổng hợp của chúng tôi về mặt độ khó của bộ dữ liệu, sự đa dạng, và thiên lệch. Nó thể hiện độ khó của bộ dữ liệu tương đương hoặc cao hơn, như được chỉ ra bởi thông tin V-usable thấp hơn (Ethayarajh et al., 2022), thể hiện độ phức tạp đầy đủ của các bộ dữ liệu của chúng tôi. Hơn nữa, các bộ dữ liệu của chúng tôi có sự đa dạng từ vựng tương đương (Yu et al., 2023) và liên tục hiển thị độ tương tự cosine thấp hơn giữa các cặp văn bản trong bộ dữ liệu, làm nổi bật nội dung phong phú và khác biệt của các bộ dữ liệu. Ngoài ra, chúng tôi đo sự đa dạng từ vựng của các bộ dữ liệu về mặt số lượng từ vựng. Kết quả của chúng tôi chỉ ra rằng các bộ dữ liệu được tạo bằng TarGEN liên tục thể hiện sự đa dạng từ vựng cao hơn so với các đối tác gốc của chúng. Về mặt thiên lệch, các bộ dữ liệu của chúng tôi phù hợp chặt chẽ với các bộ dữ liệu gốc, thể hiện đại diện cân bằng của các thực thể được đặt tên khác nhau như Thực thể Địa chính trị (GPE), Thực thể Người, và Quốc tịch hoặc Nhóm Tôn giáo hoặc Chính trị (NORP). Phân tích chi tiết có mặt trong §5.

2 Công việc Liên quan
Phương pháp|Không cần hạt giống|Huấn luyện yêu cầu như một phần của quy trình|Đa dạng tập trung|Khả năng tạo mẫu cho một tác vụ phức tạp|Chiến lược giảm thiểu nhiễu
Self Instruct Wang et al. (2023)|Không|Không|Không|Không|Có
CODA Evuru et al. (2024)|Không|Không|Không|Không|Không
Tạo Dữ liệu Tổng hợp Li et al. (2023b)|Có|Không|Không|Không|Không
ZeroGEN Ye et al. (2022b)|Có|Có|Không|Không|Không
SuperGEN Meng et al. (2022b)|Có|Có|Không|Không|Không
SunGEN Gao et al.|Có|Có|Không|Không|Có
ProGEN Ye et al. (2022d)|Có|Có|Không|Không|Có
Của chúng tôi (TarGEN)|Có|Không|Có|Có|Có

Bảng 1: So sánh định tính với các công việc trước đây theo các khía cạnh khác nhau của phương pháp.

Nghiên cứu gần đây đã chứng kiến sự xuất hiện của các phương pháp khác nhau khai thác LLM như các trình tạo dữ liệu tổng hợp (Long et al., 2024; Lu et al., 2023; Anantheswaran et al., 2024; Li et al., 2023a). Cụ thể, trong bối cảnh của các tác vụ phân loại few-shot nơi dữ liệu được gán nhãn khan hiếm, một số cách tiếp cận đã được giới thiệu. SuperGEN (Schick & Schütze, 2021) và ZeroGEN (Meng et al., 2022a) tận dụng LLM để tạo ra dữ liệu tổng hợp. Đối với các tác vụ zero-shot, SunGen (Gao et al., 2023) sử dụng các kỹ thuật lọc nhiễu, trong khi ProGEN (Ye et al., 2022c) sử dụng phản hồi mô hình để đảm bảo chất lượng dữ liệu được tạo. Tương tự, (Chia et al., 2022) giới thiệu các nhắc có cấu trúc cho các tác vụ như trích xuất triplet quan hệ. Hơn nữa, (Liu et al., 2022a) và (Wiegreffe et al., 2022) đề xuất các phương pháp tạo dữ liệu tổng hợp cho các tác vụ suy luận ngôn ngữ tự nhiên (NLI) và giải thích văn bản tự do trong môi trường hợp tác người-AI. Cũng đã có các cách tiếp cận được đề xuất để tạo dữ liệu bảng (Borisov et al., 2023) và dữ liệu hướng dẫn (Peng et al., 2023; Sun et al., 2023).

Hướng nghiên cứu trong tạo dữ liệu tổng hợp chủ yếu tập trung vào phân loại zero/few-shot hoặc bao gồm tinh chỉnh (Chen et al., 2023) hoặc tinh chỉnh lặp của các LLM mã nguồn mở (Yu et al., 2023). Ngược lại, phương pháp của chúng tôi đơn giản, nhẹ, và có thể thích ứng ngay cả với các LLM mã nguồn đóng. Ngoài ra, các phương pháp được đề cập ở trên thường dựa vào các mẫu hạt giống nơi các chú thích viên chuyên gia được sử dụng để quản lý và gán nhãn một tập mẫu có kích thước đáng kể. Những mẫu hạt giống này được sử dụng để tạo ra các mẫu tổng hợp bổ sung cho các tác vụ hạ nguồn khác nhau.

Chúng tôi khai thác khả năng tạo sinh của LLM để hoàn toàn tạo ra các thực thể hạt giống thúc đẩy việc tạo bộ dữ liệu tổng hợp đa dạng. Hơn nữa, cách tiếp cận của chúng tôi sử dụng chiến lược nhắc nhiều bước cùng với tự sửa lỗi để tạo dữ liệu có mục tiêu trong khi bảo tồn chất lượng về mặt đa dạng, thiên lệch, nhiễu, và gán nhãn sai. Cuối cùng, các phương pháp tạo bộ dữ liệu hiện có thường bị giới hạn bởi sự phụ thuộc vào các tác vụ hạt giống từ bộ dữ liệu gốc (Wang et al., 2023) trong khi quy trình không cần hạt giống của chúng tôi tận dụng các đặc tính bộ dữ liệu cấp cao cho quá trình tạo.

Chúng tôi so sánh khung của chúng tôi với các cách tiếp cận tạo dữ liệu phổ biến khác trong bảng dưới đây. Các trục so sánh:
• Không cần hạt giống: Liệu cách tiếp cận có yêu cầu các mẫu tác vụ được gán nhãn.

3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 2: Ma trận cho thấy hiệu ứng của bước tự sửa lỗi trên các bộ dữ liệu khác nhau của SuperGLUE. Các giá trị hàng cho thấy số lượng nhãn ban đầu được gán cho nhãn đó (ent, non: entailment, non-entailment; neutr, contr: neutral, contradiction). Số trong ô (i,j) phản ánh số lượng nhãn ban đầu được gán cho nhãn i được gán lại nhãn thành nhãn j sau tự sửa lỗi. Trong khi phần lớn các thực thể có nhãn của chúng được khẳng định lại bởi tự sửa lỗi, một số lượng đáng kể các thực thể đã được gán lại nhãn kết quả của bước này.

• Đa dạng tập trung: Liệu cách tiếp cận có tích cực tiêm đa dạng, khuyến khích các mẫu được tạo trên các ngữ cảnh khác nhau.
• Chiến lược giảm thiểu nhiễu: Liệu các cách tiếp cận có tích cực giảm thiểu nhiễu, một cách thuật toán hoặc thông qua việc bao gồm một mô-đun như Tự Sửa lỗi của chúng tôi như một phần của quy trình của chúng.

Để so sánh thực nghiệm với những cách tiếp cận này, xem §B.4 của Phụ lục.

3 TarGEN

3.1 Xây dựng Vấn đề

Bộ dữ liệu|Loại Tác vụ|Thực thể|Phân chia
AX-g|NLI|Not Ent: 146 | Ent: 138
BoolQ|Phân loại Nhị phân|True: 2535 | False: 1764
CB|NLI|Cont: 119 | Ent: 115 | Neut: 16
COPA|Phân loại Nhị phân|Choice 1: 195 | Choice 2: 107
ReCoRD|MCQ|1778 MCQs
RTE|NLI|Not Ent: 1241 | Ent: 1249
WiC|Phân loại Nhị phân|True: 2433 | False: 2410
WSC|Phân loại Nhị phân|True: 259 | False: 285

Bảng 2: Thống kê của bộ dữ liệu. Các từ viết tắt sau được sử dụng: Phân loại Nhị phân: Binary Classification; Ent: Entailment, Cont: Contradiction, Neut: Neutral; NLI: Natural Language Inference

Cho một bộ dữ liệu cho một tác vụ ngôn ngữ t, các điểm dữ liệu của nó có thể được biểu diễn như (d,l) sao cho tồn tại một hàm f: D → L
ft(d) = l, ∀d ∈ D, l ∈ L (1)

trong đó ft là biểu diễn toán học của tác vụ t, d ∈ D là đầu vào thực thể, và l là nhãn thực thể từ L, không gian nhãn cho tác vụ đã cho. Chúng tôi chính thức hóa việc tạo bộ dữ liệu như một chuỗi các vấn đề tạo văn bản bị ràng buộc nhãn, trong đó việc tạo ra một thực thể bị ràng buộc bởi giá trị nhãn của nó. Điều này cho phép chúng tôi kiểm soát phân phối nhãn trong bộ dữ liệu tổng hợp của chúng tôi và tạo ra các thực thể chất lượng cao bằng cách định nghĩa rõ ràng các mối quan hệ giữa thực thể và nhãn. Điều này tránh được nhu cầu về bất kỳ thực thể hạt giống nào từ bộ dữ liệu gốc; tức là đối với bất kỳ tác vụ đã cho nào, một bộ dữ liệu có thể được tạo từ đầu bằng cách xây dựng hàm tạo của nó từ mô tả tác vụ. Cách tiếp cận tạo bộ dữ liệu bị ràng buộc nhãn đặc thù tác vụ được đưa ra dưới đây:

⋃[l∈L] ⋃[n=1 to Nl] (Gl,t,n(l,in), L=l) (2)

4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

trong đó Nl là số lượng mẫu cho nhãn l, t là tác vụ, và Gl,t là một hàm nghịch đảo sao cho ft(Gl,t(l)) = l, và in là hạt giống thực thể thứ n. Việc xây dựng những chiến lược nhắc đặc thù tác vụ và nhãn này tạo thành cốt lõi của quy trình tổng hợp dữ liệu đơn giản của chúng tôi. Trong khi những nhắc riêng lẻ này là đặc thù tác vụ, bản chất của những nhắc này, và chuỗi chúng xảy ra, tuân theo một khung được thiết kế để tạo ra sự đa dạng và cải thiện độ bao phủ. Các giai đoạn của khung này như sau:

Bước 1 Chúng tôi tạo ra một tập C của "ngữ cảnh", hoặc "thiết lập" cung cấp phạm vi ngữ nghĩa độc đáo, như "tin tức địa chính trị", "đánh giá sách", hoặc "kịch bản phim". Những điều này cung cấp ngữ cảnh trong đó một mô hình có thể mô phỏng một đoạn trích tự nhiên xảy ra, để duy trì sự đa dạng ngữ nghĩa.

Bước 2 Chúng tôi tạo ra một tập hợp các đoạn văn, câu, hoặc các yếu tố đặc thù tác vụ, mà chúng tôi gọi là "hạt giống thực thể". Những hạt giống này tạo thành cơ sở cho mỗi thực thể của tác vụ.

Bước 3 Chúng tôi sử dụng các hạt giống thực thể làm đầu vào cho nhắc tạo. Nhắc này là một xây dựng mô tả của hàm tạo nghịch đảo Gl,t cho phép chúng tôi tạo ra các thực thể dữ liệu cho tác vụ.

Bước 4 Chúng tôi chuyển tất cả các thực thể dữ liệu được tạo qua một quá trình tự sửa lỗi một bước, trong đó chúng tôi sử dụng một nhắc đặc thù tác vụ để củng cố các hướng dẫn tác vụ và xác định và sửa chữa bất kỳ thực thể nào bị gán nhãn sai. Điều này giúp giảm nhiễu và cải thiện chất lượng bộ dữ liệu tổng thể.

3.2 Chiến lược nhắc đặc thù tác vụ

Hàm tạo Gl,t cho mỗi tác vụ được bao gồm trong §D. Chúng tôi tạo ra một tập hợp chung các ngữ cảnh (Bước 1) cho hầu như tất cả các tác vụ.

Thống kê Bộ dữ liệu: Chúng tôi chọn các tác vụ sau từ điểm chuẩn SuperGLUE: 1. CommitmentBank (CB). 2. Choice of Plausible Alternatives (COPA). 3. Recognizing Textual Entailment (RTE). 4. Word-in-Context (WiC). 5. Winograd Schema Challenge (WSC). 6. BoolQ. 7. Reading Comprehension with Commonsense Reasoning (ReCoRD). và 8. Winogender Diagnostics (AX-g). Cho những thách thức do mỗi bộ dữ liệu đặt ra, chúng tôi sử dụng TarGEN để tạo ra các thực thể tổng hợp có thể được sử dụng để đánh giá hiệu suất mô hình ngôn ngữ. Bảng 2 thể hiện phân chia thực thể được sử dụng trong các bộ dữ liệu gốc và tổng hợp. Vì việc tạo nhãn được kiểm soát, SuperGLUE tổng hợp được tạo để khớp với số lượng thực thể gốc chính xác, trong khi duy trì phân phối nhãn cân bằng2. Quy trình tổng hợp dữ liệu chính xác và các nhắc được sử dụng cho mỗi bộ dữ liệu có thể được tìm thấy trong §D.

CB (De Marneffe et al., 2019): Chúng tôi tạo ra các cặp mẫu chia sẻ mối quan hệ giữa nhãn l ∈ {entailment, neutral, contradiction}, và dựa trên một ngữ cảnh đã cho c ∈ C.

COPA (Roemmele et al., 2011): Chúng tôi tạo ra các thực thể cho các quan hệ CAUSE và EFFECT. Trong trường hợp này, Bước 3 bao gồm việc tạo ra một tiền đề và 2 giả thuyết cho một ngữ cảnh đã cho. Đối với mỗi quan hệ r, chúng tôi tạo ra (1) các cặp câu (P,C) sao cho tiền đề và giả thuyết chia sẻ mối quan hệ được chỉ định, và (2) một giả thuyết thay thế Calt không chia sẻ rõ ràng mối quan hệ được chỉ định với tiền đề P. Do đó (P,C) ∈ r, (P,Calt) ∉ r. Không gian nhãn cho tác vụ này được định nghĩa là L = {C1,C2}. Để đảm bảo phân chia nhãn đều, chúng tôi luân phiên chọn các thực thể thuộc tính của C như C1 và Calt như C2 và ngược lại.

RTE: Một tập hợp các thách thức suy luận văn bản. Đối với tác vụ này, bước 2 bao gồm việc tạo ra một tập hợp các tiền đề P làm hạt giống thực thể. Đối với mỗi p ∈ P, chúng tôi sau đó tạo ra các giả thuyết hoặc là hợp lý logic (l = entailment) hoặc không hợp lý logic, tức là không tuân theo tiền đề (l = not entailment).

WiC (Pilehvar & Camacho-Collados, 2019): Chúng tôi quản lý một danh sách các từ đồng âm (S) và tất cả các định nghĩa của chúng (Ms ∀s ∈ S). Những điều này hoạt động như hạt giống thực thể. Đối với mỗi m ∈ Ms, cho nhãn True, chúng tôi tạo ra một cặp câu (d1,d2) chứa từ s, sao cho định nghĩa của s trong d1 và d2 là m. Đối với nhãn False, chúng tôi ngẫu nhiên chọn m1,m2 ∈ Ms, m1 ≠ m2 và tạo ra d1,d2 sao cho các định nghĩa của s trong d1 và d2 là m1 và m2 tương ứng, làm cho chúng khác biệt về nghĩa từ.

WSC (Levesque et al., 2012): Đối với mỗi ngữ cảnh, chúng tôi tạo ra các cặp cụm danh từ khác biệt (N1,N2) với cùng số nhiều. Đại từ và cụm danh từ này hoạt động như hạt giống thực thể. Đối với mỗi cặp (N1,N2), chúng tôi sau đó tạo ra văn bản s chứa N1 và N2 với tất cả các đại từ được gán nhãn với các cụm danh từ đồng tham chiếu.

2Do hạn chế ngân sách ChatGPT, bộ dữ liệu ReCoRD được cắt ngắn để có 1778 thực thể, bộ dữ liệu BoolQ có 4299 thực thể và bộ dữ liệu MultiRC đã bị bỏ qua.

5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

[Bảng hiệu suất với các cột: AX-g, BoolQ, Og, Syn, Og-I, Syn-I cho các mô hình Cerebras, Pythia, T5, Flan, RoBERTa qua các tác vụ khác nhau]

Bảng 3: Hiệu suất của các mô hình khác nhau trên các tác vụ SuperGLUE. Đối với mỗi bộ dữ liệu, chúng tôi so sánh hiệu suất của những mô hình này (Cerebras, Pythia, T5, FLAN, RoBERTa) khi được huấn luyện trên 4 biến thể khác biệt: Og (dữ liệu huấn luyện gốc), Syn (dữ liệu huấn luyện tổng hợp), Og-I (Tinh chỉnh hướng dẫn trên dữ liệu gốc), Syn-I (Tinh chỉnh hướng dẫn trên dữ liệu tổng hợp). Đối với mỗi tác vụ, chúng tôi ký hiệu mô hình hoạt động tốt nhất được huấn luyện trên dữ liệu huấn luyện gốc bằng màu xanh lá cây, và mô hình hoạt động tốt nhất được huấn luyện trên dữ liệu huấn luyện tổng hợp bằng màu xanh lam. Tất cả kết quả được trình bày bằng %. Chúng tôi đo hiệu suất bằng độ chính xác, ngoại trừ trường hợp ReCoRD, nơi chúng tôi sử dụng điểm Rouge-L.

[Bảng kết quả đa tác vụ với các cột cho BoolQ, WiC, CB, AX-g, ReCoRD, RTE, WSC, COPA]

Bảng 4: Kết quả trên T5-3B, Llama2-7B, và Mistral-7B, được huấn luyện theo cách đa tác vụ. Og: Sử dụng một tập hợp kết hợp các bộ dữ liệu gốc để huấn luyện mô hình. Syn: Sử dụng các phiên bản tổng hợp để huấn luyện mô hình. Tất cả các số đều tính bằng %. Đối với mỗi tác vụ, chúng tôi ký hiệu mô hình hoạt động tốt nhất được huấn luyện trên dữ liệu huấn luyện gốc bằng màu xanh lá cây, và mô hình hoạt động tốt nhất được huấn luyện trên dữ liệu huấn luyện tổng hợp bằng màu xanh lam.

Chúng tôi ngẫu nhiên chọn một đại từ mơ hồ đồng tham chiếu P từ văn bản. Dựa trên đại từ này và ràng buộc nhãn, chúng tôi gắn một cụm danh từ vào thực thể đầu vào.

BoolQ (Clark et al., 2019): Đối với tác vụ này, chúng tôi tạo ra các đoạn văn với nhiều thực thể và các quan hệ giữa các thực thể hoạt động như hạt giống thực thể trong Bước 2. Trong Bước 3, chúng tôi tạo ra một truy vấn, dựa trên đoạn văn

6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

[Bảng so sánh hiệu suất với và không có mô-đun tự sửa lỗi]

Bảng 5: Kết quả sử dụng mô hình T5-3B không có và có mô-đun tự sửa lỗi (SC). Chúng tôi quan sát thấy rằng việc sử dụng tự sửa lỗi cải thiện hiệu suất trên tất cả các bộ dữ liệu trung bình 5.9%.

[Bảng thống kê đa dạng từ vựng]

Bảng 7: Đa dạng từ vựng của bộ dữ liệu. Các số màu đỏ tương ứng với nơi đa dạng của bộ dữ liệu tổng hợp bị hạn chế. Màu xanh lam tương ứng với nơi bộ dữ liệu tổng hợp có nhiều đa dạng hơn so với bộ gốc.

p ∈ P và ràng buộc nhãn l ∈ {Yes,No}. Trong trường hợp l = Yes, truy vấn là thông tin có thể được suy ra từ đoạn văn. Đối với l = No, chúng tôi tạo ra một truy vấn mâu thuẫn với đoạn văn.

ReCoRD (Zhang et al., 2018): Bộ dữ liệu này đánh giá việc hiểu các mối quan hệ suy luận ngụ ý. Trong Bước 2, chúng tôi tạo ra một tập hợp các đoạn văn A để hoạt động như hạt giống thực thể. Tiếp theo, đối với mỗi bài báo a ∈ A, chúng tôi tạo ra một câu phức tạp, liên quan đến ngữ cảnh, và sau đó che khuất một tham chiếu thực thể duy nhất.

AX-g (Rudinger et al., 2018): Đối với tác vụ này, chúng tôi tạo ra 10 cặp chủ đề từ cùng một không gian miền, trong Bước 2. Đối với mỗi cặp chủ đề, chúng tôi sau đó tạo ra một mệnh đề độc lập chứa những chủ đề này. Những mệnh đề độc lập này sau đó được sử dụng để tạo ra các mệnh đề phụ thuộc đồng tham chiếu với mỗi chủ đề, để hoạt động như hạt giống thực thể. Trong bước 3, chúng tôi sử dụng những mệnh đề phụ thuộc đặc thù chủ đề này và các cặp chủ đề để tạo ra các giả thuyết trung lập giới tính dựa trên các ràng buộc nhãn.

3.3 Tự sửa lỗi

Mặc dù có khả năng học, LLM thể hiện lý luận không nhất quán (Ye & Durrett, 2022). Chúng tôi khắc phục chúng bằng cách thực hiện tự sửa lỗi, một chiến lược đánh giá sửa chữa các nhãn không nhất quán trong quá trình tổng hợp dữ liệu. Chúng tôi tận dụng một LLM như một mô hình đánh giá (ChatGPT trong trường hợp này) để xác minh sự phù hợp giữa các thực thể được tạo và nhãn của chúng, cũng như sự phù hợp giữa những thực thể này và mô tả tác vụ. Tự sửa lỗi bao gồm một meta-nhắc duy nhất chung cho tất cả các tác vụ. Các hướng dẫn tác vụ và các ví dụ xác thực đặc thù tác vụ được sử dụng để tăng cường meta-nhắc và điều chỉnh nó cho mỗi bộ dữ liệu được tạo. Meta-nhắc này và các hướng dẫn tác vụ có trong §F.

[Bảng hiệu suất trung bình theo tác vụ đơn]

Bảng 6: Hiệu suất trung bình tác vụ đơn qua các tác vụ, mô hình, và biến thể dữ liệu. Og, Syn, và I đại diện cho Original, Synthetic, và Instruction tuning tương ứng.

Dựa trên đầu vào được cung cấp, meta-nhắc giúp đánh giá tính đúng đắn của nhãn được quy cho. Nếu nhãn này được coi là không đúng, mô hình đánh giá tạo ra nhãn đúng dựa trên các hướng dẫn và đầu vào thực thể. Đáng chú ý, từ Hình 2 có thể thấy rằng một số lượng đáng kể các thực thể yêu cầu gán lại nhãn đặc biệt đối với các tác vụ độ phức tạp cao như AX-g và WSC.

4 Thí nghiệm và Kết quả

Chúng tôi huấn luyện năm mô hình trong thiết lập học tác vụ đơn (STL) cho mỗi bộ dữ liệu gốc và tổng hợp riêng biệt. Chúng tôi đánh giá chúng trên bộ kiểm tra gốc. Các thí nghiệm trên được lặp lại với tinh chỉnh hướng dẫn cũng như vậy. Chúng tôi cũng thực hiện một thí nghiệm học đa tác vụ (MTL) nơi ba mô hình được tinh chỉnh theo cách đa tác vụ trên các bộ dữ liệu gốc và tổng hợp riêng biệt (Mishra et al., 2021). Kết quả trong §4.1 là trung bình của năm lần chạy.

7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 3: So sánh đa dạng ngữ nghĩa giữa các bộ dữ liệu trong bộ dữ liệu gốc và bộ dữ liệu được tạo tổng hợp. Có thể thấy rằng độ tương tự cosine của các bộ dữ liệu gốc cao hơn đối với hầu hết các tác vụ so với các bộ dữ liệu tổng hợp có độ tương tự cosine thấp hơn một cách nhất quán, cho thấy sự đa dạng ngữ nghĩa cao hơn.

Mô hình: Chúng tôi sử dụng RoBERTa Large (354M) (Liu et al., 2019), Pythia GPT (410M) (Biderman et al., 2023), Cerebras GPT (590M) (Dey et al., 2023), Flan T5 Large (780 M) (Chung et al., 2022), T5 Large (780M) (Raffel et al., 2020) trong thiết lập STL, và T5-3B, Llama2-7B (Touvron et al., 2023b), và Mistral-7B (Jiang et al., 2023) trong thiết lập MTL.

Hình 4: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu BoolQ và bộ dữ liệu BoolQ được tạo tổng hợp. Điều này đại diện cho phân phối cho thẻ thực thể được đặt tên GPE (Thực thể Chính trị Địa lý). Phân phối cho các thực thể khác có thể được tìm thấy trong §B.

Metric Đánh giá: Theo điểm chuẩn SuperGLUE, chúng tôi sử dụng độ chính xác để đo hiệu suất trên tất cả các tác vụ, ngoại trừ ReCoRD, mà chúng tôi đánh giá trên điểm Rouge-L.

4.1 Kết quả

Bảng 3 thể hiện kết quả của các mô hình được huấn luyện trên cả bộ dữ liệu gốc và tổng hợp cho mỗi tác vụ. Các mô hình được tinh chỉnh trên dữ liệu tổng hợp nhất quán khớp hoặc vượt trội hơn các biến thể được tinh chỉnh trên dữ liệu gốc. Hơn nữa, hiệu suất cải thiện 3% bằng tinh chỉnh hướng dẫn (Mishra et al., 2021; Wei et al., 2021; Scaria et al., 2023; Gupta et al., 2023).

Bảng 6 đưa ra kết quả trung bình theo mô hình cho cùng. Bảng 4 ký hiệu kết quả khi các bộ dữ liệu được huấn luyện theo cách đa tác vụ với các mô hình lớn hơn. Bảng 5 làm nổi bật hiệu ứng của mô-đun tự sửa lỗi khi T5 3B được huấn luyện không có mô-đun tự sửa lỗi.

5 Phân tích

Trong phần này, chúng tôi trình bày phân tích toàn diện về dữ liệu tổng hợp được tạo, xem xét cả khía cạnh định lượng và định tính.

Đánh giá Con người Như một phần của đánh giá chất lượng, một đánh giá con người có hệ thống đã được thực hiện trên tất cả các bộ dữ liệu được tạo với khả năng tốt nhất của chúng tôi. Để đảm bảo bao phủ đầy đủ, chúng tôi thực hiện đánh giá con người trên

8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

20% của tất cả các bộ dữ liệu tổng hợp. Đánh giá được tiến hành một cách công bằng bởi 3 nhóm gồm 2 đánh giá viên mỗi nhóm. Những nhóm này bao gồm các tác giả, và do đó không yêu cầu lao động bên ngoài.

[Bảng so sánh L2SSG với L2SI]

Bảng 8: So sánh Synthetic SuperGLUE, Self Instruct Dataset, và AttrPrompt. T5-3B được tiền tinh chỉnh trên cả hai bộ dữ liệu riêng lẻ và tinh chỉnh trên các bộ dữ liệu OpenLLM.

Các thực thể được tạo được chấm điểm trên thang điểm từ A (xuất sắc) đến D (kém). Các đánh giá viên chấm điểm các thực thể được tạo trong ngữ cảnh của ràng buộc nhãn gốc, cũng như trong ngữ cảnh của nhãn được cập nhật sau tự sửa lỗi. Chúng tôi nhận thấy rằng khi các thực thể trải qua tự sửa lỗi, số lượng mẫu được đánh giá "xuất sắc" tăng lên và số lượng mẫu "không thể chấp nhận" giảm. Để đánh giá độ tin cậy của cách tiếp cận đánh giá con người của chúng tôi, chúng tôi đo sự đồng ý giữa các đánh giá viên bằng cách sử dụng Cohen's Kappa (kết quả trong K = 0.71, 0.67, 0.79) và hệ số tương quan Spearman (thu được r = 0.85, 0.83, 0.91). Chi tiết có thể được tìm thấy trong §H. Chúng tôi cũng phát hiện ra rằng dữ liệu tổng hợp không chứa thực thể nào từ bộ dữ liệu SuperGLUE gốc, ngăn chặn bất kỳ rò rỉ dữ liệu nào.

So sánh với Self-Instruct: Để so sánh bộ dữ liệu SuperGLUE tổng hợp với các điểm chuẩn tuân theo hướng dẫn tổng hợp khác, chúng tôi chọn Self-Instruct (Wang et al., 2023) một khung tạo bộ dữ liệu tổng hợp phổ biến. Chúng tôi chọn Llama2 (7B) và tiền tinh chỉnh bằng cách sử dụng SuperGLUE tổng hợp. Chúng tôi gọi mô hình được tinh chỉnh này là L2SSG.

Hình 5: So sánh PVI (thông tin V-usable) cho AXG, BoolQ, và WiC gốc và bộ dữ liệu được tạo tổng hợp. Dữ liệu tổng hợp dường như có chất lượng tốt hơn vì PVI của các bộ dữ liệu gốc tập trung xung quanh -0.1 đến 0.1 trong khi dữ liệu tổng hợp được tạo có hỗn hợp đa dạng của mức độ khó khăn giữa các mẫu. So sánh độ khó của bộ dữ liệu cho tất cả các bộ dữ liệu có thể được tìm thấy trong §6.

Chúng tôi gọi mô hình Llama2 được huấn luyện trên bộ dữ liệu Self-Instruct là L2SI. Cả hai mô hình đều được đánh giá trên điểm chuẩn OpenLLM. Bảng 8 cho thấy L2SSG ghi điểm cao hơn L2SI 2.62%, nhấn mạnh chất lượng của việc tạo bộ dữ liệu có mục tiêu bởi khung TarGEN so với Self-Instruct.

Đa dạng Từ vựng: Chúng tôi phân tích sự đa dạng của bộ dữ liệu, theo các dòng được trình bày trong (Yu et al., 2023). Chúng tôi khởi động khám phá của chúng tôi với một kiểm tra dựa trên từ vựng đơn giản để đánh giá sự đa dạng từ vựng, như được tóm tắt trong Bảng 7. Đáng chú ý, dữ liệu tổng hợp TarGEN của chúng tôi thể hiện sự đa dạng từ vựng trung bình cao hơn 25% qua các tác vụ bộ dữ liệu khác nhau.

Đa dạng Ngữ nghĩa: Để phân tích sự đa dạng ngữ nghĩa, chúng tôi xem xét phân phối độ tương tự cosine của nhúng SentenceBERT của các cặp mẫu trong bộ dữ liệu như được trình bày trong Hình 3. Qua hầu hết các tác vụ SuperGLUE, các bộ dữ liệu TarGEN nhất quán hiển thị độ tương tự cosine thấp hơn so với bộ dữ liệu gốc, cho thấy sự tương tự ngữ nghĩa giảm của các mẫu trong bộ dữ liệu và, do đó, sự đa dạng ngữ nghĩa cao hơn. Quan sát này nhấn mạnh khả năng nội tại của cách tiếp cận của chúng tôi để tạo ra các mẫu đa dạng. Hơn nữa, các phát hiện của chúng tôi về độ tương tự cosine cao hơn của các bộ dữ liệu gốc phù hợp với những phát hiện của (Parmar et al., 2023), nơi các tác giả làm nổi bật xu hướng của các bộ dữ liệu được tạo bởi đám đông thể hiện thiên lệch cao và đa dạng thấp. Hiện tượng này phát sinh khi các công nhân được tạo bởi đám đông thường tuân theo các mẫu được cung cấp bởi những người tạo bộ dữ liệu. Bộ dữ liệu được tạo bằng cách tiếp cận của chúng tôi có thể tạo ra các mẫu đa dạng cao.

9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Độ khó Bộ dữ liệu: Để ước tính độ khó của bộ dữ liệu, chúng tôi sử dụng thông tin V-usable (Ethayarajh et al., 2022), ước tính thông tin mà một đầu vào X giữ để dự đoán mục tiêu Y qua một họ mô hình V.

Thông tin V-usable thấp hơn cho thấy độ khó bộ dữ liệu cao hơn đối với V. Hình 5 cung cấp cái nhìn so sánh về độ khó bộ dữ liệu giữa các bộ dữ liệu gốc và được tạo tổng hợp bởi TarGEN. Đáng chú ý, các bộ dữ liệu tổng hợp thể hiện một phạm vi đa dạng các mẫu với thông tin V-usable điểm khác nhau, thể hiện sự đa dạng của chúng về mặt độ khó. Hơn nữa, sự vắng mặt của các mẫu bị gán nhãn sai, được chỉ ra bởi thông tin V-usable dương trong các bộ dữ liệu được tạo tổng hợp, nhấn mạnh hiệu quả của các nhắc tự sửa lỗi.

Thiên lệch Bộ dữ liệu: Để ước tính thiên lệch bộ dữ liệu, chúng tôi trực quan hóa phân phối của các token liên quan đến các thực thể được đặt tên, thuộc về Thực thể Địa chính trị (GPE), Sản phẩm, và Quốc tịch hoặc Nhóm Tôn giáo hoặc Chính trị (NORP). Phân phối của các token đầu vào cho bộ dữ liệu BoolQ gốc và được tạo tổng hợp được trình bày trong Hình 4. Kết quả của chúng tôi cho thấy rằng phân phối của các thực thể GPE, Sản phẩm, và NORP trong bộ dữ liệu TarGEN gần giống với bộ dữ liệu gốc³. Các thí nghiệm bổ sung và phân tích mở rộng (bao gồm các biểu đồ so sánh bộ dữ liệu) của cách tiếp cận của chúng tôi có thể được tìm thấy trong §A và §B của Phụ lục.

6 Kết luận

Trong công việc này, chúng tôi đã giới thiệu TarGEN, một chiến lược nhắc nhiều bước để tạo ra các bộ dữ liệu tổng hợp chất lượng cao và đa dạng sử dụng LLM mà không cần giám sát của con người.

Chúng tôi đã mô tả phương pháp từng bước cho TarGEN để tổng hợp một bộ dữ liệu từ các hướng dẫn mà không cần bất kỳ mẫu tác vụ nào. Để đánh giá khung được đề xuất của chúng tôi, chúng tôi đã mô phỏng tám tác vụ từ điểm chuẩn SuperGLUE và so sánh nó với SuperGLUE gốc bằng cách huấn luyện các họ mô hình khác nhau. Kết quả thí nghiệm cho thấy các mô hình được tinh chỉnh trên SuperGLUE tổng hợp của chúng tôi vượt trội hơn các mô hình được tinh chỉnh trên SuperGLUE gốc. Phân tích toàn diện về điểm chuẩn tổng hợp so với điểm chuẩn gốc dẫn đến một số phát hiện thú vị như việc các thực thể dữ liệu trong điểm chuẩn được tổng hợp của chúng tôi khó khăn hơn và đa dạng hơn so với điểm chuẩn gốc, và cũng thể hiện thiên lệch bộ dữ liệu tương tự.

Hạn chế

So sánh thêm với Self-Instruct và AttrPrompt cho thấy SuperGLUE tổng hợp phục vụ như tập dữ liệu tiền tinh chỉnh tốt hơn khi được đánh giá trên điểm chuẩn OpenLLM dẫn đến hiệu suất ấn tượng khi sử dụng T5-3B và Llama2-7B. Mặc dù TarGEN hỗ trợ tạo dữ liệu chất lượng cao, chúng tôi tin rằng điều quan trọng là đánh giá các khung được đề xuất của chúng tôi trong bối cảnh đa ngôn ngữ và cũng trên các điểm chuẩn bổ sung, bao gồm BigBench, LILA, và HELM. Hơn nữa, TarGEN hiện tại dựa vào mô hình ChatGPT để tổng hợp điểm chuẩn, nhưng kế hoạch của chúng tôi bao gồm khám phá tác động của các LLM khác như GPT-4, Llama2, và Falcon khi được sử dụng với TarGEN. Chúng tôi tin rằng TarGEN có thể phục vụ như một công cụ có giá trị để nâng cao chất lượng tạo dữ liệu, do đó giảm nỗ lực của con người.

³Để đảm bảo kiểm tra quy mô rộng trên tất cả các mẫu, chúng tôi đã sử dụng thư viện Spacy và mô hình en_core_web_sm của nó để trích xuất các thẻ thực thể được đặt tên.

10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Tài liệu Tham khảo

Họ mô hình claude 3: Opus, sonnet, haiku. URL https://api.semanticscholar.org/CorpusID:268232499.

Ujjwala Anantheswaran, Himanshu Gupta, Kevin Scaria, Shreyas Verma, Chitta Baral, và Swaroop Mishra. Điều tra tính bền vững của llms trên các bài toán từ toán học. arXiv preprint arXiv:2406.15444, 2024.

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: Một bộ để phân tích các mô hình ngôn ngữ lớn qua huấn luyện và mở rộng quy mô. In International Conference on Machine Learning, pp. 2397–2430. PMLR, 2023.

Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, và Gjergji Kasneci. Các mô hình ngôn ngữ là những trình tạo dữ liệu bảng thực tế, 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Các mô hình ngôn ngữ là những người học few-shot. Advances in neural information processing systems, 33:1877–1901, 2020.

Derek Chen, Celine Lee, Yunan Lu, Domenic Rosati, và Zhou Yu. Hỗn hợp của các nhắc mềm để tạo dữ liệu có thể điều khiển. CoRR, abs/2303.01580, 2023. doi: 10.48550/arXiv.2303.01580. URL https://doi.org/10.48550/arXiv.2303.01580.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên code. ArXiv, abs/2107.03374, 2021. URL https://api.semanticscholar.org/CorpusID:235755472.

Yew Ken Chia, Lidong Bing, Soujanya Poria, và Luo Si. RelationPrompt: Tận dụng các nhắc để tạo dữ liệu tổng hợp cho việc trích xuất triplet quan hệ zero-shot. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 45–57, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.5. URL https://aclanthology.org/2022.findings-acl.5.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Mở rộng các mô hình ngôn ngữ được tinh chỉnh hướng dẫn. arXiv preprint arXiv:2210.11416, 2022.

Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, và Kristina Toutanova. Boolq: Khám phá độ khó đáng ngạc nhiên của các câu hỏi có/không tự nhiên. ArXiv, abs/1905.10044, 2019. URL https://api.semanticscholar.org/CorpusID:165163607.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Nghĩ bạn đã giải quyết việc hỏi đáp? hãy thử arc, thử thách lý luận ai2, 2018.

Marie-Catherine De Marneffe, Mandy Simons, và Judith Tonhauser. Ngân hàng cam kết: Điều tra projection trong diễn ngôn tự nhiên xảy ra. In proceedings of Sinn und Bedeutung, volume 23, pp. 107–124, 2019.

Nolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness, et al. Cerebras-gpt: Các mô hình ngôn ngữ tối ưu tính toán mở được huấn luyện trên cụm quy mô wafer cerebras. arXiv preprint arXiv:2304.03208, 2023.

11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. Đàn mô hình llama 3. arXiv preprint arXiv:2407.21783, 2024.

Kawin Ethayarajh, Yejin Choi, và Swabha Swayamdipta. Hiểu độ khó của bộ dữ liệu với thông tin V-usable. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, và Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 5988–6008. PMLR, 2022. URL https://proceedings.mlr.press/v162/ethayarajh22a.html.

Chandra Kiran Reddy Evuru, Sreyan Ghosh, Sonal Kumar, S. Ramaneswaran, Utkarsh Tyagi, và Dinesh Manocha. Coda: Tăng cường dữ liệu dựa trên tạo bị ràng buộc cho nlp tài nguyên thấp. ArXiv, abs/2404.00415, 2024. URL https://api.semanticscholar.org/CorpusID:268819699.

Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan Liang, Zhenguo Li, và Lingpeng Kong. Tạo dữ liệu không nhiễu tự hướng dẫn cho học zero-shot hiệu quả. In The Eleventh International Conference on Learning Representations.

Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, và Lingpeng Kong. Tạo dữ liệu không nhiễu tự hướng dẫn cho học zero-shot hiệu quả. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=h5OpjGd_lo6.

Himanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, Mutsumi Nakamura, Arindam Mitra, Santosh Mashetty, và Chitta Baral. Các mô hình được tinh chỉnh hướng dẫn là những người học nhanh. arXiv preprint arXiv:2306.05539, 2023.

Qi He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, và Yanghua Xiao. Các mô hình ngôn ngữ lớn có thể hiểu các hướng dẫn phức tạp thế giới thực không? ArXiv, abs/2309.09150, 2023. URL https://api.semanticscholar.org/CorpusID:262043773.

Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

Hector Levesque, Ernest Davis, và Leora Morgenstern. Thử thách lược đồ winograd. In Thirteenth international conference on the principles of knowledge representation and reasoning, 2012.

Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, và Ming Yin. Tạo dữ liệu tổng hợp với các mô hình ngôn ngữ lớn cho phân loại văn bản: Tiềm năng và hạn chế. In Houda Bouamor, Juan Pino, và Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10443–10461, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.647. URL https://aclanthology.org/2023.emnlp-main.647.

Zhuoyan Li, Hangxiao Zhu, Zhuoran Lu, và Ming Yin. Tạo dữ liệu tổng hợp với các mô hình ngôn ngữ lớn cho phân loại văn bản: Tiềm năng và hạn chế. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10443–10461, 2023b.

Alisa Liu, Swabha Swayamdipta, Noah A. Smith, và Yejin Choi. WANLI: Hợp tác công nhân và AI để tạo bộ dữ liệu suy luận ngôn ngữ tự nhiên. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 6826–6847, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.508. URL https://aclanthology.org/2022.findings-emnlp.508.

12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: Một cách tiếp cận tiền huấn luyện bert được tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692, 2019.

Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq R. Joty, Chien-Sheng Wu, Caiming Xiong, và Dragomir R. Radev. Xem xét lại tiêu chuẩn vàng: Nền tảng đánh giá tóm tắt với đánh giá con người mạnh mẽ. ArXiv, abs/2212.07981, 2022b. URL https://api.semanticscholar.org/CorpusID:254685611.

Lin Long, Rui Wang, Rui Xiao, Junbo Zhao, Xiao Ding, Gang Chen, và Haobo Wang. Về việc tạo, quản lý, và đánh giá dữ liệu tổng hợp được điều khiển bởi llms: Một khảo sát. ArXiv, abs/2406.15126, 2024. URL https://api.semanticscholar.org/CorpusID:270688337.

Ying-Cheng Lu, Huazheng Wang, và Wenqi Wei. Học máy cho việc tạo dữ liệu tổng hợp: một đánh giá. ArXiv, abs/2302.04062, 2023. URL https://api.semanticscholar.org/CorpusID:256662279.

Yu Meng, Jiaxin Huang, Yu Zhang, và Jiawei Han. Tạo dữ liệu huấn luyện với các mô hình ngôn ngữ: Hướng tới hiểu ngôn ngữ zero-shot. In NeurIPS, 2022a. URL http://papers.nips.cc/paper_files/paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc-Abstract-Conference.html.

Yu Meng, Jiaxin Huang, Yu Zhang, và Jiawei Han. Tạo dữ liệu huấn luyện với các mô hình ngôn ngữ: Hướng tới hiểu ngôn ngữ zero-shot. Advances in Neural Information Processing Systems, 35:462–477, 2022b.

Swaroop Mishra, Daniel Khashabi, Chitta Baral, và Hannaneh Hajishirzi. Tổng quát hóa tác vụ chéo thông qua các hướng dẫn crowdsourcing ngôn ngữ tự nhiên. In Annual Meeting of the Association for Computational Linguistics, 2021. URL https://api.semanticscholar.org/CorpusID:237421373.

OpenAI. Báo cáo kỹ thuật gpt-4, 2023.

Mihir Parmar, Swaroop Mishra, Mor Geva, và Chitta Baral. Đừng đổ lỗi cho người chú thích: Thiên lệch đã bắt đầu trong các hướng dẫn chú thích. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1779–1789, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.130. URL https://aclanthology.org/2023.eacl-main.130.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Tinh chỉnh hướng dẫn với gpt-4, 2023.

Mohammad Taher Pilehvar và Jose Camacho-Collados. WiC: bộ dữ liệu từ trong ngữ cảnh để đánh giá các biểu diễn nghĩa nhạy cảm ngữ cảnh. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1128. URL https://aclanthology.org/N19-1128.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Khám phá giới hạn của học chuyển giao với một biến đổi văn bản sang văn bản thống nhất. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

Melissa Roemmele, Cosmin Adrian Bejan, và Andrew S Gordon. Lựa chọn các lựa chọn thay thế hợp lý: Một đánh giá lý luận nhân quả thông thường. In 2011 AAAI Spring Symposium Series, 2011.

Rachel Rudinger, Jason Naradowsky, Brian Leonard, và Benjamin Van Durme. Thiên lệch giới tính trong giải quyết đồng tham chiếu. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 8–14, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002.

13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Kevin Scaria, Himanshu Gupta, Saurabh Arjun Sawant, Swaroop Mishra, và Chitta Baral. Instructabsa: Học hướng dẫn cho phân tích cảm xúc dựa trên khía cạnh. ArXiv, abs/2302.08624, 2023. URL https://api.semanticscholar.org/CorpusID:257020097.

Timo Schick và Hinrich Schütze. Tạo bộ dữ liệu với các mô hình ngôn ngữ được tiền huấn luyện. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6943–6951, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.555. URL https://aclanthology.org/2021.emnlp-main.555.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, và Chuang Gan. Tự sắp xếp theo nguyên tắc của các mô hình ngôn ngữ từ đầu với giám sát con người tối thiểu, 2023.

Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, và Jason Wei. Thách thức các tác vụ big-bench và liệu chain-of-thought có thể giải quyết chúng không. In Annual Meeting of the Association for Computational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID:252917648.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. ArXiv, abs/2302.13971, 2023a. URL https://api.semanticscholar.org/CorpusID:257219404.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Các mô hình nền tảng mở và chat được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023b.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Các mô hình nền tảng mở và chat được tinh chỉnh. ArXiv, abs/2307.09288, 2023c. URL https://api.semanticscholar.org/CorpusID:259950998.

Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, và Subbarao Kambhampati. Các mô hình ngôn ngữ lớn vẫn không thể lập kế hoạch (một điểm chuẩn cho llms về lập kế hoạch và lý luận về thay đổi). ArXiv, abs/2206.10498, 2022. URL https://api.semanticscholar.org/CorpusID:249889477.

Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddharth Deepak Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hanna Hajishirzi, và Daniel Khashabi. Super-naturalinstructions: Tổng quát hóa thông qua các hướng dẫn khai báo trên 1600+ tác vụ nlp. In Conference on Empirical Methods in Natural Language Processing, 2022. URL https://api.semanticscholar.org/CorpusID:253098274.

14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh Hajishirzi. Self-instruct: Sắp xếp các mô hình ngôn ngữ với các hướng dẫn tự tạo. In Anna Rogers, Jordan L. Boyd-Graber, và Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 13484–13508. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.754. URL https://doi.org/10.18653/v1/2023.acl-long.754.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. Các mô hình ngôn ngữ được tinh chỉnh là những người học zero-shot. arXiv preprint arXiv:2109.01652, 2021.

Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, và Yejin Choi. Tái khung hóa hợp tác người-AI để tạo ra các giải thích văn bản tự do. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 632–658, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.47. URL https://aclanthology.org/2022.naacl-main.47.

Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, và Lingpeng Kong. Zerogen: Học zero-shot hiệu quả thông qua tạo bộ dữ liệu. In Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 11653–11669. Association for Computational Linguistics, 2022a. doi: 10.18653/v1/2022.emnlp-main.801. URL https://doi.org/10.18653/v1/2022.emnlp-main.801.

Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, và Lingpeng Kong. ZeroGen: Học zero-shot hiệu quả thông qua tạo bộ dữ liệu. In Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11653–11669, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.801. URL https://aclanthology.org/2022.emnlp-main.801.

Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, và Lingpeng Kong. Progen: Tạo bộ dữ liệu zero-shot tiến bộ thông qua phản hồi trong ngữ cảnh. In Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 3671–3683. Association for Computational Linguistics, 2022c. doi: 10.18653/v1/2022.findings-emnlp.269. URL https://doi.org/10.18653/v1/2022.findings-emnlp.269.

Jiacheng Ye, Jiahui Gao, Zhiyong Wu, Jiangtao Feng, Tao Yu, và Lingpeng Kong. ProGen: Tạo bộ dữ liệu zero-shot tiến bộ thông qua phản hồi trong ngữ cảnh. In Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 3671–3683, Abu Dhabi, United Arab Emirates, December 2022d. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.269. URL https://aclanthology.org/2022.findings-emnlp.269.

Xi Ye và Greg Durrett. Sự không đáng tin cậy của các giải thích trong nhắc few-shot cho lý luận văn bản. Advances in neural information processing systems, 35:30378–30392, 2022.

Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, và Chao Zhang. Mô hình ngôn ngữ lớn như trình tạo dữ liệu huấn luyện được quy cho: Một câu chuyện về đa dạng và thiên lệch. CoRR, abs/2306.15895, 2023. doi: 10.48550/arXiv.2306.15895. URL https://doi.org/10.48550/arXiv.2306.15895.

Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, và Benjamin Van Durme. Record: Bắc cầu khoảng cách giữa hiểu đọc thông thường của con người và máy. arXiv preprint arXiv:1810.12885, 2018.

15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Phụ lục

A Thí nghiệm Bổ sung

A.1 Biến thể nhắc trên các tác vụ hạ nguồn khác nhau

Để hiểu tác động của các biến thể nhắc trên các tác vụ hạ nguồn khác nhau, chúng tôi đã tạo ra 2 biến thể bổ sung của nhắc Bước 3 cho bộ dữ liệu CommitmentBank. Llama-2 (7B) được tinh chỉnh trên những bộ dữ liệu tổng hợp thay thế này cùng với bộ dữ liệu tổng hợp ban đầu được tạo của chúng tôi và được đánh giá trên bộ kiểm tra gốc. Chúng tôi sử dụng các nhắc sau cho thí nghiệm này:

Biến thể 1
• Đầu vào chứa một tiền đề và một giả thuyết.
• Nếu giả thuyết tuân theo logic tiền đề và giả thuyết có thể được rút ra từ thông tin trong tiền đề, Xuất "entailment"
• Nếu giả thuyết trực tiếp mâu thuẫn với thông tin trong tiền đề, Xuất "contradiction"
• Nếu giả thuyết không liên quan đến tiền đề, hoặc không thể được chứng minh đầy đủ từ thông tin trong tiền đề, Xuất "neutral"

Biến thể 2
• Đầu vào bao gồm một tiền đề và một giả thuyết.
• Xuất "neutral" nếu giả thuyết hoặc không liên quan đến tiền đề hoặc không thể được rút ra một cách kết luận từ nó.
• Xuất "contradiction" nếu giả thuyết trực tiếp đối lập với thông tin trong tiền đề.
• Xuất "entailment" nếu giả thuyết tuân theo logic từ tiền đề và có thể được suy ra từ thông tin của nó.

Biến thể 3
• Đầu vào bao gồm một tiền đề và một giả thuyết.
• Nếu giả thuyết trực tiếp đối lập với thông tin trong tiền đề, xuất "contradiction."
• Nếu giả thuyết tuân theo logic từ tiền đề và có thể được suy ra từ thông tin của nó, xuất "entailment."
• Nếu giả thuyết không liên quan đến tiền đề hoặc thiếu bằng chứng đầy đủ từ tiền đề, xuất "neutral."

[Bảng so sánh các biến thể với độ tương tự cosine và độ chính xác]

Bảng 9: Độ chính xác của kết quả suy luận cho nhắc Bước 3 gốc và 2 biến thể. Độ tương tự cosine trung bình: Trung bình của độ tương tự cosine giữa nhắc đã cho và các biến thể khác.

Chúng tôi thấy từ kết quả trong Bảng 9 rằng có biến thiên nhẹ trong kết quả cuối cùng và độ tương tự cosine trung bình tương tự. Chúng tôi cũng muốn làm nổi bật rằng động lực đằng sau việc đề xuất một khung hướng dẫn bất khả tri tác vụ được định nghĩa rõ ràng như TarGEN là để ràng buộc việc tạo nhắc thành một loạt các nhiệm vụ phụ đơn giản hơn, được định nghĩa rõ ràng. Cách tiếp cận theo từng bước có lợi thế sau: Ở mỗi bước, nhiệm vụ phụ mà chúng tôi nhắc mô hình là đơn giản với rất ít chỗ cho việc hiểu sai và lỗi. Điều này giảm đáng kể nỗ lực kỹ thuật nhắc, đặc biệt so với chi phí con người của việc kỹ thuật một nhắc duy nhất mô hình hóa hoàn toàn và chính xác một tác vụ phức tạp. Bất kỳ nhắc nào tuân thủ các hướng dẫn cho bước cụ thể đó trong khung có khả năng tạo ra kết quả tương tự, làm cho khung này bất khả tri tác vụ và nhắc.

16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Ngoài ra - bản chất bị ràng buộc, đơn giản của nhắc tác vụ ở mỗi bước làm cho việc bất kỳ biến thể nào trong nhắc sẽ gây ra thay đổi đáng kể trong kết quả ít có khả năng hơn, cho phạm vi hẹp hơn của mục tiêu ở mỗi bước và thực tế là kết quả được dựa trên phản hồi cho bước trước đó. Hơn nữa, bất kỳ sự lệch lạc nhẹ nào có thể quy cho tính ngẫu nhiên vốn có trong việc suy luận với LLM, nhưng chúng tôi sẽ không mong đợi thấy sự khác biệt đáng kể về chất lượng hoặc đa dạng của các mẫu đầu ra.

A.2 Hiệu ứng của các thực thể hạt giống

Đối với các thực thể hạt giống, chúng tôi đã thử nghiệm để xem tác động của việc giảm số lượng ngữ cảnh ngữ nghĩa. Bảng 10 cho thấy hiệu ứng của việc tạo mẫu trên một tỷ lệ của các ngữ cảnh ngữ nghĩa gốc:

[Bảng hiệu ứng của ngữ cảnh ngữ nghĩa]

Bảng 10: Độ chính xác của các mô hình được huấn luyện trên các mẫu được tạo trên toàn bộ (100%), một nửa, và một phần tư của các ngữ cảnh ngữ nghĩa gốc

A.3 So sánh với các mô hình tạo sinh khác

Chúng tôi chạy quy trình tạo dữ liệu với Claude Sonnet và Llama-3 (70B) để tạo ra 2 biến thể nữa của SuperGLUE tổng hợp với mỗi mô hình giữ nguyên tập tác vụ, nhắc, và các bước quy trình. Llama-2 (7B) được tinh chỉnh trên các bộ dữ liệu được đề cập ở trên và được đánh giá trên bộ kiểm tra gốc. Kết quả của các thí nghiệm được đính kèm trong Bảng 11. Chúng tôi thấy điểm số gần như giống nhau hoặc cải thiện nhẹ khi sử dụng Llama-3 (70B) (Dubey et al., 2024) và Claude Sonnet (The).

[Bảng so sánh các mô hình khác nhau]

Bảng 11: Chúng tôi quan sát thấy rằng quy trình tạo dữ liệu có thể thích ứng với các mô hình tạo sinh khác nhau với sự suy giảm hiệu suất không đáng kể.

B Phân tích Mở rộng

B.1 So sánh độ khó bộ dữ liệu

Phần này chứa các biểu đồ liên quan đến độ khó bộ dữ liệu và thiên lệch cho tất cả các bộ dữ liệu.

B.2 So sánh thiên lệch bộ dữ liệu

B.3 Khả năng tổng quát của khung

Khung cơ bản được đề xuất trong công việc này là bất khả tri tác vụ và mô hình. Trong khi RTE là một tác vụ tương đối đơn giản, AXg là một tác vụ NLU phức tạp. Mặc dù có sự khác biệt trong các nhắc của những tác vụ này, khung TarGEN cơ bản mô tả các bước vẫn chung. Khung được đề xuất này có thể được thích ứng cho bất kỳ tác vụ nào, dựa trên vấn đề được đánh giá. Cho các tham số của một tác vụ - lược đồ nhãn, và tuyên bố vấn đề, khung này dễ thích ứng cho nhiều tác vụ có độ phức tạp khác nhau. Chúng tôi muốn làm nổi bật rằng bản chất đơn giản của khung của chúng tôi

17

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 6: So sánh PVI (thông tin V-usable) qua các bộ dữ liệu cho bộ dữ liệu gốc và bộ dữ liệu được tạo tổng hợp. Dữ liệu tổng hợp dường như có chất lượng tốt hơn vì PVI của các bộ dữ liệu gốc tập trung xung quanh -0.1 đến 0.1 trong khi dữ liệu tổng hợp được tạo có hỗn hợp đa dạng của mức độ khó khăn giữa các mẫu.

cho phép linh hoạt và kiểm soát lớn hơn, làm cho nó phù hợp tốt với các tác vụ đặc thù về miền hoặc đặc biệt phức tạp. Chúng tôi sẵn lòng tạo ra bất kỳ tác vụ nào khác mà người đánh giá coi là cần thiết để xác minh cách tiếp cận này.

B.4 So sánh thực nghiệm với các phương pháp khác

Để cho thấy so sánh định lượng với các phương pháp tạo dữ liệu khác, chúng tôi tạo ra các phiên bản tổng hợp của các tác vụ cụ thể từ bộ dữ liệu SuperGLUE bằng cách sử dụng các khung tạo dữ liệu gần đây: Các tác vụ chúng tôi tạo ra là RTE (suy luận văn bản) và AXg (phân biệt giới tính). Chúng tôi sau đó huấn luyện một mô hình Llama-2 trên những biến thể tác vụ tổng hợp này và đánh giá chúng trên các bộ kiểm tra gốc của các bộ dữ liệu. Kết quả có mặt trong Bảng 12.

[Bảng so sánh phương pháp]

Bảng 12: Kết quả trên RTE, Axg cho các mô hình Llama-2 được huấn luyện trên dữ liệu tổng hợp được tạo bởi các khung tạo dữ liệu hiện có.

18

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 7: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu BoolQ và bộ dữ liệu BoolQ được tạo tổng hợp.

Hình 8: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu CB và bộ dữ liệu CB được tạo tổng hợp.

B.5 Về Thông tin V-Usable

Hr(Y) − Hr(Y|X), trong đó Hr(Y) là entropy dự đoán từ một mô hình null LM Null (được huấn luyện trên các đầu vào trống), và Hr(Y|X) là entropy có điều kiện từ một mô hình LM X (được huấn luyện trên các mẫu đầu vào và nhãn). Entropy được đo bằng bit sử dụng log2(P), trong đó P là phân phối xác suất

19

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 9: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu Record và bộ dữ liệu Record được tạo tổng hợp.

Hình 10: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu RTE và bộ dữ liệu RTE được tạo tổng hợp.

của các nhãn được gán bởi mô hình cụ thể. Thông tin V-usable cao hơn cho thấy các mẫu khó hơn, trong khi các giá trị thấp hơn cho thấy các mẫu dễ hơn cho họ mô hình V. Các bộ dữ liệu gốc có thông tin V-usable tập trung trong một phạm vi hẹp của các mẫu dễ hơn. Chiến lược của chúng tôi tạo ra một phạm vi rộng hơn của thông tin V-usable, từ dễ đến khó, dẫn đến kết quả tinh chỉnh tốt hơn.

20

--- TRANG 21 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Hình 11: So sánh thiên lệch bộ dữ liệu cho bộ dữ liệu WIC và bộ dữ liệu WIC được tạo tổng hợp.

C Siêu tham số

Chúng tôi sử dụng 6xNvidia Tesla P40 GPU. Kích thước Batch: 16 cho STL và thiết lập 1 MTL. Các Bước Tích lũy Gradient: 1, Tốc độ học: 5e-5, Số Epoch: 5 cho STL và thiết lập 1 MTL.

C.1 Kết quả Bổ sung

[Bảng đa dạng từ vựng bổ sung]

Bảng 13: Đa dạng từ vựng của bộ dữ liệu. Các số màu đỏ tương ứng với nơi đa dạng của bộ dữ liệu tổng hợp bị hạn chế. Màu xanh lam tương ứng với nơi bộ dữ liệu tổng hợp có nhiều đa dạng hơn so với bộ gốc.

D Quy trình tạo thực thể đặc thù tác vụ

Phần này chi tiết chiến lược TarGEN đặc thù tác vụ được sử dụng để tạo thực thể.

D.1 COPA

Bước 1: Tạo ra một danh sách các miền hoặc thiết lập trong đó các sự kiện có thể diễn ra.

Bước 2: Tạo ra N câu mô tả các sự kiện có thể diễn ra trong miền [DOMAIN].

Bước 3: query = CAUSE. Thêm "Nguyên nhân của điều này là gì?" vào tiền đề trong quá trình xử lý hậu kỳ

Đối với câu đã cho, tạo ra 2 giả thuyết (Giả thuyết 1, Giả thuyết 2), sao cho Giả thuyết 1 là nguyên nhân có thể có của câu. Giả thuyết 2 rất khó có thể là nguyên nhân của câu.

21

--- TRANG 22 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Ví dụ:
Câu: Tôi tạo ra một bóng dài.
Giả thuyết 1: Mặt trời ở thấp trên bầu trời.
Giả thuyết 2: Cỏ cao.
Giải thích: Giả thuyết 1, vị trí thấp của mặt trời có nhiều khả năng gây ra bóng dài hơn. Chiều cao của cỏ không liên quan gì đến bóng dài, và do đó không có khả năng là nguyên nhân.
Câu: [SENTENCE]

query = RESULT. Thêm "Kết quả của điều này là gì?" vào tiền đề trong quá trình xử lý hậu kỳ

Đối với câu đã cho, tạo ra 2 giả thuyết (Giả thuyết 1, Giả thuyết 2), sao cho
Giả thuyết 1 là kết quả có thể có của câu.
Giả thuyết 2 rất khó có thể là kết quả của câu.

Ví dụ:
Câu: Tôi ngã xuống cầu thang.
Giả thuyết 1: Tôi bị thương.
Giả thuyết 2: Mẹ tôi mua một chiếc xe mới.
Giải thích: Giả thuyết 1, chấn thương có nhiều khả năng là kết quả của việc ngã. Việc mua xe không được ngụ ý bởi câu nói về việc ngã xuống cầu thang - do đó ít có khả năng là kết quả của câu.
Câu: [SENTENCE]

Xây dựng toán học của Bước 3:
Gl,t,r(l,r=result) = (P,C1,C2) : P⇒C1,P̸⇒C2 l=C1
                                  P⇒C2,P̸⇒C2 l=C2

Gl,t,r(l,r=cause) = (P,C1,C2) : C1⇒P,C2̸⇒P l=C1
                                 C2⇒P,C1̸⇒P l=C2

Mối quan hệ r ∈ {cause, result} được gắn vào tiền đề P như một truy vấn. Bộ ba được tạo (P,C1,C2) và nhãn bị ràng buộc l ∈ {C1,C2} trả lời truy vấn, tạo thành một thực thể của bộ dữ liệu này.

Tự sửa lỗi
Hướng dẫn:
Bạn được cho một tiền đề và 2 giả thuyết có thể (Lựa chọn 1 và Lựa chọn 2) làm đầu vào. Chọn giả thuyết có nhiều khả năng có liên kết nhân quả với câu.

Nếu tiền đề hỏi về NGUYÊN NHÂN: Nếu tiền đề có nhiều khả năng là kết quả của Lựa chọn 1, xuất Lựa chọn 1. Ngược lại, xuất Lựa chọn 2.

Nếu tiền đề hỏi về KẾT QUẢ: Nếu Lựa chọn 1 có nhiều khả năng là kết quả của tiền đề, xuất Lựa chọn 1. Ngược lại, xuất Lựa chọn 2.

D.2 CommitmentBank

Bước 1: Tạo ra một danh sách các miền hoặc thiết lập trong đó các sự kiện có thể diễn ra.

Bước 2: N/A

Bước 3: label = entailment
Đối với miền đã cho [DOMAIN], tạo ra [N] cặp câu (Câu 1, Câu 2) sao cho Câu 2 tuân theo logic, hoặc được ngụ ý bởi, Câu 1.

Ví dụ:
Câu 1: Ca sĩ rất lo lắng.
Câu 2: Ca sĩ thấy các nhà phê bình ở hàng ghế đầu.
Bây giờ tạo ra N cặp câu như vậy.

22

--- TRANG 23 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Các câu được tạo: label = neutral Đối với miền đã cho [DOMAIN], tạo ra [N] cặp câu (Câu 1, Câu 2) sao cho Câu 2 không có mối quan hệ với Câu 1, và không thể được rút ra từ nó.

Ví dụ:
Câu 1: Tôi pha cà phê.
Câu 2: Bài tập của tôi đến hạn vào ngày mai.
Bây giờ tạo ra N cặp câu như vậy.

Các câu được tạo: label = contradiction
Đối với miền đã cho [DOMAIN], tạo ra [N] cặp câu (Câu 1, Câu 2) sao cho Câu 2 bị mâu thuẫn rõ ràng bởi Câu 1.

Ví dụ:
Câu 1: Nhạc sĩ không đúng giờ cho buổi diễn.
Câu 2: Người tổ chức khen ngợi nhạc sĩ vì đúng giờ.
Bây giờ tạo ra N cặp câu như vậy.

Các câu được tạo:

Xây dựng toán học của Bước 3:
Gl,t(l) = (P,H) : P⇒H         l=entailment
                  P̸⇒H        l=neutral
                  P⇒¬H        l=contradiction

Mỗi cặp được tạo (P,H) của tiền đề và giả thuyết, cùng với nhãn bị ràng buộc l ∈ {entailment, neutral, contradiction}, tạo thành một thực thể tổng hợp cho bộ dữ liệu này.

Tự sửa lỗi
Hướng dẫn:
Đầu vào chứa một tiền đề và một giả thuyết. Giả sử tiền đề luôn đúng.

1. Nếu giả thuyết tuân theo logic tiền đề và giả thuyết có thể được rút ra từ thông tin trong tiền đề, Xuất "entailment"
2. Nếu giả thuyết trực tiếp mâu thuẫn với thông tin trong tiền đề, Xuất "contradiction"
3. Nếu giả thuyết không liên quan đến tiền đề, hoặc không thể được chứng minh đầy đủ từ thông tin trong tiền đề, Xuất "neutral"

D.3 MultiRC

Bước 1: Danh sách các danh mục: Tin tức*; Wikipedia*; Lịch sử và nhân chủng học*; Xã hội, luật pháp và công lý*; Sách giáo khoa khoa học tiểu học*; Báo cáo 9/11*; Tiểu thuyết - văn học hoặc cốt truyện phim*; nghệ thuật; khoa học máy tính; các quá trình sinh học; vật lý; hóa học; ngôn ngữ học; tâm thần học và tâm lý học; hiện tượng siêu nhiên (* - có mặt trong bộ dữ liệu gốc)

Bước 2: Tạo ra 50 chủ đề hoặc tiêu đề độc đáo trong danh mục [CATEGORY] Tạo ra 7 đoạn văn ngắn về chủ đề sau: [TOPIC]

Bước 3: Cho một đoạn văn, khung một câu hỏi yêu cầu thông tin từ nhiều câu của đoạn văn để được trả lời đúng. Sau đó, tạo ra một tập hợp các lựa chọn. Câu trả lời đúng có thể là sự kết hợp của một, một số, hoặc tất cả các lựa chọn. Cũng bao gồm các lựa chọn không trả lời câu hỏi trên. Cuối cùng, xuất sự kết hợp của các lựa chọn tạo thành câu trả lời đúng.

Đoạn văn: [PARAGRAPH]

Xây dựng toán học của Bước 3: Không áp dụng trong trường hợp này

23

--- TRANG 24 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Tự sửa lỗi
Hướng dẫn:
Bạn được cho một đoạn văn theo sau bởi một câu hỏi và một danh sách các lựa chọn. Dựa trên thông tin trong đoạn văn, xuất tất cả các lựa chọn có thể trả lời câu hỏi. Đầu ra không được bao gồm các lựa chọn không trả lời câu hỏi. Đầu ra không được thiếu bất kỳ lựa chọn nào trả lời câu hỏi.

D.4 RTE

Bước 1: Tạo ra một danh sách các chủ đề hoặc miền để nói về.

Bước 2: Đối với miền đã cho [DOMAIN], tạo ra N câu phức tạp chứa thông tin liên quan đến miền này.

Bước 3: label = entailment
Cho một câu làm tiền đề, thêm một giả thuyết hợp lý logic về thông tin trong tiền đề.

Tiền đề: Một nơi buồn bã sau khi Giáo hoàng John Paul II qua đời trở thành nơi ăn mừng khi các tín đồ Công giáo La Mã tụ tập tại trung tâm Chicago để đánh dấu việc bổ nhiệm Giáo hoàng mới Benedict XVI.
Giả thuyết: Benedict XVI là Giáo hoàng Công giáo mới.

Tiền đề: [SENTENCE]

label = non-entailment
Cho một câu làm tiền đề, thêm một giả thuyết không hợp lý logic về thông tin trong tiền đề.

Tiền đề: Một nơi buồn bã sau khi Giáo hoàng John Paul II qua đời trở thành nơi ăn mừng khi các tín đồ Công giáo La Mã tụ tập tại trung tâm Chicago để đánh dấu việc bổ nhiệm Giáo hoàng mới Benedict XVI.
Giả thuyết: Benedict XVI đã qua đời gần đây.

Tiền đề: [SENTENCE]

Xây dựng toán học của Bước 3:
Gl,t,p(l,p) = (P=p,H) : P⇒H    l=True
                        P̸⇒H    l=False

Các cặp tiền đề và giả thuyết được tạo (P,H), cũng như nhãn liên quan l ∈ {True,False} tạo thành các thực thể được gán nhãn nhị phân cho tác vụ này.

Hướng dẫn Tự sửa lỗi: Bạn được cho một tiền đề và một giả thuyết. Nếu giả thuyết tuân theo logic từ tiền đề, xuất entailment. Nếu giả thuyết không thể được rút ra logic từ thông tin trong tiền đề, xuất not entailment.

D.5 WSC

Bước 1: Tạo ra một danh sách các miền hoặc thiết lập trong đó các sự kiện có thể diễn ra và nơi mọi người có thể tương tác. Ví dụ: 'Một nhà hàng', 'một bảo tàng', 'một buổi hòa nhạc rock sôi động', 'Một nhà hát opera'

Bước 2: Xác định một cặp chủ đề (Chủ đề 1, Chủ đề 2) trong bối cảnh của [DOMAIN]. Ví dụ, trong bối cảnh của "một lớp học", một cặp chủ đề có thể là (Chủ đề1: giáo viên, Chủ đề 2: học sinh), hoặc (Chủ đề1: học sinh, Chủ đề2: đối thủ). Đối với miền đã cho, tạo ra [10] cặp chủ đề như vậy. Đối với 5 cặp, cả hai chủ đề phải là số ít Đối với 5 cặp, cả hai chủ đề phải là số nhiều. Cả hai chủ đề phải là con người hoặc nhóm người. Xuất tất cả 10 cặp trong một danh sách được đánh số.

Bước 3: Đối với một cặp chủ đề đã cho (Chủ đề 1, Chủ đề 2), tạo ra 2 đoạn văn, S1 và S2. Mỗi đoạn văn phải có 2 câu hoặc ít hơn. Bạn cũng được cho giới tính của cả hai chủ đề. Đối với mỗi đại từ trong các câu, xác định chủ đề nào đang được nói đến.

Ví dụ:
Đầu vào:
Chủ đề 1: Giáo viên, Chủ đề 2: Học sinh
Đại từ: He/him

24

--- TRANG 25 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Đầu ra:
S1: Giáo viên thất vọng về học sinh vì [he=teacher] có kỳ vọng cao về [him=student].
S2: Giáo viên và học sinh không có quan hệ tốt. [He=student] rất nổi loạn, và không đến lớp.
Giải thích: Trong S1: Rõ ràng rằng học sinh đã làm giáo viên thất vọng, người có kỳ vọng cao về học sinh. Do đó, "he had high hopes" - "he" nói đến giáo viên, "for him" - "him" nói đến học sinh. Trong S2: Học sinh nổi loạn và không đến lớp - do đó "he" trong "he is very rebellious" nói đến học sinh

Đầu vào:
Chủ đề 1: [SUBJECT 1] Chủ đề 2: [SUBJECT 2]
Đại từ: (được chọn ngẫu nhiên)
Đầu ra:

Xây dựng toán học của Bước 3:
Gl = (s,N1,P):N1↔P    l=True
     (s,N2,P):N2↔P    l=True
     (s,N1,P):N2↔P    l=False
     (s,N2,P):N2↔P    l=False

trong đó s chứa N1, N2, và P. Chúng tôi ký hiệu đồng tham chiếu với toán tử ↔.

Hướng dẫn Tự sửa lỗi: Đầu vào đã cho nói về 2 cụm danh từ (Chủ đề 1 và Chủ đề 2). Bạn được cho Chủ đề 1 và một Đại từ xuất hiện trong đầu vào.

Quy tắc:
1. Nếu đại từ nói đến cụm danh từ Chủ đề 1, đầu ra tác vụ là TRUE.
2. Nếu đại từ nói đến cụm danh từ Chủ đề 2, đầu ra tác vụ là FALSE.

D.6 BoolQ

Bước 1: Tạo ra một danh sách các miền để viết một bài báo.

Bước 2: Tạo ra 50 chủ đề hoặc tiêu đề độc đáo trong danh mục [DOMAIN] Tạo ra 7 đoạn văn ngắn về chủ đề sau: [TOPIC]

Bước 3: label = TRUE
Bạn được cho một đoạn văn. Tạo ra một truy vấn boolean. Câu trả lời cho truy vấn này, dựa trên đoạn văn, phải là YES.

Ví dụ:
Đoạn văn: Millennium Falcon, một con tàu huyền thoại được điều khiển bởi Han Solo và Chewbacca, đã trở thành biểu tượng của sự nổi loạn và hy vọng trong cuộc đấu tranh chống lại Đế chế Thiên hà áp bức. Cầu mong Lực lượng ở cùng bạn, khi những cuộc phiêu lưu epic của Luke Skywalker, Công chúa Leia, và Darth Vader nhắc nhở chúng ta rằng ngay cả trong những thời khắc đen tối nhất, luôn có một tia sáng hy vọng và cơ hội cho sự cứu chuộc trong vũ trụ Star Wars.

Truy vấn: Han Solo có làm việc với Chewbacca không? Trả lời: YES, Han Solo và Chewbacca cùng điều khiển Falcon.

Tương tự, tạo ra một truy vấn cho đoạn văn sau.
Đoạn văn: [PASSAGE]
Truy vấn:

label = FALSE
Bạn được cho một đoạn văn. Tạo ra một truy vấn boolean. Câu trả lời cho truy vấn này, dựa trên đoạn văn, phải là NO.

Ví dụ:
Đoạn văn: Millennium Falcon, một con tàu huyền thoại được điều khiển bởi Han Solo và Chewbacca, đã trở thành biểu tượng của sự nổi loạn và hy vọng trong cuộc đấu tranh chống lại Đế chế Thiên hà áp bức. Cầu mong Lực lượng ở cùng bạn, khi những cuộc phiêu lưu epic của Luke Skywalker, Công chúa Leia, và Darth Vader nhắc nhở chúng ta rằng ngay cả trong những thời khắc đen tối nhất, luôn có một tia sáng hy vọng và cơ hội cho sự cứu chuộc trong vũ trụ Star Wars.

Truy vấn: Han Solo có làm việc một mình không? Trả lời: NO, Han Solo làm việc với Chewbacca

Tương tự, tạo ra một truy vấn cho đoạn văn sau.
Đoạn văn: [PASSAGE]
Truy vấn:

25

--- TRANG 26 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Xây dựng toán học của Bước 3:
Gl,t,p(l,p) = (P=p,Q) : P⇒Q     l=Yes
                        P⇒¬Q    l=No

Tự sửa lỗi
Hướng dẫn: Bạn được cho một đoạn văn theo sau bởi một câu hỏi. Các câu hỏi yêu cầu xác nhận một sự thật có thể có hoặc không có mặt trong đoạn văn. Nếu đoạn văn xác nhận rõ ràng sự thật được hỏi trong câu hỏi, xuất TRUE làm câu trả lời cho câu hỏi. Nếu đoạn văn không cung cấp thông tin nào xác nhận rõ ràng sự thật, và sự thật không có cơ sở logic hoặc bằng chứng mạnh trong đoạn văn, xuất FALSE làm câu trả lời cho câu hỏi.

D.7 ReCoRD

Bước 1: Tạo ra một danh sách các miền để viết một bài báo.

Bước 2: Tạo ra 50 chủ đề hoặc tiêu đề độc đáo trong danh mục [DOMAIN] Tạo ra 7 đoạn văn ngắn về chủ đề sau: [TOPIC]

Bước 3: Ví dụ:
Khi gió xuân thổi vào chiến hào của mình, chỉ huy Pháp Georges Lamour thấy một thứ gì đó siêu thực trôi về phía mình - một đám mây màu vàng-xanh. 'Tất cả các chiến hào của tôi đều bị nghẹt,' anh ta hét vào điện thoại trường với trụ sở chỉ huy. 'Tôi cũng đang ngã!' Khí chlorine — được gió thuận lợi mang qua Flanders Fields từ các vị trí của Đức — đã được sử dụng lần đầu tiên. Đó là ngày 22 tháng 4 năm 1915. Cuộn xuống để xem video Khí chlorine — được gió thuận lợi mang qua Flanders Fields từ các vị trí của Đức — đã gieo ra nỗi kinh hoàng và đau khổ lần đầu tiên vào ngày 22 tháng 4 năm 1915. Phía trên, các nhân viên Hội Chữ thập đỏ Đức mang chai nước để giúp hồi sức các binh lính. Lực lượng Đức phát động cuộc tấn công đầu tiên bằng khí độc vào ngày 22 tháng 4 năm 1915. 150.000 tấn khí độc đã được sử dụng bởi lực lượng Đức và Đồng minh trong WWI.

Truy vấn: Nếu họ có thể nhìn xa hơn một chút qua vùng đất không người thì họ sẽ thấy [X] binh đã đào hào, dưới sự che phủ của màn đêm, hơn 5.000 bình khí với ống hướng về phía họ.
Trả lời: Đức

Giải thích: Truy vấn phù hợp với bối cảnh của WWI và nói về cả hai thực thể tham gia vào cuộc chiến. Đoạn văn đề cập đến lực lượng Đức sử dụng khí độc. Đức là thực thể được thay thế bằng [X] trong truy vấn.

Hướng dẫn: Tạo ra một câu phức tạp phù hợp với bối cảnh của đoạn văn đã cho.
Truy vấn được tạo phải là một tuyên bố về các sự kiện trong đoạn văn. Đặt [X] vào chỗ của bất kỳ một đề cập thực thể nào. Truy vấn không được chứa bất kỳ sự kiện nào được đề cập trong đoạn văn. Câu trả lời phải chứa đề cập thực thể có thể thay thế [X].

Truy vấn:

Xây dựng toán học của Bước 3:
Ga,t(a) = (A=a,s,e) : s,e ∈ a, l=e

trong đó a là bài báo, và s,e là câu được diễn đạt lại (với một thực thể e bị che khuất) cũng như thực thể e bị che khuất hoạt động như nhãn.

Hướng dẫn Tự sửa lỗi: Bạn được cho một đoạn văn, theo sau bởi một truy vấn. Truy vấn chứa [X] vào chỗ của bất kỳ một thực thể nào được đề cập. Xuất thực thể có thể thay thế [X] trong truy vấn một cách logic.

26

--- TRANG 27 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

D.8 AXg

Bước 1: Tạo ra một danh sách các miền hoặc thiết lập trong đó các sự kiện có thể diễn ra và nơi mọi người có thể tương tác. Ví dụ: 'Một nhà hàng', 'một bảo tàng', 'một buổi hòa nhạc rock sôi động', 'Một nhà hát opera'

Bước 2: Xác định một cặp chủ đề (Chủ đề 1, Chủ đề 2) trong bối cảnh của [DOMAIN]. Ví dụ, trong bối cảnh của "một lớp học", một cặp chủ đề có thể là (Chủ đề1: giáo viên, Chủ đề 2: học sinh), hoặc (Chủ đề1: học sinh, Chủ đề2: đối thủ). Đối với miền đã cho, tạo ra [10] cặp chủ đề như vậy.

Đối với một cặp chủ đề đã cho, Tạo ra 4 câu với các đặc điểm sau: Mệnh đề ban đầu: Một mệnh đề chứa Chủ đề 1 và Chủ đề 2 Câu 1: Mệnh đề ban đầu với một mệnh đề phụ thuộc chứa một đại từ có giới tính. Mệnh đề phụ thuộc nên nói đến Chủ đề 1, không phải Chủ đề 2. Câu 2: Hoàn toàn giống hệt câu 1 nhưng với một đại từ có giới tính khác Câu 3: Mệnh đề ban đầu với một mệnh đề phụ thuộc chứa một đại từ có giới tính. Mệnh đề phụ thuộc nên nói đến Chủ đề 2, không phải Chủ đề 1.

Câu 4: Giống hệt câu 3 nhưng với một đại từ có giới tính khác Đảm bảo các mệnh đề phụ thuộc trong Câu 1 và Câu 2 nói đến Chủ đề 1 và Chủ đề 2 tương ứng. Câu 1 và 2 phải giống hệt nhau về mặt phụ thuộc, không có gì thay đổi ngoài đại từ. Tương tự, Câu 3 và 4 phải giống hệt nhau, chỉ có đại từ thay đổi. Câu 1 đến 4 phải bắt đầu bằng cùng một mệnh đề.

Chỉ một đại từ có giới tính phải có mặt trong các mệnh đề phụ thuộc. Tất cả các câu nên có nghĩa logic. Bây giờ tạo ra 4 câu theo những đặc điểm này cho các chủ đề sau.

Chủ đề 1: [SUBJECT 1]
Chủ đề 2: [SUBJECT 2]

Bước 3:
label = entailment Cho một câu, (mệnh đề độc lập + mệnh đề phụ thuộc), và chủ đề được nói đến trong mệnh đề phụ thuộc của nó: tạo ra một câu chứa chủ đề, tuân theo logic câu. Câu được tạo này không nên có đại từ có giới tính. label = non-entailment
Cho một câu, (mệnh đề độc lập + mệnh đề phụ thuộc), và chủ đề được nói đến trong mệnh đề phụ thuộc của nó: tạo ra một câu chứa chủ đề, không tuân theo logic câu. Câu được tạo này không nên có đại từ có giới tính.

Xây dựng toán học của Bước 3:
Gl,t(s,i,d) = (i,d,h) ⇒(s,d)⇒h          l=entailment
                      (s,d)̸⇒h          l=notentailment

trong đó i và d nói đến các mệnh đề độc lập và phụ thuộc tương ứng, s là chủ đề mà mệnh đề phụ thuộc đồng tham chiếu với, và h là giả thuyết trung lập giới tính được tạo dựa trên chủ đề s và nhãn thực thể l ∈ {entailment, notentailment}.

Hướng dẫn Tự sửa lỗi: Bạn được cho một tiền đề và một giả thuyết. Nếu giả thuyết tuân theo logic từ tiền đề, xuất entailment. Nếu giả thuyết không thể được rút ra logic từ thông tin trong tiền đề, xuất not entailment.

D.9 WiC

Bước 1: N/A

Bước 2: Tạo ra một danh sách 50 động từ hoặc danh từ có nhiều hơn một nghĩa. Cho một từ [WORD], in một danh sách được đánh số gồm 4 hoặc ít hơn các định nghĩa khác biệt của từ đó. Ví dụ:

Từ: shoot
Định nghĩa: 1. bắn đạn 2. chụp ảnh 3. quay video 4. một bối cảnh phim.

27

--- TRANG 28 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Từ: [WORD]
Định nghĩa:

Bước 3: label = TRUE
Đối với từ đã cho và danh sách tất cả các định nghĩa có thể, in bất kỳ một định nghĩa nào, theo sau bởi 2 câu chứa từ trong định nghĩa này.

Ví dụ:
Từ: key
Định nghĩa:
1. một mảnh kim loại có hình dạng được sử dụng để mở hoặc đóng ổ khóa 2. một nút hoặc cần gạt trên bàn phím hoặc nhạc cụ 3. một yếu tố then chốt hoặc trung tâm 4. cung cấp cho cái gì đó một khóa hoặc mã nhận dạng

Định nghĩa được chọn: 1. một mảnh kim loại có hình dạng được sử dụng để mở hoặc đóng ổ khóa
Câu:
1. Tôi mất chìa khóa hôm qua 2. Anh ta không nên lấy trộm chìa khóa của người khác.

Từ: [WORD]
Định nghĩa: [DEFINITIONS]
Định nghĩa được chọn:

label = FALSE
Cho một từ và một danh sách định nghĩa: Đối với mỗi định nghĩa, in một câu chứa từ trong định nghĩa đó.

Ví dụ:
Từ: key
Định nghĩa:
1. một mảnh kim loại có hình dạng được sử dụng để mở hoặc đóng ổ khóa 2. một nút hoặc cần gạt trên bàn phím hoặc nhạc cụ 3. một yếu tố then chốt hoặc trung tâm 4. cung cấp cho cái gì đó một khóa hoặc mã nhận dạng

Câu:
1. Tôi mất chìa khóa hôm qua
2. Phím này trên piano bị lệch.
3. Chìa khóa của chiến thắng là lập kế hoạch trước.
4. Tôi không biết nên nhập gì để có quyền truy cập.

Giải thích:
1. Tôi mất [chìa khóa] hôm qua - ở đây [chìa khóa] có nghĩa là 1. một mảnh kim loại có hình dạng được sử dụng để mở hoặc đóng ổ khóa
2. [Phím] này trên piano bị lệch. - ở đây [phím] có nghĩa là 2. một nút hoặc cần gạt trên bàn phím hoặc nhạc cụ
3. [Chìa khóa] của chiến thắng là lập kế hoạch trước. - ở đây [chìa khóa] có nghĩa là 3. một yếu tố then chốt hoặc trung tâm
4. Tôi không biết nên [nhập] gì để có quyền truy cập. - ở đây [nhập] có nghĩa là 4. cung cấp cho cái gì đó một khóa hoặc mã nhận dạng

Từ: [WORD]
Định nghĩa: [DEFINITIONS]
Câu:

Xây dựng toán học của Bước 3:
Gl,t(l) = (d1,d2) ⇒f(s,d1)=f(s,d2)     l=True
                    f(s,d1)̸=f(s,d2)     l=False

trong đó s ∈ S và f(s,d1) = m ∈ Ms là nghĩa từ của s trong ngữ cảnh của d1.

Hướng dẫn Tự sửa lỗi: Bạn được cho một từ (Từ khóa) và 2 câu, cả hai đều chứa Từ khóa.

1. Nếu định nghĩa của Từ khóa trong cả hai câu gần như giống nhau, in TRUE
2. Nếu Từ khóa có nghĩa khác trong câu 1 so với câu 2, in FALSE.

28

--- TRANG 29 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

E Nhắc cho tinh chỉnh hướng dẫn

CommitmentBank Đầu vào chứa một tiền đề và một giả thuyết.

1. Nếu giả thuyết tuân theo logic tiền đề và giả thuyết có thể được rút ra từ thông tin trong tiền đề, Xuất "entailment"
2. Nếu giả thuyết trực tiếp mâu thuẫn với thông tin trong tiền đề, Xuất "contradiction"
3. Nếu giả thuyết không liên quan đến tiền đề, hoặc không thể được chứng minh đầy đủ từ thông tin trong tiền đề, Xuất "neutral"

Ví dụ:
Tiền đề: "Không có thức ăn trong tủ, chúng ta không bao giờ đi mua sắm! Chúng ta có thể chết đói!", cô ấy khóc.
Giả thuyết: Cô ấy không đi mua sắm để mua thức ăn.
Đầu ra: entailment
Giải thích: Người nói đang phàn nàn về việc thiếu thức ăn do không đi mua sắm. Giả thuyết được hỗ trợ mạnh mẽ bởi thông tin trong tiền đề. Do đó đầu ra là entailment, tức là tiền đề kéo theo giả thuyết.

Tiền đề: Hội nghị thượng đỉnh ngoại giao là một thất bại. Sự thù địch giữa các đại diện đã dẫn đến sự đổ vỡ trong các cuộc đàm phán. Thế giới đã chứng kiến họ trao đổi những lời chế giễu và kết thúc cuộc họp trong mối quan hệ xấu.
Giả thuyết: Một giải pháp ngoại giao đã được đạt được thông qua đàm phán.
Đầu ra: contradiction
Giải thích: Tiền đề liên quan đến sự thất bại của các cuộc đàm phán ngoại giao, và ngụ ý thiếu đàm phán, mâu thuẫn với giả thuyết. Do đó, đầu ra là contradiction.

Tiền đề: "Tôi hy vọng con mèo khỏe mạnh," tôi nói. Tôi đã nói dối, vì tôi muốn con mèo chết.
Giả thuyết: Con mèo đã chết.
Đầu ra: neutral
Giải thích: Tiền đề đề cập đến việc người nói muốn con mèo chết, nhưng nói họ hy vọng nó sống. Từ tiền đề, không thể rút ra kết luận nào về tình trạng thực tế của con mèo. Vì tiền đề không thể cung cấp bằng chứng để xác nhận hoặc phủ nhận giả thuyết, đầu ra là neutral.

Tiền đề: [PREMISE]
Giả thuyết:

COPA Bạn được cho một tiền đề và 2 giả thuyết có thể (Lựa chọn 1 và Lựa chọn 2) làm đầu vào. Chọn giả thuyết có nhiều khả năng có liên kết nhân quả với câu.

Nếu tiền đề hỏi về NGUYÊN NHÂN:
Nếu tiền đề có nhiều khả năng là kết quả của Lựa chọn 1, xuất Lựa chọn 1. Ngược lại, xuất Lựa chọn 2.

Nếu tiền đề hỏi về KẾT QUẢ:
Nếu Lựa chọn 1 có nhiều khả năng là kết quả của tiền đề, xuất Lựa chọn 1. Ngược lại, xuất Lựa chọn 2.

Ví dụ:
Tiền đề: Cơ thể tôi tạo ra một cái bóng trên cỏ. Nguyên nhân của điều này là gì?
Lựa chọn 1: Mặt trời đang mọc.
Lựa chọn 2: Cỏ được cắt.
Đầu ra: Lựa chọn 1
Giải thích: Tiền đề hỏi về NGUYÊN NHÂN. Trong 2 lựa chọn, lựa chọn 1 mô tả vị trí của mặt trời có nhiều khả năng gây ra bóng. Việc cắt cỏ không có mối quan hệ.

Ví dụ:
Tiền đề: Người phụ nữ lớn tuổi bị đột quỵ. Điều gì đã xảy ra như một KẾT QUẢ?
Lựa chọn 1: Con gái của người phụ nữ đến dọn dẹp nhà cô ấy.
Lựa chọn 2: Con gái của người phụ nữ chuyển đến để chăm sóc cô ấy.
Đầu ra: Lựa chọn 2
Giải thích: Tiền đề hỏi về KẾT QUẢ. Lựa chọn 2 có nhiều khả năng tuân theo logic tiền đề của người phụ nữ lớn tuổi bị ốm. Không có gì gợi ý việc dọn dẹp nhà.

29

--- TRANG 30 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Tiền đề: [PREMISE]
Lựa chọn 1: [CHOICE 1]
Lựa chọn 2: [CHOICE 2]

MultiRC Bạn được cho một đoạn văn theo sau bởi một câu hỏi và một danh sách các lựa chọn. Dựa trên thông tin trong đoạn văn, xuất tất cả các lựa chọn có thể trả lời câu hỏi.

Ví dụ:
Đoạn văn:
Phát quang sinh học là một hiện tượng tự nhiên quyến rũ chiếu sáng độ sâu của đại dương và các môi trường trên cạn khác nhau. Đó là khả năng mê hoặc của các sinh vật sống tạo ra ánh sáng thông qua phản ứng hóa học trong cơ thể chúng. Sự phát xạ ánh sáng đầy mê hoặc này xảy ra chủ yếu ở các sinh vật biển như phù du phát sáng, sứa, và các sinh vật đáy biển sâu, biến thế giới đại dương thành một chương trình ánh sáng rực rỡ. Ánh sáng mê hoặc phục vụ một loạt mục đích, từ thu hút con mồi và bạn tình đến xua đuổi kẻ săn mồi. Phát quang sinh học không chỉ thêm một nét kỳ diệu cho những vương quốc ẩn giấu của Trái Đất mà còn là một lĩnh vực nghiên cứu khoa học quan trọng, cung cấp cái nhìn sâu sắc về các thích ứng tiến hóa và các ứng dụng y sinh tiềm năng. Khám phá những bí ẩn của phát quang sinh học tiếp tục hé lộ những bí mật của những sinh vật phát sáng này và thêm fuel cho sự tò mò của chúng ta về những kỳ quan của thế giới tự nhiên.

Câu hỏi: Những sinh vật nào thể hiện phát quang sinh học?
Lựa chọn:
A) Phù du phát sáng
B) Các sinh vật đáy biển sâu
C) Môi trường trên cạn
D) Sứa
E) Các sinh vật trên cạn

Đầu ra: A) Phù du phát sáng
B) Các sinh vật đáy biển sâu
D) Sứa

Đoạn văn:
[PARAGRAPH]

Câu hỏi: [QUERY]

Lựa chọn:
[LIST OF OPTIONS]

Đầu ra:

RTE Bạn được cho một tiền đề và một giả thuyết. Nếu giả thuyết tuân theo logic từ tiền đề, xuất entailment. Nếu giả thuyết không thể được rút ra logic từ thông tin trong tiền đề, xuất not entailment.

Tiền đề: Xe của tôi hết dầu diesel và tôi phải đi bộ 6 dặm về nhà.
Giả thuyết: Xe của tôi cần dầu diesel để chạy.
Đầu ra: entailment

Tiền đề: Các cuộc đàm phán ngoại giao đánh dấu sự giảm bớt dần dần căng thẳng giữa Anh và Argentina.
Giả thuyết: Argentina và Anh đang hướng tới một cuộc chiến tranh.
Đầu ra: not entailment

Tiền đề: [PREMISE]
Giả thuyết: [HYPOTHESIS]
Đầu ra:

WiC Bạn được cho một từ (Từ khóa) và 2 câu, cả hai đều chứa Từ khóa.

1. Nếu định nghĩa của Từ khóa trong cả hai câu gần như giống nhau, in TRUE
2. Nếu Từ khóa có nghĩa khác trong câu 1 so với câu 2, in FALSE.

30

--- TRANG 31 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Ví dụ:
Từ: shoot
Câu 1: anh ấy quay đám cưới bằng camera cầm tay
Câu 2: anh ấy bắn tôi bằng súng Đầu ra:
FALSE

Ví dụ:
Từ: shoot
Câu 1: việc quay phim bị tạm dừng do sự vắng mặt của diễn viên
Câu 2: đạo diễn kết thúc việc quay phim vào buổi tối. Đầu ra:
TRUE

Từ: [WORD]
Câu 1: [SENTENCE 1]
Câu 2: [SENTENCE 2]
Đầu ra:

WSC Đầu vào đã cho nói về 2 cụm danh từ (Chủ đề 1 và Chủ đề 2). Bạn được cho Chủ đề 1 và một Đại từ xuất hiện trong đầu vào.

Quy tắc:
1. Nếu đại từ nói đến cụm danh từ Chủ đề 1, xuất LABEL: TRUE.
2. Nếu đại từ nói đến cụm danh từ Chủ đề 2, xuất LABEL: FALSE.

Giải thích lý do của bạn.

Ví dụ:
Đầu vào: Các thành viên hội đồng thành phố từ chối cấp phép cho người biểu tình vì họ sợ bạo lực. Chủ đề 1: Các thành viên hội đồng thành phố. Đại từ: Họ.

Đầu ra: TRUE
Giải thích:
Cụm danh từ: thành viên hội đồng thành phố (Chủ đề 1); người biểu tình (Chủ đề 2)
Đại từ "họ" xuất hiện trong cụm từ "họ sợ bạo lực". Trong 2 chủ đề, người biểu tình có nhiều khả năng gây ra bạo lực, và thành viên hội đồng thành phố có nhiều khả năng sợ bạo lực. Cụm từ này phải nói về các thành viên hội đồng thành phố (Chủ đề 1). Điều này tuân theo Quy tắc 1.

Ví dụ:
Đầu vào: Nhà khoa học nghiên cứu con sư tử vì anh ấy được trả tiền để làm việc đó. Chủ đề 1: sư tử. Đại từ: anh ấy

Đầu ra: FALSE
Giải thích:
Cụm danh từ: sư tử (Chủ đề 1); nhà khoa học (Chủ đề 2)
Đại từ "anh ấy" xuất hiện trong cụm từ "anh ấy được trả tiền để làm việc đó". Điều này không có khả năng nói đến con sư tử, vì nhà khoa học (Chủ đề 2) có nhiều khả năng được trả tiền để nghiên cứu cái gì đó hơn. Do đó, đại từ phải nói đến Chủ đề 2.

Đầu vào:
[INPUT]
Đầu ra:

BoolQ Bạn được cho một đoạn văn theo sau bởi một câu hỏi. Các câu hỏi yêu cầu xác nhận một sự thật có thể có hoặc không có mặt trong đoạn văn. Nếu đoạn văn xác nhận rõ ràng sự thật được hỏi trong câu hỏi, xuất TRUE làm câu trả lời cho câu hỏi. Nếu đoạn văn không cung cấp thông tin nào xác nhận rõ ràng sự thật, và sự thật không có cơ sở logic hoặc bằng chứng mạnh trong đoạn văn, xuất FALSE làm câu trả lời cho câu hỏi.

Đoạn văn:
Trong Middle-earth của J. R. R. Tolkien, Half-elven (Sindarin số ít Peredhel, số nhiều Peredhil, Quenya số ít Perelda) là con cái của sự kết hợp giữa Elf và Men. Trong số này, quan trọng nhất là sản phẩm của sự kết hợp giữa Eldar (những Elf đã tuân theo Lời gọi đến Valinor) và Edain (những Men của Ba Nhà của những Men đầu tiên đã liên minh với Eldar trong cuộc chiến chống lại Morgoth).

Câu hỏi:
elf và human có thể giao phối trong lord of the rings không Trả lời: TRUE
Giải thích: Sự thật cần xác nhận là liệu elf và human có thể giao phối không, trong bối cảnh của Lord of the Rings, bởi J R R Tolkien. Đoạn văn rõ ràng đề cập rằng trong vũ trụ của ông, con cái của elf và human hoặc men tồn tại, do đó chứng minh rằng elf và human có thể giao phối trong vũ trụ này. Do đó, câu trả lời cho câu hỏi là TRUE

31

--- TRANG 32 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Đoạn văn:
Qua những năm, Movie Maker đã trải qua nhiều cập nhật khác nhau đã thêm cải tiến cho phần mềm. Nó thậm chí còn tích hợp với các công cụ khác để cung cấp mức độ chỉnh sửa tiên tiến hơn. Tuy nhiên, nó vẫn là một ứng dụng chỉnh sửa video cơ bản. Nếu bạn đang tìm cách tạo ra một bộ phim trông chuyên nghiệp hơn, các ứng dụng như Adobe Premiere Pro là lựa chọn tốt nhất của bạn.

Câu hỏi:
Ứng dụng movie maker có được ngành công nghiệp điện ảnh sử dụng không

Trả lời: FALSE Giải thích: Câu hỏi yêu cầu xác nhận sự thật liệu ứng dụng movie maker có được ngành công nghiệp điện ảnh sử dụng không. Đoạn văn rõ ràng nói rằng movie maker là một ứng dụng chỉnh sửa video cơ bản. Hơn nữa, đoạn văn đề cập Adobe Premiere Pro cụ thể như một công cụ chuyên nghiệp. Vì bất kỳ ngành công nghiệp nào đều sử dụng công cụ chuyên nghiệp, đoạn văn rõ ràng mâu thuẫn với sự thật được trình bày trong câu hỏi. Do đó, câu trả lời là FALSE.

Đoạn văn: [PARAGRAPH]

Câu hỏi:
[QUERY]

Trả lời:

ReCoRD Bạn được cho một đoạn văn, theo sau bởi một truy vấn. Truy vấn chứa [X] vào chỗ của bất kỳ một thực thể nào được đề cập. Xuất thực thể có thể thay thế [X] trong truy vấn một cách logic.

Ví dụ:
Đoạn văn: Georges Lamour thấy một thứ gì đó trôi về phía mình - một đám mây màu vàng-xanh. 'Tất cả các chiến hào của tôi đều bị nghẹt,' anh ta hét lên. Khí chlorine được gió mang qua Flanders Fields từ các vị trí của Đức. Khí chlorine đã gieo ra nỗi kinh hoàng và đau khổ lần đầu tiên vào ngày 22 tháng 4 năm 1915. Các nhân viên Hội Chữ thập đỏ mang chai nước. Lực lượng Đức phát động cuộc tấn công đầu tiên bằng khí độc vào ngày 22 tháng 4 năm 1915. 150.000 tấn khí độc đã được sử dụng bởi lực lượng Đức và Đồng minh trong WWI.

Truy vấn: Nếu họ có thể nhìn xa hơn một chút, họ sẽ thấy [X] binh đã đào hào, dưới sự che phủ của màn đêm, hơn 5.000 bình khí với ống hướng về phía họ.

Đầu ra:
[X]: Đức

Đoạn văn:
Truy vấn: [QUERY]
Đầu ra:
[X]:

AXg Bạn được cho một tiền đề và một giả thuyết. Nếu giả thuyết tuân theo logic từ tiền đề, xuất entailment. Nếu giả thuyết không thể được rút ra logic từ thông tin trong tiền đề, xuất not entailment.

Ví dụ:
Đầu vào:
Tiền đề: Trợ lý pháp lý quên nộp giấy tờ của khách hàng, vì vậy anh ta bị sa thải.
Giả thuyết: Khách hàng bị sa thải.
Đầu ra: not entailment
Giải thích: Trợ lý pháp lý đã mắc lỗi ở đây, và khách hàng không có khả năng bị sa thải. Do đó, giả thuyết không có ý nghĩa logic. Do đó, xuất "non-entailment".

Ví dụ:
Đầu vào:

32

--- TRANG 33 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

Tiền đề: Vận động viên đã lấy trộm đồng phục của cổ động viên để cô ấy có thể mặc nó đến một bữa tiệc.
Giả thuyết: Vận động viên sẽ đến một bữa tiệc.
Đầu ra: entailment
Giải thích: Mặc dù có định kiến giới tính, "cô ấy" ở đây nói đến vận động viên, không phải cổ động viên. Theo tiền đề, vận động viên đã lấy trộm đồng phục để vận động viên có thể mặc nó đến một bữa tiệc, không phải cổ động viên, người không còn trang phục nữa. Giả thuyết rằng vận động viên được chứng minh mạnh mẽ bởi tiền đề. Do đó, xuất "entailment".

Đầu vào:
Tiền đề: [PREMISE]
Giả thuyết: [HYPOTHESIS]
Đầu ra:

F Khung cho meta-nhắc để tự sửa lỗi

Đây là "hướng dẫn tác vụ" bạn được cho để hoàn thành một tác vụ:
chèn hướng dẫn tác vụ ở đây.

Nhiệm vụ của bạn là đánh giá liệu, dựa trên những hướng dẫn này và một đầu vào, đầu ra có đúng hay sai. Cũng cung cấp giải thích cho lý do của bạn.

chèn các mẫu tự sửa lỗi đặc thù tác vụ ở đây.

Bây giờ đánh giá đầu vào và đầu ra dưới đây dựa trên hướng dẫn tác vụ, và in liệu đầu ra có đúng hay sai. Nhớ cung cấp giải thích cho đánh giá của bạn. Nhớ rằng, câu trả lời thực tế dựa trên đầu vào và hướng dẫn tác vụ, không phải đầu ra.

Đầu vào:
Đầu ra:
Đánh giá:

Hướng dẫn đặc thù tác vụ có thể được tìm thấy trong §E. Các ví dụ tự sửa lỗi đặc thù tác vụ tuân theo lược đồ sau:

Đối với đầu ra đúng
Kết quả thực tế: đầu ra đúng
Đầu ra: đầu ra đúng
Dựa trên đầu vào này và hướng dẫn tác vụ đã cho, đầu ra là ĐÚNG.
Giải thích cho Kết quả thực tế: giải thích về mối quan hệ đầu vào và đầu ra dựa trên hướng dẫn tác vụ.
Kết quả thực tế: đầu ra đúng
Đầu ra: đầu ra đúng
Kết quả thực tế khớp với đầu ra, vì vậy đầu ra là ĐÚNG.

Đối với đầu ra sai
Kết quả thực tế: đầu ra đúng
Đầu ra: đầu ra sai
Dựa trên đầu vào này và hướng dẫn tác vụ đã cho, đầu ra là SAI.
Giải thích cho Kết quả thực tế: giải thích về mối quan hệ đầu vào và đầu ra dựa trên hướng dẫn tác vụ. Tùy chọn, điều này có thể bao gồm giải thích tại sao đầu ra dự đoán là sai.
Kết quả thực tế: đầu ra đúng
Đầu ra: đầu ra sai
Kết quả thực tế không khớp với đầu ra, vì vậy đầu ra là SAI.

G Thiết kế và Đánh giá của quá trình kỹ thuật nhắc

Kỹ thuật nhắc là một quá trình lặp đi lặp lại, với chuỗi hành động sau:

33

--- TRANG 34 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

• Một nhắc đơn giản chỉ bao gồm định nghĩa tác vụ, tức là cách giải quyết tác vụ. Ví dụ: "Tạo ra một câu khó phân biệt đại từ."

• Xác định các đặc điểm ngôn ngữ đặc trưng trong các thế hệ mong muốn. Chúng tôi xác định rằng có những đặc điểm chung nhất định trong cấu trúc câu và các yếu tố ngữ pháp cần phải là một phần của thực thể cho một tác vụ cụ thể. Ví dụ: Chúng tôi xác định rằng đối với tác vụ phân biệt đại từ, câu phải chứa nhiều chủ đề và ít nhất một đại từ.

• Xác định những thách thức ngôn ngữ độc đáo được kiểm tra bởi các thực thể tác vụ. Bước 2 và 3 yêu cầu phân tích chi tiết và nghiên cứu các tác vụ đang được mô phỏng. Trong những lần lặp này, chúng tôi đã sử dụng một tập đánh giá gồm 50 ví dụ được chọn kỹ lưỡng từ đơn giản đến cực kỳ phức tạp mà chúng tôi tận dụng để tinh chỉnh các nhắc hướng dẫn.

• Sau đó chúng tôi xác định các lỗi phổ biến và những hiểu lầm có thể có và tinh chỉnh thêm các nhắc để làm rõ hướng dẫn. Trong bước này, chúng tôi chọn các ví dụ phù hợp bao trùm toàn bộ lược đồ nhãn cho tác vụ

• Cuối cùng, chúng tôi tinh chỉnh các hướng dẫn để đơn giản, gọn gàng, và không mơ hồ.

Tập đánh giá bao gồm 50 ví dụ cho mỗi tác vụ được sử dụng trong bước 2 và 3 để tinh chỉnh hướng dẫn thêm. Đánh giá này được tiến hành một cách tùy hứng, do các ràng buộc chi phí ngăn cản việc tạo và đánh giá quy mô lớn được hỗ trợ bởi LLM.

H Đánh giá Con người

Như một phần của đánh giá chất lượng, một đánh giá con người có hệ thống đã được thực hiện trên tất cả các bộ dữ liệu được tạo với khả năng tốt nhất của chúng tôi. Xem xét quy mô lớn của các bộ dữ liệu được tạo như WiC (4843 mẫu) và BoolQ (4299 mẫu), và để đảm bảo bao phủ đầy đủ, tức là đánh giá toàn diện tất cả các tác vụ, chúng tôi thực hiện đánh giá con người trên một tỷ lệ của những bộ dữ liệu này. Tổng thể, ~2700 mẫu (~20% của tất cả dữ liệu được tạo) đã được đánh giá. Đánh giá được tiến hành bởi 6 tác giả tổng cộng, được chia thành 3 nhóm gồm 2 đánh giá viên mỗi nhóm. Mỗi nhóm được giao một phần bằng nhau của mỗi bộ dữ liệu để đánh giá. Để ngăn chặn thiên lệch, tác giả hướng dẫn không là một phần của các nhóm đánh giá. Đối với mỗi thực thể được chấm điểm, đánh giá viên được cung cấp hướng dẫn Bước 3 của tác vụ tạo ra thực thể (tức là một xây dựng mô tả của hàm tạo nghịch đảo sử dụng hạt giống thực thể), thực thể được tạo, ràng buộc nhãn, và nhãn được tự sửa lỗi. Các thực thể được tạo được chấm điểm trên thang điểm từ A (xuất sắc) đến D (kém). Để duy trì sự nhất quán trong chấm điểm và loại bỏ bất kỳ chủ quan nào, một bảng chấm điểm đã được xây dựng tập thể với các hướng dẫn đánh giá sau.

[Bảng kích thước và tỷ lệ bộ dữ liệu]

Bảng 14: Kết quả sử dụng mô hình T5-3B được huấn luyện theo cách đa tác vụ.

Các đánh giá viên chấm điểm các thực thể được tạo trong ngữ cảnh của ràng buộc nhãn gốc, cũng như trong ngữ cảnh của nhãn được cập nhật sau tự sửa lỗi. Phân phối nhãn như sau:

Chúng tôi nhận thấy rằng khi các thực thể trải qua tự sửa lỗi, số lượng mẫu ban đầu được chấm điểm B (được gán cho các thực thể mà không thì đúng nhưng không phù hợp với ràng buộc nhãn) giảm và số lượng mẫu được chấm điểm A tăng, chỉ ra thành công của mô-đun tự sửa lỗi. Tổng số mẫu "không thể chấp nhận" (C, D) cũng giảm.

Để đo độ tin cậy của cách tiếp cận đánh giá con người của chúng tôi, chúng tôi đo sự đồng ý giữa các đánh giá viên bằng cách sử dụng các metric sau:

34

--- TRANG 35 ---
Xuất bản như một bài báo hội nghị tại COLM 2024

[Bảng đặc điểm chấm điểm]

Bảng 15: Kết quả sử dụng mô hình T5-3B được huấn luyện theo cách đa tác vụ.

[Bảng kết quả trước và sau tự sửa lỗi]

Bảng 16: Kết quả sử dụng mô hình T5-3B được huấn luyện theo cách đa tác vụ.

• Cohen's Kappa (K), mà chúng tôi tận dụng bằng cách xử lý các lớp chữ cái của chúng tôi như nhãn phân loại. Chúng tôi quan sát các giá trị K là 0.51, 0.48, 0.60 cho mỗi nhóm đánh giá viên, cho thấy sự đồng ý từ trung bình đến đáng kể trong tất cả các nhóm. Chúng tôi tiếp tục đơn giản hóa phản hồi của đánh giá viên thành "có thể chấp nhận" (A, B) và "không thể chấp nhận" (C, D) và tính toán lại K, dẫn đến các giá trị K là 0.71, 0.67, 0.79, cho thấy sự đồng ý đáng kể. Điều này chỉ ra rằng nhìn chung, các đánh giá viên đồng ý về những thực thể nào tạo thành ví dụ "tốt", và những thực thể nào thì không.

• Chúng tôi tiếp tục phân tích sự đồng ý của đánh giá viên bằng cách tính hệ số tương quan Spearman (r) cho mỗi nhóm trên các lớp được gán bởi đánh giá viên, mà chúng tôi xử lý như một biến thứ tự (A > B > C > D về giá trị). Chúng tôi tính 3 giá trị của hệ số này, một cho mỗi nhóm đánh giá viên. Các giá trị thu được là r = 0.85, 0.83, 0.91 - cho thấy sự đồng ý mạnh trong tất cả các nhóm đánh giá viên.

Cũng được phát hiện rằng không có thực thể nào của bộ kiểm tra SuperGLUE trong bộ huấn luyện tổng hợp.

35
