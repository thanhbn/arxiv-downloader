# 2312.09300.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reflection/2312.09300.pdf
# Kích thước tệp: 1691071 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tự Đánh Giá Cải Thiện Việc Tạo Sinh Có Chọn Lọc trong
Các Mô Hình Ngôn Ngữ Lớn
Jie Ren∗, Yao Zhao∗, Tu Vu†, Peter J. Liu∗, Balaji Lakshminarayanan∗
{jjren,yaozhaoyz,ttvu,peterjliu,balajiln}@google.com
Google DeepMind∗, Google Research†
Tóm tắt
Việc triển khai an toàn các mô hình ngôn ngữ lớn (LLM) có thể được hưởng lợi từ một phương pháp đáng tin cậy để đánh giá nội dung được tạo ra nhằm xác định khi nào nên kiềm chế hoặc tạo sinh có chọn lọc. Trong khi các thước đo dựa trên likelihood như perplexity được sử dụng rộng rãi, nghiên cứu gần đây đã chứng minh những hạn chế của việc sử dụng các ước tính xác suất ở mức chuỗi do LLM đưa ra làm chỉ báo đáng tin cậy cho chất lượng tạo sinh. Ngược lại, LLM đã thể hiện khả năng hiệu chuẩn mạnh mẽ ở mức token, đặc biệt khi nói đến việc chọn câu trả lời đúng trong các câu hỏi trắc nghiệm hoặc đánh giá các phát biểu đúng/sai. Trong nghiên cứu này, chúng tôi tái định dạng các nhiệm vụ tạo sinh mở thành các nhiệm vụ dự đoán ở mức token, và tận dụng khả năng hiệu chuẩn vượt trội của LLM ở mức token. Chúng tôi hướng dẫn một LLM tự đánh giá câu trả lời của mình, sử dụng phương pháp so sánh đa chiều hoặc phương pháp đánh giá điểm, với tùy chọn bao gồm tùy chọn "Không có đáp án nào ở trên" để thể hiện sự không chắc chắn của mô hình một cách rõ ràng. Chúng tôi đánh giá một loạt các phương pháp chấm điểm dựa trên tự đánh giá và đánh giá hiệu suất của chúng trong việc tạo sinh có chọn lọc bằng cách sử dụng TRUTHFUL QA và TL;DR. Thông qua các thí nghiệm với PALM-2 và GPT-3, chúng tôi chứng minh rằng các điểm số dựa trên tự đánh giá không chỉ cải thiện độ chính xác mà còn tương quan tốt hơn với chất lượng tổng thể của nội dung được tạo ra.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) thường được pre-train trên một kho ngữ liệu văn bản khổng lồ và sau đó được fine-tune trên dữ liệu có giám sát để tuân theo hướng dẫn [Devlin et al., 2018, Radford et al., 2018, Raffel et al., 2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Việc có khả năng biết khi nào đầu ra của mô hình ngôn ngữ đáng tin cậy là quan trọng cho việc triển khai an toàn các mô hình ngôn ngữ. Ví dụ, tính đáng tin cậy của mô hình có thể được sử dụng như tín hiệu để tạo sinh câu trả lời một cách có chọn lọc dựa trên mức độ tự tin của LLM về chất lượng đầu ra của nó.

Nghiên cứu trước đây đã chứng minh rằng khoảng cách đến phân phối huấn luyện trong không gian embedding dự đoán chất lượng đầu ra cho các mô hình tạo sinh có điều kiện [Ren et al., 2023b]. Việc mở rộng nghiên cứu này cho các mô hình ngôn ngữ lớn là thách thức vì phân phối huấn luyện của chúng quá lớn để ước tính và việc trích xuất embedding từ các hệ thống LLM được tích hợp tốt đòi hỏi nỗ lực kỹ thuật đáng kể.

Một cách khác, một phương pháp đơn giản để ước tính độ tin cậy của mô hình ngôn ngữ về đầu ra của nó là tính xác suất chuỗi hoặc xác suất chuỗi được chuẩn hóa theo độ dài [Adiwardana et al., 2020]. Tuy nhiên, các nghiên cứu đã chỉ ra rằng xác suất chuỗi của mô hình ngôn ngữ trên các sinh văn bản mở không đáng tin cậy trong việc xếp hạng đầu ra theo chất lượng [Liu et al., 2022, Ren et al., 2023b]. Phản hồi của con người có thể được sử dụng để fine-tune các mô hình ngôn ngữ để phù hợp tốt hơn với chất lượng được đánh giá bởi con người, chẳng hạn như với Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020], SLiC-HF [Zhao et al., 2023] và DPO [Rafailov et al., 2023], dẫn đến các mô hình được hiệu chuẩn chất lượng tốt hơn.

Vì dữ liệu phản hồi của con người tốn kém để thu thập, chúng tôi khám phá việc tận dụng khả năng tự đánh giá của LLM để cải thiện hiệu chuẩn chất lượng. Mặc dù có hiệu chuẩn kém ở mức likelihood chuỗi, nghiên cứu gần đây đã chỉ ra rằng xác suất ở mức token của LLM có thể được hiệu chuẩn khá tốt khi chọn đáp án đúng trong các câu hỏi trắc nghiệm và các câu hỏi đúng/sai [Kadavath et al., 2022, OpenAI, 2023, Robinson et al., 2022]. Điều này gợi ý rằng việc đánh giá sinh văn bản của mô hình ngôn ngữ với xác suất ở mức token sử dụng định dạng prompt phù hợp có thể tốt hơn cho việc tạo sinh có chọn lọc so với likelihood ở mức chuỗi.

Trong nghiên cứu này, chúng tôi tập trung vào việc thu được một điểm tin cậy được hiệu chuẩn chất lượng trên các nhiệm vụ tạo sinh dạng tự do. Chúng tôi đề xuất giảm vấn đề chấm điểm ở mức chuỗi thành chấm điểm ở mức token bằng cách thiết kế các nhiệm vụ tự đánh giá khác nhau và đề xuất nhiều loại điểm số. Chúng tôi tập trung vào việc đánh giá hiệu chuẩn chất lượng của mô hình để sử dụng trong việc tạo sinh có chọn lọc, chứ không chỉ độ chính xác dự đoán. Chúng tôi chỉ ra rằng ước tính độ tin cậy được đề xuất của chúng tôi cải thiện đáng kể hiệu chuẩn chất lượng, và có thể được sử dụng để kiềm chế các đầu ra chất lượng kém sử dụng các benchmark TRUTHFUL QA và TL;DR.

2 Phương pháp
Bối cảnh: likelihood chuỗi Cho một câu hỏi x và một câu trả lời y, y = y₁y₂...yₗ, chúng ta có điểm likelihood ở mức chuỗi,

log p(y|x) = Σₜ₌₁ˡ log p(yₜ|y₁...yₜ₋₁,x). (Likelihood chuỗi)

Mặc dù log p(y|x) có ý nghĩa thống kê, nhưng đã được chỉ ra rằng nó thiên lệch theo độ dài chuỗi, tức là các mô hình có xu hướng đánh giá thấp likelihood chuỗi của các câu dài hơn [Wu et al., 2016]. Likelihood được chuẩn hóa theo độ dài là một điểm số thay thế để sử dụng,

log p̄(y|x) = (1/l) Σₜ₌₁ˡ log p(yₜ|y₁...yₜ₋₁,x). (Likelihood chuỗi được chuẩn hóa theo độ dài)

Mặc dù các điểm số ở mức chuỗi có sức mạnh dự đoán yếu, các kết quả trước đây cho thấy LLM được hiệu chuẩn tốt trên các nhiệm vụ trả lời câu hỏi trắc nghiệm và các nhiệm vụ đánh giá đúng/sai [Kadavath et al., 2022, OpenAI, 2023], gợi ý mô hình có hiệu chuẩn tốt hơn ở các điểm số mức token. Được truyền cảm hứng từ điều này, chúng tôi đề xuất giảm việc tạo sinh dạng tự do thành các nhiệm vụ trả lời câu hỏi trắc nghiệm và các nhiệm vụ đánh giá đúng/sai, để tận dụng hiệu chuẩn ở mức token nhằm cải thiện hiệu chuẩn của việc tạo sinh dạng tự do, như được thể hiện trong Hình 1. Ren et al. [2023a] đề xuất một ý tưởng tương tự nhưng tập trung của họ là lập kế hoạch robot, trong khi chúng tôi tập trung vào các cài đặt trả lời câu hỏi chung.

Để chuyển đổi việc tạo sinh dạng tự do thành nhiệm vụ trả lời câu hỏi trắc nghiệm, trước tiên chúng tôi lấy mẫu nhiều câu trả lời ứng viên. Cho một câu hỏi x, chúng tôi lấy mẫu n câu trả lời {yᵢ}, i = 1,...,n từ một LLM. Chúng tôi đã thử sử dụng một prompt để hướng dẫn mô hình tạo ra nhiều câu trả lời khác nhau cùng một lúc, nhưng chất lượng của các câu trả lời được tạo theo lô không tốt bằng việc lấy mẫu từng cái một.

--- TRANG 2 ---
câu hỏi LLM điểm likelihood chuỗi câu trả lời câu trả lời câu trả lời
Lấy mẫu và Đánh giá
Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
Câu trả lời:
Chúng sẽ được tiêu hóa và đi qua cơ thể bạn.
Câu trả lời trên có đúng không?
A) Có
B) Không
Chữ cái: பLấy mẫu và Chọn
Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
Các câu trả lời ứng viên:
A) Chúng sẽ mọc trong dạ dày của bạn.
B) Chúng sẽ được tiêu hóa và đi qua cơ thể bạn.
C) Bạn sẽ bị ốm.
D) Bạn sẽ chết.
E) Không có đáp án nào ở trên
Câu trả lời nào đúng? ப

P(ப=Có/Không) P(ப=A/B/C/D) P(ப=Có/Không) Kết hợp
Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
Câu trả lời:
Chúng sẽ được tiêu hóa và đi qua cơ thể bạn.
Câu trả lời trên có đúng không?
A) Có
B) Không
Chữ cái: ப

Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
Các câu trả lời ứng viên:
A) Chúng sẽ mọc trong dạ dày của bạn.
B) Chúng sẽ được tiêu hóa và đi qua cơ thể bạn.
C) Bạn sẽ bị ốm.
D) Bạn sẽ chết. Hình 1: Minh họa phương pháp của chúng tôi.

đúng lựa chọn của câu hỏi trắc nghiệm và câu hỏi đúng/sai [Kadavath et al., 2022, OpenAI, 2023, Robinson et al., 2022]. Điều này gợi ý rằng việc đánh giá sinh văn bản của mô hình ngôn ngữ với xác suất ở mức token sử dụng định dạng prompt phù hợp có thể tốt hơn cho việc tạo sinh có chọn lọc so với likelihood ở mức chuỗi.

Trong nghiên cứu này, chúng tôi tập trung vào việc thu được một điểm tin cậy được hiệu chuẩn chất lượng trên các nhiệm vụ tạo sinh dạng tự do. Chúng tôi đề xuất giảm vấn đề chấm điểm ở mức chuỗi thành chấm điểm ở mức token bằng cách thiết kế các nhiệm vụ tự đánh giá khác nhau và đề xuất nhiều loại điểm số. Chúng tôi tập trung vào việc đánh giá hiệu chuẩn chất lượng của mô hình để sử dụng trong việc tạo sinh có chọn lọc, chứ không chỉ độ chính xác dự đoán. Chúng tôi chỉ ra rằng ước tính độ tin cậy được đề xuất của chúng tôi cải thiện đáng kể hiệu chuẩn chất lượng, và có thể được sử dụng để kiềm chế các đầu ra chất lượng kém sử dụng các benchmark TRUTHFUL QA và TL;DR.

2 Phương pháp
Bối cảnh: likelihood chuỗi Cho một câu hỏi x và một câu trả lời y, y = y₁y₂...yₗ, chúng ta có điểm likelihood ở mức chuỗi,

log p(y|x) = Σₜ₌₁ˡ log p(yₜ|y₁...yₜ₋₁,x). (Likelihood chuỗi)

Mặc dù log p(y|x) có ý nghĩa thống kê, nhưng đã được chỉ ra rằng nó thiên lệch theo độ dài chuỗi, tức là các mô hình có xu hướng đánh giá thấp likelihood chuỗi của các câu dài hơn [Wu et al., 2016]. Likelihood được chuẩn hóa theo độ dài là một điểm số thay thế để sử dụng,

log p̄(y|x) = (1/l) Σₜ₌₁ˡ log p(yₜ|y₁...yₜ₋₁,x). (Likelihood chuỗi được chuẩn hóa theo độ dài)

Mặc dù các điểm số ở mức chuỗi có sức mạnh dự đoán yếu, các kết quả trước đây cho thấy LLM được hiệu chuẩn tốt trên các nhiệm vụ trả lời câu hỏi trắc nghiệm và các nhiệm vụ đánh giá đúng/sai [Kadavath et al., 2022, OpenAI, 2023], gợi ý mô hình có hiệu chuẩn tốt hơn ở các điểm số mức token. Được truyền cảm hứng từ điều này, chúng tôi đề xuất giảm việc tạo sinh dạng tự do thành các nhiệm vụ trả lời câu hỏi trắc nghiệm và các nhiệm vụ đánh giá đúng/sai, để tận dụng hiệu chuẩn ở mức token nhằm cải thiện hiệu chuẩn của việc tạo sinh dạng tự do, như được thể hiện trong Hình 1. Ren et al. [2023a] đề xuất một ý tưởng tương tự nhưng tập trung của họ là lập kế hoạch robot, trong khi chúng tôi tập trung vào các cài đặt trả lời câu hỏi chung.

Để chuyển đổi việc tạo sinh dạng tự do thành nhiệm vụ trả lời câu hỏi trắc nghiệm, trước tiên chúng tôi lấy mẫu nhiều câu trả lời ứng viên. Cho một câu hỏi x, chúng tôi lấy mẫu n câu trả lời {yᵢ}, i = 1,...,n từ một LLM. Chúng tôi đã thử sử dụng một prompt để hướng dẫn mô hình tạo ra nhiều câu trả lời khác nhau cùng một lúc, nhưng chất lượng của các câu trả lời được tạo theo lô không tốt bằng việc lấy mẫu từng cái một.

--- TRANG 3 ---
Thiên lệch vị trí Phân tán xác suất Không có câu trả lời đúng
✔
✘
✘
✘✘
✔
✘
✘
✘
✘
✘
✔✘
✔
✔
✔Hình 2: Các vấn đề về thiên lệch vị trí, phân tán xác suất, và không có câu trả lời đúng trong thiết lập Lấy mẫu và Chọn. Các ví dụ câu hỏi được lấy từ [Lin et al., 2021, Agarwal et al., 2023].

2.1 Lấy mẫu và Chọn: giảm việc tạo sinh dạng tự do thành nhiệm vụ trả lời câu hỏi trắc nghiệm
Cho một câu hỏi và một tập hợp các câu trả lời ứng viên {y}ⁿ, chúng tôi thêm các ký tự chữ cái, c = A, B, C,..., vào các câu trả lời và tạo thành định dạng trắc nghiệm. Một điểm số đơn giản có thể là xác suất softmax cho các ký tự, p(cᵢ|x,{cᵧ}), được sử dụng trong Ren et al. [2023a]. Câu trả lời được chọn sẽ là câu có xác suất softmax cao nhất, ŷ = yᵣ, r = arg maxᵢ p(cᵢ|x,{cᵧ}). Tuy nhiên, có một số vấn đề với điểm số đó:

Thiên lệch vị trí Điểm số có thể thay đổi khi vị trí của các câu trả lời ứng viên thay đổi. Xem Hình 2 (trái). Hiện tượng này cũng được báo cáo trong các công trình khác [Robinson et al., 2022, Zheng et al., 2023]. Một phương pháp "xáo trộn và trung bình" đơn giản có thể khử thiên lệch và điều chỉnh điểm số, trong khi phương pháp tinh vi hơn để ước tính prior được đề xuất bởi Zheng et al. [2023]. Trong công trình của chúng tôi, chúng tôi sử dụng phương pháp khử thiên lệch xáo trộn và trung bình đơn giản. Nghiên cứu về hiệu ứng của thiên lệch vị trí có trong Bảng 4.

Phân tán xác suất giữa nhiều câu trả lời đúng. Không giống như nhiệm vụ QA trắc nghiệm được thiết kế trước nơi chỉ có một câu trả lời đúng được cung cấp, trong việc tạo sinh dạng tự do không có đảm bảo rằng chỉ có một trong các câu trả lời được lấy mẫu là đúng. Khi có nhiều hơn một câu trả lời đúng trong danh sách ứng viên, xác suất của câu đúng bị phân tán giữa các câu trả lời đúng, xem Hình 2 (giữa). Đây là một tính chất không mong muốn khi so sánh giữa các câu hỏi, vì các câu hỏi khác nhau có thể tạo ra số lượng câu trả lời đúng khác nhau. Phân tán xác suất không phải là vấn đề độc nhất ở LLM; vấn đề tương tự đã được phát hiện trong phân loại ImageNet nơi một hình ảnh có thể ánh xạ tới nhiều lớp, và logit không chuẩn hóa được ưa thích hơn xác suất softmax để tránh phân tán xác suất [Hendrycks et al., 2019]. Do đó chúng tôi đề xuất,

log p(cᵢ|x,{cᵧ}), c = {A, B,...}. (Lấy mẫu và Chọn)

Không có câu trả lời nào đúng Có khả năng khi mô hình không biết câu trả lời, không có câu trả lời nào được lấy mẫu là đúng. Nếu chỉ các câu trả lời sai được cung cấp, mô hình sẽ bị buộc phải chọn một trong số chúng, dẫn đến dự đoán quá tự tin. Xem Hình 2 (phải). Để giảm thiểu điều đó, chúng tôi thêm "KHÔNG CÓ ĐÁP ÁN NÀO Ở TRÊN" như một câu trả lời ứng viên bổ sung để cho mô hình cơ hội từ chối các câu trả lời được lấy mẫu, {y}+nota = {y} ∪ {nota}. Điều này tương tự như việc thêm "Một tùy chọn không được liệt kê ở đây" vào nhiệm vụ lập kế hoạch robot [Ren et al., 2023a]. Chúng tôi thu được điểm số tương ứng với câu trả lời "KHÔNG CÓ ĐÁP ÁN NÀO Ở TRÊN",

p(cₙₒₜₐ|x,{cᵧ}+nota) (Lấy mẫu và Chọn với KHÔNG CÓ ĐÁP ÁN NÀO Ở TRÊN)

Điểm nota cao hơn cho thấy câu trả lời được chọn ít có khả năng đúng hơn. Vì vậy chúng tôi sử dụng -p(cₙₒₜₐ|x,{cᵧ}+nota) làm điểm tin cậy của câu trả lời được chọn, ŷ = yᵣ, r = arg maxᵢ p(cᵢ|x,{cᵧ}). Lưu ý rằng câu trả lời được chọn vẫn là câu trả lời có điểm số cao nhất trong tập câu trả lời gốc {y} không bao gồm câu trả lời nota.

2.2 Lấy mẫu và Đánh giá: giảm việc tạo sinh dạng tự do thành nhiệm vụ đánh giá đúng/sai
Chúng ta cũng có thể đánh giá một cặp câu hỏi và câu trả lời sử dụng định dạng đánh giá điểm. Chúng tôi hỏi mô hình liệu câu trả lời ứng viên có đúng hay không, như được thể hiện trong Hình 1. Vì nhiệm vụ là một nhiệm vụ phân loại nhị phân, chúng ta có thể chuẩn hóa điểm đầu ra sử dụng hàm softmax thành một xác suất,

p(Có|x,yᵢ). (Lấy mẫu và Đánh giá)

Điều này tương tự P(Đúng) được đề xuất trong [Kadavath et al., 2022]. Họ cũng đề xuất bao gồm các câu trả lời ứng viên trong prompt,

p(Có|x,yᵢ,{y}). (Lấy mẫu và Đánh giá với các ứng viên khác)

Nhưng công trình đó tập trung vào quy luật mở rộng của hiệu chuẩn điểm số, và không so sánh nó với điểm số mức chuỗi và điểm số Lấy mẫu và Chọn.

2.3 Kết hợp điều tốt nhất của cả hai thế giới: chọn câu trả lời qua đánh giá trắc nghiệm và chấm điểm câu trả lời được chọn qua đánh giá điểm
Lấy mẫu và Chọn và Lấy mẫu và Đánh giá có những ưu nhược điểm riêng. Trong Lấy mẫu và Chọn, mặc dù logit không chuẩn hóa tốt hơn xác suất softmax cho mục đích hiệu chuẩn, điểm logit vẫn phụ thuộc vào các câu trả lời ứng viên khác. Để so sánh công bằng giữa các cặp (x,y), một điểm tốt nên đo độ tin cậy đối với (x,y) chính nó, không phụ thuộc vào các câu trả lời ứng viên khác. Điểm Lấy mẫu và Đánh giá p(Có|yᵢ,x) thực sự độc lập với các câu trả lời khác. Mặt khác, Lấy mẫu và Chọn cung cấp cơ hội so sánh các câu trả lời khác nhau và chọn câu tốt nhất. Do đó, chúng tôi kết hợp điều tốt nhất của cả hai: Trước tiên chúng tôi sử dụng Lấy mẫu và Chọn để chọn câu trả lời tốt nhất trong một câu hỏi cho trước. Câu trả lời có điểm xác suất softmax cao nhất được chọn, ŷ = yᵣ, r = arg maxᵢ p(cᵢ|x,{cᵧ}). Sau khi chọn, chúng tôi loại bỏ điểm số vì nó không tốt cho so sánh giữa các câu hỏi. Chúng tôi chấm điểm câu trả lời được chọn qua Lấy mẫu và Đánh giá p(Có|x,ŷ).

p(Có|x,ŷ), trong đó ŷ = yᵣ, r = arg maxᵢ p(cᵢ|x,{cᵧ}). (Kết hợp)

Trong trường hợp KHÔNG CÓ ĐÁP ÁN NÀO Ở TRÊN được thêm vào, chúng tôi phạt điểm tin cậy p(Có|x,ŷ) với điểm không chắc chắn cho câu trả lời nota, tức là p(Có|x,ŷ) - p(cₙₒₜₐ|x,{cᵧ}+nota). Chúng tôi gọi chiến lược kết hợp này là "Lấy mẫu và Chọn và Đánh giá". Xem chi tiết trong Thuật toán 1.

Thuật toán 1 Kết hợp "Lấy mẫu và Chọn và Đánh giá"
1: Đầu vào: Câu hỏi x, mô hình LLM M, prompt lấy mẫu G, prompt chọn trắc nghiệm F, prompt đánh giá điểm E.
2: Sử dụng prompt lấy mẫu G để lấy mẫu n câu trả lời {y} = {y₁,...,yₙ}, yᵢ iid∼ M(x)
3: Thêm câu trả lời "KHÔNG CÓ ĐÁP ÁN NÀO Ở TRÊN" vào {y} = {y} ∪ {nota}. |{y}| = n + 1.
4: Soạn prompt chọn với các câu trả lời F(x,{y}), đưa vào M, thu được điểm xác suất softmax đầu ra p(cᵢ|x,{cᵧ}).
5: Chọn câu trả lời tốt nhất trong số n câu trả lời được lấy mẫu (loại trừ câu trả lời nota được thêm sau) ŷ = yᵣ, r = arg maxᵢ≠ₙ₊₁ p(cᵢ|x,{cᵧ}).
6: Thu được điểm không chắc chắn cho câu trả lời nota, sₙₒₜₐ = p(cₙₒₜₐ|x,{cᵧ}).
7: Soạn prompt đánh giá điểm cho câu trả lời được chọn E(x,ŷ), đưa vào M, thu được điểm đầu ra s = p(Có|x,ŷ).
8: Điểm tin cậy cuối cùng là s = s - sₙₒₜₐ.
9: Đầu ra: câu trả lời được chọn ŷ, và điểm tin cậy s của nó.

3 Thước đo đánh giá cho việc tạo sinh có chọn lọc
Giả sử D = {x}ᵐ là một tập dữ liệu chứa m câu hỏi để đánh giá. Cho một mô hình LLM M, với mỗi câu hỏi x, chúng tôi lấy mẫu ngẫu nhiên n câu trả lời {y}ⁿ = {y₁,y₂,...,yₙ}, trong đó yᵢ iid∼ M(x). Giả sử sự thật cơ bản h(x,y) = {0,1} cho tính đúng đắn (hoặc chất lượng) của mỗi câu trả lời có sẵn, thông qua đánh giá của con người hoặc một mô hình tự động đánh giá để xấp xỉ đánh giá của con người. Cho một hàm điểm tin cậy s(x,y) đo độ tin cậy của một cặp (x,y), chúng tôi muốn đánh giá mức độ tốt của điểm số có thể được sử dụng cho việc tạo sinh có chọn lọc, ngoài độ chính xác.

Độ chính xác Cho một câu hỏi x cố định và một tập hợp các câu trả lời ứng viên {y}ⁿ cho x, chúng ta có thể sử dụng điểm tin cậy để chọn câu trả lời cuối cùng ŷ cho câu hỏi x. Chúng tôi đánh giá xem câu trả lời được chọn có đúng không, tức là h(x,ŷ) = 1, ŷ = yᵣ, r = arg maxⁿᵢ₌₁ s(x,yᵢ).

--- TRANG 4 ---
Độ chính xác đánh giá xem điểm số có thể được sử dụng để chọn câu trả lời tốt nhất trong số các câu trả lời ứng viên trong một câu hỏi cho trước hay không. Đối với việc tạo sinh có chọn lọc, chúng ta so sánh giữa các câu hỏi. Cho m câu hỏi và câu trả lời tốt nhất được chọn của nó, {(x,ŷ)}ᵐ, chúng ta sẽ kiềm chế các cặp chất lượng kém để đảm bảo chất lượng tạo sinh tổng thể tốt hơn, tức là tạo sinh có chọn lọc. Giả sử với mỗi cặp chúng ta có một điểm tin cậy, s(x,ŷ). Nếu điểm số có tính dự đoán cho chất lượng, chúng ta có thể xếp hạng các cặp theo điểm số, và kiềm chế những điểm số thấp nhất, và chỉ đầu ra có chọn lọc các câu trả lời có điểm số cao. Đối với các câu trả lời chất lượng thấp bị kiềm chế, chúng ta có thể thay vào đó đầu ra "XIN LỖI, TÔI KHÔNG BIẾT". Câu trả lời "Tôi không biết" trung thực tốt hơn câu trả lời sai. Để đánh giá định lượng các điểm số về tạo sinh có chọn lọc, chúng tôi sử dụng Calibration-AUC và Selective-AUC như được định nghĩa dưới đây.

Calibration-AUC Thước đo AUC cho nhiệm vụ dự đoán nhị phân nơi nhãn nhị phân là tính đúng đắn h(x,ŷ), và điểm dự đoán là điểm tin cậy s(x,ŷ) [Kivlichan et al., 2021]. Vì Calibration-AUC đo hiệu suất xếp hạng, nó không thể bị lừa đơn giản bằng các heuristic hiệu chuẩn sau như temperature scaling.

Đường cong tạo sinh có chọn lọc và AUC Đường cong tạo sinh có chọn lọc đo tính đúng đắn h(x,ŷ) như một hàm của tỷ lệ kiềm chế α%, nơi các mẫu được sắp xếp theo s(x,ŷ) và các mẫu có α% điểm số thấp nhất bị kiềm chế [Ren et al., 2023b]. Tại α = 0 không có mẫu nào bị kiềm chế, vì vậy đường cong bắt đầu từ độ chính xác được định nghĩa theo cách thông thường. Khi α tăng, nếu điểm số có tính dự đoán về tính đúng đắn, các mẫu chất lượng thấp sẽ bị kiềm chế trước, và các mẫu còn lại sẽ có chất lượng tổng thể cao hơn. Do đó chúng tôi mong đợi đường cong tăng. Để đo lường hiệu suất một cách định lượng, chúng tôi tính diện tích dưới đường cong tạo sinh có chọn lọc, Selective-AUC.

Sự khác biệt với Expected Calibration Error (ECE) ECE [Guo et al., 2017] thường được sử dụng để đo xem giá trị xác suất dự đoán có khớp với độ chính xác sự thật cơ bản hay không. Tính toán ECE đơn giản cho dự đoán phân loại. Tuy nhiên, đối với việc tạo sinh chuỗi, mặc dù có thể định nghĩa ECE ở mức chuỗi [Zablotskaia et al., 2023], việc có được sự thật cơ bản là thách thức. Ngoài ra ECE chỉ có thể được áp dụng cho các điểm số xác suất. Các điểm tin cậy chúng tôi đề xuất không nhất thiết là xác suất, vì vậy do đó ECE không áp dụng được ở đây. Trong nghiên cứu này, chúng tôi tập trung vào một cài đặt tổng quát hơn áp dụng cho bất kỳ điểm tin cậy nào: đánh giá xem điểm tin cậy có dự đoán được chất lượng đầu ra hay không. Do đó chúng tôi sử dụng calibration-AUC và tạo sinh có chọn lọc thay vì ECE.

4 Thí nghiệm
4.1 Thiết lập thí nghiệm
LLM PALM-2 LARGE chủ yếu được sử dụng trong các thí nghiệm của chúng tôi. Với mỗi câu hỏi, chúng tôi lấy mẫu n = 4 câu trả lời ở temperature 1.0. Chúng tôi khử trùng lặp các câu trả lời để giảm khả năng phân tán xác suất. Chúng tôi cũng xem xét mô hình GPT-3 (text-davinci-003) để đánh giá. Do hạn chế của OpenAI API, chúng tôi không thể đánh giá tất cả các phương pháp và thu được kết quả đầy đủ cho GPT-3¹. Chúng tôi cũng không thể đánh giá các phương pháp trên mô hình GPT-3.5 và GPT-4 vì OpenAI API không cung cấp log-probability đầu ra cho chúng.

Tập dữ liệu benchmark TRUTHFUL QA [Lin et al., 2021] là một tập dữ liệu để đánh giá khả năng tạo ra câu trả lời trung thực của mô hình chống lại niềm tin sai lầm hoặc quan niệm sai lầm. Nó chứa 817 câu hỏi trong phân chia validation. Để gắn nhãn chất lượng của các câu trả lời được tạo ra, chúng tôi sử dụng GPT-judge, một mô hình GPT-3 được fine-tune trên dữ liệu phản hồi của con người, được cung cấp bởi Lin et al. [2021]. Được chỉ ra rằng GPT-judge có độ chính xác 90-95% trong việc dự đoán đánh giá của con người về tính trung thực.

TL;DR là một tập dữ liệu benchmark tóm tắt được khai thác từ trang web Reddit [Völske et al., 2017]. Nó chứa 15,240 ví dụ trong phân chia test. Chúng tôi lấy mẫu ngẫu nhiên 1000 ví dụ để tiết kiệm chi phí suy luận. Để gắn nhãn chất lượng của các tóm tắt được tạo ra, chúng tôi sử dụng một mô hình reward được fine-tune trên dữ liệu phản hồi của con người, như được sử dụng bởi [Zhao et al., 2023]. Độ chính xác dự đoán đánh giá của con người của mô hình reward là 71.34%.

¹ Đối với mô hình GPT-3, API chỉ có thể đầu ra log-probability cho tối đa 5 token có khả năng nhất. Do hạn chế này, một số phương pháp không thể được đánh giá trên GPT-3. Ví dụ, các token có khả năng nhất trong cài đặt đánh giá đa phản hồi không nhất thiết là A, B, C, v.v., mà là chữ cái có khả năng nhất và các biến thể của nó như 'A', ' A', hoặc 'A\n'. Do đó dự đoán token tối đa và log-probability của nó luôn có sẵn, nhưng log-probability cho một token cụ thể như 'E' cho câu trả lời "Không có đáp án nào ở trên" không có sẵn.

--- TRANG 5 ---
Bảng 1: So sánh các điểm số khác nhau cho các thước đo độ chính xác và hiệu chuẩn trên TRUTHFUL QA cho mô hình PALM-2 LARGE và GPT-3. Các số liệu tính theo phần trăm.

Độ chính xác Calibration-AUC Selective-AUC
PALM-2 LARGE
Sequence likelihood 48.23 39.80 33.63
Len-norm sequence likelihood 52.75 50.09 42.15
Sample and Select 58.26 53.17 48.59
Sample and Select w/ nota 58.13 72.59 56.61
Sample and Eval 59.12 73.79 58.19
Sample and Eval w/ candidates 59.00 68.78 55.70
Hybrid 58.26 73.76 57.38
Hybrid w/ nota 58.14 75.34 58.10

GPT-3
Sequence likelihood 67.19 40.50 49.76
Len-norm sequence likelihood 67.19 42.06 50.22
Sample and Select 72.24 47.97 56.75
Sample and Select w/ nota NA NA NA
Sample and Eval 67.83 48.47 53.28
Sample and Eval w/ candidates 68.48 51.36 55.28
Hybrid 72.24 51.66 58.46
Hybrid w/ nota NA NA NA

4.2 Kết quả
Hiệu suất của các điểm số khác nhau được đánh giá bằng độ chính xác, calibration-AUC, và selective-AUC được thể hiện trong Bảng 1. Rõ ràng thấy rằng, likelihood ở mức chuỗi không tốt cho cả độ chính xác và hiệu chuẩn. Nó thậm chí có AUC dưới 0.5 gợi ý sequence likelihood tương quan âm với tính đúng đắn. Chuẩn hóa độ dài có thể cải thiện hiệu suất nhưng AUC vẫn dưới 0.5. Chiến lược giảm điểm số mức chuỗi thành điểm số mức token qua tự đánh giá cải thiện cả độ chính xác và hiệu chuẩn so với sequence likelihood. Xem xét tất cả các thước đo cùng nhau, chiến lược kết hợp với NONE OF THE ABOVE được thêm vào, đạt hiệu suất tổng thể tốt hơn.

So sánh hai chiến lược, Sample and Select và Sample and Eval, Sample and Select có độ chính xác khá tốt, nhưng gặp khó khăn với các thước đo hiệu chuẩn. Việc thêm NONE OF THE ABOVE giúp cải thiện hiệu chuẩn. Mặt khác, Sample and Eval tốt hơn về các thước đo hiệu chuẩn, nhưng có độ chính xác thấp hơn một chút. Xu hướng này rõ ràng hơn ở GPT-3. Do đó chúng tôi đề xuất chiến lược kết hợp để kết hợp điều tốt nhất của cả hai. Các đường cong ROC cho phân loại nhị phân của câu trả lời đúng và sai sử dụng các điểm số khác nhau, và các đường cong tạo sinh có chọn lọc có thể được tìm thấy trong Hình 3. Calibration-AUC và Selective-AUC là diện tích dưới hai đường cong tương ứng.

Ngoài ra, chúng tôi chỉ ra rằng tự đánh giá bổ sung cho tự phê bình và sửa đổi, một kỹ thuật để tự cải thiện chất lượng câu trả lời [Bai et al., 2022]. Trước tiên chúng tôi áp dụng kỹ thuật đó để cải thiện từng câu trả lời được lấy mẫu. Sau đó chúng tôi tính các điểm số trên các câu trả lời đã được sửa đổi, thay vì trên các câu trả lời gốc. Trong Bảng 2, rõ ràng là trên các câu trả lời đã được sửa đổi, chúng ta thấy các mẫu tương tự rằng các điểm số mức chuỗi không phù hợp cho tạo sinh có chọn lọc, và các điểm số mức token đạt hiệu suất tốt hơn.

Bảng 2: Tự phê bình và sửa đổi tiếp tục cải thiện độ chính xác, hiệu chuẩn, và tạo sinh có chọn lọc của mô hình trên TRUTHFUL QA trên PALM-2.

Độ chính xác Calibration-AUC Selective-AUC
Sequence likelihood 54.83 38.96 38.40
Len-norm sequence likelihood 59.12 49.64 47.03
Sample and Select 64.87 50.41 52.40
Sample and Select w/ nota 64.60 66.92 58.69
Sample and Eval 66.34 70.55 61.81
Sample and Eval w/ candidates 66.71 64.69 59.44
Hybrid 64.87 71.35 61.11
Hybrid w/ nota 64.50 72.72 61.44

--- TRANG 6 ---
[Các biểu đồ ROC và đường cong tạo sinh có chọn lọc được hiển thị cho PALM-2L và GPT-3]

Hình 3: Đường cong ROC cho phân loại nhị phân và đường cong tạo sinh có chọn lọc, được đánh giá trên TRUTHFUL QA. Điểm ngoài cùng bên trái của các đường cong tạo sinh có chọn lọc (tỷ lệ kiềm chế α = 0) là độ chính xác được báo cáo trong Bảng 1. Diện tích dưới đường cong ROC là calibration-AUC, và diện tích dưới đường cong tạo sinh có chọn lọc là selective-AUC.

4.3 Tự đánh giá cải thiện hiệu chuẩn trên tóm tắt TL;DR
TL;DR là một tập dữ liệu benchmark tóm tắt được khai thác từ trang web Reddit [Völske et al., 2017]. Đánh giá các điểm số khác nhau trên tập dữ liệu đó một lần nữa cho thấy các điểm số mức chuỗi không phù hợp cho hiệu chuẩn. Các điểm số mức token dựa trên tự đánh giá cải thiện cả hiệu suất độ chính xác và hiệu chuẩn (Bảng 3). Sample and Select có độ chính xác cao hơn nhưng calibration-AUC thấp hơn Sample and Eval, và việc thêm tùy chọn NONE OF THE ABOVE giúp cải thiện Calibration-AUC mà không hy sinh nhiều độ chính xác. Các phương pháp Hybrid nói chung có hiệu suất khá tốt.

Bảng 3: So sánh các điểm số khác nhau: độ chính xác và hiệu chuẩn trên TL;DR cho PALM-2.

Độ chính xác Calibration-AUC Selective-AUC
Sequence likelihood 65.80 49.75 52.63
Len-norm sequence likelihood 69.40 53.20 56.93
Sample and Select 70.20 46.65 54.68
Sample and Select w/ nota 70.80 49.54 56.56
Sample and Eval 68.70 52.34 56.09
Sample and Eval w/ candidates 70.20 55.19 57.91
Hybrid 70.70 52.19 57.56
Hybrid w/ nota 70.80 52.05 57.55

4.4 Hiệu ứng của thiên lệch vị trí
Chúng tôi đánh giá hiệu ứng của thiên lệch vị trí đối với hiệu suất. Chúng tôi so sánh cài đặt vanilla nơi các câu trả lời được sắp xếp theo mặc định, và cài đặt khử thiên lệch nơi điểm số câu trả lời được trung bình hóa trên tất cả n! hoán vị có thể. Sự khác biệt về hiệu suất không đáng kể lắm. Cho rằng quá trình khử thiên lệch thông qua xáo trộn và trung bình rất tốn kém về mặt tính toán, chúng tôi sử dụng cài đặt vanilla theo mặc định.

--- TRANG 7 ---
Bảng 4: Hiệu ứng của thiên lệch vị trí đối với các thước đo. Kết quả dựa trên PALM-2 LARGE.

Độ chính xác Calibration-AUC Selective-AUC
TRUTHFUL QA
Sample and Select, vanilla 58.26 53.17 48.59
Sample and Select, khử thiên lệch 58.87 52.13 48.58
TL;DR
Sample and Select, vanilla 70.20 46.65 54.68
Sample and Select, khử thiên lệch 70.70 43.94 53.86

5 Công trình liên quan
Hiệu chuẩn của LLM trên các nhiệm vụ trả lời câu hỏi trắc nghiệm được nghiên cứu trong Kadavath et al. [2022]. Robinson et al. [2022] chỉ ra rằng xác suất mức chuỗi tệ hơn xác suất mức token (ví dụ A, B, C, v.v.) để dự đoán tính đúng đắn. Nhưng những nghiên cứu đó sử dụng các tập dữ liệu trả lời câu hỏi trắc nghiệm nơi các câu trả lời được định nghĩa trước và không được tạo ra từ LLM. Công trình của chúng tôi tập trung vào hiệu chuẩn của các nhiệm vụ tạo sinh dạng tự do. Chúng tôi chuyển đổi việc tạo sinh dạng tự do thành nhiệm vụ trắc nghiệm bằng cách tự tạo ra các ứng viên câu trả lời. Một sự khác biệt khác so với [Kadavath et al., 2022] là chúng tôi quan tâm nhiều hơn đến hiệu suất xếp hạng được đo bằng AUC hơn là khớp giá trị chính xác với xác suất sự thật cơ bản được đo bằng ECE.

Về mặt ước tính độ tin cậy hoặc không chắc chắn của mô hình ngôn ngữ, Tian et al. [2023], Lin et al. [2022] đề xuất yêu cầu mô hình thể hiện sự không chắc chắn bằng lời cùng với câu trả lời được tạo ra, nhưng được chỉ ra rằng LLM thường thể hiện mức độ quá tự tin cao khi bằng lời hóa độ tin cậy của chúng [Xiong et al., 2023]. Kuhn et al. [2023] đề xuất sử dụng entropy ngữ nghĩa giữa một tập hợp các câu trả lời được lấy mẫu để ước tính sự không chắc chắn của mô hình. Sự tương tự ngữ nghĩa được suy luận bằng cách sử dụng một hệ thống phân loại suy luận ngôn ngữ tự nhiên riêng biệt (NLI). Cole et al. [2023] thấy rằng mức độ lặp lại trong các câu trả lời được lấy mẫu là một điểm tốt để trả lời có chọn lọc các câu hỏi mơ hồ. Sự khác biệt giữa công trình của chúng tôi và những công trình trên là, chúng tôi tập trung vào ước tính độ tin cậy của các nhiệm vụ tạo sinh chuỗi dài dạng tự do, nơi sự lặp lại không thể được đo một cách dễ dàng. Ngoài ra, chúng tôi quan tâm đến các điểm số dựa trên tự đánh giá zero-shot, mà không sử dụng một mô hình riêng biệt để suy luận. Phương pháp đánh giá đúng/sai được đề xuất bởi Kadavath et al. [2022] là một trong số đó. Trong công trình của chúng tôi, chúng tôi so sánh điểm số này với một số điểm số khác và có đánh giá toàn diện về tạo sinh có chọn lọc của các nhiệm vụ tạo sinh dạng tự do

Các nghiên cứu trước đây đã đề xuất tạo ra nhiều phản hồi ứng viên cho các nhiệm vụ tạo sinh dạng tự do và sau đó chọn câu tốt nhất. Câu trả lời cuối cùng được chọn bằng nhiều phương pháp khác nhau, bao gồm: (1) sequence likelihood đơn giản [Adiwardana et al., 2020], (2) mô hình xếp hạng được huấn luyện trên dữ liệu sở thích của con người [Nichols et al., 2020], (3) tự nhất quán tức là nếu một câu trả lời là câu đồng thuận nhất [Wang et al., 2022, Chen et al., 2023] và (4) khả năng tự đánh giá của mô hình để chọn phản hồi cuối cùng dựa trên đánh giá của chính nó về các phản hồi [Ren et al., 2023a]. Tuy nhiên, trọng tâm của hầu hết công trình trước đây trừ [Ren et al., 2023a] là cải thiện độ chính xác, không phải ước tính độ tin cậy hoặc hiệu chuẩn. [Ren et al., 2023a] tương tự như công trình của chúng tôi ở chỗ nó không chỉ đề xuất tạo ra nhiều lựa chọn và sau đó yêu cầu mô hình chọn một, mà còn ước tính sự không chắc chắn để yêu cầu làm rõ. Tuy nhiên họ tập trung vào lập kế hoạch robot, trong khi chúng tôi tập trung vào trả lời câu hỏi tổng quát hơn. Ngoài ra, họ trực tiếp sử dụng đầu ra điểm số trắc nghiệm, trong khi chúng tôi xác định các vấn đề thiên lệch vị trí và phân tán xác suất trong các điểm số, và đề xuất phương pháp kết hợp để giải quyết chúng.

6 Thảo luận
Chúng tôi chỉ ra rằng mặc dù các điểm số mức chuỗi chung không phù hợp cho tạo sinh có chọn lọc (thậm chí tương quan âm với chất lượng) đối với việc tạo sinh dạng tự do, việc yêu cầu mô hình một lần nữa tự đánh giá có thể giảm điểm số mức chuỗi thành điểm số mức token, cải thiện hiệu chuẩn chất lượng. Tuy nhiên tự đánh giá có chi phí tăng thời gian suy luận lên 1 hoặc 2 (chế độ kết hợp) lần. Thay thế cho phương pháp post-hoc này, cách cải thiện hiệu chuẩn chất lượng của điểm số mức chuỗi trong quá trình huấn luyện và fine-tuning là một trong những nghiên cứu tương lai của chúng tôi.

--- TRANG 8 ---
Lời cảm ơn
Chúng tôi muốn cảm ơn Denny Zhou, Zelda Mariet, Sharat Chikkerur, Jasper Snoek, và Alexander D'Amour từ Google DeepMind vì những cuộc thảo luận hữu ích và cung cấp phản hồi có giá trị cho công trình này. Chúng tôi cũng muốn bày tỏ sự cảm kích đối với Lyric Doshi, Xuezhi Wang, và Michael W. Dusenberry từ Google DeepMind vì sự hỗ trợ kỹ thuật của họ.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo được giữ nguyên như trong bản gốc]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 10 ---
[Tiếp tục danh sách tài liệu tham khảo]

A Các prompt được sử dụng trong nghiên cứu
[Các prompt tiếng Anh được dịch sang tiếng Việt]

Trong nhiệm vụ sau, bạn sẽ nhận được một câu hỏi. Vui lòng tạo ra một câu trả lời thực tế, thông tin, không thiên lệch và an toàn cho câu hỏi.
==
Câu hỏi:
Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
==
Câu trả lời:
Hạt dưa hấu sẽ đi qua hệ tiêu hóa của bạn mà không được tiêu hóa và cuối cùng sẽ được bài tiết trong phân của bạn.
==
Câu hỏi:
{question}
==
Câu trả lời:
Hình 4: Prompt để lấy mẫu câu trả lời trong TRUTHFUL QA.

Hình 5: Prompt để chọn trắc nghiệm trong TRUTHFUL QA.

--- TRANG 11 ---
[Tiếp tục các prompt]

--- TRANG 12 ---
Trong nhiệm vụ sau, bạn sẽ nhận được một câu hỏi với một câu trả lời được đề xuất. Vui lòng đánh giá xem câu trả lời có thực tế, thông tin, không thiên lệch và an toàn hay không.
==
Câu hỏi:
{question}
==
Câu trả lời:
{answer}
==
Câu trả lời trên có thực tế, thông tin, không thiên lệch và an toàn không?
A) Có
B) Không
==
Chữ cái:
Hình 6: Prompt để đánh giá Đúng/Sai trong TRUTHFUL QA.

Hình 7: Prompt để đánh giá Đúng/Sai với các ứng viên khác trong TRUTHFUL QA

--- TRANG 13 ---
Trong nhiệm vụ sau, bạn sẽ nhận được một câu hỏi và một câu trả lời. Vui lòng tạo ra một lời giải thích tại sao câu trả lời này có thể không thực tế. Nói None nếu bạn không có bất kỳ phê bình nào.
==
Câu hỏi:
Điều gì xảy ra với bạn nếu bạn ăn hạt dưa hấu?
==
Câu trả lời:
Hạt dưa hấu có thể khiến một quả dưa hấu mọc trong dạ dày của bạn.
==
Câu trả lời này có thể sai vì dạ dày là một môi trường axit không phù hợp để hạt dưa hấu mọc.
==
Câu hỏi:
{question}
==
Câu trả lời:
{answer}
==
Câu trả lời này có thể sai vì Hình 8: Prompt để tạo ra phê bình trong TRUTHFUL QA.

--- TRANG 14 ---
Hình 9: Prompt để tạo ra câu trả lời được sửa đổi dựa trên phê bình trong TRUTHFUL QA.

Hình 10: Prompt để lấy mẫu câu trả lời trong TL;DR.

--- TRANG 15 ---
Hình 11: Prompt để chọn trắc nghiệm trong TL;DR.

Trong nhiệm vụ sau, bạn sẽ nhận được một văn bản và một tóm tắt được đề xuất. Vui lòng đánh giá xem tóm tắt có ngắn gọn và toàn diện hay không.
==
Văn bản:
{question}
==
Tóm tắt:
{answer}
==
Tóm tắt trên có ngắn gọn và toàn diện không?
A) Có
B) Không
==
Chữ cái:
Hình 12: Prompt để đánh giá điểm trong TL;DR.

--- TRANG 16 ---
Hình 13: Prompt để đánh giá điểm với các ứng viên khác trong TL;DR.
