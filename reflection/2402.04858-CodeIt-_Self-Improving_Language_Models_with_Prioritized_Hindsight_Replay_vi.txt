# 2402.04858.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reflection/2402.04858.pdf
# Kích thước tệp: 1304985 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CodeIt: Mô hình Ngôn ngữ Tự cải tiến với Phát lại Hồi tưởng Ưu tiên
Natasha Butt1 2Blazej Manczak2Auke Wiggers2
Corrado Rainone2David W. Zhang2Micha ¨el Defferrard2Taco Cohen2 3

Tóm tắt
Các mô hình ngôn ngữ lớn ngày càng giải quyết được những nhiệm vụ thường được cho là đòi hỏi khả năng suy luận ở mức con người. Tuy nhiên, những mô hình này vẫn hoạt động rất kém trên các bộ kiểm tra trí tuệ tổng quát như Abstraction and Reasoning Corpus (ARC). Trong bài báo này, chúng tôi tiếp cận ARC như một bài toán lập trình-bằng-ví dụ, và giới thiệu một phương pháp mới và có thể mở rộng cho việc tự cải tiến mô hình ngôn ngữ gọi là Code Iteration (CodeIt). Phương pháp của chúng tôi lặp lại giữa 1) lấy mẫu chương trình và gán nhãn lại hồi tưởng, và 2) học từ phát lại trải nghiệm ưu tiên. Bằng cách gán nhãn lại mục tiêu của một episode (tức là đầu ra chương trình mục tiêu cho đầu vào) thành đầu ra thực hiện được tạo ra bởi chương trình được lấy mẫu, phương pháp của chúng tôi xử lý hiệu quả tính thưa thớt cực kỳ của phần thưởng trong tổng hợp chương trình. Áp dụng CodeIt vào tập dữ liệu ARC, chúng tôi chứng minh rằng phát lại hồi tưởng ưu tiên, cùng với tiền huấn luyện và tăng cường dữ liệu, dẫn đến khái quát hóa thành công giữa các nhiệm vụ. CodeIt là phương pháp neuro-symbolic đầu tiên mở rộng đến toàn bộ tập đánh giá ARC. Phương pháp của chúng tôi giải quyết 15% nhiệm vụ đánh giá ARC, đạt được hiệu suất tốt nhất và vượt trội hơn các baseline neural và symbolic hiện có. Mã nguồn của chúng tôi có sẵn tại https://github.com/Qualcomm-AI-research/codeit .

1. Giới thiệu
Abstraction and Reasoning Corpus (ARC) là một benchmark trí tuệ nhân tạo tổng quát nhắm vào cả con người và hệ thống AI (Chollet, 2019). ARC là một 

1University of Amsterdam2Qualcomm AI Research. Qualcomm AI Research là một sáng kiến của Qualcomm Technologies, Inc.3Công việc được hoàn thành khi còn là nhân viên tại Qualcomm Technologies Netherlands B.V .. Liên hệ: Natasha Butt <n.e.butt@uva.nl >.

Proceedings of the 41stInternational Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).

Hình 1. Tổng quan về Code Iteration. Trong giai đoạn lấy mẫu, các chương trình ρ được lấy mẫu từ chính sách Qθ có điều kiện trên các cặp đầu vào-đầu ra. Chương trình có thể không tạo ra đầu ra mục tiêu O∗ cho I, vì vậy chúng ta sử dụng gán nhãn lại hồi tưởng: chúng ta thực thi chương trình, và thêm chương trình ρ, đầu vào I, và đầu ra thực hiện O vào buffer. Trong giai đoạn học, chúng ta huấn luyện chính sách trên các mẫu từ buffer.

benchmark thách thức vì nó chứa các nhiệm vụ ví dụ few-shot giả định quyền truy cập vào bốn hệ thống kiến thức cốt lõi bẩm sinh: đối tượng, hành động, số và không gian (Spelke & Kinzler, 2007). Nó được thiết kế để không yêu cầu kiến thức nào ngoài những tiền đề này, và vì vậy khả năng ghi nhớ khổng lồ của các mô hình ngôn ngữ tiền huấn luyện có tác dụng hạn chế cho vấn đề này. Con người có thể giải quyết 80% (một tập con ngẫu nhiên của) các nhiệm vụ ARC trong các nghiên cứu người dùng (Johnson et al., 2021), trong khi các phương pháp neural tốt nhất dựa trên GPT-4 chỉ giải quyết 12% nhiệm vụ đánh giá (Gendron et al., 2023).

Mỗi nhiệm vụ ARC bao gồm một số ví dụ minh họa, mỗi ví dụ gồm một lưới đầu vào và đầu ra, và một hoặc nhiều đầu vào kiểm tra mà đầu ra tương ứng phải được dự đoán (xem Hình 2). Các agent hiệu quả sử dụng các trừu tượng liên quan đến bốn hệ thống kiến thức cốt lõi, khái quát hóa từ minh họa đến ví dụ kiểm tra, và khái quát hóa giữa các nhiệm vụ. Ví dụ, một agent có thể suy luận rằng các ô liền kề (không gian) có cùng giá trị màu (số) tạo thành một đối tượng. Một agent cũng có thể suy luận rằng nhiều đối tượng đôi khi hút hoặc đẩy (hành động). Sử dụng những trừu tượng này để suy luận về giá trị của đầu ra kiểm tra, một agent có thể khái quát hóa từ các ví dụ minh họa đến ví dụ kiểm tra.

Các phương pháp hiện có cho ARC có thể được phân loại là neural (Gendron et al., 2023; Mirchandani et al., 2023), nghĩa là chúng trực tiếp dự đoán lưới đầu ra sử dụng mạng neural, hoặc (neuro-) symbolic (Ainooson et al., 2023; Ferr ´e, 2021; 2023), nghĩa là chúng đầu tiên dự đoán một chương trình hoặc biểu diễn symbolic khác của ánh xạ giữa lưới đầu vào và đầu ra, trước khi sử dụng nó để tạo ra lưới đầu ra. Thông qua việc sử dụng ngôn ngữ đặc thù miền (DSL) được thiết kế tốt, các phương pháp symbolic có thể được trang bị kiến thức tiên nghiệm tương tự như các hệ thống kiến thức cốt lõi được tìm thấy ở con người. Bằng cách kết hợp mạng neural và biểu diễn symbolic như chương trình, hệ thống có thể tận dụng cả kiến thức tiên nghiệm và dữ liệu để giải quyết các nhiệm vụ ARC.

Tuy nhiên, các phương pháp hiệu quả nhất hiện có, dù neural hay symbolic, đều không sử dụng kinh nghiệm để khái quát hóa giữa các nhiệm vụ. Chúng tôi đề xuất sử dụng Expert Iteration (ExIt) (Anthony et al., 2017) để kết hợp kinh nghiệm. Các phương pháp ExIt làm điều này bằng cách xen kẽ giữa hai giai đoạn: thu thập dữ liệu với chính sách khám phá (thường tốn kém), và cải thiện chính sách bằng cách huấn luyện trên các kinh nghiệm mới được tìm thấy. Thay vì thực hiện ExIt trong không gian lưới, chúng tôi áp dụng phương pháp neuro-symbolic và huấn luyện mô hình của chúng tôi để học viết chương trình. Điều này đưa chúng ta gần hơn với hệ thống mô phỏng trí tuệ lưu loát tổng quát được mô tả bởi Chollet (2019): bằng cách kết hợp các kinh nghiệm mới dưới dạng trừu tượng.

Các phương pháp ExIt gần đây sử dụng mô hình ngôn ngữ tự cải tiến (Gulcehre et al., 2023; Aksitov et al., 2023; Wang et al., 2023c) để thay thế chuyên gia tốn kém bằng việc lấy mẫu từ chính sách mô hình ngôn ngữ và lọc dựa trên phần thưởng, chỉ lưu các quỹ đạo đạt được phần thưởng cao. Điều này cho phép chúng mở rộng tốt và hưởng lợi từ kiến thức đã được nắm bắt trong chính sách. Những phương pháp này chứng minh hiệu quả trên các nhiệm vụ tổng hợp chương trình với đặc tả ngôn ngữ tự nhiên (Singh et al., 2023) và đặc tả mã (Haluptzok et al., 2022). Tuy nhiên, khi giải quyết ARC, các agent bắt đầu ExIt với kiến thức tiên nghiệm kém về không gian tìm kiếm, vì nhiệm vụ nằm ngoài phân phối. Tìm một chương trình chính xác là thách thức: phần thưởng tích cực cực kỳ thưa thớt. Do đó, những phương pháp này không hiệu quả về mẫu trong bối cảnh ARC, và lập trình-bằng-ví dụ nói chung. Để cho phép học trong các thiết lập phần thưởng thưa thớt, gán nhãn lại hồi tưởng (Andrychowicz et al., 2017) tạo ra các quỹ đạo chuyên gia nhân tạo sau thực tế, và các phương pháp kết hợp ExIt và kỹ thuật này đã cải thiện hiệu quả mẫu (Gauthier, 2022; Butt et al., 2022). Tuy nhiên, vì phân phối dữ liệu được gán nhãn lại liên tục thay đổi, có nguy cơ quên thảm khốc (French, 1999).

Trong công trình này, chúng tôi giới thiệu một phương pháp expert iteration mới, có thể mở rộng cho các thiết lập phần thưởng thưa thớt mà không bị quên thảm khốc. Phương pháp của chúng tôi, được gọi là Code Iteration hay CodeIt cho ngắn gọn, lặp lại giữa 1) giai đoạn lấy mẫu và gán nhãn lại hồi tưởng và 2) giai đoạn học với phát lại trải nghiệm ưu tiên. Chúng tôi hiển thị một hình ảnh trực quan trong

Hình 2. Một nhiệm vụ ARC đơn giản hóa. Cho hai cặp đầu vào-đầu ra minh họa, mục tiêu là xác định lưới đầu ra cho ví dụ kiểm tra, trong ba lần thử hoặc ít hơn. Kích thước của lưới và số lượng ví dụ minh họa và kiểm tra khác nhau giữa các nhiệm vụ.

Hình 1. Quy trình lặp lại này do đó cho phép chúng ta tự động tạo dữ liệu mới mà không cần can thiệp của con người. Không giống các phương pháp tự cải tiến hiện tại thực hiện lấy mẫu và lọc (Singh et al., 2023), CodeIt học từ tất cả các mẫu chương trình, cải thiện hiệu quả mẫu. Bằng cách ưu tiên huấn luyện trên các kinh nghiệm giải quyết nhiệm vụ thực, chúng ta giảm thiểu nguy cơ quên thảm khốc.

CodeIt giải quyết 59/400 nhiệm vụ đánh giá ARC, đạt được hiệu suất tốt nhất bằng cách học từ kinh nghiệm dưới dạng trừu tượng và khái quát hóa đến nhiệm vụ mới. Chúng tôi phân tích các chương trình được khám phá bởi CodeIt và thấy rằng những chương trình này trung bình ngắn hơn và sử dụng các nguyên thủy khác nhau so với các baseline symbolic tùy chỉnh của chúng tôi. Hơn nữa, sau khi tìm thấy một giải pháp ban đầu, CodeIt tiếp tục cải thiện nó theo thời gian; các giải pháp ngắn hơn được tìm thấy trong 53% nhiệm vụ ARC đã giải quyết, nhấn mạnh khả năng thực hiện tinh chỉnh chương trình. Chúng tôi thực hiện các ablation cẩn thận để hiểu rõ hơn tác động đến hiệu suất nhiệm vụ của các thành phần chính: ExIt, phát lại hồi tưởng ưu tiên, và kiến thức tiên nghiệm.

2. Phương pháp
Chúng tôi tiếp cận ARC như một bài toán lập trình-bằng-ví dụ: cho một tập hợp các nhiệm vụ mà chúng ta gọi là tập tìm kiếm, chúng ta nhằm tìm các chương trình khớp chính xác đầu vào với đầu ra tương ứng của chúng, và chúng ta làm điều này bằng cách huấn luyện một chính sách để tạo ra chương trình khi được hiển thị các ví dụ minh họa. Điều này được thực hiện bằng cách lặp lại giữa hai giai đoạn: 1) viết chương trình sử dụng chính sách và áp dụng gán nhãn lại hồi tưởng, và 2) học từ các chương trình và ví dụ đầu vào-đầu ra của chúng. Chúng tôi đầu tiên mô tả các lựa chọn thiết kế chính dưới đây, và sau đó giải thích quy trình lặp lại.

2.1. Lựa chọn thiết kế

Ngôn ngữ lập trình Chúng tôi hạn chế ngôn ngữ lập trình của chúng tôi thành ngôn ngữ đặc thù miền (DSL) mã nguồn mở của Hodel (2023). Mặc dù có nhiều DSL mã nguồn mở khác nhau cho ARC, Hodel đã thiết kế DSL của họ chỉ sử dụng phần huấn luyện ARC, trong khi một số tác giả kết hợp các tiền đề từ phần đánh giá ARC vào DSL của họ (Icecuber, 2020). DSL của Hodel chứa các hàm thao tác lưới (ví dụ, vmirror hoặc hmirror, phản chiếu lưới dọc theo trục dọc hoặc ngang), các hàm điền thay thế tất cả pixel của một màu nhất định, và các hàm trả về vị trí của các nhóm pixel cụ thể. Xem Phụ lục B.4 để biết chi tiết về DSL và thêm các nguyên thủy ví dụ, và xem Hodel (2023) để thảo luận về các nguyên thủy và khả năng của DSL.

Chính sách Lựa chọn chính sách của chúng tôi là một Mô hình Ngôn ngữ Lớn (LLM) encoder-decoder tiền huấn luyện. Chúng tôi sử dụng mô hình CodeT5+ 220 triệu tham số (Wang et al., 2023b) và tokenizer mặc định của nó, được tiền huấn luyện trên một tập đa dạng các nhiệm vụ lập trình. Chúng tôi đưa các ví dụ minh họa vào encoder, và để decoder tạo ra chương trình tương ứng. Nếu cần thiết, các ví dụ minh họa được cắt ngắn để vừa với cửa sổ ngữ cảnh encoder.

Biểu diễn lưới Để điều kiện hóa chính sách mô hình ngôn ngữ trên lưới đầu vào-đầu ra, chúng tôi biểu diễn chúng dưới dạng văn bản. Thay vì mã hóa lưới như một mảng 2 chiều, chúng tôi sử dụng biểu diễn văn bản tập trung vào đối tượng. Mỗi màu được mã hóa như một số nguyên, và cho mỗi màu trong lưới chúng tôi liệt kê tất cả các ô lưới có màu đó như tọa độ [x, y]. Vì phần lớn các ô thuộc về màu nền, quy trình này giảm đáng kể số lượng token cần thiết để mã hóa lưới (xem Hình 16 trong Phụ lục A.3). Một ví dụ về biểu diễn lưới thưa thớt được hiển thị trong Hình 3. Biểu diễn văn bản tập trung vào đối tượng này, tương tự như của Xu et al. (2023), hoạt động tốt cho lưới thưa thớt và có thể hiểu được bởi con người.

2.2. Thuật toán Code Iteration

Chúng tôi khởi tạo mạng chính sách bằng cách huấn luyện trên dữ liệu sự thật nền tảng. Sau đó chúng tôi bắt đầu CodeIt, lặp lại giữa lấy mẫu và gán nhãn lại hồi tưởng và học. Chúng tôi gọi một lần đi qua đầy đủ lấy mẫu và học là một meta-iteration. Chúng tôi hiển thị quy trình trong Hình 1, và giải thích chi tiết từng giai đoạn dưới đây. Để xem mã giả, xem Phụ lục A.1.

Khởi tạo Chúng tôi bắt đầu từ một tập dữ liệu các nhiệm vụ huấn luyện ARC và các chương trình giải pháp được viết trong ngôn ngữ đặc thù miền (DSL) của Hodel (2023), mà chúng tôi gọi là tập huấn luyện. Tập dữ liệu này được mở rộng bằng cách biến đổi ngẫu nhiên các chương trình (để biết chi tiết về quy trình này, xem Phụ lục A.2), tạo ra một tập huấn luyện tăng cường.

Bước tăng cường dữ liệu ban đầu phục vụ nhiều mục đích. Trộn các chương trình đã biến đổi hoạt động như một hình thức tăng cường dữ liệu, và là một phương pháp phổ biến trong cải thiện chính sách cho tổng hợp chương trình (Ellis et al., 2020; Fawzi et al., 2022). Trước khi các kinh nghiệm được lấy mẫu từ chính sách, mô hình đã có thể học cú pháp DSL, điều này có thể khó khăn nếu tập huấn luyện nhỏ. Nó cũng cho phép mô hình học cách diễn giải các ví dụ minh họa nhiệm vụ trước khi chúng ta bắt đầu học lặp lại, cải thiện chất lượng mẫu chính sách của chúng ta trong các meta-iteration sớm.

Lấy mẫu và gán nhãn lại hồi tưởng Trong giai đoạn lấy mẫu, chúng tôi thu được các chương trình mới sử dụng chính sách Qθ. Để tập tìm kiếm là tập hợp các nhiệm vụ mà chúng ta muốn tìm chương trình tương ứng. Cho mỗi nhiệm vụ trong tập tìm kiếm, chúng ta chuyển đổi đầu vào I và đầu ra mục tiêu O∗ của các ví dụ minh họa từ lưới sang biểu diễn văn bản, mã hóa chúng sử dụng chính sách, và sau đó giải mã autoregressive một chương trình: ρ∼Qθ(ρ|I, O∗). Sau đó chúng ta chạy chương trình thu được trên lưới đầu vào. Nếu chương trình không đúng cú pháp hoặc thời gian chạy quá cao, chúng ta loại bỏ nó. Nếu không, chúng ta thu được đầu ra chương trình O=ρ(I), và có thể thêm một bộ ba mới vào buffer phát lại: chương trình ρ, đầu vào minh họa I, và đầu ra thực hiện O (có thể khớp hoặc không khớp với đầu ra mục tiêu O∗). Trong mỗi giai đoạn lấy mẫu chúng ta lặp lại quy trình này nρ lần trên mỗi nhiệm vụ, trong đó nρ là một siêu tham số.

Thay thế đầu ra mục tiêu bằng đầu ra thực hiện là một dạng phát lại kinh nghiệm hồi tưởng (Andrychowicz et al., 2017), và đảm bảo rằng chúng ta thu được một kinh nghiệm mỗi khi chúng ta tìm thấy một chương trình đúng cú pháp, từ đó ngăn chặn sự trì trệ của buffer. Mặc dù những chương trình này có thể không giải quyết các nhiệm vụ chúng ta quan tâm, chúng luôn hợp lệ về mặt cú pháp và ngữ nghĩa (ánh xạ chính xác ρ(I)→O). Do đó chúng có thể được sử dụng để dạy chính sách về cú pháp chương trình và hành vi chương trình, điều này có thể dẫn đến chuyển giao tích cực đến tập tìm kiếm. Chúng tôi nhấn mạnh rằng chúng ta không bao giờ thêm các ví dụ kiểm tra hay hiệu suất trên các ví dụ kiểm tra vào buffer của chúng ta, vì ta không nên có quyền truy cập vào lưới đầu ra mục tiêu của chúng trong quá trình lấy mẫu.

Học Trong giai đoạn học, chính sách Qθ được huấn luyện trên các kinh nghiệm được lấy mẫu từ buffer, tập huấn luyện và tập huấn luyện tăng cường. Những kinh nghiệm này bao gồm lưới đầu vào I, lưới đầu ra O và chương trình tương ứng ρ. Mục tiêu huấn luyện sau đó là một mục tiêu negative log-likelihood đơn giản:

L(ρ, I, O ) =−logQθ(ρ|I, O). (1)

Chúng tôi chỉ giữ một bản sao duy nhất của mạng chính sách, và tiếp tục cập nhật nó trong mỗi giai đoạn học. Vì chúng ta không so sánh chính sách với các phiên bản trước đó của nó, không có đảm bảo cho cải thiện. Mặc dù việc cập nhật liên tục có thể dẫn đến hiệu suất tồi tệ hơn trong lần lặp tiếp theo, chúng tôi thấy điều này không phải là vấn đề trong thực tế.

Theo mặc định, chúng tôi thực hiện lấy mẫu ưu tiên từ buffer phát lại (Schaul et al., 2015). Cho mỗi kinh nghiệm, độ ưu tiên tỷ lệ với phần trăm đầu ra minh họa bằng đầu ra chương trình. Điều này có nghĩa là các chương trình giải quyết các ví dụ minh họa của nhiệm vụ ARC thực được lấy mẫu thường xuyên hơn so với các chương trình cho nhiệm vụ được gán nhãn lại hồi tưởng.

3. Thí nghiệm
Trong phần này, chúng tôi nhằm chứng minh hiệu quả của CodeIt, và phân tích xem các thành phần khác nhau của phương pháp đóng góp bao nhiêu vào hiệu suất. Chúng tôi đầu tiên điều chỉnh siêu tham số trên một phần chia huấn luyện và xác thực tùy chỉnh (để mô tả các tham số này và chi tiết, xem Phụ lục B). Sử dụng những siêu tham số này, chúng tôi kiểm tra phương pháp của chúng tôi trên phần chia đánh giá ARC và so sánh với các phương pháp tốt nhất trước đó. Cuối cùng, chúng tôi ablate tầm quan trọng của các thành phần riêng lẻ của CodeIt.

Chúng tôi định nghĩa hiệu suất minh họa là phần trăm các ví dụ minh họa đã giải quyết trên một nhiệm vụ cho trước. Chúng tôi đầu tiên sắp xếp các chương trình giải pháp theo hiệu suất minh họa, và sau đó theo độ dài chương trình, ưu tiên các chương trình ngắn hơn. Chúng tôi đánh giá ba chương trình hàng đầu trên tập hợp các ví dụ kiểm tra. Theo quy trình đánh giá ARC, nếu ít nhất một trong ba chương trình này ánh xạ tất cả đầu vào ví dụ kiểm tra thành đầu ra, nhiệm vụ được giải quyết và hiệu suất kiểm tra là 1. Chúng tôi nhấn mạnh rằng quy trình ExIt chỉ sử dụng các ví dụ minh họa, và chúng ta sử dụng hiệu suất kiểm tra chỉ để đánh giá cuối cùng.

Baseline tùy chỉnh Chúng tôi sử dụng baseline ngẫu nhiên lấy mẫu chương trình từng dòng. Ở đầu mỗi dòng, chúng ta lấy mẫu một hàm nguyên thủy từ DSL, sau đó lấy mẫu đối số cho các loại đầu vào mong đợi của nó. Khi một biến loại "grid" được tạo ra, chúng ta kết thúc chương trình với xác suất 0.8, nếu không chúng ta thêm một dòng khác vào chương trình.

Chúng tôi cũng sử dụng baseline dựa trên biến đổi. Đây là một quy trình tiến bộ hơn, được thiết kế với DSL trong tâm trí. Ở mỗi meta-iteration, nó biến đổi tập hợp các chương trình huấn luyện được cung cấp bởi Hodel (2023). Chúng tôi sử dụng hai biến thể: "d1" chỉ biến đổi tập huấn luyện ban đầu, và "d∞" cũng có thể tăng cường các chương trình mới được tìm thấy. Chúng tôi cung cấp thuật toán chính xác trong Phụ lục A.2.

Cho tất cả ba baseline, chúng tôi lấy mẫu nm=nρ·ntasks chương trình trên mỗi meta-iteration. Ở đây, nρ là số chương trình mong muốn trên mỗi meta-iteration trên mỗi nhiệm vụ, và ntasks tổng số nhiệm vụ trong quần thể. Để tăng cường những baseline này, chúng tôi đánh giá đầy đủ mỗi chương trình được tìm thấy trên tất cả đầu vào trong tập tìm kiếm, và kiểm tra đầu ra so với lưới đầu ra ARC.

Baseline từ tài liệu Chúng tôi cũng bao gồm các phương pháp từ tài liệu như baseline. Việc so sánh trực tiếp đôi khi khó khăn, vì không phải tất cả baseline đều áp dụng phương pháp của họ cho toàn bộ tập đánh giá ARC: ví dụ, Kolev et al. (2020) và Alford et al. (2021) chỉ tập trung vào một tập con của ARC. Ngoài ra, một số phương pháp symbolic thiết kế DSL dựa trên cả tập huấn luyện và đánh giá ARC và báo cáo kết quả trên tập kiểm tra ẩn (Icecuber, 2020). Do đó chúng tôi chỉ so sánh với các phương pháp báo cáo điểm số trên toàn bộ tập đánh giá ARC.

Ainooson et al. (2023) và Ferr ´e (2023) đều chạy quy trình tìm kiếm cho DSL tùy chỉnh trên toàn bộ tập. Vì Ainooson et al. (2023) báo cáo hiệu suất cao nhất trên toàn bộ tập đánh giá ARC, đây là baseline symbolic chính của chúng tôi. Mặc dù Mirchandani et al. (2023) và Gendron et al. (2023) sử dụng giao thức đánh giá khác, chúng tôi bao gồm những phương pháp này như baseline neural chính của chúng tôi, vì chúng dựa trên LLM mạnh mẽ (text-davinci và GPT-4).

3.1. Thiết lập
Chúng tôi khởi tạo tập huấn luyện của chúng tôi với 400 ví dụ từ phần chia huấn luyện ARC và các chương trình giải pháp liên quan được cung cấp bởi Hodel (2023). Chúng tôi cũng lấy mẫu 19,200 chương trình như dữ liệu huấn luyện bổ sung thông qua quy trình biến đổi được nêu trong Phụ lục A.2. Chúng tôi sử dụng các chương trình đúng cú pháp để khởi tạo tập huấn luyện tăng cường. Chúng tôi sử dụng 400 ví dụ đánh giá ARC như tập tìm kiếm của chúng tôi.

Trong giai đoạn lấy mẫu của mỗi meta-iteration, chúng tôi sử dụng lấy mẫu nhiệt độ với nhiệt độ τ= 0.95, và lấy mẫu lên tới nρ= 24 chương trình trên mỗi nhiệm vụ. Điều này khuyến khích khám phá và, do đó, tăng độ đa dạng của dữ liệu được thêm vào buffer phát lại. Chúng tôi từ chối các chương trình được lấy mẫu bởi chính sách nếu chúng không đúng cú pháp, hoặc nếu chúng chạy quá 0.25 giây trên mỗi dòng chương trình. Tất cả chương trình hợp lệ được thêm vào buffer phát lại.

Trong mỗi giai đoạn học, chúng tôi bắt đầu bằng cách lấy mẫu một tập hợp kinh nghiệm từ buffer theo phân phối được cho bởi các độ ưu tiên. Mỗi meta-iteration, chúng tôi lấy mẫu rt= 10,000 kinh nghiệm từ việc nối tập huấn luyện và tập huấn luyện tăng cường, và rp= 90,000 kinh nghiệm từ buffer. Tập kết quả được sử dụng cho 1 epoch huấn luyện. Để có danh sách đầy đủ siêu tham số, xem Bảng 3 trong Phụ lục.

3.2. Kết quả chính trên tập đánh giá ARC
Trong Hình 4, chúng tôi hiển thị hiệu suất như một hàm của số chương trình được lấy mẫu, cho CodeIt, các baseline tùy chỉnh của chúng tôi, Ainooson et al. (2023) và Ferr ´e (2023). Chúng tôi hiển thị hiệu suất tích lũy ở đây, có nghĩa là bất kỳ chương trình nào trong buffer hoặc tập huấn luyện tăng cường được coi là ứng viên giải pháp. Đối với các baseline biến đổi, chúng ta thấy tăng hiệu suất nhanh chóng tiếp theo là trì trệ. So với đó, CodeIt mất vài meta-iteration để bắt đầu tạo ra giải pháp bên ngoài tập huấn luyện tăng cường và sau đó hiệu suất tăng nhanh chóng. CodeIt nhanh chóng vượt trội hơn baseline biến đổi, cho thấy rằng nó thực sự tìm thấy các mẫu chất lượng cao hơn để huấn luyện.

Chúng tôi báo cáo hiệu suất cuối cùng của CodeIt sau 100 meta-iteration, và hiệu suất của các baseline khác nhau, trong Bảng 1. Để cho phép so sánh với Gendron et al. (2023), chúng tôi bao gồm kết quả trên tập "ARC Eval 412", xem mỗi ví dụ kiểm tra trong tập đánh giá ARC như một nhiệm vụ riêng biệt. Phương pháp của chúng tôi vượt trội hơn các phương pháp symbolic (Ainooson et al., 2023; Ferr ´e, 2021; 2023), mà còn cả các phương pháp neural dựa trên mô hình ngôn ngữ lớn (Gendron et al., 2023; Mirchandani et al., 2023), đạt được hiệu suất tốt nhất trên tập đánh giá ARC.

Để bối cảnh, chúng tôi hiển thị một giải pháp được viết bởi CodeIt cho một nhiệm vụ ví dụ trong Hình 5. Để minh họa thêm sự khác biệt giữa các chương trình được tìm thấy bởi CodeIt và các baseline biến đổi, chúng tôi phân tích các giải pháp được tìm thấy bởi mỗi phương pháp trong Phụ lục E.3, bao gồm so sánh định tính trong Bảng 4. Một phát hiện là có 29 nhiệm vụ mà cả CodeIt và baseline biến đổi đều tìm thấy giải pháp, nhưng có 23 nhiệm vụ mà chỉ CodeIt tìm thấy giải pháp, so với 13 cho baseline biến đổi. Đối với các nhiệm vụ mà cả hai phương pháp đều giải quyết, CodeIt tìm thấy các chương trình ngắn hơn trung bình và sử dụng các nguyên thủy khác nhau.

Trong Phụ lục E.4, chúng tôi quan sát CodeIt tinh chỉnh giải pháp ban đầu của nó cho 53% nhiệm vụ đã giải quyết, tạo ra giải pháp ngắn hơn trong meta-iteration sau. Hơn nữa, trong Phụ lục E.1, chúng tôi phân tích các trường hợp thất bại trên tập xác thực khách hàng và quan sát rằng CodeIt không giải quyết các nhiệm vụ với chương trình giải pháp dài hơn 11 dòng nhưng thường học sử dụng các nguyên thủy của chúng. Hơn nữa, trong Phụ lục E.2, chúng tôi xem xét các nguyên thủy DSL mà CodeIt học và thấy rằng một số loại nguyên thủy được học nhanh hơn các loại khác. Cuối cùng, trong Phụ lục C, chúng tôi thấy rằng CodeIt dường như hoạt động tốt nhất trên các nhiệm vụ liên quan đến tương tác đối tượng và tồi tệ nhất trên các nhiệm vụ dựa trên số học hoặc logic bằng cách phân tích hiệu suất trên tập dữ liệu ConceptARC (Moskvichev et al., 2023).

3.3. Ablation
Trong Hình 6 và 7, chúng tôi báo cáo hiệu suất tích lũy và hiệu suất chính sách theo thời gian cho CodeIt và tất cả ablation. Trong tất cả trường hợp, chúng tôi khởi tạo phương pháp với tập huấn luyện ARC, và sử dụng tập đánh giá ARC như tập tìm kiếm. Chúng tôi hiển thị kết quả ablation ở cuối huấn luyện trong Bảng 2. Chúng tôi cũng thực hiện nghiên cứu mở rộng với các kích thước mô hình khác nhau, kết quả được hiển thị trong Phụ lục D.

A1: Không ExIt Ablation này loại bỏ phản hồi chính sách, để cô lập đóng góp của Expert Iteration. Trong mỗi meta-iteration, thay vì điền buffer với mẫu chính sách, chúng ta lấy các chương trình được tạo ra trong meta-iteration đó của baseline biến đổi d1. Cho mỗi chương trình, chúng ta ngẫu nhiên chọn một nhiệm vụ từ tập tìm kiếm và thực hiện gán nhãn lại hồi tưởng, thêm bộ ba chương trình, đầu vào, đầu ra vào buffer. Chúng tôi lấy mẫu rp+rt= 100,000 kinh nghiệm từ việc nối tập huấn luyện, tập huấn luyện tăng cường và buffer tại mỗi meta-iteration để học. Chúng ta thấy rằng A1 vượt trội hơn baseline biến đổi, có nghĩa là học có giám sát từ kinh nghiệm biến đổi đơn thuần thực sự dẫn đến một số khái quát hóa giữa các nhiệm vụ. Tuy nhiên, hiệu suất tích lũy thấp hơn đáng kể so với CodeIt. Điều này nhấn mạnh tầm quan trọng của phản hồi chính sách.

A2: Không gán nhãn lại Chúng tôi kiểm tra hiệu ứng của gán nhãn lại hồi tưởng bằng cách chỉ thêm kinh nghiệm vào buffer nếu chương trình tạo ra đầu ra chính xác cho tất cả ví dụ minh họa. Chúng tôi huấn luyện trên tất cả kinh nghiệm trong buffer mà không lấy mẫu ưu tiên. Mặc dù hiệu suất tăng trong các meta-iteration sớm, A2 trì trệ sau khoảng 30 meta-iteration, cho thấy rằng dữ liệu được tạo ra bởi lấy mẫu và lọc đơn thuần là không đủ. Lấy mẫu và gán nhãn lại hồi tưởng (CodeIt) hoạt động tốt hơn lấy mẫu và lọc (A2).

A3: Không ưu tiên Để kiểm tra giả thuyết rằng lấy mẫu ưu tiên giảm thiểu quên thảm khốc, chúng tôi rút kinh nghiệm đều từ buffer trong giai đoạn học. A3 dẫn đến giảm nhỏ hiệu suất tích lũy, nhưng giảm lớn hiệu suất chính sách, cho thấy rằng chính sách thực sự quên các kinh nghiệm quan trọng. Lấy mẫu ưu tiên dẫn đến việc giữ lại kiến thức tốt hơn.

A4: Không tiền huấn luyện Để xác định liệu chính sách tiền huấn luyện của chúng tôi có chứa kiến thức tiên nghiệm có lợi hay không, chúng tôi khởi tạo lại ngẫu nhiên trọng số của chính sách ở đầu CodeIt. Hiệu suất chính sách cho thấy rằng cải thiện hiệu suất chậm hơn nhiều. Hơn nữa, khái quát hóa giữa các nhiệm vụ bắt đầu muộn hơn, như được hiển thị bởi hiệu suất tích lũy, chỉ bắt đầu tăng sau khoảng 50 meta-iteration. Mặc dù sự chậm lại mong đợi, việc thấy CodeIt dường như có thể bootstrap từ trọng số ngẫu nhiên là đáng khích lệ.

A5: Một demo Chúng tôi điều tra việc sử dụng biểu diễn nhiệm vụ bằng cách giảm số lượng ví dụ minh họa được hiển thị cho chính sách. Điều này dẫn đến giảm mạnh cả hiệu suất tích lũy và chính sách. Điều này cho thấy CodeIt hình thành trừu tượng trên nhiều ví dụ minh họa.

A6: Không biến đổi Trong ablation này, chúng tôi bỏ qua bước tăng cường dữ liệu huấn luyện dựa trên biến đổi. Chúng tôi quan sát rằng việc loại bỏ bootstrap dựa trên biến đổi dẫn đến huấn luyện chậm hơn, mặc dù hiệu suất có tăng theo thời gian và không trì trệ. Do đó chúng tôi phỏng đoán rằng tăng cường dựa trên biến đổi không thực sự bắt buộc nhưng vẫn hữu ích.

4. Công trình liên quan

4.1. Abstraction and Reasoning Corpus (ARC)
Nhiều công trình khác nhau đã áp dụng các phương pháp tổng hợp chương trình cho các tập con của tập dữ liệu ARC. Xu et al. (2022) đề xuất biểu diễn lưới như đồ thị, và áp dụng các chương trình logic cho các nút đồ thị, giải quyết 63 trong 160 nhiệm vụ. Kolev et al. (2020) áp dụng Differentiable Neural Computer cho ARC, giải quyết 78% nhiệm vụ với lưới có kích thước 10×10 và nhỏ hơn. Alford et al. (2022) áp dụng DreamCoder (Ellis et al., 2020) và tổng hợp chương trình được hướng dẫn thực thi, giải quyết 22 trong 36 nhiệm vụ được xem xét. Park et al. (2023) đầu tiên thu thập phản hồi của con người, sau đó thực hiện behavioral cloning cho một tập con nhiệm vụ ARC sử dụng decision transformer (Chen et al., 2021). Tuy nhiên, không có phương pháp nào trong số này được áp dụng trên toàn bộ tập đánh giá ARC, thường do hành vi mở rộng kém.

Một số ít công trình mở rộng đến toàn bộ tập đánh giá có xu hướng giải quyết từng nhiệm vụ một cách riêng biệt. Ferr ´e (2021) và công trình tiếp theo Ferr´e (2023) thiết kế DSL tùy chỉnh và thực hiện tìm kiếm nhanh cho mỗi nhiệm vụ. Ainooson et al. (2023) cũng thiết kế DSL tùy chỉnh và đạt được hiệu suất tốt nhất với tìm kiếm brute-force, giải quyết 36 trong 400 nhiệm vụ đánh giá. Mirchandani et al. (2023) và Gendron et al. (2023) chứng minh rằng một mô hình ngôn ngữ tiền huấn luyện với tokenizer tùy chỉnh sẽ xuất ra lưới chính xác sau khi được hiển thị nhiều cặp đầu vào-đầu ra, giải quyết 27 trong 400 và 49 trong 412 nhiệm vụ đánh giá tương ứng. Wang et al. (2023a) tăng cường phương pháp này bằng cách tạo ra giả thuyết trong nhiều vòng, mặc dù họ chỉ hiển thị hiệu suất trên một tập con của tập huấn luyện ARC do chi phí tiền tệ cao của việc truy vấn mô hình ngôn ngữ. Trong công trình này, chúng tôi thiết kế một phương pháp ExIt có thể mở rộng kết hợp mô hình ngôn ngữ nhỏ hơn với trừu tượng cấp cao hơn của DSL. Chúng tôi cũng đảm bảo rằng phương pháp của chúng tôi kết hợp kinh nghiệm để hưởng lợi từ khái quát hóa giữa các nhiệm vụ.

Nhiều phương pháp chưa được xuất bản cũng tồn tại, bao gồm các bài nộp cho thử thách ARC cũng như một cuộc thi Kaggle. Những cuộc thi này sử dụng bảng xếp hạng riêng tư, không được tiết lộ cho những người tham gia. Điều này có nghĩa là những người tham gia thường sử dụng tập đánh giá ARC công khai cho mục đích huấn luyện hoặc thiết kế DSL. Ví dụ, người chiến thắng Kaggle 2020 bình luận rằng tìm kiếm trong DSL được thiết kế sử dụng tập huấn luyện dẫn đến hiệu suất thấp, và hiệu suất cao hơn đạt được sau khi điều kiện hóa DSL trên các nhiệm vụ đánh giá (Icecuber, 2020). Điều này làm cho việc so sánh trực tiếp với các phương pháp được đánh giá trên tập đánh giá trở nên khó khăn. Để tham khảo, chúng tôi bao gồm tóm tắt kết quả cuộc thi trong Phụ lục F Bảng 7, tuy nhiên, lưu ý rằng tóm tắt này báo cáo hiệu suất trên tập kiểm tra ẩn, và kết quả cuộc thi không thể được so sánh trực tiếp với công trình này và tài liệu.

4.2. Expert Iteration
Expert iteration (ExIt) (Anthony et al., 2017) bao gồm giai đoạn tìm kiếm được hướng dẫn bởi chính sách thu thập kinh nghiệm mới, và giai đoạn học cải thiện chính sách bằng imitation learning. Các chuyên gia thường được sử dụng có xu hướng là các thuật toán tìm kiếm cây mạnh mẽ và tính toán chuyên sâu như Monte Carlo Tree Search (Kocsis & Szepesv ´ari, 2006) và greedy search (Daum ´e et al., 2009). ExIt đã đạt được hiệu suất siêu con người bao gồm các trò chơi (Silver et al., 2016; 2018; Anthony et al., 2017) và các vấn đề tổ hợp như bin-packing (Laterre et al., 2019). Công trình liên quan sử dụng gán nhãn lại hồi tưởng trong expert iteration là Ayg ¨un et al. (2021), Butt et al. (2022) và Gauthier & Urban (2022).

Các ứng dụng của ExIt cho lập trình-bằng-ví dụ (Mankowitz et al., 2023; Ellis et al., 2020) liên quan nhất đến CodeIt. Mankowitz et al. (2023) chỉ xem xét một nhiệm vụ: viết thuật toán sắp xếp nhanh. Đối với vấn đề này, khái quát hóa giữa các nhiệm vụ do đó không quan trọng bằng. DreamCoder (Ellis et al., 2020) liên quan nhất đến công trình của chúng tôi, vì phương pháp ExIt này được áp dụng cho nhiều nhiệm vụ lập trình-bằng-ví dụ. DreamCoder sử dụng DSL liên tục tăng trưởng để lưu trữ trừu tượng, và quy trình tìm kiếm tính toán chuyên sâu. Thay vào đó, CodeIt sử dụng mô hình để lưu trữ kiến thức chưng cất, và tạo ra kinh nghiệm thông qua lấy mẫu từ mô hình. Hơn nữa, DreamCoder lọc giải pháp dựa trên tính chính xác trong khi CodeIt sử dụng gán nhãn lại hồi tưởng và phát lại trải nghiệm ưu tiên.

4.3. Mô hình Ngôn ngữ Lớn Tự cải tiến
Công trình trước đó cho thấy rằng học từ dữ liệu tổng hợp là một chiến lược khả thi cho chứng minh định lý (Wang & Deng, 2020) và lập trình-bằng-ví dụ (Balog et al., 2017; Devlin et al., 2017; Bunel et al., 2018; Parisotto et al., 2017; Polosukhin & Skidanov, 2018; Zohar & Wolf, 2018), thường huấn luyện mô hình từ đầu. Thay vào đó, tinh chỉnh các mô hình ngôn ngữ lớn tiền huấn luyện (LLM) trên dữ liệu tổng hợp cho phép chuyển giao kiến thức do kiến thức miền tiên nghiệm được nắm bắt trong trọng số của chúng (Butt et al., 2022). Gần đây, các phương pháp sử dụng LLM để tổng hợp dữ liệu huấn luyện đã cho thấy thành công trong các miền tổng quát bao gồm chứng minh định lý (Polu et al., 2022), trả lời câu hỏi (Zelikman et al., 2022; Aksitov et al., 2023), suy luận toán học (Ni et al., 2023), dịch máy (Gulcehre et al., 2023), tạo mã từ ngôn ngữ (Zhou et al., 2023; Singh et al., 2023) và tạo mã từ mã (Haluptzok et al., 2022). Chúng tôi chứng minh trong công trình này rằng một phương pháp như vậy có thể được áp dụng cho miền ARC thách thức cũng vậy.

5. Thảo luận
Nhiều yếu tố khác nhau làm cho ARC đặc biệt thách thức đối với các phương pháp dựa trên học, ví dụ như số lượng hạn chế dữ liệu huấn luyện, và độ phức tạp của các nhiệm vụ riêng lẻ. Một vấn đề khác là các chương trình có thể khác nhau về số lượng ví dụ minh họa và chiều đầu vào, đòi hỏi các agent suy luận về các khái niệm ở các quy mô khác nhau. Trong công trình này, chúng tôi cho thấy rằng một phương pháp dựa trên expert iteration có thể học giải quyết 59/400 nhiệm vụ ARC chưa thấy. Ở đây, chúng tôi cung cấp trực giác về lý do tại sao CodeIt hoạt động tốt trên benchmark này.

Các ablation cho thấy rằng gán nhãn lại hồi tưởng có tác động lớn đến hiệu suất. Nhiều phương pháp expert iteration dựa vào sự xuất hiện của một chương trình giảng dạy của các nhiệm vụ ngày càng khó khăn, thậm chí tạo ra một chương trình giảng dạy bằng cách so sánh agent hiện tại với các phiên bản trước đó của chính nó (Silver et al., 2016; Fawzi et al., 2022) hoặc reward shaping (Laterre et al., 2019; Gulcehre et al., 2023). Gán nhãn lại hồi tưởng tạo thành một chương trình giảng dạy ngầm (Andrychowicz et al., 2017): ban đầu chúng ta thu thập các nhiệm vụ dễ có thể được giải quyết trong vài dòng mã, trong khi sau đó, các chương trình trở nên phức tạp hơn. Điều này hữu ích cho ARC, nơi việc đạt được thậm chí một nhiệm vụ đã giải quyết là thách thức.

Vì gán nhãn lại thêm nhiều chương trình vào buffer, bao gồm một số xa hơn so với các nhiệm vụ mục tiêu, chúng tôi đã sử dụng lấy mẫu ưu tiên để tránh quên thảm khốc.

Một hạn chế tiềm năng của CodeIt là đối với ARC, nó dựa vào các thành phần được thiết kế thủ công: một ngôn ngữ đặc thù miền (DSL), quyền truy cập vào một trình thông dịch để đánh giá tự động, và một tập hợp ban đầu các chương trình sự thật nền tảng. Mặc dù chúng tôi có lợi từ DSL được thiết kế bởi chuyên gia của Hodel, chúng tôi cũng cho thấy rằng một phương pháp neuro-symbolic (ablation A1) vượt trội hơn một phương pháp symbolic (baseline biến đổi), cho thấy rằng cả DSL và học đều đóng góp vào hiệu suất. Hơn nữa, CodeIt vượt trội hơn cả hai, cho thấy rằng ExIt khuếch đại hiệu ứng này. Chúng tôi cũng sử dụng LLM tiền huấn luyện và quy trình biến đổi để tăng tốc huấn luyện, nhưng các ablation cho thấy rằng huấn luyện có thể thậm chí không có những thứ này, mặc dù với tốc độ chậm hơn. Tuy nhiên, các phương pháp có thể bắt đầu học tabula rasa, hoặc hình thành DSL riêng của chúng (Ellis et al., 2020) vẫn là một lĩnh vực nghiên cứu quan trọng.

Đối với tập dữ liệu ARC, hiện tại có lợi khi kết hợp cả kiến thức tiên nghiệm (thông qua DSL hoặc LLM tiền huấn luyện) và kinh nghiệm (thông qua expert iteration). Chollet (2019) định nghĩa trí tuệ của một hệ thống là "một thước đo hiệu quả thu nhận kỹ năng của nó trên một phạm vi nhiệm vụ, liên quan đến tiền đề, kinh nghiệm, và độ khó khái quát hóa". Chollet đặt ra rằng, nếu hai hệ thống được khởi tạo với cùng kiến thức tiên nghiệm và trải qua cùng lượng kinh nghiệm liên quan đến một tập hợp nhiệm vụ chưa thấy, hệ thống thông minh hơn sẽ kết hợp kiến thức tiên nghiệm và kinh nghiệm của nó hiệu quả hơn, giải quyết nhiều nhiệm vụ hơn.

Mặc dù nhiều phương pháp hiện có kết hợp kiến thức tiên nghiệm thông qua ngôn ngữ lập trình hoặc DSL (Ainooson et al., 2023; Ferr ´e, 2023), một mô hình ngôn ngữ lớn tiền huấn luyện (Gendron et al., 2023; Mirchandani et al., 2023), hoặc cả hai (Wang et al., 2023a), chúng không thể kết hợp kinh nghiệm mới, và do đó không hưởng lợi từ khái quát hóa giữa các nhiệm vụ. Alford (2021) đề xuất một phương pháp expert iteration học từ kinh nghiệm, nhưng nó không mở rộng tốt cũng không hưởng lợi từ kiến thức tiên nghiệm trong chính sách của nó. Chúng tôi đặt ra rằng CodeIt là phương pháp expert iteration hiệu quả hơn do việc sử dụng các thành phần có thể mở rộng: mô hình ngôn ngữ tiền huấn luyện, huấn luyện dựa trên likelihood, và chạy chương trình trong trình thông dịch. Cũng có mối quan hệ ngầm giữa hiệu quả tính toán và kinh nghiệm: vì chính sách của CodeIt học trên miền ARC, có thể sử dụng mô hình ngôn ngữ nhỏ hơn nhiều so với ví dụ Gendron et al. (2023), những người sử dụng GPT-4 như một chính sách. Điều này phù hợp với tài liệu LLM cho thấy rằng dữ liệu huấn luyện chất lượng cao với chương trình giảng dạy cho phép LM nhỏ hơn cạnh tranh với những LM lớn hơn nhiều trên các nhiệm vụ mã hóa (Gunasekar et al., 2023).

6. Kết luận
Chúng tôi giới thiệu một phương pháp mới và có thể mở rộng cho mô hình ngôn ngữ tự cải tiến, CodeIt, sử dụng phát lại hồi tưởng ưu tiên. CodeIt đạt được hiệu suất tốt nhất trên Abstraction and Reasoning Corpus (ARC) so với các baseline symbolic và neural, giải quyết 59 trong 400 nhiệm vụ đánh giá. Các ablation cho thấy rằng gán nhãn lại hồi tưởng dẫn đến cải thiện hiệu quả mẫu dẫn đến cải thiện 40% hiệu suất. Chúng tôi cũng thấy rằng ưu tiên các kinh nghiệm quan trọng trong quá trình huấn luyện giảm thiểu quên thảm khốc. Ngoài ra, chúng tôi quan sát rằng CodeIt có thể tinh chỉnh giải pháp theo thời gian, xác định chương trình ngắn hơn cho 53% nhiệm vụ đã giải quyết trong các lần lặp sau. Kết quả chứng minh rằng mô hình ngôn ngữ tự cải tiến của chúng tôi có khả năng suy luận trong không gian chương trình và khái quát hóa giữa các nhiệm vụ. Đối với benchmark ARC thách thức, cả khả năng mở rộng và học từ kinh nghiệm đều chứng minh là các thành phần chính cho thành công.

Lời cảm ơn
Chúng tôi cảm ơn Michael Hodel đã tạo ra DSL của họ để giải quyết các nhiệm vụ ARC, có sẵn tại https://github.com/michaelhodel/arc-dsl .

Tuyên bố Tác động
Công trình này trình bày một phương pháp để huấn luyện chính sách dựa trên mô hình ngôn ngữ trên các vấn đề lập trình-bằng-ví dụ nơi dữ liệu khan hiếm. Phương pháp của chúng tôi học viết chương trình trong các lần lặp, và nhận phản hồi từ một trình thông dịch.

Một mặt, các mô hình ngôn ngữ lớn có tác động xã hội tiêu cực tiềm năng, làm trầm trọng thêm các thiên kiến có trong dữ liệu huấn luyện. Điều này có thể đặc biệt có hại nếu tập dữ liệu nhỏ, như trong công trình của chúng tôi. Ngoài ra, các mô hình tạo ra mã có thể được sử dụng để tự động hóa việc viết mã độc hại.

Mặt khác, cho phép sử dụng các mô hình ngôn ngữ lớn trên các vấn đề nơi dữ liệu khan hiếm mở ra các lĩnh vực ứng dụng nơi hiện tại khó triển khai chúng. Các mô hình tạo ra mã từ đặc tả đầu vào-đầu ra có thể áp dụng rộng rãi và có thể được sử dụng trong nhiều thiết lập lập trình.

Tài liệu tham khảo
[Tiếp tục với danh sách tài liệu tham khảo được dịch...]
