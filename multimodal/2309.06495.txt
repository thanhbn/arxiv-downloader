# 2309.06495.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.06495.pdf
# File size: 1185257 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
AGIB ENCH : A M ULTI -GRANULARITY , MULTIMODAL ,
HUMAN -REFERENCED , AUTO-SCORING BENCHMARK FOR
LARGE LANGUAGE MODELS
EDITED BY
FEITANG
WANLING GAO
LUZHOU PENG
JIANFENG ZHAN
BenchCouncil: International Open Benchmark Council
Chinese Academy of Sciences
Beijing, China
http://www.benchcouncil.org/agibench
TECHNICAL REPORT NO. BENCH COUNCIL -AGIB ENCH -2023
AUG22, 2023arXiv:2309.06495v1  [cs.CL]  5 Sep 2023

--- PAGE 2 ---
AGIBench: A Multi-granularity, Multimodal, Human-referenced,
Auto-scoring Benchmark for Large Language Models
Fei Tang1,3, Wanling Gao1,2,3, Luzhou Peng4, and Jianfeng Zhan *1,2,3
1Research Center for Advanced Computer Systems, State Key Lab of Processors, Institute of
Computing Technology, Chinese Academy of Sciences, {tangfei, gaowanling,
zhanjianfeng}@ict.ac.cn
2BenchCouncil (International Open Benchmark Council)
3University of Chinese Academy of Sciences
4Shanghai Lixin University of Accounting and Finance
Aug 22, 2023
Abstract
Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to
evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but
challenging issue. First, the question-solving abilities are interlaced with different ability branches like
understanding and massive knowledge categories like mathematics. Second, the inputs of questions
are multimodal that may involve text and images. In addition, they may have varying levels of
difficulty while lacking a unified standard to judge which one is more difficult. Third, the response
format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. Even
though several benchmarks have been proposed to evaluate LLMs, however, a majority of them
only evaluate and report the performance on a collection of blended text questions that may involve
multiple ability branches, knowledge categories, or different difficulty levels. Thus, the evaluation
may result in indistinguishable performance data and a biased optimization direction. A small fraction
of efforts attempt to classify the questions according to the ability branches, however, they still provide
coarse-grained benchmarking such as lacking detailed knowledge categories and human-referenced
difficulty levels. In addition, existing efforts either adopt prompt engineering to normalize the response
format or analyze the result manually, which would introduce unpredictable performance impacts or
unacceptable evaluation costs.
In this paper, to tackle the above challenges, we propose AGIBench – a multi-granularity, mul-
timodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a
collection of blended questions, AGIBench focuses on three typical ability branches and adopts a
four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question. First,
it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, per-knowledge,
per-modal, per-dataset, and per-difficulty level granularities. Second, it contains multimodal input,
including text and images. Third, it classifies all the questions into five degrees of difficulty according
to the average accuracy rate of abundant educated humans (human-referenced). Fourth, it adopts zero-
shot learning to avoid introducing additional unpredictability and provides an auto-scoring method to
extract and judge the result. Finally, it defines multi-dimensional metrics, including accuracy under
the average, worst, best, and majority voting cases, and repeatability. Our experiments on twelve
state-of-the-art LLMs show the effectiveness of our benchmark. AGIBench is publically available
from https://www.benchcouncil.org/agibench .
*Jianfeng Zhan is the corresponding author.
1

--- PAGE 3 ---
1 Introduction
Intelligence is an abstract concept and has no unified definition yet [ 8]. Research about human intelligence
has been conducted for decades and formed a series of theories, e.g., triarchic theory of intelligence [ 13],
fluid and crystallized intelligence [ 2], theory of multiple intelligences [ 5], etc., while having no unified
standard about how to evaluate human intelligence. In this condition, the difficulties aggravate evaluating
artificial intelligence (AI), which has shown powerful abilities to solve problems or questions and reflects
the tremendous potential to approach human intelligence, especially the emerging large language models
(LLMs) like ChatGPT. Different from the previous AI applications that mainly target a single task or
a specific domain, LLMs anticipate achieving general intelligence. Hence, the previous benchmarking
methodologies that focus on specific tasks or application domains are not applicable anymore. A new
benchmarking methodology is a necessity but no easy feat.
On the one hand, how to construct a benchmark with diverse, typical, and difficulty-differentiated
input questions is challenging. Similar to human intelligence, the intelligence of LLMs has no unified
benchmarking standard since the input questions or problems to be solved are massive and interlaced.
First, the input questions or problems may not only involve multiple ability branches like understanding
and reasoning but also involve numerous knowledge categories with varying levels of difficulties like
mathematics and geography. Second, the input questions or problems are multimodal with a combination
of text and images, for example, geometry problems in mathematics, talking about pictures in linguistics,
etc.
On the other hand, how to evaluate the benchmarking results and choose comprehensive and important
metrics are challenging. From the perspective of analyzing the results, the response formats of LLMs are
diverse. For example, the response may (1) answer the choice from four choices marked A, B, C, and
D, with an explanation and analysis of the above four choices one by one; (2) only answer the choice
without an explanation; (3) answer the choice with a partial explanation. In addition, the orders of choices
and explanations vary, and the text words are discrepant. Even though a human being can distinguish the
result easily, however, it is extremely hard for a computer program. From the perspective of the evaluation
metrics, the performance of LLMs is multidimensional and not merely average accuracy. For example, the
LLMs may be adept in specific ability branches, knowledge categories, or difficulty levels; the response
results may change during multiple runs, etc.
Many efforts have been proposed to benchmark the LLMs [ 12,9,19,20,7,18]; however, they fail to
solve the above challenges. First, almost all adopt one or more open-source datasets with a collection of
blended text questions at a per-dataset or per-ability branch granularity. However, these blended questions
may cover different knowledge categories and different difficulty levels. We may hardly distinguish
the performance on each ability branch, knowledge, or difficulty level, except for a total score on a
dataset. Furthermore, even some of those benchmarks [ 20] provide different difficulty levels; however,
these difficulty levels are applied to a per-dataset granularity, which means the overall difficulty of the
entire question dataset while not a specific question. Second, to solve the challenges of analyzing the
response results, most of them [ 9,20,7] adopt prompt engineering like few-shot and chain-of-thought
(CoT) to increase the accuracy or normalize the response format. However, our experiments and the
related work show that no matter few-shot or CoT would introduce unpredictable performance impacts.
Our experiments especially show that with few-shot, the accuracy variance achieves to 5% when using
different random seeds. C-Eval [ 7] shows that with CoT, the accuracy increases on some models (e.g.,
ChatGLM-6B, +3%) while decreases on other models (e.g., Chinese-LLaMA-13B, -11.9%). Some of
the efforts [ 18] adopt a zero-shot approach to avoid the unpredictable impact, however, they have to
analyze the response result manually, which results in unacceptable evaluation costs. Third, most of those
efforts only report average accuracy and lack a comprehensive and multidimensional evaluation. While
HELM [ 9] incorporates multiple metrics, it omits human-referenced evaluation and does not include a
multimodal dataset.
2

--- PAGE 4 ---
Table 1: Observations and Implications for LLMs using AGIBench.
Observations Implications
Multi-granularity: (1) ChatGLM v2-6B outperforms ChatGLM-130B even
with less parameters; (2) LLMs reflect good common sense (e.g., GPT-4 outper-
forms humans by 15.84%) and understanding ability (e.g., GPT-4 is comparable
with humans) while poor reasoning ability (e.g., GPT-4 underperforms humans
by 25.94%). (3) GPT-4 has the highest performance on most ability branches
and knowledge categories.(1) Architecture improve-
ment and high-quality train-
ing data are more pivotal
than large model size; (2)
Reasoning ability is a direc-
tion of optimization.
Multimodal: (1) The image understanding and reasoning abilities are poor for
the four LLMs that support open image access. (2) For LLMs that have no image
processing ability, a majority of the responses cannot admit the limitation and
output hallucinational and nonsense responses.(1) The multimodal abili-
ties need optimizations.
Human-referenced Difficulty: (1) Humans perform better than LLMs on simple
questions (i.e., Level 1 to 3) while worse on difficult questions (i.e., Level 4
and 5). (2) The accuracy of GPT-4 is higher than humans for common sense,
comparable for understanding, and worse for reasoning.(1) The solving abilities of
simple questions and rea-
soning need optimizations.
Multi-dimensional metrics: (1) The worst-case accuracy of LLMs is signifi-
cantly below the corresponding average one, which means the model does not
always give a correct answer during three times evaluations, indicating a poor
reliability of LLMs. (2) The best-case accuracy of LLMs is much higher than
the other cases, which means the model has a high probability to give a correct
answer during three evaluations. (3) The majority voting accuracy is similar to
the average one, which means most of the time, the model can give a correct
answer.(1) The reliability of LLMs
needs optimization.
This paper proposes AGIBench – a multi-granularity, multimodal, human-referenced, and auto-scoring
benchmarking methodology and benchmark for LLMs. Instead of a collection of blended questions,
AGIBench focuses on fundamental ability branches and adopts a four-tuple <ability branch, knowledge,
difficulty, modal> to label the attributes of each question. In total, AGIBench provides 927 questions,
covering three kinds of ability branches, i.e., common sense, reasoning, and understanding, 20 knowledge
categories and 68 knowledge subclasses. First, it supports multi-granularity benchmarking, e.g., per-
question, per-ability branch, per-knowledge, per-difficulty level, per-dataset, and per-modal granularities.
Second, it contains multimodal input, including various contexts like text-only, image-only, text with
images, text with tables, and a combination of text, images, and tables. Third, it classifies all the questions
into five degrees of difficulty according to the average accuracy rate of abundant educated humans
(human-referenced). Fourth, it adopts zero-shot learning to avoid additional unpredictability and provides
an auto-scoring method to extract and judge the result. Finally, it defines multi-dimensional metrics,
including accuracy under the average, worst, best, and majority voting cases, and repeatability. Our
experiments on twelve state-of-the-art LLMs show the effectiveness of our benchmark. Table 1 presents
the main observations and implications of LLMs using AGIBench.
2 Related Work
Many efforts have been proposed to evaluate traditional natural language processing (NLP) algorithms and
LLMs. GLUE [ 15] and SuperGLUE [ 14] are benchmarks for traditional NLP tasks, primarily evaluating
understanding capabilities. However, the questions are limited in scope and mainly focus on sentence
classification, which cannot comprehensively reflect the complexities of human language [ 11]. Thus,
these benchmarks cannot meet the need for evaluating LLMs.
For evaluating LLMs, from the perspective of the question dataset, most of the related work uses
closed-ended questions since the answer is definitive without subjectivity. BIG-Bench [ 12] and HELM [ 9]
use a combination of multiple datasets like MATH [ 6] and GSM8K [ 3]. On the one hand, such an approach
may incur question redundancy and result in limited coverage. On the other hand, they only support the
benchmarking at the per-dataset or per-ability branch granularity and thus cannot reveal the abilities from
different perspectives. For example, they only output a score on a specific dataset with blended questions
that involve different difficulty levels. Hence, we can hardly know the ability for a specific knowledge
category or a specific difficulty level. ScienceQA [ 11] collects questions from elementary and high school
science curricula. However, these questions are too easy for humans. AGI-Eval [ 20], C-Eval [ 7], and
3

--- PAGE 5 ---
GAOKAO [ 18] focus on the LLMs benchmarking using the Chinese language. They utilize exams like
national civil service and college entrance exams as question datasets. However, these benchmarks only
use average accuracy as the evaluation metric, which is overly simplistic. Several efforts attempt to use
open-ended questions for LLMs benchmarking. Chandrasekaran et al. [ 1] collect a series of open-ended
and multimodal questions to evaluate GPT-4; however, evaluating and scoring the results is extremely
hard. In addition, the evaluation results are hard to reproduce, considering the subjectivity of different
persons.
From the perspective of evaluation and scoring method, a majority of the existing efforts adopt prompt
engineering for evaluation, such as few-shot learning [ 16] and Chain-of-Thought (CoT) [ 17]. Although
these methods have been proven to be the potential to standardize the format of the response result or
increase model accuracy, however, C-Eval [ 7] and our experiments show that they may not always be
effective and would introduce unpredictable performance impacts. In this condition, we cannot evaluate
the abilities of LLMs accurately since the response results may be impacted by prompt engineering.
Additionally, using these methods requires case-by-case tuning, which exacerbates the benchmarking
costs and cannot assure the fairness of benchmarking. MT-Bench [ 19] explores the use of GPT-4 as an
approach to judge the correctness of the response results of LLMs and compare it with human judgment.
They find that the decisions from GPT-4 and humans have an 80% similarity. However, MT-Bench only
selects 80 questions and thus has limited representativeness. Additionally, using GPT-4 directly for scoring
would incur huge labor costs since we still need to check the judgment for every question.
3 The Design and Implementation
This section illustrates the design and implementation of AGIBench. Section 3.1 describes the bench-
marking methodology. Section 3.2 presents an AGIBench overview.
3.1 Methodology
To evaluate the different question-solving abilities of LLMs and their degrees of intelligence, we
adopt a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology,
covering the question dataset construction, evaluation methodology, and evaluation metrics.
First, to support multi-granularity benchmarking instead of only per-dataset or per-ability branch
benchmarking adopted in the related work, we use a four-tuple <ability branch, knowledge, difficulty,
modal> to label the attributes of each question. The ability branch focuses on the most fundamental and
essential abilities like understanding. Knowledge includes a broad spectrum of knowledge categories
within each ability branch, e.g., passage reading within understanding ability. Difficulty indicates a
question’s difficulty level, i.e., Levels 1 to 5, from easy to difficult. Modal indicates the modal of a
question like text or image.
Second, to construct a representative and typical question dataset, we aim to cover many questions
with diverse and varied attributes. Specifically, we choose the three most fundamental ability branches:
common sense, understanding, and reasoning. We single out twenty comprehensive and representative
knowledge categories for these three ability branches in total. Knowledge for common sense covers six
categories: humanities, technology, law, geography, politics, and economy; Knowledge for understanding
includes passage reading, sentence grammar, fill-in-the-blank, and long text reading; Knowledge for
reasoning includes graphical reasoning, definition judgment, comprehensive materials, tabular materials,
textual materials, graphical materials, analogical reasoning, logical judgment, mathematical calculation,
and numerical reasoning. For the difficulty attribute of each question, we adopt a human-referenced
methodology based on big data statistics, which uses the accuracy rate answered by millions of well-
educated humans to label the question’s difficulty level. For example, Level 1 is the easiest one with
an 80% to 100% accuracy rate, which means 80% to 100% of millions of humans can give a correct
answer. Level 2 has a 60% to 80% accuracy rate. Level 3 has a 40% to 60% accuracy rate. Level 4
4

--- PAGE 6 ---
has a 20% to 40% accuracy rate. Level 5 is the most difficult and has a 0% to 20% accuracy rate. We
cover multimodal input that encompasses various contexts, including text-only, text with tables, text with
images, image-only, and a combination of text, images, and tables.
Third, to cope with the diversity of the response format and avoid the unpredictable performance
impacts of prompt engineering, we do not use prompt engineering and attempt to find a series of regex
patterns to extract the answers and perform judgment.
Fourth, we define multi-dimensional metrics for LLMs benchmarking: accuracy under different cases
and repeatability.
3.2 AGIBench Design and Implementation
We design and implement the AGIBench based on the methodology, including the question dataset
construction, evaluation and scoring, and metrics.
Question Dataset. AGIBench selects questions related to human life, especially for the Chinese.
We mainly choose the questions from national civil service examinations since they satisfy the diversity
and fundamentality requirements. We use a four-tuple <ability branch, knowledge, difficulty, modal>
to label the attributes of each question. The ability branch covers common sense, understanding, and
reasoning abilities. The knowledge contains 20 categories and 68 subclasses covering humanities, physics,
chemistry, economics, law, politics, culture, geography, history, engineering, mathematics, etc. Tab. 2
shows the ability branch and knowledge attributes of the AGIBench dataset. Regarding difficulty, our
dataset uses human accuracy as the reference, and the accuracy is based on the high-educated human.
We carefully select the difficulty of questions and classify five levels from Levels 1 to Level 5. A higher
number means a more challenging degree. From Level 1 to Level 5, the responding human accuracy is
[80%, 100%], [60%, 80%), [40%, 60%), [20%, 40%), and [0%, 20%). Fig. 1 shows the distribution of
difficulty levels of AGIBench. As for the modal, we include multimodal input covering various contexts,
i.e., text-only, image-only, text with images, text with tables, and a combination of text, images, and tables.
The corresponding number of questions for these contexts are 863, 5, 38, 10, and 11, respectively. In total,
we provides 927 questions. Note that the image is processed as a URL in the questions. Additionally, the
text dataset contains plain text, complex mathematical formulas, and table data. We adopt the latex format
for mathematical formulas, and for table data, we use the markdown format.
5

--- PAGE 7 ---
Table 2: Ability Branch and Knowledge Attributes of Question Dataset in AGIBench.
Ability Branch Knowledge Knowledge Subclass (Percentage)
Common SenseEconomics Economics (1.08%)
Geography Environmental (0.76%), National and Social Conditions (1.08%), Natural
(1.08%)
Humanities Chinese History (1.08%), Cultural (1.08%), Literary (1.08%), World History
(1.08%)
Law Administrative Law (1.08%), Civil Law (1.08%), Commercial and Economic
Law (1.08%), Constitutional Law (1.08%), Criminal Law (0.97%), Jurispru-
dence (1.08%), Procedural Law (2.16%)
Politics Politics (1.08%)
Technology Biology Fundamentals (1.08%), Chemistry Fundamentals (1.08%), Everyday
Knowledge (1.08%), Physics Fundamentals (1.08%), Technology Theories and
Achievements (1.08%)
ReasoningAnalogical Reasoning Grammatical Relations (0.86%), Logical Relations (5.07%), Semantic Relations
(3.02%)
Comprehensive Materials Comprehensive Materials (1.08%)
Definition Judgment Multiple Definitions (1.08%), Single Definition (1.08%)
Graphic Materials Graphic Materials (1.08%)
Graphic Reasoning Attribute Principles (0.86%), Pattern Principles (1.83%), Positional Principles
(1.94%), Quantity Principles (1.4%), Spatial Reconstruction (0.22%), Special
Principles (0.86%)
Logical Judgment Combinations and Arrangements (1.08%), Daily Conclusions (1.08%), Reason
Explanations (0.97%), Strengthened Types (1.08%), Translation Reasoning
(1.08%), True-False Reasoning (0.97%), Weakened Types (1.08%)
Mathematical Calculation Core Methods (5.18%), Economic Profit and Comprehensive Planning (2.91%),
Engineering (1.08%), Inclusion-Exclusion Principle (12.51%), Journey (1.08%),
Permutation, Combination, and Probability (2.16%), Solution Problems (0.97%)
Numerical Reasoning Basic Sequences (0.76%), Exponential Sequences (0.97%), Fractional Se-
quences (1.08%), Mechanical Split Sequences (1.94%), Multi-level Sequences
(1.08%), Multiple Sequences (0.54%), Recursive Sequences (1.08%)
Tabular Materials Tabular Materials (1.08%)
Textual Materials Textual Materials (1.08%)
UnderstandingFill-in-the-blank Content Word Fill-in-the-blank (1.08%), Idiom Fill-in-the-blank (1.08%),
Mixed Fill-in-the-blank (1.08%)
Long Text Reading Long Text Reading (1.08%)
Passage Reading Central Understanding (1.62%), Detail Judgment (1.08%), Sentence Under-
standing (1.08%), Title Insertion (1.08%)
Sentence Grammar Flawed and Ambiguous (1.08%), Following Sentence Choice (1.08%), Sentence
Fill-in-the-blank (1.08%), Sentence Ordering (1.08%)
We further consider the length of questions. Fig 2 shows the length distribution of questions, and our
dataset covers a broad spectrum. The size of most questions is more significant than 100 while many
others are less than ten words [ 10]. And our dataset contains several long-length questions whose length
exceeds 1000, which also shows the difficulty and variety of our dataset.
Evaluation and Scoring. To avoid the impact of prompt engineering and meanwhile essentially
reduce labor costs, we adopt an auto-scoring methodology that combines a heuristic regular expression
searching algorithm (HRE for short) and GPT-4. On the one hand, we use an HRE algorithm to search
the regex patterns, as shown in Algorithm 1. We repeat N iterations and randomly select M responses
from the total responses during each iteration. For the M responses of each iteration, we set a threshold
“minimum_limit", indicating the minimum number of occurrences that a response format can be added as
6

--- PAGE 8 ---
Level 1 Level 2 Level 3 Level 4 Level 5
Hard Level0100200300CountFigure 1: The Distribution of Five Difficulty Levels. Using the accuracy rate answered by millions of
well-educated humans as references.
0 250 500 750 1000 1250 1500 1750
Problem Length0.0000.0020.004Density
Figure 2: The Length Distribution of the Question Dataset in AGIBench.
a new regex pattern. After that, we obtain a set that contains frequently occurring regex patterns. Our
evaluation statistics in Section 4.2 verify the effectiveness of the HRE algorithm. Among hundreds of
thousands of responses from LLMs, about 67% of the response results can be adequately extracted using
HRE. On the other hand, in terms of the remaining small fraction that cannot be extracted by HRE, we use
GPT-4 to extract the results. Note that we do not use GPT-4 directly because its accuracy cannot achieve
100%, and we still need to verify the judgments artificially. By adopting HRE, we reduce about seventy
percent labor costs.
Metrics. We collect many metrics to evaluate LLMs comprehensively, not merely average accuracy,
which is the only metric for most related work. We widely include the average accuracy, the worst-case
accuracy, the best-case accuracy, the majority voting accuracy, and the repeatability to indicate the
performance of LLMs under different cases. We detail these metrics as follows. Note that we evaluate
each LLM using a question for three times to assure the fairness and the reliability of benchmarking. For
each time, if the answer is correct, the score is 1, otherwise 0.
(1) Average accuracy. For the three times’ evaluations on an LLM using the same question, we use
the average score as the score of the LLM for that question. Then we calculate the average score for all
questions as the final average accuracy.
(2) The worst-case accuracy. Different from the average accuracy, if all three times’ evaluations give
the correct answer, the score is 1, otherwise 0. Then the average value on all questions is reported as the
final worst-case accuracy.
(3) The best-case accuracy. More relaxed compared to the worst-case one, if great than or equal to
one answer gives the correct answer, the score is 1, otherwise 0.
(4) The majority voting accuracy. If at least two answers are correct, the score is 1, otherwise 0.
(5) Repeatability. The similarity of the responses during three different runs. A high similarity
indicates good repeatability.
7

--- PAGE 9 ---
Algorithm 1 Heuristic regular expression searching (HRE)
1:pattern_sets ←[]
2:fori←1toNdo
3: sampled_responses ←sample(total_responses, M)
4: while max_number_of_common_patterns(sampled_responses) >minimum_limit do
5: temp_pattern ←propose_pattern(sampled_responses)
6: patched_to_pattern_sets ←False
7: forpattern ∈pattern_sets do
8: ifcan_patch(pattern, temp_pattern) then
9: pattern ←patch(pattern, temp_pattern)
10: patched_to_pattern_sets ←True
11: break
12: end if
13: end for
14: ifnot patched_to_pattern_sets then
15: pattern_sets.append(temp_pattern)
16: end if
17: sampled_responses ←sampled_responses −match(sampled_responses, temp_pattern)
18: end while
19:end for
4 Evaluation
This section presents the evaluation, including the evaluation methodology (Section 4.1), experiment
setup 4.2, and evaluation results 4.3.
4.1 Evaluation Methodology
4.1.1 LLMs Overview
We choose representative LLMs with different underlying technology, differentiated model sizes, and
state-of-the-art performance. We also consider additional training data, architectures, target purposes,
and whether open sourced. Specifically, we choose 12 models from OpenAI, Anthropic, Meta, Tsinghua,
Baidu, Alibaba, and iFlytek, including GPT-3.5, ChatGPT, and GPT-4 from OpenAI, Claude from
Anthropic, LLaMA-13B and Vicuna-13B (LLaMA based) from Meta, ChatGLM-6B, ChatGLM v2-6B,
and ChatGLM-13B from Tsinghua, Ernie from Baidu, Qianwen from Alibaba, and Spark from iFlytek.
The model size ranges from 6 billion to 175 billion. The detailed information is listed in Table. 3.
4.1.2 Evaluation Method
We do not use prompt engineering like few-shot learning and chain-of-thought (CoT) to avoid unpredicted
impacts, verified by related work [ 7]. They find the CoT performs better on some models (e.g., ChatGLM-
6B, +3%) and worse on others (e.g., Chinese-LLaMA-13B, -11.9%). We also conduct an experiment to
evaluate the impact of few-shot learning. On ChatGLM v2-6B, we randomly select several question-and-
answer pairs as examples to evaluate the impact. Then we use different random seeds and run them three
times for each seed. Table. 4 shows the result. The bold text shows the best results (about 37.54%), and
the underline text shows the worst results (about 32.69%). The accuracy gap using different random seeds
is large, achieving about 5%.
8

--- PAGE 10 ---
Table 3: The Overview of Evaluated LLMs.
Model Model Size Training Data Composition Temperature Developer Open Source Access
ChatGLM-6B 6 billion Chinese and English 0.5 / 1 Tsinghua Yes Local
ChatGLM v2-6B 6 billion Chinese and English 0.5 / 1 Tsinghua YES Local
ChatGLM-130B 130 billion Chinese and English Not support Tsinghua No Web
GPT-3.5 175 billion Primarily English 1.0 / 2 OpenAI No API
ChatGPT 175 billion Primarily English 1.0 / 2 OpenAI No API
GPT-4 Unknown Primarily English 1.0 / 2 OpenAI No API
Claude Unknown Primarily English Not support Anthropic No Web
LLaMA-13B 13 billion Primarily English 0.5 / 1 Meta Yes Local
Vicuna-13B 13 billion Primarily English 0.5 / 1 UC Berkeley et al. Yes Local
Ernie 175 billion Chinese and English Not support Baidu No Web
Qianwen Unknown Chinese and English Not support Alibaba No Web
Spark Unknown Chinese and English Not support iFlytek No Web
Table 4: Few-shot experiment on ChatGLM v2-6B using different random seeds and run three times for
each seed. Bold text represents the best results, and underlined test represents the worst results.
Seed Run #1 Run #2 Run #3
0 37.54% 37.54% 37.32%
1 33.44% 33.33% 34.52%
2 32.15% 34.84% 32.69%
3 35.28% 35.49% 34.41%
4 33.23% 33.01% 33.33%
4.2 Experiment Setup
We locally deploy ChatGLM-6B and ChatGLM v2-6B on a single NVidia V100 GPU and deploy
LLaMA-13B and Vicuna-13B on four NVidia 1080 Ti GPUs. For close-sourced models that provide
API access, we perform evaluations using their APIs, including GPT-3.5, ChatGPT, and GPT-4. For
other close-sourced models which do not offer API access but provide web-based products, we perform
evaluations by simulating user input in the browser, including Claude, ChatGLM-130B, Ernie, Qianwen,
and Spark. To ensure fairness, we set the temperature parameter as half of the maximum value of the
model for all twelve models, except for the web-based products, which do not provide the interface to set
the temperature parameter.
For the response extraction and judgment, we use the HRE algorithm illustrated in Algorithm 1 and
GPT-4 to avoid the unpredictable performance impacts of prompt engineering. We set N as 10, M as
100, and minimum_limit as 2, which means repeating 10 iterations and randomly selecting 100 responses
during each iteration. If a response format occurs at least two times, we will add this pattern. We collect
66744 responses from LLMs, and the regex patterns identified by the HRE algorithm are shown in Table 5.
These patterns support extracting and judging 67% answers of the total responses, demonstrating the
effectiveness of our algorithm. The remaining 33% responses that have no unified regex pattern are
processed by GPT-4, and the judgments are manually checked. We use the chain-of-thought (CoT) prompt
shown in Fig. 3 to fix the judgment output of GPT-4 when extracting the answers. Note that this prompt is
used to judge the correctness of the remaining responses without impacting the model evaluation process.
4.3 Evaluation Results
We report the evaluation results including multi-granularity, multimodal, and human-referenced
difficulty level, and response repeatability benchmarking. Further, using comprehensive metrics, we
evaluate the accuracy of LLMs under different cases.
9

--- PAGE 11 ---
Table 5: Regex Patterns Identified by HRE Algorithm.
Regex Number Ratio
Not Matching 32875 33%
【?答案】?(?:和原因)?(?:为|(?:应该)?是|选择)?[:：]?\s?(?:选项)?([A-Z])[^A-Z]*?(?: 。|$) 29508 29%
^(?:选项\s?)?([A-Z])[\. 。，\s][^A-Z]*?$ 20038 20%
选择?：?\s?([A-Z])(?: 选项)?[^A-Z]*?(?: 。|$) 6179 6%
(?:[Aa]nswer|statement|[Ss]olution)\s?(?:to (?:your|this) question)? is:? (?: option )?\"?([A-Z])\"?(?:\.| 。|:|$) 3706 4%
作为一个人 工智能语言模型，我还没学习如何回答这个问题，您可以向我问一些其它的问题，我会尽力帮您解决的。 2299 2%
^(?:答案[：:]\s?)?([A-Z])(?:\.| 。|\s)?$ 2173 2%
^(?:选项\s?)?([A-Z])[\. 。，\s](?:[^A-Z]|(\1))*?$ 1765 2%
(?:选项)?\s?([A-Z])\s?[ 是|为]正确答案 683 1%
^(?:答案[：:]\s?)?((?:[A-Z]\s?[ 、\s\.-]?\s?)+) 。?$ 458 0%
^\s*$ 432 0%
问题：`{question}`回答：`{response}`仅根据以上问题和答案，不要引⼊其他知识，不要判断对错，从这个回答中可以看出选择了哪个选项？逐句分析并在最后按照`选项：A、B、C、D`或`选项：⽆`的格式给出选项Chain-of-Thought PromptingFixed Format OutputQuestion: `{question}`Answer: `{response}`Based on the above question and answer only, without introducing any other knowledge and without judging right or wrong, which option has been chosen from this response? Analyze sentence by sentenceand give the options at the end in the format `Option:A,B,C,D` or `Option:None`Tricks:
Figure 3: The Prompt Used to Extract and Judge Answers.
4.3.1 Multi-granularity Benchmarking
Table 6 shows the multi-granularity benchmarking results at a per-dataset, per-ability branch, and per-
knowledge granularities. Note that the “Human" column indicates human accuracy as a reference. The
average accuracy on the whole dataset is 60.91%. GPT-4 outperforms others largely on many ability
branches and knowledge categories. Most LLMs have better understanding abilities than common sense
and reasoning abilities, and the reasoning ability performs the worst. Compared to the human accuracy,
GPT-4 outperforms humans by 15.84% for the common sense branch and has similar accuracy to humans
(73.16% vs. 74.82%) for the understanding branch, while performing worse than humans for the reasoning
branch with a gap of 25.94%.
00.10.20.30.40.50.60.70.80.91
Level 1Level 2Level 3Level 4Level 5AccuracyHumanChaGLM-6BChatGLM v2-6BClaudeErnieChatGPTGPT-4LLaMA-13bGPT-3.5Vicuna-13BChatGLM-130BQianwenSpark
Figure 4: The Average Accuracy on Five Difficulty Levels. Level 1 is the easiest, and Lever 5 is the most
difficult. The human label means the human accuracy on that level.
4.3.2 Multimodal Benchmarking
We mainly perform the multimodal benchmarking on Ernie, ChatGLM-130B, Qianwen, and Spark
models, since they provide Internet connectivity and can access images. For other eight models that do not
10

--- PAGE 12 ---
Table 6: Multi-granularity Benchmarking Results.
Ability Knowledge ChatGLM ChatGLM v2-6B ChatGLM-130B GPT-3.5 ChatGPT GPT-4 Claude LLaMA-13B Vicuna-13B Ernie Qianwen Spark Human
Common SenseAvg 32.15% 46.30% 36.99% 34.88% 40.28% 65.84% 36.47% 23.25% 23.97% 31.28% 17.23% 19.65% 50.00%
Humanities 31.67% 41.67% 28.06% 19.17% 36.94% 56.11% 34.44% 18.33% 18.89% 25.56% 17.22% 9.72% 48.54%
Technology 33.78% 47.11% 46.00% 36.89% 48.00% 75.78% 36.22% 28.44% 26.22% 35.56% 22.22% 37.78% 51.10%
Law 29.25% 44.16% 31.50% 35.30% 33.61% 57.81% 33.47% 22.22% 25.74% 30.52% 13.50% 8.72% 49.00%
Geography 36.63% 52.67% 44.03% 32.10% 43.21% 79.42% 34.98% 21.40% 20.99% 31.28% 23.87% 35.39% 49.43%
Politics 26.67% 65.56% 41.11% 82.22% 54.44% 68.89% 56.67% 22.22% 18.89% 35.56% 6.67% 14.44% 60.70%
Economics 42.22% 41.11% 47.78% 44.44% 45.56% 78.89% 53.33% 31.11% 32.22% 34.44% 14.44% 17.78% 49.10%
ReasoningAvg 27.95% 29.29% 29.29% 25.82% 32.95% 36.03% 27.87% 18.85% 21.75% 24.91% 23.91% 30.75% 61.97%
Graphic Reasoning 26.60% 24.41% 21.89% 12.96% 21.21% 9.43% 27.27% 21.21% 18.35% 20.37% 20.37% 20.20% 71.76%
Definition Judgment 32.22% 34.44% 57.78% 40.56% 63.33% 72.22% 36.11% 31.11% 24.44% 33.89% 20.00% 41.11% 76.20%
Analogical Reasoning 26.10% 31.86% 37.88% 12.72% 38.02% 29.72% 22.09% 12.32% 17.67% 25.84% 14.59% 34.94% 67.08%
Logical Judgment 34.48% 38.89% 44.12% 37.58% 45.92% 61.27% 42.65% 21.90% 32.03% 35.46% 31.21% 38.07% 70.73%
Mathematical Calculation 27.04% 26.99% 24.77% 28.61% 30.00% 36.11% 25.42% 20.28% 20.32% 25.28% 27.27% 34.81% 49.53%
Numerical Reasoning 22.22% 25.93% 27.05% 26.57% 32.37% 34.62% 28.34% 15.46% 23.03% 17.71% 21.74% 24.15% 67.08%
Text Analysis 38.89% 52.22% 0.00% 36.67% 38.89% 34.44% 22.22% 11.11% 21.11% 28.89% 18.89% 0.00% 72.10%
Table Analysis 53.33% 25.56% 22.22% 25.56% 17.78% 46.67% 23.33% 21.11% 27.78% 6.67% 30.00% 12.22% 76.36%
Graphic Analysis 20.00% 22.22% 20.00% 20.00% 13.33% 16.67% 20.00% 13.33% 14.44% 27.78% 30.00% 10.00% 78.50%
Comprehensive Analysis 32.22% 31.11% 18.89% 33.33% 23.33% 37.78% 36.67% 12.22% 30.00% 10.00% 10.00% 13.33% 59.00%
UnderstandingAvg 44.53% 57.42% 46.22% 44.89% 62.58% 73.16% 42.84% 26.40% 33.78% 39.82% 22.04% 40.27% 74.82%
Passage Reading 51.11% 68.40% 60.25% 55.80% 73.83% 79.51% 48.40% 29.88% 43.70% 43.70% 24.44% 51.85% 74.66%
Sentence Grammar 41.94% 45.56% 43.33% 35.00% 56.67% 66.11% 38.33% 26.39% 28.33% 40.28% 22.78% 41.39% 74.43%
Fill-in-the-blank 34.44% 51.48% 42.22% 33.33% 44.44% 67.41% 41.85% 25.93% 27.04% 39.63% 24.81% 32.22% 78.38%
Long Text Reading 55.56% 73.33% 6.67% 70.00% 90.00% 90.00% 38.89% 12.22% 31.11% 21.11% 0.00% 7.78% 66.40%
ChatGLM-6BChatGLM v2-6B ChatGLM-130BGPT-3.5 ChatGPTGPT-4 Claude
LLaMA-13B Vicuna-13BErnieQianwenSpark0200400600800Count
All three answers are the same
One answer is different
All three answers are completely different
Figure 5: Sensibility to Prompts and Response Repeatability of LLMs.
provide Internet connectivity including GPT-3.5, ChatGPT, GPT-4, Claude, LLaMA-13B, Vicuna-13B,
ChatGLM-6B, and ChatGLM v2-6B, we also input the image url to them.
For the eight models that have no image processing ability, the ideal response is to admit they have
no such ability and cannot comprehend images. However, our evaluations find that only slight responses
of GPT-3.5, ChatGPT, and GPT-4 can admit the image processing limitations. The other models just
generate hallucinational and nonsense responses.
For the four models that have image processing ability, through comprehensive multimodal bench-
marking, we discover that none of the evaluated LLMs accurately comprehend image content. When
facing a multimodal question that contains both text and image, their responses are either text or text with
images. However, we find that the output response has low correlation with the input image. Even though
Ernie has a certain probability to accurately comprehend the conventional images like ImageNet [ 4],
however, the accuracy decreases largely (nearly zero) when facing geometric images.
4.3.3 Difficulty Benchmarking using Human-referenced Accuracy
We evaluate the ability to solve questions with different difficulty levels, using human accuracy as
references, as shown in Fig. 4. Note that Level 1 to Level 5 is from simple to complex, with the easiest
Level 1 and the most difficult Lever 5. The human label indicates the human accuracy on that level, and
11

--- PAGE 13 ---
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyChatGLM-6B
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyChatGLM v2-6B
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyChatGLM-130B
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyGPT-3.5
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyChatGPT
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyGPT-4
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyClaude
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyLLaMA-13B
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyVicuna-13B
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyErnie
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracyQianwen
Average
Major votingBest-case
Worst-case
Prompt I Prompt II Prompt 0.250.500.751.00AccuracySpark
Average
Major votingBest-case
Worst-caseFigure 6: The Average, Worst-case, Best-case, Majority V oting Accuracy of LLMs.
humans can achieve 89.92%, 69.20%, 51.11%, 32.64%, and 14.98% average accuracy from Level 1 to 5,
respectively.
From Level 1 to 4, GPT-4 performs best among all twelve LLMs. For Level 5, Claude performs the
best. Another exciting and counterintuitive phenomenon is that humans usually perform better than LLMs
for simple difficulty levels like Levels 1 to 3. In contrast, humans perform worse than some LLMs for
challenging levels like Levels 4 and 5. For example, GPT-4 has higher average accuracy than humans on
Level 4. On Level 5, many LLMs have similar or even higher average accuracy than humans, including
ChaGLM, Claude, Ernie, ChatGPT, GPT-4, GPT-3.5, Vicuna-13b, and ChatGLM-130b. The gap between
the human and the best LLM on each level is 31.67%, 16.68%, 6.09%, -5.23%, -14.51%, respectively.
4.3.4 Response Repeatability
We further evaluate the repeatability of the responses. For each question, we ask LLMs using three
different prompt types and each of which repeats three times. The three prompt types are (i) only question
without any prompt; add (ii) “The answer is:" and (iii) “The answer and the reason are:" at the end of the
question, respectively. Fig. 5 shows the results. We classify the responses into three categories: (1) all
three answers are the same, like choosing A three times, (2) one answer is different, like A, A, and B for
three answers. And (3) all three answers are entirely different, like A, B, and C for three answers. Note
that the (1) category means the best repeatability while (3) means the worst repeatability. We find that
Spark and Ernie have the best response repeatability, while LLaMA-13B is the worst.
12

--- PAGE 14 ---
4.3.5 The Average, Worst-case, Best-case, Majority Voting Accuracy
Fig. 6 presents the accuracy results under the average, worst, best, and majority voting cases. We also
use three kinds of prompt types, which use the same prompt setting with the above response repeatability
evaluation. We find the following observations. (1) GPT-4 achieves the highest accuracy under four cases.
(2) ChatGLM v2-6B performs better than ChatGLM-130B, which means the model architecture and the
quality of training data are more important than merely increasing the model size. (3) The worst-case
accuracy of LLMs is significantly below corresponding average one, which means the model not always
gives a correct answer during three times evaluation, indicating a poor reliability of LLMs. (4) The
best-case accuracy of LLMs is much higher than the other cases, which means the model has a high
probability to give a correct answer during three times evaluation. (5) The majority voting accuracy is
similar with the average one, which means most of the time, the model can give a correct answer.
5 Conclusion
This paper provides a multi-granularity, multimodal, human-referenced, and auto-scoring benchmark for
evaluating large language models – AGIBench, including a question dataset, auto-scoring evaluation, and
comprehensive metrics. Through labeling each question with four attributes, including ability branch,
knowledge, difficulty, modal, AGIBench supporting multi-granularity benchmarking at per-dataset, per-
ability branch, per-knowledge, per-difficulty level, per-modal, and per-question granularities. We use the
accuracy rate answered by millions of well-educated humans to label each question’s difficulty level and
include text and image modals. We further propose an HRE algorithm to avoid the unpredictable impacts
of prompt engineering. Instead of only using average accuracy as a metric, we define multi-dimensional
metrics to evaluate the LLMs comprehensively. Our experiments on twelve LLMs show the effectiveness
of AGIBench.
References
[1]Bubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y .T.,
Li, Y ., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M.T., Zhang, Y .: Sparks of artificial general
intelligence: Early experiments with gpt-4 (2023)
[2]Cattell, R.B.: Theory of fluid and crystallized intelligence: A critical experiment. Journal of
educational psychology 54(1), 1 (1963)
[3]Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J.,
Hilton, J., Nakano, R., Hesse, C., Schulman, J.: Training verifiers to solve math word problems
(2021)
[4]Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical
image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255.
Ieee (2009)
[5] Gardner, H.: The theory of multiple intelligences. Annals of dyslexia pp. 19–35 (1987)
[6]Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.:
Measuring mathematical problem solving with the math dataset. In: Thirty-fifth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) (2021)
[7]Huang, Y ., Bai, Y ., Zhu, Z., Zhang, J., Zhang, J., Su, T., Liu, J., Lv, C., Zhang, Y ., Lei, J., Fu, Y ., Sun,
M., He, J.: C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models
(2023)
13

--- PAGE 15 ---
[8]Legg, S., Hutter, M., et al.: A collection of definitions of intelligence. Frontiers in Artificial
Intelligence and applications 157, 17 (2007)
[9]Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y ., Narayanan, D.,
Wu, Y ., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C.D., R é,
C., Acosta-Navas, D., Hudson, D.A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao,
H., Wang, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N.,
Chatterji, N., Khattab, O., Henderson, P., Huang, Q., Chi, R., Xie, S.M., Santurkar, S., Ganguli, S.,
Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V ., Wang, W., Li, X., Mai, Y ., Zhang, Y ., Koreeda,
Y .: Holistic evaluation of language models (2022)
[10] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.W., Zhu, S.C., Tafjord, O., Clark, P., Kalyan, A.: Learn
to explain: Multimodal reasoning via thought chains for science question answering. Advances in
Neural Information Processing Systems 35, 2507–2521 (2022)
[11] Raji, I.D., Bender, E.M., Paullada, A., Denton, E., Hanna, A.: Ai and the everything in the whole
wide world benchmark. arXiv preprint arXiv:2111.15366 (2021)
[12] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown, A.R., Santoro, A.,
Gupta, A., Garriga-Alonso, A., et al.: Beyond the imitation game: Quantifying and extrapolating the
capabilities of language models. Transactions on Machine Learning Research (2023)
[13] Sternberg, R.J.: The triarchic theory of intelligence. (1997)
[14] Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.:
Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in
neural information processing systems 32(2019)
[15] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: A multi-task benchmark
and analysis platform for natural language understanding. In: International Conference on Learning
Representations (2019)
[16] Wang, Y ., Yao, Q., Kwok, J.T., Ni, L.M.: Generalizing from a few examples: A survey on few-shot
learning. ACM Computing Surveys (CSUR) 53(3), 1–34 (2020)
[17] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V ., Zhou, D., et al.: Chain-
of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems 35, 24824–24837 (2022)
[18] Zhang, X., Li, C., Zong, Y ., Ying, Z., He, L., Qiu, X.: Evaluating the performance of large language
models on gaokao benchmark (2023)
[19] Zheng, L., Chiang, W.L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D., Xing,
E.P., Zhang, H., Gonzalez, J.E., Stoica, I.: Judging llm-as-a-judge with mt-bench and chatbot arena
(2023)
[20] Zhong, W., Cui, R., Guo, Y ., Liang, Y ., Lu, S., Wang, Y ., Saied, A., Chen, W., Duan, N.: Agieval: A
human-centric benchmark for evaluating foundation models (2023)
14
