# SAM-CLIP: Kết hợp các Mô hình Nền tảng Thị giác hướng đến Hiểu biết Ngữ nghĩa và Không gian

Haoxiang Wang2†, Pavan Kumar Anasosalu Vasu1, Fartash Faghri1, Raviteja Vemulapalli1
Mehrdad Farajtabar1, Sachin Mehta1, Mohammad Rastegari1, Oncel Tuzel1, Hadi Pouransari1†
1Apple2University of Illinois Urbana-Champaign

## Tóm tắt

Cảnh quan của các mô hình nền tảng thị giác (VFM) công khai, chẳng hạn như CLIP và Segment Anything Model (SAM), đang mở rộng nhanh chóng. VFM được obặc với các khả năng riêng biệt xuất phát từ các mục tiêu tiền huấn luyện của chúng. Ví dụ, CLIP xuất sắc trong hiểu biết ngữ nghĩa, trong khi SAM chuyên môn hóa trong hiểu biết không gian cho phân đoạn. Trong công trình này, chúng tôi giới thiệu một công thức đơn giản để kết hợp hiệu quả các VFM thành một mô hình thống nhất hấp thụ chuyên môn của chúng. Phương pháp của chúng tôi tích hợp các kỹ thuật học đa nhiệm vụ, học liên tục và chưng cất. Hơn nữa, nó đòi hỏi chi phí tính toán ít hơn đáng kể so với huấn luyện đa nhiệm vụ truyền thống từ đầu, và chỉ cần một phần nhỏ của các tập dữ liệu tiền huấn luyện ban đầu được sử dụng để huấn luyện các mô hình riêng lẻ. Bằng cách áp dụng phương pháp của chúng tôi vào SAM và CLIP, chúng tôi có được SAM-CLIP: một mô hình thống nhất kết hợp các khả năng của SAM và CLIP thành một transformer thị giác duy nhất. So với việc triển khai SAM và CLIP độc lập, mô hình đã kết hợp của chúng tôi, SAM-CLIP, giảm chi phí lưu trữ và tính toán cho suy luận, làm cho nó phù hợp cho các ứng dụng thiết bị biên. Chúng tôi cho thấy rằng SAM-CLIP không chỉ giữ lại những điểm mạnh cơ bản của SAM và CLIP, mà còn giới thiệu các chức năng hiệp lực, đáng chú ý trong phân đoạn ngữ nghĩa zero-shot, nơi SAM-CLIP thiết lập kết quả tiên tiến mới trên 5 benchmark. Nó vượt trội hơn các mô hình trước đây được thiết kế riêng cho nhiệm vụ này với biên độ lớn, bao gồm cải thiện mean IoU +6.8% và +5.9% trên tập dữ liệu Pascal-VOC và COCO-Stuff tương ứng.

## 1. Giới thiệu

Các Mô hình Nền tảng Thị giác (VFM) như CLIP [68], SAM [38], MAE [26] và DINOv2 [62] cung cấp các backbone mạnh mẽ hoạt động tốt cho một loạt các nhiệm vụ thị giác khi được tinh chỉnh trên dữ liệu cụ thể theo miền. Ngoài ra, một số trong các mô hình này thể hiện khả năng đáng chú ý dựa trên prompt mở (còn được gọi là zero-shot), chẳng hạn như phân loại từ text prompt [68] và phân đoạn từ geometric prompt (ví dụ: điểm, bounding box và mask) [38]. Tùy thuộc vào các mục tiêu tiền huấn luyện của chúng, VFM có thể hoạt động như các bộ trích xuất đặc trưng phù hợp cho các nhiệm vụ downstream đa dạng. Ví dụ, các mô hình sử dụng contrastive loss trong quá trình huấn luyện [11, 62, 68], sử dụng tín hiệu tần số thấp và tạo ra các đặc trưng có thể phân tách tuyến tính các mẫu dựa trên nội dung ngữ nghĩa của chúng [65]. Ngược lại, các mục tiêu tiền huấn luyện cho MAE và SAM liên quan đến việc khử nhiễu hình ảnh bị che và phân đoạn mask instance tương ứng. Những mục tiêu này dẫn đến việc thu nhận các đặc trưng sử dụng tín hiệu tần số cao với kiến thức định vị nhưng hiểu biết ngữ nghĩa hạn chế (Hình 4).

Duy trì và triển khai các mô hình thị giác riêng biệt cho các nhiệm vụ downstream khác nhau là không hiệu quả (dấu chân bộ nhớ và thời gian chạy cao, đặc biệt trên thiết bị biên) và thiếu cơ hội học liên mô hình [76]. Học đa nhiệm vụ [97] là một mô hình có khả năng giải quyết vấn đề này. Tuy nhiên, nó thường đòi hỏi huấn luyện tốn kém và truy cập đồng thời vào tất cả các nhiệm vụ [20]. Huấn luyện các mô hình nền tảng thường dựa vào phương pháp không giám sát hoặc bán giám sát, đòi hỏi tài nguyên tính toán đáng kể. Ví dụ, các mô hình CLIP tiên tiến được huấn luyện trên các tập dữ liệu rộng lớn, chẳng hạn như LAION [77] và DataComp [22], tiêu thụ một lượng lớn sức mạnh tính toán. Tương tự, việc tiền huấn luyện SAM trên 1.1 tỷ mask đòi hỏi tính toán lớn. Một phương pháp tiền huấn luyện đa mục tiêu đòi hỏi dữ liệu và sức mạnh tính toán tương đương hoặc nhiều hơn so với huấn luyện VFM mục tiêu đơn. Ngoài ra, vẫn còn những thách thức cần được giải quyết, chẳng hạn như cách trộn tập dữ liệu tốt nhất, cách xử lý gradient giao thoa và bất ổn trong huấn luyện đa nhiệm vụ [15], và cách truy cập các tập dữ liệu tiền huấn luyện VFM thường là độc quyền [68], điều này hạn chế khả năng mở rộng và tính khả thi của phương pháp này.

Để vượt qua những thách thức này, kết hợp mô hình đã nổi lên như một lĩnh vực nghiên cứu phát triển nhanh chóng [83, 90]. Phần lớn các kỹ thuật kết hợp tập trung vào việc kết hợp nhiều mô hình cụ thể theo nhiệm vụ thành một mô hình duy nhất mà không cần huấn luyện bổ sung. Ví dụ, điều này có thể đạt được thông qua các kỹ thuật như nội suy trọng số mô hình [31], phân tích tầm quan trọng tham số [54], hoặc tận dụng các bất biến trong các mô hình [1]. Mặt khác, các kỹ thuật này đặt quá nhiều áp lực lên việc không sử dụng dữ liệu hoặc không thực hiện huấn luyện/tinh chỉnh bổ sung dẫn đến hiệu suất giảm hoặc thiếu khả năng tổng quát hóa cho các tập hợp nhiệm vụ đa dạng [83]. Trong công trình này, mục tiêu của chúng tôi là kết hợp các VFM được huấn luyện với các mục tiêu khác nhau cơ bản, có khả năng riêng biệt và có thể tương tác với các phương thức khác. Trong thiết lập này, các phương pháp kết hợp ngây thơ như nội suy trọng số dẫn đến quên lãng đáng kể [56], như được hiển thị trong Phụ lục D.

Chúng tôi nhằm mục đích lấp đầy khoảng trống giữa kết hợp mô hình không cần huấn luyện và huấn luyện đa nhiệm vụ bằng cách rút ra các kỹ thuật từ học liên tục [46, 64] và chưng cất kiến thức [27]. Chúng tôi coi việc kết hợp mô hình như một bài toán học liên tục, trong đó, với một VFM tiền huấn luyện, kiến thức của VFM thứ hai được kết hợp mà không quên kiến thức ban đầu. Một mặt, trái ngược với các kỹ thuật lấy trung bình trọng số, chúng tôi cho phép truy cập vào một phần nhỏ dữ liệu tiền huấn luyện hoặc các đại diện của nó để được phát lại trong quá trình kết hợp. Chúng tôi tận dụng chưng cất đa nhiệm vụ trên dữ liệu phát lại để tránh quên kiến thức ban đầu của các VFM tiền huấn luyện trong quá trình kết hợp. Mặt khác, quá trình kết hợp của chúng tôi hiệu quả hơn đáng kể so với huấn luyện đa nhiệm vụ truyền thống bằng cách yêu cầu ít hơn 10% dữ liệu và chi phí tính toán so với tiền huấn luyện ban đầu của chúng (Phần 3).

Chúng tôi khởi tạo phương pháp kết hợp được đề xuất bằng cách kết hợp SAM và CLIP thành một mô hình đa nhiệm vụ duy nhất, được gọi là SAM-CLIP, phù hợp cho triển khai thiết bị biên. Mô hình đã kết hợp này thừa hưởng các khả năng zero-shot dựa trên prompt từ cả CLIP và SAM với quên lãng tối thiểu: cụ thể, phân loại zero-shot và truy xuất hình ảnh-văn bản từ CLIP, và phân đoạn instance zero-shot từ SAM (xem Hình 1 trái). Hơn nữa, chúng tôi minh họa rằng SAM-CLIP học các biểu diễn thị giác phong phú hơn so với SAM và CLIP, được obặc với cả đặc trưng không gian và ngữ nghĩa, dẫn đến hiệu suất head-probing cải thiện trên các nhiệm vụ mới (xem Hình 4). Cuối cùng, SAM-CLIP cho thấy khả năng mới nổi của việc chuyển giao zero-shot sang một nhiệm vụ mới: phân đoạn ngữ nghĩa zero-shot nhờ vào các kỹ năng kết hợp được thừa hưởng từ SAM và CLIP. Nhiệm vụ này liên quan đến việc tạo ra một mask phân đoạn dựa trên một text prompt tự do. Nó đòi hỏi cả hiểu biết ngữ nghĩa từ văn bản và khả năng phân đoạn, đó là các kỹ năng mà SAM-CLIP học từ CLIP và SAM tương ứng. Chúng tôi chứng minh rằng SAM-CLIP đạt được hiệu suất tiên tiến trên phân đoạn ngữ nghĩa zero-shot trong thiết lập suy luận một giai đoạn trên nhiều tập dữ liệu (Hình 1 phải). Với sự thỏa hiệp của một sự sụt giảm không đáng kể so với hiệu suất của các mô hình riêng lẻ trên các nhiệm vụ gốc (phân loại zero-shot và phân đoạn instance), chúng tôi có được một mô hình duy nhất không chỉ thành thạo cả hai nhiệm vụ mà còn có khả năng hoàn thành một nhiệm vụ mới.

## 2. Kiến thức nền tảng

Các Mô hình Thị giác-Ngôn ngữ (VLM) như CLIP và ALIGN [32] được huấn luyện trên các tập dữ liệu hình ảnh-văn bản quy mô tỷ, thường có nhiều tiếng. Những mô hình này bao gồm các encoder cụ thể theo phương thức (hình ảnh và văn bản) tạo ra một embedding cho mỗi phương thức. Đối với một batch các cặp hình ảnh-văn bản được lấy mẫu ngẫu nhiên, các mô hình này được huấn luyện với một mục tiêu contrastive để tối đa hóa sự căn chỉnh giữa các embedding của các cặp hình ảnh và văn bản tích cực. Một ứng dụng trực tiếp của các mô hình như vậy là truy xuất hình ảnh-văn bản zero-shot, hoặc phân loại zero-shot thông qua text prompt [68]. Các công trình khác như ViLT [36], VLMo [4] và BLIP [42] đã khám phá các kiến trúc được chia sẻ hoặc hỗn hợp giữa các phương thức hình ảnh và văn bản và cho phép các khả năng zero-shot bổ sung như Trả lời Câu hỏi Thị giác (VQA) và tạo chú thích.

Các phương pháp như LiT [95], APE [75] và BLIP-2 [43] giảm chi phí huấn luyện của các mô hình giống CLIP bằng cách triển khai các mô hình một phương thức tiền huấn luyện. Điều này tương tự như phương pháp của chúng tôi về mặt khai thác kiến thức của các mô hình tiền huấn luyện có sẵn. Tuy nhiên, chúng tôi tập trung vào việc kết hợp các backbone thị giác thành một mô hình thống nhất trong thiết lập đa phương thức đa encoder. Hơn nữa, trên cơ sở khả năng học biểu diễn, chúng tôi chuyển giao các khả năng zero-shot của các mô hình tiền huấn luyện.

Segment Anything Model (SAM) [38] giới thiệu một tập dữ liệu quy mô lớn, một mô hình và một công thức huấn luyện để cho phép phân đoạn với một prompt. Tập dữ liệu bao gồm các bộ ba của một hình ảnh, một geometric prompt và một mask phân đoạn. SAM bao gồm một image encoder, một prompt encoder và một mask decoder. Image encoder của SAM là một ViT-Det [45] tiền huấn luyện với mục tiêu MAE [26], được obặc với kiến thức định vị tần số cao phong phú [65]. Prompt-encoder nhận một đầu vào hình học dưới dạng điểm, vùng mask hoặc bounding box. Mask decoder nhận đầu ra của cả hai encoder và tạo ra một mask phân đoạn độ phân giải cao. SAM được huấn luyện sử dụng một kết hợp tuyến tính của Focal [48] và Dice [58] loss và có khả năng tạo ra các mask phân đoạn ngay cả khi prompt đầu vào là mơ hồ/chất lượng thấp. Đáng chú ý là Kirillov et al. [38] thảo luận ngắn gọn về một chiến lược tiền huấn luyện đa nhiệm vụ có thể để cho phép khả năng text-to-mask dạng tự do, nhưng chưa phát hành mô hình.

Có một số công trình tiếp theo đối với SAM mà chúng tôi thảo luận ngắn gọn ở đây. HQ-SAM [34] thêm một token bổ sung và một lớp có thể học nhẹ vào một mô hình SAM đông lạnh để cho phép phân đoạn chất lượng cao sử dụng một tập dữ liệu phân đoạn có chú thích chất lượng cao nhỏ. FastSAM [99] và MobileSAM [96] sử dụng kiến trúc CNN và chưng cất kiến thức tương ứng để huấn luyện các biến thể nhỏ hơn và nhanh hơn của mô hình SAM. Không giống như công trình của chúng tôi, tất cả các phương pháp này nhắm đến cùng một nhiệm vụ như SAM gốc và có thể được sử dụng như VFM cơ sở trong phương pháp được đề xuất của chúng tôi. SemanticSAM [41] và SEEM [102] sử dụng các chú thích phân đoạn ngữ nghĩa để huấn luyện nhằm cho phép phân đoạn nhận biết ngữ nghĩa và đa độ hạt, do đó chúng không phải là các mô hình phân đoạn ngữ nghĩa zero-shot. Những công trình này khác với phương pháp của chúng tôi, không sử dụng bất kỳ chú thích phân đoạn ngữ nghĩa nào và thay vào đó có được kiến thức ngữ nghĩa từ chưng cất với CLIP. Bên cạnh đó, đã được chứng minh rằng việc tổng hợp SAM và CLIP để phân đoạn ngữ nghĩa là khả thi bằng cách sử dụng SAM để tạo ra tất cả các mask phân đoạn có thể và sau đó sử dụng CLIP để cung cấp nhãn [28]. Tuy nhiên, phương pháp này yêu cầu tải hai mô hình đồng thời (dấu chân bộ nhớ gấp 2 lần) và, đối với mỗi hình ảnh, cần một lượt truyền tiến của SAM backbone để tạo ra K phân đoạn đối tượng, theo sau là một lượt truyền tiến của mô hình CLIP cho mỗi phân đoạn để lọc (tổng cộng K+ 1 lượt truyền tiến).

Chưng cất Kiến thức (KD) [5, 27] ban đầu được đề xuất để huấn luyện một bộ phân loại nén (học sinh) sử dụng kiến thức tích lũy trong một mô hình lớn tiền huấn luyện (giáo viên). Liên quan đến công trình của chúng tôi, các công trình gần đây đã khám phá các phương pháp chưng cất cho VLM như EVA [17, 18], DIMEFM [82], CLIPPING [67] và CLIP-KD [91]. Chúng cho thấy việc chuyển giao cùng một khả năng zero-shot của mô hình giáo viên sang học sinh. Ở đây, trong thiết lập đa nhiệm vụ, chúng tôi thực hiện chưng cất và tự chưng cất [21], và chứng minh việc chuyển giao các khả năng zero-shot khác nhau (từ hai giáo viên) vào một mô hình duy nhất, cũng như sự xuất hiện của khả năng zero-shot mới cụ thể cho mô hình học sinh.

Học Liên tục (CL) Thiết lập của chúng tôi cũng liên quan đến Học Liên tục [64], nơi kiến thức mới được thêm vào một mô hình hiện có. Thách thức chính trong học liên tục là quên lãng thảm khốc [55, 56] đề cập đến việc mất kiến thức đã học trước đó do học các nhiệm vụ mới. Các thuật toán Học Liên tục thường giảm thiểu quên lãng thông qua chính quy hóa [39, 94], phát lại kinh nghiệm [25, 70], phát lại được chính quy hóa [9, 19], mở rộng động [78, 92] và các phương pháp dựa trên tối ưu hóa [59, 63], trong số đó, các phương pháp dựa trên phát lại đã được chứng minh là đơn giản nhưng rất thành công [3, 51]. Trong công trình này, chúng tôi đề xuất một công thức đơn giản dựa trên phát lại bộ nhớ và chưng cất để kết hợp VFM với quên lãng tối thiểu.

Nhiệm vụ Phân đoạn Ngữ nghĩa Zero-shot nhằm mục đích dự đoán một mask phân đoạn dày đặc với một text prompt dạng mở, mà không có kiến thức trước về các lớp đối tượng cụ thể quan tâm hoặc bất kỳ tinh chỉnh nào. Các phương pháp gần đây đối với phân đoạn từ vựng mở triển khai các tập dữ liệu cặp hình ảnh-văn bản và VLM tiền huấn luyện như CLIP và các biểu diễn nội bộ của chúng để có được các mask phân đoạn dày đặc, ví dụ GroupViT [88], ViewCo [72], CLIPpy [69], ViL-Seg [49], OVS [89], TCL [7] và SegCLIP [53]. Trong công trình này, chúng tôi không trực tiếp sử dụng bất kỳ dữ liệu văn bản nào. Thay vào đó, tất cả kiến thức ngữ nghĩa văn bản đều được lấy từ một CLIP tiền huấn luyện. Một phương pháp thay thế là triển khai các mô hình hiện có, không có bất kỳ huấn luyện nào, và tạo ra các mask phân đoạn sử dụng nhiều backbone trong một thiết lập đa giai đoạn. Ví dụ, người ta có thể chạy SAM để có được một số đề xuất đối tượng và chạy từng cái qua CLIP để phân loại ngữ nghĩa [50]. Một số công trình gần đây [33, 85] sử dụng các bản đồ attention nội bộ của các mô hình sinh thị giác có điều kiện như StableDiffusion [74] để có được các mask phân đoạn. Mặc dù các phương pháp này không cần huấn luyện, chúng yêu cầu nhiều giai đoạn với xử lý phức tạp, nhiều encoder thị giác và nhiều lượt truyền tiến, làm cho việc triển khai của chúng cho các thiết bị biên bị hạn chế.

Các kỹ thuật Kết hợp Mô hình nhằm mục đích kết hợp khả năng của các mô hình khác nhau bằng các phép toán nội suy đơn giản như lấy trung bình trọng số [86] và phép toán nhiệm vụ [31]. Gần đây có nhiều kỹ thuật như vậy [2, 13, 30, 35, 54, 61, 80, 87] sử dụng các chế độ trọng số khác nhau và độ nhạy cảm và tầm quan trọng của tham số. Cách chúng tôi huấn luyện SAM-CLIP có thể được coi là một phương pháp kết hợp phụ thuộc vào dữ liệu trong đó kiến thức của các mô hình được kết hợp bằng cách liên tục nhắc nhở chúng về hành vi ban đầu thông qua phát lại, trong khi thuật toán tối ưu hóa khám phá không gian tham số để tìm tối ưu.

## 3. Phương pháp đề xuất

Trong phần này, chúng tôi giải thích phương pháp của chúng tôi để kết hợp hiệu quả các VFM tiền huấn luyện. Chúng tôi bắt đầu với một VFM cơ sở, sau đó chuyển giao kiến thức từ các VFM phụ trợ khác vào nó với quên lãng tối thiểu. Chúng tôi giả định rằng mỗi VFM sở hữu một vision encoder, và có thể có các encoder phương thức khác, cũng như các decoder/head cụ thể theo nhiệm vụ. Mục tiêu của chúng tôi là kết hợp các vision encoder thành một backbone duy nhất sao cho nó có thể được sử dụng cùng với các encoder phương thức khác, vẫn giữ đông lạnh.

Để tập trung vào việc trình bày, chúng tôi giới hạn thảo luận của mình vào trường hợp cụ thể trong đó SAM đóng vai trò là VFM cơ sở, trong khi một mô hình CLIP đóng vai trò là VFM phụ trợ. Cặp này đưa ra một sự kết hợp hấp dẫn, vì cả hai mô hình đều đã được triển khai thành công trong các nhiệm vụ đa dạng và thể hiện các khả năng bổ sung. SAM xuất sắc trong định vị và phân đoạn hình ảnh độ phân giải cao nhưng có hạn chế trong hiểu biết ngữ nghĩa. Ngược lại, CLIP cung cấp một image backbone mạnh mẽ để hiểu ngữ nghĩa. Chúng tôi chứng minh điều này bằng một số thí nghiệm thăm dò (xem Hình 4). Về tiềm năng, người ta có thể bắt đầu với CLIP làm VFM cơ sở và kết hợp kiến thức của SAM vào nó. Tuy nhiên, các mô hình CLIP ViT tiền huấn luyện hiện có không hiệu quả trong việc xử lý hình ảnh độ phân giải cao được sử dụng để huấn luyện SAM. Do đó, chúng tôi chọn SAM làm mô hình cơ sở và thừa hưởng cấu trúc ViT-Det của nó có thể xử lý đầu vào độ phân giải cao một cách hiệu quả.

Chúng tôi giả định truy cập vào các tập con hạn chế của các tập dữ liệu (hoặc các proxy của chúng) được sử dụng để huấn luyện VFM cơ sở và phụ trợ, hoạt động như phát lại bộ nhớ trong thiết lập CL của chúng tôi. Chúng được ký hiệu là DSAM và DCLIP tương ứng với chi tiết được cung cấp trong Phần 4.1.

Chúng tôi sử dụng kiến trúc đa head, được minh họa trong Hình 2. VFM cơ sở của chúng tôi, SAM, có một image encoder (EncSAM), một prompt encoder (PromptEncSAM) và một mask decoder nhẹ (MaskDecSAM). VFM phụ trợ, CLIP, có một image encoder (EncCLIP) và một text encoder (TextEncCLIP). Mục tiêu của chúng tôi là kết hợp cả hai image encoder thành một backbone duy nhất được gọi là EncSAM-CLIP được khởi tạo bởi EncSAM. Hơn nữa, chúng tôi xem xét các head nhẹ tương ứng với mỗi VFM, cụ thể là HeadSAM và HeadCLIP. HeadSAM được khởi tạo với MaskDecSAM và HeadCLIP được khởi tạo với trọng số ngẫu nhiên (vì CLIP không đi kèm với một head mà chúng tôi có thể triển khai). Chúng tôi triển khai các encoder phương thức khác (tức là PromptEncSAM và TextEncCLIP) mà không có thay đổi (đông lạnh).

Là một phương pháp kết hợp cơ bản, chúng tôi thực hiện KD trên DCLIP sử dụng cosine distillation loss [23]:

LCLIP = Ex∼DCLIP[1 - (ϕPooling(HeadCLIP(EncSAM-CLIP(x)))TEncCLIP(x))]

trong đó ϕPooling là một toán tử pooling không gian nhận các đặc trưng cấp patch từ HeadCLIP và tạo ra một embedding cấp hình ảnh được chuẩn hóa. Trong thiết lập này, các tham số của cả HeadCLIP và EncSAM-CLIP đều có thể học được, trong khi CLIP encoder, EncCLIP, được đông lạnh và sử dụng làm giáo viên. Mặc dù điều này làm cho SAM có thêm khả năng ngữ nghĩa của CLIP, nó phát sinh với chi phí quên lãng thảm khốc của các khả năng ban đầu của SAM. Hơn nữa, chúng tôi cho thấy rằng các phương pháp giảm thiểu không cần huấn luyện chống lại quên lãng thảm khốc, như Wise-FT [86], không hiệu quả trong bối cảnh kết hợp VFM của chúng tôi, như được chứng minh trong phần D.

Để giải quyết những thách thức này, chúng tôi đề xuất một chưng cất đa nhiệm vụ dựa trên luyện tập. Điều này phục vụ hai mục tiêu chính: 1) tạo điều kiện cho việc chuyển giao kiến thức hiệu quả từ VFM phụ trợ sang mô hình cơ sở, và 2) bảo tồn các khả năng ban đầu của mô hình cơ sở. Lấy cảm hứng từ Kumar et al. [40], chúng tôi xem xét một huấn luyện hai giai đoạn: head-probing và chưng cất đa nhiệm vụ. Một giai đoạn tùy chọn của thích ứng độ phân giải có thể được thêm vào nếu nhiều head được huấn luyện dưới các độ phân giải khác nhau, đó là trường hợp trong thí nghiệm của chúng tôi về việc kết hợp SAM và CLIP. Xem Phần 4.1 để biết chi tiết về thích ứng độ phân giải.

I. Head probing: Trong giai đoạn này, chúng tôi đầu tiên đông lạnh image backbone, EncSAM-CLIP, và chỉ huấn luyện HeadCLIP với loss trong Phương trình (1). Trực quan, với phương pháp này, chúng tôi đầu tiên học một số giá trị hợp lý cho các tham số của HeadCLIP (được khởi tạo ngẫu nhiên) trước khi cho phép bất kỳ thay đổi nào trong EncSAM-CLIP dễ bị quên lãng.

II. Chưng cất đa nhiệm vụ: Trong giai đoạn này, chúng tôi cho phép tất cả các head cũng như image encoder của chúng tôi có thể học được. Chúng tôi thực hiện một huấn luyện đa nhiệm vụ trên LCLIP + λLSAM, với:

LSAM = E(x,g)∼DSAM LFD(HeadSAM(EncSAM-CLIP(x), PromptEncSAM(g)), z)

trong đó, x là một hình ảnh thô, g là một geometric prompt, z = MaskDecSAM(EncSAM(x)) là điểm số mask phân đoạn được tạo ra bởi giáo viên SAM đông lạnh, và LFD đề cập đến một kết hợp tuyến tính của Focal [48] và Dice [58] được sử dụng trong huấn luyện SAM ban đầu được điều chỉnh cho chưng cất. Chúng tôi huấn luyện trên DSAM ∪ DCLIP với tổng loss là LCLIP + λLSAM. Trong quá trình huấn luyện, mỗi batch có một số mẫu từ DCLIP và một số từ DSAM, góp phần vào LCLIP và LSAM tương ứng (tức là, các mẫu từ tập dữ liệu CLIP không đóng góp vào loss SAM và ngược lại). Để khuyến khích ít quên lãng hơn, chúng tôi sử dụng tỷ lệ học nhỏ hơn một bậc độ lớn cho các tham số của EncSAM-CLIP và HeadSAM so với HeadCLIP ở giai đoạn này.

## 4. Thí nghiệm

### 4.1. Chi tiết triển khai

Các lựa chọn thiết kế của chúng tôi, như được giải thích dưới đây, nhằm mục đích cân bằng sự đánh đổi giữa học từ CLIP (phân loại zero-shot) và giữ lại kiến thức của SAM (phân đoạn instance).

Kiến trúc Mô hình. Chúng tôi sử dụng phiên bản ViT-B/16 của Segment Anything Model (SAM) làm kiến trúc cơ sở [38], bao gồm 12 lớp transformer. Để tích hợp khả năng CLIP, chúng tôi thêm một CLIP head nhẹ gồm 3 lớp transformer vào SAM backbone. Các đầu ra patch token từ CLIP head này trải qua một lớp pooling để tạo ra một embedding cấp hình ảnh, tương tự như vai trò của đầu ra CLS token trong các mô hình ViT. Chúng tôi áp dụng max-pooling vì chúng tôi quan sát thấy rằng nó có thể dẫn đến hiệu suất phân loại zero-shot và phân đoạn ngữ nghĩa tốt hơn của SAM-CLIP so với average pooling. Đáng chú ý rằng max-pooling đã được tìm thấy có thể khuyến khích việc học các đặc trưng thị giác không gian [69]. Với lớp pooling, CLIP head có thể đưa ra một embedding cho toàn bộ hình ảnh, có thể được căn chỉnh với một text embedding giống như mô hình CLIP ban đầu [68].

Chuẩn bị Tập dữ liệu. Đối với chưng cất CLIP, chúng tôi kết hợp hình ảnh từ một số tập dữ liệu: CC3M [79], CC12M [8], YFCC-15M [68] (một tập con được tuyển chọn của YFCC-100M [84] bởi OpenAI) và ImageNet-21k [73]. Điều này tạo thành DCLIP của chúng tôi chứa 40.6M hình ảnh không có nhãn. Đối với tự chưng cất SAM, chúng tôi lấy mẫu 5.7% tập con từ tập dữ liệu SA-1B để tạo thành DSAM, ban đầu bao gồm 11M hình ảnh và 1.1B mask. Chúng tôi chọn ngẫu nhiên 1% của DCLIP và DSAM làm tập validation. Tổng thể, chúng tôi có 40.8M hình ảnh để huấn luyện, mà chúng tôi gọi là Merged-41M trong công trình này.

Huấn luyện. Như chúng tôi đã thảo luận trong Phần 3, việc huấn luyện được thực hiện trong hai giai đoạn để tối ưu hóa sự hội tụ, theo phong cách "probing rồi full finetuning". Giai đoạn đầu tiên của CLIP-head probing mất 20 epoch trên DCLIP, trong khi backbone được giữ đông lạnh. Ở đây, mô hình giáo viên là OpenCLIP [29] ViT-L/14 được huấn luyện trên tập dữ liệu DataComp-1B [22]. Trong giai đoạn thứ hai (16 epoch), chúng tôi mở đóng băng backbone EncSAM-CLIP và tiến hành tinh chỉnh kết hợp cùng với HeadCLIP và HeadSAM, kết hợp cả CLIP và SAM distillation loss ở tỷ lệ 1:10. Mô hình SAM ViT-B ban đầu đóng vai trò là giáo viên trong SAM loss. Hơn nữa, tỷ lệ học được áp dụng cho EncSAM-CLIP và HeadSAM nhỏ hơn 10 lần so với HeadCLIP để giảm việc quên khả năng SAM ban đầu. Bên cạnh đó, chúng tôi áp dụng một chiến lược độ phân giải đầu vào hỗn hợp để huấn luyện. Một sự khác biệt đáng chú ý giữa SAM và CLIP là độ phân giải tiền huấn luyện của chúng. SAM được huấn luyện và hoạt động tốt nhất ở độ phân giải 1024px trong khi thường các độ phân giải thấp hơn (ví dụ: 224/336/448px) được áp dụng cho huấn luyện và suy luận CLIP [12, 68, 81]. Do đó, chúng tôi sử dụng các độ phân giải biến thiên 224/448px cho chưng cất CLIP thông qua phương pháp variable batch sampler của Mehta et al. [57], trong khi chưng cất SAM sử dụng độ phân giải 1024px theo hướng dẫn huấn luyện SAM ban đầu [38]. Trong mỗi bước tối ưu hóa, chúng tôi tạo thành một batch 2048 hình ảnh từ DCLIP và 32 hình ảnh (mỗi hình ảnh có 32 chú thích mask) từ DSAM và thực hiện huấn luyện theo cách đa nhiệm vụ (xem Phụ lục A để biết thêm chi tiết).

Thích ứng Độ phân giải. Sau hai giai đoạn huấn luyện, SAM-CLIP có thể hoàn thành các nhiệm vụ CLIP (ví dụ: phân loại zero-shot) sử dụng CLIP-head dưới 224/336/448px, và chạy suy luận với SAM-head dưới 1024px. Tuy nhiên, nếu muốn áp dụng hai head cùng nhau trên một hình ảnh đầu vào duy nhất cho các nhiệm vụ nhất định (chúng tôi trình bày demo về điều này trong Phần 4.4), sẽ không hiệu quả khi truyền hình ảnh hai lần cho image encoder với hai độ phân giải cho hai head tương ứng. Để khắc phục vấn đề này, chúng tôi điều chỉnh CLIP head cho đầu vào 1024px sử dụng một giai đoạn tinh chỉnh rất ngắn và hiệu quả: đông lạnh image encoder và chỉ tinh chỉnh CLIP-head với LCLIP trong 3 epoch (nó giống như giai đoạn đầu tiên của huấn luyện, cũng là CLIP-head probing) dưới các độ phân giải biến thiên 224/448/1024px. Lưu ý: các chiến lược nâng cấp độ phân giải rất phổ biến trong huấn luyện CLIP: Li et al. [44], Radford et al. [68], Sun et al. [81] cho thấy nó hiệu quả hơn so với huấn luyện với độ phân giải cao từ đầu.

Thêm chi tiết về triển khai và huấn luyện được trình bày trong Phụ lục A.

### 4.2. Đánh giá Zero-Shot

Các nhiệm vụ CLIP: Phân loại Hình ảnh Zero-Shot & Truy xuất Văn bản-sang-Hình ảnh. Để kiểm tra các khả năng liên quan đến CLIP của SAM-CLIP, chúng tôi đánh giá nó với phân loại hình ảnh zero-shot trên ImageNet [14], ImageNet-v2 [71] và Places365 [100], cũng như truy xuất văn bản-sang-hình ảnh zero-shot trên Flickr30K [93] và COCO [47], dưới độ phân giải hình ảnh 336px. Đối với phân loại, chúng tôi sử dụng các text template như Radford et al. [68] sử dụng các textual embedding từ text encoder của SAM-CLIP (được giữ đông lạnh từ giáo viên CLIP của chúng tôi) để thực hiện phân loại zero-shot mà không cần tinh chỉnh gì. Đối với truy xuất, chúng tôi tính toán độ tương tự cosine giữa các embedding hình ảnh và văn bản để xếp hạng các hình ảnh cho mỗi text query và báo cáo metric Recall@1. Kết quả đánh giá được trình bày trong Bảng 1. Sử dụng kiến trúc ViT-B, mô hình của chúng tôi đạt được độ chính xác zero-shot tương đương với các mô hình CLIP ViT-B tiên tiến được tiền huấn luyện trên LAION-2B [77] và DataComp-1B [22] (cả hai đều được phát hành bởi Ilharco et al. [29]), trên ba tập dữ liệu phân loại. Hơn nữa, SAM-CLIP vượt trội hơn mô hình CLIP ViT-B/16 được huấn luyện trên DataComp-1B trên cả hai tập dữ liệu truy xuất Flickr30K và COCO. Những kết quả này xác nhận hiệu quả của phương pháp kết hợp của chúng tôi trong việc thừa hưởng các khả năng của CLIP. Lưu ý: Chúng tôi quan sát thấy rằng SAM-CLIP hưởng lợi từ độ phân giải 336px cho phân loại hình ảnh zero-shot, trong khi các mô hình CLIP cơ bản thì không, vì chúng được huấn luyện ở độ phân giải 224px (kết quả được báo cáo của các mô hình CLIP cơ bản trong Bảng 1 được đánh giá ở 224px). Kết quả đánh giá của SAM-CLIP ở độ phân giải 224px so với 336px được cung cấp trong Phụ lục A.

Nhiệm vụ SAM: Phân đoạn Instance Zero-Shot. Đối với thành phần SAM của SAM-CLIP, chúng tôi đánh giá hiệu suất của nó trong phân đoạn instance, một nhiệm vụ mà mô hình SAM ban đầu xuất sắc [38], với các tập dữ liệu COCO [47] và LVIS [24]. Theo các thực hành ban đầu của Kirillov et al. [38], chúng tôi đầu tiên tạo ra các bounding box phát hiện đối tượng sử dụng mô hình ViT-Det (phiên bản ViT-B) [45]. Các bounding box này hoạt động như geometric prompt cho prompt encoder của SAM, sau đó dự đoán mask cho mỗi instance đối tượng. Kết quả đánh giá của SAM-CLIP và SAM ViT-B ban đầu được cung cấp trong Bảng 1 (cả hai dưới độ phân giải 1024px), cho thấy rằng SAM-CLIP rất gần với SAM trên hai benchmark, không gặp phải quên lãng thảm khốc trong quá trình huấn luyện.

Chuyển giao Zero-Shot sang Phân đoạn Ngữ nghĩa. Chúng tôi mở rộng đánh giá của mình sang phân đoạn ngữ nghĩa zero-shot (được prompt bằng văn bản) trên 5 tập dữ liệu, Pascal VOC [16], Pascal Context [60], ADE20k [101], COCO-Stuff [6] và COCO-Panoptic [37]. Chúng tôi áp dụng một giao thức đánh giá phổ biến cho nhiệm vụ này: i) mỗi hình ảnh đầu vào được thay đổi kích thước thành 448×448px và truyền qua image encoder và CLIP-head của SAM-CLIP để có được các đặc trưng patch 28×28; ii) 80 CLIP text template được xác định trước của OpenAI được sử dụng để tạo ra các textual embedding cho mỗi lớp ngữ nghĩa, và các embedding này hoạt động như các bộ phân loại dự đoán mask và hoạt động trên các đặc trưng patch từ CLIP head; iii) chúng tôi tăng tỷ lệ tuyến tính các logit dự đoán mask để phù hợp với kích thước của hình ảnh đầu vào. Kết quả đánh giá của SAM-CLIP và các mô hình zero-shot trước đây trên năm tập dữ liệu được thể hiện trong Hình 2. Đáng chú ý, SAM-CLIP thiết lập hiệu suất tiên tiến mới trên tất cả 5 tập dữ liệu, với biên độ đáng kể so với các công trình trước đây. Thêm chi tiết được cung cấp trong Phụ lục C.

### 4.3. Đánh giá Head-Probing trên Biểu diễn Đã học

Bằng cách kết hợp các mô hình SAM và CLIP, chúng tôi dự đoán rằng mô hình kết quả sẽ thừa hưởng những lợi thế ở cấp độ biểu diễn từ cả hai mô hình cha. Cụ thể, SAM xuất sắc trong việc nắm bắt các chi tiết thị giác không gian cấp thấp liên quan đến các nhiệm vụ phân đoạn, trong khi CLIP chuyên môn hóa trong thông tin thị giác ngữ nghĩa cấp cao bao quát toàn bộ hình ảnh. Chúng tôi giả thuyết rằng mô hình đã kết hợp kết hợp những điểm mạnh này, từ đó nâng cao tính hữu ích của nó trong một phạm vi rộng các nhiệm vụ thị giác downstream. Để điều tra giả thuyết này, chúng tôi thực hiện các đánh giá head-probing (tức là học một head cụ thể theo nhiệm vụ với một image backbone đông lạnh) trên SAM, CLIP và SAM-CLIP, sử dụng các cấu trúc head phân đoạn khác nhau (linear head, DeepLab-v3 [10] và PSPNet [98]) trên hai tập dữ liệu phân đoạn ngữ nghĩa, Pascal VOC và ADE20k. Kết quả được trình bày trong Bảng 3. Chúng tôi quan sát thấy rằng các biểu diễn SAM không hoạt động tốt bằng các biểu diễn CLIP cho các nhiệm vụ đòi hỏi hiểu biết ngữ nghĩa, ngay cả đối với phân đoạn ngữ nghĩa. Tuy nhiên, SAM-CLIP vượt trội hơn cả SAM và CLIP trên các cấu trúc head và tập dữ liệu khác nhau, từ đó xác nhận khả năng biểu diễn đặc trưng thị giác vượt trội của nó.

Bên cạnh đó, chúng tôi áp dụng linear probing cho các mô hình này cho các nhiệm vụ phân loại hình ảnh trên hai tập dữ liệu, ImageNet và Places365. Kết quả trong Bảng 4 cho thấy rằng SAM-CLIP đạt được hiệu suất tương đương với CLIP, ngụ ý rằng biểu diễn cấp hình ảnh của SAM-CLIP cũng được học tốt. Tất cả kết quả đánh giá head probing được trực quan hóa trong Hình 4 để truyền đạt thông điệp một cách trực quan hơn.

### 4.4. Tổng hợp cả CLIP và SAM Head để Phân đoạn Tốt hơn

Cho rằng SAM-CLIP là một mô hình đa nhiệm vụ với SAM và CLIP head, người ta sẽ tự nhiên hỏi liệu hai head có thể làm việc cùng nhau để đạt hiệu suất tốt hơn trên một số nhiệm vụ. Ở đây, chúng tôi trình bày rằng một sự tổng hợp đơn giản của CLIP và SAM head có thể dẫn đến phân đoạn ngữ nghĩa zero-shot tốt hơn. Cụ thể, chúng tôi thay đổi kích thước hình ảnh đầu vào thành 1024px và truyền qua EncSAM-CLIP, và sử dụng CLIP head để tạo ra dự đoán mask độ phân giải thấp (32×32) sử dụng text prompt. Sau đó, chúng tôi tạo ra một số point prompt từ dự đoán mask (lấy mẫu quan trọng dựa trên độ tin cậy dự đoán mask), và truyền dự đoán mask và point prompt cùng nhau cho prompt encoder module như geometric prompt. Cuối cùng, HeadSAM nhận các embedding từ cả prompt encoder và image encoder để tạo ra các dự đoán mask độ phân giải cao (256×256) như được hiển thị trong Hình 2 (phải). Các ví dụ về pipeline này được hiển thị trong Hình 3. Người ta có thể quan sát rõ ràng rằng phân đoạn được tinh chỉnh bởi SAM-head chi tiết hơn. Chi tiết triển khai được thảo luận trong Phụ lục C.

Lưu ý rằng pipeline này chỉ yêu cầu một lượt truyền tiến trên EncSAM-CLIP với độ phân giải 1024px. Để so sánh công bằng, trong Bảng 1 và Hình 1, chúng tôi báo cáo hiệu suất phân đoạn zero-shot SAM-CLIP với độ phân giải 448px chỉ sử dụng HeadCLIP. Sử dụng pipeline độ phân giải cao của chúng tôi, chúng tôi có được thêm lợi ích trong phân đoạn ngữ nghĩa zero-shot như được hiển thị trong Bảng 5.

## 5. Kết luận

Chúng tôi đã thảo luận về việc kết hợp các mô hình nền tảng thị giác công khai, như các nguồn kiến thức thị giác được tiêu hóa, thành một kiến trúc thống nhất duy nhất. Chúng tôi đã đề xuất một công thức đơn giản và hiệu quả dựa trên chưng cất đa nhiệm vụ và luyện tập bộ nhớ. Cụ thể, chúng tôi đã khởi tạo phương pháp được đề xuất để kết hợp SAM và CLIP vision foundation model, và giới thiệu SAM-CLIP. SAM và CLIP có các khả năng thị giác bổ sung: một cái giỏi về hiểu biết không gian, trong khi cái kia xuất sắc trong hiểu biết ngữ nghĩa của hình ảnh. Chúng tôi chứng minh nhiều lợi ích như là kết quả của phương pháp được đề xuất: 1) Chúng tôi có được một vision backbone duy nhất với quên lãng tối thiểu các khả năng zero-shot của các mô hình ban đầu, phù hợp cho triển khai thiết bị biên. 2) Chúng tôi chứng minh rằng mô hình đã kết hợp tạo ra các biểu diễn phong phú hơn có thể sử dụng cho các nhiệm vụ downstream đa dạng hơn khi so sánh với các mô hình ban đầu trong thiết lập đánh giá head-probing. 3) Mô hình đã kết hợp thể hiện khả năng zero-shot hiệp lực mới nhờ vào các kỹ năng bổ sung được thừa hưởng từ các mô hình cha. Cụ thể, chúng tôi cho thấy rằng SAM-CLIP đạt được hiệu suất tiên tiến trên phân đoạn ngữ nghĩa zero-shot bằng cách kết hợp hiểu biết ngữ nghĩa của CLIP và kiến thức định vị của SAM.
