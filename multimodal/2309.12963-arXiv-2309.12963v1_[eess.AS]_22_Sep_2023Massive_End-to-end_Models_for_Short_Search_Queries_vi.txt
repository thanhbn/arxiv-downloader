# 2309.12963.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.12963.pdf
# Kích thước tệp: 195038 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2309.12963v1  [eess.AS]  22 Sep 2023Mô hình End-to-end Khổng lồ cho Truy vấn Tìm kiếm Ngắn
Weiran Wang Rohit Prabhavalkar Dongseong Hwang Qiujia Li
Khe Chai Sim Bo Li James Qin Xingyu Cai Adam Stooke
Zhong Meng CJ Zheng Yanzhang He Tara Sainath
Pedro Moreno Mengibar
Google LLC
{weiranwang,prabhavalkar,dongseong }@google.com
25 tháng 9, 2023
Tóm tắt
Trong công trình này, chúng tôi nghiên cứu hai mô hình nhận dạng giọng nói tự động (ASR) end-to-end phổ biến, cụ thể là Phân loại Thời gian Kết nối (CTC) và RNN-Transducer (RNN-T), cho việc nhận dạng offline các truy vấn tìm kiếm bằng giọng nói, với tối đa 2B tham số mô hình. Các bộ mã hóa của các mô hình chúng tôi sử dụng kiến trúc neural của mô hình giọng nói universal của Google (USM), với các lớp pooling funnel bổ sung để giảm đáng kể tốc độ khung hình và tăng tốc quá trình huấn luyện và suy luận. Chúng tôi thực hiện các nghiên cứu sâu rộng về kích thước từ vựng, chiến lược giảm thời gian, và hiệu suất tổng quát hóa trên các tập kiểm tra dạng dài. Mặc dù có suy đoán rằng, khi kích thước mô hình tăng lên, CTC có thể tốt như RNN-T vốn xây dựng sự phụ thuộc nhãn vào dự đoán, chúng tôi quan sát thấy rằng một RNN-T 900M rõ ràng vượt trội hơn một CTC 1.8B và chịu đựng tốt hơn việc giảm thời gian nghiêm trọng, mặc dù khoảng cách WER có thể được loại bỏ phần lớn bằng LM shallow fusion.

1 Giới thiệu
Gần đây có sự tập trung vào việc mở rộng quy mô các mô hình ASR end-to-end, chẳng hạn như phân loại thời gian kết nối (CTC) [1] và neural transducers (RNN-T, [2]), lên kích thước cực lớn [3, 4, 5, 6, 7] để khám phá lợi ích của các mô hình như vậy ở quy mô lớn. Nhiều mô hình này được thúc đẩy bởi mục tiêu huấn luyện một mô hình ASR đa ngôn ngữ duy nhất có thể hoạt động tốt trên nhiều ngôn ngữ. Ví dụ, Mô hình Giọng nói Universal (USM, [6]) là một mô hình CTC full-context 2B tham số được huấn luyện trên dữ liệu YouTube từ 300+ ngôn ngữ; Pratap et al. [7] huấn luyện các hệ thống ASR trên 1000+ ngôn ngữ.

Công trình hiện tại theo bước chân của những công trình trước đây. Khi các mô hình ngôn ngữ lớn (LLMs) đã nhanh chóng trở nên phổ biến trong lĩnh vực xử lý ngôn ngữ tự nhiên [8, 9, 10], việc tự nhiên là phải hỏi các mô hình ASR hoạt động như thế nào ở kích thước lớn. Cụ thể, hai lớp mô hình chính - CTC và RNN-T - so sánh như thế nào ở kích thước mô hình lớn? Đặc biệt, các mô hình này so sánh như thế nào trên một tác vụ (tìm kiếm bằng giọng nói, trong công trình này) nơi dữ liệu huấn luyện được ghép cặp có nhiều, cho một ngôn ngữ duy nhất (tiếng Anh) thay vì trong thiết lập đa ngôn ngữ.

Một câu hỏi liên quan và quan trọng trong thực tế liên quan đến khó khăn trong việc huấn luyện các mô hình end-to-end khổng lồ khi kích thước mô hình tăng lên. Vì chi phí tính toán các đầu ra của các bộ mã hóa trong

1

--- TRANG 2 ---
mô hình (hầu hết các tham số mô hình thường nằm trong bộ mã hóa) tăng tệ trong các bộ mã hóa dựa trên attention như transformers [11] và conformers [12], điều này có thể cấm đoán về mặt tính toán và bộ nhớ. Mô hình RNN-T, đặc biệt, đòi hỏi tính toán và bộ nhớ bổ sung cho mạng dự đoán và mạng kết hợp, điều này càng làm tăng thêm vấn đề.

Trong công trình này, chúng tôi nghiên cứu các vấn đề nêu trên và thấy rằng hai vấn đề - CTC và RNN-T so sánh như thế nào, và cách có thể huấn luyện chúng hiệu quả - không độc lập như có thể tưởng ban đầu: chúng tôi thấy rằng việc giảm tốc độ khung hình đầu ra của bộ mã hóa có thể được áp dụng lặp đi lặp lại tại nhiều lớp trong bộ mã hóa để có được việc giảm tốc độ khung hình đầu ra lớn; điều này lần lượt rất quan trọng để huấn luyện các mô hình hiệu quả. Trong khi nghiên cứu trước đây đã tích hợp việc giảm thời gian để giảm tốc độ khung hình cuối cùng xuống 60ms cho tìm kiếm bằng giọng nói [13, 14], chúng tôi thấy rằng chúng tôi có thể tăng việc giảm thời gian lên 6x với CTC lớn, và tất cả cách lên đến 16x với RNN-T lớn, của tốc độ khung hình cơ sở 40ms với funnel pooling [15], mà không hy sinh nhiều độ chính xác.

Ngoài ra, chúng tôi so sánh CTC và RNN-T như hệ thống ASR xương sống. CTC tạo ra các xác suất nhãn độc lập với các khung thời gian trước đó, và do đó nhanh hơn nhiều trong suy luận so với RNN-T. Đây là một lý do tại sao CTC được ưu tiên cho USM trong [6]. Tuy nhiên, chúng tôi thấy rằng RNN-T thực sự chính xác hơn CTC, và cụ thể một RNN-T 900M rõ ràng vượt trội hơn một CTC 1.8B, mặc dù mô hình CTC có lợi đáng kể từ việc tăng kích thước mô hình so với CTC 340M. Hơn nữa, chúng tôi thấy rằng RNN-T có thể chịu đựng được hệ số giảm thời gian lớn hơn nhiều. Để giải quyết bất kỳ lo ngại nào về RNN-T chậm hơn CTC, chúng tôi chạy cả hai mô hình (bao gồm beam search) trên TPU để đạt được độ trễ tốt nhất có thể.

Cuối cùng, với số lượng khung đầu ra nhỏ hơn nhiều và beam search trên TPU, shallow fusion [16, 17] với neural LM trở thành ứng viên khả thi để tăng cường độ chính xác của mô hình. Chúng tôi kết hợp cả CTC và RNN-T với các LM bên ngoài được huấn luyện với lượng lớn văn bản, và quan sát thấy rằng khoảng cách tỷ lệ lỗi từ (WER) giữa chúng có thể được loại bỏ phần lớn bằng LM fusion. Nghiên cứu của chúng tôi về shallow fusion là kịp thời, khi cộng đồng bắt đầu quan tâm đến việc sử dụng LLMs cho ASR. Chúng tôi chỉ ra rằng shallow fusion vẫn là một kỹ thuật hiệu quả để tích hợp LM, ngay cả đối với các mô hình lớn được huấn luyện với hàng trăm nghìn giờ âm thanh, và nên được xem xét như một baseline cho các kỹ thuật tiên tiến hơn.

2 Mô hình ASR End-to-end
Trong phần này, chúng tôi mô tả ngắn gọn các mô hình end-to-end được sử dụng trong công trình này - phân loại thời gian kết nối (CTC) [1], và neural transducers [2] - đặc biệt, hybrid autoregressive transducer (HAT) [18]. Vì các mô hình end-to-end hiện là một phần của dòng chính trong nhận dạng giọng nói tự động (ASR), chúng tôi chọn sự ngắn gọn; những độc giả quan tâm có thể tìm thêm thông tin trong các bài báo tổng quan gần đây [19, 20].

Ký hiệu Chúng tôi giả định rằng tín hiệu âm thanh đầu vào đã được tham số hóa thành các vector đặc trưng âm thanh phù hợp: x= [x1,...,xT′], có độ dài T′, trong đó xt∈Rd (đặc trưng log-mel 128 chiều, trong công trình này). Các đặc trưng âm thanh đầu vào được xử lý bằng cách sử dụng một bộ mã hóa, một mạng neural phù hợp (một Conformer [12], trong công trình này), chuyển đổi đầu vào thành một biểu diễn cấp cao hơn: h= [h1,...,hT], trong đó độ dài của biểu diễn được mã hóa thường ngắn hơn độ dài đầu vào ban đầu (T≤T′). Chúng tôi giả định rằng chúng tôi có một bản ghi tương ứng với mỗi phát âm: y= [y0=/an}bracketle{ts/an}bracketri}ht,y1,...,yU], trong đó mỗi yu∈ V, tập hợp các ký hiệu đầu ra (word-pieces [21], trong công trình này), và /an}bracketle{ts/an}bracketri}ht đại diện cho một ký hiệu đặc biệt bắt đầu câu.

2

--- TRANG 3 ---
2.1 Phân loại Thời gian Kết nối

Phân loại Thời gian Kết nối (CTC) được giới thiệu bởi Graves et al. [1], như một cách để huấn luyện các mô hình sequence-to-sequence có thể chuyển đổi chuỗi đầu vào x, thành chuỗi đầu ra y khi sự căn chỉnh giữa hai chuỗi không được biết. CTC thực hiện điều này bằng cách mô hình hóa phân phối có điều kiện, P(y|x), bằng cách biên hóa trên tất cả các đường căn chỉnh có thể giữa hai chuỗi:

P(y|x) =P(y|h(x)) =/summationdisplay
a∈Bctc(y)T/productdisplay
t=1P(at|h) (1)

trong đó, Bctc(y) tương ứng với tập hợp tất cả các căn chỉnh hợp lệ. Cụ thể, một căn chỉnh a∈ Bctc(y) là một căn chỉnh hợp lệ nếu nó chứa |h(x)|=T ký hiệu từ tập hợp các đầu ra được tăng cường với một ký hiệu blank đặc biệt - V ∪ {/an}bracketle{tb/an}bracketri}ht}; và nếu thêm vào đó, việc loại bỏ các ký hiệu không blank lặp lại liên tiếp và sau đó loại bỏ tất cả các ký hiệu /an}bracketle{tb/an}bracketri}ht tạo ra chuỗi nhãn ban đầu y. Như có thể thấy trong (1), các mô hình CTC đưa ra một giả định độc lập có điều kiện mạnh - rằng các nhãn đầu ra có điều kiện độc lập với các đặc trưng âm thanh được mã hóa đầu vào. Tuy nhiên, các mô hình này hoạt động tốt trong thực tế [22, 23], đặc biệt với các bộ mã hóa lớn [6]. Cuối cùng, như có thể thấy trong (1), mô hình CTC tạo ra một ký hiệu đầu ra (blank, hoặc không blank) cho mỗi bước thời gian của bộ mã hóa.

2.2 Neural Transducers

Recurrent neural network transducer (RNN-T) được đề xuất bởi Graves et al. [2], như một cải tiến so với mô hình CTC, để giảm các giả định độc lập có điều kiện trong mô hình. Điều này được thực hiện bằng cách giới thiệu một mạng dự đoán riêng biệt, mô hình hóa sự phụ thuộc nhãn (và do đó làm cho các đầu ra mô hình có điều kiện phụ thuộc vào chuỗi các dự đoán trước đó).

P(y|x) =P(y|h(x)) =/summationdisplay
a∈Bnt(y)T+U/productdisplay
τ=1P(aτ|qiτ,hτ−iτ)

trong đó, Bnt(y) là tập hợp tất cả các căn chỉnh hợp lệ - các chuỗi có độ dài T+U, giống hệt với y sau khi loại bỏ tất cả các blank (tức là, mỗi chuỗi có chính xác T ký hiệu blank); iτ đại diện cho số lượng nhãn không blank trong căn chỉnh một phần a1,...,aτ−1; và, qj đại diện cho đầu ra của mạng dự đoán sau khi xử lý j−1 nhãn đầu ra đầu tiên: qj= PredNetwork(yj−1,...,y0).

Trong khi các công trình ban đầu sử dụng mạng LSTM hồi quy để mô hình hóa mạng dự đoán, trong công trình này, mạng dự đoán được mô hình hóa như mạng nhúng |V|2 [24], có đầu ra chỉ phụ thuộc vào hai nhãn không blank cuối cùng yj−1, và yj−2. Đáng chú ý, neural transducers cho phép mô hình đầu ra nhiều nhãn đầu ra (không blank) tại mỗi khung; các blank tương ứng với sự chuyển đổi sang khung mã hóa đầu vào tiếp theo.

2.2.1 Hybrid Autoregressive Transducer

Mô hình hybrid autoregressive transducer (HAT) được đề xuất bởi Variani et al. [18] cải tiến so với cấu trúc neural transducer cơ bản theo hai cách. Thứ nhất, nó nhân tố hóa phân phối đầu ra P(aτ|qiτ,hτ−iτ) thành hai phân phối riêng biệt: một phân phối Bernoulli để mô hình hóa các ký hiệu blank (tức là, nếu aτ=/an}bracketle{tb/an}bracketri}ht), và một phân phối xác suất riêng biệt cho các nhãn không blank. Thứ hai, mô hình đề xuất khái niệm về một mô hình ngôn ngữ nội bộ (ILM), PILM(y), đại diện cho LM được học bởi mô hình dựa trên dữ liệu huấn luyện; ILM do đó có thể được trừ ra khỏi phân phối posterior, trước khi fusion với một LM bên ngoài, PEXT(y), trong quá trình giải mã, bằng cách thêm các siêu tham số có thể điều chỉnh α, và β có thể được đặt dựa trên một tập phát triển để tạo ra giả thuyết có khả năng nhất [25, 26, 27, 28]:

y∗= arg max
ylogP(y|x)−αlogPILM(y) +βlogPEXT(y)

3 Kiến trúc mô hình

3.1 USM Conformer

USM sử dụng kiến trúc Transformer được tăng cường bằng convolution [12], hoặc Conformer. Mỗi khối Conformer bao gồm một mô-đun feed-forward (FFN), một mô-đun multi-head self-attention (MHSA), một mô-đun convolution (Conv), và một mô-đun feed-forward thứ hai. Nếu đầu vào cho khối Conformer thứ l là x(l), thì đầu ra của nó (và, đầu vào cho khối tiếp theo), x(l+1), được tính như sau:

˜x(l)=x(l)+1
2FFN1/parenleftbig
x(l)/parenrightbig
x′(l)=˜x(l)+MHSA/parenleftbig
Wq˜x(l),Wk˜x(l),Wv˜x(l)/parenrightbig
(2)
x′′(l)=x′(l)+Conv/parenleftbig
x′(l)/parenrightbig
x(l+1)=LayerNorm/parenleftbig
x′′(l)+1
2FFN2(x′′(l))/parenrightbig

Đối với lớp conformer tiêu chuẩn, độ dài đầu vào và độ dài đầu ra là như nhau.

3.2 Funnel Pooling

Chúng tôi cải thiện tốc độ huấn luyện và suy luận trong các mô hình của chúng tôi bằng cách giảm độ dài chuỗi nhúng bộ mã hóa so với độ dài âm thanh. Tại các khối Conformer được chọn trong bộ mã hóa, chúng tôi sử dụng kỹ thuật pooling trong mô-đun MHSA được giới thiệu trong Funnel-Transformers [15]. Tại một lớp funnel self-attention, toàn bộ đầu vào được sử dụng để tạo ra các chuỗi key và value như thường lệ, nhưng strided-average pooling được áp dụng cho chiều thời gian của đầu vào được sử dụng trong việc tạo ra các vector query. Tức là, chúng tôi thay thế mô-đun MHSA trong (2) bằng (3) dưới đây.

ˆx(l)=StridedPooling (˜x(l)),
x′(l)=ˆx(l)+MHSA/parenleftbig
Wqˆx(l),Wk˜x(l),Wv˜x(l)/parenrightbig
.(3)

Stride của pooling trong (3) là hệ số giảm thời gian trong lớp này. Trong một mô hình ngôn ngữ, phương pháp pool-query-only này đã được chứng minh là mang lại một lợi thế nhỏ so với việc đơn giản pooling toàn bộ chuỗi nhúng ẩn giữa các lớp [15]; có thể, độ chi tiết cao hơn trong các chuỗi key và value cho phép mạng học được một nén ít mất mát hơn. Việc tiết kiệm tính toán tích lũy trong tất cả các lớp Conformer tiếp theo theo độ phức tạp O(T2D) cho self-attention, và tuyến tính cho mạng feed-forward.

Công trình liên quan về giảm thời gian Giảm thời gian đã là một kỹ thuật hữu ích để đạt được sự cân bằng giữa độ dài đầu vào và đầu ra cho ASR, và đã phát triển theo thời gian với sự lựa chọn kiến trúc neural và đơn vị mô hình hóa. Trong [29, 30], các tốc độ khung hình thấp hơn được đạt được bằng cách nối các khung đầu vào và striding, cho các mô hình DNN hoặc LSTM dự đoán các trạng thái HMM phụ thuộc ngữ cảnh.

4

--- TRANG 4 ---
[31, 32, 33] đề xuất sử dụng RNN phân cấp/kim tự tháp nơi các đầu ra của các bước liên tiếp được kết hợp trước khi đưa vào lớp tiếp theo. Sau khi cộng đồng chuyển sang các hệ thống end-to-end và các đơn vị mô hình hóa kiểu word-piece, một tốc độ khung hình phổ biến là 40ms được đạt được bằng convolutional subsampling, và được áp dụng bởi các thư viện mã nguồn mở được sử dụng rộng rãi [34]. Với kiến trúc Conformer [12], công trình trước đây về tìm kiếm bằng giọng nói chủ yếu có tốc độ khung hình cuối cùng là 60ms, được đạt được bằng tốc độ khung hình đầu vào 30ms và một lớp giảm thời gian 2x sớm trong bộ mã hóa, với stacking [13] hoặc funnel pooling [14]. Về mặt hàm mất mát, công trình gần đây của [35] đề xuất sử dụng CTC để (không đều) chọn các khung đầu ra bộ mã hóa cho mô hình hóa RNN-T, hiệu quả giảm tốc độ khung hình cho bộ giải mã. Tuy nhiên, việc chọn đầu ra bộ mã hóa dựa trên CTC như vậy không tiết kiệm bất kỳ tính toán nào trong bộ mã hóa, mà chỉ giảm tính toán trong các mô-đun hạ nguồn. Mục tiêu của chúng tôi trong công trình này là giảm mạnh độ dài chuỗi sớm trong kiến trúc để tiết kiệm tính toán.

4 Thí nghiệm

4.1 Tập dữ liệu

Các thí nghiệm của chúng tôi tập trung vào các truy vấn tìm kiếm bằng giọng nói ngắn. Đối với phần lớn các thí nghiệm, chúng tôi sử dụng 520M phát âm của các truy vấn tìm kiếm bằng giọng nói để huấn luyện; tổng lượng âm thanh là 490K giờ và thời lượng trung bình mỗi phát âm là 3,4 giây. Một tỷ lệ nhỏ của dữ liệu huấn luyện được ghi chép bởi con người trong khi phần còn lại được gắn nhãn giả bởi một giáo viên RNN-T hai chiều 600M [36]. Chúng tôi tokenize các bản ghi huấn luyện bằng các mô hình word-piece [21].

Chúng tôi sử dụng cả dữ liệu âm thanh thật và dữ liệu được tạo bởi TTS để đánh giá. Các phát âm âm thanh thật đại diện cho lưu lượng tìm kiếm bằng giọng nói điển hình, với thời lượng trung bình 3,9 giây. Tập phát triển của chúng tôi bao gồm 9K phát âm âm thanh thật (được ký hiệu là VS-dev); chúng tôi sử dụng một tập kiểm tra riêng biệt gồm 5K phát âm để kiểm tra (được ký hiệu là VS-test). Các tập TTS chứa các danh từ riêng hiếm (RPN) xuất hiện ít hơn 5 lần trong tập huấn luyện, và chúng là các testbed tốt để tích hợp LM bên ngoài. Mỗi tập TTS chứa 10K phát âm và bao gồm một trong năm miền: Maps (được ký hiệu là RPNM), News (RPNN), Play (RPNP), Search query logs (RPNS), và Youtube (RPNY); chúng có thời lượng trung bình lần lượt là 5,9, 10,1, 5,3, 5,4, và 5,8 giây. Chúng tôi sử dụng VS-dev, RPNM và RPNN để điều chỉnh kiến trúc mô hình và các siêu tham số khác và báo cáo WER cuối cùng trên các tập còn lại.

Để huấn luyện LM cho fusion, mỗi minibatch được lấy mẫu 50/50 từ các bản ghi của dữ liệu huấn luyện âm thanh, và dữ liệu chỉ có văn bản chứa 50B phát âm. Dữ liệu chỉ có văn bản chứa các truy vấn tìm kiếm văn bản từ các miền Maps, Textual search query logs, News, Play, và Youtube, và một chiến lược cắt tỉa dựa trên tần suất, được thiết kế để cải thiện mô hình hóa từ hiếm, được triển khai để điều chỉnh xác suất chọn mỗi truy vấn [37]. Chúng tôi huấn luyện transformer LM với 128M và 1B tham số (không tính các tham số trong softmax cuối cùng) với wordpieces tương thích với những của các mô hình E2E.

4.2 Kiến trúc mô hình

Chúng tôi sử dụng năng lượng bộ lọc Mel-log 128 chiều (được trích xuất từ cửa sổ 32ms và shift 10ms) làm đặc trưng frontend. Sau hai lớp 2D-convolution, cả hai đều có stride (2,2), chuỗi đặc trưng kết quả có tốc độ khung hình 40ms và trở thành đầu vào cho kiến trúc Conformer của chúng tôi. Kiến trúc này bắt chước của mô hình giọng nói universal của Google (USM, [6]). Số lượng attention head được sử dụng trong các khối Conformer là 8, và chiều trung gian của các FFN là 4

5

--- TRANG 5 ---
lần chiều mô hình. Các khối Conformer sử dụng local self-attention với một attention span lớn, và đầu ra bộ mã hóa có một trường tiếp nhận đủ lớn để bao phủ toàn bộ phát âm. Như quan sát bởi [3], pre-training không giám sát không giúp ích cho độ chính xác ASR khi sử dụng một lượng lớn dữ liệu âm thanh có giám sát, và do đó chúng tôi bỏ qua bước pre-training trong công trình này (ngoại trừ việc bao gồm một mô hình CTC baseline với pretraining trong Bảng 1).

Chúng tôi khám phá hai cấu hình bộ mã hóa khác nhau cho CTC, với các kích thước khác nhau: cấu hình bộ mã hóa nhỏ hơn bao gồm 24 lớp Conformer có chiều 768, dẫn đến tổng cộng 340M tham số; bộ mã hóa lớn hơn có 32 lớp Conformer có chiều 1536, dẫn đến tổng cộng 1.8B tham số. Đối với RNN-T, bộ mã hóa bao gồm 16 lớp Conformer có chiều 1536, dẫn đến tổng cộng 870M tham số. Chúng tôi sử dụng bộ giải mã nhúng |V|2 [24], tức là, mạng dự đoán tính toán các đặc trưng LM dựa trên hai token không blank trước đó, điều này đã được chứng minh là hoạt động tốt trên dữ liệu tìm kiếm bằng giọng nói. Đầu ra mô hình sử dụng nhân tố hóa HAT [18] đã được chứng minh là có lợi cho việc tích hợp LM bên ngoài.

Khác biệt với USM, chúng tôi thực hiện giảm thời gian đáng kể trong kiến trúc bộ mã hóa. Theo [14], chúng tôi ban đầu bắt đầu funnel pooling tại chỉ số lớp 4 (dựa trên zero), và áp dụng pooling trong các lớp tiếp theo để đạt được hệ số giảm thời gian mong muốn. Như một ví dụ, nếu chúng tôi thực hiện pooling tại các lớp với chỉ số (dựa trên zero) 4 và 5, mỗi lớp với hệ số giảm 2, chúng tôi đạt được tổng hệ số giảm 4 cho lớp 6 trở đi, tức là, độ dài chuỗi sau lớp 6 là 1/4 độ dài đầu vào ban đầu và tốc độ khung hình tại đầu ra bộ mã hóa là 160ms, đây cũng là tốc độ khung hình mà bộ giải mã hoạt động. Lưu ý rằng [14] đã sử dụng funnel pooling cho mô hình hóa trên thiết bị với các yêu cầu độ trễ nghiêm ngặt, trong khi ở đây chúng tôi sử dụng funnel pooling cho mô hình full-context để tăng tốc huấn luyện và suy luận. Chúng tôi quan sát thấy rằng độ chính xác ASR hóa ra chịu đựng tốt hơn với việc giảm thời gian. Trong Sec 4.5, chúng tôi nghiên cứu tác động của việc bắt đầu funnel pooling tại các lớp sớm hơn để giảm thêm chi phí suy luận.

Mỗi mô hình được huấn luyện với bộ tối ưu hóa Adafactor [38] với batch size 4096 phát âm. Chúng tôi huấn luyện các mô hình CTC đến 500K bước và các mô hình RNN-T đến 300K bước, tại thời điểm đó WER của chúng trên các tập phát triển đã ổn định. Trong một nghiên cứu thí điểm, chúng tôi đã thực hiện huấn luyện phân biệt với mục tiêu tỷ lệ lỗi từ tối thiểu [39]. Chúng tôi không đạt được lợi ích WER trên các tập VS và cải thiện tương đối 3% trên các tập RPN. Chúng tôi chỉ trình bày kết quả với huấn luyện maximum likelihood trong công trình này, và việc nghiên cứu cẩn thận huấn luyện phân biệt cho các mô hình lớn là công việc tương lai.

Đối với CTC, chúng tôi thực hiện giải mã tham lam khi không sử dụng LM bên ngoài, trong trường hợp này beam search không cung cấp lợi ích WER bổ sung. Chúng tôi thực hiện prefix beam search [40] với beam size 8 khi kết hợp với LM bên ngoài. Hai loại xác suất tiên nghiệm được sử dụng cho CTC shallow fusion: tiên nghiệm đồng nhất trên các không-blank như được triển khai bởi kỹ thuật blank probability downscaling [41], so với tiên nghiệm uni-gram dựa trên posterior mô hình trên tập huấn luyện [42]. Chúng tôi thực hiện label synchronous beam search (với path merging) cho RNN-T với beam size 8, và thực hiện trừ điểm ILM nội bộ trong trường hợp shallow fusion.

4.3 Kết quả CTC

Trực quan, với từ vựng lớn hơn, các chuỗi nhãn ngắn hơn và chúng tôi có thể cho phép giảm thời gian nhiều hơn. Ràng buộc cứng cho CTC là, vì nó chỉ phát ra một token (blank hoặc không blank) tại mỗi khung đầu ra bộ mã hóa, độ dài chuỗi đầu ra bộ mã hóa phải dài hơn độ dài chuỗi nhãn (tuy nhiên RNN-T không phải chịu ràng buộc này). Trong quá trình huấn luyện CTC, chúng tôi loại bỏ các phát âm vi phạm ràng buộc này, mặc dù các trường hợp như vậy không phổ biến trong các tập VS. Ví dụ, đối với kích thước từ vựng 16K, tại tốc độ khung hình cuối cùng 320ms (giảm 8x), khoảng 1% phát âm vi phạm ràng buộc này như ước tính trên VS-dev.

6

--- TRANG 6 ---
Bảng 1: WER (%) trên các tập dev bởi CTC, với các kiến trúc và kích thước từ vựng khác nhau. Funnel pooling được sử dụng tại các lớp 4 và 5, với hệ số 2,2 cho tốc độ khung hình 160ms và 3,2 cho tốc độ khung hình 240ms.

Kích thước mô hình (tốc độ khung hình) VS-dev RPNM RPNN
baseline: không pooling, kích thước từ vựng=4096
340M (40ms) 4.7 15.1 12.4
1.8B (40ms), với pretraining 4.2 14.2 10.4
với funnel pooling, kích thước từ vựng=16384
340M (160ms) 4.5 15.3 16.6
340M (240ms) 4.5 14.9 21.6
1.8B (160ms) 4.3 13.7 12.7
1.8B (240ms) 4.2 13.8 17.3
1.8B (320ms) 5.0 14.0 26.4
1.8B (160ms) + fusion LM 128M
blank downscaling [41] 3.8 10.7 11.0
model-based prior [42] 3.8 10.5 10.8
1.8B (160ms) + fusion LM 1B
blank downscaling [41] 3.8 10.1 10.4
model-based prior [42] 3.8 9.8 10.1

Chúng tôi báo cáo WER của một lựa chọn các mô hình CTC trong Bảng 1. Như baseline, chúng tôi báo cáo WER với từ vựng 4K tại tốc độ khung hình 40ms, một thiết lập theo sát kiến trúc USM [6]. Chúng tôi khớp những baseline này trên VS với kích thước từ vựng 16K và tốc độ khung hình thấp hơn nhiều là 160ms và 240ms: đối với CTC 340M, chúng tôi có được cải thiện từ 4.7% lên 4.5% trên VS-dev, trong khi đối với CTC 1.8B, chúng tôi khớp 4.2% mà không cần pretraining. Đối với huấn luyện có giám sát với CTC 1.8B, chúng tôi quan sát thấy tăng tốc 4x trong thời gian huấn luyện với tốc độ khung hình 160ms, và tăng tốc 4.5x với tốc độ khung hình 240ms so với mô hình 40ms (mặc dù sử dụng phép toán softmax tốn kém hơn do từ vựng lớn hơn). Tuy nhiên, chúng tôi quan sát thấy rằng với tốc độ khung hình 320ms, WER trên VS-dev giảm đáng kể xuống 5.0%, và điều này không thể được giảm nhẹ bằng cách tăng kích thước từ vựng lên 32K. Điều này cho thấy rằng độ phân giải thời gian quá thô không hoạt động tốt với CTC loss và các giả định độc lập cơ bản của nó.

Xu hướng WER trên RPNM tương tự như VS-dev. Tuy nhiên, chúng tôi quan sát thấy sự giảm tệ hơn trên RPNN khi chúng tôi áp dụng việc giảm thời gian nặng hơn. Như đã đề cập trong Sec 4.1, các bản ghi của RPNN đến từ miền News có đặc điểm ngôn ngữ khác biệt so với tìm kiếm bằng giọng nói, và độ dài âm thanh khá dài hơn (trung bình 10 giây) so với tập VS (trung bình 4 giây). Chúng tôi giả thuyết rằng các mô hình với tốc độ khung hình thấp hơn có thể không tổng quát hóa tốt cho độ dài âm thanh chưa thấy, và nghiên cứu thêm vấn đề này trong Sec 4.6.

Cho rằng công trình gần đây về các mô hình lớn dựa trên CTC [5, 6], người ta có thể suy đoán rằng khi mô hình CTC hai chiều trở nên lớn hơn, và với lượng lớn dữ liệu huấn luyện, giả định mô hình hóa cơ bản của CTC giữ gần đúng và nó có thể đạt độ chính xác tiên tiến bằng chính nó. Chúng tôi thách thức suy đoán này bằng cách thực hiện LM shallow fusion cho mô hình CTC 1.8B với tốc độ khung hình 160ms. Chúng tôi quan sát thấy giảm WER đáng kể từ 4.2% xuống 3.7 - 3.8% với LM 128M nhỏ hơn đã trên tập VS-dev trong miền. Kết quả chúng tôi trình bày trong Bảng 1 sử dụng trọng số LM đạt được sự cân bằng tốt giữa VS-dev và các tập RPN. Nếu chúng tôi chỉ tập trung vào VS,

7

--- TRANG 7 ---
tốt nhất chúng tôi có thể đạt được trên VS-dev là 3.7%, với WER rõ ràng tệ hơn trên các tập RPN. Trong số hai phương pháp ước tính tiên nghiệm, model-based prior [42] vượt trội nhất quán so với blank probability downscaling [41] qua các điểm hoạt động WER khác nhau. Khi tăng kích thước LM bên ngoài lên 1B, chúng tôi không thể cải thiện thêm trên VS-dev nhưng đạt được lợi ích đáng kể trên các tập từ hiếm.

4.4 Kết quả RNN-T

Chúng tôi thực hiện một tập hợp thí nghiệm giảm thời gian tương tự với RNN-T và kết quả được hiển thị trong Bảng 2. Nhìn chung RNN-T khá bền vững với các kích thước từ vựng khác nhau. Với từ vựng 16K, chúng tôi nghiên cứu nhiều thiết lập giảm thời gian nhất, và chúng tôi quan sát thấy chỉ có những suy giảm WER nhỏ từ tốc độ khung hình 160ms tất cả cách lên đến tốc độ khung hình 640ms trên VS-dev và RPNM. Giống như CTC, hiệu suất trên tập dữ liệu "ngoài miền" RPNN giảm khi tốc độ khung hình giảm. So sánh các mô hình ở cùng tốc độ khung hình 160ms và kích thước từ vựng 16K, RNN-T có WER 3.7% trên VS-dev, vượt trội hơn 4.2% của CTC với biên độ lớn, và đạt được sự ngang bằng với CTC + shallow fusion trên tập này. Điều này chứng minh lợi ích của việc có một bộ mã hóa đặc trưng LM có thể học được để mô hình hóa sự phụ thuộc nhãn trong ASR end-to-end.

Khi kết hợp mô hình RNN-T tốt nhất với LM 128M, chúng tôi quan sát thấy thú vị rằng nó có xu hướng giảm đáng kể RPNN (với lỗi xóa nặng) và WER khá nhạy cảm với trọng số LM nội bộ và bên ngoài. Chúng tôi liệt kê một tập kết quả trong Bảng 2 (bảng dưới) đạt được sự cân bằng tốt giữa VS-dev và RPNM, tuy nhiên nó làm giảm RPNN từ 12.8% xuống 32.6%. Chúng tôi giả thuyết điều này là do thiên lệch của mô hình ngôn ngữ nội bộ của HAT được học hoàn toàn trên các phát âm ngắn. Trong Sec 4.6, chúng tôi cung cấp thêm bằng chứng cho điều này, bằng cách chứng minh rằng huấn luyện trên dữ liệu đa dạng độ dài cải thiện WER và độ bền vững với shallow fusion trên RPNN.

Bảng 2: WER (%) tập dev bởi RNN-T với các kích thước từ vựng và tốc độ khung hình khác nhau. Bộ mã hóa chứa 870M tham số. Funnel pooling bắt đầu tại lớp 4 và áp dụng cho các lớp liền kề. Chúng tôi sử dụng ký hiệu 3x2 để chỉ ra rằng lớp 4 có hệ số giảm 3, và lớp 5 có hệ số giảm 2, mang lại tốc độ khung hình cuối cùng 240ms.

Tốc độ khung hình (hệ số giảm) VS-dev RPNM RPNN
kích thước từ vựng=4096, kích thước decoder=10M
240ms (3x2) 3.8 12.6 12.1
320ms (2x2x2) 3.8 12.9 14.2
kích thước từ vựng=16384, kích thước decoder=33M
160ms (2x2) 3.7 12.3 12.8
240ms (3x2) 3.8 12.5 12.2
320ms (2x2x2) 3.8 12.5 13.6
400ms (5x2) 3.8 12.7 15.6
480ms (3x2x2) 3.9 12.6 19.5
640ms (2x2x2x2) 3.9 12.9 28.7
kích thước từ vựng=32768, kích thước decoder=65M
320ms (2x2x2) 3.8 12.9 14.8
640ms (2x2x2x2) 3.9 12.7 25.9
kích thước từ vựng=16384, fusion LM 128M
160ms (2x2) 3.8 11.0 32.6

8

--- TRANG 8 ---
Bảng 3: WER (%) tập dev bởi RNN-T 900M với kích thước từ vựng 16K và tốc độ khung hình 240ms (giảm 3x2), với funnel pooling bắt đầu trong các lớp Conformer sớm hơn. Hàng đầu tiên được lấy từ Bảng 2.

Chỉ số lớp bắt đầu VS-dev RPNM RPNN
4 3.8 12.5 12.2
3 3.7 12.6 12.0
2 3.7 12.4 11.7
1 3.7 12.6 12.3

Bảng 4: WER (%) tập dev bởi RNN-T 900M với từ vựng 16K và hai tốc độ khung hình 240ms và 640ms được huấn luyện trên dữ liệu đa miền. Funnel pooling bắt đầu từ lớp 4. Hai hàng đầu tiên được lấy từ Bảng 2.

Tốc độ khung hình VS-dev RPNM RPNN
Huấn luyện tìm kiếm bằng giọng nói
240ms 3.8 12.5 12.2
640ms 3.9 12.9 28.7
Huấn luyện đa miền
240ms 3.6 13.6 6.9
640ms 3.8 12.6 9.5
+ fusion LM 128M
240ms 3.7 11.5 7.1
Giáo viên đa miền 600M, kích thước từ vựng=4096
60ms 4.0 11.5 7.1

4.5 Vị trí của Funnel Pooling

Cho đến nay, chúng tôi đã bắt đầu funnel pooling từ lớp 4, theo công trình trước đây [14] làm việc trên các mô hình nhỏ hơn với các ràng buộc độ trễ cứng. Trong phần này, chúng tôi bắt đầu pooling sớm hơn trong kiến trúc, dẫn đến suy luận hiệu quả hơn.

Chúng tôi sử dụng mô hình RNN-T tốc độ khung hình 240ms từ Bảng 2 làm baseline, với hệ số giảm thời gian 3 và 2 trong hai lớp liên tiếp, và thay đổi chỉ số lớp bắt đầu pooling thành 3, 2, và 1. Huấn luyện của mô hình với pooling bắt đầu tại lớp 0 phân kỳ với cùng tham số học tập và chúng tôi không báo cáo hiệu suất của nó. Kết quả của các mô hình này trên các tập phát triển được báo cáo trong Bảng 3, cho thấy rằng trên tốc độ khung hình cơ sở 40ms, mô hình khá bền vững với vị trí của các lớp pooling, và thực tế chúng tôi có được lợi ích WER nhỏ trên các tập RPN bằng cách bắt đầu pooling tại lớp 2. Chúng tôi dự định thay thế các lớp convolutional subsampling 2D trước lớp Conformer bằng các lớp funnel pooling để khám phá thêm khả năng giảm khung hình.

4.6 Thêm Dữ liệu Huấn luyện Dạng Dài

Để xác minh rằng hiệu suất kém của các mô hình end-to-end trên RPNN là do thiếu âm thanh huấn luyện dài hơn, chúng tôi lặp lại một số thí nghiệm RNN-T với dữ liệu huấn luyện đa miền bổ sung [43]. Đáng chú ý nhất, chúng tôi bao gồm dữ liệu âm thanh YT được phân đoạn chứa 520M phát âm với thời lượng trung bình 9.8 giây, cho chúng tôi tổng cộng 600K giờ dữ liệu huấn luyện dạng dài hơn.

Các so sánh giữa các mô hình RNN-T được huấn luyện trên dữ liệu tìm kiếm bằng giọng nói và dữ liệu đa miền

9

--- TRANG 9 ---
được trình bày trong Bảng 4. Đối với hai tốc độ khung hình rất khác nhau 240ms và 640ms, dữ liệu huấn luyện dạng dài bổ sung cải thiện WER trên cả VS-dev và RPNN rất nhiều. Chỉ với dữ liệu huấn luyện tìm kiếm bằng giọng nói, khoảng cách WER trên RPNN giữa hai tốc độ khung hình từng rất lớn (12.2% vs 28.7%), trong khi với huấn luyện đa miền, khoảng cách được giảm đáng kể (6.9% vs 9.5%) với WER tuyệt đối được cải thiện nhiều. Khi thực hiện shallow fusion cho mô hình tốc độ khung hình 240ms đa miền với cùng LM 128M được sử dụng trong Sec 4.4, chúng tôi đạt được sự cân bằng tốt hơn giữa các tập kiểm tra trong miền và ngoài miền, cải thiện RPNM từ 13.6% lên 11.5% và không ảnh hưởng nhiều đến RPNN.

Thú vị khi thêm giáo viên RNN-T hai chiều [36] vào so sánh, như hiển thị trong bảng dưới của Bảng 4. Giáo viên 600M được huấn luyện trên phiên bản sớm hơn của dữ liệu đa miền, nơi phần tìm kiếm bằng giọng nói nhỏ hơn nhưng được ghi chép hoàn toàn bằng tay, và được sử dụng để tạo ra các nhãn giả của dữ liệu tìm kiếm bằng giọng nói được sử dụng trong bài báo này. Lưu ý rằng chúng tôi đang vượt qua hoặc ngang bằng với hiệu suất của giáo viên trên VS và RPNN, có thể do lượng lớn hơn dữ liệu huấn luyện và ghi nhãn nhất quán hơn. Giáo viên vượt trội hơn các mô hình mới trên RPNM, có thể vì, như một mô hình end-to-end, nó dự đoán thậm chí ít từ hiếm hơn trong các nhãn giả của nó so với các bản ghi chép tay, cho học sinh bắt chước.

4.7 Đánh giá Cuối cùng

Cuối cùng, chúng tôi so sánh các cấu hình tốt nhất từ cả CTC và RNN-T trên các tập kiểm tra, cụ thể là VS-test, RPNP, RPNS, và RPNY. Tính đến cả hiệu suất trong miền và ngoài miền, chúng tôi chọn mô hình CTC 1.8B với tốc độ khung hình 160ms, và mô hình RNN-T 900M với tốc độ khung hình 240ms. Kết quả được hiển thị trong Bảng 5. Ưu điểm tương đối giữa các phương pháp nhất quán với hiệu suất trên các tập phát triển. Tức là, với cùng dữ liệu huấn luyện tìm kiếm bằng giọng nói, RNN-T 900M vượt trội hơn CTC 1.8B trên tất cả các tập kiểm tra, và với biên độ tương đối 8.0% cho tập VS-test trong miền (4.5% vs 4.9%). Để có hiệu suất shallow fusion ổn định hơn trên các tập kiểm tra ngoài miền, có lợi khi huấn luyện RNN-T trên dữ liệu đa miền, và chúng tôi mong đợi điều tương tự cho CTC. Như một tham chiếu, chúng tôi cung cấp kết quả của một mô hình RNN-T nhỏ khác, với tốc độ khung hình 60ms và ngữ cảnh bên phải hạn chế 0.9s, từ công trình trước đây [14].

Bảng 5: WER (%) tập kiểm tra bởi CTC và RNN-T, với từ vựng 16K. Funnel pooling bắt đầu từ lớp 4. Theo mặc định, chúng tôi sử dụng dữ liệu huấn luyện VS.

VS-test RPNP RPNS RPNY
CTC 1.8B, tốc độ khung hình 160ms
4.9 39.8 23.1 26.0
+ fusion LM 128M với model prior
4.5 34.1 17.0 20.8
RNN-T 900M, tốc độ khung hình 240ms
4.5 37.8 20.6 23.3
+ Dữ liệu huấn luyện đa miền
4.4 36.4 19.9 22.3
+ fusion LM 128M với ILM
4.4 33.9 16.9 20.2
RNN-T đa miền 120M [14]
tốc độ khung hình 60ms, look-ahead 0.9s
5.0 35.9 19.2 23.2

5 Kết luận

Chúng tôi đã so sánh hai mô hình ASR end-to-end chính, CTC và RNN-T, trên một tác vụ tìm kiếm bằng giọng nói quy mô lớn. Chúng tôi đã xác minh rằng RNN-T rõ ràng chính xác hơn CTC cho dữ liệu kiểm tra trong miền ngay cả với kích thước mô hình nhỏ hơn, mặc dù khoảng cách có thể được loại bỏ phần lớn bằng LM fusion bù đắp cho giả định độc lập nhãn cơ bản của CTC. Chúng tôi cũng đã quan sát thấy rằng, đối với các mô hình lớn, việc giảm thời gian có hiệu quả trong việc giảm chi phí suy luận và huấn luyện mà không hy sinh độ chính xác, và do đó hữu ích cho các tác vụ phiên âm offline. Trong tương lai, chúng tôi sẽ tối ưu hóa thêm kiến trúc mô hình và mở rộng việc sử dụng cho âm thanh dài hơn nhiều.

Tài liệu tham khảo

[1] Alex Graves, Santiago Fernández, Faustino Gomez, và Jürgen Schmidhuber, "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks," trong ICML, 2006.

[2] Alex Graves, "Sequence transduction with recurrent neural networks," trong ICML Workshop on Representation Learning, 2012.

[3] Yu Zhang, Daniel S Park, Wei Han, James Qin, Anmol Gulati, Joel Shor, Aren Jansen, Yuanzhong Xu, Yanping Huang, Shibo Wang, và cộng sự, "Bigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition," IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1519–1532, 2022.

[4] Bo Li, Ruoming Pang, Yu Zhang, Tara N. Sainath, Trevor Strohman, Parisa Haghani, Yun Zhu, Brian Farris, Neeraj Gaur, và Manasa Prasad, "Massively multilingual asr: A lifelong learning solution," trong ICASSP, 2022.

[5] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, và Ilya Sutskever, "Robust speech recognition via large-scale weak supervision," trong ICML, 2023.

[6] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, và cộng sự, "Google USM: Scaling automatic speech recognition beyond 100 languages," arXiv preprint arXiv:2303.01037, 2023.

[7] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, và cộng sự, "Scaling speech technology to 1,000+ languages," arXiv preprint arXiv:2305.13516, 2023.

[8] OpenAI, "GPT-4 technical report," arXiv preprint arXiv:2303.08774, 2023.

[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample, "LLaMA: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

11

--- TRANG 10 ---
[10] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, và cộng sự, "PaLM 2 technical report," arXiv preprint arXiv:2305.10403, 2023.

[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin, "Attention is all you need," trong NeurIPS, 2017.

[12] Anmol Gulati, James Qin, Chung-Cheng Chiu, và cộng sự, "Conformer: Convolution-augmented transformer for speech recognition," trong Interspeech, 2020.

[13] Yanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez, Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang, Deepti Bhatia, Yuan Shangguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo-yiin Chang, Kanishka Rao, và Alexander Gruenstein, "Streaming end-to-end speech recognition for mobile devices," trong ICASSP, 2019.

[14] Shaojin Ding, Weiran Wang, Ding Zhao, Tara N. Sainath, Yanzhang He, Robert David, Rami Botros, Xin Wang, Rina Panigrahy, Qiao Liang, Dongseong Hwang, Ian McGraw, Rohit Prabhavalkar, và Trevor Strohman, "A unified cascaded encoder ASR model for dynamic model sizes," trong Interspeech, 2022.

[15] Zihang Dai, Guokun Lai, Yiming Yang, và Quoc Le, "Funnel-Transformer: Filtering out sequential redundancy for efficient language processing," trong NeurIPS, 2020.

[16] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger Schwenk, và Yoshua Bengio, "On using monolingual corpora in neural machine translation," arXiv:1503.03535, 2015.

[17] Jan Chorowski và Navdeep Jaitly, "Towards better decoding and language model integration in sequence to sequence models," trong Interspeech, 2017.

[18] Ehsan Variani, David Rybach, Cyril Allauzen, và Michael Riley, "Hybrid autoregressive transducer (HAT)," trong ICASSP, 2020.

[19] Jinyu Li và cộng sự, "Recent advances in end-to-end automatic speech recognition," APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1, 2022.

[20] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schlüter, và Shinji Watanabe, "End-to-end speech recognition: A survey," arXiv preprint arXiv:2303.03329, 2023.

[21] Mike Schuster và Kaisuke Nakajima, "Japanese and korean voice search," trong ICASSP, 2012.

[22] Yajie Miao, Mohammad Gowayyed, và Florian Metze, "EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding," trong ASRU, 2015.

[23] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, và Tomohiro Nakatani, "Improving Transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration," trong Interspeech, 2019.

[24] Rami Botros, Tara N. Sainath, Robert David, Emmanuel Guzman, Wei Li, và Yanzhang He, "Tied & reduced RNN-T decoder," trong Interspeech, 2021.

12

--- TRANG 11 ---
[25] Nelson Morgan và Hervé Bourlard, "Continuous speech recognition using multilayer perceptrons with hidden Markov models," trong ICASSP, 1990.

[26] Ehsan Variani, Erik McDermott, và Georg Heigold, "A Gaussian mixture model layer jointly optimized with discriminative features within a deep neural network architecture," trong ICASSP, 2015.

[27] Naoyuki Kanda, Xugang Lu, và Hisashi Kawai, "Minimum Bayes risk training of CTC acoustic models in maximum a posteriori based decoding framework," trong ICASSP, 2017.

[28] Erik McDermott, Hasim Sak, và Ehsan Variani, "A density ratio approach to language model fusion in end-to-end automatic speech recognition," trong ASRU, 2019.

[29] Vincent Vanhoucke, Matthieu Devin, và Georg Heigold, "Multiframe deep neural networks for acoustic modeling," trong ICASSP, 2013.

[30] Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong Zhang, và Yifan Gong, "Simplifying long short-term memory acoustic models for fast training and decoding," trong ICASSP, 2016.

[31] Salah Hihi và Yoshua Bengio, "Hierarchical recurrent neural networks for long-term dependencies," trong NeurIPS, 1996.

[32] Jan Koutnik, Klaus Greff, Faustino Gomez, và Juergen Schmidhuber, "A clockwork RNN," trong ICML, 2014.

[33] William Chan, Navdeep Jaitly, Quoc V. Le, và Oriol Vinyals, "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," trong ICASSP, 2016.

[34] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, và Tsubasa Ochiai, "ESPnet: End-to-end speech processing toolkit," trong Interspeech, 2018.

[35] Yongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhang, Wei Han, và Parisa Haghani, "Accelerating RNN-T training and inference using CTC guidance," trong ICASSP, 2023.

[36] Dongseong Hwang, Khe Chai Sim, Zhouyuan Huo, và Trevor Strohman, "Pseudo label is better than human label," trong Interspeech, 2022.

[37] W. Ronny Huang, Cal Peyser, Tara N. Sainath, Ruoming Pang, Trevor Strohman, và Shankar Kumar, "Sentence-select: Large-scale language model data selection for rare-word speech recognition," trong Interspeech, 2022.

[38] Noam Shazeer và Mitchell Stern, "Adafactor: Adaptive learning rates with sublinear memory cost," arXiv preprint arXiv:1804.04235, 2018.

[39] Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu, và Anjuli Kannan, "Minimum word error rate training for attention-based sequence-to-sequence models," trong ICASSP, 2018.

13

--- TRANG 12 ---
[40] Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, và Andrew Y. Ng, "First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs," arXiv preprint arXiv:1408.2873, 2014.

[41] Hasim Sak, Andrew W. Senior, Kanishka Rao, Ozan Irsoy, Alex Graves, Françoise Beaufays, và Johan Schalkwyk, "Learning acoustic frame labeling for speech recognition with recurrent neural networks," trong ICASSP, 2015.

[42] Qiujia Li, Chao Zhang, và Philip C. Woodland, "Integrating source-channel and attention-based sequence-to-sequence models for speech recognition," trong ASRU, 2019.

[43] Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng Chiu, David Rybach, Tara Sainath, và Trevor Strohman, "Recognizing long-form speech using streaming end-to-end models," trong ASRU, 2019.

14
