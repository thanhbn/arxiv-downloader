# 2311.15075.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.15075.pdf
# File size: 7421181 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 1
Mug-STAN: Adapting Image-Language Pretrained
Models for General Video Understanding
Ruyang Liu , Jingjia Huang , Wei Gao , Thomas H. Li , Ge Li
Abstract —Large-scale image-language pretrained models, e.g.,
CLIP, have demonstrated remarkable proficiency in acquiring
general multi-modal knowledge through web-scale image-text
data. Despite the impressive performance of image-language
models on various image tasks, how to effectively expand them
on general video understanding remains an area of ongoing
exploration. In this paper, we investigate the image-to-video
transferring from the perspective of the model and the data,
unveiling two key obstacles impeding the adaptation of image-
language models: non-generalizable temporal modeling and par-
tially misaligned video-text data. To address these challenges,
we propose Spatial- Temporal Auxiliary Network with Mutual-
guided alignment module (Mug-STAN) – a simple yet effec-
tive framework extending image-text model to diverse video
tasks and video-text data. Specifically, STAN adopts a branch
structure with decomposed spatial-temporal modules to en-
able generalizable temporal modeling, while Mug suppresses
misalignment by introducing token-wise feature aggregation of
either modality from the other. Extensive experimental results
verify Mug-STAN significantly improves adaptation of language-
image pretrained models such as CLIP and CoCa at both
video-text post-pretraining and finetuning stages. With our
solution, state-of-the-art zero-shot and finetuning results on
various downstream datasets, including MSR-VTT, DiDeMo,
LSMDC, Kinetics-400, Something-Something-2, HMDB-51, UCF-
101, and A V A, are achieved. Moreover, by integrating pretrained
Mug-STAN with the emerging multimodal dialogue model, we
can realize zero-shot video chatting. Codes are available at
https://github.com/farewellthree/STAN
Index Terms —Image-language models, temporal modeling,
partial misalignment, Mug-STAN, general video understanding
I. I NTRODUCTION
IN the past three years, the computer vision community has
witnessed the remarkable success of web-scale pretrained
image-language models, such as CLIP [1], CoCa [2], and
BEiTv3 [3]. However, the development of fundamental video-
language models is challenging, due to the high cost of
computation resources needed for pretraining and the limited
availability of data in terms of scale, quality, and diversity.
Rather than focusing on developing video-language pretrained
models [4], [5], an alternative and promising approach is to
transfer the abundant knowledge in image-language pretrained
models to the video domain, which has garnered increasing
attention in recent years [6]–[11].
The extension of pretrained 2D image models to the realm
of videos has been extensively explored within the field of
Ruyang Liu, Wei Gao, Thomas H. Li, and Ge Li are with the School of
Electronic and Computer Engineering, Peking University Shenzhen Gradu-
ate School. Ruyang Liu is also with the Peng Cheng Laboratory. E-mail:
{ruyang@stu, gaowei262@, geli@ece., thomas@ }pku.edu.cn
Jingjia Huang is with ByteDance Inc. E-mail: huangjingjia@bytedance.com
Corresponding author: Gao Wei.video learning [12], [13]. The central challenge lies in the
modality disparity between images and videos. Specifically,
videos inherently contain unique temporal information, and
video-text data is generally more complex and noisy when
compared to image-text data. Consequently, our investigation,
built upon existing temporal modeling methods and various
video-language datasets, has revealed two often overlooked
points. As depicted in Fig. 1, we have found that current
efforts in temporal modeling are predominantly confined to
either video-language tasks [6], [7], [14], [15] or video-
specific tasks [9]–[11], resulting in reduced efficiency when
applied to a different category of video task. Meanwhile, our
observation indicates that video-text paired training samples
typically suffer from partial misalignment in both pretraining
and downstream datasets.
To gain a deep insight into the first issue, we further dive
into the structures of existing CLIP-based temporal modules.
We find current efforts can be roughly categorized into poste-
rior structure based methods and intermediate structure based
methods as shown in Fig. 2. Posterior structure based methods
[7], [14]–[17] adopt a late modeling strategy, utilizing CLIP
as a feature extractor and applying temporal modeling to
embeddings independently extracted from different frames.
Built upon the highly semantic embeddings, this structure,
while beneficial for preserving well-aligned visual-language
representations, falls short in capturing the low-level spatial-
temporal visual patterns among frames, which are essential for
video understanding. As a result, methods based on posterior
structures tend to exhibit marginal performance improvements,
a trend that becomes particularly pronounced in action recog-
nition tasks where low-level spatial-temporal visual patterns
are crucial. Unlike posterior structure based methods, inter-
mediate structure based methods [10], [11], [13] equip CLIP
with temporal modeling capability by integrating temporal
modeling modules between CLIP layers, which sees significant
improvements in the video recognition task. Nevertheless, we
have observed that incorporating additional modules inside
CLIP would impact the pretrained high-level semantic knowl-
edge in the model, leading to trivial or even negative impacts
on the text-video retrieval task. These statistical patterns are
more pronounced in Fig. 3, where both the posterior structure
and intermediate structure excel only in their respective tasks.
In contrast to the extensive research on temporal mod-
eling, another critical issue has received limited attention:
video-text paired training samples generally exhibit partial
misalignment. Partial misalignment refers to the situation
in which the aligned information between a video and its
corresponding text is distributed only across specific framesarXiv:2311.15075v1  [cs.CV]  25 Nov 2023

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 2
1.4 2.0 -0.5 0.2 3.8 
0.5 0.5 2.0 3.7 4.2 
-1012345Improvement on CLIP Retrieval (R@1)Recognition (Top1 Acc)
CLIP4clip-seqTransCLIP2video-TDBSTadapterXCLIPSTAN(Ours)(a)Query: a trailer for an upcoming movie with people on a beach
(b)38%23%39%upmiddlebottom42%23%35%19%39%42%MSRVTTDiDeMoWebvid
Fig. 1. The two issues in image-to-video transfer for vision-language models. (a) Generalizability: We illustrate CLIP-based temporal modules struggle to
generalize across different video tasks. We present the performance of various models concerning the baseline, which is based on CLIP with mean pooling.
The models include the text-video retrieval models CLIP4clip-seqTrans [7] and CLIP2video-TDB [14], as well as video recognition models STadapter [11]
and XCLIP [10]. Evaluation is based on Recall@1 for MSRVTT [18] and Top-1 accuracy for Kinetics-400 [19]. (b) Partial Misalignment: Above, we
showcase a misaligned training sample in MSRVTT, where only “people on a beach” and the 1st,5thand 6thframes are aligned to each other. Below,
we quantitatively assess the extent of partial misalignment in video-text datasets, including MSRVTT, DiDeMo [20], and WebVid2.5M [21]. The degree of
alignment progressively deteriorates from “up” to “bottom”.
videooutput⊕
videooutput
videooutput
Temporal modeling layerData flowCLIP layer
Fig. 2. Different structures of temporal modeling: posterior structure (left),
intermediate structure (middle), and our branch structure (right).
and phrases, while other components of the video/text are
noisy which hinders precise vision-language alignment and
strong image-to-video adaptation. Fig. 1(b) shows a case of
partial misalignment, where only the phrase “people on a
beach” and the red-marked frames are semantically aligned.
Due to the complexity and redundancy of video content, such
cases occur much more frequently in video-text than in image-
text data. Moreover, the situation is even more severe in video
pretraining datasets, which are constructed using instructional
videos and noisy narrations [4], [22], [23]. To quantitatively
assess the partial misalignment present in video datasets, we
have selected and analyzed two downstream datasets (MSR-
VTT [18] and DiDeMo [20]) and one pretraining dataset
(WebVid2.5m [21]). Specifically, we employ CLIP-ViT-L/14
[1] to measure misalignment, utilizing dot-product similarity
followed by sigmoid to compute the correlation between text
and each frame. A frame is considered aligned with the text if
the probability exceeds 0.5. Then, we categorize the video-text
alignment degree into three levels: (1)up when more than 2/3
frames are aligned with the text. (2)bottom when less than 1/3frames are aligned with the text. (3)middle in between the two.
As revealed in Fig. 1(b), in all three datasets, more than half of
the video-text pairs suffer from partial misalignment (middle
and bottom), even if these datasets are widely recognized for
their high quality in video-text tasks.
Partial misalignment, together with the temporal modeling,
has raised a subsequent challenge: post-pretraining1image-
language models on large-scale video-language datasets shows
very limited gains. As depicted in Fig. 3(b), we can observe
that CLIP, after being post-pretrained on either WebVid10M
or HowTo100M, does not significantly outperform the baseline
without post-pretraining.
From the aforementioned analysis, we conclude two key
factors for extending image-language pretrained models to
the video domain: (1) Effective temporal modeling while
taking advantage of knowledge in different levels of rep-
resentation. (2) Suppressing the partial misalignment during
training on video-text data. To this end, we propose Spatial-
Temporal Auxiliary Network with Mutual-guided alignment
module (Mug-STAN) - a plug-and-use framework adapting
image-language models to general video tasks, where STAN
introduces effective temporal modeling and Mug mitigates
partial misalignment during training. In Fig. 2 and 3(a), it is
noticeable that temporal modeling structure in STAN exhibits
strong performance in both retrieval tasks and recognition
tasks. In Fig. 3(b), we can see that STAN and Mug con-
tribute significantly to the effectiveness of post-pretraining
respectively, where Mug excels particularly well on the noisy
HowTo100M dataset.
Specifically, rather than posterior or intermediate structure,
our proposed STAN introduces a distinctive branch structure
located outside the visual backbone , featuring multiple levels
1Further pretraining on relatively large scale video-text corpora based on
pretrained image models for downstream video tasks is termed as post-
pretraining. Finetuning means directly tuning for adapting image-text models
on downstream video datasets.

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 3
434547495153
CLIPSTANMug-STANR@1 on DiDeMono post-pretrainingWebVid10M post-pretrainingHowTo100M post-pretraining
(b)-10123456
01234R@1 on RetrievalTop 1 Acc on RecognitionCLIP4clip-seqTransCLIP2video-TDBDRL-WtiTS2NetHunYuanSeparate-STStadapterXCLIPOurs
(a)Posterior StructureIntermediate Structure
Fig. 3. (a) Performance comparison of various methods on both text-to-
video retrieval and video recognition. Evaluation is based on Recall@1
for MSRVTT [18] and Top-1 accuracy for Kinetics-400 [19]. The methods
are clustered into posterior structure, intermediate structure, and our branch
structure. (b) Performance comparison of post-pretraining on different models.
We report the finetuned result of Recall@1 on DiDemo text-video retrieval.
Based on CLIP , effective temporal modeling (STAN) and partial-misalignment
suppression (Mug) respectively bring noticeable improvements.
of input, as shown in Fig. 2. This novel structure enables STAN
to enrich the features of video frames with spatial-temporal
contexts, leveraging different output levels of image-text
model, while preserving the forward-propagation of source
model. Thereby, it can effectively utilizes both high-level and
low-level knowledge from the pretrained model simultane-
ously, making it adaptable to various downstream video tasks.
STAN comprises multiple layers with a spatial-temporal sep-
arated design. Each layer conducts spatial-temporal modeling
by alternately stacking two distinct modules: an intra-frame
module and a cross-frame module. This approach allows the
layer to enhance model performance by reusing pretrained
parameters from image-text pretrained models to initialize the
intra-frame spatial modules. Meanwhile, Mug is constructed
using a parameter-free, token-wise interaction modeling mech-
anism with negligible computational cost, which can be easily
plugged into existing state-of-the-arts. Given a video-text
pair, we can get its frame-wise feature sequence and text
feature sequence, respectively. To realize the mutual-guided
alignment, we first perform the frame-token interaction to
obtain the frame-specific text embedding for each frame and
token-specific video embedding for each token. Then, for
each modality, we attain its final global embedding through
guidance from the other modality. At last, the pair of mutually
guided representations are employed in contrastive learning
during post-pretraining or finetuning. In this way, we can
capture and align the relevant parts of video and text, freeing
the adaptation of image-text pretrained models from the video-
text partial misalignment problem.
Through extensive experiments, we have demonstrated the
impressive performance of our proposed Mug-STAN. Specif-
ically, we have implemented Mug-STAN on two well-known
image-language models, CLIP and CoCa. Furthermore, we
have adopted a fresh perspective on post-training by evaluating
our model on datasets with varying levels of noise, such
as WebVid10M and HowTo100M. The comprehensive results
highlight the efficacy of Mug-STAN not only in the finetuning
but also in post-pretraining. Remarkably, we achieve state-of-
the-art results in both zero-shot and finetuning settings across
a diverse range of video tasks, including text-video retrieval,
video action recognition, and video detection. Moreover, giventhe current popularity of multimodal dialogue systems, we
have also plugged the pretrained Mug-STAN on LLaVa [24],
achieving the capability of zero-shot video chatting without
any instruction tuning.
The main contributions of this paper are:
•We present an in-depth analysis of the factors that im-
pede the adaptation of image-language models to video
domains. By revisiting the temporal modeling on CLIP
in current research and carefully examining video-text
datasets, we identify non-generalizable temporal mod-
eling and partially misaligned video-text data as the
primary culprits affecting the performance.
•We propose Spatial- Temporal Auxiliary Network with
Mutual-guided alignment module (Mug-STAN) - a sim-
ple but strong framework that extends image-text pre-
trained models to general video tasks. In Mug-STAN,
we leverage the novel branch structure of STAN for
effective temporal modeling, enabling temporal learn-
ing that incorporates spatial-temporal contexts at various
levels. Additionally, Mug plays a crucial role in noise
suppression and encourages the contribution of well-
aligned parts to achieve robust video-language alignment.
•We conduct comprehensive experiments under various
settings to evaluate the effectiveness of Mug-STAN. The
numerous results demonstrate that Mug-STAN achieves
state-of-the-art zero-shot and finetuning results on a wide
range of video datasets and tasks, as well as the capability
of zero-shot video dialogue.
II. R ELATED WORK
A. Image-Language PreTraining
Image-Language pre-training has been drawing increasing
attention from researchers in the computer vision community
[23], [25]–[27]. Recently, contrastive language-image pretrain-
ing on web-scale data [1]–[3], [28], [29] has experienced sig-
nificant success, primarily due to its outstanding performance
when applied to various downstream tasks. One of the most
renowned works is CLIP [1], which has demonstrated surpris-
ing capabilities in zero-shot recognition and domain general-
ization [30], [31]. The wealth of knowledge contained within
these image-language pretrained models holds a promising
future for their adaptation to video tasks. Thankfully, our Mug-
STAN can be implemented on these image-language models
in a plug-and-play manner, leading to substantial performance
improvements in various video tasks. It’s worth noting that
recent advancements in multimodal understanding have been
largely propelled by the fusion of image-based vision models
with LLMs, such as Flamingo [32], BLIP-2 [33], and LLaV A
[24], fortunately, these multimodal dialogue models generally
employ CLIP-L/14 as the visual encoder. Consequently, our
Mug-STAN can be seamlessly implemented on these models
to achieve zero-shot video chatting.
B. Video-Language Pretraining
As a subset of vision-language pretraining, video-language
pretraining has also been the subject of numerous explorations

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 4
in recent years, such as Violet [34], clipBert [35], Frozen
[21], BridgeFormer [36], and Clover [5]. In video-language
pretraining, models typically initialize the video encoder and
text encoder with separately pre-trained weights [13], [37]–
[39], and then use multiple pretraining targets to achieve cross-
modal alignment and multimodal learning, such as contrastive
learning, masked language modeling, and video-text matching.
However, video-language pretrained models face difficulties
in simultaneously handling temporal modeling and modality
alignment due to the challenges posed by unaligned initial-
ization. In contrast, image-text pretrained models inherently
possess extensive knowledge as a result of the vast diversity
and scale of image-text data they are trained on. As a result,
when finetuned on downstream video-language datasets, we
have observed significant advantages of image-text pretrained
models over video-language pretrained models, even if the
former have not been pretrained on video datasets.
Similar to our research, CLIP-ViP [6] is among the few
studies that delve into the realm of video post-pretraining.
However, CLIP-ViP relies on large-scale data and the an-
notation from an additional captioner for its post-pretraining
process. In contrast, our work demonstrates that with an
appropriate method, post-pretraining can yield superior results
on both smaller datasets (Webvid10M) and noisy datasets
(HowTo100M) without requiring extra frame-wise annotation.
In addition, several studies have also ventured into the domain
of pretraining under noisy and misaligned video-text data
[40]–[42]. Miech et al. [40] and Han et al. [42] introduced
the MIL-NCE loss and the Temporal Alignment Network,
respectively, for noisy video-narration pretraining. Compared
to these works, our paper differs in three aspects: (1)Setting.
The previous works primarily focus on the datasets filled with
completely misaligned video-text pairs and ASR captions ( e.g.,
Howto100M ), while our focus lies on the issue of partial
misalignment, which is a more general problem and can even
occur in relatively high-quality datasets, as depicted in Fig.
2(b). (2) Method. [42] employs the black-box network to learn
the similarity between video and text, while we propose a
parameter-free video-text mutual-guided module to identify
and filter out the unrelated parts from video and text. (3)
Results. In experiments, we convey much better results than
those works under the same setting.
C. Image-Language Pretrained Models For Video Tasks
In contrast to further post-pretraining, the majority of cur-
rent studies primarily concentrate on the direct fine-tuning of
image-text models for video tasks. An intuitive direction is
temporal modeling [6], [7], [9]–[11], [14], [15], [24], [43],
[44], as the image model cannot capture temporal information.
In video-language tasks, such as text-video retrieval, most
adaptation models tend to utilize posterior-based structures
to handle temporal aspects , e.g.,, the sequential transformer
in [7], the temporal difference block in [14], and token
selection module in [15]. Despite the advancements achieved
by these methods, the temporal modeling they provide is
restricted to high-level embeddings and lacks effectiveness,
as illustrated in Fig. 1(a). In video-only tasks such as actionrecognition, the mainstream expansion of CLIP for temporal
modeling is to utilize the intermediate structure. For instance,
Niet al [10] developed a message token mechanism to pass
messages among different frames. Pan et al [11] inserted
the 3D convolution adapter inside the transformer to activate
temporal modeling. Besides temporal modeling, there are also
other efforts focused on adapting image-language models for
video tasks from different perspectives. For example, [10], [45]
explored the prompt modeling, while [15], [16], [46], [47]
improved the ways of cross-modal interaction. However, most
of the aforementioned methods tend to perform worse when
transferred to another video task, whereas our model performs
well across various video tasks.
III. M ETHOD
In this section, we will elaborate on our proposed strong
and flexible Mug-STAN for adapting image-language models
to general video tasks.
A. Motivation
Large-scale image-language models, such as CLIP and
CoCa, which undergo pretraining on hundreds of millions to
billions of image-text pairs, typically comprise two encoders
as fundamental components. Each encoder is responsible for
encoding one modality to facilitate cross-modal alignment. As
we ascend through the layers of the visual transformer [48],
the model gradually learns visual patterns at different levels of
abstraction [49]. Eventually, the visual encoder produces high-
level visual embeddings that are semantically aligned with the
corresponding embeddings in the text modality. Formally, as
illustrated in Fig. 4(left), given a video clip with Tframes and
a text description with Ktokens, we feed them into a standard
image-text pretrained visual encoder and text encoder, treating
each frame as an individual image. This process generates
frame-wise video representations denoted as V, and token-
wise text representations denoted as C:
V={vi}T
i=1∈RT×D, C ={cj}K
j=1∈RK×D(1)
where Dis the feature dimension. Note that vican be
obtained from either the CLS token [1], [2] or the average
of all patch tokens [29] of each frame. Then, frame-wise
video representations {vi}T
i=1are averaged as the global video
embedding vand the CLS token embedding is chosen from C
as the global text representation c, where vandcare employed
for cross-modal alignment. However, in the above process, two
important issues are dismissed: temporal modeling and video-
text partial misalignment.
Firstly, each frame is encoded independently as it passes
through the visual encoder, which neglects the interactions be-
tween frames and hinders temporal understanding. To address
this problem, existing research often introduces additional
modules as either a posterior or intermediate structure for
the visual encoder to explicitly incorporate temporal modeling
for various downstream video tasks. For high-level semantic
knowledge dominated tasks, i.e.,video-language task, the pos-
terior structure fully leverages the pretrained visual-language
alignment knowledge by applying temporal modeling to the

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 5
⊕Multi-Head Self-AttentionLayer NormLayer NormFeed ForwardNetworkLayer NormSpatial Self-AttentionCross-Frame Module
initialize⊕CLIP layerIntra-Frame ModuleSTAN layera trailer for an upcoming movie ……TextVision
Mug 𝑐!!"#$𝑣%%"#&"𝑣̃𝑐
STAN𝑣!!"#$
Fig. 4. (left) The overall architecture of our proposed method, including the text and visual encoders, the temporal modeling module (STAN), and the
cross-modal interaction module (Mug). (middle) Schematic diagram of feature forward propagation in and between pretrained visual encoder and STAN.
(right) Details of the internal structure of the STAN spatial-temporal module.
visual encoder output {vi}T
i=1. Nevertheless, the highly seman-
tic nature of viT
i=1makes it challenging to capture low-level
spatial-temporal patterns, leading to less effective temporal
modeling. As for visual pattern dominated tasks, i.e.,video-
only task, the intermediate structure integrated within the
visual encoder fully leverages the pretrained low-level visual
patterns. This empowers the encoder with the capability of
learning spatial-temporal patterns from the video. However,
the plug-in modules disrupt the original model’s structure and
internal feature flow, resulting in the inability to inherit the
high-level semantic information alignment capability from the
pretrained models.
Secondly, the simple strategy in cross-modal interaction
overlooks the prevalent issue of partial misalignment within
video-text pairs. This misalignment results in aligned infor-
mation being distributed selectively across specific frames and
phrases, while other contextual elements may lack relevance
to each other. The irrelevant parts are a kind of noise to
video-language alignment. Therefore, simply representing the
video and text with averaged representation or CLS embedding
would introduce the noise hindering the learning of cross-
modal alignment.
In response to the issue of existing models not being able
to simultaneously inherit the pretrained high-level and low-
level knowledge, we introduce Spatial-Temporal Auxiliary
Network (STAN), a novel temporal modeling mechanism
for image-language pretrained models. As shown in Fig.
4(middle), STAN functions as a branch structure alongside
the pretrained visual encoder. With the sophisticated design,
STAN leverages various levels of features while retaining
the pretrained knowledge. The operation of STAN will be
detailed in Sec. III-B. Additionally, as depicted in Figure 5,
to address the problem of partial misalignment, we introduce
a novel cross-modal interaction module called Mutual-guided
cross-modal alignment (Mug). This module takes frame-wise
video representations Vand token-wise text representations
Cas inputs. With guidance from the other modality, Mug
efficiently filters out unrelated content and preserves aligned
information in each modality, yielding new global video andtext representation evandec. Details about Mug will be provided
in Section III-C.
B. Spatial-Temporal Auxiliary Network
Again, in the case of a video with Tframes, the frames
are fed into the pretrained visual backbone, which generates
intermediate outputs at the last K+ 1levels of visual layers.
We denote the outputs of the k thselected visual layer as:
Vk={fk
i,l∈ RD|i∈[1, T], l∈[0, L]}, (2)
which is a visual embedding sequence of the video where T,L
andDrepresents the frame number, per-frame patch number
and embedding dimension, respectively. In Vk,fk
i,0refers to
the embedding of the [CLS] token in the i-th frame of the
video, while fk
i,l>0represents the visual embedding of the l-
th patch within that frame. Then, we take each intermediate
output Vkand pass it through the corresponding level of
layer in STAN to model the spatial-temporal correspondence
between video frames. At last, frame-wise outputs of the last
pretrained visual layer are fuesed with the output of STAN
to obtain the frame-wise video representation contextualized
with temporal information, denoted as {vi}T
i=1in Eq. 1.
STAN is composed of a stack of Kspatial-temporal layers,
with the input for each layer constructed upon the output of a
pretrained vision layer and the last STAN layer. For the k th
layer in STAN, its input is an embedding sequence of the
whole video denoted as:
V′k={f′k
0,0, f′k
1,1, .., f′k
1,L, .., f′k
T,1, .., f′k
T,L}, (3)
where f′k
0,0is the embedding representing the whole video
while others denote the embedding of image patches in
different frames. The output of the STAN layer is also an
embedding sequence maintaining the same size as its input,
which is denoted as:
ˆVk={ˆfk
0,0,ˆfk
1,1, ..,ˆfk
1,L, ..,ˆfk
T,1, ..,ˆfk
T,L}. (4)
At the first STAN layer, to construct its input from output
of any pretrained visual layer Vm, we first average the

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 6
…
frame-specific t
softmaxsoftmaxtoken-specific vtrailormoviebeach...frame1frame2frame3…softmax
softmaxtext-guided video
video-guided textToken-Frame Interaction
…element-wise multiplication matrix multiplication 
similarity matrixframe-wise video featurestoken-wise text featuresalignment
𝑐!!"#$𝑣!!"#$VisionSTAÑ𝑐#𝑣
……Text……
Fig. 5. The overview of our proposed Mug. Based on the outputs of the video and text encoder, we first implement the mutual token-frame interaction on
frame-wise video features and token-wise text features. Then, we compute the global video embedding and text embedding through guidance from another
modality. Finally, we align the text-guided video embedding and video-guided text embedding.
embedding of [CLS] tokens in each frame as a new embedding
f′1
0,0=1
TP
i∈Tfm
i,0, and then update patch embeddings in Vk
with both spatial and temporal position embeddings as:
f′1
i,l= Dropout( fm
i,l+ Pos t(t) + Pos s(l)), (5)
where l >0andPostandPossare the learnable embeddings
for the temporal and spatial positions of each patch. For the
other layers in STAN, the input V′kis built based on the
output from the previous STAN layer eVk−1and pretrained
visual layer output Vm+k−1as follows:
f′k
0,0=efk−1
0,0+ Wk
proj1
TX
i∈Tfm+k−1
i,0 , (6)
f′k
i,l=efk−1
i,l+ Wk
projfm+k−1
i,l, (7)
where i∈[1, T], l∈[1, L], and Wk
proj∈RD×Dis a
projection layer. When compared to posterior structure based
methods, STAN conducts spatial-temporal modeling on multi-
level pretrained visual representations, enabling it to effec-
tively capture visual dynamics information in the video. Mean-
while, unlike previous intermediate structure based methods
that insert modules into pretrained visual encoder, STAN’s
branch structure protects the pretrained knowledge without
disrupting the inherent encoder structure.
Given the input embedding sequence of a video, the STAN
layer learns spatiotemporal information between video frames.
As depicted in Fig. 4(right), it performs temporal modeling
through the alternating stacking of two independent modules
– the intra-frame module and the inter-frame module. Thanks
to this separated design, we can reuse the structure of the
pretrained visual encoder layer as our intra-frame spatial
module and initialize it with the pre-trained parameter. This
approach significantly reduces the optimization search space
and improves the performance of downstream tasks. Same
as most image-text pretrained models like CLIP, the intra-
frame module is also a self-attention block designed for spatial
modeling. To simplify notation, we omit the superscript of
embedding and denote the embedding representation of the
i-th frame as Xi∈R(L+1)×D. Here, the embedding of the
[CLS] token in the video is duplicated and concatenated withthe patch embeddings. Within each frame, the spatial module
updates the embeddings using self-attention:
ˆXi= softmax( XiWQ(XiWK)T/√
D)(XiWV) +Xi,(8)
where WQ/WK/WVdenote the linear projections for the
query, key and value in self-attention layer of the spatial
module. Afterward, the duplicated [CLS] embeddings in each
frame are averaged to form the video [CLS] embedding.
The cross-frame module is dedicated to temporal modeling.
To simplify notation, we omit the superscript of the embedding
and represent the collection of l-th patch embeddings in
different frames as Yl∈RT×D. At each spatial position, the
patch embeddings are updated using the function Temp (),
which denotes the message passing strategy across temporal
dimensions. In experiments, we will show that this strategy can
be instantiated in various ways to facilitate temporal informa-
tion exchange among frames. Here, we detail the instantiation
of temporal self-attention, which possesses a natural advantage
in sequence modeling. At each specific spatial position, the
patch embeddings from different frames can be updated as:
ˆYl= W proj(softmax( YlWQ(YlWK)T/√
D)(YlWV) +Yl),
(9)
where WQ/WK/WVdenote the linear projections for the
query, key, and value in the self-attention layer of the cross-
frame module, and Wproj is the extra temporal linear projec-
tion initialized as zero. By employing temporal attention, each
patch in the video is contextualized with temporal information
from the same locations, while the zero projection helps
maintain training stability during the early stages.
At the final stage, with the output of the last pretrained
visual layer V−1and the output of the last STAN layer ˆVK,
we can simply combine them through addition to form the
ultimate output of the video encoder:
V= W vproj(LN(V−1⊕ˆVK)), (10)
where LNis the final layer normalization in pretrained visual
encoder and Wvproj is the linear weight projecting the visual
embedding into joint visual-text feature space. Furthermore,
⊕means the global [CLS] token of STAN is duplicated T

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 7
times and added to the [CLS] of each frame in V−1, while the
patch tokens are combined through simple addition. Finally,
the same as the image encoder, we only have L+ 1 tokens
for the video encoding. This property significantly reduces the
computational burden if we need to further feed these tokens
into multimodal encoders or LLMs, in comparison to the joint
space-time video encoder [5], [6], [13].
C. Mutual-Guided Cross-Modal Alignment
In the previous section, we have acquired the token-wise
text embeddings Cand frame-wise video embeddings V.
In this section, we will further delve into how to filter out
misaligned information using Mug, as depicted in Fig. 5.
Mug first establishes token-frame-wise correspondences by
calculating the dot-product similarity between CandV. With
the similarity matrix, we then introduce how to provide mutual
guidance for feature aggregation from the perspective of each
modality, respectively.
From the perspective of video modality, we first filter out
the most relevant information in the text for each video frame.
This is achieved by calculating the frame-to-token attention
distribution , which assigns a score to each text token based
on its relevance to the current video frame. Specifically, the
attention score of the ithvideo frame with respect to the jth
text token is given by:
si,j=exp(τcj·vi)PK
j=1exp(τcj·vi), (11)
wherePK
j=1si,j= 1,·represents dot-product operation and
τcontrols the sharpness of attention distribution. For example,
in Fig. 2(a), the 1stvideo frame is expected to have dominant
attention on the tokens corresponding to the action of “ people
on a beach ” across all the text tokens.
Then, we aggregate the text embeddings based on the at-
tention distribution and get the frame-specific text embedding
for each frame:
ci=KX
j=1si,jcj,where ci∈RD. (12)
The set of frame-specific text embedding {ci}T
i=1represents
updated text embeddings specified for each frame, where
irrelevant information in the original text that is not aligned
with the frame is suppressed and information that is relevant
to the frame is strengthened. We use these updated text
embeddings to evaluate the correspondence of each frame with
respect to the text. This evaluation is done using dot-product
similarity scores as the metric:
esi=exp(τci·vi)PT
n=1exp(τcn·vn), (13)
whereesirepresents the attention weight of each frame towards
the text. Through esi, we can further aggregate frame-wise
embedding to the global video-level representation with the
guidance of text. Formally, we define this global text-guided
video embedding as:
ev=TX
i=1esivi,whereev∈RD. (14)Analogously, from the perspective of text modality, we
follow the same procedure to get improved text embedding
under the guidance of video. Specifically, we first calculate
token-to-frame attention distribution , which assigns a score to
each frame embedding based on its relevance to the current
text token:
s′
i,j=exp(τcj·vi)PT
i=1exp(τcj·vi), (15)
where iandjindicate the index of text token and frame. Then,
we get token-specific video embedding for each text token to
assess the token-to-video correspondence:
vj=TX
i=1s′
i,jvi,where vi∈RD(16)
es′
j=exp(τcj·vj)PK
n=1exp(τcn·vn), (17)
wherees′
jrepresents the attention weight of each text token
towards the video. We obtain the global video-guided text em-
beddingecby aggregating the text token embeddings according
to{es′
j}K
j=1:
ec=KX
j=1es′
jcj,whereec∈RD, (18)
In our proposed Mug, we default to using the frame-to-token
interaction. However, our method can be readily adapted to
various granularities of video-text interaction, such as video-
to-token, frame-to-text, and token-to-token interaction. This
flexibility allows for a trade-off between computation and
interaction granularity, catering to different requirements based
on the specific application. We will further explore and discuss
this in our experiments.
D. Training
Post-pretraining & text-video retrieval. Both post-
pretraining and retrieval tasks utilize video-text pairs as
training sources, resulting in the same training pipeline.
Specifically, given text-guided video embedding evand
video-guided text embedding ec, we calculate the dot-product
similarity between the two embeddings, which serves as the
similarity metric for the video and text in contrastive learning
in a B-batch by:
Lt2v=−1
BBX
m=1logexp(τgcmn·gvnm)PB
n=1exp(τgcmn·gvnm),
Lv2t=−1
BBX
n=1logexp(τgvnm·gcmn)PB
m=1exp(τgvnm·gcmn),
Lco=Lt2v+Lv2t,(19)
wheregcmnandgvnmdenotes the mutual-guided text/video
embedding of the mthtext and nthvideo in the batch, and
Lcodenotes the final contrastive loss. It is worth noting that
the video and text embeddings have been normalized before
computing Mug, thereby the normalization is not included in
calculating the similarity.

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 8
TABLE I
THE ZERO -SHOT RESULTS OF TEXT -TO-VIDEO RETRIEVAL AND VIDEO RECOGNITION ON SIX DOWNSTREAM DATASETS . MODELS EXHIBITING OBVIOUS
UNFAIR COMPARISON ARE DE -EMPHASIZED ,i.e., INVOLVING EXTRA MODALITY ,MUCH LARGER MODELS ,OR SELF -SUPERVISED PRETRAINING .
MethodMSR-VTT DiDeMo LSMDC HMDB-51 UCF-101 Kinetics400
R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR Acc@1 Acc@1 Acc@1
Non-CLIP models
VideoCLIP [53] 10.4 22.2 30.0 - 16.6 46.9 - - - - - - - - -
Frozen [21] 18.7 39.5 51.6 10.0 21.1 46.0 56.2 7.0 9.3 22.0 30.1 51.0 27.5 45.4 -
ALPRO [54] 24.1 44.7 55.4 - 23.8 47.3 57.9 - - - - - - - -
VIOLET [34] 25.9 49.5 59.7 - 23.5 49.8 59.8 - - - - - - - -
BridgeFormer [36] 26.0 46.4 56.4 7.0 25.6 50.6 61.1 5.0 12.2 25.9 32.2 42.0 38.0 51.1 -
Clover [5] 26.4 49.5 60.0 6.0 29.5 55.2 66.3 4.0 17.4 29.2 38.2 24.0 - - -
OmniVL [55] 34.6 58.4 66.6 - 33.4 58.7 68.5 - - - - - - - -
CLIP-B/32
CLIP [1] 30.6 54.4 64.3 4.0 24.7 49.3 60.9 6.0 13.6 27.9 35.5 32.0 - - 42.1
CLIP-straight [56] 31.2 53.7 64.2 4.0 - - - - 11.3 22.7 29.2 56.5 - - -
CLIP4Clip [7] 32.0 57.0 66.9 4.0 - - - - 15.1 28.5 36.4 28.0 - - -
BridgeFormer [36] 33.2 58.0 68.6 4.0 - - - - 15.5 30.7 38.7 22.0 - - -
CLIP-ViP [6] 29.0 51.2 61.3 5.0 22.6 43.9 56.4 7.0 11.3 25.3 31.3 38.0 - - -
Mug-STAN-B/32 35.9 60.8 69.6 3.0 33.7 60.5 70.3 3.0 17.4 32.7 40.4 21.5 - - 48.1
CLIP-B/16
CLIP [1] 31.8 53.9 64.5 4.0 27.7 51.0 62.5 5.0 15.2 29.7 37.6 25.0 43.2 68.9 48.0
ActionCLIP [57] - - - - - - - - - - - - 40.8 58.3 -
CLIP-ViP [6] 31.7 53.8 63.2 4.0 24.6 50.7 59.7 5.0 12.5 26.1 33.3 39.0 41.2 48.9 37.6
X-CLIP [10] 31.7 53.8 63.2 4.0 24.6 50.7 59.7 5.0 12.5 26.1 33.3 39.0 44.6 72.0 -
Mug-STAN-B/16 38.7 64.0 74.0 2.0 36.2 62.3 71.1 3.0 18.0 33.3 41.4 19.0 50.9 70.3 55.7
CLIP-L/14
CLIP [1] 35.4 58.8 68.1 3.0 30.3 54.9 65.4 4 18.5 33.8 42.3 19.0 46.5 72.7 55.9
ImageBind∗[58] 36.8 61.8 70.0 - - - - - - - - - - - 50.0
InternVideo [59] 40.0 65.3 74.1 2.0 31.5 57.6 68.2 3.0 17.6 32.4 40.2 23.0 - - 64.2
Mug-STAN-L/14 41.7 65.7 75.8 2.0 39.6 64.3 72.6 2.0 20.7 38.8 46.2 14.0 52.1 76.9 65.0
Video action recognition. Different from video-language
tasks, action recognition tasks have fixed textual labels. Hence,
we freeze the text encoder and only train the video encoder
during finetuning. Besides, we do not employ any additional
prompt templates like “a video of action { }” [1], [10] to wrap
the tags. Then, we compute the loss with evandecas follows:
Lcr=NX
n=1ynlogexp(τev·ecn)PN
i=1exp(τev·eci), (20)
where Nis the class number, ynis the one-hot label for class
n,cnis the value of class nin global text embedding, and
Lcrdenotes the final cross-entropy loss.
Video action detection. Following the action detection
pipeline in Slowfast [50] and VideoMAE [51], we add
ROIAlign [52] with MaxPooling to generate the regions of
interest in the last layer, following a cross-entropy with sig-
moid loss for multi-label prediction.
IV. E XPERIMENTS
A. Datasets
We evaluate our Mug-STAN on both video-language tasks,
i.e.,, video-text retrieval, and video-only tasks, i.e.,, video
recognition and video detection, which trials our methods
from the two different perspectives. For video-text retrieval,
we use MSR-VTT [18], DiDemo [20] and LSMDC [60]; for
video recognition, we use Kinetics-400 [19] and Something-
Something-v2 [61]; for video detection, we adopt Atomic
Visual Action V2.2 [62]. Besides, we conduct the video-
text post-pretraining on datasets with different levels of noise,
including WebVid10M [21] and HowTo100M [4].Video-Language Datasets :MSR-VTT is the most widely
used benchmark for video-text retrieval. It consists of 10,000
YouTube videos, each associated with 20 captions. We report
our results on the 1K-A split [63], which contains 9000
videos for training and 1000 for testing. DiDemo includes
10,611 videos sourced from Flicker, accompanied by 40,000
sentences. Notably, this dataset features longer video durations
compared to other retrieval datasets. Following previous works
[7], [14], we concatenate all captions of a video into a single
query. LSMDC is a large-scale video-text retrieval benchmark
comprising 118,081 videos sourced from 202 movies. This
dataset offers a higher level of diversity in terms of concepts
and video durations compared to other datasets.
Video-only Datasets :Kinetics-400 (K-400) is the most pop-
ular video recognition benchmark. Comprising over 300,000
video clips, Kinetics-400 covers 400 human action classes
with average 300 frames. Something-Something-v2 (SSv2) is a
video action recognition benchmark specifically designed for
temporal modeling capabilities. It consists of 220,485 videos,
each associated with 174 action classes. In contrast, K-400 has
a bias towards action categories with static scene context, as
noted in [64]. However, in SSv2 , the action classes are less
influenced by static scene context and instead focus more on
dynamic information within the videos. Atomic Visual Action
(AVA) v2.2 is designed for spatial-temporal action detection. It
provides dense annotation for 80 atomic visual actions across
430 15-minute movie clips, resulting in 1.62M action labels
with multiple labels per human occurring frequently.
Video Pretraining Datasets :WebVid10M is a large-scale
video-text pretraining dataset of short videos with textual

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 9
TABLE II
THE FINETUNING RESULTS OF TEXT -TO-VIDEO RETRIEVAL ON MSRVTT, D IDEMO,AND LSMDC. M ODELS EXHIBITING OBVIOUS UNFAIR
COMPARISON ARE DE -EMPHASIZED . FORCLIP- BASED METHODS , *MEANS EXTRA TRICKS (e.g., DSL [67] AND QB-N ORM [68]) ARE UTILIZED
DURING INFERENCE ;AND †DENOTES POST -PRETRAINING THE MODELS ON VIDEO -TEXT DATASETS BEFORE FINETUNING .
MethodMSR-VTT DiDeMo LSMDC
R@1↑R@5↑R@10 ↑MdR↓R@1↑R@5↑R@10 ↑MdR↓R@1↑R@5↑R@10 ↑MdR↓
Non-CLIP models
CLIPBert [35] 22.0 46.8 59.9 6.0 20.4 48.0 60.8 6.0 - - - -
MMT [69] 26.6 57.1 69.6 4.0 - - - - 12.9 29.9 40.1 19.3
Frozen [21] 31.0 59.5 70.5 3.0 31.0 59.8 72.4 3.0 15.0 30.8 40.3 20.0
VIOLET [34] 34.5 63.0 73.7 - 32.6 62.8 74.7 - 16.1 36.6 41.2 -
HD-VILA [70] 35.6 65.3 78.0 3.0 28.8 57.4 69.1 4.0 17.4 34.1 44.1 15.0
All-in-one [71] 37.9 68.1 77.1 - 32.7 61.4 73.5 3.0 - - - -
BridgeFormer [36] 37.6 64.8 75.1 3.0 37.0 62.2 73.9 3.0 17.9 35.4 44.5 15.0
Clover [5] 40.5 68.8 79.4 2.0 50.1 76.7 85.6 1.0 24.8 44.0 54.5 8.0
CLIP-B/32
CLIP4Clip [7] 44.5 71.4 81.6 2.0 43.4 70.2 80.6 2.0 21.6 41.8 49.8 11.0
CenterCLIP [72] 44.2 71.6 82.1 2.0 - - - - 21.7 39.8 49.8 11.0
CAMoE* [67] 47.3 74.2 84.5 3.0 43.8 71.4 - - 25.9 46.1 53.7 -
CLIP2tv [43] 46.1 72.5 82.9 2.0 45.5 69.7 80.6 2.0 15.5 30.7 38.7 22.0
ts2net [15] 47.0 74.5 83.8 2.0 41.8 71.6 82.0 - 23.4 42.3 50.9 9.0
DRL [16] 47.4 74.6 83.8 2.0 47.9 73.8 82.7 2.0 24.9 45.7 55.3 7.0
CLIP-ViP† [6] 50.1 74.8 84.6 1.0 48.6 77.1 84.5 2.0 25.6 45.3 54.4 8.0
CLIP-ViP*† [6] 55.9 77.0 86.8 1.0 53.8 79.6 86.5 1.0 26.0 46.4 54.9 8.0
Mug-STAN-B/32 48.9 74.5 84.1 2.0 49.6 75.3 84.6 2.0 25.0 44.7 54.0 8.0
Mug-STAN-B/32† 50.9 74.6 84.1 1.0 52.4 78.1 85.8 1.0 25.8 45.9 54.6 8.0
Mug-STAN-B/32*† 55.9 77.3 88.0 1.0 57.2 79.9 87.3 1.0 26.9 46.0 55.4 7.0
CLIP-B/16
CenterCLIP [72] 48.4 73.8 82.0 2.0 - - - - 24.2 46.2 55.9 8.0
DRL* [16] 53.3 80.3 87.6 1.0 49.0 76.5 84.5 2.0 26.5 47.6 56.8 7.0
CLIP-ViP† [6] 54.2 77.2 84.8 1.0 50.5 78.4 84.5 1.0 29.4 50.6 59.0 5.0
CLIP-ViP*† [6] 57.7 80.5 88.2 1.0 55.3 82.0 89.3 1.0 30.7 51.4 60.6 5.0
Mug-STAN-B/16 51.9 77.8 85.3 1.0 53.1 78.9 86.8 1.0 28.0 49.7 59.4 6.0
Mug-STAN-B/16† 53.9 77.4 85.8 1.0 56.6 79.7 87.1 1.0 29.2 50.5 59.6 5.0
Mug-STAN-B/16*† 57.3 81.6 88.4 1.0 61.2 84.1 88.9 1.0 30.8 51.9 61.2 5.0
CLIP-L/14
InternVideo† [59] 55.2 - - - 57.9 - - - 34.0 - - -
Mug-STAN-L/14† 56.6 80.0 87.8 1.0 60.4 85.0 90.5 1.0 35.3 58.0 65.2 3.0
Mug-STAN-L/14*† 61.3 82.6 90.1 1.0 65.0 87.5 92.0 1.0 37.2 59.2 67.0 3.0
descriptions sourced from stock footage sites. With 10.7M
video-caption pairs and 52K total video hours, the videos
are diverse and rich in their content, which has demonstrated
fancy results in both downstream video-language tasks [65]
and video generation tasks [66]. HowTo100M is a large-scale
dataset of narrated videos from Youtube videos. It features a
total of 136M video clips with captions and 23k activities.
Unlike Webvid, most of the captions in HowTo100M are
derived from automated speech recognition (ASR) or subti-
tles. Consequently, this leads to a more severe misalignment
between the video and text.
B. Experiment Settings
Model Setting. In most experiments, we adopt CLIP as
the baseline image-language pretrained models for a fair
comparison with previous works. For STAN, the number of
STAN layers is set as 4 for all datasets except on SSv2 when
it is set to 6. The STAN layers and CLIP layers are one-to-
one corresponded from top to bottom. For Mug, we employ
frame-to-token interaction by default. The temperature scalar
τin Mug is set to the same unlearnable value as the logitscale in CLIP because Mug does not change the scale of CLIP
features during feature transformation. To further evaluate
the generalizability of Mug-STAN, we also implement Mug-
STAN upon CoCa using the same configuration as CLIP.
Post-pretraining. On both datasets, we employ a sparse
sampling strategy [35] to sample 12 frames with each frame
resized to 224*224 for each video clip, and for text, the token
length is set to 64. We use AdamW [73] optimizer with a
weight decay of 0.001, and set the initial learning rate as 4e-
6 and 4e-5 for CLIP layers and STAN layers with a cosine
annealing decay schedule. We train our model using only
normalized contrastive loss and do not include other targets
like masked language modeling or video-text matching. We
train models with a batch size of 1024 for 3 epochs. It takes
1.6k GPU hours with 32 A100 GPUs for post-pretraining on
HowTo100M, while the consumption is 0.8k GPU hours on
WebVid10M. To evaluate the efficacy of post-pretraining, we
compare the performance of post-pretrained models through
both zero-shot and fine-tuning settings on downstream tasks.
Finetuning. For all datasets, the batch size is set to 128, and
we adopt AdamW as our optimizer with a weight decay of

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 10
TABLE III
THE FINETUNING RESULTS OF VIDEO RECOGNITION ON KINETICS -400 AND SOMETHING -SOMETHING -2. W E PRESENT METHODS OF COMPARABLE
SCALE FOR FAIR COMPARISON . W E REPORT THE FLOP S OF ALL VIEWS .
Methods Frames Testing Views GFLOPs K400 Acc@1 K400 Acc@5 SSv2 Acc@1 SSv2 Acc@5
Non-CLIP models
TimeSformer-L [13] 96 1×3 7140 80.7 94.7 62.4 -
Video-Swin-B [38] 32 10×5 14729 82.7 95.5 69.6 92.7
MViT [74] 32 3×1 1362 82.9 95.7 67.7 90.9
ViViT-L [37] 32 4×3 11940 83.5 94.3 65.9 89.9
MTV-B [75] 32 4×3 11160 82.4 95.2 68.5 90.4
CLIP-B/16
CLIP -B/16 [1] 8 4×3 - 81.1 94.8 44.0 76.8
Action-CLIP-B/16 [57] 32 10×3 16890 83.8 96.2 - -
A6 [45] 16 − - 76.9 93.5 - -
STadapter-CLIP-B/16 [11] 8 1×3 455 82.0 95.7 67.1 91.2
STadapter-CLIP-B/16 [11] 32 1×3 1821 82.7 96.2 69.5 92.6
X-CLIP-B/16 [10] 8 4×3 1740 83.8 96.7 63.1 89.0
X-CLIP-B/16 [10] 16 4×3 3444 84.7 96.8 - -
Mug-STAN-B/16 8 1×3 593 84.7 96.7 67.7 91.5
Mug-STAN-B/16 16 1×3 1187 85.1 96.9 69.5 92.8
TABLE IV
THE FINETUNING RESULTS OF VIDEO DETECTION ON AVA 2.2. M ODELS
UTILIZING SELF -SUPERVISED RECONSTRUCTION ARE DE -EMPHASIZED . *
MEANS OUR IMPLEMENTATION .
Method Pretrain Frames GFLOPs mAP
SlowFast [50] K400 32 138 23.8
MViTv1-B [74] K400 64 455 27.3
MViTv2-B [76] K400 32 255 28.1
MVD-B [77] K400 8 180 29.8
VideoMAE-B [51] K400 8 180 31.8
CLIP-B/16* [1] K400 8 180 24.9
XCLIP-B/16* [10] K400 8 185 27.6
Mug-STAN-B/16 K400 8 197 29.3
0.02. For video-text retrieval, we adopt a frame number of 12
and a token length of 32 for MSRVTT, LSMDC. On Didemo
where videos have a longer duration, the frame number and
token number are set to 64 and 64. The learning rates are
initialized to 2e-6 and 2e-5 for parameters in CLIP and STAN
respectively. For video-only tasks, we sample 8 frames by
default. The learning rates are initialized to 8e-6 and 8e-5
for CLIP and STAN layers. For action detection, we further
pretrain Mug-STAN on K400 following previous work, and
adopt a frame span of 300, which aligns with the default frame
number of Kinetics videos.
C. Comparison With State-of-the-Art Methods
Zero-Shot Results. The zero-shot results of WebVid10M
post-pretraining are posted in Table. I. We evaluate Mug-
STAN on three text-video retrieval datasets and three video
recognition datasets. We report our results under different
model capacities, including on CLIP-B/32 ,CLIP-B/16 , and
CLIP-L/14 . As evident from the presentation, numerous ap-
proaches that introduce new structures onto CLIP tend to
compromise its zero-shot capabilities, despite achieving im-
proved fine-tuning outcomes, such as ActionCLIP, CLIP-ViP,
and XCLIP. In contrast, Mug-STAN demonstrates clear zero-
shot advantages over CLIP following post-pretraining. Note
that our comparison with CLIP is conducted fairly, considering
the little improvement achieved through CLIP post-pretrainingdetailed in Table I. Moreover, in comparison to the previ-
ous SOTA methods in the zero-shot setting, our approach
demonstrates significant advantages across all datasets, even
when the comparisons are conducted unfairly for us. For
instance, InternVideo [59] utilizes dual visual encoders, and
generative self-supervised techniques, and involves 50 times
more GPU days compared to our approach. Nevertheless,
our method outperforms InternVideo by significant margins,
achieving improvements of 1.7%, 8.1%, 3.1%, and 0.8% on
the MSRVTT, DiDeMo, LSMDC, and Kinetics400 datasets,
respectively. The results demonstrate our post-pretraining on
Mug-STAN does not damage the rich knowledge in the CLIP
while providing a stronger zero-shot capacity for video tasks.
Video-Language Tasks. We report the finetuning results of
text-to-video retrieval in Table II. We compare our Mug-STAN
with current SOTAs with various setting, including directly
finetuning, finetuning after post-pretraining and using extra
tricks during inference. As demonstrated in the results, when
directly fine-tuning for video-text retrieval tasks, Mug-STAN
brings about obvious advantage over CLIP, outperforming
CLIP4clip by 4.7% at R@1 on average across the three
datasets with CLIP -B/32 as backbone. Compared to another
state-of-the-art method DRL [16], which also leverages frame-
token wise interaction to boost performance, Mug-STAN out-
performs it by 1.1% at R@1 on average across the three
datasets. When it comes to post-pretraining, it is worth noting
that only a few methods [6], [7], [59] have explored this area,
with CLIP-ViP [6] being the strongest competitor. Compared
toCLIP-ViP , which introduces an external strong captioner
[78] to augment pre-training datasets with additional captions,
our method is free from such complex data augmentation
and achieves competitive or even better performance across
different datasets. Moreover, Mug-STAN is able to bring about
performance gains by post-pretraining on smaller or noisier
datasets, while CLIP-ViP requires larger dataset i.e.,HDVilla-
100M [70]. Furthermore, compared to large competitors [59],
despite the disadvantages in terms of training cost, pretraining
method, and model scale, MugSTAN still outperforms Inter-

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 11
TABLE V
ABLATION RESULTS OF DIFFERENT COMPONENTS IN OUR MODEL ON DIFFERENT SETTINGS . “FT” MEANS DIRECT FINETUNING RESULTS WITHOUT
PERTAINING ; “ZS” MEANS THE ZERO -SHOT RESULT AFTER PERTAINING . W E REPORT THE RESULT OF RECALL @1.
Components Results
Branch structure Multi-level Separated-ST Mutual-Guided FT-MSRVTT FT-DiDemo ZS-MSRVTT ZS-DiDemo
43.1 43.4 30.6 24.7
✓ ✓ ✓ 46.9 46.2 33.0 28.1
✓ 46.1 45.4 33.1 29.8
✓ 44.9 43.5 31.9 25.4
✓ ✓ 44.2 43.6 31.8 25.4
✓ ✓ 45.5 44.7 32.2 26.9
✓ ✓ ✓ ✓ 48.9 49.6 35.9 33.7
TABLE VI
ABLATION RESULTS ON THE POST -PRETRAINING . W E REPORT THE FINETUNING RESULTS AFTER POST -PRETRAINING WITH CLIP-B/32 ON BOTH
MSR-VTT AND DIDEMO . W E CONDUCT THE PRETRAINING WITH DIFFERENT METHODS AND PRETRAINING DATASETS .
Model Pretrain DatasetDiDemo MSR-VTT
R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR
CLIP - 43.4 70.9 79.2 2 43.1 69.8 81.8 2
CLIP HowTo100M 43.0 70.2 80.3 2 43.4 69.7 82.3 2
CLIP WebVid10M 43.6 70.4 80.5 2 43.9 70.1 80.0 2
STAN+CLIP - 46.5 71.5 80.9 2 46.9 72.8 82.8 2
STAN+CLIP HowTo100M 47.0 72.1 81.6 2 47.1 72.3 82.5 2
STAN+CLIP WebVid10M 48.2 76.7 85.0 2 47.5 72.8 82.9 2
Mug+STAN+CLIP - 49.6 75.3 84.6 2 48.9 74.5 84.1 2
Mug+STAN+CLIP HowTo100M 51.6 77.3 85.2 1 50.0 75.1 83.6 2
Mug+STAN+CLIP WebVid10M 52.4 78.1 85.8 1 50.9 74.6 84.1 1
TABLE VII
FINETUNING RESULTS OF MUG-STAN ONCOCA[2] ONMSR-VTT AND
DIDEMO RETRIEVAL . †DENOTES FINETUNING AFTER POST -PRETRAINING .
Model R@1 R@5 R@10 MdR
MSR-VTT
CoCa (Baseline) 42.7 70.2 79.2 2.0
STAN-CoCa 44.7 72.9 81.9 2.0
Mug-STAN-CoCa 46.2 73.4 82.3 2.0
Mug-STAN-CoCa † 48.0 73.9 82.4 2.0
DiDemo
CoCa (Baseline) 39.6 67.9 77.0 2.0
STAN-CoCa 43.5 72.9 82.0 2.0
Mug-STAN-CoCa 46.7 73.9 82.0 2.0
Mug-STAN-CoCa † 48.8 73.8 83.7 1.5
Video across the three datasets.
Video-Only Tasks. We report the finetuning results of video
recognition and video detection on Kinetics-400, Something-
Something-2, and A V A-v2.2 in Table III and IV respectively.
In the K400-recognition benchmark, CLIP-based methods
demonstrate competitive performance with smaller model
scales compared to image-pretrained methods. For instance,
our VIT-B/16 based STAN achieves superior results compared
to models like ViViT [37] and Video-swin [38], which have
more than 10× GFLOPs compared to our method. As for
SSv2 and A V A benchmark, we observe that, without temporal
modeling, bare CLIP model [1] achieves only 44.0% top-1
accuracy and 25.9 mAP which dramatically under-performs
ImageNet-Kinetics pretrained models, though it owns pre-
trained knowledge obtained from a much larger image-textdataset. The result suggests that the domain gap is significant
between SSv2/A V A and CLIP model, and temporal modeling
capability is desired for the two datasets. STAN brings about
more than 25.5% and 4.4% performance improvement over
the CLIP baseline on SSv2 and A V A, which demonstrates that
Mug-STAN empowers CLIP with strong temporal modeling
capability. It is worth noting that, in comparison to video-
language tasks, the contrastive video-text pretraining does
not demonstrate significant advantages over image-pretraining
on video-only tasks. This is particularly evident for self-
supervised reconstruction methods. Nevertheless, Mug-STAN
manages to achieve competitive performance even in the
face of this challenge when compared to single-modality
pretrained methods. Moreover, in comparison to other CLIP-
based methods, Mug-STAN consistently exhibits advantages
across various datasets.
D. Ablation Study
Ablations on components of Mug-STAN. To evaluate the
contribution of different components in our method, we con-
duct ablation experiments on both finetuning setting and zero-
shot setting as shown in Table. V. First of all, in the first
three lines are the overall performance of STAN and Mug, we
can conclude that STAN and Mug are compatible with each
other while each of them contributes to the adaption of image-
language pretraining models, i.e., Mug addresses the issue of
partial misalignment in video-text data and STAN focuses on
the temporal modeling. Moreover, combining Mug and STAN,
the performance is further increased by a considerable margin,
which demonstrates that the temporal modeling capability and
the addressing of partial misalignment are mutually beneficial

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 12
434853586368
434445464748
024681012Top1 Acc on SSv2R@1 on MSRVTTNumber of STAN layerMSRVTTSSv24248546066
4344454647
Baseline1～45～89～12Top1 Acc on SSv2R@1 on MSRVTTLocation of STAN layerMSRVTTSSv24248546066
4344454647
Baseline321Top1 Acc on SSv2R@1 on MSRVTTInterval of STAN layerMSRVTTSSv24249566370
4344454647
0123Top1 Acc on SSv2R@1 on MSRVTTNumber of STAN networkMSRVTTSSv2
Fig. 6. Ablation results on the hyper-parameter setting of STAN. We report the finetuning results without post-pretraining on both MSRVTT and SSv2. We
study the number of STAN layers, the relative location of STAN layer respect to CLIP, the interval of STAN layer ( i.e., the number of CLIP layers between
STAN layer), and the number of STAN networks.
TABLE VIII
ABLATION RESULTS ON THE INTERACTION MODULE ,INCLUDING
INTERACTION GRANULARITY (MIDDLE )AND INTERACTION STRATEGIES
(BOTTOM ). W E REPORT THE RESULTS ON BOTH MSRVTT AND DIDEMO .
ModelMSR-VTT DiDemo
R@1 mean R@1 mean
STAN (Baseline) 46.9 67.5 46.5 66.3
Frame-Text Interaction 47.5 68.2 48.7 68.3
Video-Token Interaction 46.8 68.0 47.2 67.1
Frame-Token Interaction 48.9 69.2 49.6 69.8
Mug-STAN (max) 47.3 68.5 47.9 68.2
Mug-STAN (top3) 48.6 69.1 48.5 68.7
Mug-STAN (top5) 48.1 68.2 49.4 69.3
Mug-STAN (top7) 47.0 67.8 48.4 68.3
WTI-STAN [16] 47.5 68.8 47.2 67.9
Hunyuan-STAN [17] 47.5 68.8 47.5 67.4
Mug-STAN (softmax) 48.9 69.2 49.6 69.8
to each other. Secondly, lines 4-7 demonstrate the internal
structure of STAN. Specifically, when we eliminate the branch
structure or multi-level feature learning, the performance of
STAN experiences a substantial decline across all four bench-
marks. This serves as strong evidence of the superiority of
our model structure over the posterior structure. Additionally,
adopting joint-ST temporal modeling in STAN also brings
noticeable improvements, albeit not surpassing the separate
approach, which underscores the significance of reusing pa-
rameters from the pretrained model.
Ablations on Post-Pretraining. CLIP-ViP [6] points out two
factors that potentially hinder the video post-pretraining to
further improve the performance on downstream video tasks:
dataset scale anddomain gap . In this paper, through ablation
study on post-pretraining, we figure out that empowering
the pretrained model with temporal modeling capability and
addressing partial-misalignment problem are also crucial for
post-pretraining. We employ HowTo100M and WebVid10M
as pretraining dataset and train different models on the two
datasets, respectively.
As shown in Table VI, for the CLIP baseline, which employs
a simple mean pooling strategy for cross-frame modeling, it
takes trivial advantages from post-pretraining. As for experi-
ments on STAN, which owns expertise on temporal modeling,
we observe that post-pretraining on WebVid10M brings more
performance gains than that on CLIP baseline. When it comes
toMug-STAN , the performance gains of post-pretraining on
WebVid10M further increase to 2.8% on DiDemo and 2.0% on
MSR-VTT. Moreover, even on HowTo100M, which consists ofTABLE IX
ABLATION RESULTS ON THE EFFECTIVENESS OF MUG IN MITIGATING
PARTIAL MISALIGNMENT . ABOVE ,WE REPORT THE R@1 SCORES FOR
DATA WITH VARYING DEGREES OF MISALIGNMENT IN MSRVTT AND
DIDEMO DATASETS . BELOW ,WE COMPARE MUG-STAN WITH OTHER
STATE -OF-THE-ART VIDEO DENOISING METHODS .
levelMSR-VTT DiDemo
% STAN +Mug % STAN +Mug
top 38 53.7 54.0 42 54.3 54.8
middle 23 47.2 49.0 23 43.3 46.9
bottom 39 40.1 43.0 35 39.2 45.1
all 100 46.9 48.9 100 46.5 49.6
Model R@1 on HTM-Align R@1 on YouCook2
MIL-NCE [40] 31.3 15.1
TAN [42] 49.4 20.1
Mug-STAN 51.6 29.7
instructional videos with noise narrations and suffers from ex-
tremely severe partial-misalignment problem, our method still
brings about 2.0% and 1.1% performance gains on DiDemo
and MSR-VTT, respectively. The results reveal that temporal
modeling capability is beneficial to the post-pretraining while
addressing partial-misalignment problem is able to further
amplify the performance gains remarkably.
Can Mug-STAN work on image-language pretrained models
beyond CLIP? To verify the generalizability of our method,
we further implement Mug-STAN based on another famous
image-text pretrained model, i.e., CoCa [2]. We only use
the visual and text encoder of CoCa and load the pre-
trained weights released by OpenCLIP , which is pretrained
on LAION2b [79]. As is illustrated in Table VII, compared to
theCoCa baseline, which is directly fine-tuned on downstream
tasks with mean pooling as its temporal modeling strategy,
both STAN and Mug bring significant performance improve-
ment, while the post-pretraining on WebVid10M further boost
the finetuning result. The expermental results demonstrate
that Mug-STAN has the potential to be migrated to various
emergent image-text pretrained models.
What is the best hyper-parameter setting of STAN? STAN
functions as a new branch positioned alongside the pretrained
visual backbone, which takes the video frame representation at
different levels of pretrained visual layers as inputs. To study
impact of different setting of STAN, we present extensive
ablation study for STAN-CLIP-B/32 in Fig. 6 on both video-
language tasks and video-only tasks. The first is the number of
STAN layers, as is shown, for MSRVTT retrieval, the perfor-
mance enhancement of STAN reaches its peak at 4 layers, after
which the performance begins to decline with further increases

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 13
Query: a saladin a bowl is being filmed on a table
Query: a class is being introduced to a digital reading deviceMug-STANXCLIP
XCLIP
Query: animated piratessing on a ship
XCLIPMug-STANMug-STAN
Fig. 7. Qualitative results of text-video retrieval on MSR-VTT. Given a text
query, we present the correct matched video returned by Mug-STAN in the
first row, and show the false result of XCLIP in the second row. The word
highlighted in red indicates the key content missed in the false result.
Query: someone folds upa stroller
Query: a figure in someone walking with bent knees through large cement rooms toward  a door as people are being shot
Query: video of gymnasts practicing to rollMug-STANCLIP4clip
Mug-STANCLIP4clip
Mug-STANCLIP4clip
Fig. 8. Qualitative results of text-video retrieval on MSR-VTT. Given a text
query, we present the correct matched video returned by Mug-STAN in the
first row, and show the false result of CLIP4clip in the second row. The word
highlighted in red indicates the key content missed in the false result.
of layers; On SSv2, the performance improvement of STAN
seems to stabilize after 6 layers. Overall, using STAN with 4
to 6 layers is recommended as a suitable choice for various
tasks, considering the optimal balance between performance
gains and computational efficiency. Secondly is the location of
STAN layer. We fix the number of STAN layers to 4 and align
STAN layers with 1-4, 5-8 and 9-12 CLIP layers respectively.
The results suggest that the mid-to-high level of pretrained
Fig. 9. Visualziation of intra-frame module of STAN on MSR-VTT. Given
a text query. The region in red gains more attention from the model. We
visualize the attention with VideoCAM.
framesa car is in a wreck
textattention score:              0.090.010.090.120.300.39guidevideo
tokensattention score:              0.000.000.050.000.000.00guide[startoftext][CLStoken][a][car][is][in][a][wreck]0.310.64
Fig. 10. The qualitative result of the softmax scores of sentence guiding
frames in Eq .13 and video guiding tokens in Eq. 17.
CLIP representation holds more significance for downstream
tasks. Then, we align the last layer of CLIP and STAN, and
vary the interval of selected CLIP layers between the STAN
layers, e.g.,, interval=2 means STAN receives outputs of the
6th, 8th, 10th, and 12th layers. As shown in Table, interval=1
is the best choice for both datasets. Finally is the number of the
whole STAN networks. We find that introducing more STAN
layers makes no difference on MSRVTT but can bring a slight
improvement to ssv2, but this is not cost-effective considering
the increase in computational complexity.
Is Mug the optimal design for aligning videos and text? To
understand the optimal design for the video-text interaction
module, we perform a detailed ablation analysis. Initially, we
explore the granularity of interaction within Mug. In Table
VIII (middle), Frame-Text Interaction indicates substituting
video-guided text embedding in Mug with conventional [CLS]
token embedding, while Video-Token Interaction represents
substituting text-guided video embedding with conventional
averaged frame-wise embedding. The results demonstrate that
text-guided video embedding is more important than the
video-guided text embedding , which reveals that the partial-
misalignment problem is more severe in the video modality.
Then, we investigate different cross-modal interaction strate-
gies. A well-known interaction modeling module is WTI in
DRL [16] and its follower hunyuan [17], which learns single-
modality based attention scores to determine which token-
frame scores are most representative of text-video correspon-
dence. In contrast, Mug utilizes token-frame correspondence
scores to introduce cross-modal mutual guidance, where the
most relevant parts between the video-text pair, which poten-
tially have higher scores, would be highlighted. Table VIII
(bottom) shows that Mug outperforms WTI and hunyuan in
terms of performance. Besides, the mutual-guided cross-modal
embedding aggregation in Mug is akin to a soft key concept
selection process. To explore this idea, we further replace
the softmax operations in Eq.11,13,15,17 with a top-k hard
selection operation. However, we find that the optimal “top-
k” value varies across datasets, and the module with softmax

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 14
What is happening when the camera is pointed stationary towards the riders and the dog?
When the camera is pointed stationary towards the riders and the dog, it is capturing a scene where a group of people are riding horses through a wooded area, and a dog is accompanying them. The riders are likely enjoying a leisurely ride through the forest, and the dog is likely a companion or a working dog, assisting the riders in navigating the terrain or providing companionship. The image captures a peaceful and serene moment in nature, showcasing the bond between humans and animals as they share the experience of exploring the outdoors together.
What happened when the female gymnast was performing a somersault?
In video, the female gymnast was performing a somersault, which is a type of acrobatic move. As she was executing the somersault, she lost her balance and fell off the balance beam. This fall could have resulted from a variety of factors, such as incorrect technique, misjudgment of the landing, or simply a momentary lapse in concentration.
What does the little boy do on the armbar?The little boy is performing a trick on the armbar. He puts both his hands on the armbar and swings his legs in the air and then puts them on the armbar.
In the video, a man is seen holding a tennis racquet and preparing to serve the ball. He is standing on a tennis court, and the camera captures the sequence of events as he moves into position, swings the racquet, and strikes the ball. The ball is then in motion, and the man follows through with his swing, demonstrating his technique and skill in serving the tennis ball. The sequence captures the essence of a tennis serve, showcasing the player's form and execution of the serve.
Can you describe the sequence of events during the serve in the video?
In the video, a man is seen hitting a tennis ball with a tennis racket. He then goes to hit the ball back and forth.
When the camera is pointed stationary towards the riders and the dog, the riders are seen riding horses down a river. The dog is also riding on a horse.
The little boy does a handstand on the armbar, and then gets down.
In the image, a female gymnast is performing a somersault on the balance beam during a gymnastics competition. As she performs the maneuver, she appears to be in mid-air, with one leg extended and the other tucked underneath her body. The somersault is a challenging element in gymnastics that requires strength, flexibility, and coordination. 
Fig. 11. The qualitative results of video chatting. We showcase the results
from LLaVa (above) and STAN-LLaVa (below).
consistently outperforms the others on both datasets.
Is Mug effective for video-text misalignment? In Table
IV-D, we investigate the effectiveness of Mug in addressing
misalignment. Firstly, as depicted in Fig 2(b), video-text
datasets often exhibit varying degrees of misalignment. In
the upper bound, we further post results of Mug on data
with different levels of misalignment. Notably, the majority
of Mug’s improvements are observed in cases of moderate to
severe misalignment. The more pronounced the misalignment
in video-text data, the greater the performance gains Mug
exhibits over STAN. Furthermore, we conduct a comparison
with other methods for mitigating misalignment, as indicatedin the lower bound of the table. To ensure a fair comparison,
following [42], we pretrain Mug-STAN on HTM-370K and
evaluate its zero-shot performance on datasets as presented
in [42]. The results clearly demonstrate that Mug-STAN has
a significant advantage over other state-of-the-art methods
when operating under the same experimental conditions. In
summary, our experimental findings across various dimensions
consistently highlight the effectiveness of Mug in addressing
misalignment in video-text data.
E. Qualitative Results
In experiments, we substantiated Mug-STAN’s capacity
for proficient temporal modeling, all the while harnessing
the benefits of pretrained knowledge. Expanding on these
quantitative findings, we now present qualitative results that
unveil the efficacy of Mug-STAN across these two aspects.
First of all, we showcase text-to-video retrieval outcomes of
the intermediate-structure based method XCLIP [10] and our
Mug-STAN. Illustrated in Fig.7, these instances can be effort-
lessly resolved if a model can effectively align the emphasized
object concepts in queries, like ”salad,” with videos that con-
tain corresponding visual content. However, XCLIP produces
inaccurate outcomes by returning results where the crucial
objects are missing from the videos. This comparison under-
scores the limitation of the intermediate structure in effectively
transferring high-level visual-text alignment knowledge, the
work at which our method excels. Subsequently, we provide
comparison results of text-to-video retrieval for CLIP4clip
[7] and Mug-STAN in Fig.8. The figure demonstrates that
CLIP4clip, which is based on the posterior structure, produces
incorrect outcomes. Although the results encompass accurate
static contexts as described in queries (such as “stroller” and
“gymnasts”), they feature erroneous dynamic information that
doesn’t align with the emphasized concepts in the queries
(such as “folds up” and “roll”). These results emphasize that
our approach can more effectively harness spatial-temporal
information for enhanced video comprehension. Then, we
visualize the attention of Mug-STAN’s intra-frame module
using VideoCAM, as depicted in Fig.9. These visualizations
demonstrate that our STAN module consistently directs its
attention towards critical content within videos, spanning
across different moments in time. Finally, to shed more light
to the effectiveness of Mug, we present a qualitative result of
the cross-modal guidance. In Fig. 10, we present both the text-
to-frame correspondence scores esi(above) and video-to-token
correspondence scoreses′
j(below). The results show that for
text-to-frame guidance, most of the attention is focused on the
last two frames where the cars are rolling, which contain the
most relevant information with the text. For video-to-token
guidance, the attention is guided towards the tokens “car”,
“wreck”, and the ending token (CLS token). It reveals that
Mug efficiently enhancing the aligned parts in the video-text
pair for cross-modal alignment.
F . Video Chatting
The domain of natural language processing has under-
gone a significant transformation with the introduction of

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 15
pretrained Large Language Models (LLMs). The achievements
of LLMs have also hastened the advancement of AI systems
that integrate visual models with LLMs, enabling multimodal
reasoning and action [24], [32], [33], [80]. Commonly, these
models construct a projection from the output of the pretrained
visual encoder ( e.g., CLIP) to the input of the LLM. They
then engage in visual instruction tuning, a process that fa-
cilitates multimodal interactions and conversations. Inspired
by these visual-language chatbots, a new wave of methods
has emerged that involve video chatting, which engages video
backbone with LLMs and performs video instruction tuning
[65]. Nonetheless, the training of video-language chatbots
encounters similar challenges as video-language pretraining,
namely huge computation costs and limited training source.
Fortunately, Mug-STAN offers a potential solution to these
challenges. Unlike existing video chatbots, our approach does
not involve the resource-intensive instruction tuning. Instead,
we harness the power of existing image-language knowledge
in a zero-shot manner. Specifically, we first post-pretraining
Mug-STAN-CLIP on video-language datasets. Following this,
we incorporate the pretrained branch networks into the visual
backbone of image-language chatbots. Given that most ex-
isting multimodal chatting commonly utilizes a frozen CLIP
as the visual backbone, our method can seamlessly empower
image-language chatbots with the capacity for video under-
standing and processing. Lastly, the video token can also
be seamlessly fed into the LLM for video chatting. This
integration is facilitated by the fact that the output of STAN
matches the token count of the image encoder. We take
LLaVa [24] as pretrained image-text chatbots and present the
qualitative results of STAN-LLaVa in Fig. 11. Compared with
LLaVa, our method empowers the chatbot to precisely narrate
the events within the video sequence and accurately recognize
temporal-extensive actions. Notably, these results are achieved
without resorting to any instruction tuning. This underscores
the significant potential of Mug-STAN in adapting pretrained
image-language chatbots to the realm of videos.
V. C ONCLUSION
In this paper, we first investigate and identify the key
point of adapting pretrained image-language models to video
domains: building generalizable temporal modeling and sup-
pressing video-text partial misalignment. To this end, we
propose the novel Spatial-Temporal Auxiliary Network with
Mutual-guided alignment module (Mug-STAN), where STAN
utilizes the multi-level branch structure for effective temporal
modeling and Mug introduces cross-modal mutual-guided fea-
ture aggregation to mitigate misalignment. Finally, we perform
comprehensive experiments to demonstrate the superiority of
Mug-STAN. Extensive experimental results show that our
adaption method achieves state-of-the-art results on a broad
range of video tasks.
REFERENCES
[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al. , “Learning transferable
visual models from natural language supervision,” in International
Conference on Machine Learning . PMLR, 2021, pp. 8748–8763.[2] J. Yu, Z. Wang, V . Vasudevan, L. Yeung, M. Seyedhosseini, and Y . Wu,
“Coca: Contrastive captioners are image-text foundation models,” arXiv
preprint arXiv:2205.01917 , 2022.
[3] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal,
O. K. Mohammed, S. Singhal, S. Som et al. , “Image as a foreign
language: Beit pretraining for vision and vision-language tasks,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2023, pp. 19 175–19 186.
[4] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and
J. Sivic, “Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2019, pp. 2630–2640.
[5] J. Huang, Y . Li, J. Feng, X. Wu, X. Sun, and R. Ji, “Clover: Towards a
unified video-language alignment and fusion model,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 14 856–14 866.
[6] H. Xue, Y . Sun, B. Liu, J. Fu, R. Song, H. Li, and J. Luo, “Clip-vip:
Adapting pre-trained image-text model to video-language alignment,”
inThe Eleventh International Conference on Learning Representations ,
2022.
[7] H. Luo, L. Ji, M. Zhong, Y . Chen, W. Lei, N. Duan, and T. Li, “Clip4clip:
An empirical study of clip for end to end video clip retrieval and
captioning,” Neurocomputing , vol. 508, pp. 293–304, 2022.
[8] R. Liu, J. Huang, G. Li, J. Feng, X. Wu, and T. H. Li, “Revisiting tem-
poral modeling for clip-based image-to-video knowledge transferring,”
arXiv preprint arXiv:2301.11116 , 2023.
[9] S. Buch, C. Eyzaguirre, A. Gaidon, J. Wu, L. Fei-Fei, and J. C.
Niebles, “Revisiting the” video” in video-language understanding,” in
Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2022, pp. 2917–2927.
[10] B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang,
and H. Ling, “Expanding language-image pretrained models for gen-
eral video recognition,” in European Conference on Computer Vision .
Springer, 2022, pp. 1–18.
[11] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, “St-adapter: Parameter-
efficient image-to-video transfer learning,” Advances in Neural Infor-
mation Processing Systems , vol. 35, pp. 26 462–26 477, 2022.
[12] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new
model and the kinetics dataset,” in proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , 2017, pp. 6299–6308.
[13] G. Bertasius, H. Wang, and L. Torresani, “Is space-time attention all
you need for video understanding?” in ICML , vol. 2, no. 3, 2021, p. 4.
[14] H. Fang, P. Xiong, L. Xu, and Y . Chen, “Clip2video: Mastering video-
text retrieval via image clip,” arXiv preprint arXiv:2106.11097 , 2021.
[15] Y . Liu, P. Xiong, L. Xu, S. Cao, and Q. Jin, “Ts2-net: Token shift
and selection transformer for text-video retrieval,” in Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part XIV . Springer, 2022, pp. 319–335.
[16] Q. Wang, Y . Zhang, Y . Zheng, P. Pan, and X.-S. Hua, “Disentan-
gled representation learning for text-video retrieval,” arXiv preprint
arXiv:2203.07111 , 2022.
[17] J. Jiang, S. Min, W. Kong, H. Wang, Z. Li, and W. Liu, “Tencent text-
video retrieval: Hierarchical cross-modal interactions with multi-level
representations,” IEEE Access , 2022.
[18] J. Xu, T. Mei, T. Yao, and Y . Rui, “Msr-vtt: A large video description
dataset for bridging video and language,” in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2016, pp. 5288–
5296.
[19] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-
narasimhan, F. Viola, T. Green, T. Back, P. Natsev et al. , “The kinetics
human action video dataset,” arXiv preprint arXiv:1705.06950 , 2017.
[20] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and
B. Russell, “Localizing moments in video with natural language,” in
Proceedings of the IEEE international conference on computer vision ,
2017, pp. 5803–5812.
[21] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, “Frozen in time: A
joint video and image encoder for end-to-end retrieval,” in Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2021,
pp. 1728–1738.
[22] H. Xue, T. Hang, Y . Zeng, Y . Sun, B. Liu, H. Yang, J. Fu, and B. Guo,
“Advancing high-resolution video-language representation with large-
scale video transcriptions,” in International Conference on Computer
Vision and Pattern Recognition (CVPR) , 2022.
[23] R. Zellers, X. Lu, J. Hessel, Y . Yu, J. S. Park, J. Cao, A. Farhadi, and
Y . Choi, “Merlot: Multimodal neural script knowledge models,” Ad-
vances in Neural Information Processing Systems , vol. 34, pp. 23 634–
23 651, 2021.

--- PAGE 16 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 16
[24] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,” arXiv
preprint arXiv:2304.08485 , 2023.
[25] Z. Huang, Z. Zeng, Y . Huang, B. Liu, D. Fu, and J. Fu, “Seeing out
of the box: End-to-end pre-training for vision-language representation
learning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2021, pp. 12 976–12 985.
[26] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, “Pixel-bert: Aligning
image pixels with text by deep multi-modal transformers,” arXiv preprint
arXiv:2004.00849 , 2020.
[27] H. Xue, Y . Huang, B. Liu, H. Peng, J. Fu, H. Li, and J. Luo,
“Probing inter-modality: Visual parsing with self-attention for vision-
and-language pre-training,” Advances in Neural Information Processing
Systems , vol. 34, pp. 4514–4528, 2021.
[28] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H.
Sung, Z. Li, and T. Duerig, “Scaling up visual and vision-language
representation learning with noisy text supervision,” in International
Conference on Machine Learning . PMLR, 2021, pp. 4904–4916.
[29] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu,
X. Huang, B. Li, C. Li et al. , “Florence: A new foundation model for
computer vision,” arXiv preprint arXiv:2111.11432 , 2021.
[30] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, “Learning to prompt for vision-
language models,” International Journal of Computer Vision , vol. 130,
no. 9, pp. 2337–2348, 2022.
[31] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski,
“Styleclip: Text-driven manipulation of stylegan imagery,” in Proceed-
ings of the IEEE/CVF International Conference on Computer Vision ,
2021, pp. 2085–2094.
[32] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,
A. Mensch, K. Millican, M. Reynolds et al. , “Flamingo: a visual
language model for few-shot learning,” Advances in Neural Information
Processing Systems , vol. 35, pp. 23 716–23 736, 2022.
[33] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language
models,” arXiv preprint arXiv:2301.12597 , 2023.
[34] T.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y . Wang, L. Wang, and Z. Liu,
“Violet: End-to-end video-language transformers with masked visual-
token modeling,” arXiv preprint arXiv:2111.12681 , 2021.
[35] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, “Less
is more: Clipbert for video-and-language learning via sparse sampling,”
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2021, pp. 7331–7341.
[36] Y . Ge, Y . Ge, X. Liu, D. Li, Y . Shan, X. Qie, and P. Luo, “Bridging
video-text retrieval with multiple choice questions,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 16 167–16 176.
[37] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu ˇci´c, and C. Schmid,
“Vivit: A video vision transformer,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 6836–6846.
[38] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu, “Video swin
transformer,” in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , 2022, pp. 3202–3211.
[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[40] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zis-
serman, “End-to-end learning of visual representations from uncurated
instructional videos,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2020, pp. 9879–9889.
[41] Z. Zeng, Y . Ge, X. Liu, B. Chen, P. Luo, S.-T. Xia, and Y . Ge,
“Learning transferable spatiotemporal representations from natural script
knowledge,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 23 079–23 089.
[42] T. Han, W. Xie, and A. Zisserman, “Temporal alignment networks
for long-term video,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 2906–2916.
[43] Z. Gao, J. Liu, W. Sun, S. Chen, D. Chang, and L. Zhao, “Clip2tv:
Align, match and distill for video-text retrieval,” arXiv preprint
arXiv:2111.05610 , 2021.
[44] H. Zhang, A. Sun, W. Jing, and J. T. Zhou, “Temporal sentence
grounding in videos: A survey and future directions,” IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2023.
[45] C. Ju, T. Han, K. Zheng, Y . Zhang, and W. Xie, “Prompting visual-
language models for efficient video understanding,” in European Con-
ference on Computer Vision . Springer, 2022, pp. 105–124.
[46] P. Hu, Z. Huang, D. Peng, X. Wang, and X. Peng, “Cross-modal retrieval
with partially mismatched pairs,” IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2023.[47] F. Liu, X. Wu, C. You, S. Ge, Y . Zou, and X. Sun, “Aligning source
visual and target language domains for unpaired video captioning,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 44,
no. 12, pp. 9255–9268, 2021.
[48] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition at
scale,” in International Conference on Learning Representations , 2020.
[49] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang, F. E. Tay, J. Feng,
and S. Yan, “Tokens-to-token vit: Training vision transformers from
scratch on imagenet,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision , 2021, pp. 558–567.
[50] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks
for video recognition,” in Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , October 2019.
[51] Z. Tong, Y . Song, J. Wang, and L. Wang, “Videomae: Masked autoen-
coders are data-efficient learners for self-supervised video pre-training,”
Advances in neural information processing systems , vol. 35, pp. 10 078–
10 093, 2022.
[52] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in
Proceedings of the IEEE international conference on computer vision ,
2017, pp. 2961–2969.
[53] H. Xu, G. Ghosh, P.-Y . Huang, D. Okhonko, A. Aghajanyan, F. Metze,
L. Zettlemoyer, and C. Feichtenhofer, “Videoclip: Contrastive pre-
training for zero-shot video-text understanding,” in Proceedings of the
2021 Conference on Empirical Methods in Natural Language Process-
ing, 2021, pp. 6787–6800.
[54] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, “Align and prompt:
Video-and-language pre-training with entity prompts,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 4953–4963.
[55] J. Wang, D. Chen, Z. Wu, C. Luo, L. Zhou, Y . Zhao, Y . Xie, C. Liu, Y .-G.
Jiang, and L. Yuan, “Omnivl: One foundation model for image-language
and video-language tasks,” Advances in neural information processing
systems , vol. 35, pp. 5696–5710, 2022.
[56] J. A. Portillo-Quintero, J. C. Ortiz-Bayliss, and H. Terashima-Mar ´ın, “A
straightforward framework for video retrieval using clip,” in Mexican
Conference on Pattern Recognition . Springer, 2021, pp. 3–12.
[57] M. Wang, J. Xing, and Y . Liu, “Actionclip: A new paradigm for video
action recognition,” arXiv preprint arXiv:2109.08472 , 2021.
[58] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V . Alwala, A. Joulin,
and I. Misra, “Imagebind: One embedding space to bind them all,”
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2023, pp. 15 180–15 190.
[59] Y . Wang, K. Li, Y . Li, Y . He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y . Liu,
Z. Wang et al. , “Internvideo: General video foundation models via gen-
erative and discriminative learning,” arXiv preprint arXiv:2212.03191 ,
2022.
[60] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle,
A. Courville, and B. Schiele, “Movie description,” International Journal
of Computer Vision , vol. 123, no. 1, pp. 94–120, 2017.
[61] R. Goyal, S. Ebrahimi Kahou, V . Michalski, J. Materzynska, S. West-
phal, H. Kim, V . Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al. ,
“The” something something” video database for learning and evaluat-
ing visual common sense,” in Proceedings of the IEEE international
conference on computer vision , 2017, pp. 5842–5850.
[62] C. Gu, C. Sun, D. A. Ross, C. V ondrick, C. Pantofaru, Y . Li, S. Vi-
jayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar et al. , “Ava: A
video dataset of spatio-temporally localized atomic visual actions,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition , 2018, pp. 6047–6056.
[63] Y . Yu, J. Kim, and G. Kim, “A joint sequence fusion model for video
question answering and retrieval,” in Proceedings of the European
Conference on Computer Vision (ECCV) , 2018, pp. 471–487.
[64] L. Sevilla-Lara, S. Zha, Z. Yan, V . Goswami, M. Feiszli, and L. Tor-
resani, “Only time can tell: Discovering temporal data for temporal
modeling,” in Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV) , January 2021, pp. 535–544.
[65] K. Li, Y . He, Y . Wang, Y . Li, W. Wang, P. Luo, Y . Wang, L. Wang, and
Y . Qiao, “Videochat: Chat-centric video understanding,” arXiv preprint
arXiv:2305.06355 , 2023.
[66] Z. Luo, D. Chen, Y . Zhang, Y . Huang, L. Wang, Y . Shen, D. Zhao,
J. Zhou, and T. Tan, “Videofusion: Decomposed diffusion models
for high-quality video generation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2023, pp.
10 209–10 218.

--- PAGE 17 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, MAY 2023 17
[67] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen, “Improving video-text
retrieval by multi-stream corpus alignment and dual softmax loss,” arXiv
preprint arXiv:2109.04290 , 2021.
[68] S.-V . Bogolin, I. Croitoru, H. Jin, Y . Liu, and S. Albanie, “Cross
modal retrieval with querybank normalisation,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 5194–5205.
[69] V . Gabeur, C. Sun, K. Alahari, and C. Schmid, “Multi-modal transformer
for video retrieval,” in Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV
16. Springer, 2020, pp. 214–229.
[70] H. Xue, T. Hang, Y . Zeng, Y . Sun, B. Liu, H. Yang, J. Fu, and B. Guo,
“Advancing high-resolution video-language representation with large-
scale video transcriptions,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2022, pp. 5036–5045.
[71] J. Wang, Y . Ge, R. Yan, Y . Ge, K. Q. Lin, S. Tsutsui, X. Lin, G. Cai,
J. Wu, Y . Shan et al. , “All in one: Exploring unified video-language
pre-training,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 6598–6608.
[72] S. Zhao, L. Zhu, X. Wang, and Y . Yang, “Centerclip: Token clustering for
efficient text-video retrieval,” in Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information
Retrieval , 2022, pp. 970–981.
[73] I. Loshchilov and F. Hutter, “Fixing weight decay regularization in
adam,” 2018.
[74] H. Fan, B. Xiong, K. Mangalam, Y . Li, Z. Yan, J. Malik, and C. Feichten-
hofer, “Multiscale vision transformers,” in Proceedings of the IEEE/CVF
international conference on computer vision , 2021, pp. 6824–6835.
[75] S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid,
“Multiview transformers for video recognition,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , 2022,
pp. 3333–3343.
[76] Y . Li, C.-Y . Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and
C. Feichtenhofer, “Mvitv2: Improved multiscale vision transformers
for classification and detection,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , June
2022, pp. 4804–4814.
[77] R. Wang, D. Chen, Z. Wu, Y . Chen, X. Dai, M. Liu, L. Yuan, and Y .-G.
Jiang, “Masked video distillation: Rethinking masked feature modeling
for self-supervised video representation learning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 6312–6322.
[78] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou,
and H. Yang, “Ofa: Unifying architectures, tasks, and modalities through
a simple sequence-to-sequence learning framework,” in International
Conference on Machine Learning . PMLR, 2022, pp. 23 318–23 340.
[79] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman,
M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al. , “Laion-
5b: An open large-scale dataset for training next generation image-text
models,” Advances in Neural Information Processing Systems , vol. 35,
pp. 25 278–25 294, 2022.
[80] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4:
Enhancing vision-language understanding with advanced large language
models,” arXiv preprint arXiv:2304.10592 , 2023.
