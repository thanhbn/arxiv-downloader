# 2204.05229.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2204.05229.pdf
# Kích thước tệp: 1147636 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
VAE hỗn hợp chuyên gia có thể bỏ qua sự biến thiên trong
dữ liệu đa phương thức toàn ánh
Jannik Wolffy
TU BerlinTassilo Klein, Moin Nabi
SAP AI ResearchRahul G. Krishnanz
University of TorontoShinichi Nakajima
TU Berlin

Tóm tắt
Các hệ thống học máy thường được triển khai trong các lĩnh vực bao gồm dữ liệu từ
nhiều phương thức, ví dụ, đặc điểm kiểu hình và kiểu gen mô tả bệnh nhân trong chăm sóc sức khỏe. Các nghiên cứu trước đây đã phát triển các bộ tự mã hóa biến phân đa phương thức (VAE) tạo ra nhiều phương thức. Chúng tôi xem xét dữ liệu toàn ánh, trong đó các điểm dữ liệu đơn lẻ từ một phương thức (như nhãn lớp) mô tả nhiều điểm dữ liệu từ phương thức khác (như hình ảnh). Chúng tôi chứng minh về mặt lý thuyết và thực nghiệm rằng các VAE đa phương thức với phân phối hậu nghiệm hỗn hợp chuyên gia có thể gặp khó khăn trong việc nắm bắt tính biến thiên trong dữ liệu toàn ánh như vậy.

1 Giới thiệu

Nhiều bộ dữ liệu bao hàm ánh xạ toàn ánh giữa các phương thức (Hình 1, "dữ liệu một-nhiều"). Có nghĩa là, một thể hiện từ một phương thức có thể tương ứng với nhiều thể hiện từ phương thức khác. Ví dụ, nhiều bộ dữ liệu thị giác máy tính chứa nhãn, thuộc tính, hoặc dữ liệu văn bản mô tả các tập hình ảnh [LeCun, 1998, Nilsback và Zisserman, 2008, Krizhevsky et al., 2009, Deng et al., 2009, Wah et al., 2011, Liu et al., 2015, Xiao et al., 2017]. Lưu ý rằng "dữ liệu một-một" như cặp hình ảnh/chú thích có thể trở thành toàn ánh khi sử dụng tăng cường dữ liệu, ví dụ, lật ngang ngẫu nhiên các hình ảnh. Việc kết hợp thêm các phương thức cũng có thể gây ra tính toàn ánh.

VAE đa phương thức tối đa hóa một cận trên mật độ chung của nhiều phương thức và do đó có thể học cách tạo ra bất kỳ phương thức nào từ bất kỳ phương thức điều kiện nào [Suzuki et al., 2016]. Đối với một số VAE đa phương thức, cận này chứa một yếu tố đại diện cho khả năng của một phương thức cho trước phương thức khác. Chúng tôi sẽ chỉ ra rằng yếu tố như vậy trong hàm mục tiêu có thể dẫn đến các giải pháp bỏ qua tính không đồng nhất trong một phương thức. Ví dụ, chúng tôi chứng minh rằng các mẫu từ các mô hình với phân phối hậu nghiệm hỗn hợp chuyên gia như MMVAE [Shi et al., 2019] có thể có xu hướng về trung bình lớp của các điểm dữ liệu quan sát được cho một phương thức nhất định.

2 Phương pháp

Cho X={{{x(n)_m}^M_{m=1}}^N_{n=1} là một tập huấn luyện với nhiều phương thức, trong đó m và n biểu thị chỉ số phương thức và mẫu, tương ứng. Chúng tôi xem xét một VAE đa phương thức với mô hình tạo sinh
g~p(g);
x_m~p(x_m|g) cho m = 1, ..., M; (1)

và một mô hình suy luận
g~q(g|{x_m}^M_{m=1}): (2)

Giả sử rằng mô hình tạo sinh (1) là một mô hình tham số, ví dụ, Gaussian,
p(x_m|g) = f_m(x_m|θ_m(g;Φ)); (3)

với các tham số {θ_m}, ví dụ, trung bình và hiệp phương sai, được định nghĩa như một hàm của g và (thường) trọng số mạng nơ-ron Φ. Giả sử rằng mô hình suy luận (2) được định nghĩa như một hỗn hợp hữu hạn với các tham số μ_m chỉ ra trung bình và hiệp phương sai cho thành phần hỗn hợp r_m (như trong MMVAE [Shi et al., 2019], ví dụ):

q(g|{x_m}^M_{m=1}) = (1/M)∑^M_{m=1} q(g|x_m) = (1/M)∑^M_{m=1} r_m(g|μ_m(x_m;Ψ)):

Không mất tính tổng quát, chúng tôi giả sử rằng x_M là phương thức nhãn, và cho S_c = {n|x^(n)_M = c} là tập các chỉ số của các mẫu thuộc về nhãn c ∈ {1, ..., C}. Chúng tôi xem xét một bài toán tối đa hóa với hàm mục tiêu sau:

L_m(Ψ,Φ;X) ≈ ∑^N_{n=1} ∫ r_M(g|μ_M(x^(n)_M;Ψ)) log f_m(x^(n)_m|θ_m(g;Φ))dg; (4)

đây là một ELBO cho
log p(x_m|x_M) = log ∫ q(g|x_M)p(x_m|g)dg ≥ ∫ q(g|x_M) log p(x_m|g)dg = L_m(Ψ,Φ;X):

Quan trọng là, MMVAE [Shi et al., 2019] dựa vào số hạng (4) để học khả năng dịch dữ liệu từ x_M sang x_m. Cụ thể, các tác giả đã sử dụng lấy mẫu phân tầng để huấn luyện, điều này ngụ ý rằng Phương trình 4 và số hạng 1 từ Phương trình 5 có liên quan:

log p({x_m}^M_{m=1}) ≥ (1/M)∑^M_{m=1} E_{q(g|x_m)}[log p(g,{x_m}^M_{m=1})/q(g|{x_m}^M_{m=1})]
= (1/M)∑^{M-1}_{m=1} E_{q(g|x_m)}[log p(g,{x_m}^M_{m=1})/q(g|{x_m}^M_{m=1})]
+ E_{q(g|x_M)}[log p(g)/q(g|{x_m}^M_{m=1})]
+ ∑^M_{i=1} E_{q(g|x_M)}[log p(x_i|g)]
|________1________|    (5)

Định lý sau đây thành lập:

Định lý 1. Giả sử một tập huấn luyện X = {x^(n)_m}_{n∈S_c} thuộc về cùng một nhãn, tức là x^(n)_M = c, ∀n ∈ S_c, và tồn tại Φ̂ sao cho θ_m(g;Φ̂) là một hằng số đối với g và ước lượng hợp lý cực đại của mô hình tham số f_m(x_m|θ_m(g;Φ)) cho dữ liệu huấn luyện. Khi đó, với bất kỳ Ψ, Φ nào, ta có
L_m(Φ̂,Ψ;X) ≥ L_m(Ψ,Φ;X): (6)

(Chứng minh) Vì chúng ta giả sử rằng x^(n)_M = c với mọi n ∈ S_c, phân phối suy luận cho g là giống nhau với mọi n, tức là r̃_M(g) = r_M(g|μ_M(x^(n)_M;Ψ)). Với bất kỳ mô hình suy luận r̃_M(g) nào như vậy, mục tiêu được

--- TRANG 2 ---
giới hạn trên bởi

L_m(Ψ,Φ;X) = ∫ r̃_M(g;Ψ) (∑^N_{n=1} log f_m(x^(n)_m|θ_m(g;Φ))) dg (7)
≤ ∫ r̃_M(g;Ψ) (∑^N_{n=1} log f_m(x^(n)_m|θ̂_m)) dg

với ước lượng hợp lý cực đại θ̂_m cho mô hình tham số {f_m} với tập huấn luyện {x^(n)_m}_{n=S_c}. Sự tồn tại giả định của Φ̂ sao cho θ_m(g;Φ̂) = θ̂_m dẫn đến Phương trình (6). □

Một cách trực quan, xem xét một lớp đơn: c ∈ {1}. Cho p(x_m|g) là Gaussian với hiệp phương sai đường chéo, trong đó g ~ q(g|x_M). Định lý 1 ngụ ý sự tồn tại của một cận trên nơi tham số trung bình từ p(x_m|g) luôn trùng với trung bình từ {x^n_m}_{n∈S_c} với bất kỳ g nào. Giải pháp này bất biến với g vì x_M không mang thông tin về tính biến thiên giữa các điểm dữ liệu trong x_m. Nói cách khác, giải pháp tối đa hóa khả năng của dữ liệu huấn luyện {x^(n)_m}_{n=S_c} với một phân phối Gaussian duy nhất. Có nghĩa là, tham số trung bình tối thiểu hóa khoảng cách đến tất cả các điểm dữ liệu từ phương thức m đồng thời: mô hình nắm bắt trung bình của phân phối mục tiêu – không phải tính biến thiên của nó.

3 Thí nghiệm

Chúng tôi tạo một bộ dữ liệu tổng hợp (lấy cảm hứng từ Johnson et al. [2016]) với phương thức x_1 ∈ R^2 và phương thức nhãn x_2 ∈ {0,1}. Chúng tôi triển khai MVAE [Wu và Goodman, 2018] và MMVAE [Shi et al., 2019]. Các phân phối tiềm ẩn là Gaussian đẳng hướng. Các phân phối tạo sinh là Gaussian đẳng hướng cho phương thức đầu tiên và phân loại cho phương thức thứ hai.

Đối với MMVAE, Hình 2 hỗ trợ lập luận của chúng tôi rằng các mẫu cho phương thức đầu tiên có xu hướng về trung bình của các điểm dữ liệu quan sát được (cho cùng một lớp). MVAE không gặp phải vấn đề này, có thể vì hàm mục tiêu của MVAE không chứa yếu tố p(x_1|x_2) (Phụ lục A).

Phụ lục B trực quan hóa các không gian tiềm ẩn, là hai chiều để tránh có thể che khuất từ các kỹ thuật giảm chiều.

4 Kết luận

Chúng tôi chỉ ra rằng các VAE đa phương thức với phân phối hậu nghiệm hỗn hợp có thể gặp khó khăn trong việc nắm bắt tính không đồng nhất trong dữ liệu toàn ánh. Phát hiện này ngụ ý rằng các nhà thực hành nên xem xét cẩn thận loại dữ liệu khi huấn luyện các mô hình như vậy: ví dụ, tăng cường dữ liệu có thể không có lợi vì thủ tục này thường thúc đẩy tính toàn ánh. Nghiên cứu tương lai có thể điều tra các giải pháp có thể, ví dụ, bằng cách xem xét các mô hình không tối đa hóa p(x_m|x_{M≠m}) một cách rõ ràng. Sẽ thú vị khi phân tích cách một giải pháp như vậy ảnh hưởng đến tính robust.

Lời cảm ơn
SN được hỗ trợ bởi Bộ Giáo dục và Nghiên cứu Đức như BIFOLD - Berlin Institute for the Foundations of Learning and Data (tham chiếu 01IS18025A và tham chiếu 01IS18037A). RGK được hỗ trợ bởi một khoản tài trợ từ SAP Corporation.

--- TRANG 3 ---
Hình 2: Các mẫu được tạo cho phương thức đầu tiên. Trái: sử dụng các mẫu từ p(g). Phải: sử dụng các mẫu từ q(g|x_2), trong đó x_2 là nhãn lớp (vàng hoặc xanh lá).

--- TRANG 4 ---
Tài liệu tham khảo

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE conference on computer vision and pattern recognition, trang 248–255. Ieee, 2009.

M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, và S. R. Datta. Composing graphical models with neural networks for structured representations and fast inference. Trong Advances in neural information processing systems, trang 2946–2954, 2016.

A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.

Z. Liu, P. Luo, X. Wang, và X. Tang. Deep learning face attributes in the wild. Trong Proceedings of International Conference on Computer Vision (ICCV), tháng 12 năm 2015.

M.-E. Nilsback và A. Zisserman. Automated flower classification over a large number of classes. Trong 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, trang 722–729. IEEE, 2008.

Y. Shi, N. Siddharth, B. Paige, và P. Torr. Variational mixture-of-experts autoencoders for multi-modal deep generative models. Trong Advances in Neural Information Processing Systems, trang 15692–15703, 2019.

M. Suzuki, K. Nakayama, và Y. Matsuo. Joint multimodal learning with deep generative models. arXiv preprint arXiv:1611.01891, 2016.

C. Wah, S. Branson, P. Welinder, P. Perona, và S. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.

M. Wu và N. Goodman. Multimodal generative models for scalable weakly-supervised learning. Trong Advances in Neural Information Processing Systems, trang 5575–5585, 2018.

H. Xiao, K. Rasul, và R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

--- TRANG 5 ---
A Định lý 1 không áp dụng cho MVAE

MVAE [Wu và Goodman, 2018] sử dụng một phân phối hậu nghiệm tích được cảm hứng từ phân phối hậu nghiệm thực:
q(g|{x_m}^M_{m=1}) ∝ p(g)∏^M_{m=1} q(g|x_m): (8)

Trong các thí nghiệm của chúng tôi từ §3, chúng tôi theo Wu và Goodman [2018] và tối đa hóa ba ELBO sau:
L(Ψ,Φ;X) := ELBO(x_1,x_2) + ELBO(x_1) + ELBO(x_2) (9)

ELBO cho M phương thức được định nghĩa là:
ELBO({x_m}^M_{m=1}) := E_{q(g|{x_m}^M_{m=1})}[log p(g)/q(g|{x_m}^M_{m=1})] + ∑^M_{m=1} E_{q(g|{x_m}^M_{m=1})}[log p(x_m|g)] ≤ log p({x_m}^M_{m=1}); (10)

Do đó, p(x_m|g) luôn được điều kiện hóa trên x_m qua phân phối quan trọng, tức là mô hình học p(x_m|{x_i}^M_{i=1}) hoặc p(x_m|x_m). Điều này ngụ ý rằng MVAE không tối ưu hóa p(x_{m≠M}|x_M) một cách rõ ràng cho bất kỳ m ≠ M nào, tức là Định lý 1 không áp dụng cho MVAE.

B Kết quả thí nghiệm bổ sung

Hình 3: Phân phối hậu nghiệm biên trên biến tiềm ẩn g.

Giải pháp q(g|x_1) = q(g|x_2) có thể hữu ích vì nó ngụ ý rằng các mẫu từ một trong hai phân phối hậu nghiệm tạo ra cùng một phân phối tạo sinh cho bất kỳ phương thức nào. Hình 3 chỉ ra rằng MVAE căn chỉnh các phân phối hậu nghiệm biên này tốt hơn MMVAE, điều này có thể giải thích khả năng tạo sinh tốt hơn của MVAE trong Hình 2. Hình 2 tiếp tục cho thấy rằng ngay cả MVAE cũng gặp khó khăn trong việc biểu diễn dữ liệu một cách hoàn hảo. Các biểu diễn tiềm ẩn của nó từ Hình 3 tiết lộ rằng mô hình tạo ra một số chồng chéo giữa các đa tạp lớp của các phân phối hậu nghiệm biên cho phương thức thứ hai – có thể trong nỗ lực khớp với tiên nghiệm Gaussian đẳng hướng p(g). Chúng tôi giả định rằng khó khăn này được gây ra bởi thực tế là chỉ có hai điểm dữ liệu nhãn duy nhất.
