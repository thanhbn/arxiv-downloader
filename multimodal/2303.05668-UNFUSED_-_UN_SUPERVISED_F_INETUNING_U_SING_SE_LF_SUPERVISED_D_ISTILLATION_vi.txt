# 2303.05668.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2303.05668.pdf
# Kích thước tệp: 697261 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
UNFUSED: TINH CHỈNH KHÔNG GIÁM SÁT SỬ DỤNG CHƯNG CẤT TỰ GIÁM SÁT

Ashish Seth2?, Sreyan Ghosh1?, S. Umesh2, Dinesh Manocha1
1Đại học Maryland, College Park, Hoa Kỳ
2Phòng thí nghiệm Âm thanh, Khoa Kỹ thuật Điện, IIT Madras, Chennai, Ấn Độ

TÓM TẮT
Trong bài báo này, chúng tôi giới thiệu UnFuSeD, một phương pháp mới để tận dụng học tự giám sát và giảm nhu cầu về lượng lớn dữ liệu có nhãn cho phân loại âm thanh. Khác với các nghiên cứu trước đây, trực tiếp tinh chỉnh bộ mã hóa được huấn luyện trước tự giám sát trên tập dữ liệu đích, chúng tôi sử dụng bộ mã hóa để tạo nhãn giả cho việc tinh chỉnh không giám sát trước bước tinh chỉnh thực tế. Đầu tiên, chúng tôi huấn luyện một bộ mã hóa sử dụng thuật toán học tự giám sát mới (SSL) trên tập dữ liệu âm thanh không có nhãn. Sau đó, chúng tôi sử dụng bộ mã hóa đó để tạo nhãn giả trên tập dữ liệu nhiệm vụ đích thông qua phân cụm các biểu diễn được trích xuất. Những nhãn giả này sau đó được sử dụng để hướng dẫn tự chưng cất trên một mô hình được khởi tạo ngẫu nhiên, mà chúng tôi gọi là tinh chỉnh không giám sát. Cuối cùng, bộ mã hóa kết quả được tinh chỉnh trên tập dữ liệu nhiệm vụ đích của chúng tôi. Thông qua UnFuSeD, chúng tôi đề xuất hệ thống đầu tiên thoát khỏi các mô hình SSL chung trong tài liệu, vốn huấn luyện trước và tinh chỉnh cùng một bộ mã hóa, và trình bày một hệ thống dựa trên tự chưng cất mới để tận dụng huấn luyện trước SSL cho phân loại âm thanh tài nguyên thấp. Trong thực tế, UnFuSeD đạt được kết quả tốt nhất hiện tại trên Bộ tiêu chuẩn LAPE, vượt trội đáng kể so với tất cả các phương pháp cơ sở của chúng tôi. Ngoài ra, UnFuSeD cho phép chúng tôi đạt được điều này với việc giảm 40% số lượng tham số so với hệ thống tốt nhất hiện tại trên LAPE. Chúng tôi công khai tất cả mã nguồn của mình1.

Thuật ngữ chỉ mục —âm thanh, tiếng nói, tự giám sát

1. GIỚI THIỆU
Học Tự Giám Sát (SSL) đã được chứng minh là một trong những thành công lớn nhất của thập kỷ qua, cho phép các mô hình học sâu học được các biểu diễn hữu ích trong điều kiện dữ liệu có nhãn tài nguyên thấp. SSL đã được áp dụng thành công trong tiếng nói [1], thị giác [2, 3], và văn bản [4] vượt trội so với tất cả các nghiên cứu trước đây được huấn luyện chỉ với giám sát dữ liệu có nhãn trên một số tập dữ liệu chuẩn [5]. Mặc dù các mô hình SSL hiện tại được huấn luyện trước bằng Mô hình Âm thanh Che dấu (MAM) đã được chứng minh là khái quát hóa tốt trên các nhiệm vụ tiếng nói như Nhận dạng Giọng nói Tự động (ASR), Nhận dạng Âm vị (PR), v.v., chúng lại không thể thực hiện tốt trên các nhiệm vụ không phải tiếng nói như phân loại cảnh âm thanh [6]. Chúng tôi liệt kê một số lý do có thể cho hiện tượng này trong Phần 2. Do đó, chúng tôi nhấn mạnh tầm quan trọng của việc học các biểu diễn âm thanh đa mục đích có thể khái quát hóa trên cả nhiệm vụ tiếng nói và không phải tiếng nói, điều này hiện tại chưa được nghiên cứu nhiều trong tài liệu so với SSL trong tiếng nói sử dụng MAM.

Trong thời gian gần đây, các nhà nghiên cứu đã đề xuất các thuật toán mới để học các biểu diễn âm thanh đa mục đích [7, 8, 9]. Một đặc điểm chung của tất cả các hệ thống này là chúng trực tiếp tinh chỉnh mô hình sau huấn luyện trước SSL. Tuy nhiên, phương pháp tinh chỉnh trực tiếp này có thể dẫn đến hiệu suất tối ưu phụ do sự khác biệt đáng kể giữa các miền huấn luyện trước và tinh chỉnh [10]. Ví dụ, hầu hết các hệ thống này thực hiện huấn luyện trước SSL trên AudioSet [11] (âm thanh hàng ngày như tiếng bàn chải đánh răng) và đánh giá các biểu diễn đã học trên các nhiệm vụ như Xác minh Người nói [12] (các phát âm được con người nói). Ngoài ra, dưới thiết lập đánh giá tuyến tính, chúng tôi cho rằng các nhiệm vụ xuôi dòng không thể tận dụng các biểu diễn SSL một cách đầy đủ do khả năng học của chúng bị hạn chế bởi một phép biến đổi affine.

Đóng góp chính: Chúng tôi trình bày UnFuSeD, một khuôn khổ mới để cải thiện hiệu suất phân loại âm thanh xuôi dòng trong điều kiện dữ liệu có nhãn tài nguyên thấp sử dụng SSL. Khác với tất cả các hệ thống trước đây trong tài liệu, UnFuSeD không trực tiếp tinh chỉnh mô hình được huấn luyện trước SSL mà sử dụng nó để trích xuất và phân cụm các đặc trưng âm thanh để tạo nhãn giả trên tập dữ liệu nhiệm vụ xuôi dòng, sau đó được sử dụng để thực hiện tinh chỉnh không giám sát. Cụ thể hơn, chúng tôi thực hiện một bước tự chưng cất, được hướng dẫn bởi các nhãn giả được tạo, trên bộ mã hóa convnet được khởi tạo ngẫu nhiên, được chia thành bộ mã hóa học sinh và giáo viên. Cuối cùng, sau tinh chỉnh không giám sát, chúng tôi thực hiện tinh chỉnh có giám sát và đánh giá hiệu suất nhiệm vụ xuôi dòng trên thiết lập đánh giá tuyến tính của mô hình. Ngoài ra, để huấn luyện trước bộ mã hóa của chúng tôi bằng SSL, chúng tôi đề xuất một thuật toán SSL mới bằng cách sửa đổi từ DECAR [13]. Hình 1 cho thấy một biểu diễn hình ảnh rõ ràng về quá trình huấn luyện hoàn chỉnh của chúng tôi. Chúng tôi nhấn mạnh rằng UnFuSeD thay đổi mô hình mà SSL được tận dụng để giải quyết tình trạng thiếu hụt dữ liệu và cải thiện hiệu suất nhiệm vụ xuôi dòng. Trong thực tế, UnFuSeD đạt được hiệu suất tốt nhất hiện tại (SOTA) trên Bộ tiêu chuẩn LAPE [7] với một bộ mã hóa có ít hơn 40% tham số so với mô hình SOTA hiện tại trên LAPE.

--- TRANG 2 ---
[Hình 1. Minh họa UnFuSeD: UnFuSeD tuân theo quy trình huấn luyện 3 bước từ huấn luyện trước SSL ngược dòng đến tinh chỉnh cụ thể nhiệm vụ xuôi dòng. 1 Huấn luyện trước SSL. Đầu tiên chúng tôi huấn luyện trước một convnet sử dụng âm thanh không có nhãn bằng DECAR-v2 (mô tả trong Phần 3). 2 Tinh chỉnh Không giám sát. Bây giờ chúng tôi truyền dữ liệu cụ thể nhiệm vụ xuôi dòng qua mô hình ngược dòng được huấn luyện trước ở giai đoạn cuối và trích xuất và phân cụm các biểu diễn này để tạo nhãn giả. Những nhãn giả này sau đó được sử dụng để thực hiện tinh chỉnh không giám sát trên một convnet được khởi tạo ngẫu nhiên. 3 Đánh giá Tuyến tính. Cuối cùng, một đầu tuyến tính cụ thể nhiệm vụ được thêm vào convnet thu được từ bước trước, và chúng tôi thực hiện tinh chỉnh có giám sát trên tập dữ liệu có nhãn cụ thể nhiệm vụ giữ cho convnet bị đóng băng.]

2. NGHIÊN CỨU LIÊN QUAN
Học Tự Giám Sát trong Tiếng nói và Âm thanh. Thập kỷ qua đã chứng kiến thành công lớn trong học tự giám sát trong thị giác (CV), tiếng nói (SLP), và văn bản (NLP), đẩy ranh giới của học biểu diễn tài nguyên thấp cho phân loại xuôi dòng [1, 4]. Các hệ thống phổ biến nhất cho SSL với tiếng nói giải quyết nhiệm vụ Mô hình Âm thanh Che dấu (MAM), sử dụng học đối lập [1], tái tạo khung [14], hoặc dự đoán nhãn giả [15]. Tuy nhiên, nghiên cứu gần đây đã chỉ ra rằng việc giải quyết MAM làm cho các biểu diễn mô hình bắt chước phản ứng phát âm của con người [16], do đó làm cho nó không phù hợp cho các nhiệm vụ không phải tiếng nói. Do đó, trong thời gian gần đây, các nhà nghiên cứu đã đề xuất các hệ thống mới để học các biểu diễn âm thanh có thể khái quát hóa trên cả nhiệm vụ tiếng nói và không phải tiếng nói. Theo SSL trong tiếng nói, các hệ thống này giải quyết hoặc nhiệm vụ phân biệt thể hiện dựa trên học đối lập [8], nhiệm vụ dự đoán nhãn giả dựa trên phân cụm [13], hoặc nhiệm vụ tái tạo [9]. Chưng cất tri thức đã cho thấy thành công lớn trong CV và NLP với các ứng dụng chính trong nén mô hình [17]. Trong một thiết lập có giám sát, các nhà nghiên cứu đã khám phá chưng cất tri thức cho nhận dạng giọng nói tự động, nhận dạng cảm xúc giọng nói, và xác minh người nói [18]. DistillHubert [19] là nghiên cứu đầu tiên về chưng cất các mô hình tiếng nói dựa trên SSL và thực hiện chưng cất tri thức theo lớp (KD) để nén một HuBERT đầy đủ [15]. Mặt khác, khi cả hai kiến trúc bộ mã hóa giống nhau, điều này được gọi là tự chưng cất (SD) [20], và đã cho thấy kết quả ấn tượng với học sinh thường vượt trội so với giáo viên gốc. Tuy nhiên, theo hiểu biết của chúng tôi, không có nghiên cứu nào tồn tại tận dụng SD để học biểu diễn âm thanh tự giám sát đa mục đích, và chúng tôi là người đầu tiên khám phá điều này thông qua UnFuSeD.

3. PHƯƠNG PHÁP LUẬN
Hình 1 minh họa thuật toán học UnFuSeD được đề xuất của chúng tôi. Thuật toán 1 cung cấp một cái nhìn tổng quan thuật toán chi tiết về cùng một điều. Trong thực tế, UnFuSeD có ba bước chính, cụ thể là, (1) Huấn luyện trước SSL Ngược dòng, (2) Tinh chỉnh Không giám sát, và (3) Tinh chỉnh Có giám sát Xuôi dòng. Trong các đoạn tiếp theo, chúng tôi mô tả chi tiết từng bước.

--- TRANG 3 ---
(1) Huấn luyện trước SSL Ngược dòng. Gọi Xpre là một tập dữ liệu không có nhãn với kích thước J trong đó Xpre = {x1, ..., xj, ..., xJ}. Trong trường hợp của chúng tôi, ở đây J = 0.25 triệu, tuân theo thiết lập huấn luyện trước chính xác được đề xuất bởi chuẩn LAPE. Mục tiêu chính của chúng tôi là học các biểu diễn âm thanh đa mục đích từ tập dữ liệu âm thanh không có nhãn này. Để đạt được điều này, chúng tôi sử dụng một kiến trúc dựa trên convnet đơn giản [21, 22], phổ biến trong các nghiên cứu trước đây [7, 9] để có một so sánh công bằng. Để huấn luyện trước SSL ngược dòng, chúng tôi đề xuất DECAR-v2, một phiên bản cải tiến của DECAR [13], dựa trên các phát hiện trong [23]. DECAR-v2 có hai bước hoặc giai đoạn chính: (a) Giai đoạn Phân công và (b) Giai đoạn Huấn luyện.

(a) Giai đoạn Phân công: Mục đích chính của giai đoạn này là thu được "nhãn giả" q cho mọi mẫu âm thanh không có nhãn x ∈ Xpre. Để đạt được điều này, đầu tiên chúng tôi lưu trữ tất cả các nhúng g̃x thu được từ đầu chiếu convnet hproj trong bộ nhớ cho toàn bộ Xpre. Sau đó, chúng tôi áp dụng Spherical K-means để phân cụm và nhận "nhãn giả" q cho mọi x như sau: minC∈R^(d×K) 1/N ∑(n=1 to N) minq g̃x^T Cq trong đó C là ma trận Trọng tâm. Cả g̃x và các cột của C đều được chuẩn hóa l2. K đại diện cho số lượng cụm, và x̃ là một phiên bản được tăng cường và lấy mẫu của mẫu âm thanh gốc. Ngoài ra, để ổn định huấn luyện ConvNet, chúng tôi giữ các tham số đầu nguyên mẫu hprot bị đóng băng trong suốt quá trình huấn luyện trước, và ở cuối mỗi giai đoạn phân công, các tham số của hprot được thay thế bởi C.

(b) Giai đoạn Huấn luyện: Chúng tôi huấn luyện mạng bằng cách sử dụng giám sát từ "nhãn giả" q thu được từ giai đoạn phân công. Để làm điều này, đầu tiên chúng tôi thu được dự đoán p bằng cách sử dụng softmax(z) trong đó z là đầu ra của hprot. Sau bước này; chúng tôi giảm thiểu tổn thất logistic đa thức giữa p và q với: ℓ(p,q) = ∑k q(k) log p(k). "Nhãn giả" được giữ cố định trong giai đoạn huấn luyện và được cập nhật cho toàn bộ X chỉ một lần mỗi epoch trong giai đoạn phân công. Tương tự như [23], giai đoạn phân công và giai đoạn huấn luyện diễn ra riêng biệt chỉ ở epoch đầu tiên, sau đó chúng tôi sử dụng các nhúng g̃x thu được từ epoch trước. Những nhúng này được lưu trữ trong bộ nhớ tại mỗi lần lặp của một epoch ngay sau bước lan truyền ngược.

(2) Tinh chỉnh Xuôi dòng Không giám sát. Sau huấn luyện trước SSL, chúng tôi không tinh chỉnh convnet được huấn luyện trước fpre trên tập dữ liệu nhiệm vụ đích trực tiếp mà thay vào đó, sử dụng nó để tinh chỉnh không giám sát trên một convnet được khởi tạo ngẫu nhiên fsd. Chúng tôi gọi bước này là tinh chỉnh không giám sát vì chúng tôi sử dụng tập dữ liệu nhiệm vụ đích nhưng không sử dụng nhãn thực tế của nó. Gọi Dtarg = {Xtarg, Ytarg} là tập dữ liệu có nhãn nhiệm vụ đích với kích thước I trong đó Ytarg là các nhãn liên kết với các mẫu âm thanh Xtarg. Để tinh chỉnh không giám sát, đầu tiên chúng tôi tạo Ypseduo bằng cách trích xuất và phân cụm các biểu diễn thu được khi truyền Xtarg qua fpre. DECAR-v2 tạo ra các nhúng có thể phân cụm, giúp tạo Ypseudo. Sau đó chúng tôi sử dụng Ypseudo để thực hiện tự chưng cất trên fsd. Đầu tiên chúng tôi chia fsd, tuân theo kiến trúc tương tự như fpre, thành một mạng học sinh (fs_sd) và giáo viên (ft_sd). fsd có 4 khối riêng biệt, trong đó 3 khối đầu tạo thành fs_sd và khối cuối tạo thành ft_sd. Để biết thêm chi tiết về kiến trúc của fs_sd, chúng tôi giới thiệu độc giả đến [7, 21]. Để tự chưng cất, chúng tôi coi mỗi khối bi là một bộ phân loại riêng biệt và thêm một phép biến đổi tuyến tính hi_proj vào bi để giải quyết ba tổn thất song song, KL-divergence Lkl, Sai số Bình phương Trung bình Lmse và Cross Entropy Lce. Lce đảm bảo rằng các khối học sinh phân loại chính xác các nhãn giả Ypseduo và do đó sử dụng kiến thức giám sát yếu ẩn trong chúng. Lmse đảm bảo rằng kiến thức của các lớp sâu nhất được tận dụng để cải thiện trích xuất đặc trưng trong các lớp nông. Lkl đảm bảo rằng kết quả phân loại của các bộ phân loại học sinh tương tự như của bộ phân loại giáo viên. Cuối cùng, để tối ưu hóa mạng của chúng tôi, chúng tôi sử dụng trung bình có trọng số của Lkl, Lmse và Lce, mà chúng tôi cân bằng bằng α, β như được hiển thị:

Lall = Lce + α∑(i=1 to 3)Li_ce + β∑(i=1 to 3)Li_kl + β∑(i=1 to 3)Li_mse (1)

Lce ≡ Lce(l, Ypseudo); Li_ce ≡ Li_ce(zi, Ypseudo);
Li_kl ≡ Li_kl(zi, l); Li_mse ≡ Li_mse(zi, fsd(Xtarg))

trong đó zi = hi_proj(fi_sd(Xtarg)); l = hcl(fsd(Xtarg))

(3) Tinh chỉnh Có giám sát Xuôi dòng Sau tinh chỉnh xuôi dòng không giám sát, chúng tôi thực hiện tinh chỉnh xuôi dòng có giám sát

--- TRANG 4 ---
[Bảng 1. So sánh kết quả của các phương pháp SSL khác nhau với phương pháp đề xuất DECAR-v2 và UnFuSeD trên thiết lập đánh giá tuyến tính với bộ mã hóa bị đóng băng. Kết quả tốt nhất cho mỗi nhiệm vụ được trình bày in đậm. UnFuSeD vượt trội so với tất cả các phương pháp cơ sở của chúng tôi.]

trên mô hình học sinh fsd sử dụng Dtarg. Để có một so sánh công bằng với các nghiên cứu trước đây trong lĩnh vực này, chúng tôi không huấn luyện tất cả các lớp của mô hình mà thay vào đó chỉ huấn luyện một đầu tuyến tính cụ thể nhiệm vụ được thêm vào bộ mã hóa. Phương pháp huấn luyện này còn được gọi là đánh giá tuyến tính và được chứng minh là một kỹ thuật hiệu quả để đánh giá các biểu diễn âm thanh đã học.

4. THIẾT LẬP THỰC NGHIỆM
Tập dữ liệu. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng chính xác cùng một thiết lập huấn luyện ngược dòng và xuôi dòng được đề xuất bởi LAPE [7]. Để huấn luyện trước dựa trên SSL, chúng tôi sử dụng một tập con cân bằng 10% của AudioSet hoàn chỉnh (0.2 triệu) và FSD50K [24]. Đối với các nhiệm vụ xuôi dòng (DT), chúng tôi đánh giá các biểu diễn đã học trên LibriSpeech (LBS) [25] và VoxCeleb (VC) [26] để nhận dạng người nói, Speech Commands (SC) v1 và v2 [27] để phát hiện từ khóa, VoxForge (VF) [12] để nhận dạng ngôn ngữ, IEMOCAP (IC) [28] để nhận dạng cảm xúc giọng nói, NSynth [29] cho TUT Urban [6] và US8K [30] để phân loại sự kiện âm thanh và cuối cùng là Bird Song Detection (BSD) [31].

Điều chỉnh Siêu tham số. Để Huấn luyện trước SSL (DECAR-v2), chúng tôi tìm các giá trị tối ưu cho số lượng cụm là 512, tốc độ học là 0.005, kích thước lô là 512, và số epoch là 100. Projector hproj thực hiện một phép biến đổi phi tuyến R^2048 → R^512 sử dụng nhiều lớp tuyến tính. Để Tinh chỉnh Không giám sát, chúng tôi sử dụng tốc độ học là 0.007, kích thước lô là 512, số epoch là 50, α là 0.7, và β là 0.003. hcl thực hiện một phép biến đổi tuyến tính R^2048 → R^t, trong đó t là số lớp trong tập dữ liệu đích. Projectors h1_proj, h2_proj và h3_proj thực hiện các phép biến đổi phi tuyến R^2048 → R^t, R^1024 → R^t và R^512 → R^t tương ứng. Cuối cùng, để Đánh giá Tuyến tính, chúng tôi sử dụng tốc độ học là 0.001, kích thước lô là 32, và số epoch là 50. Tất cả các lựa chọn siêu tham số được đưa ra dựa trên tìm kiếm lưới mở rộng trong khi xem xét hiệu suất trung bình trên tất cả các nhiệm vụ xuôi dòng.

5. KẾT QUẢ VÀ PHÂN TÍCH KẾT QUẢ
Như rõ ràng từ Bảng 1, UnFuSeD vượt trội so với tất cả các phương pháp khác trong tài liệu với một khoảng cách đáng kể. Kết quả của BYOL-A được mượn từ các bài báo gốc của họ. SimCLR được đề xuất như phương pháp huấn luyện trước trong COLA [32] và được lặp lại trên bộ mã hóa convnet của chúng tôi sử dụng thiết lập tập dữ liệu ngược dòng LAPE. Chúng tôi giả thuyết rằng khoảng cách trong kết quả từ bài báo gốc có thể do sử dụng bộ mã hóa mạnh mẽ và gấp 10 lần dữ liệu từ AudioSet được sử dụng trong bài báo. Đo lường tác động của thay đổi bộ mã hóa nằm ngoài phạm vi của bài báo này. DECAR-v2 được đề xuất của chúng tôi vượt trội so với DECAR-v1 đã được đề xuất với khoảng cách 4.6% (trung bình trên tất cả các nhiệm vụ). Ngoài ra, UnFuSeD vượt trội so với DECAR-v2 với khoảng cách 5.8% (trung bình trên tất cả các nhiệm vụ). Do hạn chế về không gian, chúng tôi cung cấp kết quả của UnFuSeD với các khuôn khổ huấn luyện SSL khác nhau trên GitHub của chúng tôi. Ngoài ra, bộ mã hóa convnet cuối cùng fs_sd của chúng tôi được sử dụng để đánh giá nhiệm vụ xuôi dòng có ít hơn 40% tham số so với DeLoRes-M [7] (hệ thống SOTA hiện tại trên Chuẩn LAPE).

6. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất UnFuSeD, một phương pháp luận mới để tận dụng SSL cho phân loại âm thanh tài nguyên thấp. Trong thực tế, UnFuSeD vượt trội đáng kể so với tất cả các phương pháp khác trong tài liệu trên chuẩn đánh giá âm thanh LAPE. Ngoài ra, chúng tôi đề xuất một thuật toán SSL mới gọi là DECAR-v2 để học các biểu diễn âm thanh đa mục đích từ dữ liệu không có nhãn.

--- TRANG 5 ---
7. TÀI LIỆU THAM KHẢO
[1] Baevski và cộng sự, "wav2vec 2.0: A framework for self-supervised learning of speech representations," NeurIPS 2020, tập 33, tr. 12449–12460.
[2] Grill và cộng sự, "Bootstrap your own latent-a new approach to self-supervised learning," NeurIPS 2020, tập 33, tr. 21271–21284.
[3] He và cộng sự, "Momentum contrast for unsupervised visual representation learning," trong IEEE CVPR 2020.
[4] Devlin và cộng sự, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[5] Yang và cộng sự, "Superb: Speech processing universal performance benchmark," arXiv preprint arXiv:2105.01051, 2021.
[6] Annamaria Mesaros, Toni Heittola, và Tuomas Virtanen, "A multi-device dataset for urban acoustic scene classification," 2018.
[7] Ghosh và cộng sự, "Decorrelating feature spaces for learning general-purpose audio representations," IEEE Journal of Selected Topics in Signal Processing, tr. 1–13, 2022.
[8] Saeed và cộng sự, "Contrastive learning of general-purpose audio representations," trong IEEE ICASSP 2021, tr. 3875–3879.
[9] Niizumi và cộng sự, "Byol for audio: Self-supervised learning for general-purpose audio representation," trong IEEE IJCNN 2021, tr. 1–8.
[10] Lee và cộng sự, "Self-distillation for further pre-training of transformers," arXiv preprint arXiv:2210.02871, 2022.
[11] Gemmeke và cộng sự, "Audio set: An ontology and human-labeled dataset for audio events," trong IEEE ICASSP 2022. IEEE, 2017, tr. 776–780.
[12] Voxforge.org, "Free speech... recognition (linux, windows and mac) - voxforge.org," truy cập 06/25/2014.
[13] Ghosh và cộng sự, "Deep clustering for general-purpose audio representations," arXiv preprint arXiv:2110.08895, 2021.
[14] Liu và cộng sự, "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders," trong IEEE ICASSP 2020, tr. 6419–6423.
[15] Hsu và cộng sự, "Hubert: Self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM TASLP, tập 29, tr. 3451–3460, 2021.
[16] Wu và cộng sự, "Speaker-independent acoustic-to-articulatory speech inversion," arXiv preprint arXiv:2302.06774, 2023.
[17] Jianping Gou, Baosheng Yu, Stephen J. Maybank, và Dacheng Tao, "Knowledge distillation: A survey," tập 129, số 6, tr. 1789–1819, tháng 6 2021.
[18] Liu và cộng sự, "Self-knowledge distillation via feature enhancement for speaker verification," trong IEEE ICASSP 2022.
[19] Chang và cộng sự, "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert," trong IEEE ICASSP 2022.
[20] Pham và cộng sự, "Revisiting self-distillation," arXiv preprint arXiv:2206.08491, 2022.
[21] Koizumi và cộng sự, "The ntt dcase2020 challenge task 6 system: Automated audio captioning with keywords and sentence length estimation," arXiv preprint arXiv:2007.00225, 2020.
[22] Takeuchi và cộng sự, "Effects of word-frequency based pre-and post-processings for audio captioning," arXiv preprint arXiv:2009.11436, 2020.
[23] Caron và cộng sự, "Unsupervised learning of visual features by contrasting cluster assignments," NeurIPS 2022.
[24] Fonseca và cộng sự, "Fsd50k: an open dataset of human-labeled sound events," IEEE/ACM TASLP, tập 30, tr. 829–852, 2021.
[25] Panayotov và cộng sự, "Librispeech: An asr corpus based on public domain audio books," trong IEEE ICASSP 2015, tr. 5206–5210.
[26] Nagrani và cộng sự, "Voxceleb: A large-scale speaker identification dataset," ISCA Interspeech 2017.
[27] Pete Warden, "Speech commands: A dataset for limited-vocabulary speech recognition," 2018.
[28] Busso và cộng sự, "Iemocap: Interactive emotional dyadic motion capture database," LREC 2008, tập 42, số 4, tr. 335–359.
[29] Engel và cộng sự, "Neural audio synthesis of musical notes with wavenet autoencoders," trong ICML 2017.
[30] Justin Salamon, Christopher Jacoby, và Juan Pablo Bello, "A dataset and taxonomy for urban sound research," trong ACM MM 2014, 2014, tr. 1041–1044.
[31] Stowell và cộng sự, "Automatic acoustic detection of birds through deep learning: the first bird audio detection challenge," Methods in Ecology and Evolution 2019.
[32] Wang và cộng sự, "Towards learning universal audio representations," trong IEEE ICASSP 2022, tr. 4593–4597.
