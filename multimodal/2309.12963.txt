# 2309.12963.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.12963.pdf
# File size: 195038 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2309.12963v1  [eess.AS]  22 Sep 2023Massive End-to-end Models for Short Search Queries
Weiran Wang Rohit Prabhavalkar Dongseong Hwang Qiujia Li
Khe Chai Sim Bo Li James Qin Xingyu Cai Adam Stooke
Zhong Meng CJ Zheng Yanzhang He Tara Sainath
Pedro Moreno Mengibar
Google LLC
{weiranwang,prabhavalkar,dongseong }@google.com
September 25, 2023
Abstract
In this work, we investigate two popular end-to-end automat ic speech recognition (ASR)
models, namely Connectionist Temporal Classiﬁcation (CTC ) and RNN-Transducer (RNN-T),
for oﬄine recognition of voice search queries, with up to 2B m odel parameters. The encoders
of our models use the neural architecture of Google’s univer sal speech model (USM), with
additional funnel pooling layers to signiﬁcantly reduce th e frame rate and speed up training
and inference. We perform extensive studies on vocabulary s ize, time reduction strategy, and
its generalization performance on long-form test sets. Des pite the speculation that, as the
model size increases, CTC can be as good as RNN-T which builds label dependency into
the prediction, we observe that a 900M RNN-T clearly outperf orms a 1.8B CTC and is more
tolerant tosevere time reduction, although the WER gap can b e largely removedbyLM shallow
fusion.
1 Introduction
There has been a recent focus on scaling up end-to-end ASR models , such as connectionist temporal
classiﬁcation (CTC) [1] and neural transducers (RNN-T, [2]), to ex tremely large sizes [3, 4, 5, 6, 7]
to explore the beneﬁts of such models at scale. Many of these mode ls are motivated by the goal of
training a single multilingual ASR model which can perform well across a range of languages. For
example, the Universal Speech Model (USM, [6]) is a 2B parameter f ull-context CTC model trained
on YouTube data from 300+ languages; Pratap et al. [7] train ASR sy stems on 1000+ languages.
The present work follows in the footsteps of these previous works . As large language models (LLMs)
have rapidly risen in popularity in the ﬁeld of natural language proces sing [8, 9, 10], it is natural to
ask how ASR models operate at large sizes. Speciﬁcally, how do the tw o main model classes – CTC
and RNN-T – compare at large model sizes? In particular, how do the se models compare on a task
(voice search, in this work) where paired training data is plentiful, fo r a single language (English)
instead of in a multilingual setup.
A related, and important practical question relates to the diﬃculty of training massive end-to-
end models as model size increases. Since the cost of computing the outputs of the encoders in
1

--- PAGE 2 ---
the model (most of the model parameters are typically in the encod er) scales poorly in attention-
based encoders such as transformers [11] and conformers [12], w hich can be prohibitive in terms of
computation and memory. The RNN-T model, in particular, requires a dditional computation and
memory for the prediction and joint network, which further compo unds the problem.
In this work, we investigate the aforementioned issues and ﬁnd tha t the two issues – how CTC
and RNN-T compare, and how one can train them eﬃciently – are not a s independent as might
seem initially: we ﬁnd that encoder output frame rate reduction can be applied repeatedly at
multiple layers in the encoder to obtain a large output frame rate red uction; this in turn is critical
for training models eﬃciently. While past research incorporated time reduction to lower the ﬁnal
frame rate to 60ms for voice search [13, 14], we ﬁnd that we can incr ease time reduction to 6x with
large CTC, and all the way up to 16x with large RNN-T, of the 40ms bas e frame rate with funnel
pooling [15], without sacriﬁcing much accuracy.
Additionally, we compare CTC and RNN-T as the backbone ASR system . CTC produces label
probabilities that are independent of previous time frames, and is th us much faster at inference
compared to RNN-T. This is one reason why CTC was preferred for U SM in [6]. However, we
ﬁnd that RNN-T is actually more accurate than CTC, and speciﬁcally a 900M RNN-T clearly
outperforms a 1.8B CTC, even though the CTC model beneﬁts signiﬁ cantly from increased model
size compared with a 340M CTC. Moreover, we ﬁnd that RNN-T can to lerate a much larger time
reduction factor. In order to address any concerns with RNN-T b eing slower than CTC, we run
both models (including beam search) on a TPU to achieve the best pos sible latency.
Finally, with the much smaller number of output frames and on-TPU be am search, shallow
fusion [16, 17] with a neural LM becomes a viable candidate for enhan cing model accuracy. We
fuse both CTC and RNN-T with external LMs trained with large amoun ts of text, and observe
that the word error rate (WER) gap between them can be largely re moved by LM fusion. Our
study on shallow fusion is timely, as the community starts to become in terested in the use of LLMs
for ASR. We show that shallow fusion remains an eﬀective technique f or LM integration, even for
large models trained with hundreds of thousands of hours of audio, and should be considered as a
baseline for more advanced techniques.
2 End-to-end ASR Models
In this section, we brieﬂy describe the end-to-end models employed in this work – connectionist
temporal classiﬁcation (CTC) [1], and neural transducers [2] – par ticularly, the hybrid autoregressive
transducer (HAT) [18]. Since end-to-end models are now part of th e mainstream in automatic
speech recognition (ASR), we opt for brevity; interested reader s can ﬁnd more information in
recent overview articles [19, 20].
Notations We assume that the input audio signal has been parameterized into s uitable acoustic
feature vectors: x= [x1,...,xT′], of length T′, wherext∈Rd(128-dimensional log-mel features, in
this work). The input acoustic features are processed using an encoder , a suitable neural network
(a Conformer [12], in this work), which transforms the input into a hig her-level representation:
h= [h1,...,hT], where the length of the encoded representation is typically short er than the
original input length ( T≤T′). We assume that we have an input transcript corresponding to
each utterance: y= [y0=/an}bracketle{ts/an}bracketri}ht,y1,...,y U], where each yu∈ V, the set of output symbols (word-
pieces [21], in this work), and /an}bracketle{ts/an}bracketri}htrepresents a special start-of-sentence symbol.
2

--- PAGE 3 ---
2.1 Connectionist Temporal Classiﬁcation
Connectionist Temporal Classiﬁcation (CTC) was introduced by Gra ves et al. [1], as a way to train
sequence-to-sequence models which can transduce the input seq uencex, into the output sequence y
when the alignments between the two sequences are unknown. CTC accomplishes this by modeling
the conditional distribution, P(y|x), by marginalizing over all possible alignment paths between
the two sequences:
P(y|x) =P(y|h(x)) =/summationdisplay
a∈Bctc(y)T/productdisplay
t=1P(at|h) (1)
where, the B ctc(y) corresponds to the set of all valid alignments. Speciﬁcally, an alignm enta∈
Bctc(y) is a valid alignment if it contains |h(x)|=Tsymbols from the set of outputs augmented
with a special blank symbol – V ∪ {/an}bracketle{t b/an}bracketri}ht}; and if additionally, removing consecutive repeated non-
blank symbols and then removing all /an}bracketle{tb/an}bracketri}htsymbols produces the original label sequence y. As can
be seen in (1), CTC models make a strong conditional independence a ssumption – that the output
labels are conditionally independent given the input acoustic encoded features. However, these
models work well in practice [22, 23], especially with large encoders [6]. F inally, as can be seen
in (1), the CTC model produces one output symbol (blank, or non- blank) per encoder time step.
2.2 Neural Transducers
The recurrent neural network transducer (RNN-T) was propos ed by Graves et al. [2], as an im-
provement over the CTC model, to reduce conditional independenc e assumptions in the model.
This is achieved by introducing a separate prediction network , which models label dependency (and
thus makes model outputs conditionally dependent on the sequenc e of previous predictions).
P(y|x) =P(y|h(x)) =/summationdisplay
a∈Bnt(y)T+U/productdisplay
τ=1P(aτ|qiτ,hτ−iτ)
where, B nt(y) is the set of all valid alignments – sequences of length T+U, which are identical to y
after removing all blanks (i.e., each sequence has exactly T blank sym bols);iτrepresents the number
of non-blank labels in the partial alignment a1,...,a τ−1; and,qjrepresents the output of the
prediction network after processing the ﬁrst j−1 output labels: qj= PredNetwork( yj−1,...,y 0).
While early works employed recurrent LSTM networks to model the p rediction network, in this
work, the prediction network is modeled as |V|2embedding network [24], whose output only depends
on the last two non-blank labels yj−1, andyj−2. Notably, neural transducers allow the model to
output multiple (non-blank) output labels at each frame; blanks cor respond to transitions to the
next input encoder frame.
2.2.1 Hybrid Autoregressive Transducer
The hybrid autoregressive transducer (HAT) model proposed by Variani et al. [18] improves over
the basic neural transducer structure in two ways. First, it fact orizes the output distribution
P(aτ|qiτ,hτ−iτ) into two separate distributions: a Bernoulli distribution to model b lank symbols
(i.e., ifaτ=/an}bracketle{tb/an}bracketri}ht), and a separate probability distribution for the non-blank labels. S econd, the
model proposes the notion of an internal language model (ILM), PILM(y), which represents the LM
learned by the model based on the training data; the ILM can thus b e subtracted out from the
posterior distribution, before fusion with an external LM, PEXT(y), during decoding, by adding
3

--- PAGE 4 ---
tunable hyperparameters α, andβwhich can be set based on a development set to produce the
most likely hypothesis [25, 26, 27, 28]:
y∗= arg max
ylogP(y|x)−αlogPILM(y) +βlogPEXT(y)
3 Model architecture
3.1 USM Conformer
The USM uses the convolution-augmented Transformer [12], or Con former architecture. Each
Conformer block consists of a feed-forward module ( FFN ), a multi-head self-attention module
(MHSA ), a convolution module ( Conv ), and a second feed-forward module. If the input to l-th
Conformer block is x(l), then its output (and, the input to the next block), x(l+1), is computed as:
˜x(l)=x(l)+1
2FFN1/parenleftbig
x(l)/parenrightbig
x′(l)=˜x(l)+MHSA/parenleftbig
Wq˜x(l),Wk˜x(l),Wv˜x(l)/parenrightbig
(2)
x′′(l)=x′(l)+Conv/parenleftbig
x′(l)/parenrightbig
x(l+1)=LayerNorm/parenleftbig
x′′(l)+1
2FFN2(x′′(l))/parenrightbig
For the standard conformer layer, the input length and output len gth are the same.
3.2 Funnel Pooling
We improve training and inference speed in our models by reducing the encoder embedding sequence
length relative to the audio length. At selected Conformer blocks wit hin the encoder, we employ
the pooling technique in the MHSA module introduced in Funnel-Transformers [15]. At a funnel
self-attention layer, the entire input is used to produce the key an d value sequences as usual, but
strided-average pooling is applied to the time dimension of the input us ed in producing the query
vectors. That is, we replace the MHSA module in (2) by (3) below.
ˆx(l)=StridedPooling (˜x(l)),
x′(l)=ˆx(l)+MHSA/parenleftbig
Wqˆx(l),Wk˜x(l),Wv˜x(l)/parenrightbig
.(3)
The stride of pooling in (3) is the time reduction factor in this layer. In a language model, this
pool-query-only method was shown to provide a slight advantage ov er simply pooling the entire
hidden embedding sequence between layers [15]; likely, the higher gra nularity in the key and value
sequences allows the network to learn a less lossy compression. Com putational savings accrue in all
subsequent Conformer layers according to the O(T2D) complexity for self-attention, and linearly
for feed-forward networks.
Related work on time reduction Time reduction has been a useful technique for achieving a
balance between input and output lengths for ASR, and has evolved over time with the choice
of neural architecture and modeling units. In [29, 30], the lower fra me rates were achieved by
concatenating input frames and striding, for DNN or LSTM models pr edicting context-dependent
4

--- PAGE 5 ---
HMM states. [31, 32, 33] proposed to use hierarchical/pyramidal R NNs where outputs of consecutive
steps are combined before feeding to the next layer. After the co mmunity switched to end-to-
end systems and word-piece type modeling units, a popular frame ra te is 40ms as achieved by
convolutional subsampling, and is adopted by widely-used open-sou rce libraries [34]. With the
Conformer architecture [12], previous work on voice search mostly had a ﬁnal frame rate of 60ms,
achieved by a 30ms input frame rate and a 2x time reduction layer ear ly in the encoder, with
stacking [13] or funnel pooling [14]. With regards to the loss function , recent work of [35] proposed
to use CTC to (irregularly) select encoder output frames for RNN- T modeling, eﬀectively reducing
the frame rate for the decoder. However, such a CTC-based enc oder output selection does not
save any computations in the encoder, but only reduces the compu tation in downstream modules.
Our goal in this work is to aggressively reduce the sequence length e arly in the architecture for
computational savings.
4 Experiments
4.1 Datasets
Our experiments focus on short voice search queries. For a major ity of the experiments, we use
520M utterances of voice search queries for training; the total a mount of audio is 490K hours
and the average duration per utterance is 3.4 seconds. A small per centage of the training data is
human transcribed while the rest is pseudo-labeled by a 600M bidirect ional RNN-T teacher [36].
We tokenize training transcripts with word-piece models [21].
We use both real audio and TTS-generated data for evaluation. Th e real audio utterances
are representative of typical voice search traﬃc, with an averag e duration of 3.9 seconds. Our
development set consists of 9K real audio utterances (denoted a s VS-dev); we use a separate held-
out test set consisting of 5K utterances for testing (denoted as VS-test). The TTS sets contain
rare proper nouns (RPN) which appear fewer than 5 times in the tra ining set, and they are good
testbeds for external LM integration. Each TTS set contains 10K utterances and covers one of ﬁve
domains: Maps (denoted as RPNM), News (RPNN), Play (RPNP), Sea rch query logs (RPNS), and
Youtube (RPNY); they have average durations of 5.9, 10.1, 5.3, 5.4 , and 5.8 seconds respectively.
We use VS-dev, RPNM and RPNN for tuning the model architecture a nd other hyperparameters
and report ﬁnal WERs on the rest sets.
For training LMs for fusion, each minibatch is sampled 50/50 from the transcripts of acoustic
training data, and text-only data which contains 50B utterances. The text-only data contains
textual search queries from the domains of Maps, Textual searc h query logs, News, Play, and
Youtube, and a frequency-based pruning strategy, designed to improve rare word modeling, is
implemented to adjust the probability of selecting each query [37]. We train transformer LMs of
128M and 1B parameters (not counting parameters in the ﬁnal sof tmax) with wordpieces compatible
with those of E2E models.
4.2 Model Architectures
We use the 128-dimensional log Mel-ﬁlterbank energies (extracted from 32ms window and 10ms
shift) as the frontend features. After two 2D-convolution layer s, both with strides (2,2), the resulting
feature sequence has a frame rate of 40ms and becomes the input to our Conformer architecture.
This architecture mimics that of Google’s universal speech model (U SM, [6]). The number of
attention heads used in Conformer blocks is 8, and the intermediate dimension of the FFNs is 4
5

--- PAGE 6 ---
times the model dimension. The Conformer blocks use local self-att ention with a large attention
span, and the encoder output has a large enough receptive ﬁeld to cover the entire utterance. As
observed by [3], unsupervised pre-training does not help with ASR ac curacy when using a large
amount of supervised audio data, and thus we skip the pre-training step in this work (except for
including a baseline CTC model with pretraining in Table 1).
We explore two diﬀerent encoder conﬁgurations for CTC, with diﬀer ent sizes: the smaller en-
coder conﬁguration consists of 24 Conformer layers of dimension 7 68, leading to a total of 340M
parameters; the larger encoder has 32 Conformer layers of dimen sion 1536, leading to a total of 1.8B
parameters. For RNN-T, the encoder consists of 16 Conformer la yers of dimension 1536, resulting
in a total of 870M parameters. We use a |V|2embedding decoder [24], i.e., the prediction network
computes LM features based on two previous non-blank tokens, w hich was shown to work well on
voice search data. The model output uses the HAT factorization [1 8] which was shown to beneﬁt
external LM integration.
Deviating from the USM, we perform signiﬁcant time reduction in the e ncoder architecture.
Following [14], we initially start funnel pooling at layer index 4 (zero-bas ed), and apply pooling in
subsequent layers to achieve the desired factor of time reduction . As an example, if we perform
pooling at layers with (zero-based) indices 4 and 5, each with the red uction factor 2, we achieve a
total reduction factor 4 for layers 6 and onwards, i.e., the sequen ce length after layer 6 is 1/4 of the
original input length and the frame rate at the encoder output is 16 0ms, which is also the frame
rate at which the decoder operates. Note that [14] used funnel p ooling for on-device modeling with
stringent latency requirements, whereas here we use funnel poo ling for the full-context model to
speed up training and inference. We observe that the ASR accurac y turns out to be more tolerant
of time reduction. In Sec 4.5, we investigate the eﬀect of starting f unnel pooling at earlier layers to
further reduce inference costs.
Each model is trained with the Adafactor optimizer [38] with a batch s ize of 4096 utterances.
We train CTC models to 500K steps and RNN-T models to 300K steps, b y which point their WERs
on development sets have stabilized. In a pilot study, we have condu cted discriminative training
with the minimum word error rate objective [39]. We achieved no WER ga in on VS sets and 3%
relative improvement on the RPN sets. We only present results with m aximum likelihood training
in this work, and it is future work to carefully study discriminative tra ining for large models.
For CTC, we perform greedy decoding when not using external LM, in which case beam search
does not provide additional WER gain. We do perform preﬁx beam sea rch [40] with a beam size
of 8 when fusing with external LM. Two types of prior probabilities we re used for CTC shallow
fusion: the uniform prior over non-blanks as implemented by the blan k probability downscaling
technique [41], versus the uni-gram prior based on the model poste riors on training set [42]. We
perform label synchronous beam search (with path merging) for R NN-T with a beam size of 8, and
perform internal LM score subtraction in the case of shallow fusion .
4.3 CTC Results
Intuitively, with a larger vocabulary, the label sequences are shor ter and we can aﬀord more time
reduction. The hard constraint for CTC is that, since it emits only on e token (blank or non-blank)
at each encoder output frame, the encoder output sequence len gth must remain longer than the
label sequence length (RNN-T however is not subject to this const raint). During CTC training, we
discard utterances that violate this constraint, although such ca ses are uncommon in the VS sets.
For example, for the 16K vocabulary size, at the ﬁnal frame rate o f 320ms (8x reduction), roughly
1% utterances violate this constraint as estimated on VS-dev.
6

--- PAGE 7 ---
Table 1: WERs (%) on dev sets by CTC, with diﬀerent architectures a nd vocabulary sizes. Funnel
pooling is employed at layers 4 and 5, with factors 2,2 for 160ms frame rate and 3,2 for a 240 frame
rate.
Model size (frame rate) VS-dev RPNM RPNN
baselines: no pooling, vocab size=4096
340M (40ms) 4.7 15.1 12.4
1.8B (40ms), w. pretraining 4.2 14.2 10.4
with funnel pooling, vocab size=16384
340M (160ms) 4.5 15.3 16.6
340M (240ms) 4.5 14.9 21.6
1.8B (160ms) 4.3 13.7 12.7
1.8B (240ms) 4.2 13.8 17.3
1.8B (320ms) 5.0 14.0 26.4
1.8B (160ms) + 128M LM fusion
blank downscaling [41] 3.8 10.7 11.0
model-based prior [42] 3.8 10.5 10.8
1.8B (160ms) + 1B LM fusion
blank downscaling [41] 3.8 10.1 10.4
model-based prior [42] 3.8 9.8 10.1
We report WERs of a selection of CTC models in Table 1. As baselines, we report the WERs
with 4K vocabulary at a 40ms frame rate, a setup closely following tha t of the USM architecture [6].
We match these baselines on VS with the 16K vocabulary size and much lower frame rates of 160ms
and 240ms: for 340M CTC, we obtain an improvement from 4.7% to 4.5% on VS-dev, while for
1.8B CTC, we match the 4.2% without pretraining. For supervised tra ining with 1.8B CTC, we
observe a 4x speedup in training time with a 160ms frame rate, and a 4 .5x speedup with a 240ms
frame rate compared to the 40ms model (despite the use of a more costly softmax operation due
to larger vocabulary). However, we do observe that with a 320ms f rame rate, the WER on VS-dev
signiﬁcantly degrades to 5.0%, and this could not be alleviated by incre asing the vocabulary size
to 32K. This suggests that a too coarse time resolution does not wo rk well with the CTC loss and
its underlying independence assumptions.
The trend of WER on RPNM is similar to that of VS-dev. However, we do observe worse degra-
dation on RPNN as we apply heavier time reduction. As mentioned in Sec 4.1, the transcriptions of
RPNN come from the News domain which has diﬀerent linguistic charact eristics from voice search,
and the audio length is quite longer (10 secs on average) than the VS set (4 secs on average). We
hypothesize that models with lower frame rates may not generalize w ell to unseen audio length,
and further investigate this issue in Sec 4.6.
Given that the recent work on large models is based on CTC [5, 6], one m ay speculate that as
the bidirectional CTC model gets larger, and with large amounts of t raining data, the underlying
modeling assumption of CTC holds approximately and it can achieve sta te-of-the-art accuracy by
itself. We challenge this speculation by performing LM shallow fusion to the 1.8B CTC model
with a 160ms frame rate. We observe a signiﬁcant WER reduction fro m 4.2% to 3.7 – 3.8% with a
smaller 128M LM already on the in-domain VS-dev set. The results we p resent in Table 1 uses LM
weights that achieve a good balance between VS-dev and RPN sets. Had we focused only on VS,
7

--- PAGE 8 ---
Table 2: Dev set WERs (%) by RNN-T with various vocabulary sizes and frame rates. The encoder
contains 870M parameters. Funnel pooling starts at layer 4 and ap plies to adjacent layers. We use
the notation 3x2 to indicate that layer 4 has a reduction factor of 3 , and layer 5 has a reduction
factor of 2, which yield the ﬁnal frame rate of 240ms.
Frame rate (reduction factors) VS-dev RPNM RPNN
vocab size=4096, decoder size=10M
240ms (3x2) 3.8 12.6 12.1
320ms (2x2x2) 3.8 12.9 14.2
vocab size=16384, decoder size=33M
160ms (2x2) 3.7 12.3 12.8
240ms (3x2) 3.8 12.5 12.2
320ms (2x2x2) 3.8 12.5 13.6
400ms (5x2) 3.8 12.7 15.6
480ms (3x2x2) 3.9 12.6 19.5
640ms (2x2x2x2) 3.9 12.9 28.7
vocab size=32768, decoder size=65M
320ms (2x2x2) 3.8 12.9 14.8
640ms (2x2x2x2) 3.9 12.7 25.9
vocab size=16384, 128M LM fusion
160ms (2x2) 3.8 11.0 32.6
the best we could achieve on VS-dev is 3.7%, with clearly worse WERs on RPN sets. Among the
two prior estimation methods, model-based prior [42] consistently outperforms blank probability
downscaling [41] over diﬀerent WER operating points. When increasin g the external LM size to
1B, we could not further improve on VS-dev but achieved sizeable ga ins on rare word sets.
4.4 RNN-T Results
We conduct a similar set of time reduction experiments with RNN-T and the results are shown in
Table 2. Overall RNN-T is quite robust to diﬀerent vocabulary sizes. With the 16K vocabulary, we
studied the most time reduction settings, and we observed only sma ll WER degradations from 160ms
frame rate all the way to 640ms frame rate on VS-dev and RPNM. Lik e CTC, the performance on
the “out-of-domain” dataset RPNN degrades as the frame rate r educes. Comparing the models at
the same frame rate of 160ms and vocabulary size 16K, RNN-T has a WER of 3.7% on VS-dev,
outperforming the 4.2% by CTC by a large margin, and achieving parity with CTC + shallow fusion
on this set. This demonstrates the beneﬁt of having a learnable LM f eature encoder for modeling
label dependency in end-to-end ASR.
When fusing the best RNN-T model with a 128M LM, we observe intere stingly that it tends
to signiﬁcantly degrade RPNN (with heavy deletion errors) and the W ERs are quite sensitive to
internal and external LM weights. We list one set of results in Table 2 (bottom panel) which
achieves a good balance between VS-dev and RPNM, yet it degrades RPNN from 12.8% to 32.6%.
We hypothesize this is due to the bias of the internal language model of HAT learned purely on
short utterances. In Sec 4.6, we provide further evidence for th is, by demonstrating that training
on length-diverse data improves the WER and robustness to shallow fusion on RPNN.
8

--- PAGE 9 ---
Table 3: Dev set WERs (%) by 900M RNN-T with a vocab size of 16K and a 240ms frame rate
(3x2 reduction), with funnel pooling started in earlier Conformer la yers. The ﬁrst row is taken from
Table 2.
Start layer index VS-dev RPNM RPNN
4 3.8 12.5 12.2
3 3.7 12.6 12.0
2 3.7 12.4 11.7
1 3.7 12.6 12.3
Table 4: Dev set WERs (%) by 900M RNN-T with 16K vocabulary and two frame rates 240ms and
640ms trained on multi-domain data. Funnel pooling starts from laye r 4. First two rows are taken
from Table 2.
Frame rate VS-dev RPNM RPNN
Voice search training
240ms 3.8 12.5 12.2
640ms 3.9 12.9 28.7
Multi-domain training
240ms 3.6 13.6 6.9
640ms 3.8 12.6 9.5
+ 128M LM fusion
240ms 3.7 11.5 7.1
Multi-domain 600M teacher, vocab size=4096
60ms 4.0 11.5 7.1
4.5 Location of Funnel Pooling
So far, we have started funnel pooling from layer 4, following prior w ork [14] that worked on smaller
models with hard latency constraints. In this section, we start poo ling earlier in the architecture,
which leads to more eﬃcient inference.
We use the 240ms frame rate RNN-T model from Table 2 as the baselin e, with time reduction
factors of 3 and 2 in two consecutive layers, and change the pooling start layer index to 3, 2, and 1.
Training of the model with pooling starting at layer 0 diverged with the same learning parameters
and we do not report its performance. The results of these models on development sets are reported
in Table 3, which shows that on top of the 40ms base frame rate, the model is quite robust to the
location of pooling layers, and in fact we obtain small WER gains on RPN s ets by starting pooling
at layer 2. We plan to replace the 2D convolutional subsampling layers before the Conformer layer
with funnel pooling layers to explore more possibilities for frame redu ction.
4.6 Adding Long-Form Training Data
To verify that the poor performance of end-to-end models on RPN N was due to the lack of
longer training audio, we repeat several RNN-T experiments with ad ditional multi-domain training
data [43]. Most notably, we include segmented YT audio data containin g 520M utterances with an
average duration of 9.8 seconds, giving us a total of 600K hours of longer-form training data.
The comparisons between RNN-T models trained on voice search dat a and multi-domain data
9

--- PAGE 10 ---
Table 5: Test set WERs (%) by CTC and RNN-T, with 16K vocabulary. F unnel pooling starts
from layer 4. By default, we use VS training data.
VS-test RPNP RPNS RPNY
1.8B CTC, 160ms frame rate
4.9 39.8 23.1 26.0
+ 128M LM fusion with model prior
4.5 34.1 17.0 20.8
900M RNN-T, 240ms frame rate
4.5 37.8 20.6 23.3
+ Multi-domain training data
4.4 36.4 19.9 22.3
+ 128M LM fusion with ILM
4.4 33.9 16.9 20.2
Multi-domain 120M RNN-T [14]
60ms frame rate, 0.9s look-ahead
5.0 35.9 19.2 23.2
are presented in Table 4. For two vastly diﬀerent frame rates 240m s and 640ms, the additional long-
form training data improves the WERs on both VS-dev and RPNN by a lo t. With only voice search
training data, the WER gap on RPNN between the two frame rates us ed to be very large (12.2%
vs 28.7%), whereas with multi-domain training, the gap is signiﬁcantly r educed (6.9% vs 9.5%)
with much improved absolute WERs. When performing shallow fusion fo r the multi-domain 240ms
frame rate model with the same 128M LM used in Sec 4.4, we achieve a b etter balance between in-
domain and out-of-domain test sets, improving RPNM from 13.6% to 1 1.5% and without aﬀecting
RPNN much. .
It is interesting to add the bidirectional RNN-T teacher [36] into com parison, as shown in the
bottom panel of Table 4. The 600M teacher is trained on an earlier ve rsion of multi-domain data,
where the voice search portion is smaller but fully hand-transcribed , and is used to generate the
pseudo-labels of voice search data used in this paper. Note that we are surpassing or on par with the
teacher’s performance on VS and RPNN, probably due to a larger am ount of training data and more
consistent labeling. The teacher does outperform new models on RP NM, probably because, as an
end-to-end model, it predicts even fewer rare words in its pseudo- labels than human transcriptions,
for the student to imitate.
4.7 Final Evaluation
Finally, we compare the best conﬁgurations from both CTC and RNN- T on the test sets, namely
VS-test, RPNP, RPNS, and RPNY. Taking into account both in-doma in and out-of-domain per-
formance, we choose the 1.8B CTC model with 160ms frame rate, an d the 900M RNN-T model
with 240ms frame rate. The results are shown in Table 5. Relative mer its between methods are
consistent with the performance on development sets. That is, wit h the same voice search training
data, the 900M RNN-T outperforms the 1.8B CTC on all test sets, a nd by 8.0% relative margin for
the in-domain VS-test set (4.5% vs 4.9%). For more stable shallow fus ion performance on out-of-
domain test sets, it is beneﬁcial to train RNN-T on multi-domain data, and we expect the same for
10

--- PAGE 11 ---
CTC. As a reference, we provide results of another small RNN-T mo del, with a 60ms frame rate
and a limited right context of 0.9s, from previous work [14].
5 Conclusions
We have compared two major end-to-end ASR models, CTC and RNN- T, on a large-scale voice
search task. We have veriﬁed that RNN-T is clearly more accurate t han CTC for in-domain test
data even with a smaller model size, although the gap can be largely re moved with LM fusion
which compensates for the label independence assumption underly ing CTC. We have also observed
that, for large models, time reduction is eﬀective at reducing infere nce and training costs without
sacriﬁcing accuracy, and is thus useful for oﬄine transcription ta sks. In the future, we will further
optimize the model architecture and extend the usage to much long er audio.
References
[1] Alex Graves, Santiago Fern´ andez, Faustino Gomez, and J¨ urg en Schmidhuber, “Connectionist
temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks,”
inICML , 2006.
[2] Alex Graves, “Sequence transduction with recurrent neural n etworks,” in ICML Workshop on
Representation Learning , 2012.
[3] Yu Zhang, Daniel S Park, Wei Han, James Qin, Anmol Gulati, Joel S hor, Aren Jansen,
Yuanzhong Xu, Yanping Huang, Shibo Wang, et al., “Bigssl: Exploring t he frontier of large-
scale semi-supervised learning for automatic speech recognition,” IEEE Journal of Selected
Topics in Signal Processing , vol. 16, no. 6, pp. 1519–1532, 2022.
[4] Bo Li, Ruoming Pang, Yu Zhang, Tara N. Sainath, Trevor Strohma n, Parisa Haghani, Yun
Zhu, Brian Farris, Neeraj Gaur, and Manasa Prasad, “Massively mu ltilingual asr: A lifelong
learning solution,” in ICASSP , 2022.
[5] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine M cLeavey, and Ilya
Sutskever, “Robust speech recognition via large-scale weak supe rvision,” in ICML , 2023.
[6] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zh ehuai Chen, Nanxin Chen,
Bo Li, Vera Axelrod, Gary Wang, et al., “Google USM: Scaling automatic speech recognition
beyond 100 languages,” arXiv preprint arXiv:2303.01037 , 2023.
[7] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Ar un Babu, Sayani Kundu, Ali
Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al., “ Scaling speech technology
to 1,000+ languages,” arXiv preprint arXiv:2305.13516 , 2023.
[8] OpenAI, “GPT-4 technical report,” arXiv preprint arXiv:2303.08774 , 2023.
[9] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet , Marie-Anne Lachaux, Tim-
oth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Fa isal Azhar, Aurelien Ro-
driguez, Armand Joulin, Edouard Grave, and Guillaume Lample, “LLaM A: Open and eﬃcient
foundation language models,” arXiv preprint arXiv:2302.13971 , 2023.
11

--- PAGE 12 ---
[10] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Le pikhin, et al., “PaLM
2 technical report,” arXiv preprint arXiv:2305.10403 , 2023.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
/suppress L ukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” in NeurIPS , 2017.
[12] Anmol Gulati, James Qin, Chung-Cheng Chiu, et al., “Conformer: Convolution-augmented
transformer for speech recognition,” in Interspeech , 2020.
[13] Yanzhang He, Tara N. Sainath, Rohit Prabhavalkar, Ian McGra w, Raziel Alvarez, Ding Zhao,
David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, Qiao Liang , Deepti Bhatia, Yuan
Shangguan, Bo Li, Golan Pundak, Khe Chai Sim, Tom Bagby, Shuo-yiin Chang, Kanishka Rao,
and Alexander Gruenstein, “Streaming end-to-end speech recog nition for mobile devices,” in
ICASSP , 2019.
[14] Shaojin Ding, Weiran Wang, Ding Zhao, Tara N. Sainath, Yanzhan g He, Robert David, Rami
Botros, Xin Wang, Rina Panigrahy, Qiao Liang, Dongseong Hwang, Ia n McGraw, Rohit Prab-
havalkar, and Trevor Strohman, “A uniﬁed cascaded encoder ASR model for dynamic model
sizes,” in Interspeech , 2022.
[15] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le, “Funnel-Trans former: Filtering out
sequential redundancy for eﬃcient language processing,” in NeurIPS , 2020.
[16] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Ba rrault, Huei-Chi Lin, Fethi
Bougares, Holger Schwenk, and Yoshua Bengio, “On using monolingu al corpora in neural
machine translation,” arXiv:1503.03535 , 2015.
[17] Jan Chorowski and Navdeep Jaitly, “Towards better decoding and language model integration
in sequence to sequence models,” in Interspeech , 2017.
[18] Ehsan Variani, David Rybach, Cyril Allauzen, and Michael Riley, “H ybrid autoregressive
transducer (HAT),” in ICASSP , 2020.
[19] Jinyu Li et al., “Recent advances in end-to-end automatic spee ch recognition,” APSIPA
Transactions on Signal and Information Processing , vol. 11, no. 1, 2022.
[20] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schl¨ u ter, and Shinji Watanabe, “End-
to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329 , 2023.
[21] Mike Schuster and Kaisuke Nakajima, “Japanese and korean vo ice search,” in ICASSP , 2012.
[22] Yajie Miao, Mohammad Gowayyed, and Florian Metze, “EESEN: En d-to-end speech recogni-
tion using deep RNN models and WFST-based decoding,” in ASRU , 2015.
[23] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Ma rc Delcroix, Atsunori Ogawa,
and Tomohiro Nakatani, “Improving Transformer-based end-to- end speech recognition with
connectionist temporal classiﬁcation and language model integrat ion,” inInterspeech , 2019.
[24] Rami Botros, Tara N. Sainath, Robert David, Emmanuel Guzma n, Wei Li, and Yanzhang He,
“Tied & reduced RNN-T decoder,” in Interspeech , 2021.
12

--- PAGE 13 ---
[25] Nelson Morgan and Herv´ e Bourlard, “Continuous speech reco gnition using multilayer percep-
trons with hidden Markov models,” in ICASSP , 1990.
[26] Ehsan Variani, Erik McDermott, and Georg Heigold, “A Gaussian m ixture model layer jointly
optimized with discriminative features within a deep neural network a rchitecture,” in ICASSP ,
2015.
[27] Naoyuki Kanda, Xugang Lu, and Hisashi Kawai, “Minimum Bayes r isk training of CTC
acoustic models in maximum a posteriori based decoding framework,” inICASSP , 2017.
[28] Erik McDermott, Hasim Sak, and Ehsan Variani, “A density ratio a pproach to language model
fusion in end-to-end automatic speech recognition,” in ASRU , 2019.
[29] Vincent Vanhoucke, Matthieu Devin, and Georg Heigold, “Multifr ame deep neural networks
for acoustic modeling,” in ICASSP , 2013.
[30] Yajie Miao, Jinyu Li, Yongqiang Wang, Shi-Xiong Zhang, and Yifan G ong, “Simplifying long
short-term memory acoustic models for fast training and decoding ,” inICASSP , 2016.
[31] Salah Hihi and Yoshua Bengio, “Hierarchical recurrent neural networks for long-term depen-
dencies,” in NeurIPS , 1996.
[32] Jan Koutnik, Klaus Greﬀ, Faustino Gomez, and Juergen Schmidh uber, “A clockwork RNN,”
inICML , 2014.
[33] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen , attend and spell: A
neural network for large vocabulary conversational speech rec ognition,” in ICASSP , 2016.
[34] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayas hi, Jiro Nishitoba, Yuya Unno,
Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Ren-
duchintala, and Tsubasa Ochiai, “ESPnet: End-to-end speech pro cessing toolkit,” in Inter-
speech , 2018.
[35] Yongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhang, W ei Han, and Parisa Haghani,
“Accelerating RNN-T training and inference using CTC guidance,” in ICASSP , 2023.
[36] Dongseong Hwang, Khe Chai Sim, Zhouyuan Huo, and Trevor St rohman, “Pseudo label is
better than human label,” in Interspeech , 2022.
[37] W. Ronny Huang, Cal Peyser, Tara N. Sainath, Ruoming Pang, T revor Strohman, and Shankar
Kumar, “Sentence-select: Large-scale language model data sele ction for rare-word speech
recognition,” in Interspeech , 2022.
[38] Noam Shazeer and Mitchell Stern, “Adafactor: Adaptive learn ing rates with sublinear memory
cost,”arXiv preprint arXiv:1804.04235 , 2018.
[39] Rohit Prabhavalkar, Tara N. Sainath, Yonghui Wu, Patrick Ngu yen, Zhifeng Chen, Chung-
Cheng Chiu, and Anjuli Kannan, “Minimum word error rate training fo r attention-based
sequence-to-sequence models,” in ICASSP , 2018.
13

--- PAGE 14 ---
[40] Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, and Andre w Y. Ng, “First-pass large
vocabulary continuous speech recognition using bi-directional rec urrent DNNs,” arXiv preprint
arXiv:1408.2873 , 2014.
[41] Hasim Sak, Andrew W. Senior, Kanishka Rao, Ozan Irsoy, Alex Gr aves, Fran¸ coise Beaufays,
and Johan Schalkwyk, “Learning acoustic frame labeling for speech recognition with recurrent
neural networks,” in ICASSP , 2015.
[42] Qiujia Li, Chao Zhang, and Philip C. Woodland, “Integrating sourc e-channel and attention-
based sequence-to-sequence models for speech recognition,” in ASRU , 2019.
[43] Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng Chiu, Dav id Rybach, Tara Sainath, and
Trevor Strohman, “Recognizing long-form speech using streaming end-to-end models,” in
ASRU , 2019.
14
