# Mô hình Đa phương tiện Matryoshka

Mu Cai1, Jianwei Yang2, Jianfeng Gao2, Yong Jae Lee1

1Đại học Wisconsin-Madison, 2Microsoft Research, Redmond

https://matryoshka-mm.github.io/

Tóm tắt

Các Mô hình Đa phương tiện Lớn (LMMs) như LLaVA đã cho thấy hiệu suất mạnh mẽ trong lý luận thị giác-ngôn ngữ. Những mô hình này trước tiên nhúng hình ảnh thành một số lượng lớn các token thị giác cố định và sau đó đưa chúng vào một Mô hình Ngôn ngữ Lớn (LLM). Tuy nhiên, thiết kế này gây ra số lượng token quá mức cho các tình huống thị giác dày đặc như hình ảnh độ phân giải cao và video, dẫn đến sự kém hiệu quả lớn. Mặc dù tồn tại các phương pháp cắt tỉa và hợp nhất token, chúng tạo ra một đầu ra có độ dài duy nhất cho mỗi hình ảnh và không thể mang lại sự linh hoạt trong việc đánh đổi giữa mật độ thông tin và hiệu quả. Lấy cảm hứng từ khái niệm Búp bê Matryoshka, chúng tôi đề xuất M3: Mô hình Đa phương tiện Matryoshka, học cách biểu diễn nội dung thị giác như các tập hợp lồng nhau của các token thị giác nắm bắt thông tin qua nhiều mức độ chi tiết từ thô đến mịn. Cách tiếp cận của chúng tôi mang lại nhiều lợi ích độc đáo cho LMMs: (1) Có thể kiểm soát rõ ràng mức độ chi tiết thị giác cho mỗi trường hợp thử nghiệm trong quá trình suy luận, ví dụ, điều chỉnh số lượng token được sử dụng để biểu diễn một hình ảnh dựa trên độ phức tạp hoặc đơn giản dự kiến của nội dung; (2) M3 cung cấp một khung cho việc phân tích mức độ chi tiết cần thiết cho các bộ dữ liệu hiện có, nơi chúng tôi phát hiện rằng các tiêu chuẩn kiểu COCO chỉ cần khoảng 9 token thị giác để có được độ chính xác tương tự như khi sử dụng tất cả 576 token; (3) Cách tiếp cận của chúng tôi cung cấp nền tảng để khám phá sự đánh đổi tốt nhất giữa hiệu suất và độ dài token thị giác ở cấp độ mẫu, nơi điều tra của chúng tôi tiết lộ rằng tồn tại một khoảng cách lớn giữa giới hạn trên oracle và các biểu diễn tỷ lệ cố định hiện tại.

1 Giới thiệu

Các mô hình Đa phương tiện Lớn (LMMs) đã cho thấy hiệu suất mạnh mẽ trong hiểu và lý luận thị giác-ngôn ngữ. Các mô hình như LLaVA trước tiên nhúng hình ảnh đầu vào với một số lượng token thị giác cố định, và sau đó đưa chúng như các token tiền tố vào một Mô hình Ngôn ngữ Lớn (LLM) để suy luận về hình ảnh đầu vào. Các thiết kế mô hình tương tự được sử dụng trong video LMMs, nơi mỗi khung hình đóng góp một số lượng token cố định để tạo thành biểu diễn video cuối cùng.

Trong thực tế, số lượng token thị giác có thể rất lớn trong trường hợp hình ảnh độ phân giải cao, và thậm chí còn nhiều hơn đối với video dài. Các công trình hiện có chủ yếu giải quyết vấn đề này bằng cách tăng độ dài ngữ cảnh đầu vào và do đó, đưa một số lượng lớn ví dụ 3-8k token thị giác vào LLM. Cách tiếp cận này có một vài nhược điểm đáng kể: (1) ngữ cảnh cực kỳ dài làm cho cả việc huấn luyện và suy luận trở nên kém hiệu quả; (2) một số lượng token thị giác quá mức thực sự có thể làm tổn hại hiệu suất của LMM, làm nó mất tập trung khỏi việc chú ý đến thông tin liên quan, như chúng tôi cho thấy trong Phần 4.3. Một số công trình gần đây sử dụng phương pháp heuristic để cắt tỉa và hợp nhất token thị giác nhằm giảm độ dài chuỗi. Tuy nhiên, chúng tạo ra một đầu ra có độ dài duy nhất và không mang lại quyền kiểm soát độ dài chuỗi cuối cùng, điều này có thể hữu ích để đánh đổi mật độ thông tin và hiệu quả trong khi tính đến các ràng buộc tài nguyên trong giai đoạn triển khai.

Preprint. Đang được xem xét. arXiv:2405.17430v2 [cs.CV] 29 Tháng 7 2024

M3...

Trong lòng một nhà hàng nhộn nhịp, một cô gái trẻ tìm thấy sự thoải mái tại một chiếc bàn...Trong lòng một nhà hàng nhộn nhịp, một cô gái trẻ có mái tóc rực rỡ đang ngồi tại một chiếc bàn gỗ, sự chú ý của cô bị cuốn hút bởi máy ảnh...Trong lòng một nhà hàng nhộn nhịp, một cô gái trẻ có mái tóc dài, đen là trung tâm chú ý. Cô ấy mặc một chiếc áo len sọc xanh trắng,... Chiếc bàn được trang trí bằng một chiếc túi giấy trắng, có thể chứa bữa ăn của cô ấy. Một chiếc cốc Pepsi màu xanh nằm trên bàn...

Hình 1: Mô hình Đa phương tiện Matryoshka. Chúng tôi ép buộc tập token thị giác thô hơn XSi−1 được bắt nguồn từ mức token thị giác mịn hơn XSi. Kết quả là, mức độ chi tiết của các token thị giác Matryoshka thay đổi dần dần một cách có thể kiểm soát được. Hình ảnh từ tập xác thực MSCOCO.

Hình ảnh và video tự nhiên thể hiện cấu trúc phân cấp từ chi tiết thô đến mịn, và hệ thống thị giác của con người đã tiến hóa để nhận biết thông tin thị giác theo cách từ thô đến mịn này, như được các nhà sinh học và tâm lý học chỉ ra từ nhiều thập kỷ trước. Liệu chúng ta có thể tạo ra một cấu trúc tương tự cho LMMs, nơi trong một bộ trọng số mô hình, các token nội dung thị giác được tổ chức thành các tỷ lệ chi tiết khác nhau? Về mặt khái niệm, mục tiêu của chúng tôi là học các token thị giác có cấu trúc lồng nhau, tương tự như Búp bê Matryoshka. Học Biểu diễn Matryoshka (MRL) xây dựng cơ chế Matryoshka trên vector biểu diễn của mạng neural, nơi mỗi phân đoạn với các chiều đặc trưng khác nhau có khả năng xử lý các tác vụ như phân loại hoặc truy xuất. Tuy nhiên, đối với LMMs, sự kém hiệu quả chủ yếu đến từ số lượng token. Do đó, lấy cảm hứng từ, nhưng khác với MRL, công trình của chúng tôi được thúc đẩy để xây dựng Mô hình Đa phương tiện Matryoshka trên chiều độ dài token, để chúng tôi có thể điều chỉnh nó một cách linh hoạt.

Hình 2: Kết quả đánh giá MMBench dưới M3, oracle dưới LLaVA-1.5-M3, LLaVA-1.5 với pooling trung bình tại thời gian suy luận, LLaVA-1.5 được huấn luyện riêng cho mỗi tỷ lệ cụ thể, và các phương pháp khác. M3 cho thấy hiệu suất ít nhất là tốt như LLaVA được huấn luyện cho mỗi tỷ lệ cụ thể. Một khoảng cách lớn tồn tại giữa giới hạn trên oracle và hiệu suất thực tế của mô hình trên một tỷ lệ cụ thể.

Cụ thể, chúng tôi đề xuất M3: Mô hình Đa phương tiện Matryoshka, ép buộc một LMM học một hệ thống phân cấp của các mức độ chi tiết biểu diễn thị giác ở mức chuỗi token, thay vì ở mức chiều đặc trưng như trong MRL. Với biểu diễn này, tại thời gian suy luận, mức độ chi tiết thị giác có thể được kiểm soát linh hoạt dựa trên các yêu cầu cụ thể, ví dụ, để tính đến mật độ thông tin của hình ảnh đầu vào và các ràng buộc hiệu quả.

Quá trình huấn luyện của chúng tôi đơn giản và trực tiếp. Trong quá trình huấn luyện, chúng tôi mã hóa hình ảnh thành M tập hợp token thị giác từ thô đến mịn, XSi, i = 1, ⋯, M, nơi số lượng token thị giác tăng dần, tức là, |XSi−1| < |XSi|. Và quan trọng là, các token thị giác ở mức thô hơn được bắt nguồn từ các token thị giác ở mức mịn hơn, tức là, XSi−1 ⊂ XSi, ∀i. Bằng cách này, thông tin thị giác trong [XS1, XS2, ⋯, XSM] dần dần bao gồm nhiều chi tiết mịn hơn. Ví dụ, cho một hình ảnh tự nhiên như được hiển thị trong Hình 1, XS1 bao gồm ngữ nghĩa cấp cao như nhà hàng và cô gái, trong khi XSM bao gồm nhiều chi tiết hơn như cốc Pepsi và túi giấy trắng. Tất cả các cài đặt huấn luyện khác, như hàm mất mát và kiến trúc mô hình, được giữ nguyên như LLaVA.

Cách tiếp cận của chúng tôi, M3, giới thiệu một số thuộc tính và lợi ích mới cho LMMs. Đầu tiên, cách tiếp cận của chúng tôi có thể biểu diễn nội dung thị giác một cách thích ứng và hiệu quả. Dưới một bộ trọng số, nó tạo ra nhiều tập hợp token thị giác lồng nhau với các mức độ chi tiết khác nhau trong mật độ thông tin. Điều này cho phép linh hoạt trong số lượng token thị giác được sử dụng cho bất kỳ hình ảnh nào trong quá trình suy luận, cho phép kiểm soát sự đánh đổi tốt nhất giữa chi phí và hiệu suất dựa trên nội dung hình ảnh hoặc video. Ví dụ, có thể sử dụng tất cả token thị giác cho hình ảnh có chi tiết dày đặc và chỉ sử dụng một vài token cho hình ảnh đơn giản hơn. Sự linh hoạt này có thể đặc biệt quan trọng khi xử lý các chuỗi thị giác rất dài, như video. Ví dụ, với ngân sách cố định 2880 token thị giác, người dùng có thể biểu diễn một video gồm 2880 khung hình mỗi khung có một token hoặc biểu diễn cùng một video bằng cách lấy mẫu 5 khung hình mỗi khung có 576 token.

Thứ hai, cách tiếp cận của chúng tôi có thể được sử dụng như một khung tổng quát để đánh giá độ phức tạp thị giác của các bộ dữ liệu hoặc tiêu chuẩn thị giác-ngôn ngữ, tức là, mức độ chi tiết nào cần thiết để thực hiện tác vụ đã cho một cách chính xác. Đáng ngạc nhiên, chúng tôi phát hiện rằng hầu hết các tiêu chuẩn, đặc biệt là những tiêu chuẩn chủ yếu được tạo ra từ cảnh tự nhiên (như COCO), có thể được xử lý tốt chỉ với ~9 token mỗi hình ảnh. Ngược lại, các tác vụ nhận thức thị giác dày đặc như hiểu tài liệu hoặc OCR yêu cầu một lượng token lớn hơn (144−576 token) mỗi hình ảnh để xử lý tác vụ tốt. Các phát hiện chi tiết được trình bày trong Phần 4.2.

Cuối cùng, cách tiếp cận của chúng tôi cung cấp nền tảng để giải quyết một nhiệm vụ quan trọng trong LMMs: Làm thế nào để sử dụng ít token thị giác nhất trong khi trả lời các câu hỏi thị giác một cách chính xác?. Dựa trên dự đoán của mô hình trên tập thử nghiệm, chúng tôi phát hiện rằng so với token thị giác đầy đủ, oracle có thể sử dụng ít token hơn nhiều trong khi thực hiện tốt hơn nhiều. Ví dụ, dưới sáu tiêu chuẩn LMM phổ biến được sử dụng trong LLaVA-NeXT, oracle với mô hình M3 đã được huấn luyện có thể sử dụng ít đến 8.9 token thị giác trung bình để đạt được hiệu suất tốt hơn 8% so với LLaVA-NeXT sử dụng 576 token mỗi lưới hình ảnh. Điều này chỉ ra rằng có một khoảng trống lớn để cải thiện so với giới hạn trên oracle, như chúng tôi cho thấy trong Phần 4.2.

Để cho phép nghiên cứu thêm về LMMs thích ứng học các mức độ chi tiết thông tin đa dạng, chúng tôi công bố mã nguồn và mô hình.

2 Công trình liên quan

Mô hình Đa phương tiện Lớn. Các Mô hình Ngôn ngữ Lớn (LLMs) như ChatGPT, GPT-4, và LLaMA đã chứng minh khả năng suy luận và tổng quát hóa ấn tượng cho văn bản. Cảnh quan của LLMs đã được biến đổi đáng kể bởi việc giới thiệu gần đây các mô hình cũng kết hợp thông tin thị giác, như GPT-4V(ision). Dựa trên các LLMs mã nguồn mở, rất nhiều mô hình đa phương tiện đã có những bước tiến đáng kể, dẫn đầu bởi các mô hình như LLaVA và MiniGPT-4, kết hợp khả năng ngôn ngữ của LLaMA với bộ mã hóa hình ảnh dựa trên CLIP. Gần đây, các LMMs cho nhiều tác vụ và phương thức hơn đã xuất hiện, như LMMs cấp vùng, LMMs 3D, và LMMs video.

Tuy nhiên, các LMMs hiện có thường biểu diễn nội dung thị giác với một số lượng lớn và cố định các token, điều này làm cho việc mở rộng đến các chuỗi thị giác rất dài như hình ảnh độ phân giải cao hoặc video dài trở nên khó khăn. Trong công trình này, chúng tôi đề xuất biểu diễn nội dung thị giác một cách thích ứng và hiệu quả bằng cách học nhiều tập hợp token thị giác lồng nhau, cung cấp sự linh hoạt trong số lượng token thị giác được sử dụng cho bất kỳ hình ảnh nào trong quá trình suy luận.

Học Biểu diễn Matryoshka. Học Biểu diễn Matryoshka (MRL) giải quyết nhu cầu về các biểu diễn linh hoạt có thể thích ứng với nhiều tác vụ hạ lưu với tài nguyên tính toán khác nhau. Cách tiếp cận này, lấy cảm hứng từ bản chất lồng nhau của búp bê Matryoshka, mã hóa thông tin ở các mức độ chi tiết khác nhau trong cùng một vector đặc trưng chiều cao được tạo ra bởi một mạng neural. Khả năng thích ứng của MRL mở rộng qua các phương thức khác nhau, bao gồm thị giác (ResNet, ViT), thị giác + ngôn ngữ (ALIGN), và ngôn ngữ (BERT), chứng minh tính linh hoạt và hiệu quả của nó. Công trình gần đây mở rộng MRL đến cả không gian nhúng văn bản và không gian lớp Transformer. Cách tiếp cận của chúng tôi được lấy cảm hứng từ MRL, nhưng thay vì học nhiều nhúng lồng nhau cho một vector đặc trưng chiều cao, chúng tôi học các token thị giác lồng nhau theo chiều độ dài token cho đầu vào thị giác. Chúng tôi là những người đầu tiên cho thấy rằng ý tưởng học Matryoshka có thể cho phép kiểm soát rõ ràng mức độ chi tiết thị giác của nội dung thị giác mà một LMM xử lý.

Giảm Token. Một trong những nguyên nhân chính gây kém hiệu quả trong các LMMs gần đây là số lượng lớn token thị giác tiền tố được đưa vào LLM. Độ phức tạp bậc hai trong Transformers là vấn đề chính trong việc mở rộng độ dài chuỗi đầu vào cho Transformers. Giảm token phục vụ như một kỹ thuật hiệu quả để giảm chi phí tính toán trong Transformers. Các phương pháp attention thưa thớt như Linformer và ReFormer thực hiện các phép toán attention trong các cửa sổ cục bộ thay vì toàn bộ ngữ cảnh, do đó giảm độ phức tạp bậc hai của phép toán attention vanilla. Một phương pháp đáng chú ý khác là Token Merging (ToMe), sử dụng attention đầy đủ nhưng dần dần giảm số lượng token trong mỗi khối transformer bằng cách chọn các token đại diện nhất thông qua so khớp lưỡng phân cho Vision Transformer (ViT). Một công trình gần đây tiếp tục nghiên cứu các họ phương pháp giảm token khác nhau cho ViT. Tuy nhiên, các cách tiếp cận trước đây tạo ra một đầu ra độ dài duy nhất cho mỗi hình ảnh đầu vào và không cung cấp nhiều mức độ chi tiết trên chuỗi token đã giảm. Cách tiếp cận M3 của chúng tôi thay vào đó học một biểu diễn token đa mức độ chi tiết, từ thô đến mịn trong cùng một kiến trúc và trọng số mô hình, cho phép nó dễ dàng được điều chỉnh theo các ràng buộc tính toán hoặc bộ nhớ khác nhau.

Một công trình đồng thời có tinh thần tương tự với cách tiếp cận của chúng tôi, biểu diễn một hình ảnh với số lượng token thị giác khác nhau sử dụng một bộ trọng số mô hình duy nhất. Trong khi phương pháp của họ định dạng lại các token thị giác thành một danh sách tuần tự thông qua các lớp biến đổi, chúng tôi sử dụng pooling trung bình để bảo toàn cấu trúc không gian của các token thị giác, chứng minh hiệu quả trong các thí nghiệm của chúng tôi.

3 M3: Mô hình Đa phương tiện Matryoshka

Mục tiêu của chúng tôi là học một Mô hình Đa phương tiện Lớn (LMM) biểu diễn nội dung thị giác như các tập hợp token thị giác lồng nhau nắm bắt thông tin qua nhiều mức độ chi tiết từ thô đến mịn, để có thể kiểm soát rõ ràng mức độ chi tiết thị giác cho mỗi trường hợp thử nghiệm trong quá trình suy luận. Ở đây chúng tôi giới thiệu cách chúng tôi học một chuỗi token giống như búp bê Matryoshka.

Các LMMs như LLaVA thường đưa vào một chuỗi các token thị giác như token tiền tố vào LLM để suy luận thị giác-ngôn ngữ. Bộ mã hóa thị giác từ các mô hình ngôn ngữ-thị giác được huấn luyện trước, như CLIP và SigLIP, thường được sử dụng để chiếu hình ảnh thành tập hợp các token thị giác. Cụ thể, bộ mã hóa thị giác CLIP biểu diễn một hình ảnh đầu vào I như một lưới H×W các token thị giác XH×W, nơi mỗi Xi ∈ RC là một vector đặc trưng C chiều. Mục tiêu của chúng tôi là học các tập hợp token thị giác lồng nhau [XS1, XS2, ⋯, XSM] mã hóa thông tin thị giác theo cách từ thô đến mịn.

Để đạt được điều này, chúng tôi ép buộc XSi ⊂ XSi+1, ∀i. Quan trọng là, chúng tôi không giới thiệu bất kỳ tham số học được mới nào cho LMM. Thay vào đó, chúng tôi tối ưu hóa bộ mã hóa thị giác CLIP để học biểu diễn thị giác lồng nhau trực tiếp, và huấn luyện LLM tiếp theo để thích ứng với tập hợp token lồng nhau đã học.

Để dễ giải thích, chúng tôi xem xét CLIP-ViT-L-336 như bộ mã hóa thị giác, nơi một hình ảnh được mã hóa thành 24×24 token thị giác (tổng cộng 576). Chúng tôi tạo ra M tập hợp token ví dụ, |Si| ∈ {1, 9, 36, 144, 576}, trong đó các token thị giác ở mức thô hơn được bắt nguồn trực tiếp từ những token ở mức mịn hơn. Cụ thể, cho 24×24 token thị giác ban đầu, chúng tôi áp dụng tuần tự pooling 2×2 với stride 2, tạo ra 12×12, 6×6, và 3×3 token thị giác. Cuối cùng, chúng tôi áp dụng pooling 3×3 và có được token thị giác đơn lẻ cô đặc nhất. Bằng cách này, các tập hợp token thị giác Matryoshka có thể dần dần bảo toàn thông tin không gian trong các token gốc đồng thời tạo thành một biểu diễn lồng nhau từ thô đến mịn.

Chúng tôi huấn luyện M3 bằng cách lấy trung bình mất mát dự đoán token tiếp theo tự hồi quy cho mỗi tỷ lệ Si cho mỗi hình ảnh Ii. Cụ thể, cho một biểu diễn thị giác Matryoshka XSi cho tỷ lệ Si, chúng tôi tối đa hóa khả năng của các token được dự đoán khớp với câu trả lời thật Xa:

P(Xa|XSi,Xq) = ∏(j=1 to L) Pθ(xj|XSi,Xq,Xa,<j), (3.1)

nơi θ là các tham số có thể huấn luyện được của mô hình, bao gồm cả bộ mã hóa thị giác CLIP và LLM tiếp theo. Xq biểu thị câu hỏi ở định dạng văn bản, L biểu thị độ dài token của câu trả lời thật Xa, và Xa,<j biểu thị tất cả các token câu trả lời thật trước token dự đoán hiện tại xj, nơi j biểu thị chỉ số token trong quá trình tạo token văn bản. Chúng tôi bỏ qua các thông điệp hệ thống để rõ ràng, mặc dù chúng là một phần của điều kiện. Hình 3 hiển thị kiến trúc mô hình của chúng tôi.

Mục tiêu cuối cùng lấy trung bình trên tất cả M tỷ lệ token thị giác:

min_θ (1/M) ∑(i=1 to M) −logP(Xa|XSi,Xq). (3.2)

Với hàm mục tiêu này, M3 học các tập hợp token thị giác lồng nhau dần dần bao gồm nhiều chi tiết hơn với tỷ lệ tăng lên. Ví dụ, trong Hình 1, tập hợp token thị giác nhỏ hơn mô tả toàn bộ cảnh ở mức cao trong khi tập hợp token thị giác lớn hơn bao gồm nhiều chi tiết hơn như cốc Pepsi.

Mục tiêu huấn luyện của chúng tôi cho phép mô hình của chúng tôi thực hiện trả lời câu hỏi thị giác dưới bất kỳ mức độ chi tiết nào trong quá trình suy luận. Điều này có thể đặc biệt hữu ích trong các ứng dụng hạn chế tài nguyên; ví dụ, mức độ chi tiết thị giác có thể được điều chỉnh linh hoạt dựa trên độ đơn giản hoặc phức tạp dự kiến của nội dung thị giác trong khi tính đến các ràng buộc tính toán và bộ nhớ.

4 Thí nghiệm

Trong phần này, chúng tôi trước tiên chi tiết các cài đặt thí nghiệm trong Phần 4.1. Sau đó chúng tôi hiển thị hiệu suất của M3 trên cả tiêu chuẩn cấp hình ảnh 4.2 và tiêu chuẩn cấp video 4.3. Cuối cùng, chúng tôi phân tích hành vi của Mô hình Đa phương tiện Matryoshka và cung cấp các nghiên cứu loại bỏ trong Phần 4.4 và 4.5.

4.1 Cài đặt Thí nghiệm

Mô hình Chúng tôi sử dụng LLaVA-1.5 và LLaVA-NeXT làm LMMs cơ sở, cả hai với Vicuna 7B làm backbone mô hình ngôn ngữ. Chúng tôi fine-tune toàn bộ mô hình sử dụng chính xác dữ liệu hướng dẫn thị giác từ LLaVA-1.5 và LLaVA-NeXT, tương ứng. Tỷ lệ học của LLM là 2×10^-5 và 1×10^-5, tương ứng cho LLaVA-1.5 và LLaVA-NeXT. Tỷ lệ học cho bộ mã hóa thị giác là 2×10^-5 cho cả hai mô hình. Chúng tôi huấn luyện cả hai mô hình trong 1 epoch sử dụng 8 GPU NVIDIA H100.

Thay vì huấn luyện mô hình ngôn ngữ từ đầu, chúng tôi khởi tạo trọng số mô hình ngôn ngữ từ LLaVA-1.5 và LLaVA-NeXT được huấn luyện trước, điều này thực nghiệm hoạt động tốt hơn. Chúng tôi đặt tên cho Mô hình Đa phương tiện Matryoshka của chúng tôi là LLaVA-1.5-M3 và LLaVA-NeXT-M3.

Tỷ lệ Token Thị giác Chúng tôi thiết kế 5 tỷ lệ cho các token thị giác. LLaVA-1.5 và LLaVA-NeXT đều tận dụng CLIP-ViT-L-336 làm bộ mã hóa thị giác, nơi một hình ảnh được nhúng thành 24×24 token thị giác. Chúng tôi dần dần áp dụng pooling 2×2 với stride 2, tạo ra 12×12, 6×6, và 3×3 token thị giác, nơi cuối cùng chúng tôi áp dụng pooling 3×3 để có token thị giác đơn lẻ cuối cùng.

Do đó, kích thước của các tập hợp token thị giác Matryoshka là S ∈ {1, 9, 36, 144, 576}, theo cách lồng nhau. Phân tích hiệu quả ở cấp hệ thống được hiển thị trong Phụ lục B, nơi M3 tăng tốc độ của quá trình prefill LMM thông qua việc giảm các phép toán dấu phẩy động (FLOPs) và giảm yêu cầu bộ nhớ tính toán.

Đánh giá. Đối với hiểu hình ảnh, chúng tôi đánh giá LLaVA-1.5 và LLaVA-NeXT trên (a) các tiêu chuẩn đa phương tiện đa dạng: POPE, GQA, MMBench, VizWiz, SEEDBench, ScienceQA, MMMU, và (b) các tiêu chuẩn hiểu tài liệu/Nhận dạng ký tự quang học (OCR): DocVQA, ChartQA, AI2D và TextVQA.

Đối với hiểu video, chúng tôi sử dụng cả (a) các tiêu chuẩn trả lời câu hỏi video mở được đánh giá bởi GPT-3.5: MSVD-QA, MSRVTT-QA và ActivityNet-QA; và (b) các tiêu chuẩn trả lời câu hỏi video đa lựa chọn: NExT-QA, IntentQA, và EgoSchema.

Bảng 1: So sánh giữa LLaVA-1.5-M3 trên các tiêu chuẩn khác nhau dưới các tiêu chuẩn hiểu hình ảnh. LLaVA-1.5-M3 duy trì hiệu suất của LLaVA-1.5 trong khi vượt trội hơn Qwen-VL và InstructBLIP với ít token hơn.

[Bảng hiển thị so sánh hiệu suất giữa các phương pháp khác nhau với số token và điểm số trên các tiêu chuẩn]

Bảng 2: So sánh các phương pháp với baseline SS và M3 trên các tiêu chuẩn khác nhau dưới LLaVA-NeXT. Ở đây # Tokens biểu thị số token thị giác mỗi lưới hình ảnh trong LLaVA-NeXT. SS biểu thị mô hình baseline được huấn luyện với một Tỷ lệ Cụ thể của token thị giác. M3 ít nhất là tốt như SS, trong khi thực hiện tốt hơn trên các tác vụ như TextVQA, ChartQA, và MMBench. Oracle biểu thị trường hợp nơi sự đánh đổi tốt nhất giữa token thị giác và hiệu suất được chọn.

4.2 Hiểu Hình ảnh

LLaVA-1.5-M3 Chúng tôi đánh giá LLaVA-1.5-M3 trên các tiêu chuẩn hiểu và suy luận đa phương tiện phổ biến. Kết quả được hiển thị trong Bảng 1. LLaVA-1.5-M3 với token đầy đủ duy trì hiệu suất của LLaVA-1.5 trên các tiêu chuẩn đa dạng. Quan trọng hơn, cách tiếp cận của chúng tôi cho thấy hiệu suất mạnh mẽ ngay cả với 1 hoặc 9 token. Cụ thể, trong MMBench, một tiêu chuẩn hiểu đa phương tiện toàn diện, LLaVA-1.5-M3 với 9 token vượt trội hơn Qwen-VL-Chat với 256 token, và đạt được hiệu suất tương tự như Qwen-VL-Chat thậm chí với 1 token. So với InstructBLIP, LLaVA-1.5 M3 với 9 token vượt trội hơn InstructBLIP-7B và InstructBLIP-13B trên tất cả các tiêu chuẩn. Điều này chứng minh rằng mô hình của chúng tôi có cả tính linh hoạt và hiệu suất thực nghiệm mạnh mẽ dưới số lượng token thị giác đa dạng.

LLaVA-NeXT-M3 Chúng tôi sử dụng Mô hình Đa phương tiện Matryoshka được đề xuất để fine-tune LLaVA-NeXT, và so sánh LLaVA-NeXT-M3 với SS, biểu thị cài đặt nơi LLaVA-NeXT được huấn luyện dưới một Tỷ lệ Cụ thể của token thị giác cũng trong 1 epoch. Chúng tôi cũng bao gồm hiệu suất giới hạn trên oracle. Cụ thể, 'Oracle' biểu thị trường hợp nơi sự đánh đổi tốt nhất giữa token thị giác và hiệu suất được chọn cho mỗi trường hợp thử nghiệm. Cụ thể, cho mỗi trường hợp thử nghiệm, chúng tôi chọn tỷ lệ với số lượng token ít nhất nhưng có thể trả lời câu hỏi chính xác. Kết quả được hiển thị trong Bảng 2. Cách tiếp cận của chúng tôi, M3, ít nhất là tốt như SS, trong khi thực hiện tốt hơn trên các tác vụ như hiểu tài liệu (TextVQA và ChartQA) và các tiêu chuẩn phổ biến như MMBench.

Kết quả của chúng tôi cũng cho thấy rằng thiên hướng cấp bộ dữ liệu hướng tới các tỷ lệ token thị giác thực sự tồn tại. Ví dụ, ScienceQA duy trì hiệu suất nhất quán trên tất cả các tỷ lệ token thị giác. AI2D và MMBench chỉ gặp phải sự giảm hiệu suất nhỏ thậm chí ít đến 9 đến 1 token. Mặt khác, các tác vụ nhận thức thị giác dày đặc như TextVQA và DocVQA cho thấy sự giảm hiệu suất đáng kể với ít token hơn. Phân tích này cho thấy M3 có thể phục vụ như một khung để phân tích mức độ chi tiết mà một tiêu chuẩn cần.

Hơn nữa, có một khoảng cách lớn giữa hiệu suất thực tế của mô hình dưới token đầy đủ và giới hạn trên oracle. Điều này chỉ ra rằng sử dụng token đầy đủ không thể luôn dẫn đến hiệu suất tối ưu cho tất cả các mẫu; tức là, có một khoảng trống lớn để cải thiện hướng tới điểm oracle.

4.3 Hiểu Video

Theo IG-VLM, chúng tôi trực tiếp thực hiện suy luận zero-shot trên các tiêu chuẩn video đa dạng sử dụng LLaVA-NeXT-M3. Cụ thể, 6 khung hình được lấy mẫu đều trên toàn bộ video, sau đó được sắp xếp như một bức tranh ghép, được đưa vào LLaVA-NeXT cùng với câu hỏi để có được phản hồi. Kết quả dưới LLaVA-NeXT-M3 và các LMMs video gần đây được hiển thị trong Bảng 3.

Bảng 3: Độ chính xác tổng thể của LLaVA-NeXT-M3 và các LMMs video gần đây trên các tiêu chuẩn hiểu video khác nhau. Ở đây # Tokens biểu thị tổng số token thị giác trên tất cả các khung hình.

LLaVA-NeXT-M3 với token thị giác đầy đủ một lần nữa cho thấy hiệu suất tương đương với LLaVA-NeXT. Thú vị hơn, kết quả chỉ ra rằng token thị giác đầy đủ thường không dẫn đến hiệu suất tốt nhất trong các tác vụ hiểu video. Cụ thể, trên 4 trong 6 tiêu chuẩn, token thị giác đầy đủ cho thấy hiệu suất kém mong muốn hơn so với 720 hoặc 180 token thị giác. Chúng tôi nghi ngờ rằng ngữ cảnh thị giác rất dài có thể mang lại sự phân tâm (ví dụ, tập trung quá nhiều vào nền có thể không liên quan) cho dự đoán của mô hình, nơi một biểu diễn cô đọng của video tập trung vào thông tin liên quan hơn có thể có lợi thế hơn.

Cuối cùng, đối với hầu hết các tác vụ hiểu video như ActivityNet, IntentQA và EgoSchema, với 9 token mỗi lưới hình ảnh (45 token tổng cộng), sự khác biệt độ chính xác so với token đầy đủ (2880 tổng cộng) là ít hơn 1%. Điều này chứng minh rằng các câu hỏi video trong các tiêu chuẩn này thường yêu cầu thông tin thị giác rất thưa thớt, vì nguồn của các tiêu chuẩn hiểu video như vậy chủ yếu đến từ cảnh tự nhiên, điều này phù hợp với quan sát của chúng tôi trong các tiêu chuẩn hiểu hình ảnh.

4.4 Phân tích Sâu

M3 cho thấy hiệu suất mạnh mẽ hơn nhiều so với lấy mẫu dựa trên heuristic tại thời gian thử nghiệm. Một cách đơn giản để giảm số lượng token thị giác thông qua cách không cần huấn luyện là thực hiện hợp nhất hoặc giảm token heuristic. Trong Bảng 4, chúng tôi so sánh M3 với ba cách tiếp cận không cần huấn luyện: pooling trung bình, lấy mẫu không gian, và lấy mẫu tuần tự. M3 có khả năng phục hồi tốt hơn nhiều khi số lượng token giảm, trong khi các cách tiếp cận lấy mẫu dựa trên heuristic cho thấy sự giảm hiệu suất đáng kể. Một hình ảnh hóa của việc lấy mẫu không gian và tuần tự được hiển thị trong Hình 5.

Bảng 4: So sánh giữa M3, và các baseline lấy mẫu dựa trên heuristic—pooling trung bình, lấy mẫu không gian, và lấy mẫu tuần tự—tại thời gian suy luận trên MMBench với kiến trúc LLaVA-NeXT.

M3 phục vụ như một thước đo tốt cho độ phức tạp hình ảnh. Chúng tôi trích xuất phản hồi từ LLaVA-NeXT-M3 trong tiêu chuẩn TextVQA, và hiển thị các mẫu nơi sử dụng token thị giác trên các tỷ lệ khác nhau có thể trả lời câu hỏi chính xác và không chính xác. Được hiển thị trong Hình 4, hiệu suất OCR phù hợp với độ phức tạp của hình ảnh, điều này chỉ ra rằng M3 có thể được sử dụng như một thước đo hướng tới độ phức tạp cấp mẫu.

Hình 4: Các mẫu thử nghiệm TextVQA với dự đoán chính xác và không chính xác trên các tỷ lệ khác nhau. Câu trả lời khác nhau với số lượng token thị giác khác nhau. Ngoài ra, M3 có thể phục vụ như một khung để đánh giá độ phức tạp của hình ảnh.

Khoảng cách lớn giữa oracle và hiệu suất thực tế. Như được hiển thị trong Bảng 2, giới hạn trên oracle có thể sử dụng rất ít (6∼64) token nhưng đạt được hiệu suất ít nhất tốt hơn 10% so với token thị giác đầy đủ. Điều này gợi ý rằng một bộ dự đoán tỷ lệ token thị giác, nơi mô hình học tự động chọn tỷ lệ token thị giác tốt nhất cho các hình ảnh đầu vào hoặc cả hình ảnh đầu vào và câu hỏi, có tiềm năng đạt được sự đánh đổi tốt hơn. Điều này sẽ là công việc tương lai thú vị.

Tổng quát hóa zero-shot đến các chuỗi thị giác dài hơn. Ở đây chúng tôi mở rộng độ dài của token thị giác tại thời gian suy luận để nghiên cứu hành vi tổng quát hóa zero-shot của mô hình. Kết quả dưới LLaVA-NeXT được hiển thị trong Bảng 5. Ở đây LLaVA-NeXT-M3 được huấn luyện trên lưới hình ảnh 2×2 nhưng được đánh giá trên lưới 3×3. Chúng tôi đặt số lượng token thị giác là 144 trong mỗi hình ảnh trong quá trình đánh giá.

Bảng 5: So sánh hiệu suất của các cấu hình lưới hình ảnh khác nhau với LLaVA-NeXT-M3.

Mô hình có được cải thiện đáng kể trong hiểu tài liệu lần lượt là 2.12, 1.80, và 4.11 trên TextVQA, ChartQA, và DocVQA, trong khi duy trì cùng hiệu suất trên các tiêu chuẩn chủ yếu được tạo thành từ hình ảnh cảnh tự nhiên. Lưới hình ảnh 3×3 với 144 token mỗi lưới sở hữu 1440 token, nhưng đạt được hiệu suất tương tự với LLaVA-NeXT mặc định lưới hình ảnh 2×2 với 2880 token tổng cộng (576 token mỗi lưới). Điều này chỉ ra rằng việc đưa vào nhiều hình ảnh con hơn trong khi làm cho số lượng token thị giác trong mỗi hình ảnh con nhỏ hơn nhiều là đầy hứa hẹn.

4.5 Nghiên cứu Loại bỏ

Hình 5: Hình ảnh hóa lấy mẫu tuần tự và không gian. Cho lưới 24×24, các ô được hình ảnh hóa biểu thị các token được lấy mẫu.

Chúng tôi loại bỏ các thiết kế chính trong M3, bao gồm phương pháp lấy mẫu token thị giác Matryoshka, và chiến lược huấn luyện.

Lấy mẫu token thị giác Matryoshka. Ở đây chúng tôi so sánh ba cách khác nhau để chọn token thị giác cho Mô hình Đa phương tiện Matryoshka, bao gồm pooling trung bình, lấy mẫu không gian, và lấy mẫu tuần tự, được minh họa trong Hình 5. Được hiển thị trong Bảng 6, pooling trung bình cho thấy hiệu suất tốt hơn so với hai lựa chọn khác trên các tiêu chuẩn đa dạng. Nói chung, lấy mẫu tuần tự thực hiện tệ nhất. Chúng tôi đưa ra giả thuyết rằng điều này là do các token thị giác có thông tin không gian, trong khi lấy mẫu tuần tự không tự nhiên phù hợp với phân bố không gian của các token thị giác.

Bảng 6: Loại bỏ về lấy mẫu token thị giác Matryoshka bao gồm pooling trung bình, lấy mẫu tuần tự, và lấy mẫu không gian.

Huấn luyện toàn bộ LMM so với chỉ huấn luyện CLIP. Vì hành vi lồng nhau của token thị giác Matryoshka được học trong bộ mã hóa thị giác CLIP, tiếp theo chúng tôi đánh giá liệu có cần thiết phải fine-tune LLM hay không. Được hiển thị trong Bảng 7, huấn luyện toàn bộ LLM đạt được hiệu suất tốt hơn. Điều này chứng minh rằng bằng cách cũng huấn luyện LLM, mô hình có thể thích ứng tốt hơn với các mẫu của token thị giác được phân bố theo cách Matryoshka.

Bảng 7: So sánh hiệu suất huấn luyện LLaVA-NeXT-M3 có và không có huấn luyện LLM trên các tiêu chuẩn đa dạng. Chúng tôi thấy một sự giảm rõ ràng khi đóng băng LLM.

Như được giải thích trong Phần 3 và 4.1, chúng tôi (a) khởi tạo trọng số LLM từ LLaVA và (b) tối thiểu hóa mất mát được lấy trung bình trên tất cả các tỷ lệ token thị giác cho mỗi mẫu trong quá trình huấn luyện. Một lựa chọn thay thế là lấy mẫu ngẫu nhiên một tỷ lệ token thị giác. Được hiển thị trong Bảng 8, khởi tạo trọng số LLM từ LLaVA và tối thiểu hóa mất mát trên tất cả các tỷ lệ cho thấy cải thiện hiệu suất nhất quán so với sử dụng trọng số LLM chỉ văn bản vanilla được huấn luyện trước và chọn ngẫu nhiên một tỷ lệ token thị giác.

Bảng 8: Tác động của (a) khởi tạo trọng số LLM từ LLaVA, và (b) lấy trung bình mất mát từ tất cả các tỷ lệ so với chọn ngẫu nhiên một tỷ lệ cho mỗi mẫu trong quá trình huấn luyện.

Khởi tạo trọng số LLM từ LLaVA làm cho quá trình huấn luyện của M3 ổn định hơn. Bằng cách học tất cả các tỷ lệ cùng lúc, mô hình bị buộc phải học hành vi lồng nhau cho mỗi mẫu, dẫn đến hiệu suất tốt hơn.

5 Kết luận và Công việc Tương lai

Chúng tôi giới thiệu M3: Mô hình Đa phương tiện Matryoshka, học cách biểu diễn nội dung thị giác như các tập hợp token thị giác lồng nhau, nắm bắt thông tin qua nhiều mức độ chi tiết từ thô đến mịn. Các LMMs được trang bị M3 mang lại quyền kiểm soát rõ ràng mức độ chi tiết thị giác cho mỗi trường hợp thử nghiệm trong quá trình suy luận. Chúng tôi cũng cho thấy rằng M3 có thể phục vụ như một khung phân tích để điều tra mức độ chi tiết thị giác cần thiết cho các bộ dữ liệu hiện có, nơi chúng tôi phát hiện rằng một số lượng lớn các tiêu chuẩn đa phương tiện chỉ cần ít đến 9 token thị giác để có được độ chính xác tương tự như khi sử dụng tất cả token thị giác, đặc biệt đối với hiểu video. Hơn nữa, chúng tôi tiết lộ một khoảng cách lớn về hiệu suất-hiệu quả giữa giới hạn trên oracle và hiệu suất của mô hình.

Công việc của chúng tôi có thể được mở rộng tự nhiên sang các miền khác. Ví dụ, ngữ cảnh dài trong LLM chỉ văn bản hoặc token thị giác trong các tác vụ thị giác dày đặc cũng có thể được biểu diễn như các tập hợp token lồng nhau theo cách Matryoshka. Một hạn chế của cách tiếp cận hiện tại của chúng tôi là chúng tôi thiếu một bộ dự đoán token thị giác hiệu quả có thể thu hẹp khoảng cách giữa oracle và hiệu suất thực tế của LMM ở một tỷ lệ cụ thể. Chúng tôi tin rằng đây sẽ là một hướng nghiên cứu thú vị tiếp theo trong không gian này.

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi NSF CAREER IIS2150012, và các khoản tài trợ của Viện Lập kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2022-0-00871, Phát triển Tự chủ AI và Tăng cường Kiến thức cho Hợp tác Tác nhân AI) và (Số RS2022-00187238, Phát triển Công nghệ Mô hình Ngôn ngữ Lớn Hàn Quốc cho Huấn luyện Trước Hiệu quả), và Chương trình Nghiên cứu Mô hình Nền tảng Tăng tốc Microsoft.

Tài liệu tham khảo

[1] OpenAI. Thẻ hệ thống Gpt-4v(ision). https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023.

[2] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Điều chỉnh hướng dẫn thị giác. NeurIPS, 2023.

[3] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Tăng cường hiểu biết thị giác-ngôn ngữ với các mô hình ngôn ngữ lớn tiên tiến. ICLR, 2024.

[4] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, và Yong Jae Lee. Llava-next: Cải thiện suy luận, ocr, và kiến thức thế giới, Tháng 1 2024.

[5] Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee. Các baseline được cải thiện với điều chỉnh hướng dẫn thị giác, 2024.

[6] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, và Jie Tang. Cogvlm: Chuyên gia thị giác cho các mô hình ngôn ngữ được huấn luyện trước, 2023.

[7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren Zhou. Qwen-vl: Một mô hình thị giác-ngôn ngữ đa năng cho hiểu biết, định vị, đọc văn bản, và hơn thế nữa. arXiv preprint arXiv:2308.12966, 2023.

[8] Vicuna. Vicuna: Một chatbot mã nguồn mở gây ấn tượng với gpt-4 với chất lượng 90%* chatgpt. https://vicuna.lmsys.org/, 2023.

[9] Meta. Llama-3. https://ai.meta.com/blog/meta-llama-3/, 2024.

[10] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, và Li Yuan. Video-llava: Học biểu diễn thị giác thống nhất bằng cách căn chỉnh trước khi chiếu. arXiv preprint arXiv:2311.10122, 2023.

[11] Hang Zhang, Xin Li, và Lidong Bing. Video-llama: Một mô hình ngôn ngữ thị giác-âm thanh được điều chỉnh hướng dẫn cho hiểu video. arXiv preprint arXiv:2306.02858, 2023.

[12] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, và Chunyuan Li. Llava-next: Một mô hình hiểu video zero-shot mạnh mẽ, Tháng 4 2024.

[13] Đội Gemini. Gemini: Một họ các mô hình đa phương tiện có khả năng cao, 2024.

[14] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, và Judy Hoffman. Hợp nhất token: ViT của bạn nhưng nhanh hơn. Trong Hội nghị Quốc tế về Học Biểu diễn, 2023.

[15] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, và Baobao Chang. Một hình ảnh đáng giá 1/2 token sau lớp 2: Tăng tốc suy luận plug-and-play cho các mô hình ngôn ngữ thị giác lớn. arXiv preprint arXiv:2403.06764, 2024.

[16] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, và Yan Yan. Llava-prumerge: Giảm token thích ứng cho các mô hình đa phương tiện lớn hiệu quả. arXiv preprint arXiv:2403.15388, 2024.

[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Các đối tượng chung trong ngữ cảnh. Trong Thị giác Máy tính–ECCV 2014: Hội nghị Châu Âu lần thứ 13, Zurich, Thụy Sĩ, 6-12 Tháng 9, 2014, Thủ tục, Phần V 13, trang 740–755. Springer, 2014.

[18] Mike G Harris và Christos D Giachritsis. Thông tin hạt thô chiếm ưu thế so với thông tin hạt mịn trong việc đánh giá thời gian đến tiếp xúc từ dòng võng mạc. Nghiên cứu thị giác, 40(6):601–611, 2000.

[19] Jay Hegdé. Tiến trình thời gian của nhận thức thị giác: xử lý từ thô đến mịn và hơn thế nữa. Tiến bộ trong sinh học thần kinh, 84(4):405–439, 2008.

[20] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Học biểu diễn matryoshka. Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 35:30233–30249, 2022.

[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, và Devi Parikh. Làm cho v trong vqa có ý nghĩa: Nâng cao vai trò của hiểu hình ảnh trong trả lời câu hỏi thị giác. Trong Thủ tục của hội nghị IEEE về thị giác máy tính và nhận dạng mẫu, trang 6904–6913, 2017.

[22] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, và Ji-Rong Wen. Đánh giá ảo giác đối tượng trong các mô hình ngôn ngữ thị giác lớn. arXiv preprint arXiv:2305.10355, 2023.

[23] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Mô hình đa phương tiện của bạn có phải là một người chơi toàn diện không? arXiv preprint arXiv:2307.06281, 2023.

[24] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, và Marcus Rohrbach. Hướng tới các mô hình vqa có thể đọc. Trong Thủ tục của hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu, trang 8317–8326, 2019.

[25] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, và Enamul Hoque. ChartQA: Một tiêu chuẩn cho việc trả lời câu hỏi về biểu đồ với suy luận thị giác và logic. Trong Findings of the Association for Computational Linguistics: ACL 2022, trang 2263–2279, Dublin, Ireland, Tháng 5 2022. Association for Computational Linguistics.

[26] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023.

[27] OpenAI. Báo cáo kỹ thuật gpt-4. 2023.

[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. arXiv preprint arXiv:2302.13971, 2023.

[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Học các mô hình thị giác có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong Hội nghị quốc tế về học máy, trang 8748–8763. PMLR, 2021.

[30] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, và Yong Jae Lee. Làm cho các mô hình đa phương tiện lớn hiểu các lời nhắc thị giác tùy ý. Trong Hội nghị IEEE về Thị giác Máy tính và Nhận dạng Mẫu, 2024.

[31] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, và Ping Luo. Gpt4roi: Điều chỉnh hướng dẫn mô hình ngôn ngữ lớn trên vùng quan tâm. arXiv preprint arXiv:2307.03601, 2023.

[32] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, và Rui Zhao. Shikra: Giải phóng phép thuật đối thoại tham chiếu của llm đa phương tiện. arXiv preprint arXiv:2306.15195, 2023.

[33] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, và Furu Wei. Kosmos-2: Nền tảng các mô hình ngôn ngữ lớn đa phương tiện cho thế giới. arXiv preprint arXiv:2306.14824, 2023.

[34] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, và Jianwei Yang. Llava-grounding: Trò chuyện thị giác có căn cù với các mô hình đa phương tiện lớn, 2023.

[35] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, và Chuang Gan. 3d-llm: Tiêm thế giới 3d vào các mô hình ngôn ngữ lớn. NeurIPS, 2023.

[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học tàn dư sâu cho nhận dạng hình ảnh. Trong CVPR, 2016.

[37] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. Một hình ảnh đáng giá 16x16 từ: Transformers cho nhận dạng hình ảnh ở quy mô. ICLR, 2021.

[38] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, và Tom Duerig. Mở rộng quy mô học biểu diễn thị giác và ngôn ngữ-thị giác với giám sát văn bản có tiếng ồn. Trong Hội nghị quốc tế về học máy, trang 4904–4916. PMLR, 2021.

[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Huấn luyện trước các transformers hai chiều sâu cho hiểu ngôn ngữ. arXiv preprint arXiv:1810.04805, 2018.

[40] Xianming Li, Zongxi Li, Jing Li, Haoran Xie, và Qing Li. Nhúng câu matryoshka 2d. arXiv preprint arXiv:2402.14776, 2024.

[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, trang 5998–6008, 2017.

[42] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention với độ phức tạp tuyến tính, 2020.

[43] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: Transformer hiệu quả. Trong Hội nghị Quốc tế về Học Biểu diễn, 2020.

[44] Joakim Bruslund Haurum, Sergio Escalera, Graham W. Taylor, và Thomas B. Moeslund. Token nào để sử dụng? điều tra giảm token trong vision transformers. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu (ICCV) Workshops, Tháng 10 2023.

[45] Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, và Kai-Wei Chang. Matryoshka query transformer cho các mô hình ngôn ngữ thị giác lớn. arXiv preprint arXiv:2405.19315, 2024.

[46] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, và Lucas Beyer. Mất mát sigmoid cho huấn luyện trước hình ảnh ngôn ngữ. Trong Thủ tục của Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, trang 11975–11986, 2023.

[47] Drew A Hudson và Christopher D Manning. Gqa: Một bộ dữ liệu mới cho suy luận thị giác thế giới thực và trả lời câu hỏi tổng hợp. Trong CVPR, 2019.

[48] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, và Jeffrey P Bigham. Thách thức lớn vizwiz: Trả lời câu hỏi thị giác từ người mù. Trong Thủ tục của hội nghị IEEE về thị giác máy tính và nhận dạng mẫu, trang 3608–3617, 2018.

[49] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, và Ying Shan. Seed-bench: Điểm chuẩn llms đa phương tiện với hiểu biết tạo sinh. arXiv preprint arXiv:2307.16125, 2023.

[50] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, và Ashwin Kalyan. Học cách giải thích: Suy luận đa phương tiện thông qua chuỗi suy nghĩ cho trả lời câu hỏi khoa học. Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 2022.

[51] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, và Wenhu Chen. Mmmu: Một tiêu chuẩn hiểu và suy luận đa phương tiện đa ngành khổng lồ cho agi chuyên gia. Trong Thủ tục của CVPR, 2024.

[52] Minesh Mathew, Dimosthenis Karatzas, và CV Jawahar. Docvqa: Một bộ dữ liệu cho vqa trên hình ảnh tài liệu. Trong Thủ tục của hội nghị mùa đông IEEE/CVF về ứng dụng thị giác máy tính, trang 2200–2209, 2021.

[53] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, và Ali Farhadi. Một sơ đồ đáng giá một chục hình ảnh. Trong Bastian Leibe, Jiri Matas, Nicu Sebe, và Max Welling, editors, Computer Vision – ECCV 2016, trang 235–251, Cham, 2016. Springer International Publishing.

[54] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, và Yueting Zhuang. Trả lời câu hỏi video thông qua attention được tinh chỉnh dần dần trên diện mạo và chuyển động. Trong Thủ tục của hội nghị ACM quốc tế lần thứ 25 về Đa phương tiện, trang 1645–1653, 2017.

[55] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, và Dacheng Tao. Activitynet-qa: Một bộ dữ liệu cho việc hiểu video web phức tạp thông qua trả lời câu hỏi. Trong AAAI, tập 33, trang 9127–9134, 2019.

[56] Junbin Xiao, Xindi Shang, Angela Yao, và Tat-Seng Chua. Next-qa: Giai đoạn tiếp theo của trả lời câu hỏi để giải thích hành động thời gian. Trong Thủ tục của hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu, trang 9777–9786, 2021.

[57] Jiapeng Li, Ping Wei, Wenjuan Han, và Lifeng Fan. Intentqa: Suy luận ý định video có nhận thức ngữ cảnh. Trong Int. Conf. Comput. Vis., trang 11963–11974, 2023.

[58] Karttikeya Mangalam, Raiymbek Akshulakov, và Jitendra Malik. Egoschema: Một tiêu chuẩn chẩn đoán cho việc hiểu ngôn ngữ video dạng rất dài. Trong Adv. Neural Inform. Process. Syst., 2024.

[59] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, và Steven Hoi. Instructblip: Hướng tới các mô hình ngôn ngữ thị giác mục đích chung với điều chỉnh hướng dẫn, 2023.

[60] Wonkyun Kim, Changin Choi, Wonseok Lee, và Wonjong Rhee. Một lưới hình ảnh có thể đáng giá một video: Trả lời câu hỏi video zero-shot sử dụng vlm. arXiv preprint arXiv:2403.18406, 2024.

[61] Hang Zhang, Xin Li, và Lidong Bing. Video-LLaMA: Một mô hình ngôn ngữ thị giác-âm thanh được điều chỉnh hướng dẫn cho hiểu video. Trong Conf. Empirical Methods in Natural Language Processing, trang 543–553, 2023.

[62] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Yu Qiao. Llama-adapter: Fine-tuning hiệu quả của các mô hình ngôn ngữ với attention zero-init. arXiv preprint arXiv:2303.16199, 2023.

[63] Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan, và Fahad Shahbaz Khan. Video-chatgpt: Hướng tới hiểu video chi tiết thông qua các mô hình ngôn ngữ và thị giác lớn. ArXiv abs/2306.05424, 2023.

[64] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, và Li Yuan. Video-llava: Học biểu diễn thị giác thống nhất bằng cách căn chỉnh trước khi chiếu. ArXiv abs/2311.10122, 2023.

[65] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, và Yu Qiao. Internvideo: Các mô hình nền tảng video chung thông qua học tạo sinh và phân biệt. ArXiv abs/2212.03191, 2022.

[66] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, et al. Suy luận llm được tiết lộ: Khảo sát và cái nhìn sâu sắc về mô hình roofline. arXiv preprint arXiv:2402.16363, 2024.

[67] Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, và Guangyu Sun. Asvd: Phân tích giá trị đơn lẻ nhận thức kích hoạt để nén các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2312.05821, 2023.

A Tác động Rộng hơn

Tác động rộng hơn của M3, một khung với biểu diễn thị giác lồng nhau, có những lợi ích và rủi ro tiềm ẩn liên quan đến việc triển khai và phát hành. Mô hình của chúng tôi được huấn luyện sử dụng chính xác cùng kiến trúc và dữ liệu của LLaVA-1.5 và LLaVA-NeXT. Tất cả các mối quan tâm đều giống như LLaVA. Cụ thể, như một ví dụ, LLaVA thực hiện điều chỉnh hướng dẫn sử dụng dữ liệu được tạo ra bởi GPT-4 và GPT-4V. Thiên hướng từ GPT-4 và GPT-4V vẫn sẽ tồn tại trong LLaVA.

B Phân tích Hiệu quả

Để làm sáng tỏ những lợi ích tính toán mà M3 mang lại, chúng tôi sử dụng phân tích LLM-Viewer dựa trên roofline như được chi tiết trong [66]. Phân tích của chúng tôi được đặt trong một ngữ cảnh giả định được thiết kế để nhấn mạnh các hiệu ứng của M3 trên hiệu quả xử lý trong LMMs. Chúng tôi nghiên cứu trường hợp LLaVA-1.5 nơi một hình ảnh độ phân giải 336×336 được xử lý sử dụng bộ mã hóa hình ảnh CLIP-ViT, tạo ra 576 token thị giác. Đi kèm với một lời nhắc văn bản với số lượng token giả định là 30, các token thị giác lồng nhau trong M3 giảm đáng kể số lượng token thị giác. Hậu quả của việc giảm này là đáng kể như được nêu trong Bảng 9, chi tiết các chi phí tính toán liên quan đến quá trình prefill LMM. Đáng chú ý, M3 không chỉ tăng tốc độ của quá trình prefill LMM thông qua việc giảm các phép toán dấu phẩy động (FLOPs) mà còn giảm yêu cầu bộ nhớ tính toán.

Quan trọng là phải nhấn mạnh rằng những lợi thế của M3 không chỉ giới hạn ở việc cải thiện hiệu quả. Cách tiếp cận giảm token của M3 cũng có thể tăng cường các phương pháp tăng tốc LMM khác, như lượng tử hóa và phân tích nhân tử, như được tham chiếu trong [67]. Mối quan hệ bổ sung này nhấn mạnh tiềm năng rộng lớn của M3 để đóng góp vào một loạt các chiến lược tăng cường hiệu quả.

Bảng 9: Phân tích Chi phí Tính toán. Thiết bị phát triển là Tesla V100 GPU, và thời gian ước tính bởi mô hình roofline đại diện cho hiệu suất lý thuyết mà phần cứng có thể đạt được.

C Thêm Hình ảnh hóa về Biểu diễn Thị giác Lồng nhau

Được hiển thị trong Hình 6, với nhiều token thị giác hơn, LMMs có thể khám phá nhiều chi tiết hơn, như đồ nội thất và thuộc tính con người. Bên cạnh đó, LMMs có thể tạo ra các mô tả chất lượng cao hơn với nhiều token thị giác hơn, như được chứng minh bởi khả năng OCR trong Hình 6 (b).

Hình 6: Thêm ví dụ hình ảnh hóa. Với nhiều token thị giác hơn, LMMs có thể khám phá nhiều chi tiết hơn, và tạo ra các mô tả chất lượng cao hơn. Các hình ảnh từ tập xác thực MSCOCO.
