# 2312.08614.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2312.08614.pdf
# File size: 6191624 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Factorization Vision Transformer: Modeling Long
Range Dependency with Local Window Cost
Haolin Qin∗, Daquan Zhou∗, Tingfa Xu†, Ziyang Bian, Jianan Li†
Abstract —Transformers have astounding representational
power but typically consume considerable computation which is
quadratic with image resolution. The prevailing Swin transformer
reduces computational costs through a local window strategy.
However, this strategy inevitably causes two drawbacks: (1)
the local window-based self-attention hinders global dependency
modeling capability; (2) recent studies point out that local
windows impair robustness. To overcome these challenges, we
pursue a preferable trade-off between computational cost and
performance. Accordingly, we propose a novel factorization self-
attention mechanism (FaSA) that enjoys both the advantages of
local window cost and long-range dependency modeling capabil-
ity. By factorizing the conventional attention matrix into sparse
sub-attention matrices, FaSA captures long-range dependencies
while aggregating mixed-grained information at a computational
cost equivalent to the local window-based self-attention. Leverag-
ing FaSA, we present the factorization vision transformer (FaViT)
with a hierarchical structure. FaViT achieves high performance
and robustness, with linear computational complexity concerning
input image spatial resolution. Extensive experiments have shown
FaViT’s advanced performance in classification and downstream
tasks. Furthermore, it also exhibits strong model robustness to
corrupted and biased data and hence demonstrates benefits in
favor of practical applications. In comparison to the baseline
model Swin-T, our FaViT-B2 significantly improves classification
accuracy by 1%and robustness by 7%, while reducing model
parameters by 14% . Our code will soon be publicly available at
https://github.com/q2479036243/FaViT.
Index Terms —Transformer, factorization, long-range depen-
dency, local window, model robustness.
I. I NTRODUCTION
SINCE the great success of Alexnet [1], revolutionary
improvements have been achieved through scaling the
model size to a larger scale with several advanced train-
ing recipes [2]. With the aid of AutoML [3], convolutional
neural networks (CNNs) have achieved state-of-the-art per-
formance across various computer vision tasks [4], [5]. On
the other hand, recently popular transformers have shown
superior performance over previously dominant CNNs [6], [7].
The fundamental difference between transformers and CNNs
resides in their aptitude for modeling long-range dependency.
Conventional vision transformers divide input images into
sequences of patches, processed concurrently, thereby yielding
Haolin Qin, Tingfa Xu, Jianan Li and Ziyang Bian are with Beijing Institute
of Technology, 100081 Beijing, China. E-mail: {3120225333, ciom xtf1,
lijianan, 3220185049 }@bit.edu.cn.
Tingfa Xu is also with Beijing Institute of Technology Chongqing Innova-
tion Center, 401135 Chongqing, China.
Daquan Zhou is with ByteDance TikTok, Singapore.
Ziyang Bian is also with North China Research Institute of Electro-Optics,
100015 Beijing, China.
∗Equal contribution.†Correspondence to: Tingfa Xu and Jianan Li.
Fig. 1. Comparison of the attention span visualization results of ViT, Swin,
and our FaViT. (a) ViT enjoys a global attention span but is computationally
intensive. (b) Swin transformer is efficient but inferior in modeling long-range
dependency. (c) The proposed FaViT factorizes tokens and hence successfully
models long-range dependency at local window cost.
a quadratic increase in computational cost with respect to
input spatial resolution. As a result, transformers consume
significantly higher computational costs compared to CNNs,
limiting their feasibility in resource-constrained devices.
To solve the above problem, several methods have been
proposed to alleviate the computational burden. For example,
the Swin transformer [8] adopts a local window strategy
where tokens are divided into several windows, and the
self-attention calculation is confined within these predefined
windows. Consequently, the computational cost changes to be
quadratic with the window size, which is intentionally set to
be significantly smaller than the input image spatial resolution.
Compared with conventional vision transformer (ViT) [9],
the Swin transformer reduces costs significantly. Nonetheless,
this local window strategy inevitably impairs the long-range
dependency modeling capability. As illustrated in Figure 1 (b),
the Swin transformer exclusively captures relationships within
a local area, leading to the loss of long-range dependency.
Additionally, the damage of this strategy to model robustness
has been demonstrated by recent investigations [10].
The trade-off between the computational cost and long-
range dependency modeling capability thus becomes a fun-arXiv:2312.08614v1  [cs.CV]  14 Dec 2023

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
Fig. 2. The factorization with multiple dilation rates. We factorize the
conventional attention matrix into several sub-attention matrices and obtain
long-range and mixed-grained information with computational complexity
linearly with respect to the image resolution.
damental challenge yet to be explored. In this paper, we
propose a novel self-attention mechanism termed factorization
self-attention (FaSA). The core operation of FaSA lies in the
factorization process, illustrated in Figure 2. We factorize the
conventional attention matrix into several sparse sub-attention
matrices such that the super-position of sub-attention matrices
effectively approximates the original full attention matrix.
In practical terms, given an input image, we take each indi-
vidual point as a query. For gathering keys, we evenly divide
the image into a series of non-overlapping local windows.
Subsequently, we uniformly sample a fixed number of points
from each window through dilated sampling and fuse the
features of the sampled points at the same position in different
windows. Since the number of keys is strictly limited and
each key incorporates information spanning various windows
across the entire image, attending to such a set of keys
enables modeling long-range dependency at local window
cost. Considering that each obtained key amalgamates multi-
point information, potentially resulting in a deficiency of fine-
grained details, we further introduce the mixed-grained multi-
head attention. Concretely, we incrementally increase the local
window size for different heads and dynamically adjust the
dilation rate of point sampling to keep the number of keys
across all heads the same. As a result, the obtained keys
incorporate features from fewer positions and have more fine-
grained details without adding additional computing costs.
By aggregating the attended features from multiple heads,
long-range and mixed-grained information can be obtained
simultaneously.
Based on the proposed FaSA, we present variants of the
transformer with different capacities termed factorization vi-
sion transformer (FaViT). As shown in Figure 1 (c), our
FaViT enjoys two essential advantages, stemming from the
innovation of FaSA, which are previously unattainable in
existing transformers. Firstly, each local window generates an
identical number of tokens, ensuring a fixed computational
cost. Consequently, the capture of long-range dependency tran-
spires without incurring supplementary overhead. Secondly,
Fig. 3. Comparison of the accuracy with respect to robustness trade-off. The
proposed FaViT achieves the best performance in both classification accuracy
and model robustness, with fewer parameters which are indicated by the circle
sizes.
the incorporation of sub-attention matrices within FaViT fos-
ters the aggregation of information at varied granularities.
Extensive experiments corroborate the exceptional perfor-
mance and superior robustness of the proposed FaViT. As de-
picted in Figure 3, FaViT consistently outperforms competing
models of similar sizes in terms of classification accuracy and
robustness. Notably, in comparison to the baseline model Swin
transformer, FaViT-B2 outperforms Swin-T in all aspects. The
robustness is significantly improved by 7%, and the classifica-
tion accuracy is improved by 1%, while the parameters drop
by a considerable 14%. Furthermore, our FaViT also exhibits
impressive performance on multiple downstream tasks such as
object detection and semantic segmentation.
To sum up, this work makes the following contributions:
•We propose a novel factorization self-attention mecha-
nism (FaSA), which is capable of modeling long-range
dependency while aggregating mixed-grained information
at local window cost.
•Based on FaSA, we present an efficient factorization
vision transformer (FaViT), which finds the trade-off
between cost and performance and exhibits state-of-the-
art robustness.
II. R ELATED WORK
A. Vision Transformers
Transformer is originally developed for NLP tasks [11],
[12], but has now achieved remarkable success across multiple
domains [13]–[16]. In the computer vision tasks, ViT [9] is the
pioneering attempt to adapt transformers for image processing.
It splits the input image into a sequence of tokens for encoding
and constructs a convolution-free network structure through
self-attention. DeiT [17] introduces a series of training strate-
gies to make ViT work on the smaller dataset ImageNet-1K,
replacing the large-scale JFT-300M previously used. Building

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
upon ViT’s foundation, subsequent endeavors [18]–[20] refine
its architecture to achieve enhanced performance. For instance,
Yu et al. [21] abstracted the general MetaFormer architecture
and explored the sources of competitiveness of transformers
and their variants. Jiao et al. [22] used sparse receptive fields
to reduce global redundancy and improve transformer feature
expression capabilities. In essence, the transformative impact
of transformers has been extended to the domain of computer
vision and gradually replaced the dominance of CNN.
B. Efficient Variants
A recent surge of methods has emerged to mitigate the
computational demands of vision transformers. These meth-
ods can be categorized into two strategies according to the
relationship between computational complexity and the input
image spatial resolution: (1) pyramid-based self-attention; and
(2) local window-based self-attention. PVTv1 [23] serves as a
representative example of the first strategy. It introduces spatial
reduction and tokens fusion to reduce the cost of multi-head
attention. The subsequently proposed PVTv2 [24] improves
it by introducing overlapping patch embedding, depth-wise
convolution [25], and linear Spatial Reduction Attention. A
shunted self-attention [26] comes up to unify multi-scale
feature extractions via multi-scale token aggregation. The
key point of this strategy is to reduce cost through spatial
compression. Therefore, the computational complexity of the
aforementioned pyramid-based self-attention models remains
essentially quadratic with the image resolution. The high
computational cost problem still exists when processing high-
resolution images. Meanwhile, the information of small targets
and delicate textures will be overwhelmed, destroying the fine-
grained features.
C. Local Self-attention
The local window-based self-attention divides the image
into distinct subsets and confines self-attention computations
within each subset. Therefore, the computational complexity
of models using local windows is theoretically linear with the
input image spatial resolution. Among these models, The most
Swin transformer [8] stands out as the most prominent. It
introduces a hierarchical sliding window structure and utilizes
the Shift operation to exchange information across subsets.
MOA [27] exploits the neighborhood and global information
among all non-local windows. SimViT [28] integrates spatial
structure and cross-window connections of sliding windows
into the visual transformer. While the local window strategy
significantly reduces the computational cost, its drawbacks
cannot be ignored. The absence of long-range dependency
restricts representational capacity, while excessive local win-
dows will compromise model robustness. Previous works are
unable to restore long-range dependency while maintaining
the cost. These issues drive us to propose FaViT for modeling
long-range dependency with the local window-based self-
attention computational cost.
D. Model Robustness
Compared with CNNs, transformers normally have stronger
model robustness against various corruptions and biases, dueto the utilization of the self-attention mechanism [29], [30].
However, some transformer-based variants use strategies in-
tended to enhance performance and reduce cost, which in-
advertently damage model robustness [10]. Hence, we have
integrated model robustness as a fundamental criterion within
our performance evaluation framework. Robustness evalua-
tion no longer relies solely on clean image datasets such
as ImageNet-1K [31]. Instead, the assessment is based on
ImageNetC which denotes the corrupted images derived from
the original dataset in a manner akin to the approach proposed
in [32]. In addition, the model’s performance against label
noise and class imbalance also serves as an indicator of its
robustness. This perspective is endorsed by Li et al. [33], who
employed images with real-world noisy labels, and Tian et al.
[34] used a long-tailed distribution dataset to analyze model
robustness through classification accuracy. Drawing inspiration
from these contributions, our comprehensive evaluation frame-
work gauges the model’s robustness across image corruption,
label noise, and class imbalance scenarios.
E. Matrix Factorization
Recently, the construction of sparse matrices to reduce
network redundancy and enhance feature extraction efficiency
has emerged as a promising approach across various tasks
[35]–[37]. This approach has inspired the formulation of
strategies aimed at reducing transformer computational costs.
One notable instance is VSA [38], which introduces a window
regression module to predict the attention region where key
and value tokens are sampled. This category of methods
endeavors to find sparse tokens that can effectively substitute
all tokens. Besides that, dilated sampling is also an effective
strategy for achieving sparse representation [39]–[41]. For ex-
ample, Jiao et al. [22] devised DilateFormer, which introduces
an additional dilation rate in the sliding window of the Swin
transformer. These methods leverage dilated convolutions to
extract sparse feature maps, thereby expanding the receptive
field while reducing computational costs. However, dilated
sampling will lose information continuity, which is neglected
by the above methods, resulting in the loss of fine-grained
features. To this end, this paper proposes FaSA to achieve at-
tention matrix factorization and obtain long-range and mixed-
grained information simultaneously.
III. M ETHOD
In this section, we present a comprehensive overview of the
proposed factorization vision transformer (FaViT). In Section
III-A, we review the fundamentals of the transformer. We
provide a main symbol comparison table facilitating a clearer
understanding of the main differences between FaViT and
existing models. Section III-B is dedicated to the elucidation
of our core innovation, the factorization self-attention (FaSA).
This novel mechanism empowers FaViT to model long-range
dependency at the local window cost. In Section III-C, we
describe the overall framework and model variants of the
FaViT. Section III-D is dedicated to a meticulous analysis of
FaViT’s computational complexity.

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
Fig. 4. Overall architecture of the proposed FaViT. We divide the attention heads into multiple groups and assign increasing dilation rates. Each group is
evenly partitioned into non-overlapping local windows and gathers the same number of keys via dilated sampling to obtain the long-range and mixed-grained
information simultaneously.
A. Preliminaries
Given the input image I∈RH×W×3, where H,Wrepre-
sent its height and width respectively. The conventional self-
attention mechanism (SA) employed in ViT first encodes it
into a feature map X∈RN×Cby patch embedding, where
N = H ×Wrepresents the input image spatial resolution
andCrepresents its channel dimensions. Subsequently, SA
employs linear embeddings, parameterized by weight matrices
WK,WQ,WV∈RC×C, to embed all the points into key
K=WKX∈RN×C, query Q=WQX∈RN×C, and value
V=WVX∈RN×C, respectively. After that, the attention
feature map is generated as indicated by the following equa-
tion:
SA(X) =Softmax (QK⊤
√
h)V, (1)
where√
his a scaling factor. As a result, the computational
complexity of ViT is given by:
Ω(ViT) = 4NC2+ 2N2C, (2)
and this complexity grows quadratically in tandem with the
increase in spatial resolution of the input image. Finally,
transformers transform the attended features via adopting a
feedforward layer, generating the ultimate feature maps that
are tailored for specific visual tasks. Notably, most feedfor-
ward layers are based on Multilayer Perceptron (MLP), usually
consisting of two linear layers and a GELU layer.
The Swin transformer adopts local window-based self-
attention (WSA) to reduce the computational cost. WSA
divides the feature map Xinto non-overlapping windows
and performs self-attention calculations within each windowin isolation. Suppose each window contains M×Mtokens,
the computational complexity of the Swin transformer can be
expressed as:
Ω(Swin) = 4NC2+ 2M2NC. (3)
Notably, this complexity exhibits linear growth with respect
to the input resolution for a fixed M.
A comparison between Equation 2 and Equation 3 high-
lights that SA enjoys a global attention span but becomes com-
putationally intensive for high input resolutions. In contrast,
WSA achieves higher efficiency while sacrificing the abil-
ity to model long-range dependency, consequently affecting
performance and robustness. To address these limitations, the
Swin transformer introduces the Shift operation to facilitate
information interaction among adjacent windows. However,
the Shift operation is employed exclusively between WSA
blocks, and expanding the modeling range necessitates the
connection of multiple blocks in series. The above constraints
prompt us to explore a novel self-attention mechanism that can
effectively model long-range dependency while adhering to the
computational advantages of local window strategy. It’s worth
noting that the main symbols used throughout this explanation
are summarized in Table I, providing a reference for the core
concepts introduced.
B. Factorization Self-attention
Figure 4 provides a visual representation of the proposed
factorization self-attention mechanism (FaSA). The essence
of FaSA lies in the factorization of the conventional atten-
tion matrix into multiple sparse sub-attention matrices. This

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
TABLE I
MAINSYMBOL COMPARISON TABLE .
Typeset Symbol Description Typeset Symbol Description
I Input image X Embedding feature
K Key tokens Q Query tokens
V Value tokens X′Attention feature
F Output feature WKKey linear embedding
H Image height W Image width
N Image spatial resolution C Image channel dimension
G Group number M2Window number
h Scaling factor Xi Group feature
Ki Group key X′
iGroup attention feature
WK
iGroup key embedding Pi Group sampling points
C′Group channel dimension Di Group dilation rate
Si Group window size Mi Group window number
hi Group scaling factor Fi Stage output
Xj
iWindow feature Pj
iWindow sampling points
Ω Computational complexity O Positive correlation
factorization methodology fosters the integration of features
across diverse granularities, culminating in an approxima-
tion of the feature representation capacity exhibited by the
complete attention matrix. Importantly, this approximation is
achieved while adhering to the linear relationship between
computational complexity and the spatial resolution of the
input image.
Specifically, we first uniformly divide the input feature map
X∈RN×Cinto multiple groups. Each group independently
undergoes a self-attention process aimed at capturing long-
range features at a specific granularity level. Regarding the
self-attention in each group, we take all points of the feature
map as query tokens and gather key tokens in three steps:
1)Localization , evenly divide the entire feature map into a
series of local windows.
2)Dilated sampling , uniformly sample a fixed number of
points in each local window.
3)Cross-window fusion , fuse the features of the sampled
points at the same position in different windows.
Hence the resulting fused key tokens encapsulate long-range
information from multiple windows spanned across the entire
feature map.
To counter the potential loss of fine-grained details in the
generated key tokens due to matrix sparsity, we introduce
mixed-grained multi-head attention. This involves a progres-
sive increase in the local window size across distinct head
groups, coupled with an adaptive escalation in the correspond-
ing dilation rate for point sampling. This step ensures a consis-
tent count of sampled points despite the window size changes,
resulting in key tokens with mixed-grained information. Next,
we delve into the principle and implementation of each step
in FaSA.
1) Head Grouping: We initiate the process by uniformly
partitioning the input feature map Xinto multiple groups
along the channel dimension:
X={Xi∈RN×C′
,|i= 1,···,G}, (4)
where Gis the number of head groups, Xiis the feature
map assigned to the i-th group, and C′= C/G. We take thedivided features as inputs for distinct attention head groups,
each executing factorization self-attention independently. This
isolation facilitates the capture of long-range information
across various granularities.
2) Gathering Queries: The gathering of query ( Q), key (K)
and value ( V) is executed individually within each attention
head group. For the i-th head group, we treat each point of
Xias a query and obtain query features as:
Qi=WQ
iXi∈RN×C′
, (5)
whereWQ
i∈RC′×C′
symbolizes a learnable linear embed-
ding achieved through a 1×1convolution operation. The
resulting query matrix Qicorresponds to the query features
within the i-th head group.
3) Gathering Keys: The acquisition of keys largely deter-
mines the attention span and computational cost of a self-
attention mechanism. In order to strike a balance between
modeling long-range dependency and retaining the local win-
dow cost, we gather keys in the following three steps.
Step 1: Local Windowing.
We commence by uniformly partitioning Xiinto multiple
non-overlapping local windows using a sliding window strat-
egy. For the sake of simplicity, we assume W = H , signifying
that each local window possesses equal length and width
denoted as Si. This can be mathematically expressed as:
Xi={Xj
i∈RSi×Si×C′
,|j= 1,···,Mi}. (6)
Here Mi= H/Sistands for the number of local windows
within the i-th attention head group. Xj
icorresponds to the
feature map of a specific local window. Notably, we gradually
enlarge the size of each local window across various head
groups, resulting in decreasing Mi.
Step 2: Dilated Sampling.
Subsequently, we proceed to uniformly sample a fixed num-
ber of M×Mpoints within each local window. In practice,
we set Mto7by default. For the i-th head group, the sampled
point set Piis represented as:
Pi={Pj
i∈RM×M×C′
,|j= 1,···,Mi}, (7)
wherePj
idenotes the sampled points for the j-th local window
in the i-th group. Since different head groups have distinct
local window sizes, it is critical to keep the number of points
sampled across local windows in different head groups the
same. In practice, we employ a concise but efficient dilated
sampling with increasing dilation rates across head groups to
achieve this objective. The corresponding dilation rate for the
i-th head group Diis computed as:
Di= (S i−1)/(M−1). (8)
Consequently, the sampled points are uniformly distributed
within each local window. As the head group index iincreases,
the interval between sampled points widens, leading to the
coarser granularity of information.
Step 3: Cross-window Fusion.
Previous studies have demonstrated that confining self-
attention exclusively within a local window detrimentally
affects modeling capability and robustness, primarily due to

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
TABLE II
ARCHITECTURE VARIANTS OF FAVIT.Pi,Ci,Hi,Ei,Bi,ANDDiINDICATE PATCH SIZE ,FEATURE DIMENSION ,NUMBER OF HEADS ,EXPANSION RATIO
OFMLP, NUMBER OF BLOCKS ,AND DILATION RATE SET RESPECTIVELY .
Output Size Layer Name FaViT-B0 FaViT-B1 FaViT-B2 FaViT-B3
Stage 1H
4×W
4Linear Embedding P1= 4; C 1= 32 P1= 4; C 1= 64 P1= 4; C 1= 96
FaSAH1= 1 E 1= 8
B1= 2 D 1= [1,8]H1= 1 E 1= 8
B1= 2 D 1= [1,8]H1= 1 E 1= 8
B1= 2 D 1= [1,8]H1= 1 E 1= 8
B1= 2 D 1= [1,8]
Stage 2H
8×W
8Linear Embedding P2= 2; C 2= 64 P2= 2; C 2= 128 P2= 2; C 2= 192
FaSAH2= 2 E 2= 6
B2= 2 D 2= [1,4]H2= 2 E 2= 6
B2= 2 D 2= [1,4]H2= 2 E 2= 6
B2= 3 D 2= [1,4]H2= 2 E 2= 6
B2= 3 D 2= [1,4]
Stage 3H
16×W
16Linear Embedding P3= 2; C 3= 128 P3= 2; C 3= 256 P3= 2; C 3= 384
FaSAH3= 4 E 3= 4
B3= 6 D 3= [1,2]H3= 4 E 3= 4
B3= 6 D 3= [1,2]H3= 4 E 3= 4
B3= 18 D 3= [1,2]H3= 4 E 3= 4
B3= 14 D 3= [1,2]
Stage 4H
32×W
32Linear Embedding P4= 2; C 4= 256 P4= 2; C 4= 512 P4= 2; C 4= 768
FaSAH4= 8 E 4= 4
B4= 2 D 4= [1]H4= 8 E 4= 4
B4= 2 D 4= [1]H4= 8 E 4= 4
B4= 3 D 4= [1]H4= 8 E 4= 4
B4= 3 D 4= [1]
the absence of cross-window information interaction [8]. To
overcome this limitation while simultaneously reducing the
number of key tokens, we introduce an innovative cross-
window fusion strategy. Specifically, we first perform feature
embedding for each sampled point.
K′
i=WK
iPi∈RMi×M2×C′
, (9)
V′
i=WV
iPi∈RMi×M2×C′
, (10)
whereWK
i,WV
i∈RC′×C′
denote learnable linear embed-
dings for the i-th group implemented by two separate 1×1
convolutions. K′
iandV′
iexpress the key and value in the i-th
group. Subsequently, we fuse the features of sampled points
situated at the same positions in different windows to obtain
the final key and value features:
Ki=σ(K′
i)∈RM2×C′
, (11)
Vi=σ(V′
i)∈RM2×C′
, (12)
where σ(·)is a symmetric aggregation function. Hereby we
implement it as a simple form, i.e., maximum pooling.
In the above process, the full attention matrix is factorized
into several sparse sub-attention matrices. Each fused feature
benefits from enriched long-range information, achieved by
amalgamating the features of Mipoints distributed uniformly
across the entire feature map. Moreover, as the head group
index iincreases, the value of Midecreases, thereby enabling
the fused feature to gather information from fewer positions.
Consequently, it captures multiple granularity details. As a
result, these sub-attention matrices can effectively approximate
the feature representation performance of the full attention
matrix.
4) Mixed-grained Multi-head Attention: Given the gathered
queries and keys for each head group, we perform self-
attention individually:
X′
i=Softmax (QiK⊤
i√hi)Vi∈RH×W×C′
, (13)where√hidenotes a scaling factor. Subsequently, we amal-
gamate the attended features from all head groups to obtain
the final output.
X′=δ(X′
i,|i= 1,···,G)∈RH×W×C, (14)
where δ(·)represents the concatenation operation along the
feature channel dimension. Consequently, FaSA is capable of
modeling long-range dependency while aggregating mixed-
grained information.
C. Overall Architecture
Building upon the FaSA mechanism elucidated above, we
propose the factorization vision transformer (FaViT) depicted
in Figure 4. Specifically, FaViT first feeds the input image
Iinto a patch embedding layer to generate the feature map
X. Typically, patch embedding is executed using convolution
to form non-overlapping patches. However, taking inspiration
from recent works [26], [42], this paper employs convolutional
layers with a kernel size of 7×7and a stride of 2to
generate overlapping patches. Subsequently, a non-overlapping
projection layer with a stride of 2 is employed to create the
input feature map Xwith size ofH
4×W
4.
Following previous designs [8], [23], we adopt a four-stage
architecture to process Xand generate hierarchical feature
mapsF. The feature maps at each stage are denoted as
F={Fi,|i= 1,2,3,4}, where Fi∈RH
2i+1×W
2i+1×(C×2i−1).
These stages are interconnected by linear embedding layers,
implemented using convolutional layers with a stride of 2.
These layers reduce the size of the feature maps by half and
double their dimension compared to the previous stage, which
are then fed into the subsequent stage.
For each stage, we first stack multiple FaSA layers. The
FaSA, as detailed above, employs local window and dilated
sampling to obtain sparse sub-attention matrices. These sub-
attention matrices facilitate the aggregation of information at
multiple granularities through cross-window fusion. Conse-
quently, they possess similar feature expression capabilities

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
to the full attention matrix while simultaneously reducing
computational costs. The calculated attention feature map X′
is then fed into a feedforward layer to generate the output
feature map Fiof the current stage. This feedforward layer is
implemented as a Multilayer Perceptron (MLP), following the
approach of previous work [26].
Based on the above network structure, we present four
model variants of FaViT with distinct model sizes by adopting
different parameter settings. More details of the model archi-
tecture can be found in table II.
D. Complexity Analysis
Our FaViT aims to achieve high performance and robustness
with computational complexity linearly with respect to the
input image spatial resolution. Since self-attention is computed
within each local window, the computational complexity of
FaViT is quadratic with respect to the window size and linear
with respect to the image resolution. This complexity can be
expressed as:
Ω(FaSA) = 4NC2+ 2M2NC, (15)
where Mis a preset fixed value, thus the computational com-
plexity of FaViT is O(N). In contrast, ViT has a complexity
ofO(N2), and pyramid-based architectures have complexi-
ties of O(N2/θ), where θis an attenuation coefficient. The
complexity of FaViT is theoretically significantly lower than
the above methods. FaViT should naturally be classified into
the Swin transformer category with higher performance and
robustness. As a result, FaSA is capable of modeling long-
range dependency at a computational cost equivalent to local
window-based self-attention.
IV. E XPERIMENTS
The effectiveness and generalizability of our proposed
FaViT have been evaluated across multiple computer visions.
These evaluations encompass image classification, object de-
tection, and semantic segmentation tasks. Furthermore, we
assess FaViT’s robustness through experimentation involving
image corruptions, label noise, and class imbalance challenges.
Ablation studies are provided to validate our design choices.
A. Image Classification
Data and setups. We evaluate the proposed FaViT variants
on the classification dataset ImageNet-1K [68]. For a fair
comparison, we follow the same training strategies with priors
[24]. We use the AdamW optimizer with 300epochs including
the initial 10warm-up epochs and the final 10cool-down
epochs. A cosine decay learning rate scheduler is applied,
reducing the learning rate by a factor of 10every 30epochs,
beginning from a base learning rate of 0.001. Weight decay is
set to 0.05, and input images are resized to 224×224. Data
augmentations and regularization methods align with those
employed in [24]. We use a mini-batch size of 128 samples and
leverage the computational power of 8 NVIDIA 3090 GPUs
for training.
Main results. Table III demonstrates the remarkable per-
formance of the proposed FaViT variants. Specifically, theTABLE III
CLASSIFICATION RESULTS ON IMAGE NET-1K. A LL MODELS ARE
TRAINED FROM SCRATCH USING THE SAME TRAINING STRATEGIES .
Model Size Method #Param FLOPs Top-1 Acc (%)
Tiny
ModelPVTv2-B0 [24] 4 M 0.6 G 70.5
FaViT-B0 (Ours) 3 M 0.6 G 71.5
Small
ModelResNet18 [43] 12 M 1.8 G 69.8
PVTv1-T [23] 13 M 1.9 G 75.1
PVTv2-B1 [24] 14 M 2.1 G 78.7
Mobile-Former [44] 14 M 1.0 G 79.3
FaViT-B1 (Ours) 13 M 2.4 G 79.4
Medium
ModelDeiT-S [17] 22 M 4.6 G 79.9
Distilled DeiT-S [17] 22 M 4.6 G 81.2
NesT-T [45] 17 M 5.8 G 81.3
T2T-ViT-14 [46] 22 M 5.2 G 81.5
ViL-S [47] 25 M 5.1 G 82.0
CrossViT-15 [48] 27 M 5.8 G 81.5
TNT-S [49] 24 M 5.2 G 81.5
DW-ViT-T [50] 30 M 5.2 G 82.0
DW-Conv-T [51] 24 M 3.8 G 81.3
GG-T [52] 28 M 4.5 G 82.0
CoAtNet-0 [53] 25 M 4.2 G 81.6
VSA [38] 29 M 4.6 G 82.3
ConvNeXt-T [54] 29 M 4.5 G 82.1
MViTv2-T [55] 24 M 4.7 G 82.3
ViTAEv2-S [56] 20 M 5.4 G 82.2
CrossFormer [57] 31 M 4.9 G 82.5
Swin-T [8] 29 M 4.5 G 81.3
FaViT-B2 (Ours) 24 M 4.5 G 82.3
Large
ModelLV-ViT-M [18] 56 M 16.0 G 84.1
SimViT-M [28] 51 M 10.9 G 83.3
TNT-B [49] 66 M 14.1 G 82.8
BoTNet-S1-59 [58] 34 M 7.3 G 81.7
Focal-S [59] 51 M 9.4 G 83.6
RSB-101 [60] 45 M 7.9 G 81.3
Shuffle-B [61] 88 M 15.6 G 84.0
Deep-ViT-L [62] 55 M 12.5 G 83.1
PoolFormer-M36 [21] 56 M 8.8 G 82.1
MetaFormer [21] 57 M 12.8 G 84.5
PVTv2-B3 [24] 45 M 6.9 G 83.2
MOA-S [27] 39 M 9.4 G 83.5
ConvNeXt-S [54] 50 M 8.7 G 83.1
MSG-S [63] 56 M 8.4 G 83.4
MPViT-B [64] 75 M 16.4 G 84.3
RepLKNet-31B [65] 79 M 15.3 G 83.5
V AN-B4 [66] 60 M 12.2 G 84.2
Next-ViT-L [67] 58 M 10.8 G 83.6
DilateFormer [22] 47 M 10.0 G 84.4
CrossFormer-L [57] 92 M 16.1 G 84.0
Swin-S [8] 50 M 8.7 G 83.0
FaViT-B3 (Ours) 48 M 8.5 G 83.4
lightweight FaViT-B0 and FaViT-B1 achieve classification
Top-1 accuracies of 71.5%and79.4%respectively, which
outperforms previous state-of-the-art models with similar num-
bers of parameters. Moreover, when compared to the baseline
Swin Transformer [8], FaViT variants consistently exhibit
superior performance with fewer parameters and FLOPs. One
striking example is FaViT-B2, which attains an impressive ac-

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
Fig. 5. Visualizing the attention spans for a given token area (red box) by
different models. (a) Original image with selected area that is marked by a
red box. (b) ViT has global attention span but high computation cost. (c)
Swin restricts the attention span to a local window. (d) FaViT achieves global
attention span at local window cost.
curacy of 82.3%, surpassing Swin-T by 1%while significantly
reducing the parameters by 14%. This exceptional accuracy
and efficiency can be attributed to FaViT’s unique ability to
model long-range dependency at the local window cost.
Visualization. FaViT achieves high classification accuracy due
to its attention span across both short and long ranges. As
shown in Figure 5, we visualize the attention spans from the
first and the second stages of different models for a given
token area. The baseline model Swin transformer [8] focuses
only on the small neighborhood of the token, which leads to
degraded accuracy and robustness. In contrast, ViT [9] has
a longer-range attention span owing to the adopted global
attention mechanism, but at the cost of quadratically increased
computational complexity with the increase of input image
spatial resolution. The proposed FaViT well takes the comple-
mentary strengths of both models. It enjoys a large attention
span similar to ViT while maintaining a computational cost
equivalent to the local window-based Swin Transformer. This
balanced approach achieves an ideal trade-off between the
capability to model long-range dependency and computational
cost.
Efficiency analysis. We compare the computational complex-
ity of the proposed FaViT with other priors in Figure 6 by
reporting the number of FLOPs under varying input image
sizes. When the input image size is small, e.g.,224×224, all
the models have a comparable number of FLOPs. However,
as the input image size increases, we observe significant
differences. The conventional DeiT [17] suffers a dramatic
rise in FLOPs as the image size grows, as its computational
complexity is quadratic with respect to the image size. This
limitation restricts its applicability in real-world scenarios.
Although PVTV2 [24] has managed to slow down the rate of
FLOP increase, it still fundamentally exhibits quadratic growth
in computational complexity. In contrast, FaViT’s FLOPs
increase linearly with the image size due to its adoption of a
local window-based self-attention strategy. This linear growth
makes FaViT a more scalable and efficient choice for handling
larger input images.
The computational superiority of FaViT becomes particu-
Fig. 6. Diagram for the input image size vs FLOPs. The FLOPs and
Throughputs of PVTv2 and FaViT at high resolution are listed at the upper
left corner.
larly evident when dealing with large input images. Specif-
ically, when the image size reaches 1000×1000 pixels,
FaViT-B2 exhibits a substantial advantage in terms of FLOPs
compared to DeiT-S and PVTv2-B2, with only 25% and50%
of their computational cost, respectively. In comparison with
Swin-T [8], FaViT-B2 achieves the same computational cost
while delivering higher accuracy. Furthermore, we report the
throughputs (imgs/s) at high image sizes for PVTv2-B2 and
FaViT-B2. The proposed FaViT is more efficient and the
advantage becomes more pronounced as the size increases.
B. Object Detection
Data and setups. We evaluate the proposed FaViT variants
in the challenging tasks of object detection and instance
segmentation on the COCO 2017 dataset [69]. To ensure a
comprehensive evaluation, we employ two well-established
frameworks, RetinaNet [70] and Mask R-CNN [71], and in-
tegrate FaViT variants as backbones. We initialize the models
with weights pre-trained on the ImageNet-1K dataset and fine-
tune them using the AdamW optimizer. We set the initial
learning rate at 0.0001 and apply a weight decay of 0.05. Our
training is conducted with a batch size of 16, adhering to a
consistent 1×training schedule, which amounts to 12 epochs.
Additionally, we adopt an image resizing strategy as priors [8],
where the shorter side is resized to 800 pixels while ensuring
that the longer side remains under 1333 pixels. The rest of the
hyperparameters are consistent with the Swin transformer [8]
to ensure a fair comparison.
Main results. In Table IV, we present the performance
results of FaViT-B0 to B3 across various size levels. Impres-
sively, all variants consistently deliver state-of-the-art results.
In comparison to the baseline Swin transformer [8], FaViT
showcases superior performance in both object detection and
instance segmentation tasks. Specifically, FaViT-B2 showcases
substantial improvements, with metrics such as AP and APL
witnessing gains of 2.9%and3.3%, respectively. For instance
segmentation, FaViT-B2 also achieves higher accuracy and
significantly improves APmby1.9%.

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
TABLE IV
OBJECT DETECTION AND INSTANCE SEGMENTATION RESULTS ON COCO 2017. A LL MODELS ARE PRE -TRAINED ON IMAGE NET-1K AND FINE -TUNED
WITH 1×SCHEDULE . THE NUMBER OF PARAMETERS AT AN INPUT RESOLUTION OF 1280×800 IS REPORTED (#PARAM ).
MethodRetinaNet 1× Mask R-CNN 1×
#Param AP AP 50 AP75 APSAPM APL #Param APbAPb
50 APb
75 APmAPm
50 APm
75
PVTv2-B0 [24] 13 M 37.2 57.2 39.5 23.1 40.4 49.7 24 M 38.2 60.5 40.7 36.2 57.8 38.6
FaViT-B0 (Ours) 12 M 37.4 57.2 39.8 22.9 40.6 49.5 23 M 37.9 59.6 41.0 35.4 56.6 37.8
ResNet18 [43] 21 M 31.8 49.6 33.6 16.3 34.3 43.2 31 M 34.0 54.0 36.7 31.2 51.0 32.7
PVTv2-B1 [24] 24 M 41.2 61.9 43.9 25.4 44.5 54.3 34 M 41.8 64.3 45.9 38.8 61.2 41.6
FaViT-B1 (Ours) 23 M 41.4 61.8 44.0 25.7 44.9 55.0 33 M 42.4 64.4 46.3 38.9 61.2 41.8
Twins-S [72] 34 M 43.0 64.2 46.3 28.0 46.4 57.5 44 M 43.4 66.0 47.3 40.3 63.2 43.4
Swin-T [8] 39 M 41.5 62.1 44.2 25.1 44.9 55.5 48 M 42.2 64.6 46.2 39.1 61.6 42.0
FaViT-B2 (Ours) 35 M 44.4 65.0 47.7 27.7 48.2 58.8 45 M 45.4 67.1 49.4 41.0 64.0 44.1
PVTv1-M [23] 54 M 41.9 63.1 44.3 25.0 44.9 57.6 64 M 42.0 64.4 45.6 39.0 61.6 42.1
Swin-S [8] 60 M 44.5 65.7 47.5 27.4 48.0 59.9 69 M 44.8 66.6 48.9 40.9 63.4 44.2
FaViT-B3 (Ours) 59 M 46.0 66.7 49.1 28.4 50.3 62.0 68 M 47.1 68.0 51.4 42.7 65.9 46.1
Fig. 7. Visualizing attention maps for detecting multiple objects. (a) Multiple
targets to be detected in the original image are marked with red boxes. (b)
ConvNeXt-T loses small target when detecting multiple targets. (c) Swin-T is
severely affected by local windows. (d) FaViT-B2 has excellent multi-target
detection capability.
Visualization. Due to the long-range dependency, FaViT
enjoys advantages in detecting various scale objects. We
visualize the attention heatmap of FaViT-B2 under the multi-
object challenge in Figure 7. To offer a comparative analysis,
we juxtapose FaViT-B2 with ConvNeXt-T [54] and Swin-T,
two models of similar size. ConvNeXt-T appears less adept
at simultaneously focusing on multiple objects, potentially
causing it to overlook individual targets. Swin transformer’s
attention heatmap exhibits a grid-like structure, which is
relatively messy. In contrast, FaViT’s attention distribution is
more uniform across the entire object space, allowing it to
simultaneously capture the positions and contours of multiple
objects with precision. This heatmap comparison serves as
compelling evidence of FaViT’s superiority.TABLE V
SEGMENTATION RESULTS ON ADE20K.
Method #Param FLOPs mIoU (%)
PVTv2-B0 [24] 8 M 25.0 G 37.2
FaViT-B0 (Ours) 7 M 24.6 G 37.2
ResNet18 [43] 16 M 32.2 G 32.9
PVTv1-T [23] 13 M 31.9 G 35.1
EFormer-L1 [75] 16 M 33.0 G 38.9
FaViT-B1 (Ours) 17 M 33.9 G 42.0
Twins-S [72] 28 M 37.5 G 43.2
Swin-T [8] 32 M 46.0 G 41.5
FaViT-B2 (Ours) 29 M 45.2 G 45.0
CAT-B [76] 55 M 76.9 G 44.9
Swin-S [8] 53 M 70.0 G 45.2
FaViT-B3 (Ours) 52 M 66.7 G 47.2
C. Semantic Segmentation
Data and setups. To evaluate the performance of the proposed
FaViT variants in semantic segmentation, we employ the
ADE20K dataset [73] and follow a benchmark framework,
Semantic FPN [74], to ensure a fair comparison. Our training
strategy aligns with established practices [8], encompassing
160K iterations, utilization of the AdamW optimizer, an initial
learning rate of 0.0002 , a weight decay of 0.0001 , and a batch
size of 16.
Main results. Table V depicts that our FaViT variants of dif-
ferent model sizes consistently outperform their corresponding
Swin transformer [8] counterparts when using Semantic FPN
for semantic segmentation. For example, with fewer param-
eters and FLOPs, FaViT-B2/B3 are at least 2%higher than
Swin-T/S. Noticeably, FaViT shows superior segmentation
accuracy across all model sizes.
D. Robustness Analysis
We evaluate the robustness of FaViT variants to image
corruptions, label noise, and class imbalance.

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
TABLE VI
ROBUSTNESS AGAINST IMAGE CORRUPTIONS ON IMAGE NET-C.
Method #Param RetentionBlur (Acc %) Noise (Acc %) Digital (Acc %) Weather (Acc %)
Motion Fafoc Glass Gauss Gauss Impul Shot Speck Contr Satur JEPG Pixel Bright Snow Fog Frost
Mobile Setting ( <15M)
MobileNetV2 [77] 4 M 49.2 33.4 29.6 21.3 32.9 24.4 21.5 23.7 32.9 57.6 49.6 38.0 62.5 28.4 45.2 37.6 28.3
EfficientNet-B0 [5] 5 M 54.7 36.4 26.8 26.9 39.3 39.8 38.1 47.1 39.9 65.2 58.2 52.1 69.0 37.3 55.1 44.6 37.4
PVTv2-B0 [24] 3 M 58.9 30.8 24.9 24.0 35.8 33.1 35.2 44.2 50.6 59.3 50.8 36.6 61.9 38.6 50.7 45.9 41.8
ResNet18 [43] 11 M 32.8 29.6 28.0 22.9 32.0 22.7 17.6 20.8 27.7 30.8 52.7 46.3 42.3 58.8 24.1 41.7 28.2
PVTv2-B1 [24] 13 M 65.4 45.7 41.3 30.5 43.9 48.1 46.2 46.6 55.0 57.6 68.6 59.9 50.2 71.0 49.8 56.8 53.0
FaViT-B0 (Ours) 3 M 59.2 38.1 31.6 24.8 37.4 38.3 35.6 39.9 45.2 47.9 60.8 51.6 38.9 63.2 38.5 44.6 42.5
FaViT-B1 (Ours) 13 M 68.1 48.2 43.2 30.7 45.6 53.8 52.4 52.6 58.7 59.6 70.1 61.7 53.5 72.1 50.9 57.1 54.7
GPU Setting (20M+)
ResNet50 [43] 25 M 62.5 42.1 40.1 27.2 42.2 42.2 36.8 41.0 50.3 51.7 69.2 59.3 51.2 71.6 38.5 53.9 42.3
ViT-S [9] 25 M 67.6 49.7 45.2 38.4 48.0 50.2 47.6 49.0 57.5 58.4 70.1 61.6 57.3 72.5 51.2 50.6 57.0
DeiT-S [17] 22 M 72.6 52.6 48.9 38.1 51.7 57.2 55.0 54.7 60.8 63.7 71.8 64.0 58.3 73.6 55.1 61.1 60.7
PVTv1-S [23] 25 M 66.9 54.3 48.4 34.7 46.4 51.7 51.7 50.0 55.8 57.6 69.4 60.7 53.7 49.5 62.3 55.2 53.1
PVTv2-B2 [24] 25 M 71.5 54.3 48.4 34.7 50.7 61.2 60.7 59.5 64.5 65.5 73.5 65.5 58.8 75.2 56.7 67.8 62.7
Swin-T [8] 29 M 66.8 49.5 45.0 31.7 47.6 54.7 51.6 52.6 58.4 62.1 71.4 62.2 54.4 73.4 60.0 64.7 60.2
FaViT-B2 (Ours) 25 M 73.4 55.9 49.8 34.8 51.7 62.6 62.1 61.2 65.3 64.9 73.3 66.1 64.8 75.0 55.3 62.9 59.5
Robustness against image corruptions. We evaluate the
robustness of our proposed FaViT variants on the ImageNetC
dataset [78], which includes various corrupted images with
effects like blur, natural noise, digital noise, and severe weather
conditions. For a fair comparison, all models are pre-trained
on ImageNet-1K without further fine-tuning [10]. The results
in Table VI demonstrate that FaViT exhibits remarkable ro-
bustness, both under Mobile and GPU settings, surpassing
CNN and transformer-based priors. This enhanced robustness
likely stems from its superior representation capabilities. To
better gauge model robustness, we introduce a new metric
called accuracy retention (Retention), designed to mitigate the
influence of a model’s capability to represent on clean images
dataset. This metric quantifies the ratio between a model’s
accuracy on corrupted images and its accuracy on clean
images, providing insight into how well a model preserves
accuracy when tested on corrupted data.
For instance, consider FaViT-B1, which achieves an ac-
curacy of 54.1%on ImageNet-C and 79.4%on ImageNet.
Its accuracy retention is 68.1%, indicating that it preserves
68.1% of its accuracy when tested on corrupted images.
In comparison, PVTv2-B1 [24], while achieving a similar
accuracy on clean images, has an accuracy retention of 65.4%.
This suggests that FaViT consistently outperforms PVTv2
when it comes to preserving accuracy on corrupted images.
Notably, FaViT-B2 significantly outperforms Swin-T [8] by
6.6%in accuracy retention and demonstrates superior perfor-
mance across various types of image corruptions. These results
underscore FaViT’s success in capturing long-range contextual
information, a crucial factor in enhancing robustness against
image corruptions.
Robustness against label noise. We assess the robustness of
FaViT against real-world label noise using the Clothing1M
[79] and WebVision [80] datasets. To ensure a fair comparison,TABLE VII
ROBUSTNESS AGAINST LABEL NOISE ON CLOTHING 1M AND WEBVISION .
Method #ParamClothing1M Webvision
Test Acc (%) Top-1 Acc (%) Top-5 Acc (%)
PVTv1-S [23] 25 M 68.83 60.08 81.84
PVTv2-B2 [24] 25 M 69.89 65.28 85.72
Shunted-S [26] 22 M 70.04 67.44 86.24
Swin-T [8] 29 M 69.12 60.84 82.48
FaViT-B2 (Ours) 25 M 70.82 67.72 85.80
TABLE VIII
ROBUSTNESS AGAINST CLASS IMBALANCE ON I NATURALIST 2018.
Method FLOPs Top-1 Acc (%)
ResMLP-12 [81] 3.0 G 60.2
Inception-V3 [82] 2.5 G 60.2
LeViT-192 [83] 0.7 G 60.4
ResNet-50 [84] 4.1 G 64.1
FaViT-B1 (Ours) 2.4 G 64.2
we adopt a consistent training strategy as [33] across all mod-
els. Table VII presents the results, with FaViT-B2 consistently
achieving the highest accuracy on both datasets. Remarkably,
FaViT-B2 attains a top-1 accuracy of 67.72% on WebVision,
outperforming the baseline Swin-T [8] by a substantial margin
of6.9%. These findings underscore the significant robustness
of FaViT in the presence of real-world label noise.
Robustness against class imbalance. We test the model
robustness against class imbalance on the long-tailed iNatu-
ralist dataset [82]. All models are pre-trained on ImageNet-
1K and fine-tuned for 100 epochs with an initial learning

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
TABLE IX
APPLY FASA TO OTHER FRAMEWORKS .
Method #Param FLOPs Acc (%)
Swin-T [8] 28 M 4.4 G 61.8
Swin-FaSA-T 26 M 4.0 G 66.1
PVTv2-B0 [24] 4 M 0.6 G 63.5
PVTv2-FaSA-B0 3 M 0.6 G 64.0
PVTv2-B1 [24] 14 M 2.1 G 70.8
PVTv2-FaSA-B1 11 M 2.1 G 73.1
TABLE X
REDUCE MIXED TO SINGLE FINE -GRAINED .
Method #Param FLOPs Acc (%)
FaSA-low 3 M 0.6 G 64.9
FaSA-high 3 M 0.6 G 64.5
FaSA 3 M 0.6 G 65.2
rate of 0.0001 . Table VIII presents the results, with FaViT-
B1 achieving the highest accuracy compared to other models.
This underscores the excellent robustness of FaViT when
dealing with long-tailed data. The outcomes from these ex-
periments demonstrate the strong robustness of FaViT against
data corruptions and biases, showcasing its potential for real-
world applications. Table VIII also illustrates that FaViT
performs competitively in terms of FLOPs compared to the
state-of-the-art. Table VIII also illustrates that FaViT performs
competitively in terms of FLOPs compared to the state-of-the-
art.
E. Ablation Study
We perform ablation studies on CIFAR100 [85]. All model
variants are trained from scratch for 100epochs with an initial
learning rate of 0.001. The rest of the training strategy is
consistent with Section IV-A.
Effectiveness of FaSA. We evaluate the effectiveness and
generalizability of the proposed factorization self-attention
(FaSA) mechanism by integrating it to other transformer-
based backbones. Concretely, we simply replace the original
self-attention in the Swin transformer [8] and PVTv2 [24]
with FaSA while keeping the rest of the network architecture
unchanged. Table IX demonstrates that FaSA consistently
improves the performance of various backbones while simul-
taneously reducing the number of parameters and FLOPs. In
particular, it significantly enhances the accuracy of Swin-T and
PVTv2-B1 by 4.3%and2.3%, respectively, while reducing
the number of parameters of PVTv2-B1 by 18%. The results
provide clear evidence of the superiority of FaSA compared
to other popular self-attention mechanisms.
Impact of dilation rate set. In FaSA, we focus on ag-
gregating mixed-grained information captured by grouped
features. To further investigate the impact of this mixed-
grained aggregation, we design two additional models, FaSA-
low and FaSA-high, which each utilize only a single level
of granularity information based on FaViT-B0. FaSA-lowTABLE XI
IMAPCT OF GLOBAL FEATURES .
Method Acc (%)Param w.r.t image size
448 672 896 1120
FaViT-B 20 75.7 18 G 41 G 72 G 113 G
FaViT-B 21/4 75.7 18 G 43 G 80 G 135 G
FaViT-B 21/8 75.8 18 G 42 G 76 G 124 G
FaViT-B 21/16 75.8 18 G 41 G 74 G 118 G
TABLE XII
COMPARISON OF DIFFERENT OPERATORS USED FOR CROSS -WINDOW
FUSION ON CIFAR100.
Operator #Param FLOPs Top-1 Acc (%)
Pointwise Convolution 3.5 M 0.6 G 68.6
Linear Layer 3.5 M 0.6 G 67.3
Average Pooling 3.4 M 0.6 G 68.6
Maximum Pooling (FaViT-B0) 3.4 M 0.6 G 68.9
represents a configuration where the dilation rate for each
group is set to 1, resulting in the extracted queries having
the lowest granularity information. FaSA-high, on the other
hand, is configured to have local windows similar in size to
the feature map, emphasizing higher granularity information.
Table X presents the results, showing that the proposed FaSA
consistently outperforms FaSA-low and FaSA-high, indicating
the effectiveness of aggregating mixed-grained information in
FaSA.
Optimization structure with global features. The proposed
FaSA introduces dilation rates to increase the local window
size and model long-range but not global dependency. How-
ever, we argue that introducing an appropriate amount of
global features may help to improve model performance. To
investigate this, we split part of the channels and extract
global features from it using the method in [24]. Table XI
demonstrates that introducing global features improves model
performance while increasing computational cost. However,
this improvement comes at the cost of increased computational
complexity. To strike a balance between performance and
cost, we extract global features from 1/8 of the channels,
while FaSA handles the rest. This configuration achieves an
ideal trade-off between improved performance and manage-
able computational cost.
Operator of cross-window fusion. In the cross-window fu-
sion step of FaSA, we use a symmetric aggregation function
σ(·)to fuse the features of the sampled points at the same
position in different windows. In practice, the fusion operator
is the average pooling. In order to find the best operator,
we use FaViT-B0 as a benchmark to test the performance
of various operators, including pointwise convolution, linear
layer, maximum pooling layer, and average pooling layer. We
evaluate the classification accuracy on CIFAR100, train 100
epochs from scratch, and other training strategies are consis-
tent with Section IV-A. Table XII shows the comparison results
of various operators, where the use of learnable operators such
as pointwise convolution and linear layer increases parame-

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
ters, but there is no performance advantage. Furthermore, we
compare two common pooling layers and finally choose the
maximum pooling with better performance.
V. F UTURE WORK
The FaViT proposed in this paper indeed finds the trade-
off between cost and performance, but there are still areas for
improvement. First, compared with some existing transformers
[24], [26], the proposed FaViT only innovates the core atten-
tion mechanism, without a specially designed patch embedding
layer and feedforward layer. These layers may not be an op-
timal match with FaSA, affecting performance. Second, some
existing transformers [38], [86], [87] have begun to explore
the use of convolution, pooling, or multi-layer perceptrons
to replace self-attention. Compared to these methods, FaViT
does not involve related work, but they are instructive and
attractive. Therefore, our future work will focus on model
optimization, designing model structures with higher efficiency
and better performance. We will also further explore the
feasibility of other approaches to implement attention matrix
decomposition. Furthermore, we will evaluate the performance
of existing methods and the proposed FaViT on high-resolution
image processing, making the transformers more valuable for
practical applications.
VI. C ONCLUSION
This paper proposes a novel factorization self-attention
mechanism (FaSA) to explore the optimal trade-off between
computational cost and the capability to model long-range
dependency. We introduce a factorization operation to obtain
the long-range and mixed-grained information simultaneously.
With the aid of FaSA, long-range dependency will be modeled
at the local window equivalent computational cost. Extensive
experiments show that the proposed model achieves state-of-
the-art performance and superior robustness. We hope this
work will provide reference and inspiration for future research
on visual transformers.
ACKNOWLEDGMENTS
This work was financially supported by the National Natural
Science Foundation of China (No. 62101032), the Young
Elite Scientist Sponsorship Program of China Association for
Science and Technology (No. YESS20220448), and the Young
Elite Scientist Sponsorship Program of Beijing Association for
Science and Technology (No. BYESS2022167).
REFERENCES
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
with deep convolutional neural networks,” Communications of the ACM ,
vol. 60, pp. 84 – 90, 2012.
[2] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, and M. Li, “Bag of
tricks for image classification with convolutional neural networks,” 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
558–567, 2019.
[3] B. Zoph and Q. V . Le, “Neural architecture search with reinforcement
learning,” ArXiv , vol. abs/1611.01578, 2017.
[4] R. Zhang, L. Jiao, D. Wang, F. Liu, X. Liu, and S. Yang, “A fast
evolutionary knowledge transfer search for multiscale deep neural archi-
tecture,” IEEE Transactions on Neural Networks and Learning Systems ,
2023.[5] M. Tan and Q. V . Le, “Efficientnet: Rethinking model scaling for
convolutional neural networks,” ArXiv , vol. abs/1905.11946, 2019.
[6] Y . Liu, Y . Zhang, Y . Wang, F. Hou, J. Yuan, J. Tian, Y . Zhang, Z. Shi,
J. Fan, and Z. He, “A survey of visual transformers,” IEEE Transactions
on Neural Networks and Learning Systems , 2023.
[7] M. Kaselimi, A. V oulodimos, I. Daskalopoulos, N. Doulamis, and
A. Doulamis, “A vision transformer model for convolution-free mul-
tilabel classification of satellite imagery in deforestation monitoring,”
IEEE Transactions on Neural Networks and Learning Systems , 2022.
[8] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted win-
dows,” 2021 IEEE/CVF International Conference on Computer Vision ,
pp. 9992–10 002, 2021.
[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-
formers for image recognition at scale,” ArXiv , vol. abs/2010.11929,
2021.
[10] D. Zhou, Z. Yu, E. Xie, C. Xiao, A. Anandkumar, J. Feng, and
J. M. ´Alvarez, “Understanding the robustness in vision transformers,”
inICML , 2022.
[11] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-
V oss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler,
J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei, “Language models are few-shot learners,” ArXiv , vol.
abs/2005.14165, 2020.
[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” ArXiv ,
vol. abs/1810.04805, 2019.
[13] W. Wang, K. Zhang, Y . Su, J. Wang, and Q. Wang, “Learning cross-
attention discriminators via alternating time–space transformers for
visual tracking,” IEEE Transactions on Neural Networks and Learning
Systems , 2023.
[14] H. Chen, G. Yang, and H. Zhang, “Hider: A hyperspectral image
denoising transformer with spatial–spectral constraints for hybrid noise
removal,” IEEE Transactions on Neural Networks and Learning Systems ,
2022.
[15] X. Zhao, M. Zhang, R. Tao, W. Li, W. Liao, L. Tian, and W. Philips,
“Fractional fourier image transformer for multimodal remote sensing
data classification,” IEEE Transactions on Neural Networks and Learn-
ing Systems , 2022.
[16] Y . Cui, J. Cheng, L. Wang, and G. Wu, “Mixformer: End-to-end
tracking with iterative mixed attention,” 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 13 598–13 608, 2022.
[17] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J’egou, “Training data-efficient image transformers & distillation
through attention,” in ICML , 2021.
[18] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, Y . Shi, X. Jin, A. Wang, and
J. Feng, “All tokens matter: Token labeling for training better vision
transformers,” in NeurIPS , 2021.
[19] M. Ding, B. Xiao, N. C. F. Codella, P. Luo, J. Wang, and L. Yuan,
“Davit: Dual attention vision transformers,” ArXiv , vol. abs/2204.03645,
2022.
[20] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. C. Bovik, and Y . Li,
“Maxvit: Multi-axis vision transformer,” ArXiv , vol. abs/2204.01697,
2022.
[21] W. Yu, M. Luo, P. Zhou, C. Si, Y . Zhou, X. Wang, J. Feng, and S. Yan,
“Metaformer is actually what you need for vision,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2022, pp. 10 819–10 829.
[22] J. Jiao, Y .-M. Tang, K.-Y . Lin, Y . Gao, J. Ma, Y . Wang, and W.-S. Zheng,
“Dilateformer: Multi-scale dilated transformer for visual recognition,”
IEEE Transactions on Multimedia , 2023.
[23] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
and L. Shao, “Pyramid vision transformer: A versatile backbone for
dense prediction without convolutions,” 2021 IEEE/CVF International
Conference on Computer Vision , pp. 548–558, 2021.
[24] ——, “Pvtv2: Improved baselines with pyramid vision transformer,”
ArXiv , vol. abs/2106.13797, 2022.
[25] F. Chollet, “Xception: Deep learning with depthwise separable con-
volutions,” 2017 IEEE Conference on Computer Vision and Pattern
Recognition , pp. 1800–1807, 2017.
[26] S. Ren, D. Zhou, S. He, J. Feng, and X. Wang, “Shunted self-attention
via multi-scale token aggregation,” arXiv preprint arXiv:2111.15193 ,
2021.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
[27] K. Patel, A. M. Bur, F. Li, and G. Wang, “Aggregating global features
into local vision transformer,” ArXiv , vol. abs/2201.12903, 2022.
[28] G. Li, D. Xu, X. Cheng, L. Si, and C. Zheng, “Simvit: Exploring a sim-
ple vision transformer with sliding windows,” 2022 IEEE International
Conference on Multimedia and Expo , pp. 1–6, 2022.
[29] Y . Bai, J. Mei, A. L. Yuille, and C. Xie, “Are transformers more robust
than cnns?” in Neural Information Processing Systems , 2021.
[30] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. ´Alvarez, and P. Luo,
“Segformer: Simple and efficient design for semantic segmentation with
transformers,” in Neural Information Processing Systems , 2021.
[31] S. Paul and P.-Y . Chen, “Vision transformers are robust learners,” in
AAAI Conference on Artificial Intelligence , 2022.
[32] D. Hendrycks and T. G. Dietterich, “Benchmarking neural network
robustness to common corruptions and perturbations,” ArXiv , vol.
abs/1903.12261, 2019.
[33] J. Li, R. Socher, and S. C. H. Hoi, “Dividemix: Learning with noisy
labels as semi-supervised learning,” ArXiv , vol. abs/2002.07394, 2020.
[34] C. Tian, W. Wang, X. Zhu, X. Wang, J. Dai, and Y . Qiao, “Vl-ltr:
Learning class-wise visual-linguistic representation for long-tailed visual
recognition,” ArXiv , vol. abs/2111.13579, 2021.
[35] H. Che, J. Wang, and A. Cichocki, “Bicriteria sparse nonnegative ma-
trix factorization via two-timescale duplex neurodynamic optimization,”
IEEE Transactions on Neural Networks and Learning Systems , 2021.
[36] X. Liu, J. Wang, Z. Li, Z. Shi, X. Fu, and L. Qiu, “Non-line-of-sight
reconstruction with signal–object collaborative regularization,” Light:
Science & Applications , vol. 10, no. 1, p. 198, 2021.
[37] Y .-X. Ren, J. Wu, Q. T. Lai, H. M. Lai, D. M. Siu, W. Wu, K. K. Wong,
and K. K. Tsia, “Parallelized volumetric fluorescence microscopy with
a reconfigurable coded incoherent light-sheet array,” Light: Science &
Applications , vol. 9, no. 1, p. 8, 2020.
[38] Q. Zhang, Y . Xu, J. Zhang, and D. Tao, “Vsa: Learning varied-size
window attention in vision transformers,” in European conference on
computer vision . Springer, 2022, pp. 466–483.
[39] Z. Zhang, Y . Chen, D. Zhang, Y . Qian, and H. Wang, “Ctfnet:
Long-sequence time-series forecasting based on convolution and time–
frequency analysis,” IEEE Transactions on Neural Networks and Learn-
ing Systems , 2023.
[40] X. Fu, W. Wang, Y . Huang, X. Ding, and J. Paisley, “Deep multiscale
detail networks for multiband spectral image sharpening,” IEEE Trans-
actions on Neural Networks and Learning Systems , vol. 32, no. 5, pp.
2090–2104, 2020.
[41] C. Liu, Y . Yao, D. Luo, Y . Zhou, and Q. Ye, “Self-supervised motion
perception for spatiotemporal representation learning,” IEEE Transac-
tions on Neural Networks and Learning Systems , 2022.
[42] P. Wang, X. Wang, H. Luo, J. Zhou, Z. Zhou, F. Wang, H. Li, and R. Jin,
“Scaled relu matters for training vision transformers,” in Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 36, no. 3, 2022, pp.
2495–2503.
[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” 2016 IEEE Conference on Computer Vision and Pattern
Recognition , pp. 770–778, 2016.
[44] Y . Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and Z. Liu,
“Mobile-former: Bridging mobilenet and transformer,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2022, pp. 5270–5279.
[45] Z. Zhang, H. Zhang, L. Zhao, T. Chen, and T. Pfister, “Aggregating
nested transformers,” arXiv preprint arXiv:2105.12723 , vol. 2, no. 3,
p. 5, 2021.
[46] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. H. Tay, J. Feng, and
S. Yan, “Tokens-to-token vit: Training vision transformers from scratch
on imagenet,” 2021 IEEE/CVF International Conference on Computer
Vision , pp. 538–547, 2021.
[47] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, and J. Gao,
“Multi-scale vision longformer: A new vision transformer for high-
resolution image encoding,” 2021 IEEE/CVF International Conference
on Computer Vision , pp. 2978–2988, 2021.
[48] C.-F. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention multi-
scale vision transformer for image classification,” 2021 IEEE/CVF
International Conference on Computer Vision , pp. 347–356, 2021.
[49] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer in
transformer,” in NeurIPS , 2021.
[50] P. Ren, C. Li, G. Wang, Y . Xiao, and Q. D. X. L. X. Chang, “Beyond fix-
ation: Dynamic window visual transformer,” ArXiv , vol. abs/2203.12856,
2022.
[51] Q. Han, Z. Fan, Q. Dai, L. Sun, M.-M. Cheng, J. Liu, and J. Wang,
“On the connection between local attention and dynamic depth-wise
convolution,” arXiv preprint arXiv:2106.04263 , 2021.[52] Q. Yu, Y . Xia, Y . Bai, Y . Lu, A. L. Yuille, and W. Shen, “Glance-and-
gaze vision transformer,” Advances in Neural Information Processing
Systems , vol. 34, pp. 12 992–13 003, 2021.
[53] Z. Dai, H. Liu, Q. V . Le, and M. Tan, “Coatnet: Marrying convolu-
tion and attention for all data sizes,” Advances in neural information
processing systems , vol. 34, pp. 3965–3977, 2021.
[54] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A
convnet for the 2020s,” in Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , 2022, pp. 11 976–11 986.
[55] Y . Li, C.-Y . Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and
C. Feichtenhofer, “Mvitv2: Improved multiscale vision transformers
for classification and detection,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
4804–4814.
[56] Q. Zhang, Y . Xu, J. Zhang, and D. Tao, “Vitaev2: Vision transformer
advanced by exploring inductive bias for image recognition and beyond,”
International Journal of Computer Vision , pp. 1–22, 2023.
[57] W. Wang, W. Chen, Q. Qiu, L. Chen, B. Wu, B. Lin, X. He, and W. Liu,
“Crossformer++: A versatile vision transformer hinging on cross-scale
attention,” arXiv preprint arXiv:2303.06908 , 2023.
[58] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,
“Bottleneck transformers for visual recognition,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition , 2021,
pp. 16 519–16 529.
[59] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, “Focal
attention for long-range interactions in vision transformers,” in NeurIPS ,
2021.
[60] R. Wightman, H. Touvron, and H. J ´egou, “Resnet strikes back: An
improved training procedure in timm,” arXiv preprint arXiv:2110.00476 ,
2021.
[61] Z. Huang, Y . Ben, G. Luo, P. Cheng, G. Yu, and B. Fu, “Shuffle
transformer: Rethinking spatial shuffle for vision transformer,” arXiv
preprint arXiv:2106.03650 , 2021.
[62] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Q. Hou, and J. Feng, “Deep-
vit: Towards deeper vision transformer,” ArXiv , vol. abs/2103.11886,
2021.
[63] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, and Q. Tian, “Msg-
transformer: Exchanging local spatial information by manipulating
messenger tokens,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2022, pp. 12 063–12 072.
[64] Y . Lee, J. Kim, J. Willette, and S. J. Hwang, “Mpvit: Multi-path vision
transformer for dense prediction,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
7287–7296.
[65] X. Ding, X. Zhang, J. Han, and G. Ding, “Scaling up your kernels
to 31x31: Revisiting large kernel design in cnns,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2022, pp. 11 963–11 975.
[66] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, and S.-M. Hu, “Visual
attention network,” Computational Visual Media , pp. 1–20, 2023.
[67] J. Li, X. Xia, W. Li, H. Li, X. Wang, X. Xiao, R. Wang, M. Zheng,
and X. Pan, “Next-vit: Next generation vision transformer for ef-
ficient deployment in realistic industrial scenarios,” arXiv preprint
arXiv:2207.05501 , 2022.
[68] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg,
and L. Fei-Fei, “Imagenet large scale visual recognition challenge,”
International Journal of Computer Vision , vol. 115, pp. 211–252, 2015.
[69] T.-Y . Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in ECCV , 2014.
[70] T.-Y . Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll ´ar, “Focal loss
for dense object detection,” IEEE Transactions on Pattern Analysis and
Machine Intelligence , vol. 42, pp. 318–327, 2020.
[71] K. He, G. Gkioxari, P. Doll ´ar, and R. B. Girshick, “Mask r-cnn,” IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 42, pp.
386–397, 2020.
[72] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and
C. Shen, “Twins: Revisiting the design of spatial attention in vision
transformers,” in NeurIPS , 2021.
[73] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,
“Semantic understanding of scenes through the ade20k dataset,” Inter-
national Journal of Computer Vision , vol. 127, pp. 302–321, 2018.
[74] A. Kirillov, R. B. Girshick, K. He, and P. Doll ´ar, “Panoptic feature
pyramid networks,” 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pp. 6392–6401, 2019.

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14
[75] Y . Li, G. Yuan, Y . Wen, E. Hu, G. Evangelidis, S. Tulyakov, Y . Wang,
and J. Ren, “Efficientformer: Vision transformers at mobilenet speed,”
ArXiv , vol. abs/2206.01191, 2022.
[76] H. Lin, X. Cheng, X. Wu, F. Yang, D. Shen, Z. Wang, Q. Song,
and W. Yuan, “Cat: Cross attention in vision transformer,” 2022 IEEE
International Conference on Multimedia and Expo , pp. 1–6, 2022.
[77] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and L.-C.
Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” 2018
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 4510–4520, 2018.
[78] D. Hendrycks and T. G. Dietterich, “Benchmarking neural network
robustness to common corruptions and perturbations,” ArXiv , vol.
abs/1903.12261, 2019.
[79] T. Xiao, T. Xia, Y . Yang, C. Huang, and X. Wang, “Learning from mas-
sive noisy labeled data for image classification,” 2015 IEEE Conference
on Computer Vision and Pattern Recognition , pp. 2691–2699, 2015.
[80] W. Li, L. Wang, W. Li, E. Agustsson, and L. V . Gool, “Webvision
database: Visual learning and understanding from web data,” ArXiv , vol.
abs/1708.02862, 2017.
[81] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave,
G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, and H. J’egou, “Resmlp:
Feedforward networks for image classification with data-efficient train-
ing,” IEEE transactions on pattern analysis and machine intelligence ,
vol. PP, 2022.
[82] G. V . Horn, O. M. Aodha, Y . Song, Y . Cui, C. Sun, A. Shepard, H. Adam,
P. Perona, and S. J. Belongie, “The inaturalist species classification and
detection dataset,” 2018 IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 8769–8778, 2018.
[83] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. J’egou, and
M. Douze, “Levit: a vision transformer in convnet’s clothing for faster
inference,” 2021 IEEE/CVF International Conference on Computer
Vision , pp. 12 239–12 249, 2021.
[84] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. J. Belongie, “Class-balanced
loss based on effective number of samples,” 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 9260–9269, 2019.
[85] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features
from tiny images,” 2009.
[86] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Un-
terthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al. , “Mlp-mixer:
An all-mlp architecture for vision,” Advances in neural information
processing systems , vol. 34, pp. 24 261–24 272, 2021.
[87] J. Guo, K. Han, H. Wu, Y . Tang, X. Chen, Y . Wang, and C. Xu,
“Cmt: Convolutional neural networks meet vision transformers,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 12 175–12 185.
