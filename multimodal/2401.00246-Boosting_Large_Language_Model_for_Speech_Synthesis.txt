# 2401.00246.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2401.00246.pdf
# File size: 458251 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Boosting Large Language Model for Speech Synthesis:
An Empirical Study
Hongkun Hao1∗, Long Zhou2, Shujie Liu2, Jinyu Li2, Shujie Hu2, Rui Wang1, Furu Wei2
1Shanghai Jiao Tong University
2Microsoft Corporation
Abstract
Large language models (LLMs) have made significant advancements in natural
language processing and are concurrently extending the language ability to other
modalities, such as speech and vision. Nevertheless, most of the previous work
focuses on prompting LLMs with perception abilities like auditory comprehension,
and the effective approach for augmenting LLMs with speech synthesis capabil-
ities remains ambiguous. In this paper, we conduct a comprehensive empirical
exploration of boosting LLMs with the ability to generate speech, by combining
pre-trained LLM LLaMA/OPT and text-to-speech synthesis model V ALL-E. We
compare three integration methods between LLMs and speech synthesis models,
including directly fine-tuned LLMs, superposed layers of LLMs and V ALL-E, and
coupled LLMs and V ALL-E using LLMs as a powerful text encoder. Experimen-
tal results show that, using LoRA method to fine-tune LLMs directly to boost
the speech synthesis capability does not work well, and superposed LLMs and
V ALL-E can improve the quality of generated speech both in speaker similarity and
word error rate (WER). Among these three methods, coupled methods leveraging
LLMs as the text encoder can achieve the best performance, making it outperform
original speech synthesis models with a consistently better speaker similarity and a
significant (10.9%) WER reduction.
1 Introduction
The emergence of large language models (LLMs), such as ChatGPT [OpenAI, 2023a] and LLaMA
[Touvron et al., 2023], has revolutionized most traditional natural language processing (NLP) tasks,
like text summarization and dialogue system. The powerful language generation capabilities of
LLMs have prompted exploration into their applications in other modalities, e.g., speech and vision
[OpenAI, 2023b, Huang et al., 2023, Zhang et al., 2023b]. For example, GPT-4V [OpenAI, 2023b]
enables users to instruct GPT-4 to analyze image inputs they provided. Video-LLaMA [Zhang et al.,
2023b] empowers LLM with the ability to comprehend both visual and auditory content present in
video. These multi-modal LLMs provide the potential to enhance the impact of text-only systems by
integrating new interfaces and functionalities, allowing them to handle new tasks and deliver fresh
experiences to users.
Regarding the application of LLMs to speech, the majority of earlier research primarily concentrates
on aligning speech representation with the LLM input space [Wu et al., 2023, Fathullah et al., 2023,
Shu et al., 2023, Tang et al., 2023]. For instance, Speech-LLaMA [Wu et al., 2023] proposes an effec-
tive method to accomplish speech-to-text tasks by leveraging Connectionist Temporal Classification
(CTC) [Graves et al., 2006] model and audio encoder to map the compressed acoustic features to the
continuous semantic space of the LLM. LLaSM [Shu et al., 2023] takes advantage of a well-trained
Whisper encoder to encode the speech signals into hidden states, and utilizes a modal adaptor to align
∗Work was done during internship at Microsoft Research Asia.arXiv:2401.00246v1  [cs.CL]  30 Dec 2023

--- PAGE 2 ---
the above output hidden states with the input text embedding of LLMs. Compared to understanding
speech, enabling LLMs to generate speech is considerably more challenging, given that speech is a
continuous signal significantly deviating from the output space of LLMs. To enable speech generation
ability, existing works such as SpeechGPT [Zhang et al., 2023a] and AudioPaLM [Rubenstein et al.,
2023] employ the approach of directly fine-tuning a pre-trained LLM, which requires substantial
computational resources and time. How to effectively enhance LLMs with the capabilities for speech
synthesis remains a relatively unexplored area.
To better understand this task, we are going to answer two questions: 1) Can the codec codes be
treated by LLMs simply as a kind of language similar to other natural languages? 2) What kind of
information can LLMs provide to improve the quality of synthesized speech? In order to answer
these two questions, in this paper, we propose and compare several integration approaches to enable
the LLMs with speech synthesis capability. In this study, we focus on zero-shot text-to-speech
(TTS) tasks following the state-of-the-art model V ALL-E [Wang et al., 2023a], which mainly uses an
auto-regressive (AR) Transformer decoder model to predict the discrete token of speech depending
on the corresponding textual tokens. To enhance the speech generation of LLMs, we first discretize
the continuous speech into multi-layer discrete codec codes via audio compression model Encodec
[Défossez et al., 2022], and expand the vocabulary of LLMs with the vocabulary of codec codes, e.g.,
1024 tokens. We design three combination strategies to achieve the first-layer codec code prediction
with LLM, like the AR model in V ALL-E, as follows:
•Directly Fine-tuned LLMs . We directly fine-tune large language models via paired text
and codec codes from speech recognition dataset, with full parameters or partial parameters
(LoRA [Hu et al., 2021]), as shown in Figure 1(a).
•Superposed LLMs and VALL-E . Figure 1(b) illustrates this strategy that we superimpose
the two models into one model. In this method, we use the large language model to encode
both textual tokens and acoustic tokens, and then we feed them into the codec language
model V ALL-E.
•Coupled LLMs and VALL-E . As shown in Figure 1(c), we use an additional text-based
large language model to encode the text sequence and then input them into the V ALL-E AR
model. The coupled method differs from the aforementioned superposed approach as it does
not utilize LLMs to model codec codes.
After that, we can use the non-autoregressive (NAR) model of V ALL-E to generate the codec codes
of the rest quantizers, and utilize the Encodec decoder to recover the waveform of the speech. Models
are trained on 44.5K hours Multilingual Librispeech English data and 960 hours LibriSpeech data
and evaluated on LibriSpeech dev-clean, dev-other, test-clean, and test-other datasets. Experimental
results demonstrate that coupled LLMs and V ALL-E can achieve the best performance among
baseline and our methods. Additionally, we perform thorough analyses of various facets of our
approach, examining the impact of model size, the benefits of continuous pre-training, the effect
of the pre-trained V ALL-E, and a comparative evaluation of LoRA versus complete fine-tuning for
V ALL-E. Based on the results, we can draw conclusions as follows:
•Codec codes can not be simply treated as another language since the results of directly fine-
tuned LLM are not promising. The reason could be that, the sequence length of codec codes
is much longer than the length of corresponding text, and also the information provided by
codec codes is much more fine-grained and more diverse than that of text.
•While LLMs with LoRA may not excel at generating codec codes, they can serve as a
unified encoder for processing both text and codec codes. The outputs generated by LLMs
can provide valuable representation for a codec language model (e.g., V ALL-E) to produce
more accurate codec codes.
•LLM can be used as a powerful text encoder alone that can model pertinent and extensive
content information, which is instrumental for V ALL-E to generate speech of superior
quality and enhanced robustness. The structure using LLM as a text encoder, coupled with a
dedicated decoder module such as V ALL-E, achieves the best performance.
2

--- PAGE 3 ---
2 Related Work
Our work is typically based on LLMs, which have made significant breakthroughs in natural language
processing, outperform previous state-of-the-art models in extensive NLP tasks, and inspire the
instruction-following capability to achieve the unseen tasks [Ouyang et al., 2022, OpenAI, 2023a,
Touvron et al., 2023, Anil et al., 2023]. The advent of ChatGPT [Ouyang et al., 2022] marks a
transformative era in the field of artificial intelligence. By leveraging vast training datasets and
extensive parameter configurations, in conjunction with instruction tuning and RLHF algorithm, it
raises amazing emergent abilities and becomes the best artificial intelligence assistant with natural
language as an interface.
Given that the world’s information not only includes text but also encompasses mediums such as
speech and images, it is a natural idea to expand from uni-modal text-based large language models to
multi-modal LLMs [Tang et al., 2023, Huang et al., 2023, Zhang et al., 2023b, Driess et al., 2023,
Moon et al., 2023, Chu et al., 2023]. Most of this work focuses on enhancing the perceptual field of
LLMs, enabling them to specifically understand auditory and visual capabilities. For example, LLaV A
[Liu et al., 2023] combines a vision encoder and LLM into an end-to-end model for general-purpose
visual-language understanding with impressive chat capabilities. Speech-LLaMA [Wu et al., 2023]
and SALMONN [Tang et al., 2023] try to perceive and understand all kinds of audio inputs with an
additional audio encoder. Different from the above work, the goal of our work is to boost LLMs to
generate speech instead of understanding speech.
Our work is also related to that large audio generative models [Borsos et al., 2022, Wang et al.,
2023a, Zhang et al., 2023c, Rubenstein et al., 2023, Zhang et al., 2023a, Chen et al., 2023]. V ALL-E
[Chen et al., 2023] is a novel and state-of-the-art zero-shot text-to-speech model, which contains
an autogressive (AR) Transformer model and a non-autoregrressive (NAR) Transformer model to
predict the first-layer quantized codes and rest-layer quantized codes separately. Our work follows
the framework of V ALL-E AR architecture to synthesize the speech with augmented LLMs. Besides,
SpeechGPT [Zhang et al., 2023a] and AudioPaLM [Rubenstein et al., 2023] convert speech into
discrete hidden units and continually pre-train LLMs with hidden unit corpus. LauraGPT [Chen et al.,
2023] also fully fine-tunes LLMs with discrete codec codes of speech, to enable speech generation
ability. However, no work has explored the use of existing speech synthesis models (e.g., V ALL-E) to
empower the speech generation capabilities of LLMs. This paper focuses on empirically investigating
and comparing different methods of endowing LLMs with speech synthesis capabilities.
3 Methodology
In this section, we will first introduce the core model components in the proposed framework in
subsection 3.1, including large language model, speech compression model, and codec language
model, then present the three integration strategies for LLMs and V ALL-E in subsection 3.2.
3.1 Model Components
There are three core components in our framework including a large language model (i.e., OPT
[Zhang et al., 2022] or LLaMA [Touvron et al., 2023]), a speech compression model (i.e., Encodec
[Défossez et al., 2022]), and a codec language model (i.e., V ALL-E [Wang et al., 2023a]). The large
language model is employed to model textual tokens, with the option to include acoustic tokens as
well. Meanwhile, the speech compression model is tasked with transforming continuous speech into
discrete codec codes and subsequently reconstructing speech from these codes. Additionally, the
codec language model is used to generate codec codes conditioning on the representation of textual
tokens.
Large Language Model We conduct extensive experiments utilizing various pre-trained large
language models including OPT [Zhang et al., 2022] models with different sizes including 125M,
350M, and 1.3B, and the LLaMA-7B [Touvron et al., 2023] model. These decoder-only models
will be adapted using either full fine-tuning or parameter-efficient fine-tuning methods such as Low-
rank Adaptation (LoRA) [Hu et al., 2021]. The OPT-125M/350M/1.3B model is a 12/24/24-layer
Transformer decoder with an attention dimension of 768/1024/2048, respectively. The LLaMA-7B
model is a 32-layer Transformer decoder with an attention dimension of 4096.
3

--- PAGE 4 ---
Large Language Model (LLaMA)
Acoustic 
PromptText 
Prompt
3-second enrolled 
recordingText for synthesisPersonalized 
Speech
TokenizerAudio Codec 
EncoderAudio Codec 
Decoder
(a) Method A: Directly Fine-tuned LLM
Large Language Model ( LLaMA )
Acoustic 
PromptText 
Prompt
3-second enrolled 
recordingText for synthesisPersonalized 
Speech
TokenizerAudio Codec 
EncoderAudio Codec 
Decoder
Codec Language Model (VALL -E)
 LoRA
LoRA
 (b) Method B: Superposed LLM and V ALL-E
Large Language Model 
(LLaMA )
Acoustic 
PromptText 
Prompt
3-second enrolled 
recordingText for synthesisPersonalized 
Speech
TokenizerAudio Codec 
EncoderAudio Codec 
Decoder
Codec Language Model (VALL -E)
LoRALoRA
Textual tokens
Frozen
Codec codes
Trainable
(c) Method C: Coupled LLM and V ALL-E
Figure 1: Overview of the proposed different integration methods. (a) Method A: Directly fine-tuned
LLMs where LLMs are trained for predicting codec codes with an expanded vocabulary. (b) Method
B: Superposed LLMs and V ALL-E, where both LLMs and V ALL-E are used to model textual tokens
and acoustic tokens successively. (c) Method C: Coupled LLMs and V ALL-E, where the better text
representation provided by LLM is regarded as the textual input of V ALL-E.
Speech Compression Model To enable the LLM with speech generation ability, we utilize an
external speech compression model EnCodec [Défossez et al., 2022] to convert continuous speech
into discrete codec codes. EnCodec model is a convolution-based encoder-decoder network with
residual vector quantization (RVQ) method. It first tokenizes speech data into L-layer acoustic tokens
using EnCodec encoder and RVQ module, and then recovers the speech waveform from all acoustic
tokens using EnCodec decoder. In this paper, we adapt EnCodec with 6 kbps bandwidth and L=8
tokens for each frame.
Codec Language Model The neural codec language model V ALL-E [Wang et al., 2023a] treats text-
to-speech synthesis as a language model task, like GPT, and employs acoustic tokens (audio codec
codes) as an intermediate representation of original speech. According to textual representations,
V ALL-E generates the codec code sequences (8 codes for each frame), from which final waveforms
can be recovered by an audio compression decoder (e.g., Encodec). V ALL-E contains two key
modules, the auto-regressive (AR) codec language model and the non-autoregressive (NAR) codec
language model. The former is responsible for predicting the acoustic tokens of the first codec code
for each frame based on the semantic tokens in an auto-regressive manner, and the latter is used to
generate the other 7-layer codes according to the sequence of the first-layer codes in parallel with the
4

--- PAGE 5 ---
layer-level iterative generation method. In this work, we follow the V ALL-E AR model, which is
identical to the model architecture of LLMs, to augment LLMs with speech synthesis ability.
3.2 Integration Strategies
We propose three methods to boost large language models with speech synthesis capability. Figure 1
illustrates the different methods, including directly fine-tuned LLMs (Method A), superposed LLMs
and V ALL-E (Method B), and coupled LLMs and V ALL-E (Method C). Initially, we propose to
directly fine-tune LLMs in Method A to determine if acoustic tokens can be integrated into LLMs by
treating them as a novel language. Furthermore, through Method B, we assess the capability of LLMs
to encode both acoustic and textual tokens into a unified continuous embedding space, enhancing the
performance of V ALL-E in text-to-speech tasks. Finally, in Method C, we explore the potential of
leveraging only the text encoding proficiency of LLMs to improve TTS outcomes without regarding
acoustic tokens as a new language.
Method A: Directly Fine-tuned LLMs In order to verify whether acoustic tokens can be incorpo-
rated into LLMs by simply regarding it as a new language, enabling the joint training of both acoustic
and textual tokens, the most straightforward approach involves fine-tuning language models directly
with TTS training data by either full fine-tuning or parameter-efficient fine-tuning, as shown in Figure
1(a). Through training on TTS data, we also augment large language models with speech synthesis
ability at the same time. In practice, we found that using parameter-efficient fine-tuning methods such
as LoRA in this way is less effective and results in relatively poor performance. We speculate that
this is because large language models do not have the ability to generate codec codes inherently and it
is more difficult for LLMs to generate speech than understand speech signals. Therefore, we directly
fully fine-tune LLMs as one kind of approach that endows LLMs with speech synthesis ability.
Method B: Superposed LLMs and V ALL-E Inspired by the observation of Method A introduced
above, we aim to further explore the suitability of LLMs for encoding both acoustic tokens and
textual tokens into continuous embedding space so that this representation can be used by V ALL-E to
perform TTS tasks better. As shown in Figure 1(b), in this approach, we superpose the pre-trained
LLMs and V ALL-E models to promote the speech generation ability of LLMs. Both textual tokens
and acoustic tokens are encoded by LLM, and are sent to the codec language model to predict the
first-layer codec code. Besides, a linear projection layer is added between LLM and codec language
model to bridge the dimension gap between them.
Method C: Coupled LLMs and VALL-E Given the distinct roles and strengths of LLMs and
V ALL-E, it would be interesting to investigate the effect of only utilizing the text encoding ability
of LLMs, instead of treating acoustic tokens as a new language in previous methods, to promote
TTS performance of V ALL-E. Therefore, another natural idea is to take full use of the advantages
of LLMs and V ALL-E, and cascade the pre-trained LLMs and V ALL-E into an end-to-end model.
LLMs excel at encoding and generating text, while V ALL-E specializes in producing speech tokens
based on textual tokens. Hence, in this text-to-speech framework, we first use LLMs to encode text
and get better text representation, then feed it to V ALL-E as text input, as shown in Figure 1(c). In
this method, we also incorporate a linear projection layer between the LLM and the codec language
model to reconcile the disparity in dimensions.
4 Experiments
4.1 Experiment Setup
Dataset: Pre-trained models are fine-tuned on two ASR datasets, which can also be used to
train TTS tasks as V ALL-E (X) [Wang et al., 2023a, Zhang et al., 2023c]. Specifically, we use
LibriSpeech (LS, 960 hours) [Panayotov et al., 2015] and the English part of Multilingual LibriSpeech
(MLS) [Pratap et al., 2020]1. The Multilingual LibriSpeech is a 50K-hour ASR corpus including
8 languages derived from read audiobooks of LibriV ox, where English accounts for about 44.5K
hours predominately. We evaluate our proposed methods on the LibriSpeech dev-clean, dev-other,
1We do not use Librilight [Kahn et al., 2020] data like V ALL-E, due to its lack of ground-truth transcriptions
required for tokenization using large language model’s tokenizer.
5

--- PAGE 6 ---
test-clean, and test-other datasets. We use the samples that range in duration from 4 to 20 seconds
from these datasets2. Following Wang et al. [2023a], we use the first 3 seconds of the ground-truth
speech as prompts for each sample synthesis. Each experiment is conducted thrice, with the average
score being reported.
Data Preprocessing: To unify the training of speech and text modalities, we transform both into
discrete tokens. In our approach, ASR data transcriptions are tokenized into subwords (semantic
tokens) with the tokenizer from large language models. Meanwhile, speech data are quantized into
acoustic tokens using the EnCodec, which operates at a 6 kbps bandwidth and a downsampling ratio
of 320, producing 8 acoustic tokens per frame and 75 frames per second of audio. We concatenate
the semantic tokens and corresponding acoustic tokens to form a cohesive training sample.
4.2 Training Details
For Method A, we employ both LoRA and full fine-tuning techniques to train OPT models. However,
due to computational resource limitations, we exclusively utilize LoRA for training the LLaMA-
7B model. Additionally, we augment the LLMs’ vocabulary with acoustic tokens, specifically
incorporating 1024 Encodec tokens in our configuration. In Method B, we introduce LoRA parameters
to LLM and codec language model respectively. The LLM is initialized with either a pre-trained OPT-
350M or LLaMA-7B, while the codec language model is initialized with a pre-trained V ALL-E. We
also expand the vocabulary of LLM with acoustic tokens like Method A. Besides, the input acoustic
and textual embeddings from V ALL-E are omitted, as the LLM now provides the representations for
both acoustic and textual tokens. Similarly, in Method C we also add LoRA parameters to pre-trained
LLM and pre-trained V ALL-E respectively, and discard the textual token embedding of V ALL-E.
We fix the LoRA parameter to R= 64 for adjusting self-attention parameters. Consequently, using
Method A for LoRA training yields approximately 14M trainable parameters for OPT-350M and
71M for LLaMA-7B. In contrast, Method B incorporates codec code embedding, LoRA, and linear
projection, resulting in around 21M trainable parameters for OPT-350M and 82M for LLaMA-7B.
Meanwhile, Method C reduces the count of trainable parameters to 20M for OPT-350M and 78M
for LLaMA-7B, as it does not utilize codec code embedding for the LLMs. Our models are trained
using the Adam optimizer with β1= 0.9andβ2= 0.98[Kingma and Ba, 2015]. All models are
trained on TTS tasks for 400K steps on 32 V100 GPUs with a batch size of 100 seconds per GPU.
The maximum learning rate is 5×10−4with a warm-up step of 40K. We follow the configuration of
V ALL-E to train our non-autoregressive language model as introduced in Section 3.1.
4.3 Evaluation Metrics
We use the automatic evaluation metrics, including the word error rate (WER), speaker similarity
(SS), and speech naturalness (SN) to evaluate the generated speech for simplicity and convenience.
The WER score is obtained by an open-source Conformer Transducer model3, ranging from 0 to
100. The lower the WER, the more accurate the generated speech is. Given generated and prompt
speech utterances, the SS is measured by an automatic speaker verification (ASV) WavLM [Chen
et al., 2022] model4, ranging from -1 to 1. The larger the SS, the more similar the speakers of the two
utterances are. SN score of generated speech is measured by the open-source NISQA5[Mittag and
Möller, 2020]. Since we mainly use LoRA to fine-tune LLMs, the original textual processing ability
of LLMs will not be affected when performing NLP tasks without LoRA parameters, therefore NLP
tasks are not evaluated in this paper.
4.4 Inference Strategies
After training, we use sampling methods for our models to generate the acoustic tokens of the
first layer codec codes. Specifically, we use top- p[Holtzman et al., 2020] sampling with p= 1.0
2Note that V ALL-E (X)’s evaluation set contains audio samples ranging from 4 to 10 seconds in length.
Given that the audio durations within the MLS dataset span 10 to 20 seconds, our model demonstrates the
capability to perform speech synthesis tasks over extended periods.
3https://github.com/NVIDIA/NeMo/
4https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_
verification
5https://github.com/gabrielmittag/NISQA
6

--- PAGE 7 ---
and temperature is 1.0. We adopt three different strategies to choose sampled sequences following
previous work [Wang et al., 2023b].
•Strategy I performs only one synthesis inference for one text, and then the sampled acoustic
sequence is chosen as the final result.
•Strategy II conducts five synthesis inferences for a single text, selecting the utterance that
yields the highest speaker similarity score.
•Strategy III also performs five synthesis inferences for a given text and selects the utterance
that exhibits the lowest word error rate.
4.5 Main Results
We synthesize the English speech of corresponding text prompted by a 3s English speech utterance on
selected samples of dev-clean, dev-other, test-clean, and test-other datasets, where Table 1 shows the
results of dev-clean and others are shown in Appendix A. As summarized in Table 1, we replicate the
V ALL-E baseline using parameters identical to those of Wang et al. [2023a], while the proposed three
methods are validated using both LLaMA-7B and OPT-350M models. We apply the three inference
strategies outlined in Section 4.4, evaluating their performance using the metrics of word error rate
(WER), sentence similarity (SS), and speaker naturalness (SN), as introduced in Section 4.3.
According to the experimental results, we can draw three conclusions: (1) Directly fine-tuning LLMs
by LoRA performs worse than the V ALL-E baseline model. Although full fine-tuning can mitigate
the problem and achieve comparable performance with V ALL-E, it needs massive computational
resources for large models. (2) Method B, when employed with both the OPT-350M or LLaMA-
7B models, surpasses the V ALL-E baseline in terms of WER, SS, and SN, which demonstrates
that augmenting LLM with V ALL-E can address the above challenge with LoRA methods, given
that LLMs are capable of encoding both acoustic and textual tokens and V ALL-E shares a portion
of the burden for speech synthesis in LLMs. (3) By fully leveraging the respective strengths of
both components, Method C achieves the best performance among the proposed methods, which
significantly outperforms V ALL-E on word error rate, speaker similarity, and speech naturalness.
Compared to the V ALL-E, the word error rate of Method C with LLaMA-7B is relatively decreased
by 10.9%, 14.3%, and 6.9% under inference Strategy I, II, and III respectively, the speaker similarity
is relatively improved by 0.02, 0.03, and 0.03, and the speech naturalness is improved by 0.03, 0.02,
and 0.02 respectively.
Methods LLMsStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
V ALL-E - 4.39 0.52 3.26 4.27 0.58 3.28 1.31 0.56 3.27
AOPT-350M 10.28 0.49 3.20 9.74 0.53 3.21 3.97 0.51 3.20
OPT-350M FT∗4.21 0.53 3.28 4.08 0.60 3.29 1.28 0.58 3.28
LLaMA-7B 9.61 0.49 3.20 9.19 0.54 3.21 3.63 0.51 3.21
BOPT-350M 4.12 0.53 3.28 3.94 0.61 3.29 1.25 0.57 3.29
LLaMA-7B 4.05 0.53 3.29 3.82 0.61 3.30 1.23 0.58 3.29
COPT-350M 3.99 0.54 3.30 3.72 0.61 3.29 1.26 0.59 3.30
LLaMA-7B 3.91 0.54 3.29 3.66 0.61 3.30 1.22 0.59 3.29
Table 1: Main evaluation results on LibriSpeech dev-clean dataset. FT∗means full fine-tuning, and
other models adopt LoRA techniques. V ALL-E is the text-to-speech baseline, Method A/B/C are
introduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.
4.6 Analysis
To facilitate a clearer comprehension of our method, we conduct detailed analyses and ablation
studies in this section.
Effect of Model Size The capacity of a large language model is significantly influenced by its
parameter number. Consequently, we explore the impact of varying model sizes within the OPT
7

--- PAGE 8 ---
framework through direct full fine-tuning (referred to as Method A in Table 1), examining models
with 125M, 350M, and 1.3B parameters. Additionally, we establish baselines by training these
models from scratch. We conduct this experiment on the dev-clean dataset, the results of which are
depicted in Figure 2. The comparison between the two curves illustrates the effectiveness of using
pre-trained LLMs. The largest OPT model with 1.3B parameters achieves the best performance
overall compared to 125M and 350M. This finding suggests that increasing the model size could be a
viable strategy for enhancing speech synthesis capabilities.
/uni00000014/uni00000015/uni00000018/uni00000030 /uni00000016/uni00000018/uni00000013/uni00000030 /uni00000014/uni00000011/uni00000016/uni00000025/uni00000016/uni00000011/uni0000001b/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000015/uni00000017/uni00000011/uni00000017/uni00000017/uni00000011/uni00000019/uni00000017/uni00000011/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000003a/uni00000028/uni00000035
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000003/uni0000002c
/uni00000014/uni00000015/uni00000018/uni00000030 /uni00000016/uni00000018/uni00000013/uni00000030 /uni00000014/uni00000011/uni00000016/uni00000025/uni00000016/uni00000011/uni00000019/uni00000016/uni00000011/uni0000001b/uni00000017/uni00000011/uni00000013/uni00000017/uni00000011/uni00000015/uni00000017/uni00000011/uni00000017/uni00000017/uni00000011/uni00000019/uni00000017/uni00000011/uni0000001b
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000003/uni0000002c/uni0000002c
/uni00000014/uni00000015/uni00000018/uni00000030 /uni00000016/uni00000018/uni00000013/uni00000030 /uni00000014/uni00000011/uni00000016/uni00000025/uni00000014/uni00000011/uni00000014/uni00000013/uni00000014/uni00000011/uni00000014/uni00000018/uni00000014/uni00000011/uni00000015/uni00000013/uni00000014/uni00000011/uni00000015/uni00000018/uni00000014/uni00000011/uni00000016/uni00000013/uni00000014/uni00000011/uni00000016/uni00000018/uni00000014/uni00000011/uni00000017/uni00000013
/uni00000036/uni00000057/uni00000055/uni00000044/uni00000057/uni00000048/uni0000004a/uni0000005c/uni00000003/uni0000002c/uni0000002c/uni0000002c/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000029/uni00000055/uni00000052/uni00000050/uni00000003/uni00000036/uni00000046/uni00000055/uni00000044/uni00000057/uni00000046/uni0000004b /uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni00000048
Figure 2: WER results of using different model sizes in Method A under three inference strategies
introduced in Section 4.4. The overall results including speaker similarity and speech naturalness are
summarized in Appendix B.
Effect of Continual Pre-training Since unlabeled speech data is more common than paired
speech-text data, we also investigate the way of taking advantage of massive unlabeled speech data
to promote the speech synthesis performance of LLMs. Specifically, inspired by the next token
prediction pre-training objective of decoder-only language models like GPT, we use EnCodec codes
of the LibriLight [Kahn et al., 2020] dataset to continually pre-train large language models, so that
they can adapt to speech modality better. Then we use paired speech-text data to fine-tune continually
pre-trained models and compare them with those that have not been continually pre-trained. Table
2 shows the comparison results of (1) training from scratch, (2) directly full fine-tuning, and (3)
continually pre-training and then full fine-tuning, on large (MLS+LS) and small (LS) datasets. The
experimental results on Method A with OPT-350M as LLM show that the continual pre-training
method achieves significant WER reduction than methods of full fine-tuning and training from scratch
on the small fine-tuning dataset, and obtains comparable performance on the large fine-tuning dataset.
Data MethodStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
MLS+LSTrain From Scratch 4.33 0.52 3.26 4.10 0.59 3.28 1.30 0.56 3.27
Full Fine-tune 4.21 0.53 3.28 4.08 0.60 3.29 1.28 0.58 3.28
Pre-train+Fine-tune 4.19 0.53 3.28 4.03 0.60 3.29 1.26 0.58 3.28
LSTrain From Scratch 5.71 0.51 3.26 5.11 0.58 3.28 1.97 0.55 3.28
Full Fine-tune 5.65 0.50 3.26 5.10 0.57 3.27 1.99 0.53 3.28
Pre-train+Fine-tune 5.47 0.51 3.26 4.99 0.58 3.29 1.91 0.55 3.30
Table 2: Effect of continual pre-training on dev-clean set with Method A and OPT-350M. MLS+LS
means that the fine-tuning data are Multilingual LibriSpeech and LibriSpeech, and LS means Lib-
rispeech only.
Effect of Pre-trained V ALL-E To validate the benefits of employing the pre-trained codec language
model V ALL-E, we undertake an ablation study focusing on the impact of random initialization
versus pre-trained initialization. Specifically, we fully fine-tune the randomly initialized V ALL-E
but use LoRA to fine-tune the V ALL-E initialized with pre-trained weights. Table 3 delineates the
performance disparity between models with Method B that begin with random weights and those
initialized with pre-trained V ALL-E. The results clearly indicate that initializing with pre-trained
8

--- PAGE 9 ---
V ALL-E results in fewer trainable parameters and significantly surpasses random initialization across
various inference strategies and evaluation criteria.
LLMs VALL-EStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
OPT-350MRandomly (FT∗) 4.31 0.52 3.27 4.09 0.59 3.28 1.36 0.56 3.27
Pre-trained 4.12 0.53 3.28 3.94 0.61 3.29 1.25 0.57 3.29
LLaMA-7BRandomly (FT∗) 4.27 0.52 3.27 4.11 0.59 3.28 1.32 0.56 3.28
Pre-trained 4.05 0.53 3.29 3.82 0.61 3.30 1.23 0.58 3.29
Table 3: Effect of pre-trained V ALL-E on dev-clean set with method B, where V ALL-E is either
randomly initialized or is leveraged as a pre-trained model. FT∗means full fine-tuning, and models
with pre-trained V ALL-E adopt LoRA techniques.
LoRA vs. Full Fine-tuning in VALL-E The previous section has demonstrated that pre-trained
V ALL-E enhanced with LoRA outperforms a randomly initialized version of V ALL-E. Besides,
the main results also indicate that fully fine-tuning OPT-350M yields better results than applying
LoRA techniques. Since the model size of V ALL-E is relatively small compared to that of LLMs,
we are now keen to investigate the peak performance achievable by substituting LoRA with full
fine-tuning in V ALL-E. Table 4 presents a comparison of performance between LoRA fine-tuning
and full fine-tuning approaches for V ALL-E, revealing that full fine-tuning can indeed lead to further
enhancements in performance.
LLMs VALL-EStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
OPT-350MLoRA 3.99 0.54 3.30 3.72 0.61 3.29 1.26 0.59 3.30
Full Fine-tune 3.97 0.54 3.31 3.64 0.61 3.30 1.25 0.59 3.31
LLaMA-7BLoRA 3.91 0.54 3.29 3.66 0.61 3.30 1.22 0.59 3.29
Full Fine-tune 3.90 0.54 3.31 3.46 0.61 3.31 1.20 0.59 3.31
Table 4: Comparison of LoRA and full fine-tuning of V ALL-E on dev-clean set with Method C.
5 Conclusion
In this study, we explore various strategies for incorporating speech synthesis capabilities into large
language models (LLMs). Our findings show that simply fine-tuning LLMs with LoRA fails to
match the performance of the baseline, indicating the challenge of enhancing LLMs with speech
synthesis capabilities. Further investigation demonstrates that LLMs augmented with a pre-trained
text-to-speech synthesis model can surpass the performance of the baseline V ALL-E model. In
particular, by leveraging the respective strengths of LLMs and V ALL-E, the coupled LLM and
V ALL-E method achieves the highest performance among the methods evaluated. Moreover, we
conduct comprehensive analyses to better understand the proposed LLMs augmented with speech
synthesis ability.
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv
preprint arXiv:2305.10403, 2023.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,
Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language
modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.
9

--- PAGE 10 ---
Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen
Wang, Siqi Zheng, et al. Lauragpt: Listen, attend, understand, and regenerate audio with gpt. arXiv
preprint arXiv:2310.04673, 2023.
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised pre-training
for full stack speech processing. IEEE Journal ofSelected Topics inSignal Processing , 16(6):
1505–1518, 2022.
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and
Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale
audio-language models. arXiv preprint arXiv:2311.07919, 2023.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio
compression. arXiv preprint arXiv:2210.13438, 2022.
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal
language model. arXiv preprint arXiv:2303.03378, 2023.
Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo,
Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. Prompting large language models with
speech recognition abilities. arXiv preprint arXiv:2307.11795, 2023.
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks. In ICML ,
pages 369–376, 2006.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In 8thInternational Conference onLearning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020. URL https://openreview.net/
forum?id=rygGQyrFvH .
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao
Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning
perception with language models. arXiv preprint arXiv:2302.14045, 2023.
J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazaré, J. Karadayi, V . Liptchinsky,
R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.
Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 -2020 IEEE
International Conference onAcoustics, Speech andSignal Processing (ICASSP) , pages 7669–
7673, 2020. doi: 10.1109/ICASSP40776.2020.9052942.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rdInternational Conference onLearning Representations,
ICLR 2015, SanDiego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015. URL
http://arxiv.org/abs/1412.6980 .
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485, 2023.
Gabriel Mittag and Sebastian Möller. Deep Learning Based Assessment of Synthetic Speech
Naturalness. In Proc. Interspeech 2020 , pages 1748–1752, 2020. doi: 10.21437/Interspeech.
2020-2382.
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain,
Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and
scalable any-modality augmented language model. arXiv preprint arXiv:2309.16058, 2023.
10

--- PAGE 11 ---
OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023a. URL https://api.
semanticscholar.org/CorpusID:257532815 .
OpenAI. Gpt-4v(ision) system card. 2023b. URL https://cdn.openai.com/papers/GPTV_
System_Card.pdf .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances inNeural Information Processing Systems , 35:
27730–27744, 2022.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus
based on public domain audio books. In 2015 IEEE International Conference onAcoustics, Speech
andSignal Processing (ICASSP), pages 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964.
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A
Large-Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020 , pages 2757–
2761, 2020. doi: 10.21437/Interspeech.2020-2826.
Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos,
Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.
Audiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925 ,
2023.
Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and
Yemin Shi. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930, 2023.
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and
Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv preprint
arXiv:2310.13289, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language
models are zero-shot text to speech synthesizers. axXiv preprint arXiv:2301.02111 , 2023a. URL
https://arxiv.org/abs/2301.02111 .
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu
Li, and Furu Wei. Viola: Unified codec language models for speech recognition, synthesis, and
translation. arXiv preprint arXiv:2305.16107, 2023b.
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu,
Bo Ren, Linquan Liu, et al. On decoder-only architecture for speech-to-text and large language
model integration. arXiv preprint arXiv:2307.03917, 2023.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.
Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.
arXiv preprint arXiv:2305.11000, 2023a.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
model for video understanding. arXiv preprint arXiv:2306.02858, 2023b.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068, 2022.
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Speak foreign languages with
your own voice: Cross-lingual neural codec language modeling. axXiv preprint arXiv:2303.03926 ,
2023c. URL https://arxiv.org/abs/2303.03926 .
11

--- PAGE 12 ---
A Main Results of dev-other, test-clean, test-other
Throughout this section, we list the main results of dev-other, test-clean, and test-other in Table 5,
6, and 7 respectively. All these three tables show the same trend as in Table 1, which can further
consolidate the conclusions summarized in Section 4.5.
Methods LLMsStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
V ALL-E - 7.65 0.47 3.07 7.44 0.55 3.08 2.53 0.53 3.08
AOPT-350M 14.76 0.41 2.97 13.93 0.50 3.00 4.48 0.47 3.03
OPT-350M FT∗7.39 0.47 3.08 7.00 0.56 3.09 2.45 0.54 3.08
LLaMA-7B 14.43 0.41 2.98 13.21 0.51 3.00 4.41 0.46 3.02
BOPT-350M 6.90 0.49 3.08 6.76 0.57 3.09 2.35 0.54 3.08
LLaMA-7B 6.99 0.49 3.08 6.81 0.58 3.08 2.31 0.55 3.08
COPT-350M 6.94 0.50 3.08 6.75 0.57 3.09 2.33 0.54 3.10
LLaMA-7B 6.85 0.50 3.09 6.60 0.58 3.10 2.31 0.54 3.10
Table 5: Main evaluation results on LibriSpeech dev-other dataset. FT∗means full fine-tuning, and
the other models adapt LoRA techniques. V ALL-E is the text-to-speech baseline, Method A/B/C are
introduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.
Methods LLMsStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
V ALL-E - 4.52 0.51 3.31 4.33 0.58 3.31 1.31 0.56 3.31
AOPT-350M 10.66 0.47 3.21 10.01 0.54 3.21 4.01 0.51 3.21
OPT-350M FT∗4.26 0.51 3.31 3.98 0.59 3.31 1.23 0.57 3.31
LLaMA-7B 10.49 0.47 3.20 9.72 0.55 3.21 3.95 0.52 3.22
BOPT-350M 3.91 0.53 3.30 3.54 0.60 3.31 1.17 0.57 3.31
LLaMA-7B 3.86 0.53 3.30 3.44 0.60 3.32 1.16 0.57 3.31
COPT-350M 3.89 0.52 3.32 3.54 0.60 3.33 1.25 0.58 3.34
LLaMA-7B 3.71 0.53 3.31 3.43 0.60 3.32 1.16 0.58 3.32
Table 6: Main evaluation results on LibriSpeech test-clean dataset. FT∗means full fine-tuning, and
the other models adapt LoRA techniques. V ALL-E is the text-to-speech baseline, Method A/B/C are
introduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.
Methods LLMsStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
V ALL-E - 8.45 0.46 3.08 8.09 0.54 3.09 2.98 0.50 3.09
AOPT-350M 14.42 0.41 2.99 13.79 0.51 3.01 4.51 0.47 3.03
OPT-350M FT∗8.23 0.46 3.09 7.86 0.54 3.10 2.86 0.50 3.10
LLaMA-7B 14.17 0.42 3.00 13.59 0.51 3.02 4.36 0.47 3.03
BOPT-350M 7.68 0.47 3.10 7.53 0.55 3.10 2.79 0.50 3.10
LLaMA-7B 7.67 0.47 3.10 7.40 0.55 3.10 2.76 0.51 3.11
COPT-350M 7.78 0.47 3.10 7.42 0.56 3.10 2.76 0.51 3.11
LLaMA-7B 7.62 0.48 3.10 7.14 0.56 3.11 2.71 0.52 3.11
Table 7: Main evaluation results on LibriSpeech test-other dataset. FT∗means full fine-tuning, and
the other models adapt LoRA techniques. V ALL-E is the text-to-speech baseline, Method A/B/C are
introduced in Section 3.2, and inference strategies I/II/III are listed in Section 4.4.
12

--- PAGE 13 ---
B Effect of Model Size: Detailed Results
Table 8 shows the detailed word error rate, speaker similarity, and speech naturalness results of using
different model sizes in Method A under three inference strategies introduced in Section 4.4.
Methods ModelStrategy I Strategy II Strategy III
WER ↓SS↑SN↑WER ↓SS↑SN↑WER ↓SS↑SN↑
Train From ScratchOPT-125M 4.94 0.51 3.26 4.51 0.58 3.26 1.35 0.56 3.27
OPT-350M 4.33 0.52 3.26 4.10 0.59 3.28 1.30 0.56 3.27
OPT-1.3B 4.17 0.52 3.27 3.82 0.59 3.27 1.25 0.58 3.28
Full Fine-tuneOPT-125M 4.63 0.53 3.26 4.17 0.60 3.28 1.30 0.58 3.28
OPT-350M 4.21 0.53 3.28 4.08 0.60 3.29 1.28 0.58 3.28
OPT-1.3B 4.01 0.53 3.28 3.77 0.60 3.29 1.21 0.59 3.30
Table 8: WER, SS, and SN results of using different model sizes in Method A under three inference
strategies.
13
