# 2311.04257.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.04257.pdf
# File size: 2604446 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model
with Modality Collaboration
Qinghao Ye∗Haiyang Xu∗Jiabo Ye∗Ming Yan†Anwen Hu
Haowei Liu Qi Qian Ji Zhang Fei Huang Jingren Zhou
Alibaba Group
{yeqinghao.yqh, shuofeng.xhy, yejiabo.yjb, ym119608}@alibaba-inc.com
Code & Demo & Models: https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2
Abstract
Multi-modal Large Language Models (MLLMs) have
demonstratedimpressiveinstructionabilitiesacrossvarious
open-endedtasks. However,previousmethodsprimarilyfo-
cus on enhancing multi-modal capabilities. In this work,
weintroduceaversatilemulti-modallargelanguagemodel,
mPLUG-Owl2,whicheffectivelyleveragesmodalitycollab-
orationtoimproveperformanceinbothtextandmulti-modal
tasks. mPLUG-Owl2utilizesamodularizednetworkdesign,
withthelanguagedecoderactingasauniversalinterfacefor
managing different modalities. Specifically, mPLUG-Owl2
incorporatessharedfunctionalmodulestofacilitatemodal-
itycollaborationandintroducesamodality-adaptivemodule
that preserves modality-specific features. Extensive experi-
ments reveal that mPLUG-Owl2 is capable of generalizing
bothtexttasksandmulti-modaltasksandachievingstate-of-
the-art performances with a single generic model. Notably,
mPLUG-Owl2 is the first MLLM model that demonstrates
the modality collaboration phenomenon in both pure-text
and multi-modal scenarios, settinga pioneering path in the
development of future multi-modal foundation models.
1. Introduction
LargeLanguageModels(LLMs)suchasGPT-3[6],LLaMA
[57, 58], and GPT-4 [46] have garnered significant at-
tention due to their exceptional generalization abilities in
text understanding and generation. To facilitate the vision-
language applications, GPT-4V 1[45] has recently demon-
stratedimpressivemulti-modalcapabilitiesindiversetasks,
e.g., description , question answering, etc., sparking inter-
est among researchers in the potential convergence of the
vision-language field. This has led to the emergence of a
∗Equal contribution
†Corresponding author
1https://openai.com/research/gpt-4v-system-card
•Text Instruction Understanding •Visual Concept Understanding (a)(b)Modality Interference •Text Instruction Understanding •Visual Concept Understanding 
ModalityCollaboration 
BLIP-2, MiniGPT-4, LLaVA, etc.(Vanilla Language Decoder)mPLUG-Owl2(Modality-Adaptive Language Decoder)Feed Forward NetworkAttention ModuleFeed Forward NetworkModality-Adaptive Module
111000000
Figure 1. An overall performance comparison between mPLUG-
Owl2 and existing MLLMs and difference between existing
MLLMs and our proposed model. (a) Previous approaches uti-
lize a standard language decoder (i.e., LLM) to manage different
types of instructions, leading to modality interference and perfor-
mancedegradation. (b)WeintroducemPLUG-Owl2,whichusesa
modality-adaptivelanguagedecodertohandledifferentmodalities
withindistinctmoduleswhilesharingsomeparametersformodal-
ity collaboration. This approach mitigates the issue of modality
interference.
1arXiv:2311.04257v2  [cs.CL]  9 Nov 2023

--- PAGE 2 ---
group of Multi-modal Large Language Models (MLLMs)
[5, 15, 31, 38, 65, 66, 68, 75], which aim to enhance LLMs
with the ability to understand and handle visual problems.
Previousstudies[27,63]inmulti-modallearningsuggest
thatdifferentmodalitiescaneffectivelycollaborate,thereby
enhancing the performance of both text and multi-modal
tasks simultaneously. However, MLLMs is a unified model
that supports different modalities and tasks without fine-
tuning for specific tasks. Recent works utilize cross-modal
alignment modules (e.g., Q-former [15, 31, 75] and linear
layer[10,38])tomapvisualfeaturesfromthevisionencoder
intothefrozenLLMstocarryoutmulti-modaltasksbylever-
aging preserved language capabilities. This strategy, unfor-
tunately,restrictsthepotentialofmodalitycollaboration. As
a result, some researchers [38, 68] opt to fine-tune LLMs
during multi-modal instruction tuning. While fine-tuning
significantly improves multi-modal tasks, it risks weaken-
ing text task performance [16]. As illustrated in Figure 1,
the challenge of modality collaboration in MLLMs is from
applyingasinglemoduletobalancethegainofmodalitycol-
laborationandmodalityinterference,wheremodalitiesmay
interfere with each other on a large number of instruction
datasets across multiple modalities.
To mitigate this challenge, we present a new general-
purpose multi-modal foundation model, mPLUG-Owl2, in
thiswork. Ourmodelfeaturesamodularizednetworkdesign
thattakesbothmodalitycollaborationandmodalityinterfer-
enceintoaccount,usingthelanguagedecoderasauniversal
interface for managing multi-modal signals. Specifically,
mPLUG-Owl2 incorporates certain shared functional mod-
ules to promote modality collaboration and introduces a
modality-adaptive module that serves as a pivot across dif-
ferent modalities. Therefore, vision and language modal-
ities are projected into a shared semantic space for cross-
modality interaction, while the proposed module helps pre-
serve modality-specific features. With our novel archi-
tecture, modalities with varying information densities are
shielded from modality interference due to the modality-
adaptive module and can collaborate effectively in captur-
ingsharedinformation. Furthermore,weintroduceaninno-
vative two-stage training paradigm that consists of vision-
language pre-training and joint vision-language instruction
tuning. This paradigm trains the vision encoder across two
stages, enabling it to capture both low-level and high-level
semantic visual information more effectively.
Extensive experiments illustrate the effectiveness and
generalization abilities of mPLUG-Owl2, which achieves
state-of-the-art performance on 8 classic vision-language
benchmarks using a single generic model . Furthermore,
it either first or second in performance on 5 recent zero-
shotmulti-modalbenchmarks,underscoringitsadaptability
and proficiency in multi-modal instruction comprehension
and generation. In addition to its cutting-edge performancein multi-modal tasks, mPLUG-Owl2 also achieves state-of-
the-art results on multiple pure-text benchmarks. More-
over, we provide in-depth analysis to demonstrate and vali-
date the impact of modality collaboration through our pro-
posed modality-adaptive module, especially in enhancing
text tasks, including understanding, knowledge, and rea-
soning. Finally, comprehensive ablation studies validate
theeffectivenessoftheproposedMLLMtrainingparadigm,
which can help inspire the development of future multi-
modal foundation models.
2. Related Work
Multi-Modal Large Language Foundation Models. The
successful application of Large Language Models (LLMs)
haspavedthewayfordevelopingseveralapproachesaiming
to augment the perceptual capacities of LLMs with addi-
tional modalities, all within a unified model. There are
three primary methods for constructing multi-modal large
language foundational models, each showing promise for
robust zero-shot generalization capabilities in the vision-
languagedomain. Forinstance,Flamingo[2]isaforerunner
in this area, using a frozen vision encoder and a large lan-
guage model equipped with gated cross-attention for cross-
modality alignment. In contrast, PaLM-E [16] integrates
extracted visual features directly through linear layers into
the pre-trained PaLM [12] model, which boasts 520 billion
parameters, thereby leading to robust performance across
numerous real-world applications. This approach has been
broadlyadoptedbymodelssuchasLLaVA[38],Shikra[10],
etc. One significant limitation of this method, however, is
the creation of lengthy visual sequences. To address this,
BLIP-2[31],drawinginspirationfromDETR[8],developed
a Q-former to reduce the sequence length of visual features
efficiently. This design has been mirrored by Kosmos-1
[23], mPLUG-Owl [68], and MiniGPT-4 [75]. Neverthe-
less,itshouldbenotedthatthesemethodsdirectlyalignthe
visualfeatureswiththeLLMs,treatingvisionandlanguage
signalsasequivalent,therebyoverlookingtheuniquegranu-
laritiesbetweenvisionandlanguagemodalities. Toalleviate
this problem, we introduce modality-adaptive module. Our
proposedmodelleadstosuperiorperformanceinbothzero-
shot and fine-tuning evaluation settings in terms of both
image and video.
Instruction Tuning with MLLMs. Instruction tuning op-
timizes pre-trained large language models to comprehend
and adhere to natural instructions, thereby enhancing their
abilitytogeneralizeunseentasksinazero-shotmanner. Re-
searchersoftenemploymodelssuchasChatGPTandGPT-4
[46] to generate diverse and expansive instruction datasets,
including those like Alpaca [56], ShareGPT [1], and Wiz-
ardLM[61]. Asmulti-modallargelanguagemodelsemerge,
research communities are beginning to create high-quality,
diverse multi-modal datasets. For instance, MiniGPT-4
[75]utilizesGPT-3.5torephrasecaptionsgeneratedbypre-
2

--- PAGE 3 ---
trained models. Concurrently, LLaVA [38], SVIT [72], and
LRV-Instruction [36] take advantage of image annotations,
such as bounding boxes of objects, image captions, and re-
giondescriptions,topromptGPT-4togenerateinstructions
and responses using self-instruction methods. Models such
as mPLUG-Owl [68] and LLaVA-1.5 [37] further advance
this area by undergoing joint training with language-only
and vision-and-language instruction data, thereby mitigat-
ing the risk of catastrophic forgetting of language knowl-
edge. Rather than merely preventing this phenomenon of
catastrophic forgetting, mPLUG-Owl2, with the help of the
modality-adaptive module, can gain from the collaborative
effortsofmodalitiesbybeingjointlytrainedwithlanguage-
only and multi-modal instruction data, thus enhancing both
multi-modal and language-only performance.
3. Methodology
3.1. Overview
Figure 2 (a) sketches the overview of the mPLUG-Owl2.
Specifically,ourmodelcomprisesavisionencoder,avisual
abstractor, a text embedding layer, and a language decoder.
Notably, the standard implementation of the text embed-
ding layer and language decoder involves the use of a large
language model, such as GPT [6] or LLaMA [57]. We
first briefly introduce our model’s architecture in Section
3.2. Furthermore, we handle different types of modali-
tiesbyintroducingthemodality-adaptivemoduleinSection
3.3. Lastly, we introduce the training paradigm for training
mPLUG-Owl2 with modality collaboration in Section 3.4.
3.2. Model Architecture
As depicted in Figure 2, our model, referred to as mPLUG-
Owl2, is composed of three main components: a funda-
mental vision encoder [48], a visual abstractor, and a lan-
guage decoder. Specifically, we utilize ViT-L/14 as the
vision encoder and LLaMA-2-7B [58] as the language de-
coder. The vision encoder processes an input image with
anH×Wresolution and produces a sequence ofH
14×W
14
tokens. Thesevisualtokenfeaturesarethencombinedwith
text token embeddings and fed into the language decoder
that serves as a universal interface that converts various
vision-language tasks into text-generation tasks. However,
with the increase in image resolution, the encoded visual
token sequences can exponentially lengthen. Additionally,
the presence of abundant redundancy in the images (e.g.,
background, similar patches) leads to computational waste
and introduces considerable noise. To address this, we
propose a visual abstractor equipped with a fixed set of
learnable queries to extract higher semantic features from
images. Specifically, we feed the extracted visual token se-
quence I= [I1, I2,···, IP]∈RP×dandafixednumberof
Klearnable queries Q ∈RK×dinto the visual abstractor.
Here, P=H
14×W
14representsthenumberofvisualpatches,andDis the hidden dimension. The visual abstractor con-
sistsofaseriesofvisualabstractorlayers. Inthe i-thlayerof
the visual abstractor, the compressed visual representations
Vi+1are computed as follows:
Ci=Attn (Vi,[I;Vi],[I;Vi]), (1)
Vi+1=SwiGLU (CiW1)W2. (2)
Here, Attn (·,·,·)represents the self-attention operation,
while W1∈Rd×d′andW2∈Rd′×dare learnable param-
eters. The function SwiGLU (···)refers to the SwiGLU
activation function [51]. We designate V0=Qto initiate
the process. Moreover, to augment the fine-grained percep-
tion ability, we integrate sinusoidal positional embeddings
with the image feature IandVi, thereby preserving posi-
tional information, which has been proven essential in [8].
Hence, the computation required by the language decoder
decreasesfrom O((P+L)2)toO((K+L)2),significantly
reducing computational load when P≫K, particularly
in scenarios involving multiple images and when the text
length Lis relatively short. Once the compressed visual
feature is obtained, it is concatenated with text token em-
beddings and then processed by the language decoder to
generate the prediction.
3.3. Modality-Adaptive Module
Prior approaches [15, 38, 68, 75] typically attempt to align
visual features with language features by projecting image
features into the language semantic space. However, this
strategy can cause a mismatch in granularity , where image
features often contain fruitful semantic information com-
pared to the discrete semantic information within text em-
beddingfeatures. Thosemethodsdisregardtheuniquechar-
acteristicsofvisualandtextualinformation,thuspotentially
limiting the model’s performance. To this end, we propose
a new approach, namely, the Modality-Adaptive Module
(MAM), which decouples vision-language representations
by projecting visual features and language features into a
sharedsemanticspacewhilepreservingthedistinctiveprop-
erties of each modality.
Formally, given a vision-language sequence X∈
R(LV+LT)×dand modality indicators M∈ {0,1}(Lv+LT),
we first define modality separated operation ϕas:
ϕ(X, M, m ) =X⊙ 1{M=m}, (3)
where m∈ {0,1}is the type of modalities (i.e., vision
or language). Given the previous layer’s output vectors
Hl−1, l∈[1, L],where Listhenumberoflanguagedecoder
layers,wefirstnormalizeddifferentmodalitiesintothesame
magnitude as follows:
˜Hl−1=LNV(ϕ(Hl−1, M,0)) + LNT(ϕ(Hl−1, M,1)),(4)
3

--- PAGE 4 ---
Vision EncoderFeed Forward Network (FFN) Modality-Adaptive ModuleLanguage Decoder
x LText EmbeddingVisual Abstractor…Learnable QueriesImage
…
Text InputExplain Why this meme is funny.This meme is funny because it is an image of a sleeping dog with the caption "MONDAY. JUST...MONDAY." The humor comes from the juxtaposition of the dog's peaceful, relaxed state, emphasizing the reluctance or disdain for Monday.
Multi-Modal InputsNorm0Norm1Modality Indicator !!!!!"MatMul & ScaleSoftmaxMatMul
!"!!""!#Norm0Norm1Linear
Modality-Adaptive ModuleCross-Modality Features
Language-DecoderVisual AbstractorVisual EncoderStage-1: Pre-trainingStage-2: JointInstruction Tuning
Language-DecoderVisual AbstractorVisual Encoder
Text<latexit sha1_base64="21DEf0/wMu4ZY4ur8f33vM9A/0A=">AAADJnicnVLLSsNAFD2NWuu71aWbYBFcSEmlqMuiG5cVbBVqkWSctkPzIpkIpfgLbvUT/Bp3Iu5c6l94Z5yCD3zghCRnzr3nTHJmvNgXqXScx5w1MTmVny7MzM7NLywuFUvLrTTKEsabLPKj5MRzU+6LkDelkD4/iRPuBp7Pj73BvqofX/AkFVF4JIcx7wRuLxRdwVypqNO4L86KZafi6GF/BVUDyjCjEZVyeZziHBEYMgTgCCEJ+3CR0tVGFQ5i4joYEZcQErrOcYlZ0mbUxanDJXZAzx7N2oYNaa48U61mtIpPd0JKG+ukiagvIaxWs3U9086K/c57pD3Vtw3p7RmvgFiJPrG/6cad/9Vt0prBD/8+MvW/uaukJLrY1QkJSizWjMqOGZdMZ65ysd9lJskhJk7hc6onhJlWjnfR1ppUJ6t2ztX1Z92pWDVnpjfDi/pKOj/Vz6flK2htVarbldphrVzfMyepgFWsYYNOyw7qOEADTfLu4wrXuLFurTvr3np4a7VyRrOCD8N6egX8LKPa</latexit> <latexit sha1_base64="21DEf0/wMu4ZY4ur8f33vM9A/0A=">AAADJnicnVLLSsNAFD2NWuu71aWbYBFcSEmlqMuiG5cVbBVqkWSctkPzIpkIpfgLbvUT/Bp3Iu5c6l94Z5yCD3zghCRnzr3nTHJmvNgXqXScx5w1MTmVny7MzM7NLywuFUvLrTTKEsabLPKj5MRzU+6LkDelkD4/iRPuBp7Pj73BvqofX/AkFVF4JIcx7wRuLxRdwVypqNO4L86KZafi6GF/BVUDyjCjEZVyeZziHBEYMgTgCCEJ+3CR0tVGFQ5i4joYEZcQErrOcYlZ0mbUxanDJXZAzx7N2oYNaa48U61mtIpPd0JKG+ukiagvIaxWs3U9086K/c57pD3Vtw3p7RmvgFiJPrG/6cad/9Vt0prBD/8+MvW/uaukJLrY1QkJSizWjMqOGZdMZ65ysd9lJskhJk7hc6onhJlWjnfR1ppUJ6t2ztX1Z92pWDVnpjfDi/pKOj/Vz6flK2htVarbldphrVzfMyepgFWsYYNOyw7qOEADTfLu4wrXuLFurTvr3np4a7VyRrOCD8N6egX8LKPa</latexit> <latexit sha1_base64="21DEf0/wMu4ZY4ur8f33vM9A/0A=">AAADJnicnVLLSsNAFD2NWuu71aWbYBFcSEmlqMuiG5cVbBVqkWSctkPzIpkIpfgLbvUT/Bp3Iu5c6l94Z5yCD3zghCRnzr3nTHJmvNgXqXScx5w1MTmVny7MzM7NLywuFUvLrTTKEsabLPKj5MRzU+6LkDelkD4/iRPuBp7Pj73BvqofX/AkFVF4JIcx7wRuLxRdwVypqNO4L86KZafi6GF/BVUDyjCjEZVyeZziHBEYMgTgCCEJ+3CR0tVGFQ5i4joYEZcQErrOcYlZ0mbUxanDJXZAzx7N2oYNaa48U61mtIpPd0JKG+ukiagvIaxWs3U9086K/c57pD3Vtw3p7RmvgFiJPrG/6cad/9Vt0prBD/8+MvW/uaukJLrY1QkJSizWjMqOGZdMZ65ysd9lJskhJk7hc6onhJlWjnfR1ppUJ6t2ztX1Z92pWDVnpjfDi/pKOj/Vz6flK2htVarbldphrVzfMyepgFWsYYNOyw7qOEADTfLu4wrXuLFurTvr3np4a7VyRrOCD8N6egX8LKPa</latexit> 
<latexit sha1_base64="21DEf0/wMu4ZY4ur8f33vM9A/0A=">AAADJnicnVLLSsNAFD2NWuu71aWbYBFcSEmlqMuiG5cVbBVqkWSctkPzIpkIpfgLbvUT/Bp3Iu5c6l94Z5yCD3zghCRnzr3nTHJmvNgXqXScx5w1MTmVny7MzM7NLywuFUvLrTTKEsabLPKj5MRzU+6LkDelkD4/iRPuBp7Pj73BvqofX/AkFVF4JIcx7wRuLxRdwVypqNO4L86KZafi6GF/BVUDyjCjEZVyeZziHBEYMgTgCCEJ+3CR0tVGFQ5i4joYEZcQErrOcYlZ0mbUxanDJXZAzx7N2oYNaa48U61mtIpPd0JKG+ukiagvIaxWs3U9086K/c57pD3Vtw3p7RmvgFiJPrG/6cad/9Vt0prBD/8+MvW/uaukJLrY1QkJSizWjMqOGZdMZ65ysd9lJskhJk7hc6onhJlWjnfR1ppUJ6t2ztX1Z92pWDVnpjfDi/pKOj/Vz6flK2htVarbldphrVzfMyepgFWsYYNOyw7qOEADTfLu4wrXuLFurTvr3np4a7VyRrOCD8N6egX8LKPa</latexit> 
ImageTextImageText
(a)(b)(c)Figure2. IllustrationoftheproposedmPLUG-Owl2anditstrainingparadigm. (a)AnoverviewofmPLUG-Owl2,whichconsistsofavision
encoder, visual abstractor, text embedding layer, and a language decoder. (b) Details of the proposed modality-adaptive module, which
takesmulti-modalinputsandemploysdifferentparameterstoprojectvariousmodalitiesintoasharedsemanticspaceforrelationallearning
whilepreservingmodality-specificfeatures,therebyenablingmodalitycollaboration. (c)ThetrainingparadigmofmPLUG-Owl2involves
first pre-training the visual-related modules, including the vision encoder and visual abstractor. Simultaneously, newly added parameters
in the language decoder are also learned during the pre-training stage. During the instruction tuning stage, both language instructions and
multi-modal instructions are used to jointly train the entire model.
where LNVandLNTarelayernormalization[4]forvisual
featuresandlanguagefeaturesrespectively. Then,werefor-
mulate the self-attention operation by leveraging separated
linear projection layers for key projection matrix and value
projection matrix while preserving query projection matrix
shared as follows:
HQ
l=˜Hl−1WQ
l, (5)
HK
l=ϕ(˜Hl−1, M,0)WK0
l+ϕ(˜Hl−1, M,1)WK1
l,(6)
HV
l=ϕ(˜Hl−1, M,0)WV0
l+ϕ(˜Hl−1, M,1)WV1
l,(7)
Cl=Softmax 
HQ
lHK
l⊤
√
d!
HV
l, (8)
where WQ
l, WK0
l, WK1
l, WV0
l, WV1
l∈Rd×dare the learn-
ableprojectionmatrices,and Cl∈R(LV+LT)×disthecon-
text features of l-th layer. In this manner, we can calculate
thesimilaritiesbetweenthesetwomodalitieswithinashared
semanticspace,whilealsopreservingtheuniquecharacter-
istics of each modality through different value projection
layers. Moreover, by decoupling the key and value pro-
jection matrix, we can avoid interference between the two
modalities, particularly in relation to granularity mismatch.
In a similar vein, we also aim to model these characteris-
tics by using different layer normalization layers. Finally,
in order to promote modality collaboration within the same
featurespace,wemaintainasharedFFNforbothmodalities.
As a consequence, the model is able to preserve modalitycharacteristics while achieving modality collaboration via
the proposed modality-adaptive module.
3.4. Training Paradigm
AsdepictedinFigure2(c),weemployatwo-stageapproach
in training mPLUG-Owl2, comprising pre-training and vi-
sual instruction tuning similar to [38, 68], which aims to
alignthepre-trainedvisionencoderandlanguagemodeldur-
ing the pre-training phase, and then fine-tune the language
model with language modeling loss during the instruction
tuning phase. However, we find that simply freezing a pre-
trained vision encoder and training a vision-language pro-
jector to align visual data with language models can limit
their capacity to interpret complex visual information, such
as scene text and visual knowledge. To address the issue,
we make the vision encoder trainable throughout both the
pre-training and instruction tuning stages. This strategy
allows the model to capture both low-level and high-level
semantic visual information more effectively. Specifically,
for the pre-training stage, we enable the vision encoder,
visual abstractor, and a part of the modality-adaptive mod-
ule to be trainable, while keeping the pre-trained language
model frozen. Meanwhile, prior research in multi-modal
learning [63] has indicated that significant enhancements
can be achieved through the collaborative learning of uni-
modal and multi-modal sources. Based on this, we adopt
a joint training approach by tuning the whole model dur-
ing the instruction tuning stage, incorporating both text and
4

--- PAGE 5 ---
Image Caption General VQA General VQA (Zero-shot)
Model Type Method #Params COCOFlickr30KVQAv2 OKVQA GQA VizWizQA TextVQA SciQA (IMG)(Zero-Shot)
GeneralistsBLIP-2 [31] 8.2B - 74.9 65.0 45.9 41.0 19.6 42.5 61.0
InstructBLIP [15] 8.2B 102.2 82.4 - - 49.2 34.5 50.1†60.5
Unified-IO XL[41]2.9B 122.3 - 77.9 54.0 - 57.4‡- -
PaLM-E-12B [16] 12B135.0 - 76.2 55.5 - - - -
Shikra [10] 7.2B 117.5 73.9 77.4 47.2 - - - -
LLaVA-1.5 [37] 7.2B - - 78.5 - 62.0 50.0 46.1/ 58.2†66.8
Qwen-VL-Chat [5] 9.6B 131.9 81.0 78.2 56.6 57.5 38.9 61.5‡68.2
mPLUG-Owl2 8.2B 137.3 85.1 79.4 57.7 56.1 54.5 54.3/58.2†68.7
SpecialistsGIT [59] 0.7B 114.8 49.6 78.6 - - 68.0 59.8 -
GIT2 [59] 5.1B 145.0 50.7 81.7 - - 71.0 59.8 -
PaLI-17B [11] 17B149.1 - 84.3 64.5 - 71.6 58.8 -
Table1. Performance comparison on image caption and visual question answering. Forimagecaption,CIDErisreportedforevaluation,
and accuracy is reported for VQA. Note that specialists are fine-tuned on each individual dataset. †denotes OCR inputs are utilized. ‡
indicates the model has trained on the dataset. We gray out those specialists’ methods which are individually fine-tuned on the dataset as
well as those fine-tuned results of generalists.
Method Vision Encoder Language Model MME MMBench MM-Vet SEED-Bench Q-Bench
BLIP-2 [31] ViT-g (1.3B) Vicuna (7B) 1293.84 - 22.4 46.4 -
MiniGPT-4 [75] ViT-g (1.3B) Vicuna (7B) 581.67 23.0 22.1 42.8 -
LLaVA [38] ViT-L (0.3B) Vicuna (7B) 502.82 36.2 28.1 33.5 54.7
mPLUG-Owl [68] ViT-L (0.3B) LLaMA (7B) 967.34 46.6 - 34.0 58.9
InstructBLIP [15] ViT-g (1.3B) Vicuna (7B) 1212.82 36.0 26.2 53.4 55.8
LLaMA-Adapter-v2 [19] ViT-L (0.3B) LLaMA (7B) 1328.40 39.5 31.4 32.7 58.1
Otter [30] ViT-L (0.3B) LLaMA (7B) 1292.26 48.3 24.6 32.9 47.2
Qwen-VL-Chat [5] ViT-G (1.9B) Qwen (7B) 1487.58 60.6 - 58.2 61.6
LLaVA-1.5 [37] ViT-L (0.3B) Vicuna (7B) 1510.70 64.3 30.5 58.6 60.7
mPLUG-Owl2 ViT-L (0.3B) LLaMA (7B) 1450.19 64.5 36.2 57.8 62.9
Table 2. Zero-shot multi-modal evaluation on multi-modal benchmarks including MME [17], MMBench [39], MM-Vet [70], SEED-
Bench [29], and Q-Bench [60]. The overall scores are reported for evaluation. For MMBench and Q-Bench, we report test results.
multi-modal instructions. This methodology enhances the
model’scomprehensionofvisualconceptsembeddedwithin
the text by the multi-modal instructions. Concurrently, the
textinstructiondataaugmentsthemodel’sunderstandingof
intricatenaturalinstructions,therebyensuringthepreserva-
tion of its linguistic capabilities.
4. Experiments
4.1. Implementation
Data sets mPLUG-Owl2 is first pre-trained on image-text
pairs and fine-tunes on mono-modal and multi-modal in-
structiondata. Forpre-trainingdata,werandomlypickabout
400millionimage-textpairsfromfivepublicdatasets: Con-
ceptual Captions (CC3M/CC12M) [9], COCO [35], Laion-
en [49], COYO [7], DataComp [18]. For instruction data,
wecollect5typesofdatasetsincluding1)imagecaptioning
(i.e.,TextCaps[53],COCO[35]);2)imagequestionanswer-
ing(i.e.,VQAv2[21],OKVQA[43],OCR-VQA[44],GQA
[24], and A-OKVQA [50]); 3) region-aware QA (i.e., Ref-
COCO [69], VisualGenome [26]); 4) multi-modal instruct
data (i.e., LLaVA-instruct-150K [38]); 5) text-only instructdata (i.e., ShareGPT-80K [1], SlimOrca [34]). Details can
be found in the Appendix.
Training Settings We pre-train the model for 42,500 iter-
ations with a batch size 8,192 for about 348 million image-
text pairs. Since we adopt the language modeling loss,
the large batch size can be easily achieved by the gradi-
ent accumulation technique. mPLUG-Owl2 adopts ViT-L
[48] with patch size 14×14and pre-trained at resolution
224×224. We use the same data augmentation in BLIP-
2 [31], including random resized cropping, and horizontal
flipping with a probability of 0.5. The number of layers in
the visual abstractor is set to 6 and it is randomly initial-
ized. The number of learnable queries is set to 64. For the
language model, LLaMA-2 [58] is employed for handling
multi-modal features with 7B parameters, and the parame-
ters of modality-adaptive modules are initialized from the
language model. We use the AdamW [40] optimizer with
β1= 0.9,β2= 0.98andϵ=1e-6 for optimization. The
cosine learning rate decay scheduler with a peak learning
rate of 1e-4 and with warmup steps 1k. For the learning
rate of the vision encoder, we employ layer-wise learning
rate decay with a factor of 0.9 to retain the low-level visual
5

--- PAGE 6 ---
representation. Fortheinstructiontuningstage,wetrainthe
whole model for 1 epoch with a learning rate of 2e-5 and
batch size 256. Besides, we increase the resolution from
224×224to448×448. Thelayer-wiselearningratedecay
is also employed which is crucial for retaining good visual
representation in our experiments.
4.2. Main Results
Image Caption and Visual Question Answering. We as-
sess mPLUG-Owl2 using a wide range of academic bench-
marks for evaluating vision-language models. Our evalua-
tion includes eight popular benchmarks, as summarized in
Table 1. As the results show, our mPLUG-Owl2 surpasses
previous generalist models in both captioning and question
answeringtasks. Specifically,mPLUG-Owl2achievesstate-
of-the-artperformanceontheFlickr30Kdatasets,evencom-
pared with models with more powerful backbones (e.g.,
Qwen-VL-Chat [5] and InstructBLIP [15]). Moreover,
mPLUG-Owl2 exhibits distinct advantages in visual ques-
tion answering, especially in OCR-free scenarios, where
mPLUG-Owl2 achieves 54.3% accuracy on the TextVQA
dataset in a zero-shot manner, demonstrating the benefits
of our training strategy. Also worth noting is that mPLUG-
Owl2showsstrongzero-shotperformanceontheScienceQA
(Image Set) and VizWizQA datasets.
MLLM-oriented Multi-modal Benchmarks. Given the
robust zero-shot capabilities of Multi-Modal Language
Models (MLLMs), traditional evaluation metrics often fall
short in providing a detailed ability assessment. This prob-
lem is further exacerbated by their inability to match the
given answer accurately, leading to significant robustness
issues. To address these challenges, research communities
have introduced a series of benchmarks including MME
[17],MMBench[39],MM-Vet[70],SEED-Bench[29],and
Q-Bench [60]. These benchmarks systematically structure
and evaluate complex multi-modal tasks. We applied our
model,inazero-shotmanner,tofiverecentlypopularmulti-
modalbenchmarks. Forafaircomparison,weselectmodels
with similar language model sizes, particularly those from
theLLaMAfamily,anddetailtheirdifferencesinthevision
encoder. The results of our evaluation are listed in Table 2.
In the table, mPLUG-Owl2 achieves higher zero-shot per-
formance in terms of MMBench, MM-Vet, and Q-Bench.
Conversely, the performance on MME is lower because of
thelimitednumberoftestsamplesinMME,whichcouldpo-
tentially lead to sensitive fluctuations in performance. Par-
ticularly, it exhibits significant improvement on Q-Bench, a
benchmarkforexaminingthelow-levelvisualperceptionof
MLLMs. Thisimprovementoccurswhenapplyingasmaller
visualbackbone(i.e.,ViT-L),leadingtoenhancedlow-level
visual perception. This demonstrates the effectiveness of
our training strategy for training visual backbone.Method MMLU BBHAGIEval ARC-c ARC-e
LLaMA-2 [58] 46.8 38.2 21.8 40.3 56.1
WizardLM [61] 38.1 34.7 23.2 47.5 59.6
LLaMA-2-Chat [58] 46.2 35.6 28.5 54.9 71.6
Vicuna-v1.5 [73] 51.1 41.2 21.2 56.6 72.8
mPLUG-Owl2 53.4 45.0 32.7 65.8 79.9
Table 3. Performance on pure-text benchmarks of mPLUG-
Owl2compared to LLaMA-2 (7B) family variants. We adopt
5-shotforMMLUand0-shotforBBH,AGIEval,andARCas[14].
Natural Language Understanding and Generation. Cur-
rent MLLMs often outperform in various multi-modal
downstream tasks by leveraging the power of large lan-
guage models. Nevertheless, the intrinsic capabilities of
these models often play a significant role in determining
the performance of MLLMs, an aspect that has often been
overlooked in prior multi-modal language model studies.
Accordingly, we have also assessed the performance of our
model in the context of natural language understanding and
generation. We perform the evaluation on MMLU [22],
BBH [55], AGIEval [74] and ARC [13]. The results are
illustrated in Table 3. As observed in the table, mPLUG-
Owl2 excels in examination and reasoning, showing a sig-
nificant improvement on MMLU and BBH by 2.3% and
3.8% respectively. This indicates that mPLUG-Owl2 not
only performs well on multi-modal tasks but also achieves
better performance compared to the other instruction-tuned
LLMs, showing the promising way for developing strong
MLLMs.
MethodMSRVTT-QA MSVD-QA TGIF-QA
Accuracy Score Accuracy Score Accuracy Score
Exacting Match
Flamingo-80B [2] 17.4 - 35.6 - - -
FrozenBiLM [64] 16.8 - 32.2 - 41.0 -
BLIP-2 [31] 9.2 - 18.3 - - -
HiTeA [67] 21.7 - 37.4 - - -
InstructBLIP [15] 22.1 - 41.8 - - -
mPLUG-Owl2 23.6 - 42.4 - 61.6 -
GPT-Assisted
Video Chat [32] 45.0 2.5 56.3 2.8 34.4 2.3
LLaMA-Adapter [19] 43.8 2.7 54.9 3.1 - -
Video-LLaMA [71] 29.6 1.8 51.6 2.5 - -
Video-ChatGPT [42] 49.3 2.864.9 3.3 51.4 3.0
mPLUG-Owl2 46.7 2.9 65.4 3.5 67.1 3.7
Table 4. Zero-shot evaluation on video question answering.
Accuracy and relevance score are reported.
Zero-Shot Video Question Answering. Giventhatvideos
can be viewed as a sequence of images, we conducted a
comprehensive quantitative evaluation using several com-
monlyemployedvideoquestion-answeringdatasets,includ-
ing MSRVTT-QA [62], MSVD-QA [62], and TGIF-QA
[25]. These datasets aided in the zero-shot evaluation of
the model’s ability to understand video content, with the
6

--- PAGE 7 ---
MMLU (Exam)AGIEval (Exam)ARC-c (Exam) ARC-e (Exam) BoolQ (Know)TrivialQA (Know)
Xsum (Understanding)HellaSwag (Reason)BBH (Reason)
T ext Datasets020406080Scores51.6
30.662.478.082.3
36.1
26.871.2
43.253.4
32.765.879.983.9
43.8
27.571.7
45.0mPLUG-Owl2
w/o MAM
w/ MAMFigure 3. Performance of text benchmarks across various capabil-
ities under modality collaboration.
results summarized in Table 4. We employed two types of
evaluations: 1) Exact matching, which is commonly used
in previous video question-answering evaluations; and 2)
GPT-assistedevaluation[42]thatassessesthemodel’scapa-
bilitiesbymeasuringtheaccuracyofthemodel’sgenerated
predictions and providing a relative score on a scale of 1-5.
We observe that our model achieves superior results on all
threevideodatasetsunderazero-shotsetting. Furthermore,
in terms of relevancy, our model generates more accurate
answers than other video MLLMs, thereby demonstrating
its superiority and excellent generalization capabilities.
4.3. Discussion
Modality Collaboration for Text Performance. To
demonstrate how modality collaboration enhances not only
the multi-modal performance but also the text capability of
MLLMs, we evaluate the performance of text benchmarks
in terms of various abilities including examination, knowl-
edge, understanding, and reasoning. As observed in Figure
3, both examination and knowledge capabilities of MLLMs
havesignificantlyimprovedthankstothebenefitsofmodal-
ity collaboration facilitated by the modality-adaptive mod-
ule. This improvement arises because multi-modal data
allowsthemodeltoutilizevisualinformationtounderstand
concepts that cannot be described through language. Sim-
ilarly, the model can generate richer and more substantial
responses due to a more concrete understanding of these
concepts. Additionally, multi-modal data enhances the rea-
soning ability of the model because images contain rich
information(suchasrelationshipsandspatialaspects). The
model learns from these aspects and associates them with
the text, thereby indirectly enhancing the reasoning ability
of the text.
Impact of Joint Vision-Language Instruction Tuning.
Table 5 presents the results of instruction tuning with vari-
oustypesofdataaswellaswhetherusingmodality-adaptive
module. These results show that even without multi-
modal instruction data, the model’s performance on multi-
modalbenchmarksisrespectableduetotheeffectivevision-
languagealignmentachievedduringpre-training. However,MAM Text Inst. MM Inst. VQAv2 Q-Bench MMLU BBH
✓ 58.2 54.4 51.8 43.6
✓ 76.3 61.3 45.4 25.7
✓ ✓ 76.2 60.3 51.6 43.2
✓ ✓ 60.5 55.6 51.8 44.0
✓ ✓ 76.5 60.2 46.1 30.6
✓ ✓ ✓ 76.8 62.2 52.8 45.0
Table 5. Performance comparison among different types of in-
struction data and structures.
Unfreeze Layer-wise lr. VQAv2 TextVQA MMBench Q-Bench
74.8 39.8 63.8 60.7
✓ 76.2(+1.4) 40.3(+0.5) 62.7(-1.1) 61.6(+0.9)
✓ ✓ 76.8(+2.0) 42.5(+2.7) 64.5(+0.7) 62.2(+1.5)
Table 6. Influence of learning strategies for visual encoder.
# Learnable Queries VQAv2 TextVQA MMBench Q-Bench
8 58.3 18.6 47.6 52.4
16 66.2 28.5 52.9 54.9
32 72.4 36.3 60.2 57.8
64 76.8 42.5 64.5 62.2
128 76.7 44.4 63.6 61.6
Table 7. Performance in terms of number of learnable queries.
whensolelyusingmulti-modalinstructiondata,weobserve
an increase in performance on multi-modal datasets, while
performance on text tasks decreases by about 5.7%. This
phenomenon can be counterbalanced by the joint vision-
languagetuningproposed,asshowninthetable’sthirdrow,
where the multi-modal performance begins to slightly de-
crease due to modality interference. To counter this draw-
back, we apply our proposed modality-adaptive module to
themodel. Resultsshowthattheperformanceonbothmulti-
modal and text benchmarks improves, with a minimum in-
creaseof0.6%ontheVQAv2datasetand1.6%onMMLU.
Impact of Trainable Vision Encoder. Table6deliversthe
performance of the training vision encoder during instruc-
tion tuning with modality collaboration. It can be observed
that enabling the vision encoder to be trainable improves
performance on VQAv2 and Q-Bench by at least 1.4% and
0.9%, respectively, suggesting the benefits of modality col-
laboration. Conversely, it results in a 1.1% performance
drop in MM-Bench, indicating a degree of forgetting and
damage to the general visual representation due to the lim-
iteddiversityofinstructiondata. Tomitigatethischallenge,
weapplylayer-wiselearningratedecaywithanexponential
decay factor of 0.9, which preserves the representation of
lower layers while modifying higher semantic representa-
tions. By applying the layer-wise learning rate decay, we
cannoticethatperformanceonTextVQAhasincreasedfur-
ther with 2.2%, showing the effectiveness of our training
strategy.
Impact of Number of Learnable Queries. To investigate
theeffectofthenumberoflearnablequeries Q,weconduct
experimentsusingdifferentnumbersofqueriesinthevisual
7

--- PAGE 8 ---
abstractor, as shown in Table 7. It can be observed that
the model consistentlyexhibits improvement as the number
of learnable queries increases until it reaches a saturation
point, suggesting that 64 may be the optimal number for
representinganimage. Notably,thereisasignificantperfor-
manceboostobservedwhenthenumberisincreasedfrom8
to 64, e.g., the performance of VQAv2 is increased 18.5%.
These findings suggest that a higher number of learnable
queries can capture image information more comprehen-
sively,therebyenhancingthemodel’simagecomprehension
capabilities.
Resolution VQAv2 TextVQA MMBench MM-Vet Q-Bench
224×224 76.8 42.5 64.5 34.0 62.2
336×33678.5(+1.7) 49.8(+7.3) 65.2(+0.7) 34.6(+0.6) 62.4(+0.2)
448×44879.4(+2.6) 54.3(+11.8) 65.4(+0.9) 36.2(+2.2) 62.6(+0.4)
Table 8. Influence of different input image resolutions.
Impact of Image Resolution. Imageresolutionplaysacru-
cial role in vision-language tasks, as a higher resolution
can reduce image blur and improve understanding of fine-
grained details. To explore the impact of image resolution
on performance across different benchmarks, we adjust the
imageresolutionfrom 224×224to448×448andtheresults
arelistedinTable8. Asobservedinthetable,usingahigher
resolution proves advantageous for multi-modal tasks, par-
ticularly in the question answering scenario. Specifically,
theperformanceofVQAv2hasincreasedfrom76.8to79.4,
representinga2.6%boost. Simultaneously,thereisan11.8
point lift in the TextVQA benchmark when enlarging the
resolutionfrom 224×224to448×448. Thissuggeststhat
OCR-related tasks benefit significantly from increasing the
resolution.
4.4. Qualitative Analysis
Impact of Modality-Adaptive Module in Multi-Modal
Scenario. We investigate the impact of the Modality-
Adaptive Module in multi-modal scenarios by visualizing
the attention maps of mPLUG-Owl2 with and without this
module using image caption input, as shown in Figure 4.
Each attention map illustrates the attention scores of gen-
erated tokens on the input sequence during the generation
process.
It can be observed that regardless of whether the
Modality-AdaptiveModuleisincorporatedornot,themodel
focusesmoreonthetextualtokensintheearlierlayerswhile
payingmoreattentiontothevisualtokensinthelaterlayers.
This suggests that the modeling of visual and textual in-
formationplaysdifferentrolesinthecollaborationofmulti-
modallanguagemodels(MLLMs). Anintuitiveexplanation
isthatMLLMsinitiallyusesyntacticinformationtocompre-
hend instructions and then identify relevant visual content
tokens by considering the textual input.
Visual TokensTextual TokensVisual TokensTextual TokensVisual TokensTextual TokensVisual TokensTextual TokensVisual TokensTextual TokensVisual TokensTextual Tokensw/ Modality-Adaptive Modulew/o Modality-Adaptive Module
w/ Modality-Adaptive Modulew/o Modality-Adaptive Module
w/ Modality-Adaptive Modulew/o Modality-Adaptive Module
Layer #0
Layer #15
Layer #31Figure4. Visualizationoftheattentionmapswithandwithoutthe
Modality-Adaptive Module. We demonstrate the attention maps
forthe0-th,15-th,and31-stlayers,wheretherangeofvisualtokens
is indicated by orange and the range of text tokens is indicated by
blue.
When using the Modality-Adaptive Module, it can be
observed that the model explicitly pays more attention to
thetextualcontentintheearlierstagesandfocusesmoreon
thevisualcontentinthelaterstages. TheModality-Adaptive
Modulepreventsvisualandtextualtokensfrombeingtreated
asthesameandencouragescollaborationbetweendifferent
modalities.
Impact of Modality-Adaptive Module in Unrelated-
Modality Scenarios. We present a question: "What are
the seven colors of the rainbow?" along with a randomly
selected image. In this example, the image input acts as a
disturbance to the model. We aim to investigate the impact
of our module on data that contains unrelated modalities.
Theresponsesandattentionmapsofthemodelareshownin
Figure5. Ourproposedmodel,mPLUG-Owl2,whichincor-
poratestheModality-AdaptiveModule,accuratelyidentifies
allsevencolors. Duringthegenerationprocess,itcanbeob-
servedthatthemodelprimarilyfocusesonthetextualinput.
On the other hand, when the Modality-Adaptive Module is
not utilized, mPLUG-Owl2 only identifies six colors. The
model’sabilitytocomprehendtextinstructionsisdisrupted,
and it is also evident that it places more emphasis on the
image during generation. Thanks to the Modality-Adaptive
Module, mPLUG-Owl2 is better able to capture modality-
specific features when modeling multimodal inputs. This
enhances the adaptability of modality collaboration, result-
ing in reduced disturbance when the text and image are
unrelated.
5. Conclusion
In this paper, we present mPLUG-Owl2, a highly capable
generalist model by leveraging modality collaboration for
8

--- PAGE 9 ---
User: <image> What are the seven colors of the rainbow?Response: orange, yellow, green, blue, indigo, violet and white.Visual TokensTextual TokensAverageAttentionMap
AverageAttentionMapUser: <image> What are the seven colors of the rainbow?Response: Orange, Yellow, Green, Blue, Purple, and White.
Visual TokensTextual Tokens
mPLUG-Owl2 w/ Modality-Adaptive Module 
mPLUG-Owl2 w/o Modality-Adaptive ModuleFigure 5. Visualization of the attention maps with and without
the Modality-Adaptive Module. We demonstrate the average of
attention maps across each layer, where the range of visual tokens
is indicated by orange and the range of text tokens is indicated by
blue.
enhancing performance across both text and multi-modal
tasks. The inclusion of shared functional modules and a
modality-adaptivemoduleinmPLUG-Owl2strengthensthe
model’s ability to harmonize modality collaboration and
preserve modality-specific characteristics. The extensive
experimental evaluations highlight mPLUG-Owl2’s profi-
ciency in generalizing across various tasks, thereby achiev-
ing state-of-the-art performances with a singular, general-
ized model. Most notably, mPLUG-Owl2 stands as the first
MLLM model to exhibit the phenomena of modality col-
laborationinbothpure-textandmulti-modalcontexts. This
not only enhances the model’s vision-language understand-
ing but also improves its language capabilities in terms of
understanding, knowledge, and reasoning. This represents
a significant contribution to the field and opens up excit-
ingopportunitiesforthefuturedevelopmentofmulti-modal
foundation models.
References
[1] Sharegpt. http://sharegpt.com , 2023. 2, 5, 17
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances inNeural Information Processing Systems , 35:23716–23736,
2022. 2, 6
[3] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
SamirGadre,ShioriSagawa,etal. Openflamingo: Anopen-
source framework for training large autoregressive vision-
language models. arXiv preprint arXiv:2308.01390 , 2023.
16
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 4
[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: Afrontierlargevision-languagemodelwith
versatile abilities. ArXiv, abs/2308.12966, 2023. 2, 5, 6, 16
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
PranavShyam,GirishSastry,AmandaAskell,SandhiniAgar-
wal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu,
ClemensWinter,ChristopherHesse,MarkChen,EricSigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-
shot learners. ArXiv, abs/2005.14165, 2020. 1, 3
[7] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-
700m: Image-text pair dataset. https://github.com/
kakaobrain/coyo-dataset , 2022. 5
[8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
Usunier,AlexanderKirillov,andSergeyZagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213–229. Springer, 2020. 2,
3
[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 3558–3568, 2021. 5
[10] Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s
referential dialogue magic. ArXiv, abs/2306.15195, 2023. 2,
5, 15, 16
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-
scaled multilingual language-image model. arXiv preprint
arXiv:2209.06794 , 2022. 5
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann,ParkerSchuh,KensenShi,SashaTsvyashchenko,
Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay,
Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif,
Nan Du, Benton C. Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
9

--- PAGE 10 ---
mawat, Sunipa Dev, Henryk Michalewski, Xavier Gar-
cía, Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar-
ret Zoph, Alexander Spiridonov, Ryan Sepassi, David Do-
han, Shivani Agrawal, Mark Omernick, Andrew M. Dai,
Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor
Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polo-
zov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan
Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav
Petrov, and Noah Fiedel. Palm: Scaling language model-
ingwithpathways. J.Mach.Learn.Res. ,24:240:1–240:113,
2022. 2
[13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2
reasoningchallenge. arXivpreprintarXiv:1803.05457 ,2018.
6
[14] OpenCompass Contributors. Opencompass: A univer-
sal evaluation platform for foundation models. https:
//github.com/open-compass/opencompass , 2023. 6
[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pas-
cale Fung, and Steven C. H. Hoi. Instructblip: Towards
general-purposevision-languagemodelswithinstructiontun-
ing.ArXiv, abs/2305.06500, 2023. 2, 3, 5, 6, 13, 14, 15, 16
[16] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey
Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
and Peter R. Florence. Palm-e: An embodied multimodal
language model. In International Conference on Machine
Learning, 2023. 2, 5
[17] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,Meng-
danZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,Xiawu
Zheng, et al. Mme: A comprehensive evaluation bench-
mark for multimodal large language models. arXiv preprint
arXiv:2306.13394 , 2023. 5, 6
[18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-
acomp: In search of the next generation of multimodal
datasets. arXiv preprint arXiv:2304.14108 , 2023. 5
[19] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shĳie
Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xi-
angyuYue,HongshengLi,andYuJiaoQiao. Llama-adapter
v2: Parameter-efficient visual instruction model. ArXiv,
abs/2304.15010, 2023. 5, 6, 15, 16
[20] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
Luo, and Kai Chen. Multimodal-gpt: A vision and lan-
guage model for dialogue with humans. arXiv preprint
arXiv:2305.04790 , 2023. 15
[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: Ele-
vating the role of image understanding in Visual QuestionAnswering. In Conference on Computer Vision and Pattern
Recognition (CVPR) , 2017. 5, 13, 17
[22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
MantasMazeika,DawnSong,andJacobSteinhardt. Measur-
ingmassivemultitasklanguageunderstanding. arXivpreprint
arXiv:2009.03300 , 2020. 6
[23] ShaohanHuang,LiDong,WenhuiWang,YaruHao,Saksham
Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan
Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan
Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and
FuruWei. Languageisnotallyouneed: Aligningperception
with language models. ArXiv, abs/2302.14045, 2023. 2
[24] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
6700–6709, 2019. 5, 13, 17
[25] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and
Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning
in visual question answering. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition ,pages
2758–2766, 2017. 6
[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
imageannotations. Internationaljournalofcomputervision ,
123:32–73, 2017. 5, 15, 17
[27] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Er-
han Bas, Rahul Bhotika, and Stefano Soatto. Masked vision
andlanguagemodelingformulti-modalrepresentationlearn-
ing.arXiv preprint arXiv:2208.02131 , 2022. 2
[28] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bek-
man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,
Matthieu Cord, and Victor Sanh. Obelics: An open web-
scale filtered dataset of interleaved image-text documents,
2023. 13, 16
[29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking multi-
modal llms with generative comprehension. arXiv preprint
arXiv:2307.16125 , 2023. 5, 6
[30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model
with in-context instruction tuning. ArXiv, abs/2305.03726,
2023. 5, 15, 16
[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.
Hoi.Blip-2: Bootstrappinglanguage-imagepre-trainingwith
frozen image encoders and large language models. ArXiv,
abs/2301.12597, 2023. 2, 5, 6, 16
[32] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centricvideounderstanding. arXivpreprint
arXiv:2305.06355 , 2023. 6
[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji rong Wen. Evaluating object hallucination in
largevision-languagemodels. ArXiv,abs/2305.10355,2023.
13
10

--- PAGE 11 ---
[34] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland,
Austin Cook, Chanvichet Vong, and "Teknium". Slimorca:
An open dataset of gpt-4 augmented flan reasoning traces,
with verification, 2023. 5, 17
[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 5, 13, 17
[36] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lĳuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXiv preprint
arXiv:2306.14565 , 2023. 3
[37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. ArXiv,
abs/2310.03744, 2023. 3, 5, 14, 16
[38] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
Visual instruction tuning. ArXiv, abs/2304.08485, 2023. 2,
3, 4, 5, 13, 14, 15, 16, 17
[39] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, et al. Mmbench: Is your multi-modal model an
all-around player? arXiv preprint arXiv:2307.06281 , 2023.
5, 6, 13, 15
[40] IlyaLoshchilovandFrankHutter. Fixingweightdecayregu-
larization in adam. 2018. 5
[41] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-io: A unified
model for vision, language, and multi-modal tasks. ArXiv,
abs/2206.08916, 2022. 5
[42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and
FahadShahbazKhan.Video-chatgpt: Towardsdetailedvideo
understanding via large vision and language models. ArXiv,
abs/2306.05424, 2023. 6, 7
[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. Ok-vqa: A visual question answering
benchmark requiring external knowledge. In Proceedings
of the IEEE/cvf conference on computer vision and pattern
recognition , pages 3195–3204, 2019. 5, 13, 17
[44] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. Ocr-vqa: Visual question answering
by reading text in images. In 2019 international conference
on document analysis and recognition (ICDAR) , pages 947–
952. IEEE, 2019. 5, 13, 17
[45] OpenAI. Gpt-4v(ision) system card. 2023. 1
[46] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,
2023. 1, 2, 16
[47] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. ArXiv,
abs/2306.14824, 2023. 13, 16
[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learning
transferable visual models from natural language supervi-
sion. InInternationalconferenceonmachinelearning ,pages
8748–8763. PMLR, 2021. 3, 5, 17[49] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man,etal. Laion-5b: Anopenlarge-scaledatasetfortraining
next generation image-text models. Advances in Neural In-
formation Processing Systems , 35:25278–25294, 2022. 5
[50] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
KennethMarino,andRoozbehMottaghi.A-okvqa: Abench-
mark for visual question answering using world knowledge.
InEuropean Conference on Computer Vision , pages 146–
162. Springer, 2022. 5, 13, 17
[51] Noam Shazeer. Glu variants improve transformer. arXiv
preprint arXiv:2002.05202 , 2020. 3
[52] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-
lm: Training multi-billion parameter language models using
model parallelism. arXiv preprint arXiv:1909.08053 , 2019.
15
[53] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
Amanpreet Singh. Textcaps: a dataset for image caption-
ingwithreadingcomprehension. In ComputerVision–ECCV
2020: 16thEuropeanConference,Glasgow,UK,August23–
28, 2020, Proceedings, Part II 16 , pages 742–758. Springer,
2020. 5, 13, 17
[54] ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,Chun-
yuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-
XiongWang,YimingYang,KurtKeutzer,andTrevorDarrell.
Aligninglarge multimodalmodelswith factuallyaugmented
rlhf.ArXiv, abs/2309.14525, 2023. 13, 15, 16
[55] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowd-
hery,QuocVLe,EdHChi,DennyZhou,etal. Challenging
big-benchtasksandwhetherchain-of-thoughtcansolvethem.
arXiv preprint arXiv:2210.09261 , 2022. 6
[56] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto. Stanfordalpaca: Aninstruction-followingllama
model. https://github.com/tatsu-lab/stanford_
alpaca, 2023. 2
[57] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar-
tinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRoz-
ière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efficient foundation language
models.ArXiv, abs/2302.13971, 2023. 1, 3
[58] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-
bert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanielM.
Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen,
GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,
Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui
Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh
Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet,TodorMihaylov,PushkarMishra,IgorMolybog,Yixin
Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
11

--- PAGE 12 ---
Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan,
IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,
Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey
Edunov, and Thomas Scialom. Llama 2: Open foundation
andfine-tunedchatmodels. ArXiv,abs/2307.09288,2023. 1,
3, 5, 6, 17
[59] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
KevinLin,ZheGan,ZichengLiu,CeLiu,andLĳuanWang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100 , 2022. 5
[60] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,
Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong
Yan,GuangtaoZhai,andWeisiLin. Q-bench: Abenchmark
for general-purpose foundation models on low-level vision.
arXiv preprint arXiv:2309.14181 , 2023. 5, 6, 13, 16
[61] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao,
Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm:
Empowering large language models to follow complex in-
structions. ArXiv, abs/2304.12244, 2023. 2, 6
[62] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. InProceedings of the 25th ACM international confer-
ence on Multimedia , pages 1645–1653, 2017. 6
[63] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,
Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,
etal.mplug-2: Amodularizedmulti-modalfoundationmodel
acrosstext,imageandvideo. In InternationalConferenceon
Machine Learning , 2023. 2, 4
[64] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. In NeurIPS, 2022. 6
[65] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,
YuhaoDan,ChenlinZhao,GuohaiXu,ChenliangLi,Junfeng
Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl:
Modularizedmultimodallargelanguagemodelfordocument
understanding. CoRR, abs/2307.02499, 2023. 2
[66] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,
Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang,
et al. Ureader: Universal ocr-free visually-situated language
understandingwithmultimodallargelanguagemodel. In The
2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing , 2023. 2
[67] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian,
Ji Zhang, and Fei Huang. Hitea: Hierarchical temporal-
aware video-language pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15405–15416, 2023. 6
[68] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 2, 3, 4, 5, 15, 16
[69] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In Computer Vision–ECCV 2016: 14th EuropeanConference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.
5, 15, 17
[70] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lĳuan Wang.
Mm-vet: Evaluating large multimodal models for integrated
capabilities. arXivpreprintarXiv:2308.02490 ,2023. 5,6,16
[71] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An
instruction-tuned audio-visual language model for video un-
derstanding. ArXiv, abs/2306.02858, 2023. 6
[72] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up
visual instruction tuning. arXiv preprint arXiv:2307.04087 ,
2023. 3
[73] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gon-
zalez,andIonStoica. Judgingllm-as-a-judgewithmt-bench
and chatbot arena, 2023. 6
[74] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang,
ShuaiLu,YanlinWang,AminSaied,WeizhuChen,andNan
Duan. Agieval: A human-centric benchmark for evaluating
foundation models. arXiv preprint arXiv:2304.06364 , 2023.
6
[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. ArXiv,
abs/2304.10592, 2023. 2, 3, 5, 14, 15, 16
12

--- PAGE 13 ---
A. Additional Experimental Results
Inthissection,weprovidemoreexperimentalresultsforthe
completeness of our proposed method.
A.1. Hallucination Evaluation
Overall
Attribute
Adversarial
Comparison
Counting RelationEnvironmentHolisticOther
1.02.03.04.0
1.02.03.04.0
1.02.03.04.0
1.0
2.0
3.0
4.01.0
2.0
3.0
4.01.0
2.0
3.0
4.01.0
2.0
3.0
4.01.02.03.04.0 1.02.03.04.0
N/A
Kosmos-2
IDEFICS9BIDEFICS80B
InstructBLIP7BLLaVA7B
LLaVA-SFT+
7BLLaVA-RLHF7B
mPLUG-Owl2
Figure6. Detailedperformanceofvariousmodelsacrosstheeight
categories in MMHal-Bench [54], where "Overall" represents the
average performance across all categories.
We measure the hallucination of our model on image
description using MMHal-Bench [54] and compare the re-
sults with other recent vision-language models, including
Kosmos-2 [47], IDEFICS [28], InstructBLIP [15], LLaVA
[38],andLLaVA-RLHF[54]. Following[54],weuseGPT-4
toevaluatetheoverallscoreandhallucinationrateofdiffer-
ent MLLMs. As depicted in Figure 6, we find that our
mPLUG-Owl2 tends to generate the response with reduced
hallucination compared to other methods, especially sur-
passing IDEFICS [28] with 80 billion parameters, showing
the superiority of our methods. Besides, we can notice that
our model excels at attribute and counting because the vi-
sual abstractor can effectively identify the main parts of the
image, which reduces the hallucination.
WealsostudythehallucinationofrecentpopularMLLMs
and present the results in Figure 7. In the first example, the
query asks the models to recognize the pattern on the wall.
However, the pattern is not clearly visible in the image,
causing other models to mistakenly perceive it as a solid
color. Our model, on the other hand, accurately notices the
whitepatternonthewallandcorrectlyanswersthequestion.
In the second example, there are only a few trees in the
image. However, InstructBLIP incorrectly considers that
there are no trees in the image. LLaVA and LLaVA-1.5,
on the other hand, hallucinate and consider the tree in the
imagetobedense. MiniGPT-4givesthecorrectanswer,but
with minimal explanation. Our mPLUG-Owl2, however,answersthequestioncorrectlyandprovidesamoredetailed
explanation.
A.2. POPE Evaluation
We also conduct the hallucination evaluation using POPE
[33], the results are shown in Table 9. As we can observe
in the table, we can find mPLUG-Owl2 achieves higher F1
scores on the popular and adversarial split, showing the
robustness of our model in terms of object hallucination
compared to other MLLMs.
A.3. Detailed Evaluation Results on MMBench
MMBench [39] is a meticulously designed benchmark
that comprehensively assesses the diverse skills of vision-
language models. The results from the test set for various
MLLMs are presented in Table 10.
A.4. Detailed Evaluation Results on MM-Vet
WeprovidethedetailedresultsofMM-VetinTable11. Itcan
be observed that by training the visual encoder of mPLUG-
Owl2, it exhibits stronger OCR capability compared to the
model with the same backbone (i.e., LLaVA, Otter). Be-
sides, mPLUG-Owl2 surpasses models with stronger lan-
guage decoders such as LLaVA-13B which equips LLM
with 13 billion parameters.
A.5. Detailed Evaluation Results on Q-Bench
For evaluating the low-level visual perception abilities, we
have included the results of Q-Bench [60] on the test set.
Bytrainingthevisualencoder,theabilityofmPLUG-Owl2
in terms of low-level perception has been improved signif-
icantly, as it outperforms the model with a stronger visual
encoder (i.e., ViT-L (0.3B) v.s. ViT-G (1.9B)), showing the
effectiveness of our training paradigm.
A.6. Detailed Evaluation Results on MMHal-Bench
We include Table 13 for the full evaluation results on
MMHal-Bench [54].
B. Implementation
B.1. Data Mixture
Inthissection,wedetailourfinaltrainingdatamixtureused
duringtheinstructiontuningstageinTable14. Specifically,
we process the VQAv2 [21] data by selecting the answer
withthehighestconfidenceandcombiningquestion-answer
pairs that share the same image. This combining strategy
is also applied to GQA [24], OKVQA [43], and OCRVQA
[44]datasets. Additionally,formultiple-choicequestionsin
A-OKVQA [50], we augment the dataset by switching the
order of options to enhance robustness in terms of multiple
choices. ForcaptiondatasetslikeCOCO[35]andTextCaps
[53], we randomly select one caption from the ground truth
13

--- PAGE 14 ---
Is the wall solid color in this image?Input Image: 
InstructBLIPLLaV ALLaV A-1.5MiniGPT-4mPLUG-Owl2QueryInput Image: 
No, there is no tree in the image. Yes, the tree in the image is dense, with a large number of branches and leaves. Yes, the tree in this image is dense.The tree is not dense in this image.No, the tree in the image is not dense. It appears to be a small tree in the background.
!
!
Is the tree dense in this image?Yes, the wall in the image is solid color. Yes, the wall in the image is solid color, which is tan or beige. Yes, the wall in the image is solid color, which is pink.The wall in this image appears to be solid color.No, the wall in this image is not solid color. It has a pink and white pattern.InstructBLIPLLaV ALLaV A-1.5MiniGPT-4mPLUG-Owl2Query
!
!
Example#1
Example#2Figure 7. Example cases compared with InstructBLIP [15], LLAVA [38], LLAVA-1.5 [37], MiniGPT-4 [75] and our mPLUG-Owl2.
14

--- PAGE 15 ---
Table 9. Object hallucination benchmark using POPE evaluation pipeline . "Yes" signifies the likelihood of the model producing a
positive response.
Datasets Metrics mPLUG-Owl2 Shikra [10] InstructBLIP [15] MiniGPT-4 [75] LLaVA [38] MM-GPT [20] mPLUG-Owl [68]
RandomAccuracy ( ↑)88.28 86.90 88.57 79.67 50.37 50.10 53.97
Precision ( ↑)94.34 94.40 84.09 78.24 50.19 50.05 52.07
Recall ( ↑) 82.20 79.27 95.13 82.20 99.13 100.00 99.60
F1-Score ( ↑) 87.85 86.19 89.27 80.17 66.64 66.71 68.39
Yes (→50%)44.91 43.26 56.57 52.53 98.77 99.90 95.63
PopularAccuracy ( ↑)86.20 83.97 82.77 69.73 49.87 50.00 50.90
Precision ( ↑)89.46 87.55 76.27 65.86 49.93 50.00 50.46
Recall ( ↑) 82.06 79.20 95.13 81.93 99.27 100.00 99.40
F1-Score ( ↑) 85.60 83.16 84.66 73.02 66.44 66.67 66.94
Yes (→50%)45.86 45.23 62.37 62.20 99.40 100.00 98.57
AdversarialAccuracy ( ↑)84.12 83.10 72.10 65.17 49.70 50.00 50.67
Precision ( ↑)85.54 85.60 65.13 61.19 49.85 50.00 50.34
Recall ( ↑) 82.13 79.60 95.13 82.93 99.07 100.00 99.33
F1-Score ( ↑) 83.80 82.49 77.32 70.42 66.32 66.67 66.82
Yes (→50%)48.00 46.50 73.03 67.77 99.37 100.00 98.67
Method Language Model Vision Model Overall LR AR RR FP-S FP-C CP
MMGPT [20] LLaMA-7B CLIP ViT-L/14 16.0 1.1 23.8 20.7 18.3 5.2 18.3
MiniGPT-4 [75] Vicuna-7B EVA-G 12.0 13.6 32.9 8.9 28.8 11.2 28.3
InstructBLIP [15] Vicuna-7B EVA-G 33.9 21.6 47.4 22.5 33.0 24.4 41.1
LLaMA-Adapter-v2 [19] LLaMA-7B CLIP ViT-L/14 38.9 7.4 45.3 19.2 45.0 32.0 54.0
LLaVA [54] Vicuna-7B CLIP ViT-L/14 36.2 15.9 53.6 28.6 41.8 20.0 40.4
G2PT [39] Vicuna-7B ViT-G 39.8 14.8 46.7 31.5 41.8 34.4 49.8
Otter-I [30] LLaMA-7B CLIP ViT-L/14 48.3 22.2 63.3 39.4 46.8 36.4 60.6
mPLUG-Owl†[68] LLaMA-7B CLIP ViT-L/14 62.3 37.5 75.4 56.8 67.352.4 67.2
Shikra [10] Vicuna-7B CLIP ViT-L/14 60.2 33.5 69.6 53.1 61.8 50.4 71.7
mPLUG-Owl2 LLaMA2-7B CLIP ViT-L/14 65.429.2 69.7 61.767.0 60.0 79.5
Table 10. CircularEval multi-choice accuracy results on MMBench [39] dev set. We adopt the following abbreviations: LR for Logical
Reasoning; AR for Attribute Reasoning; RR for Relation Reasoning; FP-C for Fine-grained Perception (Cross Instance); FP-S for Fine-
grained Perception (Single Instance); CP for Coarse Perception. Baseline results are taken from [39].†denotes the model is carefully
optimized for MMBench.
for each image. Concurrently, some regional-VQA [26, 69]
datasets are also used to improve regional abilities.
B.2. Training Hyper-parameters
We report the detailed training hyper-parameter settings of
mPLUG-Owl2 in Table 15. Specifically, we leverage the
model parallelism with Megatron [52] distributed training
frameworktoensurealargerresolutiontrainingwhilemain-
taining efficiency.
C. Summary of the Evaluation Benchmarks
We provide a detailed summary of the used evaluation
benchmarks and corresponding metrics in Table 16.
D. Broader Impact
mPLUG-Owl2employsoff-the-shelfLLMandweb-sourced
data. Consequently, it inherits some of the weaknesses of
theoriginalLLMandweb-crawleddata,suchasgeneratinguncensored text or producing biased outputs. We address
these shortcomings by enhancing the model’s grounding
on the visual and instructional input and executing joint
vision-language instruction tuning on a diverse range of
high-quality datasets. However, we advise against deploy-
ingmPLUG-Owl2modelsforanydownstreamapplications
withoutpriorevaluationofsafetyandfairnessspecifictothe
respective application.
15

--- PAGE 16 ---
Model Rec OCR Know Gen Spat Math Total
Transformers Agent (GPT-4) [46] 18.2 3.9 2.2 3.2 12.4 4.0 13.4 ±0.5
MiniGPT-4-7B [75] 27.4 15.0 12.8 13.9 20.3 7.7 22.1 ±0.1
BLIP-2-12B [31] 27.5 11.1 11.8 7.0 16.2 5.8 22.4 ±0.2
LLaVA-7B [38] 28.0 17.1 16.3 18.9 21.2 11.5 23.8 ±0.6
MiniGPT-4-13B [75] 29.9 16.1 20.4 22.1 22.2 3.8 24.4 ±0.4
Otter-9B [30] 27.3 17.8 14.2 13.8 24.4 3.8 24.7 ±0.3
OpenFlamingo-9B [3] 28.7 16.7 16.4 13.1 21.0 7.7 24.8 ±0.2
InstructBLIP-13B [15] 30.8 16.0 9.8 9.0 21.1 10.5 25.6 ±0.3
InstructBLIP-7B [15] 32.4 14.6 16.5 18.2 18.6 7.7 26.2 ±0.2
LLaVA-7B (LLaMA-2) [38] 32.9 20.1 19.0 20.1 25.7 5.2 28.1 ±0.4
LLaMA-Adapter v2-7B [19] 38.5 20.3 31.4 33.4 22.9 3.8 31.4 ±0.1
LLaVA-13B (V1.3) [38] 38.1 22.3 25.2 25.8 31.311.2 32.5 ±0.1
LLaVA-13B (LLaMA-2) [38] 39.2 22.7 26.5 29.3 29.6 7.7 32.9 ±0.1
mPLUG-Owl2 41.3 27.4 27.5 27.9 30.3 7.7 36.2±0.1
Table 11. Evaluation results on various MLLMs regarding each core VL capability on MM-Vet [70]. Rec stands for recognition; Know
indicates knowledge; Gen is generation; Spat means spatial. All the numbers are presented in % and the full score is 100%.
Method Yes-or-no What How Distortion Others In-context Distortion In-context Others Overall
IDEFICS [28] 0.6004 0.4642 0.4671 0.4038 0.5990 0.4726 0.6477 0.5151
InstructBLIP [15] 0.7099 0.5141 0.4300 0.4500 0.6301 0.5719 0.6439 0.5585
Kosmos-2 [47] 0.6058 0.3124 0.3539 0.3865 0.4654 0.4349 0.4735 0.4334
LLaMA-Adapter-v2 [19] 0.6661 0.5466 0.5165 0.5615 0.6181 0.5925 0.5455 0.5806
LLaVA-1.5 [37] 0.6460 0.5922 0.5576 0.4798 0.6730 0.5890 0.7376 0.6007
LLaVA [38] 0.5712 0.5488 0.5185 0.4558 0.5800 0.5719 0.6477 0.5472
MiniGPT-4 [75] 0.6077 0.5033 0.4300 0.4558 0.5251 0.5342 0.6098 0.5177
mPLUG-Owl [68] 0.7245 0.5488 0.4753 0.4962 0.6301 0.6267 0.6667 0.5893
Otter [30] 0.5766 0.3970 0.4259 0.4212 0.4893 0.4760 0.5417 0.4722
Qwen-VL [5] 0.6533 0.6074 0.5844 0.5413 0.6635 0.5822 0.7300 0.6167
Shikra [10] 0.6909 0.4793 0.4671 0.4731 0.6086 0.5308 0.6477 0.5532
mPLUG-Owl2 0.7318 0.5531 0.5864 0.5374 0.7136 0.5788 0.7338 0.6294
Table 12. Detailed evaluation results for different MLLMs on the test set of Q-Bench [60].
MethodOverall Hallucination Score in Each Question Type ↑
Score↑ Rate↓Attribute Adversarial Comparison Counting Relation Environment Holistic Other
Kosmos-2 [47] 1.69 0.68 2.00 0.25 1.42 1.67 1.67 2.67 2.50 1.33
IDEFICS 9B[28] 1.89 0.64 1.58 0.75 2.75 1.83 1.83 2.50 2.17 1.67
IDEFICS 80B[28] 2.05 0.61 2.33 1.25 2.00 2.50 1.50 3.33 2.33 1.17
InstructBLIP 7B[15] 2.10 0.58 3.42 2.08 1.33 1.92 2.17 3.67 1.17 1.08
InstructBLIP 13B[15] 2.14 0.58 2.75 1.75 1.25 2.08 2.50 4.08 1.50 1.17
LLaVA 7B[38] 1.55 0.76 1.33 0.00 1.83 1.17 2.00 2.58 1.67 1.83
LLaVA-RLHF 7B[54]2.05 0.68 2.92 1.83 2.42 1.92 2.25 2.25 1.75 1.08
mPLUG-Owl2 2.17 0.56 3.67 2.25 2.17 2.75 1.25 2.08 1.50 1.75
Table 13. Detailed evaluation results for different MLMMs on MMHal-Bench.
16

--- PAGE 17 ---
Data Type Data Name Size
TextShareGPT [1] 40K
SlimOrca [34] 518K
Dialogue LLaVA [38] 158K
CaptionCOCO [35] 82K
TextCaps [53] 22K
VQAVQAv2 [21] 83K
GQA [24] 72K
OKVQA [43] 9K
OCRVQA [44] 80K
A-OKVQA [50] 50K
Regional-VQARefCOCO [69] 30K
VisualGenome [26] 86K
Total 1.23M
Table 14. Instruction-following Data Mixture of mPLUG-Owl2.
Configuration Pre-training Instruction Tuning
ViT init. CLIP-L/14 [48] Pre-train stage
LLM init. LLaMA-2 [58] LLaMA-2 [58]
Visual Abstractor init. Random Pre-train stage
Image resolution 224×224 448 ×448
ViT sequence length 256 1024
LLM sequence length 256 2048
Learnable query numbers 64 64
Optimizer AdamW
Optimizer hyperparameter β1= 0.9, β2= 0.98, ϵ= 1e−6
Peak learning rate 1e−42e−5
Minimum learning rate 1e−61e−7
ViT learning rate decay 0
ViT Drop path rate 0
Learning rate schedule Cosine
Weight decay 0.05 0
Gradient clip 1.0
Training steps 42,500 4,800
Warm-up steps 1,000 250
Global batch size 8,192 256
Gradient Acc. 16
Numerical precision bfloat16
Optimizer sharding ✓
Activation checkpointing ✓
Model parallelism 1 2
Pipeline parallelism 1
Table 15. Training hyper-parameters of mPLUG-Owl2.
17

--- PAGE 18 ---
Task Dataset Description Split Metric
Image CaptionCOCO Captioning of natural images karpathy-test CIDEr ( ↑)
Flickr30K Captioning of natural images karpathy-test CIDEr ( ↑)
General VQAVQAv2 VQA on natural images test-dev VQA Score ( ↑)
OKVQA VQA on natural images requiring outside knowledge val VQA Score ( ↑)
GQA VQA on scene understanding and reasoning test-balanced EM (↑)
VizWizQA VQA on photos taken by people who are blind test-dev VQA Score ( ↑)
TextVQA VQA on natural images containing text val VQA Score ( ↑)
SciQA-Img Multi-choice VQA on a diverse set of science topics test Accuracy ( ↑)
VideoQAMSRVTT-QA Video Question Answering test Accuracy ( ↑) / Relevance Score ( ↑)
MSVD-QA Video Question Answering test Accuracy ( ↑) / Relevance Score ( ↑)
TGIF-QA GIF Question Answering test Accuracy ( ↑) / Relevance Score ( ↑)
Text BenchmarkMMLU A benchmark designed to measure knowledge acquirement dev Accuracy ( ↑)
BBH A suite of 23 challenging BIG-Bench tasks test Accuracy ( ↑)
AGIEval A human-centric benchmark specifically designed to evaluate the general abilities of foundation model test Accuracy ( ↑)
ARC-c A multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. test Accuracy ( ↑)
ARC-e A multiple-choice question-answering dataset, containing questions from science exams from grade 3 to grade 9. test Accuracy ( ↑)
Instruction FollowingMME Open-ended VL Benchmark by yes/no questions Perception Accuracy ( ↑)
MMBench Open-ended VL Benchmark by Multi-choice VQA with Circular Evaluation test Accuracy ( ↑)
MM-Vet Open-ended VL Benchmark with Various Abilities test GPT- 4Score ( ↑)
SEED-Bench Open-ended VL Benchmark by Multi-choice VQA Image & Video Accuracy ( ↑)
Q-Bench Open-ended Low-level Vision Benchmark by Multi-choice VQA test Accuracy ( ↑)
HallucinationPOPE Object existence by yes/no questions random/popular/adversarial Accuracy / Precision / Recall / F1 ( ↑)
MMHal-Bench Open-ended hallucination benchmarks test GPT- 4Score ( ↑)
Table 16. Summary of the evaluation benchmarks of mPLUG-Owl2. EM stands for exacting matching.
18
