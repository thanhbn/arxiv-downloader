MoAI: Hỗn hợp của Tất cả Trí tuệ
cho Mô hình Ngôn ngữ và Thị giác Lớn

Byung-Kwan Lee, Beomchan Park, Chae Won Kim, và Yong Man Ro
Khoa Kỹ thuật Điện
Viện Khoa học và Công nghệ Tiên tiến Hàn Quốc (KAIST)
{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr

Tóm tắt. Sự phát triển của các mô hình ngôn ngữ lớn (LLMs) và điều chỉnh hướng dẫn đã dẫn đến xu hướng hiện tại của các mô hình ngôn ngữ và thị giác lớn được điều chỉnh hướng dẫn (LLVMs). Xu hướng này bao gồm việc tỉ mỉ tuyển chọn nhiều bộ dữ liệu điều chỉnh hướng dẫn phù hợp với các mục tiêu cụ thể hoặc mở rộng LLVMs để quản lý lượng lớn dữ liệu ngôn ngữ thị giác (VL). Tuy nhiên, các LLVMs hiện tại đã bỏ qua khả năng hiểu cảnh thế giới thực chi tiết và toàn diện có sẵn từ các mô hình thị giác máy tính (CV) chuyên biệt trong các tác vụ nhận thức thị giác như phân đoạn, phát hiện, tạo đồ thị cảnh (SGG), và nhận dạng ký tự quang học (OCR). Thay vào đó, các LLVMs hiện tại chủ yếu dựa vào khả năng lớn và các khả năng nổi lên của xương sống LLM của chúng. Do đó, chúng tôi trình bày một LLVM mới, Hỗn hợp của Tất cả Trí tuệ (MoAI), tận dụng thông tin thị giác phụ trợ thu được từ các đầu ra của các mô hình phân đoạn, phát hiện, SGG, và OCR bên ngoài. MoAI hoạt động thông qua hai mô-đun mới được giới thiệu: MoAI-Compressor và MoAI-Mixer. Sau khi chuyển đổi các đầu ra của các mô hình CV bên ngoài thành ngôn ngữ, MoAI-Compressor căn chỉnh và nén chúng để sử dụng hiệu quả thông tin thị giác phụ trợ có liên quan cho các tác vụ VL. MoAI-Mixer sau đó kết hợp ba loại trí tuệ—(1) đặc trưng thị giác, (2) đặc trưng phụ trợ từ các mô hình CV bên ngoài, và (3) đặc trưng ngôn ngữ—sử dụng khái niệm Hỗn hợp các Chuyên gia. Thông qua sự tích hợp này, MoAI vượt trội đáng kể so với cả LLVMs mã nguồn mở và mã nguồn đóng trong nhiều tác vụ VL zero-shot, đặc biệt là những tác vụ liên quan đến hiểu cảnh thế giới thực như sự tồn tại của đối tượng, vị trí, mối quan hệ, và OCR mà không cần mở rộng kích thước mô hình hoặc tuyển chọn thêm bộ dữ liệu điều chỉnh hướng dẫn thị giác. Mã nguồn có sẵn tại https://github.com/ByungKwanLee/MoAI.

Từ khóa: Mô hình Ngôn ngữ và Thị giác Lớn · Hỗn hợp các Chuyên gia

1 Giới thiệu

Việc kết hợp các mô hình ngôn ngữ lớn (LLMs) như PaLM [15] và T5 [78] với các bộ dữ liệu điều chỉnh hướng dẫn từ Flan [86], Chung et al. [17] đã phát triển Flan-PaLM và Flan-T5 cho các LLMs được điều chỉnh hướng dẫn. Các mô hình này tận dụng bộ dữ liệu điều chỉnh hướng dẫn mở rộng bao gồm các tác vụ khác nhau, và đã được mở rộng thêm để tăng khả năng của chúng, dẫn đến những cải thiện đáng chú ý trong hiệu suất zero-shot trên nhiều tác vụ ngôn ngữ.

Cùng với thành công của các LLMs được điều chỉnh hướng dẫn, một số bộ dữ liệu điều chỉnh hướng dẫn thị giác [4,13,19,65,85] đã được tỉ mỉ tuyển chọn để nâng cao hiệu suất ngôn ngữ thị giác (VL) zero-shot trong các mô hình ngôn ngữ và thị giác lớn (LLVMs). Hơn nữa, những nỗ lực phối hợp đã được thực hiện để mở rộng đáng kể LLVMs [1,4,64,85], nhằm mục tiêu đạt được hiệu suất zero-shot mạnh mẽ trong các bộ dữ liệu VL. Với việc mở rộng bộ dữ liệu điều chỉnh hướng dẫn thị giác và việc mở rộng LLVMs, các LLVMs mã nguồn mở [1,4,10,13,19,29,64,65,85,92,97] đã thu hẹp khoảng cách về hiệu suất VL zero-shot so với các LLVMs mã nguồn đóng như GPT-4V [72,73], Gemini-Pro [83], và Qwen-VL-Plus [4].

Tuy nhiên, các LLVMs mã nguồn mở hiện tại chưa tận dụng một cách rõ ràng hoặc đầy đủ khả năng hiểu cảnh thế giới thực chi tiết và toàn diện, chủ yếu dựa vào khả năng lớn và các khả năng nổi lên của xương sống LLM của chúng. Một số nghiên cứu trong khoa học nhận thức và học máy [6,22,25] lập luận rằng khả năng nhận thức cảnh cơ bản có thể bắt nguồn từ các chức năng nhận thức khác nhau, bao gồm nhận dạng sự hiện diện của đối tượng, xác định vị trí của chúng, nhận dạng trạng thái của chúng, hiểu mối quan hệ của chúng, trích xuất bố cục cảnh không gian, và nắm bắt các khái niệm không phải đối tượng có thể bao gồm văn bản viết. May mắn thay, các chức năng nhận thức này có thể được thu được từ các mô hình thị giác máy tính (CV) chuyên biệt đã được nghiên cứu và phát triển qua nhiều thập kỷ cho các tác vụ nhận thức thị giác như phân đoạn [14,37], phát hiện [70,98], tạo đồ thị cảnh (SGG) [42,88], và nhận dạng ký tự quang học (OCR) [23,57].

Chuyển hướng tập trung từ điều chỉnh hướng dẫn sang sử dụng các mô hình CV bên ngoài này được kỳ vọng sẽ nâng cao khả năng hiểu cảnh thế giới thực của LLVMs, bao gồm sự tồn tại của đối tượng, vị trí, mối quan hệ, và OCR. Nhận dạng các đối tượng và vị trí của chúng [52] có thể được hỗ trợ bởi các mô hình phân đoạn toàn cảnh và phát hiện đối tượng thế giới mở. Để hiểu toàn diện hơn, bao gồm trạng thái và mối quan hệ của đối tượng (tức là lý luận tổng hợp [22]), cần có mô hình tạo đồ thị cảnh (SGG). Hơn nữa, các mô tả văn bản trong hình ảnh như một khái niệm không phải đối tượng có thể được nhận dạng thông qua mô hình OCR.

Dưới ánh sáng này, chúng tôi đề xuất một LLVM mới, Hỗn hợp của Tất cả Trí tuệ (MoAI), tận dụng thông tin thị giác phụ trợ thu được từ các nguồn khác nhau: (1) phân đoạn toàn cảnh [14], (2) phát hiện đối tượng thế giới mở [70], (3) SGG [88], và (4) mô hình OCR [23]. Để tận dụng hiệu quả thông tin này, chúng tôi giới thiệu hai mô-đun mới: MoAI-Compressor và MoAI-Mixer. MoAI-Compressor căn chỉnh và nén các đầu ra đã được chuyển đổi thành ngôn ngữ của các mô hình CV bên ngoài thành thông tin thị giác phụ trợ, cho phép sử dụng hiệu quả thông tin có liên quan cho các tác vụ VL. Tiếp theo, MoAI-Mixer kết hợp ba loại trí tuệ—(1) đặc trưng thị giác, (2) đặc trưng phụ trợ từ các mô hình CV bên ngoài, và (3) đặc trưng ngôn ngữ—thành một tổng thể gắn kết.

Trong việc xây dựng MoAI-Mixer, chúng tôi lấy cảm hứng từ khái niệm Hỗn hợp các Chuyên gia (MoE) [71,79,80,96]. Thách thức của chúng tôi nằm ở việc tích hợp liền mạch các đặc trưng gốc (tức là đặc trưng thị giác và ngôn ngữ) được sử dụng trong mô hình ngôn ngữ đa phương thức (MLM) của MoAI—một xương sống LLM nhận các token thị giác được xuất ra bởi bộ mã hóa thị giác cùng với các token văn bản—với các đặc trưng phụ trợ thu được từ các mô hình CV bên ngoài và MoAI-Compressor. Chúng tôi sử dụng các mô-đun attention chéo và tự-attention để xây dựng sáu mô-đun chuyên gia trong MoAI-Mixer, bao gồm ba loại trí tuệ nói trên. Hơn nữa, chúng tôi sử dụng các mạng cổng để xác định tổ hợp trọng số tối ưu cho các mô-đun chuyên gia này.

Bằng cách kết hợp MoAI-Compressor và MoAI-Mixer, MoAI hiệu quả sử dụng các đầu ra từ các mô hình CV bên ngoài và trộn ba nguồn trí tuệ, do đó nâng cao khả năng nhận thức thị giác để giải quyết các tác vụ trả lời câu hỏi phức tạp. Như được miêu tả trong Hình 2, kết quả của chúng tôi chứng minh rằng MoAI đã vượt trội đáng kể về điểm nhận thức thị giác so với ba mô hình LLVM mạnh mẽ: InstructBLIP [19], Qwen-VL [4], LLaVA1.5 [63], ngay cả khi không cần tuyển chọn thêm bộ dữ liệu điều chỉnh hướng dẫn thị giác hoặc mở rộng LLVMs. Hơn nữa, nhờ vào khả năng nhận thức thị giác được cải thiện, MoAI thể hiện hiệu suất zero-shot mạnh mẽ trong các tác vụ VL, vượt trội hơn các LLVMs mã nguồn đóng, như được minh họa trong Hình 1. Thành công của MoAI được gán cho việc sử dụng thông tin thị giác phụ trợ đa dạng từ các mô hình CV bên ngoài và sự tích hợp của ba loại trí tuệ để thực hiện hiệu quả các tác vụ VL. Đóng góp của chúng tôi có thể được tóm tắt trong hai khía cạnh chính như sau:

– Chúng tôi giới thiệu một mô hình ngôn ngữ và thị giác lớn mới, MoAI, xử lý các thông tin thị giác phụ trợ khác nhau từ các mô hình CV bên ngoài (MoAI-Compressor) và kết hợp ba loại trí tuệ (MoAI-Mixer).

– MoAI nổi bật với khả năng nhận thức thị giác đặc biệt trong các tác vụ VL, vượt trội hơn cả LLVMs mã nguồn mở và mã nguồn đóng trong hiệu suất VL zero-shot. Khả năng này đạt được bằng cách xem xét khả năng hiểu cảnh thế giới thực chi tiết và toàn diện mà không cần mở rộng kích thước mô hình hay kích thước bộ dữ liệu.

2 Các Công trình Liên quan

LLMs và LLVMs. LLMs đã xuất hiện cùng với khả năng tổng quát hóa thành thạo và hiệu quả của các bộ dữ liệu điều chỉnh hướng dẫn. GPTs [7, 76,77] đóng vai trò quan trọng trong việc mở đường cho LLMs bằng cách chứng minh hiệu suất zero-shot hoặc few-shot mạnh mẽ trên các tác vụ ngôn ngữ khác nhau, bao gồm phân loại văn bản, trả lời câu hỏi, dịch máy, các tác vụ lý luận phức tạp, v.v. Những khả năng tổng quát hóa này của LLMs đã đạt được bằng cách tăng to cả khả năng mô hình và bộ dữ liệu huấn luyện, như thấy trong các công trình như T5 [78], PaLM [15], OPT [93]. Tiến bộ trong phương pháp huấn luyện và bộ dữ liệu tiếp tục nâng cao khả năng tổng quát hóa zero-shot của LLMs, chuyển từ các bộ dữ liệu tiền huấn luyện quy mô lớn sang các bộ dữ liệu điều chỉnh hướng dẫn [17,34,74, 86]. Điều chỉnh hướng dẫn [86] cho phép LLMs tuân theo hướng dẫn bằng ngôn ngữ tự nhiên của con người trong các tình huống thế giới thực phức tạp. Các LLMs được điều chỉnh hướng dẫn, như Flan-T5, Flan-PaLM [17], OPT-IML [34], và InstructGPT [74], rõ ràng chứng minh hiệu quả của điều chỉnh hướng dẫn. Các nhà nghiên cứu đã tiến xa hơn bằng cách áp dụng các chiến lược tương tự cho các đối tác đa phương thức, LLVMs, bao gồm một bộ mã hóa thị giác và một mô hình ngôn ngữ đa phương thức xương sống (MLM).

Ví dụ, LLaVA [65] và ShareGPT4V [13] sử dụng GPT-4 [2] và GPT-4V [72,73], tương ứng, để tạo các bộ dữ liệu điều chỉnh hướng dẫn thị giác, trong khi những nhà nghiên cứu khác [4,19,85] cũng đã phát triển các bộ dữ liệu điều chỉnh hướng dẫn thị giác khác nhau cho các mục tiêu riêng biệt của họ. Tuy nhiên, các LLVMs hiện tại đã bỏ qua khả năng hiểu cảnh thế giới thực chi tiết và toàn diện có sẵn từ các mô hình CV với những tiến bộ lớn trong nhiều thập kỷ qua. Các mô hình CV đã bị lu mờ bởi các LLVMs với khả năng mở rộng và bộ dữ liệu điều chỉnh hướng dẫn thị giác trong kỷ nguyên LLVMs. Từ góc độ này, MoAI nhấn mạnh hiệu quả của việc sử dụng thông tin thị giác phụ trợ thu được từ các mô hình CV bên ngoài, cho thấy khả năng nhận thức thị giác được nâng cao cho các tiêu chuẩn VL.

Hỗn hợp các Chuyên gia. Jacobs et al. [36] đã đầu tiên giới thiệu khái niệm Hỗn hợp các Chuyên gia (MoE) cho học máy, nơi các mạng riêng biệt gọi là 'chuyên gia' xử lý các phần khác nhau của không gian đầu vào, và mỗi phần được hướng dẫn đến các chuyên gia có liên quan bởi một mạng cổng. Ý tưởng này được phát triển thêm bởi MoE sâu [24] nơi các lớp MoE được xếp chồng theo chiều sâu, và bởi tính toán có điều kiện [5] nơi chỉ một vài chuyên gia được kích hoạt có điều kiện bởi một đầu vào nhất định. Trong học sâu hiện đại, Shazeer et al. [80] tích hợp lớp MoE với LSTMs [32] nơi mạng cổng độc lập định tuyến mỗi token đến các chuyên gia được kích hoạt một cách có chọn lọc. Sự tích hợp này nâng cao hiệu suất trong các tác vụ mô hình hóa ngôn ngữ và dịch máy. Hơn nữa, Switch Transformers [26] hợp nhất lớp MoE và Transformers [84] bằng cách thay thế mạng feed forward dày đặc (FFN) bên trong lớp Transformer bằng nhiều chuyên gia và một mạng cổng, mở đường cho việc sử dụng thành công MoE trong các LLVMs dựa trên Transformer như MoE-LLaVA [59]. Triết lý của MoE trong học sâu là mở rộng khả năng mô hình mà không hy sinh hiệu quả tính toán [24,26,38,45,59,80, 99]. Mặt khác, chúng tôi tập trung vào một khía cạnh khác nhau nhưng cơ bản của MoE, nơi chúng tôi có ý định rằng mỗi chuyên gia được thiết kế để chuyên môn hóa trong một phần cụ thể của đầu vào. Trong khi các phương pháp MoE trước đây không gán vai trò rõ ràng cho từng chuyên gia và thay vào đó mong đợi chuyên môn hóa xuất hiện trong quá trình tối ưu hóa, MoAI chỉ định các mô-đun attention chéo và tự-attention làm chuyên gia và học chúng một cách rõ ràng để trộn thông tin giữa các phương thức (tức là đặc trưng thị giác, phụ trợ, và ngôn ngữ). Cụ thể, MoAI tạo thuận lợi cho các cặp (1) đặc trưng thị giác-phụ trợ, (2) đặc trưng thị giác-ngôn ngữ, (3) đặc trưng thị giác-thị giác, (4) đặc trưng ngôn ngữ-phụ trợ, (5) đặc trưng ngôn ngữ-thị giác, và (6) đặc trưng ngôn ngữ-ngôn ngữ. Mỗi cặp được xem xét như một cặp query-key cho một mô-đun attention chéo hoặc tự-attention tương ứng phục vụ như chuyên gia, làm rõ việc hợp nhất thông tin qua các phương thức đa dạng.

3 MoAI: Hỗn hợp của Tất cả Trí tuệ

Kiến trúc Mô hình. Như được mô tả trong Hình 3, MoAI bao gồm một bộ mã hóa thị giác, một mô hình ngôn ngữ đa phương thức xương sống (MLM) được trang bị MoAI-Mixers, các kết nối MLP trung gian giữa bộ mã hóa thị giác và MLM, và một MoAI-Compressor tận dụng bốn mô hình thị giác máy tính (CV) bên ngoài cho phân đoạn toàn cảnh [14], phát hiện đối tượng thế giới mở [70], tạo đồ thị cảnh (SGG) [88], và nhận dạng ký tự quang học (OCR) [23]. MoAI-Compressor được giới thiệu để xử lý thông tin thị giác phụ trợ đa dạng thu được từ các mô hình CV bên ngoài, nơi các đầu ra của mô hình CV được xử lý thông qua chuyển đổi thành ngôn ngữ như được hiển thị trong Hình 4 để làm cho chúng được căn chỉnh và có thể diễn giải đối với MLM được sử dụng trong MoAI. Ngoài ra, MoAI-Mixer được trình bày thêm để hài hòa hiệu quả hai đặc trưng gốc (tức là đặc trưng thị giác và ngôn ngữ) với các đặc trưng phụ trợ từ các mô hình CV bên ngoài. Chi tiết về chuyển đổi thành ngôn ngữ, MoAI-Compressor, và MoAI-Mixer sẽ được giải thích trong phần này.

Xương sống Thị giác và Ngôn ngữ. CLIP-L/14 [75] được chọn làm bộ mã hóa thị giác, do khả năng hiểu hình ảnh được đảm bảo phù hợp với văn bản cho các tác vụ ngôn ngữ thị giác [13,63–65]. MLM được sử dụng trong MoAI dựa trên InternLM2-7B [8], một mô hình nền tảng đa ngôn ngữ được điều chỉnh hướng dẫn bởi các bộ dữ liệu đa ngôn ngữ với 1.6T token thông qua một loạt các giai đoạn tiền huấn luyện tiến bộ và học tăng cường từ phản hồi của con người (RLHF) [16,74,82]. Hai lớp tuyến tính với hàm kích hoạt GELU [31] phục vụ như kết nối cầu nối giữa các thành phần thị giác và ngôn ngữ, được ký hiệu là 'MLP' trong Hình 3.

Chuyển đổi thành Ngôn ngữ. Vì một mô hình ngôn ngữ đa phương thức (MLM) được áp dụng để xây dựng MoAI, chúng tôi chuyển đổi các đầu ra của mô hình CV thành định dạng ngôn ngữ tự nhiên để làm cho chúng có thể hiểu được đối với MLM thông qua một quá trình gọi là chuyển đổi thành ngôn ngữ. Hình 4 minh họa cách các đầu ra của bốn mô hình CV trải qua chuyển đổi thành ngôn ngữ cùng với việc tạo ra các token phụ trợ được căn chỉnh ngữ nghĩa với MLM.

Một mô hình phân đoạn toàn cảnh cho phép chúng ta phân biệt các đối tượng tiền cảnh và nền cùng lúc. Hơn nữa, chúng ta có thể tính toán tọa độ khung giới hạn (ví dụ, [xmin, ymin, xmax, ymax]) từ bản đồ phân đoạn. Do đó, việc chuyển đổi thành ngôn ngữ các đầu ra từ phân đoạn toàn cảnh (PS) bao gồm việc tuần tự hóa tọa độ khung giới hạn và tên đối tượng của chúng như được giải thích trong Hình 4. Những mô tả đã được chuyển đổi thành ngôn ngữ này sau đó được chuyển đổi thành các token phụ trợ thông qua nhúng từ của MLM. Ngoài ra, để sử dụng trực tiếp bản đồ phân đoạn toàn cảnh, chúng tôi sử dụng bộ mã hóa thị giác và kết nối MLP trong MoAI để tạo ra các token phụ trợ bảo toàn tính địa phương. Các token phụ trợ được tạo ra được làm phẳng và nối với những token từ các khung giới hạn được tuần tự hóa và tên đối tượng của chúng để tạo thành các token phụ trợ PS cuối cùng APS. Chúng được nối theo cách này để MLM của MoAI có thể liên kết chúng một cách tương thích thông qua việc ngữ cảnh hóa. Quy trình này đảm bảo việc chuyển đổi toàn diện thông tin thị giác từ PS thành thông tin ngôn ngữ trong khi bảo toàn tính địa phương không gian vốn có trong bản đồ phân đoạn toàn cảnh. Lưu ý rằng nếu mô hình phân đoạn toàn cảnh không thể phân loại các đối tượng trong số cố định các danh mục đối tượng toàn cảnh, ví dụ, những danh mục trong MS-COCO 2017 [60] bao gồm 133 danh mục đối tượng, lớp không xác định được gán.

Một mô hình phát hiện đối tượng thế giới mở đóng vai trò phát hiện các lớp đối tượng bị bỏ lỡ bởi mô hình phân đoạn toàn cảnh. Điều này là do mô hình phân đoạn toàn cảnh được huấn luyện trên một bộ dữ liệu cụ thể với số lượng danh mục đối tượng cố định. Khi kết quả phát hiện được tạo ra cho một hình ảnh, tọa độ khung giới hạn và tên đối tượng của chúng được chuyển đổi thành ngôn ngữ theo định dạng mẫu sau: 'Hình ảnh bao gồm các khung giới hạn và đối tượng của chúng: {kết quả phát hiện đối tượng thế giới mở (OWOD) đã được chuyển đổi thành ngôn ngữ}'. Sau đó, kết quả được chuyển đổi thành các token phụ trợ OWOD AOWOD bởi nhúng từ của MLM. Tương tự, các đầu ra của mô hình SGG và OCR được chuyển đổi thành ngôn ngữ, và các token phụ trợ tương ứng ASGG và AOCR được tạo ra, nơi chúng tôi sử dụng các mẫu chuyển đổi thành ngôn ngữ sau: 'Hình ảnh bao gồm các mối quan hệ giữa các đối tượng: {kết quả SGG đã được chuyển đổi thành ngôn ngữ}' và 'Hình ảnh bao gồm các mô tả văn bản: {kết quả OCR đã được chuyển đổi thành ngôn ngữ}', tương ứng.

MoAI-Compressor. Sau khi chuyển đổi thành ngôn ngữ các đầu ra của mô hình CV, bốn token phụ trợ APS, AOWOD, ASGG, và AOCR được tạo ra và tiêm vào MoAI-Compressor, được mượn cấu trúc từ Perceiver Resampler [3]. Tất cả bốn token phụ trợ [APS, AOWOD, ASGG, AOCR] được nối trước khi được đưa vào MoAI-Compressor cùng với số lượng cố định các token có thể học Ainput, mà các đầu ra A của chúng cũng cố định về độ dài bằng cùng một số và đại diện cho thông tin thị giác phụ trợ được nén và căn chỉnh, được công thức hóa như sau:

A = MoAI-Compressor([APS, AOWOD, ASGG, AOCR], Ainput). (1)

Do độ dài biến thiên của các token phụ trợ được nối qua các hình ảnh và độ dài đáng kể của chúng sau khi nối, MoAI-Compressor được thiết kế để nén các token đó [APS, AOWOD, ASGG, AOCR] với kích thước cố định tương đối nhỏ là 64, tạo ra A ∈ Rd×64 nơi d đại diện cho chiều nhúng. Các token được nén này sau đó được sử dụng để trích xuất thông tin có liên quan cho các tác vụ VL bởi MoAI-Mixer. Việc nén này nâng cao hiệu quả tính toán.

MoAI-Mixer được nhúng trong mỗi lớp MLM của MoAI. Nó nhận các token phụ trợ A từ MoAI-Compressor, đặc trưng thị giác I(l) ∈ Rd×NI, và đặc trưng ngôn ngữ L(l) ∈ Rd×NL nơi l = 0, 1, ···, N−1 biểu thị chỉ số lớp, d biểu thị chiều nhúng, NI biểu thị độ dài của đặc trưng thị giác, và NL biểu thị độ dài của đặc trưng ngôn ngữ. Thông thường, một lớp MLM chỉ bao gồm một khối Transformer decoder TransDec(l) sao cho [I(l+1), L(l+1)] = TransDec(l)([I(l), L(l)]). Trong MoAI, lớp MLM thứ l với MoAI-Mixer được công thức hóa như sau:

[Î(l), L̂(l)] = MoAI-Mixer(l)(A, I(l), L(l)),
[I(l+1), L(l+1)] = TransDec(l)(Î(l), L̂(l)), (2)

nơi Î(l) và L̂(l) là đặc trưng thị giác hỗn hợp và đặc trưng ngôn ngữ hỗn hợp. Trong mỗi MoAI-Mixer, chúng tôi thiết kế sáu mô-đun chuyên gia là các mô-đun attention chéo hoặc tự-attention như được minh họa trong Hình 5: ba cho đặc trưng thị giác I và ba cho đặc trưng ngôn ngữ L. Mỗi trong ba mô-đun chuyên gia cho đặc trưng thị giác xuất ra IAUX, ILANG, và ISELF nơi chữ hoa biểu thị đặc trưng truy vấn và chỉ số dưới biểu thị đặc trưng khóa/giá trị. Tương tự, mỗi trong ba mô-đun chuyên gia cho đặc trưng ngôn ngữ xuất ra LAUX, LIMG, và LSELF. Thao tác attention chéo tại lớp thứ l được công thức hóa như sau:

I(l){AUX hoặc LANG} = CA(l)(q = I(l), k = {A hoặc L(l)}, v = k),
L(l){AUX hoặc IMG} = CA(l)(q = L(l), k = {A hoặc I(l)}, v = k). (3)

Ngoài ra, thao tác tự-attention được công thức hóa là I(l)SELF = SA(l)(I(l)) và L(l)SELF = SA(l)(L(l)). Sáu mô-đun chuyên gia này chuyên môn hóa rõ ràng trong một trong sáu hỗn hợp trí tuệ riêng biệt sau: IAUX, ILANG, ISELF, LAUX, LIMG, và LSELF. Khi huấn luyện các mô-đun chuyên gia, chúng tôi mượn khái niệm LoRA [33] để giảm gánh nặng tính toán. Hãy ký hiệu W như một ký hiệu chung cho một lớp phép chiếu tuyến tính trong mô-đun multi-head attention [84], có thể là Wq, Wk, Wv, hoặc Wo. Chúng tôi phân tách W ∈ Rd×d, không phải ΔW như trong LoRA, thành hai lớp tuyến tính WA ∈ Rd×r và WB ∈ Rr×d sao cho W = WAWB. Siêu tham số r biểu thị chiều giảm như được minh họa trong Hình 6(a). Vì gánh nặng tính toán của một mô-đun attention chủ yếu đến từ chiều nhúng cao, thường d = 4096, công thức hóa các ma trận phép chiếu như vậy giảm đáng kể tính toán. Hơn nữa, các đặc trưng truy vấn đầu vào được cộng trực tiếp vào các đặc trưng đầu ra để hỗn hợp trí tuệ xảy ra mà không thay đổi quá nhiều các đầu ra của lớp MLM trước đó, ổn định quá trình tối ưu hóa với các khối Transformer decoder bị đóng băng.

Bước Huấn luyện Đầu tiên. Cùng với kết nối MLP, chúng tôi đầu tiên huấn luyện Ainput, MoAI-Compressor, và MoAI-Mixer bằng cách sử dụng các bộ dữ liệu điều chỉnh hướng dẫn thị giác [13, 63]. Bước này đảm bảo rằng sáu mô-đun chuyên gia trong MoAI-Mixer tạo ra các đặc trưng có ý nghĩa để thực hiện các tác vụ VL. Để làm điều này, chúng tôi ngẫu nhiên chọn đầu ra từ một trong ba mô-đun chuyên gia cho đặc trưng thị giác và ngôn ngữ, tương ứng, như sau:

Î(l) = Sample(I(l)AUX, I(l)LANG, I(l)SELF), L̂(l) = Sample(L(l)AUX, L(l)IMG, L(l)SELF). (4)

Sau đó, chúng được tiêm vào khối transformer decoder TransDecl(Î(l), L̂(l)). Quá trình lấy mẫu này nhằm mục đích cho mỗi mô-đun chuyên gia tạo ra các đặc trưng có ý nghĩa một cách độc lập.

Bước Huấn luyện Thứ hai. Trong bước này, chúng tôi mở rộng quá trình học tập vượt ra ngoài các tham số đã học trong bước huấn luyện đầu tiên. Chúng tôi học hai mạng cổng cho mỗi MoAI-Mixer, bao gồm một lớp tuyến tính duy nhất, mỗi lớp cho đặc trưng thị giác và ngôn ngữ: WGatingI và WGatingL ∈ Rd×3, được minh họa trong Hình 6(b). Các mạng cổng nhằm mục đích xuất ra tổ hợp trọng số tốt nhất cho ba mô-đun chuyên gia cho đặc trưng thị giác và ngôn ngữ mỗi loại bằng cách sử dụng một lớp tuyến tính và một hàm softmax như sau: Softmax(xTWGatingx, dim=1). Lưu ý rằng x ∈ Rd×Nx, nơi x là đặc trưng thị giác I hoặc ngôn ngữ L và Nx là độ dài của đặc trưng, dẫn đến xTWGatingx ∈ RNx×3. Sau đó, chúng tôi chia ma trận softmax thành ba vector trọng số: Softmax(xTWGatingx, dim=1) → [wAUX, wLANG, wSELF] nơi mỗi trọng số có chiều RNx. Các trọng số phục vụ như điểm tin cậy để xác định có sử dụng thông tin từ mỗi mô-đun chuyên gia hay không. Từ các đầu ra của các mạng cổng, luồng truyền cho ba nguồn trí tuệ: 'AUX', 'IMG', 'LANG' có thể được biểu diễn như sau:

[wAUX, wLANG, wSELF] ← Softmax(I(l)TWGatingI, dim=1),
Î(l) = wAUX ⊙ I(l)AUX + wLANG ⊙ I(l)LANG + wSELF ⊙ I(l)SELF

[wAUX, wIMG, wSELF] ← Softmax(L(l)TWGatingL, dim=1),
L̂(l) = wAUX ⊙ L(l)AUX + wIMG ⊙ L(l)IMG + wSELF ⊙ L(l)SELF, (5)

nơi ⊙ đại diện cho tích từng phần tử trong mỗi token. Các mạng cổng cho đặc trưng thị giác và ngôn ngữ được huấn luyện độc lập mà không chia sẻ tham số, đảm bảo rằng cả hai mạng cổng đều pha trộn ba trí tuệ với các trọng số khác nhau. Theo cách này, MoAI-Mixer và các mạng cổng tạo thuận lợi cho sự tương tác giữa ba nguồn trí tuệ.

4 Thí nghiệm

Chi tiết Triển khai. Để đảm bảo khả năng tái tạo thành công, chúng tôi nêu ra ba chi tiết kỹ thuật quan trọng của MoAI: (a) các mô hình CV bên ngoài, (b) MoAI-Compressor và MoAI-Mixer, (c) chi tiết huấn luyện và suy luận.

(a) Đối với phân đoạn toàn cảnh, chúng tôi áp dụng Mask2Former [14] (kích thước mô hình: 106M) với Swin-B/4 [67]. Để dự đoán bản đồ phân đoạn toàn cảnh, chúng tôi đặt ngưỡng để giữ các mặt nạ thể hiện được dự đoán là 0.5 và đặt ngưỡng mặt nạ để sử dụng các mặt nạ là 0.95. Đối với phát hiện đối tượng thế giới mở, chúng tôi sử dụng OWLv2 [70] (kích thước mô hình: 154M) với CLIP-B/16 [75]. Để đạt được phát hiện đối tượng thế giới mở, chúng tôi xử lý 1847 danh mục đối tượng kết hợp những danh mục trong ADE20K-847 [94,95] và ImageNet [20]. Chúng tôi đặt ngưỡng để giữ các dự đoán phát hiện đối tượng là 0.1 và đặt ngưỡng đối tượng để sử dụng chúng là 0.5. Đối với tạo đồ thị cảnh (SGG), chúng tôi sử dụng panoptic SGG [88] (kích thước mô hình: 44M) với ResNet-50 [30] để thực hiện tương tác linh hoạt với các đối tượng tiền cảnh và nền, nơi ngưỡng 0.8 để sử dụng các vị từ SGG được đặt. Đối với OCR, chúng tôi sử dụng PaddleOCRv2 [23] (kích thước mô hình: 18M), một trong những framework OCR mã nguồn mở hiệu quả, nơi chúng tôi đặt các ngôn ngữ có thể nhận dạng thành tiếng Trung & tiếng Anh và đặt cài đặt siêu tham số để có thể đọc các mô tả văn bản xoay. Kích thước kết hợp của các mô hình CV bên ngoài là khoảng 332M, đóng góp một ít vào tổng kích thước mô hình.

(b) Trong MoAI-Compressor, các token có thể học Ainput có chiều R4096×64 nơi 64 biểu thị số lượng token (độ dài) và 4096 đại diện cho chiều kênh d cho đầu vào MLM. Ngoài ra, MoAI-Compressor bao gồm 4 lớp Transformer encoder tiêu chuẩn [84]. Trong tự-attention, 4 số lượng đầu và 64 chiều đầu được đặt. Để xây dựng MoAI-Mixer, chúng tôi trang bị nó với các chỉ số lớp MLM cụ thể l = 7, 15, 23, 31. Đối với các mô-đun chuyên gia CA/SA, 64 chiều giảm, 4 số lượng đầu, và 4096/4 = 1024 chiều đầu được sử dụng.

(c) Đối với tất cả các bước huấn luyện, chúng tôi xử lý một bộ dữ liệu điều chỉnh hướng dẫn thị giác tiêu chuẩn: LLaVA-Instruct-665K [63] được lọc bởi [13]. Về bước huấn luyện đầu tiên, chúng tôi huấn luyện các token có thể học Ainput, các tham số của MoAI-Compressor, và sáu mô-đun chuyên gia của MoAI-Mixer trong một epoch sử dụng optimizer AdamW [69], được lập lịch bởi cosine annealing [68] từ tỷ lệ học 1e-4 đến 1e-6. Trong bước huấn luyện thứ hai, chúng tôi không chỉ học các tham số đã huấn luyện trong bước huấn luyện đầu tiên mà còn cả các mạng cổng, nơi tỷ lệ học được lập lịch từ 2e-5 đến 1e-6 trong một epoch. Để suy luận hiệu quả, chúng tôi lượng tử hóa MoAI ở 4-bit nơi lượng tử hóa kép và float 4-bit chuẩn hóa (nf4) [21] được sử dụng, và chúng tôi sử dụng tìm kiếm chùm tia tất định (n = 3) [27] để tạo văn bản.

Đánh giá Khả năng Nhận thức Thị giác. Đi sâu vào việc xác thực hiệu quả của MoAI, chúng tôi nhìn sâu hơn vào khả năng nhận thức thị giác liên quan đến hiểu cảnh thế giới thực trong nhiều tiêu chuẩn VL, như MME, SEED, MM-Bench, và MM-Vet. Hình 2 minh họa chi tiết hiệu suất zero-shot của MoAI và ba LLVMs mã nguồn mở hiện đại như InstructBLIP [19], Qwen-VL [4], LLaVA1.5 [63]. Đối với mỗi tiêu chuẩn VL, tồn tại các chiều cụ thể (tiểu tiêu chuẩn) liên quan đến hiểu cảnh thế giới thực mà MoAI nhằm mục đích chứng minh hiệu quả của nó. Tham khảo Phụ lục A để biết thêm chi tiết về những gì mỗi chiều cụ thể chỉ ra. Như có thể thấy từ Hình 2, MoAI vượt trội đáng kể so với các LLVMs khác, chứng minh hiệu quả của việc sử dụng thông tin thị giác phụ trợ từ các mô hình CV bên ngoài. Đáng chú ý rằng MoAI đặc biệt xuất sắc ở các chiều liên quan đến quan hệ và văn bản, nhấn mạnh tầm quan trọng của việc sử dụng thông tin thị giác phụ trợ mà chúng gặp khó khăn để hiểu đầy đủ. Tham khảo Phụ lục D để đánh giá định tính với minh chứng trên một vài mẫu. Hơn nữa, Bảng 1 thể hiện đánh giá toàn diện trên nhiều tiêu chuẩn VL nổi tiếng, và chứng minh hiệu suất đặc biệt của MoAI. Tính linh hoạt của MoAI xác nhận rằng nâng cao hiểu cảnh thế giới thực có thể tăng cường không chỉ nhận thức thị giác liên quan đến nó mà còn khả năng VL tổng thể trong Hình 1(b).

Nghiên cứu Loại bỏ. Để xác thực hiệu quả của các mô hình CV bên ngoài chúng tôi sử dụng, chúng tôi thực hiện đánh giá bằng cách loại bỏ chúng từng cái một. Bảng 2 cho thấy sự sụt giảm đáng kể của sự tồn tại đối tượng và nhận dạng khi không sử dụng phân đoạn toàn cảnh (PS) và phát hiện đối tượng thế giới mở (OWOD). Mặt khác, khi SGG không được sử dụng, các điểm liên quan đến quan hệ như Vị trí và Không gian giảm trong Bảng 2. Ngoài ra, các điểm OCR cũng giảm nếu OCR không được sử dụng. Do đó, chúng tôi có thể nói rằng mỗi mô hình CV bên ngoài đều quan trọng cho hiểu cảnh thế giới thực dựa trên các điểm nhận thức cho MME, SEED, MM-Bench, và MM-Vet. Ngoài ra, chúng tôi kiểm soát ba yếu tố của MoAI-Mixer và các mạng cổng trong Bảng 3: (a) hai bước huấn luyện, (b) chọn top-k trong các mô-đun chuyên gia, và (c) trọng số của các mạng cổng, để xác thực hiệu quả của chúng. Lưu ý rằng, chọn top-k dựa trên độ lớn trọng số giữa wAUX, wIMG, wLANG, và wSELF, vì trọng số cao hơn biểu thị tầm quan trọng lớn hơn của thông tin liên quan. Hơn nữa, Phụ lục B cho thấy các thí nghiệm bổ sung cho nghiên cứu loại bỏ.

Thảo luận. Từ kết quả, chúng tôi có thể có được một cái nhìn sâu sắc rằng ưu tiên hiểu cảnh thế giới thực quan trọng hơn việc dựa vào việc tuyển chọn thêm các bộ dữ liệu hướng dẫn thị giác hoặc mở rộng kích thước mô hình. Như được minh họa trong Hình 7(a), MoAI-7B vượt trội về hiệu suất zero-shot, mặc dù tương đối nhỏ so với các mô hình mã nguồn mở và mã nguồn đóng lớn hơn đáng kể. Đáng chú ý, Hình 7(b) cũng cho thấy rằng MoAI hoạt động tốt ngay cả trên các bộ dữ liệu zero-shot về ảo giác: POPE [58] và HallusionBench [62]. Điều này gợi ý rằng việc nhận dạng chính xác các đối tượng và mối quan hệ của chúng có thể giúp ngăn chặn LLVMs mắc lỗi. Nhìn về phía trước, vì MoAI được thiết kế riêng cho hiểu cảnh thế giới thực, chúng tôi dự định kết hợp thêm các mô hình CV bên ngoài để cung cấp cho LLVMs các khả năng đa dạng cho hiểu thị giác cấp thấp, kiến thức thông thường, và nhận thức về các khái niệm không phải đối tượng ngoài mô tả văn bản, như biểu đồ, sơ đồ, dấu hiệu, và ký hiệu, cũng như giải quyết các bài toán toán học tiên tiến. Hơn nữa, các mô hình CV mạnh mẽ [41,47,50,53], không thiên vị [43,51,61], và có thể giải thích [9,39,40] có thể được áp dụng để đạt được đầu ra chính xác và không thiên vị cho các tác vụ ngôn ngữ thị giác. Hơn nữa, Phụ lục C mô tả thảo luận thêm.

5 Kết luận

Để đạt được hiểu cảnh thế giới thực, chúng tôi tận dụng các khả năng nhận thức cơ bản có gốc rễ trong khoa học nhận thức và học máy. Điều này bao gồm việc kết hợp thông tin thị giác phụ trợ từ các mô hình CV bên ngoài giàu lịch sử, mà chúng tôi tích hợp liền mạch với các đặc trưng thị giác và ngôn ngữ trong MLM sử dụng các mô-đun chuyên gia và mạng cổng. Như một kết quả của những tiến bộ này, MoAI chứng minh khả năng nhận thức thị giác được cải thiện, dẫn đến những nâng cao đáng kể trong hiệu suất ngôn ngữ thị giác zero-shot. Điều này nhấn mạnh tiềm năng của MoAI trong việc tiến bộ mô hình hóa LLVM bằng cách tận dụng hiệu quả thông tin thị giác phụ trợ đa dạng và tích hợp nhiều hình thức trí tuệ.

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi hai quỹ: tài trợ IITP được tài trợ bởi chính phủ Hàn Quốc (MSIT) (RS-2022-II220984) và tài trợ Trung tâm Nghiên cứu Ứng dụng Trí tuệ Nhân tạo (CARAI) được tài trợ bởi DAPA và ADD (UD230017TD). Ngoài ra, nó được hỗ trợ bởi Trung tâm Siêu máy tính Quốc gia KISTI với tài nguyên siêu máy tính bao gồm hỗ trợ kỹ thuật (KSC-2024-CRE-0160).

Tài liệu tham khảo

[1-99: Danh sách các tài liệu tham khảo được giữ nguyên như trong bản gốc]

A Chi tiết cho Các Chiều Cụ thể trong Hình 2

Phần này giải thích chi tiết về các chiều (tiểu tiêu chuẩn hoặc tiểu tác vụ) liên quan đến hiểu cảnh thế giới thực trong các tiêu chuẩn sau: MME, SEED, MM-Bench, và MM-Vet.

A.1 MME

– Sự tồn tại liên quan đến các câu hỏi về sự hiện diện của một đối tượng duy nhất trong một hình ảnh được chỉ định.
– Đếm biểu thị quá trình định lượng các thể hiện của đối tượng được chỉ định được mô tả trong một hình ảnh.
– Vị trí mô tả khả năng của một mô hình để phân biệt sự sắp xếp không gian giữa hai đối tượng trong một hình ảnh nhất định.
– Cảnh tập trung vào việc nhận dạng một địa điểm được hiển thị trong hình ảnh, bao gồm các vị trí trong nhà và ngoài trời như phòng trưng bày, phòng thí nghiệm, bờ hồ, điểm nổi bật, v.v.
– OCR phục vụ để đo lường khả năng của một mô hình nhận dạng văn bản trong hình ảnh.
– Dịch văn bản yêu cầu một mô hình, hỗ trợ cả tiếng Anh và tiếng Trung, dịch chữ Trung Quốc trong hình ảnh sang tiếng Anh tương ứng.

A.2 SEED

– Cảnh (hiểu cảnh) tập trung vào thông tin tổng thể được truyền đạt trong hình ảnh, nơi các câu hỏi có thể được trả lời thông qua sự nắm bắt toàn diện về nội dung của hình ảnh.
– Danh tính (danh tính thể hiện) bao gồm khả năng nhận dạng đối tượng của một mô hình, bao gồm việc xác định sự hiện diện hoặc phân loại của một thể hiện.
– Thuộc tính (thuộc tính thể hiện) liên quan đến các đặc điểm phân biệt của một thể hiện, như màu sắc, hình dạng, hoặc thành phần vật liệu của nó, phục vụ để đánh giá sự hiểu biết của mô hình về diện mạo thị giác của một đối tượng.
– Vị trí (vị trí thể hiện) liên quan đến tọa độ không gian chính xác của một thể hiện cụ thể trong hình ảnh, đòi hỏi việc định vị chính xác đối tượng được tham chiếu bởi mô hình.
– Đếm (đếm thể hiện) liên quan đến khả năng của một mô hình đếm số lần xuất hiện của một đối tượng được chỉ định trong hình ảnh, đòi hỏi sự hiểu biết toàn diện và liệt kê các thể hiện của đối tượng được chỉ định.
– Mối quan hệ (mối quan hệ không gian) thúc đẩy mô hình thiết lập kết nối không gian giữa hai đối tượng được chỉ định trong hình ảnh, định vị chúng và nhận dạng sự sắp xếp không gian tương đối của chúng.
– Tương tác (tương tác thể hiện) yêu cầu mô hình nhận dạng trạng thái quan hệ hoặc động lực tương tác giữa hai thực thể con người hoặc đối tượng được mô tả trong hình ảnh.
– Nhận dạng văn bản (hiểu văn bản) bao gồm các câu hỏi về các thành phần văn bản có trong hình ảnh, thúc đẩy mô hình hiểu và diễn giải các yếu tố văn bản tương ứng.

A.3 MM-Bench

– Thuộc tính (nhận dạng thuộc tính) bao gồm khả năng của một mô hình nhận dạng các đặc điểm khác nhau của một hình ảnh như kết cấu, hình dạng, diện mạo, cảm xúc, danh mục, người nổi tiếng, địa điểm nổi tiếng, đối tượng, và ký tự quang học trong một hình ảnh.
– Định vị (định vị đối tượng) bao gồm các câu hỏi về vị trí của một đối tượng, tọa độ tuyệt đối, số lượng, và định hướng trong hình ảnh.
– OCR thể hiện khả năng của một mô hình nhận dạng văn bản, công thức, và bảng biểu trong một hình ảnh.
– Mối quan hệ (mối quan hệ không gian) đánh giá khả năng của một mô hình hiểu vị trí tương đối giữa các đối tượng được mô tả trong hình ảnh.

A.4 MM-Vet

– Nhận dạng đánh giá năng lực nhận dạng thị giác tổng thể của một mô hình, bao gồm việc nhận dạng cảnh, đối tượng, thuộc tính đối tượng, đếm, và các tác vụ nhận dạng thị giác tiên tiến khác trong thị giác máy tính.
– Không gian (nhận thức không gian) bao gồm một loạt các khả năng liên quan đến hiểu không gian, bao gồm hiểu mối quan hệ không gian giữa các đối tượng và vùng văn bản trong một cảnh.
– OCR đánh giá khả năng của một mô hình hiểu và lý luận về văn bản cảnh, nơi mô hình được đánh giá về khả năng đọc văn bản trong hình ảnh và sử dụng thông tin này để giải quyết các tác vụ khác nhau.

B Thí nghiệm Thêm

Thông tin Phụ trợ trên Các Mô hình Cơ sở Khác. Trong bảng sau, chúng tôi đã thí nghiệm nghiên cứu loại bỏ đơn giản để xác định hiệu quả của thông tin phụ trợ, và sau đó chúng tôi quan sát các kết quả tăng tương tự.

[Bảng dữ liệu được giữ nguyên]

Phức tạp Quá mức. Chúng tôi đã xác thực thêm nghiên cứu loại bỏ kỹ lưỡng hơn trong bảng sau. Từ những kết quả này, chúng tôi có thể nói rằng việc thêm khung giới hạn hoặc bản đồ phân đoạn trực tiếp ảnh hưởng đến các tiểu tiêu chuẩn về vị trí, định vị, mối quan hệ không gian so với sự tồn tại và nhận dạng. Chúng có liên quan lớn đến hiểu tinh tế đối tượng. Từ những quan sát này, chúng tôi muốn khẳng định rằng chúng tôi đã tỉ mỉ lựa chọn các mô hình thị giác máy tính bên ngoài và lời nhắc chuyển đổi thành ngôn ngữ đầu ra của chúng, vì vậy tất cả những gì chúng tôi sử dụng đều cần thiết để nâng cao khả năng nhận thức một cách cân bằng. Tuy nhiên, phân đoạn thế giới mở có thể là một giải pháp để tích hợp phân đoạn và phát hiện thế giới mở, nhưng, theo hiểu biết tốt nhất của chúng tôi, nó có cấu trúc phức tạp với kích thước mô hình lớn hơn. Đây là lý do tại sao chúng tôi xem xét hai cái độc lập trong phiên bản hiện tại.

[Bảng dữ liệu được giữ nguyên]

Độ trễ Suy luận. Chúng tôi đo số lượng token được tạo ra mỗi giây trong Qwen-VL (16 tok/s), LLaVA1.5 (22 tok/s), và MoAI (20 tok/s) dưới flash attention trong môi trường tài nguyên bằng nhau: Intel(R) Xeon(R) Gold 6230, RAM 512GB, và NVIDIA RTX A6000. Mặc dù sử dụng các mô hình bên ngoài, kích thước của chúng tương đối nhỏ và tốc độ tạo có liên quan cao đến kích thước mô hình ngôn ngữ đa phương thức, vì vậy chúng không ảnh hưởng nhiều đến tốc độ suy luận. Điều này tương tự như quá trình giải mã SAM [44] (các đặc trưng được mã hóa được lưu).

Số lượng Lớp. Dưới đây, chúng tôi đã thực hiện nghiên cứu loại bỏ đơn giản để cho thấy việc xem xét tổng cộng 1847 danh mục là đủ bằng cách chọn ngẫu nhiên các danh mục với 20 lần lặp lại.

[Bảng dữ liệu được giữ nguyên]

C Thảo luận Thêm

Thiên vị Bộ dữ liệu Tiềm năng. Chúng tôi đã đánh giá MoAI trên MMStar [12] để xử lý các thiên vị bộ dữ liệu tiềm năng: một số câu hỏi có thể không cần thông tin thị giác để có câu trả lời chính xác. Từ kết quả này, chúng tôi có thể nói rằng MoAI vượt qua các thiên vị tốt so với các mô hình cơ sở khác.

[Bảng dữ liệu được giữ nguyên]

Hiệu suất Hạn chế cho Các Mô hình Thị giác Máy tính. Như chúng ta đều biết, không có mô hình hoàn hảo nào tồn tại trên thế giới cung cấp độ chính xác dự đoán 100%. Do đó, các hạn chế vốn có như dự đoán sai và thiên vị từ bộ dữ liệu huấn luyện là không thể tránh khỏi. Tuy nhiên, bất chấp những thiếu sót của chúng, các mô hình bên ngoài đã chứng minh tiềm năng cao trong việc nâng cao khả năng nhận thức của LLVMs, mà trước đây gặp khó khăn với những vấn đề này.

Công việc Tương lai. Bất chấp hiệu suất tiên tiến cho các khả năng nhận thức cơ bản, MoAI vẫn cần hiểu thị giác cấp thấp, kiến thức thông thường, biểu đồ, sơ đồ, dấu hiệu, và ký hiệu, bài toán toán học tiên tiến, tất cả đều vượt ra ngoài nhận thức [49] với các mô hình thị giác máy tính. Do đó, chúng tôi nên mở rộng trí tuệ theo nghĩa đen có nguồn gốc từ các mô hình thị giác máy tính bổ sung để bao gồm thông tin đa diện bao gồm hiểu hình ảnh cơ bản, kiến thức thông thường thế giới thực, và các khái niệm không phải đối tượng. Sự mở rộng này nên bao gồm biểu đồ, sơ đồ, ký hiệu, dấu hiệu, bài toán toán học, và các quy trình từng bước để giải quyết các câu hỏi phức tạp. Bên cạnh đó, chúng tôi đang cố gắng với việc xây dựng các kiến trúc tiên tiến [48] để nhúng kiến thức ngôn ngữ thị giác phong phú hơn, kích thích hiệu ứng của việc pha trộn nhiều trí tuệ.

D Minh chứng MoAI cho Đánh giá Định tính

[Các ví dụ minh chứng với hình ảnh và câu trả lời được giữ nguyên như trong bản gốc]
