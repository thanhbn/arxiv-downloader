# 2312.03700.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.03700.pdf
# Kích thước tệp: 3785896 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OneLLM: Một Khung Thống Nhất để Căn Chỉnh Tất Cả Các Phương Thức với Ngôn Ngữ
Jiaming Han1,2, Kaixiong Gong1,2, Yiyuan Zhang1,2, Jiaqi Wang2, Kaipeng Zhang2
Dahua Lin1,2, Yu Qiao2, Peng Gao2, Xiangyu Yue1†
1MMLab, Đại học Trung văn Hồng Kông
2Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải

Tóm tắt
Các mô hình ngôn ngữ lớn đa phương thức (MLLM) đã thu hút sự chú ý đáng kể do khả năng hiểu đa phương thức mạnh mẽ của chúng. Tuy nhiên, các nghiên cứu hiện tại phụ thuộc rất nhiều vào các bộ mã hóa đặc thù cho từng phương thức, thường khác nhau về kiến trúc và bị giới hạn ở các phương thức phổ biến. Trong bài báo này, chúng tôi trình bày OneLLM, một MLLM căn chỉnh tám phương thức với ngôn ngữ bằng một khung thống nhất. Chúng tôi đạt được điều này thông qua một bộ mã hóa đa phương thức thống nhất và một đường ống căn chỉnh đa phương thức tích lũy. Cụ thể, đầu tiên chúng tôi huấn luyện một mô-đun chiếu hình ảnh để kết nối bộ mã hóa thị giác với LLM. Sau đó, chúng tôi xây dựng một mô-đun chiếu phổ quát (UPM) bằng cách kết hợp nhiều mô-đun chiếu hình ảnh và định tuyến động. Cuối cùng, chúng tôi từng bước căn chỉnh thêm các phương thức với LLM thông qua UPM. Để tận dụng đầy đủ tiềm năng của OneLLM trong việc tuân theo hướng dẫn, chúng tôi cũng đã tuyển chọn một tập dữ liệu hướng dẫn đa phương thức toàn diện, bao gồm 2M mục từ hình ảnh, âm thanh, video, đám mây điểm, bản đồ độ sâu/bình thường, IMU và hoạt động não fMRI. OneLLM được đánh giá trên 25 điểm chuẩn đa dạng, bao gồm các nhiệm vụ như mô tả đa phương thức, trả lời câu hỏi và suy luận, nơi nó mang lại hiệu suất xuất sắc. Mã, dữ liệu, mô hình và demo trực tuyến có sẵn tại https://github.com/csuhan/OneLLM.

1. Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) đang ngày càng phổ biến trong cộng đồng nghiên cứu và ngành công nghiệp do khả năng hiểu ngôn ngữ và suy luận mạnh mẽ của chúng. Đáng chú ý, các LLM như GPT4 [62] đã đạt được hiệu suất gần như ngang bằng với con người trong các kỳ thi học thuật khác nhau. Tiến bộ trong LLM cũng đã truyền cảm hứng cho các nhà nghiên cứu sử dụng LLM như một giao diện cho các nhiệm vụ đa phương thức, chẳng hạn như học tập thị giác-ngôn ngữ [4, 44], nhận dạng âm thanh và giọng nói [25, 99], hiểu video [11, 45, 100], v.v.

†Tác giả liên hệ

[Hình 1 minh họa so sánh giữa các MLLM khác nhau]

Trong số các nhiệm vụ này, học tập thị giác-ngôn ngữ là lĩnh vực năng động nhất, với hơn 50 LLM thị giác được đề xuất chỉ trong nửa năm gần đây [20]. Thông thường, một LLM thị giác bao gồm một bộ mã hóa thị giác, một LLM, và một mô-đun chiếu kết nối hai thành phần. LLM thị giác đầu tiên được huấn luyện trên dữ liệu hình ảnh-văn bản ghép đôi khổng lồ [70] để căn chỉnh thị giác-ngôn ngữ và sau đó được tinh chỉnh trên các tập dữ liệu hướng dẫn thị giác, cho phép nó hoàn thành các hướng dẫn khác nhau liên kết với đầu vào thị giác. Ngoài thị giác, những nỗ lực đáng kể đã được đầu tư vào việc phát triển các LLM đặc thù cho phương thức khác, chẳng hạn như âm thanh [25], video [45], và đám mây điểm [28]. Các mô hình này thường phản ánh khung kiến trúc và phương pháp huấn luyện của LLM thị giác, và dựa vào nền tảng vững chắc của các bộ mã hóa đặc thù phương thức được huấn luyện trước và các tập dữ liệu tinh chỉnh hướng dẫn được tuyển chọn kỹ lưỡng để đạt hiệu quả.

Cũng có một số nỗ lực tích hợp nhiều phương thức vào một MLLM [10, 31, 59, 104]. Như một phần mở rộng của LLM thị giác, hầu hết các nghiên cứu trước đây căn chỉnh từng phương thức với LLM bằng cách sử dụng các bộ mã hóa và mô-đun chiếu đặc thù cho phương thức (giữa Hình 1). Ví dụ, X-LLM [10] và ChatBridge [104] kết nối các bộ mã hóa hình ảnh, video và âm thanh được huấn luyện trước với LLM bằng cách sử dụng các mô hình Q-Former [44] hoặc Perceiver [35] riêng biệt. Tuy nhiên, các bộ mã hóa đặc thù phương thức này thường khác nhau về kiến trúc và cần nỗ lực đáng kể để thống nhất chúng thành một khung duy nhất. Hơn nữa, các bộ mã hóa được huấn luyện trước mang lại hiệu suất đáng tin cậy thường bị giới hạn ở các phương thức được sử dụng rộng rãi như hình ảnh, âm thanh và video. Hạn chế này đặt ra ràng buộc về khả năng của MLLM trong việc mở rộng sang nhiều phương thức hơn. Do đó, một thách thức quan trọng đối với MLLM là làm thế nào để xây dựng một bộ mã hóa thống nhất và có thể mở rộng có khả năng xử lý một loạt rộng các phương thức.

Chúng tôi lấy cảm hứng từ các nghiên cứu gần đây về việc chuyển giao các transformer được huấn luyện trước sang các phương thức downstream [51, 57, 88, 103]. Lu et al. [51] đã chứng minh rằng một transformer đông lạnh được huấn luyện trước về ngôn ngữ có thể đạt được hiệu suất mạnh mẽ trên các phương thức downstream như phân loại hình ảnh. Meta-Transformer [103] đã chứng minh rằng một bộ mã hóa thị giác đông lạnh có thể đạt được kết quả cạnh tranh trên 12 phương thức dữ liệu khác nhau. Những hiểu biết từ các nghiên cứu được đề cập ở trên gợi ý rằng các bộ mã hóa được huấn luyện trước cho từng phương thức có thể không cần thiết. Thay vào đó, một transformer được huấn luyện trước tốt có thể đóng vai trò như một bộ mã hóa đa phương thức phổ quát.

Trong bài báo này, chúng tôi trình bày OneLLM, một MLLM căn chỉnh tám phương thức với ngôn ngữ bằng một khung thống nhất. Như thể hiện trong Hình 1, OneLLM bao gồm các tokenizer phương thức nhẹ, một bộ mã hóa phổ quát, một mô-đun chiếu phổ quát (UPM), và một LLM. Trái với các nghiên cứu trước, bộ mã hóa và mô-đun chiếu trong OneLLM được chia sẻ trên tất cả các phương thức. Các tokenizer đặc thù phương thức, mỗi cái chỉ bao gồm một lớp tích chập, chuyển đổi tín hiệu đầu vào thành một chuỗi các token. Ngoài ra, chúng tôi thêm các token phương thức có thể học để cho phép chuyển đổi phương thức và chuyển đổi các token đầu vào có độ dài đa dạng thành các token có độ dài cố định.

Huấn luyện một mô hình có độ phức tạp này từ đầu đặt ra những thách thức đáng kể. Chúng tôi bắt đầu từ một LLM thị giác và căn chỉnh các phương thức khác với LLM theo cách tích lũy. Cụ thể, (i) chúng tôi xây dựng một LLM thị giác với CLIP-ViT [67] được huấn luyện trước làm bộ mã hóa hình ảnh, kèm theo một số lớp transformer làm mô-đun chiếu hình ảnh, và LLaMA2 [78] làm LLM. Sau khi huấn luyện trước trên dữ liệu hình ảnh-văn bản khổng lồ, mô-đun chiếu học cách ánh xạ các biểu diễn thị giác vào không gian nhúng của LLM. (ii) Để căn chỉnh với nhiều phương thức hơn, chúng tôi cần một bộ mã hóa phổ quát và mô-đun chiếu. Như đã thảo luận trước đây, CLIP-ViT được huấn luyện trước có thể đóng vai trò như một bộ mã hóa phổ quát. Đối với UPM, chúng tôi đề xuất kết hợp nhiều chuyên gia chiếu hình ảnh như một giao diện phổ quát X-to-language. Để tăng khả năng của mô hình, chúng tôi cũng thiết kế một bộ định tuyến động để điều khiển trọng số của mỗi chuyên gia cho các đầu vào đã cho, biến UPM thành hỗn hợp mềm của các chuyên gia [66]. Cuối cùng, chúng tôi từng bước căn chỉnh thêm các phương thức với LLM dựa trên quy mô dữ liệu của chúng.

Chúng tôi cũng tuyển chọn một tập dữ liệu hướng dẫn đa phương thức quy mô lớn, bao gồm các nhiệm vụ mô tả, trả lời câu hỏi và suy luận trên tám phương thức: hình ảnh, âm thanh, video, đám mây điểm, bản đồ độ sâu/bình thường, Đơn vị Đo lường Quán tính (IMU), và Hình ảnh Cộng hưởng Từ chức năng (fMRI). Bằng cách tinh chỉnh trên tập dữ liệu này, OneLLM có khả năng hiểu, suy luận và tuân theo hướng dẫn đa phương thức mạnh mẽ. Chúng tôi đánh giá OneLLM trên các điểm chuẩn mô tả, trả lời câu hỏi và suy luận đa phương thức, nơi nó mang lại hiệu suất vượt trội so với các mô hình chuyên biệt và MLLM trước đây. Tóm lại, chúng tôi tóm tắt những đóng góp của mình như sau:

• Chúng tôi đề xuất một khung thống nhất để căn chỉnh các đầu vào đa phương thức với ngôn ngữ. Khác với các nghiên cứu hiện tại với các bộ mã hóa đặc thù phương thức, chúng tôi chỉ ra rằng một bộ mã hóa đa phương thức thống nhất, tận dụng mô hình thị giác-ngôn ngữ được huấn luyện trước và hỗn hợp các chuyên gia chiếu, có thể đóng vai trò như một thành phần chung và có thể mở rộng cho MLLM.

• Theo hiểu biết của chúng tôi, OneLLM là MLLM đầu tiên tích hợp tám phương thức riêng biệt trong một mô hình duy nhất. Với khung thống nhất và đường ống căn chỉnh đa phương thức tích lũy, OneLLM có thể dễ dàng được mở rộng để kết hợp thêm các phương thức dữ liệu.

• Chúng tôi tuyển chọn một tập dữ liệu hướng dẫn đa phương thức quy mô lớn. OneLLM được tinh chỉnh trên tập dữ liệu này đạt được hiệu suất vượt trội trong các nhiệm vụ đa phương thức, vượt trội hơn cả các mô hình chuyên gia và MLLM hiện tại.

2. Nghiên cứu Liên quan
Mô hình Thị giác-Ngôn ngữ Lớn. Các Mô hình Ngôn ngữ Lớn (LLM) đã thu hút nhiều sự chú ý gần đây. Do đó, việc mở rộng LLM sang lĩnh vực thị giác là một lĩnh vực nghiên cứu mới nổi và phát triển nhanh chóng. Flamingo [4] là người tiên phong trong việc tiêm các đặc trưng thị giác đông lạnh vào LLM với các lớp chú ý chéo, đạt được hiệu suất vượt trội trên một loạt rộng các nhiệm vụ thị giác-ngôn ngữ. BLIP2 [44] sử dụng Q-Former để tổng hợp các đặc trưng thị giác thành một số token được căn chỉnh với LLM. Gần đây, với sự phổ biến của LLM tuân theo hướng dẫn, LLM thị giác đã trải qua một sự bùng nổ mới. LLaMA-Adapter [21, 102] kết nối CLIP [67] và LLaMA [78] được huấn luyện trước với các phương pháp tinh chỉnh hiệu quả tham số, có thể giải quyết các nhiệm vụ trả lời câu hỏi thị giác đóng và mô tả hình ảnh. Các nghiên cứu tiếp theo [21, 48, 95, 105] đề xuất huấn luyện mô hình như vậy trên dữ liệu hình ảnh-văn bản quy mô lớn, cho phép nó hoàn thành các hướng dẫn khác nhau về hình ảnh. Trong số đó, LLaVA [48] sử dụng một lớp tuyến tính để chiếu trực tiếp các token thị giác vào LLM, trong khi MiniGPT-4 [105] và một số nghiên cứu khác [21, 95] lấy mẫu lại các token thị giác thành các token có độ dài cố định, giảm chi phí tính toán của LLM. Nghiên cứu của chúng tôi cũng thuộc nhánh sau. Chúng tôi đặt trước các token có thể học cho mỗi phương thức (tức là, token phương thức), sau đó được sử dụng để tổng hợp thông tin đầu vào và tạo ra các token có độ dài cố định cho tất cả các phương thức.

Mô hình Ngôn ngữ Lớn Đa phương thức. Ngoài LLM thị giác, các nghiên cứu gần đây đã đề xuất mở rộng LLM sang các phương thức khác, chẳng hạn như âm thanh [25, 99], video [11, 45, 100] và đám mây điểm [28, 92]. Các nghiên cứu này làm cho việc thống nhất nhiều phương thức vào một LLM trở nên khả thi. X-LLM [10] sử dụng Q-Former [44] và adapter đặc thù phương thức để kết nối các bộ mã hóa hình ảnh, âm thanh và video được huấn luyện trước với LLM. ChatBridge [104] và AnyMAL [59] tuân theo kiến trúc tương tự với X-LLM nhưng sử dụng Perceiver [35] và các lớp tuyến tính tương ứng để căn chỉnh các bộ mã hóa phương thức với LLM. Trong khi đó, PandaGPT [77] và ImageBind-LLM [31] sử dụng ImageBind [23] làm bộ mã hóa phương thức và do đó tự nhiên hỗ trợ các đầu vào đa phương thức. Tuy nhiên, các MLLM hiện tại bị giới hạn trong việc hỗ trợ các phương thức phổ biến như hình ảnh, âm thanh và video. Vẫn chưa rõ làm thế nào để mở rộng MLLM sang nhiều phương thức hơn với một khung thống nhất. Trong nghiên cứu này, chúng tôi đề xuất một bộ mã hóa đa phương thức thống nhất để căn chỉnh tất cả các phương thức với ngôn ngữ. Chúng tôi chỉ ra rằng một bộ mã hóa phổ quát và mô-đun chiếu có thể ánh xạ hiệu quả các đầu vào đa phương thức vào LLM. Theo hiểu biết của chúng tôi, OneLLM là MLLM đầu tiên có khả năng hỗ trợ tám phương thức riêng biệt.

Căn chỉnh Đa phương thức-Văn bản. Việc căn chỉnh nhiều phương thức vào một không gian nhúng chung rất quan trọng cho các nhiệm vụ đa phương thức, có thể được chia thành hai hướng nghiên cứu: căn chỉnh phân biệt và căn chỉnh tạo sinh. Nghiên cứu đại diện nhất của căn chỉnh phân biệt là CLIP [67], sử dụng học tập đối lập để căn chỉnh hình ảnh và văn bản. Các nghiên cứu tiếp theo mở rộng CLIP sang âm thanh-văn bản [30, 85], video-văn bản [53, 90], điểm-văn bản [101], v.v. Bên cạnh đó, ImageBind [23] đề xuất liên kết các phương thức khác nhau với hình ảnh bằng học tập đối lập. Mặt khác, căn chỉnh tạo sinh đã thu hút nhiều sự chú ý trong kỷ nguyên LLM. GIT [82] căn chỉnh hình ảnh và văn bản bằng một transformer tạo sinh từ hình ảnh sang văn bản. BLIP2 [44] đề xuất huấn luyện trước tạo sinh để kết nối bộ mã hóa thị giác đông lạnh và LLM. VALOR [12] và VAST [13] mở rộng mô hình huấn luyện của BLIP2 sang nhiều phương thức hơn như âm thanh và video. Nghiên cứu của chúng tôi cũng thuộc về căn chỉnh tạo sinh. Trái với các nghiên cứu trước, chúng tôi căn chỉnh trực tiếp các đầu vào đa phương thức với LLM, do đó loại bỏ giai đoạn huấn luyện các bộ mã hóa phương thức.

3. Phương pháp
Trong phần này, chúng tôi sẽ trước tiên giới thiệu kiến trúc của OneLLM (Mục 3.1) và sau đó trình bày hai giai đoạn huấn luyện của chúng tôi: căn chỉnh đa phương thức tích lũy (Mục 3.2) và tinh chỉnh hướng dẫn đa phương thức thống nhất (Mục 3.3).

--- TRANG 4 ---
3.1. Kiến trúc Mô hình
Hình 2 mô tả bốn thành phần chính của OneLLM: các tokenizer đặc thù phương thức, một bộ mã hóa phổ quát, một mô-đun chiếu phổ quát (UPM) và một LLM. Các mô tả chi tiết được trình bày trong các phần sau.

Tokenizer Phương thức Nhẹ. Tokenizer phương thức dùng để chuyển đổi tín hiệu đầu vào thành một chuỗi các token, do đó một bộ mã hóa dựa trên transformer có thể xử lý các token này. Chúng tôi ký hiệu các token đầu vào là x∈RL×D, trong đó L là độ dài chuỗi và D là chiều của token. Xem xét các biến thể vốn có của các phương thức dữ liệu khác nhau, chúng tôi thiết kế một tokenizer riêng cho mỗi phương thức. Đối với các đầu vào thị giác có thông tin vị trí 2D như hình ảnh và video, chúng tôi trực tiếp sử dụng một lớp tích chập 2D duy nhất làm tokenizer. Đối với các phương thức khác, chúng tôi chuyển đổi đầu vào thành chuỗi 2D hoặc 1D, sau đó được token hóa bằng lớp tích chập 2D/1D. Ví dụ, chúng tôi chuyển đổi tín hiệu âm thanh thành spectrogram 2D và lấy mẫu một tập con đám mây điểm với tiên nghiệm hình học 2D. Do hạn chế về không gian, vui lòng tham khảo Mục C.1 của phụ lục để biết thêm chi tiết.

Bộ mã hóa Phổ quát. Như đã thảo luận trong Mục 1, các transformer đông lạnh được huấn luyện trước thể hiện khả năng chuyển giao phương thức mạnh mẽ [51, 103]. Do đó, chúng tôi tận dụng các mô hình thị giác-ngôn ngữ được huấn luyện trước làm bộ mã hóa phổ quát cho tất cả các phương thức. Các mô hình thị giác-ngôn ngữ, khi được huấn luyện trên dữ liệu hình ảnh-văn bản mở rộng, thường học được sự căn chỉnh mạnh mẽ giữa thị giác và ngôn ngữ, do đó chúng có thể dễ dàng được chuyển giao sang các phương thức khác. Trong OneLLM, chúng tôi sử dụng CLIP-ViT [67] làm động cơ tính toán phổ quát. Theo các nghiên cứu trước [51, 103], chúng tôi giữ các tham số của CLIP-ViT đông lạnh trong quá trình huấn luyện. Lưu ý rằng đối với tín hiệu video, chúng tôi sẽ đưa tất cả các khung hình video vào bộ mã hóa song song và thực hiện trung bình theo token giữa các khung hình để tăng tốc độ huấn luyện. Các chiến lược khác, chẳng hạn như nối token, có thể tiếp tục nâng cao khả năng hiểu video của mô hình.

Mô-đun Chiếu Phổ quát. Trái với các nghiên cứu hiện tại với chiếu đặc thù phương thức, chúng tôi đề xuất Mô-đun Chiếu Phổ quát (UPM) để chiếu bất kỳ phương thức nào vào không gian nhúng của LLM. Như thể hiện trong Hình 2, UPM bao gồm K chuyên gia chiếu {Pk}, trong đó mỗi chuyên gia là một chồng các lớp transformer được huấn luyện trước trên dữ liệu hình ảnh-văn bản (sẽ thảo luận trong Mục 3.2). Mặc dù một chuyên gia cũng có thể thực hiện chiếu từ bất kỳ phương thức nào sang LLM, các phát hiện thực nghiệm của chúng tôi cho thấy nhiều chuyên gia hiệu quả và có thể mở rộng hơn. Khi mở rộng sang nhiều phương thức hơn, chúng tôi chỉ cần thêm một số chuyên gia song song.

Để tích hợp nhiều chuyên gia vào một mô-đun, chúng tôi đề xuất một bộ định tuyến phương thức động R để kiểm soát sự đóng góp của mỗi chuyên gia và tăng khả năng của mô hình. Bộ định tuyến R được cấu trúc như một Perception Đa lớp đơn giản nhận các token đầu vào và tính toán trọng số định tuyến cho mỗi chuyên gia, tức là, một bộ định tuyến mềm [66]. Chúng tôi cũng sẽ thảo luận về các loại bộ định tuyến khác trong Mục 4.3, chẳng hạn như bộ định tuyến không đổi và bộ định tuyến thưa thớt. Bên cạnh đó, chúng tôi thêm các token phương thức có thể học {qm}m∈M để chuyển đổi giữa các phương thức, trong đó M là tập hợp các phương thức và qm∈RN×D chứa N token có chiều D. Trong một lần chuyển tiếp cho phương thức m, chúng tôi đưa sự nối của token đầu vào xm∈RL×D và token phương thức qm vào UPM:

[q̄m, x̄m] = UPM([qm, xm]) = ∑(k=1 to K) wm · Pk([qm, xm]), (1)

wm = σ ∘ Rm([qm, xm]), (2)

trong đó wm∈RN×K là trọng số định tuyến và hàm SoftMax σ đảm bảo ∑(k=1 to K) wm,k = 1. Đối với bất kỳ phương thức m nào, chúng tôi chỉ trích xuất các token phương thức được chiếu q̄m như một tóm tắt của tín hiệu đầu vào, chuyển đổi xm từ độ dài khác nhau thành token thống nhất, có độ dài cố định.

LLM. Chúng tôi sử dụng LLaMA2 [79] mã nguồn mở làm LLM trong khung của chúng tôi. Đầu vào của LLM bao gồm các token phương thức được chiếu q̄m và lời nhắc văn bản sau khi nhúng từ. Lưu ý rằng chúng tôi luôn đặt token phương thức ở đầu chuỗi đầu vào cho đơn giản. Sau đó LLM được yêu cầu tạo ra phản hồi thích hợp dựa trên token phương thức và lời nhắc văn bản.

3.2. Căn chỉnh Đa phương thức Tích lũy
Căn chỉnh hình ảnh-văn bản đã được nghiên cứu kỹ lưỡng trong các nghiên cứu trước [21, 49, 105]. Do đó, một cách tiếp cận ngây thơ cho căn chỉnh đa phương thức là huấn luyện chung mô hình trên dữ liệu đa phương thức-văn bản. Tuy nhiên, việc huấn luyện mô hình trực tiếp trên dữ liệu đa phương thức có thể dẫn đến biểu diễn thiên lệch giữa các phương thức do sự mất cân bằng về quy mô dữ liệu. Ở đây chúng tôi đề xuất huấn luyện một mô hình hình ảnh-sang-văn bản làm khởi tạo và từng bước gắn kết các phương thức khác vào LLM.

Căn chỉnh Hình ảnh-Văn bản. Chúng tôi bắt đầu với một khung LLM thị giác cơ bản, bao gồm một tokenizer hình ảnh, một CLIP-ViT được huấn luyện trước, một mô-đun chiếu hình ảnh PI và một LLM. Xem xét rằng dữ liệu hình ảnh-văn bản tương đối phong phú so với các phương thức khác, chúng tôi trước tiên huấn luyện mô hình trên dữ liệu hình ảnh-văn bản để căn chỉnh tốt CLIP-ViT và LLM, tức là, học một mô-đun chiếu hình ảnh-sang-văn bản tốt. PI được huấn luyện trước không chỉ đóng vai trò như một cầu nối kết nối hình ảnh và ngôn ngữ, mà còn cung cấp một khởi tạo tốt cho căn chỉnh đa phương thức-văn bản. Sau đó chúng tôi xây dựng UPM bằng cách kết hợp nhiều PI được huấn luyện trước: UPM = {Pk} = {Init(PI)}, trong đó Init là khởi tạo trọng số, điều này giảm hiệu quả chi phí căn chỉnh các phương thức khác với ngôn ngữ.

Căn chỉnh Đa phương thức-Văn bản. Chúng tôi hình thức hóa căn chỉnh đa phương thức-văn bản như một quá trình học tập liên tục [80]. Tại thời điểm t, chúng tôi đã huấn luyện mô hình trên một tập các phương thức M1 ∪ M2 ··· Mt-1, và dữ liệu huấn luyện hiện tại là từ Mt. Để ngăn chặn quên thảm khốc, chúng tôi sẽ lấy mẫu đều từ cả dữ liệu được huấn luyện trước và dữ liệu hiện tại. Trong trường hợp của chúng tôi, chúng tôi chia căn chỉnh đa phương thức-văn bản thành nhiều giai đoạn huấn luyện dựa trên quy mô dữ liệu của chúng: giai đoạn I (hình ảnh), giai đoạn II (video, âm thanh và đám mây điểm) và giai đoạn III (bản đồ độ sâu/bình thường, IMU và fMRI). Nếu chúng tôi muốn hỗ trợ các phương thức mới, chúng tôi có thể lặp lại tập huấn luyện, tức là, lấy mẫu một lượng dữ liệu tương tự từ các phương thức trước và huấn luyện chung mô hình với các phương thức hiện tại.

Tập dữ liệu Đa phương thức-Văn bản. Chúng tôi thu thập các cặp X-văn bản cho mỗi phương thức. Các cặp hình ảnh-văn bản bao gồm LAION-400M [70] và LAION-COCO [69]. Dữ liệu huấn luyện cho video, âm thanh và đám mây điểm là WebVid-2.5M [8], WavCaps [56] và Cap3D [54], tương ứng. Vì không có dữ liệu bản đồ độ sâu/bình thường-văn bản quy mô lớn, chúng tôi sử dụng mô hình DPT được huấn luyện trước [19, 68] để tạo ra bản đồ độ sâu/bình thường. Các hình ảnh nguồn và văn bản từ CC3M [73]. Đối với các cặp IMU-văn bản, chúng tôi sử dụng dữ liệu cảm biến IMU của Ego4D [27]. Đối với các cặp fMRI-văn bản, chúng tôi sử dụng tín hiệu fMRI từ tập dữ liệu NSD [5] và lấy các chú thích liên quan đến kích thích thị giác làm chú thích văn bản. Lưu ý rằng đầu vào cho LLM là sự nối của token phương thức và token chú thích. Chúng tôi không thêm lời nhắc hệ thống ở giai đoạn này để giảm số lượng token và tăng tốc độ huấn luyện.

3.3. Tinh chỉnh Hướng dẫn Đa phương thức Thống nhất
Sau khi căn chỉnh đa phương thức-văn bản, OneLLM trở thành một mô hình mô tả đa phương thức có thể tạo ra mô tả ngắn cho bất kỳ đầu vào nào. Để giải phóng hoàn toàn khả năng hiểu và suy luận đa phương thức của OneLLM, chúng tôi tuyển chọn một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức quy mô lớn để tinh chỉnh thêm OneLLM.

Tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức. Chúng tôi thu thập tập dữ liệu tinh chỉnh hướng dẫn (IT) cho mỗi phương thức. Theo các nghiên cứu trước [15, 48], các tập dữ liệu IT hình ảnh được lấy mẫu từ các tập dữ liệu sau: LLaVA-150K [49], COCO Caption [14], VQAv2 [26], GQA [34], OKVQA [55], A-OKVQA [71], OCRVQA [58], RefCOCO [36] và Visual Genome [38]. Các tập dữ liệu IT video bao gồm MSRVTT-Cap [91], MSRVTT-QA [89] và dữ liệu hướng dẫn video từ [104]. Các tập dữ liệu IT âm thanh bao gồm AudioCaps [37] và dữ liệu hội thoại âm thanh từ [104]. Tập dữ liệu IT đám mây điểm là một tập dữ liệu mô tả, hội thoại và suy luận đám mây điểm 70K từ [92]. Các tập dữ liệu IT bản đồ độ sâu/bình thường được tạo ra từ các tập dữ liệu IT hình ảnh: chúng tôi lấy mẫu ngẫu nhiên 50K dữ liệu hướng dẫn thị giác từ LLaVA-150K và tạo ra bản đồ độ sâu/bình thường bằng mô hình DPT [19]. Đối với các tập dữ liệu IT IMU và fMRI, chúng tôi cũng lấy mẫu ngẫu nhiên một tập con từ Ego4D [27] và NSD [5], tương ứng. Cuối cùng, các tập dữ liệu IT đa phương thức của chúng tôi có khoảng 2M mục, bao gồm nhiều nhiệm vụ như mô tả/suy luận chi tiết, hội thoại, trả lời câu hỏi ngắn và mô tả.

Thiết kế Lời nhắc. Với các phương thức và nhiệm vụ đa dạng trong các tập dữ liệu IT đa phương thức của chúng tôi, chúng tôi thiết kế cẩn thận các lời nhắc để tránh xung đột giữa chúng. (a) Khi sử dụng các tập dữ liệu IT được tạo ra bởi GPT4 (ví dụ, LLaVA-150K), chúng tôi sử dụng các lời nhắc gốc được cung cấp bởi các tập dữ liệu này. (b) Đối với các nhiệm vụ mô tả, chúng tôi sử dụng lời nhắc: Cung cấp một câu mô tả cho {modal} được cung cấp. (c) Đối với các nhiệm vụ trả lời câu hỏi mở, chúng tôi tăng cường câu hỏi với Trả lời câu hỏi bằng một từ hoặc cụm từ duy nhất. (d) Đối với các nhiệm vụ trả lời câu hỏi có tùy chọn, lời nhắc là: {Câu hỏi} {Tùy chọn} Trả lời bằng chữ cái của tùy chọn từ các lựa chọn đã cho trực tiếp. (e) Đối với các tập dữ liệu IMU và fMRI, chúng tôi áp dụng lời nhắc như Mô tả chuyển động và Mô tả cảnh này dựa trên dữ liệu fMRI. Mặc dù sử dụng các lời nhắc cố định này, các thí nghiệm của chúng tôi chỉ ra rằng OneLLM có khả năng tổng quát hóa sang các lời nhắc mở trong quá trình suy luận. Để biết các lời nhắc chi tiết về từng nhiệm vụ và phương thức, vui lòng xem Mục C.4 của phụ lục.

Trong giai đoạn tinh chỉnh hướng dẫn, chúng tôi tổ chức chuỗi đầu vào như: {q̄, Sys, [Inst, Anst]T t=1} trong đó q̄ là các token phương thức, Sys là lời nhắc hệ thống, [Inst, Anst] tương ứng với cặp hướng dẫn-trả lời thứ t trong một cuộc hội thoại. Lưu ý rằng đối với các đầu vào đa phương thức liên quan đến nhiều phương thức, chẳng hạn như các nhiệm vụ âm thanh-thị giác [42], chúng tôi đặt tất cả các token phương thức ở đầu chuỗi đầu vào.

Chúng tôi tinh chỉnh hoàn toàn LLM và giữ các tham số còn lại đông lạnh. Mặc dù các nghiên cứu gần đây thường sử dụng các phương pháp hiệu quả tham số [33], chúng tôi chỉ ra thực nghiệm rằng cách tiếp cận tinh chỉnh hoàn toàn khai thác các khả năng đa phương thức của OneLLM hiệu quả hơn, đặc biệt với việc sử dụng các LLM nhỏ hơn (ví dụ, LLaMA2-7B).

4. Thí nghiệm
4.1. Chi tiết Triển khai
Kiến trúc. Bộ mã hóa phổ quát là CLIP VIT Large được huấn luyện trước trên LAION [70]. LLM là LLaMA2-7B [79]. UPM có K=3 chuyên gia chiếu, trong đó mỗi chuyên gia có tám khối Transformer và 88M tham số. Kích thước của token phương thức cho mỗi phương thức là R30×1024.

Chi tiết Huấn luyện. Chúng tôi sử dụng bộ tối ưu hóa AdamW với β1=0.9, β2=0.95 và suy giảm trọng số 0.1. Chúng tôi áp dụng khởi động tốc độ học tuyến tính trong 2K lần lặp đầu tiên. Đối với giai đoạn I, chúng tôi huấn luyện OneLLM trên 16 GPU A100 trong 200K lần lặp. Kích thước lô hiệu quả (sử dụng tích lũy gradient) là 5120. Tốc độ học tối đa là 5e-5. Đối với giai đoạn II (resp. III), chúng tôi huấn luyện OneLLM trên 8 GPU trong 200K (resp. 100K) với kích thước lô hiệu quả 1080 và tốc độ học tối đa 1e-5. Trong giai đoạn tinh chỉnh hướng dẫn, chúng tôi huấn luyện OneLLM trên 8 GPU trong 1 epoch (96K) với kích thước lô hiệu quả 512 và tốc độ học tối đa 2e-5.

4.2. Đánh giá Định lượng
Chúng tôi đánh giá OneLLM trên các nhiệm vụ đa phương thức và đưa chi tiết đánh giá vào Mục D của phụ lục.

Đánh giá Hình ảnh-Văn bản. Trong Bảng 1, chúng tôi đánh giá OneLLM trên trả lời câu hỏi thị giác (VQA), mô tả hình ảnh và các điểm chuẩn đa phương thức gần đây. Đối với các nhiệm vụ VQA, OneLLM-7B vượt trội hơn các MLLM khác như ChatBridge-13B [104] và AnyMAL-13B [59] với biên độ lớn. Mô hình 7B của chúng tôi thậm chí còn tốt hơn AnyMAL với 70B tham số. Đối với các nhiệm vụ mô tả hình ảnh, OneLLM-7B ngang bằng với ChatBridge-13B. Mặc dù OneLLM không được thiết kế đặc biệt cho các nhiệm vụ thị giác, kết quả của chúng tôi chứng minh rằng OneLLM cũng có thể đạt được mức độ hàng đầu trong các LLM thị giác chuyên biệt, và khoảng cách giữa MLLM và LLM thị giác đã thu hẹp thêm.

Đánh giá Video-Văn bản. Như thể hiện trong Bảng 2, chúng tôi đánh giá OneLLM trên các nhiệm vụ trả lời câu hỏi và mô tả video. Mô hình của chúng tôi vượt trội hơn cả MLLM (ChatBridge và AnyMAL) và các mô hình chuyên biệt về video (FrozenBiLM [94] và InternVideo [84]) trong các nhiệm vụ trả lời câu hỏi video. Đáng chú ý, các tập dữ liệu huấn luyện của chúng tôi không bao gồm dữ liệu trả lời câu hỏi video như NextQA [86] và How2QA [46], đây là các nhiệm vụ trả lời câu hỏi video cung cấp các tùy chọn trả lời. Tuy nhiên, việc huấn luyện mô hình của chúng tôi trên các tập dữ liệu VQA tương tự (ví dụ, A-OKVQA [71]) đã rõ ràng tăng cường khả năng đa phương thức nổi bật của nó, góp phần cải thiện hiệu suất trong các nhiệm vụ trả lời câu hỏi video.

Đánh giá Âm thanh-Văn bản. Chúng tôi đánh giá OnLLM trên các nhiệm vụ mô tả và trả lời câu hỏi âm thanh. Trong Bảng 3, chúng tôi vượt trội hơn cả ChatBridge và LTU [25] trên Clotho Caption [18]. Đáng chú ý, kết quả zero-shot của chúng tôi trên Clotho AQA [47] ngang bằng với Pengi [17] được tinh chỉnh đầy đủ. Tương tự như kết luận của chúng tôi về trả lời câu hỏi video, chúng tôi tin rằng nhiệm vụ mô tả đòi hỏi huấn luyện đặc thù cho tập dữ liệu nhiều hơn, trong khi nhiệm vụ trả lời câu hỏi có thể là thước đo chính xác hơn về khả năng hiểu zero-shot vốn có của mô hình.

Đánh giá Âm thanh-Video-Văn bản. Chúng tôi đánh giá OneLLM trên các nhiệm vụ âm thanh-video-văn bản, chẳng hạn như trả lời câu hỏi (MUSIC AVQA [42]), mô tả (VALOR-32K [12]) và hoàn thành đối thoại (AVSD [3]) dựa trên video và âm thanh nền. Như thể hiện trong Bảng 4, OneLLM-7B vượt trội hơn ChatBridge-13B trên cả ba tập dữ liệu. Lưu ý rằng ChatBridge được huấn luyện trên tập dữ liệu âm thanh-thị giác [12], trong khi OneLLM chưa được huấn luyện trên bất kỳ tập dữ liệu âm thanh-thị giác nào. Vì tất cả các phương thức trong OneLLM đều được căn chỉnh tốt với ngôn ngữ, chúng tôi có thể trực tiếp đưa tín hiệu video và âm thanh vào OneLLM trong quá trình suy luận.

Đánh giá Đám mây Điểm-Văn bản. Trong Bảng 5, chúng tôi đánh giá OneLLM trên các nhiệm vụ mô tả và phân loại đám mây điểm. OneLLM có thể đạt được kết quả mô tả xuất sắc do các lời nhắc hướng dẫn được thiết kế cẩn thận của chúng tôi để chuyển đổi giữa các nhiệm vụ (Mục 3.3), trong khi InstructBLIP [15] và PointLLM [92] gặp khó khăn trong việc tạo ra các mô tả ngắn và chính xác. Trong nhiệm vụ phân loại, OneLLM cũng có thể đạt được kết quả tương đương với PointLLM.

Đánh giá Bản đồ Độ sâu/Bình thường-Văn bản. Vì hiện tại không có các nhiệm vụ trả lời câu hỏi và mô tả sử dụng bản đồ độ sâu/bình thường, chúng tôi đánh giá OneLLM trên hai tập dữ liệu phân loại cảnh [60, 76]. Hiệu suất, như được hiển thị trong Bảng 6, cho thấy OneLLM đạt được độ chính xác phân loại zero-shot vượt trội so với CLIP. Những kết quả này khẳng định rằng OneLLM được huấn luyện trên dữ liệu bản đồ độ sâu/bình thường tổng hợp có thể thích ứng với các tình huống thế giới thực.

Đánh giá IMU-Văn bản và fMRI-Văn bản. Vì việc tạo văn bản từ IMU/fMRI hiếm khi được khám phá trong tài liệu trước đây, chúng tôi chỉ báo cáo kết quả của chúng tôi về mô tả IMU/fMRI. Đối với mô tả IMU trên Ego4D [27], chúng tôi đánh giá OneLLM trên một tập con giữ lại với 2000 mục. Điểm CIDEr và ROUGE-L là 24.9 và 19.5, tương ứng. Đối với mô tả fMRI trên NSD [5], chúng tôi đánh giá OneLLM trên tập thử nghiệm của nó, nơi OneLLM đạt được 31.7 CIDEr và 25.1 ROUGE-L.

4.3. Thí nghiệm Loại bỏ
Trong phần này, chúng tôi sẽ khám phá một số thiết kế chính của OneLLM. Các thí nghiệm loại bỏ của chúng tôi được thực hiện trên một tập con của dữ liệu huấn luyện, chỉ bao gồm các tập dữ liệu căn chỉnh đa phương thức và tinh chỉnh hướng dẫn của hình ảnh, âm thanh và video, ngoại trừ các nghiên cứu về số lượng chuyên gia. Các cài đặt khác không thay đổi nếu không được chỉ định.

Huấn luyện Riêng biệt so với Huấn luyện Chung. Một câu hỏi quan trọng đối với MLLM là liệu một MLLM được huấn luyện chung có tốt hơn MLLM đặc thù phương thức hay không? Để giải quyết điều này, chúng tôi so sánh hiệu suất của các MLLM được huấn luyện riêng biệt với một MLLM được huấn luyện chung trong Bảng 7 (a). Trong huấn luyện riêng biệt, mô hình chỉ có thể truy cập dữ liệu của riêng mình; trong huấn luyện chung, mô hình được huấn luyện chung trên tất cả dữ liệu. Trên hai nhiệm vụ hình ảnh-văn bản NoCaps và VQAv2, chúng ta có thể thấy rằng các mô hình được huấn luyện riêng biệt và chung đạt được kết quả tương đương; Trong khi các mô hình âm thanh và video được huấn luyện riêng biệt kém hơn nhiều so với mô hình được huấn luyện chung trên ClothoQA và MSVDQA, tương ứng. Điều này gợi ý rằng huấn luyện chung có lợi đáng kể cho các phương thức ít dữ liệu (ví dụ, âm thanh và video), bằng cách cho phép chuyển giao kiến thức đã học (ví dụ, trả lời câu hỏi) giữa các phương thức.

Căn chỉnh Hình ảnh Có lợi cho Căn chỉnh Đa phương thức. Bảng 7 (b) chứng minh rằng OneLLM với căn chỉnh hình ảnh-văn bản có thể giúp căn chỉnh đa phương thức-văn bản. Nếu chúng tôi căn chỉnh trực tiếp tất cả các phương thức với văn bản bằng một mô hình được khởi tạo ngẫu nhiên (tức là mô-đun chiếu phổ quát), hiệu suất trên hình ảnh và video sẽ giảm đáng kể. Thay vào đó, OneLLM với huấn luyện trước hình ảnh-văn bản có thể cân bằng tốt hơn các phương thức khác nhau.

Số lượng Chuyên gia Chiếu. Số lượng chuyên gia chiếu trong UPM có liên quan chặt chẽ với số lượng phương thức mà OneLLM có thể chứa. Như thể hiện trong Bảng 7, OneLLM với ba chuyên gia chiếu đủ để giữ tất cả các phương thức. Việc tăng số lượng chuyên gia không mang lại sự cải thiện mong muốn, trong khi kết quả với một chuyên gia cũng không thỏa mãn.

Loại Bộ định tuyến. Bộ định tuyến phương thức dùng để liên kết nhiều chuyên gia chiếu thành một mô-đun duy nhất. Ở đây chúng tôi thảo luận ba loại bộ định tuyến: bộ định tuyến không đổi, bộ định tuyến thưa thớt và bộ định tuyến mềm mặc định. (a) Bộ định tuyến không đổi liên kết K chuyên gia với một số không đổi 1/K. Đầu ra của bộ định tuyến không đổi là ∑(k=1 to K) 1/K · Pk(x). (b) Bộ định tuyến thưa thớt chỉ chọn một chuyên gia với trọng số định tuyến tối đa. Đầu ra là wk* Pk*(x) trong đó k* = arg max k wk. Như thể hiện trong Bảng 7 (d), bộ định tuyến mềm vượt trội hơn hai bộ định tuyến khác, cho thấy hiệu quả của nó trong việc định tuyến động các tín hiệu đa phương thức.

4.4. Phân tích Định tính
Hình 3 đưa ra một số kết quả định tính của OneLLM trên tám phương thức. Chúng tôi cho thấy OneLLM có thể (a) hiểu cả nội dung thị giác và văn bản trong hình ảnh, (b) tận dụng thông tin thời gian trong video, (c) viết sáng tạo dựa trên nội dung âm thanh, (d) hiểu chi tiết của các hình dạng 3D, (e) phân tích các cảnh thị giác được ghi lại trong dữ liệu fMRI, (f) đoán hành động của người dựa trên dữ liệu chuyển động, và (g)-(h) hiểu cảnh bằng bản đồ độ sâu/bình thường. Do hạn chế về không gian, chúng tôi đưa thêm kết quả định tính vào Mục F của phụ lục.

5. Kết luận
Trong nghiên cứu này, chúng tôi giới thiệu OneLLM, một MLLM căn chỉnh tám phương thức với ngôn ngữ bằng một khung thống nhất. Ban đầu, chúng tôi trước tiên huấn luyện một LLM thị giác cơ bản. Dựa trên điều này, chúng tôi thiết kế một khung đa phương thức với một bộ mã hóa phổ quát, một UPM và một LLM. Bằng một đường ống căn chỉnh tích lũy, OneLLM có thể xử lý các đầu vào đa phương thức với một mô hình duy nhất. Hơn nữa, chúng tôi tuyển chọn một tập dữ liệu hướng dẫn đa phương thức quy mô lớn để giải phóng hoàn toàn khả năng tuân theo hướng dẫn của OneLLM. Cuối cùng, chúng tôi đánh giá OneLLM trên 25 điểm chuẩn đa dạng, cho thấy hiệu suất xuất sắc của nó.

Hạn chế và Công việc Tương lai. Nghiên cứu của chúng tôi đối mặt với hai thách thức chính: (i) Việc thiếu các tập dữ liệu quy mô lớn, chất lượng cao cho các phương thức ngoài hình ảnh, dẫn đến khoảng cách nhất định giữa OneLLM và các mô hình chuyên biệt trên những phương thức này. (ii) Hiểu đa phương thức tinh tế trong hình ảnh độ phân giải cao, video và âm thanh chuỗi dài, v.v. Trong tương lai, chúng tôi sẽ thu thập các tập dữ liệu chất lượng cao và thiết kế các bộ mã hóa mới để thực hiện hiểu đa phương thức tinh tế, ví dụ, hỗ trợ các đầu vào có độ dài khác nhau [9].

Lời cảm ơn. Nghiên cứu này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số hiệu 62306261), CUHK Direct Grants (Số hiệu 4055190), Chương trình R&D Trọng điểm Quốc gia Trung Quốc (2022ZD0160201) và Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải.

A. Tổng quan Phụ lục
• Mục B: Thí nghiệm Loại bỏ Bổ sung.
• Mục C: Chi tiết Triển khai Bổ sung.
• Mục D: Chi tiết Đánh giá.
• Mục E: So sánh với Nghiên cứu Trước.
• Mục F: Kết quả Định tính Bổ sung.

B. Thí nghiệm Loại bỏ Bổ sung
Trong bài báo chính, chúng tôi theo các nghiên cứu trước [103] và đặt một CLIP-ViT đông lạnh làm bộ mã hóa phổ quát. Ở đây chúng tôi khám phá các lựa chọn thiết kế khác như CLIP-ViT có thể huấn luyện và DINOv2 [63] làm bộ mã hóa.

Bộ mã hóa Đông lạnh so với Có thể Huấn luyện. Chúng tôi trước tiên bật tất cả các tham số trong giai đoạn căn chỉnh đa phương thức-văn bản. Như thể hiện trong Bảng 8, hiệu suất cho các phương thức thị giác (hình ảnh và video) giảm đáng kể, trong khi kết quả cho trả lời câu hỏi âm thanh (ClothoQA) cải thiện 4.7%. Chúng tôi nghĩ rằng CLIP có thể huấn luyện sẽ phá vỡ các biểu diễn thị giác-ngôn ngữ được huấn luyện trước nhưng có thể để lại nhiều không gian hơn cho việc học các phương thức khác. Tuy nhiên, xem xét việc sử dụng bộ nhớ (46Gb so với 74Gb), CLIP đông lạnh sẽ là lựa chọn tốt hơn cho khung của chúng tôi.

Ngoài Bộ mã hóa Thị giác-Ngôn ngữ. Ngoài bộ mã hóa thị giác-ngôn ngữ CLIP-ViT, chúng tôi cũng khám phá các mô hình khác, chẳng hạn như mô hình thị giác tự giám sát DINOv2 [63], làm bộ mã hóa phổ quát. Trong Bảng 8, chúng tôi nhận thấy rằng hiệu suất của OneLLM sử dụng DINOv2 thấp hơn mô hình sử dụng CLIP-ViT vì DINOv2 không được căn chỉnh với ngôn ngữ và chúng tôi cần học căn chỉnh thị giác-ngôn ngữ từ đầu.

C. Chi tiết Triển khai Bổ sung
C.1. Tokenizer Phương thức Nhẹ
Tokenizer phương thức dùng để chuyển đổi tín hiệu đầu vào thành một chuỗi các token. Ở đây chúng tôi sẽ giới thiệu chi tiết tokenizer của mỗi phương thức.

Tokenizer Thị giác. Chúng tôi sử dụng cùng cài đặt tokenizer cho các phương thức thị giác, tức là, hình ảnh, video, bản đồ độ sâu/bình thường. Tokenizer thị giác là một lớp tích chập 2D duy nhất:
Conv2D(Cin=3, Cout=1024, K=(14,14), S=(14,14)), (3)
trong đó Cin, Cout, K và S biểu thị kênh đầu vào, kênh đầu ra, kích thước kernel và stride, tương ứng. Lưu ý rằng đối với đầu vào video x∈RT×H×W với T khung hình, chiều cao H và chiều rộng W, chúng tôi đưa song song các khung hình của nó vào tokenizer, dẫn đến T×H/14×W/14 token. Tương tự, hình ảnh, bản đồ độ sâu/bình thường cũng có thể được coi như đầu vào video một khung hình x∈R1×H×W.

Tokenizer Âm thanh. Chúng tôi trước tiên chuyển đổi tín hiệu âm thanh thành các đặc trưng spectrogram 2D x∈R1×H×W, trong đó H=128 và W=1024 theo mặc định. Theo [24], tokenizer âm thanh là một lớp tích chập 2D duy nhất:
Conv2D(Cin=1, Cout=1024, K=(16,16), S=(10,10)). (4)

Tokenizer Điểm. Đối với một đám mây điểm thô, chúng tôi lấy mẫu 8192 điểm bằng Lấy mẫu Điểm Xa nhất (FPS), dẫn đến một tensor 2D x∈R8192×6. Sau đó chúng tôi sử dụng thuật toán KNN để nhóm các điểm này thành 512 nhóm: x∈R512×32×6 trong đó 32 là kích thước của mỗi nhóm. Sau đó, chúng tôi mã hóa đám mây điểm với một lớp tích chập 2D:
Conv2D(Cin=6, Cout=1024, K=(1,1), S=(1,1)), (5)
theo sau bởi một phép toán max trên chiều 1. Cuối cùng, hình dạng của token đầu ra là R1024×1024.

Tokenizer IMU. Đối với đầu vào IMU có hình dạng R2000×6, chúng tôi token hóa nó với một lớp tích chập 1D:
Conv1D(Cin=6, Cout=1024, K=10, S=1), (6)
dẫn đến một chuỗi token x∈R1024×391.

Tokenizer fMRI. Hình dạng của tín hiệu fMRI là R15724. Chúng tôi token hóa nó với một lớp tích chập 1D:
Conv1D(Cin=15724, Cout=8196, K=1, S=1). (7)
Sau đó chúng tôi thay đổi kích thước tensor đầu ra x∈R8196 thành tensor 2D x∈R1024×8 để căn chỉnh với đầu vào của bộ mã hóa transformer.

C.2. Tập dữ liệu Căn chỉnh Đa phương thức-Văn bản
Chúng tôi tóm tắt tập dữ liệu căn chỉnh đa phương thức-văn bản trong Bảng 9. Đối với các cặp độ sâu/bình thường-văn bản, chúng tôi sử dụng mô hình DPT [68] được huấn luyện trước trên ominidata [19] để tạo ra bản đồ độ sâu/bình thường. Tập dữ liệu nguồn là một tập con của CC3M [73], khoảng 0.5M cặp hình ảnh-văn bản. Đối với các cặp IMU-văn bản, chúng tôi sử dụng dữ liệu cảm biến IMU của Ego4D [27] và các tường thuật video tương ứng (tức là, chú thích văn bản). Đối với các cặp fMRI-văn bản, chúng tôi sử dụng phiên hình ảnh subj01 của NSD [5] và tuân theo cùng một phân chia dữ liệu với [72]. Lưu ý rằng kích thích thị giác, tức là, hình ảnh được hiển thị cho người tham gia, là từ MS COCO [14]. Do đó, chúng tôi sử dụng các chú thích hình ảnh trong COCO Captions làm chú thích văn bản của các cặp fMRI-văn bản.

C.3. Tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức
Chúng tôi tóm tắt tập dữ liệu tinh chỉnh hướng dẫn đa phương thức trong Bảng 9.

C.4. Thiết kế Lời nhắc
Các định dạng lời nhắc cho mỗi tập dữ liệu được thể hiện trong Bảng 10.

D. Chi tiết Đánh giá
Trong phần này, chúng tôi trước tiên liệt kê các lời nhắc đánh giá cho mỗi tập dữ liệu trong Bảng 11. Sau đó chúng tôi sẽ đưa ra thêm chi tiết đánh giá.

Các Nhiệm vụ Hình ảnh, Video và Âm thanh. Chúng tôi đánh giá tất cả các tập dữ liệu bằng các giao thức đánh giá chính thức của chúng. Như thể hiện trong Bảng 11, đối với các nhiệm vụ trả lời câu hỏi có tùy chọn, chúng tôi yêu cầu OneLLM dự đoán trực tiếp các chữ cái tùy chọn; Đối với các nhiệm vụ trả lời câu hỏi mở, chúng tôi yêu cầu OneLLM dự đoán một từ hoặc cụm từ duy nhất. Đối với các nhiệm vụ mô tả, chúng tôi yêu cầu OneLLM tạo ra một câu mô tả duy nhất. Lưu ý rằng đối với các nhiệm vụ âm thanh-video-văn bản, chuỗi đầu vào cho LLM là: {Token Video} {Token Âm thanh} {Lời nhắc Văn bản}.

Các Nhiệm vụ Đám mây Điểm. Đánh giá của chúng tôi về các nhiệm vụ đám mây điểm chủ yếu theo PointLLM [92]. Đối với nhiệm vụ phân loại đám mây điểm, chúng tôi sử dụng cùng lời nhắc như PointLLM: Đây là gì, và đánh giá độ chính xác bằng GPT4.

Các Nhiệm vụ Bản đồ Độ sâu/Bình thường. Đối với phân loại cảnh sử dụng bản đồ độ sâu/bình thường, chúng tôi trước tiên đặt danh sách danh mục ở đầu lời nhắc, sau đó chúng tôi yêu cầu OneLLM chọn một lớp từ danh sách.

Các Nhiệm vụ IMU/fMRI. Chúng tôi đánh giá các nhiệm vụ mô tả IMU/fMRI. Các lời nhắc giống như lời nhắc huấn luyện của chúng: Mô tả chuyển động cho mô tả IMU và Mô tả cảnh dựa trên dữ liệu fMRI cho mô tả fMRI.

E. So sánh với Nghiên cứu Trước
Sự khác biệt chính giữa OneLLM và các MLLM trước là chúng tôi cho thấy một bộ mã hóa thống nhất đủ để căn chỉnh đa phương thức với LLM. Như thể hiện trong Bảng 12, OneLLM với một bộ mã hóa phổ quát, một mô-đun chiếu và ít tham số hơn (0.6B) có thể thống nhất nhiều phương thức hơn vào một khung. Các kết quả trong bài báo chính (Bảng 1-6) cũng chứng minh rằng OneLLM có thể đạt được hiệu suất tốt hơn so với các nghiên cứu trước. Các thí nghiệm loại bỏ trong Bảng 7 (a) cũng cho thấy rằng việc huấn luyện chung tất cả các phương thức với khung thống nhất của chúng tôi có thể có lợi cho các phương thức ít dữ liệu. Ở đây chúng tôi không cố gắng chứng minh rằng kiến trúc của OneLLM là tối ưu, mà để chỉ ra khả năng xây dựng MLLM bằng một khung thống nhất và có thể mở rộng.

F. Kết quả Định tính Bổ sung
Trong phần này, chúng tôi cung cấp thêm kết quả định tính trong Hình 4, Hình 5 và Hình 6.

Tài liệu tham khảo
[1] Sharegpt. https://sharegpt.com/, 2023. 10
[2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, và Peter Anderson. nocaps: novel object captioning at scale. Trong ICCV, 2019. 6, 7, 10
[3] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim K. Marks, Chiori Hori, Peter Anderson, Stefan Lee, và Devi Parikh. Audio-visual scene-aware dialog. Trong CVPR, 2019. 6, 7, 10
[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, và cộng sự. Flamingo: a visual language model for few-shot learning. NeurIPS, 35:23716–23736, 2022. 1, 2, 6

[Tiếp tục với tất cả các tài liệu tham khảo còn lại...]
