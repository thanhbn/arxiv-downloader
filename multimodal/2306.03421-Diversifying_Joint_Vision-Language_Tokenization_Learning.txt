# 2306.03421.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2306.03421.pdf
# File size: 2107058 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Diversifying Joint Vision-Language Tokenization Learning
Vardaan Pahuja1∗, AJ Piergiovanni2, Anelia Angelova2
1The Ohio State University2Google DeepMind
pahuja.9@osu.edu, {ajpiergi, anelia }@google.com
Abstract
Building joint representations across images and text is
an essential step for tasks such as Visual Question Answer-
ing and Video Question Answering. In this work, we find
that the representations must not only jointly capture fea-
tures from both modalities but should also be diverse for
better generalization performance. To this end, we propose
joint vision-language representation learning by diversify-
ing the tokenization learning process, enabling tokens that
are sufficiently disentangled from each other to be learned
from both modalities. We observe that our approach out-
performs the baseline models in a majority of settings and
is competitive with state-of-the-art methods.
1. Introduction
Visual Question Answering (VQA) has received consid-
erable attention in the research community in recent years.
The task involves predicting a textual response to a natural
language question based on an image. This can be achieved
by either classifying the response from a fixed set of an-
swers or generating a free-form textual response. More
recently, video question answering (VideoQA) has gained
popularity as a more complex task in multimodal AI. It is
more challenging compared to image-based question an-
swering because it involves reasoning about the content of
objects and sequences of actions in different frames and
linking them to the natural language used in the question.
Consider the example shown in Figure 1. Given a question
-Who is the person in pink dress playing with? , the model
is tasked to infer the correct response - child .
The Transformer model [7] has revolutionized the field
of NLP through self-supervised learning, resulting in state-
of-the-art results across benchmarks [7,24,27] and remark-
able few-shot generalization capabilities [3]. This has moti-
vated similar efforts for vision-language (VL) tasks, where
models like CLIP [21], ALIGN [14], and Florence [34]
show promising potential for transfer learning on down-
∗Work done while at Googlestream tasks. Self-supervised vision-language models, such
as LXMERT [26], ViLBERT [18], and VisualBERT [16],
employ this paradigm to pre-train on a related dataset and
fine-tune on downstream tasks, including VQA. A line of
approaches in the literature first extracts the features for
each of the two modalities separately and then attempts
cross-modal fusion using a simple concatenation of VL
tokens [16, 17, 25] or by using cross-attention [18, 26].
However, the Co-tokenization model [20] demonstrates that
cross-modality interaction during the feature extraction pro-
cess is a much more effective way to reason across the
two modalities. It involves iterative learning of a set of
token representations using TokenLearner [23] conditioned
on both video and text. Such learned token representations
allow the subsequent Transformer layers to process only a
few tokens, resulting in more efficient models.
Representation learning is a central problem in mod-
ern machine learning. A disentangled representation is
one where each feature captures information about only
one salient factor of variation [2]. Disentangled represen-
tations have superior out-of-domain (OOD) generalization
[9, 11, 28], better interpretability [1, 12], sample efficiency
[12], and transfer learning capabilities [35]. To this end,
we propose a different perspective on joint image-language
learning, with a focus on disentangling the tokens that con-
stitute the cross-modal feature representation. We use a
diversity enforcing loss to encourage that the feature rep-
resentations are disentangled and thus representative in an
economical way. Our representations outperform the base-
lines in most settings and show competitive performance
with state-of-the-art methods.
2. Background
TokenLearner : TokenLearner [23] aims to adaptively learn
a fixed set of token representations corresponding to one
or more modalities, e.g. images, video, and text. The key
idea is to select a series of informative combinations of spa-
tial locations in the image/video conditioned on all modali-
ties. More formally, let X∈RH×W×Cbe an input image,
where H, W, andCdenote the image dimensions and the
no. of channels, respectively. For the ithtoken zi, it learnsarXiv:2306.03421v2  [cs.CV]  15 Jun 2023

--- PAGE 2 ---
Figure 1. The VideoQA task involves answering a natural language question based on a video clip. Different from visual question
answering, VideoQA requires understanding the sequence of actions/events being performed by characters in a clip and their interaction
with objects to answer a question correctly. Some examples from the MSRVTT dataset [31] with the corresponding GT answer are shown.
a spatial attention map αi(X)which is multiplied with the
input to generate a token output Ai(X),
zi=Ai(X) =ρ(X⊙γ(αi(X))),
where ⊙denotes Hadamard product, γ(·)denotes the
broadcasting function, and ρ(·)denotes spatial global
average pooling.
Video-text iterative co-tokenization model : The Video-
text iterative co-tokenization model [20] (Co-tokenization
henceforth) takes a unique approach to VideoQA by in-
tegrating interactions between video and text directly into
the visual feature extraction process rather than considering
them as an afterthought. Such interactions enable better rea-
soning across the two modalities. This model uses multiple
streams of video at different spatio-temporal scales for mul-
timodal representation learning. Better temporal resolution
may be required to infer certain actions whereas better spa-
tial resolution aids correct identification of objects in video
frames, thus resulting in a trade-off. This necessitates the
use of multiple streams of different resolutions for superior
performance.
The Co-tokenization model is based on a T5-style [22]
encoder-decoder Transformer architecture. During the
encoding process, the Transformer iteratively generates
learned token representations in each layer by adaptive fu-
sion of visual tokens. The input to the Transformer model
is a concatenation of language features and fused visual to-
kens from the previous layer. Finally, the decoder generates
textual output based on token representations from the last
layer as the response. The model obtains initial visual and
language features using an X3D model [8] and a T5 en-
coder, respectively. We use the Co-tokenization model as
the baseline for VideoQA.
Decoder LayersEncoder LayersAdaptive Tokenization + DisentanglementVision EncoderLanguage
EncoderImage/VideoQuestionAnswerFusion Module
Disentangled Token Attention MapsDisentangled Token Representations
Figure 2. Overall model architecture. The two modalities are fused
by adaptive tokenization to generate a fixed set of disentangled
learned tokens.
3. Proposed Approach
3.1. Baseline Model
The baseline model consists of an encoder-decoder
model based on T5 text encoder [22] and a ResNet-50 [10]
vision encoder. The two modalities are fused together to
form a joint feature space from which a smaller set of to-
kens are learned. For VideoQA, we use the Co-tokenization
model as baseline. For the VQA task, we use a simplified
version of the Co-tokenization model which generates a sin-
gle set of learned tokens instead of iterative tokenization.
Figure 2 shows the overall model architecture.
3.2. Disentangled representations
To encourage the model to learn diverse token represen-
tations, we propose a new loss function - diversity loss to be

--- PAGE 3 ---
used in conjunction with the main model objective.
Ldiv=NX
k=1MX
j=1,j̸=i< αk
i, αk
j>2(1)
Here, αk
idenotes the spatial attention weights for the ith
token in example k.NandMdenote the number of ex-
amples and the number of fused tokens respectively. This
is inspired by a similar technique to disentangle the sphere
of attention of multiple agents in robotic manipulation [36].
The disentangled representations effectively enforce the se-
lected representations to be orthogonal to each other. Our
baseline model presented in the experiments uses identical
model hyperparameters and the number of learned tokens,
so as to be directly comparable to the proposed approach.
4. Experiments
4.1. Datasets
We evaluate our approach on the following datasets:
MSRVTT-QA [31]: This dataset was created using MSR-
VTT video descriptions dataset [32]. It contains 10K video
clips and 243K question-answer pairs.
MSVD-QA [31]: This dataset is based on Microsoft
Research Video Description Corpus [5]. It contains 50K
questions based on 1970 video clips.
IVQA [33]: It is a dataset consisting of ‘how-to’ videos. It
has 10K video clips with one question and 5 answers/clip.
SNLI-VE [30]: The visual entailment task involves
predicting whether the given statement is entail-
ment/contradiction/neural in the context of the image.
GQA [13]: It is a popular benchmark for visual reasoning,
which was developed to address the biases of existing VQA
datasets. The questions in this dataset are compositional
and are grounded in Visual Genome [15] scene graphs.
4.2. Implementation Details
For both VQA and VideoQA models, we use the Adam
optimizer with a weight decay of 1e-4 and L= 12 , A=
12, H= 768 for the Transformer model where L,A, andH
denote the number of layers, the number of attention heads
per layer, and the hidden size, respectively.
The VideoQA models use 16 frames and 224×224im-
ages. The two streams are 8×224×224and16×112×112.
Furthermore, in order to save compute we demonstrate
competitive performance by using only one-third of the
Transformer layers (and training iterations) of the original
Co-tokenization model, which allows our model to run at
fewer FLOPs. For VideoQA models, pre-training is per-
formed on a 10% subset of the Howto69MVQA dataset
[33]. For VQA models, pre-training is done on the Concep-
tual Captions dataset [4]. We use 8 and 16 learned tokens
for VideoQA and VQA experiments, respectively.4.3. Results
VideoQA. We compare our approach with the baseline
model both with and without pre-training (Table 1). We
observe that our approach consistently outperforms the
baseline models both for both settings on MSRVTT-QA,
MSVD-QA, and IVQA. Table 2 compares it to the state-
of-the-art Co-tokenization model, which shows the compet-
itive performance of our model. We note that our model
has the same GFLOPS as the baseline model as the number
of learned tokens is the same for both. However since the
disentangled tokenization is able to select the tokens in a
more economical way (i.e. some tokens might be ‘empty’),
in general, fewer tokens will be needed in the end, resulting
in fewer FLOPs overall.
Dataset Pre-training Model Accuracy
MSRVTT-QABaseline 31.06
Ours 31.37
✓Baseline 31.78
Ours 33.05
MSVD-QABaseline 27.98
Ours 28.22
✓Baseline 28.08
Ours 30.11
IVQABaseline 9.48
Ours 9.96
✓Baseline 8.86
Ours 9.97
Table 1. Video QA results with and without pre-training (PT) in
the open vocabulary setting (validation set). The baseline is a sim-
ilar capacity Co-tokenization model.
Model MSRVTT-QA MSVD-QA GFLOPs
Co-tok. [20] 33.7 32.5 67
Ours 33.1 30.1 41
Table 2. Comparison to state-of-the-art approaches for VideoQA.
The comparison is done for the open-vocabulary setting. Our
model FLOPs are much fewer than the prior work. We pretrain
on 10% subset of the HowTo69MVQA dataset [33], whereas Co-
tokenization pretrained on the full HowTo100M dataset [19]. We
demonstrate competitive performance despite having a smaller
model capacity.
VQA. For the pre-training setting, our approach obtains
consistent improvements on the validation set for both
datasets and on the test set for SNLI-VE (Table 3). Simi-
larly, we outperform the baseline for the SNLI-VE dataset
in the no pre-training setting (Table 4). This shows that the
proposed disentangled tokens provide better performance

--- PAGE 4 ---
compared to the baseline for the same quota of tokens al-
located. Table 5 compares to the state-of-the-art models,
and while our models perform well, they do not outperform
particularly large models.
Val. set Test set
Dataset Model E.M. F1 E.M. F1
SNLI-VE Baseline 76.70 76.70 76.59 76.59
Ours 78.06 78.06 77.36 77.37
GQA Baseline 73.48 73.56 73.5 73.57
Ours 75.02 75.11 75.01 75.1
Table 3. VQA results in the pre-training setting.
Val. set Test set
Dataset Model E.M. F1 E.M. F1
SNLI-VE Baseline 73.08 73.08 72.5 72.5
Ours 73.15 73.15 72.69 72.69
GQA Baseline 68.08 68.13 68.14 68.2
Ours 67.98 68.02 67.98 68.02
Table 4. VQA results in the no pre-training setting.
Model GQA SNLI-VE
SimVLM (Huge) [29] – 86.32
UNITER [6] – 79.38
VinVL [37] 65.05 –
LXMERT [26] 60.0 –
Ours 76.79 80.15
Table 5. Comparison to state-of-the-art approaches (test-dev set
for GQA and test set for SNLI-VE). Missing values are denoted
by –. Our model uses a 3M dataset for pre-training and has about
300M parameters. Approaches which use much larger data or
model are shown in the top section.
4.4. Visualization
Figure 3 visualizes the learned disentangled represen-
tations, along with their corresponding attention maps for
a VQA example. We observe that they localize their at-
tention to much more specific areas of the image, that are
vital for answering the question. Furthermore, the tokens
that are selected prioritize joint visual-language represen-
tation, thereby capturing essential features from both the
visual and linguistic inputs. Additional visualizations are
shown in Section A in the Appendix.
(a) Question Image
(b) Token visualization (Baseline)
(c) Token visualization (Ours)
Figure 3. (a) Question : what kind of climbing vine or plant is this?
Baseline : tombppry, Ours : ivy ,Ground truth answers = [‘fern’,
‘grape’, ‘vine’, ‘ivy’ , ‘unanswerable’, ‘creeping fig’, ‘unanswer-
able’, ‘unanswerable’, ‘ivy’ , ‘green’]; Bottom left : Weights as-
signed to each image patch for every token, lighter shades like
yellow correspond to higher weights; Bottom right : Token atten-
tion masks grounded to the input image.
5. Conclusion
In this work, we propose learning disentangled repre-
sentations for the learned tokens in Transformer models
for VQA and VideoQA tasks. This simple-yet-effective
approach leads to a performance boost in a majority of
training settings across datasets. Future work will involve
benchmarking this approach with higher capacity models
and more pre-training for improved performance. Another
promising future direction is to utilize the learned token rep-
resentations for related downstream tasks.

--- PAGE 5 ---
References
[1] Tameem Adel, Zoubin Ghahramani, and Adrian Weller. Dis-
covering interpretable representations for both deep genera-
tive and discriminative models. In International Conference
on Machine Learning , pages 50–59. PMLR, 2018. 1
[2] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-
resentation learning: A review and new perspectives. IEEE
transactions on pattern analysis and machine intelligence ,
35(8):1798–1828, 2013. 1
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1
[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3558–3568, 2021. 3
[5] David Chen and William Dolan. Collecting highly parallel
data for paraphrase evaluation. In Proceedings of the 49th
Annual Meeting of the Association for Computational Lin-
guistics: Human Language Technologies , pages 190–200,
Portland, Oregon, USA, June 2011. Association for Com-
putational Linguistics. 3
[6] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:
Universal image-text representation learning. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part XXX , pages
104–120. Springer, 2020. 4
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 1
[8] Christoph Feichtenhofer. X3d: Expanding architectures for
efficient video recognition. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition ,
pages 203–213, 2020. 2
[9] Stathi Fotiadis, Shunlong Hu, Mario Lino, Chris Cantwell,
and Anil Bharath. Disentangled generative models for
robust dynamical system prediction. arXiv preprint
arXiv:2108.11684 , 2021. 1
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2
[11] Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey,
Christopher Burgess, Alexander Pritzel, Matthew Botvinick,
Charles Blundell, and Alexander Lerchner. Darla: Improv-
ing zero-shot transfer in reinforcement learning. In Interna-
tional Conference on Machine Learning , pages 1480–1490.
PMLR, 2017. 1
[12] Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal,
Christopher P Burgess, Matko Bo ˇsnjak, Murray Shanahan,
Matthew Botvinick, Demis Hassabis, and Alexander Lerch-
ner. SCAN: Learning hierarchical compositional visual con-cepts. In International Conference on Learning Representa-
tions , 2018. 1
[13] Drew A. Hudson and Christopher D. Manning. GQA: A new
dataset for real-world visual reasoning and compositional
question answering. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2019, Long Beach, CA,
USA, June 16-20, 2019 , pages 6700–6709. Computer Vision
Foundation / IEEE, 2019. 3
[14] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
Conference on Machine Learning , pages 4904–4916. PMLR,
2021. 1
[15] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 3
[16] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. Visualbert: A simple and perfor-
mant baseline for vision and language. arXiv preprint
arXiv:1908.03557 , 2019. 1
[17] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics
aligned pre-training for vision-language tasks. In Andrea
Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm, editors, Computer Vision - ECCV 2020 - 16th Eu-
ropean Conference, Glasgow, UK, August 23-28, 2020, Pro-
ceedings, Part XXX , volume 12375 of Lecture Notes in Com-
puter Science , pages 121–137. Springer, 2020. 1
[18] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. Advances in neural information
processing systems , 32, 2019. 1
[19] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2630–2640, 2019. 3
[20] AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael
Ryoo, and Anelia Angelova. Video question answering with
iterative video-text co-tokenization. ECCV , 2022. 1, 2, 3
[21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1
[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Pe-
ter J Liu, et al. Exploring the limits of transfer learning
with a unified text-to-text transformer. J. Mach. Learn. Res. ,
21(140):1–67, 2020. 2

--- PAGE 6 ---
[23] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa
Dehghani, and Anelia Angelova. Tokenlearner: Adaptive
space-time tokenization for videos. Advances in Neural In-
formation Processing Systems , 34:12786–12797, 2021. 1
[24] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. Superglue: Learning feature
matching with graph neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4938–4947, 2020. 1
[25] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. VL-BERT: pre-training of generic
visual-linguistic representations. In 8th International Con-
ference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.
1
[26] Hao Tan and Mohit Bansal. LXMERT: Learning cross-
modality encoder representations from transformers. In
Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , pages 5100–5111, Hong Kong, China, Nov. 2019.
Association for Computational Linguistics. 1, 4
[27] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R Bowman. Glue: A multi-task
benchmark and analysis platform for natural language un-
derstanding. arXiv preprint arXiv:1804.07461 , 2018. 1
[28] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Cross-domain and disentangled face manipu-
lation with 3d guidance. IEEE transactions on visualization
and computer graphics , PP, 2022. 1
[29] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. SimVLM: Simple visual language
model pretraining with weak supervision. In International
Conference on Learning Representations , 2022. 4
[30] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual
entailment: A novel task for fine-grained image understand-
ing. CoRR , abs/1901.06706, 2019. 3
[31] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. In Qiong Liu, Rainer Lienhart, Haohong Wang, Sheng-
Wei ”Kuan-Ta” Chen, Susanne Boll, Yi-Ping Phoebe Chen,
Gerald Friedland, Jia Li, and Shuicheng Yan, editors, Pro-
ceedings of the 2017 ACM on Multimedia Conference, MM
2017, Mountain View, CA, USA, October 23-27, 2017 , pages
1645–1653. ACM, 2017. 2, 3
[32] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A
large video description dataset for bridging video and lan-
guage. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June
27-30, 2016 , pages 5288–5296. IEEE Computer Society,
2016. 3
[33] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Just ask: Learning to answer questions
from millions of narrated videos. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 1686–1697, 2021. 3[34] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
Pengchuan Zhang. Florence: A new foundation model for
computer vision. CoRR , abs/2111.11432, 2021. 1
[35] Siyang Yuan, Pengyu Cheng, Ruiyi Zhang, Weituo Hao, Zhe
Gan, and Lawrence Carin. Improving zero-shot voice style
transfer via disentangled representation learning. In Interna-
tional Conference on Learning Representations , 2021. 1
[36] Minghao Zhang, Pingcheng Jian, Yi Wu, Huazhe Xu, and
Xiaolong Wang. Dair: Disentangled attention intrinsic regu-
larization for safe and efficient bimanual manipulation. arXiv
preprint arXiv:2106.05907 , 2021. 3
[37] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 5579–
5588, 2021. 4

--- PAGE 7 ---
A. Visualizations
(a) Question Image
(b) Token visualization (Baseline)
(c) Token visualization (Ours)
Figure A.1. Question : what color are the straps on the san-
dal? Baseline : pink sugar, Ours : pink green ,Ground truth an-
swers = [‘pink green’ , ‘pink green’ , ‘slipper’, ‘pink green’ , ‘green
pink’, ‘pink mint green’, ‘green pink’, ‘pink green’ , ‘green pink’,
‘green’]; Bottom left : Weights assigned to each image patch for ev-
ery token, lighter shades like yellow correspond to higher weights;
Bottom right : Token attention masks grounded to the input image.
(a) Question Image
(b) Token visualization (Baseline)
(c) Token visualization (Ours)
Figure A.2. (a) Question : what is this? Baseline : hot baby dar1,
Ours : hot chocolate mix ,Ground truth answers = [’cocoa mix’,
’hot cocoa’, ’hot chocolate mix’ , ’choco hot’, ’choco hot’, ’choco
hot’, ’hot cocoa’, ’hot chocolate’, ’hot chocolate’, ’hot cocoa’];
Bottom left : Weights assigned to each image patch for every token,
lighter shades like yellow correspond to higher weights; Bottom
right : Token attention masks grounded to the input image.
