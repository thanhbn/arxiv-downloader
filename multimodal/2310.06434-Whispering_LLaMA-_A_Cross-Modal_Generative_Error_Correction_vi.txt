# Whispering LLaMA: Một Khung Sửa Lỗi Sinh Tạo Đa Phương Thức
# cho Nhận Dạng Giọng Nói
Srijith Radhakrishnan1,2,5, Chao-Han Huck Yang1,3, Sumeer Ahmad Khan1,5, Rohit
Kumar1, Narsis A. Kiani4, David Gomez-Cabrero1, Jesper N. Tegner1,4
1Đại học Khoa học và Công nghệ Vua Abdullah2Viện Công nghệ Manipal
3Viện Công nghệ Georgia4Viện Karolinska
5Trung tâm Xuất sắc về Khoa học Dữ liệu và Trí tuệ Nhân tạo SDAIA-KAUST
srijithrkr@gmail.com; huckiyang@gatech.edu

## Tóm tắt
Chúng tôi giới thiệu một kỹ thuật fusion đa phương thức mới được thiết kế cho việc sửa lỗi sinh tạo trong nhận dạng giọng nói tự động (ASR). Phương pháp của chúng tôi tận dụng cả thông tin âm thanh và các biểu diễn ngôn ngữ bên ngoài để tạo ra các ngữ cảnh phiên âm giọng nói chính xác. Đây là một bước tiến hướng tới một mô hình mới trong việc sửa lỗi sinh tạo trong lĩnh vực các giả thuyết n-best. Khác với các phương pháp tái xếp hạng dựa trên ranking hiện có, cách tiếp cận của chúng tôi khéo léo sử dụng các kỹ thuật khởi tạo riêng biệt và các thuật toán hiệu quả về tham số để cải thiện hiệu suất ASR xuất phát từ các mô hình giọng nói và văn bản được huấn luyện trước. Thông qua đánh giá trên các bộ dữ liệu ASR đa dạng, chúng tôi đánh giá kỹ thuật fusion của mình, thể hiện cải thiện 37.66% về tỷ lệ lỗi từ (WER) so với hiệu suất tương đối của n-best Oracle. Để khuyến khích nghiên cứu tương lai, chúng tôi đã công khai mã nguồn và các mô hình được huấn luyện trước tại: https://github.com/Srijith-rkr/Whispering-LLaMA.

## 1 Giới thiệu
Các mô hình giọng nói được huấn luyện đầu cuối (E2E) đã chứng minh hiệu suất tiên tiến trong các nhiệm vụ Nhận dạng giọng nói tự động (ASR). Một số phương pháp (Xia et al., 2017; Guo et al., 2019; Hu et al., 2021b; Yang et al., 2021a; Salazar et al., 2020) đã áp dụng rộng rãi mô hình rescoring hai giai đoạn để tận dụng các mô hình ngôn ngữ nhằm nâng cao thêm khả năng của những mô hình này. Trong mô hình hai giai đoạn, hệ thống ASR giai đoạn đầu "tạo ra" các giả thuyết n-best sử dụng mô hình âm thanh E2E, trong khi giai đoạn thứ hai "tái xếp hạng" những giả thuyết này bằng cách kết hợp một mô hình ngôn ngữ (LM).

Cách tiếp cận tái xếp hạng hai giai đoạn này có một số ưu điểm đáng chú ý so với các hệ thống ASR Đầu cuối (E2E) một giai đoạn (Amodei et al., 2016; Chan et al., 2016). Thứ nhất, mô hình ngôn ngữ lớn tiếp theo thường nắm bắt được sự hiểu biết toàn diện hơn (Stooke et al., 2023; Tur và De Mori, 2011) về cấu trúc ngôn ngữ vượt ra ngoài kiến thức của âm thanh được phiên âm có trong dữ liệu huấn luyện trước của mô hình ASR, từ đó cải thiện hiệu suất trên các từ chưa thấy. Hơn nữa, việc điều chỉnh mô hình hai giai đoạn để phù hợp với sự thay đổi miền (Li et al., 2023; Liu et al., 2021; Yu et al., 2023) dễ dàng hơn nhiều vì chỉ cần mô hình ngôn ngữ cần được tinh chỉnh trên bộ dữ liệu mới. Điều này giảm bớt nhu cầu về corpus phiên âm được nói, điều này có thể đặc biệt có lợi cho các ngôn ngữ nói thiếu tài nguyên hoặc đang bị đe dọa.

Sự xuất hiện gần đây của khả năng đối thoại trong các mô hình ngôn ngữ lớn, như ChatGPT (OpenAI, 2023a) và GPT-4 (OpenAI, 2023b), đã thúc đẩy thêm sự quan tâm đến việc tận dụng sức mạnh biểu diễn của các mô hình lớn được huấn luyện trước cho các nhiệm vụ phức tạp hơn liên quan đến các phương thức dữ liệu đa dạng (Yang et al., 2021b; Chang et al., 2023). Hơn nữa, hướng nghiên cứu mới này cũng giới thiệu một tập hợp các thách thức độc đáo liên quan đến việc xem xét thông tin từ các phương thức đầu vào khác, như điều kiện âm thanh và thị giác (Peng et al., 2023; Zhang et al., 2023), trong đó có thể làm giàu việc sử dụng ngữ cảnh vượt ra ngoài đầu vào chỉ có văn bản.

Nhận dạng tín hiệu giọng nói là một nhiệm vụ đòi hỏi cả thông tin âm thanh (Hu et al., 2021a; Hung et al., 2023) (ví dụ, môi trường nói) và thông tin ngôn ngữ (Meng et al., 2023; Chen et al., 2023b,c) (ví dụ, ngữ cảnh và miền). Việc kết hợp hoặc tích hợp hiệu quả việc học biểu diễn từ mô hình hóa âm thanh vào mô hình hóa ngôn ngữ để tăng cường hiệu suất của nó đại diện cho một miền nghiên cứu đặc biệt phức tạp đáng được khám phá thêm. Trong bài báo này, chúng tôi trình bày một khung fusion cấp token, kết hợp hai mô hình nền tảng (được huấn luyện trước quy mô lớn) thành một mô hình sửa lỗi nhận dạng, với mục tiêu nâng cao hiệu suất của các hệ thống ASR.

## 2 Công trình Liên quan về Xử lý Hậu ASR
Các mô hình ngôn ngữ dựa trên Transformer (Shin et al., 2019; Salazar et al., 2020) tiếp cận mô hình hai giai đoạn bằng cách sử dụng tổng của log-likelihoods âm của các token riêng lẻ từ mô hình ngôn ngữ để tái chấm điểm đầu ra n-best. Các công trình gần đây về phương pháp deliberation (Hu et al., 2020; Prabhavalkar et al., 2018) và rescoring dựa trên audio-attention (Futami et al., 2021; Gandhe và Rastrow, 2020; Tanaka et al., 2021) trong việc cải thiện rescoring ASR-LM với việc kết hợp các đặc trưng âm thanh. Các công trình gần đây về prompting decoder (Yang et al., 2023a) và sửa lỗi dựa trên encoder-decoder (Chen et al., 2023a; Ma et al., 2023) đã chứng minh lợi ích trong việc sử dụng mô hình ngôn ngữ bên ngoài để giảm tỷ lệ lỗi phiên âm. Trong khi đó, cách tiêm hoặc fusion các biểu diễn từ một mô hình âm thanh lớn vào một mô hình ngôn ngữ khác vẫn đang được nghiên cứu.

## 3 Phương pháp
Chúng tôi thảo luận về kiến trúc mô hình và trực giác đằng sau sự kết hợp đặc trưng được đề xuất trong Phần 3.1. Cơ chế fusion đa phương thức và khởi tạo trọng số được giải thích trong Phần 3.2 và Phần 3.3, tương ứng.

### 3.1 Sửa Lỗi Sinh Tạo cho ASR
Cách tiếp cận của chúng tôi kết hợp hai mô hình được huấn luyện trước, Whisper (Radford et al., 2022) và LLaMA (Touvron et al., 2023), để tạo điều kiện cho việc sửa lỗi sinh tạo (Yang et al., 2023a; Chen et al., 2023a). Thứ nhất, chúng tôi sử dụng Whisper, một mô hình giọng nói transformer đa nhiệm vụ dựa trên encoder-decoder được huấn luyện trên 680,000 giờ dữ liệu đa ngôn ngữ, để mã hóa các biểu diễn âm thanh và tạo ra các phiên âm của các giả thuyết n-best. Thứ hai, chúng tôi sử dụng LLaMA, một mô hình transformer ngôn ngữ lớn dựa trên decoder để tạo ra các phiên âm được sửa lỗi bằng cách sử dụng các giả thuyết n-best thông qua prompt (được minh họa trong Phụ lục, Hình 5) và các biểu diễn âm thanh thông qua khung được đề xuất của chúng tôi làm đầu vào.

Whisper sử dụng encoder của mô hình Transformer để rút ra các đặc trưng từ đầu vào âm thanh, sau đó được đưa vào decoder thông qua cross-attention đa đầu, cho phép dự đoán token văn bản tự hồi quy (Wang et al., 2023; Irie et al., 2022). Các đặc trưng được mã hóa cung cấp thông tin từ đầu vào âm thanh thông qua cross-attention, trong khi self-attention của decoder chú ý đến các token trước đó sử dụng cơ chế key-value caching.

Chúng tôi fusion các đặc trưng âm thanh và các lớp tuyến tính của Whisper tạo ra các cặp key và value trong cơ chế cross-attention của decoder với mô hình LLaMA để tiêm thông tin âm thanh. Các module self-attention vốn có trong LLaMA kết hợp với module cross-attention được thêm vào làm cho nó tương tự như decoder của Whisper. Tổng quan về phương pháp đề xuất được trình bày trong Phụ lục, Hình 2.

### 3.2 Cơ chế Fusion Đa Phương thức
Chúng tôi giới thiệu cơ chế của mình trong Hình 1. Để tinh chỉnh hiệu quả các mô hình lớn, chúng tôi kết hợp hai module adapter dư (Houlsby et al., 2019; Radhakrishnan et al., 2023; Chen et al., 2023d; Yang et al., 2023b) (A^i_L và A^i_W) sau các module self-attention (SA^i_L) của mô hình LLaMA đông lạnh ở mỗi lớp. Biến đầu tiên A^i_L đại diện cho adapter trong lớp i được sử dụng để tinh chỉnh mô hình LLaMA sử dụng cơ chế scaled dot product attention. Biến thứ hai A^i_W đề cập đến một adapter khác trong lớp i được sử dụng để fusion các đặc trưng Whisper với mô hình LLaMA bằng cách tuân theo cơ chế autoencoder.

Trong mỗi A^i_L, chúng tôi kết hợp một ma trận có thể học M^i_θ ∈ R^{N_θ×N_L}. N_θ biểu thị chiều của các embedding adapter, trong khi N_L chỉ ra chiều của các embedding LLaMA. Đặc trưng embedding ngôn ngữ được trích xuất từ LLM được huấn luyện trước được biểu diễn bởi H^i_L cho mỗi lớp.

Chúng tôi tái sử dụng các lớp tuyến tính LLaMA đông lạnh K^i_llama và L^i_llama từ self-attention SA^i_L của LLaMA để biến đổi M^i_θ thành các cặp key và value, do đó giảm số lượng tham số có thể huấn luyện. Chúng tôi cũng tái sử dụng tensor query từ module self-attention LLaMA đông lạnh SA^i_L để tính toán A^i_L, như được hiển thị bên dưới; S biểu thị Softmax:

S(Q^i_llama(H^i_L)·K^i_llama(M^i_θ)^T/√d_k) V^i_llama(M^i_θ)   (1)

Để tích hợp các biểu diễn âm thanh và các tensor key-value từ module cross-attention decoder của Whisper vào mô hình LLaMA, chúng tôi giới thiệu hai phép biến đổi tuyến tính đông lạnh bổ sung (K^i_whisper và V^i_whisper) ở mỗi lớp của mô hình LLaMA. Những phép biến đổi này được khởi tạo với các trọng số tương ứng từ module cross-attention của decoder Whisper. Bằng cách áp dụng các biểu diễn âm thanh cho những phép biến đổi tuyến tính bổ sung này, chúng tôi tạo ra các cặp key-value phản ánh những cặp được tạo ra bởi Whisper.

Sau đó chúng tôi sử dụng module adapter thứ hai A^i_W, để thêm các thành phần có thể huấn luyện để học biểu diễn đa phương thức. Chúng tôi áp dụng một ma trận chiếu có thể học M^i_down ∈ R^{N_W×N_W/r} để down project các cặp key và value thu được. Trong đó N_W biểu thị kích thước của các biểu diễn âm thanh được mã hóa bởi Whisper (x). Sau đó chúng tôi áp dụng hàm kích hoạt SiLU (Elfwing et al., 2018) theo sau bởi một up-projection có thể học M^i_up ∈ R^{N_W/r×N_W}, để tính toán đầu ra có thể huấn luyện:

A^i_W(x) ← SiLU(x·M^i_down)M^i_up.   (2)

Sử dụng thiết lập này, chúng tôi biến đổi cặp key-value ở mỗi lớp, kết hợp biểu diễn ẩn (H_audio) từ đầu ra của encoder Whisper được huấn luyện trước đông lạnh với decoder từ LLaMA:

K̂^i_whisper ← A^i_W(K^i_whisper(H_audio));   (3)
V̂^i_whisper ← A^i_W(V^i_whisper(H_audio)).   (4)

Khi chúng tôi thu được các cặp key và value Whisper tương ứng, chúng tôi áp dụng cơ chế padding được mô tả trong 3.3 để điều chỉnh hình dạng của K̂^i_whisper và V̂^i_whisper để phù hợp với Q^i_llama để cho phép tính toán Multi Head Attention (MHA) và để bảo toàn cấu trúc tiềm ẩn của Key và Value embeddings của Whisper. Chúng tôi kết hợp nó với mô hình LLaMA bằng cách tính toán MHA với query (Q^i_llama) từ mô hình LLaMA đông lạnh như trước để có được đầu self-attention có thể điều chỉnh của nó (SA^i_W) như sau:

S(Q^i_llama(H^i_L)·K̂^i_whisper^T/√d_k) V̂^i_whisper   (5)

Sau đó, chúng tôi sử dụng cơ chế fusion có cổng, Whispering-LLaMA (WL), để fusion tất cả các module lại với nhau như hiển thị bên dưới:

SA^i_WL ← SA^i_L + λ_L·A^i_L + λ_W·SA^i_W,   (6)

trong đó λ_L và λ_W là các scalar có thể học.

### 3.3 Khởi tạo Trọng số
Các chiều tiềm ẩn của các mô hình Whisper và LLaMA khác nhau, làm cho việc định hình lại các tensor Whisper để phù hợp với hình dạng của mô hình LLaMA trong khi vẫn bảo toàn cấu trúc tiềm ẩn và thông tin vốn có trong mô hình Whisper trở nên cần thiết. Các tensor được định hình theo định dạng [B, N_H, T, H_S] biểu thị Batch size, Number of heads, context length và Head Size, tương ứng. Hai chiều cuối cùng trải qua biến đổi trong cơ chế attention. Do đó để bảo toàn cấu trúc tiềm ẩn của Whisper, chúng tôi khởi tạo một ma trận số không có hình dạng ∈ R^{N_H^llama×T^whisper×H_S^llama} và điền đường chéo chính của hai chiều cuối cùng với số một. Sau đó chúng tôi đặt K^i và V^i trên góc trên bên trái của template padding. Chúng tôi khởi tạo thêm các ma trận chiếu M^i_down, M^i_up trên module adapter thứ hai A^i_W như các ma trận đơn vị. Khung được đề xuất gặp phải những tổn thất đáng kể và không thể hội tụ trừ khi chiến lược khởi tạo này được tuân theo để bảo toàn các biểu diễn tiềm ẩn của Whisper.

## 4 Thiết lập Thực nghiệm

### 4.1 Các Mô hình
Cho các thí nghiệm của chúng tôi, chúng tôi sử dụng kiến trúc mô hình LLaMA-7B. Vì chúng tôi hướng dẫn mô hình ngôn ngữ với các giả thuyết được tạo ra (như được minh họa trong Hình 4.3.1) để thực hiện sửa lỗi sinh tạo, chúng tôi khởi tạo trọng số mô hình của mình với Alpaca (Taori et al., 2023), một mô hình được tinh chỉnh từ LLaMA-7B, sử dụng 52,000 demonstration tuân theo hướng dẫn để cho phép khả năng tuân theo hướng dẫn. Để trích xuất các biểu diễn âm thanh từ các clip âm thanh đầu vào, chúng tôi sử dụng Whisper-Large V2, một mô hình với 1.55B tham số được huấn luyện trên 620,000 giờ dữ liệu âm thanh. Ngoài ra, chúng tôi sử dụng Whisper-Tiny, một mô hình với 70M tham số, để tạo ra các phiên âm của chúng tôi, như được mô tả trong phần tiếp theo 4.2. Chúng tôi đặt tên mô hình của mình là Whispering LLaMA (WL) và huấn luyện ba biến thể với khung được đề xuất của chúng tôi với N_θ = 10 và r = 8, 16, 32 được đặt tên là WL_L (large), WL_M (medium), WL_S (small), tương ứng. Chúng tôi thiết kế WL_L với hai module adapter A_W riêng biệt cho key và value, tương ứng. WL_M và WL_S sử dụng cùng một adapter A_W trong phần 3.2 để giảm tham số có thể huấn luyện.

### 4.2 Bộ dữ liệu
Chúng tôi tuyển chọn các phiên âm riêng của mình bằng cách tận dụng hai bộ dữ liệu: Airline Travel Information System (Hemphill et al., 1990) (ATIS) và GigaSpeech (Chen et al., 2021). ATIS bao gồm các bản ghi âm thanh của các cá nhân truy vấn thông tin chuyến bay. GigaSpeech, chứa âm thanh từ audiobook, podcast và video YouTube về các chủ đề đa dạng. ATIS đại diện cho một bộ dữ liệu đúng về mặt ngữ nghĩa, dành riêng cho miền, trong khi GigaSpeech đại diện cho một thiết lập thực tế nhiều tiếng ồn hơn trong đánh giá của chúng tôi. Chúng tôi chọn các tập con dành riêng cho miền trong GigaSpeech và tập trung vào ba danh mục cụ thể: Entertainment, People and Blogs, và Science and Technology. Để khám phá các biến thể hiệu suất liên quan đến số lượng điểm dữ liệu, chúng tôi chia thêm danh mục Science and Technology thành hai tập con. Bảng 1 cung cấp thông tin chi tiết về số lượng điểm huấn luyện cho mỗi bộ dữ liệu.

Chúng tôi chọn Whisper-Tiny để tạo ra baseline giả thuyết n-best để thiết lập một môi trường đánh giá mạnh mẽ phù hợp hơn với các thiết lập thực tế xử lý các giả thuyết không tối ưu. Bằng cách sử dụng Whisper-Tiny, chúng tôi mô phỏng một mô hình âm thanh yếu với các giả thuyết chất lượng thấp hơn. Việc cung cấp cho LM các giả thuyết chất lượng tốt hơn từ Whisper-Large sẽ làm cho nhiệm vụ sửa lỗi sinh tạo ít thách thức hơn cho việc điều chỉnh LM và không khám phá hiệu suất của mô hình trong các thiết lập thực tế nơi phương pháp của chúng tôi được dự định sử dụng. Tuy nhiên, chúng tôi nhấn mạnh rằng phương pháp của chúng tôi vẫn hiệu quả khi bắt đầu với giả thuyết Whisper-Large trong Phụ lục E.

Cho mỗi clip âm thanh, chúng tôi tạo ra 200 giả thuyết sử dụng giá trị top-k là 200 và một nhiệt độ được chọn ngẫu nhiên trong khoảng [0.7, 0.8]. Tiếp theo, chúng tôi lọc ra các câu dư thừa và chọn top-15 của nó với xác suất log cao nhất.

### 4.3 Pipeline Huấn luyện
Đầu vào cho mô hình của chúng tôi bao gồm các biểu diễn âm thanh được mã hóa được trích xuất từ mô hình Whisper-Large, kèm theo các phiên âm 15-best được tạo ra bởi Whisper-Tiny. Chúng tôi sử dụng template prompt được sử dụng bởi mô hình Alpaca như được hiển thị trong Phụ lục Hình 5. Chúng tôi sử dụng bộ tối ưu hóa Adam (Kingma và Ba, 2014) và thí nghiệm với tỷ lệ học 1×10^-2, 1×10^-3, và 5×10^-4, chọn giá trị tối ưu. Mô hình được huấn luyện trong 25 epoch, sử dụng early stopping để ngăn chặn overfitting. Huấn luyện được tiến hành trên hai GPU Nvidia A100 để tận dụng xử lý song song hiệu quả. Kích thước batch hiệu quả là 32 được sử dụng, và weight decay 1×10^-2 được áp dụng.

#### 4.3.1 Ví dụ Prompting LLM cho ASR
Chúng tôi sử dụng template prompt Alpaca (Taori et al., 2023), như được minh họa trong Hình 5 của Phụ lục, để tạo ra các giả thuyết n-best. Template này có một phần hướng dẫn được chỉ định bởi thẻ Instruction, cung cấp hướng dẫn cho mô hình. Dữ liệu ngữ cảnh thiết yếu mà mô hình yêu cầu được đặt dưới thẻ Input. Prompt kết thúc với thẻ Response, hướng dẫn mô hình thực hiện hướng dẫn được chỉ định trong ngữ cảnh đầu vào được cung cấp. Thay vì áp dụng những tiến bộ gần đây của Task-Activating Prompting (Yang et al., 2023a) (TAP), chúng tôi chọn cung cấp cho LLM dữ liệu dành riêng cho nhiệm vụ của nó (ví dụ, nhận dạng giọng nói trong trường hợp của chúng tôi). Cách tiếp cận thay thế của chúng tôi tạo điều kiện cho việc sửa lỗi giai đoạn thứ hai, giảm thiểu các vấn đề về độ trễ được quan sát trong các cửa sổ ngữ cảnh mở rộng của việc sửa lỗi ASR sinh tạo dựa trên TAP.

### 4.4 Nghiên cứu Hiệu suất
Kết quả từ các thí nghiệm của chúng tôi đã được báo cáo trong Bảng 2. Mô hình WL_M đạt được hiệu suất tốt nhất với tỷ lệ lỗi từ tương đối (WERR) là 37.66%, như được định nghĩa trong B.2. Một so sánh giữa WL_L và WL_M cho thấy rằng việc có các module adapter riêng biệt cho các cặp key và value KHÔNG dẫn đến cải thiện hiệu suất. Các phân tích cụ thể theo bộ dữ liệu được trình bày chi tiết trong Phụ lục B. Các mô hình thể hiện hiệu suất tốt hơn trên Gigaspeech với nhiều dữ liệu trong miền hơn.

### 4.5 Nghiên cứu Ablation
Chúng tôi khám phá thực nghiệm rằng việc che khuất prompt ngoại trừ ground truth trong hàm cross entropy loss cải thiện đáng kể hiệu suất. Chúng tôi quy cải thiện này cho khả năng nâng cao của mô hình để nắm bắt ngữ nghĩa chính xác, đạt được bằng cách kiềm chế việc phạt mô hình vì các câu sai được tìm thấy trong các giả thuyết n-best. Hàng 5 đại diện cho hiệu suất của WL_M mà không có masking. Chúng tôi điều tra thêm xem khung được đề xuất có sử dụng các biểu diễn âm thanh từ Whisper bằng cách thay thế chúng bằng các tensor ngẫu nhiên được tạo ra từ phân phối chuẩn làm đầu vào (Hàng 6). Ngoài ra, chúng tôi khám phá tầm quan trọng của cơ chế khởi tạo trọng số của chúng tôi bằng cách thay thế nó bằng khởi tạo ngẫu nhiên (Hàng 7). Cả hai nghiên cứu ablation này đều xác nhận trực giác của chúng tôi, chứng minh rằng phương pháp sử dụng các đặc trưng âm thanh một cách hiệu quả và làm nổi bật tầm quan trọng của cơ chế khởi tạo trong việc bảo toàn cấu trúc tiềm ẩn của các embedding âm thanh. Để có thêm thông tin chi tiết, vui lòng tham khảo Phụ lục D. Chúng tôi cũng loại bỏ module adapter Whisper (SA_W) cho hiệu suất baseline chỉ có đặc trưng văn bản sử dụng adapter (Hàng 8). Vì sự chênh lệch giữa số lượng tham số có thể huấn luyện là cao, chúng tôi huấn luyện một mô hình khác với chiều ngữ cảnh adapter tăng N'_θ = 4N_θ (Hàng 9).

## 5 Kết luận
Chúng tôi đề xuất một khung mới để tận dụng kiến thức bên ngoài từ LLM để cải thiện độ chính xác phiên âm của các hệ thống ASR. Khung của chúng tôi trình bày một cách hiệu quả về tham số để tích hợp các mô hình Giọng nói và Ngôn ngữ nền tảng lớn để đạt được cải thiện WERR cạnh tranh. Chúng tôi tiến hành thêm các thí nghiệm ablation mở rộng để xác nhận trực giác của mình và mở mã nguồn và trọng số được huấn luyện trước của chúng tôi cho cộng đồng nghiên cứu.

## 6 Hạn chế
Việc sử dụng các mô hình lớn như LLaMA là trực quan, vì nó cung cấp sự hiểu biết toàn diện về cấu trúc ngôn ngữ nhờ vào việc huấn luyện trước quy mô internet của nó. Tuy nhiên, việc triển khai các hệ thống này và tiến hành nghiên cứu với chúng trong các kịch bản thực tế là thách thức do tính chất tính toán chuyên sâu của chúng. Trong cách tiếp cận của chúng tôi, chúng tôi nhắm đến việc thiết kế khung của mình để hiệu quả về tham số bằng cách tái sử dụng nhiều thành phần mô hình với adapter cho fusion mô hình. Tuy nhiên, việc kết hợp các biểu diễn âm thanh vào pipeline huấn luyện kéo dài thời gian huấn luyện thêm 394.76%. Điều này nhấn mạnh tầm quan trọng của các vấn đề alignment (Yen et al., 2023). Hơn nữa, giải pháp được đề xuất của chúng tôi chứng minh nhu cầu về một khối lượng dữ liệu lớn hơn để đạt được hiệu suất tối ưu, mặc dù có số lượng tham số khiêm tốn chỉ 7.97M để tích hợp các mô hình nền tảng. Trong quá trình thử nghiệm của chúng tôi, chúng tôi gặp phải các vấn đề liên quan đến over-fitting trên các bộ dữ liệu. Để giảm thiểu vấn đề này, chúng tôi huấn luyện với tỷ lệ học giảm và theo dõi hiệu suất Word Error Rate (WER) trong suốt quá trình huấn luyện và chọn checkpoint mô hình với hiệu suất tốt nhất để thực hiện early stopping.
