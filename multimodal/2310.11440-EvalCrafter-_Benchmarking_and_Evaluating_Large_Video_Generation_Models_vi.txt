# 2310.11440.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.11440.pdf
# Kích thước tệp: 45231416 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
EvalCrafter: Đánh Giá và So Sánh Các Mô Hình Tạo Video Lớn
Yaofang Liu1,2,*Xiaodong Cun1,*Xuebo Liu3Xintao Wang1
Yong Zhang1Haoxin Chen1Yang Liu4†Tieyong Zeng4Raymond Chan2†Ying Shan1
1Tencent AI Lab2Đại học Thành phố Hồng Kông3Đại học Macau
4Đại học Trung văn Hồng Kông
Trang dự án: http://evalcrafter.github.io
Tóm tắt
Các mô hình tạo sinh cho thị giác và ngôn ngữ đã phát triển vượt bậc trong những năm gần đây. Đối với việc tạo video, nhiều mô hình mã nguồn mở và dịch vụ công khai đã được phát triển để tạo ra các video chất lượng cao. Tuy nhiên, các phương pháp này thường chỉ sử dụng một vài chỉ số, ví dụ như FVD [57] hoặc IS [46], để đánh giá hiệu suất. Chúng tôi cho rằng việc đánh giá các mô hình tạo sinh có điều kiện lớn từ các chỉ số đơn giản là khó khăn vì những mô hình này thường được huấn luyện trên các tập dữ liệu rất lớn với khả năng đa khía cạnh. Do đó, chúng tôi đề xuất một khung và quy trình mới để đánh giá toàn diện hiệu suất của các video được tạo ra. Phương pháp của chúng tôi bao gồm việc tạo ra một danh sách đa dạng và toàn diện gồm 700 lời nhắc cho việc tạo video từ văn bản, dựa trên phân tích dữ liệu người dùng thực tế và được tạo ra với sự hỗ trợ của một mô hình ngôn ngữ lớn. Sau đó, chúng tôi đánh giá các mô hình tạo video tiên tiến nhất trên điểm chuẩn được thiết kế cẩn thận của chúng tôi, xét về chất lượng hình ảnh, chất lượng nội dung, chất lượng chuyển động, và sự liên kết văn bản-video với 17 chỉ số khách quan được chọn lọc kỹ lưỡng. Để có được bảng xếp hạng cuối cùng của các mô hình, chúng tôi tiếp tục khớp một loạt hệ số để căn chỉnh các chỉ số khách quan với ý kiến của người dùng. Dựa trên phương pháp căn chỉnh con người được đề xuất, điểm số cuối cùng của chúng tôi cho thấy mối tương quan cao hơn so với việc đơn giản lấy trung bình các chỉ số, chứng minh tính hiệu quả của phương pháp đánh giá được đề xuất.

1. Giới thiệu
Sức hấp dẫn của các mô hình tạo sinh lớn đang lan tỏa khắp thế giới, ví dụ như ChatGPT và GPT4 [38] nổi tiếng đã cho thấy khả năng ngang tầm con người trong nhiều khía cạnh, bao gồm lập trình, giải toán, và thậm chí hiểu biết thị giác, có thể được sử dụng để tương tác với con người sử dụng bất kỳ kiến thức nào theo cách đối thoại. Đối với các mô hình tạo sinh cho việc tạo nội dung thị giác, Stable Diffusion (SD) [44] và SDXL [41] đóng vai trò rất quan trọng vì chúng là những mô hình mạnh mẽ nhất có sẵn công khai có thể tạo ra hình ảnh chất lượng cao từ bất kỳ lời nhắc văn bản nào.

Ngoài text-to-image (T2I), việc điều khiển mô hình diffusion cho tạo video cũng đã tiến bộ nhanh chóng. Các công trình đầu tiên (Imagen-Video [23], Make-A-Video [50]) sử dụng trực tiếp các mô hình xếp tầng cho việc tạo video. Được hỗ trợ bởi các kiến thức trước về tạo hình ảnh trong SD, LVDM [21] và MagicVideo [70] đã được đề xuất để huấn luyện các lớp thời gian để tạo video hiệu quả. Ngoài các bài báo học thuật, một số dịch vụ thương mại cũng có thể tạo video từ văn bản hoặc hình ảnh, ví dụ như Gen2 [18] và PikaLabs [5]. Mặc dù chúng ta không thể có được chi tiết kỹ thuật của những dịch vụ này, chúng không được đánh giá và so sánh với các phương pháp khác.

Tuy nhiên, tất cả các mô hình text-to-video (T2V) lớn hiện tại chỉ sử dụng các chỉ số dựa trên GAN trước đây như FVD [57] để đánh giá, chỉ quan tâm đến việc khớp phân phối giữa video được tạo và video thực, chứ không phải cặp giữa lời nhắc văn bản và video được tạo. Khác biệt với điều đó, chúng tôi cho rằng một phương pháp đánh giá tốt nên xem xét các chỉ số trong các khía cạnh khác nhau, ví dụ như chất lượng chuyển động và tính nhất quán thời gian. Ngoài ra, tương tự như các mô hình ngôn ngữ lớn (LLMs), một số mô hình không có sẵn công khai và chúng ta chỉ có thể tiếp cận các video được tạo ra, điều này càng làm tăng khó khăn trong việc đánh giá. Mặc dù việc đánh giá đã tiến bộ nhanh chóng trong các mô hình tạo sinh lớn, bao gồm các lĩnh vực LLM [38], MLLM [33], và T2I [26], vẫn khó để sử dụng trực tiếp những phương pháp này cho việc tạo video. Vấn đề chính ở đây là khác với việc đánh giá T2I hoặc đối thoại, chuyển động và tính nhất quán rất quan trọng đối với việc tạo video mà các công trình trước đây bỏ qua.

Chúng tôi thực hiện bước đầu tiên để đánh giá các mô hình T2V tổng quát. Cụ thể, chúng tôi đầu tiên xây dựng một danh sách lời nhắc toàn diện chứa các đối tượng, thuộc tính và chuyển động hàng ngày khác nhau. Để đạt được phân phối cân bằng của các khái niệm nổi tiếng, chúng tôi bắt đầu từ các loại meta được định nghĩa rõ ràng của kiến thức thế giới thực và sử dụng kiến thức của LLM, tức là ChatGPT [38], để mở rộng meta-prompt của chúng tôi thành một phạm vi rộng. Bên cạnh các lời nhắc được tạo bởi mô hình, chúng tôi cũng chọn các lời nhắc từ người dùng thế giới thực và lời nhắc T2I. Sau đó, chúng tôi thu được siêu dữ liệu (ví dụ: màu sắc, kích thước, v.v.) từ lời nhắc để đánh giá thêm. Thứ hai, chúng tôi đánh giá hiệu suất của các mô hình T2V lớn từ bốn khía cạnh, tức là chất lượng video, căn chỉnh văn bản-video, chất lượng chuyển động, và tính nhất quán thời gian. Đối với mỗi khía cạnh, chúng tôi sử dụng nhiều chỉ số khách quan làm biện pháp đánh giá, và chúng tôi thực hiện một nghiên cứu người dùng để có điểm số con người đối với bốn khía cạnh này. Sau đó, chúng tôi huấn luyện các hệ số của mô hình hồi quy cho mỗi khía cạnh, căn chỉnh điểm đánh giá với sở thích của người dùng. Điều này cho phép chúng tôi có được điểm số mô hình cuối cùng và đánh giá các video mới bằng cách sử dụng các hệ số đã được huấn luyện.

Nhìn chung, chúng tôi tóm tắt đóng góp của bài báo như sau:
• Chúng tôi thực hiện bước đầu tiên đánh giá mô hình T2V lớn và xây dựng một danh sách lời nhắc toàn diện với các chú thích chi tiết cho việc đánh giá T2V.
• Chúng tôi xem xét các khía cạnh về chất lượng hình ảnh video, chất lượng chuyển động video, tính nhất quán thời gian video, và căn chỉnh văn bản-video để đánh giá việc tạo video. Đối với mỗi khía cạnh, chúng tôi căn chỉnh ý kiến của con người và cũng xác minh tính hiệu quả của chỉ số được đề xuất bằng phân tích tương quan.
• Trong quá trình đánh giá, chúng tôi cũng thảo luận về một số kết luận và phát hiện, có thể cũng đóng góp vào sự đổi mới và phát triển thêm của các mô hình T2V.

2. Công trình liên quan

2.1. Tạo Video từ Văn bản và Đánh giá
Tạo video từ văn bản (T2V) nhằm tạo ra video từ các lời nhắc văn bản. Các công trình đầu tiên sử dụng Variational AutoEncoders (VAEs [29]) hoặc mạng đối sinh sinh tạo (GANs [20]) nhưng thường tạo ra kết quả chất lượng thấp hoặc đặc trụ theo miền, chẳng hạn như khuôn mặt [67] hoặc phong cảnh [51, 64]. Các phương pháp gần đây tận dụng những tiến bộ trong các mô hình diffusion [24, 25, 61] và việc tiền huấn luyện văn bản-hình ảnh quy mô lớn [43] để cải thiện chất lượng tạo sinh. Các ví dụ bao gồm Make-A-Video [50], Imagen-Video [23], LVDM [21], Align Your Latent [9], và MagicVideo [70]. Các thực thể thương mại và phi thương mại cũng đã thể hiện sự quan tâm đến việc tạo T2V, với các dịch vụ trực tuyến như Gen1 [18], Gen2 [18], và các mô hình mã nguồn mở như ZeroScope [6], ModelScope [58]. Các máy chủ dựa trên Discord như Pika-Lab [5] và Morph Studio [4] đã chứng minh kết quả cạnh tranh.

Tuy nhiên, một điểm chuẩn công bằng và chi tiết để đánh giá các phương pháp này vẫn đang thiếu. Các chỉ số hiện có như FVD [57], IS [46], và độ tương tự CLIP [43] có thể hoạt động tốt trên các phương pháp tạo T2I trong miền trước đây nhưng không đánh giá đầy đủ sự căn chỉnh với văn bản đầu vào, chất lượng chuyển động, và tính nhất quán thời gian, những yếu tố quan trọng đối với T2V.

2.2. Đánh giá về Các Mô hình Tạo sinh Lớn
Đánh giá các mô hình tạo sinh lớn [38, 41, 44, 55, 56] là một thử thách lớn cho cả các nhiệm vụ NLP và thị giác. Đối với LLMs, các phương pháp hiện tại thiết kế nhiều chỉ số về các khả năng khác nhau, loại câu hỏi, và nền tảng người dùng [15, 22, 62, 69, 71]. Chi tiết hơn về đánh giá LLM và đánh giá Multi-model LLM có thể được tìm thấy trong các khảo sát gần đây [11, 68]. Tương tự, việc đánh giá mô hình tạo sinh đa phương thức cũng thu hút sự chú ý của các nhà nghiên cứu [8, 63]. Ví dụ, Seed-Bench [33] tạo ra VQA cho việc đánh giá LLM đa phương thức.

Đối với các mô hình trong các nhiệm vụ tạo thị giác, Imagen [45] chỉ đánh giá mô hình thông qua nghiên cứu người dùng. DALL-Eval [14] đánh giá kỹ năng lý luận thị giác và cơ sở xã hội của mô hình T2I thông qua cả người dùng và thuật toán phát hiện đối tượng [10]. HRS-Bench [7] đề xuất một điểm chuẩn toàn diện và đáng tin cậy bằng cách tạo lời nhắc với ChatGPT [38] và sử dụng 17 chỉ số để đánh giá 13 kỹ năng của mô hình T2I. TIFA [26] đề xuất một điểm chuẩn sử dụng trả lời câu hỏi thị giác (VQA). Tuy nhiên, các phương pháp này vẫn hoạt động cho việc đánh giá T2I hoặc đánh giá mô hình ngôn ngữ. Đối với đánh giá T2V, chúng tôi tiếp tục xem xét chất lượng chuyển động và tính nhất quán thời gian.

3. Xây dựng Điểm chuẩn
Điểm chuẩn của chúng tôi nhằm tạo ra một danh sách lời nhắc đáng tin cậy để đánh giá khả năng của các mô hình T2V khác nhau một cách công bằng. Để đạt được điều này, chúng tôi đầu tiên thu thập và phân tích các lời nhắc của người dùng thế giới thực quy mô lớn. Sau đó, chúng tôi đề xuất một quy trình tự động để tạo ra một danh sách lời nhắc với tính đa dạng cao. Vì việc tạo video tốn thời gian, chúng tôi thu thập 700 lời nhắc làm phiên bản ban đầu để đánh giá với chú thích cẩn thận. Trong phần này, chúng tôi giới thiệu chi tiết về việc xây dựng điểm chuẩn của chúng tôi.

Thu thập Dữ liệu Thế giới Thực. Để hiểu rõ hơn về các loại lời nhắc chúng ta nên tạo, chúng tôi thu thập lời nhắc từ người dùng discord tạo T2V thế giới thực, bao gồm FullJourney [2] và PikaLab [5]. Tổng cộng, chúng tôi thu thập hơn 600k lời nhắc với các video tương ứng và lọc chúng thành 200k bằng cách loại bỏ các lời nhắc lặp lại và vô nghĩa.

Thông qua việc phân tích dữ liệu thu thập được bao gồm các khía cạnh như độ dài lời nhắc và tần suất từ, chúng tôi biết được rằng hầu hết các lời nhắc chứa từ 3 đến 40 từ. Bên cạnh đó, chúng tôi xác định bốn lớp meta-subject cho việc tạo T2V: con người, động vật, đối tượng, và phong cảnh. Đối với mỗi loại, chúng tôi xem xét các chuyển động và phong cách của từng loại, mối quan hệ giữa metaclass hiện tại và các metaclass khác, và chuyển động và chuyển động camera để xây dựng điểm chuẩn. Chúng tôi cung cấp thêm chi tiết trong tài liệu bổ sung.

Tạo Lời nhắc Có thể Nhận biết Chung. Dựa trên các metaclass được xác định trong bước trước, chúng tôi tạo ra các lời nhắc có thể nhận biết với sự giúp đỡ của LLM và đầu vào của con người. Như được hiển thị trong Hình 2, đối với mỗi loại metaclass, chúng tôi yêu cầu GPT-4 [38] mô tả các cảnh về metaclass này và các thuộc tính của nó với thông tin meta được lấy mẫu ngẫu nhiên. Bằng cách này, chúng tôi có được sự thật cơ bản cho các mô hình thị giác máy tính để đánh giá. Tuy nhiên, chúng tôi thấy rằng GPT-4 không hoàn hảo cho nhiệm vụ này, vì các thuộc tính được tạo ra không rất nhất quán với mô tả được tạo ra. Do đó, chúng tôi đưa vào một việc tự kiểm tra trong quá trình xây dựng điểm chuẩn nơi chúng tôi sử dụng GPT-4 để xác định sự tương tự giữa mô tả được tạo ra và từng siêu dữ liệu. Cuối cùng, chúng tôi lọc các lời nhắc bằng chính chúng tôi để đảm bảo mỗi lời nhắc đều chính xác và có ý nghĩa cho việc tạo T2V.

Ngoài các lời nhắc được tạo tự động, chúng tôi cũng tích hợp các lời nhắc từ người dùng thế giới thực và các lời nhắc đánh giá T2I có sẵn, chẳng hạn như DALL-Eval [14] và Draw-Bench [45]. Chúng tôi lọc và tạo siêu dữ liệu bằng GPT-4, chọn các lời nhắc phù hợp với thông tin meta tương ứng như được hiển thị trong Hình 2, và kiểm tra tính nhất quán của thông tin meta.

Tổng quan Điểm chuẩn. Nhìn chung, chúng tôi có hơn 700 lời nhắc trong các metaclass của con người, động vật, đối tượng, và phong cảnh. Mỗi lớp chứa các cảnh tự nhiên, các lời nhắc được cách điệu hóa, và kết quả với các điều khiển chuyển động camera rõ ràng. Chúng tôi cung cấp cái nhìn ngắn gọn về điểm chuẩn trong Hình 3. Để tăng tính đa dạng của các lời nhắc, điểm chuẩn của chúng tôi chứa 3 loại phụ khác nhau, nơi chúng tôi có tổng cộng 50 phong cách và 20 lời nhắc chuyển động camera. Chúng tôi thêm chúng ngẫu nhiên vào 250 lời nhắc của toàn bộ điểm chuẩn. Điểm chuẩn của chúng tôi chứa độ dài trung bình 12,3 từ cho mỗi lời nhắc, tương tự với các lời nhắc thế giới thực mà chúng tôi thu thập.

4. Chỉ số Đánh giá
Khác với các chỉ số đánh giá dựa trên FID [47] trước đây, chúng tôi đánh giá các mô hình T2V trong các khía cạnh khác nhau, bao gồm chất lượng hình ảnh của video được tạo, căn chỉnh văn bản-video, chất lượng chuyển động, và tính nhất quán thời gian. Dưới đây, chúng tôi cung cấp các chỉ số chi tiết.

4.1. Đánh giá Chất lượng Video Tổng thể
Chúng tôi tập trung vào chất lượng hình ảnh của video được tạo, điều quan trọng đối với sự hấp dẫn của người dùng. Vì các phương pháp dựa trên phân phối như FVD [57] yêu cầu video sự thật cơ bản, chúng tôi cho rằng chúng không phù hợp cho các trường hợp tạo T2V tổng quát.

Đánh giá Chất lượng Video (VQA A, VQA T). Chúng tôi sử dụng phương pháp Dover [60] để đánh giá chất lượng video được tạo về mặt thẩm mỹ và kỹ thuật. Xếp hạng kỹ thuật đo lường các biến dạng phổ biến như nhiễu và tạo tác. Dover [60] được huấn luyện trên một tập dữ liệu quy mô lớn với các nhãn được xếp hạng bởi người dùng thực. Chúng tôi ký hiệu điểm thẩm mỹ và kỹ thuật lần lượt là VQA A và VQA T.

Điểm Inception (IS). Chúng tôi cũng sử dụng điểm inception [46] như một chỉ số đánh giá chất lượng video, theo các bài báo tạo T2V trước đây. Điểm inception đánh giá hiệu suất GAN [20] bằng cách sử dụng Mạng Inception [53] được tiền huấn luyện trên tập dữ liệu ImageNet [17]. Điểm inception cao hơn cho thấy nội dung được tạo ra đa dạng hơn.

4.2. Căn chỉnh Văn bản-Video
Chúng tôi đánh giá sự căn chỉnh của văn bản đầu vào và video được tạo trong các khía cạnh khác nhau, bao gồm lời nhắc văn bản toàn cục, tính chính xác nội dung, và các thuộc tính cụ thể. Chi tiết của từng điểm như sau.

Tính Nhất quán Văn bản-Video (CLIP-Score). Chúng tôi sử dụng CLIP-Score để định lượng sự khác biệt giữa lời nhắc văn bản đầu vào và video được tạo. Sử dụng mô hình CLIP ViT-B/32 [43] được tiền huấn luyện như một bộ trích xuất đặc trưng, chúng tôi thu được embedding hình ảnh theo khung và embedding văn bản, và tính toán độ tương tự cosine của chúng. CLIP-Score tổng thể sau đó được tính bằng cách lấy trung bình các điểm riêng lẻ trên tất cả các khung.

Tính Nhất quán Hình ảnh-Video (SD-Score). Chúng tôi đề xuất một chỉ số mới, SD-Score, để so sánh chất lượng được tạo với SD [44] theo khung, xem xét rằng hầu hết các mô hình diffusion video hiện tại được tinh chỉnh trên SD cơ sở với tập dữ liệu quy mô lớn hơn. Sử dụng SDXL [41], chúng tôi tạo N1 hình ảnh {dk}N1 k=1 cho mỗi lời nhắc và trích xuất embedding thị giác trong cả hình ảnh được tạo và khung video. Chúng tôi tính toán độ tương tự embedding giữa video được tạo và hình ảnh SDXL, điều này giúp giải quyết vấn đề quên khái niệm khi tinh chỉnh mô hình diffusion T2I thành mô hình video. SD-Score cuối cùng được tính như sau:

SSD=1/M ∑(i=1 to M)(1/N ∑(t=1 to N)(1/N1 ∑(k=1 to N1)C(emb(xi_t), emb(di_k))))     (1)

trong đó xi_t là khung thứ t của video thứ i, C(·,·) là hàm độ tương tự cosine, emb(·) có nghĩa là embedding CLIP, M là tổng số video thử nghiệm, và N là tổng số khung trong mỗi video, trong đó N1= 5.

Tính Nhất quán Văn bản-Văn bản (BLIP-BLEU). Chúng tôi cũng xem xét việc đánh giá giữa các mô tả văn bản được tạo của video và lời nhắc đầu vào. Chúng tôi sử dụng BLIP2 [35] để tạo chú thích và sử dụng BLEU [40] để đánh giá căn chỉnh văn bản:

SBB=1/M ∑(i=1 to M)(1/N2 ∑(k=1 to N2)B(pi, li_k)),     (2)

trong đó pi là lời nhắc thứ i, B(·,·) là hàm tính điểm độ tương tự BLEU, {li_k}N2 k=1 là các chú thích do BLIP2 tạo cho video thứ i, và N2 được đặt là 5 theo thực nghiệm.

Tính Nhất quán Đối tượng và Thuộc tính (Detection-Score, Count-Score và Color-Score). Chúng tôi sử dụng SAMTrack [13] để phân tích tính chính xác của nội dung video. Chúng tôi đánh giá các mô hình T2V về sự tồn tại của đối tượng, cũng như tính chính xác của màu sắc và số lượng đối tượng trong lời nhắc văn bản. Cụ thể, chúng tôi đánh giá Detection-Score, Count-Score, và Color-Score như sau:

1. Detection-Score (SDet): Đo lường sự hiện diện trung bình của đối tượng trên các video, được tính như:
SDet=1/M1 ∑(i=1 to M1)(1/K ∑(k=1 to K)σi_tk)     (3)

trong đó M1 là số lượng lời nhắc có đối tượng, K là số lượng khung mà phát hiện được thực hiện, và σi_tk là kết quả phát hiện cho khung tk trong video i (1 nếu một đối tượng được phát hiện, 0 nếu ngược lại). Trong phương pháp của chúng tôi, chúng tôi thực hiện phát hiện mỗi I= 5 khung. Do đó, K=N/I.

2. Count-Score (SCount): Đánh giá sự khác biệt số lượng đối tượng trung bình, được tính như:
SCount=1/M2 ∑(i=1 to M2)(1-1/K ∑(k=1 to K)|ci_tk-ĉi|/ĉi)     (4)

trong đó M2 là số lượng lời nhắc có số lượng đối tượng, ci_tk là số lượng đối tượng được phát hiện tại khung tk trong video i, và ĉi là số lượng đối tượng sự thật cơ bản cho video i.

3. Color-Score (SColor): Đánh giá độ chính xác màu sắc trung bình, được tính như:
SColor=1/M3 ∑(i=1 to M3)(1/K ∑(k=1 to K)si_tk)     (5)

trong đó M3 là số lượng lời nhắc có màu sắc đối tượng và si_tk là kết quả độ chính xác màu sắc cho khung tk trong video i (1 nếu màu sắc được phát hiện khớp với màu sắc sự thật cơ bản, 0 nếu ngược lại).

Phân tích Con người (Celebrity ID Score). Con người quan trọng đối với các video được tạo như được hiển thị trong các lời nhắc thế giới thực mà chúng tôi thu thập. Để đạt được điều này, chúng tôi đánh giá tính chính xác của khuôn mặt con người bằng cách sử dụng DeepFace [48], một hộp công cụ phân tích khuôn mặt phổ biến. Chúng tôi tính toán khoảng cách giữa khuôn mặt người nổi tiếng được tạo ra và hình ảnh thực của người nổi tiếng.

SCIS=1/M4 ∑(i=1 to M4)(1/N ∑(t=1 to N)(min k∈{1,...,N3} D(xi_t, fi_k)))     (6)

trong đó M4 là số lượng lời nhắc chứa người nổi tiếng, D(·,·) là hàm khoảng cách của Deepface, {fi_k}N3 k=1 là các hình ảnh người nổi tiếng được thu thập cho lời nhắc thứ i, và N3= 3.

Nhận dạng Văn bản (OCR-Score) Một trường hợp khó khăn khác cho việc tạo thị giác là tạo văn bản trong lời nhắc đầu vào. Để kiểm tra khả năng của các mô hình T2V hiện tại trong việc tạo văn bản, chúng tôi sử dụng hộp công cụ được sử dụng rộng rãi PaddleOCR [39] để phát hiện văn bản tiếng Anh từ các video được tạo. Sau đó, tương tự như HRS-Bench [7], chúng tôi tính toán Tỷ lệ Lỗi Từ (WER) [30], Khoảng cách Chỉnh sửa Chuẩn hóa (NED) [52], Tỷ lệ Lỗi Ký tự (CER) [37], và lấy trung bình.

4.3. Chất lượng Chuyển động
Đối với video, chúng tôi tin rằng chất lượng chuyển động là một sự khác biệt chính so với các miền khác, chẳng hạn như hình ảnh. Để đạt được điều này, chúng tôi xem xét chất lượng chuyển động như một trong những chỉ số đánh giá chính trong hệ thống đánh giá của chúng tôi. Ở đây, chúng tôi xem xét hai chất lượng chuyển động khác nhau được giới thiệu dưới đây.

Nhận dạng Hành động (Action-Score). Đối với các video về con người, chúng ta có thể dễ dàng nhận ra các hành động phổ biến thông qua các mô hình được tiền huấn luyện. Chúng tôi sử dụng hộp công cụ MMAction2 [16] và mô hình VideoMAE V2 [59] được tiền huấn luyện để suy luận các hành động của con người trong các video được tạo. Chúng tôi lấy độ chính xác phân loại làm Action-Score của chúng tôi, tập trung vào các lớp hành động Kinetics 400 [27].

Luồng Trung bình (Flow-Score). Chúng tôi cũng xem xét thông tin chuyển động chung của video. Để đạt được điều này, chúng tôi sử dụng RAFT [54], để trích xuất các luồng dày đặc của video trong mỗi hai khung. Sau đó, chúng tôi tính toán luồng trung bình trên các khung này để có được điểm luồng trung bình của mỗi clip video được tạo cụ thể vì một số phương pháp có khả năng tạo ra các video tĩnh khó được xác định bởi các chỉ số tính nhất quán thời gian.

Điểm Phân loại Biên độ (Motion AC-Score). Dựa trên luồng trung bình, chúng tôi tiếp tục xác định xem biên độ chuyển động trong video được tạo có nhất quán với biên độ được chỉ định bởi lời nhắc văn bản hay không. Để đạt được điều này, chúng tôi đặt ngưỡng luồng trung bình ρ mà nếu vượt quá ρ, một video sẽ được coi là lớn, và ở đây ρ được đặt là 5 dựa trên quan sát chủ quan của chúng tôi.

4.4. Tính Nhất quán Thời gian
Tính nhất quán thời gian cũng là một lĩnh vực rất có giá trị trong video được tạo của chúng tôi. Để đạt được điều này, chúng tôi bao gồm một số chỉ số để tính toán. Chúng tôi liệt kê chúng dưới đây.

Lỗi Warping. Chúng tôi đầu tiên xem xét lỗi warping, được sử dụng rộng rãi trong các phương pháp tính nhất quán thời gian mù trước đây [31, 32, 42]. Cụ thể, chúng tôi đầu tiên thu được luồng quang học của mỗi hai khung bằng cách sử dụng mạng ước lượng luồng quang học được tiền huấn luyện [54], sau đó, chúng tôi tính toán sự khác biệt theo pixel giữa hình ảnh được warp và hình ảnh được dự đoán. Chúng tôi tính toán sự khác biệt warp trên mỗi hai khung và tính toán điểm cuối cùng bằng cách sử dụng trung bình của tất cả các cặp.

Tính Nhất quán Ngữ nghĩa (CLIP-Temp). Bên cạnh lỗi theo pixel, chúng tôi cũng xem xét tính nhất quán ngữ nghĩa giữa mỗi hai khung, điều này cũng được sử dụng trong các công trình chỉnh sửa video trước đây [18, 42]. Cụ thể, chúng tôi xem xét độ tương tự cosine của các embedding của mỗi hai khung liên tiếp (emb(xt), emb(xt+1)) của các video được tạo và sau đó lấy trung bình trên mỗi hai khung.

Tính Nhất quán Khuôn mặt. Tương tự như CLIP-Temp, chúng tôi đánh giá tính nhất quán danh tính con người của các video được tạo. Cụ thể, chúng tôi chọn khung đầu tiên x1 làm tham chiếu và tính toán độ tương tự cosine của emb(x1) với {emb(xt)}N t=2. Sau đó, chúng tôi lấy trung bình các độ tương tự làm điểm cuối cùng.

4.5. Căn chỉnh Ý kiến Người dùng
Bên cạnh các chỉ số khách quan ở trên, chúng tôi đánh giá ý kiến người dùng thông qua các nghiên cứu tập trung vào năm khía cạnh chính: (1) Chất lượng Video, cho thấy chất lượng của video được tạo trong đó điểm cao hơn cho thấy ít mờ, nhiễu hoặc suy giảm thị giác khác; (2) Căn chỉnh Văn bản và Video, kiểm tra mối quan hệ giữa video được tạo và lời nhắc văn bản đầu vào, yêu cầu người dùng đánh giá tính chính xác của các chuyển động được tạo; (3) Chất lượng Chuyển động, yêu cầu người dùng xác định tính chính xác của các chuyển động được tạo từ video. (4) Tính Nhất quán Thời gian, đánh giá tính nhất quán theo khung, khác với Chất lượng Chuyển động, cần người dùng xếp hạng cho chuyển động chất lượng cao; (5) Sự thích chủ quan, tương tự như chỉ số thẩm mỹ, giá trị cao hơn cho thấy video được tạo thường đạt được sở thích của con người, và chúng tôi để lại chỉ số này được sử dụng trực tiếp.

Để đánh giá, chúng tôi tạo video bằng cách sử dụng điểm chuẩn lời nhắc được cung cấp trên năm phương pháp tiên tiến của ModelScope [58], ZeroScope [6], Gen2 [18], Floor33 [1], và PikaLab [5], nhận được 2,5k video tổng cộng. Để so sánh công bằng, chúng tôi thay đổi tỷ lệ khung hình của Gen2 và PikaLab thành 16:9 để phù hợp với các phương pháp khác. Ngoài ra, vì PikaLab không thể tạo nội dung mà không có watermark thị giác, chúng tôi thêm watermark của PikaLab vào tất cả các phương pháp khác để so sánh công bằng. Chúng tôi cũng xem xét rằng một số người dùng có thể không hiểu rõ lời nhắc, vì mục đích này, chúng tôi sử dụng SDXL [41] để tạo ba hình ảnh tham chiếu của mỗi lời nhắc để giúp người dùng hiểu rõ hơn, điều này cũng truyền cảm hứng cho chúng tôi thiết kế SD-Score để đánh giá căn chỉnh văn bản-video của các mô hình. Đối với mỗi chỉ số, chúng tôi yêu cầu 7 người dùng đưa ra ý kiến từ 1 đến 5, trong đó giá trị lớn cho thấy căn chỉnh tốt hơn. Chuỗi video đã được xáo trộn ngẫu nhiên trước khi được đưa cho người dùng, và chúng tôi nhận được 8647 điểm phản hồi tổng cộng. Cuối cùng, sau khi lọc, chúng tôi giữ lại 1024 điểm khách quan và chuyên nghiệp nhất như được minh họa trong Hình ??.

Sau khi thu thập dữ liệu người dùng, chúng tôi tiến hành thực hiện căn chỉnh con người cho các chỉ số đánh giá của chúng tôi, với mục tiêu thiết lập một đánh giá đáng tin cậy và mạnh mẽ hơn về các thuật toán T2V. Ban đầu, chúng tôi thực hiện căn chỉnh trên dữ liệu bằng cách sử dụng các chỉ số riêng lẻ được đề cập ở trên để xấp xỉ điểm số con người cho ý kiến của người dùng trong các khía cạnh cụ thể. Tương tự như các công trình đánh giá xử lý ngôn ngữ tự nhiên [19, 34], chúng tôi sử dụng mô hình hồi quy tuyến tính để khớp các tham số trong mỗi chiều. Cụ thể, chúng tôi chọn ngẫu nhiên 80% mẫu từ bốn phương pháp khác nhau làm mẫu khớp và để lại 20% mẫu còn lại để xác minh tính hiệu quả của phương pháp được đề xuất. Các tham số hệ số được thu được bằng cách tối thiểu hóa tổng bình phương phần dư giữa nhãn con người và dự đoán từ mô hình hồi quy tuyến tính. Trong giai đoạn tiếp theo, chúng tôi tích hợp kết quả căn chỉnh của bốn khía cạnh này và tính toán điểm tổng để có được điểm cuối cùng toàn diện.

5. Kết quả
Chúng tôi thực hiện đánh giá trên các lời nhắc điểm chuẩn của chúng tôi, trong đó mỗi lời nhắc có một metafile cho thông tin bổ sung làm câu trả lời của việc đánh giá. Sau đó chúng tôi tạo video bằng cách sử dụng tất cả các mô hình T2V độ phân giải cao có sẵn, bao gồm ModelScope [58], Floor33 Pictures [1], và ZeroScope [6], Show-1 [66], Hotshot-XL [3], và VideoCrafter1 [12]. Chúng tôi giữ tất cả các siêu tham số, chẳng hạn như hướng dẫn không có phân loại, như giá trị mặc định. Đối với mô hình dựa trên dịch vụ, chúng tôi đánh giá hiệu suất của các công trình đại diện của Gen2 [18] và PikaLab [5]. Chúng tạo ra ít nhất video 512p với video chất lượng cao không có watermark. Chúng tôi chạy tất cả các mô hình có sẵn trên NVIDIA A100 để so sánh tốc độ. Chúng tôi đầu tiên hiển thị kết quả căn chỉnh con người tổng thể trong Hình 4, với cũng các khía cạnh khác nhau của điểm chuẩn của chúng tôi trong Bảng 2, cung cấp cho chúng tôi các chỉ số cuối cùng và chính của điểm chuẩn của chúng tôi. Cuối cùng, như trong Hình 6, chúng tôi cung cấp kết quả của mỗi phương pháp trên 4 loại meta khác nhau (tức là, động vật, con người, phong cảnh, đối tượng) và hai loại video khác nhau (tức là, thực tế, phong cách) trong điểm chuẩn của chúng tôi.

5.1. Phân tích về Căn chỉnh Sở thích Con người
Để chứng minh tính hiệu quả của mô hình của chúng tôi trong việc căn chỉnh với điểm số con người, chúng tôi tính toán hệ số tương quan hạng Spearman [65] và hệ số tương quan hạng Kendall [28], cả hai đều là các biện pháp không tham số của tương quan hạng. Những hệ số này cung cấp cái nhìn sâu sắc về độ lớn và hướng của mối liên kết giữa kết quả phương pháp của chúng tôi và điểm số con người, như được liệt kê trong Bảng 3. Từ bảng này, phương pháp trọng số được đề xuất cho thấy tương quan tốt hơn trên 20% mẫu chưa thấy so với việc trực tiếp lấy trung bình.

5.2. Phát hiện
Phát hiện #1: Đánh giá một chiều là không đủ cho các mô hình T2V ngày nay. Xếp hạng của các mô hình trong Bảng 2 thay đổi đáng kể trên các khía cạnh khác nhau, nhấn mạnh tầm quan trọng của phương pháp đánh giá đa khía cạnh để hiểu toàn diện hiệu suất mô hình.

Phát hiện #2: Đánh giá loại meta là cần thiết. Như được hiển thị trong Hình 6, các mô hình hoạt động khác nhau trong các loại meta khác nhau, làm nổi bật tầm quan trọng của việc đánh giá khả năng của chúng theo loại meta. Ví dụ, Gen2 [18] hoạt động tốt hơn Floor33 Pictures [1] đối với VQA A trong con người, động vật, và video phong cách. Ngược lại, nó tụt hậu so với Floor33 Pictures trong phong cảnh, đối tượng, và video thực tế.

Phát hiện #3: Người dùng ưu tiên sự hấp dẫn thị giác hơn căn chỉnh T2V. Như được hiển thị trong ??, mặc dù Gen2 [18] hoạt động tương đối kém trong căn chỉnh T2V, nó vượt qua tất cả các mô hình khác trong Sự Thích Chủ quan. Chúng tôi cho rằng đó là vì người dùng thích video với sự hấp dẫn thị giác tốt hơn như chất lượng hình ảnh tốt và tính nhất quán thời gian cao.

Phát hiện #4: Tất cả các phương pháp không thể thực hiện điều khiển chuyển động camera trực tiếp bằng lời nhắc văn bản. Mặc dù một số siêu tham số bổ sung có thể được đặt làm tay cầm điều khiển bổ sung cho Gen2 [18] và PikaLab [5], tất cả các mô hình T2V hiện tại vẫn thiếu hiểu biết về các lời nhắc thế giới mở, như chuyển động camera.

Phát hiện #5: Độ phân giải không tương quan nhiều với sự hấp dẫn thị giác. Như được hiển thị trong Bảng 1 và Bảng 2, Gen2 [18] và Hotshot-XL [3] có độ phân giải nhỏ nhưng cả hai đều cạnh tranh trong chất lượng hình ảnh.

Phát hiện #6: Biên độ chuyển động lớn hơn không đảm bảo sở thích người dùng. Trong nghiên cứu của chúng tôi, hầu hết các video mà người dùng thích là những video có chuyển động nhẹ, chẳng hạn như những video được tạo bởi PikaLab [5] và Gen2 [18].

Phát hiện #7: Tạo văn bản vẫn còn thách thức. Hầu hết các phương pháp đều gặp khó khăn trong việc tạo văn bản chất lượng cao và nhất quán từ lời nhắc, như được thể hiện từ OCR-Scores. Kết quả thô của tất cả các chỉ số được cung cấp trong tài liệu bổ sung.

Phát hiện #8: Nhiều mô hình đôi khi có thể tạo ra video hoàn toàn sai. Từ nghiên cứu của chúng tôi, chúng tôi thấy khá nhiều trường hợp thất bại như nhiễu nghiêm trọng và biến dạng từ các mô hình cơ sở của chúng tôi như ZeroScope [6], ModelScope [58] và Floor33 Pictures [1]. Chúng tôi cho rằng điều này có thể được xem như một vấn đề quên thảm khốc [49], vì chúng ta biết nhiều mô hình T2V hiện tại được tinh chỉnh từ các mô hình cơ sở như SD [44]. Chúng tôi trình bày kết quả định tính chi tiết của chúng tôi trong tài liệu bổ sung.

Phát hiện #9: Các chỉ số hiệu quả và không hiệu quả lắm. Các chỉ số như Warp Error, CLIP-Temp, VQA T, và VQA A dường như hoạt động tốt vì tất cả đều có tương quan cao với điểm số con người được hiển thị trong Bảng 3. Tuy nhiên, một số chỉ số không tốt như chúng ta nghĩ. Đặc biệt là Clip-Score, một chỉ số được sử dụng rộng rãi trong các công trình trước đây [18,23,50], chỉ có Spearsman's ρ 6.3 và Kendall's ϕ 4.3 so với BLIP-BLEU trong cùng khía cạnh có 26.7 và 19.0. Kết quả tương quan chi tiết có thể được tìm thấy trong tài liệu bổ sung.

Phát hiện #10: Tất cả các mô hình hiện tại đều chưa đủ thỏa mãn. Từ đánh giá khách quan và quan sát chủ quan của chúng tôi, chúng tôi cho rằng các mô hình T2V ngày nay vẫn còn nhiều việc phải cải thiện. Ngay cả đối với mô hình tốt nhất trong đánh giá của chúng tôi, Gen2 [18] cũng có những hạn chế như gặp khó khăn với các cảnh phức tạp, tuân theo hướng dẫn, và chi tiết thực thể.

5.3. Hạn chế
Mặc dù chúng tôi đã thực hiện một bước trong việc đánh giá việc tạo T2V, vẫn còn nhiều thách thức. (i) Hiện tại, chúng tôi chỉ thu thập 700 lời nhắc làm điểm chuẩn, trong khi tình huống thế giới thực rất phức tạp. Nhiều lời nhắc hơn sẽ cho thấy một điểm chuẩn tổng quát hơn. (ii) Đánh giá chất lượng chuyển động của ý nghĩa chung cũng khó. Tuy nhiên, trong thời đại của LLM đa mô hình và các mô hình nền tảng video lớn, chúng tôi tin rằng các mô hình hiểu video tốt hơn và lớn hơn sẽ được phát hành và chúng ta có thể sử dụng chúng làm chỉ số của mình. (iii) Các nhãn được sử dụng để căn chỉnh được thu thập từ chỉ ít người chú thích hơn, có thể đưa vào một số thiên vị trong kết quả. Để giải quyết hạn chế này, chúng tôi dự định mở rộng nhóm người chú thích và thu thập nhiều điểm đa dạng hơn để đảm bảo đánh giá chính xác và không thiên vị hơn.

6. Kết luận
Khám phá khả năng của các mô hình tạo sinh lớn là rất quan trọng để cải thiện thiết kế và sử dụng mô hình. Trong bài báo này, chúng tôi thực hiện bước đầu tiên hướng tới việc đánh giá các mô hình T2V lớn, chất lượng cao bằng cách xây dựng một điểm chuẩn lời nhắc toàn diện cho đánh giá T2V. Chúng tôi cũng cung cấp một số chỉ số đánh giá khách quan để đo lường hiệu suất mô hình T2V liên quan đến chất lượng video, căn chỉnh văn bản-video, tính nhất quán thời gian, và chất lượng chuyển động. Hơn nữa, chúng tôi thực hiện căn chỉnh con người để tương quan điểm số người dùng với các chỉ số khách quan, dẫn đến các chỉ số đánh giá chính xác cho các phương pháp T2V. Các thí nghiệm của chúng tôi chứng minh rằng các phương pháp được đề xuất hiệu quả căn chỉnh với ý kiến người dùng, do đó cung cấp đánh giá đáng tin cậy về các phương pháp T2V. Chúng tôi tin rằng điểm chuẩn đánh giá toàn diện này sẽ phục vụ như một nền tảng và thúc đẩy phát triển cho nghiên cứu tương lai.

Lời cảm ơn
Xuebo Liu được tài trợ bởi CCF-Tencent Rhino-Bird Open Research Fund. Chúng tôi muốn cảm ơn các nhà đánh giá ẩn danh và meta-reviewer cho những gợi ý sâu sắc của họ.

Tài liệu tham khảo
[1] Floor33 pictures discord server. https://www.morphstudio.com/. Truy cập: 2023-08-30. 6, 7, 8, 12, 14
[2] Fulljourney discord server. https://www.fulljourney.ai/. Truy cập: 2023-08-30. 3, 12
[3] Hotshot-xl. https://huggingface.co/hotshotco/Hotshot-XL. Truy cập: 2023-10-11. 6, 8, 12, 14
[4] Morph studio discord server. https://www.morphstudio.com/. Truy cập: 2023-08-30. 2
[5] Pika Lab discord server. https://www.pika.art/. Truy cập: 2023-08-30. 1, 2, 3, 6, 7, 8, 12, 13, 14
[6] Zeroscope. https://huggingface.co/cerspense/zeroscope_v2_576w. Truy cập: 2023-08-30. 2, 6, 8, 12, 14
[7] Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, và Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. Apr 2023. 2, 5
[8] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. 2
[9] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, và Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2
[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong European conference on computer vision, trang 213–229. Springer, 2020. 2
[11] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023. 2
[12] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 6, 12
[13] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, và Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. 4
[14] Jaemin Cho, Abhay Zala, và Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers. 2, 4
[15] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, và David Jurgens. Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark. arXiv preprint arXiv:2305.14938, 2023. 2
[16] MMAction2 Contributors. Openmmlab's next generation video understanding toolbox and benchmark. https://github.com/open-mmlab/mmaction2, 2020. 5
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, trang 248–255, 2009. 4
[18] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, và Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 1, 2, 5, 6, 7, 8, 12, 14
[19] Kallirroi Georgila, Carla Gordon, Volodymyr Yanov, và David Traum. Predicting ratings of real dialogue participants from artificial data and ratings of human dialogue observers. Trong Proceedings of the Twelfth Language Resources and Evaluation Conference, trang 726–734, 2020. 6
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, và Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 2, 4
[21] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, và Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. 1, 2
[22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, và Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. 2
[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 2, 8
[24] Jonathan Ho, Ajay Jain, và Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. 2
[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, và David J. Fleet. Video diffusion models, 2022. 2
[26] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, và Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023. 2
[27] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 5
[28] Maurice George Kendall. Rank correlation methods. 1948. 7
[29] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2
[30] Dietrich Klakow và Jochen Peters. Testing the correlation of word error rate and perplexity. Speech Communication, 38(1-2):19–28, 2002. 5
[31] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, và Ming-Hsuan Yang. Learning blind video temporal consistency. Trong Proceedings of the European conference on computer vision (ECCV), trang 170–185, 2018. 5, 15
[32] Chenyang Lei, Yazhou Xing, và Qifeng Chen. Blind video temporal consistency via deep video prior. Trong Advances in Neural Information Processing Systems, 2020. 5
[33] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, và Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. Jul 2023. 2
[34] Dingquan Li, Tingting Jiang, và Ming Jiang. Unified quality assessment of in-the-wild videos with mixed datasets training. International Journal of Computer Vision, 129:1238–1257, 2021. 6
[35] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4
[36] George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995. 12
[37] Andrew Cameron Morris, Viktoria Maier, và Phil Green. From wer and ril to mer and wil: improved evaluation measures for connected speech recognition. Trong Eighth International Conference on Spoken Language Processing, 2004. 5
[38] OpenAI. Gpt-4 technical report, 2023. 1, 2, 4
[39] PaddlePaddle. Paddleocr. https://github.com/PaddlePaddle/PaddleOCR, 2013. 5
[40] Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, trang 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. 4
[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, và Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 1, 2, 4, 6, 14
[42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, và Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. 5
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, trang 8748–8763. PMLR, 2021. 2, 4
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 1, 2, 4, 8, 14
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. 2, 4
[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, và Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 1, 2, 4
[47] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, August 2020. Version 0.3.0. 4
[48] Sefik Ilkin Serengil và Alper Ozpinar. Hyperextended lightface: A facial attribute analysis framework. Trong 2021 International Conference on Engineering and Emerging Technologies (ICEET), trang 1–4. IEEE, 2021. 5
[49] Chenze Shao và Yang Feng. Overcoming catastrophic forgetting beyond continual learning: Balanced training for neural machine translation. arXiv preprint arXiv:2203.03910, 2022. 8
[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 1, 2, 8
[51] Ivan Skorokhodov, Sergey Tulyakov, và Mohamed Elhoseiny. Stylegan-v: A continuous video generator with the price, image quality and perks of stylegan2. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 3626–3636, 2022. 2
[52] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. Trong 2019 International Conference on Document Analysis and Recognition (ICDAR), trang 1557–1562. IEEE, 2019. 5
[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, và Andrew Rabinovich. Going deeper with convolutions. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 1–9, 2015. 4
[54] Zachary Teed và Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, trang 402–419. Springer, 2020. 5
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2
[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2
[57] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, và Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 1, 2, 4
[58] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, và Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 6, 8, 12, 14
[59] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, và Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking, 2023. 5
[60] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, và Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. Trong International Conference on Computer Vision (ICCV), 2023. 4
[61] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, và Yu-Gang Jiang. A survey on video diffusion models. arXiv preprint arXiv:2310.10647, 2023. 2
[62] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, và Jian Zhang. On the tool manipulation capability of open-source large language models, 2023. 2
[63] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2
[64] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, và Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. Trong International Conference on Learning Representations, 2022. 2
[65] Jerrold H Zar. Spearman rank correlation. Encyclopedia of Biostatistics, 7, 2005. 7
[66] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, và Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 6, 12
[67] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, và Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation, 2022. 2
[68] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023. 2
[69] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 2
[70] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, và Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 1, 2
[71] Gu Zhouhong, Zhu Xiaoxuan, Ye Haoning, Zhang Lin, Wang Jianchen, Jiang Sihang, Xiong Zhuozhi, Li Zihan, He Qianyu, Xu Rui, Huang Wenhao, Zheng Weiguo, Feng Hongwei, và Xiao Yanghua. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. arXiv:2304.11679, 2023. 2

--- TRANG 12 ---
Phụ lục

Nội dung Phụ lục
A. Phân tích Chi tiết Dữ liệu Người dùng Thế giới Thực 12
A.1. Phân phối Độ dài Lời nhắc . . . . . . . . . 12
A.2. Từ Quan trọng trong Lời nhắc . . . . . . . . 12
A.3. Lớp Meta trong Lời nhắc . . . . . . . . . . 12
B. Kết quả Định lượng 12
B.1. Kết quả Thô của Mỗi Chỉ số cho Mỗi Mô hình 12
B.2. Tương quan Giữa Chỉ số và Nhãn Con người . . . . . . . . . . . . . . . . . . . . 12
C. Kết quả Định tính 13
C.1. Tạo Nội dung . . . . . . . . . . . . . . 14
C.2. Tạo Chuyển động . . . . . . . . . . . . . . 14
C.3. Tạo Phong cách . . . . . . . . . . . . . . . 14
C.4. Điều khiển Chuyển động Camera . . . . . . . . . 14
C.5. Tạo Nhiệm vụ Cụ thể . . . . . . . . . . 14
D. Phân tích và Giải thích Bổ sung 14
D.1. Tính Đầy đủ của 700 Lời nhắc . . . . . . . . . 14
D.2. Nhân khẩu học, Giao diện và Tính Đầy đủ của Nghiên cứu Người dùng . . . . . . . . . . . . . . . . . . . . 15
D.3. Lời nhắc Crowdsourced & Động lực Tập dữ liệu 15
D.4. Sự Khác biệt Chỉ số Biên độ Chuyển động . . . 15
D.5. Sự Phụ thuộc vào Mô hình Tiền huấn luyện . . . . 15
D.6. Chi phí Đánh giá . . . . . . . . . . . . . . 15
D.7. Tính Ổn định Điểm chuẩn và Biến số Nhiễu . . 15
D.8. Lỗi Warping . . . . . . . . . . . . . . . 15
D.9. Sự Khác biệt trong Nghiên cứu Người dùng và Đánh giá . 15

A. Phân tích Chi tiết Dữ liệu Người dùng Thế giới Thực
Trong phần này, chúng tôi trình bày phân tích chi tiết về dữ liệu người dùng thế giới thực được thu thập từ người dùng discord tạo text-to-video (T2V), bao gồm FullJourney [2] và PikaLab [5]. Chúng tôi cung cấp cái nhìn sâu sắc về phân phối độ dài lời nhắc, từ quan trọng, và lớp meta.

A.1. Phân phối Độ dài Lời nhắc
Hình 7 (a) cho thấy phân phối độ dài lời nhắc trong dữ liệu người dùng thế giới thực. Chúng tôi thấy rằng 90% lời nhắc chứa từ trong khoảng [3,40]. Quan sát này giúp chúng tôi xác định độ dài phù hợp cho các lời nhắc trong điểm chuẩn của chúng tôi.

A.2. Từ Quan trọng trong Lời nhắc
Hình 7 (b) trình bày một đám mây từ của tất cả các từ trong dữ liệu người dùng thế giới thực. Từ đám mây từ này, chúng ta có thể quan sát các từ thường xuyên nhất trong các lời nhắc và có cái nhìn sâu sắc về các khái niệm chính mà người dùng yêu cầu trong việc tạo T2V.

A.3. Lớp Meta trong Lời nhắc
Hình 7 (c) cho thấy phân phối các loại danh từ trong dữ liệu người dùng thế giới thực. Chúng tôi sử dụng WordNet [36] để xác định các lớp meta. Loại trừ các từ về giao tiếp, thuộc tính và nhận thức, chúng tôi thấy rằng các tạo tác (đối tượng do con người tạo ra), con người, động vật, và vị trí (phong cảnh) đóng vai trò quan trọng trong các lời nhắc. Chúng tôi cũng bao gồm từ quan trọng nhất phong cách từ Hình 7 (b) trong các lớp meta.

Dựa trên phân tích này, chúng tôi chia việc tạo T2V thành bốn lớp meta-subject: con người, động vật, đối tượng, và phong cảnh. Phân loại này giúp chúng tôi tạo ra một điểm chuẩn đa dạng và đại diện để đánh giá các mô hình T2V.

B. Kết quả Định lượng
Trong phần này, chúng tôi trình bày kết quả định lượng của điểm chuẩn đánh giá của chúng tôi. Chúng tôi đã thực hiện thí nghiệm trên các mô hình tạo video tiên tiến khác nhau và đánh giá hiệu suất của chúng bằng 17 chỉ số khách quan. Chúng tôi cung cấp kết quả thô của mỗi chỉ số cho từng mô hình và tương quan giữa các chỉ số và nhãn con người. Kết quả được minh họa trong hai bảng. Bảng đầu tiên (Bảng 4) cho thấy kết quả thô của mỗi chỉ số cho từng mô hình. Bảng thứ hai (Bảng 5) hiển thị tương quan giữa các chỉ số và nhãn con người.

B.1. Kết quả Thô của Mỗi Chỉ số cho Mỗi Mô hình
Bảng 4 cho thấy kết quả thô của tất cả 17 chỉ số được giới thiệu cho mỗi mô hình được đánh giá. Tất cả các chỉ số được biểu thị dưới dạng phần trăm, ngoại trừ Warping Error và Flow-Score. Bảng được tổ chức như sau:
• Cột đầu tiên liệt kê các chỉ số được sử dụng để đánh giá.
• Các cột tiếp theo hiển thị kết quả thô cho mỗi mô hình, bao gồm ModelScope [58], Floor33 Pictures [1], và ZeroScope [6], Show-1 [66], Hotshot-XL [3], VideoCrafter1 [12], Gen2 [18], và PikaLab [5].
• Các mũi tên bên cạnh tên chỉ số cho biết liệu giá trị cao hơn (↑) hay thấp hơn (↓) có tốt hơn cho chỉ số cụ thể đó. Đối với Flow-Score, mũi tên được thay thế bằng mũi tên hướng phải (→) vì đây là chỉ số trung tính.

B.2. Tương quan Giữa Chỉ số và Nhãn Con người
Ngoài kết quả thô, Bảng 5 trình bày phân tích tương quan giữa các chỉ số khách quan và đánh giá của con người về việc tạo T2V. Chúng tôi sử dụng Spearman's ρ và Kendall's ϕ để tính toán tương quan. Bảng được tổ chức thành bốn phần, đại diện cho bốn khía cạnh của việc đánh giá: chất lượng hình ảnh, biên độ chuyển động, tính nhất quán thời gian, và căn chỉnh văn bản-video. Trong mỗi phần, chúng tôi so sánh các phương pháp khác nhau với phương pháp đánh giá được đề xuất của chúng tôi, được làm nổi bật bằng chữ đậm.

Như có thể thấy từ bảng, phương pháp của chúng tôi liên tục đạt được giá trị tương quan cao hơn so với trung bình của các phương pháp khác. Điều này cho thấy tính hiệu quả của phương pháp đánh giá được đề xuất của chúng tôi trong việc căn chỉnh các chỉ số khách quan với ý kiến của người dùng. Ví dụ, trong khía cạnh chất lượng hình ảnh, phương pháp của chúng tôi đạt được Spearman's ρ là 55.4 và Kendall's ϕ là 41.1, cả hai đều cao hơn giá trị trung bình lần lượt là 55.0 và 41.0. Những cải thiện tương tự có thể được quan sát ở các khía cạnh khác.

Ngoài các phát hiện được đề cập trước đó, chúng ta có thể quan sát rằng một số chỉ số cho thấy tương quan âm với đánh giá của con người, chẳng hạn như Color-Score và OCR-Score trong khía cạnh TV Alignment. Điều này cho thấy rằng các chỉ số này có thể không đáng tin cậy để đánh giá sự căn chỉnh giữa nội dung văn bản và video trong các mô hình tạo sinh. Mặt khác, các chỉ số như Detection-Score và Count-Score thể hiện tương quan tương đối cao hơn với đánh giá của con người, gợi ý về tính hữu ích tiềm năng của chúng trong việc đánh giá căn chỉnh T2V.

Nhìn chung, kết quả trong Bảng 5 cung cấp phân tích toàn diện về các chỉ số khách quan khác nhau và tương quan của chúng với đánh giá của con người. Những kết quả này có thể có giá trị cho các nhà nghiên cứu và thực hành trong lĩnh vực tạo T2V để chọn các chỉ số phù hợp để đánh giá mô hình của họ và để hiểu rõ hơn về điểm mạnh và điểm yếu của các phương pháp đánh giá khác nhau.

C. Kết quả Định tính
Trong phần này, chúng tôi trình bày kết quả định tính của các mô hình T2V được đánh giá cho các khía cạnh khác nhau của việc tạo video, có tính đến các phát hiện được liệt kê trong bài báo. Kết quả được hình ảnh hóa trong Hình 11 đến Hình 13. Chúng tôi thảo luận về hiệu suất của mỗi mô hình về điều khiển chuyển động camera, tạo nội dung, tạo chuyển động, tạo phong cách, và tạo nhiệm vụ cụ thể.

C.1. Tạo Nội dung
Trong Hình 9, chúng tôi trình bày kết quả định tính của các mô hình T2V và SDXL [41] cho bốn loại meta của việc tạo nội dung: con người, đối tượng, phong cảnh, và động vật. Phát hiện #5 cho thấy rằng độ phân giải không tương quan nhiều với sự hấp dẫn thị giác, như được chứng minh bởi Gen2 [18] và Hotshot-XL [3], có độ phân giải nhỏ nhưng cả hai đều cạnh tranh trong chất lượng hình ảnh. Bên cạnh đó, chúng ta cũng có thể thấy rằng Gen2 [18] và PikaLab [5] có thể phân biệt rõ ràng hơn với SDXL [41] trong cả nội dung và phong cách video so với các phương pháp khác.

C.2. Tạo Chuyển động
Hình 10 hiển thị kết quả định tính của các mô hình T2V đối với việc tạo chuyển động. Theo Phát hiện #6, biên độ chuyển động lớn hơn không đảm bảo sở thích của người dùng. Trong nghiên cứu của chúng tôi, hầu hết các video mà người dùng thích là những video có chuyển động nhẹ, chẳng hạn như những video được tạo bởi PikaLab [5] và Gen2 [18].

C.3. Tạo Phong cách
Kết quả định tính của các mô hình T2V liên quan đến việc tạo phong cách được hiển thị trong Hình 11. Chúng ta có thể thấy từ hình rằng hầu hết các phương pháp đều có khả năng tạo video với phong cách cụ thể, có thể được kế thừa từ các mô hình cơ sở. Tuy nhiên, các phương pháp khác nhau như ZeroScope [6] và ModelScope [58] cũng đang gặp khó khăn trong việc tạo ra nội dung phong cách chất lượng cao và nhất quán từ lời nhắc.

C.4. Điều khiển Chuyển động Camera
Hình 12 cho thấy kết quả định tính của các mô hình T2V về các lời nhắc với điều khiển chuyển động camera. Như được chỉ ra bởi Phát hiện #4, tất cả các phương pháp không thể thực hiện điều khiển chuyển động camera bằng lời nhắc văn bản, cho thấy tất cả các mô hình T2V đều thiếu hiểu biết về chuyển động camera.

C.5. Tạo Nhiệm vụ Cụ thể
Cuối cùng, Hình 13 trình bày kết quả định tính của các mô hình T2V và SDXL [41] về các nhiệm vụ khác nhau, tức là tạo khuôn mặt, tạo đối tượng với màu sắc, tạo đối tượng với số lượng, tạo văn bản, và tạo hoạt động. Phát hiện #8 cho thấy rằng nhiều mô hình đôi khi có thể tạo ra video hoàn toàn sai, với nhiễu nghiêm trọng và biến dạng được quan sát trong các mô hình cơ sở như ZeroScope [6], ModelScope [58], và Floor33 Pictures [1]. Điều này có thể được xem như một vấn đề quên thảm khốc, vì nhiều mô hình T2V hiện tại được tinh chỉnh từ các mô hình cơ sở như SD [44].

Tóm lại, kết quả định tính được trình bày trong phụ lục này cung cấp cái nhìn sâu sắc có giá trị về điểm mạnh và điểm yếu của các mô hình T2V khác nhau trong các khía cạnh khác nhau của việc tạo video. Như đã nêu trong Phát hiện #10, tất cả các mô hình hiện tại đều chưa đủ thỏa mãn, và các mô hình T2V vẫn còn đáng kể chỗ để cải thiện. Ngay cả mô hình tốt nhất trong đánh giá của chúng tôi, Gen2 [18], cũng có những hạn chế như gặp khó khăn với các cảnh phức tạp, tuân theo hướng dẫn, và chi tiết thực thể. Những kết quả này, cùng với khung đánh giá và quy trình được đề xuất của chúng tôi, cho phép đánh giá toàn diện và đáng tin cậy hơn về hiệu suất của các mô hình tạo video lớn.

D. Phân tích và Giải thích Bổ sung

D.1. Tính Đầy đủ của 700 Lời nhắc
Như một nỗ lực ban đầu, một lý do quan trọng để sử dụng những lời nhắc này là chúng tôi thấy rằng các chỉ số có xu hướng đạt đến một mức ổn định khi mẫu tăng lên như trong Hình 8. Bên cạnh đó, các điểm chuẩn đồng thời (ví dụ, FETV (619 lời nhắc tổng cộng), VBench (100 lời nhắc cho mỗi chỉ số)) sử dụng một lượng lời nhắc tương tự. Từ góc độ thực tế, tốc độ lấy mẫu của các mô hình T2V thường chậm, một điểm chuẩn nhỏ nhưng hiệu quả là rất quan trọng để đánh giá nhanh các phương pháp khác nhau.

D.2. Nhân khẩu học, Giao diện và Tính Đầy đủ của Nghiên cứu Người dùng
Giao diện được hiển thị dưới đây. Chúng tôi sử dụng nền tảng thử nghiệm AI nội bộ của chúng tôi để tìm người đánh giá con người. Mỗi người đánh giá được yêu cầu thực hiện 100 nhiệm vụ như gắn nhãn trước, người chú thích có độ chính xác hơn 90% sẽ được đánh dấu là đủ tiêu chuẩn. Nếu không, chúng tôi sẽ xem xét nhà cung cấp khác. Sau đó, những người chú thích đủ tiêu chuẩn sẽ gắn nhãn toàn bộ điểm chuẩn.

D.3. Lời nhắc Crowdsourced & Động lực Tập dữ liệu
Phương pháp của chúng tôi nhằm nắm bắt một cái nhìn tổng quan về kỳ vọng người dùng hiện tại và khả năng mô hình. Bên cạnh đó, điểm chuẩn được dự định là động với các cập nhật định kỳ.

D.4. Sự Khác biệt Chỉ số Biên độ Chuyển động
Điều này chủ yếu do sở thích của người dùng ưa chuộng các chuyển động tinh tế như đã nêu trong Phát hiện #6, ví dụ như Gen2 (luôn tạo ra chuyển động nhỏ) xếp hạng thứ 7 đối với Motion AC-Score trong Bảng 4, nhưng nó xếp hạng nhất đối với chất lượng chuyển động trong nghiên cứu người dùng, dẫn đến Gen2 xếp hạng nhất trong Bảng 2.

D.5. Sự Phụ thuộc vào Mô hình Tiền huấn luyện
Như nỗ lực ban đầu của chúng tôi (cũng như toàn bộ cộng đồng), các mô hình tiền huấn luyện cung cấp một tham chiếu để tìm các chỉ số khách quan có ý nghĩa. Chúng tôi sẽ tích cực khám phá các chỉ số đơn giản hơn để tránh sử dụng các mô hình tiền huấn luyện, ví dụ như huấn luyện một mô hình đánh giá end-to-end cho mỗi khía cạnh bằng cách sử dụng nhiều ý kiến người dùng hơn.

D.6. Chi phí Đánh giá
Chúng tôi đồng ý rằng đánh giá trực tuyến là rất quan trọng cho việc huấn luyện mô hình. Tuy nhiên, không thể giám sát các Mô hình T2V trong quá trình chạy (ngay cả khi sử dụng FVD) vì mỗi mẫu video yêu cầu hơn 2 phút để tạo ra. Phương pháp của chúng tôi được thiết kế cho đánh giá ngoại tuyến (đóng vai trò tương tự như đánh giá FVD trước đây). Toàn bộ điểm chuẩn yêu cầu khoảng 2 giờ trên GPU A100 và ít nhất 16 GB chi phí bộ nhớ mà không có bất kỳ tối ưu hóa mã nào, mà chúng tôi nghĩ là đòi hỏi cao hơn so với các phương pháp truyền thống như FVD. Tuy nhiên, FVD chỉ có thể phản ánh một khía cạnh của mô hình T2V và cần tập dữ liệu video thực làm tham chiếu.

D.7. Tính Ổn định Điểm chuẩn và Biến số Nhiễu
Chúng tôi cung cấp một số phân tích về tính ổn định. Đầu tiên, như trong Hình 8, chúng tôi thấy điểm khách quan ổn định giữa các phương pháp khác nhau khi lời nhắc tăng lên. Bên cạnh đó, chúng tôi cũng thử giới thiệu nhiễu vào các lời nhắc, tức là thêm, loại bỏ, hoặc hoán đổi từ/ký hiệu trong 100 lời nhắc được chọn ngẫu nhiên. Đáng chú ý, những thay đổi trong tất cả điểm số trong Bảng 2 là biên thiết giữa các phương pháp, với hầu hết các biến động dưới 0,2 điểm. Thứ hạng vẫn nhất quán trên tất cả các mô hình.

D.8. Lỗi Warping
Lỗi warping đánh giá sự khác biệt giữa khung tiếp theo thực tế và dự đoán của nó, được tạo ra bằng cách warp khung hiện tại bằng luồng quang học. Lỗi warping lớn hơn có nghĩa là mỗi khung thay đổi đáng kể, điều này thường không mong muốn đối với video thế giới thực. Nó cũng được sử dụng rộng rãi trong các phương pháp chỉ số tính nhất quán video mù trước đây [31] cho các chỉ số tính nhất quán thời gian.

D.9. Sự Khác biệt trong Nghiên cứu Người dùng và Đánh giá
Sự khác biệt này phát sinh từ những nỗ lực của chúng tôi để liên tục cập nhật và nâng cao danh sách lời nhắc của chúng tôi. Điểm chuẩn ban đầu của chúng tôi chứa 512 lời nhắc cho nghiên cứu người dùng, và chúng tôi tiếp tục mở rộng nó thành 700 lời nhắc để làm cho nó toàn diện và cân bằng hơn. Tuy nhiên, tương tự như Hình 8, không có thay đổi đáng kể trong kết quả của chúng tôi sau khi tăng lời nhắc. Do đó, chúng tôi sử dụng cùng kết quả nghiên cứu người dùng để tránh lãng phí tài nguyên như phiên bản ban đầu.

--- TRANG 16 ---
Hình 9. Kết quả định tính của các mô hình T2V về bốn loại meta (tức là, con người, đối tượng, phong cảnh, và động vật)

--- TRANG 17 ---
Hình 10. Kết quả định tính của các mô hình T2V đối với việc tạo chuyển động

--- TRANG 18 ---
Hình 11. Kết quả định tính của các mô hình T2V đối với việc tạo phong cách

--- TRANG 19 ---
Hình 12. Kết quả định tính của các mô hình T2V về các lời nhắc với điều khiển chuyển động camera

--- TRANG 20 ---
Hình 13. Kết quả định tính của các mô hình T2V về các nhiệm vụ khác nhau (tức là, tạo khuôn mặt, tạo đối tượng với màu sắc, tạo đối tượng với số lượng, tạo văn bản, và tạo hoạt động)
