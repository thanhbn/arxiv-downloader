# 2310.00653.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.00653.pdf
# Kích thước tệp: 1930303 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tái Cấu Trúc Các Mô Hình Nền Tảng Thị Giác-Ngôn Ngữ và Tập Dữ Liệu
Hướng Tới Trợ Lý Đa Phương Thức Toàn Diện
Tianyu Yu*1Jinyi Hu*1Yuan Yao1†Haoye Zhang1Yue Zhao2
Chongyi Wang3Shan Wang3Yinxu Pan4Jiao Xue3Dahai Li3
Zhiyuan Liu1†Hai-Tao Zheng1†Maosong Sun1†
1Đại học Thanh Hoa2Đại học Bưu điện Bắc Kinh
3Zhihu Inc.4ModelBest Inc.
yiranytianyu@gmail.com

Tóm tắt
Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) gần đây thể hiện khả năng ấn tượng trong việc nhận thức hình ảnh và tuân theo các hướng dẫn mở. Khả năng của MLLMs phụ thuộc vào hai yếu tố quan trọng: kiến trúc mô hình để tạo điều kiện cho việc căn chỉnh đặc trưng của các mô-đun thị giác và mô hình ngôn ngữ lớn; các tập dữ liệu tinh chỉnh hướng dẫn đa phương thức để tuân theo hướng dẫn của con người. (i) Đối với kiến trúc mô hình, hầu hết các mô hình hiện tại giới thiệu một mô-đun cầu nối bên ngoài để kết nối bộ mã hóa thị giác với mô hình ngôn ngữ, điều này cần một quá trình tiền huấn luyện căn chỉnh đặc trưng bổ sung. Trong công trình này, chúng tôi phát hiện rằng các mô hình thị giác ngôn ngữ tiền huấn luyện nhỏ gọn có thể vốn dĩ đóng vai trò là cầu nối "sẵn sàng sử dụng" giữa thị giác và ngôn ngữ. Dựa trên điều này, chúng tôi đề xuất khung Muffin, trực tiếp sử dụng các mô hình thị giác-ngôn ngữ tiền huấn luyện để đóng vai trò nhà cung cấp tín hiệu thị giác. (ii) Đối với các tập dữ liệu tinh chỉnh hướng dẫn đa phương thức, các phương pháp hiện tại bỏ qua mối quan hệ bổ sung giữa các tập dữ liệu khác nhau và đơn giản trộn lẫn các tập dữ liệu từ các nhiệm vụ khác nhau. Thay vào đó, chúng tôi đề xuất tập dữ liệu UniMM-Chat khám phá tính bổ sung của các tập dữ liệu để tạo ra 1.1M hướng dẫn đa phương thức chất lượng cao và đa dạng. Chúng tôi hợp nhất thông tin mô tả cùng một hình ảnh từ các tập dữ liệu đa dạng và chuyển đổi nó thành dữ liệu hội thoại chuyên sâu về kiến thức hơn. Kết quả thực nghiệm chứng minh hiệu quả của khung Muffin và tập dữ liệu UniMM-Chat. Muffin đạt được hiệu suất tiên tiến trên một loạt rộng các nhiệm vụ thị giác-ngôn ngữ, vượt trội đáng kể so với các mô hình tiên tiến như LLaVA và InstructBLIP. Mô hình và tập dữ liệu của chúng tôi đều có thể truy cập tại https://github.com/thunlp/muffin.

1 Giới thiệu
Xây dựng một mô hình tổng quát có khả năng giải quyết các nhiệm vụ đa dạng trên nhiều phương thức đã và vẫn là mục tiêu lâu dài trong lĩnh vực Trí tuệ Nhân tạo. Gần đây, các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) mạnh mẽ đã nổi lên như một trong những cách tiếp cận hứa hẹn nhất để đạt được mục tiêu này, chẳng hạn như MiniGPT-4 (Zhu et al. 2023), LLaVA (Liu et al. 2023), và InstructBLIP (Dai et al. 2023). Những mô hình này trao quyền cho các mô hình ngôn ngữ lớn (LLMs) với khả năng tuân theo hướng dẫn đa phương thức ấn tượng bằng cách trang bị LLMs với bộ mã hóa thị giác để nhận thức nội dung hình ảnh.

*Những tác giả này đóng góp như nhau.
†Tác giả liên hệ.

[Biểu đồ hiệu suất cho thấy Muffin đạt được hiệu suất tiên tiến trên các nhiệm vụ khác nhau so với các MLLMs mạnh khác]

Hình 1: Muffin đạt được hiệu suất tiên tiến trên các nhiệm vụ khác nhau so với các MLLMs mạnh. Trả lời Câu hỏi Thị giác: điểm trung bình trên bốn tập dữ liệu trả lời câu hỏi thị giác. Trò chuyện Thị giác: điểm trung bình trên nhiệm vụ hội thoại, nhiệm vụ lý luận phức tạp và nhiệm vụ tạo mô tả chi tiết.

Mặc dù có những khả năng hiện tại, một số yếu tố quan trọng trong việc phát triển MLLM vẫn chưa được khám phá đầy đủ. Trong công trình này, chúng tôi tập trung vào hai thách thức chính trong việc xây dựng MLLMs: (i) hiệu quả của kiến trúc mô hình để đạt được căn chỉnh đặc trưng; (ii) xây dựng tập dữ liệu tinh chỉnh hướng dẫn đa phương thức.

Đối với kiến trúc mô hình, các MLLMs hiện tại có thể được tóm tắt thành hai dòng: (1) Một bộ chiếu tuyến tính được tối ưu hóa để căn chỉnh bộ mã hóa thị giác đông lạnh với LLM đông lạnh, chẳng hạn như LLaVA (Liu et al. 2023) và PaLM-E (Driess et al. 2023); (2) Một bộ lấy mẫu lại đặc trưng thị giác (Alayrac et al. 2022; Li et al. 2023; Dai et al. 2023) được tối ưu hóa để nén đầu ra của bộ mã hóa thị giác thành một chuỗi đặc trưng có độ dài cố định và căn chỉnh các đặc trưng này với LLMs. Tuy nhiên, chỉ sử dụng một bộ chiếu tuyến tính hạn chế khả năng học kiến thức mới của mô hình và độ dài chuỗi đặc trưng có liên quan bậc hai với độ phân giải của hình ảnh đầu vào, dẫn đến gánh nặng tính toán đáng kể. Mặt khác, việc giới thiệu bộ lấy mẫu lại đặc trưng thị giác đòi hỏi một quá trình huấn luyện bổ sung tốn kém tài nguyên để chủ yếu

--- TRANG 2 ---
đạt được sự căn chỉnh các phương thức trước khi kết nối bộ mã hóa thị giác với LLMs (Li et al. 2023).

Để giải quyết những hạn chế nêu trên, chúng tôi đề xuất Muffin1, một kiến trúc hiệu quả để xây dựng các MLLMs mạnh mẽ. Một cách trực quan, chúng tôi nhận thấy rằng các mô hình thị giác-ngôn ngữ (VLMs) tiền huấn luyện nhỏ gọn, chẳng hạn như ALBEF (Li et al. 2021), CoCa (Yu et al. 2022), và BEiT-3 (Wang et al. 2023a), đã thể hiện hiệu suất đáng chú ý trong các nhiệm vụ thị giác-ngôn ngữ (V-L) thông qua tiền huấn luyện trên các tập dữ liệu đa phương thức rộng lớn. Kết quả là, những VLMs này vốn dĩ đạt được sự căn chỉnh các phương thức và có tiềm năng thành thạo như các mô-đun cầu nối "sẵn sàng sử dụng" để trao quyền cho LLMs với khả năng thị giác. Dựa trên trực giác này, Muffin trực tiếp tận dụng các VLMs tiền huấn luyện và học một tập hợp các vector truy vấn trong không gian nhúng của VLMs để nhận thức biểu diễn thị giác cho LLMs. Bằng cách này, chúng tôi có thể trực tiếp tối ưu hóa mô-đun thị giác để kết nối với LLMs mà không mất khả năng hoặc trải qua quá trình căn chỉnh bổ sung. Kết quả thực nghiệm cho thấy Muffin có thể đạt được hiệu suất tiên tiến trong số các MLLMs hiện tại.

Về mặt xây dựng các tập dữ liệu tinh chỉnh hướng dẫn đa phương thức, hầu hết các công trình gần đây (Gong et al. 2023; Dai et al. 2023) đơn giản định dạng tập dữ liệu thị giác-ngôn ngữ hạ nguồn thành một định dạng thống nhất, trong khi định dạng ngắn và hạn chế của các phản hồi trong những tập dữ liệu này sẽ làm tổn hại khả năng tạo sinh của LLMs. Một hướng công trình khác (Zhu et al. 2023; Liu et al. 2023) chuyển đổi các tập dữ liệu riêng lẻ thành kho dữ liệu hội thoại dựa trên ChatGPT hoặc GPT-4 (OpenAI 2023). Tuy nhiên, họ bỏ qua tính bổ sung của các tập dữ liệu khác nhau điều này rất quan trọng để hình thành cái nhìn toàn diện về nội dung hình ảnh và do đó dẫn đến sự khan hiếm kiến thức trong dữ liệu được tạo ra.

Để khắc phục những thiếu sót như vậy, chúng tôi thiết kế một cách tiếp cận đơn giản và hiệu quả để tái định dạng nhiều tập dữ liệu thành kho dữ liệu trò chuyện với định dạng linh hoạt trong các phản hồi. Do đó, mặc dù thiếu thông tin trong một chú thích, nhiều chú thích cho cùng một hình ảnh có thể được hợp nhất một cách bổ sung để tạo thành một mô tả toàn diện hơn về hình ảnh. Cụ thể, chúng tôi sử dụng hình ảnh từ COCO (Lin et al. 2014) để xây dựng tập dữ liệu. Dựa trên các chú thích kết hợp, chúng tôi yêu cầu ChatGPT tạo ra kho dữ liệu trò chuyện chất lượng cao chính xác và chuyên sâu về kiến thức. Theo quy trình này, chúng tôi xây dựng UniMM-Chat, một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức chất lượng cao chứa hơn 1.1M hướng dẫn. Chúng tôi tiến hành một loạt thí nghiệm để chứng minh hiệu quả của đường ống xây dựng dữ liệu và tập dữ liệu UniMM-Chat kết quả. Ngoài ra, chúng tôi xây dựng điểm chuẩn UniMM-Bench để đánh giá khả năng của MLLMs trong lý luận và kiến thức thế giới. Cụ thể, chúng tôi thu thập câu hỏi từ các điểm chuẩn VL hiện tại và tận dụng GPT-4 để chấm điểm đầu ra của mô hình.

Nói chung, chúng tôi tóm tắt đóng góp của mình như sau:
• Chúng tôi đề xuất một kiến trúc mới, Muffin, tái định dạng các VLMs tiền huấn luyện làm cầu nối giữa các mô-đun thị giác và LLMs. Muffin đạt được hiệu suất tiên tiến trong số các đường cơ sở hiện tại trên một loạt rộng các nhiệm vụ.

1Các mô hình nền tảng đa phương thức được phát hiện là giao diện đa phương thức "sẵn sàng sử dụng" cho LLMs.

[Bảng 1 tóm tắt cấu trúc của các MLLMs hiện tại]

• Chúng tôi xây dựng một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức chuyên sâu về kiến thức, UniMM-Chat, được xây dựng bằng cách yêu cầu ChatGPT tạo ra các cuộc đối thoại dựa trên thông tin hợp nhất từ các tập dữ liệu khác nhau.

• Chúng tôi xây dựng điểm chuẩn UniMM-Bench để đánh giá khả năng tổng thể của MLLMs liên quan đến các nhiệm vụ đa dạng và đánh giá Muffin và các mô hình MLLM khác trên đó.

• Chúng tôi mở nguồn Muffin, UniMM-Chat, và UniMM-Bench cho cộng đồng.

2 Công trình Liên quan

Mô hình Thị giác Ngôn ngữ Nghiên cứu về các VLMs tiền huấn luyện đã là chủ đề nóng trong nhiều năm. Những mô hình này được tiền huấn luyện trên quy mô lớn các cặp hình ảnh-văn bản để đạt được sự căn chỉnh giữa các phương thức thị giác và văn bản. Một số công trình tập trung vào cải thiện các mục tiêu huấn luyện, chẳng hạn như mất mát tương phản (Radford et al. 2021; Jia et al. 2021), mô hình hóa dữ liệu có mặt nạ (Wang et al. 2023a), và khớp hình ảnh-văn bản (Li et al. 2021). Một số công trình dành công sức để tối ưu hóa kiến trúc mô hình, chẳng hạn như UNITER (Chen et al. 2020) và VinVL (Zhang et al. 2021), và kiến trúc transformer thống nhất gần đây VLMo (Bao et al. 2022). Dựa trên những kỹ thuật này, một số VLMs quy mô lớn được đề xuất, chẳng hạn như Florence (Yuan et al. 2021) và BEiT-3 (Wang et al. 2023a). Những mô hình này thể hiện hiệu suất tốt trên các nhiệm vụ VL trong khi thiếu khả năng tuân theo hướng dẫn của con người.

Mô hình Ngôn ngữ Lớn Đa phương thức MLLMs nhằm mục đích kết nối một mô-đun thị giác với LLMs tiền huấn luyện để tương tác đa phương thức. Công trình tiên phong, BLIP-2 (Li et al. 2023), giới thiệu kiến trúc Q-Former, một transformer nông để căn chỉnh đặc trưng thị giác từ bộ mã hóa thị giác đông lạnh với LLMs. Các công trình tiếp theo phần lớn áp dụng kiến trúc Q-Former, chẳng hạn như MiniGPT-4 (Zhu et al. 2023), VisualGLM (Du et al. 2022), Ziya-Visual (Wang et al. 2022). LLaVA (Liu et al. 2023) sử dụng một lớp tuyến tính để ánh xạ đặc trưng thị giác từ bộ mã hóa thị giác đông lạnh vào không gian nhúng của LLM tiền huấn luyện (Chiang et al. 2023). mPLUG-Owl (Ye et al. 2023) tận dụng một mô-đun Q-Former được sửa đổi để căn chỉnh bộ mã hóa thị giác CLIP với LLM sử dụng cả các tập dữ liệu tinh chỉnh hướng dẫn chỉ có văn bản và đa phương thức. InstructBLIP (Dai et al. 2023) cải thiện Q-Former, thu được các đặc trưng thị giác nhận biết hướng dẫn bằng cách đưa hướng dẫn vào Q-Former. Bảng 1 tóm tắt cấu trúc chi tiết của những mô hình này.

--- TRANG 3 ---
Tập dữ liệu Tinh chỉnh Hướng dẫn Đa phương thức Để trang bị cho MLLMs khả năng tuân theo hướng dẫn mạnh mẽ, một số tập dữ liệu tinh chỉnh hướng dẫn đa phương thức được đề xuất. MiniGPT-4 (Zhu et al. 2023) đề xuất sử dụng ChatGPT để viết lại mô tả hình ảnh và thu thập gần 3.5K trường hợp hướng dẫn. InstructBLIP (Dai et al. 2023) định dạng 26 tập dữ liệu công khai của các nhiệm vụ khác nhau với các mẫu thủ công cho mỗi tập dữ liệu. LLaVA (Liu et al. 2023) đề xuất tận dụng GPT-4 để viết hướng dẫn cho ba danh mục khác nhau, bao gồm mô tả chi tiết, lý luận phức tạp, và hội thoại, dựa trên các chú thích của hình ảnh từ tập dữ liệu COCO (Lin et al. 2014). Mặc dù dữ liệu được tạo ra của LLaVA đa dạng hơn so với MiniGPT-4, các hướng dẫn vẫn khan hiếm kiến thức vì các chú thích chỉ từ một tập dữ liệu khó có thể đưa ra sự hiểu biết toàn diện về hình ảnh.

3 Khung Muffin

3.1 Kiến trúc

Kiến trúc của Muffin được đề xuất được thể hiện trong Hình 2. Thay vì huấn luyện một mô-đun riêng biệt để kết nối bộ mã hóa thị giác và LLMs, Muffin trực tiếp sử dụng một mô hình VLM tiền huấn luyện, ký hiệu là G, để tóm tắt biểu diễn thị giác cho LLMs, ký hiệu là F. Thông thường, VLMs bao gồm một kênh thị giác và một kênh văn bản, được hợp nhất sâu với nhau để đạt được sự căn chỉnh phương thức. Bằng việc tiền huấn luyện rộng rãi trong các tập dữ liệu V-L quy mô lớn, VLMs vốn dĩ xuất sắc trong việc đóng vai trò cầu nối "sẵn sàng sử dụng" cho LLM. Trong công trình này, chúng tôi tận dụng BEiT-3 (Wang et al. 2023a) làm xương sống VLM, được tiền huấn luyện với mô hình hóa dữ liệu có mặt nạ và đạt được hiệu suất tốt trên nhiều nhiệm vụ thị giác và thị giác-ngôn ngữ.

Để tận dụng VLM cho việc trích xuất đặc trưng thị giác, Muffin giới thiệu một chuỗi các vector truy vấn có thể huấn luyện trong không gian nhúng văn bản của VLM, ký hiệu là Q=[q1,q2,···,qn],qn∈Rd, trong đó n là số lượng vector truy vấn có thể huấn luyện, d là kích thước ẩn của VLM G. Những vector truy vấn Q và hình ảnh Xv được đưa vào các kênh văn bản và thị giác, tương ứng. Trong mỗi khối của Transformer, để hợp nhất sâu hai phương thức, các trạng thái ẩn từ mỗi kênh sẽ thực hiện cả tự chú ý và chú ý chéo với nhau.

Sau khi hợp nhất sâu giữa các vector truy vấn có thể huấn luyện Q và hình ảnh, đầu ra cuối cùng trong lớp cuối tương ứng với vị trí của các vector truy vấn hiệu quả nắm bắt một đặc trưng thị giác của hình ảnh đầu vào. Quá trình này có thể được biểu thị một cách súc tích như một công thức đầu cuối đến cuối:

Zv=G(Xv,Qϑ). (1)

Tiếp theo, chúng tôi áp dụng một lớp chiếu kết nối đầy đủ để chuyển đổi đặc trưng thị giác được nhận thức Zv vào không gian nhúng của LLM tiền huấn luyện Hv=Wξ·Zv, điều này sẽ đóng vai trò là ngữ cảnh tiền tố cho LLM và nối với nhúng văn bản của Xt như đầu vào cuối cùng để được chuyển tiếp đến LLM.

3.2 Tiền huấn luyện

Theo hầu hết các MLLMs hiện tại (Liu et al. 2023; Dai et al. 2023), chúng tôi đầu tiên tiến hành tiền huấn luyện trên một số lượng lớn các cặp hình ảnh-văn bản để căn chỉnh VLM G và LLM F. Vì dữ liệu văn bản được sử dụng trong giai đoạn này chia sẻ các định dạng tương đối đơn giản, chúng tôi đông lạnh các tham số của LLM trong quá trình tiền huấn luyện để giữ lại kiến thức mạnh mẽ và khả năng lý luận phức tạp của LLM. Đối với một cặp hình ảnh và văn bản (Xv, Xt), chúng tôi chọn ngẫu nhiên một hướng dẫn Xins từ một tập hợp được định nghĩa trước, như được sử dụng trong Liu et al. (2023). Hướng dẫn này, chẳng hạn như "Mô tả hình ảnh một cách ngắn gọn.", đóng vai trò tiền tố cho chú thích và thu hẹp khoảng cách trong định dạng dữ liệu huấn luyện giữa giai đoạn hiện tại và tinh chỉnh hướng dẫn tiếp theo. Mục tiêu huấn luyện là tối đa hóa xác suất của phản hồi mục tiêu dựa trên hướng dẫn Xins và hình ảnh đầu vào Xv, được thể hiện như:

L=kΣi=1logp(xi|Hv, Xins, Xt,<i) (2)

[Hình 2: Kiến trúc của Muffin được đề xuất. Một chuỗi các vector truy vấn có thể huấn luyện và các miếng hình ảnh được đưa vào các kênh văn bản và thị giác của VLM, tương ứng. Bằng cách hợp nhất sâu trong các khối VLM, đầu ra từ kênh văn bản đóng vai trò là đặc trưng hình ảnh được tóm tắt.]

3.3 Tinh chỉnh Hướng dẫn Đa phương thức

Trong khi các mô hình tiền huấn luyện đa phương thức ngây thơ thể hiện khả năng hiểu nội dung của hình ảnh đầu vào và tạo ra các chú thích ngắn gọn, chúng thường thiếu khả năng thực hiện các nhiệm vụ phức tạp dựa trên hướng dẫn của con người. Kết quả là, chúng tôi tiến hành tinh chỉnh hướng dẫn đa phương thức thêm.

Không giống như giai đoạn trước, chúng tôi làm cho LLM có thể huấn luyện trong quá trình tinh chỉnh hướng dẫn để khai thác toàn bộ tiềm năng của dữ liệu hướng dẫn chất lượng cao. Chúng tôi cấu trúc mỗi trường hợp dữ liệu dưới dạng một đoạn hội thoại theo Vicuna (Chiang et al. 2023) và huấn luyện mô hình để giải mã các token của các đoạn trả lời. Chúng tôi sử dụng cùng mục tiêu huấn luyện, như được biểu thị bởi Phương trình (2), được sử dụng trong giai đoạn tiền huấn luyện.

4 UniMM-Chat

Để xây dựng các tập dữ liệu tinh chỉnh hướng dẫn chất lượng cao và đa dạng với các mô tả hình ảnh toàn diện, chúng tôi đề

--- TRANG 4 ---
xuất tập dữ liệu UniMM-Chat, bao gồm 1.1M hướng dẫn đa dạng. Chúng tôi kết hợp các chú thích bổ sung từ các tập dữ liệu VL khác nhau và sử dụng ChatGPT để tạo ra các cuộc đối thoại đa lượt tương ứng với mỗi hình ảnh. Như được thể hiện trong Hình 3, các chú thích được kết hợp cung cấp một ngữ cảnh hình ảnh phong phú hơn và hiệu quả trao quyền cho ChatGPT để tạo ra các tập dữ liệu hội thoại chuyên sâu về kiến thức hơn.

[Hình 3: Minh họa khung được thiết kế để xây dựng tập dữ liệu UniMM-Chat. UniMM-Chat kết hợp các tập dữ liệu VL khác nhau để tạo ra các cuộc đối thoại chuyên sâu về kiến thức. Các đoạn văn bản được tô sáng bằng màu sắc cho thấy kiến thức khác nhau từ các chú thích gốc cần thiết để trả lời các câu hỏi.]

4.1 Xây dựng Tập dữ liệu

Năm tập dữ liệu VL thường được sử dụng, như được nêu trong Bảng 2, đóng vai trò là hạt giống để tạo ra các hướng dẫn đa phương thức. Vì hình ảnh trong năm tập dữ liệu này được lấy từ COCO (Lin et al. 2014), chúng tôi đầu tiên tổng hợp các chú thích cho mỗi hình ảnh từ các tập dữ liệu VL hạt giống. Đối với VQAv2 (Goyal et al. 2017), OKVQA (Marino et al. 2019), và Visual Dialog (Das et al. 2017), chúng tôi sử dụng chú thích của cả câu hỏi và câu trả lời tương ứng của chúng. Đối với AOKVQA (Schwenk et al. 2022), chúng tôi sử dụng cặp câu hỏi-trả lời và các lý luận được chú thích. Năm chú thích từ COCO (Lin et al. 2014) được sử dụng trực tiếp làm mô tả cơ bản cho mỗi hình ảnh.

Tiếp theo, những chú thích này được cấu trúc một cách tỉ mỉ thành một định dạng tinh chế, kết hợp một số trường hợp học ít-shot được viết bổ sung bởi con người. Những yếu tố này được trình bày chung như các lời nhắc, khuyến khích ChatGPT tạo ra các cuộc đối thoại đa lượt tập trung vào các hình ảnh tương ứng. Chúng tôi giới thiệu độc giả đến Phụ lục cho các lời nhắc mà chúng tôi đã sử dụng trong quá trình xây dựng dữ liệu.

4.2 Thống kê Tập dữ liệu

Tổng cộng, chúng tôi thu thập 117,238 cuộc đối thoại, với trung bình 9.89 lượt mỗi cuộc đối thoại. Mỗi cuộc đối thoại được liên kết với một hình ảnh riêng biệt. Để định lượng sự đa dạng của tập dữ liệu, chúng tôi theo (Wang et al. 2023b) và phân tích các loại câu hỏi và danh từ trực tiếp hoặc động từ của chúng với công cụ Berkeley Neural Parser (Stern, Andreas, và Klein 2017). Chúng tôi vẽ biểu đồ bảy loại câu hỏi phổ biến nhất và các đối tượng danh từ trực tiếp hoặc động từ hàng đầu của chúng, chiếm 44% hướng dẫn trong UniMM-Chat trong Hình 4. Biểu đồ này nhấn mạnh độ rộng đáng kể của ý định và định dạng trong UniMM-Chat.

[Hình 4: Phân bố hướng dẫn trong UniMM-Chat.]

4.3 UniMM-Bench

Chúng tôi đề xuất UniMM-Bench, một điểm chuẩn trả lời câu hỏi được thiết kế cho MLLMs để đánh giá các khả năng liên quan đến lý luận và kiến thức thế giới. Vì độ chính xác khớp chính xác truyền thống không phù hợp để đánh giá MLLMs, thường phản hồi một câu hoàn chỉnh để trả lời câu hỏi, chúng tôi tận dụng GPT-4 để chấm điểm câu trả lời được tạo ra. Xem xét chi phí đánh giá, chúng tôi lấy mẫu một trăm mẫu từ tập kiểm tra của OKVQA (Marino et al. 2019), AOKVQA (Schwenk et al. 2022), GQA (Hudson và Man-

[Bảng 2: Thống kê của các tập dữ liệu thị giác-ngôn ngữ nguồn để xây dựng UniMM-Chat.]

--- TRANG 5 ---
ning 2019) và VQAv2 (Goyal et al. 2017), tương ứng, có các chú thích đã trải qua kiểm tra tỉ mỉ. Điểm chuẩn này đánh giá khả năng của MLLMs trong lý luận, thường thức, và kiến thức thế giới (Schwenk et al. 2022).

5 Thí nghiệm

5.1 Cài đặt Thí nghiệm

Chi tiết Đánh giá. Chúng tôi đánh giá MLLMs trên UniMM-Bench được đề xuất và tập kiểm tra LLaVA (Liu et al. 2023). Tập kiểm tra LLaVA bao gồm 90 câu hỏi từ ba danh mục trải dài hội thoại, lý luận phức tạp, và mô tả chi tiết. UniMM-Bench chủ yếu đánh giá khả năng của mô hình trong lý luận và kiến thức thế giới, trong khi tập kiểm tra LLaVA đánh giá hiệu suất của mô hình trên hội thoại đa phương thức. Chúng tôi tận dụng GPT-4 để chấm điểm đầu ra của mô hình dựa trên câu trả lời chân lý cơ sở. Chúng tôi đã xác minh thực nghiệm rằng điểm số của GPT-4 được căn chỉnh tốt với đánh giá của con người. Chúng tôi giới thiệu độc giả đến Phụ lục cho các lời nhắc hoàn chỉnh.

Chi tiết Huấn luyện. Tiền huấn luyện của Muffin được thực hiện với 180M cặp hình ảnh-văn bản được thu thập từ Visual Genome (Krishna et al. 2017), COCO (Lin et al. 2014), CC3M (Sharma et al. 2018), CC12M (Changpinyo et al. 2021) và LAION-COCO (Christoph Schuhmann 2022) và kéo dài trong 100K bước với kích thước lô 2048 và tốc độ học 1e-4. Đối với tinh chỉnh hướng dẫn, chúng tôi sử dụng cả tập dữ liệu tinh chỉnh hướng dẫn LLaVA-Instruct-150K và UniMM-Chat. Việc huấn luyện kéo dài trong 3200 bước với kích thước lô 512 và tốc độ học 2e-5. Chúng tôi áp dụng độ phân giải 448 trong quá trình tiền huấn luyện và 672 trong giai đoạn tinh chỉnh hướng dẫn.

Đường cơ sở. Chúng tôi so sánh phương pháp của mình với một loạt các đường cơ sở mạnh hiện tại:

• MiniGPT-4: MiniGPT-4 (Zhu et al. 2023) là một trong những thử nghiệm mã nguồn mở sớm nhất của MLLMs, được tinh chỉnh trên hơn 3.5K hướng dẫn đơn giản yêu cầu mô hình tạo ra mô tả hình ảnh.

• VisualGLM: VisualGLM (Du et al. 2022) là một mô hình trợ lý đa phương thức song ngữ được xây dựng trên ChatGLM-6B và bộ mã hóa thị giác của BLIP2 thúc đẩy quá trình huấn luyện căn chỉnh đặc trưng phức tạp.

• Ziya-Visual: (Wang et al. 2022) là một mô hình trợ lý đa phương thức song ngữ dựa trên Ziya-LLaMA-13B và bộ mã hóa thị giác tiền huấn luyện của BLIP-2.

• mPLUG-owl: mPLUG-owl (Ye et al. 2023) là một mô hình trợ lý đa phương thức dựa trên CLIP ViT-L/14 và LLaMA-7B, sử dụng cả các tập dữ liệu tinh chỉnh hướng dẫn chỉ có văn bản và đa phương thức.

• InstructBLIP: InstructBLIP (Dai et al. 2023) xây dựng một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức dựa trên 26 tập dữ liệu công cộng bằng cách áp dụng các mẫu được định nghĩa trước để trực tiếp định dạng những tập dữ liệu này thành một định dạng thống nhất. Họ thiết kế một Q-Former nhận biết hướng dẫn mới và huấn luyện mô hình trên tập dữ liệu được đề xuất.

• LLaVA: LLaVA (Liu et al. 2023) xây dựng 150K hướng dẫn đa phương thức dựa trên tập dữ liệu COCO. Nó đơn giản tận dụng một bộ chiếu tuyến tính để kết nối bộ mã hóa thị giác và LLM.

5.2 Kết quả Chính

Bảng 3 trình bày hiệu suất của Muffin và các đường cơ sở trên UniMM-Bench và tập kiểm tra LLaVA. Trên cả hai điểm chuẩn này, Muffin đạt được hiệu suất tiên tiến và vượt trội đáng kể so với tất cả các đường cơ sở.

Dựa trên những kết quả thí nghiệm này, chúng tôi có những quan sát sau:

• So với LLaVA, Muffin đạt được sự tiến bộ ấn tượng 5.1 điểm trên trung bình. Ngoài ra, ngay cả khi sử dụng cùng tập dữ liệu LLaVA-Instruct-150K để tinh chỉnh hướng dẫn, tương tự như huấn luyện của LLaVA, Muffin vẫn đạt được kết quả tốt hơn với 1.2 điểm, chứng minh hiệu quả của khung Muffin.

• So với InstructBLIP, Muffin thể hiện những cải thiện hiệu suất đáng kể so với InstructBLIP. Cụ thể, mặc dù huấn luyện trực tiếp trên OKVQA, AOKVQA, và VQAv2, InstructBLIP đạt được hiệu suất thấp hơn trên những tập dữ liệu này so với Muffin, đặc biệt là trên OKVQA và AOKVQA, chứa các chú thích hạn chế. Điều này cho thấy việc đơn giản kết hợp các mẫu huấn luyện của các tập dữ liệu khác nhau là không tối ưu cho mô hình học một loạt rộng kiến thức. Trên tập kiểm tra LLaVA, chúng tôi giả định rằng định dạng hạn chế của các phản hồi trong dữ liệu huấn luyện làm tổn hại khả năng tạo sinh và do đó dẫn đến InstructBLIP tụt hậu đáng kể so với Muffin.

• Loại bỏ UniMM-Chat khỏi tập huấn luyện dẫn đến sự giảm hiệu suất đáng kể trên tất cả các nhiệm vụ trả lời câu hỏi thị giác. Điều này nhấn mạnh vai trò then chốt của UniMM-Chat trong việc trang bị cho MLLMs các kỹ năng để giải quyết hiệu quả nhiều loại nhiệm vụ.

Những kết quả này chung cộng chứng minh hiệu quả của khung Muffin và nêu bật vai trò quan trọng của UniMM-Chat. Được làm phong phú bởi UniMM-Chat, Muffin thể hiện khả năng lý luận mạnh mẽ và kiến thức phong phú.

[Bảng 3: Hiệu suất của Muffin được đề xuất và các đường cơ sở trên UniMM-Bench và Tập Kiểm tra LLaVA.]

5.3 Đánh giá Con người

Để phân tích toàn diện hơn, chúng tôi tiến hành đánh giá con người theo cặp của các mô hình khác nhau trên một loạt đa dạng các hướng dẫn. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên tám mười mẫu từ UniMM-Bench và hai mười mẫu từ tập kiểm tra LLaVA. Chúng tôi tuyển dụng sáu người chú thích có học vấn tốt và trình bày cho họ các cặp câu trả lời được tạo ra bởi Muffin và ba đường cơ sở khác, LLaVA, InstructBLIP, và MiniGPT-4. Chúng tôi đảm bảo rằng tên mô hình được ẩn trong suốt quá trình

[Hình 5: Tỷ lệ thắng trong đánh giá con người của Muffin so với các mô hình đường cơ sở khác nhau.]

--- TRANG 6 ---
đánh giá. Các người chú thích được yêu cầu quyết định câu trả lời nào tốt hơn trong mỗi cặp, dựa trên ba tiêu chí: Hữu ích, Chính xác, và Nhất quán câu hỏi-trả lời. Chi tiết hơn được giới thiệu trong Phụ lục. Kết quả đánh giá được thể hiện trong Hình 5. Muffin vượt trội hơn tất cả các mô hình khác trên tất cả các chỉ số. Lợi thế rõ ràng về Hữu ích và Chính xác bắt nguồn từ việc huấn luyện trên các cuộc đối thoại chuyên sâu về kiến thức từ tập dữ liệu UniMM-Chat. Cụ thể, chúng tôi thấy MiniGPT-4 (Zhu et al. 2023), được huấn luyện chỉ với vài nghìn mẫu hướng dẫn đơn giản, thường phản hồi không liên quan đến câu hỏi và do đó đạt được tỷ lệ thắng nhất quán thấp nhất. Chúng tôi giới thiệu độc giả đến Phụ lục cho thống kê chi tiết của kết quả đánh giá con người so sánh Muffin với các mô hình đường cơ sở.

5.4 Kết quả Khấu trừ

Tái định dạng các tập dữ liệu VL. Để xác minh hiệu quả của khung xây dựng tập dữ liệu, chúng tôi xây dựng một biến thể của UniMM-Chat mà không hợp nhất các chú thích qua các tập dữ liệu khác nhau, có tên UniMM-Chat-sep. Chi tiết hơn về UniMM-Chat-sep được trình bày trong Phụ lục. Chúng tôi huấn luyện nhiều mô hình sử dụng các cấu hình dữ liệu khác nhau và trình bày kết quả trong Bảng 4. Trên tập kiểm tra LLaVA, việc sử dụng các tập dữ liệu gốc không thể thiết lập một mô hình trò chuyện do văn bản ngắn trong những tập dữ liệu này. Kết hợp UniMM-Chat-sep, trong khi dẫn đến hiệu suất cải thiện so với việc sử dụng các tập dữ liệu gốc, vẫn mang lại kết quả không tối ưu do thông tin hạn chế có sẵn trong quá trình xây dựng. Trên UniMM-Bench, việc sử dụng trực tiếp các tập dữ liệu gốc tương ứng với tinh chỉnh trong miền, đóng vai trò là giới hạn trên hiệu suất cho tập dữ liệu được xây dựng. Kết quả thí nghiệm cho thấy rằng việc sử dụng UniMM-Chat không dẫn đến sự giảm hiệu suất trong miền, trong khi việc sử dụng UniMM-Chat-sep sẽ trải qua sự suy giảm hiệu suất. Những kết quả này nhấn mạnh sự cần thiết của khung xây dựng tập dữ liệu của chúng tôi.

[Bảng 4: Muffin được huấn luyện với các cài đặt tập dữ liệu tinh chỉnh hướng dẫn khác nhau.]

Cài đặt Huấn luyện. Chúng tôi cũng phân tích hiệu ứng của một số cài đặt huấn luyện và trình bày kết quả trong Bảng 5. Về độ phân giải hình ảnh, khi giảm độ phân giải hình ảnh đầu vào trong quá trình tiền huấn luyện (224) và tinh chỉnh hướng dẫn

[Bảng 5: Kết quả khấu trừ cho các cài đặt huấn luyện khác nhau.]

--- TRANG 7 ---
(448), hiệu suất có sự giảm rõ rệt trên UniMM-Bench, vì thông tin hình ảnh chi tiết cần thiết để giải quyết các nhiệm vụ đa phương thức phức tạp khó được giữ lại với độ phân giải thấp. Ngoài ra, chúng tôi cũng quan sát thấy việc đông lạnh LLM trong quá trình tinh chỉnh hướng dẫn có thể hạn chế khả năng của mô hình trên tất cả các tập dữ liệu được đánh giá.

5.5 Kết quả Định tính

Nhờ vào sự hợp nhất sâu trong VLM và dữ liệu hướng dẫn chuyên sâu về kiến thức từ UniMM-Chat, Muffin có thể hiệu quả kích hoạt kiến thức được nhúng trong LLM và tạo ra phản hồi hữu ích hơn cho các câu hỏi mở. Hình 6 cho thấy một số ví dụ về phản hồi từ Muffin, LLaVA, và InstructBLIP để minh họa. Trong ví dụ đầu tiên, mô hình của chúng tôi chính xác xác định quốc gia xuất xứ của máy bay dựa trên chi tiết hình ảnh, trong khi cả LLaVA và InstructBLIP đều không cung cấp câu trả lời đúng, làm nổi bật sự vượt trội của mô hình chúng tôi trong việc hiểu và xác định chi tiết hình ảnh. Trong ví dụ thứ hai, mô hình của chúng tôi kết hợp các manh mối văn bản từ hình ảnh với kiến thức vốn có của nó để tạo ra một phản hồi chính xác hơn. Hơn nữa, trong ví dụ thứ tư, Muffin chính xác xác định tên chính xác của trang phục truyền thống Ấn Độ được thể hiện trong hình ảnh. Ngoài việc có khả năng trả lời câu hỏi với một loạt rộng kiến thức, Muffin cũng có thể tạo ra các phản hồi hữu ích hơn. Trong ví dụ cuối được thể hiện trong 6, mặc dù cả hai đều chỉ ra rau củ là hành tây, Muffin đưa ra phản hồi chi tiết và hữu ích hơn.

[Hình 6: Các ví dụ được tạo ra bởi Muffin và các đường cơ sở khác.]

6 Kết luận

Trong bài báo này, chúng tôi trình bày Muffin, một khung sáng tạo để trực tiếp sử dụng các VLMs tiền huấn luyện để kết nối tín hiệu thị giác và LLMs. Ngoài ra, chúng tôi phát triển một mô hình mới để xây dựng tập dữ liệu tinh chỉnh hướng dẫn đa phương thức bằng cách hợp nhất các chú thích từ các tập dữ liệu khác nhau mô tả cùng một hình ảnh. Bằng cách này, chúng tôi xây dựng một tập dữ liệu tinh chỉnh hướng dẫn đa phương thức chất lượng cao và đa dạng, UniMM-Chat. Chúng tôi thực hiện các thí nghiệm toàn diện để chứng minh hiệu quả của Muffin và UniMM-Chat, cho thấy rằng Muffin đạt được hiệu suất tiên tiến trên một loạt rộng các nhiệm vụ. Trong tương lai, chúng tôi sẽ áp dụng khung Muffin và tập dữ liệu UniMM-Chat cho nhiều kết hợp của VLMs và LLMs hơn.

Tài liệu tham khảo

[Tiếp tục với danh sách tài liệu tham khảo đầy đủ...]

--- TRANG 8 ---
[Tiếp tục từ trang trước với các phần phụ lục...]

Phụ lục

A Lời nhắc

Trong phần này, chúng tôi liệt kê chi tiết của tất cả các lời nhắc mà chúng tôi sử dụng trong công trình này để có thể tái tạo, bao gồm các lời nhắc để xây dựng UniMM-Chat, lời nhắc để đánh giá cho UniMM-Bench và lời nhắc được sử dụng để tiền huấn luyện Muffin.

A.1 Lời nhắc Xây dựng UniMM-Chat

Chúng tôi thể hiện lời nhắc đầy đủ mà chúng tôi đã sử dụng để yêu cầu ChatGPT tạo ra các cuộc đối thoại chất lượng cao chuyên sâu về kiến thức cho UniMM-Chat trong Bảng 7. Chúng tôi trình bày lời nhắc thô và cách chúng tôi tổ chức các minh họa được chú thích bởi con người cùng với lời nhắc thô. Chúng tôi cũng liệt kê lời nhắc để tập hợp các chú thích gốc từ các tập dữ liệu VL khác nhau trong Bảng 6, tạo ra đầu vào để được sử dụng trong Bảng 7. Chúng tôi cũng trình bày phân bố độ dài câu hỏi và câu trả lời của UniMM-Chat trong Hình 7 để tham khảo.

[Các bảng và hình tiếp theo với thông tin chi tiết về lời nhắc và thống kê...]

--- TRANG 9-12 ---
[Tiếp tục với các phần còn lại của phụ lục bao gồm các lời nhắc đánh giá, ví dụ từ UniMM-Bench, thống kê đánh giá con người và các chi tiết kỹ thuật khác...]
