# 2310.19909.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.19909.pdf
# Kích thước tệp: 766691 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cuộc Chiến của các Backbone: So Sánh Quy Mô Lớn về các Mô Hình Được Huấn Luyện Trước trong các Tác Vụ Thị Giác Máy Tính
Micah Goldblum1∗Hossein Souri2∗Renkun Ni3Manli Shu3Viraj Prabhu4
Gowthami Somepalli3Prithvijit Chattopadhyay4Mark Ibrahim6Adrien Bardes5,6
Judy Hoffman4Rama Chellappa2Andrew Gordon Wilson1Tom Goldstein3

Tóm tắt
Các hệ thống thị giác máy tính dựa trên mạng nơ-ron thường được xây dựng trên một backbone, một bộ trích xuất đặc trưng được huấn luyện trước hoặc khởi tạo ngẫu nhiên. Vài năm trước, lựa chọn mặc định là một mạng nơ-ron tích chập được huấn luyện trên ImageNet. Tuy nhiên, thời gian gần đây đã chứng kiến sự xuất hiện của vô số backbone được huấn luyện trước bằng các thuật toán và bộ dữ liệu khác nhau. Trong khi sự phong phú lựa chọn này đã dẫn đến việc tăng hiệu suất cho một loạt hệ thống, việc các nhà thực hành đưa ra quyết định sáng suốt về backbone nào nên chọn trở nên khó khăn. Cuộc Chiến của các Backbone (BoB) làm cho lựa chọn này dễ dàng hơn bằng cách đánh giá một bộ sưu tập đa dạng các mô hình được huấn luyện trước, bao gồm các mô hình thị giác-ngôn ngữ, những mô hình được huấn luyện thông qua học tự giám sát, và backbone Stable Diffusion, trên một tập đa dạng các tác vụ thị giác máy tính từ phân loại đến phát hiện đối tượng đến khái quát hóa OOD và nhiều hơn nữa. Hơn nữa, BoB làm sáng tỏ các hướng đầy hứa hẹn cho cộng đồng nghiên cứu để thúc đẩy thị giác máy tính bằng cách làm rõ điểm mạnh và điểm yếu của các phương pháp hiện tại thông qua một phân tích toàn diện được thực hiện trên hơn 1500 lần chạy huấn luyện. Trong khi các transformer thị giác (ViT) và học tự giám sát (SSL) ngày càng phổ biến, chúng tôi thấy rằng các mạng nơ-ron tích chập được huấn luyện theo cách có giám sát trên các tập huấn luyện lớn vẫn hoạt động tốt nhất trên hầu hết các tác vụ trong số các mô hình chúng tôi xem xét. Hơn nữa, trong các so sánh táo-với-táo trên cùng kiến trúc và bộ dữ liệu huấn luyện trước có kích thước tương tự, chúng tôi thấy rằng các backbone SSL có tính cạnh tranh cao, cho thấy rằng các nghiên cứu tương lai nên thực hiện huấn luyện trước SSL với các kiến trúc tiên tiến và bộ dữ liệu huấn luyện trước lớn hơn. Chúng tôi phát hành kết quả thô của các thí nghiệm cùng với mã cho phép các nhà nghiên cứu đưa backbone của riêng họ qua thử thách tại đây: https://github.com/hsouri/Battle-of-the-Backbones .

1 Giới thiệu
Mô hình chiếm ưu thế để xây dựng các hệ thống thị giác máy bao gồm một mạng trích xuất đặc trưng, còn được gọi là backbone, kết nối với một head đặc thù cho tác vụ. Backbone có thể xuất ra một mảng đặc trưng dày đặc cho phát hiện và định vị đối tượng, hoặc một vector đặc trưng đơn cho phân loại hoặc truy xuất hình ảnh. Trong khi các backbone có thể được huấn luyện từ đầu trên dữ liệu đặc thù cho tác vụ, nhiều backbone có sẵn được huấn luyện trước trên các bộ dữ liệu chuẩn lớn và sau đó được tinh chỉnh cho tác vụ cần giải quyết. Phương pháp học chuyển giao này có một số ưu điểm. Đầu tiên, nó giảm đáng kể yêu cầu dữ liệu đặc thù cho ứng dụng của học sâu và đã dẫn đến cải thiện hiệu suất trên một loạt rộng

∗Các tác giả đóng góp ngang nhau. Liên hệ với goldblum@nyu.edu và hsouri1@jhu.edu . Công trình này được thực hiện tại Đại học New York1, Đại học Johns Hopkins2, Đại học Maryland3, Viện Công nghệ Georgia4, Inria5, và Meta AI Research6.

37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2310.19909v2  [cs.CV]  20 Nov 2023

--- TRANG 2 ---
ứng dụng. Thứ hai, nó có thể tăng tốc độ huấn luyện và giảm chi phí tính toán ngay cả khi có sẵn lượng lớn dữ liệu đặc thù cho tác vụ [29]. Cuối cùng, các bộ dữ liệu huấn luyện trước thường chứa hình ảnh từ nhiều miền khác biệt, dẫn đến tính bền vững của mô hình có thể được chuyển giao sang các tác vụ xuôi dòng.

Các hệ thống thị giác dựa trên học sâu ban đầu phụ thuộc nhiều vào huấn luyện trước ImageNet [23,59]. Ngược lại, các nhà thực hành ngày nay có quyền truy cập vào một kho tàng lựa chọn, với các mô hình được huấn luyện trước khác nhau dẫn đến sự khác biệt hiệu suất đáng kể. Có ba yếu tố chính ảnh hưởng đến hiệu suất của một mô hình như vậy: kiến trúc của nó, thuật toán huấn luyện trước, và bộ dữ liệu huấn luyện trước. Mỗi chiều thiết kế này trình bày nhiều lựa chọn, dẫn đến một loạt lựa chọn choáng ngợp cho các nhà thực hành xây dựng hệ thống thị giác máy tính. Bất chấp sự đa dạng rộng lớn của các lựa chọn này, các nhà thực hành không có tài nguyên nào để tham khảo và thay vào đó bị bỏ lại để ghép nối kết quả từ các bài báo phương pháp hoặc tự kiểm tra các backbone.

Chúng tôi đưa các backbone này vào một Cuộc Chiến của các Backbone (BoB). BoB so sánh nhiều checkpoint được huấn luyện trước phổ biến có sẵn công khai, cũng như các baseline khởi tạo ngẫu nhiên, trên một loạt rộng các tác vụ xuôi dòng bao gồm phân loại hình ảnh trên hình ảnh tự nhiên, y tế và vệ tinh (Phần 3.1), phát hiện và phân đoạn đối tượng (Phần 3.2), khái quát hóa ngoài phân phối (Phần 3.3), và truy xuất hình ảnh (Phần 3.4).

Ngoài việc hỗ trợ các nhà thực hành xây dựng hệ thống thị giác máy tính, một mục tiêu trung tâm khác của benchmark này là giúp hướng dẫn cộng đồng nghiên cứu hướng tới các hướng nghiên cứu hiệu quả trong việc tìm kiếm thiết kế backbone tốt hơn. BoB làm sáng tỏ điểm mạnh và điểm yếu của các quy trình huấn luyện trước và kiến trúc, tiết lộ các quan niệm sai lầm phổ biến và hạn chế cơ bản, cũng như các hướng cải tiến đầy hứa hẹn. Dưới đây, chúng tôi tóm tắt một số phát hiện chính và thảo luận về các nỗ lực trước đây để so sánh các backbone.

1.1 Cuộc Chiến của các Backbone: Tóm tắt ngắn gọn
Các phần tiếp theo trong bài báo này chứa nhiều chi tiết thí nghiệm. Do đó, chúng tôi chắt lọc một số phát hiện chính dưới đây:

▷Trên bộ đánh giá toàn diện trong BoB, bao gồm các tác vụ, bộ dữ liệu và cài đặt (bao gồm ID và OOD), ConvNeXt-Base có giám sát, SwinV2-Base có giám sát được huấn luyện bằng ImageNet-21k, và CLIP ViT-Base nổi lên dẫn đầu. Những người chiến thắng tương tự cũng thắng ở quy mô nhỏ hơn. Trong số các backbone nhỏ hơn, ConvNeXt-Tiny và SwinV2-Tiny xuất hiện chiến thắng, theo sau bởi DINO ViT-Small.

[BIỂU ĐỒ: Hiển thị mối tương quan hiệu suất giữa các tác vụ với các mô hình khác nhau được biểu diễn theo tham số, kiến trúc và phương pháp huấn luyện trước]

Hình 1: Hiệu suất có mối tương quan giữa các tác vụ. Hiệu suất cho mỗi mô hình được báo cáo theo độ lệch chuẩn trên/dưới trung bình qua các bộ dữ liệu. Trái: So sánh giữa phân loại và phát hiện. Phải: So sánh giữa phân loại và phân loại OOD.

--- TRANG 3 ---
▷Bất chấp sự chú ý gần đây dành cho các kiến trúc dựa trên transformer và học tự giám sát, các mạng tích chập hiệu suất cao được huấn luyện trước thông qua học có giám sát vượt trội hơn transformer trên đa số các tác vụ chúng tôi xem xét.

▷Sự vượt trội quan sát được của huấn luyện trước có giám sát xảy ra bởi vì các mô hình như vậy thường được huấn luyện trên các bộ dữ liệu lớn hơn. Trong các so sánh táo-với-táo trên cùng quy mô bộ dữ liệu, các mô hình SSL vượt trội hơn các đối tác có giám sát của chúng.

▷ViT nhạy cảm hơn với lượng dữ liệu huấn luyện trước và số lượng tham số so với CNN.

▷Hiệu suất giữa các tác vụ có mối tương quan mạnh – các backbone hiệu suất cao nhất trong BoB có xu hướng tốt phổ quát trên các tác vụ và cài đặt. Xem Hình 1.

1.2 Các Benchmark Trước đây
Trong phần lớn thập kỷ qua, các backbone phổ biến nhất được huấn luyện trước trên ImageNet [17]. Kể từ năm 2020, SimCLR [10] và CLIP [73] đã phổ biến các backbone tự giám sát và sinh ra nhiều nghiên cứu mới. Trong khi các bài báo phương pháp đề xuất quy trình huấn luyện trước mới thường so sánh với các đối thủ cạnh tranh tương tự trên một số tác vụ xuôi dòng, chúng tôi tập trung trong phần này vào các nghiên cứu đặc biệt đánh giá các bộ sưu tập lớn backbone trên các tác vụ đa dạng.

Năm 2019, Goyal et al. [25] so sánh các mô hình AlexNet [47] và ResNet-50 [28] được huấn luyện trước bằng các tác vụ pretext tô màu và jigsaw với các mô hình học có giám sát, thấy rằng học có giám sát vượt trội massively hơn SSL vào thời điểm đó. Kolesnikov et al. [44] tương tự so sánh một số tác vụ pretext và kiến trúc mạng nơ-ron tích chập, cho thấy rằng các tiến bộ kiến trúc trong học có giám sát không phải lúc nào cũng chuyển giao sang cải thiện học tự giám sát. Kornblith et al. [45] thay vào đó đánh giá khả năng chuyển giao của các mô hình học có giám sát được huấn luyện trên ImageNet trên các tác vụ phân loại xuôi dòng, thay đổi kiến trúc và thấy rằng mối tương quan giữa hiệu suất xuôi dòng và độ chính xác kiểm tra ImageNet gần như hoàn hảo giữa các kiến trúc. Trong cùng năm, Zhai et al. [107] xây dựng Visual Task Adaptation Benchmark (VTAB) và kiểm tra các phương pháp học tự giám sát khác nhau bao gồm VAE và discriminator GAN, cũng thể hiện hiệu suất chiếm ưu thế của các mô hình học có giám sát. Năm 2020, Ericsson et al. [21] đánh giá các mô hình ResNet-50 được huấn luyện trên ImageNet bằng các thuật toán SSL khác nhau, thấy rằng hiệu suất của các thuật toán SSL hiện có khi đó trên một tập phong phú hơn các tác vụ xuôi dòng có mối tương quan mạnh với độ chính xác kiểm tra ImageNet-1k của chúng và thấy hiệu suất cải thiện của các thuật toán SSL mới hơn so với các nghiên cứu trước đây.

Kể từ các nghiên cứu trên, các thuật toán huấn luyện trước cùng với các tập huấn luyện và kiến trúc của chúng đã có tiến bộ to lớn, và trong khi học có giám sát trước đây là phương pháp mặc định cho huấn luyện trước, các lựa chọn bây giờ là vô tận. Do đó, việc đánh giá benchmark các backbone xứng đáng được chú ý mới. Xem Phụ lục A để khảo sát thêm về các benchmark đặc thù cho tác vụ.

2 Hướng dẫn về BoB
Trong số các đặc điểm phân biệt của các backbone đa dạng cạnh tranh trong cuộc chiến của chúng tôi là kiến trúc, quy trình huấn luyện trước, và các bộ dữ liệu mà chúng được huấn luyện trước. Bảng 1 chứa tổng quan về các backbone chúng tôi đánh giá bao gồm thuật toán huấn luyện trước, bộ dữ liệu huấn luyện trước, và kiến trúc của chúng. Chúng tôi cũng cung cấp mô tả chi tiết hơn về các đặc điểm này và các checkpoint được huấn luyện trước chính xác chúng tôi sử dụng trong Phụ lục B.

Lưu ý về Quy mô và So sánh Táo-với-Táo. Nhiều nhà thực hành có tính toán hạn chế và hơn nữa sẽ cần tinh chỉnh siêu tham số trên bộ dữ liệu riêng của họ mà không vượt quá ngân sách tính toán. Để mô phỏng kịch bản này, chúng tôi thực hiện các lần quét siêu tham số vừa phải, chúng tôi loại trừ các lịch trình huấn luyện đặc biệt dài, và chúng tôi không xem xét các kiến trúc lớn hơn ConvNeXt-Base, ngoại trừ backbone Stable Diffusion không có kích thước nhỏ hơn. Các lưới siêu tham số cụ thể được chi tiết trong các phần tiếp theo. Hơn nữa, chúng tôi chỉ sử dụng các checkpoint có sẵn công khai cũng sẽ có thể truy cập được cho các nhà thực hành. Các checkpoint có sẵn được huấn luyện trước với lượng tinh chỉnh siêu tham số khác nhau, và các thuật toán huấn luyện trước khác nhau được huấn luyện trên các bộ dữ liệu và kiến trúc khác nhau làm cho việc so sánh táo-với-táo chính xác trở nên không khả thi. Tuy nhiên, việc so sánh các checkpoint hiện có này là phù hợp cho các nhà thực hành, vì nó đại diện cho điều kiện thực tế, và chúng tôi sử dụng các lần quét siêu tham số có kích thước giống hệt nhau cho mỗi backbone trên các tác vụ xuôi dòng.

--- TRANG 4 ---
Bảng 1: Tóm tắt về các backbone chúng tôi đánh giá. Các cột tương ứng với thuật toán huấn luyện trước, phân loại thô, bộ dữ liệu huấn luyện trước, và các kiến trúc chúng tôi bao gồm. Mô tả chi tiết về mỗi thuật toán, bộ dữ liệu huấn luyện trước, và kiến trúc có thể được tìm thấy trong Phụ lục B.

| Huấn luyện trước | Phong cách | Bộ dữ liệu | Kiến trúc(s) |
|------------------|------------|------------|---------------|
| MoCo v3 [12] | SSL | ImageNet-1k [17] | ViT [18] |
| VICReg [3] | SSL | ImageNet-1k | ResNet [28] |
| VICRegL [4] | SSL | ImageNet-21k | ConvNeXt [58] |
| DINO [8] | SSL | ImageNet-1k | ResNet, ViT |
| MAE [30] | SSL | ImageNet-1k | ViT |
| Stable Diffusion [77] | Thị giác-Ngôn ngữ | LAION-2B [81] | Stable Diffusion encoder |
| CLIP [73] | Thị giác-Ngôn ngữ | LAION-2B, CLIP | ResNet, ViT |
| MiDaS [75] | Có giám sát | 12 × Bộ dữ liệu Độ sâu | SwinV2 [57] |
| Phân loại hình ảnh | Có giám sát | ImageNet-21k,-1k | Tất cả kiến trúc trên |
| Khởi tạo ngẫu nhiên | Không có | N/A | Tất cả kiến trúc trên |

2.1 Các Tác vụ
Để thăm dò toàn diện khả năng của các backbone, chúng tôi đánh giá hiệu suất của chúng cả được tinh chỉnh và đóng băng trên một số tác vụ xuôi dòng thuộc các danh mục sau:

• **Phân loại**: Chúng tôi đo cả hiệu suất tinh chỉnh và linear probe của các backbone trên các tác vụ phân loại xuôi dòng khác nhau bao gồm bộ dữ liệu hình ảnh tự nhiên, y tế, hoặc vệ tinh trong Phần 3.1. Các tác vụ phân loại hình ảnh yêu cầu backbone trích xuất các đặc trưng xác định nội dung của foreground hình ảnh nhưng không nhất thiết là có bao nhiêu đối tượng hoặc chúng được định vị ở đâu trong hình ảnh.

• **Phát hiện và phân đoạn đối tượng**: Không giống như phân loại hình ảnh, các tác vụ dự đoán dày đặc yêu cầu các backbone trích xuất các đặc trưng chứa vị trí chính xác của các đối tượng, trên cơ sở pixel cho phân đoạn và với độ chính xác đủ để vẽ các hộp bao quanh cho phát hiện đối tượng. Chúng tôi đánh giá các backbone trên cả hai tác vụ này trong Phần 3.2.

• **Khái quát hóa ngoài phân phối**: Trong các ứng dụng thực tế, các hệ thống thị giác máy tính thường được triển khai trên dữ liệu không phản ánh phân phối tập huấn luyện của chúng. Ngay cả các mô hình hiệu suất cao cũng được biết là thất bại dưới các dịch chuyển miền [71,32]. Do đó, chúng tôi đánh giá khả năng của các mô hình để khái quát hóa sang các miền xuôi dòng mới trong Phần 3.3.

• **Truy xuất hình ảnh**: Truy xuất hình ảnh yêu cầu backbone khớp các hình ảnh giống nhau thông qua sự gần gũi trong không gian đặc trưng. Chúng tôi khám phá các tác vụ yêu cầu khớp hình ảnh đối với các tiêu chí khác nhau như nội dung ngữ nghĩa và sự tương tự thị giác trong Phần 3.4.

3 Thiết lập Thí nghiệm
Bây giờ chúng tôi mô tả thiết lập thí nghiệm cho mỗi tác vụ. Cụ thể, chúng tôi liệt kê các giao thức học, bộ dữ liệu, và các metric đánh giá. Tìm chi tiết thí nghiệm và triển khai đầy đủ trong Phụ lục C.

3.1 Phân loại
**Giao thức học**. Chúng tôi đánh giá các backbone được huấn luyện trước trên các bộ dữ liệu khác nhau dưới hai giao thức tinh chỉnh, theo các nghiên cứu trước [12,30,8,10]: **tinh chỉnh từ đầu đến cuối** (bao gồm các thí nghiệm chỉ với một số lượng nhỏ mẫu được gán nhãn) và **linear probing**. Trong kịch bản trước, chúng tôi tinh chỉnh toàn bộ mô hình từ đầu đến cuối trên một bộ dữ liệu cho trước hoặc trên một phần của nó, và chúng tôi đo độ chính xác trên phần kiểm tra. Trong kịch bản linear probing, chúng tôi trích xuất các đặc trưng từ backbone được huấn luyện trước đóng băng, và chỉ học một bộ phân loại tuyến tính trên đầu các biểu diễn được huấn luyện trước này. Hai giao thức này được sử dụng rộng rãi trong nghiên cứu trước để đánh giá chất lượng của các phương pháp huấn luyện trước như trong học tự giám sát [12, 30, 8, 10] và huấn luyện trước thị giác-ngôn ngữ [1, 106].

**Bộ dữ liệu và metric đánh giá**. Chúng tôi thực hiện các thí nghiệm trên 6 bộ dữ liệu phân loại hình ảnh phổ biến, bao gồm nhiều miền như hình ảnh tự nhiên (ImageNet-1K [17], CIFAR-100 [46], Flowers-102 [65], Aircraft [61]), hình ảnh vệ tinh (EuroSAT [31]), và dữ liệu X-quang y tế (CheXpert [37]) cho thấy khái quát hóa và khả năng chuyển giao của các backbone được huấn luyện trước. Tất cả bộ dữ liệu chúng tôi sử dụng đều có sẵn công khai, và chúng tôi liệt kê chi tiết của chúng bao gồm kích thước và số lượng lớp trong Phụ lục C. Đối với các thí nghiệm chỉ với một phần của tập huấn luyện, chúng tôi lấy mẫu ngẫu nhiên 1% và 10% của các mẫu huấn luyện và tinh chỉnh các backbone được huấn luyện trước trên các tập con này. Khi lấy mẫu các tập con, chúng tôi duy trì phân phối nhãn gốc của bộ dữ liệu. Lưu ý rằng chúng tôi chỉ xem xét khái quát hóa trong miền ở đây, nơi các phần huấn luyện và kiểm tra đến từ cùng nguồn.

Để đánh giá, chúng tôi đo độ chính xác phân loại và **Diện tích Dưới Đường cong ROC (AUC)** trên phần kiểm tra làm metric hiệu suất cho các tác vụ phân loại đơn nhãn và đa nhãn tương ứng. Ngoài điểm số tốt nhất trong số các vector siêu tham số, chúng tôi cũng vẽ độ chính xác cho một số epoch đầu để hiển thị tốc độ hội tụ của các backbone được huấn luyện trước khác nhau. Hơn nữa, chúng tôi đánh giá độ trễ và sử dụng bộ nhớ của mỗi backbone trên cùng thiết bị.

3.2 Phát hiện và Phân đoạn Đối tượng
**Giao thức học**. Đối với đánh giá phát hiện đối tượng và phân đoạn instance, chúng tôi sử dụng framework Cascade Mask R-CNN [5]. Chúng tôi thực hiện thí nghiệm với ba giao thức: (1) **huấn luyện từ đầu đến cuối từ khởi tạo ngẫu nhiên**, (2) **tinh chỉnh từ đầu đến cuối sử dụng backbone được huấn luyện trước**, và (3) **tinh chỉnh với backbone đóng băng**. Trong khi tinh chỉnh với backbone đóng băng là không điển hình trong phân đoạn và phát hiện, giao thức sau này cho phép chúng tôi thăm dò định vị trong các đặc trưng được trích xuất bởi các mô hình được huấn luyện trước và bổ sung cho các thí nghiệm phân loại linear probing. Xem Phụ lục C.1 để thảo luận về tiềm năng cho ViT, đặc biệt là những cái lớn, vượt trội hiệu suất của các mô hình khác dưới các giao thức huấn luyện đắt tiền hơn.

**Bộ dữ liệu và metric đánh giá**. Chúng tôi thực hiện đánh giá phát hiện đối tượng và phân đoạn instance trên bộ dữ liệu COCO phổ biến [54]. Chúng tôi theo metric average precision (AP) kiểu COCO, tính trung bình qua các ngưỡng Intersection over Union (IoU) khác nhau. Chúng tôi báo cáo box Average Precision (box AP), box AP@50, và AP@75 cho phát hiện đối tượng và mask Average Precision (mask AP), mask AP@50, và mask AP@75 cho phân đoạn instance [55].

3.3 Khái quát hóa Ngoài Phân phối
Trong khi các mạng hiện đại có thể thể hiện hiệu suất mạnh trên các phân phối dữ liệu chúng được huấn luyện, một lượng lớn nghiên cứu trước [71,32] đã thấy rằng hiệu suất của các mô hình như vậy có thể giảm đáng kể dưới các dịch chuyển phân phối. Ngoài việc đánh giá hiệu suất trong phân phối của các backbone trên một tập đa dạng các tác vụ xuôi dòng, chúng tôi cũng xem xét cách hiệu suất này chuyển giao sang các cài đặt ngoài phân phối (OOD).

**Giao thức học**. Một số bộ dữ liệu và benchmark đặc thù cho tác vụ đã được đề xuất để đánh giá tính bền vững của các mô hình đối với các lệch khỏi phân phối huấn luyện của chúng. Cụ thể, chúng tôi nghiên cứu khái quát hóa của các backbone được huấn luyện trên hai tác vụ, (1) **phân loại hình ảnh** và (2) **phát hiện đối tượng**, và trên hai loại dịch chuyển phân phối, (A) **biến đổi cấu trúc và phong cách trong ImageNet** và (B) **khái quát hóa tổng hợp-thực tế**.

**Bộ dữ liệu và metric đánh giá**. Chúng tôi xem xét các benchmark rộng sau đây cho đánh giá OOD:

(A) **Tính bền vững đối với thay đổi cấu trúc và phong cách**. Chúng tôi đo khái quát hóa OOD của các mô hình được huấn luyện hoặc tinh chỉnh trên ImageNet trên các benchmark sau: (i) **ImageNet-A** [34]. ImageNet-A(dversarial) chứa một tập con được tuyển chọn của hình ảnh kiểm tra ImageNet trải rộng 200 danh mục đặc biệt thách thức cho các mô hình sâu được huấn luyện. (ii) **ImageNet-V2** [76]. ImageNet-V2 là một tập kiểm tra bổ sung của hình ảnh giống ImageNet được thu thập một thập kỷ sau bộ dữ liệu gốc theo giao thức thu thập giống hệt. (iii) **ImageNet-R** [33]. ImageNet-R(endition) chứa các bản thể hiện nghệ thuật cho 200 danh mục từ ImageNet, bao gồm hoạt hình, graffiti, thêu, origami, tác phẩm điêu khắc, v.v. (iv) **ImageNet-S** [93]. ImageNet-S(ketch) là một bộ sưu tập hình ảnh phác thảo đen trắng được thu thập từ web và làm sạch thủ công từ các danh mục ImageNet.

(B) **Khái quát hóa Syn-to-real**. Chúng tôi cũng đo hiệu suất của các mô hình được huấn luyện trên dữ liệu tổng hợp và kiểm tra trên dữ liệu thực. Dữ liệu tổng hợp đã nổi lên như một thay thế phổ biến trong các cài đặt nơi có thể khó hoặc đắt để tuyển chọn dữ liệu thế giới thực được chú thích đáng tin cậy. Chúng tôi đo khái quát hóa syn-to-real cho phân loại hình ảnh và phát hiện đối tượng trên hai benchmark phổ biến sau: (i) **VisDA Syn → Real**. Benchmark phân loại VisDA bao gồm ~152k hình ảnh tổng hợp và ~55k hình ảnh thực trên 12 lớp. Hình ảnh tổng hợp trong VisDA là các kết xuất 3D của đối tượng từ nhiều góc nhìn và dưới các điều kiện ánh sáng khác nhau. Các đối tác thực là các cắt của 12 lớp thu được từ bộ dữ liệu COCO. (2) **Sim10k → Cityscapes**. Đối với phát hiện đối tượng, chúng tôi sử dụng Sim10k làm bộ dữ liệu huấn luyện tổng hợp và Cityscapes làm bộ dữ liệu đánh giá thực. Sim10k bao gồm ~10k hình ảnh đường phố (được vẽ từ GTA V). Cityscapes bao gồm ~5k hình ảnh đường phố được chú thích dày đặc được tuyển chọn từ góc nhìn xe cộ trong thế giới thực. Theo nghiên cứu trước [13], chúng tôi huấn luyện trên toàn bộ Sim10k để phát hiện các instance của "xe hơi" và đo hiệu suất phát hiện trên phần validation của Cityscapes.

Chúng tôi báo cáo hiệu suất khái quát hóa sử dụng độ chính xác phân loại trên tập kiểm tra OOD cho phân loại hình ảnh và mean average precision hoặc mAP@50 cho phát hiện đối tượng.

3.4 Truy xuất Hình ảnh
Chúng tôi thực hiện đánh giá trên một tập đa dạng các bộ dữ liệu truy xuất bao gồm truy xuất hình ảnh dựa trên nội dung và bộ dữ liệu phân loại mà chúng tôi tái sử dụng cho các tác vụ truy xuất ngữ nghĩa. Đối với truy xuất landmark địa lý, chúng tôi sử dụng bộ dữ liệu Oxford [69] và bộ dữ liệu Paris [70]. Để đảm bảo độ chính xác, chúng tôi sử dụng các phiên bản được làm sạch của các bộ dữ liệu này với nhãn được sửa [72]. Bộ dữ liệu INSTRE [95] bao gồm các đối tượng như đồ chơi và các sản phẩm có hình dạng bất thường được đặt ở các vị trí và điều kiện khác nhau. Để kiểm tra truy xuất tinh vi, chúng tôi sử dụng bộ dữ liệu Caltech-UCSD Birds-200 (CUB-200) [91], chứa các lớp chim khác nhau được chụp dưới các nền, tư thế và điều kiện ánh sáng khác nhau. Đối với một tập đa dạng hình ảnh tự nhiên, chúng tôi sử dụng bộ dữ liệu iNaturalist [88]. Bộ dữ liệu này cung cấp một loạt rộng các danh mục tinh vi được phân loại thành 13 siêu danh mục, bao gồm Thực vật, Côn trung, Chim, và Động vật có vú. Để đánh giá hiệu suất truy xuất trong các kịch bản thế giới thực, chúng tôi sử dụng bộ dữ liệu Objectnet [2]. Bộ dữ liệu này bao gồm 313 lớp đối tượng với nền, xoay và góc nhìn hình ảnh thay đổi ngẫu nhiên. Đối với nhận dạng landmark quy mô lớn, chúng tôi sử dụng bộ dữ liệu Google Landmarks v2 [99], bao gồm khoảng 200.000 landmark độc đáo. Cuối cùng, chúng tôi sử dụng bộ dữ liệu INRIA Copydays [19], bao gồm một bộ sưu tập nhỏ các bức ảnh kỳ nghỉ.

Trong số các bộ dữ liệu được đề cập, iNaturalist, Objectnet, và CUB-200 có thể được phân loại là bộ dữ liệu truy xuất ngữ nghĩa, trong khi các bộ dữ liệu còn lại thuộc về bộ dữ liệu truy xuất dựa trên nội dung.

Để đánh giá, chúng tôi đo hiệu suất mô hình sử dụng mean-Average-Precision hoặc mAP [68]. Chúng tôi đầu tiên tính average precision cho một hình ảnh truy vấn cho trước, và sau đó tính trung bình trên tất cả truy vấn để tìm mAP. Chúng tôi cũng đo **Recall@k**, đo tỷ lệ khớp đúng trong top k, và **MRR (Mean Reciprocal Rank)**, ghi lại số lượng kết quả được trả về trước khớp đúng đầu tiên và tính trung bình của nghịch đảo của các miss này. Cao hơn là tốt hơn cho tất cả metric.

4 Tôi là một Nhà thực hành. Tôi nên chọn Backbone nào?
Các nhà thực hành ngày nay có thể chọn từ một danh mục lớn các backbone với kích thước, phương pháp huấn luyện và dữ liệu huấn luyện trước khác nhau: nhà thực hành nên chọn backbone nào cho một tác vụ cụ thể hoặc nói chung? Để trả lời câu hỏi này, trong BoB, chúng tôi so sánh có hệ thống các backbone có sẵn công khai (xem Bảng 1) trên nhiều tác vụ, bộ dữ liệu và cài đặt. Để thực hiện các so sánh này, chúng tôi sử dụng giao thức xếp hạng sau:

(1) **Z-Scores đặc thù cho cài đặt**. Đối với một tác vụ và cài đặt cụ thể (ví dụ, độ chính xác phân loại top-1 trên ImageNet), chúng tôi đầu tiên tính z-scores cho tất cả các backbone được đánh giá – tức là, đối với các giá trị hiệu suất đặc thù cho cài đặt (ví dụ, độ chính xác) {xi}N i=1, z-scores được tính là {xi−µ σ}N i=1 trong đó µ và σ là trung bình và độ lệch chuẩn của mẫu. Điều này cho phép chúng tôi đo mức độ tốt của một backbone cụ thể (std trên hoặc dưới) so với hiệu suất "trung bình" của tất cả backbone trong cài đặt đó.

(2) **So sánh Chéo-cài đặt**. Để so sánh các backbone trên các tác vụ và cài đặt khác nhau, chúng tôi đơn giản tổng hợp và so sánh các z-scores thu được trước đó để có được xếp hạng tương đối (thô) của các backbone.

Sử dụng xếp hạng, chúng tôi có thể báo cáo không chỉ các backbone hiệu suất tốt nhất cho mỗi tác vụ mà còn backbone tốt nhất về hiệu suất tổng thể trên các tác vụ, bộ dữ liệu và cài đặt (xem Bảng 2 để tóm tắt).

--- TRANG 7 ---
Bảng 2: Tôi nên chọn backbone nào? Chúng tôi liệt kê 3 backbone hiệu suất cao nhất (từ trái sang phải) cho các tác vụ và cài đặt khác nhau. Màu đỏ tương ứng với đánh giá OOD và màu xanh lá cây cho thấy so sánh tổng thể.

| Tác vụ | Tốt | Tốt hơn | Tốt nhất |
|--------|-----|---------|----------|
| 1 Cls | ConvNeXt-B (IN-21k) | CLIP ViT-B (LAION-2B) | Sup. SwinV2-B (IN-21k,1k) |
| 2 Det | Sup. ConvNeXt-B (IN-1k) | Sup. SwinV2-B (IN-21k,1k) | Sup. ConvNeXt-B (IN-21k) |
| 3 Seg | Sup. ConvNeXt-B (IN-1k) | Sup. SwinV2-B (IN-21k,1k) | Sup. ConvNeXt-B (IN-21k) |
| 4 Ret | CLIP ViT-B (LAION-2B) | Sup. SwinV2-B (IN-21k,1k) | Sup. ConvNeXt-B (IN-21k) |
| 5 (OOD) Cls | CLIP ViT-B (LAION-2B) | Sup. SwinV2-B (IN-21k,1k) | Sup. ConvNeXt-B (IN-21k) |
| 6 (OOD) Det | Sup. ConvNeXt-B (IN-21k) | Sup. ConvNeXt-T (IN-1k) | Sup. ConvNeXt-B (IN-1k) |
| 7 All | CLIP ViT-B (LAION-2B) | Sup. SwinV2-B (IN-21k,1k) | Sup. ConvNeXt-B (IN-21k) |

4.1 Backbone Đặc thù cho Tác vụ

**Phân loại**. Đối với phân loại, trên nhiều bộ dữ liệu và cài đặt thí nghiệm (tinh chỉnh, linear probing, huấn luyện đầy đủ và ít shot), chúng tôi thấy "Supervised SwinV2-Base được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k)" là backbone hiệu suất tốt nhất, theo sau bởi "CLIP ViT-Base" và "Supervised ConvNeXt-Base được huấn luyện trên IN-21k" (xem hàng 1, Bảng 2).

**Phát hiện & Phân đoạn Đối tượng**. Đối với phát hiện đối tượng và phân đoạn instance, chúng tôi thấy "Supervised ConvNeXt-Base được huấn luyện trên IN-21K" > "Supervised SwinV2-Base được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k)" > "Supervised ConvNeXt-Base được huấn luyện trên IN-1k".

**Truy xuất Hình ảnh**. Đối với truy xuất hình ảnh, chúng tôi thấy "Supervised ConvNeXt-Base được huấn luyện trên IN-21k" là lựa chọn tốt nhất, với "Supervised SwinV2-Base được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k)" và "CLIP ViT-B được huấn luyện trên LAION-2B" đứng thứ hai và thứ ba.

**(OOD) Phân loại**. Trên các đánh giá OOD cho phân loại, chúng tôi thấy "Supervised ConvNeXt-Base được huấn luyện trên IN-21k" > "Supervised SwinV2-B được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k)" > "CLIP ViT-Base được huấn luyện trên LAION-2B".

**(OOD) Phát hiện Đối tượng**. Đối với phát hiện đối tượng Syn → Real, chúng tôi thấy "Supervised ConvNeXt-Base được huấn luyện trên IN-1k" là backbone tốt nhất, theo sau bởi "Supervised ConvNeXt-Tiny được huấn luyện trên IN-1k" và "Supervised ConvNeXt-Base được huấn luyện trên IN-21k".

4.2 Backbone Tốt nhất Tổng thể

Đối với các nhà thực hành không có tác vụ cụ thể nào trong đầu, các mô hình hiệu suất tốt nhất về hiệu suất tổng hợp là "Supervised ConvNeXt-Base được huấn luyện trên IN-21k" theo sau bởi "Supervised SwinV2-Base được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k)" và "CLIP ViT-Base được huấn luyện trên LAION-2B". Nhìn chung, chúng tôi lưu ý rằng các backbone được huấn luyện theo cách có giám sát (SwinV2-Base, ConvNeXt-Base) hoặc với giám sát thị giác và ngôn ngữ (CLIP ViT-Base) vượt trội hơn phần còn lại. Hơn nữa, chúng tôi thấy rằng CLIP ViT-Base được theo sát bởi Supervised ViT-Base được huấn luyện trên IN-21k (tinh chỉnh trên IN-1k). Chúng tôi so sánh các phương pháp chính xác hơn và phân tích xu hướng trong Phần 5.

4.3 Backbone với Ngân sách Eo hẹp

Nhiều ứng dụng thị giác máy tính đòi hỏi các backbone hiệu quả cho suy luận nhanh hoặc trên thiết bị. Trong phần này, chúng tôi đánh giá ba backbone nhỏ: RegNetX-400F [74], EfficientNet-B0 [84] và ResNet-18 [28] tất cả được huấn luyện trước theo cách có giám sát trên ImageNet-1k. Chúng tôi xếp hạng hiệu suất của các backbone nhỏ này trên tập các tác vụ trong Bảng 3. Chúng tôi thấy rằng EfficientNet-B0 hoạt động tốt nhất tổng thể và trên phân loại, truy xuất và phân loại OOD, theo sau bởi RegNetX-400MF và sau đó là ResNet-18. Thú vị, ResNet vẫn vượt trội hơn các kiến trúc hiệu quả mới hơn cho phát hiện và phân đoạn.

--- TRANG 8 ---
Bảng 3: Tôi nên chọn backbone tiny nào? Chúng tôi xếp hạng các backbone siêu nhẹ hiệu suất cao nhất (từ trái sang phải) cho các tác vụ và cài đặt khác nhau. Màu đỏ tương ứng với đánh giá OOD và màu xanh lá cây cho thấy so sánh tổng thể.

| Tác vụ | Tốt | Tốt hơn | Tốt nhất |
|--------|-----|---------|----------|
| 1 Cls | ResNet-18 | RegNetX-400MF | EfficientNet-B0 |
| 2 Det | RegNetX-400MF | EfficientNet-B0 | ResNet-18 |
| 3 Seg | RegNetX-400MF | EfficientNet-B0 | ResNet-18 |
| 4 Ret | ResNet-18 | RegNetX-400MF | EfficientNet-B0 |
| 5 (OOD) Cls | ResNet-18 | RegNetX-400MF | EfficientNet-B0 |
| 6 (OOD) Det | EfficientNet-B0 | ResNet-18 | RegNetX-400MF |
| 7 All | ResNet-18 | RegNetX-400MF | EfficientNet-B0 |

5 Quan sát và Xu hướng

▷**So sánh hiệu suất của ViT và CNN**. Các kiến trúc hiện đại vượt trội mạnh mẽ hơn ViT vanilla. Chúng tôi thấy trong Bảng 2 rằng backbone hiệu suất tốt nhất (ConvNeXt-Base) là tích chập, với transformer phân cấp (SwinV2-Base) đứng thứ hai sát nút. Kiến trúc transformer sau kết hợp một bias quy nạp không gian mạnh. Những phát hiện này cho thấy rằng cộng đồng nên vượt qua ViT vanilla vẫn được sử dụng thường xuyên. Như một lưu ý, chúng tôi không đánh giá các mô hình rất lớn, và có thể ViT có thể vượt trội hơn các biến thể tiên tiến hơn của chúng hoặc mạng tích chập ở quy mô lớn hơn.

▷**ViT hưởng lợi nhiều hơn từ quy mô so với CNN**. Đối với bộ backbone được xem xét trong BoB, chúng tôi thấy rằng hiệu suất tương đối (z-scores) cho cả CNN và ViT có mối tương quan tích cực với số lượng tham số nhưng nhiều hơn đối với ViT (spearman ρ = 0.58) so với CNN (spearman ρ = 0.35). Tương tự, trong khi hiệu suất tương đối tổng thể có mối tương quan với kích thước dữ liệu huấn luyện trước, mối tương quan lại cao hơn đáng kể đối với ViT (ρ = 0.72) so với CNN (ρ = 0.33). Quan sát này cho thấy rằng việc đánh giá các backbone lớn hơn nhiều có thể mang lại những người chiến thắng khác nhau, có thể là những người có kiến trúc dựa trên transformer.

▷**Có giám sát hay không?** **Các backbone học có giám sát chiếm ưu thế, nhưng chủ yếu vì chúng có sẵn được huấn luyện trước trên các bộ dữ liệu lớn hơn. Các backbone SSL có thể vượt trội hơn huấn luyện trước có giám sát với các bộ dữ liệu huấn luyện trước có kích thước tương tự**. Chúng tôi có được điểm số trung bình của 3 backbone hàng đầu trong các phong cách huấn luyện trước khác nhau, cụ thể là tự giám sát, có giám sát với ImageNet-1K, và có giám sát với ImageNet-21K, cho mỗi tác vụ (xem Phụ lục D). ConvNeXt và SwinV2 được huấn luyện trước có giám sát trên ImageNet-21K vượt trội hơn các backbone SSL trên tất cả tác vụ. Kết quả cho thấy rằng chúng ta nên thử sử dụng các kiến trúc tiên tiến, tích chập hoặc transformer, khi áp dụng các phương pháp SSL, và chúng ta nên huấn luyện trên các bộ dữ liệu lớn để cạnh tranh với học có giám sát. Trong các thí nghiệm này, các checkpoint huấn luyện trước có giám sát thường có sẵn được huấn luyện trên các bộ dữ liệu lớn hơn nhiều (ImageNet-21k). Khi so sánh các mô hình được huấn luyện trước trên các bộ dữ liệu có kích thước tương tự, các phương pháp SSL hoặc huấn luyện trước thị giác-ngôn ngữ đạt được hiệu suất tốt hơn trên phân loại (cả trong và ngoài phân phối) và các tác vụ truy xuất, phụ thuộc nhiều vào các biểu diễn học được. Tuy nhiên, các backbone học có giám sát duy trì lợi thế quyết định cho phát hiện và phân đoạn. Chúng tôi cũng có thể so sánh các backbone sử dụng cùng kiến trúc ViT-Base và thấy rằng các phương pháp SSL thực sự vượt trội hơn các backbone có giám sát ImageNet-1k nhưng tệ hơn các backbone được huấn luyện ImageNet-21k.

▷**Hiệu suất trên các tác vụ có mối tương quan cao**. Trên các tác vụ được kiểm tra, chúng tôi thấy mối tương quan Spearman tích cực mạnh giữa hiệu suất trên các cặp tác vụ (thường ρ > 0.8). Phát hiện này hỗ trợ xu hướng hiện tại của các mô hình nền tảng đa mục đích cho thị giác máy tính. Hơn nữa, phát hiện này cũng hỗ trợ nghiên cứu gần đây lập luận rằng một bias quy nạp duy nhất có thể giải quyết một loạt rộng các vấn đề dường như khác nhau [24]. Tuy nhiên, đáng chú ý là tác vụ truy xuất thể hiện mối tương quan thấp hơn tương đối nhưng vẫn có ý nghĩa thống kê (ρ = 0.49) đối với phân loại và xếp hạng truy xuất. Mối tương quan thấp hơn này có thể được quy cho những hạn chế hiệu suất của các mô hình được huấn luyện trước MiDaS và MAE trong bối cảnh truy xuất. Khi loại bỏ hai backbone này, hệ số tương quan ρ tăng lên 0.8, củng cố ảnh hưởng của các mô hình được đề cập trên kết quả quan sát.

--- TRANG 9 ---
[BIỂU ĐỒ: Hiển thị sự khác biệt hiệu suất giữa tinh chỉnh end-to-end và đóng băng backbone cho các tác vụ detection, segmentation và Sim10k→Cityscapes]

Hình 2: Transformer hưởng lợi đáng kể nhiều hơn từ tinh chỉnh end-to-end so với CNN trên các tác vụ dự đoán dày đặc. Chúng tôi hiển thị sự khác biệt hiệu suất giữa tinh chỉnh end-to-end và chỉ huấn luyện head trên đầu bộ trích xuất đặc trưng đóng băng trên các tác vụ khác nhau. Trục x là sự khác biệt hiệu suất tương đối (z-score tinh chỉnh trừ z-score backbone cố định). Trên các panel, sự khác biệt hiệu suất có mối tương quan giữa các tác vụ.

▷**Transformer xuất sắc dưới tinh chỉnh end-to-end trong khi mạng tích chập xuất sắc dưới linear probing**. Đối với các thí nghiệm "linear probing", chúng tôi đóng băng backbone được huấn luyện trước và chỉ học head. Lưu ý rằng đối với phát hiện và phân đoạn, head không chỉ là một lớp tuyến tính. Bằng cách kiểm tra sự khác biệt hiệu suất giữa hai chiến lược tinh chỉnh (Hình 2), chúng tôi thấy rằng ViT hưởng lợi đáng kể nhiều hơn từ tinh chỉnh end-to-end so với CNN, cả cho huấn luyện trước có giám sát và tự giám sát. Xem Hình 2 để so sánh trên các tác vụ dự đoán dày đặc.

▷**Mô hình CLIP và lời hứa của kiến trúc tiên tiến trong mô hình thị giác-ngôn ngữ**. Đối với hầu hết tất cả các tác vụ (ngoại trừ phát hiện OOD), huấn luyện trước CLIP là tốt nhất trong số các vision transformer vanilla, ngay cả so với các backbone được huấn luyện có giám sát ImageNet-21k. Trong số tất cả các backbone, CLIP chỉ tệ hơn SwinV2 và ConvNeXt được huấn luyện ImageNet-21k, điều này cho thấy sức mạnh của huấn luyện trước thị giác-ngôn ngữ và một lần nữa, gợi ý rằng chúng ta nên xem xét nhiều backbone khác ngoài ViT đơn giản khi thực hiện học tự hoặc yếu giám sát.

▷**Còn các backbone sinh tạo thì sao?** Trái ngược với các mô hình được huấn luyện bằng phương pháp có giám sát hoặc tự giám sát với loss contrastive, các backbone được huấn luyện với mục tiêu sinh tạo, như MAE hoặc Stable Diffusion, có hiệu suất tương đối kém hơn. Chúng tôi khuyên nên thận trọng khi diễn giải kết quả này, vì việc đánh giá Stable Diffusion hiện tại bị giới hạn ở một số tác vụ được chọn. Tuy nhiên, Stable Diffusion là một backbone lớn hơn so với những cái khác được xem xét trong benchmark này và được huấn luyện trên một bộ dữ liệu rất lớn, nhưng nó vẫn thể hiện hiệu suất kém hơn.

▷**Cuộc chiến của các backbone "nhỏ"**. Giữ trong tâm trí tài nguyên hạn chế, chúng tôi cũng so sánh tập con "nhỏ" của các backbone trong BoB (<30M tham số) – với các kiến trúc ViT-Small, ConvNeXt-Tiny, Swin-Tiny và ResNet-50. Nhìn chung, chúng tôi thấy Supervised ConvNeXt-T được huấn luyện trên IN-1k là tốt nhất, theo sau bởi Supervised SwinV2-T được huấn luyện trên IN-1k và DINO ViT-S được huấn luyện trên IN-1k. Thú vị, học có giám sát lại chiếm ưu thế, và các backbone được huấn luyện trước chỉ trên IN-1k vượt trội hơn những cái được huấn luyện trên bộ dữ liệu đa dạng và lớn hơn đáng kể (MiDaS).

▷**Hiệu suất vs. Tốc độ?** Phân tích của chúng tôi tiết lộ mối tương quan âm mạnh (ρ = -0.41) giữa throughput (được tính trên NVIDIA RTX A5000) và z-scores hiệu suất trung bình trên tất cả tác vụ khi xem xét mỗi backbone. Phát hiện này phù hợp với quan sát trước đây của chúng tôi rằng các mô hình lớn hơn có xu hướng thể hiện hiệu suất vượt trội. Do đó, để đạt được hiệu suất nâng cao, người ta có thể cần hy sinh tốc độ.

▷**Ước lượng độ sâu đơn mắt như một chiến lược huấn luyện trước đa mục đích**. Trong các thí nghiệm của chúng tôi, MiDaS đạt được hiệu suất cạnh tranh với các backbone SSL và có giám sát thông thường hàng đầu trong phân loại, phát hiện đối tượng và phân đoạn, ngay cả bên ngoài miền hình ảnh tự nhiên, ví dụ trên hình ảnh vệ tinh. Quan sát này cho thấy rằng ước lượng độ sâu có thể phục vụ như một tác vụ huấn luyện trước chính hoặc phụ mạnh mẽ và có thể khái quát hóa cho các mô hình nền tảng, hỗ trợ phát hiện của Lao et al. [49].

▷**Hiệu chuẩn và likelihood kiểm tra có mối tương quan với độ chính xác**. Chúng tôi đo expected calibration error (ECE) cũng như test cross-entropy loss trên tập kiểm tra ImageNet. Trong khi test likelihood có mối tương quan mạnh với độ chính xác (r = -0.8278), ECE thể hiện mối tương quan yếu hơn (r = -0.4876). Trong cả hai trường hợp, chúng tôi quan sát p-values dưới 0.05. Chúng tôi cũng lưu ý rằng huấn luyện trước tự giám sát thường dẫn đến hiệu chuẩn kém hơn.

▷**CNN và SSL bền vững hơn với adversarial**. Chúng tôi bổ sung đo tính bền vững adversarial của mỗi backbone trên tập kiểm tra ImageNet sử dụng một cuộc tấn công PGD bị ràng buộc ℓ∞ với nhiều bán kính (xem Phụ lục Bảng 19). Đối với mỗi kiến trúc mà chúng tôi sở hữu các phiên bản học tự giám sát, chúng tôi thấy rằng huấn luyện trước có giám sát luôn mang lại tính bền vững kém hơn. Hơn nữa, ViT dễ bị tổn thương hơn với các ví dụ adversarial so với mạng tích chập. Đáng chú ý, ConvNeXt bền vững hơn với adversarial ngay cả khi được huấn luyện theo cách có giám sát.

6 Mọi thứ sẽ đi đâu từ đây?
Trung tâm của mỗi mô hình thị giác máy tính là một backbone. Trong cuộc chiến backbone của chúng tôi, chúng tôi đã so sánh hơn 1.500 lần chạy huấn luyện để nổi bật những hiểu biết cho các nhà thực hành và nhà nghiên cứu thị giác máy tính.

Để hướng dẫn các nhà thực hành, chúng tôi đã phân tích hiệu suất của các backbone thị giác có sẵn công khai trên một loạt rộng các tác vụ từ phân đoạn và phát hiện đến phân loại và truy xuất. Chúng tôi thấy rằng ConvNext có giám sát, SwinV2 có giám sát, và các mô hình CLIP hoạt động tốt trên loạt rộng tác vụ này. Đối với các cài đặt bị ràng buộc tính toán, trong cuộc chiến của các backbone "nhỏ" chúng tôi thấy rằng các đối tác nhỏ hơn của cùng kiến trúc ConvNext-T và SwinV2 có giám sát, theo sau bởi DINO với ViT nhỏ hoạt động khá tốt. BoB cung cấp cho các nhà thực hành một hướng dẫn để chọn các backbone hợp lý từ loạt lựa chọn choáng ngợp.

Đối với các nhà nghiên cứu nhìn về phía trước, chúng tôi cũng quan sát một số xu hướng đáng chú ý. Đầu tiên, chúng tôi thấy hiệu suất trên các tác vụ có mối tương quan mạnh, gợi ý một sự chuyển dịch khỏi các backbone thị giác chuyên biệt sang các backbone phổ quát hoạt động tốt trên một loạt tác vụ. Tiếp theo, chúng tôi thấy throughput và hiệu suất có mối quan hệ nghịch đảo, gợi ý việc mở rộng quy mô vẫn là một hướng đầy hứa hẹn để cải thiện backbone. Cuối cùng, chúng tôi thấy rằng trong khi các khuyến nghị thực tế của chúng tôi bao gồm nhiều mô hình có giám sát, trong các so sánh apple-to-apple với huấn luyện có giám sát tiêu chuẩn, học tự giám sát mang lại nhiều hứa hẹn.

Bằng cách phát hành tất cả kết quả thí nghiệm của chúng tôi cùng với mã để đưa các backbone mới vào thử nghiệm, chúng tôi hy vọng BoB phục vụ như một hướng dẫn hữu ích cho cả các nhà thực hành ngày nay và các nhà nghiên cứu nhìn về phía trước vào ngày mai.

**Hạn chế**. Chúng tôi lưu ý rằng những hiểu biết thu được từ BoB phụ thuộc vào từ vựng của các tác vụ, backbone và cài đặt được xem xét trong công trình này. Chúng tôi dự định các điều rút ra từ nghiên cứu này cung cấp các cân nhắc thực tế hữu ích cho các nhà nghiên cứu thị giác máy tính, nhận ra rằng những hiểu biết như vậy cần liên tục phát triển khi nhiều backbone được giới thiệu và nhiều tác vụ và cài đặt được tính đến. Cuối cùng, chúng tôi lưu ý rằng các nghiên cứu trong BoB tập trung chủ yếu vào các khía cạnh liên quan đến hiệu suất, và việc khám phá theo các trục quan trọng khác (bias trong mô hình, v.v.) vẫn còn.

Benchmark của chúng tôi không bao gồm các backbone lớn hơn ConvNext-Base, ngoài Stable Diffusion, và một số xếp hạng có thể thay đổi ở quy mô lớn. Ví dụ, trong khi chúng tôi thấy rằng các kiến trúc tích chập hiện đại được huấn luyện trước thông qua học có giám sát hoạt động tốt nhất trên hầu hết các tác vụ, chúng tôi cũng thấy rằng transformer hưởng lợi nhiều hơn từ quy mô, cả về dữ liệu huấn luyện trước và kích thước kiến trúc. Có thể các backbone transformer sẽ vượt lên trước các backbone tích chập ở quy mô rất lớn.

7 Chi phí Tính toán và Dấu chân Carbon
Các thí nghiệm trong bài báo này tốn tổng cộng 127k giờ GPU trên card NVIDIA RTX A100. Giả sử GPU chạy với hiệu quả carbon trung bình 0.37 kgCO2eq/kWh, tổng lượng khí thải ước tính là 11792.36 kgCO2eq [48].

**Lời cảm ơn**
MG và AGW được hỗ trợ một phần bởi NSF CAREER IIS-2145492, NSF I-DISRE 193471, NIH R01DA048764-01A1, NSF IIS-1910266, BigHat Biosciences, Capital One, và Amazon Research Award. HS và RC được hỗ trợ một phần bởi ONR MURI grant N00014-20-1-2787. VP, PC, và JH được hỗ trợ một phần bởi ARL, NASA ULI, Google, và NSF #2144194. RN, MS, GS, và TG được hỗ trợ bởi chương trình ONR MURI, Office of Naval Research (N000142112557), chương trình AFOSR MURI, và National Science Foundation (IIS-2212182 & 2229885).

[Tài liệu tham khảo và phần còn lại được dịch tiếp theo...]
