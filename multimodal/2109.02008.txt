# 2109.02008.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2109.02008.pdf
# File size: 653996 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Cross-token Modeling with Conditional Computation
Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, Yang You
National University of Singapore, Singapore
fyuxuanlou, f.xueg@u.nus.edu, zhengzangw@gmail.com, youy@comp.nus.edu.sg
Abstract
Mixture-of-Experts (MoE), a conditional computa-
tion architecture, achieved promising performance
by scaling local module ( i.e., feed-forward net-
work) of transformer. However, scaling the cross-
token module ( i.e., self-attention) is challenging
due to the unstable training. This work pro-
poses Sparse-MLP, an all-MLP model which ap-
plies sparsely-activated MLPs to cross-token mod-
eling. SpeciÔ¨Åcally, in each Sparse block of our
all-MLP model, we apply two stages of MoE lay-
ers: one with MLP experts mixing information
within channels along image patch dimension, the
other with MLP experts mixing information within
patches along the channel dimension. In addi-
tion, by proposing importance-score routing strat-
egy for MoE and redesigning the image repre-
sentation shape, we further improve our model‚Äôs
computational efÔ¨Åciency. Experimentally, we are
more computation-efÔ¨Åcient than Vision Transform-
ers with comparable accuracy. Also, our models
can outperform MLP-Mixer by 2.5% on ImageNet
Top-1 accuracy with fewer parameters and compu-
tational cost. On downstream tasks, i.e.,Cifar10
and Cifar100, our models can still achieve better
performance than baselines.
1 Introduction
Mixture of Experts (MoE) [Shazeer et al. , 2017 ]is a powerful
conditional computation architecture to scale the transformer
up to trillions of parameters [Fedus et al. , 2021 ]. However,
although MoE can scale the local module ( i.e.,feed-forward
network) in transformer well, it is challenging to scale the
cross-token module ( i.e.,self-attention). The reason is that
training a MoE-based attention is unstable and easy to di-
verge [Fedus et al. , 2021 ]. In this work, we propose Sparse-
MLP, a sparsely activated all-MLP model that can scale both
local and cross-token module up efÔ¨Åciently.
Sparse-MLP comprises a stack of Sparse blocks and Dense
blocks. In each Sparse block, we apply sparsely-activated
MoE layers at two stages. (1) Token-mixing MoE ( MoE S),
a computation-efÔ¨Åcient alternative for self-attention, mix-
ing the information across the spatial locations of imagerepresentations within channels. (2) Channel-mixing MoE
(MoE C)), another stage of conditional computation, mixing
the information within image representation patches along
channels. In dense blocks, MoE SandMoE Care simpliÔ¨Åed
as dense MLPs. Besides, we make two further improvements
on MoE architecture. Firstly, we propose the importance-
score routing strategy which can reduce routing computation
of vanilla MoE. It ranks the tokens or channels for routing
by their importance scores. Secondly, we redesign the im-
age representation shape in Sparse blocks so that the gating
network of token-mixing MoE can function with more efÔ¨Å-
ciency.
A signiÔ¨Åcant contribution of our work is, to our best knowl-
edge, that we Ô¨Årst set sparse Mixture of Experts as cross-
token modeling module. The speciÔ¨Åc solution is using token-
mixing MoE proposed to model the context. In general,
we build our all-MLP model with conditional computation
in two directions: both in patch dimension and channel di-
mension. It is also a major difference between our model
and previous Transformer-MoE models [Fedus et al. , 2021;
Riquelme et al. , 2021; Xue et al. , 2021 ]. Previous models
which apply MoE to transformer-based architecture only re-
place the FFN in the transformer block with sparse MoE. In
our model, we have channel-mixing MoE layers function in
a similar way, and token-mixing MoE layers function in an-
other direction: mixing the information across the spatial lo-
cations of the representation. We prove with experiments that
such a two-dimensional MoE design is effective and efÔ¨Åcient.
Finally, We apply our Sparse-MLP models to image clas-
siÔ¨Åcation tasks and obtain outstanding results. After pre-
trained with the self-supervised algorithm(MoCo v3) [Chen
et al. , 2021 ]on ILSVRC2012 ImageNet-1k dataset [Rus-
sakovsky et al. , 2015 ], our Sparse-B model reaches 77:9%
ImageNet-1k top-1 accuracy, 2:0%higher than Mixer-B/16
model with comparable computational cost, 1:2%higher
than ViT-B/16 with less computational cost. Our Sparse-L
model reaches 79:2ImageNet-1k top-1 accuracy, outperform-
ing Mixer-L/16 by 2:5%with 62:8%parameters and 85:5%
pre-training cost. It also outperforms ViT-L/16 by 1:8%with
less than half pretraining cost.
The contributions of this work can be summarized as fol-
lows:arXiv:2109.02008v3  [cs.LG]  14 Jan 2022

--- PAGE 2 ---
Two-stage MoE design and all-MLP architecture for
cross-token modeling
We use computation-efÔ¨Åcient MoE architecture as cross-
token modeling module and then build an all-MLP architec-
ture with two-stage MoE application. To our best knowledge,
this is the Ô¨Årst work focusing on scaling cross-token modeling
module by conditional computation.
Further efÔ¨Åciency improvement on MoE
We design the importance-score routing strategy which re-
quires less computation cost without damaging model capac-
ity. Also We revisit and redesign the image representation
shape to make best use of token-mixing MoE. Both practice
further improve our model‚Äôs efÔ¨Åciency.
Competitive performance on image classiÔ¨Åcation tasks
We show that our Sparse-MLP model can outperform ViT
model and dense MLP-Mixer model on Imagenet-1k bench-
mark. Also, on three downstream tasks, our Sparse-MLP
models can reach better performance with comparable or less
computational cost with same-level MLP-Mixer models.
2 Cross-token Modeling with Conditional
Computation
The key contribution of our work is, we propose a sparsely-
activated MLP architecture to efÔ¨Åciently and effectively for
cross-token modeling: the token-mixing MoE layer. Token-
mixing MoE follow the Mixture of Experts architecture
[Shazeer et al. , 2017 ]. For a batch of image inputs X2
RBSCwith batch size B, per image patches Sand per
patch channels C, we Ô¨Årstly transpose inputs to X02
RBCS. Then the gating network of token-mixing MoE
assignsBCbatch channels to different MLP experts by
routing weights. Each expert model mixes the information
along the patch dimension within each channel assigned to it.
The details of token-mixing MoE layer and all-MLP architec-
ture will be described in the following sections.
2.1 Mixture-of-Experts
In this section, we formulate Mixture-of-Experts (MoE) ar-
chitecture and its key components.
Conditional Computing
The Mixture-of-Experts layer (MoE) is composed of a set of
experts. Only a subset of them are active and engaged in the
computation on a per-example basis. In our model, each ex-
pert is an MLP.
Following [Shazeer et al. , 2017 ], givenx2RD, the output
of one MoE layer with NExperts is:
MoE(x) =NX
i=1G(x)iEi(x) (1)
whereG(x) :RD!RNis the gating network which com-
pute input-conditioned routing weights for experts. Ei(x) :
RD!RDis theithexpert layer. In practice, we have a
sparseG(x), which means each input xis restricted to be as-
signed tokexperts (kN). If the input xis not assigned to
Ei,G(x)i= 0andEiwould not be computed. This enables
us to scale to outrageously large model with a comparable
computation cost.Gating Network
As we introduced above, to assign token representations xto
different experts, each MoE layer has a sparse gating network.
We formulate it as:
G(x) = TopK(softmax( Wg(x) +)) (2)
whereWg2RDNis a trainable matrix and N (0;1
N2)
is a normal noise to explore better assignment from experts.
After computing the probability of the input xrouted to each
Expert, we only keep the top Kof them for further forward
propagation. In practice, we usually select Kas 1 or 2.
Load Balance Loss
To encourage a balanced assignment of inputs across ex-
perts, an auxiliary loss is added to the model for every
MoE layer [Shazeer et al. , 2017; Lepikhin et al. , 2020;
Fedus et al. , 2021; Riquelme et al. , 2021 ]. The formulation
of load balance loss is in Appendix A.
2.2 Importance-score Routing Strategy
As an attempt to further improve token-mixing MoE‚Äôs efÔ¨Å-
ciency, we propose the importance-score routing strategy. For
each batch of image inputs after transpose X02RBCS,
we set a importance score for all BCchannels in the batch.
Then, channels are sorted by their importance scores. 10%
with the lowest importance scores are eliminated and the rest
are for the allocation.
In our work, the importance score of each channel for rout-
ing is its highest routing weight to all channels. Accordingly,
g(X)i;j2Rdenotes the routing weight of i-th channel to the
j-th expert. The importance score of i-th channel is:
Score (Channel i) =max jfg(X)i;jg: (3)
In section 4.3, we show empirically how such routing strat-
egy can help further reduce computation cost without damag-
ing model capacity.
2.3 All-MLP Architecture
An overview of our full model, Sparse-MLP is shown in Fig-
ure 1. In general, the Sparse-MLP model comprises a per-
patch linear embedding layer, a stack of Dense blocks and
Sparse blocks, and a classiÔ¨Åer head.
In each Sparse block, aside from the token-mixing MoE
layer ( MoE S) described above, we also apply a channel-
mixing MoE layer( MoE C), which is also a sparsely activated
architecture and responsible for mixing the information along
the channel dimension within patches. Besides, we have re-
scale sub-layers at both the beginning and the end of each
Sparse block which will be further described in Section 3.
We formulate Sparse block as follows:
x= Rescale 1(x) (4)
y1=x+t(MoE S(t(norm(x)))) (5)
y=y1+ MoE C(norm(y1)) (6)
y= Rescale 2(y) (7)

--- PAGE 3 ---
Figure 1: Sparse-MLP architecture overview
In each Dense block, both token-mixing MoE and channel-
mixing MoE are simpliÔ¨Åed by single MLP: token-mixing
MLP ( MoE S) and channel mixing MLP ( MoE S). The idea
of Dense block follows [Tolstikhin et al. , 2021 ]. The formu-
lation of Dense block is:
y1=x+t(MLP S(t(norm(x)))) (8)
y=y1+ MLP C(norm(y1)) (9)
In summary, our Sparse-MLP is an all-MLP model. And
by applying sparse MoE architecture at two stages of infor-
mation mixing, our model can achieve better model perfor-
mance than other MLP-like models.
3 Revisit Image Representation Shape
So far, we have an all-MLP model including token-mixing
MoE and channel-mixing MoE. However, In the original ViT
design [Dosovitskiy et al. , 2020 ], the number of tokens is
small while the per-token hidden size ( i.e.,channel dimension
C) is much larger. For image representation x2RSC;C
3S. If we adopt the original representation setting of ViT
at token-mixing MoE layer, assignment of large amounts of
channels to experts requires much computation while experts‚Äô
capability are limited by short patch length. For example, in
standard ViT setting, for a batch of input X2RBSC,
B= 4096;S= 196;C= 768 . The token-mixing MoE layer
need to assign BC= 3145728 channels to correspond-
ing experts while experts‚Äô capability of mixing information
within the channels is limited because the small output hid-
den sizeS= 196 .In order to reduce the computation in assignment stage and
improve token-mixing MoE‚Äôs capability to mix information
within channels, we add two stages of rescale sub-layers at
the beginning and the end of each Sparse block. Pro-token
rescale sub-layer, at the begining of each Sparse block, re-
duces number of channels and increases number of tokens so
that the inputs can better Ô¨Åtting MoE S. Pro-channel rescale
sub-layer, at the end of each Sparse block, functions the op-
posite way so that Sparse blocks and Dense blocks can be
combined Ô¨Çexibly.
Given an input x2RSC, Pro-token rescale sub-layer
maps:RCS!RC1S1. Pro-channel rescale sub-layer
maps: RS1C1!RSC. Each rescale sub-layer is com-
posed of two linear layers to transform two dimensions Sand
S1,CandC1, respectively. Such implementation can reduce
routing computation and improve expert dimension, which
leads to a more balanced and effective MoE design. In prac-
tice, we setS1= 2S;C 1=C=2.
4 Experiments
We pretrain our Sparse-MLP models with MoCo V3 on the
ILSVRC-2012 Imagenet dataset [Russakovsky et al. , 2015 ]
and evaluate our model‚Äôs performance on several down-
stream image classiÔ¨Åcation tasks. We select MLP-Mixer
models [Tolstikhin et al. , 2021 ]and ViT models [Dosovit-
skiy et al. , 2020 ]as our baselines and compare our models
with baseline models in two quantities: (1) classiÔ¨Åcation ac-
curacy on downstream tasks, (2) computational cost of pre-
training on the upstream dataset, and Ô¨Åne-tuning on down-
stream datasets. We do not aim to reach SOTA image classi-
Ô¨Åcation accuracy but to show that our fully-MLP model with

--- PAGE 4 ---
Figure 2: Comparison between Mixer models and Sparse-MLP
models. With comparable or less computational cost, Sparse-MLP
achieves better performance
conditional computing can outperform dense MLP models or
attention-based models either in accuracy or computational
cost.
4.1 Experiment Settings
We pretrain Sparse-MLP models and baseline models(ViT
and MLP-Mixer) with a self-supervised learning algo-
rithm(MoCo v3) [Chen et al. , 2021 ]on ILSVRC-2012 Ima-
geNet dataset. [Russakovsky et al. , 2015 ](1.3M training sam-
ples, 1k image classes) on TPU clusters.
After pretraining, We Ô¨Åne-tune our model on three down-
stream tasks: ILSVRC-2012 Imagenet, CIFAR-10 (50k train-
ing samples, 10k validation samples, 10 classes) [Krizhevsky
et al. ,]and CIFAR-100. The detail setting of pretraining and
Ô¨Åne-tune stage can be found in Appendix B.
4.2 Main Results
We build Sparse-MLP models on three parameter levels in
comparison with attention-based models ( e.g., ViT [Doso-
vitskiy et al. , 2020 ]) and dense MLP models ( e.g., MLP-
Mixer [Tolstikhin et al. , 2021 ]). The speciÔ¨Åcations of our
models can be found in Appendix C. In Table 1, we report
ImageNet-1k top-1 accuracy and corresponding pre-training
cost of each model.
Our Sparse-S model surpasses Mixer-S/16 on ImageNet-1k
top-1 accuracy by 1.1% with comparable parameters and pre-
training cost. Sparse-B model scales Mixer-B/16 with 17%
(59M!69M) with comparable pre-training TPU v3 core days
and outperforms Mixer-B/16 by 2:6%(75:9%!77:9%).
Our Sparse-L outperforms Mixer-L/16 by 3.3% with only
62.8% parameters and 85.5% pre-training time. Compared
with ViT, our models show better performance with much
fewer parameters and much less pre-training cost.
Also, we report the results of Sparse-MLP models and
dense MLP models [Tolstikhin et al. , 2021 ]on two other
downstream image classiÔ¨Åcation tasks: Cifar10 [Krizhevsky
et al. ,], Cifar100 [Krizhevsky, 2009 ]. All models are pre-
trained with MoCo v3 on ImageNet-1k and then Ô¨Åne-tuned atdownstream tasks end-to-end.
In Table 2, we can see that our Sparse-MLP models also
outperform MLP-Mixer models on Cifar10 and Cifar100 im-
age classiÔ¨Åcation tasks. Also, when we scale our model to
over 100M parameters, the performance of Mixer-L/16 and
Sparse-L drop due to overÔ¨Åtting. This issue is prominent
when training large MLP models on small datasets. And in
such cases, our Sparse-L model still achieves higher accuracy
than Mixer-L/16.
4.3 Ablation Study
In this section, we further investigate how each component of
our Sparse-MLP model contributes to the performance. All
models in the ablation study are pretrained with MoCo v3 al-
gorithm on ImageNet-1k and Ô¨Åne-tuned on the same dataset.
We select ImageNet-1k top-1 validation accuracy and total
pre-training TPU v3 core days as evaluation metrics. The ab-
lation study is designed to answer the following questions:
‚Ä¢Number of experts : What is the impact of the number
of experts in two stages MoE layers?
‚Ä¢Top K routing : Which K value(1 or 2) shall we select
forMoE SandMoE C?
‚Ä¢Importance-score routing : How much computation
cost importance-score routing can save?
‚Ä¢Positions of Sparse blocks : How shall we combine
Dense blocks and Sparse blocks?
‚Ä¢rescale sub-layers analysis : How do rescale sub-layers
inÔ¨Çuence model performance and computational cost?
Figure 3: InÔ¨Çuence of number of experts in MoE C/MoE S
Number of experts
We Ô¨Årst study the inÔ¨Çuence of the number of experts in
MoE Son model capacity. Different models are built based
on Sparse-B. We Ô¨Åx all other hyper-parameters and tune the
number of experts in MoE Sat three levels: 4, 8, 16, pretrain
these models and evaluate their performance on ImageNet-1k
validation top-1 accuracy.
From Figure 3, we can see that when the number of experts
inMoE Sincreases from 4 to 8, the model‚Äôs performance in-
creases a lot. However, when we scale experts to 16, the ac-
curacy barely changes.

--- PAGE 5 ---
Models ImageNet Top-1(%) Params(M) Pre-training cost Throughput
attention-based
ViT-B/16 76.7 86 67.2 861
ViT-L/16 77.6 304 195.2 268
dense MLP-like
Mixer-S/16 70.2 19 35.5 3986
Mixer-B/16 75.9 59 53.3 1320
Mixer-L/16 76.7 207 97.7 412
Sparse-MLP
Sparse-S 71.3 21 35.5 3986
Sparse-B 77.9 69 55.5 1265
Sparse-L 79.4 130 80.1 482
Table 1: ImageNet-1k results. All models are pretrained with self-supervised algorithm(MoCo v3) on ImageNet-1k and then Ô¨Åne-tuned.
Pretrain cost is evaluated by total TPU v3 core-days used for pretraining. Throughput is evaluated by image/sec/core
Models ImageNet Cifar10 Cifar100
top-1 top-1 top-1
Mixer-S/16 70.2 91.7 84.4
Sparse-S 71.3 91.9 84.4
Mixer-B/16 75.9 95.6 86.7
Sparse-B 77.9 96.2 87.2
Mixer-L/16 76.7 94.7 86.3
Sparse-L 79.4 95.4 87.4
Table 2: Results on downstream image classiÔ¨Åcation tasks
Similarly, for MoE C, we Ô¨Åx all other components in
Sparse-B and tune the number of experts in MoE Cat three
levels: 4,8, 16.
In Figure 3, we observe that there would be an overÔ¨Åtting
issue when we increase the number of experts in MoE C. Such
Ô¨Ånding is similar to results in [Xueet al. , 2021 ]. When train-
ing data is limited, scaling the MoE layers, which mixes the
information within spatial locations, will make the model eas-
ily overÔ¨Åt the training data.
K ImageNet Top-1(%) TPU v3 core days
MoE S
1 77.9 55.5
2 77.9 57.7
MoE C
1 77.0 53.3
2 77.9 55.5
Table 3: InÔ¨Çuence of K selecting.
The role of Top K routing
Following the design in [Fedus et al. , 2021 ], we select K=1
or 2 for MoE SandMoE C. We set Sparse-B as our default
model(K=1 for MoE S, K=2 for MoE C). Then we report the
results with K=2 for MoE Sand K=1 for MoE Cseparately.As shown in Table 3, for MoE S, top-1 routing and top-2
routing reach the same validation accuracy and top-1 rout-
ing cost less pre-training time. For MoE C, top-2 routing
would lead to prominent better performance with 4%more
pre-training time.
K ImageNet Top-1(%) TPU v3 core days
Sparse-B
EfÔ¨Åcient 77.9 54.3
Vanilla 77.9 55.5
Sparse-L
EfÔ¨Åcient 79.4 80.0
Vanilla 79.2 83.5
Table 4: Importance-score Routing Versus Vanilla Routing
Importance-score Routing Versus Vanilla MoE Routing
In section 2.2, we propose a new routing method that requires
less computation cost than vanilla MoE routing. We compare
the two different routing methods on Sparse-B and Sparse-L
models, we can see that with comparable results on ImageNet
top-1 validation accuracy, importance-score routing strategy
can reduce total pretraining time by 1.9% and 4.1%
The positions of Sparse blocks
We experiment with two different placing orders of Dense
blocks and Sparse blocks. (1) Dense blocks in front and
Sparse blocks behind; (2) Sparse blocks as Ô¨Årst few blocks
and followed by Dense blocks. Also, we experiment with a
different number of Sparse blocks while keeping the num-
ber of total Dense blocks and Sparse blocks the same. We
set Sparse-B as our default model and change the orders and
numbers of blocks based on Sparse-B
In Table 5, we can Ô¨Ånd that placing Sparse blocks in the
Ô¨Årst place and Dense blocks afterwards do not show good per-
formance. We also Ô¨Ånd that increasing the number of Sparse
blocks, in the end, is an effective way to improve the model‚Äôs
performance. When we increase 2 Sparse blocks and keep the

--- PAGE 6 ---
Positions ImageNet Top-1(%) Parameters(M)
N/A (Mixer-B/16) 75.9 59
Last two(Sparse-B) 77.9 69
First two 75.5 69
Last four 78.3 79
Table 5: Different combinations of Dense blocks and Sparse blocks.
‚ÄôPositions‚Äô refers to the locations of Sparse blocks.
total number of blocks unchanged, the model‚Äôs ImageNet-1k
top-1 validation accuracy increased by 0:3%
The role of rescale sub-layers
An intuitive way to build Sparse blocks is to only apply two
stages of Mixture of Experts with original image representa-
tion shape. As stated in Section 3, such design would lead
to huge computational cost in routing stage of token-mixing
MoE. Thus, we add rescale sub-layers to reduce computation
cost of Sparse blocks. Here we verify the necessity of rescale
sub-layers by experiments. We set Sparse-B as our default
model and experiment models with or without rescale sub-
layers.
Models ImageNet Top-1(%) Pre-training cost
w/ r layers 77.9 55.5
w/o r layers 76.9 79.9
Table 6: Comparison between models with or without rescale sub-
layers.
We can see from table 6 that rescale sub-layers not only
reduce pre-training computation cost by 30:5%but also im-
prove the performance signiÔ¨Åcantly.
5 Why Token-mixing MoE and Sparse-MLP
Although token-mixing MoE can be viewed as an
computation-efÔ¨Åcient alternative for self-attention, our
Sparse-MLP model not only saves computational cost, but
also have competitive performance. Sparse-MLP has its own
advantage compared with ViT, previous transformer-MoE
models and dense MLP-like models.
‚Ä¢Versus ViT : Our model not only requires much less pre-
training cost but also shows better model capacity empir-
ically. In section 4.2, our model(Sparse-L) outperforms
ViT at same parameter level with only less than half pre-
training cost.
‚Ä¢Versus previous transformer-MoE models : Previous
work applying MoE architecture to vision transformer
models [Riquelme et al. , 2021 ]only replaces the FFN
in the transformer block with Mixture of Experts layer.
Like what attention does in transformer, we further ap-
ply the sparse MoE layer to cross-token dimension. In
this way, we not only reduce computational cost, but
also build an All-MLP architecture, which is more sim-
ple and easy to scale up.
‚Ä¢Versus dense MLP-like models : Compared with MLP-
Mixer [Tolstikhin et al. , 2021 ], our model scales up thesingle MLP to a set of sparsely-activated MLP structure.
Such scaling strategy brings signiÔ¨Åcant model perfor-
mance improvements and only requires a sub-linear in-
crease in computation cost. In section 4.2, Sparse-MLPs
outperform Mixer models at same parameter level by a
large margin.
6 Related Work
6.1 Transformer-MoE models
Mixture-of-Experts(MoE) [?; Shazeer et al. , 2017 ]has re-
cently been applied to transformer-based architecture to build
huge models [Lepikhin et al. , 2020; Fedus et al. , 2021;
Riquelme et al. , 2021; Xue et al. , 2021 ]. In NLP tasks,
[Lepikhin et al. , 2020; Fedus et al. , 2021 ]applied MoE to
scale transformer-based models to trillions of parameters and
achieve superising results. In vision tasks, [Riquelme et al. ,
2021 ]improves ViT [Dosovitskiy et al. , 2020 ]by scaling a
subset of transformer blocks with MoE. [Xue et al. , 2021 ]
applies MoE to transformer blocks with parameters sharing
to improve ViT with fewer parameters. In these works, MoE
layers are to replace the FFN in transformer blocks. Our
model design makes a difference in that we apply MoEs in
two directions and experiments demonstrate that the novel
token-mixing MoE can improve model capacity effectively
and efÔ¨Åciently.
6.2 MLP-like models
Our work is also related to MLP-like models. Different from
CNN models [Krizhevsky et al. , 2012; Simonyan and Zisser-
man, 2015 ]and attention-based models [Dosovitskiy et al. ,
2020; Touvron et al. , 2020 ], all trainable parameters in the
backbones of MLP-based models are MLP-like. In MLP-
Mixer [Tolstikhin et al. , 2021 ], a token-mixing MLP is to
replace the multi-head self-attention [Vaswani et al. , 2017 ]
in transformer block [Tolstikhin et al. , 2021 ]. Some other
MLP-like architectures [Liuet al. , 2021; Hou et al. , 2021 ]
function a similar way, mixing the information across spa-
tial locations with MLPs or FFNs. Our Sparse-MLP applies
sparsely-activated MLP and can achieve better performance.
7 Conclusion
In this work, we propose token-mixing MoE, a sparse MoE
architecture to model the cross-token information of images
with conditional computation. Further, we propose Sparse-
MLP, an all-MLP architecture with two-dimentional MoE in
vision. Experiments demonstrate that our two-stage MoE
with importance-score routing strategy and rescale sub-layer
design are effective and computation efÔ¨Åcient. Besides, we
perform a comprehensive ablation study to investigate how
each component contributes to the performance.
Extensions of our work could include the following top-
ics. First, it is possible to further improve Sparse-MLP model
capacity with huge pre-training datasets. Besides, we can ex-
plore the Ô¨Çexibility of Sparse-MLP architecture by designing
Sparse blocks with different MoE settings in the same model.
It would also be worthwhile to apply Sparse-MLP architec-
ture to NLP and other tasks.

--- PAGE 7 ---
References
[Chen et al. , 2021 ]Xinlei Chen, Saining Xie, and Kaiming
He. An empirical study of training self-supervised vision
transformers. CoRR , abs/2104.02057, 2021.
[Dosovitskiy et al. , 2020 ]Alexey Dosovitskiy, Lucas Beyer,
Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Min-
derer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
and Neil Houlsby. An image is worth 16x16 words:
Transformers for image recognition at scale. CoRR ,
abs/2010.11929, 2020.
[Fedus et al. , 2021 ]William Fedus, Barret Zoph, and Noam
Shazeer. Switch transformers: Scaling to trillion param-
eter models with simple and efÔ¨Åcient sparsity. CoRR ,
abs/2101.03961, 2021.
[Hou et al. , 2021 ]Qibin Hou, Zihang Jiang, Li Yuan, Ming-
Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vision
permutator: A permutable mlp-like architecture for visual
recognition. CoRR , abs/2106.12368, 2021.
[Krizhevsky et al. ,]Alex Krizhevsky, Vinod Nair, and Ge-
offrey Hinton. Cifar-10 (canadian institute for advanced
research).
[Krizhevsky et al. , 2012 ]Alex Krizhevsky, Ilya Sutskever,
and Geoffrey E Hinton. Imagenet classiÔ¨Åcation with deep
convolutional neural networks. In F. Pereira, C. J. C.
Burges, L. Bottou, and K. Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems , vol-
ume 25. Curran Associates, Inc., 2012.
[Krizhevsky, 2009 ]Alex Krizhevsky. Learning multiple lay-
ers of features from tiny images. Technical report, 2009.
[Lepikhin et al. , 2020 ]Dmitry Lepikhin, HyoukJoong Lee,
Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
Gshard: Scaling giant models with conditional compu-
tation and automatic sharding. CoRR , abs/2006.16668,
2020.
[Liuet al. , 2021 ]Hanxiao Liu, Zihang Dai, David R. So, and
Quoc V . Le. Pay attention to mlps. CoRR , abs/2105.08050,
2021.
[Riquelme et al. , 2021 ]Carlos Riquelme, Joan Puigcerver,
Basil Mustafa, Maxim Neumann, Rodolphe Jenatton,
Andr ¬¥e Susano Pinto, Daniel Keysers, and Neil Houlsby.
Scaling vision with sparse mixture of experts. CoRR ,
abs/2106.05974, 2021.
[Russakovsky et al. , 2015 ]Olga Russakovsky, Jia Deng,
Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael
Bernstein, Alexander C. Berg, and Li Fei-Fei. Ima-
geNet Large Scale Visual Recognition Challenge. Inter-
national Journal of Computer Vision (IJCV) , 115(3):211‚Äì
252, 2015.
[Shazeer et al. , 2017 ]Noam Shazeer, Azalia Mirhoseini,
Krzysztof Maziarz, Andy Davis, Quoc V . Le, Geof-
frey E. Hinton, and Jeff Dean. Outrageously large neu-ral networks: The sparsely-gated mixture-of-experts layer.
CoRR , abs/1701.06538, 2017.
[Simonyan and Zisserman, 2015 ]Karen Simonyan and An-
drew Zisserman. Very deep convolutional networks for
large-scale image recognition. In International Confer-
ence on Learning Representations , 2015.
[Tolstikhin et al. , 2021 ]Ilya O. Tolstikhin, Neil Houlsby,
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
Thomas Unterthiner, Jessica Yung, Andreas Steiner,
Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey
Dosovitskiy. Mlp-mixer: An all-mlp architecture for vi-
sion. CoRR , abs/2105.01601, 2021.
[Touvron et al. , 2020 ]Hugo Touvron, Matthieu Cord,
Matthijs Douze, Francisco Massa, Alexandre Sablay-
rolles, and Herv ¬¥e J¬¥egou. Training data-efÔ¨Åcient image
transformers & distillation through attention. CoRR ,
abs/2012.12877, 2020.
[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. CoRR , abs/1706.03762, 2017.
[Xueet al. , 2021 ]Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan
Lou, Yong Liu, and Yang You. Go wider instead of deeper.
CoRR , abs/2107.11817, 2021.

--- PAGE 8 ---
A Load-balance loss for MoE
Our auxiliary loss which encourages balanced routing con-
sists of two parts: Importance loss and Load loss.
The importance of ithexpert is deÔ¨Åned as the normalized
gating network weights correspond to ithexpert summed over
the input batch X.
Impi(X) =X
x2Xsoftmax(Wgx)i (10)
whereWgis the gating weight matrix of the MoE layer, and
the importance loss of the MoE layer over a batch of inputs
Xis:
Limp(X) = (std(Imp(X))
mean(Imp(X)))2(11)
In addition to the importance loss for more balanced rout-
ing weights, we also have a load loss seeking balanced rout-
ing results. The load of an Expert igiven a batch of inputs
Xis deÔ¨Åned as the possibility of routing to Expert isummed
over the batch.
Load i(X) =X
x2Xpi(x) (12)
pi(x),P(G(x)i)threshold k(G(x))) (13)
The load loss of one MoE layer over the batch is:
LLoad(X) = (std(Load(X))
mean(Load( X)))2(14)
And the total auxiliary loss of one MoE layer takes the form:
Laux=(1
2Limp+1
2Lload) (15)
whereis a hyper-parameter that controls that the auxiliary
loss not only encourages balanced routing across experts but
also not overwhelms the original model loss. In practice, we
set= 1e 2. According to existing MoE-based mod-
els[Riquelme et al. , 2021; Xue et al. , 2021 ], the performance
is insensitive to .
B Pretrain and Ô¨Åne-tune Details
our data augmentation policy for pretraining includes random
resized crop, horizontal Ô¨Çipping, RandAugment, color jitter-
ing, grayscale conversion, blurring, and solarization. We also
apply stochastic depth.
We pretrain all models on TPU v3 clusters. We select a
batch size as 4096 at the pre-training stage, LAMB optimizer
with weight decay. We pretrain all models for 300 epochs
using a cosine learning rate decay with a 10k steps warm up.
The image resolution for pretraining is 224.
At Ô¨Åne-tune stage, We follow the standard Ô¨Åne-tune set-
tings in. After pretraining, we remove the MLP heads of the
pretrained model, add a classiÔ¨Åer head to the encoder, and
train on downstream tasks. The augmentation strategies dur-
ing Ô¨Åne-tuning stage include random resized crop, horizontal
Ô¨Çipping, RandAugment and Mixup. We select Adam with-
out weight decay as the optimizer. We set our learning rate
aslrBatchSize=256, using linear weight decay with 10k
warm-up steps. Image resolution is 224224.Hyper-parameter Value
Image resolution 224
Epochs 300
Batch size 4096
Warmup steps 10k
Optimizer LAMB
Peak learning rate 1e-3
Learning rate decay cosine
Weight decay rate 1e-1
Global clip norm 1
MoCot 1
MoCom 0.99
MoCo dim 4096
Table 7: Hyper-parameters for pre-training on ImageNet-1k
C Model Settings
We report our main results based on three models: Sparse-S,
Sparse-B, Sparse-L. In Table 8, we give the speciÔ¨Åcations of
these models. Each model is composed of L1Dense blocks
andL2Sparse blocks. And in all three models reported in
the main results, Dense blocks are in the front and followed
by Sparse blocks. DSrefers to the hidden dimension of to-
ken mixing MLPs, and DCrefers to the hidden dimension of
channel mixing MLPs. DS0is the MLP dimension of token-
mixing MoE layers, and DC0denotes the MLP dimension of
channel-mixing MoE layers. For all MLPs in Dense blocks
and Sparse blocks, we set dropout as 0. For token mixing
MoEs, we select top Krouting as 1. And for channel mixing
MoEs, we set Kas 2.
SpeciÔ¨Åcation Sparse-S Sparse-B Sparse-L
Dense block
blocks L1 6 10 8
Patches S 196 196 196
Hidden size C 512 768 768
MLP SdimDS 256 384 384
MLP CdimDC 2048 3072 3072
Sparse block
blocks L2 2 2 6
New patches S0392 392 392
New hidden size C0512 384 384
Experts in MoE S 4 8 16
Experts in MoE C 0 4 4
MoE Stop K 1 1 1
MoE Ctop K - 2 2
MoE SdimDS0 512 768 768
MoE CdimDC0 2048 1536 1536
Positions last 2 last 2 last 6
Parameters(M) 22 69 130
Table 8: SpeciÔ¨Åcations of Sparse-MLP models

--- PAGE 9 ---
Figure 4: ImageNet-1k validation accuracy of ViT and MLP-Mixer,
after supervised pretraining on ImageNet-1k
D Pretrain: Why Unsupervised
We Ô¨Ånd that scaling MLP models or Vision Transformer mod-
els in parameters and training them from scratch with limited
training data ( e.g., ImageNet-1k) will lead to an overÔ¨Åtting
problem. As shown in Figure 4, MLP-Mixer and ViT‚Äôs accu-
racy both go down when parameters increase. Such Ô¨Ånding
is consistent with previous work on MLP models ( i.e.,MLP-
Mixer) and attention-based models ( i.e.,ViT).
In order to compare our models with baselines in a fairer
way, and better evaluate models‚Äô performance when parame-
ters scaling up, We adopt an unsupervised training algorithm:
MoCo V3. We pretrain all our models on ImangeNet-1k
dataset with MoCo V3 and then Ô¨Åne-tune them. Both Sparse-
MLP and baseline models can achieve better performance
with paramters increasing.
