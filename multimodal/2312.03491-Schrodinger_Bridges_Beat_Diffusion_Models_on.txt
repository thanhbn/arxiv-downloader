# 2312.03491.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2312.03491.pdf
# File size: 3456566 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Schrodinger Bridges Beat Diffusion Models on
Text-to-Speech Synthesis
Zehua Chen∗1Guande He∗1Kaiwen Zheng∗1Xu Tan2Jun Zhu†1
1Dept. of Comp. Sci. & Tech., Institute for AI, THU-Bosch Joint Center for ML, Tsinghua University
2Microsoft Research Asia
Abstract
In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality.
However, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted
to a noisy representation, which provides little information of the generation target. In this work, we
present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior
in established diffusion-based TTS methods with a clean and deterministic one, which provides strong
structural information of the target. Specifically, we leverage the latent representation obtained from
text input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth
mel-spectrogram, leading to a data-to-data process. Moreover, the tractability and flexibility of our
formulation allow us to empirically study the design spaces such as noise schedules, as well as to develop
stochastic and deterministic samplers. Experimental results on the LJ-Speech dataset illustrate the
effectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly
outperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS
models in few-step scenarios. Project page: https://bridge-tts.github.io/ .
Latent 𝑥𝑡Text latent: 𝑧“Bridge -TTS is 
awesome.”
Prior:𝑥𝑇=𝑧Prior:𝑥𝑇~𝑁(𝜇𝑧,𝛴𝑧) Data: 𝑥0
Data: 𝑥0Diffusion -TTS: Data -to-Noise Process
Bridge -TTS: Data -to-Data ProcessLatent 𝑥𝑡
𝑥𝑇 𝑥0
𝑥𝑇 𝑥0Bridge SDE Sampling
Bridge ODE Sampling
𝑥𝑇 𝑥0
𝑥𝑇 𝑥0
Figure 1: An overview of Bridge-TTS built on Schrodinger bridge.
∗Equal contribution;†Corresponding author: dcszj@tsinghua.edu.cn
1arXiv:2312.03491v1  [cs.LG]  6 Dec 2023

--- PAGE 2 ---
1 Introduction
Diffusion models, including score-based generative models (SGMs) [Song et al., 2021b] and denoising
diffusion probabilistic models [Ho et al., 2020], have been one of the most powerful generative models across
different data generation tasks [Bao et al., 2023, Leng et al., 2022, Ramesh et al., 2022, Wang et al., 2023].
In speech community, they have been extensively studied in waveform synthesis [Chen et al., 2021, 2022b,
Kong et al., 2021], text-to-audio generation [Huang et al., 2023a,b, Liu et al., 2023b,c], and text-to-speech
(TTS) synthesis [Popov et al., 2021, Shen et al., 2023, Tan et al., 2021]. Generally, these models contain two
processes between the data distribution and the prior distribution: 1) the forward diffusion process gradually
transforms the data into a known prior distribution, e.g., Gaussian noise; 2) the reverse denoising process
gradually generates data samples from the prior distribution.
In diffusion-based TTS systems [Chen et al., 2023, Popov et al., 2021, Ye et al., 2023], the text input is
usually first transformed into latent representation by a text encoder, which contains a phoneme encoder and
a duration predictor, and then diffusion models are employed as a decoder to generate the mel-spectrogram
conditioned on the latent. The prior distribution in these systems can be classified into two types: 1) one
is using the standard Gaussian noise to generate target [Chen et al., 2022c, Huang et al., 2022, Liu et al.,
2022b]; 2) the other improves the prior to be more informative of the target. For example, Grad-TTS
[Popov et al., 2021] learns the latent representation from the text encoder with the ground-truth target in
training, and takes it as the mean of prior distribution to obtain a mean-shifted Gaussian. PriorGrad [Lee
et al., 2022] utilizes the statistical values from training data, computing a Gaussian with covariance matrix.
DiffSinger [Liu et al., 2022a] employs an auxiliary model to acquire an intractable prior distribution, enabling
a shallow reverse process. However, because diffusion models pre-specify the noise-additive diffusion process,
the prior distribution of the above systems is confined to a noisy representation, which is not indicative of the
mel-spectrogram.
In this work, as shown in Figure 1, we propose a new design to generate mel-spectrogram from a clean
and deterministic prior, i.e., the text latent representation supervised by ground-truth target [Popov et al.,
2021]. It has provided structural information of the target and is utilized as the condition information in both
recent diffusion [Chen et al., 2023, Ye et al., 2023] and conditional flow matching [Guo et al., 2023, Mehta
et al., 2023] based TTS systems, while we argue that replacing the noisy prior in previous systems with
this clean latent can further boost the TTS sample quality and inference speed. To enable this design, we
leverage Schrodinger bridge [Chen et al., 2022a, Schrödinger, 1932] instead of diffusion models, which seeks a
data-to-data process rather than the data-to-noise process in diffusion models. As the original Schrodinger
bridge is generally intractable that hinders the study of the design spaces in training and sampling, we
propose a fully tractable Schrodinger bridge between paired data with a flexible form of reference SDE in
alignment with diffusion models [Ho et al., 2020, Song et al., 2021b].
With the tractability and flexibility of our proposed framework, aiming at TTS synthesis with superior
generation quality and efficient sampling speed, we make an investigation of noise schedule, model parameter-
ization, and training-free samplers, which diffusion models have greatly benefited from [Hoogeboom et al.,
2023, Salimans and Ho, 2022, Song et al., 2021a], while not been thoroughly studied in Schrodinger bridge
related works. To summarize, we make the following key contributions in this work:
•In TTS synthesis, we make the first attempt to generate the mel-spectrogram from clean text latent
representation ( i.e., the condition information in diffusion counterpart) by means of Schrodinger bridge,
exploring data-to-data process rather than data-to-noise process.
•By proposing a fully tractable Schrodinger bridge between paired data with a flexible form of reference
SDE, we theoretically elucidate and empirically explore the design spaces of noise schedule, model
parameterization, and sampling process, further enhancing TTS quality with asymmetric noise schedule,
data prediction, and first-order bridge samplers.
•Empirically, we attain both state-of-the-art generation quality and inference speed with a single training
session. In both 1000-step and 50-step generation, we significantly outperform our diffusion counterpart
Grad-TTS [Popov et al., 2021]; in 4-step generation, we accomplish higher quality than FastGrad-TTS
[Vovk et al., 2022]; in 2-step generation, we surpass the state-of-the-art distillation method CoMoSpeech
[Ye et al., 2023], and the transformer-based model FastSpeech 2 [Ren et al., 2021].
2

--- PAGE 3 ---
2 Background
2.1 Diffusion Models
Given a data distribution pdata(x),x∈Rd, SGMs [Song et al., 2021b] are built on a continuous-time
diffusion process defined by a forward stochastic differential equation (SDE):
dxt=f(xt, t)dt+g(t)dwt,x0∼p0=pdata (1)
where t∈[0, T]for some finite horizon T,f:Rd×[0, T]→Rdis a vector-valued drift term, g: [0, T]→R
is a scalar-valued diffusion term, and wt∈Rdis a standard Wiener process. Under proper construction of
f, g, the boundary distribution pT(xT)is approximately a Gaussian prior distribution pprior=N(0, σ2
TI).
The forward SDE has a corresponding reverse SDE [Song et al., 2021b] which shares the same marginal
distributions {pt}T
t=0with the forward SDE:
dxt= [f(xt, t)−g2(t)∇logpt(xt)]dt+g(t)d¯wt,xT∼pT≈pprior (2)
where ¯wtis the reverse-time Wiener process, and the only unknown term ∇logpt(xt)is thescore function of
the marginal density pt. By parameterizing a score network sθ(xt, t)to predict ∇logpt(xt), we can replace
the true score in Eqn. (2) and solve it reversely from pprioratt=T, yielding generated data samples at t= 0.
sθ(xt, t)is usually learned by the denoising score matching (DSM) objective [Song et al., 2021b, Vincent,
2011] with a weighting function λ(t)>0:
Ep0(x0)pt|0(xt|x0)Et
λ(t)∥sθ(xt, t)− ∇logpt|0(xt|x0)∥2
2
, (3)
where t∼ U(0, T)andpt|0is the conditional transition distribution from x0toxt, which is determined by
the pre-defined forward SDE and is analytical for a linear drift f(xt, t) =f(t)xt.
2.2 Diffusion-Based TTS Systems
The goal of TTS systems is to learn a generative model pθ(x|y)over mel-spectrograms (Mel) x∈Rd
given conditioning text y1:Lwith length L. Grad-TTS [Popov et al., 2021] provides a strong baseline for
TTS with SGM, which consists of a text encoder and a diffusion-based decoder. Specifically, they alter the
Gaussian prior in SGMs to another one ˜penc(z|y) =N(z,I)with informative mean z, where z∈Rdis a
latent acoustic feature transformed from a text string ythrough the text encoder network E, i.e.,z=E(y).
The diffusion-based decoder utilizes ˜pencas prior for SGM and builds a diffusion process via the following
modified forward SDE:
dxt=1
2(z−xt)βtdt+p
βtdwt,x0∼p0=pdata(x|y) (4)
where p0=pdata(x|y)is the true conditional data distribution and βtis a non-negative noise schedule. The
forward SDE in Eqn. (4) will yield xT∼pT≈˜pencwith sufficient large T[Popov et al., 2021]. During training,
the text encoder and the diffusion-based decoder are jointly optimized, where the encoder is optimized with
a negative log-likelihood loss Lenc=−Epdata(x|y)[log ˜penc(x|y)]and the decoder is trained with the DSM
objective in Eqn. (3), denoted as Ldiff. Apart from LencandLdiff, the TTS system also optimizes a duration
predictor ˆAas a part of the encoder that predicts the alignment map A∗between encoded text sequence
˜z1:Land the latent feature z1:Fwith Fframes given by Monotonic Alignment Search [Kim et al., 2020],
where zj=˜zA∗(j). Denote the duration prediction loss as Ldp, the overall training objective of Grad-TTS is
Lgrad-tts =Lenc+Ldp+Ldiff.
2.3 Schrodinger Bridge
The Schrodinger Bridge (SB) problem [Chen et al., 2022a, De Bortoli et al., 2021, Schrödinger, 1932]
originates from the optimization of path measures with constrained boundaries:
min
p∈P[0,T]DKL(p∥pref),s.t.p0=pdata, pT=pprior (5)
3

--- PAGE 4 ---
where P[0,T]is the space of path measures on a finite time horizon [0, T],prefis thereference path measure ,
andp0, pTare the marginal distributions of pat boundaries. Generally, prefis defined by the same form of
forward SDE as SGMs in Eqn. (1) (i.e., the reference SDE ). In such a case, the SB problem is equivalent to a
couple of forward-backward SDEs [Chen et al., 2022a, Wang et al., 2021]:
dxt= [f(xt, t) +g2(t)∇log Ψ t(xt)]dt+g(t)dwt,x0∼pdata (6a)
dxt= [f(xt, t)−g2(t)∇logbΨt(xt)]dt+g(t)d¯wt,xT∼pprior (6b)
where fandgare the same as in the reference SDE. The extra non-linear drift terms ∇logΨt(xt)and
∇logbΨt(xt)are also described by the following coupled partial differential equations (PDEs):
(∂Ψ
∂t=−∇xΨ⊤f−1
2Tr 
g2∇2
xΨ
∂bΨ
∂t=−∇x·(bΨf) +1
2Tr
g2∇2
xbΨs.t.Ψ0bΨ0=pdata,ΨTbΨT=pprior. (7)
The marginal distribution ptof the SB at any time t∈[0, T]satisfies pt= Ψ tbΨt. Compared to SGMs
where pT≈pprior=N(µ, σ2
TI), SB allows for a flexible form of ppriorand ensures the boundary condition
pT=pprior. However, solving the SB requires simulating stochastic processes and performing costly iterative
procedures [Chen et al., 2022a, De Bortoli et al., 2021, Shi et al., 2023]. Therefore, it suffers from scalability
and applicability issues. In certain scenarios, such as using paired data as boundaries, the SB problem can
be solved in a simulation-free approach [Liu et al., 2023a, Somnath et al., 2023]. Nevertheless, SBs in these
works are either not fully tractable or limited to restricted families of pref, thus lacking a comprehensive and
theoretical analysis of the design spaces.
3 Bridge-TTS
We extend SB techniques to the TTS task and elucidate the design spaces with theory-grounded analyses.
We start with a fully tractable SB between paired data in TTS modeling. Based on such formulation, we
derive different training objectives and theoretically study SB sampling in the form of SDE and ODE, which
lead to novel first-order sampling schemes when combined with exponential integrators. In the following
discussions, we say two probability density functions are the same when they are up to a normalizing factor.
Besides, we assume the maximum time T= 1for convenience.
3.1 Schrodinger Bridge between Paired Data
As we have discussed, with the properties of unrestricted prior form and strict boundary condition, SB is
a natural substitution for diffusion models when we have a strong informative prior. In the TTS task, the
pairs of the ground-truth data (x, y)and the deterministic prior z=E(y)given by the text encoder can be
seen as mixtures of dual Dirac distribution boundaries (δx, δz), which simplifies the solving of SB problem.
However, in such a case, the SB problem in Eqn. (5) will inevitably collapse given a stochastic reference
process that admits a continuous density pref
1att= 1, since the KL divergence between a Dirac distribution
and a continuous probability measure is infinity.
To tackle this problem, we consider a noisy observation of boundary data points x0,x1polluted by a
small amount of Gaussian noise N(0, ϵ2
1I)andN(0, ϵ2
2I)respectively, which helps us to identify the SB
formulation between clean data when ϵ1, ϵ2→0. Actually, we show that in general cases where the reference
SDE has a linear drift f(xt, t) =f(t)xt(which is aligned with SGMs), SB has a fully tractable and neat
solution when ϵ2=eR1
0f(τ)dτϵ1. We formulate the result in the following theorem.
Proposition 3.1 (Tractable Schrodinger Bridge between Gaussian-Smoothed Paired Data with Reference
SDE of Linear Drift, proof in Appendix A.1) .Assume f=f(t)xt, the analytical solution to Eqn. (7) when
pdata=N(x0, ϵ2I)andpprior =N(x1, e2R1
0f(τ)dτϵ2I)is
bΨϵ
t=N(αta,(α2
tσ2+α2
tσ2
t)I),Ψϵ
t=N(¯αtb,(α2
tσ2+α2
t¯σ2
t)I) (8)
4

--- PAGE 5 ---
where t∈[0,1],
a=x0+σ2
σ2
1(x0−x1
α1),b=x1+σ2
σ2
1(x1−α1x0), σ2=ϵ2+p
σ4
1+ 4ϵ4−σ2
1
2, (9)
and
αt=eRt
0f(τ)dτ,¯αt=e−R1
tf(τ)dτ, σ2
t=Zt
0g2(τ)
α2τdτ,¯σ2
t=Z1
tg2(τ)
α2τdτ. (10)
In the above theorem, αt,¯αt, σt,¯σtare determined by f, gin the reference SDE (Eqn. (1)) and are
analogous to the noise schedule in SGMs [Kingma et al., 2021]. When ϵ→0,bΨϵ
t,Ψϵ
tconverge to the tractable
solution between clean paired data (x0,x1):
bΨt=N(αtx0, α2
tσ2
tI),Ψt=N(¯αtx1, α2
t¯σ2
tI) (11)
The advantage of such tractability lies in its ability to facilitate the study of training and sampling under the
forward-backward SDEs (Eqn. (6)), which we will discuss in the following sections. Besides, the marginal
distribution pt=bΨtΨtof the SB also has a tractable form:
pt= Ψ tbΨt=Nαt¯σ2
tx0+ ¯αtσ2
tx1
σ2
1,α2
t¯σ2
tσ2
t
σ2
1I
, (12)
which is a Gaussian distribution whose mean is an interpolation between x0,x1, and variance is zero at
boundaries and positive at the middle. A special case is that, when the noise schedule f(t) = 0and
g(t) =σ >0, we have pt=N((1−t)x0+tx1, σ2t(1−t)I), which recovers the Brownian bridge used in
previous works [Qiu et al., 2023, Tong et al., 2023a,b]. Actually, Eqn. (12) reveals the form of generalized
Brownian bridge with linear drift and time-varying volatility between x0andx1. We put the detailed analysis
in Appendix B.1.
3.2 Model Training
The TTS task aims to learn a model to generate the Mel x0given text y. Denote x1=E(y)as the
latent acoustic feature produced by text encoder E, since the SB is tractable given x0,x1(∇logΨ,∇logbΨ
in Eqn. (6) are determined by Eqn. (11)), a direct training approach is to parameterize a network xθto
predict x0given xtat different timesteps, which allows us to simulate the process of SB from t= 1tot= 0.
This is in alignment with the data prediction in diffusion models, and we have the bridge loss:
Lbridge =E(x0,y)∼pdata,x1=E(y)Et[∥xθ(xt, t,x1)−x0∥2
2] (13)
where xt=αt¯σ2
t
σ2
1x0+¯αtσ2
t
σ2
1x1+αt¯σtσt
σ1ϵ,ϵ∼ N(0,I)by the SB (Eqn. (12)). x1is also fed into the network as
a condition, following Grad-TTS [Popov et al., 2021].
Analogous to the different parameterizations in diffusion models, there are alternative choices of training
objectives that are equivalent in bridge training, such as the noise prediction corresponding to ∇logbΨt[Liu
et al., 2023a] or the SB score ∇logpt, and the velocity prediction related to flow matching techniques [Lipman
et al., 2023]. However, we find they perform worse or poorly in practice, which we will discuss in detail in
Appendix D. Except for the bridge loss, we jointly train the text encoder E(including the duration predictor
ˆA) following Grad-TTS. Since the encoder no longer parameterizes a Gaussian distribution, we simply adopt
an MSE encoder loss L′
enc=E(x0,y)∼pdata∥E(y)−x0∥2. And we use the same duration prediction loss Ldpas
Grad-TTS. The overall training objective of Bridge-TTS is Lbridge-tts =L′
enc+Ldp+Lbridge.
In our framework, the flexible form of reference SDE facilitates the design of noise schedules f, g, which
constitutes an important factor of performance as in SGMs. In this work, we directly transfer the well-behaved
noise schedules from SGMs, such as variance preserving (VP). As shown in Table 1, we set f, g2linear to t,
and the corresponding αt, σ2
thave closed-form expressions. Such designs are new in both SB and TTS-related
contexts and distinguish our work from previous ones with Brownian bridges [Qiu et al., 2023, Tong et al.,
2023a,b].
5

--- PAGE 6 ---
Table 1: Demonstration of the noise schedules in Bridge-TTS.
Schedule f(t) g2(t) αt σ2
t
Bridge-gmax10 β0+t(β1−β0) 11
2(β1−β0)t2+β0t
Bridge-VP −1
2(β0+t(β1−β0))β0+t(β1−β0)e−1
2Rt
0(β0+τ(β1−β0))dτeRt
0(β0+τ(β1−β0))dτ−1
3.3 Sampling Scheme
Assume we have a trained data prediction network xθ(xt, t)2. If we replace x0withxθin the tractable
solution of bΨ,Ψ(Eqn. (11)) and substitute them into Eqn. (6), which describes the SB with SDEs, we can
obtain the parameterized SB process. Analogous to the sampling in diffusion models, the parameterized SB
can be described by both stochastic and deterministic processes, which we call bridge SDE/ODE, respectively.
Bridge SDE We can follow the reverse SDE in Eqn. (6b). By substituting Eqn. (11) into it and replace x0
withxθ, we have the bridge SDE :
dxt=
f(t)xt+g2(t)xt−αtxθ(xt, t)
α2
tσ2
t
dt+g(t)d¯wt (14)
Bridge ODE Theprobability flow ODE [Song et al., 2021b] of the forward SDE in Eqn. (6a) is [Chen
et al., 2022a]:
dxt=
f(t)xt+g2(t)∇log Ψ t(xt)−1
2g2(t)∇logpt(xt)
dt
=
f(t)xt+1
2g2(t)∇log Ψ t(xt)−1
2g2(t)∇logbΨt(xt)
dt(15)
where we have used ∇logpt(xt) =∇logΨt(xt) +∇logbΨt(xt)since pt= Ψ tbΨt. By substituting Eqn. (11)
into it and replace x0withxθ, we have the bridge ODE :
dxt=
f(t)xt−1
2g2(t)xt−¯αtx1
α2
t¯σ2
t+1
2g2(t)xt−αtxθ(xt, t)
α2
tσ2
t
dt (16)
To obtain data sample x0, we can solve the bridge SDE/ODE from the latent x1att= 1tot= 0. However,
directly solving the bridge SDE/ODE may cause large errors when the number of steps is small. A prevalent
technique in diffusion models is to handle them with exponential integrators [Gonzalez et al., 2023, Lu et al.,
2022a,b, Zheng et al., 2023a], which aims to “cancel” the linear terms involving xtand obtain solutions with
lower discretization error. We conduct similar derivations for bridge sampling, and present the results in the
following theorem.
Proposition 3.2 (Exact Solution and First-Order Discretization of Bridge SDE/ODE, proof in Appendix A.2) .
Given an initial value xsat time s >0, the solution at time t∈[0, s]of bridge SDE/ODE is
xt=αtσ2
t
αsσ2sxs−αtσ2
tZt
sg2(τ)
α2τσ4τxθ(xτ, τ)dτ+αtσts
1−σ2
t
σ2sϵ,ϵ∼ N(0,I) (17)
xt=αtσt¯σt
αsσs¯σsxs+¯αtσ2
t
σ2
1
1−σs¯σt
¯σsσt
x1−αtσt¯σt
2Zt
sg2(τ)
α2τσ3τ¯στxθ(xτ, τ)dτ (18)
1The main hyperparameter for the Bridge-gmax schedule is β1, which is exactly the maximum of g2(t).
2We omit the condition x1for simplicity and other parameterizations such as noise prediction can be first transformed to xθ.
6

--- PAGE 7 ---
The first-order discretization (with the approximation xθ(xτ, τ)≈xθ(xs, s)forτ∈[t, s]) gives
xt=αtσ2
t
αsσ2sxs+αt
1−σ2
t
σ2s
xθ(xs, s) +αtσts
1−σ2
t
σ2sϵ,ϵ∼ N(0,I) (19)
xt=αtσt¯σt
αsσs¯σsxs+αt
σ2
1
¯σ2
t−¯σsσt¯σt
σs
xθ(xs, s) +
σ2
t−σsσt¯σt
¯σsx1
α1
(20)
To the best of our knowledge, such derivations are revealed for the first time in the context of SB. We find
that the first-order discretization of bridge SDE (Eqn. (19)) recovers posterior sampling [Liu et al., 2023a] on
a Brownian bridge, and the first-order discretization of bridge ODE (Eqn. (20)) in the limit ofσs
σ1,σt
σ1→0
recovers deterministic DDIM sampler [Song et al., 2021a] in diffusion models. Besides, we can easily discover
that the 1-step case of Eqn. (19) and Eqn. (20) are both 1-step deterministic prediction by xθ. We put more
detailed analyses in Appendix B.2.
We can also develop higher-order samplers by taking higher-order Taylor expansions for xθin the exact
solutions. We further discuss and take the predictor-corrector method as the second-order case in Appendix C.
In practice, we find first-order sampler is enough for the TTS task, and higher-order samplers do not make a
significant difference.
4 Experiments
4.1 Training Setup
DataWe utilize the LJ-Speech dataset [Ito and Johnson, 2017], which contains 13,100samples, around 24
hours in total, from a female speaker at a sampling rate of 22.05kHz. The test samples are extracted from
both LJ- 001and LJ- 002, and the remaining 12577samples are used for training. We follow the common
practice, using the open-source tools [Park, 2019] to convert the English grapheme sequence to phoneme
sequence, and extracting the 80-band mel-spectrogram with the FFT 1024points, 80Hz and 7600Hz lower
and higher frequency cutoffs, and a hop length of 256.
Modeltraining Toconductafaircomparisonwithdiffusionmodels, weadoptthesamenetworkarchitecture
and training settings used in Grad-TTS [Popov et al., 2021]: 1) the encoder ( i.e., text encoder and duration
predictor) contains 7.2M parameters and the U-Net based decoder contains 7.6M parameters; 2) the model is
trained with a batch size of 16, and 1.7M iterations in total on a single NVIDIA RTX 3090, using 2.5days; 3)
the Adam optimizer [Kingma and Ba, 2015] is employed with a constant learning rate of 0.0001. For noise
schedules, we set β0= 0.01, β1= 20for Bridge-VP (exactly the same as VP in SGMs) and β0= 0.01, β1= 50
for Bridge-gmax.
Evaluation Following previous works [Huang et al., 2022, Liu et al., 2022a, Popov et al., 2021], we conduct
the subjective tests MOS (Mean Opinion Score) and CMOS (Comparison Mean Opinion Score) to evaluate
the overall subjective quality and comparison sample quality, respectively. To guarantee the reliability of
the collected results, we use the open platform Amazon Mechanical Turk , and require Master workers to
complete the listening test. Specifically, the MOS scores of 20 test samples are given by 25 Master workers to
evaluate the overall performance with a 5-point scale, where 1and5denote the lowest (“Bad") and highest
(“Excellent") quality respectively. The result is reported with a 95% confidence interval. Each CMOS score is
given by 15 Master workers to compare 20 test samples synthesized by two different models. More details of
CMOS test can be visited in Appendix G. In both MOS and CMOS tests, each of the test samples has been
normalized for a fair comparison3. To measure the inference speed, we calculate the real-time factor (RTF)
on an NVIDIA RTX 3090.
3https://github.com/slhck/ffmpeg-normalize
7

--- PAGE 8 ---
4.2 Results and Analyses
We demonstrate the performance of Bridge-TTS on sample quality and inference speed separately, which
guarantees a more precise comparison between multiple models. In Table 2 and Table 3, the test samples in
LJ-Speech dataset are denoted as Recording , the samples synthesized from ground-truth mel-spectrogram by
vocoder are denoted as GT-Mel+voc. , and the number of function evaluations is denoted as NFE. We take the
pre-trained HiFi-GAN [Kong et al., 2020]4as the vocoder, aligned with other baseline settings. More details
of baseline models are introduced in Appendix F. In the sampling process of both tests, Grad-TTS employs
ODE sampling and sets the prior distribution pT=N(z, τ−1
dI)with a temperature parameter τd= 1.5. In
Bridge-TTS, we use our first-order SDE sampler shown in Eqn. (19) with a temperature parameter τb= 2for
the noise distribution ϵ=N(0, τ−1
bI), which is helpful to the TTS quality in our observation.
Table 2: The MOS comparison with 95% confidence
interval given numerous sampling steps.
Model NFE RTF ( ↓) MOS ( ↑)
Recording / / 4.10 ±0.06
GT-Mel + voc. / / 3.93 ±0.07
FastSpeech 2 1 0.004 3.78 ±0.07
VITS 1 0.018 3.99 ±0.07
DiffSinger 71 0.157 3.92 ±0.06
ResGrad 50 0.135 3.97 ±0.07
Grad-TTS 50 0.116 3.99 ±0.07
Ours (VP) 50 0.117 4.09±0.07
Ours (gmax) 50 0.117 4.07±0.07
Grad-TTS 1000 2.233 3.98 ±0.07
Ours (VP) 1000 2.267 4.05±0.07
Ours (gmax) 1000 2.267 4.07±0.07Table 3: The MOS comparison with 95% confidence
interval in few-step generation.
Model NFE RTF ( ↓) MOS ( ↑)
Recording / / 4.12 ±0.06
GT-Mel + voc. / / 4.01 ±0.06
FastSpeech 2 1 0.004 3.84 ±0.07
CoMoSpeech 1 0.007 3.74 ±0.07
ProDiff 2 0.019 3.67 ±0.07
CoMoSpeech 2 0.009 3.87 ±0.07
Ours (gmax) 2 0.009 4.04±0.06
DiffGAN-TTS 4 0.014 3.78 ±0.07
Grad-TTS 4 0.013 3.88 ±0.07
FastGrad-TTS 4 0.013 3.87 ±0.07
ResGrad 4 0.017 4.02 ±0.06
Ours (gmax) 4 0.013 4.10±0.06
Generation quality Table 2 compares the generation quality between Bridge-TTS and previous TTS
systems. As shown, both Bridge-TTS models outperform three strong diffusion-based TTS systems: our
diffusion counterpart Grad-TTS [Popov et al., 2021], the shallow diffusion model DiffSinger [Liu et al., 2022a]
and the residual diffusion model ResGrad [Chen et al., 2022c]. In comparison with the transformer-based
model FastSpeech 2 [Ren et al., 2021] and the end-to-end TTS system [Kim et al., 2021], we also exhibit
stronger subjective quality. When NFE is either 1000 or 50, our Bridge-TTS achieves superior quality. One
reason is that the condition information ( i.e., text encoder output) in TTS synthesis is strong, and the other
is that our first-order Bridger sampler maintains the sample quality when reducing the NFE.
Sampling speed Table 3 shows the evaluation of sampling speed with the Bridge-TTS-gmax model, as we
observe that it achieves higher quality than the VP-based Bridge-TTS system. To conduct a fair comparison,
we choose the NFE reported in the baseline models. As shown, in 4-step sampling, we not only outperform
our diffusion counterpart Grad-TTS [Popov et al., 2021], FastGrad-TTS [Vovk et al., 2022] using a first-order
SDE sampler, and DiffGAN-TTS [Liu et al., 2022b] by a large margin, but also achieve higher quality than
ResGrad [Chen et al., 2022c] which stands on a pre-trained FastSpeech 2 [Ren et al., 2021]. In 2-step sampling
with a RTF of 0.009, we achieve higher quality than the state-of-the-art fast sampling method CoMoSpeech
[Ye et al., 2023]. In comparison with 1-step method, FastSpeech 2 and CoMoSpeech, although our 2-step
generation is slightly slower, we achieve distinctively better quality.
4https://github.com/jik876/hifi-gan
8

--- PAGE 9 ---
4.3 Case Study
We show a sample when NFE=4 in Figure 2 (a), using our first-order ODE sampler shown in Eqn (20).
As shown, Bridge-TTS clearly generates more details of the target than the diffusion counterpart Grad-TTS
(τd= 1.5). Moreover, we show a 2-step ODE sampling trajectory of Bridge-TTS in Figure 2 (b). As shown,
with our data-to-data generation process, each sampling step is adding more details to refine the prior which
has provided strong information about the target. More generated samples can be visited in Appendix H.
Grad -TTS (𝜏=1.5)
Bridge -TTS (ODE)
Ground -truth mel-spectrogramFirst sampling stepPrior Ground -truth mel-spectrogram
Second sampling step
(a) 4-step generation result (b) 2 -step ODE sampling trajectory
Figure 2: We show a 4-step ODE generation result of Grad-TTS [Popov et al., 2021] and Bridge-TTS in
the left figure, and a 2-step ODE sampling trajectory of Bridge-TTS in the right one. The ground-truth
mel-spectrogram is shown for comparison.
4.4 Ablation Study
We conduct several comparison studies by showing the CMOS results between different designs of prior,
noise schedule, and sampler when NFE =1000 and NFE =4. The base setup is the Bridge-gmax schedule, x0
predictor, and temperature-scaled first-order SDE sampler ( τb= 2).
PriorWe explore two training strategies that differ in their prior: 1) like Grad-TTS [Popov et al.,
2021], the encoder and decoder part are joint trained from scratch (i.e., mutable prior); 2) the encoder
is first trained with a warm-up stage and then the decoder is trained from scratch (i.e., fixed prior).
Table 4: CMOS comparison of training and
sampling settings of Bridge-TTS.
Method NFE =4 NFE =1000
Bridge-TTS (gmax) 0 0
w.mutable prior - 0.13 - 0.17
w.constant g(t) - 0.12 - 0.14
w.VP - 0.03 - 0.08
w.SDE ( τb= 1) - 0.07 - 0.19
w.ODE - 0.10 + 0.00It should be noted that in both strategies, the text encoder
is trained with an equivalent objective. As shown, the latter
consistently has better sample quality across different NFEs.
Hence, we adopt it as our default setting.
Noiseschedule Wecomparethreedifferentconfigurations
for noise schedules: Bridge-gmax, Bridge-VP, and a simple
schedule with f(t) = 0 , g(t) = 5that has virtually the same
maximum marginal variance as Bridge-gmax, which we refer
to as “constant g(t)”. As shown in Table 4, Bridge-gmax
and Bridge-VP have overall similar performance, while the
constant g(t)has noticeably degraded quality than Bridge-
gmax when NFE =1000. Intuitively, the Bridge-gmax and Bridge-VP have an asymmetric pattern of marginal
variance that assigns more steps for denoising, while the constant g(t)yields a symmetric pattern. Empirically,
such an asymmetric pattern of marginal variance helps improve sample quality. We provide a more detailed
illustration of the noise schedules in Appendix E.
Sampling process For comparison between different sampling processes, the temperature-scaled SDE ( τb
= 2) achieves the best quality at both NFE =4 and NFE =1000. Compared with the vanilla SDE sampling
(i.e., τb= 1), introducing the temperature sampling technique for SDE can effectively reduce artifacts in
9

--- PAGE 10 ---
the background and enhance the sample quality when NFE is large, which is clearly reflected in the CMOS
score in Table 4. Meanwhile, the ODE sampler exhibits the same quality as the temperature-scaled SDE at
NFE=1000, but it has more evident artifacts at NFE =4.
5 Related Work
Diffusion-based TTS Synthesis Grad-TTS [Popov et al., 2021] builds a strong TTS baseline with SGMs,
surpassing the transformer-based [Ren et al., 2019] and flow-based model [Kim et al., 2020]. In the following
works, fast sampling methods are extensively studied, such as improving prior distribution [Lee et al., 2022],
designing training-free sampler [Jeong et al., 2021, Vovk et al., 2022], using auxiliary model [Chen et al.,
2022c, Liu et al., 2022a], introducing adversarial loss [Ko and Choi, 2023, Liu et al., 2022b], employing
knowledge distillation [Huang et al., 2022, Ye et al., 2023], developing lightweight U-Net [Chen et al., 2023],
and leveraging CFM framework [Guan et al., 2023, Guo et al., 2023, Mehta et al., 2023]. However, these
methods usually explore to find a better trade-off between TTS quality and sampling speed than diffusion
models instead of simultaneously improving both of them, and some of these methods require extra procedures,
such as data pre-processing, auxiliary networks, and distillation stage, or prone to training instability. In
contrast to each of the previous methods that study a data-to-noise process, we present a novel TTS system
with a tractable Schrodinger bridge, demonstrating the advantages of the data-to-data process.
Schrodinger bridge Solving the Schrodinger bridge problem with an iterative procedure to simulate
the intractable stochastic processes is widely studied [Chen et al., 2022a, De Bortoli et al., 2021, Liu et al.,
2023d, Peluchetti, 2023, Shi et al., 2023, Vargas et al., 2021, Wang et al., 2021]. Two recent works [Liu et al.,
2023a, Somnath et al., 2023] build the bridge in image translation and a biology task, while neither of them
investigates the design space discussed in our work, which is of importance to sample quality and inference
speed.
6 Conclusions
We present Bridge-TTS, a novel TTS method built on data-to-data process, enabling mel-spectrogram
generation from a deterministic prior via Schrodinger bridge. Under our theoretically elaborated tractable,
flexible SB framework, we exhaustively explore the design space of noise schedule, model parameterization,
and stochastic/deterministic samplers. Experimental results on sample quality and sampling efficiency in TTS
synthesis demonstrate the effectiveness of our approach, which significantly outperforms previous methods
and becomes a new baseline on this task. We hope our work could open a new avenue for exploiting the
board family of strong informative prior to further unleash the potential of generative models on a wide range
of applications.
References
Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun
Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference
on Machine Learning , 2023.
Mari Paz Calvo and César Palencia. A class of explicit multistep exponential integrators for semilinear
problems. Numerische Mathematik , 102:367–381, 2006.
JieChen, XingchenSong, ZhendongPeng, BinbinZhang, FupingPan, andZhiyongWu. Lightgrad: Lightweight
diffusion probabilistic model for text-to-speech. In IEEE International Conference on Acoustics, Speech,
and Signal Processing , 2023.
Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad:
Estimating gradients for waveform generation. In International Conference on Learning Representations ,
2021.
10

--- PAGE 11 ---
Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrödinger bridge using
forward-backward SDEs theory. In International Conference on Learning Representations , 2022a.
Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo P. Mandic, Lei He, and Sheng Zhao. Infergrad: Improving
diffusion models for vocoder by considering inference in training. In IEEE International Conference on
Acoustics, Speech, and Signal Processing , 2022b.
Zehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei He, Sheng
Zhao, Jiang Bian, and Danilo P. Mandic. Resgrad: Residual denoising diffusion probabilistic models for
text to speech. arXiv preprint arXiv:2212.14518 , 2022c.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrödinger bridge with
applications to score-based generative modeling. Advances in Neural Information Processing Systems , 2021.
Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, and Nader Masmoudif. Seeds: Ex-
ponential sde solvers for fast high-quality sampling from diffusion models. arXiv preprint arXiv:2305.14267 ,
2023.
Wenhao Guan, Qi Su, Haodong Zhou, Shiyu Miao, Xingjia Xie, Lin Li, and Qingyang Hong. Reflow-tts: A
rectified flow model for high-fidelity text-to-speech. arXiv preprint arXiv:2309.17056 , 2023.
Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, and Kai Yu. Voiceflow: Efficient text-to-speech with
rectified flow matching. arXiv preprint arXiv:2309.05027 , 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural
Information Processing Systems , 2020.
Marlis Hochbruck, Alexander Ostermann, and Julia Schweitzer. Exponential rosenbrock-type methods. SIAM
Journal on Numerical Analysis , 47(1):786–803, 2009.
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: end-to-end diffusion for high
resolution images. In International Conference on Machine Learning , 2023.
Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin,
Zejun Ma, and Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-to-audio generation. arXiv preprint
arXiv:2305.18474 , 2023a.
Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, and Yi Ren. Prodiff: Progressive fast
diffusion model for high-quality text-to-speech. In ACM Multimedia , 2022.
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu,
Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion
models. In International Conference on Machine Learning , 2023b.
Keith Ito and Linda Johnson. The LJ speech dataset. https://keithito.com/LJ-Speech-Dataset/ , 2017.
Kiyosi Itô. On a formula concerning stochastic differentials. Nagoya Mathematical Journal , 3:55–65, 1951.
Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-tts: A
denoising diffusion model for text-to-speech. In INTERSPEECH , 2021.
Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon. Glow-tts: A generative flow for text-to-speech
via monotonic alignment search. Advances in Neural Information Processing Systems , 2020.
Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning
for end-to-end text-to-speech. In International Conference on Machine Learning , 2021.
Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances
in Neural Information Processing Systems , 2021.
11

--- PAGE 12 ---
D.P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on
Learning Representations , 2015.
Myeongjin Ko and Yong-Hoon Choi. Adversarial training of denoising diffusion model using dual discriminators
for high-fidelity multi-speaker tts. arXiv preprint arXiv:2308.01573 , 2023.
Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and
high fidelity speech synthesis. In Advances in Neural Information Processing Systems , 2020.
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion
model for audio synthesis. In International Conference on Learning Representations , 2021.
Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh
Yoon, and Tie-Yan Liu. Priorgrad: Improving conditional denoising diffusion models with data-driven
adaptive prior. In International Conference on Learning Representations , 2022.
Yichong Leng, Zehua Chen, Junliang Guo, Haohe Liu, Jiawei Chen, Xu Tan, Danilo P. Mandic, Lei He,
Xiang-Yang Li, Tao Qin, Sheng Zhao, and Tie-Yan Liu. Binauralgrad: A two-stage conditional diffusion
probabilistic model for binaural audio synthesis. In Advances in Neural Information Processing Systems ,
2022.
Christian Léonard. Some properties of path measures. Séminaire de Probabilités XLVI , pages 207–230, 2014.
Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for
generative modeling. In International Conference on Learning Representations , 2023.
Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A Theodorou, Weili Nie, and Anima Anandkumar.
I2sb: Image-to-image schrödinger bridge. In International Conference on Machine Learning , 2023a.
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D.
Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. In International Conference
on Machine Learning , 2023b.
Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang,
Yuxuan Wang, and Mark D. Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised
pretraining. arXiv preprint arXiv:2308.05734 , 2023c.
Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow
diffusion mechanism. In AAAI Conference on Artificial Intelligence , 2022a.
Songxiang Liu, Dan Su, and Dong Yu. Diffgan-tts: High-fidelity and efficient text-to-speech with denoising
diffusion gans. arXiv preprint arXiv:2201.11972 , 2022b.
Xingchao Liu, Lemeng Wu, Mao Ye, and qiang liu. Learning diffusion bridges on constrained domains. In
International Conference on Learning Representations , 2023d.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE
solver for diffusion probabilistic model sampling in around 10 steps. In Advances in Neural Information
Processing Systems , 2022a.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for
guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 , 2022b.
Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, and Gustav Eje Henter. Matcha-tts: A fast tts
architecture with conditional flow matching. arXiv preprint arXiv:2309.03199 , 2023.
Jongseok Park, Kyubyong & Kim. g2pe. https://github.com/Kyubyong/g2p , 2019.
Stefano Peluchetti. Diffusion bridge mixture transports, schrödinger bridge problems and generative modeling.
arXiv, 2023.
12

--- PAGE 13 ---
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A
diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning , 2021.
Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, and Jiansheng Wei.
Diffusion-basedvoiceconversionwithfastmaximumlikelihoodsamplingscheme. In International Conference
on Learning Representations , 2022.
Zhibin Qiu, Mengfan Fu, Fuchun Sun, Gulila Altenbek, and Hao Huang. Se-bridge: Speech enhancement
with consistent brownian bridge. arXiv preprint arXiv:2305.13796 , 2023.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional
image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast,
robust and controllable text to speech. In Advances in Neural Information Processing Systems , 2019.
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast and
high-quality end-to-end text to speech. In International Conference on Learning Representations , 2021.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International
Conference on Learning Representations , 2022.
Erwin Schrödinger. Sur la théorie relativiste de l’électron et l’interprétation de la mécanique quantique. In
Annales de l’institut Henri Poincaré , volume 2, pages 269–310, 1932.
Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian.
Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv
preprint arXiv:2304.09116 , 2023.
Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrödinger bridge
matching. arXiv preprint arXiv:2303.16852 , 2023.
Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, and
Charlotte Bunne. Aligned diffusion schrödinger bridges. In Uncertainty in Artificial Intelligence , 2023.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International
Conference on Learning Representations , 2021a.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In International Conference on
Learning Representations , 2021b.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International
Conference on Machine Learning , 2023.
Xu Tan, Tao Qin, Frank K. Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint
arXiv:2106.15561 , 2021.
Alexander Tong, Nikolay Malkin, Kilian FATRAS, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet,
Guy Wolf, and Yoshua Bengio. Simulation-free schrödinger bridges via score and flow matching. In ICML
Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023a.
Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy
Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal
transport. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems , 2023b.
Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrödinger bridges via
maximum likelihood. Entropy, 23(9):1134, 2021.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation , 23:
1661–1674, 2011.
13

--- PAGE 14 ---
Ivan Vovk, Tasnima Sadekova, Vladimir Gogoryan, Vadim Popov, Mikhail Kudinov, and Jiansheng Wei. Fast
grad-tts: Towards efficient diffusion-based speech generation on cpu. In INTERSPEECH , 2022.
Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep generative learning via schrödinger
bridge. In International Conference on Machine Learning , 2021.
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-
fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213 ,
2023.
Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising
diffusion gans. In International Conference on Learning Representations , 2022.
Zhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, and Yike Guo. Comospeech: One-step speech and singing
voice synthesis via consistency model. In ACM Multimedia , 2023.
Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with
empirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems , 2023a.
Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood estimation
for diffusion odes. arXiv preprint arXiv:2305.03935 , 2023b.
14

--- PAGE 15 ---
Contents
1 Introduction 2
2 Background 3
2.1 Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Diffusion-Based TTS Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 Schrodinger Bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 Bridge-TTS 4
3.1 Schrodinger Bridge between Paired Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.3 Sampling Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4 Experiments 7
4.1 Training Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 Results and Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.4 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Related Work 10
6 Conclusions 10
Appendix 16
A Proofs 16
A.1 Tractable Schrodinger Bridge between Gaussian-Smoothed Paired Data . . . . . . . . . . . . . . . . . 16
A.2 Bridge Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B Relationship with Brownian Bridge, Posterior Sampling and DDIM 19
B.1 Schrodinger Bridge Problem and Brownian Bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Posterior Sampling on a Brownian Bridge and DDIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C High-Order Samplers 21
D Model Parameterization 22
E Forward Process 23
F Baseline Models 24
G Additional Results 25
G.1 CMOS Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
G.2 Preference Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
H Generated Samples 26
15

--- PAGE 16 ---
A Proofs
A.1 Tractable Schrodinger Bridge between Gaussian-Smoothed Paired Data
Proof of Proposition 3.1. First, we conduct a similar transformation to Liu et al. [2023a], which reverses the
forward-backward SDE system of the SB in Eqn. (6) and absorb the intractable term bΨ,Ψinto the boundary
condition. On one hand, by inspecting the backward SDE (Eqn. (6b)) and its corresponding PDE (the
second equation in Eqn. (7)), we can discover that if we regard bΨas a probability density function (up to a
normalizing factor, which is canceled when we compute the score by operator ∇log), then the PDE of the
backward SDE is a realization of the following forward SDE due to the Fokker-Plank equation [Song et al.,
2021b]:
dxt=f(xt, t)dt+g(t)dwt,x0∼bΨ0, (21)
and its associated density of xtisbΨt. When we assume f(xt, t) =f(t)xtas a linear drift, then Eqn. (21)
becomes a narrow-sense linear SDE , whose conditional distribution bΨt|0(xt|x0)is a tractable Gaussian, which
we will prove as follows.
Specifically, Itô’s formula [Itô, 1951] tells us that, for a general SDE with drift µtand diffusion σt:
dxt=µt(xt)dt+σt(xt)dwt (22)
Iff(x, t)is a twice-differentiable function, then
df(xt, t) =∂f
∂t(xt, t) +µt(xt)∂f
∂x(xt, t) +σ2
t(xt)
2∂2f
∂x2(xt, t)
dt+σt(xt)∂f
∂x(xt, t)dwt(23)
Denote αt=eRt
0f(τ)dτ, if we choose f(x, t) =x
αt, by Itô’s formula we have
dxt
αt
=g(t)
αtdwt (24)
which clearly leads to the result
xt
αt−x0
α0∼ N
0,Zt
0g2(τ)
α2τdτI
(25)
If we denote σ2
t=Rt
0g2(τ)
α2τdτ, finally we conclude that bΨt|0(xt|x0) =N(αtx0, α2
tσ2
tI).
On the other hand, due to the symmetry of the SB, we can reverse the time tbys= 1−tand conduct
similar derivations for Ψ, which finally leads to the result Ψt|1(xt|x1) =N(¯αtx1, α2
t¯σ2
tI).
Since we have Gaussian boundary conditions:
pdata=bΨ0Ψ0=N(x0, ϵ2I), pprior=bΨ1Ψ1=N(x1, α2
1ϵ2I) (26)
Due to the properties of Gaussian distribution, it is intuitive to assume that the marginal distributions bΨ0,Ψ1
are also Gaussian. We parameterize them with undetermined mean and variance as follows:
bΨ0=N(a, σ2I),Ψ1=N(b, α2
1σ2I) (27)
Since the conditional transitions bΨt|0,Ψt|1are known Gaussian as we have derived, the marginals at any
t∈[0,1]are also Gaussian (which can be seen as a simple linear Gaussian model):
bΨt=N(αta,(α2
tσ2+α2
tσ2
t)I),Ψt=N(¯αtb,(α2
tσ2+α2
t¯σ2
t)I) (28)
Then we can solve the coefficients a,b, σby the boundary conditions. Note that ¯σ2
0=σ2
1,¯α0=1
α1, and the
product of two Gaussian probability density functions is given by
N(µ1, σ2
1)N(µ2, σ2
2) =Nσ2
2µ1+σ2
1µ2
σ2
1+σ2
2,σ2
1σ2
2
σ2
1+σ2
2
(29)
16

--- PAGE 17 ---
We have
(bΨ0Ψ0=N(a, σ2I)N(¯α0b,(α2
0σ2+α2
0¯σ2
0)I) =N(x0, ϵ2I)
bΨ1Ψ1=N(α1a,(α2
1σ2+α2
1σ2
1)I)N(b, α2
1σ2I) =N(x1, α2
1ϵ2I)(30)
⇒

(σ2+σ2
1)a+σ2b
α1
2σ2+σ2
1=x0
α1σ2a+ (σ2+σ2
1)b
2σ2+σ2
1=x1
σ2(σ2+σ2
1)
2σ2+σ2
1=ϵ2⇒

a=x0+σ2
σ2
1
x0−x1
α1
b=x1+σ2
σ2
1(x1−α1x0)
σ2=ϵ2+p
σ4
1+ 4ϵ4−σ2
1
2(31)
The proof is then completed by substituting these solved coefficients back into Eqn. (28).
A.2 Bridge Sampling
First of all, we would like to give some background information about exponential integrators [Calvo and
Palencia, 2006, Hochbruck et al., 2009], which are widely used in recent works concerning fast sampling of
diffusion ODE/SDEs [Gonzalez et al., 2023, Lu et al., 2022a,b, Zheng et al., 2023a]. Suppose we have an
SDE (or equivalently an ODE by setting g(t) = 0):
dxt= [a(t)xt+b(t)Fθ(xt, t)]dt+g(t)dwt (32)
where Fθis the parameterized prediction function that we want to approximate with Taylor expansion. The
usual way of representing its analytic solution xtat time twith respect to an initial condition xsat time sis
xt=xs+Zt
s[a(τ)xτ+b(τ)Fθ(xτ, τ)]dτ+Zt
sg(τ)dwτ (33)
By approximating the involved integrals in Eqn. (33), we can obtain direct discretizations of Eqn. (32) such as
Euler’s method. The key insight of exponential integrators is that, it is often better to utilize the “semi-linear”
structure of Eqn. (32) and analytically cancel the linear term a(t)xt. This way, we can obtain solutions that
only involve integrals of Fθand result in lower discretization errors. Specifically, by the “variation-of-constants”
formula, the exact solution of Eqn. (32) can be alternatively given by
xt=eRt
sa(τ)dτxs+Zt
seRt
τa(r)drb(τ)Fθ(xτ, τ)dτ+Zt
seRt
τa(r)drg(τ)dwτ (34)
or equivalently (assume t < s)
xt=eRt
sa(τ)dτxs+Zt
seRt
τa(r)drb(τ)Fθ(xτ, τ)dτ+s
−Zt
se2Rt
τa(r)drg2(τ)dτϵ,ϵ∼ N(0,I)(35)
Then we prove Proposition 3.2 below.
Proof of Proposition 3.2. First, we consider the bridge SDE in Eqn. (14). By collecting the linear terms w.r.t.
xt, the bridge SDE can be rewritten as
dxt=
f(t) +g2(t)
α2
tσ2
t
xt−g2(t)
αtσ2
txθ(xt, t)
dt+g(t)dwt (36)
By corresponding it to Eqn. (32), we have
a(t) =f(t) +g2(t)
α2
tσ2
t, b(t) =−g2(t)
αtσ2
t(37)
17

--- PAGE 18 ---
The exponents in Eqn. (35) can be calculated as
Zt
sa(τ)dτ=Zt
sf(τ)dτ+Zt
s(σ2
τ)′
σ2τdτ=Zt
sf(τ)dτ+ logσ2
t
σ2s(38)
Thus
eRt
sa(τ)dτ=αtσ2
t
αsσ2s, eRt
τa(r)dr=αtσ2
t
ατσ2τ(39)
Therefore, the exact solution in Eqn. (35) becomes
xt=αtσ2
t
αsσ2sxs−αtσ2
tZt
sg2(τ)
α2τσ4τxθ(xτ, τ)dτ+αtσ2
ts
−Zt
sg2(τ)
α2τσ4τdτϵ,ϵ∼ N(0,I) (40)
where Zt
sg2(τ)
α2τσ4τdτ=Zt
s(σ2
τ)′
σ4τdτ=1
σ2s−1
σ2
t(41)
Substituting Eqn. (41) into Eqn. (40), we obtain the exact solution in Eqn. (17). If we take the first-order
approximation (i.e., xθ(xτ, τ)≈xθ(xs, s)forτ∈[t, s]), then we obtain the first-order transition rule
in Eqn. (19).
Then we consider the bridge ODE in Eqn. (16). By collecting the linear terms w.r.t. xt, the bridge ODE
can be rewritten as
dxt=
f(t)−g2(t)
2α2
t¯σ2
t+g2(t)
2α2
tσ2
t
xt+g2(t)¯αt
2α2
t¯σ2
tx1−g2(t)
2αtσ2
txθ(xt, t)
dt (42)
By corresponding it to Eqn. (32), we have
a(t) =f(t)−g2(t)
2α2
t¯σ2
t+g2(t)
2α2
tσ2
t, b 1(t) =g2(t)¯αt
2α2
t¯σ2
t, b 2(t) =−g2(t)
2αtσ2
t(43)
The exponents in Eqn. (35) can be calculated as
Zt
sa(τ)dτ=Zt
sf(τ)dτ−Zt
sg2(τ)
2α2τ¯σ2τdτ+Zt
sg2(τ)
2α2τσ2τdτ
=Zt
sf(τ)dτ+Zt
s(¯σ2
τ)′
2¯σ2τdτ+Zt
s(σ2
τ)′
2σ2τdτ
=Zt
sf(τ)dτ+1
2log¯σ2
t
¯σ2s+1
2logσ2
t
σ2s(44)
Thus
eRt
sa(τ)dτ=αtσt¯σt
αsσs¯σs, eRt
τa(r)dr=αtσt¯σt
ατστ¯στ(45)
Therefore, the exact solution in Eqn. (35) becomes
xt=αtσt¯σt
αsσs¯σsxs+¯αtσt¯σt
2Zt
sg2(τ)
α2τστ¯σ3τx1dτ−αtσt¯σt
2Zt
sg2(τ)
α2τσ3τ¯στxθ(xτ, τ)dτ (46)
Due the relation σ2
t+ ¯σ2
t=σ2
1, the integrals can be computed by the substitution θt= arctan( σt/¯σt)
Zt
sg2(τ)
α2τστ¯σ3τdτ=Zt
s(σ2
τ)′
στ¯σ3τdτ
=Zθt
θs1
σ4
1sinθcos3θd(σ2
1sin2θ)
=2
σ2
1Zθt
θs1
cos2θdθ
=2
σ2
1(tanθt−tanθs)
=2
σ2
1σt
¯σt−σs
¯σs(47)
18

--- PAGE 19 ---
and similarlyZt
sg2(τ)
α2τσ3τ¯στdτ=2
σ2
1¯σs
σs−¯σt
σt
(48)
Substituting Eqn. (47) and Eqn. (48) into Eqn. (46), we obtain the exact solution in Eqn. (18). If we take the
first-order approximation (i.e., xθ(xτ, τ)≈xθ(xs, s)forτ∈[t, s]), then we obtain the first-order transition
rule in Eqn. (20).
BRelationship with Brownian Bridge, Posterior Sampling and
DDIM
B.1 Schrodinger Bridge Problem and Brownian Bridge
For any path measure µon[0,1], we have µ=µ0,1µ|0,1, where µ0,1denotes the joint distribution of
µ0, µ1, and µ|0,1denotes the conditional path measure on (0,1)given boundaries x0,x1. A high-level
perspective is that, using the decomposition formula for KL divergence DKL(p∥pref) =DKL(p0,1∥pref
0,1) +
DKL(p|0,1∥pref
|0,1)[Léonard, 2014], the SB problem in Eqn. (5) can be reduced to the static SB prob-
lem [De Bortoli et al., 2021, Shi et al., 2023, Tong et al., 2023a,b]:
min
p0,1∈P2DKL(p0,1∥pref
0,1),s.t.p0=pdata, p1=pprior (49)
which is proved to be an entropy-regularized optimal transport problem when prefis defined by a scaled
Brownian process dxt=σdwt. We can draw similar conclusions for the more general case of reference SDE
in Eqn. (1) with linear drift f(xt, t) =f(t)xt. Specifically, the KL divergence between the joint distribution
of boundaries is
DKL(p0,1∥pref
0,1) =−Ep0,1[logpref
0,1]− H(p0,1)
=−Ep0[logpref
0]−Ep0,1[logpref
1|0]− H(p0,1)(50)
where H(·)is the entropy. As we have proved in Appendix A.1, pref
t|0(xt|x0) =N(αtx0, α2
tσ2
tI), thus
logpref
1|0(x1|x0) =−∥x1−α1x0∥2
2
2α2
1σ2
1(51)
Since Ep0[logpref
0] =Epdata[logpdata]is irrelevant to p, the static SB problem is equivalent to
min
p0,1∈P2Ep0,1(x0,x1)[∥x1−α1x0∥2
2]−2α2
1σ2
1H(p0,1),s.t.p0=pdata, p1=pprior (52)
Therefore, it is an entropy-regularized optimal transport problem when α1= 1.
While the static SB problem is generally non-trivial, there exists application cases when we can skip it:
when the coupling p0,1ofpdataandpprioris unique and has no room for further optimization. (1) When pdata
is a Dirac delta distribution and pprioris a usual distribution [Liu et al., 2023a]. In this case, the SB is half
tractable, and only the bridge SDE holds. (2) When paired data are considered, i.e., the coupling of pdata
andpprioris mixtures of dual Dirac delta distributions. In this case, however, DKL(p0,1∥pref
0,1) =∞, and the
SB problem will collapse. Still, we can ignore such singularity, so that the SB is fully tractable, and bridge
ODE can be derived.
After the static SB problem is solved, we only need to minimize DKL(p|0,1∥pref
|0,1)in order to solve the
original SB problem. In fact, since there is no constraints, such optimization directly leads to pt|0,1=pref
t|0,1
fort∈(0,1). When prefis defined by a scaled Brownian process dxt=σdwt,pref
t|0,1is the common
Brownian bridge [Qiu et al., 2023, Tong et al., 2023a,b]. When prefis defined by the narrow-sense linear SDE
dxt=f(t)xtdt+g(t)dwtwhich we considered, pref
t|0,1can be seen as the generalized Brownian bridge with
linear drift and time-varying volatility, and we can derive its formula as follows.
Similar to the derivations in Appendix A.1, the transition probability from time sto time t(s < t)
following the reference SDE dxt=f(t)xtdt+g(t)dwtis
pref
t|s(xt|xs) =N(xt;αt|sxs, α2
t|sσ2
t|sI) (53)
19

--- PAGE 20 ---
where αt|s, σt|sare the corresponding coefficients to αt, σt, while modifying the lower limit of integrals from 0
tos:
αt|s=eRt
sf(τ)dτ, σ2
t|s=Zt
sg2(τ)
α2
τ|sdτ (54)
We can easily identify that αt|s, σt|sare related to αt, σtby
αt|s=αt
αs, σ2
t|s=α2
s(σ2
t−σ2
s) (55)
Therefore
pref
t|s(xt|xs) =N
xt;αt
αsxs, α2
t(σ2
t−σ2
s)I
(56)
Due to the Markov property of the SDE, we can compute pref
t|0,1as
pref
t|0,1(xt|x0,x1) =pref
t,1|0(xt,x1|x0)
pref
1|0(x1|x0)
=pref
t|0(xt|x0)pref
1|t(x1|xt)
pref
1|0(x1|x0)
∝exp
−∥xt−αtx0∥2
2
2α2
tσ2
t
exp
−∥x1−α1
αtxt∥2
2
2α2
1(σ2
1−σ2
t)
exp
−∥x1−α1x0∥2
2
2α2
1σ2
1
∝exp
−∥xt−αtx0∥2
2
2α2
tσ2
t−∥xt−¯αtx1∥2
2
2α2
t¯σ2
t
∝exp
−∥xt−αt¯σ2
tx0+¯αtσ2
tx1
σ2
1∥2
2
2α2
tσ2
t¯σ2
t
σ2
1
(57)
Therefore, pref
t|0,1=N
αt¯σ2
tx0+¯αtσ2
tx1
σ2
1,α2
t¯σ2
tσ2
t
σ2
1I
, which equals the SB marginal in Eqn. (12).
B.2 Posterior Sampling on a Brownian Bridge and DDIM
Posterior Sampling and Bridge SDE Liu et al. [2023a] proposes a method called posterior sampling to
sample from bridge: when prefis defined by dxt=√βtdwt, we can sample xN−1, . . . ,xn+1,xn, . . . ,x0at
timesteps tN−1, . . . , t n+1, tn, . . . , t 0sequentially, where at each step the sample is generated from the DDPM
posterior [Ho et al., 2020]:
p(xn|x0,xn+1) =N
xn;α2
n
α2n+σ2nx0+σ2
n
α2n+σ2nxn+1,σ2
nα2
n
α2n+σ2nI
, (58)
where α2
n=Rtn+1
tnβ(τ)dτis the accumulated noise between two timesteps (tn, tn+1),σ2
n=Rtn
0β(τ)dτ, and
x0is predicted by the network.
While they only consider f(t) = 0and prove the case for discrete timesteps by onerous mathematical
induction, such posterior is essentially a “shortened” Brownian bridge. Suppose we already draw a sample
xs∼pref
s|0,1, then the sample at time t < scan be drawn from pref
t|0,1,s, which equals pref
t|0,sdue to the Markov
property of the SDE. Similar to the derivation in Eqn. (57), such shortened Brownian bridge is
pref
t|0,s(xt|x0,xs) =N 
xt;αt(σ2
s−σ2
t)x0+αt
αsσ2
txs
σ2s,α2
tσ2
t(σ2
s−σ2
t)
σ2sI!
(59)
which is exactly the same as the first-order discretization of bridge SDE in Eqn. (19) when x0is predicted by
the network xθ(xs, s).
20

--- PAGE 21 ---
DDIM and Bridge ODE DDIM [Song et al., 2021a] is a sampling method for diffusion models, whose
deterministic case is later proved to be the first-order discretization of certain solution forms of the diffusion
ODE [Lu et al., 2022a,b]. Under our notations of αt, σ2
t, the update rule of DDIM is [Lu et al., 2022b]
xt=αtσt
αsσsxs+αt
1−σ2
t
σ2s
xθ(xs, s) (60)
In the limit ofσs
σ1,σt
σ1→0, we have¯σs
σ1,¯σt
σ1→1. Therefore,¯σt
¯σs→1, and we can discover that Eqn. (20)
reduces to Eqn. (60).
Corollary B.1 (1-step First-Order Bridge SDE/ODE Sampler Recovers Direct Data Prediction) .When
s= 1andt= 0, the first-order discretization of bridge SDE/ODE is
x0=xθ(x1,1) (61)
C High-Order Samplers
We can develop high-order samplers by approximating xθ(xτ, τ), τ∈[t, s]with high-order Taylor ex-
pansions. Specifically, we take the second-order case of the bridge SDE as an example. For the integralRt
sg2(τ)
α2τσ4τxθ(xτ, τ)dτin Eqn. (17), we can use the change-of-variable λt=−1
σ2
t. Since (λt)′=g2(t)
α2
tσ4
t, the integral
becomesZt
sg2(τ)
α2τσ4τxθ(xτ, τ)dτ=Zλt
λsxθ(xτλ, τλ)dλ
≈Zλt
λsxθ(xs, s) + (λ−λs)x(1)
θ(xs, s)dλ
= (λt−λs)xθ(xs, s) +(λt−λs)2
2x(1)
θ(xs, s)(62)
where τλis the inverse mapping of λτ,x(1)
θis the first-order derivative of xθw.r.t λ, and we have used the
second-order Taylor expansion xθ(xτλ, τλ)≈xθ(xs, s) + (λ−λs)x(1)
θ(xs, s).x(1)
θcan be estimated by finite
difference, and a simple treatment is the predictor-corrector method. We first compute ˆxtby the first-order
update rule in Eqn. (19), which is used to estimate x(1)
θ(xs, s):x(1)
θ(xs, s)≈xθ(ˆxt,t)−xθ(xs,s)
λt−λs. Substituting
it into Eqn. (62), we haveRt
sg2(τ)
α2τσ4τxθ(xτ, τ)dτ≈(λt−λs)xθ(xs,s)+xθ(ˆxt,t)
2which literally can be seen as
replacing xθ(xs, s)in Eqn. (19) withxθ(xs,s)+xθ(ˆxt,t)
2. Similar derivations can be done for the bridge ODE.
We summarize the second-order samplers in Algorithm 1 and Algorithm 2.
Algorithm 1 Second-order sampler for the bridge SDE
Input:Number of function evaluations (NFE) 2N, timesteps 1 =tN> tN−1>···> tn> tn−1>···> t0=
0, initial condition x1
1:forn=Nto1do
2:s←tn
3:t←tn−1
4:Prediction: ˆxt←αtσ2
t
αsσ2sxs+αt
1−σ2
t
σ2s
xθ(xs, s) +αtσtq
1−σ2
t
σ2sϵ,ϵ∼ N(0,I)
5:Correction: xt←αtσ2
t
αsσ2sxs+αt
1−σ2
t
σ2s
xθ(xs,s)+xθ(ˆxt,t)
2+αtσtq
1−σ2
t
σ2sϵ,ϵ∼ N(0,I)
6:end for
Output: x0
21

--- PAGE 22 ---
Algorithm 2 Second-order sampler for the bridge ODE
Input:Number of function evaluations (NFE) 2N, timesteps 1 =tN> tN−1>···> tn> tn−1>···> t0=
0, initial condition x1
1:forn=Nto1do
2:s←tn
3:t←tn−1
4:Prediction: ˆxt←αtσt¯σt
αsσs¯σsxs+αt
σ2
1h
¯σ2
t−¯σsσt¯σt
σs
xθ(xs, s) +
σ2
t−σsσt¯σt
¯σs
x1
α1i
5:Correction: xt←αtσt¯σt
αsσs¯σsxs+αt
σ2
1h
¯σ2
t−¯σsσt¯σt
σs
xθ(xs,s)+xθ(ˆxt,t)
2+
σ2
t−σsσt¯σt
¯σs
x1
α1i
6:end for
Output: x0
D Model Parameterization
Apart from x0predictor xθpresented in Section 3.2, we can consider other parameterizations:
•Noise predictor ϵbΨ
θcorresponding to ∇logbΨt=−xt−αtx0
α2
tσ2
tthat used in I2SB [Liu et al., 2023a]. The
prediction target of ϵbΨ
θis:
ϵbΨ
θ→xt−αtx0
αtσt(63)
•Noise predictor ϵSB
θcorresponding to the score ∇logptof the SB. Since ∇logpt(xt) =−xt−αt¯σ2
tx0+¯αtσ2
tx1
σ2
1
α2
t¯σ2
tσ2
t
σ2
1,
the prediction target of ϵSB
θis
ϵSB
θ→xt−αt¯σ2
tx0+¯αtσ2
tx1
σ2
1
αt¯σtσt
σ1(64)
•Velocity predictor vθarising from flow matching techniques [Lipman et al., 2023, Tong et al., 2023a,b,
Zheng et al., 2023b], which aims to directly predict the drift of the PF-ODE:
vθ→f(t)xt−1
2g2(t)xt−¯αtx1
α2
t¯σ2
t+1
2g2(t)xt−αtx0
α2
tσ2
t(65)
Empirically, across all parameterizations, we observe that the x0predictor and the noise predictor ϵbΨ
θwork
well in the TTS task and Table 5 shows that the x0predictor is generally better in sample quality. Hence, we
adopt the x0predictor as the default training setup for Bridge-TTS. For the ϵSB
θpredictor and vθpredictor,
we find that they lead to poor performance on the TTS task. We can intuitively explain this phenomenon by
taking a simple case f(t) = 0 , g(t) =σ. In this case, we have xt= (1−t)x0+tx1+σp
t(1−t)ϵ,ϵ∼ N(0,I),
and the prediction targets are
xθ→x0
ϵbΨ
θ→xt−x0
σ√
t=√
t(x1−x0) +σ√
1−tϵ
ϵSB
θ→xt−(1−t)x0−tx1
σp
t(1−t)=ϵ
p
t(1−t)vθ→(1−2t)xt−(1−t)x0+tx1
2p
t(1−t)=p
t(1−t)(x1−x0) +σ1−2t
2ϵ(66)
Therefore, ϵSB
θandvθboth predict ϵwhen t→1, while xθandϵbΨ
θtends to predict x0,x1-related terms in
such scenario. We can conclude that the former way of prediction is harmful on TTS task.
22

--- PAGE 23 ---
Table 5: CMOS comparison of different parameterizations of Bridge-TTS.
Method NFE =4 NFE =1000
Bridge-TTS (gmax + x0predictor) 0 0
Bridge-TTS (gmax + ϵbΨ
θpredictor) - 0.15 - 0.12
E Forward Process
In this section, we display the stochastic trajectory of the Bridge-SDE in Eqn. (14) and compare it with
the diffusion counterpart in Eqn. (4). In general, the marginal distribution of these SDEs shares the form
pt=N(xt;wtx0+¯wtx1,˜σ2
tI). In Figure 3, we show the scaling factors wtand ¯wtforx0andx1and the
variance ˜σ2
tat time t. As described in Section 4.4, the Bridge-gmax and Bridge-VP have an asymmetric
pattern of marginal variance that uses more steps to denoise towards the ground truth x0, while the constant
g(t)schedule specifies the same noise-additive and denoising steps. As a comparison, the diffusion-based
model only performs denoising steps.
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Scaling Factor wt for x0
Grad-TTS
Bridge-VP
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Scaling Factor wt for x1
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Variance t
0.0 0.2 0.4 0.6 0.8 1.0
t0.00.20.40.60.81.0Bridge-gmax
Bridge-constant g(t)
0.0 0.2 0.4 0.6 0.8 1.0
t0.00.20.40.60.81.0
0.0 0.2 0.4 0.6 0.8 1.0
t0123456
Figure 3: The scaling factor and variance in Grad-TTS and Bridge-TTS.
23

--- PAGE 24 ---
F Baseline Models
Apart from ground-truth recording and the sample synthesized by vocoder from ground-truth mel-
spectrogram, we take seven diffusion-based TTS systems, one end-to-end TTS system, and one transformer-
based TTS model as our baseline models. We follow their official implementation or the settings reported in
their publications to produce the results. We introduce each of our baseline models below:
1. FastSpeech 2 [Ren et al., 2021] is one of the most popular non-autoregressive TTS models, and
widely used as the baseline in previous diffusion-based TTS systems [Chen et al., 2022c, Liu et al., 2022a, Ye
et al., 2023]. Following its original setting, we train the model with a batch size of 48 sentences and 160k
training steps until convergence by using 8 NVIDIA V100 GPU.
2. VITS [Kim et al., 2021] provides a strong baseline of end-to-end TTS systems and is widely taken
as a baseline in TTS systems for sample quality comparison. Different from other baseline models using
pre-trained vocoder to generate waveform, VITS directly synthesizes waveform from text input. In training
and testing, we follow their open-source implementation5.
3. DiffSinger [Liu et al., 2022a] is a TTS model developed for TTS synthesis and text-to-singing synthesis.
It is built on denoising diffusion probabilistic models [Ho et al., 2020], using standard Gaussian noise N(0,I)
in the diffusion process. Moreover, an auxiliary model is trained to enable its shallow reverse process,
i.e., reducing the distance between prior distribution and data distribution. We follow their open-source
implementation6, which contains a warm-up stage for auxiliary model training and a main stage for diffusion
model training.
4. DiffGAN-TTS [Liu et al., 2022b]7develops expressive generator and time-dependent discriminator
to learn the non-Gaussian denoising distribution [Xiao et al., 2022] in few-step sampling process of diffusion
models. Following their publication, we train DiffGAN-TTS with time steps T= 4. For both the generator
and the discriminator, we use the Adam optimizer, with β1= 0.5andβ2= 0.9. Models are trained using a
single NVIDIA V100 GPU. We set the batch size as 32, and train models for 400k steps until loss converges.
5. ProDiff [Huang et al., 2022] is a fast TTS model using progressive distillation [Salimans and Ho, 2022].
The standard Gaussian noise N(0,I)is used in the diffusion process and taken as the prior distribution. We
use their 2-step diffusion-based student model, which is distilled from a 4-step diffusion-based teacher model
(x0prediction). We follow their open-source implementation8.
6. Grad-TTS [Popov et al., 2021]9is a widely used baseline in diffusion models [Chen et al., 2023, 2022c,
Huang et al., 2022, Ye et al., 2023] and conditional flow matching [Guo et al., 2023, Mehta et al., 2023] based
TTS systems. It is established on SGMs, providing a strong baseline of generation quality. Moreover, it
realizes fast sampling with the improved prior distribution N(µ,I)and the temperature parameter τ= 1.5
in inference. Following its original setting and publicly available implementation, we train the model with a
batch size of 16 and 1.7 million steps on 1 NVIDIA 2080 GPU. The Adam optimizer is used and the learning
rate is set to a constant, 0.0001.
7. FastGrad-TTS [Vovk et al., 2022] equips pre-trained Grad-TTS [Popov et al., 2021] with the
first-order SDE sampler proposed by [Popov et al., 2022]. The Maximum Likelihood solver reduces the
mismatch between the reverse and the forward process. In comparison with the first-order Euler scheme, this
solver has shown improved quality in both voice conversion and TTS synthesis. We implement it for the
pre-trained Grad-TTS model with the Equation (6)-(9) in its publication.
8. ResGrad [Chen et al., 2022c] is a diffusion-based post-processing module to improve the TTS sample
quality, where the residual information of a pre-trained FastSpeech 2 [Ren et al., 2021] model is generated by
a diffusion model. The standard Gaussian noise N(0,I)is used in the diffusion process and taken as prior.
We invite the authors to generate some test samples for us.
9. CoMoSpeech [Ye et al., 2023]10is a recent fast sampling method in TTS and text-to-singing synthesis,
achieving one-step generation with the distillation technique in consistency models [Song et al., 2023]. As
Grad-TTS is employed as its TTS backbone, the model uses N(µ,I)as prior distribution and is trained for
5https://github.com/jaywalnut310/vits
6https://github.com/MoonInTheRiver/DiffSinger
7https://github.com/keonlee9420/DiffGAN-TTS
8https://github.com/Rongjiehuang/ProDiff
9https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS
10https://github.com/zhenye234/CoMoSpeech
24

--- PAGE 25 ---
1.7 million iterations on a single NVIDIA A100 GPU with a batch size of 16. The Adam optimizer is adopted
with a learning rate 0.0001.
G Additional Results
G.1 CMOS Test
Table 6: CMOS criteria.
Comparison Quality Left better Equal Right better
Rating +3+2+10-1-2-3
We conduct the Comparison Mean Opinion Score (CMOS) test by 15 Master workers on Amazon
Mechanical Turk to compare the generation quality of two different models. The raters give a score from
+3 (left better) to -3 (right better) with 1 point increments on each of the 20pair samples generated by
two models, as shown in Table 6. The three scores 3, 2, 1 denotes much better, better, and slightly better,
respectively. The plus sign +and the minus sign −denotes the left and the right model respectively.
Table 7: CMOS comparison between Grad-TTS and Bridge-TTS.
Method CMOS ( ↑)
Grad-TTS 0
Bridge-TTS (gmax) +0.21
To further demonstrate the advantage of data-to-data process in Bridge-TTS over the data-to-noise process
in Grad-TTS, we conduct a CMOS test between Bridge-TTS (Bridge-gmax schedule with β0= 0.01, β1= 50,
x0predictor, and first-order SDE sampler with τb= 2) and Grad-TTS ( τd= 1.5). As shown in Table 7, our
Bridge-TTS distinctively outperforms our diffusion counterpart Grad-TTS.
G.2 Preference Test
Apart from using the MOS and CMOS tests to evaluate sample quality, we conducted a blind preference
test when NFE=1000 and NFE=2, in order to demonstrate our superior generation quality and efficient
sampling process, respectively. In each test, we generated 100 identical samples with two different models
from the test set LJ001 and LJ002, and invited 11 judges to compare their overall subjective quality. The
judge gives a preference when he thinks a model is better than the other, and an identical result when he
thinks it is hard to tell the difference or the models have similar overall quality. In both preference tests, the
settings of noise schedule, model parameterization and sampling process in Bridge-TTS are Bridge-gmax
schedule with β0= 0.01, β1= 50,x0predictor, and first-order SDE sampler with τb= 2, respectively.
In the case of NFE=1000, as shown in Figure 4 (a), when Bridge-TTS-1000 is compared with our diffusion
counterpart Grad-TTS-1000 [Popov et al., 2021] (temperature τd= 1.5), 8 of the 11 invited judges vote for
Bridge-TTS-1000, and 3 of them think the overall quality is similar. In our blind test, none of the 11 judges
preferred Grad-TTS-1000 to Bridge-TTS-1000. The comparison result is aligned with the MOS test shown in
Table and CMOS test shown in Table 7.
In the case of NFE=2, as shown in Figure 4 (b), when Bridge-TTS-2 is compared with state-of-the-art
fast sampling method in diffusion-based TTS systems, CoMoSpeech (1-step generation) [Ye et al., 2023],
9 of the 11 invited judges vote for Bridge-TTS-2, and 2 of the judges vote for CoMoSpeech-1. Although
Bridge-TTS employs 2 sampling steps while CoMoSpeech-1 only uses 1, the RTF of both methods have
been very small (0.007 for CoMoSpeech-1 vs 0.009 for Bridge-TTS-2), and Bridge-TTS does not require any
distillation process. According to our collected feedback, 9 judges think the overall quality (e.g., quality,
naturalness, and accuracy) of Bridge-TTS is significantly better.
25

--- PAGE 26 ---
(a) (b)Figure 4: The preference test between Bridge-TTS and diffusion-based TTS systems.
H Generated Samples
With the pre-trained HiFi-GAN [Kong et al., 2020] vocoder, we show the 80-band mel-spectrogram
of several synthesized test samples of baseline models and our Bridge-TTS (Bridge-gmax schedule with
β0= 0.01, β1= 50,x0predictor, and first-order SDE sampler with τb= 2) below. The mel-spectrogram
of ground-truth recording is shown for comparison. More generated speech samples can be visited on our
website: https://bridge-tts.github.io/ .
26

--- PAGE 27 ---
1000-step generation As exhibited in Figure 5a and Figure 5b, when NFE=1000, our method generates
higher-quality speech than Grad-TTS (temperature τd= 1.5) built on data-to-noise process, demonstrating
the advantage of our proposed data-to-data process over data-to-noise process in TTS.
Grad -TTS
Recording
Bridge -TTS
(a)
Grad -TTS
Bridge -TTS
Recording
(b)
Figure 5: The mel-spectrogram of synthesized (NFE=1000) and ground-truth LJ001-0006 and LJ002-0029.
27

--- PAGE 28 ---
50-Step Generation In Figure 6a, our method shows higher generation quality than Grad-TTS [Popov
et al., 2021]. In Figure 6b, we continue to use the test sample LJ002-0029 to demonstrate our performance. In
comparison with NFE=1000 shown in Figure 5b, when reducing NFE from 1000 to 50, Grad-TTS generates
fewer details and sacrifices the sample quality, while our method still generates high-quality samples.
Grad -TTS
Bridge -TTS
Recording
(a)
Grad -TTS
Bridge -TTS
Recording
(b)
Figure 6: The mel-spectrogram of synthesized (NFE=50) and ground-truth LJ001-0035 and LJ002-0029.
28

--- PAGE 29 ---
4-Step Generation In Figure 7, we show our comparison with two baseline models, i.e., Grad-TTS [Popov
et al., 2021] and FastGrad-TTS [Vovk et al., 2022]. The latter one employs a first-order maximum-likelihood
solver [Popov et al., 2022] for the pre-trained Grad-TTS, and reports stronger quality than Grad-TTS in
4-step synthesis. In our observation, when NFE=4, FastGrad-TTS achieves higher quality than Grad-TTS,
while our method Bridge-TTS achieves higher generation quality than both of them, demonstrating the
advantage of our proposed data-to-data process on sampling efficiency in TTS synthesis.
Grad -TTS
FastGrad -TTS
Bridge -TTS
Recording
Figure 7: The mel-spectrogram of synthesized (NFE=4) and ground-truth sample LJ001-0032.
29

--- PAGE 30 ---
2-Step Generation When NFE is reduced to 2, we compare our method Bridge-TTS with the transformer-
based model FastSpeech 2 [Ren et al., 2021] and two diffusion-based TTS systems using distillation techniques.
ProDiff [Huang et al., 2022] employs progressive distillation achieving 2-step generation. CoMoSpeech [Ye
et al., 2023] employs consistency distillation achieving 1-step generation. In our observation, in this case, the
RTF of each model has been very small, and the overall generation quality is reduced. In the subjective test,
our Bridge-TTS outperforms the other three methods. We show a short test sample, LJ001-0002, in Figure 8.
CoMoSpeech (NFE=1)FastSpeech 2
ProDiff (NFE=2)
Bridge -TTS (NFE=2)
Recording
Figure 8: The mel-spectrogram of synthesized (NFE ≤2) and ground-truth sample LJ001-0002.
30
