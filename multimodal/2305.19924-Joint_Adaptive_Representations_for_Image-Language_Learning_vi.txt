# 2305.19924.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2305.19924.pdf
# Kích thước tệp: 447365 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Biểu diễn Thích ứng Kết hợp cho Học tập Hình ảnh-Ngôn ngữ
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia}@google.com
Tóm tắt
Các mô hình transformer hình ảnh-ngôn ngữ đã đạt được thành công to lớn, nhưng chúng đi kèm với chi phí tính toán cao. Chúng tôi đề xuất học biểu diễn hình ảnh-ngôn ngữ thích ứng kết hợp, thích ứng và lặp đi lặp lại kết hợp các đặc trưng đa phương thức. Điều này liên tục giảm chi phí và kích thước mô hình, cho phép mô hình mở rộng quy mô mà không tăng đáng kể FLOPs hoặc bộ nhớ, và vượt trội hơn các mô hình lớn hơn và đắt đỏ hơn nhiều. Chỉ với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình của chúng tôi vượt trội hơn các mô hình lớn gấp nhiều lần, một số đạt 800 GFLOPs.

1. Giới thiệu
Học tập thị giác-và-ngôn ngữ đã có những bước tiến lớn gần đây [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. Các mô hình này có thể gán thành công của chúng cho việc mở rộng quy mô các mô hình Transformer nổi tiếng [25], do đó cần các tập dữ liệu rất lớn. Một thành phần quan trọng của các mô hình này là xây dựng biểu diễn thị giác-ngôn ngữ kết hợp cơ bản nắm bắt mối quan hệ giữa các phương thức [5, 5, 7, 9–12, 12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. Tuy nhiên, các cơ chế attention đắt đỏ được áp dụng trong Transformers, trong đó tính toán cần thiết tăng theo bậc hai với sự gia tăng kích thước đầu vào; hơn nữa, các mô hình này hoạt động tốt hơn với nhiều dữ liệu hơn đáng kể [6] và các bước huấn luyện để học biểu diễn kết hợp; và cuối cùng, vì các tập dữ liệu lớn khó thu thập, các tập dữ liệu được thu thập tự động chứa lượng lớn nhiễu. Tất cả điều này làm cho các mô hình này thậm chí còn kém hiệu quả và đắt đỏ hơn để huấn luyện: mở rộng quy mô mô hình, kết hợp với việc mở rộng quy mô dữ liệu tương ứng cần thiết, và huấn luyện với lượng lớn nhiễu, đòi hỏi lượng lớn tính toán. Do đó, mong muốn xây dựng các biểu diễn thị giác-ngôn ngữ hiệu quả hơn về bộ nhớ, FLOPs và dữ liệu, nơi người ta có thể tận dụng quy mô mô hình nhưng theo cách hiệu quả hơn.

Để đạt được điều đó, chúng tôi đề xuất Biểu diễn Thích ứng Kết hợp cho học tập hình ảnh-ngôn ngữ hiệu quả (Hình 1). Phương pháp của chúng tôi đầu tiên giảm số lượng token trong các phương thức đầu vào, sau đó kết hợp chúng một cách thích ứng. Quá trình này giảm đáng kể FLOPs, trong khi duy trì hoặc cải thiện hiệu suất. Nó tạo ra biểu diễn nhỏ gọn và hiệu quả hơn, giảm 33% FLOPs so với phép nối thường được sử dụng, trong khi cải thiện hiệu suất. Điều này dẫn đến các mô hình hiệu quả hơn về dữ liệu và tính toán.

Chúng tôi đánh giá phương pháp trên các tác vụ Trả lời Câu hỏi Thị giác (VQA), nơi hiểu biết kết hợp hình ảnh trong bối cảnh đầu vào ngôn ngữ là quan trọng. Mô hình của chúng tôi hoạt động cạnh tranh với các mô hình hiện đại (SOTA), thậm chí vượt trội hơn các mô hình có quy mô tham số và dữ liệu lớn (Hình 1). Các phương pháp trước đây, Perceiver [9], Co-Tokenization [19] cũng đề xuất các phương pháp học tập thị giác-ngôn ngữ kết hợp hiệu quả, phương pháp của chúng tôi đề xuất cơ chế 'cập nhật' thông tin giữa các phương thức và kết hợp đặc trưng tốt hơn, vượt qua hai phương pháp này cả về độ chính xác và giảm FLOPs.

Phương pháp của chúng tôi cho phép mở rộng quy mô mô hình tốt hơn, sử dụng ít FLOPs hơn nhiều với kích thước mô hình và kích thước hình ảnh đầu vào tăng (Hình 2). Đóng góp chính của công trình chúng tôi là phương pháp kết hợp hình ảnh-văn bản mới hiệu quả và chính xác hơn các phương pháp trước đây. Điều này cho phép chúng tôi trình bày một mô hình hình ảnh-ngôn ngữ nhỏ gọn mới với hiệu suất xuất sắc, đạt được với một phần chi phí và dữ liệu.

2. Biểu diễn Thích ứng Kết hợp
Câu hỏi chính chúng tôi giải quyết là làm thế nào để kết hợp các đặc trưng từ các phương thức đầu vào thị giác và ngôn ngữ. Một vài phương pháp cơ bản sử dụng: (1) nối hoặc (2) cross-attention. Một vấn đề chính với nối là nó tăng đáng kể số lượng token bằng cách thêm H*W vào độ dài văn bản (H, W là chiều cao và chiều rộng của đặc trưng hình ảnh). Do đó, khi kích thước hình ảnh tăng, việc nối tăng đáng kể yêu cầu FLOPs và bộ nhớ của mô hình (Hình 2), ví dụ [5, 7, 12, 14, 15, 23, 29]. Ở đây, chúng tôi đề xuất phương pháp giảm số lượng token, cải thiện hiệu quả. Các phương pháp dựa trên cross-attention có các vấn đề khác, chủ yếu là phương thức được sử dụng cho truy vấn (thường là văn bản, ví dụ ALBEF [11], BLIP [10]), xác định kích thước biểu diễn đầu ra. Thường cho các tác vụ thị giác-ngôn ngữ, các đặc trưng thị giác có nhiều token (ví dụ, các token thị giác là 14x14 = 196 cho kích thước hình ảnh đầu vào khiêm tốn 224x224), trong khi văn bản khá ngắn, ví dụ 10 token trong VQA2.0. Khi sử dụng cross-attention, toàn bộ đầu vào thị giác phải được nén vào các biểu diễn token văn bản ít này, hạn chế đáng kể lượng thông tin thị giác có thể được sử dụng. Trong khi phương pháp này có ít FLOPs hơn việc nối, nó mất thông tin, có thể giảm hiệu suất tác vụ, và tạo sự phụ thuộc vào độ dài văn bản đầu vào. Tự nhiên, biểu diễn đa phương thức này sẽ có ít tiện ích hơn khi tăng kích thước hình ảnh đầu vào.

Thay vào đó, chúng tôi đề xuất một mô-đun cho phép học tập đặc trưng thị giác-ngôn ngữ tốt hơn bằng cách kết hợp thông tin thị giác hiệu quả hơn và kết hợp nó với thông tin văn bản. Bằng cách tokenize thích ứng và lặp lại các đầu vào, mô hình có thể tinh chỉnh biểu diễn đặc trưng học từ cả hai phương thức trong quá trình huấn luyện, trong khi giữ số lượng FLOPs hợp lý (Hình 3).

Phương pháp của chúng tôi dựa trên một số hiểu biết. Đầu tiên, chúng tôi truy vấn hình ảnh để có được các token thị giác thông tin hơn. Trước đây, điều này được thực hiện bằng phương pháp giống TokenLearner [19, 21]. Tuy nhiên, phương pháp này, trong khi giảm FLOPs, đáng chú ý cho các ứng dụng video trong [19], vẫn sử dụng khá nhiều FLOPs để tạo ra và áp dụng bản đồ attention, và không mở rộng quy mô tốt với kích thước hình ảnh. Thay vào đó, chúng tôi sử dụng phương pháp lai được truyền cảm hứng từ Perceiver [9]. Chúng tôi tạo ra N token độc lập từ mỗi phương thức như bước đầu tiên. Thứ hai, sau đó chúng tôi sử dụng cơ chế cross-attention trực tiếp giữa các đặc trưng văn bản mới và thị giác nhỏ gọn để tạo ra biểu diễn đa phương thức tốt hơn. Cơ chế này bao gồm một lớp cross-attention, sau đó một lớp self-attention và một Multi-Layer Perceptron (MLP), tương tự như lớp Transformer tiêu chuẩn [25], nhưng do các token giảm, nhẹ hơn nhiều.

Cuối cùng, quá trình này được thực hiện lặp lại, do đó tinh chỉnh biểu diễn hiện tại dựa trên tập hợp các đặc trưng từ Transformer. Điều này cho phép mô hình động cập nhật và chọn các đặc trưng thị giác và văn bản khác nhau ở mỗi bước để có thể thực hiện tác vụ tốt nhất, mà không tăng chi phí tính toán. Phương pháp của chúng tôi được mô tả chi tiết dưới đây.

Gọi Xtext và Xim là đầu vào cho văn bản và hình ảnh, tương ứng. Cụ thể hơn Xtext ∈ RL×D và Xim ∈ RH×W×C, giả sử đầu vào thị giác có kích thước W×H, văn bản có độ dài L. Mục tiêu là tạo ra biểu diễn đặc trưng mới, thấp chiều hơn. Điều này có thể được thực hiện bằng cách giảm biểu diễn xuống số lượng token thấp hơn, điều này đặc biệt quan trọng cho các đặc trưng thị giác vì chúng nhiều hơn. Điều này được thực hiện bằng cách đầu tiên thống nhất các chiều biểu diễn, cụ thể hơn là chiếu các đặc trưng thị giác vào không gian H*W×D, trong đó D là chiều đặc trưng cho đầu vào văn bản:

P(Xim) = W1Xim, (1)

trong đó P(Xim) ∈ RH*W×D. Ở đây, bằng W1 chúng tôi ký hiệu phép toán có thể học, ví dụ áp dụng lớp kết nối đầy đủ, chiếu đặc trưng hình ảnh vào không gian D-chiều. Về nguyên tắc cả đầu vào thị giác và đầu vào văn bản có thể được chiếu vào một chiều đặc trưng mới ví dụ D', do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào, tuy nhiên Phương trình 1 được sử dụng ở đây để đơn giản. Về nguyên tắc cả đầu vào thị giác và đầu vào văn bản có thể được chiếu vào một chiều đặc trưng mới ví dụ D', do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào.

Như bước thứ hai, chúng tôi tiến hành học một tập hợp N token có thể học mới XN ∈ RN×D, được thực hiện theo kiểu học đặc trưng DETR [4]. Tức là, XN là biểu diễn được khởi tạo ngẫu nhiên được học thông qua lan truyền ngược cùng với các tham số khác để giảm thiểu mất mát.

fN = W2Φ(XN, P(Xim)). (2)

Ở đây P(Xim) đại diện cho chiếu đặc trưng thị giác từ Phương trình 1, XN là các đặc trưng tiềm ẩn đã học, Φ là phép toán multi-head attention tiêu chuẩn. Điều này tạo ra fN, biểu diễn trung gian nhỏ gọn với N đặc trưng. Điều này cũng có thể được xem như học N token mới, đại diện cho đầu vào của M token, trong đó N≪M, cho đầu vào thị giác lớn M=H*W. Chúng tôi lưu ý rằng điều này tương tự như kiến trúc Perceiver [9], mặc dù nó chỉ được thực hiện một lần ở đây.

Quá trình này cũng được thực hiện với Xtext, tạo ra N đặc trưng văn bản (tN). Do đó, không giống như công trình trước đây (ví dụ [10,11]), N không cần phải gắn với độ dài văn bản đầu vào; vì vậy một biểu diễn phong phú hơn nhưng nhỏ gọn hơn được xây dựng.

Tiếp theo, cho hai đầu vào tN, fN chúng tôi học biểu diễn đặc trưng kết hợp mới F(tN, fN) thông qua cross attention. Quan trọng, chúng tôi lưu ý rằng cả hai đầu vào này sẽ ảnh hưởng đến biểu diễn tiếp theo để tạo ra phiên bản đa phương thức của đặc trưng văn bản và hình ảnh. Trong phương pháp co-tokenization [19], hai phương thức cũng được kết hợp để học tập tốt hơn, nhưng ở đây có hai khác biệt chính: 1) việc giảm token ban đầu không được thực hiện ở mỗi lần lặp, điều này tốn kém về mặt tính toán; và 2) của chúng tôi sử dụng cross-attention nhẹ so với phương pháp co-tokenization.

Quá trình này sử dụng các thành phần sau. Chúng tôi đầu tiên sử dụng LayerNorm [3] (ký hiệu là Ln) để chuẩn hóa các đặc trưng. Sau đó chúng tôi tính cross-attention giữa tN (đặc trưng văn bản) và fN (đặc trưng hình ảnh). Ý tưởng là chúng sẽ giúp xây dựng biểu diễn là sự kết hợp của các phương thức này. Sau đó chúng tôi sử dụng lớp Transformer tiêu chuẩn với self-attention và MLPs để tính các đặc trưng.

Pcr(tN, fN) = Ln(tN) + tanh(α)Φ(Ln(tN), Ln(fN))
F(tN, fN) = Pcr(tN, fN) + tanh(β)MLP(Pcr(tN, fN))
(3)

trong đó α và β là các tham số có thể học kiểm soát cách các đặc trưng văn bản và thị giác được kết hợp (Φ là phép toán multi-head attention tiêu chuẩn). Chúng tôi lưu ý rằng ở đây, xuyên suốt, Pcross(tN, fN) ∈ RN×D, tức là một biểu diễn nhỏ gọn kết hợp hai phương thức. Chúng tôi cũng thêm cơ chế cổng tanh, mà chúng tôi thấy có lợi trong các thí nghiệm ablation. Biểu diễn kết quả F(tN, fN) ∈ RN×D sau đó được đưa vào transformer để tạo ra biểu diễn trung gian được biến đổi cùng chiều F = T(F(tN, fN)) ∈ RN×D. Chúng tôi sử dụng lớp transformer tiêu chuẩn (T) với multi-headed attention [25].

Biểu diễn đặc trưng mới này có thể được tinh chỉnh thêm để tạo ra học tập đa phương thức thậm chí tốt hơn bằng cách lặp lại cùng quá trình, nhưng lần này lấy đặc trưng đã có được làm đầu vào. Phép toán giống như Phương trình 3 nhưng với đầu vào được cập nhật liên tục bằng cách thay thế tN bằng F+tN, điều này thêm vào đầu ra của lớp Transformer trước đó. Điều này cho phép mô hình liên tục tinh chỉnh và kết hợp các đặc trưng. Giả sử Fi là biểu diễn hiện tại và Fi+1 là tiếp theo, điều này sử dụng các phương trình trước đó để cập nhật lặp lại.

--- TRANG 2 ---
Hình 2. Mở rộng FLOPs với kích thước hình ảnh, kích thước mô hình và độ sâu mô hình (tức là số lớp). Màu xanh (đường cong trên) là nối, màu đỏ là Biểu diễn Thích ứng Kết hợp của chúng tôi. Như thấy, phương pháp của chúng tôi mở rộng quy mô nhẹ nhàng hơn cho tất cả.

[THIS IS FIGURE/CHART: Three graphs showing FLOPs scaling]

GFs Data GQA SNLI VQA2
Mô hình Dữ liệu lớn
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890*1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130*404M - 80.86 77.68
BLIP-L [10] 250*129M - - 78.25
Mô hình Dữ liệu nhỏ
FLAVA [22] 70*70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122*14M - - 77.54
ALBEF [11] 165*14M - 80.14 74.54
12-in-1 [15] - - 60.5 - 71.3
UNITER [5] - 10M - 79.39 72.5
LXMERT [24] - 6.5M 60.0 - 69.9
Ours-Base 38.9 40M 81.9 82.1 79.20
Ours 54.5 40M 83.1 84.2 80.15

Bảng 1. Chúng tôi vượt trội hoặc hoạt động cạnh tranh với các mô hình hiện đại, mặc dù sử dụng rất ít GFLOPs(GFs) và lượng dữ liệu nhỏ. Thực tế với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình nhỏ của chúng tôi (350M tham số) vượt trội tất cả các phương pháp đã sử dụng ∼Tỷ ví dụ để tiền huấn luyện. Các mô hình như ALBEF và BLIP sử dụng dữ liệu nhỏ hơn nhưng có nhiều FLOPs hơn. Đánh giá từ vựng mở. *Tính toán FLOPs của chúng tôi.

GFLOPs GQA SNLI-VE
Perceiver [9] 40.3 78.2 77.4
CoTokenization [19] 43.8 78.5 77.5
Ours 38.9 79.1 77.9

Bảng 2. So sánh với phương pháp Perceiver [9] và phương pháp Iterative Co-tokenization [19] cho kết hợp hình ảnh+văn bản. Cả hai đều là triển khai của chúng tôi. Mô hình cơ sở.

GF GQA SNLI-VE
Concat (Baseline) 58.4 78.9 77.4
Ours (no Gating) 38.9 78.5 77.2
Ours 38.9 79.1 77.9

Bảng 3. So sánh với đường cơ sở nối: phương pháp của chúng tôi chính xác hơn và giảm FLOPs 1.5x. Điều này có ý nghĩa lớn hơn vì hầu hết các mô hình thị giác-ngôn ngữ đều dựa trên nối.

[Continues with equations and technical details...]

--- TRANG 3 ---
GF GQA SNLI-VE
134.2 78.3 77.1
235.5 78.8 77.6
438.9 79.1 77.9
842.5 79.2 77.6
(a)Số lần lặp được sử dụng để tính token.

GF GQA SNLI
16 18.5 76.5 75.8
32 28.4 78.3 76.8
64 38.9 79.1 77.9
128 72.9 79.2 78.1
(b)Số Token được sử dụng trong mô hình.

GF GQA SNLI
Spatial 42.5 78.9 77.4
Latent 38.9 79.1 77.9
(c)Phương pháp Lấy mẫu lại
Cross-attention tiềm ẩn tốt hơn.

GF GQA SNLI
None 38.9 78.1 76.5
Residual 38.9 78.7 77.6
Weighted 38.9 79.1 77.9
(d)Kết hợp lặp lại các đặc trưng sau mỗi lần lặp.

GF GQA SNLI
822.4 76.7 74.2
1630.5 78.3 75.4
3238.9 79.1 77.9
(e)Số Lớp được sử dụng trong mô-đun kết hợp.

Bảng 4. Nghiên cứu ablation khám phá các biến thể của phương pháp đề xuất.

[Content continues with detailed experimental results and comparisons...]

--- TRANG 4 ---
[References section with 33 numbered references in Vietnamese translation]

--- TRANG 5 ---
[Continuation of references 1-33 in Vietnamese]

--- TRANG 6 ---
[Final references 28-33 in Vietnamese]
