# 2305.19924.pdf
<<<<<<< Updated upstream
# ƒê∆∞·ª£c chuy·ªÉn ƒë·ªïi t·ª´ PDF sang TXT
=======
# ƒê√£ chuy·ªÉn ƒë·ªïi t·ª´ PDF sang TXT
>>>>>>> Stashed changes
# ƒê∆∞·ªùng d·∫´n ngu·ªìn: /home/admin88/arxiv-downloader/multimodal/2305.19924.pdf
# K√≠ch th∆∞·ªõc t·ªáp: 447365 bytes

===============================================
N·ªòI DUNG T·ªÜP PDF
===============================================

--- TRANG 1 ---
<<<<<<< Updated upstream
Bi·ªÉu di·ªÖn Th√≠ch ·ª©ng K·∫øt h·ª£p cho H·ªçc t·∫≠p H√¨nh ·∫£nh-Ng√¥n ng·ªØ
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia}@google.com
T√≥m t·∫Øt
C√°c m√¥ h√¨nh transformer h√¨nh ·∫£nh-ng√¥n ng·ªØ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c th√†nh c√¥ng to l·ªõn, nh∆∞ng ch√∫ng ƒëi k√®m v·ªõi chi ph√≠ t√≠nh to√°n cao. Ch√∫ng t√¥i ƒë·ªÅ xu·∫•t h·ªçc bi·ªÉu di·ªÖn h√¨nh ·∫£nh-ng√¥n ng·ªØ th√≠ch ·ª©ng k·∫øt h·ª£p, th√≠ch ·ª©ng v√† l·∫∑p ƒëi l·∫∑p l·∫°i k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng ƒëa ph∆∞∆°ng th·ª©c. ƒêi·ªÅu n√†y li√™n t·ª•c gi·∫£m chi ph√≠ v√† k√≠ch th∆∞·ªõc m√¥ h√¨nh, cho ph√©p m√¥ h√¨nh m·ªü r·ªông quy m√¥ m√† kh√¥ng tƒÉng ƒë√°ng k·ªÉ FLOPs ho·∫∑c b·ªô nh·ªõ, v√† v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh l·ªõn h∆°n v√† ƒë·∫Øt ƒë·ªè h∆°n nhi·ªÅu. Ch·ªâ v·ªõi 40M v√≠ d·ª• hu·∫•n luy·ªán v√† 39 GFLOPs, m√¥ h√¨nh c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh l·ªõn g·∫•p nhi·ªÅu l·∫ßn, m·ªôt s·ªë ƒë·∫°t 800 GFLOPs.

1. Gi·ªõi thi·ªáu
H·ªçc t·∫≠p th·ªã gi√°c-v√†-ng√¥n ng·ªØ ƒë√£ c√≥ nh·ªØng b∆∞·ªõc ti·∫øn l·ªõn g·∫ßn ƒë√¢y [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. C√°c m√¥ h√¨nh n√†y c√≥ th·ªÉ g√°n th√†nh c√¥ng c·ªßa ch√∫ng cho vi·ªác m·ªü r·ªông quy m√¥ c√°c m√¥ h√¨nh Transformer n·ªïi ti·∫øng [25], do ƒë√≥ c·∫ßn c√°c t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn. M·ªôt th√†nh ph·∫ßn quan tr·ªçng c·ªßa c√°c m√¥ h√¨nh n√†y l√† x√¢y d·ª±ng bi·ªÉu di·ªÖn th·ªã gi√°c-ng√¥n ng·ªØ k·∫øt h·ª£p c∆° b·∫£n n·∫Øm b·∫Øt m·ªëi quan h·ªá gi·ªØa c√°c ph∆∞∆°ng th·ª©c [5, 5, 7, 9‚Äì12, 12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. Tuy nhi√™n, c√°c c∆° ch·∫ø attention ƒë·∫Øt ƒë·ªè ƒë∆∞·ª£c √°p d·ª•ng trong Transformers, trong ƒë√≥ t√≠nh to√°n c·∫ßn thi·∫øt tƒÉng theo b·∫≠c hai v·ªõi s·ª± gia tƒÉng k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o; h∆°n n·ªØa, c√°c m√¥ h√¨nh n√†y ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi nhi·ªÅu d·ªØ li·ªáu h∆°n ƒë√°ng k·ªÉ [6] v√† c√°c b∆∞·ªõc hu·∫•n luy·ªán ƒë·ªÉ h·ªçc bi·ªÉu di·ªÖn k·∫øt h·ª£p; v√† cu·ªëi c√πng, v√¨ c√°c t·∫≠p d·ªØ li·ªáu l·ªõn kh√≥ thu th·∫≠p, c√°c t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª± ƒë·ªông ch·ª©a l∆∞·ª£ng l·ªõn nhi·ªÖu. T·∫•t c·∫£ ƒëi·ªÅu n√†y l√†m cho c√°c m√¥ h√¨nh n√†y th·∫≠m ch√≠ c√≤n k√©m hi·ªáu qu·∫£ v√† ƒë·∫Øt ƒë·ªè h∆°n ƒë·ªÉ hu·∫•n luy·ªán: m·ªü r·ªông quy m√¥ m√¥ h√¨nh, k·∫øt h·ª£p v·ªõi vi·ªác m·ªü r·ªông quy m√¥ d·ªØ li·ªáu t∆∞∆°ng ·ª©ng c·∫ßn thi·∫øt, v√† hu·∫•n luy·ªán v·ªõi l∆∞·ª£ng l·ªõn nhi·ªÖu, ƒë√≤i h·ªèi l∆∞·ª£ng l·ªõn t√≠nh to√°n. Do ƒë√≥, mong mu·ªën x√¢y d·ª±ng c√°c bi·ªÉu di·ªÖn th·ªã gi√°c-ng√¥n ng·ªØ hi·ªáu qu·∫£ h∆°n v·ªÅ b·ªô nh·ªõ, FLOPs v√† d·ªØ li·ªáu, n∆°i ng∆∞·ªùi ta c√≥ th·ªÉ t·∫≠n d·ª•ng quy m√¥ m√¥ h√¨nh nh∆∞ng theo c√°ch hi·ªáu qu·∫£ h∆°n.

ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t Bi·ªÉu di·ªÖn Th√≠ch ·ª©ng K·∫øt h·ª£p cho h·ªçc t·∫≠p h√¨nh ·∫£nh-ng√¥n ng·ªØ hi·ªáu qu·∫£ (H√¨nh 1). Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë·∫ßu ti√™n gi·∫£m s·ªë l∆∞·ª£ng token trong c√°c ph∆∞∆°ng th·ª©c ƒë·∫ßu v√†o, sau ƒë√≥ k·∫øt h·ª£p ch√∫ng m·ªôt c√°ch th√≠ch ·ª©ng. Qu√° tr√¨nh n√†y gi·∫£m ƒë√°ng k·ªÉ FLOPs, trong khi duy tr√¨ ho·∫∑c c·∫£i thi·ªán hi·ªáu su·∫•t. N√≥ t·∫°o ra bi·ªÉu di·ªÖn nh·ªè g·ªçn v√† hi·ªáu qu·∫£ h∆°n, gi·∫£m 33% FLOPs so v·ªõi ph√©p n·ªëi th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng, trong khi c·∫£i thi·ªán hi·ªáu su·∫•t. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn c√°c m√¥ h√¨nh hi·ªáu qu·∫£ h∆°n v·ªÅ d·ªØ li·ªáu v√† t√≠nh to√°n.

Ch√∫ng t√¥i ƒë√°nh gi√° ph∆∞∆°ng ph√°p tr√™n c√°c t√°c v·ª• Tr·∫£ l·ªùi C√¢u h·ªèi Th·ªã gi√°c (VQA), n∆°i hi·ªÉu bi·∫øt k·∫øt h·ª£p h√¨nh ·∫£nh trong b·ªëi c·∫£nh ƒë·∫ßu v√†o ng√¥n ng·ªØ l√† quan tr·ªçng. M√¥ h√¨nh c·ªßa ch√∫ng t√¥i ho·∫°t ƒë·ªông c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh hi·ªán ƒë·∫°i (SOTA), th·∫≠m ch√≠ v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh c√≥ quy m√¥ tham s·ªë v√† d·ªØ li·ªáu l·ªõn (H√¨nh 1). C√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√¢y, Perceiver [9], Co-Tokenization [19] c≈©ng ƒë·ªÅ xu·∫•t c√°c ph∆∞∆°ng ph√°p h·ªçc t·∫≠p th·ªã gi√°c-ng√¥n ng·ªØ k·∫øt h·ª£p hi·ªáu qu·∫£, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë·ªÅ xu·∫•t c∆° ch·∫ø 'c·∫≠p nh·∫≠t' th√¥ng tin gi·ªØa c√°c ph∆∞∆°ng th·ª©c v√† k·∫øt h·ª£p ƒë·∫∑c tr∆∞ng t·ªët h∆°n, v∆∞·ª£t qua hai ph∆∞∆°ng ph√°p n√†y c·∫£ v·ªÅ ƒë·ªô ch√≠nh x√°c v√† gi·∫£m FLOPs.

Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i cho ph√©p m·ªü r·ªông quy m√¥ m√¥ h√¨nh t·ªët h∆°n, s·ª≠ d·ª•ng √≠t FLOPs h∆°n nhi·ªÅu v·ªõi k√≠ch th∆∞·ªõc m√¥ h√¨nh v√† k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o tƒÉng (H√¨nh 2). ƒê√≥ng g√≥p ch√≠nh c·ªßa c√¥ng tr√¨nh ch√∫ng t√¥i l√† ph∆∞∆°ng ph√°p k·∫øt h·ª£p h√¨nh ·∫£nh-vƒÉn b·∫£n m·ªõi hi·ªáu qu·∫£ v√† ch√≠nh x√°c h∆°n c√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√¢y. ƒêi·ªÅu n√†y cho ph√©p ch√∫ng t√¥i tr√¨nh b√†y m·ªôt m√¥ h√¨nh h√¨nh ·∫£nh-ng√¥n ng·ªØ nh·ªè g·ªçn m·ªõi v·ªõi hi·ªáu su·∫•t xu·∫•t s·∫Øc, ƒë·∫°t ƒë∆∞·ª£c v·ªõi m·ªôt ph·∫ßn chi ph√≠ v√† d·ªØ li·ªáu.

2. Bi·ªÉu di·ªÖn Th√≠ch ·ª©ng K·∫øt h·ª£p
C√¢u h·ªèi ch√≠nh ch√∫ng t√¥i gi·∫£i quy·∫øt l√† l√†m th·∫ø n√†o ƒë·ªÉ k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng t·ª´ c√°c ph∆∞∆°ng th·ª©c ƒë·∫ßu v√†o th·ªã gi√°c v√† ng√¥n ng·ªØ. M·ªôt v√†i ph∆∞∆°ng ph√°p c∆° b·∫£n s·ª≠ d·ª•ng: (1) n·ªëi ho·∫∑c (2) cross-attention. M·ªôt v·∫•n ƒë·ªÅ ch√≠nh v·ªõi n·ªëi l√† n√≥ tƒÉng ƒë√°ng k·ªÉ s·ªë l∆∞·ª£ng token b·∫±ng c√°ch th√™m H*W v√†o ƒë·ªô d√†i vƒÉn b·∫£n (H, W l√† chi·ªÅu cao v√† chi·ªÅu r·ªông c·ªßa ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh). Do ƒë√≥, khi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh tƒÉng, vi·ªác n·ªëi tƒÉng ƒë√°ng k·ªÉ y√™u c·∫ßu FLOPs v√† b·ªô nh·ªõ c·ªßa m√¥ h√¨nh (H√¨nh 2), v√≠ d·ª• [5, 7, 12, 14, 15, 23, 29]. ·ªû ƒë√¢y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t ph∆∞∆°ng ph√°p gi·∫£m s·ªë l∆∞·ª£ng token, c·∫£i thi·ªán hi·ªáu qu·∫£. C√°c ph∆∞∆°ng ph√°p d·ª±a tr√™n cross-attention c√≥ c√°c v·∫•n ƒë·ªÅ kh√°c, ch·ªß y·∫øu l√† ph∆∞∆°ng th·ª©c ƒë∆∞·ª£c s·ª≠ d·ª•ng cho truy v·∫•n (th∆∞·ªùng l√† vƒÉn b·∫£n, v√≠ d·ª• ALBEF [11], BLIP [10]), x√°c ƒë·ªãnh k√≠ch th∆∞·ªõc bi·ªÉu di·ªÖn ƒë·∫ßu ra. Th∆∞·ªùng cho c√°c t√°c v·ª• th·ªã gi√°c-ng√¥n ng·ªØ, c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c c√≥ nhi·ªÅu token (v√≠ d·ª•, c√°c token th·ªã gi√°c l√† 14x14 = 196 cho k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o khi√™m t·ªën 224x224), trong khi vƒÉn b·∫£n kh√° ng·∫Øn, v√≠ d·ª• 10 token trong VQA2.0. Khi s·ª≠ d·ª•ng cross-attention, to√†n b·ªô ƒë·∫ßu v√†o th·ªã gi√°c ph·∫£i ƒë∆∞·ª£c n√©n v√†o c√°c bi·ªÉu di·ªÖn token vƒÉn b·∫£n √≠t n√†y, h·∫°n ch·∫ø ƒë√°ng k·ªÉ l∆∞·ª£ng th√¥ng tin th·ªã gi√°c c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng. Trong khi ph∆∞∆°ng ph√°p n√†y c√≥ √≠t FLOPs h∆°n vi·ªác n·ªëi, n√≥ m·∫•t th√¥ng tin, c√≥ th·ªÉ gi·∫£m hi·ªáu su·∫•t t√°c v·ª•, v√† t·∫°o s·ª± ph·ª• thu·ªôc v√†o ƒë·ªô d√†i vƒÉn b·∫£n ƒë·∫ßu v√†o. T·ª± nhi√™n, bi·ªÉu di·ªÖn ƒëa ph∆∞∆°ng th·ª©c n√†y s·∫Ω c√≥ √≠t ti·ªán √≠ch h∆°n khi tƒÉng k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o.

Thay v√†o ƒë√≥, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt m√¥-ƒëun cho ph√©p h·ªçc t·∫≠p ƒë·∫∑c tr∆∞ng th·ªã gi√°c-ng√¥n ng·ªØ t·ªët h∆°n b·∫±ng c√°ch k·∫øt h·ª£p th√¥ng tin th·ªã gi√°c hi·ªáu qu·∫£ h∆°n v√† k·∫øt h·ª£p n√≥ v·ªõi th√¥ng tin vƒÉn b·∫£n. B·∫±ng c√°ch tokenize th√≠ch ·ª©ng v√† l·∫∑p l·∫°i c√°c ƒë·∫ßu v√†o, m√¥ h√¨nh c√≥ th·ªÉ tinh ch·ªânh bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng h·ªçc t·ª´ c·∫£ hai ph∆∞∆°ng th·ª©c trong qu√° tr√¨nh hu·∫•n luy·ªán, trong khi gi·ªØ s·ªë l∆∞·ª£ng FLOPs h·ª£p l√Ω (H√¨nh 3).

Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i d·ª±a tr√™n m·ªôt s·ªë hi·ªÉu bi·∫øt. ƒê·∫ßu ti√™n, ch√∫ng t√¥i truy v·∫•n h√¨nh ·∫£nh ƒë·ªÉ c√≥ ƒë∆∞·ª£c c√°c token th·ªã gi√°c th√¥ng tin h∆°n. Tr∆∞·ªõc ƒë√¢y, ƒëi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng ph∆∞∆°ng ph√°p gi·ªëng TokenLearner [19, 21]. Tuy nhi√™n, ph∆∞∆°ng ph√°p n√†y, trong khi gi·∫£m FLOPs, ƒë√°ng ch√∫ √Ω cho c√°c ·ª©ng d·ª•ng video trong [19], v·∫´n s·ª≠ d·ª•ng kh√° nhi·ªÅu FLOPs ƒë·ªÉ t·∫°o ra v√† √°p d·ª•ng b·∫£n ƒë·ªì attention, v√† kh√¥ng m·ªü r·ªông quy m√¥ t·ªët v·ªõi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh. Thay v√†o ƒë√≥, ch√∫ng t√¥i s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p lai ƒë∆∞·ª£c truy·ªÅn c·∫£m h·ª©ng t·ª´ Perceiver [9]. Ch√∫ng t√¥i t·∫°o ra N token ƒë·ªôc l·∫≠p t·ª´ m·ªói ph∆∞∆°ng th·ª©c nh∆∞ b∆∞·ªõc ƒë·∫ßu ti√™n. Th·ª© hai, sau ƒë√≥ ch√∫ng t√¥i s·ª≠ d·ª•ng c∆° ch·∫ø cross-attention tr·ª±c ti·∫øp gi·ªØa c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n m·ªõi v√† th·ªã gi√°c nh·ªè g·ªçn ƒë·ªÉ t·∫°o ra bi·ªÉu di·ªÖn ƒëa ph∆∞∆°ng th·ª©c t·ªët h∆°n. C∆° ch·∫ø n√†y bao g·ªìm m·ªôt l·ªõp cross-attention, sau ƒë√≥ m·ªôt l·ªõp self-attention v√† m·ªôt Multi-Layer Perceptron (MLP), t∆∞∆°ng t·ª± nh∆∞ l·ªõp Transformer ti√™u chu·∫©n [25], nh∆∞ng do c√°c token gi·∫£m, nh·∫π h∆°n nhi·ªÅu.

Cu·ªëi c√πng, qu√° tr√¨nh n√†y ƒë∆∞·ª£c th·ª±c hi·ªán l·∫∑p l·∫°i, do ƒë√≥ tinh ch·ªânh bi·ªÉu di·ªÖn hi·ªán t·∫°i d·ª±a tr√™n t·∫≠p h·ª£p c√°c ƒë·∫∑c tr∆∞ng t·ª´ Transformer. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh ƒë·ªông c·∫≠p nh·∫≠t v√† ch·ªçn c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c v√† vƒÉn b·∫£n kh√°c nhau ·ªü m·ªói b∆∞·ªõc ƒë·ªÉ c√≥ th·ªÉ th·ª±c hi·ªán t√°c v·ª• t·ªët nh·∫•t, m√† kh√¥ng tƒÉng chi ph√≠ t√≠nh to√°n. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c m√¥ t·∫£ chi ti·∫øt d∆∞·ªõi ƒë√¢y.

G·ªçi Xtext v√† Xim l√† ƒë·∫ßu v√†o cho vƒÉn b·∫£n v√† h√¨nh ·∫£nh, t∆∞∆°ng ·ª©ng. C·ª• th·ªÉ h∆°n Xtext ‚àà RL√óD v√† Xim ‚àà RH√óW√óC, gi·∫£ s·ª≠ ƒë·∫ßu v√†o th·ªã gi√°c c√≥ k√≠ch th∆∞·ªõc W√óH, vƒÉn b·∫£n c√≥ ƒë·ªô d√†i L. M·ª•c ti√™u l√† t·∫°o ra bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng m·ªõi, th·∫•p chi·ªÅu h∆°n. ƒêi·ªÅu n√†y c√≥ th·ªÉ ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch gi·∫£m bi·ªÉu di·ªÖn xu·ªëng s·ªë l∆∞·ª£ng token th·∫•p h∆°n, ƒëi·ªÅu n√†y ƒë·∫∑c bi·ªát quan tr·ªçng cho c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c v√¨ ch√∫ng nhi·ªÅu h∆°n. ƒêi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch ƒë·∫ßu ti√™n th·ªëng nh·∫•t c√°c chi·ªÅu bi·ªÉu di·ªÖn, c·ª• th·ªÉ h∆°n l√† chi·∫øu c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c v√†o kh√¥ng gian H*W√óD, trong ƒë√≥ D l√† chi·ªÅu ƒë·∫∑c tr∆∞ng cho ƒë·∫ßu v√†o vƒÉn b·∫£n:

P(Xim) = W1Xim, (1)

trong ƒë√≥ P(Xim) ‚àà RH*W√óD. ·ªû ƒë√¢y, b·∫±ng W1 ch√∫ng t√¥i k√Ω hi·ªáu ph√©p to√°n c√≥ th·ªÉ h·ªçc, v√≠ d·ª• √°p d·ª•ng l·ªõp k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß, chi·∫øu ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh v√†o kh√¥ng gian D-chi·ªÅu. V·ªÅ nguy√™n t·∫Øc c·∫£ ƒë·∫ßu v√†o th·ªã gi√°c v√† ƒë·∫ßu v√†o vƒÉn b·∫£n c√≥ th·ªÉ ƒë∆∞·ª£c chi·∫øu v√†o m·ªôt chi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi v√≠ d·ª• D', do ƒë√≥ kh√¥ng nh·∫•t thi·∫øt ph·∫£i ph·ª• thu·ªôc v√†o chi·ªÅu ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o, tuy nhi√™n Ph∆∞∆°ng tr√¨nh 1 ƒë∆∞·ª£c s·ª≠ d·ª•ng ·ªü ƒë√¢y ƒë·ªÉ ƒë∆°n gi·∫£n. V·ªÅ nguy√™n t·∫Øc c·∫£ ƒë·∫ßu v√†o th·ªã gi√°c v√† ƒë·∫ßu v√†o vƒÉn b·∫£n c√≥ th·ªÉ ƒë∆∞·ª£c chi·∫øu v√†o m·ªôt chi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi v√≠ d·ª• D', do ƒë√≥ kh√¥ng nh·∫•t thi·∫øt ph·∫£i ph·ª• thu·ªôc v√†o chi·ªÅu ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o.

Nh∆∞ b∆∞·ªõc th·ª© hai, ch√∫ng t√¥i ti·∫øn h√†nh h·ªçc m·ªôt t·∫≠p h·ª£p N token c√≥ th·ªÉ h·ªçc m·ªõi XN ‚àà RN√óD, ƒë∆∞·ª£c th·ª±c hi·ªán theo ki·ªÉu h·ªçc ƒë·∫∑c tr∆∞ng DETR [4]. T·ª©c l√†, XN l√† bi·ªÉu di·ªÖn ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n ƒë∆∞·ª£c h·ªçc th√¥ng qua lan truy·ªÅn ng∆∞·ª£c c√πng v·ªõi c√°c tham s·ªë kh√°c ƒë·ªÉ gi·∫£m thi·ªÉu m·∫•t m√°t.

fN = W2Œ¶(XN, P(Xim)). (2)

·ªû ƒë√¢y P(Xim) ƒë·∫°i di·ªán cho chi·∫øu ƒë·∫∑c tr∆∞ng th·ªã gi√°c t·ª´ Ph∆∞∆°ng tr√¨nh 1, XN l√† c√°c ƒë·∫∑c tr∆∞ng ti·ªÅm ·∫©n ƒë√£ h·ªçc, Œ¶ l√† ph√©p to√°n multi-head attention ti√™u chu·∫©n. ƒêi·ªÅu n√†y t·∫°o ra fN, bi·ªÉu di·ªÖn trung gian nh·ªè g·ªçn v·ªõi N ƒë·∫∑c tr∆∞ng. ƒêi·ªÅu n√†y c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ h·ªçc N token m·ªõi, ƒë·∫°i di·ªán cho ƒë·∫ßu v√†o c·ªßa M token, trong ƒë√≥ N‚â™M, cho ƒë·∫ßu v√†o th·ªã gi√°c l·ªõn M=H*W. Ch√∫ng t√¥i l∆∞u √Ω r·∫±ng ƒëi·ªÅu n√†y t∆∞∆°ng t·ª± nh∆∞ ki·∫øn tr√∫c Perceiver [9], m·∫∑c d√π n√≥ ch·ªâ ƒë∆∞·ª£c th·ª±c hi·ªán m·ªôt l·∫ßn ·ªü ƒë√¢y.

Qu√° tr√¨nh n√†y c≈©ng ƒë∆∞·ª£c th·ª±c hi·ªán v·ªõi Xtext, t·∫°o ra N ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n (tN). Do ƒë√≥, kh√¥ng gi·ªëng nh∆∞ c√¥ng tr√¨nh tr∆∞·ªõc ƒë√¢y (v√≠ d·ª• [10,11]), N kh√¥ng c·∫ßn ph·∫£i g·∫Øn v·ªõi ƒë·ªô d√†i vƒÉn b·∫£n ƒë·∫ßu v√†o; v√¨ v·∫≠y m·ªôt bi·ªÉu di·ªÖn phong ph√∫ h∆°n nh∆∞ng nh·ªè g·ªçn h∆°n ƒë∆∞·ª£c x√¢y d·ª±ng.

Ti·∫øp theo, cho hai ƒë·∫ßu v√†o tN, fN ch√∫ng t√¥i h·ªçc bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng k·∫øt h·ª£p m·ªõi F(tN, fN) th√¥ng qua cross attention. Quan tr·ªçng, ch√∫ng t√¥i l∆∞u √Ω r·∫±ng c·∫£ hai ƒë·∫ßu v√†o n√†y s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn bi·ªÉu di·ªÖn ti·∫øp theo ƒë·ªÉ t·∫°o ra phi√™n b·∫£n ƒëa ph∆∞∆°ng th·ª©c c·ªßa ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† h√¨nh ·∫£nh. Trong ph∆∞∆°ng ph√°p co-tokenization [19], hai ph∆∞∆°ng th·ª©c c≈©ng ƒë∆∞·ª£c k·∫øt h·ª£p ƒë·ªÉ h·ªçc t·∫≠p t·ªët h∆°n, nh∆∞ng ·ªü ƒë√¢y c√≥ hai kh√°c bi·ªát ch√≠nh: 1) vi·ªác gi·∫£m token ban ƒë·∫ßu kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán ·ªü m·ªói l·∫ßn l·∫∑p, ƒëi·ªÅu n√†y t·ªën k√©m v·ªÅ m·∫∑t t√≠nh to√°n; v√† 2) c·ªßa ch√∫ng t√¥i s·ª≠ d·ª•ng cross-attention nh·∫π so v·ªõi ph∆∞∆°ng ph√°p co-tokenization.

Qu√° tr√¨nh n√†y s·ª≠ d·ª•ng c√°c th√†nh ph·∫ßn sau. Ch√∫ng t√¥i ƒë·∫ßu ti√™n s·ª≠ d·ª•ng LayerNorm [3] (k√Ω hi·ªáu l√† Ln) ƒë·ªÉ chu·∫©n h√≥a c√°c ƒë·∫∑c tr∆∞ng. Sau ƒë√≥ ch√∫ng t√¥i t√≠nh cross-attention gi·ªØa tN (ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n) v√† fN (ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh). √ù t∆∞·ªüng l√† ch√∫ng s·∫Ω gi√∫p x√¢y d·ª±ng bi·ªÉu di·ªÖn l√† s·ª± k·∫øt h·ª£p c·ªßa c√°c ph∆∞∆°ng th·ª©c n√†y. Sau ƒë√≥ ch√∫ng t√¥i s·ª≠ d·ª•ng l·ªõp Transformer ti√™u chu·∫©n v·ªõi self-attention v√† MLPs ƒë·ªÉ t√≠nh c√°c ƒë·∫∑c tr∆∞ng.

Pcr(tN, fN) = Ln(tN) + tanh(Œ±)Œ¶(Ln(tN), Ln(fN))
F(tN, fN) = Pcr(tN, fN) + tanh(Œ≤)MLP(Pcr(tN, fN))
(3)

trong ƒë√≥ Œ± v√† Œ≤ l√† c√°c tham s·ªë c√≥ th·ªÉ h·ªçc ki·ªÉm so√°t c√°ch c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† th·ªã gi√°c ƒë∆∞·ª£c k·∫øt h·ª£p (Œ¶ l√† ph√©p to√°n multi-head attention ti√™u chu·∫©n). Ch√∫ng t√¥i l∆∞u √Ω r·∫±ng ·ªü ƒë√¢y, xuy√™n su·ªët, Pcross(tN, fN) ‚àà RN√óD, t·ª©c l√† m·ªôt bi·ªÉu di·ªÖn nh·ªè g·ªçn k·∫øt h·ª£p hai ph∆∞∆°ng th·ª©c. Ch√∫ng t√¥i c≈©ng th√™m c∆° ch·∫ø c·ªïng tanh, m√† ch√∫ng t√¥i th·∫•y c√≥ l·ª£i trong c√°c th√≠ nghi·ªám ablation. Bi·ªÉu di·ªÖn k·∫øt qu·∫£ F(tN, fN) ‚àà RN√óD sau ƒë√≥ ƒë∆∞·ª£c ƒë∆∞a v√†o transformer ƒë·ªÉ t·∫°o ra bi·ªÉu di·ªÖn trung gian ƒë∆∞·ª£c bi·∫øn ƒë·ªïi c√πng chi·ªÅu F = T(F(tN, fN)) ‚àà RN√óD. Ch√∫ng t√¥i s·ª≠ d·ª•ng l·ªõp transformer ti√™u chu·∫©n (T) v·ªõi multi-headed attention [25].

Bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng m·ªõi n√†y c√≥ th·ªÉ ƒë∆∞·ª£c tinh ch·ªânh th√™m ƒë·ªÉ t·∫°o ra h·ªçc t·∫≠p ƒëa ph∆∞∆°ng th·ª©c th·∫≠m ch√≠ t·ªët h∆°n b·∫±ng c√°ch l·∫∑p l·∫°i c√πng qu√° tr√¨nh, nh∆∞ng l·∫ßn n√†y l·∫•y ƒë·∫∑c tr∆∞ng ƒë√£ c√≥ ƒë∆∞·ª£c l√†m ƒë·∫ßu v√†o. Ph√©p to√°n gi·ªëng nh∆∞ Ph∆∞∆°ng tr√¨nh 3 nh∆∞ng v·ªõi ƒë·∫ßu v√†o ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c b·∫±ng c√°ch thay th·∫ø tN b·∫±ng F+tN, ƒëi·ªÅu n√†y th√™m v√†o ƒë·∫ßu ra c·ªßa l·ªõp Transformer tr∆∞·ªõc ƒë√≥. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh li√™n t·ª•c tinh ch·ªânh v√† k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng. Gi·∫£ s·ª≠ Fi l√† bi·ªÉu di·ªÖn hi·ªán t·∫°i v√† Fi+1 l√† ti·∫øp theo, ƒëi·ªÅu n√†y s·ª≠ d·ª•ng c√°c ph∆∞∆°ng tr√¨nh tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c·∫≠p nh·∫≠t l·∫∑p l·∫°i.

--- TRANG 2 ---
H√¨nh 2. M·ªü r·ªông FLOPs v·ªõi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh, k√≠ch th∆∞·ªõc m√¥ h√¨nh v√† ƒë·ªô s√¢u m√¥ h√¨nh (t·ª©c l√† s·ªë l·ªõp). M√†u xanh (ƒë∆∞·ªùng cong tr√™n) l√† n·ªëi, m√†u ƒë·ªè l√† Bi·ªÉu di·ªÖn Th√≠ch ·ª©ng K·∫øt h·ª£p c·ªßa ch√∫ng t√¥i. Nh∆∞ th·∫•y, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i m·ªü r·ªông quy m√¥ nh·∫π nh√†ng h∆°n cho t·∫•t c·∫£.

[THIS IS FIGURE/CHART: Three graphs showing FLOPs scaling]

GFs Data GQA SNLI VQA2
M√¥ h√¨nh D·ªØ li·ªáu l·ªõn
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890*1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130*404M - 80.86 77.68
BLIP-L [10] 250*129M - - 78.25
M√¥ h√¨nh D·ªØ li·ªáu nh·ªè
FLAVA [22] 70*70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122*14M - - 77.54
ALBEF [11] 165*14M - 80.14 74.54
=======
H·ªçc bi·ªÉu di·ªÖn th√≠ch ·ª©ng k·∫øt h·ª£p cho h·ªçc h√¨nh ·∫£nh-ng√¥n ng·ªØ
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia }@google.com
T√≥m t·∫Øt
C√°c m√¥ h√¨nh transformer h√¨nh ·∫£nh-ng√¥n ng·ªØ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c th√†nh c√¥ng to l·ªõn, nh∆∞ng ch√∫ng ƒëi k√®m v·ªõi chi ph√≠ t√≠nh to√°n cao. Ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt ph∆∞∆°ng ph√°p h·ªçc bi·ªÉu di·ªÖn h√¨nh ·∫£nh-ng√¥n ng·ªØ th√≠ch ·ª©ng k·∫øt h·ª£p, nh·∫±m k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng ƒëa ph∆∞∆°ng th·ª©c m·ªôt c√°ch th√≠ch ·ª©ng v√† l·∫∑p l·∫°i. Ph∆∞∆°ng ph√°p n√†y li√™n t·ª•c gi·∫£m chi ph√≠ v√† k√≠ch th∆∞·ªõc m√¥ h√¨nh, cho ph√©p m√¥ h√¨nh m·ªü r·ªông quy m√¥ m√† kh√¥ng tƒÉng ƒë√°ng k·ªÉ FLOPs ho·∫∑c b·ªô nh·ªõ, v√† v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh l·ªõn h∆°n v√† ƒë·∫Øt ƒë·ªè h∆°n nhi·ªÅu. Ch·ªâ v·ªõi 40M v√≠ d·ª• hu·∫•n luy·ªán v√† 39 GFLOPs, m√¥ h√¨nh c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh l·ªõn h∆°n nhi·ªÅu l·∫ßn, m·ªôt s·ªë ƒë·∫°t t·ªõi 800 GFLOPs.

1. Gi·ªõi thi·ªáu
H·ªçc th·ªã gi√°c v√† ng√¥n ng·ªØ ƒë√£ c√≥ nh·ªØng b∆∞·ªõc ti·∫øn v∆∞·ª£t b·∫≠c g·∫ßn ƒë√¢y [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. Nh·ªØng m√¥ h√¨nh n√†y c√≥ th·ªÉ quy th√†nh c√¥ng c·ªßa m√¨nh cho vi·ªác m·ªü r·ªông quy m√¥ c√°c m√¥ h√¨nh Transformer n·ªïi ti·∫øng [25], m√† l·∫ßn l∆∞·ª£t c·∫ßn nh·ªØng b·ªô d·ªØ li·ªáu r·∫•t l·ªõn. M·ªôt th√†nh ph·∫ßn quan tr·ªçng c·ªßa nh·ªØng m√¥ h√¨nh n√†y l√† x√¢y d·ª±ng bi·ªÉu di·ªÖn th·ªã gi√°c-ng√¥n ng·ªØ k·∫øt h·ª£p c∆° b·∫£n, n·∫Øm b·∫Øt m·ªëi quan h·ªá gi·ªØa c√°c ph∆∞∆°ng th·ª©c [5, 5, 7, 9‚Äì12, 12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. Tuy nhi√™n, c√°c c∆° ch·∫ø attention ƒë·∫Øt ƒë·ªè ƒë∆∞·ª£c √°p d·ª•ng trong Transformers, trong ƒë√≥ t√≠nh to√°n c·∫ßn thi·∫øt tƒÉng theo b·∫≠c hai v·ªõi s·ª± gia tƒÉng k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o; h∆°n n·ªØa, nh·ªØng m√¥ h√¨nh n√†y ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi d·ªØ li·ªáu nhi·ªÅu h∆°n ƒë√°ng k·ªÉ [6] v√† c√°c b∆∞·ªõc hu·∫•n luy·ªán ƒë·ªÉ h·ªçc bi·ªÉu di·ªÖn k·∫øt h·ª£p; v√† cu·ªëi c√πng, v√¨ c√°c b·ªô d·ªØ li·ªáu l·ªõn kh√≥ thu th·∫≠p, c√°c b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c thu th·∫≠p t·ª± ƒë·ªông ch·ª©a l∆∞·ª£ng l·ªõn nhi·ªÖu. T·∫•t c·∫£ ƒëi·ªÅu n√†y l√†m cho nh·ªØng m√¥ h√¨nh n√†y c√†ng k√©m hi·ªáu qu·∫£ v√† ƒë·∫Øt ƒë·ªè h∆°n ƒë·ªÉ hu·∫•n luy·ªán: vi·ªác m·ªü r·ªông quy m√¥ m√¥ h√¨nh, k·∫øt h·ª£p v·ªõi vi·ªác m·ªü r·ªông quy m√¥ d·ªØ li·ªáu t∆∞∆°ng ·ª©ng c·∫ßn thi·∫øt, v√† hu·∫•n luy·ªán v·ªõi l∆∞·ª£ng l·ªõn nhi·ªÖu, ƒë√≤i h·ªèi l∆∞·ª£ng l·ªõn t√≠nh to√°n. Do ƒë√≥, vi·ªác x√¢y d·ª±ng c√°c bi·ªÉu di·ªÖn th·ªã gi√°c-ng√¥n ng·ªØ hi·ªáu qu·∫£ h∆°n v·ªÅ b·ªô nh·ªõ, FLOPs v√† d·ªØ li·ªáu, n∆°i c√≥ th·ªÉ t·∫≠n d·ª•ng quy m√¥ m√¥ h√¨nh nh∆∞ng theo c√°ch hi·ªáu qu·∫£ h∆°n l√† ƒëi·ªÅu mong mu·ªën.

ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c ƒëi·ªÅu ƒë√≥, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t Bi·ªÉu di·ªÖn th√≠ch ·ª©ng k·∫øt h·ª£p cho h·ªçc h√¨nh ·∫£nh-ng√¥n ng·ªØ hi·ªáu qu·∫£ (H√¨nh 1). Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i tr∆∞·ªõc ti√™n gi·∫£m s·ªë l∆∞·ª£ng token trong c√°c ph∆∞∆°ng th·ª©c ƒë·∫ßu v√†o, sau ƒë√≥ k·∫øt h·ª£p ch√∫ng m·ªôt c√°ch th√≠ch ·ª©ng. Qu√° tr√¨nh n√†y gi·∫£m ƒë√°ng k·ªÉ FLOPs, trong khi duy tr√¨ ho·∫∑c c·∫£i thi·ªán hi·ªáu su·∫•t. N√≥ d·∫´n ƒë·∫øn bi·ªÉu di·ªÖn g·ªçn g√†ng v√† hi·ªáu qu·∫£ h∆°n, ƒë·∫°t ƒë∆∞·ª£c √≠t h∆°n 33% FLOPs so v·ªõi ph∆∞∆°ng ph√°p n·ªëi th∆∞·ªùng d√πng, trong khi c·∫£i thi·ªán hi·ªáu su·∫•t. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn c√°c m√¥ h√¨nh hi·ªáu qu·∫£ h∆°n v·ªÅ d·ªØ li·ªáu v√† t√≠nh to√°n.

Ch√∫ng t√¥i ƒë√°nh gi√° ph∆∞∆°ng ph√°p tr√™n c√°c nhi·ªám v·ª• Tr·∫£ l·ªùi c√¢u h·ªèi th·ªã gi√°c (VQA), n∆°i vi·ªác hi·ªÉu k·∫øt h·ª£p h√¨nh ·∫£nh trong ng·ªØ c·∫£nh ƒë·∫ßu v√†o ng√¥n ng·ªØ l√† quan tr·ªçng. M√¥ h√¨nh c·ªßa ch√∫ng t√¥i ho·∫°t ƒë·ªông c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh ti√™n ti·∫øn (SOTA), th·∫≠m ch√≠ v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh c√≥ quy m√¥ tham s·ªë v√† d·ªØ li·ªáu l·ªõn (H√¨nh 1). C√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√¢y, Perceiver [9], Co-Tokenization [19] c≈©ng ƒë·ªÅ xu·∫•t c√°c ph∆∞∆°ng ph√°p h·ªçc th·ªã gi√°c-ng√¥n ng·ªØ k·∫øt h·ª£p hi·ªáu qu·∫£, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë·ªÅ xu·∫•t c∆° ch·∫ø 'c·∫≠p nh·∫≠t' th√¥ng tin gi·ªØa c√°c ph∆∞∆°ng th·ª©c v√† k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng c·ªßa ch√∫ng t·ªët h∆°n, v∆∞·ª£t tr·ªôi h∆°n hai ph∆∞∆°ng ph√°p n√†y c·∫£ v·ªÅ ƒë·ªô ch√≠nh x√°c v√† gi·∫£m FLOPs. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i cho ph√©p m·ªü r·ªông quy m√¥ m√¥ h√¨nh t·ªët h∆°n, s·ª≠ d·ª•ng √≠t FLOPs h∆°n nhi·ªÅu v·ªõi vi·ªác tƒÉng k√≠ch th∆∞·ªõc m√¥ h√¨nh v√† k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o (H√¨nh 2). ƒê√≥ng g√≥p ch√≠nh c·ªßa c√¥ng tr√¨nh ch√∫ng t√¥i l√† m·ªôt ph∆∞∆°ng ph√°p k·∫øt h·ª£p h√¨nh ·∫£nh-vƒÉn b·∫£n m·ªõi hi·ªáu qu·∫£ v√† ch√≠nh x√°c h∆°n c√°c ph∆∞∆°ng ph√°p tr∆∞·ªõc ƒë√¢y. ƒêi·ªÅu n√†y cho ph√©p ch√∫ng t√¥i tr√¨nh b√†y m·ªôt m√¥ h√¨nh h√¨nh ·∫£nh-ng√¥n ng·ªØ g·ªçn g√†ng m·ªõi v·ªõi hi·ªáu su·∫•t xu·∫•t s·∫Øc, ƒë·∫°t ƒë∆∞·ª£c v·ªõi m·ªôt ph·∫ßn chi ph√≠ v√† d·ªØ li·ªáu.

--- TRANG 2 ---
H√¨nh 2. M·ªü r·ªông quy m√¥ FLOPs theo k√≠ch th∆∞·ªõc h√¨nh ·∫£nh, k√≠ch th∆∞·ªõc m√¥ h√¨nh v√† ƒë·ªô s√¢u m√¥ h√¨nh (t·ª©c l√† s·ªë l·ªõp). M√†u xanh (ƒë∆∞·ªùng cong tr√™n) l√† n·ªëi, m√†u ƒë·ªè l√† Bi·ªÉu di·ªÖn th√≠ch ·ª©ng k·∫øt h·ª£p c·ªßa ch√∫ng t√¥i. Nh∆∞ th·∫•y, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i m·ªü r·ªông quy m√¥ m·ªôt c√°ch uy·ªÉn chuy·ªÉn h∆°n cho t·∫•t c·∫£ ch√∫ng.

2. Bi·ªÉu di·ªÖn th√≠ch ·ª©ng k·∫øt h·ª£p
C√¢u h·ªèi ch√≠nh ch√∫ng t√¥i gi·∫£i quy·∫øt l√† l√†m th·∫ø n√†o ƒë·ªÉ k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng t·ª´ c√°c ph∆∞∆°ng th·ª©c ƒë·∫ßu v√†o th·ªã gi√°c v√† ng√¥n ng·ªØ. M·ªôt v√†i ph∆∞∆°ng ph√°p c∆° b·∫£n s·ª≠ d·ª•ng: (1) n·ªëi ho·∫∑c (2) cross-attention. M·ªôt v·∫•n ƒë·ªÅ ch√≠nh v·ªõi vi·ªác n·ªëi l√† n√≥ tƒÉng ƒë√°ng k·ªÉ s·ªë l∆∞·ª£ng token b·∫±ng c√°ch th√™m H‚àóW v√†o ƒë·ªô d√†i vƒÉn b·∫£n (H,W l√† chi·ªÅu cao v√† chi·ªÅu r·ªông c·ªßa c√°c ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh). Do ƒë√≥, khi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh tƒÉng, vi·ªác n·ªëi tƒÉng ƒë√°ng k·ªÉ y√™u c·∫ßu FLOPs v√† b·ªô nh·ªõ c·ªßa m√¥ h√¨nh (H√¨nh 2), v√≠ d·ª•, [5, 7, 12, 14, 15, 23, 29]. ·ªû ƒë√¢y, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt ph∆∞∆°ng ph√°p ƒë·ªÉ gi·∫£m s·ªë l∆∞·ª£ng token, c·∫£i thi·ªán hi·ªáu qu·∫£. C√°c ph∆∞∆°ng ph√°p d·ª±a tr√™n Cross-attention c√≥ v·∫•n ƒë·ªÅ kh√°c, ch·ªß y·∫øu l√† ph∆∞∆°ng th·ª©c ƒë∆∞·ª£c s·ª≠ d·ª•ng cho truy v·∫•n (th∆∞·ªùng l√† vƒÉn b·∫£n, v√≠ d·ª•, ALBEF [11], BLIP [10]), x√°c ƒë·ªãnh k√≠ch th∆∞·ªõc c·ªßa bi·ªÉu di·ªÖn ƒë·∫ßu ra. Th∆∞·ªùng ƒë·ªëi v·ªõi c√°c nhi·ªám v·ª• th·ªã gi√°c-ng√¥n ng·ªØ, c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c c√≥ nhi·ªÅu token (v√≠ d·ª•, c√°c token th·ªã gi√°c l√† 14x14 = 196 cho k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o khi√™m t·ªën 224x224), trong khi vƒÉn b·∫£n kh√° ng·∫Øn, v√≠ d·ª•, 10 token trong VQA2.0. Khi s·ª≠ d·ª•ng cross-attention, to√†n b·ªô ƒë·∫ßu v√†o th·ªã gi√°c ph·∫£i ƒë∆∞·ª£c n√©n v√†o nh·ªØng bi·ªÉu di·ªÖn token vƒÉn b·∫£n √≠t n√†y, h·∫°n ch·∫ø ƒë√°ng k·ªÉ l∆∞·ª£ng th√¥ng tin th·ªã gi√°c c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng. Trong khi ph∆∞∆°ng ph√°p n√†y c√≥ √≠t FLOPs h∆°n vi·ªác n·ªëi, n√≥ m·∫•t th√¥ng tin, c√≥ th·ªÉ l√†m gi·∫£m hi·ªáu su·∫•t nhi·ªám v·ª•, v√† t·∫°o ra s·ª± ph·ª• thu·ªôc v√†o ƒë·ªô d√†i vƒÉn b·∫£n ƒë·∫ßu v√†o. T·ª± nhi√™n, bi·ªÉu di·ªÖn ƒëa ph∆∞∆°ng th·ª©c n√†y s·∫Ω c√≥ √≠t ti·ªán √≠ch h∆°n khi tƒÉng k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o.

Thay v√†o ƒë√≥, ch√∫ng t√¥i ƒë·ªÅ xu·∫•t m·ªôt m√¥-ƒëun cho ph√©p h·ªçc t·ªët h∆°n c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c-ng√¥n ng·ªØ b·∫±ng c√°ch k·∫øt h·ª£p th√¥ng tin th·ªã gi√°c hi·ªáu qu·∫£ h∆°n v√† k·∫øt h·ª£p v·ªõi th√¥ng tin vƒÉn b·∫£n. B·∫±ng c√°ch th√≠ch ·ª©ng v√† l·∫∑p l·∫°i tokenization c√°c ƒë·∫ßu v√†o, m√¥ h√¨nh c√≥ th·ªÉ tinh ch·ªânh bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng h·ªçc t·ª´ c·∫£ hai ph∆∞∆°ng th·ª©c trong qu√° tr√¨nh hu·∫•n luy·ªán, trong khi gi·ªØ s·ªë l∆∞·ª£ng FLOPs h·ª£p l√Ω (H√¨nh 3).

Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i d·ª±a tr√™n m·ªôt s·ªë hi·ªÉu bi·∫øt. ƒê·∫ßu ti√™n, ch√∫ng t√¥i truy v·∫•n h√¨nh ·∫£nh ƒë·ªÉ c√≥ ƒë∆∞·ª£c c√°c token th·ªã gi√°c th√¥ng tin h∆°n. Tr∆∞·ªõc ƒë√¢y, ƒëi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng ph∆∞∆°ng ph√°p gi·ªëng TokenLearner [19, 21]. Tuy nhi√™n, ph∆∞∆°ng ph√°p n√†y, trong khi gi·∫£m FLOPs, ƒë√°ng ch√∫ √Ω cho c√°c ·ª©ng d·ª•ng video trong [19], v·∫´n s·ª≠ d·ª•ng kh√° nhi·ªÅu FLOPs ƒë·ªÉ t·∫°o v√† √°p d·ª•ng c√°c b·∫£n ƒë·ªì attention, v√† kh√¥ng m·ªü r·ªông quy m√¥ t·ªët v·ªõi k√≠ch th∆∞·ªõc h√¨nh ·∫£nh. Thay v√†o ƒë√≥, ch√∫ng t√¥i s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p hybrid l·∫•y c·∫£m h·ª©ng t·ª´ Perceiver [9]. Ch√∫ng t√¥i t·∫°o N token ƒë·ªôc l·∫≠p t·ª´ m·ªói ph∆∞∆°ng th·ª©c nh∆∞ b∆∞·ªõc ƒë·∫ßu ti√™n. Th·ª© hai, sau ƒë√≥ ch√∫ng t√¥i s·ª≠ d·ª•ng c∆° ch·∫ø cross-attention tr·ª±c ti·∫øp gi·ªØa c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n m·ªõi v√† th·ªã gi√°c g·ªçn g√†ng ƒë·ªÉ t·∫°o ra bi·ªÉu di·ªÖn ƒëa ph∆∞∆°ng th·ª©c t·ªët h∆°n. C∆° ch·∫ø n√†y bao g·ªìm m·ªôt l·ªõp cross-attention, sau ƒë√≥ l√† m·ªôt l·ªõp self-attention

ƒê·∫∑c tr∆∞ng vƒÉn b·∫£n ƒê·∫∑c tr∆∞ng h√¨nh ·∫£nh Self-Attention MLP 
(T+H*W) x D 
ƒê·∫∑c tr∆∞ng vƒÉn b·∫£n ƒê·∫∑c tr∆∞ng h√¨nh ·∫£nh Cross-Attention MLP 
Self-Attention 
T x D 
ƒê·∫∑c tr∆∞ng vƒÉn b·∫£n ƒê·∫∑c tr∆∞ng h√¨nh ·∫£nh Cross-Attention MLP 
Self-Attention L
Cross-Attention 
Latent NxD Latent NxD tNfNCross-Attention ùù∞MLP ùõÉ
ƒê·∫∑c tr∆∞ng vƒÉn b·∫£n ƒê·∫∑c tr∆∞ng h√¨nh ·∫£nh Cross-Attention Cross-Attention 
Latent NxD Latent NxD tNfN(a) N·ªëi, t·∫°o ra ƒë·ªô d√†i chu·ªói d√†i 
L(b) Cross-attention, n√©n c√°c ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh v√†o c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n. 
(c) Cross attention ki·ªÉu Perceiver, ƒë∆∞·ª£c √°p d·ª•ng cho m·ªói ph∆∞∆°ng th·ª©c v√† l·∫∑p l·∫°i L l·∫ßn. ƒêi·ªÅu n√†y y√™u c·∫ßu t√≠nh to√°n cross attention tr√™n t·∫•t c·∫£ c√°c ƒë·∫ßu v√†o L l·∫ßn. (d) Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ch·ªâ s·ª≠ d·ª•ng cross-attention ƒë·∫ßu v√†o m·ªôt l·∫ßn v√† s·ª≠ d·ª•ng c·ªïng ƒë·ªÉ c·∫≠p nh·∫≠t c√°c ƒë·∫∑c tr∆∞ng. FiH√¨nh 3. Tr·ª±c quan h√≥a Bi·ªÉu di·ªÖn th√≠ch ·ª©ng k·∫øt h·ª£p (d) trong b·ªëi c·∫£nh c√°c ph∆∞∆°ng ph√°p kh√°c.

v√† m·ªôt Multi-Layer Perceptron (MLP), t∆∞∆°ng t·ª± nh∆∞ m·ªôt l·ªõp Transformer ti√™u chu·∫©n [25], nh∆∞ng do token gi·∫£m, nh·∫π h∆°n nhi·ªÅu.

Cu·ªëi c√πng, qu√° tr√¨nh n√†y ƒë∆∞·ª£c th·ª±c hi·ªán l·∫∑p l·∫°i, do ƒë√≥ tinh ch·ªânh bi·ªÉu di·ªÖn hi·ªán t·∫°i d·ª±a tr√™n t·∫≠p h·ª£p c√°c ƒë·∫∑c tr∆∞ng t·ª´ Transformer. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh c·∫≠p nh·∫≠t ƒë·ªông v√† ch·ªçn c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c v√† vƒÉn b·∫£n kh√°c nhau t·∫°i m·ªói b∆∞·ªõc ƒë·ªÉ c√≥ th·ªÉ th·ª±c hi·ªán t·ªët nh·∫•t nhi·ªám v·ª•, m√† kh√¥ng tƒÉng chi ph√≠ t√≠nh to√°n. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c m√¥ t·∫£ chi ti·∫øt d∆∞·ªõi ƒë√¢y.

G·ªçi Xtext v√† Xim l√† c√°c ƒë·∫ßu v√†o cho vƒÉn b·∫£n v√† h√¨nh ·∫£nh, t∆∞∆°ng ·ª©ng. C·ª• th·ªÉ h∆°n Xtext‚ààRL√óD v√† Xim‚ààRH√óW√óC, gi·∫£ s·ª≠ ƒë·∫ßu v√†o th·ªã gi√°c c√≥ k√≠ch th∆∞·ªõc W√óH, vƒÉn b·∫£n c√≥ ƒë·ªô d√†i L. M·ª•c ti√™u l√† t·∫°o ra bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng m·ªõi, chi·ªÅu th·∫•p h∆°n. ƒêi·ªÅu n√†y c√≥ th·ªÉ ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch gi·∫£m bi·ªÉu di·ªÖn xu·ªëng s·ªë l∆∞·ª£ng token th·∫•p h∆°n, ƒëi·ªÅu n√†y ƒë·∫∑c bi·ªát quan tr·ªçng ƒë·ªëi v·ªõi c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c v√¨ ch√∫ng nhi·ªÅu h∆°n. ƒêi·ªÅu n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·∫±ng c√°ch ƒë·∫ßu ti√™n th·ªëng nh·∫•t c√°c chi·ªÅu bi·ªÉu di·ªÖn, c·ª• th·ªÉ h∆°n l√† chi·∫øu c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c l√™n kh√¥ng gian H‚àóW√óD, trong ƒë√≥ D l√† chi·ªÅu ƒë·∫∑c tr∆∞ng cho ƒë·∫ßu v√†o vƒÉn b·∫£n:

P(Xim) =W1Xim, (1)

trong ƒë√≥ P(Xim)‚ààRH‚àóW√óD. ·ªû ƒë√¢y, b·∫±ng W1 ch√∫ng t√¥i k√Ω hi·ªáu ph√©p to√°n c√≥ th·ªÉ h·ªçc, v√≠ d·ª•, √°p d·ª•ng m·ªôt l·ªõp fully-connected, chi·∫øu c√°c ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh v√†o kh√¥ng gian D-chi·ªÅu. V·ªÅ nguy√™n t·∫Øc, c·∫£ ƒë·∫ßu v√†o th·ªã gi√°c v√† ƒë·∫ßu v√†o vƒÉn b·∫£n ƒë·ªÅu c√≥ th·ªÉ ƒë∆∞·ª£c chi·∫øu l√™n m·ªôt chi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi, v√≠ d·ª•, do ƒë√≥ kh√¥ng nh·∫•t thi·∫øt ph·∫£i ph·ª• thu·ªôc v√†o chi·ªÅu ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o, tuy nhi√™n Eq. 1 ƒë∆∞·ª£c s·ª≠ d·ª•ng ·ªü ƒë√¢y cho ƒë∆°n gi·∫£n. V·ªÅ nguy√™n t·∫Øc, c·∫£ ƒë·∫ßu v√†o th·ªã gi√°c v√† ƒë·∫ßu v√†o vƒÉn b·∫£n ƒë·ªÅu c√≥ th·ªÉ ƒë∆∞·ª£c chi·∫øu l√™n m·ªôt chi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi, v√≠ d·ª•, D‚Ä≤, do ƒë√≥ kh√¥ng nh·∫•t thi·∫øt ph·∫£i ph·ª• thu·ªôc v√†o chi·ªÅu ƒë·∫∑c tr∆∞ng ƒë·∫ßu v√†o.

Nh∆∞ b∆∞·ªõc th·ª© hai, ch√∫ng t√¥i ti·∫øn h√†nh h·ªçc m·ªôt t·∫≠p h·ª£p N token c√≥ th·ªÉ h·ªçc m·ªõi XN‚ààRN√óD, ƒë∆∞·ª£c th·ª±c hi·ªán theo phong c√°ch h·ªçc ƒë·∫∑c tr∆∞ng DETR [4]. T·ª©c l√†, XN l√† m·ªôt bi·ªÉu di·ªÖn ƒë∆∞·ª£c kh·ªüi t·∫°o ng·∫´u nhi√™n ƒë∆∞·ª£c h·ªçc qua lan truy·ªÅn ng∆∞·ª£c c√πng v·ªõi c√°c tham s·ªë kh√°c ƒë·ªÉ t·ªëi thi·ªÉu h√≥a m·∫•t m√°t.

fN=W2Œ¶(XN, P(Xim)). (2)

·ªû ƒë√¢y P(Xim) ƒë·∫°i di·ªán cho ph√©p chi·∫øu c√°c ƒë·∫∑c tr∆∞ng th·ªã gi√°c t·ª´ Eq. 1, XN l√† c√°c ƒë·∫∑c tr∆∞ng latent ƒë√£ h·ªçc, Œ¶ l√† ph√©p to√°n multi-head attention ti√™u chu·∫©n. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn fN, c√°c

--- TRANG 3 ---
bi·ªÉu di·ªÖn trung gian g·ªçn g√†ng v·ªõi N ƒë·∫∑c tr∆∞ng. ƒêi·ªÅu n√†y c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c xem nh∆∞ vi·ªác h·ªçc N token m·ªõi, ƒë·∫°i di·ªán cho ƒë·∫ßu v√†o c·ªßa M token, trong ƒë√≥ N‚â™M, cho ƒë·∫ßu v√†o th·ªã gi√°c l·ªõn M=H‚àóW. Ch√∫ng t√¥i l∆∞u √Ω r·∫±ng ƒëi·ªÅu n√†y t∆∞∆°ng t·ª± nh∆∞ ki·∫øn tr√∫c Perceiver [9], m·∫∑c d√π ch·ªâ ƒë∆∞·ª£c th·ª±c hi·ªán m·ªôt l·∫ßn ·ªü ƒë√¢y. Qu√° tr√¨nh n√†y c≈©ng ƒë∆∞·ª£c th·ª±c hi·ªán v·ªõi Xtext, d·∫´n ƒë·∫øn N ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n (tN). Do ƒë√≥, kh√°c v·ªõi c√¥ng tr√¨nh tr∆∞·ªõc ƒë√¢y (v√≠ d·ª•, [10,11]), N kh√¥ng b·∫Øt bu·ªôc ph·∫£i g·∫Øn v·ªõi ƒë·ªô d√†i vƒÉn b·∫£n ƒë·∫ßu v√†o; v√¨ v·∫≠y m·ªôt bi·ªÉu di·ªÖn phong ph√∫ h∆°n, nh∆∞ng g·ªçn g√†ng h∆°n ƒë∆∞·ª£c x√¢y d·ª±ng.

Ti·∫øp theo, cho hai ƒë·∫ßu v√†o tN,fN ch√∫ng t√¥i h·ªçc m·ªôt bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng k·∫øt h·ª£p m·ªõi F(tN, fN) th√¥ng qua cross attention. Quan tr·ªçng, ch√∫ng t√¥i l∆∞u √Ω r·∫±ng c·∫£ hai ƒë·∫ßu v√†o n√†y s·∫Ω ·∫£nh h∆∞·ªüng ƒë·∫øn bi·ªÉu di·ªÖn ti·∫øp theo ƒë·ªÉ t·∫°o ra phi√™n b·∫£n ƒëa ph∆∞∆°ng th·ª©c c·ªßa c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† h√¨nh ·∫£nh. Trong ph∆∞∆°ng ph√°p co-tokenization [19], hai ph∆∞∆°ng th·ª©c c≈©ng ƒë∆∞·ª£c k·∫øt h·ª£p ƒë·ªÉ h·ªçc t·ªët h∆°n, nh∆∞ng ·ªü ƒë√¢y c√≥ hai kh√°c bi·ªát ch√≠nh: 1) vi·ªác gi·∫£m token ban ƒë·∫ßu kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán t·∫°i m·ªói l·∫ßn l·∫∑p, ƒëi·ªÅu n√†y t·ªën k√©m v·ªÅ m·∫∑t t√≠nh to√°n; v√† 2) c·ªßa ch√∫ng t√¥i s·ª≠ d·ª•ng cross-attention nh·∫π so v·ªõi ph∆∞∆°ng ph√°p co-tokenization.

Qu√° tr√¨nh n√†y s·ª≠ d·ª•ng c√°c th√†nh ph·∫ßn sau. Ch√∫ng t√¥i ƒë·∫ßu ti√™n s·ª≠ d·ª•ng LayerNorm [3] (k√Ω hi·ªáu l√† Ln) ƒë·ªÉ chu·∫©n h√≥a c√°c ƒë·∫∑c tr∆∞ng. Sau ƒë√≥ ch√∫ng t√¥i t√≠nh to√°n cross-attention gi·ªØa tN (ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n) v√† fN (ƒë·∫∑c tr∆∞ng h√¨nh ·∫£nh). √ù t∆∞·ªüng l√† ch√∫ng s·∫Ω gi√∫p x√¢y d·ª±ng m·ªôt bi·ªÉu di·ªÖn l√† s·ª± k·∫øt h·ª£p c·ªßa nh·ªØng ph∆∞∆°ng th·ª©c n√†y. Sau ƒë√≥ ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt l·ªõp Transformer ti√™u chu·∫©n v·ªõi self-attention v√† MLPs ƒë·ªÉ t√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng.

Pcr(tN, fN) =Ln(tN) +tanh (Œ±)Œ¶(Ln(tN), Ln(fN))
F(tN, fN) =Pcr(tN, fN) +tanh (Œ≤)MLP (Pcr(tN, fN))
(3)

trong ƒë√≥ Œ± v√† Œ≤ l√† c√°c tham s·ªë c√≥ th·ªÉ h·ªçc ki·ªÉm so√°t c√°ch c√°c ƒë·∫∑c tr∆∞ng vƒÉn b·∫£n v√† th·ªã gi√°c ƒë∆∞·ª£c k·∫øt h·ª£p (Œ¶ l√† ph√©p to√°n multi-head attention ti√™u chu·∫©n). Ch√∫ng t√¥i l∆∞u √Ω r·∫±ng ·ªü ƒë√¢y, xuy√™n su·ªët, Pcross(tN, fN)‚ààRN√óD, t·ª©c l√†, l√† m·ªôt bi·ªÉu di·ªÖn g·ªçn g√†ng k·∫øt h·ª£p hai ph∆∞∆°ng th·ª©c. Ch√∫ng t√¥i c≈©ng th√™m c∆° ch·∫ø gating tanh, m√† ch√∫ng t√¥i th·∫•y c√≥ l·ª£i trong c√°c th·ª≠ nghi·ªám ablation. Bi·ªÉu di·ªÖn k·∫øt qu·∫£ F(tN, fN)‚ààRN√óD sau ƒë√≥ ƒë∆∞·ª£c ƒë∆∞a v√†o m·ªôt transformer ƒë·ªÉ t·∫°o ra bi·ªÉu di·ªÖn trung gian ƒë∆∞·ª£c bi·∫øn ƒë·ªïi c√πng chi·ªÅu F=T(F(tN, fN))‚ààRN√óD. Ch√∫ng t√¥i s·ª≠ d·ª•ng m·ªôt l·ªõp transformer ti√™u chu·∫©n (T) v·ªõi multi-headed attention [25].

Bi·ªÉu di·ªÖn ƒë·∫∑c tr∆∞ng m·ªõi n√†y c√≥ th·ªÉ ƒë∆∞·ª£c tinh ch·ªânh th√™m ƒë·ªÉ t·∫°o ra h·ªçc ƒëa ph∆∞∆°ng th·ª©c th·∫≠m ch√≠ t·ªët h∆°n b·∫±ng c√°ch l·∫∑p l·∫°i c√πng m·ªôt qu√° tr√¨nh, nh∆∞ng l·∫ßn n√†y l·∫•y ƒë·∫∑c tr∆∞ng ƒë√£ c√≥ ƒë∆∞·ª£c nh∆∞ ƒë·∫ßu v√†o. Ph√©p to√°n gi·ªëng nh∆∞ Eq. 3 nh∆∞ng v·ªõi ƒë·∫ßu v√†o ƒë∆∞·ª£c c·∫≠p nh·∫≠t li√™n t·ª•c b·∫±ng c√°ch thay th·∫ø tN b·∫±ng F+tN, th√™m v√†o ƒë·∫ßu ra c·ªßa l·ªõp Transformer tr∆∞·ªõc ƒë√≥. ƒêi·ªÅu n√†y cho ph√©p m√¥ h√¨nh li√™n t·ª•c tinh ch·ªânh v√† k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng. Gi·∫£ s·ª≠ Fi l√† bi·ªÉu di·ªÖn hi·ªán t·∫°i v√† Fi+1 l√† bi·ªÉu di·ªÖn ti·∫øp theo, ƒëi·ªÅu n√†y s·ª≠ d·ª•ng c√°c ph∆∞∆°ng tr√¨nh tr∆∞·ªõc ƒë√≥ ƒë·ªÉ c·∫≠p nh·∫≠t l·∫∑p l·∫°i

GFs D·ªØ li·ªáu GQA SNLI VQA2
M√¥ h√¨nh d·ªØ li·ªáu l·ªõn
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890‚àó1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130‚àó404M - 80.86 77.68
BLIP-L [10] 250‚àó129M - - 78.25
M√¥ h√¨nh d·ªØ li·ªáu nh·ªè
FLA V A [22] 70‚àó70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122‚àó14M - - 77.54
ALBEF [11] 165‚àó14M - 80.14 74.54
>>>>>>> Stashed changes
12-in-1 [15] - - 60.5 - 71.3
UNITER [5] - 10M - 79.39 72.5
LXMERT [24] - 6.5M 60.0 - 69.9
Ours-Base 38.9 40M 81.9 82.1 79.20
Ours 54.5 40M 83.1 84.2 80.15

<<<<<<< Updated upstream
B·∫£ng 1. Ch√∫ng t√¥i v∆∞·ª£t tr·ªôi ho·∫∑c ho·∫°t ƒë·ªông c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh hi·ªán ƒë·∫°i, m·∫∑c d√π s·ª≠ d·ª•ng r·∫•t √≠t GFLOPs(GFs) v√† l∆∞·ª£ng d·ªØ li·ªáu nh·ªè. Th·ª±c t·∫ø v·ªõi 40M v√≠ d·ª• hu·∫•n luy·ªán v√† 39 GFLOPs, m√¥ h√¨nh nh·ªè c·ªßa ch√∫ng t√¥i (350M tham s·ªë) v∆∞·ª£t tr·ªôi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë√£ s·ª≠ d·ª•ng ‚àºT·ª∑ v√≠ d·ª• ƒë·ªÉ ti·ªÅn hu·∫•n luy·ªán. C√°c m√¥ h√¨nh nh∆∞ ALBEF v√† BLIP s·ª≠ d·ª•ng d·ªØ li·ªáu nh·ªè h∆°n nh∆∞ng c√≥ nhi·ªÅu FLOPs h∆°n. ƒê√°nh gi√° t·ª´ v·ª±ng m·ªü. *T√≠nh to√°n FLOPs c·ªßa ch√∫ng t√¥i.
=======
B·∫£ng 1. Ch√∫ng t√¥i v∆∞·ª£t tr·ªôi ho·∫∑c ho·∫°t ƒë·ªông c·∫°nh tranh v·ªõi c√°c m√¥ h√¨nh ti√™n ti·∫øn, m·∫∑c d√π s·ª≠ d·ª•ng r·∫•t √≠t GFLOPs(GFs) v√† l∆∞·ª£ng d·ªØ li·ªáu nh·ªè. Th·ª±c t·∫ø v·ªõi 40M v√≠ d·ª• hu·∫•n luy·ªán v√† 39 GFLOPs, m√¥ h√¨nh nh·ªè c·ªßa ch√∫ng t√¥i (350M tham s·ªë) v∆∞·ª£t tr·ªôi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p ƒë√£ s·ª≠ d·ª•ng ‚àºT·ª∑ v√≠ d·ª• cho pre-training. C√°c m√¥ h√¨nh nh∆∞ ALBEF v√† BLIP s·ª≠ d·ª•ng d·ªØ li·ªáu nh·ªè h∆°n nh∆∞ng c√≥ nhi·ªÅu FLOPs h∆°n. ƒê√°nh gi√° t·ª´ v·ª±ng m·ªü. ‚àóT√≠nh to√°n FLOPs c·ªßa ch√∫ng t√¥i.
>>>>>>> Stashed changes

GFLOPs GQA SNLI-VE
Perceiver [9] 40.3 78.2 77.4
CoTokenization [19] 43.8 78.5 77.5
Ours 38.9 79.1 77.9

B·∫£ng 2. So s√°nh v·ªõi ph∆∞∆°ng ph√°p Perceiver [9] v√† ph∆∞∆°ng ph√°p Iterative Co-tokenization [19] cho k·∫øt h·ª£p h√¨nh ·∫£nh+vƒÉn b·∫£n. C·∫£ hai ƒë·ªÅu l√† tri·ªÉn khai c·ªßa ch√∫ng t√¥i. M√¥ h√¨nh c∆° s·ªü.

GF GQA SNLI-VE
Concat (Baseline) 58.4 78.9 77.4
Ours (no Gating) 38.9 78.5 77.2
Ours 38.9 79.1 77.9

<<<<<<< Updated upstream
B·∫£ng 3. So s√°nh v·ªõi ƒë∆∞·ªùng c∆° s·ªü n·ªëi: ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ch√≠nh x√°c h∆°n v√† gi·∫£m FLOPs 1.5x. ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a l·ªõn h∆°n v√¨ h·∫ßu h·∫øt c√°c m√¥ h√¨nh th·ªã gi√°c-ng√¥n ng·ªØ ƒë·ªÅu d·ª±a tr√™n n·ªëi.

[Continues with equations and technical details...]

--- TRANG 3 ---
=======
B·∫£ng 3. So s√°nh v·ªõi baseline n·ªëi: ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ch√≠nh x√°c h∆°n v√† gi·∫£m FLOPs 1.5x. ƒêi·ªÅu n√†y c√≥ √Ω nghƒ©a l·ªõn h∆°n v√¨ h·∫ßu h·∫øt c√°c m√¥ h√¨nh th·ªã gi√°c-ng√¥n ng·ªØ d·ª±a tr√™n n·ªëi.

c√°c ƒë·∫∑c tr∆∞ng nh∆∞ sau:

Pcr(Fi+tN, fN) =Ln(Fi+tN)+
tanh (Œ±)Œ¶(Ln(Fi+tN), Ln(fN))
Fi+1=Pcr(Fi+tN, fN)+
tanh (Œ≤)MLP (Pcross(Fi+tN, fN)
Fi+1=T(Fi+1)(4)

--- TRANG 4 ---
>>>>>>> Stashed changes
GF GQA SNLI-VE
134.2 78.3 77.1
235.5 78.8 77.6
438.9 79.1 77.9
842.5 79.2 77.6
<<<<<<< Updated upstream
(a)S·ªë l·∫ßn l·∫∑p ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√≠nh token.
=======
(a)S·ªë l·∫ßn l·∫∑p
ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√≠nh to√°n token.
>>>>>>> Stashed changes

GF GQA SNLI
16 18.5 76.5 75.8
32 28.4 78.3 76.8
64 38.9 79.1 77.9
128 72.9 79.2 78.1
<<<<<<< Updated upstream
(b)S·ªë Token ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m√¥ h√¨nh.
=======
(b)S·ªë Token
ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m√¥ h√¨nh.
>>>>>>> Stashed changes

GF GQA SNLI
Spatial 42.5 78.9 77.4
Latent 38.9 79.1 77.9
<<<<<<< Updated upstream
(c)Ph∆∞∆°ng ph√°p L·∫•y m·∫´u l·∫°i
Cross-attention ti·ªÅm ·∫©n t·ªët h∆°n.
=======
(c)Ph∆∞∆°ng ph√°p t√°i l·∫•y m·∫´u
Latent cross-attention t·ªët h∆°n.
>>>>>>> Stashed changes

GF GQA SNLI
None 38.9 78.1 76.5
Residual 38.9 78.7 77.6
Weighted 38.9 79.1 77.9
<<<<<<< Updated upstream
(d)K·∫øt h·ª£p l·∫∑p l·∫°i c√°c ƒë·∫∑c tr∆∞ng sau m·ªói l·∫ßn l·∫∑p.
=======
(d)K·∫øt h·ª£p l·∫∑p l·∫°i
c√°c ƒë·∫∑c tr∆∞ng sau m·ªói l·∫ßn l·∫∑p.
>>>>>>> Stashed changes

GF GQA SNLI
822.4 76.7 74.2
1630.5 78.3 75.4
3238.9 79.1 77.9
<<<<<<< Updated upstream
(e)S·ªë L·ªõp ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m√¥-ƒëun k·∫øt h·ª£p.

B·∫£ng 4. Nghi√™n c·ª©u ablation kh√°m ph√° c√°c bi·∫øn th·ªÉ c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t.

[Content continues with detailed experimental results and comparisons...]

--- TRANG 4 ---
[References section with 33 numbered references in Vietnamese translation]

--- TRANG 5 ---
[Continuation of references 1-33 in Vietnamese]

--- TRANG 6 ---
[Final references 28-33 in Vietnamese]
=======
(e)S·ªë l·ªõp ƒë∆∞·ª£c s·ª≠ d·ª•ng
trong m√¥-ƒëun k·∫øt h·ª£p.

B·∫£ng 4. Nghi√™n c·ª©u ablation kh√°m ph√° c√°c bi·∫øn th·ªÉ c·ªßa ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t.

VƒÉn b·∫£n ƒë∆∞·ª£c s·ª≠ d·ª•ng t·∫°i l·∫ßn l·∫∑p ƒë·∫ßu ti√™n, c√°c ƒë·∫∑c tr∆∞ng k·∫øt h·ª£p sau ƒë√≥.

ƒêi·ªÅu quan tr·ªçng l√† trong qu√° tr√¨nh h·ªçc ƒëa ph∆∞∆°ng th·ª©c, ch√∫ng t√¥i s·ª≠ d·ª•ng s·ª± t∆∞∆°ng t√°c c·ªßa c·∫£ hai ph∆∞∆°ng th·ª©c. C·ª• th·ªÉ, ch√∫ng t√¥i s·ª≠ d·ª•ng attention ƒë·ªÉ x√°c ƒë·ªãnh c√°c ph√©p chi·∫øu chi·ªÅu th·∫•p h∆°n t·ª´ c·∫£ hai ph∆∞∆°ng th·ª©c, kh√°c v·ªõi Transformer [25] b·∫£o t·ªìn k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o, v√† l√† m·ªôt qu√° tr√¨nh hi·ªáu qu·∫£ h∆°n so v·ªõi Iterative Co-Tokenization [19] v√† Perceiver [9], c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi Flamingo [2], v√¨ b∆∞·ªõc tokenization ƒë·∫Øt ƒë·ªè tr√™n to√†n b·ªô ƒë·∫ßu v√†o ch·ªâ ƒë∆∞·ª£c th·ª±c hi·ªán m·ªôt l·∫ßn ·ªü ƒë√¢y. H∆°n n·ªØa, kh√°c v·ªõi Flamingo l√† c√°c c·∫≠p nh·∫≠t l·∫∑p l·∫°i, Eq. 4, n∆°i ch√∫ng t√¥i l·∫∑p l·∫°i k·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng, thay v√¨ ch·ªâ d·ª±a v√†o cross-attention. Ph∆∞∆°ng ph√°p c≈©ng kh√°c v·ªõi c√°c ph∆∞∆°ng ph√°p nh∆∞ TokenLearner ch·ªâ ƒë∆∞·ª£c √°p d·ª•ng tr√™n m·ªôt ƒë·∫ßu v√†o duy nh·∫•t, c√≥ th·ªÉ d·∫´n ƒë·∫øn m·∫•t ƒë·ªô ch√≠nh x√°c n·∫øu kh√¥ng ƒë∆∞·ª£c ƒë·∫∑t ph√π h·ª£p [21]. N√≥ c≈©ng kh√°c v·ªõi c√°c ph∆∞∆°ng ph√°p cross-attention [7, 10, 11] do vi·ªác h·ªçc ƒë·∫∑c tr∆∞ng ban ƒë·∫ßu v√† c·∫≠p nh·∫≠t l·∫∑p l·∫°i th√¥ng tin ƒëa ph∆∞∆°ng th·ª©c (Eq. 2). Ph∆∞∆°ng ph√°p n√†y c≈©ng cung c·∫•p hi·ªáu su·∫•t t·ªët h∆°n so v·ªõi c√°c baseline n·ªëi trong khi s·ª≠ d·ª•ng √≠t nh·∫•t 33% √≠t FLOPs h∆°n.

Pre-training. Ch√∫ng t√¥i th·∫•y r·∫±ng h·ªón h·ª£p c·ªßa m·ªôt s·ªë nhi·ªám v·ª• ƒëa ph∆∞∆°ng th·ª©c [7, 11, 18] c√≥ l·ª£i h∆°n cho pre-training m√¥ h√¨nh th·ªã gi√°c-ng√¥n ng·ªØ c·ªßa ch√∫ng t√¥i. L·∫•y c·∫£m h·ª©ng t·ª´ curriculum learning, ch√∫ng t√¥i th√≠ch ·ª©ng thay ƒë·ªïi t·ª∑ l·ªá h·ªón h·ª£p gi·ªØa c√°c nhi·ªám v·ª• trong qu√° tr√¨nh pre-training (vui l√≤ng xem t√†i li·ªáu b·ªï sung cho danh s√°ch ƒë·∫ßy ƒë·ªß c√°c nhi·ªám v·ª• v√† m√¥ t·∫£ chi ti·∫øt).

3. Th√≠ nghi·ªám
Ch√∫ng t√¥i ƒë√°nh gi√° ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i tr√™n ba b·ªô d·ªØ li·ªáu VQA VQA2.0 [1], GQA [8], v√† Visual Entailment (SNLI-VE [30]) n∆°i ch√∫ng t√¥i tu√¢n theo c√°c ch·ªâ s·ªë ƒë·ªô ch√≠nh x√°c ti√™u chu·∫©n. M√¥ h√¨nh c·ªßa ch√∫ng t√¥i s·ª≠ d·ª•ng vƒÉn b·∫£n ƒë∆∞·ª£c t·∫°o ra m·ªü, m·ªôt t√¨nh hu·ªëng th√°ch th·ª©c h∆°n so v·ªõi nhi·ªÅu c√¥ng tr√¨nh tr∆∞·ªõc ƒë√¢y s·ª≠ d·ª•ng t·ª´ v·ª±ng c·ªë ƒë·ªãnh (3K) v√† thi·∫øt l·∫≠p ph√¢n lo·∫°i. B·∫£ng 1 cho th·∫•y so s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p ti√™n ti·∫øn (SOTA). Ch√∫ng t√¥i th·∫•y r·∫±ng ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i ho·∫°t ƒë·ªông c·∫°nh tranh ho·∫∑c v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh tr∆∞·ªõc ƒë√¢y. ƒê√°ng ch√∫ √Ω l√† c·∫£ m√¥ h√¨nh c∆° s·ªü v√† m√¥ h√¨nh l·ªõn h∆°n c·ªßa ch√∫ng t√¥i th·ª±c s·ª± l√† FLOPs th·∫•p nh·∫•t trong s·ªë c√°c m√¥ h√¨nh ƒë∆∞∆°ng ƒë·∫°i v√† v∆∞·ª£t tr·ªôi h∆°n c√°c m√¥ h√¨nh c√≥ nhi·ªÅu FLOPs h∆°n (M√¥ h√¨nh c·ªßa ch√∫ng t√¥i s·ª≠ d·ª•ng √≠t FLOPs h∆°n 2-20 l·∫ßn). M√¥ h√¨nh nh·ªè c·ªßa ch√∫ng t√¥i (300M tham s·ªë) v∆∞·ª£t tr·ªôi t·∫•t c·∫£ c√°c ph∆∞∆°ng ph√°p SOTA ngo·∫°i tr·ª´ c√°c m√¥ h√¨nh c·ª±c l·ªõn, Flamingo, SimVLM, c·∫£ hai ƒë·ªÅu pre-train tr√™n c√°c b·ªô d·ªØ li·ªáu r·∫•t l·ªõn. M√¥ h√¨nh ch√≠nh c·ªßa ch√∫ng t√¥i th√™m v√†o ƒë√≥ v∆∞·ª£t tr·ªôi SimVLM tr√™n VQA2.0. So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p ƒë∆∞∆°ng ƒë·∫°i v·ªÅ GFLOPs, ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i m·∫•t 38.9-54.5 GFLOPs, nh·ªè h∆°n nhi·ªÅu so v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c, v√≠ d·ª• ALBEF [11] kho·∫£ng 165, ho·∫∑c BLIP t·ª´ 120 ƒë·∫øn 250, v√† nh·ªè h∆°n nhi·ªÅu so v·ªõi SimVLM g·∫ßn 900 GFLOPs. Trong khi FLOPs l√† m·ªôt bi·ªán ph√°p kh√¥ng ho√†n h·∫£o, n√≥ ƒë∆∞·ª£c ∆∞a chu·ªông do s·ª± kh√°c bi·ªát trong tri·ªÉn khai v√† ph·∫ßn c·ª©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi c√°c ph∆∞∆°ng ph√°p kh√°c. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i gi·∫£m b·ªô nh·ªõ 40%, b·ªô nh·ªõ gi·∫£m t·ª´ 15GB c·ªßa baseline n·ªëi xu·ªëng 9GB cho ch√∫ng t√¥i.

So s√°nh h·ªçc h√¨nh ·∫£nh-ng√¥n ng·ªØ k·∫øt h·ª£p. Trong B·∫£ng 2, ch√∫ng t√¥i so s√°nh c·∫°nh nhau ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i v·ªõi c√°c ph∆∞∆°ng ph√°p h·ªçc bi·ªÉu di·ªÖn h√¨nh ·∫£nh-ng√¥n ng·ªØ hi·ªáu qu·∫£ kh√°c: Perceiver [9] v√† Iterative Co-Tokenization [19]. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i v∆∞·ª£t tr·ªôi h∆°n nh·ªØng ph∆∞∆°ng ph√°p k·∫øt h·ª£p ti√™n ti·∫øn n√†y, trong khi s·ª≠ d·ª•ng √≠t FLOPs h∆°n (B·∫£ng 2). N√≥ c≈©ng m·ªü r·ªông quy m√¥ t·ªët h∆°n nhi·ªÅu so v·ªõi ch√∫ng v·ªõi s·ª± gia tƒÉng k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o (H√¨nh 4).

H√¨nh 4. M·ªü r·ªông quy m√¥ v·ªõi c√°c k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o kh√°c nhau. V·ªõi c·∫≠p nh·∫≠t l·∫∑p l·∫°i c√≥ tr·ªçng s·ªë, ch√∫ng t√¥i m·ªü r·ªông quy m√¥ t·ªët h∆°n.

Nghi√™n c·ª©u ablation
Trong B·∫£ng 3, ch√∫ng t√¥i so s√°nh v·ªõi baseline n·ªëi, ƒë∆∞·ª£c s·ª≠ d·ª•ng ph·ªï bi·∫øn nh·∫•t [5,7,12,14,15,23,29]. Ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i c·∫£i thi·ªán hi·ªáu su·∫•t v√† gi·∫£m t√≠nh to√°n 33% t·ª©c l√† s·ª≠ d·ª•ng √≠t FLOPs h∆°n 1.5x. H√¨nh 2 th√™m v√†o ƒë√≥ cho th·∫•y ph∆∞∆°ng ph√°p c·ªßa ch√∫ng t√¥i c√≥ l·ª£i th·∫ø nhi·ªÅu h∆°n cho vi·ªác tƒÉng k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o, ho·∫∑c m·ªü r·ªông quy m√¥ m√¥ h√¨nh, v√† m·ªü r·ªông quy m√¥ t·ªët h∆°n so v·ªõi c√°c ph∆∞∆°ng ph√°p n·ªëi. Ch√∫ng t√¥i th·ª±c hi·ªán c√°c ablation chi ti·∫øt ƒë·ªÉ nghi√™n c·ª©u ph∆∞∆°ng ph√°p ƒë·ªÅ xu·∫•t. Cho m·ªói th√≠ nghi·ªám, ch√∫ng t√¥i s·ª≠a ƒë·ªïi m·ªôt th√†nh ph·∫ßn c·ªßa ph∆∞∆°ng ph√°p ch√≠nh ƒë·ªÉ x√°c minh t√°c ƒë·ªông ƒë·ªôc l·∫≠p c·ªßa n√≥ ('x√°m' l√† ph∆∞∆°ng ph√°p ch√≠nh). B·∫£ng 4 (a) v√† (b) cung c·∫•p ablation v·ªÅ c√°c b∆∞·ªõc l·∫∑p l·∫°i v√† s·ªë l∆∞·ª£ng token h·ªçc ƒë∆∞·ª£c, cho th·∫•y s·ª± ƒë√°nh ƒë·ªïi gi·ªØa t√≠nh to√°n nhi·ªÅu h∆°n vs ƒë·ªô ch√≠nh x√°c cao h∆°n, nh∆∞ng v·ªõi l·ª£i √≠ch gi·∫£m d·∫ßn. B·∫£ng 4 (c) minh h·ªça r·∫±ng m·ªôt single, latent cross-attention resampling c·ªßa ph∆∞∆°ng ph√°p ch√∫ng t√¥i ƒë·ªÅu cho hi·ªáu su·∫•t t·ªët h∆°n v√† s·ª≠ d·ª•ng √≠t FLOPs h∆°n. ƒêi·ªÅu n√†y tr√°i ng∆∞·ª£c v·ªõi spatial resampling ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c c√¥ng tr√¨nh tr∆∞·ªõc ƒë√¢y [19, 21]. B·∫£ng 4 (d), (e) nghi√™n c·ª©u weighting ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t (Eq. 4), v√† s·ªë l∆∞·ª£ng l·ªõp.

--- TRANG 5 ---
T√†i li·ªáu tham kh·∫£o
[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, v√† Devi Parikh. Vqa: Visual question answering. Trong ICCV, 2015. 4

[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, v√† Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022. 3, 4

[3] Lei Jimmy Ba, Jamie Ryan Kiros, v√† Geoffrey E. Hinton. Layer normalization. Trong CoRR abs/1607.06450, 2016. 3

[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, v√† Sergey Zagoruyko. End-to-end object detection with transformers. Trong ECCV, 2020. 2

[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, v√† Jingjing Liu. Uniter: Universal image-text representation learning. Trong ECCV, 2020. 1, 2, 3, 4

[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, v√† Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong 9th International Conference on Learning Representations, ICLR 2021, 2021. 1

[7] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-language transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 18166‚Äì18176, 2022. 1, 2, 3, 4

[8] Drew A Hudson v√† Christopher D Manning. Gqa: a new dataset for compositional question answering over realworld images. Trong CVPR, 2019. 4

[9] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, v√† Joao Carreira. Perceiver: General perception with iterative attention. Trong ICML, 2021. 1, 2, 3, 4

[10] Junnan Li, Dongxu Li, Caiming Xiong, v√† Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 1, 2, 3, 4

[11] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare, Shafiq Joty, Caiming Xiong, v√† Steven C.H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Trong NeurIPS, 2021. 1, 2, 3, 4

[12] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, v√† Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. 2019. 1, 2, 4

[13] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, v√† Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. Trong ECCV, 2020. 1

[14] Jiasen Lu, Dhruv Batra, Devi Parikh, v√† Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Trong CVPR, 2019. 1, 2, 4

[15] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, v√† Stefan Lee. 12-in-1: Multi-task vision and language representation learning. Trong CVPR, 2020. 2, 3, 4

[16] Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D, v√† Anh Nguyen Tran. Coarse-to-fine reasoning for visual question answering. Trong CVPR MULA Workshop, 2022. 3

[17] Duy-Kien Nguyen v√† Takayuki Okatani. Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. Trong CVPR, 2018. 1

[18] AJ Piergiovanni, Weicheng Kuo, v√† Anelia Angelova. Pre-training image-language transformers for open-vocabulary tasks. Trong T4V: Transformers for Vision Workshop, Conference on Computer Vision and Pattern Recognition, 2022. 4

[19] AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael Ryoo, v√† Anelia Angelova. Video question answering with iterative video-text co-tokenization. ECCV, 2022. 1, 2, 3, 4

[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. Trong International Conference on Machine Learning, trang 8748‚Äì8763. PMLR, 2021. 1

[21] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, v√† Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. 2021. 2, 4

[22] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami abd Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, v√† Douwe Kiela. Flava: A foundational language and vision alignment model. Trong arxiv.org/pdf/2112.04482.pdf, 2022. 1, 3

[23] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, v√† Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 2, 4

[24] Hao Tan v√† Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. Trong EMNLP, 2019. 1, 3

[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, v√† Illia Polosukhin. Attention is all you need. Trong NeurIPS, 2017. 1, 2, 3, 4

[26] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, v√† Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. 2022. 1

[27] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, v√† Lijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022. 1

--- TRANG 6 ---
[28] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, v√† Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 3

[29] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, v√† Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision, 2021. 1, 2, 3, 4

[30] Ning Xie, Farley Lai, Derek Doran, v√† Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. Trong https://arxiv.org/abs/1901.06706, 2019. 4

[31] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, v√† Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. ICLR, 2022. 1

[32] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, v√† Pengchuan Zhang. Florence: A new foundation model for computer vision. Trong arxiv.org/pdf/2110.02095.pdf, 2022. 1

[33] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, v√† Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 5579‚Äì5588, 2021. 1, 3
>>>>>>> Stashed changes
