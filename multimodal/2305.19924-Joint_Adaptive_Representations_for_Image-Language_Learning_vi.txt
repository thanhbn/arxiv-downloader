# 2305.19924.pdf
<<<<<<< Updated upstream
# Được chuyển đổi từ PDF sang TXT
=======
# Đã chuyển đổi từ PDF sang TXT
>>>>>>> Stashed changes
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2305.19924.pdf
# Kích thước tệp: 447365 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
<<<<<<< Updated upstream
Biểu diễn Thích ứng Kết hợp cho Học tập Hình ảnh-Ngôn ngữ
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia}@google.com
Tóm tắt
Các mô hình transformer hình ảnh-ngôn ngữ đã đạt được thành công to lớn, nhưng chúng đi kèm với chi phí tính toán cao. Chúng tôi đề xuất học biểu diễn hình ảnh-ngôn ngữ thích ứng kết hợp, thích ứng và lặp đi lặp lại kết hợp các đặc trưng đa phương thức. Điều này liên tục giảm chi phí và kích thước mô hình, cho phép mô hình mở rộng quy mô mà không tăng đáng kể FLOPs hoặc bộ nhớ, và vượt trội hơn các mô hình lớn hơn và đắt đỏ hơn nhiều. Chỉ với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình của chúng tôi vượt trội hơn các mô hình lớn gấp nhiều lần, một số đạt 800 GFLOPs.

1. Giới thiệu
Học tập thị giác-và-ngôn ngữ đã có những bước tiến lớn gần đây [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. Các mô hình này có thể gán thành công của chúng cho việc mở rộng quy mô các mô hình Transformer nổi tiếng [25], do đó cần các tập dữ liệu rất lớn. Một thành phần quan trọng của các mô hình này là xây dựng biểu diễn thị giác-ngôn ngữ kết hợp cơ bản nắm bắt mối quan hệ giữa các phương thức [5, 5, 7, 9–12, 12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. Tuy nhiên, các cơ chế attention đắt đỏ được áp dụng trong Transformers, trong đó tính toán cần thiết tăng theo bậc hai với sự gia tăng kích thước đầu vào; hơn nữa, các mô hình này hoạt động tốt hơn với nhiều dữ liệu hơn đáng kể [6] và các bước huấn luyện để học biểu diễn kết hợp; và cuối cùng, vì các tập dữ liệu lớn khó thu thập, các tập dữ liệu được thu thập tự động chứa lượng lớn nhiễu. Tất cả điều này làm cho các mô hình này thậm chí còn kém hiệu quả và đắt đỏ hơn để huấn luyện: mở rộng quy mô mô hình, kết hợp với việc mở rộng quy mô dữ liệu tương ứng cần thiết, và huấn luyện với lượng lớn nhiễu, đòi hỏi lượng lớn tính toán. Do đó, mong muốn xây dựng các biểu diễn thị giác-ngôn ngữ hiệu quả hơn về bộ nhớ, FLOPs và dữ liệu, nơi người ta có thể tận dụng quy mô mô hình nhưng theo cách hiệu quả hơn.

Để đạt được điều đó, chúng tôi đề xuất Biểu diễn Thích ứng Kết hợp cho học tập hình ảnh-ngôn ngữ hiệu quả (Hình 1). Phương pháp của chúng tôi đầu tiên giảm số lượng token trong các phương thức đầu vào, sau đó kết hợp chúng một cách thích ứng. Quá trình này giảm đáng kể FLOPs, trong khi duy trì hoặc cải thiện hiệu suất. Nó tạo ra biểu diễn nhỏ gọn và hiệu quả hơn, giảm 33% FLOPs so với phép nối thường được sử dụng, trong khi cải thiện hiệu suất. Điều này dẫn đến các mô hình hiệu quả hơn về dữ liệu và tính toán.

Chúng tôi đánh giá phương pháp trên các tác vụ Trả lời Câu hỏi Thị giác (VQA), nơi hiểu biết kết hợp hình ảnh trong bối cảnh đầu vào ngôn ngữ là quan trọng. Mô hình của chúng tôi hoạt động cạnh tranh với các mô hình hiện đại (SOTA), thậm chí vượt trội hơn các mô hình có quy mô tham số và dữ liệu lớn (Hình 1). Các phương pháp trước đây, Perceiver [9], Co-Tokenization [19] cũng đề xuất các phương pháp học tập thị giác-ngôn ngữ kết hợp hiệu quả, phương pháp của chúng tôi đề xuất cơ chế 'cập nhật' thông tin giữa các phương thức và kết hợp đặc trưng tốt hơn, vượt qua hai phương pháp này cả về độ chính xác và giảm FLOPs.

Phương pháp của chúng tôi cho phép mở rộng quy mô mô hình tốt hơn, sử dụng ít FLOPs hơn nhiều với kích thước mô hình và kích thước hình ảnh đầu vào tăng (Hình 2). Đóng góp chính của công trình chúng tôi là phương pháp kết hợp hình ảnh-văn bản mới hiệu quả và chính xác hơn các phương pháp trước đây. Điều này cho phép chúng tôi trình bày một mô hình hình ảnh-ngôn ngữ nhỏ gọn mới với hiệu suất xuất sắc, đạt được với một phần chi phí và dữ liệu.

2. Biểu diễn Thích ứng Kết hợp
Câu hỏi chính chúng tôi giải quyết là làm thế nào để kết hợp các đặc trưng từ các phương thức đầu vào thị giác và ngôn ngữ. Một vài phương pháp cơ bản sử dụng: (1) nối hoặc (2) cross-attention. Một vấn đề chính với nối là nó tăng đáng kể số lượng token bằng cách thêm H*W vào độ dài văn bản (H, W là chiều cao và chiều rộng của đặc trưng hình ảnh). Do đó, khi kích thước hình ảnh tăng, việc nối tăng đáng kể yêu cầu FLOPs và bộ nhớ của mô hình (Hình 2), ví dụ [5, 7, 12, 14, 15, 23, 29]. Ở đây, chúng tôi đề xuất phương pháp giảm số lượng token, cải thiện hiệu quả. Các phương pháp dựa trên cross-attention có các vấn đề khác, chủ yếu là phương thức được sử dụng cho truy vấn (thường là văn bản, ví dụ ALBEF [11], BLIP [10]), xác định kích thước biểu diễn đầu ra. Thường cho các tác vụ thị giác-ngôn ngữ, các đặc trưng thị giác có nhiều token (ví dụ, các token thị giác là 14x14 = 196 cho kích thước hình ảnh đầu vào khiêm tốn 224x224), trong khi văn bản khá ngắn, ví dụ 10 token trong VQA2.0. Khi sử dụng cross-attention, toàn bộ đầu vào thị giác phải được nén vào các biểu diễn token văn bản ít này, hạn chế đáng kể lượng thông tin thị giác có thể được sử dụng. Trong khi phương pháp này có ít FLOPs hơn việc nối, nó mất thông tin, có thể giảm hiệu suất tác vụ, và tạo sự phụ thuộc vào độ dài văn bản đầu vào. Tự nhiên, biểu diễn đa phương thức này sẽ có ít tiện ích hơn khi tăng kích thước hình ảnh đầu vào.

Thay vào đó, chúng tôi đề xuất một mô-đun cho phép học tập đặc trưng thị giác-ngôn ngữ tốt hơn bằng cách kết hợp thông tin thị giác hiệu quả hơn và kết hợp nó với thông tin văn bản. Bằng cách tokenize thích ứng và lặp lại các đầu vào, mô hình có thể tinh chỉnh biểu diễn đặc trưng học từ cả hai phương thức trong quá trình huấn luyện, trong khi giữ số lượng FLOPs hợp lý (Hình 3).

Phương pháp của chúng tôi dựa trên một số hiểu biết. Đầu tiên, chúng tôi truy vấn hình ảnh để có được các token thị giác thông tin hơn. Trước đây, điều này được thực hiện bằng phương pháp giống TokenLearner [19, 21]. Tuy nhiên, phương pháp này, trong khi giảm FLOPs, đáng chú ý cho các ứng dụng video trong [19], vẫn sử dụng khá nhiều FLOPs để tạo ra và áp dụng bản đồ attention, và không mở rộng quy mô tốt với kích thước hình ảnh. Thay vào đó, chúng tôi sử dụng phương pháp lai được truyền cảm hứng từ Perceiver [9]. Chúng tôi tạo ra N token độc lập từ mỗi phương thức như bước đầu tiên. Thứ hai, sau đó chúng tôi sử dụng cơ chế cross-attention trực tiếp giữa các đặc trưng văn bản mới và thị giác nhỏ gọn để tạo ra biểu diễn đa phương thức tốt hơn. Cơ chế này bao gồm một lớp cross-attention, sau đó một lớp self-attention và một Multi-Layer Perceptron (MLP), tương tự như lớp Transformer tiêu chuẩn [25], nhưng do các token giảm, nhẹ hơn nhiều.

Cuối cùng, quá trình này được thực hiện lặp lại, do đó tinh chỉnh biểu diễn hiện tại dựa trên tập hợp các đặc trưng từ Transformer. Điều này cho phép mô hình động cập nhật và chọn các đặc trưng thị giác và văn bản khác nhau ở mỗi bước để có thể thực hiện tác vụ tốt nhất, mà không tăng chi phí tính toán. Phương pháp của chúng tôi được mô tả chi tiết dưới đây.

Gọi Xtext và Xim là đầu vào cho văn bản và hình ảnh, tương ứng. Cụ thể hơn Xtext ∈ RL×D và Xim ∈ RH×W×C, giả sử đầu vào thị giác có kích thước W×H, văn bản có độ dài L. Mục tiêu là tạo ra biểu diễn đặc trưng mới, thấp chiều hơn. Điều này có thể được thực hiện bằng cách giảm biểu diễn xuống số lượng token thấp hơn, điều này đặc biệt quan trọng cho các đặc trưng thị giác vì chúng nhiều hơn. Điều này được thực hiện bằng cách đầu tiên thống nhất các chiều biểu diễn, cụ thể hơn là chiếu các đặc trưng thị giác vào không gian H*W×D, trong đó D là chiều đặc trưng cho đầu vào văn bản:

P(Xim) = W1Xim, (1)

trong đó P(Xim) ∈ RH*W×D. Ở đây, bằng W1 chúng tôi ký hiệu phép toán có thể học, ví dụ áp dụng lớp kết nối đầy đủ, chiếu đặc trưng hình ảnh vào không gian D-chiều. Về nguyên tắc cả đầu vào thị giác và đầu vào văn bản có thể được chiếu vào một chiều đặc trưng mới ví dụ D', do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào, tuy nhiên Phương trình 1 được sử dụng ở đây để đơn giản. Về nguyên tắc cả đầu vào thị giác và đầu vào văn bản có thể được chiếu vào một chiều đặc trưng mới ví dụ D', do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào.

Như bước thứ hai, chúng tôi tiến hành học một tập hợp N token có thể học mới XN ∈ RN×D, được thực hiện theo kiểu học đặc trưng DETR [4]. Tức là, XN là biểu diễn được khởi tạo ngẫu nhiên được học thông qua lan truyền ngược cùng với các tham số khác để giảm thiểu mất mát.

fN = W2Φ(XN, P(Xim)). (2)

Ở đây P(Xim) đại diện cho chiếu đặc trưng thị giác từ Phương trình 1, XN là các đặc trưng tiềm ẩn đã học, Φ là phép toán multi-head attention tiêu chuẩn. Điều này tạo ra fN, biểu diễn trung gian nhỏ gọn với N đặc trưng. Điều này cũng có thể được xem như học N token mới, đại diện cho đầu vào của M token, trong đó N≪M, cho đầu vào thị giác lớn M=H*W. Chúng tôi lưu ý rằng điều này tương tự như kiến trúc Perceiver [9], mặc dù nó chỉ được thực hiện một lần ở đây.

Quá trình này cũng được thực hiện với Xtext, tạo ra N đặc trưng văn bản (tN). Do đó, không giống như công trình trước đây (ví dụ [10,11]), N không cần phải gắn với độ dài văn bản đầu vào; vì vậy một biểu diễn phong phú hơn nhưng nhỏ gọn hơn được xây dựng.

Tiếp theo, cho hai đầu vào tN, fN chúng tôi học biểu diễn đặc trưng kết hợp mới F(tN, fN) thông qua cross attention. Quan trọng, chúng tôi lưu ý rằng cả hai đầu vào này sẽ ảnh hưởng đến biểu diễn tiếp theo để tạo ra phiên bản đa phương thức của đặc trưng văn bản và hình ảnh. Trong phương pháp co-tokenization [19], hai phương thức cũng được kết hợp để học tập tốt hơn, nhưng ở đây có hai khác biệt chính: 1) việc giảm token ban đầu không được thực hiện ở mỗi lần lặp, điều này tốn kém về mặt tính toán; và 2) của chúng tôi sử dụng cross-attention nhẹ so với phương pháp co-tokenization.

Quá trình này sử dụng các thành phần sau. Chúng tôi đầu tiên sử dụng LayerNorm [3] (ký hiệu là Ln) để chuẩn hóa các đặc trưng. Sau đó chúng tôi tính cross-attention giữa tN (đặc trưng văn bản) và fN (đặc trưng hình ảnh). Ý tưởng là chúng sẽ giúp xây dựng biểu diễn là sự kết hợp của các phương thức này. Sau đó chúng tôi sử dụng lớp Transformer tiêu chuẩn với self-attention và MLPs để tính các đặc trưng.

Pcr(tN, fN) = Ln(tN) + tanh(α)Φ(Ln(tN), Ln(fN))
F(tN, fN) = Pcr(tN, fN) + tanh(β)MLP(Pcr(tN, fN))
(3)

trong đó α và β là các tham số có thể học kiểm soát cách các đặc trưng văn bản và thị giác được kết hợp (Φ là phép toán multi-head attention tiêu chuẩn). Chúng tôi lưu ý rằng ở đây, xuyên suốt, Pcross(tN, fN) ∈ RN×D, tức là một biểu diễn nhỏ gọn kết hợp hai phương thức. Chúng tôi cũng thêm cơ chế cổng tanh, mà chúng tôi thấy có lợi trong các thí nghiệm ablation. Biểu diễn kết quả F(tN, fN) ∈ RN×D sau đó được đưa vào transformer để tạo ra biểu diễn trung gian được biến đổi cùng chiều F = T(F(tN, fN)) ∈ RN×D. Chúng tôi sử dụng lớp transformer tiêu chuẩn (T) với multi-headed attention [25].

Biểu diễn đặc trưng mới này có thể được tinh chỉnh thêm để tạo ra học tập đa phương thức thậm chí tốt hơn bằng cách lặp lại cùng quá trình, nhưng lần này lấy đặc trưng đã có được làm đầu vào. Phép toán giống như Phương trình 3 nhưng với đầu vào được cập nhật liên tục bằng cách thay thế tN bằng F+tN, điều này thêm vào đầu ra của lớp Transformer trước đó. Điều này cho phép mô hình liên tục tinh chỉnh và kết hợp các đặc trưng. Giả sử Fi là biểu diễn hiện tại và Fi+1 là tiếp theo, điều này sử dụng các phương trình trước đó để cập nhật lặp lại.

--- TRANG 2 ---
Hình 2. Mở rộng FLOPs với kích thước hình ảnh, kích thước mô hình và độ sâu mô hình (tức là số lớp). Màu xanh (đường cong trên) là nối, màu đỏ là Biểu diễn Thích ứng Kết hợp của chúng tôi. Như thấy, phương pháp của chúng tôi mở rộng quy mô nhẹ nhàng hơn cho tất cả.

[THIS IS FIGURE/CHART: Three graphs showing FLOPs scaling]

GFs Data GQA SNLI VQA2
Mô hình Dữ liệu lớn
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890*1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130*404M - 80.86 77.68
BLIP-L [10] 250*129M - - 78.25
Mô hình Dữ liệu nhỏ
FLAVA [22] 70*70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122*14M - - 77.54
ALBEF [11] 165*14M - 80.14 74.54
=======
Học biểu diễn thích ứng kết hợp cho học hình ảnh-ngôn ngữ
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia }@google.com
Tóm tắt
Các mô hình transformer hình ảnh-ngôn ngữ đã đạt được thành công to lớn, nhưng chúng đi kèm với chi phí tính toán cao. Chúng tôi đề xuất một phương pháp học biểu diễn hình ảnh-ngôn ngữ thích ứng kết hợp, nhằm kết hợp các đặc trưng đa phương thức một cách thích ứng và lặp lại. Phương pháp này liên tục giảm chi phí và kích thước mô hình, cho phép mô hình mở rộng quy mô mà không tăng đáng kể FLOPs hoặc bộ nhớ, và vượt trội hơn các mô hình lớn hơn và đắt đỏ hơn nhiều. Chỉ với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình của chúng tôi vượt trội hơn các mô hình lớn hơn nhiều lần, một số đạt tới 800 GFLOPs.

1. Giới thiệu
Học thị giác và ngôn ngữ đã có những bước tiến vượt bậc gần đây [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. Những mô hình này có thể quy thành công của mình cho việc mở rộng quy mô các mô hình Transformer nổi tiếng [25], mà lần lượt cần những bộ dữ liệu rất lớn. Một thành phần quan trọng của những mô hình này là xây dựng biểu diễn thị giác-ngôn ngữ kết hợp cơ bản, nắm bắt mối quan hệ giữa các phương thức [5, 5, 7, 9–12, 12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. Tuy nhiên, các cơ chế attention đắt đỏ được áp dụng trong Transformers, trong đó tính toán cần thiết tăng theo bậc hai với sự gia tăng kích thước đầu vào; hơn nữa, những mô hình này hoạt động tốt hơn với dữ liệu nhiều hơn đáng kể [6] và các bước huấn luyện để học biểu diễn kết hợp; và cuối cùng, vì các bộ dữ liệu lớn khó thu thập, các bộ dữ liệu được thu thập tự động chứa lượng lớn nhiễu. Tất cả điều này làm cho những mô hình này càng kém hiệu quả và đắt đỏ hơn để huấn luyện: việc mở rộng quy mô mô hình, kết hợp với việc mở rộng quy mô dữ liệu tương ứng cần thiết, và huấn luyện với lượng lớn nhiễu, đòi hỏi lượng lớn tính toán. Do đó, việc xây dựng các biểu diễn thị giác-ngôn ngữ hiệu quả hơn về bộ nhớ, FLOPs và dữ liệu, nơi có thể tận dụng quy mô mô hình nhưng theo cách hiệu quả hơn là điều mong muốn.

Để đạt được điều đó, chúng tôi đề xuất Biểu diễn thích ứng kết hợp cho học hình ảnh-ngôn ngữ hiệu quả (Hình 1). Phương pháp của chúng tôi trước tiên giảm số lượng token trong các phương thức đầu vào, sau đó kết hợp chúng một cách thích ứng. Quá trình này giảm đáng kể FLOPs, trong khi duy trì hoặc cải thiện hiệu suất. Nó dẫn đến biểu diễn gọn gàng và hiệu quả hơn, đạt được ít hơn 33% FLOPs so với phương pháp nối thường dùng, trong khi cải thiện hiệu suất. Điều này dẫn đến các mô hình hiệu quả hơn về dữ liệu và tính toán.

Chúng tôi đánh giá phương pháp trên các nhiệm vụ Trả lời câu hỏi thị giác (VQA), nơi việc hiểu kết hợp hình ảnh trong ngữ cảnh đầu vào ngôn ngữ là quan trọng. Mô hình của chúng tôi hoạt động cạnh tranh với các mô hình tiên tiến (SOTA), thậm chí vượt trội hơn các mô hình có quy mô tham số và dữ liệu lớn (Hình 1). Các phương pháp trước đây, Perceiver [9], Co-Tokenization [19] cũng đề xuất các phương pháp học thị giác-ngôn ngữ kết hợp hiệu quả, phương pháp của chúng tôi đề xuất cơ chế 'cập nhật' thông tin giữa các phương thức và kết hợp các đặc trưng của chúng tốt hơn, vượt trội hơn hai phương pháp này cả về độ chính xác và giảm FLOPs. Phương pháp của chúng tôi cho phép mở rộng quy mô mô hình tốt hơn, sử dụng ít FLOPs hơn nhiều với việc tăng kích thước mô hình và kích thước hình ảnh đầu vào (Hình 2). Đóng góp chính của công trình chúng tôi là một phương pháp kết hợp hình ảnh-văn bản mới hiệu quả và chính xác hơn các phương pháp trước đây. Điều này cho phép chúng tôi trình bày một mô hình hình ảnh-ngôn ngữ gọn gàng mới với hiệu suất xuất sắc, đạt được với một phần chi phí và dữ liệu.

--- TRANG 2 ---
Hình 2. Mở rộng quy mô FLOPs theo kích thước hình ảnh, kích thước mô hình và độ sâu mô hình (tức là số lớp). Màu xanh (đường cong trên) là nối, màu đỏ là Biểu diễn thích ứng kết hợp của chúng tôi. Như thấy, phương pháp của chúng tôi mở rộng quy mô một cách uyển chuyển hơn cho tất cả chúng.

2. Biểu diễn thích ứng kết hợp
Câu hỏi chính chúng tôi giải quyết là làm thế nào để kết hợp các đặc trưng từ các phương thức đầu vào thị giác và ngôn ngữ. Một vài phương pháp cơ bản sử dụng: (1) nối hoặc (2) cross-attention. Một vấn đề chính với việc nối là nó tăng đáng kể số lượng token bằng cách thêm H∗W vào độ dài văn bản (H,W là chiều cao và chiều rộng của các đặc trưng hình ảnh). Do đó, khi kích thước hình ảnh tăng, việc nối tăng đáng kể yêu cầu FLOPs và bộ nhớ của mô hình (Hình 2), ví dụ, [5, 7, 12, 14, 15, 23, 29]. Ở đây, chúng tôi đề xuất một phương pháp để giảm số lượng token, cải thiện hiệu quả. Các phương pháp dựa trên Cross-attention có vấn đề khác, chủ yếu là phương thức được sử dụng cho truy vấn (thường là văn bản, ví dụ, ALBEF [11], BLIP [10]), xác định kích thước của biểu diễn đầu ra. Thường đối với các nhiệm vụ thị giác-ngôn ngữ, các đặc trưng thị giác có nhiều token (ví dụ, các token thị giác là 14x14 = 196 cho kích thước hình ảnh đầu vào khiêm tốn 224x224), trong khi văn bản khá ngắn, ví dụ, 10 token trong VQA2.0. Khi sử dụng cross-attention, toàn bộ đầu vào thị giác phải được nén vào những biểu diễn token văn bản ít này, hạn chế đáng kể lượng thông tin thị giác có thể được sử dụng. Trong khi phương pháp này có ít FLOPs hơn việc nối, nó mất thông tin, có thể làm giảm hiệu suất nhiệm vụ, và tạo ra sự phụ thuộc vào độ dài văn bản đầu vào. Tự nhiên, biểu diễn đa phương thức này sẽ có ít tiện ích hơn khi tăng kích thước hình ảnh đầu vào.

Thay vào đó, chúng tôi đề xuất một mô-đun cho phép học tốt hơn các đặc trưng thị giác-ngôn ngữ bằng cách kết hợp thông tin thị giác hiệu quả hơn và kết hợp với thông tin văn bản. Bằng cách thích ứng và lặp lại tokenization các đầu vào, mô hình có thể tinh chỉnh biểu diễn đặc trưng học từ cả hai phương thức trong quá trình huấn luyện, trong khi giữ số lượng FLOPs hợp lý (Hình 3).

Phương pháp của chúng tôi dựa trên một số hiểu biết. Đầu tiên, chúng tôi truy vấn hình ảnh để có được các token thị giác thông tin hơn. Trước đây, điều này được thực hiện bằng phương pháp giống TokenLearner [19, 21]. Tuy nhiên, phương pháp này, trong khi giảm FLOPs, đáng chú ý cho các ứng dụng video trong [19], vẫn sử dụng khá nhiều FLOPs để tạo và áp dụng các bản đồ attention, và không mở rộng quy mô tốt với kích thước hình ảnh. Thay vào đó, chúng tôi sử dụng phương pháp hybrid lấy cảm hứng từ Perceiver [9]. Chúng tôi tạo N token độc lập từ mỗi phương thức như bước đầu tiên. Thứ hai, sau đó chúng tôi sử dụng cơ chế cross-attention trực tiếp giữa các đặc trưng văn bản mới và thị giác gọn gàng để tạo ra biểu diễn đa phương thức tốt hơn. Cơ chế này bao gồm một lớp cross-attention, sau đó là một lớp self-attention

Đặc trưng văn bản Đặc trưng hình ảnh Self-Attention MLP 
(T+H*W) x D 
Đặc trưng văn bản Đặc trưng hình ảnh Cross-Attention MLP 
Self-Attention 
T x D 
Đặc trưng văn bản Đặc trưng hình ảnh Cross-Attention MLP 
Self-Attention L
Cross-Attention 
Latent NxD Latent NxD tNfNCross-Attention 𝝰MLP 𝛃
Đặc trưng văn bản Đặc trưng hình ảnh Cross-Attention Cross-Attention 
Latent NxD Latent NxD tNfN(a) Nối, tạo ra độ dài chuỗi dài 
L(b) Cross-attention, nén các đặc trưng hình ảnh vào các đặc trưng văn bản. 
(c) Cross attention kiểu Perceiver, được áp dụng cho mỗi phương thức và lặp lại L lần. Điều này yêu cầu tính toán cross attention trên tất cả các đầu vào L lần. (d) Phương pháp của chúng tôi chỉ sử dụng cross-attention đầu vào một lần và sử dụng cổng để cập nhật các đặc trưng. FiHình 3. Trực quan hóa Biểu diễn thích ứng kết hợp (d) trong bối cảnh các phương pháp khác.

và một Multi-Layer Perceptron (MLP), tương tự như một lớp Transformer tiêu chuẩn [25], nhưng do token giảm, nhẹ hơn nhiều.

Cuối cùng, quá trình này được thực hiện lặp lại, do đó tinh chỉnh biểu diễn hiện tại dựa trên tập hợp các đặc trưng từ Transformer. Điều này cho phép mô hình cập nhật động và chọn các đặc trưng thị giác và văn bản khác nhau tại mỗi bước để có thể thực hiện tốt nhất nhiệm vụ, mà không tăng chi phí tính toán. Phương pháp của chúng tôi được mô tả chi tiết dưới đây.

Gọi Xtext và Xim là các đầu vào cho văn bản và hình ảnh, tương ứng. Cụ thể hơn Xtext∈RL×D và Xim∈RH×W×C, giả sử đầu vào thị giác có kích thước W×H, văn bản có độ dài L. Mục tiêu là tạo ra biểu diễn đặc trưng mới, chiều thấp hơn. Điều này có thể được thực hiện bằng cách giảm biểu diễn xuống số lượng token thấp hơn, điều này đặc biệt quan trọng đối với các đặc trưng thị giác vì chúng nhiều hơn. Điều này được thực hiện bằng cách đầu tiên thống nhất các chiều biểu diễn, cụ thể hơn là chiếu các đặc trưng thị giác lên không gian H∗W×D, trong đó D là chiều đặc trưng cho đầu vào văn bản:

P(Xim) =W1Xim, (1)

trong đó P(Xim)∈RH∗W×D. Ở đây, bằng W1 chúng tôi ký hiệu phép toán có thể học, ví dụ, áp dụng một lớp fully-connected, chiếu các đặc trưng hình ảnh vào không gian D-chiều. Về nguyên tắc, cả đầu vào thị giác và đầu vào văn bản đều có thể được chiếu lên một chiều đặc trưng mới, ví dụ, do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào, tuy nhiên Eq. 1 được sử dụng ở đây cho đơn giản. Về nguyên tắc, cả đầu vào thị giác và đầu vào văn bản đều có thể được chiếu lên một chiều đặc trưng mới, ví dụ, D′, do đó không nhất thiết phải phụ thuộc vào chiều đặc trưng đầu vào.

Như bước thứ hai, chúng tôi tiến hành học một tập hợp N token có thể học mới XN∈RN×D, được thực hiện theo phong cách học đặc trưng DETR [4]. Tức là, XN là một biểu diễn được khởi tạo ngẫu nhiên được học qua lan truyền ngược cùng với các tham số khác để tối thiểu hóa mất mát.

fN=W2Φ(XN, P(Xim)). (2)

Ở đây P(Xim) đại diện cho phép chiếu các đặc trưng thị giác từ Eq. 1, XN là các đặc trưng latent đã học, Φ là phép toán multi-head attention tiêu chuẩn. Điều này dẫn đến fN, các

--- TRANG 3 ---
biểu diễn trung gian gọn gàng với N đặc trưng. Điều này cũng có thể được xem như việc học N token mới, đại diện cho đầu vào của M token, trong đó N≪M, cho đầu vào thị giác lớn M=H∗W. Chúng tôi lưu ý rằng điều này tương tự như kiến trúc Perceiver [9], mặc dù chỉ được thực hiện một lần ở đây. Quá trình này cũng được thực hiện với Xtext, dẫn đến N đặc trưng văn bản (tN). Do đó, khác với công trình trước đây (ví dụ, [10,11]), N không bắt buộc phải gắn với độ dài văn bản đầu vào; vì vậy một biểu diễn phong phú hơn, nhưng gọn gàng hơn được xây dựng.

Tiếp theo, cho hai đầu vào tN,fN chúng tôi học một biểu diễn đặc trưng kết hợp mới F(tN, fN) thông qua cross attention. Quan trọng, chúng tôi lưu ý rằng cả hai đầu vào này sẽ ảnh hưởng đến biểu diễn tiếp theo để tạo ra phiên bản đa phương thức của các đặc trưng văn bản và hình ảnh. Trong phương pháp co-tokenization [19], hai phương thức cũng được kết hợp để học tốt hơn, nhưng ở đây có hai khác biệt chính: 1) việc giảm token ban đầu không được thực hiện tại mỗi lần lặp, điều này tốn kém về mặt tính toán; và 2) của chúng tôi sử dụng cross-attention nhẹ so với phương pháp co-tokenization.

Quá trình này sử dụng các thành phần sau. Chúng tôi đầu tiên sử dụng LayerNorm [3] (ký hiệu là Ln) để chuẩn hóa các đặc trưng. Sau đó chúng tôi tính toán cross-attention giữa tN (đặc trưng văn bản) và fN (đặc trưng hình ảnh). Ý tưởng là chúng sẽ giúp xây dựng một biểu diễn là sự kết hợp của những phương thức này. Sau đó chúng tôi sử dụng một lớp Transformer tiêu chuẩn với self-attention và MLPs để tính toán các đặc trưng.

Pcr(tN, fN) =Ln(tN) +tanh (α)Φ(Ln(tN), Ln(fN))
F(tN, fN) =Pcr(tN, fN) +tanh (β)MLP (Pcr(tN, fN))
(3)

trong đó α và β là các tham số có thể học kiểm soát cách các đặc trưng văn bản và thị giác được kết hợp (Φ là phép toán multi-head attention tiêu chuẩn). Chúng tôi lưu ý rằng ở đây, xuyên suốt, Pcross(tN, fN)∈RN×D, tức là, là một biểu diễn gọn gàng kết hợp hai phương thức. Chúng tôi cũng thêm cơ chế gating tanh, mà chúng tôi thấy có lợi trong các thử nghiệm ablation. Biểu diễn kết quả F(tN, fN)∈RN×D sau đó được đưa vào một transformer để tạo ra biểu diễn trung gian được biến đổi cùng chiều F=T(F(tN, fN))∈RN×D. Chúng tôi sử dụng một lớp transformer tiêu chuẩn (T) với multi-headed attention [25].

Biểu diễn đặc trưng mới này có thể được tinh chỉnh thêm để tạo ra học đa phương thức thậm chí tốt hơn bằng cách lặp lại cùng một quá trình, nhưng lần này lấy đặc trưng đã có được như đầu vào. Phép toán giống như Eq. 3 nhưng với đầu vào được cập nhật liên tục bằng cách thay thế tN bằng F+tN, thêm vào đầu ra của lớp Transformer trước đó. Điều này cho phép mô hình liên tục tinh chỉnh và kết hợp các đặc trưng. Giả sử Fi là biểu diễn hiện tại và Fi+1 là biểu diễn tiếp theo, điều này sử dụng các phương trình trước đó để cập nhật lặp lại

GFs Dữ liệu GQA SNLI VQA2
Mô hình dữ liệu lớn
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890∗1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130∗404M - 80.86 77.68
BLIP-L [10] 250∗129M - - 78.25
Mô hình dữ liệu nhỏ
FLA V A [22] 70∗70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122∗14M - - 77.54
ALBEF [11] 165∗14M - 80.14 74.54
>>>>>>> Stashed changes
12-in-1 [15] - - 60.5 - 71.3
UNITER [5] - 10M - 79.39 72.5
LXMERT [24] - 6.5M 60.0 - 69.9
Ours-Base 38.9 40M 81.9 82.1 79.20
Ours 54.5 40M 83.1 84.2 80.15

<<<<<<< Updated upstream
Bảng 1. Chúng tôi vượt trội hoặc hoạt động cạnh tranh với các mô hình hiện đại, mặc dù sử dụng rất ít GFLOPs(GFs) và lượng dữ liệu nhỏ. Thực tế với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình nhỏ của chúng tôi (350M tham số) vượt trội tất cả các phương pháp đã sử dụng ∼Tỷ ví dụ để tiền huấn luyện. Các mô hình như ALBEF và BLIP sử dụng dữ liệu nhỏ hơn nhưng có nhiều FLOPs hơn. Đánh giá từ vựng mở. *Tính toán FLOPs của chúng tôi.
=======
Bảng 1. Chúng tôi vượt trội hoặc hoạt động cạnh tranh với các mô hình tiên tiến, mặc dù sử dụng rất ít GFLOPs(GFs) và lượng dữ liệu nhỏ. Thực tế với 40M ví dụ huấn luyện và 39 GFLOPs, mô hình nhỏ của chúng tôi (350M tham số) vượt trội tất cả các phương pháp đã sử dụng ∼Tỷ ví dụ cho pre-training. Các mô hình như ALBEF và BLIP sử dụng dữ liệu nhỏ hơn nhưng có nhiều FLOPs hơn. Đánh giá từ vựng mở. ∗Tính toán FLOPs của chúng tôi.
>>>>>>> Stashed changes

GFLOPs GQA SNLI-VE
Perceiver [9] 40.3 78.2 77.4
CoTokenization [19] 43.8 78.5 77.5
Ours 38.9 79.1 77.9

Bảng 2. So sánh với phương pháp Perceiver [9] và phương pháp Iterative Co-tokenization [19] cho kết hợp hình ảnh+văn bản. Cả hai đều là triển khai của chúng tôi. Mô hình cơ sở.

GF GQA SNLI-VE
Concat (Baseline) 58.4 78.9 77.4
Ours (no Gating) 38.9 78.5 77.2
Ours 38.9 79.1 77.9

<<<<<<< Updated upstream
Bảng 3. So sánh với đường cơ sở nối: phương pháp của chúng tôi chính xác hơn và giảm FLOPs 1.5x. Điều này có ý nghĩa lớn hơn vì hầu hết các mô hình thị giác-ngôn ngữ đều dựa trên nối.

[Continues with equations and technical details...]

--- TRANG 3 ---
=======
Bảng 3. So sánh với baseline nối: phương pháp của chúng tôi chính xác hơn và giảm FLOPs 1.5x. Điều này có ý nghĩa lớn hơn vì hầu hết các mô hình thị giác-ngôn ngữ dựa trên nối.

các đặc trưng như sau:

Pcr(Fi+tN, fN) =Ln(Fi+tN)+
tanh (α)Φ(Ln(Fi+tN), Ln(fN))
Fi+1=Pcr(Fi+tN, fN)+
tanh (β)MLP (Pcross(Fi+tN, fN)
Fi+1=T(Fi+1)(4)

--- TRANG 4 ---
>>>>>>> Stashed changes
GF GQA SNLI-VE
134.2 78.3 77.1
235.5 78.8 77.6
438.9 79.1 77.9
842.5 79.2 77.6
<<<<<<< Updated upstream
(a)Số lần lặp được sử dụng để tính token.
=======
(a)Số lần lặp
được sử dụng để tính toán token.
>>>>>>> Stashed changes

GF GQA SNLI
16 18.5 76.5 75.8
32 28.4 78.3 76.8
64 38.9 79.1 77.9
128 72.9 79.2 78.1
<<<<<<< Updated upstream
(b)Số Token được sử dụng trong mô hình.
=======
(b)Số Token
được sử dụng trong mô hình.
>>>>>>> Stashed changes

GF GQA SNLI
Spatial 42.5 78.9 77.4
Latent 38.9 79.1 77.9
<<<<<<< Updated upstream
(c)Phương pháp Lấy mẫu lại
Cross-attention tiềm ẩn tốt hơn.
=======
(c)Phương pháp tái lấy mẫu
Latent cross-attention tốt hơn.
>>>>>>> Stashed changes

GF GQA SNLI
None 38.9 78.1 76.5
Residual 38.9 78.7 77.6
Weighted 38.9 79.1 77.9
<<<<<<< Updated upstream
(d)Kết hợp lặp lại các đặc trưng sau mỗi lần lặp.
=======
(d)Kết hợp lặp lại
các đặc trưng sau mỗi lần lặp.
>>>>>>> Stashed changes

GF GQA SNLI
822.4 76.7 74.2
1630.5 78.3 75.4
3238.9 79.1 77.9
<<<<<<< Updated upstream
(e)Số Lớp được sử dụng trong mô-đun kết hợp.

Bảng 4. Nghiên cứu ablation khám phá các biến thể của phương pháp đề xuất.

[Content continues with detailed experimental results and comparisons...]

--- TRANG 4 ---
[References section with 33 numbered references in Vietnamese translation]

--- TRANG 5 ---
[Continuation of references 1-33 in Vietnamese]

--- TRANG 6 ---
[Final references 28-33 in Vietnamese]
=======
(e)Số lớp được sử dụng
trong mô-đun kết hợp.

Bảng 4. Nghiên cứu ablation khám phá các biến thể của phương pháp đề xuất.

Văn bản được sử dụng tại lần lặp đầu tiên, các đặc trưng kết hợp sau đó.

Điều quan trọng là trong quá trình học đa phương thức, chúng tôi sử dụng sự tương tác của cả hai phương thức. Cụ thể, chúng tôi sử dụng attention để xác định các phép chiếu chiều thấp hơn từ cả hai phương thức, khác với Transformer [25] bảo tồn kích thước đầu vào, và là một quá trình hiệu quả hơn so với Iterative Co-Tokenization [19] và Perceiver [9], cũng được sử dụng bởi Flamingo [2], vì bước tokenization đắt đỏ trên toàn bộ đầu vào chỉ được thực hiện một lần ở đây. Hơn nữa, khác với Flamingo là các cập nhật lặp lại, Eq. 4, nơi chúng tôi lặp lại kết hợp các đặc trưng, thay vì chỉ dựa vào cross-attention. Phương pháp cũng khác với các phương pháp như TokenLearner chỉ được áp dụng trên một đầu vào duy nhất, có thể dẫn đến mất độ chính xác nếu không được đặt phù hợp [21]. Nó cũng khác với các phương pháp cross-attention [7, 10, 11] do việc học đặc trưng ban đầu và cập nhật lặp lại thông tin đa phương thức (Eq. 2). Phương pháp này cũng cung cấp hiệu suất tốt hơn so với các baseline nối trong khi sử dụng ít nhất 33% ít FLOPs hơn.

Pre-training. Chúng tôi thấy rằng hỗn hợp của một số nhiệm vụ đa phương thức [7, 11, 18] có lợi hơn cho pre-training mô hình thị giác-ngôn ngữ của chúng tôi. Lấy cảm hứng từ curriculum learning, chúng tôi thích ứng thay đổi tỷ lệ hỗn hợp giữa các nhiệm vụ trong quá trình pre-training (vui lòng xem tài liệu bổ sung cho danh sách đầy đủ các nhiệm vụ và mô tả chi tiết).

3. Thí nghiệm
Chúng tôi đánh giá phương pháp của chúng tôi trên ba bộ dữ liệu VQA VQA2.0 [1], GQA [8], và Visual Entailment (SNLI-VE [30]) nơi chúng tôi tuân theo các chỉ số độ chính xác tiêu chuẩn. Mô hình của chúng tôi sử dụng văn bản được tạo ra mở, một tình huống thách thức hơn so với nhiều công trình trước đây sử dụng từ vựng cố định (3K) và thiết lập phân loại. Bảng 1 cho thấy so sánh với các phương pháp tiên tiến (SOTA). Chúng tôi thấy rằng phương pháp của chúng tôi hoạt động cạnh tranh hoặc vượt trội hơn các mô hình trước đây. Đáng chú ý là cả mô hình cơ sở và mô hình lớn hơn của chúng tôi thực sự là FLOPs thấp nhất trong số các mô hình đương đại và vượt trội hơn các mô hình có nhiều FLOPs hơn (Mô hình của chúng tôi sử dụng ít FLOPs hơn 2-20 lần). Mô hình nhỏ của chúng tôi (300M tham số) vượt trội tất cả các phương pháp SOTA ngoại trừ các mô hình cực lớn, Flamingo, SimVLM, cả hai đều pre-train trên các bộ dữ liệu rất lớn. Mô hình chính của chúng tôi thêm vào đó vượt trội SimVLM trên VQA2.0. So sánh với các phương pháp đương đại về GFLOPs, phương pháp của chúng tôi mất 38.9-54.5 GFLOPs, nhỏ hơn nhiều so với các phương pháp khác, ví dụ ALBEF [11] khoảng 165, hoặc BLIP từ 120 đến 250, và nhỏ hơn nhiều so với SimVLM gần 900 GFLOPs. Trong khi FLOPs là một biện pháp không hoàn hảo, nó được ưa chuộng do sự khác biệt trong triển khai và phần cứng được sử dụng bởi các phương pháp khác. Phương pháp của chúng tôi giảm bộ nhớ 40%, bộ nhớ giảm từ 15GB của baseline nối xuống 9GB cho chúng tôi.

So sánh học hình ảnh-ngôn ngữ kết hợp. Trong Bảng 2, chúng tôi so sánh cạnh nhau phương pháp của chúng tôi với các phương pháp học biểu diễn hình ảnh-ngôn ngữ hiệu quả khác: Perceiver [9] và Iterative Co-Tokenization [19]. Phương pháp của chúng tôi vượt trội hơn những phương pháp kết hợp tiên tiến này, trong khi sử dụng ít FLOPs hơn (Bảng 2). Nó cũng mở rộng quy mô tốt hơn nhiều so với chúng với sự gia tăng kích thước hình ảnh đầu vào (Hình 4).

Hình 4. Mở rộng quy mô với các kích thước đầu vào khác nhau. Với cập nhật lặp lại có trọng số, chúng tôi mở rộng quy mô tốt hơn.

Nghiên cứu ablation
Trong Bảng 3, chúng tôi so sánh với baseline nối, được sử dụng phổ biến nhất [5,7,12,14,15,23,29]. Phương pháp của chúng tôi cải thiện hiệu suất và giảm tính toán 33% tức là sử dụng ít FLOPs hơn 1.5x. Hình 2 thêm vào đó cho thấy phương pháp của chúng tôi có lợi thế nhiều hơn cho việc tăng kích thước đầu vào, hoặc mở rộng quy mô mô hình, và mở rộng quy mô tốt hơn so với các phương pháp nối. Chúng tôi thực hiện các ablation chi tiết để nghiên cứu phương pháp đề xuất. Cho mỗi thí nghiệm, chúng tôi sửa đổi một thành phần của phương pháp chính để xác minh tác động độc lập của nó ('xám' là phương pháp chính). Bảng 4 (a) và (b) cung cấp ablation về các bước lặp lại và số lượng token học được, cho thấy sự đánh đổi giữa tính toán nhiều hơn vs độ chính xác cao hơn, nhưng với lợi ích giảm dần. Bảng 4 (c) minh họa rằng một single, latent cross-attention resampling của phương pháp chúng tôi đều cho hiệu suất tốt hơn và sử dụng ít FLOPs hơn. Điều này trái ngược với spatial resampling được sử dụng trong các công trình trước đây [19, 21]. Bảng 4 (d), (e) nghiên cứu weighting được đề xuất (Eq. 4), và số lượng lớp.

--- TRANG 5 ---
Tài liệu tham khảo
[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, và Devi Parikh. Vqa: Visual question answering. Trong ICCV, 2015. 4

[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, và Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022. 3, 4

[3] Lei Jimmy Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. Layer normalization. Trong CoRR abs/1607.06450, 2016. 3

[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong ECCV, 2020. 2

[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, và Jingjing Liu. Uniter: Universal image-text representation learning. Trong ECCV, 2020. 1, 2, 3, 4

[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong 9th International Conference on Learning Representations, ICLR 2021, 2021. 1

[7] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-language transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 18166–18176, 2022. 1, 2, 3, 4

[8] Drew A Hudson và Christopher D Manning. Gqa: a new dataset for compositional question answering over realworld images. Trong CVPR, 2019. 4

[9] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, và Joao Carreira. Perceiver: General perception with iterative attention. Trong ICML, 2021. 1, 2, 3, 4

[10] Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 1, 2, 3, 4

[11] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare, Shafiq Joty, Caiming Xiong, và Steven C.H. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Trong NeurIPS, 2021. 1, 2, 3, 4

[12] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, và Kai-Wei Chang. Visualbert: A simple and performant baseline for vision and language. 2019. 1, 2, 4

[13] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, và Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. Trong ECCV, 2020. 1

[14] Jiasen Lu, Dhruv Batra, Devi Parikh, và Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Trong CVPR, 2019. 1, 2, 4

[15] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, và Stefan Lee. 12-in-1: Multi-task vision and language representation learning. Trong CVPR, 2020. 2, 3, 4

[16] Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra, Quang D, và Anh Nguyen Tran. Coarse-to-fine reasoning for visual question answering. Trong CVPR MULA Workshop, 2022. 3

[17] Duy-Kien Nguyen và Takayuki Okatani. Improved fusion of visual and language representations by dense symmetric co-attention for visual question answering. Trong CVPR, 2018. 1

[18] AJ Piergiovanni, Weicheng Kuo, và Anelia Angelova. Pre-training image-language transformers for open-vocabulary tasks. Trong T4V: Transformers for Vision Workshop, Conference on Computer Vision and Pattern Recognition, 2022. 4

[19] AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael Ryoo, và Anelia Angelova. Video question answering with iterative video-text co-tokenization. ECCV, 2022. 1, 2, 3, 4

[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. Trong International Conference on Machine Learning, trang 8748–8763. PMLR, 2021. 1

[21] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, và Anelia Angelova. Tokenlearner: Adaptive space-time tokenization for videos. 2021. 2, 4

[22] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami abd Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, và Douwe Kiela. Flava: A foundational language and vision alignment model. Trong arxiv.org/pdf/2112.04482.pdf, 2022. 1, 3

[23] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, và Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 2, 4

[24] Hao Tan và Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. Trong EMNLP, 2019. 1, 3

[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong NeurIPS, 2017. 1, 2, 3, 4

[26] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, và Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. 2022. 1

[27] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, và Lijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022. 1

--- TRANG 6 ---
[28] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, và Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 3

[29] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, và Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision, 2021. 1, 2, 3, 4

[30] Ning Xie, Farley Lai, Derek Doran, và Asim Kadav. Visual entailment: A novel task for fine-grained image understanding. Trong https://arxiv.org/abs/1901.06706, 2019. 4

[31] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, và Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. ICLR, 2022. 1

[32] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, và Pengchuan Zhang. Florence: A new foundation model for computer vision. Trong arxiv.org/pdf/2110.02095.pdf, 2022. 1

[33] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, và Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 5579–5588, 2021. 1, 3
>>>>>>> Stashed changes
