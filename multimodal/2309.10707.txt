# 2309.10707.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.10707.pdf
# File size: 424415 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CORPUS SYNTHESIS FOR ZERO-SHOT ASR DOMAIN ADAPTATION USING
LARGE LANGUAGE MODELS
Hsuan Su♡♢∗Ting-Yao Hu♢Hema Swetha Koppula♢Raviteja Vemulapalli♢
Jen-Hao Rick Chang♢Karren Yang♢Gautam Varma Mantena♢Oncel Tuzel♢
♡National Taiwan University♢Apple
ABSTRACT
While Automatic Speech Recognition (ASR) systems are
widely used in many real-world applications, they often do
not generalize well to new domains and need to be finetuned
on data from these domains. However, target-domain data
usually are not readily available in many scenarios. In this
paper, we propose a new strategy for adapting ASR mod-
els to new target domains without any text or speech from
those domains. To accomplish this, we propose a novel
data synthesis pipeline that uses a Large Language Model
(LLM) to generate a target domain text corpus, and a state-
of-the-art controllable speech synthesis model to generate
the corresponding speech. We propose a simple yet effec-
tive in-context instruction finetuning strategy to increase the
effectiveness of LLM in generating text corpora for new do-
mains. Experiments on the SLURP dataset show that the
proposed method achieves an average relative word error rate
improvement of 28% on unseen target domains without any
performance drop in source domains.
Index Terms —automatic speech recognition, large lan-
guage models, controllable speech synthesis, zero-shot ASR
adaptation
1. INTRODUCTION
Adapting an End-to-End (E2E) Automatic Speech Recogni-
tion (ASR) system to new target domains is a challenging
task due to the limited availability of paired speech-text data.
Recently, text-only adaptation methods [1, 2, 3, 4, 5, 6] have
been developed to address the data scarcity problem. Some of
these works [5, 6] use a Controllable Speech Synthesis (CSS)
model to generate speech for the target domain text corpus,
and create a paired dataset with real text and synthetic speech
for ASR model adaptation. However, in many scenarios, col-
lecting a target domain text corpus may be costly, time con-
suming, or even infeasible due to privacy concerns.
Large Language Models (LLMs) have recently been
shown to work extremely well on numerous natural language
processing tasks, especially in few/zero-shot settings. This
motivates us to leverage LLMs for adapting ASR models to
∗Work done when interning at Apple.
LLMControllable
Speech Synthesis
 Synthetic
Speech
Synthetic
Text
Data Synthesis Pipeline
Synthetic
Speech
Source Domain
Real Speech
ASR
ASR AdaptationFig. 1 .Data Synthesis Pipeline. The pipeline consists of
a LLM and a CSS model. We use a LLM to generate text
corpus, and then synthesize speech data using a CSS model.
ASR Adaptation - The synthetic target domain data is used
to finetune a pretrained ASR model along with real speech
from source domains. See Section 3 for details.
new domains without any speech or text data from those do-
mains. Previous works exploit LLMs in ASR systems during
inference using re-scoring [7, 8] or fusion [9] techniques, and
they suffer from the costly overhead of LLM inference. In
contrast, we use LLMs to generate target domain synthetic
data for adapting a pretrained ASR model (see Fig. 1).
First, we generate a synthetic text corpus by prompting an
LLM with the target domain name (a word or short phrase).
To improve the quality of the synthetic text corpus, we pro-
pose a simple yet effective in-context instruction finetuning
(ICIF) strategy. Assuming that the pretrained ASR model
has been trained on a source dataset with multiple domains
(e.g. a personal assistant with a set of existing features), ICIF
learns to relate domain names to the knowledge of LLMs
from source text sentences. Then, we use a state-of-the-art
CSS model [10] to generate speech corresponding to the syn-
thetic texts. Finally, the fully synthetic paired speech-text cor-
pus is used to finetune a pretrained ASR model, improving
performance on the target domain of interest while retaining
the performance on the source domain.
Major contributions : (1) We demonstrate that text cor-
pus synthesis using LLMs enables zero-shot ASR domain
adaptation. (2) Our proposed in-context instruction finetuning
strategy improves the quality of the synthetic text corpus re-
sulting in significant gains in the final ASR performance. (3)
We show that the proposed data synthesis pipeline achieves
an average of 28% relative Word Error Rate (WER) reduction
on unseen target domains in the SLURP dataset [11].arXiv:2309.10707v1  [eess.AS]  18 Sep 2023

--- PAGE 2 ---
2. RELATED WORKS
Many works have shown that LLMs can synthesize data use-
ful for downstream tasks. Ye et al. [12], Yoo et al. [13], and
Meng et al. [14] prompt LLMs with handcrafted instructions
to generate data for finetuning downstream models. In this
work, we finetune the LLMs with instruction data to improve
the format consistency of the generated text corpus.
Some previous works also use LLMs to improve the per-
formance of ASR. Dingliwa et al. [7] and Ma et al. [8] con-
duct second-pass re-scoring using the perplexity score from
LLMs. Li et al. [9] propose deep LLM-fusion, which inte-
grates an LLM into the decoder of an encoder-decoder based
E2E ASR model. While these methods improve performance,
they require LLM inference during ASR decoding, which in-
creases the computational cost. In contrast, our method trans-
fers knowledge from the LLM to an ASR model through a
synthetic text corpus.
3. METHODOLOGY
Fig. 1 shows an overview of the proposed approach. Our
pipeline consists of a LLM, a CSS model, and a pretrained
ASR model. Given a target domain of interest dt, we
generate a fully synthetic text-speech paired corpus Ct=
{(xt
i, yt
i)}N
i=1, where xt
i,yt
iare the text content and speech
signal of the i-th sample respectively. To do this, we first
generate a text sentence xt
i∼pLLM (x|dt)from the LLM
conditioned on dt. Then, we synthesize the corresponding
speech yt
i∼pCSS(y|xt
i)using the CSS model. Finally, we
use the synthetic text-speech data to finetune the ASR model
for target domain dt.
3.1. Text Synthesis with LLMs
Our goal is to synthesize a text corpus that matches the text
distribution of a given target domain dt. To this end, we ask
LLMs which are pretrained on trillions of text tokens to gener-
ate sentences relevant to the target domain using the prompt:
“Please generate a sentence related to dt:”. Our initial ex-
periments show that naively prompting off-the-shelf LLMs
in our pipeline leads to some ASR improvement on the tar-
get domain. However, the quality of the generated text and
its relevance to the target domain are both insufficient. Since
off-the-shelf LLMs are trained on large-scale general text cor-
pora, it is difficult for them to produce high quality in-domain
text using only the target domain name dt. To address this
issue, we propose a simple yet effective in-context instruction
finetuning (ICIF) strategy that improves the ability of LLMs
to generate in-domain text when prompted with the target do-
main name.
3.1.1. In-Context Instruction Finetuning (ICIF)
Our proposed in-context instruction finetuning (ICIF) strategy
combines instruction finetuning (IF) [15] with demonstration
LLMPlease generate ... related to ‘ alarm ’:set alarm at ten amDomain : alarm          Text : set alarm at ten am
Domain : cooking      Text : how can i cook meal
Domain : takeaway    Text : any fast food meal of fersSource Domain Dataset
Please generate ... related to ‘ alarm ’:set alarm at ten am
Please generate ... related to ‘ alarm ’:set alarm at ten am
Source domain instruction data
Finetuned
LLMInstruction with
target domain
tell me who received my last email
remove the message
...Please generate ... ‘ alarm ’:set alarm ... am
Please generate ... ‘ cooking ’:how ... meal
...
Please generate a sentence related to ‘ email ’:Source domain
demonstrations
Finetuning Stage Infer ence StageFig. 2 .Illustration of In-Context Instruction Finetuning.
We explicitly relate domain name dto text xby reformulating
an instruction to finetune the LLMs. During inference, we use
source domain demonstrations and an unseen target domain
(email in the figure) instruction to prompt the LLMs.
or in-context learning (ICL) [16] using a unified instruction
format. Specifically, we first finetune the LLM with instruc-
tion data. Then, during inference, we prompt the LLM with
additional demonstrations in the same instruction format.
To construct the instruction data and demonstrations, we
use a source text corpus Cs={(xs
j, ds
j)}M
j=1, which con-
tains text xs
jfrom source domains ds
jdistinct from dt. As
shown in Fig. 2, we reformulate each (xs
j, ds
j)∈Csas a natu-
ral language instruction– “ Please generate a sentence related
tods
j:xs
j”– and finetune the LLM on these instructions. In
the inference stage, we prepend a subset of these instructions
from the source domain to the original prompt for the unseen
target domain (“ Please generate a sentence related to dt:”)
as additional demonstrations. The LLM uses the extended
prompt to generate a text sentence in target domain.
Our ICIF strategy learns the structure and format of the
source corpus Cs, and relates the target domain name dtto the
knowledge from pretrained LLMs. As shown in Section 5.2,
the resulting synthetic text corpus comprises high quality, di-
verse sentences which are semantically related to the unseen
target domain.
3.2. Controllable Speech Synthesis
We use a state-of-the-art Controllable Speech Synthesis
(CSS) model [10] to synthesize speech yt
i∼pCSS(y|xt
i),
given target domain text xt
igenerated by the instruction fine-
tuned LLM model. The CSS model learns a prior distribution
to model the acoustic style of speech. By sampling from this
prior distribution, the model can produce a synthetic speech
corpus in various acoustic conditions.
3.3. ASR Model Adaptation
Finally, we finetune the ASR model on the synthetic speech
prepared by the LLM and CSS model. In our initial exper-
iments, we observed that the ASR model usually overfits to
synthetic speech artifacts during finetuning, which limits its
performance. To address this problem, we add real speech
data ( i.e.,from source domains) to the synthetic speech from
the target domain to regularize the ASR model finetuning.

--- PAGE 3 ---
Zero-shot (WER) Target DomainsAverage
Methods Alarm Audio Calendar Cooking Datetime Email General IOT Lists Music News Play QA Recommendation Social Takeaway Transport Weather
Source domain ASR
(Baseline)8.0 13.1 12.8 18.2 11.2 19.0 14.4 19.2 14.6 10.5 15.3 24.8 22.3 15.7 26.3 26.5 17.1 12.9 16.77
ICIF 4.90 7.50 10.27 9.93 8.33 12.70 13.33 12.17 11.17 8.00 10.67 18.90 19.43 12.57 16.80 19.33 9.80 9.37 11.95
Relative WER (%) ↑38.75% 42.75% 19.79% 45.42% 25.60% 33.16% 7.41% 36.63% 23.52% 23.81% 30.28% 23.79% 12.86% 19.96% 36.12% 27.04% 42.69% 27.39% 28.73%
Table 1 .ASR Adaptation with Synthetic Text Corpus. Results of ASR models finetuned on target domain synthetic data
from our pipeline with ICIF. For each target domain, the source domain ASR (baseline) is trained on LibriSpeech followed by
the data from 17 domains (excluding the target domain) in SLURP dataset. The metric shown is WER (lower is better).
4. EXPERIMENTAL SETUP
4.1. Dataset
SLURP [11] is a spoken language understanding dataset con-
taining 16521 utterances of human commands towards a vir-
tual agent, based on 200 pre-defined prompts such as “How
would you ask for the time.” The utterances are recorded in
two types of acoustic environments (headset and far-field),
and categorized into 18 domains (email, alarm, and takeaway,
etc.). We use the headset subset to conduct experiments of
zero-shot ASR domain adaptation. In each of our experi-
ments, we select one of these domains as the target domain
and combine the remaining 17 domains to form the source do-
main. Our goal is to improve the performance of a pretrained
source domain ASR model on the target domain, without us-
ing any real speech or real text data from the target domain.
4.2. Large Language Models
We use LLaMA-7B [17] to synthesize the text corpus.
LLaMA is a state-of-the-art LLM with a decoder-based trans-
former architecture that is pretrained on trillions of tokens.
LLaMA has shown excellent performance on downstream
tasks with instruction finetuning [17, 18]. We apply low-rank
adaptation (LoRA) [19] to freeze most of the model parame-
ters and improve efficiency of instruction finetuning. During
inference/synthesis, we follow [20] to use typical decoding
[18] with τ= 0.9and set the repetition penalty [21] to 1.1.
We include 10demonstrations in the inference prompt.
4.3. Controllable Speech Synthesis (CSS)
Our CSS model is adopted from Style Equalization [10],
which is based on a Variational Recurrent Neural Net (VRNN).
We make the following four modifications to enhance its
acoustic style modeling: (1) increasing the number of Gaus-
sian mixtures of VRNN output distribution (from 3 to 10); (2)
increasing the size of acoustic style feature (from 512 to 768);
(3) initializing the hidden states of VRNN using the average
of the style vector sequence; and (4) using the acoustic style
feature to modulate the output linear layers, similar to what
is done in [22]. We train the modified CSS model on the
training set of LibriTTS [23].4.4. ASR Model Adaptation
We use ESPNet [24] to build the E2E ASR model, which is
composed of a conformer-based encoder [25] and a transformer-
based decoder [26]. In each of our experiments, we first ob-
tain a pretrained source domain ASR model by training on
LibriSpeech [27] followed by the source domain data ( i.e.,17
pre-defined SLURP domains excluding the target domain).
We then adapt this pretrained ASR model to the target do-
main using the synthetic data from LLM and CSS. For fair
comparison between models, we select all final checkpoints
using the target domain development set as a validation set.
5. RESULTS AND DISCUSSION
5.1. ASR Adaptation with Synthetic Text Corpus
In Table 1, we report the performance of ASR models
finetuned on target domain data from our corpus synthesis
pipeline. For each target domain, we prepare the synthetic
text corpus using LLMs with ICIF and the corresponding
synthetic speech using CSS. Remarkably, we achieve large
reductions in WER across the board (average relative im-
provement of 28.73%), without using any real text or speech
from the target domain for finetuning. For some target do-
mains ( i.e., Audio ,Cooking , and Transport ), we achieve more
than 40% relative improvement compared to the pretrained
source domain models. In addition, the finetuned ASR mod-
els also yield a small improvement (average relative WER
reduction of 5.98%) in source domains. Overall, these results
demonstrate the efficacy of our corpus synthesis pipeline for
adapting ASR models to unseen text domains.
We also finetune the source domain ASR models with
(1) real target domain text and synthetic speech, and (2) real
text and real speech, receiving average WERs of 10.77% and
10.74%, respectively. Note that, the purpose of this experi-
ment is to establish an upper-bound for adaptation, and real
target domain data is not available in zero-shot adaptation.
5.2. Analysis of ICIF
Contribution of IF and ICL
As detailed in Section 3.1.1, ICIF involves two steps: (1)
instruction (IF), which finetunes the LLM using instructions
formulated from a source text corpus, and (2) demonstration

--- PAGE 4 ---
WER↓Relative WER
Improvement (%) ↑Diversity ↓
(SB-4)Similarity ↓
(JS-Div)
Source Domain ASR 16.77 - - -
ICIF (IF+ICL) 11.95 28.73 0.596 0.466
Demo (ICL) 12.13 27.67 0.424 0.482
Instruct (IF) 12.59 24.92 0.74 0.451
Naive Prompting 14.02 16.40 0.471 0.521
Table 2 .Analysis of ICIF. We investigate the individual con-
tributions of instruction (IF) and demonstration (ICL) to ICIF.
In addition to the WER, we report the diversity of the syn-
thetic text (SB-4), and its similarity to the target domain text
corpus (JS-Div). See Section 5.2 for details.
(ICL), which prompts the LLM with some example instruc-
tions. Table 2 analyzes the individual contributions of these
components. We observe that both are useful for improving
the WER of the finetuned ASR model: using either instruction
(IF) or demonstration (ICL) improves the WER over naive
prompting ( i.e.,from 14.02to12.13and12.59respectively).
Combining IF and ICL (ICIF) further improves the WER to
11.95. These results indicate that both instruction and demon-
stration are useful to our synthetic corpus pipeline. Next, we
ask whether instruction and demonstration have overlapping
effects on the synthetic text quality, or whether they play dis-
tinct roles. To address this question, we profile the synthetic
text along two additional axes: (1) diversity, measured by
Self-BLEU 4-gram [28] and (2) similarity to the real target
corpus, measured by the JS divergence between token distri-
butions [29]. As shown in Table 2, instruction (IF) is highly
effective at generating text similar to the target domain, but at
the cost of diversity. On the other hand, demonstration (ICL)
achieves high diversity with a modest improvement in similar-
ity. Combining the two techniques strikes a balance between
improving diversity and similarity of the synthetic text to the
target domain. We conclude that ICIF enables the LLM to
map from domain names to more relevant and diverse texts,
which in turn improves the generalization of ASR models to
unseen target domains.
Impact of synthetic text corpus size on WER
Fig. 3 shows the performance of ASR models finetuned on
varying amounts of synthetic text for two randomly-selected
target domains ( ‘Transport’ and‘Cooking’ ). In general, we
find that using more synthetic text data to finetune the ASR
models improves the WER, which suggests that the models
benefit from exposure to greater text diversity. On the other
hand, we also observe that ASR performance saturates at
some point ( e.g., at around 55K samples for the “Cooking”
domain). This may be due to synthetic artifacts or noise. We
leave the problem of synthetic data selection to future work.
Impact of number of demonstrations on WER
Since demonstrations increase synthetic text diversity, we
also investigate the impact of the number of demonstrations
on the performance of finetuned ASR models. Fig. 4 shows
the WER on two randomly-selected target domains when
1.1k 11k 22k 55k 110k
Number of synthetic text810121416WER (%)
Transport
ICIF
Source Domain ASR
Real Text (Oracle)
1.1k 11k 22k 55k 110k
Number of synthetic text1012141618WER (%)
Cooking
ICIF
Source Domain ASR
Real Text (Oracle)Fig. 3 .Number of synthetic text v. WER. We vary the
number of synthetic text samples used to finetune the ASR
models and report the WER for two randomly-selected target
domains. Number of samples and WER are shown on the x
and y axes respectively.
0 2 4 6 8 10
Number of demonstration9.69.810.010.210.410.6WER (%)
Transport
0 2 4 6 8 10
Number of demonstration9.69.810.010.210.410.610.811.011.2WER (%)
Cooking
Fig. 4 .Number of demonstrations v. WER. We vary
the number of demonstrations used for promting the LLM
model and report the WER of finetuned ASR models for two
randomly-selected target domains. Number of demonstra-
tions and WER are shown on the x and y axes respectively.
varying the number of demonstrations from 0to10. We
observe that WER is improved significantly even with two
demonstrations and continues to improve with more demon-
strations. Interestingly, we also observe that the standard
deviation of the WER increases with more demonstrations.
We hypothesize this is due to increased text diversity, which
leads to variable outcomes during finetuning. The selection
and ordering of demonstrations may also impact the synthetic
text quality. We leave these investigations to future work.
6. CONCLUSIONS
In this paper, we propose a pipeline which consists of a
LLM and a CSS model to adapt ASR models with synthesize
speech corpus. We apply the data synthesis pipeline to ASR
domain adaptation with no target domain data, and receive
16% relative improvements with pretrained LLMs. To further
improve synthesized text quality, we employ an innovative
in-context instruction finetuning (ICIF) method on LLMs.
The results show that our proposed method yields 28% av-
erage WER relative improvement on unseen target domains
without dropping the performance on source domains.

--- PAGE 5 ---
7. REFERENCES
[1] Zhong Meng, Yashesh Gaur, Naoyuki Kanda, Jinyu Li, Xie
Chen, Yu Wu, and Yifan Gong, “Internal language model adap-
tation with text-only data for end-to-end speech recognition,”
Proc. InterSpeech , 2022.
[2] Janne Pylkk ¨onen, Antti Ukkonen, Juho Kilpikoski, Samu Tam-
minen, and Hannes Heikinheimo, “Fast text-only domain
adaptation of rnn-transducer prediction network,” Proc. Inter-
Speech , 2021.
[3] Keqi Deng and Philip C Woodland, “Adaptable end-to-end asr
models using replaceable internal lms and residual softmax,”
inICASSP . IEEE, 2023, pp. 1–5.
[4] Ashish Mittal, Sunita Sarawagi, and Preethi Jyothi, “In-
situ text-only adaptation of speech models with low-overhead
speech imputations,” in ICLR , 2022.
[5] Raviraj Joshi and Anupam Singh, “A simple baseline for do-
main adaptation in end to end asr systems using synthetic data,”
arXiv preprint arXiv:2206.13240 , 2022.
[6] Vladimir Bataev, Roman Korostik, Evgeny Shabalin, Vitaly
Lavrukhin, and Boris Ginsburg, “Text-only domain adaptation
for end-to-end asr using integrated text-to-mel-spectrogram
generator,” Proc. InterSpeech , 2023.
[7] Saket Dingliwa, Ashish Shenoy, Sravan Bodapati, Ankur
Gandhe, Ravi Teja Gadde, and Katrin Kirchhoff, “Domain
prompts: Towards memory and compute efficient domain
adaptation of asr systems,” in Proc. Interspeech , 2022.
[8] Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, and
Kate Knill, “Can generative large language models perform asr
error correction?,” arXiv preprint arXiv:2307.04172 , 2023.
[9] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, “Prompting large
language models for zero-shot domain adaptation in speech
recognition,” arXiv preprint arXiv:2306.16007 , 2023.
[10] Jen-Hao Rick Chang, Ashish Shrivastava, Hema Koppula, Xi-
aoshuai Zhang, and Oncel Tuzel, “Style equalization: Unsu-
pervised learning of controllable generative sequence models,”
inProc. ICML , 2022.
[11] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and
Verena Rieser, “SLURP: A spoken language understanding
resource package,” in Proc. EMNLP , 2020.
[12] Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng,
Zhiyong Wu, Tao Yu, and Lingpeng Kong, “ZeroGen: Efficient
zero-shot learning via dataset generation,” in Proc. EMNLP ,
2022.
[13] Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee,
and Woomyoung Park, “GPT3Mix: Leveraging large-scale
language models for text augmentation,” in Findings of the As-
sociation for Computational Linguistics: EMNLP 2021 , 2021.
[14] Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han, “Gener-
ating training data with language models: Towards zero-shot
language understanding,” in NeurIPS , 2022.
[15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph,
Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa De-
hghani, Siddhartha Brahma, et al., “Scaling instruction-
finetuned language models,” arXiv preprint arXiv:2210.11416 ,
2022.[16] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al., “Lan-
guage models are few-shot learners,” NeurIPS , 2020.
[17] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample, “Llama: Open and efficient foundation language
models,” 2023.
[18] Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan Cot-
terell, “Locally typical sampling,” Trans. of ACL , vol. 11,
pp. 102–121, 2023.
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen, “Lora:
Low-rank adaptation of large language models,” Proc. ICLR ,
2022.
[20] Yen-Ting Lin, Alexandros Papangelis, Seokhwan Kim,
Sungjin Lee, Devamanyu Hazarika, Mahdi Namazifar, Di Jin,
Yang Liu, and Dilek Hakkani-Tur, “Selective in-context
data augmentation for intent detection using pointwise V-
information,” in Proc. EACL , 2023.
[21] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caim-
ing Xiong, and Richard Socher, “Ctrl: A conditional trans-
former language model for controllable generation,” arXiv
preprint arXiv:1909.05858 , 2019.
[22] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and
Gordon Wetzstein, “pi-gan: Periodic implicit generative adver-
sarial networks for 3d-aware image synthesis,” in Proc. CVPR ,
2021.
[23] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss,
Ye Jia, Zhifeng Chen, and Yonghui Wu, “Libritts: A corpus
derived from librispeech for text-to-speech,” arXiv preprint
arXiv:1904.02882 , 2019.
[24] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-
plin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al.,
“Espnet: End-to-end speech processing toolkit,” arXiv preprint
arXiv:1804.00015 , 2018.
[25] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-
mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-
dong Zhang, Yonghui Wu, et al., “Conformer: Convolution-
augmented transformer for speech recognition,” Proc. Inter-
Speech , 2020.
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin, “Attention is all you need,” NeurIPS , 2017.
[27] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev
Khudanpur, “Librispeech: an asr corpus based on public do-
main audio books,” in Proc. ICASSP , 2015.
[28] Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan
Zhang, Jun Wang, and Yong Yu, “Texygen: A benchmarking
platform for text generation models,” SIGIR , 2018.
[29] Sebastian Ruder, Parsa Ghaffari, and John G. Breslin, “Data
selection strategies for multi-domain sentiment analysis,”
2017.
