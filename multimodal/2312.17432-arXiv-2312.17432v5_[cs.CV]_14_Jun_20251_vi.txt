# 2312.17432.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.17432.pdf
# Kích thước tệp: 3781007 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2312.17432v5 [cs.CV] 14 Jun 20251
Hiểu Video với Mô hình Ngôn ngữ Lớn:
Một Khảo sát
Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Sinh viên Thành viên, IEEE, Teng Wang,
Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Sinh viên Thành viên, IEEE, Chao Huang,
Zeliang Zhang, Pinxin Liu, Mingqian Feng, Feng Zheng, Thành viên, IEEE, Jianguo Zhang, Thành viên Cao cấp, IEEE,
Ping Luo, Thành viên, IEEE, Jiebo Luo, Thành viên, IEEE, và Chenliang Xu, Thành viên, IEEE

Tóm tắt—Với sự phát triển nhanh chóng của các nền tảng video trực tuyến và khối lượng nội dung video ngày càng tăng, nhu cầu về các công cụ hiểu video thành thạo đã tăng lên đáng kể. Với những khả năng đáng chú ý của các mô hình ngôn ngữ lớn (LLM) trong các nhiệm vụ ngôn ngữ và đa phương thức, khảo sát này cung cấp một tổng quan chi tiết về những tiến bộ gần đây trong hiểu video khai thác sức mạnh của LLM (Vid-LLM). Các khả năng nổi bật của Vid-LLM là đặc biệt tiên tiến, đặc biệt là khả năng lý luận đa độ phân giải mở (trừu tượng, thời gian và không gian-thời gian) kết hợp với kiến thức thông thường, gợi ý một con đường đầy hứa hẹn cho hiểu video trong tương lai. Chúng tôi khảo sát các đặc điểm và khả năng độc đáo của Vid-LLM, phân loại các phương pháp thành ba loại chính: Video Analyzer ×LLM, Video Embedder ×LLM, và (Analyzer + Embedder) ×LLM. Chúng tôi xác định năm loại phụ dựa trên chức năng của LLM trong Vid-LLM: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, và LLM as Hidden Layer. Khảo sát này cũng trình bày một nghiên cứu toàn diện về các nhiệm vụ, tập dữ liệu, điểm chuẩn và phương pháp đánh giá cho Vid-LLM. Ngoài ra, nó khám phá các ứng dụng rộng rãi của Vid-LLM trong các lĩnh vực khác nhau, nhấn mạnh khả năng mở rộng và tính linh hoạt đáng chú ý của chúng trong các thách thức hiểu video thực tế. Bổ sung, nó tóm tắt các hạn chế của Vid-LLM hiện tại và phác thảo hướng nghiên cứu tương lai. Để biết thêm thông tin, độc giả được khuyến khích truy cập kho lưu trữ tại https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.

Từ khóa chỉ mục—Hiểu Video, Mô hình Ngôn ngữ Lớn, Mô hình Thị giác-Ngôn ngữ, Học Đa phương thức

I. GIỚI THIỆU

CHÚNG ta sống trong một thế giới đa phương thức nơi video đã trở thành hình thức phương tiện truyền thông chủ đạo. Với sự mở rộng nhanh chóng của các nền tảng video trực tuyến và sự phổ biến ngày càng tăng của camera trong giám sát, giải trí và lái xe tự động, nội dung video đã nổi lên như một phương tiện hấp dẫn và phong phú, vượt trội hơn các kết hợp văn bản và hình ảnh-văn bản truyền thống về cả độ sâu và sức hấp dẫn. Tiến bộ này đã thúc đẩy sự gia tăng theo cấp số nhân trong sản xuất video, với hàng triệu video được tạo ra mỗi ngày. Tuy nhiên, xử lý thủ công một khối lượng nội dung video khổng lồ như vậy là tốn nhiều lao động và thời gian. Kết quả là, có

Y. Tang, J. Bi, L. Song, S. Liang, D. Zhang, J. An, J. Lin, R. Zhu, A. Vosoughi, C. Huang, Z. Zhang, P. Liu, M. Feng, J. Luo, và C. Xu thuộc Đại học Rochester
T. Wang và P. Luo thuộc Đại học Hồng Kông
S. Xu, T. Wang, F. Zheng, và J. Zhang thuộc Đại học Khoa học và Công nghệ Phương Nam
Liên hệ với Y. Tang, J. Luo, và C. Xu ({yunlong.tang@, jluo@cs., chenliang.xu@}rochester.edu)

nhu cầu ngày càng tăng về các công cụ để quản lý, phân tích và xử lý hiệu quả lượng nội dung video dồi dào này. Để đáp ứng nhu cầu này, các phương pháp hiểu video đã xuất hiện sử dụng các kỹ thuật phân tích thông minh để tự động nhận dạng và diễn giải nội dung video, giảm đáng kể khối lượng công việc cho các nhà điều hành con người. Ngoài ra, sự phát triển liên tục của các phương pháp này đang cải thiện khả năng giải quyết nhiệm vụ của chúng, cho phép chúng xử lý một loạt các nhiệm vụ hiểu video với độ thành thạo ngày càng tăng.

A. Phát triển của Các Phương pháp Hiểu Video
Sự tiến hóa của các phương pháp hiểu video có thể được chia thành bốn giai đoạn, như được hiển thị trong Hình 1:

1) Các Phương pháp Thông thường: Trong giai đoạn đầu của hiểu video, các kỹ thuật trích xuất đặc trưng thủ công như Scale-Invariant Feature Transform (SIFT) [1], Speeded-Up Robust Features (SURF) [2], và Histogram of Oriented Gradients (HOG) [3] đã được sử dụng để nắm bắt thông tin chính trong video. Background Subtraction [4], các phương pháp optical flow [5], và Improved Dense Trajectories (IDT) [6], [7] được sử dụng để mô hình hóa thông tin chuyển động để theo dõi. Vì video có thể được xem như dữ liệu chuỗi thời gian, các kỹ thuật phân tích thời gian như Hidden Markov Models (HMM) [8] cũng đã được sử dụng để hiểu nội dung video. Trước khi học sâu trở nên phổ biến, các thuật toán học máy cơ bản như Support Vector Machines (SVM) [9], Decision Trees [10], và Random Forests cũng đã được sử dụng trong các nhiệm vụ phân loại và nhận dạng video. Phân tích cụm [11] để phân loại các đoạn video, hoặc Principal Component Analysis (PCA) [12], [13] để giảm chiều dữ liệu cũng đã là các phương pháp thường được sử dụng để phân tích video.

2) Các Mô hình Video Neural Sớm: So với các phương pháp cổ điển, các phương pháp học sâu cho hiểu video có khả năng giải quyết nhiệm vụ vượt trội. DeepVideo [14] và [15] là các phương pháp sớm giới thiệu mạng neural sâu, cụ thể là Convolutional Neural Network (CNN), cho hiểu video. Tuy nhiên, hiệu suất không vượt trội so với phương pháp đặc trưng thủ công tốt nhất do việc sử dụng thông tin chuyển động không đầy đủ. Mạng hai luồng [16] kết hợp cả CNN và IDT để nắm bắt thông tin chuyển động nhằm cải thiện hiệu suất, điều này đã xác minh khả năng của mạng neural sâu trong hiểu video. Để xử lý hiểu video dài, Long Short-Term Memory (LSTM) đã được áp dụng [17]. Temporal Segment Network (TSN) [18] cũng

--- TRANG 2 ---
2

Hình 1. Sự phát triển của các phương pháp hiểu video có thể được tóm tắt thành bốn giai đoạn: (1) Các Phương pháp Thông thường, (2) Các Mô hình Video Neural Sớm, (3) Tiền huấn luyện Video Tự giám sát, và (4) Mô hình Ngôn ngữ Lớn cho Hiểu Video, tức là Vid-LLM. Khả năng giải quyết nhiệm vụ của chúng không ngừng cải thiện, và chúng có tiềm năng nâng cao hơn nữa.

được thiết kế cho hiểu video dài bằng cách phân tích và tổng hợp các đoạn video. Bên cạnh TSN, mã hóa Fisher Vectors (FV) [19], mã hóa Bi-Linear [20], và mã hóa Vector of Locally Aggregated Descriptors (VLAD) [21] đã được giới thiệu [22]. Các phương pháp này cải thiện hiệu suất trên các tập dữ liệu UCF-101 [23] và HMDB51 [24]. Khác với mạng hai luồng, mạng 3D bắt đầu một nhánh khác bằng cách giới thiệu 3D CNN vào hiểu video (C3D) [25]. Inflated 3D ConvNets (I3D) [26] sử dụng việc khởi tạo và kiến trúc của 2D CNN, Inception [27], để đạt được cải thiện lớn trên các tập dữ liệu UCF-101 và HMDB51. Tiếp theo, mọi người bắt đầu sử dụng các tập dữ liệu Kinetics-400 (K-400) [28] và Something-Something [29] để đánh giá hiệu suất mô hình trong các tình huống thách thức hơn. ResNet [30], ResNeXt [31], và SENet [32] cũng được điều chỉnh từ 2D sang 3D, dẫn đến sự xuất hiện của R3D [33], MFNet [34], và STC [35]. Để cải thiện hiệu quả, convolution 3D đã được phân tách thành cascade 2D và 1D convolution trong các nghiên cứu khác nhau (ví dụ, S3D [36], ECO [37], P3D [38]). LTC [39], T3D [40], Non-local [41], và V4D [42] tập trung vào mô hình hóa thời gian dài, trong khi CSN [43], SlowFast [44], và X3D [45] có xu hướng đạt hiệu quả cao. Việc giới thiệu Vision Transformers (ViT) [46] thúc đẩy một loạt các mô hình nổi bật (ví dụ, TimeSformer [47], VidTr [48], ViViT [49], MViT [50]).

3) Tiền huấn luyện Video Tự giám sát: Khả năng chuyển giao [51], [52] trong các mô hình tiền huấn luyện tự giám sát [53] cho hiểu video cho phép chúng tổng quát hóa qua các nhiệm vụ đa dạng với việc gán nhãn bổ sung tối thiểu, vượt qua yêu cầu của các mô hình học sâu sớm về dữ liệu cụ thể nhiệm vụ mở rộng. VideoBERT [54] là một nỗ lực sớm để thực hiện tiền huấn luyện video. Dựa trên mô hình ngôn ngữ hai chiều BERT [55], các nhiệm vụ tiền huấn luyện được thiết kế cho học tự giám sát từ dữ liệu video-văn bản. Nó token hóa các đặc trưng video với hierarchical k-means. Mô hình được tiền huấn luyện có thể được tinh chỉnh để xử lý nhiều nhiệm vụ downstream, bao gồm phân loại hành động và mô tả video. Theo mô hình "tiền huấn luyện-tinh chỉnh", nhiều nghiên cứu về các mô hình được tiền huấn luyện cho hiểu video, đặc biệt là các mô hình video-ngôn ngữ, đã xuất hiện. Chúng sử dụng các kiến trúc khác nhau (ActBERT [56], SpatiotemporalMAE [57], OmniMAE [58], VideoMAE [59], MotionMAE [60]) hoặc các chiến lược huấn luyện (MaskFeat [61], VLM [62], ALPRO [63], All-in-One transformer [64], MaskViT [65], CLIP-ViP [66], Singularity [67], LF-VILA [68], EMCL [69], HiTeA [70], CHAMPAGNE [71]).

4) Mô hình Ngôn ngữ Lớn cho Hiểu Video: Gần đây, các mô hình ngôn ngữ lớn (LLM) đã tiến bộ nhanh chóng [72]. Sự xuất hiện của các mô hình ngôn ngữ lớn được tiền huấn luyện trên các tập dữ liệu mở rộng đã giới thiệu một khả năng học trong bối cảnh mới [73]. Điều này cho phép chúng xử lý các nhiệm vụ khác nhau bằng cách sử dụng các lời nhắc mà không cần tinh chỉnh. ChatGPT [74] là ứng dụng đột phá đầu tiên được xây dựng trên nền tảng này. Điều này bao gồm các khả năng như tạo mã và gọi các công cụ hoặc API của các mô hình khác để sử dụng. Nhiều nghiên cứu đang khám phá việc sử dụng LLM như ChatGPT để gọi các API mô hình thị giác để giải quyết các vấn đề trong lĩnh vực thị giác máy tính, bao gồm Visual-ChatGPT [75]. Sự ra đời của instruct-tuning đã tăng cường hơn nữa khả năng của các mô hình này trong việc phản hồi hiệu quả các yêu cầu của người dùng và thực hiện các nhiệm vụ cụ thể [76]. LLM tích hợp với khả năng hiểu video mang lại lợi thế của việc hiểu đa phương thức tinh vi hơn, cho phép chúng xử lý và diễn giải các tương tác phức tạp giữa dữ liệu thị giác và văn bản. Tương tự như tác động của chúng trong Xử lý Ngôn ngữ Tự nhiên (NLP) [77], các mô hình này hoạt động như những người giải quyết nhiệm vụ đa năng hơn, thành thạo trong việc xử lý một loạt các nhiệm vụ rộng hơn bằng cách tận dụng cơ sở kiến thức mở rộng và hiểu biết ngữ cảnh thu được từ lượng lớn dữ liệu đa phương thức. Điều này cho phép chúng không chỉ hiểu nội dung thị giác mà còn lý luận về nó theo cách phù hợp hơn với hiểu biết giống con người. Nhiều công trình cũng khám phá việc sử dụng LLM trong các nhiệm vụ hiểu video, cụ thể là Vid-LLM.

B. Các Khảo sát Liên quan

Các bài báo khảo sát trước đây hoặc nghiên cứu các nhiệm vụ phụ cụ thể trong lĩnh vực hiểu video hoặc tập trung vào các phương pháp luận ngoài hiểu video. Ví dụ, [78] khảo sát các mô hình nền tảng đa phương thức cho các nhiệm vụ thị giác-ngôn ngữ tổng quát, bao gồm

--- TRANG 3 ---
3

Hình 2. Một dòng thời gian toàn diện mô tả sự phát triển của các phương pháp hiểu video với mô hình ngôn ngữ lớn (Vid-LLM). Khảo sát này dựa trên những tiến bộ cho đến cuối tháng 6 năm 2024.

cả ứng dụng hình ảnh và video. [79] và [80] tập trung vào khảo sát các nhiệm vụ mô tả video và nhận dạng hành động, tương ứng. Các nhiệm vụ hiểu video khác, như trả lời câu hỏi video và grounding, không được xem xét. Hơn nữa, [81], [82], và [77] khảo sát các phương pháp luận liên quan đến video, như các mô hình khuếch tán video và LLM, thiếu sự tập trung vào hiểu video. [83] tập trung chủ yếu vào các mô hình nền tảng video, với sự chú ý không đầy đủ dành cho các phương pháp dựa trên mô hình ngôn ngữ. Mặc dù có giá trị đáng kể cho cộng đồng, các bài báo khảo sát trước đây để lại một khoảng trống trong việc khảo sát nhiệm vụ hiểu video tổng quát dựa trên mô hình ngôn ngữ lớn. Bài báo này lấp đầy khoảng trống đó bằng cách khảo sát toàn diện nhiệm vụ hiểu video sử dụng mô hình ngôn ngữ lớn.

C. Cấu trúc Khảo sát

Khảo sát này được cấu trúc như sau: Mục II cung cấp các khái niệm cơ bản cho hiểu video với LLM, bao gồm tóm tắt các nhiệm vụ hiểu video khác nhau yêu cầu xử lý các mức độ phân giải khác nhau, các tập dữ liệu liên quan và các chỉ số đánh giá. Nền tảng của LLM cũng được giới thiệu trong mục này. Trong Mục III, chúng tôi đi sâu vào chi tiết của nghiên cứu gần đây tận dụng LLM cho hiểu video, trình bày các phương pháp độc đáo và tác động của chúng trong lĩnh vực, nơi chúng tôi chia các Vid-LLM này thành ba danh mục chính, Video Analyzer ×LLM và Video Embedder ×LLM, và (Analyzer + Embedder) ×LLM; và năm danh mục phụ, LLM as Summarizer/Manager/Text Decoder/Regressor/Hidden Layer, được hiển thị như Hình 5. Mục này cũng bao gồm các chiến lược huấn luyện của Vid-LLM. Mục IV bổ sung thêm thông tin về các cách phổ biến để đánh giá Vid-LLM, cùng với một số điểm chuẩn và hiệu suất của một số Vid-LLM trên các điểm chuẩn được sử dụng phổ biến nhất. Mục V khám phá

ứng dụng của Vid-LLM qua nhiều lĩnh vực quan trọng và xác định các thách thức chưa được giải quyết và các lĩnh vực tiềm năng cho nghiên cứu tương lai.

Bên cạnh khảo sát này, chúng tôi đã thành lập một kho lưu trữ GitHub tổng hợp các tài nguyên hỗ trợ khác nhau cho hiểu video với mô hình ngôn ngữ lớn (Vid-LLM). Kho lưu trữ này, dành riêng cho việc nâng cao hiểu video thông qua Vid-LLM, có thể được truy cập tại Awesome-LLMs-for-Video-Understanding.

II. KHÁI NIỆM CƠ BẢN

Trong mục này, chúng tôi giới thiệu nền tảng của hiểu video và Mô hình Ngôn ngữ Lớn (LLM).

A. Các Nhiệm vụ Hiểu Video

Hiểu video là một nhiệm vụ cơ bản nhưng đầy thách thức đã truyền cảm hứng cho sự xuất hiện của nhiều nhiệm vụ trong một lĩnh vực tương tự nhằm diễn giải nội dung video phức tạp. Công trình tiên phong cho hiểu video bao gồm các phương pháp phân loại video và nhận dạng hành động, phân loại video thành các nhãn lớp và danh mục hành động, tương ứng. Với sự phát triển của các mô hình nền tảng thị giác và việc mở rộng các tập dữ liệu công khai, các phương pháp hiểu video hiện tại có thể nắm bắt, phân tích và lý luận cho nội dung video phức tạp hơn. Ví dụ, mô tả video, như một nhiệm vụ cụ thể của hiểu video, không chỉ yêu cầu mô hình tạo ra các mô tả chi tiết về nội dung video, mà các mô tả video được tạo ra phải có logic và tuân theo lẽ thông thường về các cảnh được mô tả. Bổ sung, nhiệm vụ Video Question-Answering (VQA) yêu cầu mô hình hiểu nội dung và tham khảo thông tin bên ngoài để cung cấp câu trả lời chính xác. Con đường phát triển của hiểu video từ phân loại đơn giản đến hiểu và lý luận ngôn ngữ tự nhiên làm nổi bật một xu hướng rõ ràng của mô hình hiểu video hướng tới mức độ

--- TRANG 4 ---
4

Hình 3. Hình này phân loại các nhiệm vụ trong hiểu video, mô tả độ phân giải cần thiết và sự tham gia của ngôn ngữ cần thiết để các mô hình thực hiện các nhiệm vụ này một cách hiệu quả. Biểu đồ này loại trừ các nhiệm vụ liên quan đến các phương thức đặc biệt hoặc cụ thể, như hiểu video nghe nhìn và egocentric. Đáng chú ý, các nhiệm vụ được trình bày có thể được thống nhất thành một mô hình trả lời câu hỏi, và tất cả được giải quyết bởi các mô hình lớn sinh tạo, tương tự như những tiến bộ gần đây trong NLP.

khả năng diễn giải video gần mức con người. Chúng tôi tóm tắt các nhiệm vụ chính trong hiểu video như sau.

1) Các Nhiệm vụ Hiểu Trừu tượng: Phân loại Video, Nhận dạng Hành động, Truy xuất Văn bản-Video, Tóm tắt Video-thành-Văn bản, và Mô tả Video.

•Phân loại Video & Nhận dạng Hành động: Phân loại video và nhận dạng hành động phân loại video dựa trên các nhãn lớp hoặc các danh mục hoạt động và sự kiện trong một chuỗi video. Các tập dữ liệu được giới thiệu cụ thể cho các nhiệm vụ này bao gồm UCF-101 [23], HMDB51 [24], Hollywood [84], ActivityNet [85], Charades [86], Kinetics-400 [28], Kinetics-600 [87], Kinetics-700 [88], SomethingSomethingV2 [29], HACS [89], YouTube8M [90], và PortraitMode-400 [91]. Thông thường, độ chính xác Top-K được áp dụng là chỉ số chính cho các nhiệm vụ này.

•Truy xuất Văn bản-Video: Nhiệm vụ truy xuất văn bản-video khớp và truy xuất các clip video liên quan dựa trên độ tương tự giữa các clip video và các mô tả văn bản đầu vào. Các tập dữ liệu như Kinetic-GEB [92], MSRVTT [93], DiDeMo [94], YouCook2 [95], và Youku-mPLUG [96] liên quan đến nhiệm vụ này. Chỉ số đánh giá tiêu chuẩn cho nhiệm vụ này là Recall at K (R@K), đo lường độ chính xác của K kết quả truy xuất đầu tiên.

•Tóm tắt Video-thành-Văn bản: Tóm tắt video-thành-văn bản là một nhiệm vụ tạo ra các tóm tắt văn bản ngắn gọn của video. Các phương pháp tóm tắt video được huấn luyện để trích xuất và diễn giải nội dung thị giác và âm thanh chính để tạo ra các tóm tắt mạch lạc và thông tin. ViTT [97], VideoXum [98], VideoInstruct-100K [99], và InstructV2Xum [100] là các tập dữ liệu liên quan đến nhiệm vụ này. Các chỉ số BLEU, METEOR, CIDEr, và ROUGE-L thường đánh giá nhiệm vụ này.

•Mô tả Video: Mô tả video tạo ra các mô tả văn bản mô tả và mạch lạc của các video đã cho. Các mô hình mô tả video thường sử dụng thông tin thị giác và âm thanh từ video để tạo ra các mô tả chính xác và phù hợp với ngữ cảnh. Các tập dữ liệu đáng chú ý cho nhiệm vụ này là MSVD [101], MSR-VTT [102], TGIF [103], Charades [86], Charades-Ego [104], YouCook2 [95], YookuMPLUG [96], VAST-27M [105], và VideoInstruct-100K [99]. Nhiệm vụ này thường được đánh giá bởi các chỉ số BLEU, METEOR, CIDEr, và ROUGE-L.

•Video QA: Video Question-Answering (VQA) nhằm trả lời các câu hỏi văn bản dựa trên một video đã cho, nơi mô hình phân tích thông tin thị giác và âm thanh, hiểu ngữ cảnh, và cuối cùng tạo ra các phản hồi chính xác. Các tập dữ liệu liên quan đến nhiệm vụ QA là VCR [106], MSVD-QA [93], MSRVTT-QA [93], TGIF-QA [107], Pororo-QA [108], TVQA [109], ActivityNet-QA [110], và NExT-QA [111]. Nhiệm vụ này được đánh giá bằng độ chính xác Top-1, Top-K.

2) Các Nhiệm vụ Hiểu Thời gian:

•Tóm tắt Video: Tóm tắt Video nhằm nén một video dài thành phiên bản ngắn hơn trong khi bảo tồn nội dung thiết yếu. F1-score, Spearman, và Kendall thường đánh giá nhiệm vụ này như các chỉ số. Các tập dữ liệu được sử dụng phổ biến bao gồm SumMe [112], TVSum [113], Ads1k [114], VideoXum [98], InstructV2Xum [100].

•Phát hiện Nổi bật Video: Phát hiện nổi bật video nhằm xác định và trích xuất các đoạn quan trọng và thú vị nhất từ một video. Các tập dữ liệu được sử dụng phổ biến cho nhiệm vụ này bao gồm YouTube Highlights [115], TV-Sum [113], và Videos Titles in the Wild (VTW) [116].

--- TRANG 5 ---
5

•Định vị Hành động/Sự kiện Thời gian: Nhiệm vụ này nhằm xác định các đoạn thời gian chính xác của các hành động hoặc sự kiện trong một video. Bằng cách phân tích các khung hình tuần tự, các mô hình được huấn luyện cho nhiệm vụ này phải chỉ ra khi nào các hoạt động cụ thể bắt đầu và kết thúc. Các tập dữ liệu cho Định vị Hành động/Sự kiện Thời gian bao gồm THUMOS'14 [117], ActivityNet-1.3 [85], và UnAV-100 [118].

•Tạo Đề xuất Hành động Thời gian: Tạo đề xuất hành động thời gian bao gồm việc tạo ra các đoạn ứng cử viên trong một video có khả năng chứa các hành động hoặc sự kiện. Các tập dữ liệu liên quan như THUMOS'14 [117], ActivityNet [85], và Charades [86] được sử dụng để huấn luyện và đánh giá cho nhiệm vụ này.

•Grounding Thời gian Video: Grounding thời gian video là nhiệm vụ định vị các khoảnh khắc hoặc khoảng thời gian cụ thể trong một video tương ứng với một truy vấn văn bản đã cho. Quá trình này bao gồm việc căn chỉnh các mô tả ngôn ngữ với nội dung thị giác, cho phép xác định chính xác các đoạn liên quan cho các ứng dụng trong tìm kiếm video và phân tích nội dung. Các điểm chuẩn phổ biến là Charades-STA [119], ViTT [97], DiDeMo [94], và PU-VALOR [120]. Các chỉ số R1@0.5 và R1@0.7 thường đánh giá nhiệm vụ này.

•Truy xuất Khoảnh khắc: Truy xuất khoảnh khắc là nhiệm vụ xác định và trích xuất các đoạn video chính xác tương ứng với một truy vấn văn bản hoặc thị giác đã cho, căn chỉnh nội dung ngữ nghĩa giữa các truy vấn và khung hình video. DiDeMo [94] là một tập dữ liệu cho nhiệm vụ này.

•Phát hiện Biên giới Sự kiện Tổng quát: Phát hiện biên giới sự kiện tổng quát bao gồm việc xác định các khung hình nhất định trong video nơi xảy ra các thay đổi đáng kể và chia tách video dựa trên các sự kiện hoặc hoạt động khác nhau. Kinetics-GEBD [121] là một tập dữ liệu được sử dụng rộng rãi cho nhiệm vụ này.

•Mô tả & Grounding Biên giới Sự kiện Tổng quát: Mô tả và grounding biên giới sự kiện tổng quát bao gồm việc xác định và mô tả các điểm chuyển tiếp giữa các sự kiện quan trọng trong video, nơi Kinetics-GEB+ [92] là tập dữ liệu cho nhiệm vụ này.

•Mô tả Video Dày đặc: Mô tả video dày đặc [122]–[126] nhằm tạo ra các mô tả văn bản chi tiết và liên tục cho nhiều sự kiện và hành động xảy ra trong suốt một video. Các chỉ số đánh giá như BLEU, METEOR, CIDEr, và ROUGE-L được sử dụng để đánh giá nhiệm vụ này. Các tập dữ liệu liên quan là ActivityNet Captions [127], VidChapters-7M [128], YouCook2 [95], và ViTT [97].

3) Các Nhiệm vụ Hiểu Không gian-Thời gian:

•Theo dõi Đối tượng: Theo dõi đối tượng nhằm liên tục xác định và theo dõi vị trí của các đối tượng cụ thể trong một video theo thời gian. Một mô hình theo dõi tốt nên duy trì các quỹ đạo chính xác và nhất quán của các đối tượng, ngay cả đối với các video có che khuất, thay đổi ngoại hình và chuyển động. Các điểm chuẩn như OTB [129], UAV [130], và VOT [131] thường được sử dụng cho nhiệm vụ này.

•Nhận dạng Lại: Nhận dạng Lại (ReID) là nhiệm vụ nhận dạng và khớp các cá nhân hoặc đối tượng qua các khung hình video khác nhau hoặc các góc nhìn camera. Các tập dữ liệu phổ biến trong ReID là Market-1501 [132], CUHK03 [133], MSMT17 [134], và DukeMTMC-reID [135].

•Phát hiện Saliency Video: Phát hiện saliency video nhằm xác định các vùng quan trọng nhất về mặt thị giác và thu hút sự chú ý trong video [136]. Nhiệm vụ này làm nổi bật các khu vực nổi bật do các yếu tố như chuyển động, độ tương phản và các đặc điểm độc đáo. Các tập dữ liệu liên quan đến nhiệm vụ này là DHF1K [137], Hollywood- [86], UCF-Sports [138], AVAD [139], Coutrot1 [140], Coutrot2 [141], ETMD [142], và SumMe [112].

•Phân đoạn Đối tượng Video: Phân đoạn đối tượng video nhằm phân chia một video thành các đoạn tương ứng với các đối tượng riêng lẻ, mô tả chính xác các ranh giới của chúng theo thời gian. YouTube-VOS [143] và DAVIS [144] là các tập dữ liệu liên quan đến nhiệm vụ này.

•Phân đoạn Thể hiện Video: Phân đoạn thể hiện video là nhiệm vụ xác định, phân đoạn và theo dõi từng thể hiện đối tượng độc đáo trong một video. YouTube-VIS [145] và Cityscapes-Seq [146] là hai điểm chuẩn phổ biến cho nhiệm vụ này.

•Phân đoạn Tham chiếu Đối tượng Video: Phân đoạn tham chiếu đối tượng video bao gồm việc phân đoạn các đối tượng cụ thể trong video dựa trên các mô tả ngôn ngữ. Nó xác định và cô lập các đối tượng được tham chiếu một cách chính xác qua các khung hình, nơi MeViS [147] là một điểm chuẩn phổ biến cho nhiệm vụ này.

•Grounding Không gian-Thời gian: Grounding không gian-thời gian nhằm xác định và định vị các đối tượng hoặc sự kiện cụ thể trong các chiều không gian và thời gian của video dựa trên một truy vấn đã cho. Các tập dữ liệu như Vid-STG [148], HC-STVG [149], Ego4D-MQ và Ego4D-NLQ [150] được đề xuất để hỗ trợ huấn luyện và thử nghiệm cho nhiệm vụ này.

B. Nền tảng cho LLM

Các mô hình ngôn ngữ được huấn luyện để học một phân phối xác suất kết hợp p(x1:L) với một chuỗi các token văn bản x1:L. Phân phối kết hợp này thường tương đương với tích của các xác suất có điều kiện được điều kiện trên mỗi token với quy tắc chuỗi:

p(x1:L) = ∏(i=1 to L) p(xi|x1:i-1), (1)

trong đó L là độ dài chuỗi.

Mô hình Ngôn ngữ Lớn (LLM) đề cập đến các mô hình ngôn ngữ với một số lượng lớn tham số, ví dụ, hàng tỷ. Kiến trúc của LLM bao gồm một tokenizer văn bản và nhiều lớp self-attention. LLM được huấn luyện theo cách teacher-forcing để dự đoán xác suất token tiếp theo, nơi quá trình tạo sử dụng mô hình autoregressive:

M(x1:i-1) = p(xi|x1:i-1), (2)

trong đó M đại diện cho một LLM.

Các chiến lược giải mã quyết định cách khai thác xác suất token tiếp theo và chọn token tiếp theo yt từ tập S của tất cả các token có thể có trong từ vựng, bao gồm các token đặc biệt như <SOS>, <EOS>, và <PAD>. Giải mã tham lam, chiến lược đơn giản nhất, chọn token có xác suất cao nhất, được hình thức hóa như:

xt = arg max(s∈S) log pM(s|x1:t-1). (3)

--- TRANG 6 ---
6

Bên cạnh các chiến lược xác định, các chiến lược lấy mẫu ngẫu nhiên chọn các token tiếp theo bằng cách sử dụng xác suất mô hình cũng phổ biến trong các ứng dụng thực tế. Các chiến lược này cung cấp đầu ra đa dạng và cho phép các phương pháp tự nhất quán.

Các mô hình ngôn ngữ lớn thường thể hiện các đặc điểm sau:

•Quy luật Mở rộng [151]: Với sự mở rộng đáng kể của kích thước mô hình (số lượng tham số), kích thước dữ liệu tiền huấn luyện và tài nguyên tính toán, hiệu suất của mô hình thể hiện một mô hình tăng trưởng đều đặn, có thể giúp các nhà nghiên cứu và kỹ sư dự đoán cải thiện hiệu suất hoặc đưa ra quyết định hiệu quả về thiết kế và huấn luyện mô hình.

•Khả năng Nổi bật [77]: Khi kích thước tham số và khối lượng dữ liệu huấn luyện cho một mô hình ngôn ngữ lớn vượt quá một độ lớn nhất định, một số khả năng mới xuất hiện, như học trong ngữ cảnh, tuân theo hướng dẫn và lý luận từng bước. Học trong ngữ cảnh cho phép mô hình học và đưa ra dự đoán dựa trên ngữ cảnh được cung cấp trong văn bản đầu vào mà không cần huấn luyện lại rõ ràng. Tuân theo hướng dẫn cho phép mô hình thực hiện các nhiệm vụ dựa trên các hướng dẫn ngôn ngữ tự nhiên. Lý luận từng bước, ví dụ chuỗi suy nghĩ (CoT), cho phép mô hình theo một chuỗi logic các bước để đi đến kết luận, đặc biệt hữu ích để giải quyết các vấn đề phức tạp.

LLM sở hữu khả năng tổng quát hóa rộng rãi có thể được áp dụng cho các nhiệm vụ downstream khác nhau, bao gồm các nhiệm vụ đa phương thức. Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) [76], [152]–[155] thường kết hợp các bộ mã hóa đa phương thức, các bộ căn chỉnh chéo phương thức và một cấu trúc lõi LLM. Bằng cách kết hợp các bộ mã hóa đa phương thức với LLM, MLLM xuất sắc trong việc tích hợp các ngữ cảnh thị giác và ngôn ngữ để tạo ra nội dung chi tiết.

III. VID-LLMS

Trong mục này, chúng tôi giới thiệu một phân loại mới về Vid-LLM, cung cấp một tổng quan toàn diện về phân loại của chúng. Sau đó, chúng tôi khám phá các chiến lược huấn luyện đa dạng giúp Vid-LLM đạt được khả năng của chúng.

A. Phân loại

Dựa trên phương pháp xử lý video đầu vào, chúng tôi phân loại Vid-LLM thành ba loại chính: Video Analyzer × LLM, Video Embedder × LLM, và (Analyzer + Embedder) × LLM. Mỗi danh mục đại diện cho một cách tiếp cận độc đáo để tích hợp xử lý video với LLM, như được minh họa trong Hình 4.

1) Video Analyzer × LLM: Video Analyzer được định nghĩa là một module lấy video đầu vào và xuất ra một phân tích của video, thường ở dạng văn bản, điều này tạo điều kiện cho việc xử lý LLM. Văn bản này có thể bao gồm mô tả video, mô tả video dày đặc (mô tả chi tiết của tất cả các sự kiện trong video với dấu thời gian), kết quả theo dõi đối tượng (nhãn, ID và hộp giới hạn của đối tượng), cũng như bản chép của các phương thức khác có trong video, như kết quả nhận dạng giọng nói từ ASR hoặc kết quả nhận dạng phụ đề từ OCR. Văn bản được tạo ra bởi Video Analyzer có thể được đưa trực tiếp vào LLM tiếp theo, chèn vào các mẫu được chuẩn bị trước trước khi được đưa vào LLM, hoặc chuyển đổi thành định dạng cơ sở dữ liệu tạm thời để LLM truy xuất sau này.

Hình 4. Hình minh họa ba framework chính cho Vid-LLM: (1) Video Analyzer × LLM, nơi các video analyzer chuyển đổi video đầu vào thành phân tích văn bản cho LLM; (2) Video Embedder × LLM, nơi các video embedder tạo ra các biểu diễn vector (embedding) cho LLM xử lý; (3) (Analyzer + Embedder) × LLM, một cách tiếp cận kết hợp cả analyzer và embedder để cung cấp cho LLM phân tích văn bản và embedding. Các mũi tên chỉ ra hướng luồng thông tin, với các mũi tên đứt nét đại diện cho các đường dẫn tùy chọn. Mũi tên xanh biểu thị luồng thông tin văn bản, trong khi mũi tên đỏ biểu thị embedding.

Đối với danh mục Video Analyzer × LLM, chúng tôi tiếp tục tạo ra hai danh mục phụ, LLM as Summarizer và LLM as Manager, dựa trên chức năng của LLM trong hệ thống Vid-LLM:

•LLM as Summarizer: Trong danh mục phụ này, chức năng chính của LLM là tóm tắt phân tích thu được từ Video Analyzer. Cách tiếp cận tóm tắt khác nhau dựa trên các lời nhắc được cung cấp cho LLM, từ văn bản tóm tắt và mô tả rất ngắn gọn đến các tóm tắt toàn diện để trả lời các câu hỏi cụ thể. Đáng chú ý, trong các hệ thống Video Analyzer × LLM as Summarizer, luồng thông tin thường là một chiều (xem Hình 4), với dữ liệu chảy từ video đến Video Analyzer rồi đến LLM, không có bất kỳ quá trình ngược nào. Các ví dụ về Vid-LLM trong danh mục Video Analyzer × LLM as Summarizer bao gồm: LaViLa [168], VLog [167], VAST [105], AntGPT [166], VIDOSC [165], Grounding-Prompter [164], LLoVi [163], Video ReCap [162], MVU [161], LangRepo [160], IG-VLM [159], MoReVQA [158], MM-Screenplayer [157],

--- TRANG 7 ---
7

Vid-LLMs
Video Analyzer × LLM
LLM as Summarizer
GIT-LLaVA [156], MM-Screenplayer [157], MoReVQA [158], IG-VLM [159], LangRepo [160], MVU [161], Video ReCap [162], LLoVi [163], Grounding-Prompter [164], VIDOSC [165], AntGPT [166], VAST [105], VLog [167], LaViLa [168]

LLM as Manager
DrVideo [169], OmAgent [170], LVNet [171], GPTSee [172], VideoTree [173], LAVAD [174], TraveLER [175], RAVA [176], SCHEMA [177], HuggingGPT [178], TV-TREES [179], VideoAgent [180], VideoAgent [181], VURF [182], KEPP [183], DoraemonGPT [184], Hawk [185], LifelongMemory [186], ProViQ [187], AssistGPT [188], Video ChatCaptioner [189], ChatVideo [189], ViperGPT [190]

Video Embedder × LLM
LLM as Text Decoder
Artemis [191], EmoLLM [192], FTFV-LLM [193], Flash-VStream [194], LLAVIDAL [195], LongVA [196], ShareGPT4Video [197], VIM [198], Video-SALMONN [199], VideoGPT+ [200], VideoLLaMA 2 [201], MotionLLM [202], VideoChat2 [203], Shotluck Holmes [204], VideoStreaming [205], VideoNarrator [206], TOPA [207], AutoAD III [208], GCG [209], LLaVA-Hound-DPO [210], RED-VILLM [211], Koala [212], LongVLM [213], MA-LMM [214], MiniGPT4-Video [215], Pegasus-v1 [216], PLLaVA [217], ST-LLM [218], COSMO [219], Tarsier [220], X-VARS [221], CAT [222], VideoLLM [223], InternVideo2 [224], MovieLLM [225], IVAwithLLM [226], LSTP [227], LVCHAT [228], OSCaR [229], Slot-VLM [230], AV-LLM [231], Emu2 [232], MMICT [233], VaQuitA [234], VILA [235], Vista-LLaMA [236], Chat-UniVi [237], LLaMA-VID [238], Video-LLaVA [239], LLaMA-VQA [240], MovieChat [241], LLMVA-GEBC [242], Macaw-LLM [243], VALLEY [244], Video-ChatGPT [99], Video-LLaMA [245], mPLUG-video [96], ChatBridge [246], Otter [247]

LLM as Regressor
Holmes-VAD [248], VideoLLM-online [249], VLM4HOI [250], V2Xum-LLaMA [100], AVicuna [120], Elysium [251], HawkEye [252], LITA [253], OmniViD [254], SeViLA [255], GroundingGPT [256], TimeChat [257], VTimeLLM [258]

LLM as Hidden Layer
VTG-LLM [259], VITRON [260], VTG-GPT [261], Momentor [262], VidDetours [263], OneLLM [264], GPT4Video [265]

(Analyzer + Embedder) × LLM
LLM as Manager
MM-VID [266]

LLM as Summarizer
SUM-shot [267]

LLM as Regressor
Vriptor [268], Merlin [269], VideoChat [270], Vid2Seq [271]

LLM as Text Decoder
Uni-AD [272], MM-narrator [273], Vamos [274], Auto-AD II [275]

LLM as Hidden Layer
PG-Video-LLaVA [276]

Hình 5. Phân loại Hiểu Video với Mô hình Ngôn ngữ Lớn (Vid-LLM), bao gồm Video Analyzer × LLM, Video Embedder × LLM và (Analyzer + Embedder) × LLM, và các danh mục phụ là LLM as Summarizer/Manager/Text Decoder/Regressor/Hidden Layer. Màu font chỉ ra độ phân giải của hiểu video được hỗ trợ bởi Vid-LLM: màu đen cho hiểu trừu tượng, màu đỏ cho hiểu thời gian, và màu xanh cho hiểu không gian-thời gian.

--- TRANG 8 ---
8

Hình 6. Bốn loại chiến lược tinh chỉnh Vid-LLM, được phân loại theo phương pháp huấn luyện cụ thể của chúng: tinh chỉnh hoàn toàn LLM, tinh chỉnh với các adapter kết nối, adapter chèn và các phương pháp kết hợp.

GIT-LLaVA [156], v.v.

•LLM as Manager: Trong danh mục phụ này, LLM chủ yếu điều phối hoạt động của toàn bộ hệ thống. Nó có thể chủ động tạo ra các lệnh để gọi các Video Analyzer khác nhau để tạo ra kết quả mong muốn cho người dùng, gọi Video Analyzer và xử lý thêm phân tích thu được trước khi trả lại cho người dùng, hoặc tham gia vào nhiều vòng tương tác với Video Analyzer. So với danh mục LLM as Summarizer, danh mục LLM as Manager linh hoạt hơn và có thể được phân biệt bởi độ phức tạp của luồng thông tin. Các ví dụ về Vid-LLM trong danh mục Video Analyzer × LLM as Manager bao gồm: ViperGPT [190], Video ChatCaptioner [277], ChatVideo [189], AssistGPT [188], HuggingGPT [178], Hawk [185], ProViQ [187], LifelongMemory [186], DoraemonGPT [184], KEPP [183], VURF [182], VideoAgent (by Stanford) [181], VideoAgent (by PKU) [180], TV-TREES [179], SCHEMA [177], RAVA [176], GPTSee [172], TraveLER [175], LAVAD [174], VideoTree [173], LVNet [171], OmAgent [170], DrVideo [169], v.v.

2) Video Embedder × LLM: Video Embedder thường đề cập đến một visual backbone/video encoder, như ViT hoặc CLIP, được sử dụng để chuyển đổi video đầu vào thành các biểu diễn vector, được gọi là video embedding hoặc video token. Một số Embedder mã hóa các phương thức khác trong video, như âm thanh (ví dụ, CLAP [278]), cũng được phân loại dưới Video Embedder ở đây (lưu ý rằng chúng tôi không coi tokenizer của LLM là một embedder). Khác với văn bản được tạo ra bởi Video Analyzer, các vector được tạo ra bởi Video Embedder không thể được sử dụng trực tiếp bởi LLM và thường yêu cầu một adapter để ánh xạ các embedding này từ không gian ngữ nghĩa của thị giác (hoặc các phương thức khác) sang không gian ngữ nghĩa văn bản của các token đầu vào của LLM.

Đối với danh mục Video Embedder × LLM, chúng tôi cũng phân loại chúng thành các danh mục phụ dựa trên chức năng của LLM trong hệ thống Vid-LLM:

•LLM as Text Decoder: Trong danh mục phụ này, LLM nhận embedding từ Video Embedder làm đầu vào và giải mã chúng thành đầu ra văn bản dựa trên các lời nhắc hoặc hướng dẫn. Các nhiệm vụ này thường không yêu cầu hiểu biết chi tiết hoặc định vị không gian-thời gian chính xác, tập trung chủ yếu vào QA hoặc mô tả tổng quát. Do đó, Vid-LLM hoạt động như một LLM tiêu chuẩn trong quá trình giải mã. Các ví dụ về Vid-LLM trong danh mục Video Embedder × LLM as Text Decoder bao gồm: VideoLLM [223], Otter [247], Video-LLaMA [245], Video-ChatGPT [99], VALLEY [244], Macaw-LLM [243], MovieChat [241], Video-LLaVA [239], Chat-UniVi [237], Vista-LLaMA [236], VILA [235], GPT4Video [265], MovieLLM [225], InternVideo2 [224], MiniGPT4-Video [215], VideoChat2 [203], VideoLLaMA 2 [201], v.v. Xem Hình 5 để có danh sách đầy đủ.

•LLM as Regressor: Trong danh mục phụ này, LLM nhận embedding từ Video Embedder làm đầu vào và, giống như Text Decoder, có thể xuất ra văn bản. Tuy nhiên, khác với Text Decoder, LLM as Regressor cũng có thể dự đoán các giá trị liên tục, như định vị timestamp trong video và tọa độ hộp giới hạn cho quỹ đạo đối tượng, hoạt động tương tự như một regressor thực hiện các nhiệm vụ hồi quy, mặc dù về cơ bản nó đang thực hiện phân loại. Các ví dụ về Vid-LLM trong danh mục Video Embedder × LLM as Regressor bao gồm: VTimeLLM [258], SeViLA [255], TimeChat [257], GroundingGPT [256], OmniViD [254], LITA [253], HawkEye [252], Elysium [251], AVicuna [120], V2Xum-LLaMA [100], VLM4HOI [250], VideoLLM-online [249], Holmes-VAD [248], v.v.

•LLM as Hidden Layer: Trong danh mục phụ này, LLM cũng nhận video embedding làm đầu vào nhưng không xuất ra văn bản trực tiếp. Thay vào đó, nó kết nối với một head cụ thể nhiệm vụ được thiết kế đặc biệt để thực hiện các nhiệm vụ hồi quy thực tế, như định vị thời gian sự kiện hoặc dự đoán hộp giới hạn đối tượng trong video, trong khi vẫn duy trì khả năng xuất ra văn bản của LLM. Các ví dụ về Vid-LLM trong danh mục Video Embedder × LLM as Hidden Layer bao gồm: GPT4Video [265], OneLLM [264], VidDetours [263], Momentor [262], VTG-GPT [261], VITRON [260], VTG-LLM [259], v.v.

--- TRANG 9 ---
9

3) (Analyzer + Embedder) × LLM: Danh mục Vid-LLM này tương đối hiếm. Như tên gọi, nó bao gồm việc đồng thời sử dụng Video Analyzer để có được phân tích văn bản của video và Video Embedder để mã hóa video thành embedding. LLM nhận cả hai loại đầu vào cùng với các lời nhắc/hướng dẫn khác và xuất ra phản hồi để hoàn thành nhiệm vụ. Các danh mục phụ ở đây có thể linh hoạt là bất kỳ danh mục nào trong số Summarizer/Manager/Text Decoder/Regressor/Hidden Layer. Vid-LLM trong danh mục (Analyzer + Embedder) × LLM bao gồm: Vid2Seq [271], VideoChat [270], MM-VID [266], Auto-AD II [275], Vamos [274], PG-Video-LLaVA [276], MM-Narrator [273], SUM-shot [267], Merlin [269], Uni-AD [272], Vriptor [268], v.v.

B. Chiến lược Huấn luyện cho Vid-LLM

1) Vid-LLM không cần Huấn luyện: Nhiều hệ thống Vid-LLM được xây dựng trên các LLM mạnh mẽ với khả năng zero-shot, học trong ngữ cảnh và Chuỗi suy nghĩ mạnh mẽ. Các hệ thống này không yêu cầu huấn luyện trong các tham số của LLM hoặc các module khác. Hầu hết Vid-LLM trong danh mục Video Analyzer × LLM không cần huấn luyện vì thông tin từ video và các phương thức đi kèm khác đã được phân tích thành văn bản. Tại thời điểm này, nhiệm vụ hiểu video đã được chuyển đổi thành nhiệm vụ hiểu văn bản. Vì LLM có thể thống nhất hầu hết tất cả các nhiệm vụ NLP thành các nhiệm vụ tạo sinh, chúng cũng có thể xử lý nhiều nhiệm vụ hiểu video. SlowFast-LLaVA [279] là một Video LLM không cần huấn luyện sử dụng thiết kế đầu vào hai luồng để nắm bắt cả ngữ nghĩa không gian và ngữ cảnh thời gian mà không cần tinh chỉnh và thể hiện khả năng qua các điểm chuẩn hiểu video khác nhau.

2) Tinh chỉnh Vid-LLM: Trái ngược với hầu hết Vid-LLM trong danh mục Video Analyzer × LLM không cần huấn luyện, gần như tất cả Vid-LLM trong danh mục Video Embedder × LLM trải qua tinh chỉnh. Các phương pháp phổ biến để tinh chỉnh Vid-LLM được phân loại dựa trên các loại adapter được sử dụng trong quá trình tinh chỉnh thành bốn loại chính: Tinh chỉnh Hoàn toàn LLM, Tinh chỉnh Adapter Kết nối, Tinh chỉnh Adapter Chèn và Tinh chỉnh với Adapter Kết hợp. Adapter là một module nhỏ, có thể huấn luyện được thêm vào một mô hình lớn để tinh chỉnh. Bằng cách chỉ cập nhật các tham số của các module này, mô hình có thể thích ứng với các nhiệm vụ cụ thể mà không thay đổi toàn bộ tham số của mô hình, đạt được cập nhật tham số hiệu quả và thích ứng nhiệm vụ trong khi tiết kiệm tài nguyên tính toán. Minh họa của từng loại được hiển thị trong Hình 6.

•Tinh chỉnh Hoàn toàn LLM: Phương pháp tinh chỉnh này không sử dụng bất kỳ adapter nào mà thay vào đó sử dụng huấn luyện có giám sát với tốc độ học thấp hơn, cập nhật tất cả các tham số trong LLM. Phương pháp này cho phép Vid-LLM thích ứng hoàn toàn với nhiệm vụ tương ứng và đạt hiệu suất tốt, đặc biệt khi nhiệm vụ mục tiêu khá khác so với các nhiệm vụ tiền huấn luyện. Đối với Vid-LLM end-to-end, đặc biệt là những Vid-LLM trong danh mục Video Embedder × LLM, Video Embedder cũng có thể được tinh chỉnh để học tổng hợp hơn. Tuy nhiên, phương pháp này tiêu tốn nhiều tài nguyên tính toán hơn so với các phương pháp tinh chỉnh dựa trên adapter và có thể làm suy giảm các khả năng vốn có của LLM, như zero-shot và học trong ngữ cảnh. Vid-LLM áp dụng Tinh chỉnh Hoàn toàn LLM bao gồm AV-LLM [231] và Vid2Seq [271]. Trong [231], có cả phiên bản tinh chỉnh hoàn toàn và tinh chỉnh adapter của Vid-LLM, và hiệu suất của phiên bản trước tốt hơn phiên bản sau.

•Tinh chỉnh Adapter Kết nối: Ở đây, thuật ngữ "Kết nối" đề cập đến các adapter kết nối Video Embedder và LLM từ bên ngoài, cho phép thông tin từ video chảy vào LLM thông qua Adapter Kết nối. Như được minh họa trong Hình 6, trong quá trình huấn luyện, các tham số của cả Video Embedder và LLM được đóng băng, và chỉ các tham số của Adapter Kết nối được cập nhật. Các Adapter Kết nối phổ biến bao gồm MLP/Linear Layer và Q-former [282], các kết hợp của chúng, v.v., có chức năng chính là ánh xạ video embedding từ không gian ngữ nghĩa thị giác sang không gian ngữ nghĩa văn bản của các token đầu vào LLM (tức là căn chỉnh phương thức). Thông thường, việc tinh chỉnh chỉ Adapter Kết nối không làm thay đổi hành vi vốn có của LLM.

•Tinh chỉnh Adapter Chèn: Như tên gọi, Adapter Chèn được chèn vào bản thân LLM. Tương tự như việc sử dụng Adapter Kết nối, trong quá trình huấn luyện, các tham số của Video Embedder và LLM được đóng băng, và chỉ các tham số của Adapter Chèn được cập nhật. Adapter Chèn, thường dựa trên LoRA, ảnh hưởng đến hành vi của LLM vì chúng được thêm vào các tham số LLM hiện có. Loại adapter này hầu như luôn có mặt trong Vid-LLM được phân loại là Video Embedder × LLM as Regressor và Video Embedder × LLM as Hidden Layer, vì các loại Vid-LLM này yêu cầu thay đổi trong hành vi của LLM, như xuất ra các giá trị dự đoán liên tục.

•Tinh chỉnh với Adapter Kết hợp: Nhiều Vid-LLM sử dụng kết hợp của Adapter Kết nối và Adapter Chèn để đạt được cả căn chỉnh phương thức và thay đổi trong hành vi vốn có của LLM. Vid-LLM sử dụng Adapter Kết hợp thường sử dụng tinh chỉnh đa giai đoạn. Một cách tiếp cận phổ biến là chỉ tinh chỉnh Adapter Kết nối trong giai đoạn đầu tiên để căn chỉnh phương thức. Trong giai đoạn thứ hai, Adapter Kết nối đã được tinh chỉnh được đóng băng, nhiệm vụ huấn luyện (từ nhiệm vụ căn chỉnh sang nhiệm vụ mục tiêu) và dữ liệu huấn luyện (từ dữ liệu được sử dụng để căn chỉnh phương thức sang dữ liệu yêu cầu cho nhiệm vụ mục tiêu) được thay đổi, và chỉ các tham số của Adapter Chèn được cập nhật. Cũng có các cách tiếp cận một giai đoạn nơi cả Adapter Kết nối và Adapter Chèn đều được cập nhật đồng thời.

IV. ĐIỂM CHUẨN VÀ ĐÁNH GIÁ

Mục này cung cấp một tổng quan về các phương pháp đánh giá cho các mô hình trả lời câu hỏi video và các nhiệm vụ liên quan, được phân loại thành ba loại: đánh giá đóng, đánh giá mở và các phương pháp đánh giá khác, được hiển thị trong Bảng III. Đánh giá đóng dựa trên các câu hỏi có câu trả lời hoặc định dạng được xác định trước, bao gồm câu hỏi trắc nghiệm và các định dạng có cấu trúc cho phép chấm điểm đơn giản. Đánh giá mở bao gồm các câu hỏi không có tùy chọn câu trả lời được xác định trước, thường yêu cầu các

--- TRANG 10 ---
10

BẢNG I
SO SÁNH CÁC MÔ HÌNH HIỂU VIDEO VỚI MÔ HÌNH NGÔN NGỮ LỚN, SẮP XẾP THEO NGÀY PHÁT HÀNH. BẢNG TRÌNH BÀY CÁC CHI TIẾT CHÍNH CHO MỖI PHƯƠNG PHÁP, BAO GỒM SỐ KHUNG HÌNH HUẤN LUYỆN, VIDEO EMBEDDER, SỬ DỤNG THÔNG TIN ÂM THANH, PHƯƠNG PHÁP THÍCH ỨNG MÔ HÌNH, TÀI NGUYÊN TÍNH TOÁN, MÔ HÌNH NGÔN NGỮ LỚN CỤ THỂ ĐƯỢC SỬ DỤNG VÀ SỐ THAM SỐ TƯƠNG ỨNG. CÁC MỤC ĐƯỢC ĐÁNH DẤU BẰNG DẤU GẠCH NGANG ("-") CHỈ RA CÁC CHI TIẾT KHÔNG ĐƯỢC TIẾT LỘ TRONG CÁC BÀI BÁO TƯƠNG ỨNG.

[THIS IS TABLE: A detailed comparison table of video understanding models with large language models, sorted by release date. The table contains columns for Model, #Frame, Video Embedder, Sound, Speech, Adapter, Hardware, LLM, LLM Size, and Date. It lists various models from 2022-2024 with their specifications.]

--- TRANG 11 ---
11

BẢNG II
TIẾP THEO CỦA BẢNG I.

[THIS IS TABLE: A continuation table with multiple columns showing Model, #Frame, Video Embedder, Sound, Speech, Adapter, Hardware, LLM, LLM Size, and Date information for various video models from 2024. The table contains detailed technical specifications for each model.]

phương pháp chấm điểm tinh vi hơn, bao gồm đánh giá dựa trên LLM. Các phương pháp đánh giá khác xử lý các khả năng hiểu video chuyên biệt như lý luận thời gian/không gian-thời gian.

A. Đánh giá Đóng

Đánh giá đóng sử dụng các câu trả lời hoặc định dạng có cấu trúc được xác định trước [283]. Chúng bao gồm câu hỏi trắc nghiệm (TVQA [109], How2QA [284], STAR [285]) và câu hỏi với định dạng có cấu trúc để so sánh trực tiếp với truth chuẩn (MSRVTT-QA [93], MSVD-QA [93], MVBench [203]). Hiệu suất trắc nghiệm được đánh giá thông qua tỷ lệ phần trăm chính xác, trong khi các định dạng có cấu trúc sử dụng các chỉ số như CIDEr [286], METEOR [287], ROUGE [288] và SPICE [289] để so sánh dự đoán với truth chuẩn. Các điểm chuẩn đáng chú ý bao gồm MSRVTT-QA [93], TVQA [109], MVBench [203], EgoSchema [290] và Video-MME [291]. Mỗi điểm chuẩn nhắm vào các khía cạnh hiểu video khác nhau: TGIF-QA [107] tập trung vào nhận dạng hành động, chuyển đổi trạng thái, QA cấp khung hình và đếm; ActivityNet-QA [110] bao gồm các chiều chuyển động, không gian, thời gian và hình thức tự do; Vid-Composition [292] nhấn mạnh lý luận tổng hợp; trong khi NExT-QA [111] và MLVU [293] bao gồm lý luận nhân quả và hành động thời gian. Những loại câu hỏi đa dạng này kiểm tra các khả năng lý luận khác nhau, mặc dù nhiều điểm chuẩn vẫn thể hiện thiên vị miền đối với các tình huống phổ biến và thiếu đa dạng trong các sự kiện hiếm hoặc bối cảnh bất thường.

B. Đánh giá Mở

Đánh giá mở bao gồm các câu hỏi không có tùy chọn hoặc định dạng có cấu trúc được xác định trước. Mặc dù các câu trả lời ground-truth phục vụ như tài liệu tham khảo, các phương pháp chấm điểm tinh vi hơn so với lựa chọn tùy chọn hoặc khớp chuỗi. Các mô hình GPT-3.5/4 thường đánh giá dự đoán bằng cách so sánh chúng với câu trả lời tham khảo. Các điểm chuẩn mở đáng chú ý bao gồm MovieChat-1K [241], MLVU [293], NExT-QA [111], VELOCITI [304] và EAGLE [305]. Chúng yêu cầu phản hồi phức tạp hơn thể hiện lý luận sâu sắc hơn. CinePile [298] kết hợp các nhiệm vụ phân tích như động lực nhân vật và phân tích tường thuật. Các phương pháp đánh giá dựa trên GPT phổ biến nhất, được đề xuất trong [99], là Đánh giá Video QA Zero-shot Mở và Benchmarking Hiệu suất Sinh tạo dựa trên Video. So sánh hiệu suất của Vid-LLM trên các chỉ số này được hiển thị trong Bảng V. Các điểm chuẩn đóng ban đầu như MSRVTT-QA [93], MSVD-QA [93], TGIF-QA [107] và ActivityNet-QA [110] có thể được tái sử dụng làm mở trong đánh giá dựa trên GPT, vì LLM tạo ra các phản hồi tự do mà các mô hình GPT so sánh với tài liệu tham khảo. Các phương pháp này có hạn chế: điểm đánh giá thay đổi theo cập nhật phiên bản GPT, làm cho việc so sánh giữa các nghiên cứu khó khăn; kết quả phụ thuộc rất nhiều vào kỹ thuật prompt engineering; và các đánh giá viên LLM có thể ưu tiên các phản hồi tương tự như mô hình tạo sinh của chúng thay vì đánh giá chất lượng một cách khách quan.

--- TRANG 12 ---
12

BẢNG III
SO SÁNH CÁC ĐIỂM CHUẨN KHÁC NHAU BAO GỒM MỘT SỐ KHÍA CẠNH QUAN TRỌNG: TỔNG SỐ VIDEO, SỐ CLIP, THỜI LƯỢNG TRUNG BÌNH CỦA VIDEO, SỐ CẶP QA VÀ NỘI DUNG VIDEO.

[THIS IS TABLE: A detailed comparison table of various benchmarks with columns for Benchmark, #Videos, #Clips, Len.(s), Video Content, #QA Pairs, and Question Type. The table contains information about multiple video understanding benchmarks from 2017-2024.]

C. Các Đánh giá Khác

Các điểm chuẩn khác đánh giá hiểu thời gian và không gian-thời gian chi tiết. Mô tả dày đặc tạo ra [122]–[126] các mô tả chi tiết cho nhiều sự kiện/đối tượng video, sử dụng các chỉ số BLEU, METEOR và CIDEr đánh giá cả định vị thời gian và độ chính xác mô tả. Hiệu suất của Vid-LLM trong mô tả video dày đặc trên ActivityNet Captions [127] được hiển thị trong Bảng IV. Một số Vid-LLM đã đạt được hiệu suất tương đương với các mô hình cụ thể nhiệm vụ truyền thống trong mô tả video dày đặc. Grounding thời gian video định vị các khoảnh khắc cụ thể dựa trên truy vấn văn bản, được đánh giá bằng tIoU và Recall@K. Grounding không gian-thời gian mở rộng điều này để định vị trong cả không gian và thời gian, được đánh giá thông qua IoU không gian-thời gian và mAP. Theo dõi đối tượng [251], [256] theo dõi đối tượng qua các khung hình, được đánh giá bằng độ chính xác, tỷ lệ thành công và độ chính xác theo dõi. Phát hiện saliency video [306] xác định các vùng nổi bật trực quan, được đánh giá với AUC-J, NSS, v.v. Các nhiệm vụ này dựa trên chú thích thời gian hoặc không gian-thời gian làm ground-truth, với các chỉ số như IoU, Recall@K và mAP được áp dụng rộng rãi. Đánh giá con người cũng được sử dụng cho các khía cạnh chủ quan, mặc dù điều này tốn nhiều lao động và thời gian.

Về đánh giá định tính, một số cách tiếp cận có thể đánh giá hiệu quả hiệu suất của Vid-LLM bên cạnh các chỉ số số lượng. Phân tích lỗi [203], [307] cho QA mở và so sánh sự khác biệt [120], [258] giữa đầu ra mô hình và chú thích ground truth (ví dụ, khoảng thời gian) cho hiểu thời gian/không gian-thời gian cung cấp cái nhìn sâu sắc về các hạn chế của mô hình. Trực quan hóa attention [308] tiết lộ những yếu tố thị giác nào mô hình ưu tiên khi tạo ra phản hồi. Tự giải thích [292], [307], nơi mô hình biện minh cho câu trả lời của chúng đối với các điểm chuẩn đóng, mang lại cái nhìn sâu sắc có giá trị về quá trình lý luận và các quan niệm sai lầm tiềm ẩn. Các nghiên cứu con người, mặc dù tốn nhiều tài nguyên, vẫn hữu ích trong việc tìm các mô hình phản ánh sở thích con người.

BẢNG IV
SO SÁNH VID-LLM VÀ CÁC MÔ HÌNH THÔNG THƯỜNG (KHÔNG DỰA TRÊN LLM) TRÊN CÁC MÔ HÌNH MÔ TẢ VIDEO DÀY ĐẶC TRÊN TẬP DỮ LIỆU ACTIVITYNET CAPTIONS.

[THIS IS TABLE: A comparison table showing performance metrics (CIDEr, SODA_c, METEOR, F1) for various Non-LLM-based and LLM-based models on dense video captioning tasks.]

D. Phân tích Hiệu suất Mô hình

Phân tích mối tương quan giữa các thuộc tính mô hình và hiệu suất điểm chuẩn tiết lộ một số yếu tố chính thúc đẩy cải thiện gần đây trong Vid-LLM. Từ Bảng IV và V, chúng tôi quan sát thấy rằng các mô hình được xây dựng trên các LLM nền tảng lớn hơn và gần đây hơn (ví dụ, IG-VLM với 34B tham số) luôn vượt trội so với các đối tác nhỏ hơn, đặc biệt trong các nhiệm vụ VideoQA zero-shot. Các mô hình sử dụng các visual embedder mạnh hơn như kiến trúc EVA-CLIP hoặc ViT-G (đáng chú ý trong PLLaVA, IG-VLM và Video LLaMA 2) thể hiện hiệu suất vượt trội trên cả điểm chuẩn mô tả dày đặc và QA. Chiến lược lấy mẫu khung hình cũng ảnh hưởng đáng kể đến kết quả, với các mô hình hiệu suất cao trên các nhiệm vụ thời gian (như VTimeLLM, AVicuna và ST-LLM) thường xử lý nhiều khung hình hơn (100+) so với các mô hình hiểu tổng quát, trong khi các cơ chế thích ứng tinh vi hơn ngoài các lớp projection đơn giản (như Q-former hoặc cross-attention) góp phần vào

--- TRANG 13 ---
13

BẢNG V
BẢNG NÀY SO SÁNH TOÀN DIỆN CÁC VID-LLM KHÁC NHAU QUA NHIỀU ĐIỂM CHUẨN TRẢ LỜI CÂU HỎI VIDEO ZERO-SHOT MỞ VÀ BENCHMARKING HIỆU SUẤT SINH TẠO DỰA TRÊN VIDEO. NÓ BAO GỒM CÁC CHỈ SỐ DỰA TRÊN GPT CHO CÁC TẬP DỮ LIỆU MSVD-QA, MSRVTT-QA VÀ ACTIVITYNET-QA, CŨNG NHƯ ĐIỂM SỐ CHO TÍNH CHÍNH XÁC THÔNG TIN, HƯỚNG CHI TIẾT, HIỂU BIẾT NGỮ CẢNH, HIỂU BIẾT THỜI GIAN VÀ CÁC KHÍA CẠNH NHẤT QUÁN TRONG HIỆU SUẤT SINH TẠO DỰA TRÊN VIDEO.

[THIS IS TABLE: A comprehensive comparison table showing various Vid-LLM models with their performance scores across different metrics including Open-end Zero-shot Video QA Evaluation and Video-based Generative Performance Benchmarking. The table has multiple columns showing scores for different models like GPT4-V, Video-LLaMA, etc.]

hiểu biết ngữ cảnh tốt hơn. Cải thiện hiệu suất xuất phát từ sự kết hợp của các mô hình nền tảng mạnh hơn, bộ mã hóa thị giác tốt hơn, mô hình hóa thời gian phù hợp và kiến trúc cầu nối tinh vi hơn thay vì bất kỳ đổi mới đơn lẻ nào.

V. ỨNG DỤNG VÀ HƯỚNG TƯƠNG LAI

A. Các Tình huống Ứng dụng

Vid-LLM đã cách mạng hóa các lĩnh vực khác nhau bằng cách cho phép các khả năng xử lý video và ngôn ngữ tiên tiến. Chúng tôi phác thảo các ứng dụng đa dạng của chúng, thể hiện tác động rộng rãi và biến đổi của Vid-LLM trên các ngành công nghiệp.

1) Truyền thông và Giải trí:

•Nền tảng Video Trực tuyến và Truy xuất Thông tin Đa phương tiện: Vid-LLM tăng cường đáng kể thuật toán tìm kiếm [318], tạo ra các đề xuất video nhận thức ngữ cảnh [319], và hỗ trợ trong các nhiệm vụ ngôn ngữ tự nhiên như tạo phụ đề và dịch thuật [271], góp phần vào các nền tảng video trực tuyến và hệ thống truy xuất đa phương tiện. Khả năng của chúng trong việc phân tích video để truy xuất từ khóa cụ thể [168], [320], [321] cải thiện hệ thống đề xuất thông minh. Trong các lĩnh vực đa phương tiện, nó kết hợp video trong các miền như âm nhạc [322], avatar [323]–[326], và cảnh [327], để hỗ trợ tạo nội dung.

•Tóm tắt và Chỉnh sửa Video: Vid-LLM là không thể thiếu trong việc tạo ra các tóm tắt ngắn gọn của nội dung video [328], phân tích các yếu tố thị giác và âm thanh để trích xuất các đặc trưng chính cho các tóm tắt nhận thức ngữ cảnh. Chúng cũng góp phần vào lĩnh vực chỉnh sửa video, như được đề cập trong tài liệu hiện có [75] và chỉnh sửa quảng cáo [114].

2) Hệ thống Tương tác và Lấy người dùng làm Trung tâm:

•Giáo dục Ảo, Khả năng Tiếp cận và Ngôn ngữ Ký hiệu: Vid-LLM phục vụ như gia sư ảo trong giáo dục, phân tích video giảng dạy cho môi trường học tập tương tác [329]. Chúng cũng tạo điều kiện dịch ngôn ngữ ký hiệu thành ngôn ngữ nói hoặc văn bản [330], [331], cải thiện khả năng tiếp cận cho người khiếm thính và khó nghe.

•Trò chơi Tương tác và Môi trường Ảo: Trong ngành công nghiệp trò chơi, Vid-LLM đóng vai trò quan trọng trong việc tạo ra các cuộc đối thoại và cốt truyện động, cũng như hỗ trợ tạo ra nội dung thủ tục, như nhiệm vụ và văn bản trong game [332], [333]. Chúng cũng cung cấp năng lượng cho chatbot dịch vụ khách hàng [334], [335]. Bổ sung, trong AR/VR/XR, Vid-LLM góp phần vào việc tạo ra nội dung tường thuật động, tăng cường sự đắm chìm của người dùng [336]–[339].

•Tương tác Người-Máy Nhận thức Trạng thái và Lập kế hoạch Robot: Trong lĩnh vực tương tác người-máy, Vid-LLM phân tích video người dùng để nhận biết ngữ cảnh và cung cấp hỗ trợ tùy chỉnh, như được nêu bật trong Bi et al. [340]. Các hình thức tương tác cũng bao gồm hiểu nội dung video như mô tả video [155], [341], [342]. Đồng thời, trong điều hướng robot tự động, phương pháp Say-Plan [343] tích hợp LLM với đồ thị cảnh 3D để cho phép robot diễn giải và điều hướng không gian phức tạp trong các tòa nhà lớn.

3) Ứng dụng Chăm sóc Sức khỏe và An ninh:

•Đổi mới Chăm sóc Sức khỏe: Trong lĩnh vực chăm sóc sức khỏe, Vid-LLM đóng vai trò quan trọng trong việc xử lý và diễn giải tài liệu y tế, hỗ trợ trong các quy trình chẩn đoán và giáo dục [344]–[347], và cung cấp hỗ trợ quyết định cho các chuyên gia chăm sóc sức khỏe. Chúng được sử dụng trong các công cụ tương tác bệnh nhân, như chatbot để đánh giá triệu chứng và giải quyết các truy vấn liên quan đến sức khỏe, do đó cải thiện chăm sóc bệnh nhân

--- TRANG 14 ---
14

và khả năng tiếp cận thông tin [348].

•An ninh, Giám sát và An ninh Mạng: Vid-LLM rất quan trọng trong an ninh và bảo vệ, phân tích giao tiếp để tìm các mối đe dọa tiềm ẩn [349], [350] và phát hiện các mô hình bất thường trong dữ liệu [351], [352]. Trong phân tích video giám sát, chúng xác định các hành vi đáng ngờ, giúp thực thi pháp luật [353]. Vai trò của chúng trong an ninh mạng bao gồm xác định các nỗ lực lừa đảo và góp phần vào phân tích pháp y bằng cách tóm tắt các văn bản liên quan đến vụ án [354]. Chúng cũng có thể cải thiện đếm đám đông video [355] cho các ứng dụng an ninh.

•Xe Tự động: Trong xe tự động, Vid-LLM có thể xử lý đầu vào ngôn ngữ để tương tác [356], hỗ trợ trong việc hiểu biển báo đường và hướng dẫn [247], [357], và cải thiện giao diện người dùng cho hệ thống điều khiển xe [356], tăng cường an toàn và trải nghiệm người dùng.

4) Các Ứng dụng Khác: Vid-LLM cung cấp các ứng dụng có giá trị ngoài những gì đã thảo luận trước đây. Trong nghiên cứu tạo video [358]–[360], Vid-LLM có thể đánh giá hiệu suất mô hình, tinh chỉnh lời nhắc văn bản và cung cấp khả năng lý luận phản ánh tốt hơn ý định con người. Bổ sung, Vid-LLM cho thấy tiềm năng trong các môi trường hạn chế tài nguyên thông qua các ứng dụng edge computing [361]–[363] và có thể tăng cường các hệ thống phân tán bảo vệ quyền riêng tư thông qua các framework học liên kết [364]–[366].

B. Hướng Tương lai

Mặc dù tăng cường nhiều nhiệm vụ downstream, Vid-LLM đối mặt với một số thách thức trong hiểu video thực tế:

1) Hiểu Video Chi tiết hơn: Hiểu chi tiết vẫn đầy thách thức do các tập dữ liệu hạn chế, nghiên cứu không đầy đủ và nhu cầu tính toán cao. Phân tích từng khung hình tăng tải tính toán trong khi nắm bắt thông tin không gian-thời gian. Hiểu ngữ nghĩa sâu hơn (cảm xúc, động lực cảnh) khó khăn hơn, mặc dù căn chỉnh văn bản-video thông qua LLM mang lại hứa hẹn [292].

2) Hiểu Video Dài: Thời lượng kéo dài của video dài làm phức tạp việc phân tích, đặc biệt trong việc hiểu các sự kiện theo thời gian. Do đó, việc xác định các sự kiện chính và duy trì sự chú ý trong video dài là khó khăn [186], [196], [213]. Cần các cơ chế hiệu quả để phát hiện và làm nổi bật các phần quan trọng, đặc biệt trong video giàu nội dung hoặc cốt truyện phức tạp.

3) Hiểu Video Đa phương thức: Hiểu video đa phương thức yêu cầu tích hợp các loại dữ liệu khác nhau, như thị giác, âm thanh và văn bản, để hiểu video tốt hơn [120], [367]. Căn chỉnh các dữ liệu này, đặc biệt về đồng bộ hóa không gian và thời gian, là đặc biệt quan trọng. Lĩnh vực này thiếu nghiên cứu liên quan và gặp khó khăn về khan hiếm tập dữ liệu. Lĩnh vực thiếu nghiên cứu và tập dữ liệu, với thách thức trong việc đảm bảo chú thích dữ liệu chất lượng cao.

4) Ảo giác trong Video LLM: Ảo giác xảy ra khi mô hình tạo ra các phản hồi không liên quan đến tài liệu nguồn [368], gây ra bởi trích xuất đặc trưng không đầy đủ, ảnh hưởng của ngữ cảnh video, khoảng cách miền giữa thị giác và ngôn ngữ, và các ảo giác LLM vốn có. Giải pháp bao gồm các chiến lược post-training [210], hiểu ngữ cảnh không gian-thời gian tăng cường và hợp tác tiềm ẩn thị giác-ngôn ngữ.

5) Triển khai Công nghiệp và Khả năng Mở rộng: Các chiến lược triển khai hiệu quả [212], [213], [362], [369]–[371] bao gồm nén mô hình, hợp nhất token, tinh chỉnh cụ thể miền, kiến trúc mô-đun, bộ nhớ đệm hiệu quả và các framework tích hợp tiêu chuẩn, cân bằng hiệu quả với hiệu suất cho các hệ thống công nghiệp.

C. Ý nghĩa Đạo đức

Các ý nghĩa đạo đức của Vid-LLM tập trung vào quyền riêng tư, bảo mật dữ liệu và khả năng lạm dụng. Các mô hình này thực hiện các nhiệm vụ như phân tích sự tham gia video, chuyển đổi, tóm tắt và mô tả, yêu cầu truy cập vào nội dung nhạy cảm. Điều này nâng cao rủi ro về quyền riêng tư, vì dữ liệu video có thể chứa thông tin riêng tư hoặc bí mật có thể bị lộ mà không có sự đồng ý thích hợp. Ngoài ra, Vid-LLM có thể bị lạm dụng cho giám sát hoặc tạo ra nội dung gây hiểu lầm. Thiên vị là một mối quan ngại khác, đặc biệt nếu dữ liệu huấn luyện thiếu đa dạng. Giải quyết các vấn đề này yêu cầu quản trị dữ liệu mạnh mẽ, cơ chế đồng ý và triển khai đạo đức để ưu tiên quyền riêng tư và công bằng.

VI. KẾT LUẬN

Khảo sát này đã xem xét việc tích hợp LLM trong hiểu video, điều này đã cho phép các khả năng xử lý tinh vi và linh hoạt hơn ngoài các phương pháp truyền thống. Chúng tôi đã phân loại các cách tiếp cận hiện tại thành ba loại chính: Video Analyzer × LLM, Video Embedder × LLM, và (Analyzer + Embedder) × LLM, với các phân loại phụ dựa trên vai trò chức năng LLM: Summarizer, Manager, Text Decoder, Regressor, và Hidden Layer. Vid-LLM thể hiện khả năng trong lý luận đa độ phân giải từ phân tích trừu tượng đến không gian-thời gian, cho thấy tiềm năng qua tóm tắt video, mô tả, trả lời câu hỏi và các ứng dụng khác. Mặc dù có tiến bộ, các hạn chế vẫn còn trong các chỉ số đánh giá, xử lý video dài và căn chỉnh phương thức thị giác-văn bản. Nghiên cứu tương lai sẽ giải quyết các thách thức này thông qua các chiến lược huấn luyện hiệu quả hơn, khả năng mở rộng Vid-LLM được cải thiện, kiến trúc đổi mới cho tích hợp đa phương thức, hiểu video dài tăng cường và các phương pháp để giảm thiểu ảo giác. Mở rộng tập dữ liệu và điểm chuẩn sẽ là quan trọng để thúc đẩy hiểu video với LLM.

TÀI LIỆU THAM KHẢO

[1] T. Lindeberg, "Scale invariant feature transform," 2012.
[2] H. Bay et al., "Speeded-up robust features (surf)," Computer vision and image understanding, vol. 110, no. 3, pp. 346–359, 2008.
[3] N. Dalal and B. Triggs, "Histograms of oriented gradients for human detection," in 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05), vol. 1. Ieee, 2005, pp. 886–893.
[4] A. Sobral and A. Vacavant, "A comprehensive review of background subtraction algorithms evaluated with synthetic and real videos," Computer Vision and Image Understanding, vol. 122, pp. 4–21, 2014.
[5] Z. Tu, H. Li et al., "Optical flow for video super-resolution: a survey," Artificial Intelligence Review, vol. 55, no. 8, pp. 6505–6546, 2022.
[6] H. Wang and C. Schmid, "Action recognition with improved trajectories," in Proceedings of the IEEE international conference on computer vision, 2013, pp. 3551–3558.
[7] Z. Shu, K. Yun, and D. Samaras, "Action detection with improved dense trajectories and sliding window," in Computer Vision-ECCV 2014 Workshops: Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I 13. Springer, 2015, pp. 541–551.

--- TRANG 15 ---
15

[8] X. Liu and T. Cheng, "Video-based face recognition using adaptive hidden markov models," in 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings., vol. 1. IEEE, 2003, pp. I–I.
[9] H. Sidenbladh, "Detecting human motion with support vector machines," in Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004., vol. 2. IEEE, 2004, pp. 188–191.
[10] Y. Yuan, Q.-B. Song, and J.-Y. Shen, "Automatic video classification using decision tree method," in Proceedings. International Conference on Machine Learning and Cybernetics, vol. 3. IEEE, 2002, pp. 1153–1157.
[11] A. B. Chan and N. Vasconcelos, "Modeling, clustering, and segmenting video with mixtures of dynamic textures," IEEE transactions on pattern analysis and machine intelligence, vol. 30, no. 5, pp. 909–926, 2008.
[12] T. Bouwmans and E. H. Zahzah, "Robust pca via principal component pursuit: A review for a comparative evaluation in video surveillance," Computer Vision and Image Understanding, vol. 122, pp. 22–34, 2014.
[13] L. Hazelhoff et al., "Video-based fall detection in the home using principal component analysis," in Advanced Concepts for Intelligent Vision Systems: 10th International Conference, ACIVS 2008, Juan-les-Pins, France, October 20-24, 2008. Proceedings 10. Springer, 2008, pp. 298–309.
[14] A. Karpathy et al., "Large-scale video classification with convolutional neural networks," in CVPR, 2014.
[15] S. Ji et al., "3d convolutional neural networks for human action recognition," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 1, pp. 221–231, 2013.
[16] C. Feichtenhofer et al., "Convolutional two-stream network fusion for video action recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 1933–1941.
[17] J. Yue-Hei Ng et al., "Beyond short snippets: Deep networks for video classification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4694–4702.
[18] L. Wang, Y. Xiong et al., "Temporal segment networks: Towards good practices for deep action recognition," in European conference on computer vision. Springer, 2016, pp. 20–36.
[19] M. Sekma et al., "Human action recognition based on multi-layer fisher vector encoding method," Pattern Recognition Letters, vol. 65, pp. 37–43, 2015.
[20] A. Diba, V. Sharma, and L. Van Gool, "Deep temporal linear encoding networks," in Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2017, pp. 2329–2338.
[21] I. Mironicǎ et al., "A modified vector of locally aggregated descriptors approach for fast video classification," Multimedia Tools and Applications, vol. 75, pp. 9045–9072, 2016.
[22] H. Li, L. Zhang et al., "Transvlad: Focusing on locally aggregated descriptors for few-shot learning," in European Conference on Computer Vision. Springer, 2022, pp. 524–540.
[23] K. Soomro, A. R. Zamir, and M. Shah, "Ucf101: A dataset of 101 human actions classes from videos in the wild," in Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
[24] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, "Hmdb: A large video database for human motion recognition," in Proceedings of the International Conference on Computer Vision (ICCV), 2011.
[25] D. Tran, L. Bourdev et al., "Learning spatiotemporal features with 3d convolutional networks," in Proceedings of the IEEE international conference on computer vision, 2015, pp. 4489–4497.
[26] J. Carreira and A. Zisserman, "Quo vadis, action recognition? a new model and the kinetics dataset," in proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308.
[27] C. Szegedy, W. Liu et al., "Going deeper with convolutions," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1–9.
[28] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., "The kinetics human action video dataset," arXiv preprint arXiv:1705.06950, 2017.
[29] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al., "The" something something" video database for learning and evaluating visual common sense," in Proceedings of the IEEE international conference on computer vision, 2017, pp. 5842–5850.
[30] K. He et al., "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

[31] S. Xie, R. Girshick et al., "Aggregated residual transformations for deep neural networks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
[32] J. Hu, L. Shen, and G. Sun, "Squeeze-and-excitation networks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[33] K. Hara, H. Kataoka, and Y. Satoh, "Learning spatio-temporal features with 3d residual networks for action recognition," in Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2017.
[34] Y. Chen, Y. Kalantidis et al., "Multi-fiber networks for video recognition," in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.
[35] A. Diba, M. Fayyaz et al., "Spatio-temporal channel correlation networks for action classification," in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.
[36] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification," in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.
[37] M. Zolfaghari, K. Singh, and T. Brox, "Eco: Efficient convolutional network for online video understanding," in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 695–712.
[38] Z. Qiu, T. Yao, and T. Mei, "Learning spatio-temporal representation with pseudo-3d residual networks," in proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 5533–5541.
[39] G. Varol, I. Laptev, and C. Schmid, "Long-term temporal convolutions for action recognition," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 6, pp. 1510–1517, 2018.
[40] A. Diba, M. Fayyaz, V. Sharma, A. H. Karami, M. M. Arzani, R. Yousefzadeh, and L. V. Gool, "Temporal 3d convnets: New architecture and transfer learning for video classification," CoRR, vol. abs/1711.08200, 2017.
[41] X. Wang, R. Girshick, A. Gupta, and K. He, "Non-local neural networks," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
[42] S. Zhang, S. Guo et al., "V4d: 4d convolutional neural networks for video-level representation learning," in International Conference on Learning Representations, 2019.
[43] D. Tran et al., "Video classification with channel-separated convolutional networks," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 5552–5561.
[44] C. Feichtenhofer et al., "Slowfast networks for video recognition," in Proceedings of the International Conference on Computer Vision (ICCV), 2019.
[45] C. Feichtenhofer, "X3d: Expanding architectures for efficient video recognition," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 203–213.
[46] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2021.
[47] G. Bertasius, H. Wang, and L. Torresani, "Is space-time attention all you need for video understanding?" in ICML, vol. 2, no. 3, 2021, p. 4.
[48] X. Li, Y. Zhang et al., "Vidtr: Video transformer without convolutions," arXiv e-prints, pp. arXiv–2104, 2021.
[49] A. Arnab, M. Dehghani et al., "Vivit: A video vision transformer," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 6836–6846.
[50] H. Fan, B. Xiong et al., "Multiscale vision transformers," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 6824–6835.
[51] C. Li, D. Zhang, W. Huang, and J. Zhang, "Cross contrasting feature perturbation for domain generalization," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 1327–1337.
[52] S. Wang et al., "Feature alignment and uniformity for test time adaptation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 20,050–20,060.
[53] D. Zhang et al., "Rethinking alignment and uniformity in unsupervised image semantic segmentation," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 9, 2023, pp. 11,192–11,200.
[54] C. Sun, A. Myers et al., "Videobert: A joint model for video and language representation learning," in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 7464–7473.
[55] J. D. M.-W. C. Kenton and L. K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in Proceedings of NAACL-HLT, 2019, pp. 4171–4186.

--- TRANG 16 ---
16

[56] L. Zhu and Y. Yang, "Actbert: Learning global-local video-text representations," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 8746–8755.
[57] C. Feichtenhofer et al., "Masked autoencoders as spatiotemporal learners," Advances in neural information processing systems, vol. 35, pp. 35,946–35,958, 2022.
[58] R. Girdhar et al., "Omnimae: Single model masked pretraining on images and videos," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 10,406–10,417.
[59] Z. Tong, Y. Song, J. Wang, and L. Wang, "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pretraining," Advances in neural information processing systems, vol. 35, pp. 10,078–10,093, 2022.
[60] H. Yang, D. Huang et al., "Self-supervised video representation learning with motion-aware masked autoencoders," arXiv preprint arXiv:2210.04154, 2022.
[61] C. Wei, H. Fan et al., "Masked feature prediction for self-supervised visual pre-training," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 14,668–14,678.
[62] H. Xu, G. Ghosh et al., "Vlm: Task-agnostic video-language model pre-training for video understanding," in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 4227–4239.
[63] D. Li, J. Li et al., "Align and prompt: Video-and-language pre-training with entity prompts," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 4953–4963.
[64] N. Moritz, G. Wichern, T. Hori, and J. Le Roux, "All-in-one transformer: Unifying speech recognition, audio tagging, and event detection." in INTERSPEECH, 2020, pp. 3112–3116.
[65] A. Gupta, S. Tian, Y. Zhang, J. Wu, R. Martín-Martín, and L. Fei-Fei, "Maskvit: Masked visual pre-training for video prediction," in The Eleventh International Conference on Learning Representations, 2022.
[66] H. Xue, Y. Sun et al., "Clip-vip: Adapting pre-trained image-text model to video-language alignment," in The Eleventh International Conference on Learning Representations, 2022.
[67] J. Lei, T. L. Berg, and M. Bansal, "Revealing single frame bias for video-and-language learning," arXiv preprint arXiv:2206.03428, 2022.
[68] Y. Sun, H. Xue et al., "Long-form video-language pre-training with multimodal temporal contrastive learning," Advances in neural information processing systems, vol. 35, pp. 38,032–38,045, 2022.
[69] P. Jin, J. Huang, F. Liu, X. Wu, S. Ge, G. Song, D. Clifton, and J. Chen, "Expectation-maximization contrastive learning for compact video-and-language representations," Advances in Neural Information Processing Systems, vol. 35, pp. 30,291–30,306, 2022.
[70] Q. Ye, G. Xu et al., "Hitea: Hierarchical temporal-aware video-language pre-training," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 15,405–15,416.
[71] S. Han, J. Hessel et al., "Champagne: Learning real-world conversation from large-scale web videos," arXiv preprint arXiv:2303.09713, 2023.
[72] H. Lyu et al., "Gpt-4v(ision) as a social media analysis engine," arXiv preprint arXiv:2311.07547, 2023.
[73] D. Zhang, W. Zhang et al., "Dnagpt: A generalized pretrained tool for multiple dna sequence analysis tasks," bioRxiv, pp. 2023–07, 2023.
[74] OpenAI, "Introducing chatgpt," https://openai.com/blog/chatgpt, 2022.
[75] C. Wu, S. Yin et al., "Visual chatgpt: Talking, drawing and editing with visual foundation models," arXiv preprint arXiv:2303.04671, 2023.
[76] H. Liu et al., "Visual instruction tuning," arXiv preprint arXiv:2304.08485, 2023.
[77] W. X. Zhao, K. Zhou et al., "A survey of large language models," arXiv preprint arXiv:2303.18223, 2023.
[78] C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao, "Multimodal foundation models: From specialists to general-purpose assistants," arXiv preprint arXiv:2309.10020, vol. 1, 2023.
[79] M. Abdar et al., "A review of deep learning for video captioning," arXiv preprint arXiv:2304.11431, 2023.
[80] Y. Zhu et al., "A comprehensive study of deep video action recognition," arXiv preprint arXiv:2012.06567, 2020.
[81] Z. Xing, Q. Feng et al., "A survey on video diffusion models," arXiv preprint arXiv:2310.10647, 2023.
[82] Y. Annepaka and P. Pakray, "Large language models: A survey of their development, capabilities, and applications," Knowledge and Information Systems, pp. 1–56, 2024.
[83] N. Madan, A. Møgelmose, R. Modi, Y. S. Rawat, and T. B. Moeslund, "Foundation models for video understanding: A survey," Authorea Preprints, 2024.

[84] M. Marszalek, I. Laptev, and C. Schmid, "Actions in context," in 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops). Los Alamitos, CA, USA: IEEE Computer Society, jun 2009, pp. 2929–2936.
[85] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, "Activitynet: A large-scale video benchmark for human activity understanding," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[86] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta, "Hollywood in homes: Crowdsourcing data collection for activity understanding," in Proceedings of the European Conference on Computer Vision (ECCV), 2016.
[87] J. Carreira, E. Noland et al., "A short note about kinetics-600," arXiv preprint arXiv:1808.01340, 2018.
[88] J. Carreira et al., "A short note on the kinetics-700 human action dataset," arXiv preprint arXiv:1907.06987, 2019.
[89] H. Zhao et al., "Hacs: Human action clips and segments dataset for recognition and temporal localization," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 8668–8678.
[90] S. Abu-El-Haija et al., "Youtube-8m: A large-scale video classification benchmark," 2016.
[91] M. Han et al., "Video recognition in portrait mode," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 21,831–21,841.
[92] Y. Wang, D. Gao et al., "Geb+: A benchmark for generic event boundary captioning, grounding and retrieval," in European Conference on Computer Vision. Springer, 2022, pp. 709–725.
[93] D. Xu et al., "Video question answering via gradually refined attention over appearance and motion," in Proceedings of the 25th ACM international conference on Multimedia, 2017, pp. 1645–1653.
[94] L. Anne Hendricks et al., "Localizing moments in video with natural language," in Proceedings of the IEEE international conference on computer vision, 2017, pp. 5803–5812.
[95] L. Zhou, C. Xu, and J. Corso, "Towards automatic learning of procedures from web instructional videos," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.
[96] H. Xu, Q. Ye et al., "Youku-mplug: A 10 million large-scale chinese video-language dataset for pre-training and benchmarks," arXiv preprint arXiv:2306.04362, 2023.
[97] G. Huang et al., "Multimodal pretraining for dense video captioning," arXiv preprint arXiv:2011.11760, 2020.
[98] J. Lin, H. Hua et al., "Videoxum: Cross-modal visual and textural summarization of videos," arXiv preprint arXiv:2303.12060, 2023.
[99] M. Maaz, H. Rasheed et al., "Video-chatgpt: Towards detailed video understanding via large vision and language models," arXiv preprint arXiv:2306.05424, 2023.
[100] H. Hua, Y. Tang, C. Xu, and J. Luo, "V2xum-llm: Cross-modal video summarization with temporal prompt instruction tuning," arXiv preprint arXiv:2404.12353, 2024.
[101] D. Chen and W. B. Dolan, "Collecting highly parallel data for paraphrase evaluation," in Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, 2011, pp. 190–200.
[102] J. Xu, T. Mei, T. Yao, and Y. Rui, "Msr-vtt: A large video description dataset for bridging video and language," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 5288–5296.
[103] Y. Li, Y. Song, L. Cao, J. Tetreault, L. Goldberg, A. Jaimes, and J. Luo, "Tgif: A new dataset and benchmark on animated gif description," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4641–4650.
[104] G. A. Sigurdsson et al., "Charades-ego: A large-scale dataset of paired third and first person videos," arXiv preprint arXiv:1804.09626, 2018.
[105] S. Chen, H. Li et al., "Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset," arXiv preprint arXiv:2305.18500, 2023.
[106] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi, "From recognition to cognition: Visual commonsense reasoning," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6720–6731.
[107] Y. Jang et al., "Tgif-qa: Toward spatio-temporal reasoning in visual question answering," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2758–2766.
[108] K.-M. Kim et al., "Deepstory: Video story qa by deep embedded memory networks," arXiv preprint arXiv:1707.00836, 2017.
[109] J. Lei, L. Yu et al., "Tvqa: Localized, compositional video question answering," in EMNLP, 2018.

--- TRANG 17 ---
17

[110] Z. Yu, D. Xu et al., "Activitynet-qa: A dataset for understanding complex web videos via question answering," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 9127–9134.
[111] J. Xiao et al., "Next-qa: Next phase of question-answering to explaining temporal actions," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 9777–9786.
[112] M. Gygli et al., "Creating summaries from user videos," in Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13. Springer, 2014, pp. 505–520.
[113] Y. Song et al., "Tvsum: Summarizing web videos using titles," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5179–5187.
[114] Y. Tang, S. Xu, T. Wang, Q. Lin, Q. Lu, and F. Zheng, "Multi-modal segment assemblage network for ad video editing with importance-coherence reward," in Proceedings of the Asian Conference on Computer Vision, 2022, pp. 3519–3535.
[115] M. Sun et al., "Ranking domain-specific highlights by analyzing edited videos," in Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13. Springer, 2014, pp. 787–802.
[116] K.-H. Zeng et al., "Title generation for user generated videos," in Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. Springer, 2016, pp. 609–625.
[117] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici, I. Laptev, M. Shah, and R. Sukthankar, "THUMOS challenge: Action recognition with a large number of classes," http://crcv.ucf.edu/THUMOS14/, 2014.
[118] T. Geng et al., "Dense-localizing audio-visual events in untrimmed videos: A large-scale benchmark and baseline," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22,942–22,951.
[119] J. Gao, C. Sun, Z. Yang, and R. Nevatia, "Tall: Temporal activity localization via language query," in Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct 2017.
[120] Y. Tang, D. Shimada, J. Bi, M. Feng, H. Hua, and C. Xu, "Empowering llms with pseudo-untrimmed videos for audio-visual temporal understanding," arXiv preprint arXiv:2403.16276, 2024.
[121] M. Z. Shou et al., "Generic event boundary detection: A benchmark for event segmentation," in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 8075–8084.
[122] Z. Shao et al., "Region-object relation-aware dense captioning via transformer," IEEE transactions on neural networks and learning systems, 2022.
[123] T. Wang et al., "End-to-end dense video captioning with parallel decoding," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 6847–6857.
[124] Z. Shao et al., "Textual context-aware dense captioning with diverse words," IEEE Transactions on Multimedia, vol. 25, pp. 8753–8766, 2023.
[125] Y. Long et al., "Capdet: Unifying dense captioning and open-world detection pretraining," in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 15,233–15,243.
[126] Z. Shao et al., "Dcmstrd: end-to-end dense captioning via multi-scale transformer decoding," IEEE Transactions on Multimedia, 2024.
[127] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. Carlos Niebles, "Dense-captioning events in videos," in Proceedings of the IEEE international conference on computer vision, 2017, pp. 706–715.
[128] A. Yang et al., "Vidchapters-7m: Video chapters at scale," arXiv preprint arXiv:2309.13952, 2023.
[129] Y. Wu, J. Lim, and M.-H. Yang, "Online object tracking: A benchmark," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2013, pp. 2411–2418.
[130] U. T. Benchmark, "A benchmark and simulator for uav tracking."
[131] M. Kristan et al., "The tenth visual object tracking vot2022 challenge results," in European Conference on Computer Vision. Springer, 2022, pp. 431–460.
[132] L. Zheng, L. Shen et al., "Scalable person re-identification: A benchmark," in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1116–1124.
[133] W. Li et al., "Deepreid: Deep filter pairing neural network for person re-identification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 152–159.

[134] L. Wei, S. Zhang, W. Gao, and Q. Tian, "Person transfer gan to bridge domain gap for person re-identification," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 79–88.
[135] E. Ristani et al., "Performance measures and a data set for multi-target, multi-camera tracking," in European conference on computer vision. Springer, 2016, pp. 17–35.
[136] A. Moskalenko, A. Bryncev, D. Vatolin, R. Timofte, G. Zhan, L. Yang, Y. Tang et al., "Aim 2024 challenge on video saliency prediction: Methods and results," arXiv preprint arXiv:2409.14827, 2024.
[137] W. Wang et al., "Revisiting video saliency: A large-scale benchmark and a new model," in Proceedings of the IEEE Conference on computer vision and pattern recognition, 2018, pp. 4894–4903.
[138] M. D. Rodriguez, J. Ahmed, and M. Shah, "Action mach a spatio-temporal maximum average correlation height filter for action recognition," in 2008 IEEE Conference on Computer Vision and Pattern Recognition, 2008, pp. 1–8.
[139] X. Min, G. Zhai, C. Hu, and K. Gu, "Fixation prediction through multimodal analysis," in 2015 Visual Communications and Image Processing (VCIP), 2015, pp. 1–4.
[140] A. Coutrot and N. Guyader, "How saliency, faces, and sound influence gaze in dynamic social scenes," Journal of vision, vol. 14, no. 8, pp. 5–5, 2014.
[141] A. Coutrot et al., "Multimodal saliency models for videos," From Human Attention to Computational Attention: A Multidisciplinary Approach, pp. 291–304, 2016.
[142] P. Koutras and P. Maragos, "A perceptually based spatio-temporal computational framework for visual saliency estimation," Signal Processing: Image Communication, vol. 38, pp. 15–31, 2015.
[143] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang, "Youtube-vos: A large-scale video object segmentation benchmark," 2018.
[144] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung, "A benchmark dataset and evaluation methodology for video object segmentation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 724–732.
[145] L. Yang, Y. Fan, and N. Xu, "The 4th large-scale video object segmentation challenge - video instance segmentation track," Jun. 2022.
[146] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban scene understanding," 2016.
[147] H. Ding, C. Liu, S. He, X. Jiang, and C. C. Loy, "Mevis: A large-scale benchmark for video segmentation with motion expressions," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 2694–2703.
[148] Z. Zhang, Z. Zhao, Y. Zhao, Q. Wang, H. Liu, and L. Gao, "Where does it exist: Spatio-temporal video grounding for multi-form sentences," in CVPR, 2020.
[149] Z. Tang et al., "Human-centric spatio-temporal video grounding with visual transformers," IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 12, pp. 8238–8249, 2021.
[150] K. Grauman et al., "Ego4d: Around the world in 3,000 hours of egocentric video," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022, pp. 18,995–19,012.
[151] J. Kaplan et al., "Scaling laws for neural language models," arXiv preprint arXiv:2001.08361, 2020.
[152] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4: Enhancing vision-language understanding with advanced large language models," arXiv preprint arXiv:2304.10592, 2023.
[153] K. Chen et al., "Shikra: Unleashing multimodal llm's referential dialogue magic," arXiv preprint arXiv:2306.15195, 2023.
[154] S. Xuan, Q. Guo, M. Yang, and S. Zhang, "Pink: Unveiling the power of referential comprehension for multi-modal llms," arXiv preprint arXiv:2310.00582, 2023.
[155] H. Hua, J. Shi, K. Kafle, S. Jenni, D. Zhang, J. Collomosse, S. Cohen, and J. Luo, "Finematch: Aspect-based fine-grained image and text mismatch detection and correction," arXiv preprint arXiv:2404.14715, 2024.
[156] A. R. Kalarani, P. Bhattacharyya, and S. Shekhar, "Seeing the unseen: Visual metaphor captioning for videos," arXiv preprint arXiv:2406.04886, 2024.
[157] Y. Wu, B. Li et al., "Zero-shot long-form video understanding through screenplay," arXiv preprint arXiv:2406.17309, 2024.
[158] J. Min et al., "Morevqa: Exploring modular reasoning models for video question answering," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13,235–13,245.

--- TRANG 18 ---
18

[159] W. Kim, C. Choi, W. Lee, and W. Rhee, "An image grid can be worth a video: Zero-shot video question answering using a vlm," 2024.
[160] K. Kahatapitiya, K. Ranasinghe, J. Park, and M. S. Ryoo, "Language repository for long video understanding," 2024.
[161] K. Ranasinghe, X. Li, K. Kahatapitiya, and M. S. Ryoo, "Understanding long videos in one multimodal language model pass," 2024.
[162] M. M. Islam, N. Ho, X. Yang, T. Nagarajan, L. Torresani, and G. Bertasius, "Video recap: Recursive captioning of hour-long videos," 2024.
[163] C. Zhang, T. Lu, M. M. Islam, Z. Wang, S. Yu, M. Bansal, and G. Bertasius, "A simple llm framework for long-range video question-answering," 2024.
[164] H. Chen, X. Wang, H. Chen, Z. Song, J. Jia, and W. Zhu, "Grounding-prompter: Prompting llm with multimodal information for temporal sentence grounding in long videos," 2023.
[165] Z. Xue, K. Ashutosh, and K. Grauman, "Learning object state changes in videos: An open-world perspective," 2024.
[166] Q. Zhao, C. Zhang et al., "Antgpt: Can large language models help long-term action anticipation from videos?" arXiv preprint arXiv:2307.16368, 2023.
[167] showlab, "Vlog: Transform video as a document with chatgpt, clip, blip2, grit, whisper, langchain," https://github.com/showlab/VLog, accessed: 2023-12-23.
[168] Y. Zhao, I. Misra, P. Krähenbühl, and R. Girdhar, "Learning video representations from large language models," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 6586–6597.
[169] Z. Ma, C. Gou, H. Shi, B. Sun, S. Li, H. Rezatofighi, and J. Cai, "Drvideo: Document retrieval based long video understanding," arXiv preprint arXiv:2406.12846, 2024.
[170] L. Zhang, T. Zhao et al., "Omagent: A multi-modal agent framework for complex video understanding with task divide-and-conquer," arXiv preprint arXiv:2406.16620, 2024.
[171] J. Park, K. Ranasinghe et al., "Too many frames, not all useful: Efficient strategies for long-form video qa," arXiv preprint arXiv:2406.09396, 2024.
[172] Y. Sun, Y. Xu, Z. Xie, Y. Shu, and S. Du, "Gptsee: Enhancing moment retrieval and highlight detection via description-based similarity features," 2024.
[173] Z. Wang, S. Yu et al., "Videotree: Adaptive tree-based video representation for llm reasoning on long videos," arXiv preprint arXiv:2405.19209, 2024.
[174] L. Zanella, W. Menapace et al., "Harnessing large language models for training-free video anomaly detection," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18,527–18,536.
[175] C. Shang, A. You et al., "Traveler: A multi-lmm agent framework for video question-answering," arXiv preprint arXiv:2404.01476, 2024.
[176] J. Cao, Y. Wu, W. Chi, W. Zhu, Z. Su, and J. Wu, "Reframe anything: Llm agent for open world video reframing," 2024.
[177] Y. Niu, W. Guo, L. Chen, X. Lin, and S.-F. Chang, "Schema: State changes matter for procedure planning in instructional videos," 2024.
[178] Y. Shen et al., "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face," Advances in Neural Information Processing Systems, vol. 36, 2024.
[179] K. Sanders, N. Weir, and B. V. Durme, "Tv-trees: Multimodal entailment trees for neuro-symbolic video reasoning," 2024.
[180] Y. Fan, X. Ma, R. Wu, Y. Du, J. Li, Z. Gao, and Q. Li, "Videoagent: A memory-augmented multimodal agent for video understanding," 2024.
[181] X. Wang, Y. Zhang, O. Zohar, and S. Yeung-Levy, "Videoagent: Long-form video understanding with large language model as agent," 2024.
[182] A. Mahmood, A. Vayani, M. Naseer, S. Khan, and F. S. Khan, "Vurf: A general-purpose reasoning and self-refinement framework for video understanding," 2024.
[183] K. R. Y. Nagasinghe, H. Zhou, M. Gunawardhana, M. R. Min, D. Harari, and M. H. Khan, "Why not use your textbook? knowledge-enhanced procedure planning of instructional videos," 2024.
[184] Z. Yang, G. Chen, X. Li, W. Wang, and Y. Yang, "Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as a video agent)," 2024.
[185] J. Tang et al., "Hawk: Learning to understand open-world video anomalies," arXiv preprint arXiv:2405.16886, 2024.
[186] Y. Wang, Y. Yang, and M. Ren, "Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos," 2024.
[187] R. Choudhury, K. Niinuma, K. M. Kitani, and L. A. Jeni, "Zero-shot video question answering with procedural programs," 2023.

[188] D. Gao, L. Ji et al., "Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn," arXiv preprint arXiv:2306.08640, 2023.
[189] J. Wang, D. Chen et al., "Chatvideo: A tracklet-centric multi-modal and versatile video understanding system," arXiv preprint arXiv:2304.14407, 2023.
[190] D. Surís, S. Menon, and C. Vondrick, "Vipergpt: Visual inference via python execution for reasoning," arXiv preprint arXiv:2303.08128, 2023.
[191] J. Qiu, Y. Zhang, X. Tang, L. Xie, T. Ma, P. Yan, D. Doermann, Q. Ye, and Y. Tian, "Artemis: Towards referential understanding in complex videos," arXiv preprint arXiv:2406.00258, 2024.
[192] Q. Yang, M. Ye, and B. Du, "Emollm: Multimodal emotional understanding meets large language models," arXiv preprint arXiv:2406.16442, 2024.
[193] S. Chen, Y. Yuan, S. Chen, Z. Jie, and L. Ma, "Fewer tokens and fewer videos: Extending video understanding abilities in large vision-language models," arXiv preprint arXiv:2406.08024, 2024.
[194] H. Zhang, Y. Wang, Y. Tang, Y. Liu, J. Feng, J. Dai, and X. Jin, "Flash-vstream: Memory-based real-time understanding for long video streams," 2024.
[195] R. Chakraborty, A. Sinha, D. Reilly, M. K. Govind, P. Wang, F. Bremond, and S. Das, "Llavidal: Benchmarking large language vision models for daily activities of living," arXiv preprint arXiv:2406.09390, 2024.
[196] P. Zhang, K. Zhang et al., "Long context transfer from language to vision," arXiv preprint arXiv:2406.16852, 2024.
[197] L. Chen, X. Wei, J. Li et al., "Sharegpt4video: Improving video understanding and generation with better captions," arXiv preprint arXiv:2406.04325, 2024.
[198] Y. Du, K. Zhou et al., "Towards event-oriented long video understanding," arXiv preprint arXiv:2406.14129, 2024.
[199] G. Sun, W. Yu et al., "Video-salmonn: Speech-enhanced audio-visual large language models," arXiv preprint arXiv:2406.15704, 2024.
[200] M. Maaz, H. Rasheed, S. Khan, and F. Khan, "Videogpt+: Integrating image and video encoders for enhanced video understanding," arXiv preprint arXiv:2406.09418, 2024.
[201] Z. Cheng, S. Leng et al., "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms," arXiv preprint arXiv:2406.07476, 2024.
[202] L.-H. Chen, S. Lu, A. Zeng, H. Zhang, B. Wang, R. Zhang, and L. Zhang, "Motionllm: Understanding human behaviors from human motions and videos," arXiv preprint arXiv:2405.20340, 2024.
[203] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo et al., "Mvbench: A comprehensive multi-modal video understanding benchmark," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 22,195–22,206.
[204] R. Luo, A. Peng, A. Vasudev, and R. Jain, "Shotluck holmes: A family of efficient small-scale large language vision models for video captioning and summarization," arXiv preprint arXiv:2405.20648, 2024.
[205] R. Qian, X. Dong et al., "Streaming long video understanding with large language models," arXiv preprint arXiv:2405.16009, 2024.
[206] D. Yang, C. Zhan et al., "Synchronized video storytelling: Generating video narrations with structured storyline," arXiv preprint arXiv:2405.14040, 2024.
[207] W. Li, H. Fan, Y. Wong, M. Kankanhalli, and Y. Yang, "Topa: Extend large language models for video understanding via text-only pre-alignment," arXiv preprint arXiv:2405.13911, 2024.
[208] T. Han et al., "Autoad iii: The prequel-back to the pixels," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18,164–18,174.
[209] H. Wang, C. Lai, Y. Sun, and W. Ge, "Weakly supervised gaussian contrastive grounding with large multimodal models for video question answering," 2024.
[210] R. Zhang, L. Gui et al., "Direct preference optimization of video large multimodal models from language model reward," arXiv preprint arXiv:2404.01258, 2024.
[211] S. Huang, H. Zhang et al., "From image to video, what do we need in multimodal llms?" arXiv preprint arXiv:2404.11865, 2024.
[212] R. Tan, X. Sun et al., "Koala: Key frame-conditioned long video-llm," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13,581–13,591.
[213] Y. Weng, M. Han et al., "Longvlm: Efficient long video understanding via large language models," arXiv preprint arXiv:2404.03384, 2024.

--- TRANG 19 ---
19

[214] B. He, H. Li et al., "Ma-lmm: Memory-augmented large multimodal model for long-term video understanding," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13,504–13,514.
[215] K. Ataallah, X. Shen et al., "Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens," arXiv preprint arXiv:2404.03413, 2024.
[216] R. Jung et al., "Pegasus-v1 technical report," arXiv preprint arXiv:2404.14687, 2024.
[217] L. Xu, Y. Zhao et al., "Pllava: Parameter-free llava extension from images to videos for video dense captioning," arXiv preprint arXiv:2404.16994, 2024.
[218] R. Liu, C. Li et al., "St-llm: Large language models are effective temporal learners," arXiv preprint arXiv:2404.00308, 2024.
[219] A. J. Wang et al., "Cosmo: Contrastive streamlined multimodal model with interleaved pre-training," arXiv preprint arXiv:2401.00849, 2024.
[220] J. Wang, L. Yuan, and Y. Zhang, "Tarsier: Recipes for training and evaluating large video description models," arXiv preprint arXiv:2407.00634, 2024.
[221] J. Held, H. Itani et al., "X-vars: Introducing explainability in football refereeing with multi-modal large language models," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 3267–3279.
[222] Q. Ye, Z. Yu, R. Shao, X. Xie, P. Torr, and X. Cao, "Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarios," 2024.
[223] G. Chen, Y.-D. Zheng et al., "Videollm: Modeling video sequence with large language models," arXiv preprint arXiv:2305.13292, 2023.
[224] Y. Wang, K. Li, X. Li, J. Yu, Y. He, G. Chen, B. Pei, R. Zheng, J. Xu, Z. Wang, Y. Shi, T. Jiang, S. Li, H. Zhang, Y. Huang, Y. Qiao, Y. Wang, and L. Wang, "Internvideo2: Scaling video foundation models for multimodal video understanding," 2024.
[225] Z. Song, C. Wang, J. Sheng, C. Zhang, G. Yu, J. Fan, and T. Chen, "Moviellm: Enhancing long video understanding with ai-generated movies," 2024.
[226] Y. Li, X. Chen, B. Hu, and M. Zhang, "Llms meet long video: Advancing long video comprehension with an interactive visual adapter in llms," 2024.
[227] Y. Wang, Y. Wang, P. Wu, J. Liang, D. Zhao, and Z. Zheng, "Lstp: Language-guided spatial-temporal prompt learning for long-form video-text understanding," 2024.
[228] Y. Wang, Z. Zhang, J. McAuley, and Z. He, "Lvchat: Facilitating long video comprehension," 2024.
[229] N. Nguyen, J. Bi, A. Vosoughi, Y. Tian, P. Fazli, and C. Xu, "Oscar: Object state captioning and state change representation," 2024.
[230] J. Xu, C. Lan, W. Xie, X. Chen, and Y. Lu, "Slot-vlm: Slowfast slots for video-language modeling," 2024.
[231] F. Shu, L. Zhang, H. Jiang, and C. Xie, "Audio-visual llm for video understanding," 2023.
[232] Q. Sun, Y. Cui, X. Zhang, F. Zhang, Q. Yu, Z. Luo, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang, "Generative multimodal models are in-context learners," 2024.
[233] T. Chen, E. Zhang, Y. Gao, K. Li, X. Sun, Y. Zhang, and H. Li, "Mmict: Boosting multi-modal fine-tuning with in-context examples," 2023.
[234] Y. Wang, R. Zhang, H. Wang, U. Bhattacharya, Y. Fu, and G. Wu, "Vaquita: Enhancing alignment in llm-assisted video understanding," 2023.
[235] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han, "Vila: On pre-training for visual language models," 2024.
[236] F. Ma, X. Jin, H. Wang, Y. Xian, J. Feng, and Y. Yang, "Vista-llama: Reliable video narrator via equal distance to visual tokens," 2023.
[237] P. Jin, R. Takanobu, W. Zhang, X. Cao, and L. Yuan, "Chat-univi: Unified visual representation empowers large language models with image and video understanding," 2024.
[238] Y. Li, C. Wang, and J. Jia, "Llama-vid: An image is worth 2 tokens in large language models," arXiv preprint arXiv:2311.17043, 2023.
[239] B. Lin, B. Zhu, Y. Ye, M. Ning, P. Jin, and L. Yuan, "Video-llava: Learning united visual representation by alignment before projection," arXiv preprint arXiv:2311.10122, 2023.
[240] D. Ko et al., "Large language models are temporal and causal reasoners for video question answering," arXiv preprint arXiv:2310.15747, 2023.
[241] E. Song, W. Chai, G. Wang, Y. Zhang, H. Zhou, F. Wu, X. Guo, T. Ye, Y. Lu, J.-N. Hwang et al., "Moviechat: From dense token to sparse memory for long video understanding," arXiv preprint arXiv:2307.16449, 2023.

[242] Y. Tang, J. Zhang, X. Wang, T. Wang, and F. Zheng, "Llmva-gebc: Large language model with video adapter for generic event boundary captioning," arXiv preprint arXiv:2306.10354, 2023.
[243] C. Lyu, M. Wu et al., "Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration," arXiv preprint arXiv:2306.09093, 2023.
[244] R. Luo, Z. Zhao et al., "Valley: Video assistant with large language model enhanced ability," arXiv preprint arXiv:2306.07207, 2023.
[245] H. Zhang, X. Li, and L. Bing, "Video-llama: An instruction-tuned audio-visual language model for video understanding," arXiv preprint arXiv:2306.02858, 2023.
[246] Z. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan, and J. Liu, "Chatbridge: Bridging modalities with large language model as a language catalyst," arXiv preprint arXiv:2305.16103, 2023.
[247] B. Li, Y. Zhang et al., "Otter: A multi-modal model with in-context instruction tuning," arXiv preprint arXiv:2305.03726, 2023.
[248] H. Zhang, X. Xu, X. Wang, J. Zuo, C. Han, X. Huang, C. Gao, Y. Wang, and N. Sang, "Holmes-vad: Towards unbiased and explainable video anomaly detection via multi-modal llm," arXiv preprint arXiv:2406.12235, 2024.
[249] J. Chen, Z. Lv et al., "Videollm-online: Online video large language model for streaming video," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18,407–18,418.
[250] S. Bansal et al., "Hoi-ref: Hand-object interaction referral in egocentric vision," arXiv preprint arXiv:2404.09933, 2024.
[251] H. Wang et al., "Elysium: Exploring object-level perception in videos via mllm," arXiv preprint arXiv:2403.16558, 2024.
[252] Y. Wang, X. Meng, J. Liang, Y. Wang, Q. Liu, and D. Zhao, "Hawkeye: Training video-text llms for grounding text in videos," 2024.
[253] D.-A. Huang, S. Liao, S. Radhakrishnan, H. Yin, P. Molchanov, Z. Yu, and J. Kautz, "Lita: Language instructed temporal-localization assistant," 2024.
[254] J. Wang, D. Chen, C. Luo, B. He, L. Yuan, Z. Wu, and Y.-G. Jiang, "Omnivid: A generative framework for universal video understanding," 2024.
[255] S. Yu et al., "Self-chained image-language model for video localization and question answering," 2023.
[256] Z. Li, Q. Xu, D. Zhang, H. Song, Y. Cai, Q. Qi, R. Zhou, J. Pan, Z. Li, V. T. Vu, Z. Huang, and T. Wang, "Groundinggpt:language enhanced multi-modal grounding model," 2024.
[257] S. Ren, L. Yao, S. Li, X. Sun, and L. Hou, "Timechat: A time-sensitive multimodal large language model for long video understanding," 2024.
[258] B. Huang, X. Wang et al., "Vtimellm: Empower llm to grasp video moments," arXiv preprint arXiv:2311.18445, 2023.
[259] Y. Guo, J. Liu et al., "Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding," arXiv preprint arXiv:2405.13382, 2024.
[260] H. Fei, S. Wu et al., "Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing," 2024.
[261] Y. Xu, Y. Sun, Z. Xie, B. Zhai, and S. Du, "Vtg-gpt: Tuning-free zero-shot video temporal grounding with gpt," 2024.
[262] L. Qian, J. Li, Y. Wu, Y. Ye, H. Fei, T.-S. Chua, Y. Zhuang, and S. Tang, "Momentor: Advancing video large language model with fine-grained temporal reasoning," 2024.
[263] K. Ashutosh, Z. Xue, T. Nagarajan, and K. Grauman, "Detours for navigating instructional videos," 2024.
[264] J. Han, K. Gong, Y. Zhang, J. Wang, K. Zhang, D. Lin, Y. Qiao, P. Gao, and X. Yue, "Onellm: One framework to align all modalities with language," 2023.
[265] Z. Wang et al., "Gpt4video: A unified multimodal large language model for lnstruction-followed understanding and safety-aware generation," arXiv preprint arXiv:2311.16511, 2023.
[266] K. Lin et al., "Mm-vid: Advancing video understanding with gpt-4v(ision)," arXiv preprint arXiv:2310.19773, 2023.
[267] M. Han, L. Yang, X. Chang, and H. Wang, "Shot2story20k: A new benchmark for comprehensive understanding of multi-shot videos," 2023.
[268] D. Yang, S. Huang et al., "Vript: A video is worth thousands of words," arXiv preprint arXiv:2406.06040, 2024.
[269] E. Yu, L. Zhao et al., "Merlin:empowering multimodal llms with foresight minds," 2023.
[270] K. Li, Y. He et al., "Videochat: Chat-centric video understanding," arXiv preprint arXiv:2305.06355, 2023.
[271] A. Yang et al., "Vid2seq: Large-scale pretraining of a visual language model for dense video captioning," in Proceedings of the IEEE/CVF

--- TRANG 20 ---
20

Conference on Computer Vision and Pattern Recognition, 2023, pp. 10,714–10,726.
[272] H. Wang, Z. Tong, K. Zheng, Y. Shen, and L. Wang, "Contextual ad narration with interleaved multimodal sequence," 2024.
[273] C. Zhang et al., "Mm-narrator: Narrating long-form videos with multimodal in-context learning," arXiv preprint arXiv:2311.17435, 2023.
[274] S. Wang et al., "Vamos: Versatile action models for video understanding," 2024.
[275] T. Han, M. Bain et al., "Autoad ii: The sequel-who, when, and what in movie audio description," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 13,645–13,655.
[276] S. Munasinghe et al., "Pg-video-llava: Pixel grounding large video-language models," arXiv preprint arXiv:2311.13435, 2023.
[277] J. Chen, D. Zhu et al., "Video chatcaptioner: Towards the enriched spatiotemporal descriptions," arXiv preprint arXiv:2304.04227, 2023.
[278] B. Elizalde et al., "Clap learning audio concepts from natural language supervision," in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.
[279] M. Xu, M. Gao et al., "Slowfast-llava: A strong training-free baseline for video large language models," arXiv preprint arXiv:2407.15841, 2024.
[280] A. Zeng et al., "Socratic models: Composing zero-shot multimodal reasoning with language," arXiv preprint arXiv:2204.00598, 2022.
[281] G. Sun, W. Yu et al., "Fine-grained audio-visual joint representations for multimodal large language models," 2023.
[282] J. Li, D. Li et al., "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," in International Conference on Machine Learning, 2023.
[283] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, and E. Chen, "A survey on multimodal large language models," arXiv preprint arXiv:2306.13549, 2023.
[284] L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu, "Hero: Hierarchical encoder for video+ language omni-representation pre-training," in EMNLP, 2020.
[285] B. Wu and S. Yu, "Star: A benchmark for situated reasoning in real-world videos," in NeurIPS, 2024.
[286] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, "Cider: Consensus-based image description evaluation," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 4566–4575.
[287] S. Banerjee and A. Lavie, "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments," in Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005, pp. 65–72.
[288] C.-Y. Lin, "Rouge: A package for automatic evaluation of summaries," in Text summarization branches out, 2004, pp. 74–81.
[289] P. Anderson et al., "Spice: Semantic propositional image caption evaluation," in Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14. Springer, 2016, pp. 382–398.
[290] K. Mangalam et al., "Egoschema: A diagnostic benchmark for very long-form video language understanding," arXiv preprint arXiv:2308.09126, 2023.
[291] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang et al., "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis," arXiv preprint arXiv:2405.21075, 2024.
[292] Y. Tang, J. Guo et al., "Vidcomposition: Can mllms analyze compositions in compiled videos?" arXiv preprint arXiv:2411.10979, 2024.
[293] J. Zhou et al., "Mlvu: A comprehensive benchmark for multi-task long video understanding," arXiv preprint arXiv:2406.04264, 2024.
[294] M. Ning, B. Zhu, Y. Xie, B. Lin, J. Cui, L. Yuan, D. Chen, and L. Yuan, "Video-bench: A comprehensive benchmark and toolkit for evaluating video-based large language models," arXiv preprint arXiv:2311.16103, 2023.
[295] X. Chen, Y. Lin, Y. Zhang, and W. Huang, "Autoeval-video: An automatic benchmark for assessing large vision language models in open-ended video question answering," ArXiv preprint, 2023.
[296] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou, "Tempcompass: Do video llms really understand videos?" arXiv preprint arXiv:2403.00476, 2024.
[297] Y. Li, X. Chen, B. Hu, L. Wang, H. Shi, and M. Zhang, "Videovista: A versatile benchmark for video understanding and reasoning," arXiv preprint arXiv:2406.11303, 2024.

[298] R. Rawal, K. Saifullah, R. Basri, D. Jacobs, G. Somepalli, and T. Goldstein, "Cinepile: A long video question answering dataset and benchmark," arXiv preprint arXiv:2405.08813, 2024.
[299] A. Wang et al., "Sok-bench: A situated video reasoning benchmark with aligned open-world knowledge," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13,384–13,394.
[300] R. Ghermi, X. Wang, V. Kalogeiton, and I. Laptev, "Short film dataset (sfd): A benchmark for story-level video understanding," arXiv preprint arXiv:2406.10221, 2024.
[301] L. Xu et al., "Beyond raw videos: Understanding edited videos with large multimodal model," arXiv preprint arXiv:2406.10484, 2024.
[302] K. Ataallah et al., "Infinibench: A comprehensive benchmark for large multimodal models in very long video understanding," arXiv preprint arXiv:2406.19875, 2024.
[303] X. He et al., "Mmworld: Towards multi-discipline multi-faceted world model evaluation in videos," arXiv preprint arXiv:2406.08407, 2024.
[304] D. Saravanan et al., "Velociti: Can video-language models bind semantic concepts through time?" arXiv preprint arXiv:2406.10889, 2024.
[305] J. Bi, Y. Tang et al., "Eagle: Egocentric aggregated language-video engine," in Proceedings of the 32nd ACM International Conference on Multimedia, ser. MM '24. New York, NY, USA: Association for Computing Machinery, 2024, p. 1682–1691. [Online]. Available: https://doi.org/10.1145/3664647.3681618
[306] Y. Tang, G. Zhan, L. Yang, Y. Liao, and C. Xu, "Cardiff: Video salient object ranking chain of thought reasoning for saliency prediction with diffusion," arXiv preprint arXiv:2408.12009, 2024.
[307] H. Hua, Y. Tang et al., "Mmcomposition: Revisiting the compositionality of pre-trained vision-language models," arXiv preprint arXiv:2410.09733, 2024.
[308] J. Bi, J. Guo, Y. Tang et al., "Unveiling visual perception in language models: An attention head analysis approach," arXiv preprint arXiv:2412.18108, 2024.
[309] L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, "End-to-end dense video captioning with masked transformer," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8739–8748.
[310] M. Kim et al., "Do you remember? dense video captioning with cross-modal memory retrieval," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13,894–13,904.
[311] Y. Guo, J. Liu, M. Li, Q. Liu, X. Chen, and X. Tang, "Trace: Temporal grounding video llm via causal event modeling," arXiv preprint arXiv:2410.05643, 2024.
[312] B. Huang et al., "Vtimellm: Empower llm to grasp video moments," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14,271–14,280.
[313] J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, "Git: A generative image-to-text transformer for vision and language," arXiv preprint arXiv:2205.14100, 2022.
[314] X. Zhou, A. Arnab, S. Buch, S. Yan, A. Myers, X. Xiong, A. Nagrani, and C. Schmid, "Streaming dense video captioning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18,243–18,252.
[315] O. or Author Name, "Gpt-4v: An overview," https://website.com/path-to-gpt-4v, 2023, accessed: 2023-xx-xx.
[316] P. Gao, J. Han, R. Zhang, Z. Lin, S. Geng, A. Zhou, W. Zhang, P. Lu, C. He, X. Yue et al., "Llama-adapter v2: Parameter-efficient visual instruction model," arXiv preprint arXiv:2304.15010, 2023.
[317] Y. Zhang, B. Li, h. Liu, Y. j. Lee, L. Gui, D. Fu, J. Feng, Z. Liu, and C. Li, "Llava-next: A strong zero-shot video understanding model," April 2024. [Online]. Available: https://llava-vl.github.io/blog/2024-04-30-llava-next-video/
[318] K. Mao et al., "Large language models know your contextual search intent: A prompting framework for conversational search," arXiv preprint arXiv:2303.06573, 2023.
[319] C. Ju et al., "Prompting visual-language models for efficient video understanding," in European Conference on Computer Vision. Springer, 2022, pp. 105–124.
[320] P. Jin, H. Li, Z. Cheng, J. Huang, Z. Wang, L. Yuan, C. Liu, and J. Chen, "Text-video retrieval with disentangled conceptualization and set-to-set alignment," arXiv preprint arXiv:2305.12218, 2023.
[321] P. Jin, H. Li, Z. Cheng, K. Li, X. Ji, C. Liu, L. Yuan, and J. Chen, "Diffusionret: Generative text-video retrieval with diffusion model," arXiv preprint arXiv:2303.09867, 2023.
[322] S. Xu, Y. Tang, and F. Zheng, "Launchpadgpt: Language model as music visualization designer on launchpad," arXiv preprint arXiv:2307.04827, 2023.

--- TRANG 21 ---
21

[323] L. Song et al., "Tacr-net: editing on deep video and voice portraits," in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 478–486.
[324] L. Song, G. Yin, Z. Jin, X. Dong, and C. Xu, "Emotional listener portrait: Neural listener head generation with emotion," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 20,839–20,849.
[325] L. Song, P. Liu, L. Chen, C. Liu, and C. Xu, "Tri 2-plane: Volumetric avatar reconstruction with feature pyramid," arXiv preprint arXiv:2401.09386, 2024.
[326] L. Song et al., "Fsft-net: face transfer video generation with few-shot views," in 2021 IEEE International Conference on Image Processing (ICIP). IEEE, 2021, pp. 3582–3586.
[327] Y. Song et al., "Objectstitch: Object compositing with diffusion model," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 18,310–18,319.
[328] S. Pramanick, Y. Song, S. Nag, K. Q. Lin, H. Shah, M. Z. Shou, R. Chellappa, and P. Zhang, "Egovlpv2: Egocentric video-language pre-training with fusion in the backbone," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023, pp. 5285–5297.
[329] W. Gan, Z. Qi, J. Wu, and J. C.-W. Lin, "Large language models in education: Vision and opportunities," arXiv preprint arXiv:2311.13160, 2023.
[330] L. Liu, L. Gao et al., "A survey on deep multi-modal learning for body language recognition and generation," arXiv preprint arXiv:2308.08849, 2023.
[331] M. De Coster et al., "Machine translation from signed to spoken languages: State of the art and challenges," Universal Access in the Information Society, pp. 1–27, 2023.
[332] M. K. Mishra, "Generating video game quests from stories," Master's thesis, University of Twente, 2023.
[333] S. Koomen, "Text generation for quests in multiplayer role-playing video games," Master's thesis, University of Twente, 2023.
[334] V. Soni, "Large language models for enhancing customer lifecycle management," Journal of Empirical Social Science Studies, vol. 7, no. 1, pp. 67–89, 2023.
[335] T. Medeiros, M. Medeiros, M. Azevedo, M. Silva, I. Silva, and D. G. Costa, "Analysis of language-model-powered chatbots for query resolution in pdf-based automotive manuals," Vehicles, vol. 5, no. 4, pp. 1384–1399, 2023.
[336] N. Gokce Narin, "The role of artificial intelligence and robotic solution technologies in metaverse design," in Metaverse: Technologies, Opportunities and Threats. Springer, 2023, pp. 45–63.
[337] T. Jung and M. C. tom Dieck, XR-Metaverse Cases: Business Application of AR, VR, XR and Metaverse. Springer Nature, 2023.
[338] Y. Yu, Z. Zeng, H. Hua, J. Fu, and J. Luo, "Promptfix: You prompt and we fix the photo," arXiv preprint arXiv:2405.16785, 2024.
[339] C. Huang, Y. Tian, A. Kumar, and C. Xu, "Egocentric audio-visual object localization," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 22,910–22,921.
[340] J. Bi, N. M. Nguyen, A. Vosoughi, and C. Xu, "Misar: A multimodal instructional system with augmented reality," in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023, pp. 1–5.
[341] T. Wang, J. Zhang, J. Fei, Y. Ge, H. Zheng, Y. Tang, Z. Li, M. Gao, S. Zhao, Y. Shan et al., "Caption anything: Interactive image description with diverse multimodal controls," arXiv preprint arXiv:2305.02677, 2023.
[342] Y. Hu, H. Hua, Z. Yang, W. Shi, N. A. Smith, and J. Luo, "Promptcap: Prompt-guided task-aware image captioning," arXiv preprint arXiv:2211.09699, 2022.
[343] K. Rana, J. Haviland et al., "Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning," in Conference on Robot Learning. PMLR, 2023, pp. 23–72.
[344] G. Eysenbach et al., "The role of chatgpt, generative language models, and artificial intelligence in medical education: a conversation with chatgpt and a call for papers," JMIR Medical Education, vol. 9, no. 1, p. e46885, 2023.
[345] H. Liu et al., "Beat: A large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis," in European conference on computer vision. Springer, 2022, pp. 612–630.
[346] H. Liu, N. Iwamoto, Z. Zhu, Z. Li, Y. Zhou, E. Bozkurt, and B. Zheng, "Disco: Disentangled implicit content and rhythm learning for diverse co-speech gestures synthesis," in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 3764–3773.

[347] H. Liu et al., "Emage: Towards unified holistic co-speech gesture generation via expressive masked audio gesture modeling," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 1144–1154.
[348] C. Li et al., "Llava-med: Training a large language-and-vision assistant for biomedicine in one day," arXiv preprint arXiv:2306.00890, 2023.
[349] M. Al-Hawawreh, A. Aljuhani, and Y. Jararweh, "Chatgpt for cybersecurity: practical applications, challenges, and future directions," Cluster Computing, vol. 26, no. 6, pp. 3421–3436, 2023.
[350] H. Mouratidis et al., "Modelling language for cyber security incident handling for critical infrastructures," Computers & Security, vol. 128, p. 103139, 2023.
[351] Y. Lee, J. Kim, and P. Kang, "Lanobert: System log anomaly detection based on bert masked language model," Applied Soft Computing, vol. 146, p. 110689, 2023.
[352] C. Almodovar et al., "Logfit: Log anomaly detection using fine-tuned language models," 2023.
[353] I. de Zarzà, J. de Curtò, and C. T. Calafate, "Socratic video understanding on unmanned aerial vehicles," Procedia Computer Science, vol. 225, pp. 144–154, 2023.
[354] J. Tang, Y. Yang et al., "Graphgpt: Graph instruction tuning for large language models," arXiv preprint arXiv:2310.13023, 2023.
[355] B. Cao et al., "Efficient masked autoencoder for video object counting and a large-scale benchmark," 2025. [Online]. Available: https://arxiv.org/abs/2411.13056
[356] C. Cui, Y. Ma et al., "Drive as you speak: Enabling human-like interaction with large language models in autonomous vehicles," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 902–909.
[357] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia, "Lisa: Reasoning segmentation via large language model," arXiv preprint arXiv:2308.00692, 2023.
[358] P. Zhou et al., "A survey on generative ai and llm for video generation, understanding, and streaming," arXiv preprint arXiv:2404.16038, 2024.
[359] H. Lin et al., "Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning," arXiv preprint arXiv:2309.15091, 2023.
[360] D. Kondratyuk et al., "Videopoet: A large language model for zero-shot video generation," arXiv preprint arXiv:2312.14125, 2023.
[361] Y. Jin et al., "Efficient multimodal large language models: A survey," arXiv preprint arXiv:2405.10739, 2024.
[362] Z. Lu et al., "B-vllm: A vision large language model with balanced spatio-temporal tokens," arXiv preprint arXiv:2412.09919, 2024.
[363] M. Hu et al., "Edge-based video analytics: A survey," arXiv preprint arXiv:2303.14329, 2023.
[364] Y. Yao et al., "Federated large language models: Current progress and future directions," arXiv preprint arXiv:2409.15723, 2024.
[365] A. Bastola et al., "Fedmil: Federated-multiple instance learning for video analysis with optimized dpp scheduling," in 2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT). IEEE, 2024, pp. 109–116.
[366] Y. Wang et al., "Fedvmr: A new federated learning method for video moment retrieval," in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.
[367] D. Zhang, W. Yao, X. Wang, Y. Hu, J. Luo, and D. Yu, "Multimedia-agent: A multimodal agent for multimedia content generation."
[368] J. Zhang et al., "Eventhallusion: Diagnosing event hallucinations in video llms," arXiv preprint arXiv:2409.16597, 2024.
[369] S.-H. Lee et al., "Video token merging for long-form video understanding," arXiv preprint arXiv:2410.23782, 2024.
[370] C. Tang et al., "Enhancing multimodal llm for detailed and accurate video captioning using multi-round preference optimization," arXiv preprint arXiv:2410.06682, 2024.
[371] Y. Shang et al., "Interpolating video-llms: Toward longer-sequence lmms in a training-free manner," arXiv preprint arXiv:2409.12963, 2024.

--- TRANG 22 ---
22

Yunlong Tang nhận bằng B.Eng. trong Khoa học và Công nghệ Thông minh từ Đại học Khoa học và Công nghệ Phương Nam (SUSTech) năm 2023, dưới sự hướng dẫn của Giáo sư Feng Zheng. Cô hiện đang theo học chương trình Tiến sĩ Khoa học Máy tính tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Chenliang Xu. Nghiên cứu của cô tập trung vào học đa phương thức, đặc biệt là hiểu và tạo sinh video.

Jing Bi hiện đang theo học chương trình Tiến sĩ Khoa học Máy tính tại Đại học Rochester từ năm 2020, dưới sự hướng dẫn của Giáo sư Chenliang Xu. Anh nhận bằng B.S. từ Đại học Shandong và M.S. từ Đại học Rochester.

Siting Xu nhận bằng B.Eng. (2019 - 2023) trong Khoa học và Công nghệ Máy tính từ Đại học Khoa học và Công nghệ Phương Nam (SUSTech), dưới sự hướng dẫn của Giáo sư Feng Zheng.

Luchuan Song hiện là nghiên cứu sinh Tiến sĩ Khoa học Máy tính tại Đại học Rochester. Anh nhận bằng M.S. và B.S. từ Đại học Khoa học và Công nghệ Trung Quốc.

Susan Liang hiện là nghiên cứu sinh Tiến sĩ tại Khoa Khoa học Máy tính, Đại học Rochester. Nghiên cứu của anh tập trung vào học đa phương thức.

Teng Wang hiện là nghiên cứu sinh Tiến sĩ tại Khoa Khoa học Máy tính, Đại học Hồng Kông. Anh nhận bằng cử nhân và thạc sĩ từ Đại học Trung Sơn vào năm 2017 và 2020, tương ứng. Sở thích nghiên cứu của anh nằm trong học đa phương thức thị giác-ngôn ngữ và hiểu video.

Daoan Zhang hiện là Sinh viên Tiến sĩ Khoa học Máy tính tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Jiebo Luo. Nghiên cứu của anh tập trung vào AI sinh tạo.

Jie An là nghiên cứu sinh Tiến sĩ Khoa học Máy tính tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Jiebo Luo. Trước đó, anh nhận bằng B.S. (2012 - 2016) và M.S. (2016 - 2019) trong Toán học Ứng dụng từ Đại học Bắc Kinh, dưới sự hướng dẫn của Giáo sư Jinwen Ma. Nghiên cứu của Jie tập trung vào cải thiện hiệu suất và mở rộng khả năng của các mô hình GenAI. Anh đặc biệt quan tâm đến chuyển đổi phong cách hình ảnh, mô hình sinh tạo, tạo sinh hình ảnh/video và tạo sinh/đánh giá đa phương thức.

Jingyang Lin là sinh viên Tiến sĩ chuyên ngành Khoa học Máy tính tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Jiebo Luo. Anh nhận bằng BE và MSc từ Đại học Trung Sơn (SYSU), Quảng Châu, Trung Quốc, vào năm 2019 và 2022, tương ứng. Sở thích nghiên cứu của anh bao gồm học đa phương thức với LLM, AI cho sức khỏe và học tự giám sát.

Rongyi Zhu hiện là sinh viên Tiến sĩ Khoa học Máy tính tại Đại học Stony Brook. Anh nhận bằng MS. trong Khoa học Máy tính từ Đại học Rochester vào năm 2024. Nghiên cứu của anh tập trung vào AI đáng tin cậy.

Ali Vosoughi là nghiên cứu sinh Tiến sĩ ngành Kỹ thuật Điện và Máy tính tại Đại học Rochester, làm việc với Giáo sư Chenliang Xu và Axel Wismueller. Nghiên cứu của anh tập trung vào việc sử dụng AI cho đa phương thức và lý luận phức tạp để hỗ trợ con người với các nhiệm vụ thách thức. Anh có bằng Cử nhân Kỹ thuật Điện từ Đại học Công nghệ Sharif (Iran), và hai bằng Thạc sĩ—một từ Đại học Bogazici (Thổ Nhĩ Kỳ) và một từ Đại học Rochester (Hoa Kỳ).

Chao Huang hiện là nghiên cứu sinh Tiến sĩ Khoa học Máy tính tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Chenliang Xu. Trước đó, anh đã dành một năm làm trợ lý nghiên cứu tại Đại học Trung Văn Hồng Kông, làm việc với Giáo sư Chi-Wing Fu. Anh nhận bằng B.Eng. từ Khoa ESE, Đại học Nam Kinh vào năm 2019.

Zeliang Zhang nhận bằng B.Eng. trong khoa học máy tính từ Đại học Khoa học và Công nghệ Hoa Trung vào năm 2022. Anh hiện là nghiên cứu sinh Tiến sĩ Khoa học Máy tính tại Đại học Rochester. Nghiên cứu của anh tập trung vào các phương pháp học sâu đáng tin cậy và hiệu quả.

Pinxin Liu là sinh viên Tiến sĩ tại Đại học Rochester, dưới sự hướng dẫn của Giáo sư Chenliang Xu. Trước đó, anh là Thực tập sinh Khoa học Nghiên cứu tại Flawless AI. Anh nhận bằng B.S. từ Khoa Khoa học Máy tính tại Đại học Rochester. Sở thích nghiên cứu của anh nằm trong các chủ đề liên quan đến con người, ví dụ như tổng hợp cử chỉ video, rendering khuôn mặt 3D và tạo sinh chuyển động từ văn bản.

Mingqian Feng nhận bằng B.S. trong Vật lý từ Đại học Khoa học và Công nghệ Trung Quốc (USTC) và bằng M.S.E. trong toán học tài chính từ Đại học Johns Hopkins. Anh hiện là sinh viên Tiến sĩ Khoa học Máy tính tại Đại học Rochester. Nghiên cứu của anh tập trung vào thiên vị, tối ưu hóa và hiểu video.

Feng Zheng là Phó Giáo sư tại Đại học Khoa học và Công nghệ Phương Nam (SUSTech). Sở thích nghiên cứu của ông bao gồm học máy (ML), thị giác máy tính (CV) và tương tác người-máy (HCI). Ông nhận bằng Tiến sĩ từ Đại học Sheffield, Vương quốc Anh. Trước khi gia nhập SUSTech, ông làm việc như một nhà nghiên cứu cao cấp tại Phòng thí nghiệm Tencent YouTu ở Thượng Hải, Trung Quốc. Trước đó, ông làm việc như một nhà nghiên cứu sau tiến sĩ tại Đại học Pittsburgh, Hoa Kỳ và như một phó giáo sư nghiên cứu tại Viện Công nghệ Tiên tiến Thâm Quyến, CAS. Về nghiên cứu học thuật, ông đã xuất bản 85 bài báo tại các tạp chí và hội nghị quốc tế hàng đầu, bao gồm IEEE TPAMI/TITS/TIP, AAAI, NeuIPS, CVPR, và ECCV.

Jianguo Zhang là Giáo sư tại Khoa Khoa học và Kỹ thuật Máy tính, Đại học Khoa học và Công nghệ Phương Nam. Trước đây, ông là Reader in Computing, Trường Khoa học và Kỹ thuật, Đại học Dundee, Vương quốc Anh. Ông nhận bằng Tiến sĩ từ Phòng thí nghiệm Quốc gia Nhận dạng Mẫu, Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc, năm 2002. Sở thích nghiên cứu của ông bao gồm nhận dạng đối tượng, phân tích hình ảnh y tế, học máy và thị giác máy tính. Ông là thành viên cao cấp của IEEE và Biên tập viên Liên kết của IEEE Transactions on Multimedia.

Ping Luo là Phó Giáo sư tại Khoa Khoa học Máy tính, Đại học Hồng Kông, Phó Giám đốc Viện Khoa học Dữ liệu Quỹ HKU Musketeers, và Phó Giám đốc Phòng thí nghiệm Nghiên cứu Chung của HKU và Phòng thí nghiệm AI Thượng Hải. Ông nhận bằng Tiến sĩ Kỹ thuật Thông tin từ Đại học Trung Văn Hồng Kông năm 2014, dưới sự hướng dẫn của Giáo sư Xiaoou Tang và Giáo sư Xiaogang Wang. Trước khi gia nhập HKU năm 2019, ông là Giám đốc Nghiên cứu tại SenseTime. Ông đã xuất bản hơn 100 bài báo tại các hội nghị và tạp chí hàng đầu, với hơn 50.000 lượt trích dẫn trên Google Scholar. Các giải thưởng của ông bao gồm Giải thưởng Bài báo Dễ tiếp cận AAAI 2015, Giải thưởng Bài báo Xuất sắc ACL 2022, Giải thưởng Bài báo Xuất sắc WAIC 2023, và Đề cử Giải thưởng Bài báo Hay nhất ICCV'23. Năm 2020, ông được vinh danh là một trong những Nhà đổi mới Dưới 35 tuổi của MIT Technology Review tại Châu Á-Thái Bình Dương. Ông đã hướng dẫn 30 sinh viên Tiến sĩ, nhiều người trong số họ đã giành được các giải thưởng danh giá như Học bổng Nvidia và Baidu.

Jiebo Luo là Giáo sư Albert Arendt Hopeman của Kỹ thuật tại Đại học Rochester, nơi ông gia nhập năm 2011 sau hơn 15 năm tại Phòng thí nghiệm Nghiên cứu Kodak. Ông đã đóng vai trò quan trọng trong nhiều hội nghị, phục vụ như Đồng Chủ tịch Tổng quát của ACM Multimedia 2018 và IEEE ICME 2024, và Đồng Chủ tịch Chương trình của ACM Multimedia 2010, IEEE CVPR 2012, và IEEE ICIP 2017. Ông đã phục vụ trong ban biên tập của một số tạp chí lớn, bao gồm IEEE Transactions on Pattern Analysis and Machine Intelligence và IEEE Transactions on Multimedia, và là Tổng biên tập của IEEE Transactions on Multimedia từ năm 2020 đến 2022. Giáo sư Luo là Fellow của ACM, AAAI, IEEE, AIMBE, IAPR, và SPIE, và là thành viên của Academia Europaea (AE) và Viện Hàn lâm Quốc gia Nhà phát minh Hoa Kỳ (NAI). Các danh hiệu của ông bao gồm Giải thưởng Thành tựu Kỹ thuật Edward J. McClusky 2024 của IEEE Computer Society, Giải thưởng Thành tựu Kỹ thuật ACM SIGMM 2021, Giải thưởng Đại học William H. Riker 2024 cho Xuất sắc trong Giảng dạy Sau đại học, Giải thưởng Giảng viên Xuất sắc Edmund A. Hajim 2024, và Giải thưởng Xuất sắc Nghiên cứu Debra Haring 2024 đầu tiên.

Chenliang Xu là Phó Giáo sư tại Khoa Khoa học Máy tính, Đại học Rochester. Ông nhận bằng Tiến sĩ Khoa học Máy tính từ Đại học Michigan năm 2016, bằng M.S. Khoa học Máy tính từ Đại học tại Buffalo năm 2012, và bằng B.S. trong Khoa học Thông tin và Tính toán từ Đại học Hàng không Vũ trụ Nam Kinh, Trung Quốc, năm 2010. Nghiên cứu của ông bắt nguồn từ thị giác máy tính và giải quyết các chủ đề liên ngành, bao gồm hiểu video, học nghe nhìn, thị giác và ngôn ngữ, và các phương pháp cho AI đáng tin cậy. Xu là người nhận Giáo sư Danh dự James P. Wilmot (2021), Giải thưởng Nghiên cứu Đại học Rochester (2021), Giải thưởng Bài báo Hay nhất tại Hội nghị ACM SIGGRAPH VRCAI lần thứ 17 (2019), Giải thưởng Bài báo Hay nhất tại Hội nghị Tính toán Âm thanh và Âm nhạc lần thứ 14 (2017), và Giải thưởng Thí điểm AR/VR Đại học Rochester (2017). Ông đã là tác giả của hơn 100 bài báo được bình duyệt tại các địa điểm thị giác máy tính, học máy, đa phương tiện và AI. Ông đã phục vụ như biên tập viên liên kết cho IEEE Transactions on Multimedia và chủ tịch khu vực/người đánh giá cho các hội nghị quốc tế khác nhau.
