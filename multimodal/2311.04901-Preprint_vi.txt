# 2311.04901.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.04901.pdf
# Kích thước tệp: 6127735 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Preprint
GENOME: SUY LUẬN THỊ GIÁC NEURO-SYMBOLIC SINH TẠO BẰNG CÁCH PHÁT TRIỂN VÀ TÁI SỬ DỤNG CÁC MODULE
Zhenfang Chen∗
MIT-IBM Watson AI LabRui Sun∗
Columbia UniversityWenjun Liu∗
Tsinghua University
Yining Hong
University of California, Los AngelesChuang Gan
MIT-IBM Watson AI Lab and UMass Amherst

TÓM TẮT
Các nghiên cứu gần đây đã cho thấy rằng các Mô hình Ngôn ngữ Lớn (LLMs) có thể trao quyền cho các mô hình neuro-symbolic truyền thống thông qua khả năng lập trình để dịch ngôn ngữ thành mô tả module, do đó đạt được kết quả suy luận thị giác mạnh mẽ trong khi vẫn duy trì tính minh bạch và hiệu quả của mô hình. Tuy nhiên, các mô hình này thường tạo ra toàn bộ đoạn mã một cách đầy đủ cho từng trường hợp mới của một nhiệm vụ, điều này cực kỳ không hiệu quả. Ngược lại, con người dần dần thu thập kiến thức có thể được tái sử dụng và phát triển thành các kỹ năng sâu sắc hơn để tổng quát hóa nhanh chóng cho các nhiệm vụ mới từ khi chúng ta còn là trẻ sơ sinh. Được lấy cảm hứng từ điều này, chúng tôi đề xuất suy luận thị giác neuro-symbolic sinh tạo bằng cách phát triển và tái sử dụng các module. Cụ thể, mô hình của chúng tôi bao gồm ba giai đoạn độc đáo: khởi tạo module, tạo module và thực thi module. Đầu tiên, với một nhiệm vụ thị giác-ngôn ngữ, chúng tôi sử dụng LLMs để kiểm tra xem liệu chúng ta có thể tái sử dụng và phát triển dựa trên các module đã thiết lập để xử lý nhiệm vụ mới này hay không. Nếu không, chúng tôi khởi tạo một module mới cần thiết cho nhiệm vụ và chỉ định đầu vào và đầu ra của module mới này. Sau đó, module mới được tạo ra bằng cách truy vấn LLMs để sinh ra các đoạn mã tương ứng phù hợp với các yêu cầu. Để hiểu rõ hơn về khả năng của module mới, chúng tôi coi các ví dụ huấn luyện few-shot như các trường hợp kiểm tra để xem liệu module mới của chúng tôi có thể vượt qua các trường hợp này hay không. Nếu có, module mới được thêm vào thư viện module để tái sử dụng trong tương lai. Cuối cùng, chúng tôi đánh giá hiệu suất của mô hình trên tập kiểm tra bằng cách thực thi các chương trình đã phân tích với các module thị giác mới tạo để có kết quả. Chúng tôi thấy rằng mô hình đề xuất sở hữu một số ưu điểm. Đầu tiên, nó hoạt động cạnh tranh trên các nhiệm vụ tiêu chuẩn như trả lời câu hỏi thị giác và hiểu biểu thức tham chiếu; Thứ hai, các module học được từ một nhiệm vụ có thể được chuyển giao một cách liền mạch sang các nhiệm vụ mới; Cuối cùng nhưng không kém phần quan trọng, nó có thể thích ứng với các nhiệm vụ suy luận thị giác mới bằng cách quan sát một số ít ví dụ huấn luyện và tái sử dụng các module1.

1 GIỚI THIỆU
Các mô hình suy luận thị giác neuro-symbolic (Andreas et al., 2016b; Mao et al., 2019b) đề cập đến họ thuật toán kết hợp các mạng nơ-ron sâu (lec, 1998; Hochreiter & Schmidhuber, 1997) để học các tương quan giữa dữ liệu huấn luyện và các phương pháp symbolic (Yi et al., 2018; Andreas et al., 2016a) để thực hiện suy luận đa bước rõ ràng và minh bạch. Trái ngược với các mô hình dựa hoàn toàn trên mạng nơ-ron (Hudson & Manning, 2018; Li et al., 2023), các phương pháp neuro-symbolic đạt được hiệu suất mạnh mẽ trong các nhiệm vụ suy luận thị giác, đồng thời cung cấp tính minh bạch và hiệu quả dữ liệu vượt trội của mô hình.

Tuy nhiên, các mô hình như vậy gặp phải một số hạn chế cố hữu. Thứ nhất, các trình phân tích ngôn ngữ của chúng (Yi et al., 2018; Andreas et al., 2016b), được sử dụng để chuyển đổi từ ngôn ngữ tự nhiên thành các chương trình symbolic, thường đòi hỏi các cặp ngôn ngữ-chương trình đặc thù cho miền rộng lớn để huấn luyện, và gặp khó khăn trong việc tổng quát hóa hiệu quả cho các hướng dẫn ngôn ngữ tự nhiên không bị ràng buộc. Ngoài ra, các mô hình này đòi hỏi thiết kế tùy chỉnh cho mỗi module, làm cho quá trình tốn nhiều công sức và thiếu khả năng mở rộng.

∗biểu thị đóng góp bằng nhau
1Trang dự án: https://vis-www.cs.umass.edu/genome

--- TRANG 2 ---
Preprint
def execute_command (image):
image_patch = ImagePatch (image)
couch_patches = image_patch.find ("couch")
color1 = couch_patch.simple_query (f"What's
the couch's color?")
sky_patches = image_patch.find ("sky")
color2 = couch_patch.simple_query (f"What's
the sky's color?")
if color1==color2:
return "yes"
else return "No"BOX0= LOC (image= IMAGE,object ='couch')
IMAGE0= CROP (image= IMAGE,box =BOX0)
BOX1= LOC (image= IMAGE,object ='sky') 
IMAGE1= CROP (image= IMAGE,box =BOX1)
ANSWER0= VQA (image=IMAGE0,question=
'What color is the couch?')
ANSWER1= VQA (image=IMAGE1,question=
'What color is the sky')
ANSWER2= EV AL (expr= f"'yes ' if {ANSWER0}
=={ANSWER1} else 'no'")Q: Are the couch and 
the sky the same 
color ?
A: No.
Q: Does the microwave have 
a different color than the 
toaster?LLMThư viện Module 
Cố định
(A) VisProg (B) ViperGPT
(C) Học Module Sinh tạo(D) Ứng dụng
A: No.
Hướng dẫn: 
thay thế quả táo 
có cùng màu với 
quả nho bằng 
quả đào
Module 1:
LOC(…)
Module 2:
VQA(…)
Module  3:
CROP(…)
…
Phương pháp hiện tại
Phương pháp của chúng tôiSuy luận thị giác neuro-symbolic
Module 1:
LOC(…)
…
Module mới:
Class COMPARE_ATTRIBUTE(  ):
""" Đầu vào: 
image: một hình ảnh
box1: một danh sách hộp
attribute:
……
Đầu ra:
True/False 
""" Cập nhật

Hình 1: Động lực của GENOME. So với VisProg và ViperGPT tạo ra một cách đầy đủ một đoạn mã cho mỗi trường hợp đầu vào, GENOME của chúng tôi có thể tạo ra các module mới và tái sử dụng các module cũ để xử lý truy vấn. Module được tạo ra bởi GENOME có thể được sử dụng để xử lý các trường hợp khác của nhiệm vụ để có hiệu suất tốt hơn. Thứ hai, module được tạo ra có thể được chuyển giao sang các nhiệm vụ khác nhau như chỉnh sửa hình ảnh. Cuối cùng, nó có thể học cách xử lý các nhiệm vụ mới như Raven (Burke, 1985; Zhang et al., 2019) bằng cách học các module chỉ từ một số ít mẫu huấn luyện. Vùng được chỉnh sửa và câu trả lời đúng cho nhiệm vụ Raven được đánh dấu bằng hộp màu đỏ để hình dung tốt hơn.

Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020; Ouyang et al., 2022) đã mở ra một kỷ nguyên mới với hiệu suất đáng chú ý của chúng trên nhiều ứng dụng khác nhau, bao gồm chatbot (Shuster et al., 2022), trợ lý ảo (Dong et al., 2023), và trợ lý lập trình (Chen et al., 2021a). Lướt trên làn sóng chưa từng có này, các nhà nghiên cứu đã tái công thức hóa trí tuệ cũ bằng cách kết hợp LLMs vào suy luận neuro-symbolic, bỏ qua tính không linh hoạt và không hiệu quả của các trình phân tích ngôn ngữ-chương trình đặc thù cho miền. Cụ thể, VisProg (Gupta & Kembhavi, 2022) định nghĩa trước một tập hợp các module thị giác và sử dụng LLMs để chuyển đổi hướng dẫn ngôn ngữ thành các chương trình symbolic bao gồm các module thị giác định nghĩa trước. Tiến thêm một bước, ViperGPT (Surís et al., 2023) giải phóng gánh nặng về các module thị giác được định nghĩa thủ công bằng cách giới thiệu một bộ tạo mã có thể tạo ra một đoạn mã dựa trên mỗi trường hợp đầu vào của một nhiệm vụ mới.

Mặc dù có tiềm năng như các mô hình neuro-symbolic dựa trên LLM này, chúng không thể tránh khỏi một số điểm yếu so với các quá trình học tập và suy luận của con người. Đầu tiên, cả VisProg và ViperGPT đều tạo ra một cách đầy đủ một đoạn mã cho mỗi trường hợp mới của một nhiệm vụ, điều này cực kỳ không hiệu quả. Điều này trái ngược hoàn toàn với quá trình học tập của con người: từ khi còn nhỏ, chúng ta tích lũy kiến thức một cách tự nhiên từ các trải nghiệm cụ thể. Kiến thức như vậy có được từ các trường hợp cụ thể có thể được tái sử dụng và tái cấu hình, cho phép chúng ta nhanh chóng thích ứng với các nhiệm vụ mới và các yêu cầu mới (Harlow, 1949; Mitchell et al., 1986; Lake et al., 2016; Ellis et al., 2023). Các khối kiến thức phát triển dần dần theo thời gian, dần dần thành một thư viện với độ phong phú và linh hoạt phi thường để tổng quát hóa nhanh chóng cho bất kỳ nhiệm vụ chưa thấy nào - thư viện kiến thức mà các mô hình này thiếu. Thứ hai, cả hai mô hình đều không xác minh và kiểm tra các mã chúng tạo ra. Có vẻ như khi các mô hình tạo ra một đoạn mã tệ không thể giải quyết trường hợp đầu vào, chúng chỉ "để nó đi" mà không thử lại để có cơ hội thành công lớn hơn. Và tất nhiên, khi chúng gặp lại các trường hợp tương tự, chúng tiếp tục "giẫm vào cùng một cái cào". Con người, mặt khác, sẽ xác minh và kiểm tra kiến thức thu được bằng cách đề xuất một tập hợp các kịch bản kiểm tra trước khi lưu trữ chúng trong thư viện (Brulé & Blount, 1989). Rất quan trọng là một mô hình suy luận neuro-symbolic được trang bị cùng khả năng để xác minh các mã nó tạo ra, lưu trữ chúng trong thư viện nếu thỏa mãn, và thử lại khi các mã thất bại.

Vì vậy, chúng tôi giới thiệu một Mô hình Suy luận Thị giác Neuro-symbolic Sinh tạo mới (GENOME), thành thạo trong việc hấp thụ các module nơ-ron mới từ một tập hợp hạn chế các ví dụ huấn luyện. Mô hình này

--- TRANG 3 ---
Preprint
xuất sắc trong việc xử lý các nhiệm vụ suy luận thị giác tiêu chuẩn như trả lời câu hỏi thị giác. Ngoài ra, nó thể hiện khả năng chuyển giao module xuất sắc cho các nhiệm vụ như chỉnh sửa hình ảnh, và có khả năng đặc biệt trong việc tổng quát hóa các nhiệm vụ suy luận mới với các ví dụ huấn luyện hạn chế. Như được minh họa trong Hình 2, GENOME bao gồm ba giai đoạn: 1) khởi tạo module, 2) tạo module, và 3) thực thi module. Từ các ví dụ tập huấn luyện ban đầu, một LLM nhận biết sự cần thiết của một module mới để giải quyết nhiệm vụ và, nếu cần, tạo ra đầu vào và đầu ra tương ứng. Trong giai đoạn thứ hai, LLMs thực hiện và tinh chỉnh module mới, đảm bảo tích hợp liền mạch với các module hiện có và dẫn đến các phản hồi chính xác cho các truy vấn huấn luyện. Trong quá trình kiểm tra, LLM đầu tiên chuyển đổi hướng dẫn ngôn ngữ thành các chương trình cấp cao có thể thực thi như COMPARE_ATTRIBUTE(IMAGE,BOX0,BOX1,ATTR) để so sánh các thuộc tính của các hộp giới hạn khác nhau. Chương trình sẽ được chạy với các tập module mới, tạo ra các đầu ra mong muốn.

Chúng tôi đã đánh giá hiệu suất của GENOME trên sáu nhiệm vụ suy luận thị giác, từ trả lời câu hỏi thị giác (Hudson & Manning, 2019) đến Ma trận Tiến triển Raven (Zhang et al., 2019). Các kết quả thí nghiệm tiết lộ rằng GENOME mang lại kết quả cạnh tranh trên các điểm chuẩn tiêu chuẩn, đảm bảo cả tính minh bạch và khả năng tương tác. Đáng chú ý, các module được mài giũa trên các nhiệm vụ tiêu chuẩn này có thể được thích ứng khéo léo với các lĩnh vực đa dạng, bao gồm chỉnh sửa hình ảnh và gắn thẻ kiến thức (Gupta & Kembhavi, 2022). Ngoài ra, với các ví dụ huấn luyện tối thiểu, GENOME thể hiện khả năng quản lý các nhiệm vụ suy luận thị giác mới (Burke, 1985; Jiang et al., 2023) bằng cách tái sử dụng các module. Mã và dữ liệu của chúng tôi sẽ được công khai.

2 CÔNG TRÌNH LIÊN QUAN
Suy luận thị giác. Công trình của chúng tôi nhằm xử lý các nhiệm vụ suy luận thị giác, đòi hỏi mô hình rút ra các suy luận mới dựa trên các gợi ý thị giác thu được trong hình ảnh hoặc video (Hudson & Manning, 2019; Kazemzadeh et al., 2014; Goyal et al., 2017; Zhang et al., 2019; Jiang et al., 2023). Các nhiệm vụ điển hình cho suy luận thị giác bao gồm trả lời câu hỏi thị giác có thể nhìn thấy (Goyal et al., 2017; Hudson & Manning, 2019), nền tảng thị giác (Kazemzadeh et al., 2014; Yu et al., 2016; Chen et al., 2020) và Ma trận Tiến triển Raven (Burke, 1985; Zhang et al., 2019). Nhiều mô hình khác nhau (Hudson & Manning, 2018; Yu et al., 2018; Zhang et al., 2021; Ding et al., 2023) đã được phát triển để xử lý các nhiệm vụ này nhưng hầu hết chúng đều là ad-hoc và được thiết kế cẩn thận cho một nhiệm vụ cụ thể, khiến việc xây dựng một mô hình tổng quát có thể xử lý các loại vấn đề suy luận thị giác khác nhau chỉ bằng cách hiển thị một số ít ví dụ vẫn là một câu hỏi nghiên cứu mở.

Suy luận thị giác neuro-symbolic. Công trình của chúng tôi cũng liên quan chặt chẽ đến các mô hình suy luận thị giác neuro-symbolic (Andreas et al., 2016b; Mao et al., 2019a; Chen et al., 2021c; 2022), trong đó các mô hình phân tách truy vấn của các nhiệm vụ suy luận thị giác thành một chuỗi các bước suy luận và biểu diễn mỗi bước suy luận bằng một module nơ-ron (tức là một đoạn mã để đạt được các chức năng cụ thể như định vị đối tượng và nhận dạng danh mục đối tượng). Trong khi các mô hình này có khả năng tương tác mô hình tốt hơn và hiệu quả dữ liệu so với các mô hình kết nối trước đó (Hudson & Manning, 2018; Anderson et al., 2018), chúng thường cho thấy những hạn chế trong việc biểu diễn hướng dẫn ngôn ngữ tự nhiên trong tự nhiên với các bước suy luận được định nghĩa trước hạn chế (Yang et al., 2020; Chen et al., 2021b). Hơn nữa, chúng cần định nghĩa và thực hiện thủ công từng module nơ-ron từng cái một, khiến việc mở rộng quy mô và xử lý nhiều nhiệm vụ trong một mô hình duy nhất trở nên khó khăn.

Các mô hình nền tảng cho suy luận. Gần đây, các mô hình ngôn ngữ lớn (LLMs) (Brown et al., 2020; Ouyang et al., 2022) đã được sử dụng rộng rãi trong hiểu biết ngôn ngữ (Hendrycks et al., 2020) và suy luận (Cobbe et al., 2021; Amini et al., 2019). Schick et al. (2023) phát triển toolformer để cho thấy rằng LLMs có thể sử dụng các công cụ bên ngoài để xử lý tốt hơn các nhiệm vụ ngôn ngữ. Cai et al. (2023) cho thấy rằng LLMs có thể tạo ra các công cụ đơn giản cho các nhiệm vụ ngôn ngữ tự nhiên bằng cách viết các đoạn mã. LLMs cũng đã được sử dụng trong các nhiệm vụ thị giác-ngôn ngữ. Hầu hết các công trình này (Li et al., 2023; Alayrac et al., 2022) kết nối LLMs với các bộ mã hóa thị giác bổ sung và tinh chỉnh chúng với các cặp thị giác-ngôn ngữ lớn. Như được đánh giá bởi Xu et al. (2023b), trong khi các mô hình này cho thấy hiệu suất tuyệt vời trên các nhiệm vụ trong miền, chúng cũng hoạt động kém trên các nhiệm vụ ngoài miền huấn luyện. Chúng cũng cực kỳ tốn kém về tính toán. Ví dụ, cần 15 ngày để sử dụng 1536 TPUv4 để huấn luyện Flamingo (Alayrac et al., 2022).

Lập trình thị giác bằng LLMs. Một hướng nghiên cứu khác đã kết hợp các mô hình thị giác (Li* et al., 2022; Kirillov et al., 2023; Radford et al., 2021) với LLMs theo cách off-the-shelf. Các mô hình đầu tiên (Yang et al., 2022; Chen et al., 2023b) chuyển đổi hình ảnh thành chú thích và thêm chú thích vào prompt của LLMs để xử lý các nhiệm vụ thị giác-ngôn ngữ. Trong khi phương pháp này đơn giản, chúng cũng hoạt động kém hơn và thiếu tính minh bạch. Gần đây, VisPROG (Gupta & Kembhavi, 2022) sử dụng LLMs để chuyển đổi hướng dẫn ngôn ngữ thành các hoạt động mô-đun được định nghĩa trước cho suy luận từng bước. Tuy nhiên, nó vẫn đòi hỏi thực hiện thủ công từng module từng cái một. Sau đó, ViperGPT (Surís et al., 2023) cho thấy rằng LLMs có thể được sử dụng để viết một đoạn mã cho mỗi trường hợp truy vấn một cách độc lập để xử lý các nhiệm vụ thị giác. Tuy nhiên, mã mà nó viết đã không được kiểm tra và thử nghiệm bởi bất kỳ ví dụ huấn luyện nào và không có bảo đảm về hiệu suất và an toàn mã. Thay vào đó, chúng tôi đề xuất GENOME yêu cầu LLMs tạo ra các mô hình nơ-ron mới (tức là các đoạn mã tổng quát để đạt được các chức năng cụ thể) và xử lý các nhiệm vụ đã cho chỉ thông qua một số ít ví dụ huấn luyện. Các module mới được tạo ra như vậy có thể hợp tác với nhau và được tái sử dụng cho các nhiệm vụ khác để có hiệu suất tốt hơn.

3 PHƯƠNG PHÁP
3.1 TỔNG QUAN
Trong phần này, chúng tôi trình bày một khung công tác mới được gọi là Mô hình Suy luận Thị giác Neuro-symbolic Sinh tạo (GENOME) để thu thập các module nơ-ron và giải quyết các nhiệm vụ suy luận thị giác chỉ với một tập hợp hạn chế các ví dụ huấn luyện. GENOME bao gồm một số toán tử được định nghĩa trước phục vụ như các khối xây dựng ban đầu. Mỗi toán tử nơ-ron tương ứng với một module nơ-ron và được thực hiện bằng một đoạn mã Python, do đó cho phép các chức năng cụ thể như định vị đối tượng trong một hình ảnh. Tuy nhiên, không thể định nghĩa trước tất cả các module nơ-ron cần thiết trước khi giải quyết các nhiệm vụ suy luận thị giác. Do đó, xuất hiện nhu cầu tạo ra các module mới dựa trên một số lượng hạn chế các ví dụ nhiệm vụ suy luận thị giác.

Hình 2 minh họa rằng GENOME bao gồm ba giai đoạn riêng biệt: 1) khởi tạo module, 2) tạo module, và 3) thực thi module. Trong giai đoạn khởi tạo module, khi được cung cấp một tập hợp nhỏ các ví dụ huấn luyện từ bộ dữ liệu suy luận thị giác, mục tiêu chính là xác định xem các module nơ-ron hiện có có đủ để giải quyết các ví dụ truy vấn hay không. Nếu các module hiện có không đủ cho trường hợp truy vấn, GENOME sẽ xác định các yêu cầu để tạo ra các module mới, bao gồm việc định nghĩa các thông số kỹ thuật đầu vào và đầu ra của các module. Trong giai đoạn tạo module, GENOME tận dụng LLM để thực hiện module nơ-ron dựa trên các ví dụ huấn luyện được cung cấp và định dạng đầu vào và đầu ra được chỉ định (tức là chữ ký hàm) và chỉ thêm module vào thư viện module khi nó vượt qua các trường hợp kiểm tra. Một khi module mới được thực hiện thành công, việc thực thi module sẽ điều phối việc chuyển đổi các truy vấn đầu vào thành một chuỗi các hoạt động nơ-ron. Tiếp theo, các hoạt động này được áp dụng cho các module nơ-ron để có được đầu ra chính xác. Tất cả ba giai đoạn này được hỗ trợ bởi khả năng tạo mã mạnh mẽ và kỹ thuật học trong ngữ cảnh. Các prompt cho từng giai đoạn có thể được tìm thấy tại Hình 15-17

3.2 CHI TIẾT MÔ HÌNH
Sử dụng một số lượng hạn chế các ví dụ từ tập huấn luyện của các nhiệm vụ suy luận thị giác, chúng tôi áp dụng khung GENOME, bao gồm ba giai đoạn riêng biệt: khởi tạo module, tạo module và thực thi module.

Khởi tạo module. Giai đoạn đầu tiên trong khung GENOME của chúng tôi là khởi tạo module, dành riêng cho việc xác định tập hợp các module mới cần thiết để giải quyết nhiệm vụ suy luận thị giác. Như được mô tả trong Hình 2-1, chúng tôi sử dụng một LLM để đánh giá tính khả thi của việc xử lý các trường hợp huấn luyện này bằng các module nơ-ron hiện có. Nếu điều này không đạt được, chúng tôi giao nhiệm vụ cho LLM chỉ định các module mới cần thiết (ví dụ COMPARE_ATTRIBUTE trong Hình 2) để có câu trả lời chính xác cho truy vấn. Kết quả của giai đoạn này bao gồm các chữ ký hàm chi tiết về định dạng đầu vào và đầu ra cho các module mới này. Hơn nữa, nó tạo điều kiện cho việc chuyển đổi truy vấn đầu vào thành một chuỗi các bước suy luận, hoạt động như các trường hợp kiểm tra để xác thực tính chính xác của chương trình được tạo ra trong tạo module. Prompt cho LLM trong giai đoạn này được hiển thị trong Hình 15.

Tạo module. Giai đoạn thứ hai của khung GENOME của chúng tôi là tạo module, tập trung vào việc thực hiện các module mới chính xác được đề xuất trong giai đoạn khởi tạo module. Cụ thể, sau khi nhận được chữ ký của một module mới, chúng tôi kết hợp các trường hợp kiểm tra tương ứng vào prompt và sử dụng các kỹ thuật học-trong-ngữ-cảnh để tạo ra nhiều ứng cử viên chương trình. Các ứng cử viên chương trình này sau đó được thực thi bằng các ví dụ huấn luyện được cung cấp. Nếu một chương trình gặp lỗi trong quá trình thực thi, chúng tôi kết hợp thông tin lỗi vào prompt của LLM và hướng dẫn nó khắc phục các vấn đề này. Chúng tôi chỉ chấp nhận các ứng cử viên chương trình đạt được tỷ lệ vượt qua vượt quá một ngưỡng được định nghĩa trước (η). Thủ tục này có điểm tương đồng với việc dịch mã của LLMs được thảo luận trong (Chen et al., 2023a), nhưng chúng tôi mở rộng nó để phù hợp với các loại đầu vào đa phương thức phức tạp hơn và hướng dẫn từ ngôn ngữ tự nhiên và hình ảnh thô. Việc bao gồm tạo module trong ngữ cảnh của các nhiệm vụ suy luận thị giác cung cấp hai lợi ích chính. Thứ nhất, nó duy trì tính minh bạch và khả năng diễn giải của các mô hình neuro-symbolic trong khi bảo tồn hiệu suất cạnh tranh. Thứ hai, nó thể hiện khả năng sinh tạo và khả năng mở rộng vì GENOME của chúng tôi có thể tự động tạo ra các module mới phù hợp với các nhiệm vụ cụ thể.

Thực thi module. Với việc tích hợp các module mới được tạo ra với các module nơ-ron hiện có được thiết kế cho suy luận thị giác, khung GENOME khởi tạo việc phân tích truy vấn từ bộ dữ liệu kiểm tra, chuyển đổi chúng thành các hoạt động có thể thực thi thông qua học trong ngữ cảnh. Một prompt minh họa cho giai đoạn này được mô tả trong Hình 17. Đáng chú ý, mặc dù các nhiệm vụ suy luận thị giác khác nhau có thể có đầu vào và đầu ra riêng biệt, chúng có thể tái sử dụng các module trung gian này được thiết kế cho các nhiệm vụ khác để nâng cao hiệu suất tổng thể. Tính năng này đại diện cho một khả năng độc đáo cho việc tạo mã ở cấp độ module, một khía cạnh cho đến nay chưa được khám phá bởi các phương pháp trước đây (Surís et al., 2023; Gupta & Kembhavi, 2022).

4 THÍ NGHIỆM
Trong phần này, chúng tôi trình bày một loạt thí nghiệm toàn diện để đánh giá hiệu suất của các mô hình của chúng tôi. Đầu tiên, chúng tôi chứng minh hiệu quả của các mô hình của chúng tôi trong việc học các module nơ-ron trên hai điểm chuẩn đã được thiết lập: GQA (Hudson & Manning, 2019), tập trung vào trả lời câu hỏi thị giác kết hợp, và RefCOCO (Kazemzadeh et al., 2014), đánh giá hiểu biểu thức tham chiếu. Tiếp theo, chúng tôi minh họa cách các module thu được từ hai bộ dữ liệu này có thể được áp dụng thành công cho các nhiệm vụ mới như chỉnh sửa hình ảnh và gắn thẻ kiến thức. Hơn nữa, chúng tôi làm nổi bật khả năng thích ứng của khung của chúng tôi để giải quyết các nhiệm vụ suy luận thị giác mới (Raven (Zhang et al., 2019) và MEWL (Jiang et al., 2023)), ngay cả với các ví dụ huấn luyện hạn chế. Trước khi đi sâu vào các thí nghiệm này, chúng tôi cung cấp tổng quan về các thiết lập thí nghiệm.

Chi tiết thí nghiệm. Sự thành công của GENOME của chúng tôi dựa trên một tập hợp các module và API được định nghĩa trước làm điểm khởi đầu. Chúng tôi sử dụng các module thủ công từ VisProg (Gupta & Kembhavi, 2022) làm thành phần ban đầu. Ngoài ra, chúng tôi kết hợp một số API mới từ ViperGPT để tăng cường việc tạo module. Chúng tôi cũng bao gồm một số API mới từ ViperGPT (Surís et al., 2023) để tạo ra các module mới. Trong Phần 4.4, chúng tôi cũng bao gồm các kết quả được phân tích bởi LLM mã nguồn mở từ WLM (Xu et al., 2023a) để điều tra ảnh hưởng của các mô hình LLM khác nhau. Danh sách toàn diện các module được huấn luyện trước được sử dụng trong phương pháp của chúng tôi có thể được tìm thấy trong Phần A.1 của Phụ lục. Chúng tôi trích xuất các ví dụ huấn luyện để thu thập các module mới. Cụ thể hơn, chúng tôi đã trích xuất 300 ví dụ từ GQA, 100 từ RefCOCO, 10 từ Raven, và 10 từ MEWL.

Bộ dữ liệu và Thước đo đánh giá. Chúng tôi trình bày các thí nghiệm của GENOME trên các điểm chuẩn thị giác-ngôn ngữ tiêu chuẩn, GQA (Hudson & Manning, 2019) và RefCOCO Kazemzadeh et al. (2014). GQA là một bộ dữ liệu suy luận thị giác kết hợp phổ biến với các câu hỏi đa bước tổng hợp, làm cho nó phù hợp cho suy luận đa bước. RefCOCO là một bộ dữ liệu grounding thị giác điển hình, đánh giá khả năng định vị đối tượng và hiểu các mối quan hệ không gian và ngữ nghĩa tinh vi của mô hình. Theo ViperGPT, chúng tôi đánh giá GQA trên test-dev split và RefCOCO trên testA split. Sau đó, chúng tôi hiển thị khả năng của GENOME trên các nhiệm vụ chuyển giao khác, chỉnh sửa hình ảnh, và gắn thẻ kiến thức và so sánh nó với VisProg. Vì các bộ dữ liệu chỉnh sửa hình ảnh và gắn thẻ kiến thức từ VisProg không được công khai, chúng tôi đã xây dựng hai bộ dữ liệu mới để đánh giá. Bộ dữ liệu chỉnh sửa mới chứa 50 hình ảnh và các cặp hướng dẫn. Bộ dữ liệu gắn thẻ kiến thức mới chứa 50 hình ảnh với 50 biểu thức tham chiếu. Chúng tôi cung cấp chi tiết thêm về bộ dữ liệu trong Phụ lục A.3. Các bộ dữ liệu sẽ được phát hành cho mục đích nghiên cứu. Cuối cùng, chúng tôi cho thấy rằng GENOME có thể học cách xử lý các nhiệm vụ suy luận thị giác mới như Raven (Zhang et al., 2019) và MEWL (Jiang et al., 2023) bằng cách quan sát một số ít ví dụ huấn luyện và học module. Raven là một nhiệm vụ cho suy luận thị giác quan hệ và tương tự của các tập hình ảnh và đã được sử dụng rộng rãi cho các thử nghiệm trí tuệ phi ngôn ngữ. MEWL là một điểm chuẩn gần đây được đề xuất để đánh giá cách máy học ý nghĩa từ trong các cảnh thị giác có nền tảng. Ví dụ của các nhiệm vụ này có thể được tìm thấy tại Hình 5 và Hình 6.

4.1 SO SÁNH VỚI CÁC BASELINE TRÊN SUY LUẬN THỊ GIÁC
Chúng tôi đã tiến hành phân tích giữa mô hình của chúng tôi và một số mô hình baseline bằng cách sử dụng các bộ dữ liệu GQA và RefCOCO. Do việc ngừng sử dụng API Codex chuyên nghiệp ban đầu (code-davinci-002), chúng tôi đã thay thế nó bằng API hiện có (gpt-3.5-turbo-instruct) và tiến hành thí nghiệm với cả mô hình của chúng tôi và các mô hình baseline để đảm bảo so sánh công bằng. Chúng tôi đã không tiến hành thí nghiệm với GPT-4 do chi phí cấm đoán.

Các kết quả, như được trình bày trong Bảng 1, chứng minh rằng mô hình của chúng tôi đạt được hiệu suất cạnh tranh trong cả trả lời câu hỏi thị giác và hiểu biểu thức tham chiếu, do đó xác nhận tính hiệu quả của nó. Hơn nữa, chúng tôi cung cấp một module minh họa từ mô hình của chúng tôi trong Hình 11. Module mới được tạo ra này có khả năng sử dụng nhiều API có sẵn khác nhau để chọn thuộc tính từ hình ảnh. Quá trình suy luận từng bước của mô hình chúng tôi được chi tiết trong Hình 3, cung cấp tính minh bạch lớn hơn so với các mô hình đầu cuối.

4.2 GENOME CHO HỌC CHUYỂN GIAO
Trong phần này, chúng tôi chứng minh khả năng mạnh mẽ của mô hình chúng tôi trong học chuyển giao. Chúng tôi tăng cường thư viện mô-đun bằng cách kết hợp các module được tạo ra

--- TRANG 7 ---
Preprint
classCHOOSE_ATTRIBUTE():
defpredict(self, img,boxes,obj,attr1,attr2):
iflen(boxes) > 0:
box = boxes[0]
box = self.expand_box (box, img.size)
out_img = img.crop(box)
else:
out_img = img
prompt1 = f'Nói cho tôi biết các thuộc tính khi {obj} là {attr1} trong một câu.'
prompt2 = f'Nói cho tôi biết các thuộc tính khi {obj} là {attr2} trong một câu.'
obj_desc1 = API.gpt3(prompt1, 'gpt3_general')
obj_desc2 = API.gpt3(prompt2, 'gpt3_general')
result1 = API.clip (out_img,obj_desc1 )
result2 = API.clip (out_img,obj_desc2 )
ifresult1 > result2:
result= attr1
else:
result= attr2
returnresult
defexecute(self, img,boxes,obj,attr1,attr2):
result= self.predict (img,boxes,obj,attr1,attr2)
returnresultBOX0=LOC(image= IMAGE,object ='shirt') 
ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,
obj='shirt',attr1='long sleeved', attr2 
='short sleeved')
FINAL_RESULT=RESULT(var=ANSWER0)
Câu hỏi: Áo có tay ngắn hay tay dài?
obj_desc1 : Áo có tay dài, cổ áo và nút.
obj_desc2 : Áo có tay ngắn với cổ tròn, tay ngắn và viền thẳng.
result1 : 0.9; result2 : 0.2
result : tay ngắn
HÌNH ẢNH:
Chương trình được tạo:
result: tay ngắnimg: out_img :GQA
RefCOCO
BOXLIST0=LOC(image=IMAGE,object='kid')
BOXLIST1=FILTER_PROPERTY(image= IMAGE,box_list = 
BOXLIST0,object=' kid',attribute ='blue')
BOX2=SORT_SPATIAL(image= IMAGE,box_list =BOXLIST1,
location=' front',index =1)
FINAL_RESULT=RESULT(var=BOX2)Chương trình được tạo:
classSORT_SPATIAL():
defparse_depth (self,img,box_list ):        
box_depth_list = [] # tính toán độ sâu cho phía trước hoặc phía sau
depth_map = API.depth (img)        
forbox inbox_list :            
x1, y1, x2, y2 = box            
depth_map = np.array (depth_map )            
avg_depth = np.median (depth_map [x1:x2, y1:y2])            
box_depth_list.append ((box, avg_depth ))        
returnbox_depth_list
defpredict( self,img,box_list,location,index ):        
if"front" inlocation or"behind" inlocation:            
box_depth_list = self.parse_depth (img, box_list )            
box_list_sorted = sorted( box_depth_list , key=lambda x: x[1])            
out_box_list = [box_i[0] forbox_iinbox_list_sorted ]            
if"behind" in location:                
out_box_list.reverse ()        
else:            
if"left" inlocation:                
box_list = sorted( box_list , key=lambda x: x[0])                    
.
.
returnout_box_list
defexecute( self,img,box_list,location,index ):        
returnself.predict (img,box_list,location,index )HÌNH ẢNH:img: box_depth_list :
depth: 0.13,
0.03
box_list_sorted :
depth: 0.13 0.03,out_box_list :
,
kết quả:
Câu hỏi: Đứa trẻ gần nhất mặc màu xanh.

Hình 3: Ví dụ định tính của GENOME trên GQA và RefCOCO. Hình ảnh truy vấn, hướng dẫn ngôn ngữ, và các chương trình đã phân tích được hiển thị ở bên trái. Các module mới tương ứng và giá trị của các biến quan trọng được hiển thị ở bên phải.

Gắn thẻ diễn viên thứ hai đã đóng vai James Bond từ trái sang phải
Gắn thẻ nhạc sĩ đoạt giải Grammy ở giữa
Gắn thẻ hành tinh khí thứ hai từ trái trong hệ mặt trời của chúng ta
Gắn thẻ logo nền tảng truyền thông xã hội có cùng màu với Facebook
Chọn siêu anh hùng có cùng màu với Hulk và tạo một color pop
Thay thế chai có cùng chất liệu với chai màu xanh bằng chai màu đỏ
Tạo color pop cho người thứ ba từ bên phải
Ẩn người đàn ông thứ hai từ bên trái với 8)

Hình 4: Ví dụ định tính của GENOME trên các nhiệm vụ chỉnh sửa hình ảnh và gắn thẻ kiến thức. Hướng dẫn ngôn ngữ của các nhiệm vụ được hiển thị trên các hình ảnh gốc trong khi kết quả sửa đổi của hình ảnh được hiển thị bên dưới hình ảnh gốc. Điểm nhấn của hướng dẫn được đánh dấu bằng màu đỏ, đòi hỏi các module mới của chúng tôi phải xử lý. Trong khi các vùng chủ chốt của hình ảnh đầu ra được giới hạn bằng màu xanh lá cây.

từ GQA và RefCOCO, sử dụng các ví dụ trong ngữ cảnh để hướng dẫn Mô hình Ngôn ngữ (LLM) tạo ra hướng dẫn từng bước để thực hiện nhiệm vụ. Kết quả định tính cho nhiệm vụ này được mô tả trong Hình 4. Như được minh họa trong Hình 4, mô hình của chúng tôi xuất sắc trong việc tạo ra hình ảnh chính xác về mặt ngữ nghĩa bằng cách sử dụng module mới được thêm vào, trong khi baseline VisProg gặp khó khăn trong việc nắm bắt các mối quan hệ cần thiết với thư viện module cố định, được định nghĩa trước của nó. Để cung cấp đánh giá toàn diện hơn về chỉnh sửa hình ảnh, chúng tôi thu hút các người chú thích để đánh giá thủ công tính chính xác của các hình ảnh được tạo ra.

--- TRANG 8 ---
Preprint
Phương pháp Chỉnh sửa hình ảnh Gắn thẻ Định vị
Độ chính xác Precision Recall F1 Precision Recall F1
VisProg 16.7 18.4 21.7 19.9 32.8 35.3 34.0
GENOME 55.3 67.1 52.3 58.8 76.9 57.9 66.0

Bảng 2: Đánh giá GENOME trên Học chuyển giao với các nhiệm vụ chỉnh sửa hình ảnh và gắn thẻ kiến thức. GENOME của chúng tôi cho thấy hiệu suất tốt hơn nhiều trên tất cả các tiêu chí, chứng minh tính hiệu quả của các module được chuyển giao. Một so sánh định tính có thể thấy trong Hình 7 ở Phụ lục.

Phương pháp Center L-R U-D
ResNet+DRT 58.1 65.8 67.1
ALANS-V 98.4 97.3 96.4
GENOME 80.1 67.6 69.1
Con người 95.5 86.4 81.8

Bảng 3: Đánh giá GENOME trên Raven (Zhang et al., 2019). So với các phương pháp được huấn luyện với dữ liệu trong miền lớn, mô hình của chúng tôi hoạt động cạnh tranh.

Phương pháp shape color material
Aloe 34.2 33.2 31.0
Flamingo-1.1B 49.3 35.3 48.5
GENOME 43.7 45.3 41.0
Con người 92.4 87.2 72.7

Bảng 4: Đánh giá GENOME trên MEWL (Jiang et al., 2023). So với các phương pháp được huấn luyện trên dữ liệu trong miền rộng lớn, mô hình của chúng tôi cho thấy hiệu suất cạnh tranh.

Hiệu suất của các mô hình được so sánh trong Bảng 2, trong đó mô hình của chúng tôi vượt trội hơn baseline. Trong ngữ cảnh gắn thẻ kiến thức, chúng tôi giao nhiệm vụ cho các người chú thích đánh dấu các vùng hình ảnh được tham chiếu bởi các biểu thức và sử dụng cùng các thước đo như RefCOCO để đánh giá độ chính xác của các hộp giới hạn và sử dụng điểm BERT để đánh giá tính chính xác của các tên được gắn nhãn. Mô hình của chúng tôi thể hiện hiệu suất vượt trội trong cả chỉnh sửa hình ảnh và gắn thẻ kiến thức. Chúng tôi hiển thị một ví dụ điển hình trong Hình 7 của Phụ lục để chỉ ra cách GENOME của chúng tôi tận dụng các module mới để thực hiện kết quả gắn thẻ kiến thức tốt hơn baseline.

4.3 GENOME TRONG HỌC NHIỆM VỤ FEW-SHOT
Là một khung học module tổng quát, mô hình của chúng tôi không chỉ có thể học các module mới để xử lý các nhiệm vụ hiện có mà còn có thể học cách xử lý các nhiệm vụ suy luận thị giác mới từ một số ít ví dụ huấn luyện. Chúng tôi đánh giá khả năng như vậy trên các nhiệm vụ mới, Raven (Zhang et al., 2019) và MEWL (Jiang et al., 2023). Cụ thể, chúng tôi đầu tiên nhắc LLM học các module nhận dạng mẫu cho hiểu biết thị giác và sau đó yêu cầu LLM tạo ra một module giải quyết để xử lý nhiệm vụ. Các trường hợp dự đoán mô hình của chúng tôi được hiển thị trong Hình 5 và Hình 6. Lưu ý rằng suy luận thị giác từ Raven được sử dụng rộng rãi trong kiểm tra trí tuệ cho con người, điều này cho thấy khả năng mạnh mẽ và tiềm năng của mô hình chúng tôi. Chúng tôi báo cáo hiệu suất của mô hình chúng tôi và các baseline trong Bảng 3 và Bảng 4. Mô hình của chúng tôi tốt hơn đáng kể so với các phương pháp được giám sát đầy đủ trước đây như ResNet+DRT (Zhang et al., 2019) và Aloe (Ding et al., 2021), cho thấy tính hiệu quả của nó. Lưu ý rằng tất cả các mô hình này ResNet+DST, ALANS-V (Zhang et al., 2022), Aloe (Ding et al., 2021) và Flamingo (Alayrac et al., 2022) là các mô hình được tinh chỉnh đầy đủ trên dữ liệu trong miền, trong khi GENOME của chúng tôi là một khung few-shot tổng quát để học các module cho giải quyết vấn đề. Hơn nữa, chúng ta có thể quan sát thấy tính kết hợp mới và việc tái sử dụng module từ Hình 8 của Phụ lục. Mặc dù module SOLVER ban đầu được học từ các vấn đề loại trung tâm, nó có thể được chuyển giao tự nhiên sang các loại khác như trái-phải và lên-xuống.

4.4 NGHIÊN CỨU LOẠI BỎ
Để đánh giá hiệu quả của mô hình chúng tôi, chúng tôi đã tiến hành một loạt các nghiên cứu loại bỏ để giải quyết các câu hỏi chính sau: Q1 Việc học module hiệu quả như thế nào? Q2 Số lượng ví dụ huấn luyện có tác động gì đến hiệu suất mô hình? Q3 Khả năng của LLM quan trọng như thế nào để có hiệu suất tối ưu? Trong các thí nghiệm của chúng tôi, GENOME w/o ML đại diện cho một cấu hình không có bất kỳ module mới nào được học

--- TRANG 9 ---
Preprint
Phương pháp RefCOCO
GENOME w/o ML 62.3
GENOME-WLM 64.4
GENOME (10) 49.4
GENOME (50) 67.0
GENOME (100) 67.1

Bảng 5: Nghiên cứu loại bỏ của GENOME trên RefCOCO.

mà dựa nhiều vào các module được định nghĩa bởi ViperGPT và VisProg, hướng dẫn LLM xác định một vùng phù hợp với biểu thức tham chiếu. Mặt khác, GENOME-WLM thay thế API gpt-3.5-turbo-instruct bằng WizardCoder-Python-34B-V1.0 từ WizardLM (Xu et al., 2023a). Các chỉ định GENOME (10)/(50)/(100) biểu thị các mô hình được huấn luyện với 10, 50, và 100 ví dụ, tương ứng. Vì hạn chế về tài nguyên, chúng tôi giới hạn thí nghiệm của mình với 800 mẫu RefCOCO.

Bảng 5 trình bày kết quả, dẫn đến những hiểu biết sau: học module, với đủ trường hợp kiểm tra, có thể tăng cường hiệu suất nhiệm vụ (Q1 được giải quyết). Thiếu ví dụ huấn luyện, chẳng hạn như 10 cho RefCOCO, có thể gây ra overfitting, nhưng điều này giảm bớt với dữ liệu huấn luyện tăng (50 ví dụ), cải thiện hiệu suất tổng thể (Q2 được giải quyết). Cuối cùng, hiệu suất mô hình có vẻ liên kết nội tại với khả năng của LLM, với các LLM vượt trội mang lại kết quả tăng cường (Q3 được giải quyết).

5 KẾT LUẬN
Trong nghiên cứu này, chúng tôi giới thiệu GENOME, được thiết kế để giải quyết các nhiệm vụ suy luận thị giác khi đối mặt với dữ liệu huấn luyện hạn chế. Phương pháp này kết hợp các mô hình ngôn ngữ để phân tích ngôn ngữ tự nhiên thành các hoạt động có thể thực thi và tạo ra các module thị giác chuyên biệt phù hợp với nhiệm vụ đã cho. Mô hình của chúng tôi thể hiện hiệu suất cạnh tranh trên các nhiệm vụ truyền thống, chuyển giao dễ dàng các module đã học sang các nhiệm vụ mới, và khả năng thích ứng với các nhiệm vụ mới ngay cả với dữ liệu huấn luyện hạn chế. GENOME của chúng tôi cũng đề xuất nhiều hướng cho nghiên cứu tương lai. Thứ nhất, nó vẫn cần các prompt đặc thù cho nhiệm vụ cho mỗi nhiệm vụ suy luận riêng biệt, và sẽ thú vị khi khám phá việc sử dụng một prompt phổ quát cho tất cả các nhiệm vụ. Thứ hai, khung có thể được mở rộng để bao gồm một phạm vi rộng hơn của các nhiệm vụ suy luận đa phương thức, kết hợp các đầu vào đa dạng như âm thanh, video, và thông tin xúc giác.

TÀI LIỆU THAM KHẢO
Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv, 2022.

Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.

Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077–6086, 2018.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705, 2016a.

Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, pp. 39–48, 2016b.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

James F. Brulé and Alexander Blount. Knowledge acquisition. 1989. URL https://api.semanticscholar.org/CorpusID:18663796.

Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, and Tal Schuster. Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation, 2022.

Henry R Burke. Raven's progressive matrices (1938): More on norms, reliability, and validity. Journal of Clinical Psychology, 41(2):231–235, 1985.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv, 2023.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.

Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for compositional visual reasoning. Proceedings of WACV, 2021b.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023a.

Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K Wong, and Qi Wu. Cops-ref: A new dataset and task on compositional referring expression comprehension. In CVPR, 2020.

Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and Chuang Gan. Grounding physical concepts of objects and events through dynamic visual reasoning. In ICLR, 2021c.

--- TRANG 11 ---
Preprint
Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B Tenenbaum, and Chuang Gan. Comphy: Compositional physical reasoning of objects and events from videos. arXiv, 2022.

Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning. arXiv, 2023b.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over learned object embeddings enables complex visual reasoning. Advances in neural information processing systems, 34:9112–9124, 2021.

Mingyu Ding, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B Tenenbaum, and Chuang Gan. Embodied concept learner: Self-supervised learning of concepts and mapping through instruction following. In CoRL, 2023.

Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. Towards next-generation intelligent assistants leveraging llm techniques. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 5792–5793, 2023.

Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: growing generalizable, interpretable knowledge with wake–sleep bayesian program learning. Philosophical Transactions of the Royal Society A, 381(2251):20220050, 2023.

Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. ArXiv, abs/2211.11559, 2022.

Harry Frederick Harlow. The formation of learning sets. Psychological review, 56 1:51–65, 1949. URL https://api.semanticscholar.org/CorpusID:22804426.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

Drew A Hudson and Christopher D Manning. Compositional attention networks for machine reasoning. arXiv preprint arXiv:1803.03067, 2018.

Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019.

Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, and Yixin Zhu. Mewl: Few-shot multimodal word learning with referential uncertainty. In ICML, 2023.

Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787–798, 2014.

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023.

--- TRANG 12 ---
Preprint
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn and think like people, 2016.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv, 2023.

Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, 2022.

Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In ICLR, 2019a.

Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In ICLR, 2019b.

Tom Michael Mitchell, Richard M. Keller, and Smadar T. Kedar-Cabelli. Explanation-based generalization: A unifying view. Machine Learning, 1:47–80, 1986. URL https://api.semanticscholar.org/CorpusID:117264.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021.

René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ICCV, 2021.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684–10695, 2022.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv, 2023.

Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv, 2022.

Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv, 2023.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023a.

Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv, 2023b.

Jianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D Cox, Joshua B Tenenbaum, and Chuang Gan. Object-centric diagnosis of visual reasoning. arXiv preprint arXiv:2012.11587, 2020.

Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In AAAI, 2022.

--- TRANG 13 ---
Preprint
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. In NeurIPS, 2018.

Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69–85. Springer, 2016.

Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1307–1315, 2018.

Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276, 2021.

Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5317–5327, 2019.

Chi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Abstract spatial-temporal reasoning via probabilistic abduction and execution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Chi Zhang, Sirui Xie, Baoxiong Jia, Ying Nian Wu, Song-Chun Zhu, and Yixin Zhu. Learning algebraic representation for systematic generalization in abstract reasoning. In European Conference on Computer Vision, pp. 692–709. Springer, 2022.

--- TRANG 14 ---
Preprint
A PHỤ LỤC
Trong phần này, chúng tôi chứng minh các khẳng định của chúng tôi trong bài báo bằng cách cung cấp chi tiết triển khai bổ sung (Phần A.1), các prompt mẫu cho mỗi giai đoạn (Phần A.2), chi tiết về thu thập dữ liệu (Phần A.3), ví dụ định tính của các module mới được học (Phần A.4).

A.1 CHI TIẾT TRIỂN KHAI
Các module và API model được định nghĩa trước. Sự thành công của mô hình chúng tôi vẫn đòi hỏi một tập hợp các API được định nghĩa trước. Theo các module trong VisProg và ViperGPT, chúng tôi áp dụng các API sau. Chúng tôi sử dụng GLIP (Li* et al., 2022) để định vị đối tượng. Chúng tôi sử dụng gpt-3.5-turbo-instruct từ OpenAI và WizardCoder-Python-34B-V1.0 từ WizardLM để tạo mã. Chúng tôi sử dụng BLIP (Li et al., 2023) để trả lời các câu hỏi đơn giản về hình ảnh. Chúng tôi sử dụng CLIP (Radford et al., 2021) và X-VLM (Zeng et al., 2021) để phân loại hình ảnh-văn bản. Chúng tôi sử dụng MiDaS (Ranftl et al., 2021) để ước tính độ sâu trong hình ảnh. Chúng tôi sử dụng stable diffusion (Rombach et al., 2022) để sửa đổi các patch hình ảnh. Dựa trên các API này, chúng tôi xây dựng một tập hợp các module được định nghĩa trước theo VisProg. Các module được định nghĩa trước này sẽ được sử dụng để hợp tác với các module mới được học để suy luận thị giác.

Mô tả Modules
Hiểu biết hình ảnh Loc cho định vị đối tượng, FaceDet cho phát hiện khuôn mặt, Select và Filter Property cho phân loại hình ảnh-văn bản. Filter Spatial cho việc chọn các vùng hình ảnh;
Thao tác hình ảnh Replace cho chỉnh sửa hình ảnh, colorPop cho thay đổi màu sắc hình ảnh, BgBlur cho làm mờ nền, Tag cho chú thích các vùng hộp và Emoji cho gắn thẻ khuôn mặt. Crop và các biến thể của nó cho việc cắt các patch từ hình ảnh.
Khác List cho truy xuất kiến thức thực tế, Count cho đếm số lượng đối tượng, Eval, Result, BOX2MASK và MASK2BOX cho định dạng đầu ra.

Bảng 6: Các module được định nghĩa trước được sử dụng trong GENOME.

A.2 PROMPT CHO TỪNG GIAI ĐOẠN
Khả năng của GENOME chúng tôi đến từ việc học trong ngữ cảnh của LLMs (Brown et al., 2020), khi các prompt là chìa khóa để nói cho LLM biết nó nên tạo ra gì. Chúng tôi hiển thị các prompt mẫu của các mô hình chúng tôi để học nhiệm vụ VQA trong Hình 15-17.

A.3 CHI TIẾT VÀ VÍ DỤ CỦA CÁC BỘ DỮ LIỆU MỚI
Để đánh giá Gắn thẻ kiến thức, 50 hướng dẫn gắn thẻ được chú thích trên 50 hình ảnh internet bao gồm các nhân vật và nhiều đối tượng khác như logo, hoa, tòa nhà, trái cây và thể thao, trong số những thứ khác. Đối với mỗi hướng dẫn, chúng tôi chú thích thủ công hộp giới hạn ground truth và thẻ liên quan. Để đánh giá chỉnh sửa hình ảnh, chúng tôi thu thập 50 hướng dẫn chỉnh sửa trên 50 hình ảnh bao gồm các nhân vật và nhiều đối tượng khác như thực phẩm, đồ nội thất, động vật, đồ dùng, v.v. 25 hình ảnh từ bộ dữ liệu COCO và 25 hình ảnh khác từ internet. Đối với các nhiệm vụ chỉnh sửa hình ảnh, chúng tôi yêu cầu ba người chú thích ước tính xem việc chỉnh sửa có chính xác hay không. Đối với nhiệm vụ gắn thẻ kiến thức, chúng tôi coi việc định vị là chính xác nếu vùng được phát hiện có IoU cao hơn 0.5 với chú thích ground-truth. Đối với gắn thẻ văn bản, chúng tôi so sánh dự đoán với văn bản được chú thích bằng matching BERT (BEM) (Bulian et al., 2022). Nếu điểm matching cao hơn 0.5, chúng tôi coi đó là matching thành công. Nhiều ví dụ của hai bộ dữ liệu có thể tìm thấy tại Hình 9 và Hình 10.

A.4 VÍ DỤ ĐỊNH TÍNH
Trong phần này, chúng tôi hiển thị các ví dụ định tính của các module được học và các trường hợp định tính về cách chúng xử lý các nhiệm vụ khác nhau. Chúng tôi hiển thị một ví dụ về GENOME hoạt động tốt hơn VisProg

--- TRANG 15 ---
Preprint
OBJ0=LOC(image=IMAGE,object='fruit or vegetable')
LIST0=LIST(query='fruits and vegetables',max=20)
OBJ1=CLASSIFY(image=IMAGE,object=OBJ0,categories=LIST0)
OBJ2=CLASSIFY(image=IMAGE,object=OBJ0,categories='grape')
OBJ3=REDUCE_MASK(mask_list1=OBJ1,mask_list2=OBJ2)
OBJ4=META_COMPARE(function_name='COMPARE_COLOR',image=IMAGE,obj_list=OBJ3,obj_cmp=OBJ2,name1='fruit or vegetable',name2='grape',attribute='same')
IMAGE0=TAG(image=IMAGE,object=OBJ4)
FINAL_RESULT=RESULT(var=IMAGE0)
Chương trình được tạo:

OBJ0=LOC(image=IMAGE,object='grape')
LIST0=LIST(query='common fruits and vegetables of the same color as the grape',max=20)
OBJ1=CLASSIFY(image=IMAGE,object=OBJ0,categories=LIST0)
IMAGE0=TAG(image=IMAGE,object=OBJ1)
FINAL_RESULT=RESULT(var=IMAGE0)
Chương trình được tạo:
VisProg
Của chúng tôi
Gắn thẻ kiến thức
OBJ2:COMPARE_COLOR
OBJ4:
KẾT QUỺ CUỐI CÙNG:
OBJ3:
,,
OBJ3,
Câu hỏi: Gắn thẻ các loại trái cây và rau quả phổ biến có cùng màu với quả nho.
HÌNH ẢNH:
. . .
OBJ1:
KẾT QUỺ CUỐI CÙNG:

Hình 7: Một ví dụ điển hình về cách GENOME của chúng tôi vượt trội hơn VisProg trong gắn thẻ kiến thức. Ở phía trên, mô hình của chúng tôi có thể tận dụng module COMPARE_COLOR được học từ GQA để định vị vùng chính xác trong khi VisProg không thể tạo ra chương trình chính xác với thư viện module cố định của nó.

trong Hình 7. Ở phía trên Hình 7, mô hình của chúng tôi hiệu quả sử dụng module COMPARE_COLOR thu được từ GQA để xác định vùng chính xác, trong khi VisProg thất bại trong việc tạo ra chương trình chính xác do thư viện module cứng nhắc của nó. Hình 8 làm nổi bật các hình thức mới nổi của tính kết hợp và tái sử dụng module. Đáng chú ý, mặc dù module SOLVER ban đầu được huấn luyện trên các vấn đề loại trung tâm trong bộ dữ liệu Raven, nó thể hiện khả năng thích ứng tự nhiên với các loại vấn đề khác, bao gồm các hướng trái-phải và lên-xuống.

Các module mới được học. Chúng tôi hiển thị các module mới được học mẫu từ GQA và RefCOCO trong Hình 11-14. Như được hiển thị trong Hình 11, module mới được học (CHOOSE_ATTRIBUTE) có thể sử dụng LLM để truy xuất kiến thức liên quan trước tiên và sau đó áp dụng bộ phân loại hình ảnh-văn bản để khớp các thuộc tính. Trong Hình 13-14, chúng ta thấy rằng module mới SORT_SPATIAL có thể định vị đối tượng với chỉ số không gian.

--- TRANG 16 ---
Preprint
Chương trình được tạo:
COLOR=DETECT_COLOR(image=IMAGE)
SHAPE=DETECT_SHAPE(image=IMAGE)
SIZE=DETECT_SIZE(image=IMAGE)
ANSWER=SOLVER(image=IMAGE,color=COLOR,shape=SHAPE,size=SIZE)
FINAL_RESULT=RESULT(var=ANSWER)
Tập câu trả lời
Ma trận bài toán
HÌNH ẢNH:

Tập câu trả lời
Ma trận bài toán
HÌNH ẢNH:

Tập câu trả lời
Ma trận bài toán
HÌNH ẢNH:

Trung tâm
Trái-Phải
Lên-Xuống

Chương trình được tạo:
BOX0=LOC(image=IMAGE,object='LEFT')
IMAGE0=CROP(image=IMAGE,box=BOX0)
BOX1=LOC(image=IMAGE,object='RIGHT')
IMAGE1=CROP(image=IMAGE,box=BOX1)
COLOR=DETECT_COLOR(image0=IMAGE0,image1=IMAGE1)
SHAPE=DETECT_SHAPE(image0=IMAGE0,image1=IMAGE1)
SIZE=DETECT_SIZE(image0=IMAGE0,image1=IMAGE1)
ANSWER=SOLVER(image0=IMAGE0,image1=IMAGE1,color=COLOR,shape=SHAPE,size=SIZE)
FINAL_RESULT=RESULT(var=ANSWER)

Chương trình được tạo:
BOX0=LOC(image=IMAGE,object='TOP')
IMAGE0=CROP(image=IMAGE,box=BOX0)
BOX1=LOC(image=IMAGE,object='BOTTOM')
IMAGE1=CROP(image=IMAGE,box=BOX1)
COLOR=DETECT_COLOR(image0=IMAGE0,image1=IMAGE1)
SHAPE=DETECT_SHAPE(image0=IMAGE0,image1=IMAGE1)
SIZE=DETECT_SIZE(image0=IMAGE0,image1=IMAGE1)
ANSWER=SOLVER(image0=IMAGE0,image1=IMAGE1,color=COLOR,shape=SHAPE,size=SIZE)
FINAL_RESULT=RESULT(var=ANSWER)

Hình 8: Tính kết hợp mới và tái sử dụng module trong bộ dữ liệu Raven. Trong khi module SOLVER ban đầu được huấn luyện trên các vấn đề loại trung tâm trong bộ dữ liệu Raven, nó thể hiện khả năng chuyển giao tự nhiên sang các loại khác, chẳng hạn như các vấn đề trái-phải và lên-xuống.

Tạo color pop cho chiếc thuyền đầu tiên từ phía trước
Tạo color pop cho đứa trẻ đầu tiên từ bên phải

Ẩn Tim Robbins với ;) và Morgan Freeman với 8)
Thay thế chiếc pizza thứ hai từ trên xuống bằng hamburger
Thay thế cái thìa có cùng chất liệu với cái thìa dẹt bằng dao

Chọn cái đèn có cùng màu với cái ở dưới cùng và tạo color pop

Hình 9: Nhiều ví dụ về bộ dữ liệu chỉnh sửa hình ảnh mới. Bộ dữ liệu yêu cầu các mô hình chỉnh sửa chi tiết tinh vi và theo vùng của hình ảnh theo các hướng dẫn ngôn ngữ đa dạng.

--- TRANG 17 ---
Preprint
Gắn thẻ bức tranh nổi tiếng của Louvre ở bên trái
Gắn thẻ địa danh nổi tiếng của châu Âu ở góc dưới bên phải
Gắn thẻ đạo diễn phim nổi tiếng thứ hai từ bên trái
Gắn thẻ loài chim phổ biến có cùng màu với cò

Gắn thẻ con chó thứ hai từ bên phải
Gắn thẻ người đoạt giải Nobel Vật lý thứ hai từ bên trái

Hình 10: Nhiều ví dụ về bộ dữ liệu gắn thẻ kiến thức mới. Bộ dữ liệu yêu cầu các mô hình định vị vùng mục tiêu và gắn thẻ vùng với thông tin mong muốn.

--- TRANG 18 ---
Preprint
1class CHOOSE_ATTRIBUTE ():
2 """
3 Đầu vào:
4 image: một đối tượng hình ảnh
5 box: một danh sách các hộp giới hạn
6 object: một chuỗi
7 attribute1: một chuỗi
8 attribute2: một chuỗi
9Đầu ra:
10 result: một chuỗi
11 Ví dụ:
12 Câu hỏi: Áo khoác dày hay mỏng?
13 BOX0=LOC(image=IMAGE,object='coat')
14 ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,object='coat',
15 attribute1='thick',attribute2='thin')
16 FINAL_RESULT=RESULT(var=ANSWER0)
17"""
18step_name = 'CHOOSE_ATTRIBUTE'
19
20 def__init__(self):
21 print(f'Đăng ký bước {self.step_name}')
22
23 defexpand_box(self,box,img_size,factor=1.5):
24 W,H = img_size
25 x1,y1,x2,y2 = box
26 dw = int(factor *(x2-x1)/2)
27 dh = int(factor *(y2-y1)/2)
28 cx = int((x1 + x2) / 2)
29 cy = int((y1 + y2) / 2)
30 x1 = max(0,cx - dw)
31 x2 = min(cx + dw,W)
32 y1 = max(0,cy - dh)
33 y2 = min(cy + dh,H)
34 return[x1,y1,x2,y2]
35
36 defpredict(self,img,boxes,obj,attr1,attr2):
37 iflen(boxes) > 0:
38 box = boxes[0]
39 box = self.expand_box(box, img.size)
40 out_img = img.crop(box)
41 else:
42 out_img = img
43 prompt1 = f'Hãy nói cho tôi biết các thuộc tính khi {obj} là {attr1} trong
44 một câu.'
45 prompt2 = f'Hãy nói cho tôi biết các thuộc tính khi {obj} là {attr2} trong
46 một câu.'
47 obj_desc1 = API.gpt3(prompt1, 'gpt3_general')
48 obj_desc2 = API.gpt3(prompt2, 'gpt3_general')
49 result1 = API.clip(out_img,obj_desc1)
50 result2 = API.clip(out_img,obj_desc2)
51 ifresult1 > result2:
52 result = attr1
53 else:
54 result = attr2
55 returnresult
56
57 defexecute(self,img,boxes,obj,attr1,attr2):
58 result = self.predict(img,boxes,obj,attr1,attr2)
59 returnresult

Hình 11: Module mẫu được tạo ra từ bộ dữ liệu GQA. Module được xây dựng tự động này có thể tận dụng các API khác nhau để so sánh các thuộc tính của một vùng hình ảnh.

--- TRANG 19 ---
Preprint
1class COMPARE_COLOR ():
2"""
3Đầu vào:
4 image: một đối tượng hình ảnh
5 box1: một danh sách các hộp giới hạn
6 box2: một danh sách các hộp giới hạn
7 object1: một chuỗi
8 object2: một chuỗi
9 compare_type: một chuỗi
10Đầu ra:
11 result: một chuỗi
12 """
13 defexpand_box(self,box,img_size,factor=1.5):
14 W,H = img_size
15 x1,y1,x2,y2 = box
16 dw = int(factor *(x2-x1)/2)
17 dh = int(factor *(y2-y1)/2)
18 cx = int((x1 + x2) / 2)
19 cy = int((y1 + y2) / 2)
20 x1 = max(0,cx - dw)
21 x2 = min(cx + dw,W)
22 y1 = max(0,cy - dh)
23 y2 = min(cy + dh,H)
24 return[x1,y1,x2,y2]
25 defpredict(self,img,boxes1,boxes2,obj1,obj2,compare_type):
26 iflen(boxes1) > 0:
27 box1 = boxes1[0]
28 box1 = self.expand_box(box1,img.size)
29 out_img1 = img.crop(box1)
30 else:
31 out_img1 = img
32 iflen(boxes2) > 0:
33 box2 = boxes2[0]
34 box2 = self.expand_box(box2,img.size)
35 out_img2 = img.crop(box2)
36 else:
37 out_img2 = img
38 color1 = API.vqa(out_img1, f'Màu của {obj1} là gì?')
39 color2 = API.vqa(out_img2, f'Màu của {obj2} là gì?')
40 prompt = f'Có thể coi {color1} là cùng màu với'
41 f'{color2} không? Bạn chỉ nên trả lời có hoặc không mà không có từ nào khác.'
42 temp = API.gpt3(prompt, 'gpt3_general')
43 if'same' == compare_type:
44 if'yes' intemp.lower():
45 result = 'yes'
46 elif'no' intemp.lower():
47 result = 'no'
48 elif'different' == compare_type:
49 if'yes' intemp.lower():
50 result = 'no'
51 elif'no' intemp.lower():
52 result = 'yes'
53 else:
54 if'yes' intemp.lower():
55 result = 'yes'
56 elif'no' intemp.lower():
57 result = 'no'
58 returnresult
59 defexecute(self,img,boxes1,boxes2,obj1,obj2,compare_type):
60 result = self.predict(img,boxes1,boxes2,obj1,obj2,compare_type)
61 returnresult

Hình 12: Module mẫu được tạo ra từ bộ dữ liệu GQA.

--- TRANG 20 ---
Preprint
1class SORT_SPATIAL ():
2"""
3Chọn các đối tượng từ hình ảnh phù hợp với vị trí không gian.
4Các đối tượng được biểu diễn bằng các hộp giới hạn.
5Trả về các hộp giới hạn thỏa mãn điều kiện.
6Đầu vào:
7 image: hình ảnh PIL thô
8 box_list: một danh sách các hộp giới hạn không chuẩn hóa
9 location: vị trí chỉ có thể là left, middle, right, top,
10 bottom, front và behind
11 index: một số cho thứ hạng đối tượng
12Đầu ra:
13 box: một hộp giới hạn
14Ví dụ:
15 Câu hỏi: bánh sandwich thứ hai từ phải ở dưới cùng
16 BOXLIST0=LOC(image=IMAGE,object='sandwich')
17 BOXLIST1=SORT_SPATIAL(image=IMAGE,box_list=BOXLIST0,location=
18 'right',index=2)
19 BOXLIST2=SORT_SPATIAL(image=IMAGE,box_list=BOXLIST1,location=
20 'bottom',index=1)
21 FINAL_RESULT=RESULT(var=BOXLIST2)
22"""
23step_name = 'SORT_SPATIAL'
24 defpredict(self,img,box_list,location,index):
25 ifindex < 0 orindex > len(box_list):
26 return[]
27 ifindex == 0:
28 return[box_list[0]]
29 if"front" inlocation or"behind" inlocation:
30 box_depth_list = self.parse_depth(img, box_list)
31 box_list_sorted = sorted(box_depth_list, key= lambdax: x[1])
32 out_box_list = [box_i[0] forbox_i inbox_list_sorted]
33 if"behind" inlocation:
34 out_box_list.reverse()
35 else:
36 if"left" inlocation:
37 box_list = sorted(box_list, key= lambdax: x[0])
38 elif"right" inlocation:
39 box_list = sorted(box_list, key= lambdax: x[2], reverse
40 =True)
41 elif"top" inlocation:
42 box_list = sorted(box_list, key= lambdax: x[1])
43 elif"bottom" inlocation:
44 box_list = sorted(box_list, key= lambdax: x[3], reverse
45 =True)
46 else:
47 return[]
48 ifindex > len(box_list):
49 return[]
50 out_box_list = [box_list[index-1]]
51 returnout_box_list
52 defcheck_location(self,img,box,location):
53 w, h = img.size
54 x1, y1, x2, y2 = box
55 cx = (x1 + x2) / 2
56 cy = (y1 + y2) / 2
57 if'left' inlocation:
58 ifcx > w / 2:
59 return False

Hình 13: Module mẫu được tạo ra từ bộ dữ liệu RefCOCO. Phần còn lại của mã ở Hình 14.

--- TRANG 21 ---
Preprint
1 elif'right' inlocation:
2 ifcx < w / 2:
3 return False
4 if'top' inlocation:
5 ifcy > h / 2:
6 return False
7 elif'bottom' inlocation:
8 ifcy < h / 2:
9 return False
10 return True
11
12 defparse_depth(self,img,box_list):
13 box_depth_list = []
14 # tính toán độ sâu cho phía trước hoặc phía sau
15 depth_map = API.depth(img)
16 forbox inbox_list:
17 x1, y1, x2, y2 = box
18 depth_map = np.array(depth_map)
19 avg_depth = np.median(depth_map[x1:x2, y1:y2])
20 box_depth_list.append((box, avg_depth))
21 returnbox_depth_list
22
23 defexecute(self,img,box_list,location,index):
24 returnself.predict(img,box_list,location,index)

Hình 14: Module mẫu được tạo ra từ bộ dữ liệu RefCOCO. Phần trước của mã ở Hình 13. Module được tạo ra này có thể định vị đối tượng dựa trên vị trí của chúng trong hình ảnh và độ sâu của hình ảnh.

--- TRANG 22 ---
Preprint
1Các module được định nghĩa trước:
2class LOC ():
3"""
4Tạo ra các hộp của đối tượng trên hình ảnh.
5Đầu vào:
6 image: một đối tượng hình ảnh
7 object: một chuỗi đối tượng
8Đầu ra:
9 box: một danh sách các hộp giới hạn
10 Ví dụ:
11 BOX0=LOC(image=IMAGE,object='camel')
12 """
13class COUNT ():
14"""
15Đếm số lượng hộp trong danh sách.
16Đầu vào:
17 box: một danh sách các hộp giới hạn
18Đầu ra:
19 number: số lượng hộp
20Ví dụ:
21 ANSWER0=COUNT(box=BOX1)
22"""
23Giả sử bạn là một chuyên gia lập trình. Với một tập hợp các module được định nghĩa trước,
24bạn có thể xác định xem có thể viết một chương trình để có được câu trả lời cho câu hỏi hay không?
25Nếu không, chúng ta cần những module mới nào?
26Lưu ý rằng bạn chỉ có thể sử dụng các module được định nghĩa trước dưới đây:
27LOC, COUNT, CROP .......
28
29Câu hỏi: Cái túi xách ở bên trái hay bên phải của người?
30Có. Chương trình là:
31BOX0=LOC(image=IMAGE,object='person')
32IMAGE0=CROP_LEFTOF(image=IMAGE,box=BOX0)
33BOX1=LOC(image=IMAGE0,object='purse')
34ANSWER0=COUNT(box=BOX1)
35ANSWER1=EVAL(expr=f"'left' if {ANSWER0} > 0 else 'right'")
36FINAL_RESULT=RESULT(var=ANSWER1)
37
38Câu hỏi: Đối tượng nào lớn hơn, hình cầu hay khối lập phương xanh?
39Không. Chúng ta cần tạo một module mới "COMPARE_SIZE" trước. Đây là header
40của class:
41class COMPARE_SIZE ():
42"""
43So sánh kích thước của hai đối tượng trong hình ảnh.
44Một đối tượng được xác định bởi hộp giới hạn đầu tiên của box0
45Đối tượng khác được xác định bởi hộp giới hạn đầu tiên của box1
46Đầu vào:
47 image: một đối tượng hình ảnh
48 box0: một danh sách các hộp giới hạn
49 box1: một danh sách các hộp giới hạn
50Đầu ra:
51 flag: trả về True nếu đối tượng đầu tiên lớn hơn, ngược lại False
52Ví dụ:
53 Câu hỏi: Đối tượng nào lớn hơn, hình cầu hay khối lập phương xanh?
54 BOX0=LOC(image=IMAGE,object='sphere')
55 BOX1=LOC(image=IMAGE,object='blue cube')
56 FLAG0=COMPARE_SIZE(image=IMAGE,box0=BOX0,box1=BOX1)
57 ANSWER2=EVAL(expr=f"'sphere' if {FLAG0} else 'blue cube'")
58 FINAL_RESULT=RESULT(var=ANSWER)
59"""
60.......
61Câu hỏi: __INSERT_NEW_QUESTION__

Hình 15: Prompt khởi tạo module (Giai đoạn 1) để đề xuất các ứng cử viên module.

--- TRANG 23 ---
Preprint
1APIs được định nghĩa trước:
2class API ():
3 defloc(cls, image: Image.Image, obj_name: str) -> List[List[int]]:
4 """
5 Trả về các hộp của đối tượng cụ thể trên hình ảnh.
6 Đầu vào:
7 image: một đối tượng hình ảnh
8 obj_name: một chuỗi văn bản
9 Đầu ra:
10 boxes: một danh sách các hộp giới hạn
11 Ví dụ:
12 [[261, 160, 525, 299]] = API.loc(image, "camel")
13 """
14 returnboxes
15 defgpt3(cls, prompt: str) -> str:
16 """
17 Trả về phản hồi từ gpt3
18 Đầu vào:
19 prompt: prompt cho gpt3
20 Đầu ra:
21 response: phản hồi gpt3
22 Ví dụ:
23 "joe biden" = API.gpt3("Who is the US President?")
24 """
25 returnresponse
26.......
27
28Với các API trên, bạn có thể viết một module mới, Loc(), không?
29class Loc ():
30"""
31Tạo ra các hộp của đối tượng trên hình ảnh.
32Đầu vào:
33 img: một đối tượng hình ảnh
34 obj_name: một chuỗi đối tượng
35Đầu ra:
36 selected_boxes: một danh sách các hộp giới hạn
37Ví dụ:
38 BOX1=LOC(image=IMAGE0,object='food')
39"""
40Phản hồi:
41class Loc ():
42step_name = 'LOC'
43 defexecute(self,img,obj_name):
44 ifobj_name=='TOP':
45 bboxes = [self.top_box(img)]
46 elifobj_name=='BOTTOM':
47 bboxes = [self.bottom_box(img)]
48 elifobj_name=='LEFT':
49 bboxes = [self.left_box(img)]
50 elifobj_name=='RIGHT':
51 bboxes = [self.right_box(img)]
52 else:
53 bboxes = self.predict(img,obj_name)
54 returnbboxes
55
56 defpredict(self,img,obj_name):
57 returnAPI.loc(img,obj_name)
58.......
59Với các API trên, bạn có thể viết một module mới, __MODULE_NAME__, không?
60__MODULE_HEAD__

Hình 16: Prompt tạo module (Giai đoạn 2) để tạo một module dựa trên đầu vào và đầu ra của module.

--- TRANG 24 ---
Preprint
1Suy nghĩ từng bước để trả lời câu hỏi.
2
3Bạn chỉ có thể sử dụng các module dưới đây:
4LOC
5COUNT
6EVAL
7RESULT
8VERIFY_ATTRIBUTE
9VERIFY_COLOR
10VERIFY_MATERIAL
11.......
12
13Câu hỏi: Có phải xe cộ ở phía trên của hình ảnh?
14Chương trình:
15BOX0=LOC(image=IMAGE,object='TOP')
16IMAGE0=CROP(image=IMAGE,box=BOX0)
17BOX1=LOC(image=IMAGE0,object='vehicle')
18ANSWER0=COUNT(box=BOX1)
19ANSWER1=EVAL(expr=f"'yes' if {ANSWER0} > 0 else 'no'")
20FINAL_RESULT=RESULT(var=ANSWER1)
21
22Câu hỏi: Ai đang cầm ô?
23Chương trình:
24BOX0=LOC(image=IMAGE,object='umbrella')
25IMAGE0=CROP(image=IMAGE,box=BOX0)
26ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')
27FINAL_RESULT=RESULT(var=ANSWER0)
28
29Câu hỏi: Khăn tắm và hộp có màu sắc khác nhau không?
30Chương trình:
31BOX0=LOC(image=IMAGE,object='towel')
32BOX1=LOC(image=IMAGE,object='box')
33ANSWER0=COMPARE_ATTRIBUTE(image=IMAGE,box1=BOX0,box2=BOX1,object1='towel'
34,object2='box',attribute='color',question=QUESTION)
35FINAL_RESULT=RESULT(var=ANSWER0)
36
37Câu hỏi: Con dao có được làm bằng gốm không?
38Chương trình:
39BOX0=LOC(image=IMAGE,object='knife')
40ANSWER0=VERIFY_MATERIAL(image=IMAGE,box=BOX0,material='ceramic',object=
41'knife',question=QUESTION)
42ANSWER1=EVAL(expr=f"'yes' if {ANSWER0} else 'no'")
43FINAL_RESULT=RESULT(var=ANSWER1)
44
45Câu hỏi: Áo khoác dày hay mỏng?
46Chương trình:
47BOX0=LOC(image=IMAGE,object='coat')
48ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,object='coat',attribute1=
49'thick',attribute2='thin')
50FINAL_RESULT=RESULT(var=ANSWER0)
51.......
52
53Câu hỏi: __INSERT_NEW_QUESTION__
54Chương trình:

Hình 17: Prompt thực thi module (Giai đoạn 3) để phân tích các chương trình cho một trường hợp kiểm tra mới.
