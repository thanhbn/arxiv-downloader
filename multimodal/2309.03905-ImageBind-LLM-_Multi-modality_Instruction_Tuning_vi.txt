# 2309.03905.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.03905.pdf
# Kích thước tệp: 4389698 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ImageBind-LLM: Điều chỉnh hướng dẫn đa phương thức
Jiaming Han1,2†, Renrui Zhang1,2†, Wenqi Shao1†, Peng Gao1‡†, Peng Xu1†,
Han Xiao1†, Kaipeng Zhang1, Chris Liu1, Song Wen1, Ziyu Guo1, Xudong Lu1,2,
Shuai Ren3, Yafei Wen3, Xiaoxin Chen3, Xiangyu Yue2*, Hongsheng Li2*,
Yu Qiao1*
1Shanghai Artificial Intelligence Laboratory, Shanghai, 200030, China.
2CUHK MMLab, Hong Kong SAR, 999077, China.
3vivo AI Lab, Shenzhen, 518000, China.
*Tác giả liên hệ. E-mail: xyyue@ie.cuhk.edu.hk; hsli@ee.cuhk.edu.hk;
qiaoyu@pjlab.org.cn;
Tác giả đóng góp: hanjiaming@pjlab.org.cn; zhangrenrui@pjlab.org.cn;
shaowenqi@pjlab.org.cn; gaopeng@pjlab.org.cn; xupeng@pjlab.org.cn;
†Đóng góp ngang bằng‡Trưởng dự án
Tóm tắt
Chúng tôi trình bày ImageBind-LLM, một phương pháp điều chỉnh hướng dẫn đa phương thức của các mô hình ngôn ngữ lớn (LLM) thông qua ImageBind. Các nghiên cứu hiện tại chủ yếu tập trung vào điều chỉnh hướng dẫn ngôn ngữ và hình ảnh, khác với điều đó, ImageBind-LLM của chúng tôi có thể phản hồi các điều kiện đa phương thức, bao gồm âm thanh, đám mây điểm 3D, video, và phép tính số học không gian nhúng của chúng chỉ bằng cách huấn luyện căn chỉnh hình ảnh-văn bản. Trong quá trình huấn luyện, chúng tôi áp dụng mạng liên kết có thể học để căn chỉnh không gian nhúng giữa LLaMA và bộ mã hóa hình ảnh của ImageBind. Sau đó, các đặc trưng hình ảnh được biến đổi bởi mạng liên kết được thêm vào các token từ của tất cả các lớp trong LLaMA, điều này tiến bộ tiêm các hướng dẫn trực quan thông qua cơ chế cổng không có sự chú ý và được khởi tạo bằng không. Được hỗ trợ bởi việc nhúng kết hợp của ImageBind, việc huấn luyện hình ảnh-văn bản đơn giản cho phép mô hình của chúng tôi thể hiện khả năng tuân theo hướng dẫn đa phương thức vượt trội. Trong quá trình suy luận, các đầu vào đa phương thức được đưa vào các bộ mã hóa ImageBind tương ứng, và được xử lý bởi mô hình bộ nhớ đệm trực quan được đề xuất để tăng cường nhúng đa phương thức. Mô hình bộ nhớ đệm không cần huấn luyện truy xuất từ ba triệu đặc trưng hình ảnh được trích xuất bởi ImageBind, điều này hiệu quả giảm thiểu sự khác biệt về phương thức huấn luyện-suy luận. Đáng chú ý, với cách tiếp cận của chúng tôi, ImageBind-LLM có thể phản hồi các hướng dẫn của nhiều phương thức khác nhau và thể hiện chất lượng tạo ngôn ngữ đáng kể. Mã nguồn được phát hành tại https://github.com/OpenGVLab/LLaMA-Adapter.
Từ khóa: Mô hình Ngôn ngữ Lớn, Học Đa Phương thức, Điều chỉnh Hướng dẫn

1 Giới thiệu
Gần đây, chúng ta đã chứng kiến những tiến bộ đáng kể trong việc điều chỉnh hướng dẫn của các mô hình ngôn ngữ lớn (LLM). Với trí tuệ đa năng và tính tương tác, ChatGPT [4] và GPT-4 [5] trình bày các hệ thống trò chuyện đa mục đích tuân theo hướng dẫn của con người bằng ngôn ngữ và hình ảnh, điều này vẫn chưa thể tái tạo được do hạn chế mã nguồn đóng. Lấy cảm hứng từ điều này, Alpaca [6], LLaMA-Adapter [7], và các nghiên cứu tiếp theo [8-10] đề xuất tinh chỉnh LLaMA có sẵn công khai [11] thành
1arXiv:2309.03905v2  [cs.MM]  11 Sep 2023

--- TRANG 2 ---
LLaMA-Adapter,		Otter,
LLaVA,	MiniGPT-4	…ImageBind-LLM
Chỉ Hình ảnh
Hướng dẫn
Phản hồi
Hình ảnh
Hướng dẫnÂm thanh
Video
…
Phản hồi(a)Mô hình Hướng dẫn Trực quan: (b)Mô hình Hướng dẫn Đa phương thức:
Hướng dẫn Đa phương thức
Hiệu quả Điều chỉnh
Truy xuất Bộ nhớ đệm Đa phương thức Tiêm Không có Sự chú ý Khởi tạo Bằng không
Đám mây Điểm
Văn bảnHình 1 Mô hình Hướng dẫn Trực quan so với ImageBind-LLM Đa phương thức của chúng tôi. Khác với các nghiên cứu hiện tại [1–3] chỉ điều kiện trên phương thức hình ảnh, ImageBind-LLM tiến hành điều chỉnh hướng dẫn đa phương thức tổng quát cho hình ảnh, văn bản, âm thanh, video, và 3D.

các mô hình hướng dẫn ngôn ngữ bằng dữ liệu tự tạo. Hơn nữa, để đạt được điều chỉnh hướng dẫn hình ảnh, LLaVA [2], LLaMA-Adapter [7], và các nghiên cứu khác [3] kết hợp khả năng hiểu trực quan vào LLM để tạo ra điều kiện hình ảnh. Mặc dù hiệu quả của các cách tiếp cận điều chỉnh hướng dẫn hiện tại, cách phát triển một LLM cho hướng dẫn đa phương thức tổng quát, ví dụ, văn bản, hình ảnh, âm thanh, đám mây điểm 3D, và video, vẫn chưa được khám phá đầy đủ.

Trong bài báo này, chúng tôi giới thiệu một mô hình tuân theo hướng dẫn đa phương thức, ImageBind-LLM, mà hiệu quả tinh chỉnh LLaMA, được hướng dẫn bởi không gian nhúng kết hợp trong ImageBind được tiền huấn luyện [12]. Như so sánh trong Hình 1, khác với các mô hình hướng dẫn trực quan trước đây (a), ImageBind-LLM của chúng tôi (b) có thể phản hồi các hướng dẫn đầu vào của nhiều phương thức ngoài hình ảnh, chỉ ra khả năng mở rộng và tổng quát hóa đầy hứa hẹn. Cụ thể, nhờ vào không gian nhúng đa phương thức được căn chỉnh hình ảnh của ImageBind, chúng tôi đề xuất chỉ tận dụng dữ liệu thị giác-ngôn ngữ cho điều chỉnh hướng dẫn đa phương thức. Đối với một cặp hình ảnh-chú thích, trước tiên chúng tôi sử dụng bộ mã hóa hình ảnh đông lạnh của ImageBind để trích xuất đặc trưng hình ảnh toàn cục, và áp dụng mạng liên kết có thể học để biến đổi nhúng. Sau đó, đặc trưng hình ảnh được biến đổi được thêm vào các token từ tại tất cả các lớp transformer trong LLaMA, điều này cung cấp các điều kiện trực quan để tạo ra chú thích văn bản tương ứng. Khác với việc chú ý được khởi tạo bằng không trong chuỗi LLaMA-Adapter [1, 7], phương pháp tiêm trực quan của chúng tôi không có sự chú ý và đơn giản được trọng số bởi một yếu tố cổng được khởi tạo bằng không có thể huấn luyện. Trong cách thức hiệu quả như vậy, các gợi ý hướng dẫn của các nhúng đa phương thức ImageBind có thể được tiêm tiến bộ vào LLaMA khi quá trình huấn luyện tiến hành, mà không làm xáo trộn kiến thức ngôn ngữ gốc.

Sau việc huấn luyện thị giác-ngôn ngữ đơn giản, ImageBind-LLM của chúng tôi có được khả năng tuân theo hướng dẫn của nhiều phương thức khác nhau, bằng cách áp dụng ImageBind cho mã hóa cụ thể theo phương thức, ví dụ, văn bản, hình ảnh, âm thanh, và video. Đối với hướng dẫn trong lĩnh vực 3D, chúng tôi sử dụng bộ mã hóa 3D được tiền huấn luyện trong Point-Bind [13] để mã hóa đầu vào đám mây điểm 3D. Để giảm thiểu sự khác biệt về phương thức của việc huấn luyện hình ảnh và tạo ra điều kiện văn bản/âm thanh/3D/video, chúng tôi tiếp tục đề xuất một mô hình bộ nhớ đệm trực quan không cần huấn luyện để tăng cường nhúng trong quá trình suy luận. Mô hình bộ nhớ đệm chứa hàng triệu đặc trưng hình ảnh trong các bộ dữ liệu huấn luyện được trích xuất bởi ImageBind, điều này cải thiện các nhúng văn bản/âm thanh/3D/video bằng cách truy xuất các đặc trưng trực quan tương tự, tham khảo Tip-Adapter [14]. Điều này góp phần vào các phản hồi ngôn ngữ chất lượng cao hơn cho các hướng dẫn đa phương thức. Trong các tình huống đa dạng, chúng tôi đánh giá khả năng tuân theo hướng dẫn đa phương thức của ImageBind-LLM, và quan sát hiệu suất vượt trội nhất quán.

Nhìn chung, ImageBind-LLM của chúng tôi thể hiện bốn đặc điểm chính như sau.
•Hướng dẫn Đa phương thức. Khác với các mô hình hướng dẫn ngôn ngữ và hình ảnh trước đây, ImageBind-LLM được điều chỉnh để phản hồi các đầu vào đa phương thức tổng quát, chẳng hạn như hình ảnh, văn bản, âm thanh, đám mây điểm 3D, video, và phép tính số học không gian nhúng của chúng được mã hóa bởi ImageBind và Point-Bind.
2

--- TRANG 3 ---
•Hiệu quả Điều chỉnh. Trong quá trình huấn luyện, chúng tôi đông lạnh bộ mã hóa hình ảnh của ImageBind, và tinh chỉnh các trọng số một phần trong LLaMA bằng các kỹ thuật hiệu quả tham số, bao gồm LoRA [15] và điều chỉnh bias-norm [1, 16–19]. Bên cạnh đó, chúng tôi chỉ huấn luyện mạng liên kết bổ sung và các yếu tố cổng được khởi tạo bằng không.
•Tiêm Không có Sự chú ý Khởi tạo Bằng không. Thay vì kết hợp các gợi ý hướng dẫn mới bằng các lớp chú ý, chúng tôi trực tiếp thêm các điều kiện đa phương thức với tất cả các token từ của LLaMA, và áp dụng cơ chế cổng có thể học để tiêm kiến thức tiến bộ, đơn giản và hiệu quả hơn.
•Truy xuất Bộ nhớ đệm Đa phương thức. Để giảm thiểu sự khác biệt về phương thức của việc huấn luyện (chỉ hình ảnh) và suy luận (nhiều phương thức), chúng tôi giới thiệu một mô hình bộ nhớ đệm trực quan được xây dựng bởi các đặc trưng hình ảnh được trích xuất bởi ImageBind, mà tiến hành truy xuất đa phương thức để tăng cường nhúng.

2 Nghiên cứu Liên quan
2.1 Mô hình Hướng dẫn Trực quan.
Với sự phát triển nhanh chóng của khả năng tuân theo hướng dẫn ngôn ngữ [6, 8, 11], cách cho phép các mô hình ngôn ngữ lớn (LLM) thực hiện hiểu biết trực quan cũng đã nhận được sự chú ý đáng kể. LLaMA-Adapter [7], lần đầu tiên, đề xuất tạo ra các phản hồi ngôn ngữ được điều kiện trên đầu vào hình ảnh. Nó tận dụng một bộ mã hóa được tiền huấn luyện để trích xuất các token hình ảnh, và kết hợp chúng với LLaMA bằng tinh chỉnh hiệu quả tham số, tuy nhiên chỉ có thể giải quyết một số tình huống trả lời câu hỏi trực quan ngây thơ, tức là ScienceQA [20]. Đối với các trường hợp tuân theo hướng dẫn trực quan tổng quát hơn, nhiều nỗ lực đã được thực hiện để sản xuất dữ liệu thị giác-ngôn ngữ chất lượng cao để huấn luyện bởi ChatGPT [4] hoặc GPT-4 [21], chẳng hạn như LLaVA [22], MiniGPT-4 [3], và Otter [23]. Họ thường tuân theo kiến trúc của BLIP-2 [24] với Vicuna tiến bộ hơn [9], hoặc tinh chỉnh toàn bộ LLM với tài nguyên huấn luyện tốn kém. LLaMA-Adapter [7] phát triển một chiến lược huấn luyện kết hợp mà chỉ yêu cầu sự kết hợp của các cặp hình ảnh-chú thích và dữ liệu hướng dẫn ngôn ngữ, nhưng vẫn hoạt động tương đương với những mô hình có dữ liệu huấn luyện được xây dựng tinh tế. VideoLLM [25] và Video-LLaMA [26] cũng kết nối các mô-đun lý luận video với LLM để cho phép khả năng tuân theo hướng dẫn video với thông tin thời gian. Khác với chúng, ImageBind-LLM của chúng tôi tiến một bước bằng cách điều chỉnh một LLM đa phương thức được điều kiện trên các câu hỏi ngôn ngữ với đầu vào hình ảnh, video, âm thanh, và đám mây điểm 3D, cho phép các ứng dụng rộng rãi.

2.2 Căn chỉnh Đa phương thức.
Việc kết nối các phương thức khác nhau trong một không gian nhúng chung để xử lý đa phương thức đã nổi lên như một lĩnh vực nghiên cứu quan trọng trong cả thị giác và ngôn ngữ. CLIP [27], ALIGN [28], và Florence [29] sử dụng các mô hình học tương phản đơn giản để căn chỉnh các cặp hình ảnh và văn bản, góp phần vào hiệu suất tổng quát hóa zero-shot đầy hứa hẹn. Flamingo [30], BLIP-2 [24], và MAGIC [31] áp dụng các mạng trung gian để kết nối các bộ mã hóa thị giác và ngôn ngữ được tiền huấn luyện. AudioCLIP [32] và PointCLIP [33] tương ứng mở rộng không gian nhúng của CLIP sang các phương thức khác, chẳng hạn như âm thanh và đám mây điểm 3D. Gần đây, ImageBind [12] được đề xuất để chia sẻ một không gian tiềm ẩn duy nhất với các phương thức khác nhau, bao gồm hình ảnh, video, văn bản, và âm thanh. Lấy cảm hứng từ ImageBind, Point-Bind [13] học cách kết hợp các phương thức đám mây điểm 3D vào ImageBind, và đạt được độ chính xác zero-shot 3D thuận lợi. Trong bài báo này, chúng tôi tập trung vào việc căn chỉnh không gian nhúng chia sẻ trong ImageBind/Point-Bind với LLaMA cho khả năng tuân theo hướng dẫn đa phương thức. PandaGPT [34] cũng nhằm mục đích điều chỉnh một LLM đa phương thức dựa trên ImageBind, mà không thể hỗ trợ đám mây điểm 3D làm đầu vào, và sử dụng một LLM mạnh hơn, Vicuna [9], làm mô hình ngôn ngữ được tiền huấn luyện. Ngược lại, ImageBind-LLM của chúng tôi vẫn dựa trên LLaMA [11] và giới thiệu tiêm không có sự chú ý khởi tạo bằng không độc đáo với truy xuất bộ nhớ đệm đa phương thức để lý luận đa phương thức tốt hơn.

3 Phương pháp
Trong Mục 3.1, trước tiên chúng tôi xem xét ngắn gọn một số nghiên cứu trước đây như một phần sơ bộ, bao gồm ImageBind, các mô hình bộ nhớ đệm, và LLaMA-Adapter. Sau đó, trong Mục 3.2, chúng tôi giới thiệu các chi tiết của điều chỉnh hướng dẫn đa phương thức được đề xuất và suy luận được tăng cường bộ nhớ đệm trong ImageBind-LLM.
3

--- TRANG 4 ---
3.1 Xem xét lại Các nghiên cứu Trước đây
3.1.1 ImageBind
Với một không gian nhúng chung duy nhất, ImageBind [12] đề xuất kết nối năm phương thức khác nhau, tức là văn bản, âm thanh, độ sâu, nhiệt, và Đơn vị Đo lường Quán tính (IMU), tất cả bằng dữ liệu ghép nối hình ảnh. Theo CLIP [27], việc tiền huấn luyện của ImageBind áp dụng mất mát tương phản, mà nhóm các đặc trưng hình ảnh với các phương thức ghép nối khác, và đẩy xa những phương thức không được ghép nối trong không gian nhúng. Tự giám sát bởi dữ liệu ghép nối hình ảnh quy mô lớn, ImageBind học cách mã hóa các phương thức khác nhau thành các nhúng đặc trưng được căn chỉnh, mà có được khả năng zero-shot đa phương thức nổi bật. Sau đó, ImageBind có thể được sử dụng để mở rộng các mô hình thị giác-ngôn ngữ hiện tại để kết hợp các phương thức mới, chẳng hạn như truy xuất văn bản-âm thanh/video, tạo hình ảnh từ âm thanh, và phát hiện đối tượng tham chiếu âm thanh. Lấy cảm hứng từ thuộc tính tập trung vào hình ảnh này, cách tiếp cận của chúng tôi chỉ tiến hành huấn luyện thị giác-ngôn ngữ để căn chỉnh không gian nhúng chung của ImageBind với LLaMA [11], đạt được điều chỉnh hướng dẫn đa phương thức hiệu quả.

3.1.2 LLaMA-Adapter
Như một phương pháp tinh chỉnh hiệu quả tham số mới, LLaMA-Adapter [7] biến đổi LLaMA thành một mô hình hướng dẫn ngôn ngữ chỉ bằng 1.2M tham số trong vòng 1 giờ, mà thể hiện hiệu suất tương đương với Alpaca được tinh chỉnh đầy đủ [6]. Trên cơ sở này, LLaMA-Adapter [7] cũng được đề xuất để đạt được khả năng tuân theo hướng dẫn trực quan vượt trội. Nó áp dụng một mô hình huấn luyện kết hợp cho dữ liệu hướng dẫn hình ảnh-văn bản và chỉ ngôn ngữ, và vẫn có tính năng hiệu quả điều chỉnh bằng cách cập nhật các tham số một phần (14M) trong LLaMA. Một trong những đổi mới cốt lõi của chuỗi LLaMA-Adapter là cơ chế chú ý được khởi tạo bằng không. Họ mã hóa các tín hiệu hướng dẫn thị giác như các token, và nối chúng với các token từ trong LLaMA như tiền tố. Trong mỗi lớp chú ý, một yếu tố cổng có thể học được sử dụng để kiểm soát thích ứng mức độ thông tin kiến thức hướng dẫn mới được kết hợp vào LLM. ImageBind-LLM của chúng tôi cũng áp dụng một chiến lược tiêm có cổng bằng không cho các hướng dẫn đa phương thức, nhưng theo cách không có sự chú ý đơn giản và hiệu quả hơn.

3.1.3 Mô hình Bộ nhớ đệm
Không cần bất kỳ huấn luyện nào, một mô hình bộ nhớ đệm có thể được sử dụng để lưu trữ các đặc trưng và nhãn của một tập huấn luyện, tổ chức chúng như một cơ sở dữ liệu khóa-giá trị. Trong quá trình suy luận, mẫu kiểm tra phục vụ như một truy vấn để truy xuất từ các khóa và tổng hợp các giá trị thông tin thông qua độ tương tự khóa-truy vấn. Bắt đầu từ thuật toán k-Láng giềng Gần nhất truyền thống (k-NN), các mô hình bộ nhớ đệm đã được áp dụng rộng rãi để hỗ trợ các mạng nơ-ron sâu trong ngôn ngữ [35], thị giác 2D [36], và đám mây điểm 3D [37]. Tip-Adapter [14] và các nghiên cứu tiếp theo [37–39] đề xuất lưu trữ các đặc trưng hình ảnh được trích xuất bởi CLIP của dữ liệu few-shot đã cho, và coi mô hình bộ nhớ đệm như một bộ điều hợp không tham số cho các nhiệm vụ downstream. Tương tự, chúng tôi lưu trữ các đặc trưng hình ảnh được trích xuất bởi ImageBind của 1 triệu hình ảnh như cả khóa và giá trị, mà tăng cường các nhúng đa phương thức trong thời gian suy luận.

3.2 ImageBind-LLM
Để có được một mô hình hướng dẫn đa phương thức, chúng tôi đề xuất ImageBind-LLM, mà bao gồm hai giai đoạn huấn luyện: tiền huấn luyện thị giác-ngôn ngữ trên dữ liệu hình ảnh-chú thích (Mục 3.2.1) và điều chỉnh hướng dẫn đa phương thức trên dữ liệu hướng dẫn trực quan (Mục 3.2.2). Bên cạnh đó, chúng tôi cũng đề xuất truy xuất bộ nhớ đệm đa phương thức để tăng cường suy luận (Mục 3.2.3). Mô hình huấn luyện tổng thể của ImageBind-LLM được thể hiện trong Hình 2.

3.2.1 Tiền huấn luyện Thị giác-Ngôn ngữ
Với thuộc tính liên kết phương thức của ImageBind [12], chúng tôi chỉ tinh chỉnh LLaMA [11] để tạo ra các phản hồi ngôn ngữ được điều kiện trên các hình ảnh được mã hóa bởi ImageBind, sau đó, mô hình có thể hiểu nội tại các hướng dẫn của các phương thức khác thông qua các bộ mã hóa ImageBind tương ứng. Do đó, chúng tôi đề xuất chỉ tận dụng dữ liệu thị giác-ngôn ngữ để điều chỉnh một mô hình hướng dẫn đa phương thức. Theo LLaMA-Adapter [7], chúng tôi áp dụng một pipeline huấn luyện hai giai đoạn cho ImageBind-LLM: đầu tiên sử dụng dữ liệu hình ảnh-chú thích quy mô lớn [40–42] để học khả năng phản hồi được điều kiện hình ảnh, sau đó tận dụng dữ liệu tuân theo hướng dẫn [3, 22] để bảo tồn chất lượng tạo câu dài. Mô hình huấn luyện tổng thể của ImageBind-LLM được thể hiện trong Hình 2. Đối với một cặp hình ảnh-chú thích đã cho, trước tiên chúng tôi áp dụng bộ mã hóa hình ảnh đông lạnh
4

--- TRANG 5 ---
…"… bơi lội"IMAGEBIND
Bộ mã hóaMạng Liên kết
LLaMA
Lớp	TransformerCổng Bằng không
+
"Một chú chó vui vẻ đang …"Chú thích:Hình ảnh Huấn luyện:
…Lớp	TransformerCổng Bằng không
…
…Tất cả 32 lớpLặp lại
Thêm vào tất cả token từTất cả 32 lớpThêm vào tất cả token từ++
Tinh chỉnh
Đông lạnh+Cộng Đặc trưng+Nhân Đặc trưng
Đặc trưng Hình ảnh
Được Biến đổiMô hình Huấn luyện của ImageBind-LLM+Điều chỉnh Một phần
Đặc trưng Hình ảnhHình 2 Mô hình Huấn luyện của ImageBind-LLM. Xem xét không gian nhúng chung trong ImageBind [12], chúng tôi chỉ sử dụng các bộ dữ liệu hình ảnh-văn bản để điều chỉnh hướng dẫn đa phương thức của LLaMA [11]. Chúng tôi giới thiệu một mạng liên kết để căn chỉnh nhúng, và một cơ chế được khởi tạo bằng không không có sự chú ý để tiêm kiến thức trực quan.

của ImageBind để trích xuất đặc trưng trực quan toàn cục. Sau đó, chúng tôi biến đổi đặc trưng trực quan với một mạng liên kết có thể học, và thêm nó vào mỗi token từ trong LLaMA. Theo cách không có sự chú ý được khởi tạo bằng không, LLaMA được tiêm bởi điều kiện hình ảnh và tạo ra chú thích hình ảnh đã cho.

Mạng Liên kết.
Trong Hình 3, chúng tôi trình bày các chi tiết của mạng liên kết, mà nhằm mục đích căn chỉnh không gian nhúng giữa ImageBind và LLaMA. Cụ thể, chúng tôi ký hiệu đặc trưng hình ảnh toàn cục chiều CI được mã hóa bởi ImageBind là FI∈R1×CI. Trong mạng liên kết, trước tiên chúng tôi áp dụng một lớp chiếu tuyến tính với ma trận trọng số w0∈RCI×C, được công thức hóa là F0I=FIw0∈R1×C, trong đó C ký hiệu chiều đặc trưng của LLaMA. Lấy cảm hứng từ Mạng Feed-Forward (FFN) trong LLaMA, sau đó chúng tôi nối tiếp ba khối chiếu với RMSNorm [43], hàm kích hoạt SiLU [44], và kết nối dư [45]. Đối với khối thứ (i+1) với Fi I làm đầu vào, chúng tôi công thức hóa việc tính toán Fi+1 I là (việc chuẩn hóa được bỏ qua để đơn giản)

Fi+1I=FiI + (FiIw2·SiLU(FiIw1))w3,0≤i <3 (1)

trong đó w1, w2∈RC×C và w3∈RCh×C, với Ch ký hiệu chiều ẩn. Sau mạng liên kết, chúng tôi có được đặc trưng hình ảnh được biến đổi, TI∈R1×C, mà học cách căn chỉnh không gian nhúng từ ImageBind đến LLaMA.

Tiêm Không có Sự chú ý Khởi tạo Bằng không.
Với đặc trưng hình ảnh được mã hóa TI, các phương pháp hướng dẫn trực quan hiện tại, ví dụ, LLaMA-Adapter [7], LLaVA [22], và MiniGPT-4 [3], nối nó như tiền tố vào dãy token từ {TjW}Nj=1 trong LLaMA, trong đó N ký hiệu độ dài dãy. Sau đó, họ tận dụng các cơ chế tự chú ý trong các khối transformer của LLaMA để kết hợp kiến thức trực quan từ TI đến {TjW}Nj=1. Tuy nhiên, cách tiếp cận dựa trên sự chú ý như vậy không chỉ gây ra ngân sách tính toán bổ sung, mà còn làm tăng độ khó huấn luyện. Trong ImageBind-LLM của chúng tôi, chúng tôi áp dụng một phương pháp đơn giản và hiệu quả hơn bằng tiêm không có sự chú ý khởi tạo bằng không. Chúng tôi trực tiếp thêm đặc trưng hình ảnh TI với mỗi token từ tại tất cả các lớp transformer của LLaMA, mà kết hợp rõ ràng các điều kiện trực quan (và đầu vào đa phương thức trong quá trình suy luận) với kiến thức ngôn ngữ trong LLM. Ngoài ra, để kiểm soát thích ứng mức độ tích hợp, chúng tôi sử dụng một yếu tố cổng có thể học được khởi tạo bằng không, được ký hiệu là gzero. Đối với bất kỳ token từ TjW nào trong LLaMA, chúng tôi công thức hóa việc tiêm trực quan là

Tj=TI·gzero+TjW. (2)

Tương tự như sự chú ý được khởi tạo bằng không trong LLaMA-Adapter [7], yếu tố cổng này có thể tăng tiến bộ trong quá trình huấn luyện, và tiêm thêm ngữ nghĩa trực quan vào LLaMA, góp phần vào việc học ổn định trong giai đoạn huấn luyện sớm.

3.2.2 Điều chỉnh Hướng dẫn Đa phương thức
Vì chúng tôi đã kết nối ImageBind và LLaMA với một mạng liên kết thông qua tiền huấn luyện hình ảnh-văn bản quy mô lớn, ImageBind-LLM có thể hiểu các đầu vào đa phương thức (âm thanh, video, và đám mây điểm 3D), và tạo ra phản hồi ngôn ngữ được điều kiện trên các đầu vào đa phương thức. Tuy nhiên, khác với LLaVA [22] và MiniGPT-4 [3] mà trực tiếp sử dụng một mô hình hướng dẫn ngôn ngữ được huấn luyện tốt Vicuna [9] làm LLM cơ sở, chúng tôi thay vào đó áp dụng một mô hình không có hướng dẫn LLaMA. Do đó, trong giai đoạn huấn luyện thứ hai, chúng tôi điều chỉnh một phần các tham số trong LLaMA để trang bị nó với khả năng tuân theo hướng dẫn, trong khi giữ các bộ mã hóa đa phương thức của ImageBind và mạng liên kết đông lạnh.

Tinh chỉnh Hiệu quả Tham số.
Dữ liệu huấn luyện giai đoạn thứ hai của chúng tôi là sự kết hợp của dữ liệu hướng dẫn ngôn ngữ [10, 46] và dữ liệu hướng dẫn trực quan [22]. Vì dữ liệu hướng dẫn ngôn ngữ không chứa hình ảnh ghép nối, chúng tôi đầu vào một hình ảnh giả (được lấp đầy bằng số không) như trình giữ chỗ trong quá trình huấn luyện. Để duy trì khả năng hiểu đa phương thức của huấn luyện giai đoạn đầu tiên, chúng tôi chỉ tinh chỉnh LLaMA với các phương pháp hiệu quả tham số bao gồm Thích ứng Hạng thấp (LoRA) [15] và điều chỉnh bias-norm [1, 16–19]. Cụ thể, chúng tôi thêm một lớp hạng thấp cho mỗi lớp tuyến tính trong transformer, trong đó hạng được đặt là 16 theo mặc định. Chúng tôi cũng mở đóng băng tất cả các lớp chuẩn hóa và thêm một thành phần bias có thể học vào các lớp tuyến tính. Các tham số của tất cả các mô-đun khác được đông lạnh trong quá trình huấn luyện.

Điều chỉnh Hướng dẫn Chất lượng cao.
Mặc dù ImageBind-LLM được tinh chỉnh có thể tạo ra các phản hồi tuân theo hướng dẫn, chúng tôi chú ý rằng nó thỉnh thoảng tưởng tượng về các đối tượng không tồn tại trong phương thức đầu vào. Do đó, chúng tôi giới thiệu giai đoạn điều chỉnh hướng dẫn bổ sung sử dụng dữ liệu hướng dẫn chất lượng cao từ MiniGPT-4 [3]. Khác với dữ liệu hướng dẫn trực quan được tạo ra bởi ChatGPT/GPT4,
6

--- TRANG 6 ---
IMAGEBIND
Bộ mã hóaChuẩn hóa𝑤!SiLU𝑤"𝑤#𝑤$X 3Mạng Liên kết
Hình ảnh Huấn luyện:
Đặc trưng Hình ảnh
Được Biến đổiĐặc trưng Hình ảnhHình 3 Chi tiết của Mạng Liên kết. Tham khảo Mạng Feed-Forward (FFN) trong LLaMA [11], chúng tôi áp dụng các khối nối tiếp của RMSNorm [43], hàm kích hoạt SiLU [44], và kết nối dư [45]. Điều này nhằm mục đích căn chỉnh đặc trưng hình ảnh từ ImageBind [12] với các nhúng từ của LLaMA.

MiniGPT-4 thu thập thủ công 3.5K dữ liệu mô tả hình ảnh để căn chỉnh thị giác-ngôn ngữ chất lượng cao. Ở đây chúng tôi cũng áp dụng dữ liệu mô tả hình ảnh 3.5K để điều chỉnh hướng dẫn thêm, điều này chỉ mất vài phút.

3.2.3 Truy xuất Bộ nhớ đệm để Suy luận
Sau khi tinh chỉnh bằng dữ liệu hướng dẫn trực quan, ImageBind-LLM có thể suy ra nội tại khả năng tuân theo hướng dẫn đa phương thức. Sau đó, bên cạnh chế độ suy luận ngây thơ (Hình 4 (a)), chúng tôi tiếp tục đề xuất xây dựng một mô hình bộ nhớ đệm trực quan bởi ImageBind để tăng cường nhúng đa phương thức (Hình 4 (b)).

Suy luận Đa phương thức Ngây thơ.
Thông qua mạng liên kết, các đặc trưng hình ảnh từ ImageBind có thể được căn chỉnh tốt với các token từ của LLaMA. Với không gian nhúng đa phương thức chung của ImageBind, ImageBind-LLM của chúng tôi một cách tự nhiên có được khả năng hiểu cho các hướng dẫn đầu vào với nhiều phương thức khác nhau, chẳng hạn như hình ảnh, văn bản, âm thanh, và video, chỉ khi chúng tôi áp dụng các bộ mã hóa tương ứng từ ImageBind trước mạng liên kết. Đối với đám mây điểm 3D, chúng tôi cũng có thể sử dụng bộ mã hóa 3D được tiền huấn luyện của Point-Bind [13] để trích xuất đặc trưng toàn cục, mà chia sẻ cùng không gian nhúng với ImageBind.

Suy luận Được tăng cường Bộ nhớ đệm.
Mặc dù hiệu quả của chế độ ngây thơ đã nêu trên, tồn tại sự khác biệt về phương thức trong ImageBind-LLM giữa huấn luyện và suy luận. Cụ thể, chúng tôi áp dụng bộ mã hóa hình ảnh của ImageBind để huấn luyện, nhưng chuyển sang các bộ mã hóa khác để suy luận, điều này hơi làm xáo trộn mạng liên kết được điều chỉnh và LLaMA. Do đó, chúng tôi xây dựng một mô hình bộ nhớ đệm không cần huấn luyện của các đặc trưng hình ảnh để tăng cường các nhúng đa phương thức trong quá trình suy luận. Như thể hiện trong Hình 4 (a), chúng tôi sử dụng ImageBind để mã hóa một tập con của dữ liệu huấn luyện thị giác-ngôn ngữ, và lưu trữ chúng như cả khóa và giá trị trong mô hình bộ nhớ đệm. Đối với một hướng dẫn đa phương thức đầu vào trong Hình 4 (b), chúng tôi coi đặc trưng được mã hóa bởi ImageBind của nó như truy vấn, FM∈R1×CI, và truy xuất top-k khóa trực quan tương tự từ mô hình bộ nhớ đệm, được ký hiệu là Fkey∈Rk×CI. Chúng tôi công thức hóa độ tương tự cosine top-k là

Stopk=FMFT_key∈R1×k, (3)
7

--- TRANG 7 ---
⋯⋯(a) Xây dựng Mô hình Bộ nhớ đệmHình ảnh từ Bộ dữ liệu Huấn luyện:IMAGEBIND
Bộ mã hóa
IMAGEBINDĐa phương thức⋯⋯Mô hình	Bộ nhớ đệm
Đặc trưng Hình ảnh(b) Suy luận Được tăng cường Bộ nhớ đệmĐầu vào Đa phương thức:
Mô hình	Bộ nhớ đệm
Khóa & Giá trịĐặc trưng Đa phương thức
Ngây thơBộ mã hóa
Truy vấnTruy xuất top-k
Đặc trưng Đa phương thức
Được tăng cườngMạng Liên kếtLLaMA
Hình 4 Truy xuất Bộ nhớ đệm để Suy luận. Để giảm thiểu sự khác biệt huấn luyện-suy luận, chúng tôi xây dựng một mô hình bộ nhớ đệm trực quan không cần huấn luyện của các đặc trưng hình ảnh được mã hóa bởi ImageBind (b). Sau đó, trong quá trình suy luận, chúng tôi tăng cường các nhúng đa phương thức bằng cách truy xuất top-k đặc trưng trực quan tương tự trong mô hình bộ nhớ đệm.

trong đó chúng tôi giả định FM và Fkey đã được chuẩn hóa L2. Sau đó, theo Stopk, chúng tôi tổng hợp các giá trị được lưu trữ tương ứng, Fvalue∈Rk×CI (các đặc trưng hình ảnh tương tự top-k), và thêm kết quả vào đặc trưng gốc FM thông qua một kết nối dư, được công thức hóa là

FeM=α·StopkFvalue + (1−α)·FM, (4)

trong đó α phục vụ như một yếu tố cân bằng. Được hỗ trợ bởi mô hình bộ nhớ đệm, đặc trưng được tăng cường FeM được kết hợp thích ứng với các ngữ nghĩa trực quan tương tự từ mô hình bộ nhớ đệm. Điều này tăng cường chất lượng biểu diễn của các phương thức khác, và giảm thiểu khoảng cách ngữ nghĩa của chúng đến các hình ảnh được sử dụng để huấn luyện. Sau điều này, FeM được đưa vào mạng liên kết để biến đổi đặc trưng và LLaMA để tạo phản hồi.

3.3 Ứng dụng Nâng cao
Bên cạnh khả năng tuân theo hướng dẫn đa phương thức vượt trội, ImageBind-LLM của chúng tôi cũng có thể được mở rộng đến một loạt các ứng dụng nâng cao với những sửa đổi đơn giản.

3.3.1 Điều chỉnh Hướng dẫn Song ngữ
Ngoài hướng dẫn tiếng Anh, ImageBind-LLM có thể dễ dàng được nâng cấp thành một mô hình tuân theo hướng dẫn song ngữ, ví dụ, tiếng Anh và tiếng Trung. Cụ thể hơn, chúng tôi thay thế LLM cơ bản từ LLaMA thành một LLM song ngữ, ChineseLLaMA1 và thêm 52K dữ liệu hướng dẫn tiếng Trung từ GPT4LLM [10] để điều chỉnh hướng dẫn kết hợp. Mặc dù chúng tôi không có dữ liệu hướng dẫn trực quan tiếng Trung trực tiếp cho giai đoạn huấn luyện thị giác-ngôn ngữ đầu tiên, chúng tôi quan sát rằng ImageBind-LLM song ngữ của chúng tôi một cách ngầm định học cách căn chỉnh giữa tiếng Trung, tiếng Anh và các đầu vào đa phương thức, và có thể tuân theo tốt các hướng dẫn tiếng Trung được điều kiện trên các đầu vào phương thức khác.

3.3.2 Tạo Bất kỳ thứ gì thành Bất kỳ thứ gì
Hiện tại, hầu hết các mô hình hướng dẫn đa phương thức bị hạn chế chỉ tạo ra các phản hồi văn bản, thiếu khả năng phản hồi với các đầu ra phương thức khác, ví dụ, hình ảnh, âm thanh, và đám mây điểm. Vì ImageBind là một phần mở rộng của CLIP [27], chúng tôi có thể nối các mô hình tạo ra được điều kiện bởi CLIP sau các bộ mã hóa của ImageBind, chẳng hạn như Stable Diffusion [47], Make-An-Audio [48], và CLIP-Forge [49], tương ứng cho việc tạo hình ảnh, âm thanh, và đám mây điểm. Thay vì trực tiếp đưa các đặc trưng ImageBind vào các mô hình tạo ra này, chúng tôi áp dụng tạo ra được tăng cường bộ nhớ đệm để giảm thiểu sự khác biệt về phương thức, tương tự như cách tiếp cận trong Suy luận Được tăng cường Bộ nhớ đệm (Mục 3.2.3). Theo cách này, chúng tôi có thể đạt được các mô hình hướng dẫn với hệ thống tạo ra bất kỳ thứ gì thành bất kỳ thứ gì, tức là phản hồi các hướng dẫn đa phương thức bằng các phản hồi đa phương thức. Như một ví dụ, ImageBind-LLM của chúng tôi có thể tạo ra cả phản hồi văn bản và hình ảnh cho các đầu vào đa phương thức (ví dụ, hình ảnh, âm thanh và đám mây điểm).

3.3.3 Tích hợp với Phát hiện Đối tượng
Các mô hình hướng dẫn trực quan có thể trả lời câu hỏi dựa trên nội dung toàn cục của hình ảnh đầu vào. Tuy nhiên, chúng không thể liên kết phản hồi văn bản với các đối tượng vùng trong hình ảnh, điều này quan trọng cho các nhiệm vụ tinh vi chẳng hạn như lý luận trực quan và grounding. Chúng tôi cung cấp một giải pháp để kết nối ImageBind-LLM với các bộ phát hiện đối tượng [50]. Đối với một phản hồi được tạo ra bởi ImageBind-LLM, chúng tôi sử dụng các trình phân tích danh từ truyền thống [51] hoặc ChatGPT [4] để trích xuất các danh từ trong phản hồi. Sau đó chúng tôi đưa hình ảnh đầu vào và các danh từ được phân tích vào các bộ phát hiện đối tượng để có được kết quả phát hiện đối tượng. Nói chung, trình phân tích danh từ truyền thống đủ để phân tích các danh từ có ý nghĩa, nhưng nó không thể xử lý các danh từ với các bộ điều chỉnh phức tạp, chẳng hạn như "một chú chó đen đang chạy". Do đó, chúng tôi cũng sẽ yêu cầu ChatGPT trích xuất các danh từ phức tạp trong phản hồi.

3.3.4 ImageBind-LLM như Chatbot
ImageBind-LLM được thiết kế ban đầu như một mô hình hướng dẫn đa phương thức một lượt. Chúng tôi biến ImageBind-LLM thành một chatbot đa lượt bằng cách huấn luyện nó trên dữ liệu hội thoại đa lượt, bao gồm dữ liệu hội thoại ngôn ngữ từ ShareGPT [46] và dữ liệu hội thoại trực quan từ LLaVA [2]. Bằng cách này, ImageBind-LLM có thể được sử dụng như một mô hình trò chuyện đa lượt để trả lời các câu hỏi mở về các đầu vào đa phương thức.
8

--- TRANG 8 ---
3.3.5 ImageBind-LLM để Điều khiển API
Ngoài khả năng tuân theo hướng dẫn đa phương thức chính, ImageBind-LLM cũng thể hiện tiềm năng để gọi các điều khiển API đa dạng cho việc sử dụng công cụ đa phương thức. Để đạt được điều này, chúng tôi tận dụng bộ dữ liệu hướng dẫn liên quan đến công cụ được giới thiệu trong GPT4Tools [52] để trao quyền cho ImageBind-LLM với khả năng sử dụng hiệu quả các công cụ khác nhau. Bằng cách huấn luyện ImageBind-LLM trên bộ dữ liệu GPT4Tools sử dụng mô hình huấn luyện được đề xuất của chúng tôi, chúng tôi quan sát thấy khả năng thành thạo ấn tượng của nó trong việc gọi các API khác nhau, cho phép nó hoàn thành một loạt các nhiệm vụ, ngay cả khi gặp phải các công cụ chưa từng thấy trước đây. Hiệu suất này trong việc điều khiển API làm nổi bật tiềm năng của ImageBind-LLM như một trợ lý trực quan đa năng có khả năng giải quyết các vấn đề thực tế đa dạng.

4 Thí nghiệm
4.1 Chi tiết Huấn luyện
4.1.1 Bộ dữ liệu
Chúng tôi huấn luyện ImageBind-LLM trên một bộ sưu tập dữ liệu cặp hình ảnh-văn bản mã nguồn mở, dữ liệu hướng dẫn chỉ ngôn ngữ và trực quan.

Dữ liệu Cặp Hình ảnh-Văn bản. ImageBind-LLM của chúng tôi được tiền huấn luyện trên việc nối các dữ liệu cặp hình ảnh-văn bản mã nguồn mở, bao gồm COCO [53], CC3M [41], CC12M [42], SBU [54], LAION-2B [40], COYO [55] và MMC4 [56]. Lưu ý rằng MMC4-Core [56] là một tập dữ liệu tỷ lệ hình ảnh được xen kẽ với văn bản. Chúng tôi trích xuất 20M cặp hình ảnh-văn bản chất lượng cao từ MMC4-Core theo điểm căn chỉnh clip được cung cấp. Đối với bộ dữ liệu LAION-2B [40], chúng tôi cũng trích xuất 100M cặp hình ảnh-văn bản chất lượng cao dựa trên điểm căn chỉnh CLIP của chúng. Việc nối tất cả các cặp hình ảnh-văn bản mã nguồn mở dẫn đến 940M dữ liệu cặp hình ảnh-văn bản. Khác với BLIP [57] mà thiết kế một pipeline làm sạch dữ liệu hiệu quả, các cặp hình ảnh-văn bản của chúng tôi nhiều nhiễu hơn nhiều. Tuy nhiên, chúng tôi quan sát thực nghiệm khả năng hiểu hình ảnh và thực tế mạnh mẽ của ImageBind-LLM khi được tiền huấn luyện với bộ dữ liệu này. Trong tương lai, chúng tôi sẽ khám phá các cách tiếp cận tiến bộ để làm sạch dữ liệu và khử trùng lặp.

Bộ dữ liệu Điều chỉnh Hướng dẫn. Dữ liệu điều chỉnh hướng dẫn của chúng tôi bao gồm dữ liệu hướng dẫn ngôn ngữ Alpaca [6], GPT4LLM [10] và ShareGPT [46], dữ liệu hướng dẫn trực quan LLaVA [22] và MiniGPT4 [3]. Đối với dữ liệu hướng dẫn ngôn ngữ, Alpaca chứa 52K dữ liệu hướng dẫn một lượt được thu thập từ GPT3.5; GPT4LLM là phiên bản GPT4 của Alpaca với chất lượng cao hơn; ShareGPT là một bộ sưu tập các cuộc hội thoại được người dùng chia sẻ với ChatGPT/GPT4. Đối với dữ liệu hướng dẫn trực quan, LLaVA áp dụng GPT4 để biến đổi các chú thích hình ảnh hoặc chú thích phát hiện đối tượng thành 150K dữ liệu hướng dẫn trực quan; MiniGPT4 tuyển chọn một bộ dữ liệu mô tả hình ảnh chất lượng cao với 3.5K ví dụ. Lưu ý rằng chúng tôi sẽ chuyển đổi dữ liệu hội thoại đa vòng thành dữ liệu một lượt để điều chỉnh hướng dẫn.

4.1.2 Chi tiết Triển khai
Đối với suy luận được tăng cường bộ nhớ đệm, chúng tôi sử dụng thư viện FAISS [58] để xây dựng hệ thống truy xuất của chúng tôi, và thư viện Autofaiss2 để tìm các siêu tham số tối ưu cho chỉ mục. Theo mặc định, tất cả hình ảnh từ CC3M [41] được sử dụng để xây dựng mô hình bộ nhớ đệm. Chúng tôi tiền huấn luyện mô hình trên 32 GPU A100 trong 3 epochs. Tổng kích thước batch và tốc độ học được đặt lần lượt là 1024 và 4e-4. Chúng tôi tinh chỉnh mô hình trên 8 GPU A100 trong 4 epochs. Epoch khởi động, tổng kích thước batch, tốc độ học được đặt lần lượt là 1, 32 và 1.25e-4.

4.2 Đánh giá Định lượng trên Các Nhiệm vụ Truyền thống
Trong mục này, chúng tôi đã tiến hành đánh giá định lượng ImageBind-LLM trên 27 bộ dữ liệu sử dụng cách tiếp cận zero-shot. Đánh giá định lượng của chúng tôi bao gồm năm nhiệm vụ cụ thể: Nhận dạng Ký tự Quang học (OCR), Trích xuất Thông tin Chính (KIE), Tạo Chú thích Hình ảnh, Trả lời Câu hỏi Trực quan (VQA), và Mô tả Hình ảnh Dựa trên Kiến thức (KGID). Đáng chú ý, tất cả các nhiệm vụ này được đánh giá theo cách tiếp cận kiểu VQA. Các so sánh của ImageBind-LLM với các Mô hình Thị giác-Ngôn ngữ (VLM) nổi tiếng khác như BLIP2 [24], InstructBLIP [60], LLaVA [22], LLaMA-Adapter (LA) [7], và mô hình LLM đa phương thức PandaGPT [34] được trình bày trong Bảng 1 và Bảng 2.

2https://github.com/criteo/autofaiss
9

--- TRANG 9 ---
Bảng 1 Hiệu suất Zero-shot trên các Nhiệm vụ OCR, KIE, và Tạo Chú thích Hình ảnh. Các chỉ số đánh giá bao gồm độ chính xác từ cho các bộ dữ liệu OCR, điểm F1 cấp thực thể cho các bộ dữ liệu KIE, và điểm CIDEr cho các bộ dữ liệu tạo chú thích hình ảnh. ImageBind-LLM-D: ImageBind-LLM được huấn luyện trên dữ liệu hội thoại đa lượt (Mục 3.3.4).

Mô hình BLIP2 InstructBLIP LA LLaVA PandaGPT ImageBind-LLM ImageBind-LLM-D
#Token 32 32 10 257 1 1 1
OCRIIIT5K 80.17 83.90 36.30 31.57 5.27 13.9 13.87
IC13 81.13 82.08 20.87 16.39 4.60 7.43 7.19
IC15 66.68 73.57 29.40 26.58 4.57 11.94 11.36
Total-Text 68.31 71.51 30.93 24.51 4.06 10.79 10.11
CUTE80 85.07 86.11 35.76 36.46 6.60 20.14 20.83
SVT 85.78 86.86 20.40 18.55 3.40 8.35 7.11
SVTP 77.34 80.93 31.01 27.44 4.96 10.39 10.08
COCO-Text 53.62 58.25 20.94 18.05 2.67 5.59 5.12
WordArt 73.66 75.12 38.98 35.87 7.81 21.24 20.58
CTW 67.43 68.58 18.13 16.73 2.74 7.12 7.38
HOST 57.28 61.22 16.60 15.94 3.97 7.53 7.82
WOST 68.83 73.26 21.73 20.49 4.01 8.73 8.57
KIESROIE 0.08 0.09 0.02 0.01 0.01 0.01 0.01
FUNSD 1.02 1.03 2.16 1.93 2.06 2.00 2.01
CaptionNoCaps 48.58 46.33 41.66 33.09 29.65 30.43 29.64
Flickr-30k 46.48 50.45 30.49 27.65 23.02 23.04 23.49

Bảng 2 Hiệu suất Zero-shot trên các Nhiệm vụ VQA, KGID, và VE. Đối với các nhiệm vụ VQA và KGID, Mean Reciprocal Rank (MRR) được sử dụng cho Visdial, trong khi độ chính xác top-1 được sử dụng cho các nhiệm vụ còn lại.

Mô hình BLIP2 InstructBLIP LA LLaVA PandaGPT ImageBind-LLM ImageBind-LLM-D
#Token 32 32 10 257 1 1 1
VQADocVQA 4.75 5.89 8.13 6.26 3.42 4.04 4.08
TextVQA 31.98 39.60 43.76 38.92 16.42 23.98 23.98
STVQA 20.98 28.30 32.33 28.40 11.23 15.55 14.75
OCR-VQA 38.85 60.20 38.12 23.40 22.39 23.24 22.31
OKVQA 44.93 60.52 55.93 54.36 50.85 51.66 51.70
GQA 45.53 49.96 43.93 41.30 41.56 41.23 41.12
Visdial 10.73 45.20 12.92 14.66 90.80 12.66 12.91
IconQA 62.82 56.25 41.83 42.95 46.04 37.97 41.81
VSR 63.63 41.28 50.63 51.24 46.75 49.37 49.78
KGIDScienceQA IMG 60.73 46.26 54.19 49.33 52.80 55.83 51.41
VizWiz 65.44 65.31 62.07 62.42 46.95 51.90 51.28

4.2.1 Cài đặt Thí nghiệm
Nhiệm vụ OCR. Chúng tôi đánh giá ImageBind-LLM trên 12 bộ dữ liệu OCR đại diện, bao gồm IIIT5K [61], ICDAR 2013(IC13) [62], ICDAR 2015 (IC15) [63], Total-Text [64], CUTE80 [65], Street View Text (SVT) [66], SVTP-Perspective (SVTP) [67], COCO-Text [68], WordArt [69], SCUT-CTW1500 (CTW) [70], Heavily Occluded Scene Text (HOST) [71], Weakly Occluded Scene Text (WOST) [71]. Các bộ dữ liệu này bao gồm một bộ sưu tập đa dạng các hình ảnh chứa thông tin văn bản, cho phép so sánh toàn diện giữa các mô hình. Việc đánh giá hiệu suất mô hình dựa trên độ chính xác top-1, sử dụng lời nhắc "Điều gì được viết trong hình ảnh?"

Nhiệm vụ KIE. Chúng tôi đánh giá ImageBind-LLM trên 2 benchmark KIE, bao gồm SROIE [72] và FUNSD citefunsd. Các benchmark này bao gồm một loạt các loại tài liệu đa dạng, bao gồm hóa đơn và biểu mẫu, mà đòi hỏi việc trích xuất thông tin cụ thể. Việc đánh giá các mô hình liên quan đến việc sử dụng điểm F1 cấp thực thể. Để tăng cường thêm quá trình đánh giá, chúng tôi sử dụng các lời nhắc được điều chỉnh theo thông tin cụ thể mà mô hình được yêu cầu trích xuất. Ví dụ, trong trường hợp benchmark SROIE, các lời nhắc như "Tên của công ty đã phát hành hóa đơn này là gì?" được sử dụng để trích xuất thông tin công ty, trong khi các lời nhắc như "Hóa đơn này được phát hành ở đâu?" được sử dụng để trích xuất thông tin địa chỉ.

Nhiệm vụ VQA. Chúng tôi sử dụng 9 benchmark trong nhiệm vụ VQA, cụ thể là DocVQA [73], TextVQA [74], STVQA [75], OCR-VQA [76], OKVQA [77], GQA [78], IconQA [79], Visual Spatial Reasoning (VSR) [80], và Visual Dialog (Visdial) [81]. Các benchmark này bao gồm một bộ sưu tập đa dạng các cặp câu hỏi-hình ảnh che phủ một loạt các chủ đề. Nhiệm vụ đòi hỏi các mô hình không chỉ hiểu nội dung trực quan mà còn hiểu và lý luận về các câu hỏi được trình bày. Để đánh giá cụ thể, chúng tôi sử dụng chỉ số Mean Reciprocal Rank (MRR) cho Visdial và độ chính xác top-1 cho các bộ dữ liệu còn lại. Các chỉ số này cung cấp thông tin có giá trị về khả năng thành thạo của mô hình trong việc trả lời chính xác câu hỏi trên các benchmark VQA khác nhau.

Nhiệm vụ KGID. Nhiệm vụ KGID nhằm đánh giá khả năng của mô hình trong việc tạo ra các chú thích hình ảnh mô tả và chính xác bằng cách kết hợp kiến thức bên ngoài. Để đánh giá hiệu suất trong nhiệm vụ này, chúng tôi sử dụng các benchmark ScienceQA [20] và VizWiz [82], mà bao gồm các hình ảnh đi kèm với các mô tả văn bản và thông tin dựa trên kiến thức. Đáng đề cập là, đối với ScienceQA, chúng tôi chỉ xem xét những mẫu chứa hình ảnh.

4.2.2 Phân tích
Bảng 1 và Bảng 2 rõ ràng thể hiện hiệu suất zero-shot đặc biệt của ImageBind-LLM trên tất cả các nhiệm vụ được đánh giá. Khi nói đến OCR, Tạo Chú thích Hình ảnh, và KGID, ImageBind-LLM đạt được hiệu suất cạnh tranh so với các VLM khác và vượt trội hơn PandaGPT, do đó thể hiện hiệu quả của chiến lược căn chỉnh phương thức của ImageBind-LLM. Hơn nữa, ImageBind-LLM cũng mang lại hiệu suất ấn tượng trên các bộ dữ liệu KIE và VQA.

Tiếp tục điều tra lý do đằng sau hiệu suất tương đối tốt hơn của ImageBind-LLM so với PandaGPT, chúng tôi đi sâu vào các chi tiết triển khai của ImageBind-LLM và PandaGPT. Đầu tiên, chúng tôi quan sát một sự khác biệt đáng kể trong việc sử dụng đặc trưng được trích xuất bởi ImageBind của ImageBind-LLM và PandaGPT. PandaGPT sử dụng một lớp chiếu tuyến tính duy nhất để xử lý đặc trưng được trích xuất bởi ImageBind, trong khi ImageBind-LLM sử dụng một mạng liên kết, mà có thể tạo điều kiện căn chỉnh tốt hơn giữa ngôn ngữ và các phương thức thông qua ImageBind. Một sự khác biệt khác nằm ở việc lựa chọn mô hình LLM của họ, với PandaGPT sử dụng Vicuna và ImageBind-LLM sử dụng LLaMA. Đáng chú ý, Vicuna, được điều chỉnh dựa trên LLaMA và có xếp hạng Elo cao hơn như chỉ ra trong [9], có thể tăng cường khả năng hiểu ngôn ngữ và tạo phản hồi của PandaGPT.

Sau đó đối với lý do tại sao cả ImageBind-LLM và PandaGPT đều có khả năng OCR kém so với các VLM khác, chúng tôi phát hiện rằng cả hai đều chỉ sử dụng một token cho đặc trưng phương thức, trong khi các VLM khác sử dụng ít nhất mười token để nắm bắt thông tin trực quan. Sự khác biệt này có thể cho phép các mô hình VLM khác hiểu tốt hơn thông tin trực quan được mô tả trong hình ảnh. Những kết quả này không chỉ làm nổi bật hiệu suất zero-shot đáng chú ý của ImageBind-LLM trong các nhiệm vụ thị giác và ngôn ngữ khác nhau mà còn nhấn mạnh khả năng hiểu đa phương thức và tạo ra các phản hồi chính xác trong các tình huống đa dạng. Hơn nữa, khả năng thành thạo của mô hình trong hiểu đa phương thức tiếp tục chứng minh tiềm năng của nó như một giải pháp mạnh mẽ và đa năng cho các ứng dụng thực tế.

4.3 Đánh giá Định lượng trên Benchmark MME
4.3.1 Cài đặt Thí nghiệm
Ngược lại với các nhiệm vụ đa phương thức truyền thống, chúng tôi cũng đánh giá ImageBind-LLM của chúng tôi trên một benchmark được đề xuất gần đây, MME [59], mà được thiết kế đặc biệt cho các VLM gần đây. Benchmark MME đo lường có hệ thống hai khả năng đa phương thức của các phương pháp hiện tại: nhận thức và nhận thức. Khả năng trước với 10 nhiệm vụ con đề cập đến việc nhận dạng các đối tượng cụ thể trong hình ảnh, trong khi khả năng sau với 4 nhiệm vụ con thách thức hơn để suy ra các câu trả lời phức tạp từ thông tin trực quan. Đối với mỗi hình ảnh kiểm tra, MME áp dụng một hướng dẫn của một câu hỏi và một mô tả "Vui lòng trả lời có hoặc không", mà nhắc nhở LLM trả lời "có" hoặc "không". Hướng dẫn-câu trả lời ngắn gọn như vậy cho phép so sánh công bằng các LLM mà không bị ảnh hưởng bởi kỹ thuật prompt engineering.

4.3.2 Phân tích
Trong Bảng 3 và 4, chúng tôi lần lượt thể hiện so sánh hiệu suất của các VLM khác nhau trên các nhiệm vụ nhận thức và nhận thức của MME, bao gồm MiniGPT-4 [3], Otter [23], LLaMA-Adapter [7], LLaVA [22], và PandaGPT [34]. Như thể hiện, MiniGPT-4 có thể đạt được điểm số tốt nhất vì nó được huấn luyện trên BLIP-2 được tiền huấn luyện [24]. Otter và PandaGPT được phát triển dựa trên OpenFlamingo [83] và Vicuna [9], mà trang bị chúng với khả năng xử lý ngôn ngữ được khởi tạo tốt. Thay vào đó, tương tự như LLaMA-Adapter, ImageBind-LLM của chúng tôi được tinh chỉnh trên mô hình LLaMA gốc, và vẫn hoạt động cạnh tranh với những mô hình khác. Đặc biệt trên 'Existence' và 'Artwork', ImageBind-LLM vượt trội hơn các phương pháp đứng thứ hai lần lượt +8.33 và +1.00 điểm. Nhìn chung, cách tiếp cận của chúng tôi chuyên môn hơn ở các nhiệm vụ 'Perception', xếp hạng thứ ba và vượt trội hơn một mô hình đa phương thức khác, PandaGPT, +133.18 điểm. Như phân tích ở trên trong Mục 3.2, chúng tôi tin rằng hiệu suất của chúng tôi có thể được cải thiện thêm nếu sử dụng nhiều token đa phương thức hơn được đưa vào LLM.

4.4 Phân tích Định tính
Trong mục này, chúng tôi sẽ đưa ra các ví dụ và phân tích định tính để giúp hiểu cách ImageBind-LLM hoạt động, và khả năng hướng dẫn đa phương thức của nó đến từ đâu.

4.4.1 Hiểu Đa phương thức
Căn chỉnh Đa phương thức với Văn bản. Giai đoạn tiền huấn luyện thị giác-ngôn ngữ là thiết yếu để kết hợp thông tin đa phương thức vào LLM. Trong Hình 5, chúng tôi đưa ra một số kết quả tạo chú thích đa phương thức sử dụng ImageBind-LLM được tiền huấn luyện. Như chúng ta có thể thấy, ImageBind-LLM có thể tạo ra các đầu ra phụ thuộc vào phương thức cho hình ảnh, âm thanh, video và đám mây điểm. Vì ImageBind-LLM được tiền huấn luyện với các cặp hình ảnh-văn bản, nó có thể đưa ra một mô tả ngắn và chính xác về hình ảnh. Nhờ vào thuộc tính liên kết của ImageBind, ImageBind-LLM có thể kết nối các phương thức khác với LLM mà không cần bất kỳ huấn luyện lại nào. Mặc dù ImageBind-LLM được tiền huấn luyện không thể tạo ra mô tả dài và chi tiết về đầu vào, chúng tôi tin rằng khả năng cơ bản có được ở giai đoạn này rất quan trọng cho việc điều chỉnh hướng dẫn tiếp theo.

Điều chỉnh Hướng dẫn Đa phương thức. Bằng cách tinh chỉnh ImageBind-LLM trên sự kết hợp của dữ liệu hướng dẫn ngôn ngữ và dữ liệu hướng dẫn trực quan, chúng tôi trang bị ImageBind-LLM với khả năng tuân theo hướng dẫn cả ngôn ngữ và trực quan. Quan trọng hơn, chúng tôi có thể nhập các hướng dẫn dạng tự do cho các phương thức khác ngoài hình ảnh. Lấy Hình 6 làm ví dụ, ImageBind-LLM không chỉ có thể trả lời câu hỏi về hình ảnh, mà còn tuân theo hướng dẫn của các phương thức khác. Chúng tôi có thể yêu cầu mô hình viết một bài thơ dựa trên âm thanh (mục thứ 4 trong Hình 6), giới thiệu nội dung của một bộ phim (mục thứ 5 trong Hình 6) và thậm chí mô tả một đối tượng 3D (mục 7-8 trong Hình 6).

4.4.2 Kết hợp Phương thức
Vì ImageBind-LLM thống nhất các phương thức khác nhau trong một không gian đặc trưng, chúng tôi có thể kết hợp các phương thức khác nhau làm đầu vào cuối cùng. Cụ thể, trước tiên chúng tôi sử dụng các bộ mã hóa phương thức để trích xuất các token phương thức. Sau đó chúng tôi kết hợp các token phương thức bằng một tập hợp các hệ số. Cuối cùng, các token được kết hợp được đưa vào LLM để lý luận và hiểu đa phương thức. Ví dụ, kết hợp hình ảnh và âm thanh cho phép mô hình hiểu tốt hơn về cảnh hiện tại (mục 1-2 trong Hình 12); Sự kết hợp của hình ảnh và đám mây điểm làm phong phú nội dung của bức tranh (mục thứ 3 trong Hình 7); Sử dụng khung hình video và âm thanh tương ứng làm đầu vào tạo điều kiện cho hiểu video.

4.5 Ứng dụng Nâng cao
ImageBind-LLM Song ngữ. Bằng cách tinh chỉnh kết hợp mô hình trên cả dữ liệu hướng dẫn tiếng Anh và tiếng Trung, chúng tôi biến ImageBind-LLM thành một mô hình song ngữ. Như thể hiện trong Hình 8, ImageBind-LLM có thể tạo ra phản hồi tiếng Trung chính xác cho một hướng dẫn tiếng Trung. Tuy nhiên, vì khả năng hướng dẫn trực quan tiếng Trung của chúng tôi xuất hiện theo cách zero-shot, nó thường không tạo ra các phản hồi tiếng Trung dài và chi tiết. Chúng tôi tin rằng thông qua một LLM song ngữ mạnh hơn và thu thập dữ liệu hướng dẫn trực quan tiếng Trung, khả năng hướng dẫn tiếng Trung của nó có thể được cải thiện trong tương lai.

Tích hợp với Stable Diffusion. Với LLM, chúng tôi thực hiện giải mã bất kỳ thứ gì thành ngôn ngữ. Tương tự, chúng tôi sử dụng Stable Diffusion để giải mã bất kỳ thứ gì thành hình ảnh. Như thể hiện trong Hình 8, ImageBind-LLM tạo ra cả phản hồi ngôn ngữ và hình ảnh cho một đầu vào đã cho (ví dụ, hình ảnh, âm thanh và đám mây điểm). So với phản hồi ngôn ngữ, hình ảnh được tạo ra mang nhiều chi tiết hơn về đầu vào. Ví dụ, một hình ảnh được tạo ra từ tiếng còi xe có thể mô tả rõ ràng cảnh và các yếu tố chính của nó.
13

--- TRANG 13 ---
Hướng dẫn: Kể cho tôi về trò chơi này.
Trả lời: Trò chơi có tên là "The Legend of Zelda: Breath of the Wild," là một trò chơi phiêu lưu hành động phổ biến được phát triển bởi Nintendo cho máy chơi Nintendo Switch và Wii U. Trò chơi được đặt trong vương quốc hư cấu Hyrule và theo dõi câu chuyện của Link, một cậu bé trẻ bắt đầu một nhiệm vụ để cứu vương quốc và giải cứu Công chúa Zelda. Trò chơi có tính năng khám phá thế giới mở, giải đố, và nhiều cơ chế chiến đấu khác nhau, làm cho nó trở thành một trải nghiệm hấp dẫn và đắm chìm cho người chơi.Hình ảnh

Hướng dẫn: Thành phố này đã chụp ảnh ở đâu và tại sao?
Trả lời: Bức ảnh được chụp ở Thượng Hải, Trung Quốc. Sự hiện diện của những tòa nhà cao tầng, dòng sông, và đường chân trời cho thấy rằng hình ảnh được chụp trong một thành phố lớn với cảnh quan đô thị nổi bật. Thượng Hải nổi tiếng với kiến trúc hiện đại, chẳng hạn như Tháp Ngọc trai Phương Đông, và gần với sông Hoàng Phố, là một điểm thu hút khách du lịch phổ biến. Đường chân trời của thành phố, với những tòa nhà cao tầng và dòng sông, tạo ra một phông nền hấp dẫn về mặt thị giác và biểu tượng cho hình ảnh.

Hướng dẫn: Bạn cảm thấy thế nào khi nghe nhạc này?
Trả lời: Khi tôi nghe nhạc này, tôi cảm thấy một cảm giác phiêu lưu và hứng thú. Sự kết hợp của nhịp điệu điện tử, hình ảnh chủ đề không gian, và bảng màu đỏ đen tạo ra một bầu không khí ly kỳ mời gọi người nghe khám phá những thế giới mới và bắt đầu một cuộc hành trình khám phá. Âm nhạc cũng có thể gợi lên cảm giác tự do và độc lập, vì nó có thể được liên kết với không gian vũ trụ hoặc những điểm đến xa xôi khác.Âm thanh

Hướng dẫn: Viết một bài thơ dựa trên âm thanh này.
Trả lời: Giữa những con sóng vỗ, một con chim cô đơn bay; Bay cao trên bầu trời xanh của đại dương; Những con sóng vỗ, gió thổi; Một bản giao hưởng của thiên nhiên, một cảnh tượng đáng xem.

Hướng dẫn: Giới thiệu cho tôi bộ phim này.
Trả lời: Bộ phim có tựa đề "Your Name" và là một bộ phim hoạt hình Nhật Bản được phát hành vào năm 2016. Đây là một câu chuyện ấm áp và đầy cảm xúc về hai thiếu niên khám phá ra mối liên hệ giữa cuộc sống của họ, và hành trình của họ để tìm ra sự thật đằng sau trải nghiệm chung. Bộ phim đã nhận được sự hoan nghênh rộng rãi từ giới phê bình và đã thành công về mặt thương mại, giành được nhiều giải thưởng và danh hiệu, bao gồm Giải thưởng Viện Hàn lâm cho Phim Hoạt hình Hay nhất. Bộ phim nổi tiếng với hoạt hình đẹp, cách kể chuyện hấp dẫn, và chủ đề sâu sắc.Video

Hướng dẫn: Bạn có thấy môn thể thao này thú vị không? Mô tả video này.
Trả lời: Vâng, môn thể thao này thú vị, vì nó thể hiện trải nghiệm ly kỳ của nhảy dù đồng thời cung cấp góc nhìn độc đáo về cảnh quan bên dưới. Video ghi lại các nhà nhảy dù giữa không trung, thực hiện các thủ thuật và tận hưởng cảm giác hồi hộp của môn thể thao. Hình ảnh của người nhảy dù bay qua bầu trời với cảnh quan bên dưới rất tuyệt đẹp về mặt thị giác và nắm bắt được bản chất của sự tự do và phiêu lưu liên quan đến nhảy dù.

Hướng dẫn: Mô tả đối tượng 3D một cách chi tiết.
Trả lời: Đối tượng 3D là một cây đàn piano, là một nhạc cụ có bàn phím và một bộ dây đàn. Cây đàn piano có bàn phím piano, bao gồm các phím trắng và đen được sắp xếp theo một mẫu giống như lưới. Các phím được kết nối với các búa đánh vào dây đàn, tạo ra âm thanh của piano. Cây đàn piano trong hình ảnh là một mô hình nhỏ, hai tầng, với bảng màu nâu và trắng. Cây đàn piano được đặt trên một chiếc ghế dài, là một phụ kiện tiêu chuẩn cho piano. Chiếc ghế dài có thiết kế hỗ trợ cho phép người chơi ngồi thoải mái khi chơi nhạc cụ.Đám mây Điểm

Hướng dẫn: Mô tả đối tượng 3D một cách chi tiết.
Trả lời: Đối tượng 3D là một chậu hoa có một cây bên trong. Cây là một cây nhỏ, xanh, và có lá, đang phát triển ra khỏi chậu. Chậu được làm bằng đất sét và có màu nâu. Cây được đặt về phía trước của chậu, với lá của nó lan ra ngoài. Chậu được đặt trên một bàn, và góc nhìn cận cảnh của cây và chậu thể hiện các chi tiết của cây và thiết kế của chậu.Hình 6 Ví dụ Hướng dẫn Đa phương thức với ImageBind-LLM.
14

--- TRANG 14 ---
Hướng dẫn: Đoán tâm trạng của cô gái dựa trên nhạc nền và giải thích lý do.
Trả lời: Cô gái trong hình ảnh dường như đang trong tâm trạng suy tư hoặc u sầu, vì nhạc nền là một giai điệu piano buồn. Sự lựa chọn âm nhạc này gợi ý rằng cảnh được dự định để gợi lên cảm giác nội tâm, cô độc, hoặc khao khát. Sự kết hợp của trang phục của cô gái, cây cô đơn, và âm nhạc u sầu tạo ra tâm trạng nội tâm và suy tư. Việc sử dụng âm nhạc như vậy có thể giúp tăng cường tác động cảm xúc của cảnh và cung cấp hiểu biết sâu hơn về suy nghĩ và cảm xúc của nhân vật.Hình ảnh + Âm thanh

Hướng dẫn: Mô tả cảnh dựa trên hình ảnh đầu vào và âm thanh.
Trả lời: Cảnh được mô tả trong hình ảnh là một con phố thành phố bận rộn đầy xe cộ. Có nhiều ô tô, xe tải, và xe buýt đang lái xuống phố, tạo ra một bầu không khí nhộn nhịp. Phố đông đúc với xe cộ, và giao thông có vẻ di chuyển chậm. Âm thanh đi kèm với hình ảnh gợi ý rằng giao thông khá tắc nghẽn, với tiếng còi và những tiếng ồn khác điển hình của một con phố thành phố bận rộn. Hình ảnh nắm bắt bản chất của cuộc sống đô thị, thể hiện sự hối hả và nhộn nhịp của cuộc sống thành phố hàng ngày.

Hình ảnh + Đám mây Điểm
Hướng dẫn: Mô tả cảnh này.
Trả lời: Hình ảnh có một chiếc xe màu xanh lam đang lái xuống con đường có hàng cây, được bao quanh bởi một khu rừng xanh tươi. Chiếc xe ở giữa đường, và nó có vẻ là một chiếc xe nhỏ gọn hoặc nhỏ. Có một số cây ở cả hai bên đường, tạo ra một bầu không khí thanh bình và yên tĩnh. Chiếc xe đang di chuyển dọc theo con đường, có thể thưởng thức vẻ đẹp cảnh quan của khu rừng.

Video + Âm thanh
Hướng dẫn: Người đàn ông đang làm gì trong video này? Mọi người nghĩ gì về anh ấy dựa trên âm thanh?
Trả lời: Người đàn ông đang đứng trước bục, đang phát biểu hoặc thuyết trình. Mọi người trong khán giả đang lắng nghe anh ấy một cách chú ý, và họ đang vỗ tay và thể hiện sự đánh giá cao cho lời nói của anh ấy.Hình 7 Kết hợp Phương thức với ImageBind-LLM.

Tích hợp với Các Chuyên gia Phương thức.
ImageBind-LLM là một LLM đa phương thức tổng quát, nhưng nó vẫn thiếu kiến thức cụ thể về lĩnh vực tinh tế. Do đó, tích hợp với các chuyên gia lĩnh vực là một cách khả thi để cải thiện khả năng hiểu đa phương thức của ImageBind-LLM. Trong Hình 9, chúng tôi sử dụng đầu ra của một chuyên gia speech-to-text như bối cảnh để tăng cường khả năng hiểu âm thanh của ImageBind-LLM. Chúng tôi cũng chỉ ra rằng các nhãn lớp được trích xuất bởi một bộ phát hiện đối tượng video có thể giúp mô hình nắm bắt chi tiết của một đoạn video.

ImageBind-LLM với Cascade Phát hiện Đối tượng. Bằng cách cascade ImageBind-LLM với các bộ phát hiện đối tượng, chúng tôi có thể liên kết phản hồi văn bản với các đối tượng trong hình ảnh, điều này giúp cải thiện độ tin cậy của các LLM đa phương thức. Như thể hiện trong Hình 10, chúng tôi có thể đánh giá độ tin cậy của ImageBind-LLM thông qua kết quả phát hiện. Đồng thời, ImageBind-LLM làm cho các bộ phát hiện đối tượng cũng có khả năng lý luận.

ImageBind-LLM như Chatbot. Tận dụng dữ liệu hội thoại đa lượt để huấn luyện, ImageBind-LLM thể hiện khả năng hiểu trực quan ấn tượng và kỹ năng hội thoại đa phương thức. Như thể hiện trong Hình 11, ImageBind-LLM xuất sắc trong những khả năng này. Hơn nữa, nó thể hiện khả năng hiểu và lý luận với nội dung đa phương thức để phản hồi các truy vấn của người dùng, làm cho nó trở thành một ứng viên khả thi cho một chatbot đa phương thức.

ImageBind-LLM để điều khiển API. Bằng cách kết hợp kiến thức sử dụng công cụ vào LLM, ImageBind-LLM có thể học hiệu quả để gọi các điều khiển API, cho phép nó giải quyết các nhiệm vụ trực quan khác nhau. Như minh họa trong Hình 12, ImageBind-LLM được tinh chỉnh thể hiện khả năng thành thạo trong việc hoàn thành các nhiệm vụ trực quan đa dạng, bao gồm nhưng không giới hạn ở phát hiện đối tượng và chỉnh sửa hình ảnh, bằng cách sử dụng hiệu quả các API khác nhau. Những kết quả này làm nổi bật tiềm năng của ImageBind-LLM được đề xuất
15

--- TRANG 15 ---
Hướng dẫn: Tên của nơi này là gì? Giới thiệu ngắn gọn về nó.
Trả lời: Nơi này được gọi là Mont Saint-Michel, một đảo commune lịch sử nổi tiếng nằm ở Normandy, Pháp. Đây là một địa điểm biểu tượng đã là một điểm chiến lược trong nhiều thế kỷ do vị trí của nó trên biển và các công trình phòng thủ. Đảo này có một tu viện và một lâu đài, và nó được kết nối với đất liền bằng một con đường. Mont Saint-Michel là Di sản Thế giới UNESCO và một điểm đến du lịch phổ biến, thu hút hàng triệu du khách mỗi năm.

Hướng dẫn (tiếng Trung): 这个地方叫什么名字？位于哪里？
Trả lời (tiếng Trung): 这个地方叫做 "Mont Saint-Michel"，位于法国的 Normandy 地区。

(tiếng còi xe)Hướng dẫn: Cảnh này là gì? Đoán tâm trạng của người liên quan.
Trả lời: Cảnh này là một con phố bận rộn có xe hơi và xe tải đang lái xuống đường. Người liên quan là một phụ nữ đang đi bộ trên vỉa hè. Dựa trên hình ảnh, khó xác định chính xác tâm trạng của người phụ nữ, vì cô ấy không trực tiếp nhìn thấy trong hình ảnh. Tuy nhiên, có thể cô ấy cảm thấy bực bội hoặc khó chịu do giao thông hoặc con phố tắc nghẽn.

Hướng dẫn (tiếng Trung): 这是什么场景？推测下当事人的心情。
Trả lời (tiếng Trung): 这是一个城市街道，车辆正在行驶，拥堵的交通可能会导致愤怒和不满。

Hướng dẫn: Mô tả đối tượng 3D một cách chi tiết.
Trả lời: Đối tượng 3D là một chai, có thể là một chai nước. Nó được làm bằng nhựa và có hình dạng hình trụ với cổ hẹp và thân rộng hơn. Chai trong suốt, cho phép chúng ta nhìn thấy nội dung của nó. Nó được đặt trên mặt đất, dựa vào một bức tường đá hoặc bức tường đá, mang lại cảm giác ổn định. Chai có thể được sử dụng bởi người trong hình ảnh như một nguồn nước uống trong cuộc đi bộ hoặc leo núi của họ.

Hướng dẫn (tiếng Trung): 请描述这个三维物体的细节。
Trả lời (tiếng Trung): 这个三维物体是一个短而圆的玻璃瓶子，有着明亮的颜色。它有一个圆形的瓶身，有两个短而圆的瓶颈，它们相互连接并固定在瓶身上。这个瓶子有一个圆形的瓶底。 phương thức đầu vào hình ảnh được tạo raHình 8 Ví dụ Nâng cao với ImageBind-LLM Song ngữ và Tích hợp với Stable Diffusion.

Hướng dẫn: Bài hát này truyền tải điều gì?
không có chuyên gia: Âm thanh truyền tải một thông điệp khuyến khích người xem "tin vào sức mạnh của tình yêu."
với chuyên gia Speech-to-text: (văn bản được nhận dạng: And ah, take me out, and take me home forever and ever. You're my mymymyLover.) Âm thanh truyền tải một thông điệp mạnh mẽ và đầy cảm xúc, có thể thể hiện mong muốn của một người phụ nữ được ở bên người yêu mãi mãi. Lời bài hát gợi ý một mối liên kết mạnh mẽ, lâu dài giữa hai người, và sự sẵn sàng của người phụ nữ cam kết với mối quan hệ này. Bài hát có thể gợi lên cảm giác yêu thương, đam mê, và lòng trung thành, vì đây là một bản ballad lãng mạn.Speech-to-Text

Hướng dẫn: Con mèo này đang làm gì?
không có chuyên gia: Con mèo đang đứng trên bàn và xem hai con cá bơi trong bát.
với chuyên gia Video Object Detection: (các lớp đối tượng được phát hiện: mèo, cá nhỏ, tôm, bồn nước, sàn nhà) Con mèo đang đứng trên sàn nhà bên cạnh một bồn nước, và nó có vẻ đang quan sát hoặc có thể đang chơi với một con cá nhỏ hoặc con tôm đang bơi trong nước.Video Object Detection

Hình 9 Ví dụ Nâng cao bằng cách Tích hợp ImageBind-LLM với Các Chuyên gia Phương thức.

trong bối cảnh sử dụng công cụ đa phương thức.

4.6 Trường hợp Thất bại
Mặc dù ImageBind-LLM có thể xử lý đầu vào đa phương thức đồng thời, nó không hoàn hảo. Như thảo luận trong phân tích định lượng trước đây, ImageBind-LLM yếu so với các VLM khác. Đầu tiên, ImageBind-LLM thường gặp phải vấn đề ảo giác cho các hướng dẫn mô tả. Như thể hiện trong Hình 13, ImageBind-LLM có xu hướng mô tả các đối tượng không được hiển thị trong hình ảnh. Có thể có hai lý do có thể: (1) ImageBind-LLM chỉ tiêm một token trực quan toàn cục vào LLM, nhỏ hơn nhiều so với các mô hình khác (10 cho LLaMA-Adapter, 32 cho MiniGPT4 và 256 cho LLaVA). Do đó, LLM không thể có được đủ thông tin trực quan trong các lớp tự-Attention. (2)
16

--- TRANG 16 ---
Hướng dẫn: Người đàn ông đang làm gì?
Trả lời: Người đàn ông đang chơi tennis trên sân, cầm vợt tennis, và chuẩn bị đánh bóng.

Danh từ: Người đàn ông, tennis, sân, vợt tennis, quả bóng.

Hướng dẫn: Con chó muốn làm gì?
Trả lời: Con chó muốn ăn cốc cà phê, có thể vì nó là một con chó và bị thu hút bởi thức ăn, đặc biệt khi nó được đặt trên một bề mặt như bàn.

Danh từ: Con chó, cốc cà phê, thức ăn, bàn.Hình 10 Ví dụ Nâng cao với Cascade Phát hiện Đối tượng.

Chất lượng của dữ liệu điều chỉnh hướng dẫn không đủ cao. Ví dụ, dữ liệu hướng dẫn trực quan từ LLaVA đều được tạo ra bởi các chuyên gia thị giác và GPT4, thiếu kiểm tra và sửa chữa của con người. Do đó, chúng tôi sẽ xây dựng một bộ dữ liệu chất lượng cao được xác minh bởi con người trong tương lai. Thứ hai, việc kết hợp phương thức dễ vỡ khi hai phương thức đại diện cho các khái niệm khác nhau. Hình 13 đưa ra một ví dụ về việc kết hợp một hình ảnh "bầu trời" và âm thanh "chó sủa". Chúng ta có thể thấy rằng ImageBind-LLM nhạy cảm với tỷ lệ kết hợp phương thức.

5 Kết luận
Trong công trình này, chúng tôi đề xuất điều chỉnh LLM thành các mô hình hướng dẫn đa phương thức với sự giúp đỡ của ImageBind, có tên ImageBind-LLM. Ngược lại với các mô hình hướng dẫn ngôn ngữ và hướng dẫn hình ảnh trước đây, ImageBind-LLM thống nhất hình ảnh, âm thanh, đám mây điểm 3D và video thành một LLM đa phương thức. Chúng tôi đạt được điều này bằng cách căn chỉnh đơn giản bộ mã hóa trực quan của ImageBind với một LLM thông qua một mạng liên kết có thể học. Nhờ vào thuộc tính liên kết của ImageBind, chúng tôi có thể trực tiếp đưa đầu vào đa phương thức vào ImageBind-LLM để suy luận mà không cần bất kỳ huấn luyện nào. Chúng tôi cũng đề xuất một mô hình bộ nhớ đệm hình ảnh không cần huấn luyện để giảm thiểu sự khác biệt về phương thức giữa huấn luyện và suy luận. Chúng tôi đánh giá ImageBind-LLM trên 27 bộ dữ liệu thị giác-ngôn ngữ truyền thống và một benchmark LLM đa phương thức mới MME, nơi ImageBind-LLM đạt được hiệu suất tương đương với các LLM đa phương thức gần đây. Chúng tôi cũng đưa ra phân tích định tính mở rộng để chứng minh khả năng hiểu đa phương thức của ImageBind-LLM. Trong tương lai, chúng tôi dự định tăng cường ImageBind-LLM bằng cách tăng số lượng token đa phương thức. Bên cạnh đó, tích hợp nhiều phương thức hơn vào ImageBind-LLM cũng là một cách tiếp cận đầy hứa hẹn để tăng cường khả năng đa phương thức của nó.

Tài liệu tham khảo
[1] Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., Qiao, Y.: LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (2023)

[2] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint arXiv:2304.08485 (2023)

[3] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023)

[4] ChatGPT. https://chat.openai.com (2023)

[5] OpenAI: Gpt-4 technical report. ArXiv abs/2303.08774 (2023)

[6] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto,
17

--- TRANG 17 ---
Giới thiệu cho tôi về bức tranh này.
Bức tranh là một tác phẩm nghệ thuật nổi tiếng có tựa đề "Starry Night" của Vincent van Gogh. Nó được tạo ra vào năm 1889 và là một trong những bức tranh được nhận dạng và biểu tượng nhất trên thế giới. Bức tranh mô tả một bầu trời đêm với một khối sao xoáy, một mặt trăng lưỡi liềm, và một ngôi làng nhỏ ở khoảng cách xa. Việc sử dụng màu sắc táo bạo, rực rỡ và nét vẽ độc đáo của nghệ sĩ đã làm cho bức tranh này trở thành một kiệt tác vượt thời gian. Bức tranh là một minh chứng cho niềm đam mê nghệ thuật của van Gogh và khả năng nắm bắt vẻ đẹp và sự kỳ diệu của bầu trời đêm.

Kể cho tôi thêm về tác phẩm của ông.
Vincent van Gogh là một họa sĩ hậu ấn tượng người Hà Lan được công nhận rộng rãi về phong cách độc đáo và chiều sâu cảm xúc trong các bức tranh của ông. Ông nổi tiếng với kiệt tác "Starry Night," nhưng ông cũng tạo ra nhiều tác phẩm biểu tượng khác, bao gồm "The Café Terrace at Night," "The Wheat Field with Starry Sky," "The Bedroom," "The Sower," "The Potato Eaters," "The Starry Night Over the Rhone," và "Irises," trong số những tác phẩm khác. Sự nghiệp nghệ thuật của van Gogh chỉ kéo dài mười năm, từ 1880 đến 1890, trong thời gian đó ông đã tạo ra hơn 2.000 bức tranh và bản vẽ. Ông nổi tiếng với loạt tranh của mình, bao gồm "Starry Night," "Irises," "The Wheat Field with the Starry Sky," và "The Café Terrace at Night." Phong cách nghệ thuật của van Gogh đã được ngưỡng mộ và có ảnh hưởng lớn, và tác phẩm của ông đã là chủ đề của nhiều triển lãm và nghiên cứu. Ông được coi là một trong những nghệ sĩ vĩ đại nhất mọi thời đại, và các bức tranh của ông tiếp tục truyền cảm hứng và làm say mê những người yêu nghệ thuật và nhà sưu tập.

+

Mô tả nội dung trong video.
Video cho thấy hai cô gái trẻ ngồi trên cỏ, có thể đang nghỉ ngơi sau khi đi bộ hoặc tận hưởng thời gian bên nhau. Họ được bao quanh bởi cùng một cánh đồng cỏ hoặc đồng cỏ như trước đây. Cảnh có vẻ là một khoảnh khắc yên bình và thư giãn được chia sẻ giữa hai cô gái.

Thời tiết như thế nào?
Thời tiết trong hình ảnh có vẻ nắng và dễ chịu, vì các cô gái đang ngồi bên ngoài dưới ánh nắng mặt trời.

Đoán tâm trạng của các cô gái dựa trên âm thanh.
Dựa trên âm thanh, tâm trạng của các cô gái có vẻ vui vẻ và thư giãn. Họ có thể đang có khoảng thời gian vui vẻ bên nhau, tận hưởng sự đồng hành và khung cảnh ngoài trời đẹp.Hình ảnh
Âm thanhVideoHình 11 Ví dụ Nâng cao với ImageBind-LLM như Chatbot.

T.B.: Stanford Alpaca: An Instruction-following LLaMA model. GitHub (2023)

[7] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., Qiao, Y.: Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199 (2023)

[8] Alpaca-LoRA. https://github.com/tloen/alpaca-lora (2023)
18

--- TRANG 18 ---
Cung cấp một hình ảnh có tên image/cd72c8a5.png.
Mô tả là: một phụ nữ đi qua một khu rừng đầy cây cối. Hiểu hình ảnh bằng cách sử dụng công cụ.
Đã nhận.
image/cd72c8a5.png Phát hiện người phụ nữ trong bức tranh.
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Phát hiện Đối tượng Cho trước
Đầu vào Hành động: image/cd72c8a5.png, người phụ nữ
Gọi API: Phát hiện Đối tượng
Quan sát: image/42c6c5c4.png

Cung cấp một hình ảnh có tên image/81681aac.png. Mô tả là: một chú chó pug ngồi trong một cánh đồng cỏ. Hiểu hình ảnh bằng cách sử dụng công cụ.
Đã nhận.
image/81681aac.png Thay thế con chó bằng con mèo.
Suy nghĩ: Tôi có cần sử dụng công cụ không? Có
Hành động: Thay thế Cái gì đó Từ Ảnh
Đầu vào Hành động: image/81681aac.png, chó, mèo
Gọi API: Chỉnh sửa Hình ảnh
Quan sát: image/72899fef.png

Hình 12 Ví dụ Nâng cao với ImageBind-LLM để Điều khiển API.

Hướng dẫn: Mô tả hình ảnh này một cách chi tiết.
Trả lời: Hình ảnh có một ngọn hải đăng đứng cao trên một ngọn đồi đá, nhìn ra đại dương. Ngọn hải đăng màu trắng và có đỉnh đỏ, làm cho nó trở thành một địa danh nổi bật. Bầu trời ở phía sau nhiều mây, tạo thêm hiệu ứng kịch tính cho cảnh. Có một số người rải rác xung quanh khu vực, có thể đang tận hưởng tầm nhìn ra ngọn hải đăng và đại dương. Một số trong họ gần ngọn hải đăng hơn, trong khi những người khác ở xa hơn, tạo ra cảm giác về chiều sâu trong cảnh.

Hướng dẫn: Mô tả hình ảnh này một cách chi tiết.
Trả lời: Hình ảnh có một vùng nước lớn với màu xanh đẹp, nơi có một số thuyền nhỏ đang nổi trên mặt nước. Có ít nhất 11 chiếc thuyền nhìn thấy được trong cảnh, rải rác khắp mặt nước. Một số thuyền gần phía trước hơn, trong khi những chiếc khác ở xa hơn, tạo ra cảm giác về chiều sâu trong cảnh. Các thuyền có nhiều kích thước và hình dạng khác nhau, tăng thêm sự đa dạng của cảnh. Nước có vẻ bình lặng và thanh bình, làm cho đây là một khung cảnh lý tưởng cho một ngày chèo thuyền và thư giãn.

Hướng dẫn: Mô tả cảnh này một cách ngắn gọn.
Hình ảnh*0.5+Âm thanh*0.5: (Thiếu từ khóa âm thanh "chó") Cảnh có bầu trời xanh với những đám mây trắng, mịn màng, tạo ra một phông nền đẹp cho hình ảnh. Bầu trời đầy những đám mây nhỏ, trắng được rải rác khắp bầu trời, mang lại vẻ ngoài thanh bình và yên tĩnh.

Hình ảnh*0.4+Âm thanh*0.6: (Bao gồm cả "bầu trời" và "chó") Cảnh có một con chó đứng trên vỉa hè, nhìn lên bầu trời. Con chó có vẻ đang nhìn bóng của một người, được đổ trên mặt đất. Con chó đang đeo cổ áo, và vỉa hè nằm gần một hàng rào.

Hình ảnh*0.2+Âm thanh*0.8: (Thiếu từ khóa hình ảnh "bầu trời") Cảnh có một con chó đứng trên vỉa hè, nhìn một người đang dắt một con mèo bằng dây xích. Con chó có vẻ tò mò về con mèo, vì nó đang nhìn chằm chằm vào con mèo và người dắt nó.

(chó sủa)Ảo giác
Kết hợp Phương thức dễ vỡ

Hình 13 Trường hợp Thất bại.
19

--- TRANG 19 ---
[9] Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ (2023)

[10] Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023)

[11] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., Lample, G.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)

[12] Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: ImageBind: One embedding space to bind them all. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180–15190 (2023)

[13] Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al.: Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615 (2023)

[14] Zhang, R., Fang, R., Gao, P., Zhang, W., Li, K., Dai, J., Qiao, Y., Li, H.: Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930 (2021)

[15] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021)

[16] Xie, E., Yao, L., Shi, H., Liu, Z., Zhou, D., Liu, Z., Li, J., Li, Z.: Difffit: Unlocking transferability of large diffusion models via simple parameter-efficient fine-tuning. arXiv preprint arXiv:2304.06648 (2023)

[17] Zaken, E.B., Ravfogel, S., Goldberg, Y.: Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021)

[18] Frankle, J., Schwab, D.J., Morcos, A.S.: Training batchnorm and only batchnorm: On the expressive power of random features in cnns. arXiv preprint arXiv:2003.00152 (2020)

[19] Giannou, A., Rajput, S., Papailiopoulos, D.: The expressive power of tuning only the norm layers. arXiv preprint arXiv:2302.07937 (2023)

[20] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for science question answering. In: The 36th Conference on Neural Information Processing Systems (NeurIPS) (2022)

[21] OpenAI: GPT-4 Technical Report (2023)

[22] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual Instruction Tuning. arXiv:2304.08485 (2023)

[23] Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)

[24] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023)

[25] Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al.: Videollm: Modeling video sequence with large language models. arXiv preprint arXiv:2305.13292 (2023)

[26] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858 (2023)

[27] Radford, A., Kim, J.W., Hallacy, C., Ramesh,
20

--- TRANG 20 ---
A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning, pp. 8748–8763 (2021). PMLR

[28] Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning with noisy text supervision. In: International Conference on Machine Learning, pp. 4904–4916 (2021). PMLR

[29] Yuan, L., Chen, D., Chen, Y.-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.: Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 (2021)

[30] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716–23736 (2022)

[31] Zhang, W., Shi, H., Guo, J., Zhang, S., Cai, Q., Li, J., Luo, S., Zhuang, Y.: Magic: Multi-modal relational graph adversarial inference for diverse and unpaired text-based image captioning. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 3335–3343 (2022)

[32] Guzhov, A., Raue, F., Hees, J., Dengel, A.: Audioclip: Extending clip to image, text and audio. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 976–980 (2022). IEEE

[33] Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P., Li, H.: Pointclip: Point cloud understanding by clip. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8552–8562 (2022)

[34] Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355 (2023)

[35] Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., Lewis, M.: Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172 (2019)

[36] Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al.: Matching networks for one shot learning. Advances in neural information processing systems 29(2016)

[37] Zhang, R., Wang, L., Wang, Y., Gao, P., Li, H., Shi, J.: Parameter is not all you need: Starting from non-parametric networks for 3d point cloud analysis. CVPR 2023 (2023)

[38] Udandarao, V., Gupta, A., Albanie, S.: Sus-x: Training-free name-only transfer of vision-language models. arXiv preprint arXiv:2211.16198 (2022)

[39] Zhu, X., Zhang, R., He, B., Zhou, A., Wang, D., Zhao, B., Gao, P.: Not all features matter: Enhancing few-shot clip with adaptive prior refinement. arXiv preprint arXiv:2304.01195 (2023)

[40] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35, 25278–25294 (2022)

[41] Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556–2565 (2018)

[42] Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558–3568
21

--- TRANG 21 ---
(2021)

[43] Zhang, B., Sennrich, R.: Root mean square layer normalization. Advances in Neural Information Processing Systems 32(2019)

[44] Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016)

[45] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)

[46] https://sharegpt.com/

[47] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695 (2022)

[48] Huang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M., Ye, Z., Liu, J., Yin, X., Zhao, Z.: Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. arXiv preprint arXiv:2301.12661 (2023)

[49] Sanghi, A., Chu, H., Lambourne, J.G., Wang, Y., Cheng, C.-Y., Fumero, M., Malekshan, K.R.: Clip-forge: Towards zero-shot text-to-shape generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18603–18613 (2022)

[50] Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)

[51] Bird, S., Klein, E., Loper, E.: Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. "O'Reilly Media, Inc.", ??? (2009)

[52] Yang, R., Song, L., Li, Y., Zhao, S., Ge, Y., Li, X., Shan, Y.: Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752 (2023)

[53] Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015)

[54] Ordonez, V., Kulkarni, G., Berg, T.L.: Im2text: Describing images using 1 million captioned photographs. In: Neural Information Processing Systems (NIPS) (2011)

[55] Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: COYO-700M: Image-Text Pair Dataset. https://github.com/kakaobrain/coyo-dataset (2022)

[56] Zhu, W., Hessel, J., Awadalla, A., Gadre, S.Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W.Y., Choi, Y.: Multimodal C4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939 (2023)

[57] Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: ICML (2022)

[58] Johnson, J., Douze, M., Jégou, H.: Billion-scale similarity search with GPUs. IEEE Transactions on Big Data 7(3), 535–547 (2019)

[59] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Qiu, Z., Lin, W., Yang, J., Zheng, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)

[60] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500 (2023)
22

--- TRANG 22 ---
[61] Mishra, A., Alahari, K., Jawahar, C.V.: Top-down and bottom-up cues for scene text recognition. In: 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2687–2694 (2012). https://doi.org/10.1109/CVPR.2012.6247990

[62] Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., Bigorda, L.G.i., Mestre, S.R., Mas, J., Mota, D.F., Almazán, J.A., Heras, L.P.: Icdar 2013 robust reading competition. In: 2013 12th International Conference on Document Analysis and Recognition, pp. 1484–1493 (2013). https://doi.org/10.1109/ICDAR.2013.221

[63] Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura, M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., Shafait, F., Uchida, S., Valveny, E.: Icdar 2015 competition on robust reading. In: 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pp. 1156–1160 (2015). https://doi.org/10.1109/ICDAR.2015.7333942

[64] Ch'ng, C.K., Chan, C.S.: Total-text: A comprehensive dataset for scene text detection and recognition. In: 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), vol. 01, pp. 935–942 (2017). https://doi.org/10.1109/ICDAR.2017.157

[65] Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary text detection system for natural scene images. Expert Systems with Applications 41(18), 8027–8048 (2014) https://doi.org/10.1016/j.eswa.2014.07.008

[66] Shi, C., Wang, C., Xiao, B., Gao, S., Hu, J.: End-to-end scene text recognition using tree-structured models. Pattern Recognition 47(9), 2853–2866 (2014) https://doi.org/10.1016/j.patcog.2014.03.023

[67] Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with perspective distortion in natural scenes. In: 2013 IEEE International Conference on Computer Vision, pp. 569–576 (2013). https://doi.org/10.1109/ICCV.2013.76

[68] Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.J.: Coco-text: Dataset and benchmark for text detection and recognition in natural images. ArXiv abs/1601.07140 (2016)

[69] Xie, X., Fu, L., Zhang, Z., Wang, Z., Bai, X.: Toward understanding wordart: Corner-guided transformer for scene text recognition (2022)

[70] Liu, Y., Jin, L., Zhang, S., Luo, C., Zhang, S.: Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recogn. 90(C), 337–345 (2019) https://doi.org/10.1016/j.patcog.2019.02.002

[71] Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: A new scene text recognizer with visual language modeling network. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 14194–14203 (2021)

[72] Huang, Z., Chen, K., He, J., Bai, X., Karatzas, D., Lu, S., Jawahar, C.V.: Icdar2019 competition on scanned receipt ocr and information extraction. In: 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 1516–1520 (2019). https://doi.org/10.1109/ICDAR.2019.00244

[73] Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document images. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2200–2209 (2021)

[74] Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., Rohrbach, M.: Towards vqa models that can read. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8309–8318 (2019). https://doi.org/10.1109/CVPR.2019.00851

[75] Furkan Biten, A., Tito, R., Mafla, A., Gomez, L., Rusiñol, M., Mathew, M., Jawahar, C.V., Valveny, E., Karatzas, D.: Icdar
23

--- TRANG 23 ---
2019 competition on scene text visual question answering. In: 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 1563–1570 (2019). https://doi.org/10.1109/ICDAR.2019.00251

[76] Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text in images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 947–952 (2019). https://doi.org/10.1109/ICDAR.2019.00156

[77] Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question answering benchmark requiring external knowledge. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3190–3199 (2019). https://doi.org/10.1109/CVPR.2019.00331

[78] Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and compositional question answering. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6693–6702 (2019). https://doi.org/10.1109/CVPR.2019.00686

[79] Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., Zhu, S.-C.: Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In: The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks (2021)

[80] Liu, F., Emerson, G.E.T., Collier, N.: Visual spatial reasoning. Transactions of the Association for Computational Linguistics (2023)

[81] Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M.F., Parikh, D., Batra, D.: Visual Dialog. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)

[82] Bigham, J.P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R.C., Miller, R., Tatarowicz, A., White, B., White, S., et al.: Vizwiz: nearly real-time answers to visual questions. In: Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology, pp. 333–342 (2010)

[83] Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe, K., Bitton, Y., Gadre, S., Jitsev, J., Kornblith, S., Koh, P.W., Ilharco, G., Wortsman, M., Schmidt, L.: OpenFlamingo. Zenodo (2023). https://doi.org/10.5281/zenodo.7733589 . https://doi.org/10.5281/zenodo.7733589
24
