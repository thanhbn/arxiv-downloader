# 2210.00077.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2210.00077.pdf
# File size: 373920 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
E-BRANCHFORMER: BRANCHFORMER WITH ENHANCED MERGING
FOR SPEECH RECOGNITION
Kwangyoun Kim1, Felix Wu1, Yifan Peng2, Jing Pan1, Prashant Sridhar1, Kyu J. Han1, Shinji Watanabe2
1ASAPP Inc., Mountain View, CA, USA
2Carnegie Mellon University, Pittsburgh, PA, USA
ABSTRACT
Conformer, combining convolution and self-attention sequentially to
capture both local and global information, has shown remarkable
performance and is currently regarded as the state-of-the-art for auto-
matic speech recognition (ASR). Several other studies have explored
integrating convolution and self-attention but they have not man-
aged to match Conformer‚Äôs performance. The recently introduced
Branchformer achieves comparable performance to Conformer by
using dedicated branches of convolution and self-attention and merg-
ing local and global context from each branch. In this paper, we
propose E-Branchformer, which enhances Branchformer by apply-
ing an effective merging method and stacking additional point-wise
modules. E-Branchformer sets new state-of-the-art word error rates
(WERs) 1.81% and 3.65% on LibriSpeech test-clean and test-other
sets without using any external training data.
Index Terms ‚ÄîAutomatic speech recognition, Conformer,
Branchformer, Librispeech
Copyright 2023 IEEE. Published in the 2022 IEEE Spoken Language Technology Workshop (SLT) (SLT 2022), scheduled for 19-22 January 2023 in Doha, Qatar. Personal use of this material is permitted. However, permission to reprint/republish this
material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager,
Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.1. INTRODUCTION
Automatic speech recognition (ASR) is a speech-to-text task, critical
to enable spoken language understanding [1, 2, 3]. Recently, there
has been a growing interest in end-to-end (E2E) ASR models for
their simplicity, efÔ¨Åciency, and competitive performance. Thanks
to many improvements in E2E ASR modeling such as new model
architectures [4, 5, 6], training objectives [4, 5, 7, 8], and data aug-
mentation methods [9, 10, 11], E2E ASR continues to take over con-
ventional hybrid ASR in various voice recognition applications.
An acoustic encoder that extracts features from audio inputs
plays a vital role in all E2E ASR models regardless of which
training objective is applied to optimization. Recurrent neural
networks [4, 12, 13, 14] used to be the de facto model choice
for an encoder. Later, many convolution-based [15, 16, 17] and
Transformer-based [6, 18, 19, 20] models have also been intro-
duced. The strength of multi-head self-attention in capturing global
context has allowed the Transformer to show competitive perfor-
mance. Transformers and their variants have been further studied to
reduce computation cost [21, 22, 23, 24, 25] or to train deep models
stably [26]. In parallel, several studies have investigated how to
merge the strengths of multi-head self-attention with those of other
neural units. The combination of a recurrent unit with self-attention,
which does not require explicit positional embeddings, shows stable
performance even in long-form speech recognition [27]. Besides,
Conformer [28] which combines convolution and self-attention se-
quentially, and Branchformer [29] which does the combination in
Work done during an internship at ASAPP.parallel branches, both exhibit superior performance to the Trans-
former by processing local and global context together.
In this work, we extend the study of combining local and
global information and propose E-Branchformer , a descendent of
Branchformer which enhances the merging mechanism. Our en-
hanced merging block utilizes one additional lightweight operation,
a depth-wise convolution, which reinforces local aggregation. Our
experiments show that E-Branchformer surpasses Conformer and
Branchformer on both test-clean and test-other in LibriSpeech. Our
speciÔ¨Åc contribution includes:
1. The Ô¨Årst Conformer baseline implementation using the
attention-based encoder-decoder model that matches the
accuracy of Google‚Äôs Conformer [28].
2. An improved Branchformer baseline that has a 0.2% and
0.5% improvement in absolute WER.
3. An extensive study on various ways to merge local and global
branches.
4. The new E-Branchformer encoder architecture which sets a
new state-of-the-art performance (1.81% and 3.65% WER on
test-clean and test-other) on LibriSpeech under the constraint
of not using any external data.
5. Our code will be released at https://anonymized url for repro-
ducibility.
2. RELATED WORK
2.1. End-to-end Automatic Speech Recognition Models
E2E ASR models can be roughly categorized into three main types
based on their training objectives and decoding algorithms: connec-
tionist temporal classiÔ¨Åcation models [7], transducer models [5], and
the attention-based encoder-decoder (AED) models, a.k.a. Listen,
Attend, and Spell (LAS) models [4].
Regardless of the training objective and high-level framework,
almost all E2E ASR models share a similar backbone; an acoustic
encoder which encodes audio features into a sequence of hidden fea-
ture vectors. Many studies have explored different modeling choices
for the acoustic encoder. Recurrent neural networks [4, 5, 12, 13, 30]
encode audio signals sequentially; convolution alternatives [15, 16,
17] process local information in parallel and aggregate them as the
network goes deeper; self-attention [6, 18, 19, 20] allows long-range
interactions and achieves superior performance. In this work, we fo-
cus only on applying the E-Branchformer encoder under the AED
framework, but it can also be applied to CTC or transducer models
like the Conformer.arXiv:2210.00077v2  [eess.AS]  14 Oct 2022

--- PAGE 2 ---
2.2. Combining Self-attention with Convolution
Taking the advantages of self-attention and convolution to capture
both long-range and local patterns has been studied in various prior
works. They can be categorized into two regimes: applying them
sequentially or in parallel (i.e., in a multi-branch manner).
2.2.1. Sequentially
To the best of our knowledge, QANet [31] for question answering is
the Ô¨Årst model that combines convolution and self-attention in a se-
quential order. QANet adds two additional convolution blocks (with
residual connections) before the self-attention block in each Trans-
former layer. Evolved Transformer [32] also combines self-attention
and convolution in a sequential manner. In computer vision, non-
local neural networks [33] also show that adding self-attention layer
after convolution layers enables the model to capture more global
information and improves the performance on various vision tasks.
Recently, a series of vision Transformer variants that apply convo-
lution and self-attention sequentially are also proposed including
CvT [34], CoAtNet [35], ViTAEv2 [36], MaxVit [37]. In speech,
Gulati et al. [28] introduce Conformer models for ASR and show that
adding a convolution block after the self-attention block achieves the
best performance compared to applying it before or in parallel with
the self-attention.
2.2.2. In parallel
Wu et al. [38] propose Lite Transformer using Long-Short Range
Attention (LSRA) which applies multi-head attention and dynamic
convolution [39] in parallel and concatenates their outputs. Lite
Transformer is more efÔ¨Åcient and accurate than Transformer base-
lines on various machine translation and summarization tasks. Jiang
et al. [40] extend this to large scale language model pretraining
and introduce ConvBERT which combines multi-head attention and
their newly proposed span-based dynamic convolution with shared
queries and values in two branches. Experiments show that Con-
vBERT outperforms attention-only baselines (ELECTRA [41] and
BERT [42]) on a variety of natural language processing tasks. In
computer vision, Pan et al. [43] share a similar concept except that
they decompose a convolution into a point-wise projection and a
shift operation. In speech, Branchformer combines self-attention
and the convolutional spatial gating unit (CSGU) [44] achieving
performance comparable with the Conformer. We will introduce
more details in Section 3.
2.2.3. Hybrid - both Sequentially and in parallel
Recently, Inception Transformer [45] which has three branches (av-
erage pooling, convolution, and self-attention) fused with a depth-
wise convolution achieves impressive performance on several vision
tasks. Our E-Branchformer shares a similar spirit of combing local
and global information both sequentially and in parallel.
3. PRELIMINARY: BRANCHFORMER
Figure 1 shows the high-level architecture of the Branchformer en-
coder, which uses a frontend and a convolutional subsampling layer
1Unlike the Lite-Transformer and the in-parallel method in the Con-
former, the Branchformer doesn‚Äôt split the inputs for each branch along the
channel dimension.
Fig. 1 : A Ô¨Ågure of the Branchformer-based Encoder and a single
Branchformer Block. There are two parallel branches1to extract
global and local context, and the merge module combines outputs
of branches.
to extract low-level speech features and then applies several Branch-
former blocks. There are three components in each Branchformer
block ‚Äî a global extractor branch, a local extractor branch, and a
merge module.
Theglobal extractor branch is a conventional self-attention
block in Transformer. It uses the pre-norm [46] setup where a
layer norm (LN) [47], a multi-head self-attention (MHSA), and a
dropout [48] are applied sequentially as follows:
YG= Dropout(MHSA(LN( X))); (1)
where X;YG2RTddenote the input and the global-extractor
branch output with a length of Tand a hidden dimension of d. Like
the Conformer, Branchformer uses relative positional embeddings,
which generally shows better performance than absolute positional
embeddings in ASR and NLU tasks [28, 49]. In the paper, the au-
thors also explore more efÔ¨Åcient attention variants to reduce compu-
tational cost, which leads to some degradation in accuracy.
Fig. 2 : A Ô¨Ågure of the Local extractor branch in Branchformer. This
branch uses the Multi-Layer Perceptron (MLP) with convolutional
gating (cgMLP) [44].

--- PAGE 3 ---
Figure 2 illustrates the local extractor branch in Branchformer.
After LayerNorm, the input features are Ô¨Årst projected to a higher
dimensiondinter (usuallydinter= 6d) followed by a GELU activa-
tion [50]. Then, a Convolutional Spatial Gating Unit (CSGU) [44] is
applied to encode local information. The CSGU block Ô¨Årst splits the
features into two parts (each with dinter=2dimensions), then applies
LayerNorm and depth-wise convolution on one of them, and Ô¨Ånally
multiplies two branches element-wisely. The output of CSGU is pro-
jected back to ddimensions followed by dropout. Precisely, the local
extractor branch processes input Xand generates YLas follows:
Z= GELU(LN( X)U); (2)
[A B ] =Z; (3)
~Z= CSGU( Z) =ADwConv(LN( B)); (4)
YL= Dropout( ~ZV); (5)
where Z2RTdinter;A;B;~Z2RTdinter=2are intermediate hid-
den features,is element-wise product, and U2Rddinter;V2
Rdinter=2ddenote the trainable weights of two linear projections.
The local extractor branch outputs YL2RTd.
In the merge module , Branchformer simply concatenates two
outputs YGandYLand then applies a linear projection to reduce
the dimensionality back to d:
YMerge= Concat( YG;YL)W; (6)
whereW2R2ddis the trainable weights of the linear projection.
Alternatively, a weighted average method can be applied, which
uses a scale for each branch and sum them up.
YMerge=wgYG+wlYL; (7)
wherewg;wl2Rare the weights for the global and the local
branch, respectively. The attention-based pooling can be applied to
compute the weights, wg;wl. It is also possible to arbitrarily adjust
the weight of each branch. A branch‚Äôs weight can also be set to 0 to
exclude the corresponding branch to reduce the amount of computa-
tion or for other purposes, which is called a branch drop. In addition,
the weighted average method can be used to share a more reliable
and interpretable analysis, such as the importance of each branch at
a speciÔ¨Åc layer, by comparing and plotting the weight values. But,
in terms of the accuracy, experiments show that the concatenation
method is simpler but more accurate.
4. E-BRANCHFORMER
4.1. Enhanced Merge Module
We argue that it is suboptimal to combine the outputs of the two
branches in the Branchformer point-wise and linearly. In this work,
we study several potential modiÔ¨Åcations of the merging module that
take temporal information into account. With such merge modules,
self-attention and convolution would be combined sequentially and
in parallel. Figure 3 depicts the original merge module and Ô¨Åve en-
hanced versions.
4.1.1. Depth-wise Convolution
We introduce depth-wise convolution to the merge module allowing
it to take adjacent features into account when combing information
from two branches. Notably, a depth-wise convolution requires little
computation and has a negligible effect on the speed of the model.The original merge module in Branchformer concatenates outputs
from the global and the local extractor and performs a linear pro-
jection along the feature dimension. In other words, the globalized
and the localized context are computed in parallel and fused only
across channels (as shown in Figure 3b). Presumably, using nearby
information can improve the merge process. Similar to Inception-
Transformer [45], we employ a depth-wise convolution to add the
spatial information exchanging (as described in Figure 3c). For-
mally, the outputs from the global YGand the local YLbranch are
merged:
YC= Concat( YG;YL); (8)
YD= DwConv( YC); (9)
YMerge= (YC+YD)W; (10)
where DwConv stands for the depth-wise convolution and W2
R2ddis the trainable weights of the linear projection.
Originally, the Inception-Transformer uses pooling and upsam-
pling to reduce the amount of computation for the multi-head self-
attention, which has quadratic time complexity with respect to the
input sequence length. However, those pooling and upsampling op-
erations make spatial information excessively smooth. Hence, when
they combine the outputs of branches they utilize the depth-wise
convolution to reÔ¨Åne them. Despite not having such a smoothing
issue, we discover that adding depth-wise convolution to spatially
reÔ¨Åne the extracted information from two branches enhances the
merge process. Beside enlarging the kernel size, we also explore
ways to obtain information from more diverse perspectives through
an additional convolution branch with different kernel size in paral-
lel [51, 52] (as shown in Figure 3d).
4.1.2. Squeeze-and-Excitation
We examine a Squeeze-and-Excitation (SE) block [17, 53] to utilize
global information during the merge process. An SE block takes a
global average pooling over the temporal dimension, and feeds it to
a tiny two-layer Feed-Forward Network (FFN) to produce a channel-
wise gate. Precisely,
yD=1
TTX
t=1YDt; (11)
g=(MLP( yD)) =(Swish( yDW1))W2); (12)
Y0
Di=giYDi8i2f1;:::;dg; (13)
YMerge= (YC+Y0
D)W; (14)
whereYDis of the output of depth-wise convolution in Eq. 9, Swish
andare Swish [54] and sigmoid non-linearity, respectively, de-
notes channel-wise multiplication, and W12Rdd=8andW22
Rd=8dare the trainable weights of the two-layer MLP. We omit the
bias terms in all linearly transformation for simplicity. The outputs
of depth-wise convolution YDare now gated by the global informa-
tion to produce the merge output YMerge.
4.2. Revisiting the Point-wise Feed-Forward Network
Since Branchformer has two linear projection layers in the cgMLP-
base local extractor branch, for computational efÔ¨Åciency, it does not
have FFN blocks like Transformer and Conformer. However, the
role of the linear projections in the cgMLP and the FFNs could be
different ‚Äî the FFNs in Transformer are used to reÔ¨Åne pointwise
information after temporal features are aggregated.

--- PAGE 4 ---
(a)
 (b)
 (c)
 (d)
 (e)
 (f)
Fig. 3 : A Ô¨Ågure of (a) the proposed E-Branchformer block and different methods for the merge module: (b) is used in Branchformer, (c)is
applying a depth-wise convolution, and (d) uses multiple convolutions with different kernel size, e.g., 31 and 3. (e) employs a squeeze-and-
excitation block, and (f) replaces a depth-wise convolution with the convolution module in Conformer [28].
Transformer and its variants commonly stack multi-head self-
attention and FFN modules in an interleaving pattern. According to
a study [55], models trained by stacking in random order sometimes
outperform the interleaving pattern. They also conduct experiments
to intentionally stack a speciÔ¨Åc module on the bottom or top in or-
der to achieve better performance. In other words, the multi-head
attention and the FFNs may not necessarily be stacked in interleav-
ing patterns. However, they also say that using a balanced number
of multi-head self-attention and FFN modules is not necessary, but
it is generally desirable. In our case, it is possible that stacking E-
Branchformer with FFN modules may perform better than stacking
only E-Branchformer. In this direction, when building the Encoder,
we revisit the FFNs or the macaron-style FFNs with 4dintermedi-
ate dimension and stack them together with E-Branchformer in an
interleaving pattern to increase the expected model capacity.
5. EXPERIMENTS
5.1. Experimental Setups
We conduct experiments mainly on the LibriSpeech dataset [56],
which contains approximately 1000 hours of transcribed speech and
an additional text-only corpus mainly used for the external language
model.
ESPnet [57] is used for all experiments. ESPnet, an open source
toolkit, has been widely used in many publications, and most paper
experimental results are released as recipes or downloadable models.
We utilize this to perform more accurate comparisons with existing
methods.
We employ the attention-based encoder-decoder model (AED)
as our base structure for all experiments. We use 80 dimensional log
Mel features as input extracted with a 32ms window size and a 10ms
stride, and 5K BPE sub-word units as output tokens. At the bottom
of the encoder is a convolutional subsampling module consisting of
two 2D convolution layers, a ReLU and a linear layer. Each convolu-
tion layer uses a (3x3) kernel and a (2x2) stride effectively subsam-
pling by a factor of 4, and the channel size of each convolution layer
is the same as the hidden dimension of the encoder. Our proposed E-
Branchformer blocks are stacked on top of the subsampling module.
We consider two main model sizes, Base and Large, and parameters
are adjusted according to the experiment. The default Base encoder
consists of 16 layers. For each layer, we use a feature hidden dimen-siondof 256. The default Large encoder has 17 layers with a feature
dimension of 512. The number of heads in the self-attention is d=64
and the intermediate hidden dimension of the cgMLP and the FFN
are6dand4d, respectively. When applying the macaron-style FFN,
we reduce the number of layers or narrow down the intermediate
hidden dimension of the FFN as 2dto Ô¨Åt the model size. For the
encoder, we use a dropout rate and a layer dropout rate of 0.1. We
use the swish activation in FFNs and convolution modules, and the
kernel size of the convolution modules is 31. For all experiments, we
employ a 6-layer Transformer decoder with the same feature hidden
dimensiondas in the encoder. The dropout rate is adjusted to 0.2.
5.2. Training
We use 8 NVIDIA A100 GPUs for model training, and each training
is Ô¨Ånished within 3 days for large models. During training, we apply
SpecAug [10] using 2 frequency-masks with parameter Fof 27, and
10 time-masks with parameters pof 0.05. In addition, we augment
our dataset by Speed Perturbation with scaling factors of 0.9, 1.0,
and 1.1. We employ a joint CTC-attention loss function with a CTC
loss weight of 0.3. A label smoothing weight of 0.1 and the Adam
optimizer with a weight decay of 10 6are also utilized. We apply
the learning rate scheduling scheme named warmuplr already im-
plemented in ESPnet, which has a warm-up and a decay stage. We
use 40K steps for the warm-up stage, and the model is trained for
80 epochs which are about 220K steps. We select the Ô¨Ånal model
by averaging 10 checkpoints with the highest validation accuracy.
Since we train and test models using online-published recipes, most
base experimental results are easily reproducible. We also use the
automatic mixed-precision training implemented in PyTorch.
5.3. Inference
We employ joint CTC-attention decoding with tuned weight. In ad-
dition, we use an external language model (LM) for shallow fusion.
It is a Transformer-based model with 16 layers, 128 embedding di-
mensions, 512 attention dimensions and 8 attention heads - in total
53.71M parameters. The recipe along with the pre-trained language
model can be found in the ESPnet toolkit2.
2https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1

--- PAGE 5 ---
To further improve the language model shallow fusion and
boost accuracy, we apply the Internal Language Model Estima-
tion (IMLE) method proposed by Meng et al. [58]. It is known that
E2E ASR models implicitly learn an internal language model (ILM)
over the training text distribution. Estimating the internal language
model distribution allows us to explicitly interpolate internal LM
and external LM with tunable hyperparameters, resulting in more
effective decoding. At each decode step, the ILME follows the same
AED decode step except that the source attention is disabled by an
attention mask ‚Äî effectively the decoder estimates the probability
over tokens solely based on the text. For each hypothesis, the score
is given by
log(P(Y)) = log(P(YjX;AED))
 ilmlog(P(Y;AED)) +elmlog(P(Y;LM));
whereilmandelmare the interpolation weights for the internal
and external langauge model, respectively. P(YjX;AED)is the
probability of the hypothesis Yyielded by the ASR model given the
input acoustic feature X.P(Y;AED)andP(Y;LM)represent the
internal language model estimation and the external language model
probability over the hypothesis. In our experiment, we set ilmand
elmto 0.2 and 0.6, respectively.
6. RESULT
6.1. Main result
Table 1 : Comparing word error rates (WER) with other published
models. The external LM used in [27, 29, 59] is downloadable from
ESPnet, and it is the same as what we used in all our experiments.
ModelParams Enc.Without LM With LM
test test test test
(M) (M) clean other clean other
Transducer
Transformer [6] 139 - 2.4 5.6 2.0 4.6
ContextNet [17] 112.7 - 2.1 4.6 1.9 4.1
Conformer (M) [28] 30.7 27.3 2.3 5.0 2.0 4.3
Conformer (L) [28] 118.8 114.932.1 4.3 1.9 3.9
AED
Transformer [60] 270 - 2.9 7.0 2.3 5.2
Conformer [59] 116.2 83.2 - - 2.1 4.9
SRU++ [27] 114.6 - - - 2.0 4.7
Branchformer [29] 116.2 83.3 2.4 5.5 2.1 4.5
Our baselines (AED)
Conformer 147.8 114.9 2.16 4.74 1.84 3.95
Branchformer 146.7 113.8 2.25 4.83 1.93 4.00
Our works (AED)
E-Branchformer (B) 41.12 27.8 2.49 5.61 1.97 4.26
E-Branchformer (L) 148.9 116.0 2.14 4.55 1.85 3.71
+ ILME - - - - 1.81 3.65
We compare the word error rates (WER) of our proposed E-
Branchformer with those of previous studies. Models are evaluated
on test-clean and test-other, with or without an external LM. First,
we note that the Transducer-based models in Table 1, especially
Conformer-Transducer [28], known as state-of-the-art in terms of an
accuracy, have not published a model or open sourced their training
recipes and we have not been able to reproduce their results. In-
stead of reproducing them, we use only the WER and information
3This is the estimated size based on the information in the paper [28]presented in each paper. In order to compare relative performance
with Conformer, we perform experiments on AED models and by
changing the Encoder structure.
As shown in Table 1, Branchformer [29] performs 0.4% better
than Conformer [59] on test-other with a LM. However, it is still
worse than Conformer-Transducer by 0.2% and 0.6% on test-clean
and test-other, respectively. As mentioned earlier, it is difÔ¨Åcult to
directly compare the two studies because the base code, the struc-
ture of the decoder, and the external LM are all different. However,
from comparison of studies, we Ô¨Ånd that the encoder of Conformer-
Transducer has more parameters than the encoder of Branchformer-
AED or Conformer-AED.
In order to fairly compare AED models, we conÔ¨Ågure the
encoder of AED-based models following the encoder of Conformer-
Transducer. In the case of the Conformer-AED model, we use the
conÔ¨Åguration as described in [28], and in the case of Branchformer,
we stack more blocks in order to achieve a similar encoder size.
The overall model size varies due to the decoder‚Äôs structure, but
encoder size is similar to that of the Conformer-Transducer. As
a result, we obtain signiÔ¨Åcantly improved performance compared
to the existing AED-based models. By decoding Branchformer
with shallow fusion, we achieve a WER of 1.93% on test-clean
and 4.0% on test-other. Similarly, we obtain 1.84% and 3.95% by
using Conformer-AED, which is very similar to the accuracy of the
existing Conformer-Transducer. We use these as new baselines.
Then, we train the proposed E-Branchformer model and obtain
performance better than that of existing studies and our baselines.
In particular, with the E-Branchformer (L) model, which exploits
depth-wise convolution with a kernel size of 31 in the merge mod-
ule as illustrated in Figure 3c and the narrowed macaron-style FFN,
with the external LM, we obtain WER of 1.85% and 3.71% on test-
clean and test-other, respectively. This is better than the 1.9% and
3.9% of the existing state-of-the-art in WER. Compared to Branch-
former [29] which has much larger size, our E-Branchformer (B),
which uses the depth-wise convolution in the merge module and a
basic FFN, shows similar performance without the LM, and outper-
forms on both test-clean and test-other by 0.1% and 0.2%, respec-
tively, with the LM.
Subsequently, we employ Internal Language Model Estimation
(ILME) to further enhance the effect of the shallow fusion. This Ô¨Å-
nally leads to the remarkable result of 1.81% and 3.65% on test-clean
and test-other respectively, which is a new state-of-the-art when no
external data is used4as far as we know. Some notable and valu-
able model results through combinations of different methods are
analyzed in Section 6.2.
6.2. Ablation Study
6.2.1. FFN module
Table 2 shows the performance changes through cooperating with
FFN or macaron-style FFN. In this experiment, we use the same
structure of the Decoder and the same dimension of the encoder
in all cases. We also adjust the number of stacked blocks ac-
cordingly to compare under similar encoder sizes. 17-layers of
Branchformer with FFN performs better than the stack of 25-layers
of Branchformer-only. In addition, we obtain better performance
when we stack 13-layers of Branchformer with macaron-style FFN.
To construct more deeper encoder, we narrow down the intermediate
dimension for FFNs from 4dto2dand stack 17-layers Branchformer
4W2v-BERT [61] achieves 1.4% and 2.5% WER using additional 60K
hours of unlabelled data from LibriLight [62].

--- PAGE 6 ---
Table 2 : Comparison of word error rates (WER) and the compu-
tational complexity (MACs) depending on whether FFN, macaron-
style FFN (FFN-mac), or macaron-style FFN with the narrow inter-
mediate dimension (FFN-mac-n) is applied or not. We use the Large
size model without any change in the original Branchformer block.
No LM is used for evaluations.
Model# of Enc. MACs dev dev test test
layers Params (M) (G) clean other clean other
BranchFormer 25 113.8 43.7 2.01 4.99 2.25 4.83
+ FFN 17 115.4 42.6 1.98 4.83 2.18 4.87
+ FFN-mac 13 117.3 42.3 1.97 4.74 2.26 4.74
+ FFN-mac-n 17 115.5 42.6 1.97 4.78 2.10 4.79
+ FFN-mac 17 151.1 51.5 2.00 4.75 2.15 4.47
without increasing the model size. And this model shows the best
performance on average among the models with similar model sizes.
However, since there is not much difference between those models
using FFNs, we employ these variants in the rest of experiments to
Ô¨Ånd the optimal combination with other modules.
Although the model sizes are similar, the amount of compu-
tations may vary depending on the structure of the model and the
number of layers. We measure the multiply-accumulate operations
(MACs) to compare the computational complexity of each models.
We utilize DeepSpeed5toolkit for proÔ¨Åling and use a randomly gen-
erated 10 seconds audio as a dummy input.
From these results, we can conÔ¨Årm that using FFN together is a
reasonable way to expect better accuracy than deeply stacking only
Branchformers.
6.2.2. Kernel sizes of the depth-wise convolution
In this study, we compare the effects of different kernel sizes in the
depth-wise convolution, stacked sequentially in the merge module.
We already know that Conformer has achieved the best performance
with the kernel size of 31 or 32 for the depth-wise convolution in the
sequentially stacked convolution module [28, 59]. However, we try
to verify through experiments whether additionally stacked depth-
wise convolution also needs the large-sized kernel, even though the
local branch of Branchformer [29] or our E-Branchformer is already
having a depth-wise convolution with the kernel size of 31. As
shown in Table 3, using 31 is the most accurate, especially on dev-
other and test-other by a large margin. We use this kernel size in
other experiments.
Table 3 : Comparison of word error rates (WER) according to differ-
ent kernel sizes of the depth-wise convolution in the merge module.
The Base size model with FFN is employed for this ablation study,
and we decode each models without an external LM.
Kernel size dev-clean dev-other test-clean test-other
3 2.33 5.99 2.64 5.97
15 2.40 5.89 2.69 5.87
31 2.25 5.68 2.49 5.61
63 2.40 5.81 2.53 5.78
5https://github.com/microsoft/DeepSpeed6.2.3. Merge module
Table 4 : Effect of different convolution-based methods added in the
merge module. (b)-(f) refers to each method in Figure 3. The Base
size model with FFN modules is used without an external LM.
MethodEnc. MACs dev dev test test
(M) (G) clean other clean other
w/o Depth-wise Conv (b) 27.5 10.8 2.34 5.99 2.57 5.79
w Depth-wise Conv (c) 27.8 10.8 2.25 5.68 2.49 5.61
+ Multiple Kernel (d) 36.2 12.9 2.34 5.87 2.58 5.90
+ SE block (e) 28.9 10.9 2.27 5.82 2.42 5.59
Conv Module (internal) (f) 40.4 14.0 2.66 6.40 2.84 6.35
Conv Module (external) 30.8 11.6 2.36 5.87 2.64 5.88
As shown in Table 4, we perform comparative experiment on
which convolution-based method is more effective to enhance the
merge module. We compare various methods illustrated in Figure 3.
First, when we exploit the depth-wise convolution as shown in Fig-
ure 3c, performance is evenly improved on all evaluation sets by
0.1% to 0.3%. Whereas adding another depth-wise convolution with
kernel size of 3, as described in Figure 3d, is harmful for all eval-
uation sets, and applying additional linear projection before each
depth-wise convolution to reduce the dimension shows even worse
result. A SE block in Figure 3e helps to obtain a marginal improve-
ment on test-sets, but not on dev-sets. We originally think that this
gating-based mechanism reinforces the depth-wise convolution out-
put by reÔ¨Çecting the global information, but we interpret this result
that the effect may be relatively reduced due to the presence of chan-
nels from the global branch. Although the total computations is al-
most the same, it is difÔ¨Åcult to say that the SE block plays a bene-
Ô¨Åcial role in this experiment. We also try to borrow the convolution
module from Conformer, which cooperates with the self-attention
module sequentially. But, when we internally replace the depth-wise
convolution with it, as depicted in Figure 3f, the performance is ex-
tremely degraded. The other way we use it is to stack outside of the
merge module before FFN externally. This method performs bet-
ter than when it is inside the merge module, but it still makes the
model perform poorly. The reason, why it does not provide addi-
tional improvement, can be assumed to be the redundancy caused by
the convolution module being more focused on extracting the local
context like what our cgMLP is already doing.
In conclusion, simply adding the depth-wise convolution is ef-
fective in terms of performance and also efÔ¨Åcient in terms of the
parameter size and the computational complexity.
7. CONCLUSION
We have proposed E-Branchformer, a new encoder architecture for
automatic speech recognition. With an enhanced merging mecha-
nism that allows hybrid application (both sequentially and in paral-
lel) of self-attention and convolution, E-Branchformer outperforms
both Conformer and Branchformer and sets a new-state-of-art on
LibriSpeech test sets without using any external data. We are inter-
ested in applying to other ASR models, such as Transducer, and ex-
ploring the potential of E-Branchformer on other speech tasks such
as self-supervised learning, speech identiÔ¨Åcation, speech enhance-
ment, and spoken language understanding tasks in the future.

--- PAGE 7 ---
8. REFERENCES
[1] Lingyun Feng, Jianwei Yu, Deng Cai, Songxiang Liu, Haitao
Zheng, and Yan Wang, ‚ÄúAsr-glue: A new multi-task bench-
mark for asr-robust natural language understanding,‚Äù arXiv
preprint arXiv:2108.13048 , 2021.
[2] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and
Verena Rieser, ‚ÄúSlurp: A spoken language understanding re-
source package,‚Äù arXiv preprint arXiv:2011.13205 , 2020.
[3] Suwon Shon, Ankita Pasad, Felix Wu, Pablo Brusco, Yoav
Artzi, Karen Livescu, and Kyu J Han, ‚ÄúSlue: New benchmark
tasks for spoken language understanding evaluation on natural
speech,‚Äù arXiv preprint arXiv:2111.10367 , 2021.
[4] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals,
‚ÄúListen, attend and spell,‚Äù arXiv preprint arXiv:1508.01211 ,
2015.
[5] A. Graves, ‚ÄúSequence transduction with recurrent neural net-
works,‚Äù arXiv preprint arXiv:1211.3711 , 2012.
[6] Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik
McDermott, Stephen Koo, and Shankar Kumar, ‚ÄúTransformer
transducer: A streamable speech recognition model with trans-
former encoders and rnn-t loss,‚Äù in ICASSP , 2020.
[7] Alex Graves, Santiago Fern ¬¥andez, Faustino Gomez, and J ¬®urgen
Schmidhuber, ‚ÄúConnectionist temporal classiÔ¨Åcation: la-
belling unsegmented sequence data with recurrent neural net-
works,‚Äù in ICML , 2006.
[8] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, ‚ÄúJoint ctc-
attention based end-to-end speech recognition using multi-task
learning,‚Äù in ICASSP , 2017.
[9] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur, ‚ÄúAudio augmentation for speech recognition,‚Äù in In-
terspeech , 2015.
[10] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
Barret Zoph, Ekin D Cubuk, and Quoc V Le, ‚ÄúSpecaugment: A
simple data augmentation method for automatic speech recog-
nition,‚Äù arXiv preprint arXiv:1904.08779 , 2019.
[11] Chengyi Wang, Yu Wu, Yujiao Du, Jinyu Li, Shujie Liu, Liang
Lu, Shuo Ren, Guoli Ye, Sheng Zhao, and Ming Zhou, ‚ÄúSe-
mantic mask for transformer based end-to-end speech recogni-
tion,‚Äù arXiv preprint arXiv:1912.03010 , 2019.
[12] Kanishka Rao, Has ¬∏im Sak, and Rohit Prabhavalkar, ‚ÄúExploring
architectures, data and units for streaming end-to-end speech
recognition with rnn-transducer,‚Äù in ASRU . IEEE, 2017.
[13] Albert Zeyer, Kazuki Irie, Ralf Schl ¬®uter, and Hermann Ney,
‚ÄúImproved training of end-to-end attention models for speech
recognition,‚Äù Interspeech , 2018.
[14] Kwangyoun Kim, Kyungmin Lee, Dhananjaya Gowda, Junmo
Park, Sungsoo Kim, Sichen Jin, Young-Yoon Lee, Jinsu Yeo,
Daehyun Kim, Seokyeong Jung, et al., ‚ÄúAttention based on-
device streaming speech recognition with large speech corpus,‚Äù
inASRU , 2019.
[15] Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary,
Oleksii Kuchaiev, Jonathan M Cohen, Huyen Nguyen, and
Ravi Teja Gadde, ‚ÄúJasper: An end-to-end convolutional neural
acoustic model,‚Äù Interspeech , 2019.
[16] Yuya Fujita, Aswin Shanmugam Subramanian, Motoi Omachi,
and Shinji Watanabe, ‚ÄúAttention-based asr with lightweight
and dynamic convolutions,‚Äù in ICASSP , 2020.[17] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-
Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, and
Yonghui Wu, ‚ÄúContextnet: Improving convolutional neural
networks for automatic speech recognition with global con-
text,‚Äù arXiv preprint arXiv:2005.03191 , 2020.
[18] Linhao Dong, Shuang Xu, and Bo Xu, ‚ÄúSpeech-transformer: a
no-recurrence sequence-to-sequence model for speech recog-
nition,‚Äù in ICASSP , 2018.
[19] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori,
Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson En-
rique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al.,
‚ÄúA comparative study on transformer vs rnn in speech applica-
tions,‚Äù in ASRU , 2019.
[20] Niko Moritz, Takaaki Hori, and Jonathan Le, ‚ÄúStreaming au-
tomatic speech recognition with the transformer model,‚Äù in
ICASSP) , 2020.
[21] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
et al., ‚ÄúRethinking attention with performers,‚Äù arXiv preprint
arXiv:2009.14794 , 2020.
[22] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma, ‚ÄúLinformer: Self-attention with linear complexity,‚Äù
arXiv preprint arXiv:2006.04768 , 2020.
[23] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen
Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind, ‚ÄúAn
attention free transformer,‚Äù arXiv preprint arXiv:2105.14103 ,
2021.
[24] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler,
‚ÄúEfÔ¨Åcient transformers: A survey,‚Äù ACM Comput. Surv. , apr
2022.
[25] Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang, and
Xing Xie, ‚ÄúFastformer: Additive attention can be all you need,‚Äù
arXiv preprint arXiv:2108.09084 , 2021.
[26] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dong-
dong Zhang, and Furu Wei, ‚ÄúDeepnet: Scaling transformers to
1,000 layers,‚Äù arXiv preprint arXiv:2203.00555 , 2022.
[27] Jing Pan, Tao Lei, Kwangyoun Kim, Kyu Han, and Shinji
Watanabe, ‚ÄúSru++: Pioneering fast recurrence with atten-
tion for speech recognition,‚Äù arXiv preprint arXiv:2110.05571 ,
2021.
[28] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-
mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-
dong Zhang, Yonghui Wu, et al., ‚ÄúConformer: Convolution-
augmented transformer for speech recognition,‚Äù in Inter-
speech , 2020.
[29] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe,
‚ÄúBranchformer: Parallel mlp-attention architectures to capture
local and global context for speech recognition and understand-
ing,‚Äù in ICML , 2022.
[30] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg
Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho
Sengupta, Adam Coates, et al., ‚ÄúDeep speech: Scaling up end-
to-end speech recognition,‚Äù arXiv preprint arXiv:1412.5567 ,
2014.
[31] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao,
Kai Chen, Mohammad Norouzi, and Quoc V Le, ‚ÄúQanet:

--- PAGE 8 ---
Combining local convolution with global self-attention for
reading comprehension,‚Äù arXiv preprint arXiv:1804.09541 ,
2018.
[32] David So, Quoc Le, and Chen Liang, ‚ÄúThe evolved trans-
former,‚Äù in ICML , 2019.
[33] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming
He, ‚ÄúNon-local neural networks,‚Äù in CVPR , 2018.
[34] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang
Dai, Lu Yuan, and Lei Zhang, ‚ÄúCvt: Introducing convolutions
to vision transformers,‚Äù in ICCV , 2021.
[35] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan,
‚ÄúCoatnet: Marrying convolution and attention for all data
sizes,‚Äù NeurIPS , 2021.
[36] Qiming Zhang, Yufei Xu, Jing Zhang, and Dacheng Tao,
‚ÄúVitaev2: Vision transformer advanced by exploring induc-
tive bias for image recognition and beyond,‚Äù arXiv preprint
arXiv:2202.10108 , 2022.
[37] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Pey-
man Milanfar, Alan Bovik, and Yinxiao Li, ‚ÄúMaxvit: Multi-
axis vision transformer,‚Äù arXiv preprint arXiv:2204.01697 ,
2022.
[38] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han,
‚ÄúLite transformer with long-short range attention,‚Äù in ICLR ,
2020.
[39] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and
Michael Auli, ‚ÄúPay less attention with lightweight and dy-
namic convolutions,‚Äù in ICLR , 2019.
[40] Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Ji-
ashi Feng, and Shuicheng Yan, ‚ÄúConvbert: Improving bert
with span-based dynamic convolution,‚Äù NeurIPS , 2020.
[41] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-
pher D Manning, ‚ÄúElectra: Pre-training text encoders
as discriminators rather than generators,‚Äù arXiv preprint
arXiv:2003.10555 , 2020.
[42] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova, ‚ÄúBert: Pre-training of deep bidirectional
transformers for language understanding,‚Äù arXiv preprint
arXiv:1810.04805 , 2018.
[43] Xuran Pan, Chunjiang Ge, Rui Lu, Shiji Song, Guanfu Chen,
Zeyi Huang, and Gao Huang, ‚ÄúOn the integration of self-
attention and convolution,‚Äù in CVPR , 2022.
[44] Jin Sakuma, Tatsuya Komatsu, and Robin Scheibler, ‚ÄúMlp-
based architecture with variable length input for automatic
speech recognition,‚Äù 2021.
[45] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao
Wang, and Shuicheng Yan, ‚ÄúInception transformer,‚Äù arXiv
preprint arXiv:2205.12956 , 2022.
[46] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,
‚ÄúIdentity mappings in deep residual networks,‚Äù in ECCV , 2016.
[47] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton,
‚ÄúLayer normalization,‚Äù arXiv preprint arXiv:1607.06450 ,
2016.
[48] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov, ‚ÄúDropout: a simple way
to prevent neural networks from overÔ¨Åtting,‚Äù JMLR , 2014.[49] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani, ‚ÄúSelf-
attention with relative position representations,‚Äù in NAACL-
HLT) , 2018.
[50] Dan Hendrycks and Kevin Gimpel, ‚ÄúGaussian error linear units
(gelus),‚Äù arXiv preprint arXiv:1606.08415 , 2016.
[51] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang
Ding, ‚ÄúScaling up your kernels to 31x31: Revisiting large ker-
nel design in cnns,‚Äù in CVPR , 2022.
[52] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao
Xiao, Boqian Wu, Mykola Pechenizkiy, Decebal Mocanu, and
Zhangyang Wang, ‚ÄúMore convnets in the 2020s: Scaling up
kernels beyond 51x51 using sparsity,‚Äù 2022.
[53] Jie Hu, Li Shen, and Gang Sun, ‚ÄúSqueeze-and-excitation net-
works,‚Äù in CVPR , 2018.
[54] Prajit Ramachandran, Barret Zoph, and Quoc V Le, ‚ÄúSearch-
ing for activation functions,‚Äù arXiv preprint arXiv:1710.05941 ,
2017.
[55] OÔ¨År Press, Noah A Smith, and Omer Levy, ‚ÄúImproving trans-
former models by reordering their sublayers,‚Äù in ACL, 2020.
[56] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev
Khudanpur, ‚ÄúLibrispeech: an asr corpus based on public do-
main audio books,‚Äù in ICASSP , 2015.
[57] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-
plin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya
Renduchintala, and Tsubasa Ochiai, ‚ÄúESPnet: End-to-end
speech processing toolkit,‚Äù in Interspeech , 2018.
[58] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh
Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu
Li, and Yifan Gong, ‚ÄúInternal language model estimation
for domain-adaptive end-to-end speech recognition,‚Äù in SLR,
2021.
[59] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki
Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo,
Chenda Li, Daniel Garcia-Romero, Jiatong Shi, et al., ‚ÄúRe-
cent developments on espnet toolkit boosted by conformer,‚Äù in
ICASSP , 2021.
[60] Gabriel Synnaeve, Qiantong Xu, Jacob Kahn, Tatiana
Likhomanenko, Edouard Grave, Vineel Pratap, Anuroop Sri-
ram, Vitaliy Liptchinsky, and Ronan Collobert, ‚ÄúEnd-to-end
asr: from supervised to semi-supervised learning with modern
architectures,‚Äù arXiv preprint arXiv:1911.08460 , 2019.
[61] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James
Qin, Ruoming Pang, and Yonghui Wu, ‚ÄúW2v-bert: Combining
contrastive learning and masked language modeling for self-
supervised speech pre-training,‚Äù in ASRU , 2021.
[62] Jacob Kahn, Morgane Rivi `ere, Weiyi Zheng, Evgeny
Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar ¬¥e, Julien
Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fue-
gen, et al., ‚ÄúLibri-light: A benchmark for asr with limited or
no supervision,‚Äù in ICASSP , 2020.
