# 2306.06446.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2306.06446.pdf
# Kích thước tệp: 6608266 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ShiftAddViT: Hỗn hợp các phép toán nguyên thuỷ nhân
Hướng tới Vision Transformers hiệu quả
Haoran You*, Huihong Shi*, Yipin Guo*, và Yingyan (Celine) Lin
Georgia Institute of Technology, Atlanta, GA
{haoran.you, celine.lin}@gatech.edu, eic-lab@groups.gatech.edu

Tóm tắt
Vision Transformers (ViTs) đã cho thấy hiệu suất ấn tượng và trở thành xương sống thống nhất cho nhiều tác vụ thị giác. Tuy nhiên, cả cơ chế attention và multi-layer perceptrons (MLPs) trong ViTs đều không đủ hiệu quả do các phép nhân dày đặc, dẫn đến việc training và inference tốn kém. Để giải quyết vấn đề này, chúng tôi đề xuất tái tham số hóa các ViTs đã được pre-trained với một hỗn hợp các phép toán nguyên thuỷ nhân, ví dụ như phép dịch bit và phép cộng, hướng tới một loại mô hình giảm nhân mới, được gọi là ShiftAddViT, nhằm đạt được tăng tốc inference end-to-end trên GPUs mà không cần training từ đầu. Cụ thể, tất cả MatMuls giữa queries, keys, và values được tái tham số hóa bằng additive kernels, sau khi ánh xạ queries và keys thành binary codes trong không gian Hamming. Các MLPs hoặc linear layers còn lại sau đó được tái tham số hóa với shift kernels. Chúng tôi sử dụng TVM để implement và tối ưu hóa những customized kernels này cho triển khai phần cứng thực tế trên GPUs. Chúng tôi nhận thấy rằng việc tái tham số hóa như vậy trên attention (bậc hai hoặc tuyến tính) duy trì độ chính xác của mô hình, trong khi không tránh khỏi việc giảm độ chính xác khi được áp dụng cho MLPs. Để kết hợp điểm mạnh của cả hai, chúng tôi tiếp tục đề xuất một framework mixture of experts (MoE) mới để tái tham số hóa MLPs bằng cách coi phép nhân hoặc các phép toán nguyên thuỷ của nó làm experts, ví dụ như nhân và dịch, và thiết kế một loss cân bằng tải nhận biết độ trễ mới. Loss này giúp train một router chung để gán một lượng token đầu vào động cho các experts khác nhau theo độ trễ của chúng. Về nguyên tắc, experts chạy càng nhanh thì càng được gán nhiều token đầu vào. Các thí nghiệm mở rộng trên nhiều tác vụ thị giác dựa trên Transformer 2D/3D liên tục xác nhận hiệu quả của ShiftAddViT được đề xuất, đạt được giảm độ trễ lên tới 5.18× trên GPUs và tiết kiệm năng lượng 42.9%, trong khi duy trì độ chính xác tương đương với ViTs gốc hoặc hiệu quả. Codes và models có sẵn tại https://github.com/GATECH-EIC/ShiftAddViT.

1 Giới thiệu
Vision Transformers (ViTs) đã nổi lên như những thay thế mạnh mẽ cho convolutional neural networks (CNNs) làm xương sống thị giác do hiệu suất ấn tượng của chúng [57,16]. Tuy nhiên, độ chính xác state-of-the-art (SOTA) của ViTs đi kèm với chi phí độ trễ phần cứng và tiêu thụ năng lượng cấm đoán cho cả training và inference, hạn chế việc triển khai và ứng dụng phổ biến của chúng [34,51], mặc dù các mô hình ViT đã được pre-trained có sẵn công khai. Các nút thắt cổ chai là hai mặt dựa trên nhiều kết quả profiling: attention và MLPs, ví dụ chúng chiếm 36%/63% FLOPs và 45%/46% độ trễ khi chạy DeiT-Base [55] trên NVIDIA Jetson TX2 GPU [72,19]). Các giải pháp ViT hiệu quả trước đây chủ yếu tập trung vào thiết kế macro-architecture [21,39,32,55,9,61] và tối ưu hóa linear attention [59,50,5,13,6,73]. Tuy nhiên, chúng ít chú ý đến việc giảm các phép nhân chiếm ưu thế cơ bản cũng như đồng tối ưu hóa cả attention và MLPs.

Chúng tôi xác định rằng một trong những cơ hội hiệu quả nhất nhưng vẫn còn thiếu để tăng tốc ViTs là tái tham số hóa các phép nhân dư thừa của chúng với một hỗn hợp các phép toán nguyên thuỷ nhân, tức là

*Đóng góp bằng nhau.
Hội nghị lần thứ 37 về Hệ thống xử lý thông tin thần kinh (NeurIPS 2023).arXiv:2306.06446v6 [cs.LG] 25 Jul 2024

--- TRANG 2 ---
phép dịch bit và phép cộng. Ý tưởng được rút ra từ thực hành thiết kế phần cứng phổ biến trong kiến trúc máy tính hoặc xử lý tín hiệu số. Đó là, phép nhân có thể được thay thế bằng phép dịch bit và phép cộng [67,22]. Cách tiếp cận lấy cảm hứng từ phần cứng như vậy có thể dẫn đến implementation phần cứng hiệu quả và nhanh mà không làm giảm độ chính xác của mô hình. Do đó, bài báo này thể hiện ý tưởng shift&add trong ViTs hướng tới một loại mô hình giảm nhân mới. Hơn nữa, không giống như ShiftAddNet trước đây [70] yêu cầu training đầy đủ và chậm cũng như hỗ trợ bộ tăng tốc phần cứng chuyên dụng, công trình này bắt đầu từ các ViTs đã được pre-trained để tránh chi phí khổng lồ của việc training từ đầu và nhắm vào tăng tốc inference end-to-end, tức là tăng tốc cả attention và MLPs, trên GPUs.

Bảng 1: Chi phí phần cứng dưới CMOS 45nm.
OPs Format Energy (pJ) Area (µm²)
Mult. FP32 3.7 7700
FP16 0.9 1640
INT32 3.1 3495
INT8 0.2 282
Add FP32 1.1 4184
FP16 0.4 1360
INT32 0.1 137
INT8 0.03 36
Shift INT32 0.13 157
INT16 0.057 73
INT8 0.024 34

Khái niệm shift&add nói trên độc đáo truyền cảm hứng cho thiết kế ViTs thân thiện phần cứng tự nhiên của chúng tôi, nhưng nó cũng đặt ra ba thách thức cho chúng tôi giải quyết. Thứ nhất, làm thế nào để tái tham số hóa ViTs hiệu quả với shifts và adds? ShiftAddNet trước đây [70] tái tham số hóa CNNs bằng cách nối tầng shift layers và add layers, dẫn đến số lượng layers hoặc tham số tăng gấp đôi. Các CUDA kernels tùy chỉnh của shift và add layers cũng bị training và inference chậm hơn nhiều so với PyTorch [42] trên GPUs. Cả hai đều thúc đẩy một phương pháp tái tham số hóa mới dễ tăng tốc cho ViTs. Thứ hai, làm thế nào để duy trì độ chính xác sau khi tái tham số hóa? Dự kiến sẽ thấy giảm độ chính xác khi các phép nhân được chuyển thành các phép toán nguyên thuỷ shift và add [8,11]. Hầu hết các công trình bù đắp cho việc giảm độ chính xác bằng cách mở rộng kích thước mô hình tận dụng lợi thế hiệu quả năng lượng được cải thiện nhiều của shifts và adds [70], ví dụ tiết kiệm năng lượng đơn vị lên tới 196× so với phép nhân như được hiển thị trong Tab. 1. Trong khi đó trong ViTs, nơi các hình ảnh đầu vào được chia thành các token không chồng lấp, người ta có thể độc đáo tận dụng độ nhạy thích ứng vốn có giữa các token đầu vào. Về nguyên tắc, các token thiết yếu chứa đối tượng mục tiêu được dự kiến sẽ được xử lý bằng các phép nhân mạnh mẽ hơn. Ngược lại, các token với thông tin nền ít quan trọng hơn có thể được xử lý bằng các phép toán nguyên thuỷ rẻ hơn. Nguyên tắc như vậy phù hợp với tinh thần của token merging gần đây [3] và các phương pháp thích ứng đầu vào [45,68,74] cho ViTs. Thứ ba, làm thế nào chúng ta có thể cân bằng việc loading và thời gian xử lý cho các token đầu vào quan trọng và ít quan trọng nói trên? Đối với ViTs, việc sử dụng hỗn hợp các phép toán nguyên thuỷ nhân có thể dẫn đến tốc độ xử lý khác nhau cho các token nhạy cảm và không nhạy cảm. Điều này cần được cân bằng; nếu không, các activation trung gian có thể mất nhiều thời gian hơn để đồng bộ hóa trước khi tiến tới layer tiếp theo.

Theo hiểu biết tốt nhất của chúng tôi, đây là nỗ lực đầu tiên để giải quyết ba thách thức trên và kết hợp khái niệm shift&add từ lĩnh vực phần cứng trong việc thiết kế ViTs giảm nhân bằng cách sử dụng hỗn hợp các phép toán nguyên thuỷ nhân. Cụ thể, chúng tôi đóng góp như sau:

• Chúng tôi lấy cảm hứng từ thực hành phần cứng để tái tham số hóa các ViTs đã được pre-trained với một hỗn hợp các phép toán nguyên thuỷ nhân bổ sung, tức là phép dịch bit và phép cộng, để mang lại một loại mạng giảm nhân mới, được gọi là ShiftAddViT. Tất cả MatMuls trong attention được tái tham số hóa bằng additive kernels, và các linear layers và MLPs còn lại được tái tham số hóa bằng shift kernels. Các kernels được xây dựng trong TVM cho triển khai thực tế.

• Chúng tôi giới thiệu một framework mixture of experts (MoE) mới cho ShiftAddViT để bảo tồn độ chính xác sau khi tái tham số hóa. Mỗi expert đại diện cho một phép nhân hoặc các phép toán nguyên thuỷ của nó, chẳng hạn như shifts. Tùy thuộc vào tầm quan trọng của một token đầu vào cho trước, expert thích hợp được kích hoạt, ví dụ phép nhân cho các token quan trọng và shifts cho các token ít quan trọng hơn.

• Chúng tôi giới thiệu một loss term cân bằng tải nhận biết độ trễ trong framework MoE của chúng tôi để phân bổ động các token đầu vào cho mỗi expert. Điều này đảm bảo rằng số lượng token được gán phù hợp với tốc độ xử lý của các experts, giảm đáng kể thời gian đồng bộ hóa.

• Chúng tôi tiến hành các thí nghiệm mở rộng để xác nhận hiệu quả của ShiftAddViT được đề xuất. Kết quả trên nhiều mô hình thị giác dựa trên Transformer 2D/3D liên tục cho thấy hiệu quả vượt trội của nó, đạt được giảm độ trễ lên tới 5.18× trên GPUs và tiết kiệm năng lượng 42.9%, trong khi duy trì độ chính xác tương đương hoặc thậm chí cao hơn so với ViTs gốc.

2 Các công trình liên quan
Vision Transformers (ViTs). ViTs [16,55] chia hình ảnh thành các patches hoặc tokens không chồng lấp và đã vượt trội hơn CNNs trên một loạt các tác vụ thị giác. Chúng sử dụng kiến trúc encoder-only đơn giản, chủ yếu được tạo thành từ các attention modules và MLPs. Các ViTs ban đầu đòi hỏi pre-training tốn kém trên các dataset mở rộng [52], và các tiến bộ tiếp theo như DeiT [55] và T2T-ViT [75] đã giảm bớt điều này bằng cách tinh chỉnh phương pháp training hoặc tạo ra các chiến lược tokenization sáng tạo. Sau đó, một làn sóng thiết kế kiến trúc ViT mới, chẳng hạn như PVT [60], CrossViT [7], PiT [26], MViT [18], và Swin-Transformer [35], đã xuất hiện để cải thiện sự đánh đổi độ chính xác-hiệu quả bằng cách sử dụng kiến trúc giống pyramid. Một xu hướng mới nổi khác xoay quanh các ViTs động như DynamicViT [45], A-ViT [68], ToME [3], và MIA-Former [74], nhằm loại bỏ thích ứng các token không thiết yếu. Framework MoE của chúng tôi cộng hưởng với khái niệm này nhưng giải quyết những token đó bằng cách sử dụng các phép toán nguyên thuỷ nhân chi phí thấp chạy đồng thời. Để triển khai edge, các giải pháp như LeViT [21], MobileViT [39], và EfficientViT [5,50] sử dụng các cơ chế attention hiệu quả hoặc tích hợp các feature extractors CNN. Cách tiếp cận này thúc đẩy ViTs đạt được tốc độ tương đương với CNNs, ví dụ MobileNets [47]. Ngược lại, ShiftAddViT được đề xuất của chúng tôi là đầu tiên rút ra từ các shortcut phần cứng shift&add để tái tham số hóa các ViTs đã được pre-trained, tăng tốc độ inference end-to-end và hiệu quả năng lượng mà không làm tổn hại độ chính xác. Do đó, ShiftAddViT trực giao với các thiết kế ViT hiện có và có thể được áp dụng trên đỉnh của chúng.

Những nỗ lực to lớn đã được dành để thiết kế các ViTs hiệu quả. Ví dụ, để giải quyết module self-attention tốn kém, có độ phức tạp bậc hai liên quan đến số lượng token đầu vào, nhiều linear attentions khác nhau đã được giới thiệu. Chúng có thể được chia rộng thành hai loại: local attention [35,2,56] và kernel-based linear attention [30,13,65,38,5,34,1]. Đối với loại trước, Swin [35] tính toán các thước đo tương tự giữa các token lân cận thay vì tất cả tokens. QnA [2] phân tán các attention queries trên tất cả tokens. MaxViT [56] sử dụng block attention và tích hợp dilated global attention để nắm bắt cả thông tin local và global. Về loại sau, hầu hết các phương pháp xấp xỉ hàm softmax [13,30,4] hoặc ma trận self-attention hoàn chỉnh [65,38] sử dụng các orthogonal features hoặc kernel embeddings, cho phép thứ tự tính toán chuyển từ (QK)V thành Q(KV). Hơn nữa, một số nỗ lực nhắm vào việc giảm số lượng phép nhân trong ViTs. Ví dụ, Ecoformer [34] giới thiệu một paradigm binarization mới thông qua kernelized hashing cho Q/K, đảm bảo rằng MatMuls trong attentions chỉ trở thành accumulations. Adder Attention [51] xem xét các thách thức của việc tích hợp các hoạt động adder vào attentions và đề xuất bao gồm một auxiliary identity mapping. ShiftViT [58] kết hợp các hoạt động spatial shift vào các attention modules. Khác với các công trình trước đây, ShiftAddViT của chúng tôi tập trung không chỉ vào attention mà còn cả MLPs hướng tới tăng tốc ViT end-to-end. Tất cả MatMuls có thể được chuyển đổi thành additions/accumulations bằng quantization, loại bỏ nhu cầu về kernelized hashing (KSH) phức tạp như trong [34]. Đối với các linear layers còn lại trong các attention modules hoặc MLPs, chúng được chuyển đổi thành bitwise shifts hoặc MoEs, thay vì sử dụng spatial shifts như [58] cho attentions.

Multiplication-less NNs. Nhiều nỗ lực nhằm giảm thiểu các phép nhân dày đặc, là những yếu tố đóng góp chính vào thời gian và tiêu thụ năng lượng trong CNNs và ViTs. Trong lĩnh vực CNNs, binary networks [14,29] nhị phân hóa cả activations và weights; AdderNet [8,66,62] hoàn toàn thay thế phép nhân bằng phép cộng với độ giảm độ chính xác đáng khen ngợi; Các mạng dựa trên Shift sử dụng spatial shifts [64] hoặc bitwise shifts [17] để thay thế phép nhân. Gần đây, các kỹ thuật ban đầu được phát triển cho CNNs đã tìm thấy ứng dụng trong ViTs. Ví dụ, BiTs [36,25] giới thiệu binary transformers duy trì độ chính xác đáng khen ngợi; Shu et al. và Wang et al. cũng di chuyển ý tưởng add hoặc shift sang ViTs với phạm vi giới hạn trong attentions [51,58]. Ngoài thiết kế mạng, cũng có những nỗ lực neural architecture search được thực hiện hướng tới cả mạng chính xác và hiệu quả [71,31]. So với công trình liên quan nhất, tức là ShiftAddNet [70] và ShiftAddNAS [71], ShiftAddViT của chúng tôi lần đầu tiên cho phép ý tưởng shift&add trong ViTs mà không làm giảm độ chính xác của mô hình, có ba đặc điểm khác làm cho nó phù hợp hơn cho các ứng dụng và triển khai thực tế: (1) bắt đầu từ các ViTs đã được pre-trained để tránh việc training tốn kém và tẻ nhạt từ đầu; (2) tập trung vào tối ưu hóa cho GPUs thay vì trên phần cứng chuyên dụng như FPGAs/ASICs; (3) tương thích liền mạch với framework MoE để cho phép chuyển đổi giữa hỗn hợp các phép toán nguyên thuỷ nhân, chẳng hạn như token-level routing và parallelism được áp dụng và thiết kế độc đáo cho ViTs.

3 Kiến thức cơ bản
Self-attention và Vision Transformers. Self-attention là thành phần cốt lõi của Transformers [57,16], và thường chứa nhiều heads H với mỗi head đo lường các mối tương quan cặp đôi giữa tất cả các token đầu vào để nắm bắt thông tin ngữ cảnh toàn cục. Nó có thể được định nghĩa như sau:

Attn(X) = Concat(H₁,···,Hₕ)W^O, trong đó Hᵢ = Softmax{f_Q(X)·f_K(X)^T/√d_k}·f_V(X), (1)

trong đó h biểu thị số lượng heads. Trong mỗi head, các token đầu vào X∈ℝⁿˣᵈ có độ dài n và chiều d sẽ được chiếu tuyến tính thành các ma trận query, key, và value, tức là Q,K,V∈ℝⁿˣᵈᵏ,

--- TRANG 4 ---
thông qua ba hàm ánh xạ tuyến tính, f_Q=XW^Q, f_K=XW^K, f_V=XW^V, trong đó d_k=d/h là chiều embedding của mỗi head. Kết quả từ tất cả các heads được nối lại và chiếu với ma trận trọng số W^O∈ℝᵈˣᵈ để tạo ra các outputs cuối cùng. Module attention như vậy được theo sau bởi MLPs với residuals để xây dựng một transformer block có thể được công thức hóa như sau:

X_Attn = Attn(LayerNorm(X)) + X, X_MLP = MLP(LayerNorm(X_Attn)) + X_Attn. (2)

Shift và Add Primitives. Implementation phần cứng trực tiếp của phép nhân không hiệu quả. Các phép toán nguyên thuỷ Shift và add phục vụ như "shortcuts" cho thiết kế phần cứng được sắp xếp hợp lý. Các hoạt động Shift, tương đương với việc nhân với lũy thừa của hai, mang lại tiết kiệm đáng kể. Như được hiển thị trong Tab. 1, các shifts như vậy có thể giảm sử dụng năng lượng lên tới 23.8× và diện tích chip 22.3× so với phép nhân [33,70], với định dạng dữ liệu INT32 và công nghệ CMOS 45nm. Tương tự, các hoạt động add, một phép toán nguyên thuỷ hiệu quả khác, có thể đạt được tiết kiệm năng lượng lên tới 31.0× và diện tích 25.5× so với phép nhân. Cả hai phép toán nguyên thuỷ đều đã thúc đẩy nhiều đổi mới kiến trúc mạng [8, 17, 70, 71].

4 ShiftAddViT được đề xuất
Tổng quan. Với sự sẵn có rộng rãi của các trọng số mô hình ViT đã được pre-trained, chúng tôi nhằm fine-tune và chuyển đổi chúng thành ShiftAddViTs giảm nhân để giảm độ trễ runtime và tăng hiệu quả năng lượng. Để đạt được điều này, chúng tôi phải tái tham số hóa cả attentions và MLPs trong ViTs thành các hoạt động shift và add. Thay vì sử dụng các shift layers và add layers nối tầng như trong ShiftAddNet [70] hoặc thiết kế các scheme forward và backpropagation cụ thể như AdderNet [8,51], chúng tôi dựa vào cách tiếp cận đơn giản nhưng hiệu quả của khả năng modeling long-dependency của ViTs. Chúng tôi đề xuất thay thế các MLPs/Linears/MatMuls chiếm ưu thế bằng shifts và adds, để nguyên attention. Cách tiếp cận này cho phép ShiftAddViTs được điều chỉnh từ các ViTs hiện có mà không cần training từ đầu. Như được minh họa trong Hình 1, đối với attentions, chúng tôi chuyển đổi bốn Linear layers và hai MatMuls thành shift và add layers, tương ứng. Đối với MLPs, việc thay thế trực tiếp bằng shift layers dẫn đến suy giảm độ chính xác đáng kể. Do đó, chúng tôi xem xét thiết kế một framework MoE chuyên dụng kết hợp một hỗn hợp các phép toán nguyên thuỷ nhân, chẳng hạn như nhân và shift, để đạt được cả mô hình cuối cùng chính xác và hiệu quả. Các bước này chuyển đổi các ViTs đã được pre-trained thành ShiftAddViTs, đạt được độ trễ runtime giảm nhiều trong khi duy trì độ chính xác tương đương hoặc thậm chí cao hơn.

4.1 ShiftAddViT: Tái tham số hóa hướng tới mạng giảm nhân
Phần này mô tả cách chúng tôi tái tham số hóa các ViTs đã được pre-trained với các hoạt động shift và add hiệu quả phần cứng, bao gồm implementation chi tiết và phân tích độ nhạy tương ứng.

Tái tham số hóa Attention. Có hai loại layers trong các attention modules: MatMuls và Linears, có thể được chuyển đổi thành Add và Shift layers, tương ứng. Để tái tham số hóa MatMuls thành Add layers, chúng tôi xem xét thực hiện binary quantization trên một toán hạng trong MatMuls, ví dụ Q hoặc K cho MatMul của QK, để các hoạt động multiply-and-accumulate (MAC) giữa hai ma trận sẽ được thay thế chỉ bằng các hoạt động add tiết kiệm năng lượng [34]. Hơn nữa, để xây dựng ShiftAddViT trên đỉnh của các linear attentions hiệu quả [59,34,38], chúng tôi trao đổi thứ tự của MatMuls từ (QK)V thành Q(KV) để đạt được độ phức tạp tuyến tính w.r.t. số lượng token đầu vào. Theo cách này, binary quantization sẽ được áp dụng cho K và Q như được minh họa trong Hình 1 (b), để nhánh V nhạy cảm hơn ở độ chính xác cao. Điều này cũng cho phép chúng tôi chèn một depthwise convolution (DWConv) nhẹ vào nhánh V song song để tăng cường khả năng nắm bắt đặc trưng địa phương với overhead không đáng kể

--- TRANG 5 ---
(<1% tổng MACs) theo các thiết kế linear attention SOTA gần đây [65,73]. Mặt khác, để tái tham số hóa bốn Linears còn lại trong attention module với Shift layers, chúng tôi dùng sign flips và power-of-two shift parameters để biểu diễn trọng số Linear [17, 70].

Chúng tôi minh họa implementation của Shift và Add layers trong Hình 2 và công thức hóa chúng như sau:

O_Shift = X^T·W_S = X^T·s·2^P, O_Add = X^T·1{W_A≠0}, (3)

trong đó X đề cập đến input activations, W_S và W_A là trọng số trong Shift và Add layers, tương ứng. Đối với trọng số Shift W_S = s·2^P, s = sign(W) ∈ {-1,1} biểu thị sign flips, P = round(log₂(abs(W))), và 2^P đại diện cho bitwise shift về trái (P>0) hoặc phải (P<0). Cả s và P đều có thể train được trong quá trình training. Đối với trọng số Add, chúng tôi đã áp dụng binary quantization và sẽ bỏ qua trực tiếp các số không, các trọng số còn lại tương ứng với additions hoặc accumulations. Kết hợp hai yếu tố yếu đó, tức là Shift và Add, dẫn đến khả năng biểu diễn cao hơn như cũng được nêu trong ShiftAddNet [70], chúng tôi nhấn mạnh rằng ShiftAddViT của chúng tôi đạt được sự kết hợp như vậy một cách liền mạch cho ViTs, mà không cần thiết phải nối tầng cả hai như một thay thế cho một convolutional layer.

Tái tham số hóa MLPs. MLPs thường bị bỏ qua nhưng cũng chiếm ưu thế trong độ trễ runtime của ViTs ngoài attentions, đặc biệt đối với số lượng token nhỏ hoặc trung bình [19,78,43]. Một suy nghĩ tự nhiên là tái tham số hóa tất cả MLPs với Shift layers như được định nghĩa ở trên. Tuy nhiên, thiết kế tích cực như vậy không thể tránh khỏi dẫn đến suy giảm độ chính xác nghiêm trọng, ví dụ ↓1.18% cho PVTv1-T [60], (DWConv được sử dụng giữa hai MLPs trong PVTv2 được giữ nguyên). Chúng tôi giải quyết điều này bằng cách đề xuất một framework MoE mới để đạt được tăng tốc với độ chính xác tương đương, như được hiển thị trong Hình 1 (c) và sẽ được trình bày chi tiết trong Phần 4.2.

Bảng 2: Phân tích độ nhạy của việc tái tham số hóa ViTs với Shift và Add.
Components Apply Acc. (%) of PVT [60, 61]
PVTv2-B0 PVTv1-T
- - 70.50 75.10
- MSA 71.25 76.21
Attention LA+Add 70.95 75.20
Shift 70.96 76.05
MLPs Shift 70.28 73.92
MoE 70.86 74.81

Phân tích độ nhạy. Để điều tra xem các phương pháp tái tham số hóa trên có hoạt động như mong đợi hay không, chúng tôi chia nhỏ tất cả các thành phần và tiến hành phân tích độ nhạy. Mỗi thành phần được áp dụng cho ViTs chỉ với 30 epochs finetuning. Như được tóm tắt trong Tab. 2, việc áp dụng linear attention (LA), Add, hoặc Shift cho các attention layers có ít ảnh hưởng đến độ chính xác. Nhưng việc áp dụng Shift cho MLPs có thể dẫn đến suy giảm độ chính xác nghiêm trọng đối với các mô hình nhạy cảm như PVTv1-T. Ngoài ra, việc làm cho MLPs hiệu quả hơn đóng góp rất nhiều vào hiệu quả năng lượng (ví dụ chiếm 65.7% trên PVTv2-B0). Cả góc độ độ chính xác và hiệu quả đều thúc đẩy chúng tôi đề xuất một scheme tái tham số hóa mới cho MLPs.

4.2 ShiftAddViT: Framework Mixture of Experts
Phần này trình bày chi tiết giả thuyết và phương pháp của framework MoE ShiftAddViT.

Giả thuyết. Chúng tôi đưa ra giả thuyết rằng có những token đầu vào quan trọng nhưng nhạy cảm cần thiết các mạng mạnh mẽ hơn nếu không sẽ bị suy giảm độ chính xác. Những token đó có khả năng chứa thông tin đối tượng trực tiếp tương quan với các mục tiêu tác vụ của chúng tôi như được xác nhận bởi [3,74,45,68]. Trong khi các token còn lại ít nhạy cảm hơn và có thể được biểu diễn đầy đủ ngay cả khi sử dụng các Shift layers rẻ hơn. Bản chất thích ứng đầu vào như vậy của ViTs đòi hỏi một framework có hỗn hợp các phép toán nguyên thuỷ nhân.

Mixture of Multiplication Primitives. Được thúc đẩy bởi phân tích độ nhạy trong Phần 4.1, chúng tôi xem xét hai experts Mult. và Shift để xử lý các token quan trọng nhưng nhạy cảm và các token không nhạy cảm, tương ứng. Như được minh họa trong Hình 1 (c), chúng tôi áp dụng framework MoE để bù đắp cho việc giảm độ chính xác. Mỗi biểu diễn token đầu vào x sẽ được chuyển đến một expert theo các gate values p_i=G(x) trong routers, được chuẩn hóa thông qua phân phối softmax p_i := e^p_i/Σ_j e^p_j và sẽ được train cùng với trọng số mô hình. Output có thể được công thức hóa là y = Σ_i G(x)·E_i(x), trong đó gate function G(x) = p_i(x)·1{p_i(x) ≥ p_j(x), ∀j≠i}, n và E_i biểu thị số lượng experts và expert thứ i, tương ứng. Cụ thể, có hai trở ngại khi implement MoE như vậy trong TVM:

--- TRANG 6 ---
• Dynamic Input Allocation. Việc phân bổ đầu vào được xác định động trong runtime, và router có thể train được trong các MoE layers tự động học cách phân phối này khi gradients giảm. Chúng tôi chỉ biết phân bổ khi mô hình được thực thi và chúng tôi nhận được outputs router. Do đó, hình dạng của expert input và các indexes tương ứng được thay đổi động. Điều này có thể được xử lý bởi PyTorch với dynamic graphs trong khi TVM mong đợi static input shape. Để giải quyết điều này, chúng tôi theo và tận dụng compiler support cho dynamism như được đề xuất trong Nimble [49] trên đỉnh Apache TVM để xử lý dynamic input allocation.

• Parallel Computing. Các experts khác nhau được dự kiến chạy song song, điều này có thể được hỗ trợ bởi một số framework training phân tán tùy chỉnh được tích hợp với PyTorch, ví dụ FasterMoE [24], và DeepSpeed [44]. Tuy nhiên, vẫn không tầm thường để hỗ trợ điều này bằng TVM. Một lựa chọn để mô phỏng là thực hiện tối ưu hóa modularized để bắt chước parallel computing, cụ thể, chúng tôi tối ưu hóa mỗi expert riêng biệt và đo độ trễ của chúng, độ trễ tối đa giữa tất cả experts sẽ được ghi lại và coi là độ trễ của MoE layer này, và tổng thời gian đã qua cho mỗi layer là độ trễ mô hình modularized cuối cùng được báo cáo. Để tránh bất kỳ nhầm lẫn tiềm ẩn nào giữa thời gian wall-clock thực đo, tức là không giả định parallelism, và độ trễ modularized mô phỏng, tức là giả định parallelism lý tưởng, chúng tôi báo cáo cả hai cho các mô hình chứa MoE layers trong Phần 5 để cung cấp nhiều insights hơn về tính khả thi và ý nghĩa chi phí của thuật toán.

Latency-aware Load-balancing Loss. Chìa khóa cho framework MoE của chúng tôi là thiết kế một routing function để cân bằng tất cả experts hướng tới độ chính xác cao hơn và độ trễ thấp hơn. Các giải pháp trước đây [20,23] sử dụng các experts đồng nhất và đối xử với chúng như nhau. Trong khi đó trong framework MoE của chúng tôi, sự khác biệt và tính dị thể giữa các experts Mult. mạnh mẽ nhưng chậm và các experts Shift nhanh nhưng ít mạnh mẽ hơn phát sinh một câu hỏi độc đáo: Làm thế nào để điều phối workload của mỗi expert để giảm thời gian đồng bộ hóa?

Câu trả lời của chúng tôi là một latency-aware load-balancing loss, đảm bảo (1) tất cả experts nhận được tổng có trọng số mong đợi của gate values; và (2) tất cả experts được gán số lượng token đầu vào mong đợi. Hai điều kiện này được thực thi bằng cách áp dụng importance và load balancing loss được định nghĩa dưới đây:

L_IMP = SCV{α_i·Σ_{x∈X} p_i(x)}^n_{i=1}, L_LOAD = SCV{α_i·Σ_{x∈X} q_i(x)}^n_{i=1}, (4)

trong đó SCV biểu thị squared coefficient of variation của các phân phối đã cho trên experts (cũng được sử dụng trong [46,48]); α_i đề cập đến hệ số nhận biết độ trễ của expert thứ i và được định nghĩa bởi (Lat_i)/(Σ_j Lat_j) vì các assignments mong đợi tỷ lệ nghịch với runtime latency của expert thứ i, Lat_i; q_i(x) là xác suất gate value của expert thứ i vượt trội hơn các expert khác, tức là top1 expert, và được cho bởi P(p_i(x) + ε ≥ p_j(x), ∀j≠i). Lưu ý rằng xác suất này là rời rạc, chúng tôi sử dụng noise proxy ε theo [48,23] để làm cho nó có thể khả vi. Latency-aware importance loss và load-balanced loss trên giúp cân bằng gate values và workload assignments, tương ứng, và có thể được tích hợp với classification loss L_CLS như total loss function L(X) = L_CLS(X) + λ·(L_IMP(X) + L_LOAD(X)) để training ViTs và gates đồng thời. λ được đặt là 0.01 cho tất cả thí nghiệm.

5 Thí nghiệm
5.1 Thiết lập thí nghiệm
Models và Datasets. Tasks và Datasets. Chúng tôi xem xét hai tác vụ thị giác dựa trên Transformer 2D và 3D đại diện để chứng minh sự ưu việt của ShiftAddViT được đề xuất, bao gồm phân loại hình ảnh 2D trên dataset ImageNet [15] với 1.2 triệu hình ảnh training và 50K hình ảnh validation và tác vụ novel view synthesis (NVS) 3D trên dataset Local Light Field Fusion (LLFF) [40] với tám scenes.

Models. Đối với tác vụ phân loại, chúng tôi xem xét PVTv1 [60], PVTv2 [61], và DeiT [55]. Đối với tác vụ NVS, chúng tôi xem xét các mô hình View- và Ray-Transformer dựa trên Transformer GNT [53].

Training và Inference Details. Đối với tác vụ phân loại, chúng tôi theo Ecoformer [34] để khởi tạo các ViTs đã được pre-trained với trọng số Multihead Self-Attention (MSA), dựa trên đó chúng tôi áp dụng tái tham số hóa của chúng tôi một finetuning hai giai đoạn: (1) chuyển đổi MSA thành linear attention [73] và tái tham số hóa tất cả MatMuls với add layers với 100 epoch finetuning, và (2) tái tham số hóa MLPs hoặc linear layers với shift hoặc MoE layers sau khi finetuning thêm 100 epoch. Lưu ý rằng chúng tôi theo PVTv2 [61] và Ecoformer để giữ stage cuối cùng là MSA để so sánh công bằng. Đối với tác vụ NVS, chúng tôi vẫn theo finetuning hai giai đoạn nhưng không chuyển đổi trọng số MSA thành linear attention để duy trì độ chính xác. Tất cả thí nghiệm được chạy trên server với tám RTX A5000 GPUs với mỗi GPU có 24GB GPU memory.

Baselines và Evaluation Metrics. Baselines. Đối với tác vụ phân loại, chúng tôi tái tham số hóa và so sánh ShiftAddViT với PVTv1 [60], PVTv2 [61], Ecoformer [34], và các biến thể MSA của chúng. Đối với tác vụ NVS, chúng tôi tái tham số hóa trên đỉnh GNT [53] và so sánh với cả vanilla NeRF [41]

--- TRANG 7 ---
và GNT [53]. Evaluation Metrics. Đối với tác vụ phân loại, chúng tôi đánh giá ShiftAddViT và baselines về độ chính xác, độ trễ GPU và throughput được đo trên RTX 3090 GPU. Đối với tác vụ NVS, chúng tôi đánh giá tất cả mô hình về PSNR, SSIM [63], và LPIPS [76]. Chúng tôi cũng đo và báo cáo tiêu thụ năng lượng của tất cả các mô hình trên dựa trên bộ tăng tốc phần cứng giống Eyeriss [12, 77], tính toán không chỉ năng lượng tính toán mà còn cả năng lượng di chuyển dữ liệu.

5.2 ShiftAddViT so với SOTA Baselines trên 2D Tasks

Bảng 3: So sánh tổng thể giữa ShiftAddViT và baseline cạnh tranh nhất trên năm mô hình.
Models Methods Acc. (%) Latency (ms) Energy (mJ)
PVTv2-B0 Ecoformer [34] 70.44 7.82 33.64
ShiftAddViT 70.59 1.51 27.13
PVTv1-T Ecoformer [34] NaN 7.43 93.47
ShiftAddViT 74.93 1.97 72.59
PVTv2-B1 Ecoformer [34] 78.38 8.02 106.2
ShiftAddViT 78.49 2.49 85.34
PVTv2-B2 Ecoformer [34] 81.28 15.43 198.2
ShiftAddViT 81.32 4.83 163.9
DeiT-T MSA [55] 72.20 5.12 66.88
ShiftAddViT 72.40 2.94 38.21

Để đánh giá hiệu quả của các kỹ thuật được đề xuất, chúng tôi áp dụng ý tưởng ShiftAddViT cho năm mô hình ViT thường được sử dụng, bao gồm DeiT [55] và các biến thể khác nhau của PVTv1 [60] và PVTv2 [61]. Chúng tôi so sánh hiệu suất của chúng với baselines trên tác vụ phân loại hình ảnh. Tab. 3 nêu bật so sánh tổng thể với baseline cạnh tranh nhất, Ecoformer [34], từ đó chúng tôi thấy rằng ShiftAddViT liên tục vượt trội hơn tất cả baselines về độ đánh đổi độ chính xác-hiệu quả, đạt được giảm độ trễ 1.74× ∼ 5.18× trên GPUs và tiết kiệm năng lượng 19.4% ∼ 42.9% được đo trên bộ tăng tốc Eyeriss [12] với độ chính xác tương đương hoặc thậm chí tốt hơn (↑0.04% ∼ ↑0.20%). Lưu ý rằng chúng tôi áp dụng MoE cho cả linear và MLP layers. Hơn nữa, chúng tôi tiếp tục cung cấp so sánh của nhiều biến thể ShiftAddViT và PVT có hoặc không có multi-head self-attention (MSA) và MoE trong Tab. 4 và 6. Chúng tôi quan sát thấy rằng ShiftAddViT có thể được tích hợp liền mạch với linear attention [73,65], Q/K quantization đơn giản hoặc nâng cao [34,27], và framework MoE được đề xuất, đạt được giảm độ trễ lên tới 3.06×/4.14×/8.25× và 2.47×/1.10×/2.09× và cải thiện throughput sau tối ưu hóa TVM trên RTX 3090 so với baselines MSA, PVT, và PVT+ MoE (lưu ý: hai Mult. experts), tương ứng, dưới độ chính xác tương đương, tức là ±0.5%.

Training Wall-clock Time. Để cung cấp nhiều insights hơn về chi phí training, chúng tôi thu thập thời gian wall-clock training, ít hơn 21% ∼ 25% so với training từ đầu như mô hình Transformer gốc và ít hơn 50× so với ShiftAddNet trước đây [70] trên GPUs. Ví dụ, training mô hình PVTv1-Tiny từ đầu mất 62 giờ trong khi finetuning của chúng tôi chỉ cần 46 giờ.

Linear Attention vs. MSA. Chúng tôi đảm bảo batch size (BS) là 1 cho tất cả đo độ trễ và tìm thấy hiện tượng phản trực giác rằng PVT với linear attention chậm hơn PVT với MSA. Lý do có hai mặt: (1) linear attention giới thiệu các hoạt động bổ sung, ví dụ normalization, DWConv, hoặc spatial reduction block tốn nhiều thời gian hơn dưới BS nhỏ; và (2) độ phức tạp tuyến tính w.r.t. số lượng tokens trong khi độ phân giải đầu vào 224 không đủ lớn, dẫn đến lợi ích hạn chế. Để xác nhận cả hai điểm, chúng tôi đo độ trễ của PVTv2-B0 với nhiều BS và độ phân giải đầu vào khác nhau như được hiển thị trong Phụ lục F, kết quả cho thấy lợi ích của linear attention xuất hiện dưới BS hoặc độ phân giải đầu vào lớn hơn.

5.3 ShiftAddViT so với SOTA Baselines trên 3D Tasks
Chúng tôi cũng mở rộng các phương pháp của chúng tôi cho các tác vụ NVS 3D và xây dựng ShiftAddViT trên đỉnh các mô hình GNT dựa trên Transformer [53]. Chúng tôi test và so sánh PSNR, SSIM, LPIPS, độ trễ, và năng lượng của cả ShiftAddViT và baselines, tức là vanilla NeRF [41] và GNT [53], trên dataset LLFF trên tám scenes. Như được hiển thị trong Tab. 5, ShiftAddViT của chúng tôi liên tục dẫn đến độ đánh đổi độ chính xác-hiệu quả tốt hơn, đạt được giảm độ trễ 22.3%/50.4% và tiết kiệm năng lượng 20.8%/54.3% dưới chất lượng generation tương đương hoặc thậm chí tốt hơn (↑0.55/↓0.19 PSNR trung bình trên tám scenes), so với baselines NeRF và GNT, tương ứng. Lưu ý rằng GNT tốn nhiều hơn NeRF vì số lượng layers tăng. Cụ thể, ShiftAddViT thậm chí đạt được chất lượng generation tốt hơn (lên tới ↑0.88 PSNR) so với GNT trên một số scenes đại diện, ví dụ Orchid và Flower, như được nêu bật trong Tab. 5, trong đó hiệu suất xếp hạng thứ nhất, thứ hai, và thứ ba được ghi chú với các màu tương ứng. Kết quả đầy đủ của tám scenes được cung cấp trong Phụ lục C. Lưu ý rằng chúng tôi chỉ finetune ShiftAddViT cho 140K steps trên đỉnh các mô hình GNT đã được pre-trained thay vì training lên đến 840K steps từ đầu.

5.4 Ablation Studies của ShiftAddViT
Performance Breakdown Analysis. Để điều tra cách mỗi kỹ thuật được đề xuất đóng góp vào hiệu suất cuối cùng, chúng tôi tiến hành ablation studies giữa nhiều loại biến thể ShiftAddViT trên các mô hình PVTv1-T, PVTv2-B0/B1/B2 để dần dần chia nhỏ và hiển thị lợi thế của mỗi thành phần. Như được hiển thị trong Tab. 4 và 6, chúng tôi có ba quan sát chung: (1) ShiftAddViT mạnh mẽ đối với phương pháp binarization của Q/K, ví dụ nó tương thích với KSH [34] hoặc vanilla binarization [27], đạt được trung bình giảm độ trễ 3.03× so với PVT gốc dưới độ chính xác tương đương (±0.5%), và cả giảm độ trễ/năng lượng 3.29×/1.32× và cải thiện độ chính xác 0.04% ∼ 0.20% so với Ecoformer [34]; (2) các phương pháp vanilla binarization hoạt động tốt hơn KSH trong framework ShiftAddViT của chúng tôi về cả độ chính xác (↓0.05% ∼ ↑0.21%) và hiệu quả (trung bình ↓5.9% giảm độ trễ và ↑5.8% tăng throughput). Hơn nữa, KSH yêu cầu Q và K giống hệt nhau trong khi vanilla binarization [27] không có hạn chế như vậy. Điều đó giải thích tại sao việc áp dụng vanilla quantization cho các linear attention layers dẫn đến cải thiện độ chính xác trung bình 0.15% so với việc sử dụng KSH. Cũng lưu ý rằng việc áp dụng linear attention theo [73] của chúng tôi hoạt động tốt hơn (↑0.29%) so với PVT [60,61]; (3) Việc thay thế linear layers trong attention modules hoặc MLPs bằng Shift layers dẫn đến giảm độ chính xác trong khi MoE được đề xuất có thể giúp bù đắp cho nó. Cụ thể, việc áp dụng Shift layers dẫn đến giảm độ chính xác trung bình 1.67% trong khi MoE thay vào đó cải thiện độ chính xác trung bình 1.37%. Tuy nhiên, MoE làm tổn hại hiệu quả của cả baseline PVT

--- TRANG 9 ---
và ShiftAddViT của chúng tôi mà không có tăng tốc hệ thống tùy chỉnh, ví dụ PVT+MoE tăng 94% độ trễ và giảm 51% throughput so với PVT trên GPUs, do hỗ trợ parallelism hạn chế, đặc biệt đối với TVM. Chúng tôi báo cáo độ trễ modularized bằng cách tối ưu hóa riêng biệt từng expert để chứng minh tiềm năng double-winning accuracy (↑0.94% ∼ ↑2.02%) và efficiency (↓15.4% ∼ ↑42.7%) của MoE.

Ngoài ra, chúng tôi muốn làm rõ rằng những cải thiện độ trễ tưởng chừng nhỏ của việc áp dụng shifts trong Tab. 4 và 6 là do tối ưu hóa đầy đủ của mô hình compiled như một tổng thể (ví dụ 6.34ms → 1ms cho PVTv2-B0) trên GPUs với diện tích chip đủ. Hầu hết gains được che giấu bởi data movements và system-level schedules. Việc áp dụng shift layers giảm đáng kể năng lượng và sử dụng diện tích chip như được xác nhận trong Tab. 1 và Phụ lục F. Dưới cùng diện tích chip, tiết kiệm độ trễ của việc áp dụng shifts phù hợp hơn, ví dụ PVTv2-B0 với shift hoặc MoE đạt được tăng tốc 3.9 ∼ 5.7× hoặc 1.6 ∼ 1.8×.

Hình 3: Energy breakdown trên bộ tăng tốc Eyeriss.

Ngoài các tác vụ 2D, chúng tôi cũng báo cáo performance breakdown trên các tác vụ NVS 3D, như được hiển thị trong Tab. 5. Ba quan sát trên vẫn đúng ngoại trừ Shift layer, vì chúng tôi thấy rằng ShiftAddViT với tất cả linear layers hoặc MLPs được thay thế bằng Shift layers đạt được PSNR (↑0.13 ∼ ↑0.20), SSIM (↑0.005 ∼ ↑0.007), và LPIPS (↓0.007 ∼ ↓0.009) thậm chí tốt hơn so với các biến thể khác. Ngoài ra, chúng tôi profile trên bộ tăng tốc Eyeriss để hiển thị energy breakdown của cả ShiftAddViT và baselines. Như được hiển thị trong Hình 3, ShiftAddViT của chúng tôi giảm 42.9% và 40.9% năng lượng trên đỉnh DeiT-T và GNT, tương ứng. Trong số đó việc áp dụng Add layers dẫn đến giảm năng lượng 93.8% và 63.8% so với MatMuls gốc, Shift layers giúp giảm 28.5% và 37.5% năng lượng so với linear/MLP layers trước đây. Tập hợp thí nghiệm này xác nhận hiệu quả của mỗi thành phần trong framework ShiftAddViT được đề xuất.

Speedups của Shifts và Adds. Ngoài so sánh tổng thể và profiling giảm năng lượng, chúng tôi cũng test tăng tốc GPU của các Shift và Add kernels tùy chỉnh, như được hiển thị trong Hình 4 và 5, tương ứng. Chúng tôi có thể thấy rằng cả hai kernels tùy chỉnh đạt được tốc độ nhanh hơn so với baselines PyTorch và TVM. Ví dụ, MatAdds của chúng tôi đạt được tăng tốc trung bình 7.54×/1.51× so với PyTorch và TVM MatMuls, tương ứng. MatShifts của chúng tôi đạt được tăng tốc trung bình 2.35×/3.07×/1.16× so với PyTorch FakeShifts [17], TVM FakeShifts, và TVM MatMuls, tương ứng.

Nhiều so sánh và phân tích tăng tốc dưới batch sizes lớn hơn được cung cấp trong Phụ lục A. Lưu ý rằng trong những so sánh đó, chúng tôi sử dụng default einsum operator cho PyTorch MatMuls, floating-point multiplication với power-of-twos cho FakeShift, và implement MatAdds và MatShifts bằng TVM [10] chúng tôi. Chúng tôi so sánh những customized operators này với các đối tác multiplication của chúng trong cùng setting để so sánh công bằng. Tăng tốc của MatAdds là vì chúng tôi thay thế multiplication-and-accumulation chỉ bằng accumulation. Tăng tốc của MatShifts chủ yếu được quy cho việc giảm bit (INT32 và INT8 cho inputs và shift signs/weights) và do đó giảm data movement thay vì computations gần như hoàn toàn bị ẩn đằng sau data movements.

MoE's Uniqueness trong ShiftAddViT. Đóng góp của chúng tôi là các ViTs giảm nhân và một framework MoE load-balanced mới. Chúng được liên kết trong một tổng thể hữu cơ. Không giống như các công trình MoE trước đây nơi tất cả experts có cùng capacity và không có sự phân biệt giữa các tokens, các MoE layers của chúng tôi kết hợp các experts không cân bằng, tức là multiplication và shift, với giả thuyết chia các object tokens quan trọng và background tokens. Thiết kế MoE mới như vậy cung cấp phiên bản mềm hơn của

--- TRANG 10 ---
ShiftAddViTs, tức là thay vì thay thế tích cực tất cả phép nhân bằng shifts và adds rẻ hơn, chúng tôi giữ lựa chọn multiplication mạnh mẽ để xử lý các tokens quan trọng để duy trì độ chính xác trong khi để tất cả các tokens không quan trọng còn lại được xử lý bởi bitwise shifts rẻ hơn, thắng một sự đánh đổi độ chính xác và hiệu quả tốt hơn.

Bảng 7: Ablation studies của ShiftAddViT w/ hoặc w/o LL-Loss trên mô hình PVT.
Models Methods Acc. (%) Norm. Latency
PVTv2-B0 w/o LL-Loss 70.38 100%
w/ LL-Loss 70.37 85.4%
PVTv1-T w/o LL-Loss 74.73 100%
w/ LL-Loss 74.66 85.5%

Ablation studies của Latency-aware Load-balancing Loss (LL-Loss). Để chứng minh hiệu quả của LL-Loss được đề xuất trong framework MoE ShiftAddViT, chúng tôi tiến hành ablation studies trên hai mô hình PVT như được hiển thị trong Tab. 7. Chúng tôi thấy rằng ShiftAddViT w/ LL-Loss đạt được độ đánh đổi độ chính xác-hiệu quả tốt hơn, ví dụ giảm độ trễ 14.6% trong khi duy trì độ chính xác tương đương (±0.1%) khi xem xét các experts Mult. và Shift. Việc giảm độ trễ có thể lớn hơn nếu chúng tôi xem xét nhiều experts không cân bằng hơn với runtimes khác biệt. Tập hợp thí nghiệm này biện minh cho hiệu quả của LL-Loss trong hệ thống MoE.

Visualization của Token Dispatches trong MoE. Để xác nhận giả thuyết rằng các tokens quan trọng nhưng nhạy cảm đòi hỏi các experts mạnh mẽ trong khi các tokens không nhạy cảm khác có thể được biểu diễn với các Shift experts rẻ hơn nhiều, chúng tôi trực quan hóa các token dispatches trong MoE routers/gates của MLP layer đầu tiên trong mô hình ShiftAddViT PVTv2-B0 như được hiển thị trong Hình 6. Chúng tôi thấy rằng router được thiết kế thành công xác định các tokens quan trọng chứa thông tin đối tượng, sau đó được dispatch đến các Multiplication experts mạnh mẽ hơn, để lại các tokens không quan trọng, ví dụ background, cho các Shift tokens rẻ hơn. Tập hợp trực quan hóa này tiếp tục xác nhận giả thuyết và giải thích tại sao framework MoE được đề xuất có thể hiệu quả tăng cường ShiftAddViT hướng tới độ đánh đổi độ chính xác-hiệu quả tốt hơn.

5.5 Limitation và Societal Impact Discussion
Chúng tôi đã thực hiện một bước vững chắc để hiển thị việc sử dụng thực tế của ShiftAddViT lấy cảm hứng từ phần cứng được đề xuất. Trong khi điều này dựa vào tối ưu hóa TVM chuyên dụng, tiềm năng đầy đủ có thể được khám phá với các bộ tăng tốc phần cứng tùy chỉnh hướng tới các ViTs hiệu quả phần cứng tự nhiên. Ngoài ra, framework MoE không cân bằng cho thấy tính tổng quát nhưng đòi hỏi cao về hỗ trợ hệ thống với parallelism lý tưởng.

6 Kết luận
Trong bài báo này, chúng tôi lần đầu tiên đề xuất một mô hình ViT giảm nhân lấy cảm hứng từ phần cứng được gọi là ShiftAddViT. Nó tái tham số hóa cả attention và MLP layers trong các ViTs đã được pre-trained với một hỗn hợp các phép toán nguyên thuỷ nhân, ví dụ bitwise shifts và adds, hướng tới tăng tốc end-to-end trên GPUs và các bộ tăng tốc phần cứng chuyên dụng mà không cần training từ đầu. Hơn nữa, một framework mixture of unbalanced experts mới được trang bị một latency-aware load-balancing loss mới được đề xuất để theo đuổi double-winning accuracy và hardware efficiency. Chúng tôi sử dụng multiplication hoặc các phép toán nguyên thuỷ của nó làm experts trong các trường hợp ShiftAddViT. Các thí nghiệm mở rộng trên cả tác vụ thị giác dựa trên Transformer 2D và 3D liên tục xác nhận sự ưu việt của ShiftAddViT được đề xuất so với nhiều baselines ViT. Chúng tôi tin rằng bài báo này mở ra một góc nhìn mới về thiết kế inference ViT tiết kiệm năng lượng dựa trên các mô hình ViT đã được pre-trained có sẵn rộng rãi.

Acknowledgement
Công trình được hỗ trợ một phần bởi National Science Foundation (NSF) thông qua chương trình RTML (Số giải thưởng: 1937592) và CoCoSys, một trong bảy trung tâm trong JUMP 2.0, một chương trình Semiconductor Research Corporation (SRC) được tài trợ bởi DARPA.
