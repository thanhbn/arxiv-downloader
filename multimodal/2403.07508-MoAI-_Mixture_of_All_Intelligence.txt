# 2403.07508.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2403.07508.pdf
# File size: 6915354 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MoAI: Mixture of All Intelligence
for Large Language and Vision Models
Byung-Kwan Lee, Beomchan Park, Chae Won Kim, and Yong Man Ro
School of Electrical Engineering
Korea Advanced Institute of Science and Technology (KAIST)
{leebk, bpark0810, chaewonkim, ymro}@kaist.ac.kr
Abstract. The rise of large language models (LLMs) and instruction
tuning has led to the current trend of instruction-tuned large language
and vision models (LLVMs). This trend involves either meticulously cu-
rating numerous instruction tuning datasets tailored to specific objec-
tives or enlarging LLVMs to manage vast amounts of vision language
(VL) data. However, current LLVMs have disregarded the detailed and
comprehensive real-world scene understanding available from specialized
computer vision (CV) models in visual perception tasks such as segmen-
tation, detection, scene graph generation (SGG), and optical character
recognition (OCR). Instead, the existing LLVMs rely mainly on the large
capacity and emergent capabilities of their LLM backbones. Therefore,
we present a new LLVM, Mixture ofAllIntelligence (
 MoAI), which
leverages auxiliary visual information obtained from the outputs of ex-
ternal segmentation, detection, SGG, and OCR models. MoAI operates
through two newly introduced modules: MoAI-Compressor andMoAI-
Mixer. After verbalizing the outputs of the external CV models, the
MoAI-Compressor aligns and condenses them to efficiently use relevant
auxiliary visual information for VL tasks. MoAI-Mixer then blends three
types of intelligenceâ€”(1) visual features, (2) auxiliary features from the
external CV models, and (3) language featuresâ€”utilizing the concept of
Mixture of Experts. Through this integration, MoAI significantly outper-
forms both open-source and closed-source LLVMs in numerous zero-shot
VL tasks, particularly those related to real-world scene understanding
such as object existence, positions, relations, and OCR without enlarg-
ing the model size or curating extra visual instruction tuning datasets.
Code is available in https://github.com/ByungKwanLee/MoAI.
Keywords: Large Language and Vision Models Â·Mixture of Experts
1 Introduction
Combining large language models (LLMs) such as PaLM [15] and T5 [78] with
instruction tuning datasets from Flan [86], Chung et al.[17] has developed Flan-
PaLM and Flan-T5 for instruction-tuned LLMs. These models leverage an ex-
panded instruction tuning dataset covering various tasks, and have been furtherarXiv:2403.07508v3  [cs.CV]  17 Jul 2024

--- PAGE 2 ---
2 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
Q-Bench
MM-Vet
MM-Bench
MME -PPOPETextVQASQA -IMG
InstructBLIP Qwen -VL LLaVA1.5 MoAI
 Gemini -Pro Qwen -VL-Plus GPT-4V MoAI
(a) Open -source LLVMs (b) Closed -source LLVMs
MME
HallusionBenchAI2D
Q-BenchSEEDMMB -CN
MathVistaMM-Bench
MMB -CN70.2
67.856.759.4
58.783.5
60.567.1
66.8
50.1
63.858.2
171478.9
148885.9
87.115111213
79.376.543.7
26.230.5
23.758.356.7
36.0
64.360.667.077.079.3
73.676.5
78.6
56.2
60.770.275.52275
70.774.4
74.3
75.9 78.2 73.9
43.3
49.945.2
63.965.856.468.9
74.170.61933
19272183
72.7 70.7 69.1
Fig. 1:Comparing the scores and accuracies of numerous VL benchmarks for various
open-source and closed-source LLVMs with those for
 MoAI.
scaled up to enlarge their capacities, resulting in notable improvements in zero-
shot performance across numerous language tasks.
Alongside the success of the instruction-tuned LLMs, several visual instruc-
tion tuning datasets [4,13,19,65,85] have been meticulously curated to enhance
zero-shot vision language (VL) performances in large language and vision models
(LLVMs). Furthermore, concerted efforts have been made to substantially scale
up LLVMs [1,4,64,85], aiming for strong zero-shot performances in VL datasets.
With the extension of visual instruction tuning datasets and the scaling up of
LLVMs, open-source LLVMs [1,4,10,13,19,29,64,65,85,92,97] have been closing
the gap in zero-shot VL performances compared to closed-source LLVMs such
as GPT-4V [72,73], Gemini-Pro [83], and Qwen-VL-Plus [4].
However, current open-source LLVMs have not explicitly or fully leveraged
detailed and comprehensive real-world scene understanding, relying mainly on
the large capacity and emergent capabilities of their LLM backbones. Several
studies in cognitive science and machine learning [6,22,25] argue that fundamen-
tal scene perception ability may stem from various cognitive functions, including
recognizing object presence, determining their positions, identifying their states,
understanding their relationships, extracting spatial scene layouts, and grasping
non-object notions which may include written texts. Fortunately, these cogni-
tive functions can be acquired from specialized computer vision (CV) models
which have been researched and developed over decades for visual perception
tasks such as segmentation [14,37], detection [70,98], scene graph generation
(SGG) [42,88], and optical character recognition (OCR) [23,57].
Shiftingthefocusfrominstruction-tuningtoutilizingtheseexternalCVmod-
elsisexpectedtoenhancethereal-worldsceneunderstandingofLLVMs,covering
object existence, positions, relations, and OCR. Recognition of objects and their
positions [52] can be facilitated by panoptic segmentation and open-world ob-
jectdetectionmodels.Foramorecomprehensiveunderstanding,involvingobject

--- PAGE 3 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 3
(a) MME
(d) MM -Vet
Accuracy (%) Score
(b) SEED
(c) MM -BenchAccuracy (%) Accuracy (%)
InstructBLIP Qwen -VL LLaVA1.5 MoAI
Fig. 2:Comparing the scores and accuracies of dimensions related to real-world scene
understanding in MME [28], SEED [55], MM-Bench [66], and MM-Vet [91] for val-
idating capabilities of various LLVMs such as InstructBLIP [19], Qwen-VL [4], and
LLaVA1.5 [63].
states and relationships ( i.e.,compositional reasoning [22]), a scene graph gen-
eration (SGG) model is necessary. Moreover, text descriptions within an image
as a non-object notion can be recognized through an OCR model.
In light of this, we propose a new LLVM, MixtureofAllIntelligence (
MoAI), which leverages auxiliary visual information obtained from various
sources: (1) panoptic segmentation [14], (2) open-world object detection [70],
(3) SGG [88], and (4) OCR [23] models. To effectively leverage this informa-
tion, we introduce two new modules: MoAI-Compressor andMoAI-Mixer . The
MoAI-Compressor aligns and condenses the verbalized outputs of the external
CV models into auxiliary visual information, enabling the efficient use of rele-
vant information for VL tasks. Subsequently, MoAI-Mixer blends three types of
intelligenceâ€”(1) visual features, (2) auxiliary features from external CV models,
and (3) language featuresâ€”into a cohesive whole.
In constructing the MoAI-Mixer, we draw inspiration from the concept of
Mixture of Experts (MoE) [71,79,80,96]. Our challenge lies in seamlessly inte-
grating original features ( i.e.,visual and language features) used in the multi-
modal language model (MLM) of MoAIâ€”an LLM backbone that takes visual
tokens outputted by the visual encoder along with text tokensâ€”with auxiliary
features acquired from external CV models and the MoAI-Compressor. We em-

--- PAGE 4 ---
4 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
ploy cross- and self-attention modules to construct six expert modules in the
MoAI-Mixer, covering the three types of aforementioned intelligence. Further-
more,weutilizegatingnetworkstodeterminetheoptimalcombinationofweights
for these expert modules.
By combining the MoAI-Compressor and MoAI-Mixer, MoAI effectively uti-
lizes outputs from external CV models and mix three sources of intelligence,
thereby enhancing its visual perception capabilities for tackling complex ques-
tion answering tasks. As depicted in Fig. 2, our results demonstrate that MoAI
has significantly outperformed in visual perception scores three strong LLVM
baselines: InstructBLIP [19], Qwen-VL [4], LLaVA1.5 [63], even without addi-
tional curation of visual instruction tuning datasets or scaling up LLVMs. Fur-
thermore, owing to its improved visual perception ability, MoAI exhibits potent
zero-shot performances in VL tasks, surpassing closed-source LLVMs, as illus-
trated in Fig. 1. The success of MoAI is attributed to its utilization of diverse
auxiliary visual information from external CV models and the integration of
three intelligence types to effectively execute VL tasks. Our contribution can be
summarized in two main aspects as follows:
â€“We introduce a new large language and vision model,
 MoAI, which han-
dles various auxiliary visual information from external CV models ( MoAI-
Compressor ) and blends three types of intelligence ( MoAI-Mixer ).
â€“
MoAIstands out for its exceptional visual perception ability in VL tasks,
surpassing both open-source and closed-source LLVMs in zero-shot VL per-
formances. This ability is achieved by considering detailed and comprehen-
sive real-world scene understanding without requiring scaling up either the
model size or dataset size.
2 Related Works
LLMs and LLVMs. LLMs have emerged alongside their competent general-
ization capability and the effectiveness of instruction tuning datasets. GPTs [7,
76,77] played a crucial role in paving the way for LLMs by demonstrating strong
zero-shot or few-shot performance across various language tasks, including text
classification, question answering, machine translation, complex reasoning tasks,
and so on. These generalization abilities of LLMs have been achieved by enor-
mously increasing both model capacities and training datasets, as seen in works
such as T5 [78], PaLM [15], OPT [93]. The progress in training methods and
datasets further enhances the zero-shot generalization of LLMs, transitioning
from large-scale pre-training datasets to instruction tuning datasets [17,34,74,
86].Instructiontuning[86]enablesLLMstofollowinstructionsinhumannatural
language under complex real-world scenarios. Instruction-tuned LLMs, such as
Flan-T5, Flan-PaLM [17], OPT-IML [34], and InstructGPT [74], clearly demon-
strate the effectiveness of instruction tuning. Researchers have taken a step fur-
ther by applying similar strategies to multimodal counterparts, LLVMs, which
consist of a visual encoder and a backbone multimodal language model (MLM).

--- PAGE 5 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 5
For example, LLaVA [65] and ShareGPT4V [13] utilize GPT-4 [2] and GPT-
4V [72,73], respectively, to create visual instruction tuning datasets, while oth-
ers [4,19,85] have also developed various visual instruction tuning datasets for
their own unique objectives. However, the existing LLVMs have overlooked the
detailed and comprehensive real-world scene understanding available from CV
models with great advancements over the last decades. The CV models have
been overshadowed by LLVMs with enlarged capacities and visual instruction
tuning datasets in the era of LLVMs. From this perspective, MoAI highlights the
effectiveness of utilizing auxiliary visual information obtained from external CV
models, showing enhanced visual perception capabilities for VL benchmarks.
Mixture of Experts. Jacobs et al.[36] has first introduced the concept of
Mixture of Experts (MoE) to machine learning, where separate networks called
â€˜expertsâ€™ handle different segments of the input space, and each segment is
guided to relevant experts by a gating network. This idea is further developed
by deep MoE [24] where MoE layers are stacked in depth, and by conditional
computation [5] where only a few experts are conditionally activated by a given
input. In modern deep learning, Shazeer et al.[80] integrates an MoE layer
with LSTMs [32] where a gating network independently routes each token to se-
lectively activated experts. This integration enhances performance in language
modeling and machine translation tasks. Furthermore, Switch Transformers [26]
merge an MoE layer and Transformers [84] by replacing a dense feed forward
network (FFN) inside a Transformer layer with multiple experts and a gating
network,pavingawaytothesuccessfuluseofMoEinTransformer-basedLLVMs
such as MoE-LLaVA [59]. The philosophy of MoE in deep learning is to enlarge
model capacity without sacrificing computational efficiency [24,26,38,45,59,80,
99]. On the other hand, we focus on a different yet fundamental aspect of MoE,
whereweintendthateachexpertisdesignedtospecializeinaparticularsegment
ofinput.WhilepreviousMoEmethodsdonotexplicitlyassignrolestoindividual
experts and instead expect specialization to emerge during optimization, MoAI
designates cross- and self-attention modules as experts and learns them explic-
itly to mix information across modalities ( i.e.,visual, auxiliary, and language
features). Specifically, MoAI facilitates pairs of (1) visual-auxiliary feature, (2)
visual-language feature, (3) visual-visual feature, (4) language-auxiliary feature,
(5) language-visual feature, and (6) language-language feature. Each pair is con-
sideredasaquery-keypairforarespectivecross-orself-attentionmoduleserving
as experts, clarifying the fusion of information across diverse modalities.
3
MoAI: Mixture of All Intelligence
Model Architecture. As depicted in Fig. 3, MoAI consists of a vision encoder,
a backbone multimodal language model (MLM) equipped with MoAI-Mixers,
intermediate MLP connectors between the vision encoder and MLM, and a
MoAI-Compressor which leverages four external computer vision (CV) mod-
els for panoptic segmentation [14], open-world object detection [70], scene graph

--- PAGE 6 ---
6 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
Vision
Language InstructionMLP
Compressed
Learnable
TokensWord Embed
Auxiliary
Visual Information
MoAI -Compressor
CV Models
VerbalizationWord Embed
Multimodal Language Model (MLM) with  MoAI -Mixer
Fig. 3:Overview of
 MoAIarchitecture. Compressed learnable tokens, the param-
eters of MoAI-Compressor and MoAI-Mixer are learned. â€˜Visionâ€™ represents vision en-
coder to embed visual features and ice/fire symbols represent the modules to freeze or
learn. Note that, â€˜Word Embedâ€™ represents the word embedding dictionary of MLM.
generation (SGG) [88], and optical character recognition (OCR) [23]. MoAI-
Compressorisintroducedtoprocessdiverseauxiliaryvisualinformationacquired
from the external CV models, where the CV model outputs are processed via
verbalization as shown in Fig. 4 to make them aligned and interpretable to the
MLM utilized in MoAI. In addition, MoAI-Mixer is further presented to effi-
ciently harmonize original two features ( i.e.,visual and language features) with
auxiliary features from the external CV models. The details of verbalization,
MoAI-Compressor, and MoAI-Mixer will be explained in this section.
Vision and Language Backbone. CLIP-L/14 [75] is selected as the vision
encoder, due to its guaranteed proficiency in image understanding aligned with
text for vision language tasks [13,63â€“65]. The MLM utilized in MoAI is based on
InternLM2-7B[8],whichisamultilingualfoundationmodelinstruction-tunedby
multilingualdatasetswith1.6Ttokensthroughaseriesofprogressivepretraining
phasesandreinforcementlearningfromhumanfeedback(RLHF)[16,74,82].Two
linear layers with GELU activation function [31] serve as the bridge connector
between vision and language components, denoted by â€˜MLPâ€™ in Fig. 3.
Verbalization. Since a multimodal language model (MLM) is adopted to con-
structMoAI,weconvertCVmodeloutputsintonaturallanguageformatinorder
to make them understandable to the MLM through a process called verbaliza-
tion. Fig. 4 illustrates how the four CV model outputs undergo verbalization
alongside the creation of auxiliary tokens semantically aligned to the MLM.
A panoptic segmentation model enables us to distinguish foreground and
background objects in an image at once. Furthermore, we can compute bound-
ing box coordinates ( e.g., [xmin, ymin, xmax, ymax]) from the segmentation map.
Consequently, verbalizing the outputs from panoptic segmentation (PS) entails
serializing bounding box coordinates and their object names as explained in
Fig. 4. These verbalized descriptions are then transformed into auxiliary to-

--- PAGE 7 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 7
Leveraging Computer Vision (CV) Model Outputs: How to Embed Auxiliary Features?
h
wdPS Resultâ€¢Panoptic Segmentation (PS)
â€¢Scene Graph Generation (SGG)â€¢Open -World Object Detection (OWOD)
â€¢Optical Character Recognition (OCR)â€¦
grass [0.93, 0.52, 1.00, 0.87]sky [0.00, 0.00, 0.41, 0.28]
person [0.11, 0.26, 0.20, 0.75]
truck [0.39, 0.07, 0.97, 0.97]
Word Embed
OWOD Result
clock [0.31, 0.26, 0.55, 0.39]flag[0.64, 0.12, 0.78, 0.17]
flagpole [0.61, 0.11, 0.63, 0.30]
statu e[0.42, 0.16, 0.46, 0.23]
Word Embed
The image includes bounding 
box coordinates and their 
objects: [0.64, 0.12, 0.78, 0.17] 
flag, and [0.61, 0.11, 0.63, 0.30] 
flagpole, and [0.42, 0.16, 0.46, 
0.23] statue, and [0.31, 0.26, 
0.55, 0.39] clock.â€¦hwâ€¦Auxiliary 
Tokens
OWOD Verbalization
Flatten
OCR Result
The Fraud ChroniclesWILLIAM
â€œ520%â€
Word Embed
The image includes text 
descriptions: WILLIAM, and â€œ520%â€, 
and MILLER, and THE, and 
PONZIFILES, and V.G.Oltmann , and 
The Fraud Chronicles.â€¦OCR Verbalization
Word Embed
The image includes relationships between objects: 
building is beside tree, and grass is attached to tree, and 
cow is on grass, and cow is lying on grass.â€¦SGG Verbalizationğ‘¨ğğ’
ğ‘¨ğğ–ğğƒ
ğ‘¨ğ’ğ†ğ†
ğ‘¨ğğ‚ğ‘sky
unknowntruck
grasspersontree
personPS Verbalization
The image includes bounding 
box coordinates and their 
objects: [0.00, 0.00, 0.41, 0.28] 
sky, and [0.11, 0.26, 0.20, 0.75] 
person, and [0.93, 0.52, 1.00, 
0.87] grass, and [0.00, 0.00, 
1.00, 0.53] tree, and [0.28, 
0.29, 0.37, 0.69] person, and 
[0.39, 0.07, 0.97, 0.97] truck.â€¦
Concat
For compatibility with Word Embed
Vision
MLP
Processing Panoptic Map
with Vision and MLP
MILLER
â€¦
sculpture [0.14, 0.05, 0.82, 1.00 ]The image includes bounding 
box coordinates and their objects: 
[0.14, 0.05, 0.82, 1.00] sculpture.Panoptic Map
The image includes relationships between objects: person is 
carrying backpack, and person is on motorcycle, and motorcycle 
is on road, and person is riding motorcycle.
Fig. 4:Verbalization process of
 MoAIfor external CV models: panoptic segmen-
tation (PS), open-world object detection (OWOD), scene graph generation (SGG),
and optical character recognition (OCR). Note that, â€˜dâ€™ denotes channel dimension of
MLM, thus auxiliary tokens have equal channel dimension.
kens through the word embeddings of MLM. Additionally, to directly utilize the
panoptic segmentation map, we use a vision encoder and an MLP connector in
MoAI to generate locality-preserving auxiliary tokens. The generated auxiliary
tokens are flattened and concatenated to those from serialized bounding boxes
and their object names to form the final PS auxiliary tokens APS. They are

--- PAGE 8 ---
8 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
concatenated in this manner so that the MLM of MoAI can associate them in
a compatible way through contextualization. This procedure ensures the com-
prehensive conversion of visual information from PS into language information
while preserving the spatial locality inherent in the panoptic segmentation map.
Note that if the panoptic segmentation model fails to classify objects within the
fixed number of panoptic object categories, for instance, those in MS-COCO
2017 [60] encompassing 133 object categories, the unknown class is assigned.
An open-world object detection model plays a role in detecting object classes
missed by the panoptic segmentation model. This is because the panoptic seg-
mentation model is trained on a specific dataset with a fixed number of object
categories. Once the detection results are generated for an image, bounding
box coordinates and their object names are verbalized according to the follow-
ing template format: â€˜The image includes bounding boxes and their objects:
{verbalized open-world object detection (OWOD) results}â€™. Then, the results
are transformed into OWOD auxiliary tokens AOWODby the word embeddings
of MLM. Similarly, the outputs of SGG and OCR models are verbalized, and
corresponding auxiliary tokens ASGGandAOCRare generated, where we use
the following verbalization templates: â€˜The image includes relationships between
objects: {verbalized SGG results}â€™ and â€˜The image includes text descriptions:
{verbalized OCR results}â€™, respectively.
MoAI-Compressor. After the verbalization of CV model outputs, four aux-
iliary tokens APS,AOWOD,ASGG, and AOCRare generated and injected into
MoAI-Compressor, which borrows the structure of Perceiver Resampler [3]. All
four auxiliary tokens [APS, AOWOD , ASGG, AOCR]are concatenated before being
fed into MoAI-Compressor along with a fixed number of learnable tokens Ainput,
whose outputs Aare also fixed in length by the same number and represent the
compressed and aligned auxiliary visual information, as formulated as follows:
A=MoAI-Compressor ([APS, AOWOD , ASGG, AOCR], Ainput).(1)
Due to the variable length of concatenated auxiliary tokens across images and
their substantial length after concatenation, MoAI-Compressor is designed to
condense those tokens [APS, AOWOD , ASGG, AOCR]with a relatively small fixed
size of 64, generating AâˆˆRdÃ—64where drepresents the embedding dimension.
These condensed tokens are then used to extract relevant information for VL
tasks by MoAI-Mixer. This compression enhances computational efficiency.
MoAI-Mixer is embedded in each MLM layer of MoAI. It receives auxiliary
tokens Afrom MoAI-Compressor, visual features I(l)âˆˆRdÃ—NI, and language
features L(l)âˆˆRdÃ—NLwhere l= 0,1,Â·Â·Â·, Nâˆ’1denotes the layer index, d
denotes the embedding dimension, NIdenotes the length of visual features,
andNLdenotes that of language features. Normally, an MLM layer only con-
sists of a Transformer decoder block TransDec(l)such that [I(l+1), L(l+1)] =
TransDec(l)([I(l), L(l)]). In MoAI, an l-th MLM layer with MoAI-Mixer is for-

--- PAGE 9 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 9
MLM Layer in MoAI
Transformer
Decoder Bloc k
MoAI -Mixer
Ã—NImage
PartLanguage
PartMoAI -Compressor
Compressed
Learnable
Tokens
Auxiliary 
Visual Information
CA AUX
IMGCA
IMGSA
IMGk/v
q qk/vLANG
CA AUX CA SA
k/v
q qk/v
LANG LANG LANGIMGğ‘°ğ€ğ”ğ— ğ‘°ğ‹ğ€ğğ† ğ‘°ğ’ğ„ğ‹ğ…
ğ‘³ğ€ğ”ğ— ğ‘³ğˆğŒğ† ğ‘³ğ’ğ„ğ‹ğ…ğ‘°
ğ‘³ğ‘° ğ‘°
ğ‘³ ğ‘³
Word Embed
ğ‘¨ğ‘¨
Fig. 5:Illustrating MoAI-Mixer in MLM Layer of
 MoAI. In MoAI-Mixer, there are
six expert modules to harmonize auxiliary features Aand two original features ( i.e.,
visual Iand language Lfeatures).
mulated as follows:
[Ë†I(l),Ë†L(l)] =MoAI-Mixer(l)(A, I(l), L(l)),
[I(l+1), L(l+1)] =TransDec(l)(Ë†I(l),Ë†L(l)),(2)
where Ë†I(l)andË†L(l)aremixedvisualfeaturesandmixedlanguagefeatures.Ineach
MoAI-Mixer, we design six expert modules that are either cross- or self-attention
modules as illustrated in Fig. 5: three for visual features Iand three for language
features L.Eachofthreeexpertmodulesforvisualfeaturesoutputs IAUX,ILANG,
andISELFwhere the capital letter indicates query features and the subscript
indicates key/value features.Similarly, eachof three expert modulesfor language
features outputs LAUX,LIMG, and LSELF. The cross-attention operation at the
l-th layer is formulated as follows:
I(l)
{AUX or LANG}=CA(l)(q=I(l), k={AorL(l)}, v=k),
L(l)
{AUX or IMG}=CA(l)(q=L(l), k={AorI(l)}, v=k).(3)
In addition, the self-attention operation is formulated as I(l)
SELF =SA(l)(I(l))
andL(l)
SELF =SA(l)(L(l)). These six expert modules explicitly specialize in one
of the following six distinct mixtures of intelligence: IAUX,ILANG,ISELF,LAUX,
LIMG, and LSELF. When training the expert modules, we borrow the concept
of LoRA [33] to reduce computational burden. Letâ€™s denote Was a general
notation for a linear projection layer in a multi-head attention module [84],
which can be Wq,Wk,Wv, orWo. We decompose WâˆˆRdÃ—d, not âˆ†Was
in LoRA, into two linear layers WAâˆˆRdÃ—randWBâˆˆRrÃ—dsuch that W=
WAWB. The hyperparameter rdenotes the reduced dimension as illustrated in
Fig.6(a).Sincecomputationalburdenofanattentionmodulemainlycomesfrom
the high embedding dimension, usually d= 4096, such formulation of projection

--- PAGE 10 ---
10 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
LANGğ‘°ğ€ğ”ğ—ğ‘°ğ‹ğ€ğğ† ğ‘°ğ’ğ„ğ‹ğ…ğ‘³Gating
NetworkÃ—
IMGğ‘°+
Ã— Ã—
ğ‘³ğ€ğ”ğ— ğ‘³ğˆğŒğ† ğ‘³ğ’ğ„ğ‹ğ…Gating
NetworkÃ—+
Ã— Ã—ğ’˜ğ’ğ„ğ‹ğ…
ğ’˜ğ‹ğ€ğğ†
ğ’˜ğ€ğ”ğ—ğ’˜ğ’ğ„ğ‹ğ…
ğ’˜ğˆğŒğ†
ğ’˜ğ€ğ”ğ—
Scaled Dot -Product 
AttentionScaled Dot -Product 
AttentionScaled Dot -Product 
AttentionScaled Dot -Product 
Attention
ğ’’
ğ’Œ
ğ’—
(a) CA/SA with Low Rank Adaptation ( LoRA ) for Expert Modules 
(b) Gating Networks for MoAI -Mixer +Split
Split
SplitConcat
ğ’“ğ’…ğ‘¾ğ‘¨ğ’’ğ‘¾ğ‘©ğ’’
ğ‘¾ğ‘¨ğ’Œğ‘¾ğ‘©ğ’Œ
ğ‘¾ğ‘¨ğ’—ğ‘¾ğ‘©ğ’—ğ‘¾ğ‘¨ğ’ğ‘¾ğ‘©ğ’
Fig. 6:The structures of (a) expert modules and (b) gating networks for MoAI-Mixer.
In (a), â€˜ qâ€™, â€˜kâ€™, and â€˜ vâ€™ denote query, key, and value, respectively, â€˜ dâ€™ and â€˜ râ€™ explains
channel dimension and reduced dimension, respectively.
matrices significantly reduces computation. Moreover, the input query features
are directly added to the output features so that mixture of intelligence occurs
without altering the outputs of the previous MLM layer too much, stabilizing
the optimization process with the frozen Transformer decoder blocks.
First Training Step. Together with the MLP connector, we first train Ainput,
MoAI-Compressor,andMoAI-Mixerbyusingvisualinstructiontuningdatasets[13,
63]. This step ensures that the six expert modules in MoAI-Mixer yield mean-
ingful features to conduct VL tasks. To do so, we randomly choose outputs from
one of three expert modules for visual and language features, respectively, as
follows:
Ë†I(l)=Sample (I(l)
AUX, I(l)
LANG , I(l)
SELF),Ë†L(l)=Sample (L(l)
AUX, L(l)
IMG, L(l)
SELF).(4)
Then, they are injected into the transformer decoder block TransDec l(Ë†I(l),Ë†L(l)).
This sampling process aims for each expert module to produce meaningful fea-
tures independently.
Second Training Step. Inthisstep,weextendthelearningprocessbeyondthe
parameters learned in the first training step. We learn two gating networks for
each MoAI-Mixer, which comprises a single linear layer, each for visual and lan-
guage features: WGatingIandWGatingLâˆˆRdÃ—3, illustrated in Fig. 6(b). The gat-
ingnetworksaimtooutputthebestcombinationofweightsforthreeexpertmod-
ules for visual and language features each by using a linear layer and a softmax
function as follows: Softmax (xTWGatingx,dim=1 ). Note that xâˆˆRdÃ—Nx, where

--- PAGE 11 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 11
xis either the visual Ior language Lfeatures and Nxis the length of features,
resulting in xTWGatingxâˆˆRNxÃ—3. Then, we split the softmax matrix into three
weight vectors: Softmax (xTWGatingx,dim=1 )â†’[wAUX, wLANG , wSELF]where
each weight has RNxdimension. The weights serve as confidence scores to deter-
mine whether to use information from each expert module. From the outputs of
the gating networks, the propagation flow for the three sources of intelligence:
â€˜AUXâ€™, â€˜IMGâ€™, â€˜LANGâ€™ can be represented as follows:
[wAUX, wLANG , wSELF]â†Softmax (I(l)TWGatingI,dim=1 ),
Ë†I(l)=wAUXâŠ™I(l)
AUX+wLANGâŠ™I(l)
LANG +wSELFâŠ™I(l)
SELF
[wAUX, wIMG, wSELF]â†Softmax (L(l)TWGatingL,dim=1 ),
Ë†L(l)=wAUXâŠ™L(l)
AUX+wIMGâŠ™L(l)
IMG+wSELFâŠ™L(l)
SELF,(5)
where âŠ™represents the element-wise product in each token. The gating networks
for visual and language features are trained independently without parameter
sharing, ensuring that both gating networks blend the three intelligence with
different weights. In this manner, MoAI-Mixer and gating networks facilitate
the interaction among the three sources of intelligence.
4 Experiments
Implementation Details. To ensure successful reproducibility, we outline
three crucial technical details of MoAI: (a) external CV models, (b) MoAI-
Compressor and MoAI-Mixer, (c) training and inference details.
(a)For panoptic segmentation, we adopt Mask2Former [14] (model size: 106M)
with Swin-B/4 [67]. To predict a panoptic segmentation map, we set the thresh-
old to keep predicted instance masks as 0.5and set the mask threshold to use the
masks as 0.95. For open-world object detection, we use OWLv2 [70] (model size:
154M) with CLIP-B/16 [75]. To achieve open-world object detection, we deal
with 1847 object categories combining those in ADE20K-847 [94,95] and Ima-
geNet [20]. We set the threshold to keep object detection predictions as 0.1and
set the object threshold to use them as 0.5. For scene graph generation (SGG),
we utilize panoptic SGG [88] (model size: 44M) with ResNet-50 [30] to conduct
flexible interactions with foreground and background objects, where 0.8thresh-
old to use SGG predicates is set. For OCR, we use PaddleOCRv2 [23] (model
size: 18M), one of performant open-source OCR frameworks, where we set rec-
ognizable languages to Chinese & English and set hyper-parameter settings to
possibly read rotated text descriptions. The combined size of the external CV
models is about 332M, contributing a little to the total model size.

--- PAGE 12 ---
12 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
Table 1: Evaluating zero-shot performances of
 MoAIon nine vision language
datasets compared with the current powerful VLMs on Q-Bench [87], SQA-IMG [35],
TextVQA [81], POPE [58], MME(-P, -C) [28], MM-Bench(-CN) [66], and MM-Vet [91].
VLMs Q-Bench SQA-IMG TextVQA POPE MME-P MME-C MM-Bench MMB-CN MM-Vet
BLIP2-13B [56] - 61.0 42.5 85.3 1294 290 - - 22.4
InstructBLIP-7B [19] 56.7 60.5 50.1 - - - 36.0 23.7 26.2
InstructBLIP-13B [19] - 63.1 50.7 78.91213 - - - 25.6
Shikra-13B [11] 54.7 - - - - - 58.8 - -
IDEFICS-9B [46] - - 25.9 - - - 48.2 25.2 -
IDEFICS-80B [46] - - 30.9 - - - 54.5 38.1 -
Qwen-VL-7B [4] 59.4 67.1 63.8 - - - 38.2 7.4 -
Qwen-VL-Chat-7B [4] - 68.2 61.5 -1488 361 60.6 56.7 -
MiniGPT-4-7B [97] - - - - 582 - 23.0 - 22.1
Otter-7B [54] 47.2 - - - 1292 - 48.3 - 24.6
LLaVA-7B [65] - 38.5 - - 807 248 34.1 14.1 26.7
MiniGPT-v2-7B [10] - - - - - - - - -
MiniGPT-v2-Chat-7B [10] - - - - - - - - -
LLaVA1.5-7B [63] 58.7 66.8 58.2 85.91511 294 64.3 58.3 30.5
LLaVA1.5-13B [63] 62.1 71.6 61.3 85.91531 295 67.7 63.6 35.4
mPLUG-Owl-7B [89] 58.9 - - - 967 - 46.6 - -
mPLUG-Owl2-7B [90] 62.9 68.7 58.2 1450 - 64.5 - 36.2
ShareGPT4V-7B [13] 63.4 68.4 - 1567 376 68.8 62.2 37.6
CogVLM-17B [85] - 68.7 58.2 - - 65.8 55.9 54.5
LLaVA-XTuner-20B [18] - - - - - - 75.1 73.7 37.2
Intern-XC-7B [92] 64.4 - - 1528 391 74.4 72.4 35.2
MoAI-7B 70.2 83.5 67.8 87.1 1714 561 79.3 76.5 43.7
(b)In MoAI-Compressor, the learnable tokens AinputhaveR4096Ã—64dimension
where 64denotes the number of tokens (length) and 4096represents the channel
dimension dfor MLM input. In addition, MoAI-Compressor comprises 4stan-
dard Transformer encoder layers [84]. In the self-attention, 4number of heads
and64head dimension are set. To build MoAI-Mixer, we equip it with specific
MLM layer indices l= 7,15,23,31. For CA/SA expert modules, 64reduced
dimension, 4number of heads, and 4096/4 = 1024 head dimension are used.
(c)For all training steps, we deal with a standard visual instruction tuning
dataset: LLaVA-Instruct-665K [63] filtered by [13]. Regarding the first training
step, we train the learnable tokens Ainput, the parameters of MoAI-Compressor,
and six expert modules of MoAI-Mixer in one epoch using the AdamW [69]
optimizer, scheduled by cosine annealing [68] from learning rate of 1e-4 to 1e-6.
In the second training step, we not only learn the parameters trained in the
first training step but also the gating networks, where learning rate is scheduled
from 2e-5 to 1e-6 in one epoch. For efficient inference, we quantize MoAI in 4-bit
where double quantization and normalized float 4-bit (nf4) [21] are used, and we
use deterministic beam search ( n= 3) [27] for text generation.
Evaluating Visual Perception Capability. Delving into validating the ef-
fectiveness of MoAI, we look deeper into visual perception capability related
to real-world scene understanding in numerous VL benchmarks, such as MME,
SEED, MM-Bench, and MM-Vet. Fig. 2 illustrates the zero-shot performances in
detail of MoAI and three state-of-the-art open-source LLVMs such as Instruct-
BLIP [19], Qwen-VL [4], LLaVA1.5 [63]. For each VL benchmark, there exist
specific dimensions (sub-benchmarks) related to real-world scene understand-
ing in which MoAI aims to demonstrate its efficacy. Refer to Appendix A for

--- PAGE 13 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 13
Table 2: Illustrating the effectiveness of external computer vision (CV) models com-
pared by the perception scores in MME [28] and MM-Bench [66]. â€˜TTâ€™ denotes text
translation task that requires OCR as a priority.
MME MM-Bench
PS+OWOD SGG OCR Existence Position Scene OCR TT Recognition Localization Spatial OCR
âœ— âœ“ âœ“ 187 154 161 145 138 77.6 54.0 32.6 84.6
âœ“ âœ— âœ“ 198 145 164 147 150 89.7 65.3 35.8 90.9
âœ“ âœ“ âœ— 199 163 166 120 95 91.8 69.2 42.8 80.1
âœ“ âœ“ âœ“ 200 165 170 148 153 92.9 71.1 43.2 93.5
Table 3: Ablation study for training step choice, selecting top- kexpert modules in
MoAI-Mixer, and the type of weights for gating network.
(a) Training step choice
Step MME-P MME-C
First 1542 369
Second 1654 511
Combined 1714 561(b) Selecting Top- kExperts
kMME-P MME-C
1 1588 387
2 1638 451
3 1714 561(c) Gating network weights
Gating MME-P MME-C
Random 1520 348
Uniform 1617 485
Trained 1714 561
more details on what each dimension specifically indicates. As it can be seen
from Fig. 2, MoAI significantly surpasses other LLVMs, demonstrating the ef-
fectiveness of utilizing auxiliary visual information from external CV models. It
is noteworthy that MoAI especially excels at relation and text-related dimen-
sions,emphasizingthesignificanceofusingauxiliaryvisualinformationthatthey
struggle to fully comprehend. Refer to Appendix D for qualitative assessment
with demonstration on a few samples. Furthermore, Tab. 1 exhibits thorough
evaluation across numerous renowned VL benchmarks, and demonstrates the
exceptional performance of MoAI. The versatility of MoAI corroborates that
enhancing real-world scene understanding can boost not only visual perception
related to it but also overall VL capabilities in Fig. 1(b).
Ablation Studies. To validate the effectiveness of the external CV models we
utilize, we conduct evaluation by subtracting them one by one. Tab. 2 shows
significant drop of object existence and recognition without using panoptic seg-
mentation (PS) and open-world object detection (OWOD). On the other hand,
once SGG is not used, the scores related with relations such as Position and
Spatial are dropped in Tab. 2. In addition, the OCR scores are also dropped if
OCR is not employed. Therefore, we can say that each of the external CV models
is crucial for real-world scene understanding based on the perception scores for
MME, SEED, MM-Bench, and MM-Vet. Additionally, we control three factors
of MoAI-Mixer and gating networks in Tab. 3: (a) the two training steps, (b)
selecting top- kin expert modules, and (c) weights of gating networks, in order
to validate their effectiveness. Note that, selecting top- kis based on the weight
magnitude among wAUX,wIMG,wLANG, and wSELF, as higher weights signify
greater importance of the associated information. Further, Appendix B shows
additional experiments for ablation studies.

--- PAGE 14 ---
14 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
Number of Parameters (B)MoAI
(a) MME/POPE/MMB/SEED by Scale
MoAI
MoAI
MoAILLaVA1.6LLaVA1.6
LLaVA1.6LLaVA1.6LLaVA1.6LLaVA1.6
LLaVA1.6
LLaVA1.6GPT-4V
Gemini -ProQwen -VL-PlusScore (MME) / Accuracy (%) (Others)Closed -Source LLVMs
(b) POPE/ HallusionBenchAccuracy ( Hallusion )/F1-Score (Others) 
InstructBLIP
Qwen -VL
LLaVA1.5
MoAI
Fig. 7:Illustrating zero-shot vision language performances (a) by model size scale
compared with the larger open-source LLVMs: LLaVA1.6-13B and -34B [64], in the
latest, and closed-source LLVMs. (b) shows the results of POPE [58] and Hallusion-
Bench [62], where â€˜Adversarialâ€™, â€˜Randomâ€™, and â€˜Popularâ€™ are metrics in POPE. Note
that, the scores of MME in (a) are scaled down by 1/25 times to fit the figure, and the
dot points for closed-source LLVMs represent averaged performances with them.
Discussion. From the results, we can obtain an insight that prioritizing real-
world scene understanding is more crucial than relying on the extra curation of
visual instruction datasets or scaling up model size. As illustrated in Fig. 7(a),
MoAI-7B surpasses the zero-shot performances, despite being relatively small
compared to the considerably larger open-source and closed-source models. No-
tably, Fig. 7(b) also indicates that MoAI performs well even on hallucination
zero-shot datasets: POPE [58] and HallusionBench [62]. This suggests that ac-
curately recognizing objects and their relationships can help prevent LLVMs
from making mistakes. Looking ahead, as MoAI is tailored for real-world scene
understanding, we plan to incorporate more external CV models to provide
LLVMs with diverse capabilities for low-level vision understanding, common-
sense knowledge, and awareness of non-object notions beyond text descriptions,
such as charts, diagrams, signs, and symbols, as well as solving advanced math
problems. Furthermore, robust [41,47,50,53], unbiased [43,51,61], and explain-
able [9,39,40] CV models can be applied to achieve precise and unbiased outputs
for vision language tasks. Further, Appendix C describes further discussion.
5 Conclusion
To achieve real-world scene understanding, we leverage fundamental perception
capabilitiesrootedincognitivescienceandmachinelearning.Thisinvolvesincor-
porating auxiliary visual information from historically rich external CV models,
which we seemlessly integrate with visual and language features in MLM using
expert modules and gating networks. As a result of these advancements,
 MoAI
demonstrates improved visual perception capabilities, resulting in significant en-
hancements in zero-shot vision language performances. This underscores MoAIâ€™s
potential to advance LLVM modeling by effectively leveraging diverse auxiliary
visual information and integrating multiple forms of intelligence.

--- PAGE 15 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 15
Acknowledgements
This work was partially supported by two funds: IITP grant funded by the Korea
government (MSIT) (RS-2022-II220984) and Center for Applied Research in Ar-
tificial Intelligence (CARAI) grant funded by DAPA and ADD (UD230017TD).
Additionally, it was supported by the KISTI National Supercomputing Cen-
ter with supercomputing resources including technical support (KSC-2024-CRE-
0160).
References
1. Yi-vl-34b, https://www.01.ai/ 2
2. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,
D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv
preprint arXiv:2303.08774 (2023) 5
3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shot learning. Advances in Neural Information Processing Systems 35, 23716â€“
23736 (2022) 8
4. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 (2023) 2, 3, 4, 5, 12
5. Bengio, Y., LÃ©onard, N., Courville, A.: Estimating or propagating gradi-
ents through stochastic neurons for conditional computation. arXiv preprint
arXiv:1308.3432 (2013) 5
6. Biederman,I.,Mezzanotte,R.J.,Rabinowitz,J.C.:Sceneperception:Detectingand
judging objects undergoing relational violations. Cognitive Psychology 14(2), 143â€“
177 (1982). https://doi.org/https://doi.org/10.1016/0010-0285(82)90007-
X,https://www.sciencedirect.com/science/article/pii/001002858290007X 2
7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners. Advances in neural information processing systems 33, 1877â€“1901 (2020)
4
8. Cai,Z.,Cao,M.,Chen,H.,Chen,K.,Chen,K.,Chen,X.,Chen,X.,Chen,Z.,Chen,
Z., Chu, P., et al.: Internlm2 technical report. arXiv preprint arXiv:2403.17297
(2024) 6
9. Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J., Bojanowski, P., Joulin,
A.: Emerging properties in self-supervised vision transformers. In: Proceedings of
the IEEE/CVF international conference on computer vision. pp. 9650â€“9660 (2021)
14
10. Chen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krishnamoorthi, R., Chan-
dra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified
interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478
(2023) 2, 12
11. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ing multimodal llmâ€™s referential dialogue magic. arXiv preprint arXiv:2306.15195
(2023) 12

--- PAGE 16 ---
16 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
12. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao,
Y., Lin, D., et al.: Are we on the right way for evaluating large vision-language
models? arXiv preprint arXiv:2403.20330 (2024) 25
13. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:
Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprint arXiv:2311.12793 (2023) 2, 5, 6, 10, 12
14. Cheng, B., Misra, I., Schwing, A.G., Kirillov, A., Girdhar, R.: Masked-attention
mask transformer for universal image segmentation. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. pp. 1290â€“1299
(2022) 2, 3, 5, 11
15. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-
guage modeling with pathways. Journal of Machine Learning Research 24(240),
1â€“113 (2023) 1, 4
16. Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep
reinforcement learning from human preferences. Advances in neural information
processing systems 30(2017) 6
17. Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,
Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 (2022) 1, 4
18. Contributors,X.:Xtuner:Atoolkitforefficientlyfine-tuningllm. https://github.
com/InternLM/xtuner (2023) 12
19. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:
InstructBLIP: Towards general-purpose vision-language models with instruction
tuning. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023) 2, 3, 4, 5, 12
20. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248â€“255. Ieee (2009) 11
21. Dettmers, T., Pagnoni, A., Holtzman, A., Zettlemoyer, L.: Qlora: Efficient finetun-
ing of quantized llms. arXiv preprint arXiv:2305.14314 (2023) 12
22. Doveh, S., Arbelle, A., Harary, S., Herzig, R., Kim, D., Cascante-Bonilla, P., Al-
fassy, A., Panda, R., Giryes, R., Feris, R., Ullman, S., Karlinsky, L.: Dense and
aligned captions (dac) promote compositional reasoning in vl models. In: Oh, A.,
Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.) Advances
in Neural Information Processing Systems. vol. 36, pp. 76137â€“76150. Curran Asso-
ciates,Inc.(2023), https://proceedings.neurips.cc/paper_files/paper/2023/
file/efe406d6d2674d176cdcd958ce605d17-Paper-Conference.pdf 2, 3
23. Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu,
X., et al.: Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint
arXiv:2109.03144 (2021) 2, 3, 6, 11
24. Eigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep
mixture of experts. arXiv preprint arXiv:1312.4314 (2013) 5
25. Epstein, R.A., Baker, C.I.: Scene perception in the human brain. Annual review of
vision science 5, 373â€“397 (2019) 2
26. Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion param-
eter models with simple and efficient sparsity. The Journal of Machine Learning
Research 23(1), 5232â€“5270 (2022) 5
27. Freitag, M., Al-Onaizan, Y.: Beam search strategies for neural machine translation.
In:Luong,T.,Birch,A.,Neubig,G.,Finch,A.(eds.)ProceedingsoftheFirstWork-
shop on Neural Machine Translation. pp. 56â€“60. Association for Computational

--- PAGE 17 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 17
Linguistics, Vancouver (Aug 2017). https://doi.org/10.18653/v1/W17-3207 ,
https://aclanthology.org/W17-3207 12
28. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,
K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal
large language models. arXiv preprint arXiv:2306.13394 (2023) 3, 12, 13
29. Gong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q., Liu, K., Zhang, W.,
Luo, P., Chen, K.: Multimodal-gpt: A vision and language model for dialogue with
humans. arXiv preprint arXiv:2305.04790 (2023) 2
30. He,K.,Zhang,X.,Ren,S.,Sun,J.:Deepresiduallearningforimagerecognition.In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770â€“778 (2016) 11
31. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415 (2016) 6
32. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation
9(8), 1735â€“1780 (1997) 5
33. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021) 9
34. Iyer, S., Lin, X.V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K.,
Wang, T., Liu, Q., Koura, P.S., et al.: Opt-iml: Scaling language model instruction
meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017
(2022) 4
35. Iyyer, M., Yih, W.t., Chang, M.W.: Search-based neural structured learning for
sequential question answering. In: Proceedings of the 55th Annual Meeting of the
AssociationforComputationalLinguistics(Volume1:LongPapers).pp.1821â€“1831
(2017) 12
36. Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local
experts. Neural computation 3(1), 79â€“87 (1991) 5
37. Jain, J., Li, J., Chiu, M.T., Hassani, A., Orlov, N., Shi, H.: Oneformer: One trans-
former to rule universal image segmentation. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. pp. 2989â€“2998 (2023) 2
38. Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C.,
Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of experts.
arXiv preprint arXiv:2401.0 4088 (2024) 5
39. Kim,J.,Lee,B.K.,Ro,Y.M.:Distillingrobustandnon-robustfeaturesinadversar-
ialexamplesbyinformationbottleneck.AdvancesinNeuralInformationProcessing
Systems 34, 17148â€“17159 (2021) 14
40. Kim, J., Lee, B.K., Ro, Y.M.: Causal unsupervised semantic segmentation. arXiv
preprint arXiv:2310.07379 (2023) 14
41. Kim,J.,Lee,B.K.,Ro,Y.M.:Demystifyingcausalfeaturesonadversarialexamples
and causal inoculation for robust network by adversarial instrumental variable
regression. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 12302â€“12312 (2023) 14
42. Kim, K., Yoon, K., In, Y., Moon, J., Kim, D., Park, C.: Adaptive self-training
framework for fine-grained scene graph generation. In: The Twelfth International
ConferenceonLearningRepresentations(2024), https://openreview.net/forum?
id=WipsLtH77t 2
43. Kim, Y., Kim, J., Lee, B.K., Shin, S., Ro, Y.M.: Mitigating dataset bias in image
captioning through clip confounder-free captioning network. In: 2023 IEEE Inter-
national Conference on Image Processing (ICIP). pp. 1720â€“1724. IEEE (2023) 14

--- PAGE 18 ---
18 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
44. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV). pp. 4015â€“4026 (October 2023) 24
45. Komatsuzaki,A.,Puigcerver,J.,Lee-Thorp,J.,Ruiz,C.R.,Mustafa,B.,Ainslie,J.,
Tay, Y., Dehghani, M., Houlsby, N.: Sparse upcycling: Training mixture-of-experts
from dense checkpoints. arXiv preprint arXiv:2212.05055 (2022) 5
46. LaurenÃ§on, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A.,
Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., et al.: Obelisc: An open
web-scale filtered dataset of interleaved image-text documents. arXiv preprint
arXiv:2306.16527 (2023) 12
47. Lee,B.K.:Trainingencoder-attentionthroughfully-connectedcrfsforefficientend-
to-end lane detection model (2020) 14
48. Lee, B.K., Chung, S., Kim, C.W., Park, B., Ro, Y.M.: Trol: Traversal of layers for
large language and vision models. arXiv preprint arXiv:2406.12246 (2024) 25
49. Lee, B.K., Kim, C.W., Park, B., Ro, Y.M.: Meteor: Mamba-based traversal of
rationale for large language and vision models. arXiv preprint arXiv:2405.15574
(2024) 25
50. Lee, B.K., Kim, J., Ro, Y.M.: Masking adversarial damage: Finding adversarial
saliency for robust and sparse network. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 15126â€“15136 (2022) 14
51. Lee, B.K., Kim, J., Ro, Y.M.: Mitigating adversarial vulnerability through causal
parameter estimation by adversarial double machine learning. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 4499â€“4509
(2023) 14
52. Lee, B.K., Park, B., Kim, C.W., Ro, Y.M.: Collavo: Crayon large language and
vision model. arXiv preprint arXiv:2402.11248 (2024) 2
53. Lee, B.K., Yu, Y., Ro, Y.M.: Towards adversarial robustness of bayesian neural
network through hierarchical variational inference (2020) 14
54. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
12
55. Li, B., Wang, R., Wang, G., Ge, Y., Ge, Y., Shan, Y.: Seed-bench: Benchmarking
multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125
(2023) 3
56. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 12
57. Li, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., Wei, F.:
Trocr: Transformer-based optical character recognition with pre-trained models.
In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp.
13094â€“13102 (2023) 2
58. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object hal-
lucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023)
12, 14
59. Lin, B., Tang, Z., Ye, Y., Cui, J., Zhu, B., Jin, P., Zhang, J., Ning, M., Yuan,
L.: Moe-llava: Mixture of experts for large vision-language models. arXiv preprint
arXiv:2401.15947 (2024) 5
60. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Visionâ€“

--- PAGE 19 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 19
ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V 13. pp. 740â€“755. Springer (2014) 8
61. Liu, B., Wang, D., Yang, X., Zhou, Y., Yao, R., Shao, Z., Zhao, J.: Show, de-
confound and tell: Image captioning with causal inference. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18041â€“
18050 (2022) 14
62. Liu, F., Guan, T., Li, Z., Chen, L., Yacoob, Y., Manocha, D., Zhou, T.: Hallu-
sionbench: You see what you think? or you think what you see? an image-context
reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-
modality models. arXiv preprint arXiv:2310.14566 (2023) 14
63. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.
arXiv preprint arXiv:2310.03744 (2023) 3, 4, 6, 10, 12
64. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: Llava-next: Improved
reasoning, ocr, and world knowledge (January 2024), https://llava-vl.github.
io/blog/2024-01-30-llava-next/ 2, 6, 14
65. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh
Conference on Neural Information Processing Systems (2023) 2, 5, 6, 12
66. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,
He, C., Liu, Z., et al.: Mmbench: Is your multi-modal model an all-around player?
arXiv preprint arXiv:2307.06281 (2023) 3, 12, 13
67. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer:Hierarchicalvisiontransformerusingshiftedwindows.In:Proceedings
of the IEEE/CVF international conference on computer vision. pp. 10012â€“10022
(2021) 11
68. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.
arXiv preprint arXiv:1608.03983 (2016) 12
69. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International
ConferenceonLearningRepresentations(2019), https://openreview.net/forum?
id=Bkg6RiCqY7 12
70. Minderer, M., Gritsenko, A.A., Houlsby, N.: Scaling open-vocabulary object de-
tection. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023), https://openreview.net/forum?id=mQPNcBWjGc 2, 3, 5, 11
71. Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., Houlsby, N.: Multimodal
contrastive learning with limoe: the language-image mixture of experts. Advances
in Neural Information Processing Systems 35, 9564â€“9576 (2022) 3
72. OpenAI: Gpt-4v(ision) system card (2023), https://openai.com/research/gpt-
4v-system-card , Last accessed on 2024-02-13 2, 5
73. OpenAI: Gpt-4v(ision) technical work and authors (2023), https://openai.com/
contributions/gpt-4v , Last accessed on 2024-02-13 2, 5
74. Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,Mishkin,P.,Zhang,C.,
Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow instruc-
tions with human feedback. Advances in Neural Information Processing Systems
35, 27730â€“27744 (2022) 4, 6
75. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable
visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.)
Proceedings of the 38th International Conference on Machine Learning. Proceed-
ings of Machine Learning Research, vol. 139, pp. 8748â€“8763. PMLR (18â€“24 Jul
2021) 6, 11
76. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-
guage understanding by generative pre-training (2018) 4

--- PAGE 20 ---
20 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
77. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language
models are unsupervised multitask learners 4
78. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,
W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research 21(1), 5485â€“5551 (2020)
1, 4
79. Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Su-
sano Pinto, A., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of
experts. Advances in Neural Information Processing Systems 34, 8583â€“8595 (2021)
3
80. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean,
J.: Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. In: International Conference on Learning Representations (2017), https:
//openreview.net/forum?id=B1ckMDqlg 3, 5
81. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh,
D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. pp. 8317â€“8326
(2019) 12
82. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A.,
Amodei, D., Christiano, P.F.: Learning to summarize with human feedback. Ad-
vances in Neural Information Processing Systems 33, 3008â€“3021 (2020) 6
83. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R., Schalk-
wyk,J.,Dai,A.M.,Hauth,A.,etal.:Gemini:afamilyofhighlycapablemultimodal
models. arXiv preprint arXiv:2312.11805 (2023) 2
84. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Å., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30(2017) 5, 9, 12
85. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv
preprint arXiv:2311.03079 (2023) 2, 5, 12
86. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M.,
Le, Q.V.: Finetuned language models are zero-shot learners. In: International Con-
ference on Learning Representations (2022) 1, 4
87. Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan,
Q., Zhai, G., et al.: Q-bench: A benchmark for general-purpose foundation models
on low-level vision. arXiv preprint arXiv:2309.14181 (2023) 12
88. Yang, J., Ang, Y.Z., Guo, Z., Zhou, K., Zhang, W., Liu, Z.: Panoptic scene graph
generation. In: European Conference on Computer Vision. pp. 178â€“196. Springer
(2022) 2, 3, 6, 11
89. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
multimodality. arXiv preprint arXiv:2304.14178 (2023) 12
90. Ye, Q., Xu, H., Ye, J., Yan, M., Liu, H., Qian, Q., Zhang, J., Huang, F., Zhou,
J.: mplug-owl2: Revolutionizing multi-modal large language model with modality
collaboration. arXiv preprint arXiv:2311.04257 (2023) 12
91. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-
vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint
arXiv:2308.02490 (2023) 3, 12
92. Zhang, P., Wang, X.D.B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S.,
Zhang, S., Duan, H., Yan, H., et al.: Internlm-xcomposer: A vision-language large

--- PAGE 21 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 21
model for advanced text-image comprehension and composition. arXiv preprint
arXiv:2309.15112 (2023) 2, 12
93. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab,
M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 (2022) 4
94. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing
throughade20kdataset.In:ProceedingsoftheIEEEconferenceoncomputervision
and pattern recognition. pp. 633â€“641 (2017) 11
95. Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Se-
mantic understanding of scenes through the ade20k dataset. International Journal
of Computer Vision 127, 302â€“321 (2019) 11
96. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A.M., Le, Q.V.,
Laudon, J., et al.: Mixture-of-experts with expert choice routing. Advances in Neu-
ral Information Processing Systems 35, 7103â€“7114 (2022) 3
97. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023) 2, 12
98. Zong, Z., Song, G., Liu, Y.: Detrs with collaborative hybrid assignments training.
In: Proceedings of the IEEE/CVF international conference on computer vision.
pp. 6748â€“6758 (2023) 2
99. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., Fedus,
W.:St-moe:Designingstableandtransferablesparseexpertmodels.arXivpreprint
arXiv:2202.08906 (2022) 5

--- PAGE 22 ---
22 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
A Details for Specific Dimensions in Fig. 2
This section explains details on dimensions (sub-benchmarks or sub-tasks) re-
lated to real-world scene understanding in the following benchmarks: MME,
SEED, MM-Bench, and MM-Vet.
A.1 MME
â€“ Existence pertains to inquiries concerning the presence of a single object
within a specified image.
â€“ Count denotes the process of quantifying instances of the specified object
depicted in an image.
â€“ Position describes the capacity of a model to discern the spatial arrange-
ment between two objects within a given image.
â€“ Scene focuses on the identification of a place shown in an image, including
indoor and outdoor locations such as a gallery, laboratory, lakeside, land-
mark, etc.
â€“ OCR serves to measure the capability of a model to recognize texts in the
image.
â€“ Text translation requires a model, supporting both English and Chinese,
to translate the Chinese script in an image to the corresponding English.
A.2 SEED
â€“ Scene(sceneunderstanding) focusesontheoverallinformationconveyed
in the image, where inquiries can be addressed through a comprehensive
grasp of the imageâ€™s content.
â€“ Identity (instance identity) involves the object recognition capability of
a model, which includes determining the presence or categorization of an
instance.
â€“ Attributes (instance attributes) relates to the distinguishing features
of an instance, such as its color, shape, or material composition, serving to
evaluate the modelâ€™s comprehension of an objectâ€™s visual appearance.
â€“ Location (instance location) concerns the precise spatial coordinates of
a specific instance within the image, necessitating accurate localization of
the referenced object by the model.
â€“ Count(instances counting) pertains to the capability of amodel to count
the occurrences of a designated object within the image, demanding a com-
prehensive understanding and enumeration of instances of the specified ob-
ject.
â€“ Relation (spatial relation) prompts the model to establish the spatial
connection between two specified objects within the image, grounding them
and recognizing their relative spatial arrangement.
â€“ Interaction (instance interaction) requires the model to identify either
the relational state or interactive dynamics between two human entities or
objects depicted in the image.

--- PAGE 23 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 23
â€“ Text recognition (text understanding) involves inquiries concerning
the textual components present within the image, prompting the model to
comprehend and interpret textual elements accordingly.
A.3 MM-Bench
â€“ Attribute (attribute recognition) involve the capability of a model to
recognize various features of an image such as texture, shape, appearance,
emotions, categories, celebrities, renowned locations, objects, and optical
characters within an image.
â€“ Localization (object localization) involves inquiries regarding an ob-
jectâ€™s position, absolute coordinates, count, and orientation within the im-
age.
â€“ OCR showcases a modelâ€™s capability to recognize text, formulas, and sheets
within an image.
â€“ Relation (spatial relationship) assesses the capability of a model to un-
derstand the relative positions between objects depicted in the image.
A.4 MM-Vet
â€“ Recognition evaluates the overall visual recognition proficiency of a model,
encompassing the identification of scenes, objects, object attributes, count-
ing, and various other advanced visual recognition tasks in computer vision.
â€“ Spatial (spatial awareness) encompasses a wide range of abilities related
to spatial comprehension, including understanding the spatial relationships
among objects and textual regions within a scene.
â€“ OCR assesses the capability of a model to understand and reason over scene
text,wherethemodelisevaluatedonitscapabilitytoreadtextwithinimages
and utilize this information to solve various tasks.
B Further Experiments
Auxiliary Information on Other Baselines. In the following table, we
have experimented simple ablation study to identify the effectiveness of auxiliary
information, and then we observed the same increased results.
Dataset Qwen-VL-Chat-7B +Aux LLaVA1.5-7B +Aux
TextVQA 63.8 69.2 58.2 65.6
MME 1849 2192 1805 2187
MM-Bench 60.6 77.5 64.3 79.1

--- PAGE 24 ---
24 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
Over-Complexity. We additionally validated more thorough ablation study in
the following table. From these results, we can say that adding bounding box or
segmentation map directly affects the sub-benchmarks for position, localization,
spatial relation compared existence and recognition. They are hugely related
with fine-grained understanding to objects. From these observations, we would
like to claim that we meticulously selected external computer vision models
and their output verbalization prompts, so all we used is necessary to enhance
perception capabilities in balance. Nonetheless, open-world segmentation might
be a solution to integrating segmentation and open-world detection, but, to the
best of our knowledge, it has complicated structures having larger model sizes.
This is why we considered the two independently in the current version.
Box Seg Exist Pos Recog Loc Spatial
âœ— âœ— 198 155 90.1 55.2 34.5
âœ“ âœ— 199 162 92.2 68.3 39.0
âœ— âœ“ 199 159 91.5 56.8 35.4
âœ“ âœ“ 200 165 92.9 71.1 43.2
Inference Latency. We measured the number of generated tokens per sec-
ond in Qwen-VL (16 tok/s), LLaVA1.5 (22 tok/s), and MoAI (20 tok/s) under
flash attention in equal resource environments: Intel(R) Xeon(R) Gold 6230,
RAM 512GB, and NVIDIA RTX A6000. Despite using external models, their
size is relatively small and generation speed is highly related with multimodal
language model size, so they did not affect inference speed much. This is similar
to SAM [44] decoding process (encoded features are saved).
Number of Classes. Below, we conducted simple ablation study to show con-
sidering total 1847 categories is enough by randomly choosing categories with
20 repetitive iterations.
Datasets 0 100 200 400 800 1200 1847
MME 1814 1955 2064 2172 2270 2275 2275
MM-Bench 65.9 73.5 76.8 79.2 79.2 79.3 79.3
MM-Vet 33.2 38.1 42.0 43.6 43.7 43.7 43.7

--- PAGE 25 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 25
C Further Discussion
Potential Dataset Biases. We evaluated
 MoAI on MMStar [12] to handle
potential dataset biases: some questions may not require visual information for
accurate responses. From this result, we can say that MoAI overcomes the biases
well compared with other baselines.
Models CP FP IR LR ST MA Avg
LLaVA1.5-7B 58.8 24.0 38.8 24.0 13.6 22.8 30.3
Qwen-VL-Chat-7B 59.6 32.0 50.8 29.2 22.0 31.6 37.5
MoAI-7B 68.5 46.3 59.1 42.9 35.7 40.2 48.7
Limited Performances for Computer Vision Models As we all know,
there are no perfect models existing in the world that provide 100% prediction
accuracy. Consequently, inherent limitations such as erroneous predictions and
biases from training datasets are inevitable. Nonetheless, despite their short-
comings, external models have demonstrated a high potential to enhance the
perception capabilities of LLVMs, which previously struggled with these issues.
Future Works. Despite its advanced performances for fundamental percep-
tion capabilities,
 MoAI still requires low-level vision understanding, common-
sense knowledge,charts,diagrams, signs,and symbols,advancedmath problems,
which are all beyond perception [49] with computer vision models. Therefore,
we should extend the literal intelligence derived from additional computer vi-
sion models to encompass multifaceted information that includes fundamental
image understanding, real-world common-sense knowledge, and non-object con-
cepts. This expansion should cover charts, diagrams, symbols, signs, math prob-
lems, and step-by-step procedures for solving complex questions. Besides, we are
struggling with building advanced architectures [48] to embed more rich vision
language knowledge, stimulating the effect of blending numerous intelligence.

--- PAGE 26 ---
26 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
D Demonstrating
 MoAI for Qualitative Assessment
SEED Visual input example, Scene Understanding:
User Which player in the image is bleeding from the face?
A. The hockey player B. The man with the beard
C. The tennis player D. None of them are bleeding
InstructBLIP A. The hockey player
Qwen-VL A. The hockey player
LLaVA1.5 A. The hockey player
MoAI B. The man with the beard
SEED Visual input example, Instance Identity:
User Which of the following is not present in the image?
A. Coral reef B. Palm tree C. Seashell D. Fish
InstructBLIP D. Fish
Qwen-VL D. Fish
LLaVA1.5 D. Fish
MoAI B. Palm Tree

--- PAGE 27 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 27
SEED Visual input example, Instance Attribute:
User What is the action of the person wearing the hat on stage?
A. Playing the guitar B. Singing C. Dancing D. Playing the drums
InstructBLIP A. Playing the guitar
Qwen-VL D. Playing the drums
LLaVA1.5 D. Playing the drums
MoAI A. Playing the guitar
SEED Visual input example, Instance Location:
User Where is the couple in the image located?
A. Behind the car B. In front of the car C. Inside
the car D. On top of the car
InstructBLIP C. Inside the car
Qwen-VL B. In front of the car
LLaVA1.5 D. On top of the car
MoAI C. Inside the car

--- PAGE 28 ---
28 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
SEED Visual input example, Instance Counting:
User How many soccer players are present in the image?
A. 3 B. 2 C. 1 D. 4
InstructBLIP D. 4
Qwen-VL D. 4
LLaVA1.5 D. 4
MoAI A. 3
SEED Visual input example, Spatial Relation:
User What is the position of the crowd in relation to
the person on stage?
A. To the left B. To the right C. Behind D. In
front
InstructBLIP C. Behind
Qwen-VL D. In front
LLaVA1.5 D. In front
MoAI C. Behind

--- PAGE 29 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 29
SEED Visual input example, Instance Interaction:
User What is the person on the bed doing?
A. Sitting up B.Reading C. Watching TV D. Sleeping
InstructBLIP D. Sleeping
Qwen-VL D. Sleeping
LLaVA1.5 D. Sleeping
MoAI C. Watching TV
SEED Visual input example, Text Recognition:
User What color is the text on the sign in the image?
A. Black B. Red C. White D. Blue
InstructBLIP B. Red
Qwen-VL B. Red
LLaVA1.5 B. Red
MoAI C. White

--- PAGE 30 ---
30 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
MM-Vet Visual input example, Recognition & Spatial Awareness:
User On the right desk, what is to the left of the laptop?
InstructBLIP Printer.
Qwen-VL Printer.
LLaVA1.5 There is a lamp to the left of the laptop on the right desk.
MoAI A Lamp.

--- PAGE 31 ---
MoAI: Mixture of All Intelligence for Large Language and Vision Model 31
MM-Vet Visual input example, Recognition & OCR:
User Can you explain this meme?
InstructBLIP No.
Qwen-VL Spongebob and patrick write down what the pro-
fessor says.
LLaVA1.5 The meme features a cartoon of two characters,
one of which is a spongebob-like character, stand-
ing in front of a campfire. The other character is
a professor, who is holding a clipboard and ap-
pears to be writing something. The scene is set in
a camping environment, with a tent in the back-
ground.The meme is captioned with a humorous
message, suggesting that the professor is writing
a paper about the spongebob-like character. The
text reads, Professor: â€œGood morning, freshmenâ€.
MoAI The image is a meme featuring the cartoon char-
acters Spongebob Squarepants and Patrick Star
from the television show Spongebob Squarepants.
Spongebob is holding a telescope and looking
through it, while Patrick is standing next to him
holding a piece of paper. The caption above the
image reads Professor: â€œGood morni...â€ and the
caption below the image reads Freshmen: Write
that down, write that down!. The meme is a hu-
morous take on the idea of students taking notes
in class.

--- PAGE 32 ---
32 B.K. Lee, B. Park, C.W. Kim, and Y.M. Ro
MM-Vet Visual input example, OCR & Spatial Awareness:
User What is in the right most bottle?
InstructBLIP Conditioner.
Qwen-VL Thicker & fuller densifying shampoo.
LLaVA1.5 The right most bottle contains a tinted moisturizer
MoAI The right most bottle contains Thicker & Fuller Mousse.
MM-Vet Visual input example, OCR & Math:
User What is the price difference between diesel and
regular gasoline?
InstructBLIP 0
Qwen-VL 0.99
LLaVA1.5 The price difference between diesel and regular
gasoline is displayed on the gas pump. In the im-
age, the diesel price is $3.60, while the regular
gasoline price is $3.40. This means that diesel is
slightly more expensive than regular gasoline.
MoAI 0.11
