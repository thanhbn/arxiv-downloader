# 2108.02625.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2108.02625.pdf
# Kích thước file: 2087124 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
MSTRE-NET: MÔ HÌNH ÂM THANH ĐA LUỒNG CHO
VIỆC CHUYỂN ĐỔI LỜI BÀI HÁT TỰ ĐỘNG
Emir Demirel
Đại học Queen Mary London
e.demirel@qmul.ac.ukSven Ahlbäck
Doremir Music Research AB
sven.ahlback@doremir.comSimon Dixon
Đại học Queen Mary London
s.e.dixon@qmul.ac.uk
TÓM TẮT
Bài báo này đóng góp nhiều khía cạnh cho nghiên cứu về chuyển đổi lời bài hát tự động (ALT). Đóng góp chính của chúng tôi là một biến thể mới của kiến trúc Mạng Nơ-ron Trễ Thời gian Đa luồng (MTDNN), được gọi là MSTRE-Net, xử lý thông tin thời gian bằng cách sử dụng nhiều luồng song song với độ phân giải khác nhau, giữ cho mạng gọn gàng hơn, và do đó có tốc độ suy luận nhanh hơn và tỷ lệ nhận dạng được cải thiện so với việc có các luồng TDNN giống hệt nhau. Ngoài ra, hai bước tiền xử lý mới trước khi huấn luyện mô hình âm thanh được đề xuất. Đầu tiên, chúng tôi đề xuất sử dụng các bản ghi âm từ cả miền đơn âm và đa âm trong quá trình huấn luyện mô hình âm thanh. Thứ hai, chúng tôi gắn thẻ các bản ghi âm đơn âm và đa âm với các nhãn riêng biệt để phân biệt khoảng lặng không có giọng hát và các trường hợp nhạc cụ trong quá trình căn chỉnh. Hơn nữa, chúng tôi trình bày một bộ test mới với kích thước lớn hơn đáng kể và tính đa dạng âm nhạc cao hơn so với các tập dữ liệu hiện có được sử dụng trong tài liệu ALT, đồng thời duy trì sự cân bằng giới tính của các ca sĩ. Mô hình hoạt động tốt nhất của chúng tôi thiết lập trạng thái nghệ thuật mới trong chuyển đổi lời bài hát với biên độ lớn. Để tái tạo được, chúng tôi công khai chia sẻ các định danh để lấy dữ liệu được sử dụng trong bài báo này.

1. GIỚI THIỆU
Các nghiên cứu thực nghiệm cho thấy rằng việc nhận dạng những từ được hát là một nhiệm vụ khó khăn ngay cả đối với người nghe, và điều này khó khăn hơn so với lời nói, do một số yếu tố liên quan đến hiệu suất, môi trường và người nghe [1]. Do đó, việc tự động lấy các từ được hát thông qua máy nghe, tức là chuyển đổi lời bài hát tự động (ALT), có thể có tác động trong việc giảm bớt một số quy trình tốn thời gian liên quan đến sáng tác nhạc, phụ đề âm thanh/video/bản nhạc và chỉnh sửa, căn chỉnh lời bài hát, tạo danh mục âm nhạc, v.v. Bất chấp tiềm năng của nó, trạng thái hiện tại của việc chuyển đổi lời bài hát vẫn còn khá xa so với việc đủ mạnh mẽ để được tận dụng trong các ứng dụng như vậy.

Với những tiến bộ gần đây trong nghiên cứu nhận dạng giọng nói tự động (ASR) và việc thích ứng thành công với dữ liệu ca hát, những cải tiến đáng kể đã được báo cáo trong nghiên cứu ALT [2-4]. Ngoài ra, các tập dữ liệu mới được phát hành đã đẩy nhanh sự phát triển của lĩnh vực nghiên cứu [5, 6]. Thông qua những cải tiến này, triển vọng áp dụng ALT trong ngành công nghiệp âm nhạc đã trở nên thực tế hơn, với giả định rằng tiến bộ tiếp tục. Mặc dù các kết quả đầy hứa hẹn đã được thu được cho các bản ghi âm a cappella [2, 4, 7], tỷ lệ nhận dạng giảm đáng kể khi có sự hiện diện của nhạc cụ đệm [8, 9].

Từ góc độ ASR, nhạc cụ đệm có thể được coi là nhiễu vì các tín hiệu âm nhạc không có giọng hát thường chứa ít hoặc không có thông tin liên quan đến việc chuyển đổi lời bài hát, trong khi sự hiện diện của chúng trong miền phổ tăng sự nhầm lẫn trong quá trình dự đoán. Để xây dựng các mô hình âm thanh mạnh mẽ hơn chống lại môi trường nhiễu, phương pháp đa luồng trong ASR đã được giới thiệu [10], được truyền cảm hứng từ cách các tín hiệu âm thanh được chia thành nhiều dải tần số và xử lý song song trong hệ thống thính giác của con người [11]. Trong khi nghiên cứu trước đây đề xuất sử dụng xử lý đặc trưng đa độ phân giải [12, 13] hoặc tái tạo biểu diễn tiềm ẩn đa dải thông qua bộ mã hóa tự động [14] để đạt được ASR đa luồng, kiến trúc mạng nơ-ron được giới thiệu gần đây trong [15], Mạng Nơ-ron Trễ Thời gian Đa luồng (MTDNN), đề xuất một giải pháp đơn giản hóa được sử dụng để tạo ra trạng thái nghệ thuật cho ASR dựa trên hybrid / Mạng Nơ-ron Sâu - Mô hình Markov Ẩn (DNN-HMM) [16, 17]. Trong công trình này, chúng tôi đề xuất một biến thể gọn gàng của MTDNN, được gọi là MSTRE-Net, trong đó các luồng được đa dạng hóa bằng cách có số lượng lớp khác nhau với mục tiêu giảm số lượng tham số có thể huấn luyện (tức là độ phức tạp mô hình), và do đó thời gian suy luận và cải thiện tỷ lệ nhận dạng từ.

Ngoài ra, chúng tôi đề xuất một số đóng góp mới khác để cải thiện hiệu suất chuyển đổi lời bài hát. Chúng tôi đề xuất huấn luyện kết hợp mô hình âm thanh trên cả bản ghi âm đơn âm (ví dụ: DAMP-Sing! 300x30x2 [6]) và đa âm (ví dụ: DALI [5]), được chỉ ra là cải thiện hiệu suất cho cả hai trường hợp. Hơn nữa, chúng tôi đề xuất gắn thẻ các phát ngôn đơn âm và đa âm với các token âm nhạc và lặng riêng biệt một cách rõ ràng. Mục tiêu của chúng tôi cho điều này là tạo ra các căn chỉnh mạnh mẽ hơn chống lại sự gián đoạn trong đường dẫn giải mã, có thể do nhạc cụ đệm gây ra trong các khung hình không có giọng hát.

Một thách thức lớn trong nghiên cứu ALT là xuất bản các kết quả có thể tái tạo, do thiếu dữ liệu đánh giá có sẵn công khai [18]. Dabike và Barker [2] đã chia sẻ các chú thích được xác minh thủ công cho một tập con của DAMP đã được sử dụng để đánh giá trong ca hát a cappella [4, 7]. Tập dữ liệu Jamendo (lời bài hát) [19] bao gồm 20 bản ghi âm nhạc đa âm đương đại được phát hành dưới giấy phép mã nguồn mở. Hơn nữa, bất chấp bản chất hạn chế của chúng về mặt kích thước và tính đa dạng âm nhạc, Hansen [20] và Mauch [21] đã thuộc về hai bộ đánh giá được sử dụng phổ biến nhất cho ALT. Ngoài những điều này, chúng tôi trình bày một bộ test mới với 240 bản ghi âm đa âm có phạm vi ngày phát hành rộng hơn và sự cân bằng giới tính ca sĩ tốt hơn để thiết lập một đánh giá chuyển đổi lời bài hát toàn diện hơn.

Phần còn lại của bài báo được cấu trúc như sau: chúng tôi bắt đầu với một bản tóm tắt về các khái niệm cần thiết trong phương pháp hiện đại cho hybrid-ASR. Phần tiếp theo giải thích cách kiến trúc MTDNN được đề xuất được xây dựng. Tiếp theo, chúng tôi đưa ra chi tiết về dữ liệu được sử dụng trong các thí nghiệm, và giới thiệu một bộ đánh giá mới. Cuối cùng, chúng tôi mô tả thiết lập thí nghiệm và trình bày kết quả xác minh các lựa chọn thiết kế của chúng tôi thông qua các bài test loại bỏ.

2. BỐI CẢNH
ALT có thể được coi là tương tự với Nhận dạng Giọng nói Liên tục Từ vựng Lớn (LVCSR) cho giọng hát. Tương tự, mục tiêu cho ALT là dự đoán chuỗi từ có khả năng nhất, w, cho một luồng quan sát âm thanh, O, có thể được biểu diễn bằng thuật ngữ toán học như sau:

bw = argmax P(w|O)
w
= argmax P(w)p(O|w)
w
= argmax P(w)∑ p(O|Q)P(Q|w);    (1)
w         Q

trong đó các phần tử của Q đại diện cho các lớp âm vị¹.

Trong Phương trình 1, p(O|Q) được thu được thông qua mô hình âm thanh. Âm vị được chuyển đổi thành nhãn từ bằng cách sử dụng từ điển xác định ánh xạ giữa từ và biểu diễn âm vị của chúng. Các posterior từ thô sau đó được làm mịn với mô hình ngôn ngữ, P(w) để thu được bản chuyển đổi đầu ra hợp lý hơn về mặt ngữ pháp, bw.

Theo cách tiếp cận xác suất của ASR, âm vị được biểu diễn bằng HMM trong đó chuyển đổi giữa các trạng thái âm liên kết xảy ra ở mỗi bước thời gian [22]. Trong hệ thống của chúng tôi, chúng tôi sử dụng bộ công cụ Kaldi [23], một khung ASR mã nguồn mở đại diện cho các trạng thái HMM sử dụng Bộ chuyển đổi Trạng thái Hữu hạn Có trọng số (WFST) [24]. Trong hoạt động, một đồ thị WFST được tạo ra thông qua việc kết hợp các posterior được lấy từ các mô hình âm thanh, ngôn ngữ và phát âm. Các đường dẫn hướng kết quả của các trạng thái tạo thành một lưới, một cấu trúc đồ thị có trọng số không chu trình có thể biểu diễn nhiều giả thuyết đầu ra.

¹Âm vị là đơn vị âm thanh cơ bản của lời nói. Trong ngôn ngữ học, từ được coi là được tạo thành từ các chuỗi âm vị.

Hình 1. ASR dựa trên DNN-HMM trong hoạt động

2.1 Thông tin Tương hỗ Tối đa Tự do Lưới
Hầu hết các hệ thống ALT gần đây sử dụng khung DNN-HMM hybrid hiện đại trong đó các mạng nơ-ron được huấn luyện theo cách phân biệt chuỗi [25]. Cụ thể hơn, các bộ chuyển đổi lời bài hát hoạt động tốt nhất cho đến nay [2-4, 7, 8] sử dụng huấn luyện Thông tin Tương hỗ Tối đa Tự do Lưới (LF-MMI) [26], trong đó các tham số mạng được điều chỉnh theo mục tiêu MMI:

F_MMI = ∑ log p(O_u|Q_u)^λ P(W_u) / ∑ p(O_u|Q)^λ P(W)    (2)
        u                                W

trong đó p(O_u|Q_u) là xác suất quan sát một thể hiện âm thanh O trong phát ngôn u, trong trạng thái âm Markov Q_u, và P(W)'s là xác suất chuỗi từ [27]. Tối ưu hóa theo MMI nhằm tối đa hóa thông tin chia sẻ giữa chuỗi tham chiếu và chuỗi đích. Cụ thể hơn, các thuật ngữ trong tử số được tính toán cho mỗi phát ngôn, trong khi mẫu số được tính toán trên toàn bộ tập huấn luyện. Do đó, các tham số mạng được cập nhật để tối đa hóa xác suất trong tử số và tối thiểu hóa mẫu số. Nói cách khác, mục tiêu của huấn luyện MMI là phân biệt một quan sát âm thanh cụ thể với phát ngôn đã cho của nó.

3. MẠNG NƠ-RON TRỄ THỜI GIAN ĐA LUỒNG
Phần thân chính của kiến trúc MTDNN bao gồm nhiều luồng của các lớp TDNN được huấn luyện song song, trong đó mỗi luồng có tốc độ giãn thời gian duy nhất (τ). Biến thể MTDNN được đề xuất của chúng tôi khác với các mô hình gốc [16,17] bằng cách có số lượng lớp khác nhau trong các luồng TDNN, tùy thuộc vào τ (Hình 2(b) và 2(c)).

Trước các luồng TDNN, các đặc trưng đầu vào x được xử lý trước bởi một Mạng Nơ-ron Tích chập 2-D (CNN) đơn luồng ở front-end của mạng,

h = Stacked-2D-CNN(x)    (3)

--- TRANG 2 ---
căn chỉnh mạnh mẽ hơn chống lại sự gián đoạn trong đường dẫn giải mã, có thể do nhạc cụ đệm gây ra trong các khung hình không có giọng hát.

Một thách thức lớn trong nghiên cứu ALT là xuất bản các kết quả có thể tái tạo, do thiếu dữ liệu đánh giá có sẵn công khai [18]. Dabike và Barker [2] đã chia sẻ các chú thích được xác minh thủ công cho một tập con của DAMP đã được sử dụng để đánh giá trong ca hát a cappella [4, 7]. Tập dữ liệu Jamendo (lời bài hát) [19] bao gồm 20 bản ghi âm nhạc đa âm đương đại được phát hành dưới giấy phép mã nguồn mở. Hơn nữa, bất chấp bản chất hạn chế của chúng về mặt kích thước và tính đa dạng âm nhạc, Hansen [20] và Mauch [21] đã thuộc về hai bộ đánh giá được sử dụng phổ biến nhất cho ALT. Ngoài những điều này, chúng tôi trình bày một bộ test mới với 240 bản ghi âm đa âm có phạm vi ngày phát hành rộng hơn và sự cân bằng giới tính ca sĩ tốt hơn để thiết lập một đánh giá chuyển đổi lời bài hát toàn diện hơn.

Phần còn lại của bài báo được cấu trúc như sau: chúng tôi bắt đầu với một bản tóm tắt về các khái niệm cần thiết trong phương pháp hiện đại cho hybrid-ASR. Phần tiếp theo giải thích cách kiến trúc MTDNN được đề xuất được xây dựng. Tiếp theo, chúng tôi đưa ra chi tiết về dữ liệu được sử dụng trong các thí nghiệm, và giới thiệu một bộ đánh giá mới. Cuối cùng, chúng tôi mô tả thiết lập thí nghiệm và trình bày kết quả xác minh các lựa chọn thiết kế của chúng tôi thông qua các bài test loại bỏ.

2. BỐI CẢNH
ALT có thể được coi là tương tự với Nhận dạng Giọng nói Liên tục Từ vựng Lớn (LVCSR) cho giọng hát. Tương tự, mục tiêu cho ALT là dự đoán chuỗi từ có khả năng nhất, w, cho một luồng quan sát âm thanh, O, có thể được biểu diễn bằng thuật ngữ toán học như sau:

bw = argmax P(w|O)
w
= argmax P(w)p(O|w)
w
= argmax P(w)∑ p(O|Q)P(Q|w);    (1)
w         Q

trong đó các phần tử của Q đại diện cho các lớp âm vị¹.

Trong Phương trình 1, p(O|Q) được thu được thông qua mô hình âm thanh. Âm vị được chuyển đổi thành nhãn từ bằng cách sử dụng từ điển xác định ánh xạ giữa từ và biểu diễn âm vị của chúng. Các posterior từ thô sau đó được làm mịn với mô hình ngôn ngữ, P(w) để thu được bản chuyển đổi đầu ra hợp lý hơn về mặt ngữ pháp, bw.

Theo cách tiếp cận xác suất của ASR, âm vị được biểu diễn bằng HMM trong đó chuyển đổi giữa các trạng thái âm liên kết xảy ra ở mỗi bước thời gian [22]. Trong hệ thống của chúng tôi, chúng tôi sử dụng bộ công cụ Kaldi [23], một khung ASR mã nguồn mở đại diện cho các trạng thái HMM sử dụng Bộ chuyển đổi Trạng thái Hữu hạn Có trọng số (WFST) [24]. Trong hoạt động, một đồ thị WFST được tạo ra thông qua việc kết hợp các posterior được lấy từ các mô hình âm thanh, ngôn ngữ và phát âm. Các đường dẫn hướng kết quả của các trạng thái tạo thành một lưới, một cấu trúc đồ thị có trọng số không chu trình có thể biểu diễn nhiều giả thuyết đầu ra.

Hình 1. ASR dựa trên DNN-HMM trong hoạt động

2.1 Thông tin Tương hỗ Tối đa Tự do Lưới
Hầu hết các hệ thống ALT gần đây sử dụng khung DNN-HMM hybrid hiện đại trong đó các mạng nơ-ron được huấn luyện theo cách phân biệt chuỗi [25]. Cụ thể hơn, các bộ chuyển đổi lời bài hát hoạt động tốt nhất cho đến nay [2-4, 7, 8] sử dụng huấn luyện Thông tin Tương hỗ Tối đa Tự do Lưới (LF-MMI) [26], trong đó các tham số mạng được điều chỉnh theo mục tiêu MMI:

F_MMI = ∑ log p(O_u|Q_u)^λ P(W_u) / ∑ p(O_u|Q)^λ P(W)    (2)
        u                                W

trong đó p(O_u|Q_u) là xác suất quan sát một thể hiện âm thanh O trong phát ngôn u, trong trạng thái âm Markov Q_u, và P(W)'s là xác suất chuỗi từ [27]. Tối ưu hóa theo MMI nhằm tối đa hóa thông tin chia sẻ giữa chuỗi tham chiếu và chuỗi đích. Cụ thể hơn, các thuật ngữ trong tử số được tính toán cho mỗi phát ngôn, trong khi mẫu số được tính toán trên toàn bộ tập huấn luyện. Do đó, các tham số mạng được cập nhật để tối đa hóa xác suất trong tử số và tối thiểu hóa mẫu số. Nói cách khác, mục tiêu của huấn luyện MMI là phân biệt một quan sát âm thanh cụ thể với phát ngôn đã cho của nó.

3. MẠNG NƠ-RON TRỄ THỜI GIAN ĐA LUỒNG
Phần thân chính của kiến trúc MTDNN bao gồm nhiều luồng của các lớp TDNN được huấn luyện song song, trong đó mỗi luồng có tốc độ giãn thời gian duy nhất (τ). Biến thể MTDNN được đề xuất của chúng tôi khác với các mô hình gốc [16,17] bằng cách có số lượng lớp khác nhau trong các luồng TDNN, tùy thuộc vào τ (Hình 2(b) và 2(c)).

Trước các luồng TDNN, các đặc trưng đầu vào x được xử lý trước bởi một Mạng Nơ-ron Tích chập 2-D (CNN) đơn luồng ở front-end của mạng,

h = Stacked-2D-CNN(x)    (3)

--- TRANG 3 ---
(a) TDNN đơn luồng (τ = 3)
(b) MTDNN với các luồng giống hệt nhau (τ = {3,6,9})
(c) MTDNN với các luồng khác biệt (τ = {3,6,9})

Hình 2. Các biến thể khác nhau của kiến trúc TDNN. Từ trái sang phải, các khối màu cam, vàng và xanh lam đại diện cho CNN 2-D front-end, các luồng TDNN và các lớp FC cuối cùng được đi trước bởi lớp softmax màu tím

trong đó Stacked-2D-CNN đại diện cho stack của các lớp tích chập 2-D với kernel 3×3. Được truyền cảm hứng từ [4], chúng tôi áp dụng subsampling trên trục chiều cao sau mỗi lớp xen kẽ với hệ số 2, để có được các embedding gọn gàng, h, sau đó được đưa vào nhiều luồng TDNN². Mỗi luồng TDNN có tốc độ giãn thời gian duy nhất, τ, mã hóa thông tin ở các độ phân giải thời gian khác nhau,

z_t^n = Stacked-TDNN(h|τ=t;N=n);    (4)

trong đó z^N là các biến tiềm ẩn tại đầu ra của lớp TDNN thứ N (cuối cùng), và t ∈ Z. Chúng được nối và chiếu đến lớp phân loại (softmax) bởi một cặp lớp kết nối đầy đủ (FC),

a^(s) = softmax(2FC(Concat(z_1^N, z_2^N, ..., z_K^N)));    (5)

trong đó a^(s) là kích hoạt của lớp softmax tương ứng với trạng thái âm vị s và K là số lượng luồng TDNN. Chúng tôi quyết định số lượng lớp trên mỗi luồng theo trường tiếp nhận (RF) của các nút tại lớp TDNN trên cùng,

RF_{z^N} = τ × l × N;    (6)

trong đó l là độ dài khung của các vector đặc trưng âm thanh. Lưu ý rằng chúng tôi bao gồm một lớp tích chập 1-D bổ sung ngay trước các luồng TDNN. Lớp này không sử dụng giãn nở, để không bỏ qua bất kỳ khung hình nào.

4. DỮ LIỆU
4.1 Tập Huấn luyện
Các mô hình âm thanh của các hệ thống ALT được trình bày trước đây trong tài liệu được xây dựng trên bản ghi âm nhạc đơn âm hoặc đa âm. Nói chung, các mô hình đơn âm được huấn luyện trên tập dữ liệu DAMP_{train} [2, 4, 7], trong khi DALI được sử dụng cho các mô hình đa âm [9]. Chúng tôi hợp nhất hai tập dữ liệu này, khai thác kích thước và tính đa dạng âm nhạc của chúng. Chúng tôi tuyển chọn tập con đa âm của tập dữ liệu từ các bản ghi âm từ phiên bản mới nhất (v2.0) của tập dữ liệu DALI [30], và chỉ bao gồm những bài hát mà các liên kết Youtube có sẵn và vẫn đang được sử dụng tại thời điểm lấy âm thanh.

4.2 Bộ Đánh giá
Chúng tôi thực hiện lựa chọn mô hình và tối ưu hóa trên các tập con của tập dữ liệu DAMP và DALI, đại diện cho miền đơn âm và đa âm tương ứng. Đối với DAMP_{test}, chúng tôi sử dụng phân chia test được giới thiệu trong [2]. Để kiểm tra hiệu suất chuyển đổi lời bài hát trên bản ghi âm đa âm, chúng tôi tuyển chọn một tập con mới của DALI-v1.0, mà chúng tôi đưa ra quy trình lựa chọn dữ liệu bên dưới.

Cuối cùng, chúng tôi đánh giá mô hình hoạt động tốt nhất của chúng tôi trên ba tập dữ liệu chuẩn được sử dụng trong tài liệu, cụ thể là các bộ Jamendo, Hansen và Mauch và cung cấp so sánh với trạng thái nghệ thuật trong Phần 6.5.

[Bảng 1 thống kê về các tập dữ liệu được sử dụng trong thí nghiệm]

Để điều chỉnh các siêu tham số trong quá trình đánh giá, hệ số tỷ lệ mô hình ngôn ngữ và penalty chèn từ, chúng tôi đã sử dụng sự kết hợp của dữ liệu từ phân chia DAMP_{dev} [2] và 20 bản ghi âm từ DALI-v2.0³.

4.2.1 Bộ DALI-test
Trong phần này, chúng tôi đưa ra chi tiết về quy trình tuyển chọn cho bộ DALI_{test}. Chúng tôi bắt đầu từ tập con được trình bày trong [31], ban đầu có 513 bản ghi âm và lọc nó theo một số tiêu chí. Nhiều mẫu âm thanh không thể lấy được từ các liên kết được cung cấp. Chúng tôi có được các liên kết Youtube thông qua tìm kiếm tự động bằng cách sử dụng các từ khóa có liên quan. Chúng tôi loại bỏ các bài hát mà phiên bản được lấy tự động là biểu diễn trực tiếp, có chất lượng âm thanh thấp hoặc chứa các phần giọng nói nền bổ sung không liên quan đến lời bài hát tương ứng. Để đảm bảo tính nhất quán và đánh giá công bằng, chúng tôi không bao gồm các bài hát mà ngôn ngữ chính không phải là tiếng Anh. Chúng tôi cho phép một nghệ sĩ có tối đa 5 bài hát. Trong số các bản ghi âm còn lại, chúng tôi thủ công lựa chọn một tập con có phân phối tương đối cân bằng về giới tính ca sĩ, ngày phát hành chính thức qua các thập kỷ (xem Hình 3) và tính đa dạng về phong cách ca hát, hiệu ứng giọng hát và thể loại nhạc. Lời bài hát ban đầu được lấy từ các chú thích được cung cấp trong [5] và được xác minh thủ công theo các bước được giải thích trong Phần 5.1. Phiên bản cuối cùng của DALI_{test} bao gồm 240 bản ghi âm, thiết lập bộ test lớn nhất cho chuyển đổi lời bài hát với các chú thích sạch. Vì khoa học mở, chúng tôi công khai chia sẻ các định danh dữ liệu, chú thích lời bài hát đã được làm sạch và hướng dẫn để lấy các liên kết Youtube tương ứng tại "https://github.com/emirdemirel/DALI-TestSet4ALT".

Hình 3. Số bài hát mỗi thập kỷ trong các bộ đánh giá ALT

5. THIẾT LẬP THÍ NGHIỆM
5.1 Tiền xử lý Lời bài hát
Trước khi được sử dụng để huấn luyện, dữ liệu lời bài hát thô được lấy tự động từ các nguồn trực tuyến (như trong DALI) cần được chuẩn hóa, vì các quy tắc chuyển đổi được áp dụng bởi các nhà cung cấp lời bài hát không được tiêu chuẩn hóa. Chúng tôi loại bỏ tất cả các ký tự ASCII đặc biệt trừ dấu nháy đơn. Chúng tôi chuyển đổi các ký tự số thành tương ứng chữ cái của chúng. Tất cả văn bản được chuyển đổi thành chữ hoa. Chúng tôi quan sát thấy một số mẫu với việc gạch nối sai, âm tiết hóa rõ ràng và lặp lại các chữ cái (có thể chỉ ra các âm tiết hoặc nguyên âm được phát âm dài hơn). Để đối phó với những điều này, chúng tôi áp dụng sửa lỗi gạch nối tự động và chuẩn hóa bằng cách sử dụng các công cụ NLTK mã nguồn mở tiêu chuẩn⁴. Lời bài hát đầu ra sau đó được xác minh và sửa chữa thủ công.

5.2 Mô hình Ngôn ngữ và Phát âm
Lời bài hát thường chứa các từ không phổ biến có khả năng không tồn tại trong từ điển phát âm tiêu chuẩn. Đối với những từ như vậy không có trong từ vựng, hoặc từ ngoài từ vựng (OOV), chúng tôi tạo ra cách phát âm bằng cách sử dụng bộ chuyển đổi grapheme-to-phoneme (G2P) được huấn luyện trước [32]. Để tạo ra kết quả công bằng và có thể tái tạo, chúng tôi tạo ra cách phát âm cho các từ OOV trong các bộ đánh giá cũng như không bỏ qua chúng trong quá trình đánh giá. Chúng tôi sử dụng Từ điển Phát âm tiếng Anh CMU được sử dụng phổ biến⁵ làm từ vựng và tạo ra các cách phát âm thay thế bằng cách nhân đôi các âm vị nguyên âm cho mỗi cách phát âm từ, được truyền cảm hứng từ những cải tiến quan sát thấy trong [7, 33]. Một mô hình ngôn ngữ (LM) 4-gram được xây dựng bằng cách sử dụng SRILM Toolkit [34]. Chúng tôi sử dụng sự kết hợp của corpus lời bài hát trong [2]⁶ và DALI_{train}. Để đánh giá khoa học, chúng tôi loại trừ bất kỳ bài hát nào chồng chéo với những bài trong các bộ đánh giá.

5.3 Phân biệt Các Vùng Nhạc cụ và Lặng
Khung ASR hybrid DNN-HMM tiếp cận nhiệm vụ nhận dạng từ liên tục về cơ bản như một vấn đề giải mã chuỗi. Trong phạm vi này, sự hiện diện của nhạc cụ đệm, đặc biệt là trong các vùng không có giọng hát, có thể làm gián đoạn đường dẫn giải mã, có thể gây ra lỗi tích lũy trong quá trình chuyển đổi và căn chỉnh. Theo truyền thống, các vùng không có giọng nói được biểu diễn bằng token lặng đặc biệt trong tập lớp đích trong quá trình nhận dạng. Ở đây, chúng tôi đề xuất sử dụng các token riêng biệt cho các trường hợp không có giọng hát trong bản ghi âm đơn âm và đa âm. Trước khi huấn luyện, chúng tôi liên kết các token này với các trường hợp lặng/nhạc tương ứng của chúng bằng cách thêm rõ ràng các thẻ ở đầu và cuối của lời bài hát chuẩn của mỗi phát ngôn (xem Bảng 2). Các thẻ này được biểu diễn như từ trong từ vựng trong đó cách phát âm của chúng tương ứng với token lặng liên quan.

[Bảng 2. Gắn thẻ âm nhạc / lặng theo tập dữ liệu]

Để gắn thẻ lặng và nhạc, chúng tôi khai thác dữ liệu huấn luyện của mình. Khi chúng tôi biết các bản ghi âm trong DAMP và DALI tương ứng là đơn âm và đa âm, chúng tôi áp dụng gắn thẻ theo tập dữ liệu. Cách phát âm của các thẻ pseudo-word này được biểu diễn bằng các âm vị khác biệt trong từ vựng.

5.4 Tạo Căn chỉnh Âm vị
Vì tối ưu hóa mạng nơ-ron được thực hiện theo posterior âm vị (như được giải thích trong Phần 2.1), chúng tôi cần trích xuất thời gian của chúng, tức là căn chỉnh. Để tạo ra những điều này, chúng tôi huấn luyện một mô hình triphone Gaussian Mixture Model (GMM) - HMM trên các đặc trưng "thích ứng ca sĩ" [35], theo công thức Kaldi tiêu chuẩn⁷. Ở giai đoạn này, chúng tôi tính toán xác suất phát âm theo từ theo các bước trong [36], và huấn luyện lại một mô hình triphone khác sử dụng bộ chuyển đổi từ vựng được cập nhật. Sử dụng mô hình mới này, chúng tôi áp dụng căn chỉnh cưỡng bức [22] trên dữ liệu huấn luyện để tạo ra căn chỉnh âm vị và từ.

5.5 Huấn luyện Mạng Nơ-ron
Huấn luyện DNN dựa trên công thức Kaldi-chain. Trong không gian đặc trưng, chúng tôi sử dụng các đặc trưng filterbank 40-band được trích xuất với kích thước bước 10ms và kích thước cửa sổ 30ms. Để đạt được huấn luyện thích ứng ca sĩ, chúng tôi sử dụng i-Vectors [37] biểu diễn thông tin nhận dạng ca sĩ thông qua các vector embedding toàn cục. Subsampling khung được áp dụng với hệ số 3 trong sơ đồ huấn luyện này trong đó mỗi khung subsampled trong đầu vào của mạng nơ-ron được coi là đại diện cho l = 3 × 10ms = 30ms ngữ cảnh. Dữ liệu được đưa vào mạng như các đoạn âm thanh 4.2 giây (140 khung) trong minibatch 32. Chúng tôi áp dụng tốc độ học tập giảm dần với tốc độ bắt đầu và cuối tương ứng là 10⁻⁴ và 10⁻⁵. Stochastic gradient descent được sử dụng làm bộ tối ưu. Huấn luyện được thực hiện trong 6 epoch.

6. KẾT QUẢ
Chúng tôi báo cáo kết quả chuyển đổi lời bài hát dựa trên tỷ lệ lỗi từ (WER). Chúng tôi bắt đầu bằng cách so sánh hiệu suất thu được khi sử dụng DAMP và/hoặc DALI trong việc huấn luyện mô hình âm thanh. Sau đó chúng tôi kiểm tra ý tưởng được đề xuất của chúng tôi về việc phân biệt các trường hợp lặng và đệm bằng cách sử dụng các token riêng biệt, và thực hiện các thí nghiệm kiểm tra các cấu trúc liên kết khác nhau của kiến trúc MTDNN. Để tăng hiệu suất hơn nữa, chúng tôi huấn luyện một mô hình cuối cùng trên dữ liệu được tăng cường và cung cấp so sánh kết quả của chúng tôi với các mô hình đã được xuất bản trước đây.

6.1 Huấn luyện Đa miền
Theo Bảng 3, mô hình được huấn luyện trên DAMP_{train} hoạt động tương đối tốt trên DAMP_{test}, tuy nhiên hiệu suất của nó giảm mạnh trên bản ghi âm đa âm. Mặt khác, tỷ lệ nhận dạng tốt hơn nhiều được quan sát thấy trên DALI_{test} khi sử dụng mô hình đa âm, nhưng sau đó mô hình đa âm hoạt động kém trên bản ghi âm a cappella. Cuối cùng, việc sử dụng bản ghi âm từ cả miền đơn âm và đa âm dẫn đến hiệu suất được cải thiện trên cả bộ test đa âm và đơn âm, mặc dù sự cải thiện là nhỏ trên bộ DAMP_{test} đơn âm.

6.2 Mô hình Âm nhạc / Lặng
Tiếp theo, chúng tôi kiểm tra liệu việc gắn thẻ âm nhạc/lặng rõ ràng có cải thiện kết quả chuyển đổi hay không. Ở giai đoạn này, chúng tôi sử dụng kiến trúc đơn luồng (M_{single}^8 trong Bảng 4). Gắn thẻ chỉ được áp dụng trong việc xây dựng mô hình GMM-HMM và để tạo ra căn chỉnh. Các thẻ âm nhạc/lặng đã được loại bỏ trong quá trình huấn luyện mạng nơ-ron. Bảng 3 cho thấy việc căn chỉnh với thẻ âm nhạc/lặng đã thực sự dẫn đến kết quả nhận dạng được cải thiện đáng kể cho bản ghi âm đa âm, nhưng không có sự cải thiện rõ ràng cho trường hợp đơn âm.

[Bảng 3. Kết quả huấn luyện đa miền và gắn thẻ âm nhạc/lặng]

6.3 Thiết kế Kiến trúc Mạng Nơ-ron
Ở đây, chúng tôi kiểm tra các tham số hóa khác nhau của kiến trúc đa luồng. Ở giai đoạn này, chúng tôi không sử dụng gắn thẻ âm nhạc và lặng rõ ràng để huấn luyện các mô hình. Như đã đề cập trong Phần 3, chúng tôi đa dạng hóa mỗi luồng TDNN về số lượng lớp ẩn và/hoặc kích thước của chúng. Ngoài việc đạt được hiệu suất được cải thiện, mục tiêu của những sửa đổi này là khai thác ngữ cảnh thời gian đến mức tối đa. Để làm điều này, chúng tôi tính toán số lượng lớp TDNN được bao gồm theo RF_{z^N} kết quả.

Trong tất cả các biến thể MTDNN được kiểm tra, chúng tôi sử dụng 3 luồng TDNN với τ ∈ {3,6,9}. Chúng tôi bắt đầu bằng việc tìm số lượng lớp TDNN tối ưu cho luồng có τ nhỏ nhất. Để thử nghiệm nhanh chóng, chúng tôi sử dụng các mô hình TDNN đơn luồng (M_{single}^N trong Bảng 4). Theo Bảng 4, việc sử dụng 9 lớp thiết lập cài đặt tối ưu cho τ = 3, có RF_{z^N=9} = 3 × 6 × 9 = 1620 ms. Lưu ý rằng việc tăng thêm số lượng lớp TDNN lên 10 (RF_{z^{10}} = 3 × 6 × 10 = 1800 ms) không dẫn đến nhận dạng được cải thiện, và độ phức tạp mô hình cao hơn nhiều (Hình 4). Do đó, chúng tôi chọn làm baseline một mô hình đơn luồng với 9 lớp TDNN.

[Bảng 4. Thí nghiệm về thiết kế NN]

Tiếp theo, chúng tôi thực hiện các bài test loại bỏ trên bốn biến thể của kiến trúc đa luồng (ký hiệu là M_{multi}^{9,{a,b,c,d}}). Mô hình M_{multi}^{9,a} có các cấu trúc TDNN giống hệt nhau (trừ τ), trong khi các biến thể M_{multi}^{9,{b,c}} có giảm N hoặc kích thước ẩn tương ứng theo τ ở mỗi luồng. Cả hai chiều của việc giảm mô hình đều được áp dụng trên M_{multi}^{9,d}. Trong các mô hình M_{multi}^{9,{b,d}}, chúng tôi giảm số lượng lớp, N cho các luồng có τ lớn hơn để giữ RF_{z^N} tương tự trên tất cả các luồng. M_{multi}^{9,{b,d}} có 4 và 3 lớp tại các luồng với τ = 6 và τ = 9 có giá trị RF tương ứng là 1440 và 1620ms. Mặt khác, việc thêm một lớp nữa trên các luồng với τ = 6,9 sẽ dẫn đến có RF_{z^N} ≈ 1800ms được chỉ ra ở trên là không tối ưu trong trường hợp đơn luồng (xem kết quả cho M_{single}^{10}).

--- TRANG 4 ---
mẫu âm thanh không thể lấy được từ các liên kết được cung cấp. Chúng tôi có được các liên kết Youtube thông qua tìm kiếm tự động bằng cách sử dụng các từ khóa có liên quan. Chúng tôi loại bỏ các bài hát mà phiên bản được lấy tự động là biểu diễn trực tiếp, có chất lượng âm thanh thấp hoặc chứa các phần giọng nói nền bổ sung không liên quan đến lời bài hát tương ứng. Để đảm bảo tính nhất quán và đánh giá công bằng, chúng tôi không bao gồm các bài hát mà ngôn ngữ chính không phải là tiếng Anh. Chúng tôi cho phép một nghệ sĩ có tối đa 5 bài hát. Trong số các bản ghi âm còn lại, chúng tôi thủ công lựa chọn một tập con có phân phối tương đối cân bằng về giới tính ca sĩ, ngày phát hành chính thức qua các thập kỷ (xem Hình 3) và tính đa dạng về phong cách ca hát, hiệu ứng giọng hát và thể loại nhạc. Lời bài hát ban đầu được lấy từ các chú thích được cung cấp trong [5] và được xác minh thủ công theo các bước được giải thích trong Phần 5.1. Phiên bản cuối cùng của DALI_{test} bao gồm 240 bản ghi âm, thiết lập bộ test lớn nhất cho chuyển đổi lời bài hát với các chú thích sạch. Vì khoa học mở, chúng tôi công khai chia sẻ các định danh dữ liệu, chú thích lời bài hát đã được làm sạch và hướng dẫn để lấy các liên kết Youtube tương ứng tại "https://github.com/emirdemirel/DALI-TestSet4ALT".

Hình 3. Số bài hát mỗi thập kỷ trong các bộ đánh giá ALT

5. THIẾT LẬP THÍ NGHIỆM
5.1 Tiền xử lý Lời bài hát
Trước khi được sử dụng để huấn luyện, dữ liệu lời bài hát thô được lấy tự động từ các nguồn trực tuyến (như trong DALI) cần được chuẩn hóa, vì các quy tắc chuyển đổi được áp dụng bởi các nhà cung cấp lời bài hát không được tiêu chuẩn hóa. Chúng tôi loại bỏ tất cả các ký tự ASCII đặc biệt trừ dấu nháy đơn. Chúng tôi chuyển đổi các ký tự số thành tương ứng chữ cái của chúng. Tất cả văn bản được chuyển đổi thành chữ hoa. Chúng tôi quan sát thấy một số mẫu với việc gạch nối sai, âm tiết hóa rõ ràng và lặp lại các chữ cái (có thể chỉ ra các âm tiết hoặc nguyên âm được phát âm dài hơn). Để đối phó với những điều này, chúng tôi áp dụng sửa lỗi gạch nối tự động và chuẩn hóa bằng cách sử dụng các công cụ NLTK mã nguồn mở tiêu chuẩn⁴. Lời bài hát đầu ra sau đó được xác minh và sửa chữa thủ công.

5.2 Mô hình Ngôn ngữ và Phát âm
Lời bài hát thường chứa các từ không phổ biến có khả năng không tồn tại trong từ điển phát âm tiêu chuẩn. Đối với những từ như vậy không có trong từ vựng, hoặc từ ngoài từ vựng (OOV), chúng tôi tạo ra cách phát âm bằng cách sử dụng bộ chuyển đổi grapheme-to-phoneme (G2P) được huấn luyện trước [32]. Để tạo ra kết quả công bằng và có thể tái tạo, chúng tôi tạo ra cách phát âm cho các từ OOV trong các bộ đánh giá cũng như không bỏ qua chúng trong quá trình đánh giá. Chúng tôi sử dụng Từ điển Phát âm tiếng Anh CMU được sử dụng phổ biến⁵ làm từ vựng và tạo ra các cách phát âm thay thế bằng cách nhân đôi các âm vị nguyên âm cho mỗi cách phát âm từ, được truyền cảm hứng từ những cải tiến quan sát thấy trong [7, 33]. Một mô hình ngôn ngữ (LM) 4-gram được xây dựng bằng cách sử dụng SRILM Toolkit [34]. Chúng tôi sử dụng sự kết hợp của corpus lời bài hát trong [2]⁶ và DALI_{train}. Để đánh giá khoa học, chúng tôi loại trừ bất kỳ bài hát nào chồng chéo với những bài trong các bộ đánh giá.

5.3 Phân biệt Các Vùng Nhạc cụ và Lặng
Khung ASR hybrid DNN-HMM tiếp cận nhiệm vụ nhận dạng từ liên tục về cơ bản như một vấn đề giải mã chuỗi. Trong phạm vi này, sự hiện diện của nhạc cụ đệm, đặc biệt là trong các vùng không có giọng hát, có thể làm gián đoạn đường dẫn giải mã, có thể gây ra lỗi tích lũy trong quá trình chuyển đổi và căn chỉnh. Theo truyền thống, các vùng không có giọng nói được biểu diễn bằng token lặng đặc biệt trong tập lớp đích trong quá trình nhận dạng. Ở đây, chúng tôi đề xuất sử dụng các token riêng biệt cho các trường hợp không có giọng hát trong bản ghi âm đơn âm và đa âm. Trước khi huấn luyện, chúng tôi liên kết các token này với các trường hợp lặng/nhạc tương ứng của chúng bằng cách thêm rõ ràng các thẻ ở đầu và cuối của lời bài hát chuẩn của mỗi phát ngôn (xem Bảng 2). Các thẻ này được biểu diễn như từ trong từ vựng trong đó cách phát âm của chúng tương ứng với token lặng liên quan.

[Bảng 2. Gắn thẻ âm nhạc / lặng theo tập dữ liệu]

Để gắn thẻ lặng và nhạc, chúng tôi khai thác dữ liệu huấn luyện của mình. Khi chúng tôi biết các bản ghi âm trong DAMP và DALI tương ứng là đơn âm và đa âm, chúng tôi áp dụng gắn thẻ theo tập dữ liệu. Cách phát âm của các thẻ pseudo-word này được biểu diễn bằng các âm vị khác biệt trong từ vựng.

5.4 Tạo Căn chỉnh Âm vị
Vì tối ưu hóa mạng nơ-ron được thực hiện theo posterior âm vị (như được giải thích trong Phần 2.1), chúng tôi cần trích xuất thời gian của chúng, tức là căn chỉnh. Để tạo ra những điều này, chúng tôi huấn luyện một mô hình triphone Gaussian Mixture Model (GMM) - HMM trên các đặc trưng "thích ứng ca sĩ" [35], theo công thức Kaldi tiêu chuẩn⁷. Ở giai đoạn này, chúng tôi tính toán xác suất phát âm theo từ theo các bước trong [36], và huấn luyện lại một mô hình triphone khác sử dụng bộ chuyển đổi từ vựng được cập nhật. Sử dụng mô hình mới này, chúng tôi áp dụng căn chỉnh cưỡng bức [22] trên dữ liệu huấn luyện để tạo ra căn chỉnh âm vị và từ.

5.5 Huấn luyện Mạng Nơ-ron
Huấn luyện DNN dựa trên công thức Kaldi-chain. Trong không gian đặc trưng, chúng tôi sử dụng các đặc trưng filterbank 40-band được trích xuất với kích thước bước 10ms và kích thước cửa sổ 30ms. Để đạt được huấn luyện thích ứng ca sĩ, chúng tôi sử dụng i-Vectors [37] biểu diễn thông tin nhận dạng ca sĩ thông qua các vector embedding toàn cục. Subsampling khung được áp dụng với hệ số 3 trong sơ đồ huấn luyện này trong đó mỗi khung subsampled trong đầu vào của mạng nơ-ron được coi là đại diện cho l = 3 × 10ms = 30ms ngữ cảnh. Dữ liệu được đưa vào mạng như các đoạn âm thanh 4.2 giây (140 khung) trong minibatch 32. Chúng tôi áp dụng tốc độ học tập giảm dần với tốc độ bắt đầu và cuối tương ứng là 10⁻⁴ và 10⁻⁵. Stochastic gradient descent được sử dụng làm bộ tối ưu. Huấn luyện được thực hiện trong 6 epoch.

⁴Các bước này có thể đặc trưng cho ngôn ngữ cụ thể.
⁵Liên kết: https://github.com/Alexir/CMUdict/blob/master/cmudict-0.7b.
⁶Corpus này chứa lời bài hát từ tất cả các bài hát của các nghệ sĩ có trong bảng xếp hạng Billboard từ 2015-2018, cộng với lời bài hát trong DAMP_train.
⁷Chúng tôi thực hiện pipeline GMM-HMM tại https://github.com/emirdemirel/ALTA, gần như cùng quy trình với công thức librispeech tiêu chuẩn, với các siêu tham số được điều chỉnh cho dữ liệu ca hát.

--- TRANG 5 ---
từ vựng. Sử dụng mô hình mới này, chúng tôi áp dụng căn chỉnh cưỡng bức [22] trên dữ liệu huấn luyện để tạo ra căn chỉnh âm vị và từ.

5.5 Huấn luyện Mạng Nơ-ron
Huấn luyện DNN dựa trên công thức Kaldi-chain. Trong không gian đặc trưng, chúng tôi sử dụng các đặc trưng filterbank 40-band được trích xuất với kích thước bước 10ms và kích thước cửa sổ 30ms. Để đạt được huấn luyện thích ứng ca sĩ, chúng tôi sử dụng i-Vectors [37] biểu diễn thông tin nhận dạng ca sĩ thông qua các vector embedding toàn cục. Subsampling khung được áp dụng với hệ số 3 trong sơ đồ huấn luyện này trong đó mỗi khung subsampled trong đầu vào của mạng nơ-ron được coi là đại diện cho l = 3 × 10ms = 30ms ngữ cảnh. Dữ liệu được đưa vào mạng như các đoạn âm thanh 4.2 giây (140 khung) trong minibatch 32. Chúng tôi áp dụng tốc độ học tập giảm dần với tốc độ bắt đầu và cuối tương ứng là 10⁻⁴ và 10⁻⁵. Stochastic gradient descent được sử dụng làm bộ tối ưu. Huấn luyện được thực hiện trong 6 epoch.

6. KẾT QUẢ
Chúng tôi báo cáo kết quả chuyển đổi lời bài hát dựa trên tỷ lệ lỗi từ (WER). Chúng tôi bắt đầu bằng cách so sánh hiệu suất thu được khi sử dụng DAMP và/hoặc DALI trong việc huấn luyện mô hình âm thanh. Sau đó chúng tôi kiểm tra ý tưởng được đề xuất của chúng tôi về việc phân biệt các trường hợp lặng và đệm bằng cách sử dụng các token riêng biệt, và thực hiện các thí nghiệm kiểm tra các cấu trúc liên kết khác nhau của kiến trúc MTDNN. Để tăng hiệu suất hơn nữa, chúng tôi huấn luyện một mô hình cuối cùng trên dữ liệu được tăng cường và cung cấp so sánh kết quả của chúng tôi với các mô hình đã được xuất bản trước đây.

6.1 Huấn luyện Đa miền
Theo Bảng 3, mô hình được huấn luyện trên DAMP_{train} hoạt động tương đối tốt trên DAMP_{test}, tuy nhiên hiệu suất của nó giảm mạnh trên bản ghi âm đa âm. Mặt khác, tỷ lệ nhận dạng tốt hơn nhiều được quan sát thấy trên DALI_{test} khi sử dụng mô hình đa âm, nhưng sau đó mô hình đa âm hoạt động kém trên bản ghi âm a cappella. Cuối cùng, việc sử dụng bản ghi âm từ cả miền đơn âm và đa âm dẫn đến hiệu suất được cải thiện trên cả bộ test đa âm và đơn âm, mặc dù sự cải thiện là nhỏ trên bộ DAMP_{test} đơn âm.

6.2 Mô hình Âm nhạc / Lặng
Tiếp theo, chúng tôi kiểm tra liệu việc gắn thẻ âm nhạc/lặng rõ ràng có cải thiện kết quả chuyển đổi hay không. Ở giai đoạn này, chúng tôi sử dụng kiến trúc đơn luồng (M_{single}^8 trong Bảng 4). Gắn thẻ chỉ được áp dụng trong việc xây dựng mô hình GMM-HMM và để tạo ra căn chỉnh. Các thẻ âm nhạc/lặng đã được loại bỏ trong quá trình huấn luyện mạng nơ-ron. Bảng 3 cho thấy việc căn chỉnh với thẻ âm nhạc/lặng đã thực sự dẫn đến kết quả nhận dạng được cải thiện đáng kể cho bản ghi âm đa âm, nhưng không có sự cải thiện rõ ràng cho trường hợp đơn âm.

[Bảng 3. Kết quả huấn luyện đa miền và gắn thẻ âm nhạc/lặng]

6.3 Thiết kế Kiến trúc Mạng Nơ-ron
Ở đây, chúng tôi kiểm tra các tham số hóa khác nhau của kiến trúc đa luồng. Ở giai đoạn này, chúng tôi không sử dụng gắn thẻ âm nhạc và lặng rõ ràng để huấn luyện các mô hình. Như đã đề cập trong Phần 3, chúng tôi đa dạng hóa mỗi luồng TDNN về số lượng lớp ẩn và/hoặc kích thước của chúng. Ngoài việc đạt được hiệu suất được cải thiện, mục tiêu của những sửa đổi này là khai thác ngữ cảnh thời gian đến mức tối đa. Để làm điều này, chúng tôi tính toán số lượng lớp TDNN được bao gồm theo RF_{z^N} kết quả.

Trong tất cả các biến thể MTDNN được kiểm tra, chúng tôi sử dụng 3 luồng TDNN với τ ∈ {3,6,9}. Chúng tôi bắt đầu bằng việc tìm số lượng lớp TDNN tối ưu cho luồng có τ nhỏ nhất. Để thử nghiệm nhanh chóng, chúng tôi sử dụng các mô hình TDNN đơn luồng (M_{single}^N trong Bảng 4). Theo Bảng 4, việc sử dụng 9 lớp thiết lập cài đặt tối ưu cho τ = 3, có RF_{z^N=9} = 3 × 6 × 9 = 1620 ms. Lưu ý rằng việc tăng thêm số lượng lớp TDNN lên 10 (RF_{z^{10}} = 3 × 6 × 10 = 1800 ms) không dẫn đến nhận dạng được cải thiện, và độ phức tạp mô hình cao hơn nhiều (Hình 4). Do đó, chúng tôi chọn làm baseline một mô hình đơn luồng với 9 lớp TDNN.

[Bảng 4. Thí nghiệm về thiết kế NN]

Tiếp theo, chúng tôi thực hiện các bài test loại bỏ trên bốn biến thể của kiến trúc đa luồng (ký hiệu là M_{multi}^{9,{a,b,c,d}}). Mô hình M_{multi}^{9,a} có các cấu trúc TDNN giống hệt nhau (trừ τ), trong khi các biến thể M_{multi}^{9,{b,c}} có giảm N hoặc kích thước ẩn tương ứng theo τ ở mỗi luồng. Cả hai chiều của việc giảm mô hình đều được áp dụng trên M_{multi}^{9,d}. Trong các mô hình M_{multi}^{9,{b,d}}, chúng tôi giảm số lượng lớp, N cho các luồng có τ lớn hơn để giữ RF_{z^N} tương tự trên tất cả các luồng. M_{multi}^{9,{b,d}} có 4 và 3 lớp tại các luồng với τ = 6 và τ = 9 có giá trị RF tương ứng là 1440 và 1620ms. Mặt khác, việc thêm một lớp nữa trên các luồng với τ = 6,9 sẽ dẫn đến có RF_{z^N} ≈ 1800ms được chỉ ra ở trên là không tối ưu trong trường hợp đơn luồng (xem kết quả cho M_{single}^{10}).

--- TRANG 6 ---
6.4 Lựa chọn Mô hình
Các thiết lập đa luồng được đề xuất trừ M_{multi}^{9,c} vượt trội hơn đối tác đơn luồng của chúng, M_{single}^9, đặc biệt là trên DALI_{test}. Kết quả tốt nhất đạt được với M_{multi}^{9,b} có số lượng lớp N duy nhất trên tất cả các luồng với cùng kích thước lớp ẩn.

Để tăng độ tin cậy trong việc lựa chọn mô hình, chúng tôi điều tra các khía cạnh hoạt động khác của các mô hình được kiểm tra. Trong Hình 4, chúng tôi so sánh số lượng tham số có thể huấn luyện là một biến liên quan đến độ phức tạp mô hình, và hệ số thời gian thực (RTF) đo lường tốc độ hoạt động của mô hình trong quá trình suy luận. Chúng tôi tính toán RTF dựa trên thời gian suy luận trên tất cả dữ liệu được sử dụng trong đánh giá. Chúng tôi lặp lại điều này 5 lần và báo cáo trung bình của tất cả các lần lặp cho mỗi mô hình. Các lần lặp này được thực hiện trên CPU Intel®Xeon®Gold 5218R.

Hình 4. Số tham số có thể huấn luyện (trái) & RTF (phải)

Theo Hình 4, mô hình hoạt động tốt nhất của chúng tôi M_{multi}^{9,b} có số lượng tham số có thể huấn luyện lớn thứ hai. Tuy nhiên, độ phức tạp mô hình của nó thấp hơn nhiều so với M_{multi}^{9,a}, kiến trúc được trình bày trong [16]. Về thời gian chạy, tất cả các mô hình đa luồng hoạt động nhanh hơn các mô hình đơn luồng, với M_{multi}^{9,b} nằm trong số những mô hình nhanh nhất. Điều này cho thấy biến thể gọn gàng của chúng tôi có thời gian suy luận giảm với tỷ lệ nhận dạng được cải thiện như đã giả thuyết trong Phần 1.

6.5 So sánh với Trạng thái Nghệ thuật
Ở bước cuối cùng này, chúng tôi huấn luyện một mô hình cuối cùng kết hợp việc căn chỉnh nhận biết âm nhạc/lặng với kiến trúc MTDNN hoạt động tốt nhất, M_{multi}^{9,b}. Để tăng hiệu suất hơn nữa, chúng tôi áp dụng tăng cường dữ liệu thông qua nhiễu tốc độ với các hệ số 0.9 và 1.1, làm tăng gấp ba kích thước của dữ liệu huấn luyện. Trong Bảng 5, chúng tôi so sánh mô hình cuối cùng của chúng tôi với các bộ chuyển đổi lời bài hát khác được báo cáo trong tài liệu. Chúng tôi huấn luyện lại các mô hình âm thanh trong [2] (M_{[2]}), và [4] (M_{[4]}), sử dụng các repository được chia sẻ công khai tương ứng. M_{[9]} dựa trên mô hình âm thanh được huấn luyện trước được chia sẻ tại https://github.com/chitralekha18/AutoLyrixAlign. Cùng một mô hình ngôn ngữ được sử dụng trong việc xây dựng các đồ thị giải mã cho tất cả các mô hình trong Bảng 5. Lưu ý rằng M_{[2], [4]} được huấn luyện trên DAMP (đơn âm) và M_{[9]} được huấn luyện trên tập dữ liệu DALI (đa âm). Chúng tôi sử dụng hệ số tỷ lệ mô hình ngôn ngữ hoạt động tốt nhất khi báo cáo kết quả trong Bảng 5. Chúng tôi không thể tạo ra kết quả trên DALI_{test} sử dụng M_{[9]} do mô hình rất tốn bộ nhớ, như cũng được báo cáo trong [8].

Ngoài những điều này, chúng tôi cung cấp so sánh với trạng thái nghệ thuật. Điểm WER tốt nhất được báo cáo trên DAMP_{test} dựa trên M_{[4]} và áp dụng tái chấm điểm trên các lưới từ được tạo ra sau quá trình giải mã đầu tiên sử dụng RNNLM [38]. Chúng tôi không áp dụng tái chấm điểm RNNLM vì chúng tôi không đạt được sự cải thiện nhất quán trên các bộ test khác nhau theo quan sát thực nghiệm của chúng tôi. Để so sánh công bằng, chúng tôi cũng bao gồm kết quả tốt nhất trong [4] đạt được thông qua n-gram LM (các điểm trong ngoặc đơn trong Bảng 5).

Trên Jamendo, điểm WER tốt nhất được báo cáo trong [8] trong đó suy luận được thực hiện trên giọng hát được tách nguồn. Đối với các tập dữ liệu Hansen và Mauch, kết quả tốt nhất được cung cấp như đã báo cáo trong [9]⁸. Để tránh kết quả lạc quan, chúng tôi đã loại bỏ các bài hát chồng chéo giữa Hansen, Mauch và DALI_{train} trong quá trình huấn luyện mô hình cuối cùng.

[Bảng 5. So sánh với trạng thái nghệ thuật]

Kết quả trên cho thấy MSTRE-Net vượt trội hơn tất cả các mô hình được trình bày trước đây trên các bộ đa âm, với hơn 15%, 7% và 6% cải thiện WER tuyệt đối đạt được trên các tập dữ liệu Jamendo, Hansen và Mauch so với trạng thái nghệ thuật trước đây tương ứng. Đáng chú ý, chúng tôi đạt được ít hơn 50% WER trên bộ DALI_{test} lớn cho thấy hơn một nửa số từ trên 240 bài hát được dự đoán chính xác. Mô hình của chúng tôi cũng có kết quả tốt nhất trên DAMP_{test} đạt được thông qua n-gram LM.

7. KẾT LUẬN
Chúng tôi đã giới thiệu MSTRE-Net, một biến thể gọn gàng mới của kiến trúc mạng nơ-ron đa luồng, vượt trội hơn các mô hình chuyển đổi lời bài hát tự động được đề xuất trước đây. Mô hình của chúng tôi đạt được những kết quả này với độ phức tạp mô hình thấp hơn và thời gian suy luận. Ngoài ra, chúng tôi đã chỉ ra rằng tỷ lệ nhận dạng được cải thiện trên tất cả các bộ đánh giá sau khi tận dụng cả dữ liệu đa âm và đơn âm trong việc huấn luyện mô hình âm thanh. Chúng tôi đề xuất một phương pháp tiền xử lý dữ liệu mới để tạo ra căn chỉnh trước khi huấn luyện mạng nơ-ron dẫn đến tỷ lệ nhận dạng từ tốt hơn đáng kể từ bản ghi âm đa âm so với cách tiếp cận baseline. Cuối cùng, chúng tôi tuyển chọn một bộ đánh giá mới toàn diện và đa dạng hơn, đồng thời có kích thước lớn hơn nhiều so với dữ liệu test trước đây được sử dụng trong nghiên cứu. Để tái tạo và khoa học mở, các định danh và hướng dẫn về việc sử dụng dữ liệu này sẽ được chia sẻ với cộng đồng nghiên cứu.

⁸Lưu ý rằng lý do cho sự khác biệt WER giữa M_{[9]} và điểm được báo cáo trong [9] là do mô hình ngôn ngữ lớn hơn mà chúng tôi sử dụng, mặc dù cả hai mô hình đều có cùng mô hình âm thanh.

--- TRANG 7 ---
Mô hình cuối cùng của chúng tôi vượt trội hơn các kết quả ALT tốt nhất đã được báo cáo trước đây với biên độ lớn, thiết lập trạng thái nghệ thuật mới. Thông qua những kết quả này, chúng tôi đã thực hiện một bước quan trọng trong việc tăng tiềm năng và khả năng để ALT trở thành một công nghệ có thể áp dụng trong cả nghiên cứu Truy xuất Thông tin Âm nhạc và ngành công nghiệp công nghệ âm nhạc.

8. TÀI LIỆU THAM KHẢO
[1] P. A. Fine và J. Ginsborg, "Making myself understood: Các yếu tố được nhận thức ảnh hưởng đến khả năng hiểu được của văn bản ca hát," Frontiers in Psychology, vol. 5, p. 809, 2014.

[2] G. R. Dabike và J. Barker, "Chuyển đổi lời bài hát tự động từ các track giọng hát karaoke: Tài nguyên và hệ thống baseline," trong Interspeech, 2019.

[3] C. Gupta, R. Tong, H. Li, và Y. Wang, "Căn chỉnh lời bài hát và ca hát solo bán giám sát," trong Hội nghị Quốc tế về Truy xuất Thông tin Âm nhạc (ISMIR), 2018.

[4] E. Demirel, S. Ahlbäck, và S. Dixon, "Chuyển đổi lời bài hát tự động sử dụng mạng nơ-ron tích chập giãn nở với self-attention," trong Hội nghị Quốc tế về Mạng Nơ-ron (IJCNN), 2020.

[5] G. Meseguer-Brocal, A. Cohen-Hadria, và G. Peeters, "DALI: Một tập dữ liệu lớn về âm thanh, lời bài hát và nốt nhạc được đồng bộ hóa, được tạo tự động sử dụng mô hình máy học teacher-student," trong Hội nghị Quốc tế về Truy xuất Thông tin Âm nhạc (ISMIR), 2019.

[6] "Tập dữ liệu Smule Sing! 300x30x2," truy cập tháng 4, 2021, https://ccrma.stanford.edu/damp/.

[7] E. Demirel, S. Ahlbäck, và S. Dixon, "Phân tích phát âm tính toán trong phát ngôn ca hát," trong Hội nghị Châu Âu về Xử lý Tín hiệu (EUSIPCO), 2021.

[8] ——, "Căn chỉnh âm thanh-lời bài hát tài nguyên thấp từ bản ghi âm nhạc đa âm," trong Hội nghị Quốc tế IEEE về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP), 2021.

[9] C. Gupta, E. Yılmaz, và H. Li, "Chuyển đổi lời bài hát tự động trong nhạc đa âm: Nhạc nền có giúp ích không?" trong Hội nghị Quốc tế IEEE về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP), 2020.

[10] H. Bourlard và S. Dupont, "Một phương pháp ASR mới dựa trên xử lý độc lập và tái kết hợp các dải tần số một phần," trong Hội nghị Quốc tế lần thứ tư về Xử lý Ngôn ngữ Nói (ICSLP), 1996.

[11] J. Allen, "Con người xử lý và nhận dạng giọng nói như thế nào?" IEEE Transactions on Speech and Audio Processing, vol. 2, no. 4, pp. 567–577, 1994.

[12] H. Hermansky và P. Fousek, "Lọc RASTA đa độ phân giải cho ASR dựa trên TANDEM," trong Interspeech, 2005.

[13] Z. Tüske, R. Schlüter, và H. Ney, "Mô hình âm thanh của sóng giọng nói dựa trên xử lý tín hiệu mạng nơ-ron đa độ phân giải," trong Hội nghị Quốc tế IEEE về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP), 2018.

[14] S. H. Mallidi, T. Ogawa, K. Veselý, P. S. Nidadavolu, và H. Hermansky, "Kết hợp đa luồng dựa trên autoencoder cho nhận dạng giọng nói mạnh mẽ với nhiễu," trong Hội nghị Hàng năm lần thứ mười sáu của Hiệp hội Truyền thông Giọng nói Quốc tế, 2015.

[15] K. J. Han, R. Prieto, và T. Ma, "Nhận dạng giọng nói hiện đại sử dụng multi-stream self-attention với tích chập 1D giãn nở," trong Hội thảo IEEE về Nhận dạng Giọng nói Tự động và Hiểu biết (ASRU), 2019.

[16] K. J. Han, J. Pan, V. K. N. Tadala, T. Ma, và D. Povey, "CNN đa luồng cho mô hình âm thanh mạnh mẽ," trong Interspeech, 2020.

[17] J. Pan, J. Shapiro, J. Wohlwend, K. J. Han, T. Lei, và T. Ma, "ASAPP-ASR: CNN đa luồng và SRU self-attentive cho nhận dạng giọng nói SOTA," trong Interspeech, 2020.

[18] A. M. Kruspe, "Huấn luyện mô hình âm vị cho ca hát với dữ liệu giọng nói đã được 'songiﬁed'." trong Hội nghị Quốc tế về Truy xuất Thông tin Âm nhạc (ISMIR), 2015.

[19] D. Stoller, S. Durand, và S. Ewert, "Căn chỉnh lời bài hát end-to-end cho nhạc đa âm sử dụng mô hình nhận dạng âm thanh-ký tự," trong Hội nghị Quốc tế IEEE về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP), 2019.

[20] J. K. Hansen, "Nhận dạng âm vị trong bản ghi âm a-cappella sử dụng mẫu thời gian và hệ số mel frequency cepstral," trong Hội nghị Sound and Music Computing lần thứ chín (SMC), 2012.

[21] M. Mauch, H. Fujihara, và M. Goto, "Tích hợp thông tin hợp âm bổ sung vào căn chỉnh lời bài hát-âm thanh dựa trên HMM," IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 200–210, 2011.

[22] M. Gales và S. Young, "Ứng dụng của mô hình Markov ẩn trong nhận dạng giọng nói," Foundations and Trends in Signal Processing, 2008.

--- TRANG 8 ---
[23] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, và P. Schwarz, "Bộ công cụ nhận dạng giọng nói Kaldi," trong Hội thảo IEEE về Nhận dạng Giọng nói Tự động và Hiểu biết, 2011.

[24] M. Mohri, F. Pereira, và M. Riley, "Bộ chuyển đổi trạng thái hữu hạn có trọng số trong nhận dạng giọng nói," Computer Speech & Language, 2002.

[25] K. Veselý, A. Ghoshal, L. Burget, và D. Povey, "Huấn luyện phân biệt chuỗi của mạng nơ-ron sâu," trong Interspeech, 2013.

[26] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, và S. Khudanpur, "Mạng nơ-ron được huấn luyện thuần túy theo chuỗi cho ASR dựa trên MMI tự do lưới." trong Interspeech, 2016.

[27] L. Bahl, P. Brown, P. De Souza, và R. Mercer, "Ước lượng thông tin tương hỗ tối đa của các tham số mô hình Markov ẩn cho nhận dạng giọng nói," trong Hội nghị Quốc tế IEEE về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP), 1986.

[28] V. Peddinti, D. Povey, và S. Khudanpur, "Kiến trúc mạng nơ-ron trễ thời gian cho mô hình hóa hiệu quả các ngữ cảnh thời gian dài," trong Hội nghị Hàng năm lần thứ mười sáu của Hiệp hội Truyền thông Giọng nói Quốc tế, 2015.

[29] D. Povey, G. Cheng, Y. Wang, K. Li, H. Xu, M. Yarmohammadi, và S. Khudanpur, "Phân tích nhân tử ma trận hạng thấp bán trực giao cho mạng nơ-ron sâu." trong Interspeech, 2018.

[30] G. Meseguer-Brocal, R. Bittner, S. Durand, và B. Brost, "Làm sạch dữ liệu với học tập đối lập cho chú thích sự kiện nốt giọng hát," trong Hội nghị Quốc tế về Truy xuất Thông tin Âm nhạc (ISMIR), 2020.

[31] A. Vaglio, R. Hennequin, M. Moussallam, G. Richard, và F. d'Alché Buc, "Căn chỉnh lời bài hát-âm thanh đa ngôn ngữ," trong Hội nghị Quốc tế về Truy xuất Thông tin Âm nhạc (ISMIR), 2020.

[32] J. R. Novak, D. Yang, N. Minematsu, và K. Hirose, "Phonetisaurus: Một phoneticizer được điều khiển bởi WFST," trong Hội thảo Quốc tế về Phương pháp Trạng thái Hữu hạn và Xử lý Ngôn ngữ Tự nhiên, 2012.

[33] C. Gupta, H. Li, và Y. Wang, "Đánh giá phát âm tự động của ca hát." trong Interspeech, 2018.

[34] T. Alumäe và M. Kurimo, "Ước lượng hiệu quả của mô hình ngôn ngữ entropy tối đa với các đặc trưng N-gram: Một phần mở rộng SRILM," trong Hội nghị Hàng năm lần thứ mười một của Hiệp hội Truyền thông Giọng nói Quốc tế, 2010.

[35] T. Anastasakos, J. McDonough, R. Schwartz, và J. Makhoul, "Một mô hình gọn gàng cho huấn luyện thích ứng người nói," trong Hội nghị Quốc tế lần thứ tư về Xử lý Ngôn ngữ Nói, 1996.

[36] G. Chen, H. Xu, M. Wu, D. Povey, và S. Khudanpur, "Mô hình hóa xác suất phát âm và lặng cho ASR," trong Hội nghị Hàng năm lần thứ mười sáu của Hiệp hội Truyền thông Giọng nói Quốc tế, 2015.

[37] G. Saon, H. Soltau, D. Nahamoo, và M. Picheny, "Thích ứng người nói của mô hình âm thanh mạng nơ-ron sử dụng i-vectors," trong Hội thảo IEEE về Nhận dạng Giọng nói Tự động và Hiểu biết, 2013.

[38] H. Xu, T. Chen, D. Gao, Y. Wang, K. Li, N. Goel, Y. Carmiel, D. Povey, và S. Khudanpur, "Một thuật toán tái chấm điểm lưới RNNLM được cắt tỉa cho nhận dạng giọng nói tự động," trong Hội nghị Quốc tế về Âm thanh, Giọng nói và Xử lý Tín hiệu (ICASSP). IEEE, 2018.
