# 2309.13876.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.13876.pdf
# Kích thước tệp: 259028 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TÁI TẠO VIỆC HUẤN LUYỆN THEO PHONG CÁCH WHISPER SỬ DỤNG BỘ CÔNG CỤ NGUỒN MỞ VÀ
DỮ LIỆU CÔNG KHAI CÓ SẴN

Yifan Peng1, Jinchuan Tian1, Brian Yan1, Dan Berrebbi1, Xuankai Chang1, Xinjian Li1, Jiatong Shi1,
Siddhant Arora1, William Chen1, Roshan Sharma1, Wangyou Zhang1,2, Yui Sudo3, Muhammad Shakeel3,
Jee-weon Jung1, Soumi Maiti1, Shinji Watanabe1
1Đại học Carnegie Mellon, Hoa Kỳ
2Đại học Giao thông Thượng Hải, Trung Quốc
3Viện Nghiên cứu Honda Nhật Bản, Nhật Bản

TÓM TẮT
Việc tiền huấn luyện các mô hình giọng nói trên khối lượng dữ liệu lớn đã đạt được thành công đáng kể. OpenAI Whisper là một mô hình đa nhiệm vụ đa ngôn ngữ được huấn luyện trên 680k giờ dữ liệu giọng nói có giám sát. Nó tổng quát hóa tốt cho các benchmark nhận dạng giọng nói và dịch thuật khác nhau ngay cả trong thiết lập zero-shot. Tuy nhiên, quy trình hoàn chỉnh để phát triển các mô hình như vậy (từ thu thập dữ liệu đến huấn luyện) không được công khai, điều này khiến các nhà nghiên cứu khó cải thiện hiệu suất của nó và giải quyết các vấn đề liên quan đến huấn luyện như hiệu quả, tính mạnh mẽ, công bằng và thiên vị. Nghiên cứu này trình bày một Mô hình Giọng nói Mở theo phong cách Whisper (OWSM), tái tạo việc huấn luyện theo phong cách Whisper sử dụng bộ công cụ nguồn mở và dữ liệu công khai có sẵn. OWSM thậm chí hỗ trợ nhiều hướng dịch thuật hơn và có thể hiệu quả hơn trong việc huấn luyện. Chúng tôi sẽ công khai phát hành tất cả các script được sử dụng cho việc chuẩn bị dữ liệu, huấn luyện, suy luận và chấm điểm cũng như các mô hình đã được tiền huấn luyện và nhật ký huấn luyện để thúc đẩy khoa học mở.1

Từ khóa chỉ mục — Tiền huấn luyện, whisper, nhận dạng giọng nói, dịch thuật giọng nói

1. GIỚI THIỆU
Các Transformer quy mô lớn [1] đã thu hút sự chú ý đáng kể trong xử lý ngôn ngữ tự nhiên (NLP) [2–7]. Những mô hình này, được huấn luyện trên các tập dữ liệu rộng lớn, đã thể hiện khả năng xuất hiện đáng kể trong các tác vụ downstream đa dạng. Đáng chú ý, việc áp dụng các kỹ thuật tiền huấn luyện tương tự cũng đã tìm thấy thành công trong lĩnh vực xử lý giọng nói. Các kỹ thuật học tự giám sát (SSL) đã chứng minh những thành tích ấn tượng [8–14]. Hơn nữa, học có giám sát quy mô lớn đã nổi lên như một con đường hứa hẹn cho việc phát triển các mô hình giọng nói toàn diện có khả năng thực hiện nhiều tác vụ giọng nói trong một mô hình duy nhất [15–18]. OpenAI Whisper [15] là một loạt các mô hình đa nhiệm vụ đa ngôn ngữ được huấn luyện trên 680k giờ dữ liệu giọng nói có nhãn được tuyển chọn cẩn thận từ các nguồn đa dạng trên Internet.

Mặc dù đã phát hành các mô hình Whisper đã được tiền huấn luyện và mã suy luận, quy trình toàn diện để phát triển mô hình (từ chuẩn bị dữ liệu đến huấn luyện) vẫn không được công khai, điều này đã là tình huống phổ biến đối với các mô hình ngôn ngữ lớn (LLM). Hạn chế này tạo ra một số lo ngại. Thứ nhất, việc sử dụng các mô hình đã được tiền huấn luyện trên các benchmark mới có nguy cơ rò rỉ dữ liệu tiềm ẩn, vì người dùng không được biết về dữ liệu huấn luyện thực tế. Thứ hai, các nhà nghiên cứu gặp khó khăn đáng kể trong việc hiểu các cơ chế cơ bản và làm rõ các phương pháp để nâng cao hiệu suất của mô hình, do họ không có quyền truy cập vào động lực học huấn luyện. Thứ ba, việc không có quyền truy cập vào quy trình phát triển mô hình hoàn chỉnh đặt ra những thách thức đáng kể trong việc giải quyết hiệu quả các vấn đề liên quan đến tính mạnh mẽ, công bằng, thiên vị và độc tính, tất cả đều thường xuyên xuất hiện do dữ liệu và quy trình huấn luyện [19–21].

Gần đây, đã có một nỗ lực phối hợp để thúc đẩy khoa học mở trong lĩnh vực nghiên cứu LLM bằng cách ủng hộ việc phát hành các quy trình huấn luyện hoàn chỉnh [5]. Được truyền cảm hứng từ điều này, chúng tôi trình bày Mô hình Giọng nói Mở theo phong cách Whisper (OWSM)2, tái tạo việc huấn luyện theo phong cách Whisper sử dụng bộ công cụ nguồn mở và dữ liệu công khai có sẵn. OWSM tuân theo thiết kế của Whisper [15] để hỗ trợ các tác vụ thiết yếu như nhận dạng ngôn ngữ (LID), nhận dạng giọng nói tự động đa ngôn ngữ (ASR), và phân đoạn cấp phát ngôn. Đáng chú ý, OWSM cũng thể hiện một số điểm mới về mặt kỹ thuật. Nó được thiết kế để hỗ trợ dịch thuật giọng nói từ bất kỳ ngôn ngữ nào sang bất kỳ ngôn ngữ nào thay vì chỉ dịch từ bất kỳ ngôn ngữ nào sang tiếng Anh (xem Mục 3.4 để biết kết quả). OWSM cũng áp dụng nhiều chiến lược để nâng cao hiệu quả (xem Mục 2.5 để thảo luận).

Chúng tôi sẽ cung cấp các công thức có thể tái tạo bao gồm toàn bộ quy trình, bao gồm chuẩn bị dữ liệu, huấn luyện, suy luận và chấm điểm. Hơn nữa, chúng tôi sẽ phát hành các mô hình đã được tiền huấn luyện và nhật ký huấn luyện, cho phép các nhà nghiên cứu đi sâu vào các chi tiết cụ thể của quy trình huấn luyện và thu được những hiểu biết có giá trị cho các nghiên cứu của riêng họ. Mặc dù OWSM cho thấy hiệu suất cạnh tranh hoặc thậm chí vượt trội so với Whisper trong một số benchmark nhất định, điều quan trọng là làm rõ rằng mục tiêu của chúng tôi không phải là tham gia vào một cuộc cạnh tranh toàn diện với Whisper. Phạm vi nỗ lực của chúng tôi bị hạn chế bởi thực tế là tập dữ liệu lớn nhất của chúng tôi chỉ bao gồm một phần tư tập huấn luyện được sử dụng bởi Whisper, và các hạn chế về tài nguyên của chúng tôi hạn chế chúng tôi không thể tiến hành nhiều lần chạy thử. Thay vào đó, bằng cách chia sẻ những tài nguyên này, chúng tôi nhằm thúc đẩy tính minh bạch và tạo điều kiện cho sự tiến bộ và phát triển trong lĩnh vực tiền huấn luyện quy mô lớn cho xử lý giọng nói.

2. HUẤN LUYỆN THEO PHONG CÁCH WHISPER

2.1. Định dạng dữ liệu đa nhiệm vụ
OpenAI Whisper [15] sử dụng một mô hình sequence-to-sequence duy nhất để thực hiện nhiều tác vụ xử lý giọng nói, bao gồm LID, ASR đa ngôn ngữ, ST từ bất kỳ ngôn ngữ nào sang tiếng Anh, và phân đoạn cấp phát ngôn.

2OWSM được phát âm là "awesome".

--- TRANG 2 ---
[Hình 1: Định dạng dữ liệu đa nhiệm vụ được sử dụng bởi OWSM của chúng tôi, chủ yếu tuân theo OpenAI Whisper [15]. Các tác vụ xử lý giọng nói khác nhau được biểu diễn trong một định dạng thống nhất, có thể được dự đoán bởi một bộ giải mã tự hồi quy. Lưu ý rằng OWSM được thiết kế để hỗ trợ dịch thuật giọng nói-sang-văn bản từ bất kỳ ngôn ngữ nào sang bất kỳ ngôn ngữ nào, trong khi Whisper chỉ có thể thực hiện dịch thuật từ bất kỳ ngôn ngữ nào sang tiếng Anh. Các hộp màu xanh dương biểu thị các token văn bản tiêu chuẩn, trong khi các hộp màu cam và xanh lá là các token đặc biệt. SOP, SOS, và EOS lần lượt đại diện cho start-of-prompt, start-of-sentence, và end-of-sentence.]

OWSM của chúng tôi chủ yếu tuân theo thiết kế này, nhưng mở rộng nó để có thể hỗ trợ ST từ bất kỳ ngôn ngữ nào sang bất kỳ ngôn ngữ nào. Hình 1 minh họa định dạng dữ liệu đa nhiệm vụ. Các mẫu dữ liệu từ các tác vụ khác nhau được biểu diễn trong một định dạng thống nhất, có thể được dự đoán bởi bộ giải mã theo cách tự hồi quy. Cụ thể, mỗi mẫu được chuyển đổi thành một chuỗi các token với hai đoạn được phân tách bởi các token đặc biệt. Đoạn đầu tiên (trước "SOS") là một text prompt tùy chọn được sử dụng như một điều kiện, trong khi đoạn thứ hai là mục tiêu thực tế. Mục tiêu bắt đầu bằng một token đặc biệt biểu thị ngôn ngữ của giọng nói đầu vào. Sau đó, nó sử dụng một token tác vụ để phân biệt giữa ASR và ST. Có một token ST riêng biệt cho mỗi ngôn ngữ đích, điều này cho phép dịch thuật sang bất kỳ ngôn ngữ nào. Cuối cùng, nó nối thêm việc phiên âm văn bản có hoặc không có timestamp cấp phát ngôn. Tất cả timestamp được lượng tử hóa và biểu diễn dưới dạng các token đặc biệt.

2.2. Chuẩn bị dữ liệu
Whisper được tiền huấn luyện trên 680k giờ dữ liệu âm thanh có nhãn được lấy từ Internet, không được công khai. Để xây dựng một tập dữ liệu giọng nói cho việc học có giám sát quy mô lớn, chúng tôi kết hợp các tập huấn luyện từ nhiều corpus ASR và ST công khai có sẵn. Những corpus đa dạng này bao gồm một phạm vi rộng các phong cách nói, môi trường ghi âm và ngôn ngữ. Dữ liệu của chúng tôi được chuẩn bị sử dụng bộ công cụ nguồn mở, ESPnet [22]. Tuy nhiên, OWSM được huấn luyện trên dữ liệu âm thanh dạng dài, khác với các công thức trước đó trong ESPnet. Do đó, chúng tôi đã phát triển các script chuẩn bị dữ liệu mới được thiết kế đặc biệt cho việc huấn luyện theo phong cách Whisper. Chúng tôi nối liền các phát ngôn liên tiếp trong cùng một cuộc nói chuyện dài dựa trên timestamp gốc của chúng. Mỗi phát ngôn dạng dài được giới hạn ở thời lượng tối đa 30 giây. Trong quá trình huấn luyện, tất cả các phát ngôn được đệm đến chính xác 30 giây, tối ưu hóa việc sử dụng tài nguyên tính toán.

Cho đến nay, chúng tôi đã phát triển ba phiên bản ở các quy mô khác nhau, được ký hiệu là OWSM v1, v2, và v3 trong Bảng 1. Tập dữ liệu lớn nhất của chúng tôi, v3, bao gồm 180k giờ dữ liệu âm thanh có nhãn. Điều này tạo thành khoảng một phần tư tổng dữ liệu được OpenAI Whisper sử dụng trong quá trình huấn luyện [15]. Các tập dữ liệu riêng lẻ được sử dụng bởi các mô hình của chúng tôi được liệt kê dưới đây:

• OWSM v1: AISHELL-1 [23], CoVoST2 [24], GigaSpeech [25], LibriSpeech [26], MuST-C [27], SPGISpeech [28], và TEDLIUM3 [29].

• OWSM v2: tất cả dữ liệu trong v1, GigaST [30], Multilingual LibriSpeech [31], và WenetSpeech [32].

• OWSM v3: tất cả dữ liệu trong v2, AIDATATANG [33], AMI [34], Babel [35], Common Voice [36], Fisher (Switchboard) [37], Fisher Callhome Spanish [38], FLEURS [39], Googlei18n3, KsponSpeech [40], MagicData [41], ReazonSpeech [42], Russian Open STT [43], VCTK [44], VoxForge [45], VoxPopuli [46], và WSJ [47].

2.3. Kiến trúc mô hình
OWSM tuân theo Whisper để sử dụng kiến trúc Transformer encoder-decoder [1], trong đó encoder và decoder có cùng số lượng lớp. Tuy nhiên, OWSM bổ sung sử dụng một joint CTC loss cho các mục tiêu ASR [48], điều này được chứng minh thực nghiệm là ổn định quá trình huấn luyện của chúng tôi. Các dạng sóng đầu vào được chuyển đổi thành 80 chiều log Mel filterbank với độ dài cửa sổ 25ms và độ dài hop 10ms. Các đặc trưng được trích xuất được tăng cường bằng SpecAugment [49] và được chuẩn hóa theo giá trị trung bình và phương sai toàn cục của chúng. Các đặc trưng sau đó được xử lý bởi một module tích chập hai chiều để giảm độ dài chuỗi. OpenAI Whisper [15] luôn downsample chuỗi bằng 2, dẫn đến độ phân giải thời gian 20ms. OWSM v2 và v3 của chúng tôi thực hiện downsampling 4 lần, điều này cải thiện hiệu quả hơn nữa. Các cấu hình chi tiết của các lớp Transformer encoder và decoder được tóm tắt trong Bảng 1. OWSM v1 và v3 sử dụng cùng cấu hình như Whisper small và medium, tương ứng, trong khi OWSM v2 nhỏ hơn một chút so với v3.4

Để suy luận, OpenAI Whisper triển khai cả greedy decoding và beam search với temperature fallback. Cái sau là một quy trình phức tạp dựa trên nhiều heuristic và siêu tham số như kích thước beam, nhiệt độ, ngưỡng xác suất log, và ngưỡng tỷ lệ nén gzip. OWSM của chúng tôi sử dụng framework ESPnet [22], do đó đảm bảo tính tương thích với nhiều thuật toán giải mã được ESPnet hỗ trợ ban đầu, bao gồm greedy search, beam search, và joint CTC/attention decoding (chỉ cho ASR) [50].

3Tài nguyên 32, 35, 36, 37, 41, 42, 43, 44, 52, 53, 54, 61, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, và 86 từ openslr.org.

4OWSM có nhiều tham số hơn một chút so với Whisper dưới cùng cấu hình, vì mô hình ESPnet có module downsampling tích chập lớn hơn và không chia sẻ input embedding và output projection trong decoder của nó.

--- TRANG 3 ---
Bảng 1: Chi tiết về dữ liệu, kiến trúc mô hình, và cấu hình huấn luyện. Chúng tôi tăng dần kích thước dữ liệu và mô hình từ v1 đến v3. Các cấu hình mô hình của OWSM v1 và v3 khớp với Whisper small và medium, tương ứng. Mặc dù OWSM v3 bao gồm nhiều ngôn ngữ hơn Whisper, kích thước dữ liệu của chúng tôi vẫn nhỏ hơn đáng kể, khiến nhiệm vụ của chúng tôi khó khăn hơn nhiều.∗Mô hình v3 của chúng tôi được khởi tạo với v2 đã được tiền huấn luyện để giảm thời gian huấn luyện (xem Mục 2.5).

[Bảng so sánh chi tiết giữa OpenAI Whisper và OWSM với các thông số về dữ liệu, kiến trúc mô hình và cấu hình huấn luyện]

2.4. Chi tiết huấn luyện
OWSM được triển khai trong ESPnet [22] dựa trên PyTorch [51]. Bảng 1 so sánh các siêu tham số huấn luyện của các mô hình khác nhau. OWSM sử dụng cùng batch size như Whisper, nhưng số lượng cập nhật tổng cộng nhỏ hơn. OWSM được huấn luyện trên GPU NVIDIA A100 (40GB). Mỗi GPU nhận hai mẫu, và gradient accumulation được áp dụng khi cần thiết để đảm bảo tổng batch size là 256. Cụ thể, OWSM v1 được huấn luyện trong khoảng 7 ngày trên 32 GPU A100 và OWSM v2 và v3 được huấn luyện trong khoảng 10 ngày trên 64 GPU A100. Sau khi huấn luyện, năm checkpoint có độ chính xác validation cao nhất được tính trung bình để tạo ra checkpoint cuối cùng.

2.5. Thách thức và mẹo huấn luyện
Huấn luyện phân tán quy mô lớn đặt ra những thách thức đáng kể, đặc biệt khi ngân sách tính toán bị hạn chế. Khi chúng tôi mở rộng từ vài nghìn giờ dữ liệu lên gần 200 nghìn giờ, chúng tôi đã gặp phải một loạt các vấn đề. Ở đây, chúng tôi thảo luận về một số thách thức và cung cấp các mẹo huấn luyện có giá trị để giúp vượt qua những trở ngại này một cách hiệu quả. Chúng tôi sẽ phát hành các script của mình hỗ trợ những kỹ thuật này.

Độ phân giải thời gian: Whisper sử dụng độ phân giải thời gian 20ms trong module encoder của nó, dẫn đến độ dài chuỗi 1500 cho đầu vào 30 giây. Điều này làm tăng đáng kể việc tiêu thụ bộ nhớ GPU và khiến việc huấn luyện chậm hơn và khó khăn hơn. Ngược lại, các mô hình ASR và ST hiện đại tiên tiến [52–55] áp dụng tỷ lệ downsampling lớn hơn. Bắt đầu từ OWSM v2, chúng tôi đã áp dụng độ phân giải thời gian 40ms, hiệu quả giảm độ dài chuỗi và giảm thiểu các yêu cầu tính toán liên quan. Chúng tôi cũng phát hiện rằng độ dài chuỗi ngắn hơn tạo điều kiện thuận lợi cho việc hội tụ mô hình dễ dàng hơn.

Joint ASR CTC loss: Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi quan sát thấy sự hội tụ không tối ưu của mô hình encoder-decoder dựa trên attention được huấn luyện trên nhiều tác vụ và dữ liệu đa dạng. Kết hợp một joint ASR CTC loss [48] vào đầu ra encoder có thể ổn định việc huấn luyện và tăng tốc hội tụ.

Khởi tạo ấm: Khi huấn luyện mô hình lớn nhất của chúng tôi, OWSM v3, chúng tôi sử dụng kỹ thuật khởi tạo ấm bằng cách tận dụng OWSM v2 đã được tiền huấn luyện. Cụ thể, 18 lớp đầu tiên của OWSM v3 được khởi tạo với v2 (có chính xác 18 lớp), trong khi 6 lớp còn lại được khởi tạo ngẫu nhiên. Mô hình v3 này hội tụ nhanh hơn nhiều so với việc huấn luyện từ đầu. Tuy nhiên, vẫn cần nghiên cứu xem khởi tạo ấm có ảnh hưởng bất lợi đến hiệu suất cuối cùng của mô hình hay không.

Vấn đề về bộ nhớ và hiệu quả: Chúng tôi đã phát triển một số chiến lược để giải quyết các vấn đề về bộ nhớ và hiệu quả gây ra bởi dữ liệu lớn. Để huấn luyện các mô hình tokenization BPE sử dụng SentencePiece [56], chúng tôi chọn ngẫu nhiên 10 triệu phiên âm văn bản thay vì sử dụng toàn bộ tập để giảm việc sử dụng bộ nhớ. Để huấn luyện, toàn bộ tệp văn bản quá lớn để được phân phối qua các worker khác nhau. Chúng tôi phân vùng tập huấn luyện thành 5 đến 12 tập con không chồng chéo và sử dụng nhiều data iterator để xây dựng mini-batch. Chúng tôi lọc thêm các mẫu có phiên âm cực kỳ dài (ví dụ, lớn hơn 600 token bao gồm cả prompt và target) được gây ra bởi việc căn chỉnh không chính xác trong corpus gốc (ví dụ, Common Voice). Không có việc lọc như vậy, việc huấn luyện sẽ thỉnh thoảng gặp lỗi hết bộ nhớ. Ngoài ra, chúng tôi xác thực các checkpoint trung gian chỉ sử dụng 10% tập validation đầy đủ. Điều này có thể tạo ra các ước tính hơi không chính xác về hiệu suất thực tế, nhưng nó giảm đáng kể thời gian validation và do đó cho phép validation và lưu checkpoint thường xuyên hơn, điều này rất quan trọng cho việc huấn luyện phân tán quy mô lớn. Trên thực tế, chúng tôi gặp phải nhiều lỗi khác nhau chủ yếu do lỗi hệ thống tệp hoặc lỗi giao tiếp, và chúng tôi phải khôi phục thủ công từ các checkpoint trước đó.

Trường hợp và dấu câu không nhất quán: Dữ liệu huấn luyện của chúng tôi được thu thập từ nhiều corpus công cộng. Một số trong số chúng cung cấp phiên âm thô trong trường hợp đúng với dấu câu, nhưng những cái khác chỉ cung cấp phiên âm được chuẩn hóa ở trường hợp viết thường hoặc viết hoa không có dấu câu nào. Trong quá trình suy luận, chúng tôi thấy rằng các mô hình OWSM mạnh mẽ đến mức chúng có thể nhận ra corpus và tạo ra đầu ra phù hợp với định dạng dữ liệu huấn luyện. Ví dụ, dữ liệu huấn luyện của WSJ ở trường hợp viết hoa. Khi được kiểm tra trên các tập test WSJ, OWSM cũng chủ yếu tạo ra văn bản ở trường hợp viết hoa. Vì chỉ một phần rất nhỏ dữ liệu huấn luyện ở trường hợp viết hoa, OWSM v3 hoạt động kém trên WSJ (xem Bảng 2). Trong tương lai, chúng tôi sẽ chuẩn hóa văn bản để giải quyết vấn đề này. Lưu ý rằng phân tích này chứng minh lợi ích của việc sử dụng dữ liệu công cộng và mã nguồn mở, mà không có chúng, chúng tôi không thể phát hiện ra những vấn đề như vậy.

3. THỰC NGHIỆM

3.1. Nhận dạng giọng nói tiếng Anh
Bảng 2 trình bày tỷ lệ lỗi từ (WER) trên các benchmark ASR tiếng Anh tiêu chuẩn. Greedy search được sử dụng mà không có bất kỳ mô hình ngôn ngữ ngoài nào. Để đảm bảo so sánh công bằng, chúng tôi chuẩn bị tất cả dữ liệu test trong ESPnet và đánh giá Whisper trong cùng thiết lập thay vì báo cáo kết quả từ bài báo của họ [15]. Văn bản được chuẩn hóa sử dụng bộ chuẩn hóa tiếng Anh hoặc cơ bản được cung cấp bởi Whisper. Whisper large không được bao gồm vì nó lớn hơn đáng kể so với các mô hình khác. Mặc dù nhiều corpus ASR công cộng được kết hợp, dữ liệu huấn luyện tiếng Anh của chúng tôi vẫn nhỏ hơn đáng kể so với Whisper (73k so với 438k giờ). Tuy nhiên, các mô hình OWSM của chúng tôi đạt được kết quả cạnh tranh trong hầu hết các benchmark. Các mô hình OWSM thậm chí vượt trội hơn Whisper trên LibriSpeech và Switchboard.

Bằng cách so sánh các phiên bản khác nhau của OWSM, chúng tôi quan sát thấy rằng khả năng ASR tiếng Anh của nó được cải thiện đáng kể từ v1 đến v2, chứng minh tính hiệu quả của việc mở rộng quy mô về số lượng tham số mô hình và lượng dữ liệu huấn luyện. Tuy nhiên, OWSM v3 không cho thấy sự cải thiện nhất quán so với v2 trong tất cả các benchmark. OWSM v3 đạt WER thấp hơn trên các tập test Switchboard và VoxPopuli, có thể vì các tập huấn luyện của chúng được thêm mới (xem Mục 2.2). OWSM v3 có sự suy giảm nhẹ trên LibriSpeech và sự suy giảm lớn trên WSJ. Điều này có thể là do sự chuyển dịch phân phối dữ liệu từ v2 đến v3. Như được hiển thị trong Bảng 1, tập dữ liệu v3 của chúng tôi chứa nhiều ngôn ngữ hơn đáng kể so với v2 (151 so với 23), nhưng kích thước mô hình chỉ tăng nhẹ (889M so với 712M). Do đó, mô hình phải điều chỉnh khả năng của nó từ tiếng Anh sang các ngôn ngữ khác hoặc từ loại giọng nói này sang loại giọng nói khác. Vấn đề này có thể được giảm thiểu với các mô hình lớn hơn và dữ liệu đa dạng hơn. Chúng tôi sẽ khám phá điều này trong tương lai. Vui lòng tham khảo đoạn cuối trong Mục 2.5 để có thêm thảo luận.

Chúng tôi cũng đã nghiên cứu tốc độ suy luận. Cụ thể, chúng tôi chọn 50 phát ngôn 30 giây từ tập dev TEDLIUM đã chuẩn bị của chúng tôi, và giải mã OWSM v3 với greedy search sử dụng một GPU NVIDIA A40 duy nhất. Thời gian giải mã trung bình cho mỗi phát ngôn 30 giây là 2.3 giây.

3.2. Nhận dạng giọng nói đa ngôn ngữ
Bảng 3 hiển thị kết quả ASR trên các benchmark đa ngôn ngữ. Nói chung, OpenAI Whisper đạt hiệu suất tốt hơn OWSM của chúng tôi, vì Whisper sử dụng nhiều dữ liệu huấn luyện hơn đáng kể trong tất cả các ngôn ngữ ngoại trừ tiếng Nhật. Đối với tiếng Nhật, OWSM v3 vượt trội hơn Whisper với biên độ lớn (CER: 11.3 so với 25.3) nhờ lượng dữ liệu huấn luyện lớn hơn (19k so với 7k giờ) từ ReazonSpeech [42]. Đáng chú ý, OWSM v2 đạt kết quả tốt nhất trên các tập test tiếng Anh và tiếng Trung từ Multilingual LibriSpeech và AISHELL, tương ứng, mặc dù được huấn luyện trên ít dữ liệu hơn.

Xu hướng qua các phiên bản khác nhau của OWSM phù hợp với những gì trong Mục 3.1. OWSM v2 được cải thiện đáng kể so với v1 trong tất cả các ngôn ngữ, điều này xác minh lợi ích của việc mở rộng quy mô. OWSM v3 vượt trội hơn v2 trong một số ngôn ngữ nhưng đạt kết quả tương đương hoặc hơi tệ hơn trong các ngôn ngữ khác. Một lần nữa, điều này có thể là do mô hình cần điều chỉnh khả năng của nó để hỗ trợ nhiều ngôn ngữ hơn trong v3.

3.3. Nhận dạng giọng nói dạng dài
Tương tự như Whisper, OWSM thực hiện ASR dạng dài bằng cách liên tiếp phiên âm các đoạn âm thanh 30 giây và dịch chuyển cửa sổ dựa trên timestamp được dự đoán. Bảng 4 trình bày kết quả ASR dạng dài trên tập test TEDLIUM, trong đó mỗi âm thanh đầu vào là một cuộc nói chuyện dài không được phân đoạn. OWSM v2 đạt WER 7.2% với greedy decoding và WER 6.6% với beam search. Các mô hình Whisper đạt WER thấp hơn trong cả hai trường hợp, có thể vì: (1) tập huấn luyện của chúng lớn hơn; (2) dữ liệu của chúng, được thu thập từ Internet, ban đầu ở dạng dài, có thể thực tế hơn so với của chúng tôi; (3) chúng áp dụng nhiều heuristic khác nhau để cải thiện dự đoán timestamp và cũng chất lượng văn bản (xem Mục 4.5 trong báo cáo chính thức của họ [15]). Trong công việc tương lai, chúng tôi sẽ khám phá thêm các chiến lược để nâng cao hiệu suất dạng dài.

Bảng 5 hiển thị hai ví dụ từ các cuộc nói chuyện TED, trong đó timestamp được tạo ra cùng với các token văn bản. Mặc dù các phát ngôn có thể được phân đoạn theo các cách khác nhau, nhưng các ranh giới được dự đoán bởi OWSM thường rất gần với tham chiếu.

3.4. Dịch thuật giọng nói
Bảng 6 so sánh các mô hình khác nhau trên hai benchmark ST: MuST-C (English-to-X) và CoVoST (X-to-English). Whisper chỉ hỗ trợ loại sau, trong khi OWSM hỗ trợ cả hai hướng.

Các mô hình OWSM đạt kết quả đáng chú ý trên MuST-C nhờ lượng dữ liệu huấn luyện đầy đủ (hơn 500 giờ cho mỗi ngôn ngữ). Điểm BLEU của tiếng Trung và tiếng Nhật thấp hơn so với các ngôn ngữ châu Âu ngay cả với dữ liệu huấn luyện đầy đủ. Điều này chỉ ra rằng mô hình gặp khó khăn trong việc dịch thuật giữa các ngôn ngữ rất khác nhau.

Trên CoVoST, hiệu suất của OWSM dao động qua các cặp ngôn ngữ khi lượng dữ liệu huấn luyện thay đổi từ 1 đến 300 giờ. Trên tiếng Trung và tiếng Nhật, đầu ra OWSM có độ hiểu thấp trong khi trên các ngôn ngữ châu Âu, đầu ra OWSM có độ hiểu vừa phải. Mặt khác, Whisper được huấn luyện trên 4k đến 12k giờ và do đó đạt điểm BLEU cao hơn trên X-to-English nói chung.

Tương tự như các phát hiện trong Mục 3.1 và Mục 3.2, OWSM v3 cho thấy hiệu suất tương đương hoặc hơi tệ hơn so với OWSM v2. Điều này là do OWSM v3 sử dụng gần như cùng lượng dữ liệu ST nhưng nó phải nhận ra nhiều ngôn ngữ hơn đáng kể (xem Bảng 1). Một số khả năng của nó cần được phân bổ cho những ngôn ngữ bổ sung này.

3.5. Nhận dạng ngôn ngữ
Như được mô tả trong Mục 2.1 và Hình 1, OWSM dự đoán một token ngôn ngữ ở đầu quá trình giải mã, điều này hiệu quả thực hiện tác vụ LID. Bảng 7 so sánh Whisper và OWSM trên tập test FLEURS được chuẩn bị trong ESPnet. OWSM v3 đạt độ chính xác top-1 81.4%, vượt trội hơn Whisper small và medium với biên độ lớn. Điều này là do OWSM v3 sử dụng dữ liệu huấn luyện từ Common Voice và FLEURS, chứa 151 ngôn ngữ tổng cộng, trong khi Whisper hỗ trợ 99 ngôn ngữ chỉ bao gồm một tập con các ngôn ngữ trong FLEURS. Tuy nhiên, kết quả này chứng minh rằng OWSM có khả năng mạnh mẽ trong phân loại giọng nói mặc dù nó được thiết kế như một mô hình sequence-to-sequence.

3.6. So sánh các thuật toán giải mã
OWSM tương thích với nhiều thuật toán giải mã khác nhau trong ESPnet. Bảng 8 so sánh ba thuật toán thường được sử dụng: chỉ CTC (greedy), chỉ attention (greedy), và joint CTC/attention (với beam size 10 và CTC weight 0.3). Beam search với joint CTC/attention đạt kết quả tốt nhất trong tất cả các tập test. Giải mã chỉ attention vượt trội hơn chỉ CTC, chỉ ra rằng decoder có khả năng mạnh mẽ.

4. THẢO LUẬN VÀ HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Nghiên cứu này đóng vai trò như một nỗ lực khám phá trong việc tái tạo huấn luyện theo phong cách Whisper sử dụng các tài nguyên nguồn mở. Tiến lên phía trước, chúng tôi sẽ đi sâu vào các hướng sau đây.

Thứ nhất, OWSM hiện tại vẫn thua kém Whisper trong nhiều benchmark, có thể vì: (1) OWSM hỗ trợ nhiều ngôn ngữ và nhiều hướng dịch thuật hơn, điều này tăng độ khó của việc học đa nhiệm vụ; (2) tập huấn luyện của chúng tôi nhỏ hơn đáng kể so với Whisper trong gần như tất cả các ngôn ngữ và tác vụ; (3) chúng tôi trực tiếp tận dụng các corpus ASR và ST công cộng có thể ít đa dạng hơn so với dữ liệu của Whisper được thu thập từ Internet. Những vấn đề này có thể được giải quyết bằng cách sử dụng các kiến trúc encoder [52–54, 57] hoặc decoder [58] tiên tiến hơn, thu thập dữ liệu ASR và ST đa dạng hơn từ các nguồn công cộng, và kết hợp các biểu diễn giọng nói tự giám sát [8, 9] như trong Google USM [18].

Thứ hai, chúng tôi dự định kết hợp các tác vụ xử lý giọng nói bổ sung vào framework đa nhiệm vụ, bao gồm hiểu ngôn ngữ nói và tạo giọng nói dựa trên các biểu diễn rời rạc, từ đó hướng tới việc phát triển "mô hình giọng nói toàn diện".

Thứ ba, những mô hình lớn đã được tiền huấn luyện này không phù hợp để triển khai trong các ứng dụng thực tế. Nhiều kỹ thuật nén khác nhau [59–64] có thể được áp dụng để giảm kích thước mô hình và tính toán.

Thứ tư, OWSM cung cấp một testbed có giá trị để nghiên cứu và khám phá các vấn đề học máy khác nhau như mất cân bằng dữ liệu, học liên tục [65], tính mạnh mẽ đối kháng [66], và machine unlearning [67].

5. KẾT LUẬN

Nghiên cứu này trình bày OWSM, tái tạo huấn luyện theo phong cách Whisper sử dụng bộ công cụ nguồn mở và dữ liệu công khai có sẵn. OWSM tuân theo framework đa nhiệm vụ của OpenAI Whisper, nhưng mở rộng nó để hỗ trợ nhiều hướng dịch thuật hơn. Một số chiến lược được phát triển để cải thiện hiệu quả. Chúng tôi sẽ mở nguồn tất cả các script cho việc chuẩn bị dữ liệu, huấn luyện, suy luận, và chấm điểm cũng như các mô hình đã được tiền huấn luyện và nhật ký huấn luyện. Chúng tôi tin rằng điều này có thể thúc đẩy tính minh bạch và tạo điều kiện cho sự tiến bộ trong việc tiền huấn luyện quy mô lớn của các mô hình giọng nói.

6. LỜI CẢM ƠN

Chúng tôi sử dụng PSC Bridges2 và NCSA Delta thông qua phân bổ ACCESS CIS210014, được hỗ trợ bởi các khoản tài trợ của Quỹ Khoa học Quốc gia #2138259, #2138286, #2138307, #2137603, và #2138296.

7. TÀI LIỆU THAM KHẢO

[1] A. Vaswani et al., "Attention is all you need," trong Proc. NeurIPS, 2017.

[2] Tom Brown et al., "Language models are few-shot learners," 2020.

[3] Jack W Rae et al., "Scaling language models: Methods, analysis & insights from training gopher," arXiv:2112.11446, 2021.

[4] Aakanksha Chowdhery et al., "Palm: Scaling language modeling with pathways," arXiv:2204.02311, 2022.

[5] Susan Zhang et al., "Opt: Open pre-trained transformer language models," arXiv:2205.01068, 2022.

[6] Hugo Touvron et al., "Llama: Open and efficient foundation language models," arXiv:2302.13971, 2023.

[7] OpenAI, "GPT-4 Technical Report," arXiv:2303.08774, 2023.

[8] Alexei Baevski, Yuhao Zhou, et al., "wav2vec 2.0: A framework for self-supervised learning of speech representations," trong Proc. NeurIPS, 2020.

[9] Wei-Ning Hsu et al., "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units," IEEE/ACM Trans. Audio, Speech, Lang. Process., tập 29, trang 3451–3460, 2021.

[10] Arun Babu, Changhan Wang, Andros Tjandra, et al., "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale," trong Proc. Interspeech, 2022.

[11] Shu wen Yang et al., "SUPERB: Speech Processing Universal PERformance Benchmark," trong Proc. Interspeech, 2021.

[12] Abdelrahman Mohamed et al., "Self-supervised speech representation learning: A review," IEEE J. Sel. Topics Signal Process., tập 16, số 6, trang 1179–1210, 2022.

[13] Xuankai Chang, Takashi Maekaku, et al., "An exploration of self-supervised pretrained representations for end-to-end speech recognition," trong Proc. ASRU, 2021.

[14] Yifan Peng et al., "A Study on the Integration of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding," trong Proc. SLT, 2022.

[15] Alec Radford et al., "Robust speech recognition via large-scale weak supervision," arXiv:2212.04356, 2022.

[16] William Chan et al., "Speechstew: Simply mix all available speech recognition data to train one large neural network," arXiv:2104.02133, 2021.

[17] Bo Li et al., "Scaling end-to-end models for large-scale multilingual asr," trong Proc. ASRU, 2021.

[18] Yu Zhang et al., "Google USM: Scaling automatic speech recognition beyond 100 languages," arXiv:2303.01037, 2023.

[19] Paul Pu Liang et al., "Towards understanding and mitigating social biases in language models," trong Proc. ICML, 2021.

[20] Jindong Wang et al., "On the robustness of chatgpt: An adversarial and out-of-distribution perspective," arXiv:2302.12095, 2023.

[21] Sébastien Bubeck et al., "Sparks of artificial general intelligence: Early experiments with gpt-4," arXiv:2303.12712, 2023.

[22] Shinji Watanabe et al., "ESPnet: End-to-End Speech Processing Toolkit," trong Proc. Interspeech, 2018.

[23] Hui Bu et al., "AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline," trong Proc. O-COCOSDA, 2017.

[24] Changhan Wang et al., "CoVoST 2 and Massively Multilingual Speech Translation," trong Interspeech, 2021.

[25] Guoguo Chen et al., "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio," trong Proc. Interspeech, 2021.

[26] Vassil Panayotov et al., "Librispeech: An ASR corpus based on public domain audio books," trong ICASSP, 2015.

[27] Roldano Cattoni et al., "Must-c: A multilingual corpus for end-to-end speech translation," Computer speech & language, tập 66, trang 101155, 2021.

[28] Patrick K O'Neill et al., "Spgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition," arXiv:2104.02014, 2021.

[29] François Hernandez et al., "Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation," trong Speech & Computer, 2018, trang 198–208.

[30] Rong Ye et al., "Gigast: A 10,000-hour pseudo speech translation corpus," arXiv:2204.03939, 2022.

[31] Vineel Pratap et al., "Mls: A large-scale multilingual dataset for speech research," arXiv:2012.03411, 2020.

[32] Binbin Zhang et al., "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition," trong Proc. ICASSP, 2022.

[33] "aidatatang 200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd," .

[34] Jean Carletta, "Unleashing the killer corpus: experiences in creating the multi-everything AMI Meeting Corpus," Lang. Res. Eval., tập 41, trang 181–190, 2007.

[35] "The babel program: https://www.iarpa.gov/index.php/research-programs/babel," .

[36] Rosana Ardila et al., "Common voice: A massively-multilingual speech corpus," arXiv:1912.06670, 2019.

[37] J.J. Godfrey et al., "SWITCHBOARD: telephone speech corpus for research and development," trong Proc. ICASSP, 1992.

[38] Matt Post et al., "Improved speech-to-text translation with the fisher and callhome Spanish-English speech translation corpus," 2013.

[39] Alexis Conneau et al., "FLEURS: Few-Shot Learning Evaluation of Universal Representations of Speech," trong Proc. SLT, 2022.

[40] Jeong-Uk Bang et al., "Ksponspeech: Korean spontaneous speech corpus for automatic speech recognition," Applied Sciences, tập 10, số 19, trang 6936, 2020.

[41] Zehui Yang et al., "Open source magicdata-ramc: A rich annotated mandarin conversational (ramc) speech dataset," arXiv:2203.16844, 2022.

[42] Yue Yin, Daijiro Mori, et al., "ReazonSpeech: A Free and Massive Corpus for Japanese ASR," 2023.

[43] Anna Slizhikova et al., "Russian Open Speech To Text (STT/ASR) Dataset," 2020.

[44] Junichi Yamagishi et al., "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit," 2019.

[45] "VoxForge: http://www.voxforge.org/," .

[46] Changhan Wang et al., "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation," trong Proc. ACL, 2021.

[47] Douglas B Paul and Janet Baker, "The design for the Wall Street Journal-based CSR corpus," trong Proc. Workshop on Speech and Natural Language, 1992.

[48] Suyoun Kim, Takaaki Hori, and Shinji Watanabe, "Joint ctc-attention based end-to-end speech recognition using multi-task learning," trong Proc. ICASSP, 2017.

[49] Daniel S. Park, William Chan, et al., "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition," trong Proc. Interspeech, 2019.

[50] Takaaki Hori, Shinji Watanabe, and John R Hershey, "Joint CTC/attention decoding for end-to-end speech recognition," trong Proc. ACL, 2017.

[51] A. Paszke et al., "Pytorch: An imperative style, high-performance deep learning library," trong NeurIPS, 2019.

[52] Anmol Gulati et al., "Conformer: Convolution-augmented Transformer for Speech Recognition," trong Proc. Interspeech, 2020.

[53] Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe, "Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding," trong Proc. ICML, 2022.

[54] Kwangyoun Kim et al., "E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition," trong Proc. SLT, 2022.

[55] Sehoon Kim et al., "Squeezeformer: An efficient transformer for automatic speech recognition," trong Proc. NeurIPS, 2022.

[56] Taku Kudo and John Richardson, "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," 2018.

[57] Yifan Peng et al., "A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks," trong Proc. Interspeech, 2023.

[58] Koichi Miyazaki, Masato Murata, and Tomoki Koriyama, "Structured State Space Decoder for Speech Recognition and Synthesis," trong Proc. ICASSP, 2023.

[59] Heng-Jui Chang et al., "DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT," trong Proc. ICASSP, 2022.

[60] Cheng-I Jeff Lai et al., "PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition," trong Proc. NeurIPS, 2021.

[61] Yifan Peng et al., "Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding," trong Proc. ICASSP, 2023.

[62] Yifan Peng, Yui Sudo, et al., "DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models," trong Proc. Interspeech, 2023.

[63] Yizeng Han, Gao Huang, et al., "Dynamic neural networks: A survey," tập 44, số 11, trang 7436–7456, 2021.

[64] Yifan Peng, Jaesong Lee, et al., "I3D: Transformer architectures with input-dependent dynamic depth for speech recognition," trong Proc. ICASSP, 2023.

[65] German I Parisi, Ronald Kemker, et al., "Continual lifelong learning with neural networks: A review," Neural networks, tập 113, trang 54–71, 2019.

[66] Raphael Olivier and Bhiksha Raj, "There is more than one kind of robustness: Fooling whisper with adversarial examples," arXiv:2210.17316, 2022.

[67] Thanh Tam Nguyen et al., "A survey of machine unlearning," arXiv:2209.02299, 2022.
