Liên kết Bản ghi với Học tập Tương phản Đa phương thức
Abhishek Arora1, Xinmei Yang1, Shao-Yu Jheng1, Melissa Dell1,2∗
1Đại học Harvard; Cambridge, MA, Hoa Kỳ.
2Cục Nghiên cứu Kinh tế Quốc gia; Cambridge, MA, Hoa Kỳ.
∗Tác giả liên hệ: melissadell@fas.harvard.edu.
Tóm tắt
Nhiều ứng dụng yêu cầu liên kết các cá nhân, công ty hoặc địa điểm qua các bộ dữ liệu. Hầu hết các phương pháp được sử dụng rộng rãi, đặc biệt trong khoa học xã hội, không sử dụng học sâu, với việc liên kết bản ghi thường được tiếp cận bằng các kỹ thuật so khớp chuỗi. Hơn nữa, các phương pháp hiện tại không khai thác bản chất đa phương thức vốn có của tài liệu. Trong các ứng dụng liên kết bản ghi lịch sử, tài liệu thường được phiên mã có nhiễu bởi nhận dạng ký tự quang học (OCR). Liên kết chỉ với văn bản OCR có thể thất bại do nhiễu, trong khi liên kết chỉ với cắt ảnh cũng có thể thất bại vì các mô hình thị giác thiếu hiểu biết ngôn ngữ (ví dụ, về các từ viết tắt hoặc các cách viết tên công ty khác nhau). Để tận dụng học tập đa phương thức, nghiên cứu này phát triển CLIPPINGS (Contrastively LInking Pooled Pre-trained Embeddings). CLIPPINGS căn chỉnh các bộ mã hóa đối xứng thị giác và ngôn ngữ, thông qua tiền huấn luyện tương phản ngôn ngữ-hình ảnh trên hình ảnh tài liệu và các văn bản OCR tương ứng. Sau đó nó học tập tương phản một không gian metric nơi embedding hình ảnh-văn bản gộp cho một thực thể nhất định gần với các embedding trong cùng lớp (ví dụ, cùng công ty hoặc địa điểm) và xa với các embedding của lớp khác. Dữ liệu được liên kết bằng cách coi liên kết như một bài toán truy xuất láng giềng gần nhất với các embedding đa phương thức. CLIPPINGS vượt trội hơn các phương pháp so khớp chuỗi được sử dụng rộng rãi với biên độ lớn trong việc liên kết các công ty Nhật Bản giữa thế kỷ 20 qua các tài liệu tài chính. Một mô hình hoàn toàn tự giám sát - chỉ được huấn luyện bằng cách căn chỉnh các embedding cho cắt ảnh tên công ty và văn bản OCR tương ứng - cũng vượt trội hơn các phương pháp so khớp chuỗi phổ biến. Thật thú vị, một bộ mã hóa chỉ thị giác được tiền huấn luyện đa phương thức hoạt động tốt hơn một bộ mã hóa chỉ thị giác được tiền huấn luyện đơn phương thức, minh họa sức mạnh của tiền huấn luyện đa phương thức ngay cả khi chỉ có một phương thức có sẵn để liên kết tại thời điểm suy luận.

1 Giới thiệu
Liên kết thông tin qua các nguồn là nền tảng cho nhiều phân tích. Ví dụ, các nhà nghiên cứu và doanh nghiệp thường xuyên liên kết các cá nhân hoặc công ty qua các cuộc điều tra dân số và hồ sơ công ty, chính phủ khử trùng lắp các danh sách trợ cấp hoặc cử tri qua các địa điểm, và các nhà phân tích tìm cách xác định cách thông tin từ cùng một nguồn lan truyền qua phương tiện truyền thông. Trong phần lớn các tài liệu có liên quan, các phương pháp mạng nơ-ron sâu đã ít xâm nhập. Ví dụ, một đánh giá toàn diện gần đây về các tài liệu liên kết bản ghi trong khoa học máy tính, khoa học xã hội và thống kê trong Science Advances (Binette và Steorts, 2022) kết luận rằng các phương pháp khác được ưa chuộng hơn các mô hình mạng nơ-ron sâu để liên kết bản ghi trong dữ liệu có cấu trúc. Điều này trái ngược hoàn toàn với tài liệu về phân biệt thực thể trong văn bản không có cấu trúc, nơi các mô hình ngôn ngữ transformer áp đảo, ví dụ Wu et al. (2019); De Cao et al. (2020); Yamada et al. (2020). Sự khác biệt này được giải thích bằng giả thiết rằng liên kết thực thể với văn bản không có cấu trúc có thể tận dụng sức mạnh của học chuyển giao từ các mô hình ngôn ngữ lớn được tiền huấn luyện, trong khi liên kết thực thể trong cơ sở dữ liệu văn bản có cấu trúc thì không thể (Binette và Steorts, 2022).

Nghiên cứu này tập trung vào liên kết bản ghi trong tài liệu lịch sử. Chúng tôi cho thấy rằng việc tận dụng sức mạnh của học sâu có thể cải thiện đáng kể độ chính xác của việc liên kết các công ty qua các nguồn dữ liệu, một ứng dụng liên kết bản ghi phổ biến, sử dụng dữ liệu lịch sử Nhật Bản làm trường hợp thử nghiệm. Hình 1 cho thấy các bản ghi cấp công ty Nhật Bản về chuỗi cung ứng (Jinji Kōshinjo, 1954) (trái) cần được liên kết với một thư mục công ty lớn (Teikoku Kōshinjo, 1957) (phải) với thông tin phong phú; các văn bản được viết với các hướng khác nhau. Mỗi công ty trong bộ dữ liệu được đại diện bởi cắt ảnh định vị của nó và bởi OCR của nó. Các cắt ảnh hữu ích cho liên kết bản ghi vì OCR chứa nhiễu; ngay cả các lỗi OCR nhỏ cũng có thể phá hủy thông tin quan trọng khi khớp các thực thể có tên tương tự. Văn bản hữu ích vì có những cách viết tên công ty tương tự về mặt thị giác mà có thể phân biệt được thông qua hiểu biết ngôn ngữ. Chúng tôi tập trung vào chuỗi cung ứng vì chúng là nền tảng cho việc truyền tải các cú sốc kinh tế (Acemoglu et al., 2016, 2012), sự tập tụ (Ellison et al., 2010), và phát triển kinh tế (Hirschman, 1958; Myrdal và Sitohang, 1957; Rasmussen, 1956; Bartelme và Gorodnichenko, 2015; Lane, 2022), nhưng vai trò của chúng trong phát triển kinh tế dài hạn đã khó nghiên cứu do những thách thức của việc liên kết chính xác các bản ghi lịch sử quy mô lớn.

Để tận dụng cả thông tin thị giác và văn bản được minh họa trong Hình 1, nghiên cứu phát triển CLIPPINGS (Contrastively LInking Pooled Pre-trained Embeddings). CLIPPINGS sử dụng huấn luyện đầu cuối đến cuối của các bộ mã hóa đối xứng thị giác và ngôn ngữ để học một không gian metric nơi biểu diễn hình ảnh-văn bản gộp cho một thực thể gần với các biểu diễn trong cùng lớp và xa với những biểu diễn trong các lớp khác.

Để huấn luyện CLIPPINGS, chúng tôi thực hiện tiền huấn luyện tương phản ngôn ngữ-hình ảnh tự giám sát trên các cặp cắt ảnh-OCR, bắt đầu với một bộ mã hóa CLIP tiếng Nhật được tiền huấn luyện (Makoto Sheen, 2022). Tiền huấn luyện ngôn ngữ-hình ảnh đảm bảo rằng các cắt ảnh được căn chỉnh với các văn bản tương ứng, ngay cả khi có nhiễu OCR.

CLIPPINGS sau đó có thể được tinh chỉnh thêm bằng cách sử dụng một loss tương phản và dữ liệu ghép cặp - ví dụ, ghép cặp các bản ghi ở bên trái và phải của Hình 1 - để căn chỉnh các embedding hình ảnh-văn bản gộp. Những dữ liệu có nhãn này được tạo bởi các tác giả nghiên cứu. Để tinh chỉnh mô hình, chúng tôi sử dụng một hàm loss căn chỉnh các embedding hình ảnh-hình ảnh, văn bản-văn bản, và hình ảnh-văn bản của dữ liệu ghép cặp. Loss tương phản có giám sát (Khosla et al., 2020) là một trường hợp đặc biệt của loss này khi bài toán được rút gọn thành một phương thức duy nhất. Dữ liệu huấn luyện ghép cặp bao gồm một số lượng khiêm tốn các chú thích tiêu chuẩn vàng của con người, trong đó một người xác định liệu các bản ghi có tham chiếu đến cùng một công ty hay không. Chúng tôi cũng tạo dữ liệu huấn luyện một cách tổng hợp. Cụ thể, chúng tôi render một danh sách các từ tiếng Nhật phổ biến thành hình ảnh sử dụng các font khác nhau, áp dụng các phép tăng cường hình ảnh, và đưa các hình ảnh kết quả vào OCR. Đối với cùng một từ, điều này tạo ra các góc nhìn khác nhau của hình ảnh, cũng như các góc nhìn khác nhau của văn bản do các lỗi OCR khác nhau được gây ra bởi các phép tăng cường hình ảnh.

Tại thời điểm suy luận, các thực thể được liên kết bằng cách truy xuất (các) láng giềng gần nhất của các truy vấn trong bộ dữ liệu đích (trong ví dụ của chúng tôi, các truy vấn đến từ danh sách nhà cung cấp và bộ dữ liệu đích là thư mục công ty với thông tin phong phú hơn về các công ty). Khung tương phản có thể linh hoạt tích hợp blocking trên các đặc tính cơ sở dữ liệu - phổ biến trong liên kết bản ghi - bằng cách sử dụng một loss cụ thể theo loại (Leszczynski et al., 2022).

CLIPPINGS vượt trội đáng kể so với các phương pháp so khớp chuỗi - chiếm ưu thế trong tài liệu liên kết bản ghi - cũng như các phương pháp mạng nơ-ron đơn phương thức. Khi CLIPPINGS được huấn luyện trên một số lượng khiêm tốn các công ty liên kết sử dụng học tập tương phản có giám sát, nó đạt được độ chính xác liên kết bản ghi 94.5%, so với độ chính xác tối đa so khớp chuỗi 73.1% đạt được với OCR tùy chỉnh. Khi chúng tôi bỏ qua huấn luyện tương phản có giám sát - chỉ sử dụng tiền huấn luyện ngôn ngữ-hình ảnh tự giám sát trên các cặp hình ảnh-OCR - điều này cũng vượt trội hơn so khớp chuỗi, với độ chính xác 84.9%. Khi chúng tôi chỉ sử dụng bộ mã hóa thị giác CLIPPINGS, độ chính xác cao hơn khi sử dụng bộ mã hóa thị giác được huấn luyện đơn phương thức, căn chỉnh các bản ghi yêu cầu một số hiểu biết ngôn ngữ. Tiền huấn luyện đa phương thức cho phép bộ mã hóa thị giác học một số hiểu biết ngôn ngữ. Các mô hình CLIPPINGS và dữ liệu huấn luyện được công bố công khai (giấy phép CC-BY).

Phần còn lại của nghiên cứu này được tổ chức như sau: Phần 2 thảo luận về tài liệu, và Phần 3 mô tả kiến trúc CLIPPINGS, và Phần 4 áp dụng CLIPPINGS cho bài toán liên kết bản ghi công ty. Cuối cùng, Phần 5 nêu ra các giới hạn, và Phần 6 thảo luận các cân nhắc đạo đức.

2 Tài liệu
Học tập tương phản: CLIPPINGS học một không gian metric cho các biểu diễn hình ảnh-văn bản gộp có thể được áp dụng cho các bài toán liên kết ngay cả khi số lượng lớp cực kỳ lớn, không biết trước, hoặc thay đổi liên tục khi cơ sở dữ liệu được cập nhật. Nó gợi nhớ đến nhiều ứng dụng bộ mã hóa đơn phương thức, chẳng hạn như độ tương tự ngữ nghĩa (Reimers và Gurevych, 2019), truy xuất đoạn văn (Karpukhin et al., 2020), phân biệt thực thể (Wu et al., 2019) và phân giải đồng tham chiếu (Hsu và Horwood, 2022) trong văn bản không có cấu trúc.

Để tạo các biểu diễn hình ảnh-văn bản gộp, cần phải có một không gian được căn chỉnh. Tiền huấn luyện Tương phản Ngôn ngữ-Hình ảnh (CLIP) (Radford et al., 2021) huấn luyện tương phản các bộ mã hóa văn bản và hình ảnh được căn chỉnh sử dụng 400 triệu cặp hình ảnh-chú thích. CLIPPINGS bắt đầu với các bộ mã hóa hình ảnh và văn bản CLIP tiếng Nhật được tiền huấn luyện (Makoto Sheen, 2022). CLIP tiếng Nhật được huấn luyện với loss CLIP tiêu chuẩn (Radford et al., 2021) nhưng sử dụng bộ mã hóa văn bản dựa trên BERT và transformer thị giác được khởi tạo bằng các trọng số từ mô hình AugReg ViT-B/16 (Steiner et al., 2021).

Liên kết Bản ghi: Nhiều ngành học - bao gồm khoa học máy tính, thống kê, quản lý cơ sở dữ liệu, kinh tế và khoa học chính trị - đã có những đóng góp phương pháp luận rộng lớn cho liên kết bản ghi, còn được gọi là phân giải thực thể, khớp mờ, khớp từ điển gần đúng, và khớp chuỗi.

Trong bối cảnh rộng lớn này, các tài liệu về phân giải thực thể trong cơ sở dữ liệu có cấu trúc (Binette và Steorts, 2022) so với văn bản ngôn ngữ tự nhiên (ví dụ (Wu et al., 2019; De Cao et al., 2020; Yamada et al., 2020)) vẫn phần lớn tách biệt. Trong tài liệu về liên kết bản ghi trong dữ liệu có cấu trúc - nhấn mạnh việc liên kết các trường văn bản có nhiễu chứa thông tin như tên cá nhân, tên công ty, tổ chức hoặc địa điểm - các metric khoảng cách chỉnh sửa thường được sử dụng, ví dụ (Levenshtein et al., 1966; Jaro, 1989; Winkler, 1990). Một cách tiếp cận rộng rãi khác tính toán độ tương tự cosine giữa các biểu diễn n-gram của chuỗi, nơi n-gram được định nghĩa là tất cả các chuỗi con có kích thước n trong một chuỗi (Okazaki và Tsujii, 2010).

Một tài liệu gần đây về các ứng dụng công nghiệp, tập trung vào khớp qua các bộ dữ liệu thương mại điện tử, cho thấy tiềm năng của các mô hình ngôn ngữ lớn transformer (LLM) để cải thiện liên kết bản ghi trong các bộ dữ liệu có cấu trúc (Li et al., 2020; Joshi et al., 2021; Brunner và Stockinger, 2020; Zhou et al., 2022; Peeters và Bizer, 2023; Tang et al., 2022). Tuy nhiên, những phương pháp này chưa xâm nhập rộng rãi trong các ứng dụng khoa học xã hội, với các phương pháp dựa trên quy tắc tiếp tục chiếm ưu thế áp đảo (ví dụ, xem các đánh giá của Binette và Steorts (2022); Abramitzky et al. (2021)). Bởi vì các bộ dữ liệu liên kết bản ghi có nhãn rất nhỏ so với các corpus lớn được sử dụng để huấn luyện các mô hình transformer từ đầu, một đánh giá toàn diện năm 2022 về tài liệu liên kết bản ghi trong Science Advances (Binette và Steorts, 2022) kết luận rằng các mô hình mạng nơ-ron sâu không có khả năng áp dụng cho phân giải thực thể sử dụng dữ liệu có cấu trúc. Xây dựng dữ liệu huấn luyện cho liên kết bản ghi thực sự rất tốn kém về lao động, nhưng phần lớn kiến thức cần thiết để cải thiện liên kết bản ghi có thể đã được bao gồm trong các bộ mã hóa hình ảnh và ngôn ngữ được tiền huấn luyện như CLIP (Radford et al., 2021), hoặc có thể được thu thập từ tiền huấn luyện ngôn ngữ-hình ảnh tự giám sát tiếp theo được theo đuổi trong nghiên cứu này.

Ở dạng đơn giản nhất, các phương pháp khớp chuỗi gần đúng chỉ đơn giản đếm số lần chỉnh sửa cần thiết (chèn, xóa, thay thế) để biến đổi một chuỗi thành chuỗi khác. Trong thực tế, không phải tất cả các phép thay thế đều có xác suất bằng nhau, dẫn đến những nỗ lực xây dựng các danh sách dựa trên quy tắc điều chỉnh chi phí của các phép thay thế. Ví dụ, gói fuzzychinese (znwang25, 2020) sử dụng nét hoặc bộ phận làm đơn vị cơ bản cho các biểu diễn chuỗi con n-gram của các thực thể, nơi những nét và bộ phận này được lấy từ một cơ sở dữ liệu bên ngoài (kfcd, 2015) bao phủ một tập con của script CJK. Ngoài ra, gói masala merge (Novosad, 2018) điều chỉnh khoảng cách Levenshtein (Levenshtein et al., 1966) để áp đặt các hình phạt nhỏ hơn cho các cách đánh vần thay thế phổ biến trong tiếng Hindi. Soundex, lần đầu được phát triển năm 1918 - cùng với Hệ thống Nhận dạng và Tình báo bang New York được cập nhật năm 1970 (NYSIIS) (Silbert, 1970) - tính đến thực tế rằng các phép thay thế có âm thanh tương tự có nhiều khả năng hơn vì các người điều tra dân số đánh vần sai tên theo âm thanh của chúng. Những phương pháp này vẫn là nền tảng cho liên kết bản ghi trong dữ liệu điều tra dân số lịch sử Hoa Kỳ (Abramitzky et al., 2021).

Các phương pháp dựa trên quy tắc như vậy có thể hoạt động tốt trong các bối cảnh mà chúng được điều chỉnh. Tuy nhiên, chúng có thể dễ vỡ và tốn kém về lao động để mở rộng cho các cài đặt mới, do việc sử dụng các đặc trưng được chế tạo thủ công. Điều này làm lệch nghiêm trọng các bộ dữ liệu liên kết trong khoa học xã hội về phía một số cài đặt có tài nguyên cao mà các phương pháp hiện tại đã được điều chỉnh, từ đó làm lệch kiến thức hạ nguồn. Ngay cả trong các cài đặt có tài nguyên cao, độ chính xác thấp trong một số ứng dụng có thể yêu cầu can thiệp con người rộng rãi trong quá trình khớp để đạt được độ chính xác mong muốn (Bailey et al., 2020), hạn chế quy mô của các vấn đề.

Cũng có một số nỗ lực ước tính các mô hình học máy cho liên kết bản ghi. Ví dụ, (Ventura et al., 2015) sử dụng một bộ phân loại rừng ngẫu nhiên được huấn luyện trên dữ liệu có nhãn để phân biệt các tác giả của bằng sáng chế Hoa Kỳ, áp dụng clustering cho các điểm số bất đồng kết quả để thực thi tính bắc cầu. Gần đây nhất, gói LinkTransformer để sử dụng các mô hình ngôn ngữ lớn để liên kết dữ liệu có cấu trúc đã được phát hành (Arora và Dell, Forthcoming). Gói này trích dẫn những thách thức của việc sử dụng các codebase kỹ thuật cao hiện tại như một trở ngại cho việc sử dụng học sâu cho liên kết bản ghi trong khoa học xã hội, và được thiết kế để thân thiện với người dùng cho các nhà khoa học xã hội thiếu quen thuộc với các framework học sâu. Codebase CLIPPINGS được thiết kế tương tự để trực quan cho người dùng khoa học xã hội.

3 Kiến trúc Mô hình
Hình 2 cho thấy kiến trúc CLIPPINGS.
Chúng tôi bắt đầu với các embedding hình ảnh và văn bản được tiền huấn luyện, căn chỉnh từ CLIP ngôn ngữ Nhật Bản (Makoto Sheen, 2022), được huấn luyện trên các cặp hình ảnh-chú thích sử dụng các chú thích được dịch máy sang tiếng Nhật. Chúng tôi tiếp tục tiền huấn luyện tự giám sát của các bộ mã hóa thị giác và văn bản CLIP tiếng Nhật trên các cặp cắt ảnh-OCR.

Sau đó chúng tôi học một không gian metric nơi biểu diễn hình ảnh-văn bản gộp cho một thực thể nhất định gần với các biểu diễn (embedding) trong cùng lớp (ví dụ, cùng công ty) và xa với các biểu diễn trong các lớp khác. Mô hình được khởi tạo với các bộ mã hóa thu được thông qua tiền huấn luyện tự giám sát được mô tả ở trên. Chúng tôi sử dụng một loss tương phản có giám sát (Khosla et al., 2020) trên các biểu diễn gộp:

−∑[i∈B] 1/|P(i)| ∑[k∈P(i)] log(exp(zi)T(zk)/τ) / ∑[j∈B] exp((zi)T(zj)/τ)

nơi zi = (f(xi) + g(ti))/2 là trung bình của các embedding hình ảnh và văn bản cho thực thể i. B biểu thị batch và τ là một siêu tham số nhiệt độ. Loss này khuyến khích căn chỉnh các biểu diễn hình ảnh-hình ảnh, văn bản-văn bản, hình ảnh-văn bản, và văn bản-hình ảnh qua các thực thể tích cực. Nó có hương vị của việc kết hợp học tập tương phản trên văn bản, học tập tương phản trên hình ảnh, và UniCL (Yang et al., 2022), có một mục tiêu độ tương tự hình ảnh-văn bản và văn bản-hình ảnh hai chiều. CLIPPINGS không mô hình hóa attention đa phương thức, vì trong các ứng dụng liên kết bản ghi nơi đầu vào là một hình ảnh của một văn bản và OCR tương ứng, attention đa phương thức không có khả năng dẫn đến các biểu diễn phong phú hơn một cách đáng kể.

Chúng tôi sử dụng bộ tối ưu AdamW cho tất cả huấn luyện mô hình, kết hợp với một bộ lập lịch tốc độ học (LR) Cosine Annealing với Warm Restarts. LR tối đa được đặt cho mỗi lần chạy, và LR tối thiểu được cố định ở 0. Khởi động lại đầu tiên được lên lịch sau 10 bước, với hệ số khởi động lại là 2, tăng gấp đôi thời gian khởi động lại sau mỗi chu kỳ. Mỗi epoch bao gồm lấy mẫu m góc nhìn (cặp hình ảnh-văn bản) của mỗi nhãn trong bộ dữ liệu và xử lý chúng một lần.

Tổng thời gian huấn luyện là 34.6 giờ trên một card GPU A6000 duy nhất: 24 giờ cho tiền huấn luyện ngôn ngữ-hình ảnh, 10 giờ cho huấn luyện có giám sát sử dụng dữ liệu tổng hợp, và 40 phút cho huấn luyện trên dữ liệu có nhãn thủ công. Các siêu tham số bổ sung và chi tiết huấn luyện mô hình được cung cấp trong tài liệu bổ sung.

Một card GPU A6000 duy nhất có thể chứa kích thước batch B là 153 cặp hình ảnh-văn bản. So sánh, mô hình CLIP gốc (Radford et al., 2021) được huấn luyện với kích thước batch 32,768 sử dụng 256 GPU V100. Để đảm bảo đủ negative trong batch với kích thước batch nhỏ hơn phù hợp cho các người dùng hạ nguồn khác nhau, chúng tôi sử dụng khai thác negative khó ngoại tuyến. Chi tiết về batching và khai thác negative khó được chi tiết trong tài liệu bổ sung.

Tại thời điểm suy luận, các thực thể được liên kết sử dụng truy xuất láng giềng gần nhất. Facebook Artificial Intelligence Similarly Search (FAISS) (Johnson et al., 2019), với IndexFlatIP, được sử dụng để tính toán khoảng cách chính xác theo cặp giữa các embedding, để truy xuất láng giềng gần nhất của mỗi truy vấn (công ty trong danh sách nhà cung cấp) trong thư mục công ty.

Blocking - tích hợp thông tin loại từ cơ sở dữ liệu có cấu trúc vào liên kết bản ghi - quan trọng trong nhiều ứng dụng và đã là chủ đề của nghiên cứu rộng rãi (xem Steorts et al. (2014); Papadakis et al. (2019) để đánh giá). Mặc dù không áp dụng cho các ứng dụng của nghiên cứu này, blocking tự nhiên để tích hợp vào một thiết lập tương phản, thông qua việc sử dụng một hàm loss cụ thể theo loại (Leszczynski et al., 2022). Điều này sẽ cho phép thực hiện các khớp ngay cả khi có một số nhiễu trong loại, một kịch bản phổ biến.

Để kiểm tra cách CLIPPINGS so sánh với một framework tương tự được huấn luyện sử dụng một bộ mã hóa đơn phương thức tiên tiến, chúng tôi huấn luyện một transformer thị giác DINO đối xứng (Caron et al., 2021) sử dụng dữ liệu cắt ảnh công ty Nhật Bản được liên kết. Chi tiết về tiền huấn luyện tự giám sát và các siêu tham số cho huấn luyện tự giám sát và có giám sát được chi tiết trong tài liệu bổ sung.

4 Liên kết Bản ghi
4.1 Dữ liệu
Ứng dụng đầu tiên của nghiên cứu này là xây dựng chuỗi cung ứng lịch sử Nhật Bản. Điều này đòi hỏi khớp các nhà cung cấp và khách hàng được ghi lại trong các bản ghi cấp công ty được thu thập năm 1956 cho hơn 7,000 công ty lớn Nhật Bản (Jinji Kōshinjo, 1954) với một thư mục cấp công ty cung cấp thông tin phong phú bổ sung về gần 70,000 công ty (Teikoku Kōshinjo, 1957). Cái trước được viết ngang và cái sau viết dọc, tạo nên một trường hợp có thể khó khăn cho khớp thị giác. Các cắt tên công ty được định vị sử dụng một mô hình Mask R-CNN (He et al., 2017) được huấn luyện tùy chỉnh với Layout Parser (Shen et al., 2021). Để tạo dữ liệu có nhãn cho huấn luyện và đánh giá, các khách hàng và nhà cung cấp của các công ty được chọn ngẫu nhiên được hợp nhất thủ công với thư mục công ty. Hai người chú thích hoàn thành nhiệm vụ này và giải quyết tất cả các bất đồng bằng tay. Nhiều công ty xuất hiện như khách hàng và nhà cung cấp của nhiều công ty, và dữ liệu được khử trùng lắp sao cho mỗi khách hàng hoặc nhà cung cấp chỉ xuất hiện một lần, để tránh rò rỉ. Đôi khi một công ty đơn lẻ được liên kết với nhiều mục trong thư mục công ty, vì các công ty có thể xuất hiện ở đó nhiều hơn một lần nếu chúng được đăng ký trong nhiều tỉnh.

Chúng tôi sử dụng phân chia 60-20-20 để chia các lớp thành một tập huấn luyện (772 ví dụ), tập xác thực (257 ví dụ), và tập kiểm tra (257 ví dụ). Dữ liệu kiểm tra liên kết danh sách khách hàng-nhà cung cấp với tất cả các công ty khớp trong thư mục (với nhiều khớp xảy ra khi một công ty được đăng ký trong nhiều tỉnh), trong khi việc ghi nhãn tốn kém này không cần thiết cho dữ liệu huấn luyện, nơi mỗi công ty có một khớp duy nhất. Trong văn bản chính, chúng tôi báo cáo kết quả sử dụng một bộ dữ liệu loại bỏ các khách hàng và nhà cung cấp như "chính phủ" không xuất hiện trong thư mục công ty. Trong tài liệu bổ sung, chúng tôi báo cáo các phân tích bao gồm những thực thể này, với các mẫu tương tự xuất hiện.

Chúng tôi cũng huấn luyện trên 19,793 tên địa danh Nhật Bản được tạo tổng hợp, với phân chia huấn luyện-xác thực 80-20. Mỗi cái được render sử dụng các font khác nhau và OCR với hai công cụ OCR khác nhau, tạo ra các lỗi khác nhau (Carlson et al., 2023; Du et al., 2022). Mỗi epoch bao gồm lấy mẫu 3 "góc nhìn" của mỗi cặp cắt ảnh-ocr.

Ngoài ra, chúng tôi tiến hành tiền huấn luyện ngôn ngữ-hình ảnh tự giám sát của các bộ mã hóa CLIP tiếng Nhật, sử dụng 111,750 cắt ảnh công ty và OCR tương ứng, cũng như 19,793 tên được tạo tổng hợp tương tự và OCR của chúng, với phân chia kiểm tra-xác thực 80-20.

Vì độ chính xác liên kết bản ghi với so khớp chuỗi có khả năng liên quan đến chất lượng của OCR, chúng tôi áp dụng các so sánh so khớp chuỗi sử dụng hai OCR khác nhau của tên công ty Nhật Bản. Bộ dữ liệu "có nhiễu" được tạo bằng cách sử dụng Google Cloud Vision có sẵn, vì việc sử dụng OCR có sẵn là chuẩn mực áp đảo (Google Cloud Vision không hỗ trợ tinh chỉnh) và thường có nhiễu. Bộ dữ liệu "sạch" được tạo sử dụng một OCR được huấn luyện tùy chỉnh đạt được tỷ lệ lỗi ký tự 0.6% (Carlson et al., 2023), một kịch bản gần như tốt nhất. Tuy nhiên, trong các ngôn ngữ dựa trên ký tự, các thực thể thường có tương đối ít ký tự trong tên của chúng, có nghĩa là ngay cả một lỗi OCR duy nhất có thể phá hủy thông tin quan trọng.

Bảng 1: Kết quả Khớp Cơ bản: Bảng này báo cáo độ chính xác trên tập kiểm tra sử dụng nhiều phương pháp khác nhau để liên kết các công ty Nhật Bản từ bản ghi chuỗi cung ứng với một thư mục công ty lớn. OCR có nhiễu sử dụng Google Cloud Vision có sẵn cho OCR, và OCR sạch sử dụng một OCR được huấn luyện tùy chỉnh chính xác.

Panel A: So khớp Chuỗi
Khoảng cách Levenshtein: 0.630 (OCR có nhiễu), 0.731 (OCR sạch)
Độ tương tự n-gram nét: 0.689 (OCR có nhiễu), 0.731 (OCR sạch)

Panel B: Huấn luyện Tự giám sát Ngôn ngữ-Hình ảnh
Liên kết Thị giác: 0.769, 0.769
Liên kết Ngôn ngữ: 0.740, 0.790
Liên kết Đa phương thức: 0.845, 0.849

Panel C: Huấn luyện Có giám sát trên Dữ liệu Liên kết với Tiền huấn luyện Thị giác
Liên kết Thị giác: 0.878, 0.878

Panel D: Huấn luyện Có giám sát trên Dữ liệu Liên kết với Tiền huấn luyện Ngôn ngữ-Hình ảnh
Liên kết Thị giác: 0.924, 0.924
Liên kết Ngôn ngữ: 0.790, 0.882
Liên kết Đa phương thức: 0.937, 0.945

4.2 Kết quả
CLIPPINGS đạt được độ chính xác liên kết bản ghi 94.5% (Bảng 1, Panel D), vượt trội đáng kể so với các metric so khớp chuỗi (Panel A) trên cả OCR có nhiễu (68.9% độ chính xác) và OCR sạch (73.1% độ chính xác). Khi sử dụng các biểu diễn đa phương thức, độ chính xác của OCR chỉ có tác động rất khiêm tốn lên độ chính xác liên kết: 93.7% với OCR có nhiễu so với 94.5% với OCR sạch.

Sử dụng các biểu diễn gộp cũng vượt trội hơn việc chỉ tinh chỉnh các bộ mã hóa hình ảnh hoặc ngôn ngữ được tiền huấn luyện trên dữ liệu thực thể ghép cặp cho phương thức đó, đạt được độ chính xác 92.4% (khớp thị giác) và 88.2% (khớp ngôn ngữ). Khi có lỗi OCR, các cắt ảnh hữu ích vì chúng tránh việc phá hủy thông tin thông qua OCR. Đồng thời, hiểu biết ngôn ngữ hữu ích do các từ viết tắt và nhiều cách để viết tên công ty.

Theo hướng này, sử dụng một bộ mã hóa thị giác được tiền huấn luyện đa phương thức trên dữ liệu ghép cặp cắt ảnh-văn bản OCR đạt được độ chính xác 92.4%, đánh bại bộ mã hóa thị giác được huấn luyện đơn phương thức - đạt được độ chính xác 87.8% - với biên độ khá rộng (Panel C). Điều này gợi ý về tính hữu ích của tiền huấn luyện đa phương thức, mà chúng tôi sẽ kiểm tra thêm trong phân tích lỗi.

Hình 3, Panel A cho thấy các lỗi đại diện được tạo bởi các bộ mã hóa chỉ thị giác có giám sát, chỉ ngôn ngữ, và đa phương thức, sử dụng tiền huấn luyện ngôn ngữ-hình ảnh (Panel D). Trong lỗi bộ mã hóa thị giác, sự thật nền là Meiji Seika, một công ty sản xuất đồ ăn vặt và sô cô la. Dự đoán của nó là công ty dược phẩm Meiji, vì 菓 và 薬 trông tương tự. Điều này minh họa cách bộ mã hóa thị giác đôi khi có thể nhầm lẫn các thực thể tương tự về mặt thị giác có ý nghĩa rất khác nhau. Trong lỗi bộ mã hóa chỉ ngôn ngữ, sự thật nền là "Japanese Processing and Manufacturing of Paper" (日本加工製紙), và dự đoán là "Japanese Paper Processing" (日本紙加工), rất tương tự về ý nghĩa. Bộ mã hóa đa phương thức dự đoán chính xác cả hai trường hợp này. Cuối cùng, lỗi đa phương thức là một công ty có tên chỉ có hai ký tự. Sự thật nền là 丸永, và dự đoán là 丸水, với 永 và 水 rất tương tự về mặt thị giác. Đây là những tên không có ngữ cảnh ngôn ngữ, vì vậy bộ mã hóa ngôn ngữ cũng không thể phân biệt chúng.

Thú vị là, bộ mã hóa chỉ thị giác được tinh chỉnh với tiền huấn luyện ngôn ngữ-hình ảnh nhận được một số khớp yêu cầu hiểu biết ngôn ngữ đúng (độ chính xác 92.4%), mà mô hình ViT được tinh chỉnh với tiền huấn luyện chỉ thị giác nhận sai (độ chính xác 87.8%), gợi ý một số thu nhận hiểu biết ngôn ngữ từ tiền huấn luyện đa phương thức. Hình 3, Panel B cung cấp một số ví dụ. Trong ví dụ đầu tiên, レナウン 靴下 (Renown Sock) được khớp bởi mô hình ViT với "レナウン 商事 (Renown Business), trong khi bộ mã hóa được tiền huấn luyện đa phương thức khớp nó chính xác với レナウン靴下工業 (Renown Sock Industry), mặc dù có thêm hai ký tự. Trong ví dụ thứ hai, sự thật nền là 太陽鑛工 (Sun Mineral Manufacturing), trong khi dự đoán ViT là 太陽紙工 (Sun Paper Manufacturing). Ví dụ thứ ba là kết quả của một lỗi trong mô hình phát hiện layout được huấn luyện tùy chỉnh của chúng tôi, nối hai khách hàng-nhà cung cấp. Công ty được phát hiện là " エヌティエヌ 販賣會社[販賣]国鉄," được dịch là "NTN Sales Company[Sales]The Government-owned Railway". ViT chỉ đơn giản dự đoán một công ty có độ dài tương tự, trong khi bộ mã hóa với tiền huấn luyện đa phương thức khớp nó với NTN Sales Company.

Mô hình đa phương thức hoàn toàn tự giám sát cũng vượt trội hơn các phương pháp so khớp chuỗi, với độ chính xác 84.9% (Bảng 1, Panel B). Độ tương tự n-gram ở cấp độ nét với fuzzychinese (znwang25, 2020) đạt được độ chính xác 73.1% trên OCR sạch, như tình cờ cũng làm khoảng cách Levenshtein. Khi chỉ bộ mã hóa thị giác hoặc bộ mã hóa ngôn ngữ của mô hình đa phương thức tự giám sát được sử dụng tại thời điểm kiểm tra, hiệu suất cũng vượt quá các kỹ thuật so khớp chuỗi tiêu chuẩn (độ chính xác 76.9% và 79.0%, tương ứng). Điều này cho thấy rằng chỉ với huấn luyện tự giám sát, các phương pháp mạng nơ-ron vẫn có thể vượt trội hơn các phương pháp so khớp chuỗi được sử dụng rộng rãi.

Bảng 2 kiểm tra sự đóng góp của các yếu tố khác nhau trong công thức huấn luyện CLIPPINGS. Panel A xem xét hiệu suất của CLIP tiếng Nhật (Makoto Sheen, 2022) có sẵn. Sử dụng chỉ bộ mã hóa thị giác, mọi thực thể đều được dự đoán sai. Bộ mã hóa văn bản có sẵn, mặc dù tốt hơn bộ mã hóa thị giác, được vượt trội bởi các phương pháp so khớp chuỗi truyền thống.

Panel B kiểm tra hiệu suất của CLIPPINGS khi chỉ huấn luyện có giám sát được sử dụng, loại bỏ huấn luyện tự giám sát trên các cặp hình ảnh-OCR. Hiệu suất giảm đáng kể so với khi tiền huấn luyện ngôn ngữ-hình ảnh tự giám sát được sử dụng - với độ chính xác tương tự như các phương pháp so khớp chuỗi truyền thống - minh họa tầm quan trọng của việc căn chỉnh đầu tiên các bộ mã hóa thị giác và văn bản cho domain hình ảnh tài liệu-OCR.

Bảng 2: Ablation Liên kết Bản ghi: Bảng này báo cáo độ chính xác để liên kết các công ty Nhật Bản từ bản ghi chuỗi cung ứng với một thư mục công ty lớn. OCR có nhiễu sử dụng Google Cloud Vision có sẵn cho OCR, và OCR sạch sử dụng một OCR được huấn luyện tùy chỉnh chính xác. Panel A sử dụng CLIP tiếng Nhật có sẵn, và Panel B chỉ sử dụng huấn luyện có giám sát, không có tiền huấn luyện tự giám sát thêm.

Panel A: CLIP tiếng Nhật Zero-shot
Liên kết Thị giác: 0.000, 0.000
Liên kết Ngôn ngữ: 0.639, 0.626
Liên kết Đa phương thức: 0.491, 0.433

Panel B: Chỉ Huấn luyện Có giám sát
Liên kết Đa phương thức: 0.676, 0.731

Vì CLIPPINGS tận dụng học chuyển giao từ tiền huấn luyện ngôn ngữ-hình ảnh, nó có thể được điều chỉnh chính xác cho các cài đặt khác nhau với các tập huấn luyện nhỏ, rẻ để tạo, hoặc được sử dụng như một giải pháp hoàn toàn tự giám sát. Hơn nữa, các negative khó trong huấn luyện tương phản khuyến khích sự tách biệt giữa các biểu diễn thực thể khác nhau ngay cả khi các thực thể rất tương tự, làm cho nó rất phù hợp để liên kết các bộ dữ liệu với nhiều confusable.

Chi phí triển khai quan trọng cho khả năng mở rộng, vì các bài toán liên kết bản ghi thường đòi hỏi liên kết hàng triệu thực thể trên một ngân sách tính toán hạn chế. Các thí nghiệm trên bộ dữ liệu đầy đủ gồm 36,673 khách hàng-nhà cung cấp, sử dụng một CPU Intel(R) i9-9980XE 18 core @ 3.00GHz và một GPU NVIDIA A6000 duy nhất, cho thấy rằng chi phí triển khai của CLIPPING khiêm tốn, làm cho nó trở thành một giải pháp có khả năng mở rộng cao. CLIPPINGS đa phương thức mất 6 phút, 21 giây để chạy trên dữ liệu đầy đủ với một GPU duy nhất, tất cả trừ một giây trong số đó là embedding các cắt và OCR. Điều này so với 54 phút để thực hiện các tính toán khoảng cách Levenshtein CPU được tối ưu hóa của chúng tôi (nhanh hơn một bậc độ lớn so với so khớp chuỗi R, ví dụ (van der Loo, 2014)) và 3 phút và 7 giây cho so khớp nét với fuzzychinese (znwang25, 2020).

Hình 4 minh họa các mạng chuỗi cung ứng được tạo bằng cách áp dụng CLIPPINGS cho bộ dữ liệu đầy đủ, với việc tô bóng cho thấy khoảng cách trung bình trong các mạng chuỗi cung ứng đến ba tập đoàn lớn nhất Nhật Bản: Mitsui, Mitsubishi, và Sumitomo. Vị trí và tỷ lệ nút được cố định qua các đồ thị, với tỷ lệ hiển thị trung tâm độ trung bình trong mạng chuỗi cung ứng. Điểm chính là sử dụng OCR có sẵn và khoảng cách Levenshtein - một cách tiếp cận phổ biến trong tài liệu - tạo ra một mạng khác biệt rõ ràng (trái) so với phương pháp đa phương thức (phải), gần với sự thật nền hơn nhiều. Ví dụ, ở góc dưới bên phải của đồ thị được tạo với khoảng cách chỉnh sửa, có một công ty với nhiều liên kết sai (và do đó có trung tâm độ cao). Một nghiên cứu về nền kinh tế Nhật Bản dựa trên mạng có nhiễu hơn có khả năng tạo ra kết quả thiên lệch (Chandrasekhar và Lewis, 2011). Điều này minh họa tính hữu ích cho nghiên cứu khoa học xã hội hạ nguồn của việc cải thiện liên kết bản ghi thông qua học tập đa phương thức.

5 Giới hạn
Giới hạn lớn nhất của CLIPPINGS là nó hiện tại được tinh chỉnh cho một ngôn ngữ và loại bộ sưu tập tài liệu cụ thể. Bởi vì hiệu suất mạnh có thể đạt được ngay cả với huấn luyện hoàn toàn tự giám sát, tinh chỉnh có giám sát không yêu cầu nhiều nhãn, và yêu cầu tính toán không quá nặng nề, sẽ tương đối đơn giản để mở rộng cho một bộ sưu tập tài liệu đa ngôn ngữ, đa dạng hơn nhiều. Chúng tôi hy vọng rằng bằng cách minh họa tính hữu ích của phương pháp này, nó sẽ khuyến khích phát triển thêm các mô hình đa phương thức cho liên kết bản ghi.

6 Cân nhắc Đạo đức
CLIPPINGS có tính đạo đức. Các phương pháp của nó hoàn toàn mã nguồn mở, và dữ liệu huấn luyện của nó hoàn toàn trong phạm vi công cộng. Mặc dù CLIPPINGS chính xác hơn so khớp chuỗi, cũng như khớp mạng nơ-ron đơn phương thức, một số lỗi thường vẫn sẽ phát sinh với khớp đa phương thức. Quan trọng là các nhà nghiên cứu phải đánh giá liệu những lỗi này có thể ảnh hưởng đến kết luận của họ hay không.

Tài liệu Bổ sung

7 Phương pháp
7.1 Mô hình Đa phương thức Nhật Bản
Các mô hình đa phương thức Nhật Bản được khởi tạo với một checkpoint CLIP tiếng Nhật (Makoto Sheen, 2022). CLIP tiếng Nhật được huấn luyện với loss CLIP tiêu chuẩn (Radford et al., 2021) nhưng sử dụng bộ mã hóa văn bản dựa trên BERT và transformer thị giác được khởi tạo bằng các trọng số từ mô hình AugReg ViT-B/16 (Steiner et al., 2021).

Dữ liệu Tổng hợp
Cả tiền huấn luyện ngôn ngữ-hình ảnh và huấn luyện có giám sát của CLIPPINGS đều sử dụng dữ liệu tổng hợp. Để tạo dữ liệu tổng hợp, chúng tôi render một danh sách các từ tiếng Nhật phổ biến thành hình ảnh sử dụng các font khác nhau (một loại augmentation), áp dụng các phép tăng cường hình ảnh, và đưa các hình ảnh kết quả vào OCR. Đối với cùng một từ, điều này tạo ra các góc nhìn khác nhau của hình ảnh từ do các phép tăng cường khác nhau, cũng như các góc nhìn khác nhau của văn bản từ do các lỗi OCR khác nhau được gây ra bởi các phép tăng cường.

Chúng tôi lấy mẫu ngẫu nhiên một cặp hình ảnh-văn bản mỗi nhãn (từ) và sử dụng tập con này cho tiền huấn luyện ngôn ngữ-hình ảnh. Để huấn luyện CLIPPINGS, chúng tôi huấn luyện trên bộ dữ liệu tổng hợp đầy đủ và sau đó tinh chỉnh trên dữ liệu có nhãn của chúng tôi.

Chi tiết Huấn luyện Khác
Các cắt văn bản là các hình chữ nhật định hướng dọc hoặc ngang mỏng, với tỷ lệ khung hình rất đa dạng, trong khi các bộ mã hóa thị giác hầu như luôn được huấn luyện trên hình ảnh vuông. Cắt trung tâm sẽ loại bỏ thông tin quan trọng và thay đổi kích thước thường làm biến dạng hình ảnh một cách tệ hại, cho rằng tỷ lệ khung hình cơ bản xa với hình vuông. Để bảo toàn tỷ lệ khung hình, chúng tôi đệm cắt hình chữ nhật với giá trị trung vị của pixel biên sao cho vùng văn bản luôn được căn giữa.

Đối với tiền huấn luyện ngôn ngữ-hình ảnh, loss CLIP tiêu chuẩn được sử dụng để căn chỉnh các bộ mã hóa hình ảnh và văn bản (Radford et al., 2021). Loss Tương phản Có giám sát (Khosla et al., 2020) được sử dụng cho tất cả huấn luyện có giám sát. Chúng tôi sử dụng bộ tối ưu AdamW cho tất cả huấn luyện mô hình cùng với một bộ lập lịch tốc độ học (LR) Cosine Annealing với Warm Restarts nơi LR tối đa được chỉ định cho mỗi lần chạy và LR tối thiểu được đặt về 0. 10 bước được chọn cho khởi động lại đầu tiên với hệ số khởi động lại là 2 tăng gấp đôi thời gian khởi động lại sau mỗi lần khởi động lại. Một epoch bao gồm lấy mẫu m góc nhìn (cặp hình ảnh-văn bản) của mỗi nhãn trong bộ dữ liệu và đi qua từng cái một lần. Phải mất 24 giờ để thực hiện tiền huấn luyện ngôn ngữ-hình ảnh, 10 giờ để thực hiện huấn luyện có giám sát sử dụng dữ liệu tổng hợp và 40 phút để huấn luyện trên dữ liệu có nhãn - tổng thời gian huấn luyện 34.6 giờ để huấn luyện CLIPPINGS trên một card GPU A6000 duy nhất. Các siêu tham số và chi tiết bổ sung về huấn luyện mô hình được liệt kê trong Bảng S-1.

Tại thời điểm suy luận, chúng tôi sử dụng IndexIPFlat từ FAISS (Johnson et al., 2019) để tìm láng giềng gần nhất trên các embedding được chuẩn hóa L2.

Khai thác Negative Khó
CLIPPINGS được huấn luyện trên một card GPU A6000 duy nhất, có thể vừa kích thước batch B là 153 cặp hình ảnh-văn bản. Điều này so với kích thước batch 32,768 được sử dụng để huấn luyện CLIP gốc (Radford et al., 2021) trên 256 GPU V100. Khai thác negative khó ngoại tuyến được sử dụng để đạt được đủ negative trong batch với kích thước batch nhỏ có thể vừa trong các thiết lập tính toán thực tế cho các người dùng hạ nguồn đa dạng.

Định nghĩa dữ liệu như một bộ ba (xn, tn, yn), nơi xn∈X là hình ảnh, tn∈T là văn bản, và yn∈Y là một nhãn liên kết (lớp). Gọi D là tập hợp tất cả các bộ ba. Đối với tiền huấn luyện có giám sát sử dụng dữ liệu tổng hợp, chúng tôi lấy mẫu ngẫu nhiên một cặp hình ảnh-văn bản mỗi nhãn trong D để tạo thành D′⊂D. Đối với mỗi cặp hình ảnh-văn bản (xa, ta) trong D′, chúng tôi sử dụng mô hình CLIP được điều chỉnh domain để tìm k cặp láng giềng gần nhất (xk, tk) của nó (bao gồm chính nó). Điều này cho chúng tôi k−1 láng giềng gần nhất cho mỗi nhãn ya trong D′⊂D.

Nhãn anchor ya và k−1 láng giềng của nó tạo thành một tập "negative khó". Trong một batch, chúng tôi lấy mẫu m = 3 góc nhìn của các cặp hình ảnh-văn bản có thay thế. Kích thước batch chia hết cho k∗m có thể vừa B/(k∗m) lớp duy nhất, mỗi lớp với m góc nhìn riêng và m góc nhìn của tất cả k láng giềng. Chúng tôi xáo trộn các tập negative khó và phân chia chúng thành các nhóm B/(k∗m) sao cho mỗi nhóm có thể tạo thành một batch. Mỗi lớp thành phần có k láng giềng và m góc nhìn trong minibatch. Đối với bước tiếp theo - tinh chỉnh với dữ liệu có nhãn - chúng tôi theo cách tiếp cận tương tự nhưng với checkpoint mô hình tốt nhất từ tiền huấn luyện tổng hợp.

7.2 Vision Transformer
Chúng tôi khởi tạo các trọng số của Vision Transformer từ checkpoint được tiền huấn luyện DINO cho ViT/B16 (Caron et al., 2021). Các siêu tham số và chi tiết huấn luyện khác được liệt kê trong Bảng S-1.

Như đối với CLIPPINGS, huấn luyện ViT sử dụng dữ liệu tổng hợp. Cùng pipeline như trên được sử dụng để tạo ra các hình ảnh có nhiễu tổng hợp. Đối với mỗi từ trong danh sách các từ tiếng Nhật, văn bản được render sử dụng các font khác nhau và được tăng cường để tạo ra các góc nhìn có nhiễu tổng hợp.

Khai thác negative khó ngoại tuyến được sử dụng để huấn luyện ViT. Cách tiếp cận tương tự như được mô tả ở trên. Chúng tôi sử dụng một checkpoint được tiền huấn luyện để tìm k láng giềng gần nhất cho mỗi lớp. Điều này được sử dụng để tạo một batch chứa m góc nhìn của lớp đó, cùng với m góc nhìn của B/m−1 lớp khác. Khi sử dụng negative khó, chúng tôi thay thế "k-1" của những lớp khác này bằng các lớp láng giềng gần nhất của anchor.

8 Kết quả Bổ sung
Bảng S-2 kiểm tra việc bao gồm các thực thể không có khớp trong thư mục công ty, ví dụ, một loạt các cơ quan chính phủ. Mặc dù hiệu suất giảm phần nào so với kết quả được báo cáo trong văn bản chính, các so sánh tương đối giữa các mô hình vẫn giữ nguyên.

Bảng S-1: Siêu tham số Huấn luyện: lr là tốc độ học tối đa, B là kích thước batch, w_decay là weight decay AdamW, im_wt là trọng số của embedding hình ảnh trong embedding gộp, m là số góc nhìn được lấy mẫu trong mỗi epoch, k là số láng giềng gần nhất trong một tập negative khó, và epochs là số epoch. nm_threshold là một tuple với hai ngưỡng độ tương tự được tinh chỉnh (cho OCR có nhiễu và sạch tương ứng) dưới đó một láng giềng được truy xuất được coi là không khớp với bất kỳ hình ảnh đích nào.

Bảng S-2: Bao gồm các thực thể không có khớp: Bảng này báo cáo độ chính xác trên tập kiểm tra sử dụng nhiều phương pháp khác nhau để liên kết các công ty Nhật Bản từ bản ghi chuỗi cung ứng với một thư mục công ty lớn. OCR có nhiễu sử dụng Google Cloud Vision có sẵn cho OCR, và OCR sạch sử dụng một OCR được huấn luyện tùy chỉnh chính xác.

Tài liệu Tham khảo
[Các tài liệu tham khảo được liệt kê chi tiết với các tác giả, tiêu đề và thông tin xuất bản]
