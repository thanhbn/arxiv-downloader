# 2310.00582.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.00582.pdf
# File size: 4117728 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs
Shiyu Xuan1*, Qingpei Guo2, Ming Yang2, Shiliang Zhang1
1National Key Laboratory for Multimedia Information Processing,
School of Computer Science, Peking University, Beijing, China.
2Ant Group
shiyu xuan@stu.pku.edu.cn, {qingpei.gqp, m.yang }@antgroup.com, slzhang.jdl@pku.edu.cn
Abstract
Multi-modal Large Language Models (MLLMs) have
shown remarkable capabilities in various multi-modal tasks.
Nevertheless, their performance in fine-grained image un-
derstanding tasks is still limited. To address this issue, this
paper proposes a new framework to enhance the fine-grained
image understanding abilities of MLLMs. Specifically, we
present a new method for constructing the instruction tun-
ing dataset at a low cost by leveraging annotations in ex-
isting datasets. A self-consistent bootstrapping method is
also introduced to extend existing dense object annotations
into high-quality referring-expression-bounding-box pairs.
These methods enable the generation of high-quality instruc-
tion data which includes a wide range of fundamental abili-
ties essential for fine-grained image perception. Moreover,
we argue that the visual encoder should be tuned during
instruction tuning to mitigate the gap between full image
perception and fine-grained image perception. Experimen-
tal results demonstrate the superior performance of our
method. For instance, our model exhibits a 5.2% accu-
racy improvement over Qwen-VL on GQA and surpasses
the accuracy of Kosmos-2 by 24.7% on RefCOCO val. We
have also attained the top rank on the leaderboard of MM-
Bench. This promising performance is achieved by train-
ing on only publicly available data, making it easily re-
producible. The models, datasets, and codes are publicly
available at https://github.com/SY-Xuan/Pink .
1. Introduction
Large Language Models (LLMs) [ 2,30,31,35] show im-
pressive capabilities across a wide range of natural language
tasks. These inspiring results have motivated researchers
to extend LLMs to Multi-modal Large Language Models
*This work was done during the internship of the first author at Ant
Group.
VQAv2
IconQA
VSR
OK-VQA
GQA RefCOCO(testB)RefCOCO+(testB)RefCOCOg(test)MMBench
76.677.277.878.4
28.833.638.443.2
56.659.261.864.4
54.6
56.2
57.8
59.450.6
54.2
57.8
61.481.6
83.2
84.8
86.473.6
75.2
76.8
78.484.085.086.087.0 43.851.659.467.2Pink
Qwen-VL
LLaV A-1.5
Shikra
Instruct-BLIP
Qwen
-VLLLaV A
-1.5Shikra Instruct
-BLIPPink100101102103104
#PT Data
#IT Data
#Trainable Param.Figure 1. With fewer trainable parameters and less training data,
Pink achieves the best performance on both conventional multi-
modal tasks and RC tasks. “#Trainable Param.”, “#PT Data”, and
“#IT Data” indicate the number of trainable parameters, the number
of samples in pre-training and instruction tuning stage, respectively.
(MLLMs) by integrating additional modalities, e.g., image,
audio, or point cloud. Visual instruction tuning [ 6,20,45],
using high-quality image-text instruction tuning data, allows
the incorporation of visual comprehension ability into LLMs
by projecting visual features into the natural language space
of the LLMs [ 46]. Powered by those methods, existing
MLLMs are capable of basic image-level comprehension.
However, they are still confronted by fine-grained image
understanding [ 13,38,39]. Limited fine-grained image un-
derstanding ability hinders the performance of MLLMs in
multi-modal tasks and restricts their potential applications
as reported in the GPT-4V(ision) test report [44].
To address this issue, some methods [ 4,27] incorporate
some datasets related to Referential Comprehension (RC)
such as RefCOCO [ 13], and PointQA [ 24] to enhance the
fine-grained image perception ability of MLLMs. How-
ever, these datasets are insufficient to cover a wide range of
abilities that MLLMs desire to have for fine-grained image
perception. Limited RC tasks also make it hard for the model
to generalize across various RC tasks through instruction
tuning. For instance, as shown in Fig. 4, Shikra [ 4] and
Qwen-VL [ 1] show limited instruction-following ability on
RC tasks beyond its instruction tuning, failing to provide
1arXiv:2310.00582v3  [cs.CV]  13 Mar 2024

--- PAGE 2 ---
relevant responses to questions.
In addition to instruction tuning, the capability of the
visual encoder is also important to the performance of
MLLMs. MLLMs typically employ a visual encoder trained
through contrastive language-image pre-training like the one
in CLIP [ 29]. Simply performing global alignment is ineffec-
tive in exploring fine-grained relationships between image
regions and text words [ 42,49]. The visual encoder stands
as a bottleneck for achieving fine-grained image perception
in MLLMs.
This paper proposes a new framework to enhance the
fine-grained image perception ability of MLLMs through
RC tasks. We refer to the trained model as Pink
1. Fine-
grained image understanding is closely tied to some funda-
mental abilities such as instance identification and recogni-
tion of relative positions between different instances. Inte-
grating tasks that demand these fundamental abilities dur-
ing instruction tuning is crucial for enhancing the model’s
fine-grained image perception ability. To this end, we pro-
pose a new dataset construction pipeline that extends the
annotations of existing datasets to various RC tasks about
these fundamental abilities. Specifically, we design sev-
eral RC tasks, such as visual relation reasoning and visual
spatial reasoning, based on the annotations from Visual
Genome [ 14]. To further incorporate more training data
for these RC tasks, we introduce a novel self-consistent
bootstrapping method to extend dense object annotations
to referring-expression-bounding-box pairs. Compared to
the expensive, and uncontrollable process of generating data
using the GPT4 API [ 4,48], our method leverages the ex-
isting annotations from datasets. This approach results in
high-quality data and precise enhancement of the necessary
capabilities required by the model. As shown in Fig. 1, the
high-quality instruction tuning data generated by our method
enables the model to achieve promising performance with a
reduced number of training samples.
Improving the fine-grained image understanding ability
of the visual encoder is not a trivial task. Most existing
MLLMs [ 4,6,20,45,50] freeze the visual encoder during
instruction tuning. Because directly tuning the visual en-
coder can result in a semantic loss due to the limited scale
of the visual instruction dataset [ 40]. To address this issue,
we tune the visual encoder by introducing several tunable
components like Adapters [ 10] and LoRA [ 11]. Freezing the
main parameters of the model avoids forgetting the learned
knowledge. The introduced tunable components are trained
to adapt the visual encoder.
We have conducted extensive experiments to test the per-
formance of the model. Benefited by the designed tasks in
the instruction tuning, our framework enhances MLLMs’
performance in both conventional vision-language tasks and
1This name is from the main character of the album The Wall by the
great rock band Pink Floyd .RC tasks. For instance, with only 6.7M tunable parame-
ters, we achieve up to a 6.0% accuracy improvement on
OK-VQA [ 26] compared to Shikra [ 4]. We also attain the
top rank on the leaderboard of MMBench [ 21]. It should be
noted that, our method also surpasses methods that rely on
more training data, e.g., Qwen-VL [1].
This work is an original effort to enhance MLLMs’ fine-
grained image perception ability by addressing two main
bottlenecks: limited instruction tuning tasks and the lack of
ability of the visual encoder. By designing tasks related to
fundamental abilities, every dataset with corresponding an-
notations can be converted into the instruction tuning dataset.
The self-consistent bootstrapping further increases the num-
ber of training data. This dataset construction pipeline sig-
nificantly reduces the cost of obtaining high-quality data
with diversified tasks and eliminates the dependency on
GPT4 APIs. The whole training pipeline is reproducible
in academia as it only relies on publicly available data and
can be trained on consumer GPUs with 24GB memory. We
will release the codes and datasets to facilitate further re-
search and evaluation.
2. Related Works
Multi-modal large language model. Several approaches
have been proposed to condition LLMs with additional
modalities. Typically, these methods utilize two-stage train-
ing. The pre-training stage is performed to align two modali-
ties with image-text pairs. The subsequent stage is adopted
to improve the ability of MLLMs to follow instructions
with high-quality instruction tuning dataset [ 18,20,43,50].
Many methods freeze the visual encoder during the pre-
training stage to reduce requirement of large-scale image-
text pairs. For example, Mini-GPT4 [ 50] and LLaV A [ 20]
only fine-tune a single fully connected layer to align the
vision and language modalities. Other methods leverage mil-
lions of image-text pairs to achieve better alignment between
two modalities. Instruct-BLIP [ 6] introduces an instruction-
aware visual feature extraction method and fine-tunes the
entire Q-Former, showing promising zero-shot generaliza-
tion ability on various multi-modal tasks. mPlug-Owl [ 45]
incorporates a visual abstractor module to align the two-
modalities. Both the visual encoder and the visual abstrac-
tor are updated during the pre-training stage. All of above
methods freeze the visual encoder during the multi-modal
instruction tuning stage to prevent the potential semantic loss
caused by the small-scale instruction tuning dataset. How-
ever, this strategy makes the visual encoder cannot benefit
from the multi-modal instruction tuning.
Referential Comprehension of MLLMs. Referential com-
prehension is important to the fine-grained image perception
of MLLMs. Therefore, enhancing MLLMs with the RC
ability is highly valuable. Inspired by Pix2Seq [ 5], many
works use discrete coordinate tokens to encode spatial in-
2

--- PAGE 3 ---
formation and unify RC tasks as sequence generation tasks,
e.g., OFA [ 41], Unified-io [ 22], and Kosmos-2 [ 27]. An-
other line of works, as seen in PVIT [3] and GPT4RoI [47],
leverage the ROI operation [ 9] to extract features of referring
objects. These works require extra modules and may lose
context information. Another limitation of these works is
that they cannot refer objects in their responses, limiting
their applications, e.g., in visual grounding.
In addition to the model design, the construction of RC in-
struction tuning data also plays a crucial role. Shikra [ 4] con-
verts existing datasets of RC tasks including RefCOCO [ 13]
and PointQA [ 24] into the instruction following format.
Kosmos-2 uses the grounding model GLIP [ 16] to extract
coordinates of noun chunks in image captions and constructs
a large-scale dataset. Datasets constructed by the above
methods only includes RC tasks, such as visual grounding,
grounding caption and pointQA, which are still not diversi-
fied enough to cover various RC tasks. The trained models
thus show a poor ability to generalize to new RC tasks be-
yond the instruction tuning. ChatSpot [ 48], PVIT [ 3], and
Shikra [ 4] all prompt GPT4 to generate instruction tuning
data for RC, which is expensive and uncontrollable.
Differences with previous works. Existing methods for
enhancing MLLMs through RC tasks construct the instruc-
tion tuning dataset by either integrating existing RC datasets
or relying on GPT4 APIs. However, these methods exhibit
some major drawbacks: 1) the diversity of RC tasks can-
not cover a wide range of fundamental abilities, and 2) data
generation through GPT4 APIs is expensive, uncontrollable,
and prone to noise. In contrast, our work effectively lever-
ages existing datasets to cover a wide variety of RC tasks.
The proposed self-consistent bootstrapping method extends
dense object annotations to referring-expression-bounding-
box pairs. This pipeline significantly reduces the cost of gen-
erating high-quality datasets. The high qualify of the data
allows our model to be trained with fewer parameters on less
training data, which is friendly to reproduce in academia,
than large commercial MLLMs.
3. Methodology
3.1. Model Architecture and Training Pipeline
Model architecture. As shown in Fig. 2, Pink follows
a similar architecture of LLaV A [ 20], which consists of a
visual encoder Φ𝑉, a projection layer Φ𝑃, and a decoder-
only LLMΦ𝐿. Given an image 𝐼and a sequence of word
embeddings 𝑄𝑇representing an instruction sentence, the
visual encoder is employed to embed the image as a sequence
of visual tokens 𝑍𝑉=Φ𝑉(𝐼). A linear layer is used as Φ𝑃to
convert𝑍𝑉into the input space of the LLM 𝑍𝑇=Φ𝑃(𝑍𝑉).
𝑍𝑇and𝑄𝑇are concatenated and fed into Φ𝐿to generate the
next word.
To enable the LLMs to take coordinates as input and
AdapterAdapterProjectionImage tokensInstruction textLocateall the objects that have a relation of on with <object>.[0.336,0.197,0.404,0.423]LLMVisual Encoder
Figure 2. The illustration of our Pink model. Pink follows the archi-
tecture of LLaV A [ 20], which consists of three main components:
a visual encoder, a projection layer, and a decoder-only LLM. The
coordinates of a bounding box are converted into texts in a specific
format. During instruction tuning, we freeze the visual encoder and
LLM and only update the Adapters and the projection layer.
output, similar to Shikra [ 4], the coordinates are converted
into texts in a specific format. Specifically, for a bound-
ing box represented by its coordinates of the top-left and
bottom-right corners [𝑥𝑚𝑖𝑛,𝑦𝑚𝑖𝑛,𝑥𝑚𝑎𝑥,𝑦𝑚𝑎𝑥], the coordi-
nates are normalized to the range [0,1]with respect to the
image size and retain 3 decimal places for each number, e.g.,
[0.222,0.333,0.444,0.555]. This design allows the coordi-
nates to be processed as regular text and can appear in both
the input and output of the model.
The visual encoder pre-trained by the contrastive loss [ 29]
lacks region-level image comprehension. Directly fine-
tuning the entire visual encoder during instruction tuning
could lead a semantic loss due to the limited instruction
tuning data [40]. To incorporate the fine-grained image per-
ception ability of the visual encoder through the multi-modal
instruction tuning with RC tasks, we freeze the visual en-
coder, meanwhile introducing tunable modules into it. This
approach prevents the visual encoder from suffering seman-
tic loss and provides an efficient way to adapt the model. In
particular, we employ the Adapter [ 10] at both the visual
encoder and LLM. Given an input token feature 𝑍∈R𝑑, the
architecture of an Adapter is defined as follows,
ˆ𝑍=𝜎(𝑍𝑊𝑑)𝑊𝑢+𝑍, (1)
where𝑊𝑑∈R𝑑×𝑑𝑠and𝑊𝑢∈R𝑑𝑠×𝑑denote the weight
matrices,𝑑𝑠is the hidden dimension which is much smaller
than𝑑, and𝜎denotes the non-linear activation function. 𝑊𝑢
is initialized to zero to ensure that at the beginning of the
training, the Adapter does not change the original output.
Training pipeline. Both the image and coordinates are
mapped into the input space of the LLM. Consequently, the
model can be trained end-to-end using a language model-
3

--- PAGE 4 ---
ing task, which predicts the next word token based on the
preceding context.
The model is trained in two stages. In the first stage, we
exclusively fine-tune the projection layer with a small set
of image-text pairs. In the second stage, we freeze both
the visual encoder and LLM and fine-tune the newly added
Adapters and the projection layer with the instruction tuning
dataset. Therefore, both the visual and language modalities
can benefit from the multi-modal instruction tuning.
3.2. Instruction tuning Dataset Construction
To create the instruction tuning dataset, we unify all the
multi-modal tasks into a vision-language dialogue format:
Image: {Image tokens }
User: {Instruction template }
Assistant: {Response }
where the placeholders {Image tokens },{Instruction tem-
plate }, and {Response }will be replaced with the image
tokens extracted by Φ𝑉, task instruction template, and the
response, respectively.
It is important to introduce diverse RC tasks for the in-
struction tuning to cover a wide range of fundamental abili-
ties for the find-grained image perception of MLLMs. Exist-
ing datasets only offer limited RC tasks, e.g., visual ground-
ing, grounding caption [ 13], and pointQA [ 24]. Besides the
RC tasks mentioned above, we design more diversified RC
tasks by incorporating annotations from Visual Genome [ 14],
which contain information about region descriptions, objects,
and relations between different objects. Following parts pro-
ceed to introduce those tasks.
Visual relation reasoning. Visual Genome has annotated
millions of relationship triplets ( subject-predicate-object ),
e.g., man-wearing-hat. We design two types of visual rela-
tion reasoning tasks by leveraging these annotations to help
the model understand visual relationships between different
objects: (1) We randomly select a relationship triplet. Given
the coordinates of subject andobject , the model is required
to predict their relation. (2) We randomly select one subject
and a relation from the annotations. The model is required
to detect all objects that have selected relation with subject
and output their coordinates and class names.
Coarse visual spatial reasoning. We introduce a coarse
visual spatial reasoning task by utilizing the object annota-
tions from Visual Genome. This task enhances MLLMs to
identify relative spatial relation between different instances.
We define four coarse spatial positions as top-left, top-right,
bottom-left, and bottom-right . Given a randomly selected
object and a coarse spatial position, the model is required
to identify all objects located at this position relative to the
selected object and predict their coordinates and class names.Object counting. To endow the model with the concept of
different instances and the capability of fine-grained object
recognition, we design an object counting task. This task
requires the model to count objects in the image that belong
to the same category as the given object or class name.
Object detection. Object detection can empower the model
to locate the position and boundaries of objects. This task
is also important for the model to identify the existence or
category of a certain object. Given a class name or a selected
object, the model is asked to identify all objects that belong
to the same category of the given object or class name, and
provide their coordinates.
By incorporating these RC tasks into the instruction tun-
ing, the model can learn a variety of RC abilities. To clarify
the designed tasks, we list some instruction templates as
follows,
Visual Relation Reasoning:
User: Assist me in finding the relation between
<subject >and<object>in the photo.
Assistant: <relation >.
User: Please locate and categorize all the ob-
jects that have a relation of <relation >with
<subject >.
Assistant: <object> <category > <object> <cate-
gory>.
Coarse Visual Spatial Reasoning:
User: Identify the objects located at <loc>of
<object>.
Assistant: <object> <category > <object> <cate-
gory>.
Object Counting:
User: How many objects in the image are of the
same category as <object>.
Assistant: <number >.
Object Detection:
User: Identify all the objects that fit the same
category as <object>and display their coordinates.
Assistant: <object> <object>.
where the placeholders <object>and<category >will be
replaced with the bounding box coordinates and the class
name of a referring object, respectively. <subject >will
be replaced with the bounding box coordinates of the se-
lected subject. <relation >,<loc>, and<number >will
be replaced with the relation between different objects, the
selected relative spatial position, and the number of the ob-
jects, respectively. All instruction templates can be found in
Supplementary Material.
4

--- PAGE 5 ---
MLLMA laptop on the desk
Grounding captionVisual groundingFigure 3. The illustration of self-consistent bootstrapping method.
Given a bounding box, our method first generates its description
by asking MLLM to perform grounding caption, then leverages
the visual grounding to locate the generated description. The low-
quality description will be filtered if the IOU between the predicted
and ground-truth bounding box is below a threshold.
3.3. Self-consistent Bootstrapping Method
The constructed instruction-following datasets are adopted
to reinforce the fine-grained image perception ability of
MLLMs. We further acquire more high quality data. Exist-
ing datasets for object detection provide valuable bounding
box annotations for objects appearing in the image, making
them promising resources for instruction tuning. We propose
a self-consistent bootstrapping method by leveraging those
datasets. This method extends bounding box annotations to
the referring-expression-bounding-box pairs. It comprises
two key stages: bounding box description bootstrapping and
self-consistent filtering as shown in Fig. 3.
At the bounding box description bootstrapping stage,
given a bounding box 𝐵of an object, we prompt the model
to generate a description 𝐷𝐵for that object by leveraging
its ability of grounding caption. Due to the complexity of
scenes or the presence of duplicate objects, the generated
description may be noisy or fails to uniquely describe the cor-
responding object. Then, the self-consistent filtering stage is
adopted to filter those low-quality descriptions. Specifically,
with the generated description 𝐷𝐵, we locate this description
in the image and predict the bounding box ˆ𝐵by leveraging
the visual grounding ability of our model. The generated
description will be removed if the Intersection Over Union
(IOU) between 𝐵andˆ𝐵is below a pre-defined threshold 𝜆.
This stage ensures that only high-quality descriptions are
retained.
These two stages are performed to extend every annotated
object in the dataset with textual description. This extended
dataset is then well-suited for a wide range of RC tasks,
e.g., coarse visual spatial reasoning, object detection, object
counting, visual grounding and grounding caption. Illustra-
tions of generated data are in Supplementary Material. This
self-consistent bootstrapping method serves as a powerful
tool to harness the potential of object detection datasets for
enhancing the RC ability.
Identify the text in [0.368,0.219,0.625,0.380].The text is "ACM".The letters are white.
Identify the text in [0.368,0.609,0.625,0.680].The text is ”1899".number on the shirt.
I want to know the coordinates and name of the object grabbed by Du Feng in the image.
The object grabbed by Du Feng is a bottle [0.322,0.631,0.392,0.835].In the image, there are two men standing next to each other, both wearing red and white shirts. One of the men is holding a drink in a bottle, while the other one is smiling. They seem to be interacting with each other, possibly discussing a sports game or simply socializing.
(a)
(b)
I cannot provide the coordinates and name of the object grabbed by Du Feng in the image, as there is no existence of any object being grabbed in the image.
ACM
1899(c)
Identify all objects that can control [0.000,0.150,0.350,0.600] in the image.
Objects that can control [0.000,0.150,0.350,0.600]in the image are a mouse and a keyboard.
Yes, there is a laptop computer sitting on the desk.
The objects that can control [0.000,0.150,0.350,0.600] are a laptop computer and a keyboard.Figure 4. A comparison of Pink
 against the recent MLLMs
Shikra
 and Qwen-VL
 on RC reasoning.
3.4. Qualitative Results on RC Reasoning
In Fig. 4, we compare some qualitative results on RC rea-
soning with Shikra [ 4] and Qwen-VL [ 1]. Pink exhibits
substantially better capability on these RC tasks.
To answer the question as shown in Fig. 4 (a), the model
needs to first identify Du Feng, a famous Chinese basket-
5

--- PAGE 6 ---
ball player, then understand the action of grabbing. Pink
successfully provides the correct answer. Pink also exhibit
better reasoning capability as shown in Fig. 4 (b). It success-
fully identifies the referred region as laptop, and inferred
that, the mouse and keyboard in the image could control the
labtop. Shikra fails to follow the instruction and provides
an un-related answer. Similarly, Qwen-VL only outputs the
category of the referred object.
As shown in Fig. 4 (c), trained with millions of OCR-
based data in the instruction tuning, Qwen-VL can give
correct responses to OCR instruction. It is interesting to
observe that, without any OCR-based data, our model also
accurately recognizes characters located at referred region.
It indicates that, Pink exhibits promising generalization ca-
pability to different RC tasks. More qualitative results can
be found in Supplementary Material. Extensive experiments
are conducted in following section.
4. Experiments
4.1. Experimental Setting
Model architecture. We employ the ViT-L/14 [ 7] as the
visual encoder, which is pre-trained with CLIP [ 29]. We
choose an instruction-tuned model Vicuna-7B [ 37] based
on LLaMA-1 [ 35] as the LLM. The projection layer is a
single fully connected layer. The Adapters are inserted be-
fore each self-attention layer of both the visual encoder and
the LLM, with a hidden dimension 𝑑𝑠=8. The tunable
parameter numbers of Adapter in the visual encoder and
LLM are 393,216 and 2,097,152, respectively. The number
of parameters in the projection layer is 4,194,304. Therefore,
the total number of tunable parameters is about 6.7M.
Training data. The first stage utilizes 595K image-text pairs
from CC3M [ 34], the same as LLaV A [ 20]. The second stage
adopts VQAv2 [ 8], LLaV A-150K [ 20], A-OKVQA [ 32],
Flickr30K [ 28], Visual Genome [ 14] and Object365 [ 33]
with referring-expression-bounding-box pairs generated by
our self-consistent bootstrapping method. At each training
iteration, when using an image in Visual Genome or Ob-
ject365, one designed RC task will be selected randomly.
The model used to generate referring-expression-bounding-
box pairs in Object365 is trained with the aforementioned
datasets, excluding Object365 itself. Note that we reduce
the probability of sampling Object365 in batch construction
to avoid a large number of training samples in Object365
dominating the training.
Training details. AdamW is adopted as the optimizer. In
the first stage, the model is trained for 1 epoch with a batch
size of 128 and weight decay of 0.0. After a warm-up period
of 200 steps, the learning rate starts at 0.03 and decays to
0 with the cosine schedule. In the second stage, the model
is trained for 6 epochs with a batch size of 32 and weight
decay of 0.05. The warm-up phase consists of 10k steps andthe learning rate starts at 5e-4. The input image is resized to
224×224without any additional data-augmentation. We set
𝜆as 0.5 to filter out low-quality descriptions. The model is
trained using 8 NVIDIA A100 GPUs. It takes about 1 and
30 hours for the first and second stage, respectively.
Evaluation settings. We evaluate our model on various
datasets under the zero-shot and fine-tuning settings to val-
idate the instruction-following ability of the trained model.
These datasets encompass conventional multi-modal reason-
ing tasks, including conventional VQA (VQAv2 [ 8], MM-
Bench [ 21]), abstract diagram understanding (IconQA [ 23]),
visual spatial reasoning (VSR [ 17]), knowledge-intensive
VQA (OK-VQA [ 26]), scene understanding (GQA [ 12]),
and RC tasks such as RefCOCO/+ [ 13], RefCOCOg [ 25],
Visual-7W [51], PointQA-Local/LookTwice [24].
4.2. Ablation Study
Instruction tuning dataset construction. To investigate the
impact of instruction tuning dataset construction, we conduct
ablation studies by excluding Visual Genome, designed RC
tasks, and Object365 with referring-expression-bounding-
box pairs. Results are summarized in Table 1.
As shown in Table 1, enhancing the MLLM with RC
task can benefit the conventional multi-modal reasoning
tasks, e.g., removing visual grounding and grounding cap-
tion tasks leads to a performance degradation of 2.8% on
VSR. When the model is trained solely with visual ground-
ing and grounding caption, it fails to provide correct re-
sponses to the questions in PointQA-Local/LookTwice, in-
dicating limited instruction-following ability for RC tasks.
As more RC tasks are included, the model begins to ex-
hibit better instruction-following ability for these tasks. The
performance on PointQA-Local/LookTwice increases from
0.0%/0.2% to 54.6%/63.1%. The combination of all de-
signed RC tasks yields the best performance on both con-
ventional multi-modal reasoning tasks and RC tasks, thus
validating the effectiveness of our instruction tuning dataset
construction method. We also observe that incorporating
Object365 further enhances the performance of our method
on RC tasks. For example, on RefCOCO val, the zero-shot
accuracy increases from 54.1% to 77.0%. Notably, the exclu-
sion of the self-consistent method results in the degradation
of performance from 77.0% to 73.8% on RefCOCO val due
to low-quality referring-expression-bounding-box pairs gen-
erated by the model. These results demonstrate the impor-
tance of the generated data and underscore the importance
of the proposed self-consistent method.
Training settings of visual encoder. We further assess
the effectiveness of different settings for training the visual
encoder, and summarize results in Table 1.
Full-tuning the visual encoder results in a significant per-
formance degradation. The performance on RefCOCO val
drops from 54.1% to 0.05%. This result aligns with the
6

--- PAGE 7 ---
Settings IconQA VSR OK-VQA RefCOCO val Local LookTwice
Baseline 44.6 65.6 58.5 55.0 0.0 0.2
w/o VG 43.1 62.8 58.3 - - -
+ R 44.4 65.7 58.5 52.1 17.1 12.8
+ R,S 46.2 65.8 58.5 52.7 50.9 60.0
+ R,S,C 47.4 65.7 58.9 53.1 53.4 60.7
+ R,S,C,D 47.8 66.3 59.5 54.1 54.6 63.1
+ R,S,C,D + Object365 † 44.6 65.9 58.7 73.8 52.1 69.2
+ R,S,C,D + Object365 47.7 67.1 59.5 77.0 57.2 70.3
Freezing 42.9 61.5 58.3 37.2 44.9 57.5
Full-tuning 36.9 48.6 33.1 0.05 26.1 54.1
LoRA 44.3 65.4 58.9 54.7 56.7 62.2
Our 47.8 66.3 59.5 54.1 54.6 63.1
Table 1. Ablation study on instruction tuning dataset construction and training settings of visual encoder under a zero-shot setting. “Baseline”
denotes leveraging Visual Genome by only performing visual grounding and grounding caption tasks. “VG” denotes Visual Genome.
“R”, “S”, “C”, and “D” denote the visual relation reasoning, coarse visual spatial reasoning, object counting and object detection tasks,
respectively.†denotes generated referring-expression-bounding-box pairs in Object365 are not filtered with the self-consistent method.
“Freezing” and “Full-tuning” denotes freezing the visual encoder and training the entire visual encoder, respectively. “LoRA” denotes using
LoRA instead of the Adapter to perform parameter-efficient tuning.
conclusion in [ 40] that fine-tuning the visual encoder using
a small-scale instruction tuning dataset can lead to a sub-
sequent drop in performance. Freezing the visual encoder
also leads to performance degradation on various datasets. It
can be attributed to the limited ability of the visual encoder
to fine-grained image understanding. Our design allows
for the optimization of both modalities and leverages the
benefits of multi-modal instruction tuning, resulting in im-
proved performance. Moreover, using LoRA [ 11] instead of
the Adapter to perform parameter-efficient tuning can also
achieve improved performance compared with Full-tuning
or Freezing, demonstrating the effectiveness of adapting the
visual encoder during multi-modal instruction tuning.
4.3. Comparison with Recent Works
This section proceeds to validate the effectiveness of our
method through comparison with other recent works.
Evaluation on the conventional multi-modal reasoning
tasks. To evaluate the instruction-following ability of our
method, we conduct experiments of five multi-modal rea-
soning tasks on public benchmarks. These benchmarks and
tasks assess various aspects of multi-modal comprehension
ability of the model. As shown in Table 2a, our model con-
sistently achieves the best performance using fewer trainable
parameters, a smaller training set, and lower input image
resolution. This demonstrates that the constructed instruc-
tion tuning dataset can effectively enhance the fine-grained
perception capability.
Zero-shot evaluation on the visual grounding task. Vi-
sual grounding is a fundamental RC task that requires the
model to predict the coordinates of a bounding box based on
a given textual description. We evaluate our model on threewell-established datasets under the zero-shot setting in Ta-
ble 2b. Our model significantly outperforms Kosmos-2 [ 27],
which is trained with a generated dataset GRIT containing
91M images and 115M referring expressions. This result
validates the effectiveness of the proposed self-consistent
bootstrapping method.
Comparison with other models under fine-tuning setting
on RC tasks. To further validate the RC ability of our
method, we compare it with other models [ 1,4,41] that can
perform RC tasks. The comparison incorporates models that
have the capability to handle various vision-language tasks.
Models specifically designed for the visual grounding task
are not included. The instruction tuning dataset of compared
models includes the training sets of datasets listed in Table 2c.
For a fair comparison, we also leverage those training sets in
the instruction tuning of Pink. Note that, the training set size
of Pink is still substantially smaller than those in compared
methods, e.g., 50M of Qwen-VL [1] vs.519K of Pink.
The results in Table 2c show that our model obtains
promising performance under the fine-tuning setting. This
can be attributed to the diversity of RC tasks in the instruc-
tion tuning. Object365 further improves the performance,
even training sets of these datasets are already included.
This demonstrates the effectiveness of self-consistent boot-
strapping method in converting existing dataset into more
valuable RC training set. Qwen-VL utilizes a stronger visual
encoder ViT-G trained by CLIP. To make a fair compari-
son, we change the visual encoder of Pink to ViT-G. Pink-G
outperforms Qwen-VL by large margins, even with a lower
input image resolution and less training samples. This result
further validates the effectiveness of our training pipeline.
Comparison with other models on MMBench test set.
7

--- PAGE 8 ---
Models Res. #PT Data #IT Data #Trainable Param. VQAv2 IconQA VSR OK-VQA GQA
Instruct-BLIP [6] 224 129M 1.2M 188M - 43.1 54.3 - 49.2
Shikra-7B [4] 224 595K 5.5M 7B 76.7† 24.3 63.3 53.5 47.4
Pink 224 595K 396K 6.7M 78.7† 47.8 66.3 59.5 52.6
Qwen-VL [1] 448 1.4B 50M 8B 78.8† - - 58.6 † 59.3†
LLaV A-1.5 [19] 336 558K 665K 7B 78.5† - - - 62.0 †
Pink+ 224 595K 477K 6.7M 78.8† 48.8 67.4 60.6† 64.5†
(a) Results on the conventional multi-modal reasoning tasks. †denotes the training set of corresponding dataset is included. + denotes
adding the training set of OK-VQA and GQA during instruction tuning. “Res.”, “#Trainable Param.”, “#PT Data”, and “#IT Data” indicate
input image resolution, the number of trainable parameters, the number of samples in pre-training and instruction tuning stage, respectively.
ModelsRefCOCO RefCOCO+ RefCOCOg
val testA testB val testA testB val test
Kosmos-2 [27] 52.3 57.4 47.3 45.5 50.7 42.2 60.6 61.7
Pink 54.1 61.2 44.2 43.9 50.7 35.0 59.1 60.1
Pink∗77.0 82.4 68.2 65.6 75.2 53.4 72.4 74.0
(b) Zero-shot results on the visual grounding task.
Models Visual Encoder Res.RefCOCO RefCOCO+ RefCOCOgVisual-7W LookTwiceval testA testB val testA testB val test
OFA-L [41] ResNet152 480 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6 - -
Shikra-7B [4] ViT-L 224 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2 84.3 72.1
Pink ViT-L 224 88.3 91.7 84.0 81.4 87.5 73.7 83.7 83.7 85.1 73.5
Pink∗ViT-L 224 88.7 92.1 84.0 81.8 88.2 73.9 83.9 84.3 85.3 73.6
Qwen-VL [1] ViT-G 448 89.4 92.3 85.3 83.1 88.3 77.2 85.6 85.5 - -
Pink-G ViT-G 224 91.5 93.4 88.0 86.0 89.5 79.8 86.8 87.8 86.8 76.6
(c) Fine-tuning results on the RC tasks. Pink-G indicates the ViT-G is used as the visual encoder for a fair comparison.
Models Overall LR AR RR FP-S FP-C CP
Kosmos-2 [27] 58.2 48.6 59.9 34.7 65.6 47.9 70.4
LLaV A-1.5 [19] 59.5 32.4 72.6 49.3 62.3 52.2 67.7
Qwen-VL [1] 61.8 40.5 74.3 47.9 66.3 46.2 72.8
mPlug-Owl [45] 68.5 56.8 77.9 62.0 72.0 58.4 72.6
Pink 74.1 58.5 78.2 73.2 77.3 67.2 78.7
(d) CircularEval results on MMBench test set [21].
Table 2. Comparison with other methods. * denotes Object365 with generated referring-expression-bounding-box pairs is used.
MMBench [ 21] has been proposed as a new benchmark to
evaluate various abilities of MLLMs, including logical rea-
soning (LR), attribute reasoning (AR), relation reasoning
(RR), fine-grained perception single instance (FP-S), fine-
grained perception cross instance (FP-C), and coarse percep-
tion (CP). We hence conduct experiments on MMBench to
validate the capabilities of our model in all aspects.
The results summarized in Table 2d show that our method
achieves the best overall performance among compared
methods. The main improvement comes from RR, FP-S,
and FP-C. For example, compared with mPlug-Owl [ 45],
the accuracy boosting of Pink on LR and FP-C is 1.7% and
8.8%, respectively. These results demonstrate the strong fine-
grained perception ability of Pink, which can be attributed to
the incorporation of various RC tasks during the multi-modal
instruction tuning.5. Conclusion
This paper presents a novel framework for enhancing fine-
grained image perception ability of MLLMs. The framework
includes a method for constructing an instruction tuning
dataset by converting annotations from existing datasets
into diverse RC tasks. A self-consistent bootstrapping
method is proposed to extend object annotations to referring-
expression-bounding-box pairs, enabling the acquisition of
more instruction tuning data at a low cost. The visual encoder
is tuned in a parameter-efficient way to gain fine-grained im-
age understanding ability. With fewer trainable parameters
and less training data, our method achieves superior perfor-
mance on both multi-modal tasks and RC tasks.
Acknowledgement This work is supported in part by Natural
Science Foundation of China under Grant No. U20B2052,
61936011, in part by the Okawa Foundation Research Award,
and in part by Ant Group Research Intern Program.
8

--- PAGE 9 ---
References
[1]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 1,
2, 5, 7, 8
[2]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot
learners. In NeurIPS , 2020. 1
[3]Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,
Maosong Sun, and Yang Liu. Position-enhanced visual in-
struction tuning for multimodal large language models, 2023.
3
[4]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s
referential dialogue magic. arXiv preprint arXiv:2306.15195 ,
2023. 1, 2, 3, 5, 7, 8
[5]Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and
Geoffrey Hinton. Pix2seq: A language modeling framework
for object detection. arXiv preprint arXiv:2109.10852 , 2021.
2
[6]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,
and Steven Hoi. Instructblip: Towards general-purpose vision-
language models with instruction tuning, 2023. 1, 2, 8
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR . OpenReview.net, 2021. 6
[8]Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answering.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 6904–6913, 2017. 6, 1
[9]Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE international
conference on computer vision , pages 2961–2969, 2017. 3
[10] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790–2799. PMLR, 2019. 2, 3
[11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In
ICLR . OpenReview.net, 2022. 2, 7
[12] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositionalquestion answering. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
6700–6709, 2019. 6, 1
[13] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. Referitgame: Referring to objects in pho-
tographs of natural scenes. In Proceedings of the 2014 con-
ference on empirical methods in natural language processing
(EMNLP) , pages 787–798, 2014. 1, 3, 4, 6
[14] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 2, 4, 6, 1
[15] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking mul-
timodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125 , 2023. 1, 2
[16] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan,
Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-
image pre-training. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10965–10975, 2022. 3
[17] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial
reasoning. Transactions of the Association for Computational
Linguistics , 11:635–651, 2023. 6, 1
[18] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXiv preprint
arXiv:2306.14565 , 2023. 2
[19] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 8
[20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. CoRR , abs/2304.08485, 2023. 1, 2,
3, 6
[21] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, et al. Mmbench: Is your multi-modal model an
all-around player? arXiv preprint arXiv:2307.06281 , 2023.
2, 6, 8, 1
[22] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-io: A unified model
for vision, language, and multi-modal tasks. arXiv preprint
arXiv:2206.08916 , 2022. 3
[23] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei
Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa:
A new benchmark for abstract diagram understanding and
visual language reasoning. arXiv preprint arXiv:2110.13214 ,
2021. 6, 1
[24] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Rus-
sakovsky. Point and ask: Incorporating pointing into visual
question answering. arXiv preprint arXiv:2011.13681 , 2020.
1, 3, 4, 6
[25] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Cam-
buru, Alan L Yuille, and Kevin Murphy. Generation and
9

--- PAGE 10 ---
comprehension of unambiguous object descriptions. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition , pages 11–20, 2016. 6, 1
[26] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. Ok-vqa: A visual question answering
benchmark requiring external knowledge. In Proceedings
of the IEEE/cvf conference on computer vision and pattern
recognition , pages 3195–3204, 2019. 2, 6, 1
[27] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824 , 2023. 1, 3, 7, 8, 2
[28] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
Flickr30k entities: Collecting region-to-phrase correspon-
dences for richer image-to-sentence models. In Proceedings
of the IEEE international conference on computer vision ,
pages 2641–2649, 2015. 6, 1
[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 2, 3, 6
[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. J. Mach. Learn. Res. , 21:
140:1–140:67, 2020. 1
[31] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn ´e,
Alexandra Sasha Luccioni, Fran c ¸ois Yvon, Matthias Gall ´e,
Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert
Webson, Pawan Sasanka Ammanamanchi, Thomas Wang,
Beno ˆıt Sagot, Niklas Muennighoff, Albert Villanova del
Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, An-
gelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile
Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo
Lauren c ¸on, Yacine Jernite, Julien Launay, Margaret Mitchell,
Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Al-
ham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg
Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christo-
pher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa
Adelani, and et al. BLOOM: A 176b-parameter open-access
multilingual language model. CoRR , abs/2211.05100, 2022.
1
[32] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A bench-
mark for visual question answering using world knowledge.
InEuropean Conference on Computer Vision , pages 146–162.
Springer, 2022. 6, 1
[33] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A
large-scale, high-quality dataset for object detection. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 8430–8439, 2019. 6, 1
[34] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, imagealt-text dataset for automatic image captioning. In ACL (1) ,
pages 2556–2565. Association for Computational Linguistics,
2018. 6
[35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur ´elien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
Lample. Llama: Open and efficient foundation language
models. CoRR , abs/2302.13971, 2023. 1, 6
[36] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-
jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya
Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 , 2023. 1, 2
[37] Vicuna. Vicuna: An open chatbot impressing gpt-4. https:
//github.com/lm-sys/FastChat , 2023. 6
[38] Dongkai Wang and Shiliang Zhang. Contextual instance
decoupling for instance-level human analysis. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 2023.
1
[39] Dongkai Wang, Shiliang Zhang, Yaowei Wang, Yonghong
Tian, Tiejun Huang, and Wen Gao. Humvis: Human-centric
visual analysis system. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 9396–9398,
2023. 1
[40] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankan-
halli, and Ying Shan. What makes for good visual tokenizers
for large language models?, 2023. 2, 3, 7
[41] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and modal-
ities through a simple sequence-to-sequence learning frame-
work. In International Conference on Machine Learning ,
pages 23318–23340. PMLR, 2022. 3, 7, 8
[42] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li,
Wentao Liu, and Chen Change Loy. Clipself: Vision trans-
former distills itself for open-vocabulary dense prediction.
arXiv preprint arXiv:2310.01403 , 2023. 2
[43] Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley.
Baize: An open-source chat model with parameter-efficient
tuning on self-chat data. CoRR , abs/2304.01196, 2023. 2
[44] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn
of lmms: Preliminary explorations with gpt-4v (ision). arXiv
preprint arXiv:2309.17421 , 9, 2023. 1
[45] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 1, 2, 8
[46] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,
Tong Xu, and Enhong Chen. A survey on multimodal large
language models. arXiv preprint arXiv:2306.13549 , 2023. 1
[47] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi:
Instruction tuning large language model on region-of-interest.
arXiv preprint arXiv:2307.03601 , 2023. 3
10

--- PAGE 11 ---
[48] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei,
Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,
Chunrui Han, et al. Chatspot: Bootstrapping multimodal
llms via precise referring instruction tuning. arXiv preprint
arXiv:2307.09474 , 2023. 2, 3
[49] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li,
Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai,
Lu Yuan, Yin Li, et al. Regionclip: Region-based language-
image pretraining. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
16793–16803, 2022. 2
[50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models, 2023. 2
[51] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
Visual7w: Grounded question answering in images. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition , pages 4995–5004, 2016. 6, 1
11

--- PAGE 12 ---
Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs
Supplementary Material
Stage Dataset Data Number
PT LLaV A-CC3M-Pretrain-595K [20] 595K
IT LLaV A-158K [20] 158K
IT VQAv2 [8] 83K
IT A-OKVQA [32] 17K
IT Visual Genome [14] 108K
IT Flickr30K [28] 30K
IT Object365∗[33] 1M
Table 3. Statistics of training datasets. PT and IT denote the pre-
training and instruction tuning stage, respectively.
A.1. Discussion of Limitations
Our method relies on the LLM. Therefore, it has some short-
comings from the LLM, such as bias or unfair response, and
hallucination. Like most MLLMs with RC ability, we also
find that our model is not good at object detection in complex
scenarios, e.g., identifying multiple tiny objects in the image.
It may be because the input resolution of the image is low.
However, increasing the input resolution of the image is not
a trivial task. Many efforts are still needed to deal with such
tasks.
A.2. Statistics of Training and Evaluation Datasets
The statistics for the training datasets and evaluation datasets
are summarized in Table 3 and Table 4, respectively. We also
provide an overview of the generated referring-expression-
bounding-box pairs in Object365 in Table 5. To ensure the
quality of data, we apply certain filters during preprocess-
ing. Firstly, we exclude images containing more than 15
objects. Moreover, for the purpose of bounding box descrip-
tion bootstrapping, we only consider objects that cover an
area of more than 2,000 pixels. As a result, our dataset
comprises 1,063,034 images, with a total of 4,961,822 gen-
erated referring-expression-bounding-box pairs. To further
enhance the reliability of our dataset, we perform a self-
consistent method that filters out 2,528,619 low-quality
referring-expression-bounding-box pairs.
A.3. More experimental results
The results that leverages a more advanced LLM LLaMA-
2 [36] are shown in Table 6 and Table 7. We can observe an
improvement in performance on most datasets.
Similar to MMBench, SEED-Bench [ 15] is a recently
proposed benchmark to evaluate the abilities of MLLMs
on various evaluation dimensions including Scene Under-
standing (SU), Instance Identity (IId), Instance Attribute
(IA), Instance Location (IL), Instance Counting (IC), SpatialDataset Split Metric
VQAv2 [8] test-dev VQA Score
IconQA [23] multi-text-choice test Accuracy
VSR [17] zero-shot test VQA Score
OK-VQA [26] val VQA Score
GQA [12] test-dev VQA Score
RefCOCO [13] val & testA & testB Accuracy
RefCOCO+ [13] val & testA & testB Accuracy
RefCOCOg [25] val & test Accuracy
PointQA-Local [24] test-dev VQA Score
PointQA-LookTwice [24] test VQA Score
Visual-7W [51] which box test Accuracy
MMBench [21] test Accuracy
Table 4. Summary of evaluation datasets.
Images Referring-expressions Avg Expression Length
1,063,034 2,433,203 3.6
Table 5. Statistics of generated referring-expression-bounding-box
pairs in Object365.
Models VQAv2 IconQA VSR OK-VQA GQA
Pink 78.7† 47.8 66.3 59.5 52.6
Pink-LLaMA-2 78.8† 49.1 67.9 60.2 52.0
Table 6. Results on the conventional multi-modal reasoning tasks
with LLaMA-2 [ 36] as the LLM.†denotes the training set of
corresponding dataset is included.
Relation (SR), Instance Interaction (IIn), Visual Reasoning
(VR), and Text Recognition (TR). We further conduct exper-
iment on this benchmark. The results are shown in Table 8.
Pink also exhibits the best performance among compared
MLLMs. Our model has significant advantages in IIn, SR,
and IL tasks, which require strong fine-grained image per-
ception ability of MLLMs. These results further validate
the effectiveness of our method to enhance the fine-grained
perception ability of MLLMs.
A.4. Qualitative Analysis
We show qualitative results on different types of vision-
language tasks and RC tasks. The results of mPlug-Owl [ 45],
Shikra [ 4] and Qwen-VL [ 1] are from their official online
demos.
Knowledge-intensive QA. Fig 5 depicts an example of this
task, where the model is required to identify an album cover
and provide an introduction for the album. mPlug-Owl and
1

--- PAGE 13 ---
ModelsRefCOCO RefCOCO+ RefCOCOgVisual-7W LookTwiceval testA testB val testA testB val test
Pink 88.3 91.7 84.0 81.4 87.5 73.7 83.7 83.7 85.1 73.5
Pink-LLaMA-2 89.0 92.1 84.6 82.6 88.3 74.5 83.8 84.4 85.1 73.8
Table 7. Fine-tuning results on RC tasks with LLaMA-2 [36] as the LLM.
Models Overall SU IId IA IL IC SR IIn VR TR
Kosmos-2 [27] 54.4 63.4 57.1 58.5 44.0 41.4 37.9 55.7 60.7 25.9
Instruct-BLIP [6] 58.8 60.2 58.9 65.6 43.6 57.2 40.3 52.6 47.7 43.5
mPlug-Owl [45] 37.9 49.7 45.3 32.5 36.7 27.3 32.7 44.3 54.7 28.8
Qwen-VL [1] 62.3 71.2 66.4 67.7 53.5 44.8 43.8 62.9 74.9 51.2
Pink 66.2 73.1 69.1 69.1 60.5 55.2 51.0 76.3 70.4 58.8
Pink-LLaMA-2 67.0 75.2 70.1 70.1 63.3 53.8 50.2 69.1 74.3 50.0
Table 8. Results on the image split of SEED-Bench [15].
Qwen-VL can identify the album The Wall correctly but
begins to make hallucination when introducing this album.
Some introductions provide by mPlug-Owl do not align with
the actual facts associated with the album. The answer of
Shikra focuses on the content of the image, indicating a
poor instruction-following ability of Shikra. In contrast,
our model not only correctly identifies the album but also
delivers an accurate and relevant introduction, showcasing
its strong instruction-following capability.
Fine-grained QA. As shown in Fig. 6, mPlug-Owl fails to
output the correct number of people in the image. It may be
because that its lack of RC ability hinders its fine-grained
understanding of images. Despite Shikra has RC ability, it
also fails to give a correct answer. This result can validate
that trained with limited RC tasks Shikra struggles to achieve
a fine-grained perception of images. In contrast, our model
makes a correct answer to the question, demonstrating its
strong capability in fine-grained image perception.
Referential comprehension reasoning. We show a case of
RC reasoning in Fig. 7. In this case, our model accurately
locates Steve Nash. Shikra seems to be unaware of who is
Steve Nash. Surprisingly, Shikra’s output coordinates point
to the face of Dirk Nowitzki. We also need to point out the
result of Shikra is generated using the instruction template
that is identical to the one used during training. Shikra shows
limited instruction-following ability when using a template
that is different from the one used during training. Our
model also shows a ability for multi-round conversation. It
can understand what “him” refers to in the second question.
As Shikra makes a wrong response for the first question, in
the second question, we give the correct coordinates man-
ually. However, Shikra fails to follow the instructions and
begins to hallucination. Qwen-VL fails to locate Steve Nash
correctly. In the multi-round conversation, it also gives a
wrong response. These results can further validate the RC
Tell me about the album.The image features a brick wall with the words "Pink Floyd" written on it, along with the phrase "Pink Floyd Wall." This could be a reference to the band's iconic album "The Wall," which was released in 1979. The album is known for its unique artwork and storyline, which features a character named Pink, who builds a wall to separate himself from the world. The album's cover art features a brick wall with the band's name written on it, and the brick wall in the image could be a nod to this iconic design.The album "The Wall" is a classic rock masterpiece by the British rock band Pink Floyd. Released in 1979, it is the band's ninth studio album and one of their most successful, selling over 23 million copies worldwide. The conceptual album is divided into 3 parts: "Part One: The Dark Side of the Moon," "Breathe," and "On the Run," each with its own theme and musical style.The album explores the isolation and loneliness of its protagonist, Piggy, who is stuck behind "the wall," a symbol for the barriers that people create in their lives. It is considered one the most influential and iconic rock albums of all time, and its themes of alienation, mental health, politics, war, love, death, time and space continue to resonate with listeners decades after its release.(a) Our Method
(b) mPlug-OwlPink Floyd wall, with white lettering on a brick wall, reads "Pink Floyd the Wall." The artwork is a black-and-white photo of a white brick wall, reminiscent of the iconic album cover of the same name. This design is likely a representation of the band's famous album, which was released in 1989 and became an enduring symbol of the music industry. The wall serves as a backdrop for the band's logo and artwork, creating a striking visual element that represents the iconic album.
(c) Shikra"The Wall" is the eleventh studio album by the English rock band Pink Floyd, released in 1979. It is the band's best-selling album and one of the best-selling albums of all time. The album features a concept album that tells the story of a man named Pink, whose life is falling apart and whose emotional defenses are gradually rebuilding until he is able to face the world again. The album's most famous song is "Another Brick in the Wall", which is a protest against the education system in England. The album cover features a white brick wall with the album title written in black ink.
(d) Qwen-VLFigure 5. A comparison of knowledge-intensive QA. We mark the
hallucination part in red.
ability of our model.
More qualitative results. More qualitative results of our
model are shown in Fig. 8. Our model demonstrates the abil-
ity to generate unique descriptions with contextual informa-
tion when provided with coordinates of a specific area. For
instance, instead of simply outputting “helmet”, our model
uses “woman’s” features for differentiation. Moreover, our
2

--- PAGE 14 ---
How many people in the image?There are seven people in the image.(a) Our MethodThere are 9 people in the image, including the main characters of the show.(b) mPlug-Owl
There are ninepeople in the image.(c) ShikraThere are seven people in the image.(d) Qwen-VL
Figure 6. A comparison of fine-grained QA.
I want to know the coordinates of Steve Nash in the image.[0.440,0.180,0.998,0.998]
The man on the left of Steve Nash is named Dirk Nowitzki.Tell me the name of the man who is on the left of him.
Can you point out Steve Nashin the image and provide the coordinates of its location?[0.270,0.068,0.484,0.328]
A photo of Tyson Wellsleyand Mark Jackson, who are talking on the court. So the answer is tysonwellsley.Tell me the name of the man who is on the left of [0.394,0.084,0.998,0.998]?
(a) Our Method
(b) Shikra
I want to know the coordinates of Steve Nash in the image.Steve Nash
The man on the left of him is named Steve Nash.Tell me the name of the man who is on the left of him.
(c) Qwen-VL
Figure 7. A comparison of referential comprehension reasoning.
We mark the hallucination part in red.
model successfully locates items mentioned in descriptions
that require outside knowledge. For example, it can correctly
identify what can be drunk in the image. In the last two
cases, our model not only provides correct answers but also
locates the mentioned items in the image. This ability can
achieve more applications.
We also present qualitative results of our model on multi-
round conversation about RC in Fig. 9 and Fig. 10. Our
method can understand the complex referential relationships
in the dialogue context, e.g., it, her and this instrument. As
shown in Fig. 10, although the model initially gives the
wrong answer about what is holding by the man, when it is
told that its answer is incorrect, the model can make correc-
Generate  a description of the region 
[0.285,0.270,0.393,0.427] in image.
woman's helmet .
Locate  the girl in white on right in the image?
[0.836,0.555,0.936,0.899] .
Identify and locate what can be drunk in the image.
[0.610,0.560,0.996,0.928]
What is unusual about this image? You need to locate the items you reference.A man [0.402,0.123,0.652,0.754]in a yellow shirt [0.450,0.189,0.642,0.420] is ironing clothes [0.284,0.414,0.652,0.598] on the side of a yellow car [0.366,0.309,0.998,0.889].
What do you see in this image? I also want the coordinates of each object.A room with a bed [0.000,0.568,0.180,0.853] and a table [0.334,0.781,0.600,0.997] with a plant [0.350,0.403,0.530,0.819] in front of a window [0.212,0.000,0.850,0.755].Figure 8. More qualitative results of our model.
tions and provide the correct answer. This result can further
demonstrate the promising instruction-following ability of
our model. Additionally, these qualitative results highlight
that the integration of RC ability significantly expands the
range of tasks our model can successfully handle, thereby
broadening its application potential.
Qualitative results of generated referring-expression-
bounding-box pairs. We illustrate some examples of gen-
erated referring-expression-bounding-box pairs in Fig. 11.
For each object in the image, the bounding box descrip-
tion bootstrapping method can generate a description related
to that object. Most of generated descriptions are correct.
However, some generated descriptions exhibit incorrect or
ambiguous descriptions that fail to uniquely identify an ob-
ject. As shown in Fig. 11 (a) and (b), when leveraging visual
grounding to locate these descriptions in the image, IOU
between the predicted bounding box and the ground-truth
bounding box is low. Our self-consistent filtering method
can effectively filter them out. These results can further
validate the effectiveness of our self-consistent bootstrap-
3

--- PAGE 15 ---
Identify the category of the object located at [0.350,0.390,0.400,0.450].The object is a strawberry.
How to plant it?
To plant strawberries, you need to prepare the soil by adding compost or well-rotted manure to improve the soil structure and fertility. Then, dig a hole about 12 inches deep and wide enough to accommodate the roots of the strawberry plant. Place the plant in the hole, making sure the crown of the plant is at the same level as the soil. Fill the hole with soil, gently tamping it down to remove any air pockets. Water the plant well, and then mulch the soil around the base of the plant to retain moisture and suppress weeds. Finally, water the plant regularly and provide support as needed, such as using a trellis or stake, to help the plant grow and bear fruit.
Locate right girl in the image.[0.480,0.432,0.590,0.703]
What is in her hand?She is holding a violin.
Further introduce this instrument to me.The violin is a stringed instrument that is played by bowing or plucking its strings. It is a member of the violin family, which includes other stringed instruments such as the viola, cello, and bass. The violin is known for its versatility, as it can be used in various musical genres, including classical, jazz, and rock. It is often played in orchestras, ensembles, and solo performances.
Is there any other instrument belonging to the violin family in the image?Yes, there is a cello in the image, which is another instrument belonging to the violin family.Figure 9. Qualitative results of our model on multi-round conversa-
tion.
Can you provide the coordinates of the man in the image?
[0.232,0.339,0.422,0.904]
What instrument he is holding in his hand?
He is holding a violin in his hand.
You are wrong.
I apologize for the mistake. The man is holding a cello in 
his hand.
Figure 10. Qualitative results of our model on multi-round conver-
sation with correction.
ping method to extend the object annotations to high-quality
referring-expression-bounding-box pairs.
A.5. Instruction Templates
Instruction templates of visual relation reasoning. We list
all the instruction templates of visual relation reasoning task
below. For the Task2, we use different instruction templates
to ask the model output different contents (coordinates, class
name, or both coordinates and class name).
chair with woman in white shirt A woman in a black shirt and blue shorts sitting in a chair woman in blackwoman in white shirt A man in a white shirt 
man in black A white pole with a white ball on itA man in a white shirt and tan shorts swinging a bat A white shoe A white shoe 
A black hat on a man's head (a)
(b)
red car A silver car green car 
(c)Figure 11. Qualitative results of generated referring-expression-
bounding-box pairs. The solid rectangle represents the ground-truth
bounding box. The dashed rectangle represents the bounding box
generated by our model with visual grounding according to the
generated description.
 denotes the generated description is
filtered out by the proposed self-consistent method.
Instruction templates of visual relation reasoning-Task1
What is the relation between <subject >and<object>?
Describe the relation between <subject >and<object>?
Assist me in finding the relation between <subject >and
<object>in the photo.
In the given image, could you find and tell me the relation
between <subject >and<object>?
I need help identifying the relation between <sub-
ject>and<object>. Can you point it out in this image?
What is the relation between <subject >and<object>in
this picture?
Could you describe the relation between <subject >and
<object>in this image?
I’m having trouble identifying the relation between <sub-
ject>and<object>. Could you clarify it for me in this
image?
Can you help me understand the relationship between
<subject >and<object>in this image?
I’m trying to understand the relation between <sub-
ject>and<object>. Can you help me by describing
it?
I need some assistance in identifying the relation between
<subject >and<object>in this image.
In this image, can you find and describe the relation be-
tween<subject >and<object>for me?
Could you please explain the relation between <sub-
ject>and<object>.4

--- PAGE 16 ---
Instruction templates of visual relation reasoning-Task2#1
Assist me in locating the position of all the objects <rela-
tion>the<subject >?
I want to know the coordinates of all the objects <rela-
tion>the<subject >?
Detect all the objects have a relationship <relation >with
the<subject >and output there locations.
There are some objects that are <relation >the<subject >.
Could you tell me there locations?
Identify all the objects that have a relationship <rela-
tion>with the <subject >. Where are they located?
Please locate all the objects that are <relation >the<sub-
ject>and provide their coordinates.
Find all the objects that have a relation of <relation >with
the<subject >. Can you give me their positions?
Point out the objects that are <relation >the<subject >.
Where can I find them?
I need to locate all the objects that are <relation >the
<subject >. Can you assist me with this task?
Could you help me find all the objects that have a relation
of<relation >with the <subject >? Please provide their
locations.
Please detect all the objects that are <relation >the<sub-
ject>. Output their positions.
Identify and provide the coordinates of all objects that are
<relation >the<subject >.
Find the objects that have a relation of <relation >with
the<subject >. Where are they situated?
What objects have the relation of <relation >with the
<subject >? Could you locate them for me?
Can you help me locate all the objects that are <rela-
tion>the<subject >and give me their positions?
Output the positions of all objects that have a relation of
<relation >with the <subject >.
Identify the objects that are <relation >the<subject >.
Where are they located?
Please locate all the objects that are <relation >the<sub-
ject>and provide their positions.Instruction templates of visual relation reasoning-Task2#2
Assist me in identifying the categories of all the objects
<relation >by the <subject >?
Detect all the objects <relation >by the <subject >and
output there categories, respectively.
There are some objects that are <relation >the<subject >.
Could you tell me there categories?
I want to know the categories of all the objects whose
relation is <relation >with the <subject >?
Identify the object categories that are <relation >the
<subject >.
Find all objects that are related to <subject >using the
relationship <relation >, and categorize them.
Your task is to recognize and classify all objects that are
<relation >by the <subject >.
Please determine the categories of all objects that are <re-
lation>by the <subject >.
Can you identify the categories of objects <relation >the
<subject >?
Your job is to identify all objects that <relation >the<sub-
ject>and list their categories.
Detect and categorize all objects that are <relation >the
<subject >.
I need you to determine the categories of all objects that
<relation >the<subject >.
Identify and classify all objects that are <relation >the
<subject >.
Please identify the categories of all objects that are <rela-
tion>the<subject >.
Please help me identify the object categories whose rela-
tionship is <relation >with<subject >.
Recognize and categorize all objects that are related to
<subject >using the relationship <relation >.
I need you to categorize all objects that is related to <sub-
ject>with relationship as <relation >.
5

--- PAGE 17 ---
Instruction templates of visual relation reasoning-Task2#3
Your task is to locate all objects that have a relation <rela-
tion>with<subject >and classify them.
I need you to categorize and locate all objects that is re-
lated to <subject >with relationship as <relation >.
Please locate and categorize all the objects that have a
relation of <relation >with<subject >.
Assist me in locating and classifying all the objects <rela-
tion>the<subject >?
Find all the objects that have a relation of <relation >with
the<subject >. Can you give me their positions and cate-
gories
Your task is to locate all objects that have a relation <rela-
tion>with<subject >and classify them.
I need you to categorize and locate all objects that is re-
lated to <subject >with relationship as <relation >.
Please locate and categorize all the objects that have a
relation of <relation >with<subject >.
Assist me in locating and classifying the position of all the
objects <relation >the<subject >?
Find all the objects that have a relation of <relation >with
the<subject >. Can you give me their positions and cate-
gories?
Your task is to locate and classify all objects that are re-
lated to <subject >using the relationship <relation >.
I need you to locate and categorize all objects having a
relationship <relation >with the given <subject >.
Find all objects related to <subject >with the relationship
<relation >. Categorize and locate them for me.
Your objective is to locate and classify the objects that are
related to <subject >through the relationship <relation >.
I require you to detect and categorize all objects that have
a relationship <relation >with<subject >.
Please find and classify all objects that has a relationship
<relation >with<subject >.
Assist me in locating and categorizing all objects that re-
lated to <subject >with the relationship <relation >.
Find all objects that are related to <subject >using the
relationship <relation >. Categorize and locate their posi-
tions.
Your task is to identify and classify all objects related to
<subject >through the relationship <relation >.
I need you to locate and categorize all objects that have a
relationship <relation >with<subject >.
Assist me in locating and classifying all objects that are
related to <subject >through the relationship <relation >.
Find all the objects that has a relationship <relation >with
<subject >. Categorize and locate their positions for me.
Instruction templates of coarse visual spatial reasoning.
We list all the instruction templates of coarse visual spatial
reasoning task below. Similar to visual relation reasoning,
different instruction templates are used to ask the model
output different contents.Instruction templates of coarse visual spatial reasoning#1
Identify the objects located at <loc>of<object>. Please
classify them by category and provide their locations.
I need to know what objects are present at <loc>of<ob-
ject>. Can you help me locate and categorize them?
Find all the objects at <loc>of<object>. Please provide
me with their categories and locations.
I want to know the categories and positions of the objects
located at <loc>of<object>.
Locate and classify all the objects at <loc>of<object>.
Could you tell me the categories and positions of the ob-
jects present at <loc>of<object>?
Help me locate and categorize all the objects at <loc>of
<object>.
I need to know the categories and locations of the objects
at<loc>of<object>.
What are the categories and positions of the objects located
at<loc>of<object>?
Identify and locate all the objects at <loc>of<object>.
I need their categories and positions.
I want to know the categories and positions of the objects
at<loc>of<object>.
Locate and classify all the objects at <loc>of<object>.
Please provide me with their categories and positions.
Instruction templates of coarse visual spatial reasoning#2
What are the categories of the objects located at <loc>of
<object>?
Detect and classify all the objects at <loc>of<object>.
I need to know their categories.
Please find and categorize all the objects present at
<loc>of<object>.
Give the categories of all the objects you can find at
<loc>of<object>.
I need you to find and categorize all the objects that are at
<loc>of<object>.
Please provide me with the categories of all the objects
present at <loc>of<object>.
What types of objects are located at <loc>of<object>?
Please list their categories.
Please find all the objects at <loc>of<object>and give
me their categories.
What are the categories of the objects that are present at
<loc>of<object>?
I need you to classify all the objects located at <loc>of
<object>.
Please give me the categories of all the objects that are
located at <loc>of<object>.
6

--- PAGE 18 ---
Instruction templates of coarse visual spatial reasoning#3
What are the coordinates of the objects located at <loc>of
<object>?
Detect and give the coordinates of all the objects at
<loc>of<object>.
Please find and locate all the objects present at <loc>of
<object>.
Give the detail locations of all the objects you can find at
of<loc> <object>.
Locate all the objects and give there coordinates found at
<loc>of<object>.
What are the positions of all the objects at <loc>of<ob-
ject>?
Can you find and list the positions of all the objects present
at<loc>of<object>?
Provide the coordinates of objects located at <loc>of
<object>.
List and indicate the positions of all objects at <loc>of
<object>.
Enumerate and specify the positions of all objects found
at<loc>of<object>.
What objects are situated at <loc>of<object>and
where precisely are they located?
What are the coordinates of all objects found at <loc>of
<object>?
Instruction templates of object counting. We list all the
instruction templates of object counting task below. <cate-
gory>will be replaced by the category name.Instruction templates of object counting#1
Can you tell me how many <category >are present in this
picture?
I need to know the number of <category >in this image.
Count how many <category >are in this picture.
Please determine the quantity of <category >shown in
this image.
How many instances of <category >can you find in this
picture?
I would like to know how many <category >are visible
in this image.
Count the number of <category >that you see in this pic-
ture.
Please provide me with the count of <category >in this
image.
How many objects of <category >are in this image?
Can you count the items of <category >in this picture?
What is the total number of <category >in this image?
How many <category >can you spot in this image?
Please determine the quantity of <category >in this im-
age.
Count the number of <category >that appear in this pic-
ture.
How many <category >are in the picture?
Counting the number of <category >appeared in the im-
age.
Please give me the number of <category >appeared in the
image.
Instruction templates of object counting#2
How many objects in the image are of the same category
as<object>?
Count the number of objects in the image that are similar
to<object>in category.
What is the total count of objects that share the same
category as <object>in the image?
How many objects in the image have the same category as
<object>?
Count all the objects in the image that fall under the same
category as <object>.
What is the number of objects that share the same category
as<object>in the image?
Count the objects that belong to the same category as
<object>in the image.
How many objects of the same category as the object
represented by <object>appear in the image?
Count all the instances whose category is the same as
<object>present in the image.
Instruction templates of object detection. We list all the
instruction templates of object detection task below. <cate-
gory>will be replaced by the category name.
7

--- PAGE 19 ---
Instruction templates of object detection#1
Locate and mark the positions of all <category >in the
image.
Find all the instances of <category >in the image and
indicate their respective locations.
Spot and record the coordinates of every <cate-
gory>present in the image.
Identify the <category >in the image and provide their
precise locations.
Can you determine the positions of all the <category >in
the image and list them?
Pinpoint the <category >in the image and give me their
exact coordinates.
Locate all the <category >in the image and provide their
locations in detail.
Detect and report the locations of all the <cate-
gory>present in the image.
Find and list the locations of every <category >in the
image.
Please identify the <category >in the image and give me
their locations.
Provide me with the precise locations of all the <cate-
gory>in the image.
Detect and record the positions of the <category >in the
image.
Spot all the instances of <category >in the image and
give me their coordinates.
Detect all the <category >in the image, and output there
location.
There are some <category >in the image, could you help
me to locate them and give me their coordinates.
What are the coordinates of the <category >in the image.
Give the detail locations of all the <category >you can
find in the image.Instruction templates of object detection#2
Locate all the items in the picture that share the same cate-
gory as <object>and provide their coordinates.
Spot every object that belongs to the same category as
<object>and indicate their positions.
Identify all the objects that fit the same category as <ob-
ject>and display their coordinates.
Find all the objects that have a similar classification as
<object>and output their locations.
Locate and report the coordinates of all the objects that
share the category with <object>.
Detect all the objects in the image that have the same clas-
sification as <object>and provide their positions.
Spot all the objects that belong to the same category as
<object>and show their coordinates.
Identify every instance that falls under the same category
as<object>and report their locations.
Find and output the coordinates of all the objects that have
the same category as <object>.
Locate all the objects in the picture that have a similar
classification as <object>and display their positions.
Detect and report the positions of all the objects that share
the category with <object>.
Spot every instance that has a similar classification as <ob-
ject>and indicate its coordinates.
Identify all the objects that have the same classification as
<object>and output their positions.
Find all the objects that belong to the same category as
<object>and report their locations.
Locate and output the coordinates of all the items that have
a similar category as <object>.
Detect all the instances in the image which have the same
category with <object>, and output there location.
Detect and report the locations of all the instances present
in the image, these instances should have similar category
with<object>.
Given an <object>, please help me to find all the in-
stances with the same category. The output should be the
coordinates of detected instances.
Instruction templates of multi-choices VQA. Our instruc-
tion tuning dataset also includes the multi-choices VQA,
e.g., A-OKVQA. Therefore, we also construct some instruc-
tion templates for this task and list them below. Placeholder
<options >will be replaced by the options.
8

--- PAGE 20 ---
Instruction templates of multi-choices VQA
Please take a look at the image and select the correct
answer for <question >from the options given below
\n<options >.
Examine the image and select the best matched answer
to the question: <question >from the options given be-
low\n<options >.
There are some options \n<options >. I have a question
for you: <question >Can you select the best matched
answers from the given options based on the image?
Regarding the image, you need to identify the correct
answer to the question <question >from the given op-
tions \n<options >.
Analyzing the image, can you identify the best matched an-
swer to <question >from the given options \n<options >.
Looking at the image, can you quickly answer my ques-
tion:<question >. Some potential answers are given in
the following options \n<options >.
Referring to the image, please select the answer for this
question: <question >from the options \n<options >.
Could you please check the image and select the answer for
my question: <question >from the options \n<options >.
Here is an image and a question: <question >for you.
Please select an option that can answer the question from
the given options \n<options >.
For this image, I want to know which option can an-
swer my question: <question >correctly. The options
are\n<options >.
Take a look at the image, can you select the best matched
answer to the following question: <question >from fol-
lowing options \n<options >.
Considering these options \n<options >. I need a correct
selection from these options that can answer this question:
<question >in regards to the image.
9
