# 2109.02008.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2109.02008.pdf
# Kích thước file: 653996 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Mô hình hóa Cross-token với Tính toán Có điều kiện
Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, Yang You
Đại học Quốc gia Singapore, Singapore
fyuxuanlou, f.xueg@u.nus.edu, zhengzangw@gmail.com, youy@comp.nus.edu.sg

Tóm tắt
Mixture-of-Experts (MoE), một kiến trúc tính toán có điều kiện, đã đạt được hiệu suất đầy hứa hẹn bằng cách mở rộng mô-đun cục bộ (tức là mạng feed-forward) của transformer. Tuy nhiên, việc mở rộng mô-đun cross-token (tức là self-attention) là thách thức do việc huấn luyện không ổn định. Công trình này đề xuất Sparse-MLP, một mô hình all-MLP áp dụng các MLP được kích hoạt thưa thớt cho mô hình hóa cross-token. Cụ thể, trong mỗi khối Sparse của mô hình all-MLP của chúng tôi, chúng tôi áp dụng hai giai đoạn của các lớp MoE: một với các chuyên gia MLP trộn thông tin trong các kênh dọc theo chiều patch hình ảnh, cái kia với các chuyên gia MLP trộn thông tin trong các patch dọc theo chiều kênh. Ngoài ra, bằng cách đề xuất chiến lược routing điểm tầm quan trọng cho MoE và thiết kế lại hình dạng biểu diễn hình ảnh, chúng tôi tiếp tục cải thiện hiệu quả tính toán của mô hình. Về mặt thử nghiệm, chúng tôi có hiệu quả tính toán hơn so với Vision Transformers với độ chính xác tương đương. Ngoài ra, các mô hình của chúng tôi có thể vượt trội hơn MLP-Mixer 2.5% về độ chính xác Top-1 ImageNet với ít tham số và chi phí tính toán hơn. Trên các tác vụ downstream, tức là Cifar10 và Cifar100, các mô hình của chúng tôi vẫn có thể đạt được hiệu suất tốt hơn so với các baseline.

1 Giới thiệu
Mixture of Experts (MoE) [Shazeer et al., 2017] là một kiến trúc tính toán có điều kiện mạnh mẽ để mở rộng transformer lên đến hàng nghìn tỷ tham số [Fedus et al., 2021]. Tuy nhiên, mặc dù MoE có thể mở rộng mô-đun cục bộ (tức là mạng feed-forward) trong transformer tốt, việc mở rộng mô-đun cross-token (tức là self-attention) là thách thức. Lý do là việc huấn luyện attention dựa trên MoE không ổn định và dễ phân kỳ [Fedus et al., 2021]. Trong công trình này, chúng tôi đề xuất Sparse-MLP, một mô hình all-MLP được kích hoạt thưa thớt có thể mở rộng cả mô-đun cục bộ và cross-token một cách hiệu quả.

Sparse-MLP bao gồm một chồng các khối Sparse và khối Dense. Trong mỗi khối Sparse, chúng tôi áp dụng các lớp MoE được kích hoạt thưa thớt ở hai giai đoạn. (1) Token-mixing MoE (MoE S), một thay thế hiệu quả tính toán cho self-attention, trộn thông tin qua các vị trí không gian của biểu diễn hình ảnh trong các kênh. (2) Channel-mixing MoE (MoE C)), một giai đoạn khác của tính toán có điều kiện, trộn thông tin trong các patch biểu diễn hình ảnh dọc theo các kênh. Trong các khối dense, MoE S và MoE C được đơn giản hóa thành các MLP dense. Bên cạnh đó, chúng tôi thực hiện hai cải tiến thêm trên kiến trúc MoE. Thứ nhất, chúng tôi đề xuất chiến lược routing điểm tầm quan trọng có thể giảm tính toán routing của MoE vanilla. Nó xếp hạng các token hoặc kênh để routing theo điểm tầm quan trọng của chúng. Thứ hai, chúng tôi thiết kế lại hình dạng biểu diễn hình ảnh trong các khối Sparse để mạng gating của token-mixing MoE có thể hoạt động với hiệu quả hơn.

Một đóng góp quan trọng của công trình chúng tôi là, theo hiểu biết tốt nhất của chúng tôi, chúng tôi đầu tiên đặt Mixture of Experts thưa thớt làm mô-đun mô hình hóa cross-token. Giải pháp cụ thể là sử dụng token-mixing MoE được đề xuất để mô hình hóa ngữ cảnh. Nói chung, chúng tôi xây dựng mô hình all-MLP với tính toán có điều kiện theo hai hướng: cả trong chiều patch và chiều kênh. Đây cũng là sự khác biệt chính giữa mô hình của chúng tôi và các mô hình Transformer-MoE trước đây [Fedus et al., 2021; Riquelme et al., 2021; Xue et al., 2021]. Các mô hình trước đây áp dụng MoE cho kiến trúc dựa trên transformer chỉ thay thế FFN trong khối transformer bằng MoE thưa thớt. Trong mô hình của chúng tôi, chúng tôi có các lớp channel-mixing MoE hoạt động theo cách tương tự, và các lớp token-mixing MoE hoạt động theo hướng khác: trộn thông tin qua các vị trí không gian của biểu diễn. Chúng tôi chứng minh bằng thử nghiệm rằng thiết kế MoE hai chiều như vậy là hiệu quả và năng suất.

Cuối cùng, chúng tôi áp dụng các mô hình Sparse-MLP của chúng tôi cho các tác vụ phân loại hình ảnh và đạt được kết quả xuất sắc. Sau khi được pre-trained với thuật toán self-supervised (MoCo v3) [Chen et al., 2021] trên bộ dữ liệu ILSVRC2012 ImageNet-1k [Russakovsky et al., 2015], mô hình Sparse-B của chúng tôi đạt 77.9% độ chính xác top-1 ImageNet-1k, cao hơn 2.0% so với mô hình Mixer-B/16 với chi phí tính toán tương đương, cao hơn 1.2% so với ViT-B/16 với chi phí tính toán ít hơn. Mô hình Sparse-L của chúng tôi đạt 79.2% độ chính xác top-1 ImageNet-1k, vượt trội hơn Mixer-L/16 2.5% với 62.8% tham số và 85.5% chi phí pre-training. Nó cũng vượt trội hơn ViT-L/16 1.8% với chi phí pretraining ít hơn một nửa.

Các đóng góp của công trình này có thể được tóm tắt như sau:

--- TRANG 2 ---
Thiết kế MoE hai giai đoạn và kiến trúc all-MLP cho mô hình hóa cross-token
Chúng tôi sử dụng kiến trúc MoE hiệu quả tính toán làm mô-đun mô hình hóa cross-token và sau đó xây dựng một kiến trúc all-MLP với ứng dụng MoE hai giai đoạn. Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên tập trung vào việc mở rộng mô-đun mô hình hóa cross-token bằng tính toán có điều kiện.

Cải thiện hiệu quả thêm trên MoE
Chúng tôi thiết kế chiến lược routing điểm tầm quan trọng yêu cầu chi phí tính toán ít hơn mà không làm hại khả năng mô hình. Chúng tôi cũng xem xét lại và thiết kế lại hình dạng biểu diễn hình ảnh để tận dụng tốt nhất token-mixing MoE. Cả hai thực hành đều cải thiện thêm hiệu quả của mô hình.

Hiệu suất cạnh tranh trên các tác vụ phân loại hình ảnh
Chúng tôi cho thấy rằng mô hình Sparse-MLP có thể vượt trội hơn mô hình ViT và mô hình MLP-Mixer dense trên benchmark Imagenet-1k. Ngoài ra, trên ba tác vụ downstream, các mô hình Sparse-MLP có thể đạt hiệu suất tốt hơn với chi phí tính toán tương đương hoặc ít hơn so với các mô hình MLP-Mixer cùng cấp.

2 Mô hình hóa Cross-token với Tính toán Có điều kiện
Đóng góp chính của công trình chúng tôi là, chúng tôi đề xuất một kiến trúc MLP được kích hoạt thưa thớt để mô hình hóa cross-token một cách hiệu quả và có hiệu suất: lớp token-mixing MoE. Token-mixing MoE tuân theo kiến trúc Mixture of Experts [Shazeer et al., 2017]. Đối với một batch đầu vào hình ảnh X ∈ R^(B×S×C) với kích thước batch B, số patch trên mỗi hình ảnh S và số kênh trên mỗi patch C, chúng tôi đầu tiên chuyển vị đầu vào thành X' ∈ R^(B×C×S). Sau đó mạng gating của token-mixing MoE gán BC kênh batch cho các chuyên gia MLP khác nhau bằng trọng số routing. Mỗi chuyên gia mô hình trộn thông tin dọc theo chiều patch trong mỗi kênh được gán cho nó. Chi tiết của lớp token-mixing MoE và kiến trúc all-MLP sẽ được mô tả trong các phần sau.

2.1 Mixture-of-Experts
Trong phần này, chúng tôi công thức hóa kiến trúc Mixture-of-Experts (MoE) và các thành phần chính của nó.

Tính toán Có điều kiện
Lớp Mixture-of-Experts (MoE) được tạo thành từ một tập hợp các chuyên gia. Chỉ có một tập con của chúng hoạt động và tham gia vào tính toán trên cơ sở từng ví dụ. Trong mô hình của chúng tôi, mỗi chuyên gia là một MLP.

Theo [Shazeer et al., 2017], với x ∈ R^D, đầu ra của một lớp MoE với N chuyên gia là:

MoE(x) = Σ(i=1 to N) G(x)_i E_i(x)     (1)

trong đó G(x) : R^D → R^N là mạng gating tính toán trọng số routing có điều kiện đầu vào cho các chuyên gia. E_i(x) : R^D → R^D là lớp chuyên gia thứ i. Trong thực tế, chúng ta có G(x) thưa thớt, có nghĩa là mỗi đầu vào x bị hạn chế được gán cho k chuyên gia (k << N). Nếu đầu vào x không được gán cho E_i, G(x)_i = 0 và E_i sẽ không được tính toán. Điều này cho phép chúng ta mở rộng đến mô hình cực lớn với chi phí tính toán tương đương.

Mạng Gating
Như chúng tôi đã giới thiệu ở trên, để gán các biểu diễn token x cho các chuyên gia khác nhau, mỗi lớp MoE có một mạng gating thưa thớt. Chúng tôi công thức hóa nó như:

G(x) = TopK(softmax(W_g(x) + ε))     (2)

trong đó W_g ∈ R^(D×N) là một ma trận có thể huấn luyện và ε ~ N(0, 1/N^2) là nhiễu chuẩn để khám phá phân công tốt hơn từ các chuyên gia. Sau khi tính toán xác suất của đầu vào x được định tuyến đến mỗi chuyên gia, chúng tôi chỉ giữ lại K hàng đầu của chúng để lan truyền tiến tiếp. Trong thực tế, chúng tôi thường chọn K là 1 hoặc 2.

Loss Cân bằng Tải
Để khuyến khích phân công cân bằng các đầu vào qua các chuyên gia, một loss phụ trợ được thêm vào mô hình cho mỗi lớp MoE [Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021]. Công thức của loss cân bằng tải trong Phụ lục A.

2.2 Chiến lược Routing Điểm Tầm quan trọng
Như một nỗ lực để cải thiện thêm hiệu quả của token-mixing MoE, chúng tôi đề xuất chiến lược routing điểm tầm quan trọng. Đối với mỗi batch đầu vào hình ảnh sau chuyển vị X' ∈ R^(B×C×S), chúng tôi đặt một điểm tầm quan trọng cho tất cả BC kênh trong batch. Sau đó, các kênh được sắp xếp theo điểm tầm quan trọng của chúng. 10% với điểm tầm quan trọng thấp nhất bị loại bỏ và phần còn lại dành cho phân bổ.

Trong công trình của chúng tôi, điểm tầm quan trọng của mỗi kênh để routing là trọng số routing cao nhất của nó đến tất cả các kênh. Tương ứng, g(X)_{i,j} ∈ R biểu thị trọng số routing của kênh thứ i đến chuyên gia thứ j. Điểm tầm quan trọng của kênh thứ i là:

Score(Channel i) = max_j{g(X)_{i,j}}     (3)

Trong phần 4.3, chúng tôi cho thấy bằng thực nghiệm cách chiến lược routing như vậy có thể giúp giảm thêm chi phí tính toán mà không làm hại khả năng mô hình.

2.3 Kiến trúc All-MLP
Tổng quan về mô hình đầy đủ của chúng tôi, Sparse-MLP được hiển thị trong Hình 1. Nói chung, mô hình Sparse-MLP bao gồm một lớp embedding tuyến tính per-patch, một chồng các khối Dense và khối Sparse, và một đầu phân loại.

Trong mỗi khối Sparse, bên cạnh lớp token-mixing MoE (MoE S) được mô tả ở trên, chúng tôi cũng áp dụng một lớp channel-mixing MoE (MoE C), cũng là một kiến trúc được kích hoạt thưa thớt và chịu trách nhiệm trộn thông tin dọc theo chiều kênh trong các patch. Bên cạnh đó, chúng tôi có các lớp con rescale ở cả đầu và cuối của mỗi khối Sparse sẽ được mô tả thêm trong Phần 3.

Chúng tôi công thức hóa khối Sparse như sau:
x = Rescale_1(x)     (4)
y_1 = x + t(MoE_S(t(norm(x))))     (5)
y = y_1 + MoE_C(norm(y_1))     (6)
y = Rescale_2(y)     (7)

--- TRANG 3 ---
Hình 1: Tổng quan kiến trúc Sparse-MLP

Trong mỗi khối Dense, cả token-mixing MoE và channel-mixing MoE đều được đơn giản hóa bằng MLP đơn: token-mixing MLP (MoE S) và channel mixing MLP (MoE S). Ý tưởng của khối Dense tuân theo [Tolstikhin et al., 2021]. Công thức của khối Dense là:

y_1 = x + t(MLP_S(t(norm(x))))     (8)
y = y_1 + MLP_C(norm(y_1))     (9)

Tóm lại, Sparse-MLP của chúng tôi là một mô hình all-MLP. Và bằng cách áp dụng kiến trúc MoE thưa thớt ở hai giai đoạn trộn thông tin, mô hình của chúng tôi có thể đạt được hiệu suất mô hình tốt hơn so với các mô hình giống MLP khác.

3 Xem xét lại Hình dạng Biểu diễn Hình ảnh
Cho đến nay, chúng tôi có một mô hình all-MLP bao gồm token-mixing MoE và channel-mixing MoE. Tuy nhiên, trong thiết kế ViT gốc [Dosovitskiy et al., 2020], số lượng token nhỏ trong khi kích thước ẩn per-token (tức là chiều kênh C) lớn hơn nhiều. Đối với biểu diễn hình ảnh x ∈ R^(S×C), C >> S. Nếu chúng ta áp dụng cài đặt biểu diễn gốc của ViT tại lớp token-mixing MoE, việc gán số lượng lớn kênh cho các chuyên gia đòi hỏi nhiều tính toán trong khi khả năng của chuyên gia bị hạn chế bởi độ dài patch ngắn. Ví dụ, trong cài đặt ViT tiêu chuẩn, đối với một batch đầu vào X ∈ R^(B×S×C), B = 4096, S = 196, C = 768. Lớp token-mixing MoE cần gán BC = 3145728 kênh cho các chuyên gia tương ứng trong khi khả năng của chuyên gia trộn thông tin trong các kênh bị hạn chế vì kích thước ẩn đầu ra nhỏ S = 196.

Để giảm tính toán trong giai đoạn gán và cải thiện khả năng của token-mixing MoE trộn thông tin trong các kênh, chúng tôi thêm hai giai đoạn của các lớp con rescale ở đầu và cuối của mỗi khối Sparse. Lớp con rescale Pro-token, ở đầu của mỗi khối Sparse, giảm số lượng kênh và tăng số lượng token để đầu vào có thể phù hợp tốt hơn với MoE S. Lớp con rescale Pro-channel, ở cuối của mỗi khối Sparse, hoạt động theo cách ngược lại để các khối Sparse và khối Dense có thể được kết hợp linh hoạt.

Với một đầu vào x ∈ R^(S×C), lớp con rescale Pro-token ánh xạ: R^(C×S) → R^(C1×S1). Lớp con rescale Pro-channel ánh xạ: R^(S1×C1) → R^(S×C). Mỗi lớp con rescale được tạo thành từ hai lớp tuyến tính để biến đổi hai chiều S và S1, C và C1, tương ứng. Việc triển khai như vậy có thể giảm tính toán routing và cải thiện chiều chuyên gia, dẫn đến thiết kế MoE cân bằng và hiệu quả hơn. Trong thực tế, chúng tôi đặt S1 = 2S, C1 = C/2.

4 Thử nghiệm
Chúng tôi pretrain các mô hình Sparse-MLP với MoCo V3 trên bộ dữ liệu ILSVRC-2012 Imagenet [Russakovsky et al., 2015] và đánh giá hiệu suất của mô hình trên một số tác vụ phân loại hình ảnh downstream. Chúng tôi chọn các mô hình MLP-Mixer [Tolstikhin et al., 2021] và các mô hình ViT [Dosovitskiy et al., 2020] làm baseline và so sánh các mô hình của chúng tôi với các mô hình baseline theo hai tiêu chí: (1) độ chính xác phân loại trên các tác vụ downstream, (2) chi phí tính toán của pre-training trên bộ dữ liệu upstream và fine-tuning trên các bộ dữ liệu downstream. Chúng tôi không nhằm mục đích đạt độ chính xác phân loại hình ảnh SOTA nhưng để cho thấy rằng mô hình fully-MLP với tính toán có điều kiện có thể vượt trội hơn các mô hình MLP dense hoặc các mô hình dựa trên attention về độ chính xác hoặc chi phí tính toán.

4.1 Cài đặt Thử nghiệm
Chúng tôi pretrain các mô hình Sparse-MLP và các mô hình baseline (ViT và MLP-Mixer) với thuật toán học tự giám sát (MoCo v3) [Chen et al., 2021] trên bộ dữ liệu ILSVRC-2012 ImageNet [Russakovsky et al., 2015] (1.3M mẫu huấn luyện, 1k lớp hình ảnh) trên các cụm TPU.

Sau pretraining, chúng tôi fine-tune mô hình trên ba tác vụ downstream: ILSVRC-2012 Imagenet, CIFAR-10 (50k mẫu huấn luyện, 10k mẫu validation, 10 lớp) [Krizhevsky et al.,] và CIFAR-100. Cài đặt chi tiết của giai đoạn pretraining và fine-tune có thể được tìm thấy trong Phụ lục B.

4.2 Kết quả Chính
Chúng tôi xây dựng các mô hình Sparse-MLP ở ba mức tham số để so sánh với các mô hình dựa trên attention (ví dụ: ViT [Dosovitskiy et al., 2020]) và các mô hình MLP dense (ví dụ: MLP-Mixer [Tolstikhin et al., 2021]). Thông số kỹ thuật của các mô hình có thể được tìm thấy trong Phụ lục C. Trong Bảng 1, chúng tôi báo cáo độ chính xác top-1 ImageNet-1k và chi phí pre-training tương ứng của mỗi mô hình.

Mô hình Sparse-S của chúng tôi vượt trội hơn Mixer-S/16 về độ chính xác top-1 ImageNet-1k 1.1% với tham số và chi phí pre-training tương đương. Mô hình Sparse-B mở rộng Mixer-B/16 với 17% (59M → 69M) với TPU v3 core days pre-training tương đương và vượt trội hơn Mixer-B/16 2.6% (75.9% → 77.9%). Sparse-L của chúng tôi vượt trội hơn Mixer-L/16 3.3% chỉ với 62.8% tham số và 85.5% thời gian pre-training. So với ViT, các mô hình của chúng tôi cho thấy hiệu suất tốt hơn với ít tham số hơn nhiều và chi phí pre-training ít hơn nhiều.

Ngoài ra, chúng tôi báo cáo kết quả của các mô hình Sparse-MLP và các mô hình MLP dense [Tolstikhin et al., 2021] trên hai tác vụ phân loại hình ảnh downstream khác: Cifar10 [Krizhevsky et al.,], Cifar100 [Krizhevsky, 2009]. Tất cả các mô hình được pre-trained với MoCo v3 trên ImageNet-1k và sau đó được fine-tuned ở các tác vụ downstream end-to-end.

Trong Bảng 2, chúng ta có thể thấy rằng các mô hình Sparse-MLP của chúng tôi cũng vượt trội hơn các mô hình MLP-Mixer trên các tác vụ phân loại hình ảnh Cifar10 và Cifar100. Ngoài ra, khi chúng tôi mở rộng mô hình lên hơn 100M tham số, hiệu suất của Mixer-L/16 và Sparse-L giảm do overfitting. Vấn đề này nổi bật khi huấn luyện các mô hình MLP lớn trên các bộ dữ liệu nhỏ. Và trong những trường hợp như vậy, mô hình Sparse-L của chúng tôi vẫn đạt độ chính xác cao hơn Mixer-L/16.

4.3 Nghiên cứu Ablation
Trong phần này, chúng tôi điều tra thêm cách mỗi thành phần của mô hình Sparse-MLP đóng góp vào hiệu suất. Tất cả các mô hình trong nghiên cứu ablation được pretrained với thuật toán MoCo v3 trên ImageNet-1k và fine-tuned trên cùng bộ dữ liệu. Chúng tôi chọn độ chính xác validation top-1 ImageNet-1k và tổng số TPU v3 core days pre-training làm thước đo đánh giá. Nghiên cứu ablation được thiết kế để trả lời các câu hỏi sau:

• Số lượng chuyên gia: Tác động của số lượng chuyên gia trong các lớp MoE hai giai đoạn là gì?
• Top K routing: Chúng ta nên chọn giá trị K nào (1 hoặc 2) cho MoE S và MoE C?
• Importance-score routing: Importance-score routing có thể tiết kiệm bao nhiêu chi phí tính toán?
• Vị trí của các khối Sparse: Chúng ta nên kết hợp các khối Dense và khối Sparse như thế nào?
• Phân tích các lớp con rescale: Các lớp con rescale ảnh hưởng như thế nào đến hiệu suất mô hình và chi phí tính toán?

Hình 3: Ảnh hưởng của số lượng chuyên gia trong MoE C/MoE S

Số lượng chuyên gia
Chúng tôi đầu tiên nghiên cứu ảnh hưởng của số lượng chuyên gia trong MoE S đến khả năng mô hình. Các mô hình khác nhau được xây dựng dựa trên Sparse-B. Chúng tôi cố định tất cả các siêu tham số khác và điều chỉnh số lượng chuyên gia trong MoE S ở ba mức: 4, 8, 16, pretrain các mô hình này và đánh giá hiệu suất của chúng trên độ chính xác validation top-1 ImageNet-1k.

Từ Hình 3, chúng ta có thể thấy rằng khi số lượng chuyên gia trong MoE S tăng từ 4 lên 8, hiệu suất của mô hình tăng rất nhiều. Tuy nhiên, khi chúng tôi mở rộng chuyên gia lên 16, độ chính xác hầu như không thay đổi.

--- TRANG 4 ---
Hình 2: So sánh giữa các mô hình Mixer và các mô hình Sparse-MLP. Với chi phí tính toán tương đương hoặc ít hơn, Sparse-MLP đạt được hiệu suất tốt hơn

tính toán có điều kiện có thể vượt trội hơn các mô hình MLP dense hoặc các mô hình dựa trên attention về độ chính xác hoặc chi phí tính toán.

4.1 Cài đặt Thử nghiệm
Chúng tôi pretrain các mô hình Sparse-MLP và các mô hình baseline (ViT và MLP-Mixer) với thuật toán học tự giám sát (MoCo v3) [Chen et al., 2021] trên bộ dữ liệu ILSVRC-2012 ImageNet [Russakovsky et al., 2015] (1.3M mẫu huấn luyện, 1k lớp hình ảnh) trên các cụm TPU.

Sau pretraining, chúng tôi fine-tune mô hình trên ba tác vụ downstream: ILSVRC-2012 Imagenet, CIFAR-10 (50k mẫu huấn luyện, 10k mẫu validation, 10 lớp) [Krizhevsky et al.,] và CIFAR-100. Cài đặt chi tiết của giai đoạn pretraining và fine-tune có thể được tìm thấy trong Phụ lục B.

4.2 Kết quả Chính
Chúng tôi xây dựng các mô hình Sparse-MLP ở ba mức tham số để so sánh với các mô hình dựa trên attention (ví dụ: ViT [Dosovitskiy et al., 2020]) và các mô hình MLP dense (ví dụ: MLP-Mixer [Tolstikhin et al., 2021]). Thông số kỹ thuật của các mô hình có thể được tìm thấy trong Phụ lục C. Trong Bảng 1, chúng tôi báo cáo độ chính xác top-1 ImageNet-1k và chi phí pre-training tương ứng của mỗi mô hình.

Mô hình Sparse-S của chúng tôi vượt trội hơn Mixer-S/16 về độ chính xác top-1 ImageNet-1k 1.1% với tham số và chi phí pre-training tương đương. Mô hình Sparse-B mở rộng Mixer-B/16 với 17% (59M → 69M) với TPU v3 core days pre-training tương đương và vượt trội hơn Mixer-B/16 2.6% (75.9% → 77.9%). Sparse-L của chúng tôi vượt trội hơn Mixer-L/16 3.3% chỉ với 62.8% tham số và 85.5% thời gian pre-training. So với ViT, các mô hình của chúng tôi cho thấy hiệu suất tốt hơn với ít tham số hơn nhiều và chi phí pre-training ít hơn nhiều.

Ngoài ra, chúng tôi báo cáo kết quả của các mô hình Sparse-MLP và các mô hình MLP dense [Tolstikhin et al., 2021] trên hai tác vụ phân loại hình ảnh downstream khác: Cifar10 [Krizhevsky et al.,], Cifar100 [Krizhevsky, 2009]. Tất cả các mô hình được pre-trained với MoCo v3 trên ImageNet-1k và sau đó được fine-tuned ở các tác vụ downstream end-to-end.

Trong Bảng 2, chúng ta có thể thấy rằng các mô hình Sparse-MLP của chúng tôi cũng vượt trội hơn các mô hình MLP-Mixer trên các tác vụ phân loại hình ảnh Cifar10 và Cifar100. Ngoài ra, khi chúng tôi mở rộng mô hình lên hơn 100M tham số, hiệu suất của Mixer-L/16 và Sparse-L giảm do overfitting. Vấn đề này nổi bật khi huấn luyện các mô hình MLP lớn trên các bộ dữ liệu nhỏ. Và trong những trường hợp như vậy, mô hình Sparse-L của chúng tôi vẫn đạt độ chính xác cao hơn Mixer-L/16.

4.3 Nghiên cứu Ablation
Trong phần này, chúng tôi điều tra thêm cách mỗi thành phần của mô hình Sparse-MLP đóng góp vào hiệu suất. Tất cả các mô hình trong nghiên cứu ablation được pretrained với thuật toán MoCo v3 trên ImageNet-1k và fine-tuned trên cùng bộ dữ liệu. Chúng tôi chọn độ chính xác validation top-1 ImageNet-1k và tổng số TPU v3 core days pre-training làm thước đo đánh giá. Nghiên cứu ablation được thiết kế để trả lời các câu hỏi sau:

• Số lượng chuyên gia: Tác động của số lượng chuyên gia trong các lớp MoE hai giai đoạn là gì?
• Top K routing: Chúng ta nên chọn giá trị K nào (1 hoặc 2) cho MoE S và MoE C?
• Importance-score routing: Importance-score routing có thể tiết kiệm bao nhiêu chi phí tính toán?
• Vị trí của các khối Sparse: Chúng ta nên kết hợp các khối Dense và khối Sparse như thế nào?
• Phân tích các lớp con rescale: Các lớp con rescale ảnh hưởng như thế nào đến hiệu suất mô hình và chi phí tính toán?

Hình 3: Ảnh hưởng của số lượng chuyên gia trong MoE C/MoE S

Số lượng chuyên gia
Chúng tôi đầu tiên nghiên cứu ảnh hưởng của số lượng chuyên gia trong MoE S đến khả năng mô hình. Các mô hình khác nhau được xây dựng dựa trên Sparse-B. Chúng tôi cố định tất cả các siêu tham số khác và điều chỉnh số lượng chuyên gia trong MoE S ở ba mức: 4, 8, 16, pretrain các mô hình này và đánh giá hiệu suất của chúng trên độ chính xác validation top-1 ImageNet-1k.

Từ Hình 3, chúng ta có thể thấy rằng khi số lượng chuyên gia trong MoE S tăng từ 4 lên 8, hiệu suất của mô hình tăng rất nhiều. Tuy nhiên, khi chúng tôi mở rộng chuyên gia lên 16, độ chính xác hầu như không thay đổi.

--- TRANG 5 ---
Bảng 1: Kết quả ImageNet-1k. Tất cả các mô hình được pretrained với thuật toán tự giám sát (MoCo v3) trên ImageNet-1k và sau đó fine-tuned. Chi phí Pretrain được đánh giá bằng tổng TPU v3 core-days được sử dụng cho pretraining. Throughput được đánh giá bằng image/sec/core

| Mô hình | ImageNet Top-1(%) | Params(M) | Chi phí Pre-training | Throughput |
|---------|-------------------|-----------|---------------------|------------|
| dựa trên attention | | | | |
| ViT-B/16 | 76.7 | 86 | 67.2 | 861 |
| ViT-L/16 | 77.6 | 304 | 195.2 | 268 |
| MLP-like dense | | | | |
| Mixer-S/16 | 70.2 | 19 | 35.5 | 3986 |
| Mixer-B/16 | 75.9 | 59 | 53.3 | 1320 |
| Mixer-L/16 | 76.7 | 207 | 97.7 | 412 |
| Sparse-MLP | | | | |
| Sparse-S | 71.3 | 21 | 35.5 | 3986 |
| Sparse-B | 77.9 | 69 | 55.5 | 1265 |
| Sparse-L | 79.4 | 130 | 80.1 | 482 |

Bảng 2: Kết quả trên các tác vụ phân loại hình ảnh downstream

| Mô hình | ImageNet top-1 | Cifar10 top-1 | Cifar100 top-1 |
|---------|----------------|---------------|----------------|
| Mixer-S/16 | 70.2 | 91.7 | 84.4 |
| Sparse-S | 71.3 | 91.9 | 84.4 |
| Mixer-B/16 | 75.9 | 95.6 | 86.7 |
| Sparse-B | 77.9 | 96.2 | 87.2 |
| Mixer-L/16 | 76.7 | 94.7 | 86.3 |
| Sparse-L | 79.4 | 95.4 | 87.4 |

Tương tự, đối với MoE C, chúng tôi cố định tất cả các thành phần khác trong Sparse-B và điều chỉnh số lượng chuyên gia trong MoE C ở ba mức: 4, 8, 16.

Trong Hình 3, chúng tôi quan sát thấy rằng sẽ có vấn đề overfitting khi chúng tôi tăng số lượng chuyên gia trong MoE C. Phát hiện như vậy tương tự với kết quả trong [Xue et al., 2021]. Khi dữ liệu huấn luyện bị hạn chế, việc mở rộng các lớp MoE, trộn thông tin trong các vị trí không gian, sẽ làm cho mô hình dễ overfit dữ liệu huấn luyện.

Bảng 3: Ảnh hưởng của việc chọn K.

| K | ImageNet Top-1(%) | TPU v3 core days |
|---|-------------------|------------------|
| MoE S | | |
| 1 | 77.9 | 55.5 |
| 2 | 77.9 | 57.7 |
| MoE C | | |
| 1 | 77.0 | 53.3 |
| 2 | 77.9 | 55.5 |

Vai trò của Top K routing
Theo thiết kế trong [Fedus et al., 2021], chúng tôi chọn K=1 hoặc 2 cho MoE S và MoE C. Chúng tôi đặt Sparse-B làm mô hình mặc định (K=1 cho MoE S, K=2 cho MoE C). Sau đó chúng tôi báo cáo kết quả với K=2 cho MoE S và K=1 cho MoE C riêng biệt.

Như được thể hiện trong Bảng 3, đối với MoE S, top-1 routing và top-2 routing đạt cùng độ chính xác validation và top-1 routing tốn ít thời gian pre-training hơn. Đối với MoE C, top-2 routing sẽ dẫn đến hiệu suất tốt hơn nổi bật với 4% thời gian pre-training nhiều hơn.

Bảng 4: Importance-score Routing so với Vanilla Routing

| K | ImageNet Top-1(%) | TPU v3 core days |
|---|-------------------|------------------|
| Sparse-B | | |
| Hiệu quả | 77.9 | 54.3 |
| Vanilla | 77.9 | 55.5 |
| Sparse-L | | |
| Hiệu quả | 79.4 | 80.0 |
| Vanilla | 79.2 | 83.5 |

Importance-score Routing so với Vanilla MoE Routing
Trong phần 2.2, chúng tôi đề xuất một phương pháp routing mới yêu cầu chi phí tính toán ít hơn so với vanilla MoE routing. Chúng tôi so sánh hai phương pháp routing khác nhau trên các mô hình Sparse-B và Sparse-L, chúng ta có thể thấy rằng với kết quả tương đương về độ chính xác validation top-1 ImageNet, chiến lược importance-score routing có thể giảm tổng thời gian pretraining 1.9% và 4.1%.

Vị trí của các khối Sparse
Chúng tôi thử nghiệm với hai thứ tự đặt khác nhau của các khối Dense và khối Sparse. (1) Các khối Dense ở phía trước và các khối Sparse ở phía sau; (2) Các khối Sparse làm một vài khối đầu tiên và theo sau bởi các khối Dense. Ngoài ra, chúng tôi thử nghiệm với số lượng khối Sparse khác nhau trong khi giữ nguyên số lượng tổng cộng của các khối Dense và khối Sparse. Chúng tôi đặt Sparse-B làm mô hình mặc định và thay đổi thứ tự và số lượng khối dựa trên Sparse-B.

Trong Bảng 5, chúng tôi có thể thấy rằng việc đặt các khối Sparse ở vị trí đầu tiên và các khối Dense sau đó không cho thấy hiệu suất tốt. Chúng tôi cũng thấy rằng tăng số lượng khối Sparse, cuối cùng, là một cách hiệu quả để cải thiện hiệu suất của mô hình. Khi chúng tôi tăng 2 khối Sparse và giữ nguyên tổng số khối, độ chính xác validation top-1 ImageNet-1k của mô hình tăng 0.3%.

Bảng 5: Các kết hợp khác nhau của các khối Dense và khối Sparse. 'Positions' đề cập đến vị trí của các khối Sparse.

| Vị trí | ImageNet Top-1(%) | Tham số(M) |
|--------|-------------------|------------|
| N/A (Mixer-B/16) | 75.9 | 59 |
| Hai cuối(Sparse-B) | 77.9 | 69 |
| Hai đầu | 75.5 | 69 |
| Bốn cuối | 78.3 | 79 |

Vai trò của các lớp con rescale
Một cách trực quan để xây dựng các khối Sparse là chỉ áp dụng hai giai đoạn của Mixture of Experts với hình dạng biểu diễn hình ảnh gốc. Như đã nêu trong Phần 3, thiết kế như vậy sẽ dẫn đến chi phí tính toán khổng lồ trong giai đoạn routing của token-mixing MoE. Do đó, chúng tôi thêm các lớp con rescale để giảm chi phí tính toán của các khối Sparse. Ở đây chúng tôi xác minh sự cần thiết của các lớp con rescale bằng thử nghiệm. Chúng tôi đặt Sparse-B làm mô hình mặc định và thử nghiệm các mô hình có hoặc không có các lớp con rescale.

Bảng 6: So sánh giữa các mô hình có hoặc không có các lớp con rescale.

| Mô hình | ImageNet Top-1(%) | Chi phí pre-training |
|---------|-------------------|---------------------|
| w/ r layers | 77.9 | 55.5 |
| w/o r layers | 76.9 | 79.9 |

Chúng ta có thể thấy từ bảng 6 rằng các lớp con rescale không chỉ giảm chi phí tính toán pre-training 30.5% mà còn cải thiện hiệu suất đáng kể.

5 Tại sao Token-mixing MoE và Sparse-MLP
Mặc dù token-mixing MoE có thể được xem như một thay thế hiệu quả tính toán cho self-attention, mô hình Sparse-MLP của chúng tôi không chỉ tiết kiệm chi phí tính toán mà còn có hiệu suất cạnh tranh. Sparse-MLP có ưu thế riêng so với ViT, các mô hình transformer-MoE trước đây và các mô hình giống MLP dense.

• So với ViT: Mô hình của chúng tôi không chỉ yêu cầu chi phí pre-training ít hơn nhiều mà còn cho thấy khả năng mô hình tốt hơn bằng thực nghiệm. Trong phần 4.2, mô hình của chúng tôi (Sparse-L) vượt trội hơn ViT ở cùng mức tham số chỉ với chi phí pre-training ít hơn một nửa.

• So với các mô hình transformer-MoE trước đây: Công trình trước đây áp dụng kiến trúc MoE cho các mô hình vision transformer [Riquelme et al., 2021] chỉ thay thế FFN trong khối transformer bằng lớp Mixture of Experts. Giống như những gì attention làm trong transformer, chúng tôi tiếp tục áp dụng lớp MoE thưa thớt cho chiều cross-token. Bằng cách này, chúng tôi không chỉ giảm chi phí tính toán mà còn xây dựng kiến trúc All-MLP, đơn giản hơn và dễ mở rộng hơn.

• So với các mô hình giống MLP dense: So với MLP-Mixer [Tolstikhin et al., 2021], mô hình của chúng tôi mở rộng MLP đơn thành một tập cấu trúc MLP được kích hoạt thưa thớt. Chiến lược mở rộng như vậy mang lại cải thiện hiệu suất mô hình đáng kể và chỉ yêu cầu tăng chi phí tính toán dưới tuyến tính. Trong phần 4.2, các Sparse-MLP vượt trội hơn các mô hình Mixer ở cùng mức tham số với biên độ lớn.

6 Công trình Liên quan

6.1 Các mô hình Transformer-MoE
Mixture-of-Experts(MoE) [?; Shazeer et al., 2017] gần đây đã được áp dụng cho kiến trúc dựa trên transformer để xây dựng các mô hình khổng lồ [Lepikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021; Xue et al., 2021]. Trong các tác vụ NLP, [Lepikhin et al., 2020; Fedus et al., 2021] áp dụng MoE để mở rộng các mô hình dựa trên transformer lên hàng nghìn tỷ tham số và đạt được kết quả đáng kinh ngạc. Trong các tác vụ thị giác, [Riquelme et al., 2021] cải thiện ViT [Dosovitskiy et al., 2020] bằng cách mở rộng một tập con các khối transformer với MoE. [Xue et al., 2021] áp dụng MoE cho các khối transformer với chia sẻ tham số để cải thiện ViT với ít tham số hơn. Trong những công trình này, các lớp MoE thay thế FFN trong các khối transformer. Thiết kế mô hình của chúng tôi tạo ra sự khác biệt ở chỗ chúng tôi áp dụng MoE theo hai hướng và các thử nghiệm chứng minh rằng token-mixing MoE mới có thể cải thiện khả năng mô hình một cách hiệu quả và năng suất.

6.2 Các mô hình giống MLP
Công trình của chúng tôi cũng liên quan đến các mô hình giống MLP. Khác với các mô hình CNN [Krizhevsky et al., 2012; Simonyan và Zisserman, 2015] và các mô hình dựa trên attention [Dosovitskiy et al., 2020; Touvron et al., 2020], tất cả các tham số có thể huấn luyện trong backbone của các mô hình dựa trên MLP đều giống MLP. Trong MLP-Mixer [Tolstikhin et al., 2021], một token-mixing MLP thay thế multi-head self-attention [Vaswani et al., 2017] trong khối transformer [Tolstikhin et al., 2021]. Một số kiến trúc giống MLP khác [Liu et al., 2021; Hou et al., 2021] hoạt động theo cách tương tự, trộn thông tin qua các vị trí không gian với MLP hoặc FFN. Sparse-MLP của chúng tôi áp dụng MLP được kích hoạt thưa thớt và có thể đạt được hiệu suất tốt hơn.

7 Kết luận
Trong công trình này, chúng tôi đề xuất token-mixing MoE, một kiến trúc MoE thưa thớt để mô hình hóa thông tin cross-token của hình ảnh với tính toán có điều kiện. Hơn nữa, chúng tôi đề xuất Sparse-MLP, một kiến trúc all-MLP với MoE hai chiều trong thị giác. Các thử nghiệm chứng minh rằng thiết kế MoE hai giai đoạn với chiến lược routing importance-score và thiết kế lớp con rescale của chúng tôi là hiệu quả và tiết kiệm tính toán. Bên cạnh đó, chúng tôi thực hiện một nghiên cứu ablation toàn diện để điều tra cách mỗi thành phần đóng góp vào hiệu suất.

Các mở rộng của công trình chúng tôi có thể bao gồm các chủ đề sau. Đầu tiên, có thể cải thiện thêm khả năng mô hình Sparse-MLP với các bộ dữ liệu pre-training khổng lồ. Bên cạnh đó, chúng ta có thể khám phá tính linh hoạt của kiến trúc Sparse-MLP bằng cách thiết kế các khối Sparse với các cài đặt MoE khác nhau trong cùng một mô hình. Cũng đáng để áp dụng kiến trúc Sparse-MLP cho NLP và các tác vụ khác.

--- TRANG 7 ---
Tài liệu tham khảo
[Chen et al., 2021] Xinlei Chen, Saining Xie, và Kaiming He. An empirical study of training self-supervised vision transformers. CoRR, abs/2104.02057, 2021.

[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. CoRR, abs/2010.11929, 2020.

[Fedus et al., 2021] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021.

[Hou et al., 2021] Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, và Jiashi Feng. Vision permutator: A permutable mlp-like architecture for visual recognition. CoRR, abs/2106.12368, 2021.

[Krizhevsky et al.,] Alex Krizhevsky, Vinod Nair, và Geoffrey Hinton.. cifar-10 (canadian institute for advanced research).

[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton.. Imagenet classification with deep convolutional neural networks. Trong F. Pereira, C. J. C. Burges, L. Bottou, và K. Q. Weinberger, biên tập, Advances in Neural Information Processing Systems, tập 25. Curran Associates, Inc., 2012.

[Krizhevsky, 2009] Alex Krizhevsky. Learning multiple layers of features from tiny images. Báo cáo kỹ thuật, 2009.

[Lepikhin et al., 2020] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. CoRR, abs/2006.16668, 2020.

[Liu et al., 2021] Hanxiao Liu, Zihang Dai, David R. So, và Quoc V. Le. Pay attention to mlps. CoRR, abs/2105.08050, 2021.

[Riquelme et al., 2021] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, và Neil Houlsby. Scaling vision with sparse mixture of experts. CoRR, abs/2106.05974, 2021.

[Russakovsky et al., 2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, và Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.

[Shazeer et al., 2017] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017.

[Simonyan và Zisserman, 2015] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. Trong International Conference on Learning Representations, 2015.

[Tolstikhin et al., 2021] Ilya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, và Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. CoRR, abs/2105.01601, 2021.

[Touvron et al., 2020] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. CoRR, abs/2012.12877, 2020.

[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.

[Xue et al., 2021] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, và Yang You. Go wider instead of deeper. CoRR, abs/2107.11817, 2021.

--- TRANG 8 ---
A Load-balance loss cho MoE
Loss phụ trợ của chúng tôi khuyến khích routing cân bằng bao gồm hai phần: Importance loss và Load loss.

Tầm quan trọng của chuyên gia thứ i được định nghĩa là trọng số mạng gating chuẩn hóa tương ứng với chuyên gia thứ i được tổng hợp trên batch đầu vào X.

Imp_i(X) = Σ_{x∈X} softmax(W_g x)_i     (10)

trong đó W_g là ma trận trọng số gating của lớp MoE, và importance loss của lớp MoE trên một batch đầu vào X là:

L_imp(X) = (std(Imp(X))/mean(Imp(X)))^2     (11)

Ngoài importance loss để có trọng số routing cân bằng hơn, chúng tôi cũng có load loss tìm kiếm kết quả routing cân bằng. Load của một chuyên gia i với một batch đầu vào X được định nghĩa là khả năng routing đến chuyên gia i được tổng hợp trên batch.

Load_i(X) = Σ_{x∈X} p_i(x)     (12)
p_i(x) ≜ P(G(x)_i ≥ threshold_k(G(x)))     (13)

Load loss của một lớp MoE trên batch là:

L_Load(X) = (std(Load(X))/mean(Load(X)))^2     (14)

Và tổng auxiliary loss của một lớp MoE có dạng:

L_aux = α(½L_imp + ½L_load)     (15)

trong đó α là một siêu tham số kiểm soát rằng auxiliary loss không chỉ khuyến khích routing cân bằng qua các chuyên gia mà cũng không áp đảo loss mô hình gốc. Trong thực tế, chúng tôi đặt α = 1e−2. Theo các mô hình dựa trên MoE hiện có [Riquelme et al., 2021; Xue et al., 2021], hiệu suất không nhạy cảm với α.

B Chi tiết Pretrain và fine-tune
Chính sách data augmentation của chúng tôi cho pretraining bao gồm random resized crop, horizontal flipping, RandAugment, color jittering, grayscale conversion, blurring, và solarization. Chúng tôi cũng áp dụng stochastic depth.

Chúng tôi pretrain tất cả các mô hình trên các cụm TPU v3. Chúng tôi chọn batch size là 4096 ở giai đoạn pre-training, optimizer LAMB với weight decay. Chúng tôi pretrain tất cả các mô hình trong 300 epoch sử dụng cosine learning rate decay với 10k bước warm up. Độ phân giải hình ảnh cho pretraining là 224.

Ở giai đoạn fine-tune, chúng tôi tuân theo các cài đặt fine-tune tiêu chuẩn. Sau pretraining, chúng tôi loại bỏ các MLP head của mô hình pretrained, thêm một classifier head vào encoder, và huấn luyện trên các tác vụ downstream. Các chiến lược augmentation trong giai đoạn fine-tuning bao gồm random resized crop, horizontal flipping, RandAugment và Mixup. Chúng tôi chọn Adam không có weight decay làm optimizer. Chúng tôi đặt learning rate của chúng tôi là lr * BatchSize/256, sử dụng linear weight decay với 10k bước warm-up. Độ phân giải hình ảnh là 224×224.

Bảng 7: Siêu tham số cho pre-training trên ImageNet-1k

| Siêu tham số | Giá trị |
|--------------|---------|
| Độ phân giải hình ảnh | 224 |
| Epochs | 300 |
| Batch size | 4096 |
| Bước Warmup | 10k |
| Optimizer | LAMB |
| Peak learning rate | 1e-3 |
| Learning rate decay | cosine |
| Tỷ lệ Weight decay | 1e-1 |
| Global clip norm | 1 |
| MoCo τ | 1 |
| MoCo m | 0.99 |
| MoCo dim | 4096 |

C Cài đặt Mô hình
Chúng tôi báo cáo kết quả chính dựa trên ba mô hình: Sparse-S, Sparse-B, Sparse-L. Trong Bảng 8, chúng tôi đưa ra thông số kỹ thuật của các mô hình này. Mỗi mô hình được tạo thành từ L1 khối Dense và L2 khối Sparse. Và trong tất cả ba mô hình được báo cáo trong kết quả chính, các khối Dense ở phía trước và theo sau bởi các khối Sparse. D_S đề cập đến chiều ẩn của các token mixing MLP, và D_C đề cập đến chiều ẩn của các channel mixing MLP. D_S' là chiều MLP của các lớp token-mixing MoE, và D_C' biểu thị chiều MLP của các lớp channel-mixing MoE. Đối với tất cả các MLP trong các khối Dense và khối Sparse, chúng tôi đặt dropout là 0. Đối với các token mixing MoE, chúng tôi chọn top K routing là 1. Và đối với các channel mixing MoE, chúng tôi đặt K là 2.

Bảng 8: Thông số kỹ thuật của các mô hình Sparse-MLP

| Thông số kỹ thuật | Sparse-S | Sparse-B | Sparse-L |
|-------------------|----------|----------|----------|
| Khối dense | | | |
| blocks L1 | 6 | 10 | 8 |
| Patches S | 196 | 196 | 196 |
| Hidden size C | 512 | 768 | 768 |
| MLP S dim D_S | 256 | 384 | 384 |
| MLP C dim D_C | 2048 | 3072 | 3072 |
| Khối sparse | | | |
| blocks L2 | 2 | 2 | 6 |
| New patches S' | 392 | 392 | 392 |
| New hidden size C' | 512 | 384 | 384 |
| Experts trong MoE S | 4 | 8 | 16 |
| Experts trong MoE C | 0 | 4 | 4 |
| MoE S top K | 1 | 1 | 1 |
| MoE C top K | - | 2 | 2 |
| MoE S dim D_S' | 512 | 768 | 768 |
| MoE C dim D_C' | 2048 | 1536 | 1536 |
| Vị trí | 2 cuối | 2 cuối | 6 cuối |
| Tham số(M) | 22 | 69 | 130 |

--- TRANG 9 ---
Hình 4: Độ chính xác validation ImageNet-1k của ViT và MLP-Mixer, sau supervised pretraining trên ImageNet-1k

D Pretrain: Tại sao Không giám sát
Chúng tôi thấy rằng việc mở rộng các mô hình MLP hoặc Vision Transformer về tham số và huấn luyện chúng từ đầu với dữ liệu huấn luyện hạn chế (ví dụ: ImageNet-1k) sẽ dẫn đến vấn đề overfitting. Như được hiển thị trong Hình 4, độ chính xác của cả MLP-Mixer và ViT đều giảm khi tham số tăng. Phát hiện như vậy phù hợp với công trình trước đây về các mô hình MLP (tức là MLP-Mixer) và các mô hình dựa trên attention (tức là ViT).

Để so sánh các mô hình của chúng tôi với baseline một cách công bằng hơn, và đánh giá tốt hơn hiệu suất của các mô hình khi tham số mở rộng, chúng tôi áp dụng thuật toán huấn luyện không giám sát: MoCo V3. Chúng tôi pretrain tất cả các mô hình trên bộ dữ liệu ImageNet-1k với MoCo V3 và sau đó fine-tune chúng. Cả Sparse-MLP và các mô hình baseline đều có thể đạt hiệu suất tốt hơn với việc tăng tham số.
