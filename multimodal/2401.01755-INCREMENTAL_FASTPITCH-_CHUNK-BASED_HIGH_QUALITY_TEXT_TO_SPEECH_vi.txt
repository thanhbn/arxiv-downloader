# 2401.01755.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2401.01755.pdf
# Kích thước tệp: 1030930 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
INCREMENTAL FASTPITCH: CHUYỂN ĐỔI VĂN BẢN SANG TIẾNG NÓI CHẤT LƯỢNG CAO DỰA TRÊN CHUNK
Muyang Du1, Chuan Liu1, Junjie Lai1
1Tập đoàn NVIDIA
TÓM TẮT
Các mô hình text-to-speech song song đã được áp dụng rộng rãi cho tổng hợp tiếng nói thời gian thực, và chúng cung cấp khả năng kiểm soát tốt hơn và quá trình tổng hợp nhanh hơn nhiều so với các mô hình tự hồi quy truyền thống. Mặc dù các mô hình song song có lợi ích trong nhiều khía cạnh, chúng trở nên không phù hợp với tổng hợp tăng dần do kiến trúc song song hoàn toàn của chúng như transformer. Trong nghiên cứu này, chúng tôi đề xuất Incremental FastPitch, một biến thể mới của FastPitch có khả năng tạo ra các chunk Mel chất lượng cao một cách tăng dần bằng cách cải thiện kiến trúc với các khối FFT dựa trên chunk, huấn luyện với mặt nạ attention bị ràng buộc trường tiếp nhận và suy luận với các trạng thái mô hình quá khứ có kích thước cố định. Kết quả thí nghiệm cho thấy đề xuất của chúng tôi có thể tạo ra chất lượng giọng nói tương đương với FastPitch song song, với độ trễ thấp hơn đáng kể cho phép thời gian phản hồi thấp hơn cho các ứng dụng tổng hợp tiếng nói thời gian thực.
Từ khóa chỉ mục — text-to-speech, tổng hợp tiếng nói, thời gian thực, độ trễ thấp, streaming tts

1. GIỚI THIỆU
Trong những năm gần đây, công nghệ Text-to-Speech (TTS) đã chứng kiến những tiến bộ đáng kể, cho phép tạo ra tiếng nói tự nhiên và biểu cảm từ đầu vào văn bản. Hệ thống TTS thần kinh chủ yếu bao gồm một mô hình âm thanh và một vocoder. Nó bao gồm việc chuyển đổi văn bản thành Mel-spectrogram bằng các mô hình âm thanh như Tacotron 2[1], FastSpeech[2], FastPitch[3], GlowTTS[4], sau đó chuyển đổi đặc trưng Mel thành dạng sóng bằng các vocoder như WaveNet[5], WaveRNN[6, 7], WaveGlow[8], và HiF-GAN[9]. Hơn nữa, với sự phát triển của các ứng dụng thời gian thực và streaming, ngày càng có nhu cầu về các hệ thống TTS có khả năng tạo ra tiếng nói một cách tăng dần, còn được gọi là streaming TTS, để cung cấp độ trễ phản hồi thấp hơn cho trải nghiệm người dùng tốt hơn. Ví dụ, Samsung[10] đã đề xuất một hệ thống streaming TTS độ trễ thấp chạy trên CPU dựa trên Tacotron 2 và LPCNet[11]. NVIDIA[12] cũng đã đề xuất một pipeline streaming TTS hiệu quả cao chạy trên GPU dựa trên BERT[13], Tacotron 2 và HiFi-GAN. Cả hai đều sử dụng mô hình âm thanh tự hồi quy cho việc tạo Mel tăng dần.

Các mô hình âm thanh tự hồi quy như Tacotron 2 có khả năng tạo ra tiếng nói tự nhiên bằng cách tận dụng việc tạo tuần tự để nắm bắt nhịp điệu và các phụ thuộc ngữ cảnh. Tuy nhiên, nó gặp phải vấn đề suy luận chậm do quá trình tạo từng khung và khả năng dễ bị các artifact tạo quá mức và lặp từ do việc căn chỉnh không ổn định được học giữa các âm vị đầu vào và các khung đầu ra. Ngược lại, các mô hình âm thanh song song như FastPitch cung cấp quá trình suy luận nhanh hơn bằng cách tạo ra Mel-spectrogram hoàn chỉnh trong một bước. Ngoài ra, nó cũng cho thấy lợi ích trong việc cung cấp tính linh hoạt để điều chỉnh pitch, thời lượng và tốc độ của tiếng nói tổng hợp vì những metadata đó được tạo trước khi giải mã.

Hình 1: Incremental FastPitch, Khối FFT dựa trên Chunk và Chunk Mask cho Huấn luyện bị ràng buộc trường tiếp nhận

Mặc dù các mô hình âm thanh song song cung cấp nhiều lợi thế, cấu trúc mô hình của chúng đặt ra thách thức cho việc sử dụng trong tổng hợp tiếng nói tăng dần. Ví dụ, FastPitch sử dụng một decoder transformer[14], trong đó attention được tính toán trên toàn bộ chuỗi đặc trưng được mã hóa để tạo ra đầu ra Mel-spectrogram. Một phương pháp đơn giản là cắt chuỗi đặc trưng được mã hóa thành các chunk và sau đó giải mã từng chunk thành một chunk Mel tương ứng. Tuy nhiên, phương pháp này buộc decoder chỉ tập trung vào một chunk, dẫn đến sự gián đoạn có thể nghe thấy ở các cạnh của các chunk Mel, ngay cả khi sử dụng việc chồng lấp giữa các chunk. Một phương pháp thay thế là sửa đổi mô hình để sử dụng decoder tự hồi quy. Tuy nhiên, điều này quay trở lại việc tạo từng khung, hy sinh lợi thế song song. Do đó, một decoder lý tưởng cho TTS tăng dần nên có khả năng tạo các chunk Mel một cách tăng dần trong khi duy trì tính song song trong quá trình tạo chunk và giữ độ phức tạp tính toán của mỗi chunk nhất quán trong thời gian đó.

Dựa trên các cân nhắc trên, chúng tôi trình bày Incremental FastPitch, có khả năng tạo ra các chunk Mel chất lượng cao trong khi duy trì tính song song tạo chunk và cung cấp độ trễ phản hồi thấp. Chúng tôi kết hợp các khối FFT dựa trên chunk với việc cache trạng thái attention có kích thước cố định, điều này rất quan trọng đối với TTS tăng dần dựa trên transformer để tránh độ phức tạp tính toán tăng theo độ dài tổng hợp. Chúng tôi cũng sử dụng huấn luyện bị ràng buộc trường tiếp nhận và nghiên cứu cả mặt nạ chunk tĩnh và động, điều này rất quan trọng để căn chỉnh mô hình với suy luận trường tiếp nhận hạn chế.

2. PHƯƠNG PHÁP
2.1. Incremental FastPitch
Hình 1A mô tả mô hình Incremental FastPitch được đề xuất, một biến thể của FastPitch song song. Nó nhận một chuỗi âm vị hoàn chỉnh làm đầu vào và tạo ra Mel-spectrogram một cách tăng dần, từng chunk một, với mỗi chunk chứa một số khung Mel cố định. Incremental FastPitch được trang bị cùng encoder, energy predictor, pitch predictor và duration predictor như FastPitch song song. Tuy nhiên, decoder của Incremental FastPitch được cấu tạo từ một stack các khối FFT dựa trên chunk. Trái ngược với decoder của FastPitch song song nhận toàn bộ đặc trưng thống nhất được upsampling ūlàm đầu vào và tạo ra toàn bộ Mel-spectrogram cùng một lúc, decoder của Incremental FastPitch trước tiên chia ūthành N chunk [ū1,ū2, ...,ūN], sau đó chuyển đổi từng chunk ūi một lúc thành một chunk Mel ȳi. Trong quá trình huấn luyện, chúng tôi áp dụng mặt nạ attention dựa trên chunk lên decoder để giúp nó thích nghi với trường tiếp nhận bị ràng buộc trong suy luận tăng dần, mà chúng tôi gọi là Huấn luyện bị ràng buộc trường tiếp nhận.

2.2. Khối FFT dựa trên Chunk
Hình 1B minh họa khối FFT dựa trên chunk, chứa một stack của một khối multi-head attention (MHA) và một khối feed forward tích chập nhân quả theo vị trí. So với FastPitch song song, khối MHA trong khối FFT dựa trên chunk yêu cầu hai đầu vào bổ sung: past key và past value, được tạo ra bởi chính nó trong quá trình tạo chunk trước đó. Thay vì sử dụng tất cả các past key và past value lịch sử tích lũy từ các chunk trước, chúng tôi sử dụng past key và past value có kích thước cố định cho suy luận bằng cách chỉ giữ lại phần đuôi của chúng. Kích thước past duy trì nhất quán trong suốt quá trình tạo tăng dần, ngăn ngừa sự gia tăng độ phức tạp tính toán theo số lượng chunk. Mặc dù chúng tôi áp đặt một giới hạn kích thước past rõ ràng, các thí nghiệm cho thấy nó có khả năng mã hóa đủ thông tin lịch sử để tạo ra Mel chất lượng cao. Tính toán MHA được định nghĩa là:

k^t_i = concat(pk^(t-1)_i, KW^K_i)
v^t_i = concat(pv^(t-1)_i, VW^V_i)
o^t_i = attention(k^t_i, v^t_i, QW^Q_i)
o^t_M = concat(o^t_1, ..., o^t_h)W^O
pk^t_i = tail_slice(k^t_i, S_p)
pv^t_i = tail_slice(v^t_i, S_p)                    (1)

trong đó pk^(t-1)_i và pv^(t-1)_i là past K và past V của head i từ chunk t-1. k^t_i và v^t_i là K và V được nhúng với past được nối theo chiều thời gian cho tính toán attention của head i tại chunk t. o^t_M là đầu ra của khối MHA tại chunk t. W^K_i, W^V_i, W^Q_i, và W^O là các trọng số có thể huấn luyện. S_p là kích thước cố định có thể cấu hình của past. pk^t_i và pv^t_i được thu được bằng cách cắt kích thước S_p từ đuôi của k^t_i và v^t_i theo chiều thời gian.

Tương tự, tính toán của khối feed forward tích chập nhân quả theo vị trí được định nghĩa là:

c^t_1 = concat(pc^(t-1)_1, o^t_M)
o^t_c1 = relu(causal_conv(c^t_1))
c^t_2 = concat(pc^(t-1)_2, o^t_c1)
o^t_c2 = relu(causal_conv(c^t_2))
pc^t_1 = tail_slice(c^t_1, S_c1)
pc^t_2 = tail_slice(c^t_2, S_c2)                   (2)

trong đó pc^(t-1)_1 và pc^(t-1)_2 là các trạng thái past của hai lớp tích chập nhân quả. Bắt đầu với pc^(t-1)_1, nó được nối với o^t_M để tạo ra c^t_1, phục vụ làm đầu vào cho lớp causal conv đầu tiên. Tiếp theo, o^t_c1, đầu ra từ lớp causal conv đầu tiên, được nối với pc^(t-1)_2 để tạo ra c^t_2. Điều này sau đó được đưa vào lớp causal conv thứ hai, tạo ra đầu ra cuối cùng o^t_c2. Cuối cùng, pc^t_1 và pc^t_2 được trích xuất bằng cách cắt kích thước S_c1 và S_c2 từ đuôi của c^t_1 và pc^t_2 theo chiều thời gian, tương ứng. Không giống như S_p có thể cấu hình, chúng tôi đặt S_c1 và S_c2 thành kích thước kernel conv tương ứng trừ 1, điều này đủ để đạt được sự tương đương với suy luận song song.

2.3. Phân tích trường tiếp nhận của Decoder
Hình 2 minh họa trường tiếp nhận của decoder dựa trên chunk được đề xuất. Để visualize tốt hơn, chúng tôi bỏ qua các khối feed-forward tích chập theo vị trí. Khối màu cam ở góc trên bên phải đại diện cho đầu ra FFT cuối cùng O_t của chunk t. Các khối MHA màu xanh lá cây đậm là những khối mà multi-head attention, past key và past value output đóng góp vào O_t. Các khối MHA màu xanh lá cây nhạt là những khối

--- TRANG 2 ---
mà past key và past value output đóng góp vào O_t. Tương tự, các khối màu xanh dương (past key và past value) và các khối màu vàng (đầu vào của các khối MHA màu xanh lá cây) là những khối đóng góp vào O_t. Bằng cách cung cấp past key và past value có kích thước cố định của chunk t-1 cho mỗi khối MHA trong quá trình tạo chunk t, chúng tôi có thể mở rộng trường tiếp nhận của chunk t tới một số chunk trước đó của nó mà không cần phải cung cấp rõ ràng những chunk trước đó đó làm đầu vào decoder.

Trường tiếp nhận R phụ thuộc vào số lượng lớp decoder và kích thước của past key và past value, được cho bởi:

R = (N_d + ⌊S_p/S_c⌋ + 1) · S_c                  (3)

trong đó N_d là số lượng lớp decoder, S_p là kích thước của past key và past value, và S_c là kích thước của chunk. Đơn vị của R là số lượng khung decoder. Nếu S_p nhỏ hơn hoặc bằng S_c, thì past key và past value output bởi một khối MHA chỉ phụ thuộc vào đầu vào của khối MHA đó, do đó R đơn giản bằng (N_d + 1) · S_c, giống như được hiển thị trong hình 2, trong khi nếu S_p lớn hơn S_c, thì past key và past value của một khối MHA tại chunk t cũng sẽ phụ thuộc vào past key và past value của khối MHA đó tại các chunk trước, dẫn đến R tăng tuyến tính với phần nguyên của S_p/S_c.

2.4. Huấn luyện bị ràng buộc trường tiếp nhận
Với trường tiếp nhận decoder hạn chế trong quá trình suy luận, việc căn chỉnh decoder với ràng buộc này trong quá trình huấn luyện trở nên quan trọng. Do đó, chúng tôi sử dụng Huấn luyện bị ràng buộc trường tiếp nhận bằng cách áp dụng mặt nạ attention dựa trên chunk cho tất cả các lớp decoder. Hình 1C visualize các mặt nạ attention khác nhau với kích thước chunk cho trước (màu xám đậm) và các kích thước past khác nhau (màu xám nhạt). Một phương pháp trực quan là chọn ngẫu nhiên kích thước chunk và kích thước past để tạo mặt nạ động cho mỗi cặp dữ liệu huấn luyện text-audio trong một batch. Phương pháp này tương tự như các mặt nạ được sử dụng trong encoder WeNet[15, 16] ASR. Mặt nạ động có thể giúp decoder tổng quát hóa với các kích thước chunk và past đa dạng. Tuy nhiên, hầu hết các hệ thống TTS tăng dần sử dụng kích thước chunk cố định cho suy luận. Sử dụng mặt nạ động cho huấn luyện có thể tạo ra khoảng cách giữa huấn luyện và suy luận. Do đó, chúng tôi cũng nghiên cứu huấn luyện với mặt nạ tĩnh được xây dựng bằng cách sử dụng kích thước chunk và kích thước past cố định trong quá trình huấn luyện.

3. THÍ NGHIỆM
3.1. Thiết lập thí nghiệm
Tập dữ liệu. Tập dữ liệu tiếng nói tiếng Quan Thoại Trung Quốc tiêu chuẩn[17] được phát hành bởi DataBaker được sử dụng cho cả huấn luyện và đánh giá. Nó chứa 10.000 clip âm thanh 48kHz 16bit của một người nói nữ tiếng Quan Thoại duy nhất và có tổng cộng 12 giờ với mỗi clip âm thanh chứa một câu ngắn trung bình 4,27 giây. Trong các thí nghiệm của chúng tôi, chúng tôi downsample tập dữ liệu xuống 22,05kHz và 100 clip âm thanh được dành riêng cho đánh giá.

Mô hình & Thông số âm thanh. Các tham số mô hình được đề xuất tuân theo việc triển khai FastPitch mã nguồn mở[18], ngoại trừ việc chúng tôi sử dụng tích chập nhân quả trong các lớp feed forward theo vị trí. Decoder được sử dụng để dự đoán Mel-spectrogram với 80 bin tần số. Nó được tạo ra thông qua kích thước FFT 1024, độ dài hop 256 và độ dài cửa sổ 1024, được áp dụng cho dạng sóng được chuẩn hóa. Để tăng tốc độ hội tụ và độ ổn định, các giá trị Mel được chuẩn hóa trong phạm vi đối xứng từ -4 đến 4.

Huấn luyện & Đánh giá. Các mô hình của chúng tôi được huấn luyện bằng bộ tối ưu hóa Adam[19] với kích thước batch 8, khởi tạo với tốc độ học 1e-4 và weight decay 1e-6. Các thí nghiệm được thực hiện trên GPU NVIDIA RTX 6000, sử dụng độ chính xác đơn và áp dụng gradient clipping[20]. Chúng tôi sử dụng khoảng cách Mel-spectrogram (MSD) và điểm ý kiến trung bình (MOS) để đo lường chất lượng giọng nói. Để đảm bảo các Mel-spectrogram của hai âm thanh được căn chỉnh đúng cách cho tính toán MSD, trước tiên chúng tôi sử dụng một FastPitch song song đã được huấn luyện để tạo ra các giá trị thời lượng, pitch và năng lượng thống nhất cho các văn bản đánh giá, sau đó sử dụng các giá trị này để xử lý đầu ra của encoder Incremental FastPitch. Về MOS, chúng tôi tổng hợp dạng sóng để đánh giá với HiFi-GAN được huấn luyện bằng cùng tập dữ liệu với FastPitch. Vì chúng tôi tập trung vào tối ưu hóa mô hình âm thanh cho TTS tăng dần, quá trình vocoding không tăng dần. Đối với Incremental FastPitch, chúng tôi nối tất cả các chunk Mel thành Mel hoàn chỉnh để vocoding. Các điểm MOS được thu thập thông qua đánh giá của 20 mẫu đánh giá cho mỗi cấu hình bởi 10 người nghe Amazon MTurk, những người gán điểm từ 1 đến 5. Đối với các mẫu âm thanh, vui lòng tham khảo trang GitHub1.

3.2. Thảo luận
3.2.1. So sánh mặt nạ chunk tĩnh và động
Hình 3 hiển thị khoảng cách Mel-spectrogram giữa Incremental FastPitch và FastPitch song song. Trong quá trình suy luận, chúng tôi sử dụng kích thước chunk cố định 30 cho tất cả các mô hình. Trong hình con A, các mô hình được huấn luyện với mặt nạ chunk tĩnh. Các kích thước chunk được cố định ở 30 và kích thước past được đặt ở 0, 5, 15, 30, 60, 90 và all. Chúng tôi có thể quan sát thấy rằng MSD nhỏ nhất của mỗi mô hình thường đạt được khi chúng tôi sử dụng kích thước chunk và kích thước past giống nhau (hoặc tương tự) cho huấn luyện và suy luận. MSD nhỏ nhất đạt được với kích thước past 5 (được đánh dấu màu đỏ). Cụ thể, chúng tôi thấy rằng nếu mô hình được huấn luyện với kích thước past nhỏ như 5, nó có MSD cao khi suy luận với kích thước past lớn như 90. Ngược lại, nếu mô hình được huấn luyện với kích thước past lớn, nó có MSD ổn định hơn khi suy luận với kích thước past nhỏ. Quan sát này cho thấy rằng ngay cả khi mô hình được huấn luyện với ngữ cảnh past lớn hơn, nó vẫn học cách tạo chunk Mel dựa trên các ngữ cảnh past gần đó, thay vì những ngữ cảnh xa chunk hiện tại.

Trong hình con B, các mô hình được huấn luyện với mặt nạ chunk động. Các kích thước chunk được chọn ngẫu nhiên từ phạm vi 1 đến 50, và kích thước past được đặt ở 0, 0.25, 0.5, 1, 2, 3 lần kích thước chunk được chọn và all. Chúng tôi quan sát thấy rằng MSD ổn định hơn và tương tự nếu kích thước past suy luận thay đổi, so với mặt nạ tĩnh. MSD nhỏ nhất đạt được khi chúng tôi sử dụng 2 lần kích thước chunk được chọn ngẫu nhiên làm kích thước past. Tuy nhiên, MSD của các mô hình mặt nạ chunk động nhìn chung cao hơn các mô hình mặt nạ chunk tĩnh. Quan sát này xác nhận nghi ngờ của chúng tôi được nêu ra trong phần 2.4 rằng huấn luyện mặt nạ động có thể tạo ra sự không khớp giữa huấn luyện và suy luận. Dựa trên phân tích trên, được khuyến nghị sử dụng mặt nạ tĩnh để có chất lượng tốt nhất nếu kích thước chunk và past suy luận có thể được biết trước.

3.2.2. Nghiên cứu ablation có hình ảnh
Chúng tôi thực hiện nghiên cứu ablation có hình ảnh để điều tra sự cần thiết của việc sử dụng past key value và past conv state. Hình 4 hiển thị các Mel-spectrogram tổng hợp của FastPitch song song và Incremental FastPitch. Chúng tôi có thể quan sát thấy rằng Incremental FastPitch có thể tạo ra Mel gần như không có sự khác biệt quan sát được so với FastPitch song song. Tuy nhiên, nếu past key value hoặc conv state bị loại bỏ, có thể thấy sự gián đoạn rõ ràng giữa các chunk Mel liền kề.

3.2.3. Đánh giá chất lượng giọng nói và hiệu suất
Để nghiên cứu chất lượng giọng nói có thể nghe được của cả mô hình Incremental FastPitch được huấn luyện với mặt nạ tĩnh (S) và động (D), chúng tôi thực hiện các bài kiểm tra nghe trên các mô hình S và D tốt nhất được chọn dựa trên phân tích MSD (được đánh dấu màu đỏ trong hình 3). Như được hiển thị trong bảng 1, chúng tôi thấy rằng Incremental FastPitch có khả năng tạo ra giọng nói chất lượng cao tương đương với FastPitch song song. Hơn nữa, điểm số của mô hình D chỉ thấp hơn mô hình S một chút, mặc dù mô hình D có MSD cao hơn 8,3% so với mô hình S. Kết quả này cho thấy sự khác biệt có thể nghe được của mô hình S và D hầu như không đáng chú ý, đặc biệt là với sự bù đắp của vocoder.

Bảng 1: Điểm ý kiến trung bình (MOS) với 95% CI, hệ số thời gian thực (RTF) và so sánh độ trễ (ms) trên tập đánh giá.

Mô hình | MOS | Độ trễ | RTF
---|---|---|---
Par. FastPitch | 4.185 ±0.043 | 125.77 | 0.029
Inc. FastPitch (S) | 4.178 ±0.047 | 30.35 | 0.045
Inc. FastPitch (D) | 4.145 ±0.052 | | 
Ground Truth | 4.545 ±0.039 | - | -

Bảng 1 cũng hiển thị RTF và độ trễ. Đối với Incremental FastPitch, RTF được định nghĩa là chia độ trễ của chunk cuối cùng cho thời lượng âm thanh, và độ trễ tương ứng với độ trễ của chunk đầu tiên. Mô hình S và D chia sẻ cùng quá trình suy luận. Chúng tôi thấy rằng Incremental FastPitch có RTF cao hơn nhưng vẫn có thể đạt được khoảng 22× thời gian thực vì nó duy trì tính song song của việc tạo chunk. Đáng chú ý, nó có độ trễ thấp hơn đáng kể so với FastPitch song song.

4. KẾT LUẬN
Trong nghiên cứu này, chúng tôi đề xuất Incremental FastPitch, có khả năng tạo ra các chunk Mel chất lượng cao một cách tăng dần với độ trễ thấp trong khi duy trì tính song song tạo chunk và độ phức tạp tính toán nhất quán. Chúng tôi cải thiện decoder với các khối FFT dựa trên chunk sử dụng state caching kích thước cố định để duy trì tính liên tục của Mel qua các chunk. Chúng tôi thêm vào thí nghiệm với nhiều cấu hình masking của huấn luyện bị ràng buộc trường tiếp nhận để thích ứng mô hình với suy luận trường tiếp nhận hạn chế. Các thí nghiệm cho thấy đề xuất của chúng tôi có thể tạo ra chất lượng giọng nói tương đương với baseline song song, với độ trễ thấp hơn đáng kể cho phép thời gian phản hồi thấp hơn cho tổng hợp tiếng nói thời gian thực.

--- TRANG 5 ---
5. TÀI LIỆU THAM KHẢO

[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al., "Natural tts synthesis by conditioning wavenet on mel spectrogram predictions," in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 4779–4783.

[2] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu, "Fastspeech: Fast, robust and controllable text to speech," Advances in neural information processing systems, vol. 32, 2019.

[3] Adrian Łańcucki, "Fastpitch: Parallel text-to-speech with pitch prediction," in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6588–6592.

[4] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon, "Glow-tts: A generative flow for text-to-speech via monotonic alignment search," Advances in Neural Information Processing Systems, vol. 33, pp. 8067–8077, 2020.

[5] A van den Oord, S Dieleman, H Zen, K Simonyan, O Vinyals, A Graves, N Kalchbrenner, AW Senior, and K Kavukcuoglu, "Wavenet: A generative model for raw audio, corr, vol. abs/1609.03499," 2017.

[6] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu, "Efficient neural audio synthesis," in International Conference on Machine Learning. PMLR, 2018, pp. 2410–2419.

[7] Muyang Du, Chuan Liu, Jiaxing Qi, and Junjie Lai, "Improving WaveRNN with Heuristic Dynamic Blending for Fast and High-Quality GPU Vocoding," in Proc. INTERSPEECH 2023, 2023, pp. 4344–4348.

[8] Ryan Prenger, Rafael Valle, and Bryan Catanzaro, "Waveglow: A flow-based generative network for speech synthesis," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 3617–3621.

[9] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis," Advances in Neural Information Processing Systems, vol. 33, pp. 17022–17033, 2020.

[10] Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos, Aimilios Chalamandaris, Georgia Maniati, Panos Kakoulidis, Spyros Raptis, June Sig Sung, Hyoungmin Park, and Pirros Tsiakoulis, "High quality streaming speech synthesis with low, sentence-length-independent latency," pp. 2022–2026, ISCA.

[11] Jean-Marc Valin and Jan Skoglund, "Lpcnet: Improving neural speech synthesis through linear prediction," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5891–5895.

[12] Muyang Du, Chuan Liu, Jiaxing Qi, and Junjie Lai, "Efficient incremental text-to-speech on gpus," in 2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2023, pp. 1422–1428.

[13] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in Proceedings of naacL-HLT, 2019, vol. 1, p. 2.

[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[15] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, "WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit," in Proc. Interspeech 2021, 2021, pp. 4054–4058.

[16] Binbin Zhang, Di Wu, Zhendong Peng, Xingchen Song, Zhuoyuan Yao, Hang Lv, Lei Xie, Chao Yang, Fuping Pan, and Jianwei Niu, "WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit," in Proc. Interspeech 2022, 2022, pp. 1661–1665.

[17] Databaker, "Chinese standard mandarin speech corpus," https://www.data-baker.com/open_source.html, 2023, Accessed: September 3, 2023.

[18] NVIDIA, "Fastpitch," https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/SpeechSynthesis/FastPitch, 2023, Accessed: September 3, 2023.

[19] Diederik P. Kingma and Jimmy Ba, "Adam: A method for stochastic optimization," in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun, Eds., 2015.

[20] Xiangyi Chen, Steven Z Wu, and Mingyi Hong, "Understanding gradient clipping in private sgd: A geometric perspective," Advances in Neural Information Processing Systems, vol. 33, pp. 13773–13782, 2020.
