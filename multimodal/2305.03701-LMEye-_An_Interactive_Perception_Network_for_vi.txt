LMEye: Một Mạng Nhận Thức Tương Tác cho
Các Mô Hình Ngôn Ngữ Lớn

Yunxin Li¹, Baotian Hu¹, Xinyu Chen¹, Lin Ma², Yong Xu¹, và Min Zhang¹
¹Viện Công Nghệ Harbin, Thâm Quyến
²Meituan, Bắc Kinh
liyunxin987@163.com, {hubaotian, zhangmin2021}@hit.edu.cn
forest.linma@gmail.com
https://github.com/YunxinLi/LingCloud

Tóm tắt
Huấn luyện một Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) từ đầu, như GPT-4, tiêu tốn rất nhiều tài nguyên. Coi các Mô hình Ngôn ngữ Lớn (LLM) là bộ xử lý cốt lõi cho thông tin đa phương thức, bài báo của chúng tôi giới thiệu LMEye, một con mắt giống con người với mạng nhận thức tương tác có thể cắm và chạy, được thiết kế để cho phép tương tác động giữa các LLM và thông tin thị giác bên ngoài. Các phương pháp trước đây kết hợp thông tin thị giác vào LLM bằng một mạng ánh xạ thị giác đơn giản hoặc Q-former từ BLIP-2. Các mạng như vậy chiếu đặc trưng hình ảnh một lần nhưng không xem xét tương tác giữa hình ảnh và truy vấn đầu vào của con người. Do đó, thông tin thị giác thu được mà không được kết nối với ý định của con người có thể không đủ để LLM tạo ra phản hồi theo ý định, điều mà chúng tôi gọi là thông tin thị giác tĩnh. LMEye giải quyết vấn đề này bằng cách cho phép LLM yêu cầu thông tin thị giác mong muốn phù hợp với các chỉ dẫn khác nhau của con người, điều mà chúng tôi gọi là tương tác thông tin thị giác động. Cụ thể, LMEye bao gồm một mạng ánh xạ thị giác đơn giản để cung cấp nhận thức cơ bản về hình ảnh cho LLM. Nó cũng chứa các mô-đun bổ sung chịu trách nhiệm thu thập yêu cầu từ LLM, thực hiện tương tác thông tin thị giác dựa trên yêu cầu, và truyền thông tin thị giác tương tác kết quả đến LLM, tương ứng. Theo cách này, LLM hoạt động để hiểu truy vấn của con người, chuyển yêu cầu tương ứng đến mô-đun tương tác thông tin thị giác dựa trên yêu cầu, và tạo ra phản hồi dựa trên thông tin đa phương thức xen kẽ. Chúng tôi đánh giá LMEye thông qua các thí nghiệm mở rộng trên một số điểm chuẩn đa phương thức, chứng minh rằng nó cải thiện đáng kể hiệu suất zero-shot trên các tác vụ đa phương thức khác nhau so với các phương pháp trước đây, với ít tham số hơn.

1 Giới thiệu
Các Mô hình Thị giác-Ngôn ngữ (VLM) [1,44] được huấn luyện trên một lượng lớn dữ liệu hình ảnh-văn bản đã cho thấy kết quả ấn tượng trong các tác vụ hiểu và tạo sinh đa phương thức khác nhau. Tuy nhiên, việc huấn luyện một MLLM (ví dụ: Flamingo [1], Kosmos-1 [16], và GPT-4 [34]) từ đầu tiêu tốn rất nhiều tài nguyên. Để giảm nhẹ vấn đề này, các nỗ lực mã nguồn mở trước đây [30,21,8,14] cho thấy rằng chúng ta có thể xây dựng một MLLM dựa trên mô hình ngôn ngữ lớn chỉ văn bản (LLM) thông qua việc chuyển đổi thông tin thị giác (thu được bởi các bộ mã hóa thị giác đã được huấn luyện trước đóng băng [38,7]) vào không gian biểu diễn của LLM. Bằng cách này, các LLM có khả năng hiểu thông tin thị giác và thực hiện tương tác người-máy đa phương thức. Đáng kể, toàn bộ quá trình huấn luyện có hiệu quả tham số vì nó chỉ cần tối ưu hóa một số ít tham số của bộ chuyển đổi đặc trưng thị giác-ngôn ngữ, tương tự như các phương pháp điều chỉnh tiền tố hoặc prompt phổ biến [23, 17].

Các nghiên cứu gần đây [30] chứng minh rằng một mạng ánh xạ tuyến tính có thể học được có thể cho phép LLM kết hợp thông tin nhận thức toàn cục cơ bản của một hình ảnh. Khác với các VLM thông thường, ví dụ: Oscar [22] và OFA [44], các MLLM được xây dựng theo cách này thường thực hiện tạo sinh đa phương thức tốt [30] vì LLM có khả năng hiểu ngữ cảnh, lý luận và tạo sinh mạnh mẽ. Để tiến xa hơn theo hướng này, [18] trình bày mô hình FROMAGe, nơi họ đóng băng LLM và bộ mã hóa thị giác và tinh chỉnh một số lớp ánh xạ tuyến tính để đạt được tương tác thông tin đa phương thức. Nó đạt được hiệu suất zero-shot mạnh mẽ trên các tác vụ truy xuất hình ảnh theo ngữ cảnh và đối thoại đa phương thức. [21] đề xuất BLIP-2 với một Querying Transformer nhẹ để bắc cầu khoảng cách ngữ nghĩa thị giác và ngôn ngữ cho các bộ mã hóa hình ảnh đóng băng và các mô hình ngôn ngữ lớn. Ngoài ra, phương pháp điều chỉnh theo hướng dẫn đa phương thức gần đây được giới thiệu bởi [25] và [54] để nâng cao khả năng tương tác đa phương thức của LLM, cho thấy hiệu suất giám sát trên các tình huống đa phương thức khác nhau.

Tuy nhiên, đối với các phương pháp trước đây như BLIP-2 và FROMAGe, đặc trưng thị giác được đưa vào LLM được chuyển đổi một lần thông qua mạng ánh xạ thị giác và không tương tác với các truy vấn đầu vào của con người, điều mà chúng tôi gọi là thông tin thị giác tĩnh. Do đó, mô hình ngôn ngữ có thể không thu được thông tin thị giác đầy đủ cho các truy vấn khác nhau. Để giải quyết vấn đề này, chúng tôi trình bày một con mắt giống con người với mạng nhận thức tương tác cho LLM, cụ thể là LMEye, cho phép LLM yêu cầu thông tin thị giác mong muốn phù hợp với các chỉ dẫn khác nhau của con người. Từ quan điểm của Agent, chúng tôi coi LLM là bộ xử lý cốt lõi của thông tin phức tạp và không sửa đổi cấu trúc của LLM. Nếu không, có thể có nguy cơ làm suy giảm hiệu suất ban đầu của chúng trên các tác vụ NLP, tương tự như [8], do đó làm suy yếu tính tổng quát của LLM. Cụ thể, LMEye chủ yếu bao gồm hai giai đoạn: 1) giai đoạn đầu tiên cung cấp thông tin nhận thức cơ bản của một hình ảnh cho LLM, được gọi là căn chỉnh đặc trưng. Chúng tôi áp dụng mạng ánh xạ thị giác Q-Former từ BLIP-2 được sử dụng rộng rãi để đạt được điều này. 2) giai đoạn khác chịu trách nhiệm thu thập yêu cầu từ LLM, thực hiện tương tác thông tin thị giác dựa trên yêu cầu, và truyền thông tin thị giác tương tác kết quả đến LLM. Chúng tôi giới thiệu mô-đun Tương tác Thông tin Thị giác dựa trên Yêu cầu (RVII) để thực hiện tương tác động như vậy giữa LLM và thông tin thị giác. Bằng cách này, LLM trước tiên hiểu các truy vấn của con người và thông tin cơ bản của hình ảnh, sau đó gửi yêu cầu để thu được thông tin thị giác mong muốn bổ sung, cuối cùng tạo ra phản hồi theo hướng dẫn dựa trên thông tin hình ảnh cơ bản, chỉ dẫn văn bản và thông tin thị giác tương tác.

Tóm lại, những đóng góp của LMEye mà chúng tôi đề xuất nằm ở ba khía cạnh sau.

• Coi LLM là bộ xử lý thông tin đa phương thức, chúng tôi đề xuất một mạng nhận thức tương tác để làm cho chúng kết hợp thông tin thị giác mong muốn cho các truy vấn khác nhau của con người. LLM hoạt động để hiểu truy vấn của con người, chuyển yêu cầu tương ứng đến mô-đun tương tác thông tin thị giác dựa trên yêu cầu, và tạo ra phản hồi dựa trên thông tin đa phương thức xen kẽ. Toàn bộ quá trình huấn luyện có hiệu quả tham số.

• LMEye có thể đạt được hiệu suất hiểu và lý luận đa phương thức vượt trội trên hai điểm chuẩn đánh giá đa phương thức (MMBench và SEED-Bench) với ít tham số hơn (4.4B), so với các MLLM khác (> 7B).

• Các nghiên cứu khử nhiễu cho thấy rằng phương pháp đề xuất cải thiện đáng kể hiệu suất đa phương thức zero-shot cho các quy mô và loại LLM khác nhau, ví dụ: tăng chính xác 5.0% trên OK-VQA cho LMEye (BLIP-2) so với BLIP-2, và tăng chính xác 20% trên VQA với câu trả lời dài cho LMEye (LLaMA-7b) so với LLaVA (Vicuna-7b).

2 Nghiên cứu liên quan
LLM hỗ trợ thị giác. Khác với các mô hình thị giác-ngôn ngữ khác nhau [1,45,16] được huấn luyện từ đầu với các cặp hình ảnh-văn bản quy mô lớn, LLM hỗ trợ thị giác dựa trên một mô hình ngôn ngữ lớn đã được huấn luyện trước, cho phép nó hiểu thông tin thị giác và có thể xử lý thông tin đa phương thức. Chúng thường áp dụng các phương pháp điều chỉnh prefix-tuning [17,23] hoặc adapter-based [52] được đề xuất gần đây để tinh chỉnh mô hình ngôn ngữ trên các tác vụ đa phương thức cụ thể, để chúng có thể thành thạo trong một số tình huống đa phương thức. Ví dụ, [55] đã sử dụng kỹ thuật text-to-image để tạo ra hình ảnh và truyền thông tin thị giác vào mô hình ngôn ngữ để tạo sinh văn bản đa phương thức. [18] đã khám phá việc sử dụng LLM để truy xuất hình ảnh-văn bản và tương tác văn bản-hình ảnh đa phương thức. Để tiến xa hơn theo hướng này, BLIP-2 [21] sử dụng Flan-T5 [6] hoặc OPT [53] với Q-Former để căn chỉnh hiệu quả các đặc trưng thị giác với mô hình ngôn ngữ. Gần đây nhất, PaLM-E [8], có 562 tỷ tham số, đã được phát triển để tích hợp các phương thức cảm biến liên tục thế giới thực vào một LLM, do đó thiết lập kết nối giữa nhận thức thế giới thực và ngôn ngữ con người. Để kết luận, các nghiên cứu trước đây chứng minh rằng đây là một hướng nghiên cứu tiềm năng để cho phép các LLM đóng băng xử lý thông tin đa phương thức.

Điều chỉnh Theo hướng dẫn Đa phương thức. Tiến bộ trong việc điều chỉnh hướng dẫn các LLM chỉ văn bản [35, 32,6] đã đạt được hiệu suất ấn tượng trên các tác vụ NLP và tình huống tương tác người-máy, như Flan-T5, Bloomz [32], và ChatGPT. Gần đây, một số nhà nghiên cứu đã khám phá việc sử dụng dữ liệu hướng dẫn đa phương thức để tinh chỉnh các LLM đã được huấn luyện trước để cải thiện khả năng tương tác người-máy đa phương thức của chúng. [25] sử dụng GPT-4 để sản xuất dữ liệu hướng dẫn đa phương thức và tinh chỉnh mô hình ngôn ngữ LLaMA [41] trên tập dữ liệu theo hướng dẫn đa phương thức tổng hợp. [54] cũng xây dựng một tập dữ liệu theo hướng dẫn đa phương thức được căn chỉnh tốt để tinh chỉnh một mô hình ngôn ngữ được điều chỉnh hướng dẫn mạnh mẽ (Vicuna), và nó đạt được hiệu suất vượt trội trên đối thoại đa phương thức miền mở. [52] trình bày một phương pháp thích ứng nhẹ để tinh chỉnh hiệu quả LLaMA thành một mô hình theo hướng dẫn. Trong bài báo này, chúng tôi giới thiệu các dữ liệu hướng dẫn đa phương thức khác nhau để làm cho LMEye thích ứng với các tình huống đa phương thức miền mở.

Công cụ Thị giác cho LLM. Một hướng nghiên cứu gần đây [31,37] điều tra các cách để tăng cường hiệu suất của LLM bằng cách cho phép chúng truy cập các công cụ bên ngoài như các mô hình nền tảng thị giác, công cụ tìm kiếm, hoặc các API khác để giải quyết các vấn đề phức tạp. Phương pháp này mở rộng phạm vi của LLM trong việc xử lý thông tin với độ phức tạp khác nhau. Ví dụ, Toolformer [39] trao quyền cho LLM quyết định API nào sử dụng, khi nào sử dụng chúng, những đối số nào truyền, và cách kết hợp thông tin kết quả vào việc tạo sinh văn bản. Low-code LLM [3] sử dụng sáu tương tác lập trình thị giác low-code đơn giản, như nhấp chuột, kéo, hoặc chỉnh sửa văn bản, để đạt được phản hồi được kiểm soát và đáng tin cậy hơn. Ngược lại, [28] đề xuất một khung lý luận tổng hợp cắm và chạy Chameleon tăng cường LLM để giải quyết các thách thức phức tạp, như sử dụng các mô hình thị giác có sẵn. [48] giới thiệu visual ChatGPT, thiết kế một tập hợp các prompt để kết hợp thông tin mô hình thị giác vào ChatGPT, xem xét các mô hình với nhiều đầu vào/đầu ra và những mô hình yêu cầu phản hồi thị giác. Không giống như các phương pháp pipeline ở trên, nghiên cứu của chúng tôi tập trung vào khung tương tác end-to-end giữa LLM và thông tin thị giác.

3 LMEye: Mạng Nhận thức Tương tác

3.1 Kiến trúc
Như được hiển thị trong Hình 1, kiến trúc tổng thể của LMEye chứa hai giai đoạn, chịu trách nhiệm cho các chức năng khác nhau. Cho một hình ảnh I và một truy vấn của con người X = (x₁, ..., xₘ), trong đó xᵢ đại diện cho token thứ i của truy vấn đầu vào của con người cho LLM, chúng tôi thu được các đặc trưng hình ảnh toàn cục và chi tiết h^I = (h^I_g, h^I_1, ..., h^I_256) thông qua bộ mã hóa thị giác đã được huấn luyện trước của BLIP-2. Đồng thời, một token đặc biệt có thể học được <img> được thêm vào bảng embedding từ của LLM như thẻ vị trí đầu vào của đặc trưng hình ảnh.

Căn chỉnh Đặc trưng. Chúng tôi sử dụng Q-former đóng băng từ BLIP-2 và một lớp chiếu tuyến tính có thể học được như mạng ánh xạ thị giác để chiếu đặc trưng hình ảnh cơ bản vào không gian embedding ngôn ngữ, ký hiệu là f(h^I). Bằng cách này, LLM có thể thu được thông tin nhận thức cơ bản của một hình ảnh, được thêm vào với biểu diễn của token <img>. Chúng tôi cũng thêm một token đặc biệt khác <img-q> ở cuối chuỗi hình ảnh và truy vấn của con người để nắm bắt toàn bộ thông tin mã hóa của hình ảnh và truy vấn của con người. Do đó, như phần bên trái được hiển thị trong Hình 1, chúng tôi có thể thu được chuỗi đầu vào đầu tiên của LLM bằng (<img>, f(h^I), X, <img-q>) → (h_img, h_X, h_img-q), trong đó h_img đề cập đến phép cộng của f(h^I) và biểu diễn token của <img>. h_X và h_img-q là các biểu diễn mã hóa từ tương ứng của X và <img-q>. Đầu ra cuối cùng của token <img-q> ở lớp cuối cùng của LLM được mong đợi chứa ý nghĩa ngữ nghĩa của truy vấn con người, được trình bày thành h_r ∈ R^(1×d_L), trong đó d_L đề cập đến kích thước trạng thái ẩn của LLM, vì các nghiên cứu trước đây [33,34,41] đã chỉ ra rằng các LLM gần đây đã có thể hiểu các ngôn ngữ con người khác nhau. Ngoài ra, h_r cũng có thể chứa nội dung hình ảnh thông qua cơ chế self-attention của LLM, tuy nhiên chúng tôi cho rằng các LLM chỉ văn bản mà không được huấn luyện trước trên dữ liệu đa phương thức không thể kết hợp thông tin thị giác [30] tốt như các mô hình hình ảnh và ngôn ngữ đa phương thức mạnh mẽ đã được huấn luyện trước. Để tạo thuận lợi cho việc LLM kết hợp thông tin thị giác mong muốn phù hợp với truy vấn của con người, thay vì tối ưu hóa các tham số của LLM (với full-parameter hoặc Low-rank adaptation [15]) trên dữ liệu cụ thể, như LLaVA [25] và mPLUG-Owl [49], chúng tôi thực hiện tương tác giữa truy vấn con người và thông tin thị giác bên ngoài LLM. Theo cách này, LLM vẫn có thể duy trì sức mạnh và tính tổng quát ban đầu của chúng trên các tác vụ ngôn ngữ tự nhiên vì cấu trúc và tham số của LLM không được thay đổi.

Tương tác Thông tin Thị giác dựa trên Yêu cầu (RVII). Đầu tiên, chúng tôi áp dụng một lớp chiếu tuyến tính để ánh xạ trạng thái ẩn h_r ở trên vào không gian của mô-đun tương tác thông tin sau, ký hiệu là h_R ∈ R^(Q×d_RV), trong đó Q và d_RV đề cập đến độ dài của vector yêu cầu và kích thước ẩn của RVII, tương ứng. Chúng tôi coi rằng quá trình này là thu thập tín hiệu yêu cầu từ LLM, ví dụ: như ví dụ được hiển thị trong Hình 1, LLM mã hóa thông tin yêu cầu từ truy vấn con người: "Q1: Họ đang làm gì trong hình ảnh? Q3: Bạn có thể thấy điều gì đặc biệt trong hình ảnh không?".

Sau khi thu được yêu cầu của LLM, chúng tôi đề xuất sử dụng h_R để thực hiện tương tác thông tin đa phương thức với các đặc trưng hình ảnh chi tiết. Để đạt được điều này, chúng tôi áp dụng các khối transformer đa lớp để đạt được tương tác thông tin thị giác dựa trên yêu cầu. Mỗi khối được tính toán như được đưa ra trong Eq. 1:

h^R_l = LayerNorm(h_{l-1} + SelfAttention(h_{l-1})),
h^F_l = LayerNorm(h^R_l + CrossAttention(h^I)),
h_l = LayerNorm(h^F_l + MLP(h^F_l)), (1)

trong đó h_{l-1} là đầu ra của lớp l-1 và đầu vào của RVII là h_R. SelfAttention và CrossAttention dựa trên cơ chế multi-head attention, được sử dụng để nắm bắt thông tin thị giác mong muốn. Sau khi thu được đầu ra của lớp cuối cùng, chúng tôi sử dụng một lớp chiếu tuyến tính có thể học được để truyền thông tin tương tác đến LLM, được ký hiệu là h_t. Sau đó, chuỗi trình bày mới (h_img, h_X, h_t) được đưa vào LLM để tạo ra câu trả lời cuối cùng. Giả sử rằng mục tiêu huấn luyện của một câu hỏi trả lời theo hướng dẫn đa phương thức là Y = (y₁, ..., y_N), trong đó y_i đại diện cho token thứ i và N đề cập đến tổng độ dài, tổn thất tối ưu hóa như sau:

L = -1/N ∑(i=1 to N) log P_i(ŷ_i = y_i|h_img; h_X; h_img-q; y₁, ..., y_{i-1}). (2)

3.2 Điều chỉnh Theo hướng dẫn Đa phương thức
Chúng tôi sử dụng các dữ liệu theo hướng dẫn đa phương thức khác nhau để làm cho mạng nhận thức tương tác hiệu quả. Đầu tiên, chúng tôi xây dựng hai loại dữ liệu khớp ngữ nghĩa hình ảnh-văn bản dựa trên các cặp hình ảnh-văn bản từ các tập dữ liệu CC3M, COCO Caption và Flick3k. Hai loại này là suy luận "Đúng hoặc Sai" và các tác vụ lựa chọn bốn lựa chọn, tương ứng, trong đó các chú thích được lấy mẫu ngẫu nhiên từ tập huấn luyện tương ứng. Bằng cách này, toàn bộ mạng có thể được huấn luyện để giúp và cải thiện việc LLM thực hiện căn chỉnh ngữ nghĩa hình ảnh-văn bản. Thứ hai, để làm cho LMEye thích ứng với các truy vấn khác nhau của con người, chúng tôi giới thiệu dữ liệu theo hướng dẫn đa phương thức về đối thoại và lý luận phức tạp, được phát hành bởi [25]. Ngoài ra, xem xét rằng một hình ảnh phức tạp chứa thông tin thị giác cấp vô hạn và có thể được đính kèm với kiến thức bên ngoài, chúng tôi giới thiệu dữ liệu về mô tả chi tiết của một hình ảnh để cải thiện khả năng tạo sinh văn bản dài đa phương thức. Nó bao gồm dữ liệu tương ứng từ [25] và tập dữ liệu mô tả tác phẩm nghệ thuật SemArt [11]. Tổng số tất cả dữ liệu hướng dẫn là khoảng 7.3M, bao gồm 7.1M dữ liệu khớp ngữ nghĩa, 20k mẫu phân tích tác phẩm nghệ thuật, và 150k mẫu bổ sung. Cuối cùng, giống hệt như InstructBLIP, chúng tôi cũng tăng cường tập dữ liệu theo hướng dẫn bằng cách giới thiệu các tập huấn luyện một phần của khoảng 20 tác vụ đa phương thức, trong khi so sánh chúng trên hai điểm chuẩn đa phương thức.

4 Thí nghiệm

4.1 Cài đặt Thí nghiệm
Tập dữ liệu. Đầu tiên, chúng tôi đánh giá LMEye trên các điểm chuẩn đa phương thức toàn diện được phát hành gần đây: MMBench [26]¹, và SEED-Bench [19], được thiết kế có hệ thống cho việc đánh giá mạnh mẽ các khả năng khác nhau của các mô hình thị giác-ngôn ngữ. Để xác minh hiệu quả của LMEye trong các điều kiện khác nhau, chúng tôi cũng đánh giá LMEye và các mô hình ngôn ngữ thị giác khác trên ba tập dữ liệu hiểu và lý luận thị giác: tập xác thực của VCR [50] và VQAv2 [13], và tập kiểm tra của OK-VQA [29]. Ngoài ra, chúng tôi cũng sử dụng GPT-3.5-turbo [33] để tạo ra năm cặp câu hỏi-trả lời tập trung xung quanh một hình ảnh dựa trên khoảng 3.5k hình ảnh và các mô tả dài của chúng từ [54]. Mẫu prompt là "Tạo ra năm cặp câu hỏi-trả lời cho mô tả hình ảnh chi tiết sau. Yêu cầu: câu trả lời của câu hỏi phải được chứa trong mô tả và loại là Question: ... Answer: ... \n Description: ". Tổng số cặp câu hỏi-trả lời là khoảng 17.5k trong đó độ dài của các câu trả lời vượt quá các tập dữ liệu VQA thông thường, với độ dài trung bình 13 từ. Dữ liệu được xây dựng được sử dụng để đánh giá và phân tích hiệu suất của các mô hình.

Các Mô hình So sánh. Flamingo [1] là một mô hình tạo sinh đa phương thức thống nhất có khả năng thích ứng nhanh chóng với nhiều tác vụ hình ảnh và video. OFA [44] là một khung học sequence-to-sequence có thể thống nhất một tập hợp đa dạng các tác vụ đa phương thức và đơn phương thức. FROMAGe [18] là một LVLM điển hình được huấn luyện hiệu quả bằng cách định vị thị giác các LLM với image captioning và contrastive learning, có khả năng image caption và truy xuất hình ảnh-văn bản. BLIP-2 [21] là một chiến lược huấn luyện hai giai đoạn bootstraps học biểu diễn thị giác-ngôn ngữ và học tạo sinh thị giác-ngôn ngữ dựa trên bộ mã hóa hình ảnh đóng băng và mô hình ngôn ngữ, đạt được hiệu suất state-of-the-art trên các tác vụ đa phương thức khác nhau. Ngoài ra, chúng tôi cũng so sánh các phương pháp của mình với mô hình được điều chỉnh hướng dẫn đa phương thức MiniGPT-4 [54] và LLaVA [25], trong đó MiniGPT-4 dựa trên Q-former đã được huấn luyện trước từ BLIP-2. So với BLIP-2 và FROMAGe, chúng được điều chỉnh với dữ liệu theo hướng dẫn đa phương thức được tạo ra bởi GPT-4. Trong giai đoạn điều chỉnh hướng dẫn đa phương thức, cả ma trận chiếu và LLM của LLaVA đều được cập nhật.

Chi tiết Triển khai. Chúng tôi chạy tất cả các thí nghiệm trên tám GPU Tesla A100-80G với môi trường Python. Để xác minh hiệu quả của LMEye, chúng tôi áp dụng OPT-iml-1.3b, Bloomz-7b1, LLaMA-7b/13b [41], và BLIP-2 (FlanT5 XL) làm backbone của khung của chúng tôi, tương ứng. Trong giai đoạn căn chỉnh đặc trưng, chúng tôi đặt tỷ lệ học ban đầu là 1e-4 và sử dụng bộ tối ưu hóa AdamW [27] để tối ưu hóa quá trình căn chỉnh đặc trưng theo cách giảm cosine. Tổng bước huấn luyện của giai đoạn này là một epoch, và kích thước batch là 768. Trong giai đoạn điều chỉnh hướng dẫn đa phương thức, chúng tôi áp dụng kích thước batch nhỏ hơn (256) và đặt tỷ lệ học ban đầu là 1e-4. Độ sâu của RVII được đặt là 12 và kích thước ẩn bằng 768. Chúng tôi sẽ đóng băng các tham số giai đoạn đầu tiên (bao gồm lớp chiếu tuyến tính trong căn chỉnh đặc trưng và biểu diễn token của <img>, hoặc Q-former trong BLIP-2) trong khi thực hiện điều chỉnh hướng dẫn đa phương thức. Trong quá trình tạo sinh, chúng tôi sử dụng chiến lược tạo sinh beam sample từ repository HuggingFace Transformer² và đặt kích thước beam là 4 và 1 cho việc tạo sinh mô tả hình ảnh chi tiết và VQA, tương ứng.

Metrics Đánh giá. Đối với trả lời câu hỏi thị giác (VQA) với câu trả lời ngắn và các tập dữ liệu lý luận thị giác, chúng tôi áp dụng cách tính toán EM (exactly matching) phổ biến làm phương pháp đánh giá độ chính xác. Đối với việc tạo sinh mô tả hình ảnh chi tiết và VQA với câu trả lời dài, chúng tôi sử dụng một số metrics đánh giá tạo sinh: BLEU [36], ROUGE [24], CIDEr [42], và METEOR [2].

4.2 Kết quả và Phân tích Tổng thể
Kết quả Đánh giá MMBench. Kết quả đánh giá trên MMBench được trình bày trong Bảng 1. Kết quả cho thấy rằng mô hình LMEye mà chúng tôi đề xuất vượt trội hơn các mô hình có thể so sánh khác trong khi sử dụng ít tham số hơn (4.4B so với > 7B). Đáng chú ý là LMEye vượt trội hơn các mô hình khác về hiệu suất lý luận, đặc biệt là trong Lý luận Logic (LR), Lý luận Thuộc tính (AR), và Lý luận Quan hệ (RR). Điều này cho thấy rằng LMEye có khả năng lý luận hiệu quả và tạo ra kết nối giữa các thông tin khác nhau, dẫn đến hiệu suất tốt hơn so với các mô hình khác. Ngoài ra, InstructBLIP có thể được coi là một biến thể LMEye được tăng cường hướng dẫn đa phương thức mà không có RVII, đạt được tương tác giữa các truy vấn của con người và hình ảnh trong Q-former, tuy nhiên LMEye vẫn vượt trội hơn nó trên nhiều khía cạnh.

Kết quả Đánh giá SEED-Bench. Kết quả thí nghiệm được trình bày trong Bảng 2 chứng minh hiệu quả của LMEye trong việc đạt được hiệu suất state-of-the-art (SOTA). Cụ thể, LMEye đã cho thấy những cải thiện đáng kể trong hiểu cảnh, với sự gia tăng 13 điểm so với SOTA trước đó. Hơn nữa, trong danh mục nhận dạng thuộc tính mẫu và hiểu kết nối không gian, LMEye cũng vượt trội hơn InstructBLIP. Những kết quả này làm nổi bật hiệu quả của khung nhận thức tương tác cắm và chạy, trong việc tăng cường khả năng của các mô hình ngôn ngữ để hiểu hình ảnh và hướng dẫn đa phương thức. Nhìn chung, những phát hiện này chứng minh tiềm năng của LLM trong việc thúc đẩy lĩnh vực hiểu hình ảnh và gợi ý rằng các khung nhận thức tương tác cắm và chạy có thể là một phương tiện hiệu quả để tận dụng những khả năng này. Nghiên cứu thêm trong lĩnh vực này có thể mở đường cho các phương pháp tiên tiến và hiệu quả hơn cho việc hiểu hình ảnh, với những ý nghĩa đối với nhiều ứng dụng và ngành công nghiệp.

Hiệu suất zero-shot trên một số tập dữ liệu đa phương thức phổ biến. Các biến thể LMEye với "∗" cho thấy rằng chúng tôi chỉ giữ lại lớp chiếu tuyến tính đã được huấn luyện trước và loại bỏ quá trình tương tác (RVII). "NumImg" đại diện cho tổng số hình ảnh được chứa trong giai đoạn căn chỉnh đặc trưng.

4.3 Nghiên cứu Khử nhiễu
Trả lời Câu hỏi Thị giác và Lý luận Đa phương thức. Kết quả thí nghiệm được trình bày trong Bảng 3, trong đó chúng tôi không trình bày kết quả VCR của LLaVA và MiniGPT-4 vì chúng không tuân theo prompt để chọn một tùy chọn từ bốn ứng cử viên. Điều này có thể được quy cho khả năng tự thân của Vicuna [5], là LLaMA chỉ được điều chỉnh với dữ liệu đối thoại. So với các VLM thông thường như Flamingo-3B và OFA, các biến thể LMEye được thiết kế và các MLLM khác có thể đạt được hiệu suất zero-shot tốt hơn trên lựa chọn câu trả lời (VCR) và các tác vụ tạo sinh câu trả lời ngắn (VQA), ngay cả trong trường hợp LMEye (Bloomz-7b1) chỉ đã nhìn thấy 1.7M hình ảnh trong giai đoạn huấn luyện trước. Do đó, việc áp dụng phương pháp xây dựng MLLM hiệu quả huấn luyện như vậy dựa trên bộ mã hóa thị giác đóng băng và LLM là khả thi. Ngoài ra, việc giới thiệu các mô hình ngôn ngữ mạnh mẽ hơn và dữ liệu hình ảnh-văn bản chất lượng cao trong giai đoạn huấn luyện trước sẽ cải thiện độ chính xác của mô hình ngôn ngữ trong việc hiểu thông tin hình ảnh, ví dụ: so sánh hiệu suất của các biến thể LMEye (Bloomz-7b1) và LMEye (OPT-iml-1.3b) khác nhau. Khi chúng tôi giới thiệu LMEye dựa trên BLIP-2 và huấn luyện khung tương tác thông qua dữ liệu hướng dẫn đa phương thức được thu thập, nó cải thiện đáng kể hiệu suất trên tác vụ vấn đề thị giác phức tạp OK-VQA khoảng 5% và có thể đạt được hiệu suất tốt hơn so với InstructBLIP. Bằng cách so sánh thêm hiệu suất của chúng trong Bảng 1 và 2, chúng ta có thể thấy hiệu quả của việc giới thiệu mạng nhận thức tương tác hai giai đoạn (RVII).

VQA với Câu trả lời Dài và Mô tả Chi tiết. Chúng tôi chủ yếu đánh giá các biến thể LMEye khác nhau trên điểm chuẩn đánh giá được xây dựng: mô tả hình ảnh chi tiết và các tác vụ trả lời câu hỏi thị giác. Theo kết quả thí nghiệm được hiển thị trong Bảng 4, chúng ta có thể thấy rằng các mô hình LMEye được điều chỉnh hướng dẫn đa phương thức cải thiện đáng kể hầu hết tất cả các metrics tạo sinh. Kết hợp với các ví dụ được đưa ra trong phần trên của Hình 2, chúng tôi gợi ý rằng phương pháp điều chỉnh theo hướng dẫn đa phương thức hữu ích cho LLM để đạt được khả năng hiểu hình ảnh tương tự như GPT-4. Ngoài ra, chúng tôi phát hiện rằng LMEye (Bloomz-7b1) có thể hiểu ý định của các câu hỏi khác nhau và tạo ra phản hồi chính xác. So với LLaVA (Vicuna-7b), MiniGPT-4, và BLIP-2, kết hợp thông tin thị giác tĩnh cho các câu hỏi khác nhau về một hình ảnh, phương pháp của chúng tôi có thể thu được thông tin thị giác tương ứng liên quan đến các truy vấn của con người và tạo ra phản hồi chính xác hơn (xem hiệu suất so sánh trong Bảng và Hình 2).

Không giống như dữ liệu theo hướng dẫn đa phương thức được sử dụng bởi MiniGPT-4 và LLaVA, chúng tôi giới thiệu dữ liệu mô tả tác phẩm nghệ thuật như một phần của dữ liệu điều chỉnh hướng dẫn và do đó cải thiện khả năng hiểu tác phẩm nghệ thuật của mô hình. Chúng tôi quan sát thấy rằng LLM có thể sử dụng kiến thức được lưu trữ của chúng để trình bày phân tích đầy đủ cho tác phẩm nghệ thuật, như việc hiểu Tác phẩm nghệ thuật được hiển thị trong Hình 2. Từ Bảng 4, chúng tôi cũng quan sát thấy rằng việc cải thiện khả năng mô tả hình ảnh chi tiết chủ yếu đến từ việc sử dụng dữ liệu hướng dẫn liên quan. Phương pháp của chúng tôi chủ yếu cải thiện hiệu suất của MLLM trên các tác vụ VQA với các truy vấn khác nhau. Chúng tôi trình bày thêm các trường hợp trong Phụ lục. Kết luận, các thí nghiệm khử nhiễu của các biến thể LMEye cho thấy rằng mạng nhận thức tương tác được đề xuất có thể cắm và chạy trong các mô hình ngôn ngữ lớn khác nhau và cải thiện hiệu suất tổng thể bằng cách kết hợp mô-đun thông tin thị giác tương tác dựa trên yêu cầu.

5 Thảo luận và Nghiên cứu Tương lai
LLM được điều chỉnh hướng dẫn có thể tổng quát hóa tốt hơn trên các tác vụ đa phương thức. Nghiên cứu trước đây [21] cho thấy rằng biến thể BLIP-2 với LLM được điều chỉnh hướng dẫn hoạt động tốt nhất trên nhiều tác vụ đa phương thức. Trong Bảng 3, chúng tôi quan sát thấy rằng LMEye (OPT-iml-1.3b)∗ có khả năng hiệu suất tốt hơn trên VCR và OK-VQA so với FROMAGe (OPT-6.7b) với phiên bản OPT quy mô lớn hơn. Điều này có thể được quy cho thực tế là LLM được điều chỉnh hướng dẫn chỉ văn bản hiểu các truy vấn đầu vào của con người tốt hơn so với LLM gốc. Do đó, chúng có hiệu suất tốt hơn trên các tác vụ QA đa phương thức.

Chất lượng và Đa dạng của dữ liệu theo hướng dẫn đa phương thức là quan trọng. Một so sánh giữa LLaVA (Vicuna-7b) và MiniGPT-4 (Vicuna-7b) cho thấy rằng LLaVA, kết hợp một số lượng lớn hơn dữ liệu hướng dẫn đa phương thức đa dạng, vượt trội hơn MiniGPT-4. Phát hiện này phù hợp với nghiên cứu được thực hiện bởi [25], những người chứng minh rằng dữ liệu hướng dẫn đa phương thức đa dạng có thể tăng cường hiệu suất tổng thể của MLLM trên các tác vụ khác nhau. Dữ liệu theo hướng dẫn đa phương thức hiện tại thường được xây dựng bởi GPT-4 mạnh mẽ thông qua kỹ thuật Self-Instruct [46]. Trong khi những dữ liệu hướng dẫn được tạo tự động này thể hiện sự đa dạng, vẫn còn chỗ để cải thiện về mặt chất lượng. Trong tương lai, sẽ có lợi khi kết hợp dữ liệu tác vụ đa phương thức chất lượng cao, bao gồm video, hình ảnh và âm thanh, để tăng cường khả năng toàn diện của MLLM được điều chỉnh hướng dẫn.

Thông tin thị giác nên tương tác với hướng dẫn của con người. Nghiên cứu trước đây InstructBLIP cố gắng đầu vào các câu hỏi văn bản vào Q-former để tinh chỉnh hiệu suất của nó trong tác vụ trả lời câu hỏi thị giác cụ thể, dẫn đến kết quả vượt trội. Những câu hỏi này tạo thuận lợi cho việc trích xuất thông tin thị giác bằng cách sử dụng các lớp self-attention trong kiến trúc Q-Former. Khác với BLIP-2, LMEye tập trung vào việc trích xuất các đặc trưng hình ảnh có tính thông tin cao cho yêu cầu mã hóa từ LLM, đạt được tương tác động giữa LLM và thông tin thị giác. Ngoài ra, chúng tôi giới thiệu dữ liệu theo hướng dẫn đa phương thức đa dạng để huấn luyện LMEye, cho phép chúng thích ứng với một loạt các truy vấn của con người. Do đó, LLM có thể tận dụng thông tin thị giác phong phú để hoàn thành các tác vụ khác nhau một cách hiệu quả. Để kết luận, việc cho phép thông tin thị giác tương tác với hướng dẫn của con người là hiệu quả để cải thiện khả năng của MLLM.

Ảo giác. Trong khi MLLM tạo ra mô tả hình ảnh chi tiết hoặc phân tích tác phẩm nghệ thuật, chúng dễ dàng tạo ra các đoạn vô nghĩa hoặc không trung thực với hình ảnh khách quan và kiến thức thông thường, hoặc bịa đặt sự kiện. Một số trường hợp được hiển thị trong Phụ lục. Để giải quyết vấn đề này, trong tương lai, chúng tôi có thể giới thiệu kỹ thuật căn chỉnh (như Reinforcement Learning from Human Feedback (RLHF) [35]), tăng cường truy xuất, hoặc multimodal chain-of-the-thought (COT) [47] để cải thiện tính thực tế của nội dung được tạo ra.

6 Hạn chế
Trong khi các mô hình của chúng tôi cố gắng tăng cường sự căn chỉnh của chúng với các truy vấn của con người, điều quan trọng là phải thừa nhận rằng chúng không hoàn toàn được căn chỉnh và cũng không hoàn toàn an toàn. Mặc dù có những nỗ lực để cải thiện chất lượng của đầu ra, các mô hình của chúng tôi vẫn có những hạn chế trong việc tránh tạo ra nội dung độc hại hoặc thiên vị, bịa đặt sự kiện và các đầu ra không mong muốn khác. Trong một số trường hợp, mô hình có thể vô tình tạo ra đầu ra gây khó chịu, phân biệt đối xử hoặc có hại. Điều này có thể được quy cho các thiên vị trong dữ liệu huấn luyện hoặc khả năng tự thân của LLM. Hơn nữa, do các ràng buộc về chất lượng và sự đa dạng của dữ liệu theo hướng dẫn đa phương thức có sẵn, mô hình có thể cung cấp phản hồi không chính xác cho một số truy vấn nhất định.

7 Kết luận
Chúng tôi trình bày LMEye, đính kèm con mắt giống con người cho các mô hình ngôn ngữ lớn với một mạng nhận thức tương tác, nhằm đạt được một mô hình ngôn ngữ thị giác lớn thông qua tương tác động giữa LLM và thông tin thị giác. Kết quả thí nghiệm cho thấy rằng phương pháp của chúng tôi với ít tham số hơn đạt được hiệu suất vượt trội trên hai điểm chuẩn đánh giá của MLLM, và các tác vụ trả lời câu hỏi thị giác, mô tả hình ảnh chi tiết và lý luận đa phương thức khác.
