# 2401.00908.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2401.00908.pdf
# Kích thước tệp: 863690 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
DOCLLM: MỘT MÔ HÌNH NGÔN NGỮ SINH TẠO NHẬN BIẾT BỐ CỤC
CHO HIỂU BIẾT TÀI LIỆU ĐA PHƯƠNG THỨC

Dongsheng Wang∗, Natraj Raman∗, Mathieu Sibue∗
Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu
JPMorgan AI Research
{first.last}@jpmchase.com

TÓM TẮT
Các tài liệu doanh nghiệp như biểu mẫu, hóa đơn, hóa đơn bán hàng, báo cáo, hợp đồng và các hồ sơ tương tự khác thường mang ngữ nghĩa phong phú ở giao điểm của các phương thức văn bản và không gian. Các tín hiệu trực quan được cung cấp bởi bố cục phức tạp của chúng đóng vai trò quan trọng trong việc hiểu hiệu quả các tài liệu này. Trong bài báo này, chúng tôi trình bày DocLLM, một mở rộng nhẹ cho các mô hình ngôn ngữ lớn truyền thống (LLM) để suy luận trên các tài liệu trực quan, có tính đến cả ngữ nghĩa văn bản và bố cục không gian. Mô hình của chúng tôi khác biệt so với các LLM đa phương thức hiện có bằng cách tránh các bộ mã hóa hình ảnh đắt đỏ và tập trung độc quyền vào thông tin hộp giới hạn để kết hợp cấu trúc bố cục không gian. Cụ thể, sự căn chỉnh chéo giữa các phương thức văn bản và không gian được nắm bắt bằng cách phân tách cơ chế chú ý trong các transformer cổ điển thành một tập hợp các ma trận tách biệt. Hơn nữa, chúng tôi thiết kế một mục tiêu tiền huấn luyện học để điền vào các đoạn văn bản. Cách tiếp cận này cho phép chúng tôi giải quyết các bố cục không đều và nội dung không đồng nhất thường gặp trong các tài liệu trực quan. Mô hình được tiền huấn luyện được tinh chỉnh bằng cách sử dụng một tập dữ liệu hướng dẫn quy mô lớn, bao gồm bốn nhiệm vụ trí tuệ tài liệu cốt lõi. Chúng tôi chứng minh rằng giải pháp của chúng tôi vượt trội so với các LLM SotA trên 14 trong 16 tập dữ liệu trên tất cả các nhiệm vụ, và khái quát hóa tốt cho 4 trong 5 tập dữ liệu chưa từng thấy trước đó.

Từ khóa: DocAI · VRDU · LLM · GPT · Chú ý không gian

1 Giới thiệu
Các tài liệu có bố cục phong phú, bao gồm hóa đơn, hóa đơn bán hàng, hợp đồng, đơn hàng và biểu mẫu, tạo thành một phần đáng kể trong các kho dữ liệu doanh nghiệp. Việc diễn giải và phân tích tự động các tài liệu này mang lại những lợi thế đáng kể [1], điều này đã thúc đẩy sự phát triển các giải pháp dựa trên AI. Những tài liệu giàu trực quan này có bố cục phức tạp, kiểu dáng đặc biệt và thường biểu hiện sự khác biệt về mẫu, định dạng và chất lượng. Mặc dù Document AI (DocAI) đã đạt được tiến bộ to lớn trong các nhiệm vụ khác nhau bao gồm trích xuất, phân loại và hỏi đáp, vẫn còn một khoảng cách hiệu suất đáng kể trong các ứng dụng thực tế. Đặc biệt, độ chính xác, độ tin cậy, hiểu biết theo ngữ cảnh và khái quát hóa cho các lĩnh vực chưa từng thấy trước đó vẫn là một thách thức [2].

Trí tuệ tài liệu vốn là một vấn đề đa phương thức với cả nội dung văn bản và tín hiệu bố cục trực quan đều quan trọng để hiểu tài liệu. Nó đòi hỏi các giải pháp khác biệt so với các mô hình ngôn ngữ lớn thông thường như GPT-3.5 [3], Llama [4], Falcon [5] hoặc PaLM [6] chủ yếu chấp nhận đầu vào chỉ là văn bản và giả định rằng các tài liệu có bố cục đơn giản và định dạng thống nhất, có thể không phù hợp để xử lý các tài liệu trực quan. Nhiều khung ngôn ngữ-thị giác [7, 8] có thể xử lý tài liệu như hình ảnh và nắm bắt các tương tác giữa các phương thức văn bản và trực quan đều có sẵn. Tuy nhiên, các khung này đòi hỏi việc sử dụng các kiến trúc xương sống thị giác phức tạp [9] để mã hóa thông tin hình ảnh, và chúng thường sử dụng thông tin không gian như một tín hiệu ngữ cảnh phụ trợ [10, 11].

Trong bài báo này, chúng tôi trình bày DocLLM, một mở rộng nhẹ cho các LLM tiêu chuẩn xuất sắc trong một số nhiệm vụ hiểu biết biểu mẫu giàu trực quan. Không giống như các LLM truyền thống, nó mô hình hóa cả bố cục không gian và ngữ nghĩa văn bản, và do đó là

*Những tác giả này đóng góp bằng nhau cho công việc này. arXiv:2401.00908v1 [cs.CL] 31 Dec 2023

--- TRANG 2 ---
Hình 1: Các yếu tố chính của DocLLM. (1) Tài liệu đầu vào chứa các token văn bản và hộp giới hạn của chúng. (2) Cơ chế chú ý của LLM được mở rộng để nắm bắt các phụ thuộc giữa ngữ nghĩa văn bản và bố cục không gian. (3) Điền vào các khối văn bản được sử dụng làm mục tiêu tiền huấn luyện. (4) Thích ứng nhiệm vụ được thực hiện trên một tập dữ liệu hướng dẫn mới được thu thập.

vốn đa phương thức. Thông tin bố cục không gian được kết hợp thông qua tọa độ hộp giới hạn của các token văn bản thu được thông thường bằng cách sử dụng nhận dạng ký tự quang học (OCR), và không dựa vào bất kỳ thành phần mã hóa thị giác nào. Do đó, giải pháp của chúng tôi bảo tồn kiến trúc bộ giải mã nhân quả, chỉ giới thiệu một sự gia tăng biên trong kích thước mô hình, và có thời gian xử lý giảm, vì nó không dựa vào một bộ mã hóa thị giác phức tạp. Chúng tôi chứng minh rằng việc chỉ bao gồm cấu trúc bố cục không gian là đủ cho các nhiệm vụ trí tuệ tài liệu khác nhau như hiểu biết biểu mẫu, căn chỉnh bảng và hỏi đáp trực quan.

Những nỗ lực hiện tại để kết hợp thông tin bố cục không gian thường liên quan đến việc nối hoặc embedding không gian và văn bản [12] hoặc cộng hai cái [13]. Ngược lại, chúng tôi coi thông tin không gian như một phương thức riêng biệt và tính toán sự phụ thuộc lẫn nhau của nó với phương thức văn bản theo cách tách biệt [14]. Cụ thể, chúng tôi mở rộng cơ chế tự chú ý của các transformer tiêu chuẩn để bao gồm các điểm chú ý mới nắm bắt các mối quan hệ xuyên phương thức. Điều này được thúc đẩy bởi quan sát rằng thường có một mối tương quan giữa nội dung, vị trí và kích thước của các trường trong một biểu mẫu. Việc biểu diễn các căn chỉnh của chúng ở các mức trừu tượng khác nhau qua các lớp transformer có thể tăng cường hiểu biết tài liệu.

Một đặc điểm chung của các tài liệu trực quan là nội dung không đồng nhất, bố cục không đều và các đoạn văn bản rời rạc. Khi làm việc với các tài liệu như vậy, việc sử dụng mục tiêu dự đoán token tiếp theo cổ điển trong giai đoạn tiền huấn luyện tự giám sát có thể hạn chế. Đặc biệt, các token trước đó có thể không phải lúc nào cũng liên quan do sự sắp xếp đa dạng của văn bản, có thể được định vị theo chiều ngang, dọc hoặc thậm chí theo kiểu lệch. Để giải quyết vấn đề này, chúng tôi đề xuất hai sửa đổi cho mục tiêu tiền huấn luyện: (a) áp dụng các khối văn bản gắn kết tính đến các ngữ cảnh rộng hơn, và (b) triển khai một cách tiếp cận điền vào bằng cách điều kiện hóa dự đoán trên cả các token trước và sau. Do những sửa đổi này, mô hình được trang bị tốt hơn để giải quyết văn bản lệch, hoàn thành theo ngữ cảnh, bố cục phức tạp và các loại dữ liệu hỗn hợp. Mặc dù các khoảng văn bản và nhiệm vụ điền vào đã được nghiên cứu trước đây [15], giải pháp của chúng tôi được thiết kế riêng cho các tài liệu trực quan với sự nhấn mạnh vào các khối có tính gắn kết về mặt ngữ nghĩa.

Chúng tôi thích ứng kiến thức được tiền huấn luyện của DocLLM cho một số nhiệm vụ trí tuệ tài liệu bằng cách tinh chỉnh nó trên dữ liệu hướng dẫn được tuyển chọn từ một số tập dữ liệu. Những nhiệm vụ này bao gồm trích xuất thông tin chính, suy luận ngôn ngữ tự nhiên, hỏi đáp trực quan và phân loại tài liệu. Dữ liệu điều chỉnh hướng dẫn của chúng tôi bao gồm cả tài liệu đơn trang và đa trang. Các gợi ý bố cục như phân cách trường, tiêu đề và chú thích có thể được tích hợp trong quá trình điều chỉnh hướng dẫn để tạo thuận lợi cho việc học cấu trúc logic của các tài liệu. Chúng tôi quan sát thấy rằng các sửa đổi được giới thiệu bởi DocLLM dẫn đến cải thiện hiệu suất từ 15% đến 61% cho mô hình Llama2-7B trong bốn trong năm tập dữ liệu chưa từng thấy trước đó.

Hình 1 tóm tắt khung. Những đóng góp của chúng tôi bao gồm:
1. Một mở rộng nhẹ cho LLM được thiết kế để hiểu các tài liệu trực quan.
2. Một cơ chế chú ý không gian tách biệt nắm bắt căn chỉnh chéo giữa các phương thức văn bản và bố cục.
3. Một mục tiêu tiền huấn luyện điền vào được thiết kế riêng để giải quyết hiệu quả các bố cục không đều.
4. Một tập dữ liệu điều chỉnh hướng dẫn được tuyển chọn đặc biệt hướng tới các nhiệm vụ trí tuệ tài liệu trực quan.
5. Các thí nghiệm toàn diện và những hiểu biết có giá trị về hành vi mô hình.

--- TRANG 3 ---
2 Công việc liên quan
2.1 LLM
Thành công đáng chú ý của ChatGPT đã tạo ra sự quan tâm nghiên cứu đáng kể về LLM trong học viện và công nghiệp. Sau đó, nhiều LLM đã được giới thiệu bắt đầu từ các LLM dựa trên văn bản [16,17,4,18] đến các LLM đa phương thức [19,20,21,22,23]. Trong phần này, chúng tôi xem xét những tiến bộ gần đây này trong LLM và thảo luận về mối liên hệ và sự phân biệt của chúng với công việc của chúng tôi.

LLM dựa trên văn bản. Việc giới thiệu mô hình transformer vào năm 2017 [24] đã là nền tảng cho các mô hình được tiền huấn luyện như BERT [25], GPT [26], và T5 [27], mỗi mô hình được thiết kế với các mục tiêu tiền huấn luyện cụ thể. Sự xuất hiện của ChatGPT và GPT-4 đánh dấu một sự thay đổi đáng chú ý, đặc trưng bởi sự gia tăng đáng kể cả về tham số mô hình và kích thước dữ liệu huấn luyện. Sự cải thiện này đã dẫn đến khả năng khái quát hóa zero-shot đáng chú ý, cho phép các mô hình này xuất sắc trong các nhiệm vụ chưa từng thấy trước đó. Thành công như vậy của LLM đã thúc đẩy sự phát triển của các LLM bổ sung như OPT [28], BLOOM [18], PaLM [17], và Llama [4]. Đặc biệt, Llama2 [4] là một LLM mã nguồn mở đạt được hiệu suất tương đương hoặc tốt hơn so với cả các mô hình mã nguồn mở và đóng, bao gồm ChatGPT, PaLM và Falcon, với các chiến lược an toàn tăng cường. Llama2 sử dụng kiến trúc Transformer tiêu chuẩn với pre-normalization [28], hàm kích hoạt SwiGLU [29], và embeddings vị trí xoay [30]. Dữ liệu tiền huấn luyện bao gồm hai nghìn tỷ token từ các nguồn có sẵn công khai.

LLM đa phương thức. Các LLM đa phương thức mở rộng phạm vi văn bản sang các phương thức đa dạng, với trọng tâm là đầu vào trực quan. Các mô hình này có thể được phân loại thành hai nhóm: LLM đa phương thức đa mục đích [19,20,21,22,23] và các mô hình được thiết kế riêng cho hiểu biết tài liệu giàu trực quan [31,32,33,34,12]. Các LLM đa phương thức đa mục đích thể hiện hiệu suất hứa hẹn trong việc nhận dạng và suy luận với thông tin hình ảnh. Tuy nhiên, chúng chưa được đánh giá một cách nghiêm ngặt về các nhiệm vụ VRDU. Ví dụ, Báo cáo Kỹ thuật GPT-4 [16] làm nổi bật các trường hợp thử nghiệm đa phương thức đa dạng, chẳng hạn như giải thích sự khác biệt của hình ảnh meme, nhưng rất ít ví dụ được bao gồm cho các trường hợp sử dụng tài liệu trực quan. Trước sự ra đời của các mô hình ngôn ngữ lớn, các mô hình dựa trên tinh chỉnh chỉ dựa vào thị giác kém hiệu quả hơn so với các mô hình phương thức bố cục (và thị giác) trong xử lý tài liệu trực quan. Ví dụ, các mô hình như UDOP [12] và LayoutLM [13] vượt trội so với các mô hình chỉ dựa vào thị giác như Donut [35] và Pix2Struct [34] trong các nhiệm vụ VRDU. Nhưng các mô hình như vậy yêu cầu tinh chỉnh cụ thể cho nhiệm vụ và tập dữ liệu, và do đó được loại trừ khỏi phân tích của chúng tôi. Các mPLUG-DocOwl [31] và UReader [32] gần đây hơn, được xây dựng dựa trên LLM, trải qua điều chỉnh hướng dẫn trên một tập hợp đa dạng các tập dữ liệu VRDU, trực quan và văn bản, và thể hiện khả năng khái quát hóa zero-shot ấn tượng. Do đó, chúng tôi bao gồm những cái này như là các cơ sở trong đánh giá của chúng tôi trong Phần 4.

Bất chấp hiệu suất đáng chú ý của LLM, các mô hình đơn phương thức không được trang bị để xử lý đầu vào đa phương thức, và các LLM đa phương thức dựa vào các bộ mã hóa thị giác phức tạp và tiêu tốn bộ nhớ cho lĩnh vực mở. Mô hình được đề xuất của chúng tôi, DocLLM, giải quyết những thách thức này bằng cách mô hình hóa rõ ràng các bố cục không gian và ngữ nghĩa văn bản, cho phép hiểu biết hiệu quả các tài liệu trực quan. Đáng chú ý, DocLLM cung cấp một mở rộng cho kiến trúc đơn phương thức bằng cách thêm tín hiệu không gian vào ngữ nghĩa văn bản, tránh bộ mã hóa thị giác đắt đỏ, dẫn đến một mô hình nhỏ gọn hơn và thời gian xử lý hiệu quả.

2.2 Kiến trúc LLM
Điền vào tự động hồi quy. Có hai cách tiếp cận điền vào tự động hồi quy chính: "fill-in-the-middle" (FIM) nơi một khoảng đơn được lấy mẫu, và "blank infilling" với nhiều khoảng.

Cách tiếp cận FIM của OpenAI [36] sử dụng mẫu (prefix, middle, suffix) để chia một tài liệu thành ba phần. Tiếp theo, các phần này được tổ chức lại thành (prefix, suffix, middle), cho phép mô hình dự đoán phần giữa. Quá trình này dựa vào ba token đặc biệt, [PRE], [SUF], và [MID], cấu trúc một tài liệu như: [PRE] prefix [SUF] suffix [MID] middle. Token [MID] biểu thị điểm bắt đầu để dự đoán, trong khi hai token đặc biệt khác hướng dẫn mô hình về nơi để điền vào. Phương pháp này chứng minh rằng các mô hình tự động hồi quy có thể học cách điền vào văn bản nơi phần giữa bị thiếu. Fill-in Language Model (FiLM) [37] là một phát triển tiếp theo cho phép tạo ra linh hoạt ở các vị trí tùy ý, không bị hạn chế bởi thứ tự tạo ra định trước. Ngược lại, các cách tiếp cận như GLM [15] lấy mẫu nhiều khoảng để điền vào. Đối với mỗi chỗ trống cần được điền vào, một cặp token đặc biệt được sử dụng: [blank_mask] và [start_to_fill]. Nhiều khoảng không chỉ yêu cầu các token đặc biệt mà còn các chỉ báo toàn cục để phân biệt khoảng giữa nào mô hình nên điền vào. Chỉ báo toàn cục này được triển khai với các vị trí token 1D, đảm bảo rằng mỗi cặp hai token đặc biệt, tức là [blank_mask] và [start_to_fill], chia sẻ cùng các vị trí. Chúng tôi áp dụng một mục tiêu điền vào tương tự với mục tiêu ngăn chặn các dự đoán token tiếp theo bị ngắt kết nối trong khi tránh chia các tài liệu thưa thành các phần rất ngắn, ví dụ, các mảnh từ và/hoặc các mảnh cụm từ.

Chú ý tách biệt. Chú ý tách biệt được giới thiệu trong mô hình DeBERTa [38], nơi embeddings token và mã hóa vị trí tương đối được giữ riêng biệt thay vì được cộng lại với nhau, và mỗi cái được sử dụng độc lập khi tính toán trọng số chú ý sử dụng các ma trận tách biệt. Động lực đằng sau điều này là tạo thuận lợi cho việc học các căn chỉnh chú ý tách biệt dựa trên nội dung và vị trí một cách riêng biệt. Sự đổi mới này tỏ ra hiệu quả vì nó cho phép DeBERTa vượt trội so với RoBERTA-large và T5 trên các benchmarks NLU, cũng như vượt qua đường cơ sở con người trên SuperGLUE [39]. Trong công việc của chúng tôi, với việc mã hóa vị trí phức tạp hơn đáng kể được sử dụng trong các tài liệu giàu trực quan, sự tách biệt trở nên quan trọng hơn bao giờ hết đối với hiệu suất của mô hình.

3 Khung DocLLM
Trong phần này, chúng tôi thảo luận về kiến trúc của DocLLM và phác thảo các quy trình tiền huấn luyện và điều chỉnh hướng dẫn. Hình 2 trình bày tổng quan về kiến trúc mô hình.

3.1 Kiến trúc mô hình
DocLLM được xây dựng trên nền tảng của một mô hình ngôn ngữ transformer tự động hồi quy [4] theo cấu trúc bộ giải mã nhân quả. Nó bao gồm các khối transformer xếp chồng, trong đó mỗi khối chứa một lớp tự chú ý đa đầu và một mạng lan truyền thẳng kết nối đầy đủ. Các mô hình ngôn ngữ tiêu chuẩn thường là đơn phương thức, chỉ chấp nhận một chuỗi token văn bản làm đầu vào. Ngược lại, DocLLM là một hệ thống đa phương thức tích hợp thông tin trực quan nhẹ bằng cách sử dụng các vị trí không gian và kích thước của các token văn bản thu được bằng OCR. Việc chỉ đơn giản tăng cường văn bản với thông tin hộp giới hạn thông qua mã hóa vị trí cộng dồn có thể không nắm bắt được các mối quan hệ phức tạp giữa ngữ nghĩa văn bản và bố cục không gian, đặc biệt là đối với các tài liệu giàu trực quan [10]. Do đó, chúng tôi coi thông tin không gian về các token văn bản như một phương thức riêng biệt. Cụ thể, chúng tôi sử dụng các vector riêng biệt để biểu diễn hai phương thức này và mở rộng cơ chế tự chú ý của kiến trúc transformer để tính toán các phụ thuộc lẫn nhau của chúng theo cách tách biệt, như được giải thích trong phần tiếp theo. Hơn nữa, thay vì dự đoán token tiếp theo từ trái sang phải truyền thống trong quá trình huấn luyện tự giám sát, chúng tôi sử dụng mục tiêu điền vào văn bản tận dụng tốt hơn thông tin ngữ cảnh.

3.2 Chú ý không gian tách biệt
Cho x = (x₁, ..., xᵢ, ..., xₜ) là một chuỗi đầu vào có độ dài T, trong đó xᵢ là một token văn bản. Trong các transformer cổ điển, sử dụng một ma trận embedding được học dựa trên từ vựng văn bản và một tập hợp các tham số được học cho vị trí token trong chuỗi, các token đầu vào trước tiên được mã hóa thành các vector ẩn H ∈ ℝᵀˣᵈ. Một đầu tự chú ý sau đó tính toán các điểm chú ý giữa các token i và j như:

Qᵗ = HWᵗ,q, Kᵗ = HWᵗ,k, Aᵗᵢ,ⱼ = QᵗᵢKᵗⱼ⊺(1)

--- TRANG 4 ---
trong đó Wq ∈ ℝᵈˣᵈ và Wk ∈ ℝᵈˣᵈ là các ma trận chiếu, và chỉ số trên t chỉ phương thức văn bản. Các điểm chú ý A ∈ ℝᵀˣᵀ cùng với một ma trận chiếu khác Wv được sử dụng thêm để tính toán các vector ẩn H', sau đó được sử dụng làm đầu vào cho một lớp tiếp theo:

Vᵗ = HWᵗ,v, H′ = softmax(Aᵗ/√d)Vᵗ. (2)

Trong DocLLM, đầu vào được biểu diễn dưới dạng x = {(xᵢ, bᵢ)}ᵀᵢ₌₁, trong đó bᵢ = (left, top, right, bottom) là hộp giới hạn tương ứng với xᵢ. Để nắm bắt phương thức mới (tức là thông tin không gian), chúng tôi mã hóa các hộp giới hạn thành các vector ẩn được biểu diễn bởi S ∈ ℝᵀˣᵈ. Sau đó chúng tôi phân tách việc tính toán ma trận chú ý thành bốn điểm khác nhau, cụ thể là văn bản-đến-văn bản, văn bản-đến-không gian, không gian-đến-văn bản và không gian-đến-không gian. Chính thức, cơ chế chú ý mới được tính toán như:

Qˢ = SWˢ,q, Kˢ = SWˢ,k(3)
Aᵢ,ⱼ = QᵗᵢKᵗⱼ⊺ + λt,sQᵗᵢKˢⱼ⊺ + λs,tQˢᵢKᵗⱼ⊺ + λs,sQˢᵢKˢⱼ⊺, (4)

trong đó Ws,q ∈ ℝᵈˣᵈ và Ws,k ∈ ℝᵈˣᵈ là các ma trận chiếu mới được giới thiệu tương ứng với phương thức không gian, và λs là các siêu tham số kiểm soát tầm quan trọng tương đối của mỗi điểm. Các vector ẩn đầu vào cho lớp tiếp theo H' được tính toán chính xác như trước đây. Tuy nhiên, trái ngược với phương trình (2), các vector ẩn mới được tính toán không chỉ dựa vào ngữ nghĩa văn bản mà còn dựa vào thông tin bố cục của các token văn bản.

Điều quan trọng cần đề cập là các vector ẩn S được tái sử dụng qua các lớp khác nhau, trong khi mỗi lớp giữ linh hoạt để sử dụng các ma trận chiếu khác nhau. Chúng tôi cũng lưu ý rằng số lượng tham số thêm cần thiết để mã hóa thông tin hộp giới hạn thấp hơn đáng kể so với overhead được giới thiệu bởi các mô hình dựa trên hình ảnh [7]. Bằng cách đơn giản thêm S vào H tương tự như [13], chúng tôi có thể đã tránh được việc sử dụng hoàn toàn các ma trận Ws và giảm thêm số lượng tham số. Tuy nhiên, điều đó sẽ đã kết hợp không thể đảo ngược thông tin bố cục với ngữ nghĩa văn bản. Ngược lại, biểu diễn tách biệt của các phương thức này trong các điểm chú ý cho phép tập trung chọn lọc khi thích hợp [38], từ đó cung cấp sự cân bằng tối ưu giữa kích thước mô hình và hiệu quả.

3.3 Tiền huấn luyện
DocLLM trước tiên được tiền huấn luyện theo cách tự giám sát trên một số lượng lớn tài liệu không được gán nhãn. Mục tiêu tiền huấn luyện tự giám sát trong các mô hình ngôn ngữ tự động hồi quy [26] thường là tối đa hóa log-likelihood của dự đoán token tiếp theo trong một chuỗi dựa trên ngữ cảnh được cung cấp bởi các token trước đó. Cho θ biểu thị tất cả các tham số của mô hình transformer, bao gồm các ma trận chiếu đã thảo luận ở trên. Hàm mất mát cross-entropy sau đó thường được tối thiểu hóa trong bước tiền huấn luyện:

LAR(θ) = -∑ᵀᵢ₌₁ log pθ(xᵢ|xⱼ<ᵢ) (5)

Các tài liệu trực quan thường thưa thớt và không đều, có các đoạn văn bản biệt lập và ngắt kết nối. Trong những trường hợp như vậy, việc xem xét các phân đoạn thô của các token liên quan trong quá trình tiền huấn luyện tốt hơn là tập trung vào từng token riêng lẻ. Một phân đoạn có thể biểu diễn một khối thông tin gắn kết, tương tự như một khối văn bản, hoặc nó có thể chỉ đơn giản là một chuỗi tuyến tính, tương tự như một khoảng văn bản. Trong Hình 2, "Name", "John Doe", và "Doctor" đều là ví dụ về các khối. Nói chung, ngữ cảnh rộng hơn được cung cấp bởi nhiều token trong một khối có thể dẫn đến hiểu biết tốt hơn.

Hơn nữa, việc học cách điền vào văn bản, nơi dự đoán được điều kiện hóa trên cả các token tiền tố và hậu tố thay vì chỉ các token trước đó, có thể có lợi. Các mục tiêu điền vào cho phép các hoàn thành liên quan theo ngữ cảnh, cung cấp khả năng chống chịu với tiếng ồn OCR hoặc các token lệch, và có thể xử lý tốt hơn các mối quan hệ giữa các trường tài liệu khác nhau. Do đó chúng tôi sửa đổi mục tiêu tiền huấn luyện tiêu chuẩn để dự đoán các khối văn bản với các khối văn bản trước và sau.

Hầu hết các công cụ OCR có thể cung cấp thông tin cấp khối, điều này làm cho việc xác định các khối văn bản gắn kết như tiêu đề hoặc địa chỉ trở nên khả thi¹. Lấy cảm hứng từ [15], chúng tôi tuân theo một mục tiêu điền vào khối tự động hồi quy, nơi các khối văn bản được che giấu ngẫu nhiên, và các khối bị che giấu được xáo trộn và tái tạo theo cách tuần tự từ trái sang phải. Thông tin khối và điền vào khối chỉ được sử dụng cho giai đoạn tiền huấn luyện, không trong điều chỉnh hướng dẫn hoặc các nhiệm vụ downstream.

Chính thức, cho c = {c₁, ..., cₖ} là một tập hợp các khối văn bản phân vùng một chuỗi đầu vào x thành các token liền kề không chồng chéo sao cho c₁ ∪ ... ∪ cₖ = x và cₖ ∩ cₖ' = ∅. Những khối văn bản này thường được xác định từ thông tin OCR. Cho

¹Lưu ý rằng để tránh bất kỳ rò rỉ thông tin hữu ích nào, thông tin khối chỉ được sử dụng cho mục tiêu che giấu trong quá trình tiền huấn luyện, và không được cung cấp cho mô hình làm đầu vào. Cụ thể, việc che giấu được thực hiện ở cấp độ khối, nhưng mô hình không được cung cấp thông tin về số lượng token trong một khối bị che giấu. Vui lòng tham khảo Hình 2 để có một ví dụ minh họa.

--- TRANG 5 ---
Bảng 1: Các mẫu prompt được sử dụng cho điều chỉnh hướng dẫn (không bao gồm các token không gian).

Nhiệm vụ | Loại mẫu | Mẫu prompt | Phản hồi mong đợi
VQA | Trích xuất | "{document} {question}" | chú thích câu trả lời
NLI | MCQ | "{document} \"{statement}\", Yes or No?" | chú thích câu trả lời
KIE | Trích xuất | "{document} What is the value for the \"{key}\"?" | chú thích giá trị liên kết
| MCQ | "{document} What is \"{value}\" in the document? Possible choices: {choices}." (trong đó choices là một tập con của tất cả các khóa trong tập dữ liệu theo thứ tự ngẫu nhiên) | chú thích khóa liên kết
| Phân loại nội bộ | "{document} What is \"{value}\" in the document?" | chú thích khóa liên kết
CLS | MCQ | "{document} What type of document is this? Possible choices: {choices}." (trong đó choices là một tập con của tất cả các lớp trong tập dữ liệu theo thứ tự ngẫu nhiên) | chú thích lớp
| Phân loại nội bộ | "{document} What type of document is this?" | chú thích lớp

z = {zₘ}ᴹₘ₌₁ là M ≪ K khối văn bản khác nhau được lấy mẫu ngẫu nhiên từ c, trong đó mỗi khối zₘ = (zₘ,₁, ..., zₘ,ₙₘ) chứa một chuỗi token liên tiếp. Hơn nữa, cho x̃ là một phiên bản bị hỏng của x trong đó các token liền kề tương ứng với một khối văn bản được lấy mẫu được thay thế bằng một token mặt nạ đặc biệt [M]. Để tạo thuận lợi cho việc xác định khối cần được điền trong quá trình tạo văn bản, mỗi khối đầu vào được tăng cường với một token bắt đầu đặc biệt [S] trong khi khối đầu ra bao gồm một token kết thúc [E]. Ví dụ, một khối với các token (x₄, x₅) trở thành [M] trong x̃, ([S], x₄, x₅) khi được điều kiện hóa, và được mong đợi tạo ra (x₄, x₅, [E]) như đầu ra tự động hồi quy (xem Hình 2 để có minh họa chi tiết về các cấu hình này). Hàm mất mát cross-entropy sau đó được tối thiểu hóa cho mục tiêu điền vào.

LIF(θ) = -∑ᴹₘ₌₁ ∑ᴺᵐⱼ₌₁ log pθ(zₘ,ⱼ|x̃, z<ₘ, zₘ,<ⱼ) (6)

3.4 Điều chỉnh hướng dẫn
Theo công việc gần đây trong lĩnh vực VRDU [12,31,32] và công việc trước đây trong NLP [40,41], chúng tôi điều chỉnh hướng dẫn DocLLM trên nhiều hướng dẫn khác nhau được rút ra từ các tập dữ liệu DocAI sử dụng các mẫu khác nhau. Do chi phí cao và thời gian chuyên sâu của việc thu thập dữ liệu thủ công, chúng tôi để lại việc xây dựng một tập dữ liệu điều chỉnh hướng dẫn VRDU với các hướng dẫn và sở thích được crowdsource cho công việc tương lai. Chúng tôi sử dụng tổng cộng 16 tập dữ liệu với các OCR tương ứng của chúng, trải dài bốn nhiệm vụ DocAI: hỏi đáp trực quan (VQA), suy luận ngôn ngữ tự nhiên (NLI), trích xuất thông tin khóa (KIE) và phân loại tài liệu (CLS).

Sự đa dạng của các hướng dẫn tinh chỉnh có giám sát (SFT) là quan trọng trong việc giúp khái quát hóa zero-shot [40,41,42]. Do đó, chúng tôi đa dạng hóa các mẫu cho mỗi nhiệm vụ khi có thể, với mỗi mẫu đặt một câu hỏi khác nhau, và trong một số trường hợp, mong đợi các loại câu trả lời khác nhau. Chúng tôi tái sử dụng các mẫu được giới thiệu trong [31,32] khi có thể áp dụng, và xem xét một lựa chọn rộng hơn các tập dữ liệu trong hỗn hợp dữ liệu điều chỉnh hướng dẫn của chúng tôi.

Chúng tôi tạo ra các mẫu theo những gì chúng tôi tin rằng người dùng cuối thường sẽ hỏi về tài liệu (Bảng 1). Đối với KIE và CLS, chúng tôi giả thuyết rằng (1) các hướng dẫn trích xuất có thể dạy DocLLM tương quan tên của các khóa trong prompts với các trường tài liệu để truy xuất giá trị, (2) các hướng dẫn phân loại nội bộ có thể giúp mô hình hiểu những gì đặc trưng nội tại cho mỗi khóa hoặc loại tài liệu, và (3) các hướng dẫn câu hỏi trắc nghiệm (MCQ) có thể dạy mô hình tận dụng sự hiểu biết của nó về tên khóa được bao gồm như các lựa chọn trong prompt (tương ứng với tên loại tài liệu) để phân loại các giá trị được trích xuất (tương ứng với toàn bộ tài liệu). Chúng tôi giới thiệu các mẫu chi tiết như sau.

Hỏi đáp trực quan. Chúng tôi thu thập DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE [46], và BizDocs², để tạo thành hỗn hợp dữ liệu điều chỉnh hướng dẫn VQA. Chúng tôi sử dụng một mẫu hướng dẫn để xây dựng các đầu vào SFT của chúng tôi cho VQA, như được hiển thị trong bảng 1. Một prompt ví dụ được rút ra từ DocVQA sẽ đọc: "{document} What is the deadline for scientific abstract submission for ACOG - 51st annual clinical meeting?"

Suy luận ngôn ngữ tự nhiên. Chúng tôi chỉ bao gồm TabFact [47] trong hỗn hợp dữ liệu điều chỉnh hướng dẫn của chúng tôi cho nhiệm vụ NLI, do thiếu các tập dữ liệu DocAI NLI bổ sung có sẵn. Mẫu hướng dẫn được hiển thị trong bảng 1. Một prompt ví dụ được rút ra từ TabFact sẽ đọc: "{document} \"The UN commission on Korea include 2 Australians.\", Yes or No?"

Trích xuất thông tin khóa. Chúng tôi thu thập Kleister Charity (KLC) [48], CORD [49], FUNSD [50], DeepForm [51], PWC [52], SROIE [53], VRDU ad-buy [54] (với việc chia train-test ngẫu nhiên), và BizDocs để xây dựng dữ liệu điều chỉnh hướng dẫn KIE, nơi chúng tôi tận dụng ba mẫu hướng dẫn: trích xuất, phân loại nội bộ, và MCQ, như được hiển thị trong 1. Đối với

²BizDocs là một bộ sưu tập các hồ sơ nộp thực thể kinh doanh sẽ được công bố công khai.

--- TRANG 6 ---
Bảng 2: Thống kê tập dữ liệu tiền huấn luyện.

Số lượng tài liệu | Số lượng trang | Số lượng token tổng
CDIP | 5,092,636 | 16,293,353 | 3,637,551,478
DocBank | 499,609 | 499,609 | 228,362,274
Tổng | 5,592,245 | 16,792,962 | 3,865,913,752

Bảng 3: Thống kê tập dữ liệu điều chỉnh hướng dẫn.

Nhiệm vụ | Số lượng huấn luyện | Số lượng kiểm tra
VQA | 145,090 | 24,347
NLI | 104,360 | 12,720
KIE | 236,806 | 38,039
CLS | 149,627 | 21,813
Tổng | 635,883 | 96,919

mẫu trích xuất, chúng tôi thêm câu trả lời "None" nếu khóa không tồn tại trong tài liệu đã cho. Để tăng tính đa dạng trong dữ liệu huấn luyện SFT, chúng tôi cũng rút ra các hướng dẫn phân loại nội bộ và MCQ từ các chú thích KIE ban đầu. Để giữ nhất quán với các benchmarks từ công việc trước đây [31,32], chúng tôi chỉ giữ các prompts được rút ra từ mẫu trích xuất trong phần kiểm tra của mỗi tập dữ liệu KIE. Một hướng dẫn trích xuất ví dụ được rút ra từ KLC sẽ đọc: "{document} What is the value for the \"charity number\"?"

Phân loại tài liệu. Chúng tôi tổng hợp RVL-CDIP [55] và BizDocs để xây dựng dữ liệu điều chỉnh hướng dẫn CLS của chúng tôi. Chúng tôi sử dụng hai loại mẫu hướng dẫn cho nhiệm vụ này: phân loại nội bộ và MCQ, như được hiển thị trong 1. Để tránh vấn đề khởi động lạnh được gây ra bởi các loại tài liệu có thể chưa được nhìn thấy trong kiểm tra hoặc thậm chí trong việc sử dụng sản xuất, chúng tôi chỉ giữ các prompts MCQ cho phần kiểm tra của mỗi tập dữ liệu CLS. Chúng tôi cũng downsample RVL-CDIP trong phần huấn luyện để tránh cản trở các tập dữ liệu khác. Một hướng dẫn MCQ ví dụ được rút ra từ RVL-CDIP sẽ đọc: "{document} What type of document is this? Possible answers: [budget, form, file folder, questionnaire]."

4 Thí nghiệm
4.1 Tập dữ liệu
Chúng tôi thu thập dữ liệu cho tiền huấn luyện từ hai nguồn chính: (1) IIT-CDIP Test Collection 1.0 [56] và (2) DocBank [57]. IIT-CDIP Test Collection 1.0 bao gồm một kho lưu trữ rộng lớn hơn 5 triệu tài liệu, bao gồm hơn 16 triệu trang tài liệu. Tập dữ liệu này được rút ra từ các tài liệu liên quan đến các thủ tục pháp lý chống lại ngành công nghiệp thuốc lá trong những năm 1990. DocBank bao gồm 500K tài liệu, mỗi tài liệu có bố cục khác biệt và một trang duy nhất mỗi tài liệu. Các thống kê liên quan cho các tập dữ liệu được sử dụng trong tiền huấn luyện được chi tiết trong Bảng 2. Chúng tôi thu được một bộ sưu tập 16.7 triệu trang bao gồm tổng cộng 3.8 tỷ token.

Chúng tôi đã giới thiệu các tập dữ liệu được sử dụng để thực hiện điều chỉnh hướng dẫn trong Phần 3.4. Những tập dữ liệu này bao gồm bốn nhiệm vụ DocAI phổ biến: VQA, NLI, KIE, và CLS. Lưu ý rằng khi một prompt bao gồm một danh sách các câu trả lời có thể, chúng tôi tạo nhiều bản sao của prompt với một câu trả lời có thể được gán cho mỗi bản. Chúng tôi chỉ thực hiện thao tác "flattening" này trong phần huấn luyện của tập dữ liệu. Thống kê chi tiết cho các nhiệm vụ này được trình bày trong Bảng 3.

4.2 Thiết lập mô hình và chi tiết huấn luyện
Bảng 4 cung cấp các cài đặt chính và siêu tham số cho hai biến thể của DocLLM: DocLLM-1B, dựa trên kiến trúc Falcon-1B [5], và DocLLM-7B, dựa trên kiến trúc Llama2-7B [4]³. DocLLM-1B bao gồm 24 lớp, mỗi lớp có 16 đầu chú ý và kích thước ẩn 1,536. DocLLM-7B bao gồm 36 lớp, 32 đầu, và kích thước ẩn 4,096. Sử dụng trọng số được tiền huấn luyện làm xương sống cho phương thức văn bản, chúng tôi mở rộng các mô hình Falcon-1B và Llama2-7B bằng cách thêm cơ chế chú ý tách biệt và mục tiêu điền vào khối như được mô tả trong Phần 3.

Đối với DocLLM-1B, chúng tôi sử dụng tốc độ học tiền huấn luyện 2×10⁻⁴ với 1,000 bước khởi động, sử dụng bộ lập lịch cosine, và trình tối ưu hóa Adam [58] với β₁ = 0.9, β₂ = 0.96 và độ suy giảm trọng số 0.1. Đối với điều chỉnh hướng dẫn, chúng tôi sử dụng tốc độ học

³Vì Llama2 không đi kèm với trọng số được tiền huấn luyện ở 1B tham số, chúng tôi sử dụng kiến trúc Falcon-1B cho phiên bản nhỏ hơn của DocLLM.

--- TRANG 7 ---
Bảng 4: Cấu hình mô hình và thiết lập siêu tham số huấn luyện cho DocLLM-1B và -7B.

| | DocLLM-1B | DocLLM-7B |
|---|---|---|
| Xương sống | Falcon-1B [5] | Llama2-7B [4] |
| Lớp | 24 | 36 |
| Đầu chú ý | 16 | 32 |
| Kích thước ẩn | 1536 | 4096 |
| Độ chính xác | bfloat16 | bfloat16 |
| Kích thước batch | 2 | 5 |
| Độ dài ngữ cảnh tối đa | 1,024 | 1,024 |
| | Tiền huấn luyện | Điều chỉnh hướng dẫn | Tiền huấn luyện | Điều chỉnh hướng dẫn |
| Tốc độ học | 2×10⁻⁴ | 1×10⁻⁴ | 3×10⁻⁴ | 1×10⁻⁴ |
| Khởi động | 1000 | 500 | 1000 | 500 |
| Loại lập lịch | cosine | cosine | cosine | cosine |
| Suy giảm trọng số | 0.1 | 0.1 | 0.1 | 0.1 |
| Adam βs | (0.9, 0.96) | (0.9, 0.96) | (0.9, 0.95) | (0.9, 0.95) |
| Adam epsilon | 1×10⁻⁵ | 1×10⁻⁵ | 1×10⁻⁶ | 1×10⁻⁶ |

1×10⁻⁴ với 500 bước khởi động và bộ lập lịch cosine, và cùng các tham số cho suy giảm trọng số và trình tối ưu hóa Adam như giai đoạn tiền huấn luyện. Adam epsilon được đặt thành 1×10⁻⁵. Chúng tôi tiền huấn luyện trong một epoch, và điều chỉnh hướng dẫn tổng cộng 10 epoch.

Đối với DocLLM-7B, tiền huấn luyện bao gồm tốc độ học 3×10⁻⁴ với 1,000 bước khởi động và bộ lập lịch cosine, suy giảm trọng số 0.1, và trình tối ưu hóa Adam với β₁ = 0.9, β₂ = 0.95. Điều chỉnh hướng dẫn sử dụng tốc độ học 1×10⁻⁴ với 500 bước khởi động và bộ lập lịch cosine, suy giảm trọng số 0.1, và trình tối ưu hóa Adam với β₁ = 0.9, β₂ = 0.95. Adam epsilon được đặt ở 1×10⁻⁶. Chúng tôi thực hiện một epoch tiền huấn luyện, sau đó là ba epoch điều chỉnh hướng dẫn, xem xét các tài nguyên tính toán có sẵn.

Độ dài chuỗi tối đa, hoặc độ dài ngữ cảnh, được đặt nhất quán thành 1,024 cho cả hai phiên bản trong toàn bộ quá trình huấn luyện. Các mô hình DocLLM-7B được huấn luyện với độ chính xác hỗn hợp 16-bit trên 8 GPU A10g 24GB sử dụng song song dữ liệu được chia sẻ đầy đủ, được triển khai với thư viện accelerate.⁴ Mô hình DocLLM-1B, mặt khác, được huấn luyện trên một GPU A10g 24GB duy nhất.

4.3 Đánh giá downstream
Thiết lập thí nghiệm. Chúng tôi điều tra hai thiết lập thí nghiệm:
• Cùng tập dữ liệu, khác phần chia (SDDS): Theo công việc trước đây trong VRDU [34,59,33,12,31,32], chúng tôi đầu tiên đánh giá DocLLM trên phần kiểm tra chưa thấy (hoặc phần dev khi phần kiểm tra không có sẵn) của mỗi tập dữ liệu trong 16 tập dữ liệu tạo thành dữ liệu điều chỉnh hướng dẫn. Động lực đằng sau thiết lập rất điển hình này là kiểm tra hiệu suất của DocLLM khi các nhiệm vụ và lĩnh vực được cho là giữ nguyên từ huấn luyện đến kiểm tra.

• Cùng nhiệm vụ, khác tập dữ liệu (STDD): Theo [40,41,60,61], chúng tôi cũng đánh giá DocLLM trên các tập dữ liệu giữ lại. Cụ thể hơn, chúng tôi điều chỉnh hướng dẫn checkpoint được tiền huấn luyện của DocLLM trên các prompts từ 11 trong 16 tập dữ liệu được xem xét trong SDDS, sau đó đánh giá DocLLM trên phần kiểm tra của ba tập dữ liệu còn lại. Lý do đằng sau thiết lập đánh giá này là đánh giá hiệu suất của DocLLM khi các nhiệm vụ không thay đổi nhưng các lĩnh vực và bố cục khác biệt từ huấn luyện đến kiểm tra. Chúng tôi tin rằng việc kiểm tra thiết lập này trong lĩnh vực DocAI là liên quan vì các trường hợp sử dụng công nghiệp thường gặp trong thực tế xoay quanh VQA, KIE, và CLS, trong khi các đặc điểm tài liệu có xu hướng thay đổi thường xuyên hơn trong sản xuất. Chúng tôi cụ thể tách DocVQA, KLC, và BizDocs cho đánh giá STDD để (1) loại trừ ít nhất một tập dữ liệu cho mỗi nhiệm vụ từ SFT khi có thể, (2) để lại đủ điểm dữ liệu cho mỗi nhiệm vụ trong phần huấn luyện của dữ liệu điều chỉnh hướng dẫn, (3) tránh rò rỉ dữ liệu (một số tập dữ liệu nhất định được thu thập từ cùng nguồn), và (4) benchmark các mô hình trên các tập dữ liệu phổ biến nhưng thách thức khi có thể. Do chi phí cao của điều chỉnh hướng dẫn, chúng tôi không thể chạy các thí nghiệm bổ sung với các tập dữ liệu giữ lại khác nhau.

Cơ sở so sánh. Trong SDDS và STDD, chúng tôi benchmark DocLLM so với các LLM có kích thước tương đương và SOTA sử dụng các prompts Zero-Shot (ZS) chứa văn bản được trích xuất từ mỗi tài liệu sử dụng công cụ OCR (loại trừ thông tin không gian) [4,42]. Trong SDDS, chúng tôi cũng báo cáo các con số từ các LLM DocAI gần đây được đánh giá trong thiết lập tương tự [31,32].

⁴https://huggingface.co/docs/accelerate

--- TRANG 8 ---
Bảng 5: So sánh hiệu suất trong thiết lập SDDS so với các LLM đa phương thức và không đa phương thức khác; các LLM không đa phương thức được prompt Zero-Shot (ZS) trong khi các LLM đa phương thức được điều chỉnh hướng dẫn trên phần huấn luyện của các tập dữ liệu được xem xét. '-' đánh dấu không có sẵn.

Tập dữ liệu | GPT-4+OCR | Llama2+OCR | mPLUG-DocOwl | UReader | DocLLM-1B | DocLLM-7B
∼1T (T) | 7B (T) | ∼7B (T+V) | ∼7B (T+V) | 1B (T+L) | 7B (T+L)
ZS | ZS | SDDS | SDDS | SDDS | SDDS
VQA DocVQA | 82.8 | 47.4 | 62.2 | 65.4 | 61.4 | 69.5
WTQ (Accuracy) | 65.4 | 25.0 | 26.9 | 29.4 | 21.9 | 27.1
VisualMRC (CIDEr) | 255.1 | 115.5 | 188.8 | 221.7 | 245.0 | 264.1
DUDE | 54.6 | 38.1 | - | - | 42.6 | 47.2
BizDocs | 76.4 | 48.8 | - | - | 84.5 | 86.7
NLI TabFact | 77.1 | 48.2 | 60.2 | 67.6 | 58.0 | 66.4
KIE KLC | 45.9 | 27.8 | 30.3 | 32.8 | 58.9 | 60.3
CORD | 58.3 | 13.8 | - | - | 66.9 | 67.4
FUNSD | 37.0 | 17.8 | - | - | 48.2 | 51.8
DeepForm | 42.1 | 20.5 | 42.6 | 49.5 | 71.3 | 75.7
PWC | 18.3 | 6.8 | - | - | 25.7 | 29.06
SROIE | 90.6 | 56.4 | - | - | 91.0 | 91.9
VRDU a.-b. | 43.7 | 18.7 | - | - | 87.6 | 88.8
BizDocs | 66.1 | 10.8 | - | - | 95.4 | 95.9
CLS RVL-CDIP | 68.2 | 32.8 | - | - | 90.9 | 91.8
BizDocs | 84.9 | 40.9 | - | - | 98.3 | 99.4

Như đã được thúc đẩy trong phần 2, chúng tôi không xem xét các mô hình DocAI đòi hỏi tinh chỉnh cụ thể cho nhiệm vụ [33,59,34] và/hoặc các prompts cụ thể cho tập dữ liệu [12], và thay vào đó tập trung vào các LLM với khả năng tuân theo hướng dẫn ngay lập tức.

Số liệu đo. Theo công việc trước đây [62,34,32,31], chúng tôi đánh giá tất cả các tập dữ liệu VQA sử dụng Average Normalized Levenshtein Similarity (ANLS) [63], với ngoại lệ là VisualMRC, mà chúng tôi sử dụng CIDEr [64] và WTQ, mà chúng tôi sử dụng độ chính xác⁵. Hiệu suất trên tất cả các tập dữ liệu CLS và NLI được đo bằng độ chính xác. Chúng tôi đánh giá tất cả các tập dữ liệu KIE với điểm F1.

Kết quả. Trong thiết lập SDDS, như được hiển thị trong Bảng 5, chúng tôi quan sát thấy rằng DocLLM-7B xuất sắc trong 12 trong 16 tập dữ liệu, so sánh toàn diện với kết quả ZS của GPT4 và Llama2, và kết quả SDDS của mPLUG-DocOwl và UReader. Trong số các mô hình tương đương (loại trừ GPT4), mô hình của chúng tôi vượt trội trong 14 trong 16 tập dữ liệu. Cụ thể, DocLLM thể hiện hiệu suất vượt trội trong các nhiệm vụ chuyên sâu về bố cục như KIE và CLS. Trong VQA và NLI, hiệu suất của nó vượt trội so với hầu hết các mô hình ngôn ngữ đa phương thức, mặc dù nó kém hiệu suất so với GPT-4. GPT-4 vượt trội so với DocLLM trong VQA, có thể do độ phức tạp cao hơn của suy luận và trừu tượng hóa liên quan đến các tập dữ liệu VQA so với các nhiệm vụ như KIE hoặc CLS. DocLLM-1B thể hiện hiệu suất gần với mô hình lớn hơn của chúng tôi, cho thấy rằng mô hình nhỏ hơn có thể thu được lợi ích đáng kể từ kiến trúc của DocLLM.

Trong thiết lập STDD, mô hình của chúng tôi thể hiện hiệu suất vượt trội so với Llama2 trên bốn trong năm tập dữ liệu, và đạt được điểm tốt nhất tổng thể cho hai trong số chúng (nhiệm vụ KIE một lần nữa). DocLLM cũng vượt trội so với mPLUG-DocOwl trên DocVQA và cả mPLUG-DocOwl và UReader trên KLC, mặc dù cả hai cơ sở đều đã được điều chỉnh hướng dẫn trên các tập dữ liệu này. Tuy nhiên, điều quan trọng cần lưu ý là độ chính xác phân loại thấp hơn đáng kể trong mô hình của chúng tôi. Sự khác biệt này có thể bắt nguồn từ thực tế là mô hình của chúng tôi đã được huấn luyện chỉ sử dụng một tập dữ liệu phân loại, hạn chế khả năng khái quát hóa hiệu quả cho các tập dữ liệu mới.

5 Nghiên cứu loại bỏ
Chúng tôi tiến hành các nghiên cứu loại bỏ để xác thực ba đóng góp của DocLLM: (1) các tính năng không gian tách biệt, (2) mục tiêu tiền huấn luyện điền vào khối, và (3) chiến lược che giấu được sử dụng cho giải mã.

Đối với tất cả các loại bỏ, chúng tôi sử dụng độ chính xác Next Token Prediction (NTP) ngoài mẫu để so sánh các cấu hình ở giai đoạn tiền huấn luyện. Do hạn chế tài nguyên, mỗi thí nghiệm sử dụng một tập con của corpus tiền huấn luyện của chúng tôi: chúng tôi lấy mẫu ngẫu nhiên 100,000 chunk và dự đoán trên 1,000 tài liệu chưa thấy. Một chunk là một gói tài liệu được nối liền với nhau với tổng độ dài nhỏ hơn độ dài đầu vào tối đa. Các siêu tham số được đặt nhất quán theo Bảng 4 trên tất cả các thí nghiệm loại bỏ.

⁵Điều này được thực hiện để duy trì nhất quán với kết quả được báo cáo bởi các mô hình SotA khác.

--- TRANG 9 ---
Bảng 6: So sánh hiệu suất trên ba tập dữ liệu VRDU giữ lại trong thiết lập STDD so với các LLM không đa phương thức.

Mô hình | Kích thước | Thiết lập | DocVQA | KLC | BizDocs
| | | VQA | KIE | VQA | KIE | CLS
GPT-4+OCR | ∼1T | ZS | 82.8 | 45.9 | 76.4 | 66.1 | 84.9
Llama2+OCR | 7B | ZS | 47.4 | 27.8 | 48.4 | 10.8 | 40.9
DocLLM-1B | 1B | STDD | 53.5 | 40.1 | 65.5 | 63.0 | 20.8
DocLLM-7B | 7B | STDD | 63.4 | 49.9 | 73.3 | 72.6 | 31.1

(a) Bộ giải mã nhân quả
(b) Bộ giải mã tiền tố

Hình 3: Minh họa đơn giản về mặt nạ chú ý cho bộ giải mã nhân quả và bộ giải mã tiền tố cho điền vào khối.

Chú ý không gian tách biệt. Để đo lường hiệu quả của chú ý không gian tách biệt trên các tương tác xuyên phương thức, chúng tôi huấn luyện các mô hình bằng cách đặt siêu tham số λ trong Phương trình 6 thành 0 hoặc 1. Bảng 7 liệt kê các kết hợp chú ý, và kết quả cho thấy rằng việc chỉ giữ tương tác không gian-đến-không gian (tức là λs,s = 1) cho kết quả độ chính xác NTP cao nhất. Sự khác biệt hiệu suất giữa các cấu hình khác, như văn bản-đến-không gian và không gian-đến-văn bản, là tinh tế. Đáng chú ý, cơ chế tự chú ý chỉ văn bản vanilla cho kết quả độ chính xác NTP thấp nhất, nhấn mạnh tầm quan trọng của việc kết hợp các tính năng không gian để hiểu các tài liệu có bố cục phong phú. Đối với tất cả các thí nghiệm trong Phần 4, do đó chúng tôi đặt λs,s = 1, λs,t = 0, và λt,s = 0. Chúng tôi chọn sự đơn giản bằng cách chọn chế độ cứng thay vì chế độ mềm trong khi thừa nhận lợi thế tiềm năng của tính linh hoạt cho cái sau.

Điền vào khối tự động hồi quy. Để đánh giá hiệu quả của mục tiêu điền vào khối tự động hồi quy được đề xuất đặc biệt so sánh với việc học nhân quả từ trái sang phải thông thường, chúng tôi benchmark ba cấu hình trong nghiên cứu loại bỏ của chúng tôi: (1) học nhân quả, (2) học nhân quả với phương thức không gian, và (3) điền vào khối với phương thức không gian. Như được làm nổi bật trong Bảng 8, điền vào khối tự động hồi quy thể hiện hiệu suất tốt nhất. Ngoài ra, lợi ích hiệu suất của việc thêm phương thức không gian vào học nhân quả chứng minh lợi thế của phương thức không gian.

Bộ giải mã tiền tố và bộ giải mã nhân quả. Đối với tạo ra có điều kiện tài liệu, một lựa chọn trực quan là sử dụng bộ giải mã tiền tố với che giấu tiền tố để làm cho toàn bộ tài liệu có thể nhìn thấy hai chiều trong chú ý, như được minh họa trong Hình 3b. Chúng tôi điều tra giả định này thông qua các thí nghiệm nơi chúng tôi so sánh bộ giải mã tiền tố với bộ giải mã nhân quả thông thường.

Bảng 7: Nghiên cứu loại bỏ về chú ý không gian tách biệt. T đại diện cho phương thức văn bản, S đại diện cho phương thức không gian, và các tương tác xuyên phương thức của chúng được biểu diễn dưới dạng X2X, ví dụ, văn bản-đến-không gian → T2S.

Tương tác xuyên phương thức | Độ chính xác NTP
T2T | 35.43
T2S + T2T | 38.08
S2T + T2T | 38.05
S2S + T2T | 39.12
T2S + S2S + T2T | 39.06
S2T + S2S + T2T | 39.07
T2S + S2T + S2S + T2T | 39.02

--- TRANG 10 ---
Bảng 8: Nghiên cứu loại bỏ về mục tiêu điền vào khối.

Mục tiêu tiền huấn luyện | Độ chính xác NTP
Học nhân quả | 32.6
Học nhân quả + Không gian | 36.2
Điền vào khối + Không gian | 39.1

Hình 4: So sánh hiệu suất về NTP giữa bộ giải mã nhân quả và bộ giải mã tiền tố.

Cụ thể, chúng tôi tiến hành các thí nghiệm đối chọi trên hai bộ giải mã này cho các thiết lập khác nhau được nêu trong chú ý không gian tách biệt để nghiên cứu hiệu suất kết quả của chúng.

Kết quả trong Hình 4 cho thấy sự khác biệt biên giới giữa hai bộ giải mã này trên năm cấu hình, với bộ giải mã nhân quả có lợi thế nhỏ so với tiền tố. Sự khác biệt nhỏ cho thấy rằng cả hai phương pháp che giấu đều có thể so sánh trong việc mô hình hóa tài liệu. Do đó chú ý hai chiều được kích hoạt bởi bộ giải mã tiền tố có thể không quan trọng trong ngữ cảnh này, và chúng tôi do đó chọn sử dụng bộ giải mã nhân quả cho tất cả các thí nghiệm trong phần 4.

6 Thảo luận và phát hiện
Ngoài tiện ích trực tiếp của nó trong các nhiệm vụ hiểu biết tài liệu giàu trực quan, chúng tôi cho rằng DocLLM cung cấp cơ hội thay đổi bối cảnh tiền huấn luyện sinh tạo bằng cách cho phép các mô hình ngôn ngữ vượt ra ngoài dự đoán token tiếp theo trong các thiết lập văn bản thuần túy. Bằng cách chứa các cấu trúc bố cục phức tạp, DocLLM cho phép sách điện tử, ấn phẩm điện tử và các tài liệu khác có bố cục phong phú được kết hợp vào corpus tiền huấn luyện mà không cần tiền xử lý rộng rãi. Cách tiếp cận đọc nhận thức không gian cho phép mô hình cảm nhận tài liệu như kiến thức có cấu trúc vốn có.

Hơn nữa, nhận thức đa trang, về cả ngắt trang và ranh giới tài liệu, tăng cường khả năng hiểu biết tài liệu của các độ dài khác nhau của mô hình. Điều này giải quyết các hạn chế của các mô hình đa phương thức nhỏ hơn trước đây (chủ yếu dành cho tài liệu một trang) và các LLM đa phương thức hiện tại (chủ yếu được thiết kế cho hình ảnh). Trong điều chỉnh hướng dẫn có giám sát, chúng tôi có thể tuân thủ các thực hành đã được thiết lập được sử dụng trong các công việc khác, dựa trên đầu ra mong muốn như văn bản hoặc hình ảnh.

Khái niệm chính cho một khối gắn kết là đảm bảo điền vào có ý nghĩa trong giai đoạn tiền huấn luyện, ngăn chặn các dự đoán bị ngắt kết nối. Tuy nhiên, lựa chọn các công cụ OCR để có được các khối gắn kết như vậy vẫn là một lĩnh vực khám phá mở. Các so sánh thực tế với các công cụ OCR khác nhau và/hoặc các parser bố cục được để lại như công việc tương lai, vì LayoutLMs nhấn mạnh tầm quan trọng của OCR chính xác để cải thiện kết quả VQA. Chúng tận dụng Microsoft Azure API, thể hiện hiệu suất vượt trội so với TesseractOCR, như được chỉ ra trong bảng xếp hạng DocVQA.⁶ Do đó, các nhà nghiên cứu cũng được khuyến khích sử dụng các công cụ OCR chính xác hơn để có khả năng cải thiện, nếu các tài nguyên như vậy có sẵn.

Chúng tôi đã trình bày một bộ sưu tập kết quả SDDS cùng với kết quả zero-shot. Để giảm thiểu ảnh hưởng của prompt trong kết quả zero-shot, một phương pháp nghiêm ngặt đã được triển khai. Điều này bao gồm việc tham gia của ba kỹ sư prompt độc lập, mỗi người trải qua năm vòng tinh chỉnh cho các thiết lập zero-shot, theo sau là một loạt các kỹ thuật hậu xử lý để tăng cường độ tin cậy của kết quả. Kết quả tốt nhất do đó được thu thập từ mỗi nhóm trong ba nhóm. Chúng tôi vẫn thừa nhận tiềm năng tinh chỉnh và cải thiện.

Chúng tôi chia sẻ một số kinh nghiệm huấn luyện nội bộ, thừa nhận sự vắng mặt của xác thực mạnh mẽ. Đầu tiên, chúng tôi quan sát thấy rằng suy giảm trọng số cao hơn (ví dụ, 0.1 so với 0.01) thường cải thiện hiệu suất trong cả tiền huấn luyện và điều chỉnh hướng dẫn. Trong giai đoạn điều chỉnh hướng dẫn, tốc độ học ban đầu cao hơn, chẳng hạn như 1e-4 so với 5e-5, dẫn đến hiệu suất tăng cường. Nhìn chung, chúng tôi đã quan sát thấy rằng bộ lập lịch cosine có xu hướng vượt trội so với các bộ lập lịch tuyến tính hoặc hằng số trong các thiết lập khác nhau.

⁶https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1

--- TRANG 11 ---
7 Kết luận
Trong bài báo này, chúng tôi đã giới thiệu DocLLM, một mở rộng nhẹ cho các mô hình ngôn ngữ lớn truyền thống, được thiết kế riêng cho suy luận sinh tạo trên các tài liệu có bố cục phong phú. Không giống như các LLM đa phương thức hiện tại, DocLLM chiến lược bỏ qua các bộ mã hóa hình ảnh đắt đỏ, thay vào đó ưu tiên thông tin hộp giới hạn để nắm bắt hiệu quả cấu trúc bố cục không gian của tài liệu. Điều này được đạt được thông qua một cách tiếp cận chú ý tách biệt, phân tách cơ chế chú ý trong các transformer cổ điển, và tăng cường với căn chỉnh chéo giữa các phương thức văn bản và không gian trong các tài liệu có cấu trúc. Đáng chú ý, mô hình của chúng tôi giải quyết các thách thức được đặt ra bởi bố cục không đều và nội dung không đồng nhất bằng cách sử dụng một mục tiêu tiền huấn luyện tập trung vào việc học cách điền vào văn bản khối. Chúng tôi đã tinh chỉnh mô hình được tiền huấn luyện bằng cách sử dụng một tập dữ liệu hướng dẫn toàn diện. Đánh giá của chúng tôi trên các nhiệm vụ trí tuệ tài liệu khác nhau chứng minh rằng DocLLM vượt trội so với các mô hình tương đương trên các nhiệm vụ đã biết cho 14 tập dữ liệu trong 16 và thể hiện khái quát hóa mạnh mẽ cho các tập dữ liệu chưa từng thấy trước đó trong 4 trong 5 thiết lập, khẳng định hiệu quả của nó trong việc trích xuất thông tin có ý nghĩa từ một loạt các tài liệu trực quan. Trong công việc tương lai, chúng tôi dự định truyền thị giác vào DocLLM theo cách nhẹ.

Lời cảm ơn
Bài báo này được chuẩn bị cho mục đích thông tin bởi nhóm nghiên cứu Trí tuệ nhân tạo của JPMorgan Chase & Co và các chi nhánh ("JP Morgan"), và không phải là sản phẩm của Phòng nghiên cứu của JP Morgan. J.P. Morgan không đưa ra tuyên bố và bảo đảm gì và từ chối tất cả trách nhiệm về tính đầy đủ, chính xác hoặc độ tin cậy của thông tin chứa trong đây. Tài liệu này không được dự định làm nghiên cứu đầu tư hoặc lời khuyên đầu tư, hoặc khuyến nghị, đề nghị hoặc thỉnh cầu mua hoặc bán bất kỳ chứng khoán, công cụ tài chính, sản phẩm tài chính hoặc dịch vụ nào, hoặc được sử dụng theo bất kỳ cách nào để đánh giá các giá trị của việc tham gia vào bất kỳ giao dịch nào, và không được cấu thành một thỉnh cầu dưới bất kỳ quyền tài phán nào hoặc đối với bất kỳ người nào, nếu thỉnh cầu như vậy dưới quyền tài phán như vậy hoặc đối với người đó sẽ là bất hợp pháp. © 2023 JP Morgan Chase & Co. Tất cả quyền được bảo lưu.

Tài liệu tham khảo
[1] Arjun Reddy Kunduru. From data entry to intelligence: Artificial intelligence's impact on financial system workflows. International Journal on Orange Technologies, 5(8):38–45, 2023.

[2] Lei Cui, Yiheng Xu, Tengchao Lv, và Furu Wei. Document ai: Benchmarks, models and applications. arXiv preprint arXiv:2111.08609, 2021.

[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners, 2020.

[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.

[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.

[7] Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, và Furu Wei. Dit: Self-supervised pre-training for document image transformer. In Proceedings of the 30th ACM International Conference on Multimedia, pages 3530–3539, 2022.

[8] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, và Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking. In Proceedings of the 30th ACM International Conference on Multimedia, pages 4083–4091, 2022.

--- TRANG 12 ---
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.

[10] Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, và Lidong Zhou. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Chengqing Zong, Fei Xia, Wenjie Li, và Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2579–2591, Online, August 2021. Association for Computational Linguistics.

[11] Chen-Yu Lee, Chun-Liang Li, Timothy Dozat, Vincent Perot, Guolong Su, Nan Hua, Joshua Ainslie, Renshen Wang, Yasuhisa Fujii, và Tomas Pfister. FormNet: Structural encoding beyond sequential modeling in form document information extraction. In Smaranda Muresan, Preslav Nakov, và Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3735–3754, Dublin, Ireland, May 2022. Association for Computational Linguistics.

[12] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, và Mohit Bansal. Unifying vision, text, and layout for universal document processing. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 19254–19264. IEEE, 2023.

[13] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, và Ming Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1192–1200, 2020.

[14] Zihang Meng, Licheng Yu, Ning Zhang, Tamara L Berg, Babak Damavandi, Vikas Singh, và Amy Bearman. Connecting what to say with where to look by modeling human attention traces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12679–12688, 2021.

[15] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, và Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021.

[16] OpenAI. Gpt-4 technical report, 2023.

[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[18] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.

[19] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.

[20] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.

[21] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.

[22] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, và Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[23] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, và Fei Huang. mplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023.

[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[25] Jacob Devlin Ming-Wei Chang Kenton và Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171–4186, 2019.

[26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

--- TRANG 13 ---
[28] Biao Zhang và Rico Sennrich. Root mean square layer normalization, 2019.

[29] Noam Shazeer. Glu variants improve transformer, 2020.

[30] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, và Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023.

[31] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, và Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. CoRR, abs/2307.02499, 2023.

[32] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Alex Lin, và Fei Huang. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. CoRR, abs/2310.05126, 2023.

[33] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, và Seunghyun Park. Ocr-free document understanding transformer. In Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXVIII, page 498–517, Berlin, Heidelberg, 2022. Springer-Verlag.

[34] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, và Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, và Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 18893–18912. PMLR, 2023.

[35] Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, và Seunghyun Park. Ocr-free document understanding transformer, 2022.

[36] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, và Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.

[37] Tianxiao Shen, Hao Peng, Ruoqi Shen, Yao Fu, Zaid Harchaoui, và Yejin Choi. Film: Fill-in language models for any-order generation. arXiv preprint arXiv:2310.09930, 2023.

[38] Pengcheng He, Xiaodong Liu, Jianfeng Gao, và Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations, 2020.

[39] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.

[40] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.

[41] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, và Jason Wei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022.

[42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, và Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.

[43] Minesh Mathew, Dimosthenis Karatzas, và C. V. Jawahar. Docvqa: A dataset for VQA on document images. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA, January 3-8, 2021, pages 2199–2208. IEEE, 2021.

[44] Panupong Pasupat và Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1470–1480. The Association for Computer Linguistics, 2015.

[45] Ryota Tanaka, Kyosuke Nishida, và Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on

--- TRANG 14 ---
Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 13878–13888. AAAI Press, 2021.

[46] Jordy Van Landeghem, Rubèn Tito, Lukasz Borchmann, Michal Pietruszka, Pawel Józiak, Rafal Powalski, Dawid Jurkiewicz, Mickaël Coustaty, Bertrand Anckaert, Ernest Valveny, Matthew B. Blaschko, Sien Moens, và Tomasz Stanislawek. Document understanding dataset and evaluation (DUDE). CoRR, abs/2305.08455, 2023.

[47] Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, và William Yang Wang. Tabfact: A large-scale dataset for table-based fact verification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.

[48] Tomasz Stanislawek, Filip Gralinski, Anna Wróblewska, Dawid Lipinski, Agnieszka Kaliska, Paulina Rosalska, Bartosz Topolski, và Przemyslaw Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In Josep Lladós, Daniel Lopresti, và Seiichi Uchida, editors, 16th International Conference on Document Analysis and Recognition, ICDAR 2021, Lausanne, Switzerland, September 5-10, 2021, Proceedings, Part I, volume 12821 of Lecture Notes in Computer Science, pages 564–579. Springer, 2021.

[49] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, và Hwalsuk Lee. Cord: A consolidated receipt dataset for post-ocr parsing. 2019.

[50] Guillaume Jaume, Hazim Kemal Ekenel, và Jean-Philippe Thiran. FUNSD: A dataset for form understanding in noisy scanned documents. In 2nd International Workshop on Open Services and Tools for Document Analysis, OST@ICDAR 2019, Sydney, Australia, September 22-25, 2019, pages 1–6. IEEE, 2019.

[51] Stacey Svetlichnaya. Deepform: Understand structured documents at scale. 2020.

[52] Marcin Kardas, Piotr Czapla, Pontus Stenetorp, Sebastian Ruder, Sebastian Riedel, Ross Taylor, và Robert Stojnic. AxCell: Automatic extraction of results from machine learning papers. In Bonnie Webber, Trevor Cohn, Yulan He, và Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8580–8594, Online, November 2020. Association for Computational Linguistics.

[53] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, và C. V. Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516–1520, 2019.

[54] Zilong Wang, Yichao Zhou, Wei Wei, Chen-Yu Lee, và Sandeep Tata. VRDU: A benchmark for visually-rich document understanding. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng Yan, Ravi Kumar, Fatma Ozcan, và Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023, pages 5184–5193. ACM, 2023.

[55] Adam W. Harley, Alex Ufkes, và Konstantinos G. Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In 13th International Conference on Document Analysis and Recognition, ICDAR 2015, Nancy, France, August 23-26, 2015, pages 991–995. IEEE Computer Society, 2015.

[56] D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, và J. Heard. Building a test collection for complex document information processing. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '06, page 665–666, New York, NY, USA, 2006. Association for Computing Machinery.

[57] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, và Ming Zhou. DocBank: A benchmark dataset for document layout analysis. In Donia Scott, Nuria Bel, và Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, pages 949–960, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics.

[58] Diederik P. Kingma và Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio và Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[59] Brian L. Davis, Bryan S. Morse, Brian L. Price, Chris Tensmeyer, Curtis Wigington, và Vlad I. Morariu. End-to-end document recognition and understanding with dessurt. In Leonid Karlinsky, Tomer Michaeli, và Ko Nishino, editors, Computer Vision - ECCV 2022 Workshops - Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IV, volume 13804 of Lecture Notes in Computer Science, pages 280–296. Springer, 2022.

[60] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, và Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. CoRR, abs/2305.06500, 2023.

--- TRANG 15 ---
[61] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, và Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. CoRR, abs/2306.17107, 2023.

[62] Lukasz Borchmann, Michal Pietruszka, Tomasz Stanislawek, Dawid Jurkiewicz, Michal Turski, Karolina Szyndler, và Filip Gralinski. DUE: end-to-end document understanding benchmark. In Joaquin Vanschoren và Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021.

[63] Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís Gómez, Marçal Rusiñol, Minesh Mathew, C. V. Jawahar, Ernest Valveny, và Dimosthenis Karatzas. ICDAR 2019 competition on scene text visual question answering. In 2019 International Conference on Document Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25, 2019, pages 1563–1570. IEEE, 2019.

[64] Ramakrishna Vedantam, C. Lawrence Zitnick, và Devi Parikh. Cider: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566–4575. IEEE Computer Society, 2015.
