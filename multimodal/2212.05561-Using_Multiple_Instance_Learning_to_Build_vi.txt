# 2212.05561.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2212.05561.pdf
# Kích thước tệp: 4559064 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Sử dụng Học Đa Thực Thể để Xây dựng
Biểu diễn Đa phương thức
Peiqi Wang1, William M. Wells1, Seth Berkowitz2,
Steven Horng2, và Polina Golland1
1CSAIL, MIT, Cambridge MA, USA
2BIDMC, Harvard Medical School, Boston, MA, USA
wpq@mit.edu, polina@csail.mit.edu
Tóm tắt. Việc học biểu diễn đa phương thức hình ảnh-văn bản căn chỉnh dữ liệu
qua các phương thức và cho phép các ứng dụng y tế quan trọng, ví dụ như phân
loại hình ảnh, định vị trực quan, và truy xuất đa phương thức. Trong nghiên cứu
này, chúng tôi thiết lập mối liên hệ giữa việc học biểu diễn đa phương thức
và học đa thực thể. Dựa trên mối liên hệ này, chúng tôi đề xuất một khung
chung để xây dựng các hàm điểm số bất biến hoán vị với nhiều phương pháp
học biểu diễn đa phương thức hiện tại như các trường hợp đặc biệt. Hơn nữa,
chúng tôi sử dụng khung này để phát triển một phương pháp học tương phản
mới và chứng minh rằng phương pháp của chúng tôi đạt kết quả tốt nhất trong
một số tác vụ hạ nguồn.
Từ khóa: học biểu diễn, học đa thực thể

1 Giới thiệu
Trong bài báo này, chúng tôi đề xuất một khung để thiết kế các phương pháp học biểu diễn đa phương thức bao gồm các phương pháp trước đó như các trường hợp đặc biệt và đưa ra một thuật toán mới cho việc học đa phương thức vượt trội so với hiện tại. Cụ thể, chúng tôi thiết lập mối liên hệ giữa học biểu diễn tự giám sát dựa trên học tương phản và học đa thực thể [3] và chỉ ra rằng chúng có các giả định và mục tiêu tương tự. Chúng tôi mang những hiểu biết từ học đa thực thể để cung cấp một góc nhìn mới về học biểu diễn tự giám sát và các ý tưởng cải thiện hiệu suất. Với mối liên hệ này, chúng tôi phát triển một thuật toán mới để học biểu diễn hình ảnh-văn bản nắm bắt cấu trúc chung giữa hai phương thức và tổng quát hóa tốt trong nhiều tác vụ hạ nguồn.

Chúng tôi nhằm thiết lập sự căn chỉnh giữa hình ảnh và văn bản liên quan để cải thiện quy trình lâm sàng. Ví dụ, một mô hình hình ảnh mô phỏng cách giải thích của bác sĩ X quang có thể gắn nhãn hình ảnh hồi tố để chọn bệnh nhân phù hợp cho thử nghiệm lâm sàng. Hơn nữa, sự căn chỉnh cục bộ giữa các vùng hình ảnh và các đoạn văn bản (ví dụ, câu) hứa hẹn có lợi cho nhiều tác vụ hạ nguồn. Ví dụ, truy xuất đa phương thức có thể cung cấp mô tả về một vùng hình ảnh để tự động hóa tài liệu hoặc cho phép so sánh với các bệnh nhân được chụp ảnh trước đó tương tự

--- TRANG 2 ---
2 P. Wang et al.
để giải thích tốt hơn dựa trên giải phẫu hoặc bệnh lý cục bộ. Tương tự, các bác sĩ X quang ghi chép phát hiện có thể xác minh độ chính xác của báo cáo bằng cách lưu ý liệu vị trí được đề cập (tức là định vị trực quan của văn bản) có phù hợp với ấn tượng của họ về hình ảnh hay không.

Việc học biểu diễn tự giám sát là một công cụ hữu ích để giảm gánh nặng chú thích cho các mô hình học máy trong hình ảnh y tế. Mặc dù có nhu cầu và cơ hội tự động hóa, việc phát triển các phương pháp học máy mạnh mẽ bị cản trở bởi việc thiếu chú thích làm tín hiệu giám sát cho việc học. Việc học biểu diễn tự giám sát trên dữ liệu hình ảnh-văn bản ghép cặp mang lại hai lợi thế: (i) việc học không cần chú thích thêm và (ii) coi văn bản như "nhãn" cho phép chúng ta sử dụng ngôn ngữ tự nhiên để tham chiếu các khái niệm trực quan và ngược lại [30]. Do đó, chúng tôi tập trung vào việc học biểu diễn đa phương thức hình ảnh-văn bản nhưng khung đề xuất có thể áp dụng rộng rãi cho việc học biểu diễn trên dữ liệu đa phương thức khác.

Việc học biểu diễn chung liên quan đến việc huấn luyện các bộ mã hóa hình ảnh và văn bản để thực hiện các tác vụ tự giám sát trên dữ liệu hình ảnh-văn bản ghép cặp [5,22,25] và đánh giá trên các tác vụ hạ nguồn liên quan. Chúng tôi tập trung vào học tương phản, tức là phân loại các cặp hình ảnh-văn bản là khớp (tức là tương ứng với cùng một sự kiện hình ảnh) hoặc không khớp. Học tương phản đã được áp dụng cho lĩnh vực y tế, thể hiện khả năng chuyển giao ấn tượng trên một tập hợp đa dạng các tác vụ [2,4,13,23,28,36]. Những cải thiện lớn nhất đến từ việc giải quyết các thách thức độc đáo với lĩnh vực này, ví dụ như việc sử dụng chú ý chéo để xử lý việc thiếu các bộ phát hiện bệnh lý hiệu quả [13] và thích ứng các mô hình ngôn ngữ để giải quyết các thách thức ngôn ngữ học trong ghi chú lâm sàng [2]. Việc huấn luyện các mô hình đã liên quan đến các hàm mất mát tương phản ngày càng phức tạp xử lý hình ảnh và văn bản một cách đối xứng [2,4,13,36] và ở nhiều quy mô [2,13,23,28]. Trái ngược với công việc trước đây dựa trên nhiều số hạng mất mát, hàm mất mát tương phản được đề xuất của chúng tôi đơn giản để triển khai và mang lại hiệu suất vượt trội.

Mượn ý tưởng từ học đa thực thể, chúng tôi coi các đặc trưng vùng hình ảnh cục bộ là "dữ liệu" và các đặc trưng câu là "nhãn" (phức tạp). Học đa thực thể là một loại học giám sát yếu có hiệu quả cho các vấn đề thiếu chú thích chi tiết [3]. Ví dụ, nó có thể giúp định vị các tế bào khối u trong hình ảnh slide toàn bộ chỉ với nhãn cấp hình ảnh [21]. Trọng tâm của học đa thực thể là việc xây dựng các hàm điểm số bất biến hoán vị [14], và việc lựa chọn cách tổng hợp các điểm số hoặc đặc trưng thực thể để đánh giá so với nhãn cấp hình ảnh. Các bộ tổng hợp thực thể hiệu quả tận dụng kiến thức chuyên ngành [8], ví dụ như bộ tổng hợp Noisy-OR cho dự đoán hoạt động thuốc [26], bộ tổng hợp Noisy-AND cho phân loại kiểu hình tế bào [19].

Trong nghiên cứu của chúng tôi, chúng tôi mở rộng phân loại đa thực thể sang học tương phản bằng cách xây dựng các hàm điểm số hình ảnh-văn bản bất biến hoán vị. Dựa trên những hiểu biết từ phân loại đa thực thể với các thực thể tương quan [21], bộ tổng hợp thực thể được đề xuất của chúng tôi khai thác sự tương quan giữa các thực thể để xây dựng biểu diễn hoạt động tốt trong các tác vụ hạ nguồn.

Nhiều phương pháp học đa thực thể trước đây tập trung vào một tác vụ cụ thể, ví dụ như phát hiện [35], phân loại vùng [7], hoặc truy xuất [17]. Một số đã

--- TRANG 3 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 3
khám phá các lựa chọn bộ tổng hợp thực thể cho nhiều hơn một tác vụ hạ nguồn [10,27] nhưng bị hạn chế về tính tổng quát (tức là không dành cho các ứng dụng khác) và phạm vi (tức là chỉ khám phá một số bộ tổng hợp thực thể đơn giản). Trái ngược, khung được đề xuất của chúng tôi để xây dựng các hàm điểm số bất biến hoán vị có thể được áp dụng dễ dàng cho các ứng dụng khác. Chúng tôi khám phá có hệ thống các bộ tổng hợp thực thể và tác động của chúng lên việc học biểu diễn, dẫn đến một phương pháp mới để học biểu diễn chung. Chúng tôi đánh giá các biểu diễn hình ảnh-văn bản kết quả trên một tập hợp đa dạng các tác vụ hạ nguồn và chứng minh hiệu suất tiên tiến trên tất cả các tác vụ trong bối cảnh một tập lớn các hình ảnh X quang ngực và các báo cáo X quang liên quan.

2 Phương pháp
Trước tiên chúng tôi giới thiệu ký hiệu và thảo luận về các phương pháp cục bộ và toàn cục để xây dựng các hàm điểm số hình ảnh-tài liệu bất biến hoán vị tại cốt lõi của quy trình học. Sau đó chúng tôi cụ thể hóa khung cho một lựa chọn cụ thể của các bộ tổng hợp cho học tương phản.

2.1 Thiết lập Vấn đề
Một biểu diễn cục bộ D-chiều của một hình ảnh có N vùng được đề xuất là một tập hợp N vectơ đặc trưng xn ∈ X ⊆ R^D, n ∈ {1, ..., N}. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng chia lưới đều để tạo ra các vùng hình ảnh và để lại các phương pháp đề xuất phức tạp hơn (ví dụ, [31]) cho công việc tương lai. Một biểu diễn cục bộ của một tài liệu M-câu (ví dụ, một báo cáo X quang) là một tập hợp các vectơ đặc trưng câu ym ∈ Y ⊆ R^D, m ∈ {1, ..., M}.

Hàm h : X × Y → R đo độ tương tự giữa các biểu diễn, ví dụ, h(xn, ym) là độ tương tự giữa một vùng và một câu. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng độ tương tự cosine h(x, y) = ⟨x, y⟩/(||x|| ||y||), mặc dù công thức chấp nhận bất kỳ hàm tương tự khả vi nào.

Với bất kỳ không gian vectơ U nào, hàm tổng hợp Φ : P(U) → U tổng hợp các phần tử trong tập hợp đầu vào thành một "đại diện". P(U) là tập hợp của tất cả các tập con hữu hạn của U. Ví dụ, Φ({xn}) = (1/N)∑n xn tổng hợp N đặc trưng vùng xn ∈ X bằng cách lấy trung bình chúng, trong khi Φ({hn}) = maxn hn tổng hợp N điểm số tương tự thành một điểm số duy nhất bằng cách tính điểm số tối đa. Chúng tôi giới hạn sự chú ý vào các bộ tổng hợp bất biến hoán vị, tức là chúng xử lý đầu vào như một tập hợp không có thứ tự hơn là một vectơ có thứ tự.

Hàm điểm số hình ảnh-tài liệu bất biến hoán vị S : P(X) × P(Y) → R đo độ tương tự giữa một hình ảnh và một tài liệu dựa trên các đặc trưng vùng {xn} và các đặc trưng câu {ym}.

2.2 Hàm Điểm Số Bất Biến Hoán Vị Cục Bộ & Toàn Cục
Việc học biểu diễn tương phản có thể được xem như việc tối đa hóa khả năng phân loại chính xác các cặp hình ảnh-văn bản là khớp hoặc không khớp. Vì giám sát được cung cấp ở cấp độ hình ảnh-tài liệu, chúng tôi định nghĩa một khung để xây dựng các hàm điểm số hình ảnh-tài liệu bất biến hoán vị.

--- TRANG 4 ---
4 P. Wang et al.
Hình 1. Hàm điểm số hình ảnh-tài liệu cục bộ (trên) và toàn cục (dưới).

Phương pháp cục bộ tổng hợp các điểm số vùng-câu thành điểm số hình ảnh-câu. Điểm số hình ảnh-câu gm cho câu m trong tài liệu được thu nhận bằng cách áp dụng hàm tổng hợp cục bộ Φl cho các điểm số vùng-câu, tức là
gm = Φl({h(xn, ym)}n) = Φl({h(x1, ym), ..., h(xN, ym)}).

Phương pháp toàn cục trước tiên tổng hợp các đặc trưng vùng cục bộ {xn} thành một vectơ đặc trưng hình ảnh duy nhất Φg({xn}) sử dụng hàm tổng hợp toàn cục Φg. Điểm số hình ảnh-câu gm được tính bằng cách sử dụng hàm tương tự h trên vectơ đặc trưng hình ảnh Φg({xn}) và vectơ đặc trưng câu ym, tức là gm = h(Φg({xn}), ym).

Trong cả hai phương pháp, điểm số hình ảnh-tài liệu S được thu nhận bằng cách tổng hợp các điểm số hình ảnh-câu với hàm tổng hợp khác Φs, tức là S({xn}, {ym}) = Φs({gm}). Hình 1 minh họa khung để xây dựng S. Tóm lại, các điểm số hình ảnh-tài liệu cục bộ và toàn cục Sl và Sg được tính như sau:
Sl({xn}, {ym}) = Φs({Φl({h(xn, ym)}n)}m);                    (1)
Sg({xn}, {ym}) = Φs({h(Φg({xn}), ym)}m).                     (2)

Vì các hàm tổng hợp là bất biến hoán vị, hàm điểm số hình ảnh-tài liệu S tự nhiên cũng bất biến hoán vị. Chúng tôi nhấn mạnh rằng S xử lý các đặc trưng hình ảnh và các đặc trưng văn bản khác nhau, và thứ tự áp dụng đánh giá tương tự h(·) và các bộ tổng hợp Φ(·) có liên quan thực nghiệm.

Quyết định thiết kế này được thúc đẩy bởi thực tế rằng mỗi câu trong báo cáo X quang đại diện cho một khái niệm và vị trí của nó trong hình ảnh, tức là nó giống như một nhãn cho một vùng nào đó trong hình ảnh. Điều ngược lại không nhất thiết đúng vì một số phần của hình ảnh không được mô tả trong báo cáo.

2.3 Học Biểu Diễn với Bộ Tổng Hợp LSE + NL
Trong phần này, chúng tôi giới thiệu phương pháp LSE + NL để học biểu diễn đa phương thức dựa trên sự kết hợp của các hàm điểm số hình ảnh-tài liệu cục bộ và toàn cục và hàm mất mát tương phản văn bản-hình ảnh bất đối xứng.

--- TRANG 5 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 5
Lấy cảm hứng từ [21], chúng tôi sử dụng hàm tối đa mềm để xác định vùng phù hợp nhất cho một câu, tức là vùng quan trọng, và chú ý nhiều hơn đến các vùng tương tự với vùng quan trọng. Cụ thể, bộ tổng hợp cục bộ Φl là hàm log-sum-exp (LSE)
Φl({hn}) = (1/βl)log∑(n=1 to N)exp(βl hn);                    (3)
trong đó βl là tham số thang đo kiểm soát mức độ hàm LSE xấp xỉ hàm max. Bộ tổng hợp toàn cục Φg kết hợp tuyến tính các đặc trưng vùng sử dụng khoảng cách đến vùng quan trọng làm trọng số, tức là
Φg({xn}) = ∑(n=1 to N)[exp(βg⟨Axn, Axk⟩)/∑(n'=1 to N)exp(βg⟨Axn', Axk⟩)]xn;     (4)
trong đó k là chỉ số của vùng quan trọng, tức là k = arg maxn h(xn, ym), A là ma trận trọng số học được, và βg là tham số thang đo cho hàm softmax.

Chúng ta có thể diễn giải Φg như một dạng chú ý trong đó các vùng tương tự hơn với vùng quan trọng được cho trọng số chú ý cao hơn. Thực tế, Φg khai thác sự tương quan giữa mỗi vùng và vùng quan trọng bằng cách sử dụng chú ý. Ngoài ra, Φg có thể được xem như một dạng mạng phi cục bộ (NL) [34]. Cả Φl và Φg đều là các hàm bất biến hoán vị. Chúng tôi chọn Φs là hàm trung bình.

Chúng tôi sử dụng các điểm số hình ảnh-tài liệu cục bộ và toàn cục trong (1) và (2) được tính với lựa chọn Φl và Φg của chúng tôi cho học tương phản. Cho một tài liệu, chúng tôi tạo một vectơ điểm số hình ảnh-tài liệu s = (s+, s-1, ..., s-K) trong đó s+ ∈ R là điểm số hình ảnh-tài liệu với hình ảnh khớp của nó và s-k ∈ R cho k = 1, ..., K là điểm số hình ảnh-tài liệu với K hình ảnh không khớp. Chúng tôi sử dụng sl và sg để ký hiệu các vectơ điểm số có độ dài (K+1) được định nghĩa ở trên tính bằng cách sử dụng các hàm điểm số cục bộ và toàn cục tương ứng. Các bộ mã hóa hình ảnh và văn bản được huấn luyện để tối thiểu hóa L(sl) + L(sg) trên các tài liệu trong tập huấn luyện trong đó L là hàm mất mát tương phản văn bản-hình ảnh [29,36]
L(s) = -log[exp(τs+)/(exp(τs+) + ∑(k=1 to K)exp(τs-k))]        (5)
với tham số thang đo τ. Trong phương trình trên, s là vectơ sl được tính bằng cách sử dụng (1) với Φl được định nghĩa trong (3) hoặc vectơ sg được tính bằng cách sử dụng (2) với Φg được định nghĩa trong (4). Hàm mất mát tương phản hình ảnh-văn bản trong đó các điểm số âm được tính cho một hình ảnh với K tài liệu không khớp khác nhau thường được sử dụng cùng với L trong công việc trước đây [2,4,13,36]. Chúng tôi chọn xử lý hình ảnh và văn bản bất đối xứng và chỉ ra rằng hàm mất mát tương phản văn bản-hình ảnh đơn giản là đủ để tạo ra các biểu diễn tổng quát hóa tốt.

3 Kết nối với Học Đa Thực Thể
Trong học đa thực thể [3], một tập hợp chứa nhiều thực thể {x1, ..., xN} được gọi là túi. Tập huấn luyện bao gồm các túi và các

--- TRANG 6 ---
6 P. Wang et al.
Bảng 1. Phân loại các phương pháp liên quan để học biểu diễn hình ảnh-ngôn ngữ trong khung lấy cảm hứng từ học đa thực thể của chúng tôi. Đối với mỗi phương pháp, chúng tôi báo cáo các đoạn hình ảnh được xử lý bởi xn (vùng hoặc video), các đoạn ngôn ngữ được xử lý bởi ym (từ, câu, hoặc âm thanh), bộ tổng hợp cục bộ Φl nếu được sử dụng (Max hoặc LSE), bộ tổng hợp toàn cục Φg nếu được sử dụng (Avg, NN cho các hàm phi tuyến tổng quát, chú ý chéo (CA) Φl({xn}, ym) = ∑n exp(⟨xn, ym⟩)/∑n' exp(⟨xn', ym⟩)xn, hoặc NL trong (4)), và bộ tổng hợp điểm số cuối cùng Φs (Sum, Max, LSE, Id, Avg).

Phương pháp     xn      ym      Φl    Φg    Φs
NeuralTalk [16] vùng    từ      Max   -     Sum
DAVEnet-MISA [10] vùng  âm thanh Max   -     Sum
MIML [9]        video   âm thanh Max   -     Max
MIL-NCE [27]    video   câu      -     Avg   LSE
ConVIRT/CLIP [36,30] vùng câu   -     NN    Avg Id
GLoRIA/BioViL [13,2] vùng từ   -     CA    LSE
                vùng    câu      -     Avg   Id
LSE+NL (Của chúng tôi) vùng câu LSE   -     Avg
                vùng    câu      -     NL    Avg

nhãn túi liên quan y trong khi các nhãn thực thể không được cung cấp. Đối với nhãn túi nhị phân, một túi dương được đảm bảo bao gồm ít nhất một thực thể dương, trong khi một túi âm không bao gồm thực thể dương nào. Các nhãn cấp túi được sử dụng để huấn luyện bộ phân loại gán nhãn cấp thực thể và cấp túi trong các túi mới, chưa thấy.

Các thuật toán học biểu diễn hình ảnh-văn bản hiện có có thể dự đoán [6] hoặc tương phản [30] có thể được xem như một dạng học đa thực thể. Cụ thể, chúng ta có thể xem một hình ảnh như một túi các đặc trưng vùng và câu tương ứng mô tả hình ảnh như nhãn túi. Thay vì nhận các giá trị nhị phân, các nhãn túi có thể đại diện cho các danh mục tùy ý thông qua ngôn ngữ tự nhiên. Mặc dù vùng chính xác tương ứng với câu không được biết, hình ảnh khớp chứa ít nhất một vùng tương ứng với văn bản trong khi một hình ảnh được lấy mẫu ngẫu nhiên rất có thể không có. Tương tự như học đa thực thể, các phương pháp học biểu diễn tự giám sát sử dụng các giả định này để học.

Tổng quát hơn, chúng tôi coi nhãn văn bản như một túi các câu. Ví dụ, các câu mô tả phát hiện trong hình ảnh X quang ngực rất có thể có thể được hoán vị mà không thay đổi ý nghĩa tổng thể. Do đó, việc học biểu diễn có thể được diễn giải như việc dự đoán túi nhãn {ym} cho túi đầu vào {xn}. Thiết lập này tương ứng với học đa thực thể đa nhãn [37].

Hơn nữa, học đa thực thể và học biểu diễn đa phương thức có cùng mục tiêu so sánh được. Học đa thực thể nhằm căn chỉnh các thực thể và túi với nhãn sao cho mô hình được tiền huấn luyện hoạt động tốt trong các tác vụ phân loại. Học biểu diễn đa phương thức nhằm căn chỉnh hình ảnh và các vùng con của chúng với văn bản sao cho mô hình được tiền huấn luyện hoạt động tốt trên các tác vụ dựa trên sự căn chỉnh đó, ví dụ phân loại hình ảnh dựa trên sự căn chỉnh hình ảnh-câu, định vị trực quan và truy xuất đa phương thức dựa trên sự căn chỉnh vùng-câu.

--- TRANG 7 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 7
Có hai phương pháp học đa thực thể chính, phương pháp cấp thực thể và cấp nhúng [1]. Phương pháp cấp thực thể tính điểm số túi bằng cách tổng hợp các điểm số thực thể, trong khi phương pháp cấp nhúng tính điểm số túi dựa trên đặc trưng túi được tổng hợp từ các đặc trưng thực thể. Các phương pháp cục bộ và toàn cục trong Phần 2.2 là các mở rộng của phương pháp thực thể và nhúng sang học tương phản.

Sự tương đồng này cho phép chúng tôi phân tích các phương pháp trước đây như các thể hiện của khung được định nghĩa trong Phần 2.2 lấy cảm hứng từ học đa thực thể (Bảng 1). Chúng tôi tạo ra một tổng quát hóa cho công thức trong Phần 2.2 để phù hợp với chú ý chéo [20]: hàm tổng hợp cục bộ Φl có thể dựa trên các đặc trưng nhãn ym để đa hóa hành vi của nó, tức là Φl : P(X) × Y → X. Tóm lại, một tập hợp đa dạng các bộ tổng hợp Φl, Φg, Φs đã được chứng minh về việc học biểu diễn đa phương thức ở các quy mô khác nhau, ngụ ý có thể không có một tập hợp duy nhất các bộ tổng hợp hoạt động tốt cho mọi vấn đề. Thực tế hơn, các hàm tổng hợp tốt nhất là những hàm phù hợp tốt với các giả định cụ thể của ứng dụng.

4 Thí nghiệm
Chúng tôi minh họa phương pháp được đề xuất bằng cách xây dựng một biểu diễn của hình ảnh X quang ngực trực diện và các báo cáo X quang liên quan và sử dụng nó trong các tác vụ hạ nguồn. Trong tất cả các thí nghiệm, dữ liệu được sử dụng để học biểu diễn tách biệt khỏi các tập kiểm tra được sử dụng để đánh giá các tác vụ hạ nguồn.

Chúng tôi chuẩn hóa các hình ảnh và thay đổi kích thước chúng thành độ phân giải 512x512. Chúng tôi áp dụng các tăng cường hình ảnh ngẫu nhiên, tức là cắt ngẫu nhiên 480x480, biến đổi độ sáng và độ tương phản, và biến đổi affine ngẫu nhiên (chỉ cho việc tinh chỉnh mô hình hình ảnh trong quá trình đánh giá). Chúng tôi sử dụng PySBD [32] để token hóa câu.

Chúng tôi sử dụng ResNet-50 [12] làm bộ mã hóa vùng hình ảnh và CXR-BERT [2] làm bộ mã hóa câu. Mỗi bộ mã hóa được theo sau bởi một phép chiếu tuyến tính vào không gian nhúng 128 chiều. Cụ thể, các kích hoạt conv-5 ResNet-50 được chiếu hoạt động như các đặc trưng vùng {xn} và các nhúng từ có ngữ cảnh được gộp trung bình được chiếu hoạt động như các đặc trưng câu {ym}.

4.1 Học biểu diễn
Chúng tôi sử dụng một tập con gồm 234,073 hình ảnh X quang ngực và báo cáo từ MIMIC-CXR [15] để học biểu diễn. Chúng tôi khởi tạo ngẫu nhiên bộ mã hóa hình ảnh và sử dụng mô hình CXR-BERT [2] được tiền huấn luyện trên corpus y sinh học (tức là mô hình giai đoạn II) làm bộ mã hóa câu. Chúng tôi sử dụng bộ tối ưu hóa AdamW [24] và giảm tốc độ học ban đầu 5e-5 bằng cách sử dụng lịch trình cosine với 2k bước khởi động. Chúng tôi khởi tạo τ thành 14 và tối ưu hóa siêu tham số này cùng với các tham số bộ mã hóa. Chúng tôi đặt các tham số thang đo khác như sau: βl = 0.1; βg = e. Chúng tôi sử dụng kích thước lô là 64. Đối với mỗi hình ảnh trong lô, chúng tôi lấy mẫu 5 câu, với thay thế nếu cần, để tạo nên túi nhãn. Ở đây, N = 225 và M = 5.

--- TRANG 8 ---
8 P. Wang et al.
4.2 Các Tác vụ Hạ nguồn
Phân loại Hình ảnh Để đánh giá hiệu suất phân loại không shot (ZS) và tinh chỉnh (FT), chúng tôi sử dụng cùng một phần chia của RSNA Pneumonia (RSNA) [33] như trong [13], cụ thể là 18,678/4,003/4,003 cho huấn luyện/xác thực/kiểm tra. Để đánh giá hiệu suất phân loại tinh chỉnh trong phân phối trong nghiên cứu khử bỏ, chúng tôi sử dụng 5 nhãn CheXpert (Atelectasis, Cardiomegaly, Edema, Pleural Effusion, Pneumothorax) trên tập dữ liệu MIMIC-CXR [15] mà chúng tôi gọi là MIMIC-CheXpert (CheX). Có khoảng 1k hình ảnh trong tập kiểm tra liên quan đến mỗi nhãn CheXpert. Để đánh giá hiệu quả dữ liệu của các phương pháp học biểu diễn, chúng tôi sử dụng các lượng dữ liệu huấn luyện khác nhau (1% và 100%).

Đối với phân loại hình ảnh không shot, trước tiên chúng tôi token hóa và mã hóa các prompt văn bản cụ thể theo lớp (ví dụ, "Findings suggesting pneumonia." và "No evidence of pneumonia."). Đối với mỗi hình ảnh, chúng tôi gán một nhãn nhị phân tương ứng với prompt có điểm số hình ảnh-câu cao hơn. Chúng tôi thấy rằng việc chuẩn hóa các điểm số thành [0,1] cho mỗi lớp trước khi áp dụng softmax là quan trọng. Đối với phân loại hình ảnh tinh chỉnh, chúng tôi sử dụng bộ tối ưu hóa Adam [18] với tốc độ học 3e-3 để tối ưu hóa các trọng số được khởi tạo ngẫu nhiên và một độ lệch trên các đặc trưng vùng được gộp trung bình trong khi giữ cố định các trọng số bộ mã hóa. Đối với RSNA Pneumonia, chúng tôi báo cáo độ chính xác và AUC. Đối với MIMIC-CheXpert, chúng tôi báo cáo AUC trung bình trên năm tác vụ phân loại nhị phân.

Định vị Trực quan Chúng tôi đánh giá hiệu suất định vị trực quan bằng cách sử dụng các chú thích vùng-câu MS-CXR [2]. Tập dữ liệu này bao gồm 1,448 hộp giới hạn trên 1,162 hình ảnh, trong đó mỗi hộp giới hạn được liên kết với một câu mô tả đặc trưng X quang chủ đạo của nó. Chúng tôi tính các điểm số vùng-câu để định lượng mức độ câu được định vị tốt trong hình ảnh. Chúng tôi báo cáo một thước đo sự khác biệt giữa các điểm số vùng-câu bên trong và bên ngoài hộp giới hạn, tức là tỷ lệ tương phản-nhiễu (CNR) [2], và mức độ các điểm số vùng-câu được ngưỡng hóa chồng lấp với hộp giới hạn trung bình, tức là giao của hợp trung bình (mIoU). Trái ngược với [2], chúng tôi chọn các ngưỡng kéo dài [-1,1] theo bước 0.05 để tính mIoU cho một so sánh công bằng.

Truy xuất Đa phương thức Chúng tôi đánh giá hiệu suất truy xuất đa phương thức cũng sử dụng tập dữ liệu MS-CXR. Chúng tôi tính các đặc trưng hộp giới hạn từ các đặc trưng vùng với RoIAlign [11]. Chúng tôi tính các điểm số hộp-câu và sắp xếp chúng để truy xuất các mục trong một phương thức cho một truy vấn từ phương thức khác. Mục được truy xuất chính xác là mục được ghép cặp với mục truy vấn. Chúng tôi báo cáo tỷ lệ lần tìm thấy mục chính xác trong K kết quả hàng đầu (R@K) và thứ hạng trung vị của mục chính xác trong danh sách xếp hạng (MedR).

4.3 Kết quả
So sánh với Các Phương pháp Tiên tiến Chúng tôi so sánh phương pháp được đề xuất LSE + NL với các phương pháp tiên tiến GLoRIA [13] và BioViL [2]. GLoRIA là một phương pháp học biểu diễn học dựa trên các cặp hình ảnh-câu và vùng-từ. BioViL cải thiện GLoRIA bằng cách sử dụng một văn bản tốt hơn

--- TRANG 9 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 9
Bảng 2. Hiệu suất phân loại hình ảnh trên tập dữ liệu RSNA Pneumonia. Chúng tôi báo cáo độ chính xác và AUC trên phân loại không shot và tinh chỉnh (tinh chỉnh trên 1% và 100% nhãn). Phương pháp của chúng tôi so sánh thuận lợi với BioViL [2].

Phương pháp  Không Shot    1%           100%
            ACC↑  AUC↑  ACC↑  AUC↑  ACC↑  AUC↑
BioViL      0.73  0.83  0.81  0.88  0.82  0.89
LSE+NL      0.80  0.84  0.84  0.87  0.85  0.89

Bảng 3. Hiệu suất định vị trực quan. Chúng tôi báo cáo tỷ lệ tương phản-nhiễu (CNR) và giao của hợp trung bình (mIoU). mIoU đo giao của hợp trung bình của bản đồ vùng-câu được ngưỡng hóa và hộp giới hạn thực tế trên một tập hợp các ngưỡng. Phương pháp của chúng tôi vượt trội hơn BioViL [2] trên cả hai thước đo.

Phương pháp  CNR↑  mIoU↑
BioViL      1.14  0.17
LSE+NL      1.44  0.19

Bảng 4. Hiệu suất truy xuất đa phương thức. Chúng tôi báo cáo recall cho 10, 50 và 100 câu trả lời hàng đầu được trả về bởi phương pháp, cũng như thứ hạng trung vị của phần tử thực tế cho truy xuất câu dựa trên truy vấn vùng và cho truy xuất vùng dựa trên truy vấn câu. Phương pháp của chúng tôi vượt trội hơn các baseline trên tất cả các thước đo.

Phương pháp  Vùng → Câu                     Câu → Vùng
            R@10↑ R@50↑ R@100↑ MedR↓    R@10↑ R@50↑ R@100↑ MedR↓
GLoRIA      0.06  0.21  0.37   162      0.06  0.21  0.34   183
BioViL      0.07  0.26  0.40   151      0.08  0.26  0.40   146
LSE+NL      0.11  0.29  0.45   119      0.11  0.36  0.51   97

bộ mã hóa, dựa trên hàm mất mát tương phản đối xứng và mô hình hóa ngôn ngữ có mặt nạ để học biểu diễn. Chúng tôi bỏ qua việc báo cáo hiệu suất phân loại và định vị trực quan của GLoRIA vì [2] đã chỉ ra rằng BioViL tốt hơn GLoRIA trên các tác vụ này. Mô hình đơn giản của chúng tôi cung cấp hiệu suất tốt hơn một cách nhất quán so với các thuật toán tiên tiến này.

Bảng 2 báo cáo độ chính xác phân loại hình ảnh dựa trên các biểu diễn học được cho các lượng dữ liệu khác nhau được sử dụng để tinh chỉnh biểu diễn cho tác vụ hạ nguồn (không shot, 1%, và 100%). Phương pháp của chúng tôi cạnh tranh hoặc tốt hơn so với baseline, đặc biệt trong thiết lập không shot, nhấn mạnh tiềm năng của nó cho các tình huống chú thích hạn chế. Bảng 3 và Bảng 4 báo cáo hiệu suất của các phương pháp trên định vị trực quan và truy xuất đa phương thức tương ứng. Phương pháp của chúng tôi vượt trội đáng kể so với baseline.

Hình 2 minh họa các ví dụ về định vị trực quan. Không giống như [2], chúng tôi không làm mượt các điểm số vùng-câu được tạo ra bởi mô hình của chúng tôi. Phương pháp của chúng tôi tạo ra các điểm số vùng-câu tốt hơn về mặt chất lượng so với BioViL trên một số trường hợp thất bại thách thức được thảo luận trong [2]. Cụ thể, mô hình được tiền huấn luyện của chúng tôi nắm bắt các đặc tả vị trí hiệu quả hơn, ví dụ như nhận biết "at both lung bases" trong hình ảnh đầu tiên và "right" trong hình ảnh thứ ba. Cả phương pháp của chúng tôi và BioViL đều dễ bị dương tính giả, tức là các vùng bên ngoài hộp giới hạn thực tế với điểm số vùng-câu cao, điều này làm nổi bật nhu cầu cải thiện thêm.

--- TRANG 10 ---
10 P. Wang et al.
Hình 2. Ví dụ kết quả định vị trực quan cho một số trường hợp thách thức đối với BioViL [2] (hàng trên) và phương pháp của chúng tôi (hàng dưới). Các truy vấn văn bản và các hộp giới hạn thực tế tương ứng được hiển thị cho mỗi hình ảnh. Lớp phủ bản đồ màu trực quan hóa các điểm số vùng-câu (xanh dương tương ứng với điểm số thấp, đỏ làm nổi bật các vùng có điểm số cao). Phương pháp của chúng tôi cung cấp các bản đồ căn chỉnh tốt hơn với các hộp giới hạn thực tế.

Bảng 5. Kết quả nghiên cứu khử bỏ. Đối với mỗi biến thể của phương pháp, thống kê hiệu suất được báo cáo cho mỗi tác vụ hạ nguồn một cách nhất quán với Bảng 2, 3, và 4. RSNA là RSNA Pneumonia. CheX là MIMIC-CheXpert. FT là phân loại tinh chỉnh sử dụng 100% nhãn. ZS là phân loại không shot. Chúng tôi báo cáo AUC cho phân loại hình ảnh. Các biểu diễn cục bộ hoạt động tốt cho phân loại hình ảnh, trong khi định vị trực quan và truy xuất đa phương thức được hưởng lợi từ việc tích hợp các biểu diễn cục bộ và toàn cục.

Phương pháp     Phân loại                        Định vị  Truy xuất Đa phương thức
                RSNA-ZS↑ RSNA-FT↑ CheX-FT↑    CNR↑    MedR(I→T)↓ MedR(T→I)↓
LSE             0.856    0.892     0.874      1.308   146         137
NL              0.636    0.871     0.854      0.836   264         272
LSE+Average     0.851    0.889     0.868      0.915   191         161
LSE+NL          0.846    0.891     0.870      1.403   110         102
w. ResNet-50    0.844    0.890     0.870      1.438   119         97

Khử bỏ Trong nghiên cứu khử bỏ (Bảng 5), chúng tôi so sánh phương pháp LSE + NL của chúng tôi với việc chỉ sử dụng phương pháp LSE cục bộ hoặc NL toàn cục, cũng như thay thế NL bằng trung bình làm bộ tổng hợp vùng, tức là LSE + Average. Để cho phép thí nghiệm rộng rãi, chúng tôi sử dụng ResNet-18 làm bộ mã hóa hình ảnh. LSE + NL cung cấp sự cân bằng tốt giữa căn chỉnh vùng-câu và hình ảnh-câu. LSE + NL có hiệu suất tương đương với LSE cho các tác vụ phân loại hình ảnh trong khi vượt trội đáng kể so với tất cả các phương án khác trong định vị trực quan và truy xuất đa phương thức. Sử dụng mô hình bộ mã hóa hình ảnh lớn hơn ResNet-50 chỉ cung cấp cải thiện khiêm tốn trong định vị trực quan.

--- TRANG 11 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 11
Hình 3. Ảnh hưởng của lựa chọn bộ tổng hợp lên hiệu suất. Hiệu suất của các mô hình được huấn luyện với các bộ tổng hợp cục bộ (các sắc thái xanh dương), bộ tổng hợp toàn cục (các sắc thái cam) và sự kết hợp của các bộ tổng hợp cục bộ và toàn cục (các sắc thái xanh lá) được hiển thị cho phân loại hình ảnh (AUC), định vị trực quan (CNR) và truy xuất đa phương thức (MedR trung bình cho cả hai hướng). Các thước đo được chuẩn hóa thành khoảng đơn vị để so sánh dễ dàng hơn qua các tác vụ. Lựa chọn bộ tổng hợp ảnh hưởng đến hiệu suất phân loại hình ảnh ít hơn nhiều so với định vị trực quan và truy xuất đa phương thức. Có sự biến đổi hiệu suất cao trong mỗi nhóm. Các phương pháp kết hợp hoạt động tốt trên tất cả các tác vụ.

Lựa chọn Bộ tổng hợp Hình 3 so sánh hiệu suất của một số bộ tổng hợp thực thể trên các tác vụ hạ nguồn. Chúng tôi so sánh phương pháp cục bộ (ví dụ, LSE, NOR [26], NAND [19]) phương pháp toàn cục (ví dụ, Max, Average, Att [14]) và sự kết hợp của các phương pháp cục bộ và toàn cục (ví dụ, LSE + Att, LSE + NL). Các bộ tổng hợp trong mỗi phương pháp thể hiện sự biến đổi hiệu suất cao. Bộ tổng hợp cục bộ tốt nhất vượt trội hơn các bộ tổng hợp toàn cục tốt nhất mà chúng tôi khám phá trên tất cả các tác vụ hạ nguồn. Kết hợp các phương pháp cục bộ và toàn cục cho ra phương pháp hoạt động tốt nhất.

4.4 Hạn chế
Mặc dù hữu ích về mặt thực nghiệm, khung của chúng tôi không cung cấp bảo đảm lý thuyết về hiệu suất tác vụ hạ nguồn. Chúng tôi không khám phá những đặc tính nào của bộ tổng hợp quyết định hành vi chuyển giao của nó. Ngoài ra, phương pháp LSE + NL được đề xuất của chúng tôi nhạy cảm với giá trị của các tham số thang đo; việc tìm các siêu tham số tối ưu tự động là quan trọng cho việc mở rộng mô hình.

5 Kết luận
Trong bài báo này, chúng tôi đề xuất một khung để xây dựng các hàm điểm số hình ảnh-tài liệu bất biến hoán vị cho học tương phản đa phương thức. Lấy cảm hứng từ học đa thực thể, chúng tôi giới thiệu LSE + NL để học biểu diễn đa phương thức dựa trên cả các hàm điểm số cục bộ và toàn cục và khai thác sự tương quan giữa các vùng hình ảnh. Phương pháp của chúng tôi vượt trội hơn các phương pháp tiên tiến trong phân loại hình ảnh, định vị trực quan, và truy xuất đa phương thức. Ngoài ra, chúng tôi chỉ ra rằng học biểu diễn tương phản là một dạng học đa thực thể, cung cấp cho chúng tôi những hiểu biết có giá trị từ một lĩnh vực liên quan để giải quyết các thách thức chung nhằm học các biểu diễn tổng quát hóa tốt.

Lời cảm ơn Công việc được hỗ trợ bởi MIT JClinic, Philips, và Wistron.

--- TRANG 12 ---
12 P. Wang et al.
Tài liệu tham khảo
1. Amores, J.: Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence (Aug 2013)
2. Boecking, B., Usuyama, N., Bannur, S., Castro, D.C., Schwaighofer, A., Hyland, S., Wetscherek, M., Naumann, T., Nori, A., Alvarez-Valle, J., Poon, H., Oktay, O.: Making the Most of Text Semantics to Improve Biomedical Vision–Language Processing. In: ECCV (Oct 2022)
3. Carbonneau, M.A., Cheplygina, V., Granger, E., Gagnon, G.: Multiple Instance Learning: A Survey of Problem Characteristics and Applications. Pattern Recognition (May 2018)
4. Chauhan, G., Liao, R., Wells, W., Andreas, J., Wang, X., Berkowitz, S., Horng, S., Szolovits, P., Golland, P.: Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary Edema Assessment. In: MICCAI (Oct 2020)
5. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.: UNITER: UNiversal Image-TExt Representation Learning. In: ECCV (2020)
6. Desai, K., Johnson, J.: VirTex: Learning Visual Representations from Textual Annotations. In: CVPR (Jun 2021)
7. Fang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Dollar, P., Gao, J., He, X., Mitchell, M., Platt, J.C., Zitnick, C.L., Zweig, G.: From captions to visual concepts and back. In: CVPR (Jun 2015)
8. Foulds, J., Frank, E.: A review of multi-instance learning assumptions. The Knowledge Engineering Review (Mar 2010)
9. Gao, R., Feris, R., Grauman, K.: Learning to Separate Object Sounds by Watching Unlabeled Video. In: ECCV (Sep 2018)
10. Harwath, D., Recasens, A., Surís, D., Chuang, G., Torralba, A.: Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input. IJCV (Mar 2020)
11. He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: ICCV (Oct 2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition. In: CVPR (Jun 2016)
13. Huang, S.C., Shen, L., Lungren, M.P., Yeung, S.: GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-Efficient Medical Image Recognition. In: ICCV (Oct 2021)
14. Ilse, M., Tomczak, J., Welling, M.: Attention-based Deep Multiple Instance Learning. In: ICML (Jul 2018)
15. Johnson, A.E.W., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P., Deng, C.y., Mark, R.G., Horng, S.: MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci Data (Dec 2019)
16. Karpathy, A., Fei-Fei, L.: Deep Visual-Semantic Alignments for Generating Image Descriptions. TPAMI (Apr 2017)
17. Karpathy, A., Joulin, A., Fei-Fei, L.: Deep fragment embeddings for bidirectional image sentence mapping. In: NIPS (Dec 2014)
18. Kingma, D., Ba, J.: Adam: A Method for Stochastic Optimization. arXiv:1412.6980 (Dec 2014)
19. Kraus, O.Z., Ba, J.L., Frey, B.J.: Classifying and segmenting microscopy images with deep multiple instance learning. Bioinformatics (Jun 2016)
20. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked Cross Attention for Image-Text Matching. In: ECCV (Sep 2018)
21. Li, B., Li, Y., Eliceiri, K.W.: Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning. In: CVPR (Jun 2021)

--- TRANG 13 ---
Sử dụng Học Đa Thực Thể để Xây dựng Biểu diễn Đa phương thức 13
22. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: VisualBERT: A Simple and Performant Baseline for Vision and Language. arXiv:1908.0355 (Aug 2019)
23. Liao, R., Moyer, D., Cha, M., Quigley, K., Berkowitz, S., Horng, S., Golland, P., Wells, W.M.: Multimodal Representation Learning via Maximization of Local Mutual Information. In: MICCAI (Sep 2021)
24. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: ICLR (May 2019)
25. Lu, J., Batra, D., Parikh, D., Lee, S.: ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. In: NeurIPS (Dec 2019)
26. Maron, O., Lozano-Pérez, T.: A framework for multiple-instance learning. In: NIPS (Jul 1998)
27. Miech, A., Alayrac, J.B., Smaira, L., Laptev, I., Sivic, J., Zisserman, A.: End-to-End Learning of Visual Representations From Uncurated Instructional Videos. In: CVPR (Jun 2020)
28. Müller, P., Kaissis, G., Zou, C., Rueckert, D.: Joint Learning of Localized Representations from Medical Images and Reports. In: ECCV (Oct 2022)
29. van den Oord, A., Li, Y., Vinyals, O.: Representation Learning with Contrastive Predictive Coding. arXiv:1807.03748 (Jul 2018)
30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning Transferable Visual Models From Natural Language Supervision. In: ICML (Jul 2021)
31. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In: NIPS (Dec 2015)
32. Sadvilkar, N., Neumann, M.: PySBD: Pragmatic Sentence Boundary Disambiguation. In: NLP-OSS (Nov 2020)
33. Shih, G., Wu, C.C., Halabi, S.S., Kohli, M.D., Prevedello, L.M., Cook, T.S., Sharma, A., Amorosa, J.K., Arteaga, V., Galperin-Aizenberg, M., Gill, R.R., Godoy, M.C., Hobbs, S., Jeudy, J., Laroia, A., Shah, P.N., Vummidi, D., Yaddanapudi, K., Stein, A.: Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia. Radiol Artif Intell (Jan 2019)
34. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local Neural Networks. In: CVPR (Jun 2018)
35. Zhang, C., Platt, J., Viola, P.: Multiple Instance Boosting for Object Detection. In: NIPS (Dec 2005)
36. Zhang, Y., Jiang, H., Miura, Y., Manning, C.D., Langlotz, C.P.: Contrastive Learning of Medical Visual Representations from Paired Images and Text. In: MLHC (Aug 2022)
37. Zhou, Z.H., Zhang, M.L., Huang, S.J., Li, Y.F.: Multi-Instance Multi-Label Learning. Artificial Intelligence (Jan 2012)
