# 2305.19924.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2305.19924.pdf
# File size: 447365 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Joint Adaptive Representations for Image-Language Learning
AJ Piergiovanni Anelia Angelova
Google DeepMind
{ajpiergi, anelia }@google.com
Abstract
Image-language transformer models have achieved
tremendous success, but they come at high computational
costs. We here propose a joint adaptive image-language
representation learning, which adaptively and iteratively
fuses the multi-modal features. This consistently reduces
the model cost and size, allows the model to scale without a
large increase in FLOPs or memory, and outperforms big-
ger and much more expensive models. With only 40M train-
ing examples and with 39 GFLOPs our model outperforms
many times larger models, some reaching 800 GFLOPs.
1. Introduction
Vision-and-language learning has made great strides re-
cently [5, 13, 13, 14, 24, 26, 27, 29, 32, 33]. These models
can attribute their success to scaling the well known Trans-
former models [25], which in turn need very large datasets.
One important component of these models is building the
underlying joint visuo-lingual representation which cap-
tures the relations between the modalities [5, 5, 7, 9‚Äì12,
12, 14, 14, 17, 19, 20, 22, 24, 24, 29, 31, 33, 33]. However,
expensive attention mechanisms are applied within Trans-
formers, in which the compute required grows quadratically
with the increase of the input sizes; further, these models
perform better with significantly more data [6] and train-
ing steps to learn the joint representations; and lastly, since
large datasets are hard to collect, automatically collected
datasets contain large amounts of noise. All this makes
these models even more ineffective and expensive to train:
scaling the models, combined with the corresponding data
scaling required, and training with large amounts of noise,
require large amounts of compute. Thus, it is desirable to
construct more memory-, FLOPs- and data- efficient vision-
language representations where one can take advantage of
model scale but in a more effective way.
To that end, we propose the Joint Adaptive Representa-
tion for efficient image-language learning (Figure 1). Our
approach first reduces the number of tokens in the input
modalities, then adaptively fuses them. This process greatly
Figure 1. GFLOPs vs. accuracy for several models. The proposed
approach enables much more efficient scaling, and achieves excel-
lent performance for fewer FLOPs. It outperforms SimVLM-huge
on VQA2.0 dataset, even though it is much larger and our model
is evaluated in the open-vocabulary setting.
reduces FLOPs, while maintaining or improving perfor-
mance. It results in a more compact and efficient representa-
tions, obtaining 33% fewer FLOPs than the commonly used
concatenation, while improving performance. This leads to
more data- and compute- efficient models.
We evaluate the approach on Visual Question Answer-
ing (VQA) tasks, where the joint understanding of the im-
age in the context of language input is important. Our
model performs competitively with respect to the state-
of-the-art (SOTA) models, outperforming even models of
large parameter and data scale (Fig. 1). Prior approaches,
Perceiver [9], Co-Tokenization [19] also proposed efficient
joint vision-language learning methods, our approach pro-
poses a better mechanism of ‚Äòupdating‚Äô the information be-
tween modalities and fusing their features, surpassing these
two approaches both in accuracy and in reducing FLOPs.
Our approach allows for better model scaling, using much
fewer FLOPs with increasing model sizes and input image
sizes (Fig. 2). The main contribution of our work is a new
image-text fusion method that is more efficient and accurate
than previous methods. This allows us to present a novel
compact image-language model of excellent performance,
obtained at the fraction of the cost and data.
2. Joint Adaptive Representations
The key question we address is how to combine the fea-
tures from vision and language input modalities. A few ba-arXiv:2305.19924v2  [cs.CV]  1 Jun 2023

--- PAGE 2 ---
Figure 2. FLOPs scaling with image size, model size, and model
depth (i.e., number of layers). Blue (top curve) is concatenation,
red is our Joint Adaptive Representation. As seen, our approach
scales more gracefully for all of them.
sic approaches use either: (1) concatenation or (2) cross-
attention. A key issue with concatenation is that it greatly
increases the number of tokens by adding H‚àóWto the
text length ( H,Ware the height and width of the image
features). Thus, as the image size increases, concatenation
greatly increases the FLOPs and memory requirements of
the model (Fig. 2), e.g., [5, 7, 12, 14, 15, 23, 29]. Here, we
propose a method to reduce the number of tokens, improv-
ing efficiency. Cross-attention based methods have other
issues, mainly that the modality used for the query (usually
text, e.g., ALBEF [11], BLIP [10]), determines the size of
the output representation. Often for vision-language tasks,
the visual features have many tokens (for example, the vi-
sual tokens are 14x14 = 196 for a modest image input size
of 224x224), while text is fairly short, e.g., 10 tokens in
VQA2.0. When using cross-attention, the entire visual in-
put must be squeezed into these few text token representa-
tions, greatly constraining the amount of visual information
that can be used. While this approach has fewer FLOPs
than concatenation, it loses information, which can reduce
task performance, and puts a dependence on the input text
length. Naturally, this cross-modal representation will have
even less utility when increasing the input image size.
Instead, we here propose a module that enables better
learning of vision-language features by more effectively in-
corporating the visual information and fusing it with the text
information. By adaptively and iteratively tokenizing the
inputs, the model is able to refine the feature representation
learned from both modalities in the training process, while
keeping a reasonable number of FLOPs (Fig. 3).
Our approach is based on several insights. First, we
query the image to obtain more informative visual tokens.
Previously, this was done using a TokenLearner-like ap-
proach [19, 21]. However, this method, while reducing
FLOPs, notably for video applications in [19], still uses
quite a few FLOPs to generate and apply the attention maps,
and does not scale well with image size. Instead, we uti-
lize a hybrid approach inspired by Perceiver [9]. We gen-
erate Ntokens independently from each modality as a first
step. Secondly, we then use a direct cross-attention mech-
anism between the new text and compact visual features to
produce a better cross-modal representation. This mecha-
nism consists of a cross-attention layer, then a self-attention
Text Feature Image Feature Self-Attention MLP 
(T+H*W) x D 
Text Feature Image Feature Cross-Attention MLP 
Self-Attention 
T x D 
Text Feature Image Feature Cross-Attention MLP 
Self-Attention L
Cross-Attention 
Latent NxD Latent NxD tNfNCross-Attention ùù∞MLP ùõÉ
Text Feature Image Feature Cross-Attention Cross-Attention 
Latent NxD Latent NxD tNfN(a) Concatenation, which 
creates long sequence lengths 
L(b) Cross-attention, which squeezes the 
image features into the text features. 
(c) Perceiver-style cross attention, applied per-modality and iterated L times. This 
requires computing the cross attention over all the inputs L times. (d) Our approach only uses the input cross-attention once and uses gates to 
update the features. FiFigure 3. Visualization of the Joint Adaptive Representation (d) in
the context of other approaches.
layer, and a Multi-Layer Perceptron (MLP), similar to a
standard Transformer layer [25], but due to the reduced to-
kens, is much more lightweight.
Finally, this process is done iteratively, thus refining the
current representation based on the set of features from the
Transformer. This allows the model to dynamically update
and select different visual and text features at each step so
it is best able to perform the task, without increasing the
compute cost. Our approach is described in detail below.
LetXtext andXimbe the inputs for text and for im-
ages, respectively. More specifically Xtext‚ààRL√óDand
Xim‚ààRH√óW√óC, assuming the visual input is of size
W√óH, the text is of length L. The goal is to produce
new, lower dimensional feature representations. This can
be done by reducing the representation to a lower number
of tokens, which is particularly important for the visual fea-
tures as they are many more. This is done by first unifying
the representation dimensions, more specifically projecting
the visual features to the H‚àóW√óDspace, where Dis the
feature dimensions for the text input:
P(Xim) =W1Xim, (1)
where P(Xim)‚ààRH‚àóW√óD. Here, by W1we denote the
learnable operation, e.g., applying a fully-connected layer,
which projects the image features into the D-dimensional
space. In principle both the visual input and the text in-
put can be projected to a new feature dimension e.g., thus
not having to be necessarily dependent on the input feature
dimension, however Eq. 1 is used here for simplicity. In
principle both the visual input and the text input can be pro-
jected to a new feature dimension e.g., D‚Ä≤, thus not having
to be necessarily dependent on the input feature dimension.
As a second step, we proceed to learn a set of new N
learnable tokens XN‚ààRN√óD, which is done in a DETR-
style [4] feature learning. That is, XNis a randomly ini-
tialized representation that is learned via back-propagation
jointly with the other parameters to minimize the loss.
fN=W2Œ¶(XN, P(Xim)). (2)
Here P(Xim)represents the projection of visual features
from Eq. 1, XNis the learned latent features, Œ¶is the stan-
dard multi-head attention operation. This results in fN, the

--- PAGE 3 ---
compact intermediate representations with Nfeatures. This
can also be viewed as learning Nnew tokens, which repre-
sent the input of Mtokens, where N‚â™M, for the large
visual input M=H‚àóW. We note that this is similar to the
Perceiver architecture [9], albeit it is done only once here.
This process is also done to Xtext, resulting in Ntext fea-
tures ( tN). Thus, unlike prior work (e.g., [10,11]), Nis not
required to be tied to the input text length; so a richer, but
more compact representation is built.
Next, for the two inputs tN,fNwe learn a new joint
feature representation F(tN, fN)via cross attention. Im-
portantly, we note that both these inputs will influence the
subsequent representation to create a cross-modal version
of text and image features. In the co-tokenization ap-
proach [19], the two modalities are also fused for better
learning, but here with two key differences: 1) the initial
token reduction is not done at each iteration, which is com-
putationally intensive; and 2) ours uses a lightweight cross-
attention compared to the co-tokenization approach.
This process uses the following components. We first
use LayerNorm [3] (denoted as Ln) in order to normalize
the features. We then compute cross-attention between tN
(text features) and fN(image features). The idea is that they
will help construct a representation which is a combination
of these modalities. We then use a standard Transformer
layer with self-attention and MLPs to compute the features.
Pcr(tN, fN) =Ln(tN) +tanh (Œ±)Œ¶(Ln(tN), Ln(fN))
F(tN, fN) =Pcr(tN, fN) +tanh (Œ≤)MLP (Pcr(tN, fN))
(3)
where Œ±andŒ≤are learnable parameters that control how the
text and vision features are fused ( Œ¶is the standard multi-
head attention operation). We note that here, throughout,
Pcross(tN, fN)‚ààRN√óD, i.e., is a compact representa-
tion which combines the two modalities. We also add the
tanh gating mechanism, which we find to be advantageous
in our ablation experiments. The resultant representation
F(tN, fN)‚ààRN√óDis then fed to a transformer to pro-
duce a transformed intermediate representation of the same
dimension F=T(F(tN, fN))‚ààRN√óD. We use a stan-
dard transformer layer ( T) with multi-headed attention [25].
This new feature representation can be further refined to
produce even better cross-modal learning by repeating the
same process, but this time taking the already obtained fea-
ture as input. The operation is the same as Eq. 3 but with
continually updated input by replacing tNwithF+tN,
which adds in the output of the previous Transformer layer.
This lets the model continually refine and fuse the features.
Assuming Fiis the current representation and Fi+1is the
next, this uses the previous equations to iteratively updateGFs Data GQA SNLI VQA2
Large-data Models
Flamingo [2] - 2.3B+ - - 82.0
SimVLM [29] 890‚àó1.8B - 86.21 80.03
GIT [28] - 800M - - 78.81
METER [7] 130‚àó404M - 80.86 77.68
BLIP-L [10] 250‚àó129M - - 78.25
Small-data Models
FLA V A [22] 70‚àó70M - 78.9 72.5
CFR [16] - - 73.6 - 69.8
VinVL [33] - 16M 65.05 - 75.95
BLIP [10] 122‚àó14M - - 77.54
ALBEF [11] 165‚àó14M - 80.14 74.54
12-in-1 [15] - - 60.5 - 71.3
UNITER [5] - 10M - 79.39 72.5
LXMERT [24] - 6.5M 60.0 - 69.9
Ours-Base 38.9 40M 81.9 82.1 79.20
Ours 54.5 40M 83.1 84.2 80.15
Table 1. We outperform or perform competitively to the state-of-
the-art models, despite using very few GFLOPs(GFs) and small
amounts of data. In fact with 40M training examples and with 39
GFLOPs our small model (350M params) outperforms all methods
that have used ‚àºBillion examples for pre-training. Models such
as ALBEF and BLIP use smaller data but use have many more
FLOPs. Open-vocabulary evaluation.‚àóOur calculation of FLOPs.
GFLOPs GQA SNLI-VE
Perceiver [9] 40.3 78.2 77.4
CoTokenization [19] 43.8 78.5 77.5
Ours 38.9 79.1 77.9
Table 2. Comparison to the Perceiver [9] method and to the Iter-
ative Co-tokenization [19] approach for image+text fusion. Both
are our implementations. Base Model.
GF GQA SNLI-VE
Concat (Baseline) 58.4 78.9 77.4
Ours (no Gating) 38.9 78.5 77.2
Ours 38.9 79.1 77.9
Table 3. Comparison to the concatenation baseline: our approach
is more accurate and reduces FLOPs 1.5x. This has larger impli-
cations as most vision-language models are concatenation based.
the features as follows:
Pcr(Fi+tN, fN) =Ln(Fi+tN)+
tanh (Œ±)Œ¶(Ln(Fi+tN), Ln(fN))
Fi+1=Pcr(Fi+tN, fN)+
tanh (Œ≤)MLP (Pcross(Fi+tN, fN)
Fi+1=T(Fi+1)(4)

--- PAGE 4 ---
GF GQA SNLI-VE
134.2 78.3 77.1
235.5 78.8 77.6
438.9 79.1 77.9
842.5 79.2 77.6
(a)Number of Iterations
used to compute tokens.GF GQA SNLI
16 18.5 76.5 75.8
32 28.4 78.3 76.8
64 38.9 79.1 77.9
128 72.9 79.2 78.1
(b)Number of Tokens
used in the model.GF GQA SNLI
Spatial 42.5 78.9 77.4
Latent 38.9 79.1 77.9
(c)Resampling Method
Latent cross-attention is
better.GF GQA SNLI
None 38.9 78.1 76.5
Residual 38.9 78.7 77.6
Weighted 38.9 79.1 77.9
(d)Iterative Combina-
tion of features after each
iteration.GF GQA SNLI
822.4 76.7 74.2
1630.5 78.3 75.4
3238.9 79.1 77.9
(e)Number Layers used
in the fusion module.
Table 4. Ablation studies exploring variants of our proposed approach.
Text is used at the first iteration, joint features afterwards.
Of key importance is that during the cross-modal learn-
ing process, we use the interaction of both modalities.
Specifically, we use attention to determine lower dimen-
sional projections from both modalities which differs both
from the Transformer [25] which preserves the input dimen-
sionalities, and is a more efficient process than the Itera-
tive Co-Tokenization [19] and Perceiver [9], also used by
Flamingo [2], as the expensive tokenization step over the
whole input is only done once here. Further, different from
Flamingo are the iterative updates, Eq. 4, where we iter-
atively combine the features, rather than relying only on
cross-attention. The approach is also different from meth-
ods like TokenLearner which is only applied on a single
input, which can lead to a loss in accuracy if not placed
appropriately [21]. It is also different from cross-attention
methods [7, 10, 11] due to the initial feature learning and
iterative updating of the cross-modal information (Eq. 2).
This approach also offers better performance than the con-
catenation baselines while using at least 33% fewer FLOPs.
Pre-training. We find that a mixture of a number of
cross-modal tasks [7, 11, 18] is more beneficial for pre-
training of our vision-language model. Inspired by cur-
riculum learning, we adaptively change the mixture ratios
between the tasks during pre-training (please see the supp
material for a full list of tasks and detailed description).
3. Experiments
We evaluate our approach on three VQA datasets
VQA2.0 [1],GQA [8], and Visual Entailment ( SNLI-VE
[30]) where we follow the standard accuracy metrics. Our
model uses the open-ended generated text which a more
challenging scenario to many previous works who used
fixed (3K) vocabulary and a classification setting. Table 1
shows the comparison with the state-of-the-art (SOTA) ap-
proaches. We see that our method performs competitively
or outperforms prior models. Of note is that both our base
and our larger model are actually the lowest FLOPs among
contemporary models and outperforming models with many
more FLOPs (Our models use 2-20x fewer FLOPs ). Our
small model (300M params) outperforms all SOTA ap-
proaches with the exception of extremely large models,
Flamingo, SimVLM, both of which pre-train on very largedatasets. Our main model further outperforms SimVLM on
VQA2.0. Comparing to contemporary methods in terms of
GFLOPs, our approach takes 38.9-54.5 GFLOPs, which is
much smaller than others, e.g. ALBEF [11] of about 165,
or BLIP ranging from 120 to 250, and much smaller than
SimVLM which is close to 900 GFLOPs. While FLOPs is
an imperfect measure, it is preferred due to differences in
implementations and hardware used by other methods. Our
method reduces memory by 40% , memory was reduced
from 15GB of the concat baseline to 9GB for ours.
Joint image-language learning comparison. In Ta-
ble 2, we compare side-by-side our approach to other effi-
cient image-language representation learning methods: Per-
ceiver [9] and Iterative Co-Tokenization [19]. Our approach
outperforms these advanced fusion methods, while using
fewer FLOPs (Table 2). It also scales much better than them
with an increase of the input image size (Fig 4).
Figure 4. Scaling with different in-
put sizes. With weighted iterative
updates, ours scales better.Ablation studies
In Table 3, we com-
pare to the concatena-
tion baseline, which is
most commonly used
[5,7,12,14,15,23,29].
Our approach im-
proves performance
and reduces compute
by 33% reduction
i.e. using 1.5x fewer
FLOPs. Fig. 2 further shows that our approach is much
more advantageous for increasing the input sizes, or model
scaling, and scales better than compared to the concatena-
tion approaches. We conduct detailed ablations to study the
proposed approach. For each experiment, we modify one
component of our main approach to verify its independent
impact (‚Äògray‚Äô is the main approach). Table 4 (a) and (b)
provide an ablation on iteration steps and the number of
tokens learned, showing a trade-off of more compute vs
higher accuracy, but with diminishing returns. Table 4 (c)
illustrates that a single, latent cross-attention resampling of
our approach gives both better performance and uses fewer
FLOPs. This is in contrast to a spatial resampling used in
prior works [19, 21]. Table 4 (d), (e) study the proposed
weighting (Eq. 4), and number of layers.

--- PAGE 5 ---
References
[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret
Mitchell, C. Lawrence Zitnick, Dhruv Batra, and Devi
Parikh. Vqa: Visual question answering. In ICCV , 2015.
4
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning,
2022. 3, 4
[3] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization. In CoRR abs/1607.06450 , 2016. 3
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In ECCV , 2020. 2
[5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:
Universal image-text representation learning. In ECCV ,
2020. 1, 2, 3, 4
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In 9th International Conference on Learning Repre-
sentations, ICLR 2021, , 2021. 1
[7] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang
Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu
Yuan, Nanyun Peng, et al. An empirical study of training
end-to-end vision-and-language transformers. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 18166‚Äì18176, 2022. 1, 2, 3, 4
[8] Drew A Hudson and Christopher D Manning. Gqa: a new
dataset for compositional question answering over realworld
images. In CVPR , 2019. 4
[9] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zis-
serman, Oriol Vinyals, and Joao Carreira. Perceiver: General
perception with iterative attention. In ICML , 2021. 1, 2, 3, 4
[10] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. arXiv
preprint arXiv:2201.12086 , 2022. 1, 2, 3, 4
[11] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh D. Gotmare,
Shafiq Joty, Caiming Xiong, and Steven C.H. Hoi. Align be-
fore fuse: Vision and language representation learning with
momentum distillation. In NeurIPS , 2021. 1, 2, 3, 4
[12] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. Visualbert: A simple and performant
baseline for vision and language. 2019. 1, 2, 4
[13] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semanticsaligned pre-training for vision-language tasks. In ECCV ,
2020. 1
[14] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. In CVPR , 2019. 1, 2, 4
[15] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi
Parikh, and Stefan Lee. 12-in-1: Multi-task vision and lan-
guage representation learning. In CVPR , 2020. 2, 3, 4
[16] Binh X. Nguyen, Tuong Do, Huy Tran, Erman Tjiputra,
Quang D, and Anh Nguyen Tran. Coarse-to-fine reasoning
for visual question answering. In CVPR MULA Workshop ,
2022. 3
[17] Duy-Kien Nguyen and Takayuki Okatani. Improved fusion
of visual and language representations by dense symmetric
co-attention for visual question answering. In CVPR , 2018.
1
[18] AJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Pre-
training image-language transformers for open-vocabulary
tasks. In T4V: Transformers for Vision Workshop, Confer-
ence on Computer Vision and Pattern Recognition , 2022. 4
[19] AJ Piergiovanni, Kairo Morton, Weicheng Kuo, Michael
Ryoo, and Anelia Angelova. Video question answering with
iterative video-text co-tokenization. ECCV , 2022. 1, 2, 3, 4
[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning ,
pages 8748‚Äì8763. PMLR, 2021. 1
[21] Michael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa
Dehghani, and Anelia Angelova. Tokenlearner: Adaptive
space-time tokenization for videos. 2021. 2, 4
[22] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami abd
Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach,
and Douwe Kiela. Flava: A foundational language and
vision alignment model. In arxiv.org/pdf/2112.04482.pdf ,
2022. 1, 3
[23] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-
linguistic representations. arXiv preprint arXiv:1908.08530 ,
2019. 2, 4
[24] Hao Tan and Mohit Bansal. Lxmert: Learning cross-
modality encoder representations from transformers. In
EMNLP , 2019. 1, 3
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 1,
2, 3, 4
[26] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Lu-
owei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang
Jiang, and Lu Yuan. Omnivl: One foundation model for
image-language and video-language tasks. 2022. 1
[27] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language, 2022. 1

--- PAGE 6 ---
[28] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100 , 2022. 3
[29] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision, 2021. 1, 2, 3, 4
[30] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual
entailment: A novel task for fine-grained image understand-
ing. In https://arxiv.org/abs/1901.06706 , 2019. 4
[31] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. ICLR , 2022. 1
[32] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
Pengchuan Zhang. Florence: A new foundation model for
computer vision. In arxiv.org/pdf/2110.02095.pdf , 2022. 1
[33] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao.
Vinvl: Revisiting visual representations in vision-language
models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 5579‚Äì
5588, 2021. 1, 3
