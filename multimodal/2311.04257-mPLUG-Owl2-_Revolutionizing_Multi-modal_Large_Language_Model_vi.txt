# 2311.04257.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.04257.pdf
# Kích thước tệp: 2604446 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
mPLUG-Owl2: Cách mạng hóa Mô hình Ngôn ngữ Lớn Đa phương thức
với Sự hợp tác Phương thức
Qinghao Ye∗Haiyang Xu∗Jiabo Ye∗Ming Yan†Anwen Hu
Haowei Liu Qi Qian Ji Zhang Fei Huang Jingren Zhou
Alibaba Group
{yeqinghao.yqh, shuofeng.xhy, yejiabo.yjb, ym119608}@alibaba-inc.com
Code & Demo & Models: https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2

Tóm tắt
Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) đã thể hiện khả năng hiểu lệnh ấn tượng trong nhiều nhiệm vụ mở. Tuy nhiên, các phương pháp trước đây chủ yếu tập trung vào việc nâng cao khả năng đa phương thức. Trong công trình này, chúng tôi giới thiệu một mô hình ngôn ngữ lớn đa phương thức linh hoạt, mPLUG-Owl2, có thể tận dụng hiệu quả sự hợp tác phương thức để cải thiện hiệu suất trong cả nhiệm vụ văn bản và đa phương thức. mPLUG-Owl2 sử dụng thiết kế mạng module hóa, với bộ giải mã ngôn ngữ hoạt động như một giao diện chung để quản lý các phương thức khác nhau. Cụ thể, mPLUG-Owl2 kết hợp các module chức năng chia sẻ để tạo điều kiện cho sự hợp tác phương thức và giới thiệu một module thích ứng phương thức để bảo tồn các đặc trưng riêng biệt của phương thức. Các thí nghiệm mở rộng cho thấy mPLUG-Owl2 có khả năng tổng quát hóa cả nhiệm vụ văn bản và đa phương thức và đạt được hiệu suất tốt nhất với một mô hình chung duy nhất. Đáng chú ý, mPLUG-Owl2 là mô hình MLLM đầu tiên thể hiện hiện tượng hợp tác phương thức trong cả kịch bản văn bản thuần và đa phương thức, thiết lập một con đường tiên phong trong phát triển các mô hình nền tảng đa phương thức tương lai.

1. Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) như GPT-3[6], LLaMA [57, 58], và GPT-4 [46] đã thu hút sự chú ý đáng kể do khả năng tổng quát hóa đặc biệt trong hiểu và tạo văn bản. Để tạo điều kiện cho các ứng dụng thị giác-ngôn ngữ, GPT-4V¹[45] gần đây đã thể hiện khả năng đa phương thức ấn tượng trong các nhiệm vụ đa dạng, ví dụ như mô tả, trả lời câu hỏi, v.v., khơi dậy sự quan tâm của các nhà nghiên cứu về tiềm năng hội tụ của lĩnh vực thị giác-ngôn ngữ. Điều này đã dẫn đến sự xuất hiện của một

∗Đóng góp bằng nhau
†Tác giả liên hệ
¹https://openai.com/research/gpt-4v-system-card

•Hiểu Lệnh Văn bản •Hiểu Khái niệm Thị giác (a)(b)Nhiễu Phương thức •Hiểu Lệnh Văn bản •Hiểu Khái niệm Thị giác 
Hợp tác Phương thức 
BLIP-2, MiniGPT-4, LLaVA, v.v.(Bộ giải mã Ngôn ngữ Vanilla)mPLUG-Owl2(Bộ giải mã Ngôn ngữ Thích ứng Phương thức)Mạng Feed ForwardModule AttentionMạng Feed ForwardModule Thích ứng Phương thức
111000000

Hình 1. So sánh hiệu suất tổng thể giữa mPLUG-Owl2 và các MLLM hiện có và sự khác biệt giữa các MLLM hiện có và mô hình được đề xuất của chúng tôi. (a) Các phương pháp trước đây sử dụng bộ giải mã ngôn ngữ chuẩn (tức LLM) để quản lý các loại lệnh khác nhau, dẫn đến nhiễu phương thức và giảm hiệu suất. (b) Chúng tôi giới thiệu mPLUG-Owl2, sử dụng bộ giải mã ngôn ngữ thích ứng phương thức để xử lý các phương thức khác nhau trong các module riêng biệt trong khi chia sẻ một số tham số cho sự hợp tác phương thức. Phương pháp này giảm thiểu vấn đề nhiễu phương thức.

1arXiv:2311.04257v2  [cs.CL]  9 Nov 2023

--- TRANG 2 ---
nhóm Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) [5, 15, 31, 38, 65, 66, 68, 75], nhằm tăng cường LLM với khả năng hiểu và xử lý các vấn đề thị giác.

Các nghiên cứu trước đây [27, 63] trong học đa phương thức gợi ý rằng các phương thức khác nhau có thể hợp tác hiệu quả, từ đó nâng cao hiệu suất của cả nhiệm vụ văn bản và đa phương thức đồng thời. Tuy nhiên, MLLM là một mô hình thống nhất hỗ trợ các phương thức và nhiệm vụ khác nhau mà không cần điều chỉnh cụ thể cho từng nhiệm vụ. Các công trình gần đây sử dụng các module căn chỉnh chéo phương thức (ví dụ: Q-former [15, 31, 75] và lớp tuyến tính [10, 38]) để ánh xạ các đặc trưng thị giác từ bộ mã hóa thị giác vào LLM đóng băng để thực hiện các nhiệm vụ đa phương thức bằng cách tận dụng khả năng ngôn ngữ được bảo tồn. Chiến lược này, thật không may, hạn chế tiềm năng hợp tác phương thức. Kết quả là, một số nhà nghiên cứu [38, 68] chọn điều chỉnh LLM trong quá trình huấn luyện lệnh đa phương thức. Mặc dù điều chỉnh cải thiện đáng kể các nhiệm vụ đa phương thức, nhưng có nguy cơ làm suy yếu hiệu suất nhiệm vụ văn bản [16]. Như được minh họa trong Hình 1, thách thức của hợp tác phương thức trong MLLM xuất phát từ việc áp dụng một module duy nhất để cân bằng lợi ích của hợp tác phương thức và nhiễu phương thức, nơi các phương thức có thể can thiệp lẫn nhau trên một số lượng lớn bộ dữ liệu lệnh qua nhiều phương thức.

Để giảm thiểu thách thức này, chúng tôi trình bày một mô hình nền tảng đa phương thức đa mục đích mới, mPLUG-Owl2, trong công trình này. Mô hình của chúng tôi có thiết kế mạng module hóa tính đến cả hợp tác phương thức và nhiễu phương thức, sử dụng bộ giải mã ngôn ngữ như một giao diện chung để quản lý tín hiệu đa phương thức. Cụ thể, mPLUG-Owl2 kết hợp các module chức năng chia sẻ nhất định để thúc đẩy hợp tác phương thức và giới thiệu một module thích ứng phương thức hoạt động như một trục xoay qua các phương thức khác nhau. Do đó, các phương thức thị giác và ngôn ngữ được chiếu vào một không gian ngữ nghĩa chia sẻ để tương tác chéo phương thức, trong khi module được đề xuất giúp bảo tồn các đặc trưng riêng biệt của phương thức. Với kiến trúc mới của chúng tôi, các phương thức với mật độ thông tin khác nhau được bảo vệ khỏi nhiễu phương thức do module thích ứng phương thức và có thể hợp tác hiệu quả trong việc nắm bắt thông tin chia sẻ. Hơn nữa, chúng tôi giới thiệu một mô hình huấn luyện hai giai đoạn sáng tạo bao gồm tiền huấn luyện thị giác-ngôn ngữ và điều chỉnh lệnh thị giác-ngôn ngữ kết hợp. Mô hình này huấn luyện bộ mã hóa thị giác qua hai giai đoạn, cho phép nó nắm bắt cả thông tin thị giác ngữ nghĩa cấp thấp và cấp cao hiệu quả hơn.

Các thí nghiệm mở rộng minh họa hiệu quả và khả năng tổng quát hóa của mPLUG-Owl2, đạt được hiệu suất tốt nhất trên 8 điểm chuẩn thị giác-ngôn ngữ cổ điển sử dụng một mô hình chung duy nhất. Hơn nữa, nó đạt thứ nhất hoặc thứ hai về hiệu suất trên 5 điểm chuẩn đa phương thức zero-shot gần đây, nhấn mạnh khả năng thích ứng và thành thạo trong hiểu và tạo lệnh đa phương thức. Ngoài hiệu suất tiên tiến trong các nhiệm vụ đa phương thức, mPLUG-Owl2 cũng đạt được kết quả tốt nhất trên nhiều điểm chuẩn văn bản thuần. Hơn nữa, chúng tôi cung cấp phân tích sâu để chứng minh và xác nhận tác động của hợp tác phương thức thông qua module thích ứng phương thức được đề xuất, đặc biệt trong việc nâng cao các nhiệm vụ văn bản, bao gồm hiểu, kiến thức và lý luận. Cuối cùng, các nghiên cứu triệt để xác nhận hiệu quả của mô hình huấn luyện MLLM được đề xuất, có thể giúp truyền cảm hứng cho việc phát triển các mô hình nền tảng đa phương thức tương lai.

2. Công trình Liên quan
Các Mô hình Nền tảng Ngôn ngữ Lớn Đa phương thức. Việc ứng dụng thành công của các Mô hình Ngôn ngữ Lớn (LLM) đã mở đường cho việc phát triển một số phương pháp nhằm tăng cường khả năng nhận thức của LLM với các phương thức bổ sung, tất cả trong một mô hình thống nhất. Có ba phương pháp chính để xây dựng các mô hình nền tảng ngôn ngữ lớn đa phương thức, mỗi phương pháp đều cho thấy tiềm năng cho khả năng tổng quát hóa zero-shot mạnh mẽ trong lĩnh vực thị giác-ngôn ngữ. Ví dụ, Flamingo [2] là một tiền bối trong lĩnh vực này, sử dụng bộ mã hóa thị giác đóng băng và một mô hình ngôn ngữ lớn được trang bị attention chéo có cổng để căn chỉnh chéo phương thức. Ngược lại, PaLM-E [16] tích hợp các đặc trưng thị giác được trích xuất trực tiếp thông qua các lớp tuyến tính vào mô hình PaLM [12] được tiền huấn luyện, có 520 tỷ tham số, từ đó dẫn đến hiệu suất mạnh mẽ trong nhiều ứng dụng thực tế. Phương pháp này đã được áp dụng rộng rãi bởi các mô hình như LLaVA [38], Shikra [10], v.v. Tuy nhiên, một hạn chế đáng kể của phương pháp này là việc tạo ra các chuỗi thị giác dài. Để giải quyết vấn đề này, BLIP-2 [31], lấy cảm hứng từ DETR [8], đã phát triển một Q-former để giảm độ dài chuỗi của các đặc trưng thị giác một cách hiệu quả. Thiết kế này đã được phản ánh bởi Kosmos-1 [23], mPLUG-Owl [68], và MiniGPT-4 [75]. Tuy nhiên, cần lưu ý rằng những phương pháp này trực tiếp căn chỉnh các đặc trưng thị giác với LLM, coi tín hiệu thị giác và ngôn ngữ là tương đương, từ đó bỏ qua các đặc tính hạt nhỏ độc đáo giữa các phương thức thị giác và ngôn ngữ. Để giảm thiểu vấn đề này, chúng tôi giới thiệu module thích ứng phương thức. Mô hình được đề xuất của chúng tôi dẫn đến hiệu suất vượt trội trong cả cài đặt đánh giá zero-shot và điều chỉnh về cả hình ảnh và video.

Điều chỉnh Lệnh với MLLM. Điều chỉnh lệnh tối ưu hóa các mô hình ngôn ngữ lớn được tiền huấn luyện để hiểu và tuân thủ các lệnh tự nhiên, từ đó nâng cao khả năng tổng quát hóa các nhiệm vụ chưa thấy theo cách zero-shot. Các nhà nghiên cứu thường sử dụng các mô hình như ChatGPT và GPT-4 [46] để tạo ra các bộ dữ liệu lệnh đa dạng và mở rộng, bao gồm những bộ như Alpaca [56], ShareGPT [1], và WizardLM [61]. Khi các mô hình ngôn ngữ lớn đa phương thức xuất hiện, cộng đồng nghiên cứu bắt đầu tạo ra các bộ dữ liệu đa phương thức chất lượng cao, đa dạng. Ví dụ, MiniGPT-4 [75] sử dụng GPT-3.5 để diễn giải lại các chú thích được tạo bởi các mô hình được tiền huấn luyện. Đồng thời, LLaVA [38], SVIT [72], và LRV-Instruction [36] tận dụng các chú thích hình ảnh, như khung bao của đối tượng, chú thích hình ảnh, và mô tả vùng, để nhắc GPT-4 tạo ra các lệnh và phản hồi sử dụng phương pháp tự hướng dẫn. Các mô hình như mPLUG-Owl [68] và LLaVA-1.5 [37] tiếp tục phát triển lĩnh vực này bằng cách trải qua huấn luyện kết hợp với dữ liệu lệnh chỉ ngôn ngữ và thị giác-ngôn ngữ, từ đó giảm thiểu nguy cơ quên thảm khốc về kiến thức ngôn ngữ. Thay vì chỉ ngăn chặn hiện tượng quên thảm khốc này, mPLUG-Owl2, với sự trợ giúp của module thích ứng phương thức, có thể thu được lợi ích từ nỗ lực hợp tác của các phương thức bằng cách được huấn luyện kết hợp với dữ liệu lệnh chỉ ngôn ngữ và đa phương thức, từ đó nâng cao cả hiệu suất đa phương thức và chỉ ngôn ngữ.

3. Phương pháp
3.1. Tổng quan
Hình 2 (a) phác thảo tổng quan về mPLUG-Owl2. Cụ thể, mô hình của chúng tôi bao gồm bộ mã hóa thị giác, bộ trừu tượng hóa thị giác, lớp nhúng văn bản, và bộ giải mã ngôn ngữ. Đáng chú ý, việc triển khai chuẩn của lớp nhúng văn bản và bộ giải mã ngôn ngữ liên quan đến việc sử dụng một mô hình ngôn ngữ lớn, như GPT [6] hoặc LLaMA [57]. Trước tiên, chúng tôi giới thiệu ngắn gọn kiến trúc mô hình của chúng tôi trong Phần 3.2. Hơn nữa, chúng tôi xử lý các loại phương thức khác nhau bằng cách giới thiệu module thích ứng phương thức trong Phần 3.3. Cuối cùng, chúng tôi giới thiệu mô hình huấn luyện để huấn luyện mPLUG-Owl2 với hợp tác phương thức trong Phần 3.4.

3.2. Kiến trúc Mô hình
Như được mô tả trong Hình 2, mô hình của chúng tôi, được gọi là mPLUG-Owl2, bao gồm ba thành phần chính: bộ mã hóa thị giác cơ bản [48], bộ trừu tượng hóa thị giác, và bộ giải mã ngôn ngữ. Cụ thể, chúng tôi sử dụng ViT-L/14 làm bộ mã hóa thị giác và LLaMA-2-7B [58] làm bộ giải mã ngôn ngữ. Bộ mã hóa thị giác xử lý một hình ảnh đầu vào có độ phân giải H×W và tạo ra một chuỗi H/14×W/14 token. Các đặc trưng token thị giác này sau đó được kết hợp với nhúng token văn bản và đưa vào bộ giải mã ngôn ngữ hoạt động như một giao diện chung chuyển đổi các nhiệm vụ thị giác-ngôn ngữ khác nhau thành các nhiệm vụ tạo văn bản. Tuy nhiên, với việc tăng độ phân giải hình ảnh, các chuỗi token thị giác được mã hóa có thể tăng lên theo cấp số nhân. Ngoài ra, sự hiện diện của sự dư thừa phong phú trong hình ảnh (ví dụ: nền, các mảng tương tự) dẫn đến lãng phí tính toán và tạo ra tiếng ồn đáng kể. Để giải quyết vấn đề này, chúng tôi đề xuất một bộ trừu tượng hóa thị giác được trang bị một tập cố định các truy vấn học được để trích xuất các đặc trưng ngữ nghĩa cao hơn từ hình ảnh. Cụ thể, chúng tôi đưa chuỗi token thị giác được trích xuất I = [I1, I2, ···, IP] ∈ RP×d và một số cố định K truy vấn học được Q ∈ RK×d vào bộ trừu tượng hóa thị giác. Ở đây, P = H/14 × W/14 đại diện cho số lượng mảng thị giác, và d là chiều ẩn. Bộ trừu tượng hóa thị giác bao gồm một chuỗi các lớp bộ trừu tượng hóa thị giác. Trong lớp thứ i của bộ trừu tượng hóa thị giác, các biểu diễn thị giác nén Vi+1 được tính như sau:

Ci = Attn(Vi, [I;Vi], [I;Vi]), (1)
Vi+1 = SwiGLU(CiW1)W2. (2)

Ở đây, Attn(·,·,·) đại diện cho hoạt động tự attention, trong khi W1 ∈ Rd×d' và W2 ∈ Rd'×d là các tham số học được. Hàm SwiGLU(···) đề cập đến hàm kích hoạt SwiGLU [51]. Chúng tôi chỉ định V0 = Q để khởi tạo quá trình. Hơn nữa, để tăng cường khả năng nhận thức hạt mịn, chúng tôi tích hợp nhúng vị trí sin với đặc trưng hình ảnh I và Vi, từ đó bảo tồn thông tin vị trí, đã được chứng minh là quan trọng trong [8]. Do đó, tính toán yêu cầu bởi bộ giải mã ngôn ngữ giảm từ O((P+L)²) xuống O((K+L)²), giảm đáng kể tải tính toán khi P≫K, đặc biệt trong các kịch bản liên quan đến nhiều hình ảnh và khi độ dài văn bản L tương đối ngắn. Khi đã thu được đặc trưng thị giác nén, nó được nối với nhúng token văn bản và sau đó được xử lý bởi bộ giải mã ngôn ngữ để tạo ra dự đoán.

3.3. Module Thích ứng Phương thức
Các phương pháp trước đây [15, 38, 68, 75] thường cố gắng căn chỉnh các đặc trưng thị giác với các đặc trưng ngôn ngữ bằng cách chiếu các đặc trưng hình ảnh vào không gian ngữ nghĩa ngôn ngữ. Tuy nhiên, chiến lược này có thể gây ra sự không khớp về độ hạt, nơi các đặc trưng hình ảnh thường chứa thông tin ngữ nghĩa phong phú so với thông tin ngữ nghĩa rời rạc trong các đặc trưng nhúng văn bản. Những phương pháp đó không xem xét các đặc điểm độc đáo của thông tin thị giác và văn bản, do đó có thể hạn chế hiệu suất của mô hình. Vì vậy, chúng tôi đề xuất một phương pháp mới, cụ thể là Module Thích ứng Phương thức (MAM), tách rời các biểu diễn thị giác-ngôn ngữ bằng cách chiếu các đặc trưng thị giác và ngôn ngữ vào một không gian ngữ nghĩa chia sẻ trong khi bảo tồn các thuộc tính đặc biệt của mỗi phương thức.

Chính thức, cho một chuỗi thị giác-ngôn ngữ X ∈ R(LV+LT)×d và các chỉ số phương thức M ∈ {0,1}(LV+LT), chúng tôi trước tiên định nghĩa hoạt động phân tách phương thức φ như:

φ(X, M, m) = X ⊙ 1{M=m}, (3)

trong đó m ∈ {0,1} là loại phương thức (tức thị giác hoặc ngôn ngữ). Cho các vector đầu ra lớp trước đó Hl-1, l ∈ [1, L], trong đó L là số lượng lớp bộ giải mã ngôn ngữ, chúng tôi trước tiên chuẩn hóa các phương thức khác nhau vào cùng một độ lớn như sau:

H̃l-1 = LNV(φ(Hl-1, M, 0)) + LNT(φ(Hl-1, M, 1)), (4)

--- TRANG 4 ---
trong đó LNV và LNT là chuẩn hóa lớp [4] cho các đặc trưng thị giác và ngôn ngữ tương ứng. Sau đó, chúng tôi tái cấu trúc hoạt động tự attention bằng cách tận dụng các lớp chiếu tuyến tính riêng biệt cho ma trận chiếu khóa và giá trị trong khi bảo tồn ma trận chiếu truy vấn được chia sẻ như sau:

HQl = H̃l-1WQl, (5)
HKl = φ(H̃l-1, M, 0)WK0l + φ(H̃l-1, M, 1)WK1l, (6)
HVl = φ(H̃l-1, M, 0)WV0l + φ(H̃l-1, M, 1)WV1l, (7)
Cl = Softmax(HQlHKl⊤/√d)HVl, (8)

trong đó WQl, WK0l, WK1l, WV0l, WV1l ∈ Rd×d là các ma trận chiếu học được, và Cl ∈ R(LV+LT)×d là các đặc trưng ngữ cảnh của lớp thứ l. Theo cách này, chúng ta có thể tính toán độ tương tự giữa hai phương thức trong một không gian ngữ nghĩa chia sẻ, đồng thời bảo tồn các đặc điểm độc đáo của mỗi phương thức thông qua các lớp chiếu giá trị khác nhau. Hơn nữa, bằng cách tách rời ma trận chiếu khóa và giá trị, chúng ta có thể tránh sự can thiệp giữa hai phương thức, đặc biệt liên quan đến sự không khớp về độ hạt. Theo tinh thần tương tự, chúng tôi cũng nhằm mô hình hóa những đặc điểm này bằng cách sử dụng các lớp chuẩn hóa khác nhau. Cuối cùng, để thúc đẩy hợp tác phương thức trong cùng một không gian đặc trưng, chúng tôi duy trì một FFN chia sẻ cho cả hai phương thức. Kết quả là, mô hình có thể bảo tồn các đặc điểm phương thức trong khi đạt được hợp tác phương thức thông qua module thích ứng phương thức được đề xuất.

3.4. Mô hình Huấn luyện
Như được mô tả trong Hình 2(c), chúng tôi sử dụng phương pháp hai giai đoạn trong việc huấn luyện mPLUG-Owl2, bao gồm tiền huấn luyện và điều chỉnh lệnh thị giác tương tự như [38, 68], nhằm căn chỉnh bộ mã hóa thị giác được tiền huấn luyện và mô hình ngôn ngữ trong giai đoạn tiền huấn luyện, sau đó điều chỉnh mô hình ngôn ngữ với mất mát mô hình hóa ngôn ngữ trong giai đoạn điều chỉnh lệnh. Tuy nhiên, chúng tôi thấy rằng việc chỉ đóng băng bộ mã hóa thị giác được tiền huấn luyện và huấn luyện một bộ chiếu thị giác-ngôn ngữ để căn chỉnh dữ liệu thị giác với các mô hình ngôn ngữ có thể hạn chế khả năng diễn giải thông tin thị giác phức tạp của chúng, như văn bản trong cảnh và kiến thức thị giác. Để giải quyết vấn đề này, chúng tôi làm cho bộ mã hóa thị giác có thể huấn luyện được trong cả giai đoạn tiền huấn luyện và điều chỉnh lệnh. Chiến lược này cho phép mô hình nắm bắt cả thông tin thị giác ngữ nghĩa cấp thấp và cấp cao hiệu quả hơn. Cụ thể, cho giai đoạn tiền huấn luyện, chúng tôi cho phép bộ mã hóa thị giác, bộ trừu tượng hóa thị giác, và một phần của module thích ứng phương thức có thể huấn luyện được, trong khi giữ mô hình ngôn ngữ được tiền huấn luyện ở trạng thái đóng băng. Trong khi đó, nghiên cứu trước đây trong học đa phương thức [63] đã chỉ ra rằng có thể đạt được những cải tiến đáng kể thông qua học hợp tác của các nguồn đơn phương thức và đa phương thức. Dựa trên điều này, chúng tôi áp dụng phương pháp huấn luyện kết hợp bằng cách điều chỉnh toàn bộ mô hình trong giai đoạn điều chỉnh lệnh, kết hợp cả dữ liệu lệnh văn bản và

--- TRANG 5 ---
Chú thích Hình ảnh VQA Tổng quát VQA Tổng quát (Zero-shot)
Loại Mô hình Phương pháp #Tham số COCOFlickr30KVQAv2 OKVQA GQA VizWizQA TextVQA SciQA (IMG)(Zero-Shot)
Người tổng quát BLIP-2 [31] 8.2B - 74.9 65.0 45.9 41.0 19.6 42.5 61.0
InstructBLIP [15] 8.2B 102.2 82.4 - - 49.2 34.5 50.1†60.5
Unified-IO XL[41]2.9B 122.3 - 77.9 54.0 - 57.4‡- -
PaLM-E-12B [16] 12B135.0 - 76.2 55.5 - - - -
Shikra [10] 7.2B 117.5 73.9 77.4 47.2 - - - -
LLaVA-1.5 [37] 7.2B - - 78.5 - 62.0 50.0 46.1/ 58.2†66.8
Qwen-VL-Chat [5] 9.6B 131.9 81.0 78.2 56.6 57.5 38.9 61.5‡68.2
mPLUG-Owl2 8.2B 137.3 85.1 79.4 57.7 56.1 54.5 54.3/58.2†68.7
Chuyên gia GIT [59] 0.7B 114.8 49.6 78.6 - - 68.0 59.8 -
GIT2 [59] 5.1B 145.0 50.7 81.7 - - 71.0 59.8 -
PaLI-17B [11] 17B149.1 - 84.3 64.5 - 71.6 58.8 -

Bảng 1. So sánh hiệu suất về chú thích hình ảnh và trả lời câu hỏi thị giác. Đối với chú thích hình ảnh, CIDEr được báo cáo để đánh giá, và độ chính xác được báo cáo cho VQA. Lưu ý rằng các chuyên gia được điều chỉnh trên từng bộ dữ liệu riêng lẻ. †biểu thị đầu vào OCR được sử dụng. ‡chỉ ra mô hình đã được huấn luyện trên bộ dữ liệu. Chúng tôi tô xám những phương pháp chuyên gia được điều chỉnh riêng lẻ trên bộ dữ liệu cũng như những kết quả được điều chỉnh của các người tổng quát.

Phương pháp Bộ mã hóa thị giác Mô hình ngôn ngữ MME MMBench MM-Vet SEED-Bench Q-Bench
BLIP-2 [31] ViT-g (1.3B) Vicuna (7B) 1293.84 - 22.4 46.4 -
MiniGPT-4 [75] ViT-g (1.3B) Vicuna (7B) 581.67 23.0 22.1 42.8 -
LLaVA [38] ViT-L (0.3B) Vicuna (7B) 502.82 36.2 28.1 33.5 54.7
mPLUG-Owl [68] ViT-L (0.3B) LLaMA (7B) 967.34 46.6 - 34.0 58.9
InstructBLIP [15] ViT-g (1.3B) Vicuna (7B) 1212.82 36.0 26.2 53.4 55.8
LLaMA-Adapter-v2 [19] ViT-L (0.3B) LLaMA (7B) 1328.40 39.5 31.4 32.7 58.1
Otter [30] ViT-L (0.3B) LLaMA (7B) 1292.26 48.3 24.6 32.9 47.2
Qwen-VL-Chat [5] ViT-G (1.9B) Qwen (7B) 1487.58 60.6 - 58.2 61.6
LLaVA-1.5 [37] ViT-L (0.3B) Vicuna (7B) 1510.70 64.3 30.5 58.6 60.7
mPLUG-Owl2 ViT-L (0.3B) LLaMA (7B) 1450.19 64.5 36.2 57.8 62.9

Bảng 2. Đánh giá đa phương thức zero-shot trên các điểm chuẩn đa phương thức bao gồm MME [17], MMBench [39], MM-Vet [70], SEED-Bench [29], và Q-Bench [60]. Điểm số tổng thể được báo cáo để đánh giá. Đối với MMBench và Q-Bench, chúng tôi báo cáo kết quả kiểm tra.

đa phương thức. Phương pháp này nâng cao khả năng hiểu các khái niệm thị giác được nhúng trong văn bản của mô hình thông qua các lệnh đa phương thức. Đồng thời, dữ liệu lệnh văn bản tăng cường khả năng hiểu các lệnh tự nhiên phức tạp của mô hình, từ đó đảm bảo bảo tồn khả năng ngôn ngữ của nó.

4. Thí nghiệm
4.1. Triển khai
Bộ dữ liệu mPLUG-Owl2 được tiền huấn luyện trên các cặp hình ảnh-văn bản và điều chỉnh trên dữ liệu lệnh đơn phương thức và đa phương thức. Đối với dữ liệu tiền huấn luyện, chúng tôi chọn ngẫu nhiên khoảng 400 triệu cặp hình ảnh-văn bản từ năm bộ dữ liệu công cộng: Conceptual Captions (CC3M/CC12M) [9], COCO [35], Laion-en [49], COYO [7], DataComp [18]. Đối với dữ liệu lệnh, chúng tôi thu thập 5 loại bộ dữ liệu bao gồm 1) chú thích hình ảnh (tức TextCaps [53], COCO [35]); 2) trả lời câu hỏi hình ảnh (tức VQAv2 [21], OKVQA [43], OCR-VQA [44], GQA [24], và A-OKVQA [50]); 3) QA nhận biết vùng (tức RefCOCO [69], VisualGenome [26]); 4) dữ liệu lệnh đa phương thức (tức LLaVA-instruct-150K [38]); 5) dữ liệu lệnh chỉ văn bản (tức ShareGPT-80K [1], SlimOrca [34]). Chi tiết có thể được tìm thấy trong Phụ lục.

Cài đặt Huấn luyện Chúng tôi tiền huấn luyện mô hình trong 42,500 vòng lặp với kích thước batch 8,192 cho khoảng 348 triệu cặp hình ảnh-văn bản. Vì chúng tôi áp dụng mất mát mô hình hóa ngôn ngữ, kích thước batch lớn có thể được đạt được dễ dàng bằng kỹ thuật tích lũy gradient. mPLUG-Owl2 sử dụng ViT-L [48] với kích thước patch 14×14 và được tiền huấn luyện ở độ phân giải 224×224. Chúng tôi sử dụng cùng một tăng cường dữ liệu như trong BLIP-2 [31], bao gồm cắt thay đổi kích thước ngẫu nhiên và lật ngang với xác suất 0.5. Số lượng lớp trong bộ trừu tượng hóa thị giác được đặt là 6 và nó được khởi tạo ngẫu nhiên. Số lượng truy vấn học được được đặt là 64. Đối với mô hình ngôn ngữ, LLaMA-2 [58] được sử dụng để xử lý các đặc trưng đa phương thức với 7B tham số, và các tham số của các module thích ứng phương thức được khởi tạo từ mô hình ngôn ngữ. Chúng tôi sử dụng bộ tối ưu hóa AdamW [40] với β1 = 0.9, β2 = 0.98 và ε = 1e-6 để tối ưu hóa. Bộ lập lịch tốc độ học suy giảm cosin với tốc độ học đỉnh 1e-4 và với các bước khởi động 1k. Đối với tốc độ học của bộ mã hóa thị giác, chúng tôi sử dụng suy giảm tốc độ học theo lớp với hệ số 0.9 để giữ lại biểu diễn thị giác cấp thấp. Đối với giai đoạn điều chỉnh lệnh, chúng tôi huấn luyện toàn bộ mô hình trong 1 epoch với tốc độ học 2e-5 và kích thước batch 256. Ngoài ra, chúng tôi tăng độ phân giải từ 224×224 lên 448×448. Suy giảm tốc độ học theo lớp cũng được sử dụng, điều này rất quan trọng để giữ lại biểu diễn thị giác tốt trong các thí nghiệm của chúng tôi.

4.2. Kết quả Chính
Chú thích Hình ảnh và Trả lời Câu hỏi Thị giác. Chúng tôi đánh giá mPLUG-Owl2 sử dụng một loạt các điểm chuẩn học thuật để đánh giá các mô hình thị giác-ngôn ngữ. Đánh giá của chúng tôi bao gồm tám điểm chuẩn phổ biến, như được tóm tắt trong Bảng 1. Như kết quả cho thấy, mPLUG-Owl2 của chúng tôi vượt trội hơn các mô hình tổng quát trước đây trong cả nhiệm vụ chú thích và trả lời câu hỏi. Cụ thể, mPLUG-Owl2 đạt được hiệu suất tốt nhất trên bộ dữ liệu Flickr30K, ngay cả khi so sánh với các mô hình có backbone mạnh hơn (ví dụ: Qwen-VL-Chat [5] và InstructBLIP [15]). Hơn nữa, mPLUG-Owl2 thể hiện những ưu điểm rõ rệt trong trả lời câu hỏi thị giác, đặc biệt trong các kịch bản không có OCR, nơi mPLUG-Owl2 đạt được 54.3% độ chính xác trên bộ dữ liệu TextVQA theo cách zero-shot, thể hiện lợi ích của chiến lược huấn luyện của chúng tôi. Cũng đáng chú ý là mPLUG-Owl2 cho thấy hiệu suất zero-shot mạnh trên các bộ dữ liệu ScienceQA (Image Set) và VizWizQA.

Điểm chuẩn Đa phương thức định hướng MLLM. Với khả năng zero-shot mạnh mẽ của các Mô hình Ngôn ngữ Đa phương thức (MLLM), các chỉ số đánh giá truyền thống thường không đủ để cung cấp đánh giá khả năng chi tiết. Vấn đề này còn được làm trầm trọng thêm bởi việc chúng không thể khớp câu trả lời đã cho một cách chính xác, dẫn đến các vấn đề về tính bền vững đáng kể. Để giải quyết những thách thức này, cộng đồng nghiên cứu đã giới thiệu một loạt điểm chuẩn bao gồm MME [17], MMBench [39], MM-Vet [70], SEED-Bench [29], và Q-Bench [60]. Những điểm chuẩn này có cấu trúc hệ thống và đánh giá các nhiệm vụ đa phương thức phức tạp. Chúng tôi áp dụng mô hình của mình, theo cách zero-shot, cho năm điểm chuẩn đa phương thức phổ biến gần đây. Để so sánh công bằng, chúng tôi chọn các mô hình có kích thước mô hình ngôn ngữ tương tự, đặc biệt là những mô hình từ họ LLaMA, và chi tiết sự khác biệt của chúng trong bộ mã hóa thị giác. Kết quả đánh giá của chúng tôi được liệt kê trong Bảng 2. Trong bảng, mPLUG-Owl2 đạt được hiệu suất zero-shot cao hơn về MMBench, MM-Vet, và Q-Bench. Ngược lại, hiệu suất trên MME thấp hơn do số lượng mẫu kiểm tra hạn chế trong MME, có thể dẫn đến biến động nhạy cảm trong hiệu suất. Đặc biệt, nó thể hiện cải thiện đáng kể trên Q-Bench, một điểm chuẩn để kiểm tra nhận thức thị giác cấp thấp của MLLM. Cải thiện này xảy ra khi áp dụng một backbone thị giác nhỏ hơn (tức ViT-L), dẫn đến nhận thức thị giác cấp thấp được nâng cao. Điều này chứng minh hiệu quả của chiến lược huấn luyện của chúng tôi để huấn luyện backbone thị giác.

Phương pháp MMLU BBHAGIEval ARC-c ARC-e
LLaMA-2 [58] 46.8 38.2 21.8 40.3 56.1
WizardLM [61] 38.1 34.7 23.2 47.5 59.6
LLaMA-2-Chat [58] 46.2 35.6 28.5 54.9 71.6
Vicuna-v1.5 [73] 51.1 41.2 21.2 56.6 72.8
mPLUG-Owl2 53.4 45.0 32.7 65.8 79.9

Bảng 3. Hiệu suất trên các điểm chuẩn văn bản thuần của mPLUG-Owl2 so với các biến thể họ LLaMA-2 (7B). Chúng tôi áp dụng 5-shot cho MMLU và 0-shot cho BBH, AGIEval, và ARC như [14].

Hiểu và Tạo Ngôn ngữ Tự nhiên. Các MLLM hiện tại thường vượt trội trong các nhiệm vụ hạ nguồn đa phương thức khác nhau bằng cách tận dụng sức mạnh của các mô hình ngôn ngữ lớn. Tuy nhiên, khả năng nội tại của những mô hình này thường đóng vai trò quan trọng trong việc xác định hiệu suất của MLLM, một khía cạnh thường bị bỏ qua trong các nghiên cứu mô hình ngôn ngữ đa phương thức trước đây. Theo đó, chúng tôi cũng đã đánh giá hiệu suất của mô hình trong bối cảnh hiểu và tạo ngôn ngữ tự nhiên. Chúng tôi thực hiện đánh giá trên MMLU [22], BBH [55], AGIEval [74] và ARC [13]. Kết quả được minh họa trong Bảng 3. Như quan sát trong bảng, mPLUG-Owl2 xuất sắc trong kiểm tra và lý luận, cho thấy cải thiện đáng kể trên MMLU và BBH lần lượt là 2.3% và 3.8%. Điều này chỉ ra rằng mPLUG-Owl2 không chỉ hoạt động tốt trên các nhiệm vụ đa phương thức mà còn đạt được hiệu suất tốt hơn so với các LLM được điều chỉnh lệnh khác, cho thấy con đường hứa hẹn để phát triển MLLM mạnh mẽ.

Phương pháp MSRVTT-QA MSVD-QA TGIF-QA
Độ chính xác Điểm số Độ chính xác Điểm số Độ chính xác Điểm số
Khớp Chính xác
Flamingo-80B [2] 17.4 - 35.6 - - -
FrozenBiLM [64] 16.8 - 32.2 - 41.0 -
BLIP-2 [31] 9.2 - 18.3 - - -
HiTeA [67] 21.7 - 37.4 - - -
InstructBLIP [15] 22.1 - 41.8 - - -
mPLUG-Owl2 23.6 - 42.4 - 61.6 -
Hỗ trợ GPT
Video Chat [32] 45.0 2.5 56.3 2.8 34.4 2.3
LLaMA-Adapter [19] 43.8 2.7 54.9 3.1 - -
Video-LLaMA [71] 29.6 1.8 51.6 2.5 - -
Video-ChatGPT [42] 49.3 2.864.9 3.3 51.4 3.0
mPLUG-Owl2 46.7 2.9 65.4 3.5 67.1 3.7

Bảng 4. Đánh giá zero-shot về trả lời câu hỏi video. Độ chính xác và điểm số liên quan được báo cáo.

Trả lời Câu hỏi Video Zero-Shot. Cho rằng video có thể được xem như một chuỗi hình ảnh, chúng tôi đã tiến hành đánh giá định lượng toàn diện sử dụng một số bộ dữ liệu trả lời câu hỏi video thường được sử dụng, bao gồm MSRVTT-QA [62], MSVD-QA [62], và TGIF-QA [25]. Những bộ dữ liệu này hỗ trợ trong đánh giá zero-shot về khả năng hiểu nội dung video của mô hình, với

--- TRANG 6 ---
kết quả được tóm tắt trong Bảng 4. Chúng tôi sử dụng hai loại đánh giá: 1) Khớp chính xác, thường được sử dụng trong các đánh giá trả lời câu hỏi video trước đây; và 2) Đánh giá hỗ trợ GPT [42] đánh giá khả năng của mô hình bằng cách đo độ chính xác của các dự đoán được tạo ra bởi mô hình và cung cấp điểm số tương đối trên thang điểm 1-5. Chúng tôi quan sát rằng mô hình của chúng tôi đạt được kết quả vượt trội trên cả ba bộ dữ liệu video dưới cài đặt zero-shot. Hơn nữa, về mặt liên quan, mô hình của chúng tôi tạo ra các câu trả lời chính xác hơn so với các MLLM video khác, từ đó chứng minh tính vượt trội và khả năng tổng quát hóa tuyệt vời.

4.3. Thảo luận
Hợp tác Phương thức cho Hiệu suất Văn bản. Để chứng minh cách hợp tác phương thức nâng cao không chỉ hiệu suất đa phương thức mà còn khả năng văn bản của MLLM, chúng tôi đánh giá hiệu suất của các điểm chuẩn văn bản về các khả năng khác nhau bao gồm kiểm tra, kiến thức, hiểu và lý luận. Như quan sát trong Hình 3, cả khả năng kiểm tra và kiến thức của MLLM đều được cải thiện đáng kể nhờ lợi ích của hợp tác phương thức được tạo điều kiện bởi module thích ứng phương thức. Cải thiện này phát sinh vì dữ liệu đa phương thức cho phép mô hình sử dụng thông tin thị giác để hiểu các khái niệm không thể được mô tả thông qua ngôn ngữ. Tương tự, mô hình có thể tạo ra các phản hồi phong phú và đáng kể hơn do hiểu biết cụ thể hơn về những khái niệm này. Ngoài ra, dữ liệu đa phương thức nâng cao khả năng lý luận của mô hình vì hình ảnh chứa thông tin phong phú (như các mối quan hệ và khía cạnh không gian). Mô hình học từ những khía cạnh này và liên kết chúng với văn bản, từ đó gián tiếp nâng cao khả năng lý luận của văn bản.

Tác động của Điều chỉnh Lệnh Thị giác-Ngôn ngữ Kết hợp. Bảng 5 trình bày kết quả điều chỉnh lệnh với các loại dữ liệu khác nhau cũng như việc có sử dụng module thích ứng phương thức hay không. Những kết quả này cho thấy rằng ngay cả khi không có dữ liệu lệnh đa phương thức, hiệu suất của mô hình trên các điểm chuẩn đa phương thức vẫn đáng kể do căn chỉnh thị giác-ngôn ngữ hiệu quả đạt được trong quá trình tiền huấn luyện. Tuy nhiên,

MAM Lệnh Văn bản Lệnh MM VQAv2 Q-Bench MMLU BBH
✓ 58.2 54.4 51.8 43.6
✓ 76.3 61.3 45.4 25.7
✓ ✓ 76.2 60.3 51.6 43.2
✓ ✓ 60.5 55.6 51.8 44.0
✓ ✓ 76.5 60.2 46.1 30.6
✓ ✓ ✓ 76.8 62.2 52.8 45.0

Bảng 5. So sánh hiệu suất giữa các loại dữ liệu lệnh và cấu trúc khác nhau.

Mở khóa Tốc độ học theo lớp VQAv2 TextVQA MMBench Q-Bench
74.8 39.8 63.8 60.7
✓ 76.2(+1.4) 40.3(+0.5) 62.7(-1.1) 61.6(+0.9)
✓ ✓ 76.8(+2.0) 42.5(+2.7) 64.5(+0.7) 62.2(+1.5)

Bảng 6. Ảnh hưởng của các chiến lược học cho bộ mã hóa thị giác.

# Truy vấn Học được VQAv2 TextVQA MMBench Q-Bench
8 58.3 18.6 47.6 52.4
16 66.2 28.5 52.9 54.9
32 72.4 36.3 60.2 57.8
64 76.8 42.5 64.5 62.2
128 76.7 44.4 63.6 61.6

Bảng 7. Hiệu suất về số lượng truy vấn học được.

khi chỉ sử dụng dữ liệu lệnh đa phương thức, chúng tôi quan sát thấy sự gia tăng hiệu suất trên các bộ dữ liệu đa phương thức, trong khi hiệu suất trên các nhiệm vụ văn bản giảm khoảng 5.7%. Hiện tượng này có thể được cân bằng bởi điều chỉnh thị giác-ngôn ngữ kết hợp được đề xuất, như được hiển thị trong hàng thứ ba của bảng, nơi hiệu suất đa phương thức bắt đầu giảm nhẹ do nhiễu phương thức. Để chống lại nhược điểm này, chúng tôi áp dụng module thích ứng phương thức được đề xuất vào mô hình. Kết quả cho thấy hiệu suất trên cả điểm chuẩn đa phương thức và văn bản đều được cải thiện, với mức tăng tối thiểu 0.6% trên bộ dữ liệu VQAv2 và 1.6% trên MMLU.

Tác động của Bộ mã hóa Thị giác Có thể Huấn luyện. Bảng 6 cung cấp hiệu suất của việc huấn luyện bộ mã hóa thị giác trong quá trình điều chỉnh lệnh với hợp tác phương thức. Có thể quan sát rằng việc cho phép bộ mã hóa thị giác có thể huấn luyện được cải thiện hiệu suất trên VQAv2 và Q-Bench ít nhất 1.4% và 0.9% tương ứng, gợi ý lợi ích của hợp tác phương thức. Ngược lại, nó dẫn đến giảm hiệu suất 1.1% trong MM-Bench, cho thấy một mức độ quên lãng và thiệt hại đối với biểu diễn thị giác tổng quát do tính đa dạng hạn chế của dữ liệu lệnh. Để giảm thiểu thách thức này, chúng tôi áp dụng suy giảm tốc độ học theo lớp với hệ số suy giảm mũ 0.9, bảo tồn biểu diễn của các lớp thấp hơn trong khi sửa đổi các biểu diễn ngữ nghĩa cao hơn. Bằng cách áp dụng suy giảm tốc độ học theo lớp, chúng ta có thể nhận thấy rằng hiệu suất trên TextVQA tăng thêm 2.2%, cho thấy hiệu quả của chiến lược huấn luyện của chúng tôi.

Tác động của Số lượng Truy vấn Học được. Để điều tra hiệu ứng của số lượng truy vấn học được Q, chúng tôi tiến hành thí nghiệm sử dụng số lượng truy vấn khác nhau trong bộ trừu tượng hóa thị giác

--- TRANG 7 ---
trừu tượng hóa thị giác, như được hiển thị trong Bảng 7. Có thể quan sát rằng mô hình liên tục thể hiện sự cải thiện khi số lượng truy vấn học được tăng lên cho đến khi đạt đến điểm bão hòa, gợi ý rằng 64 có thể là số lượng tối ưu để biểu diễn một hình ảnh. Đáng chú ý, có một sự tăng hiệu suất đáng kể được quan sát khi số lượng tăng từ 8 lên 64, ví dụ, hiệu suất của VQAv2 tăng 18.5%. Những phát hiện này gợi ý rằng số lượng truy vấn học được cao hơn có thể nắm bắt thông tin hình ảnh một cách toàn diện hơn, từ đó nâng cao khả năng hiểu hình ảnh của mô hình.

Độ phân giải VQAv2 TextVQA MMBench MM-Vet Q-Bench
224×224 76.8 42.5 64.5 34.0 62.2
336×336 78.5(+1.7) 49.8(+7.3) 65.2(+0.7) 34.6(+0.6) 62.4(+0.2)
448×448 79.4(+2.6) 54.3(+11.8) 65.4(+0.9) 36.2(+2.2) 62.6(+0.4)

Bảng 8. Ảnh hưởng của các độ phân giải hình ảnh đầu vào khác nhau.

Tác động của Độ phân giải Hình ảnh. Độ phân giải hình ảnh đóng vai trò quan trọng trong các nhiệm vụ thị giác-ngôn ngữ, vì độ phân giải cao hơn có thể giảm mờ hình ảnh và cải thiện hiểu biết về các chi tiết hạt mịn. Để khám phá tác động của độ phân giải hình ảnh đối với hiệu suất trên các điểm chuẩn khác nhau, chúng tôi điều chỉnh độ phân giải hình ảnh từ 224×224 lên 448×448 và kết quả được liệt kê trong Bảng 8. Như quan sát trong bảng, việc sử dụng độ phân giải cao hơn tỏ ra có lợi cho các nhiệm vụ đa phương thức, đặc biệt trong kịch bản trả lời câu hỏi. Cụ thể, hiệu suất của VQAv2 tăng từ 76.8 lên 79.4, đại diện cho mức tăng 2.6%. Đồng thời, có sự tăng 11.8 điểm trong điểm chuẩn TextVQA khi tăng độ phân giải từ 224×224 lên 448×448. Điều này gợi ý rằng các nhiệm vụ liên quan đến OCR hưởng lợi đáng kể từ việc tăng độ phân giải.

4.4. Phân tích Định tính
Tác động của Module Thích ứng Phương thức trong Kịch bản Đa phương thức. Chúng tôi điều tra tác động của Module Thích ứng Phương thức trong các kịch bản đa phương thức bằng cách trực quan hóa các bản đồ attention của mPLUG-Owl2 có và không có module này sử dụng đầu vào chú thích hình ảnh, như được hiển thị trong Hình 4. Mỗi bản đồ attention minh họa điểm số attention của các token được tạo ra trên chuỗi đầu vào trong quá trình tạo ra.

Có thể quan sát rằng bất kể Module Thích ứng Phương thức có được kết hợp hay không, mô hình tập trung nhiều hơn vào các token văn bản trong các lớp trước đó trong khi chú ý nhiều hơn đến các token thị giác trong các lớp sau. Điều này gợi ý rằng việc mô hình hóa thông tin thị giác và văn bản đóng vai trò khác nhau trong sự hợp tác của các mô hình ngôn ngữ đa phương thức (MLLM). Một giải thích trực quan là MLLM ban đầu sử dụng thông tin cú pháp để hiểu các lệnh và sau đó xác định các token nội dung thị giác có liên quan bằng cách xem xét đầu vào văn bản.

Token Thị giác Token Văn bản Token Thị giác Token Văn bản Token Thị giác Token Văn bản Token Thị giác Token Văn bản Token Thị giác Token Văn bản Token Thị giác Token Văn bản có Module Thích ứng Phương thức không có Module Thích ứng Phương thức

có Module Thích ứng Phương thức không có Module Thích ứng Phương thức

có Module Thích ứng Phương thức không có Module Thích ứng Phương thức

Lớp #0

Lớp #15

Lớp #31

Hình 4. Trực quan hóa các bản đồ attention có và không có Module Thích ứng Phương thức. Chúng tôi trình bày các bản đồ attention cho các lớp thứ 0, 15, và 31, trong đó phạm vi của các token thị giác được chỉ ra bằng màu cam và phạm vi của các token văn bản được chỉ ra bằng màu xanh.

Khi sử dụng Module Thích ứng Phương thức, có thể quan sát rằng mô hình rõ ràng chú ý nhiều hơn đến nội dung văn bản trong các giai đoạn trước và tập trung nhiều hơn vào nội dung thị giác trong các giai đoạn sau. Module Thích ứng Phương thức ngăn chặn các token thị giác và văn bản được coi là giống nhau và khuyến khích sự hợp tác giữa các phương thức khác nhau.

Tác động của Module Thích ứng Phương thức trong Kịch bản Phương thức Không liên quan. Chúng tôi đưa ra một câu hỏi: "Bảy màu của cầu vồng là gì?" cùng với một hình ảnh được chọn ngẫu nhiên. Trong ví dụ này, đầu vào hình ảnh hoạt động như một sự nhiễu loạn đối với mô hình. Chúng tôi nhằm điều tra tác động của module của chúng tôi đối với dữ liệu chứa các phương thức không liên quan. Các phản hồi và bản đồ attention của mô hình được hiển thị trong Hình 5. Mô hình được đề xuất của chúng tôi, mPLUG-Owl2, kết hợp Module Thích ứng Phương thức, chính xác xác định tất cả bảy màu. Trong quá trình tạo ra, có thể quan sát rằng mô hình chủ yếu tập trung vào đầu vào văn bản. Mặt khác, khi Module Thích ứng Phương thức không được sử dụng, mPLUG-Owl2 chỉ xác định được sáu màu. Khả năng hiểu lệnh văn bản của mô hình bị gián đoạn, và cũng rõ ràng là nó chú ý nhiều hơn đến hình ảnh trong quá trình tạo ra. Nhờ Module Thích ứng Phương thức, mPLUG-Owl2 có thể nắm bắt tốt hơn các đặc trưng riêng biệt của phương thức khi mô hình hóa các đầu vào đa phương thức. Điều này nâng cao khả năng thích ứng của hợp tác phương thức, dẫn đến giảm nhiễu loạn khi văn bản và hình ảnh không liên quan.

5. Kết luận
Trong bài báo này, chúng tôi trình bày mPLUG-Owl2, một mô hình tổng quát có khả năng cao bằng cách tận dụng hợp tác phương thức để

--- TRANG 8 ---
Người dùng: <hình ảnh> Bảy màu của cầu vồng là gì?
Phản hồi: cam, vàng, xanh lá cây, xanh dương, chàm, tím và trắng.
Token Thị giác Token Văn bản Bản đồ Attention Trung bình

Bản đồ Attention Trung bình Người dùng: <hình ảnh> Bảy màu của cầu vồng là gì?
Phản hồi: Cam, Vàng, Xanh lá cây, Xanh dương, Tím, và Trắng.

Token Thị giác Token Văn bản

mPLUG-Owl2 có Module Thích ứng Phương thức 

mPLUG-Owl2 không có Module Thích ứng Phương thức

Hình 5. Trực quan hóa các bản đồ attention có và không có Module Thích ứng Phương thức. Chúng tôi trình bày trung bình của các bản đồ attention qua từng lớp, trong đó phạm vi của các token thị giác được chỉ ra bằng màu cam và phạm vi của các token văn bản được chỉ ra bằng màu xanh.

nâng cao hiệu suất trong cả nhiệm vụ văn bản và đa phương thức. Việc bao gồm các module chức năng chia sẻ và module thích ứng phương thức trong mPLUG-Owl2 tăng cường khả năng của mô hình để hài hòa sự hợp tác phương thức và bảo tồn các đặc điểm riêng biệt của phương thức. Các đánh giá thí nghiệm mở rộng nô tại khả năng thành thạo của mPLUG-Owl2 trong việc tổng quát hóa các nhiệm vụ khác nhau, từ đó đạt được hiệu suất tốt nhất với một mô hình tổng quát duy nhất. Đáng chú ý nhất, mPLUG-Owl2 đứng như mô hình MLLM đầu tiên thể hiện hiện tượng hợp tác phương thức trong cả bối cảnh văn bản thuần và đa phương thức. Điều này không chỉ nâng cao khả năng hiểu thị giác-ngôn ngữ của mô hình mà còn cải thiện khả năng ngôn ngữ của nó về hiểu, kiến thức và lý luận. Điều này đại diện cho một đóng góp quan trọng cho lĩnh vực và mở ra những cơ hội thú vị cho việc phát triển tương lai của các mô hình nền tảng đa phương thức.

Tài liệu tham khảo
[1] Sharegpt. http://sharegpt.com, 2023. 2, 5, 17
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 2, 6
[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 16
[4] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 4
[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. ArXiv, abs/2308.12966, 2023. 2, 5, 6, 16
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. 1, 3
[7] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, và Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. 5
[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213–229. Springer, 2020. 2, 3
[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, và Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558–3568, 2021. 5
[10] Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, và Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. ArXiv, abs/2306.15195, 2023. 2, 5, 15, 16
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 5
[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,

--- TRANG 9 ---
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1–240:113, 2022. 2

[13] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. 6

[14] OpenCompass Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass, 2023. 6

[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, và Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023. 2, 3, 5, 6, 13, 14, 15, 16

[16] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, và Peter R. Florence. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, 2023. 2, 5

[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 5, 6

[18] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 5

[19] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shĳie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, và Yu Jiao Qiao. Llama-adapter v2: Parameter-efficient visual instruction model. ArXiv, abs/2304.15010, 2023. 5, 6, 15, 16

[20] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, và Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023. 15

[21] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, và Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 5, 13, 17

[22] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 6

[23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, và Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. 2

[24] Drew A Hudson và Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700–6709, 2019. 5, 13, 17

[25] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, và Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2758–2766, 2017. 6

[26] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32–73, 2017. 5, 15, 17

[27] Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, và Stefano Soatto. Masked vision and language modeling for multi-modal representation learning. arXiv preprint arXiv:2208.02131, 2022. 2

[28] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, và Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023. 13, 16

[29] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, và Ying Shan. Seed-bench: Benchmarking multi-modal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 5, 6

[30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, và Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. ArXiv, abs/2305.03726, 2023. 5, 15, 16

[31] Junnan Li, Dongxu Li, Silvio Savarese, và Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023. 2, 5, 6, 16

[32] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, và Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 6

[33] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, và Ji rong Wen. Evaluating object hallucination in large vision-language models. ArXiv, abs/2305.10355, 2023. 13

--- TRANG 10 ---
[34] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, và "Teknium". Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification, 2023. 5, 17

[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5, 13, 17

[36] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, và Lĳuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. 3

[37] Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023. 3, 5, 14, 16

[38] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. 2, 3, 4, 5, 13, 14, 15, 16, 17

[39] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 5, 6, 13, 15

[40] Ilya Loshchilov và Frank Hutter. Fixing weight decay regularization in adam. 2018. 5

[41] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, và Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022. 5

[42] Muhammad Maaz, Hanoona Rasheed, Salman Khan, và Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. ArXiv, abs/2306.05424, 2023. 6, 7

[43] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195–3204, 2019. 5, 13, 17

[44] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, và Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947–952. IEEE, 2019. 5, 13, 17

[45] OpenAI. Gpt-4v(ision) system card. 2023. 1

[46] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 1, 2, 16

[47] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, và Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. ArXiv, abs/2306.14824, 2023. 13, 16

[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 3, 5, 17

[49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278–25294, 2022. 5

[50] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, và Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146–162. Springer, 2022. 5, 13, 17

[51] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 3

[52] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, và Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. 15

[53] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, và Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pages 742–758. Springer, 2020. 5, 13, 17

[54] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, và Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. ArXiv, abs/2309.14525, 2023. 13, 15, 16

[55] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. 6

[56] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 2

[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023. 1, 3

[58] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,

--- TRANG 11 ---
Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. 1, 3, 5, 6, 17

[59] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, và Lĳuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 5

[60] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, và Weisi Lin. Q-bench: A benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. 5, 6, 13, 16

[61] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. ArXiv, abs/2304.12244, 2023. 2, 6

[62] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, và Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645–1653, 2017. 6

[63] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: A modularized multi-modal foundation model across text, image and video. In International Conference on Machine Learning, 2023. 2, 4

[64] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, và Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. In NeurIPS, 2022. 6

[65] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, và Fei Huang. mplug-docowl: Modularized multimodal large language model for document understanding. CoRR, abs/2307.02499, 2023. 2

[66] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. 2

[67] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi Qian, Ji Zhang, và Fei Huang. Hitea: Hierarchical temporal-aware video-language pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15405–15416, 2023. 6

[68] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. 2, 3, 4, 5, 15, 16

[69] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, và Tamara L Berg. Modeling context in referring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69–85. Springer, 2016. 5, 15, 17

[70] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, và Lĳuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 5, 6, 16

[71] Hang Zhang, Xin Li, và Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. ArXiv, abs/2306.02858, 2023. 6

[72] Bo Zhao, Boya Wu, và Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. 3

[73] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. 6

[74] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, và Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. 6

[75] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592, 2023. 2, 3, 5, 14, 15, 16

--- TRANG 12 ---
A. Kết quả Thí nghiệm Bổ sung
Trong phần này, chúng tôi cung cấp thêm kết quả thí nghiệm để hoàn thiện phương pháp được đề xuất.

A.1. Đánh giá Ảo giác
Tổng thể
Thuộc tính
Đối kháng
So sánh
Đếm Quan hệ Môi trường Toàn diện Khác
1.02.03.04.0
1.02.03.04.0
1.02.03.04.0
1.0
2.0
3.0
4.01.0
2.0
3.0
4.01.0
2.0
3.0
4.01.0
2.0
3.0
4.01.02.03.04.0 1.02.03.04.0
N/A
Kosmos-2
IDEFICS9BIDEFICS80B
InstructBLIP7BLLaVA7B
LLaVA-SFT+
7BLLaVA-RLHF7B
mPLUG-Owl2

Hình 6. Hiệu suất chi tiết của các mô hình khác nhau trên tám danh mục trong MMHal-Bench [54], trong đó "Tổng thể" đại diện cho hiệu suất trung bình trên tất cả các danh mục.

Chúng tôi đo lường ảo giác của mô hình trên mô tả hình ảnh sử dụng MMHal-Bench [54] và so sánh kết quả với các mô hình thị giác-ngôn ngữ gần đây khác, bao gồm Kosmos-2 [47], IDEFICS [28], InstructBLIP [15], LLaVA [38], và LLaVA-RLHF [54]. Theo [54], chúng tôi sử dụng GPT-4 để đánh giá điểm số tổng thể và tỷ lệ ảo giác của các MLLM khác nhau. Như được mô tả trong Hình 6, chúng tôi thấy rằng mPLUG-Owl2 có xu hướng tạo ra phản hồi với ảo giác giảm so với các phương pháp khác, đặc biệt vượt trội IDEFICS [28] với 80 tỷ tham số, cho thấy tính ưu việt của phương pháp của chúng tôi. Ngoài ra, chúng ta có thể nhận thấy rằng mô hình của chúng tôi xuất sắc về thuộc tính và đếm vì bộ trừu tượng hóa thị giác có thể xác định hiệu quả các phần chính của hình ảnh, giúp giảm ảo giác.

Chúng tôi cũng nghiên cứu ảo giác của các MLLM phổ biến gần đây và trình bày kết quả trong Hình 7. Trong ví dụ đầu tiên, câu hỏi yêu cầu các mô hình nhận diện hoa văn trên tường. Tuy nhiên, hoa văn không thể hiện rõ ràng trong hình ảnh, khiến các mô hình khác nhầm lẫn coi đó là màu đặc. Mô hình của chúng tôi, ngược lại, chính xác nhận thấy hoa văn trắng trên tường và trả lời đúng câu hỏi. Trong ví dụ thứ hai, chỉ có một vài cây trong hình ảnh. Tuy nhiên, InstructBLIP sai lầm khi cho rằng không có cây nào trong hình ảnh. LLaVA và LLaVA-1.5, mặt khác, có ảo giác và cho rằng cây trong hình ảnh dày đặc. MiniGPT-4 đưa ra câu trả lời đúng, nhưng với lời giải thích tối thiểu. mPLUG-Owl2 của chúng tôi, tuy nhiên, trả lời câu hỏi chính xác và cung cấp lời giải thích chi tiết hơn.

A.2. Đánh giá POPE
Chúng tôi cũng tiến hành đánh giá ảo giác sử dụng POPE [33], kết quả được hiển thị trong Bảng 9. Như chúng ta có thể quan sát trong bảng, chúng tôi thấy mPLUG-Owl2 đạt điểm F1 cao hơn trên phần phổ biến và đối kháng, cho thấy tính bền vững của mô hình về ảo giác đối tượng so với các MLLM khác.

A.3. Kết quả Đánh giá Chi tiết trên MMBench
MMBench [39] là một điểm chuẩn được thiết kế tỉ mỉ đánh giá toàn diện các kỹ năng đa dạng của các mô hình thị giác-ngôn ngữ. Kết quả từ tập kiểm tra cho các MLLM khác nhau được trình bày trong Bảng 10.

A.4. Kết quả Đánh giá Chi tiết trên MM-Vet
Chúng tôi cung cấp kết quả chi tiết của MM-Vet trong Bảng 11. Có thể quan sát rằng bằng cách huấn luyện bộ mã hóa thị giác của mPLUG-Owl2, nó thể hiện khả năng OCR mạnh hơn so với mô hình có cùng backbone (tức LLaVA, Otter). Ngoài ra, mPLUG-Owl2 vượt trội các mô hình có bộ giải mã ngôn ngữ mạnh hơn như LLaVA-13B được trang bị LLM với 13 tỷ tham số.

A.5. Kết quả Đánh giá Chi tiết trên Q-Bench
Để đánh giá khả năng nhận thức thị giác cấp thấp, chúng tôi đã bao gồm kết quả của Q-Bench [60] trên tập kiểm tra. Bằng cách huấn luyện bộ mã hóa thị giác, khả năng của mPLUG-Owl2 về nhận thức cấp thấp đã được cải thiện đáng kể, vì nó vượt trội mô hình có bộ mã hóa thị giác mạnh hơn (tức ViT-L (0.3B) so với ViT-G (1.9B)), cho thấy hiệu quả của mô hình huấn luyện của chúng tôi.

A.6. Kết quả Đánh giá Chi tiết trên MMHal-Bench
Chúng tôi bao gồm Bảng 13 cho kết quả đánh giá đầy đủ trên MMHal-Bench [54].

B. Triển khai
B.1. Hỗn hợp Dữ liệu
Trong phần này, chúng tôi chi tiết hỗn hợp dữ liệu huấn luyện cuối cùng được sử dụng trong giai đoạn điều chỉnh lệnh trong Bảng 14. Cụ thể, chúng tôi xử lý dữ liệu VQAv2 [21] bằng cách chọn câu trả lời có độ tin cậy cao nhất và kết hợp các cặp câu hỏi-trả lời chia sẻ cùng một hình ảnh. Chiến lược kết hợp này cũng được áp dụng cho các bộ dữ liệu GQA [24], OKVQA [43], và OCRVQA [44]. Ngoài ra, đối với các câu hỏi trắc nghiệm trong A-OKVQA [50], chúng tôi tăng cường bộ dữ liệu bằng cách thay đổi thứ tự các tùy chọn để nâng cao tính bền vững về lựa chọn nhiều phương án. Đối với các bộ dữ liệu chú thích như COCO [35] và TextCaps [53], chúng tôi chọn ngẫu nhiên một chú thích từ sự thật cơ bản

--- TRANG 13 ---
Tường có màu đặc trong hình ảnh này không?
Hình ảnh đầu vào: 

InstructBLIP LLaVA LLaVA-1.5 MiniGPT-4 mPLUG-Owl2
Truy vấn Hình ảnh đầu vào: 

Không, không có cây nào trong hình ảnh. Có, cây trong hình ảnh dày đặc, với số lượng lớn cành và lá. Có, cây trong hình ảnh này dày đặc. Cây không dày đặc trong hình ảnh này. Không, cây trong hình ảnh không dày đặc. Nó có vẻ là một cây nhỏ ở phía sau.

!

!

Cây có dày đặc trong hình ảnh này không?
Có, tường trong hình ảnh có màu đặc. Có, tường trong hình ảnh có màu đặc, màu nâu hoặc be. Có, tường trong hình ảnh có màu đặc, màu hồng. Tường trong hình ảnh này có vẻ là màu đặc. Không, tường trong hình ảnh này không có màu đặc. Nó có hoa văn hồng và trắng.
InstructBLIP LLaVA LLaVA-1.5 MiniGPT-4 mPLUG-Owl2
Truy vấn

!

!

Ví dụ #1

Ví dụ #2

Hình 7. Các trường hợp ví dụ so sánh với InstructBLIP [15], LLAVA [38], LLAVA-1.5 [37], MiniGPT-4 [75] và mPLUG-Owl2 của chúng tôi.

--- TRANG 14 ---
Bảng 9. Điểm chuẩn ảo giác đối tượng sử dụng đường dẫn đánh giá POPE. "Có" biểu thị khả năng của mô hình tạo ra phản hồi tích cực.

Bộ dữ liệu Chỉ số mPLUG-Owl2 Shikra [10] InstructBLIP [15] MiniGPT-4 [75] LLaVA [38] MM-GPT [20] mPLUG-Owl [68]
Ngẫu nhiên Độ chính xác (↑) 88.28 86.90 88.57 79.67 50.37 50.10 53.97
Độ chính xác (↑) 94.34 94.40 84.09 78.24 50.19 50.05 52.07
Nhớ lại (↑) 82.20 79.27 95.13 82.20 99.13 100.00 99.60
Điểm F1 (↑) 87.85 86.19 89.27 80.17 66.64 66.71 68.39
Có (→50%) 44.91 43.26 56.57 52.53 98.77 99.90 95.63
Phổ biến Độ chính xác (↑) 86.20 83.97 82.77 69.73 49.87 50.00 50.90
Độ chính xác (↑) 89.46 87.55 76.27 65.86 49.93 50.00 50.46
Nhớ lại (↑) 82.06 79.20 95.13 81.93 99.27 100.00 99.40
Điểm F1 (↑) 85.60 83.16 84.66 73.02 66.44 66.67 66.94
Có (→50%) 45.86 45.23 62.37 62.20 99.40 100.00 98.57
Đối kháng Độ chính xác (↑) 84.12 83.10 72.10 65.17 49.70 50.00 50.67
Độ chính xác (↑) 85.54 85.60 65.13 61.19 49.85 50.00 50.34
Nhớ lại (↑) 82.13 79.60 95.13 82.93 99.07 100.00 99.33
Điểm F1 (↑) 83.80 82.49 77.32 70.42 66.32 66.67 66.82
Có (→50%) 48.00 46.50 73.03 67.77 99.37 100.00 98.67

Phương pháp Mô hình ngôn ngữ Mô hình thị giác Tổng thể LR AR RR FP-S FP-C CP
MMGPT [20] LLaMA-7B CLIP ViT-L/14 16.0 1.1 23.8 20.7 18.3 5.2 18.3
MiniGPT-4 [75] Vicuna-7B EVA-G 12.0 13.6 32.9 8.9 28.8 11.2 28.3
InstructBLIP [15] Vicuna-7B EVA-G 33.9 21.6 47.4 22.5 33.0 24.4 41.1
LLaMA-Adapter-v2 [19] LLaMA-7B CLIP ViT-L/14 38.9 7.4 45.3 19.2 45.0 32.0 54.0
LLaVA [54] Vicuna-7B CLIP ViT-L/14 36.2 15.9 53.6 28.6 41.8 20.0 40.4
G2PT [39] Vicuna-7B ViT-G 39.8 14.8 46.7 31.5 41.8 34.4 49.8
Otter-I [30] LLaMA-7B CLIP ViT-L/14 48.3 22.2 63.3 39.4 46.8 36.4 60.6
mPLUG-Owl†[68] LLaMA-7B CLIP ViT-L/14 62.3 37.5 75.4 56.8 67.3 52.4 67.2
Shikra [10] Vicuna-7B CLIP ViT-L/14 60.2 33.5 69.6 53.1 61.8 50.4 71.7
mPLUG-Owl2 LLaMA2-7B CLIP ViT-L/14 65.4 29.2 69.7 61.7 67.0 60.0 79.5

Bảng 10. Kết quả độ chính xác đa lựa chọn CircularEval trên tập phát triển MMBench [39]. Chúng tôi sử dụng các viết tắt sau: LR cho Lý luận Logic; AR cho Lý luận Thuộc tính; RR cho Lý luận Quan hệ; FP-C cho Nhận thức Hạt mịn (Thể hiện Chéo); FP-S cho Nhận thức Hạt mịn (Thể hiện Đơn); CP cho Nhận thức Thô. Kết quả cơ sở được lấy từ [39].†biểu thị mô hình được tối ưu hóa cẩn thận cho MMBench.

cho mỗi hình ảnh. Đồng thời, một số bộ dữ liệu VQA khu vực [26, 69] cũng được sử dụng để cải thiện khả năng khu vực.

B.2. Siêu tham số Huấn luyện
Chúng tôi báo cáo cài đặt siêu tham số huấn luyện chi tiết của mPLUG-Owl2 trong Bảng 15. Cụ thể, chúng tôi tận dụng song song mô hình với khung huấn luyện phân tán Megatron [52] để đảm bảo huấn luyện độ phân giải lớn hơn trong khi duy trì hiệu quả.

C. Tóm tắt các Điểm chuẩn Đánh giá
Chúng tôi cung cấp tóm tắt chi tiết về các điểm chuẩn đánh giá được sử dụng và các chỉ số tương ứng trong Bảng 16.

D. Tác động Rộng hơn
mPLUG-Owl2 sử dụng LLM có sẵn và dữ liệu nguồn web. Do đó, nó thừa hưởng một số điểm yếu của LLM gốc và dữ liệu thu thập web, như tạo ra văn bản không được kiểm duyệt hoặc tạo ra các đầu ra thiên vị. Chúng tôi giải quyết những thiếu sót này bằng cách nâng cao việc căn cứ của mô hình trên đầu vào thị giác và hướng dẫn và thực hiện điều chỉnh lệnh thị giác-ngôn ngữ kết hợp trên một loạt các bộ dữ liệu chất lượng cao đa dạng. Tuy nhiên, chúng tôi khuyên không nên triển khai các mô hình mPLUG-Owl2 cho bất kỳ ứng dụng hạ nguồn nào mà không có đánh giá trước về an toàn và công bằng cụ thể cho ứng dụng tương ứng.

--- TRANG 15 ---
Mô hình Rec OCR Know Gen Spat Math Tổng
Transformers Agent (GPT-4) [46] 18.2 3.9 2.2 3.2 12.4 4.0 13.4 ±0.5
MiniGPT-4-7B [75] 27.4 15.0 12.8 13.9 20.3 7.7 22.1 ±0.1
BLIP-2-12B [31] 27.5 11.1 11.8 7.0 16.2 5.8 22.4 ±0.2
LLaVA-7B [38] 28.0 17.1 16.3 18.9 21.2 11.5 23.8 ±0.6
MiniGPT-4-13B [75] 29.9 16.1 20.4 22.1 22.2 3.8 24.4 ±0.4
Otter-9B [30] 27.3 17.8 14.2 13.8 24.4 3.8 24.7 ±0.3
OpenFlamingo-9B [3] 28.7 16.7 16.4 13.1 21.0 7.7 24.8 ±0.2
InstructBLIP-13B [15] 30.8 16.0 9.8 9.0 21.1 10.5 25.6 ±0.3
InstructBLIP-7B [15] 32.4 14.6 16.5 18.2 18.6 7.7 26.2 ±0.2
LLaVA-7B (LLaMA-2) [38] 32.9 20.1 19.0 20.1 25.7 5.2 28.1 ±0.4
LLaMA-Adapter v2-7B [19] 38.5 20.3 31.4 33.4 22.9 3.8 31.4 ±0.1
LLaVA-13B (V1.3) [38] 38.1 22.3 25.2 25.8 31.3 11.2 32.5 ±0.1
LLaVA-13B (LLaMA-2) [38] 39.2 22.7 26.5 29.3 29.6 7.7 32.9 ±0.1
mPLUG-Owl2 41.3 27.4 27.5 27.9 30.3 7.7 36.2±0.1

Bảng 11. Kết quả đánh giá trên các MLLM khác nhau về từng khả năng VL cốt lõi trên MM-Vet [70]. Rec là nhận diện; Know chỉ ra kiến thức; Gen là tạo ra; Spat có nghĩa là không gian. Tất cả các số được trình bày bằng % và điểm số đầy đủ là 100%.

Phương pháp Có-hoặc-không Gì Như thế nào Méo mó Khác Trong bối cảnh Méo mó Trong bối cảnh Khác Tổng thể
IDEFICS [28] 0.6004 0.4642 0.4671 0.4038 0.5990 0.4726 0.6477 0.5151
InstructBLIP [15] 0.7099 0.5141 0.4300 0.4500 0.6301 0.5719 0.6439 0.5585
Kosmos-2 [47] 0.6058 0.3124 0.3539 0.3865 0.4654 0.4349 0.4735 0.4334
LLaMA-Adapter-v2 [19] 0.6661 0.5466 0.5165 0.5615 0.6181 0.5925 0.5455 0.5806
LLaVA-1.5 [37] 0.6460 0.5922 0.5576 0.4798 0.6730 0.5890 0.7376 0.6007
LLaVA [38] 0.5712 0.5488 0.5185 0.4558 0.5800 0.5719 0.6477 0.5472
MiniGPT-4 [75] 0.6077 0.5033 0.4300 0.4558 0.5251 0.5342 0.6098 0.5177
mPLUG-Owl [68] 0.7245 0.5488 0.4753 0.4962 0.6301 0.6267 0.6667 0.5893
Otter [30] 0.5766 0.3970 0.4259 0.4212 0.4893 0.4760 0.5417 0.4722
Qwen-VL [5] 0.6533 0.6074 0.5844 0.5413 0.6635 0.5822 0.7300 0.6167
Shikra [10] 0.6909 0.4793 0.4671 0.4731 0.6086 0.5308 0.6477 0.5532
mPLUG-Owl2 0.7318 0.5531 0.5864 0.5374 0.7136 0.5788 0.7338 0.6294

Bảng 12. Kết quả đánh giá chi tiết cho các MLLM khác nhau trên tập kiểm tra Q-Bench [60].

Phương pháp Điểm số Tổng thể Ảo giác ↑ Tỷ lệ ↓ Điểm số Ảo giác Tổng thể trong Từng Loại Câu hỏi ↑
Thuộc tính Đối kháng So sánh Đếm Quan hệ Môi trường Toàn diện Khác
Kosmos-2 [47] 1.69 0.68 2.00 0.25 1.42 1.67 1.67 2.67 2.50 1.33
IDEFICS 9B[28] 1.89 0.64 1.58 0.75 2.75 1.83 1.83 2.50 2.17 1.67
IDEFICS 80B[28] 2.05 0.61 2.33 1.25 2.00 2.50 1.50 3.33 2.33 1.17
InstructBLIP 7B[15] 2.10 0.58 3.42 2.08 1.33 1.92 2.17 3.67 1.17 1.08
InstructBLIP 13B[15] 2.14 0.58 2.75 1.75 1.25 2.08 2.50 4.08 1.50 1.17
LLaVA 7B[38] 1.55 0.76 1.33 0.00 1.83 1.17 2.00 2.58 1.67 1.83
LLaVA-RLHF 7B[54] 2.05 0.68 2.92 1.83 2.42 1.92 2.25 2.25 1.75 1.08
mPLUG-Owl2 2.17 0.56 3.67 2.25 2.17 2.75 1.25 2.08 1.50 1.75

Bảng 13. Kết quả đánh giá chi tiết cho các MLLM khác nhau trên MMHal-Bench.

--- TRANG 16 ---
Loại dữ liệu Tên dữ liệu Kích thước
Văn bản ShareGPT [1] 40K
SlimOrca [34] 518K
Đối thoại LLaVA [38] 158K
Chú thích COCO [35] 82K
TextCaps [53] 22K
VQA VQAv2 [21] 83K
GQA [24] 72K
OKVQA [43] 9K
OCRVQA [44] 80K
A-OKVQA [50] 50K
VQA khu vực RefCOCO [69] 30K
VisualGenome [26] 86K
Tổng cộng 1.23M

Bảng 14. Hỗn hợp Dữ liệu Tuân thủ Lệnh của mPLUG-Owl2.

Cấu hình Tiền huấn luyện Điều chỉnh lệnh
Khởi tạo ViT CLIP-L/14 [48] Giai đoạn tiền huấn luyện
Khởi tạo LLM LLaMA-2 [58] LLaMA-2 [58]
Khởi tạo Trừu tượng hóa thị giác Ngẫu nhiên Giai đoạn tiền huấn luyện
Độ phân giải hình ảnh 224×224 448 ×448
Độ dài chuỗi ViT 256 1024
Độ dài chuỗi LLM 256 2048
Số lượng truy vấn học được 64 64
Bộ tối ưu hóa AdamW
Siêu tham số bộ tối ưu hóa β1= 0.9, β2= 0.98, ε= 1e−6
Tốc độ học đỉnh 1e−4 2e−5
Tốc độ học tối thiểu 1e−6 1e−7
Suy giảm tốc độ học ViT 0
Tỷ lệ drop path ViT 0
Lịch trình tốc độ học Cosin
Suy giảm trọng số 0.05 0
Cắt gradient 1.0
Bước huấn luyện 42,500 4,800
Bước khởi động 1,000 250
Kích thước batch toàn cục 8,192 256
Tích lũy gradient 16
Độ chính xác số bfloat16
Chia sẻ bộ tối ưu hóa ✓
Checkpoint kích hoạt ✓
Song song mô hình 1 2
Song song đường ống 1

Bảng 15. Siêu tham số huấn luyện của mPLUG-Owl2.

--- TRANG 17 ---
Nhiệm vụ Bộ dữ liệu Mô tả Phân chia Chỉ số
Chú thích hình ảnh COCO Chú thích hình ảnh tự nhiên karpathy-test CIDEr (↑)
Flickr30K Chú thích hình ảnh tự nhiên karpathy-test CIDEr (↑)
VQA tổng quát VQAv2 VQA trên hình ảnh tự nhiên test-dev Điểm VQA (↑)
OKVQA VQA trên hình ảnh tự nhiên yêu cầu kiến thức bên ngoài val Điểm VQA (↑)
GQA VQA về hiểu cảnh và lý luận test-balanced EM (↑)
VizWizQA VQA trên ảnh chụp bởi người mù test-dev Điểm VQA (↑)
TextVQA VQA trên hình ảnh tự nhiên chứa văn bản val Điểm VQA (↑)
SciQA-Img VQA trắc nghiệm trên một tập hợp đa dạng các chủ đề khoa học test Độ chính xác (↑)
VideoQA MSRVTT-QA Trả lời câu hỏi video test Độ chính xác (↑) / Điểm liên quan (↑)
MSVD-QA Trả lời câu hỏi video test Độ chính xác (↑) / Điểm liên quan (↑)
TGIF-QA Trả lời câu hỏi GIF test Độ chính xác (↑) / Điểm liên quan (↑)
Điểm chuẩn văn bản MMLU Một điểm chuẩn được thiết kế để đo lường việc thu thập kiến thức dev Độ chính xác (↑)
BBH Một bộ 23 nhiệm vụ BIG-Bench thách thức test Độ chính xác (↑)
AGIEval Một điểm chuẩn lấy con người làm trung tâm được thiết kế đặc biệt để đánh giá khả năng chung của mô hình nền tảng test Độ chính xác (↑)
ARC-c Một bộ dữ liệu trả lời câu hỏi trắc nghiệm, chứa câu hỏi từ kỳ thi khoa học từ lớp 3 đến lớp 9. test Độ chính xác (↑)
ARC-e Một bộ dữ liệu trả lời câu hỏi trắc nghiệm, chứa câu hỏi từ kỳ thi khoa học từ lớp 3 đến lớp 9. test Độ chính xác (↑)
Tuân thủ lệnh MME Điểm chuẩn VL mở bằng câu hỏi có/không Perception Độ chính xác (↑)
MMBench Điểm chuẩn VL mở bằng VQA trắc nghiệm với Đánh giá Vòng tròn test Độ chính xác (↑)
MM-Vet Điểm chuẩn VL mở với Các khả năng đa dạng test Điểm GPT-4 (↑)
SEED-Bench Điểm chuẩn VL mở bằng VQA trắc nghiệm Hình ảnh & Video Độ chính xác (↑)
Q-Bench Điểm chuẩn thị giác cấp thấp mở bằng VQA trắc nghiệm test Độ chính xác (↑)
Ảo giác POPE Tồn tại đối tượng bằng câu hỏi có/không ngẫu nhiên/phổ biến/đối kháng Độ chính xác / Độ chính xác / Nhớ lại / F1 (↑)
MMHal-Bench Điểm chuẩn ảo giác mở test Điểm GPT-4 (↑)

Bảng 16. Tóm tắt các điểm chuẩn đánh giá của mPLUG-Owl2. EM là viết tắt của khớp chính xác.

18
