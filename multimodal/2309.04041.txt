# 2309.04041.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.04041.pdf
# File size: 3321050 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Evaluation and Enhancement of Semantic Grounding
in Large Vision-Language Models
Jiaying Lu*‚ô†‚Ä†, Jinmeng Rao‚ô¶‚Ä†, Kezhen Chen‚ô¶, Xiaoyuan Guo‚ô¶, Yawen Zhang‚ô¶, Baochen Sun‚ô¶,
Carl Yang‚ô†, Jie Yang‚ô¶
‚ô¶Mineral,‚ô†Emory University
‚ô¶{jinmengrao, kezhenchen, xiaoyuanguo, yawenz, baochens, yangjie }@mineral.ai
‚ô†{jiaying.lu, j.carlyang }@emory.edu
Abstract
Large Vision-Language Models (LVLMs) offer remarkable
benefits for a variety of vision-language tasks. However, a
challenge hindering their application in real-world scenarios,
particularly regarding safety, robustness, and reliability, is
their constrained semantic grounding ability, which pertains
to connecting language to the physical-world entities or con-
cepts referenced in images. Therefore, a crucial need arises
for a comprehensive study to assess the semantic ground-
ing ability of widely used LVLMs. Despite the significance,
sufficient investigation in this direction is currently lacking.
Our work bridges this gap by designing a pipeline for gen-
erating large-scale evaluation datasets covering fine-grained
semantic information, such as color, number, material, etc.,
along with a thorough assessment of seven popular LVLMs‚Äô
semantic grounding ability. Results highlight prevalent mis-
grounding across various aspects and degrees. To address this
issue, we propose a data-centric enhancement method that
aims to improve LVLMs‚Äô semantic grounding ability through
multimodal instruction tuning on fine-grained conversations.
Experiments on enhanced LVLMs demonstrate notable im-
provements in addressing misgrounding issues.
Introduction
Large Vision-Language Models (LVLMs) (Zhao et al.
2023b; Tang et al. 2023; OpenAI 2023; Yang et al. 2023)
expand the powerful large language models (Ouyang et al.
2022; Touvron et al. 2023; Ling et al. 2023) to versatile
general-purpose vision-language understanding and genera-
tion interfaces. This is achieved through the integration of
the vision encoder and the autoregressive large language
model (Liu et al. 2023a; Li et al. 2023a; Luo et al. 2023).
While demonstrating promising performance in solving var-
ious vision-language benchmarks, comprehensive examina-
tion and analysis are desired before deploying LVLMs into
real-world critical-sensitive applications. Recent studies re-
veal that many LVLMs still suffer from text-image misalign-
ment (Yarom et al. 2023), adversary perturbed input (Zhao
et al. 2023a), and object hallucination (Li et al. 2023c) even
in some seemingly simple cases. In this paper, we specifi-
*Work was done when Jiaying Lu was an intern at Mineral.
‚Ä†These authors contributed equally.
Copyright ¬© 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.cally focus on the evaluation of the under-explored semantic
grounding ability in LVLMs.
Semantic grounding ( i.e.the capability to connect words
to the physical-world entities or concepts they refer to) (Yun,
Sun, and Pavlick 2021; Li et al. 2022) is critical to the safe,
robust, and reliable development of LVLMs. Although iden-
tifying ‚Äúa Siberian tiger‚Äù as ‚Äúa tabby cat‚Äù in the user-shared
image under a social chatbot setting seems harmless, stakes
escalate significantly when an LVLM-aided disease diagno-
sis assistant interprets instruction to analyze the patient‚Äôs
‚Äúleft lung‚Äù as ‚Äúright lung‚Äù. In this study, we comprehen-
sively evaluate the semantic grounding ability of existing
LVLMs through our proposed evaluation suite . Specifically,
we automatically generate 9,000vision-language test sam-
ples, exploring semantic grounding proficiency across four
formats and addressing six types of grounding targets. Seven
state-of-the-art LVLMs are evaluated using the 9,000 test
samples, and the experimental results reveal that most of
them exhibit semantic grounding deficiency across various
aspects and degrees. To facilitate the scalability of test sam-
ple generation and the interpretability of evaluation metrics,
we propose to adopt multiple-choice questions as the test
format. Further technical details are presented in the Evalu-
ation of Semantic Grounding Section.
Moreover, we introduce a data-centric enhancement
method designed to enhance the semantic grounding capa-
bilities of LVLMs. Diverging from classical model-centric
approaches (Zha et al. 2023) that emphasize advancements
in model architecture, our approach focuses on curating a
substantial volume of diverse multimodal, multi-round con-
versation data that can be leveraged by any LVLM. In to-
tal, we curate 180,000fine-grained instructional instances
for the purpose of semantic grounding enhancement. Exper-
imental results with LVLMs fine-tuned on our enhancement
data consistently demonstrate improvements in multimodal
semantic grounding.
Related Work
Trustworthiness Evaluation of LVLMs . Many works
leverage existing vision-language datasets to derive trust-
worthiness evaluation benchmarks for LVLMs. MME (Fu
et al. 2023) consists of 14 subtasks based on public im-
ages with manually constructed annotations, which measure
both perception and cognition abilities of LVLMs in thearXiv:2309.04041v2  [cs.CV]  13 Jan 2024

--- PAGE 2 ---
form of Yes-or-No question answering. The LAMM bench-
mark (Yin et al. 2023) covers nine common 2D image tasks
and three common point cloud tasks with specifically cu-
rated inference instruction. Other similar benchmarks in-
clude LVLM-eHub (Xu et al. 2023), MM-Vet (Yu et al.
2023), and MMBench (Liu et al. 2023b), etc. There also ex-
ist benchmarks focusing on evaluating specific properties of
LVLMs. POPE (Li et al. 2023c) focuses on evaluating ob-
ject hallucination by asking Yes-or-No questions regarding
the object existence of input images. M-HalDetect (Gun-
jal, Yin, and Bas 2023) proposes the hallucination task as
sentence-level classification, using human-annotated labels.
(Zhao et al. 2023a) proposes evaluating the robustness of
LVLMs by adding adversarial noise into input images. Our
work also provides a valuable resource to serve as a compre-
hensive trusworthiness benchmark for LVLMs, from a novel
perspective focusing on semantic grounding. Moreover, we
provide a generic framework to enable researchers to conve-
niently create test samples for evaluating LVLMs.
Methods for Building Safe, Robust, and Reliable
LVLMs . There are two streams of approaches to improve
the safeness, robustness, and responsibilities of LLMs and
LVLMs (Ji et al. 2023): model-centric approaches and data-
centric approaches. Model-centric approaches often focus
on the model advancements, which involve (1) designing ro-
bust training paradigms (Berg et al. 2022; Dong et al. 2023),
(2) robust inference (Wang et al. 2022; Zhang et al. 2023),
(3) refining generated response (Madaan et al. 2023), etc.
Data-centric approaches (Zha et al. 2023; Bai et al. 2024), on
the other hand, focus on ensuring data quality and reliability,
which often involve (1) faithful training data development
such as data collection (Liu et al. 2023a; Lu et al. 2023) and
data cleaning (Northcutt, Jiang, and Chuang 2021; Monarch
and Munro 2021), (2) inference stage data augmentation
such as retrieving supporting knowledge (Chen et al. 2022;
Cui et al. 2023). Our framework follows the data-centric
idea. Instead of modifying the model structure, our work
aims to improve the semantic grounding ability of LVLMs
via multimodal instruction tuning, which has proved to be a
generic and efficient approach for improving LVLMs.
Preliminaries
We illustrate one representative architecture of LVLMs in
Figure 1, which typically consists of the following compo-
nents:
1. a visual encoder that extracts features from the input im-
age;
2. a projector that projects visual features to the language
embedding space;
3. a tokenizer that tokenizes textual input into textual tokens
and maps them into language embedding space;
4. a decoder-only LLM (e.g., LLaMA) that generates tex-
tual responses based on the multimodal inputs.
The overall output generation process of a LVLM FŒòcan be
formally described by
Y=FŒò(Xv,Xq), (1)
ü¶ô
Large Language Model ‚Ä¶The image describes 
a  sun ‚Ä¶ Visual 
Encoder 
Projector Tokenizer Figure 1: One representative architecture of existing
LVLMs. Cyan hexagons denote visual embeddings, and
green hexagons denote textual embeddings.
where Œòare parameters of the LVLM, Xvdenotes the vi-
sual input, Xqdenotes the textual input, and Ydenotes the
generated output sequence.
Specifically, given a visual input Xv, a visual encoder fœï
firstly extracts visual features by
Zv=fœï(Xv), (2)
whereZv‚ààRlv√ód, and œïare parameters of fœïtypically
frozen in LVLMs training. Here lvdenotes the length of vi-
sual tokens, and dis the dimension of visual tokens. Re-
garding the visual encoder fœï, most existing LVLMs employ
ViT-based structures (Dosovitskiy et al. 2020) and select cer-
tain layers outputs to construct a certain length of visual to-
kens. For instance, LLaV A (Liu et al. 2023a) utilizes grid
features before and after the last Transformer layer of ViT,
while LaVIN (Luo et al. 2023) utilizes the [CLS]embed-
dings from every fourth layer of ViT.
A trainable projector, denoted as gœâ, is then applied to
convert Zvinto language embedding space, which can be
defined by
Hv=gœâ(Zv) =gœâ(fœï(Xv)), (3)
whereHv‚ààRlv√óh, and œâare trainable parameters of gœâ.
hdenotes the dimension of language embedding space. In
practise, an efficient implementation of gœâcan be a projec-
tion matrix W‚ààRd√óh(Liu et al. 2023a), thus Hv=ZvW.
More sophisticated projectors are also proposed, such as Q-
former (Li et al. 2023b).
Given the input textual query Xq, a tokenizer kœàis em-
ployed to tokenize and map them into ‚Äútextual tokens‚Äù by:
Hq=kœà(Xq), (4)
whereHq‚ààRlq√óh, andœàare trainable parameters of kœà. It
is worth noting that Hqand projected Hvhave the same di-
mensionality Rhas the input of the large language model pŒ∏.
Similarly, the length of textual tokens varies depending on
the choice of tokenizer, as words are chunked into subwords.
Moreover, special tokens [s],[/s]can be added to indicate
the span of visual tokens and textual tokens. Therefore, we
denote the process of LVLM to prepare the multimodal input
of its LLM component as:
H= [Hv,Hq]‚ààR(lq+lv)√óh. (5)

--- PAGE 3 ---
Once the multimodal input features are processed, the
LLM component of an LVLM is responsible for generating
responses. The LLM component is essentially a probabilis-
tic auto-regressive model pŒ∏with trainable parameters Œ∏that
can predict the next token ytstep by step based on the input
Hand tokens predicted so far Y0:t‚àí1. The process can be
formulated by:
Pr(Y0:œÑ|H) =œÑY
t=1pŒ∏(yt|H,Y0:t‚àí1). (6)
where Prdenotes the probability of generating the output
sequence Y0:œÑ.
Evaluation of Semantic Grounding
A wide range of forms can be used for the evaluation of
semantic grounding in LVLMs, and each form has its own
advantages and disadvantages. Free-form questions (Yarom
et al. 2023) are easy to design but require resource-intensive
human evaluations and are difficult to score consistently.
Similarity-based evaluation uses less resources, but is heav-
ily reliant on high-quality ground truth responses and the
bias of similarity metrics. Yes-or-No questions (Fu et al.
2023) are non-ambiguous and easier to evaluate. However,
they may be too easy and cannot capture all aspects of se-
mantic grounding in LVLMs.
We propose MSG-MCQ for the novel Multimodal
Semantic Grounding evaluation based on Multiple- Choice
Questions (MCQs) (Lu et al. 2022a,b). This form presents a
question along with a set of predetermined choices, allow-
ing respondents to select the option they believe to be cor-
rect. The MCQs facilitate efficient grading and analysis of
responses, and the difficulty level can be controlled by ad-
justing the number and the feasibility of distractor choices
(Lu et al. 2022a,b). Moreover, the Yes-or-No form can be
regarded as a special case in MCQs where the choices are
‚Äú(A)Yes. (B) No.‚Äù. We use accuracy as the evaluation met-
ric for MCQs. Following the notation in the Preliminaries
section, we extend the textual input question Xqinto two
parts: the question body Xqband the multiple choices Xqc.
Therefore, we formally define the task of M SG-MCQ:
Definition 1 (M SG-MCQ)Given an input image Xv, in-
put question body Xqb, and Kinput choices Xqc=
{C1,C2, . . . ,CK},MSG-MCQexpects a LVLM FŒòto se-
lect the correct choice Ciwhere 1‚â§i‚â§Kfrom the input
choices Xqc.
In the scope of LVLM, the selection action is typically de-
termined by the generated sequence of LVLM. Depending
on the specific implementation of LVLMs, the output can be
as simple as a choice indicator like ‚ÄúC‚Äù, or a comprehen-
sive response that includes both the selected choice and an
explanation of selecting it such as ‚ÄúThe correct choice is A,
because . . .‚Äù.
MSG-MCQGeneration Pipeline
In this work, we propose an efficient automated MCQ gen-
eration method to derive our M SG-MCQ. Figure 2 pro-
vides several automatically generated evaluation examples.Specifically, we pose four specific kinds of MCQs: Yes-or-
No, Fill-in-the-Blank, What, Correction in different columns
in Figure 2. Orthogonal to the specific kinds of MCQs,
the evaluation focuses on different targets of semantic
grounding that indicate the particular deficiency of evalu-
ated LVLMs. In this work, we include six targets of seman-
tic grounding: Entity, Number, Color, Material, Action, and
Spatial . These target categories are shown in different rows
in Figure 2. It is worth noting that the evaluation module can
be easily extended to more types and targets in the future.
All these MCQs are generated from ground-truth source
data through a four-step generation pipeline. In step 1 , we
curate a pool of question templates with placeholders for
each kind of MCQ focusing on one specific target of seman-
tic grounding, according to the source data we have. In step
2, we randomly sample one question template from the pool.
As an illustration, a question template of What question fo-
cusing on color grounding is:
Example 1 (What question) What color of [obj-attr] ob-
ject is featured in [bbox-color] bounding box of the image?
(A) [distractor#1] (B) [ground-truth]
(C) [distractor#2] (D) [distractor#3].
As can be seen, the sampled question template con-
tains several placeholders, and the ground-truth is randomly
placed into the choice (A), (B), (C), or (D). In step 3 , we
use source data to fill in these placeholders. For example,
‚Äú[obj-attr], [ground-truth] ‚Äù can be filled using the gold an-
notation in the source data. Regarding the ‚Äú [bbox-color] ‚Äù,
we first use gold bounding box coordinates referring to the
querying object to draw a box in the original image using
a random color (green or red), then replace ‚Äú [bbox-color] ‚Äù
with the name of that color. In step 4 , we generate the dis-
tractors of ground truth based on the multimodal input infor-
mation. This is one of the most critical steps in the evaluation
pipeline, since distractors determine the difficulty level of
MCQs. Distractors should not be semantically equivalent to
ground truth, but they should be plausible enough to serve as
an answer candidate to the question. Various distractor gen-
eration methods (Lu et al. 2022a) can be used, such as man-
ual generation, thesaurus-based generation, and end-to-end
generation. In this work, we utilize thesaurus-based genera-
tion with post-human verification. According to the type of
ground truth (entity, number, color, etc.), we randomly sam-
ple15distractors. For entity grounding, we use the sentence
transformer model (Reimers and Gurevych 2019) to filter
out candidates that have overly high similarity scores to the
gold answer. For other targets of grounding, the thesaurus
is guaranteed to contain semantically different antonyms of
the gold answer.
We introduce the necessary tweak of the overall pipeline
for each specific kind of MCQ, and explain the purpose of
why we need them as below:
‚Ä¢Yes-or-No: Can a model identify whether a textual de-
scription is appropriate for a given image? For each Yes-
or-No MCQ, two choices are given, as shown in the first
column of Figure 2. The correct textual descriptions are
either directly taken from the source data (image caption-
ing datasets), or obtained using some sentence templates

--- PAGE 4 ---
Is the text ‚ÄúTwo baseball players in striped uniform are hugging‚Äô‚Äô genuinely describe the image?(A)Yes(B) No
Based on the image details, select the appropriate option to fill in the blank: ‚ÄúA guy kayaking through ____‚Äù.(A) a gravel field (B) a long city street (C) wavy water(D) two glasses of water
Identify the optimal revision for the statement of the image: ‚ÄúA yellow poodle playing on a beach.‚Äù(A) A yellow poodle -> Two dogs(B) A yellow poodle -> Some cattle(C) A yellow poodle -> A pair of birds(D) none of the above
üêßEntity GroundingYes-or-NoFill-in-the-BlankCorrection
Do you perceive any factual mistakes within the image textual description ‚ÄúFive men on a street, one holding a red flag‚Äù?(A)Yes(B) NoIndicate the correct choice to complete the sentence: ‚Äú____ females are climbing a rock wall outdoors‚Äù.(A) Five  (B) 3 (C) Two(D) FourDetermine the most appropriate modification for the image caption: ‚ÄúThree men are riding down the street on a bicycle next to a gray building.‚Äù(A) Three men -> One men(B) Three men -> Two men(C) Three men -> Four men(D) none of the above
üî¢Number GroundingYes-or-NoFill-in-the-BlankCorrection
Does the caption "a vase that is green" genuinely describe the red        bounding box?(A) Yes (B) NoWhat type of plastic object-part is featured in the green bounding box of the image?",(A) orange    (B) dark orange(C) light red  (D) pinkDetermine the most appropriate modification for the green bounding box's caption: "a blue opaque pillow".(A) change "blue" to "brown" (B) change "blue" to "dark yellow". (C) change "blue" to "dark orange". (D) none of the above.",
üåàColor GroundingYes-or-NoWhatCorrection
Would you consider the text, ‚Äúa belt made from leather and metal‚Äù, to be factually contradictory with the green bounding box in the image?(A)Yes (B) NoIndicate the option that characterizes the attribute of black object-part for the green bounding box within the image.(A) rattan      (B) paper(C) glass      (D) leatherFix the description of the red bounding box by choosing one option: ‚Äúa scarf with plain plastic fringes‚Äù.(A) change ‚Äúplastic‚Äù to ‚Äúwood‚Äù(B) change ‚Äúplastic‚Äù to ‚Äúfabric‚Äù(C) change ‚Äúplastic‚Äù to ‚Äúleather‚Äù(D) none of the above
ü™®Material GroundingYes-or-NoWhatCorrection
Would you consider the text, "man reads rugby ball", to be factually contradictory with the green bounding box and the red bounding box in the image?(A)Yes(B) NoWhat kind of relationship exists between the red bounding box (denoted as <red>)and the green bounding box (denoted as <green>)?(A) <red> eats <green> (B) <red> kicks <green> (C) <red> throws <green> (D) <red> hangs <green>Which choice provides the most accurate amendment to the textual description "girl surfs surfboard", aligning it factually with the relation of the green bounding box and the red bounding box?",(A) change "surfs" to "hugs" (B) change "surfs" to "dances"(C) change "surfs" to "plays"(D) none of the above.
üëüAction GroundingYes-or-NoWhatCorrection
Is the caption ‚Äústuffed animal in basketball net‚Äô‚Äô genuinely describe the red bounding box and green bounding box?(A)Yes(B) NoCan you describe the association between the green bounding box (denoted as <green>)and the red bounding box (denoted as <red>)in the image?(A) <green > on <red> (B) <green> in <red> (C) <green> behind <red> (D) <green> above <red>Amend the description of the green bounding box and the red bounding box in the image by choosing one of the provided options: ‚Äúhorse next to grass‚Äù.(A) ‚Äúnext to‚Äù -> ‚Äúto the left of‚Äù(B) ‚Äúnext to‚Äù -> ‚Äúto the right of‚Äù(C) ‚Äúnext to‚Äù -> ‚Äúabove‚Äù(D) none of the above
üìçSpatial GroundingYes-or-NoWhatCorrection
Figure 2: Instances of our M SG-MCQ, which covers four types of MCQs and six types of grounding targets.
(object detection). The wrong descriptions are generated
by negative replacement on correct descriptions, where
we change specific textual spans for corresponding se-
mantic grounding target ( e.g.sampling ‚Äúyellow‚Äù to re-
place the original ‚Äúblue‚Äù in color grounding).
‚Ä¢Fill-in-the-Blank. Can a model infer the missing pieces
of information regarding multimodal input? For each
Fill-in-the-Blank MCQ, four choices are given where
only one choice is correct. The second column of the first
two rows of Figure 2 gives real examples of such type
MCQs. A text span that contains the concept of interest
(e.g.entity, number) is blanked, and the span is used as
one choice. Three distracting choices are generated using
negative sampling.‚Ä¢What. Can a model recognize and identify an object or
attribute that is specified in the multimodal input? For
each What MCQ, four choices are given where only one
choice is correct. The second column of the last four rows
of Figure 2 gives real examples of such type MCQs. Sim-
ilarly, three distracting choices are generated using nega-
tive sampling.
‚Ä¢Correction Can a model identify the inconsistency
across different modalities and propose an appropriate
correction? For each Correction MCQ, four choices are
given where only one choice is correct. The last column
of Figure 2 provides real examples of such type MCQs.
The choice ‚Äú(D) none of the above‚Äù is always included in
each Correction MCQ, indicating the scenario where the

--- PAGE 5 ---
original description is grounded. Correction MCQs are
challenging, as they require models to imagine whether
the corrected text is consistent with the image.
Enhancement of Semantic Grounding
Our solution for enhancing the semantic grounding of
LVLMs is a data-centric approach (Zha et al. 2023) to
systematically and algorithmically generate the instruction-
tuning dataset to feed LVLMs. The emerging success of
transformer model architectures in both large language mod-
els (Ouyang et al. 2022; OpenAI 2023; Touvron et al. 2023)
and large vision-language models (Zhu et al. 2023; Awadalla
et al. 2023) advocates a fundamental shift from the model-
centric AI to data-centric AI. Instead of focusing on de-
signing specific model architectures for a particular down-
stream task, we curate a substantial (180K instances) of new
multimodal multi-round conversational instruction data for
LVLMs to be further improved upon.
Instruction Tuning Data Generation
Image
User What colors are the strawberries the man is
holding?
LVLM The man is holding one white strawberry and
one red strawberry.
User Do you think the white strawberry is to the right
of the red strawberry?
LVLM No, the white strawberry appears to be on the
left of the red strawberry in the image.
Table 1: Toy Example of Multimodal Multi-Round
Instruction-Tuning Data
For the instruction data generation, we first curate a set
of fine-grained conversations of multimodal inputs that help
LVLMs interpret the information in the image step by step
and eventually answer the question. Table 1 provides a toy
example of such instruction data. All the conversations are
provided as multi-step chain-of-thought (Zhang et al. 2023)
instructions to simulate the human reasoning process and
guide LVLMs to pay attention to the right information at
each step. Specifically, we design three types of instructions:
‚Ä¢Multi-Round Conversation . Given an image, we pro-
vide a multi-round conversation between users and mod-
els. The user asks a few questions about some fine-
grained content in the image and the LVLM gives factual
and precise answers based on the observation.‚Ä¢Vision-Prompted Recognition . Given an image, we
draw visual prompts (e.g., bounding boxes) on the im-
age and ask an LVLM to tell the names, attributes, or
relations of the objects indicated by visual prompts. The
LVLM needs to learn how to follow the visual prompts
to first localize the indicated objects and then recognize
detailed attributes or relations of them.
‚Ä¢Fact Checking . Given an image and a statement of some
facts in the image, we ask the LVLM if and why a given
statement is factually consistent. In the instructions, we
provide reference answers to guide the LVLM to localize
main objects, recognize their attributes and relations, and
determine if there are any factual misalignments. Even-
tually, the LVLM determines whether it is factual.
All three types of instructions essentially consist of two fun-
damental components: the question and the response. We
manually curate diverse templates for both questions and re-
sponses. In a similar fashion, the response templates include
placeholders to be filled with ground-truth data. Once these
placeholders are populated, we employ chatGPT to rephrase
these instructions, enhancing their diversity.
Instruction Tuning
After the fine-grained conversations are generated, we con-
duct instruction tuning (Liu et al. 2023a; Ouyang et al. 2022)
on LVLMs to enhance semantic grounding. Following the
notation we introduce in the Preliminary section , the infer-
ence process of LVLM can be described as:
ÀÜY=FŒò(Xv,Xq), (7)
where FŒòdenotes the LVLM with trainable parameters Œò,
Xvdenotes the visual inputs, Xqdenotes the textual in-
puts, and ÀÜYdenotes the textual response. It is worth not-
ing that the trainable parameters ŒòofFactually contains
Œò = {œï, œâ, œà, Œ∏ }. We denote the n-th training sample
(Xv
(n),Xq
(n),Yr
(n))from our generated instruction tuning
data, where they represent the visual input, textual query in-
put, and the ground truth response, respectively. The loss
function of LVLM is
L(Œò) = ‚àíNX
n=1log Pr (Yr
(n)|Xv
(n),Xq
(n)), (8)
where Prdenotes the probability of generating the output
sequence Yr
(n)from input (Xv
(n),Xq
(n))using equation (6).
Experiments and Analysis
Evaluation Examples
Table 2 gives an overview of key statistics of test multiple-
choice questions generated by M SG-MCQ. We collect six
subsets for each semantic grounding type, together with
9K MCQs, 8.1K images, and 1.7K unique answers. These
MCQs are built from four public datasets: Flickr30K (Young
et al. 2014), PACO (Ramanathan et al. 2023), OpenImage-
V7 (Krasin et al. 2017), SpatialSense (Yang, Russakovsky,
and Deng 2019). Flickr30K is an image captioning dataset
that includes images obtained from Flickr and each im-
age is provided with five manually annotated captions.

--- PAGE 6 ---
Grounding Target Q Type #IMGs #Qs #As Data Source
Entity YoN, FiB, Corr 1,339 1,500 628 Flickr30K
Number YoN, FiB, Corr 977 1,500 389 Flickr30K
Color YoN, What, Corr 1,500 1,500 153 PACO
Material YoN, What, Corr 1,500 1,500 68 PACO
Action YoN, What, Corr 1,498 1,500 322 OpenImage-V7
Spatial YoN, What, Corr 1,500 1,500 144 SpatialSense
Overall YoN, FiB, What, Corr 8,119 9,000 1,683
Table 2: Overview of MCQs generated by M SG-MCQ. #IMGs denotes unique images, #Qs denotes unique questions, and #A
denotes unique answers.
PACO, OpenImage-V7 and SpatialSense are object de-
tection datasets that contain fine-grained object/object-part
bounding boxes, categories, and attribute annotations. Fur-
thermore, OpenImage-V7 and SpatialSense provide rela-
tional annotations between two objects within one image.
We curate these testing MCQs by our proposed M SG-MCQ.
Also, we balance the answer distribution and the concept of
semantic grounding target for promising evaluation.
Evaluated Models
We select seven SOTA LVLMs for evaluation and all use the
7B parameters variants for a fair comparison:
‚Ä¢ mPLUG-Owl (Ye et al. 2023), MiniGPT4 (Zhu et al.
2023), LLaV A (Liu et al. 2023a), InstructBLIP (Dai et al.
2023): LVLMs based on the visual encoder and the pre-
trained LLM that are similar to the architecture we in-
troduce in the Preliminary Section . Both follow a two-
stage training, where stage-1 is a pre-training stage to
align visual and textual concepts when LLM is frozen,
and stage-2 is a fine-tuning stage to feed into instruction-
following data to train both visual encoder and LLM si-
multaneously.
‚Ä¢ Otter (Li et al. 2023a), LLaMA-AdapterV2 (Gao et al.
2023), LaVIN (Luo et al. 2023): LVLMs with a slightly
different architecture, where visual features after adap-
tation/fusion are attended with text embedding in each/-
some layers of the LLM component.
We also test one large language model (language-only):
LLaMA-2-chat (Touvron et al. 2023) to verify the necessity
of multimodal perception ability for answering the gener-
ated MCQs. To extract the choice indices (A, B, C, or D)
from the free-form responses of LVLMs, we use regular ex-
pressions if they explicitly contain indices. Otherwise, we
use ChatGPT to help extract indices.
Zero-Shot Setting
Figure 3 provides the radar charts of seven advanced LVLMs
on each semantic grounding target or each question type.
Among these LVLMs, Otter, LLaV A, and LaVIN are top
competitors that consistently deliver better performance
than other LVLMs. In terms of semantic grounding targets,
LVLMs are more effective at perceiving and understanding
Entity and Action, but extremely struggle in Spatial rela-
tions. We speculate the different performances on differentkinds of semantic grounding come from the bias of training
data used by LVLMs. mPLUG-owl, LLaMA-AdapterV2,
MiniGPT-4, and InstructBLIP all mainly trained on coarse-
grained text-image pairs corpus. On the other hand, Otter
has been trained on their curated MIMIC-IT corpus that cov-
ers perception, reasoning, and planning-oriented text-image
QA pairs. LLaV A has been trained on not only the cap-
tions, but also bounding boxes with detailed descriptions.
The model checkpoint of LaVIN we used here is trained on
ScienceQA (Lu et al. 2022b) corpus, which is in the format
of MCQs. These additional training corpora provide more
fine-grained information on the multimodal input, thus help-
ing these LVLMs achieve better performance.
Table 3 provides a detailed view of the zero-shot accuracy
including human, random-guess, and LLaMA2 baselines.
The accuracy of random-guess in each semantic ground-
ing type is always 33.33, since each semantic grounding
subset contains 500 two-way ( 50.00accuracy by random
guess) and 1000 four-way MCQs ( 25.00accuracy by ran-
dom guess). Human performance (five annotators carefully
answer random 500 MCQs from M SG-MCQ) sets up the up-
per bound of the zero-shot experiments, which significantly
outperforms all other LVLMs. Interestingly, humans do not
perform well in color, material, and spatial. One possible
reason for human deficiency in color and material is that
human perception organs are a bit weaker at distinguish-
ing attributes, as compared to recognizing objects and ac-
tions. For Spatial MCQs, we observe that the source data
quality is not high. For example, sometimes the spatial re-
lations are labeled according to the physical position, while
sometimes they are labeled according to what the annota-
tor perceived. Comparing the language only LLaMA2-chat
and other LVLMs, the best LVLMs are consistently better
in every semantic grounding type. This indicates the impor-
tance of the perception ability of input vision modality. Be-
sides, LVLM models struggle more with correction ques-
tions, which is also more challenging for humans.
Moreover, Table 4 supplies LVLMs performance on dif-
ferent Q-Types with human, random-guess, and LLaMA2
baselines. As can be seen, Correction type MCQs are most
challenging for both humans and LVLMs. Given the ac-
curacy of the Random-Guess baseline as 25.00(four-way
MCQs with one correct answer), the best LVLM (Otter)
only outperforms it by 11.63. While for other four-way What
and Fill-in-the-Blank MCQs, the best LVLM achieves 17.70

--- PAGE 7 ---
EntityNumber Color
Material
Action Spatial102030405060
mPLUG-owl LLaMA-AdapterV2 InstructBLIP
miniGPT-4 LLaV A Otter LaVIN
(a) Zero shot accuracy on each semantic grounding target.Yes-or-NoWhat
Fill-in-
the-Blank
Correction102030405060
mPLUG-owl LLaMA-AdapterV2 InstructBLIP
miniGPT-4 LLaV A Otter LaVIN
(b) Zero shot accuracy on each question type.
Figure 3: Comparison of seven advanced LVLMs on M SG-MCQgenerated MCQs in accuracy (maximum 100).
Entity Number Color Material Action Spatial Overall
Human 94.74 83.95 75.95 79.55 91.14 70.10 81.00
zero-shot
Random-Guess 33.33 33.33 33.33 33.33 33.33 33.33 33.33
LLaMA2-chat 39.20 35.13 34.60 39.60 38.07 34.67 36.88
mPLUG-owl 40.53 34.93 32.73 33.60 34.93 30.27 34.50
LLaMA-AdapterV2 40.20 35.20 32.93 36.13 31.67 21.33 32.91
LaVIN 47.93 37.27 33.33 40.47 40.46 33.53 38.83
MiniGPT-4 40.00 35.07 33.67 35.53 38.27 34.93 36.24
LLaV A 46.67 34.27 35.20 38.53 43.73 35.00 38.90
Otter 51.13 36.87 42.20 44.47 46.87 36.13 42.94
InstructBLIP 30.53 18.00 30.40 37.33 49.67 31.33 29.63
Table 3: Accuracy on M SG-MCQdifferent targets of semantic grounding.
YoN What FiB Corr. Overall
Human 78.63 85.15 91.11 76.42 81.00
zero-shot
Random-Guess 50.00 25.00 25.00 25.00 33.00
LLaMA2-chat 50.20 28.55 37.20 9.00 36.88
mPLUG-owl 48.77 25.75 35.30 25.80 34.50
LLaMA-AdapterV2 55.90 18.55 51.90 13.17 32.91
LaVIN 49.00 28.85 51.10 31.23 38.83
MiniGPT-4 47.77 30.45 37.90 28.03 36.24
LLaV A 50.77 34.65 43.70 28.27 38.90
Otter 48.53 40.70 49.60 36.63 42.94
InstructBLIP 29.63 42.70 13.10 33.87 29.63
Table 4: Experimental results (Accuracy) on M SG-MCQdif-
ferent types of questions (Q-Type).
and26.10accuracy gains, separately. On the other hand,
Yes-or-No type MCQs are quite challenging too. Given the
Random-Guess accuracy as 50.00, the best LVLM (LLaMA-Adapter) only achieves 5.9higher accuracy. The patterns
of experimental outcomes indicate that LVLMs are better
at responding to descriptive queries (What and Fill-in-the-
Blank MCQs). One speculation for that is these LVLMs have
been trained on a great amount of image captioning data and
image description instruction data. With the pretraining on
such training corpora, LVLMs may be relatively less pre-
pared for judgemental and corrective queries.
We further conduct an in-depth analysis using critical dif-
ference (CD) analysis (Dem Àásar 2006). Figure 4 shows the
CD diagram, which is a powerful tool to compare outcomes
of multiple compared models over multiple observations.
The CD analysis involves several hypothesis tests, and mod-
els connected with each other in the diagram means that the
performances of these models are not that different in the
sense of statistical significance. As shown in Figure 4, the
performances of LVLMs are relatively similar, although they
can be divided into two groups. Noticeably, there still ex-

--- PAGE 8 ---
Entity Number Color Material Action Spatial Overall
LLaMA-AdapterV2+ 53.00 (‚Üë12.80) 47.80 (‚Üë12.60) 44.40 (‚Üë11.47) 48.93 (‚Üë12.80) 46.20 (‚Üë14.53) 43.53 (‚Üë22.20) 47.31 (‚Üë14.40)
mPLUG-owl+ 51.61 (‚Üë11.08) 48.81 (‚Üë13.48) 39.92 (‚Üë7.19) 43.10 (‚Üë9.50) 44.63 (‚Üë9.70) 39.70 (‚Üë9.43) 43.66 (‚Üë9.16)
LaVIN+ 47.20 (‚Üë9.27) 49.53 (‚Üë12.26) 66.80 (‚Üë33.47) 73.33 (‚Üë32.86) 69.99 (‚Üë29.54) 49.80 (‚Üë16.27) 61.11 (‚Üë22.28)
LLaV A+ 60.27 (‚Üë13.60) 45.20 (‚Üë10.93) 41.27 (‚Üë6.07) 60.00 (‚Üë21.47) 72.93 (‚Üë29.20) 37.47 (‚Üë2.47) 52.86 (‚Üë13.96)
Otter+ 62.73 (‚Üë11.60) 46.20 (‚Üë9.33) 48.60 (‚Üë6.40) 61.60 (‚Üë17.13) 57.53 (‚Üë10.66) 40.47 (‚Üë4.34) 52.85 (‚Üë9.91)
Table 5: Accuracy (numbers in regular font) and accuracy gain (numbers in italic font in parentheses) on M SG-MCQdifferent
grounding targets with enhanced LVLMs (denoted as [method+] ).
YoN What FiB Corr. Overall
LLaMA-AdapterV2+ 51.63 (‚Üì4.27) 33.75 (‚Üë15.20) 47.20 (‚Üì4.70) 52.07 (‚Üë38.90) 47.31 (‚Üë14.40)
mPLUG-owl+ 56.04 (‚Üë7.27) 37.93 (‚Üë12.18) 53.00 (‚Üë17.70) 34.72 (‚Üë8.92) 43.66 (‚Üë9.16)
LaVIN+ 74.00 (‚Üë25.00) 52.20 (‚Üë23.35) 35.90 (‚Üì15.20) 62.56 (‚Üë31.33) 61.11 (‚Üë22.28)
LLaV A+ 61.04 (‚Üë10.27) 49.65 (‚Üë15.00) 58.30 (‚Üë15.60) 44.67 (‚Üë16.40) 52.86 (‚Üë13.96)
Otter+ 53.70 (‚Üë5.17) 41.10 (‚Üë0.40) 57.90 (‚Üë8.30) 57.06 (‚Üë21.53) 52.85 (‚Üë9.91)
Table 6: Accuracy (numbers in regular font) and accuracy gain (numbers in italic font in parentheses) on M SG-MCQdifferent
question types with enhanced LVLMs (denoted as [method+] ).
1 2 3 4 5 6 7 8
Human
Otter
LaVIN
LLaV A InstructBLIPMiniGPT4LLaMA-AdapterV2mPLUG-owl
Figure 4: The critical difference diagrams of each LVLM
for each semantic grounding target with each question type.
The lower rank (further to the right) represents the better
performance. LVLMs connected by thick bars indicate that
these models are not significantly different ( p <0.05).
ists a significant gap between humans and all LVLMs. Over-
all, M SG-MCQserves as a good evaluation module for re-
searchers to understand the limitations of LVLMs with re-
gard to specific fine-grained semantic grounding.
Enhanced LVLMs Setting
In order to enhance semantic grounding in LVLMs, we con-
duct instruction tuning1on LVLMs by generating 180K
fine-grained multimodal instruction data covering multi-
round conversations, vision-prompted recognition, and fact-
checking. Table 5 shows the performance gains on several
enhanced LVLMs by instruction tuning. Following the same
format, we use bold font to highlight the best accuracy or
accuracy gain, and underline font to highlight the second-
best accuracy or accuracy gain. For those LVLMs not in-
cluded, they typically do not release well-established in-
struction tuning scripts (or scripts are specifically for im-
age captioning tasks). As can be seen, we observe consistent
improvements over all tuned LVLMs, which indicates the
effectiveness of our enhancement method. Interestingly, the
performance gain for LA VIN is significantly higher than for
other models. We believe this is primarily due to LA VIN in-
1Instruction tuning details are elaborated in the Appendix.corporating trainable adaptors into both the vision encoder
and the LLM, enabling end-to-end optimization of the en-
tire model. Moreover, we supply Table 6 that offers a de-
tailed breakdown of accuracy improvements based on MCQ
types. A closer investigation of the table reveals that while
performance enhancements are evident across most MCQs,
there are instances of decreased accuracy in specific MCQ
types. For instance, LaVIN exhibits a notable decrease of ‚Üì
15.20accuracy in Fill-in-the-Blank MCQs, while LLaMA-
AdapterV2 records slightly lower accuracies in Yes-or-No
and Fill-in-the-Blank MCQs. Despite these isolated varia-
tions, consistent performance improvements are observed in
other categories. In summary, LVLMs enhanced with our au-
tomatically generated instruction data deliver better seman-
tic grounding performance. The instruction data is funda-
mentally different from the training data, since instruction
data does not share the same data distribution and formats of
the M SG-MCQtesting data. The instruction data also serves
as a valuable resource for instruction tuning, readily avail-
able for any LVLMs to utilize.
Conclusion and Future Work
In this work, we evaluate and enhance the ability of LVLMs
to ground fine-grained vision-language inputs. We propose
an evaluation method M SG-MCQto automatically gener-
ate testing samples focusing on specific grounding targets
in various question formats, along with comprehensive ex-
periments to understand how well the current state-of-the-
art LVLMs perform in semantic grounding. We further pro-
pose a data-centric approach to enhance LVLMs. Its effec-
tiveness is validated by observing consistent performance
improvement of these LVLMs, after instruction tuning on
multimodal multil-round conversations generated by the en-
hancement method. In the future, we aim to extend the scope
of semantic grounding in LVLMs into (1) more modalities
such as audio, time-series, and tabular; (2) more semantic
grounding targets such as social-emotion, terrains, and hu-
man organs.

--- PAGE 9 ---
References
Awadalla, A.; Gao, I.; Gardner, J.; Hessel, J.; Hanafy, Y .;
Zhu, W.; Marathe, K.; Bitton, Y .; Gadre, S.; Sagawa, S.; Jit-
sev, J.; Kornblith, S.; Koh, P. W.; Ilharco, G.; Wortsman, M.;
and Schmidt, L. 2023. OpenFlamingo: An Open-Source
Framework for Training Large Autoregressive Vision-
Language Models. arXiv preprint arXiv:2308.01390 .
Bai, G.; Chai, Z.; Ling, C.; Wang, S.; Lu, J.; Zhang, N.; Shi,
T.; Yu, Z.; Zhu, M.; Zhang, Y .; Yang, C.; Cheng, Y .; and
Zhao, L. 2024. Beyond Efficiency: A Systematic Survey of
Resource-Efficient Large Language Models. arXiv preprint
arXiv:2401.00625 .
Berg, H.; Hall, S.; Bhalgat, Y .; Kirk, H.; Shtedritski, A.; and
Bain, M. 2022. A Prompt Array Keeps the Bias Away: Debi-
asing Vision-Language Models with Adversarial Learning.
InProceedings of the 2nd Conference of the Asia-Pacific
Chapter of the Association for Computational Linguistics
and the 12th International Joint Conference on Natural Lan-
guage Processing , 806‚Äì822.
Chen, W.; Hu, H.; Chen, X.; Verga, P.; and Cohen, W. 2022.
MuRAG: Multimodal Retrieval-Augmented Generator for
Open Question Answering over Images and Text. In Pro-
ceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing , 5558‚Äì5570.
Cui, H.; Lu, J.; Wang, S.; Xu, R.; Ma, W.; Yu, S.; Yu,
Y .; Kan, X.; Fu, T.; Ling, C.; et al. 2023. A Survey on
Knowledge Graphs for Healthcare: Resources, Application
Progress, and Promise. In ICML 3rd Workshop on Inter-
pretable Machine Learning in Healthcare (IMLH) .
Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang,
W.; Li, B.; Fung, P.; and Hoi, S. 2023. Instructblip: To-
wards general-purpose vision-language models with instruc-
tion tuning. arXiv preprint arXiv:2305.06500 .
Dem Àásar, J. 2006. Statistical comparisons of classifiers over
multiple data sets. The Journal of Machine learning re-
search , 7(1): 1‚Äì30.
Dong, X.; Zhu, Z.; Wang, Z.; Teleki, M.; and Caverlee,
J. 2023. Co2PT: Mitigating Bias in Pre-trained Language
Models through Counterfactual Contrastive Prompt Tuning.
Findings-EMNLP .
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16
Words: Transformers for Image Recognition at Scale. In In-
ternational Conference on Learning Representations .
Fu, C.; Chen, P.; Shen, Y .; Qin, Y .; Zhang, M.; Lin, X.;
Qiu, Z.; Lin, W.; Yang, J.; Zheng, X.; et al. 2023. MME:
A Comprehensive Evaluation Benchmark for Multimodal
Large Language Models. arXiv preprint arXiv:2306.13394 .
Gao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;
Zhang, W.; Lu, P.; He, C.; Yue, X.; et al. 2023. Llama-
adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010 .
Gunjal, A.; Yin, J.; and Bas, E. 2023. Detecting and Prevent-
ing Hallucinations in Large Vision Language Models. arXiv
preprint arXiv:2308.06394 .Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;
Bang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of hal-
lucination in natural language generation. ACM Computing
Surveys , 55(12): 1‚Äì38.
Krasin, I.; Duerig, T.; Alldrin, N.; Ferrari, V .; Abu-El-Haija,
S.; Kuznetsova, A.; Rom, H.; Uijlings, J.; Popov, S.;
Kamali, S.; Malloci, M.; Pont-Tuset, J.; Veit, A.; Belongie,
S.; Gomes, V .; Gupta, A.; Sun, C.; Chechik, G.; Cai, D.;
Feng, Z.; Narayanan, D.; and Murphy, K. 2017. Open-
Images: A public dataset for large-scale multi-label and
multi-class image classification. Dataset available from
https://storage.googleapis.com/openimages/web/index.html .
Li, B.; Zhang, Y .; Chen, L.; Wang, J.; Yang, J.; and Liu, Z.
2023a. Otter: A Multi-Modal Model with In-Context In-
struction Tuning. arXiv preprint arXiv:2305.03726 .
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. BLIP-2: Boot-
strapping Language-Image Pre-training with Frozen Image
Encoders and Large Language Models. In ICML .
Li, L. H.; Zhang, P.; Zhang, H.; Yang, J.; Li, C.; Zhong, Y .;
Wang, L.; Yuan, L.; Zhang, L.; Hwang, J.-N.; et al. 2022.
Grounded language-image pre-training. In CVPR .
Li, Y .; Du, Y .; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen, J.-
R. 2023c. Evaluating Object Hallucination in Large Vision-
Language Models. arXiv preprint arXiv:2305.10355 .
Ling, C.; Zhao, X.; Lu, J.; Deng, C.; Zheng, C.; Wang, J.;
Chowdhury, T.; Li, Y .; Cui, H.; Zhao, T.; et al. 2023. Beyond
One-Model-Fits-All: A Survey of Domain Specialization for
Large Language Models. arXiv preprint arXiv:2305.18703 .
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023a. Visual instruc-
tion tuning. arXiv preprint arXiv:2304.08485 .
Liu, Y .; Duan, H.; Zhang, Y .; Li, B.; Zhang, S.; Zhao, W.;
Yuan, Y .; Wang, J.; He, C.; Liu, Z.; et al. 2023b. MMBench:
Is Your Multi-modal Model an All-around Player? arXiv
preprint arXiv:2307.06281 .
Lu, J.; Qian, Y .; Zhao, S.; Xi, Y .; and Yang, C. 2023. MuG:
A Multimodal Classification Benchmark on Game Data with
Tabular, Textual, and Visual Fields. In Findings-EMNLP .
Lu, J.; Ye, X.; Ren, Y .; and Yang, Y . 2022a. Good, bet-
ter, best: Textual distractors generation for multiple-choice
visual question answering via reinforcement learning. In
CVPR 2022 Workshop on Open-Domain Retrieval Under a
Multi-Modal Setting , 4921‚Äì4930.
Lu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-
C.; Tafjord, O.; Clark, P.; and Kalyan, A. 2022b. Learn to
Explain: Multimodal Reasoning via Thought Chains for Sci-
ence Question Answering. In The 36th Conference on Neu-
ral Information Processing Systems (NeurIPS) .
Luo, G.; Zhou, Y .; Ren, T.; Chen, S.; Sun, X.; and Ji,
R. 2023. Cheap and quick: Efficient vision-language in-
struction tuning for large language models. arXiv preprint
arXiv:2305.15023 .
Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;
Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,
Y .; et al. 2023. Self-refine: Iterative refinement with self-
feedback. arXiv preprint arXiv:2303.17651 .

--- PAGE 10 ---
Monarch, R.; and Munro, R. 2021. Human-in-the-Loop Ma-
chine Learning: Active learning and annotation for human-
centered AI . Simon and Schuster.
Northcutt, C.; Jiang, L.; and Chuang, I. 2021. Confident
learning: Estimating uncertainty in dataset labels. Journal
of Artificial Intelligence Research , 70: 1373‚Äì1411.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. Advances in Neural Information Pro-
cessing Systems , 35: 27730‚Äì27744.
Ramanathan, V .; Kalia, A.; Petrovic, V .; Wen, Y .; Zheng, B.;
Guo, B.; Wang, R.; Marquez, A.; Kovvuri, R.; Kadian, A.;
et al. 2023. PACO: Parts and Attributes of Common Objects.
arXiv preprint arXiv:2301.01795 .
Reimers, N.; and Gurevych, I. 2019. Sentence-BERT: Sen-
tence Embeddings using Siamese BERT-Networks. In Pro-
ceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing .
Tang, Z.; Yang, Z.; Zhu, C.; Zeng, M.; and Bansal, M. 2023.
Any-to-Any Generation via Composable Diffusion. arXiv
preprint arXiv:2305.11846 .
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi, E. H.;
Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-
Consistency Improves Chain of Thought Reasoning in Lan-
guage Models. In ICLR .
Xu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,
F.; Huang, S.; Qiao, Y .; and Luo, P. 2023. Lvlm-ehub:
A comprehensive evaluation benchmark for large vision-
language models. arXiv preprint arXiv:2306.09265 .
Yang, D.; Chen, K.; Rao, J.; Guo, X.; Zhang, Y .; Yang,
J.; and Zhang, Y . 2023. Tackling Vision Language Tasks
Through Learning Inner Monologues. arXiv preprint
arXiv:2308.09970 .
Yang, K.; Russakovsky, O.; and Deng, J. 2019. SpatialSense:
An Adversarially Crowdsourced Benchmark for Spatial Re-
lation Recognition. In International Conference on Com-
puter Vision (ICCV) .
Yarom, M.; Bitton, Y .; Changpinyo, S.; Aharoni, R.; Herzig,
J.; Lang, O.; Ofek, E.; and Szpektor, I. 2023. What You
See is What You Read? Improving Text-Image Alignment
Evaluation. arXiv preprint arXiv:2305.10400 .
Ye, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;
Hu, A.; Shi, P.; Shi, Y .; Jiang, C.; Li, C.; Xu, Y .; Chen, H.;
Tian, J.; Qi, Q.; Zhang, J.; and Huang, F. 2023. mPLUG-
Owl: Modularization Empowers Large Language Models
with Multimodality. arXiv:2304.14178.
Yin, Z.; Wang, J.; Cao, J.; Shi, Z.; Liu, D.; Li,
M.; Sheng, L.; Bai, L.; Huang, X.; Wang, Z.; et al.
2023. LAMM: Language-Assisted Multi-Modal Instruction-
Tuning Dataset, Framework, and Benchmark. arXiv preprint
arXiv:2306.06687 .Young, P.; Lai, A.; Hodosh, M.; and Hockenmaier, J. 2014.
From image descriptions to visual denotations: New simi-
larity metrics for semantic inference over event descriptions.
TACL , 2: 67‚Äì78.
Yu, W.; Yang, Z.; Li, L.; Wang, J.; Lin, K.; Liu, Z.; Wang,
X.; and Wang, L. 2023. MM-Vet: Evaluating Large Mul-
timodal Models for Integrated Capabilities. arXiv preprint
arXiv:2308.02490 .
Yun, T.; Sun, C.; and Pavlick, E. 2021. Does Vision-
and-Language Pretraining Improve Lexical Grounding? In
Findings-EMNLP .
Zha, D.; Bhat, Z. P.; Lai, K.-H.; Yang, F.; and Hu, X. 2023.
Data-centric ai: Perspectives and challenges. In SDM , 945‚Äì
948.
Zhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and
Smola, A. 2023. Multimodal chain-of-thought reasoning in
language models. arXiv preprint arXiv:2302.00923 .
Zhao, Y .; Pang, T.; Du, C.; Yang, X.; Li, C.; Cheung, N.-
M.; and Lin, M. 2023a. On evaluating adversarial ro-
bustness of large vision-language models. arXiv preprint
arXiv:2305.16934 .
Zhao, Z.; Guo, L.; Yue, T.; Chen, S.; Shao, S.; Zhu, X.; Yuan,
Z.; and Liu, J. 2023b. Chatbridge: Bridging modalities with
large language model as a language catalyst. arXiv preprint
arXiv:2305.16103 .
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.
2023. MiniGPT-4: Enhancing Vision-Language Under-
standing with Advanced Large Language Models. arXiv
preprint arXiv:2304.10592 .

--- PAGE 11 ---
Appendices
Implementations of Compared LVLMs
Implementations of the compared baselines are from code-
bases of original authors, with their open-sourced URLs as
follows:
‚Ä¢ LLaMA2-chat (Touvron et al. 2023): https://github.
com/facebookresearch/llama, with the LLaMA2-7B-chat
checkpoint.
‚Ä¢ mPLUG-owl (Ye et al. 2023): https://github.com/X-
PLUG/mPLUG-Owl, with the mPLUG-Owl-7B check-
point.
‚Ä¢ LLaMA-AdapterV2 (Gao et al. 2023): https://github.
com/OpenGVLab/LLaMA-Adapter, with the LLaMA-
AdapterV2-multimodal checkpoint.
‚Ä¢ LaVIN (Luo et al. 2023): https://github.com/luogen1996/
LaVIN, with the LaVIN-7B checkpoint.
‚Ä¢ MiniGPT-4 (Zhu et al. 2023): https://github.com/Vision-
CAIR/MiniGPT-4, with the MiniGPT-4-Vicuna7B-V0
checkpoint.
‚Ä¢ LLaV A (Liu et al. 2023a): https://github.com/haotian-liu/
LLaV A, with the LLaVA-Vicuna7B-v1.1 checkpoint.
‚Ä¢ Otter (Li et al. 2023a): https://github.com/Luodian/Otter,
with the OTTER-Image-MPT7B checkpoint.
‚Ä¢ InstructBLIP (Dai et al. 2023): https://github.com/
salesforce/LA VIS/tree/main/projects/instructblip, with
theblip2-vicuna7B-instruct checkpoint.
As can be seen, we use 7B parameter size checkpoints for
all compared LVLMs and the text-only LLaMA2-chat. All
LVLMs utilize LLaMA1 as the large language model com-
ponent for a fair comparison. The hyper-parameters for in-
ference ( e.g.temperature, number of beams, etc.) for each
LVLM are directly taken from the default ones recom-
mended by their authors in the original codebases, respec-
tively.
For instruction tuning, we also keep the default hyper-
parameters ( e.g.batch size, optimizer, learning rate, etc.) ex-
cept explicitly set the max tuning epoch as3for all LVLMs.
Specifically, for instruction tuning of LLaMA-AdapterV2,
the visual projector and language model adaptor are updated
while leaving the pre-trained visual encoder and the pre-
trained language model frozen. For mPLUG-Owl, only the
pre-trained language model is updated. For LaVIN, the vi-
sual encoder adaptors, the visual projector, and the language
model adaptors (namely ‚ÄúMixture-of-Modality Adapter‚Äù in
the original paper) are updated. For LLaV A, the visual pro-
jector and the language model are updated. For Otter, the
Perceiver resampler of the visual encoder, input/output em-
beddings of the language model, and the cross-attention lay-
ers are updated.
Case Studies for Enhanced LVLMs
We investigate the differences between responses from
original LVLMs and responses from treated LVLMs. Ta-
ble 7 shows two real examples using the same datasource with free-form questions. The compared LVLM is
LLaV A, between the original 7B checkpoint version and the
instruction-tuned version (denoted by [method+] ). In Case
#1, the image depicts a well-furnished bedroom softly il-
luminated, creating a cozy and inviting ambiance. A red
bounding box highlights a chair, while a green bounding box
highlights a bed adorned with a blue comforter and pillows.
Before instruction tuning, LLaV A struggled to provide ac-
curate responses to questions about this image. For exam-
ple, LLaV A mistakenly identified the chair as highlighted
by the green bounding box, and the bed as highlighted by
the red bounding box. In contrast, LLaV A+ produced more
accurate responses to these questions. Furthermore, when
asked about the spatial relationship between the chair and
the bed, LLaV A responded incorrectly, stating that ‚Äúthe chair
is in front of the bed.‚Äù In contrast, LLaV A+ correctly in-
dicated that the bed is to the left of the chair. In Case#2,
the image features a woman wearing a Santa hat while rid-
ing a surfboard in the water. This time, both LLaV A and
LLaV A+ successfully identify the object within the green
bounding box, which is the woman. When moving to the
red bounding box, LLaV A fails to ground it and still rec-
ognizes it as the woman, while LLaV A+ adeptly recognizes
it as the surfboard. LLaV A‚Äôs failure with the red bounding
box results in an incorrect response to the subsequent ques-
tion concerning the interaction between these two bounding
boxes. These two illustrative examples support the effective-
ness of our proposed data-centric enhancement method in
aiding LVLMs in recognizing and interpreting nuanced mul-
timodal information.

--- PAGE 12 ---
Case#1
Source: SpatialSense (Yang, Russakovsky, and Deng 2019)
User What is in the green bounding box in the image?
LLaV A In the green bounding box in the image, there is a chair.
LLaV A+ The image shows the bed located in the green bounding box.
User What is in the red bounding box in the image?
LLaV A In the red bounding box in the image, there is a bed.
LLaV A+ The image shows the chair located in the green bounding box.
User What is the spatial relationship between them?
LLaV A The bed is located next to the chair, with the chair positioned in front of the bed.
LLaV A+ In the image, the bed in the green bounding box is to the left of the chair in the red bounding box.
Case#2
Source: OpenImage-V7 (Krasin et al. 2017)
User What is in the green bounding box?
LLaV A In the green bounding box in the image, there is a woman wearing a Santa hat while riding a surfboard
in the ocean.
LLaV A+ The image displays the woman located in the green bounding box.
User What is in the red bounding box?
LLaV A In the red bounding box, there is a close-up of a woman wearing a Santa hat while riding a surfboard
in the ocean.
LLaV A+ The surfboard is situated within the red bounding box in the picture.
User What is the interaction between them?
LLaV A The interaction between the objects in the red bounding box and the green bounding box is that the
woman in the red box is riding a surfboard in the ocean, while the woman in the green box is also
riding a surfboard in the ocean.
LLaV A+ In the context of the image, the woman in the red bounding box surf the surfboard in the green
bounding box.
Table 7: Case study regarding spatial misgrounding. LLaV A+ denotes the enhanced LLaV A using our proposed instruction
tuning data.Text in cyan indicates the precise statement. Text in orange indicates the ambiguous statement.
