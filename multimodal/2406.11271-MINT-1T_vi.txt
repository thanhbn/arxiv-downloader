# 2406.11271.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2406.11271.pdf
# Kích thước tệp: 2131363 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MINT-1T:
Mở rộng dữ liệu đa phương thức mã nguồn mở gấp 10 lần:
Một bộ dữ liệu đa phương thức với một nghìn tỷ token
Anas Awadalla1,2∗Le Xue2Oscar Lo1Manli Shu2
Hannah Lee1Etash Guha1Matt Jordan4Sheng Shen5Mohamed Awadalla1
Silvio Savarese⋄,2,3Caiming Xiong⋄,2Ran Xu⋄,2Yejin Choi⋄,1Ludwig Schmidt⋄,1
1Đại học Washington,2Nghiên cứu Salesforce,3Đại học Stanford,
4Đại học Texas tại Austin,5Đại học California, Berkeley,⋄Tác giả cao cấp
Tóm tắt
Các bộ dữ liệu đa phương thức xen kẽ có các chuỗi xen kẽ tự do của hình ảnh và văn bản là rất quan trọng để đào tạo các mô hình đa phương thức lớn tiên tiến (LMMs). Mặc dù sự tiến bộ nhanh chóng của các LMMs mã nguồn mở, vẫn còn thiếu hụt rõ rệt về các bộ dữ liệu đa phương thức xen kẽ quy mô lớn, mã nguồn mở. Để đáp ứng, chúng tôi giới thiệu MINT-1T, bộ dữ liệu đa phương thức xen kẽ mã nguồn mở rộng lớn và đa dạng nhất cho đến nay. MINT-1T bao gồm một nghìn tỷ token văn bản và 3.4 tỷ hình ảnh, tăng quy mô 10 lần so với các bộ dữ liệu mã nguồn mở hiện có. Ngoài ra, chúng tôi bao gồm các nguồn chưa được khai thác trước đây như PDFs và các bài báo ArXiv. Vì việc mở rộng quy mô các bộ dữ liệu đa phương thức xen kẽ đòi hỏi nỗ lực kỹ thuật đáng kể, việc chia sẻ quy trình tuyển chọn dữ liệu và phát hành bộ dữ liệu mang lại lợi ích lớn cho cộng đồng. Các thí nghiệm của chúng tôi cho thấy rằng các LMMs được đào tạo trên MINT-1T có thể sánh ngang với hiệu suất của các mô hình được đào tạo trên bộ dữ liệu hàng đầu trước đó, OBELICS. Chúng tôi phát hành dữ liệu tại https://github.com/mlfoundations/MINT-1T .

MMC4
OBELICSCM3 MM1
ChameleonMINT-1T
Bộ dữ liệu đa phương thức xen kẽ02004006008001000Số lượng Token văn bản (tỷ)
Mã nguồn mở
Mã nguồn đóng
MINT - 1THTMLP D F
HTMLOBELIC SHTMLMMC 4

Hình 1: MINT-1T là một bộ dữ liệu tiền đào tạo đa phương thức xen kẽ một nghìn tỷ token. Đây là bộ dữ liệu lớn nhất thuộc loại này và đa dạng hơn các bộ dữ liệu mã nguồn mở trước đây.

∗Công việc thực hiện khi thực tập tại Nghiên cứu Salesforce
Preprint. Đang xem xét.arXiv:2406.11271v5  [cs.CV]  31 Oct 2024

--- TRANG 2 ---
Vui lòng lưu ý cân bằng thương mại của các bang châu Âu: Ý có thặng dư,
Đức thâm hụt. Thực sự là một thế giới khác. Hãy xem xét rằng
nợ công của Ý không khác biệt so với ngày nay, nhưng nó
không phải là vấn đề đáng quan tâm, cú sốc đã phản ánh lên
tỷ giá hối đoái, không ai nghĩ đến việc bán
trái phiếu chính phủ dưới mệnh giá...Vui lòng kiểm tra,
biểu đồ sau: màu đỏ là thay đổi gdp% trong nợ tư nhân và
màu xanh là nợ công từ 1999 đến 2007, rất nhiều cho một
trong những huyền thoại khác của cuộc khủng hoảng này, cái mà nói rằng "lỗi là do
nợ công"...

Libya: Leptis Magna, Di tích La Mã vĩ đại nhất châu Phi
Leptis Magna, ở Libya hiện đại, từng là thành phố La Mã hàng đầu của châu Phi. Đây là một trong những địa điểm khảo cổ vĩ đại nhất
trong toàn bộ Địa Trung Hải. Nếu Leptis Magna ở Tunisia
hoặc Morocco hoặc Ai Cập...

Khoảng một hoặc hai km trên đường là khu vực đua xe và
đấu trường trong phần thứ hai của khu
phức hợp Leptis Magna. Đấu trường được xây dựng để chứa tới 16,000
khán giả đến để được giải trí...

Hình 1: Các điểm tạo ra hadron chính trong
không gian tham số tác động được tạo ra trong khoảng
giả nhanh độ trung tâm trong các sự kiện mẫu từ pp tại 7 TeV
(hàng trên), pPb tại 5.02 TeV (giữa), và PbPb
va chạm tại 2.76 TeV (dưới) cho các khoảng khác nhau của
đa trung tâm (|η| < 0.5) tích điện: ~ 25 (cột
đầu), ~ 50 (thứ hai), ~ 100 (thứ ba)\nvà >1000 (cột
cuối). Màu sắc của các điểm chỉ ra κe ﬀ/κ
được sử dụng trong việc phá vỡ chuỗi nơi các hadron chính
được tạo ra. Vectơ tham số tác động dọc theo
trục x. khung cho cặp này. Giả sử rằng
phần chủ yếu của tương tác chuỗi được cho bởi
trường điện màu, sử dụng "phép chiếu Abelian" của
trường SU(3) [9, 10], tổng tương tác là tổng của
tương tác giữa tất cả các cặp ( d3x E2 d3x EiEj).

Ví dụ về Tài liệu đa phương thức MINT
Hình 2: Mẫu tài liệu đa phương thức từ các tập con HTML (Trái), PDF (Giữa), và ArXiv (Phải)
của MINT-1T với mỗi tài liệu chứa một chuỗi hình ảnh xen kẽ với văn bản.

Nghiên cứu trước đây đã chỉ ra rằng dữ liệu xen kẽ cải thiện hiệu suất trả lời câu hỏi trong
bối cảnh của các mô hình kiểu Flamingo [Laurençon et al., 2023] và để đào tạo các mô hình đa phương thức lớn
với hiệu suất mạnh trên cả các benchmark chỉ văn bản và đa phương thức [McKinzie et al., 2024].
MINT-1T là công trình mã nguồn mở đầu tiên mở rộng quy mô bộ dữ liệu xen kẽ đến một nghìn tỷ token văn bản và
thu thập tài liệu xen kẽ từ PDFs và ArXiv ở quy mô lớn. Các mẫu trong hình này được cắt ngắn văn bản do không gian.

1 Giới thiệu

Các bộ dữ liệu tiền đào tạo mã nguồn mở lớn đã là những hiện vật quan trọng cho cộng đồng nghiên cứu
trong việc nghiên cứu kỹ thuật dữ liệu và đào tạo các mô hình mã nguồn mở minh bạch. Trong lĩnh vực văn bản, chúng ta
đã thấy các công trình sớm như C4 [Raffel et al., 2019] và The Pile [Gao et al., 2020] quan trọng như thế nào
đối với cộng đồng để đào tạo bộ mô hình ngôn ngữ lớn mã nguồn mở đầu tiên (GPT-J [Wang
and Komatsuzaki, 2021], GPT-Neo [Black et al., 2021], và các mô hình khác). Những công trình này cũng đặt nền tảng
cho các nghiên cứu tiếp theo cải thiện các phương pháp lọc dữ liệu và quy mô. Xu hướng tương tự trong không gian ảnh-văn bản
các bộ dữ liệu mã nguồn mở quy mô lớn đã dẫn đến sự đổi mới về các phương pháp tuyển chọn dữ liệu tốt hơn
như mạng lọc dữ liệu [Fang et al., 2023], T-MARS [Maini et al., 2023], và các phương pháp khác.

Chúng ta đang chứng kiến sự chuyển đổi lớn từ các phòng thí nghiệm tiên tiến để đào tạo các mô hình đa phương thức lớn (LMMs) [Google, 2023,
Meta, 2024, Achiam et al., 2023] đòi hỏi các bộ dữ liệu đa phương thức xen kẽ lớn—bao gồm
các chuỗi tự do của hình ảnh và văn bản (một ví dụ về tài liệu xen kẽ có thể tìm thấy
trong Hình 2). Khi khả năng của các mô hình tiên tiến phát triển nhanh chóng, có một khoảng cách ngày càng tăng
trong dữ liệu đào tạo đa phương thức giữa các mô hình đóng và mở nguồn. Các bộ dữ liệu đa phương thức xen kẽ mã nguồn mở hiện có
nhỏ hơn và ít đa dạng hơn so với các đối tác chỉ văn bản
và chỉ được lấy nguồn từ tài liệu HTML, hạn chế độ rộng và sự đa dạng của dữ liệu. Hạn chế này
cản trở sự phát triển của các LMMs mã nguồn mở mạnh mẽ và tạo ra sự chênh lệch giữa
khả năng của các mô hình mở và đóng nguồn.

Để thu hẹp khoảng cách này, chúng tôi đã xây dựng MINT-1T (Đa phương thức xen kẽ), bộ dữ liệu đa phương thức xen kẽ mã nguồn mở
lớn nhất và đa dạng nhất cho đến nay. MINT-1T chứa tổng cộng một nghìn tỷ token văn bản
và ba tỷ hình ảnh, được lấy nguồn từ các nguồn đa dạng như HTML/PDFs/ArXiv.
Trước MINT-1T, bộ dữ liệu mã nguồn mở lớn nhất trong lĩnh vực này là OBELICS [Laurençon et al., 2023],
một bộ dữ liệu 115 tỷ token văn bản với 353M hình ảnh chỉ được lấy nguồn từ HTML.

Những đóng góp của chúng tôi với MINT-1T như sau:
Kỹ thuật dữ liệu Việc mở rộng quy mô dữ liệu đa phương thức xen kẽ này đưa ra nhiều thách thức kỹ thuật
hơn là xây dựng các bộ dữ liệu chỉ văn bản hoặc cặp ảnh-văn bản. Chúng tôi đang xử lý kích thước tài liệu lớn hơn nhiều,
và thứ tự ban đầu của hình ảnh và văn bản phải được bảo toàn.

--- TRANG 3 ---
Bộ dữ liệu # Token # Hình ảnh # Tài liệu Mã nguồn mở Nguồn dữ liệu
CM3 223B 373M 10.7M ✗ HTML
Multimodal-C4 43B 571M 101M ✓ HTML
OBELICS 115B 353M 141M ✓ HTML
MM1 400B – – ✗ HTML
Chameleon 400B – – ✗ HTML
MINT-1T 1.02T 3.42B 1054M ✓ HTML/PDFs/ArXiv

Bảng 1: Khảo sát các bộ dữ liệu đa phương thức xen kẽ: Các bộ dữ liệu xen kẽ mã nguồn mở hiện tại
nhỏ hơn các bộ dữ liệu độc quyền. Ngoài các nguồn dữ liệu HTML, MINT-1T độc đáo bao gồm dữ liệu
từ tài liệu PDFs và ArXiv.

8461B4319B1434B346B922B111B93B9BMINT-1TTệp WARC Tệp WATArXiv9BPhân tích DOMLoại bỏ trùng lặp văn bảnPhân tích PDFPhân tích TexLọc chất lượng văn bảnLọc chất lượng văn bảnTải xuống hình ảnhLọc hình ảnhLoại bỏ trùng lặp văn bảnLoại bỏ trùng lặp hình ảnhLoại bỏ trùng lặp hình ảnhPhát hiện NSFW

Hình 3: Quy trình lọc cho MINT-1T cho thấy cách các token được loại bỏ trong suốt
đường ống dữ liệu cho HTML, PDFs, và các bài báo ArXiv.

Đa dạng Chúng tôi là công trình đầu tiên trong không gian đa phương thức xen kẽ thu thập tài liệu đa phương thức
chất lượng cao ở quy mô lớn từ các nguồn như PDFs CommonCrawl và ArXiv.

Thí nghiệm mô hình Các thí nghiệm của chúng tôi chứng minh rằng các LMMs được đào tạo trên MINT-1T không chỉ
khớp mà còn có thể vượt qua hiệu suất của các mô hình được đào tạo trên bộ dữ liệu mã nguồn mở tốt nhất hiện có,
OBELICS, đồng thời cung cấp sự gia tăng quy mô gấp mười lần.

2 Xây dựng bộ dữ liệu

MINT-1T tuyển chọn một bộ dữ liệu mã nguồn mở quy mô lớn khai thác các nguồn tài liệu xen kẽ
đa dạng hơn như PDFs và các bài báo ArXiv. Phần này phác thảo các phương pháp của chúng tôi để lấy nguồn tài liệu
đa phương thức, lọc tài liệu chất lượng thấp, loại bỏ trùng lặp dữ liệu, và loại bỏ nội dung không an toàn
cho công việc và không mong muốn. Bộ dữ liệu cuối cùng chứa 922 tỷ (B) token HTML, 93B token PDF,
và 9B token ArXiv (để biết số chi tiết hơn và tỷ lệ lọc tham khảo Hình 3).

2.1 Lấy nguồn số lượng lớn tài liệu đa phương thức

2.1.1 Đường ống HTML

Chúng tôi theo phương pháp của OBELICS để trích xuất tài liệu đa phương thức xen kẽ từ các tệp WARC CommonCrawl
bằng cách phân tích cây DOM của mỗi entry WARC. Trong khi OBELICS chỉ xử lý tài liệu
từ các dump CommonCrawl từ tháng 2 năm 2020 đến tháng 2 năm 2023, chúng tôi đã mở rộng pool tài liệu
để bao gồm tài liệu HTML từ tháng 5 năm 2017 đến tháng 4 năm 2024 (lưu ý rằng chúng tôi sử dụng các dump đầy đủ từ
tháng 10 năm 2018 đến tháng 4 năm 2024 và các dump từng phần từ những năm trước). Theo OBELICS, chúng tôi lọc bỏ
bất kỳ tài liệu nào không chứa hình ảnh hoặc có hơn ba mươi hình ảnh hoặc bất kỳ hình ảnh nào có URL
bao gồm các chuỗi con không phù hợp như logo, avatar, porn, và xxx.

2.1.2 Đường ống PDF

Chúng tôi lấy nguồn tài liệu PDF từ các tệp WAT CommonCrawl từ các dump từ tháng 2 năm 2023 đến tháng 4 năm 2024.
Ban đầu, chúng tôi trích xuất tất cả các liên kết PDF từ những dump này. Sau đó chúng tôi cố gắng tải xuống và đọc PDFs

--- TRANG 4 ---
sử dụng PyMuPDF2. Chúng tôi loại bỏ các tài liệu PDF có kích thước lớn hơn 50MB (vì chúng có thể chứa
chủ yếu các hình ảnh lớn) và PDFs có hơn 50 trang. Chúng tôi loại trừ các trang không chứa
văn bản và trích xuất thứ tự đọc cho các trang còn lại. Thứ tự đọc được thu được bằng cách tìm
hộp giới hạn của tất cả các khối văn bản trên một trang, nhóm các khối dựa trên cột, và sắp xếp
các khối từ trên trái đến dưới phải. Hình ảnh được neo trong chuỗi dựa trên sự gần gũi
giữa hộp giới hạn của hình ảnh và các khối văn bản trên cùng một trang. Chúng tôi thảo luận về các hạn chế của
phương pháp này trong Phụ lục A.1.

2.1.3 Đường ống ArXiv

Các tài liệu ArXiv xen kẽ được xây dựng từ mã nguồn LaTeX. Chúng tôi sử dụng TexSoup3 để tìm
các thẻ hình và xen kẽ hình ảnh với văn bản bài báo. Đối với các bài báo nhiều tệp (tức là khi mỗi phần được
viết trong một tệp Tex khác nhau), chúng tôi xác định tệp Tex chính và thay thế các thẻ input bằng nội dung
của tệp đó. Chúng tôi cũng dọn dẹp mã LaTeX loại bỏ imports, thư mục tài liệu tham khảo, bảng, và
các thẻ trích dẫn. Vì ArXiv đã là nguồn dữ liệu được tuyển chọn cao, chúng tôi không thực hiện bất kỳ
việc lọc và loại bỏ trùng lặp nào được mô tả trong phần còn lại của mục này.

2.2 Lọc chất lượng văn bản

Phù hợp với các thực hành được thiết lập bởi RefinedWeb [Penedo et al., 2023], Dolma [Soldaini et al., 2024],
và FineWeb [Penedo et al., 2024], chúng tôi tránh sử dụng các heuristic dựa trên mô hình để lọc văn bản. Phương pháp này
đã được chứng minh là mở rộng hiệu quả cho các mô hình chỉ văn bản. Ban đầu, chúng tôi loại bỏ các tài liệu
không phải tiếng Anh sử dụng mô hình nhận dạng ngôn ngữ của Fasttext [Joulin et al., 2017] (với ngưỡng tin cậy 0.65).
Ngoài ra, các tài liệu có URL chứa chuỗi con NSFW được loại bỏ để loại trừ nội dung khiêu dâm
và không mong muốn. Chúng tôi áp dụng các phương pháp lọc văn bản từ RefinedWeb, cụ thể là loại bỏ
tài liệu với n-gram trùng lặp quá mức hoặc những tài liệu được xác định là chất lượng thấp
sử dụng các quy tắc MassiveText [Rae et al., 2021].

2.3 Lọc hình ảnh

Sau khi thu được bộ PDFs và tệp HTML được tuyển chọn, chúng tôi cố gắng tải xuống tất cả URL hình ảnh trong
bộ dữ liệu HTML, loại bỏ bất kỳ liên kết nào không thể truy xuất và loại bỏ tài liệu không có
liên kết hình ảnh hợp lệ. Chúng tôi loại bỏ hình ảnh nhỏ hơn 150pixel để tránh hình ảnh nhiễu như logo và biểu tượng
và hình ảnh lớn hơn 20,000pixel vì chúng thường tương ứng với hình ảnh ngoài chủ đề. Đối với tài liệu HTML,
chúng tôi loại bỏ hình ảnh có tỷ lệ khung hình lớn hơn hai để loại bỏ hình ảnh chất lượng thấp
như băng rôn quảng cáo. Tuy nhiên, đối với PDFs, chúng tôi điều chỉnh ngưỡng này thành ba để bảo tồn
hình và bảng khoa học, thường bị loại trừ một cách sai lầm bởi tiêu chí nghiêm ngặt hơn.

2.4 Lọc an toàn

Lọc hình ảnh NSFW Chúng tôi áp dụng một bộ phát hiện hình ảnh NSFW [Laborde] cho tất cả hình ảnh trong bộ dữ liệu.
Nếu chúng tôi thấy rằng một tài liệu chứa một hình ảnh NSFW duy nhất, chúng tôi loại bỏ toàn bộ tài liệu.

Loại bỏ thông tin nhận dạng cá nhân Để giảm thiểu rủi ro rò rỉ dữ liệu cá nhân, chúng tôi
ẩn danh địa chỉ email và địa chỉ IP trong dữ liệu văn bản. Theo FineWeb, chúng tôi thay thế email
bằng các mẫu như "email@example.com" và IP bằng IP không chức năng được tạo ngẫu nhiên.

2.5 Loại bỏ trùng lặp

Để loại bỏ nội dung trùng lặp, chúng tôi thực hiện loại bỏ trùng lặp văn bản đoạn văn và tài liệu trong mỗi
snapshot CommonCrawl và loại bỏ trùng lặp hình ảnh để loại bỏ hình ảnh lặp lại, không có thông tin như
biểu tượng và logo. Tất cả các bước loại bỏ trùng lặp được thực hiện riêng biệt cho mỗi nguồn dữ liệu.

2https://github.com/pymupdf/PyMuPDF-Utilities/blob/master/text-extraction/multi_
column.py
3https://github.com/alvinwan/TexSoup

--- TRANG 5 ---
100 1000 10000 100000
Số lượng Token văn bản0.00000.00020.00040.00060.0008Mật độOBELICS
MINT-1T (HTML)
MINT-1T (PDF)
MINT-1T (ArXiv)

Hình 4: Chúng tôi vẽ biểu đồ phân bố
token văn bản trên mỗi tài liệu cho OBELICS và
MINT-1T. Chúng tôi quan sát thấy rằng tập con HTML
của MINT-1T theo một phân bố tương tự
với OBELICS, nhưng PDFs và ArXiv có
độ dài đáng kể hơn.

1 3 5 7 9 11 13 15 17
Số lượng hình ảnh0.000.050.100.150.200.250.30Mật độOBELICS
MINT-1T (HTML)
MINT-1T (PDF)
MINT-1T (ArXiv)

Hình 5: Các tài liệu trong MINT-1T,
trung bình, chứa nhiều hình ảnh hơn
OBELICS. Tập con HTML của chúng tôi chứa
nhiều hình ảnh hơn OBELICS, và chúng tôi thấy
rằng trong MINT-1T, PDFs có mật độ hình ảnh
cao hơn một chút so với HTML, với
ArXiv có mật độ hình ảnh cao nhất.

2.5.1 Loại bỏ trùng lặp đoạn văn và tài liệu

Theo phương pháp của Dolma [Groeneveld, 2023], chúng tôi sử dụng Bloom Filter để loại bỏ trùng lặp văn bản
hiệu quả. Chúng tôi đặt tỷ lệ dương tính giả thành 0.01 cho bloom filter và loại bỏ trùng lặp
các đoạn văn 13-gram (được chỉ ra thông qua dấu phân cách xuống dòng đôi) từ mỗi tài liệu. Nếu hơn 80%
các đoạn văn của tài liệu là trùng lặp, chúng tôi loại bỏ toàn bộ tài liệu.

2.5.2 Loại bỏ văn bản boilerplate thông thường

Sau khi loại bỏ trùng lặp đoạn văn, chúng tôi nhận thấy rằng các câu boilerplate thông thường ngắn trong tài liệu HTML,
như "Bỏ qua nội dung" hoặc "Lưu trữ Blog," vẫn còn. Để loại bỏ những câu nhiễu này, chúng tôi chạy
loại bỏ trùng lặp đoạn văn chính xác trên 2% của mỗi snapshot CommonCrawl, phù hợp với CCNet [Wenzek
et al., 2019]; việc làm này ở quy mô nhỏ đảm bảo chúng tôi chủ yếu loại bỏ chỉ văn bản boilerplate thông thường.

2.5.3 Loại bỏ trùng lặp hình ảnh

Trong mỗi snapshot CommonCrawl, chúng tôi loại bỏ hình ảnh xuất hiện thường xuyên dựa trên
hash SHA256. Thay vì loại bỏ trùng lặp nghiêm ngặt, chúng tôi theo Multimodal-C4 [Zhu et al., 2023] bằng cách chỉ
loại bỏ hình ảnh xuất hiện hơn mười lần trong một snapshot. Phù hợp với OBELICS [Laurençon
et al., 2023], chúng tôi loại bỏ hình ảnh lặp lại trong một tài liệu duy nhất và chỉ giữ lại lần xuất hiện đầu tiên.

2.6 Cơ sở hạ tầng

Trong suốt quá trình xử lý dữ liệu, chúng tôi có quyền truy cập vào trung bình 2,350 CPU cores từ hỗn hợp
các node 190-processor và 90-processor. Tổng cộng, chúng tôi đã sử dụng ~4.2M giờ CPU để xây dựng bộ dữ liệu này.

3 Phân tích

3.1 So sánh thành phần tài liệu trong MINT-1T với OBELICS

Trong việc đánh giá thành phần của các bộ dữ liệu xen kẽ, hai đặc điểm chính được kiểm tra: phân bố
token văn bản trên mỗi tài liệu và số lượng hình ảnh trên mỗi tài liệu. Để phân tích này,
chúng tôi lấy mẫu ngẫu nhiên 50,000 tài liệu từ cả OBELICS và mỗi nguồn dữ liệu trong MINT-1T.
Chúng tôi sử dụng tokenizer của GPT-2 để tính số lượng token văn bản. Chúng tôi loại bỏ các ngoại lệ, loại trừ
tài liệu nằm ngoài 1.5 * khoảng tứ phân vị cho số lượng token văn bản và hình ảnh.

Như được hiển thị trong Hình 4, tập con HTML của MINT-1T phù hợp chặt chẽ với phân bố token được thấy
trong OBELICS. Tuy nhiên, các tài liệu được lấy nguồn từ PDFs và ArXiv có xu hướng dài hơn tài liệu HTML
trung bình, nhấn mạnh lợi ích của việc lấy nguồn dữ liệu từ các nguồn đa dạng. Hình 5

--- TRANG 6 ---
2.0%
6.0% 12.0%
41.5%16.5%8.0%14.0%OBELICS
6.0%
4.0%16.0%
12.5%
16.5%20.0%25.0%MINT-1T (HTML)
2.5%
4.0%12.5%
17.5%
19.5%26.5%17.5%MINT-1T (PDF)
Nghệ thuật & Thiết kế
Kinh doanh Y học & Sức khỏe
Nhân văn & Xã hội Khác
Khoa học Công nghệ & Kỹ thuật

Hình 6: Phân bố lĩnh vực tài liệu: Tỷ lệ phần trăm tài liệu từ mỗi lĩnh vực trong
MMMU cho OBELICS và các tập con của MINT-1T. Chúng tôi tìm thấy hai xu hướng thú vị: (1) Đa số
tài liệu trong OBELICS liên quan đến Nhân văn và Khoa học Xã hội; xu hướng này không được tìm thấy trong
tập con HTML của MINT-1T. (2) Đa số tài liệu PDF liên quan đến Khoa học.

kiểm tra mật độ hình ảnh trên tất cả tài liệu, tiết lộ rằng tài liệu PDFs và ArXiv chứa
nhiều hình ảnh hơn so với tài liệu HTML, với các mẫu ArXiv có mật độ hình ảnh cao nhất.

3.2 Các nguồn dữ liệu khác nhau cải thiện sự đa dạng tài liệu như thế nào?

Một động lực quan trọng để mở rộng pool tài liệu đa phương thức ngoài HTML là
cải thiện phạm vi bao phủ lĩnh vực. Để định lượng sự đa dạng và chiều sâu của phạm vi bao phủ này, chúng tôi sử dụng
một mô hình Latent Dirichlet Allocation [Campbell et al., 2003] (LDA) được đào tạo trên 100,000 tài liệu
được lấy mẫu từ bộ dữ liệu OBELICS, tập con HTML của MINT-1T, và tập con PDF (loại trừ
ArXiv) từ MINT-1T để có 200 chủ đề. Sau đó chúng tôi sử dụng GPT-4 để phân loại tập hợp các từ để xác định
các lĩnh vực chủ đạo – như Y học & Sức khỏe, Khoa học, Kinh doanh, Nhân văn, Lịch sử, v.v. –
dựa trên các lĩnh vực MMMU.

Phân tích của chúng tôi tiết lộ các xu hướng riêng biệt trong phân bố lĩnh vực:

OBELICS: Bộ dữ liệu này cho thấy sự tập trung rõ nét vào "Nhân văn và Khoa học Xã hội".
Điều này có thể được quy cho quá trình xây dựng dữ liệu, bao gồm việc lọc bỏ tài liệu
không giống bài viết Wikipedia, do đó có thể thay đổi phân bố thành nội dung tập trung hơn vào kiến thức tổng quát
và nhân văn.

Tập con HTML của MINT-1T: Trái ngược với OBELICS, tập con HTML của MINT-1T không
bị thiên vị mạnh mẽ về bất kỳ lĩnh vực cụ thể nào, gợi ý về biểu diễn lĩnh vực rộng hơn và cân bằng hơn.

Tập con PDF của MINT-1T: Có tỷ lệ cao hơn các tài liệu "Khoa học và Công nghệ" trong
các tài liệu PDF của MINT-1T. Xu hướng này có thể do bản chất của giao tiếp khoa học,
nơi PDFs là định dạng ưa thích để chia sẻ các bài báo nghiên cứu chi tiết và báo cáo kỹ thuật.

4 Thí nghiệm mô hình

4.1 Thiết lập đào tạo

Trong phần này, chúng tôi phác thảo kiến trúc của các LMMs chúng tôi đào tạo, các siêu tham số được sử dụng, và
các phương pháp để đánh giá khả năng đa phương thức xen kẽ của các mô hình.

Mô hình hóa Kiến trúc của chúng tôi được áp dụng từ XGen-MM [Xue et al., 2024]. Chúng tôi sử dụng bộ mã hóa thị giác ViT-H
với độ phân giải 378 từ Data Filtering Networks [Fang et al., 2023] và chuyển các embeddings patch
vào một perceiver resampler sáu lớp [Alayrac et al., 2022]. Mỗi hình ảnh được biểu diễn
dưới dạng 128 token. Các embeddings patch được gộp được xen kẽ với embeddings token văn bản và
được chuyển vào mô hình ngôn ngữ Phi-3 mini [Abdin et al., 2024]. Chúng tôi giữ bộ mã hóa thị giác đóng băng trong khi
đào tạo resampler và mô hình ngôn ngữ. Chúng tôi sử dụng batch size trung bình 1.8M token đa phương thức.
Đối với tất cả các lần chạy đào tạo, chúng tôi sử dụng 2,000 bước khởi động với tốc độ học tối đa
5∗10−5 và decay tốc độ học cosine. Chúng tôi cũng áp dụng weight decay 0.05 cho tất cả các tham số có thể đào tạo.

--- TRANG 7 ---
Tất cả việc đào tạo của chúng tôi được thực hiện sử dụng codebase OpenFlamingo [Awadalla et al., 2023]. Chúng tôi đào tạo tất cả
các mô hình trên 32 GPU H100 với tổng cộng 1,920 giờ GPU cho mỗi thí nghiệm.

Dữ liệu đào tạo Đối với tất cả thí nghiệm, chúng tôi đào tạo mô hình trên 50% batch chú thích ảnh-văn bản và
50% batch đa phương thức xen kẽ. Chúng tôi lấy mẫu tối đa 2048 token đa phương thức từ mỗi
tài liệu xen kẽ và 340 token từ mỗi mẫu ảnh-văn bản. Như trong Flamingo Alayrac et al.
[2022], chúng tôi thêm token <|endofchunk|> để chỉ ra kết thúc của chuỗi ảnh-văn bản liền kề.
Trong quá trình đào tạo, chúng tôi ngẫu nhiên bỏ 50% tài liệu xen kẽ một hình ảnh để tăng mẫu
tài liệu nhiều hình ảnh. Đối với bộ dữ liệu ảnh-văn bản của chúng tôi, chúng tôi sử dụng hỗn hợp các bộ dữ liệu chú thích được tuyển chọn nội bộ.

4.2 Thiết lập đánh giá

Chúng tôi đánh giá khả năng của mô hình để suy luận về các chuỗi đa phương thức xen kẽ thông qua
khả năng học trong bối cảnh và hiệu suất suy luận nhiều hình ảnh.

1 2 4 8
Số lượng Shots52545658606264Hiệu suất học trong bối cảnh trung bình
MINT-1T
MINT-1T (HTML)
OBELICS

(a) Hiệu suất trung bình trên các benchmark chú thích và trả lời câu hỏi thị giác
khi sử dụng một đến tám minh chứng. Mô hình được đào tạo trên MINT-1T
hoạt động tốt hơn trên tất cả minh chứng so với phần HTML của MINT-1T hoặc OBELICS.

Art Business Health Humanities Science Tech
Lĩnh vực MMMU01020304050Độ chính xác (%)MINT-1T
MINT-1T (HTML)
OBELICS

(b) MINT-1T vượt trội hơn OBELICS trong nhiều lĩnh vực.
Chúng tôi cải thiện so với OBELICS trong các lĩnh vực Khoa học và Công nghệ
vì tập con PDF của MINT-1T chứa đại diện lớn của các lĩnh vực này. MINT-1T
cũng hoạt động tốt hơn OBELICS về Nghệ thuật.

Học trong bối cảnh Các mô hình được đánh giá về hiệu suất học trong bối cảnh bốn-shot và tám-shot
trên các benchmark chú thích khác nhau (COCO (Karpathy test) [Lin et al., 2014] và
TextCaps (validation) [Sidorov et al., 2020]) và bộ dữ liệu trả lời câu hỏi thị giác (VQAv2 (validation) [Agrawal et al., 2015], OK-VQA (validation) [Marino et al., 2019], TextVQA (validation) [Singh
et al., 2019], và VizWiz (validation) [Gurari et al., 2018]). Đối với tất cả đánh giá, chúng tôi lấy mẫu ngẫu nhiên
minh chứng từ tập đào tạo. Điểm được báo cáo của chúng tôi được tính trung bình trên nhiều lần chạy đánh giá
nơi chúng tôi randomize minh chứng. Chúng tôi thấy rằng hiệu suất nhạy cảm với prompt được chọn, vì vậy
chúng tôi ablate qua các prompt khác nhau cho mỗi nhiệm vụ và chọn prompt hoạt động tốt nhất. Danh sách
các prompt chúng tôi sử dụng và tham số tạo có thể tìm thấy trong Phụ lục A.2.

Suy luận nhiều hình ảnh Chúng tôi cũng đánh giá các mô hình trên MMMU [Yue et al., 2024] (chứa
cả câu hỏi một hình ảnh và nhiều hình ảnh) và Mantis-Eval [Jiang et al., 2024] (tất cả câu hỏi nhiều hình ảnh)
để thăm dò khả năng suy luận nhiều hình ảnh của mô hình ngoài đánh giá học trong bối cảnh.

4.3 Thí nghiệm

Đào tạo trên tài liệu HTML

Trước tiên chúng tôi đánh giá cách phần HTML của MINT-1T so sánh với OBELICS, vì OBELICS là
bộ dữ liệu xen kẽ hàng đầu trước đây và cũng được tuyển chọn từ tài liệu HTML. Chúng tôi đào tạo hai mô hình
trên các phần HTML của MINT-1T và OBELICS với tổng cộng 10B token đa phương thức và đánh giá

--- TRANG 8 ---
Mô hình ShotsDatasets Trung bình
COCO TextCaps OKVQA TextVQA VQAv2 VizWiz
OBELICS4 108.1 ±0.91 80.4±0.96 48.4±0.79 42.4±0.74 61.8±0.12 26.0±0.28 61.2±0.28
8 109.4±0.71 83.9±0.36 49.6±0.08 43.8±0.43 62.3±0.42 30.9±1.10 63.3±0.24
MINT-1T (HTML)4 105.4±0.99 79.2±1.17 48.0±0.12 43.7±0.35 63.5±0.11 25.6±0.44 60.9±0.27
8 107.9±0.58 81.9±1.29 49.7±0.37 44.2±0.30 64.3±0.15 30.3±0.87 63.0±0.28
MINT-1T4 107.0±0.13 79.7±0.62 49.7±0.19 45.0±0.39 64.9±0.07 27.5±0.32 62.3±0.13
8 108.8±0.34 84.3±0.51 51.1±0.18 45.6±0.10 66.1±0.09 31.8±0.53 64.6±0.14

Bảng 2: Đánh giá học trong bối cảnh: Chúng tôi đào tạo ba mô hình đa phương thức 4.6B trên 10B token
hỗn hợp các tài liệu mẫu ảnh-văn bản và xen kẽ. Các mô hình được đánh giá sử dụng bốn và
tám ví dụ học trong bối cảnh và mỗi đánh giá được chạy cho ba lần thử. Chúng tôi báo cáo hiệu suất trung bình
và độ lệch chuẩn. Khoảng cách hiệu suất có ý nghĩa thống kê được in đậm và các mô hình
hoạt động tương đương được gạch dưới.

MINT-1T MINT-1T
(HTML)OBELICS9192939495969798Hiệu suất trung bình93.35
92.3094.2597.05
94.9096.65Hiệu suất chú thích
MINT-1T MINT-1T
(HTML)OBELICS44454647484950
46.78
45.20
44.6548.65
47.12
46.52Hiệu suất VQA
MINT-1T MINT-1T
(HTML)OBELICS60616263646566
62.30
60.9061.1864.78
63.0563.23Hiệu suất tổng thể
4-shot
8-shot

Hình 8: Hiệu suất trên các nhiệm vụ chú thích và trả lời câu hỏi thị giác (VQA). Tập con HTML
của MINT-1T vượt trội hơn OBELICS trong các nhiệm vụ VQA nhưng hoạt động kém hơn trên benchmark chú thích.

hiệu suất học trong bối cảnh của họ. Trong Bảng 2, chúng tôi trình bày hiệu suất 4-shot và 8-shot trên
benchmark phổ biến; mô hình được đào tạo trên tài liệu HTML MINT-1T hoạt động tốt hơn OBELICS
trên nhiệm vụ VQA nhưng kém hơn trên benchmark chú thích. Trung bình OBELICS hoạt động tốt hơn một chút
so với MINT-1T (HTML). Chúng tôi khám phá cách kiến trúc mô hình ảnh hưởng đến kết quả này trong Phần 4.5.

MôhìnhDatasets
MMMU Mantis-Eval
OBELICS 37.6 44.2
MINT-1T (HTML) 35.2 41.5
MINT-1T 41.3 43.3

Bảng 3: Hiệu suất trên benchmark suy luận hình ảnh
MMMU và Mantis-Eval.

Thêm tài liệu PDF và ArXiv Tiếp theo, chúng tôi đào tạo
trên các nguồn dữ liệu đầy đủ của MINT-1T, với hỗn hợp
tài liệu HTML, PDF, và ArXiv. Chúng tôi lấy mẫu 50%
tài liệu xen kẽ từ HTML, 45% từ PDFs, và 5% từ ArXiv.
Chúng tôi đào tạo tổng cộng 10B token đa phương thức.
Như thấy trong Bảng 2, mô hình được đào tạo trên hỗn hợp
dữ liệu MINT-1T đầy đủ vượt trội hơn OBELICS và MINT-1T (HTML) trên hầu hết benchmark học trong bối cảnh.
Trên benchmark suy luận đa phương thức phức tạp hơn, mô hình MINT-1T vượt trội hơn
OBELICS trên MMMU nhưng hoạt động kém hơn trên Mantis-Eval.

4.4 Xu hướng chi tiết

Hiệu suất học trong bối cảnh mở rộng như thế nào với minh chứng? Chúng tôi đánh giá
hiệu suất học trong bối cảnh của mô hình khi được nhắc với một đến tám minh chứng. Chúng tôi chạy một
lần thử duy nhất cho mỗi số lượng shot cho mỗi benchmark đánh giá được mô tả trong Phần 4.2. Như thấy trong Hình 7a,
chúng tôi thấy rằng mô hình được đào tạo trên MINT-1T vượt trội hơn mô hình được đào tạo trên tập con HTML
của MINT-1T và OBELICS trên tất cả shot. Hơn nữa, chúng tôi thấy rằng mô hình MINT-1T (HTML) hoạt động
kém hơn một chút so với OBELICS.

Hiệu suất trên nhiệm vụ chú thích và trả lời câu hỏi thị giác Trong Hình 8, chúng tôi trình bày
hiệu suất học trong bối cảnh trung bình trên benchmark chú thích và trả lời câu hỏi thị giác (VQA).
OBELICS vượt trội hơn tất cả biến thể MINT-1T trên benchmark chú thích bốn shot
và hoạt động kém hơn một chút so với MINT-1T trên chú thích tám shot. Tuy nhiên, chúng tôi thấy rằng trên VQA

--- TRANG 9 ---
MINT-1T
(HTML)OBELICS6061626364Hiệu suất trung bình60.9061.1863.0563.23Thí nghiệm XGen-MM
MINT-1T
(HTML)OBELICS636465666768
65.03
64.2567.07
66.73Thí nghiệm Idefics2
4-shot
8-shot

Hình 9: Tác động của kiến trúc: Trên benchmark học trong bối cảnh, các mô hình XGen-MM hoạt động
tốt hơn một chút khi được đào tạo trên OBELICS so với tập con HTML của MINT-1T. Ngược lại,
các mô hình Idefics2 cho thấy lợi thế nhẹ cho MINT-1T (HTML) so với OBELICS.

Mô hình ShotsDatasets Trung bình
COCO TextCaps OKVQA TextVQA VQAv2 VizWiz
OBELICS4 110.2±0.38 83.1±1.28 52.8±0.04 46.7±0.23 63.8±0.10 28.9±0.50 64.3±0.24
8 111.8±1.04 86.6±0.31 54.8±0.04 46.9±0.02 64.1±0.09 36.2±0.51 66.7±0.20
MINT-1T (HTML)4 110.9±0.01 84.8±0.05 52.9±0.26 47.0±0.31 65.6±0.04 29.0±0.64 65.0±0.12
8 111.3±0.05 87.5±0.11 54.1±0.49 48.1±0.12 64.8±1.04 36.6±0.13 66.9±0.19

Bảng 4: Kết quả mô hình Idefics2: Chúng tôi so sánh OBELICS và MINT-1T (HTML), khi đào tạo
LMM Idefics2. Các mô hình được đánh giá sử dụng bốn và tám ví dụ học trong bối cảnh, với mỗi
đánh giá chạy cho hai lần thử. Chúng tôi báo cáo hiệu suất trung bình và độ lệch chuẩn.

benchmark MINT-1T tốt hơn đáng kể so với cả hai baseline. Chúng tôi cũng thấy rằng MINT-1T (HTML)
cũng vượt trội hơn OBELICS trên nhiệm vụ VQA.

Hiệu suất trên các lĩnh vực khác nhau Một động lực để bao gồm các lĩnh vực đa dạng trong MINT-1T là
để cải thiện khả năng tổng quát của mô hình. Trong Hình 7b, chúng tôi phân tích hiệu suất trên MMMU cho mỗi
lĩnh vực. Ngoại trừ lĩnh vực Kinh doanh, MINT-1T vượt trội hơn OBELICS và MINT-1T
(HTML). Chúng tôi nhấn mạnh sự gia tăng hiệu suất trong các lĩnh vực Khoa học và Công nghệ cho MINT-1T
và suy đoán rằng điều này có thể được quy cho sự phổ biến của các lĩnh vực này trong tài liệu ArXiv và PDF.

4.5 Tác động của kiến trúc mô hình

Các thí nghiệm của chúng tôi, được trình bày trong Phần 4.3, sử dụng kiến trúc của XGen-MM. Không giống như với
mô hình ngôn ngữ lớn, không gian thiết kế cho mô hình đa phương thức đa dạng hơn nhiều với nhiều kiến trúc
để căn chỉnh bộ mã hóa thị giác với mô hình ngôn ngữ. Tự nhiên, chúng tôi tò mò liệu kết quả của chúng tôi có
giữ nguyên cho các thiết lập đào tạo phổ biến khác không.

Để điều tra điều này, chúng tôi nhân bản các thí nghiệm đào tạo sử dụng kiến trúc của Idefics2. Idefics2
khác với XGen-MM ở chỗ nó đóng băng một mô hình ngôn ngữ lớn không được tinh chỉnh theo hướng dẫn và
thêm các ma trận LoRA [Laurenccon et al., 2024] trên tất cả các lớp tuyến tính. Để tái tạo Idefics2
chúng tôi sử dụng mô hình ngôn ngữ Mistral-7B-v0.3 và bộ mã hóa thị giác DFN ViT-H với độ phân giải 384.
Không giống như Idefics2, chúng tôi không thí nghiệm với độ phân giải hình ảnh linh hoạt trong đào tạo và giữ
bộ mã hóa thị giác hoàn toàn đóng băng. Chúng tôi trình bày kết quả học trong bối cảnh cho thí nghiệm mô hình Idefics2 trong
Bảng 4. Chúng tôi thấy rằng tập con HTML của MINT-1T hoạt động tốt hơn OBELICS với cải thiện đáng chú ý trên
TextVQA, VQAv2, và TextCaps. Chúng tôi nhấn mạnh sự khác biệt khoảng cách hiệu suất giữa
ablation XGen-MM và Idefics2 trong Hình 9. Một khác biệt chính trong thí nghiệm Idefics2 là
tập con HTML của MINT-1T hoạt động cạnh tranh hơn nhiều trên benchmark chú thích so với
OBELICS.

--- TRANG 10 ---
5 Nghiên cứu liên quan

5.1 Dữ liệu đa phương thức xen kẽ

Các bộ dữ liệu đa phương thức xen kẽ quy mô lớn đầu tiên được trình bày trong Flamingo [Alayrac et al., 2022]
và CM3 [Aghajanyan et al., 2022]. Kosmos [Huang et al., 2023] cho thấy các tính chất tương tự và
được theo sau bởi Multimodal-C4 [Zhu et al., 2023] và OBELICS [Laurençon et al., 2023], các bộ dữ liệu đa phương thức xen kẽ mã nguồn mở đầu tiên. Những công trình gần đây hơn như Chameleon [Meta, 2024]
và MM1 [McKinzie et al., 2024] đã mở rộng quy mô OBELICS để đào tạo các mô hình đa phương thức tiên tiến.
Một dòng công trình bổ sung, Mantis [Jiang et al., 2024] và MIMIC-IT [Li et al., 2023] xây dựng
các bộ dữ liệu tinh chỉnh hướng dẫn xen kẽ. Tương tự, Multimodal Arxiv [Li et al., 2024a] xây dựng
dữ liệu chú thích và tinh chỉnh hướng dẫn chất lượng cao từ các bài báo ArXiv.

5.2 Các bộ dữ liệu tiền đào tạo mã nguồn mở lớn

Các bộ dữ liệu tiền đào tạo lớn, chất lượng cao là xương sống của nghiên cứu mã nguồn mở. Trong các bộ dữ liệu ảnh-văn bản,
nơi các công trình sơ bộ [Schuhmann et al., 2021, Byeon et al., 2022, Schuhmann et al.,
2022, Gadre et al., 2023] tập trung vào việc mở rộng quy mô bộ dữ liệu ảnh-văn bản đến hàng tỷ mẫu và đã
rất quan trọng để đào tạo các mô hình đa phương thức mã nguồn mở mạnh mẽ [Ilharco et al., 2021, Sun et al., 2023a].
Tương tự trong mô hình hóa ngôn ngữ, các bộ dữ liệu lớn như Pile [Gao et al., 2020], Redpajama [Computer,
2023], RefinedWeb [Penedo et al., 2023], Dolma [Soldaini et al., 2024], Datacomp-LM [Li et al.,
2024b], và FineWeb [Penedo et al., 2024] đã rất quan trọng để đào tạo các mô hình mã nguồn mở hoàn toàn minh bạch.

5.3 Các mô hình đa phương thức lớn

Năm qua đã chứng kiến một dòng lớn các mô hình đa phương thức lớn (LMMs) mạnh mẽ. Có một dòng
công trình tìm cách tiền đào tạo các mô hình ngôn ngữ hiện có trên các bộ dữ liệu đa phương thức xen kẽ và ảnh-văn bản
quy mô lớn. Điều này được trình bày đầu tiên trong Flamingo [Alayrac et al., 2022] và được áp dụng bởi các mô hình mã nguồn mở
như OpenFlamingo [Awadalla et al., 2023], Idefics [Laurençon et al., 2023], và
Emu [Sun et al., 2023b]. Những công trình gần đây hơn như Idefics2 [Laurenccon et al., 2024], MM1 [McKinzie
et al., 2024], VILA [Lin et al., 2024], và XGen-MM [Xue et al., 2024] cũng đã áp dụng hỗn hợp dữ liệu tương tự.
Một dòng công trình riêng biệt căn chỉnh các mô hình ngôn ngữ lớn với bộ mã hóa thị giác sử dụng dữ liệu tinh chỉnh hướng dẫn
chất lượng cao và bộ dữ liệu ảnh-văn bản như LLaVA [Liu et al., 2023a,b], InstructBLIP [Dai
et al., 2023], QwenVL [Bai et al., 2023], Yi-VL [Young et al., 2024], MiniCPM-V [Hu et al., 2024],
và nhiều hơn nữa. Hơn nữa, thế hệ mô hình lớn mới nhất như GPT4-o [Achiam et al., 2023],
Gemini [Google, 2023], và Chameleon [Meta, 2024] được đào tạo trên dữ liệu đa phương thức từ đầu.

6 Hạn chế và kết luận

Trong công trình này, chúng tôi trình bày MINT-1T, bộ dữ liệu đa phương thức xen kẽ một nghìn tỷ token mã nguồn mở đầu tiên
và một thành phần quan trọng để đào tạo các mô hình đa phương thức lớn. Chúng tôi tin rằng MINT-1T sẽ là
một tài nguyên có giá trị cho cộng đồng nghiên cứu để thực hiện khoa học mở về các bộ dữ liệu đa phương thức xen kẽ.
Một cân nhắc quan trọng khi phát hành các bộ dữ liệu lớn là tránh phơi bày nội dung có hại từ
web. Chúng tôi đã cẩn thận về việc lọc bỏ thông tin nhận dạng cá nhân và nội dung không an toàn
cho công việc từ MINT-1T. Tuy nhiên, nghiên cứu tương lai nên khám phá nhiều cách hơn để cải thiện tính an toàn
của dữ liệu internet đa phương thức. Hơn nữa, công trình tiếp theo nên đào tạo các mô hình trên các tập con lớn hơn của
MINT-1T, xây dựng các phương pháp lọc để cải thiện chất lượng dữ liệu, và tuyển chọn các chuỗi đa phương thức từ
các nguồn khác.

Lời cảm ơn và tiết lộ nguồn tài trợ

Chúng tôi cảm ơn Srinath Meadusani và Lavanya Karanam đã làm việc về cơ sở hạ tầng, Jeffrey Li và
Alex Fang vì những thảo luận hữu ích về lọc và loại bỏ trùng lặp dữ liệu, Irena Gao đã dẫn dắt
phát triển codebase đào tạo OpenFlamingo mới mà chúng tôi sử dụng, Honglu Zhou đã duy trì
mã đánh giá, James Park và Marianna Nezhurina vì những thảo luận hữu ích về
phân tích PDF, và Paul Josel đã giúp chúng tôi với thiết kế hình.

--- TRANG 11 ---
Tài liệu tham khảo

Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, và
Victor Sanh. Obelics: Một bộ dữ liệu lọc quy mô web mở của các tài liệu ảnh-văn bản xen kẽ,
2023.

Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter,
Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet
Singh, Doug Kang, Ankur Jain, Hongyu He, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan
Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee,
Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, và Yinfei Yang. Mm1: Phương pháp,
phân tích & góc nhìn từ tiền đào tạo llm đa phương thức. ArXiv, abs/2403.09611, 2024. URL
https://api.semanticscholar.org/CorpusID:268384865.

Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, và Peter J. Liu. Khám phá giới hạn của học chuyển giao với
transformer text-to-text thống nhất. J. Mach. Learn. Res., 21:140:1–140:67, 2019. URL https://api.
semanticscholar.org/CorpusID:204838007.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor Leahy. The pile:
Một bộ dữ liệu 800gb văn bản đa dạng cho mô hình hóa ngôn ngữ. ArXiv, abs/2101.00027, 2020. URL
https://api.semanticscholar.org/CorpusID:230435736.

Ben Wang và Aran Komatsuzaki. GPT-J-6B: Một mô hình ngôn ngữ tự hồi quy 6 tỷ tham số.
https://github.com/kingoflolz/mesh-transformer-jax, Tháng 5 2021.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. GPT-Neo: Mô hình ngôn ngữ
tự hồi quy quy mô lớn với Mesh-Tensorflow, Tháng 3 2021. URL https://doi.
org/10.5281/zenodo.5297715. Nếu bạn sử dụng phần mềm này, vui lòng trích dẫn sử dụng metadata này.

Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, và Vaishaal
Shankar. Mạng lọc dữ liệu. ArXiv, abs/2309.17425, 2023. URL https://api.
semanticscholar.org/CorpusID:263310452.

Pratyush Maini, Sachin Goyal, Zachary Chase Lipton, J. Zico Kolter, và Aditi Raghunathan. T-mars:
Cải thiện biểu diễn thị giác bằng cách tránh học tính năng văn bản. ArXiv, abs/2307.03132,
2023. URL https://api.semanticscholar.org/CorpusID:259360759.

Google. Gemini: Một họ mô hình đa phương thức có khả năng cao, 2023.

Meta. Chameleon: Các mô hình nền tảng fusion sớm hỗn hợp, 2024.

OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor
Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff
Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bog-
donoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea
Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,
Ruby Chen, Jason Chen, Mark Chen, Benjamin Chess, Chester Cho, Casey Chu, Hyung Won
Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah
Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien
Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim'on Posada Fishman,
Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,
Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan
Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter
Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain,
Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie
Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish

--- TRANG 12 ---
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik
Kirchner, Jamie Ryan Kiros, Matthew Knight, Daniel Kokotajlo, Lukasz Kondraciuk, Andrew
Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai
Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,
Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Adeola Makanju,
Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer,
Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake
McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira Murati, Oleg Murk,
David M'ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo,
Hyeonwoo Noh, Ouyang Long, Cullen O'Keefe, Jakub W. Pachocki, Alex Paino, Joe Palermo, Ash-
ley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alexandre Passos, Mikhail
Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Hen-
rique Pondé de Oliveira Pinto, Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell,
Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick
Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David
Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,
Ian Sohl, Benjamin D. Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie
Summers, Ilya Sutskever, Jie Tang, Nikolas A. Tezak, Madeleine Thompson, Phil Tillet, Amin
Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer'on
Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll L. Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welin-
der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah
Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang,
Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, và Barret Zoph. Báo cáo kỹ thuật gpt-4. 2023. URL https://api.semanticscholar.org/CorpusID:257532815.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée Cojocaru, Alessandro
Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. Bộ dữ liệu
refinedweb cho falcon llm: Vượt trội hơn corpus được tuyển chọn với dữ liệu web, và chỉ dữ liệu web.
ArXiv, abs/2306.01116, 2023. URL https://api.semanticscholar.org/CorpusID:
259063761.

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,
Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha,
Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison,
Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander,
Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,
Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,
và Kyle Lo. Dolma: một corpus mở ba nghìn tỷ token cho nghiên cứu tiền đào tạo mô hình ngôn ngữ.
ArXiv, abs/2402.00159, 2024. URL https://api.semanticscholar.org/CorpusID:
267364861.

Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, và Thomas Wolf. Fineweb, 2024. URL
https://huggingface.co/datasets/HuggingFaceFW/fineweb.

Armand Joulin, Edouard Grave, Piotr Bojanowski, và Tomas Mikolov. Túi thủ thuật cho phân loại văn bản hiệu quả. Trong Kỷ yếu Hội nghị thường niên lần thứ 15 của Chương châu Âu của Hiệp hội
Ngôn ngữ học Tính toán: Tập 2, Bài báo ngắn, trang 427–431. Hiệp hội Ngôn ngữ học Tính toán, Tháng 4 2017.

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,
John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-
nigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne
Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri,
Saffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan
McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-

--- TRANG 13 ---
raine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki
Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug
Fritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cy-
prien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson,
Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Si-
mon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L.
Bennett, Demis Hassabis, Koray Kavukcuoglu, và Geoffrey Irving. Mở rộng quy mô mô hình ngôn ngữ:
Phương pháp, phân tích & góc nhìn từ việc đào tạo gopher. ArXiv, abs/2112.11446, 2021. URL
https://api.semanticscholar.org/CorpusID:245353475.

Gant Laborde. Deep nn cho phát hiện nsfw. URL https://github.com/GantMan/nsfw_model.

Dirk Groeneveld. Bộ lọc lớn thân thiện. https://github.com/allenai/bff, 2023.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán,
Armand Joulin, và Edouard Grave. Ccnet: Trích xuất bộ dữ liệu đơn ngữ chất lượng cao từ
dữ liệu thu thập web. Trong Hội nghị Quốc tế về Tài nguyên và Đánh giá Ngôn ngữ, 2019. URL
https://api.semanticscholar.org/CorpusID:207870323.

Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae
Yu, Ludwig Schmidt, William Yang Wang, và Yejin Choi. Multimodal c4: Một corpus mở, quy mô tỷ
của hình ảnh xen kẽ với văn bản. ArXiv, abs/2304.06939, 2023. URL https://api.
semanticscholar.org/CorpusID:258170467.

Hazel Victoria Campbell, Abram Hindle, và Eleni Stroulia. Phân bổ dirichlet tiềm ẩn. Trong Nghệ thuật
và Khoa học Phân tích Dữ liệu Phần mềm, 2003. URL https://api.semanticscholar.org/
CorpusID:215924728.

Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj
Prabhu, Yutong Dai, Michael S. Ryoo, Shrikant B. Kendre, Jieyu Zhang, Can Qin, Shu Zhen Zhang,
Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin
Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, và
Ran Xu. xgen-mm (blip-3): Một họ mô hình đa phương thức lớn mở. ArXiv, abs/2408.08872,
2024. URL https://api.semanticscholar.org/CorpusID:271891872.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Ruther-
ford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob
Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko-
laj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, và Karen Simonyan.
Flamingo: một mô hình ngôn ngữ thị giác cho học few-shot. ArXiv, abs/2204.14198, 2022. URL
https://api.semanticscholar.org/CorpusID:248476411.

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Hassan
Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Singh Behl, Alon Benhaim, Misha
Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen,
Vishrav Chaudhary, Parul Chopra, Allison Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen
Eldan, Dan Iter, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J.
Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,
Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh
Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang
Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olli Saarikivi, Amin Saied, Adil Salim,
Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xianmin Song, Olatunji Ruwase,
Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu,
Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Cheng-Yuan Zhang, Cyril Zhang, Jianwen
Zhang, Li Lyna Zhang, Yi Zhang, Yunan Zhang, và Xiren Zhou. Báo cáo kỹ thuật phi-3: Một
mô hình ngôn ngữ có khả năng cao cục bộ trên điện thoại của bạn. 2024. URL https://api.semanticscholar.
org/CorpusID:269293048.

--- TRANG 14 ---
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,
Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel
Ilharco, Mitchell Wortsman, và Ludwig Schmidt. Openflamingo: Một framework mã nguồn mở cho
đào tạo mô hình ngôn ngữ-thị giác tự hồi quy lớn, 2023.

Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, và C. Lawrence Zitnick. Microsoft coco: Các đối tượng thông thường trong bối cảnh. Trong Hội nghị
Thị giác Máy tính châu Âu, 2014. URL https://api.semanticscholar.org/CorpusID:
14113767.

Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, và Amanpreet Singh. Textcaps: một bộ dữ liệu cho
chú thích hình ảnh với hiểu đọc. ArXiv, abs/2003.12462, 2020. URL https:
//api.semanticscholar.org/CorpusID:214693197.

Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi
Parikh, và Dhruv Batra. Vqa: Trả lời câu hỏi thị giác. Tạp chí Quốc tế Thị giác Máy tính, 123:4 – 31, 2015. URL https://api.semanticscholar.org/CorpusID:3180429.

Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và Roozbeh Mottaghi. Ok-vqa: Một benchmark trả lời câu hỏi thị giác
yêu cầu kiến thức bên ngoài. Hội nghị 2019 IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 3190–3199, 2019. URL https://api.
semanticscholar.org/CorpusID:173991173.

Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, và
Marcus Rohrbach. Hướng tới các mô hình vqa có thể đọc. Trong Kỷ yếu Hội nghị IEEE về
Thị giác Máy tính và Nhận dạng Mẫu, trang 8317–8326, 2019.

Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, và
Jeffrey P. Bigham. Thử thách lớn vizwiz: Trả lời câu hỏi thị giác từ người khiếm thị. Hội nghị
2018 IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, trang 3608–3617, 2018.
URL https://api.semanticscholar.org/CorpusID:3831582.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu
Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin,
Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, và Wenhu Chen.
Mmmu: Một benchmark hiểu và suy luận đa phương thức đa lĩnh vực lớn cho expert
agi. Trong Kỷ yếu CVPR, 2024.

Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max W.F. Ku, Qian Liu, và Wenhu Chen. Mantis:
Tinh chỉnh hướng dẫn nhiều hình ảnh xen kẽ. 2024. URL https://api.semanticscholar.org/
CorpusID:269502715.

Hugo Laurenccon, Léo Tronchon, Matthieu Cord, và Victor Sanh. Điều gì quan trọng khi xây dựng
mô hình ngôn ngữ-thị giác? 2024. URL https://api.semanticscholar.org/CorpusID:
269587869.

Armen Aghajanyan, Po-Yao (Bernie) Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman
Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, và Luke Zettlemoyer. Cm3:
Một mô hình đa phương thức có mặt nạ nhân quả của internet. ArXiv, abs/2201.07520, 2022. URL https:
//api.semanticscholar.org/CorpusID:246035820.

Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao
Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,
Vishrav Chaudhary, Subhojit Som, Xia Song, và Furu Wei. Ngôn ngữ không phải là tất cả những gì bạn cần:
Căn chỉnh nhận thức với mô hình ngôn ngữ. ArXiv, abs/2302.14045, 2023. URL https://api.
semanticscholar.org/CorpusID:257219775.

Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, C. Li, và Ziwei
Liu. Mimic-it: Tinh chỉnh hướng dẫn trong bối cảnh đa phương thức. ArXiv, abs/2306.05425, 2023. URL
https://api.semanticscholar.org/CorpusID:259108295.

Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, và Qi Liu. Multimodal
arxiv: Một bộ dữ liệu để cải thiện khả năng hiểu khoa học của mô hình ngôn ngữ-thị giác lớn. ArXiv,
abs/2403.00231, 2024a. URL https://api.semanticscholar.org/CorpusID:268201930.

--- TRANG 15 ---
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
Aarush Katta, Theo Coombes, Jenia Jitsev, và Aran Komatsuzaki. Laion-400m: Bộ dữ liệu mở
400 triệu cặp ảnh-văn bản được lọc bởi clip. ArXiv, abs/2111.02114, 2021. URL https://api.
semanticscholar.org/CorpusID:241033103.

Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, và Saehoon Kim.
Coyo-700m: Bộ dữ liệu cặp ảnh-văn bản. https://github.com/kakaobrain/coyo-dataset,
2022.

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi
Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski,
Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, và Jenia Jitsev.
Laion-5b: Một bộ dữ liệu mở quy mô lớn để đào tạo mô hình ảnh-văn bản thế hệ tiếp theo. ArXiv,
abs/2210.08402, 2022. URL https://api.semanticscholar.org/CorpusID:252917726.

Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,
Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari,
Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Muss-
mann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander J.
Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alexan-
dros G. Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, và Ludwig Schmidt. Datacomp:
Tìm kiếm thế hệ tiếp theo của bộ dữ liệu đa phương thức. ArXiv, abs/2304.14108, 2023. URL
https://api.semanticscholar.org/CorpusID:258352812.

Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,
Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali
Farhadi, và Ludwig Schmidt. Openclip, Tháng 7 2021. URL https://doi.org/10.5281/zenodo.
5143773. Nếu bạn sử dụng phần mềm này, vui lòng trích dẫn như bên dưới.

Quan Sun, Yuxin Fang, Ledell Yu Wu, Xinlong Wang, và Yue Cao. Eva-clip: Các kỹ thuật đào tạo
được cải thiện cho clip ở quy mô lớn. ArXiv, abs/2303.15389, 2023a. URL https://api.semanticscholar.
org/CorpusID:257766387.

Together Computer. Redpajama: một bộ dữ liệu mở để đào tạo mô hình ngôn ngữ lớn, 2023. URL
https://github.com/togethercomputer/RedPajama-Data.

Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal,
Etash Kumar Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff,
Reinhard Heckel, Jean-Pierre Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon
Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh
Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco,
Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen,
Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh,
Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie
Wang, Dirk Groeneveld, Luca Soldani, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G.
Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, và Vaishaal Shankar. Datacomp-lm: Tìm kiếm
thế hệ tiếp theo của bộ đào tạo cho mô hình ngôn ngữ. ArXiv, abs/2406.11794, 2024b.
URL https://api.semanticscholar.org/CorpusID:270560330.

Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang,
Yongming Rao, Jingjing Liu, Tiejun Huang, và Xinlong Wang. Các mô hình đa phương thức sinh
là những người học trong bối cảnh. ArXiv, abs/2312.13286, 2023b. URL https://api.semanticscholar.
org/CorpusID:266374640.

Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,
Mohammad Shoeybi, và Song Han. VILA: về tiền đào tạo cho mô hình ngôn ngữ thị giác. Trong CVPR,
2024.

Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee. Các baseline được cải thiện với tinh chỉnh hướng dẫn thị giác, 2023a.

Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Tinh chỉnh hướng dẫn thị giác. Trong NeurIPS,
2023b.

--- TRANG 16 ---
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Albert Li, Pascale Fung, và Steven C. H. Hoi. Instructblip: Hướng tới mô hình ngôn ngữ-thị giác
đa mục đích với tinh chỉnh hướng dẫn. ArXiv, abs/2305.06500, 2023. URL https:
//api.semanticscholar.org/CorpusID:258615266.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou, và Jingren Zhou. Qwen-vl: Một mô hình ngôn ngữ-thị giác đa năng cho hiểu biết, định vị,
đọc văn bản, và hơn thế nữa. 2023. URL https://api.semanticscholar.org/CorpusID:
261101015.

01.AI Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,
Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin
Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,
Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, và
Zonghong Dai. Yi: Các mô hình nền tảng mở bởi 01.ai. ArXiv, abs/2403.04652, 2024. URL
https://api.semanticscholar.org/CorpusID:268264158.

Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,
Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kaihuo Zhang, Chongyi Wang,
Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chaochao Jia, Guoyang
Zeng, Dahai Li, Zhiyuan Liu, và Maosong Sun. Minicpm: Khám phá tiềm năng của mô hình ngôn ngữ
nhỏ với chiến lược đào tạo có thể mở rộng. ArXiv, abs/2404.06395, 2024. URL https:
//api.semanticscholar.org/CorpusID:269009975.

Những người đóng góp OpenCompass. Opencompass: Một nền tảng đánh giá phổ quát cho mô hình nền tảng.
https://github.com/open-compass/opencompass, 2023.

Danh mục kiểm tra

1. Đối với tất cả tác giả...
(a) Các tuyên bố chính trong tóm tắt và giới thiệu có phản ánh chính xác đóng góp và phạm vi của bài báo không? [Có]
(b) Bạn có mô tả các hạn chế của công trình không? [Có] Xem Phần 6
(c) Bạn có thảo luận bất kỳ tác động xã hội tiêu cực tiềm năng nào của công trình không? [Có] Xem Phần 6.
(d) Bạn có đọc hướng dẫn xem xét đạo đức và đảm bảo rằng bài báo tuân theo chúng không? [Có]

2. Nếu bạn bao gồm kết quả lý thuyết...
(a) Bạn có nêu tập hợp đầy đủ các giả định của tất cả kết quả lý thuyết không? [N/A]
(b) Bạn có bao gồm bằng chứng hoàn chỉnh của tất cả kết quả lý thuyết không? [N/A]

3. Nếu bạn chạy thí nghiệm (ví dụ: cho benchmark)...
(a) Bạn có bao gồm mã, dữ liệu và hướng dẫn cần thiết để tái tạo kết quả thí nghiệm chính không (trong tài liệu bổ sung hoặc dưới dạng URL)? [Có] Chi tiết về hỗn hợp dữ liệu chúng tôi đào tạo và đánh giá cũng như codebase chúng tôi sử dụng có thể tìm thấy trong Phần 4.1 và Phần 4.2 tương ứng.
(b) Bạn có chỉ rõ tất cả chi tiết đào tạo không (ví dụ: chia dữ liệu, siêu tham số, cách chúng được chọn)? [Có] Tất cả siêu tham số đào tạo và quyết định kiến trúc có thể tìm thấy trong Phần 4.1.
(c) Bạn có báo cáo thanh lỗi không (ví dụ: đối với seed ngẫu nhiên sau khi chạy thí nghiệm nhiều lần)? [Có] Chúng tôi chạy ba lần thử cho đánh giá học trong bối cảnh để giảm nhiễu. Chúng tôi báo cáo điểm trung bình và độ lệch chuẩn trong Bảng 2
(d) Bạn có bao gồm tổng lượng tính toán và loại tài nguyên được sử dụng không (ví dụ: loại GPU, cụm nội bộ, hoặc nhà cung cấp đám mây)? [Có] Chúng tôi bao gồm chi tiết tính toán để xử lý bộ dữ liệu trong Phần 2 và chi tiết về cơ sở hạ tầng đào tạo trong Phần 4.1)

4. Nếu bạn sử dụng tài sản hiện có (ví dụ: mã, dữ liệu, mô hình) hoặc tuyển chọn/phát hành tài sản mới...

--- TRANG 17 ---
(a) Nếu công trình của bạn sử dụng tài sản hiện có, bạn có trích dẫn người tạo ra không? [N/A]
(b) Bạn có đề cập đến giấy phép của tài sản không? [N/A]
(c) Bạn có bao gồm bất kỳ tài sản mới nào trong tài liệu bổ sung hoặc dưới dạng URL không? [N/A]
(d) Bạn có thảo luận xem có và cách thu được sự đồng ý từ những người có dữ liệu bạn đang sử dụng/tuyển chọn không? [N/A]
(e) Bạn có thảo luận xem dữ liệu bạn đang sử dụng/tuyển chọn có chứa thông tin nhận dạng cá nhân hoặc nội dung xúc phạm không? [Có] Chúng tôi thực hiện nhiều biện pháp để loại bỏ thông tin nhận dạng cá nhân và nội dung xúc phạm từ bộ dữ liệu. Bao gồm che giấu PII trong văn bản và loại bỏ hình ảnh xúc phạm (xem Phần 2.4).

5. Nếu bạn sử dụng crowdsourcing hoặc tiến hành nghiên cứu với đối tượng con người...
(a) Bạn có bao gồm văn bản đầy đủ của hướng dẫn đưa cho người tham gia và ảnh chụp màn hình, nếu có không? [N/A]
(b) Bạn có mô tả bất kỳ rủi ro tiềm ẩn nào cho người tham gia, với liên kết đến Phê duyệt Hội đồng Xem xét Thể chế (IRB), nếu có không? [N/A]
(c) Bạn có bao gồm mức lương theo giờ ước tính trả cho người tham gia và tổng số tiền chi cho bồi thường người tham gia không? [N/A]

--- TRANG 18 ---
A Phụ lục

A.1 Vấn đề phân tích PDF

Mẹo an toàn cháy nổ
cho
Chủ nhà nông thôn

Hiệp hội Cứu hỏa Nông thôn bao gồm
năm thị trấn xung quanh River
Falls hợp đồng với Thành phố River
Falls để bảo vệ người dân River Falls
và các thị trấn xung quanh khỏi hỏa hoạn và
các mối nguy hiểm liên quan. Là một chủ nhà nông thôn
bạn ở xa hơn so với bảo vệ khỏi hỏa hoạn và nên thực hiện mọi
biện pháp phòng ngừa để ngăn chặn hỏa hoạn trên tài sản của bạn hoặc trong nhà.

Cháy rừng

Thị trấn River Falls yêu cầu
rằng bạn phải có giấy phép đốt
trước khi đốt bất kỳ cây bụi hoặc cỏ nào.
Giấy phép đốt hàng năm của Thị trấn River Falls
chỉ rõ những gì có thể được đốt và có hướng dẫn cần
tuân theo trước, trong và sau khi bạn đốt. Liên hệ một quan chức của Thị trấn River Falls để xin giấy phép đốt
và thông báo cho Trung tâm Điều phối Quận Pierce trước khi đốt.
715- 273-5051). Điều này sẽ ngăn chặn
chi phí không cần thiết —tất cả cuộc gọi
cháy tài sản, ngay cả báo động giả, hiện tại
được tính phí tối thiểu $800.00.

Tránh đốt vào những ngày có gió hoặc
trong thời kỳ khô hạn.

Tạo một vùng an toàn xung quanh
nhà và tòa nhà của bạn. Dọn dẹp
thực vật dễ cháy trong ít nhất
bán kính 30 feet quanh nhà để tạo
cảnh quan khôn ngoan về hỏa hoạn.

Xếp củi cách xa nhà.

Luôn có nước sẵn sàng.

Bếp đốt củi

Bếp đốt củi là nguồn sưởi ấm
phổ biến trong khu vực của chúng ta. Sử dụng
an toàn —hãy xem xét các yếu tố sau.

Không đặt bếp trong khu vực
giao thông cao.

Giữ bất cứ thứ gì có thể bắt lửa
cách bếp ít nhất 36''.

Không phơi quần áo trên bếp —
quần áo có thể rơi và bốc cháy.

Lắp đặt bếp theo quy định
địa phương và những gì được đề xuất
bởi nhà sản xuất.

Đảm bảo thông gió đúng cách trong
nhà.

Tránh đặt tro gần vật liệu
dễ cháy trước khi bạn chắc chắn tất cả
than hồng đã tắt.

Kiểm tra và làm sạch bếp, đầu nối
ống khói, và ống khói ít nhất
một lần một năm.

Kiểm tra hai lần một tháng để tích tụ
creosote trong đầu nối ống khói
và ống khói.

Không bao giờ sử dụng hoặc cất giữ chất lỏng
dễ cháy, đặc biệt là xăng, gần bếp.

Chỉ đốt củi khô. Củi xanh
đốt không hiệu quả và gây
tích tụ creosote.

Nhận biết nguy cơ của ngộ độc CO,
có thể xảy ra do đốt cháy không hoàn toàn
trong bếp đốt củi, lò sưởi, hoặc thiết bị của bạn.
Máy dò CO có sẵn trong các cửa hàng.

BẮT ĐẦU

Hình 10: Kết quả trích xuất thứ tự đọc PDF không chính xác. Như được hiển thị trong hình này, một vấn đề phổ biến
là đọc không chính xác qua ranh giới cột. Các vấn đề khác bao gồm việc bắt đầu đọc một cách
sai từ cột ngoài cùng bên phải.

Chúng tôi nhận thấy hai hạn chế khi sử dụng gói PyMuPDF để phân tích PDF: 1) đôi khi chúng tôi
không thể phát hiện hình ảnh 2) vì PDF có bố cục tùy ý, trong một số trường hợp chúng tôi trích xuất
thứ tự đọc không chính xác (xem Hình 10). Sự thỏa hiệp giữa tốc độ và độ chính xác này là một hạn chế được ghi nhận,
và nghiên cứu tương lai nên tập trung vào việc phát triển các phương pháp nhanh, mạnh mẽ để xác định thứ tự đọc
trên các bố cục tài liệu đa dạng.

A.2 Chi tiết đánh giá

Trong phần này chúng tôi mô tả chi tiết cách chúng tôi nhắc các mô hình cho từng nhiệm vụ đánh giá và
tham số tạo chúng tôi áp dụng.

Học trong bối cảnh Chúng tôi ablate nhiều prompt cho các benchmark chú thích và trả lời câu hỏi thị giác
và tìm thấy prompt hoạt động tốt nhất trên tất cả các mô hình đa phương thức lớn dựa trên Phi-3 mini.
Đối với chú thích COCO chúng tôi sử dụng prompt "Một mô tả ngắn gọn về hình ảnh này trong một câu: ".
Vì TextCaps là nhiệm vụ chú thích chuyên sâu OCR hơn, chúng tôi sử dụng "Một mô tả ngắn gọn về hình ảnh này
mô tả nội dung thị giác và đánh vần rõ ràng bất kỳ văn bản nào có trong đó: ".
Đối với VQAv2, VizWiz, và OKVQA chúng tôi sử dụng prompt "Câu hỏi: <câu hỏi> Câu trả lời ngắn gọn trong một cụm từ hoặc từ đơn: "
và đối với TextVQA chúng tôi sử dụng "Câu hỏi: <câu hỏi> Câu trả lời ngắn gọn trong một cụm từ rất đơn giản: ".
Đối với tất cả nhiệm vụ chúng tôi sử dụng giải mã tham lam với độ dài tạo tối đa 56 token.
Đối với mỗi lần chạy đánh giá, chúng tôi thí nghiệm bằng cách tách minh chứng sử dụng <|endofchunk|>
hoặc dấu phân cách xuống dòng đôi. Chúng tôi thấy rằng các mô hình được đào tạo MINT hoạt động tốt hơn khi sử dụng
dấu phân cách xuống dòng đôi trong khi mô hình được đào tạo OBELICS hoạt động tốt hơn sử dụng dấu phân cách <|endofchunk|>.
Chúng tôi không báo cáo điểm tốt nhất cho mỗi lần thử đánh giá bằng cách ablate qua các dấu phân cách mà
chọn prompt tổng thể tốt nhất cho một mô hình dựa trên điểm tổng hợp và báo cáo kết quả sử dụng prompt đó cho tất cả đánh giá.

--- TRANG 19 ---
MMMU Đánh giá MMMU của chúng tôi trong Phần 4 dựa trên triển khai từ
Những người đóng góp VLMEvalKit [2023]4. Chúng tôi sửa đổi codebase để hỗ trợ định nghĩa mô hình của chúng tôi. Chúng tôi sử dụng
chiến lược prompting mặc định từ codebase này, thêm " Câu trả lời: " vào câu hỏi. Kết quả
của chúng tôi trên MMMU được thu được theo cách zero-shot trên tập validation. Chúng tôi sử dụng lấy mẫu tham lam
cho tạo ngôn ngữ với độ dài token đầu ra tối đa được đặt thành 32.

Mantis-Eval Chúng tôi sử dụng codebase Mantis-Eval Jiang et al. [2024] chính thức5 để đánh giá mô hình
của chúng tôi trên benchmark này. Mantis-Eval bao gồm hai loại câu hỏi: câu hỏi "nhiều lựa chọn" và "câu trả lời ngắn".
Tại thời điểm suy luận, chúng tôi thêm một minh chứng one-shot vào đầu mỗi câu hỏi. Các minh chứng
được cung cấp trong codebase eval bởi tác giả của Mantis. Đối với mỗi loại câu hỏi,
chúng tôi sử dụng một ví dụ cố định làm minh chứng. Minh chứng one-shot và câu hỏi thực tế
được phân tách bằng token <|endofchunk|>. Đối với tạo ngôn ngữ, chúng tôi sử dụng cấu hình
được cung cấp bởi Mantis-Eval, lấy mẫu đầu ra mô hình với tìm kiếm beam với num_beams=3, và
độ dài token đầu ra tối đa là 512.

A.3 Datasheet

A.3.1 Động lực

•Bộ dữ liệu được tạo ra cho mục đích gì? Có nhiệm vụ cụ thể nào trong đầu không? Có khoảng trống cụ thể nào cần lấp đầy không? Vui lòng cung cấp mô tả.
MINT-1T được xây dựng để tiền đào tạo các mô hình đa phương thức lớn có thể xử lý nhiều hình ảnh
và văn bản xen kẽ. Chúng tôi lấp đầy khoảng trống trong không gian mã nguồn mở nơi thiếu
bộ dữ liệu tiền đào tạo đa phương thức xen kẽ quy mô lớn.

•Ai tạo ra bộ dữ liệu (ví dụ: nhóm nào, nhóm nghiên cứu nào) và thay mặt cho thực thể nào (ví dụ: công ty, tổ chức, tổ chức)?
Bộ dữ liệu được tạo ra bởi một nhóm từ Đại học Washington, Nghiên cứu Salesforce,
Đại học Stanford, Đại học Texas tại Austin, và Đại học California Berkeley.

•Ai tài trợ cho việc tạo ra bộ dữ liệu? Nếu có khoản tài trợ liên quan, vui lòng cung cấp
tên của người tài trợ và tên cũng như số của khoản tài trợ.
Tính toán để xây dựng MINT-1T và đào tạo các mô hình ablation đến từ Nghiên cứu Salesforce.

•Có nhận xét nào khác không?

A.3.2 Thành phần

•Các thể hiện bao gồm bộ dữ liệu đại diện cho gì (ví dụ: tài liệu, ảnh,
người, quốc gia)? Có nhiều loại thể hiện không (ví dụ: phim, người dùng, và xếp hạng;
người và tương tác giữa họ; node và cạnh)? Vui lòng cung cấp mô tả.
Bộ dữ liệu phát hành chứa tài liệu đa phương thức HTML, PDF, và ArXiv.

•Có bao nhiêu thể hiện tổng cộng (của mỗi loại, nếu thích hợp)?
Tổng cộng có 1054.3 triệu (M) tài liệu (1029.4M tài liệu HTML, 24.0M tài liệu PDF,
và 0.87M tài liệu ArXiv).

•Bộ dữ liệu có chứa tất cả các thể hiện có thể không hoặc là một mẫu (không nhất thiết ngẫu nhiên)
các thể hiện từ một tập hợp lớn hơn? Nếu bộ dữ liệu là một mẫu, thì tập hợp lớn hơn là gì? Mẫu
có đại diện cho tập hợp lớn hơn không (ví dụ: phạm vi bao phủ địa lý)? Nếu có, vui lòng mô tả
cách tính đại diện này đã được xác thực/xác minh. Nếu nó không đại diện cho tập hợp lớn hơn,
vui lòng mô tả tại sao không (ví dụ: để bao phủ phạm vi thể hiện đa dạng hơn, vì các thể hiện
bị giữ lại hoặc không có sẵn).
Tài liệu HTML của MINT-1T được lọc từ các dump WARC CommonCrawl từ 2017 đến
2024 dựa trên chất lượng văn bản, sự hiện diện của nội dung trùng lặp và không mong muốn, và
khả năng sẵn có của hình ảnh để tải xuống. Tài liệu PDF MINT-1T được lọc từ CommonCrawl
dumps WAT từ 2023 đến 2024 cũng dựa trên chất lượng văn bản và sự hiện diện của nội dung trùng lặp
và không mong muốn. Tài liệu ArXiv MINT-1T là một tập con của tất cả tài liệu ArXiv.

--- TRANG 20 ---
•Mỗi thể hiện bao gồm dữ liệu gì? Dữ liệu "thô" (ví dụ: văn bản hoặc hình ảnh chưa xử lý)
hay tính năng? Trong mọi trường hợp, vui lòng cung cấp mô tả.
Đối với tài liệu HTML, chúng tôi phát hành văn bản của tài liệu cùng với URL hình ảnh. Đối với PDF và
ArXiv, chúng tôi cũng phát hành văn bản cùng với URL cho PDF và danh sách số tham chiếu hình ảnh
được sử dụng để phân tích hình ảnh từ tài liệu.

•Có nhãn hoặc mục tiêu liên quan đến mỗi thể hiện không? Nếu có, vui lòng cung cấp mô tả.
Không áp dụng.

•Có thông tin nào thiếu từ các thể hiện riêng lẻ không? Nếu có, vui lòng cung cấp mô tả,
giải thích tại sao thông tin này thiếu (ví dụ: vì nó không có sẵn). Điều này không
bao gồm thông tin bị loại bỏ có chủ ý, nhưng có thể bao gồm, ví dụ: văn bản được che.
Không có.

•Mối quan hệ giữa các thể hiện riêng lẻ có được làm rõ không (ví dụ: xếp hạng phim của người dùng,
liên kết mạng xã hội)? Nếu có, vui lòng mô tả cách các mối quan hệ này được làm rõ.
Không áp dụng.

•Có phân chia dữ liệu được khuyến nghị không (ví dụ: đào tạo, phát triển/xác thực, kiểm tra)? Nếu
có, vui lòng cung cấp mô tả về các phân chia này, giải thích lý do đằng sau chúng.
Chỉ có phân chia đào tạo cho MINT-1T.

•Có lỗi, nguồn nhiễu, hoặc dư thừa nào trong bộ dữ liệu không? Nếu có, vui lòng
cung cấp mô tả.
Không có.

•Bộ dữ liệu có tự chứa không, hay nó liên kết đến hoặc phụ thuộc vào tài nguyên bên ngoài
(ví dụ: trang web, tweet, bộ dữ liệu khác)? Nếu nó liên kết đến hoặc phụ thuộc vào tài nguyên bên ngoài, a) có
đảm bảo rằng chúng sẽ tồn tại và duy trì không đổi theo thời gian không; b) có
phiên bản lưu trữ chính thức của bộ dữ liệu hoàn chỉnh không (tức là bao gồm tài nguyên bên ngoài khi
chúng tồn tại tại thời điểm bộ dữ liệu được tạo); c) có bất kỳ hạn chế nào (ví dụ: giấy phép, phí)
liên quan đến bất kỳ tài nguyên bên ngoài nào có thể áp dụng cho người tiêu dùng bộ dữ liệu không? Vui lòng
cung cấp mô tả về tất cả tài nguyên bên ngoài và bất kỳ hạn chế nào liên quan đến chúng, cũng như
liên kết hoặc điểm truy cập khác, nếu thích hợp.
MINT-1T không tự chứa và phụ thuộc vào việc tải xuống URL hình ảnh bên ngoài để thu thập
bộ dữ liệu đầy đủ. Không có đảm bảo rằng các URL này sẽ vẫn có sẵn theo thời gian vì link rot là
vấn đề phổ biến cho bộ dữ liệu hình ảnh quy mô lớn. Hơn nữa, vì bộ dữ liệu này lớn hơn 300TB,
việc chúng tôi lưu trữ bộ dữ liệu đầy đủ (với hình ảnh bao gồm) là không khả thi. Không có
hạn chế nào liên quan đến việc tải xuống hình ảnh từ các URL bên ngoài này.

•Bộ dữ liệu có chứa dữ liệu có thể được coi là bí mật không (ví dụ: dữ liệu được bảo vệ
bởi đặc quyền pháp lý hoặc bởi bảo mật bác sĩ-bệnh nhân, dữ liệu bao gồm nội dung
giao tiếp không công khai của cá nhân)? Nếu có, vui lòng cung cấp mô tả.
Có, trong khi MINT-1T được lấy nguồn từ web công cộng, có thể có nội dung được coi là
bí mật.

•Bộ dữ liệu có chứa dữ liệu mà nếu xem trực tiếp, có thể xúc phạm, sỉ nhục,
đe dọa, hoặc có thể gây lo lắng không? Nếu có, vui lòng mô tả tại sao.
Có, có thể tìm thấy nội dung như vậy trong MINT-1T. Chúng tôi thực hiện nhiều bước để loại bỏ
nội dung như vậy bằng cách loại bỏ URL với chuỗi con có thể liên quan đến nội dung không an toàn cho công việc và
không mong muốn. Chúng tôi che giấu tất cả địa chỉ email và địa chỉ IP để tránh rò rỉ dữ liệu như vậy.
Hơn nữa, chúng tôi chạy một bộ phân loại hình ảnh trên toàn bộ bộ dữ liệu để loại bỏ hình ảnh khiêu dâm
và không mong muốn.

Nếu bộ dữ liệu không liên quan đến con người, bạn có thể bỏ qua các câu hỏi còn lại trong phần này.

•Bộ dữ liệu có xác định bất kỳ nhóm con nào không (ví dụ: theo tuổi, giới tính)? Nếu có, vui lòng
mô tả cách các nhóm con này được xác định và cung cấp mô tả về phân phối tương ứng
của chúng trong bộ dữ liệu.
Chúng tôi không xác định rõ ràng bất kỳ nhóm con nào nhưng có thể trích xuất thông tin này
từ bộ dữ liệu.

--- TRANG 21 ---
•Có thể xác định cá nhân (tức là một hoặc nhiều người tự nhiên), trực tiếp hoặc
gián tiếp (tức là kết hợp với dữ liệu khác) từ bộ dữ liệu không? Nếu có, vui lòng mô tả
cách thức.
Có, nếu nó có mặt trên web công cộng thì có thể tìm thấy hình ảnh của cá nhân cũng như
văn bản về cá nhân cụ thể. Chúng tôi đã che giấu thông tin nhận dạng cá nhân như
email và địa chỉ IP để loại bỏ dữ liệu cá nhân từ MINT-1T.

•Bộ dữ liệu có chứa dữ liệu có thể được coi là nhạy cảm theo bất kỳ cách nào không (ví dụ:
dữ liệu tiết lộ nguồn gốc chủng tộc hoặc dân tộc, khuynh hướng tình dục, tín ngưỡng tôn giáo,
quan điểm chính trị hoặc tư cách thành viên công đoàn, hoặc địa điểm; dữ liệu tài chính hoặc sức khỏe;
dữ liệu sinh trắc học hoặc di truyền; hình thức nhận dạng chính phủ, như số an sinh xã hội;
lịch sử tội phạm)? Nếu có, vui lòng cung cấp mô tả.
Có, nếu nó có mặt trên web công cộng thì có thể tìm thấy dữ liệu như vậy trong MINT-1T.

•Có nhận xét nào khác không?

A.3.3 Quy trình thu thập

•Dữ liệu liên quan đến mỗi thể hiện được thu thập như thế nào? Dữ liệu có thể quan sát trực tiếp
(ví dụ: văn bản thô, xếp hạng phim), do đối tượng báo cáo (ví dụ: phản hồi khảo sát), hoặc
được suy luận/dẫn xuất gián tiếp từ dữ liệu khác (ví dụ: thẻ part-of-speech, dự đoán dựa trên mô hình
cho tuổi hoặc ngôn ngữ)? Nếu dữ liệu được báo cáo bởi đối tượng hoặc được suy luận/dẫn xuất gián tiếp từ
dữ liệu khác, dữ liệu có được xác thực/xác minh không? Nếu có, vui lòng mô tả cách thức.
Dữ liệu không được lấy nguồn từ phản hồi của con người. Dữ liệu đến từ dump CommonCrawl
và được lọc sử dụng một loạt heuristic dựa trên quy tắc và phương pháp loại bỏ trùng lặp.

•Cơ chế hoặc quy trình nào được sử dụng để thu thập dữ liệu (ví dụ: thiết bị phần cứng
hoặc cảm biến, tuyển chọn thủ công của con người, chương trình phần mềm, API phần mềm)? Các
cơ chế hoặc quy trình này được xác thực như thế nào?
Dữ liệu được phân tích từ tài liệu HTML và PDF đến từ dump CommonCrawl và
ArXiv. Chúng tôi áp dụng các phương pháp lọc chất lượng và loại bỏ trùng lặp đã được xác thực trước đó
trong các bộ dữ liệu quy mô lớn khác. Chúng tôi xác thực phương pháp bằng cách đào tạo nhiều
mô hình đa phương thức lớn.

•Nếu bộ dữ liệu là một mẫu từ một tập hợp lớn hơn, chiến lược lấy mẫu là gì (ví dụ:
xác định, xác suất với xác suất lấy mẫu cụ thể)?
Không áp dụng.

•Ai tham gia vào quy trình thu thập dữ liệu (ví dụ: sinh viên, crowdworker, nhà thầu)
và họ được bồi thường như thế nào (ví dụ: crowdworker được trả bao nhiêu)?
Không có crowdworker hoặc nhà thầu nào tham gia vào quy trình thu thập dữ liệu; chỉ có
tác giả của công trình này tham gia.

•Dữ liệu được thu thập trong khoảng thời gian nào? Khung thời gian này có khớp với khung thời gian tạo
của dữ liệu liên quan đến các thể hiện không (ví dụ: thu thập gần đây các bài báo tin tức cũ)?
Nếu không, vui lòng mô tả khung thời gian mà dữ liệu liên quan đến các thể hiện được
tạo ra.
Dữ liệu được thu thập từ tháng 1 năm 2024 đến tháng 6 năm 2024. Chúng tôi bao gồm dữ liệu web từ 2013 đến
2024.

•Có quy trình xem xét đạo đức nào được tiến hành không (ví dụ: bởi hội đồng xem xét thể chế)? Nếu
có, vui lòng cung cấp mô tả về các quy trình xem xét này, bao gồm kết quả, cũng như
liên kết hoặc điểm truy cập khác đến bất kỳ tài liệu hỗ trợ nào.
Không có xem xét đạo đức nào được tiến hành.

Nếu bộ dữ liệu không liên quan đến con người, bạn có thể bỏ qua các câu hỏi còn lại trong phần này.

•Bạn có thu thập dữ liệu trực tiếp từ các cá nhân được đề cập, hay thu thập qua
bên thứ ba hoặc nguồn khác (ví dụ: trang web)?
Dữ liệu được thu thập từ một bên thứ ba thu thập web, CommonCrawl, cho tài liệu HTML và PDF
và trực tiếp từ bucket S3 ArXiv cho tài liệu ArXiv.

--- TRANG 22 ---
•Các cá nhân được đề cập có được thông báo về việc thu thập dữ liệu không? Nếu có, vui lòng mô tả
(hoặc hiển thị bằng ảnh chụp màn hình hoặc thông tin khác) cách thông báo được cung cấp, và cung cấp liên kết
hoặc điểm truy cập khác đến, hoặc tái tạo, ngôn ngữ chính xác của thông báo.
Không, các cá nhân không được thông báo rõ ràng.

•Các cá nhân được đề cập có đồng ý với việc thu thập và sử dụng dữ liệu của họ không? Nếu có,
vui lòng mô tả (hoặc hiển thị bằng ảnh chụp màn hình hoặc thông tin khác) cách sự đồng ý được yêu cầu
và cung cấp, và cung cấp liên kết hoặc điểm truy cập khác đến, hoặc tái tạo, ngôn ngữ chính xác
mà các cá nhân đã đồng ý.
Chúng tôi xây dựng bộ dữ liệu dựa trên CommonCrawl tôn trọng tệp robots.txt và do đó
các cá nhân không muốn dữ liệu của họ bị thu thập sẽ không được bao gồm trong bộ dữ liệu. Chúng tôi
tiến hành các bước bổ sung để che giấu thông tin nhận dạng cá nhân và loại bỏ hình ảnh không an toàn
cho công việc.

•Nếu sự đồng ý được thu thập, các cá nhân đồng ý có được cung cấp cơ chế
để thu hồi sự đồng ý trong tương lai hoặc cho một số cách sử dụng nhất định không? Nếu có, vui lòng cung cấp mô tả,
cũng như liên kết hoặc điểm truy cập khác đến cơ chế (nếu thích hợp).
Có, khi phát hành bộ dữ liệu, chúng tôi sẽ cung cấp biểu mẫu Google để loại bỏ mẫu khỏi
MINT-1T.

•Có phân tích tác động tiềm tàng của bộ dữ liệu và việc sử dụng nó đối với đối tượng dữ liệu
(ví dụ: phân tích tác động bảo vệ dữ liệu) được tiến hành không? Nếu có, vui lòng cung cấp mô tả về
phân tích này, bao gồm kết quả, cũng như liên kết hoặc điểm truy cập khác đến bất kỳ tài liệu hỗ trợ nào.
Không.

•Có nhận xét nào khác không?

A.3.4 Tiền xử lý/làm sạch/gán nhãn

•Có tiền xử lý/làm sạch/gán nhãn nào của dữ liệu được thực hiện không (ví dụ: rời rạc hóa hoặc
phân nhóm, tokenization, gán thẻ part-of-speech, trích xuất tính năng SIFT, loại bỏ thể hiện,
xử lý giá trị thiếu)? Nếu có, vui lòng cung cấp mô tả. Nếu không, bạn có thể bỏ qua
các câu hỏi còn lại trong phần này.
Có, bộ dữ liệu được tiền xử lý để loại bỏ văn bản chất lượng thấp, tài liệu/phần văn bản trùng lặp,
và loại bỏ hình ảnh không an toàn cho công việc và chất lượng thấp.

•Dữ liệu "thô" có được lưu ngoài dữ liệu đã tiền xử lý/làm sạch/gán nhãn không (ví dụ:
để hỗ trợ việc sử dụng tương lai không lường trước)? Nếu có, vui lòng cung cấp liên kết hoặc điểm truy cập khác đến
dữ liệu "thô".
Không, chúng tôi không cung cấp quyền truy cập vào dữ liệu thô vì nó có thể chứa nội dung xúc phạm và không
hữu ích để đào tạo các mô hình đa phương thức có khả năng.

•Phần mềm được sử dụng để tiền xử lý/làm sạch/gán nhãn dữ liệu có sẵn không? Nếu có, vui lòng
cung cấp liên kết hoặc điểm truy cập khác.
Phát hiện hình ảnh NSFW - https://github.com/GantMan/nsfw_model
Nhận dạng ngôn ngữ - https://fasttext.cc/
Bộ lọc chất lượng văn bản - https://github.com/huggingface/datatrove
Loại bỏ trùng lặp - https://github.com/allenai/bff
Phân tích PDF - https://github.com/pymupdf/PyMuPDF
Phân tích HTML - https://github.com/huggingface/OBELICS

•Có nhận xét nào khác không?

A.3.5 Sử dụng

•Bộ dữ liệu đã được sử dụng cho bất kỳ nhiệm vụ nào chưa? Nếu có, vui lòng cung cấp mô tả.
Có, chúng tôi đã sử dụng MINT-1T để đào tạo các mô hình đa phương thức lớn để xác thực chất lượng dữ liệu
so với đối thủ cạnh tranh.

•Có kho lưu trữ nào liên kết đến bất kỳ hoặc tất cả bài báo hoặc hệ thống sử dụng bộ dữ liệu không? Nếu
có, vui lòng cung cấp liên kết hoặc điểm truy cập khác.
Chúng tôi dự định phát hành kho lưu trữ như vậy khi chúng tôi công khai bộ dữ liệu.

--- TRANG 23 ---
•Bộ dữ liệu có thể được sử dụng cho những nhiệm vụ nào (khác)?
Bộ dữ liệu có thể được sử dụng cho các mục tiêu đào tạo khác như tạo ra các chuỗi hình ảnh và
văn bản xen kẽ hoặc để xây dựng hệ thống truy xuất tài liệu đa phương thức.

•Có điều gì về thành phần của bộ dữ liệu hoặc cách nó được thu thập
và tiền xử lý/làm sạch/gán nhãn có thể ảnh hưởng đến việc sử dụng trong tương lai không? Ví dụ: có
điều gì mà người tiêu dùng bộ dữ liệu cần biết để tránh việc sử dụng có thể dẫn đến đối xử không công bằng
với cá nhân hoặc nhóm (ví dụ: khuôn mẫu, vấn đề chất lượng dịch vụ) hoặc các rủi ro
hoặc tác hại khác (ví dụ: rủi ro pháp lý, tác hại tài chính)? Nếu có, vui lòng cung cấp mô tả. Có
điều gì người tiêu dùng bộ dữ liệu có thể làm để giảm thiểu những rủi ro hoặc tác hại này không?
Chúng tôi theo các nghiên cứu trước đây trong việc không sử dụng mô hình phân loại chất lượng văn bản vì chúng đã được chỉ ra
là thiên vị văn bản từ một số nhân khẩu học nhất định. Trong khi chúng tôi thực hiện các bước để giảm thiểu thiên vị và rủi ro trong
bộ dữ liệu (xem A.3.2), chúng tôi khuyến khích người tiêu dùng bộ dữ liệu lọc thêm dữ liệu dựa trên
trường hợp sử dụng của họ.

•Có nhiệm vụ nào mà bộ dữ liệu không nên được sử dụng không? Nếu có, vui lòng cung cấp
mô tả.
MINT-1T được xây dựng để làm cho nghiên cứu về các mô hình đa phương thức lớn dễ tiếp cận hơn. Sử dụng
bộ dữ liệu để đào tạo các mô hình tiêu thụ hoặc tạo ra thông tin nhận dạng cá nhân (như
hình ảnh khuôn mặt của mọi người và nội dung nhạy cảm khác) cũng như các ứng dụng quân sự đều là
trường hợp sử dụng không phù hợp của MINT-1T.

•Có nhận xét nào khác không?

A.3.6 Phân phối

•Bộ dữ liệu sẽ được phân phối cho các bên thứ ba bên ngoài thực thể (ví dụ: công ty,
tổ chức, tổ chức) thay mặt cho bộ dữ liệu được tạo ra không? Nếu có, vui lòng
cung cấp mô tả.
Có, bộ dữ liệu sẽ được phát hành công khai thông qua giao diện Huggingface.

•Bộ dữ liệu sẽ được phân phối như thế nào (ví dụ: tarball trên trang web, API, GitHub)? Bộ dữ liệu
có nhận dạng đối tượng số (DOI) không?
Bộ dữ liệu sẽ được phân phối dưới dạng shard JSON trong đó mỗi mục chứa văn bản của tài liệu
và liên kết đến hình ảnh. Chúng tôi sẽ cung cấp DOI khi phát hành.

•Khi nào bộ dữ liệu sẽ được phân phối?
Chúng tôi dự định phát hành bộ dữ liệu vào tháng 7 năm 2024.

•Bộ dữ liệu sẽ được phân phối dưới bản quyền hoặc giấy phép sở hữu trí tuệ (IP) khác,
và/hoặc dưới các điều khoản sử dụng (ToU) áp dụng không? Nếu có, vui lòng mô tả giấy phép
và/hoặc ToU này, và cung cấp liên kết hoặc điểm truy cập khác đến, hoặc tái tạo, bất kỳ
điều khoản cấp phép hoặc ToU liên quan cũng như bất kỳ phí nào liên quan đến những hạn chế này.
Chúng tôi phát hành MINT-1T dưới giấy phép CC-BY-4.0.

•Có bên thứ ba nào áp đặt hạn chế dựa trên IP hoặc hạn chế khác đối với dữ liệu liên quan
đến các thể hiện không? Nếu có, vui lòng mô tả những hạn chế này, và cung cấp liên kết hoặc điểm truy cập khác
đến, hoặc tái tạo, bất kỳ điều khoản cấp phép liên quan, cũng như bất kỳ phí nào
liên quan đến những hạn chế này.
Không.

•Có kiểm soát xuất khẩu hoặc hạn chế quy định nào khác áp dụng cho bộ dữ liệu hoặc
các thể hiện riêng lẻ không? Nếu có, vui lòng mô tả những hạn chế này, và cung cấp liên kết hoặc điểm truy cập khác
đến, hoặc tái tạo, bất kỳ tài liệu hỗ trợ nào.
Không.

•Có nhận xét nào khác không?

A.3.7 Bảo trì

•Ai sẽ hỗ trợ/lưu trữ/bảo trì bộ dữ liệu?
Các tác giả của công trình này sẽ tích cực phản hồi các vấn đề trong MINT-1T và bảo trì bộ dữ liệu.

•Làm thế nào để liên hệ với chủ sở hữu/người tuyển chọn/người quản lý bộ dữ liệu (ví dụ: địa chỉ email)?
Chủ sở hữu có thể được liên hệ qua email hoặc tab Issues trong giao diện Huggingface.

--- TRANG 24 ---
•Có erratum không? Nếu có, vui lòng cung cấp liên kết hoặc điểm truy cập khác.
Có, erratum sẽ có mặt trên README của trang bộ dữ liệu Huggingface.

•Bộ dữ liệu sẽ được cập nhật không (ví dụ: để sửa lỗi gán nhãn, thêm thể hiện mới, xóa
thể hiện)? Nếu có, vui lòng mô tả tần suất, bởi ai, và cách cập nhật sẽ được thông báo
đến người tiêu dùng bộ dữ liệu (ví dụ: danh sách gửi thư, GitHub)?
Có, bộ dữ liệu sẽ được cập nhật bởi tác giả của công trình này. Chúng tôi sẽ thông báo cập nhật
thông qua README trong giao diện Huggingface.

•Nếu bộ dữ liệu liên quan đến con người, có giới hạn áp dụng nào về việc lưu giữ dữ liệu
liên quan đến các thể hiện không (ví dụ: các cá nhân được đề cập có được nói rằng dữ liệu của họ
sẽ được lưu giữ trong một khoảng thời gian cố định và sau đó bị xóa)? Nếu có, vui lòng mô tả
những giới hạn này và giải thích cách chúng sẽ được thực thi.
Trong khi các cá nhân không được nói về việc lưu giữ dữ liệu, chúng tôi sẽ cung cấp biểu mẫu để từ chối
dữ liệu khỏi MINT-1T.

•Các phiên bản cũ của bộ dữ liệu sẽ tiếp tục được hỗ trợ/lưu trữ/bảo trì không? Nếu có,
vui lòng mô tả cách thức. Nếu không, vui lòng mô tả cách sự lỗi thời của nó sẽ được thông báo
đến người tiêu dùng bộ dữ liệu.
Các phiên bản cũ của bộ dữ liệu có thể được tìm thấy bằng cách sử dụng lịch sử Git của kho lưu trữ bộ dữ liệu.
Chúng tôi sẽ thông báo về các vấn đề với các phiên bản trước trong erratum.

•Nếu những người khác muốn mở rộng/tăng cường/xây dựng trên/đóng góp vào bộ dữ liệu, có
cơ chế nào để họ làm điều đó không? Nếu có, vui lòng cung cấp mô tả. Những đóng góp này
có được xác thực/xác minh không? Nếu có, vui lòng mô tả cách thức. Nếu không, tại sao không? Có
quy trình nào để thông báo/phân phối những đóng góp này đến người tiêu dùng bộ dữ liệu không? Nếu có, vui lòng cung cấp
mô tả.
Chúng tôi sẽ chấp nhận đóng góp vào bộ dữ liệu thông qua cơ chế Pull Request của
giao diện Huggingface.

•Có nhận xét nào khác không?

A.4 Giấy phép và tuyên bố tác giả

Chúng tôi phát hành MINT-1T dưới giấy phép CC-BY-4.0, xác định nó chủ yếu là hiện vật nghiên cứu. Trong khi
bộ dữ liệu có sẵn miễn phí, người dùng có trách nhiệm đảm bảo việc sử dụng hợp pháp trong môi trường thương mại.
Người dùng phải xác minh độc lập việc tuân thủ luật pháp áp dụng trước khi sử dụng MINT-1T cho
mục đích thương mại.
