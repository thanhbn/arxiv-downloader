# FLatten Transformer: Vision Transformer sử dụng Focused Linear Attention

Dongchen Han*Xuran Pan∗Yizeng Han Shiji Song Gao Huang†
Khoa Tự động hóa, BNRist, Đại học Thanh Hoa

## Tóm tắt

Độ phức tạp tính toán bậc hai của self-attention đã là một thách thức dai dẳng khi áp dụng các mô hình Transformer vào các tác vụ thị giác. Mặt khác, linear attention cung cấp một giải pháp thay thế hiệu quả hơn nhiều với độ phức tạp tuyến tính bằng cách xấp xỉ phép toán Softmax thông qua các hàm ánh xạ được thiết kế cẩn thận. Tuy nhiên, các phương pháp linear attention hiện tại hoặc gặp phải suy giảm hiệu suất đáng kể hoặc gây ra chi phí tính toán bổ sung từ các hàm ánh xạ. Trong bài báo này, chúng tôi đề xuất một module Focused Linear Attention mới để đạt được cả hiệu quả cao và khả năng biểu đạt. Cụ thể, chúng tôi đầu tiên phân tích các yếu tố góp phần vào suy giảm hiệu suất của linear attention từ hai góc độ: khả năng tập trung và tính đa dạng của đặc trưng. Để vượt qua những hạn chế này, chúng tôi giới thiệu một hàm ánh xạ đơn giản nhưng hiệu quả và một module khôi phục hạng hiệu quả để tăng cường khả năng biểu đạt của self-attention trong khi duy trì độ phức tạp tính toán thấp. Các thử nghiệm mở rộng cho thấy module linear attention của chúng tôi có thể áp dụng cho nhiều Vision Transformer tiên tiến và đạt được hiệu suất cải thiện nhất quán trên nhiều benchmark. Mã nguồn có sẵn tại https://github.com/LeapLabTHU/FLatten-Transformer.

## 1. Giới thiệu

Những năm gần đây đã chứng kiến sự phát triển mạnh mẽ của Transformer và self-attention trong lĩnh vực thị giác máy tính. Với sự ra đời của Vision Transformer [11, 39], các kỹ thuật self-attention đã thể hiện tiềm năng lớn trong nhiều tác vụ thị giác bao gồm phân loại hình ảnh [41, 43, 30, 46], phân đoạn ngữ nghĩa [6, 49], phát hiện đối tượng [4, 61, 22], và các tác vụ đa phương thức [35, 31].

Tuy nhiên, áp dụng Transformer vào các mô hình thị giác không phải là một nhiệm vụ tầm thường. Khác với các mạng nơ-ron tích chập nhẹ [37, 16, 44, 33], độ phức tạp tính toán bậc hai O(n²) theo độ dài chuỗi n dẫn đến chi phí tính toán cao khi sử dụng self-attention với trường tiếp nhận toàn cục. Các nghiên cứu trước đây đã tìm cách giảm thiểu thách thức này bằng cách giới hạn trường tiếp nhận toàn cục vào một vùng nhỏ hơn, chẳng hạn như thiết kế các mẫu attention toàn cục thưa thớt [41, 46] hoặc áp dụng các cửa sổ attention nhỏ hơn [24, 17]. Mặc dù hiệu quả, những phương pháp này hoặc dễ bỏ qua các đặc trưng thông tin trong các vùng khác do mẫu attention của chúng hoặc không tránh khỏi việc hy sinh khả năng mô hình hóa các phụ thuộc tầm xa.

Mặt khác, linear attention được coi là một giải pháp thay thế đơn giản nhưng hiệu quả để giải quyết bài toán tính toán bằng cách giảm độ phức tạp tổng quát. Nghiên cứu sớm tận dụng sơ đồ băm nhạy cảm cục bộ [21] nén độ phức tạp tính toán từ O(n²) xuống O(nlog(n)). Tuy nhiên, nó giới thiệu một hằng số lớn trước thuật ngữ độ phức tạp, khiến nó vẫn không thể chi trả trong các trường hợp thông thường. Các nghiên cứu gần đây hơn đã nhận thấy rằng việc sử dụng hàm Softmax trong phép toán self-attention thực tế buộc phải tính toán theo cặp giữa tất cả queries và keys, dẫn đến độ phức tạp O(n²) chủ đạo. Để giải quyết điều này, một số phương pháp áp dụng các hàm kích hoạt đơn giản [19, 38] hoặc các hàm ánh xạ được điều chỉnh [7, 26] để xấp xỉ hàm Softmax gốc.

Như minh họa trong Hình 1, bằng cách thay đổi thứ tự tính toán từ (query·key)·value thành query·(key·value), độ phức tạp tính toán tổng thể có thể được giảm xuống O(n). Tuy nhiên, so với Softmax attention, các phương pháp linear attention hiện tại vẫn gặp phải sự sụt giảm hiệu suất nghiêm trọng và có thể liên quan đến chi phí tính toán bổ sung từ hàm ánh xạ, do đó hạn chế ứng dụng thực tế của chúng.

Trong bài báo này, chúng tôi nhắm đến những hạn chế của các phương pháp linear attention hiện tại và đề xuất một module Focused Linear Attention mới, đạt được cả hiệu quả cao và khả năng biểu đạt. Cụ thể, chúng tôi thực hiện phân tích hai hướng về các yếu tố góp phần vào sự suy giảm hiệu suất trong linear attention và sau đó đề xuất các giải pháp tương ứng. Đầu tiên, phân phối trọng số attention trong các module linear attention trước đây tương đối mượt mà, thiếu khả năng tập trung để xử lý các đặc trưng thông tin nhất. Như một biện pháp khắc phục, chúng tôi đề xuất một hàm ánh xạ đơn giản để điều chỉnh hướng đặc trưng của queries và keys, làm cho trọng số attention dễ phân biệt hơn. Thứ hai, chúng tôi nhận thấy rằng hạng giảm của ma trận attention hạn chế tính đa dạng của các đặc trưng trong linear attention. Để giải quyết điều này, chúng tôi đề xuất một module khôi phục hạng bằng cách áp dụng một tích chập theo chiều sâu (DWC) bổ sung vào ma trận attention gốc, giúp khôi phục hạng ma trận và giữ cho đặc trưng đầu ra của các vị trí khác nhau được đa dạng hóa. Tận dụng những kỹ thuật cải tiến này, module của chúng tôi thể hiện hiệu suất tương đương hoặc vượt trội so với các đối tác Softmax, trong khi vẫn hưởng lợi từ độ phức tạp tính toán thấp.

Chúng tôi xác thực thực nghiệm hiệu quả của module trên các tác vụ phân loại hình ảnh, phân đoạn ngữ nghĩa và phát hiện đối tượng bằng cách sử dụng năm mô hình Vision Transformer tiên tiến. Kết quả cho thấy sự cải thiện nhất quán so với tất cả baseline và các phương pháp linear attention khác.

## 2. Các nghiên cứu liên quan

### 2.1. Vision Transformer

Transformer và cơ chế self-attention lần đầu được giới thiệu trong lĩnh vực xử lý ngôn ngữ tự nhiên và đã thu hút sự quan tâm nghiên cứu rộng rãi trong thị giác máy tính. Tuy nhiên, độ phức tạp tính toán cao của self-attention đặt ra những ràng buộc đối với việc áp dụng trực tiếp vào các tác vụ thị giác. Các nghiên cứu trước đây đã cố gắng giải quyết mối quan tâm này từ một số góc độ. Vision Transformer tiên phong [11] xem xét việc giảm độ phân giải đầu vào bằng cách hợp nhất các pixel lân cận thành một token duy nhất. Những hiểu biết tương tự đã được áp dụng trong các nghiên cứu tiếp theo [55, 54] và cũng mở rộng đến các tác vụ downstream [22]. Một hướng nghiên cứu khác giảm độ phân giải đặc trưng dần dần và áp dụng các mẫu attention được thiết kế cẩn thận để hạn chế số lượng token attention. Ví dụ, PVT [41, 42] sử dụng mẫu attention thưa thớt và chọn token attention từ góc độ toàn cục. DAT [46] theo con đường này và thiết kế module attention biến dạng để đạt được mẫu attention phụ thuộc dữ liệu. Swin Transformer [24] chọn token attention cục bộ bằng cách chia đầu vào thành các cửa sổ biệt lập. NAT [17] theo mẫu tập trung vào query trong tích chập và thiết kế token attention độc lập cho tất cả queries. Một số nghiên cứu cũng nhận thấy rằng các phép toán tích chập có giá trị đối với các mô hình Transformer và có thể giúp cải thiện hiệu quả tổng thể [48]. CMT [12] kết hợp các khối Transformer với các toán tử tích chập hiệu quả như tích chập theo chiều sâu [37], và đạt được sự cân bằng hiệu quả-hiệu suất tốt hơn. ACmix [30] chia sẻ chi phí tính toán của tích chập và self-attention, và tích hợp cả hai module với chi phí hạn chế. Các phương pháp cũng đã được đề xuất cho việc huấn luyện hiệu quả của Transformers [45, 29]. Trong các kịch bản ứng dụng đòi hỏi hiệu quả cao, MobileFormer [5] duy trì hai đường dẫn cho tích chập và Transformer tương ứng và hưởng lợi từ cả hai module. Dyn-Perceiver [13] đạt được nhận dạng thị giác hiệu quả thông qua thoát sớm động [15, 14, 51]. MobileViT [28] tận dụng thành công của MobileNets [37] và sử dụng sự kết hợp của các khối mobilenet và khối Transformer để đạt được trọng lượng nhẹ và độ trễ thấp.

Tuy nhiên, những phương pháp này vẫn dựa vào toán tử Softmax, có độ phức tạp tính toán cao vốn có và không tránh khỏi dẫn đến bất tiện trong thiết kế kiến trúc mô hình và ứng dụng thực tế.

### 2.2. Linear Attention

Ngoài các phương pháp trên, một hướng nghiên cứu khác giải quyết độ phức tạp tính toán cao với linear attention [19]. Cụ thể, linear attention thay thế hàm Softmax trong self-attention bằng các hàm kernel riêng biệt. Trong trường hợp này, linear attention không phải tính toán độ tương tự theo cặp QK^T trước. Như minh họa trong Hình 1, dựa trên tính chất kết hợp của phép nhân ma trận, linear attention có thể thay đổi thứ tự tính toán bằng cách tính K^TV trước, do đó giảm độ phức tạp tính toán từ O(N²d) xuống O(Nd²). Mặc dù hiệu quả, cách thiết kế module linear attention hiệu quả như softmax attention là một vấn đề không tầm thường. Performer [7] xấp xỉ phép toán Softmax với các đặc trưng ngẫu nhiên trực giao. Efficient attention [38] áp dụng hàm Softmax cho Q và K tương ứng, điều này tự nhiên đảm bảo mỗi hàng của QK^T có tổng bằng 1. Nyströmformer [50] và SOFT [26] xấp xỉ ma trận self-attention đầy đủ thông qua phân tích ma trận. Hydra attention [1] thay thế Softmax bằng độ tương tự cosine và đề xuất thủ thuật hydra giảm độ phức tạp tính toán xuống O(Nd). EfficientVit [2] sử dụng tích chập theo chiều sâu để cải thiện khả năng trích xuất đặc trưng cục bộ của linear attention. Castling-ViT [52] đề xuất linear angular kernel để đo độ tương tự phổ giữa mỗi Qi và Kj.

Tuy nhiên, các thiết kế linear attention hiện tại hoặc không có đủ khả năng biểu đạt để bắt kịp với Softmax attention hoặc liên quan đến chi phí tính toán bổ sung từ hàm kernel phức tạp. Trong nghiên cứu này, chúng tôi phân tích lý do suy giảm hiệu suất của linear attention từ góc độ khả năng tập trung và tính đa dạng đặc trưng. Dựa trên những phân tích này, chúng tôi đề xuất một module linear attention mới gọi là focused linear attention đạt được hiệu suất tốt hơn Softmax attention với độ phức tạp tính toán thấp hơn (Hình 2).

## 3. Kiến thức cơ bản

### 3.1. Vision Transformer và Self-Attention

Chúng tôi đầu tiên xem xét lại dạng tổng quát của self-attention trong Vision Transformers. Cho N token đầu vào x∈R^(N×C), trong mỗi head, self-attention có thể được viết như:

Q=xW_Q, K=xW_K, V=xW_V,
O_i=∑_{j=1}^N Sim(Q_i, K_j)/∑_{j=1}^N Sim(Q_i, K_j) V_j,

trong đó W_Q, W_K, W_V∈R^(C×C) là các ma trận chiếu và Sim(·,·) biểu thị hàm độ tương tự. Các Vision Transformer hiện đại chủ yếu áp dụng Softmax attention [40] trong đó độ tương tự được đo như Sim(Q, K)=exp(QK^T/√d). Trong trường hợp này, bản đồ attention được thu nhận bằng cách tính toán độ tương tự giữa tất cả các cặp query-key, dẫn đến độ phức tạp tính toán O(N²).

Do độ phức tạp tính toán bậc hai, việc đơn giản sử dụng self-attention với trường tiếp nhận toàn cục trở nên không khả thi, thường dẫn đến chi phí tính toán quá mức. Các nghiên cứu trước đây hoặc giải quyết mối quan tâm này bằng cách thiết kế mẫu attention toàn cục thưa thớt [41, 46] hoặc áp dụng các cửa sổ attention nhỏ hơn [24, 10]. Mặc dù hiệu quả, những phương pháp này trở nên dễ bị ảnh hưởng bởi các mẫu attention được thiết kế cẩn thận, hoặc không tránh khỏi hy sinh khả năng mô hình hóa các phụ thuộc tầm xa.

### 3.2. Linear Attention

Tương đối, linear attention [19] được coi là một giải pháp thay thế hiệu quả hạn chế độ phức tạp tính toán từ O(N²) xuống O(N). Cụ thể, các kernel được thiết kế cẩn thận được giới thiệu như xấp xỉ của hàm độ tương tự gốc, tức là,

Sim(Q, K) = φ(Q)φ(K)^T,

trong đó module self-attention có thể được viết lại như:

O_i=∑_{j=1}^N φ(Q_i)φ(K_j)^T/∑_{j=1}^N φ(Q_i)φ(K_j)^T V_j.

Theo cách này, chúng ta có thể thay đổi thứ tự tính toán từ (QK^T)V thành Q(K^TV) dựa trên tính chất kết hợp của phép nhân ma trận (như minh họa trong Hình 1):

O_i=φ(Q_i)∑_{j=1}^N φ(K_j)^T V_j / φ(Q_i)∑_{j=1}^N φ(K_j)^T,

trong đó độ phức tạp tính toán theo số token được giảm xuống O(N).

Tuy nhiên, các phương pháp linear attention hiện tại cũng đối mặt với tình tiến thoái giữa độ phức tạp mô hình và khả năng biểu đạt. Một mặt, các xấp xỉ đơn giản, ví dụ như sử dụng kích hoạt ReLU [2], quá lỏng lẻo và dẫn đến sự sụt giảm hiệu suất đáng kể. Mặt khác, các hàm kernel được thiết kế cẩn thận [7] hoặc các phương pháp phân tích ma trận [26, 50] có thể phát sinh chi phí tính toán bổ sung. Nói chung, vẫn còn khoảng cách giữa hiệu suất thực tế của linear attention và Softmax attention.

## 4. Focused Linear Attention

Mặc dù có độ phức tạp tính toán tuyến tính, nhiều nghiên cứu trước đây khác nhau cũng đã chứng minh rằng việc đơn giản thay thế Softmax attention bằng linear attention thường dẫn đến sự sụt giảm hiệu suất nghiêm trọng [34, 2, 7, 27]. Trong phần này, chúng tôi đầu tiên thực hiện phân tích chi tiết về hiệu suất kém của linear attention từ hai góc độ: khả năng tập trung và tính đa dạng đặc trưng. Sau đó, chúng tôi giới thiệu Focused Linear Attention của chúng tôi, giải quyết đầy đủ những mối quan tâm này và đạt được hiệu quả cao và khả năng biểu đạt.

### 4.1. Khả năng tập trung

Softmax attention thực tế cung cấp cơ chế tái trọng số phi tuyến, giúp dễ dàng tập trung vào các đặc trưng quan trọng [34, 2, 58]. Như thể hiện trong Hình 3, phân phối của bản đồ attention từ Softmax attention đặc biệt sắc nét trên các vùng nhất định, ví dụ như các đối tượng nền trước. Tương đối, phân phối trong linear attention tương đối mượt mà, làm cho đầu ra của nó gần với trung bình của tất cả các đặc trưng và không thể tập trung vào các vùng thông tin hơn.

Như một biện pháp khắc phục, chúng tôi đề xuất một giải pháp đơn giản nhưng hiệu quả bằng cách điều chỉnh hướng của từng đặc trưng query và key, đẩy các cặp query-key tương tự lại gần nhau trong khi đẩy các cặp query-key không tương tự xa nhau. Cụ thể, chúng tôi trình bày một hàm ánh xạ đơn giản f_p gọi là Focused Function:

Sim(Q_i, K_j) = φ_p(Q_i)φ_p(K_j)^T,

trong đó φ_p(x)=f_p(ReLU(x)), f_p(x)=||x||/||x**p|| x**p,

và x**p biểu thị lũy thừa p theo từng phần tử của x. Chúng tôi theo các module linear attention trước đây để sử dụng hàm ReLU trước để đảm bảo tính không âm của đầu vào và tính hợp lệ của mẫu số trong Eq.(4). Một quan sát trực tiếp là norm của đặc trưng được bảo toàn sau khi ánh xạ, tức là ||x||=||f_p(x)||, cho thấy chỉ hướng đặc trưng được điều chỉnh. Trên cơ sở này, chúng tôi cho thấy rằng dưới các giả định nhẹ, hàm ánh xạ f_p được đề xuất thực tế ảnh hưởng đến phân phối của attention.

**Mệnh đề 1** (Điều chỉnh hướng đặc trưng với f_p) Cho x=(x_1,···,x_n), y=(y_1,···,y_n)∈R^n, x_i, y_j≥0. Giả sử x và y có giá trị lớn nhất duy nhất x_m và y_n tương ứng. Đối với một cặp đặc trưng {x,y} với m=n:
∃p>1, sao cho ⟨φ_p(x), φ_p(y)⟩>⟨x,y⟩.

Đối với một cặp đặc trưng {x,y} với m≠n:
∃p>1, sao cho ⟨φ_p(x), φ_p(y)⟩<⟨x,y⟩.

**Chứng minh.** Vui lòng tham khảo Phụ lục để có chứng minh đầy đủ.

Do đó, với p thích hợp, hàm focused f_p(·) của chúng tôi thực tế đạt được sự khác biệt rõ rệt hơn giữa các cặp query-key tương tự (Eq. (7)) và các cặp query-key không tương tự (Eq. (8)), khôi phục phân phối attention sắc nét như hàm Softmax gốc.

Để hiểu rõ hơn, chúng tôi đưa ra một ví dụ để hiển thị hiệu ứng của f_p trong Hình 4. Có thể thấy rằng f_p thực tế "kéo" mỗi vector về trục gần nhất của nó, và p xác định mức độ "kéo" này. Bằng cách này, f_p giúp chia các đặc trưng thành nhiều nhóm theo các trục gần nhất của chúng, cải thiện độ tương tự trong mỗi nhóm trong khi giảm độ tương tự giữa các nhóm. Các hình ảnh hóa phù hợp với phân tích của chúng tôi ở trên.

### 4.2. Tính đa dạng đặc trưng

Ngoài khả năng tập trung, tính đa dạng đặc trưng cũng là một trong những yếu tố hạn chế sức mạnh biểu đạt của linear attention. Một trong những lý do có thể có thể ghi công cho hạng của ma trận attention [36, 53], nơi có thể thấy sự khác biệt đáng kể. Lấy một trong các lớp Transformer từ DeiT-Tiny [39] với N=14×14 làm ví dụ, chúng ta có thể thấy từ Hình 5 (a) rằng ma trận attention có hạng đầy đủ (196 trên 196), cho thấy tính đa dạng khi tổng hợp các đặc trưng từ các giá trị.

Tuy nhiên, điều này khó có thể đạt được trong trường hợp linear attention. Trên thực tế, hạng của ma trận attention trong linear attention bị giới hạn bởi số token N và chiều kênh d cho mỗi head:

rank(φ(Q)φ(K)^T) ≤ min{rank(φ(Q)), rank(φ(K))} ≤ min{N,d},

trong đó d thường nhỏ hơn N trong các thiết kế Vision Transformer thông thường, ví dụ, d=64, N=196 trong DeiT [39] và d=32, N=49 trong Swin Transformer [24]. Trong trường hợp này, giới hạn trên của hạng ma trận attention bị hạn chế ở tỷ lệ thấp hơn, cho thấy nhiều hàng của bản đồ attention bị đồng nhất hóa nghiêm trọng. Vì đầu ra của self-attention là tổng có trọng số của cùng một tập hợp V, việc đồng nhất hóa trọng số attention không tránh khỏi dẫn đến sự giống nhau giữa các đặc trưng được tổng hợp.

Để minh họa tốt hơn, chúng tôi thay thế Softmax attention gốc trong DeiT-Tiny bằng linear attention, và hiển thị hạng của bản đồ attention trong Hình 5 (b). Có thể quan sát thấy rằng hạng giảm đáng kể (54 trên 196) và nhiều hàng của ma trận attention tương tự nhau.

Như một biện pháp khắc phục, chúng tôi trình bày một giải pháp đơn giản nhưng hiệu quả để giải quyết hạn chế này của linear attention. Cụ thể, một module tích chập theo chiều sâu (DWC) được thêm vào ma trận attention và đầu ra có thể được công thức hóa như:

O = φ(Q)φ(K)^T V + DWC(V).

Để hiểu rõ hơn hiệu ứng của module DWC này, chúng ta có thể coi nó như một loại attention, trong đó mỗi query sẽ chỉ tập trung vào một số đặc trưng lân cận trong không gian thay vì tất cả các đặc trưng V. Tính địa phương này đảm bảo rằng ngay cả khi các giá trị linear attention tương ứng với hai query giống nhau, chúng ta vẫn có thể nhận được các đầu ra khác nhau từ các đặc trưng cục bộ khác nhau, do đó duy trì tính đa dạng đặc trưng. Hiệu ứng của DWC cũng có thể được giải thích từ góc độ hạng ma trận. Dựa trên Eq.(10), chúng ta có:

O = [φ(Q)φ(K)^T + M_DWC] V = M_eq V,

trong đó chúng tôi ký hiệu M_DWC là ma trận thưa thớt tương ứng với hàm tích chập theo chiều sâu, và ký hiệu M_eq là bản đồ attention đầy đủ tương đương. Vì M_DWC có khả năng là ma trận hạng đầy đủ, chúng tôi thực tế tăng giới hạn trên của hạng của ma trận attention tương đương, điều này phát sinh ít chi phí tính toán trong khi cải thiện đáng kể hiệu suất của linear attention.

Để minh họa tốt hơn, chúng tôi thực hiện các sửa đổi tương tự trên DeiT-Tiny. Với module DWC bổ sung, hạng của bản đồ attention trong linear attention có thể được khôi phục thành hạng đầy đủ (196 trên 196 như thể hiện trong Hình 5 (c)), giữ tính đa dạng đặc trưng như Softmax attention gốc.

### 4.3. Module focused linear attention

Dựa trên phân tích nêu trên, chúng tôi đề xuất một module linear attention mới, được gọi là focused linear attention, giảm độ phức tạp tính toán trong khi duy trì sức mạnh biểu đạt. Cụ thể, chúng tôi đầu tiên thiết kế một hàm ánh xạ mới để bắt chước phân phối sắc nét của Softmax attention gốc. Trên cơ sở này, chúng tôi tập trung vào bài toán hạng thấp trong các module linear attention trước đây, và áp dụng một tích chập theo chiều sâu đơn giản để khôi phục tính đa dạng đặc trưng. Theo cách này, module mới của chúng tôi có thể hưởng lợi từ cả độ phức tạp tuyến tính và khả năng biểu đạt cao. Cụ thể, module của chúng tôi có thể được công thức hóa như:

O = Sim(Q,K)V = φ_p(Q)φ_p(K)^T V + DWC(V).

Nói chung, module của chúng tôi có những ưu điểm sau:

**(1) Độ phức tạp tính toán thấp như linear attention.**
Bằng cách thay đổi thứ tự tính toán của self-attention, độ phức tạp được chuyển từ O(N²d) thành O(Nd²), trong đó N và d biểu thị số token và chiều kênh của mỗi head tương ứng. d thường nhỏ hơn N trong các thiết kế Vision Transformer thông thường, ví dụ, d=64, N=196 trong DeiT [39] và d=32, N=49 trong Swin Transformer [24], tính toán tổng thể thực tế giảm. Ngoài ra, so với các module linear attention trước đây [7] thiết kế hàm kernel phức tạp, hàm focused f_p được đề xuất của chúng tôi chỉ áp dụng các toán tử đơn giản đạt được xấp xỉ với chi phí tính toán tối thiểu.

**(2) Khả năng biểu đạt cao như Softmax attention.**
Như chúng tôi đã phân tích ở trên, các thiết kế linear attention dựa trên kernel trước đây nói chung kém hơn đối tác Softmax từ góc độ khả năng tập trung và tính đa dạng đặc trưng. Với hàm focused f_p và tích chập theo chiều sâu được đề xuất, focused linear attention của chúng tôi có thể đạt được hiệu suất thậm chí tốt hơn Softmax attention.

Ngoài ra, module của chúng tôi cũng có tiềm năng thích ứng với trường tiếp nhận lớn hơn và các kiến trúc mô hình khác nhau. Các mô hình Transformer hiện đại dựa trên Softmax attention chủ yếu sử dụng số lượng hạn chế các cặp key/value vì độ phức tạp bậc hai theo số token. Tuy nhiên, độ phức tạp tuyến tính của module chúng tôi cho phép chúng tôi mở rộng trường tiếp nhận đến một vùng lớn hơn trong khi duy trì cùng lượng tính toán, và hưởng lợi từ việc mô hình hóa các phụ thuộc tầm xa. Ngoài ra, module của chúng tôi có thể phục vụ như một module plug-in và dễ dàng được áp dụng trên nhiều kiến trúc Vision Transformer hiện đại. Chúng tôi thực nghiệm triển khai module của chúng tôi trên năm mô hình tiên tiến bao gồm DeiT [39], PVT [41], PVT-v2 [42], Swin Transformer [24] và CSwin Transformer [10]. Xem xét ưu điểm của trường tiếp nhận mở rộng, chúng tôi áp dụng khối focused linear attention ở các giai đoạn đầu của Vision Transformers, và giữ phần còn lại của các khối không thay đổi. Kiến trúc mô hình chi tiết được hiển thị trong Phụ lục.

## 5. Thử nghiệm

Để xác minh hiệu quả của phương pháp, chúng tôi thực hiện thử nghiệm trên phân loại ImageNet-1K [9], phân đoạn ngữ nghĩa ADE20K [60], và phát hiện đối tượng COCO [23]. Chúng tôi cũng cung cấp so sánh chi tiết với các module linear attention khác dựa trên hai cấu trúc mô hình đại diện. Ngoài ra, chúng tôi thực hiện các nghiên cứu ablation toàn diện để phân tích từng yếu tố thiết kế quan trọng.

### 5.1. Phân loại ImageNet-1K

ImageNet-1K [9] chứa 1.28M hình ảnh để huấn luyện và 50K hình ảnh để xác thực. Chúng tôi thực tế triển khai module của chúng tôi trên năm mô hình Vision Transformer tiên tiến, và báo cáo độ chính xác Top-1 trên tập xác thực để so sánh với các mô hình state-of-the-art khác nhau.

Để so sánh công bằng, chúng tôi sử dụng cài đặt chính xác giống như mô hình baseline tương ứng để huấn luyện mô hình FLatten của chúng tôi. Cụ thể, chúng tôi sử dụng bộ tối ưu AdamW [25] để huấn luyện tất cả các mô hình của chúng tôi trong 300 epoch với phân rã tốc độ học cosine và 20 epoch khởi động tuyến tính. Tốc độ học cơ bản cho kích thước batch 1024 được đặt thành 1×10^(-3), và sau đó được mở rộng tuyến tính theo kích thước batch. Chúng tôi theo DeiT [39] và áp dụng RandAugment [8], Mixup [57], CutMix [56] và random erasing [59] để tránh overfitting. Ngoài ra, weight decay 0.05 được sử dụng. Để nhất quán với [10], chúng tôi cũng áp dụng EMA [32] trong việc huấn luyện các mô hình FLatten-CSwin của chúng tôi. Về việc tinh chỉnh độ phân giải lớn hơn, chúng tôi theo cài đặt trong [24, 10] tinh chỉnh các mô hình trong 30 epoch.

Kết quả phân loại được cung cấp trong Hình 6. Nó cho thấy rằng phương pháp của chúng tôi đạt được những cải thiện nhất quán so với các mô hình baseline dưới FLOPs hoặc tham số tương đương. Ví dụ, FLatten-PVT-T/S của chúng tôi vượt trội PVT-T/S lần lượt 2.7% và 1.9% với FLOPs tương tự. Dựa trên Swin, mô hình của chúng tôi đạt được hiệu suất tương đương với 60% FLOPs. Mô hình của chúng tôi dựa trên PVT-v2 và CSwin cũng đạt được sự cân bằng tốt hơn giữa chi phí tính toán và hiệu suất mô hình. Những kết quả này chứng minh rằng module của chúng tôi có khả năng biểu đạt cao và có thể áp dụng cho các cấu trúc mô hình khác nhau.

### 5.2. Phân đoạn ngữ nghĩa

ADE20K [60] là một benchmark được áp dụng rộng rãi cho phân đoạn ngữ nghĩa với 20K/2K hình ảnh huấn luyện/xác thực. Chúng tôi sử dụng mô hình của chúng tôi trên hai mô hình phân đoạn đại diện, SemanticFPN [20] và UperNet [47]. Như thể hiện trong Bảng 1, mô hình của chúng tôi đạt được kết quả tốt hơn nhất quán dưới tất cả các cài đặt. Cụ thể, chúng ta có thể thấy cải thiện mIoU 0.5∼1% với chi phí tính toán và tham số tương đương. Những cải thiện trong mAcc đáng kể hơn.

### 5.3. Phát hiện đối tượng

Bộ dữ liệu phát hiện đối tượng và phân đoạn instance COCO [23] có 118K hình ảnh huấn luyện và 5K hình ảnh xác thực. Chúng tôi sử dụng mô hình được huấn luyện trước ImageNet làm backbone trong các framework Mask R-CNN [18] và Cascade Mask R-CNN [3] để đánh giá hiệu quả. Chúng tôi thực hiện thử nghiệm trên lịch trình 1x và 3x với các head phát hiện khác nhau và hiển thị kết quả trong Bảng 2. Tận dụng ưu điểm của trường tiếp nhận lớn hơn, mô hình của chúng tôi cho thấy kết quả tốt hơn dưới tất cả các cài đặt.

### 5.4. So sánh với Linear Attention khác

Để hiển thị so sánh công bằng với các module linear attention khác, chúng tôi thực hiện thử nghiệm dựa trên hai cấu trúc Vision Transformer đại diện, DeiT và Swin Transformer tương ứng. Dựa trên hai mô hình này, chúng tôi so sánh module focused linear attention của chúng tôi với bốn thiết kế linear attention trước đây, bao gồm hydra attention [1], efficient attention [38], linear angular attention [52] và enhanced linear attention [2].

Như thể hiện trong Bảng 3, các module linear attention trước đây nói chung kém hơn đối tác Softmax, trong khi mô hình của chúng tôi vượt trội đáng kể so với tất cả các thiết kế khác và baseline Softmax. Điều này cho thấy rằng module của chúng tôi có khả năng biểu đạt cao và có thể đạt được hiệu suất tốt hơn Softmax attention với độ phức tạp tính toán thấp hơn.

### 5.5. Thời gian suy luận

Chúng tôi tiếp tục đánh giá hiệu quả thực tế của mô hình và so sánh với hai baseline cạnh tranh. Kết quả được trình bày trong Hình 7. Chúng tôi kiểm tra độ trễ suy luận trên nhiều nền tảng phần cứng, bao gồm CPU desktop (Intel i5-8265U) và hai GPU server (RTX2080Ti và RTX3090). Có thể quan sát thấy rằng mô hình của chúng tôi đạt được sự cân bằng tốt hơn đáng kể giữa thời gian chạy và độ chính xác trên cả CPU và GPU, hưởng lợi từ tốc độ suy luận nhanh hơn lên đến 2.1x với hiệu suất ngang bằng hoặc thậm chí tốt hơn.

### 5.6. Nghiên cứu Ablation

Trong phần này, chúng tôi ablate các thành phần chính trong focused linear attention của chúng tôi để xác minh hiệu quả của những thiết kế này. Chúng tôi báo cáo kết quả trên phân loại ImageNet-1K dựa trên FLatten-DeiT-T và FLatten-Swin-T.

**Hàm focused f_p và DWC.** Chúng tôi đầu tiên đánh giá hiệu quả của hàm focused f_p được đề xuất và tích chập theo chiều sâu. Chúng tôi bắt đầu từ vanilla linear attention và giới thiệu f_p và DWC lần lượt. Như thể hiện trong Bảng 4, áp dụng hàm focused f_p được đề xuất cung cấp cải thiện +1.3. Sử dụng DWC để duy trì tính đa dạng đặc trưng tiếp tục dẫn đến tăng độ chính xác +2.3, đạt được độ chính xác tổng thể 74.1. Những kết quả này chứng minh rằng f_p và DWC được đề xuất của chúng tôi có thể cải thiện đáng kể khả năng biểu đạt của linear attention, do đó giúp module focused linear attention của chúng tôi đạt được hiệu suất tốt hơn Softmax attention.

**Ablation về p khác nhau.** Trong Bảng 5, chúng tôi nghiên cứu hiệu ứng của hệ số focused p đối với hiệu suất mô hình. Chúng tôi thấy rằng khi p thay đổi từ 2 đến 32, độ chính xác phân loại Top-1 không thay đổi nhiều, ngụ ý tính robustness của module đối với siêu tham số này. Thực tế, chúng tôi chọn p=3 cho tất cả các mô hình trong bài báo mà không cần tinh chỉnh bổ sung.

**Trường tiếp nhận.** Chúng tôi cũng nghiên cứu tác động của trường tiếp nhận dựa trên FLatten-Swin-tiny. Như minh họa trong Bảng 6, với sự tăng lên của kích thước cửa sổ, hiệu suất của mô hình chúng tôi cải thiện dần dần. Điều này tiếp tục chứng minh rằng mặc dù window attention hiệu quả, nó không tránh khỏi hy sinh phụ thuộc tầm xa của self-attention từ góc độ toàn cục và vẫn kém hơn global attention. Với độ phức tạp tuyến tính, module của chúng tôi có thể thực hiện trường tiếp nhận lớn thậm chí toàn cục trong khi duy trì cùng lượng tính toán.

**Focused linear attention ở các giai đoạn khác nhau.** Chúng tôi thay thế shift-window attention của Swin-T bằng module của chúng tôi ở các giai đoạn khác nhau. Như thể hiện trong Bảng 7, chúng ta có thể thấy rằng thay thế hai giai đoạn đầu dẫn đến tăng hiệu suất 0.8, trong khi thay thế hai giai đoạn cuối giảm nhẹ độ chính xác tổng thể. Chúng tôi quy kết quả này cho thực tế là hai giai đoạn đầu của Swin có độ phân giải lớn hơn và phù hợp hơn cho module của chúng tôi với trường tiếp nhận lớn.

## 6. Kết luận

Trong bài báo này, chúng tôi đề xuất một module focused linear attention mới. Bằng cách giải quyết những hạn chế của các phương pháp linear attention trước đây từ góc độ khả năng tập trung và tính đa dạng đặc trưng, module của chúng tôi đạt được sự kết hợp ấn tượng giữa hiệu quả cao và khả năng biểu đạt. Các thử nghiệm mở rộng về phân loại hình ảnh, phát hiện đối tượng và phân đoạn ngữ nghĩa chứng minh rằng module của chúng tôi có thể được áp dụng rộng rãi cho nhiều Vision Transformer và đạt được sự cân bằng tốt hơn giữa hiệu quả tính toán và hiệu suất mô hình.
