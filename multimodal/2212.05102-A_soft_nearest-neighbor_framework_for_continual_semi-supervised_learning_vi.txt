# Một khung làm việc láng giềng mềm gần nhất cho học bán giám sát liên tục

Zhiqi Kang∗1Enrico Fini∗2Moin Nabi3Elisa Ricci2,4Karteek Alahari1
1Inria† 2University of Trento3SAP AI Research4Fondazione Bruno Kessler

## Tóm tắt

Mặc dù có những tiến bộ đáng kể, hiệu suất của các phương pháp học liên tục tiên tiến phụ thuộc vào kịch bản không thực tế của dữ liệu được gán nhãn đầy đủ. Trong bài báo này, chúng tôi giải quyết thách thức này và đề xuất một phương pháp cho học bán giám sát liên tục—một thiết lập mà không phải tất cả các mẫu dữ liệu đều được gán nhãn. Vấn đề chính trong kịch bản này là mô hình quên các biểu diễn của dữ liệu không được gán nhãn và overfitting các mẫu được gán nhãn. Chúng tôi tận dụng sức mạnh của bộ phân loại láng giềng gần nhất để phân chia không tuyến tính không gian đặc trưng và mô hình hóa linh hoạt phân phối dữ liệu cơ bản nhờ vào tính chất phi tham số của nó. Điều này cho phép mô hình học một biểu diễn mạnh cho nhiệm vụ hiện tại, và chưng cất thông tin liên quan từ các nhiệm vụ trước đó. Chúng tôi thực hiện đánh giá thực nghiệm kỹ lưỡng và cho thấy phương pháp của chúng tôi vượt trội so với tất cả các phương pháp hiện có với biên độ lớn, thiết lập một trạng thái nghệ thuật vững chắc trên mô hình học bán giám sát liên tục. Ví dụ, trên CIFAR-100 chúng tôi vượt qua một số phương pháp khác ngay cả khi sử dụng ít nhất 30 lần ít giám sát hơn (0.8% so với 25% chú thích). Cuối cùng, phương pháp của chúng tôi hoạt động tốt trên cả hình ảnh độ phân giải thấp và cao và mở rộng một cách liền mạch đến các tập dữ liệu phức tạp hơn như ImageNet-100. Mã nguồn của chúng tôi được công bố công khai tại https://github.com/kangzhiq/NNCSL.

## 1. Giới thiệu

Nhiều nỗ lực đã được dành cho mô hình học liên tục (CL) [18] trong đó các mẫu dữ liệu huấn luyện đến một cách tuần tự. Tuy nhiên, hầu hết các phương pháp CL tiên tiến [11, 12, 20] đều dựa trên một giả định mạnh: dữ liệu được gán nhãn đầy đủ. Đây là một yêu cầu không thực tế vì việc gán nhãn dữ liệu thường tốn kém do chuyên môn cần thiết hoặc số lượng chú thích, nguy hiểm do lo ngại về quyền riêng tư hoặc an toàn, hoặc không thực tế trong kịch bản trực tuyến thời gian thực. Một cách tự nhiên để giải quyết vấn đề này là tận dụng khung học bán giám sát, nơi không phải tất cả các mẫu dữ liệu đều được gán nhãn.

∗Zhiqi Kang và Enrico Fini đóng góp bằng nhau cho công trình này
†Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.

Trong những năm gần đây, điều này đã thúc đẩy cộng đồng nghiên cứu một dòng nghiên cứu mới có tên là học bán giám sát liên tục [8,44,52]. Nó đề cập đến thiết lập mà mỗi nhiệm vụ trong chuỗi đều bán giám sát. Kịch bản học này mang lại những thách thức mới, vì các mô hình quên thảm khốc các biểu diễn của dữ liệu không được gán nhãn trong khi cũng overfitting tập được gán nhãn. Điều này còn được làm trầm trọng thêm bởi một hiện tượng được nghiên cứu kỹ khác trong CL: overfitting bộ đệm phát lại kinh nghiệm [10]. Những thách thức này dẫn đến việc các phương pháp CL thông thường hoạt động kém, vì chúng thiếu khả năng trích xuất thông tin từ tập không được gán nhãn, do đó phần lớn overfitting tập được gán nhãn [53]. Mặt khác, các phương pháp học bán giám sát [15,29,46,58] cân bằng tốt các tập được gán nhãn và không được gán nhãn nhưng không thể xử lý kịch bản liên tục, và bị quên ngay cả khi được ghép nối với các phương pháp CL nổi tiếng (xem Hình. 1 (phải) và Bảng. 1).

Một vài phương pháp gần đây [8, 52] một phần giảm thiểu những vấn đề này trên các tập dữ liệu quy mô nhỏ. Phương pháp trong [52] dựa vào các mô hình sinh để phát lại các lớp trước đó, điều này dẫn đến chi phí tính toán và bộ nhớ đáng kể, khiến việc mở rộng đến các tập dữ liệu lớn hơn như ImageNet trở nên khó khăn. Boschini và cộng sự [8] đề xuất đối chiếu giữa các mẫu của các lớp và nhiệm vụ khác nhau. Tuy nhiên, [8] thiếu sót với các bộ đệm bộ nhớ nhỏ hơn, các tập dữ liệu lớn hơn, nhiều nhiệm vụ hơn, hoặc hình ảnh độ phân giải cao hơn (xem Hình. 1 (trái) và kết quả trong Mục. 6). Do đó, chúng tôi lập luận rằng có nhu cầu rõ ràng cho các phương pháp học bán giám sát liên tục mạnh mẽ hơn, với việc sử dụng phù hợp hơn tập được gán nhãn, và học biểu diễn hiệu quả và ổn định từ tập không được gán nhãn.

Trong bài báo này, chúng tôi giải phóng sức mạnh của láng giềng gần nhất trong bối cảnh học bán giám sát liên tục. Cụ thể, chúng tôi đề xuất một phương pháp mới, NNCSL (Láng giềng Gần nhất cho Học Bán giám sát Liên tục), tận dụng khả năng của bộ phân loại láng giềng gần nhất để phân chia không tuyến tính không gian đặc trưng theo hai cách: i) để học các biểu diễn mạnh mẽ và ổn định của nhiệm vụ hiện tại bằng cách sử dụng chiến lược tự giám sát đa góc nhìn, và ii) để chưng cất kiến thức trước đó và chuyển giao cấu trúc cục bộ của không gian đặc trưng. Cái sau được đạt được thông qua NND (Chưng cất Láng giềng Gần nhất) được đề xuất của chúng tôi, một mất mát chưng cất bán giám sát mới giảm thiểu việc quên trong học bán giám sát liên tục tốt hơn so với các phương pháp chưng cất cạnh tranh khác. Trái ngược với chưng cất kiến thức [11, 22, 30, 38] và chưng cất đặc trưng [19, 21, 24], tập trung độc quyền vào phân phối cấp lớp và cấp mẫu tương ứng, NND đồng thời chưng cất mối quan hệ giữa các lớp và mẫu bằng cách tận dụng bộ phân loại láng giềng gần nhất. Nhìn chung, NNCSL vượt trội so với tất cả các phương pháp liên quan với biên độ rất lớn trên cả tập dữ liệu quy mô nhỏ và lớn và cả hình ảnh độ phân giải thấp và cao. Ví dụ, như được hiển thị trong Hình. 1, NNCSL bằng hoặc vượt qua tất cả các phương pháp khác với hơn 30 lần ít giám sát hơn (0.8% so với 25% chú thích) trên CIFAR100.

Các đóng góp chính của công trình này như sau:
• Chúng tôi đề xuất NNCSL, một phương pháp học bán giám sát liên tục dựa trên láng giềng gần nhất mới mà theo thiết kế ít bị tác động bởi hiện tượng overfitting liên quan đến bộ đệm được gán nhãn nhỏ.
• Chúng tôi đề xuất NND, một chiến lược chưng cất mới chuyển giao cả kiến thức cấp biểu diễn và cấp lớp từ mô hình được huấn luyện trước đó bằng cách sử dụng đầu ra của bộ phân loại láng giềng gần nhất mềm, giúp giảm thiểu hiệu quả việc quên.
• Chúng tôi cho thấy NNCSL vượt trội so với các phương pháp hiện có trên một số tiêu chuẩn với biên độ lớn, thiết lập một trạng thái nghệ thuật mới trên học bán giám sát liên tục. Trái ngược với các phương pháp trước đó, phương pháp của chúng tôi hoạt động tốt trên cả hình ảnh độ phân giải thấp và cao và mở rộng một cách liền mạch đến các tập dữ liệu phức tạp hơn.

## 2. Công trình liên quan

**Học bán giám sát.** Các phương pháp bán giám sát tập trung vào việc học các mô hình từ các tập dữ liệu quy mô lớn mà chỉ một vài mẫu có chú thích liên kết [17]. Các chiến lược sớm cho mô hình học này áp dụng cho các kiến trúc sâu đã tận dụng nhãn giả và thực hiện tự huấn luyện dựa trên chúng [29]. Sơ đồ này sau đó được cải thiện với ngưỡng tin cậy [2] và ngưỡng tin cậy thích ứng [56, 59]. Các phương pháp tinh vi hơn để kết hợp độ tin cậy của các dự đoán và lọc ra các mẫu giả mạo cũng được phát triển, chẳng hạn như FixMatch [46], sử dụng kiến trúc học sinh-giáo viên. Các phương pháp khác đã chứng minh lợi ích của đồng huấn luyện [36] và chưng cất [55].

Một lớp phương pháp khác được phát triển với ý tưởng áp đặt các dự đoán tương tự từ mạng cho hai mẫu thu được với các nhiễu loạn đầu vào khác nhau [3,4,27,33, 35, 41, 47, 50, 60]. Ví dụ, [3] đã xem xét một mất mát nhất quán và nhãn giả mềm được tạo ra bằng cách so sánh các biểu diễn của các góc nhìn hình ảnh với những góc nhìn của một tập các hình ảnh được gán nhãn được lấy mẫu ngẫu nhiên. Gần đây, các kỹ thuật trộn mẫu, chẳng hạn như MixUp, cũng được nghiên cứu trong bối cảnh học bán giám sát để cải thiện hiệu suất mô hình trên các vùng mật độ mẫu thấp [5,6,31]. Tuy nhiên, không có công trình nào được đề cập ở trên giải quyết vấn đề học trong thiết lập tăng dần.

**Học liên tục.** Một số phương pháp CL đã được đề xuất trong vài năm qua để học từ dữ liệu theo cách tăng dần. Theo một khảo sát gần đây [18], các phương pháp CL hiện có có thể được phân loại thô thành ba nhóm. Nhóm đầu tiên bao gồm các phương pháp dựa trên chính quy hóa, giải quyết vấn đề quên thảm khốc bằng cách giới thiệu các thuật ngữ chính quy hóa thích hợp trong hàm mục tiêu [12, 19, 22, 24, 30] hoặc xác định một tập các tham số có liên quan nhất đến các nhiệm vụ nhất định [13, 25, 54]. Các phương pháp dựa trên phát lại tương ứng với nhóm thứ hai, và chúng lưu trữ một vài mẫu từ các nhiệm vụ trước đó [9, 14, 32, 38] hoặc tạo ra chúng [34, 43] để nhắc nhở kiến thức trong các giai đoạn huấn luyện cho các nhiệm vụ tiếp theo. Cuối cùng, nhóm thứ ba là các phương pháp cô lập tham số [40, 42], hoạt động bằng cách phân bổ các tham số cụ thể cho nhiệm vụ.

Trong khi phần lớn các phương pháp này hoạt động trong thiết lập giám sát, các công trình gần đây đã giải quyết vấn đề vượt qua việc quên thảm khốc trong trường hợp đầy thách thức của giám sát hạn chế [8, 28, 44, 52] hoặc không có giám sát [1, 21, 37, 45]. Tuy nhiên, hầu hết chúng có thiết lập mặc định khác nhau đáng kể, ví dụ, việc sử dụng các tập dữ liệu bên ngoài, và khả năng tiếp cận dữ liệu được gán nhãn/không được gán nhãn trong các giai đoạn học liên tục, chỉ để lại một vài [8, 52] có thể so sánh trong thiết lập thực tế mong muốn của chúng tôi. Wang và cộng sự [52] đã giải quyết vấn đề học bán giám sát liên tục và đề xuất ORDisCo, một phương pháp liên tục học một mạng đối nghịch sinh có điều kiện với một bộ phân loại từ dữ liệu được gán nhãn một phần. Tuy nhiên, phương pháp này mang lại chi phí cấm đoán trên hình ảnh độ phân giải cao hơn, ví dụ, trên ImageNet-100. Tính nhất quán nội suy liên tục đối chiếu (CCIC) [8] là một phương pháp khác, đã đề xuất đối chiếu các mẫu giữa các lớp và nhiệm vụ khác nhau. Tuy nhiên, học đối chiếu cấp nhiệm vụ cho thấy cải thiện không đáng kể trong nghiên cứu khử. Chúng tôi suy đoán nó có thể làm nhầm lẫn mô hình vì nó tập hợp các lớp không nhất thiết liên quan trong không gian đặc trưng. Sự nhầm lẫn này dẫn đến hiệu suất tệ hơn với bộ đệm bộ nhớ nhỏ, nhiều mẫu hơn, nhiều nhiệm vụ hơn, hoặc hình ảnh độ phân giải cao hơn như được hiển thị bởi các thí nghiệm của chúng tôi. Công trình của chúng tôi khác biệt hoàn toàn so với các phương pháp trước đó này, vì chúng tôi thiết kế NNCSL, một phương pháp mới cho học bán giám sát liên tục dựa trên bộ phân loại láng giềng gần nhất mềm. Đánh giá thực nghiệm của chúng tôi chứng minh rằng NNCSL vượt qua các công trình trước đó với biên độ lớn.

## 3. Học bán giám sát liên tục

Bây giờ chúng tôi định nghĩa chính thức vấn đề học bán giám sát liên tục. Cho dữ liệu huấn luyện đến tuần tự, tức là, như một chuỗi T nhiệm vụ. Tập dữ liệu liên kết với nhiệm vụ t được ký hiệu là Dt, với t ∈ {1, ..., T}. Do đó việc học được thực hiện theo nhiệm vụ, trong đó chỉ dữ liệu huấn luyện hiện tại Dt có sẵn trong nhiệm vụ t. Khi chuyển từ một nhiệm vụ sang nhiệm vụ tiếp theo, dữ liệu trước đó được loại bỏ một cách có hệ thống. Vì tập dữ liệu có sẵn không được gán nhãn đầy đủ, chúng tôi chia thêm nó thành hai tập con sao cho Dt = Ut ∪ Lt. Thông thường trong kịch bản học bán giám sát, chúng ta có |Lt| ≪ |Ut|, tỷ lệ |Lt|/|Ut| được giữ không đổi cho tất cả các nhiệm vụ. Ngoài ra, đó là thực hành phổ biến trong tài liệu CL [8, 38] để cho phép giữ lại một bộ đệm bộ nhớ M lưu trữ và phát lại các mẫu đã thấy trước đó, như được hiển thị trong Hình. 2.

Cho fθ là mô hình, được tham số hóa bởi θ, và bao gồm ba thành phần: một xương sống g, một máy chiếu h và một bộ phân loại p. Xương sống, ở đây được mô hình hóa như một mạng nơ-ron tích chập, được sử dụng để trích xuất các biểu diễn z = g(x) từ một hình ảnh đầu vào x. Bộ phân loại lấy biểu diễn này để dự đoán một tập các logit p = p(z), trong khi máy chiếu (được triển khai như một perceptron đa lớp) ánh xạ các đặc trưng xương sống đến một không gian thấp chiều h = h(z). Ngoài ra, chúng tôi sử dụng chỉ số trên để đề cập đến trạng thái tại một thời điểm nhất định, ví dụ cho nhiệm vụ t là ft θ, và cho nhiệm vụ trước đó t−1 là ft−1 θ. Tương tự, chúng tôi sử dụng xt u và xt l để đề cập đến các mẫu được rút ra từ Ut và Lt tương ứng. Ngoài hình ảnh, tập dữ liệu được gán nhãn cũng chứa các chú thích sự thật căn bản một-nóng y.

Trong các phần sau, chúng tôi giới thiệu phương pháp NNCSL được đề xuất cho học bán giám sát liên tục. Trước tiên chúng tôi trình bày PAWS [3], đã truyền cảm hứng cho NNCSL của chúng tôi, (Mục. 4) và cho thấy tại sao phương pháp này không thể áp dụng ngay lập tức cho thiết lập liên tục. Tiếp theo, chúng tôi trình bày CSL, người học bán giám sát liên tục cơ sở của chúng tôi (Mục. 5.1), giải quyết nhiều vấn đề của nó. Tuy nhiên, phương pháp cơ sở này thiếu cơ chế để chống lại việc quên. Do đó, chúng tôi giới thiệu NND, phương pháp chưng cất mới của chúng tôi dựa trên bộ phân loại láng giềng gần nhất mềm trong Mục. 5.2. Tất cả các yếu tố này được tích hợp hài hòa trong phương pháp đầy đủ của chúng tôi: NNCSL, có mục tiêu tổng thể được tóm tắt trong Mục. 5.3.

## 4. Láng giềng gần nhất gặp học liên tục: điểm mạnh và điểm yếu

Bây giờ chúng tôi thảo luận về việc sử dụng các kỹ thuật láng giềng gần nhất [3] trong bối cảnh học liên tục, và mô tả các điểm mạnh và điểm yếu của nó trong các kịch bản với sự thay đổi phân phối dữ liệu. Trong quá trình huấn luyện, các mini-batch mà mô hình nhận được bao gồm dữ liệu được gán nhãn và không được gán nhãn, với K và N là kích thước batch cho hai tập này tương ứng. Các hình ảnh không được gán nhãn trong batch được tăng cường hai lần bằng cách sử dụng các kỹ thuật tăng cường dữ liệu phổ biến để thu được hai góc nhìn tương quan của cùng một mẫu (x, x̂). Mô hình xử lý batch, tạo ra các biểu diễn được chiếu hl và (hu, ĥu) cho các mẫu được gán nhãn và không được gán nhãn tương ứng.

Ý tưởng chính từ [3] là gán nhãn giả cho các mẫu không được gán nhãn theo cách phi tham số bằng cách xem xét mối quan hệ của chúng với các mẫu được gán nhãn, tức là, việc gán nhãn láng giềng gần nhất. Các mẫu được so sánh trong không gian đặc trưng bằng cách sử dụng độ tương tự cosine của các đặc trưng được chiếu, và sau đó nhãn giả được thu được bằng cách tổng hợp nhãn theo độ tương tự. Chính thức hơn, cho chỉ số trên k đại diện cho chỉ số của mẫu thứ k trong mini-batch được gán nhãn, và δ là độ tương tự cosine. Ta có thể áp dụng một bộ phân loại láng giềng gần nhất mềm để phân loại mẫu không được gán nhãn được tăng cường x̂u như sau:

v̂ = SNN(ĥu, S, ϵ) = ∑K k eckδ(ĥu,hkl)/ϵ / ∑K i eciδ(ĥu,hil)/ϵ yk, (1)

trong đó S = [h1l, ..., hKl] là các đặc trưng của các mẫu hỗ trợ và ϵ là một tham số làm sắc nét kiểm soát entropy của nhãn giả. Tương tự, chúng ta có thể phân loại góc nhìn khác của cùng một mẫu v = SNN(hu, S, τ), với sự khác biệt duy nhất là chúng ta sử dụng một tham số làm sắc nét nhẹ nhàng hơn τ > ϵ, được gọi là nhiệt độ. Bây giờ, chúng ta có thể sử dụng v̂ như một nhãn giả mục tiêu và huấn luyện mạng thông qua mất mát entropy chéo:

LSNN = H(v, v̂). (2)

Cơ chế được mô tả ở trên khuyến khích mạng xuất ra các biểu diễn nhất quán cho các đầu vào tương tự, đồng thời cũng tính đến phân phối của các lớp trong không gian đặc trưng. Tuy nhiên, một vấn đề với công thức này là mạng có thể xuất ra các dự đoán không cân bằng hoặc thậm chí suy thoái khi một số lớp được dự đoán thường xuyên hơn những lớp khác. Để tránh điều này, PAWS áp đặt khả năng phân phối của tất cả các lớp là đồng nhất bằng cách sử dụng một thuật ngữ chính quy hóa được gọi là mất mát Tối đa hóa Entropy Trung bình (MEM) được định nghĩa là:†

LMEM = H(1/N ∑N n v̂n). (3)

Với hai mất mát này, tổng mất mát cho PAWS là một trung bình có trọng số của hai mất mát.

Lợi thế của công thức láng giềng gần nhất mềm này là nó sử dụng các mẫu được gán nhãn như các vectơ hỗ trợ, không phải như các mẫu huấn luyện, điều này làm giảm overfitting. Tính chất này thú vị từ quan điểm học liên tục, vì chúng ta muốn trích xuất càng nhiều tín hiệu huấn luyện càng tốt từ bộ đệm bộ nhớ mà không overfitting với nó. Tuy nhiên, PAWS không được thiết kế để hoạt động dưới sự thay đổi phân phối dữ liệu. Vấn đề chính của PAWS trong thiết lập CL là giả định rằng các tập được gán nhãn và không được gán nhãn thể hiện cùng một phân phối. Điều này không thể chấp nhận được trong CL, vì bộ đệm bộ nhớ chứa các lớp không có trong tập không được gán nhãn của nhiệm vụ hiện tại. Mất mát MEM làm trầm trọng thêm vấn đề này, vì nó cố gắng phân tán nhãn giả trên tất cả các lớp, ngay cả những lớp mà các mẫu không được gán nhãn không có sẵn. Một giải pháp đơn giản sẽ là sử dụng dữ liệu được gán nhãn của nhiệm vụ hiện tại và loại bỏ bộ đệm, nhưng điều này là không tối ưu vì bộ đệm rất quan trọng cho CL.

Một nhược điểm quan trọng khác của PAWS trong bối cảnh CL là sự mơ hồ của các phương pháp hai giai đoạn. PAWS hoạt động tốt nhất khi xương sống đã học được tinh chỉnh bằng cách sử dụng tập được gán nhãn. Đường ống hai giai đoạn này (pre-training và sau đó fine-tuning) dễ bị mất hiệu suất tổng quát hóa vì tập được gán nhãn rất nhỏ. Các phương pháp ngoại tuyến như PAWS có thể chấp nhận mất mát này để có được kiến thức chuyên biệt về các nhiệm vụ được nhắm mục tiêu. Tuy nhiên, CL yêu cầu tái sử dụng mô hình cho nhiệm vụ tiếp theo. Một mặt, sử dụng mô hình đã tinh chỉnh dẫn đến mất hiệu suất đáng chú ý trong các nhiệm vụ tiếp theo. Mặt khác, sử dụng checkpoint pre-trained (trước khi fine-tuning) bảo tồn nhiều tính chất tổng quát hơn nhưng mang lại chi phí bộ nhớ bổ sung, vì hai mô hình cần được lưu trữ, điều này không mong muốn trong CL. Trong phần sau, chúng tôi mô tả giải pháp của mình để vượt qua những hạn chế này. Một minh họa chi tiết về sự mơ hồ của các phương pháp hai giai đoạn có thể được tìm thấy trong tài liệu bổ sung.

†Với một sự lạm dụng ký hiệu nhẹ, chúng tôi đề cập đến H(·) với một đối số như hàm entropy, trong khi khi hai đối số được truyền, chúng tôi xem xét nó như hàm entropy chéo H(·,·).

## 5. NNCSL: Phương pháp láng giềng gần nhất của chúng tôi cho học bán giám sát liên tục

### 5.1. Người học bán giám sát liên tục của chúng tôi

Bây giờ chúng tôi mô tả phương pháp được đề xuất của chúng tôi tận dụng các điểm mạnh của phương pháp láng giềng gần nhất được mô tả trong Mục. 4, đồng thời vượt qua các điểm yếu của nó. Cách dễ nhất để làm cho phân phối được gán nhãn và không được gán nhãn khớp nhau là bỏ qua bộ đệm bộ nhớ. Điều này rõ ràng là không mong muốn đối với CL. Tuy nhiên, ta có thể đa nhiệm vụ PAWS với một mục tiêu khác cũng tính đến thông tin của bộ đệm bộ nhớ. Cụ thể, chúng tôi đề xuất xử lý các mẫu được gán nhãn của tất cả các lớp đã thấy cho đến nay, nhưng lọc ra các mẫu từ các nhiệm vụ trước đó để chúng không can thiệp vào việc tính toán Phương trình 1. Tuy nhiên, chúng ta có thể sử dụng đầu ra của bộ phân loại tuyến tính p và tối ưu hóa một mất mát entropy chéo tiêu chuẩn:

LLIN = ∑J j H(pk, yj), (4)

trên tất cả J mẫu được gán nhãn trong batch hiện tại (cũng chứa K mẫu được gán nhãn của nhiệm vụ hiện tại). Mất mát hoàn chỉnh cho người học bán giám sát liên tục cơ sở của chúng tôi (được đặt tên là CSL) như sau:

LCSL = LSNN + λMEM · LMEM + λLIN · LLIN. (5)

Mất mát này có một số hiệu ứng có lợi; nó: i) kích thích mạng tập trung vào các lớp cũ trong khi học các biểu diễn của các lớp mới thông qua PAWS, ii) tạo ra một hiệu ứng ensemble giữa hai bộ phân loại, iii) hoàn toàn loại bỏ nhu cầu fine-tuning, vì bộ phân loại tuyến tính được huấn luyện trực tuyến, và iv) cho phép chúng ta kiểm soát sự đánh đổi giữa việc khớp dữ liệu được gán nhãn hoặc không được gán nhãn thông qua tham số λLIN. Thú vị là, chúng tôi thấy rằng các giá trị rất nhỏ của λLIN hoạt động tốt trong thực tế, trong khi các giá trị lớn hơn tăng overfitting. Chúng tôi tin rằng, do tính chất tự giám sát một phần của nó, LSNN học các biểu diễn được cải thiện, có thể dễ dàng phân biệt bởi bộ phân loại tuyến tính. Một minh họa về kiến trúc của CSL được hiển thị trong Hình. 3.

### 5.2. Chưng cất láng giềng gần nhất mềm

Chưng cất thông tin [23] là một thực hành phổ biến trong CL, sử dụng các mô hình đông lạnh (hoặc một ngân hàng các đặc trưng và/hoặc xác suất) được huấn luyện trên các nhiệm vụ trước đó như một giáo viên để chính quy hóa mô hình hoạt động hiện tại, là một học sinh. Cho t là chỉ số cho nhiệm vụ hiện tại. Mô hình học sinh ft θ nhằm bắt chước các đầu ra của giáo viên ft−1 θ, trong khi học nhiệm vụ mới. Các công trình trước đó [22, 30] thường chưng cất hoặc logit của bộ phân loại tuyến tính hoặc các đặc trưng của các lớp ẩn của mạng. Tuy nhiên, trong CSL, động lực chính cho mạng học các biểu diễn là mất mát được áp dụng cho bộ phân loại láng giềng gần nhất mềm. Như được giải thích trong Mục. 5.1, mất mát này không đưa ra bất kỳ tín hiệu nào trên dữ liệu trước đó, vì các mẫu cũ bị lọc ra và chỉ được đưa vào bộ phân loại tuyến tính. Điều này được làm tệ hơn bởi thực tế rằng bộ phân loại láng giềng gần nhất được áp dụng trên một đầu chiếu riêng biệt h, không có động lực để nhớ kiến thức trước đó.

Để giảm thiểu những vấn đề này, chúng tôi thiết kế một mất mát Chưng cất Láng giềng Gần nhất (NND) mới kết hợp tốt với khung của chúng tôi. Mất mát dựa trên trực giác rằng chúng ta có thể đánh giá bộ phân loại láng giềng gần nhất trên không gian đặc trưng cũ bằng cách sử dụng cùng các mẫu hỗ trợ. Điều này tương đương với việc tính toán hai vectơ sau: w = SNN(hu, R, τ) và wt−1 = SNN(ht−1 u, Rt−1, τ), trong đó ht−1 u là một vectơ đặc trưng được xuất ra bởi giáo viên cho một mẫu không được gán nhãn xu, trong khi R và Rt−1 đại diện cho tập hỗ trợ của các lớp trước đó được nhúng trong không gian đặc trưng cũ và mới tương ứng. Để giảm thiểu việc quên, chúng tôi sử dụng các xác suất được dự đoán bởi giáo viên như một mục tiêu chưng cất:

LNND = H(w, wt−1). (6)

Lưu ý rằng đầu ra của giáo viên không được làm sắc nét như được thực hiện trong Phương trình 1. Chúng tôi áp dụng cùng nhiệt độ cho cả đặc trưng mới và cũ. Chúng tôi nhấn mạnh rằng ở đây chúng tôi sử dụng một bộ lọc đảo ngược như trong Mục. 5.1, để chưng cất kiến thức chỉ về các lớp trước đó. Xem Hình. 4 để có trực giác trực quan và tài liệu bổ sung để có hình ảnh về tác động của nó đối với các đặc trưng sâu. Đáng đề cập rằng chi phí bộ nhớ được giới thiệu bởi việc lưu trữ ft−1 θ có thể dễ dàng giảm bằng cách lưu trữ thay vào đó các đặc trưng, có trọng lượng nhẹ so với hình ảnh. Ví dụ, lưu trữ một đặc trưng 128 chiều mất khoảng 1/1000 bộ nhớ so với một hình ảnh RGB kích thước 224 × 224.

Mất mát NND của chúng tôi khác với mất mát chưng cất tiêu chuẩn. Chưng cất kiến thức tập trung vào phân phối cấp lớp được tính toán bằng cách so sánh các mẫu của các nhiệm vụ mới với nguyên mẫu của mỗi lớp. Điều này gây ra mất thông tin về các biểu diễn của các mẫu riêng lẻ trong không gian tiềm ẩn. Ngược lại, NND chưng cất các mối quan hệ tổng hợp của mỗi mẫu không được gán nhãn đối với tất cả các mẫu được gán nhãn trong mini-batch. Điều này khuyến khích mô hình duy trì các biểu diễn ổn định hơn, bằng cách neo các mẫu không được gán nhãn vào các mẫu được gán nhãn. Ngoài ra, NND có thể nắm bắt và chuyển giao các mối quan hệ mẫu-lớp phi tuyến theo định nghĩa, không giống như chưng cất kiến thức bị giới hạn ở các ranh giới tuyến tính. Ngoài ra, bộ phân loại SNN được tính toán trên một tập hỗ trợ khác nhau được lấy mẫu từ bộ đệm tại mỗi lần lặp, điều này giới thiệu tính ngẫu nhiên giúp chính quy hóa mô hình thêm. Hơn nữa, khi chưng cất ở nhiệt độ rất thấp (ví dụ, 0.1), hàm softmax từ Phương trình 1 hoạt động như một argmax (tức là, chọn các mẫu gần nhất trong không gian đặc trưng với độ tương tự cosine cao), dẫn đến việc chuyển giao thông tin chủ yếu về vùng lân cận cục bộ từ giáo viên đến học sinh. Cuối cùng, đối với chưng cất đặc trưng thiếu sự liên kết với nhãn lớp, NND mang nhiều thông tin hơn về phân phối lớp, dẫn đến hiệu suất được cải thiện.

### 5.3. Mất mát tổng thể

Mô hình NNCLS tổng thể, bao gồm hai thành phần mới, tức là, CSL và NND, được huấn luyện với mất mát:

LNNCSL = LCSL + λNND · LNND. (7)

## 6. Đánh giá và phân tích

### 6.1. Thiết lập thí nghiệm

**Tập dữ liệu.** Chúng tôi đánh giá phương pháp của mình trên ba tập dữ liệu. CIFAR-10 [26] là một tập dữ liệu gồm 10 lớp với 50k hình ảnh huấn luyện và 10k hình ảnh kiểm tra. Mỗi hình ảnh có kích thước 32×32. CIFAR-100 [26] tương tự như CIFAR-10, ngoại trừ nó có 100 lớp chứa 500 hình ảnh huấn luyện và 100 hình ảnh kiểm tra mỗi lớp. ImageNet-100 [48] là một tập con 100 lớp của tập dữ liệu ImageNet-1k từ ImageNet Large Scale Visual Recognition Challenge 2012, chứa 1300 hình ảnh huấn luyện và 50 hình ảnh kiểm tra mỗi lớp.

**Thiết lập bán giám sát liên tục.** Đối với cả CIFAR-10 và CIFAR-100, chúng tôi huấn luyện các mô hình với ba mức độ giám sát khác nhau, tức là, λ ∈ {0.8%, 5%, 25%}. Ví dụ, điều này tương ứng với 4, 25, 125 mẫu được gán nhãn mỗi lớp trong CIFAR-100. Đối với ImageNet-100, chúng tôi chọn 1% dữ liệu được gán nhãn. Để xây dựng các tập dữ liệu liên tục, chúng tôi sử dụng thiết lập tiêu chuẩn trong tài liệu [8, 52], và chia các tập dữ liệu thành các nhiệm vụ rời rạc bằng nhau: 5/10/20 nhiệm vụ cho CIFAR-10/CIFAR-100/ImageNet-100, tức là, 2/10/5 lớp mỗi nhiệm vụ, tương ứng. Chúng tôi tuân theo thiết lập học tăng dần lớp tiêu chuẩn trong tất cả các thí nghiệm của mình: trong các giai đoạn CL, chúng tôi giả định rằng tất cả dữ liệu của các nhiệm vụ trước đó đều bị loại bỏ. Một bộ đệm bộ nhớ có thể được xây dựng cuối cùng, nhưng chỉ cho dữ liệu được gán nhãn. Theo [8], chúng tôi đặt kích thước bộ đệm cho dữ liệu được gán nhãn là 500 hoặc 5120, để đảm bảo so sánh công bằng.

**Metric.** Chúng tôi chủ yếu đánh giá hiệu suất của các phương pháp khác nhau xem xét độ chính xác trung bình trên tất cả các lớp đã thấy sau mỗi nhiệm vụ, như thường thấy trong các phương pháp CL [18]. Phân tích với các metric khác, chẳng hạn như chuyển giao tiến và lùi, có thể được tìm thấy trong tài liệu bổ sung.

**Chi tiết triển khai.** Như trong [8], chúng tôi sử dụng ResNet18 làm xương sống cho tất cả các tập dữ liệu. Chúng tôi áp dụng việc triển khai của [3], trừ khi được nêu rõ. Cụ thể, chúng tôi sử dụng bộ tối ưu hóa LARS [57] với giá trị momentum là 0.9. Chúng tôi đặt weight decay là 10−5, batch-size là 256. Tỷ lệ học được đặt thành 0.4 cho CIFAR-10 và 1.2 cho CIFAR-100 và ImageNet-100, tương ứng. Chúng tôi áp dụng 10 epochs warm-up và giảm nó với một bộ lập lịch cosine. Đối với hai góc nhìn tương quan của cùng một mẫu, chúng tôi tạo ra hai crop lớn và hai crop nhỏ của mỗi mẫu. Các crop lớn phục vụ như mục tiêu cho nhau, trong khi chúng đều là mục tiêu cho các crop nhỏ. Chúng tôi áp dụng tăng cường dữ liệu như trong [16]. Label smoothing được áp dụng với hệ số smoothing là 0.1. Đầu đánh giá tuyến tính bổ sung là một lớp tuyến tính đơn giản, mà chúng tôi sử dụng để dự đoán nhãn tại thời điểm kiểm tra. Chúng tôi chọn λNND = 0.2 và λLIN = 0.005. Đối với bộ đệm bộ nhớ, chúng tôi sử dụng một chiến lược lấy mẫu ngẫu nhiên đơn giản. Chúng tôi chạy thí nghiệm 3 lần với các seed ngẫu nhiên khác nhau. Độ lệch chuẩn cũng được báo cáo, nếu có thể áp dụng. Chi tiết triển khai thêm và phân tích tăng cường dữ liệu có thể được tìm thấy trong tài liệu bổ sung.

**Baseline.** Như baseline, trước tiên chúng tôi xem xét các phương pháp CL được giám sát đầy đủ truyền thống. Một cách đơn giản để chuyển đổi chúng thành thiết lập bán giám sát liên tục là chỉ sử dụng dữ liệu được gán nhãn trong quá trình huấn luyện và loại bỏ dữ liệu không được gán nhãn. Chúng tôi xem xét hai loại phương pháp: các phương pháp không có replay, cụ thể là, liên tục fine-tuning mô hình trên mỗi nhiệm vụ mới, online elastic weight consolidation (oEWC) [25], và các phương pháp dựa trên replay, tức là, experience replay (ER) [39], iCaRL [38], FOSTER [51] và X-DER [7]. Chúng tôi ký hiệu PseudoER như một phương pháp baseline hai giai đoạn bổ sung, là sự kết hợp của học bán giám sát (PAWS) và phương pháp CL (ER). Chính xác hơn, chúng tôi liên tục huấn luyện một PAWS và sử dụng nó để tự gán nhãn dữ liệu không được gán nhãn. Một phương pháp ER được áp dụng sau đó trên dữ liệu được gán nhãn và nhãn giả. Chúng tôi cũng xem xét các phương pháp baseline học bán giám sát liên tục, chẳng hạn như CCIC [8] và ORDisCo [52]. Trong khi CCIC có định nghĩa rõ ràng về bộ đệm bộ nhớ cho dữ liệu được gán nhãn, là 500 hoặc 5120, ORDisCo trực tiếp lưu trữ tất cả dữ liệu được gán nhãn mà mô hình nhận được. Do đó chúng tôi ký hiệu kích thước bộ đệm bộ nhớ của nó với giá trị lớn nhất, là M = 12500, tương đương với 25% của CIFAR-10. Đối với cận trên, thường được hiển thị trong CL [18], nó được thu được bằng cách huấn luyện cùng nhau mô hình với tất cả dữ liệu có sẵn. Cận dưới đề cập đến fine-tuning liên tục. CSL cũng được bao gồm như một ablation của NNCSL không có mất mát NND.

### 6.2. Kết quả

**CIFAR-10 và CIFAR-100.** Trước tiên chúng tôi báo cáo hiệu suất của các phương pháp khác nhau trên CIFAR-10 và CIFAR-100 trong Bảng. 1. Cận trên trên CIFAR-10 là 92.1 ±0.1%, và trên CIFAR-100 là 67.7 ±0.9%. NNCSL vượt trội so với tất cả các đối thủ cạnh tranh trong tất cả các thiết lập trừ một, với biên độ đáng kể. Ví dụ, khi sử dụng bộ đệm 5120 và 0.8% dữ liệu được gán nhãn, NNCSL hoạt động tốt hơn hoặc tương đương đáng kể với hầu hết tất cả các phương pháp khác, ngay cả khi chúng sử dụng 25% dữ liệu được gán nhãn, tức là, khoảng 30 lần nhiều giám sát hơn. Cũng thú vị khi lưu ý rằng NNCSL có phương sai rất thấp (≤0.7) trên tất cả các thiết lập, cho thấy sự hội tụ tốt hơn và học biểu diễn trong quá trình huấn luyện.

Từ kết quả trong Bảng. 1, bộ đệm bộ nhớ được chứng minh là hiệu quả để giảm thiểu việc quên, vì các phương pháp dựa trên replay vượt trội đáng kể so với các phương pháp dựa trên chính quy hóa. PseudoER hoạt động tốt hơn ER khi dữ liệu được gán nhãn bị hạn chế (0.8%), nhưng hoạt động kém hơn khi tỷ lệ cao hơn (5%, 25%). Chúng tôi suy đoán điều này là kết quả của dữ liệu nhãn giả nhiễu thay thế sự thật căn bản trong bộ đệm bộ nhớ bởi chiến lược lấy mẫu của ER, gây ra việc quên mạnh hơn. Ngoài ra, PseudoER bị giới hạn trên bởi PAWS, vì ER phụ thuộc vào độ chính xác của việc gán nhãn giả.

Các phương pháp như CCIC và ORDisCo† hưởng lợi từ thiết kế cụ thể của chúng cho kịch bản học bán giám sát liên tục. Mặc dù ORDisCo có bộ đệm bộ nhớ lớn hơn, hiệu suất của nó kém hơn so với CCIC. Chúng tôi tin rằng điều này là do khó khăn trong việc huấn luyện cùng nhau một bộ phân loại liên tục với một mô hình GAN. CCIC hoạt động khá tốt trên CIFAR-10, đặc biệt với bộ đệm bộ nhớ lớn. Khi kích thước bộ đệm là 5120, CCIC hoạt động tốt hơn NNCSL trong thiết lập 25%. Chúng tôi nghi ngờ rằng phương pháp của chúng tôi underfit trong thiết lập này, do trọng số rất nhỏ của mất mát đánh giá tuyến tính. Ngược lại, CCIC phụ thuộc nhiều hơn vào dữ liệu được gán nhãn, với tầm quan trọng bằng nhau hoặc thậm chí cao hơn cho mất mát giám sát. Để xác thực giả thuyết của chúng tôi, chúng tôi tăng trọng số λLIN cho mất mát bộ phân loại tuyến tính của chúng tôi. Chúng tôi thu được độ chính xác 84.5 ±0.4%, có thể so sánh với CCIC trong cùng thiết lập (84.7 ±0.9%). Trong trường hợp của tập dữ liệu với nhiều lớp hơn như CIFAR-100, sự vượt trội của NNCSL rõ ràng rõ ràng. So sánh thêm với chiến lược replay của ORDisCo có thể được tìm thấy trong tài liệu bổ sung.

†Lưu ý rằng kết quả của ORDisCo được cung cấp trực tiếp bởi các tác giả của [52] vì không có triển khai mã nguồn mở của phương pháp của họ.

**ImageNet-100.** Chúng tôi cũng đánh giá trên tiêu chuẩn đầy thách thức hơn của ImageNet-100 trong thiết lập bán giám sát liên tục 20 nhiệm vụ và kích thước bộ đệm 5120. Như được hiển thị trong Bảng. 2, NNCSL là phương pháp hoạt động tốt nhất. CCIC không cho thấy cải thiện đáng kể với nhiều dữ liệu được gán nhãn hơn (5% & 25%) và cũng trở nên kém hơn so với các phương pháp CL thông thường, ví dụ, ER. Chúng tôi nghi ngờ đây là hạn chế của việc học biểu diễn để trích xuất thông tin từ hình ảnh với độ phân giải cao hơn và phương sai lớn hơn, vì chúng tôi quan sát độ chính xác huấn luyện và xác thực của nó trong thiết lập này thấp hơn đáng kể. Ngoài ra, Hình. 5 minh họa sự phát triển của độ chính xác trung bình. Chúng tôi lưu ý rằng độ chính xác trung bình của NNCSL ổn định ở khoảng 30% sau nhiệm vụ thứ 10 và có sự phục hồi rõ ràng giữa các nhiệm vụ 11 và 16, cho thấy NNCSL có thể giữ lại hiệu quả kiến thức thu được trong các bước học liên tục. Ngược lại, các phương pháp cạnh tranh trước đó rõ ràng bị quên trong thiết lập đầy thách thức này, đặc biệt trong một số nhiệm vụ đầu tiên. Mặc dù có hiệu ứng phục hồi tương tự trong một số phương pháp, ví dụ, FOSTER, chúng không thể duy trì gần với NNCSL.

### 6.3. Phân tích bổ sung

**Ablation.** Chúng tôi ablate các thành phần của khung của chúng tôi trong Bảng. 3 trên cả CIFAR-10 (C10 trong bảng), CIFAR-100 (C100) và ImageNet100 (IN100). Cải thiện lớn nhất đến từ lọc (10.8% và 6.2% cải thiện trên CIFAR-10 và CIFAR-100 tương ứng) và chưng cất (9.4% và 4% cải thiện trên CIFAR-10 và CIFAR-100 tương ứng). Điều này xác nhận các đóng góp của các thành phần được đề xuất của chúng tôi. Mặc dù mất mát đánh giá tuyến tính không mang lại cải thiện đáng kể về hiệu suất tổng thể, chúng tôi thấy nó hữu ích để ổn định quá trình học, đặc biệt cho tập dữ liệu nhỏ với giám sát rất hạn chế. Thú vị là, PAWS và CSL (w/o filter) phân kỳ trên ImageNet-100, vì chúng không thể học một biểu diễn hiệu quả với quy mô tăng lên, do sự bất ổn định được giới thiệu bởi sự dịch chuyển phân phối và việc quên dữ liệu không được gán nhãn (Mục. 4). Chúng tôi đã bao gồm phân tích bổ sung về hành vi sụp đổ này trong tài liệu bổ sung.

**Tác động của chưng cất.** Bổ sung cho Hình. 1 (phải), cho thấy định tính hiệu suất vượt trội của NND được đề xuất của chúng tôi, chúng tôi báo cáo trong Bảng. 4 độ chính xác cuối cùng của mỗi nhiệm vụ sau khi huấn luyện trên tất cả các nhiệm vụ. Một phương pháp chưng cất hiệu quả nên duy trì hiệu suất của các nhiệm vụ cũ (ví dụ, nhiệm vụ 1 cho NND so với chưng cất đặc trưng) và học hiệu quả các nhiệm vụ mới (ví dụ, nhiệm vụ 5 cho NND so với chưng cất kiến thức).

**Bằng chứng về hiệu quả.** Chúng tôi hình ảnh hóa trong Hình. 6 đường cong học của độ chính xác trung bình cho dữ liệu huấn luyện không được gán nhãn. Như chúng tôi minh họa trong Mục. 5.1, mất mát MEM thông thường có hại cho việc học vì nó buộc mô hình gán nhãn giả không chính xác cho dữ liệu không được gán nhãn. CSL được đề xuất của chúng tôi hiệu quả giải quyết vấn đề này và cho phép mô hình học một biểu diễn tốt hơn với dữ liệu không được gán nhãn. Lưu ý rằng chúng tôi chỉ sử dụng nhãn của dữ liệu không được gán nhãn để giám sát quá trình huấn luyện, và không có thông tin nhãn nào bị rò rỉ tại thời điểm huấn luyện.

**Đầu đánh giá tuyến tính.** Chúng tôi báo cáo độ chính xác huấn luyện và xác thực trong Bảng. 5. Lưu ý rằng đối với độ chính xác tại thời điểm huấn luyện, chúng tôi sử dụng trung bình của độ chính xác trên nhiệm vụ hiện tại, tức là, đánh giá trên nhiệm vụ t trong khi huấn luyện trên nhiệm vụ t. Nói chung, có độ chính xác huấn luyện cao của dữ liệu không được gán nhãn có nghĩa là mô hình học một biểu diễn tốt, dẫn đến độ chính xác xác thực tốt. Ta có thể quan sát overfitting khi λLIN tăng. Điều này biện minh cho lựa chọn của chúng tôi về i) có trọng số nhỏ cho đầu đánh giá tuyến tính, và ii) chọn PAWS làm cơ sở, không trực tiếp sử dụng dữ liệu được gán nhãn như các mẫu huấn luyện để tránh overfitting. Hơn nữa, chúng tôi quan sát underfitting khi λLIN nhỏ hơn 0.005. Nó xác nhận nhu cầu cho bộ phân loại tuyến tính này trong kịch bản bán giám sát liên tục. Do đó chúng tôi đặt λLIN là 0.005 trong các thí nghiệm của mình.

**Ablation của bộ đệm bộ nhớ.** Để xác minh hiệu quả của bộ đệm replay, chúng tôi đánh giá NNCSL của chúng tôi mà không có điều này và quan sát sự giảm mạnh về hiệu suất (19.7% so với 73.2%) trên CIFAR-10 5 nhiệm vụ với 0.8% dữ liệu được gán nhãn. Tuy nhiên, phương pháp của chúng tôi phục hồi 95% hiệu suất (69.2% so với 73.2%) trong khi chỉ sử dụng 10% bộ nhớ (50 so với 500). Chúng tôi suy đoán hiệu quả dữ liệu như vậy là kết quả từ bộ phân loại tuyến tính cung cấp gradient mạnh trong phương pháp của chúng tôi. Chi tiết thêm của nghiên cứu này có thể được tìm thấy trong tài liệu bổ sung.

## 7. Kết luận

Trong công trình này, chúng tôi đã nghiên cứu học bán giám sát liên tục và đề xuất NNCSL, một phương pháp mới dựa trên láng giềng gần nhất mềm và chưng cất. Các thí nghiệm rộng rãi của chúng tôi cho thấy hiệu suất vượt trội của NNCSL đối với các phương pháp hiện có, thiết lập một trạng thái nghệ thuật mới. Công trình trước đó [3] cho thấy rằng sử dụng một mạng mạnh mẽ hơn như ResNet rộng hơn hoặc sâu hơn có thể cải thiện thêm hiệu suất. Trong khi điều này không được đề cập trong công trình này, chúng tôi xem xét nó là một hướng thú vị cho công trình tương lai. Trong bài báo này, chúng tôi đã xem xét một tỷ lệ cố định giữa các mẫu được gán nhãn và không được gán nhãn trên tất cả các nhiệm vụ. Một tỷ lệ thay đổi sẽ là một thiết lập thậm chí đầy thách thức hơn cho nghiên cứu tương lai.
