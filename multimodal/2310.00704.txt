# 2310.00704.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.00704.pdf
# File size: 1262790 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Under review as a conference paper at ICLR 2024
UNIAUDIO : ANAUDIO FOUNDATION MODEL TOWARD
UNIVERSAL AUDIO GENERATION
Dongchao Yang1∗, Jinchuan Tian2∗, Xu Tan3†, Rongjie Huang4, Songxiang Liu, Xuankai Chang2,
Jiatong Shi2, Sheng Zhao3, Jiang Bian3, Zhou Zhao4, Xixin Wu1, Helen Meng1†
1The Chinese University of Hong Kong,2Carnegie Mellon University,
3Microsoft Research Asia,4Zhejiang University
dcyang@se.cuhk.edu.hk, jinchuat@andrew.cmu.edu
ABSTRACT
Large Language models (LLM) have demonstrated the capability to handle a vari-
ety of generative tasks. This paper presents the UniAudio system, which, unlike
prior task-specific approaches, leverages LLM techniques to generate multiple
types of audio (including speech, sounds, music, and singing) with given input
conditions. UniAudio 1) first tokenizes all types of target audio along with other
condition modalities, 2) concatenates source-target pair as a single sequence, and
3) performs next-token prediction using LLM. Also, a multi-scale Transformer
model is proposed to handle the overly long sequences caused by the residual
vector quantization-based neural codec in tokenization. Training of UniAudio is
scaled up to 165K hours of audio and 1B parameters, based on all generative tasks,
aiming to obtain sufficient prior knowledge not only in the intrinsic properties of
audio but also the inter-relationship between audio and other modalities. Therefore,
the trained UniAudio model has the potential to become a foundation model for
universal audio generation: it shows strong capability in all trained tasks and can
seamlessly support new audio generation tasks after simple fine-tuning. Experi-
ments demonstrate that UniAudio achieves state-of-the-art or at least competitive
results on most of the 11 audio generation tasks. Demo and code are released.1.
1 I NTRODUCTION
Audio generation is an important component of generative AI. Recently, the popularity of generative
AI has induced increasingly emergent and varying needs in audio generation: audio is expected to
be generated based on humans’s demands, such as speech synthesis (TTS), voice conversion (VC),
singing voice synthesis (SVS), text-to-sound, and text-to-music. Prior works on audio generation
tasks are commonly task-specific: their designs heavily leverage domain knowledge and their usage
is restricted to fixed setups (Tan et al., 2021; Luo & Mesgarani, 2019; Zmolikova et al., 2023; Huang
et al., 2021b; Cho et al., 2021). Instead of taking care of each task independently, this work is an
attempt to achieve universal audio generation, which intends to accomplish multiple audio generation
tasks with only one unified model. The universal audio generation model is expected to obtain
sufficient prior knowledge in audio and related modalities, which has the potential to provide simple
and effective solutions for the increasing needs of generating diverse types of audio.
The superiority of Large Languge Models (LLM) in text-generative tasks inspires a series of LLM-
based models in audio generation (Wang et al., 2023a; Kharitonov et al., 2023; Huang et al., 2023b;
Agostinelli et al., 2023; Borsos et al., 2023). Among these works, LLM’s capability in independent
tasks has been extensively studied in tasks like text-to-speech (TTS) (Wang et al., 2023a; Kharitonov
et al., 2023; Huang et al., 2023b) and music generation (Agostinelli et al., 2023; Copet et al., 2023),
and achieves competitive performance. However, LLM’s ability to process multiple tasks with a
unified model is less exploited in audio generation research: most existing LLM-based works are
still designed for single tasks (Wang et al., 2023a; Kharitonov et al., 2023). We argue that achieving
universality and versatility in audio generation through the LLM paradigm is promising but has not
yet been comprehensively studied before this work.
1https://uniaudio666.github.io/demo_UniAudio/
* Equal contribution; †Corresponding author
1arXiv:2310.00704v6  [cs.SD]  10 Dec 2024

--- PAGE 2 ---
Under review as a conference paper at ICLR 2024
Toward universal audio generation, this work presents UniAudio, which adopts LLM techniques
and is able to generate multiple types of audio (speech, sounds, music, and singing) conditioned on
various input modalities, such as phoneme sequences, textual descriptions, and audio itself. The
proposed UniAudio is mainly featured as follows: First, all types of audio, along with all other input
modalities, are tokenized as discrete sequences. Specifically, a universal neural codec model is built to
effectively tokenize audio regardless of the audio type, and other tokenizers are used to tokenize other
different modalites. Then, UniAudio concatenates the source-target pair as a single sequence. Lastly,
UniAudio performs next-token prediction using LLM. The residual vector quantization (Zeghidour
et al., 2021) based on neural codecs is used in the tokenization process, resulting in overly long token
sequences (one frame corresponding to multiple tokens) that cannot be processed efficiently by LLM.
A multi-scale Transformer architecture is designed to reduce computational complexity by modeling
the inter- and intra-frame correlation separately. Specifically, a global Transformer module is used to
model the inter-frame correlation ( e.g.semantic level), and a local Transformer module is used to
model the intra-frame correlation ( e.g.acoustic level).
To demonstrate the scalability of UniAudio for new tasks, the building process of UniAudio takes
two stages. Firstly, the proposed UniAudio is trained on multiple audio generation tasks jointly,
which allows the model to obtain sufficient prior knowledge not only of the intrinsic properties of
audio but also of the interrelationship between audio and other input modalities. Secondly, through
fine-tuning, the trained model can seamlessly support more unseen audio generation tasks. Thus,
UniAudio has the potential to become a foundation model for universal audio generation: it is able to
continuously support emergent needs in audio generation. Experimentally, our UniAudio supports 11
audio generation tasks: the training stage includes 7 audio generation tasks, while 4 tasks are further
added in the fine-tuning stage. The building process of UniAudio is scaled up to 165k hours of audio
and 1B parameters. Among the 11 tasks, UniAudio consistently obtains competitive performance in
both objective and subjective evaluations. State-of-the-art results are even achieved on most of these
tasks. Further investigation suggests that training multiple tasks simultaneously in the training stage
is mutually beneficial to each task involved. In addition, UniAudio can effectively adapt to new audio
generation tasks and outperform task-specific models with a non-trivial gap.
To sum up, this work reveals that building universal audio generation models is necessary, promising,
and beneficial. The main contributions of this work are summarized as follows:
(1) Toward universal audio generation, UniAudio is presented as a unified solution for 11 audio
generation tasks.
(2) Per methodology, UniAudio provides novel approaches for (i) sequential representations of audio
and other input modalities; (ii) uniform formulation for LLM-based audio generation tasks; and (iii)
efficient model architecture specifically designed for audio generation.
(3) Per experiments, the overall performance of UniAudio is well validated, and the benefits of
building a versatile audio generation model are verified by exhaustive experimental results.
(4) Demo and code are released, in the hope that UniAudio can become a foundation model that
supports emergent audio generation in future research.
2 U NIAUDIO
This section introduces the technical details of the proposed UniAudio. Section 2.1 explains how
audio and other modalities are tokenized. Then, all considered audio generation tasks are uniformly
formulated in Section 2.2. Subsequently, the multi-scale Transformer architecture is proposed in
Section 2.3 to handle the overly long sequence challenge caused by the adoption of neural codecs.
2.1 T OKENIZATION
LLM are commonly used for sequential modeling, so audio and all other input modalities are
tokenized before being processed. These processes for each modality are completed by independent
modules. All of these modules are fixed in the optimization of UniAudio or parameter-free.
2.1.1 A UDIO
For all audio generation tasks considered in this work, audio, regardless of its types (speech, sounds,
music, or singing), is the target to predict. Instead of modeling different types of audio separately,
UniAudio intends to tokenize all types of audio as a single and unified modality (even though they
2

--- PAGE 3 ---
Under review as a conference paper at ICLR 2024
commonly have distinct patterns, such as frequency span), which requires a model that is well-suited
to mapping all audio types into a shared latent space. Following Wang et al. (2023a); Kharitonov
et al. (2023), neural codec models (Défossez et al., 2022; Yang et al., 2023b; Kumar et al., 2023) are
used in this work for audio tokenization. An audio signal of duration dwith sample rate fscan be
represented by a sequence x∈[−1,1]d∗fs. An audio neural codec intends to compress xand then
recover it as ˆ xusing an encoder-decoder architecture with a quantization module:
h=Encoder (x)∈ RT∗L;ˆh=Quantization (h);ˆ x=Decoder (ˆh) (1)
where Tdenotes the number of audio frames after down-sampling in the encoder, and Ldenotes the
feature dimension of the encoder. The discrete representations of audio are the intermediate product
of the quantization process. Given any frame of hidden output ht, the integer vector zt= [z1
t, ..., znq
t]
is generated by Residual Vector Quantization (RVQ) (Zeghidour et al., 2021), where nqdenotes the
number of vector quantization layers. Iteratively, each element zk
tis the index among all pre-learned
and fixed k-th level quantizer vectors {q∗
k}that has the smallest L2 distance to the residual between
htand the sum of all previous chosen quantizer vectors {qzj
t
j, j= 1, ..., k−1}. With the discrete
representation zt,ˆhtis reconstructed as a close estimation of htthat can be used to recover xtwith
the decoder.
zk
t= arg min
mDistance (ht−k−1X
j=1qzj
t
j,qm
k);ˆht=nqX
j=1qzj
t
j; 1≤k≤nq (2)
The discrete representation of all audio frames z∈ZT×nqis a matrix and needs to be converted into
a sequence before being processed by LM: it is simply flattened as a sequence, in which every nq
element for one frame is consecutive. Without specifically stated, we set nq= 3in our experiments.
As the waveform can be recovered from zwith a neural codec decoder, the rest of this paper mainly
discusses how to predict the audio token sequence zusing LLM techniques. As UniAudio intends to
generate both speech and non-speech content, we build the codec model on our own and with broader
data coverage. Details of our codec configuration is in Appendix E.
2.1.2 O THER MODALITIES
Besides audio, other modalities considered in UniAudio also need to be represented as sequences.
In addition, most of these sequences are transformed into discrete ones through tokenization. The
serialization and tokenization of these input modalities, along with their key features, are briefly
summarized as below.
Phoneme: Phonemes are the basic units of speech pronunciation in linguistics. Phoneme sequences
have multiple sources: (1) when only text is available, phoneme sequence without duration informa-
tion can be obtained by text-to-phoneme mapping using a pronunciation dictionary; (2) when only
speech is available, phoneme sequence with duration information is obtained by beam search of the
DNN-HMM system (Hinton et al., 2012); (3) when both text and speech are available, phoneme
sequence with duration information is obtained by forced alignment of the DNN-HMM system2.
MIDI: MIDI (Zhang et al., 2022) is widely used for singing voice synthesis tasks. F0 and duration
information are included in the MIDI. We use the duration information to flatten the F0 sequence, so
that the frame-level F0 sequence is obtained.
Text: Text acts as a effective carrier of human instructions in audio generation tasks (Yang et al.,
2023a; Copet et al., 2023). In this work, these textual instructions are represented as continuous
embeddings derived from pre-trained text LLM (Raffel et al., 2020), as these embeddings contain
rich textual semantics. Processing these continuous embeddings with LLM is further clarified in
Section 2.33.
Semantic Token: The semantic tokens are derived from the continuous embeddings output by audio
self-supervised learning (SSL) models. These continuous representations are highly informative and
2CMUDict (http://www.speech.cs.cmu.edu/cgi-bin/cmudict) is adopted as the pronunciation dict; kaldi recipe
(https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech/s5/local/chain/run_tdnn.sh) is adopted to build the
deep neural network-hidden Markov model (DNN-HMM) system.
3The encoder of T5 (https://github.com/google-research/text-to-text-transfer-transformer) is used to extract
the continuous text embeddings.
3

--- PAGE 4 ---
Under review as a conference paper at ICLR 2024
can be adopted in both speech understanding (Rubenstein et al., 2023) and generative tasks (Borsos
et al., 2023). Following Huang et al. (2023b), these continuous representations are tokenized by
performing K-means clustering (Hsu et al., 2021) over these continuous representations. Since the
continuous representations are frame-level, the semantic tokens also encode duration information4.
2.2 U NIFIED TASK FORMULATION
Table 1: Sequence formats of all tasks supported by UniAudio. Text color represents modality. black:
audio; green: phoneme; blue: MIDI; purple: text; brown: semantic token. ♣means tasks that
generate audio with deterministic length. ♢: means tasks that are only included in the fine-tuning
stage. The speaker prompt is a 3-second speech and is used to represent the speaker identification.
Task Conditions Audio Target
Text-to-Speech (TTS) (Wang et al., 2023a) phoneme, speaker prompt speech
V oice Conversion (VC)♣(Wang et al., 2023e) semantic token, speaker prompt speech
Speech Enhancement (SE)♣(Wang et al., 2023b) noisy speech speech
Target Speech Extraction (TSE)♣(Wang et al., 2018) mixed speech, speaker prompt speech
Singing V oice Synthesis (SVS) (Liu et al., 2022) phoneme (with duration), speaker prompt, MIDI singing
Text-to-Sound (Sound) (Yang et al., 2023c) textual description sounds
Text-to-Music (Music) (Agostinelli et al., 2023) textual description music
Audio Edit (A-Edit)♣♢(Wang et al., 2023d) textual description, original sounds sounds
Speech dereverberation (SD)♣♢(Wu et al., 2016) reverberant speech speech
Instruct TTS (I-TTS)♢(Guo et al., 2023) phoneme, textual instruction speech
Speech Edit (S-Edit)♢(Tae et al., 2021) phoneme (with duration), original speech speech
For all tasks considered in UniAudio, the target audio is generated based on given conditions. With
the same target modality, i.e., audio, it is the conditions that define different audio generation tasks.
However, even with the variance in conditions, all tasks can still be uniformly formulated as sequential
modeling tasks that can be processed by LLM: both the target audio and the conditions are first
transformed as sub-sequences and spliced as [ conditions ,target ] sequences to be processed.
UniAudio supports 11 audio generation tasks in total. The sequential formats of each task are defined
in Table 1, in which the sub-sequences of all modalities are derived as in Section 2.1. However,
due to the unique configurations of each task, some of the condition sub-sequences are subject to
task-specific pre-processing operations during the tokenization. For audio, these operations are
mainly for data corruption, such as adding noise, reverberation, and speech mixed with other speakers
in the raw audio before tokenization. For phoneme and semantic tokens, duration information is
reserved by default but can also be removed. For singing voice synthesis and speech edit tasks, the
duration information of phoneme is used. For TTS and I-TTS tasks, the duration information is not
used. For MIDI, the duration information is used repeat the F0 sequence. For text embeddings, no
operations are applied in this work.
To avoid ambiguity, some special discrete tokens (enclosed by <>) are inserted to indicate (1) the start
and end of the whole sequence; (2) the start and end of each sub-sequence of a certain modality; and
(3) the task identifier. For example, for a text-to-sound task sequence that generates target audio based
on textual description, the whole sequence is like: <start> <sound_task> <text_start> text_sequence
<text_end> <audio_start> audio_sequence <audio_end> <end> .
2.3 M ULTI -SCALE TRANSFORMER
Previous work on LLM-based audio generation (Copet et al., 2023) advocates to modeling the discrete
audio tokens as flattened sequences. If so, these sequences are processed in the length of T×nq,
which is highly challenging considering the quadratic space complexity of Transformer (Vaswani
et al., 2017) with respect to the lengths. Inspired by Yu et al. (2023), a multi-scale Transformer
architecture is specifically designed for discrete audio sequences, which is a hierarchical model that
processes the inter- and intra-frame correlation by global and local Transformer modules separately.
An overview of the proposed architecture is in Figure 1. Instead of processing the whole flattened
sequence token-by-token like prior works (Kharitonov et al., 2023), the multi-scale transformer
4The 9-th layer hidden output of Hubert (Hsu et al., 2021) is adopted as the semantic token representations
(https://github.com/facebookresearch/fairseq/hubert). The number of clusters for K-means is 500.
4

--- PAGE 5 ---
Under review as a conference paper at ICLR 2024
Audio Phoneme MIDI TextTokenization for Multiple Modalities
Semantic
TokenCondition Sequences Target Audio Sequence
TTS VC SE TSE ...Task Formulation for Multiple Tasks
Global TransformerPredicted Audio Sequence
Multi-Scale Transformer
<e><e>Local
TransformerLocal
TransformerLocal
Transformer<e><e><e>
Figure 1: Overview of UniAudio (left) and multi-scale Transformer architecture (right). <e> represent
the end of the sequence. zk
tdenotes the k-th audio token at t-th frame.
considers patches (i.e., every consecutive nqtoken) as the global modeling units and then handles the
tokens within each patch locally. Note that both the global and local Transformers are causal.
For audio token sequences, each patch accounts for nqconsecutive audio tokens that exactly represent
one audio frame. First, as suggested in Equation 2, regardless of the exact choices of each quantization
vector qz∗
t∗, it is the summed quantization vector ˆhtthat is used to represent the audio frame. Thus,
in the embedding stage, each patch (a.k.a., frame) is represented by the summed vector of the
corresponding embeddings before entering the global Transformer. Second, the global Transformer
is to predict audio frame-by-frame: to predict the frame xt, it outputs the continuous representations
that include frame xt−1and all previous content. These continuous representations will be further
processed by the local Transformer. Third, also as in Equation 2, given the hidden representation
ht, the acquisition of ztis independent of any hidden output other than ht. Inspired by this, it is
reasonable to predict the discrete tokens for frame xt, a.k.a., patch zt, only with the hidden output
of global Transformer corresponding to frame xt−1. To be more detailed, as the acquisition of each
token zk
tis auto-regressively dependent on its prior tokens {zj
t|j < k}, a local Transformer is adopted
to predict the patch sequence ztin auto-regressive style. During this process, the corresponding
vector output by the global transformer acts as a patch-level context, which is linearly transformed
and then added to the embedded results of each token zk
t.
The proposed multi-scale Transformer architecture is also compatible with discrete and continuous
sequences besides audio. For all discrete tokens except audio (phoneme, semantic, MIDI and special
tokens), each token has independent semantics and thus should account for one patch. So these
discrete tokens repeat for nqtimes to fill each patch. The continuous text embeddings are also
repeated for nqtimes for the same purpose. Additionally, their embedding process is replaced by
a linear transformation while their predicting targets for local Transformer are consecutive special
tokens <continuous_token> .
The design of the proposed multi-scale Transformer can effectively reduce computational complexity.
First, the equivalent sequence length for the global Transformer is reduced from T×nqtoT, which
makes the global modeling cost independent to nqand thus the adoption of a larger nqbecomes
feasible. Second, the intra-patch computation to generate the discrete tokens for each frame is
offloaded to the local Transformer. The computation on the local transformer is comparatively light
since it only processes the very short sequence (fixed to the length of nq) and empirically has fewer
parameters than the global Transformer by design.
3 E XPERIMENTS
This section first introduces the experimental setup in Section 3.1. The results for the training stage
and the fine-tuning stage are presented in Section 3.2 and 3.3 respectively. Ablation studies are
presented in Section 3.4.
3.1 E XPERIMENTAL SETUP
Data and Model: UniAudio is built on labeled datasets. Specifically, 12 datasets are adopted in this
work, all of which are publicly available. The overall audio volume is 165K hours. Detailed data
statistics and their adoption for each task are in Appendix A.1. Discrete tokens from all modalities
5

--- PAGE 6 ---
Under review as a conference paper at ICLR 2024
Table 2: Performance evaluation for UniAudio and selected prior works in the training stage
Task ModelObjective Evaluation Subjective Evaluation
Metrics Results Metrics Results
Text-to-SpeechShen et al. (2023)SIM(↑)/ WER (↓)0.62 / 2.3 MOS (↑)
/ SMOS (↑)3.83±0.10 / 3.11±0.10
UniAudio 0.71 / 2.0 3.81±0.07 / 3.56±0.10
V oice
ConversionWang et al. (2023e)SIM(↑)/ WER (↓)0.82 / 4.9 MOS (↑)
/ SMOS (↑)3.41±0.08 / 3.17 ±0.09
UniAudio 0.87 / 4.8 3.54 ±0.07 / 3.56 ±0.07
Speech
EnhancementRichter et al. (2023) PESQ (↑)/ VISQOL (↑)
/ DNSMOS (↑)3.21 / 2.72 / 3.29MOS (↑)3.56±0.08
UniAudio 2.63 / 2.44 / 3.66 3.68 ±0.07
Target Speaker
ExtractionWang et al. (2018) PESQ (↑)/ VISQOL (↑)
/ DNSMOS (↑)2.41 / 2.36 / 3.35MOS (↑)3.43±0.09
UniAudio 1.88 / 1.68 / 3.96 3.72 ±0.06
Singing V oice
SynthesisLiu et al. (2022)- -MOS (↑)
/ SMOS (↑)3.94±0.02 / 4.05±0.06
UniAudio 4.08 ±0.04 / 4.04 ±0.05
Text-to-SoundLiu et al. (2023a)FAD (↓)/ KL (↓)4.93 / 2.6 OVL (↑)
/ REL (↑)61.0±1.9 / 65.7 ±1.8
UniAudio 3.12 / 2.6 61.9 ±1.9 / 66.1 ±1.5
Text-to-MusicCopet et al. (2023)FAD (↓)/ KL (↓)4.52 / 1.4 OVL (↑)
/ REL (↑)73.3±1.5 / 71.3 ±1.7
UniAudio 3.65 / 1.9 67.9 ±1.7 / 70.0 ±1.5
form a joint vocabulary of size 4212, including all special tokens. Vanilla Transformer decoder layers
with causality are consistently adopted in global and local Transformer. The overall parameter budget
is roughly 1B. Detailed model configuration is in Appendix A.2. Existing neural codec models are
sub-optimal for universal audio generation, mainly due to data coverage. An improved neural codec
model is then built with fewer quantization levels nq, smaller frame-per-second rate, higher quality,
and wider coverage (see Appendix E).
Training and Inference: The training stage includes 7 tasks while 4 new tasks are added in the
fine-tuning stage. Table 1 specifies the tasks for fine-tuning only. Both the training and fine-tuning
are completed with 16 AMD MI200-64G GPUs. The detailed configuration of optimization is in
Appendix A.3. To retain the performance of previous tasks during fine-tuning, following Conneau
et al. (2020), the training data are re-sampled with respect to tasks with α= 0.05. Top-k sampling is
adopted consistently for inference, in which kand the temperature are set to 30 and 0.8, respectively.
As the global Transformer does not directly predict tokens, the sampling process only happens in the
local Transformer inference.
Evaluation: For evaluation, most tasks are evaluated using both objective and subjective metrics5.
Generally, for objective evaluation, Word Error Rate (WER) is used to evaluate the intelligibility of
generated speech; Similarity Score (SIM) is for similarity in terms of speaker identity6; Perceptual
Evaluation of Speech Quality (PESQ), VISQOL7, DNSMOS8and Mel Cepstral Distortion (MCD)
are signal-level quality metrics derived from human auditory research; Following (Copet et al., 2023),
Fréchet Audio Distance (FAD), Kullback-Leiber (KL) Divergence, and Fréchet Distance (FD) are for
audio fidelity and audio similarity; For subjective evaluation, MOS and SMOS are adopted to provide
human-centric judgment for speech and sing related tasks. For text-to-sound and text-to-music tasks,
we use overall quality (OVL), and relevance to the text input (REL) (Copet et al., 2023). Note all
subjective results are obtained from Amazon Mechanical Turk9for fair comparison. Appendix F
shows details of the subjective evaluation process.
3.2 T HERESULTS OF 7GENERATIVE TASKS IN THE TRAINING STAGE
This section presents the overall evaluation results of the proposed UniAudio model over all 7 audio
generation tasks during the training stage. A comprehensive comparison is conducted between
UniAuduio and multiple prior works on each task, including not only the LM-based methods but also
the diffusion model-based methods as well as other conventional audio generation methods. The
detailed comparison is presented in Appendix B. We selected one of the most advanced prior work in
each task and present the results in Table 2.
5Following the setting of DiffSinger (Liu et al., 2022), SVS tasks don’t report the objective results
6WER and SIM evaluation models follow Wang et al. (2023a)
7https://github.com/google/visqol
8https://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS
9https://www.mturk.com/
6

--- PAGE 7 ---
Under review as a conference paper at ICLR 2024
As suggested in Table 2, UniAudio is a versatile system that can handle all 7 audio generation tasks
together and achieve competitive performance. Per subjective evaluation, UniAudio surpasses the
baselines in 3 out of 6 tasks (TTS, VC, Sound); per objective evaluation, it achieves better results
on 5 out of the 7 tasks except SVS and Music. We also find UniAudio under-perform on several
metrics. UniAudio’s subjective performance for SE and TSE is less competitive compared with its
competitors, which is also observed in previous literature (Erdogan et al., 2023) that the signal-level
evaluation metrics may not be suitable for LM-based generative methods. UniAudio cannot surpass
the selected competitor (Copet et al., 2023) in the Text-to-Music task. We note that (Copet et al.,
2023) is built with more private labeled data than our UniAudio.
3.3 T HERESULTS OF 4GENERATIVE TASKS IN THE FINE -TUNING STAGE
Table 3: Performance evaluation for UniAudio and selected prior works in the fine-tuning stage
Task ModelEvaluation
Metrics Results
Audio EditAUDIT (Wang et al., 2023d)FD(↓)/ KL (↓)20.78 / 0.86
UniAudio 17.78 / 0.77
Speech Dereverb.SGMSE+ Richter et al. (2023)PESQ (↑)/ DNSMOS (↑)2.87 / 3.42
UniAudio 2.13 / 3.51
Instructed TTSGroundTruthMOS (↑)/ SMOS (↑)3.77±0.07 /3.85±0.08
UniAudio 3.61 ±0.09 / 3.71 ±0.09
Speech EditTTS system regenerationMCD(↓) / MOS (↑)6.98 / 3.69 ±0.08
UniAudio 5.12 /3.82±0.06
As UniAudio is designed to continuously support new audio generation tasks, this section reports
UniAudio’s performance on unseen tasks. The model is obtained by fine-tuning over 4 new tasks
jointly and the results are presented in Table 3. Similar to section 3.2, for each task, we compare
UniAudio’s performance with one selected prior work and report the detailed results in Appendix B.
As shown in Table 3, the fine-tuned UniAudio model surpasses its baselines in audio edit and speech
dereverberation and is approaching the ground-truth quality in the Instructed TTS task. For speech
editing, UniAudio shows considerable improvement compared to generating the whole sentence.
3.4 A BLATION STUDY
3.4.1 B ENEFIT OF BUILDING UNIFIED AUDIO GENERATION MODEL
To further validate our claim that building a unified model for all 11 audio generation tasks is
promising and beneficial, more ablation studies are conducted. In Appendix C.1, we demonstrate
that the joint-trained UniAudio model consistently outperforms the models that are trained for each
specific task10, regardless they are included in the training stage or the fine-tuning stage. In Appendix
C.2, we additionally validate that fine-tuning over the 4 new audio generation tasks does not affect
UniAudio’s performance on the original 7 tasks. In Appendix C.3, we observe that UniAudio can
consistently benefit from increased training data volume of each task, which provides another reason
to build universal audio generation models: these models are easier to scale up as the data collection
is more feasible. We provide more discussion in Appendix D about the effectiveness of building a
universal audio generation model.
3.4.2 T HE EFFECTIVENESS OF MULTI -SCALE TRANSFORMER MODEL
As in section 2.3, the adoption of neural codecs has become a popular choice of LLM-based audio
generation but causes an overly long sequence issue that needs further consideration. This section
compares the proposed multi-scale Transformer with four representative approaches in this field:
Flattening Prediction ( e.g. SPEARTTS (Kharitonov et al., 2023)), Coarse first prediction ( e.g.
V ALL-E (Wang et al., 2023a)), Parallel prediction ( e.g.AudioGen (Kreuk et al., 2022)), and Delay
prediction ( e.g.MusicGen (Copet et al., 2023)). Figure 2 illustrates the prediction order of these five
architectures. Experiments are conducted on text-to-speech and text-to-music tasks and the results
are reported in Table 4 and 5 respectively11.
Auto-Regression and Performance: Among all 4 baselines aforementioned, Copet et al. (2023)
claims that the flattening method provides the best audio generation quality. they further claim that
10Note the task-specific models are built with the corresponding subset of the training data.
11Results are based on unofficial implementations.
7

--- PAGE 8 ---
Under review as a conference paper at ICLR 2024
Coarse ﬁrst Parallel1
1
Multi-scale T ransformer (Ours)Delay0
0
10
3
2
16
5
49
8
7 ABC
E
D
FlatteningD
Figure 2: Order of token prediction for 4 representative methods in audio generation (Copet et al.,
2023) and the proposed multi-scale Transformer. Assume nq= 3 andT= 3. Current token
prediction (red) is conditioned on prior tokens (in green). Tokens in orange are concurrently predicted
with the current token. 0 is a special token indicating empty positions in the delay prediction.
Table 4: Model comparison among Coarse first, Flattening, Parallel, delay prediction, and multi-scale
Transformer. Experiments were conducted on the LibriTTS. GPU memory and training time are
obtained by a 20-second audio (average of 100 trials). All models have a similar parameter budget.
Structure nq MOS ( ↑) MCD ( ↓) GPU Mem. (GB) Time (s) / Iter.
Coarse first 8 3.48 ±0.05 7.37 18.7 0.58
Parallel 3 3.14 ±0.07 7.89 13.56 0.53
Delay 3 3.48 ±0.05 6.95 13.65 0.59
Flattening 3 3.80 ±0.09 6.56 36.7 1.63
Multi-Scale Transformer (ours) 3 3.77 ±0.05 6.52 19.4 0.73
Multi-Scale Transformer (ours) 8 3.84 ±0.06 6.27 24.0 1.10
the superior performance of flattening prediction is mainly attributed to the auto-regressive property;
the other three methods do not reserve this property as the concurrent prediction is introduced
(see Fig. 2). Under the scenario of codec adoption, we reinterpret the auto-regressive property
as: current token prediction is based on all tokens of previous frames and the previous tokens
within the current frame, or formally, the prediction of the current token zk
tis based on tokens:
{zk′
t′|t′< t} ∪ {zk′
t′|t′=t, k′< k}. With this definition, we claim that the proposed multi-scale
transformer is also auto-regressive.
Aligned with Copet et al. (2023), our experiments also validate the importance of the auto-regressive
property. As in Table 4 and 5, flattening prediction brings better generation quality than parallel,
coarse first, and delay prediction. Additionally, with the same auto-regressive property, our proposed
multi-scale transformer achieves a comparable performance with flattening prediction in terms of
generation quality, which, again, validates the importance of auto-regression.
Efficiency: Besides generation quality, efficiency is a major concern of audio generation. Although
with the auto-regressive property, the flattening prediction is sub-optimal in terms of efficiency: the
modeling is based on the T×nqlong sequence, which has a space complexity of O((T∗nq)2)in
self-attention. As increasing nqgives higher reconstruction quality at the cost of longer sequences and
more computation, this issue becomes more severe when a larger nqis adopted. Since the sequence
length grows proportionally with nq, we experimentally find it difficult to train with nq≥4. By
contrast, the proposed multi-scale Transformer distributes the inter- and intra-frame modeling to the
global and local sub-modules respectively, which thus alleviates the space complexity to O(T∗2).
Finally, without the requirement of auto-regression, methods like parallel, coarse first, and delay
predictions achieve better efficiency due to the adoption of concurrent predictions. Since the space
complexity is independent to nq, training a larger nqwith the multi-scale transformer is then feasible.
8

--- PAGE 9 ---
Under review as a conference paper at ICLR 2024
Table 5: The ablation study to explore the effectiveness of our proposed multi-scale transformer.
Experiments were conducted on text-to-music tasks with the Million Song dataset.
Structure nqFAD (↓) KL ( ↓)OVL. ( ↑) REL. ( ↑)
Parallel 3 6.92 2.36 60.4±2.3 61.3 ±1.5
Delay 3 6.07 2.23 62.8±1.9 63.9 ±1.6
Flatten 3 5.18 1.83 64.8±1.8 65.2 ±2.0
Multi-Scale Transformer (ours) 3 5.24 1.80 64.4±2.1 66.2 ±2.4
Experimentally, the proposed multi-scale transformer considerably reduces the time and memory cost
compared with the flatting prediction. It still costs more time and memory compared with the other
three baselines.
Based on the observations above, we claim that the proposed multi-scale transformer is an auto-
regressive architecture that achieves a better trade-off between generation quality and efficiency.
4 R ELATED WORKS
This work is an attempt to achieve universal audio generation through LLM-based techniques. There
is a long research history for many audio generation tasks. Conventionally, the design of these tasks
heavily leverages the domain knowledge of each specific task, and their workflows are distinctive
from each other: For tasks like TTS, SE, TSE, TT-Music, VC, S-Edit, SD, SVS, (1) their neural
network architectures are based on Transformer (Ren et al., 2020) or others (Oord et al., 2016; Luo &
Mesgarani, 2019); (2) their training objectives can be either in time-domain (Luo & Mesgarani, 2019),
frequency-domain (Yu et al., 2017) or others (Gu et al., 2021; Shen et al., 2023); (3) their designs are
inspired by and derived from linguistics and phonetics (Zen et al., 2013), signal processing (Griffin &
Lim, 1984), auditory perception (Shadle & Damper, 2001) and machine learning (Wang et al., 2016)
research, etc; (4) they use different generative models, such as diffusion model (Shen et al., 2023;
Wang et al., 2023b), flow (Le et al., 2023), Seq2Seq (Ren et al., 2020; Liu et al., 2021).
The prosperity of LLM techniques (Radford et al., 2019; OpenAI, 2023) significantly promotes
progress in audio generation research in several directions. First, the large language models, along
with the prompt methods, inspired multiple emergent audio generation tasks that are based on textual
instruction or descriptions from humans, such as Instruct-TTS (Yang et al., 2023a), Text-to-sound
(Kreuk et al., 2022; Huang et al., 2023a) and text-to-music Copet et al. (2023); Agostinelli et al.
(2023). Second, besides the text, audio can also be tokenized as discrete sequences (Zeghidour et al.,
2021; Défossez et al., 2022; Kumar et al., 2023) that can be further processed by LMs. LM-based
audio generative models then show superior capability in generalization towards unseen speakers
(Wang et al., 2023a), low resources (Kharitonov et al., 2023) and multilingual (Zhang et al., 2023)
scenarios. These methods also achieve state-of-the-art results in overall performance within their own
scopes. Finally, the LM-like model can be further combined with existing generative models (e.g.,
diffusion models Rombach et al. (2022)) to obtain improved generation quality.
It is laborious to handle each audio generation task case-by-case, especially when considering the data
shortage as well as the emergent and varying needs in this area. Alternatively, building a universal
audio generation model is a promising and practical paradigm. Given the rapid progress in audio
generation research, recent designs of audio generation, including LM-based ones, tend to support
multiple audio generation tasks simultaneously. Some pioneer works (Wang et al., 2023c; Le et al.,
2023; Shen et al., 2023; Liu et al., 2023b; Jiang et al., 2023) clearly consider supporting multiple
tasks as a key strength; the designs of other prior works (Borsos et al., 2023; Kharitonov et al., 2023;
Shen et al., 2023) do show the potential to generate audio in a broader sense than what they originally
claim. Following these pioneering research works, UniAudio supports an extended coverage of 11
audio generation tasks in a unified LM-based model.
5 L IMITATION
Not all known audio generation tasks are included in the proposed UniAudio, such as noise removal,
noise speech edit (Wang et al., 2023c) and speech-to-speech translation (Rubenstein et al., 2023;
Barrault et al., 2023). All new tasks added in fine-tuning are formulated with the known modalities
in the training stage; Introducing new modalities during fine-tuning is unexplored in this work.
Current UniAudio considers neither unlabeled data nor domain-specific foundation models, which
9

--- PAGE 10 ---
Under review as a conference paper at ICLR 2024
can possibly further improve the overall performance. The samples generated by UniAudio are not
guaranteed in quality and may contain errors.
6 C ONCLUSION
To handle the emergent and varying needs in audio generation, this work is an attempt to achieve
universal audio generation. UniAudio is proposed as a unified LM-based generative model that
supports 11 different audio generation tasks. In experiments, the proposed UniAudio provides com-
petitive performance on all 11 tasks. It also empirically demonstrates the capability of continuously
integrating unseen audio generation tasks. Demo and code are released, in the hope that UniAudio
can become a foundation model for universal audio generation in further research.
7 E THICAL STATEMENT
We are delving into the revolutionary field of generating diverse audio using large language model
techniques. We find ourselves at the confluence of innovation and responsibility. It is imperative to
acknowledge the ethical dimensions of our work and ensure that our contributions are employed for
the betterment of society.
Being Open: As we advance in this domain, it’s crucial to ensure that the benefits of this technology
are widespread and not limited to a privileged few. Our code is released publicly along with this
submission to ensure equal access for each person. All experiments are based on open-accessible
datasets that allow research-oriented comparison and reproduction.
Avoid Misuse: While our model can produce a myriad of audio content ranging from music to
speech, there’s potential for misuse in the generation of misinformation, deepfake audio, or any
harmful content. We advocate for adopting our code and model responsibly, with full respect to
individual privacy and observance of regulations. Concerning the potential misuse of our model,
checkpoints will not be released.
REFERENCES
Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,
Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating
music from text. arXiv preprint arXiv:2301.11325 , 2023.
Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise
Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. Seamlessm4t-
massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596 ,
2023.
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,
Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a language
modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language
Processing , 2023.
Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Gölge, and
Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for
everyone. In International Conference on Machine Learning , pp. 2709–2720. PMLR, 2022.
Jun Chen, Zilin Wang, Deyi Tuo, Zhiyong Wu, Shiyin Kang, and Helen Meng. Fullsubnet+: Channel
attention fullsubnet with complex spectrograms for speech enhancement. In ICASSP 2022-2022
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 7857–
7861. IEEE, 2022.
Yin-Ping Cho, Fu-Rong Yang, Yung-Chuan Chang, Ching-Ting Cheng, Xiao-Han Wang, and Yi-Wen
Liu. A survey on recent deep learning-driven singing voice synthesis systems. In 2021 IEEE
International Conference on Artificial Intelligence and Virtual Reality (AIVR) , pp. 319–323. IEEE,
2021.
Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.
Unsupervised cross-lingual representation learning for speech recognition. arXiv preprint
arXiv:2006.13979 , 2020.
10

--- PAGE 11 ---
Under review as a conference paper at ICLR 2024
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and
Alexandre Défossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284 ,
2023.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio
compression. arXiv preprint arXiv:2210.13438 , 2022.
Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset.
InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 736–740. IEEE, 2020.
Hakan Erdogan, Scott Wisdom, Xuankai Chang, Zalán Borsos, Marco Tagliasacchi, Neil Zeghidour,
and John R Hershey. Tokensplit: Using discrete speech representations for direct, refined, and
transcript-conditioned speech separation and recognition. arXiv preprint arXiv:2308.10415 , 2023.
Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for
audio events. In 2017 IEEE international conference on acoustics, speech and signal processing
(ICASSP) , pp. 776–780. IEEE, 2017.
Daniel Griffin and Jae Lim. Signal estimation from modified short-time fourier transform. IEEE
Transactions on acoustics, speech, and signal processing , 32(2):236–243, 1984.
Rongzhi Gu, Shi-Xiong Zhang, Yuexian Zou, and Dong Yu. Complex neural spatial filter: Enhancing
multi-channel target speech separation in complex domain. IEEE Signal Processing Letters , 28:
1370–1374, 2021.
Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. Prompttts: Controllable text-to-
speech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pp. 1–5. IEEE, 2023.
Xiang Hao, Xiangdong Su, Radu Horaud, and Xiaofei Li. Fullsubnet: A full-band and sub-band
fusion model for real-time single-channel speech enhancement. In ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 6633–6637.
IEEE, 2021.
Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury. Deep
neural networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Processing Magazine , 29(6):82–97, 2012. doi: 10.1109/MSP.2012.2205597.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
29:3451–3460, 2021.
Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Multi-singer: Fast
multi-singer singing voice vocoder with a large-scale corpus. In Proceedings of the 29th ACM
International Conference on Multimedia , pp. 3945–3954, 2021a.
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin
Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced
diffusion models. arXiv preprint arXiv:2301.12661 , 2023a.
Rongjie Huang, Chunlei Zhang, Yongqi Wang, Dongchao Yang, Luping Liu, Zhenhui Ye, Ziyue
Jiang, Chao Weng, Zhou Zhao, and Dong Yu. Make-a-voice: Unified voice synthesis with discrete
representation. arXiv preprint arXiv:2305.19269 , 2023b.
Tzu-hsien Huang, Jheng-hao Lin, and Hung-yi Lee. How far are we from robust voice conversion: A
survey. In 2021 IEEE Spoken Language Technology Workshop (SLT) , pp. 514–521. IEEE, 2021b.
Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie
Huang, Chunfeng Wang, Xiang Yin, et al. Mega-tts: Zero-shot text-to-speech at scale with intrinsic
inductive bias. arXiv preprint arXiv:2306.03509 , 2023.
11

--- PAGE 12 ---
Under review as a conference paper at ICLR 2024
Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel
Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light:
A benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 7669–7673. IEEE, 2020.
Eugene Kharitonov, Damien Vincent, Zalán Borsos, Raphaël Marinier, Sertan Girgin, Olivier Pietquin,
Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity
text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540 , 2023.
Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating
captions for audios in the wild. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume
1 (Long and Short Papers) , pp. 119–132, 2019.
Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L Seltzer, and Sanjeev Khudanpur. A study on
data augmentation of reverberant speech for robust speech recognition. In 2017 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 5220–5224. IEEE, 2017.
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi
Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv
preprint arXiv:2209.15352 , 2022.
Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-
fidelity audio compression with improved RVQGAN. In Thirty-seventh Conference on Neural
Information Processing Systems , 2023. URL https://openreview.net/forum?id=
qjnl1QUnFA .
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson,
Vimal Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. V oicebox: Text-guided multi-
lingual universal speech generation at scale. 2023. URL https://dl.fbaipublicfiles.
com/voicebox/paper.pdf .
Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and
Mark D Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv
preprint arXiv:2301.12503 , 2023a.
Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu
Wang, Yuxuan Wang, and Mark D. Plumbley. AudioLDM 2: Learning holistic audio generation
with self-supervised pretraining. arXiv preprint arXiv:2308.05734 , 2023b.
Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis
via shallow diffusion mechanism. In Proceedings of the AAAI conference on artificial intelligence ,
volume 36, pp. 11020–11028, 2022.
Songxiang Liu, Yuewen Cao, Disong Wang, Xixin Wu, Xunying Liu, and Helen Meng. Any-to-many
voice conversion with location-relative sequence-to-sequence modeling. IEEE/ACM Transactions
on Audio, Speech, and Language Processing , 29:1717–1728, 2021.
Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, and Yu Tsao. Con-
ditional diffusion probabilistic model for speech enhancement. In ICASSP 2022-2022 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 7402–7406.
IEEE, 2022.
Yi Luo and Nima Mesgarani. Conv-tasnet: Surpassing ideal time–frequency magnitude masking for
speech separation. IEEE/ACM transactions on audio, speech, and language processing , 27(8):
1256–1266, 2019.
Brian McFee, Thierry Bertin-Mahieux, Daniel PW Ellis, and Gert RG Lanckriet. The million song
dataset challenge. In Proceedings of the 21st International Conference on World Wide Web , pp.
909–916, 2012.
Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley,
Yuexian Zou, and Wenwu Wang. Wavcaps: A chatgpt-assisted weakly-labelled audio captioning
dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395 , 2023.
12

--- PAGE 13 ---
Under review as a conference paper at ICLR 2024
Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, Emmanuel
Vincent, Bhiksha Raj, and Tuomas Virtanen. Dcase 2017 challenge setup: Tasks, datasets and
baseline system. In DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes
and Events , 2017.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499 , 2016.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2204.06125 , 2023.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus
based on public domain audio books. In 2015 IEEE international conference on acoustics, speech
and signal processing (ICASSP) , pp. 5206–5210. IEEE, 2015.
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and
Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.
arXiv preprint arXiv:1904.08779 , 2019.
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A
large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411 , 2020.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast
and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558 , 2020.
Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, and Timo Gerkmann. Speech
enhancement and dereverberation with diffusion-based generative models. IEEE/ACM Transactions
on Audio, Speech, and Language Processing , 2023.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition , pp. 10684–10695, 2022.
Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos,
Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.
Audiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925 ,
2023.
Christine H Shadle and Robert I Damper. Prospects for articulatory synthesis: A position paper. In
4th ISCA tutorial and research workshop (ITRW) on speech synthesis , 2001.
Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang
Bian. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing
synthesizers. arXiv preprint arXiv:2304.09116 , 2023.
Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming Li. Aishell-3: A multi-speaker mandarin tts
corpus and the baselines. arXiv preprint arXiv:2010.11567 , 2020.
Jaesung Tae, Hyeongju Kim, and Taesu Kim. Editts: Score-based editing for controllable text-to-
speech. arXiv preprint arXiv:2110.02584 , 2021.
Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. A survey on neural speech synthesis. arXiv preprint
arXiv:2106.15561 , 2021.
13

--- PAGE 14 ---
Under review as a conference paper at ICLR 2024
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
systems , 30, 2017.
Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Cstr vctk corpus: English multi-
speaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre for Speech
Technology Research (CSTR) , 6:15, 2017.
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech
synthesizers. arXiv preprint arXiv:2301.02111 , 2023a.
Quan Wang, Hannah Muckenhirn, Kevin Wilson, Prashant Sridhar, Zelin Wu, John Hershey, Rif A
Saurous, Ron J Weiss, Ye Jia, and Ignacio Lopez Moreno. V oicefilter: Targeted voice separation
by speaker-conditioned spectrogram masking. arXiv preprint arXiv:1810.04826 , 2018.
Wen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, and Yuexian Zou. Nadiffuse: Noise-aware
diffusion-based model for speech enhancement. arXiv preprint arXiv:2309.01212 , 2023b.
Wenfu Wang, Shuang Xu, Bo Xu, et al. First step towards end-to-end parametric tts synthesis:
Generating spectral parameters with neural attention. In Interspeech , pp. 2243–2247, 2016.
Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen,
Min Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as
a versatile speech transformer. arXiv preprint arXiv:2308.06873 , 2023c.
Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei
Xie, and Mengxiao Bi. Opencpop: A high-quality open source chinese popular song corpus for
singing voice synthesis. arXiv preprint arXiv:2201.07429 , 2022.
Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, and Sheng Zhao. Audit: Au-
dio editing by following instructions with latent diffusion models. arXiv preprint arXiv:2304.00830 ,
2023d.
Zhichao Wang, Yuanzhe Chen, Lei Xie, Qiao Tian, and Yuping Wang. Lm-vc: Zero-shot voice
conversion via speech generation based on language models. arXiv preprint arXiv:2306.10521 ,
2023e.
Bo Wu, Kehuang Li, Minglei Yang, and Chin-Hui Lee. A reverberation-time-aware approach to
speech dereverberation based on deep neural networks. IEEE/ACM transactions on audio, speech,
and language processing , 25(1):102–111, 2016.
Dongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, and Dong
Yu. Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt.
arXiv preprint arXiv:2301.13662 , 2023a.
Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou.
Hifi-codec: Group-residual vector quantization for high fidelity audio codec. arXiv preprint
arXiv:2305.02765 , 2023b.
Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.
Diffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on
Audio, Speech, and Language Processing , 2023c.
Dong Yu, Morten Kolbæk, Zheng-Hua Tan, and Jesper Jensen. Permutation invariant training of
deep models for speaker-independent multi-talker speech separation. In 2017 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 241–245. IEEE, 2017.
Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.
Megabyte: Predicting million-byte sequences with multiscale transformers. arXiv preprint
arXiv:2305.07185 , 2023.
14

--- PAGE 15 ---
Under review as a conference paper at ICLR 2024
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-
stream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and
Language Processing , 30:495–507, 2021.
Heiga Zen, Andrew Senior, and Mike Schuster. Statistical parametric speech synthesis using deep
neural networks. In 2013 ieee international conference on acoustics, speech and signal processing ,
pp. 7962–7966. IEEE, 2013.
Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu.
Libritts: A corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882 ,
2019.
Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie
Huang, Jieming Zhu, Xiao Chen, et al. M4singer: A multi-style, multi-singer and musical score
provided mandarin singing corpus. Advances in Neural Information Processing Systems , 35:
6914–6926, 2022.
Ziqiang Zhang, Long Zhou, Chengyi Wang, Sanyuan Chen, Yu Wu, Shujie Liu, Zhuo Chen, Yanqing
Liu, Huaming Wang, Jinyu Li, et al. Speak foreign languages with your own voice: Cross-lingual
neural codec language modeling. arXiv preprint arXiv:2303.03926 , 2023.
Kate ˇrina Žmolíková, Marc Delcroix, Keisuke Kinoshita, Tsubasa Ochiai, Tomohiro Nakatani, Lukáš
Burget, and Jan ˇCernock `y. Speakerbeam: Speaker aware neural network for target speaker
extraction in speech mixtures. IEEE Journal of Selected Topics in Signal Processing , 13(4):
800–814, 2019.
Katerina Zmolikova, Marc Delcroix, Tsubasa Ochiai, Keisuke Kinoshita, Jan ˇCernock `y, and Dong
Yu. Neural target speech extraction: An overview. IEEE Signal Processing Magazine , 40(3):8–29,
2023.
15

--- PAGE 16 ---
Under review as a conference paper at ICLR 2024
Appendices
A E XPERIMENTAL SETUP
This appendix describes experimental setups in detail, including data statistics, model architecture
and optimization strategy.
A.1 D ATA DESCRIPTION
12 public datasets are adopted in this work for training. Besides, several test sets are additionally used
only for zero-shot evaluation. The statistics of these datasets are in Table 6. Datasets adoption for
each task is described in Table 7. Note some datasets are adopted by more than one task.
Table 6: Data statistics
Dataset Type Annotation V olume (hrs)
Training
LibriLight (Kahn et al., 2020) speech - 60k
LibriTTS (Zen et al., 2019) speech text 1k
MLS (Pratap et al., 2020) speech - 20k
AudioSet (Gemmeke et al., 2017) sound - 5.8k
AudioCaps (Kim et al., 2019) sound text description 500
WavCaps (Mei et al., 2023) sound text description 7k
Million Song Dataset (McFee et al., 2012) music text description 7k
OpenCPOP (Wang et al., 2022) singing text, MIDI 5.2
OpenSinger (Huang et al., 2021a) singing text, MIDI 50
AISHELL3 (Shi et al., 2020) speech text 85
PromptSpeech (Guo et al., 2023) speech text, instruction 200
openSLR26,openSLR28 (Ko et al., 2017) room impulse response - 100
Test
LibriSpeech test-clean Panayotov et al. (2015) speech text 8
VCTK (Veaux et al., 2017) speech text 50
TUT2017 Task1 (Mesaros et al., 2017) Noise - 10
Cloth (Drossos et al., 2020) Sound text description 3
MusicCaps (Agostinelli et al., 2023) Music text description 15
M4Singer(Zhang et al., 2022) singing text, MIDI 1
Table 7: Dataset adoption of all tasks
Task Training dataset Test set Train V olume (hrs)
Training Stage
TTS Librilight LibriSpeech clean-test 60k
VC Librilight VCTK 60k
SE MLS, Audioset TUT2017 Task1, VCTK 20k
TSE MLS Libri2Mix test set 10k
Sound AudioCaps, WavCaps Cloth test set 7k
Music MSD MusicCaps 7k
Singing OpenCPOP, OPenSinger, AISHEELL-3 M4Singer test set 150
Fine-Tuning Stage
I-TTS PromptSpeech PromptSpeech test set 200
Speech dereverberation LibriTTS, openSLR26, openSLR28 LibriTTS test set 100
Speech edit LibriTTS LibriTTS test set 100
Audio edit AudioCaps, WavCaps AudioCaps test set 500
Sum - - 166k
A.2 M ODEL CONFIGURATION
The model configuration of the proposed multi-scale Transformer is described in Table 8.
16

--- PAGE 17 ---
Under review as a conference paper at ICLR 2024
Table 8: Model configuration (with nq= 3)
Hyper-parameter Global Transforemr Local Transformer
#layer 24 8
#Attention dim 1536 1536
#Attention head 12 12
#Feed-Forward dim 6144 6144
#Params (M) 744 238
Max context length (in #tokens) 3,000 3
Causality Yes Yes
A.3 O PTIMIZATION
The optimization configurations adopted in both the training and fine-tuning stages are presented in
Table 9
Table 9: Optimization Configuration
Hyper-parameter Pre-training Fine-Tuning
Batch Size (#patches/GPU) 8k 8k
Peak Learning Rate 1e-4 1e-5
Warm-up Steps 10000 1000
Training Steps 800k 50k
Learning rate decay Noam (Vaswani et al., 2017) Noam (Vaswani et al., 2017)
B T HEDETAILS OF EXPERIMENTS
This section presents detailed experimental results on each task. In the following, if the training set
and test sets come from different datasets, we label them as zero-shot settings.
B.1 TTS AND VC TASKS
For TTS tasks, UniAudio is compared with the many previous SOTA models, Table 10 presents the
results. For FastSpeech 2, we only conduct QMOS evaluation as its implementation adopts speaker
id as input12. We can see that UniAudio obtains better performance in terms of WER, SIM than
YourTTS, V ALL-E, NaturalSpeech 2 and Make-A-V oice. Compared with V oiceBox, UniAudio also
gets comparable performance in terms of objective metrics. From the MOS evaluation, we can see
that UniAudio can generate high-quality speech compared with previous SOTA works. Furthermore,
UniAudio realizes the best zero-shot clone ability ( e.g. SMOS is 3.56 and SIM is 0.708). More
experiments, such as cross-lingual zero-shot TTS and Mandarin Chinese speech synthesis can be
found in demo page. For VC task, we conducted experiments on VCTK dataset, we randomly chose
200 audio pairs. PPG-VC and YourTTS are trained on small-scale datasets. Make-A-V oice and
LM-VC13are trained on large-scale datasets as the same as UniAudio. Compared with previous
work, UniAudio got better performance in voice conversion tasks.
B.2 S PEECH ENHANCEMENT AND TARGET SPEAKER EXTRACTION
For the SE task, we compare with previous SOTA methods, including discriminative methods (such as
FullSubNet and FullSubNet+) and generative methods (such as SGMSE+ and NADiffuSE). Note that
the CDiffuSE and NADiffuSE are both trained on the voicebank-demand dataset. Other models never
saw the VCTK dataset in the training stage. We obtain the inference results based on their open-source
models. Table 11 presents the results, we can see that UniAuido obtains the best DNSMOS score.
The PESQ and VISQOL scores are lower than other SOTA methods, we think these metrics may not
12https://github.com/ming024/FastSpeech2
13We seek help from the authors, they provide the inference results.
17

--- PAGE 18 ---
Under review as a conference paper at ICLR 2024
Table 10: The performance comparison with previous SOTA methods in TTS and VC tasks. We
do not conduct MOS evaluation for V ALL-E, SPEARTTS and V oiceBox due to the models are not
released.
Model Zero-shot SIM ( ↑) WER ( ↓) MOS ( ↑) SMOS ( ↑)
Text-to-Speech
GroundTruth - - 1.9 3.99 ±0.08 -
FastSpeech 2 (Ren et al., 2020) ✗ - - 3.81 ±0.10 -
YourTTS (Casanova et al., 2022) ✓ 0.337 7.7 3.66 ±0.07 3.02 ±0.07
V ALL-E (Wang et al., 2023a) ✓ 0.580 5.9 - -
Make-A-V oice (TTS) (Huang et al., 2023b) ✓ 0.498 5.7 3.74 ±0.08 3.11 ±0.06
NaturalSpeech 2 (Shen et al., 2023) ✓ 0.620 2.3 3.83±0.10 3.11±0.10
SPEAR-TTS (Kharitonov et al., 2023) ✓ 0.560 / - -
V oiceBox (Le et al., 2023) ✓ 0.681 1.9 - -
UniAudio ✓ 0.708 2.0 3.81 ±0.07 3.56±0.10
Voice Conversion
GroundTruth - - 3.25 3.74 ±0.08 -
PPG-VC (Liu et al., 2021) ✗ 0.78 12.3 3.41 ±0.10 3.47 ±0.10
YourTTS (Casanova et al., 2022) ✓ 0.719 10.1 3.61 ±0.10 3.26 ±0.10
Make-A-V oice (VC) (Huang et al., 2023b) ✓ 0.678 6.2 3.43 ±0.09 3.47 ±0.10
LM-VC (Wang et al., 2023e) ✓ 0.820 4.91 3.41 ±0.08 3.17 ±0.09
UniAudio ✓ 0.868 4.8 3.54 ±0.07 3.56±0.07
accurately assess the performance of generative methods. The similar finding is also observed in
previous literature (Erdogan et al., 2023) that the signal-level evaluation metrics may not be suitable
for generative methods. In contrast, we recommend using DNSMOS and MOS scores as the main
metrics. UniAuido can get good results in extremely noisy environments, we recommend readers
refer to the demo page. For the TSE task, we conducted experiments on the LibriMix test set. The
popular TSE systems: V oiceFilter14and SpeakBeam15are used as baseline systems. As Table 11
shows, we can see that UniAudio obtains the best performance in terms of DNSMOS and MOS.
Table 11: The performance of SE and TSE tasks comparison with previous SOTA methods.
Model Zero-shot PESQ ( ↑) VISQOL( ↑) DNSMOS( ↑) MOS( ↑)
Speech Enhancement
CDiffuSE (Lu et al., 2022) ✗ 1.88 1.21 2.54 -
NADiffuSE (Wang et al., 2023b) ✗ 2.96 2.41 3.03 3.30 ±0.08
SGMSE+ (Richter et al., 2023) ✓ 3.21 2.72 3.29 3.56 ±0.08
FullSubNet (Hao et al., 2021) ✓ 3.21 2.77 3.37 3.61 ±0.10
FullSubNet+ (Chen et al., 2022) ✓ 3.41 2.99 3.34 3.42 ±0.08
UniAudio ✓ 2.63 2.44 3.66 3.68 ±0.07
Target Speaker Extraction
SpeakerBeam (Žmolíková et al., 2019) ✗ 2.89 2.25 3.18 3.68 ±0.1
V oiceFilter (Wang et al., 2018) ✗ 2.41 2.36 3.35 3.43 ±0.09
UniAudio ✓ 1.88 1.68 3.96 3.72 ±0.06
B.3 S INGING VOICE SYNTHESIS
Following Make-A-V oice, we conduct experiments on the M4Singer test set. We compare the
generated singing samples with other systems, including 1) Diffsinger; 2) Make-A-V oice, a two-stage
audio language model for singing voice generation. As illustrated in Table 12, we can see that
UniAudio gets comparable results with Make-A-V oice and Diffsinger.
B.4 T EXT-TO-SOUND AND TEXT -TO-MUSIC GENERATION
The text-to-sound generation task has attracted great interest in audio research. Following Diffsound
(Yang et al., 2023c), most of the methods evaluate their systems on the AudioCaps (Kim et al., 2019)
14https://github.com/Edresson/V oiceSplit
15https://github.com/BUTSpeechFIT/speakerbeam
18

--- PAGE 19 ---
Under review as a conference paper at ICLR 2024
Table 12: Quality and style similarity of generated samples in singing voice synthesis.
Model MOS ( ↑) SMOS ( ↑)
Diffsinger (Liu et al., 2022) 3.94 ±0.02 4.05±0.06
Make-A-V oice (Huang et al., 2023b) 3.96 ±0.03 4.04 ±0.05
UniAudio 4.08±0.04 4.04±0.05
test set. However, we found that if the training data includes the AudioCaps data, the model is easy to
overfit with AudioCaps. As a result, the best performance can be obtained when the model only trains
on the Audiocaps. In this study, we conduct a zero-shot evaluation on the Cloth test set (Drossos
et al., 2020). Table 13 shows the results. We can see that UniAudio obtains better performance than
Diffsound and AudioLDM. Compared to recent SOTA models, such as Tango and Make-an-Audio
2, UniAudio also gets comparable performance. For the text-to-music task, we follow MusicGen
(Copet et al., 2023), evaluating our methods on MusicCaps (Agostinelli et al., 2023). Compared
with previous SOTAs, UniAudio gets a comparable performance with other models. From the MOS
evaluation performance, we can see that MusicGen is better than our current models. We speculate
one of the reasons is that MusicGen uses a large-scale high-quality dataset (20k hours).
Table 13: Text-to-sound and text-to-music evaluation. We report the subjective metrics including
FAD(↓), and KL( ↓). Furthermore, we also conduct objective evaluation. Note that the training data
of AudioGen includes Cloth datatset, thus can not be seen as zero-shot setting.
Model Training Data (Hours) FAD KL OVL. REL.
Text-to-Sound Generation
Reference / / / 70.47±1.9 78.84 ±1.5
Diffsound 2k 7.8 6.53 - -
AudioGen 4k 2.55 2.5 63.84±2.1 72.12±1.8
Tango 3.3k 3.61 2.59 66.2±1.7 68.57±1.5
Make-an-Audio 2 8.7k 2.13 2.49 61.52±1.6 69.9 ±1.5
AudioLMD 9k 4.93 2.6 60.95±1.9 65.7 ±1.8
UniAudio 7k 3.12 2.57 61.9±1.9 66.1 ±1.5
Text-to-Music Generation
Riffusion - 14.8 2.06 - -
Mousai - 7.5 1.59 - -
MusicLM 280k 4.0 - - -
Noise2Music 280k 2.1 - - -
MusicGen 20k 4.52 1.41 73.28±1.5 71.28 ±1.7
UniAudio 8k 3.65 1.87 67.85±1.70 70.0 ±1.5
B.5 A UDIO EDIT
Audio edit aims to edit the original audio based on Human’s instruction. AUDIT (Wang et al.,
2023d) is the SOTA model in audio edit task, which designs a data simulation strategy to get triplet
training and test data ( e.g., {audio, audio, text}). The authors set 5 different tasks, including adding,
dropping, replacing, inpainting and super-resolution, and simulated large-scale data for each task.
To validate that our pre-trained model can be fine-tuned with small-scale data, we choose adding,
dropping and super-resolution tasks to fine-tune simultaneously. To finish the fine-tuning process, we
define a new task label: Audit_task . The experimental results as Table 14 shows. We can observe
that: (1) UniAudio can get better performance with the previous SOTA model. (2) Fine-tuning
pre-trained UniAudio can get better performance than training it from scratch, which further validates
the effectiveness of pre-training a model on large-scale training data.
B.6 I NSTRUCTED TTS
Using instruction to guide speech synthesis has received great attention (Guo et al., 2023; Yang et al.,
2023a). In this part, we fine-tune the UniAudio model on the PromptSpeech (Guo et al., 2023) dataset.
19

--- PAGE 20 ---
Under review as a conference paper at ICLR 2024
Table 14: Audio edit task evaluation.
Type Model FD KL
Adding task
AUDIT 21.80 0.92
UniAudio (scratch) 20.2 0.99
UniAudio (fine-tune) 19.69 0.934
Dropping task
AUDIT 22.40 0.95
UniAudio (scratch) 27.76 1.38
UniAudio (fine-tune) 23.1 1.10
Super-Resolution task
AUDIT 18.14 0.73
UniAudio (scratch) 11.51 0.29
UniAudio (fine-tune) 10.54 0.289
Table 15: Quality and style similarity of generated samples for Instructed TTS task.
Model MOS ( ↑) SMOS ( ↑)
GT 3.77 ±0.07 3.85 ±0.08
UniAudio (scratch) 3.62 ±0.07 3.67 ±0.08
UniAudio (tuning) 3.61 ±0.09 3.71 ±0.09
Furthermore, we also try to train a UniAudio model from scratch with the PromptSpeech dataset.
Different from previous works that designed special style encoders to capture the style information
from text descriptions, we directly use the T5 text encoder to extract representations from text and
then combine it with the phoneme sequence input to the UniAudio, which is more convenient.16
Table 15 shows the results, we can see that UniAudio has good performance in terms of style control
and speech quality when compared with the ground truth samples.
B.7 S PEECH DEREVERBERATION
For the speech dereverberation task, we use the Room Impulse Response (RIR) data from the
openSLR26 and openSLR28 dataset, and the speech data from the LibriTTS clean part. We simulate
about 100 hours of training data and 1 hour of test data. We compare with previous SOTA systems,
such as FullSubNet, FullSubNet+ and SGMSE+. Table 16 presents the results. We can see that
UniAudio obtains the SOTA performance in speech dereverberation tasks with small-scale training
data in terms of DNSMOS metric. Similar with speech enhancement task, we speculate that PESQ
may not suitable for the generative methods.
Table 16: Results comparison with previous speech Dereverberation systems.
Model PESQ ( ↑) DNSMOS( ↑)
SGMSE+ 2.87 3.42
FullSubNet 2.29 3.32
FullSubNet+ 2.27 3.25
UniAudio (scratch) 1.23 3.18
UniAudio (tuning) 2.13 3.51
16Note that the authors of PromptTTS (Guo et al., 2023) told us their objective metrics tools, checkpoints, and
generated samples have been lost due to the machine errors. Thus we cannot fairly compare with them.
20

--- PAGE 21 ---
Under review as a conference paper at ICLR 2024
B.8 S PEECH EDIT
For the speech edit task, we use the LibriTTS dataset. In practice, we randomly choose some words
to mask in the training stage. We expect the model to recover the whole speech based on the phoneme
sequence. In the inference stage, we can mask the region that we want to update in the speech and
input the new words so that the model can edit the speech. For this task, we take the TTS system
that regenerates a complete waveform from the whole sentence to be edited as the baseline. In the
evaluation, we mainly validate three situations: (1) word replacement; (2) insert a new word; and (3)
delete a word. For each situation, we randomly chose 10 sentences from the LibriTTS test clean set.
C A BLATION STUDY
C.1 T HE INFLUENCE OF MULTI -TASK TRAINING
In this part, we explore whether multi-task training can bring better performance than task-specific
training. To answer this question, we use the same model trained on different tasks, respectively.
Table 17 shows the experimental results, UniAudio (single) means that the model is trained on a
single task. We observe that multi-task training brings the gain over all of the tasks. In Appendix D,
we give some potential reasons why multi-task training can bring improvement.
Table 17: The ablation study of the effectiveness of multi-task training.
Task ModelObjective Evaluation Subjective Evaluation
Metrics Results Metrics Results
Text-to-SpeechUniAudio (Single)SIM(↑)/ WER (↓)0.64 / 2.4 MOS (↑)
/ SMOS (↑)3.77±0.06 / 3.46 ±0.10
UniAudio 0.71 / 2.0 3.81 ±0.07 /3.56±0.10
V oice
ConversionUniAudio (Single)SIM(↑)/ WER (↓)0.84 / 5.4 MOS (↑)
/ SMOS (↑)3.45±0.07 / 3.44 ±0.07
UniAudio 0.87 / 4.8 3.54 ±0.07 / 3.56 ±0.07
Speech
EnhancementUniAudio (Single) PESQ (↑)
/ VISQOL (↑)/ DNSMOS (↑)2.35 / 2.30 / 3.45MOS (↑)3.65±0.08
UniAudio 2.63 /2.44 /3.66 3.68 ±0.07
Target Speaker
ExtractionUniAudio (Single) PESQ (↑)
/ VISQOL (↑)/ DNSMOS (↑)1.97 / 1.61 / 3.93MOS (↑)3.58±0.08
UniAudio 1.88 / 1.68 /3.96 3.72 ±0.06
Singing V oice
SynthesisUniAudio (Single)- -MOS (↑)
/ SMOS (↑)4.14±0.07 / 4.02±0.02
UniAudio 4.08±0.04 / 4.04±0.05
Text-to-SoundUniAudio (Single)FAD (↓)/ KL (↓)3.84 / 2.7 OVL (↑)
/ REL (↑)60.0±2.1 / 61.2 ±1.8
UniAudio 3.12 / 2.6 61.9 ±1.9 / 66.1 ±1.5
Text-to-MusicUniAudio (Single)FAD (↓)/ KL (↓)5.24 / 1.8 OVL (↑)
/ REL (↑)64.4±2.1 / 66.2 ±2.4
UniAudio 3.65 / 1.9 67.9±1.7 / 70.0 ±1.5
Audio EditUniAudio (single)FD(↓)/ KL (↓)19.82 / 0.92--
UniAudio 17.78 / 0.77 -
Speech Dereverb.UniAudio (single)PESQ (↑)/ DNSMOS (↑)1.23 / 3.18--
UniAudio 2.13 / 3.51 -
Instructed TTSUniAudio (single)--MOS (↑)/ SMOS (↑)3.62±0.07 / 3.67±0.08
UniAudio - 3.61 ±0.09 / 3.71±0.09
Speech EditUniAudio (single)MCD (↓)5.26MOS (↑)3.73±0.07
UniAudio 5.12 3.82 ±0.06
C.2 F INE-TUNING THE PRE -TRAINED MODEL ON THE NEW TASK WILL INFLUENCE THE
PERFORMANCE ON PREVIOUS TASKS ?
In this part, we conduct experiments to explore whether fine-tuning the pre-trained model on new
tasks will influence the performance of previous tasks. We evaluate the pre-trained UniAudio model
(trained on 7 tasks) and fine-tuned UniAudio model (fine-tuned on 4 new tasks) on 7 tasks. Figure 3
shows the results. We can see that the performance does not significantly drop on previous training
tasks, which demonstrates that UniAudio has the potential to add new tasks continuously without
losing previous task knowledge.
C.3 T HE INFLUENCE OF DATA QUANTITY
In this part, we conduct experiments to explore the influence of data quantity, we give three settings:
(1) using all of the data; (2) using 1/2training data for each task; (3) using 1/4training data for each
task. We present the results in Figure 4. Based on the experimental results, this work claims that
the data quantity is a key point to building a strong audio foundation model. In the future, we will
explore to use of more unlabeled data to help improve the performance.
21

--- PAGE 22 ---
Under review as a conference paper at ICLR 2024
SIM()
WER()
SIM()
WER()
PESQ()
VISQOL()
D-MOS()
PESQ()
VISQOL()
D-MOS()
KL()
FAD()
KL()
FAD()
MOS()
SMOS()
0.71
2.00
0.86
4.80
2.65
2.36
3.66
1.75
1.69
3.96
3.12
2.57
3.65
1.87
4.08
4.040.71
2.00
0.85
4.90
2.63
2.44
3.60
1.88
1.68
3.86
3.12
2.59
3.68
1.91
4.02
3.98TTS VC SE TSE TT-Sound TT-Music SVSBefore Fine-Tuning After Fine-Tuning
Figure 3: Performance comparison over 7 audio generation tasks before/after fine-tuning.
SIM()
WER()
SIM()
WER()
PESQ()
DNSMOS()
PESQ()
DNSMOS()
KL()
FAD()
KL()
FAD()
MCD()
0.71
2.00
0.86
4.80
2.65
3.66
1.75
3.96
3.12
2.57
3.65
1.87
6.260.60
3.20
0.84
5.30
2.48
3.30
1.23
3.60
5.29
2.96
5.41
1.96
6.680.57
3.90
0.80
5.90
2.07
3.20
1.20
3.50
5.75
3.03
9.28
2.39
7.16TTS VC SE TSE TT-Sound TT-Music SVSfull data 1/2 data 1/4 data
Figure 4: Performance comparison over different data quantity.
D W HYUNIAUDIO CANWORK WELL?
From the previous discussions, we can see that the universal modeling strategy brings improvement
for different tasks. In this part, we try to give some potential explanations.
(1)Deterministic latent space : we formulate different modalities into a deterministic latent space
(fixed vocabulary) by tokenization. Different tokens can be seen as specific ’words’, and we can use a
next-token prediction strategy to train the model. Similar to GPT-series (Radford et al., 2018; 2019),
such strategy creates the opportunity for the model to learn the intrinsic properties of audio and the
interrelationship between audio and other modalities.
(2)Shared information between different types of audio : Although multiple types of audio (speech,
sounds, music, and singing) present significant differences in the time domain or frequency domain,
neural audio codec models effectively capture their shared information (rethinking the working
principle of neural codecs, which similar information will be allocated the same token id). Due to the
shared information that exists in different types of audio, multi-task training can be seen as increasing
training data for each task.
(3)Data augmentation perspective : We speculate that multi-task training can be viewed as data
augmentation for some tasks. Considering the TTS and VC task’s definition:
TTS: <phoneme_sequence> <prompt> <audio_sequence>
VC: <semantic_token> <prompt> <audio_sequence>
We can see that the difference in task formulation for TTS and VC is that they use different ways to
denote the phonetic information. In essence, they carry the same phonetic information. The difference
is that semantic tokens include the duration information. Thus we can view the phoneme sequence as
a special semantic sequence that drops the duration information. Such dropping operation is widely
used as a data augmentation strategy (Park et al., 2019).
E T HE DETAILS OF AUDIO CODEC MODELS
In this part, we give more details about our neural audio codec model in Section 2.1.1. We adopt a
similar encoder-decoder framework with the Encodec model, the difference includes: (1) we replace
22

--- PAGE 23 ---
Under review as a conference paper at ICLR 2024
Table 18: Performance comparison between encodec and our universal neural codec. FPS: frame
per second; TPS: token per second. Perceptual evaluation of speech quality (PESQ ↑); Short Term
Objective Intelligibility (STOI ↑).
TypeSpeech (VCTK) Sound (cloth) Music (musiccaps) Sing (m4sing) Average
(Veaux et al., 2017) (Drossos et al., 2020) (Agostinelli et al., 2023) (Zhang et al., 2022) -
Model nqFPS TPS PESQ STOI PESQ STOI PESQ STOI PESQ STOI PESQ STOI
Encodec 8 75 600 2.18 0.79 2.23 0.48 1.86 0.57 1.95 0.76 2.05 0.65
Ours 3 50 150 2.96 0.85 2.42 0.49 1.99 0.57 3.13 0.85 2.62 0.69
Ours 4 50 200 3.11 0.86 2.5 0.51 2.08 0.59 3.27 0.86 2.73 0.71
Ours 8 50 400 3.36 0.88 2.67 0.54 2.31 0.65 3.49 0.89 2.95 0.74
the multi-scale STFT-based (MS-STFT) discriminator as our multi-scale Mel-based discriminator.
(2) We rewrite the vector quantization implementation17based on Encodec’s open-source version18,
making it more suitable for DDP training. Figure 5 shows the details of the mel-based discriminator.
We combine the mel-spectrogram and log-mel-spectrogram features and then input them into a
network consisting of several convolutional layers. Our motivation is that the mel-spectrogram has
a strong intrinsic inductive bias, especially for sounds and music-related audio (the SOTA sounds
or music classification systems are based on the log-mel-spectrogram in the literature.). Thus, we
speculate that choosing a mel-spectrogram-based discriminator can better promote high-fidelity audio
reconstruction. In our experiments, we use 6 different discriminators with different configurations19.
Specifically, we set the hidden_dim as {64, 128, 256, 512, 512, 512} and the hop length as {32, 64,
128, 256, 512, 1024}. We train the neural audio codec model based on the Librilight and AudioSet
datasets. Table 18 demonstrates that the neural codec model adopted in this work outperforms prior
Encodec (Défossez et al., 2022).
STFT Mel Log-mel
Conv2D(2, hidden_dim//32 )
Conv2D(hidden_dim//32,
hidden_dim//16 )
Conv2D(hidden_dim//16,
hidden_dim//8 )
Conv2D(hidden_dim//8,
hidden_dim//4 )
Conv2D(hidden_dim//4,
hidden_dim//2 )
Conv2D(hidden_dim//2,
hidden_dim )
Conv2D(hidden_dim, 1 )
logit
Figure 5: The overview of a single Mel-based discriminator. In practice, we will use multiple
discriminators by setting different hop lengths and hidden dimensions.
F S UBJECTIVE EVALUATION
For TTS and VC tasks, we focus on speech quality (QMOS) and speaker similarity (SMOS). The
details are as follows. For speech quality evaluation, we conduct the MOS (mean opinion score) tests
and explicitly ask the raters to focus on examining the audio quality and naturalness, and ignore the
differences of style (timbre, emotion, and prosody . The testers present and rate the samples, and each
tester is asked to evaluate the subjective naturalness on a 1-5 Likert scale.
For speaker similarity evaluation, we ask the raters to focus on the similarity of the speaker identity
(timbre) to the reference, and ignore the differences in content, grammar, or audio quality . We paired
each synthesized utterance with a reference utterance to evaluate how well the synthesized speech
matched that of the target speaker.
17Please refer to our source code to find the details.
18https://github.com/facebookresearch/encodec/blob/main/encodec/quantization/core_vq.py
19In our experiments, we find the mel-based discriminator brings better reconstruction performance when we
train a universal neural audio codec.
23

--- PAGE 24 ---
Under review as a conference paper at ICLR 2024
（a）Speech Enhancement （b）T arget Speaker Extraction
（c）Qverall quality  of Audio 
（d）Relevance to the text input
（e）Speech quality MOS  
 （f）Speech Similarity  MOS  
Figure 6: Screenshots of subjective evaluations.
For SE and TSE tasks, we write explicit instructions to ask the rater to assess the generated speech.
Refer to Figure 6 to see the details.
For SVS, we also conduct quality MOS (QMOS) and style similarity MOS (SMOS). Different from
TTS’s SMOS evaluation, we explicitly instruct the raters to focus on the similarity of the style (timbre,
emotion, and prosody) to the reference, and ignore the differences in content, grammar, or audio
quality .
For sound and music generation tasks, we follow AudioGen (Kreuk et al., 2022) and MusicGen
(Copet et al., 2023) to evaluate (1) overall quality (OVL), and (2) relevance to the text input (REL).
Our subjective evaluation tests are crowd-sourced and conducted by 20 native speakers via Amazon
Mechanical Turk. The screenshots of instructions for testers have been shown in Figure 6. We paid
about $500 on participant compensation. A small subset of speech samples used in the test is available
athttps://uniaudio666.github.io/demo_UniAudio/ .
24
