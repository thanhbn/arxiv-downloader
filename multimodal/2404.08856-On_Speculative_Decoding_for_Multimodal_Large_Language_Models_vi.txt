# Về Giải Mã Suy Đoán cho Mô Hình Ngôn Ngữ Lớn Đa Phương Thức

Mukul Gagrani* Raghavv Goel* Wonseok Jeon Junyoung Park Mingu Lee Christopher Lott
Qualcomm AI Research

## Tóm Tắt

Suy luận với các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) chậm do phần nền tảng mô hình ngôn ngữ lớn của chúng gặp phải tắc nghẽn băng thông bộ nhớ và tạo ra các token theo cách tự hồi quy. Trong bài báo này, chúng tôi khám phá việc ứng dụng giải mã suy đoán để nâng cao hiệu quả suy luận của MLLM, cụ thể là mô hình LLaVA 7B. Chúng tôi chỉ ra rằng một mô hình chỉ có ngôn ngữ có thể phục vụ như một mô hình dự thảo tốt cho giải mã suy đoán với LLaVA 7B, bỏ qua nhu cầu về các token hình ảnh và các thành phần xử lý liên quan từ mô hình dự thảo. Các thí nghiệm của chúng tôi trên ba nhiệm vụ khác nhau cho thấy giải mã suy đoán có thể đạt được tốc độ tăng tốc bị giới hạn bởi bộ nhớ lên tới 2.37 lần sử dụng mô hình ngôn ngữ 115M tham số mà chúng tôi đã huấn luyện từ đầu. Ngoài ra, chúng tôi giới thiệu một mô hình dự thảo LLaVA nhỏ gọn có tích hợp bộ chuyển đổi hình ảnh, cho thấy những cải thiện hiệu suất nhỏ trong việc tạo chú thích hình ảnh trong khi duy trì kết quả tương đương trong các nhiệm vụ khác.

## 1. Giới Thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) đã trở nên phổ biến trong nhiều lĩnh vực khác nhau do hiệu suất ấn tượng của chúng. Tuy nhiên, LLM chỉ nhận các truy vấn văn bản làm đầu vào nhưng dữ liệu thế giới thực đến dưới dạng nhiều phương thức bao gồm cả dữ liệu hình ảnh. Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) [1, 13, 21, 22] cung cấp cho LLM khả năng hiểu hình ảnh, và sự kết hợp giữa các token hình ảnh và văn bản tăng cường tương tác của mô hình với người dùng, dẫn đến các phản hồi có thông tin hơn. MLLM bao gồm một bộ mã hóa hình ảnh để xử lý thông tin hình ảnh và một bộ chuyển đổi biến đổi các mã hóa hình ảnh thành không gian nhúng mô hình ngôn ngữ. Ngoài ra, MLLM có một nền tảng mô hình ngôn ngữ dưới dạng LLM và do đó kế thừa việc tạo tự hồi quy và tắc nghẽn băng thông bộ nhớ dẫn đến suy luận chậm [19].

Giải mã suy đoán [3, 7, 9, 15, 20] đã được đề xuất như một giải pháp để tăng tốc suy luận LLM mà không mất độ chính xác, trong đó một mô hình dự thảo nhỏ hơn dự đoán nhiều token tương lai được xác minh trong một lần gọi duy nhất của LLM. Cho rằng MLLM có nền tảng LLM, giải mã suy đoán có thể được sử dụng để làm cho suy luận với MLLM hiệu quả hơn. Nhiều công trình gần đây đã nghiên cứu việc ứng dụng giải mã suy đoán và các biến thể của nó [2, 5, 7, 8, 18, 20] cho LLM, nhưng không có công trình nào như vậy tồn tại trong bối cảnh MLLM theo hiểu biết của chúng tôi.

Trong bài báo này, chúng tôi áp dụng giải mã suy đoán cho mô hình LLaVA 7B (với mô hình LLaMA 7B làm nền tảng mô hình ngôn ngữ) để làm cho suy luận hiệu quả hơn, sơ đồ khối được hiển thị trong Hình 1. Do thiếu các mô hình có sẵn công khai của họ LLaVA và LLaMA nhỏ hơn 7B tham số, chúng tôi huấn luyện một mô hình ngôn ngữ kích thước 115M từ đầu cho giải mã suy đoán. Chúng tôi chỉ ra rằng mô hình chỉ có ngôn ngữ không xem xét các token hình ảnh (và do đó không yêu cầu bộ mã hóa hình ảnh và bộ chuyển đổi) có thể phục vụ như một mô hình dự thảo tốt cho LLaVA 7B. Chúng tôi tiến hành thí nghiệm trên ba nhiệm vụ khác nhau bao gồm hỏi đáp hình ảnh trên tập dữ liệu LLaVA Instruct 150K [13], tạo chú thích hình ảnh trên tập dữ liệu Coco [11] và tập dữ liệu ScienceQA [14], sử dụng các ứng viên mô hình dự thảo đã trải qua các giai đoạn huấn luyện và tinh chỉnh khác nhau. Kết quả của chúng tôi cho thấy chúng tôi có thể đạt được tốc độ tăng tốc bị giới hạn bởi bộ nhớ lên tới 2.37 lần chỉ sử dụng mô hình ngôn ngữ làm mô hình dự thảo. Chúng tôi cũng tạo ra một mô hình dự thảo LLaVA nhỏ bao gồm một bộ chuyển đổi hình ảnh cùng với mô hình ngôn ngữ được huấn luyện của chúng tôi và cho thấy nó cải thiện hiệu suất một chút trên nhiệm vụ tạo chú thích COCO và nhiệm vụ ScienceQA trong khi thực hiện tương tự như các mô hình dự thảo chỉ có mô hình ngôn ngữ trên các nhiệm vụ khác.

## 2. Nền Tảng

### 2.1. Giải Mã Suy Đoán

Giải Mã Suy Đoán (SPD) [3, 9] bao gồm một mô hình dự thảo nhỏ hơn tạo ra nhiều token được xác minh song song bởi LLM mục tiêu. Cho một bối cảnh đầu vào X₁:n := [X₁, . . . , Xn], mô hình dự thảo tạo ra một chuỗi token X̂n+1:n+L theo cách tự hồi quy, X̂n+j ~ p(·|X₁:n, X̂n+1:n+j-1). Các token dự thảo sau đó được xác minh thông qua một lần gọi duy nhất của LLM mục tiêu (q) sử dụng tiêu chí lấy mẫu từ chối đảm bảo cùng một phân phối token đầu ra như LLM mục tiêu. Cụ thể, token X̂n+j được chấp nhận với xác suất

min(1, q(X̂j|X₁:n, X̂n+1:n+j-1) / p(X̂j|X₁:n, X̂n+1:n+j-1))

Nếu một token dự thảo X̂n+j bị từ chối, thì một token mới được lấy mẫu từ phân phối dư được định nghĩa là pres(x) = max(0, q(x) - p(x)).

### 2.2. Mô Hình Ngôn Ngữ Lớn Đa Phương Thức

Một Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) dựa trên hình ảnh bao gồm 1) một bộ mã hóa thị giác để mã hóa hình ảnh đầu vào, 2) một bộ chuyển đổi để chuyển đổi các mã hóa hình ảnh thành nhúng mô hình ngôn ngữ, và 3) một nền tảng mô hình ngôn ngữ. Chúng tôi mô tả khung của mô hình LLaVA chi tiết hơn như sau; cho một hình ảnh đầu vào I và truy vấn văn bản Q, hình ảnh I được chuyển đổi thành một chuỗi H₁, H₂, . . . , Hm của m mã hóa hình ảnh, và truy vấn văn bản được chuyển đổi thành một chuỗi nhúng token X₁, X₂, . . . Xn. Các mã hóa hình ảnh được biến đổi thêm thông qua một bộ chuyển đổi gθ (một mạng perceptron đa lớp nhỏ) để có được nhúng hình ảnh, Vi = gθ(Hi). Điều này được thực hiện để chuyển đổi các mã hóa Hi vào không gian nhúng mô hình ngôn ngữ. Các token sau đó được tạo ra bởi mô hình ngôn ngữ có điều kiện trên các nhúng hình ảnh và nhúng token như sau:

Xn+1 ~ q(·|V₁:m, X₁:n) (1)

## 3. SPD cho MLLM

Để đạt được lợi ích cao hơn với giải mã suy đoán, chúng ta cần một mô hình dự thảo nhỏ hơn đáng kể và được căn chỉnh tốt với mô hình mục tiêu của chúng ta (LLaVA-7B). Lựa chọn phổ biến nhất cho các mô hình dự thảo trong các công trình trước đây về LLM là sử dụng một mô hình được huấn luyện trước nhỏ từ cùng họ mô hình như mô hình mục tiêu hoặc huấn luyện một mô hình nhỏ hơn có cùng kiến trúc như mô hình mục tiêu [15]. Vì không có mô hình nhỏ hơn có sẵn công khai trong họ LLaVA, chúng ta cần huấn luyện một mô hình dự thảo từ đầu. Một lựa chọn tự nhiên cho kiến trúc mô hình dự thảo là theo kiến trúc của LLaVA trong đó mô hình dự thảo bao gồm một bộ chuyển đổi và một nền tảng mô hình ngôn ngữ với số lượng tham số nhỏ hơn so với LLaVA 7B. Trong phương pháp của chúng tôi, chúng tôi sử dụng cả hai, 1) một mô hình dự thảo LLaVA nhỏ hơn bao gồm một bộ chuyển đổi hình ảnh nhỏ hơn và một mô hình ngôn ngữ dự thảo, và 2) mô hình dự thảo chỉ có ngôn ngữ tạo ra các token dự thảo bằng cách chỉ điều kiện trên các token văn bản đầu vào. Cho một hình ảnh đầu vào với nhúng hình ảnh V₁:m, nhúng token X₁:n, mô hình dự thảo tạo ra các token dự thảo X̂n+1:n+L trong đó token dự thảo

X̂n+j ~ p(·|X₁:n, X̂n+1:n+j-1)

được tạo ra bằng cách chỉ điều kiện trên các token văn bản. Mô hình LLaVA mục tiêu xác minh các token dự thảo bằng cách tính toán phân phối mục tiêu được điều kiện trên cả nhúng hình ảnh V₁:m và nhúng token văn bản X₁:n, tức là, token dự thảo X̂n+j được chấp nhận với xác suất

min(1, q(X̂n+j|V₁:m, X₁:n, X̂n+1:n+j-1) / p(X̂n+j|X₁:n, X̂n+1:n+j-1))

Sử dụng mô hình dự thảo chỉ có mô hình ngôn ngữ hiệu quả hơn so với mô hình dự thảo có kiến trúc LLaVA vì 1) nó không cần một bộ chuyển đổi bổ sung vì nó không điều kiện trên các nhúng hình ảnh để tạo ra các token dự thảo, và 2) nó không yêu cầu việc huấn luyện bộ chuyển đổi. Hình 1 cho thấy SPD với MLLM bao gồm mô hình ngôn ngữ dự thảo nhỏ hơn thực hiện tạo tự hồi quy tiếp theo bởi mô hình mục tiêu lớn đánh giá các token được dự đoán bởi mô hình dự thảo song song trong khi sử dụng hình ảnh.

## 4. Thí Nghiệm

Chúng tôi chạy thí nghiệm trên ba nhiệm vụ hướng dẫn thị giác sử dụng SPD với LLaVA-7B [12] làm mô hình mục tiêu của chúng tôi, mô hình này sử dụng mô hình LLaMA-7B làm nền tảng mô hình ngôn ngữ. Chúng tôi sử dụng các mô hình dự thảo đã trải qua các giai đoạn huấn luyện khác nhau với kích thước phần ngôn ngữ của mỗi mô hình dự thảo được cố định ở 115M.

**Các Ứng Viên Mô Hình Dự Thảo.** Chúng tôi huấn luyện mô hình dự thảo kích thước 115M theo kiến trúc LLaMA-2. Chúng tôi làm theo pipeline huấn luyện của [6] để huấn luyện trước mô hình dự thảo từ đầu và tinh chỉnh mô hình dự thảo trên các tập dữ liệu tinh chỉnh hướng dẫn sử dụng mất mát TVD++ [6]. Chúng tôi tinh chỉnh thêm mô hình dự thảo của chúng tôi trên một tập con của tập dữ liệu LLaVA Instruct 150K [13]. Cho các thí nghiệm của chúng tôi, chúng tôi xem xét bốn mô hình dự thảo sau đây sau mỗi giai đoạn huấn luyện và tinh chỉnh: 1) base-LLaMA, một mô hình dự thảo LLaMA sau huấn luyện trước sử dụng mất mát dự đoán token tiếp theo trên 600B token tiếng Anh, 2) chat-LLaMA, một mô hình dự thảo LLaMA được tinh chỉnh hướng dẫn theo [6] được khởi tạo với mô hình dự thảo base-LLaMA, và 3) fine-tuned-LLaVA (ft-llava), một mô hình dự thảo LLaVA được tinh chỉnh trong đó bộ chuyển đổi hình ảnh được khởi tạo sử dụng sao chép con [17] của bộ chuyển đổi hình ảnh LLaVA-7B và mô hình ngôn ngữ được khởi tạo từ mô hình dự thảo chat-LLaMA (mô hình sau đó được tinh chỉnh trên tập dữ liệu LLaVA). Chúng tôi cũng bao gồm một mô hình dự thảo khác 4) fine-tuned-LLaVA-text (ft-llava-text), chỉ sử dụng phần mô hình ngôn ngữ của 3). Lưu ý rằng chỉ có mô hình dự thảo fine-tuned-LLaVA sử dụng thông tin hình ảnh trong khi tất cả các mô hình dự thảo khác chỉ tiêu thụ phần văn bản của lời nhắc đầu vào; khi mô hình dự thảo sử dụng thông tin hình ảnh, bộ mã hóa thị giác (dựa trên CLIP [16]) được chia sẻ với mô hình mục tiêu để tránh tính toán lại các nhúng hình ảnh. Các tham số chi tiết được đưa ra trong Phụ lục A.1

**Nhiệm Vụ Đánh Giá.** Chúng tôi tập trung vào tạo văn bản mở và trả lời câu hỏi trắc nghiệm với lý luận để khuyến khích số lượng token được tạo ra cao hơn, điều này có lợi khi sử dụng SPD. Với mục đích này, chúng tôi đánh giá trên 1) tập dữ liệu LLaVA Instruct 150K [13], 2) nhiệm vụ tạo chú thích hình ảnh trên hình ảnh từ tập dữ liệu COCO [11], và 3) Science QA (SQA) với lý luận chuỗi tư duy (CoT) [14]. Các cài đặt lời nhắc hệ thống cho tất cả các nhiệm vụ được mô tả trong Phụ lục A.2

**Các Thước Đo.** Hiệu quả của SPD được đánh giá bằng các thước đo sau; 1) hiệu quả khối (τ), số lượng token trung bình được tạo ra mỗi khối (hoặc lần chạy mô hình mục tiêu), cho một khối có kích thước γ và đầu vào x, giá trị tối đa của τ(x) có thể là γ + 1, kích thước khối (γ) cũng được biết đến như độ dài dự thảo (DL) trong một số công trình; 2) tốc độ tăng tốc bị giới hạn bởi bộ nhớ (MBSU), tốc độ tăng tốc giả định đạt được bởi SPD cho một hiệu quả khối τ(x) và độ trễ tương đối c được định nghĩa là tỷ lệ giữa số lượng tham số của mô hình dự thảo và mô hình mục tiêu, tức là, MBSU(x) = cτ(x)/(cγ+1); 3) tỷ lệ token, tổng số token được tạo ra chia cho tổng thời gian tạo ra, đưa ra ước tính các token được tạo ra mỗi giây. Chúng tôi đo các thước đo này trên các nhiệm vụ khác nhau sử dụng kích thước khối khác nhau γ ∈ {3,5}

**Giải Mã.** Chúng tôi sử dụng giải mã tham lam cho tất cả các thí nghiệm để tạo ra SPD giống hệt với tạo ra tự hồi quy của mô hình mục tiêu. Chúng tôi để lại như công việc tương lai việc khám phá giải mã dựa trên lấy mẫu (nhiệt độ thay đổi, top-p thay đổi, top-k) trong bối cảnh SPD cho MLLM.

**Kết Quả.** Kết quả của chúng tôi cho thấy rằng sử dụng SPD với mô hình mục tiêu LLaVA 7B mang lại tốc độ tăng tốc đáng kể trong tạo ra đầu ra, và chúng tôi nhấn mạnh rằng khi sử dụng mô hình dự thảo không có thông tin hình ảnh nào, SPD vẫn có thể mang lại tốc độ tăng tốc đáng kể và cạnh tranh với mô hình dự thảo sử dụng thông tin hình ảnh.

Từ Hình 2 (các biểu đồ trên và giữa), chúng tôi quan sát thấy rằng sử dụng SPD mang lại hơn 2 lần lợi ích về hiệu quả khối và MBSU. Xu hướng hiệu suất khi tăng kích thước khối từ 3 lên 5 cho mỗi nhiệm vụ là tương tự ngoại trừ nhiệm vụ SQA trong đó mô hình dự thảo base-llama thực hiện tốt hơn các mô hình dự thảo chỉ có văn bản khác cho kích thước khối = 5. Đối với nhiệm vụ LLaVA-eval trên cả hai kích thước khối (3 hoặc 5), mô hình dự thảo ft-llava-text thực hiện tốt nhất theo sát bởi ft-llava. Đối với nhiệm vụ COCO-caption, ft-llava thực hiện tốt nhất, theo sau bởi ft-llava-text cho cả hai kích thước khối. Cuối cùng, đối với nhiệm vụ SQA, cho kích thước khối=3, mô hình dự thảo ft-llava thực hiện tốt nhất theo sau bởi ft-llava-text trong khi cho kích thước khối=5, mô hình dự thảo ft-llava thực hiện tốt nhất theo sau bởi base-llama.

Ngoài ra, tất cả các mô hình dự thảo của chúng tôi cho thấy tỷ lệ token được cải thiện so với tạo ra tự hồi quy trong Hình 2 (dưới) với kích thước khối 3 mang lại tỷ lệ token tốt hơn so với kích thước khối 5, do đó, SPD tạo ra nhiều token hơn mỗi giây so với giải mã tự hồi quy. Tỷ lệ token được hiển thị tương ứng với tỷ lệ của tỷ lệ token của SPD sử dụng một mô hình dự thảo cụ thể với tỷ lệ token của tạo ra tự hồi quy sử dụng mô hình mục tiêu.

Chúng tôi cũng cung cấp kết quả định tính trên nhiệm vụ tạo chú thích COCO để hiển thị các token được chấp nhận trong quá trình tạo ra khi sử dụng mô hình dự thảo fine-tune-LLaVA-text để không có thông tin hình ảnh nào được sử dụng bởi mô hình dự thảo trong Hình 5. Dựa trên các kết quả đầu ra trong hình, trong đó các token màu xanh và được gạch chân là các token được chấp nhận, chúng tôi quan sát thấy rằng mô hình dự thảo có thể dự đoán các từ và giới từ phổ biến, cùng với các nửa từ. Ví dụ, mô hình dự thảo có thể dự đoán "tables" khi cho "vege". Tương tự trong ví dụ thứ hai, cho bối cảnh và token bổ sung "app", mô hình dự thảo có thể dự đoán "liances". Chúng tôi tin rằng nói chung tạo ra văn bản mở có một số token bao gồm các từ phổ biến, giới từ, và hoàn thiện từ không yêu cầu kiến thức về các token hình ảnh, do đó, ngay cả một mô hình dự thảo không sử dụng thông tin hình ảnh vẫn mang lại hiệu suất cạnh tranh. Hơn nữa, mô hình dự thảo cũng có thể dự đoán sự lặp lại của các token nhất định một khi chúng đã được tạo ra. Ví dụ, trong hình ảnh thứ hai, từ "counter" và "bowls" có thể được dự đoán bởi mô hình dự thảo nhiều lần một khi nó đã được tạo ra bởi mô hình mục tiêu. Cuối cùng, việc thực hiện huấn luyện nghiêm ngặt hơn trên một mô hình ngôn ngữ đa phương thức nhỏ được để lại như công việc tương lai của chúng tôi.

## 5. Kết Luận

Trong bài báo này, chúng tôi trình bày nỗ lực đầu tiên hướng tới việc sử dụng giải mã suy đoán để tăng tốc suy luận khi sử dụng các mô hình ngôn ngữ lớn đa phương thức, cụ thể cho lĩnh vực hình ảnh-văn bản. Chúng tôi chỉ ra rằng sử dụng mô hình dự thảo chỉ có văn bản đạt được hiệu suất cạnh tranh với việc sử dụng mô hình dự thảo sử dụng các đặc trưng hình ảnh. Chúng tôi thực hiện các thí nghiệm khác nhau trên các nhiệm vụ trả lời câu hỏi hình ảnh khác nhau tập trung vào tạo ra số lượng token đầu ra cao hơn: tạo ra văn bản mở và tạo ra văn bản với lý luận sử dụng các mô hình dự thảo khác nhau (chỉ có văn bản và hình ảnh-văn bản). Chúng tôi đạt được tốc độ tăng tốc đáng kể lên tới 2.37 lần cho mô hình dự thảo chỉ có văn bản và tốc độ tăng tốc tốt hơn một chút cho mô hình dự thảo hình ảnh-văn bản, chứng minh thực nghiệm tiềm năng của việc sử dụng SPD cho MLLM.

Công trình của chúng tôi mở ra một số hướng tương lai do khung tổng quát được trình bày. Công trình của chúng tôi có thể được mở rộng cho các mô hình mục tiêu khác như BLIP-2 [10], MiniGPT-4 [22] và OpenFlamingo [1], và các phương thức khác như âm thanh [4] cũng bị tắc nghẽn bởi tạo ra tự hồi quy. Hơn nữa, tiến bộ gần đây trong thuật toán SPD thành giải mã dựa trên cây cũng có thể được sử dụng theo [2, 7, 15, 20] để tăng thêm tốc độ tạo ra.

## Tài Liệu Tham Khảo

[1] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 1, 4

[2] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple framework for accelerating llm generation with multiple decoding heads. https://github.com/FasterDecoding/Medusa, 2023. 1, 4

[3] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. 1

[4] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023. 4

[5] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of LLM inference using lookahead decoding, 2023. 1

[6] Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee, and Christopher Lott. Direct alignment of draft model for speculative decoding with chat-fine-tuned llms. arXiv preprint arXiv:2403.00858, 2024. 3

[7] Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee, and Christopher Lott. Recursive speculative decoding: Accelerating llm inference via sampling without replacement. arXiv preprint arXiv:2402.14160, 2024. 1, 4

[8] Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt Keutzer. Big little transformer decoder. arXiv preprint arXiv:2302.07863, 2023. 1

[9] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. 1

[10] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 19730–19742. PMLR, 2023. 4

[11] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 1, 3

[12] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 2

[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 3, 6

[14] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 1, 3, 6

[15] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. SpecInfer: Accelerating generative LLM serving with speculative inference and token tree verification. arXiv preprint arXiv:2305.09781, 2023. 1, 2, 4

[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 3, 6

[17] Mohammad Samragh, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Fartash Faghri, Devang Naik, Oncel Tuzel, and Mohammad Rastegari. Weight subcloning: direct initialization of transformers using larger pretrained ones. arXiv preprint arXiv:2312.09299, 2023. 3

[18] Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating transformer inference for translation via parallel decoding. arXiv preprint arXiv:2305.10427, 2023. 1

[19] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. 1

[20] Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix Yu. SpecTr: Fast speculative decoding via optimal transport. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 1, 4

[21] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200–212, 2021. 1

[22] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 4

## A. Phụ Lục

### A.1. Cấu Hình Mô Hình

Mô hình LLaVA-7B sử dụng: (i) bộ mã hóa thị giác, (ii) bộ chuyển đổi/dự án hình ảnh dựa trên mạng perceptron đa lớp (MLP), và (iii) mô hình ngôn ngữ LLaMA 7B. Bộ mã hóa thị giác là CLIP ViT-L/14 với chi tiết có trong [16], bộ chuyển đổi hình ảnh dựa trên MLP có 2 lớp tuyến tính với các kích thước sau: 1024×4096 và 4096×4096. Đối với trường hợp khi mô hình dự thảo cũng có bộ chuyển đổi hình ảnh, các kích thước là 1024×1024 và 1024×1024.

Các cấu hình sau được sử dụng cho phần mô hình ngôn ngữ mục tiêu và dự thảo của chúng tôi theo kiến trúc LLaMA:

| | mục tiêu (7B) | dự thảo (115M) |
|---|---|---|
| Lớp | 32 | 4 |
| Đầu attention | 32 | 8 |
| Chiều trung gian | 11,008 | 2,816 |
| Chiều ẩn | 2,048 | 1,024 |
| Kích hoạt | SiLU | SiLU |

### A.2. Lời Nhắc Hệ Thống

Chúng tôi sử dụng các lời nhắc hệ thống sau cho nhiệm vụ tương ứng. Token hình ảnh đặc biệt được sử dụng để bao gồm dữ liệu hình ảnh (<image>)

**LLaVA-eval.** Chúng tôi làm theo phong cách lời nhắc được đưa ra trong [13], LLaVA có nhiều câu hỏi và phản hồi mà chúng tôi chia thành các mẫu khác nhau.

<s>Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo. Trợ lý đưa ra những câu trả lời hữu ích, chi tiết và lịch sự cho các câu hỏi của người dùng. USER: <image>
Câu hỏi Q1 ASSISTANT: phản hồi R1. USER: Câu hỏi Q2. . ..

**COCO-caption.** Vì tập dữ liệu COCO không có bất kỳ lời nhắc câu hỏi nào, chúng tôi nhắc mô hình bằng lời nhắc tương tự như trên.

<s>Một cuộc trò chuyện giữa một người dùng tò mò và một trợ lý trí tuệ nhân tạo. Trợ lý đưa ra những câu trả lời hữu ích, chi tiết và lịch sự cho các câu hỏi của người dùng. USER: <image>
Cung cấp mô tả chi tiết về hình ảnh đã cho ASSISTANT:

**Science QA.** Chúng tôi làm theo phong cách lời nhắc được cung cấp trong [14] với một ví dụ trong bối cảnh duy nhất của câu hỏi, lựa chọn, câu trả lời và lý luận để kích hoạt lý luận Chuỗi Tư duy (CoT). Ngoài ra, chúng tôi chỉ xem xét các mẫu kiểm tra có hình ảnh liên quan.

Câu hỏi: câu hỏi: Iᵢqᵘᵉˢ
Tùy chọn: (0) tùy chọn: Iᵢopt1 (1) tùy chọn: Iᵢopt2 (2) tùy chọn: Iᵢopt3
Bối cảnh: bối cảnh: Iᵢcont
Câu trả lời: Câu trả lời là Iᵢans. BỞI VÌ: bài giảng Iᵢlect giải thích: Iᵢexp

<image>
Câu hỏi: câu hỏi: Itest qᵘᵉˢ
Tùy chọn: (0) tùy chọn: Itest, opt1 (1) tùy chọn: Itest, opt2 (2) tùy chọn: Itest, opt3
Bối cảnh: bối cảnh: Itest cont
Câu trả lời: Câu trả lời là

trong đó, chỉ số i là cho ví dụ trong bối cảnh.

Trong bài báo SQA, trường bối cảnh được cung cấp bằng cách tạo ra chú thích cho hình ảnh liên quan sử dụng mô hình tạo chú thích hình ảnh, tuy nhiên, những chú thích này thường đơn giản và không cung cấp mô tả chi tiết về hình ảnh cần thiết để trả lời câu hỏi. Vì lý do này, trường bối cảnh được điền bằng trường "gợi ý" được cung cấp trong tập dữ liệu SQA. Đối với mẫu trong bối cảnh, chúng tôi chọn một mẫu không có hình ảnh liên quan nào vì LLaVA 7B mục tiêu không thể xử lý nhiều hình ảnh. Chúng tôi để lại như công việc tương lai việc thí nghiệm SPD với hơn 1 ví dụ trong bối cảnh.
