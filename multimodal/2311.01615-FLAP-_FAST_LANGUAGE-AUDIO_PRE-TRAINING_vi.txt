# FLAP: TIỀN HUẤN LUYỆN NGÔN NGỮ-ÂM THANH NHANH

Ching-Feng Yeh, Po-Yao Huang, Vasu Sharma, Shang-Wen Li và Gargi Gosh
{cfyeh,berniehuang,vasusharma,shangwel,gghosh }@meta.com
FAIR, Meta

TÓM TẮT

Chúng tôi đề xuất Tiền Huấn Luyện Ngôn Ngữ-Âm Thanh Nhanh (FLAP), một phương pháp tự giám sát học hiệu quả và hiệu suất biểu diễn âm thanh và ngôn ngữ được căn chỉnh thông qua che giấu, học tương phản và tái tạo. Để tăng hiệu quả, FLAP ngẫu nhiên loại bỏ các token phổ âm thanh, chỉ tập trung vào những token còn lại cho tự giám sát. Thông qua học tương phản liên phương thức, FLAP học cách căn chỉnh các biểu diễn âm thanh và văn bản được ghép đôi trong không gian tiềm ẩn chung. Đáng chú ý, FLAP tận dụng nhiều góc nhìn được tăng cường qua che giấu cho tương phản liên phương thức và học cách tái tạo phần bị che giấu của các token âm thanh. Hơn nữa, FLAP tận dụng các mô hình ngôn ngữ lớn (LLM) để tăng cường đầu vào văn bản, đóng góp vào hiệu suất được cải thiện. Những phương pháp này dẫn đến các biểu diễn âm thanh-văn bản mạnh mẽ và nhiều thông tin hơn, cho phép FLAP đạt hiệu suất tối tân (SoTA) trên các tác vụ truy xuất âm thanh-văn bản trên AudioCaps (đạt 53.0% R@1) và Clotho (đạt 25.5% R@1).

Thuật ngữ chỉ mục —Học tương phản, truy xuất âm thanh-văn bản

1. GIỚI THIỆU

Học biểu diễn [1] đã thu hút động lực đáng kể trong việc tạo ra các nhúng giàu thông tin cho các tác vụ hạ nguồn. Gần đây, học biểu diễn tự giám sát (SSL) [2, 3] đã nổi lên như một lĩnh vực nghiên cứu nổi bật với hy vọng giảm bớt các chú thích của con người. Truyền thống, các phương pháp SSL đã được phát triển dưới thiết lập đơn phương thức cho hình ảnh [4, 5], văn bản [6, 7], hoặc âm thanh/lời nói [8, 9, 10] một cách độc lập. Tuy nhiên, có một sự quan tâm ngày càng tăng đối với học biểu diễn qua nhiều phương thức [11, 12, 13, 14], điều này mang lại cả thách thức và những khả năng mới thú vị. Một đột phá là Tiền Huấn Luyện Ngôn Ngữ-Hình Ảnh Tương Phản (CLIP) [11] chiếu các nhúng văn bản và hình ảnh vào không gian tiềm ẩn chung, cho phép các ứng dụng như truy xuất liên phương thức và tự động tạo chú thích. Gần đây hơn, Tiền Huấn Luyện Ngôn Ngữ-Âm Thanh Tương Phản (CLAP) [15, 16] học biểu diễn cho cả văn bản và âm thanh và mang lại hiệu suất mạnh mẽ trên các tác vụ truy xuất âm thanh-văn bản.

Các thành phần chính trong CLIP và CLAP là các mục tiêu SSL và kiến trúc mô hình của chúng. Về mục tiêu, cả CLIP và CLAP đều sử dụng học tương phản, nhằm tối thiểu hóa khoảng cách giữa các nhúng trong các phương thức khác nhau của cùng một thể hiện, đồng thời phân biệt các nhúng từ các thể hiện khác nhau [17, 18, 19]. Về kiến trúc mô hình, cả CLIP và CLAP đều áp dụng các mô hình giống Transformer [20], đã được chứng minh là hiệu quả. Các nghiên cứu trước đề xuất kết hợp transformer + học tương phản này tạo ra các nhúng chất lượng cao cho cả tác vụ đơn phương thức [4, 18, 21] và đa phương thức [14, 22, 23]. Một hạn chế lớn của các mô hình giống Transformer là độ phức tạp bậc hai đối với độ dài chuỗi, điều này trở thành nút thắt tính toán và hạn chế hiệu quả tổng thể.

Để cải thiện hiệu quả tính toán, các kỹ thuật với Bộ Mã Hóa Tự Động Có Che Giấu (MAE) như MAE hình ảnh [5], VideoMAE [24, 25] và AudioMAE [8] đã được đề xuất gần đây và đạt được những cải thiện hiệu quả đáng kể với sự đánh đổi hiệu suất nhỏ. Gần đây, Tiền Huấn Luyện Ngôn Ngữ-Hình Ảnh Nhanh (FLIP) [26] đã áp dụng các kỹ thuật tương tự cho SSL hình ảnh-văn bản. Nhận thức rằng tín hiệu âm thanh trong tự nhiên là liên tục và có độ dài thay đổi, chúng tôi đã khám phá các chiến lược che giấu cho học biểu diễn ngôn ngữ-âm thanh tự giám sát. Chúng tôi gọi mô hình của chúng tôi là Tiền Huấn Luyện Ngôn Ngữ-Âm Thanh Nhanh (FLAP). FLAP nỗ lực thiết lập các biểu diễn âm thanh và ngôn ngữ được căn chỉnh bằng cách kết hợp các kỹ thuật che giấu, học tương phản và tái tạo. Đối với các tập dữ liệu ngôn ngữ-âm thanh, thường xuyên các tín hiệu âm thanh chứa thông tin phong phú hơn nhiều so với các đối tác văn bản. Ví dụ, một đoạn âm thanh tiếng chó sủa có thể tiết lộ thông tin bổ sung như âm lượng và tần số, thường bị thiếu trong văn bản. Ngoài ra, mô tả văn bản có thể thay đổi trong phong cách viết và tạo ra các nhúng không nhất quán cho cùng một ngữ nghĩa. Với sự mất cân bằng về độ phong phú thông tin giữa âm thanh và văn bản, chúng tôi sử dụng các mô hình ngôn ngữ lớn (LLM) [27, 28, 29] để làm phong phú và thống nhất phong cách viết cho văn bản trong tác vụ ngôn ngữ-âm thanh.

Các công trình trước đây [16, 30, 31, 32] về tiền huấn luyện ngôn ngữ-âm thanh đã nhận được sự quan tâm nghiên cứu rộng rãi. Gần đây, CLAP quy mô lớn (LS-CLAP) [16] đã chứng minh kết quả mạnh mẽ trên truy xuất âm thanh-văn bản trên các điểm chuẩn AudioCaps và Clotho. Trong nghiên cứu này, chúng tôi cải thiện thêm kết quả LS-CLAP bằng cách 1) sử dụng Người Học Âm Thanh-Video Có Che Giấu (MA ViL) làm bộ mã hóa âm thanh được tiền huấn luyện 2) che giấu hiệu quả cho hiệu quả và tính mạnh mẽ 3) thêm tái tạo âm thanh cho nhúng tốt hơn 4) sử dụng LLM để tăng cường văn bản. Chúng tôi quan sát thấy những cải thiện hiệu suất đáng kể từ FLAP, vượt trội hơn các hệ thống tối tân được đề xuất gần đây [16].

Hình 1. Kiến trúc của FLAP, bao gồm các bộ mã hóa âm thanh/văn bản, che giấu hiệu quả và tái tạo âm thanh.

2. HỌC TƯƠNG PHẢN

Khung làm việc cơ bản của học tương phản bao gồm việc chọn một mẫu dữ liệu "neo", một điểm dữ liệu từ cùng một phân phối được gọi là mẫu "tích cực", và một điểm dữ liệu từ một phân phối khác được biết đến là mẫu "tiêu cực". Học tương phản nhằm giảm khoảng cách giữa các mẫu neo và tích cực, là những phần của cùng một phân phối, trong không gian tiềm ẩn. Đồng thời, nó tìm cách tối đa hóa khoảng cách giữa các mẫu neo và tiêu cực. Để học các biểu diễn âm thanh và văn bản được căn chỉnh, các ví dụ "tích cực" đề cập đến các biểu diễn của các mẫu âm thanh và văn bản được ghép đôi (tức là, một âm thanh và chú thích tương ứng của nó), trong khi các ví dụ tiêu cực là tất cả các kết hợp của các âm thanh và chú thích không được ghép đôi được lấy mẫu trong một batch. Trong công việc này, chúng tôi sử dụng mất mát InfoNCE [33] cho học tương phản liên phương thức trên các cặp âm thanh và văn bản được lấy mẫu từ một tập dữ liệu (a,t)∈ D. Hãy để a và t lần lượt biểu thị các biểu diễn âm thanh và văn bản cấp độ thể hiện. Mất mát InfoNCE Lc(a,t) được định nghĩa là:

Lc(a,t) = -1/B ∑[i=1 to B] log(exp(S(ai,ti)/τ) / ∑[j=1 to B] exp(S(ai,tj)/τ))    (1)

trong đó S(ai,tj) = ai^T tj / (||ai|| ||tj||) là độ tương tự cosine giữa ai, tj và τ là nhiệt độ softmax. Trong Phương trình 1, hàm mất mát khuyến khích khoảng cách giữa các nhúng từ âm thanh và văn bản từ cùng một mẫu được tối thiểu hóa và được tối đa hóa từ các mẫu khác nhau, do đó đạt được hiệu quả "tương phản" mong muốn. Đáng chú ý rằng hiệu suất của học tương phản phụ thuộc cao vào số lượng mẫu (B) được tương phản với nhau trong cùng một batch. Kích thước batch lớn hơn (B) cung cấp nhiều kết nối giữa các mẫu hơn để ổn định các gradient tích lũy để cập nhật các tham số mô hình, với nhu cầu tăng về tiêu thụ tính toán và bộ nhớ.

3. FLAP: CHE GIẤU HIỆU QUẢ

Được truyền cảm hứng bởi thành công gần đây của FLIP [26] cố gắng sử dụng kỹ thuật che giấu để học biểu diễn hình ảnh-văn bản, chúng tôi đề xuất Tiền Huấn Luyện Ngôn Ngữ-Âm Thanh Nhanh (FLAP) để học biểu diễn ngôn ngữ-âm thanh tự giám sát bằng cách sử dụng che giấu cho cả học tương phản và tái tạo. Như được mô tả trong Hình 1, FLAP bao gồm một bộ mã hóa âm thanh, một bộ mã hóa văn bản, và bộ giải mã âm thanh. Đối với bộ mã hóa âm thanh của FLAP, chúng tôi áp dụng backbone âm thanh từ MA ViL [22], mô hình âm thanh SoTA được tiền huấn luyện trên các clip âm thanh và video của AudioSet [34]. MA ViL là một khung làm việc tự giám sát để học biểu diễn âm thanh-video bao gồm hai giai đoạn. Trong giai đoạn đầu tiên, MA ViL đồng thời học cách tái tạo phổ âm và pixel, tận dụng thông tin bổ sung từ cả hai phương thức. Trong giai đoạn thứ hai, MA ViL thực hiện tự chưng cất nơi mô hình học sinh dự đoán các đặc trưng được ngữ cảnh hóa được tạo ra bởi mô hình giáo viên giai đoạn đầu tiên.

FLAP thực hiện học tương phản liên phương thức theo thể hiện bằng cách sử dụng Phương trình 1 trên phần không bị che giấu (có thể nhìn thấy) của các token phổ âm thanh. Chiến lược che giấu trong FLAP tăng cường đáng kể hiệu quả tính toán và thúc đẩy biểu diễn mạnh mẽ hơn vì che giấu cũng có thể được xem như một phương pháp tăng cường dữ liệu trên các token âm thanh. Cụ thể, cho một tensor đầu vào có hình dạng (B, N, D), trong đó B là kích thước batch, N là độ dài chuỗi, D là chiều nhúng, che giấu giảm hình dạng xuống (B, N', D), trong đó N' nhỏ hơn N. Điều này cho phép giảm tính toán đáng kể cho các mô hình giống Transformer vì độ phức tạp mô hình tăng theo bậc hai với độ dài chuỗi (tức là O(N²)).

Chúng tôi đã nghiên cứu hai chiến lược che giấu, cụ thể là che giấu 1-D và 2-D, như được minh họa trong Hình 2. Trước khi che giấu, đầu vào (dưới dạng mel-spectrogram) được chuyển đổi thành các nhúng patch. Đối với che giấu 1-D, tensor đầu vào có hình dạng (B, N, D) đầu tiên được tăng cường với các nhúng vị trí và sau đó được lấy mẫu ngẫu nhiên trên trục T để trở thành (B, N', D). Việc lấy mẫu ngẫu nhiên được thực hiện trên cơ sở xáo trộn và từng khung hình đến độ dài mong muốn N'. Che giấu 1-D đơn giản và hiệu quả trong việc tăng cường tính mạnh mẽ bằng cách loại bỏ khung hình ngẫu nhiên và giảm tính toán cùng với độ dài chuỗi N. Mặt khác, che giấu 2-D nhằm xây dựng một chiến lược lấy mẫu có cấu trúc hơn trên cơ sở che giấu 1-D. Thay vì lấy mẫu trực tiếp trên trục N, che giấu 2-D đầu tiên chia trục N thành M nhóm, mỗi nhóm có K=N/M khung hình liên tiếp. Tiếp theo, cả M nhóm và K khung hình trong mỗi nhóm đều được lấy mẫu riêng biệt theo cùng một cách như trong che giấu 1-D và được giảm xuống M' và K' tương ứng. Cuối cùng, cả M' và K' đều được hợp nhất lại với nhau và trở thành N' mới = M' * K'. Che giấu 2-D về cơ bản chia chuỗi tổng thể (N) thành nhiều (M) phân đoạn chi tiết (K), do đó cho phép lấy mẫu có cấu trúc hơn thông qua cả lấy mẫu đồng nhất và loại bỏ trong mỗi phân đoạn chi tiết. Cả che giấu 2-D và 1-D đều có thể đạt được cải thiện hiệu quả tương tự với các tỷ lệ che giấu khác nhau. Ví dụ, tỷ lệ che giấu 75% trên N dẫn đến 25% (= 100% - 75%) chi phí tính toán cho che giấu 1-D, trong khi 50% trên M và 50% trên K cho che giấu 2-D cũng dẫn đến 25% (= 50% * 50%). Các tensor bị che giấu sau đó được gửi trực tiếp đến bộ mã hóa âm thanh để tính toán các nhúng đầu ra cho mỗi khung hình và sau đó được trung bình hóa trên trục N cho các nhúng theo thể hiện. Những chiến lược che giấu này đặc biệt hữu ích cho các tác vụ học tương phản vì các đầu ra theo ví dụ mạnh mẽ hơn đối với việc loại bỏ khung hình. Ngoài ra, độ dài chuỗi giảm bằng che giấu cũng cho phép kích thước batch lớn hơn để phù hợp với GPU, điều này có lợi cho học tương phản vì nhiều cặp hơn được tham gia vào hàm mất mát cho một batch duy nhất. Hơn nữa, chiến lược che giấu có thể được xem như một loại tăng cường âm thanh (ví dụ SpecAug [35]) thúc đẩy tính mạnh mẽ của các biểu diễn được học. Che giấu được áp dụng trong giai đoạn huấn luyện và được vô hiệu hóa trong quá trình đánh giá.

Hình 2. Loại bỏ khung hình bằng Che giấu 1-D và 2-D.

4. TÁI TẠO ÂM THANH

Để tăng cường tính mạnh mẽ của các nhúng âm thanh được học, chúng tôi tiếp tục đề xuất một mục tiêu bổ sung thúc đẩy việc kết hợp thông tin âm thanh vào các nhúng. Điều này có thể đạt được bằng cách giao cho mô hình nhiệm vụ tái tạo các token phổ âm thanh gốc bằng cách sử dụng các nhúng theo mẫu. Như được mô tả trong Hình 1, trước khi được tổng hợp qua độ dài chuỗi để tạo ra các nhúng âm thanh theo mẫu, các nhúng theo khung hình (có hình dạng (B, T', D)) được gửi đến bộ giải mã âm thanh để tái tạo mel-spectrogram. Thực nghiệm, chúng tôi quan sát thấy rằng tái tạo chỉ phổ âm mà không phải các token văn bản mang lại hiệu suất tốt hơn. Chúng tôi sử dụng các khối Transformer thông thường như các bộ giải mã âm thanh f⁻¹ₐ(.). Các đầu ra của bộ mã hóa (amm) đầu tiên được chiếu và đệm với các token [MASK] có thể huấn luyện. Sau khi khôi phục thứ tự gốc (thời gian-tần số cho âm thanh và không gian-thời gian cho token video), chúng tôi thêm các nhúng vị trí (sin cosin 2-D cố định) của bộ giải mã và đưa các chuỗi được khôi phục vào bộ giải mã. Ở đỉnh của các bộ giải mã, chúng tôi kết hợp các đầu tuyến tính để tái tạo các đầu vào thô. Cụ thể, các đầu ra của bộ giải mã để tái tạo phổ âm được ký hiệu là â = f⁻¹ₐ(gₐᵥ(fₐ(a'))). Để rõ ràng về ký hiệu, chúng tôi bỏ qua các token [MASK] và đầu chiếu tuyến tính. Hãy để âᵢ, aʳᵃʷᵢ ∈ Rᴴᵃʳᵃʷⁱ⁻¹...n biểu thị đầu ra của bộ giải mã âm thanh và tham chiếu sự thật cơ bản của patch phổ âm bị che giấu thứ i. Trong tái tạo âm thanh bị che giấu, FLAP được tự giám sát bằng cách tối thiểu hóa mất mát lỗi bình phương trung bình (MSE) Lʳᵃʷᵣ được định nghĩa là:

Lʳᵃʷᵣ = 1/n ∑ᵢ₌₁ⁿ (âᵢ - aʳᵃʷᵢ)²    (2)

Mất mát MSE từ tái tạo sau đó được cân bằng và được thêm vào mất mát cuối cùng cùng với mất mát tương phản. Với tái tạo, mô hình được khuyến khích bảo tồn thông tin cô đọng vào các nhúng theo mẫu, vì những nhúng này không chỉ phải gần với các đối tác miền văn bản của chúng, mà còn hữu ích trong việc tạo ra các đầu vào gốc. Đáng chú ý rằng tái tạo đi kèm với sự đánh đổi về hiệu quả và kích thước batch, vì bộ giải mã âm thanh yêu cầu tính toán và sử dụng bộ nhớ không tầm thường.

5. TĂNG CƯỜNG PHONG PHÚ BẰNG LLM

Học biểu diễn âm thanh-văn bản đối mặt với một thách thức bổ sung xuất phát từ sự khan hiếm của các cặp âm thanh-văn bản trong các kho ngữ liệu âm thanh-văn bản hiện có. Việc thu thập chú thích của con người cho âm thanh vừa đắt đỏ vừa không thể mở rộng. Để giải quyết vấn đề này, chúng tôi trình bày một phương pháp mới tận dụng sức mạnh của các mô hình ngôn ngữ lớn (LLM) và các mô hình phát hiện sự kiện âm thanh (AED) để tăng cường số lượng hạn chế các mô tả văn bản có sẵn cho âm thanh. Bảng 1 hiển thị các ví dụ về mô tả văn bản gốc từ dữ liệu huấn luyện và chuyển đổi danh sách lớp thành chú thích. Từ các mô tả văn bản gốc, rõ ràng rằng cả độ phong phú thông tin đều đứng sau các tín hiệu âm thanh tương ứng và phong cách viết không nhất quán giữa các mẫu. Để tái diễn giải và làm phong phú cùng một ngữ nghĩa cho ngôn ngữ tự nhiên, chúng tôi tận dụng sức mạnh của LLM [27, 36, 29] để tăng cường tính mô tả của các chú thích âm thanh trên các tập dữ liệu âm thanh-văn bản như AudioCaps và Clotho, chỉ chứa các chú thích mô tả yếu và hạn chế. Chúng tôi đầu tiên sử dụng mô hình AED có sẵn (tức là, MA ViL [22]) để phát hiện các sự kiện âm thanh trong một mẫu. Và sau đó chúng tôi khai thác một LLM (tức là, Vicuna [36]) cùng với các lời nhắc được thiết kế để kết hợp các đầu ra phân loại và chú thích gốc để tạo ra các chú thích phong phú hơn cho các mẫu trong AudioCaps và Clotho. Vicuna là một mô hình tuân theo hướng dẫn nguồn mở được tinh chỉnh trên mô hình Llama-7b [37]. Từ các ví dụ trong Bảng 1, việc sử dụng mô hình này tạo ra các chú thích ngữ pháp hơn vẫn trung thành với các sự kiện âm thanh.

Để làm phong phú các chú thích văn bản với LLM và các sự kiện âm thanh được phát hiện, chúng tôi đã sử dụng lời nhắc sau: "Mô tả một tình huống với âm thanh kết quả AED và kết hợp nó với chú thích gốc cùng nhau." Một hạn chế của mô hình Vicuna là xu hướng thêm chi tiết không cần thiết hoặc bỏ qua các nhãn liên quan khi tạo chú thích. Bằng cách thêm đầu ra AED và chú thích gốc vào lời nhắc, chúng tôi đã tận dụng khả năng học trong ngữ cảnh của Vicuna để làm phong phú chú thích. Trong quá trình huấn luyện, cùng một tập hợp tín hiệu âm thanh với mô tả văn bản được thay thế bằng chú thích được tạo ra được tăng cường vào các tập dữ liệu.

Bảng 1. So sánh giữa chú thích gốc và chú thích được tăng cường được tạo ra bởi LLM và AED.

6. THỰC NGHIỆM

6.1. Tập dữ liệu và Thiết lập

Qua tất cả các thí nghiệm, tương tự như LS-CLAP [16], chúng tôi sử dụng AudioCaps, Clotho, và 5 tập dữ liệu khác (Freesound, Epidemic Sound, BBC Sound Effects, Free To Use Sounds, Sonniss Game effects) để huấn luyện, trong khi AudioCaps [38] và Clotho [39] được sử dụng để đánh giá. Đáng chú ý rằng so với LS-CLAP, chúng tôi loại bỏ AudioStock do tính không khả dụng của nó và do đó kích thước của tập dữ liệu để huấn luyện nhỏ hơn LS-CLAP. Các tập đánh giá giống hệt nhau để so sánh công bằng. Chúng tôi xây dựng thí nghiệm dựa trên bộ công cụ LS-CLAP [16] và áp dụng fvcore [40] để phân tích hiệu quả. Truy xuất đa phương thức giữa âm thanh và văn bản được sử dụng để đánh giá chất lượng của các nhúng. Đối với truy xuất văn bản-âm thanh (T-A), với văn bản làm truy vấn, các bản ghi âm thanh trong tập đánh giá được xếp hạng dựa trên độ tương tự cosine giữa các nhúng văn bản và âm thanh. Cùng một quy trình áp dụng cho truy xuất âm thanh-văn bản (A-T). Recall ở top 1, 5 và 10 (R@1, R@5 và R@10) được báo cáo như các chỉ số cho cả hai tác vụ trên các tập dữ liệu AudioCaps và Clotho.

Đối với các thí nghiệm không có fusion đặc trưng, tùy thuộc vào độ dài âm thanh, chúng tôi hoặc cắt ngẫu nhiên 10 giây từ các âm thanh dài hơn hoặc đệm đến 10 giây cho các âm thanh ngắn hơn để tạo dữ liệu đầu vào có độ dài thống nhất. Để trích xuất đặc trưng, kích thước cửa sổ 25ms và dịch chuyển cửa sổ 10ms được sử dụng để trích xuất các đặc trưng mel-spectrogram với 128 mel-bin. Đối với các thí nghiệm có kích hoạt fusion đặc trưng, chúng tôi tuân theo cùng một quy trình như LS-CLAP [16], nơi các âm thanh được đệm hoặc phân tách để tạo các phiên bản toàn cục và cục bộ theo sau bởi các phép tích chập 2-D để hợp nhất. Đối với SpecAug [35], lên đến 192 khung hình âm thanh (ví dụ 1.92 giây) và lên đến 48 mel-bin được thay thế ngẫu nhiên bằng số không cho mỗi mẫu. Để tạo nhúng văn bản, các văn bản được ghép đôi với dữ liệu âm thanh được token hóa với độ dài giới hạn là 77. RoBERTa [7] được sử dụng như bộ mã hóa văn bản cho tất cả các thí nghiệm để nhất quán với LS-CLAP [16].

Bộ tối ưu hóa Adam [41] với β₁ = 0.99, β₂ = 0.9 được sử dụng trong quá trình huấn luyện mô hình. Tỷ lệ học bắt đầu với giai đoạn khởi động, đạt đỉnh ở 10⁻⁴ và được giảm theo lịch cosine cho đến khi số epoch mục tiêu (45) được đạt. Vì cả che giấu hoặc tái tạo đều ảnh hưởng đến việc sử dụng bộ nhớ GPU dẫn đến kích thước batch lớn nhất được phép cho mỗi GPU, chúng tôi báo cáo kết quả với kích thước batch tương tự như baseline (2304) và cũng kết quả với kích thước batch lớn hơn được kích hoạt bởi che giấu hiệu quả nhưng sử dụng tài nguyên tính toán tương đương (tức là cùng số GPU).

6.2. Kết quả về Che giấu Hiệu quả và Tái tạo

Để đánh giá hiệu suất của che giấu hiệu quả và tái tạo, kết quả thí nghiệm được tóm tắt trong Bảng 2, trong đó tất cả kết quả đều không có fusion đặc trưng. Kết quả từ LS-CLAP [16] được liệt kê ở hàng 1 phục vụ như baseline. Ở hàng 2, bộ mã hóa âm thanh được thay thế bằng mô hình MA ViL [22] gần đây với tiền huấn luyện tự giám sát âm thanh-và-video đạt hiệu suất tối tân trên các tác vụ phân loại âm thanh. Lưu ý rằng chúng tôi chỉ đơn giản huấn luyện MA ViL với mất mát tương phản Phương trình 1 trên các tập dữ liệu âm thanh-văn bản mà không áp dụng che giấu hoặc tái tạo. Kết quả xác nhận rằng phương thức âm thanh mạnh hơn mang lại các biểu diễn âm thanh-văn bản được cải thiện trong các tác vụ truy xuất âm thanh-văn bản. Ở hàng 3 và 5, che giấu 1-D và 2-D được áp dụng với tỷ lệ che giấu được chọn từ các nghiên cứu loại bỏ bổ sung, 0.4 cho 1-D và 0.2/0.2 cho 2-D tương ứng. Đối với che giấu 2-D, chúng tôi chia chuỗi thành 64 (ví dụ N = 64) nhóm của 8 (ví dụ K = 8) khung hình từ các nhúng patch có độ dài 256. Từ so sánh, chúng tôi quan sát thấy giảm độ dài chuỗi tương tự từ 1-D (1 - 0.4 = 60%) và 2-D ((1 - 0.2) × (1 - 0.2) = 64%). Nhưng che giấu 2-D mang lại cải thiện tốt hơn do chiến lược che giấu có cấu trúc hơn. Cả che giấu 1-D và 2-D đều giảm việc sử dụng bộ nhớ và bảo tồn chỗ cho các hoạt động bổ sung. Trên cơ sở che giấu, tái tạo âm thanh được áp dụng với 4 lớp của các lớp giải mã Transformer với 4 đầu và 512 chiều nhúng cho mỗi lớp. Kết quả với tái tạo âm thanh được liệt kê ở hàng 4 và 6. Mục tiêu tái tạo khuyến khích FLAP nắm bắt các khái niệm trừu tượng hơn từ ngữ cảnh âm thanh để biểu diễn và dự đoán các phổ âm thanh thô, mà không dựa vào các nhãn lớp bổ sung. Điều này dẫn đến hiệu suất truy xuất âm thanh-văn bản mạnh hơn trên AudioCaps. Thay vào đó, việc tiết kiệm bộ nhớ từ che giấu cũng có thể được sử dụng để xử lý nhiều mẫu hơn trong một batch duy nhất thay vì tái tạo âm thanh. Tăng gấp đôi kích thước batch tạo ra kết quả ở hàng 7. So với hàng 5 và 6, tăng kích thước batch cải thiện tính mạnh mẽ của mục tiêu tương phản. Trong Phương trình 1, các cặp tích cực được khuyến khích tương phản với một tập hợp lớn hơn các mẫu tiêu cực trong mẫu số, dẫn đến không gian tiềm ẩn âm thanh-văn bản được căn chỉnh tốt hơn nơi các cặp âm thanh-văn bản tương quan về ngữ nghĩa gần nhau hơn và những cặp không tương quan thì xa nhau. Đối với học tương phản, kích thước batch đủ lớn là quan trọng đối với hiệu suất mô hình. Đáng chú ý rằng số lượng GPU được giữ giống nhau qua các so sánh và kích thước batch lớn hơn được đạt thông qua che giấu hiệu quả, không chỉ cải thiện tính mạnh mẽ của mô hình mà còn giảm dấu chân tính toán và bộ nhớ.

Bảng 2. Kết quả thí nghiệm về loại che giấu, tỷ lệ che giấu và tái tạo âm thanh (không có fusion đặc trưng).

6.3. Phân tích Hiệu quả của Che giấu 1-D/2-D

Che giấu mang lại lợi ích bao gồm giảm độ dài chuỗi để tăng hiệu quả và cải thiện tính mạnh mẽ của mô hình. Tuy nhiên, tương tự như nhiều phương pháp tập trung vào hiệu quả, sự đánh đổi hiệu quả/hiệu suất điển hình cũng áp dụng ở đây. Để phân tích mối tương quan giữa tỷ lệ che giấu và tác động đến hiệu suất mô hình, các mô hình với các chiến lược che giấu khác nhau và tỷ lệ che giấu tăng dần được huấn luyện và so sánh trong các đường cong hoạt động trong Hình 3, với kết quả AudioCaps ở trên và kết quả Clotho ở dưới. Trong các đường cong hoạt động, độ phức tạp tính toán (theo GFLOPs) phục vụ như trục ngang trong khi recall top 1 trong truy xuất (theo R@1) phục vụ như trục dọc. Chúng tôi cũng chú thích mỗi điểm dữ liệu với (tỷ lệ che giấu, R@1) để so sánh số hữu ích hơn. Các GFLOP được tính toán bằng công cụ fvcore [40] chỉ cho bộ mã hóa âm thanh với một batch của 8 mẫu có độ dài 10 giây. Kích thước batch được giữ giống nhau cho tất cả các tỷ lệ che giấu để so sánh công bằng. Kết quả baseline không có che giấu cũng được bao gồm ở các vị trí ngoài cùng bên phải trong Hình 3.

Đối với mỗi tập dữ liệu, che giấu 1-D và 2-D được so sánh với tỷ lệ che giấu tăng dần. Các tỷ lệ che giấu theo cơ sở từng chiều, có nghĩa là đối với cùng một tỷ lệ che giấu, che giấu 2-D trình bày loại bỏ khung hình tích cực hơn. Ví dụ, khi tỷ lệ che giấu là 0.3, che giấu 1-D bảo tồn 70% chuỗi trong khi che giấu 2-D chỉ bảo tồn 49% (= 0.7 * 0.7). Từ các đường cong trong Hình 3, che giấu hiệu quả bắt đầu cải thiện tính mạnh mẽ của mô hình cho đến khi quá nhiều khung hình trong chuỗi bị loại bỏ (khoảng 0.5). Điều này được mong đợi vì mất mát thông tin tăng cùng với tỷ lệ che giấu. Ngoài ra, tương tự như quan sát trong Bảng 2, che giấu 2-D cung cấp recall tốt hơn xung quanh GFLOP tương tự do đó mang lại sự đánh đổi tốt hơn che giấu 1-D vì có cấu trúc hơn. Lấy tỷ lệ che giấu = 0.2 làm ví dụ, che giấu 2-D tiết kiệm khoảng 25% tính toán và mang lại recall tốt hơn kết quả không có che giấu. Điều này cho thấy rằng che giấu hiệu quả hiệu quả trong cả việc cải thiện hiệu quả và tính mạnh mẽ của mô hình.

Hình 3. Text-Audio R@1 so với GFLOPs trên AudioCaps và Clotho với Các Tỷ lệ Khác nhau cho Che giấu 1-D và 2-D.

6.4. Kết quả về Fusion Đặc trưng và Tăng cường LLM

Thiết lập không có fusion đặc trưng trong LS-CLAP thêm đệm cho các tín hiệu âm thanh ngắn hơn và áp dụng cắt ngẫu nhiên cho các tín hiệu dài hơn để tạo ra đầu vào cho mô hình có độ dài thống nhất là 10 giây. Nó hoạt động tốt để đưa các tín hiệu âm thanh dài vào bộ mã hóa âm thanh mà không tăng độ phức tạp tính toán. Tuy nhiên, cắt ngẫu nhiên cũng ngụ ý mất mát thông tin. Do đó, fusion đặc trưng [16] được giới thiệu để tăng cường hiệu suất truy xuất cuối cùng và đạt được những cải thiện đáng kể. Để đánh giá FLAP trên cùng một thiết lập, chúng tôi áp dụng cùng một fusion đặc trưng và các kết quả tương ứng được liệt kê trong Bảng 3. Trong Bảng 3, kết quả không có fusion đặc trưng được liệt kê ở hàng 1 đến 5 và kết quả có fusion đặc trưng ở hàng 6 đến 10. Hàng 1 đến 5 chia sẻ cùng thiết lập trong Bảng 2, nơi hàng 1 là cùng baseline CLAP, hàng 2 là MA ViL bị che giấu 2-D với tỷ lệ 0.2, hàng 3 kết hợp mất mát tái tạo trên cơ sở hàng 2, hàng 4 tăng gấp đôi kích thước batch so với hàng 2 và hàng 5 tăng cường mô tả văn bản được tạo ra bởi LLM trên cơ sở hàng 4. Hàng 6 đến 10 lặp lại cùng thiết lập như hàng 1 đến 5 trừ đầu vào với fusion đặc trưng được sử dụng.

So với hàng 1 đến 5, hàng 6 đến 10 được cải thiện hiệu quả với fusion đặc trưng, vì fusion đặc trưng kết hợp các phân đoạn toàn cục và được cắt làm đầu vào cho mô hình. Điều này có lợi hơn cho các tín hiệu âm thanh dài, như quan sát thấy từ những cải thiện lớn hơn trên Clotho, chứa nhiều đoạn âm thanh dài hơn 10 giây. So sánh hàng 7-10 với hàng 6, FLAP mang lại cải thiện hiệu suất tương tự cho các thiết lập fusion đặc trưng tương tự như hàng 2-5. Điều này chứng minh rằng FLAP rất linh hoạt và thêm các lợi ích bổ sung trên cơ sở kết quả fusion đặc trưng đã cạnh tranh. Ở hàng 5 và 10, tăng cường LLM được đề cập trong phần 5 cũng được áp dụng trên cơ sở các mô hình tốt nhất để chứng minh tác động từ các mô tả văn bản được làm phong phú và nhất quán hơn. So với hàng 4 và 9, kết quả với tăng cường từ các mô tả văn bản được tạo ra bởi LLM cho thấy hiệu suất tương tự hoặc tốt hơn. Đặc biệt, kết quả trên Clotho với fusion đặc trưng cho thấy cải thiện lớn hơn. Vì mô tả văn bản được làm phong phú có xu hướng dài hơn như quan sát thấy từ các ví dụ trong Bảng 1, các thiết lập fusion đặc trưng có thể có lợi hơn cho khớp và căn chỉnh âm thanh-văn bản tốt hơn. Hàng 5 và 10 cũng phục vụ như kết quả tốt nhất có và không có fusion đặc trưng cho khung làm việc FLAP được đề xuất. So với CLAP, kết hợp che giấu hiệu quả dẫn đến kích thước batch tăng cùng với mô tả văn bản được làm phong phú bởi LLM mang lại những cải thiện đáng kể qua cả tác vụ truy xuất văn bản-âm thanh và âm thanh-văn bản trên cả hai tập dữ liệu. Đối với recall top 1 (R@1), FLAP ở hàng 5 không có fusion đặc trưng hoạt động tốt hơn trên đa số các tác vụ so với kết quả tốt nhất trước đó có fusion đặc trưng ở hàng 6 (36.2 đến 40.4 cho văn bản-âm thanh và 45.0 đến 51.5 cho âm thanh-văn bản trên AudioCaps, 17.2 đến 17.4 cho văn bản-âm thanh trên Clotho, với ngoại lệ trên 24.2 đến 21.6 cho âm thanh-văn bản trên Clotho). Trên cùng thiết lập fusion đặc trưng, FLAP ở hàng 10 tiếp tục vượt trội hơn kết quả tốt nhất trước đó ở hàng 6 trên tất cả các tác vụ (36.2 đến 41.5 cho văn bản-âm thanh và 45.0 đến 53.0 cho âm thanh-văn bản trên AudioCaps, 17.2 đến 20.3 cho văn bản-âm thanh và 24.2 đến 25.5 cho âm thanh-văn bản trên Clotho). Theo hiểu biết của chúng tôi, những kết quả này cũng phục vụ như hiệu suất tốt nhất hiện tại trên các tác vụ truy xuất âm thanh-văn bản và văn bản-âm thanh cho AudioCaps và Clotho.

Bảng 3. Kết quả Thí nghiệm về Fusion Đặc trưng và Tăng cường Văn bản với Mô hình Ngôn ngữ Lớn (LLM).*LƯU Ý: FLAP sử dụng cùng tập dữ liệu như LS-CLAP [16], loại trừ AudioStock do tính không khả dụng của nó.

7. KẾT LUẬN

Trong bài báo này, chúng tôi giới thiệu Tiền Huấn Luyện Ngôn Ngữ-Âm Thanh Nhanh (FLAP) nơi học tương phản gặp che giấu. FLAP dẫn đến hiểu biết âm thanh tốt hơn, hiệu suất tác vụ, và cho phép học hiệu quả và hiệu suất trên các phương thức chuỗi như âm thanh và video. Ngoài ra, tái tạo âm thanh và tăng cường mô tả văn bản được làm phong phú bởi các mô hình ngôn ngữ lớn (LLM) cũng được nghiên cứu. Che giấu hiệu quả giảm cả dấu chân tính toán và bộ nhớ cho các mẫu huấn luyện, do đó cho phép kích thước batch lớn hơn cho học tương phản. Tăng cường văn bản từ LLM tiếp tục làm phong phú các mô tả văn bản cho tín hiệu âm thanh và tạo ra các phong cách viết nhất quán hơn. Kết hợp cả hai, FLAP mang lại hiệu suất mạnh mẽ trên các tác vụ truy xuất âm thanh-văn bản với đánh giá trên các điểm chuẩn AudioCaps và Clotho. Các kỹ thuật trong FLAP linh hoạt và có thể áp dụng cho học biểu diễn trong các phương thức chuỗi như văn bản, âm thanh và video.

8. TÀI LIỆU THAM KHẢO

[1] Yoshua Bengio, Aaron C. Courville, và Pascal Vincent, "Unsupervised feature learning and deep learning: A review and new perspectives," CoRR, vol. abs/1206.5538, 2012.

[2] Linus Ericsson, Henry Gouk, Chen Change Loy, và Timothy M. Hospedales, "Self-supervised representation learning: Introduction, advances and challenges," CoRR, vol. abs/2110.09327, 2021.

[3] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, và Thomas Brox, "Discriminative unsupervised feature learning with convolutional neural networks," trong Advances in Neural Information Processing Systems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, và K.Q. Weinberger, Eds. 2014, vol. 27, Curran Associates, Inc.

[4] Xinlei Chen, Saining Xie, và Kaiming He, "An empirical study of training self-supervised vision transformers," trong ICCV, 2021.

[5] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross Girshick, "Masked autoencoders are scalable vision learners," trong CVPR, 2022.

[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova, "BERT: pre-training of deep bidirectional transformers for language understanding," trong Proc. NAACL-HLT, 2019.

[7] Yinhan Liu, Myle Ott, và Naman Goyal et al., "RoBERTa: A robustly optimized BERT pretraining approach," CoRR, vol. abs/1907.11692, 2019.

[8] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, và Christoph Feichtenhofer, "Masked autoencoders that listen," trong Advances in Neural Information Processing Systems, 2022.

[9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, và Abdelrahman Mohamed, "HuBERT: self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.

[10] Ashish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh, Debapriya Banerjee, và Fillia Makedon, "A survey on contrastive self-supervised learning," Technologies, vol. 9, no. 1, pp. 2, 2020.

[11] Alec Radford, Jong Wook Kim, và Chris Hallacy et al., "Learning transferable visual models from natural language supervision," CoRR, vol. abs/2103.00020, 2021.

[12] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, và Christoph Feichtenhofer, "VideoCLIP: Contrastive pre-training for zero-shot video-text understanding," trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 6787–6800.

[13] Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, và Alexander Hauptmann, "Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models," trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 2443–2459.

[14] Mandela Patrick, Po-Yao Huang, Ishan Misra, Florian Metze, Andrea Vedaldi, Yuki M. Asano, và João F. Henriques, "Space-time crop & attend: Improving cross-modal video representation learning," trong 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021. 2021, pp. 10540–10552, IEEE.

[15] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, và Huaming Wang, "CLAP: learning audio concepts from natural language supervision," trong ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1–5.

[16] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, và Shlomo Dubnov, "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation," trong IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, 2023.

[17] R. Hadsell, S. Chopra, và Y. LeCun, "Dimensionality reduction by learning an invariant mapping," trong 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), 2006, vol. 2, pp. 1735–1742.

[18] Ting Chen, Simon Kornblith, Mohammad Norouzi, và Geoffrey E. Hinton, "A simple framework for contrastive learning of visual representations," CoRR, vol. abs/2002.05709, 2020.

[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, và Ross Girshick, "Momentum contrast for unsupervised visual representation learning," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 9729–9738.

[20] Ashish Vaswani, Noam Shazeer, và Niki Parmar et al., "Attention is all you need," trong Proc. NeurIPS, 2017.

[21] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, và Michael Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," trong Advances in Neural Information Processing Systems, 2020.

[22] Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh, Jitendra Malik, và Christoph Feichtenhofer, "MA ViL: masked audio-video learners," trong Advances in Neural Information Processing Systems, 2023.

[23] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, và Juan Pablo Bello, "Wav2CLIP: learning robust audio representations from CLIP," trong Proc. ICASSP, 2022.

[24] Zhan Tong, Yibing Song, Jue Wang, và Limin Wang, "VideoMAE: masked autoencoders are data-efficient learners for self-supervised video pre-training," Advances in neural information processing systems, vol. 35, pp. 10078–10093, 2022.

[25] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, và Kaiming He, "Masked autoencoders as spatiotemporal learners," trong NeurIPS, 2022.

[26] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, và Kaiming He, "Scaling language-image pre-training via masking," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 23390–23400.

[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al., "LLaMA: open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

[28] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing, "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality," https://lmsys.org/blog/2023-03-30-vicuna/, 2023.

[29] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto, "Stanford alpaca: An instruction-following llama model," https://github.com/tatsu-lab/stanford_alpaca, 2023.

[30] Soham Deshmukh, Benjamin Elizalde, và Huaming Wang, "Audio retrieval with wavtext5k and clap training," arXiv preprint arXiv:2209.14275, 2022.

[31] A.S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, và S. Albanie, "Audio retrieval with natural language queries: A benchmark study," trong IEEE Transactions on Multimedia, 2022.

[32] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D. Plumbley, và Wenwu Wang, "On metric learning for audio-text cross-modal retrieval," arXiv preprint arXiv:2203.15537, 2022.

[33] Aaron van den Oord, Yazhe Li, và Oriol Vinyals, "Representation learning with contrastive predictive coding," arXiv preprint arXiv:1807.03748, 2018.

[34] Jort F. Gemmeke, Daniel P. W. Ellis, và Dylan Freedman et al., "AudioSet: an ontology and human-labeled dataset for audio events," trong Proc. ICASSP, 2017.

[35] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, và Quoc V Le, "SpecAugment: a simple data augmentation method for automatic speech recognition," arXiv preprint arXiv:1904.08779, 2019.

[36] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica, "Judging LLM-as-a-judge with MT-bench and chatbot arena," CoRR, vol. abs/2306.05685, 2023.

[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample, "LLaMA: open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.

[38] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, và Gunhee Kim, "AudioCaps: generating captions for audios in the wild," trong Proc. NAACL-HLT, 2019.

[39] Konstantinos Drossos, Samuel Lipping, và Tuomas Virtanen, "Clotho: an audio captioning dataset," trong Proc. ICASSP, 2020.

[40] Meta AI, "fvcore: Collection of common code that's shared among different research projects in fair computer vision team.," https://github.com/facebookresearch/fvcore.

[41] Diederik P Kingma và Jimmy Ba, "Adam: A method for stochastic optimization," trong Proc. ICLR, 2014.
