# 2303.13755.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2303.13755.pdf
# File size: 5978427 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparsiﬁner: Learning Sparse Instance-Dependent Attention
for Efﬁcient Vision Transformers
Cong Wei1*Brendan Duke1,3*Ruowei Jiang3Parham Aarabi1,3Graham W. Taylor2,4Florian Shkurti1,4
1University of Toronto2University of Guelph3Modiface, Inc .4Vector Institute
Abstract
Vision Transformers (ViT) have shown their competitive ad-
vantages performance-wise compared to convolutional neu-
ral networks (CNNs) though they often come with high com-
putational costs. To this end, previous methods explore dif-
ferent attention patterns by limiting a ﬁxed number of spa-
tially nearby tokens to accelerate the ViT’s multi-head self-
attention (MHSA) operations. However, such structured at-
tention patterns limit the token-to-token connections to their
spatial relevance, which disregards learned semantic con-
nections from a full attention mask. In this work, we pro-
pose a novel approach to learn instance-dependent atten-
tion patterns, by devising a lightweight connectivity predic-
tor module to estimate the connectivity score of each pair
of tokens. Intuitively, two tokens have high connectivity
scores if the features are considered relevant either spa-
tially or semantically. As each token only attends to a small
number of other tokens, the binarized connectivity masks
are often very sparse by nature and therefore provide the
opportunity to accelerate the network via sparse computa-
tions. Equipped with the learned unstructured attention pat-
tern, sparse attention ViT (Sparsiﬁner) produces a superior
Pareto-optimal trade-off between FLOPs and top-1 accu-
racy on ImageNet compared to token sparsity. Our method
reduces 48%69% FLOPs of MHSA while the accuracy
drop is within 0.4%. We also show that combining attention
and token sparsity reduces ViT FLOPs by over 60%.
1. Introduction
Vision Transformers (ViTs) [13] have emerged as a domi-
nant model for fundamental vision tasks such as image clas-
siﬁcation [13], object detection [3], and semantic segmen-
tation [6, 7]. However, scaling ViTs to a large number of
tokens is challenging due to the quadradic computational
complexity of multi-head self-attention (MHSA) [35].
This is particularly disadvantageous for large-scale vi-
sion tasks because computing on high-resolution and high-
*Equal contribution.
(c)  Axial Attention Pattern (Fixed) (d)  Proposed Instance -Dependent 
Attention Pattern (Dynamic)(b)  Window Attention Pattern (Fixed)
Query Patch
Attended Patch
(a)  Local Attention Pattern (Fixed)
Input ImageFigure 1. Comparison of Sparsiﬁner and ﬁxed attention patterns.
Twins [10] (a), Swin [22] (b), and Axial [16] (c) address quadratic
MHSA complexity using ﬁxed attention patterns, which does not
consider the instance-dependent nature of semantic information in
images. To address this, we propose Sparsiﬁner (d): an efﬁcient
module for sparse instance-dependent attention pattern prediction.
dimensionality inputs is desirable. For example, input
modalities such as video frames and 3D point clouds have
a large number of tokens even for basic use cases. Novel
algorithms are needed to continue to scale ViTs to larger,
more complex vision tasks.
Prior works have largely taken two approaches to im-
prove the computational efﬁciency of ViTs: token prun-
ing and using ﬁxed sparse attention patterns in MHSA.
Token pruning methods [27] reduce the number of to-
kens by a ﬁxed ratio called the keep rate, but accuracy
degrades quickly when pruning early layers in the net-
work [15, 30, 31]. For example, introducing token prun-
ing into shallower layers of EViT [15] causes a signiﬁ-
cant3:16% top-1 accuracy drop on ImageNet [12]. This is-arXiv:2303.13755v1  [cs.CV]  24 Mar 2023

--- PAGE 2 ---
sue is due to the restriction of pruning an entire token, which
account to pruning an entire row and column of the attention
matrix at once. One way to alleviate this is to prune indi-
vidual connectivities of the attention matrix instead of en-
tire tokens. Existing methods that take this attention matrix
connectivity-pruning approach use ﬁxed sparse attention
patterns [8]. For example, local and strided ﬁxed attention
patterns are used [8, 14], in combination with randomly-
initialized connectivities [40]. However, such ﬁxed atten-
tion patterns limit the the capacity of the self-attention con-
nections to a ﬁxed subset of tokens (Fig. 1). These atten-
tion patterns’ ﬁxed nature is less effective compared with
the direct communication between tokens in full self atten-
tion. For example, Swin transformer [21, 22] has a limited
receptive ﬁeld at shallower layers and needs many layers to
model long-range dependencies. And BigBird [40] needs to
combine multiple ﬁxed attention patterns to achieve good
performance. Rather, it is desirable to design sparse at-
tention algorithms that mimic full self attention’s instance-
dependent nature [35], thereby capturing the variable distri-
bution of semantic information in the input image content.
To address aforementioned challenges, we propose a
method called Sparsiﬁner that learns to compute sparse
connectivity patterns over attention that are both instance-
dependent and unstructured. The instance-dependent nature
of the attention pattern allows each token to use its lim-
ited attention budget of nonzero elements more efﬁciently
compared to ﬁxed sparse attention patterns. For example,
in attention heads that attend to semantic rather than posi-
tional content [35, 36], tokens containing similar semantic
information should be considered to have high connectiv-
ity scores despite their spatial distance. Similarly, nearby
tokens with irrelevant semantic relation should have lower
connectivity scores despite their spatial proximity. Further-
more, Sparsiﬁner improves attention pattern ﬂexibility com-
pared to token pruning by pruning individual connectivities,
instead of entire rows and columns of the attention matrix.
This allows Sparsiﬁner to reduce FLOPs in the early layers
of the network without incurring signiﬁcant top-1 accuracy
degradation (§4). By pruning individual connectivities de-
pendent on image content, Sparsiﬁner generalizes prior ap-
proaches to sparsifying MHSA in ViTs, and in doing so pro-
duces a favourable trade-off between accuracy and FLOPs.
Our contributions can be summarized as:
• We propose a novel efﬁcient algorithm called Spar-
siﬁner to predict instance-dependent sparse attention
patterns using low-rank connectivity patterns. Our in-
vestigation into instance-dependent unstructured spar-
sity is to the best of our knowledge novel in the context
of ViTs.
• We show that such learned unstructured attention spar-
sity produces a superior Pareto-optimal tradeoff be-tween FLOPs and top-1 accuracy on ImageNet com-
pared to token sparsity. Furthermore, we show that
Sparsiﬁner is complementary to token sparsity meth-
ods, and the two approaches can be combined to
achieve superior performance-accuracy tradeoffs.
• We propose a knowledge distillation-based approach
for training Sparsiﬁner from pretrained ViTs using a
small number of training epochs.
2. Related Work
Efﬁcient Attention — Developing an efﬁcient attention
mechanism for high resolution image encoding is the fo-
cus of this work. Efﬁcient attention mechanisms have been
widely studied in NLP tasks to model long sequences. They
can be categorized as follows: Low-rank methods such as
Linformer [37] use a low-rank projection to linearize the
multi-head attention operation. Linformer [37] replaces the
scaled dot product with linear attention that approximates
the attention with a low-rank matrix. Kernelization , includ-
ing Performer [9], Linear Transformers [18], and Random
Feature Attention [24] use kernels to avoid explicitly com-
puting the attention matrix. Sparse attention with ﬁxed
attention patterns [8, 10, 16, 23, 25]. This type of tech-
nique sparsiﬁes the attention matrix by limiting the ﬁeld of
view to predeﬁned patterns such as local and strided win-
dows. Similarity and clustering-based methods includ-
ing Routing Transformer [29], Reformer [19], and Sinkhorn
Transformer [33]. These models measure token relevance
by sorting or clustering and then assign tokens to buck-
ets for within-bucket attention. Neural memory mecha-
nisms such as Set Transformer [20], Compressive Trans-
former [26], and Longformer [1]. These use extra global
tokens that gather long-range information as a model mem-
ory.
Vision Transformers — Recent progress has demon-
strated that variants of Transformers [35] can also be com-
petitive alternatives to CNNs and achieve promising results
on different vision tasks. In addition to image classiﬁca-
tion, Transformers have also been applied to various vision
tasks, including object detection [4, 11, 44, 46], image gen-
eration [5,23], and video processing [42,45]. Vision Trans-
former (ViT) [13] splits images as small patches and treats
the patches as the input word tokens. ViT shows better per-
formance than CNN-type models with sufﬁcient extensive
training data. DeiT [34] incorporates knowledge distillation
techniques into ViT training so that we can train a competi-
tive Transformer using only ImageNet-1k [12]. LV-ViT [17]
further improves the performance of ViT by introducing a
new training objective named token labelling. Most of these
methods have quadratic complexity of self-attention with
respect to the input image size.
Efﬁcient Vision Transformers — There is a thrust to

--- PAGE 3 ---
model long sequences of image patches at much higher
resolutions. Recent works such as Pyramid Vision Trans-
former (PVT) [38], Swin-Transformer [22], T2T-ViT [39],
and Vision Longformer (ViL) [43] apply transformer lay-
ers on different resolution scales by stacking a pyramid of
ViTs to form a multi-scale architecture. To achieve linear
complexity, Swin-Transformer [22] uses shifted local win-
dow attention. Vision Longformer [43] adapts the local at-
tention pattern with the global memory tokens from Long-
former [1]. TimeSformer [2] applies multiple attentions,
each along a single axis of the input video. Those methods
all leverage ﬁxed, predeﬁned attention patterns to reduce
the quadratic cost. In contrast, our method generates sparse
dynamic attention patterns based on the input content. An-
other group of works reduce the number of tokens by prun-
ing [15,27,32], or merging tokens [28,30,41]. Recent work,
DynamicViT [27] and EViT [15] study unstructured token
sparsiﬁcation by gradually dropping tokens in the inference
of ViTs [13]. However, quadratic attention cost remains in
early layers where input tokens cannot be largely sparsiﬁed.
Our method instead prunes connectivities at every layer, al-
lowing complexity savings at early layers.
3. Method
Our proposed method to learn sparse attention patterns,
Sparsiﬁner, consists of a normal ViT [13] as the backbone
with sparse attention modules at each layer. Our sparse at-
tention module consists of a connectivity mask predictor
and a sparse multi-head self-attention (MHSA) module. In
both training and inference, we generate a sparse connectiv-
ity mask by restricting the number of connections predicted
by the mask predictor according to a hyperparameter bud-
get sizeB. Following this, a sparse MHSA module is used
to perform sparse attention based on the connectivity mask.
The sparse MHSA module implements an efﬁcient compu-
tation using a sparse element-wise product between the full
attention map and the sparse connectivity mask to produce
a sparse reconstructed attention map. Then, a sparse-dense
attention-value product between the sparse reconstructed at-
tention map and the value matrix produces the output of the
sparse MHSA module.
For clarity, in the following we describe MHSA for a
single attention head only. In practice, we apply the pro-
posed method to each attention head in a ViT. We concate-
nate the resulting output values from all attention heads and
feed them to a linear layer to produce the input to the next
transformer layer [35].
ViT Architecture and Na ¨ıve MHSA — We base our
method on the existing ViT model architecture [13] and
na¨ıve implementation of MHSA [35]. A ViT ﬁrst tok-
enizes an input image I2Rhw3into a set of nto-
kensX2Rnd, each with dimension d. Each token con-
sists of a patch embedding, retrieved via linear projectionof the non-overlapping image patches, and a positional en-
coding. The resulting sequence of tokens is then fed into
MHSA modules to compute the attention matrix A2Rnn
as the product of query Q2Rnd=XlWQand keyK2
Rnd=XlWKmatrices, where the learned projection ma-
tricesWQ2RddandWK2Rddcompute query and
key as projections of the input Xl2Rndto layerl. Na¨ıve
MHSA then computes the attention matrix Aas the softmax
of outer product of query and key matrices as shown in the
left part of Fig. 2.
Connectivity Mask Predictor — To enable instance-
dependent and meaningful attention patterns while limiting
the number of connections, we train a connectivity mask
predictor and achieve sparsity by thresholding. Speciﬁ-
cally, we ﬁrst compute the low-rank approximation Adown2
Rnndownof the attention matrix A
Adown=softmaxQ(WdownK)>
p
d
; (1)
which we sparsify by thresholding:
~Adown
ij=(
Adown
ij ifAdown
ij>
0 otherwise: (2)
In the low-rank attention computation (Eq. 1), we ﬁrst
down-project the token dimension of key matrix Kto
a lower dimension ndown using a learned projection ma-
trixWdown2Rndownn. Then, a low-rank approximation
of the attention matrix is computed from the outer product
of query and down-projected key matrices. Note that in the
low-rank attention sparsiﬁcation (Eq. 2), with a sparse ma-
trix representation we need not explicitly store the zeros.
Next, the connectivity mask predictor (Eq. 3) performs a
sparse matrix multiplication of a sparse up-projection ma-
trixWup2Rndownnfollowed by binarization. This pro-
duces an up-projected sparse connectivity mask:
M=1
Top-k(~AdownWup)
: (3)
Here, ~AdownWupdenotes sparse-sparse matrix multiplica-
tion, which is efﬁciently computed. Our key insight is that
the post-softmax low-rank attention matrix (Eq. 1) should
naturally be sparse. We show an example in Fig. 7.
We apply top- kon the up-projected sparse attention ma-
trix ~AdownWup, which is the attention connectivity score
map.kis set to the budget size B. We discard zero val-
ues and binarize to produce a sparse low-rank connectivity
maskM2Rnn. We indicate binarization by the indicator
function 1[]in the connectivity mask predictor (Eq. 3).
Sparse MHSA — In Fig. 2, we compare our method
to na ¨ıve MHSA [35] and Linformer [37] in a single head
example. In our method, guided by the sparse connectiv-
ity maskM, we compute only the nonzero elements of the

--- PAGE 4 ---
Figure 2. Single head comparison of the MHSA module for na ¨ıve MHSA [35], Linformer [37], and Sparsiﬁner. Na¨ıve MHSA incurs
quadratic O(n2)complexity in the number of tokens n.Linformer reduces the complexity to linear O(nndown)by using a projection of
the key and value matrices to projected key Kproj2Rndowndand value Vproj2Rndowndmatrices in a low-rank approximation of the
attention matrix. Sparsiﬁner ’s key insight is to use the low-rank approximation to learn a sparse connectivity mask M2Rnnand sparse
up-projection basis Wup. Using sparse matrix multiplication, Sparsiﬁner reduces overall MHSA FLOPs relative to Linformer without
restricting the attention matrix to be low rank. Note that in the rightmost column only (Sparsiﬁner), the attention matrix Ais not explicitly
constructed, and rather is used to represent sparse attention reconstruction (Eq. 5).
sparse full-rank attention matrix ~A. In order to ensure com-
putational efﬁciency, we want to have both a sparse up-
projection and a sparse low-rank attention matrix. This is
equivalent to reconstructing the sparse attention matrix ~A
as an afﬁne combination over a set of sparse basis vectors
using a sparse coefﬁcient vector:
~Aij=softmaxQK>
p
d
ijiffMij= 1: (4)
Another way of formulating the sparse full-rank attention
matrix is as a sparse element-wise product of the sparse con-
nectivity mask Mwith the full-rank attention matrix:
~A=MsparseA: (5)
Here,sparse is the sparse element-wise product operator,
which skips multiplications by zero. Therefore, computing
the sparse full-rank attention matrix ~A(Eq. 4) costs only
as many FLOPs as there are nonzero elements in the con-
nectivity mask M. In particular, computing the sparse full-
rank attention matrix costs less than the O(n2d)required by
na¨ıve MHSA.
Finally, Sparsiﬁner computes a sparse attention-value
product using the sparse full-rank attention matrix ~Aand
the value matrix V:
Xl+1=~AV: (6)
By computing the sparse full-rank attention matrix ~A
(Eq. 4) guided by the sparse connectivity mask, and then
computing the sparse attention-value product, we remove
theO(n2d)complexity required by the na ¨ıve MHSA op-
eration. Instead, the sparse MHSA operation in Sparsiﬁner
performs a number of operations proportional to the number
of nonzero elements in the connectivity mask M.Objective functions — The training of Sparsiﬁner in-
cludes training the attention connectivity predictor modules
and ﬁne-tuning the backbone to make it adapt to sparse at-
tention. We adopt the standard cross-entropy loss:
Lcls=CrossEntropy (ypred;y) (7)
where ypredis the predicted class distribution and yis the
ground-truth class distribution.
To minimize the inﬂuence on performance of the atten-
tion sparsiﬁcation process, we use a pre-trained backbone
model as a teacher within a knowledge distillation frame-
work. First, we make the tokens at the last layer close to the
ones of the teacher model, where xandxteachare the tokens
after the last block of the Sparsiﬁner and the teacher model,
respectively.
Ltoken
distill=MSE(x;xteach): (8)
Second, we minimize the difference of Sparsiﬁner and the
teacher model’s predictions via KL divergence:
Lcls
distill=KL(ypredjjyteach): (9)
Third, we want the connectivity score map generated by
the connectivity mask predictor to be a good low-rank ap-
proximation of the teacher attention, which can be viewed
as knowledge distillation of the attention map. We mini-
mize the Euclidean distance between them:
Lattn
distill=MSE(~AdownWup;Ateach): (10)
Finally, to enforce the sparsity of the up-projection ma-
trix, we use the L2regularization. We tried L1regulariza-
tion but found that L2gives better training convergence with
sufﬁcient sparsity in practice.
Lspa=X
i(wup
i)2(11)

--- PAGE 5 ---
The full training objective combines all objectives:
L=Lcls+token
distillLtoken
distill+cls
distillLcls
distill+attn
distillLattn
distill+spaLspa
(12)
Where we set the weight decay as 0:05in the optimizer in-
stead of directly adding spaLspato the objective.
4. Experiments and Results
Implementation details — We train all of the mod-
els on the ImageNet dataset [12]. By default, the con-
nectivity mask predictor module is incorporated into ev-
ery layer of DeiT-S [34] and LV-ViT-S [17]. In all
of our experiments, we set the reduced dimension ndown
to32andto0:05which ensures 87% sparsity ratio of
the basis coefﬁcient. The attention budget Bis in the
range (0;number of tokens ]. BudgetBis directly deter-
mined by the attention keep rate in (0;1]as the ceiling of
the keep rate multiplied by the total number of tokens.
We follow most of the training techniques used in DeiT-S
and LV-ViT-S. We use pre-trained ViT models to initialize
the backbone models. To improve speed of convergence,
we propose a two-phase training strategy. In the ﬁrst phase,
we freeze the backbone model and train the connectivity
mask predictor module with attention distillation loss and
L2 regularization only. Speciﬁcally, we set token
distill = 0:0,
cls
distill = 0:0,attn
distill = 1:0and we also apply a threshold
1e-2 on basis Wupto ensure 90% sparsity. We found that
this setting helps the connectivity mask predictor to learn
Wupquickly and loss converges within 5 epochs. In the
second phase, we jointly train the backbone model and the
connectivity mask predictor module for another 40 epochs.
we settoken
distill= 0:5,cls
distill= 0:5,attn
distill= 0:0. More details
can be found in the supplementary material.
Sparse connectivities and attention visualization — In
order to qualitatively investigate the quality of Sparsiﬁner’s
sparse attention approximation, we visualize its connectiv-
ity mask and sparse reconstructed attention map (Fig. 3).
We show the original input image and the connectivity mask
of the query patch, where the dark regions represent tokens
that are not attended to by the query patch token. For each
attention head, Sparsiﬁner generates a corresponding con-
nectivity mask. We ﬁnd that the connectivity mask acts as
a region proposal mechanism, which allows different atten-
tion heads to locate different informative tokens and gather
diverse semantic information. Furthermore, we visualize
the sparse attention map efﬁciently generated using the con-
nectivity mask and compare it with the full attention map.
We ﬁnd that the sparse attention map retains all of the high-
est connectivity values, while discarding lower connectiv-
ity values. Hence the visualizations show that Sparsiﬁner
retains the most salient relations for a given token, while
discarding noisy background relations.
Comparison with token pruning — We train and eval-ModelTok.
keep
rateAtt.
keep
rateMHSA
(MFLOPs)Top-1
Acc.
(%)
DeiT-S [34] 1:0 1 :0 357 :7 79 :8
EViT [15] 0:7 1 :0 193 :1( 46%) 79:5
DynamicViT [27] 0:7 1 :0 193 :1( 46%) 79:3
Sparsif-EViT (ours) 0:7 0 :25 113 :3( 68%) 79:5
Sparsiﬁner (ours) 0:7 0 :25 113 :3( 68%) 79:3
EViT [15] 0:5 1 :0 149 :1( 58%) 78:5
DynamicViT [27] 0:5 1 :0 149 :1( 58%) 77:3
Sparsif-EViT (ours) 0:5 0 :25 86:6( 76%) 78:7
Sparsiﬁner (ours) 0:5 0 :25 86:6( 76%) 78:4
LV-ViT-S [17] 1:0 1 :0 476 :9 83 :3
EViT-LV-S [15] 0:7 1 :0 256 :0( 46%) 83:0
EViT-LV-S [15] 0:5 1 :0 198 :8( 58%) 82:5
DynViT-LV-S [27] 0:7 1 :0 256 :0( 46%) 83:0
DynViT-LV-S [27] 0:5 1 :0 198 :8( 58%) 82:0
Sparsif-LV-S (ours) 1:0 0 :5 339 :7( 29%) 83:4
Sparsif-LV-S (ours) 1:0 0 :25 221 :7( 54%) 83:3
Sparsif-LV-S (ours) 1:0 0 :1149:5( 69%) 82:8
Table 1. Comparison with token pruning methods on DeiT-S [34]
and LV-ViT-S [17] base models. Token pruning methods such
as EViT [15] and DynamicViT [27] prune tokens at ﬁxed layers.
We show that token pruning methods combine with Sparsiﬁner’s
sparse attention connectivities to produce a complementary effect.
Sparsiﬁner combined with EViT [15] achieves a 68% reduction in
FLOPs compared with the DeiT-S [34] baseline, while maintain-
ing a top-1 accuracy of 79:5%. Hence Sparsiﬁner achieves the
same top-1 accuracy as EViT [15] with signiﬁcantly better MHSA
FLOPs reduction. The input resolution is 224224.
uate Sparsiﬁner on ImageNet and compare to state-of-the-
art token pruning baselines (Tab. 1). Since our research
question addresses the problem of reducing MHSA com-
plexity, we report trade-offs between top-1 accuracy on Im-
ageNet and computation in terms of MHSA FLOPs. We
compare Sparsiﬁner against baselines by adjusting two hy-
perparameters: token and attention keep rate. The token
keep rate is the fraction of tokens kept in the network at pre-
determined layers where pruning occurs, which we set ac-
cording to established token pruning baselines [15,27]. The
attention keep rate is the fraction of attention connectivities
at any given MHSA layer, as determined by the connectivity
mask predictor (Eq. 3). Hence, varying the attention keep
rate reduces FLOPs without necessitating removal of tokens
as in token pruning. But both techniques can be combined
to achieve complementary effects.
To provide a variety of comparisons we experiment with
adding token pruning and Sparsiﬁner to two common base-
line ViT models: DeiT [34] and LV-ViT [17]. On both
models, Sparsiﬁner achieves signiﬁcant computation saving
while maintaining a relatively modest drop in top-1 accu-

--- PAGE 6 ---
4 Heads 4 Heads
4 Heads
Figure 3. Visualization of connectivity mask (b) with sparse (c) and full (d) attention maps for a given query patch (a). In the heatmaps,
the blue darker color indicates lower, and yellow brighter color indicates higher attention value. Here we visualize the attention maps for
only 3 layers and 4 heads of the ViT. For the dog image (top) we visualize layers 3–5, while for the bear image (bottom) we visualize
layers 6–8. We observe that in earlier layers the attention map focuses more on positional information such as nearby tokens, while in
later layers semantic relations with distant tokens are more important. For each query patch indicated by a yellow square in the input
image, Sparsiﬁner predicts a sparse connectivity mask using a low-rank approximation to full attention. Using the sparse connectivity
mask, Sparsiﬁner efﬁciently computes a sparse full-rank attention matrix. By comparison with the rightmost full attention, sparse attention
retains all of the most salient relations with the given query patch, while discarding redundant or noisy information in the rest of the image.
racy. For example, LV-ViT-S [17] trained with Sparsiﬁner
with an attention keep rate of 0:25reduces the MHSA
FLOPs by 53:5% while maintaining the top-1 accuracy of
the baseline LV-ViT-S model on ImageNet. When used
in combination with token pruning, Sparsiﬁner achieves an
even superior reduction in MHSA FLOPs while maintain-
ing comparable top-1 accuracy to EViT, and superior top-1
accuracy to DynamicViT.
Varying MHSA attention budget — We varied the at-
tention budget of MHSA in order to investigate the tradeoff
between MHSA FLOPs and top-1 accuracy for Sparsiﬁner-
S (Tab. 2). The results evaluated on ImageNet show that
Sparsiﬁner-S produces a superior Pareto frontier compared
with previous approaches (Fig. 4). In particular, Sparsiﬁner-
S models with attention budgets of 40and above achieved
top-1 accuracy within 0:1% of the full-rank DeiT-S [34]
model, while using 58:8% fewer FLOPs in MHSA. Fur-
thermore, Sparsiﬁner-S models with high attention budgets
of79and above achieved superior top-1 accuracy compared
with the full-rank DeiT-S [34] model, while using fewer
FLOPs in MHSA. This suggests that Sparsiﬁner’s sparse
full-rank attention reconstruction mechanism induces a use-
ful regularization effect that improves model generalization.
Accelerating ViT on high-resolution images — To
show the effectiveness of our method on larger input size,Att. keep rate Att. num.MHSA
(MFLOPs)Top-1
Acc (%)
1:0(DeiT-S [34]) 197 357 :7 79 :82
0:9 178 396 :8 80 :02
0:8 158 360 :6 79 :97
0:7 138 324 :6( 9%) 79:96
0:6 119 290 :3( 19%) 79:98
0:5 99 254 :2( 29%) 79:94
0:4 79 218 :0( 39%) 79:92
0:3 60 183 :6( 49%) 79:83
0:2 40 147 :5( 59%) 79:71
0:1 20 111 :4( 69%) 79:42
0:05 10 93 :3( 74%) 78:75
0:01 2 78 :9( 78%) 73:03
Table 2. Effect of attention budget on FLOPs and top-1 accuracy.
Here the “keep rate” refers to the number of attention connec-
tivities retained at each layer. All other attention connectivities
in the sparse full-rank attention matrix (Eq. 4) are set to zero.
When keeping only 10attention connectivities, Sparsiﬁner pro-
duces a top-1 accuracy reduced by only 1:0% compared to the
full-attention baseline DeiT-S [34], but with a 73:9% reduction in
FLOPs. The input resolution is 224224.
we apply our method to DeiT-T [34] with 384384res-

--- PAGE 7 ---
100 150 200 250 300 350 400 450
MHSA MFLOPs7778798081828384ImageNet T op-1 Acc(%)
Spa-S/0.05Spa-S/0.1Spa-S/0.2Spa-S/0.3Spa-S/0.4 Spa-S/0.5Spa-S/0.6 Spa-S/0.7 Spa-S/0.8Spa-S/0.9 Spa-S/1.0
DeiT-S EViT-S/0.7
EViT-S/0.5DynamicViT-S/0.7
DynamicViT-S/0.5Spa-EViT-S/0.7/0.25
Spa-EViT-S/0.5/0.25LV-ViT-SEViT-LV-S/0.7
EViT-LV-S/0.5DynamicViT-LV-S/0.7
DynamicViT-LV-S/0.5Spa-LV-S/0.5Spa-LV-S/0.25
Spa-LV-S/0.1Spa-LV-M/0.25
Spa-LV-M/0.1
DynamicViT-LV-M/0.7Figure 4. MHSA computation (FLOPs) and top-1 accuracy trade-
offs on ImageNet. We compare Sparsiﬁner with the state-of-the-
art token pruning methods. Sparsiﬁner achieves superior trade-offs
compared to the baseline. We also report MHSA FLOPs and top-1
accuracy for Sparsiﬁner-S under varying attention keep rate.
ModelAtt.
keep
rateMHSA
(MFLOPs)Overall
(GFLOPs)Top-1
Acc
(%)
DeiT-T 1:0 1534 :1 3 :58 75 :45
Sparsiﬁner-T 0:5 851 :0( 45%) 2:89( 19%) 75:45
Sparsiﬁner-T 0:25 452 :9( 70%) 2:49( 30%) 75:35
Sparsiﬁner-T 0:1 240 :5( 84%) 2:28( 36%) 74:58
Table 3. Results on high resolution 384384 images. We ap-
ply Sparsiﬁner on DeiT-T [34] with resolution 384. We show that
Sparsiﬁner reduce the MHSA complexity of DeiT-T-384 [34] by
over 84% with modest accuracy drop. Since the number of to-
kens is quadratic in the resolution, Sparsiﬁner can reduce a larger
portion of MHSA complexity on high-resolution images.
olution (Tab. 3). When dealing with high-resolution im-
ages, due to quadratic complexity in the number of tokens,
MHSA becomes increasingly expensive compared to the
feedforward operations. We reduce the MHSA complexity
of the DeiT-T [34] model with 384384input by over 80%
with less than 1% accuracy drop. Our method shows a great
potential to accelerate ViT on even higher resolution images
where token quantity dominates the model complexity.
Low-rank: connectivities or attention? — Our ap-
proach raises a research question: does the utility of the
dense low-rank attention matrix come from its use as a con-Model MHSA (MFLOPs) Top-1 Acc (%)
Linformer [37] 246:73 77.54
Sparsiﬁner-S (ours) 224:04 79.79
Table 4. Comparison of sparse full-attention reconstruction with
low-rank attention reconstruction. Sparsiﬁner-S achieves a 2:1%
absolute percentage point improvement in top-1 accuracy com-
pared with Linformer [37].
(a) Query in input image
 (b) Full attention (na ¨ıve MHSA)
(c) Low-rank attention (Linformer)
 (d) Full-rank sparse att. (Sparsiﬁner)
Figure 5. Low-rank attention (Linformer) and full-rank sparse at-
tention (Sparsiﬁner) heatmaps. For a given query patch indicated
by a yellow square (a), we visualize its low-rank attention map
(Linformer) (c) and full-rank sparse attention map (Sparsiﬁner)
(d). Due to discarding the long tail of the attention matrix’s eigen-
spectrum, low-rank attention produces a coarse attention map. By
contrast, full-rank sparse attention bears closer resemblance to full
attention (b) with low-salience connectivities discarded.
nectivity mask? Or is it sufﬁcient to directly use the dense
low-rank attention matrix, foregoing the need to reconstruct
the sparse full-rank attention matrix, i.e., the Linformer [37]
approach? We answered this question by comparing the
top-1 accuracy of the two approaches (Tab. 4). In this exper-
iment, Sparsiﬁner-S and Linformer [37] were trained under
identical settings, differing only in the attention approxima-
tion method. Sparsiﬁner-S uses a reconstructed sparse full-
rank attention matrix, while Linformer uses the dense low-
rank attention matrix directly. In order to give both models
similar representational capacity, we set the low-rank di-
mension of Linformer [37] equal to the sparse attention bud-

--- PAGE 8 ---
Figure 6. Visualization of the up-projection matrix Wupof the ﬁrst 6layers of Sparsiﬁner-S, which we refer to here as a sparse basis. We
visualize 24dimensions of the sparse basis. Dark blue weights indicate low values, which are pruned after training so that only the bright
yellow weights are left over. Qualitatively, the sparse basis has a high level of sparsity, making sparse attention reconstruction efﬁcient.
Figure 7. Visualization of the sparse basis coefﬁcient of the 5th
attention head over 12layers of Sparsiﬁner-S. Dark blue regions
indicate low values that are pruned before sparse attention recon-
struction during inference, leaving only bright yellow coefﬁcients.
get of Sparsiﬁner-S. This enforces that the attention-value
product of both models’ MHSA has the same complexity.
Using the sparse full-rank attention matrix produces a
2:1% absolute percentage point improvement in top-1 ac-
curacy compared with Linformer. This improvement rein-
forces the superiority of using the low-rank query-key prod-
uct as a connectivity mask, rather than using the low-rank
attention matrix directly. Using the low-rank attention ma-
trix to directly compute the attention-value product with a
down-projected value discards the long tail of the full at-
tention matrix’s eigenspectrum [37]. In contrast, using the
low-rank query-key product as a connectivity mask reduces
computation by a different mechanism. By using a low-rank
connectivity mask to produce a sparse full-rank attention
matrix, the long-tail of the full attention matrix’s eigenspec-
trum is preserved. Based on the signiﬁcant improvement in
top-1 accuracy, we conclude that these long-tail eigenvalues
are important for model predictive quality in ViTs.
Low- and full-rank attention visualization — In or-
der to further illuminate the qualitative difference between
low- and full-rank attention in ViTs, we also present the
masked attention heatmap and the full attention heatmap of
the query patch (Fig. 5). We show that a connectivity mask
can accurately preserve key tokens that are highly related tothe query patch and remove the irrelevant ones. As a result,
the masked attention heatmap preserves structure and dis-
cards noise compared with the full attention heatmap. The
visualization results also validate that our Sparsiﬁner can
effectively approximate the full attention ViT.
Sparse low-rank basis and up-projection matrix visu-
alization — To demonstrate that the connectivity mask can
be computed by sparse-sparse matrix multiplication, we vi-
sualize the up-projection matrix Wupof the ﬁrst six layers
of Sparsiﬁner (Fig. 6). Because the reconstructed sparse
attention matrix is a combination of the up-projection ma-
trix’s weights, we refer to it as a sparse basis. We show that
Sparsiﬁner naturally learns a sparse basis of local regions
resembling 2D Gaussians. For a given token, the sparse
bases corresponding to object locations with salient seman-
tic and/or spatial information will activate. Since the sparse
attention reconstruction (Eq. 5) is a product of the sparse
low-rank attention matrix with the up-projection matrix, we
also visualize the post-softmax low-rank attention matrix.
Here we view the low-rank attention matrix as a sparse coef-
ﬁcient of the sparse basis (Fig. 7). Qualitatively, the sparse
coefﬁcient also exhibits a high degree of sparsity, further
validating the efﬁciency of the sparse attention reconstruc-
tion via sparse-sparse matrix multiplication.
5. Conclusions
We presented a novel computationally efﬁcient approach
to learn unstructured, instance-dependent attention in ViTs.
The development of sparse attention mechanisms such as
Sparsiﬁner opens the door to further research into accel-
erating sparse ViTs using software-hardware systems ap-
proaches. Sparsiﬁner shows the promise of sparse attention
for scaling ViTs to larger and more complex vision tasks.
But software-hardware systems approaches are needed to
realize its full potential. We hope that our work inspires
further research at the intersection of sparse algorithms for
ViTs and software-hardware systems approaches to support
those sparse algorithms.

--- PAGE 9 ---
References
[1] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
[2] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
space-time attention all you need for video understanding?
InICML , volume 2, page 4, 2021.
[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
End Object Detection with Transformers. In ECCV , 2020.
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213–229. Springer, 2020.
[5] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In International conference on machine
learning , pages 1691–1703. PMLR, 2020.
[6] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. 2022.
[7] Bowen Cheng, Alexander G. Schwing, and Alexander Kir-
illov. Per-pixel classiﬁcation is not all you need for semantic
segmentation. 2021.
[8] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
Generating long sequences with sparse transformers. In
arXiv:1904.10509 , 2019.
[9] Krzysztof Marcin Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,
and Adrian Weller. Rethinking attention with performers. In
ICLR , 2021.
[10] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-
ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.
Twins: Revisiting the design of spatial attention in vision
transformers. Advances in Neural Information Processing
Systems , 34:9355–9366, 2021.
[11] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.
Up-detr: Unsupervised pre-training for object detection with
transformers. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 1601–
1610, 2021.
[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. Imagenet: A Large-scale Hierarchical Image
Database. In CVPR , 2009.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is
Worth 16x16 Words: Transformers for Image Recognition at
Scale. In ICLR , 2021.
[14] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham
Aarabi, and Graham W. Taylor. SSTVOS: Sparse Spa-
tiotemporal Transformers for Video Object Segmentation. In
CVPR , 2021.[15] Qihua Feng, Peiya Li, Zhixun Lu, Chaozhuo Li, Zefang
Wang, Zhiquan Liu, Chunhui Duan, and Feiran Huang.
Evit: Privacy-preserving image retrieval via encrypted vi-
sion transformer in cloud computing. arXiv preprint
arXiv:2208.14657 , 2022.
[16] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim
Salimans. Axial attention in multidimensional transformers.
arXiv preprint arXiv:1912.12180 , 2019.
[17] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun
Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng. All tokens
matter: Token labeling for training better vision transform-
ers. Advances in Neural Information Processing Systems ,
34:18590–18602, 2021.
[18] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and
Franc ¸ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In International Confer-
ence on Machine Learning , pages 5156–5165. PMLR, 2020.
[19] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
former: The efﬁcient transformer. In International Confer-
ence on Learning Representations , 2020.
[20] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-
ungjin Choi, and Yee Whye Teh. Set transformer: A frame-
work for attention-based permutation-invariant neural net-
works. In International conference on machine learning ,
pages 3744–3753. PMLR, 2019.
[21] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu
Wei, and Baining Guo. Swin transformer v2: Scaling up
capacity and resolution. In CVPR , 2022.
[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021.
[23] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In International conference on machine
learning , pages 4055–4064. PMLR, 2018.
[24] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz,
Noah Smith, and Lingpeng Kong. Random feature atten-
tion. In International Conference on Learning Representa-
tions , 2021.
[25] Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong
Wang, and Jie Tang. Blockwise self-attention for long docu-
ment understanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 2555–2565,
Online, Nov. 2020. Association for Computational Linguis-
tics.
[26] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
Chloe Hillier, and Timothy P. Lillicrap. Compressive trans-
formers for long-range sequence modelling. In International
Conference on Learning Representations , 2020.
[27] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. DynamicViT: Efﬁcient Vision
Transformers with Dynamic Token Sparsiﬁcation. In Ad-
vances in Neural Information Processing Systems (NeurIPS) ,
2021.

--- PAGE 10 ---
[28] Cedric Renggli, Andr ´e Susano Pinto, Neil Houlsby, Basil
Mustafa, Joan Puigcerver, and Carlos Riquelme. Learn-
ing to merge tokens in vision transformers. arXiv preprint
arXiv:2202.12015 , 2022.
[29] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David
Grangier. Efﬁcient content-based sparse attention with rout-
ing transformers. Transactions of the Association for Com-
putational Linguistics , 9:53–68, 2021.
[30] Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa
Dehghani, and Anelia Angelova. TokenLearner: Adaptive
Space-Time Tokenization for Videos. In NeurIPS , 2021.
[31] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan
Guo, Chao Xu, and Dacheng Tao. Patch slimming for ef-
ﬁcient vision transformers. In CVPR , 2022.
[32] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan
Guo, Chao Xu, and Dacheng Tao. Patch slimming for ef-
ﬁcient vision transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12165–12174, 2022.
[33] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-
Cheng Juan. Sparse sinkhorn attention. In International
Conference on Machine Learning , pages 9438–9447. PMLR,
2020.
[34] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herve Jegou. Training
Data-efﬁcient Image Transformers & Distillation through
Attention. In ICML , 2021.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS . 2017.
[36] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,
and Ivan Titov. Analyzing multi-head self-attention: Spe-
cialized heads do the heavy lifting, the rest can be pruned. In
ACL, 2019.
[37] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020.
[38] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao
Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
Pyramid vision transformer: A versatile backbone for dense
prediction without convolutions. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 568–578, 2021.
[39] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 558–567,
2021.
[40] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big
Bird: Transformers for Longer Sequences. In NeurIPS ,
2020.
[41] Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo,
Wanli Ouyang, and Xiaogang Wang. Not all tokens are
equal: Human-centric visual analysis via token clusteringtransformer. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11101–
11111, 2022.
[42] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning
joint spatial-temporal transformations for video inpainting.
InEuropean Conference on Computer Vision , pages 528–
543. Springer, 2020.
[43] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu
Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-
former: A new vision transformer for high-resolution image
encoding. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 2998–3008, 2021.
[44] Minghang Zheng, Peng Gao, Renrui Zhang, Kunchang Li,
Xiaogang Wang, Hongsheng Li, and Hao Dong. End-to-end
object detection with adaptive clustering transformer. 2021.
[45] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher,
and Caiming Xiong. End-to-end dense video captioning with
masked transformer. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 8739–
8748, 2018.
[46] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable fdetrg: Deformable transform-
ers for end-to-end object detection. In International Confer-
ence on Learning Representations , 2021.
