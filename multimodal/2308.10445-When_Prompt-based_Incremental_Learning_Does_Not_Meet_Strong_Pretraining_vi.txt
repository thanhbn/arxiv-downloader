# Khi Học Tăng Cường Dựa Trên Prompt Không Đáp Ứng Được Việc Tiền Huấn Luyện Mạnh

Yu-Ming Tang1,3Yi-Xing Peng1,3Wei-Shi Zheng1,2,3*
1Trường Khoa học Máy tính và Kỹ thuật, Đại học Trung Sơn, Trung Quốc
2Phòng thí nghiệm Bành Thành, Thâm Quyến, Trung Quốc
3Phòng thí nghiệm Trọng điểm Trí tuệ Máy và Tính toán Tiên tiến, Bộ Giáo dục, Trung Quốc
{tangym9, pengyx23 }@mail2.sysu.edu.cn wszheng@ieee.org

Tóm tắt
Học tăng cường nhằm khắc phục hiện tượng quên thảm khốc khi huấn luyện mạng sâu từ các tác vụ tuần tự. Với hiệu quả học tập và hiệu suất ấn tượng, các phương pháp dựa trên prompt sử dụng một backbone cố định cho các tác vụ tuần tự bằng cách học các prompt đặc trung cho từng tác vụ. Tuy nhiên, các phương pháp dựa trên prompt hiện tại phụ thuộc nhiều vào việc tiền huấn luyện mạnh (thường được huấn luyện trên ImageNet-21k), và chúng tôi nhận thấy rằng các mô hình của họ có thể bị mắc kẹt nếu khoảng cách tiềm năng giữa tác vụ tiền huấn luyện và các tác vụ tương lai chưa biết là lớn. Trong công trình này, chúng tôi phát triển một Bộ Tạo Prompt Thích Ứng (APG) có thể học được. Điểm mốt là thống nhất quá trình truy xuất prompt và học prompt vào một bộ tạo prompt có thể học được. Do đó, toàn bộ quá trình prompting có thể được tối ưu hóa để giảm hiệu quả tiêu cực của khoảng cách giữa các tác vụ một cách hiệu quả. Để làm cho APG của chúng tôi tránh học kiến thức không hiệu quả, chúng tôi duy trì một kho kiến thức để điều chỉnh APG với phân phối đặc trưng của mỗi lớp. Các thí nghiệm mở rộng cho thấy phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp tiên tiến trong học tăng cường không dùng mẫu mà không cần tiền huấn luyện (mạnh). Ngoài ra, với việc tiền huấn luyện mạnh, phương pháp của chúng tôi cũng có hiệu suất tương đương với các mô hình dựa trên prompt hiện có, cho thấy rằng phương pháp của chúng tôi vẫn có thể hưởng lợi từ tiền huấn luyện. Mã nguồn có thể tìm thấy tại https://github.com/TOM-tym/APG

1. Giới thiệu
Mạng nơ-ron sâu (DNN) đã trở thành công cụ mạnh mẽ trong nhiều lĩnh vực [5, 7, 12, 15, 28, 56]. Tuy nhiên, khi đối mặt với các tác vụ huấn luyện tuần tự, DNN học các tác vụ mới cùng với sự suy giảm hiệu suất nghiêm trọng trên các tác vụ trước đó khi không có dữ liệu cũ, đó là hiện tượng quên thảm khốc khét tiếng [11, 41, 49]. Học tăng cường nhằm khắc phục hiện tượng quên thảm khốc trong DNN, thúc đẩy DNN hướng tới các ứng dụng thực tế phức tạp, ví dụ như robot AI [4, 10, 42, 52] hoặc tự lái [13, 16, 43, 45].

Các công trình trước đây thường duy trì một bộ đệm bộ nhớ với một số lượng nhỏ các mẫu cũ để lặp lại khi học các tác vụ mới [18, 26, 48, 54, 58, 62, 63, 65]. Vì việc giữ dữ liệu cũ có thể không khả thi do các mối quan tâm về quyền riêng tư/lưu trữ, một nhánh công việc khác [64, 66, 67] khám phá học tăng cường không dùng mẫu, điều chỉnh mạng dựa trên các tiên nghiệm được giới thiệu nhưng hiệu suất vẫn còn kém xa so với các phương pháp dựa trên lặp lại.

Gần đây, một phát triển hấp dẫn [59–61] dựa trên prompting [21,27,29,34,35] quản lý để mã hóa kiến thức vào các tập prompt để điều hướng một backbone cố định để xử lý các tác vụ tuần tự. Ngoài hiệu suất ấn tượng, nó có một số lợi ích. (1) Vấn đề quên thảm khốc được giảm thiểu hiệu quả vì backbone được cố định; (2) học prompt thay vì backbone giảm đáng kể chi phí huấn luyện và cải thiện hiệu quả học tập; (3) các phương pháp dựa trên prompt không cần giữ các mẫu. Để sử dụng prompt cho học tăng cường lớp bất khả tri về tác vụ, một bước quan trọng là chọn các prompt đặc trung cho tác vụ với bất kỳ hình ảnh đầu vào nào. Các phương pháp hiện tại duy trì một kho prompt và truy xuất prompt bằng cách tính toán trực tiếp độ tương tự giữa đặc trưng hình ảnh được trích xuất bởi mô hình tiền huấn luyện và các prompt trong kho, điều này đơn giản nhưng hiệu quả với một mô hình tiền huấn luyện mạnh. Tuy nhiên, vì mô hình tiền huấn luyện thống trị quá trình truy xuất, quá trình truy xuất không thể học này sẽ có vấn đề vì các tác vụ tương lai chưa biết và khoảng cách giữa tác vụ tiền huấn luyện và các tác vụ tương lai chưa biết có thể lớn. Như trong Hình 1 (c), khi tác vụ đầu tiên trong học tăng cường được sử dụng để tiền huấn luyện, các lớp trong tác vụ tiền huấn luyện hoàn toàn khác với các tác vụ khác, mà chúng tôi gọi là khoảng cách ngữ nghĩa làm suy giảm các mô hình hiện có. Mặc dù khoảng cách ngữ nghĩa giữa các miền cũng được nghiên cứu trong một số công trình như học chuyển giao [14, 44, 53], các công trình của họ không xem xét vấn đề quên trong các tác vụ tuần tự. Cần nhấn mạnh rằng ý định của công trình này KHÔNG phải là từ chối tiền huấn luyện mà là đề xuất một phương pháp tổng quát hơn không phụ thuộc nhiều vào tiền huấn luyện mạnh và có thể hưởng lợi từ nó nếu tiền huấn luyện liên quan đến tác vụ có sẵn. Để có thêm các thí nghiệm về tính cần thiết của công trình của chúng tôi, vui lòng tham khảo tài liệu bổ sung.

Trong công trình này, chúng tôi phát triển một Bộ Tạo Prompt Thích Ứng (APG) có thể học được để kết nối hiệu quả khoảng cách tiềm năng giữa các tác vụ tiền huấn luyện và các tác vụ tương lai chưa biết. Cốt lõi của phương pháp của chúng tôi là thống nhất quá trình truy xuất prompt và quá trình học prompt vào một bộ tạo prompt có thể học được. Theo cách này, toàn bộ quá trình prompting có thể được tối ưu hóa để giảm hiệu quả tiêu cực của khoảng cách giữa các tác vụ một cách hiệu quả. Ngoài ra, thay vì truy xuất prompt từ một kho prompt có kích thước cố định, học để tạo ra prompt tăng cường khả năng biểu đạt của prompt. Kết quả là, APG có thể được áp dụng cho một mô hình mà không cần tiền huấn luyện mạnh, và đáng chú ý, việc sử dụng APG không giảm nỗ lực trong việc khắc phục quên vì backbone vẫn được cố định.

Đối với học tăng cường, APG nắm giữ một danh sách ứng viên prompt có thể mở rộng để tổng hợp kiến thức từ các tác vụ đã thấy vào một nhóm prompt. Để prompt thích ứng cho backbone, việc tổng hợp kiến thức trong APG được điều kiện hóa trên đặc trưng ngay lập tức từ backbone. Ngoài ra, chúng tôi hình thành một kho kiến thức để tóm tắt kiến thức được mã hóa trong không gian đặc trưng. Kiến thức được tóm tắt được sử dụng thêm để điều chỉnh APG để ngăn nó học kiến thức không hiệu quả.

Tóm lại, đóng góp của chúng tôi như sau. (1) Chúng tôi đề xuất một bộ tạo prompt thích ứng có thể học được (APG) để giảm hiệu quả tiêu cực của khoảng cách giữa tác vụ tiền huấn luyện và các tác vụ tương lai chưa biết, điều này quan trọng nhưng bị bỏ qua bởi công trình trước đây. Prompting thích ứng của chúng tôi làm giảm sự phụ thuộc vào tiền huấn luyện chuyên sâu. (2) Để điều chỉnh APG, chúng tôi đề xuất kho kiến thức, giữ lại kiến thức một cách hiệu quả chỉ với thống kê của mỗi lớp. (3) Các thí nghiệm mở rộng cho thấy phương pháp của chúng tôi vượt trội đáng kể so với các phương pháp học tăng cường không dùng mẫu tiên tiến mà không cần tiền huấn luyện. Ngoài ra, với tiền huấn luyện mạnh, phương pháp của chúng tôi cũng đạt được hiệu suất tương đương thỏa mãn với các mô hình dựa trên prompt hiện có.

2. Công trình liên quan
Học Tăng Cường Lớp Không Tiền Huấn Luyện. Các phương pháp học tăng cường lớp dựa trên lặp lại [2,3,8,18,19,22,38,54,55] có quyền truy cập vào một số lượng nhỏ các mẫu cũ. Với các mẫu được giữ lại, một số công trình đề xuất chưng cất kiến thức cũ cho mạng hiện tại [2,8,18,19,54] hoặc duy trì không gian đặc trưng cũ [22,55]. Khác với các phương pháp dựa trên lặp lại, chúng tôi đề xuất loại bỏ bộ đệm bộ nhớ và kết nối khoảng cách giữa các tác vụ cũ và mới với một mô-đun có thể học được. Một ý tưởng đơn giản để xử lý học tăng cường là mở rộng động mạng cho mỗi tác vụ [1,23,39,40,46,47,50,58,63]. Mặc dù các phương pháp này trực quan hơn, phương pháp của họ thường yêu cầu thiết kế cẩn thận kiến trúc mạng và thường yêu cầu một ngân hàng bộ nhớ như các phương pháp dựa trên lặp lại. Cũng tồn tại các phương pháp không dùng mẫu nhằm học các tác vụ tuần tự mà không lưu bất kỳ hình ảnh nào. Các công trình này đề xuất ước tính độ trôi ngữ nghĩa [64], hoặc giữ kiến thức trong các nguyên mẫu lớp [66,67]. Mặc dù các phương pháp không dùng mẫu đề xuất một triển vọng hấp dẫn cho học tăng cường, chúng thường giới thiệu các tiên nghiệm được thiết kế thủ công cho việc học các tác vụ mới, và các tiên nghiệm được thiết kế bởi con người ít khả năng tổng quát hóa hơn, điều này làm cho kết quả của chúng không thỏa mãn.

Học Tăng Cường Lớp Dựa Trên Prompt. Lấy cảm hứng từ prompting trong xử lý ngôn ngữ tự nhiên, một số phương pháp điều tra học tăng cường dựa trên prompt và đạt được thành công lớn [59–61]. Các phương pháp này thường được kế thừa trực tiếp từ NLP, sử dụng một transformer tiền huấn luyện làm cơ sở. S-prompt [59] tập trung vào học tăng cường miền và học các prompt khác nhau giữa các miền. Đối với học tăng cường lớp, L2P [61] và DualPrompt [60] đầu tiên đề xuất học một kho prompt và truy vấn các prompt dựa trên đặc trưng được trích xuất bởi backbone tiền huấn luyện. DualPrompt [60] tiếp tục đề xuất gắn các prompt bổ sung để chia kiến thức cũ thành kiến thức chung và kiến thức chuyên gia. Ý tưởng chính của các phương pháp dựa trên prompt là mã hóa kiến thức từ các tác vụ cũ vào các tập vector (tức là prompt) và truy xuất chúng để hướng dẫn backbone khi dữ liệu cũ không thể truy cập. Các phương pháp này cần một backbone tiền huấn luyện mạnh để hỗ trợ quá trình truy xuất như vậy. Khác với phương pháp của họ, chúng tôi đề xuất một sơ đồ prompting thống nhất giảm sự phụ thuộc vào tiền huấn luyện chuyên sâu và làm cho nó phù hợp cho cả các tình huống có và không có tiền huấn luyện.

3. Adaptive-prompting cho Học Tăng Cường

3.1. Sơ bộ
Học tăng cường lớp. Đối với một mô hình sâu Φ(f(·)) bao gồm một bộ phân loại Φ(·) và backbone f(·), mục tiêu của học tăng cường lớp là huấn luyện mô hình Φ(f(·)) trên các tác vụ {Tt}n t=1 tuần tự sao cho mô hình có thể phân loại các mẫu kiểm tra từ bất kỳ tác vụ nào. Cụ thể, tại tác vụ thứ t Tt, tập huấn luyện là Dt={xt m, yt m}Nt m=1, trong đó xt m là hình ảnh thứ m tại tác vụ Tt với nhãn yt m. Và không gian nhãn Yt của tác vụ Tt rời rạc với các tác vụ khác, tức là Tn t=1Yt=∅. Khi mô hình hoàn thành học tập trên tác vụ Tt, tập huấn luyện tương ứng Dt sẽ bị loại bỏ và trở nên không thể truy cập khi học từ Tt+1, điều này gây ra rủi ro quên các tác vụ cũ khi học các tác vụ mới.

Vision transformers và prompting. Khác với mạng nơ-ron tích chập, Vision Transformers (ViTs) xử lý hình ảnh như một chuỗi token. Cụ thể, hình ảnh xt m sẽ được chuyển đổi thành một chuỗi E∈RNE×d thông qua các lớp nhúng patch, trong đó NE là tổng số patch hình ảnh (tức là token) và d là chiều nhúng. Một token bổ sung gọi là class token ecls∈ Rd sẽ được nối với E để thu thập thông tin từ các patch. Chuỗi token kết quả X= [ecls;E] sẽ được đưa vào các lớp transformer để trích xuất đặc trưng, và cốt lõi là phép toán self-attention:

SelfAttn (X) = Attn(XWQ,XWK,XWV),
Attn(Q, K, V ) =Softmax (QKT √ d)V, (1)

trong đó WQ,WK và WV là các phép chiếu có thể học được.

Để tạo điều kiện cho việc trích xuất đặc trưng thị giác, một kỹ thuật prompting [21,30,32,33,35,36] phổ biến là chèn một tập các token bổ sung có thể học được P∈RNP×d vào chuỗi ban đầu như [ecls;P;E]. Trong công trình này, chúng tôi chỉ thảo luận về ViTs vanilla [7,56] do tính đơn giản và linh hoạt của chúng. Chúng tôi trình bày chi tiết phương pháp của chúng tôi như sau, và tổng quan được thể hiện trong Hình 2.

3.2. Tạo Prompt Thích Ứng
Bộ tạo prompt thích ứng. Chúng tôi đề xuất một bộ tạo prompt thích ứng (APG) để tổng hợp thích ứng kiến thức cũ và mới. Giả sử có NL lớp transformer được ký hiệu là {Ll}NL l=1, và đầu ra của lớp transformer thứ l là Xl=Ll(Ll−1(···L1(x)))∈R(NE+1)×d, trong đó x1 là hình ảnh đầu vào. APG lấy đặc trưng trung gian làm đầu vào, được ký hiệu là vl=Xl[0,:], và đưa ra các prompt đặc trưng lớp để tạo điều kiện trích xuất đặc trưng ở các lớp sâu hơn:

P=APG(vl)∈RNP×d. (2)

Cụ thể, trong APG, chúng tôi tiếp tục cập nhật danh sách ứng viên prompt để duy trì các loại kiến thức khác nhau. Bắt đầu từ một danh sách trống được ký hiệu là I0= [], danh sách được mở rộng tại mỗi tác vụ, và danh sách tại tác vụ Tt được ký hiệu là:

It=h It−1;bP1, . . . ,bPc, . . . ,bP|Yt| i , (3)

trong đó bPc∈RNg×d đại diện cho một nhóm ứng viên prompt, và tổng cộng có Ng×|Y t| ứng viên prompt được mở rộng tại Tt để tạo thành It. Để tạo ra prompt với kiến thức đặc trưng lớp một cách thích ứng cho vl, chúng tôi đầu tiên áp dụng một mô-đun chiếu Min vào vl, được ký hiệu là z=Min(vl), và sau đó áp dụng một phép toán cross-attention giữa phép chiếu z và danh sách ứng viên It:

eP=CrossAttn (z,It) =MMHA (z,It,It), (4)

trong đó MMHA ký hiệu Multi-output Multi-head Attention, một phần mở rộng của Multi-head attention (MHA) [57]. Trong khi MHA ban đầu đưa ra cùng số lượng token với truy vấn đầu vào, trong trường hợp của chúng tôi, truy vấn là một token duy nhất z và chúng tôi muốn thu được NP prompt sau cross-attention. Để tránh hạn chế NP= 1, chúng tôi trực tiếp mở rộng MHA và tạo thành MMHA như sau. Đầu tiên, với đầu vào z và It, chúng tôi định nghĩa token thứ j được tính từ đầu attention thứ h là:

rj h=Attn(zWQ h,j,ItWK h,j,ItWV h,j). (5)

Giả sử số lượng đầu là nh, chúng tôi nối kết quả từ mỗi đầu như token thứ j Rj, và truyền token qua một mô-đun chiếu như đầu ra của MMHA:

Rj=Concat (rj 1, . . . , rj h, . . . , rj nh),
eP=MMHA (z,It,It) = [R1, . . . ,RNP]Wo.(6)

Chúng tôi tiếp tục đưa eP vào một mô-đun chiếu đầu ra Mout, và các token thu được P=Mout(eP)∈RNP×d là các prompt thích ứng cho hình ảnh hiện tại x.

Các prompt được tạo ra P được tích hợp thêm với đầu ra ban đầu Xl của lớp thứ l, và chuỗi token cho lớp tiếp theo trở thành Xl= [ecls,l;P;El]. chúng tôi đưa Xl vào các lớp sâu hơn còn lại {Li}NL i=l+1. Đầu ra cuối cùng của mạng với APG được ký hiệu là XNL= [ecls,N L;PNL;ENL]. Chúng tôi lấy class token1 ecls,N L làm đặc trưng hình ảnh. Bộ phân loại Φ(·) bao gồm một lớp kết nối đầy đủ và một hàm softmax và đưa ra xác suất phân loại Φ(ecls,N L).

Đáng chú ý rằng tất cả các thành phần bên trong APG đều có thể huấn luyện được, bao gồm mô-đun chiếu Min, Mout, danh sách ứng viên prompt It và các tham số trong phép toán cross-attention (Wo,{WQ/K/V h,j}).

Tối ưu hóa APG. Các prompt được tạo ra được cho là chứa đủ kiến thức để mô hình có thể xử lý tác vụ hiện tại. Do đó, chúng tôi áp dụng một hàm mất mát phân loại để học kiến thức mới từ tác vụ hiện tại:

Lcls=−log Φ( ecls,N L)(y), (7)

trong đó Φ(ecls,N L)(y) là phần tử thứ y của Φ(ecls,N L).

Vì Lcls được đánh giá trên biểu diễn đặc trưng cuối cùng ecls,N L, chúng tôi tiếp tục áp dụng trực tiếp các ràng buộc trên các prompt được tạo ra và danh sách ứng viên prompt để học hiệu quả hơn. Đối với các ứng viên prompt, thông tin của nó được tổng hợp thông qua phép toán cross-attention trong Phương trình 5. Khi tính token thứ j rj h trong đầu attention thứ h, điểm số attention (Phương trình 5) được tính như:

Aj h=Softmax ((zWQ h,j)(ItWK h,j)T √ d), (8)

trong đó mỗi phần tử Aj,(c) h chỉ ra điểm số giữa phép chiếu z và prompt thứ c trong danh sách ứng viên It dưới phép chiếu WQ h,j,WK h,j. Các ứng viên prompt nên chứa kiến thức đa dạng từ mỗi lớp, ngược lại, APG không thể tạo ra prompt thích ứng cho các hình ảnh khác nhau. Nói cách khác, nếu các ứng viên chỉ học kiến thức từ một phần của các lớp, APG không thể xử lý hình ảnh từ các lớp khác. Do đó, chúng tôi hướng dẫn rõ ràng các nhóm ứng viên prompt riêng biệt để học kiến thức đặc trưng lớp. Chúng tôi áp dụng một ràng buộc đặc trưng lớp trên điểm số attention:

Lattn=−npX j=1nhX h=1X c∈bCylog(Aj,(c) h), (9)

trong đó bCy là tập các chỉ số của các prompt trong bPy.

Hơn nữa, vì các hình ảnh của cùng một lớp có đặc điểm tương tự, chẳng hạn như ngoại hình, thật tự nhiên khi mạng có thể được prompted theo cách tương tự khi trích xuất đặc trưng cho các hình ảnh khác nhau trong một lớp. Vì vậy, chúng tôi tiếp tục ràng buộc mối quan hệ giữa các prompt thông qua một hàm mất mát triplet, khuyến khích APG học kiến thức chung của mỗi lớp. Giả sử (x1, x2, x3) là một bộ ba hình ảnh và các nhãn của chúng thỏa mãn y1=y2, y1̸=y3. Các prompt tương ứng được tạo ra bởi APG là P1,P2 và P3. Vì x1 và x2 từ cùng một lớp trong khi x3 từ lớp khác, chúng tôi ràng buộc khoảng cách giữa P1 và P2 nên nhỏ hơn khoảng cách giữa P1 và P3. Hình thức, khoảng cách cosine giữa P1 và P2 được ký hiệu là dp= cos(P1,P2), và tương tự chúng tôi có dn= cos(P1,P3). Hàm mất mát triplet với lề được áp dụng:

Ltri= [dp−dn+α]+, (10)

trong đó [·]+ ký hiệu hinge loss, và α là lề.

Thảo luận. APG được đề xuất có các ưu điểm sau. Thứ nhất, APG liên tục học để tạo ra các prompt đa dạng được điều kiện hóa trên đầu vào của mạng. Do đó, APG phá vỡ giới hạn của việc chọn prompt từ một kho prompt có kích thước cố định và giảm thiểu sự phụ thuộc vào mô hình tiền huấn luyện được sử dụng để truy vấn các prompt rời rạc từ một kho. Thứ hai, danh sách ứng viên prompt có thể mở rộng để xử lý kiến thức ngày càng tăng. Điều này cho phép mô hình có khả năng học tăng cường dài (xem Phần 4.2 và Phần 4.3) vì APG có thể tổng hợp kiến thức từ các tác vụ khác nhau bằng phép toán cross-attention (Phương trình 5).

3.3. Học Chống Quên: Một Kho Kiến Thức
Xây dựng kho kiến thức. Bất kỳ mô-đun có thể huấn luyện nào cũng đối mặt với vấn đề quên (tức là suy thoái kiến thức cũ) khi học từ các tác vụ tuần tự, bao gồm APG được đề xuất và bộ phân loại Φ(·). Do đó, chúng tôi phát triển một kho kiến thức để duy trì kiến thức cũ tốt hơn.

Sau khi hoàn thành học tập tại tác vụ Tt, chúng tôi tóm tắt kiến thức trong tập huấn luyện Dt vào kho kiến thức trước khi loại bỏ tập dữ liệu. Cụ thể, ký hiệu tất cả các hình ảnh trong lớp c là Dc t={xt m, yt m|xt m∈ Dt, yt m=c}. Chúng tôi đưa các hình ảnh này vào backbone và nhận được một tập đặc trưng Vl c={vl m|xt m∈ Dc t} từ lớp transformer thứ l. Sau đó, tâm lớp µl c được tính bằng cách lấy trung bình các đặc trưng và Σl c là một ma trận trong đó mỗi phần tử Σl,(i,k) c là hiệp phương sai giữa đặc trưng thứ i và đặc trưng thứ k. Chúng tôi sử dụng thống kê đặc trưng lớp để tạo thành một phân phối chuẩn đa biến Nl c=N(µl c,Σl c) cho mỗi lớp. Ngoài ra, để ràng buộc APG tốt hơn, tâm đặc trưng µl c được sử dụng để trích xuất các prompt tương ứng bằng cách đưa tâm vào APG, tức là Pc=APG(µc).

Cuối cùng, chúng tôi tạo thành kho kiến thức cho lớp c là Sc= (Nl c,NNLc,Pc), trong đó NNLc được tính từ các đặc trưng từ lớp cuối cùng. Nói chung, chúng tôi lưu trữ thống kê cho mỗi lớp {Sc}Ncls t c=1, trong đó Ncls t=P t|Yt| là số lượng lớp đã thấy cho đến nay.

Học chống quên. Đối với học tăng cường không dùng mẫu, chúng tôi chỉ có thể truy cập tập huấn luyện hiện tại Dt trong tác vụ Tt. Kho kiến thức {Sc}Ncls t−1 c=1 được mô tả ở trên được sử dụng để duy trì kiến thức cũ. Để giảm thiểu quên trong APG, chúng tôi lấy mẫu từ Nl c, và nhận được vector kiến thức trung gian evc∼ Nl c. Với vector và tâm prompt tương ứng Pj, chúng tôi áp dụng một ràng buộc trên APG:

LConA =ϕ(APG(evc),Pc), (11)

trong đó ϕ là một metric khoảng cách và chúng tôi thấy rằng hàm mất mát L1 hữu ích. Với ràng buộc này, APG có thể được nhắc nhở về kiến thức cũ và thực hiện tổng hợp trên chúng.

Bộ phân loại Φ(·) được mở rộng sau mỗi tác vụ, và cần được cập nhật trong mọi tác vụ. Do đó, quên cũng xảy ra ở bộ phân loại. Tương tự như ràng buộc trên APG, chúng tôi lấy mẫu vector kiến thức từ evc∼ NNLc. Sau đó, một hàm mất mát cross-entropy đơn giản nhưng hiệu quả được áp dụng:

LConC =−log Φ(evc m)(c), (12)

trong đó Φ(evc)(c) là phần tử thứ c của Φ(evc).

Mục tiêu huấn luyện cho toàn bộ mô hình. Hàm mục tiêu để tối ưu hóa mô hình và APG được tóm tắt như sau:

L=Lattn+Ltri+Lcls+LconA+LconC. (13)

Đối với backbone f(·), nó chỉ được huấn luyện trên tác vụ đầu tiên và sau đó được cố định trong trường hợp không có tiền huấn luyện, và ngược lại nó được cố định mọi lúc. Bộ phân loại Φ(·) và APG được tối ưu hóa qua tất cả các tác vụ.

4. Thí nghiệm

4.1. Thiết lập Thí nghiệm
Tập dữ liệu. Chúng tôi tiến hành thí nghiệm trên ba tập dữ liệu: CIFAR100 [25], ImageNet-Subset [6], và ImageNet-R [17]. CIFAR100 [25] chứa 100 lớp. Có 50.000 hình ảnh để huấn luyện và 10.000 hình ảnh để đánh giá. ImageNet [6] là một tập dữ liệu quy mô lớn chứa 1.000 lớp, 1300 hình ảnh mỗi lớp. Chúng tôi theo các công trình trước [8,18,19,26,54,58,62,63,65] và sử dụng ImageNet-Subset chứa 100 lớp. Tập dữ liệu ImageNet-R [17] chứa dữ liệu mới được thu thập có các phong cách khác nhau của các lớp ImageNet ban đầu. Chúng tôi theo công trình trước [60] để chia tập dữ liệu với 80% mẫu để huấn luyện và 20% mẫu còn lại được sử dụng để đánh giá.

Giao thức huấn luyện và kiểm tra. Đối với các giao thức huấn luyện và kiểm tra, chúng tôi theo các công trình trước [8,18,19,54,58,60,61,63] để tiến hành học tăng cường lớp. Sau mỗi giai đoạn huấn luyện, chúng tôi đánh giá mô hình bằng cách kiểm tra trên hợp của tất cả các tập kiểm tra của các tác vụ đã thấy. Độ chính xác phân loại trong mọi tác vụ được lấy trung bình, và chúng tôi báo cáo độ chính xác trung bình trong tất cả các thí nghiệm [8,19,48,54,60–63]. Khi so sánh với L2P [61] và DualPrompt [60], chúng tôi cũng báo cáo metric quên theo các bài báo của họ.

Chúng tôi không lưu bất kỳ hình ảnh nào như một bộ đệm bộ nhớ và chỉ giữ thống kê lớp của các lớp cũ ở mức độ đặc trưng, theo học tăng cường lớp không dùng mẫu được đề xuất trong PASS [66] và SSRE [67].

Chi tiết triển khai. Đối với tất cả các thí nghiệm dưới đây, chúng tôi sử dụng ViT vanilla làm backbone. Đối với thiết lập không tiền huấn luyện tiêu chuẩn, chúng tôi điều chỉnh chiều nhúng và số lượng đầu để phù hợp với số lượng tham số của ResNet-18 [15] được sử dụng rộng rãi. Để biết thêm thông tin về backbone được điều chỉnh, vui lòng tham khảo phụ lục. Chúng tôi theo chiến lược huấn luyện được đề xuất trong DeiT (không có chưng cất) [56], sử dụng AdamW làm bộ tối ưu hóa với học tập ban đầu là 5e-4. Đối với học tăng cường lớp với backbone tiền huấn luyện, chúng tôi theo các công trình trước [60,61] sử dụng ViT-Base làm backbone.

Ký hiệu thiết lập. Chúng tôi ký hiệu các thiết lập học tăng cường lớp dưới dạng: 'B x-Ty', có nghĩa là tác vụ đầu tiên chứa x lớp, và các lớp còn lại được chia đều thành y tác vụ để học tăng cường. Ví dụ, 'B50-T10' có nghĩa là tác vụ đầu tiên chứa 50 lớp và các lớp còn lại được chia thành 10 tác vụ đều nhau.

4.2. Học Tăng Cường Lớp Không Tiền Huấn Luyện
Trong phần này, chúng tôi đánh giá phương pháp của chúng tôi trong thiết lập học tăng cường không tiền huấn luyện tiêu chuẩn [8,9,18,19,22,24,31,37,51,58,63,64,66,67].

So sánh với các phương pháp dựa trên prompt. Trong tình huống không tiền huấn luyện phổ biến, chúng tôi đầu tiên so sánh phương pháp của chúng tôi với các phương pháp dựa trên prompt L2P [61] và DualPrompt [60]. Vì các phương pháp dựa trên prompt hiện tại dựa vào backbone tiền huấn luyện để tiến hành học tăng cường, đối với thiết lập không tiền huấn luyện, chúng tôi xem tác vụ đầu tiên trong quá trình học tăng cường như một tác vụ 'tiền huấn luyện' và huấn luyện backbone theo cách có giám sát. Sau đó, chúng tôi triển khai lại L2P và DualPrompt theo mã chính thức và tiến hành học tăng cường trên các tác vụ còn lại dựa trên trọng số tiền huấn luyện. Theo cách này, chúng tôi có thể nghiên cứu tình huống khi các tác vụ tương lai hoàn toàn tách biệt khỏi tác vụ tiền huấn luyện.

Kết quả thí nghiệm được thể hiện trong Bảng 1. Về độ chính xác trung bình, phương pháp được đề xuất của chúng tôi vượt trội hơn L2P và DualPrompt với một khoảng cách lớn trong các thiết lập khác nhau về số lượng tác vụ. Metric quên của L2P và DualPrompt cực kỳ thấp. Chúng tôi thấy rằng họ về cơ bản duy trì hiệu suất tác vụ đầu tiên và hoạt động kém trên các tác vụ mới. Do đó, metric quên (có thể được coi là một sự sụt giảm độ chính xác tối đa trung bình) là thấp. Tuy nhiên, phương pháp của chúng tôi vẫn đạt được kết quả cạnh tranh về quên so với hai phương pháp này. Chúng tôi xác nhận quan sát này bằng cách thay đổi số lượng lớp tại tác vụ đầu tiên và tổng số tác vụ trên hai tập dữ liệu khác nhau.

So sánh với các phương pháp không dùng mẫu. Để so sánh phương pháp được đề xuất với các công trình không dùng mẫu [31,64,66,67] tiên phong một cách công bằng, chúng tôi tiến hành thí nghiệm trên ImageNet-Subset và CIFAR100 trong các thiết lập số lượng tác vụ khác nhau. Kết quả chi tiết trên ImageNet-Subset được liệt kê trong Bảng 2. Đối với thiết lập 10 tác vụ, chúng tôi thấy rằng phương pháp của chúng tôi vượt trội hơn các phương pháp không dùng mẫu khác với một khoảng cách lớn. Cụ thể, phương pháp của chúng tôi đạt được 75.52% độ chính xác trung bình, cao hơn 7.95%, 13.84%, 14.52% so với các phương pháp tiên tiến SSRE [67], PASS [66] và SDC [64]. Trên CIFAR100, phương pháp của chúng tôi vẫn hoạt động tốt hơn các phương pháp không dùng mẫu khác (xem Bảng 3). Cụ thể, phương pháp của chúng tôi vượt trội hơn phương pháp không dùng mẫu SSRE lần lượt là 2.56%, 1.65%, và 0.66% về độ chính xác trung bình trong thiết lập 5, 10 và 20 tác vụ.

So sánh với các phương pháp dựa trên lặp lại. Để cho thấy hiệu quả của sơ đồ adaptive-prompting của chúng tôi, chúng tôi tiếp tục báo cáo kết quả của các phương pháp dựa trên lặp lại trên ImageNet-Subset và CIFAR100 trong các thiết lập số lượng tác vụ khác nhau và kích thước ngân sách bộ nhớ khác nhau.

Kết quả so sánh của các thiết lập khác nhau trên ImageNet-Subset được thể hiện trong Bảng 2. Phương pháp của chúng tôi (không có mẫu) đạt được 75.52% Avg. Acc. tại thiết lập B50-T10, vượt trội hơn PODNet [8] và AFC [22] (cả hai đều có 20 mẫu mỗi lớp) lần lượt là 1.06% và 0.23%. Để khám phá hiệu quả của phương pháp được đề xuất của chúng tôi trong một quá trình tăng cường dài, chúng tôi cũng tiến hành thí nghiệm B50-T50 (tức là học tăng cường 50 tác vụ). Khi quá trình học tập kéo dài từ 10 tác vụ đến 50 tác vụ, hiệu suất của chúng tôi duy trì khoảng 75% và vượt trội hơn các phương pháp khác với một khoảng cách lớn. Điều này chủ yếu là do danh sách ứng viên có thể mở rộng được đề xuất của chúng tôi trong APG cho phép phương pháp của chúng tôi học trong các tác vụ tăng cường dài trong khi các phương pháp khác bị ảnh hưởng nặng nề bởi vấn đề quên. Khi chúng tôi thắt chặt ngân sách bộ nhớ, các phương pháp dựa trên lặp lại đối mặt với sự sụt giảm hiệu suất mạnh mẽ. So với Foster [58] và AFC [22] (cả hai đều giữ 10 hình ảnh mỗi lớp) trong thiết lập B50-T10, phương pháp của chúng tôi đạt được độ chính xác trung bình cao hơn 4.56% và 0.67%. Ngay cả đối với một phương pháp như Imagine [54] yêu cầu một tập dữ liệu phụ trợ bổ sung để đa dạng hóa bộ nhớ hạn chế, phương pháp của chúng tôi vẫn vượt trội hơn nó 0.7% độ chính xác khi nó giữ 10 hình ảnh mỗi lớp. Kết quả khác trên CIFAR100 có trong Bảng 3.

4.3. Đánh giá với Backbone Tiền Huấn Luyện
Học tăng cường với backbone tiền huấn luyện chuyên sâu. Để chứng minh tính linh hoạt của phương pháp được đề xuất, chúng tôi tiếp tục mở rộng phương pháp của chúng tôi cho thiết lập có trọng số tiền huấn luyện và tiến hành thí nghiệm với trọng số tiền huấn luyện như các phương pháp trước đây [60,61] đã làm. Chúng tôi theo các công trình trước [60,61] để tải một trọng số tiền huấn luyện ImageNet 21k [56] cho ViT-Base/16 vanilla. Chúng tôi tiến hành thí nghiệm trên hai tập dữ liệu khác nhau CIFAR100 và ImageNet-R theo DualPrompt [60]. Đối với mỗi tập dữ liệu, chúng tôi tiến hành 5, 10, và 20 tác vụ tăng cường để xác nhận các phương pháp khác nhau. Hơn nữa, chúng tôi cũng tiến hành một cận trên và một baseline fine-tuning (FT) để tham khảo. Một cận trên được xây dựng bằng cách xem tất cả các mẫu huấn luyện có sẵn qua các tác vụ và điều chỉnh backbone ViT tiền huấn luyện sử dụng các chiến lược huấn luyện có giám sát. Baseline FT là để điều chỉnh toàn bộ backbone (cũng được khởi tạo với trọng số tiền huấn luyện) mà không có bất kỳ ràng buộc nào đối với kiến thức cũ trong quá trình học tập.

Như được thể hiện trong Bảng 4, phương pháp của chúng tôi tạo ra hiệu suất tương đương khi số lượng tác vụ là 5. Khi chuỗi học tập trở nên dài hơn, phương pháp của chúng tôi vượt trội hơn L2P và DualPrompt với một khoảng cách lớn hơn. Cụ thể, phương pháp được đề xuất của chúng tôi vượt trội hơn DualPrompt và L2P lần lượt là 1.03% và 2.07% về độ chính xác trung bình trong học tăng cường 20 tác vụ trên CIFAR100. Một kết luận tương tự có thể được quan sát trong tập dữ liệu thách thức hơn ImageNet-R trong Bảng 5. Khi quy trình huấn luyện trở nên dài hơn, tính ưu việt của mô hình của chúng tôi bắt đầu xuất hiện. Trong học tăng cường 20 tác vụ trên ImageNet-R, chúng tôi đạt được cải thiện 0.74% và 3.06% so với DualPrompt về độ chính xác trung bình và quên tương ứng.

Đáng chú ý rằng phương pháp của chúng tôi không dựa vào một mẫu prompting phức tạp và tinh tế (tức là tập trung vào nơi hoặc cách chèn prompt), chỉ sử dụng cách vanilla [21] để chèn các prompt. Các thí nghiệm trong phần này cho thấy rằng adaptive-prompting là một thiết kế hiệu quả và quá trình lựa chọn tĩnh từ một kho prompt trong các phương pháp trước [60,61] có thể là hạn chế của các phương pháp dựa trên prompt.

Phân tích thêm về các trọng số tiền huấn luyện khác nhau. Để điều tra tác động của khoảng cách giữa kiến thức tiền huấn luyện và các tác vụ tương lai, chúng tôi tiếp tục tiến hành thí nghiệm trên CIFAR100 sử dụng trọng số tiền huấn luyện TinyImageNet [20]. Như được thể hiện trong Bảng 4, chúng tôi có thể quan sát rằng phương pháp của chúng tôi vượt trội hơn L2P lần lượt là 3.04%, 5.10% và 8.81% về Avg. Acc. trong 5, 10 và 20 tác vụ. Qua các chiến lược tiền huấn luyện khác nhau (ImageNet-21k và TinyImageNet trong Bảng 4 và không tiền huấn luyện tiêu chuẩn trong Bảng 1), phương pháp của chúng tôi đạt được Avg. Acc. cao hơn lần lượt là 1.03%, 8.37% và 39.54% so với DualPrompt trong thiết lập 20 tác vụ. Điều này chủ yếu là do kiến thức tiền huấn luyện thống trị hiệu suất tăng cường đối với các phương pháp dựa trên prompt hiện tại, làm cho phương pháp của họ thất bại khi khoảng cách ngữ nghĩa giữa tác vụ tiền huấn luyện và các tác vụ tăng cường tương đối lớn. Ngược lại, phương pháp của chúng tôi phá vỡ sự phụ thuộc vào backbone tiền huấn luyện mạnh vì APG có thể học hiệu quả để giảm tác động tiêu cực của khoảng cách ngữ nghĩa.

4.4. Nghiên cứu Loại bỏ
Trong phần này, chúng tôi thảo luận về hiệu quả của mỗi hàm mất mát được đề xuất trong Phần 3. Chúng tôi thực hiện nghiên cứu loại bỏ trên ImageNet-Subset với backbone ViT vanilla không tiền huấn luyện trong thiết lập học tăng cường 10 tác vụ.

Hiệu quả của kho kiến thức. Kho kiến thức của chúng tôi được phục vụ trong hai khía cạnh: một là ràng buộc trên APG (LconA) và thứ hai là hướng dẫn bộ phân loại (LconC). Trong Bảng 6, c-1 chỉ ra rằng chúng tôi chỉ áp dụng hàm mất mát phân loại Lcls và hiệu suất còn kém xa so với mức thỏa mãn vì mạng không thể truy cập bất kỳ kiến thức cũ nào. Chúng tôi có thể quan sát rằng với ràng buộc LconC trên bộ phân loại, vấn đề mất cân bằng của bộ phân loại [62] có thể được giảm thiểu đáng kể (Avg. Acc. từ 19.37% lên 71.84%). Điều này chỉ ra rằng vector kiến thức có thể đại diện cho các lớp ban đầu tốt, có thể khắc phục hiệu quả quên. Với sự giúp đỡ của LconA, APG có thể đưa ra các prompt hiệu quả cho các lớp cũ. Độ chính xác trung bình được tăng từ 71.84% lên 73.46%. Điều này chủ yếu là do mô-đun có thể huấn luyện APG đang đối mặt với vấn đề quên nghiêm trọng trong quá trình học tập có thể được giải quyết hiệu quả bằng LconA.

Hiệu quả của APG và các hàm mất mát liên quan. Để tiếp tục giảm thiểu vấn đề quên, chúng tôi thiết kế hai hàm mất mát trong Phần 3 để hướng dẫn APG học kiến thức đặc trưng lớp trong khi huấn luyện. Đầu tiên là Lattn, hướng dẫn phép toán attention trong APG trở nên liên quan đến lớp hơn. Với hàm mất mát Lattn, hiệu suất tăng từ 73.46% lên 74.70%. Triplet Ltri phục vụ như một ràng buộc tránh APG tạo ra các prompt xuống cấp và thu hẹp khoảng cách giữa hai prompt với cùng lớp, mang lại sự cải thiện hiệu suất 1.24% (so sánh c-4 với mô hình đầy đủ).

Tác động của vị trí của APG. Như đã nêu trong Phần 3, APG được đề xuất lấy đặc trưng trung gian từ lớp Ll và tạo ra các prompt đặc trưng lớp. Để điều tra tác động của vị trí l, chúng tôi tiến hành thí nghiệm trên l={6,7,8,9,10}, và kết quả được thể hiện trong Hình 3(a). Được thể hiện rằng phương pháp của chúng tôi hoạt động nhất quán khi APG được chèn vào các lớp l∈ {6,7,8,9} nhưng giảm tại lớp l= 10. Điều này chủ yếu là do mức độ sâu hơn của backbone mã hóa thông tin đặc trưng tác vụ hơn và nó không phù hợp cho APG để tổng hợp kiến thức. Mô hình của chúng tôi hoạt động tốt nhất khi l= 9, và chúng tôi chọn l= 9 làm thiết lập mặc định để cân bằng độ phức tạp và hiệu suất.

Tác động của số lượng prompt. Như đã nêu trong Phần 3, Multi-head Self Attention (MSA) được đề cập để thực hiện phép toán cross-attention. Tính chất đa đầu như vậy cho phép chúng tôi tạo ra bất kỳ số lượng prompt nào để chèn vào các lớp sâu. Để khám phá tác động của số lượng prompt, chúng tôi tiến hành thí nghiệm trên NP∈ {0,1,2,3,4}, có kết quả trong Hình 3(b). So với baseline (NP= 0), chúng tôi đạt được cải thiện độ chính xác 4.1% khi tạo ra 1 prompt. Khi số lượng prompt tăng lên, hiệu suất ở mức khoảng 75%. Điều này chỉ ra rằng với APG có thể huấn luyện, prompt được tạo ra đủ mạnh cho một prompt duy nhất để hướng dẫn các lớp sâu hơn. Do đó, chúng tôi chọn NP= 1 làm mặc định.

5. Kết luận
Trong công trình này, chúng tôi tiết lộ thực tế rằng các mô hình dựa trên prompt hiện tại đạt được hiệu suất ấn tượng trong học tăng cường lớp nhưng tiếc là chúng bị ảnh hưởng bởi các backbone tiền huấn luyện mạnh. Khoảng cách tiềm năng giữa tác vụ tiền huấn luyện và các tác vụ tương lai có thể là một rào cản đối với các sơ đồ prompting hiện tại trong học tăng cường. Do đó, chúng tôi đóng góp một sơ đồ adaptive-prompting có thể học được. Các thí nghiệm mở rộng cho thấy phương pháp của chúng tôi đạt được hiệu suất thỏa mãn mà không cần tiền huấn luyện và cũng đạt được hiệu suất tương đương với các mô hình khác dưới tiền huấn luyện mạnh. Ngoài việc áp dụng kiến thức từ tác vụ tiền huấn luyện cho các tác vụ khác, công trình của chúng tôi cũng cho thấy rằng mô hình dựa trên prompt có thể học thích ứng kiến thức mới khi kiến thức hiện tại không đủ cho các tác vụ tương lai, điều này làm cho các mô hình dựa trên prompt khả thi cho học tăng cường tổng quát hơn (ví dụ, các tác vụ phức tạp hơn, dữ liệu đa phương thức).

6. Lời cảm ơn
Công trình này được hỗ trợ một phần bởi NSFC (U21A20471, U1911401, U1811461), Dự án Guangdong NSF (Số 2023B1515040025, 2020B1515120085). Các tác giả muốn cảm ơn Kun-Yu Lin vì các cuộc thảo luận sâu sắc. Các tác giả cũng cảm ơn ba nhà phê duyệt ẩn danh vì các đề xuất xây dựng của họ.

Tham khảo
[1] Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, và Babak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2
[2] Arjun Ashok, KJ Joseph, và Vineeth N Balasubramanian. Class-incremental learning with cross-space clustering and controlled transfer. Trong Proceedings of the European Conference on Computer Vision, 2022. 2
[3] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, và Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2
[4] Michel Breyer, Jen Jen Chung, Lionel Ott, Siegwart Roland, và Nieto Juan. Volumetric grasping network: Real-time 6 dof grasp detection in clutter. Trong Conference on Robot Learning, 2020. 1
[5] Joao Carreira và Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. Trong proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2017. 1
[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2009. 5, 7
[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations, 2021. 1, 3
[8] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, và Eduardo Valle. Podnet: Pooled outputs distillation for small-tasks incremental learning. Trong Proceedings of the European Conference on Computer Vision, 2020. 2, 5, 6, 7
[9] Arthur Douillard, Alexandre Ramé, Guillaume Couairon, và Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 6
[10] Hao-Shu Fang, Chenxi Wang, Minghao Gou, và Cewu Lu. Graspnet-1billion: A large-scale benchmark for general object grasping. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 1
[11] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 1999. 1
[12] Ziteng Gao, Limin Wang, Bing Han, và Sheng Guo. Adamixer: A fast-converging query-based object detector. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1
[13] Abhishek Gupta, Alagan Anpalagan, Ling Guan, và Ahmed Shaharyar Khwaja. Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues. Array, 2021. 1
[14] Kaiming He, Ross Girshick, và Piotr Dollár. Rethinking imagenet pre-training. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2016. 1, 5
[16] David Held, Sebastian Thrun, và Silvio Savarese. Learning to track at 100 fps with deep regression networks. Trong Proceedings of the European Conference on Computer Vision, 2016. 1
[17] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, và Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. 5
[18] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, và Dahua Lin. Learning a unified classifier incrementally via rebalancing. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 1, 2, 5, 6, 7
[19] Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, và Hanwang Zhang. Distilling causal effect of data in class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2, 5, 6, 7
[20] Ethan Huynh. Vision transformers in 2022: An update on tiny imagenet. arXiv preprint arXiv:2205.10660, 2022. 8
[21] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning. Trong Proceedings of the European Conference on Computer Vision, 2022. 1, 3, 7
[22] Minsoo Kang, Jaeyoo Park, và Bohyung Han. Class-incremental learning by knowledge distillation with adaptive feature consolidation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 6, 7
[23] Zixuan Ke, Bing Liu, và Xingchang Huang. Continual learning of a mixed sequence of similar and dissimilar tasks. Trong Advances in Neural Information Processing Systems, 2020. 2
[24] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, và cộng sự. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. 6, 7
[25] Alex Krizhevsky, Geoffrey Hinton, và cộng sự. Learning multiple layers of features from tiny images. Báo cáo kỹ thuật, Đại học Toronto, 2009. 1, 5
[26] Kibok Lee, Kimin Lee, Jinwoo Shin, và Honglak Lee. Overcoming catastrophic forgetting with unlabeled data in the wild. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 1, 5
[27] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 1
[28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, và Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1
[29] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 1
[30] Xiang Lisa Li và Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 3
[31] Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 2017. 6, 7
[32] Lingbo Liu, Bruce XB Yu, Jianlong Chang, Qi Tian, và Chang-Wen Chen. Prompt-matched semantic segmentation. arXiv preprint arXiv:2208.10159, 2022. 3
[33] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, và Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021. 3
[34] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, và Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 2023. 1
[35] Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, và Jie Tang. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602, 2021. 1, 3
[36] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, và Jie Tang. Gpt understands, too. arXiv:2103.10385, 2021. 3
[37] Yu Liu, Sarah Parisot, Gregory Slabaugh, Xu Jia, Ales Leonardis, và Tinne Tuytelaars. More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning. Trong Proceedings of the European Conference on Computer Vision, 2020. 6, 7
[38] Yaoyao Liu, Yuting Su, An-An Liu, Bernt Schiele, và Qianru Sun. Mnemonics training: Multi-class incremental learning without forgetting. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2
[39] Arun Mallya, Dillon Davis, và Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. Trong Proceedings of the European Conference on Computer Vision, 2018. 2
[40] Arun Mallya và Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. 2
[41] Michael McCloskey và Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of learning and motivation, 1989. 1
[42] Arsalan Mousavian, Clemens Eppner, và Dieter Fox. 6-dof graspnet: Variational grasp generation for object manipulation. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. 1
[43] Filipe Mutz, Vinicius Cardoso, Thomas Teixeira, Luan FR Jesus, Michael A Golçalves, Rânik Guidolini, Josias Oliveira, Claudine Badue, và Alberto F De Souza. Following the leader using a tracking system based on pre-trained deep neural networks. Trong 2017 international joint conference on neural networks (IJCNN), 2017. 1
[44] Sinno Jialin Pan và Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 2010. 2
[45] Evan Prianto, MyeongSeop Kim, Jae-Han Park, Ji-Hun Bae, và Jung-Su Kim. Path planning for multi-arm manipulators using deep reinforcement learning: Soft actor–critic with hindsight experience replay. Sensors, 2020. 1
[46] Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fahad Shahbaz Khan, và Ling Shao. Random path selection for incremental learning. Advances in Neural Information Processing Systems, 2019. 2
[47] Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, và Mubarak Shah. itaml: An incremental task-agnostic meta-learning approach. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2
[48] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2017. 1, 5
[49] Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 1995. 1
[50] Joan Serra, Didac Suris, Marius Miron, và Alexandros Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. Trong International Conference on Machine Learning, 2018. 2
[51] Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip HS Torr, Song Bai, và Vincent YF Tan. Mimicking the oracle: An initial phase decorrelation approach for class incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 6, 7
[52] Martin Sundermeyer, Arsalan Mousavian, Rudolph Triebel, và Dieter Fox. Contact-graspnet: Efficient 6-dof grasp generation in cluttered scenes. Trong 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2021. 1
[53] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, và Chunfang Liu. A survey on deep transfer learning. Trong Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27, 2018. 2
[54] Yu-Ming Tang, Yi-Xing Peng, và Wei-Shi Zheng. Learning to imagine: Diversify memory for incremental learning using unlabeled data. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 5, 6, 7
[55] Rishabh Tiwari, Krishnateja Killamsetty, Rishabh Iyer, và Pradeep Shenoy. Gcr: Gradient coreset based replay buffer selection for continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2
[56] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. Trong International Conference on Machine Learning, 2021. 1, 3, 6, 7
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Advances in neural information processing systems, 2017. 3
[58] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, và De-Chuan Zhan. Foster: Feature boosting and compression for class-incremental learning. Trong Proceedings of the European Conference on Computer Vision, 2022. 1, 2, 5, 6, 7
[59] Yabin Wang, Zhiwu Huang, và Xiaopeng Hong. S-prompts learning with pre-trained transformers: An occam's razor for domain incremental learning. arXiv preprint arXiv:2207.12819, 2022. 1, 2
[60] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, và cộng sự. Dualprompt: Complementary prompting for rehearsal-free continual learning. Trong Proceedings of the European Conference on Computer Vision, 2022. 1, 2, 5, 6, 7, 8
[61] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, và Tomas Pfister. Learning to prompt for continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 5, 6, 7, 8
[62] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, và Yun Fu. Large scale incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 1, 5, 8
[63] Shipeng Yan, Jiangwei Xie, và Xuming He. Der: Dynamically expandable representation for class incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1, 2, 5, 6
[64] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, và Joost van de Weijer. Semantic drift compensation for class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 1, 2, 6, 7
[65] Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry Heck, Heming Zhang, và C-C Jay Kuo. Class-incremental learning via deep model consolidation. Trong Proceedings of the IEEE Winter Conference on Applications of Computer Vision, 2020. 1, 5
[66] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, và Cheng-Lin Liu. Prototype augmentation and self-supervision for incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1, 2, 5, 6, 7
[67] Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, và Zheng-Jun Zha. Self-sustaining representation expansion for non-exemplar class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 1, 2, 5, 6, 7
