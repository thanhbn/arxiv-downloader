# 2309.15701.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.15701.pdf
# Kích thước tệp: 1063479 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
HyPoradise: Một Baseline Mở cho Nhận Diện Giọng Nói Sinh Tạo
với Các Mô Hình Ngôn Ngữ Lớn
Chen Chen1,†Yuchen Hu1,†Chao-Han Huck Yang2∗
Sabato Marco Siniscalchi2,3Pin-Yu Chen4Eng Siong Chng1
1Đại học Công nghệ Nanyang, Singapore;2Viện Công nghệ Georgia, Mỹ
3Đại học Khoa học và Công nghệ Na Uy, Na Uy;4IBM Research AI, Mỹ
{chen1436,yuchen005}@e.ntu.edu.sg ,huckiyang@gatech.edu
Tóm tắt
Những tiến bộ trong mạng nơ-ron sâu đã cho phép các hệ thống nhận diện giọng nói tự động
(ASR) đạt được khả năng ngang bằng con người trên một số bộ dữ liệu giọng nói sạch có sẵn công khai.
Tuy nhiên, ngay cả các hệ thống ASR tiên tiến nhất cũng gặp phải sự suy giảm hiệu suất
khi đối mặt với các điều kiện bất lợi, vì một mô hình âm học được huấn luyện tốt
nhạy cảm với các biến thể trong miền giọng nói, ví dụ như tiếng ồn nền.
Một cách trực quan, con người giải quyết vấn đề này bằng cách dựa vào kiến thức ngôn ngữ của họ:
ý nghĩa của các từ ngữ nói không rõ ràng thường được suy luận từ các manh mối ngữ cảnh
do đó giảm sự phụ thuộc vào hệ thống thính giác. Được lấy cảm hứng từ quan sát này,
chúng tôi giới thiệu benchmark mã nguồn mở đầu tiên để sử dụng các mô hình ngôn ngữ lớn (LLM)
bên ngoài cho việc sửa lỗi ASR, trong đó các giả thuyết giải mã N-best
cung cấp các yếu tố thông tin cho việc dự đoán phiên âm thực sự. Phương pháp này là
một sự thay đổi mô hình từ chiến lược tính điểm lại mô hình ngôn ngữ truyền thống chỉ
có thể chọn một giả thuyết ứng viên làm phiên âm đầu ra. Benchmark được đề xuất
chứa một bộ dữ liệu mới, "HyPoradise" (HP), bao gồm hơn
334,000 cặp giả thuyết N-best và các phiên âm chính xác tương ứng
trên các miền giọng nói phổ biến. Dựa trên bộ dữ liệu này, chúng tôi kiểm tra ba loại
kỹ thuật sửa lỗi dựa trên LLM với số lượng khác nhau của các cặp giả thuyết-phiên âm được gán nhãn,
từ đó đạt được sự giảm đáng kể tỷ lệ lỗi từ (WER).
Bằng chứng thực nghiệm chứng minh kỹ thuật được đề xuất đạt được một đột phá bằng cách
vượt qua giới hạn trên của các phương pháp dựa trên xếp hạng lại truyền thống.
Đáng ngạc nhiên hơn, LLM với prompt hợp lý và khả năng sinh tạo của nó
thậm chí có thể sửa những token bị thiếu trong danh sách N-best. Chúng tôi cung cấp
kết quả công khai để có các pipeline có thể tái tạo với các mô hình đã được huấn luyện trước,
do đó cung cấp một mô hình đánh giá mới cho việc sửa lỗi ASR với LLM.

1 Giới thiệu
Nhận diện giọng nói tự động (ASR) đã trở nên ngày càng quan trọng trong xã hội hiện đại, vì nó
cho phép phiên âm hiệu quả và chính xác các ngôn ngữ nói. Khả năng này tạo điều kiện truy cập
thông tin và tăng cường giao tiếp qua các lĩnh vực khác nhau, bao gồm giáo dục [7], chăm sóc
sức khỏe [50], và kinh doanh [36]. Được thúc đẩy bởi những tiến bộ gần đây trong deep learning,
thành công đáng kể đã đạt được trong một số tác vụ ASR thông qua các kỹ thuật huấn luyện
end-to-end [28,27,9,22,30,100,15].
Tuy nhiên, một thách thức lớn của việc áp dụng ASR trong các điều kiện thực tế nằm ở việc
xử lý hiệu quả các biến thể trong giọng nói do các yếu tố khác nhau gây ra như tiếng ồn nền [11],
giọng địa phương của người nói [85], và phong cách nói [82,2]. Những yếu tố bất lợi này
phổ biến và không thể tránh khỏi trong tín hiệu giọng nói, ảnh hưởng đáng kể đến độ chính xác
của kết quả nhận diện [55].

∗Công việc được thực hiện & mã nguồn mở khi tác giả còn ở Georgia Tech; Tác giả liên hệ. †Đóng góp ngang nhau.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2023) Track về Bộ dữ liệu và Benchmark.arXiv:2309.15701v2 [cs.CL] 16 Oct 2023

--- TRANG 2 ---
 ASR 
Giả thuyết N-best
Đầu vào âm thanh
Trường hợp (i) tốt hơn : trong :
 T1 T2 T3 T4 T5 
 T1 T2 T3 T4 T5 Trường hợp (ii) T2 
Exp:
 Câu nói  Câu nói 0.14 0.34
 T1 T2 T3 T4 T5 
 T1 T2 T3 T4 T5 Exp:
 Câu nói 
 Câu nói 
 Câu nói...Phiên âm
Đầu ra
LMHình 1: Phần bên trái hiển thị pipeline để tạo giả thuyết N-best bằng một
engine ASR vanilla với giải mã tìm kiếm chùm. Phần bên phải đếm xác suất của trường hợp (i) và trường hợp (ii) trên
tập test của bộ dữ liệu LibriSpeech. Nó chỉ ra thông tin bị loại bỏ trong câu nói thứ 2∼20.
Màu xanh lá cây và đỏ Ti trong "Exp" tương ứng biểu thị các token đúng và sai so với ground-truth.

Con người thể hiện sự bền vững đáng kể khi đối mặt với những biến thể nêu trên trong môi trường âm thanh,
vì hệ thống nhận diện của con người không chỉ dựa vào các manh mối âm thanh – chúng ta thường suy đoán
các từ ngữ nói không rõ ràng hoặc bị méo mó dựa trên ngữ cảnh giọng nói và kiến thức ngôn ngữ vốn có của chúng ta.
Tương tự, hệ thống ASR hiện tại thường sử dụng một mô hình ngôn ngữ (LM) độc lập để tính điểm lại
trong quá trình giải mã [83,46,43,25]. Như được hiển thị trong Hình 1, với giả thuyết N-best được tạo
bởi một engine ASR với giải mã tìm kiếm chùm, một mô hình ngôn ngữ (LM) đã được huấn luyện có thể được sử dụng để tính điểm lại
mỗi câu nói và chọn câu có khả năng cao nhất (được gọi là câu nói thứ 1) làm
đầu ra của ASR; trong khi đó, các câu khác (câu nói thứ 2–N) bị loại bỏ. Tuy nhiên,
người ta tin rằng [68] danh sách N-best chứa thông tin hữu ích [87,37,56], vì mỗi giả thuyết
là một đại diện văn bản độc lập của đầu vào giọng nói. Do đó, các câu bị loại bỏ
cũng có thể mang các token đúng để dự đoán chính xác phiên âm thực sự. Để xác thực niềm tin này,
chúng tôi đã thực hiện các thí nghiệm trên bộ dữ liệu LibriSpeech [66], đếm xác suất của hai
tình huống quan sát được trong quá trình tính điểm lại LM: (i) các câu nói bị loại bỏ chứa một ứng viên tốt hơn với
WER thấp hơn, và (ii) các giả thuyết bị loại bỏ khác có thể cung cấp câu trả lời đúng cho
các token sai trong câu nói thứ 1. Kết quả thống kê của câu nói thứ 2∼20 được hiển thị trong
phần bên trái của Hình 1. Lấy câu nói thứ 2 bị loại bỏ làm ví dụ, nó có 14% xác suất có
WER thấp hơn câu nói thứ 1. Hơn nữa, với một token sai trong câu nói thứ 1, có 34%
xác suất tìm thấy token đúng trong câu nói thứ 2.

Để khai thác tốt hơn thông tin trong giả thuyết N-best, chúng tôi đề xuất nỗ lực đầu tiên trên benchmark
sửa lỗi sinh tạo ASR có sẵn công khai mà trực tiếp dự đoán phiên âm thực sự, thay vì
chọn một ứng viên từ danh sách N-best. Để đưa ra benchmark này, chúng tôi giới thiệu một bộ dữ liệu mới
có tên HyPoradise (HP), bao gồm các giả thuyết N-best mã nguồn mở khác nhau được cung cấp bởi
các hệ thống ASR tiên tiến và các phiên âm thực sự được ghép cặp của chúng. Xem xét các ứng dụng thực tế,
bộ dữ liệu HP bao gồm nhiều miền giọng nói thách thức, bao gồm các tình huống với tiếng ồn nền,
ngữ cảnh cụ thể, và giọng địa phương của người nói. Hơn nữa, về mặt tính khả dụng của tài nguyên, chúng tôi định nghĩa ba
cài đặt để mô phỏng việc triển khai các hệ thống ASR trong các tình huống thực tế: (i) Zero-shot Learning.
Trong cài đặt này, chỉ có giả thuyết tập test cho suy luận. Điều này tương ứng với việc áp dụng
một mô hình ASR đã được huấn luyện tốt cho các tình huống mới mà không có bất kỳ dữ liệu huấn luyện nào.
(ii) Few-shot Learning. Một vài giả thuyết trong miền với phiên âm thực sự có sẵn cho huấn luyện.
Cài đặt này nhằm giải quyết các tác vụ ASR cụ thể miền với một vài chú thích thủ công.
(iii) Fine-tuning. Một tập huấn luyện đủ lớn có sẵn để học ánh xạ giữa giả thuyết và phiên âm.

Để khai thác ba tình huống nêu trên, chúng tôi trình bày nhiều kỹ thuật sửa lỗi sử dụng
mô hình ngôn ngữ lớn (LLM), đã cho thấy khả năng vượt trội trong việc sinh tạo ngôn ngữ
và lý luận trong các nghiên cứu gần đây [5,107,48,84]. Cho cài đặt zero-shot và few-shot,
chúng tôi thiết kế một phương pháp học trong ngữ cảnh mà không cần bất kỳ điều chỉnh tham số nào,
trực tiếp thực hiện sửa lỗi dựa trên prompt tác vụ và minh họa trong miền. Trong tình huống fine-tuning,
chúng tôi phát triển hai giải pháp huấn luyện sequence-to-sequence, H2T-ft và H2T-LoRA,
điều chỉnh các LLM đã được huấn luyện trước cho các miền phiên âm cụ thể. Kết quả thực nghiệm
cho thấy tất cả các chiến lược học có thể có lợi để giảm WER trong các cài đặt tài nguyên khác nhau,
cung cấp các giải pháp tiềm năng để giảm thiểu

--- TRANG 3 ---
tác động tiêu cực của biến thể giọng nói. Ngoài ra, với thiết kế prompt hợp lý, LLM có thể sửa
những token không chính xác mà không có trong danh sách N-best. Chúng tôi sẽ phát hành bộ dữ liệu HP,
các pipeline có thể tái tạo, và các mô hình đã được huấn luyện trước trên Github2 theo giấy phép MIT.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
•Chúng tôi đề xuất benchmark mở và có thể tái tạo đầu tiên để đánh giá cách LLM có thể được sử dụng
để nâng cao kết quả ASR với giả thuyết N-best, trong đó một bộ dữ liệu mới HyPoradise3 với
hơn 334K cặp giả thuyết-phiên âm được thu thập từ các corpus ASR khác nhau
trong các miền giọng nói phổ biến nhất.
•Chúng tôi phát triển ba kỹ thuật sửa lỗi ASR dựa trên LLM trong các cài đặt tài nguyên khác nhau
để trực tiếp dự đoán phiên âm thực sự từ giả thuyết N-best. Kết quả thực nghiệm
trong cài đặt fine-tuning cho thấy phương pháp mới của chúng tôi có thể vượt qua giới hạn trên
hiệu suất (ví dụ, WER oracle từ danh sách n-best) của các phương pháp dựa trên xếp hạng lại truyền thống.
•Chúng tôi giới thiệu một mô hình đánh giá về sửa lỗi sinh tạo cho ASR. Mô hình âm thanh
tạo các yếu tố word-piece trong danh sách giả thuyết; sau đó, LLM dự đoán
phiên âm chính xác sử dụng kiến thức ngôn ngữ và thông tin ngữ cảnh.

2 Công trình liên quan
2.1 Tính điểm lại ASR và Sửa lỗi
Để cải thiện tính chấp nhận ngôn ngữ của kết quả ASR, tính điểm lại LM đã được sử dụng rộng rãi
và đạt được cải thiện hiệu suất ổn định cho các hệ thống ASR [79,62,4]. Thông thường, một
LM bên ngoài được huấn luyện riêng biệt và sử dụng để tính điểm lại danh sách N-best của giả thuyết
được tạo bởi giải mã ASR với tìm kiếm chùm. Nhiều phương pháp khác nhau cho việc tích hợp LM
đã được đề xuất, như shallow fusion [17,104,46,83], deliberation [98,32,41,40,91,39],
component fusion [76], và cold fusion [81]. Một số tác giả đã sử dụng các mô hình LM đã được huấn luyện trước
để thay thế các LM có thể huấn luyện [86,74], và log-likelihood của mỗi giả thuyết được tính toán
sử dụng các mô hình một chiều, ví dụ GPT-2, hoặc pseudo-log-likelihood sử dụng các mô hình hai chiều
như BERT [21] và RoBERTa [59]. Trong ASR, LM cũng được sử dụng rộng rãi cho tác vụ sửa lỗi
bằng các ngôn ngữ khác nhau [96,29], chỉ tận dụng giả thuyết 1-best được tạo bởi mô hình ASR
[53,61,106,23,109,77]. Hơn nữa, các công trình gần đây hơn [60, 52, 51] sử dụng danh sách ứng viên
sau giải mã để sửa lỗi. Mặc dù Grammatical Error Correction (GEC) đã được khám phá tích cực [20,93,100],
sửa lỗi ASR khác biệt với GEC do tính tùy ý của ngôn ngữ nói [2], đòi hỏi nỗ lực từ cả
cộng đồng nghiên cứu giọng nói và NLP [18].

2.2 Mô hình Ngôn ngữ Lớn
Gần đây, đã có sự quan tâm mạnh mẽ đến các LLM dựa trên Transformer [84,70,75,107] trong cả
học viện và công nghiệp. Bằng cách học từ lượng dữ liệu văn bản khổng lồ, LLM có thể nắm bắt
các mẫu ngôn ngữ và mối quan hệ ngữ nghĩa, dẫn đến hiệu suất ấn tượng cho một loạt
các tác vụ xử lý ngôn ngữ tự nhiên (NLP) [5, 65, 95].

In-context Learning. Với các mô tả tác vụ cụ thể hoặc thông tin ngữ cảnh theo cặp, LLM
cho thấy khả năng thích ứng xuất sắc trên các tác vụ NLP hạ lưu mà không cần bất kỳ điều chỉnh tham số nào
[63,64,100]. Khả năng suy luận cụ thể tác vụ như vậy cũng được biết đến như in-context learning (ICL)
[99], sử dụng LLM để tạo văn bản mạch lạc hơn và phù hợp với miền hoặc tác vụ cụ thể
[44,16,49,73,8,108]. Gần đây, Task-activating Prompting (TAP) [100] là một trong những công trình
liên quan nhất, sử dụng việc inject các cặp input-output của ngữ cảnh hướng tác vụ (ví dụ, khởi tạo
câu hỏi prompt từ miền rộng để tinh chỉnh các ngữ cảnh trước đó như được hiển thị trong Hình 2)
với mục tiêu nâng cao khả năng zero-shot và few-shot của các LLM đã được huấn luyện trước đông lạnh
cho ASR lượt thứ hai. Chúng tôi tiếp tục đánh giá các phương pháp zero-shot và few-shot dựa trên TAP
với các ví dụ.

Low-rank Approximation based Neural Adapter. Điều chỉnh tất cả các tham số LLM cho một tác vụ
hạ lưu nhất định thường không khả thi do các ràng buộc về bộ nhớ. Nhiều nhà nghiên cứu tìm cách
giảm thiểu vấn đề đó bằng cách chỉ thích ứng một vài tham số hoặc tận dụng các mô-đun huấn luyện
bên ngoài cho

2https://github.com/Hypotheses-Paradise/Hypo2Trans
3Được ký hiệu là Hypo theses Pa radise, lấy cảm hứng từ "Icha Icha Paradise" từ Naruto.

--- TRANG 4 ---
một tác vụ mới [58,33]. Một công trình tiên phong [1] cho thấy rằng các mô hình được học quá tham số hóa
thực tế nằm trên một chiều intrinsic thấp, do đó, một phương pháp low-rank adaptation (LoRA) [38]
đã được đề xuất để gián tiếp điều chỉnh một số lớp dày đặc bằng cách tối ưu hóa các ma trận phân rã rank
của các lớp dày đặc. Do hiệu quả tính toán của nó, adaptation LoRA đã được áp dụng nhanh chóng
như một mô hình mới cho việc điều chỉnh LLM, hữu ích trong nhiều tác vụ hạ lưu khác nhau [105, 24, 42, 92].

3 Tạo Giả thuyết và Tạo Bộ dữ liệu
Chúng tôi giới thiệu quá trình tạo bộ dữ liệu HyPoradise trong phần này. Hệ thống ASR được sử dụng
để tạo giả thuyết N-best được minh họa trong 3.1, và sau đó chúng tôi giới thiệu miền giọng nói được chọn
trong 3.2. Cuối cùng, chúng tôi cung cấp thông tin thống kê và HP được tạo trong 3.2.

3.1 Hệ thống ASR
Chúng tôi sử dụng hai mô hình ASR tiên tiến, cụ thể là WavLM [14] và Whisper [69] để tạo giả thuyết N-best.
Bên cạnh hiệu suất xuất sắc và tính phổ biến của chúng, những mô hình này có tính đại diện
trong việc triển khai ASR vì: (1) WavLM là một mô hình ASR được huấn luyện tốt trên
LibriSpeech [66] nhưng gặp phải vấn đề không khớp miền, và (2) Whisper là một mô hình ASR
phổ quát nhưng thiếu tính đặc thù miền. Thêm chi tiết về hai mô hình ASR này được mô tả bên dưới:

WavLM: Chúng tôi sử dụng bộ công cụ ESPnet [94] cùng với mô hình đã được huấn luyện trước từ HuggingFace
để triển khai hệ thống ASR dựa trên WavLM. Kiến trúc WavLM gồm hai khối: front-end,
và mô hình ASR (tổng cộng 433 triệu tham số). Front-end gồm 24 lớp encoder
dựa trên Transformer [88] và được huấn luyện trước bằng kết hợp LibriLight [45]
(60k giờ dữ liệu), Gigaspeech [12] (10k giờ dữ liệu), và VoxPopuli [90] (24k giờ dữ liệu).
Các đặc trưng front-end được đưa vào ASR back-end để fine-tuning. Back-end gồm 12 lớp encoder
dựa trên Conformer [30], và 6 lớp decoder dựa trên Transformer. Quá trình fine-tuning
được thực hiện trên dữ liệu LibriSpeech 960 giờ. Ngoài ra, công thức giải mã WavLM
kết hợp tùy chọn tính điểm lại LM bên ngoài, trong đó LM bên ngoài áp dụng kiến trúc Transformer
với 16 lớp encoder và được huấn luyện sử dụng văn bản của dữ liệu LibriSpeech 960 giờ và
dữ liệu huấn luyện LM bổ sung từ web.

Whisper: Chúng tôi sử dụng mô hình Whisper-Large do OpenAI phát triển để tạo giả thuyết,
mà không có tính điểm lại mô hình ngôn ngữ trong miền. Cấu hình được sử dụng gồm một
kiến trúc Transformer encoder-decoder với 1,550 triệu tham số, được huấn luyện trên 680,000 giờ
dữ liệu giọng nói đa ngôn ngữ được gán nhãn yếu thu thập từ web.

Tận dụng hai mô hình ASR đã được huấn luyện trước này, chúng tôi đã sử dụng thuật toán beam search
trong quá trình giải mã và tạo danh sách N-best của giả thuyết câu cho mỗi waveform đầu vào.
Cho cả WavLM và Whisper, beam size mặc định được đặt là 60. Sau khi loại bỏ các câu nói có thể lặp lại,
chúng tôi chọn top-5 câu nói có xác suất cao nhất làm danh sách N-best, vì chúng đã mang
đủ yếu tố để dự đoán chính xác phiên âm. Các thí nghiệm tiếp theo xác nhận niềm tin này bằng cách
tính toán WER giới hạn trên chính xác sử dụng danh sách giả thuyết 5-best. Để xây dựng bộ dữ liệu HP,
chúng tôi thực hiện chiến lược giải mã này trên nhiều bộ dữ liệu ASR phổ biến (vui lòng xem Phần 3.2)
và tạo dữ liệu được ghép cặp bao gồm một danh sách giả thuyết 5-best và 1 phiên âm ground-truth.
Mã tiền xử lý và tạo cũng được phát hành để tích hợp corpus ASR mới vào HP. Tất cả các liên kết
của tài nguyên liên quan được trình bày trong Phụ lục.

3.2 Các Corpus Giọng nói Được chọn
Đối với việc chọn corpus, mục tiêu của chúng tôi là bao gồm các tình huống phổ biến của tác vụ ASR,
ví dụ, nền tiếng ồn và giọng địa phương của người nói. Do đó, chúng tôi thu thập và sửa đổi các corpus
sau với đặc điểm miền rõ ràng để tạo thành bộ dữ liệu HP.

LibriSpeech [66]: LibriSpeech là một corpus công cộng của giọng nói đọc từ sách nói, bao gồm 1,000
giờ dữ liệu giọng nói với người nói, giới tính, và giọng địa phương đa dạng. Để tạo dữ liệu huấn luyện HP,
chúng tôi loại trừ một số trường hợp đơn giản từ split train-960 có kết quả WER là 0, tạo ra 88,200
câu nói huấn luyện. Chúng tôi sử dụng toàn bộ split test-clean và test-other để tạo dữ liệu test HP.

CHiME-4 [89]: CHiME-4 là một bộ dữ liệu cho nhận diện giọng nói trường xa. Nó bao gồm các bản ghi
tiếng ồn thực và mô phỏng trong bốn môi trường tiếng ồn, tức là xe buýt, quán cà phê, khu vực người đi bộ,
và ngã tư đường phố. Chúng tôi

--- TRANG 5 ---
Bảng 1: Thống kê bộ dữ liệu HP về số lượng cặp giả thuyết-phiên âm và độ dài trung bình
câu nói trong các miền khác nhau.

Miền Tập Huấn luyện # Cặp Độ dài Tập Test # Cặp Độ dài Nguồn Danh mục
LibriSpeech Sách nói train-960 88,200 33.7 test-clean 2,620 20.1
test-other 2,939 17.8
CHiME4 Tiếng ồn train 8,738 17.0 test-real 1,320 16.4
WSJ Tin tức kinh doanh train-si284 37,514 17.5 dev93 503 16.7
eval92 333 17.3
SwitchBoard Điện thoại train 36,539 11.8 eval2000 2,000 11.8
CommonVoice Tiếng Anh có giọng train-accent 49,758 10.5 test-accent 2,000 10.5
Tedlium-3 Bài nói TED train 47,500 12.6 test 2,500 12.6
LRS2 Âm thanh BBC train 42,940 7.6 test 2,259 7.6
ATIS Thông tin hàng không train 3,964 12.4 test 809 11.3
CORAAL Phỏng vấn train 1,728 24.2 test 100 24.0
Tổng cộng train 316,881 18.1 test 17,383 14.1

sử dụng split train (với 8,738 câu nói) và test-real (với 1,320 câu nói) để tạo dữ liệu huấn luyện
và test HP. Bốn tiếng ồn khác nhau trong split test-real cũng được đánh giá riêng biệt trong Bảng 3.

WSJ [67]: Wall Street Journal (WSJ) là một benchmark được sử dụng rộng rãi cho nhận diện giọng nói.
Nó bao gồm giọng nói đọc từ người nói trong môi trường được kiểm soát, tập trung vào tin tức kinh doanh
và dữ liệu tài chính. Chúng tôi sử dụng split train-si284 (với 37,514 câu nói) để tạo tập huấn luyện HP.
Dev93 (với 503 câu nói) và eval92 (với 333 câu nói) được áp dụng để xây dựng các tập test.

SwitchBoard [26]: Corpus SwitchBoard là một bộ dữ liệu giọng nói điện thoại được thu thập từ các
cuộc hội thoại giữa các cặp người nói. Nó tập trung vào tiếng Anh Bắc Mỹ và bao gồm hơn 2.4k
cuộc hội thoại từ khoảng 200 người nói. Chúng tôi chọn ngẫu nhiên 36,539 mẫu từ split train
để tạo tập huấn luyện HP, cũng như 2,000 câu nói từ split eval2000 cho tập test HP.

CommonVoice [3]: CommonVoice 5.1 là một bộ dữ liệu có sẵn miễn phí cho nhận diện giọng nói.
Nó chứa các bản ghi giọng nói từ người nói đa dạng bằng hơn 60 ngôn ngữ. Để tạo bộ dữ liệu HP,
chúng tôi chọn ngẫu nhiên 51,758 mẫu từ split train-en với nhãn giọng, tức là Châu Phi, Úc, Ấn Độ,
và Singapore, trong đó tập huấn luyện chứa 49,758 mẫu và tập test chứa 2,000 mẫu.

Tedlium-3 [35]: Tedlium-3 là một bộ dữ liệu giọng nói được ghi từ TED Talks bằng nhiều ngôn ngữ.
Nó chứa một loạt tiếng ồn nền, giọng địa phương của người nói, chủ đề giọng nói, v.v. Xét đến
kích thước lớn của nó, chúng tôi chọn ngẫu nhiên 50,000 mẫu từ split train để tạo bộ dữ liệu HP,
trong đó tập huấn luyện chứa 47,500 mẫu và tập test chứa 2,500 mẫu.

LRS2 [19]: Lip Reading Sentences 2 (LRS2) là một bộ dữ liệu âm thanh-hình ảnh có nhãn quy mô lớn
có sẵn công khai, gồm 224 giờ video clip từ các chương trình BBC. Chúng tôi chọn ngẫu nhiên
42,940 mẫu từ split train làm tập huấn luyện, và 2,259 mẫu còn lại được sử dụng cho tập test.

ATIS [34]: Airline Travel Information System (ATIS) là một bộ dữ liệu gồm các truy vấn nói
về thông tin du lịch hàng không, như thời gian chuyến bay, giá cả, và tình trạng có sẵn. Nó chứa
khoảng 5,000 đến 5,400 câu nói, được ghi từ khoảng 500 đến 550 người nói.

CORAAL [47]: Corpus of Regional African American Language (CORAAL) là corpus công cộng đầu tiên
về dữ liệu AAL. Nó bao gồm các bản ghi âm thanh cùng với phiên âm chính tả được căn chỉnh thời gian
từ hơn 150 cuộc phỏng vấn ngôn ngữ xã hội. Để tạo bộ dữ liệu HP, chúng tôi chọn 1,728 mẫu
làm tập huấn luyện và 100 mẫu làm tập test.

3.3 Thống kê Bộ dữ liệu HyPoradise (HP)
Sau khi thực hiện giải mã beam search trên các bộ dữ liệu giọng nói được chọn được giới thiệu trong Phần 3.2,
chúng tôi đã thu thập hơn 334K cặp danh sách giả thuyết và phiên âm để tạo thành bộ dữ liệu HP,
bao gồm các tập huấn luyện và test. Thống kê cho bộ dữ liệu HP được đưa ra trong Bảng 1, cho thấy số lượng

--- TRANG 6 ---
Q: Bạn có biết về nhận diện giọng nói tự động không?
R: Có, ASR là [...]
Q: Bạn có biết về tính điểm lại mô hình ngôn ngữ trong ASR không?
R: Chắc chắn, tính điểm lại LM [...]
Q: Bạn có thể tạo một ví dụ với danh sách 5-best không?
R: Danh sách giả thuyết 5-best: [...], và câu trả lời là [...] Học trong ngữ cảnh tự kích hoạt
Q: Vui lòng làm điều tương tự trên danh sách N-best sau:
   [Giả thuyết N-best]
R: Tất nhiên. Phiên âm thực sự nên là: [...] Tôi cung cấp #n trường hợp thực trong miền [CLS]:
     Danh sách giả thuyết N-best là: [...],
     và phiên âm thực sự là: [...] 
 
   Minh họa n-shot

Hình 2: Đánh giá có thể mở rộng của Task-Activating Prompting [100] (TAP) dựa trên học trong ngữ cảnh.
Minh họa trong hộp màu xanh được rút từ tập huấn luyện, là tùy chọn cho đầu vào LLM.

Mô hình Sửa lỗi
Prompt + Danh sách giả thuyết N-best Phiên âm dự đoán
Trọng số
Đã huấn luyện trước
A= B=0
（a） （b）
Đầu vào Ẩn

Hình 3: (a) Cấu trúc của H2T-ft. (b) Tham số hóa lại trong H2T-LoRA. Hộp đặc biểu thị mô-đun
được cố định trong quá trình điều chỉnh trong khi hộp đứt nét đại diện cho có thể huấn luyện. Màu xanh
biểu thị trọng số đã được huấn luyện trước trên bộ dữ liệu khác.

cặp và độ dài trung bình trong các miền và split khác nhau. Chúng tôi sẽ phát hành các bộ dữ liệu
đã tạo và kêu gọi thêm nhiều cặp giả thuyết-phiên âm hướng đến nỗ lực cộng đồng bền vững.

4 Sửa lỗi ASR từ Giả thuyết đến Phiên âm
Chúng tôi giới thiệu một sơ đồ huấn luyện hypotheses-to-transcription (H2T) sử dụng bộ dữ liệu HP
được thu thập để nâng cao hiệu suất ASR với tích hợp LLM. Với dữ liệu có nhãn hạn chế, học trong ngữ cảnh [100]
được sử dụng để tạo thành các prompt cụ thể tác vụ và minh họa trong miền: Kiến thức ngôn ngữ
trong LLM được khai thác mà không cần điều chỉnh tham số. Hơn nữa, chúng tôi trình bày hai phương pháp
có thể huấn luyện fine-tuning (ft) và H2T-LoRA để học ánh xạ hypotheses-to-transcription khi
có đủ lượng dữ liệu có nhãn.

4.1 Huấn luyện Hypotheses-to-Transcription (H2T)
Ngoài học trong ngữ cảnh, chúng tôi giới thiệu hai phương pháp có thể điều chỉnh tham số để học
ánh xạ hypotheses-to-transcription theo cách sequence-to-sequence: H2T-ft và H2T-LoRA.

H2T-ft biểu thị fine-tuning tất cả các tham số của một mô hình nơ-ron với dữ liệu có nhãn
của mỗi miền HP. Cụ thể, chúng tôi giới thiệu một phương pháp tương tự với N-best T5,
sử dụng các giả thuyết khác để cải thiện giả thuyết 1-best như được hiển thị trong Hình 3.
Để hạn chế không gian giải mã, chúng tôi thêm một tiêu chí mục mới Lft=∑ᵢ₌₁ᴺ αᵢlogP(x⁽ⁱ⁾|x, θ),
trong đó x⁽ⁱ⁾ là giả thuyết thứ i trong danh sách N-best. Mục này nhằm khuyến khích mô hình sửa lỗi
ưu tiên xem xét các token vào danh sách giả thuyết N-best, ngăn chặn việc sửa đổi tùy ý trong
không gian giải mã khổng lồ. αᵢ là một siêu tham số cho giả thuyết thứ i giảm theo thứ tự
được xếp hạng bởi mô hình âm thanh.

H2T-LoRA tránh điều chỉnh toàn bộ tập tham số của một mô hình đã được huấn luyện trước bằng cách
chèn một mô-đun nơ-ron với một số lượng nhỏ tham số có thể huấn luyện bổ sung để xấp xỉ
các cập nhật tham số đầy đủ, cho phép học hiệu quả ánh xạ H2T mà không ảnh hưởng đến
các tham số đã được huấn luyện trước của LLM. H2T-LoRA giới thiệu các ma trận phân rã
low-rank có thể huấn luyện vào các lớp hiện có của LLM, cho phép mô hình thích ứng với
dữ liệu mới trong khi giữ LLM gốc cố định để duy trì kiến thức trước đó. Cụ thể, LoRA
thực hiện tham số hóa lại mỗi lớp mô hình được biểu thị như phép nhân ma trận bằng cách
inject các ma trận phân rã low-rank (Hình 3 (b)). Kết quả là, các

--- TRANG 7 ---
Bảng 2: Kết quả WER (%) của H2T-ft và H2T-LoRA trong cài đặt fine-tuning. "onb" và "ocp" tương ứng
biểu thị n-best oracle và compositional oracle được định nghĩa trong 5.2.

Tập Test Baseline LMrank H2T-ft H2T-LoRA Oracle
T5 LLaMA T5 LLaMA onb ocp
WSJ 4.5 4.3 4.0 3.8 2.7−40.0% 2.2−51.1% 4.1 1.2
ATIS 8.3 6.9 2.7 3.4 1.7−79.5% 1.9−77.1% 5.2 1.1
CHiME-4 11.1 11.0 7.9 8.2 7.0−36.9% 6.6−40.5% 9.1 2.8
Tedlium-3 8.5 8.0 6.6 5.2 7.4−12.9% 4.6−45.9% 3.0 0.7
CV-accent 14.8 16.0 12.9 15.5 11.0−25.7% 11.0−25.7% 11.4 7.9
SwitchBoard 15.7 15.4 15.9 18.4 14.9−5.1% 14.1−10.2% 12.6 4.2
LRS2 10.1 9.6 9.5 10.2 6.6−34.7% 8.8−12.9% 6.9 2.6
CORAAL 21.4 21.4 23.1 22.9 20.9−2.3% 19.2−10.3% 21.8 10.7

biểu diễn được tạo bởi LLM không bị méo mó do điều chỉnh cụ thể tác vụ, trong khi mô-đun adapter
có được khả năng dự đoán phiên âm thực sự từ giả thuyết N-best.

Nhờ vào huấn luyện hiệu quả, chúng tôi có thể sử dụng một mô hình ngôn ngữ quy mô lớn trong
phương pháp H2T-LoRA, dự kiến sẽ hiểu mô tả tác vụ và nắm bắt mối tương quan trong
danh sách N-best. Trong khi đó, thay vì thêm một mục tiêu huấn luyện bổ sung trong H2T-ft,
chúng tôi hạn chế không gian giải mã của H2T-LoRA bằng cách thêm yêu cầu trong mô tả tác vụ.

5 Kết quả Thực nghiệm
5.1 Cấu hình Mô hình Ngôn ngữ
T5(0.75B ∼3B): Họ T5 [72] là một tập các mô hình encoder-decoder được huấn luyện trước trên
một hỗn hợp đa tác vụ của các tác vụ không có giám sát và có giám sát và mỗi tác vụ được chuyển đổi
thành định dạng text-to-text. T5 hoạt động tốt trên nhiều tác vụ khác nhau ngay từ đầu bằng cách
thêm tiền tố khác nhau vào đầu vào tương ứng với mỗi tác vụ, ví dụ, cho dịch máy hoặc tóm tắt văn bản.
Trong bài báo này, chúng tôi chọn T5-large (0.75B) làm mô hình sửa lỗi trong phương pháp H2T-ft.

LLaMA (7B∼65B): Được đề xuất bởi Meta AI, LLaMA [84] là một bộ sưu tập các mô hình ngôn ngữ
nền tảng từ 7B, 13B, 30B, và 65B tham số. Nó được huấn luyện độc quyền trên các bộ dữ liệu có sẵn
công khai, và cho thấy hiệu quả đáng kể trên các benchmark NLP. Chúng tôi chọn LLaMA-13B cho
adaptation LoRA trong phương pháp H2T-LoRA như một cài đặt tốt nhất dưới các ablation.

GPT-3.5 (175B): Được đề xuất bởi OpenAI, GPT-3.5-turbo là một trong những mô hình ngôn ngữ lớn
tiên tiến nhất, cung cấp sức mạnh cho ChatGPT phổ biến. Nó đã được tối ưu hóa từ GPT-3 [5]
cho mục đích trò chuyện nhưng hoạt động tốt cho các tác vụ hoàn thành truyền thống. Chúng tôi sử dụng
GPT-3.5-turbo trong học trong ngữ cảnh kích hoạt tác vụ [100], thực hiện các thí nghiệm học
zero-shot và few-shot với prompt tác vụ được thiết kế.

5.2 Huấn luyện và Đánh giá
Cho cài đặt few-shot, các prompt tác vụ cụ thể, cùng với các phản hồi của LLM từ prompting
ICL kích hoạt tác vụ [100], được cung cấp trong Phụ lục (trang 20). Cho cài đặt fine-tuning,
cấu hình chi tiết của H2T-ft và H2T-LoRA cũng được giải thích trong Phụ lục. Hơn nữa, chúng tôi
phát hành một số mô hình sửa lỗi đã được huấn luyện trước để cho phép độc giả quan tâm
tái tạo kết quả của chúng tôi.

Chúng tôi báo cáo kết quả WER làm thước đo đánh giá cho tất cả các phương pháp. Ngoài ra,
chúng tôi báo cáo hai WER oracle để so sánh, đó là 1) n-best oracle onb: WER của "ứng viên tốt nhất"
trong danh sách giả thuyết N-best, và 2) phương pháp compositional oracle ocp: WER có thể đạt được
sử dụng "tất cả token" trong danh sách giả thuyết N-best. onb có thể được xem như hiệu suất giới hạn trên
của phương pháp dựa trên re-rank, trong khi ocp biểu thị giới hạn trên của sửa lỗi sử dụng
các yếu tố xuất hiện trong danh sách.

--- TRANG 8 ---
Bảng 3: Kết quả WER cross-domain bằng ICL kích hoạt tác vụ [100] trong cài đặt zero-shot và few-shot.
"onb" và "ocp" tương ứng biểu thị n-best oracle và compositional oracle được định nghĩa trong 5.2.

Miền Tập Test Baseline Học trong Ngữ cảnh (ICL) n-shot Oracle
Chuyển đổi n= 0 n= 1 n= 5 n= 10 onb ocp
Tình huống
Cụ thể WSJ-dev93 9.0 8.5−5.6% 7.8−13.3% 7.7−14.4% 7.1−21.1% 6.5 5.3
WSJ-eval92 7.6 7.3−3.9% 6.6−13.2% 6.6−13.2% 6.3−17.1% 5.5 4.7
ATIS 5.8 5.5−5.2% 5.1−12.1% 5.0−13.8% 4.7−19.0% 3.5 2.4
Tiếng ồn
Phổ biến CHiME4-bus 18.8 17.6−6.4% 16.7−11.2% 16.2−13.8% 15.9−20.7% 16.8 10.7
CHiME4-caf 16.1 14.7−8.7% 14.3−11.1% 13.7−14.9% 13.2−18.0% 13.3 9.1
CHiME4-ped 11.5 10.9−5.2% 9.9−14.4% 9.7−15.7% 9.4−18.3% 8.5 5.5
CHiME4-str 11.4 10.9−4.4% 10.0−12.3% 9.7−14.9% 9.2−19.3% 9.0 6.0
Giọng
Người nói CV-af 25.3 24.9−1.6% 24.2−4.3% 23.6−6.7% 22.6−10.7% 23.6 21.7
CV-au 25.8 25.1−2.7% 24.1−6.6% 24.0−7.0% 23.3−9.7% 24.9 21.8
CV-in 28.6 27.6−3.5% 25.6−10.5% 25.0−12.6% 24.4−14.7% 27.1 22.6
CV-sg 26.4 26.5+0.4% 25.0−5.3% 25.1−4.9% 23.7−10.2% 25.5 22.2

Bảng 4: Nghiên cứu trường hợp ICL. Câu nói được rút từ bộ dữ liệu WSJ-dev93.

Loại Câu nói WER
Giả thuyết 1 Các nhà ngân hàng tại Hồng Kông kỳ vọng xinnepec sẽ quay lại xin thêm khoản vay 16.7
bởi AM khi nó phát triển ngành công nghiệp hóa dầu của Trung Quốc.
Giả thuyết 2 Các nhà ngân hàng tại Hồng Kông kỳ vọng xinepec sẽ quay lại xin thêm khoản vay 8.3
bởi AM khi nó phát triển ngành công nghiệp hóa dầu của Trung Quốc.
Sửa lỗi Các nhà ngân hàng tại Hồng Kông kỳ vọng Sinopec sẽ quay lại xin thêm khoản vay 0
bởi LLM khi nó phát triển ngành công nghiệp hóa dầu của Trung Quốc.
Phiên âm Các nhà ngân hàng tại Hồng Kông kỳ vọng Sinopec sẽ quay lại xin thêm khoản vay -
Ground-truth khi nó phát triển ngành công nghiệp hóa dầu của Trung Quốc.

5.3 Kết quả của H2T-ft và H2T-LoRA
Chúng tôi đầu tiên báo cáo kết quả WER cho H2T-ft và H2T-LoRA trong cài đặt fine-tuning,
trong đó tập huấn luyện của HP có sẵn để học ánh xạ H2T. Whisper được sử dụng làm mô hình âm thanh
để tạo giả thuyết, và một mô hình ngôn ngữ vanilla LMrank được huấn luyện sử dụng phiên âm
trong miền của tập huấn luyện, và sau đó nó xếp hạng lại các giả thuyết theo perplexity.
Từ Bảng 2, chúng tôi quan sát thấy rằng 1) các kỹ thuật sửa lỗi đạt được cải thiện hiệu suất
đáng kể trong các tình huống cụ thể, trong đó H2T-LoRA tương ứng giảm 77.1% và 55.1% WER tương đối
trên ATIS và WSJ. 2) Hiệu suất WER trên CHiME-4 và CV-accent chứng minh các phương pháp
sửa lỗi được đề xuất cải thiện độ bền vững trên tiếng ồn nền và giọng địa phương của người nói.
Ngoài ra, H2T-LoRA trên hai bộ dữ liệu này đều vượt qua giới hạn trên của phương pháp dựa trên
xếp hạng lại tham chiếu đến onb. 3) Nói chung, H2T-LoRA thường tạo ra kết quả WER tốt hơn
so với H2T-ft, vì adapter low-rank cho phép LLM giữ kiến thức đã được huấn luyện trước
và tránh vấn đề over-fitting.

Hạn chế và Nghiên cứu Thất bại. Chúng tôi nhận thấy rằng có hiện tượng over-fitting trong
các kỹ thuật sửa lỗi của chúng tôi, đặc biệt là trong H2T-ft nơi tất cả các tham số đều có thể điều chỉnh.
Hơn nữa, giá trị trung bình và phương sai của độ dài câu nói có thể ảnh hưởng đến kết quả WER,
vì kết quả H2T-ft trên CORAAL (giọng nói dạng dài) và SwitchBoard (phương sai lớn về độ dài)
đều thất bại trong việc nâng cao hiệu suất ASR. Trên LibriSpeech, khi WER thấp (1.8% bởi WavLM),
có ít chỗ để sửa lỗi nhận diện với framework được đề xuất. Kết quả thực nghiệm và liệt kê
các trường hợp thất bại đại diện có thể được tìm thấy trong Bảng 6 và Bảng 7 của Phụ lục.
Với bằng chứng về nhiều chỗ cho cải thiện hiệu suất hơn nữa, đề xuất của chúng tôi do đó
phục vụ như một benchmark phù hợp để đánh giá đóng góp của các LLM hiện tại và tương lai cho ASR.

5.4 Kết quả Học trong Ngữ cảnh
Chúng tôi thực hiện các thí nghiệm học trong ngữ cảnh trong tình huống thực tế khi một hệ thống ASR
đã được huấn luyện tốt gặp phải sự không khớp miền. Với mục đích này, WavLM được chọn làm
mô hình âm thanh trong miền

--- TRANG 9 ---
mô hình, và GPT-3.5 phục vụ như LLM để sửa lỗi. Chúng tôi chủ yếu xem xét các sự chuyển đổi miền
phổ biến của ứng dụng: tình huống cụ thể, tiếng ồn nền phổ biến, và giọng địa phương của người nói,
trong đó giả thuyết 5-best được chọn làm đầu vào ngữ cảnh. Từ Bảng 3, chúng tôi có thể quan sát thấy rằng:
(1) Mà không cần bất kỳ dữ liệu trong miền nào, LLM có thể được hưởng lợi từ kết quả ASR dựa trên
danh sách giả thuyết. Cải thiện hiệu suất này chủ yếu dựa vào kiến thức ngôn ngữ của LLM
và các mô tả kích hoạt tác vụ [100] (ví dụ, chuỗi gợi ý tác vụ) trong pipeline.
(2) Một vài cặp trong miền có hiệu quả nâng cao cải thiện hiệu suất về mặt WER.
Từ đầu ra cuối cùng của quá trình lý luận, chúng tôi thấy rằng LLM cố gắng tóm tắt quy tắc
từ minh họa và sau đó áp dụng nó cho ví dụ test đã cho.
(3) Tận dụng cơ sở kiến thức rộng lớn, LLM thậm chí có thể sửa các token bị thiếu
mà không có trong danh sách giả thuyết về mặt thông tin ngữ cảnh.

Để minh họa quan sát thứ ba, chúng tôi thực hiện nghiên cứu trường hợp trên WSJ-dev93 trong Bảng 4.
Theo phiên âm ground-truth, hai lỗi (được hiển thị màu đỏ) được bao gồm trong giả thuyết thứ 1,
trong đó "petro chemical" bị nhận diện sai thành hai token có lẽ do phong cách nói của người nói.
LLM sửa lỗi này vì "petrochemical" có thể được tìm thấy trong giả thuyết thứ 2. Tuy nhiên,
"Sinopec" không được thấy trong quá trình huấn luyện ASR, dẫn đến việc nó được nhận diện
thành các token lạ ("xinnepec" hoặc "xinepec") trong giả thuyết. Trong trường hợp này,
LLM cho thấy sự sửa lỗi giống con người – nó thành công suy luận token đúng dựa trên
cách phát âm của "xinnepec", cũng như ngữ cảnh của "China's petrochemical". Thực tế,
Sinopec là một công ty hóa dầu Trung Quốc.

5.5 Thảo luận Bổ sung
Ảnh hưởng đến Phát hiện Ý định Ngôn ngữ Nói. Chúng tôi kiểm tra ảnh hưởng của sửa lỗi
đến một tác vụ hạ lưu của phát hiện ý định ngôn ngữ nói [80] (SID). Với mục đích này,
chúng tôi tái tạo một mô hình SID dựa trên BERT [13] và tương ứng đưa vào câu nói 1-best
và câu nói đã được sửa lỗi bởi H2T-LoRA để so sánh. Kết quả ablation trên bộ dữ liệu ATIS
được báo cáo trong Phụ lục, cho thấy rằng kỹ thuật sửa lỗi của chúng tôi cũng có thể có lợi
cho tác vụ SID về mặt độ chính xác phát hiện. (3) Sửa lỗi LLM dựa trên giả thuyết N-best
có thể nâng cao hiệu quả kết quả SIT hạ lưu, đạt được độ chính xác tương đương với việc
sử dụng phiên âm ground-truth (97.4% so với 97.9%).

Kết quả Prompting Zero-shot. Chúng tôi cuối cùng báo cáo một đánh giá prompting ban đầu
trên CHiME-4 trong cài đặt zero-shot. Xét đến độ khó của tác vụ, T5 và LLaMA được sử dụng
để sửa lỗi giả thuyết. Để so sánh, chúng tôi cũng cung cấp kết quả sửa lỗi sử dụng
GPT-2 nhỏ hơn nhiều (1.5B) với baseline LM 5-gram được huấn luyện bởi phiên âm trong miền.
Chúng tôi sử dụng LLaMA 13B để thực hiện các tác vụ sửa lỗi zero-shot này. Sử dụng tập test
được trích xuất từ Whisper, chúng tôi quan sát thấy rằng phương pháp zero-shot không mang lại
kết quả cải thiện trên CHiME-4 (11.5 ±0.5%) và CV-accent (14.9% ±1.5%). Pipeline zero-shot
này thực hiện kém ổn định hơn trên tập test khác được thảo luận trong Bảng 2, mà chúng tôi
xem xét là trường hợp thất bại với độ lệch chuẩn vượt quá giá trị tuyệt đối 10% về mặt WER.
Đối với sửa lỗi dựa trên T5, chúng tôi nhận thấy rằng phương pháp này cũng thất bại trong việc
thực hiện sửa lỗi zero-shot bằng cách sử dụng 0.75B.

Công việc tương lai. Chúng tôi thấy rằng LLM có thể nhận biết thông tin âm thanh trong quá trình
huấn luyện trước, vì chúng có xu hướng thực hiện sửa lỗi sử dụng các token có cách phát âm tương tự.
Do đó, công việc tương lai đầu tiên của chúng tôi là bao gồm thêm thông tin âm thanh trong bộ dữ liệu HP,
như độ tin cậy cấp token được cung cấp bởi engine ASR. Hơn nữa, xét đến lượng dữ liệu khác nhau
của mỗi miền, các phương pháp huấn luyện hiệu quả tham số hơn bên cạnh adaptation low-rank
nên được thảo luận cho việc điều chỉnh LLM [54], ví dụ, lập trình lại mô hình [102,31],
prompting [10] và adaptation cross-modal [97, 101, 71].

6 Kết luận
Để khám phá lợi ích trong việc học kết hợp giọng nói-ngôn ngữ, công trình này giới thiệu
một benchmark ASR mới sử dụng LLM để dự đoán phiên âm từ giả thuyết N-best.
Benchmark của chúng tôi chứa một bộ dữ liệu HP mới gồm hơn 334K cặp giả thuyết-phiên âm
được thu thập từ 9 corpus ASR công cộng khác nhau. Trong cài đặt few-shot, chúng tôi chứng minh
rằng LLM với học trong ngữ cảnh có thể phục vụ như một back end plug-and-play để giảm thiểu
hiệu quả sự chuyển đổi miền của ASR. Trong cài đặt fine-tuning, kỹ thuật sửa lỗi được đề xuất
dựa trên LLM đạt được hiệu suất WER tốt hơn so với giới hạn trên của phương pháp dựa trên
xếp hạng lại, cung cấp một mô hình mới để áp dụng ASR trong một số điều kiện thách thức,
như tiếng ồn nền và giọng địa phương của người nói. Chúng tôi tin rằng benchmark và
phát hiện của chúng tôi cung cấp những hiểu biết mới và độc đáo về ASR nâng cao bằng LLM.

--- TRANG 10 ---
Tài liệu tham khảo
[1]Armen Aghajanyan, Luke Zettlemoyer, và Sonal Gupta. Intrinsic dimensionality explains the
effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255, 2020.
[2]Alena Aksenova, Daan van Esch, James Flynn, và Pavel Golik. How might we create better
benchmarks for speech recognition? In Proceedings of the 1st Workshop on Benchmarking:
Past, Present and Future, pages 22–34, 2021.
[3]Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer,
Reuben Morais, Lindsay Saunders, Francis M Tyers, và Gregor Weber. Common voice: A
massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019.
[4]Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, và Stanley Chen. Bidirectional recurrent
neural network language models for automatic speech recognition. In 2015 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5421–5425. IEEE,
2015.
[5]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems, 33:1877–
1901, 2020.
[6]Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, và Hao Zheng. Aishell-1: An open-source
mandarin speech corpus and a speech recognition baseline. In 2017 20th conference of the
oriental chapter of the international coordinating committee on speech databases and speech
I/O systems and assessment (O-COCOSDA), pages 1–5. IEEE, 2017.
[7]Daniela Caballero, Roberto Araya, Hanna Kronholm, Jouni Viiri, André Mansikkaniemi,
Sami Lehesvuori, Tuomas Virtanen, và Mikko Kurimo. Asr in classroom today: Automatic
visualization of conceptual network in science classrooms. In Data Driven Approaches in
Digital Education: 12th European Conference on Technology Enhanced Learning, EC-TEL
2017, Tallinn, Estonia, September 12–15, 2017, Proceedings 12, pages 541–544. Springer,
2017.
[8]Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre
Richemond, James McClelland, và Felix Hill. Data distributional properties drive emergent
in-context learning in transformers. Advances in Neural Information Processing Systems,
35:18878–18891, 2022.
[9]William Chan, Navdeep Jaitly, Quoc Le, và Oriol Vinyals. Listen, attend and spell: A neural
network for large vocabulary conversational speech recognition. In 2016 IEEE international
conference on acoustics, speech and signal processing (ICASSP), pages 4960–4964. IEEE,
2016.
[10] Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing Kang, Wei-Cheng Tseng, Shang-Wen Li,
và Hung-yi Lee. Speechprompt v2: Prompt tuning for speech classification tasks. arXiv
preprint arXiv:2303.00733, 2023.
[11] Chen Chen, Nana Hou, Yuchen Hu, Shashank Shirol, và Eng Siong Chng. Noise-robust
speech recognition with 10 minutes unparalleled in-domain data. In ICASSP 2022-2022
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
4298–4302. IEEE, 2022.
[12] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan
Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr
corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021.
[13] Qian Chen, Zhu Zhuo, và Wen Wang. Bert for joint intent classification and slot filling. arXiv
preprint arXiv:1902.10909, 2019.
[14] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li,
Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. Wavlm: Large-scale self-supervised
pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal
Processing, 16(6):1505–1518, 2022.

--- TRANG 11 ---
[15] Zih-Ching Chen, Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Shou-Yiin Chang,
Rohit Prabhavalkar, Hung-yi Lee, và Tara N Sainath. How to estimate model transferability
of pre-trained speech models? Proc. Interspeech, 2023.
[16] Hyunsoo Cho, Hyuhng Joon Kim, Junyeob Kim, Sang-Woo Lee, Sang-goo Lee, Kang Min
Yoo, và Taeuk Kim. Prompt-augmented linear probing: Scaling beyond the limit of few-shot
in-context learners. arXiv preprint arXiv:2212.10873, 2022.
[17] Jan Chorowski và Navdeep Jaitly. Towards better decoding and language model integration
in sequence to sequence models. arXiv preprint arXiv:1612.02695, 2016.
[18] Grzegorz Chrupała. Putting natural in natural language processing. arXiv preprint
arXiv:2305.04572, 2023.
[19] Joon Son Chung, Andrew Senior, Oriol Vinyals, và Andrew Zisserman. Lip reading sentences
in the wild. In 2017 IEEE conference on computer vision and pattern recognition (CVPR),
pages 3444–3453. IEEE, 2017.
[20] Daniel Dahlmeier và Hwee Tou Ng. Better evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 568–572, 2012.
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.
[22] Linhao Dong, Shuang Xu, và Bo Xu. Speech-transformer: a no-recurrence sequence-to-
sequence model for speech recognition. In 2018 IEEE international conference on acoustics,
speech and signal processing (ICASSP), pages 5884–5888. IEEE, 2018.
[23] Samrat Dutta, Shreyansh Jain, Ayush Maheshwari, Souvik Pal, Ganesh Ramakrishnan, và
Preethi Jyothi. Error correction in asr using sequence-to-sequence models. arXiv preprint
arXiv:2202.01157, 2022.
[24] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-
language pre-training: Basics, recent advances, and future trends. Foundations and Trends ®
in Computer Graphics and Vision, 14(3–4):163–352, 2022.
[25] Ankur Gandhe và Ariya Rastrow. Audio-attention discriminative language model for asr
rescoring. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 7944–7948. IEEE, 2020.
[26] John J Godfrey, Edward C Holliman, và Jane McDaniel. Switchboard: Telephone speech
corpus for research and development. In Acoustics, Speech, and Signal Processing, IEEE
International Conference on, volume 1, pages 517–520. IEEE Computer Society, 1992.
[27] Alex Graves. Sequence transduction with recurrent neural networks. arXiv preprint
arXiv:1211.3711, 2012.
[28] Alex Graves, Santiago Fernández, Faustino Gomez, và Jürgen Schmidhuber. Connectionist
temporal classification: labelling unsegmented sequence data with recurrent neural networks.
In Proceedings of the 23rd international conference on Machine learning, pages 369–376,
2006.
[29] Terri A Greenslade và J César Félix-Brasdefer. Error correction and learner perceptions in l2
spanish writing. In Selected Proceedings of the 7th Conference on the Acquisition of Spanish
and Portuguese as First and Second Languages, pages 185–194. Somerville, MA: Cascadilla
Proceedings Project, 2006.
[30] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,
Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented
transformer for speech recognition. Proc. Interspeech 2020, pages 5036–5040, 2020.

--- TRANG 12 ---
[31] Karen Hambardzumyan, Hrant Khachatrian, và Jonathan May. Warp: Word-level adversarial
reprogramming. arXiv preprint arXiv:2101.00121, 2021.
[32] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Feder-
mann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving hu-
man parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567,
2018.
[33] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low,
Lidong Bing, và Luo Si. On the effectiveness of adapter-based tuning for pretrained language
model adaptation. arXiv preprint arXiv:2106.03164, 2021.
[34] Charles T Hemphill, John J Godfrey, và George R Doddington. The atis spoken language
systems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at
Hidden Valley, Pennsylvania, June 24-27, 1990, 1990.
[35] François Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, và Yannick
Esteve. Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker
adaptation. In Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig,
Germany, September 18–22, 2018, Proceedings 20, pages 198–208. Springer, 2018.
[36] Pavel Hlubík, Martin Španˇel, Marek Bohác, và Lenka Weingartová. Inserting punctuation to
asr output in a real-time production environment. In Text, Speech, and Dialogue: 23rd Inter-
national Conference, TSD 2020, Brno, Czech Republic, September 8–11, 2020, Proceedings,
pages 418–425. Springer, 2020.
[37] Duc Tam Hoang, Shamil Chollampatt, và Hwee Tou Ng. Exploiting n-best hypotheses to
improve an smt approach to grammatical error correction. arXiv preprint arXiv:1606.00210,
2016.
[38] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.
[39] Ke Hu, Bo Li, và Tara N Sainath. Scaling up deliberation for multilingual asr. In 2022 IEEE
Spoken Language Technology Workshop (SLT), pages 771–776. IEEE, 2023.
[40] Ke Hu, Ruoming Pang, Tara N Sainath, và Trevor Strohman. Transformer based deliberation
for two-pass speech recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT),
pages 68–74. IEEE, 2021.
[41] Ke Hu, Tara N Sainath, Ruoming Pang, và Rohit Prabhavalkar. Deliberation model based
two-pass end-to-end speech recognition. In ICASSP 2020-2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 7799–7803. IEEE, 2020.
[42] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing,
và Soujanya Poria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of
large language models. arXiv preprint arXiv:2304.01933, 2023.
[43] Hongzhao Huang và Fuchun Peng. An empirical study of efficient asr rescoring with
transformers. arXiv preprint arXiv:1910.11450, 2019.
[44] Yukun Huang, Yanda Chen, Zhou Yu, và Kathleen McKeown. In-context learning distilla-
tion: Transferring few-shot learning ability of pre-trained language models. arXiv preprint
arXiv:2212.10670, 2022.
[45] Jacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-
Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen,
et al. Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
7669–7673. IEEE, 2020.

--- TRANG 13 ---
[46] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, và Rohit
Prabhavalkar. An analysis of incorporating an external language model into a sequence-to-
sequence model. In 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 1–5828. IEEE, 2018.
[47] Tyler Kendall và Charlie Farrington. The corpus of regional african american language.
version 2021.07. eugene, or: The online resources for african american language project, 2021.
[48] Anis Koubaa. Gpt-4 vs. gpt-3.5: A concise showdown. arXiv preprint, 2023.
[49] Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry
Tessler, Antonia Creswell, James L McClelland, Jane X Wang, và Felix Hill. Can language
models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.
[50] Siddique Latif, Junaid Qadir, Adnan Qayyum, Muhammad Usama, và Shahzad Younis.
Speech technology for healthcare: Opportunities, challenges, and state of the art. IEEE
Reviews in Biomedical Engineering, 14:342–356, 2020.
[51] Yichong Leng, Xu Tan, Wenjie Liu, Kaitao Song, Rui Wang, Xiang-Yang Li, Tao Qin, Ed Lin,
và Tie-Yan Liu. Softcorrect: Error correction with soft detection for automatic speech
recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,
pages 13034–13042, 2023.
[52] Yichong Leng, Xu Tan, Rui Wang, Linchen Zhu, Jin Xu, Wenjie Liu, Linquan Liu, Tao Qin,
Xiang-Yang Li, Edward Lin, et al. Fastcorrect 2: Fast error correction on multiple candidates
for automatic speech recognition. arXiv preprint arXiv:2109.14420, 2021.
[53] Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiangyang
Li, Edward Lin, và Tie-Yan Liu. Fastcorrect: Fast error correction with edit alignment for
automatic speech recognition. Advances in Neural Information Processing Systems, 34:21708–
21719, 2021.
[54] Brian Lester, Rami Al-Rfou, và Noah Constant. The power of scale for parameter-efficient
prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
[55] Jinyu Li, Li Deng, Yifan Gong, và Reinhold Haeb-Umbach. An overview of noise-robust
automatic speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language
Processing, 22(4):745–777, 2014.
[56] Mingda Li, Weitong Ruan, Xinyue Liu, Luca Soldaini, Wael Hamza, và Chengwei Su.
Improving spoken language understanding by exploiting asr n-best hypotheses. arXiv preprint
arXiv:2001.05284, 2020.
[57] Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn, Gilad
Avidov, Ronan Collobert, và Gabriel Synnaeve. Rethinking evaluation in asr: Are our models
robust enough? arXiv preprint arXiv:2010.11745, 2020.
[58] Zhaojiang Lin, Andrea Madotto, và Pascale Fung. Exploring versatile generative language
model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.
[59] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[60] Rao Ma, Mark JF Gales, Kate Knill, và Mengjie Qian. N-best t5: Robust asr error cor-
rection using multiple input hypotheses and constrained decoding space. arXiv preprint
arXiv:2303.00456, 2023.
[61] Anirudh Mani, Shruti Palaskar, Nimshi Venkat Meripo, Sandeep Konam, và Florian Metze.
Asr error correction and domain adaptation using machine translation. In ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
6344–6348. IEEE, 2020.

--- TRANG 14 ---
[62] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, và Sanjeev Khudanpur.
Recurrent neural network based language model. In Interspeech, volume 2, pages 1045–1048.
Makuhari, 2010.
[63] Sewon Min, Mike Lewis, Luke Zettlemoyer, và Hannaneh Hajishirzi. Metaicl: Learning to
learn in context. arXiv preprint arXiv:2110.15943, 2021.
[64] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, và
Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning
work? arXiv preprint arXiv:2202.12837, 2022.
[65] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models
to follow instructions with human feedback. Advances in Neural Information Processing
Systems, 35:27730–27744, 2022.
[66] Vassil Panayotov, Guoguo Chen, Daniel Povey, và Sanjeev Khudanpur. Librispeech: an
asr corpus based on public domain audio books. In 2015 IEEE international conference on
acoustics, speech and signal processing (ICASSP), pages 5206–5210. IEEE, 2015.
[67] Douglas B Paul và Janet Baker. The design for the wall street journal-based csr corpus. In
Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York,
February 23-26, 1992, 1992.
[68] Fuchun Peng, Scott Roy, Ben Shahshahani, và Françoise Beaufays. Search results based
n-best hypothesis rescoring with maximum entropy classification. In 2013 IEEE Workshop on
Automatic Speech Recognition and Understanding, pages 422–427. IEEE, 2013.
[69] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, và Ilya
Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint
arXiv:2212.04356, 2022.
[70] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[71] Srijith Radhakrishnan, Chao-Han Huck Yang, Sumeer Ahmad Khan, Rohit Kumar, Narsis A
Kiani, David Gomez-Cabrero, và Jesper N Tegner. Whispering llama: A cross-modal gener-
ative error correction framework for speech recognition. arXiv preprint arXiv:2310.06434,
2023.
[72] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.
[73] Ohad Rubin, Jonathan Herzig, và Jonathan Berant. Learning to retrieve prompts for in-context
learning. arXiv preprint arXiv:2112.08633, 2021.
[74] Julian Salazar, Davis Liang, Toan Q Nguyen, và Katrin Kirchhoff. Masked language model
scoring. arXiv preprint arXiv:1910.14659, 2019.
[75] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A
176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,
2022.
[76] Changhao Shan, Chao Weng, Guangsen Wang, Dan Su, Min Luo, Dong Yu, và Lei Xie.
Component fusion: Learning replaceable language model component for end-to-end speech
recognition system. In ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 5361–5635. IEEE, 2019.
[77] Kai Shen, Yichong Leng, Xu Tan, Siliang Tang, Yuan Zhang, Wenjie Liu, và Edward Lin.
Mask the correct tokens: An embarrassingly simple approach for error correction. arXiv
preprint arXiv:2211.13252, 2022.

--- TRANG 15 ---
[78] Xian Shi, Qiangze Feng, và Lei Xie. The asru 2019 mandarin-english code-switching
speech recognition challenge: Open datasets, tracks, methods and results. arXiv preprint
arXiv:2007.05916, 2020.
[79] Joonbo Shin, Yoonhyung Lee, và Kyomin Jung. Effective sentence scoring method using
bert for speech recognition. In Asian Conference on Machine Learning, pages 1081–1093.
PMLR, 2019.
[80] Prashanth Gurunath Shivakumar, Mu Yang, và Panayiotis Georgiou. Spoken language intent
detection using confusion2vec. Proc. of Interspeech, 2019.
[81] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, và Adam Coates. Cold fusion: Training
seq2seq models together with language models. arXiv preprint arXiv:1708.06426, 2017.
[82] Piotr Szymanski, Piotr Zelasko, Mikolaj Morzy, Adrian Szymczak, Marzena Zyla-Hoppe,
Joanna Banaszczak, Lukasz Augustyniak, Jan Mizgajski, và Yishay Carmiel. Wer we are and
wer we think we are. arXiv preprint arXiv:2010.03432, 2020.
[83] Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara N Sainath, và
Karen Livescu. A comparison of techniques for language model integration in encoder-
decoder speech recognition. In 2018 IEEE spoken language technology workshop (SLT), pages
369–375. IEEE, 2018.
[84] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[85] Mehmet Ali Tuğtekin Turan, Emmanuel Vincent, và Denis Jouvet. Achieving multi-accent
asr via unsupervised acoustic model adaptation. In INTERSPEECH 2020, 2020.
[86] Takuma Udagawa, Masayuki Suzuki, Gakuto Kurata, Nobuyasu Itoh, và George Saon. Effect
and analysis of large-scale language model rescoring on competitive asr systems. arXiv
preprint arXiv:2204.00212, 2022.
[87] Ehsan Variani, Tongzhou Chen, James Apfel, Bhuvana Ramabhadran, Seungji Lee, và Pedro
Moreno. Neural oracle search on n-best hypotheses. In ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7824–7828. IEEE,
2020.
[88] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[89] Emmanuel Vincent, Shinji Watanabe, Jon Barker, và Ricard Marxer. The 4th
chime speech separation and recognition challenge. URL: http://spandh.dcs.shef.ac.
uk/chime_challenge/(last accessed on 1 August, 2018), 2016.
[90] Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza,
Mary Williamson, Juan Pino, và Emmanuel Dupoux. Voxpopuli: A large-scale multilingual
speech corpus for representation learning, semi-supervised learning and interpretation. arXiv
preprint arXiv:2101.00390, 2021.
[91] Weiran Wang, Ke Hu, và Tara N Sainath. Deliberation of streaming rnn-transducer by non-
autoregressive decoding. In ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 7452–7456. IEEE, 2022.
[92] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya
Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark
for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023.
[93] Yu Wang, Yuelin Wang, Kai Dang, Jie Liu, và Zhuo Liu. A comprehensive survey of
grammatical error correction. ACM Transactions on Intelligent Systems and Technology
(TIST), 12(5):1–51, 2021.

--- TRANG 16 ---
[94] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno,
Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. Espnet:
End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015, 2018.
[95] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large
language models. arXiv preprint arXiv:2206.07682, 2022.
[96] Johannes Wirth và Rene Peinl. Automatic speech recognition in german: A detailed error
analysis. In 2022 IEEE International Conference on Omni-layer Intelligent Systems (COINS),
pages 1–8. IEEE, 2022.
[97] Haibin Wu, Kai-Wei Chang, Yuan-Kuei Wu, và Hung-yi Lee. Speechgen: Unlocking the
generative power of speech language models with prompts. arXiv preprint arXiv:2306.02207,
2023.
[98] Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, và Tie-Yan Liu. De-
liberation networks: Sequence generation beyond one-pass decoding. Advances in neural
information processing systems, 30, 2017.
[99] Sang Michael Xie, Aditi Raghunathan, Percy Liang, và Tengyu Ma. An explanation of
in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.
[100] Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini Ghosh, Ivan Bulyko, và Andreas
Stolcke. Generative Speech Recognition Error Correction with Large Language Models and
Task-Activating Prompting. arXiv preprint arXiv:2309.15649, 2023.
[101] Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Rohit Prabhavalkar, Tara N Sainath, và
Trevor Strohman. From english to more languages: Parameter-efficient model reprogramming
for cross-lingual speech recognition. In Proc. of ICASSP, pages 1–5. IEEE, 2023.
[102] Chao-Han Huck Yang, Yun-Yun Tsai, và Pin-Yu Chen. Voice2series: Reprogramming
acoustic models for time series classification. In International Conference on Machine
Learning, pages 11808–11819. PMLR, 2021.
[103] Xi Ye và Greg Durrett. Explanation selection using unlabeled data for in-context learning.
arXiv preprint arXiv:2302.04813, 2023.
[104] Albert Zeyer, Kazuki Irie, Ralf Schlüter, và Hermann Ney. Improved training of end-to-end
attention models for speech recognition. arXiv preprint arXiv:1805.03294, 2018.
[105] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,
Peng Gao, và Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init
attention. arXiv preprint arXiv:2303.16199, 2023.
[106] Shiliang Zhang, Ming Lei, và Zhijie Yan. Automatic spelling correction with transformer for
ctc-based end-to-end speech recognition. arXiv preprint arXiv:1904.10045, 2019.
[107] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformer language models. arXiv preprint arXiv:2205.01068, 2022.
[108] Yuanhan Zhang, Kaiyang Zhou, và Ziwei Liu. What makes good examples for visual
in-context learning? arXiv preprint arXiv:2301.13670, 2023.
[109] Yun Zhao, Xuerui Yang, Jinchao Wang, Yongyu Gao, Chao Yan, và Yuanfu Zhou. Bart
based semantic correction for mandarin automatic speech recognition system. arXiv preprint
arXiv:2104.05507, 2021.

--- TRANG 17 ---
Danh sách kiểm tra
Danh sách kiểm tra theo sau tài liệu tham khảo. Vui lòng đọc kỹ hướng dẫn danh sách kiểm tra để biết thông tin về
cách trả lời các câu hỏi này. Đối với mỗi câu hỏi, thay đổi [TODO] mặc định thành [Có], [Không], hoặc
[N/A]. Bạn được khuyến khích mạnh mẽ bao gồm lời giải thích cho câu trả lời của mình, hoặc bằng cách tham khảo
phần thích hợp của bài báo hoặc cung cấp mô tả ngắn gọn. Ví dụ:
• Bạn có bao gồm giấy phép cho mã và bộ dữ liệu không? [Có]
Vui lòng không sửa đổi các câu hỏi và chỉ sử dụng các macro được cung cấp cho câu trả lời của bạn. Lưu ý rằng
phần Danh sách kiểm tra không tính vào giới hạn trang. Trong bài báo của bạn, vui lòng xóa khối hướng dẫn này
và chỉ giữ tiêu đề phần Danh sách kiểm tra ở trên cùng với các câu hỏi/câu trả lời bên dưới.

1. Đối với tất cả tác giả...
(a) Các tuyên bố chính được đưa ra trong tóm tắt và giới thiệu có phản ánh chính xác
đóng góp và phạm vi của bài báo không? [Có]
(b) Bạn có mô tả các hạn chế của công trình không? [Có] Xem Phần 5.3
(c) Bạn có thảo luận về bất kỳ tác động tiêu cực nào có thể có đối với xã hội của công trình không? [N/A]
(d) Bạn có đọc hướng dẫn đánh giá đạo đức và đảm bảo rằng bài báo tuân thủ
chúng không? [Có]

2. Nếu bạn bao gồm kết quả lý thuyết...
(a) Bạn có nêu tập đầy đủ các giả định của tất cả kết quả lý thuyết không? [N/A]
(b) Bạn có bao gồm chứng minh đầy đủ của tất cả kết quả lý thuyết không? [N/A]

3. Nếu bạn chạy thí nghiệm (ví dụ cho benchmark)...
(a) Bạn có bao gồm mã, dữ liệu, và hướng dẫn cần thiết để tái tạo các kết quả thí nghiệm chính
(trong tài liệu bổ sung hoặc như URL) không? [Có] Xem Phần 3.
(b) Bạn có chỉ định tất cả chi tiết huấn luyện (ví dụ, phân chia dữ liệu, siêu tham số, cách chúng
được chọn) không? [Có] Xem Phần 5.1.
(c) Bạn có báo cáo thanh lỗi (ví dụ, đối với hạt giống ngẫu nhiên sau khi chạy thí nghiệm
nhiều lần) không? [N/A]
(d) Bạn có bao gồm tổng lượng tính toán và loại tài nguyên được sử dụng (ví dụ, loại
GPU, cluster nội bộ, hoặc nhà cung cấp đám mây) không? [Có]

4. Nếu bạn sử dụng tài sản hiện có (ví dụ, mã, dữ liệu, mô hình) hoặc quản lý/phát hành tài sản mới...
(a) Nếu công trình của bạn sử dụng tài sản hiện có, bạn có trích dẫn người tạo không? [Có] Xem Phụ lục.
(b) Bạn có đề cập đến giấy phép của tài sản không? [Có] Xem Phụ lục.
(c) Bạn có bao gồm bất kỳ tài sản mới nào trong tài liệu bổ sung hoặc như URL không? [Có]
(d) Bạn có thảo luận về việc liệu và cách thức sự đồng ý được thu thập từ những người có dữ liệu
bạn đang sử dụng/quản lý không? [N/A]
(e) Bạn có thảo luận về việc liệu dữ liệu bạn đang sử dụng/quản lý có chứa thông tin nhận dạng cá nhân
hoặc nội dung xúc phạm không? [N/A]

5. Nếu bạn sử dụng crowdsourcing hoặc thực hiện nghiên cứu với đối tượng con người...
(a) Bạn có bao gồm văn bản đầy đủ của hướng dẫn được đưa cho người tham gia và ảnh chụp màn hình,
nếu có không? [N/A]
(b) Bạn có mô tả bất kỳ rủi ro tiềm ẩn nào đối với người tham gia, với liên kết đến
phê duyệt Hội đồng Đánh giá Thể chế (IRB), nếu có không? [N/A]
(c) Bạn có bao gồm mức lương theo giờ ước tính được trả cho người tham gia và tổng số tiền
chi cho bồi thường người tham gia không? [N/A]

--- TRANG 18 ---
Phụ lục

LLM có thấy phiên âm ASR trong quá trình huấn luyện trước không?
Chúng tôi đặt ra mối quan tâm về việc sửa lỗi LLM: liệu kết quả sửa lỗi đáng kể có do thực tế
rằng các phiên âm ASR tồn tại trong corpus được huấn luyện trước của LLM?

Để đánh giá mối quan tâm này đối với GPT-3.5, chúng tôi tạo thủ công một số giả thuyết để mô phỏng
câu nói trong nghiên cứu trường hợp trong Bảng 4, và chúng tôi hiển thị kết quả sửa lỗi trong Bảng 5.

Bảng 5: Giả thuyết tổng hợp cho việc sửa lỗi LLM
ID Câu nói
1 Patrue China và xinnepec sở hữu tất cả cơ sở hạ tầng đường ống dầu quốc gia.
Mục được sửa: "Patrue China" → "PetroChina", "xinnepec" → "Sinopec"
2 The Legend of Saida là trò chơi được mong đợi nhất năm nay từ nine ten door.
Mục được sửa: "Saida" → "Zelda", "nine ten door" → "Nintendo"
3 Tỷ lệ lỗi thế giới là matrix chính để đánh giá hiệu suất ASR.
Mục được sửa: "world" → "word", "matrix" → "metric"
4 Anh ấy đã lỡ chuyến bay mid nine từ Singapore đến rose island.
Mục được sửa: "mid nine" → "midnight", "rose" → "Rhodes"

Chúng tôi quan sát thấy rằng với các câu nói được chế tạo tùy ý trong các tình huống khác nhau,
LLM vẫn có thể đưa ra kết quả sửa lỗi hợp lý, đặc biệt trên các thuật ngữ cụ thể. Hơn nữa,
chúng tôi đính kèm quy trình lý luận LLM cho hai ví dụ, cho thấy việc sử dụng thông tin ngữ cảnh:

ID 1: Vui lòng lưu ý rằng phiên âm giả định "Patrue China" là một lỗi, và "PetroChina" được
dự định thay thế. Ngoài ra, "xinnepec" đã được diễn giải là "Sinopec," là một công ty dầu khí
lớn khác của Trung Quốc.

ID 2: Vui lòng lưu ý rằng có thể đã có một số nhầm lẫn với cụm từ "nine ten door" vì nó
không khớp với bất kỳ công ty nào được biết đến hoặc thuật ngữ được công nhận rộng rãi.
Kết quả là, tôi đã suy luận rằng nó có thể là một diễn giải sai hoặc lỗi đánh máy.
Phiên âm giả định "Zelda" là sự thay thế có thể cho "Saida" dựa trên sự tương đồng
với series trò chơi nổi tiếng "The Legend of Zelda" do Nintendo phát triển.

Trong H2T-ft, mô hình T5 được huấn luyện trước trên bộ dữ liệu C4 không bao gồm WSJ,
nhưng nó vẫn đạt được giảm WER đáng kể trên WSJ. Trong H2T-LoRA, chúng tôi không tìm thấy
bất kỳ phiên âm ASR nào tồn tại trong dữ liệu huấn luyện trước của LlaMa theo các bài báo
hoặc thông tin công khai của họ.

Nói chung, chúng tôi lập luận rằng kết quả cải thiện của việc sửa lỗi LLM chủ yếu xuất phát
từ cơ sở kiến thức rộng lớn và khả năng sử dụng thông tin ngữ cảnh như một phần của
quá trình thí nghiệm khoa học chính thức. Tuy nhiên, các phương pháp mà qua đó kiến thức
gián tiếp được nắm bắt và truyền bá trên một khu vực rộng lớn vẫn là câu hỏi mở cho
nghiên cứu trong tương lai. Chúng tôi hy vọng rằng bằng cách khởi xướng nghiên cứu này,
kết quả sẽ khuyến khích thêm các công trình tiếp theo và liên tục về việc căn cứ kiến thức
trong tương lai.

Tài nguyên cho việc Tạo Hypotheses Paradise (HP)
Các toolkit và mô hình đã được huấn luyện trước liên quan được sử dụng để tạo bộ dữ liệu HP
được liệt kê như sau:
• Đối với mô hình ASR, chúng tôi sử dụng WavLM đã được huấn luyện trước từ HuggingFace4
và bộ công cụ ESPNet5.
• Khi sử dụng WavLM để giải mã ASR, chúng tôi cũng kết hợp tùy chọn tính điểm lại
LM bên ngoài, được huấn luyện sử dụng dữ liệu văn bản bổ sung từ web6.
• Ngoài WavLM, chúng tôi cũng chọn Whisper-LargeV27 để giải mã ASR.

4https://huggingface.co/espnet/simpleoier_librispeech_asr_train_asr_conformer7_
wavlm_large_raw_en_bpe5000_sp
5https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1
6http://www.openslr.org/resources/11/librispeech-lm-norm.txt.gz
7https://github.com/openai/whisper

--- TRANG 19 ---
Task-Activating Prompting (TAP) cho Đánh giá Sửa lỗi
Chúng tôi minh họa prompt đã cho cho task-activating prompting [100] (TAP), cũng như phản hồi
từ LLM đã được huấn luyện trước đông lạnh cho mỗi lượt, như được đề xuất bởi Yang et al. [100]
trong một công trình gần đây.

Q: Bạn có quen thuộc với nhận diện giọng nói không?
R: Có, tôi quen thuộc với nhận diện giọng nói. Nhận diện giọng nói, còn được biết đến là
nhận diện giọng nói tự động (ASR) hoặc speech-to-text, là quá trình chuyển đổi ngôn ngữ nói
thành văn bản. Công nghệ này bao gồm việc sử dụng thuật toán và mô hình học máy để phân tích
và phiên âm các đặc trưng âm thanh của từ và cụm từ nói. Nhận diện giọng nói có nhiều ứng dụng,
bao gồm trợ lý điều khiển bằng giọng nói, hệ thống điện thoại tự động, và dịch vụ phiên âm.

Q: Bạn có quen thuộc với tính điểm lại mô hình ngôn ngữ trong ASR không?
R: Có, tôi quen thuộc với tính điểm lại mô hình ngôn ngữ cho nhận diện giọng nói. Tính điểm lại
mô hình ngôn ngữ là một kỹ thuật được sử dụng để cải thiện độ chính xác của hệ thống nhận diện
giọng nói. Nó bao gồm việc sử dụng một mô hình ngôn ngữ riêng biệt để đánh giá khả năng
của một danh sách giả thuyết đã cho. Mô hình riêng biệt này thường phức tạp và mạnh mẽ hơn
mô hình ngôn ngữ ban đầu được sử dụng cho phiên âm, và nó được sử dụng để tính điểm lại
phiên âm dựa trên xác suất của các từ xuất hiện trong ngữ cảnh đã cho. Quá trình tính điểm lại
bao gồm việc lấy đầu ra của mô hình ngôn ngữ ban đầu, thường dựa trên các phương pháp thống kê
như Hidden Markov Models, và sau đó áp dụng một mô hình ngôn ngữ tiên tiến hơn, như mô hình
ngôn ngữ dựa trên mạng nơ-ron, để tạo ra phiên âm chính xác hơn. Điều này được thực hiện
bằng cách xếp hạng lại các phiên âm có thể dựa trên xác suất được gán bởi mô hình ngôn ngữ
tiên tiến hơn. Tính điểm lại mô hình ngôn ngữ đã được chứng minh là cải thiện đáng kể độ chính xác
của hệ thống nhận diện giọng nói, đặc biệt trong môi trường ồn hoặc thách thức nơi mô hình
ngôn ngữ ban đầu có thể không hoạt động tốt.

Q: Bạn có thể đưa ra một ví dụ có thể về tính điểm lại mô hình ngôn ngữ với giả thuyết 5-best không?
R: Chắc chắn, đây là một ví dụ về tính điểm lại mô hình ngôn ngữ cho ASR với giả thuyết 5-best:
1. Tôi muốn đi đến cửa hàng.
2. Tôi muốn đi đến cơn bão.
3. Tôi muốn đi đến bếp.
4. Tôi muốn đi đến ngôi sao.
5. Tôi muốn đi đến kho.
Sau khi tính điểm lại, tôi nghĩ ground-truth của giọng nói này nên là: Tôi muốn đi đến cửa hàng.

Q: Tốt lắm, tôi sẽ cho bạn một ví dụ thực tế làm minh họa từ [miền đích]. Giả thuyết 5-best
là: [danh sách giả thuyết từ tập huấn luyện], và tôi kỳ vọng đầu ra của bạn là: [phiên âm tương ứng].
Theo ví dụ này, bạn có thể báo cáo phiên âm thực sự từ giả thuyết 5-best sau không:?
[danh sách giả thuyết để suy luận]

Cần lưu ý rằng các phản hồi từ GPT-3.5 khác nhau tùy theo hạt giống ngẫu nhiên. Trong các thí nghiệm
của chúng tôi, chúng tôi sử dụng API key từ OpenAI, và cố định template trên trước lượt sửa lỗi cuối cùng.

Chúng tôi quan sát thấy rằng nhiều công trình gần đây [103] của ICL tập trung vào chiến lược lựa chọn
cho minh họa trong miền, có thể có tác động rõ ràng đến kết quả suy luận. Chúng tôi để việc lựa chọn
minh họa như công việc tương lai, và trong học few-shot của chúng tôi, chúng tôi chọn thủ công
những câu nói có độ dài dài theo [64].

Cấu hình Huấn luyện Hypotheses-to-Transcription (H2T)
H2T-ft. Chúng tôi sử dụng mô hình T5-v1.1-large đã được huấn luyện trước (0.75B) được tải xuống
từ HuggingFace8. So với mô hình T5 gốc, GELU phục vụ như hàm kích hoạt trong lớp feed-forward
để thay thế ReLU. Hơn nữa, T5 Version 1.1 chỉ được huấn luyện trước trên C4 loại trừ bất kỳ
huấn luyện có giám sát nào. Do đó, mô hình này phải được fine-tuned trước khi áp dụng
cho một tác vụ hạ lưu.

8https://huggingface.co/google/t5-v1_1-large

--- TRANG 20 ---
Chúng tôi fine-tune 20 epoch trên mỗi miền của bộ dữ liệu HP với batch size là 16. Để chọn
mô hình tốt nhất, chúng tôi đầu tiên chia một tập validation với 5% lượng dữ liệu của tập huấn luyện.
Tỷ lệ học thay đổi từ 1×e−4∼1×e−3 theo lượng dữ liệu của mỗi miền, và AdamW được sử dụng
để tối ưu hóa. α₁ đến α₁ được đặt là 0.1, 0.05, 0.05, 0.05 tương ứng, vì câu nói thứ 2
thường có thông tin hơn so với những câu khác như được hiển thị trong Hình 1. Trong thực tế,
chúng tôi quan sát hiện tượng over-fitting trong quá trình huấn luyện. WER trên tập huấn luyện
có thể thấp hơn 1%, tuy nhiên, hiệu suất trên bộ dữ liệu CORAAL thậm chí còn tệ hơn so với
baseline. Nói cách khác, H2T-ft vẫn còn chỗ cho cải thiện bằng cách thêm một số kỹ thuật
để tránh over-fitting.

H2T-LoRA. Chúng tôi chọn LlaMa-13B làm mô hình đã được huấn luyện trước đông lạnh trong
phương pháp của chúng tôi, được tải xuống từ HuggingFace9. Tỷ lệ học được đặt là 1e−4, và
batch size là 128. Đối với adapter low-rank, chúng tôi triển khai bằng peft10, trong đó cấu hình
của rank r được đặt là 8. Tương tự, chúng tôi cũng sử dụng T5-v1.1-large làm mô hình đã được
huấn luyện trước với adapter low-rank cho thí nghiệm, trong đó tỷ lệ học được đặt là 3e−4
và lora_r được đặt là 16.

Chúng tôi huấn luyện 10 epoch sử dụng optimizer AdamW, và prompt cho LLM được thiết kế như sau:
"Bên dưới là giả thuyết tốt nhất được phiên âm từ một hệ thống nhận diện giọng nói tự động.
Viết phản hồi để dự đoán phiên âm thực sự sử dụng các token từ giả thuyết khác.### giả thuyết tốt nhất:
{câu nói thứ 1}### giả thuyết khác:{câu nói thứ 2∼5} ###Phản hồi:"

Template prompt không phải là duy nhất, và nó để lại tác động nhẹ đến kết quả WER cuối cùng.
Ngoài ra, chúng tôi tính toán WER sử dụng toolkit Sclite11, giữ nhất quán với script đánh giá
của ESPNet12.

LMrank là một mô hình ngôn ngữ dựa trên Transformer được triển khai sử dụng toolkit ESPNet13,
trong đó phiên âm huấn luyện từ mỗi miền HP được sử dụng cho huấn luyện LM điển hình.
Lớp Transformer của mỗi mô hình thay đổi từ 8 đến 16 theo lượng dữ liệu. Epoch huấn luyện
được đặt là 20, và Adam được sử dụng làm optimizer. Tỷ lệ học ban đầu được đặt là 0.002
với chiến lược warm up. Trong quá trình giải mã, perplexity của mỗi giả thuyết được tính toán
để xếp hạng lại danh sách N-best, và câu nói với perplexity thấp nhất được chọn làm đầu ra cuối cùng.

Kết quả LibriSpeech và Nghiên cứu Trường hợp Thất bại
Chúng tôi liệt kê hai trường hợp thất bại đại diện từ LibriSpeech-test-other trong Bảng 7.
Đối với trường hợp đầu tiên, "ward" được sửa thành "warde" vì có "his" phía sau nó.
Ngoài ra, chúng tôi quan sát thấy rằng "warde" cũng xuất hiện trong giả thuyết thứ 2,
vì vậy LLM áp dụng nó theo thông tin ngữ cảnh. Đối với trường hợp thứ hai, LLM trực tiếp
áp dụng câu nói thứ 2 trong danh sách N-best, vì "think" không thường xuất hiện ở đầu câu
từ góc độ ngữ pháp. Do đó, như [100] và được giải thích trong công việc tương lai của 5.5,
chúng tôi lập luận rằng việc sửa lỗi LLM cũng nên xem xét thông tin âm thanh được cung cấp
bởi hệ thống ASR, giúp tránh các trường hợp "sửa quá mức" và giữ sự trung thực với ngôn ngữ nói.

Bảng 6: Kết quả WER (%) trên bộ dữ liệu LibriSpeech. "onb" và "ocp" tương ứng biểu thị
n-best oracle và compositional oracle được định nghĩa trong 5.2.

Tập Test Baseline Sửa lỗi với Oracle
LMrank H2T-ft H2T-LoRA onb ocp
LS-clean 1.8 1.8 1.8−0.0% 1.7−5.6% 1.0 0.6
LS-other 3.7 3.7 3.9+5.4% 3.8+2.7% 2.7 1.6

Kết quả trên Bộ dữ liệu Mandarin và Code-switching
Chúng tôi bao gồm AISHELL-1 [6] như một bộ dữ liệu tiếng Mandarin vào benchmark HP,
gồm một tập huấn luyện với 120098 câu nói và một tập test với 7176 câu nói. Chúng tôi chọn
ngẫu nhiên 20k ví dụ (16.7%) từ tập huấn luyện để đánh giá hiệu quả của H2T-LoRA được đề xuất.
Đối với mô hình nền tảng, chúng tôi

9https://huggingface.co/decapoda-research/llama-13b-hf
10https://github.com/huggingface/peft
11https://github.com/usnistgov/SCTK/blob/master/doc/sclite.htm
12https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/asr1/asr.sh
13https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1

--- TRANG 21 ---
Bảng 7: Trường hợp thất bại được sửa bởi H2T-LoRA. Các câu nói được rút từ LibriSpeech-test-other.

Loại Câu nói WER
Giả thuyết 1 Tuy nhiên có cờ bạc một lần nữa đêm thứ hai giữa 0
bởi AM ward và một số người khác trong nghề của anh ấy.
Sửa lỗi Tuy nhiên có cờ bạc một lần nữa đêm thứ hai giữa 6.25
bởi H2T warde và một số người khác trong nghề của anh ấy.
Phiên âm Tuy nhiên có cờ bạc một lần nữa đêm thứ hai giữa -
Ground-truth ward và một số người khác trong nghề của anh ấy.

Giả thuyết 1 Nghĩ rằng anh ấy thực sự cần nó anh ấy theo đuổi 0
bởi AM
Sửa lỗi Anh ấy thực sự cần nó anh ấy theo đuổi 14.3
bởi H2T
Phiên âm Nghĩ rằng anh ấy thực sự cần nó anh ấy theo đuổi -
Ground-truth

sử dụng Chinese LlaMa2-7b từ Huggingface14, và giữ các cài đặt khác nhất quán với H2T-LoRA
trong bài báo này. Đáng chú ý, ASR trên bộ dữ liệu tiếng Mandarin thường được đánh giá
bằng tỷ lệ lỗi ký tự (CER), vì ký tự bằng từ trong tiếng Trung. Chúng tôi cũng đánh giá
H2T-LoRA trên ASRU [78] là một bộ dữ liệu code-switching Mandarin-English. Mỗi câu nói
trong ASRU chứa 8.6 ký tự Trung Quốc và 1.6 từ tiếng Anh trung bình, và các phiên âm
bao gồm nhiều lĩnh vực phổ biến, bao gồm giải trí, du lịch, cuộc sống hàng ngày, và tương tác
xã hội. Tương tự với AISHELL-1, Chinese LlaMa2-7b được chọn làm mô hình nền tảng,
và 15k cặp Hypotheses-Transcription được sử dụng ngẫu nhiên trong huấn luyện H2T.
Ngoài ra, thước đo đánh giá cho ASR code-switching là tỷ lệ lỗi hỗn hợp.

Từ kết quả được báo cáo trong Bảng 8, chúng tôi quan sát thấy rằng H2T-LoRA chứng minh
khả năng tổng quát hóa trên cả bộ dữ liệu tiếng Mandarin và code-switching, tương ứng
đạt được giảm 20.6% CER và 24.5% MER. Hơn nữa, H2T-LoRA thể hiện hiệu quả dữ liệu
đáng kể vì các cặp huấn luyện tương đương với ít hơn 20 giờ dữ liệu giọng nói.

Bảng 8: Kết quả tiếng Mandarin và code-switching trên bộ dữ liệu AISHELL-1 (CER) và ASRU (MER),
trong đó 20k và 15k cặp Hypotheses-Transcription tương ứng được sử dụng trong huấn luyện H2T.

Bộ dữ liệu Ngôn ngữ Baseline H2T-LoRA onb ocp
AISHELL-1 Man 6.3 5.0−20.6% 4.1 3.1
ASRU Man & Eng 11.0 8.3−24.5% 8.6 6.5

Kết quả trên tác vụ Phát hiện Ý định Ngôn ngữ Nói (SID)
Chúng tôi đầu tiên huấn luyện một mô hình phát hiện ý định sử dụng phiên âm của tập huấn luyện ATIS,
vì nhãn ý định có sẵn cho mỗi ví dụ. Sau đó, trong quá trình test, chúng tôi tương ứng đưa vào
câu nói thứ 1∼5 trong danh sách giả thuyết Whisper, câu nói sau sửa lỗi, và phiên âm ground-truth
làm văn bản đầu vào cho phát hiện ý định. Kết quả độ chính xác được báo cáo trong Bảng 9.

Bảng 9: Kết quả độ chính xác (%) của phát hiện ý định với đầu vào khác nhau trên tập test ATIS.

Đầu vào văn bản Câu nói thứ n trong danh sách Giả thuyết, n= Sau Sửa lỗi Oracle
1 2 3 4 5
Độ chính xác (%) 94.9 95.5 94.2 94.3 94.2 97.4 +2.5% 97.9

14https://huggingface.co/ziqingyang/chinese-llama-2-7b

--- TRANG 22 ---
Chúng tôi quan sát thấy rằng: (1) Khi chúng tôi sử dụng văn bản đã được sửa lỗi để phát hiện ý định,
độ chính xác là 97.4% đạt được cải thiện tuyệt đối 2.5% so với câu nói thứ 1 trong danh sách giả thuyết.
(2) Câu nói thứ 2 phù hợp hơn cho phát hiện ý định so với câu nói thứ 1 về mặt độ chính xác.
Hiện tượng này xác nhận trường hợp (ii) từ góc độ khác ngoài WER, trong đó các câu nói bị loại bỏ
trong giả thuyết N-best có thể tốt hơn so với câu nói được chọn.

Kết quả Sơ bộ về Sửa lỗi dựa trên Prompting Zero-shot
Để kiểm tra khả năng zero-shot của LLM, chúng tôi đề xuất một framework yêu cầu LLM mục tiêu
thực hiện (i) sửa lỗi dựa trên xếp hạng hoặc (ii) sửa lỗi sinh tạo câu đơn. Chúng tôi theo
phương pháp prompting tự kích hoạt được đề cập trong phần phụ lục trước đó để tái sử dụng
mô hình ngôn ngữ dưới dạng sửa lỗi zero-shot, mà không cung cấp hướng dẫn. Sử dụng cùng
tập test giải mã từ WavLM và Whisper, một mô hình ngôn ngữ 5-gram (hệ số 0.1) kết hợp
với điểm mô hình âm thanh của nó cho thấy cải thiện WER tương đối 2.95%. Kết quả này
hơi tệ hơn so với baseline LM rank. Các hạn chế hiện tại về kết quả liên quan đến khả năng
zero-shot của LM có thể do quy mô mô hình. Khả năng zero-shot hoặc emergent của các
mô hình này đã được báo cáo [100] là đáng kể hơn khi quy mô tham số của LLM vượt quá 100B.

Trực quan hóa Bộ dữ liệu Hypotheses Paradise (HP)
Chúng tôi đã mở nguồn một ví dụ Colab15 cho trực quan hóa và phân tích bộ dữ liệu HP.
Đầu tiên, giống như Hình 1, chúng tôi trực quan hóa và phân tích thông tin trong giả thuyết N-best
từ cả cấp độ câu nói và token. Hình 4 minh họa thêm trực quan hóa trên tập test CHiME-4,
nơi chúng tôi có thể quan sát thông tin có giá trị trong giả thuyết N-best.

Hình 4: Xác suất của trường hợp (i) và (ii) trên tập test CHiME-4, tương tự như phần bên phải của Hình 1.

Hơn nữa, chúng tôi cũng trực quan hóa và so sánh tần suất từ trong giả thuyết N-best và
phiên âm ground-truth trong Hình 5, nơi chúng tôi có thể quan sát một số nhưng hạn chế
khoảng cách giữa chúng.

Phân bố Giả thuyết N-best
Hình 6 trực quan hóa phân bố giả thuyết N-best được tạo bởi các mô hình Whisper có kích thước
khác nhau, tức là từ 'tiny' đến 'large'. Chúng tôi có thể quan sát sự đa dạng rất hạn chế trong
giả thuyết N-best được tạo bởi các mô hình Whisper. Xét đến tính đơn điệu cao như vậy,
chúng tôi chỉ thu thập top-5 giả thuyết để tạo thành bộ dữ liệu HP của chúng tôi.

Hạn chế
Mặc dù benchmark HP được đề xuất cung cấp một mô hình mới về sửa lỗi sinh tạo cho ASR,
chúng tôi phân tích và thảo luận các hạn chế của công trình này từ góc độ sau:

15https://colab.research.google.com/drive/1traA2scdnmAKFq6yIEZhHwrhCBVxB2ig

--- TRANG 23 ---
the và to of in is a hundred point one
Từ 0.000.010.020.030.040.050.06 Tần suất Tần suất từ của giả thuyết N-best
the to of in a is and point hundred one
Từ 0.000.010.020.030.040.050.06 Tần suất Tần suất từ của phiên âm ground-truth

Hình 5: Tần suất 10 từ hàng đầu trong giả thuyết N-best và phiên âm ground-truth của tập test CHiME-4.

whose feet are as the feet of hearts and
underneath the ev erlasting arms (WER: 7.7)
who is feet are as the feet of hearts and
underneath the ev erlasting arms (WER: 23.1)
GT whose feet are as the feet of harts and
underneath the ev erlasting arms

Hình 6: Phân bố giả thuyết N-best với các mô hình Whisper khác nhau. Mỗi màu biểu thị một
giả thuyết độc nhất, 'GT' biểu thị phiên âm ground-truth. Mẫu được chọn từ tập test-clean LibriSpeech,
tức là '1089/134691/1089-134691-0005.flac'.

• Thước đo đánh giá. Như một benchmark sửa lỗi ASR, HP sử dụng WER làm thước đo
chính để đánh giá hiệu suất hệ thống. Tuy nhiên, công trình trước đây [2] đã chỉ ra rằng
WER có thể quá thô để mô tả hiệu suất của các mô hình ASR. Hơn nữa, [82] nâng cao
nhận thức cộng đồng về các vấn đề do thiên lệch lạc quan đối với độ chính xác ASR gây ra.
Trong tương lai, chúng tôi nhằm cung cấp thêm chú thích cho ngôn ngữ nói, ví dụ, khoảng
thực thể và cấu trúc phụ thuộc. Theo đó, một framework đánh giá toàn diện có thể được
thiết lập để đánh giá chất lượng và khả năng diễn giải đầu ra từ hệ thống ASR được nâng cao
bởi LLM.

• Độ bền vững trong thực tế. Benchmark HP bao gồm các miền chính nơi tác vụ ASR thường
được triển khai. Tuy nhiên, như được hiển thị trong [57], không có tập validation hoặc test
đơn lẻ nào từ các bộ dữ liệu công cộng đủ để đo lường chuyển giao đến dữ liệu âm thanh
thực tế. Vì tất cả các tập test của benchmark HP được rút từ corpus ASR hiện có, bất chấp
việc nâng cao hiệu suất WER, chúng tôi không thể xác định mức độ nó có thể giảm thiểu
khoảng cách giữa các mô hình ASR được huấn luyện tốt và các tình huống ứng dụng thực tế.
Hơn nữa, xét đến sự khác biệt giữa ngôn ngữ nói và ngôn ngữ viết, cần thêm nỗ lực từ
cả cộng đồng nghiên cứu giọng nói và NLP để xây dựng hệ thống ASR bền vững giống con người
vượt ra ngoài một phương thức đơn lẻ [18].

Tác động Rộng rãi
Với những tiến bộ gần đây trong việc sử dụng các mô hình ngôn ngữ nơ-ron quy mô lớn để giải quyết
các vấn đề từng được cho là thách thức đối với máy móc học và hiểu, chúng tôi tin rằng đây là thời điểm
thích hợp để chuyển đến cột mốc tiếp theo: cung cấp giả thuyết n-best có thể truy cập công khai
như tài nguyên phiên âm từ giải mã LLM. Động lực này truyền cảm hứng cho công trình này,
cung cấp một bộ sưu tập hypotheses paradise, lấy cảm hứng từ học trong ngữ cảnh.

• Ai có thể được hưởng lợi từ nghiên cứu này: Các nhà nghiên cứu làm việc về công nghệ giọng nói
và sửa lỗi dựa trên mô hình ngôn ngữ; cũng như người dùng sử dụng các kỹ thuật liên quan
cho công nghệ học máy có trách nhiệm và có thể tái tạo.

• Ai có thể bị bất lợi từ nghiên cứu này: Khi công trình của chúng tôi tiết lộ rằng giả thuyết
mã nguồn mở có thể được sử dụng để tạo ra nhận diện độc hại, chúng tôi hiểu trách nhiệm
của việc giải thích kết quả đúng cách cho công chúng và cung cấp đánh giá có thể tái tạo.
Chúng tôi đã thảo luận điều khoản sử dụng để tái sử dụng các giả thuyết này với các chuyên gia
pháp lý và quy định, giải quyết các rủi ro và mối quan tâm tiềm ẩn.

• Liệu tác vụ/phương pháp có tận dụng thiên lệch trong dữ liệu không: Để giảm thiểu thiên lệch
có thể có trong dữ liệu và mô hình, chúng tôi đã nỗ lực thiết kế thước đo có thể tái tạo
và đánh giá nhiều nguồn dữ liệu và cấu hình huấn luyện có thể tái tạo. Chúng tôi cũng đã
thực hiện các nghiên cứu người dùng để làm nổi bật thiên lệch tiềm ẩn trong "điều khoản sử dụng"
được cung cấp trong repo Github của chúng tôi.

Kế hoạch Bảo trì
• Ai sẽ hỗ trợ/host/duy trì bộ dữ liệu? Hypotheses Paradise đã được duy trì tích cực bởi
các tác giả của bài báo này. Chúng tôi vẫn đang tích cực cập nhật bộ dữ liệu tập trung
vào tình huống ASR cụ thể, đó là ASR bền vững với tiếng ồn và ASR đa ngôn ngữ.
Trong INTERSPEECH 2023, chúng tôi sẽ có một hướng dẫn để giới thiệu Hypotheses
Paradise-V2 liên quan với một số kết quả thí nghiệm thú vị. Hơn nữa, chúng tôi cũng
mở liên kết để thu thập thêm cặp hypothesis-transcription từ công chúng.

• Làm thế nào có thể liên lạc với chủ sở hữu/quản lý/người quản lý của bộ dữ liệu?
Để liên lạc với các nhà phát triển chính, chúng tôi khuyến khích người dùng sử dụng
email của chúng tôi: {chen1436,yuchen005}@e.ntu.edu.sg, huckiyang@gatech.edu.

• Có errata không? Người dùng có thể sử dụng GitHub để báo cáo vấn đề/lỗi, và chúng tôi
sẽ tích cực cải thiện mã tương ứng. Chúng tôi cũng có thẻ Mô hình HuggingFace dưới
một tổ chức phi lợi nhuận tại https://huggingface.co/datasets/PeacefulData/HP-v0.

• Bộ dữ liệu có được cập nhật không? Có, chúng tôi đang tích cực cập nhật mã và nguồn dữ liệu
Hypotheses Paradise. Người dùng có thể nhận thông tin và phiên bản được cập nhật mới
thông qua kho GitHub của chúng tôi.

• Nếu bộ dữ liệu liên quan đến con người, có giới hạn áp dụng nào về việc lưu giữ dữ liệu
liên quan đến các trường hợp không? Không đối với Bộ dữ liệu.

• Các phiên bản cũ của bộ dữ liệu có tiếp tục được hỗ trợ/host/duy trì không? Có, chúng tôi
sẽ giữ phiên bản cũ được tạo bởi Whisper. Tất cả các phiên bản có thể được tìm thấy
trên kho GitHub của chúng tôi.

• Nếu người khác muốn mở rộng/tăng cường/xây dựng trên/đóng góp cho bộ dữ liệu, có cơ chế
nào để họ làm điều đó không? Chúng tôi duy trì Hypotheses Paradise trên GitHub và
chúng tôi khuyến khích tất cả người dùng chia sẻ ý tưởng của họ để mở rộng Hypotheses Paradise
đến nhiều trường hợp nhận diện giọng nói hơn. Người dùng có thể sử dụng GitHub để báo cáo
vấn đề/lỗi, và gửi email cho chúng tôi để thảo luận giải pháp.
