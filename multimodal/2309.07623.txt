# 2309.07623.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.07623.pdf
# File size: 10159249 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SwitchGPT: Adapting Large Language Models for Non-Text Outputs
Xinyu Wang1, Bohan Zhuang2, Qi Wu1*
1The University of Adelaide, Australia
2Monash Univeristy, Australia
Abstract
Large Language Models (LLMs), primarily trained on text-
based datasets, exhibit exceptional proficiencies in under-
standing and executing complex linguistic instructions via
text outputs. However, they falter when requests to gener-
ate non-text ones. Concurrently, modality conversion mod-
els, such as text-to-image, despite generating high-quality im-
ages, suffer from a lack of extensive textual pretraining. As
a result, these models are only capable of accommodating
specific image descriptions rather than comprehending more
complex instructions. To bridge this gap, we propose a novel
approach, SwitchGPT, from a modality conversion perspec-
tive that evolves a text-based LLM into a multi-modal one.
We specifically employ a minimal dataset to instruct LLMs
to recognize the intended output modality as directed by the
instructions. Consequently, the adapted LLM can effectively
summon various off-the-shelf modality conversion models
from the model zoos to generate non-text responses. This cir-
cumvents the necessity for complicated pretraining that typi-
cally requires immense quantities of paired multi-modal data,
while simultaneously inheriting the extensive knowledge of
LLMs and the ability of high-quality generative models. To
evaluate and compare the adapted multi-modal LLM with its
traditional counterparts, we have constructed a multi-modal
instruction benchmark that solicits diverse modality outputs.
The experiment results reveal that, with minimal training,
LLMs can be conveniently adapted to comprehend requests
for non-text responses, thus achieving higher flexibility in
multi-modal scenarios. Code and data will be made available
at https://github.com/xinke-wang/SwitchGPT.
Introduction
The emergence of Large Language Models (LLMs) (Brown
et al. 2020; Zhang et al. 2022; Touvron et al. 2023a; Chowd-
hery et al. 2022) that are capable of understanding and ex-
ecuting complex linguistic tasks has been a remarkable ad-
vancement in recent years. Their prowess in comprehending,
processing, and generating text-based responses has paved
the way for groundbreaking applications. This includes
fields such as natural language understanding (Wang et al.
2019), automated question-answering systems (Lin, Hilton,
and Evans 2022), and conversational AI assistants (OpenAI
2023). Predominantly, these models are trained on extensive
crowdsourced text-based datasets. Such datasets encompass
*Correspondence: Prof. Qi Wu (qi.wu01@adelaide.edu.au)
The famous tower where Galileo 
Galilei conducted his free fall experiment is the Leaning Tower of 
Pisa, located in Pisa, Italy. The 
experiment involved dropping 
objects of different masses from the 
tower to demonstrate that they would all hit the ground at the same 
time, disproving the prevailing 
belief at the time that heavier 
objects fall faster than lighter ones. 
The tower's unique lean has made it 
an iconic landmark and a popular 
destination for tourists from around 
the world.
Show me the famous tower where Galileo conducted the free fall experiment.
Stable -Diffusion Chat -GPT3.5 SwitchGP T
ChatGPT
Human Experts
Modality Datasets
Seed Instructions
Text→Image
Text→Speech
…
Instruction DatasetGenerated Instruction: Can you generate 
an image that represents the concept of 
“time travel”, using everyday objects such 
as a bicycle and a clock?
Responses: [Image] A black metal bicycle 
with a clock inside the front wheel.
Caption: A black metal bicycle with a 
clock inside the front wheel.Generation of Aligned Modality -aware Instructions
 Tuning/Inference Pipeline
Instruction Dataset
AdaptorModality Conversion
Text→Image 
Model
Text→Speech 
Model
…
Example Generation Process Instruction Tuning Inference
Inputs
Structured Response
Text
 Speech
 Image
LossSelect
Conversion Route
OutputLLM
I plan to travel to New York 
next week. I heard there’s a 
famous sculpture that was 
a gift from France. What is 
it called?
The famous sculpture you 
are referring to is the 
Statue of Liberty.
Wow, that’s great! Could 
you show me a photo of the Statue of Liberty?
Beautiful! Now, please use 
your voice to introduce me 
the history of the Statue of Liberty.
The Statue of Liberty was designed by French sculptor Fredric August Barthodi . It stands on liberty island 
in New York Harbor and has become an iconic representation of American values.Audio Content
Ours
Prompts: A surreal and 
fantastical landscape 
with strange creatures 
and objects.
Iamreading Alice inWonderland,
and Iamvery curious about the
‘wonderland’ inside the book .
Could you show mewhat does it
look like?
…Instruction Hugging -GPT
Prompts :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Input :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Failed to parse.
LLM -Grounded Diffusion
Generated Layout
BookBackgroundObjects to include
Book [140, 250, 211, 311]
Stable -Diffusion
Of course! "Alice in 
Wonderland" by Lewis Carroll is a whimsical and fantastical story that takes place in a world filled with surreal and imaginative elements. 
Wonderland is a place where 
logic and reality are often turned upside down, and strange occurrences are the norm. ChatGPT
Plan
Text→Image
Text→Audio
???
Expected Output?
Please use an image to answer this riddle (Tips: it is a kind of animal): The height of my legs, usually around 6 feet, is ta ller than most humans. What am I?
An Ostrich
[85, 32, 342, 480]The height of 
my legs, 
usually around 
6 feet, is taller 
than most 
humans. What 
am I.A photo of 
giraffe.
Show me an iconic landmark in Xian, China.
An iconic 
landmark in Xian
[12, 45, 488, 421]Show me an 
iconic 
landmark in 
Xian, ChinaThe Terracotta 
Army in Xian, 
China.
Figure 1: Given an instruction expecting a non-text response,
text-based LLMs like ChatGPT (OpenAI 2023) are con-
strained to providing text responses, while popular text-to-
image models such as Stable Diffusion (Rombach et al.
2022) generate imagery based on direct description. In con-
trast, our proposed SwitchGPT comprehensively interprets
the underlying intent of the instruction, accurately produc-
ing a more appropriate response.
a myriad of topics and languages, capturing the vast ex-
panse of human knowledge (Raffel et al. 2020; Radford et al.
2019). Nonetheless, due to their text-centric training, tradi-
tional LLMs primarily operate with textual inputs and out-
puts, resulting in unsatisfactory responses when tasked with
generating non-textual outputs (see Figure 1). Despite not
being directly exposed to non-textual data such as images
and speech during their training, recent research (Li et al.
2023a; Wang et al. 2023; Li et al. 2023c; Su et al. 2023;
Zhu et al. 2023) has indicated that LLMs possess a profound
potential to understand such non-textual data. This revela-
tion opens the door to evolving pure text-based LLMs into a
multi-modal paradigm. For example, Mini-GPT4 (Zhu et al.
2023) trains a linear layer that connects BLIP-2 (Li et al.
2023a) with Vicuna (Chiang et al. 2023), demonstrating the
possibility that LLMs can understand image inputs. How-
ever, studies on enabling LLMs to produce non-textual out-
puts remain relatively limited, restricting the LLM’s interac-
tive capabilities in multimodal scenarios.
Unlike traditional LLMs, which are primarily designed
for unimodal interaction, specifically text-to-text communi-
cation, modality conversion models are adept at handling
data across different modalities. For example, image cap-arXiv:2309.07623v1  [cs.CV]  14 Sep 2023

--- PAGE 2 ---
tioning (Vinyals et al. 2015; Hossain et al. 2019) exemplifies
the route of image →text, whereas text-conditioned image
generation (Goodfellow et al. 2014; Ramesh et al. 2021) il-
lustrates the transition from text →image. These models sig-
nal a significant advancement in producing realistic samples
across diverse data types. Yet, their training chiefly relies
on paired data, such as image-text pairs. The available vol-
ume of such paired data is substantially smaller than sin-
gle modality data, with comparisons often being in the ball-
park of hundreds of billion tokens (pure text) v.s.mere hun-
dreds of million pairs (image-text). As a result, modal con-
version models often lack the richness of knowledge and
depth of understanding exhibited by LLMs. This limitation
also means they usually struggle with comprehending and
executing more complex instructions that rely on acquired
knowledge and common sense. (see Figure 1).
Given the aforementioned strengths and limitations of
both LLMs and modality conversion models, an intrigu-
ing proposition emerges: Can we amalgamate the profound
knowledge and understanding capabilities of LLMs with the
modality conversion models? Hence, the integrated models
could potentially interpret more intricate instructions and
deliver outputs in various modalities. A natural idea that
emerges from this conundrum is to position the LLM as a
coordinator to orchestrate and utilize modality conversion
models. Some recent works (Schick et al. 2023; Shen et al.
2023; Lian et al. 2023) have demonstrated the immense
potential of leveraging LLMs as a controller, showcasing
their superior orchestration abilities to plan and execute
fine-grained instructions with external tools. For instance,
HuggingGPT (Shen et al. 2023) devised a workflow where
ChatGPT is used as a controller to invoke HuggingFace’s
open-sourced models, thereby accomplishing sophisticated
AI tasks. However, it suffers from a few shortcomings. Inef-
ficiency: HuggingGPT operates as an online model, heavily
leaning on OpenAI’s ChatGPT API. This dependency neces-
sitates frequent invocations of ChatGPT for functions such
as Task Planning and Model Selection, substantially inflat-
ing both the operational cost and latency. Instability: Hug-
gingGPT uses ChatGPT as a black box without any tuning,
and the LLM’s outputs are uncontrolled and do not always
return the desired results, leading to exceptions in the work-
flow. Another example is LLM-Grounded Diffusion (Lian
et al. 2023), which harnesses the reasoning capabilities of
LLMs to produce scene layouts in the form of bounding
boxes based on the input instructions. As a result, it offers
more meticulous control over the positioning and layout of
objects with the generated image. However, the inflexibility
of such methods confines them to a singular modality con-
version pathway, and they often falter when interpreting in-
direct requests. The above shortcomings highlight the chal-
lenges in current integration efforts and the full potential of
combining LLMs with modality conversion models.
In this paper, we endeavor to address the above challenges
and present an instruction-tuned LLM to unlock its ability to
generate non-text outputs, with the help of several modality
conversion models. The primary contributions of this paper
are threefold:• We present a Modality-aligned Instruction Tuning that
efficiently enables LLMs to discern the intended output
modality as dictated by the instructions. Thus, LLMs are
empowered to summon appropriate modality conversion
models for non-text responses while fully retaining their
original reasoning capabilities. With this technique, any
LLM can be easily adapted to produce non-text outputs
with minimal training.
• We introduce a new evaluation set that includes thou-
sands of instructions targeting text, image, and audio
outputs, to assess LLMs’ abilities in handling multi-
modal output requests, enabling a better understanding of
performance variations across different LLMs in multi-
modal scenarios.
• We conduct comprehensive experiments to validate our
approach against state-of-the-art open-source LLMs as
well as OpenAI’s ChatGPT API in multi-modal scenar-
ios. The experimental results indicate that our models
tuned with modality-aligned instructions are able to re-
tain their original reasoning abilities while consistently
generating accurate and appropriate non-text outputs.
Related Work
LLM as Controller. The AI community has witnessed
a transformative surge in Natural Language Processing
advancements over recent years, largely driven by the
emergence of LLMs, such as GPT (Brown et al. 2020),
OPT (Zhang et al. 2022), PaLM (Chowdhery et al. 2022),
Bloom (Scao et al. 2022), and LLaMa (Touvron et al.
2023a). The potential of LLMs extends beyond their im-
mediate function of text generation. With vast knowledge
bases and intricate reasoning capabilities, LLMs closely em-
ulate human-like comprehension. With these capabilities, a
promising direction has emerged, which treats the LLM as
a controller. Instead of using LLMs merely as standalone
entities, they can serve as a coordinator between exter-
nal tools (Shen et al. 2023; Schick et al. 2023; Li et al.
2023b; Qin et al. 2023) or even manipulate robotic sys-
tems (Driess et al. 2023; Mai et al. 2023). For example,
HuggingGPT (Shen et al. 2023) employs ChatGPT as a con-
troller to manage a list of open-sourced models in Hugging-
Face’s Hub to solve AI tasks. However, these methods often
treat LLMs as a black box, utilizing custom prompts for spe-
cific invocation rules, leading to notably unstable outputs.
Contrary to solely depending on pre-trained LLMs for gen-
erating control commands, our method fine-tunes the LLM
to yield structured responses, thereby ensuring more consis-
tent and reliable outcomes.
LLM Finetuning. Given that LLMs typically possess tens
to hundreds of billions of parameters and necessitate training
on vast datasets, the barriers to their training and fine-tuning
are significantly raised, both in terms of computational re-
sources and data collection. Recently, many efforts have
delved into more resource-efficient methods of LLM fine-
tuning. From a data perspective, instruction tuning (Wang
et al. 2022; Liu et al. 2023; Taori et al. 2023) leverages high-
performing, large-scale LLMs such as GPT-3.5 or GPT-
4 to generate vast amounts of instructions and responses,

--- PAGE 3 ---
The famous tower where Galileo 
Galilei conducted his free fall 
experiment is the Leaning Tower of 
Pisa, located in Pisa, Italy. The 
experiment involved dropping objects of different masses from the 
tower to demonstrate that they 
would all hit the ground at the same 
time, disproving the prevailing belief at the time that heavier 
objects fall faster than lighter ones. 
The tower's unique lean has made it 
an iconic landmark and a popular 
destination for tourists from around the world.
Show me the famous tower where Galileo conducted the free fall experiment.
Stable -Diffusion Chat -GPT3.5 Ours
Ours
Prompts: A surreal and 
fantastical landscape 
with strange creatures 
and objects.
Iamreading Alice inWonderland,
and Iamvery curious about the
‘wonderland’ inside the book .
Could you show mewhat does it
look like?
…Instruction Hugging -GPT
Prompts :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Input :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Failed to parse.
LLM -Grounded Diffusion
Generated Layout
BookBackgroundObjects to include
Book [140, 250, 211, 311]
Stable -Diffusion
Of course! "Alice in 
Wonderland" by Lewis Carroll is a whimsical and fantastical story that takes place in a world filled with surreal and imaginative elements. 
Wonderland is a place where 
logic and reality are often turned upside down, and strange occurrences are the norm. ChatGPT
ChatGPT
Human Experts
Modality Datasets
Seed Instructions
Text→Image
Text→Speech
…
Instruction DatasetGenerated Instruction: Can you generate 
an image that represents the concept of 
“time travel”, using everyday objects such 
as a bicycle and a clock?
Responses: [Image] A black metal bicycle 
with a clock inside the front wheel.
Caption: A black metal bicycle with a 
clock inside the front wheel.Generation of Aligned Modality -aware Instructions
 Tuning/Inference Pipeline
Instruction Dataset
AdaptorModality Conversion
Text→Image 
Model
Text→Speech 
Model
…
Example Generation Process Instruction Tuning Inference
Inputs
Structured Response
Text
 Speech
 Image
LossSelect
Conversion Route
OutputLLM
I plan to travel to New York 
next week. I heard there’s a 
famous sculpture that was 
a gift from France. What is 
it called?
The famous sculpture you 
are referring to is the 
Statue of Liberty.
Wow, that’s great! Could you show me a photo of the Statue of Liberty?
Beautiful! Now, please use 
your voice to introduce me 
the history of the Statue of Liberty.
The Statue of Liberty was designed 
by French sculptor Fredric August 
Barthodi . It stands on liberty island 
in New York Harbor and has 
become an iconic representation of 
American values.Audio Content
Plan
Text→Image
Text→Audio
???
Expected Output?Figure 2: Comparison of responses to the user’s instruction by different models. While traditional text-to-image models like
Stable Diffusion often generate images based on superficial keywords, they might miss the underlying intent of the instruction.
Hugging-GPT can produce unstable results, for instance, it outputs three responses in this case. LLM-Grounded Diffusion,
though adept at controlling the layout, still falls short in grasping the deeper nuances behind the user’s request, as evidenced
by the unwarranted inclusion of a ‘book’ in its image. In contrast, our proposed approach not only captures the true essence of
the instruction but also aligns it with the desired input for the modality conversion model, resulting in a more faithful visual
representation. (The figure is best viewed zoomed in.)
which are then used to fine-tune relatively smaller LLMs
or even those large models themselves. From the view of
model parameters, Parameter Efficient Fine-Tuning (PEFT)
explores the use of post-training quantization (Dettmers
et al. 2022; Frantar et al. 2023) or freezing LLM param-
eters to train an adapter (Hu et al. 2022; Dettmers et al.
2023), with the goal of reducing computational overhead.
Based on these techniques, low-cost, customizable training
of LLMs has become feasible, for example, Alpaca (Taori
et al. 2023) and Vicuna (Chiang et al. 2023) significantly im-
prove the performance of Llama (Touvron et al. 2023a) by
conducting an instruction tuning. LLaV A (Liu et al. 2023)
presents visual instruction tuning, enabling LLMs to under-
stand image contents by tuning on language-image paired
instructions. However, these efforts either focus on enhanc-
ing the LLM’s reasoning performance or on its image →text
understanding abilities. In contrast, our approach delves
deeper into understanding the intention behind input instruc-
tions, enabling the LLM to decide the most fitting output
modality. This unique capability is achieved using our pro-
posed modality-aligned instruction tuning, which not only
retains the text →text reasoning prowess of the LLMs but
also enables them to produce non-text outputs, including
text→image and text →speech.
Modality Conversion Models. Multi-modal applications
have been a long-standing research topic in the AI commu-
nity. The unique capability of these models to comprehend
and translate between different data representations facili-
tates conversions between various modalities, such as im-
age captioning (Vinyals et al. 2015; Hossain et al. 2019)
(image →text), text-conditioned image generation (Good-
fellow et al. 2014; Ramesh et al. 2021) (text →image),
speech recognition (Ao et al. 2022) (audio →text), or evenmultiple modality conversion (text/image →text/image) such
as Uni-Diffuser (Bao et al. 2023). We specifically focus
on text →image and text →speech conversion in this paper.
Given a text description, a text →image model aims at syn-
thesizing an image that accurately depicts the content de-
scribed in the text. Early methods were mostly based on
GAN (Xu et al. 2018; Goodfellow et al. 2014) and VQ-
V AE (Ramesh et al. 2021), while recently, due to their stabil-
ity and improved generation quality, diffusion-based (Rom-
bach et al. 2022) approaches have become increasingly pop-
ular. For text →speech, the goal is to transform a piece of
text content to corresponding audio. While these models can
convert text into other modalities, they struggle to under-
stand complex instructions, making it difficult to apply them
to advanced interactive features, such as AI assistants. To
tackle these challenges, our method synergizes LLMs with
modality conversion models, thereby aligning the advanced
reasoning abilities inherent to LLMs with the conversion ca-
pabilities of multi-modal models.
SwitchGPT
Preliminary
Why multi-modal output matter? While the text remains the
predominant medium for human communication, images,
and sounds often assume indispensable roles in various con-
texts. For instance, a photograph capturing a golden sunset
over a tranquil beach can elicit emotions that mere words
may find challenging to express. Similarly, visually impaired
individuals heavily depend on auditory cues and descrip-
tions to comprehend their surroundings. Therefore, the value
of non-text outputs cannot be understated.
However, there is still limited research on adapting LLMs

--- PAGE 4 ---
The famous tower where Galileo 
Galilei conducted his free fall 
experiment is the Leaning Tower of 
Pisa, located in Pisa, Italy. The 
experiment involved dropping objects of different masses from the 
tower to demonstrate that they 
would all hit the ground at the same 
time, disproving the prevailing belief at the time that heavier 
objects fall faster than lighter ones. 
The tower's unique lean has made it 
an iconic landmark and a popular 
destination for tourists from around the world.
Show me the famous tower where Galileo conducted the free fall experiment.
Stable -Diffusion Chat -GPT3.5 Ours
Ours
Prompts: A surreal and 
fantastical landscape 
with strange creatures 
and objects.
Iamreading Alice inWonderland,
and Iamvery curious about the
‘wonderland’ inside the book .
Could you show mewhat does it
look like?
…Instruction Hugging -GPT
Prompts :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Input :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Failed to parse.
LLM -Grounded Diffusion
Generated Layout
BookBackgroundObjects to include
Book [140, 250, 211, 311]
Stable -Diffusion
Of course! "Alice in 
Wonderland" by Lewis Carroll is a whimsical and fantastical story that takes place in a world filled with surreal and imaginative elements. 
Wonderland is a place where 
logic and reality are often turned upside down, and strange occurrences are the norm. ChatGPT
ChatGPT
Human Experts
Modality Datasets
Seed Instructions
Text→Image
Text→Speech
…
Instruction DatasetGenerated Instruction: Can you generate 
an image that represents the concept of 
“time travel”, using everyday objects such 
as a bicycle and a clock?
Responses: [Image] A black metal bicycle 
with a clock inside the front wheel.
Caption: A black metal bicycle with a 
clock inside the front wheel.Generation of Aligned Modality -aware Instructions
 Tuning/Inference Pipeline
Instruction Dataset
AdaptorModality Conversion
Text→Image 
Model
Text→Speech 
Model
…
Example Generation Process Instruction Tuning Inference
Inputs
Structured Response
Text
 Speech
 Image
LossSelect
Conversion Route
OutputLLM
I plan to travel to New York 
next week. I heard there’s a 
famous sculpture that was 
a gift from France. What is 
it called?
The famous sculpture you 
are referring to is the 
Statue of Liberty.
Wow, that’s great! Could 
you show me a photo of the Statue of Liberty?
Beautiful! Now, please use 
your voice to introduce me 
the history of the Statue of Liberty.
The Statue of Liberty was designed 
by French sculptor Fredric August 
Barthodi . It stands on liberty island 
in New York Harbor and has 
become an iconic representation of 
American values.Audio Content
Plan
Text→Image
Text→Audio
???
Expected Output?
Please use an image to answer this riddle (Tips: it is a kind of animal): The height of my legs, usually around 6 feet, is ta ller than most humans. What am I?
An Ostrich
[85, 32, 342, 480]The height of 
my legs, 
usually around 
6 feet, is taller 
than most 
humans. What 
am I.A photo of 
giraffe.
Show me an iconic landmark in Xian, China.
An iconic 
landmark in Xian
[12, 45, 488, 421]Show me an 
iconic 
landmark in 
Xian, ChinaThe Terracotta 
Army in Xian, 
China.
Figure 3: Left: Flowchart illustrating the process of generating modality-aligned instructions. By embedding the text from
modality datasets into prompts, our method ensures alignment between ChatGPT-generated instructions and the modality con-
version models. Right: Pipeline of instruction tuning and inference. During training, the parameters of the LLM are frozen,
and an adapter is trained on the generated modality-aligned instructions dataset. At the inference stage, the structured response
of LLM is parsed and used to select the appropriate modality conversion route to produce the final outputs.
for non-text outputs. One primary reason for the lim-
ited research in this area is the prohibitive cost associ-
ated with multi-modal LLM pre-training. A pioneering work
Emu (Sun et al. 2023) aims to bridge this gap by introduc-
ing a vision-language pre-trained LLM. Emu is pre-trained
on massive datasets, comprising billions of image-text pairs
and millions of video-subtitle pairs, using 128 A100 GPUs.
This enables Emu to not only be capable of accepting im-
age/text inputs but also generate textual and visual outputs.
Even though Emu has achieved impressive performance on
various tasks, its adaptability comes at a high cost. For ex-
ample, introducing a new modality such as audio necessi-
tates the collection of corresponding paired data and com-
plete retraining.
Given the flourishing developments in recent years, the
AI community has amassed countless open-source models
for diverse tasks. Integrating these models into LLMs offers
another solution for adapting them to non-text outputs. This
not only conserves computational resources and reduces car-
bon emissions but also provides a cost-effective and smooth
transition to newer models without the need for extensive re-
training. This paper falls into this category. Existing methods
such as HuggingGPT (Shen et al. 2023) can invoke external
models to generate non-text responses beyond the capabili-
ties of traditional LLMs like ChatGPT. However, its results
are unstable. For example, Figure 2 depicts that Hugging-
GPT may produce an audio response when an image out-
put is expected. Moreover, it inputs the full instruction as
a prompt to Stable Diffusion, leading to suboptimal results.
The reasons leading to the above issues can be summarized
as follows:
• Relying solely on designing prompts to utilize the zero-
shot capability of LLM for invoking external models can
introduce ambiguity, leading to unstable outputs.
• The misalignment between LLMs outputs and externalmodel inputs often results in subpar performance.
Modality-aligned Instruction Generation
To solve the aforementioned issues, we introduce Modality-
aligned Instruction Tuning (MaIT). The primary purpose of
MaIT is twofold. First, MaIT aims to cheaply tune the LLM
Lto understand and interpret the expected output modality t
from a given instruction I. Formally, this can be written as:
L(I) = (r)MaIT−→L′(I) = (r, t), (1)
where L′is the adapted LLM after instruction tuning, and
ris the output response. For example, consider a simple
instruction “What is the answer to the following equation
1+1=?”. While Lmight respond to ‘2’, L′is expected to
produce (‘2’, ‘text’), with ‘text’ being a flag denoting the
desired output modality. This modality type flag, t, informs
the LLM about when to invoke which modality conversion
model. For instance, if tis ‘image’, the LLM knows to use
text→image conversion model rather than simply giving the
response r. However, recognizing when and which model
to invoke is not the sole requirement. It is equally crucial
to guide the LLM on how to use the modality conversion
model. Without this guidance, the LLM might misinterpret
the instruction or use a misaligned response as input when
interfacing with the modality conversion model. This can be
seen in the failure example of HuggingGPT depicted in Fig-
ure 2, which results in less-than-ideal outcomes. As such,
a secondary goal of MaIT is to ensure the LLM’s outputs
align seamlessly with the inputs of the modality conversion
model. This is built upon the fact that for a single route of
modality conversion, such as text →image, different models
often share almost the same training dataset. Therefore, in-
stead of aligning the LLM with a particular model, it is more
effective to align it with the training samples themselves. In
other words, the objective is to minimize the distribution gap

--- PAGE 5 ---
between the LLM’s outputs and the textual descriptions in
the modal conversion task training dataset. We can rewrite
Equation 1 to represent this purpose as follows:
L(I) = (r)MaIT−→
min ∆( Dr′,Dtext→t)L′(I) = (r′, t), (2)
where Dr′andDtext→trespectively represent the distribu-
tion of the output r′by the adapted L′, and the text descrip-
tion distribution of the training dataset for the text →ttask.
To achieve this, we directly use the training data from the
modality conversion tasks, such as the caption of images, to
construct the response.
According to Equation 2, we are able to generate
modality-aligned instructions. Specifically, for the modal-
ity type t, we consider the three most common modali-
ties in this paper, i.e., text, image, and speech. Following
the approaches of previous studies (Taori et al. 2023), we
initiate the process by designing seed instructions. Each
record comprises three components: the instruction, the an-
ticipated output modality, and the response. For example,
a record might look like this: {“instruction”: “How do you
pronounce the name of the fast food brand with a yellow
golden arch logo?”, “response”: {“type”: “speech”, “Mc-
Donald’s” }}. Notably, with an abundance of open-source
text-to-text instruction datasets available, we concentrated
our efforts on devising seed instructions specifically for the
text→image and text →speech modalities. To generate in-
structions in larger quantities, we employed OpenAI’s Chat-
GPT API. This involved integrating descriptions from the
modality conversion task’s training data into the ChatGPT
prompt to produce modality-aligned instructions. An exam-
ple of this process is illustrated in Figure 3. Using a picture
caption like “A black metal bicycle with a clock inside the
front wheel”, we prompt ChatGPT to formulate a suitable
instruction that solicits the generation of such an image. In
this case, the ChatGPT comes up with an intriguing instruc-
tion “Can you generate an image that represents the concepts
of ‘time travel’, using everyday objects such as a bicycle
and a clock?”. This showcases ChatGPT’s strong capability
in producing diverse instructions. Additionally, the original
caption, combined with the specified modality type, is em-
ployed to construct the ground-truth response. Specifically,
we use the LAION-aesthetic (Schuhmann et al. 2022) and
LibriTTS (Zen et al. 2019) as referenced modality datasets
to sample image captions or speech contents. In this man-
ner, we synchronize the output of LLM with the input of
the modality conversion model. Importantly, since we ex-
clusively used textual descriptions of images for instruction
creation, it is only necessary to conduct text-to-text tuning
on the LLM. This avoids potential computational overhead
from multi-modal fine-tuning. We will provide more details
about the generated instructions in the appendix.
Training and Inference
Once the modality-aligned instruction dataset has been gen-
erated, we integrate it with the existing text-only instruc-
tions. This results in the training set comprising three
routes of instructions, i.e., text→text, text →image, andtext→speech. In line with previous research, we maintain
a comparable number of instructions for training, totaling
approximately 52k, with each route accounting for roughly
one-third of this total. While conducting instruction-tuning,
it is essential to preserve the original reasoning and gener-
ation capabilities of the LLM. Therefore, for Equation 2,
when tis text, it is desired that the distribution of responses
randr′remain as similar as possible:
min ∆( Dr′, Dr) , t=text
min ∆( Dr′, Dtext→t), t̸=text(3)
This consistency is attained by incorporating the text →text
instructions during the instruction-tuning phase. Further-
more, the parameters of the LLM are kept constant. Only
a Low-Rank adapter (LoRA) (Hu et al. 2022) is fine-tuned,
which not only safeguards the pre-trained weights but also
significantly reduces computational costs.
As shown in Figure 2, a challenge of the current method
such as HuggingGPT (Shen et al. 2023) is the unpredictable
format of the output. Such variability can lead to inconsis-
tencies during subsequent parsing stages, often resulting in
exceptions. To address this issue, we encode all responses
into a structured JSON format, ensuring that the LLM pro-
duces outputs with a consistent structure. Due to its exten-
sive pre-training, LLM is already familiar with the JSON
formatting rules. Therefore, training with a modest amount
of data can effectively guide it to generate appropriately for-
matted outputs. As shown in the right block of Figure 3, the
instructional data consists solely of text, so the LLM does
not require interaction with any modality conversion mod-
els during the training phase. When moving to the inference
stage, the LLM outputs a structured JSON format response
for the given instruction. This determines when to choose
which modality conversion model and how to use it. In this
manner, all modality conversion models remain static with-
out any fine-tuning.
Experiments
Implementation
In implementing the proposed pipeline, we used the Llama-
2 (Touvron et al. 2023b) as our foundation LLM. For modal-
ity conversion, StableDiffusion-v1-5 (Rombach et al. 2022)
and SpeechT5 (Ao et al. 2022) models were employed. The
parameters of the LLM were loaded using Int8 (Dettmers
et al. 2022) and subsequently frozen. With the modality-
aligned instructions generated by GPT-3.5-turbo, we trained
a LoRA adapter (Hu et al. 2022) upon the frozen LLM. Our
training was executed on four Nvidia A100 (40GB) GPUs
(however, training on a single GPU is possible), utilizing a
per-device batch size of 4 and gradient accumulation steps
of 8. Optimization of the model was carried out using the
AdamW (Loshchilov and Hutter 2019) for 3 epochs, with a
consistent learning rate 3−4. The entire training procedure
was efficiently completed in approximately 3 hours.
Validation Set and Metrics
Evaluating LLM performance objectively is crucial for con-
trasting different methods. Current benchmarks, such as The

--- PAGE 6 ---
Method Modality Acc. (%) ↑Vision Language Speech
CLIP↑FID↓ QA↑ BLEU ↑
OPT-2.7B (Zhang et al. 2022) 68.4 16.6 131.3 0.53 0.07
OPT-6.7B (Zhang et al. 2022) 16.2 6.4 277.3 0.66 0.00
Llama-7B (Touvron et al. 2023a) 66.8 17.9 88.6 0.55 0.07
Llama2-7B (Touvron et al. 2023b) 76.2 18.7 86.1 0.69 0.08
Alpaca-7B (Taori et al. 2023) 72.3 17.2 94.7 0.58 0.07
Vicuna-7B (Chiang et al. 2023) 73.1 17.0 95.0 0.59 0.07
GPT-3.5-turbo (OpenAI 2023) 88.1 21.4 84.1 0.75 0.17
HuggingGPT (Shen et al. 2023) 81.7 19.9 85.2 0.62 0.10
SwitchGPT-7B (Ours) 86.9 22.6 82.4 0.67 0.16
Table 1: Comparison of the performance between the proposed SwitchGPT and state-of-the-art models.
Open LLM Leaderboard (Beeching et al. 2023), are pri-
marily tailored to assess text →text reasoning and gener-
ation capabilities. In contrast, this paper focuses on en-
abling the LLM to produce non-text outputs. Given this dis-
crepancy, existing evaluation standards are not suited for a
comprehensive assessment of our model. To address this
gap, we introduce a new validation set (which will be re-
leased) specifically designed to facilitate the assessment of
multi-modal output LLMs. This set contains 2,400 instruc-
tions, each demanding an output in one of three modalities:
text, image, or speech. For the text →text instructions, we
sample questions from established benchmarks, including
Truthful-QA benchmark (Lin, Hilton, and Evans 2022) and
MMLU (Hendrycks et al. 2021), applying a variant multiple-
choice metric. The text →image instructions are bifurcated
into two categories: 200 intricate instructions crafted by hu-
mans, and 600 instructions chosen from proposals gener-
ated by ChatGPT-4 using the COCO-caption (Chen et al.
2015) as a reference. The performance of LLM on this task
is evaluated using both CLIP (Radford et al. 2021) and
FID (Heusel et al. 2017) scores. Regarding the text →speech
tasks, a similar strategy as of the text →image is used to con-
struct the instructions, while Libritts (Zen et al. 2019) is em-
ployed as reference. As the text →speech model translates
input text verbatim, the audio quality remains unaffected
by the LLM’s output. We thus employ the BLEU (Papineni
et al. 2002) score to gauge the similarity between the LLM’s
output and the referenced contents. Additionally, to ascertain
if the model can discern the desired output modality from
the instruction, we evaluate its performance using classifi-
cation accuracy, and only those predictions that match the
ground-truth modality will be further assessed for their vi-
sion, language, or speech scores. Notably, since the default
output modality of LLM is text, these outputs will not con-
tribute to classification accuracy. We explain more details
about evaluation metrics in the appendix.
Quantitative Results
To assess the efficacy of the proposed SwitchGPT, we com-
pared them with a range of state-of-the-art techniques. These
include pretrained LLMs like OPT (Zhang et al. 2022) and
Llama (Touvron et al. 2023a,b); instruction-tuned LLMs
such as Alpaca (Taori et al. 2023) and Vicuna (Chiang et al.
2023); and the commercial API, GPT-3.5-turbo. Addition-Method CLIP FID
Stable Diffusion v1-5 15.2 140.3
LLM-grounded Diffusion 18.9 85.9
SwitchGPT-7B (Ours) 22.6 82.4
Table 2: Comparison of the performance between Stable
Diffusion v1-5, LLM-grounded Diffusion, and our method
for text →image instructions.
ally, we considered methods that employ LLMs as con-
trollers, such as HuggingGPT (Shen et al. 2023) and LLM-
grounded Diffusion (Lian et al. 2023). Given that many text-
based LLMs are not inherently designed to produce non-text
responses, a few-shot assessment approach was adopted.
Specifically, four instructions and corresponding responses
are given as examples, prompting these methods to gener-
ate responses to new inputs (please refer to supplementary
for more details). Notably, all of the LLMs share identical
modality conversion models to produce non-text responses.
Table 1 demonstrates the performance of the proposed
method in comparison with existing techniques. For modal-
ity accuracy, our method achieves comparable performance
with GPT-3.5-turbo and surpasses all other techniques. This
highlights its capability to understand the expected modality
of the instruction. Interestingly, the OPT-6.7B model lags
considerably in accuracy when compared to all other meth-
ods, even its own 2.7B counterpart. We observed that it pre-
dominantly provides text output for most instructions. This
discrepancy might be attributed to its heightened sensitiv-
ity to specific prompts and examples. The CLIP score indi-
cates the congruence of the generated images with their cor-
responding descriptions. As all LLM models utilize the same
text→image foundation model, the score can be used to
measure the quality of the prompt generated by the LLM. the
score serves as an indicator of the prompt quality produced
by the LLM. Our model consistently excels in this metric,
highlighting the effectiveness of the modality-alignment in-
struction tuning. Regarding language scoring, our method
achieves an accuracy comparable to the original Llama2-
7B. This suggests that the foundational reasoning capability
remains unaffected. Additionally, since stable diffusion and
LLM-grounded diffusion can only generate image results,
we compare their performance in Table 2.

--- PAGE 7 ---
The famous tower where Galileo 
Galilei conducted his free fall 
experiment is the Leaning Tower of 
Pisa, located in Pisa, Italy. The 
experiment involved dropping objects of different masses from the 
tower to demonstrate that they 
would all hit the ground at the same 
time, disproving the prevailing belief at the time that heavier 
objects fall faster than lighter ones. 
The tower's unique lean has made it 
an iconic landmark and a popular 
destination for tourists from around the world.
Show me the famous tower where Galileo conducted the free fall experiment.
Stable -Diffusion Chat -GPT3.5 Ours
Ours
Prompts: A surreal and 
fantastical landscape 
with strange creatures 
and objects.
Iamreading Alice inWonderland,
and Iamvery curious about the
‘wonderland’ inside the book .
Could you show mewhat does it
look like?
…Instruction Hugging -GPT
Prompts :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Input :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Failed to parse.
LLM -Grounded Diffusion
Generated Layout
BookBackgroundObjects to include
Book [140, 250, 211, 311]
Stable -Diffusion
Of course! "Alice in 
Wonderland" by Lewis Carroll is a whimsical and fantastical story that takes place in a world filled with surreal and imaginative elements. 
Wonderland is a place where 
logic and reality are often turned upside down, and strange occurrences are the norm. ChatGPT
ChatGPT
Human Experts
Modality Datasets
Seed Instructions
Text→Image
Text→Speech
…
Instruction DatasetGenerated Instruction: Can you generate 
an image that represents the concept of 
“time travel”, using everyday objects such 
as a bicycle and a clock?
Responses: [Image] A black metal bicycle 
with a clock inside the front wheel.
Caption: A black metal bicycle with a 
clock inside the front wheel.Generation of Aligned Modality -aware Instructions
 Tuning/Inference Pipeline
Instruction Dataset
AdaptorModality Conversion
Text→Image 
Model
Text→Speech 
Model
…
Example Generation Process Instruction Tuning Inference
Inputs
Structured Response
Text
 Speech
 Image
LossSelect
Conversion Route
OutputLLM
I plan to travel to New York 
next week. I heard there’s a 
famous sculpture that was 
a gift from France. What is 
it called?
The famous sculpture you 
are referring to is the 
Statue of Liberty.
Wow, that’s great! Could 
you show me a photo of the Statue of Liberty?
Beautiful! Now, please use 
your voice to introduce me 
the history of the Statue of Liberty.
The Statue of Liberty was designed 
by French sculptor Fredric August 
Barthodi . It stands on liberty island 
in New York Harbor and has 
become an iconic representation of 
American values.Audio Content
Plan
Text→Image
Text→Audio
???
Expected Output?
Please use an image to answer this riddle (Tips: it is a kind of animal): The height of my legs, usually around 6 feet, is ta ller than most humans. What am I?
An Ostrich
[85, 32, 342, 480]The height of 
my legs, 
usually around 
6 feet, is taller 
than most 
humans. What 
am I.A photo of 
giraffe.
Show me an iconic landmark in Xian, China.
An iconic 
landmark in Xian
[12, 45, 488, 421]Show me an 
iconic 
landmark in 
Xian, ChinaThe Terracotta 
Army in Xian, 
China.
Figure 4: Qualitative results. From left to right: images produced by Stable Diffusion, LLM-grounded Diffusion, HuggingGPT,
and the proposed SwitchGPT. The right side of each image showcases the intermediate artifacts such as the layout and prompts
(Stable Diffusion uses instruction as prompt directly) generated by the corresponding methods.
Qualitative Results
In Figure 4, we provide a qualitative comparison of vari-
ous methods based on instructions intended for image out-
put generation. The results demonstrate that our proposed
method aligns well with modality conversion models, yield-
ing enhanced performance. Other methods, such as Hug-
gingGPT, tend to use the instruction itself as the prompt to
input into the modality conversion model, resulting in unsat-
isfactory outputs. Meanwhile, Figure 5 presents a demo (to
be released) showcasing an example conversation between
our model and users on the topic ‘The Statue of Liberty’.
The interaction highlights our model’s capability to discern
the underlying intentions of the instructions, despite these
conversational inputs being quite different from the instruc-
tions used during training. Consequently, it is capable of pro-
ducing appropriate responses involving different modalities.
See more results in the appendix.
Conclusion
In this paper, we have presented modality-aligned instruc-
tion tuning, a method designed to adapt text-based LLMs for
generating non-text responses. This is the inaugural effort to
align the outputs of LLMs and the inputs of modality conver-
sion models from a data-driven perspective. A key advantage
is that it allows LLMs to be trained without direct exposure
to non-text modality data. Instead of delving into the com-
plex real of multi-modal pre-training, they simply undergo a
standard instruction tuning process, ensuring computational
efficiency. To objectively evaluate our method, we have in-
troduced a validation set comprising instructions that request
text, image, and audio outputs. Based on this dataset, we ex-
The famous tower where Galileo 
Galilei conducted his free fall 
experiment is the Leaning Tower of 
Pisa, located in Pisa, Italy. The 
experiment involved dropping 
objects of different masses from 
the tower to demonstrate that 
they would all hit the ground at 
the same time, disproving the 
prevailing belief at the time that 
heavier objects fall faster than 
lighter ones. The tower's unique 
lean has made it an iconic 
landmark and a popular destination for tourists from 
around the world.
Show me the famous tower where Galileo conducted the free fall experiment.
Stable -Diffusion Chat -GPT3.5 Ours
Ours
Text -to-Image
Prompts: A surreal and 
fantastical landscape 
with strange creatures 
and objects.
Iamreading Alice inWonderland,
and Iamvery curious about the
‘wonderland’ inside the book .
Could you show mewhat does it
look like?
…Instruction Hugging -GPT
Text -to-Image
Prompts :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Text -to-Speech
Input :Iam reading Alice in
Wonderland, and Iam very
curious about ……
Failed to parse.
LLM -Grounded Diffusion
Generated Layout
BookBackgroundObjects to include
Book [140, 250, 211, 311]
Stable -Diffusion
Of course! "Alice in 
Wonderland" by Lewis Carroll is a whimsical and fantastical story that takes place in a world filled with surreal and imaginative elements. Wonderland is a place where logic and reality are often 
turned upside down, and 
strange occurrences are the norm. ChatGPT
ChatGPT
Human Experts
Modality Datasets
Seed Instructions
Text→Image
Text→Speech
…
Instruction DatasetGenerated Instruction: Can you generate 
an image that represents the concept of 
“time travel”, using everyday objects such 
as a bicycle and a clock?
Responses: [Image] A black metal bicycle 
with a clock inside the front wheel.
Caption: A black metal bicycle with a 
clock inside the front wheel.Generation of Aligned Modality -aware Instructions
 Tuning/Inference Pipeline
Instruction Dataset
AdaptorModality Conversion
Text→Image 
Model
Text→Speech 
Model…
Example Generation Process Instruction Tuning Inference
Inputs
Structured Response
Text
 Speech
 Image
LossSelect
Conversion Route
OutputLLM
I plan to travel to New York 
next week. I heard there’s a famous sculpture that was a gift from France. What is 
it called?
The famous sculpture you 
are referring to is the Statue of Liberty.
Wow, that’s great! Could you show me a photo of the Statue of Liberty?
Beautiful! Now, please use 
your voice to introduce me 
the history of the Statue of Liberty.
The Statue of Liberty was designed by French sculptor Fredric August Barthodi . It stands on liberty island 
in New York Harbor and has become an iconic representation of American values.Audio ContentFigure 5: The proposed SwitchGPT is able to generate non-
text responses to fulfill the given instructions.
amined several state-of-the-art LLMs in a few-shot setting
and benchmarked our approach against them. We anticipate
that our proposed SwitchGPT will serve as a baseline for fu-
ture studies in multimodal output LLMs. In future research,

--- PAGE 8 ---
one promising direction could be to design adapters at the
encoder stage that would enable the integration of multi-
modal inputs. By seamlessly connecting this feature with the
method proposed in this study, it could potentially give rise
to a comprehensive Any-to-Any LLM capable of handling a
wider array of tasks and functionalities.
References
Ao, J.; Wang, R.; Zhou, L.; Wang, C.; Ren, S.; Wu, Y .;
Liu, S.; Ko, T.; Li, Q.; Zhang, Y .; et al. 2022. Speecht5:
Unified-modal encoder-decoder pre-training for spoken lan-
guage processing. In Proc. Annu. Meet. Assoc. Comput. Lin-
guist.
Bao, F.; Nie, S.; Xue, K.; Li, C.; Pu, S.; Wang, Y .; Yue,
G.; Cao, Y .; Su, H.; and Zhu, J. 2023. One transformer
fits all distributions in multi-modal diffusion at scale. arXiv
preprint arXiv:2303.06555 .
Beeching, E.; Fourrier, C.; Habib, N.; Han, S.; Lambert,
N.; Rajani, N.; Sanseviero, O.; Tunstall, L.; and Wolf, T.
2023. Open LLM Leaderboard. https://huggingface.co/
spaces/HuggingFaceH4/open llmleaderboard.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners.
Proc. Advances in Neural Inf. Process. Syst. , 33: 1877–1901.
Chen, X.; Fang, H.; Lin, T.-Y .; Vedantam, R.; Gupta, S.;
Doll´ar, P.; and Zitnick, C. L. 2015. Microsoft coco cap-
tions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325 .
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.
2023. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality. See https://vicuna. lmsys. org .
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;
Gehrmann, S.; et al. 2022. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 .
Conover, M.; Hayes, M.; Mathur, A.; Meng, X.; Xie, J.;
Wan, J.; Shah, S.; Ghodsi, A.; Wendell, P.; Zaharia, M.; et al.
2023. Free dolly: Introducing the world’s first truly open
instruction-tuned llm.
Dettmers, T.; Lewis, M.; Belkada, Y .; and Zettlemoyer, L.
2022. Llm. int8 (): 8-bit matrix multiplication for transform-
ers at scale. Proc. Advances in Neural Inf. Process. Syst.
Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
L. 2023. Qlora: Efficient finetuning of quantized llms. arXiv
preprint arXiv:2305.14314 .
Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.;
Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al.
2023. Palm-e: An embodied multimodal language model.
arXiv preprint arXiv:2303.03378 .
Frantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D.
2023. Gptq: Accurate post-training quantization for gen-
erative pre-trained transformers. In Proc. Int. Conf. Learn.
Representations .Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .
2014. Generative adversarial nets. In Proc. Advances in
Neural Inf. Process. Syst. , volume 27.
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;
Song, D.; and Steinhardt, J. 2021. Measuring massive mul-
titask language understanding. Proc. Int. Conf. Learn. Rep-
resentations .
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and
Hochreiter, S. 2017. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. Proc. Advances in
Neural Inf. Process. Syst. , 30.
Hossain, M. Z.; Sohel, F.; Shiratuddin, M. F.; and Laga, H.
2019. A comprehensive survey of deep learning for image
captioning. ACM Computing Surveys , 51(6): 1–36.
Hu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,
S.; Wang, L.; and Chen, W. 2022. Lora: Low-rank adapta-
tion of large language models. In Proc. Int. Conf. Learn.
Representations .
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2:
Bootstrapping language-image pre-training with frozen im-
age encoders and large language models. In Proc. Int. Conf.
Mach. Learn.
Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li,
Y . 2023b. Api-bank: A benchmark for tool-augmented llms.
arXiv preprint arXiv:2304.08244 .
Li, Y .; Hu, B.; Chen, X.; Ma, L.; and Zhang, M. 2023c. LM-
Eye: An Interactive Perception Network for Large Language
Models. arXiv preprint arXiv:2305.03701 .
Lian, L.; Li, B.; Yala, A.; and Darrell, T. 2023. LLM-
grounded Diffusion: Enhancing Prompt Understanding of
Text-to-Image Diffusion Models with Large Language Mod-
els.arXiv preprint arXiv:2305.13655 .
Lin, S.; Hilton, J.; and Evans, O. 2022. Truthfulqa: Measur-
ing how models mimic human falsehoods. In Proc. Annu.
Meet. Assoc. Comput. Linguist.
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual instruc-
tion tuning. arXiv preprint arXiv:2304.08485 .
Loshchilov, I.; and Hutter, F. 2019. Decoupled weight decay
regularization. In Proc. Int. Conf. Learn. Representations .
Mai, J.; Chen, J.; Li, B.; Qian, G.; Elhoseiny, M.; and
Ghanem, B. 2023. LLM as A Robotic Brain: Uni-
fying Egocentric Memory and Control. arXiv preprint
arXiv:2304.09349 .
OpenAI. 2023. ChatGPT. https://openai.com/blog/chatgpt.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a method for automatic evaluation of machine transla-
tion. In Proc. Annu. Meet. Assoc. Comput. Linguist. , 311–
318.
Qin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin,
Y .; Cong, X.; Tang, X.; Qian, B.; et al. 2023. ToolLLM:
Facilitating Large Language Models to Master 16000+ Real-
world APIs. arXiv preprint arXiv:2307.16789 .
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;

--- PAGE 9 ---
et al. 2021. Learning transferable visual models from natu-
ral language supervision. In Proc. Int. Conf. Mach. Learn. ,
8748–8763. PMLR.
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;
Sutskever, I.; et al. 2019. Language models are unsupervised
multitask learners. OpenAI blog , 1(8): 9.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-
ing the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(1): 5485–5551.
Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-
ford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-
image generation. In Proc. Int. Conf. Mach. Learn. , 8821–
8831. PMLR.
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-
mer, B. 2022. High-resolution image synthesis with latent
diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt.
Recogn. , 10684–10695.
Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili ´c, S.; Hesslow,
D.; Castagn ´e, R.; Luccioni, A. S.; Yvon, F.; Gall ´e, M.; et al.
2022. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 .
Schick, T.; Dwivedi-Yu, J.; Dess `ı, R.; Raileanu, R.; Lomeli,
M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.
Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761 .
Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.;
Wightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis,
C.; Wortsman, M.; et al. 2022. Laion-5b: An open large-
scale dataset for training next generation image-text mod-
els.Proc. Advances in Neural Inf. Process. Syst. , 35: 25278–
25294.
Shen, Y .; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang,
Y . 2023. Hugginggpt: Solving ai tasks with chatgpt and its
friends in huggingface. arXiv preprint arXiv:2303.17580 .
Su, Y .; Lan, T.; Li, H.; Xu, J.; Wang, Y .; and Cai, D. 2023.
Pandagpt: One model to instruction-follow them all. arXiv
preprint arXiv:2305.16355 .
Sun, Q.; Yu, Q.; Cui, Y .; Zhang, F.; Zhang, X.; Wang,
Y .; Gao, H.; Liu, J.; Huang, T.; and Wang, X. 2023.
Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222 .
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;
Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford
Alpaca: An Instruction-following LLaMA model. https:
//github.com/tatsu-lab/stanford alpaca.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023a. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 .
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023b. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 .
Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.
Show and tell: A neural image caption generator. In Proc.
IEEE Conf. Comp. Vis. Patt. Recogn. , 3156–3164.Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2019. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. In
Proc. Int. Conf. Learn. Representations .
Wang, G.; Ge, Y .; Ding, X.; Kankanhalli, M.; and Shan, Y .
2023. What Makes for Good Visual Tokenizers for Large
Language Models? arXiv preprint arXiv:2305.12223 .
Wang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi, D.; and Hajishirzi, H. 2022. Self-instruct: Align-
ing language model with self generated instructions. arXiv
preprint arXiv:2212.10560 .
Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang,
X.; and He, X. 2018. Attngan: Fine-grained text to image
generation with attentional generative adversarial networks.
InProc. IEEE Conf. Comp. Vis. Patt. Recogn. , 1316–1324.
Zen, H.; Dang, V .; Clark, R.; Zhang, Y .; Weiss, R. J.; Jia, Y .;
Chen, Z.; and Wu, Y . 2019. Libritts: A corpus derived from
librispeech for text-to-speech. In Proc. Interspeech .
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.
Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 .
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.
2023. Minigpt-4: Enhancing vision-language understand-
ing with advanced large language models. arXiv preprint
arXiv:2304.10592 .

--- PAGE 10 ---
Appendix
Instruction Generation
Here, we show more details of the generation of modality-
aligned instructions.
Seed Instruction and Generation Prompts
We have manually crafted 100 seed instructions, of which
50 are intended for image output and 50 for audio output.
Listing 1 and Listing 2 show some examples of image and
audio seeds, respectively.
Listing 1: Examples of Image Seed Instruction
1{
2"instruction": "Show me a good
design for web UI.",
3"output":
4 {
5 "type": "image",
6 "response": "A master web UI
design concept sketch
featuring clean layout,
intuitive navigation bar,
and coherent color scheme."
7 }
8},
9{
10 "instruction": "Showcasing the city
a hundred years from now.",
11 "output":
12 {
13 "type": "image",
14 "response": "A futuristic
cityscape with towering
skyscrapers and flying
vehicles."
15 }
16 },
17 {
18 "instruction": "Turn this sentence
into an image: a photo of an
astronaut riding a horse on
Mars.",
19 "output":
20 {
21 "type": "image",
22 "response": "A photo of an
astronaut riding a horse on
Mars"
23 }
24 }Listing 2: Examples of Speech Seed Instruction
1{
2"instruction": "Generate a voice
clip to introduce Wuhan.",
3"output":
4 {
5 "type": "audio",
6 "response": "Wuhan, located in
Central China, is known for
its educational institutions
like Wuhan University and
landmarks such as the Yellow
Crane Tower. Famous for its
street food, the city
embodies the spirit of
modern China with its rich
history and vibrant culture."
7 }
8},
9{
10 "instruction": "How do you
pronounce this name: John?",
11 "output":
12 {
13 "type": "audio",
14 "response": "John"
15 }
16 },
17 {
18 "instruction": "Perform a famous
line from the movie 'The
Godfather' using voice.",
19 "output":
20 {
21 "type": "audio",
22 "response": "I'm going to make
him an offer he can't
refuse."
23 }
24 },
25 {
26 "instruction": "Read this tongue
twister 'She sells sea shells by
the seashore. The shells she
sells are surely seashells. So
if she sells shells on the
seashore, I'm sure she sells
seashore shells.'",
27 "output":
28 {
29 "type": "audio",
30 "response": "She sells sea
shells by the seashore. The
shells she sells are surely
seashells. So if she sells
shells on the seashore, I'm
sure she sells seashore
shells."
31 }
32 }

--- PAGE 11 ---
After constructing seed instructions, we devised templates
to prompt ChatGPT to generate instructions in bulk.
You are asked to generate instructions based on given
image descriptions. Here are some requests:
1. Each instruction should be short and concise, ideally
1 to 2 sentences long in English.
2. Each instruction should be based on the image de-
scriptions. But, try not to directly use the contents in
descriptions to form the instruction, and reduce the use
of the words that already appeared in descriptions.
3. Do not generate a request that might lead to con-
fusing answers. For example, do not ask to generate a
character without any further information.
4. The language of instructions should be diverse, try to
use different verbs for each instruction to maximize the
diversity. Either an imperative sentence or a question is
permitted.
Here are some examples:
// Each time we randomly insert 3 examples from seed
instructions here
[A photo of avocado]
Instruction: Visualize the answer to this riddle: I’m a
fruit that’s green when I’m raw and black when I’m
ripe. I’m buttery but I’m not a dairy product. What am
I?
[A futuristic cityscape with towering skyscrapers and
flying vehicles.]
Instruction: Showcasing the city a hundred years from
now.
[Mythical creature with dragon’s body, phoenix’s
wings, and both covered in flames.]
Instructions: Create a visual representation of a mythi-
cal creature that combines elements of a dragon and a
phoenix.
The following are some image descriptions, Please
generate an appropriate instruction for each of them:
// Image captions are sampled from LAION-aesthetics
[A large passenger airplane flying over some palm
trees]
[A dog watching a man cut a piece of food]
...
The template provided above illustrates the prompt input
for OpenAI’s GPT API (OpenAI 2023), tailored for generat-
ing image instructions. To generate speech instructions, we
simply adjust some keywords, such as changing ‘image’ to
‘speech’, and use the text-to-speech training set as a refer-
ence. For each time the API is utilized, three examples from
the seed instructions are embedded into the prompt, and
60 captions are sampled from the reference dataset. Con-
sequently, the API can deliver up to 60 instructions per call,
with a cost of <USD$ 0.01 each time.
generatecreatewrite
rewrite
give
explain
edit
provide
suggestdescribemakesummarizeclassifyconstructfindnamedesignidentifycalculatelist
list
sentence
story
title
list
story
sentence
poem
story
sentence
description
function
sentence
paragraph
statement
phrase
example
list
tip
reason
concept
difference
meaning
importance
sentence
text
paragraph
code
example
list
tip
definition
way
activity
strategy
title
process
difference
way
character
list
 sentence
prediction
argument
article
text
point
story
sentence
item
animal
text
sentence
query
argument
expression
word
number
example
area
type
country
benefit
animal
logo
algorithm
 game
interface
type
word
difference
theme
area
cost
sum
average
advantage
step
benefit
reason(a) Text-only Instructions (Taori et al. 2023)
generate
create
show
design providerecitecomposeproducewritegivedescribeexplainsuggestconstructfinddrawnamevisualizelistidentify
image
clip
list
sentence
image
clip
representation
list
image
picture
sunset
photo
logo
image
poster
cover
clip
image
example
representation
quote
line
speech
poem
clip
speech
poem
tweet
image
clip
representation
story
sentence
description
 function
example
clip
speech
 list
process
taste
character
difference
concept
difference
 meaning
purpose
way
strategy
idea
title
sentence
query
algorithm
timeline
word
number
area
synonym
landscape
portrait
cityscape
character
benefit
type
character
thing
cityscape
scene
cabin
sunset
benefit
advantage
example
element
type
strategy
country
way
(b) Modality-aligned Instructions (Ours)
Figure 6: Comparison of the distribution of some top root
verbs and corresponding nouns between the text-only in-
structions generated by Alpaca (Taori et al. 2023) and ours
modality-aligned instructions.
Based on the above method, we generated approximately
40k instructions with expected image or audio outputs. Sub-
sequently, we removed those that were malformed and could
not be parsed. Additionally, we eliminated instructions that
requested nonspeech audio, such as the sounds of a cat
purring, thunder clapping, or music, as well as those asking
for sentences to be read in other languages, such as Japanese
or Arabic.
Figure 6 contrasts the distribution of root verbs and their
corresponding nouns between our modality-aligned instruc-
tions and a text-based instruction dataset produced by Al-
paca (Taori et al. 2023). As depicted, our instructions con-
tain a substantial number of non-text directives, like “Gener-
ate image”, “Design logo”, and “Recite quote”, which were
deliberately excluded by Alpaca. We merge

--- PAGE 12 ---
Evaluation
Few-shot Inference for Text-based LLMs
In the main paper, we have explained that with proper
prompts, the pre-trained pure text-based LLMs are also ca-
pable of analyzing the intended output modality of given in-
structions. Therefore, we designed the following template
prompt to evaluate these LLMs.
You are a helpful assistant. Given an instruction, your
task is to generate an appropriate JSON-formatted
response. Here’s how you can do it:
1. Determine the expected output modality of the given
instruction. The available options are ‘text’, ‘image’,
and ‘audio’.
2. Place the expected modality in the ‘type’ field of
your response.
3. Proceed to fulfill the instruction based on the deter-
mined modality:
• If the desired modality is ‘text’ or ‘speech’, provide
the response directly in the ‘response’ field.
• If the desired modality is ‘image’, generate a con-
cise and brief prompt. This prompt will be sent to
a text-to-image model, which will then generate the
image that fulfills the request.
Instruction: “Show me the architectural style of the
Baroque era.”
Response: ‘“type”: “image”, “response”: “Ornate,
grand Baroque-style palace with dramatic lighting and
complex shapes.”’
Instruction: “Show me a tranquil Japanese Zen garden.”
Response: ’“type”: “image”, “response”: “A tranquil
Japanese Zen garden with raked sand, smooth rocks,
moss, and a wooden bridge over a small pond.”’
Instruction: “How do you say ‘October 1, 2000’ in spo-
ken English?”
Response: ‘“type”: ”audio”, “response”: “October first,
two thousand.”’
Instruction: “Create an audio clip with Easter as the
theme.”
Response: ‘“type”: “audio”, “response”: “Easter, a
cherished holiday, is celebrated as a time of renewal
and rebirth. It brings joy with its symbols of spring,
such as eggs, bunnies, and flowers.”’
Instruction: “What are the three primary colors?”
Response: ‘“type”: “text”, “response”: “The three
primary colors are red, blue, and yellow.”’
Instruction: {instruction }
Response:
During the inference stage, the ‘ {instruction }’ will bereplaced with actual inputs. While this method can measure
the performance of existing methods to a certain extent, it’s
important to note that the outputs can vary significantly de-
pending on the method employed. A clear drawback of such
approaches is the need to encode lengthy explanations and
examples into the prompt each time the LLM is tasked with
generating a response. This can be inefficient; for instance,
the template prompt itself contains over 400 tokens, lead-
ing to significant redundancy. To ensure a fair comparison,
all text-based LLMs undergo few-shot inference using the
identical template.
Evaluation Instructions
There is no off-the-shelf suite to evaluate multi-modality
output LLMs. Given that the training set consists of short in-
structions generated by GPT, employing similar instructions
as a validation set for assessing model performance is in-
appropriate. Hence, rather than using short instructions, we
utilize lengthier, conversation-like inputs for validation. This
approach avoids potential overlap with the training set and
better assesses the model’s generalization ability. We man-
ually gathered conversation segments as validation inputs
(200 for each modality, excluding text). These segments,
combined with our instruction generation pipeline and man-
ually filtered, yielded a total of 800 validation instructions
per modality. For the text modality, given the abundance
of existing evaluation sets, we directly sampled 800 entries
from datasets like TruthfulQA (Lin, Hilton, and Evans 2022)
and MMLU (Hendrycks et al. 2021). Listing 3 shows some
examples from the validation set.
Listing 3: Examples of Validation Instruction
1{
2"instruction": "I am confused by so
many types of camelids. Could you
show me a photo of different types
of these creatures? Thank you!",
3"output": {
4"type": "image",
5"response": "A photo compares a
llama, an alpaca, a vicuna, and
a Gunaco side by side.",
6"image_id": "24531"
7}},{
8"instruction": "Can you show me the
famous Japanese painting which
includes wave and mountain fuji?",
9"output": {
10 "type": "image",
11 "response": "The Great Wave off
Kanagawa.",
12 "image_id": "42153"
13 }}
Metrics
Modality Classification Accuracy. We employ classifica-
tion accuracy as the metric to evaluate whether a method
can accurately interpret the desired output modality speci-
fied by the given instructions. It can be simply calculated as
follows:

--- PAGE 13 ---
I am confused by so many types of camelids. Could you show me a photo of different types of these creatures? Thank you!
A  llama, a 
vicuna, a camel, 
an alpaca, a 
guanacoA photo of  
different types 
of camelids.A photo of 
different types 
of camelids, 
including 
camels, llamas, 
and alpacas.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours)
 Reference (GT)
Can you show me the famous Japanese painting which includes wave and mountain Fuji?
A wave
[10, 250, 292, 212]
Mountain Fuji
[320, 115, 182, 297]Can you show 
me the famous 
Japanese 
painting which 
include wave 
and mountain 
Fuji?Katsushika 
Hokusai’s ‘The 
Great Wave off 
Kanagawa’
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) Reference (GT)
I heard you went to the Louvre in Paris. You must have seen that most famous portrait authored by Davinci! Show me a photo of this!
Return Errors 
for 3 times.A photo of the 
Mona Lisa 
painting by 
Leonardo da 
Vinci at the 
Louvre 
Museum in 
Paris.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) Reference (GT)
A certain organism once ruled the Earth for two hundred million years and then went extinct in the Cretaceous -Paleogene extincti on event sixty million years ago. Could you paint 
a watercolor picture of this organism for me?
A certain 
organism once 
ruled the Earth 
for two 
hundred 
million years 
and then when 
extinct in 
the …A watercolor 
painting of a 
dinosaur.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) Reference (GT)
I am writing a popular science article about supernova explosions. Please help me design an illustration for this topic.
I am writing a 
popular 
science article 
about 
supernova 
explosion.An illustration 
of a supernova 
explosion, 
wish a star at 
the center and 
a bright, 
expanding 
cloud of gas 
and dust.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) Reference (GT)The Mona Lisa
[50, 60, 412, 392]
A dinosaur
[25, 96, 462, 316]
A supernova explosion
[50, 70, 412, 372]
A star
[218, 176, 75, 75]
Figure 7: Qualitative Results (Best viewed zoomed in).
A=ncorrect
ntotal(4)
In this equation, ncorrect represents the number of correctly
classified modalities, while ntotal denotes the total count oftest instructions.
Language. For language evaluation, existing datasets re-
sembling multiple-choice formats already provide several
correct and incorrect answers for each instruction. We em-
ploy the BLEU score to match the responses generated by

--- PAGE 14 ---
My daughter  tole me that  she wants  to draw  a mythical creature she saw in a cartoon. It is a colorful horse -like animal with a horn on its head and wings on its back. Can yo u show 
me a painting example?  
A colorful 
horse -like 
animal with a 
horn on its 
head and 
wings on its 
back.A painting of a 
mythical 
creature, a 
colorful horse -
like animal 
with a horn on 
its head and 
wings on its 
back.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) R eference (GT)A colorful horse -like 
animal with a horn on 
its head and wings on 
its back.
[62, 137, 390, 250]
I am designing a poster for a beer festival. Can you show me an example with two cups of beer in the center of the poster?
A poster for a 
beer festival 
with two cups 
of beer in the 
center
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) R eference (GT)A cup of beer
[136, 206, 120, 180
A cup of beer
[256, 206, 120, 180]
I am designing a 
poster for a beer 
festival.
Two cups of beer in 
the center of the 
poster
The theme of this contest is “Future Space in My Eye”. I hope you can show me the world in your eyes through your art works.
The theme of 
this contest is 
“Future Space 
in My Eye”. I 
hope you can 
show me the 
world in your 
eyes through 
your artworks.I will create a 
futuristic 
space -themed 
artwork that 
showcases the 
beauty of the 
cosmos 
through my 
eyes.
Stable Diffusion LLM -grounded Diffusion (ChatGPT -4) HuggingGPT (GPT3.5) SwitchGPT (ours) R eference (GT)[]No Image 
Generated
No Image 
GeneratedFigure 8: Failure Cases.
LLMs against the reference answers. Subsequently, we se-
lect the response that is most similar to determine the final
accuracy.
Image. We utilize two widely accepted metrics to assess the
quality of generated images based on their instructions: the
CLIP and FID scores. The CLIP score measures the rele-
vance between the generated image and the corresponding
ground-truth descriptions. For instance, Listing 3 presents
several records from the test set. Within each record, there’s
a textual description of the anticipated image in the ‘re-
sponse’ field, which we employ to compute the CLIP score.
On the other hand, the FID score evaluates the quality of im-
age generation by calculating the Fr ´echet inception distance
between the generated images and their reference counter-
parts. The referenced image is indicated by the ‘image id’
field in the records.
Speech. All LLMs utilize the same speech generation
model, resulting in nearly the same audio quality. There-
fore, our primary concern is whether the content aligns with
the instructions. As such, we approach the evaluation of this
task as a translation/generation task, employing the BLEU
score to measure the similarity between the response and
the ground truth.Qualitative Results
In the main paper, we have presented some qualitative re-
sults. Here, we show more comparisons between our meth-
ods and existing methods.
More comparisons
Figure 7 displays additional examples from both existing
methods and our approach. Several interesting observations
are as follows:
• While Stable Diffusion (Rombach et al. 2022) was
not specifically optimized for processing extended
conversation-like inputs, it remains adept at capturing
key terms within such instructions. Thus, if the antic-
ipated output is evident or directly associated with the
input, Stable Diffusion can still produce satisfactory re-
sults. For instance, in the last row of Figure 7, the term
‘supernova explosions’ is present in the input instruction,
prompting Stable Diffusion to generate a commendable
representation of this concept. Similarly, in the second
row of Figure 7, even though the instruction does not ex-
plicitly mention the painting’s title — ‘The Great Wave
off Kanagawa’ — Stable Diffusion creates an almost per-
fect image (though the position and size of the mountain

--- PAGE 15 ---
are different from the referenced image) based on the
given input. This proficiency might be attributed to Sta-
ble Diffusion’s prior exposure to similar images during
its training. Phrases like ‘wave’, ‘Japanese painting’, and
‘Mount Fuji’ are sufficient cues for it to recognize and re-
trieve from its stored knowledge. In essence, for widely
recognized concepts, Stable Diffusion can produce com-
mendable results with sparse information, leveraging its
extensive pre-training. However, lesser-known concepts
or requests that require stronger reasoning ability, might
not always yield promising results.
• LLM-grounded Diffusion (Lian et al. 2023), leveraging
the extremely strong reasoning capabilities of ChatGPT-
4, yields promising results. However, when converted to
ChatGPT-3.5, it fails to produce any results for a major-
ity of inputs. Notably, for concepts that are composed of
multiple sub-items, the LLM-grounded Diffusion creates
layouts for each individual item. This approach can in-
advertently segment a cohesive concept into disjointed
fragments. For instance, as depicted in the second row
of Figure 7, the LLM-grounded Diffusion separated the
painting into two distinct elements: a wave and Mount
Fuji. Consequently, the final output appears as if two un-
related images were pieced together, leading to gaps and
extraneous content in their juxtaposition.
• In the case of HuggingGPT (Shen et al. 2023), given
its support for multi-step tasks, it occasionally produces
unnecessary and irrelevant modality outputs. For in-
stance, it might frequently provide an audio clip along-
side the output image. Additionally, HuggingGPT some-
times struggles to properly parse its own output, lead-
ing to unexpected errors. As depicted in the third row of
Figure 7, HuggingGPT failed thrice in executing the pro-
vided instruction. Another limitation is its misalignment
between its output and the input of the text-to-image
models. Consequently, it often returns a segment of the
instruction as a prompt for image generation. This results
in its outputs being notably similar to the original stable
diffusion in many cases.
Failure Cases
In Figure 8, we present several failure cases of our proposed
model, SwitchGPT.
In the first row, all methods fail to recognize the intended
entity ”a colorful unicorn” from the inputs. Instead, they de-
fault to using the description “a colorful horse-like animal”
provided in the given instruction, leading to erroneous out-
puts.
The second row illustrates that when given an instruction
requesting a specific layout, such as “two cups of beer in
the center”, our method predominantly relies on the modal-
ity conversion models to interpret the prompt, even though
it offers a relatively accurate description. On the other hand,
LLM-grounded Diffusion excels in this instance, being opti-
mized for such scenarios. It directly generates a layout with
bounding boxes for two beers, yielding an optimal result. In-
terestingly, HuggingGPT segments the instruction into two
parts. It uses the first segment “I am designing a posterfor a beer festival” for a text →image task and then em-
ploys the generated image, along with the second part “two
cups of beer in the center of the poster”, as prompts for
an image →image task. The final output successfully places
“two cups of beer in the center.”
In the third row, both LLM-grounded Diffusion and
SwitchGPT fail to produce an image, albeit for distinct rea-
sons. The former’s failure arises because ChatGPT-4 returns
an empty object list for this instruction, preventing LLM-
grounded Diffusion from creating any images. In contrast,
our model struggles to discern the intended modality of the
given instruction, leading it to produce a purely textual re-
sponse.
