# 2309.08637.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.08637.pdf
# File size: 14529794 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TEXTBIND:
Multi-turn Interleaved Multimodal Instruction-following in the Wild
Huayang Li♡♠*Siheng Li♡♣*Deng Cai♡*Longyue Wang♡
Lemao Liu♡Taro Watanabe♠Yujiu Yang♣Shuming Shi♡
♡Tencent AI Lab♠Nara Institute of Science and Technology♣Tsinghua University
Abstract
Large language models with instruction-
following abilities have revolutionized the field
of artificial intelligence. These models show
exceptional generalizability to tackle various
real-world tasks through their natural language
interfaces. However, their performance heav-
ily relies on high-quality exemplar data, which
is often difficult to obtain. This challenge is
further exacerbated when it comes to multi-
modal instruction following. We introduce
TEXTBIND, an almost annotation-free frame-
work for empowering LLMs with multi-turn
interleaved multimodal instruction-following
capabilities. Our approach requires only image-
caption pairs and generates multi-turn multi-
modal instruction-response conversations from
a language model. To accommodate inter-
leaved image-text inputs and outputs, we de-
vise MIM , a language model-centric architec-
ture that seamlessly integrates image encoder
and decoder models. Extensive quantitative
and qualitative experiments demonstrate that
MIM trained on TEXTBINDachieves remark-
able generation capability in multimodal con-
versations compared to recent baselines.
1 Introduction
Artificial intelligence (AI) has experienced a signif-
icant paradigm shift with the rise of large language
models (LLMs). These models are capable of pro-
cessing a wide range of natural language process-
ing (NLP) applications through natural language
interactions with users (OpenAI, 2022, 2023).
Recently, a number of efforts have been made
to augment LLMs with visual perception and un-
derstanding abilities. Prior work uses template-
based instruction-following datasets for training
(Xu et al., 2023b; Dai et al., 2023; Li et al., 2023c).
*Equal Contribution. Work done during HL and SL’s
internships at Tencent AI Lab. Correspondence to DC
(thisisjcykcd@gmail.com ).These datasets comprise a variety of classic com-
puter vision (CV) tasks, e.g., object detection, with
each task being converted into an instructional for-
mat using a handful of human-written natural lan-
guage instructions. However, classic CV tasks of-
ten represent manageable and focused abstractions
or simplifications of real-world tasks (Marr, 2010),
they generally fall short in representing the true va-
riety and complexity of real-world tasks and captur-
ing the lexical diversity of human language. For ex-
ample, most of them are single-turn inquiries about
a single input image, albeit a small fraction sup-
ports multi-turn textual interactions or multiple im-
age inputs. Consequently, the instruction-following
capabilities of models trained on these datasets re-
main limited in open-world scenarios (Xu et al.,
2023a). This is reminiscent of the early develop-
ment of instruction tuning in NLP, where public
NLP tasks were initially adopted but eventually
superseded by high-quality, diverse open-world in-
struction data (Ouyang et al., 2022). Nevertheless,
collecting such data for multimodal models can be
extremely costly.
In this paper, we address the above challenge
by introducing TEXTBIND, an almost annotation-
free framework for augmenting LLMs with multi-
turn interleaved multimodal instruction-following
capabilities. The main idea is to represent im-
ages through their textual descriptions, e.g., cap-
tions, and utilize an LLM to generate multi-turn in-
structions and responses. To ensure the coherence
and meaningfulness of the constructed multi-turn
conversations, we propose a series of strategies
such as topic-aware image sampling and human-
in-the-loop refinement of in-context demonstra-
tions. TEXTBINDcan harvest large-scale datasets
given the abundance of public image-caption pairs.
TEXTBINDprovides examples of processing and
generating arbitrarily interleaved image-text con-
tent. To accommodate interleaved image-text in-
puts and outputs, we devise MIM , a multimodalarXiv:2309.08637v5  [cs.CL]  3 Jun 2024

--- PAGE 2 ---
model that emphasizes the reasoning abilities of
LLMs and seamlessly integrates image encoder and
decoder models.
For a comprehensive evaluation of open-world
multimodal instruction following, we also con-
struct TEXTBINDEVAL, an evaluation set of
TEXTBINDvalidated and refined by human annota-
tors. To verify the effectiveness of TEXTBIND, we
compare MIM models trained on TEXTBINDand
various existing datasets such as MiniGPT-4 (Zhu
et al., 2023) and LLaV A (Liu et al., 2023b). The re-
sults demonstrate that TEXTBINDsubstantially out-
performs these datasets across multiple evaluation
metrics. Additional evaluations on conventional
vision-language, including MME (Fu et al., 2023),
MMBench (Liu et al., 2023c), and MM-Vet (Yu
et al., 2023), further confirm that TEXTBINDpro-
vides complementary benefits to existing multi-
modal datasets. For image generation quality, we
compare the proposed MIM with GILL (Koh et al.,
2023a) and SDXL (Podell et al., 2023), showcasing
improved performance (e.g., 0.673 vs. 0.608 CLIP
scores). The advantage is particularly pronounced
in multi-turn scenarios. Moreover, a holistic human
evaluation further substantiates the superior perfor-
mance of MIM trained on TEXTBINDin contrast to
baseline models in the context of open-world multi-
turn interleaved multimodal instruction-following.
Most interestingly, rather than requiring users to
provide explicit commands and direct descriptions
of the desired image, our model can spontaneously
generate images in proper conversation contexts.
Our qualitative experiments show that MIM trained
onTEXTBINDcan perform a wide range of tasks,
including composing engaging stories inspired by a
set of images, identifying commonalities and differ-
ences across multiple images, explaining concepts
with vivid images, generating long coherent stories
with illustrations, etc. Examples can be found in
Appx. A. We hope TEXTBINDserves as an initial
step towards building AGI that can interact with
humans flexibly in different modalities and broad
real-world scenarios.
2 Related Work
Multimodal Datasets Existing multimodal
datasets can be broadly classified into two
categories: (1) Conventional datasets for specific
vision-language tasks such as image captioning
(Chen et al., 2015; Agrawal et al., 2019; Young
et al., 2014) and visually-grounded question
answering (Hudson and Manning, 2019; Marinoet al., 2019; Singh et al., 2019; Lu et al., 2022;
Zhou et al., 2018; Goyal et al., 2017; Gurari et al.,
2018). (2) Recent datasets for general instruction
following. For instance, MultiInstruct (Xu et al.,
2023b), InstructBLIP (Dai et al., 2023), and M3IT
(Li et al., 2023c) convert existing vision-language
datasets into a unified instructional format with
handcrafted templates. This approach is remi-
niscent of the early explorations on instruction
tuning in NLP (Wei et al., 2022; Sanh et al.,
2022), where existing NLP tasks were phrased
as instructions. However, it has been reported
that such instruction-tuned multimodal models
still generalize poorly to open-world scenarios
(Xu et al., 2023a). This finding also aligns with
the observations in NLP (Ouyang et al., 2022),
where template-based instruction tuning is less
effective than instruction tuning data collected
from real-world scenarios due to its restricted
diversity. There are also some attempts to convert
the output of existing vision-language models
into natural language answers for constructing
instruction-tuning data (Liu et al., 2023b; Zhu
et al., 2023; Chen et al., 2023a).
Compared to existing instruction-tuning data,
the examples in TEXTBIND(1) generally exhibit
greater task and lexicon diversity; (2) typically in-
volve multiple images scattered throughout a multi-
urn conversation; (3) support multimodal output
(image generation).
Multimodal Models To augment existing LLMs
with visual abilities, one straightforward approach
is to employ off-the-shelf vision models as external
tools. That is, the LLM calls expert vision models
through their language interfaces for completing
specific visual tasks when needed (Wu et al., 2023a;
Shen et al., 2023; Chen et al., 2023b; Zou et al.,
2022; Yang et al., 2023; Surís et al., 2023).However,
these approaches may suffer from cross-modal in-
formation loss and lack of generality.
Recently, end-to-end multimodal language mod-
els have garnered significant interest. Flamingo
(Alayrac et al., 2022) and OpenFlamingo (Alayrac
et al., 2022) are among the pioneering work to
extend LLMs to vision-language pretraining. Dif-
ferent from training from scratch, subsequent re-
search efforts have focused on integrating pre-
trained vision and language models. BLIP-2 (Li
et al., 2023b) proposes Qformer to align the fea-
ture spaces of vision models and language models.
To date, various network architectures and training

--- PAGE 3 ---
strategies have been proposed (Zhu et al., 2023;
Liu et al., 2023b; Ye et al., 2023; Li et al., 2023a;
Zhang et al., 2023; Du et al., 2022; Chen et al.,
2023a; Dai et al., 2023; Liu et al., 2023a). How-
ever, these models are limited to the use of visual
content as input. Our work is inspired by recent
work on LLM-empowered image retrieval or gen-
eration (Koh et al., 2023b,a) and the pioneer work
of (Sun et al., 2022) for chitchat in the context of
single photo sharing. Contrary to prior work, we
aim to present the first instruction-following model
capable of processing and generating arbitrarily
interleaved image-text inputs and outputs.
Numerous contemporary studies also exist in
this field (Team, 2023; Yao et al., 2023; Dong et al.,
2023; Zheng et al., 2023; Ge et al., 2023). A unique
characteristic of our work lies in our emphasis on
the aspect of data creation, whereas other stud-
ies primarily concentrate on architecture design or
training algorithms. Pan et al. (2023) and Chen
et al. (2023c) focus on image editing, while Wu
et al. (2023b) and Moon et al. (2023) place empha-
sis on incorporating additional modalities, such as
audio and video.
Evaluation Conventional vision datasets de-
signed for specific tasks and scenarios may suf-
fer from data contamination issues for evaluating
LLMs. Recently, efforts have been made to provide
systematic evaluations with a broader coverage of
diverse visual abilities. MME (Fu et al., 2023) is
an evaluation dataset containing visually-grounded
Yes/No questions. OwlEval (Ye et al., 2023) is a
benchmark comprising 82 questions based on 50
images and relies on human feedback evaluation.
The test size is limited, and the results may suffer
from subjective bias. In response to these chal-
lenges, MMbench (Liu et al., 2023c) and MM-Vet
(Yu et al., 2023) are two recent benchmarks aim-
ing to offer more comprehensive evaluations by
incorporating the use of ChatGPT/GPT4 for an-
swer verification. LVLM Arena (Xu et al., 2023a),
an online evaluation framework that ranks different
models using human judgment, is also introduced.
However, the above benchmarks primarily focus
on question answering based on a single image at
the beginning of a conversation.
3 T EXTBIND
In this work, we seek to enhance the multi-turn
instruction-following capabilities of a language
model in the context of arbitrarily interleaved im-ages and text. Constructing such datasets poses sig-
nificant challenges: 1) it demands inventive think-
ing for devising high-quality visually-grounded in-
structions and their responses; 2) it requires spe-
cialized expertise to craft appropriate images. To
tackle these issues, we introduce TEXTBIND, a
method that predominantly resorts to existing text-
only language models to produce the desired data.
3.1 Definition of Data
The goal of TEXTBIND is to construct a
collection of multi-turn conversation such as
[x1
u,x1
a, . . . ,xT
u,xT
a], where Tis the number of
turns, xi
udenotes the i-th instruction from the user,
andxi
arepresents the i-th response from the assis-
tant. The conversation is also accompanied by an
image set {m1, . . . ,mn}, where nis the number
of unique images in this conversation. Each instruc-
tionxi
uor response xi
ais a sequence of tokens in
Vlang∪V img, where Vlangis the ordinary vocabulary
of a language model and Vimgcontains ndistinct
pointers to the images m1, . . . ,mnrespectively. It
is worth noting that every image can appear at any
point within the conversation.
3.2 Automatic Data Generation
TEXTBIND consists of a three-step pipeline: 1)
topic-aware image sampling for ensuring the co-
herence of each conversation and the diversity
across conversations; 2) LLM-empowered multi-
turn instruction-response generation to create natu-
ral and practical conversations; 3) post-processing
and filtering to eliminate low-quality data. An
overview of the pipeline is shown in Fig. 1.
Topic-Aware Image Sampling The initial step
ofTEXTBINDentails assembling groups of images
that will serve as the foundation for generating
multi-turn conversations. In order to facilitate co-
herent, meaningful, and practical conversations, the
images within each group should exhibit meaning-
ful interconnections. Furthermore, to guarantee a
comprehensive representation of real-world scenar-
ios, the topics of images across different conversa-
tions should demonstrate a wide range of diversity.
Following the above inspirations, we employ
unsupervised clustering algorithms to group the
images in our dataset into clusters and execute a
two-step image sampling process for each conver-
sation. Concretely, we use the image encoder of
the CLIP model (Radford et al., 2021) to obtain
vector representations of images. Then, we exe-
cute the k-means algorithm to classify all images

--- PAGE 4 ---
intoKclusters (topics). Examples of such clusters
are given in Fig. 1. For each conversation, we
randomly sample a cluster from the available K
clusters, then sample n∈ {2,3,4}images from
the chosen cluster. We want to higlight that the
clustered images are semantically relevant, rather
than visually similar.
Generation of Multi-turn Conversations After
selecting a list of images, we proceed to leverage a
text-only LLM, such as GPT-4, to simulate a con-
versation between a user and an assistant based on
the chosen images. The core idea is to let LLMs
receive and process the textual descriptions of the
images as if they see the actual images. Given
the abundance of publicly available image-caption
pairs, we propose representing an image with an
XML-like string <imgX> DESCRIPTION </imgX> ,
where DESCRIPTION serves as a placeholder for the
image caption, <imgX> and</imgX> mark the cap-
tion boundaries, and Xdenotes the image index in
the input image list. After generating the conversa-
tion, we replace the XML-like strings in the con-
versation with the original images. Importantly, to
ensure that a caption faithfully describes its corre-
sponding image, we employ the CLIP model (Rad-
ford et al., 2021) to filter out image-caption pairs
with matching scores below a high threshold.
The detailed prompt can be found in Appx. B,
and examples of generated conversations before
mapping the textual descriptions back to visual im-
ages are shown in Appx. C. In the prompt, we also
provide in-context examples to improve the gener-
ation quality. We collect the in-context examples
through a human-in-the-loop refinement process,
which is elaborated in §3.3.
Post-processing and Low-quality Filtering To
ensure data quality, we filter out conversations
where there is a pair of input and output image
descriptions with an edit distance higher than 0.1.
We also exclude conversations containing image de-
scriptions not present in the provided image list and
conversations containing formatting errors such as
co-reference errors and invalid image tags.
3.3 Human-in-the-loop Refinement
In-context learning has been demonstrated to be
crucial for enhancing the generation quality of
LLMs (Brown et al., 2020; Wang et al., 2023).
Therefore, we also construct a seed set of high-
quality in-context examples S. The seed set S
begins as an empty set and is iteratively updatedStatistics
# of conversations 25,629
Avg. # turns in conversations 3.36
Avg. # images
in conversations 2.46
in instructions 0.94
in responses 1.52
Avg. # words
in conversations 285 .90
in instructions 78.66
in responses 207 .24
Table 1: Statistics of the dataset by applying TEXTBIND
to GPT-4.
with human feedback. In each iteration, we follow
the steps detailed below:
1.We employ the latest Sand the template in
Appx. B, and generate 100 new conversations
using T EXTBIND(§3).
2.We manually analyze the generated conversa-
tions. Each conversation is assigned a quality
label (“Excellent”, “Satisfactory”, or “Poor”).
Besides, we label the visual abilities required
for each conversation.The detailed annotation
guideline for quality labels and visual abilities
is outlined in Tab. 8 (Appx. E).
3.We add the generated conversations with “Ex-
cellent” or “Satisfactory” labels to S.
To ensure diversity across different conversations,
we randomly sample three in-context examples
from the seed set for each generation. We fur-
ther require that at least one in-context example is
labeled “Excellent” and the three examples must
cover all four visual abilities. After three iterations,
we fix the seed set and employ it to generate the
remaining data. The percentage of “Poor” data
annotated by humans declines from 30% to9%.
4 T EXTBINDData from GPT4
We apply TEXTBIND to GPT4 and the Concep-
tual Captions (CC3M) dataset (Sharma et al., 2018;
Changpinyo et al., 2021) as a case study. The de-
tails of the construction process can be found in
Appx. F. In this section, we present comprehensive
analyses of the constructed dataset.
Statistics As depicted in Tab. 1, our constructed
dataset comprises 25,629conversations. The aver-
age number of turns per conversation is 3.36(each
turn is defined as a pair of instruction and response).

--- PAGE 5 ---
Hi! I'm trying to write a cartoon story for my kids featuring a grumpy clown and a lovable cupcake. Can you help me craft this?Of course! That sounds like a wonderful idea. We can start by establishing our characters. The grumpy clown could be someone who struggles finding joy in his own performances.
That’s a wonderful idea! How about the cute cupcake?The cupcake, on the other hand, is always upbeat and tries to help the clown find his happiness. Despite being a tiny cupcake, it has a big heart and a contagious joy. 
Interesting! What should be their adventure? They could go on a journey to find the ultimate joke that guarantees laughter. The lovable cupcake, with his relentless positivity and cheer, helps the grumpy clown see the lighter side of life and finally find his joy.
Cartoon
KidsBasketball Players···Topic-awared Image SamplingConversation Based on Sampled Images
LLM
In-context ExamplesExample SetHuman Rating1
Post-processing
Dataset32Figure 1: Illustration of the TEXTBINDmethod. In the top-left corner, we display five representative images from
each of the three example clusters obtained via unsupervised clustering. On the right-hand side, a conversation is
showcased and constructed using two randomly sampled images from the cartoon cluster.
The mean number of images in each conversation is
2.46. The length of each dialogue is about 285.90.
Diversity To understand the lexical and task di-
versity of our constructed data, we identify four
types of required visual abilities: image creation,
image comparison, intrinsic understanding, extrin-
sic understanding (See Tab. 8 for detailed defini-
tions). The distribution is shown in Fig. 2b. We
observe that a significant portion of conversations
in our dataset focuses on more insightful and infor-
mative tasks, such as extrinsic understanding and
image comparison. For topic diversity, we display
three randomly sampled clusters in Fig. 1. The
distribution of images across different turns is de-
picted in Fig. 2c. We also compare the lexical
diversity of our dataset and existing datasets in Tab.
2. It can be seen that our datasets exhibit higher
lexical diversity than existing datasets.
Quality To check the quality of the generated
data, we randomly sample 100 conversations and
perform an in-depth error analysis. The percent-
ages of “Excellent”, “Good”, and “Poor” data are
57%,34%, and9%respectively. Note that we label
the whole conversation as “Poor” if any of its turns
has a problem. We identify three major sources:
image-caption mismatch, incoherence, and halluci-
nation. Detailed analysis is in Appx. G.5 Augmenting LLMs with Visual I/O
5.1 Model
To support interleaved multimodal inputs and out-
puts, we supplement LLMs with visual input and
output modules. Specifically, LLama2-Chat1(Tou-
vron et al., 2023) is employed as the backbone LM.
For visual input, we use the vision encoder from
BLIP2 (Li et al., 2023b)2, followed by a pretrained
Q-Former model (Li et al., 2023b) that maps the
features from the vision model into the embedding
space of the LM. Inspired by GILL (Koh et al.,
2023a), we attempt to learn a mapping from the
output space of the LM to the input space of a stable
diffusion (SD) model (Rombach et al., 2022) (in
this work, the embeddings produced by the text en-
coder of Stable Diffusion XL (Podell et al., 2023)).
To this end, we explore three model variants in our
preliminary experiments.
Q-Former as Medium . We add a special token
<IMG> to the vocabulary of the LM, indicating that
an image should be generated when it is emitted.
We then use a Q-Former (Li et al., 2023b) that takes
all previous hidden states of the LM as input and
outputs the SD embeddings.
Q-Former with Prompt Tokens as Medium . To
further leverage the reasoning abilities of the LM,
1https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
2https://huggingface.co/Salesforce/blip2-flan-t5-xxl

--- PAGE 6 ---
9%
57%
34%ExcellentSatisfactoryPoorImg CreateImg CompareIntrinsic  Understand.Extrinsic Understand.Percentage0%20%40%60%80%
A VG. Image Num0.00.20.40.50.7
Turn Num012345
UserAssistant(a)
9%
57%
34%ExcellentSatisfactoryPoorImg CreateImg CompareIntrinsic  Understand.Extrinsic Understand.Percentage0%20%40%60%80%
A VG. Image Num0.00.20.40.50.7
Turn Num012345
UserAssistant (b)
9%
57%
34%ExcellentSatisfactoryPoorImg CreateImg CompareIntrinsic  Understand.Extrinsic Understand.Percentage0%20%40%60%80%
A VG. Image Num0.00.20.40.50.7
Turn Num012345
UserAssistant (c)
Figure 2: Statistics of data quality and diversity. The results in Fig. 2a and 2b are based on the human annotations
on 100 randomly sampled conversations.
Large Language Model
Hi, Do you know the breed of this cat?Yes,thecatintheimageappearstobeaRagdoll,abreedknownforitsaffectionatenature. Here is another one:
Vision EncoderQ-FormerLinearWow,Isee,theyaresoadorable.Doyouknowanyotherbreedsofcats?Stable Diffusion<IMG>…<Assis>…<start>Ragdoll<user>…<start>…<start>…Persian<Assis>Wow…<user>…Stable DiffusionSure!TheMaineCoonisalargedomesticcatbreedfromtheU.S.
and thePersiancatisalong-hairedbreedwithadistinctiveflatfaceandlargeeyes, like this:
Figure 3: The architecture of MIM . It integrates a vision model, a language model, and a stable diffusion model.
MIM is able to process multi-turn interleaved multimodal inputs and outputs. Notably, the images generated by
stable diffusion model will be fed back to the MIM model to enhance the image understanding.
Dataset Instruct Response Overall
LLAVA 1.56 1 .84 1 .70
MINIGPT-4 0.00 1 .11 0 .89
MULTI INSTRUCT 0.51 1 .69 0 .51
PLATYPUS 0.98 0 .75 0 .78
SHIKRA 0.89 1 .08 0 .87
TEXTBIND 1.76 1 .92 1 .84
Table 2: Averaged diversity scores of roles in various
datasets. Details of this analysis are in Appx. D.
we incorporate a series of special tokens ( <img1> ,
...,<IMG{r}> ), instead of a single token ( <IMG> ), to
the LM. When <img1> is emitted, the generation of
the special token sequence is enforced, serving as
additional reasoning steps for predicting the forth-
coming image. Subsequently, the Q-Former only
accepts the hidden states of special tokens as input.
Language Description as Medium . The previ-
ous two variants try to align the continuous hidden
spaces of different models. An alternative is to
use discrete language descriptions for information
exchange, as depicted in Fig. 3. Specifically, we
add two special tokens, <start> and<end> , andencode the generated text between these two tokens
using the text encoder in the SD model.
The first two variants are optimized by minimiz-
ing the mean squared error (MSE) loss between the
output and SD embeddings. For the third variant,
we employ the standard cross-entropy loss. We em-
pirically find that only the last method demonstrates
satisfactory performance on multi-turn interleaved
multimodal instruction-following, for which we
name it MIM . The key difference between MIM
and GILL is that MIM uses discrete text as medium
rather than continuous embeddings. See §6.3 for
detailed comparisons.
5.2 Training
Our training process consists of two stages, namely,
the multimodal alignment stage and the multimodal
instruction tuning stage.
Multimodal Alignment The first stage aims to
align the feature spaces of the vision model and
the language model. We utilize massive image-
caption pairs for training, drawing from datasets
such as Conceptual Captions (Changpinyo et al.,

--- PAGE 7 ---
2021; Sharma et al., 2018) and SBU (Ordonez et al.,
2011). During training, only the Q-Former connect-
ing the vision and language models is optimized
while other model components remain frozen.
Multimodal Instruction Following The second
stage further trains the joint model on multimodal
instruction tuning data to improve its instruction-
following capabilities. The Q-Former model and
LLM are optimized in this stage. In addition to
TEXTBIND, we also explore existing multimodal
instruction data, including MultiInstruct (Xu et al.,
2023b), MiniGPT-4 (Zhu et al., 2023), LLaV A (Liu
et al., 2023b), and Shikra (Chen et al., 2023a).
6 Experiments
To verify the effectiveness of the proposed meth-
ods, we carry out quantitative evaluations against a
set of recent baselines. Our quantitative evaluations
are divided into three parts: textual response gener-
ation, image generation, and a holistic evaluation
of multimodal instruction-following.
6.1 T EXTBINDEVAL
To facilitate comprehensive and dedicated evalua-
tion for instruction-following in realistic scenarios,
we construct a new dataset named TEXTBINDE-
VAL.TEXTBINDEVAL is initially generated
through the automatic pipeline of TEXTBIND(§3)
and subsequently refined by human annotators.
These annotators are tasked with discarding low-
quality examples or rectifying amendable issues
such as revising incoherent or hallucinated content.
After a rigorous review, we establish an evaluation
dataset comprising 278 conversations in total.
6.2 Textual Response Generation
Setup We consider each assistant turn of each
conversation in TEXTBINDEVAL as a test point.
All its preceding context is treated as input (which
may contain interleaved images and text), and the
goal is to generate a coherent and helpful response.
We measure the response quality using a set of
reference-based evaluation metrics such as BLEU
(Papineni et al., 2002), ROUGE (Lin, 2004), and
BERTScore (Zhang et al., 2020). We also report the
Diversity (Su et al., 2022) scores of the generated
responses. For simplicity, we replace any image in
the responses with a special token <image> .
For a fair comparison, we compare different
MIM models trained on different datasets (Xu
et al., 2023b; Zhu et al., 2023; Liu et al., 2023b;Chen et al., 2023a)4and GILL (Koh et al., 2023a)5.
The implementation details are shown in Appx. H.
Results As shown in Tab. 3, the MIM model
trained on TEXTBINDoutperforms all other base-
lines by wide margins across all evaluation met-
rics. The results suggest that more realistic and
diverse training data such as TEXTBIND is nec-
essary for tackling open-world tasks, which can-
not be well-supported by existing template-based
and VQA-like datasets. Nevertheless, we also
find that the performance can be further improved
when combining different datasets (i.e., the mix-
ture of MultiInstruct, MiniGPT-4, LLaV A, Shikra,
andTEXTBIND, denoted by MIX), indicating that
there is a complementary relationship between
TEXTBINDand existing datasets.
Additional Results on Conventional Bench-
marks Finally, we report the results on three
popular multimodal benchmarks, MME (Fu et al.,
2023), MMBench (Liu et al., 2023c), and MM-
Vet (Yu et al., 2023). As shown in Tab. 4,
TEXTBINDgets a relatively lower score than other
datasets. The reason stems from the intrinsic differ-
ence between TEXTBINDand the two benchmarks.
TEXTBINDfocuses more on realistic instructions
(i.e., free-form text generation based on arbitrarily
interleaved images and text across diverse user-
assistant conversations). In contrast, MME, MM-
Bench and MM-Vet focus more on VQA questions
(e.g., who is this person, what is the color of the
object), which are more similar to the data in Mul-
tiInstruct, LLaV A, and Shikra. For example, the
model trained on MultiInstruct achieves the best
performance on MME, though it displays the worst
performance in open-world scenarios in Tab. 3.
Another interesting observation is that the mix of
all datasets ( MIX) attains the best overall perfor-
mance on MMBench and MM-Vet, indicating that
different datasets are complementary. This again
confirms that the capabilities that TEXTBINDcan
bring are almost orthogonal to existing multimodal
instruction-following datasets.
6.3 Image Generation
Setup The models trained on existing datasets,
i.e., the baselines in §6.2 except for GILL, are in-
4The original papers of these datasets used distinct model
architectures such as different pretrained language models.
Nevertheless, all of them do not support image generation.
5For a fair comparison, we replicate GILL using the same
image-captioning data to train by our models.

--- PAGE 8 ---
Methods BLEU-2 BLEU-4 ROUGE-2 ROUGE-L BERTScore Diversity
GILL (Koh et al., 2023a) 3.97 1.44 4.61 13.97 0.847 0.902
MultiInstruct (Xu et al., 2023b)37.16 2.27 3.16 10.60 0.830 0.654
MiniGPT-4 (Zhu et al., 2023) 9.24 3.29 6.77 17.56 0.858 0.658
LLaV A (Liu et al., 2023b) 12.16 4.41 8.66 19.79 0.872 0.852
Shikra (Chen et al., 2023a) 10.37 3.83 7.79 18.63 0.864 0.722
TEXTBIND 24.45 11.83 15.45 28.69 0.891 0.927
MIX 27.64 14.49 17.90 31.22 0.896 0.912
Table 3: Evaluation results of textual response generation on T EXTBINDEVAL.
Training DatasetMME MMBench MM-Vet
Perception Cognition Overall Overall
MultiInstruct (2023b) 1099.16 302.50 31.54 17.2
MiniGPT-4 (2023) 0.00 0.00 31.87 9.8
LLaV A (2023b) 683.28 267.86 42.10 23.4
Shikra (2023a) 166.87 2.86 41.10 19.9
TEXTBIND 549.00 226.43 22.64 19.4
MIX 1023.33 255.00 44.94 23.9
Table 4: Evaluation Results on MME (Fu et al., 2023),
MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023).
capable of generating images. To showcase the
image generation capabilities of our model, we
compare it with SDXL (Podell et al., 2023) and
GILL (Koh et al., 2023a). In addition, we present
the results of the two model variants described in
§5.1, namely, Q-former as Medium andQ-former
with Prompt Tokens as Medium .
We take each image from the assistant in
TEXTBINDEVAL as a test point. All its preced-
ing context is taken as input, and the models are
enforced to output an image. We take the original
images in TEXTBINDEVAL as references. Follow-
ing Koh et al. (2023a), we evaluate image genera-
tion with two reference-based metrics: (1) CLIP
Similarity . We use the CLIP vision encoder to
produce image representations and compute the co-
sine similarity between generated images and refer-
ence images. A higher score means better semantic
similarity. (2) Learned Perceptual Image Path
Similarity (LPIPS) . LPIPS (Zhang et al., 2018)
measures the distance between generated images
and reference images. A lower score means that
images are more similar in perceptual space. (3)
Frechet Inception Distance (FID). FID measures
the distributional difference between the generated
images and reference images. A lower score indi-
cates better resemblance to reference images.
Results To gain further insights into the multi-
turn instruction-following abilities, we group differ-
ent test points by the number of previous conversa-
tion turns. The results are shown in Tab. 5. As seen,MIM generally achieves better performance than
SDXL and GILL across different turns and evalua-
tion metrics. Importantly, the performance gaps are
enlarged as the number of turns increases. This in-
dicates that our model exhibits a better understand-
ing ability of multi-turn conversations. Compared
to the two model variants, MIM is substantially
better. Our case study reveals that the disparity
stems from the one-to-many nature of image gener-
ation in real-world conversations. Unlike generat-
ing images for explicit descriptions, there can exist
numerous distinct images for a given conversation
context. Operating in the hidden space may inad-
vertently average all possibilities, resulting in am-
biguous or noisy images. However, MIM mitigates
theone-to-many issue by taking full advantage of
the autoregressive generation of language models
for decision-making.
6.4 Holistic Evaluation
In addition to the above automatic evaluation, we
also conduct a holistic evaluation of instruction-
following abilities through human annotation.
Setup We randomly sample 100 contexts from
TEXTBINDEVAL and evaluate the responses gen-
erated by MIM and two representative baselines,
LLaV A (Liu et al., 2023b) and GILL (Koh et al.,
2023a). We instruct three human annotators to
score the quality of each generated response on a
Likert scale from 1 to 4 (major error, minor error,
acceptable, and excellent). The details of evalua-
tion guideline are in Appx. I. For inter-annotator
agreement, the Krippendorff’s α= 0.75, which
indicates a high consistency across annotators.
Results As depicted in Table 6, MIM achieves
substantially higher human scores than GILL and
LLaV A, in terms of both average score and accep-
tance rate, indicating its remarkable superiority in
open-world multimodal conversations. To further
elucidate where the derived dataset and training
helps, we ask human annotators to evaluate the

--- PAGE 9 ---
ModelCLIP Similarity (↑) LPIPS (↓) FID (↓)
Turn-1 Turn-2 Turn-3 Turn-1 Turn-2 Turn-3 All
SDXL (Podell et al., 2023) 0.612 0.599 0.608 0.712 0.735 0.735 144.76
GILL (Koh et al., 2023a) 0.569 0.550 0.530 0.712 0.734 0.742 158.64
Q-Former as Medium 0.558 0.568 0.592 0.717 0.728 0.729 155.01
Q-Former with Prompt Tokens as Medium 0.566 0.571 0.606 0.718 0.727 0.732 152.23
MIM (Language Description as Medium) 0.640 0.645 0.673 0.712 0.720 0.726 139.46
Table 5: Evaluation results of image generation on T EXTBINDEVAL.
Methods Overall Acc. rate IR CU RI
GILL 1.71 19% - - -
LLaV A 2.93 70% 3.59 3.56 3.78
MIM 3.39 89% 3.99 3.82 3.72
Table 6: Human evaluation results. Acc. rate denotes
the percentage of cases with an average score ≥3. IR,
CU, and RI represent intent recognition, context under-
standing and response informativeness, respectively.
quality of the generated responses across three fine-
grained dimensions: intent recognition, context
understanding and response informativeness. As
seen, MIM is more adept at following human in-
structions and leveraging the multimodal context,
while the informativeness scores of models trained
on T EXTBINDand LLaV A remain comparable.
7 Conclusion
In conclusion, the introduction of the TEXTBIND
framework has opened new doors for enhancing
large language models with multi-turn interleaved
multimodal instruction-following capabilities. By
requiring only image-caption pairs, our approach
significantly reduces the need for high-quality ex-
emplar data, making it a more accessible and scal-
able solution for various real-world tasks. The
MIM architecture seamlessly integrates image en-
coder and decoder models, enabling the model to
effectively handle interleaved image-text inputs and
outputs. Comprehensive quantitative and qualita-
tive experiments demonstrate the remarkable per-
formance of MIM , trained on TEXTBIND, when
compared to recent baselines in open-world multi-
modal conversations.
Limitations
Despite the amazing instruction-following capa-
bilities demonstrated by our models trained on
TEXTBIND, there are many avenues for further
improvement.1.Our best-performing models use textual de-
scriptions as a medium for image generation.
Nevertheless, it is impossible to describe ev-
erything in an image through words. Thus,
our current models have difficulties complet-
ing tasks such as precise image editing. In-
vestigating fine-grained multimodal feature
combinations may enhance performance in
these areas.
2.The multimodal capabilities of TEXTBIND
can be further enriched by incorporating ad-
ditional modalities. For example, future work
may use audio-text pairs for data synthesis.
3.Similar to existing multimodal language mod-
els, models trained on TEXTBIND exhibit
some common deficiencies such as halluci-
nation, toxicity, and stereotypes.
References
Harsh Agrawal, Peter Anderson, Karan Desai, Yufei
Wang, Xinlei Chen, Rishabh Jain, Mark Johnson,
Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
nocaps: novel object captioning at scale. In 2019
IEEE/CVF International Conference on Computer
Vision, ICCV 2019, Seoul, Korea (South), October 27
- November 2, 2019 .
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.

--- PAGE 10 ---
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Soravit Changpinyo, Piyush Sharma, Nan Ding, and
Radu Soricut. 2021. Conceptual 12m: Pushing web-
scale image-text pre-training to recognize long-tail
visual concepts. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2021, virtual,
June 19-25, 2021 .
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. 2023a. Shikra: Unleashing
multimodal llm’s referential dialogue magic. ArXiv
preprint , abs/2306.15195.
Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang,
Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Zi-
wei Liu. 2023b. Language models are visual reason-
ing coordinators. In ICLR 2023 Workshop on Mathe-
matical and Empirical Understanding of Foundation
Models .
Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jian-
feng Gao, and Chunyuan Li. 2023c. Llava-
interactive: An all-in-one demo for image chat, seg-
mentation, generation and editing. arXiv preprint
arXiv:2311.00571 .
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and
C Lawrence Zitnick. 2015. Microsoft coco captions:
Data collection and evaluation server. ArXiv preprint ,
abs/1504.00325.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. 2023. In-
structblip: Towards general-purpose vision-language
models with instruction tuning. ArXiv preprint ,
abs/2305.06500.
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,
Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,
Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xi-
angyu Zhang, Kaisheng Ma, and Li Yi. 2023. Dream-
llm: Synergistic multimodal comprehension and cre-
ation. ArXiv preprint , abs/2309.11499.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) .
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-
rui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-
grong Ji. 2023. Mme: A comprehensive evaluation
benchmark for multimodal large language models.
ArXiv preprint , abs/2306.13394.Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,
Xintao Wang, and Ying Shan. 2023. Making llama
see and draw with seed tokenizer. ArXiv preprint ,
abs/2310.01218.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. 2017. Making the V in VQA
matter: Elevating the role of image understanding in
visual question answering. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017 .
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo,
Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.
Bigham. 2018. Vizwiz grand challenge: Answering
visual questions from blind people. In 2018 IEEE
Conference on Computer Vision and Pattern Recog-
nition, CVPR 2018, Salt Lake City, UT, USA, June
18-22, 2018 .
Drew A. Hudson and Christopher D. Manning. 2019.
GQA: A new dataset for real-world visual reason-
ing and compositional question answering. In IEEE
Conference on Computer Vision and Pattern Recogni-
tion, CVPR 2019, Long Beach, CA, USA, June 16-20,
2019 .
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
Billion-scale similarity search with GPUs. IEEE
Transactions on Big Data , 7(3).
Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.
2023a. Generating images with multimodal language
models. ArXiv preprint , abs/2305.17216.
Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
2023b. Grounding language models to images for
multimodal inputs and outputs. ICML .
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. 2023a. Otter: A
multi-modal model with in-context instruction tuning.
ArXiv preprint , abs/2305.03726.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023b. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. ArXiv preprint , abs/2301.12597.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.
Hoi. 2022. BLIP: bootstrapping language-image pre-
training for unified vision-language understanding
and generation. In International Conference on Ma-
chine Learning, ICML 2022, 17-23 July 2022, Balti-
more, Maryland, USA , volume 162 of Proceedings
of Machine Learning Research .
Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi
Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,
Jingjing Xu, Xu Sun, et al. 2023c. M3it: A large-
scale dataset towards multi-modal multilingual in-
struction tuning. ArXiv preprint , abs/2306.04387.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out .

--- PAGE 11 ---
Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. 2023a. Mitigating hal-
lucination in large multi-modal models via robust
instruction tuning. arXiv preprint arXiv:2306.14565 ,
1.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023b. Visual instruction tuning. ArXiv
preprint , abs/2304.08485.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2023c. Mm-
bench: Is your multi-modal model an all-around
player? ArXiv preprint , abs/2307.06281.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. 2022. Learn to explain:
Multimodal reasoning via thought chains for science
question answering. Advances in Neural Information
Processing Systems , 35.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
and Roozbeh Mottaghi. 2019. OK-VQA: A visual
question answering benchmark requiring external
knowledge. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2019, Long Beach,
CA, USA, June 16-20, 2019 .
David Marr. 2010. Vision: A computational investiga-
tion into the human representation and processing of
visual information .
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin,
Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-
Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue
Liu, et al. 2023. Anymal: An efficient and scal-
able any-modality augmented language model. ArXiv
preprint , abs/2309.16058.
OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report. ArXiv preprint ,
abs/2303.08774.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Advances in Neural In-
formation Processing Systems 24: 25th Annual Con-
ference on Neural Information Processing Systems
2011. Proceedings of a meeting held 12-14 December
2011, Granada, Spain .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35.
Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,
Wenhu Chen, and Furu Wei. 2023. Kosmos-g: Gen-
erating images in context with multimodal large lan-
guage models. ArXiv preprint , abs/2310.02992.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics .
Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna,
and Robin Rombach. 2023. Sdxl: Improving latent
diffusion models for high-resolution image synthesis.
ArXiv preprint , abs/2307.01952.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural language
supervision. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event , volume 139 of Proceedings
of Machine Learning Research .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,
and Yuxiong He. 2020. Deepspeed: System opti-
mizations enable training deep learning models with
over 100 billion parameters. In KDD ’20: The 26th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, Virtual Event, CA, USA, August
23-27, 2020 .
Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. 2022. High-
resolution image synthesis with latent diffusion mod-
els. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition .
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault Févry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M. Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In The Tenth International Conference on
Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022 .
Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic im-
age captioning. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) .
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
huggingface. ArXiv preprint , abs/2303.17580.

--- PAGE 12 ---
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards VQA mod-
els that can read. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, June 16-20, 2019 .
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation. In Advances
in Neural Information Processing Systems .
Qingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yam-
ing Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo
Geng, and Daxin Jiang. 2022. Multimodal dialogue
response generation. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) .
Dídac Surís, Sachit Menon, and Carl V ondrick. 2023.
Vipergpt: Visual inference via python execution for
reasoning. ArXiv preprint , abs/2303.08128.
InternLM Team. 2023. Internlm: A multilin-
gual language model with progressively enhanced
capabilities. https://github.com/InternLM/
InternLM-techreport .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. ArXiv preprint ,
abs/2302.13971.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) .
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V . Le. 2022. Finetuned
language models are zero-shot learners. In The Tenth
International Conference on Learning Representa-
tions, ICLR 2022, Virtual Event, April 25-29, 2022 .
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language processing.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations .
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong
Wang, Zecheng Tang, and Nan Duan. 2023a. Visual
chatgpt: Talking, drawing and editing with visual
foundation models. ArXiv preprint , abs/2303.04671.Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and
Tat-Seng Chua. 2023b. Next-gpt: Any-to-any multi-
modal llm.
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao,
Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang,
Yu Qiao, and Ping Luo. 2023a. Lvlm-ehub: A com-
prehensive evaluation benchmark for large vision-
language models. ArXiv preprint , abs/2306.09265.
Zhiyang Xu, Ying Shen, and Lifu Huang. 2023b. Multi-
Instruct: Improving multi-modal zero-shot learning
via instruction tuning. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) .
Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,
Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-
react: Prompting chatgpt for multimodal reasoning
and action. ArXiv preprint , abs/2303.11381.
Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang,
Heyang Qin, Olatunji Ruwase, Ammar Ahmad
Awan, Samyam Rajbhandari, and Yuxiong He. 2023.
DeepSpeed-VisualChat: Multi-Round Multi-Image
Interleave Chat via Multi-Modal Causal Attention.
ArXiv preprint , abs/2309.14327.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming
Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,
Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl:
Modularization empowers large language models
with multimodality. ArXiv preprint , abs/2304.14178.
Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to visual
denotations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of the
Association for Computational Linguistics , 2.
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan
Wang. 2023. Mm-vet: Evaluating large multimodal
models for integrated capabilities. ArXiv preprint ,
abs/2308.02490.
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and
Yu Qiao. 2023. Llama-adapter: Efficient fine-tuning
of language models with zero-init attention. ArXiv
preprint , abs/2303.16199.
Richard Zhang, Phillip Isola, Alexei A. Efros, Eli
Shechtman, and Oliver Wang. 2018. The unreason-
able effectiveness of deep features as a perceptual
metric. In 2018 IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2018, Salt Lake
City, UT, USA, June 18-22, 2018 .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-
ating text generation with BERT. In 8th International
Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020 .

--- PAGE 13 ---
Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023.
Minigpt-5: Interleaved vision-and-language genera-
tion via generative vokens.
Luowei Zhou, Chenliang Xu, and Jason J. Corso. 2018.
Towards automatic learning of procedures from web
instructional videos. In Proceedings of the Thirty-
Second AAAI Conference on Artificial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ficial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artificial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018 .
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing
vision-language understanding with advanced large
language models. ArXiv preprint , abs/2304.10592.
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie
Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jian-
feng Wang, Lu Yuan, et al. 2022. Generalized decod-
ing for pixel, image, and language. ArXiv preprint ,
abs/2212.11270.

--- PAGE 14 ---
A Demonstrations
The four high-level characteristics of the
TEXTBINDdataset equips MIM with a variety of
capabilities. We demonstrate those capabilities
with concrete user cases.
Image Creation One core innovation of
TEXTBINDis that it enables the model to create
images based on the conversation context without
explicit dictations from the users. This characteris-
tic is extremely useful for open-world scenarios,
because in many cases people may just have an
implicit intention and have no clear thoughts about
what the images should be. We observe that our
model can explain concepts and ideas for users
with vivid images (Figure 5a), creating images
with correct emotions (Figure 5b), and editing
images based on the whole context (Figure 5c
and 5d). Furthermore, as shown in Figure 4, we
discover that our model is proficient in generating
long stories featuring interleaved text and images
while maintaining exceptional coherence.
Image Comparison Another interesting feature
ofTEXTBINDis that it can compare or relate the
information in multiple images. For example, our
model can correctly explain the different and com-
mon parts in multiple images in Figure 6.
Intrinsic & Extrinsic Image Understanding
The model trained on TEXTBINDcan understand
the content in images precisely in a multi-turn con-
versation. In all the three sub-figures of Figure
7, the model precisely follows the human instruc-
tions and explains the details of the images to users.
Moreover, TEXTBINDalso enables the model to
explore the meaning of an image beyond the sym-
bols in it. For example, the model also explains the
the influence of Bob Dylan’s album in Figure 7b
and the impact of iPhone in Figure 7c.
B Prompt of T EXTBIND
The prompt is shown in Fig. 8.
C Examples of Constructed
Conversations
The examples are given in Fig. 9.
D Comparison with previous datasets.
We further investigate the lexical diversity of gener-
ated data, including both instructions and responses.
The diversity of text in a dataset is defined asP4
n=2(#Unique n-grams
#Total n-grams), in line with previous works
(Su et al., 2022). As shown in Tab. 2, our dataset
achieves better diversity scores than most existing
visual-language datasets, indicating that the lan-
guage used in our dataset is more informative.
E Human annotation guideline.
The comprehensive guideline for human evaluation
is shown in Table 8.
F Implementation Details (Data)
We construct our TEXTBINDdataset based on the
CONCEPTUAL CAPTIONS 3M(CC3M) (Sharma
et al., 2018; Changpinyo et al., 2021) dataset, which
only provides image-caption pairs. In our exper-
iments, we employ the clip-vit-base-patch16
model released by OpenAI6(Radford et al., 2021)
to filter out image-caption pairs with matching
scores lower than 30. We use the k-means clus-
tering algorithm implemented by FAISS (Johnson
et al., 2019) toolkit to classify the cleaned CC3M
dataset into 4096 clusters. The features used for
k-means clustering are the hidden representations
of images encoded by clip-vit-base-patch16
model. In addition, clusters with less than 32im-
ages are regarded as outliers and will not be consid-
ered. The number of images in each conversation
is sampled from {2,3,4}. We access the GPT-4
model through the OpenAI API7, and set top_p
andtemperature hyper-parameters to 1.0.
GConstructed Conversations with “Poor”
Label
In Table 9, we identify three typical errors present
in the constructed dataset. Despite setting a high
threshold to filter out mismatched image-caption
pairs, some mismatched cases cannot be detected
by the CLIP model (Radford et al., 2021). A few
conversations suffering from incoherence and hal-
lucinations may be attributed to the GPT-4 model.
Overall, while a small number of conversations
are affected by errors that are difficult to detect
using rules, most generated conversations exhibit
high quality. We present several cases labeled with
“Poor”. We can find that most of those “Poor” cases
only have minor and non-obvious problems.

--- PAGE 15 ---
123Figure 4: Generation of a long story with interleaved text and images.
Dataset Multi-turnArbitrarily
InterleavedMultimodalInstruction
FollowingImage
Generation
PLATYPUS ✗ ✗ ✗ ✓ ✗
FLAMINGO ✗ ✓ ✓ ✗ ✓
MULTI INSTRUCT ✗ ✗ ✓ ✓ ✗
MINIGPT-4 ✗ ✗ ✓ ✓ ✗
LLAVA ✓ ✗ ✓ ✓ ✗
TEXTBIND ✓ ✓ ✓ ✓ ✓
Table 7: Comparison with previous datasets.

--- PAGE 16 ---
(a) Explaining concepts with multiple images.
 (b) Creating images with correct emotions.
(c) Editing images based on context.
 (d) Creating images based on context.
Figure 5: User cases of creating images.

--- PAGE 17 ---
(a) Comparing music styles.
 (b) Relating images.
(c) Comparing movies.
 (d) Comparing different concepts.
Figure 6: User cases of comparing images.

--- PAGE 18 ---
(a)
 (b)
(c)
 (d)
Figure 7: User cases of understanding both intrinsic & extrinsic information in the images.

--- PAGE 19 ---
GPT-4 Prompt
Please construct a dialogue between a human and a helpful, honest and harmless assistant.
The dialogue contains interleaved text and images. Each image is represented by <imgX >
DESCRIPTION </imgX >, where DESCRIPTION is a textual description of the image and X is
an index of the image. Please do not assume any further visual information beyond the description.
The constructed dialogues must and can only contain the following input images:
<img0>museum - the 1st nuclear submarine </img0 >
<img1>response to the production of heavy </img1 >
Characteristics about the assistant:
1. The assistant is trained to understand text, images, and their combinations.
2. The assistant can reply to the human with images and/or text.
3. The assistant has exceptional world knowledge and commonsense reasoning capabilities.
4. The assistant does not have access to the Internet or any other external tools.
5. If the assistant is asked to create an image, it can only show the image in the provided image list.
6. Please do not copy the images appearing in the dialogue. The assistant should refer to the
previously mentioned image by natural language.
Characteristics about the human:
1. The human may send images and/or text to the assistant.
2. The human may ask questions requiring visual reasoning and/or understanding the relations
between multiple images.
3. The human may ask the assistant to show images based on his/her intention.
4. The human may ask the assistant to do interesting things, rather than simply describing the
content of the image.
Properties of a bad dialogue:
1. Simply describing or analyzing the content in the image.
2. Dialogue without a good logic.
Properties of a good dialogue:
1. Introducing extrinsic and attractive information of stuff shown in the image.
2. Discovering the connection between multiple images.
3. The dialogue happens in a reasonable context.
Examples:
========
Case 1:
Input Images: . . .
Output Dialogue: . . .
———
Case 2:
Input Images: . . .
Output Dialogue: . . .
———
Case 3:
Input Images: . . .
Output Dialogue: . . .
======
Please directly give the dialogue if you understand. The number of turns of the dialogue should
be less than 6. The dialogue should be self-contained. Do NOT assume any previous dialogue
between the human and the assistant. Please use the same format <imgX >DESCRIPTION
</imgX >to denote images in the dialogue and do not modify the description of the image. Note
that the dialogue can stop at any time without an explicit ending.
Figure 8: Illustration of the prompt used for T EXTBIND.

--- PAGE 20 ---
GPT-4 Ouput
Example #1
Human : Hi! I’m trying to write a cartoon story for my kids featuring a grumpy clown and a
lovable cupcake. Can you help me craft this?
Assistant : Of course! That sounds like a wonderful idea. We can start by establishing
our characters. The grumpy clown could be someone who struggles finding joy in his own
performances. <img0>a cartoon illustration of a clown looking angry </img0 >
Human : That’s a wonderful idea! How about the cute cupcake ?
Assistant : The cupcake, on the other hand, is always upbeat and tries to help the clown find his
happiness. Despite being a tiny cupcake, it has a big heart and a contagious joy. <img1>cartoon
illustration of a cupcake with a happy expression </img1 >
Human : Interesting! What should be their adventure?
Assistant : They could go on a journey to find the ultimate joke that guarantees laughter. The
lovable cupcake, with his relentless positivity and cheer, helps the grumpy clown see the lighter
side of life and finally find his joy.
Example #2
Human : Hi, I am researching small business operations. Can you show me an example of
informal small business?
Assistant : Of course. Here’s an example of informal small business: <img0>a mother and
daughter selling gum and cigarettes in person </img0 >. This concept of business is most
commonly found in rural or underdeveloped areas where formal establishments might not be as
prevalent.
Human : Interesting! On a somewhat related note, what sells well in such arrangements?
Assistant : It depends on the location and the demand. Mostly, such businesses sell products of
daily use such as food items, candy, toiletries etc. For example an ice cream cart is a common
sight in many areas, as it’s a popular treat among people of all ages.
Human : Can you show me an image of a kid indulging in something like that?
Assistant : Absolutely! Here’s a delightful image of <img1>a child eating ice cream </img1 >.
Ice cream and similar snacks are often popular products for small informal businesses.
Example #3
Human : Hi there! I’m preparing a speech on unity and cooperation in global issues. I have a few
images but I’m not sure how to describe them in a way that fits my theme.
Assistant : I’d be happy to help with that! Could you show me the images?
Human : Sure, here they are. <img0>large group of people in the shape of flag </img0 >and
<img1>rear view of a male boxer holding globe with flag painted on his back </img1 >
Assistant : The first image can be seen as a symbol of unity. Here, numerous individuals come
together to form the shape of a flag, showing that collective action can lead to a powerful represen-
tation, like a flag - often a symbol of identity and unity. The second image illustrates the idea of
global responsibility and commitment. The male boxer, suggesting strength and determination,
has a globe – representing the world – in his hands, which could indicate a keen sense of global
responsibility. The flag painted on his back can symbolize allegiance or commitment to his country,
and by extension to global harmony if we’re expanding the theme to encompass international
cooperation.
Human : That sounds great! I also have this image. Not sure if it fits. <img2>diplomatic
handshake between countries : flags overprinted the hands stock photo </img2 >
Assistant : This image very much fits your theme. It can represent a formal agreement or cohesion
between countries. The flags overprinted on the shaking hands signify the countries involved in
the meet. This image can be used to illustrate diplomatic dialogue or mutual understanding which
is an integral component to solving global issues through cooperation and unity.
Figure 9: Illustration of in-context examples of T EXTBIND.

--- PAGE 21 ---
It'ssobeautifuloutside!Lookatthescenery:
Iwanttogooutsideandhavesomefun.Doyouhaveanysuggestions?
YesTheimageshowsasnowylandscapewithapondintheforegroundandapagodainthebackground.Thepondissurroundedbysnow-coveredtreesandrocks,andthereisapathleadingtothepagoda.Theskyisclearandblue,withsomefluffycloudsinthedistance.Theoverallmoodoftheimageispeacefulandserene.
Sure!Youcouldtryplayingagameofsnowballfightwithyourfriends.It'safunandexcitingwaytoenjoythesnow.Here'sanimageofasnowballfight:MiniGPT-4MultiInstruct
TextBind(ours)
Recently, I've been really interested in them:
Can You create a fun story based on these two characters?
YesTheimageshowsamaninaredandblackcostumestandingonarooftopwithacityinthebackground.Themaniswearingamaskandhasaseriousexpressiononhisface.Thecityislitupbythesun,withbuildingsandstreetsvisibleinthebackground.Themanisholdingaswordinhishandandappearstobereadytofight.
Sure!Onceuponatime,inaworldofsuperheroes,therewasalegendaryheronamedSpiderman.TheSpidermanwasknownforhisincrediblestrength,agility,andhisabilitytoclimbwallsandceilings.Hewasatruehero,alwaysreadytosavetheday.Oneday,whileSpidermanwaspatrollingthecity,hestumbleduponamysteriousfigure.Thefigurewasapowerfulsorcerer,whohadtheabilitytocontroltheelements.ThesorcererwasknownastheMonkeyKing.TheMonkeyKingwasamischievouscharacter,alwayscausingtroubleandchaoswhereverhewent.Hehadapetmonkey,whowasalsohisloyalcompanion.TheMonkeyKingandSpidermanhadalonghistoryofbattles.Theywerealwaysatodds,withtheMonkeyKingtryingtooutsmartSpidermanandcausechaosinthecity.Oneday …
Giventhebeautifulsnowyscenery,thereareplentyoffunactivitiesyoucanenjoyoutside.Somesuggestionsinclude:1.Goingforawalkorajogalongtheriver,takingintheserenewinterlandscape.2.Buildingasnowmanorsnowfortinthesnow-coveredpark.3. Havingasnowballfightwithfriendsorfamily.4. Takingaleisurelyskateonthefrozenriverorpond,enjoyingthecrispwinterair.5.Exploringthesnow-coveredgarden,admiringthewinter-adaptedplantsandtrees…LLaVA
Onceuponatime,inabustlingcityfilledwithskyscrapers,ayoungmandressedinaredandblackcostume,resemblingamonkey,wasperchedontopofabuilding.Hewasasuperhero,knownastheMonkeyMan,andhehadjustsavedthecityfromanevilvillain.Asherestedontherooftop,henoticedagroupofpeoplegatheredbelow,admiringhisheroicfeat.TheMonkeyManfeltasenseofprideandsatisfaction,knowingthathisactionshadmadeadifferenceinthelivesofthepeopleheprotected. Suddenly,anewchallengeemergedasagroupofthievessnuckintothecity,attemptingtostealvaluableitemsfromtheresidents.TheMonkeyMansprangintoaction,usinghisincrediblestrengthandagilitytochasethethievesandbringthemtojustice …
User
Figure 10: Comparison among models trained on different datasets.
Annotation Labels Description
Overall QualityExcellent This conversation is very interesting, practical, or intricate.
Satisfactory This conversation is coherent and reasonable without any factual errors.
Poor At least one turn in the conversation is unreasonable in some aspects, e.g.,
unrealistic content, illegal formats, etc.
Required AbilitiesImage Creation To create new images in appropriate contexts.
Image Comparison To combine, relate, or compare the information in different images.
Intrinsic Image Understanding To identify and recognize the objects, colors, shapes, and patterns in
images.
Extrinsic Image Understanding To interpret the underlying meaning of images, e.g., the context, emotions,
symbolism, or narrative conveyed by the images. It goes beyond merely
recognizing the elements in the images and often requires external knowl-
edge and/or deep analysis.
Table 8: Human annotation guideline.
Error Type Percentage
Img-Cap Mismatch 0.03
Incoherence 0.03
Hallucination 0.03
Table 9: Error types and percentage in dataset con-
structed by T EXTBIND.H Implementation Details (Model)
Our experiments are based on Huggingface Trans-
formers8(Wolf et al., 2020) and DeepSpeed9
6https://huggingface.co/openai/
clip-vit-base-patch16
7https://openai.com/blog/openai-api
8https://huggingface.co/docs/transformers/
index
9https://github.com/microsoft/DeepSpeed

--- PAGE 22 ---
(a) Hallucination Case: The topic is about rain boot.
However, not all the boots shown in the second image
are rain boots.
(b) Incoherence Case: The first turn discusses about
relaxing while the second turn is about outdoor activi-
ties. There are no connections.
(c) Image-caption Mismatch Case: The first image only
shows a bathroom, but the caption is “double room with
private bathroom in a cottage”.
Figure 11: Constructed conversations with “Poor” Label. The caption is shown below the image with gray color.

--- PAGE 23 ---
Training Stage Epoch Learning Rate Batch Size Max Sequence Length Training Modules
Multimodel Alignment 2 1e-4 256 256 Q-Former, Linear
Multimodel Instruction Following 3 1e-5 64 768 Q-Former, Linear, LLM
Table 10: Training Configures of our Experiments
(Rasley et al., 2020). We use the filtered syn-
thetic captions given by BLIP10(Li et al., 2022),
including Conceptual Captions (Changpinyo et al.,
2021; Sharma et al., 2018) and SBU (Ordonez et al.,
2011), totally 12M image-caption pairs. We em-
ploy the same visual encoder and Q-Former as used
in BLIP-2 (Li et al., 2023b) and use their weights
for initialization. LLama2-Chat11(Touvron et al.,
2023) is utilized as the backbone language model.
For the image generation model, we use Stable
Diffusion XL12(Podell et al., 2023). The train-
ing configures are shown in Table 10. We use 8
NVIDIA A100 (40G) GPUs for all experiments.
I Human Evaluation Guideline
For overall quality assessment, we adopt the fol-
lowing criteria.
• Score 4: The response is excellent.
•Score 3: The response is acceptable but may
not be very informative and interesting.
•Score 2: The response has minor issues, such
as slight hallucinations when describing the
images in the context.
•Score 1: The response is invalid and has signif-
icant drawbacks, e.g., irrelevant to the context.
For fine-grained annotations, intent recognition
evaluates whether the response fulfills the the users’
intent, context understanding examines whether the
response correctly comprehends the information in
text and images, and the response informativeness
measures the usefulness of the response. For each
dimension, a human annotator will assign a score
in{1,2,3,4}. The four scores ranging from 1 to 4
indicate “major error”, “minor error”, “acceptable”,
and “perfect”, respectively.
J Data Format of Model Input
The data format of model input is given in Fig. 12.
10https://github.com/salesforce/BLIP
11https://huggingface.co/meta-llama/
Llama-2-7b-chat-hf
12https://github.com/Stability-AI/
generative-modelsK T EXTBINDUsage
TEXTBINDis completely open-source and can be
used for academic research and commercial pur-
poses.

--- PAGE 24 ---
1{
2 " conversation " {
3 {
4 " role ": " user ",
5 " content ": "<image > Do you ... ",
6 " image_list ": [" 1235. png "]
7 " caption_list ": ["a runing husky ... "]
8 },
9 {
10 " role ": " assistant ",
11 " content ": "Yes , I do! ... ",
12 " image_list ": [],
13 " caption_list ": []
14 },
15 ...
16 }
17}
Figure 12: Data Format of Model Input.
