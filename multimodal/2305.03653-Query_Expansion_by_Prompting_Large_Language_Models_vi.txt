# Mở rộng truy vấn bằng cách nhắc nhở các mô hình ngôn ngữ lớn

Rolf Jagerman
Google Research
jagerman@google.com

Honglei Zhuang
Google Research
hlz@google.com

Zhen Qin
Google Research
zhenqin@google.com

Xuanhui Wang
Google Research
xuanhui@google.com

Michael Bendersky
Google Research
bemike@google.com

TÓM TẮT
Mở rộng truy vấn là một kỹ thuật được sử dụng rộng rãi để cải thiện khả năng thu hồi của các hệ thống tìm kiếm. Trong bài báo này, chúng tôi đề xuất một phương pháp mở rộng truy vấn tận dụng khả năng sinh của các Mô hình Ngôn ngữ Lớn (LLM). Không giống như các phương pháp mở rộng truy vấn truyền thống như Phản hồi Giả Liên quan (PRF) dựa vào việc thu thập một tập hợp tốt các tài liệu giả liên quan để mở rộng truy vấn, chúng tôi dựa vào khả năng sinh và sáng tạo của LLM và tận dụng kiến thức vốn có trong mô hình. Chúng tôi nghiên cứu nhiều lời nhắc khác nhau, bao gồm zero-shot, few-shot và Chain-of-Thought (CoT). Chúng tôi thấy rằng lời nhắc CoT đặc biệt hữu ích cho mở rộng truy vấn vì những lời nhắc này hướng dẫn mô hình phân tích truy vấn theo từng bước và có thể cung cấp một số lượng lớn các thuật ngữ liên quan đến truy vấn gốc. Kết quả thực nghiệm trên MS-MARCO và BEIR cho thấy rằng việc mở rộng truy vấn được tạo ra bởi LLM có thể mạnh mẽ hơn các phương pháp mở rộng truy vấn truyền thống.

1 GIỚI THIỆU
Mở rộng truy vấn là một kỹ thuật được sử dụng rộng rãi để cải thiện khả năng thu hồi của các hệ thống tìm kiếm bằng cách thêm các thuật ngữ bổ sung vào truy vấn gốc. Truy vấn đã mở rộng có thể thu hồi được các tài liệu liên quan mà không có sự trùng lặp từ vựng với truy vấn gốc. Các phương pháp mở rộng truy vấn truyền thống thường dựa trên Phản hồi Giả Liên quan (PRF) [1,20,21,23], coi tập hợp các tài liệu thu được từ truy vấn gốc là "giả liên quan" và sử dụng nội dung của những tài liệu đó để trích xuất các thuật ngữ truy vấn mới. Tuy nhiên, các phương pháp dựa trên PRF giả định rằng các tài liệu được thu thập hàng đầu có liên quan đến truy vấn. Trong thực tế, các tài liệu được thu thập ban đầu có thể không hoàn toàn phù hợp với truy vấn gốc, đặc biệt nếu truy vấn ngắn hoặc mơ hồ. Do đó, các phương pháp dựa trên PRF có thể thất bại nếu tập hợp các tài liệu được thu thập ban đầu không đủ tốt.

Trong bài báo này, chúng tôi đề xuất việc sử dụng các Mô hình Ngôn ngữ Lớn (LLM) [3,8,19] để hỗ trợ mở rộng truy vấn. LLM đã thấy sự quan tâm ngày càng tăng trong cộng đồng Truy xuất Thông tin (IR) trong những năm gần đây. Chúng thể hiện một số tính chất, bao gồm khả năng trả lời câu hỏi và tạo văn bản, khiến chúng trở thành những công cụ mạnh mẽ. Chúng tôi đề xuất sử dụng những khả năng sinh đó để tạo ra các mở rộng truy vấn hữu ích. Cụ thể, chúng tôi nghiên cứu các cách để nhắc nhở LLM và yêu cầu nó tạo ra nhiều thuật ngữ thay thế và mới cho truy vấn gốc. Điều này có nghĩa là, thay vì dựa vào kiến thức trong các tài liệu PRF hoặc cơ sở dữ liệu kiến thức từ vựng, chúng tôi dựa vào kiến thức vốn có trong LLM. Một ví dụ về phương pháp đề xuất được trình bày trong Hình 1.

Các đóng góp chính của chúng tôi trong công trình này như sau: Thứ nhất, chúng tôi xây dựng các lời nhắc khác nhau để thực hiện mở rộng truy vấn (zero-shot, few-shot và CoT) có và không có PRF để nghiên cứu hiệu suất tương đối của chúng. Thứ hai, chúng tôi thấy rằng các lời nhắc Chain-of-Thought (CoT) hoạt động tốt nhất và đưa ra giả thuyết rằng điều này là do các lời nhắc CoT hướng dẫn mô hình phân tích câu trả lời theo từng bước bao gồm nhiều từ khóa có thể hỗ trợ trong mở rộng truy vấn. Cuối cùng, chúng tôi nghiên cứu hiệu suất trên các kích thước mô hình khác nhau để hiểu rõ hơn về khả năng thực tế và hạn chế của phương pháp LLM cho mở rộng truy vấn.

Trả lời câu hỏi sau:
{query}
Đưa ra lý luận trước khi trả lời

Mô hình Ngôn ngữ Lớn (LLM)
Concat({query}, {model output})
Hệ thống Truy xuất (BM25)

Hình 1: Tổng quan cao cấp về việc sử dụng lời nhắc Chain-of-Thought (CoT) zero-shot để tạo các thuật ngữ mở rộng truy vấn.

2 CÔNG TRÌNH LIÊN QUAN
Mở rộng truy vấn được nghiên cứu rộng rãi [4,11]. Về cốt lõi, mở rộng truy vấn giúp các hệ thống truy xuất bằng cách mở rộng các thuật ngữ truy vấn thành các thuật ngữ mới thể hiện cùng một khái niệm hoặc nhu cầu thông tin, tăng khả năng khớp từ vựng với các tài liệu trong tập dữ liệu. Các công trình đầu về mở rộng truy vấn tập trung vào việc sử dụng cơ sở dữ liệu kiến thức từ vựng [2,18,29] hoặc Phản hồi Giả Liên quan (PRF) [1,20,23]. Các phương pháp dựa trên PRF đặc biệt hữu ích trong thực tế vì chúng không cần xây dựng cơ sở dữ liệu kiến thức cụ thể theo lĩnh vực và có thể được áp dụng cho bất kỳ tập dữ liệu nào. Trực giao với mở rộng truy vấn là mở rộng tài liệu [10,16,25,33] áp dụng các kỹ thuật tương tự nhưng mở rộng các thuật ngữ tài liệu trong quá trình lập chỉ mục thay vì các thuật ngữ truy vấn trong quá trình truy xuất.

Các công trình gần đây về mở rộng truy vấn đã tận dụng mạng neural để tạo ra hoặc chọn lựa các thuật ngữ mở rộng [13,24,33,34], thường bằng cách huấn luyện hoặc tinh chỉnh một mô hình. Ngược lại, công trình của chúng tôi tận dụng các khả năng vốn có trong LLM đa mục đích mà không cần huấn luyện hoặc tinh chỉnh mô hình.

Chúng tôi lưu ý rằng công trình của chúng tôi tương tự như các công trình gần đây của [7] và [31]: tận dụng LLM để mở rộng truy vấn. Tuy nhiên, chúng tôi phân biệt công trình của mình theo một số cách quan trọng: Thứ nhất, chúng tôi nghiên cứu một số lời nhắc khác nhau trong khi [31] tập trung vào một lời nhắc few-shot duy nhất và [7] không nghiên cứu các lời nhắc. Thứ hai, không giống như [31] và [7], chúng tôi tập trung vào việc tạo ra các thuật ngữ mở rộng truy vấn thay vì toàn bộ tài liệu giả. Để làm điều này, chúng tôi chứng minh hiệu suất của các lời nhắc trên nhiều kích thước mô hình nhỏ hơn giúp hiểu cả hạn chế và khả năng thực tế của phương pháp LLM cho mở rộng truy vấn. Cuối cùng, chúng tôi thử nghiệm với các mô hình mã nguồn mở hoàn toàn, khuyến khích tính tái tạo và cởi mở của nghiên cứu, trong khi [31] thử nghiệm với một loại mô hình duy nhất chỉ có thể truy cập thông qua API bên thứ ba.

3 PHƯƠNG PHÁP
Chúng tôi xây dựng bài toán mở rộng truy vấn như sau: cho một truy vấn q, chúng tôi muốn tạo ra một truy vấn mở rộng q' chứa các thuật ngữ truy vấn bổ sung có thể giúp trong việc truy xuất các tài liệu liên quan. Cụ thể, chúng tôi nghiên cứu việc sử dụng LLM để mở rộng các thuật ngữ truy vấn và tạo ra một truy vấn mới q'. Vì đầu ra của LLM có thể dài dòng, chúng tôi lặp lại các thuật ngữ truy vấn gốc 5 lần để tăng tầm quan trọng tương đối của chúng. Đây là cùng một thủ thuật được sử dụng bởi [31]. Chính thức hơn:

q' = Concat(q, q, q, q, q, LLM(prompt_q)), (1)

trong đó Concat là toán tử nối chuỗi, q là truy vấn gốc, LLM là Mô hình Ngôn ngữ Lớn và prompt_q là lời nhắc được tạo dựa trên truy vấn (và có thể là thông tin phụ như các ví dụ few-shot hoặc tài liệu PRF).

Trong bài báo này, chúng tôi nghiên cứu tám lời nhắc khác nhau:

Q2D: Lời nhắc few-shot Query2Doc [31], yêu cầu mô hình viết một đoạn văn trả lời truy vấn.

Q2D/ZS: Phiên bản zero-shot của Q2D.

Q2D/PRF: Một lời nhắc zero-shot như Q2D/ZS nhưng cũng chứa ngữ cảnh bổ sung dưới dạng top-3 tài liệu PRF được truy xuất cho truy vấn.

Q2E: Tương tự như lời nhắc Query2Doc few-shot nhưng với các ví dụ về thuật ngữ mở rộng truy vấn thay vì tài liệu.

Q2E/ZS: Phiên bản zero-shot của Q2E.

Q2E/PRF: Một lời nhắc zero-shot như Q2E/ZS nhưng với ngữ cảnh bổ sung dưới dạng tài liệu PRF như Q2D/PRF.

CoT: Một lời nhắc Chain-of-Thought zero-shot hướng dẫn mô hình cung cấp lý luận cho câu trả lời của nó.

CoT/PRF: Một lời nhắc như CoT nhưng cũng chứa ngữ cảnh bổ sung dưới dạng top-3 tài liệu PRF được truy xuất cho truy vấn.

Các lời nhắc zero-shot (Q2D/ZS và Q2E/ZS) là đơn giản nhất vì chúng bao gồm một hướng dẫn văn bản đơn giản và truy vấn đầu vào. Các lời nhắc few-shot (Q2D và Q2E) bổ sung chứa một số ví dụ để hỗ trợ học trong ngữ cảnh, ví dụ chúng chứa các truy vấn và các mở rộng tương ứng. Các lời nhắc Chain-of-Thought (CoT) xây dựng hướng dẫn của chúng để có được đầu ra dài dòng hơn từ mô hình bằng cách yêu cầu nó phân tích phản hồi theo từng bước. Cuối cùng, các biến thể Phản hồi Giả Liên quan (·/PRF) của các lời nhắc sử dụng top-3 tài liệu được truy xuất làm ngữ cảnh bổ sung cho mô hình. Xem Phụ lục A để biết các lời nhắc chính xác được sử dụng trong các thí nghiệm.

4 THÍ NGHIỆM
Để xác nhận hiệu quả của việc mở rộng truy vấn dựa trên LLM, chúng tôi chạy thí nghiệm trên hai nhiệm vụ truy xuất: truy xuất đoạn văn MS-MARCO [15] và BEIR [27]. Đối với hệ thống truy xuất, chúng tôi sử dụng BM25 [21,22] như được triển khai bởi Terrier [17]. Chúng tôi sử dụng các tham số BM25 mặc định (b=0.75, k1=1.2, k3=8.0) được cung cấp bởi Terrier.

4.1 Baseline
Để phân tích các phương pháp mở rộng truy vấn dựa trên LLM, chúng tôi so sánh với một số phương pháp mở rộng truy vấn dựa trên PRF cổ điển [1]:
• Bo1: Trọng số Bose-Einstein 1
• Bo2: Trọng số Bose-Einstein 2
• KL: Trọng số Kullback-Leibler

Các triển khai cho những phương pháp này được cung cấp bởi Terrier. Trong tất cả các trường hợp, chúng tôi sử dụng cài đặt Terrier mặc định cho mở rộng truy vấn: 3 tài liệu PRF và 10 thuật ngữ mở rộng.

Hơn nữa, chúng tôi bao gồm lời nhắc từ Query2Doc [31] như một baseline. Tuy nhiên, chúng tôi không so sánh với thiết lập chính xác của họ vì họ sử dụng một mô hình lớn hơn đáng kể so với các mô hình chúng tôi nghiên cứu trong bài báo này. Các so sánh trong bài báo này tập trung vào các lời nhắc chứ không phải trên các con số chính xác được tạo ra bởi các mô hình khác nhau, có thể lớn hơn nhiều. Hơn nữa, đối với các mô hình có trường tiếp nhận nhỏ (cụ thể là các mô hình Flan-T5), chúng tôi chỉ sử dụng lời nhắc Q2D 3-shot thay vì lời nhắc 4-shot tiêu chuẩn để tránh lời nhắc bị cắt bớt.

4.2 Mô hình Ngôn ngữ
Chúng tôi so sánh các lời nhắc trên hai loại mô hình, Flan-T5 [6,19] và Flan-UL2 [26], với các kích thước mô hình khác nhau:
• Flan-T5-Small (60M tham số)
• Flan-T5-Base (220M tham số)
• Flan-T5-Large (770M tham số)
• Flan-T5-XL (3B tham số)
• Flan-T5-XXL (11B tham số)
• Flan-UL2 (20B tham số)

Chúng tôi chọn sử dụng các phiên bản Flan [6,32] của các mô hình T5 [19] và UL2 [26] vì chúng được tinh chỉnh để tuân theo hướng dẫn, điều này rất quan trọng khi sử dụng các phương pháp dựa trên lời nhắc. Hơn nữa, tất cả các mô hình này đều có sẵn như mã nguồn mở.

4.3 Chỉ số
Vì chúng tôi quan tâm đến mở rộng truy vấn, chủ yếu tập trung vào việc cải thiện khả năng thu hồi của truy xuất giai đoạn đầu, chúng tôi sử dụng Recall@1K làm chỉ số đánh giá cốt lõi. Chúng tôi cũng báo cáo các chỉ số xếp hạng top-heavy sử dụng MRR@10 [30] và NDCG@10 [14] để hiểu rõ hơn về cách các mô hình thay đổi các kết quả được truy xuất hàng đầu. Chúng tôi báo cáo tất cả kết quả với kiểm định ý nghĩa sử dụng paired t-test và coi một kết quả có ý nghĩa tại p<0.01.

5 KẾT QUẢ
5.1 Xếp hạng Đoạn văn MS-MARCO
Bảng 1 trình bày các kết quả trên nhiệm vụ xếp hạng đoạn văn MS-MARCO. Các baseline mở rộng truy vấn cổ điển (Bo1, Bo2 và KL) đã cung cấp một sự cải thiện hữu ích về Recall@1K so với việc truy xuất BM25 tiêu chuẩn. Phù hợp với kết quả của [12], chúng tôi quan sát thấy rằng sự gia tăng về khả năng thu hồi này đi kèm với chi phí của các chỉ số xếp hạng top-heavy như MRR@10 và NDCG@10.

Tiếp theo, chúng tôi thấy kết quả của việc mở rộng truy vấn dựa trên LLM phụ thuộc rất nhiều vào loại lời nhắc được sử dụng. Tương tự như các phát hiện của [31], lời nhắc Query2Doc (Q2D) có thể cung cấp một sự cải thiện đáng kể về Recall@1K so với các phương pháp cổ điển. Thú vị là, Query2Doc không chỉ cải thiện khả năng thu hồi mà còn cải thiện các chỉ số xếp hạng top-heavy như MRR@10 và NDCG@10, cung cấp một sự cải thiện tốt trên các chỉ số. Điều này tương phản với các phương pháp mở rộng truy vấn cổ điển thường hy sinh các chỉ số xếp hạng top-heavy để cải thiện khả năng thu hồi.

Cuối cùng, hiệu suất tốt nhất đạt được bởi CoT (và lời nhắc được tăng cường PRF tương ứng CoT/PRF). Lời nhắc cụ thể này hướng dẫn mô hình tạo ra một giải thích dài dòng bằng cách phân tích câu trả lời thành các bước. Chúng tôi đưa ra giả thuyết rằng tính dài dòng này có thể dẫn đến nhiều từ khóa tiềm năng hữu ích cho mở rộng truy vấn. Cuối cùng, chúng tôi thấy rằng việc thêm tài liệu PRF vào lời nhắc giúp ích đáng kể trong các chỉ số xếp hạng top-heavy như MRR@10 và NDCG@10 trên các mô hình và lời nhắc. Một lời giải thích có thể cho điều này là LLM hiệu quả trong việc chưng cất các tài liệu PRF, có thể đã chứa các đoạn văn liên quan, bằng cách chú ý đến các từ khóa hứa hẹn nhất và sử dụng chúng trong đầu ra. Chúng tôi cung cấp một ví dụ cụ thể hơn về đầu ra lời nhắc trong Phụ lục B.

5.2 BEIR
Các tập dữ liệu BEIR bao gồm nhiều nhiệm vụ truy xuất thông tin zero-shot khác nhau từ nhiều lĩnh vực. Chúng tôi so sánh hiệu suất của các lời nhắc khác nhau trên các tập dữ liệu BEIR trong Bảng 2. Điều đầu tiên quan sát ở đây là các baseline mở rộng truy vấn dựa trên PRF cổ điển vẫn hoạt động rất tốt, đặc biệt trên các tập dữ liệu cụ thể theo lĩnh vực như trec-covid, scidocs và touche2020. Các tập dữ liệu này chủ yếu có tính chất học thuật và khoa học, và các tài liệu PRF có thể cung cấp các thuật ngữ truy vấn hữu ích trong những trường hợp này. Ngược lại, các LLM đa mục đích có thể không có đủ kiến thức lĩnh vực để hữu ích cho các tập dữ liệu này. Thứ hai, chúng tôi lưu ý rằng các tập dữ liệu kiểu hỏi-đáp (fiqa, hotpotqa, msmarco và nq) dường như được hưởng lợi nhiều nhất từ phương pháp LLM cho mở rộng truy vấn. Có khả năng mô hình ngôn ngữ đang tạo ra các câu trả lời liên quan đến truy vấn giúp truy xuất các đoạn văn liên quan hiệu quả hơn. Trên tất cả các tập dữ liệu, lời nhắc Q2D/PRF tạo ra Recall@1K trung bình cao nhất, với lời nhắc CoT là gần thứ hai.

5.3 Tác động của Kích thước Mô hình
Để hiểu khả năng thực tế và hạn chế của bộ mở rộng truy vấn dựa trên LLM, chúng tôi so sánh các kích thước mô hình khác nhau trong Hình 2. Chúng tôi khảo sát kích thước mô hình từ 60M tham số (Flan-T5-small) lên đến 11B (Flan-T5-XXL) và cũng thử một mô hình 20B tham số (Flan-UL2) nhưng lưu ý rằng mô hình sau cũng có một mục tiêu tiền huấn luyện khác. Nói chung, chúng tôi quan sát xu hướng mong đợi rằng các mô hình lớn hơn có xu hướng hoạt động tốt hơn. Phương pháp Q2D yêu cầu ít nhất một mô hình 11B tham số để đạt được sự ngang bằng với baseline BM25+Bo1. Ngược lại, phương pháp CoT chỉ cần một mô hình 3B tham số để đạt được sự ngang bằng. Hơn nữa, việc thêm tài liệu PRF vào lời nhắc CoT dường như giúp ổn định hiệu suất cho các kích thước mô hình nhỏ hơn nhưng ngăn cản hiệu suất của nó ở các khả năng lớn hơn. Một lời giải thích có thể cho hành vi này là các tài liệu PRF làm giảm tính sáng tạo của mô hình, vì nó có thể tập trung quá nhiều vào các tài liệu được cung cấp. Mặc dù điều này giúp ngăn chặn mô hình mắc lỗi ở các kích thước mô hình nhỏ hơn, nó cũng ngăn cản các khả năng sáng tạo mà chúng tôi muốn tận dụng ở các kích thước mô hình lớn hơn. Lời nhắc CoT/PRF có thể vượt trội hơn các lời nhắc khác ở kích thước mô hình 770M tham số, khiến nó trở thành một ứng cử viên tốt cho việc triển khai có thể trong các cài đặt tìm kiếm thực tế nơi phục vụ một mô hình lớn hơn có thể là không thể.

Nhìn chung, rõ ràng rằng các mô hình lớn có thể cung cấp những cải thiện đáng kể có thể hạn chế việc ứng dụng thực tế của phương pháp LLM cho mở rộng truy vấn. Chưng cất đã được chứng minh là một cách hiệu quả để chuyển giao khả năng của một mô hình lớn sang một mô hình nhỏ hơn. Chúng tôi để việc nghiên cứu chưng cất các mô hình này cho mở rộng truy vấn như công việc tương lai.

6 HẠN CHẾ & CÔNG VIỆC TƯƠNG LAI
Có một số hạn chế trong công trình của chúng tôi: Thứ nhất, chúng tôi chỉ nghiên cứu truy xuất thưa thớt (BM25) là nơi mở rộng truy vấn quan trọng. Các hệ thống truy xuất dày đặc (ví dụ: dual encoder) ít dễ bị ảnh hưởng bởi khoảng cách từ vựng và do đó ít có khả năng được hưởng lợi từ mở rộng truy vấn. Wang et al. [31] đã nghiên cứu cài đặt này chi tiết hơn và chúng tôi để việc phân tích các lời nhắc của chúng tôi cho cài đặt truy xuất dày đặc như công việc tương lai. Thứ hai, công trình của chúng tôi tập trung vào các mô hình ngôn ngữ được tinh chỉnh hướng dẫn Flan [32]. Chúng tôi chọn các mô hình này do khả năng tuân theo hướng dẫn và thực tế là các mô hình này là mã nguồn mở. Công trình của chúng tôi có thể được mở rộng tự nhiên sang các mô hình ngôn ngữ khác [3,5,9,28] và chúng tôi để việc nghiên cứu các mô hình như vậy như một chủ đề cho nghiên cứu tương lai. Thứ ba, chúng tôi nghiên cứu các mẫu lời nhắc cụ thể (xem Phụ lục A) và có thể có những cách khác để xây dựng các lời nhắc khác nhau. Cuối cùng, chi phí tính toán của LLM có thể ngăn cản việc triển khai các mở rộng truy vấn dựa trên LLM trong thực tế. Có thể chưng cất đầu ra của mô hình lớn thành một mô hình có thể phục vụ nhỏ hơn. Cách sản xuất hóa các mở rộng truy vấn dựa trên LLM được để lại như một bài toán mở.

7 KẾT LUẬN
Trong bài báo này, chúng tôi nghiên cứu các mở rộng truy vấn dựa trên LLM. Trái ngược với mở rộng truy vấn dựa trên PRF truyền thống, LLM không bị hạn chế bởi tập hợp tài liệu được truy xuất ban đầu và có thể tạo ra các thuật ngữ mở rộng không được bao phủ bởi các phương pháp truyền thống. Phương pháp đề xuất của chúng tôi đơn giản: chúng tôi nhắc nhở một mô hình ngôn ngữ lớn và cung cấp cho nó một truy vấn, sau đó chúng tôi sử dụng đầu ra của mô hình để mở rộng truy vấn gốc với các thuật ngữ mới giúp trong quá trình truy xuất tài liệu.

Kết quả của chúng tôi cho thấy rằng các lời nhắc Chain-of-Thought đặc biệt hứa hẹn cho mở rộng truy vấn, vì chúng hướng dẫn mô hình tạo ra các giải thích dài dòng có thể bao phủ nhiều từ khóa mới. Hơn nữa, kết quả của chúng tôi chỉ ra rằng việc bao gồm các tài liệu PRF trong các lời nhắc khác nhau có thể cải thiện hiệu suất chỉ số xếp hạng top-heavy trong giai đoạn truy xuất và mạnh mẽ hơn khi sử dụng với các kích thước mô hình nhỏ hơn, có thể giúp triển khai thực tế việc mở rộng truy vấn dựa trên LLM.

Như đã chứng minh trong bài báo này, các nhiệm vụ IR như mở rộng truy vấn có thể được hưởng lợi từ LLM. Khi các khả năng của LLM tiếp tục cải thiện, thật hứa hẹn khi thấy khả năng của chúng chuyển đổi sang các nhiệm vụ IR khác nhau. Hơn nữa, khi LLM trở nên có sẵn rộng rãi hơn, chúng sẽ dễ sử dụng và triển khai hơn như các phần cốt lõi của các hệ thống IR, điều này thú vị cho cả các nhà thực hành và nghiên cứu của các hệ thống như vậy.

TÀI LIỆU THAM KHẢO
[1] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 357–389.
[2] Jagdev Bhogal, Andrew MacFarlane, and Peter Smith. 2007. A review of ontology based query expansion. Information processing & management 43, 4 (2007), 866–886.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[4] Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. Acm Computing Surveys (CSUR) 44, 1 (2012), 1–50.
[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).
[6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).
[7] Vincent Claveau. 2021. Neural text generation for query expansion in information retrieval. In IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology. 202–209.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[9] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning. PMLR, 5547–5569.
[10] Miles Efron, Peter Organisciak, and Katrina Fenlon. 2012. Improving retrieval of short texts through document expansion. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. 911–920.
[11] Efthimis N Efthimiadis. 1996. Query Expansion. Annual review of information science and technology (ARIST) 31 (1996), 121–87.
[12] D Harman. [n.d.]. Relevance feedback and other query modification techniques. Information Retrieval: Data Structures & Algorithms ([n. d.]), 241–263.
[13] Ayyoob Imani, Amir Vakili, Ali Montazer, and Azadeh Shakery. 2019. Deep neural networks for query expansion using word embeddings. In Advances in Information Retrieval: 41st European Conference on IR Research, ECIR 2019, Cologne, Germany, April 14–18, 2019, Proceedings, Part II 41. Springer, 203–210.
[14] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422–446.
[15] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. choice 2640 (2016), 660.
[16] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document expansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).
[17] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Douglas Johnson. 2005. Terrier information retrieval platform. In Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005. Proceedings 27. Springer, 517–519.
[18] Yonggang Qiu and Hans-Peter Frei. 1993. Concept based query expansion. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval. 160–169.
[19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485–5551.
[20] Stephen E Robertson. 1990. On term selection for query expansion. Journal of documentation 46, 4 (1990), 359–364.
[21] Stephen E Robertson and K Sparck Jones. 1976. Relevance weighting of search terms. Journal of the American Society for Information science 27, 3 (1976), 129–146.
[22] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995), 109.
[23] Joseph John Rocchio Jr. 1971. Relevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing (1971).
[24] Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, and Utpal Garain. 2016. Using word embeddings for automatic query expansion. arXiv preprint arXiv:1606.07608 (2016).
[25] Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. 2006. Language model information retrieval with document expansion. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. 407–414.
[26] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131 (2022).
[27] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=wCu6T5xFjeJ
[28] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).
[29] Ellen M Voorhees. 1994. Query expansion using lexical-semantic relations. In SIGIR'94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University. Springer, 61–69.
[30] Ellen M Voorhees et al. 1999. The trec-8 question answering track report.. In Trec, Vol. 99. 77–82.
[31] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. arXiv preprint arXiv:2303.07678 (2023).
[32] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).
[33] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2020. BERT-QE: contextualized query expansion for document re-ranking. arXiv preprint arXiv:2009.07258 (2020).
[34] Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, and Andrew Yates. 2021. Contextualized query expansion via unsupervised chunk selection for text retrieval. Information Processing & Management 58, 5 (2021), 102672.

A LỜI NHẮC
Bảng 3 chứa tất cả các lời nhắc được thử trong bài báo này. Trong mỗi lời nhắc {query} biểu thị truy vấn mà chúng tôi muốn tạo mở rộng truy vấn. Chúng tôi ký hiệu với {query 1}, ..., {query 4} các truy vấn mẫu từ tập huấn luyện MS-MARCO. Tương tự, {doc 1}, ..., {doc 4} đại diện cho các đoạn văn liên quan tương ứng với các truy vấn được lấy mẫu, và {expansion 1}, ..., {expansion 4} đại diện cho các mở rộng tương ứng được tạo bằng phương pháp Terrier KL (tối đa 20 thuật ngữ) từ những đoạn văn liên quan đó. Cuối cùng, chúng tôi ký hiệu với {PRF doc 1}, ..., {PRF doc 3} top 3 tài liệu được truy xuất sử dụng truy vấn gốc, hoạt động như các tài liệu Phản hồi Giả Liên quan. Đối với lời nhắc CoT, chúng tôi lưu ý rằng mô hình có xu hướng xuất ra "The final answer:" hoặc "So the final answer is:" về phía cuối và chúng tôi lọc bỏ hai câu đó trước khi nối đầu ra mô hình với truy vấn.

Bảng 3: Các lời nhắc mở rộng truy vấn khác nhau.

ID Lời nhắc
Q2D [31] Viết một đoạn văn trả lời truy vấn đã cho:
Truy vấn: {query 1}
Đoạn văn: {doc 1}
Truy vấn: {query 2}
Đoạn văn: {doc 2}
Truy vấn: {query 3}
Đoạn văn: {doc 3}
Truy vấn: {query 4}
Đoạn văn: {doc 4}
Truy vấn: {query}
Đoạn văn:

Q2D/ZS Viết một đoạn văn trả lời truy vấn sau: {query}

Q2D/PRF Viết một đoạn văn trả lời truy vấn đã cho dựa trên ngữ cảnh:
Ngữ cảnh: {PRF doc 1}
{PRF doc 2}
{PRF doc 3}
Truy vấn: {query}
Đoạn văn:

Q2E Viết một danh sách từ khóa cho truy vấn đã cho:
Truy vấn: {query 1}
Từ khóa: {expansion 1}
Truy vấn: {query 2}
Từ khóa: {expansion 2}
Truy vấn: {query 3}
Từ khóa: {expansion 3}
Truy vấn: {query 4}
Từ khóa: {expansion 4}
Truy vấn: {query}
Từ khóa:

Q2E/ZS Viết một danh sách từ khóa cho truy vấn sau: {query}

Q2E/PRF Viết một danh sách từ khóa cho truy vấn đã cho dựa trên ngữ cảnh:
Ngữ cảnh: {PRF doc 1}
{PRF doc 2}
{PRF doc 3}
Truy vấn: {query}
Từ khóa:

CoT Trả lời truy vấn sau:
{query}
Đưa ra lý luận trước khi trả lời

CoT/PRF Trả lời truy vấn sau dựa trên ngữ cảnh:
Ngữ cảnh: {PRF doc 1}
{PRF doc 2}
{PRF doc 3}
Truy vấn: {query}
Đưa ra lý luận trước khi trả lời

B VÍ DỤ ĐẦU RA
Bảng 4 cho thấy kết quả của một mở rộng truy vấn cho cả kích thước mô hình Flan-T5-Large (770M) và kích thước mô hình Flan-UL2 (20B). Trước tiên, lưu ý rằng ở kích thước mô hình nhỏ hơn, các lời nhắc CoT và Q2D không tạo ra câu trả lời đúng điều này có hại cho hiệu suất truy xuất. Lời nhắc CoT/PRF, được căn cứ nhiều hơn vào các tài liệu PRF của nó, tránh được vấn đề này và tạo ra đúng câu trả lời "Tata Motors" giúp truy xuất đoạn văn liên quan. Ở kích thước mô hình lớn hơn (Flan-UL2), tất cả các lời nhắc Q2D, CoT và CoT/PRF đều tạo ra câu trả lời đúng "Tata Motors". Tuy nhiên, lời nhắc CoT cung cấp giải thích dài dòng nhất hướng đến câu trả lời của nó và có nhiều sự trùng lặp thuật ngữ với đoạn văn liên quan, cải thiện hiệu suất truy xuất tổng thể của nó.

Bảng 4: Ví dụ về mở rộng truy vấn với Flan-UL2. Màu xanh lá cây làm nổi bật câu trả lời đúng trong khi màu xanh dương chỉ ra các sự trùng lặp thuật ngữ cụ thể cho thấy các mở rộng truy vấn có thể hữu ích.

Truy vấn: ai sở hữu jaguar motors? (ID=1045405)
Đoạn văn: Jaguar là thương hiệu xe sang của Jaguar Land Rover, một nhà sản xuất ô tô đa quốc gia người Anh có trụ sở tại Whitley, Coventry England, thuộc sở hữu của công ty Ấn Độ Tata Motors kể từ năm 2008.

Flan-T5-Large (770M)
Q2D: Jaguar Motors là một nhà sản xuất xe sang người Anh có trụ sở tại Vương quốc Anh.
CoT: Jaguar Land Rover là chủ sở hữu của Jaguar. Câu trả lời: Jaguar Land Rover.
CoT/PRF: Thông tin liên quan là: Jaguar thuộc sở hữu của nhà sản xuất ô tô Ấn Độ Tata Motors Ltd. Vậy câu trả lời cuối cùng là nhà sản xuất ô tô Ấn Độ Tata Motors Ltd.

Flan-UL2 (20B)
Q2D: Jaguar là một bộ phận của Tata Motors, một công ty thuộc sở hữu của gia đình Tata.
CoT: Jaguar Land Rover là một nhà sản xuất ô tô đa quốc gia người Anh, được thành lập bởi William Lyons vào năm 1931. Trụ sở của nó ở Whitley, Coventry, Vương quốc Anh và là một thành phần của Chỉ số FTSE 250. Công ty là một công ty con hoàn toàn thuộc sở hữu của Tata Motors của Ấn Độ. Vậy câu trả lời cuối cùng là Tata Motors.
CoT/PRF: Jaguar thuộc sở hữu của nhà sản xuất ô tô Ấn Độ Tata Motors Ltd. Câu trả lời cuối cùng: Tata Motors Ltd.
