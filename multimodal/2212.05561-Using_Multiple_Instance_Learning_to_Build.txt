# 2212.05561.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2212.05561.pdf
# File size: 4559064 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Using Multiple Instance Learning to Build
Multimodal Representations
Peiqi Wang1, William M. Wells1, Seth Berkowitz2,
Steven Horng2, and Polina Golland1
1CSAIL, MIT, Cambridge MA, USA
2BIDMC, Harvard Medical School, Boston, MA, USA
wpq@mit.edu, polina@csail.mit.edu
Abstract. Image-text multimodal representation learning aligns data
acrossmodalitiesandenablesimportantmedicalapplications,e.g.,image
classiﬁcation, visual grounding, and cross-modal retrieval. In this work,
we establish a connection between multimodal representation learning
and multiple instance learning. Based on this connection, we propose
a generic framework for constructing permutation-invariant score func-
tions with many existing multimodal representation learning approaches
as special cases. Furthermore, we use the framework to derive a novel
contrastive learning approach and demonstrate that our method achieves
state-of-the-art results in several downstream tasks.
Keywords: representation learning, multiple instance learning
1 Introduction
In this paper, we propose a framework for designing multimodal representation
learning methods that encompasses previous approaches as special cases and
implies a new algorithm for multimodal learning that advances the state of the
art. Speciﬁcally, we establish a connection between self-supervised representa-
tion learning based on contrastive learning and multiple instance learning [3]
and show that they share similar assumptions and goals. We bring insights from
multipleinstancelearningtooﬀerafreshperspectiveonself-supervisedrepresen-
tation learning and ideas for performance improvements. With this connection
in mind, we derive a novel algorithm for learning image-text representations that
capture the structure shared between the two modalities and generalize well in
a variety of downstream tasks.
We aim to establish alignment between images and associated text to im-
prove clinical workﬂow. For example, an image model that mimics the radiolo-
gists’ interpretation could retroactively label images to select relevant patients
for a clinical trial. Further, local alignment between image regions and text frag-
ments(e.g.,sentences)promisestobeneﬁtmanydownstreamtasks.Forexample,
cross-modal retrieval can provide description of an image region for automated
documentation or enable comparisons with similar previously imaged patientsarXiv:2212.05561v2  [cs.CV]  9 Mar 2023

--- PAGE 2 ---
2 P. Wang et al.
for better interpretation based on local anatomy or pathology. Similarly, radiol-
ogists documenting ﬁndings can verify the accuracy of the report by noting if
the referred location (i.e., visual grounding of the text) is consistent with their
impression of the image.
Self-supervised representation learning is a useful tool for reducing annota-
tion burden for machine learning models in medical imaging. Despite the need
andopportunitiesforautomation,developmentofrobustmachinelearningmeth-
ods is held back by the lack of annotations that serve as the supervision signal for
learning. Self-supervised representation learning on paired image-text data of-
fers two advantages: (i) learning requires no further annotations and (ii) treating
text as “labels” enables us to use natural language to reference visual concepts
and vice versa [30]. Thus, we focus on learning image-text multimodal repre-
sentations but the proposed framework is broadly applicable to representation
learning on other multimodal data.
Learning joint representations involves training image and text encoders to
perform self-supervised tasks on paired image-text data [5,22,25] and evaluat-
ing on relevant downstream tasks. We focus on contrastive learning, i.e., clas-
sifying image-text pairs as matched (i.e., corresponding to the same imaging
event), or mismatched. Contrastive learning has been applied to the medical
domain, demonstrating impressive transfer capabilities on a diverse set of tasks
[2,4,13,23,28,36]. The biggest improvements come from addressing challenges
unique to this domain, e.g., the use of cross attention to deal with the lack
of eﬀective pathology detectors [13] and adaptation of language models to ad-
dress linguistic challenges in clinical notes [2]. Training the models has involved
increasingly complex contrastive loss functions that treat image and text sym-
metrically [2,4,13,36] and on multiple scales [2,13,23,28]. In contrast to previous
work that relies on many loss terms, our proposed contrastive loss is simple to
implement and yields superior performance.
Borrowing ideas from multiple instance learning, we treat local image region
features as “data” and sentence features as (complex) “labels”. Multiple instance
learning is a type of weakly supervised learning that is eﬀective for problems
that lack ﬁne-grain annotations [3]. For example, it can help to locate tumor
cells in whole slide images with just image-level labels [21]. Central to multiple
instance learning is the construction of permutation-invariant score functions
[14], and the choice of how the instance scores or features are aggregated to be
evaluated against an image-level label. Eﬀective instance aggregators leverage
domain knowledge [8], e.g., the Noisy-OR aggregator for drug activity predic-
tion [26], the Noisy-AND aggregator for cellular phenotype classiﬁcation [19].
In our work, we extend multiple instance classiﬁcation to contrastive learning
by constructing permutation-invariant image-text score functions. Drawing on
insights from multiple instance classiﬁcation with correlated instances [21], our
proposed instance aggregator exploits correlation among instances to build rep-
resentations that perform well in downstream tasks.
Manypriormultipleinstancelearningmethodsfocusedononeparticulartask
of interest, e.g., detection [35], region classiﬁcation [7], or retrieval [17]. Some in-

--- PAGE 3 ---
Using Multiple Instance Learning to Build Multimodal Representations 3
vestigated the choices of instance aggregators for more than one downstream
task [10,27] but are limited in generality (i.e., not intended for other applica-
tions) and scope (i.e., explored a few simple instance aggregators). In contrast,
our proposed framework for constructing permutation-invariant score functions
can be readily applied to other applications. We systematically investigate in-
stance aggregators and their eﬀect on representation learning, leading to a novel
approach for learning joint representations. We evaluate the resulting image-text
representations on a diverse set of downstream tasks and demonstrate state-of-
the-art performance across all tasks in the context of a large set of chest X-ray
images and associated radiological reports.
2 Method
We ﬁrst introduce notation and discuss the local and global approaches for con-
structing permutation-invariant image-document score functions at the core of
the learning procedure. We then instantiate the framework for a speciﬁc choice
of aggregators for contrastive learning.
2.1 Problem Setup
A localD-dimensional representation of an image with Nproposed regions is
a collection of Nfeatures vectors xn2XRD,n2f1;;Ng. In our ex-
periments, we use regular tiling to generate image regions and leave more so-
phisticated proposal methods (e.g., [31]) for future work. A local representation
of aM-sentence document (e.g., a radiology report) is a collection of sentence
feature vectors ym2YRD,m2f1;;Mg.
Functionh:XY!Rmeasures the similarity between representations, e.g.,
h(xn;ym)is the similarity between a region and a sentence. In our experiments,
we use cosine similarity h(x;y) =hx;yi=(kxkkyk), though the formulation ac-
cepts any diﬀerentiable similarity function.
For any vector space U, aggregator function :P(U)!Uaggregates ele-
ments in the input set into a “representative”. P(U)is the set of all ﬁnite subsets
ofU. For example, (fxng) =1
NP
nxnaggregates Nregion features xn2X
by averaging them, while (fhng) = max nhnaggregates Nsimilarity scores
into a single score by computing the maximum score. We restrict our attention
to aggregators that are permutation-invariant, i.e., they treat their input as an
unordered set rather than an ordered vector.
Permutation-invariant image-document score function S:P(X)P(Y)!
Rmeasures the similarity between an image and a document based on region
featuresfxngand sentence features fymg.
2.2 Local & Global Permutation-Invariant Score Functions
Contrastive representation learning can be seen as maximizing the likelihood of
correctly classifying image-text pairs as matched or mismatched. Since supervi-

--- PAGE 4 ---
4 P. Wang et al.
Fig. 1.Local (top) and global (bottom) image-document score functions.
sion is provided at the image-document level, we deﬁne a framework to build
permutation-invariant image-document score functions.
Thelocalapproach aggregatesregion-sentencescoresintoanimage-sentence
score. The image-sentence score gmfor sentence min the document is ob-
tained by applying a local aggregator function lto region-sentence scores, i.e.,
gm=l(fh(xn;ym)gn),l(fh(x1;ym);;h(xN;ym)g).
Theglobalapproach ﬁrstaggregateslocalregionfeatures fxngintoasingle
image feature vector g(fxng)using a global aggregator function g. The image-
sentence score gmis computed using the similarity function hon the image fea-
ture vector g(fxng)and sentence feature vector ym, i.e.,gm=h(g(fxng);ym).
In both approaches, the image-document score Sis obtained by aggregating
image-sentencescoreswithanotheraggregatorfunction s,i.e.,S(fxng;fymg) =
s(fgmg). Figure 1 illustrates the framework for constructing S. To summarize,
the local and global image-document scores SlandSgare computed as follows:
Sl(fxng;fymg) =s(fl(fh(xn;ym)gn)gm); (1)
Sg(fxng;fymg) =s(fh(g(fxng);ym)gm): (2)
As the aggregator functions are permutation-invariant, the image-document
score function Sis naturally permutation-invariant as well. We emphasize that
Streats image features and text features diﬀerently, and that the order of appli-
cation of similarity evaluation h()and aggregators ()is empirically relevant.
This design decision is motivated by the fact that each sentence in a radiology
report represent a concept and its location in the image, i.e., it is akin to a label
for some region in the image. The converse is not necessarily true as some parts
of the image are not described in the report.
2.3 Representation Learning with LSE +NL Aggregators
In this section, we introduce our method LSE +NL for learning multimodal rep-
resentations that relies on a combination of local and global image-document
score functions and an asymmetric text-to-image contrastive loss.

--- PAGE 5 ---
Using Multiple Instance Learning to Build Multimodal Representations 5
Inspiredby[21],weuseasoftmaximumfunctiontoidentifythemostrelevant
region for a sentence, i.e., the critical region, and attend more to regions that
are similar to the critical region. Speciﬁcally, the local aggregator lis the log-
sum-exp (LSE) function
l(fhng) =1
llogNX
n=1exp(lhn); (3)
wherelis a scale parameter that controls how well the LSE function approxi-
mates the max function. The global aggregator glinearly combines the region
features using the distance to the critical region as weights, i.e.,
g(fxng) =NX
n=1exp(ghAxn;Axki)PN
n0=1exp(ghAxn0;Axki)xn; (4)
wherekis the index of the critical region, i.e., k= arg maxnh(xn;ym),Ais a
learned weight matrix, and gis the scale parameter for the softmax function.
We can interpret gas a form of attention where regions that are more similar
to the critical region are given a higher attention weight. In eﬀect, gexploits
the correlation between each region and the critical region using attention. In
addition,gcan be seen as a form of non-local (NL) network [34]. Both land
gare permutation-invariant functions. We choose sto be the average function.
We use the local and global image-document scores in (1) and (2) computed
with our choice of landgfor contrastive learning. Given a document, we
form an image-document score vector s,(s+;s 
1;;s 
K)wheres+2Ris the
image-document score with its matched image and s 
k2Rfork= 1;;K
is the image-document score with Kmismatched images. We use slandsgto
denote (K+ 1)-length score vectors deﬁned above computed using the local and
the global score functions respectively. The image and text encoders are trained
to minimize L(sl) +L(sg)over documents in the training set where Lis the
text-to-image contrastive loss [29,36]
L(s), logexp(s+)
exp(s+) +PK
k=1exp(s 
k)(5)
with scale parameter . In the equation above, sis either vector slcomputed
using (1) with ldeﬁned in (3) or vector sgcomputed using (2) with gdeﬁned
in(4).Theimage-to-textcontrastivelosswherethenegativescoresarecomputed
for an image with Kdiﬀerent mismatched documents is often used alongside L
in prior work [2,4,13,36]. We choose to treat images and text asymmetrically
and show that the simple text-to-image contrastive loss is suﬃcient to induce
representations that generalize well.
3 Connection to Multiple Instance Learning
Inmultipleinstancelearning[3],asetthatcontainsmanyinstances fx1;;xNg
is referred to as a bag. The training set consists of bags and their associated

--- PAGE 6 ---
6 P. Wang et al.
Table 1. Taxonomy of related methods for image-language representation learning in
our multiple instance learning inspired framework. For each method, we report image
segments captured by xn(region or video), language segments captured by ym(word,
sentence, or audio), local aggregator lif used (Max or LSE), global aggregator gif
used (Avg, NN for generic non-linear functions, cross attention (CA) l(fxng;ym) =P
nexp(hxn;ymi)=P
n0exp(hxn0;ymi)xn, or NL in (4)), and the ﬁnal score aggregator
s(Sum, Max, LSE, Id, Avg).
Methods xnymlg s
NeuralTalk [16] region word Max - Sum
DAVEnet-MISA [10] region audio Max - Sum
MIML [9] video audio Max - Max
MIL-NCE [27] video sentence - Avg LSE
ConVIRT/CLIP [36,30] region sentence - NN Avg Id
GLoRIA/BioViL [13,2] region word - CA LSE
region sentence - Avg Id
LSE+NL (Ours) region sentence LSE - Avg
region sentence - NL Avg
bag labelsywhile the instance labels are not provided. For binary bag labels,
a positive bag is guaranteed to include at least one positive instance, while a
negative bag includes no positive instances. The bag-level labels are used to
train classiﬁer to assign instance-level and bag-level labels in new, unseen bags.
Existing image-text representation learning algorithms that are either pre-
dictive [6] or contrastive [30] can be seen as a form of multiple instance learning.
Speciﬁcally, we can view an image as a bag of region features and the corre-
sponding sentence that describes the image as the bag label. Instead of taking
on binary values, the bag labels can represent arbitrary categories via natural
language. Although the exact region that corresponds to the sentence is un-
known, the matched image contains at least one region that corresponds to the
text while a randomly sampled image most likely does not. Similar to multi-
ple instance learning, self-supervised representation learning methods use these
assumptions for learning.
More generally, we consider the text label as a bag of sentences. For example,
sentences describing ﬁndings within a chest X-ray image most likely can be per-
muted without changing the overall meaning. Therefore, representation learning
can be interpreted as predicting the label bag fymggiven the input bag fxmg.
This setup corresponds to multi-instance multi-label learning [37].
Moreover, multiple instance learning and multimodal representation learning
share comparable goals. Multiple instance learning aims to align instances and
bags with labels such that the pre-trained model performs well in classiﬁcation
tasks. Multimodal representation learning aims to align images and their subre-
gions with text such that the pre-trained model perform well on tasks that rely
on such alignment, e.g., image classiﬁcation relies on image-sentence alignment,
visual grounding and cross-modal retrieval rely on region-sentence alignment.

--- PAGE 7 ---
Using Multiple Instance Learning to Build Multimodal Representations 7
There are two main multiple instance learning approaches, instance-level and
embedding-level approaches [1]. The instance-level approach computes the bag
score by aggregating the instance scores, while the embedding-level approach
computes the bag score based on a bag feature that is aggregated from the
instance features. The local and global approaches in Section 2.2 are extensions
of the instance and embedding approaches to contrastive learning.
This parallel enables us to analyze prior methods as instances of the frame-
work deﬁned in Section 2.2 that is inspired by multiple instance learning (Ta-
ble1).WemakeonegeneralizationtotheformulationinSection2.2toaccommo-
date cross attention [20]: the local aggregator function lcan potentially rely on
label features ymto multiplex its behavior, i.e., l:P(X)Y!X. In summary,
a diverse set of aggregators l;g;shave been demonstrated on multimodal
representation learning at varying scales, implying there may not be a single
set of aggregators that works well for every problem. More realistically, the best
aggregator functions are the ones that ﬁt application-speciﬁc assumptions well.
4 Experiments
We illustrate the proposed approach by building a representation of frontal chest
X-ray images and associated radiology reports and using it in downstream tasks.
In all of the experiments, the data used for representation learning is disjoint
from the test sets used to evaluate the downstream tasks.
We normalize the images and resize them to 512x512 resolution. We apply
random image augmentations, i.e., 480x480 random crops, brightness and con-
trast variations, and random aﬃne transforms (only for image model ﬁne-tuning
during evaluation). We use PySBD [32] for sentence tokenization.
We employ ResNet-50 [12] as the image region encoder and CXR-BERT [2]
as the sentence encoder. Each encoder is followed by a linear projection to a
128 dimension embedding space. In particular, the projected ResNet-50 conv-
5 activations act as the region features fxngand the projected mean-pooled
contextualized word embeddings acts as the sentence features fymg.
4.1 Representation learning
We use a subset of 234,073 chest X-ray images and report from MIMIC-CXR
[15] for representation learning. We randomly initialize the image encoder and
use the CXR-BERT model [2] pre-trained on a biomedical corpus (i.e., the stage
II model) as the sentence encoder. We use the AdamW optimizer [24] and decay
the initial learning rate of 5e-5 using a cosine schedule with 2k warmup steps.
we initialize to 14 and optimize this hyperparameter alongside the encoder
parameters. We set other scale parameters as follows: l= 0:1;g=e. We use
a batch size of 64. For each image in the batch, we sample 5 sentences, with
replacement if needed, to make up the label bag. Here, N= 225andM= 5.

--- PAGE 8 ---
8 P. Wang et al.
4.2 Downstream Tasks
ImageClassiﬁcation Toevaluatezero-shot(ZS)andﬁne-tuned(FT)classiﬁca-
tion performance, we use the same split of RSNA Pneumonia (RSNA) [33] as in
[13], speciﬁcally, 18,678/4,003/4,003 for training/validation/testing. To evaluate
in-distributionﬁne-tunedclassiﬁcationperformanceintheablationstudy,weuse
5 CheXpert labels (Atelectasis, Cardiomegaly, Edema, Pleural Eﬀusion, Pneu-
mothorax) on the MIMIC-CXR data set [15] that we denote MIMIC-CheXpert
(CheX). There are roughly 1k images in the test set associated with each CheX-
pert label. To evaluate the data eﬃciency of representation learning approaches,
we use diﬀerent amounts of training data (1% and 100%).
For zero-shot image classiﬁcation, we ﬁrst tokenize and encode the class-
speciﬁc text prompts (e.g., “Findings suggesting pneumonia.” and “No evidence
ofpneumonia.”).Foreachimage,weassignabinarylabelthatcorrespondstothe
prompt with the higher image-sentence score. We ﬁnd it important to normalize
the scores to [0;1]for each class before applying the softmax. For ﬁne-tuned
image classiﬁcation, we use the Adam optimizer [18] with a learning rate of 3e-3
to optimize the randomly initialized weights and a bias over the mean-pooled
region features while keeping the encoder weights ﬁxed. For RSNA Pneumonia,
wereportaccuracyandAUC.ForMIMIC-CheXpert,wereporttheaverageAUC
over ﬁve binary classiﬁcation tasks.
Visual Grounding We evaluate visual grounding performance using the MS-
CXR region-sentence annotations [2]. This data set consists of 1,448 bounding
boxes over 1,162 images, where each bounding box is associated with a sentence
that describes its dominant radiological feature. We compute region-sentence
scores to quantify how well the sentence is localized in the image. We report
a measure of discrepancy between region-sentence scores inside and outside the
bounding box, i.e., contrast-to-noise ratio (CNR) [2], and how well the thresh-
oldedregion-sentencescoresoverlapwiththeboundingboxonaverage,i.e.,mean
intersection over union (mIoU). In contrast to [2], we pick thresholds that span
[ 1;1]in0:05increments to compute the mIoU for a fair comparison.
Cross-Modal Retrieval We evaluate cross-modal retrieval performance using
the MS-CXR data set as well. We compute the bounding box features from the
region features with RoIAlign [11]. We compute box-sentence scores and sort
them to retrieve items in one modality given a query from the other modality.
The correctly retrieved item is the one that is paired with the query item. We
report the fraction of times the correct item was found in the top K results
(R@K) and the median rank of the correct item in the ranked list (MedR).
4.3 Results
Comparison with State-of-the-art Methods We compare the proposed ap-
proach LSE +NL with the state-of-the-art methods GLoRIA [13] and BioViL [2].
GLoRIAisarepresentationlearningmethodthatlearnsbasedonimage-sentence
and region-word pairs. BioViL improves upon GLoRIA by using a better text

--- PAGE 9 ---
Using Multiple Instance Learning to Build Multimodal Representations 9
Table 2. Image classiﬁcation performance on the RSNA Pneumonia data set. We
report accuracy and AUC on zero-shot and ﬁne-tuned classiﬁcation (ﬁne-tuned on 1%
and 100% labels). Our approach compares favorably to BioViL [2].
Method Zero-Shot 1% 100%
ACC" AUC" ACC" AUC" ACC" AUC"
BioViL 0.73 0.83 0.81 0.88 0.82 0.89
LSE+NL 0.80 0.84 0.84 0.87 0.85 0.89
Table 3. Visual grounding performance. We report
contrast-to-noise ratio (CNR) and mean intersection-
over-union (mIoU). mIoU measures mean IoU of a
thresholded region-sentence map and the ground truth
bounding box over a set of thresholds. Our approach
outperforms BioViL [2] on both measures.Method CNR"mIoU"
BioViL 1.14 0.17
LSE+NL 1.44 0.19
Table 4. Cross-modal retrieval performance. We report recall for the top 10, 50 and
100 answers returned by the method, as well as the median rank of the ground truth
element for sentence retrieval based on region queries and for region retrieval based on
sentence queries. Our method outperforms the baselines on all measures.
Method Region !Sentence Sentence !Region
R@10"R@50"R@100"MedR# R@10"R@50"R@100"MedR#
GLoRIA 0.06 0.21 0.37 162 0.06 0.21 0.34 183
BioViL 0.07 0.26 0.40 151 0.08 0.26 0.40 146
LSE+NL 0.11 0.29 0.45 119 0.11 0.36 0.51 97
encoder, relying on a symmetric contrastive loss and masked language model-
ing for representation learning. We omit reporting GLoRIA’s classiﬁcation and
visual grounding performance for GLoRIA as [2] showed that BioViL is better
than GLoRIA on these tasks. Our simple model provides consistently better
performance than these state-of-the-art algorithms.
Table 2 reports image classiﬁcation accuracy based on the learned represen-
tations for diﬀerent amounts of data used to ﬁne-tune the representation for the
downstream task (zero-shot, 1%, and 100%). Our method is competitive or bet-
ter than the baseline, especially in the zero-shot setup, underscoring its promise
for limited annotation scenarios. Table 3 and Table 4 report the methods’ perfor-
mance on visual grounding and cross-modal retrieval respectively. Our method
signiﬁcantly outperforms the baseline.
Figure 2 illustrates examples of visual grounding. Unlike [2], we do not
smooth the region-sentence scores produced by our model. Our method yield
qualitatively better region-sentence scores than BioViL on a few challenging
failure cases discussed in [2]. In particular, our pre-trained model captures lo-
cation speciﬁcations more eﬀectively, e.g., recognizing “at both lung bases” in
the ﬁrst image and “right” in the third image. Both our method and BioViL are
prone to false positives, i.e., regions outside the ground-truth bounding box with
high region-sentence scores, which highlights the need for further improvements.

--- PAGE 10 ---
10 P. Wang et al.
Fig. 2.ExamplevisualgroundingresultsforseveralchallengingcasesforBioVil[2](top
row) and our method (bottom row). Text queries and the corresponding ground truth
bounding boxes are shown for each image. Colormap overlay visualizes region-sentence
scores (blue corresponds to low scores, red highlights regions with high scores). Our
method provides maps that align better with the ground truth bounding boxes.
Table 5. Ablation study results. For each variant of the method, performance statis-
tics are reported for each downstream task consistently with Tables 2, 3, and 4. RSNA
is RSNA Pneumonia. CheX is MIMIC-CheXpert. FT is ﬁne-tuned classiﬁcation using
100% of the labels. ZS is zero-shot classiﬁcation. We report AUC for image classiﬁca-
tion. Local representations perform well for image classiﬁcation, while visual grounding
and cross-modal retrieval beneﬁt from integration of local and global representations.
Method Classiﬁcation Grounding Cross-Modal Retrieval
RSNA-ZS"RSNA-FT"CheX-FT" CNR" MedR(I T)#MedR(T I)#
LSE 0.856 0.892 0.874 1.308 146 137
NL 0.636 0.871 0.854 0.836 264 272
LSE+Average 0.851 0.889 0.868 0.915 191 161
LSE+NL 0.846 0.891 0.870 1.403 110 102
w. ResNet-50 0.844 0.890 0.870 1.438 119 97
Ablation In the ablation study (Table 5), we compare our method LSE +NL
with using either the local LSE or the global NL approach only, as well as replac-
ing the NL with average as the region aggregator, i.e., LSE +Average. To enable
extensive experimentation, we use ResNet-18 as the image encoder. LSE +NL
provides good trade-oﬀ between region-sentence and image-sentence alignment.
LSE+NLhascomparableperformancetoLSEforimageclassiﬁcationtaskswhile
signiﬁcantly outperforming all alternatives in visual grounding and cross-modal
retrieval. Using a larger image encoder model ResNet-50 provides only a modest
improvement in visual grounding.

--- PAGE 11 ---
Using Multiple Instance Learning to Build Multimodal Representations 11
Fig. 3.Eﬀects of aggregator choice on the performance. Performance of models trained
with local aggregators (shades of blue), global aggregators (shades of orange) and com-
binations of local and global aggregators (shades of green) is shown for image classiﬁ-
cation (AUC), visual grounding (CNR) and cross-modality retrieval (MedR averaged
for both directions). The metrics are normalized to unit interval for easier comparisons
across tasks. The choice of aggregators eﬀects image classiﬁcation performance much
less than that of visual grounding and cross-modality retrieval. There is high perfor-
mance variations within each group. Combination approaches do well on all tasks.
Aggregator Choices Figure 3 compares a few instance aggregators’ perfor-
mance on downstream tasks. We compare the local approach (e.g., LSE, NOR
[26], NAND [19]) the global approach (e.g., Max, Average, Att [14]) and a com-
bination of local and global approaches (e.g., LSE +Att, LSE +NL). Aggregators
within each approach exhibits high performance variations. The best local aggre-
gator is superior to the best global aggregators we explored on all downstream
tasks.Combininglocalandglobalapproachesyieldsthebestperformingmethod.
4.4 Limitations
Though empirically useful, our framework doesn’t provide theoretical guarantees
on downstream task performance. We did not investigate what properties of an
aggregator determine its transfer behaviors. In addition, our proposed method
LSE+NL is sensitive to the value of scaling parameters; Finding the optimal
hyperparameters automatically is crucial for model scaling.
5 Conclusions
Inthispaper,weproposeaframeworktoconstructpermutation-invariantimage-
documentscorefunctionsformultimodalcontrastivelearning.Takinginspiration
from multiple instance learning, we introduce LSE +NL for learning multimodal
representations that rely on both local and global score functions and exploit
correlation between image regions. Our method outperforms the state-of-the-art
approaches on image classiﬁcation, visual grounding, and cross-modal retrieval.
Inaddition,weshowthatcontrastiverepresentationlearningisaformofmultiple
instance learning, providing us with valuable insights from a related ﬁeld for
solving shared challenges to learn representations that generalized well.
Acknowledgements Work supported by MIT JClinic, Philips, and Wistron.

--- PAGE 12 ---
12 P. Wang et al.
References
1. Amores, J.: Multiple instance classiﬁcation: Review, taxonomy and comparative
study. Artiﬁcial Intelligence (Aug 2013)
2. Boecking, B., Usuyama, N., Bannur, S., Castro, D.C., Schwaighofer, A., Hyland,
S., Wetscherek, M., Naumann, T., Nori, A., Alvarez-Valle, J., Poon, H., Oktay,
O.: Making the Most of Text Semantics to Improve Biomedical Vision–Language
Processing. In: ECCV (Oct 2022)
3. Carbonneau, M.A., Cheplygina, V., Granger, E., Gagnon, G.: Multiple Instance
Learning: A Survey of Problem Characteristics and Applications. Pattern Recog-
nition (May 2018)
4. Chauhan, G., Liao, R., Wells, W., Andreas, J., Wang, X., Berkowitz, S., Horng,
S., Szolovits, P., Golland, P.: Joint Modeling of Chest Radiographs and Radiology
Reports for Pulmonary Edema Assessment. In: MICCAI (Oct 2020)
5. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:
UNITER: UNiversal Image-TExt Representation Learning. In: ECCV (2020)
6. Desai, K., Johnson, J.: VirTex: Learning Visual Representations from Textual An-
notations. In: CVPR (Jun 2021)
7. Fang, H., Gupta, S., Iandola, F., Srivastava, R.K., Deng, L., Dollar, P., Gao, J.,
He, X., Mitchell, M., Platt, J.C., Zitnick, C.L., Zweig, G.: From captions to visual
concepts and back. In: CVPR (Jun 2015)
8. Foulds, J., Frank, E.: A review of multi-instance learning assumptions. The Knowl-
edge Engineering Review (Mar 2010)
9. Gao, R., Feris, R., Grauman, K.: Learning to Separate Object Sounds by Watching
Unlabeled Video. In: ECCV (Sep 2018)
10. Harwath,D.,Recasens,A.,Surís,D.,Chuang,G.,Torralba,A.:JointlyDiscovering
Visual Objects and Spoken Words from Raw Sensory Input. IJCV (Mar 2020)
11. He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: ICCV (Oct 2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recognition.
In: CVPR (Jun 2016)
13. Huang, S.C., Shen, L., Lungren, M.P., Yeung, S.: GLoRIA: A Multimodal Global-
Local Representation Learning Framework for Label-Eﬃcient Medical Image
Recognition. In: ICCV (Oct 2021)
14. Ilse, M., Tomczak, J., Welling, M.: Attention-based Deep Multiple Instance Learn-
ing. In: ICML (Jul 2018)
15. Johnson, A.E.W., Pollard, T.J., Berkowitz, S.J., Greenbaum, N.R., Lungren, M.P.,
Deng, C.y., Mark, R.G., Horng, S.: MIMIC-CXR, a de-identiﬁed publicly available
database of chest radiographs with free-text reports. Sci Data (Dec 2019)
16. Karpathy, A., Fei-Fei, L.: Deep Visual-Semantic Alignments for Generating Image
Descriptions. TPAMI (Apr 2017)
17. Karpathy, A., Joulin, A., Fei-Fei, L.: Deep fragment embeddings for bidirectional
image sentence mapping. In: NIPS (Dec 2014)
18. Kingma,D.,Ba,J.:Adam:AMethodforStochasticOptimization.arXiv:1412.6980
(Dec 2014)
19. Kraus, O.Z., Ba, J.L., Frey, B.J.: Classifying and segmenting microscopy images
with deep multiple instance learning. Bioinformatics (Jun 2016)
20. Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked Cross Attention for Image-
Text Matching. In: ECCV (Sep 2018)
21. Li, B., Li, Y., Eliceiri, K.W.: Dual-stream Multiple Instance Learning Network for
Whole Slide Image Classiﬁcation with Self-supervised Contrastive Learning. In:
CVPR (Jun 2021)

--- PAGE 13 ---
Using Multiple Instance Learning to Build Multimodal Representations 13
22. Li, L.H., Yatskar, M., Yin, D., Hsieh, C.J., Chang, K.W.: VisualBERT: A Simple
and Performant Baseline for Vision and Language. arXiv:1908.0355 (Aug 2019)
23. Liao, R., Moyer, D., Cha, M., Quigley, K., Berkowitz, S., Horng, S., Golland,
P., Wells, W.M.: Multimodal Representation Learning via Maximization of Local
Mutual Information. In: MICCAI (Sep 2021)
24. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: ICLR (May
2019)
25. Lu, J., Batra, D., Parikh, D., Lee, S.: ViLBERT: Pretraining Task-Agnostic Visi-
olinguisticRepresentationsforVision-and-LanguageTasks.In:NeurIPS(Dec2019)
26. Maron,O.,Lozano-Pérez,T.:Aframeworkformultiple-instancelearning.In:NIPS
(Jul 1998)
27. Miech, A., Alayrac, J.B., Smaira, L., Laptev, I., Sivic, J., Zisserman, A.: End-to-
End Learning of Visual Representations From Uncurated Instructional Videos. In:
CVPR (Jun 2020)
28. Müller, P., Kaissis, G., Zou, C., Rueckert, D.: Joint Learning of Localized Repre-
sentations from Medical Images and Reports. In: ECCV (Oct 2022)
29. van den Oord, A., Li, Y., Vinyals, O.: Representation Learning with Contrastive
Predictive Coding. arXiv:1807.03748 (Jul 2018)
30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning Transferable
Visual Models From Natural Language Supervision. In: ICML (Jul 2021)
31. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks. In: NIPS (Dec 2015)
32. Sadvilkar, N., Neumann, M.: PySBD: Pragmatic Sentence Boundary Disambigua-
tion. In: NLP-OSS (Nov 2020)
33. Shih, G., Wu, C.C., Halabi, S.S., Kohli, M.D., Prevedello, L.M., Cook, T.S.,
Sharma, A., Amorosa, J.K., Arteaga, V., Galperin-Aizenberg, M., Gill, R.R.,
Godoy, M.C., Hobbs, S., Jeudy, J., Laroia, A., Shah, P.N., Vummidi, D., Yad-
danapudi, K., Stein, A.: Augmenting the National Institutes of Health Chest Ra-
diograph Dataset with Expert Annotations of Possible Pneumonia. Radiol Artif
Intell (Jan 2019)
34. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local Neural Networks. In: CVPR
(Jun 2018)
35. Zhang, C., Platt, J., Viola, P.: Multiple Instance Boosting for Object Detection.
In: NIPS (Dec 2005)
36. Zhang,Y.,Jiang,H.,Miura,Y.,Manning,C.D.,Langlotz,C.P.:ContrastiveLearn-
ing of Medical Visual Representations from Paired Images and Text. In: MLHC
(Aug 2022)
37. Zhou, Z.H., Zhang, M.L., Huang, S.J., Li, Y.F.: Multi-Instance Multi-Label Learn-
ing. Artiﬁcial Intelligence (Jan 2012)
