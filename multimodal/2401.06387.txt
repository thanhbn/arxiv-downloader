# 2401.06387.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2401.06387.pdf
# File size: 21383604 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Towards High-Quality and Efficient Speech
Bandwidth Extension with Parallel Amplitude and
Phase Prediction
Ye-Xin Lu, Yang Ai, Member, IEEE , Hui-Peng Du, Zhen-Hua Ling, Senior Member, IEEE
Abstract —Speech bandwidth extension (BWE) refers to widen-
ing the frequency bandwidth range of speech signals, enhancing
the speech quality towards brighter and fuller. This paper
proposes a generative adversarial network (GAN) based BWE
model with parallel prediction of Amplitude and Phase spectra,
named AP-BWE, which achieves both high-quality and efficient
wideband speech waveform generation. The proposed AP-BWE
generator is entirely based on convolutional neural networks
(CNNs). It features a dual-stream architecture with mutual
interaction, where the amplitude stream and the phase stream
communicate with each other and respectively extend the high-
frequency components from the source narrowband amplitude
and phase spectra. To improve the naturalness of the extended
speech signals, we employ a multi-period discriminator at the
waveform level and design a pair of multi-resolution amplitude
and phase discriminators at the spectral level, respectively.
Experimental results demonstrate that our proposed AP-BWE
achieves state-of-the-art performance in terms of speech quality
for BWE tasks targeting sampling rates of both 16 kHz and 48
kHz. In terms of generation efficiency, due to the all-convolutional
architecture and all-frame-level operations, the proposed AP-
BWE can generate 48 kHz waveform samples 292.3 times faster
than real-time on a single RTX 4090 GPU and 18.1 times faster
than real-time on a single CPU. Notably, to our knowledge, AP-
BWE is the first to achieve the direct extension of the high-
frequency phase spectrum, which is beneficial for improving the
effectiveness of existing BWE methods.
Index Terms —Speech bandwidth extension, generative adver-
sarial network, amplitude prediction, phase prediction.
I. I NTRODUCTION
IN practical speech signal transmission scenarios, limita-
tions in communication devices or transmission channels
may lead to the truncation of the frequency bandwidth of
speech signals. The deficiency of high-frequency information
can induce distortion, muffling, or a lack of clarity in speech.
Speech bandwidth extension (BWE) aims to supplement the
missing high-frequency bandwidth from the low-frequency
components, thereby enhancing the quality and intelligibility
of the narrowband speech signals. In the earlier years, the
bandwidth of communication devices was extremely limited.
For instance, the bandwidth of speech signals in the public
switching telephone network (PSTN) is less than 4 kHz. So
The authors are with the National Engineering Research Center of Speech
and Language Information Processing, University of Science and Technology
of China, Hefei 230027, China (e-mail: yxlu0102@mail.ustc.edu.cn; yan-
gai@ustc.edu.cn; redmist@mail.ustc.edu.cn; zhling@ustc.edu.cn).
This work was funded by the National Nature Science Foundation of China
under Grant 62301521, the Anhui Provincial Natural Science Foundation
under Grant 2308085QF200, and the Fundamental Research Funds for the
Central Universities under Grant WK2100000033.early BWE efforts were primarily focused on extending the
bandwidth to a maximum target frequency of 8 kHz. With
the advancement of communication technology, the signal
bandwidth that communication devices can transmit has been
widening. Therefore, recent speech BWE research has increas-
ingly focused on extending the bandwidth to the perceptual
frequency limits of the human ear (e.g., 22.05 kHz or 24 kHz),
enabling applications in high-quality mobile communication,
audio remastering and enhancement, and more. Speech BWE
can be applied to various speech signal processing areas,
such as text-to-speech (TTS) synthesis [1], automatic speech
recognition (ASR) [2], [3], speech enhancement (SE) [4], [5],
and speech codec [6].
In the time domain, speech BWE is a conditionally stringent
form of speech super-resolution (SR). Speech SR aims to
increase the temporal resolution of low-resolution speech sig-
nals by generating high-frequency components, whereas low-
resolution speech signals may contain aliased high-frequency
components. In contrast, in BWE, only the low-frequency
components are preserved in the narrowband signals. Con-
sequently, the BWE task poses greater challenges than SR.
Nevertheless, the majority of SR methods are applicable to
the BWE task.
Early research on BWE was predominantly based on sig-
nal processing techniques, encompassing approaches such as
source-filter-based methods [4], [7], mapping-based meth-
ods [8]–[10], statistical methods [11]–[17], and so forth.
Source-filter-based methods introduced the source-filter model
to extend bandwidth by separately restoring high-frequency
residual signals and spectral envelopes. The high-frequency
residual signals are often derived by folding the spectrum of
narrowband signals, while predicting high-frequency spectral
envelopes presents more challenges. Mapping-based methods
utilized codebook mapping or linear mapping to map lower-
band speech representations to their corresponding upper-
band envelopes. Additionally, statistical methods leveraged
Gaussian mixture models (GMMs) and hidden Markov models
(HMMs) to establish the mapping relationship between low-
frequency spectral parameters and their corresponding high-
frequency counterparts. Despite the effective performance
achieved by these statistical methods in speech BWE, the
limited modeling capability of GMMs and HMMs may lead
to generating over-smoothed spectral parameters [18].
With the renaissance of deep learning, deep neural networks
(DNNs) have shown strong modeling capability. DNN-based
BWE methods can be broadly classified into two categories:arXiv:2401.06387v2  [eess.AS]  14 Dec 2024

--- PAGE 2 ---
2
waveform-based methods and spectrum-based methods. In the
waveform-based methods, neural networks were employed to
learn the direct mapping from the narrowband waveforms to
the wideband ones [19]–[23], in which both the amplitude and
phase information were implicitly restored. Nevertheless, due
to the all-sample-level operations, this category of methods
still suffered from the bottleneck of low generation efficiency,
especially in generating high-resolution waveforms, limiting
the application of this category of methods in low com-
putational power scenarios. In the spectrum-based methods,
neural networks have been adopted to predict high-frequency
amplitude-related spectral parameters. However, it’s difficult to
parameterize and predict the phase due to its wrapping charac-
teristic and non-structured nature. The common practice was
to replicate [24] or mirror-inverse [25]–[27] the low-frequency
phase to obtain the high-frequency one, which constrained the
quality of the extended wideband speech. Another approach
was to use vocoders for phase recovery from the vocal-tract
filter parameters [28]–[30] or mel-spectrogram [31]. These
vocoder-based methods involved a two-step generation pro-
cess, where the prediction errors accumulated and the gener-
ation efficiency was significantly constrained. Other methods
chose to implicitly recover phase information by predicting
the phase-contained spectra, e.g., short-time Fourier transform
(STFT) complex spectrum [32] and modified discrete cosine
transform (MDCT) spectrum [33], but they were still limited
in the precise modeling and optimization of phase. Overall,
existing BWE methods have yet to achieve a precise extension
of the high-frequency phase, leaving room for improvement in
both speech quality and generation efficiency.
In our previous works [34], [35], we proposed a neural
speech phase prediction method based on parallel estimation
architecture and anti-wrapping losses. The proposed phase
prediction method has been proven to be applicable to various
speech-generation tasks, such as speech synthesis [36] and
speech enhancement [37]. We also have tried to apply it to
speech BWE by predicting the wideband phase spectra from
the extended log-amplitude spectra, and the final extended
waveforms were obtained through inverse STFT (iSTFT).
However, in our preliminary experiments, we found that this
method still faced the same issue of error accumulation and
two-step generation as vocoder-based methods, and the low-
frequency phase information was not utilized. Therefore, in-
tegrating phase prediction into end-to-end speech BWE might
be a preferable option.
Hence, in this paper, we propose AP-BWE, a generative
adversarial network (GAN) based end-to-end speech BWE
model that achieves high-quality and efficient speech BWE
with the parallel extension of amplitude and phase spectra.
The generator features a dual-stream architecture, with each
stream incorporating ConvNeXt [38] as its foundational back-
bone. With narrowband log-amplitude and phase spectra as
input conditions respectively, the amplitude stream predicts
the residual high-frequency log-amplitude spectrum, while the
phase stream directly predicts the wrapped wideband phase
spectrum. Additionally, connections are established between
these two streams which has been proven to be crucial
for phase prediction [39]. To further enhance the subjectiveperceptual quality of the extended speech, we first employ the
multi-period discriminator (MPD) [40] at the waveform level.
Subsequently, inspired by the multi-resolution discriminator
proposed by Jang et al. [41] to alleviate the spectral over-
smoothing, we respectively design a multi-resolution am-
plitude discriminator (MRAD) and a multi-resolution phase
discriminator (MRPD) at the spectral level, aiming to enforce
the generator to produce more realistic amplitude and phase
spectra. Experimental results demonstrate that our proposed
AP-BWE surpasses state-of-the-art (SOTA) BWE methods in
terms of speech quality for target sampling rates of both 16
kHz and 48 kHz. It’s worth noting that while ensuring high
generation quality, our model exhibits significantly faster-than-
real-time generation efficiency. For waveform generation at a
sampling rate of 48 kHz, our model achieves a generation
speed of up to 292.3 times real-time on a single RTX 4090
GPU and 18.1 times real-time on a single CPU. Compared to
the SOTA speech BWE methods, we can also achieve at least
a fourfold acceleration on both GPU and CPU.
The main contributions of this work are twofold. On the
one hand, we propose to achieve speech BWE with parallel
modeling and optimization of amplitude and phase spectra,
which effectively avoids the amplitude-phase compensation
issues present in previous works, significantly enhancing the
quality of the extended speech. Additionally, benefiting from
the parallel phase estimation architecture and anti-wrapping
phase losses, we achieve the precise prediction of the wide-
band phase spectrum. Through the multi-resolution discrimi-
nation on the phase spectra, we further enhance the realism
of the extended phase at multiple resolutions. To the best
of our knowledge, we are the first to achieve the direct
extension of the phase spectrum. On the other hand, with the
all-convolutional architecture and all-frame-level operations,
our approach achieves a win-win situation in terms of both
generation quality and efficiency.
The rest of this paper is organized as follows. Section II
briefly reviews previous waveform-based and spectrum-based
BWE methods. In Section III, we give details of our proposed
AP-BWE framework. The experimental setup is presented in
Section IV , while Section V gives the results and analysis.
Finally, we give conclusions in Section VI.
II. R ELATED WORK
A. Waveform-based BWE Methods
Waveform-based BWE methods aim to directly predict
wideband waveforms from narrowband ones without any fre-
quency domain transformation. AudioUNet [19] proposed to
use a U-Net [42] based architecture to reconstruct wideband
waveforms without involving specialized audio processing
techniques. TFiLM [21] and AFiLM [22] proposed to use
recurrent neural networks (RNNs) and the self-attention mech-
anism [43] to capture the long-term dependencies, respectively.
Wang et al. [23] proposed to use an autoencoder convolutional
neural network (AECNN) based architecture and cross-domain
losses to predict and optimize the wideband waveforms,
respectively. However, the operations in the aforementioned
methods were all performed at the sample-point level, leading

--- PAGE 3 ---
3
Wideband WaveformNarrowband WaveformSTFTSincInterp.Narrowband Log-AmplitudeConv1dConv1dConvNeXtConvNeXt
ConvNeXtConvNeXtNarrowbandPhase
Conv1dConv1dWideband Log-Amplitude
WidebandPhaseArctan2iSTFTAmplitude Stream
Phase StreamGeneratorSTFT.Abs
STFT.Anglelog(Abs( ⋅⋅ ))
Angle( ⋅⋅ )
MPDMRADMRPDMulti-ResolutionAmplitude
Multi-ResolutionPhaseReal/FakeReal/FakeReal/FakeDiscriminator
exp( ⋅⋅ )
ReshapeLayer NormLayer Norm
Layer NormLayer Norm×N×N
×N×N
Fig. 1. The overall structure of the proposed AP-BWE. The Abs( ·)andAngle( ·)denote the amplitude and phase calculation functions, while log(·)and
exp(·)denote the logarithmic and exponential functions, respectively. The Arctan2 refers to the two-argument arc-tangent function.
to relatively lower generation efficiency when compared to
spectrum-based methods with frame-level operations.
Recently, diffusion probabilistic models [44], [45] have been
successfully applied to audio processing tasks. They have been
effectively utilized in speech BWE [46]–[48] by conditioning
the network of the noise predictor with narrowband wave-
forms, with remarkably high perceptual quality. The diffusion-
based methods decomposed the BWE process into two sub-
processes: the forward process, and the reverse process. In the
forward process, Gaussian noises were incrementally added
to the narrowband waveforms to obtain whitened latent vari-
ables. Conversely, the wideband waveforms were gradually
recovered by removing Gaussian noises step by step in the
reverse process. While these diffusion-based BWE methods
have demonstrated promising performance, they still required
numerous time steps in the reverse process for waveform
reconstruction, thereby imposing significant constraints on
generation efficiency. The comparison between our proposed
AP-BWE and these diffusion-based methods will be presented
in Section V-B.
B. Spectrum-based BWE Methods
Spectrum-based BWE methods aim to restore high-
frequency spectral parameters for reconstructing wideband
waveforms. However, as these spectral parameters were mostly
amplitude-related, recovering high-frequency phase informa-
tion remains the primary challenge. The most primitive method
involved replicating or mirror-inversing the low-frequency
phase, but such an approach introduces significant errors.
Another method entailed the use of a vocoder to recover
the phase from the extended amplitude-related spectrum. For
instance, NVSR [31] divided the BWE process into two
stages: 1) wideband mel-spectrogram prediction stage; 2)
vocoder-based waveform synthesis and post-processing stage.
Initially, NVSR employed ResUNet [49] to predict wideband
mel-spectrograms from narrowband ones. Subsequently, these
predicted mel-spectrograms were fed into a neural vocoder
to reconstruct high-resolution waveforms. Finally, the low-
frequency components of the high-resolution waveforms were
replaced with the original low-frequency ones.
Other methods involved recovering phase information from
the phase-contained spectrum. AERO [32] directly predictedthe wideband short-time complex spectrum from the narrow-
band one, implicitly recovering both amplitude and phase.
However, the lack of an explicit optimization method for the
phase can lead to the compensation effect [50] between am-
plitude and phase, thereby impacting the quality of generated
waveforms. mdctGAN [33] utilized the MDCT to encode both
amplitude and phase information to a real-valued MDCT spec-
trum. While successfully avoiding additional phase prediction
through the prediction of the wideband MDCT spectrum, the
performance of the MDCT spectrum in waveform generation
tasks has been demonstrated to be significantly weaker than
that of the STFT spectrum [51], which may be attributed to
the advantageous impact of an over-complete Fourier basis on
enhancing training stability [52].
Both waveform-based and spectrum-based methods men-
tioned above failed to achieve precise recovery of the high-
frequency phase, thereby inevitably limiting the quality of the
extended speech. Building upon our previous work on phase
prediction [34], we preliminarily tried to apply it to the BWE
task by predicting the wideband phase spectrum from the
extended log-amplitude spectrum. However, we found that this
two-stage prediction approach failed to fully leverage the low-
frequency phase information in narrowband waveforms, and its
prediction errors accumulate across stages. Therefore, in this
study, we opted to integrate the phase prediction method into
the end-to-end speech BWE.
III. M ETHODOLOGY
The overview of the proposed AP-BWE is illustrated in
Fig. 1. Given the narrowband waveform x∈RLas input, AP-
BWE aims to extend its bandwidth in the spectral domain as
well as increase its resolution in the time domain to predict the
wideband waveform y∈RnL. Here, nrefers to the sampling
rate ratio between wideband and narrowband waveforms (i.e.,
extension factor), while nLandLrepresent the length of the
wideband and narrowband waveforms, respectively. Specifi-
cally, the narrowband waveform xis first interpolated ntimes
using the sinc filter to match the temporal resolution of y. Sub-
sequently, the narrowband amplitude spectrum Xa∈RT×F
and wrapped phase spectrum Xp∈RT×Fare extracted
from the interpolated narrowband waveform through STFT,
where TandFdenote the number of temporal frames and
frequency bins, respectively. Through the mutual coupling of

--- PAGE 4 ---
4
the amplitude stream and the phase stream, AP-BWE predicts
wideband log-amplitude spectrum log(ˆYa)∈RT×Fas well
as wideband wrapped phase spectrum ˆYp∈RT×Fseparately
from log(Xa)andXp. Eventually the wideband waveform
ˆy∈RnLwas reconstructed through iSTFT. The details of the
model structure and training criteria are described as follows.
A. Model Structure
1) Generator: We denote the generator of our proposed
AP-BWE as G, and ˆy=G(x). As depicted in Fig. 1, the
generator Gcomprises a dual-stream architecture, which is
entirely based on convolutional neural networks. Both the
amplitude and phase streams utilize the ConvNeXt [38] as the
foundational backbone due to its strong modeling capability.
The original two-dimensional convolution-based ConvNeXt is
modified into a one-dimensional convolution-based version
and integrated it into our model. As depicted in Fig. 2, the
ConvNeXt block is a cascade of a large-kernel-sized depth-
wise convolutional layer and a pair of point-wise convolutional
layers that respectively expand and restore feature dimen-
sions. Layer normalization [53] and Gaussian error linear unit
(GELU) activation [54] are interleaved between the layers.
Finally, the residual connection is added before the output to
prevent the gradient from vanishing.
The amplitude stream comprises a convolutional layer,
NConvNeXt clocks, and another convolutional layer, with
the aim to predict the residual high-frequency log-amplitude
spectrum and add it to the narrowband log(Xa)to obtain
the wideband log-amplitude spectrum log(ˆYa). Differs slightly
from the amplitude stream, the phase stream incorporates
two output convolutional layers to respectively predict the
pseudo-real part component ˆY(r)
pand pseudo-imaginary part
component ˆY(i)
p, and further calculate the wrapped phase
spectrum ˆYpfrom them with the two-argument arc-tangent
(Arctan2) function:
ˆYp= arctanˆY(i)
p
ˆY(r)
p
−π
2·Sgn∗(ˆY(i)
p)·[Sgn∗(ˆY(r)
p)−1],(1)
where arctan( ·)denotes the arc-tangent function, and Sgn∗(x)
is a redefined symbolic function: when x≥0,Sgn∗(x) = 1 ,
otherwise Sgn∗(x) =−1. Additionally, connections are estab-
lished between two streams for information exchange, which
is crucial for phase prediction [39]. Finally, the predicted
wideband waveform ˆy∈RnLis reconstructed from ˆYaand
ˆYpusing iSTFT:
ˆy= iSTFT( ˆYa·ejˆYp)
= iSTFT( ˆYr+jˆYi),(2)
where ˆYr=ˆYa·cos(ˆYp)∈RT×FandˆYi=ˆYa·sin(ˆYp)∈
RT×Fdenote the real and imaginary parts of the extended
short-time complex spectrum, respectively.
2) Discriminator: Directly predicting amplitude and phase
and then reconstructing the speech waveform through iSTFT
can result in over-smoothed spectral parameters, manifesting
as a robotic or muffled quality in the reconstructed waveforms.
To this end, we utilize discriminators defined both in the
Depth-wise Conv1dLayer NormPoint-wise Conv1dGELUActivationPoint-wise Conv1dConvNeXt  BlockFig. 2. Details of the ConvNeXt block [38]: Each ConvNeXt block consists
of a7×1depth-wise convolution, followed by layer normalization, a 1×1
point-wise convolution for dimensionality projection with an expansion factor
of 3, a GELU activation layer, and another 1×1point-wise convolution for
dimensionality restoration followed by residual connection.
Conv2d(64, (7, 5), stride=(2, 2))Leaky ReLUConv2d(64, kernel_size=k, stride=s)Leaky ReLU4 ×× k ∈∈ [(5, 3), (5, 3), (3, 3), (3, 3)]s ∈∈ [(2, 1), (2, 2), (2, 1), (2, 2)]Conv2d(1, (3, 3))Real/FakeSTFT.AbsMRADRaw Waveform
Conv2d(c, (5, 1), stride=(3, 1))Leaky ReLU4 ××c ∈∈ [32, 128, 512, 1024] Conv2d(1024, (5, 1), stride=(1, 1))Leaky ReLUConv2d(1, (3, 1))Real/FakeMPD
Reshape
Real/FakeConv2d(64, (7, 5), stride=(2, 2))Leaky ReLUConv2d(64, kernel_size=k, stride=s)Leaky ReLU4 ×× k ∈∈ [(5, 3), (5, 3), (3, 3), (3, 3)]s ∈∈ [(2, 1), (2, 2), (2, 1), (2, 2)]Conv2d(1, (3, 3))STFT.AngleMRPD
Fig. 3. Details of the discriminators. The parameters inside the parentheses
for each convolutional layer respectively represent the number of channels,
kernel size, and stride.
spectral domain and time domain to guide generator Gin
generating spectra and waveforms that closely resemble real
ones. Firstly, considering that the speech signal is composed
of sinusoidal signals with various frequencies, with some
frequency bands generated through BWE. Due to the sta-
tistical characteristics of speech signals varying in different
frequency bands, we employ an MPD [40] to capture periodic
patterns, with the aim of matching the natural wideband speech
across multiple frequency bands. Moreover, since the statistical
characteristics of amplitude and phase also differ across fre-
quency bands, and the sole utilization of MPD cannot cover
all frequency bands, we consequently define discriminators
on both amplitude and phase spectra. Drawing inspiration
from the multi-resolution discriminator [41], we respectively
introduce MRAD and MRPD, with the aim to capture full-
band amplitude and phase patterns at various resolutions. The
details of MPD, MRAD, and MRPD are described as follows.
•Multi-Period Discriminator : As depicted in Fig. 3, the
MPD contains multiple sub-discriminators, each of which
comprises a waveform two-dimensional reshaping module,
multiple convolutional layers with an increasing number
of channels, and an output convolutional layer. Firstly,
the reshaping module reshapes the one-dimensional raw
waveform into a two-dimensional format by sampling with a
period p, which is set to prime numbers to prevent overlaps.
Subsequently, the reshaped waveform undergoes multiple

--- PAGE 5 ---
5
convolutional layers with leaky rectified linear unit (ReLU)
activation [55] before finally producing the discriminative
score, which indicates the likelihood that the input data is
real.
•Multi-Resolution Discriminators : As depicted in Fig. 3,
both MRAD and MRPD share a unified structure. They
both consist of multiple sub-discriminators, each comprising
a spectrum extraction module and multiple convolutional
layers interleaved with leaky ReLU activation to capture
features along both temporal and frequency axes. The raw
waveform first undergoes an initial transformation into am-
plitude or phase spectra using STFT with diverse parameter
sets, encompassing FFT point number, window size, and hop
size. Subsequently, the multi-resolution amplitude or phase
spectra are processed through multiple convolutional layers
to yield the discriminative score.
B. Training Criteria
1) Spectrum-based Losses: We first define loss functions
in the spectral domain to capture time-frequency distributions
and generate realistic spectra.
•Amplitude Spectrum Loss : The amplitude spectrum loss is
the mean square error (MSE) of the wideband log-amplitude
spectrum log(Ya)∈RT×Fand the extended log-amplitude
spectrum log(ˆYa), which is defined as:
LA=1
TFE(Ya,ˆYa)
∥log Ya
ˆYa
∥2
F
. (3)
•Phase Spectrum Loss : Considering the phase wrapping
issue, we follow our previous work [34] to use three anti-
wrapping losses to explicitly optimize the wrapped phase
spectrum, which are respectively defined as the mean ab-
solute error (MAE) between the anti-wrapped wideband
and extended instantaneous phase (IP) spectra YpandˆYp,
group delay (GD) spectra YGDandˆYGD, and instantaneous
angular frequency (IAF) spectra YIAF andˆYIAF:
LIP=1
TFE(Yp,ˆYp)
∥fAW(Yp−ˆYp)∥1
, (4)
LGD=1
TFE(YGD,ˆYGD)
∥fAW(YGD−ˆYGD))∥1
,(5)
LIAF=1
TFE(YIAF,ˆYIAF)
∥fAW(YIAF−ˆYIAF))∥1
,
(6)
where (YGD,ˆYGD) = (∆ DFYp,∆DFˆYp)and
(YIAF,ˆYIAF) = (∆ DTYp,∆DTˆYp). The ∆DF and
∆DT represent the differential operator along the
frequency and temporal axes, respectively. The fAW(x)
denotes the anti-wrapping function, which is defined as
fAW(x) =|x−2π·round x
2π
|, x∈R. The final phase
spectrum loss is the sum of these three anti-wrapping
losses:
LP=LIP+LGD+LIAF. (7)
•Complex Spectrum Loss : To further optimize the am-
plitude and phase within the complex spectrum and en-
hance the spectral consistency of iSTFT, we define theMSE loss between the wideband short-time complex spec-
trum (Yr,Yi)∈RT×F×2and extend short-time complex
spectrum (ˆYr,ˆYi)∈RT×F×2as well as the MSE loss
between (ˆYr,ˆYi)and re-extracted short-time complex spec-
trum (ˆY′
r,ˆY′
i)∈RT×F×2from the extended waveform ˆy.
So the complex spectrum loss is defined as:
LC=1
TFE(Yr,Yi),(ˆYr,ˆYi)
∥(Yr,Yi)−(ˆYr,ˆYi)∥2
F
+1
TFE(ˆYr,ˆYi),(ˆY′r,ˆY′
i)
∥(ˆYr,ˆYi)−(ˆY′
r,ˆY′
i)∥2
F
.(8)
•Final Spectral Loss : The final spectral loss is the linear
combination of the spectrum-based losses mentioned above:
LS=λALA+λPLP+λCLC, (9)
where λA,λP, and λCare hyper-parameters and we set
them to 45, 100, and 45, respectively.
2) GAN-based Losses:
•GAN Loss : For brevity, we represent MPD, MRAD, and
MRPD collectively as D. The discriminator Dand generator
Gare trained alternately. The discriminator is trained to
classify wideband samples as 1 and samples extended by
the generator as 0; conversely, the generator is trained to
generate samples that approach being classified as 1 by the
discriminator as closely as possible. We use the hinge GAN
loss [56] which is defined as:
Ladv(D;G) =Ex
max(0 ,1 +D(G(x))
+Ey
max(0 ,1−D(y))
,(10)
Ladv(G;D) =Ex
max(0 ,1−D(G(x)))
. (11)
•Feature Matching Loss : To encourage the generator to
produce samples that not only fool the discriminator but
also match the features of real samples at multiple levels
of abstraction, we define the feature matching loss [57]
between the features extracted from the natural wideband
waveforms and those from the extended waveforms at
certain intermediate layers of the discriminator as follows:
LFM(G;D) =E(x,y)MX
i=11
Ni∥Di(y)−Di(G(x))∥1
,
(12)
where Mdenotes the number of layers in the discriminator,
DiandNidenotes the features and the number of features
in the i-th layer of the discriminator, respectively.
3) Final Loss: Since the discriminator Dis a set of sub-
discriminators of MPD, MRAD, and MRPD, the final losses
of the generator and discriminator are defined as:
LG=KX
k=1
λadvLadv(G;Dk) +λFMLFM(G;Dk)
+λSLS,
(13)
LD=KX
k=1Ladv(Dk;G). (14)

--- PAGE 6 ---
6
where Kdenotes the numbers of sub-discriminators, and
Dkdenotes the k-th sub-discriminator in MPD, MRAD, and
MRPD. λadv,λFM, and λSare hyper-parameters and in
all our experiments, we set λS= 1 . For MPD, we set
λadv= 1 andλFM= 1, while for MRAD and MRPD, we
setλadv= 0.1andλFM= 0.1.
IV. E XPERIMENTAL SETUP
A. Data Configuration
We trained all models on the VCTK-0.92 dataset [58],
which contains approximately 44 hours of speech recordings
from 110 speakers with diverse accents. Adhering to the
data preparation approach adopted in previous speech BWE
studies [31], [32], [46]–[48], we exclusively utilized the mic1-
microphone data and excluded speakers p280andp315due to
technical issues. Among the remaining 108 speakers, the last 8
were allocated for testing, while the remaining 100 were used
for training. Given the historical focus of early speech BWE
methods on a sampling rate of 16 kHz and the contemporary
emphasis on higher target sampling rates (e.g., 44.1 kHz and
48 kHz) in recent methods, we employed the original VCTK-
0.92 dataset with a 48 kHz sampling rate for high-sampling-
rate BWE experiments. Subsequently, we downsampled the
VCTK-0.92 dataset to 16 kHz for low-sampling-rate BWE
experiments.
To generate pairs of wideband and narrowband speech
signals, we employed a sinc filter to eliminate high-frequency
components in the speech signals above a specified bandwidth.
This process retained only the low-frequency components,
ensuring no aliasing occurred. For experiments targeting a 16
kHz sampling rate, we configured the downsampling rate nto
2, 4, and 8, corresponding to the extension from 8 kHz, 4 kHz,
and 2 kHz to 16 kHz, respectively. In experiments aiming for
a 48 kHz sampling rate, we set the downsampling rate nto
2, 3, 4, and 6, denoting the extension from 24 kHz, 16 kHz,
12 kHz, and 8 kHz to 48 kHz, respectively.
B. Model Details
We used the same configuration for experiments with target
sampling rates of 16 kHz and 48 kHz. For training our pro-
posed AP-BWE model, all the audio clips underwent silence
trimming with VCTK silence labels1and sliced into 8000-
sample-point segments. To extract the amplitude and phase
spectra from raw waveforms, we used STFT with the FFT
point number of 1024, Hanning window size, and hop size
of 320 and 80 sample points, respectively. So for the training
set, the number of frequency bins Fis 513, and the number
of temporal frames Tis 101.
For the generator, the number of the ConvNeXt block Nwas
set to 8. The period pfor each sub-discriminator in the MPD
was configured as 2, 3, 5, 7, and 11. In the case of MRAD
and MRPD, the FFT point numbers, rectangular window sizes,
and hop sizes of the STFT parameter sets were set to [512,
128, 512], [1024, 256, 1024], and [2048, 512, 2048] for the
three sub-discriminators, respectively. Both the generator and
1https://github.com/nii-yamagishilab/vctk-silence-labels.discriminator were trained until 500k steps using the AdamW
optimizer [59], with β1= 0.8,β2= 0.99, and weight decay
λ= 0.01. The learning rate was set initially to 2×10−4and
scheduled to decay with a factor of 0.999 at every epoch.2
C. Evaluation Metrics
1) Metrics on Speech Quality: We comprehensively evalu-
ated the quality of the extended speech signals using metrics
defined on the amplitude spectra, phase spectra, and recon-
structed speech waveforms, including:
•Log-Spectral Distance (LSD) : LSD is a commonly used
objective metric in the BWE task. Given the wideband and
extended speech waveform yandˆy, their corresponding
amplitude spectra Ya∈RT×FandˆYa∈RT×Fwere first
extracted using STFT with the FFT point number of 2048,
Hanning window size of 2048, and hop size of 512. Then
the LSD is defined as:
LSD =1
TTX
t=1vuut1
FFX
f=1
log10 Ya[t, f]
ˆYa[t, f]2
(15)
•Anti-Wrapping Phase Distance (A WPD) : To assess the
model’s capability of recovering high-frequency phase, on
the basis of the anti-wrapping losses defined in Eq. 4-6, we
defined three anti-wrapping phase metrics to evaluate the
extended phase instantaneous error as well as its continuity
in both the temporal and frequency domains:
AWPD IP=1
TTX
t=1vuut1
FFX
f=1f2
AW(Yp[t, f]−ˆYp[t, f]),(16)
AWPD GD=1
TTX
t=1vuut1
FFX
f=1f2
AW(YGD[t, f]−ˆYGD[t, f]),
(17)
AWPD IAF=1
TTX
t=1vuut1
FFX
f=1f2
AW(YIAF[t, f]−ˆYIAF[t, f]),
(18)
where all the spectra are extracted using the same STFT
parameters as those used in LSD.
•Virtual Speech Quality Objective Listener (ViSQOL) : To
access the overall perceived audio quality of the extended
speech signals in an objective manner, we employed the
ViSQOL3[60] which uses a spectral-temporal measure of
similarity between a reference and a test speech signal to
produce a mean opinion score - listening quality objective
(MOS-LQO) score. For the audio mode of ViSQOL at a
required sampling rate of 48 kHz, the MOS-LQO score
ranges from 1 to 4.75, the higher the better. For the speech
mode of ViSQOL at a required sampling rate of 16 kHz,
the MOS-LQO score ranges from 1 to 5.
•Mean Opinion Score (MOS) : To further subjectively access
the overall audio quality, MOS tests were conducted to
2Source codes and audio samples of the proposed AP-BWE can be accessed
at https://github.com/yxlu-0102/AP-BWE.
3https://github.com/google/visqol.

--- PAGE 7 ---
7
TABLE I
EXPERIMENTAL RESULTS IN SPEECH QUALITY (LSD AND VISQOL) AND GENERATION EFFICIENCY (RTF AND FLOP S)FOR BWE M ETHODS
EVALUATED ON THE VCTK D ATASET WITH TARGET SAMPLING RATE OF 16KHZ, W HERE IN RTF (a×) REPRESENTING aTIMES REAL-TIME
Method8kHz→16kHz 4kHz→16kHz 2kHz→16kHzRTF(CPU) RTF(GPU) FLOPs
LSD ViSQOL LSD ViSQOL LSD ViSQOL
sinc 1.80 4.34 2.68 3.52 3.15 2.73 - - -
TFiLM [21] 1.31 4.46 1.65 3.84 1.97 3.10 0.3287 (3.04 ×) 0.0244 (41.01 ×) 232.85G
AFiLM [22] 1.24 4.39 1.63 3.83 1.79 2.75 0.5029 (1.99 ×) 0.0477 (20.96 ×) 260.76G
NVSR [31] 0.79 4.52 0.95 4.11 1.10 3.41 0.7577 (1.32 ×) 0.0512 (19.54 ×) 34.28G
AERO [32] 0.87 4.57 1.00 4.19 - - 0.4395 (2.28 ×) 0.0217 (46.01 ×) 141.77G
AP-BWE* 0.71 4.66 0.88 4.28 0.99 3.770.0338 (29.61 ×) 0.0026 (382.56 ×) 5.97GAP-BWE 0.69 4.71 0.87 4.30 0.99 3.76
evaluate the naturalness of the wideband speech and speech
waveforms extended by the speech BWE models. Defining
the extension ratio as the ratio between the target sampling
rate and the source sampling rate, we selected configurations
with the highest extension ratios for subjective evaluations.
In each MOS test, twenty utterances from the test set
were evaluated by at least 30 native English listeners on
the crowd-sourcing platform Amazon Mechanical Turk. For
each utterance, listeners were asked to rate a naturalness
score between 1 and 5 with an interval of 0.5. All the MOS
results were reported with 95% confidence intervals (CI).
We also conducted paired t-tests to assess the significance
of differences between our proposed AP-BWE and the
baseline models, reporting p-values to indicate the statistical
significance of these comparisons.
2) Metrics on Generation Efficiency: We first used the real-
time factor (RTF) to evaluate the inference speed of the model.
The RTF is defined as the ratio of the total inference time for
processing narrowband source signals into wideband output
signals, to the total duration of the wideband signals. In our
implementation, RTF was calculated using the complete test
set on an RTX 4090 GPU and an Intel(R) Xeon(R) Silver
4310 CPU (2.10 GHz). Additionally, we used floating point
operations (FLOPs) to assess the computational complexity
of the model. All the FLOPs were calculated using 1-second
speech signals as inputs to the models.
3) Metrics on Speech Intelligibility: The main frequency
components of human speech are concentrated within the
range of approximately 300 Hz to 3400 Hz. This frequency
range encompasses crucial information for vowels and con-
sonants, significantly impacting speech intelligibility. Conse-
quently, we analyzed the intelligibility of waveforms extended
by speech BWE methods with the target sampling rate of 16
kHz. Firstly, we employed an advanced ASR model, Whisper
[61] to transcribe the extended 16 kHz speech signals into
corresponding texts. Subsequently, we calculated the word
error rate (WER) and character error rate (CER) based on
the transcription results. Additionally, short-time objective
intelligibility (STOI) was also included as an objective metric
to indicate the percentage of speech signals that are correctly
understood.V. R ESULTS AND ANALYSIS
A. BWE Experiments Targeting 16 kHz
1) Baseline Methods: For BWE targeting a 16 kHz sam-
pling rate, we first used the sinc filter interpolation as the
lower-bound method, and further compared our proposed AP-
BWE with two waveform-based methods (TFiLM [21] and
AFiLM [22]), a vocoder-based method (NVSR [31]), and a
complex-spectrum-based method (AERO [32]). For TFiLM
and AFiLM, we used their official implementations4. How-
ever, their original papers used the old-version VCTK dataset
[62] and employed subsampling to obtain the narrowband
waveforms, which aliased high-frequency components. Thus,
they performed not a strict BWE task but an SR task. For a
fair comparison, we re-trained the TFiLM and AFiLM models
with our data-preprocessing manner on the VCTK-0.92 dataset
until 50 epochs. For AERO and NVSR, we used their official
implementations5 6. Notably, AERO did not conduct the
experiment at a 2 kHz source sampling rate, and thus, this
result was excluded from our analysis.
Additionally, considering some recent BWE methods [31],
[47], [48] demonstrated the ability to handle various source
sampling rates with a single model, we also trained our AP-
BWE with the source sampling rate uniformly sampled from
2 kHz to 8 kHz, denoted as AP-BWE*, to unifiedly extend
speech signals at all these three sampling rates to 16 kHz.
2) Evaluation on Speech Quality:
•Objective Evaluation : As depicted in Table I, our proposed
AP-BWE achieved the best performance in speech qual-
ity with all kinds of source sampling rates. Compared to
sinc filter interpolation, our proposed AP-BWE exhibited
significant improvements of 61.7%, 67.5%, and 68.6% in
terms of LSD as well as 8.5%, 22.2%, and 37.7% in terms
ViSQOL for source sampling rates of 8 kHz, 4 kHz, and
2 kHz, respectively. With the narrowing of source speech
bandwidth, the performance advantage of our proposed AP-
BWE became more pronounced, indicating the powerful
BWE capability of our model. In general, waveform-based
methods (TFiLM and AFiLM) performed less effectively
4https://github.com/ncarraz/AFILM.
5https://github.com/haoheliu/ssr eval.
6https://github.com/slp-rl/aero.

--- PAGE 8 ---
8
TABLE II
PHASE -RELATED EVALUATION RESULTS FOR BWE METHODS EVALUATED ON THE VCTK D ATASET WITH TARGET SAMPLING RATE OF 16KHZ
Method8kHz→16kHz 4kHz→16kHz 2kHz→16kHz
AWPD IPAWPD GD AWPD IAF AWPD IPAWPD GD AWPD IAF AWPD IPAWPD GD AWPD IAF
sinc 1.27 0.87 1.06 1.57 1.18 1.28 1.69 1.34 1.38
TFiLM [21] 1.28 0.91 1.07 1.54 1.18 1.27 1.68 1.35 1.37
AFiLM [22] 1.32 0.98 1.11 1.54 1.19 1.27 1.70 1.38 1.39
NVSR [31] 1.38 0.89 1.11 1.61 1.14 1.29 1.72 1.29 1.38
AERO [32] 1.31 0.93 1.08 1.56 1.15 1.27 - - -
AP-BWE* 1.27 0.86 1.05 1.53 1.12 1.25 1.67 1.27 1.35
AP-BWE 1.26 0.84 1.04 1.53 1.12 1.25 1.67 1.27 1.35
than spectrum-based methods (NVSR, AERO, and our pro-
posed AP-BWE), indicating the importance of capturing
time-frequency domain characteristics for the BWE task.
Within spectrum-based methods, NVSR, relying on high-
frequency mel-spectrogram prediction and vocoder-based
waveform reconstruction, demonstrated advantages in the
LSD metric assessing the extended amplitude. However,
the vocoder-based phase recovery was not as effective as
the complex spectrum-based approach, so it lagged behind
AERO in the ViSQOL metric assessing overall speech
quality. Compared to AERO, our AP-BWE, benefiting from
explicit amplitude and phase optimizations, successfully
avoided the compensation effects between amplitude and
phase and consequently achieved better performance in both
spectral and waveform-based metrics. Worth noting is that
the unified AP-BWE* model exhibited only a slight decrease
in performance compared to AP-BWE, and even achieved
the highest ViSQOL score at the source sampling rate
of 2 kHz. This indicated that our model exhibited strong
adaptability to the source sampling rate.
The key distinction between our approach and others lay
in our implementation of explicit high-frequency phase
extension. As illustrated in Table II, our proposed AP-BWE
consistently outperformed other baselines across various
source sampling rates, demonstrating superior performance
in terms of instantaneous phase error and phase continuity
along both time and frequency axes. For the AP-BWE*,
only slight decreases in the AWPD metrics were observed
when the source sampling rate was 8 kHz. Under other
source sampling rate conditions, the metrics were the same
as AP-BWE, indicating the robustness of our unified model
on phase. Remarkably, for other baseline methods, some
of their AWPD metrics exhibited degradation compared
to those of the source sinc-interpolated waveforms. This
suggests a limitation in the effective utilization of low-
frequency phase information during the speech BWE pro-
cess by these baseline methods. Moreover, all methods
here directly generated waveforms without substituting the
original low-frequency components, so their low-frequency
phase might be partially compromised, leading to a signif-
icant impact on the quality of the extended speech. This
observation underscored the critical importance of precise
phase prediction and optimization in the context of BWE
tasks, further emphasizing the advantage of our approach.TABLE III
MOS T ESTS RESULTS FOR BWE M ETHODS WITH SOURCE SAMPLING
RATE OF 2KHZ AND TARGET SAMPLING RATE OF 16KHZ
Methods MOS (CI)
sinc 3.34 ( ±0.09)
TFiLM [21] 3.41 ( ±0.09)
AFiLM [22] 3.50 ( ±0.08)
NVSR [31] 3.68 ( ±0.08)
AP-BWE 3.93 ( ±0.07)
Ground Truth 4.01 ( ±0.06)
•Subjective Evaluation : To compare the BWE capabilities
of our proposed AP-BWE with those of other baseline
models, we conducted MOS tests on natural wideband 16
kHz speech waveforms, as well as on speech waveforms
extended by AP-BWE and other baseline methods at a
source sampling rate of 2 kHz. The subjective experimental
results are presented in Table III. For a more intuitive
comparison, we visualized the spectrograms of these speech
waveforms, as illustrated in Fig.4. According to the MOS
results, our proposed AP-BWE outperformed other baseline
models very significantly in terms of subjective quality
(p < 0.01). The MOS of TFiLM and AFiLM showed
a slight improvement to that of the sinc filter interpola-
tion, demonstrating their insufficient modeling capability for
high-frequency components, particularly in the case of high-
frequency unvoiced segments as shown in the in Fig. 4.
NVSR achieved a decent MOS score compared to TFiLM
and AFiLM but still lagged behind our proposed AP-BWE.
We can observe that, compared to the spectrograms of
natural wideband speech and AP-BWE-extended speech,
the NVSR-extended speech spectrogram exhibited relatively
low energy in both high-frequency unvoiced segments (e.g.,
0.2∼0.3s) and low-frequency harmonics (e.g., 1.1 ∼1.5s).
As a result, the speech signals extended by NVSR would
sound duller, negatively impacting its perceived speech qual-
ity. In contrast, our proposed AP-BWE effectively extended
more robust harmonic structures, demonstrating their strong
modeling capabilities and highlighting the effectiveness of
explicit predictions of amplitude and phase spectra.
3) Evaluation on Generation Efficiency: We respectively
evaluated the generation efficiency of our proposed AP-BWE
as well as other baseline methods as outlined in Table I.

--- PAGE 9 ---
9
0 0.5 1.0 1.5 2.0 2.5
Time(s)01248Frequency(kHz)Wideband
0 0.5 1.0 1.5 2.0 2.5
Time(s)01248TFiLM
0 0.5 1.0 1.5 2.0 2.5
Time(s)01248AFiLM
0 0.5 1.0 1.5 2.0 2.5
Time(s)01248NVSR
0 0.5 1.0 1.5 2.0 2.5
Time(s)01248AP-BWE
Fig. 4. Spectrogram visualization of the original wideband 16 kHz speech waveform and speech waveforms extended by baseline methods and our proposed
AP-BWE from the source sampling rate of 2 kHz.
TABLE IV
EXPERIMENTAL RESULTS IN INTELLIGIBILITY FOR BWE METHODS EVALUATED ON THE VCTK D ATASET WITH TARGET SAMPLING RATE OF 16KHZ
Method8kHz→16kHz 4kHz→16kHz 2kHz→16kHz
WER (%) CER (%) STOI (%) WER (%) CER (%) STOI (%) WER (%) CER (%) STOI (%)
sinc 3.67 1.67 99.76 11.45 7.08 89.91 47.43 33.56 79.04
TFiLM [21] 3.69 1.69 99.24 11.32 7.24 91.27 45.95 33.58 80.23
AFiLM [22] 3.67 1.67 98.54 9.28 5.53 90.51 45.16 33.01 76.83
NVSR [31] 4.38 2.02 98.84 13.56 8.51 92.04 59.53 44.43 82.38
AERO [32] 3.97 1.84 99.38 9.78 5.51 93.74 - - -
AP-BWE 3.72 1.67 99.77 6.69 3.54 94.75 36.69 25.61 87.00
Ground Truth 3.07 1.26 100.00 3.07 1.26 100.00 3.07 1.26 100.00
Considering the inference speed, since NVSR divided the
BWE process into the mel-spectrogram extension stage and
vocoder synthesis stage, it lagged far behind other end-to-end
methods. For TFiLM and AFiLM, since they both operated
on the waveform level and utilized RNNs or self-attention
to capture long-term dependencies, their inference speeds
were consequently constrained. For AERO, although it and
our proposed AP-BWE both operated on the spectral level,
the utilization of transformer [43] blocks in multiple layers
severely slowed down its inference speed. Nevertheless, our
AP-BWE model, based on fully convolutional networks and
all-frame-level operations, has achieved an astonishingly high-
speed waveform generation (29.61 times real-time speed on
CPU and 382.56 times on GPU), far surpassing other baseline
methods. Considering the models’ computational complexity,
the FLOPs of AP-BWE were at least five times smaller
than those of the baseline models, further demonstrating the
advantage of our proposed model in generation efficiency.
4) Evaluation on Speech Intelligibility: As shown in Ta-
ble IV, it is obvious that our proposed AP-BWE exhibited
a remarkable improvement in terms of intelligibility met-
rics compared to baseline models. Under the condition of
extending from 8 kHz to 16 kHz, the performance of the
sinc filter interpolation was already very close to the Ground
Truth. Our proposed AP-BWE and other baseline models
struggled to further improve WER and CER on top of the
waveform interpolated by the sinc filter, suggesting that the
ASR model focused on information from frequencies below
4 kHz for transcription. When the source sampling rate was
further reduced to 4 kHz and 2 kHz, all the baseline models
showed slight improvements in WER and CER compared to
sinc filter interpolation, except for NVSR. The decline in
NVSR’s performance in WER and CER was due to its useof a vocoder to restore the waveform, which made the low-
frequency components unnatural, but its STOI metric was still
improved. Overall, these baseline models demonstrated limited
extension capabilities under the extremely high extension
ratio. However, our proposed AP-BWE significantly improved
WER, CER, and STOI by 41.57%, 50.00%, 5.38% at the
4 kHz source sampling rate, and by 22.64%, 23.69%, and
10.07% at the 2 kHz source sampling rate, compared to sinc
filter interpolation. This indicated that benefiting from our
precise phase prediction, our model possessed strong harmonic
restoration capabilities, reconstructing the key information of
vowels and consonants as well as significantly enhancing the
intelligibility of the extended speech.
B. BWE Experiments Targeting 48 kHz
1) Baseline Methods: For BWE targeting a 48 kHz sam-
pling rate, the sinc filter interpolation was still used as the
low-bound method. We subsequently compared our proposed
AP-BWE with three diffusion-based methods (NU-Wave [46],
NU-Wave 2 [47], and UDM+ [48]) and an MDCT-spectrum-
based method (mdctGAN [33]). For NU-Wave, we used the
community-contributed checkpoints from their official imple-
mentation7. Notably, NU-Wave did not conduct the experi-
ment at source sampling rates of 8 kHz and 12 kHz, so we
excluded these results from our analysis. For NU-Wave 2 and
UDM+, we used the reproduced NU-Wave 2 checkpoint and
official UDM+ checkpoint8. It is worth noting that in the
original paper of mdctGAN [33], the mdctGAN model was
trained on the combination of the VCTK training set and the
HiFi-TTS dataset, and tested on the VCTK test set. Here, for a
7https://github.com/maum-ai/nuwave.
8https://github.com/yoyololicon/diffwave-sr.

--- PAGE 10 ---
10
TABLE V
EXPERIMENTAL RESULTS IN SPEECH QUALITY (LSD AND VISQOL) AND GENERATION EFFICIENCY (RTF AND FLOP S)FOR BWE M ETHODS
EVALUATED ON THE VCTK D ATASET WITH TARGET SAMPLING RATE OF 48KHZ, W HERE IN RTF (a×) REPRESENTING aTIMES REAL-TIME
Method24kHz →48kHz 16kHz →48kHz 12kHz →48kHz 8kHz→48kHzRTF(CPU) RTF(GPU) FLOPs
LSD ViSQOL LSD ViSQOL LSD ViSQOL LSD ViSQOL
sinc 2.17 2.99 2.57 2.26 2.75 2.09 2.94 2.07 - - -
NU-Wave [46] 0.85 3.18 0.99 2.36 - - - - 95.57 (0.01 ×) 0.5018 (1.99 ×) 4039.13G
NU-Wave2 [47] 0.72 3.74 0.86 3.00 0.94 2.75 1.09 2.48 92.58 (0.01 ×) 0.5195 (1.92 ×) 1385.27G
UDM+ [48] 0.64 4.02 0.79 3.35 0.88 3.08 1.03 2.81 74.03 (0.01 ×) 0.8335 (1.20 ×) 2369.50G
mdctGAN [33] 0.71 3.69 0.83 3.27 0.85 3.12 0.93 3.03 0.2461 (4.06 ×) 0.0129 (77.80 ×) 103.38G
AP-BWE* 0.62 4.17 0.72 3.63 0.79 3.46 0.85 3.320.0551 (18.14 ×) 0.0034 (292.28 ×) 17.87GAP-BWE 0.61 4.25 0.72 3.70 0.78 3.46 0.84 3.35
TABLE VI
EXPERIMENTAL RESULTS FOR THE BAND -WISE ANALYSIS WITH SOURCE SAMPLING RATE OF 8KHZ AND TARGET SAMPLING RATE OF 48KHZ
Method4kHz∼8kHz 8kHz∼12kHz 12kHz ∼24kHz
LSD AWPD IPAWPD GDAWPD IAF LSD AWPD IPAWPD GDAWPD IAF LSD AWPD IPAWPD GDAWPD IAF
NU-Wave2 [47] 1.35 1.81 1.48 1.47 1.24 1.81 1.48 1.47 1.09 1.82 1.47 1.46
UDM+ [48] 1.21 1.80 1.45 1.46 1.26 1.81 1.46 1.46 1.03 1.82 1.46 1.46
mdctGAN [33] 1.11 1.80 1.45 1.46 1.07 1.81 1.46 1.47 0.93 1.82 1.46 1.46
AP-BWE 0.98 1.75 1.41 1.44 0.98 1.81 1.44 1.46 0.86 1.82 1.45 1.46
fair comparison, we re-trained all the mdctGAN models solely
on the VCTK training set following its official implementation
9. In addition, the AP-BWE* was trained using randomly
selected sampling rates from 8 kHz, 12 kHz, 16 kHz, and
24 kHz to handle inputs of various resolutions.
2) Evaluation on Speech Quality:
•Objective Evaluation :
As depicted in Table V, for the high-sampling-rate wave-
form generation at 48 kHz, our proposed AP-BWE still
achieved the SOTA performance in objective metrics, irre-
spective of the source sampling rates. In general, compared
to the baseline models, our approach exhibited a notably
significant improvement in ViSQOL, particularly under
lower extension ratios, which underscored the substantial
impact of precise phase prediction on speech quality. For
diffusion-based methods, since both NU-Wave2 and UDM+
implemented a single model for extension across different
source sampling rates, we compared our unified AP-BWE*
model with them. Compared to NU-Wave 2 and UDM+,
our AP-BWE* model exhibited growing superiority in LSD
as the source sampling rate decreased. This suggested that
diffusion-based methods, operating at the waveform level,
struggled to effectively recover spectral information in sce-
narios with restricted bandwidth. Although both mdctGAN
and our proposed AP-BWE were spectrum-based methods,
AP-BWE significantly outperformed it across all source
sampling rates. Especially in terms of the overall speech
quality, our proposed AP-BWE surpassed mdctGAN by
15.2%, 13.1%, 10.9%, and 10.6% in ViSQOL at source
sampling rates of 24 kHz, 16 kHz, 12 kHz, and 8 kHz,
respectively. This suggested that STFT spectra were more
suitable for waveform generation tasks compared to MDCT
spectra. Additionally, similar to the results at 16 kHz, our
9https://github.com/neoncloud/mdctGAN.unified AP-BWE* model demonstrated competence across
different sampling rate inputs, with only a slight decline in
quality compared to AP-BWE, reaffirming the adaptability
of our approach to source sampling rates.
Unlike the strong harmonic structure observed in the high-
frequency components of 16 kHz waveforms, the high-
frequency portion of 48 kHz waveforms exhibited more
randomness. In our preliminary experiments, we observed
the phase metrics of extended 48 kHz waveforms show
minimal variation between systems, especially in scenarios
with relatively higher source sampling rates. Therefore, with
the source sampling rate of 8 kHz, we calculated LSD
and AWPD separately for different frequency bands of the
extended 48 kHz waveform to assess the performance of
these models within different frequency ranges, and the
evaluation results are depicted in Table VI. For the LSD
metric, our AP-BWE outperformed other baseline models
within each frequency band. Regarding the AWPD metrics,
we only exhibited an advantage in the 4 kHz ∼8 kHz
frequency band, while the differences between systems were
minimal in the 8 kHz ∼12 kHz and 12 kHz ∼24 kHz
frequency bands. This indicated that our proposed AP-BWE,
benefiting from explicit phase prediction, was capable of ef-
fectively recovering the harmonic structure in the waveform,
thereby significantly improving the speech quality in the
mid-to-low-frequency range. For high-frequency phases, due
to their strong randomness, the current methods exhibited
comparable predictive capabilities.
•Subjective Evaluation : As shown in the right half of
Table VII, we conducted MOS tests on the wideband 48 kHz
speech waveform, along with speech waveforms extended
by AP-BWE and other baseline methods at a source sam-
pling rate of 8 kHz. We also visualized the corresponding
spectrograms, as illustrated in Fig. 5. In this configura-
tion, the initial 4 kHz bandwidth already contained the

--- PAGE 11 ---
11
0 0.5 1.0 1.5 2.0 2.5
Time(s)04681224Frequency(kHz)Wideband
0 0.5 1.0 1.5 2.0 2.5
Time(s)04681224NU-Wave 2
0 0.5 1.0 1.5 2.0 2.5
Time(s)04681224UDM+
0 0.5 1.0 1.5 2.0 2.5
Time(s)04681224mdctGAN
0 0.5 1.0 1.5 2.0 2.5
Time(s)04681224AP-BWE
Fig. 5. Spectrogram visualization of the original wideband 48 kHz speech waveform and speech waveforms extended by baseline methods and our proposed
AP-BWE from the source sampling rate of 8 kHz.
TABLE VII
MOS T ESTS RESULTS FOR BWE M ETHODS WITH SOURCE SAMPLING
RATE OF 8KHZ AND TARGET SAMPLING RATE OF 48KHZ
Methods MOS (CI)
sinc 3.69 ( ±0.07)
NU-Wave2 [47] 3.75 ( ±0.08)
UDM+ [48] 3.98 ( ±0.06)
mdctGAN [33] 4.00 ( ±0.07)
AP-BWE 4.11 ( ±0.06)
Ground Truth 4.17 ( ±0.06)
full fundamental frequency and most harmonic structures,
resulting in less pronounced subjective listening differences
between the speech extended by different models. Never-
theless, the MOS results demonstrated that our proposed
AP-BWE still showed substantial advantages in subjective
quality over other baseline models ( p <0.05). Firstly, NU-
Wave2 scored very significantly lower in MOS compared to
our proposed AP-BWE ( p <0.01), showing only a slight
improvement over sinc filter interpolation, with spectrogram
analysis revealing poor recovery of mid-to-high frequency
components. UDM+ performed well in recovering mid-
frequency components of speech, but it seemed to struggle
with restoring higher-frequency components, particularly
with low energy in the unvoiced segments, resulting in
extended speech that sounded less bright. Consequently, the
subjective quality of UDM+ remained significantly lower
than that of our proposed AP-BWE ( p= 0.020). This find-
ing aligned with the results obtained at the target sampling
rate of 16 kHz, suggesting a potential limitation in model-
ing high-frequency unvoiced segments for waveform-based
methods. The mdctGAN achieved the optimal MOS among
the baseline methods, with its corresponding spectrogram
displaying brighter and more complete structures. However,
the high-frequency components of the spectrogram exhibited
higher randomness and poorer continuity, resulting in a less
stable auditory perception. In contrast, our proposed AP-
BWE demonstrated a more robust restoration capability for
the high-frequency components, especially in the unvoiced
segments, giving it a substantial advantage in subjective
speech quality over mdctGAN ( p= 0.026). While there
were still differences in the high-frequency components of
the voiced segments compared to natural wideband speech,
these distinctions had minimal impact on the perceptualquality of the speech. Therefore, AP-BWE achieved a MOS
close to that of natural wideband speech.
3) Evaluation on Generation Efficiency: Considering the
inference speed, as depicted in Table V, our proposed AP-
BWE remained capable of efficient 48 kHz speech waveform
production at a speed of 18.14 times real-time on CPU and
292.28 times real-time on GPU. For diffusion-based methods
(i.e., NU-Wave, NU-Wave2, and UDM+), their generation
efficiency took a significant hit as they require multiple
time steps in the reverse process to continuously denoise
and recover the extended waveform from latent variables.
Remarkably, our AP-BWE achieved a speedup over them by
approximately 1000 times on CPU and 100 times on GPU,
respectively. Despite both mdctGAN and our proposed AP-
BWE operating at the spectral level, the generation speed of
mdctGAN was still constrained by its two-dimensional convo-
lution and transformer-based structure. Consequently, our AP-
BWE which is fully based on one-dimensional convolutions
enabled an approximately fourfold acceleration compared to it.
Compared to running on a GPU, our model exhibited a more
significant efficiency improvement on the CPU. This indicated
that our model could efficiently generate high-sampling-rate
samples even without the parallel acceleration support of
GPUs, making it more suitable for applications in scenarios
with limited computational resources. Considering the models’
computational complexity, the FLOPs of diffusion models
are heavily constrained by their reverse steps (82.78G ×
50 steps for NU-Wave, 27.71G ×50 steps for NU-Wave2,
and 47.39G ×50 steps for UDM+). However, even with
just one step generation, the FLOPs of our proposed AP-
BWE were still smaller than theirs, further demonstrating the
superiority of our proposed AP-BWE in terms of generation
efficiency. Comparing Table I and Table V, it can be observed
that for generating speech waveforms of the same duration,
the inference speed of our model at 48 kHz sampling rate
was relatively lower, while the computational complexity was
higher compared to the 16 kHz sampling rate. This was
attributed to the fact that our model under both sampling rate
configurations utilized the same STFT settings, resulting in
different frame numbers processed by the model.
C. Analysis and Discussion
1) Ablation Studies: We implemented ablation studies on
discriminators and between-stream connections to investigate
the roles of each discriminator and the effects of interactions

--- PAGE 12 ---
12
0 0.5 1.0 1.5 2.0
Time(s)04681224Frequency(kHz)Wideband
0 0.5 1.0 1.5 2.0
Time(s)04681224AP-BWE
0 0.5 1.0 1.5 2.0
Time(s)04681224AP-BWE w/o MPD
0 0.5 1.0 1.5 2.0
Time(s)04681224AP-BWE w/o MRDs
0 0.5 1.0 1.5 2.0
Time(s)04681224AP-BWE w/o Disc.
Fig. 6. Spectrogram visualization of the original wideband speech waveform and speech waveforms extended by the ablation models of our proposed AP-BWE
with a source sampling rate of 8 kHz and target sampling rate of 48 kHz. “AP-BWE w/o MRDs” represents the ablation of both MRAD and MRPD, while
“AP-BWE w/o Disc.” denotes the ablation of all discriminators.
TABLE VIII
EXPERIMENTAL RESULTS FOR THE ABLATION STUDIES WITH SOURCE
SAMPLING RATE OF 8KHZ AND TARGET SAMPLING RATE OF 48KHZ
Ablation on Discriminators
MPD MRAD / MRPD LSD AWPD IPAWPD GDAWPD IAF ViSQOL
! ! /! 0.84 1.75 1.41 1.44 3.35
% ! /! 0.85 1.76 1.42 1.44 3.29
! % /! 0.86 1.75 1.41 1.44 3.26
! ! /% 0.85 1.76 1.42 1.44 3.31
! % /% 0.88 1.75 1.41 1.44 3.26
% % /% 1.50 1.74 1.42 1.50 3.26
Ablation on Between-Stream Connections
A→P P →A LSD AWPD IPAWPD GDAWPD IAF ViSQOL
! ! 0.84 1.75 1.41 1.44 3.35
% ! 0.85 1.77 1.42 1.45 3.31
! % 0.85 1.76 1.42 1.44 3.32
% % 0.86 1.77 1.42 1.45 3.32
between amplitude stream and phase stream. All the exper-
iments were conducted with the source sampling rate of 8
kHz and target sampling rate of 48 kHz, and the experimental
results are depicted in Table VIII. Due to the minimal phase
differences in the high-frequency components for the BWE
targeting 48 kHz, we calculated the AWPD metric only in the
frequency band of 4 kHz to 8 kHz while calculating the LSD
metric on the whole frequency band. We further visualized
the spectrograms of the natural wideband 48 kHz speech
waveform and speech waveforms generated by the ablation
models of AP-BWE on discriminator, as illustrated in Fig. 6.
As shown in the upper half of Table VIII, for the ablation of
discriminators, all the discriminators contributed to the overall
performance of our proposed AP-BWE. We first ablated the
MPD to train the AP-BWE model solely with discriminators
at the spectral level. Although there is only a slight decrease
in all metrics, the spectrogram (AP-BWE w/o MPD) reveals
a smearing effect along the frequency axis as shown in
Fig. 6, resulting in a perceptible harshness in the extended
speech. Subsequently, in our preliminary experiments, we
separately ablated MRPD and MRAD. In both cases, the
metrics showed only slight decreases, and the spectrograms
appeared normal. However, when we simultaneously ablated
both of them (AP-BWE w/o MRDs), although the metrics still
decreased insignificantly, a noticeable over-smoothing could
be observed in the frequency band from 12 kHz to 24 kHz
of the spectrogram. This is because with the sole utilizationTABLE IX
EXPERIMENTAL RESULTS FOR THE CROSS -DATASET EVALUATION ON THE
LIBRI-TTS AND HIFI-TTS D ATASETS
Libri-TTS (8 kHz →24 kHz)
Methods LSD AWPD IP AWPD GD AWPD IAF ViSQOL
NU-Wave2 1.83 1.82 1.51 1.47 2.92
UDM+ 1.79 1.82 1.50 1.46 2.88
mdctGAN 1.27 1.80 1.44 1.46 3.42
AP-BWE 1.22 1.79 1.40 1.44 3.44
HiFi-TTS (8 kHz →44.1 kHz)
Methods LSD AWPD IP AWPD GD AWPD IAF ViSQOL
NU-Wave2 1.67 1.81 1.48 1.48 2.16
UDM+ 1.56 1.80 1.47 1.47 2.17
mdctGAN 1.69 1.80 1.47 1.47 2.43
AP-BWE 1.49 1.77 1.42 1.45 2.51
of MPD, the minimum period of its sub-discriminator was
2, so the frequency band range it can discriminate was only
from 0 to 12 kHz. As this over-smoothing phenomenon is
present in the high-frequency range, it has not had a substantial
impact on the perceived quality of extended speech. We further
ablated all discriminators (AP-BWE w/o Disc.), the LSD
metric experienced a significant decline, and the extended
portions across the entire spectrogram exhibited severe over-
smoothing, greatly compromising the quality of the extended
speech. This indicated that the training strategy of GAN was
indispensable for the current AP-BWE model.
Moreover, we ablated the between-stream connections. As
shown in the last row of Table VIII, the information interaction
between the amplitude stream and the phase stream did
contribute to the quality of the extended speech. To investigate
the influence of one stream on another, we selectively ablated
each of the connections. Our observations revealed that when
ablating the connection from the amplitude stream to the phase
stream (A →P), the AWPD metrics exhibited a deterioration
compared to ablating the connection from the phase stream to
the amplitude stream (P →A), and there was also a decrease
in ViSQOL, indicating that amplitude information played a
role in the modeling of phase. This conclusion aligns with our
previous work [34], where the phase spectrum can be predicted
from the amplitude spectrum.
2) Cross-Dataset Validation: Since the speech data in a
corpus is recorded in a fixed environment, models trained
exclusively on a single corpus may adapt to the specific

--- PAGE 13 ---
13
characteristics of the recording environment. To evaluate the
models’ generalization abilities across different corpora, we
conducted cross-dataset experiments on models trained with
the source and target sampling rates of 8 kHz and 48 kHz,
respectively. We selected two high-quality datasets, namely
Libri-TTS [63] and HiFi-TTS [64]. The Libri-TTS dataset
consists of 585 hours of speech data at a 24 kHz sampling rate.
For evaluation, we exclusively utilized the “test-clean” set,
containing 4,837 audio clips. The HiFi-TTS dataset contains
about 292 hours of speech from 10 speakers with at least 17
hours per speaker sampled at 44.1 kHz, and we also only
evaluated the models on its test set which contains 1000
audio clips. The experimental results are depicted in Table IX,
where the LSD scores were computed by downsampling all
the extended speech waveforms from 48 kHz to the original
sampling rates of the datasets, the AWPD metrics were calcu-
lated only in the 4kHz ∼8 kHz frequency band for a more
intuitive comparison, and the ViSQOL scores were computed
by upsampling all the speech waveforms to 48 kHz.
For the evaluation on the Libri-TTS dataset, as depicted
in the upper half in Table IX, our proposed AP-BWE still
achieved the best performance among all the metrics. For NU-
Wave2 and UDM+, their performance on the Libri-TTS dataset
was noticeably degraded compared to VCTK. This indicated a
strong dependency of waveform-based methods on the training
corpus, whereas spectrum-based approaches, by capturing
temporal and spectral characteristics from the waveforms,
exhibited adaptability to various data recording environments.
The evaluation results on the HiFi-TTS dataset are depicted in
the lower half in Table IX. Compared to the waveform-based
methods, spectrum-based methods still outperform in terms
of the overall speech quality as indicated by the ViSQOL
metric. Compared to NU-Wave2 and UDM+, the advantage
of mdctGAN on the HiFi-TTS test set is far less pronounced
than on the Libri-TTS test set, especially in terms of the LSD
metric. This suggests that different models exhibit varying
generalization abilities across different datasets. However, our
proposed AP-BWE still shows a significant advantage over
other baseline models across all metrics, further demonstrating
the superior generalization ability of our model.
VI. C ONCLUSION
In this paper, we introduced AP-BWE, a GAN-based BWE
model that can efficiently achieve high-quality wideband
waveform generation. The generator of AP-BWE performed
direct recovery of high-frequency amplitude and phase in-
formation from the narrowband amplitude and phase spectra
through an all-convolutional structure and all-frame-level op-
erations, significantly enhancing generation efficiency. More-
over, multiple discriminators applied on the time-domain
waveform, amplitude spectrum, and phase spectrum noticeably
elevated the overall generation quality. The major contribution
of the AP-BWE lay in the direct extension of the phase spec-
trum. This allowed for the precise modeling and optimization
of both the amplitude and phase spectra simultaneously, signif-
icantly enhancing the quality of the extended speech without
compromising the trade-offs between the two. Experimentalresults on the VCTK-0.92 dataset showcased that our proposed
AP-BWE achieved SOTA performance for tasks with target
sampling rates of both 16 kHz and 48 kHz. Spectrogram
visualizations underscored the robust capability of our model
in recovering high-frequency harmonic structures, effectively
enhancing the intelligibility of speech signals, even in scenar-
ios with extremely low source speech bandwidth. In future
work, our AP-BWE model can be further applied to assist
generative models trained on low-sampling-rate datasets in
improving their synthesized speech quality.
REFERENCES
[1] K. Nakamura, K. Hashimoto, K. Oura, Y . Nankaku, and K. Tokuda,
“A mel-cepstral analysis technique restoring high frequency components
from low-sampling-rate speech,” in Proc. Interspeech , 2014, pp. 2494–
2498.
[2] M. M. Goodarzi, F. Almasganj, J. Kabudian, Y . Shekofteh, and I. S.
Rezaei, “Feature bandwidth extension for persian conversational tele-
phone speech recognition,” in Proc. ICEE , 2012, pp. 1220–1223.
[3] A. Albahri, C. S. Rodriguez, and M. Lech, “Artificial bandwidth
extension to improve automatic emotion recognition from narrow-band
coded speech,” in Proc. ICSPCS , 2016, pp. 1–7.
[4] S. Chennoukh, A. Gerrits, G. Miet, and R. Sluijter, “Speech enhancement
via frequency bandwidth extension using line spectral frequencies,” in
Proc. ICASSP , vol. 1, 2001, pp. 665–668.
[5] F. Musti `ere, M. Bouchard, and M. Boli ´c, “Bandwidth extension for
speech enhancement,” in Proc. CCECE , 2010, pp. 1–4.
[6] W. Xiao, W. Liu, M. Wang, S. Yang, Y . Shi, Y . Kang, D. Su, S. Shang,
and D. Yu, “Multi-mode neural speech coding based on deep generative
networks,” in Proc. Interspeech , 2023, pp. 819–823.
[7] J. Makhoul and M. Berouti, “High-frequency regeneration in speech
coding systems,” in Proc. ICASSP , vol. 4, 1979, pp. 428–431.
[8] H. Carl, “Bandwidth enhancement of narrowband speech signals,” in
Proc. EUSIPCO , vol. 2, 1994, pp. 1178–1181.
[9] J. Sadasivan, S. Mukherjee, and C. S. Seelamantula, “Joint dictionary
training for bandwidth extension of speech signals,” in Proc. ICASSP ,
2016, pp. 5925–5929.
[10] T. Unno and A. McCree, “A robust narrowband to wideband extension
system featuring enhanced codebook mapping,” in Proc. ICASSP , vol. 1,
2005, pp. I–805.
[11] H. Pulakka, U. Remes, K. Palom ¨aki, M. Kurimo, and P. Alku, “Speech
bandwidth extension using Gaussian mixture model-based estimation of
the highband mel spectrum,” in Proc. ICASSP , 2011, pp. 5100–5103.
[12] Y . Ohtani, M. Tamura, M. Morita, and M. Akamine, “GMM-based
bandwidth extension using sub-band basis spectrum model,” in Proc.
Interspeech , 2014, pp. 2489–2493.
[13] Y . Wang, S. Zhao, Y . Yu, and J. Kuang, “Speech bandwidth extension
based on GMM and clustering method,” in Proc. CSNT , 2015, pp. 437–
441.
[14] G. Chen and V . Parsa, “HMM-based frequency bandwidth extension for
speech enhancement using line spectral frequencies,” in Proc. ICASSP ,
vol. 1, 2004, pp. I–709.
[15] P. Bauer and T. Fingscheidt, “An HMM-based artificial bandwidth ex-
tension evaluated by cross-language training and test,” in Proc. ICASSP ,
2008, pp. 4589–4592.
[16] G.-B. Song and P. Martynovich, “A study of HMM-based bandwidth
extension of speech signals,” Signal Processing , vol. 89, no. 10, pp.
2036–2044, 2009.
[17] Z. Yong and L. Yi, “Bandwidth extension of narrowband speech based
on hidden markov model,” in Proc. ICALIP , 2014, pp. 372–376.
[18] Z.-H. Ling, S.-Y . Kang, H. Zen, A. Senior, M. Schuster, X.-J. Qian,
H. M. Meng, and L. Deng, “Deep learning for acoustic modeling in para-
metric speech generation: A systematic review of existing techniques
and future trends,” IEEE Signal Processing Magazine , vol. 32, no. 3,
pp. 35–52, 2015.
[19] V . Kuleshov, S. Z. Enam, and S. Ermon, “Audio super-resolution using
neural nets,” in Proc. ICLR (Workshop Track) , 2017.
[20] Z.-H. Ling, Y . Ai, Y . Gu, and L.-R. Dai, “Waveform modeling and
generation using hierarchical recurrent neural networks for speech
bandwidth extension,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing , vol. 26, no. 5, pp. 883–894, 2018.

--- PAGE 14 ---
14
[21] S. Birnbaum, V . Kuleshov, Z. Enam, P. W. W. Koh, and S. Ermon,
“Temporal FiLM: Capturing long-range sequence dependencies with
feature-wise modulations.” Proc. NeurIPS , vol. 32, 2019.
[22] N. C. Rakotonirina, “Self-attention for audio super-resolution,” in Proc.
MLSP , 2021, pp. 1–6.
[23] H. Wang and D. Wang, “Towards robust speech super-resolution,”
IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
vol. 29, pp. 2058–2066, 2021.
[24] J. Abel, M. Strake, and T. Fingscheidt, “A simple cepstral domain DNN
approach to artificial speech bandwidth extension,” in Proc. ICASSP ,
2018, pp. 5469–5473.
[25] K. Li and C.-H. Lee, “A deep neural network approach to speech
bandwidth expansion,” in Proc. ICASSP , 2015, pp. 4395–4399.
[26] B. Liu, J. Tao, Z. Wen, Y . Li, and D. Bukhari, “A novel method
of artificial bandwidth extension using deep architecture.” in Proc.
Interspeech , 2015, pp. 2598–2602.
[27] Y . Gu, Z.-H. Ling, and L.-R. Dai, “Speech bandwidth extension using
bottleneck features and deep recurrent neural networks,” in Proc. Inter-
speech , 2016, pp. 297–301.
[28] C. V . Botinhao, B. S. Carlos, L. P. Caloba, and M. R. Petraglia,
“Frequency extension of telephone narrowband speech signal using
neural networks,” in Proc. CESA , vol. 2, 2006, pp. 1576–1579.
[29] J. Kontio, L. Laaksonen, and P. Alku, “Neural network-based artificial
bandwidth expansion of speech,” IEEE Transactions on Audio, Speech,
and Language Processing , vol. 15, no. 3, pp. 873–881, 2007.
[30] H. Pulakka and P. Alku, “Bandwidth extension of telephone speech
using a neural network and a filter bank implementation for highband
mel spectrum,” IEEE Transactions on Audio, Speech, and Language
Processing , vol. 19, no. 7, pp. 2170–2183, 2011.
[31] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian, and D. Wang, “Neural
vocoder is all you need for speech super-resolution,” in Proc. Inter-
speech , 2022, pp. 4227–4231.
[32] M. Mandel, O. Tal, and Y . Adi, “AERO: Audio super resolution in the
spectral domain,” in Proc. ICASSP , 2023, pp. 1–5.
[33] C. Shuai, C. Shi, L. Gan, and H. Liu, “mdctGAN: Taming transformer-
based GAN for speech super-resolution with modified DCT spectra,” in
Proc. Interspeech , 2023, pp. 5112–5116.
[34] Y . Ai and Z.-H. Ling, “Neural speech phase prediction based on parallel
estimation architecture and anti-wrapping losses,” in Proc. ICASSP ,
2023, pp. 1–5.
[35] Y . Ai, Y .-X. Lu, and Z.-H. Ling, “Long-frame-shift neural speech phase
prediction with spectral continuity enhancement and interpolation error
compensation,” IEEE Signal Processing Letters , vol. 30, pp. 1097–1101,
2023.
[36] Y . Ai and Z.-H. Ling, “APNet: An all-frame-level neural vocoder incor-
porating direct prediction of amplitude and phase spectra,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing , vol. 31, pp.
2145–2157, 2023.
[37] Y .-X. Lu, Y . Ai, and Z.-H. Ling, “MP-SENet: A speech enhancement
model with parallel denoising of magnitude and phase spectra,” in Proc.
Interspeech , 2023, pp. 3834–3838.
[38] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A
convnet for the 2020s,” in Proc. CVPR , 2022, pp. 11 976–11 986.
[39] D. Yin, C. Luo, Z. Xiong, and W. Zeng, “PHASEN: A phase-and-
harmonics-aware speech enhancement network,” in Proc. AAAI , vol. 34,
no. 05, 2020, pp. 9458–9465.
[40] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial
networks for efficient and high fidelity speech synthesis,” Advances in
neural information processing systems , vol. 33, pp. 17 022–17 033, 2020.
[41] W. Jang, D. Lim, J. Yoon, B. Kim, and J. Kim, “UnivNet: A neural
vocoder with multi-resolution spectrogram discriminators for high-
fidelity waveform generation,” in Proc. Interspeech , 2021, pp. 2207–
2211.
[42] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional
networks for biomedical image segmentation,” in Proc. MICCAI , 2015,
pp. 234–241.
[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc.
NeurIPS , vol. 30, 2017.
[44] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
unsupervised learning using nonequilibrium thermodynamics,” in Proc.
ICML , 2015, pp. 2256–2265.
[45] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
Proc. NeurIPS , vol. 33, pp. 6840–6851, 2020.
[46] J. Lee and S. Han, “NU-Wave: A diffusion probabilistic model for neural
audio upsampling,” Proc. Interspeech , pp. 1634–1638, 2021.[47] S. Han and J. Lee, “NU-Wave 2: A general neural audio upsampling
model for various sampling rates,” in Proc. Interspeech , 2022, pp. 4401–
4405.
[48] C.-Y . Yu, S.-L. Yeh, G. Fazekas, and H. Tang, “Conditioning and
sampling in variational diffusion models for speech super-resolution,”
inProc. ICASSP , 2023, pp. 1–5.
[49] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “ResUNet-a: A
deep learning framework for semantic segmentation of remotely sensed
data,” ISPRS Journal of Photogrammetry and Remote Sensing , vol. 162,
pp. 94–114, 2020.
[50] Z.-Q. Wang, G. Wichern, and J. Le Roux, “On the compensation between
magnitude and phase in speech separation,” IEEE Signal Processing
Letters , vol. 28, pp. 2018–2022, 2021.
[51] H. Siuzdak, “V ocos: Closing the gap between time-domain and fourier-
based neural vocoders for high-quality audio synthesis,” arXiv preprint
arXiv:2306.00814 , 2023.
[52] A. Gritsenko, T. Salimans, R. van den Berg, J. Snoek, and N. Kalch-
brenner, “A spectral energy distance for parallel speech synthesis,” Proc.
NeurIPS , vol. 33, pp. 13 062–13 072, 2020.
[53] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” stat, vol.
1050, p. 21, 2016.
[54] D. Hendrycks and K. Gimpel, “Gaussian error linear units (GELUs),”
inProc. ICML , vol. 70, 2017, pp. 3441–3450.
[55] A. L. Maas, A. Y . Hannun, A. Y . Ng et al. , “Rectifier nonlinearities
improve neural network acoustic models,” in Proc. ICML , vol. 30, no. 1,
2013, p. 3.
[56] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi,
“Soundstream: An end-to-end neural audio codec,” IEEE/ACM Transac-
tions on Audio, Speech, and Language Processing , vol. 30, pp. 495–507,
2021.
[57] K. Kumar, R. Kumar, T. De Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo,
A. De Brebisson, Y . Bengio, and A. C. Courville, “MelGAN: Generative
adversarial networks for conditional waveform synthesis,” in Proc.
NeurIPS , vol. 32, 2019.
[58] J. Yamagishi, C. Veaux, K. MacDonald et al. , “CSTR VCTK corpus:
English multi-speaker corpus for CSTR voice cloning toolkit (version
0.92),” University of Edinburgh. The Centre for Speech Technology
Research (CSTR) , 2019.
[59] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
arXiv preprint arXiv:1711.05101 , 2017.
[60] M. Chinen, F. S. Lim, J. Skoglund, N. Gureev, F. O’Gorman, and
A. Hines, “ViSQOL v3: An open source production ready objective
speech and audio metric,” in Proc. QoMEX , 2020, pp. 1–6.
[61] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and
I. Sutskever, “Robust speech recognition via large-scale weak super-
vision,” in Proc. ICML , 2023, pp. 28 492–28 518.
[62] C. Veaux, J. Yamagishi, K. MacDonald et al. , “CSTR VCTK corpus:
English multi-speaker corpus for CSTR voice cloning toolkit,” Univer-
sity of Edinburgh. The Centre for Speech Technology Research (CSTR) ,
vol. 6, p. 15, 2017.
[63] H. Zen, V . Dang, R. Clark, Y . Zhang, R. J. Weiss, Y . Jia, Z. Chen,
and Y . Wu, “LibriTTS: A corpus derived from LibriSpeech for text-to-
speech,” in Proc. Interspeech , 2019, pp. 1526–1530.
[64] E. Bakhturina, V . Lavrukhin, B. Ginsburg, and Y . Zhang, “Hi-Fi multi-
speaker English TTS dataset,” in Proc. Interspeech , 2021, pp. 2776–
2780.
