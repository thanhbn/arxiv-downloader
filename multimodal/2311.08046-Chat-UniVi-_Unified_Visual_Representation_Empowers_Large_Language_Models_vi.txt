# 2311.08046.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.08046.pdf
# Kích thước file: 9902651 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Chat-UniVi: Biểu diễn Thị giác Thống nhất Trao quyền cho các Mô hình Ngôn ngữ Lớn
với Khả năng Hiểu Ảnh và Video
Peng Jin1,2,3Ryuichi Takanobu Wancai Zhang4Xiaochun Cao5Li Yuan1,2,3*
1Trường Kỹ thuật Điện tử và Máy tính, Đại học Bắc Kinh, Thâm Quyến, Trung Quốc2Phòng thí nghiệm Bằng Thành, Thâm Quyến, Trung Quốc
3Chương trình Ưu tiên AI cho Khoa học (AI4S), Trường Đại học Sau đại học Bắc Kinh Thâm Quyến, Thâm Quyến, Trung Quốc
4Công ty Công nghệ Nari, Trung Quốc5Trường Khoa học và Công nghệ An ninh mạng, Cơ sở Thâm Quyến của Đại học Trung Sơn, Thâm Quyến, Trung Quốc
jp21@stu.pku.edu.cn yuanli-ece@pku.edu.cn
https://github.com/PKU-YuanGroup/Chat-UniVi
Tóm tắt
Các mô hình ngôn ngữ lớn đã thể hiện khả năng toàn diện ấn tượng
trên một loạt các tác vụ mở rộng và đã mở rộng tiện ích của chúng
để bao gồm các cuộc trò chuyện đa phương thức. Tuy nhiên, các
phương pháp hiện có gặp phải thách thức trong việc xử lý hiệu quả
cả hiểu ảnh và video, đặc biệt với số lượng token thị giác hạn chế.
Trong công trình này, chúng tôi giới thiệu Chat-UniVi, một mô hình
ngôn ngữ-thị giác Thống nhất có khả năng hiểu và tham gia vào các
cuộc trò chuyện liên quan đến ảnh và video thông qua một biểu diễn
thị giác thống nhất. Cụ thể, chúng tôi sử dụng một tập hợp các token
thị giác động để biểu diễn thống nhất ảnh và video. Khung biểu diễn
này trao quyền cho mô hình để sử dụng hiệu quả một số lượng hạn
chế các token thị giác để đồng thời nắm bắt các chi tiết không gian
cần thiết cho ảnh và mối quan hệ thời gian toàn diện cần thiết cho
video. Hơn nữa, chúng tôi tận dụng một biểu diễn đa tỷ lệ, cho phép
mô hình cảm nhận cả các khái niệm ngữ nghĩa cấp cao và các chi tiết
thị giác cấp thấp. Đáng chú ý, Chat-UniVi được huấn luyện trên một
tập dữ liệu hỗn hợp chứa cả ảnh và video, cho phép áp dụng trực tiếp
cho các tác vụ liên quan đến cả hai phương tiện mà không cần bất kỳ
sửa đổi nào. Kết quả thực nghiệm rộng rãi cho thấy Chat-UniVi luôn
vượt trội hơn ngay cả các phương pháp hiện có được thiết kế độc
quyền cho ảnh hoặc video. Mã nguồn có sẵn tại
https://github.com/PKU-YuanGroup/Chat-UniVi.

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM), như GPT-3 [7] và
LLaMA [63, 64], thể hiện khả năng toàn diện đáng kể
mở đường cho việc đạt được trí tuệ nhân tạo tổng quát.
Tuy nhiên, ngôn ngữ chỉ đại diện cho một khía cạnh của
*Tác giả liên hệ: Li Yuan.

Input Token vanilla Token động
(1, 3, H, W) (1, L, D) (1, C, D)
(M, 3, H, W) (M, L, D) (E, C, D)
Ảnh Video

Hình 1. Khung biểu diễn thống nhất cho ảnh và video sử dụng
một tập hợp các token thị giác động. "H" và "W" biểu thị chiều
cao và chiều rộng của đầu vào, tương ứng. "L", "D", "M", "C", 
và "E" biểu thị số lượng token thị giác vanilla, chiều đặc trưng,
độ dài khung hình, số lượng token thị giác động, và số lượng sự
kiện, tương ứng.

giao tiếp. Thông tin thị giác phục vụ để bổ sung và
nâng cao sự hiểu biết của chúng ta về thế giới. Do đó, có
một sự quan tâm đang nổi lên trong việc phát triển một mô
hình trò chuyện đa phương thức có thể chứa đựng các phương
thức đầu vào khác nhau đồng thời, bao gồm ảnh và video.

Những tiến bộ gần đây trong các mô hình trò chuyện đa
phương thức, như MiniGPT-4 [84], LLaVA [39, 40], và
mPLUG-Owl [73], tập trung vào việc tích hợp các token thị
giác vào LLM. Mặc dù có tiến bộ đáng khen ngợi, các phương
pháp hiện có thường chuyên về đầu vào ảnh hoặc video. Ví
dụ, các phương pháp [39, 40] ưu tiên đầu vào ảnh thường sử
dụng một số lượng lớn hơn các token thị giác để đạt được sự
hiểu biết không gian tinh tế hơn. Ngược lại, các phương pháp
[45] tập trung vào đầu vào video thường thỏa hiệp sự hiểu biết
không gian cho mỗi khung hình để chứa đựng nhiều khung hình
hơn cho việc mô hình hóa các mối quan hệ thời gian. Mặc dù
một số phương pháp, ví dụ như Flamingo [1], có thể trích xuất
một số lượng token cố định cho mỗi ảnh và video bằng cách sử
dụng một transformer truy vấn, sự nhấn mạnh chính của chúng
vẫn là hiểu ảnh, thiếu khả năng mô hình hóa hiệu quả sự hiểu
biết thời gian, do đó dẫn đến sự hiểu biết hạn chế về video. Do
đó, việc cho phép LLM hiểu cả ảnh và video trong một khung
thống nhất là rất quan trọng và thách thức.

Trong bài báo này, chúng tôi giới thiệu Chat-UniVi, một
mô hình ngôn ngữ-thị giác Thống nhất được thiết kế để hiểu
thành thạo và tham gia vào các cuộc trò chuyện về cả ảnh và
video. Chat-UniVi biểu diễn thống nhất ảnh và video bằng
cách sử dụng một tập hợp các token thị giác động, cho phép
nó đồng thời nắm bắt các chi tiết không gian của ảnh và mối
quan hệ thời gian toàn diện của video. Như được minh họa
trong Hình 1, ảnh có thể được mô tả thông qua các token thị
giác với các kích thước đa dạng. Ví dụ, đối tượng chính, tức
là con cừu trong Hình 1, cần một biểu diễn tinh tế với nhiều
token thị giác, trong khi nền, tức là ngọn núi tuyết phủ, có
thể được mô hình hóa đầy đủ chỉ với một token thị giác. Trong
trường hợp video, video ban đầu được chia thành nhiều sự
kiện, và sau đó, các token thị giác này mở rộng qua các khung
hình trong mỗi sự kiện để gói gọn động lực cấp khung hình.
Biểu diễn thống nhất như vậy cho cả ảnh và video làm giảm
đáng kể số lượng token thị giác trong khi duy trì khả năng
biểu đạt của mô hình. Đáng chú ý rằng video dài hơn được
gán nhiều token thị giác hơn trong phương pháp của chúng
tôi. Do đó, phương pháp của chúng tôi phù hợp hơn cho việc
hiểu video có độ dài thay đổi so với các phương pháp hiện có.

Để có được các token thị giác động này, chúng tôi đề xuất
một phương pháp hợp nhất token để dần dần hợp nhất các
token thị giác có ý nghĩa ngữ nghĩa tương tự. Cụ thể, bắt đầu
với các token thị giác được khởi tạo bởi vision transformer
[15], chúng tôi dần dần nhóm chúng bằng cách áp dụng thuật
toán phân cụm density peaks dựa trên k-nearest-neighbor,
tức là DPC-KNN [16], trên các đặc trưng token. Khi nói đến
video, chúng tôi cũng sử dụng DPC-KNN trên các đặc trưng
khung hình để có được các sự kiện. Tại mỗi bước hợp nhất,
các token thị giác được gán cho cùng một cụm được hợp nhất
bằng cách lấy trung bình các đặc trưng token của chúng. Cuối
cùng, chúng tôi cung cấp một biểu diễn đa tỷ lệ cho LLM,
trong đó các tầng trên của biểu diễn đa tỷ lệ bao gồm các khái
niệm ngữ nghĩa cấp cao, trong khi các tầng dưới nhấn mạnh
vào biểu diễn chi tiết thị giác.

Chat-UniVi đề xuất có hai ưu điểm hấp dẫn: Thứ nhất,
phương pháp mô hình hóa ảnh và video thống nhất của nó cho
phép huấn luyện trên tập dữ liệu hỗn hợp của ảnh và video,
cho phép áp dụng trực tiếp cho cả tác vụ ảnh và video mà
không cần bất kỳ sửa đổi nào. Thứ hai, biểu diễn đa tỷ lệ
đóng góp vào sự hiểu biết toàn diện về ảnh và video, trao
quyền cho Chat-UniVi để thích ứng với các tác vụ khác nhau,
bao gồm sử dụng biểu diễn cấp cao cho hiểu ngữ nghĩa và
biểu diễn cấp thấp cho việc tạo ra các mô tả chi tiết. Chúng tôi
đánh giá Chat-UniVi trên cả tác vụ hiểu ảnh và video. Như
được hiển thị trong Hình 2, so với các phương pháp khác tập
trung độc quyền vào ảnh hoặc video, Chat-UniVi luôn thể
hiện sự vượt trội trong việc hiểu ảnh và video. Hơn nữa,
chúng tôi cũng cung cấp bằng chứng về lợi ích của việc huấn
luyện kết hợp ảnh và video cho các mô hình ngôn ngữ lớn đa
phương thức. Các đóng góp chính được tóm tắt như sau:

• Chúng tôi đề xuất một biểu diễn thị giác thống nhất cho
LLM, cho phép LLM hiểu cả ảnh và video.
• Chúng tôi biểu diễn thống nhất ảnh và video bằng cách sử
dụng các token thị giác động đa tỷ lệ và đề xuất một phương
pháp hợp nhất token để có được các token thị giác động này.
• Mà không cần tinh chỉnh, Chat-UniVi đạt được hiệu suất
cạnh tranh trong cả tác vụ ảnh và video và đạt được kết quả
ấn tượng trong benchmark ảo giác đối tượng.

2. Công trình liên quan
Các Mô hình Ngôn ngữ Lớn. Các mô hình ngôn ngữ lớn [50,
53, 65] đã có tiến bộ đột phá, chủ yếu được quy cho sự mở
rộng của dữ liệu huấn luyện và sự gia tăng đáng kể trong các
tham số mô hình. Lấy cảm hứng từ thành công của GPT-3 [7],
nhiều LLM sau đó đã được phát triển, bao gồm PaLM [13],
OPT [78], BLOOM [57], InstructGPT [48], và ChatGPT [46].
Tuy nhiên, ngôn ngữ chỉ đại diện cho một khía cạnh của giao
tiếp. Thông tin thị giác phục vụ để bổ sung và nâng cao sự
hiểu biết của chúng ta về thế giới [5, 24–27, 29, 66, 82]. Trong
công trình này, chúng tôi giới thiệu Chat-UniVi, được thiết kế
để hiểu cả đầu vào ảnh và video.

Các Mô hình Đa phương thức Quy mô lớn. Các mô hình
đa phương thức quy mô lớn hiện có [4, 9–11, 17–19, 31, 36–
38, 43, 56, 68, 81, 83] có thể được phân loại rộng rãi thành
hai lớp. Lớp phương pháp đầu tiên [59, 61, 67, 72] liên quan
đến việc sử dụng LLM như một bộ lập lịch điều phối, tạo điều
kiện kết nối giữa các mô hình chuyên gia khác nhau để xử lý
các tác vụ thị giác khác nhau. Lớp phương pháp thứ hai [32,
32, 47] nhấn mạnh vào việc tích hợp các mô hình từ các
phương thức khác nhau thành các mô hình có thể huấn luyện
end-to-end. Gần đây hơn, cũng đã có một số mô hình đa
phương thức chuyên dụng được thiết kế riêng cho xử lý video,
như Video-LLaVA [35], Video-ChatGPT [45], VideoChat [33],
và Video-LLaMA [76]. Mặc dù có tiến bộ đáng khen ngợi,
các phương pháp hiện có thường tập trung độc quyền vào đầu
vào ảnh hoặc video. Trong công trình này, chúng tôi tập trung
vào việc phát triển một mô hình đa phương thức được huấn
luyện end-to-end cho cả tác vụ ảnh và video. Mặc dù Flamingo
cũng hỗ trợ cả đầu vào ảnh và video, nó chỉ có thể trích xuất
một số lượng token cố định cho các video có độ dài khác nhau
với một transformer truy vấn. Các công trình gần đây [9, 68]
đã khám phá việc sử dụng các bộ mã hóa ảnh và video được
tiền huấn luyện riêng biệt để xử lý, nhưng các phương pháp
này đưa ra sự dư thừa mô hình và tỏ ra thách thức để huấn
luyện cùng nhau. Do đó, nó không phù hợp với trọng tâm của
chúng tôi trong việc đạt được một mô hình ngôn ngữ-thị giác
thống nhất. Trái ngược với các công trình trước đó, phương
pháp đề xuất biểu diễn thống nhất ảnh và video bằng cách sử
dụng các token thị giác động đa tỷ lệ.

Token Thị giác Động. Cũng đã có các phương pháp gần đây
[6, 44, 54, 55, 70, 75] để khám phá vai trò của các token động
trong khung transformer. Tuy nhiên, không có phương pháp
nào trong số này có thể được mở rộng trực tiếp cho video.
Chúng tôi tóm tắt các ưu điểm của phương pháp chúng tôi như
sau: (i) Hỗ trợ đầu vào video. Trái ngược với các phương pháp
khác, Chat-UniVi mở rộng phương pháp token động để kết
hợp đầu vào video, đạt được sự tích hợp của biểu diễn ảnh và
video lần đầu tiên. Công trình của chúng tôi là công trình đầu
tiên chứng minh rằng biểu diễn thống nhất này có thể hòa giải
các chi tiết không gian phức tạp của ảnh với sự hiểu biết thời
gian rộng lớn hơn cần thiết cho video. (ii) Không có tham số.
Phương pháp phân cụm của chúng tôi là không có tham số.
Thú vị là, chúng tôi thấy rằng phương pháp phân cụm không
có tham số này đóng vai trò như điểm trụ cho thành công của
mô hình chúng tôi. Chúng tôi quy hiện tượng này cho sự bất
ổn gradient trong huấn luyện trò chuyện đa phương thức, điều
này cản trở sự hội tụ của các phương pháp có tham số. So sánh
giữa Chat-UniVi và các phương pháp token động khác được
cung cấp trong phụ lục.

3. Phương pháp luận
Chat-UniVi nhằm mục đích mô hình hóa ảnh và video đồng
thời trong một chuỗi ngôn ngữ có thể được hiểu bởi các Mô
hình Ngôn ngữ Lớn (LLM) trong một khung thống nhất. Chat-
UniVi đạt được điều này bằng cách biểu diễn thống nhất ảnh
và video thông qua một tập hợp các token thị giác động, kết
nối các chi tiết không gian phức tạp của ảnh với sự hiểu biết
thời gian rộng lớn hơn cần thiết cho video. Tổng quan về Chat-
UniVi đề xuất được hiển thị trong Hình 3.

3.1. Token Thị giác Động cho Ảnh và Video
Dựa trên Vision Transformer vanilla, hầu hết các phương
pháp tạo ra các token thị giác có tầm quan trọng như nhau
bằng cách chia ảnh thành các lưới thông thường và cố định.
Tuy nhiên, rõ ràng là không phải tất cả các vùng đều có tầm
quan trọng như nhau trong các tác vụ ngôn ngữ-thị giác. Ví
dụ, việc nắm bắt nền có thể chỉ cần một token thị giác duy
nhất. Lấy cảm hứng từ cái nhìn sâu sắc này, chúng tôi kết hợp
các token không thiết yếu để tạo ra các vùng thị giác động làm
đầu vào cho LLM.

Hợp nhất Token Thị giác Không gian. Đối với một ảnh đầu
vào, chúng tôi áp dụng bộ mã hóa thị giác của CLIP [51] để
cung cấp các token thị giác ban đầu Z={zi}L i=1, trong đó L
là số lượng token thị giác mà mỗi ảnh được chia thành. Để
kết hợp các token thị giác không thiết yếu, chúng tôi sử dụng
DPC-KNN [16], một thuật toán phân cụm density peaks dựa
trên k-nearest-neighbor, để phân cụm các token thị giác. Bắt
đầu với các token thị giác Z={zi}L i=1 được khởi tạo bởi vision
transformer, chúng tôi trước tiên tính toán mật độ cục bộ ρi
của mỗi token zi theo K-nearest neighbors của nó, được công
thức hóa như sau:

ρi=exp(−1/K ∑zk∈KNN(zi,Z) ∥zk−zi∥2), (1)

trong đó KNN(zi,Z) là K-nearest neighbors của zi trong Z\{zi}.
"Z\{zi}" biểu thị loại bỏ {zi} khỏi Z. Trực quan, ρi biểu thị
mật độ cục bộ của token zi. Sau đó, chúng tôi tính toán chỉ số
khoảng cách δi của token zi:

δi = {
  min(j:ρj>ρi) ∥zj−zi∥2, nếu ∃j s.t. ρj > ρi.
  max(j) ∥zj−zi∥2, ngược lại.
}                              (2)

Về bản chất, δi biểu thị khoảng cách giữa token zi đã cho từ
các token mật độ cao khác. Chúng tôi xác định những token
có ρi×δi tương đối cao như các trung tâm cụm và sau đó phân
bổ các token khác cho trung tâm cụm gần nhất của chúng theo
khoảng cách Euclidean. Cuối cùng, chúng tôi sử dụng token
trung bình trong mỗi cụm để đại diện cho cụm tương ứng.
Vùng thị giác của token đã hợp nhất là hợp của các vùng thị
giác trong cụm tương ứng.

Hợp nhất Token Thị giác Thời gian. Để thích ứng các token
thị giác động với đầu vào video, chúng tôi mở rộng các token
thị giác qua các khung hình. Tuy nhiên, việc trực tiếp hợp
nhất tất cả các khung hình thành một số lượng hạn chế các
token thị giác có thể dẫn đến mất thông tin thời gian trong
video. Ví dụ, trong Hình 3, video chứng minh quá trình nấu
mì ống trước khi chuẩn bị nước sốt. Việc chỉ hợp nhất tất cả
các khung hình sẽ tạo ra thách thức cho mô hình trong việc
xác định trình tự đúng, chẳng hạn như có nên chuẩn bị nước
sốt trước, nấu mì ống trước, hay đồng thời nấu mì ống trong
khi chuẩn bị nước sốt. Do đó, chúng tôi đề xuất hợp nhất
token thị giác thời gian để trước tiên chia video thành nhiều
sự kiện quan trọng. Sau đó, chúng tôi chỉ để các token thị
giác mở rộng qua các khung hình trong cùng một sự kiện.

Cho khung hình thứ m Zm={zm i}L i=1 của một video, chúng
tôi trước tiên áp dụng mean-pooling trên tất cả các token để
có được biểu diễn cấp khung hình fm. Tương tự như phương
pháp hợp nhất token thị giác không gian, chúng tôi tận dụng
DPC-KNN để kết hợp các khung hình không thiết yếu. Cụ thể,
chúng tôi trước tiên tính toán mật độ cục bộ ρm và chỉ số
khoảng cách δm của mỗi khung hình fm. Các khung hình có
ρm×δm tương đối cao được xác định là các trung tâm cụm,
và các khung hình khác sau đó được gán cho trung tâm cụm
gần nhất của chúng dựa trên khoảng cách Euclidean. Chúng
tôi coi mỗi cụm như một sự kiện quan trọng và biểu thị tập
hợp các chỉ số của các khung hình trong cụm là F. Do đó,
tập hợp các token thị giác trong sự kiện thứ n Fn có thể được
công thức hóa như sau:

Z̃n = {zm i | m ∈ Fn, i ∈ {1,2, ..., L}}.          (3)

Sau khi hoàn thành hợp nhất token thị giác thời gian, chúng
tôi có được tập hợp các token thị giác trong sự kiện, tức là
Z̃. Để làm cho các token thị giác mở rộng qua các khung hình
trong sự kiện, chúng tôi điều chỉnh Phương trình (1) và
Phương trình (2) trong phương pháp hợp nhất token thị giác
không gian thành dạng sau:

ρ̃i=exp(−1/K ∑zk∈KNN(zi,Z̃) ∥zk−zi∥2),
δ̃i = {
  min(j:ρ̃j>ρ̃i) ∥zj−zi∥2, nếu ∃j s.t. ρ̃j > ρ̃i.
  max(j) ∥zj−zi∥2, ngược lại.
}                              (4)

Cuối cùng, chúng tôi nối các token thị giác động đã mở rộng
cùng nhau theo thứ tự sự kiện để đảm bảo sự hiểu biết thời
gian rộng lớn hơn cần thiết cho video.

Biểu diễn Đa tỷ lệ. Để nâng cao thêm khả năng của mô hình
chúng tôi, chúng tôi đề xuất một phương pháp tổng hợp đa
bước được thiết kế để cung cấp các đặc trưng thị giác đa tỷ
lệ cho LLM. Cụ thể, trong Chat-UniVi, các token thị giác ban
đầu ở bước hợp nhất đầu tiên được tạo ra từ bộ mã hóa thị
giác của CLIP. Sau đó, chúng tôi dần dần hợp nhất các token
thị giác có ý nghĩa ngữ nghĩa tương tự và có được số lượng
token khác nhau ở các bước khác nhau. Các đặc trưng cấp
cao hơn bao gồm các khái niệm ngữ nghĩa trừu tượng, trong
khi các cấp thấp hơn nhấn mạnh vào biểu diễn chi tiết thị
giác. Trong thực tế, chúng tôi thực hiện một quá trình tổng
hợp ba bước cho mỗi ảnh hoặc video đầu vào. Cuối cùng,
chúng tôi nối các đầu ra từ mỗi bước hợp nhất và sử dụng
một ma trận chiếu có thể huấn luyện W để chuyển đổi các
đặc trưng thị giác đa tỷ lệ này thành các token nhúng ngôn
ngữ, phục vụ như đầu vào cho LLM.

Đáng chú ý rằng mặc dù có sự nối, số lượng token thị giác
trong phương pháp của chúng tôi vẫn thấp hơn đáng kể so với
các token thị giác ban đầu được tạo ra bởi vision transformer.
Ví dụ, trong khi LLaVA [40] sử dụng 256 token thị giác,
phương pháp của chúng tôi chỉ sử dụng 112 token thị giác.

3.2. Sơ đồ Huấn luyện Đa phương thức
Tiền huấn luyện Đa phương thức. Theo cách tiếp cận của
các công trình trước đó [40], huấn luyện của chúng tôi được
chia thành hai giai đoạn. Ở giai đoạn đầu tiên, chúng tôi tiền
huấn luyện ma trận chiếu W trong khi đóng băng cả LLM và
bộ mã hóa thị giác. Việc đóng băng chiến lược LLM này trao
quyền cho phương pháp của chúng tôi để nắm bắt hiệu quả
thông tin thị giác ngữ nghĩa mà không có bất kỳ sự thỏa hiệp
nào có thể nhận thấy được trong hiệu suất của LLM.

Tinh chỉnh Hướng dẫn Kết hợp. Sau khi hoàn thành giai
đoạn đầu tiên, mô hình có thể hiểu các truy vấn của con
người nhưng vẫn không thể tạo ra các phản hồi ngôn ngữ hợp
lý và mạch lạc. Ở giai đoạn thứ hai, chúng tôi tinh chỉnh hoàn
toàn mô hình ngôn ngữ lớn và ma trận chiếu W trên một tập
dữ liệu tuân theo hướng dẫn đa phương thức. Tập dữ liệu này
là một hợp thành của các cuộc trò chuyện đa lượt và các cuộc
trò chuyện đơn lượt được trình bày trong định dạng trò chuyện,
cùng với ảnh đơn, nhiều ảnh, và video như đầu vào thị giác.
Thông qua huấn luyện kết hợp trên tập dữ liệu hỗn hợp, Chat-
UniVi đạt được sự hiểu biết vượt trội về các chỉ thị khác nhau
và tạo ra đầu ra tự nhiên và đáng tin cậy hơn. Hơn nữa, nó
thể hiện khả năng đặc biệt để xử lý liền mạch cả ảnh và video
mà không cần bất kỳ sự điều chỉnh lại nào.

4. Thực nghiệm
4.1. Thiết lập Thực nghiệm
Cài đặt Mô hình. Chúng tôi áp dụng bộ mã hóa thị giác của
CLIP (ViT-L/14) [51] như mô hình nền tảng thị giác. Bên
cạnh đó, chúng tôi chọn mô hình Vicuna-v1.5 [62], bao gồm
7B tham số, như mô hình nền tảng ngôn ngữ của chúng tôi.

Dữ liệu và Chi tiết Huấn luyện. Đối với giai đoạn tiền huấn
luyện đa phương thức, chúng tôi sử dụng các cặp ảnh-chú
thích từ nhiều tập dữ liệu khác nhau, bao gồm COCO [12]
và CC3M-595K được sàng lọc từ CC3M [58] bởi LLaVA [40].
Chúng tôi tiền huấn luyện Chat-UniVi cho một epoch với
kích thước lô 128, sử dụng bộ tối ưu hóa AdamW [28, 41]
với lịch trình cosine. Tỷ lệ học được đặt là 2e-3, và tỷ lệ
khởi động là 0.03. Đối với giai đoạn tinh chỉnh hướng dẫn
kết hợp, chúng tôi kết hợp dữ liệu hướng dẫn đa phương thức
từ nhiều nguồn: (i) tập dữ liệu hướng dẫn đa phương thức
trong ngữ cảnh, như MIMIC-IT [2, 22, 30], (ii) tập dữ liệu
hướng dẫn thị giác, như LLaVA, (iii) dữ liệu hướng dẫn video
từ Video-ChatGPT [45]. Tất cả ảnh đầu vào hoặc khung hình
được thay đổi kích thước thành 224×224. Chúng tôi huấn
luyện Chat-UniVi cho 2 epoch với kích thước lô 128, và tỷ
lệ học được đặt là 2e-5.

4.2. Đánh giá dựa trên GPT
Hiểu Ảnh. Để đo lường định lượng khả năng hiểu ảnh, chúng
tôi báo cáo kết quả đánh giá GPT-4 trong Bảng 1. Theo Liu
et al. [40], Zhang et al. [79], chúng tôi sử dụng 90 câu hỏi
dựa trên 30 ảnh xác thực COCO, bao gồm nhiều khía cạnh
khác nhau, bao gồm cuộc trò chuyện, mô tả chi tiết (Detail),
và suy luận phức tạp (Reason). Chúng tôi sử dụng mô hình
GPT-4 để đánh giá các đầu ra của mô hình trong ba khía cạnh
này, cũng như cung cấp điểm số tổng thể. Để có mô tả toàn
diện về các chỉ số hiểu ảnh, vui lòng tham khảo phụ lục.
Như được hiển thị trong Bảng 1, Chat-UniVi sử dụng ít
token thị giác hơn trong khi đạt được hiệu suất vượt trội.
Đáng chú ý, phương pháp của chúng tôi, ngay cả như một
mô hình 7B, có thể đạt được mức hiệu suất của một mô hình
13B, chứng minh hiệu quả của phương pháp chúng tôi.

Hiểu Video. Để đo lường định lượng khả năng hiểu video,
chúng tôi báo cáo kết quả đánh giá GPT trong Bảng 2. Theo
Maaz et al. [45], chúng tôi sử dụng một tập kiểm tra dựa trên
tập dữ liệu ActivityNet [8] và sử dụng mô hình GPT-3.5 để
gán điểm số tương đối cho các đầu ra của mô hình trong năm
khía cạnh sau: Tính đúng đắn của Thông tin (Correct), Định
hướng Chi tiết (Detail), Hiểu Ngữ cảnh (Context), Hiểu Thời
gian (Temporal), và Tính nhất quán. Vui lòng tham khảo phụ
lục để biết thêm chi tiết. Như được hiển thị trong Bảng 2,
Chat-UniVi, ngay cả như một mô hình thống nhất, vượt trội
đáng kể so với các phương pháp tiên tiến được đề xuất gần
đây chỉ tập trung độc quyền vào video, điều này chứng minh
hiệu quả của phương pháp chúng tôi.

4.3. Đánh giá Câu hỏi-Trả lời
Hiệu suất ScienceQA. ScienceQA [42] là một tập dữ liệu
trả lời câu hỏi khoa học đa phương thức gồm 21k câu hỏi
trắc nghiệm. Mỗi ví dụ trong ScienceQA chứa một ngữ cảnh
thị giác, một ngữ cảnh văn bản, một câu hỏi, và nhiều lựa
chọn. Chúng tôi báo cáo cả kết quả zero-shot và tinh chỉnh
trong Bảng 3. Như được hiển thị trong Bảng 3, Chat-UniVi
thể hiện hiệu suất cạnh tranh trên tất cả các chỉ số. Đáng chú
ý, Chat-UniVi vượt trội hơn LLaMA-SciTune [20], một mô
hình được thiết kế riêng cho trả lời câu hỏi khoa học, điều
này hoàn toàn chứng minh sự vượt trội của phương pháp
chúng tôi.

Hiệu suất Trả lời Câu hỏi Video Zero-shot. Trong Bảng 4,
chúng tôi hiển thị hiệu suất trả lời câu hỏi video zero-shot
trên một số tập dữ liệu câu hỏi-trả lời mở thường được sử
dụng, bao gồm MSRVTT-QA [69], MSVD-QA [69], TGIF-QA
FrameQA [23], và ActivityNet-QA [74]. Giao thức đánh giá
của chúng tôi tuân theo Maaz et al. [45], sử dụng đánh giá
hỗ trợ GPT để đánh giá khả năng của các mô hình. Như được
hiển thị trong Bảng 4, Chat-UniVi vượt trội hơn các phương
pháp tiên tiến được đề xuất gần đây, ví dụ như FrozenBiLM
[71], trên nhiều tập dữ liệu khác nhau.

4.4. Đánh giá Ảo giác Đối tượng
Trong Bảng 5, chúng tôi báo cáo kết quả của đánh giá thăm
dò đối tượng dựa trên bỏ phiếu [34] (POPE). Để biết chi tiết
về đánh giá thăm dò đối tượng dựa trên bỏ phiếu, vui lòng
tham khảo phụ lục. Như được hiển thị trong Bảng 5, Chat-
UniVi vượt trội hơn các phương pháp tiên tiến được đề xuất
gần đây. Hơn nữa, chúng tôi thấy rằng biểu diễn đa tỷ lệ
cải thiện khả năng chống lại ảo giác. Đáng chú ý rằng, như
một mô hình 7B, phương pháp của chúng tôi thậm chí vượt
trội hơn mô hình 13B, như MiniGPT-4. Chúng tôi quy thành
công này cho biểu diễn đa tỷ lệ trang bị cho phương pháp
của chúng tôi để cảm nhận cả các khái niệm ngữ nghĩa cấp
cao và hình dạng thị giác cấp thấp.

4.5. Phân tích Loại bỏ
Ảnh hưởng của Sơ đồ Tinh chỉnh. Trong Bảng 6, chúng tôi
cung cấp nghiên cứu loại bỏ về sơ đồ tinh chỉnh hướng dẫn.
Chúng tôi thấy rằng tinh chỉnh hướng dẫn thị giác chỉ sử dụng
một loại phương tiện, như ảnh, dẫn đến sự sụt giảm trong
việc hiểu phương tiện khác, như video. Tuy nhiên, tiền huấn
luyện trên một phương tiện và tinh chỉnh trên phương tiện
khác dẫn đến sự suy giảm kiến thức từ giai đoạn tiền huấn
luyện. Trái lại, chiến lược huấn luyện kết hợp của chúng tôi,
liên quan đến huấn luyện trên một tập dữ liệu hỗn hợp của
ảnh và video, trao cho mô hình khả năng xử lý cả hai loại
đầu vào thị giác. Trong tất cả các sơ đồ tinh chỉnh, huấn
luyện kết hợp luôn đạt được hiệu suất cao nhất, xác nhận
hiệu quả của nó.

Ảnh hưởng của Số lượng Cụm Thị giác Không gian. Để
khám phá ảnh hưởng của số lượng cụm thị giác không gian,
chúng tôi cung cấp kết quả loại bỏ trong Bảng 7. Chúng tôi
thấy rằng một số lượng nhỏ hơn các cụm thị giác có thể giảm
khả năng nắm bắt các chi tiết thị giác tinh tế, trong khi một
số lượng lớn hơn các cụm thị giác có thể đưa ra sự dư thừa
và có thể giảm hiệu suất tổng thể của mô hình. Để đạt được
sự cân bằng giữa hiểu biết chi tiết và độ phức tạp học của
mô hình, chúng tôi đặt số lượng cụm ở ba cấp độ lần lượt là
64, 32, và 16 trong thực tế.

Ảnh hưởng của Số lượng Cụm Thị giác Thời gian. Video
có độ dài khác nhau, với video dài hơn thường chứa nhiều
sự kiện hơn. Do đó, trong Chat-UniVi, số lượng cụm thị giác
thời gian được xác định theo tỷ lệ dựa trên số lượng khung
hình video đầu vào. Như được hiển thị trong Bảng 8, chúng
tôi thấy rằng một tỷ lệ phân cụm nhỏ hơn có thể dẫn đến mất
thông tin thời gian quan trọng trong video. Ngược lại, một
tỷ lệ phân cụm lớn hơn tăng chi phí tính toán của mô hình.
Chúng tôi quan sát rằng mô hình hoạt động tối ưu khi tỷ lệ
phân cụm được đặt là 1/16. Do đó, trong thực tế, chúng tôi
áp dụng tỷ lệ phân cụm thời gian mặc định là 1/16 để có hiệu
suất tối ưu.

4.6. Phân tích Định tính
Đánh giá Con người. Trong đánh giá của chúng tôi, chúng
tôi đánh giá thủ công hiệu suất của Chat-UniVi và các
baseline trong 30 kịch bản trò chuyện ảnh và 30 kịch bản
trò chuyện video. Kết quả được trình bày trong Hình 4.
OpenFlamingo [3], được tạo ra từ Flamingo [1], và Otter
[30], một biến thể tinh chỉnh hướng dẫn trong ngữ cảnh
của OpenFlamingo, cũng được bao gồm trong so sánh của
chúng tôi. Như được hiển thị trong Hình 4, chúng tôi thấy
rằng các phương pháp dựa trên Flamingo thể hiện hạn chế
trong khả năng hiểu video của chúng. Hạn chế này được quy
cho việc sử dụng một transformer truy vấn để trích xuất một
số lượng token thị giác cố định từ các video có độ dài khác
nhau, điều này cản trở hiệu quả của chúng trong mô hình
hóa sự hiểu biết thời gian. Trái lại, Chat-UniVi, hoạt động
như một mô hình thống nhất, không chỉ vượt trội hơn các
phương pháp được xây dựng dựa trên Flamingo mà còn vượt
qua các mô hình được thiết kế riêng cho ảnh và video.

Trực quan hóa Token Thị giác Động. Chúng tôi cung cấp
trực quan hóa trong Hình 5 và mời độc giả khám phá thêm
nhiều trực quan hóa trong phụ lục. Quan trọng là phải nhấn
mạnh rằng phương pháp hợp nhất token đề xuất của chúng
tôi hoạt động mà không cần nhãn viền đối tượng. Như được
hiển thị trong Hình 5, các token thị giác động đề xuất hiệu
quả tổng quát hóa các đối tượng và nền. Khả năng này cho
phép Chat-UniVi hòa giải các sắc thái không gian phức tạp
của ảnh với sự hiểu biết thời gian rộng lớn hơn cần thiết cho
video với một số lượng hạn chế các token thị giác.

5. Kết luận
Trong bài báo này, chúng tôi giới thiệu Chat-UniVi, một
mô hình ngôn ngữ lớn đa phương thức thống nhất được thiết
kế để hiểu và tham gia vào các cuộc trò chuyện về cả ảnh
và video. Để kết nối liền mạch các sắc thái không gian phức
tạp của ảnh với sự hiểu biết thời gian rộng lớn hơn cần thiết
cho video, chúng tôi đề xuất một khung biểu diễn thống nhất
sử dụng các token thị giác động. Biểu diễn này tận dụng
DPC-KNN để dần dần phân cụm các token thị giác và cung
cấp các đặc trưng đa tỷ lệ. Đáng khích lệ hơn, Chat-UniVi
được huấn luyện trên một tập dữ liệu hỗn hợp bao gồm cả
ảnh và video, cho phép nó được áp dụng trực tiếp cho các
tác vụ liên quan đến cả hai loại phương tiện mà không cần
bất kỳ sửa đổi nào. Kết quả thực nghiệm rộng rãi chứng minh
rằng Chat-UniVi, như một mô hình thống nhất, luôn vượt
trội hơn ngay cả các phương pháp được thiết kế độc quyền
cho ảnh hoặc video.

Lời cảm ơn. Công trình này được hỗ trợ bởi Chương trình
Nghiên cứu và Phát triển Trọng điểm Quốc gia của Trung
Quốc (2022ZD0118101), Quỹ Khoa học Tự nhiên của Trung
Quốc (Số 62202014), và Chương trình Nghiên cứu Cơ bản
Thâm Quyến (Số JCYJ20220813151736001).

--- TRANG 9 ---
Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: một
mô hình ngôn ngữ thị giác cho học few-shot. Trong NeurIPS,
trang 23716–23736, 2022. 2, 8, 13

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, và Devi Parikh.
VQA: Trả lời câu hỏi thị giác. Trong ICCV, trang 2425–
2433, 2015. 5, 14

[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: Một khung
mã nguồn mở để huấn luyện các mô hình ngôn ngữ thị giác
tự hồi quy lớn. arXiv preprint arXiv:2308.01390, 2023.
8, 13

[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren
Zhou. Qwen-VL: Một mô hình ngôn ngữ thị giác lớn tiên
phong với khả năng đa năng. arXiv preprint arXiv:2308.12966,
2023. 2

[5] Max Bain, Arsha Nagrani, Gül Varol, và Andrew Zisserman.
Đóng băng trong thời gian: Một bộ mã hóa video và ảnh
chung cho truy xuất end-to-end. Trong ICCV, trang 1728–
1738, 2021. 2

[6] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, và Judy Hoffman. Hợp
nhất token: ViT của bạn nhưng nhanh hơn. arXiv preprint
arXiv:2210.09461, 2022. 3

[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Các
mô hình ngôn ngữ là những người học few-shot. Trong
NeurIPS, trang 1877–1901, 2020. 1, 2, 14

[8] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
và Juan Carlos Niebles. ActivityNet: Một benchmark video
quy mô lớn để hiểu hoạt động của con người. Trong CVPR,
trang 961–970, 2015. 6, 18

[9] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang,
Jing Shi, Shuang Xu, và Bo Xu. X-LLM: Khởi động các
mô hình ngôn ngữ lớn tiên tiến bằng cách coi các phương
thức đa dạng như ngôn ngữ nước ngoài. arXiv preprint
arXiv:2305.04160, 2023. 2, 3, 13

[10] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
Vikas Chandra, Yunyang Xiong, và Mohamed Elhoseiny.
MiniGPT-v2: mô hình ngôn ngữ lớn như một giao diện
thống nhất cho học đa tác vụ ngôn ngữ-thị giác. arXiv
preprint arXiv:2310.09478, 2023.

[11] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, và Rui Zhao. Shikra: Giải phóng phép thuật
đối thoại tham chiếu của LLM đa phương thức. arXiv
preprint arXiv:2306.15195, 2023. 2

[12] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam,
Saurabh Gupta, Piotr Dollár, và C Lawrence Zitnick.
Microsoft COCO Captions: Thu thập dữ liệu và máy chủ
đánh giá. arXiv preprint arXiv:1504.00325, 2015. 5, 14

[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. PaLM: Mở rộng mô hình ngôn ngữ với
pathways. arXiv preprint arXiv:2204.02311, 2022. 2

[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, và Steven Hoi. InstructBLIP: Hướng tới các mô hình
ngôn ngữ thị giác đa năng với tinh chỉnh hướng dẫn. arXiv
preprint arXiv:2305.06500, 2023. 6

[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain
Gelly, Jakob Uszkoreit, và Neil Houlsby. Một ảnh đáng giá
16x16 từ: Transformer cho nhận dạng ảnh ở tỷ lệ lớn. Trong
ICLR, 2021. 2

[16] Mingjing Du, Shifei Ding, và Hongjie Jia. Nghiên cứu về
phân cụm density peaks dựa trên k-nearest neighbors và
phân tích thành phần chính. Knowledge-Based Systems, 99:
135–145, 2016. 2, 4

[17] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu
Yue, et al. LLaMA-Adapter V2: Mô hình hướng dẫn thị giác
hiệu quả về tham số. arXiv preprint arXiv:2304.15010, 2023. 2

[18] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan
Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin,
Peng Jin, et al. SPHINX-X: Mở rộng dữ liệu và tham số
cho một họ mô hình ngôn ngữ lớn đa phương thức. arXiv
preprint arXiv:2402.05935, 2024.

[19] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
Luo, và Kai Chen. MultiModal-GPT: Một mô hình thị giác
và ngôn ngữ cho đối thoại với con người. arXiv preprint
arXiv:2305.04790, 2023. 2, 6

[20] Sameera Horawalavithana, Sai Munikoti, Ian Stewart, và
Henry Kvinge. Scitune: Căn chỉnh các mô hình ngôn ngữ
lớn với hướng dẫn đa phương thức khoa học. arXiv preprint
arXiv:2307.01139, 2023. 5, 6

[21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. LoRA:
Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn.
Trong ICLR, 2022. 15

[22] Drew A Hudson và Christopher D Manning. Gqa: Một tập
dữ liệu mới cho suy luận thị giác thế giới thực và trả lời
câu hỏi có tính tổng hợp. Trong CVPR, trang 6700–6709,
2019. 5, 14

[23] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, và
Gunhee Kim. TGIF-QA: Hướng tới suy luận không gian-thời
gian trong trả lời câu hỏi thị giác. Trong CVPR, trang 2758–
2766, 2017. 6

[24] Peng Jin, Jinfa Huang, Fenglin Liu, Xian Wu, Shen Ge,
Guoli Song, David Clifton, và Jie Chen. Học tương phản
tối đa hóa kỳ vọng cho biểu diễn video-và-ngôn ngữ nhỏ
gọn. Trong NeurIPS, trang 30291–30306, 2022. 2

[25] Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian,
Chang Liu, Xiangyang Ji, Li Yuan, và Jie Chen. Video-text
như người chơi game: Tương tác banzhaf phân cấp cho
học biểu diễn đa phương thức. Trong CVPR, trang 2472–
2482, 2023. 13

[26] Peng Jin, Hao Li, Zesen Cheng, Jinfa Huang, Zhennan
Wang, Li Yuan, Chang Liu, và Jie Chen. Truy xuất văn bản-video
với khái niệm hóa tách biệt và căn chỉnh tập-thành-tập.
Trong IJCAI, trang 938–946, 2023.

[27] Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji,
Chang Liu, Li Yuan, và Jie Chen. Diffusionret: Truy xuất
văn bản-video tạo sinh với mô hình khuếch tán. Trong ICCV,
trang 2470–2481, 2023. 2

[28] Diederik P Kingma và Jimmy Ba. Adam: Một phương pháp
cho tối ưu hóa ngẫu nhiên. arXiv preprint arXiv:1412.6980,
2014. 5

[29] Nathan Labiosa, Dat Huynh, và Ser-Nam Lim. Thông tin
thị giác và các mô hình ngôn ngữ lớn: Một phân tích sâu
hơn. 2

[30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, và Ziwei Liu. Otter: Một mô hình đa phương
thức với tinh chỉnh hướng dẫn trong ngữ cảnh. arXiv
preprint arXiv:2305.03726, 2023. 5, 8, 13, 14

[31] Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi.
BLIP: Khởi động tiền huấn luyện ngôn ngữ-ảnh cho hiểu
biết và tạo sinh ngôn ngữ thị giác thống nhất. Trong ICML,
trang 12888–12900, 2022. 2

[32] Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi.
BLIP-2: Khởi động tiền huấn luyện ngôn ngữ-ảnh với các
bộ mã hóa ảnh đóng băng và các mô hình ngôn ngữ lớn.
arXiv preprint arXiv:2301.12597, 2023. 3

[33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, và Yu Qiao.
Videochat: Hiểu video tập trung vào trò chuyện. arXiv
preprint arXiv:2305.06355, 2023. 3, 5, 6

[34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, và Ji-Rong Wen. Đánh giá ảo giác đối tượng trong
các mô hình ngôn ngữ thị giác lớn. arXiv preprint
arXiv:2305.10355, 2023. 6, 16, 20, 21

[35] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, và
Li Yuan. Video-LLaVA: Học biểu diễn thị giác thống nhất
bằng căn chỉnh trước chiếu. arXiv preprint arXiv:2311.10122,
2023. 3

[36] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng
Jin, Junwu Zhang, Munan Ning, và Li Yuan. MoE-LLaVA:
Hỗn hợp chuyên gia cho các mô hình ngôn ngữ thị giác lớn.
arXiv preprint arXiv:2401.15947, 2024. 2

[37] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
Yacoob, Dinesh Manocha, và Tianyi Zhou. Hallusionbench:
Bạn thấy những gì bạn nghĩ? hay bạn nghĩ những gì bạn
thấy? một benchmark suy luận ngữ cảnh ảnh thách thức
cho gpt-4v(ision), llava-1.5, và các mô hình đa phương
thức khác. arXiv preprint arXiv:2310.14566, 2023.

[38] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, và Lijuan Wang. Giảm thiểu ảo giác trong các
mô hình đa phương thức lớn thông qua tinh chỉnh hướng
dẫn mạnh mẽ. arXiv preprint arXiv:2306.14565, 2023. 2

[39] Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee.
Baseline cải tiến với tinh chỉnh hướng dẫn thị giác. arXiv
preprint arXiv:2310.03744, 2023. 1

[40] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee.
Tinh chỉnh hướng dẫn thị giác. arXiv preprint arXiv:2304.08485,
2023. 1, 4, 5, 6, 14, 15, 18

[41] Ilya Loshchilov và Frank Hutter. Regularization phân rã
trọng số tách biệt. arXiv preprint arXiv:1711.05101, 2017. 5

[42] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, và
Ashwin Kalyan. Học để giải thích: Suy luận đa phương
thức thông qua chuỗi suy nghĩ cho trả lời câu hỏi khoa học.
Trong NeurIPS, trang 2507–2521, 2022. 5, 6

[43] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai
Sun, và Rongrong Ji. Rẻ và nhanh: Tinh chỉnh hướng dẫn
ngôn ngữ-thị giác hiệu quả cho các mô hình ngôn ngữ lớn.
Trong NeurIPS, 2023. 2

[44] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang
Liu, và Yun Fu. Ảnh như tập hợp điểm. Trong ICLR, 2023.
3, 13, 14

[45] Muhammad Maaz, Hanoona Rasheed, Salman Khan, và
Fahad Shahbaz Khan. Video-ChatGPT: Hướng tới hiểu
video chi tiết thông qua các mô hình thị giác và ngôn ngữ
lớn. arXiv preprint arXiv:2306.05424, 2023. 1, 3, 5, 6,
14, 18, 19

[46] OpenAI. Giới thiệu chatgpt. CoRR, 2022. 2

[47] OpenAI. Báo cáo kỹ thuật GPT-4. CoRR, 2023. 3

[48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal,
Katarina Slama, Alex Ray, et al. Huấn luyện các mô hình
ngôn ngữ để tuân theo hướng dẫn với phản hồi của con
người. Trong NeurIPS, trang 27730–27744, 2022. 2

[49] Ofir Press, Noah A Smith, và Mike Lewis. Huấn luyện
ngắn, kiểm tra dài: Attention với bias tuyến tính cho phép
ngoại suy độ dài đầu vào. Trong ICLR, 2022. 14

[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Các mô hình ngôn ngữ là
những người học đa tác vụ không giám sát. OpenAI blog,
1(8):9, 2019. 2

[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, và Ilya Sutskever. Học các mô hình thị giác có
thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong
ICML, trang 8748–8763, 2021. 4, 5, 15

[52] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
Jordan Hoffmann, Francis Song, John Aslanides, Sarah
Henderson, Roman Ring, Susannah Young, et al. Mở rộng
các mô hình ngôn ngữ: Phương pháp, phân tích & cái nhìn
sâu sắc từ huấn luyện gopher. arXiv preprint arXiv:2112.11446,
2021. 14

[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và
Peter J Liu. Khám phá giới hạn của học chuyển giao với
một transformer văn bản-thành-văn bản thống nhất. The
Journal of Machine Learning Research, 21(1):5485–5551,
2020. 2

[54] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, và Cho-Jui Hsieh. DynamicViT: Transformer thị giác
hiệu quả với thưa thớt token động. Trong NeurIPS, trang
13937–13949, 2021. 3

[55] Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, và
Lu Hou. TESTA: Tổng hợp token thời gian-không gian
cho hiểu video-ngôn ngữ dài. arXiv preprint arXiv:2310.19060,
2023. 3

[56] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, và Lu
Hou. TimeChat: Một mô hình ngôn ngữ lớn đa phương
thức nhạy cảm thời gian cho hiểu video dài. arXiv preprint
arXiv:2312.02051, 2023. 2

[57] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,
Alexandra Sasha Luccioni, François Yvon, Matthias Gallé,
et al. BLOOM: Một mô hình ngôn ngữ đa ngữ mở truy
cập 176b tham số. arXiv preprint arXiv:2211.05100, 2022. 2

[58] Piyush Sharma, Nan Ding, Sebastian Goodman, và Radu
Soricut. Conceptual Captions: Một tập dữ liệu văn bản
thay thế ảnh được làm sạch, hypernym cho tạo chú thích
ảnh tự động. Trong ACL, trang 2556–2565, 2018. 5, 14

[59] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, và Yueting Zhuang. HuggingGPT: Giải quyết
các tác vụ ai với chatgpt và bạn bè của nó trong huggingface.
arXiv preprint arXiv:2303.17580, 2023. 3

[60] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, và Yue
Cao. Eva-clip: Kỹ thuật huấn luyện cải tiến cho clip ở
quy mô lớn. arXiv preprint arXiv:2303.15389, 2023. 15

[61] Dídac Surís, Sachit Menon, và Carl Vondrick. ViperGPT:
Suy luận thị giác thông qua thực thi python để suy luận.
arXiv preprint arXiv:2303.08128, 2023. 3

[62] Vicuna Team. Vicuna: Một chatbot mở gây ấn tượng với
gpt-4 với 90% chất lượng chatgpt. 2023. 5, 15

[63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
LLaMA: Các mô hình ngôn ngữ nền tảng mở và hiệu quả.
arXiv preprint arXiv:2302.13971, 2023. 1

[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
LLaMA 2: Các mô hình trò chuyện nền tảng mở và được
tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023. 1, 15

[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia
Polosukhin. Attention là tất cả những gì bạn cần. Trong
NeurIPS, 2017. 2

[66] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo,
Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang
Jiang, và Lu Yuan. OmniVL: Một mô hình nền tảng cho
các tác vụ ngôn ngữ ảnh và ngôn ngữ video. Trong NeurIPS,
trang 5696–5710, 2022. 2

[67] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,
Zecheng Tang, và Nan Duan. Visual ChatGPT: Nói chuyện,
vẽ và chỉnh sửa với các mô hình nền tảng thị giác. arXiv
preprint arXiv:2303.04671, 2023. 3

[68] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, và Tat-Seng
Chua. NExT-GPT: LLM đa phương thức bất kỳ-thành-bất
kỳ. arXiv preprint arXiv:2309.05519, 2023. 2, 3, 13

[69] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, và Yueting Zhuang. Trả lời câu hỏi video
thông qua attention được tinh chỉnh dần dần về hình dạng
và chuyển động. Trong ACM MM, trang 1645–1653, 2017. 6

[70] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, và Xiaolong Wang. GroupViT:
Phân đoạn ngữ nghĩa xuất hiện từ giám sát văn bản. Trong
CVPR, trang 18134–18144, 2022. 3, 13

[71] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, và
Cordelia Schmid. Trả lời câu hỏi video zero-shot thông
qua các mô hình ngôn ngữ hai chiều đóng băng. Trong
NeurIPS, trang 124–141, 2022. 6

[72] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,
Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,
Michael Zeng, và Lijuan Wang. MM-REACT: Thúc đẩy
chatgpt cho suy luận và hành động đa phương thức. arXiv
preprint arXiv:2303.11381, 2023. 3

[73] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mPLUG-Owl: Modularization trao quyền
cho các mô hình ngôn ngữ lớn với đa phương thức. arXiv
preprint arXiv:2304.14178, 2023. 1, 6

[74] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting
Zhuang, và Dacheng Tao. ActivityNet-QA: Một tập dữ liệu
để hiểu các video web phức tạp thông qua trả lời câu hỏi.
Trong AAAI, trang 9127–9134, 2019. 6

[75] Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo,
Wanli Ouyang, và Xiaogang Wang. Không phải tất cả các
token đều bằng nhau: Phân tích thị giác tập trung vào con
người thông qua transformer phân cụm token. Trong CVPR,
trang 11101–11111, 2022. 3, 13

[76] Hang Zhang, Xin Li, và Lidong Bing. Video-LLaMA: Một
mô hình ngôn ngữ âm thanh-thị giác được tinh chỉnh hướng
dẫn để hiểu video. arXiv preprint arXiv:2306.02858, 2023.
3, 5, 6

[77] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Yu Qiao.
LLaMA-Adapter: Tinh chỉnh hiệu quả của các mô hình
ngôn ngữ với attention khởi tạo zero. arXiv preprint
arXiv:2303.16199, 2023. 5, 6

[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,
Xian Li, Xi Victoria Lin, et al. OPT: Các mô hình ngôn
ngữ transformer tiền huấn luyện mở. arXiv preprint
arXiv:2205.01068, 2022. 2

[79] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,
Nedim Lipka, Diyi Yang, và Tong Sun. LLaVAR: Tinh chỉnh
hướng dẫn thị giác nâng cao cho hiểu ảnh giàu văn bản.
arXiv preprint arXiv:2306.17107, 2023. 6, 18

[80] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, và Sameer
Singh. Hiệu chuẩn trước khi sử dụng: Cải thiện hiệu suất
few-shot của các mô hình ngôn ngữ. Trong ICML, trang
12697–12706, 2021. 14

[81] Kaizhi Zheng, Xuehai He, và Xin Eric Wang. MiniGPT-5:
Tạo sinh thị giác-và-ngôn ngữ xen kẽ thông qua vokens
tạo sinh. arXiv preprint arXiv:2310.02239, 2023. 3

[82] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa
Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei
Li, et al. LanguageBind: Mở rộng tiền huấn luyện video-ngôn
ngữ thành n-phương thức bằng căn chỉnh ngữ nghĩa dựa
trên ngôn ngữ. arXiv preprint arXiv:2310.01852, 2023. 2

[83] Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang,
Qi Song, Mingjun Pan, và Li Yuan. LLMBind: Một khung
tích hợp phương thức-tác vụ thống nhất. arXiv preprint
arXiv:2402.14891, 2024. 3

[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed
Elhoseiny. MiniGPT-4: Nâng cao hiểu biết ngôn ngữ-thị
giác với các mô hình ngôn ngữ lớn tiên tiến. arXiv preprint
arXiv:2304.10592, 2023. 1, 6
