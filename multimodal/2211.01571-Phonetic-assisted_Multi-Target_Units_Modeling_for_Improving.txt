# 2211.01571.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2211.01571.pdf
# File size: 652263 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Phonetic-assisted Multi-Target Units Modeling for Improving
Conformer-Transducer ASR system
Li Li1, Dongxing Xu2, Haoran Wei3, Yanhua Long1∗
1Shanghai Engineering Research Center of Intelligent Education and Bigdata,
Shanghai Normal University, Shanghai, China
2Unisound AI Technology Co., Ltd., Beijing, China
3Department of ECE, University of Texas at Dallas, Richardson, TX 75080, USA
lili a0@163.com, xudongxing@unisound.com, haoran.wei@utdallas.edu, yanhua@shnu.edu.cn
Abstract
Exploiting effective target modeling units is very impor-
tant and has always been a concern in end-to-end automatic
speech recognition (ASR). In this work, we propose a phonetic-
assisted multi-target units (PMU) modeling approach, to en-
hance the Conformer-Transducer ASR system in a progres-
sive representation learning manner. Specifically, PMU first
uses the pronunciation-assisted subword modeling (PASM) and
byte pair encoding (BPE) to produce phonetic-induced and text-
induced target units separately; Then, three new frameworks are
investigated to enhance the acoustic encoder, including a basic
PMU, a paraCTC and a paCTC, they integrate the PASM and
BPE units at different levels for CTC and transducer multi-task
training. Experiments on both LibriSpeech and accented ASR
tasks show that, the proposed PMU significantly outperforms
the conventional BPE, it reduces the WER of LibriSpeech clean,
other, and six accented ASR testsets by relative 12.7%, 4.3%
and 7.7%, respectively.
Index Terms : multi-target units, PMU, paCTC, Conformer-
Transducer, end-to-end ASR
1. Introduction
The Conformer-Transducer (ConformerT) has achieved state-
of-the-art results in many ASR tasks [1–4] because of its perfect
inheritance of the advantages of conformer and transducer. It
captures both local and global features by combining the convo-
lution module and transformer in a parameter-efficient way. To-
gether with the natural streaming property of transducer, Con-
formerT has become increasingly appealing in recent end-to-
end (E2E) ASR systems.
As many E2E ASR systems, exploring effective target mod-
eling units for ConformerT is also very crucial. The main types
of E2E ASR target units can be divided into the text-induced
units and phonetic-induced ones. The character, word and sub-
word are all text-induced units and have been extensively stud-
ied [5–10]. Compared with character, subword can avoid too
long output sequence and dependency, which reduces the dif-
ficulty of modeling and decoding [7]. Many subword mod-
eling techniques have been proposed: the byte pair encoding
(BPE) [11], WordPieceModel (WPM) [12] and unigram lan-
guage model (ULM) [13], etc. However, all of these techniques
are purely text-induced without any access to the underlying
phonetic/pronunciation information which is the key of ASR.
The syllable, phoneme [5, 6, 14, 15] belong to the phonetic-
induced target units, they enable the model to learn better pho-
∗Yanhua Long is the corresponding author, she is also with the
Key Innovation Group of Digital Humanities Resource and Research,
Shanghai Normal University. The work is supported by the National
Natural Science Foundation of China (Grant No.62071302).netic patterns of a language, however, an additional pronunci-
ation lexicon is required during both model training and infer-
ence. Therefore, how to well exploit the information in both
text-induced and phonetic-induced target units become very im-
portant and fundamental.
In the literature, several recent works have been proposed
to combine the text and phonetic information for building better
E2E ASR system. Such as, [16] proposed a hybrid target unit
of syllable-char-subword in a joint CTC/Attention multi-task
learning for the Mandarin ASR system; While in [17], a pro-
nunciation assisted subword modeling (PASM) was introduced
to extract ASR target units by exploring their acoustic structure
from the pronunciation lexicon. In addition, [18] tried to exploit
the text and underlying phonetic information in acoustics in an-
other way, the authors used a set of hierarchically increasing
text units to the CTC modeling of intermediate Transformer en-
coder layers. All these works have been verified to be effective
for improving current E2E ASR systems.
Motivated by the PASM and work in [18], this study aims
to improve the Conformer-Transducer ASR system by propos-
ing a phonetic-assisted multi-target units modeling (PMU) ap-
proach. The PMU is designed to learn information from both
the phonetic-induced PASM and text-induced BPE units, using
three new acoustic modeling frameworks as follows: 1) Basic
PMU. The ConformerT is trained with both CTC and trans-
ducer losses, but assigning PASM and BPE units to them re-
spectively; 2) paraCTC. With the same BPE units as in 1) for
transducer, a parallel CTC loss with both PASM and BPE as
Conformer encoder’s target units is taken as the auxiliary task
of ConformerT model training; 3) paCTC. Different from 1)
and 2), paCTC adopts a phonetic conditioned acoustic encoder,
by using the PASM and BPE units in a interactive manner to the
CTC loss of different intermediate Conformer encoders. From
the experiments that conducted on LibriSpeech and Common-
V oice datasets, we see that the standard ConformerT is signifi-
cantly improved by our proposed PMU, up to relative 4.3% to
12.7% WER reductions are achieved on the LibriSpeech clean
and other, or the six accented English test sets of Common-
V oice.
2. Conformer-Transducer
The Conformer-Transducer (ConformerT) was first proposed
in [1,2]. It can be trained using the end-to-end RNN-T loss [19]
with a label encoder and a Conformer-based acoustic encoder
(AEncoder). The architecture is illustrated in Fig.1 (a). Given
an input acoustic feature with Tframes as x= (x1,. . . ,xT), and
its transcription label sequence of length Uasy= (y1, . . . , y U).
The AEncoder first transforms xinto a high representation
ht, t≤T, and the label encoder, acting as a language model,arXiv:2211.01571v2  [eess.AS]  7 Jul 2023

--- PAGE 2 ---
Figure 1: Structure of (a) the proposed phonetic-assisted multi-target units (PMU) and (b) its phonetic-conditioned AEncoder (paCTC).
produces a representation hugiven its previous emitted label se-
quence yu−1
1. Then, htandhuare combined using the joint net-
work composed of feed-forward layers and a non-linear func-
tion to compute output logits. Finally, by applying a Softmax
to the output logits, we can produce the distribution of current
target probabilities as:
P(ˆyt,u|x, yu−1
1) =Softmax (Joint (ht, hu)) (1)
The label ˆyt,ucan optionally be blank symbol. Remov-
ing all the blank symbols in ˆyt,usequence yields y. Given A,
the set of all possible alignments ˆy(with blank symbols ϕ) be-
tween input xand output y, ConformerT loss function can be
computed as the following negative log posterior:
Ltrans =−logP(y|x) =−logX
ˆy∈AP(ˆy|x) (2)
Besides the transducer loss Ltrans , as in [20], we also
jointly train ConformerT with an auxiliary CTC loss LCTC[21]
to learn frame-level acoustic representations and provide su-
pervision to the AEncoder. The overall ConformerT objective
function is defined as:
Lobj=λtransLtrans +λctcLCTC (3)
where λtrans, λctc∈[0,1]are tunable loss weights.
3. Proposed Methods
Although the PASM [17] has been proposed to enhance the ex-
traction of E2E ASR target units, by leveraging the phonetic
structure of acoustics in speech using a pronunciation lexicon,
most current ConformerT ASR systems still only use the purely
text-induced subwords, such as BPEs, wordpieces as their target
modeling units [2,3]. This may because constrained by the pho-
netic pattern in lexicon, PASM tends to produce short subwords
and avoids modeling larger or full-words with single tokens, the
resulting relative small vocabulary size greatly limits the per-
formance upper bound of PASM. Therefore, in this study, we
propose a phonetic-assisted multi-target units (PMU) modeling
to integrate the advantages of both PASM and BPE units forimproving ConformerT in a CTC/transducer multi-task training
framework.
The whole architecture of ConformerT with PMU is
demonstrated in Fig.1(a), where we use different types of tar-
get units for the CTC and transducer branch. The BPE-trans
means using text-induced BPE units to align the transducer out-
puts during ConformerT training, while for the shared acous-
tic encoder (AEncoder) with CTC branch, we investigate three
new target units modeling methods, as illustrated in the left
part of Fig.1(a) and Fig.1(b), the first one is the basic PMU
withPASM-CTC , where only PASM units are taken as the CTC
targets, the other two replace PASM-CTC with a paraCTC
and a paCTC separately. All the PASM-CTC ,BPE-CTC and
BPE-trans are composed of a single fully-connected feed-
forward layer with different target units followed by Softmax
function. Given both PASM and BPE units, the overall objec-
tive loss of basic PMU is defined as,
LPMU =λtransLBPE-trans +λctcLPASM-CTC (4)
Where LBPE-trans andLPASM-CTC represent the loss of trans-
ducer and CTC using BPE and PASM units as their targets, re-
spectively. If we use paraCTC orpaCTC , the loss of LPASM-CTC
in Eq.(4) will be replaced by their corresponding CTC loss
LparaCTC andLpaCTC , respectively. The details of how to pro-
duce PASM units with a given pronunciation lexicon and train-
ing texts can be found in [17].
3.1. ParaCTC
Training a model with CTC loss applied in parallel to the final
layer has recently achieved success [22–25]. In our paraCTC ,
as shown in Fig.1(a), we use two different linear layers to trans-
form the AEncoder representation to BPE and PASM units with
LCTC(yBPE|x)andLCTC(yPASM|x)loss, respectively. The over-
all loss function of paraCTC is defined as,
LparaCTC =αLCTC(yPASM|x) + (1 −α)LCTC(yBPE|x)(5)
where α∈(0,1),yPASM andyBPErepresent the target units of
CTC is PASM and BPE respectively. With Eq.(5), the underly-
ing phonetic and text structure information in PASM and BPE
are effectively exploited and combined to boost the AEncoder.

--- PAGE 3 ---
3.2. PaCTC
Different from basic PMU and paraCTC , our proposed paCTC
enhances the AEncoder in a phonetic-conditioned manner, by
using the CTC alignments between yPASM oryBPEand the out-
put of intermediate AEncoder layers. The overview structure of
paCTC is shown in Fig.1(b). We first cut the whole AEncoder
into the lower N1and top N3layers. Then, the PASM-CTC
andBPE-CTC joint training are applied to these two AEn-
coder blocks for aligning their frame-level outputs hN1andhN3
respectively, using their corresponding loss of LN1
PASM-CTC and
LN3
BPE-CTC as,
LpaCTC =βLN1
PASM-CTC + (1−β)LN3
BPE-CTC (6)
Where LpaCTC is the total paCTC loss and β∈(0,1)is a weight
parameter.
Moreover, as illustrated in Fig.1(b), a self-condition(SC)
mechanism [26] is applied to further improve the AEncoder, by
making the subsequent AEncoder layers conditioned on both
the previous layer representation and the intermediate CTC pre-
dictions. The Linear in SC means using a fully-connected
layer to linearly transform the dimension of intermediate CTC
predictions to the same dimension of AEncoder layers. We
expect paCTC can outperform the other two PMU variants,
because it integrates both PASM and BPE advantages in a
more effective way, by applying PASM-CTC on lower AEn-
coder enables ConformerT to learn better acoustic representa-
tions from the phonetic-induced PASM modeling, while apply-
ingBPE-CTC on the top AEncoder helps to produce more ro-
bust linguistic embeddings.
In addition, inspired by the idea of hierarchically increas-
ing subword units in [18], we also design an optional structure
(dashed block in Fig.1) in paCTC , by inserting an intermediate
BPE-CTC alignment at the middle AEncoder block with N2
layers. With this optional structure, Eq.(6) is then modified as
follows:
LpaCTC =β
2
LN1
PASM-CTC +LN2
BPE-CTC
+ (1−β)LN3
BPE-CTC (7)
where LN2
BPE-CTC is the BPE-CTC loss of intermediate mid-
dle AEncoder block. It’s worth noting that the intermediate
BPE-CTC andPASM-CTC have the same vocabulary size that
is much smaller than the one of BPE-CTC applied to the top
AEncoder block, such as 194 versus 3000. This paCTC with
optional structure can not only leverage low-level phonetic in-
formation to produce better high-level linguistic targets, but also
achieve a progressive representation learning process which can
integrate different types of subwords in a fine-to-coarse man-
ner. What’s more, we explore two different variants of paCTC
with optional structure, namely paCTC-s andpaCTC-us .
paCTC-s means we not only share two SC linear layers, but
also share the linear layer parameters of both PASM-CTC and
intermediate BPE-CTC , while paCTC-us means not.
4. Experiments and Results
4.1. Datasets
Our experiments are performed on two open-source English
ASR tasks, one is the LibriSpeech dataset [27] with 100hrs
training data and its clean and other test sets, the other is an
accented ASR task with data selected from CommonV oice cor-
pus [28]. Our accented English training data has 150 hours (hrs)
of speech, including Indian, US and England accents and each
with 50 hrs. We construct six test sets to evaluate the pro-
posed methods for accented ASR, including three in-domaintests with 2 hrs US, 1.92 hrs England and 3.87 hrs Indian ac-
cent speech, three out-of-domain test sets with 2 hrs Singapore,
2.2 hrs Canada and 2 hrs Australia accent speech.
4.2. Experimental Setup
All our experiments are implemented using library from the
end-to-end speech recognition toolkit ESPnet [29]. We use
global mean-variance normalized 80-dimensional log-mel fil-
terbank as input acoustic features. No data augmentation tech-
niques and no extra language model are applied.
For the acoustic encoder of ConformerT, we sub-sample the
input features by a factor of 4 using two 2D-convolutional lay-
ers, followed by 12 conformer encoder layers with 2048 feed-
forward dimension and 512 attention dimension with 8 self-
attention heads. For the label encoder, we only use a 512-
dimensional LSTM. The joint network is a 640-dimensional
feed-forward network with tanh activation function. The
warmup is set to 25000, and both label smoothing [30] weight
and dropout is set to 0.1 for model regularization. The BPE
units are generated by SentencePiece [31], and fast align [32]
is used to produce PASM units with the CMU pronunciation
lexicon1. In Table 1 and Table 2, βis set to 0.5 and 0.7, re-
spectively, α= 0.7for the paraCTC, λtrans =λctc= 0.5for
all the systems with paCTC. All the system performances are
evaluated using word error rates (WER (%)).
4.3. Results
4.3.1. Results on Librispeech
Table 1: WER(%) on the clean and other test sets of Libri-
100hrs ASR task. TU ctcand TU trans represent the type of
target units for CTC and transducer in ConformerT, respec-
tively. In paCTC, system 9-10 use the optional structure with
N1=N2=N3= 4, while system 8 does not ( N2= 0, N1=
N3= 6).
ID Methods TUctc TUtransEvaluation
Clean Other
1
ConformerTBPE-194 11.2 30.6
2 BPE-3000 11.0 29.9
3 PASM-194 10.5 30.5
4
PMUPASM-194 BPE-194 10.2 30.0
5 BPE-194 PASM-194 10.7 30.2
6 PASM-194 BPE-3000 10.1 28.4
7 paraCTC BPE-3000 9.8 28.4
8 paCTC BPE-3000 9.7 28.4
9 paCTC-s BPE-3000 9.7 28.3
10 paCTC-us BPE-3000 9.6 28.6
We first examine our proposed methods on the clean and
other test sets of Librispeech ASR task. Results are shown in
Table 1. System 1 to 3 are our ConformerT baselines, each with
its both CTC and transducer branches using a single type of
target units. ‘BPE/PASM-*’ means using BPE or PASM units
with different vocabulary size. In our extensive experiments,
we find 194 and 3000 are the best setups for PASM and BPE
on the Libri-100hrs dataset, respectively. ‘BPE-194’ is used to
make a fair comparison with ‘PASM-194’. System 4-10 are the
ConformerT models trained using our proposed PMU frame-
work with three different variants: the basic PMU (system 4-6),
PMU with paraCTC (system 7) and PMU with different struc-
ture of paCTC (system 8-10).
1http://www.speech.cs.cmu.edu/cgi-bin/cmudict

--- PAGE 4 ---
Table 2: WER(%) on the in-domain and out-of-domain test sets on accented CommonVoice ASR task. In the paCTC, setup 9-10 use the
optional structure, while setup 7-8 do not.
ID Methods TUctc TUtransIn-domain Out-of-domainOverallEngland Indian US Australia Canada Singapore
1
ConformerTBPE-3000 21.9 26.8 18.7 24.1 17.1 35.4 24.4
2 BPE-205 21.7 25.0 17.6 24.5 16.2 33.8 23.4
3 PASM-205 21.6 24.7 17.8 24.6 16.6 35.1 23.9
4
PMUPASM-205 BPE-205 21.4 24.7 17.8 23.7 16.3 33.7 23.2
5 BPE-205 PASM-205 21.9 24.9 17.8 24.4 16.1 33.7 23.4
6 paraCTC BPE-205 21.3 24.7 17.3 24.0 16.2 33.1 23.1
7 paCTC BPE-205 20.5 24.1 16.5 23.3 15.3 32.6 22.4
8 paCTC BPE-3000 19.6 24.0 16.6 22.5 15.2 32.3 22.1
9 paCTC-s BPE-3000 19.9 23.6 16.4 22.0 15.4 31.3 21.8
10 paCTC-us BPE-3000 19.8 23.1 15.8 22.5 14.8 31.4 21.6
Comparing results of system 1-3 in Table 1, we see
there is no big difference performance gap between using
phonetic-induced PASM and text-induced BPE as their both
CTC/transducer target units. PASM achieves the best result on
the clean test set, while BPE obtains the best one on the other
test set. However, when the proposed PMU modeling meth-
ods are applied, both WERs on the clean and other test sets are
greatly reduced. When comparing the results of ConformerT
with conventional BPE-3000 (system 2), even with the basic
PMU, system 6 still achieves relative 8.2% and 5.0% WER re-
ductions on the clean and other sets, respectively. Meanwhile,
by comparing system 4 to 6, it’s clear that using PASM as CTC
alignments, while larger BPE units as transducer targets is the
best setup for basic PMU, it may due to the fact that, the clear
phonetic correspondence of target units is critical for such time
synchronous model. When comparing system 6 with 7-10, we
see continuous WER reduction on the clean test set, even the
performance improvement on the other set is limited. Finally,
thepaCTC-us achieves the best results on the clean testset.
Compared with the best baseline (system 2), system 10 achieves
12.7% and 4.3% relative WER reduction on clean and other test
set, respectively.
Table 3: WER(%) on Libri-100hrs clean and other test sets for
PMU with paCTC (Fig.1 (b), Eq.(6)) without optional structure
under different setup conditions. Setup 4 means replacing the
PASM-CTC with BPE-194 CTC at N1layers.
ID#layerβEvaluation
N1N3 Clean Other
1 6 6 0.3 9.8 28.0
2 6 6 0.5 9.7 28.4
3 6 6 0.7 10.0 28.6
4 6 6 0.5, BPE-194 10.0 29.4
5 3 9 0.5 10.3 30.1
6 9 3 0.5 9.8 28.6
In fact, before we propose the paCTC with optional struc-
ture, we perform a set of parameter tuning experiments to see
how they affect the paCTC performance. Results are shown
in Table 3. Setup 1-3, 5-6 are all with the PASM-194 at N1
AEncoder block, and BPE-3000 at the top block. We see that,
N1=N3= 6 withβ= 0.5achieves relatively stable results.
Furthermore, when we replacing the PASM-194 with BPE-194
for aligning the first N1layers outputs, it obtains worse WERs
than setup 2, however, when we compare it with system 2 in
Table 1, we still see performance improvements. This tells us
that PASM is more suitable for low-level acoustic information
learning than BPE, and introducing small-to-large target unitsprogressive representation learning will be helpful. All these
observations lead us to propose the whole structure of paCTC
that shown in Fig.1(b).
4.3.2. Results on Accented ASR
In Table 2, the effectiveness of PMU with its different variants
are examined on the CommonV oice accented ASR task. Dif-
ferent from the Librispeech task, we find the best vocabulary
size of both PASM and BPE baselines is 205, larger BPE size
doesn’t result in better WERs under the ConformerT with single
type target units. And consistent with the findings in Table 1,
both the in-domain and out-of-domain performances are contin-
uously reduced by the proposed PMU methods, such as, system
4 performs better than 5 because PASM is applied on CTC and
while BPE is applied on transducer; paraCTC achieves better
results than basic PMU, and paCTC significantly outperforms
other two PMU variants, especially on the three in-domain test
sets. It’s worth noting that, in paCTC , both the intermediate
BPE-CTC andPASM-CTC are with the same 205 vocabulary
size. By comparing system 7 with 8, the results also prove
that introducing progressive learning with small to larger tar-
get units is useful. Finally, the paCTC with optional structure
achieves the best overall WERs, compared with the best Con-
formerT baseline system 2, system 10 produces relative 7.7%
overall WER reduction on this accented ASR testsets. Specif-
ically, relative 8.8%, 7.6% and 10.2% WER reductions are for
the in-domain England, Indian and US test sets, 8.2%, 8.6%
and 7.1% WER reductions are for the out-of-domain Australia,
Canada and Singapore test set, respectively.
5. Conclusion
In this work, we propose a phonetic-assisted multi-target
units (PMU) modeling approach, to effectively leverage both
the phonetic-induced PASM and conventional text-induced
BPE target units modeling for improving the state-of-the-art
Conformer-Transducer end-to-end ASR system. Three PMU
structures are proposed with different implementation of multi-
targets CTC/transducer modeling, including the basic PMU
with PASM and BPE applied to CTC and transducer separately,
the PMU with paraCTC where the PASM and BPE units are
also integrated in a parallel way as CTC’s target units, and
the PMU with paCTC that uses BPE units conditioned on the
PASM CTC in a progressive representation learning manner.
Results on both the LibriSpeech and accented English ASR taks
show that, the proposed PMU can significantly outperform the
conventional BPE-based Conformer-Transducer E2E ASR sys-
tem.

--- PAGE 5 ---
6. References
[1] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y . Wu et al. , “Conformer: Convolution-
augmented transformer for speech recognition,” in Proc. Inter-
speech , 2020, pp. 5036–5040.
[2] W. Huang, W. Hu, Y . T. Yeung, and X. Chen, “Conv-transformer
transducer: Low latency, low frame rate, streamable end-to-end
speech recognition,” in Proc. Interspeech , 2020, pp. 5001–5005.
[3] J. Li, Y . Wu, Y . Gaur, C. Wang, R. Zhao, and S. Liu, “On the
comparison of popular end-to-end models for large scale speech
recognition,” in Proc. Interspeech , 2020, pp. 1–5.
[4] F. Boyer, Y . Shinohara, T. Ishii, H. Inaguma, and S. Watanabe,
“A study of transducer based end-to-end ASR with ESPnet: Ar-
chitecture, auxiliary loss and decoding strategies,” in IEEE Auto-
matic Speech Recognition and Understanding Workshop (ASRU) ,
2021, pp. 16–23.
[5] W. Zou, D. Jiang, S. Zhao, G. Yang, and X. Li, “Comparable study
of modeling units for end-to-end mandarin speech recognition,” in
Proc. ISCSLP , 2018, pp. 369–373.
[6] S. Zhou, L. Dong, S. Xu, and B. Xu, “A comparison of modeling
units in sequence-to-sequence speech recognition with the trans-
former on mandarin chinese,” in Neural Information Processing ,
2018, pp. 210–220.
[7] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,
Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al. , “State-
of-the-art speech recognition with sequence-to-sequence models,”
inProc. ICASSP , 2018, pp. 4774–4778.
[8] J. Li, G. Ye, A. Das, R. Zhao, and Y . Gong, “Advancing acoustic-
to-word CTC model,” in Proc. ICASSP , 2018, pp. 5794–5798.
[9] A. Zeyer, K. Irie, R. Schl ¨uter, and H. Ney, “Improved training
of end-to-end attention models for speech recognition,” in Proc.
Interspeech , 2018, pp. 7–11.
[10] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures,
data and units for streaming end-to-end speech recognition with
RNN-transducer,” in IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU) , 2017, pp. 193–199.
[11] R. Sennrich, B. Haddow, and A. Birch, “Neural machine transla-
tion of rare words with subword units,” in Proc. ACL , 2016, pp.
1715–1725.
[12] M. Schuster and K. Nakajima, “Japanese and Korean voice
search,” in Proc. ICASSP , 2012, pp. 5149–5152.
[13] T. Kudo, “Subword regularization: Improving neural network
translation models with multiple subword candidates,” in Proc.
ACL, 2018, pp. 66–75.
[14] W. Wang, G. Wang, A. Bhatnagar, Y . Zhou, C. Xiong, and
R. Socher, “An investigation of phone-based subword units for
end-to-end speech recognition,” in Proc. Interspeech , 2020, pp.
1778–1782.
[15] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schl ¨uter, and H. Ney,
“A systematic comparison of grapheme-based vs. phoneme-based
label units for encoder-decoder-attention models,” arXiv preprint
arXiv:2005.09336 , 2020.
[16] S. Chen, X. Hu, S. Li, and X. Xu, “An investigation of using hy-
brid modeling units for improving end-to-end speech recognition
system,” in Proc. ICASSP , 2021, pp. 6743–6747.
[17] H. Xu, S. Ding, and S. Watanabe, “Improving end-to-end speech
recognition with pronunciation-assisted sub-word modeling,” in
Proc. ICASSP , 2019, pp. 7110–7114.
[18] Y . Higuchi, K. Karube, T. Ogawa, and T. Kobayashi, “Hierarchi-
cal conditional end-to-end ASR with CTC and multi-granular sub-
word units,” in Proc. ICASSP , 2022, pp. 7797–7801.
[19] A. Graves, “Sequence transduction with recurrent neural net-
works,” in Proc. ICML , 2012.
[20] J.-J. Jeon and E. Kim, “Multitask learning and joint optimization
for transformer-RNN-transducer speech recognition,” in Proc.
ICASSP , 2021, pp. 6793–6797.[21] A. Graves, S. Fern ´andez, F. Gomez, and J. Schmidhuber, “Con-
nectionist temporal classification: labelling unsegmented se-
quence data with recurrent neural networks,” in Proc. ICML ,
2006, pp. 369–376.
[22] R. Sanabria and F. Metze, “Hierarchical multitask learning with
CTC,” in Proc. SLT , 2018, pp. 485–490.
[23] J. Li, G. Ye, R. Zhao, J. Droppo, and Y . Gong, “Acoustic-to-word
model without OOV,” in IEEE Automatic Speech Recognition and
Understanding Workshop (ASRU) , 2017, pp. 111–117.
[24] J. Kremer, L. Borgholt, and L. Maaløe, “On the inductive bias of
word-character-level multi-task learning for speech recognition,”
arXiv preprint arXiv:1812.02308 , 2018.
[25] A. Heba, T. Pellegrini, J.-P. Lorr ´e, and R. Andre-Obrecht, “Char+
CV-CTC: combining graphemes and consonant/vowel units for
CTC-based ASR using multitask learning,” in Proc. Interspeech ,
2019, pp. 1611–1615.
[26] J. Nozaki and T. Komatsu, “Relaxing the conditional indepen-
dence assumption of CTC-based ASR by conditioning on inter-
mediate predictions,” in Proc. Interspeech , 2021, pp. 3735–3739.
[27] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-
riSpeech: an asr corpus based on public domain audio books,”
inProc. ICASSP , 2015, pp. 5206–5210.
[28] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,
J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber,
“Common V oice: A massively-multilingual speech corpus,” in
Proc. LREC , 2020, pp. 4218–4222.
[29] S. Watanabe, T. Hori, S. Karita, T. Hayashi et al. , “ESPnet: End-
to-end speech processing toolkit,” in Proc. Interspeech , 2018, pp.
2207–2211.
[30] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Re-
thinking the inception architecture for computer vision,” in Proc.
CVPR , 2016, pp. 2818–2826.
[31] T. Kudo and J. Richardson, “SentencePiece: A simple and lan-
guage independent subword tokenizer and detokenizer for neural
text processing,” in Proc. The Conference on Empirical Methods
in Natural Language Processing: System Demonstrations , 2018,
pp. 66–71.
[32] C. Dyer, V . Chahuneau, and N. A. Smith, “A simple, fast, and
effective reparameterization of IBM model 2,” in Proc. NAACL ,
2013, pp. 644–648.
