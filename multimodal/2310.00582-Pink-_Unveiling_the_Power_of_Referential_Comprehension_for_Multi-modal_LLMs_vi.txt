Pink: Tiết Lộ Sức Mạnh Của Khả Năng Hiểu Tham Chiếu Cho Các Mô Hình Ngôn Ngữ Lớn Đa Phương Thức

Shiyu Xuan1*, Qingpei Guo2, Ming Yang2, Shiliang Zhang1
1Phòng Thí Nghiệm Trọng Điểm Quốc Gia về Xử Lý Thông Tin Đa Phương Thức,
Trường Khoa Học Máy Tính, Đại Học Bắc Kinh, Bắc Kinh, Trung Quốc.
2Ant Group
shiyu xuan@stu.pku.edu.cn, {qingpei.gqp, m.yang}@antgroup.com, slzhang.jdl@pku.edu.cn

Tóm tắt

Các Mô Hình Ngôn Ngữ Lớn Đa Phương Thức (MLLM) đã thể hiện khả năng đáng chú ý trong nhiều tác vụ đa phương thức khác nhau. Tuy nhiên, hiệu suất của chúng trong các tác vụ hiểu hình ảnh chi tiết vẫn còn hạn chế. Để giải quyết vấn đề này, bài báo này đề xuất một khung công tác mới để nâng cao khả năng hiểu hình ảnh chi tiết của MLLM. Cụ thể, chúng tôi trình bày một phương pháp mới để xây dựng tập dữ liệu tinh chỉnh hướng dẫn với chi phí thấp bằng cách tận dụng các chú thích trong các tập dữ liệu hiện có. Một phương pháp khởi tạo tự nhất quán cũng được giới thiệu để mở rộng các chú thích đối tượng dày đặc hiện có thành các cặp biểu thức tham chiếu-hộp giới hạn chất lượng cao. Các phương pháp này cho phép tạo ra dữ liệu hướng dẫn chất lượng cao bao gồm một loạt các khả năng cơ bản thiết yếu cho nhận thức hình ảnh chi tiết. Hơn nữa, chúng tôi cho rằng bộ mã hóa thị giác nên được tinh chỉnh trong quá trình tinh chỉnh hướng dẫn để giảm thiểu khoảng cách giữa nhận thức hình ảnh đầy đủ và nhận thức hình ảnh chi tiết. Kết quả thực nghiệm chứng minh hiệu suất vượt trội của phương pháp chúng tôi. Ví dụ, mô hình của chúng tôi cho thấy cải thiện độ chính xác 5,2% so với Qwen-VL trên GQA và vượt qua độ chính xác của Kosmos-2 24,7% trên RefCOCO val. Chúng tôi cũng đã đạt được vị trí hàng đầu trên bảng xếp hạng MMBench. Hiệu suất đầy hứa hẹn này được đạt được bằng cách huấn luyện chỉ trên dữ liệu có sẵn công khai, làm cho nó dễ dàng tái tạo. Các mô hình, tập dữ liệu và mã nguồn được công bố công khai tại https://github.com/SY-Xuan/Pink.

1. Giới thiệu

Các Mô Hình Ngôn Ngữ Lớn (LLM) [2,30,31,35] cho thấy khả năng ấn tượng trong một loạt các tác vụ ngôn ngữ tự nhiên. Những kết quả truyền cảm hứng này đã thúc đẩy các nhà nghiên cứu mở rộng LLM thành Các Mô Hình Ngôn Ngữ Lớn Đa Phương Thức

*Công việc này được thực hiện trong thời gian thực tập của tác giả thứ nhất tại Ant Group.

[Biểu đồ hiệu suất so sánh các mô hình trên các tác vụ khác nhau]

Hình 1. Với ít tham số có thể huấn luyện hơn và ít dữ liệu huấn luyện hơn, Pink đạt được hiệu suất tốt nhất trên cả các tác vụ đa phương thức thông thường và tác vụ RC. "#Trainable Param.", "#PT Data", và "#IT Data" lần lượt chỉ số lượng tham số có thể huấn luyện, số lượng mẫu trong giai đoạn tiền huấn luyện và tinh chỉnh hướng dẫn.

(MLLM) bằng cách tích hợp các phương thức bổ sung, ví dụ: hình ảnh, âm thanh, hoặc đám mây điểm. Tinh chỉnh hướng dẫn thị giác [6,20,45], sử dụng dữ liệu tinh chỉnh hướng dẫn hình ảnh-văn bản chất lượng cao, cho phép việc tích hợp khả năng hiểu thị giác vào LLM bằng cách chiếu các đặc trưng thị giác vào không gian ngôn ngữ tự nhiên của LLM [46]. Được hỗ trợ bởi những phương pháp đó, các MLLM hiện có có khả năng hiểu cơ bản ở mức độ hình ảnh.

Tuy nhiên, chúng vẫn đối mặt với việc hiểu hình ảnh chi tiết [13,38,39]. Khả năng hiểu hình ảnh chi tiết hạn chế cản trở hiệu suất của MLLM trong các tác vụ đa phương thức và hạn chế các ứng dụng tiềm năng của chúng như được báo cáo trong báo cáo thử nghiệm GPT-4V(ision) [44].

Để giải quyết vấn đề này, một số phương pháp [4,27] tích hợp một số tập dữ liệu liên quan đến Hiểu Tham Chiếu (RC) như RefCOCO [13], và PointQA [24] để nâng cao khả năng nhận thức hình ảnh chi tiết của MLLM. Tuy nhiên, những tập dữ liệu này không đủ để bao phủ một loạt các khả năng mà MLLM mong muốn có cho nhận thức hình ảnh chi tiết. Các tác vụ RC hạn chế cũng khiến mô hình khó tổng quát hóa trên các tác vụ RC khác nhau thông qua tinh chỉnh hướng dẫn. Ví dụ, như được thể hiện trong Hình 4, Shikra [4] và Qwen-VL [1] cho thấy khả năng tuân theo hướng dẫn hạn chế trên các tác vụ RC ngoài việc tinh chỉnh hướng dẫn của nó, không thể cung cấp phản hồi có liên quan đến các câu hỏi.

Ngoài tinh chỉnh hướng dẫn, khả năng của bộ mã hóa thị giác cũng quan trọng đối với hiệu suất của MLLM. MLLM thường sử dụng bộ mã hóa thị giác được huấn luyện thông qua tiền huấn luyện ngôn ngữ-hình ảnh tương phản như trong CLIP [29]. Việc chỉ thực hiện căn chỉnh toàn cục là không hiệu quả trong việc khám phá các mối quan hệ chi tiết giữa các vùng hình ảnh và các từ văn bản [42,49]. Bộ mã hóa thị giác đóng vai trò như một nút thắt cổ chai để đạt được nhận thức hình ảnh chi tiết trong MLLM.

Bài báo này đề xuất một khung công tác mới để nâng cao khả năng nhận thức hình ảnh chi tiết của MLLM thông qua các tác vụ RC. Chúng tôi gọi mô hình đã huấn luyện là Pink¹. Hiểu hình ảnh chi tiết có liên quan chặt chẽ với một số khả năng cơ bản như nhận dạng thực thể và nhận biết vị trí tương đối giữa các thực thể khác nhau. Việc tích hợp các tác vụ đòi hỏi những khả năng cơ bản này trong quá trình tinh chỉnh hướng dẫn là rất quan trọng để nâng cao khả năng nhận thức hình ảnh chi tiết của mô hình. Để làm điều này, chúng tôi đề xuất một quy trình xây dựng tập dữ liệu mới mở rộng các chú thích của tập dữ liệu hiện có thành các tác vụ RC khác nhau về những khả năng cơ bản này. Cụ thể, chúng tôi thiết kế một số tác vụ RC, như lý luận quan hệ thị giác và lý luận không gian thị giác, dựa trên các chú thích từ Visual Genome [14]. Để tích hợp thêm dữ liệu huấn luyện cho những tác vụ RC này, chúng tôi giới thiệu một phương pháp khởi tạo tự nhất quán mới để mở rộng các chú thích đối tượng dày đặc thành các cặp biểu thức tham chiếu-hộp giới hạn. So với quá trình tạo dữ liệu đắt đỏ và không thể kiểm soát bằng API GPT4 [4,48], phương pháp của chúng tôi tận dụng các chú thích hiện có từ tập dữ liệu. Cách tiếp cận này dẫn đến dữ liệu chất lượng cao và nâng cao chính xác các khả năng cần thiết mà mô hình yêu cầu. Như được thể hiện trong Hình 1, dữ liệu tinh chỉnh hướng dẫn chất lượng cao được tạo ra bởi phương pháp của chúng tôi cho phép mô hình đạt được hiệu suất đầy hứa hẹn với số lượng mẫu huấn luyện giảm.

Cải thiện khả năng hiểu hình ảnh chi tiết của bộ mã hóa thị giác không phải là một nhiệm vụ tầm thường. Hầu hết các MLLM hiện có [4,6,20,45,50] đóng băng bộ mã hóa thị giác trong quá trình tinh chỉnh hướng dẫn. Bởi vì việc tinh chỉnh trực tiếp bộ mã hóa thị giác có thể dẫn đến mất ngữ nghĩa do quy mô hạn chế của tập dữ liệu hướng dẫn thị giác [40]. Để giải quyết vấn đề này, chúng tôi tinh chỉnh bộ mã hóa thị giác bằng cách giới thiệu một số thành phần có thể tinh chỉnh như Adapter [10] và LoRA [11]. Việc đóng băng các tham số chính của mô hình tránh quên kiến thức đã học. Các thành phần có thể tinh chỉnh được giới thiệu được huấn luyện để thích ứng với bộ mã hóa thị giác.

Chúng tôi đã tiến hành các thí nghiệm mở rộng để kiểm tra hiệu suất của mô hình. Được hưởng lợi từ các tác vụ được thiết kế trong tinh chỉnh hướng dẫn, khung công tác của chúng tôi nâng cao hiệu suất của MLLM trong cả các tác vụ ngôn ngữ-thị giác thông thường và tác vụ RC. Ví dụ, chỉ với 6,7M tham số có thể tinh chỉnh, chúng tôi đạt được cải thiện độ chính xác lên đến 6,0% trên OK-VQA [26] so với Shikra [4]. Chúng tôi cũng đạt được vị trí hàng đầu trên bảng xếp hạng MMBench [21]. Cần lưu ý rằng, phương pháp của chúng tôi cũng vượt qua các phương pháp dựa trên nhiều dữ liệu huấn luyện hơn, ví dụ: Qwen-VL [1].

Công việc này là một nỗ lực ban đầu để nâng cao khả năng nhận thức hình ảnh chi tiết của MLLM bằng cách giải quyết hai nút thắt cổ chai chính: các tác vụ tinh chỉnh hướng dẫn hạn chế và thiếu khả năng của bộ mã hóa thị giác. Bằng cách thiết kế các tác vụ liên quan đến khả năng cơ bản, mọi tập dữ liệu có chú thích tương ứng đều có thể được chuyển đổi thành tập dữ liệu tinh chỉnh hướng dẫn. Việc khởi tạo tự nhất quán tiếp tục tăng số lượng dữ liệu huấn luyện. Quy trình xây dựng tập dữ liệu này giảm đáng kể chi phí thu thập dữ liệu chất lượng cao với các tác vụ đa dạng và loại bỏ sự phụ thuộc vào API GPT4. Toàn bộ quy trình huấn luyện có thể tái tạo trong môi trường học thuật vì nó chỉ dựa trên dữ liệu có sẵn công khai và có thể được huấn luyện trên GPU tiêu dùng với bộ nhớ 24GB. Chúng tôi sẽ phát hành mã nguồn và tập dữ liệu để hỗ trợ nghiên cứu và đánh giá tiếp theo.

2. Các Công Trình Liên Quan

Mô hình ngôn ngữ lớn đa phương thức. Một số cách tiếp cận đã được đề xuất để điều kiện LLM với các phương thức bổ sung. Thông thường, những phương pháp này sử dụng huấn luyện hai giai đoạn. Giai đoạn tiền huấn luyện được thực hiện để căn chỉnh hai phương thức với các cặp hình ảnh-văn bản. Giai đoạn tiếp theo được áp dụng để cải thiện khả năng của MLLM trong việc tuân theo hướng dẫn với tập dữ liệu tinh chỉnh hướng dẫn chất lượng cao [18,20,43,50]. Nhiều phương pháp đóng băng bộ mã hóa thị giác trong giai đoạn tiền huấn luyện để giảm yêu cầu về các cặp hình ảnh-văn bản quy mô lớn. Ví dụ, Mini-GPT4 [50] và LLaVA [20] chỉ tinh chỉnh một lớp kết nối đầy đủ duy nhất để căn chỉnh các phương thức thị giác và ngôn ngữ. Các phương pháp khác tận dụng hàng triệu cặp hình ảnh-văn bản để đạt được sự căn chỉnh tốt hơn giữa hai phương thức. Instruct-BLIP [6] giới thiệu một phương pháp trích xuất đặc trưng thị giác nhận thức hướng dẫn và tinh chỉnh toàn bộ Q-Former, cho thấy khả năng tổng quát hóa không cần mẫu đầy hứa hẹn trên các tác vụ đa phương thức khác nhau. mPlug-Owl [45] tích hợp một mô-đun tóm tắt thị giác để căn chỉnh hai phương thức. Cả bộ mã hóa thị giác và bộ tóm tắt thị giác đều được cập nhật trong giai đoạn tiền huấn luyện. Tất cả các phương pháp trên đều đóng băng bộ mã hóa thị giác trong giai đoạn tinh chỉnh hướng dẫn đa phương thức để ngăn chặn sự mất ngữ nghĩa tiềm ẩn do tập dữ liệu tinh chỉnh hướng dẫn quy mô nhỏ gây ra. Tuy nhiên, chiến lược này khiến bộ mã hóa thị giác không thể hưởng lợi từ việc tinh chỉnh hướng dẫn đa phương thức.

Hiểu Tham Chiếu của MLLM. Hiểu tham chiếu quan trọng đối với nhận thức hình ảnh chi tiết của MLLM. Do đó, việc nâng cao MLLM với khả năng RC có giá trị cao. Lấy cảm hứng từ Pix2Seq [5], nhiều công trình sử dụng các token tọa độ rời rạc để mã hóa thông tin không gian và thống nhất các tác vụ RC như các tác vụ tạo chuỗi, ví dụ: OFA [41], Unified-io [22], và Kosmos-2 [27]. Một hướng nghiên cứu khác, như được thấy trong PVIT [3] và GPT4RoI [47], tận dụng thao tác ROI [9] để trích xuất đặc trưng của các đối tượng tham chiếu. Những công trình này yêu cầu các mô-đun bổ sung và có thể mất thông tin ngữ cảnh. Một hạn chế khác của những công trình này là chúng không thể tham chiếu đối tượng trong phản hồi của mình, hạn chế các ứng dụng của chúng, ví dụ: trong định vị thị giác.

Ngoài thiết kế mô hình, việc xây dựng dữ liệu tinh chỉnh hướng dẫn RC cũng đóng vai trò quan trọng. Shikra [4] chuyển đổi các tập dữ liệu hiện có của các tác vụ RC bao gồm RefCOCO [13] và PointQA [24] thành định dạng tuân theo hướng dẫn. Kosmos-2 sử dụng mô hình định vị GLIP [16] để trích xuất tọa độ của các đoạn danh từ trong chú thích hình ảnh và xây dựng một tập dữ liệu quy mô lớn. Các tập dữ liệu được xây dựng bởi các phương pháp trên chỉ bao gồm các tác vụ RC, như định vị thị giác, chú thích định vị và pointQA, vẫn chưa đủ đa dạng để bao phủ các tác vụ RC khác nhau. Do đó, các mô hình được huấn luyện thể hiện khả năng tổng quát hóa kém đối với các tác vụ RC mới ngoài việc tinh chỉnh hướng dẫn. ChatSpot [48], PVIT [3], và Shikra [4] đều nhắc GPT4 tạo dữ liệu tinh chỉnh hướng dẫn cho RC, điều này đắt đỏ và không thể kiểm soát.

Khác biệt với các công trình trước đây. Các phương pháp hiện có để nâng cao MLLM thông qua các tác vụ RC xây dựng tập dữ liệu tinh chỉnh hướng dẫn bằng cách tích hợp các tập dữ liệu RC hiện có hoặc dựa vào API GPT4. Tuy nhiên, những phương pháp này thể hiện một số nhược điểm chính: 1) tính đa dạng của các tác vụ RC không thể bao phủ một loạt các khả năng cơ bản, và 2) việc tạo dữ liệu thông qua API GPT4 đắt đỏ, không thể kiểm soát và dễ có nhiễu. Ngược lại, công trình của chúng tôi tận dụng hiệu quả các tập dữ liệu hiện có để bao phủ nhiều tác vụ RC khác nhau. Phương pháp khởi tạo tự nhất quán được đề xuất mở rộng các chú thích đối tượng dày đặc thành các cặp biểu thức tham chiếu-hộp giới hạn. Quy trình này giảm đáng kể chi phí tạo ra các tập dữ liệu chất lượng cao. Chất lượng cao của dữ liệu cho phép mô hình của chúng tôi được huấn luyện với ít tham số hơn trên ít dữ liệu huấn luyện hơn, điều này thân thiện với việc tái tạo trong môi trường học thuật, so với các MLLM thương mại lớn.

3. Phương Pháp Luận

3.1. Kiến Trúc Mô Hình và Quy Trình Huấn Luyện

Kiến trúc mô hình. Như được thể hiện trong Hình 2, Pink tuân theo kiến trúc tương tự như LLaVA [20], bao gồm bộ mã hóa thị giác ΦV, lớp chiếu ΦP, và LLM chỉ giải mã ΦL. Cho một hình ảnh I và một chuỗi embedding từ QT đại diện cho một câu hướng dẫn, bộ mã hóa thị giác được sử dụng để nhúng hình ảnh như một chuỗi token thị giác ZV=ΦV(I). Một lớp tuyến tính được sử dụng như ΦP để chuyển đổi ZV vào không gian đầu vào của LLM ZT=ΦP(ZV). ZT và QT được nối lại và đưa vào ΦL để tạo từ tiếp theo.

Để cho phép LLM nhận tọa độ làm đầu vào và đầu ra, tương tự như Shikra [4], các tọa độ được chuyển đổi thành văn bản theo định dạng cụ thể. Cụ thể, đối với một hộp giới hạn được biểu diễn bởi tọa độ của góc trên bên trái và dưới bên phải [xmin,ymin,xmax,ymax], các tọa độ được chuẩn hóa về khoảng [0,1] theo kích thước hình ảnh và giữ lại 3 chữ số thập phân cho mỗi số, ví dụ: [0.222,0.333,0.444,0.555]. Thiết kế này cho phép các tọa độ được xử lý như văn bản thông thường và có thể xuất hiện trong cả đầu vào và đầu ra của mô hình.

Bộ mã hóa thị giác được tiền huấn luyện bởi hàm mất tương phản [29] thiếu khả năng hiểu hình ảnh ở mức vùng. Việc tinh chỉnh trực tiếp toàn bộ bộ mã hóa thị giác trong quá trình tinh chỉnh hướng dẫn có thể dẫn đến mất ngữ nghĩa do dữ liệu tinh chỉnh hướng dẫn hạn chế [40]. Để tích hợp khả năng nhận thức hình ảnh chi tiết của bộ mã hóa thị giác thông qua tinh chỉnh hướng dẫn đa phương thức với các tác vụ RC, chúng tôi đóng băng bộ mã hóa thị giác, đồng thời giới thiệu các mô-đun có thể tinh chỉnh vào đó. Cách tiếp cận này ngăn chặn bộ mã hóa thị giác khỏi bị mất ngữ nghĩa và cung cấp một cách hiệu quả để thích ứng mô hình. Cụ thể, chúng tôi sử dụng Adapter [10] ở cả bộ mã hóa thị giác và LLM. Cho một đặc trưng token đầu vào Z∈Rd, kiến trúc của một Adapter được định nghĩa như sau:

Ẑ=σ(ZWd)Wu+Z, (1)

trong đó Wd∈Rd×ds và Wu∈Rds×d biểu thị các ma trận trọng số, ds là chiều ẩn nhỏ hơn nhiều so với d, và σ biểu thị hàm kích hoạt phi tuyến. Wu được khởi tạo về không để đảm bảo rằng ở đầu quá trình huấn luyện, Adapter không thay đổi đầu ra ban đầu.

Quy trình huấn luyện. Cả hình ảnh và tọa độ đều được ánh xạ vào không gian đầu vào của LLM. Do đó, mô hình có thể được huấn luyện từ đầu đến cuối bằng cách sử dụng tác vụ mô hình hóa ngôn ngữ, dự đoán token từ tiếp theo dựa trên ngữ cảnh trước đó.

Mô hình được huấn luyện trong hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi chỉ tinh chỉnh lớp chiếu với một tập nhỏ các cặp hình ảnh-văn bản. Trong giai đoạn thứ hai, chúng tôi đóng băng cả bộ mã hóa thị giác và LLM và tinh chỉnh các Adapter mới được thêm vào và lớp chiếu với tập dữ liệu tinh chỉnh hướng dẫn. Do đó, cả hai phương thức thị giác và ngôn ngữ đều có thể hưởng lợi từ việc tinh chỉnh hướng dẫn đa phương thức.

3.2. Xây Dựng Tập Dữ Liệu Tinh Chỉnh Hướng Dẫn

Để tạo tập dữ liệu tinh chỉnh hướng dẫn, chúng tôi thống nhất tất cả các tác vụ đa phương thức thành định dạng đối thoại ngôn ngữ-thị giác:
Hình ảnh: {Token hình ảnh}
Người dùng: {Mẫu hướng dẫn}
Trợ lý: {Phản hồi}
trong đó các placeholder {Token hình ảnh}, {Mẫu hướng dẫn}, và {Phản hồi} sẽ được thay thế bằng các token hình ảnh được trích xuất bởi ΦV, mẫu hướng dẫn tác vụ, và phản hồi, tương ứng.

Việc giới thiệu các tác vụ RC đa dạng cho việc tinh chỉnh hướng dẫn để bao phủ một loạt các khả năng cơ bản cho nhận thức hình ảnh chi tiết của MLLM là quan trọng. Các tập dữ liệu hiện có chỉ cung cấp các tác vụ RC hạn chế, ví dụ: định vị thị giác, chú thích định vị [13], và pointQA [24]. Bên cạnh các tác vụ RC được đề cập ở trên, chúng tôi thiết kế nhiều tác vụ RC đa dạng hơn bằng cách tích hợp các chú thích từ Visual Genome [14], chứa thông tin về mô tả vùng, đối tượng, và quan hệ giữa các đối tượng khác nhau. Các phần tiếp theo sẽ giới thiệu những tác vụ đó.

Lý luận quan hệ thị giác. Visual Genome đã chú thích hàng triệu bộ ba quan hệ (chủ thể-vị ngữ-đối tượng), ví dụ: người đàn ông-mặc-mũ. Chúng tôi thiết kế hai loại tác vụ lý luận quan hệ thị giác bằng cách tận dụng những chú thích này để giúp mô hình hiểu các mối quan hệ thị giác giữa các đối tượng khác nhau: (1) Chúng tôi chọn ngẫu nhiên một bộ ba quan hệ. Cho tọa độ của chủ thể và đối tượng, mô hình được yêu cầu dự đoán quan hệ của chúng. (2) Chúng tôi chọn ngẫu nhiên một chủ thể và một quan hệ từ các chú thích. Mô hình được yêu cầu phát hiện tất cả các đối tượng có quan hệ đã chọn với chủ thể và đưa ra tọa độ và tên lớp của chúng.

Lý luận không gian thị giác thô. Chúng tôi giới thiệu một tác vụ lý luận không gian thị giác thô bằng cách sử dụng các chú thích đối tượng từ Visual Genome. Tác vụ này nâng cao MLLM để xác định quan hệ không gian tương đối giữa các thực thể khác nhau. Chúng tôi định nghĩa bốn vị trí không gian thô là trên-trái, trên-phải, dưới-trái, và dưới-phải. Cho một đối tượng được chọn ngẫu nhiên và một vị trí không gian thô, mô hình được yêu cầu xác định tất cả các đối tượng nằm ở vị trí này tương đối với đối tượng đã chọn và dự đoán tọa độ và tên lớp của chúng.

Đếm đối tượng. Để trang bị cho mô hình khái niệm về các thực thể khác nhau và khả năng nhận dạng đối tượng chi tiết, chúng tôi thiết kế một tác vụ đếm đối tượng. Tác vụ này yêu cầu mô hình đếm các đối tượng trong hình ảnh thuộc cùng một danh mục với đối tượng hoặc tên lớp đã cho.

Phát hiện đối tượng. Phát hiện đối tượng có thể trao quyền cho mô hình để xác định vị trí và ranh giới của các đối tượng. Tác vụ này cũng quan trọng để mô hình xác định sự tồn tại hoặc danh mục của một đối tượng nhất định. Cho một tên lớp hoặc một đối tượng được chọn, mô hình được yêu cầu xác định tất cả các đối tượng thuộc cùng danh mục với đối tượng hoặc tên lớp đã cho, và cung cấp tọa độ của chúng.

Bằng cách tích hợp những tác vụ RC này vào việc tinh chỉnh hướng dẫn, mô hình có thể học được nhiều khả năng RC khác nhau. Để làm rõ các tác vụ được thiết kế, chúng tôi liệt kê một số mẫu hướng dẫn như sau:

Lý luận Quan hệ Thị giác:
Người dùng: Hỗ trợ tôi tìm quan hệ giữa <chủ thể> và <đối tượng> trong ảnh.
Trợ lý: <quan hệ>.
Người dùng: Vui lòng xác định vị trí và phân loại tất cả các đối tượng có quan hệ <quan hệ> với <chủ thể>.
Trợ lý: <đối tượng> <danh mục> <đối tượng> <danh mục>.

Lý luận Không gian Thị giác Thô:
Người dùng: Xác định các đối tượng nằm ở <vị trí> của <đối tượng>.
Trợ lý: <đối tượng> <danh mục> <đối tượng> <danh mục>.

Đếm Đối tượng:
Người dùng: Có bao nhiêu đối tượng trong hình ảnh cùng danh mục với <đối tượng>.
Trợ lý: <số>.

Phát hiện Đối tượng:
Người dùng: Xác định tất cả các đối tượng phù hợp với cùng danh mục với <đối tượng> và hiển thị tọa độ của chúng.
Trợ lý: <đối tượng> <đối tượng>.

trong đó các placeholder <đối tượng> và <danh mục> sẽ được thay thế bằng tọa độ hộp giới hạn và tên lớp của đối tượng tham chiếu, tương ứng. <chủ thể> sẽ được thay thế bằng tọa độ hộp giới hạn của chủ thể được chọn. <quan hệ>, <vị trí>, và <số> sẽ được thay thế bằng quan hệ giữa các đối tượng khác nhau, vị trí không gian tương đối được chọn, và số lượng đối tượng, tương ứng. Tất cả các mẫu hướng dẫn có thể được tìm thấy trong Tài liệu Bổ sung.

3.3. Phương Pháp Khởi Tạo Tự Nhất Quán

Các tập dữ liệu tuân theo hướng dẫn được xây dựng được áp dụng để tăng cường khả năng nhận thức hình ảnh chi tiết của MLLM. Chúng tôi tiếp tục thu thập thêm dữ liệu chất lượng cao. Các tập dữ liệu hiện có cho phát hiện đối tượng cung cấp các chú thích hộp giới hạn có giá trị cho các đối tượng xuất hiện trong hình ảnh, làm cho chúng trở thành nguồn tài nguyên đầy hứa hẹn cho việc tinh chỉnh hướng dẫn. Chúng tôi đề xuất một phương pháp khởi tạo tự nhất quán bằng cách tận dụng những tập dữ liệu đó. Phương pháp này mở rộng các chú thích hộp giới hạn thành các cặp biểu thức tham chiếu-hộp giới hạn. Nó bao gồm hai giai đoạn chính: khởi tạo mô tả hộp giới hạn và lọc tự nhất quán như được thể hiện trong Hình 3.

Tại giai đoạn khởi tạo mô tả hộp giới hạn, cho một hộp giới hạn B của một đối tượng, chúng tôi nhắc mô hình tạo ra một mô tả DB cho đối tượng đó bằng cách tận dụng khả năng chú thích định vị của nó. Do sự phức tạp của cảnh hoặc sự hiện diện của các đối tượng trùng lặp, mô tả được tạo ra có thể có nhiễu hoặc không thể mô tả duy nhất đối tượng tương ứng. Sau đó, giai đoạn lọc tự nhất quán được áp dụng để lọc những mô tả chất lượng thấp. Cụ thể, với mô tả được tạo ra DB, chúng tôi xác định vị trí mô tả này trong hình ảnh và dự đoán hộp giới hạn B̂ bằng cách tận dụng khả năng định vị thị giác của mô hình. Mô tả được tạo ra sẽ bị loại bỏ nếu Giao trên Hợp (IOU) giữa B và B̂ dưới ngưỡng được định nghĩa trước λ. Giai đoạn này đảm bảo rằng chỉ những mô tả chất lượng cao được giữ lại.

Hai giai đoạn này được thực hiện để mở rộng mỗi đối tượng được chú thích trong tập dữ liệu với mô tả văn bản. Tập dữ liệu mở rộng này sau đó rất phù hợp cho một loạt các tác vụ RC, ví dụ: lý luận không gian thị giác thô, phát hiện đối tượng, đếm đối tượng, định vị thị giác và chú thích định vị. Minh họa về dữ liệu được tạo ra có trong Tài liệu Bổ sung. Phương pháp khởi tạo tự nhất quán này phục vụ như một công cụ mạnh mẽ để khai thác tiềm năng của các tập dữ liệu phát hiện đối tượng để nâng cao khả năng RC.

3.4. Kết Quả Định Tính về Lý Luận RC

Trong Hình 4, chúng tôi so sánh một số kết quả định tính về lý luận RC với Shikra [4] và Qwen-VL [1]. Pink thể hiện khả năng tốt hơn đáng kể trong những tác vụ RC này.

Để trả lời câu hỏi như được thể hiện trong Hình 4 (a), mô hình cần trước tiên xác định Du Feng, một cầu thủ bóng rổ nổi tiếng của Trung Quốc, sau đó hiểu hành động cầm nắm. Pink thành công cung cấp câu trả lời đúng. Pink cũng thể hiện khả năng lý luận tốt hơn như được thể hiện trong Hình 4 (b). Nó thành công xác định vùng được tham chiếu là laptop, và suy luận rằng chuột và bàn phím trong hình ảnh có thể điều khiển laptop. Shikra không thể tuân theo hướng dẫn và cung cấp câu trả lời không liên quan. Tương tự, Qwen-VL chỉ đưa ra danh mục của đối tượng được tham chiếu.

Như được thể hiện trong Hình 4 (c), được huấn luyện với hàng triệu dữ liệu dựa trên OCR trong việc tinh chỉnh hướng dẫn, Qwen-VL có thể đưa ra phản hồi chính xác cho hướng dẫn OCR. Thú vị khi quan sát rằng, không có dữ liệu dựa trên OCR nào, mô hình của chúng tôi cũng nhận dạng chính xác các ký tự nằm ở vùng được tham chiếu. Điều này chỉ ra rằng Pink thể hiện khả năng tổng quát hóa đầy hứa hẹn đối với các tác vụ RC khác nhau. Thêm kết quả định tính có thể được tìm thấy trong Tài liệu Bổ sung. Các thí nghiệm mở rộng được tiến hành trong phần tiếp theo.

4. Thí Nghiệm

4.1. Cài Đặt Thí Nghiệm

Kiến trúc mô hình. Chúng tôi sử dụng ViT-L/14 [7] làm bộ mã hóa thị giác, được tiền huấn luyện với CLIP [29]. Chúng tôi chọn mô hình được tinh chỉnh hướng dẫn Vicuna-7B [37] dựa trên LLaMA-1 [35] làm LLM. Lớp chiếu là một lớp kết nối đầy đủ duy nhất. Các Adapter được chèn trước mỗi lớp tự chú ý của cả bộ mã hóa thị giác và LLM, với chiều ẩn ds=8. Số lượng tham số có thể tinh chỉnh của Adapter trong bộ mã hóa thị giác và LLM lần lượt là 393,216 và 2,097,152. Số lượng tham số trong lớp chiếu là 4,194,304. Do đó, tổng số tham số có thể tinh chỉnh là khoảng 6,7M.

Dữ liệu huấn luyện. Giai đoạn đầu tiên sử dụng 595K cặp hình ảnh-văn bản từ CC3M [34], giống như LLaVA [20]. Giai đoạn thứ hai áp dụng VQAv2 [8], LLaVA-150K [20], A-OKVQA [32], Flickr30K [28], Visual Genome [14] và Object365 [33] với các cặp biểu thức tham chiếu-hộp giới hạn được tạo ra bởi phương pháp khởi tạo tự nhất quán của chúng tôi. Tại mỗi lần lặp huấn luyện, khi sử dụng một hình ảnh trong Visual Genome hoặc Object365, một tác vụ RC được thiết kế sẽ được chọn ngẫu nhiên. Mô hình được sử dụng để tạo ra các cặp biểu thức tham chiếu-hộp giới hạn trong Object365 được huấn luyện với các tập dữ liệu đã nêu trên, ngoại trừ chính Object365. Lưu ý rằng chúng tôi giảm xác suất lấy mẫu Object365 trong việc xây dựng batch để tránh một số lượng lớn mẫu huấn luyện trong Object365 chiếm ưu thế trong việc huấn luyện.

Chi tiết huấn luyện. AdamW được áp dụng làm bộ tối ưu hóa. Trong giai đoạn đầu tiên, mô hình được huấn luyện trong 1 epoch với kích thước batch 128 và weight decay 0.0. Sau giai đoạn khởi động 200 bước, tốc độ học bắt đầu ở 0.03 và giảm về 0 với lịch trình cosine. Trong giai đoạn thứ hai, mô hình được huấn luyện trong 6 epoch với kích thước batch 32 và weight decay 0.05. Giai đoạn khởi động bao gồm 10k bước và tốc độ học bắt đầu ở 5e-4. Hình ảnh đầu vào được thay đổi kích thước thành 224×224 không có tăng cường dữ liệu bổ sung. Chúng tôi đặt λ là 0.5 để lọc ra những mô tả chất lượng thấp. Mô hình được huấn luyện bằng 8 GPU NVIDIA A100. Mất khoảng 1 và 30 giờ cho giai đoạn thứ nhất và thứ hai, tương ứng.

Cài đặt đánh giá. Chúng tôi đánh giá mô hình trên các tập dữ liệu khác nhau dưới cài đặt zero-shot và fine-tuning để xác thực khả năng tuân theo hướng dẫn của mô hình được huấn luyện. Những tập dữ liệu này bao gồm các tác vụ lý luận đa phương thức thông thường, bao gồm VQA thông thường (VQAv2 [8], MMBench [21]), hiểu sơ đồ trừu tượng (IconQA [23]), lý luận không gian thị giác (VSR [17]), VQA chuyên sâu kiến thức (OK-VQA [26]), hiểu cảnh (GQA [12]), và các tác vụ RC như RefCOCO/+ [13], RefCOCOg [25], Visual-7W [51], PointQA-Local/LookTwice [24].

4.2. Nghiên Cứu Loại Bỏ

Xây dựng tập dữ liệu tinh chỉnh hướng dẫn. Để điều tra tác động của việc xây dựng tập dữ liệu tinh chỉnh hướng dẫn, chúng tôi tiến hành các nghiên cứu loại bỏ bằng cách loại trừ Visual Genome, các tác vụ RC được thiết kế, và Object365 với các cặp biểu thức tham chiếu-hộp giới hạn. Kết quả được tóm tắt trong Bảng 1.

Như được thể hiện trong Bảng 1, việc nâng cao MLLM với tác vụ RC có thể có lợi cho các tác vụ lý luận đa phương thức thông thường, ví dụ: việc loại bỏ các tác vụ định vị thị giác và chú thích định vị dẫn đến sự suy giảm hiệu suất 2,8% trên VSR. Khi mô hình chỉ được huấn luyện với định vị thị giác và chú thích định vị, nó không thể cung cấp phản hồi chính xác cho các câu hỏi trong PointQA-Local/LookTwice, cho thấy khả năng tuân theo hướng dẫn hạn chế cho các tác vụ RC. Khi nhiều tác vụ RC được bao gồm hơn, mô hình bắt đầu thể hiện khả năng tuân theo hướng dẫn tốt hơn cho những tác vụ này. Hiệu suất trên PointQA-Local/LookTwice tăng từ 0,0%/0,2% lên 54,6%/63,1%. Sự kết hợp của tất cả các tác vụ RC được thiết kế cho hiệu suất tốt nhất trên cả các tác vụ lý luận đa phương thức thông thường và tác vụ RC, do đó xác thực hiệu quả của phương pháp xây dựng tập dữ liệu tinh chỉnh hướng dẫn. Chúng tôi cũng quan sát thấy rằng việc tích hợp Object365 tiếp tục nâng cao hiệu suất của phương pháp chúng tôi trên các tác vụ RC. Ví dụ, trên RefCOCO val, độ chính xác zero-shot tăng từ 54,1% lên 77,0%. Đặc biệt, việc loại trừ phương pháp tự nhất quán dẫn đến sự suy giảm hiệu suất từ 77,0% xuống 73,8% trên RefCOCO val do các cặp biểu thức tham chiếu-hộp giới hạn chất lượng thấp được tạo ra bởi mô hình. Những kết quả này chứng minh tầm quan trọng của dữ liệu được tạo ra và nhấn mạnh tầm quan trọng của phương pháp tự nhất quán được đề xuất.

Cài đặt huấn luyện của bộ mã hóa thị giác. Chúng tôi tiếp tục đánh giá hiệu quả của các cài đặt khác nhau để huấn luyện bộ mã hóa thị giác, và tóm tắt kết quả trong Bảng 1.

Việc tinh chỉnh đầy đủ bộ mã hóa thị giác dẫn đến sự suy giảm hiệu suất đáng kể. Hiệu suất trên RefCOCO val giảm từ 54,1% xuống 0,05%. Kết quả này phù hợp với kết luận trong [40] rằng việc tinh chỉnh bộ mã hóa thị giác bằng cách sử dụng tập dữ liệu tinh chỉnh hướng dẫn quy mô nhỏ có thể dẫn đến sự suy giảm hiệu suất tiếp theo. Việc đóng băng bộ mã hóa thị giác cũng dẫn đến sự suy giảm hiệu suất trên các tập dữ liệu khác nhau. Điều này có thể được quy cho khả năng hạn chế của bộ mã hóa thị giác trong việc hiểu hình ảnh chi tiết. Thiết kế của chúng tôi cho phép tối ưu hóa cả hai phương thức và tận dụng lợi ích của việc tinh chỉnh hướng dẫn đa phương thức, dẫn đến hiệu suất được cải thiện. Hơn nữa, việc sử dụng LoRA [11] thay vì Adapter để thực hiện tinh chỉnh hiệu quả tham số cũng có thể đạt được hiệu suất được cải thiện so với Tinh chỉnh đầy đủ hoặc Đóng băng, chứng minh hiệu quả của việc thích ứng bộ mã hóa thị giác trong quá trình tinh chỉnh hướng dẫn đa phương thức.

4.3. So Sánh với Các Công Trình Gần Đây

Phần này tiến hành xác thực hiệu quả của phương pháp chúng tôi thông qua so sánh với các công trình gần đây khác.

Đánh giá trên các tác vụ lý luận đa phương thức thông thường. Để đánh giá khả năng tuân theo hướng dẫn của phương pháp chúng tôi, chúng tôi tiến hành thí nghiệm về năm tác vụ lý luận đa phương thức trên các benchmark công khai. Những benchmark và tác vụ này đánh giá các khía cạnh khác nhau của khả năng hiểu đa phương thức của mô hình. Như được thể hiện trong Bảng 2a, mô hình của chúng tôi liên tục đạt được hiệu suất tốt nhất bằng cách sử dụng ít tham số có thể huấn luyện hơn, tập huấn luyện nhỏ hơn, và độ phân giải hình ảnh đầu vào thấp hơn. Điều này chứng minh rằng tập dữ liệu tinh chỉnh hướng dẫn được xây dựng có thể nâng cao hiệu quả khả năng nhận thức chi tiết.

Đánh giá zero-shot trên tác vụ định vị thị giác. Định vị thị giác là một tác vụ RC cơ bản yêu cầu mô hình dự đoán tọa độ của hộp giới hạn dựa trên mô tả văn bản đã cho. Chúng tôi đánh giá mô hình trên ba tập dữ liệu được thiết lập tốt dưới cài đặt zero-shot trong Bảng 2b. Mô hình của chúng tôi vượt trội đáng kể so với Kosmos-2 [27], được huấn luyện với tập dữ liệu được tạo ra GRIT chứa 91M hình ảnh và 115M biểu thức tham chiếu. Kết quả này xác thực hiệu quả của phương pháp khởi tạo tự nhất quán được đề xuất.

So sánh với các mô hình khác dưới cài đặt fine-tuning trên các tác vụ RC. Để tiếp tục xác thực khả năng RC của phương pháp chúng tôi, chúng tôi so sánh nó với các mô hình khác [1,4,41] có thể thực hiện các tác vụ RC. So sánh bao gồm các mô hình có khả năng xử lý các tác vụ ngôn ngữ-thị giác khác nhau. Các mô hình được thiết kế đặc biệt cho tác vụ định vị thị giác không được bao gồm. Tập dữ liệu tinh chỉnh hướng dẫn của các mô hình được so sánh bao gồm các tập huấn luyện của các tập dữ liệu được liệt kê trong Bảng 2c. Để so sánh công bằng, chúng tôi cũng tận dụng những tập huấn luyện đó trong việc tinh chỉnh hướng dẫn của Pink. Lưu ý rằng, kích thước tập huấn luyện của Pink vẫn nhỏ hơn đáng kể so với những trong các phương pháp được so sánh, ví dụ: 50M của Qwen-VL [1] so với 519K của Pink.

Kết quả trong Bảng 2c cho thấy mô hình của chúng tôi đạt được hiệu suất đầy hứa hẹn dưới cài đặt fine-tuning. Điều này có thể được quy cho sự đa dạng của các tác vụ RC trong việc tinh chỉnh hướng dẫn. Object365 tiếp tục cải thiện hiệu suất, th甚至 các tập huấn luyện của những tập dữ liệu này đã được bao gồm. Điều này chứng minh hiệu quả của phương pháp khởi tạo tự nhất quán trong việc chuyển đổi tập dữ liệu hiện có thành tập huấn luyện RC có giá trị hơn. Qwen-VL sử dụng bộ mã hóa thị giác mạnh hơn ViT-G được huấn luyện bởi CLIP. Để thực hiện so sánh công bằng, chúng tôi thay đổi bộ mã hóa thị giác của Pink thành ViT-G. Pink-G vượt trội hơn Qwen-VL với biên độ lớn, thậm chí với độ phân giải hình ảnh đầu vào thấp hơn và ít mẫu huấn luyện hơn. Kết quả này tiếp tục xác thực hiệu quả của quy trình huấn luyện của chúng tôi.

So sánh với các mô hình khác trên tập thử nghiệm MMBench. MMBench [21] đã được đề xuất như một benchmark mới để đánh giá các khả năng khác nhau của MLLM, bao gồm lý luận logic (LR), lý luận thuộc tính (AR), lý luận quan hệ (RR), nhận thức chi tiết thực thể đơn lẻ (FP-S), nhận thức chi tiết thực thể chéo (FP-C), và nhận thức thô (CP). Do đó, chúng tôi tiến hành thí nghiệm trên MMBench để xác thực khả năng của mô hình trong tất cả các khía cạnh.

Kết quả được tóm tắt trong Bảng 2d cho thấy phương pháp của chúng tôi đạt được hiệu suất tổng thể tốt nhất trong số các phương pháp được so sánh. Cải thiện chính đến từ RR, FP-S, và FP-C. Ví dụ, so với mPlug-Owl [45], sự tăng cường độ chính xác của Pink trên LR và FP-C lần lượt là 1,7% và 8,8%. Những kết quả này chứng minh khả năng nhận thức chi tiết mạnh mẽ của Pink, có thể được quy cho việc tích hợp các tác vụ RC khác nhau trong quá trình tinh chỉnh hướng dẫn đa phương thức.

5. Kết Luận

Bài báo này trình bày một khung công tác mới để nâng cao khả năng nhận thức hình ảnh chi tiết của MLLM. Khung công tác bao gồm một phương pháp để xây dựng tập dữ liệu tinh chỉnh hướng dẫn bằng cách chuyển đổi các chú thích từ tập dữ liệu hiện có thành các tác vụ RC đa dạng. Một phương pháp khởi tạo tự nhất quán được đề xuất để mở rộng các chú thích đối tượng thành các cặp biểu thức tham chiếu-hộp giới hạn, cho phép thu thập thêm dữ liệu tinh chỉnh hướng dẫn với chi phí thấp. Bộ mã hóa thị giác được tinh chỉnh theo cách hiệu quả tham số để đạt được khả năng hiểu hình ảnh chi tiết. Với ít tham số có thể huấn luyện hơn và ít dữ liệu huấn luyện hơn, phương pháp của chúng tôi đạt được hiệu suất vượt trội trên cả các tác vụ đa phương thức và tác vụ RC.

Lời Cảm Ơn

Công việc này được hỗ trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc theo Grant No. U20B2052, 61936011, một phần bởi Giải thưởng Nghiên cứu Okawa Foundation, và một phần bởi Chương trình Thực tập Nghiên cứu Ant Group.
