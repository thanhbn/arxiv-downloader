# 2312.15821.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.15821.pdf
# Kích thước tệp: 2573110 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Audiobox: Tạo Audio Thống Nhất 
với Lời Nhắc Ngôn Ngữ Tự Nhiên

Apoorv Vyas∗,Bowen Shi∗,Matthew Le∗,Andros Tjandra∗,Yi-Chiao Wu∗,Baishan Guo ,Jiemin Zhang ,
Xinyue Zhang ,Robert Adkins ,William Ngan ,Jeff Wang ,Ivan Cruz ,Bapi Akula ,Akinniyi Akinyemi ,Brian
Ellis,Rashel Moritz ,Yael Yungster ,Alice Rakotoarison ,Liang Tan ,Chris Summers ,Carleigh Wood ,
Joshua Lane ,Mary Williamson†,Wei-Ning Hsu†
Nhóm Audiobox, Nghiên cứu AI Cơ bản (FAIR) tại Meta
∗Nhóm nghiên cứu, đóng góp ngang nhau
†Lãnh đạo nghiên cứu và kỹ thuật, đóng góp ngang nhau

Audio là một phần thiết yếu trong cuộc sống của chúng ta, nhưng việc tạo ra nó thường đòi hỏi chuyên môn và tốn thời gian. Cộng đồng nghiên cứu đã đạt được tiến bộ lớn trong năm qua trong việc nâng cao hiệu suất của các mô hình tạo audio quy mô lớn cho một modalité duy nhất (lời nói, âm thanh hoặc âm nhạc) thông qua việc áp dụng các mô hình tạo sinh mạnh hơn và mở rộng dữ liệu. Tuy nhiên, các mô hình này thiếu khả năng kiểm soát ở một số khía cạnh: các mô hình tạo lời nói không thể tổng hợp các phong cách mới dựa trên mô tả văn bản và bị hạn chế về phạm vi miền như môi trường ngoài trời; các mô hình tạo âm thanh chỉ cung cấp kiểm soát thô ráp dựa trên các mô tả như "một người đang nói" và sẽ chỉ tạo ra tiếng người lẩm bẩm. Bài báo này trình bày Audiobox, một mô hình thống nhất dựa trên flow-matching có khả năng tạo ra các modalité audio khác nhau. Chúng tôi thiết kế lời nhắc dựa trên mô tả và dựa trên ví dụ để tăng cường khả năng kiểm soát và thống nhất các paradigm tạo lời nói và âm thanh. Chúng tôi cho phép transcript, giọng nói và các phong cách audio khác được kiểm soát độc lập khi tạo lời nói. Để cải thiện tổng quát hóa mô hình với nhãn hạn chế, chúng tôi điều chỉnh một mục tiêu infilling tự giám sát để pre-train trên lượng lớn audio không gán nhãn. Audiobox thiết lập các benchmark mới về tạo lời nói và âm thanh (0.745 độ tương tự trên Librispeech cho zero-shot TTS; 0.77 FAD trên AudioCaps cho text-to-sound) và mở khóa các phương pháp mới để tạo audio với các phong cách giọng nói và âm thanh mới. Chúng tôi tích hợp thêm Bespoke Solvers, giúp tăng tốc độ tạo sinh hơn 25 lần so với ODE solver mặc định cho flow-matching, mà không mất hiệu suất trên một số tác vụ.

Liên hệ: Apoorv Vyas vyasapoorv@meta.com , Wei-Ning Hsu wnhsu@meta.com
Demo: https://audiobox.metademolab.com/

1 Giới thiệu

Tại sao xây dựng các mô hình tạo audio: Audio là một thành phần quan trọng trong việc tạo ra nhiều dạng nội dung, như phim, podcast, audiobook và quảng cáo. Tuy nhiên, việc tạo audio tốn thời gian và đòi hỏi nhiều chuyên môn khác nhau, như diễn xuất giọng nói, sáng tác và biểu diễn âm nhạc, tạo hiệu ứng âm thanh Foley và kỹ thuật âm thanh. Điều này tạo ra rào cản lớn cho công chúng, khiến mọi người khó trở thành người tạo audio. Ngay cả đối với các chuyên gia, việc thực hiện các tác vụ này vẫn có thể tốn nhiều thời gian và tài nguyên, hạn chế năng suất của họ. Phát triển các mô hình tạo audio có thể tổng quát hóa, kiểm soát được và chất lượng cao có thể mang lại những thay đổi mang tính chuyển đổi cho quy trình tạo audio, cải thiện hiệu quả của các chuyên gia cũng như giải phóng sự sáng tạo cho mọi người.

Tiến bộ của các mô hình tạo audio: Gần đây, các nhà nghiên cứu đã đạt được tiến bộ đáng kể trong việc phát triển các mô hình tạo audio. Các mô hình tạo lời nói có thể bắt chước bất kỳ phong cách giọng nói nào bằng cách sử dụng các lời nhắc audio ngắn chỉ ba giây (Wang et al., 2023a; Shen et al., 2023; Le et al., 2023; Kharitonov et al., 2023), điền vào một phần lời nói để loại bỏ tiếng ồn tạm thời hoặc chỉnh sửa từ cho bất kỳ người nói nào (Le et al., 2023; Shen et al., 2023), tổng hợp các ngôn ngữ nước ngoài bằng giọng nói của bất kỳ ai (Zhang et al., 2023; Le et al., 2023), và tạo ra các cuộc hội thoại (Borsos et al., 2023). Các mô hình tạo âm nhạc có thể tạo ra âm nhạc theo nhiều phong cách khác nhau bằng cách sử dụng một mô tả văn bản ngắn (Schneider et al., 2023; Huang et al., 2023a; Agostinelli et al., 2023; Copet et al., 2023) và điền vào

--- TRANG 2 ---
Hình 1 Sơ đồ mô hình Audiobox

một phần âm nhạc (Li et al., 2023). Các mô hình tạo hiệu ứng âm thanh theo một paradigm tương tự. Chúng có khả năng tạo ra và điền vào các cảnh âm thanh phức tạp như "chim hót và nước nhỏ giọt với một số tiếng đập ở phía sau" dựa vào mô tả văn bản (Yang et al., 2023c; Kreuk et al., 2022; Huang et al., 2023b; Ghosal et al., 2023; Liu et al., 2023b,c). Các mô hình gần đây cũng mở rộng sang chỉnh sửa tổng quát hơn, như loại bỏ hoặc thêm các sự kiện âm thanh với hướng dẫn ngôn ngữ tự nhiên (Wang et al., 2023b; Liu et al., 2023d).

Hạn chế của các mô hình hiện tại: Các mô hình tạo audio hiện tại vẫn bị hạn chế về khả năng kiểm soát và tổng quát hóa. Trước tiên, nội dung audio trong thế giới thực thường chứa hỗn hợp lời nói, âm nhạc và hiệu ứng âm thanh. Tuy nhiên, các mô hình tạo audio hiện tại chủ yếu là specific-modalité, chỉ tạo ra lời nói, âm nhạc hoặc hiệu ứng âm thanh. Đặc biệt, các mô hình tạo lời nói quy mô lớn hiện tại (Wang et al., 2023a; Le et al., 2023; Shen et al., 2023) được huấn luyện chủ yếu trên audiobook (Zen et al., 2019; Kahn et al., 2019; Pratap et al., 2020), thiếu đa dạng so với dữ liệu thực sự in-the-wild như AudioSet (Gemmeke et al., 2017) về tính biểu cảm (ví dụ: âm thanh phi ngôn ngữ như ho, la hét, cười) và điều kiện âm thanh (ví dụ: thành thị, nông thôn, trong nhà công cộng, sân vận động). Các mô hình này chỉ có thể tạo ra audio với phong cách hạn chế và không nắm bắt được mối tương quan giữa các modalité audio khác nhau.

Mặt khác, có sự bất đồng giữa paradigm tạo lời nói và âm thanh/lời nói. Các mô hình tạo lời nói gần đây chủ yếu sử dụng kiểm soát dựa trên ví dụ, trong đó một mẫu audio của phong cách mục tiêu được cung cấp và việc kiểm soát phong cách chính xác hơn; ngược lại, kiểm soát dựa trên mô tả được áp dụng cho tạo âm nhạc và âm thanh, trong đó mô hình có thể tạo ra các phong cách mới dựa trên lời nhắc ngôn ngữ tự nhiên. Cả hai cách tiếp cận đều có điểm mạnh và điểm yếu riêng, nhưng sự bất đồng như vậy ngăn cản việc phát triển các mô hình thống nhất tận hưởng được những điều tốt nhất từ cả hai thế giới.

Cuối cùng nhưng không kém phần quan trọng, các mô hình tạo âm thanh hiện tại chỉ cung cấp kiểm soát thô ráp như "một người đàn ông đang nói" khi tạo lời nói. Các bộ dữ liệu hiện tại không cung cấp các caption chi tiết hơn đặc trưng cho phong cách giọng nói một cách chi tiết hơn, như "Một phụ nữ trung niên từ miền Nam nước Mỹ đang nói qua điện thoại với giọng nói đầy cảm xúc. Cô ấy nói với tốc độ nhanh và cao độ cao." Những mô hình này cũng không cho phép đầu vào transcript để kiểm soát nội dung văn bản. Do đó, các mô hình này chỉ có thể tạo ra lời nói lẩm bẩm.

Do thiếu sự xem xét trong việc tạo lời nói có hướng dẫn ngôn ngữ trong môi trường tự nhiên, việc thiết kế các metric đánh giá khách quan phù hợp cho các mô hình universal như vậy vẫn là một câu hỏi mở chưa được các nghiên cứu trước đây giải quyết đầy đủ. Trong đánh giá khách quan, các nghiên cứu hướng lời nói trước đây Guo et al. (2023); Leng et al. (2023); Yang et al. (2023a) thường áp dụng các metric đánh giá ad-hoc (ví dụ: độ chính xác của các thuộc tính được định nghĩa trước), khiến việc tổng quát hóa sang các hướng dẫn dạng tự do trở nên khó khăn. Mạng nhúng audio-text kết hợp (ví dụ: CLAP Wu et al. (2023)), được sử dụng rộng rãi trong tạo text-to-audio, được điều chỉnh cho các sự kiện âm thanh và thường không đủ khả năng nắm bắt các thuộc tính phức tạp như giọng trong lời nói (xem Mục 7.1.1).

--- TRANG 3 ---
Mục tiêu và tổng quan về mô hình của chúng tôi: Để giải quyết những vấn đề này, có ba mục tiêu chính của công trình này. Đầu tiên, chúng tôi nhằm xây dựng một mô hình thống nhất cho âm thanh và lời nói để tạo ra nhiều loại audio thế giới thực hơn, thường là sự kết hợp của cả hai. Thứ hai, chúng tôi muốn cải thiện khả năng kiểm soát để tạo ra các phong cách mới thông qua việc cho phép nhiều phương pháp đầu vào, sử dụng audio tham chiếu, mô tả văn bản hoặc sự kết hợp của cả hai. Cuối cùng nhưng không kém phần quan trọng, để cải thiện tổng quát hóa mô hình, chúng tôi muốn mở rộng dữ liệu huấn luyện và sử dụng dữ liệu với các mức độ giám sát khác nhau.

Để đạt được điều đó, chúng tôi trình bày framework Audiobox. Audiobox được xây dựng dựa trên Voicebox (Le et al., 2023) và SpeechFlow (Liu et al., 2023a), là các mô hình dựa trên flow-matching cho tạo lời nói có hướng dẫn transcript và pre-training lời nói tự giám sát, tương ứng. Để tạo điều kiện cho việc mở rộng dữ liệu và phát triển các mô hình downstream, trước tiên chúng tôi áp dụng phương pháp pre-training SpeechFlow và pre-train một mô hình thống nhất sử dụng lượng lớn lời nói, âm nhạc và hiệu ứng âm thanh không gán nhãn, được gọi là Audiobox SSL (Mục 4). Để xác nhận hiệu quả của mô hình pre-trained thống nhất, chúng tôi fine-tune Audiobox SSL cho tạo lời nói có hướng dẫn transcript (Audiobox Speech, Mục 5) và tạo âm thanh có hướng dẫn mô tả (Audiobox Sound, Mục 6), cho thấy sự cải thiện đáng kể so với các nghiên cứu trước.

Kết hợp những điều tốt nhất từ cả hai thế giới, chúng tôi trình bày Audiobox, mô hình thống nhất cho tạo âm thanh và lời nói trong Mục 7. Nó bắc cầu khoảng cách giữa tạo âm thanh và lời nói bằng cách cho phép lời nhắc ngôn ngữ tự nhiên để kiểm soát phong cách toàn diện, và thúc đẩy kiểm soát lời nói tách biệt với voice prompts. Mô hình kết hợp của chúng tôi đạt được khả năng kiểm soát chưa từng có cho tạo audio universal và tính linh hoạt vượt trội với các khả năng bổ sung trên những gì Voicebox cung cấp. Audiobox vượt trội hơn các mô hình specific miền hiện tại trên nhiều tác vụ và gần với Audiobox Speech và Audiobox Sound trên các tác vụ benchmark tương ứng của chúng.

Để tạo điều kiện cho việc đánh giá Audiobox và thúc đẩy nghiên cứu trong các mô hình tạo audio universal có hướng dẫn văn bản, chúng tôi đề xuất Joint-CLAP, được huấn luyện trên cả dữ liệu mô tả âm thanh và lời nói. So với CLAP Wu et al. (2023), Joint-CLAP vượt trội đáng kể so với CLAP trong việc truy xuất lời nói dựa trên mô tả, và độ tương tự text-to-audio thể hiện mối tương quan mạnh hơn với đánh giá của con người.

Một cách trực giao, để cải thiện trade-off hiệu suất-hiệu quả, chúng tôi tích hợp Bespoke Solver, một phương pháp tối ưu hóa suy luận post-training mới cho các mô hình flow-matching. Với Bespoke Solver, các mô hình của chúng tôi có thể tăng tốc 25x so với việc sử dụng adaptive step size dopri5 solver mà không mất hiệu suất.

Khi các mô hình tạo sinh trở nên mạnh mẽ hơn và trở thành những phần thiết yếu trong cuộc sống của mọi người, việc tiến hành nghiên cứu có trách nhiệm và giảm thiểu các rủi ro tiềm ẩn trở nên quan trọng hơn bao giờ hết. Chúng tôi đã tiến hành một loạt nghiên cứu chứng minh sự công bằng được đạt được thông qua việc đại diện tốt hơn cho giọng nói của các nhóm nhân khẩu học khác nhau với việc mở rộng dữ liệu. Chúng tôi cũng xác nhận hiệu quả của một hệ thống watermarking gần đây (Seamless Communication, 2023), cho thấy việc xác minh rất hiệu quả và mạnh mẽ trước các nhiễu loạn đối kháng.

2 Nghiên cứu liên quan

Bài báo này liên quan đến một lượng lớn công trình về mô hình tạo sinh quy mô lớn cho audio. Vì trọng tâm của công trình này là về tính universal và khả năng kiểm soát, trước tiên chúng tôi thảo luận về tạo sinh có thể kiểm soát cho các mô hình specific modalité và sau đó so sánh với các nghiên cứu gần đây về các mô hình universal có thể thực hiện nhiều tác vụ hoặc tạo audio trong nhiều modalité và miền. Trong phần còn lại của bài báo, chúng tôi sẽ gọi lời nói, âm thanh, âm nhạc là các modalité audio khác nhau, và trong biến thể phong cách modalité, như lời nói đọc, lời nói tự phát, lời nói đối thoại, là các miền khác nhau.

Các mô hình tạo text-to-speech in-context quy mô lớn: Trong vài tháng qua, đã có tiến bộ đáng kể trong việc phát triển các mô hình tạo lời nói quy mô lớn (Wang et al., 2023a; Shen et al., 2023; Kharitonov et al., 2023; Le et al., 2023; Yang et al., 2023b; Borsos et al., 2023) được huấn luyện trên dữ liệu in-the-wild ở quy mô gần 100K giờ (Kahn et al., 2019; Pratap et al., 2020) với giám sát tối thiểu, dẫn đến tổng quát hóa tốt hơn nhiều để tổng hợp các phong cách lời nói chưa thấy theo cách zero-shot. Các mô hình này tương phản rõ rệt với các mô hình dựa trên regression thông thường như Ren et al. (2021); Shen et al. (2017); Łańcucki (2021), được huấn luyện trên các bộ dữ liệu được tuyển chọn cẩn thận (Yamagishi et al., 2019) chứa audio sạch, biến thể phong cách hạn chế và nhãn mở rộng (ví dụ: nhãn speaker và emotion).

--- TRANG 4 ---
Chìa khóa cho việc mở rộng dữ liệu thành công trong các công trình gần đây là việc áp dụng các mô hình tạo sinh mạnh mẽ có thể nắm bắt các mối quan hệ đầu vào-đầu ra rất ngẫu nhiên. Ví dụ, VALL-E (Wang et al., 2023a) áp dụng cách tiếp cận mô hình ngôn ngữ tự hồi quy dựa trên token, chuyển đổi lời nói thành các token rời rạc với mô hình neural codec (Défossez et al., 2022) và hình thức hóa text-to-speech (TTS) như một bài toán mô hình ngôn ngữ có điều kiện cho trước một transcript và một lời nhắc audio (vài giây đầu của lời nói mục tiêu). NaturalSpeech2 (Shen et al., 2023) và Voicebox (Le et al., 2023) áp dụng các mô hình diffusion không tự hồi quy (Ho et al., 2020) và conditional flow-matching (Lipman et al., 2023). Cho trước một transcript và một ngữ cảnh audio (audio xung quanh lời nói mục tiêu), các mô hình này lặp đi lặp lại biến đổi một nhiễu được lấy mẫu từ một prior đơn giản thành lời nói, được biểu diễn dưới dạng các đặc trưng latent đã học hoặc mel spectrograms.

Ở mức độ cao, VALL-E thực hiện tiếp nối lời nói có hướng dẫn transcript trong khi NaturalSpeech2 và Voicebox thực hiện infilling lời nói có hướng dẫn transcript. Các mô hình này được huấn luyện chỉ với giám sát transcript, tạo điều kiện cho việc mở rộng dữ liệu. Phong cách của audio được tạo ra được kiểm soát thông qua lời nhắc audio hoặc ngữ cảnh audio. Lưu ý rằng phong cách không chỉ đề cập đến giọng nói, mà còn mọi thứ khác ngoài transcript, bao gồm prosody, emotion, môi trường âm thanh, kênh, nhiễu, v.v. Điều này có thể được hiểu như một dạng học in-context: vì phong cách audio có xu hướng nhất quán trong một utterance, các mô hình này học cách suy luận phong cách của mục tiêu dựa trên ngữ cảnh của nó. Ngược lại, nó cho phép tổng quát hóa đến phong cách chưa thấy, sao cho lời nói của bất kỳ phong cách nào có thể được tạo ra bằng cách điều kiện hóa trên một lời nhắc audio/ngữ cảnh của phong cách mong muốn.

Mặc dù paradigm chuyển giao phong cách in-context rất mạnh mẽ, nó cũng có một số hạn chế về khả năng kiểm soát. Đầu tiên, lời nhắc audio là cơ chế đầu vào duy nhất để kiểm soát phong cách audio. Người dùng không thể cung cấp một văn bản mô tả, như "một người đàn ông trẻ nói với giọng vui vẻ trong một auditorium" để tạo ra lời nói đa dạng phù hợp với mô tả, trong khi tính năng này thường được hỗ trợ và được ưa chuộng rộng rãi cho tạo hình ảnh (Ramesh et al., 2022; Rombach et al., 2022), âm nhạc (Agostinelli et al., 2023) và âm thanh (Kreuk et al., 2022). Thứ hai, kiểm soát phong cách tách biệt không được kích hoạt với paradigm này, trong đó giọng nói và các thuộc tính khác, như emotion và điều kiện âm thanh, có thể được kiểm soát độc lập. Tính năng này thường được mong muốn như được minh họa trong các công trình trước đây nơi emotion và giọng nói có thể được kiểm soát độc lập (Hsu et al., 2019; Kulkarni et al., 2021; Nguyen et al., 2023).

Lời nhắc phong cách ngôn ngữ tự nhiên cho tạo lời nói có thể kiểm soát: Các nghiên cứu về tạo lời nói có thể kiểm soát nhằm phát triển các mô hình có thể tạo ra lời nói của nhiều miền khác nhau và cung cấp các phương pháp đầu vào cho việc kiểm soát tách biệt, linh hoạt và chính xác. Các mô hình trước đây thường chỉ cho phép kiểm soát một số lượng nhỏ thuộc tính (ví dụ: speaker và emotion) với một số lượng cố định các tùy chọn (ví dụ: happy/sad/neutral cho emotion) thông qua các vector one-hot (Nguyen et al., 2023). Các phương pháp như vậy khó tổng quát hóa vì khó biểu diễn nhiều thuộc tính lời nói, như chất lượng audio, môi trường âm thanh, bằng các vector one-hot. Cũng không thể biểu diễn chính xác thông tin như "một speaker bắt đầu với tốc độ chậm và tăng tốc".

Các mô hình TTS in-context (Wang et al., 2023a) cải thiện đáng kể phạm vi miền, nhưng có hạn chế về tính linh hoạt và kiểm soát tách biệt được mô tả ở trên.

Để giải quyết hạn chế này, một số nghiên cứu gần đây cũng đề xuất kiểm soát phong cách lời nói thông qua lời nhắc ngôn ngữ tự nhiên. InstructTTS (Yang et al., 2023a) và PromptTTS (Guo et al., 2023) là hai công trình sớm nhất. Chúng được huấn luyện trên dữ liệu quy mô nhỏ với chủ yếu biến thể emotion và số lượng speaker hạn chế (7 cho InstructTTS và 2 cho thiết lập synthetic PromptTTS). Đặc biệt, InstructTTS thu thập các mô tả của con người cho 44 giờ lời nói tập trung chỉ vào emotion và một đầu vào speaker ID riêng biệt được sử dụng làm đầu vào mô hình. Do đó, lời nhắc ngôn ngữ tự nhiên chỉ được sử dụng để kiểm soát emotion. PromptTTS tuyển dụng các annotator con người để viết mô tả cho bốn đến năm nhãn thuộc tính đã cho (emotion, gender, volume, speed và pitch; nhãn emotion không có sẵn cho dữ liệu thực), và huấn luyện các mô hình trên dữ liệu synthetic 2-voice cũng như LibriTTS (Zen et al., 2019). Vì các mô tả của PromptTTS được tạo dựa trên các nhãn thuộc tính thay vì các mẫu lời nói, những mô tả này không chứa thông tin bổ sung so với các nhãn và về mặt lý thuyết không cho phép kiểm soát thuộc tính chi tiết hơn.

PromptTTS2 (Leng et al., 2023) là một công trình đồng thời cải thiện PromptTTS ở hai khía cạnh. Đầu tiên, nó đề xuất một pipeline tạo mô tả tự động dựa trên speech attribute labeler và các mô hình ngôn ngữ lớn, cho phép mở rộng đến huấn luyện trên 44K giờ dữ liệu audiobook. Thứ hai, PromptTTS2 áp dụng mô hình diffusion để nắm bắt mối quan hệ một-nhiều cho trước đầu vào (transcript và mô tả), trong khi PromptTTS áp dụng mô hình regression giả định ánh xạ deterministic. Tuy nhiên, tương tự như PromptTTS, tất cả các mô tả mà PromptTTS2 tạo ra đều được dẫn xuất từ bốn thuộc tính phân loại với hai đến ba tùy chọn

--- TRANG 5 ---
mỗi cái (tổng cộng 54 kết hợp). Do đó, PromptTTS2 không cung cấp kiểm soát chi tiết hơn so với PromptTTS và có phạm vi hạn chế về các thuộc tính mà nó có thể kiểm soát thông qua lời nhắc ngôn ngữ tự nhiên.

Các mô hình quy mô lớn cho miền tổng quát cho tạo âm thanh và âm nhạc: Text-to-sound (Kreuk et al., 2022) và text-to-music (Schneider et al., 2023) là các paradigm mới nổi cho tạo âm thanh và âm nhạc miền tổng quát, trái ngược với các nghiên cứu trước đây tạo ra hiệu ứng âm thanh hữu hạn (Donahue et al., 2018) hoặc nhạc cụ (Huang et al., 2018). Văn bản ở đây đề cập đến một mô tả toàn diện về audio mục tiêu, như "Một đứa trẻ la hét trong khi tiếng còi xe cấp cứu kêu với tiếng còi thổi." (Kim et al., 2019) và "Bản ghi âm chất lượng thấp có một bài ballad chứa dây kéo dài... Nó nghe buồn và sâu sắc, như thứ bạn sẽ nghe ở các buổi lễ Chủ nhật." cho âm nhạc (Agostinelli et al., 2023).

Tương tự như tạo lời nói, tiến bộ gần đây có thể được quy cho phần lớn sự tiến bộ trong các mô hình tạo sinh cho dữ liệu liên tục (Ho et al., 2020; Huang et al., 2023a; Liu et al., 2023b) và các audio tokenizer (Zeghidour et al., 2022; Défossez et al., 2022; Kreuk et al., 2022; Copet et al., 2023; Agostinelli et al., 2023), cho phép các phương pháp mô hình hóa có khả năng nắm bắt các phân phối có điều kiện rất ngẫu nhiên của audio cho trước các mô tả cho dữ liệu âm thanh/âm nhạc miền tổng quát.

Một hạn chế chính của các mô hình này là khả năng kiểm soát transcript và tạo ra lời nói hoặc vocals dễ hiểu. Các mô hình này chỉ nhận một mô tả làm đầu vào, không chỉ định transcript khi lời nói được trình bày. Do đó, việc tạo ra các mẫu với lời nhắc như "một người đang nói" thường dẫn đến âm thanh lẩm bẩm giống lời nói với nội dung không thể hiểu được (Liu et al., 2023b). Nói cách khác, các mô hình này không cung cấp đầu vào cho người dùng kiểm soát transcript, và chưa học các mô hình ngôn ngữ cho phép nó xây dựng và tổng hợp các câu có ý nghĩa chỉ cho trước mô tả.

Mô hình thống nhất cho tạo audio: Với tiến bộ lớn được thực hiện trong việc phát triển các mô hình miền tổng quát cho mỗi modalité audio, các nhà nghiên cứu cũng bắt đầu khám phá mô hình thống nhất có thể tạo ra audio vượt quá một modalité duy nhất và thực hiện nhiều tác vụ tạo sinh. Một mô hình như vậy có thể học từ các nguồn giám sát khác nhau và hưởng lợi từ việc chuyển giao kiến thức qua các tác vụ. Có ba nghiên cứu đồng thời liên quan đến công trình này.

UniAudio (Yang et al., 2023b) tập trung vào việc xây dựng một mô hình duy nhất có thể thực hiện nhiều tác vụ, bao gồm text-to-music, text-to-sound, và TTS in-context và TTS có lời nhắc phong cách ngôn ngữ tự nhiên. Nó theo framework VALL-E (Wang et al., 2023a), tokenize audio và serialize đầu vào điều kiện và token audio đầu ra để huấn luyện một mô hình ngôn ngữ có điều kiện dựa trên token. Nó được huấn luyện trên cùng các mô tả lời nói được thu thập bởi PromptTTS, kế thừa cùng các hạn chế về những thuộc tính nào và mức độ chi tiết nào chúng có thể được kiểm soát thông qua lời nhắc ngôn ngữ tự nhiên như đã thảo luận trước đây.

VoiceLDM (Lee et al., 2023) là công trình liên quan nhất. Nó giới thiệu một đầu vào transcript vào AudioLDM (Liu et al., 2023b) và kiểm soát phong cách thông qua mô tả văn bản được nhúng với một mô hình Contrastive Language-Audio Pre-training (CLAP) đông lạnh (Wu et al., 2023). Trong quá trình huấn luyện, embedding CLAP từ audio được sử dụng để điều kiện hóa. VoiceLDM được huấn luyện trên các bộ dữ liệu với biến thể âm thanh phong phú, và do đó có khả năng tạo ra lời nói trong các môi trường âm thanh đa dạng. Tuy nhiên, hiệu suất về khả năng kiểm soát bị giới hạn bởi mô hình CLAP được pre-train. Vì mô hình CLAP được huấn luyện trên các cặp audio-caption tập trung vào các sự kiện âm thanh, embedding chỉ mã hóa thông tin rất thô ráp về các thuộc tính lời nói. Hơn nữa, VoiceLDM cũng theo paradigm tạo âm thanh luôn tạo ra các clip audio có kích thước cố định (10 giây), không lý tưởng cho tạo lời nói thường có độ dài biến đổi nói chung. Cuối cùng, mặc dù mô hình có thể tạo ra âm thanh không phải lời nói khi được điều kiện hóa trên transcript rỗng, hiệu suất của việc tạo âm thanh tụt hậu so với các mô hình hiện đại một khoảng cách lớn.

AudioLDM 2 (Liu et al., 2023c) trình bày một mô hình hai giai đoạn áp dụng cho tạo lời nói, âm thanh và âm nhạc. Nó bao gồm một mô hình deterministic auto-regressive ánh xạ đầu vào điều kiện (ví dụ: audio được nhúng CLAP, mô tả, transcript, hình ảnh) đến chuỗi đặc trưng semantic, và một mô hình diffusion ánh xạ semantic đến đặc trưng acoustic. Cấu trúc tương tự như SPEAR-TTS (Kharitonov et al., 2023) nhưng với các phương pháp mô hình hóa và biểu diễn khác nhau cho mỗi giai đoạn. Do đó, tương tự như vậy, nó có thể tận dụng audio không gán nhãn để huấn luyện mô hình giai đoạn thứ hai. Mặc dù AudioLDM 2 trình bày một framework thống nhất, về mặt thực nghiệm các mô hình riêng biệt cho tạo lời nói và âm thanh/âm nhạc được huấn luyện, như các tác giả lưu ý rằng các hyperparameter kiến trúc mô hình khác nhau được yêu cầu cho các modalité khác nhau.

--- TRANG 6 ---
3 Nền tảng

Công trình này được xây dựng chủ yếu dựa trên mục tiêu huấn luyện và kiến trúc mô hình của Voicebox (Le et al., 2023), và mục tiêu tự giám sát của SpeechFlow (Liu et al., 2023a). Cả hai nghiên cứu đều áp dụng conditional flow-matching (Lipman et al., 2023) làm backbone mô hình hóa, đây là một mô hình tạo sinh không tự hồi quy mạnh mẽ cho dữ liệu liên tục. Chúng tôi cung cấp một tổng quan kỹ thuật ở đây.

Conditional flow-matching: Conditional flow-matching (FM) (Lipman et al., 2023) là một phương pháp mô hình tạo sinh mới được dẫn xuất từ framework continuous normalizing flow (Chen et al., 2018). Nó mô hình hóa các đường dẫn biến đổi các mẫu từ một phân phối prior đơn giản p0 đến các mẫu tương ứng từ phân phối dữ liệu phức tạp p1 theo cách liên tục. Chúng tôi sử dụng flow step t để mô tả tiến trình biến đổi, trong đó prior ở t=0 và dữ liệu ở t=1.

Mục tiêu huấn luyện của FM giống với mục tiêu của các mô hình diffusion (Ho et al., 2020): trong quá trình huấn luyện, cho trước một mẫu x1 được rút ra từ phân phối dữ liệu, một flow step ngẫu nhiên t∼U[0,1] được lấy mẫu, và một phiên bản nhiễu của dữ liệu xt cũng như đạo hàm vt=dxt/dt của nó cho đường dẫn điều kiện đã chọn được tính toán. Một mô hình FM u được huấn luyện để dự đoán đạo hàm vt cho trước t và xt. Trong quá trình suy luận, để rút ra một mẫu x1 từ phân phối dữ liệu đã học, một mẫu x0 trước tiên được rút ra từ phân phối prior, và sau đó ODE solver được sử dụng để ước tính x1 cho trước x0 và đạo hàm được tham số hóa bởi mô hình FM thông qua tích phân. Trade-off giữa độ chính xác của ước tính x1 và tốc độ có thể được lựa chọn linh hoạt bằng cách cấu hình ODE solver.

Ở mức độ cao, FM bao gồm các mô hình diffusion, tương ứng với các đường dẫn cụ thể của việc biến đổi. Các tác giả của Lipman et al. (2023) đã trình bày một lựa chọn thay thế được gọi là optimal transport (OT), là các đường dẫn có điều kiện với các hướng và tốc độ không đổi. Nó được cho là dễ học hơn và có thể được ước tính chính xác hơn bởi ODE solver với ít bước hơn. Đường dẫn OT dẫn đến hiệu quả huấn luyện và suy luận tốt hơn như được xác minh thực nghiệm trong Lipman et al. (2023) và Le et al. (2023).

Cho trước một mẫu x1 và một flow-step t, với đường dẫn có điều kiện OT chúng ta có xt=(1−(1−σmin)t)x0+tx1 và vt=x1−(1−σmin)x0, trong đó x0 được rút ra từ phân phối prior N(0,I) và σmin là một giá trị nhỏ (10−5). Mô hình FM u tối thiểu hóa:

Et,x1,x0||u(xt,t)−vt||2. (1)

Voicebox: Voicebox (Le et al., 2023) là một mô hình tạo sinh có điều kiện dựa trên FM bổ sung điều kiện hóa trên transcript phonetic căn chỉnh frame và audio bị che để dự đoán audio, và điều kiện hóa trên transcript phonetic và chuỗi duration bị che để dự đoán phone duration. Audio được biểu diễn dưới dạng Mel spectrograms 80 chiều và được chuyển đổi thành dạng sóng bằng vocoder HiFi-GAN (Kong et al., 2020). Chuỗi duration biểu thị số frame cho mỗi phoneme trong transcript.

Voicebox áp dụng mô hình Transformer (Vaswani et al., 2017) với các kết nối U-Net (Ronneberger et al., 2015). Spectrogram bị che (hoặc duration bị che), phone embeddings căn chỉnh frame (hoặc phone embeddings), và audio nhiễu xt (hoặc duration nhiễu) được nối theo chiều kênh và được chiếu đến chiều đặc trưng Transformer. Embedding sinusoidal flow step sau đó được nối với các đặc trưng đã chiếu theo chiều thời gian, được đưa làm đầu vào cho mô hình Transformer. Đầu ra Transformer sau đó được chiếu đến 80 chiều (hoặc 1 chiều cho duration) và dự đoán đạo hàm vt.

Nó là một mô hình có giám sát được huấn luyện trên 60K giờ audiobook và đạt hiệu suất hiện đại trên tổng hợp text-to-speech in-context có thể bắt chước phong cách audio cho trước một lời nhắc audio ba giây. Nó cũng rất linh hoạt do tính tổng quát của infilling có hướng dẫn transcript, trong đó mô hình có thể thực hiện loại bỏ nhiễu tạm thời, tạo phong cách đa dạng, chỉnh sửa lời nói, chuyển giao phong cách đa ngôn ngữ bằng cách đơn giản hình thành các đầu vào transcript và audio khác nhau.

SpeechFlow: SpeechFlow (Liu et al., 2023a) là một framework tự giám sát dựa trên FM học cách infill lời nói cho trước ngữ cảnh audio. Điều này tương đương với Voicebox mà không điều kiện hóa trên transcript. Mục tiêu tự giám sát giải quyết các vấn đề thiếu nhãn và cho phép mô hình học từ lượng lớn lời nói không gán nhãn về phân phối lời nói cũng như mối tương quan giữa các phân đoạn thời gian trong một utterance.

--- TRANG 7 ---
Fine-tuning SpeechFlow với cùng mục tiêu infilling có hướng dẫn transcript như Voicebox cho thấy hiệu suất và hiệu quả mẫu vượt trội, phù hợp với độ tương tự phong cách của VALL-E (Wang et al., 2023a) chỉ với 10 giờ dữ liệu gán nhãn. Mô hình được pre-train cũng thể hiện những cải thiện đầy hứa hẹn trên các tác vụ tạo lời nói khác, bao gồm tách nguồn và nâng cao lời nói. Nó cũng cho phép fine-tuning hiệu quả tham số như LoRA (Hu et al., 2021) và fine-tuning với batch size thấp hơn nhiều, chứng minh hiệu quả và khả năng tái sử dụng của các mô hình pre-train tự giám sát.

4 Audiobox SSL: Pre-training Audio Tạo sinh Tự giám sát

Bước đầu tiên của chúng tôi là phát triển Audiobox SSL, một mô hình nền tảng có thể được fine-tune cho bất kỳ tác vụ tạo audio downstream nào. Vì dữ liệu gán nhãn không phải lúc nào cũng có sẵn hoặc chất lượng cao, và việc mở rộng dữ liệu là chìa khóa cho tổng quát hóa, chiến lược của chúng tôi là huấn luyện mô hình nền tảng này sử dụng audio mà không có bất kỳ giám sát nào, như transcript, caption hoặc nhãn thuộc tính, có thể được tìm thấy với số lượng lớn hơn.

4.1 Phương pháp

Chúng tôi điều chỉnh Audiobox SSL từ SpeechFlow, ban đầu được thiết kế cho pre-training tạo lời nói. Cùng mục tiêu học cũng có ý nghĩa đối với audio tổng quát: thông qua việc học infill, mô hình cũng có thể nắm bắt mối quan hệ thời gian của các sự kiện audio (ví dụ: âm thanh đồng hồ tick tại khoảng thời gian cố định, tàu hỏa đến gần tạo ra âm thanh với âm lượng tăng dần), và học phân phối của audio tổng quát. Do đó, trong quá trình fine-tuning có giám sát, một mô hình không cần học một mẫu audio tự nhiên nghe như thế nào, mà chỉ cần học căn chỉnh nhãn với mode phân phối tương ứng.

Mô hình SpeechFlow gốc được huấn luyện để dự đoán spectrogram và sử dụng một mô hình HiFi-GAN để tạo dạng sóng cho trước spectrogram. Tuy nhiên, HiFi-GAN không tổng quát hóa tốt cho audio không phải lời nói như âm thanh hoặc âm nhạc (Lee et al., 2022). Để giải quyết vấn đề đó, chúng tôi huấn luyện mô hình để dự đoán các đặc trưng latent được học bởi một autoencoder. Đặc biệt, chúng tôi sử dụng các đặc trưng Encodec dense (Défossez et al., 2022) được trích xuất trước lớp quantization dư thừa, chứng minh chất lượng tái tổng hợp tốt trong các modalité audio khác nhau và đã được áp dụng cho tạo âm thanh và âm nhạc (Kreuk et al., 2022; Copet et al., 2023). Điều này tương tự như framework latent diffusion (Rombach et al., 2022) cũng được áp dụng trong NaturalSpeech2 (Shen et al., 2023).

Trong quá trình huấn luyện, mô hình được điều kiện hóa trên các đặc trưng bị che hoàn toàn với xác suất pcond. Với xác suất 1−pcond, một tập con (nmask) các frame bị che với độ dài span tối thiểu lmask. Mất mát FM chỉ được tính toán trên các frame bị che. Khi một frame bị che, các đặc trưng của nó được đặt thành 0.

4.2 Thiết lập Thực nghiệm

Dữ liệu huấn luyện: Chúng tôi thu thập một bộ dữ liệu audio quy mô lớn tăng đáng kể phạm vi miền, phạm vi modalité và số lượng so với các nghiên cứu mô hình tạo audio quy mô lớn trước đây (Yang et al., 2023b; Borsos et al., 2023; Wang et al., 2023a; Liu et al., 2023c), tận dụng các bộ dữ liệu từ 10K đến 100K giờ chứa chủ yếu lời nói từ một miền duy nhất (ví dụ: audiobook).

Cụ thể, bộ dữ liệu của chúng tôi bao gồm hơn 160K giờ lời nói (chủ yếu tiếng Anh), 20K giờ âm nhạc và 6K giờ mẫu âm thanh. Phần lời nói bao gồm audiobook, podcast, câu đọc, bài thuyết trình, cuộc trò chuyện và ghi âm in-the-wild bao gồm các điều kiện âm thanh khác nhau và giọng nói không ngôn ngữ. Để đảm bảo công bằng và đại diện tốt cho mọi người từ các nhóm khác nhau, nó bao gồm người nói từ hơn 150 quốc gia nói hơn 200 ngôn ngữ chính khác nhau. Chúng tôi gọi tập này là "Mix-185K."

Mô hình và huấn luyện: Chúng tôi huấn luyện một Transformer 24 lớp Vaswani et al. (2017) với convolutional position embeddings Baevski et al. (2020) và symmetric bi-directional ALiBi self-attention bias Press et al. (2021). Mô hình có 16 attention head, 1024/4096 embedding/feed-forward network (FFN) dimension, và 330M tham số. Chúng tôi thêm các kết nối skip kiểu UNet, trong đó các state được nối theo kênh và sau đó kết hợp bằng một lớp tuyến tính.

Mô hình được huấn luyện trong 1 triệu update với batch size hiệu quả 480K frame. Để hiệu quả, các mẫu được cắt ngẫu nhiên nếu chúng vượt quá 1,600 frame. Chúng tôi đặt pcond=0.1, nmask∼U[70%,100%], và lmask=10.

--- TRANG 8 ---
Chúng tôi sử dụng optimizer Adam Kingma and Ba (2014) với learning rate 1e-4, làm ấm tuyến tính trong 5k bước và giảm tuyến tính trong phần còn lại của quá trình huấn luyện. Để ổn định, chúng tôi sử dụng gradient norm clipping với ngưỡng norm là 0.2.

5 Audiobox Speech: Mở rộng Tổng hợp Text-to-speech In-context

Trong phần này, chúng tôi nghiên cứu hiệu quả của pre-training và mở rộng dữ liệu fine-tuning cho tạo lời nói. Chúng tôi trình bày Audiobox Speech, fine-tune Audiobox SSL với cùng mục tiêu infilling lời nói có hướng dẫn transcript như Voicebox sử dụng lời nói có transcript. Mô hình kết quả có thể được áp dụng cho nhiều tác vụ downstream giống như Voicebox.

5.1 Phương pháp

Để kết hợp transcript căn chỉnh frame z, chúng tôi theo Liu et al. (2023a). Cụ thể, cho trước các đặc trưng Encodec nhiễu xt tại flow-step t, các đặc trưng Encodec bị che xctx, trước tiên chúng tôi nối xt và xctx theo kênh và áp dụng một phép chiếu tuyến tính để có xh. Sau đó chúng tôi áp dụng một lớp tuyến tính khác lên transcript embeddings căn chỉnh frame zemb, và thêm điều này vào hidden state xh. Các đặc trưng kết quả được nối với flow step sinusoidal embedding theo chiều thời gian và đưa vào Transformer làm đầu vào. Đầu ra Transformer được chiếu và dự đoán đạo hàm vt.

Có hai cách tiếp cận khác nhau để fine-tune mô hình. Cách thứ nhất là low-rank adaptation (LoRA) Hu et al. (2021), trong đó chúng tôi thêm các adapter LoRA vào phép chiếu đầu vào tuyến tính của mỗi lớp self-attention. Với cách tiếp cận này, chỉ transcript embedding, tham số chiếu, cùng với các tham số adapter LoRA được tối ưu hóa. Cách tiếp cận thứ hai là full fine-tuning, trong đó tất cả các tham số được tối ưu hóa cùng nhau. Liu et al. (2023a) cho thấy LoRA đạt hiệu suất tốt hơn khi fine-tune SpeechFlow trên 960 giờ lời nói, nhưng chúng tôi nghi ngờ rằng full fine-tuning có thể thắng thế khi chúng ta mở rộng dữ liệu fine-tuning.

Ngoài ra, nhiều nghiên cứu trước đây (Le et al., 2023; Wang et al., 2023a) biểu diễn transcript dưới dạng chuỗi phoneme và sử dụng Montreal Forced Aligner có sẵn (McAuliffe et al., 2017) để căn chỉnh dữ liệu huấn luyện. Thay vào đó, chúng tôi biểu diễn transcript bằng các ký tự thô, bao gồm dấu câu và với trường hợp đúng, và sử dụng trình căn chỉnh char-to-unit đa ngôn ngữ SeamlessM4T v2 được trình bày trong Seamless Communication (2023) được điều chỉnh từ RAD-TTS (Shih et al., 2021). Trình căn chỉnh này được huấn luyện trên lượng lớn dữ liệu đa ngôn ngữ và có thể căn chỉnh văn bản thô với lời nói. Có một số lợi ích với việc thay thế này. Đầu tiên, nó tránh được sự cần thiết của phonemizer và tránh truyền lỗi do phonemization không chính xác. Thứ hai, văn bản thô bảo tồn nhiều thông tin hơn văn bản đã phonemize, như casing (ví dụ: tất cả chữ hoa để nhấn mạnh) và dấu câu. Thứ ba, trình căn chỉnh SeamlessM4T v2 mạnh mẽ hơn nhiều so với MFA và có thể xử lý văn bản đa ngôn ngữ/code-switching, cho phép mở rộng dễ dàng hơn đến các hệ thống TTS đa ngôn ngữ và phù hợp hơn để căn chỉnh lời nói khó như mẫu đối thoại và nhiễu.

Theo Le et al. (2023), chúng tôi huấn luyện một mô hình duration flow-matching chỉ với dữ liệu gán nhãn. Nó được chỉ ra trong Le et al. (2023) rằng mô hình duration FM có tính đa dạng tốt hơn so với các mô hình duration regression. Tuy nhiên, nó kém ổn định hơn và đôi khi tạo ra prosody không tự nhiên. Để giảm thiểu vấn đề này, chúng tôi đề xuất lấy trung bình trên một số lượng nhỏ chuỗi duration để ổn định, điều này thực nghiệm cho thấy trade-off tốt hơn giữa tính đa dạng và chất lượng. Phép toán lấy trung bình là hợp lý vì các phân phối duration tương đối unimodal. Khi lấy trung bình nhiều mẫu hơn, nó tiến gần đến trung bình, đó là ước tính được tạo ra bởi các mô hình regression.

5.2 Tác vụ và Đánh giá

Chúng tôi xem xét tác vụ TTS in-context (còn được biết đến là zero-shot TTS). TTS in-context nhằm tổng hợp lời nói giống với phong cách audio của một ví dụ audio đã cho có thể chưa thấy trong quá trình huấn luyện. Phong cách audio không chỉ đề cập đến giọng nói, mà còn mọi thứ khác ngoài transcript, như prosody và điều kiện âm thanh. Để thực hiện tác vụ này, transcript thô/frame-level đầu vào là sự nối của transcript thô/frame-level của ví dụ audio và transcript thô/frame-level mục tiêu, trong khi audio/duration bị che là sự nối của audio/duration ví dụ và một mask cho lời nói/duration cần được tạo ra. Chúng tôi

--- TRANG 9 ---
trước tiên lấy mẫu chuỗi duration cho transcript thô mục tiêu để tạo transcript mục tiêu frame-level bằng mô hình duration, và sau đó lấy mẫy audio với mô hình audio.

Hiệu suất được đo bằng độ tương tự phong cách, tính chính xác nội dung và chất lượng. Một metric tự động proxy cho độ tương tự phong cách là độ tương tự cosine giữa lời nhắc audio và audio được tạo ra trong một không gian embedding phản ánh phong cách audio. WavLM-TDCNN (Chen et al., 2022b) thường được sử dụng cho embedding (Wang et al., 2023a; Kharitonov et al., 2023; Le et al., 2023). Le et al. (2023) ủng hộ việc báo cáo cả độ tương tự đối với audio thô (SIM-orig) và đối với audio được tái tổng hợp từ cùng vocoder (SIM-resyn) để có thể so sánh qua các nghiên cứu (SIM-orig). Tính chính xác nội dung có thể được ước lượng bằng word error rate (WER) từ một mô hình nhận dạng lời nói nào đó; tuy nhiên, WER có thể là kết quả từ cả lỗi tổng hợp và lỗi nhận dạng, và do đó kém tin cậy khi các con số gần nhau hoặc khi phong cách mục tiêu khó nhận dạng hơn (ví dụ: lời nói có giọng, lời nói đối thoại, lời nói nhiễu). Trong bài báo này chúng tôi sử dụng Whisper large-v2 thay vì HuBERT-L Hsu et al. (2021) được sử dụng trong các nghiên cứu trước (Wang et al., 2023a; Le et al., 2023) vì cái sau kém mạnh mẽ hơn và có WER cao hơn trên dữ liệu thực cho các miền không phải audiobook.

Đánh giá chủ quan thường được sử dụng để đánh giá độ tương tự phong cách và chất lượng audio, được đo bằng mean opinion scores (MOS).

5.3 Thiết lập Thực nghiệm

Dữ liệu huấn luyện: Chúng tôi huấn luyện Audiobox Speech trên một tập con tiếng Anh có transcript của dữ liệu lời nói được sử dụng cho pre-training. Tập con chứa 100K giờ lời nói bao gồm các miền tương tự như tập đầy đủ, mà chúng tôi gọi là "SP-multi-100K." Chúng tôi tạo tập con có transcript với các phương pháp tiền xử lý sau:

Đối với các bộ dữ liệu đối thoại đa người nói không được phân đoạn, chúng tôi trước tiên phân đoạn bộ dữ liệu của mình bằng toolkit diarization PyAnnote (Plaquet and Bredin, 2023; Bredin, 2023) để tạo ra các phân đoạn lời nói một người nói. Đối với lời nói không có transcript, chúng tôi transcript dữ liệu bằng hai mô hình nhận dạng lời nói, Whisper Radford et al. (2022) large-v2 và medium.en. Đối với mỗi audio có ngôn ngữ không xác định, chúng tôi bổ sung sử dụng mô hình Whisper large-v2 để nhận dạng ngôn ngữ (LID). Sau đó chúng tôi loại bỏ các utterance trong đó xác suất là tiếng Anh thấp hơn 50% hoặc word error rate (WER) giữa các transcript từ hai mô hình lớn hơn 50%.

Để tạo ra các phân phối văn bản tương tự qua nhiều bộ dữ liệu, chúng tôi áp dụng inverse text normalization để tạo transcript có chữ hoa đúng và dấu câu cho bất kỳ bộ dữ liệu nào có transcript đã chuẩn hóa bằng thư viện Whisper-punctuation.¹ Nó thực hiện tác vụ thông qua tìm kiếm có ràng buộc trong đó transcript được tạo ra cần phù hợp với transcript gốc sau khi chuẩn hóa.

Mô hình và huấn luyện: Chúng tôi áp dụng phương pháp full fine-tuning và huấn luyện mô hình audio trong 200K bước với batch size hiệu quả 240K frame. Các mẫu được cắt ngẫu nhiên nếu vượt quá 1,600 frame. Character embeddings có 128 chiều. Đối với mỗi batch, audio được che hoàn toàn với xác suất 0.3; nếu không thì một chunk liền kề được che trong đó kích thước chunk từ 70% đến 100% của các frame. Cùng optimizer, learning rate, scheduler và gradient clipping như Audiobox SSL được sử dụng.

Mô hình duration có 8 head, 768/2048 embedding/FFN dimensions, 10 lớp, với 40 dimension character embeddings. Nó được huấn luyện trong 600K update với batch size hiệu quả 120K frame. Đối với mỗi batch, duration được che hoàn toàn với xác suất 0.2 và nếu không thì một chunk từ 10% đến 100% độ dài chuỗi được che. Các tham số tối ưu hóa còn lại giống như mô hình audio.

Dữ liệu đánh giá và cấu hình: Đối với TTS in-context, các lời nhắc ba giây được sử dụng theo Wang et al. (2023a). Voicebox sử dụng ba giây cuối của tham chiếu làm lời nhắc, thường chứa một lượng đáng kể im lặng cuối. Thay vào đó, chúng tôi sử dụng ba giây cuối sau khi loại bỏ im lặng cuối dựa trên forced alignment cho tất cả các thí nghiệm trong bài báo này. Duration được ước tính bằng cách lấy trung bình trên năm mẫu và theo (Le et al., 2023) im lặng được dự đoán ở cả hai đầu được cắt tối đa 0.1 giây.

Gói torchdiffeq (Chen, 2018) được sử dụng. Theo mặc định, chúng tôi sử dụng midpoint solver với step size 0.0625, gọi các đạo hàm được đánh giá 32 lần. Khi sử dụng classifier free guidance, mô hình thực hiện 2 lần forward pass mỗi lần đánh giá, dẫn đến tổng cộng 64 lần gọi mô hình. Một guidance weight cho classifier-free guidance (Ho and Salimans, 2022) là 0.7 được áp dụng.

¹https://github.com/jumon/whisper-punctuator

--- TRANG 10 ---
Các mô hình được đánh giá trên năm bộ dữ liệu đại diện cho các miền khác nhau. (1) Librispeech test-clean (LS) (Panayotov et al., 2015): ghi âm audiobook được kịch bản hóa và tương đối sạch. Theo Wang et al. (2023a), chúng tôi chỉ giữ các mẫu từ 4 đến 10 giây để đánh giá so sánh với các nghiên cứu trước. (2) CommonVoice v13.0 English test set (CV) (Ardila et al., 2019): câu được đọc bởi các tình nguyện viên trên toàn thế giới. Nó bao gồm giọng rộng hơn và nhiễu hơn so với Librispeech. (3) Switchboard (SWBD) (Godfrey et al., 1992): một corpus lời nói đối thoại. Chúng tôi đánh giá trên một tập con 611 mẫu từ 8 người nói. (4) Expresso (Nguyen et al., 2023) (Expr) là một bộ dữ liệu lời nói biểu cảm đa người nói bao gồm 7 phong cách nói khác nhau, mà chúng tôi đánh giá trên một tập con 999 mẫu. (5) Một bộ dữ liệu biểu cảm và có giọng nội bộ (Accent): câu đọc với người nói bao gồm phạm vi giọng rộng hơn và 10 cảm xúc. Chúng tôi tạo một tập con 500 mẫu để đánh giá.

5.4 Kết quả Chính

Chúng tôi so sánh Audiobox Speech với một số mô hình tạo lời nói in-context hiện đại. Voicebox, VALL-E, NaturalSpeech 2 (NS2) và YourTTS được huấn luyện trên 60K, 60K, 44K, 600 giờ audiobook tương ứng. UniAudio được huấn luyện trên khoảng 100K giờ audio, trong đó lời nói chiếm 81K giờ và chủ yếu là audiobook. Kết quả được trình bày trong Bảng 1 và 2.

Audiobox Speech đạt mức tốt nhất mới về độ tương tự phong cách (0.745 vs. 0.710 từ UniAudio) trên tập kiểm tra miền audiobook (LS). Quan trọng hơn, Audiobox Speech cải thiện đáng kể Voicebox trên tất cả các miền khác, với cải thiện độ tương tự từ 0.096 đến 0.156. Kết quả cho thấy Audiobox Speech tổng quát hóa tốt hơn nhiều nhờ mở rộng dữ liệu để bao gồm nhiều miền hơn. Các đánh giá chủ quan được trình bày trong Bảng 2 một lần nữa xác nhận rằng Audiobox Speech chuyển giao phong cách tốt hơn đáng kể so với các baseline, và tạo ra audio với chất lượng tốt hơn.

Bảng 1 Độ tương tự phong cách và tính chính xác nội dung TTS in-context. Chúng tôi trích dẫn Yang et al. (2023b) cho kết quả NS2 không có trong bài báo gốc (Shen et al., 2023). WER với ∗ được tính bằng HuBERT-L ASR không thể so sánh với các con số khác.

[Bảng dữ liệu được bảo toàn như trong gốc]

Bảng 2 Đánh giá chủ quan độ tương tự phong cách và chất lượng TTS in-context

[Bảng dữ liệu được bảo toàn như trong gốc]

5.5 Nghiên cứu Ablation

Chúng tôi trình bày các nghiên cứu ablation trong Bảng 3. Để hiểu tác động của việc mở rộng dữ liệu, chúng tôi tạo một tập con chứa 60K giờ lời nói audiobook được gọi là "SP-book-60K", là một tập con của 100K giờ lời nói đa miền mà chúng tôi có (SP-multi-100K).

--- TRANG 11 ---
Chúng tôi trước tiên so sánh hai hàng đầu, khác nhau ở dữ liệu pre-training và đều được fine-tune bằng LoRA. Kết quả cho thấy rằng trong khi WER vẫn tương tự, việc mở rộng dữ liệu pre-training cải thiện đáng kể độ tương tự phong cách, đặc biệt trên các miền không được bao gồm trong dữ liệu fine-tuning (CV, SWBD, Expr, Accent). Mặt khác, việc mở rộng dữ liệu fine-tuning từ SP-book-60K đến SP-multi-100K không cải thiện nhiều về độ tương tự. Điều này có thể do thực tế là dữ liệu pre-training là một tập cha của dữ liệu fine-tuning, và do đó fine-tuning có ít thứ để học về chuyển giao phong cách và tập trung vào việc căn chỉnh transcript với lời nói.

So sánh hàng thứ ba và thứ tư, chúng ta thấy rằng bằng cách fine-tune toàn bộ mô hình, độ tương tự phong cách cải thiện một chút và WER cải thiện đáng kể trên hầu hết các miền (giảm WER tương đối từ 23% đến 43%). Ngoại lệ duy nhất là trên SWBD, là các bản ghi băng hẹp 8kHz có thể ít được đại diện trong dữ liệu fine-tuning. Cuối cùng, chúng tôi so sánh hai hàng cuối và xác nhận rằng việc sử dụng lời nhắc audio không có im lặng dẫn đến cải thiện đáng kể về độ tương tự trên các bộ dữ liệu có xu hướng có im lặng cuối dài (CV, Accent), trong khi vẫn duy trì WER. Điều này là do im lặng không có thông tin để suy luận phong cách mục tiêu.

Bảng 3 Nghiên cứu ablation cho TTS in-context. Dữ liệu PT và FT biểu thị dữ liệu được sử dụng cho pre-training và fine-tuning tương ứng. Phương pháp FT biểu thị việc LoRA hay full fine-tuning (full) được áp dụng. "has sil" biểu thị việc lời nhắc audio có điều kiện có chứa im lặng hay không.

[Bảng dữ liệu được bảo toàn như trong gốc]

6 Audiobox Sound: Tạo và Infilling Text-to-sound Đơn giản

Trong phần này, chúng tôi trình bày Audiobox Sound, một mô hình cho tạo âm thanh tổng quát có hướng dẫn văn bản. Tác vụ này cũng được gọi là tạo text-to-audio (TTA) trong nhiều công trình trước (Liu et al., 2023b; Huang et al., 2023b; Kreuk et al., 2022). Nó nhằm tạo ra audio tổng quát cho trước một mô tả văn bản toàn diện. Trái ngược với tổng hợp text-to-speech, văn bản không thể được căn chỉnh theo frame với audio. Hơn nữa, dữ liệu âm thanh chỉ chiếm một phần nhỏ của toàn bộ dữ liệu huấn luyện. Do đó chúng tôi điều tra việc pre-training audio tổng quát có thể mang lại lợi ích cho việc tạo audio của miền cụ thể hay không, mà chúng tôi lấy tạo âm thanh làm ví dụ. Mặc dù chúng tôi tập trung vào tạo các sự kiện âm thanh, kỹ thuật này có thể áp dụng tương tự cho các lĩnh vực khác (ví dụ: âm nhạc).

Hầu hết các công trình trước Liu et al. (2023b); Ghosal et al. (2023); Liu et al. (2023c); Huang et al. (2023b); Yang et al. (2023c) xây dựng các mô hình diffusion dựa trên một không gian latent có ràng buộc, thường được học thông qua autoencoding. Chiến lược như vậy đã được chỉ ra cải thiện hiệu quả dữ liệu Rombach et al. (2021). Trong công trình này, chúng tôi áp dụng một cách tiếp cận khác, xây dựng trực tiếp mạng flow matching trên biểu diễn latent dựa trên auto-encoding của dạng sóng thô. Phương pháp như vậy đã được khám phá rộng rãi trong không gian mô hình ngôn ngữ Kreuk et al. (2022); Copet et al. (2023); Agostinelli et al. (2023), thường yêu cầu xây dựng một mô hình quy mô tỷ để đạt hiệu suất so sánh với các lựa chọn thay thế đã đề cập ở trên. Ở đây chúng tôi chỉ ra rằng bằng cách tận dụng chiến lược đơn giản như vậy, các mô hình flow matching có thể đạt hiệu suất SOTA trong khi rất hiệu quả (ví dụ: nhỏ hơn >2x so với Kreuk et al. (2022)).

--- TRANG 12 ---
6.1 Phương pháp

Tương tự như tạo lời nói, chúng tôi mô hình hóa phân phối âm thanh có điều kiện văn bản với flow matching. Trái ngược với việc học phoneme encoding từ đầu, chúng tôi sử dụng một text encoder được pre-train để ánh xạ các caption audio thành word embeddings. Do thiếu căn chỉnh giữa audio và text embedding, một lớp cross-attention được áp dụng trong mỗi lớp transformer để cho phép mô hình attend đến toàn bộ chuỗi văn bản trong việc mô hình hóa phân phối gradient, tương tự như Ghosal et al. (2023); Liu et al. (2023b,c); Kreuk et al. (2022).

Khác với các công trình trước trong TTA như AudioLDM (Liu et al., 2023b), AudioLDM2 (Liu et al., 2023c), Tango (Ghosal et al., 2023), chúng tôi không dựa vào một variational auto-encoder có sẵn (Kingma and Welling, 2014) để ánh xạ biểu diễn audio low-level (mel spectrogram) vào một không gian latent và mô hình hóa phân phối trong không gian embedding gốc trực tiếp. Điều này đơn giản hóa kiến trúc mô hình và giảm sự cần thiết của việc giới thiệu các tham số có thể huấn luyện quá mức trong quá trình fine-tuning, do đó bắc cầu khoảng cách giữa pre-training và fine-tuning.

Ngoại trừ các lớp cross-attention, tất cả các tham số còn lại được khởi tạo dựa trên mô hình pre-trained được giới thiệu trong Mục 4. Tương tự như tổng hợp text-to-speech, chiến lược fine-tuning hiệu quả tham số như LoRA Hu et al. (2021) có thể được áp dụng trong tạo text-to-audio. Trong thực tế, chúng tôi quan sát thấy fine-tuning toàn bộ mô hình dẫn đến hiệu suất tốt hơn đáng kể và do đó chọn fine-tune toàn bộ mô hình theo mặc định (xem Mục 6.5).

Fine-tuning nhiều giai đoạn: So với transcript cho tổng hợp text-to-speech, dữ liệu captioning audio chất lượng cao khan hiếm hơn nhiều. Thông thường, các bộ dữ liệu captioning audio công khai bao gồm ít hơn 1000 giờ audio, nhỏ hơn nhiều bậc độ lớn so với các bộ dữ liệu lời nói. Mặt khác, dữ liệu âm thanh quy mô lớn hơn thường chứa các nhãn danh mục nhiễu và có sự chuyển dịch phân phối trong danh mục audio (Kim et al., 2019). Để giảm thiểu vấn đề này, chúng tôi chia quá trình fine-tuning thành hai giai đoạn, dựa trên các mô tả audio chất lượng thấp (ví dụ: tag) và chất lượng cao (ví dụ: caption do con người viết) tương ứng. Trọng số của mô hình đầu tiên được sử dụng để khởi tạo mô hình tiếp theo. Chúng tôi lập luận rằng dữ liệu gán nhãn được sử dụng trong giai đoạn đầu tiên, mặc dù bản chất nhiễu của nó, có ích cho việc học phân phối có điều kiện văn bản (xem Mục 6.5).

6.2 Tác vụ và Đánh giá

Chúng tôi xem xét hai tác vụ tạo âm thanh sau: tạo text-to-sound (TTA) và infilling audio có hướng dẫn văn bản (TAI). Chúng tôi sử dụng tập kiểm tra AudioCaps (Kim et al., 2019), một benchmark chuẩn cho tạo âm thanh (Kreuk et al., 2022; Liu et al., 2023b,c; Yang et al., 2023b; Lee et al., 2023; Ghosal et al., 2023), để đánh giá tất cả các mô hình. Đối với TTA, mô hình được đánh giá bằng Frechet Audio Distance (FAD) chuẩn (Kilgour et al., 2019), Frechet Distance (FD) và KL divergence (KLD) dựa trên pre-trained audio event tagger PANN (Kong et al., 2019), và Inception score (IS) (Salimans et al., 2016). FAD và FD đo độ tương tự mức phân phối giữa các mẫu tham chiếu và các mẫu được tạo ra. KLD là một metric mức instance tính toán sự phân kỳ của posterior sự kiện âm thanh giữa mẫu tham chiếu và mẫu được tạo ra cho một mô tả đã cho. IS đo tính đặc hiệu và độ bao phủ cho một tập mẫu mà không yêu cầu tham chiếu, gán điểm cao hơn nếu posterior instance có entropy thấp và posterior biên có entropy cao. Các metric được triển khai theo toolkit audioldm_eval.² Ngoài ra, chúng tôi tính độ tương tự giữa audio được tạo ra và mô tả văn bản bằng mô hình CLAP Wu et al. (2023)³.

Trong TAI, mô hình được điều kiện hóa trên p% của audio ground-truth làm ngữ cảnh để infill (100−p)% còn lại, ngoài mô tả văn bản của toàn bộ audio. Đặc biệt, p được đặt là 30 và 70% giữa là vùng cần điền. Ngoài các metric cho TTA, chúng tôi đo thêm độ tương tự với audio tham chiếu (CLAP-aa), là độ tương tự cosine giữa các embedding CLAP của audio được tạo ra và tham chiếu.

Ngoài các metric khách quan đã đề cập, chúng tôi cũng tiến hành đánh giá chủ quan để đánh giá hai khía cạnh chính của audio được tạo ra: tính tự nhiên tổng thể (OVL) và sự liên quan đến đầu vào văn bản (REL), tương tự như Kreuk et al. (2022); Liu et al. (2023b). Đối với hai metric này, người đánh giá được yêu cầu đánh giá chất lượng cảm thức và sự phù hợp giữa audio và văn bản của các mẫu audio trong khoảng từ 1 đến 5 tương tự như MOS. Dựa trên giao thức đánh giá Kreuk et al. (2022), đánh giá chủ quan được thực hiện trên 100 tệp được lấy mẫu ngẫu nhiên từ tập kiểm tra AudioCaps. Mỗi mẫu được đánh giá bởi 5 annotator từ dịch vụ annotation chuyên nghiệp. Chúng tôi liệt kê giao diện annotation trong Phụ lục D.

²https://github.com/haoheliu/audioldm_eval
³Chúng tôi sử dụng checkpoint 630k-best của https://github.com/LAION-AI/CLAP

--- TRANG 13 ---
6.3 Thiết lập Thực nghiệm

Dữ liệu: Để huấn luyện Audiobox Sound, chúng tôi sử dụng khoảng 6K giờ dữ liệu audio, trong đó ~150 giờ là audio có caption (SD-cap-150) và phần còn lại chỉ bao gồm các tag audio (SD-tag-6K). Trong quá trình fine-tuning giai đoạn đầu tiên, toàn bộ bộ dữ liệu được sử dụng trong khi chỉ dữ liệu captioning được sử dụng trong giai đoạn thứ hai. Để giải quyết vấn đề ontology của các tag audio, chúng tôi nối các tag của các cấp độ khác nhau làm pseudo-caption của audio. Xem Bảng 4 cho ví dụ mô tả audio trong hai nguồn này.

Bảng 4 Ví dụ về mô tả audio trong các bộ dữ liệu dựa trên tag và dựa trên caption (Lưu ý: hai cột của mỗi hàng không được căn chỉnh.)

[Bảng dữ liệu được bảo toàn như trong gốc]

Chi tiết Triển khai: Chúng tôi sử dụng T5-base (Raffel et al., 2020) để ánh xạ mô tả văn bản thành embeddings. Mỗi lớp cross-attention có 16 head và triển khai của nó giống như các lớp self-attention ngoại trừ việc key và value là text embeddings. Time-step embedding được thêm vào T5 embedding trước khi được attend. Trong giai đoạn đầu tiên, chúng tôi fine-tune mô hình trong 200K update với batch size hiệu quả 720K frame. Trong giai đoạn thứ hai, chúng tôi fine-tune thêm mô hình trong 100K update với batch size hiệu quả 240K frame. Đối với cả hai giai đoạn, learning rate và gradient clipping được đặt lần lượt là 0.0002 và 0.2. Đối với suy luận, chúng tôi sử dụng dopri5 solver với độ dung sai tuyệt đối và tương đối là 10−5 làm tùy chọn mặc định. Trọng số classifier-free guidance được điều chỉnh giữa 0 và 5 và chúng tôi thấy đặt nó thành 1 dẫn đến kết quả tốt nhất. Đối với mỗi lời nhắc văn bản, chúng tôi tạo ra 32 mẫu ngẫu nhiên và chọn mẫu có độ tương tự CLAP cao nhất với lời nhắc văn bản. Đối với infilling audio, audio bị che luôn được giữ để điều kiện hóa và chỉ mô tả văn bản được bỏ tùy chọn cho classifier free guidance.

Baseline: Chúng tôi so sánh Audiobox Sound với các mô hình từ họ AudioLDM2 Liu et al. (2023c) và TANGO Ghosal et al. (2023), đại diện cho các cách tiếp cận SOTA hiện tại cho tạo audio tổng quát Liu et al. (2023c).

6.4 Kết quả Chính

Text-To-Audio: Bảng 5 so sánh mô hình của chúng tôi với các mô hình tạo audio trước đây trong TTA. Audiobox Sound vượt trội nhất quán so với tất cả các công trình trước trong cả đánh giá khách quan và chủ quan với một khoảng cách lớn, mặc dù nó hiệu quả tham số hơn đáng kể. Cũng đáng chú ý là so với nhiều cách tiếp cận được liệt kê trong Bảng 5, dữ liệu huấn luyện âm thanh mà chúng tôi sử dụng cũng ít hơn. Điều này tiếp tục tiết lộ tác động của pre-training miền tổng quát cho tạo âm thanh.

Text-To-Audio Infilling: Bảng 6 cho thấy hiệu suất của Audiobox Sound trên TAI, cũng như so sánh với các công trình trước. Mô hình của chúng tôi vượt trội so với các công trình trước với một khoảng cách lớn trên tác vụ này. So với TAI, chúng tôi nhận thấy một kết quả hỗn hợp theo các metric khác nhau. Đáng chú ý, xu hướng trên FAD và KLD không nhất quán, như trong so sánh giữa TTA và TAI. Điều này có thể liên quan đến độ nhạy của các metric. Mặt khác, độ tương tự giữa việc tạo sinh và tham chiếu tăng đáng kể (CLAP-aa: 0.61→0.77) khi ngữ cảnh được đưa vào mô hình, cho thấy sự cải thiện của tính mạch lạc với audio gốc khi ngữ cảnh được sử dụng.

Hiệu quả suy luận: Ngoài các metric chất lượng, chúng tôi tiếp tục chỉ ra trade-off chất lượng-tốc độ tại thời gian suy luận trong Hình 2. Cụ thể, chúng tôi thay đổi số bước suy luận, tương ứng với step size trong

--- TRANG 14 ---
Bảng 5 Kết quả tạo text-to-audio trên tập đánh giá AudioCaps. Baseline được đánh giá dựa trên các repo chính thức tương ứng. Điểm chủ quan được tính dựa trên khoảng tin cậy 95%.

[Bảng dữ liệu được bảo toàn như trong gốc]

Bảng 6 Kết quả infilling text-to-audio trên tập đánh giá AudioCaps. Baseline được đánh giá dựa trên các repo chính thức tương ứng. Điểm chủ quan được tính dựa trên khoảng tin cậy 95%.

[Bảng dữ liệu được bảo toàn như trong gốc]

ODE solver cho mô hình của chúng tôi và số bước DDIM trong TANGO và AudioLDM2. Audiobox Sound đạt chất lượng cao hơn nhất quán (FAD thấp hơn) với cùng số bước suy luận so với AudioLDM2 và Tango. Điều này ngụ ý hiệu quả tốt hơn của cách tiếp cận flow-matching mà Audiobox dựa trên, như được chứng minh tương tự trong Le et al. (2023).

6.5 Phân tích và Nghiên cứu Ablation

Nghiên cứu Ablation: Ở đây chúng tôi tiến hành nghiên cứu ablation cho thấy tác động của các thành phần khác nhau của Audiobox Sound. Cụ thể, chúng tôi thay đổi các chiến lược huấn luyện sau: huấn luyện chỉ với SD-cap-150, huấn luyện với SD-tag-6K và SD-cap-150, huấn luyện với toàn bộ bộ dữ liệu lời nói, âm nhạc và âm thanh.

Như được trình bày trong Bảng 7, việc sử dụng mô hình pre-trained tổng quát tăng hiệu suất ~20% trong FAD. Mặc dù có sự khác biệt trong tác vụ và miền dữ liệu, việc tạo sinh audio universal là một pretext task có lợi cho tạo text-to-sound. Vì âm nhạc và lời nói chiếm một phần đáng kể trong tập đánh giá của chúng tôi, việc tăng quy mô của hai modalité này trong pre-training cung cấp lợi ích bổ sung. Hơn nữa, fine-tuning hai giai đoạn cũng vượt trội nhất quán so với fine-tuning chỉ với SD-cap-150 bất kể việc sử dụng mô hình pre-trained hay không. Lợi ích chủ yếu được quy cho việc mở rộng dữ liệu huấn luyện trong miền (tức là chỉ âm thanh). Mặc dù các nhãn khác nhau, việc chỉ sử dụng các tag audio vẫn có thể tăng cường việc học ánh xạ giữa mô tả các sự kiện và audio thực tế. Cuối cùng, so sánh hai hàng cuối của Bảng 7 cho thấy reranking với mô hình CLAP là một cách tiếp cận hiệu quả để cải thiện hiệu suất tổng thể trong cả chất lượng audio (FAD: 0.91→0.78) và tính liên quan text-audio (điểm CLAP: 0.60→0.71).

Chiến lược Fine-tuning Chúng tôi so sánh hai chiến lược fine-tuning khác nhau: LoRA vs. full model fine-tuning. Đối với LoRA, chúng tôi thêm các adapter LoRA được mô tả trong Mục 5 vào các lớp self-attention. Trái ngược với full-tuning nơi toàn bộ mô hình được fine-tune, chỉ các adapter và lớp cross-attention sẽ được cập nhật trong quá trình fine-tuning và tất cả các phần còn lại được đông lạnh. Fine-tuning LoRA trung bình kém hơn 15% đến 30% (tương đối) so với đối tác full fine-tuning. Việc kết hợp các lớp cross-attention tạo ra thay đổi kiến trúc lớn cho mô hình, làm tăng sự cần thiết của việc fine-tune toàn bộ mô hình.

--- TRANG 15 ---
Hình 2 Trade-off chất lượng-tốc độ của Audiobox Sound, Tango và AudioLDM2. NFE: Số lần đánh giá hàm.

Bảng 7 Ablation cho tạo âm thanh trên tập đánh giá AudioCaps. Tag: dữ liệu tagging audio, Cap: dữ liệu captioning. Lưu ý kết quả của bảng này dựa trên midpoint solver với step size 1/32 (tương đương 64 NFE) nhằm mục đích tăng tốc suy luận.

[Bảng dữ liệu được bảo toàn như trong gốc]

7 Audiobox: Hướng tới Tạo Audio Universal và Có thể Kiểm soát

Trong các phần trước, chúng tôi đã thảo luận về tạo lời nói và âm thanh một cách độc lập. Phần này trình bày Audiobox, một mô hình duy nhất có thể tạo ra cả lời nói và audio được điều kiện hóa trên mô tả văn bản hoặc ví dụ audio. Fine-tuning mô hình pre-trained của chúng tôi cho tác vụ kết hợp này cho phép hướng dẫn ngôn ngữ tự nhiên kiểm soát các thuộc tính lời nói đầu ra như tuổi cảm nhận, giới tính, chất lượng trên kiểm soát dựa trên ví dụ (ZS-TTS). Hơn nữa, việc huấn luyện trên dữ liệu đa dạng rộng cho phép mô phỏng giọng nói trong các môi trường khác nhau và đi kèm với các sự kiện âm thanh như chim hót, vỗ tay. Chúng tôi tiếp tục hình dung một kịch bản trong đó người dùng muốn restyle ví dụ audio đã cho bằng hướng dẫn ngôn ngữ tự nhiên. Ví dụ: thay đổi phong cách audio để làm cho nó nghe như được ghi trong một nhà thờ lớn. Điều này yêu cầu kiểm soát phong cách giọng nói tách biệt bằng cách sử dụng một utterance bổ sung từ cùng người nói được gọi là voice prompt.

Chúng tôi thiết kế Audiobox để cho phép khả năng tạo lời nói và âm thanh đã thảo luận trước đây trong Mục 5 và 6. Hơn nữa thông qua voice prompt và mô tả chúng tôi cũng hình dung việc chuyển giao phong cách giọng nói đến các cảnh âm thanh phức tạp hơn được kích hoạt thông qua huấn luyện kết hợp. Dưới đây chúng tôi thảo luận chi tiết về mô hình caption lời nói và voice prompt, tạo dữ liệu và thí nghiệm.

--- TRANG 16 ---
7.1 Tạo Dữ liệu

7.1.1 Caption Lời nói

Chúng tôi nhằm bắc cầu khoảng cách giữa các bộ dữ liệu lời nói và âm thanh bằng cách hỗ trợ kiểm soát dựa trên mô tả cho tạo lời nói. Chúng tôi xem xét cả annotation con người và caption được tạo tự động

Caption tự động: Do thiếu bất kỳ bộ dữ liệu nào có mô tả chi tiết cho lời nói, chúng tôi tạo ra các caption lời nói bằng một mô hình ngôn ngữ lớn (LLM) với các tag thuộc tính lời nói được trích xuất bằng metadata hiện có hoặc sử dụng pseudo label bằng các classifier. Chúng tôi trích xuất các thuộc tính sau: (1) tuổi: 4 lớp (2) giới tính: 2 lớp (3) chất lượng audio: 3 lớp (4) cao độ: 3 lớp (5) tốc độ nói: 3 lớp (6) giọng: open-vocabulary (7) cảm xúc: open-vocabulary (8) môi trường: open-vocabulary. Thêm chi tiết có thể tìm thấy trong Phụ lục A.

Cho trước các thuộc tính trên, chúng tôi sử dụng mô hình LLAMA2 7B Touvron et al. (2023) để chuyển đổi chúng thành caption. Để nắm bắt các phong cách viết khác nhau, chúng tôi nhắc mô hình một ngân hàng phong cách bắt chước các nhân vật khác nhau với các mẫu viết ví dụ. Một vài trong số chúng được liệt kê dưới đây:

• Một giọng nam trẻ trưởng thành, truyền tải sự tức giận và thất vọng. Audio, chất lượng bình thường, được ghi trong một không gian nhỏ. Người này nói với giọng Nam Á và tốc độ nói bình thường.

• Anh chàng trẻ này tức giận, audio ổn. Anh ta ở trong một không gian nhỏ nào đó và có giọng Nam Á. Nói với tốc độ bình thường.

• Có anh chàng trẻ này tức giận, audio tạm được. Anh ta ở trong chỗ chật, có giọng Nam Á đó, và nói với tốc độ bình thường.

• Người đàn ông trẻ tức giận. Audio ổn, nơi nhỏ. Giọng từ Nam Á. Nói bình thường.

Để cải thiện thêm phạm vi bao gồm các môi trường và âm thanh nền khác nhau, đối với mỗi utterance, chúng tôi áp dụng một augmentation ngẫu nhiên bằng cách convolve với một room impulse response (RIR) ngẫu nhiên từ một tập các môi trường đã biết và tùy chọn thêm một background noise từ một tập với các tag đã biết.

Chúng tôi cũng tạo ra caption tương ứng với môi trường và background noise được cập nhật bằng mô hình LLAMA2 7B. Khi thêm bất kỳ background noise nào vào utterance, chúng tôi cập nhật chất lượng thành "thấp". Đối với các utterance chỉ áp dụng RIR, chúng tôi cập nhật chất lượng thành "bình thường" nếu chất lượng gốc là "studio". Chúng tôi không áp dụng các utterance có chất lượng audio thấp vì những utterance đó có thể không phù hợp cho RIR augmentation.

Annotation con người: Chúng tôi tạo ra annotation dựa trên con người để thu thập mô tả chi tiết hơn và căn chỉnh tốt hơn hướng tới nhận thức nghe của con người. Chúng tôi chọn một tập con 500 giờ từ SP-multi-100K được mô tả trong Mục 5.5.

Trong hướng dẫn annotation, chúng tôi yêu cầu annotator mô tả các thuộc tính cảm nhận như: giới tính, tuổi, giọng, cảm xúc, môi trường, biến thể âm điệu, tốc độ nói, cao độ, cảm xúc, chất lượng audio, phong cách giọng nói và bất kỳ chi tiết tạp nào từ các utterance lời nói. Ngoài ra chúng tôi cũng thu thập các danh mục cho các thuộc tính. Để đảm bảo chúng tôi nhận được mô tả chất lượng cao, chúng tôi lọc các annotator trong hai giai đoạn. Đầu tiên, chúng tôi giữ các annotator đã gắn nhãn thành công các mẫu vàng đã chọn trước với độ chính xác cao. Chúng tôi bổ sung sử dụng một LLM để tự động đánh giá chất lượng annotation để đảm bảo caption chi tiết chất lượng cao bổ sung cho caption tự động của chúng tôi ở trên. Thêm chi tiết về chất lượng có thể tìm thấy trong Phụ lục B. Đây là một số ví dụ caption được tuyển chọn bởi annotator con người của chúng tôi:

1. Một phụ nữ trẻ với giọng Mỹ nói với giọng cao hơn. Cô ấy nói với tốc độ bình thường với giọng hơi bị bịt. Cô ấy ở ngoài trời trong khu vực thành thị và có thể nghe thấy tiếng xe hơi đi qua phía sau. Cô ấy có âm điệu hạnh phúc và phấn khích hơi du dương. Audio chất lượng kém và có thể nghe thấy tiếng chó sủa ở cuối.

2. Một người đàn ông trung niên với giọng hơi nam tính dường như ở ngoài trời trong môi trường nông thôn hoặc tự nhiên với tiếng ồn nền vừa phải của chim hót. Anh ta dường như trong tâm trạng trung tính khi giới thiệu một ngôi nhà cho một số người. Giọng anh ta khản/thô nói với tốc độ chậm với cao độ giọng trung bình.

--- TRANG 17 ---
7.1.2 Voice Prompts

Chỉ mô tả ngôn ngữ tự nhiên cho phép người dùng kiểm soát phong cách thông qua việc mô tả các thuộc tính như tuổi, giọng, cảm xúc, cao độ và môi trường. Tuy nhiên, một người dùng có thể quan tâm đến việc tổng hợp một phong cách giọng nói cụ thể và trong khi thay đổi các thuộc tính khác như chất lượng, cảm xúc, nền. Điều này yêu cầu kiểm soát tách biệt giữa mẫu giọng nói đầu vào và lời nhắc văn bản ngôn ngữ tự nhiên.

Đối với mỗi utterance mục tiêu, chúng tôi lấy mẫu một utterance bổ sung từ cùng người nói để phục vụ như voice prompt trong quá trình huấn luyện. Voice prompt được chọn sao cho nó khác với utterance mục tiêu trên một hoặc nhiều thuộc tính như cảm xúc, môi trường và tốc độ nói. Điều này nhằm de-correlate mục tiêu và prompt trên mọi thứ ngoại trừ độ tương tự giọng nói. Chúng tôi bổ sung áp dụng một room impulse response ngẫu nhiên và background noise augmentation cho voice prompt để tăng tính mạnh mẽ cũng như de-correlation tiếp theo.

Lưu ý rằng điều này khác với việc truyền audio làm ngữ cảnh audio (zero-shot TTS) nơi chúng tôi mong đợi mô hình sao chép cảm xúc, môi trường và các chi tiết nền khác. Ở đây chúng tôi muốn mô hình chỉ chuyển giao phong cách giọng nói từ prompt và sử dụng mô tả cho các chi tiết khác như môi trường và cảm xúc.

7.2 Phương pháp

Audiobox (Hình 1) điều kiện hóa trên cả transcript và các đặc trưng audio bị che (giống như Audiobox Speech) và caption (giống như Audiobox Sound) cho tạo sinh có điều kiện mô tả. Để thống nhất huấn luyện, đối với đầu vào âm thanh không có transcript, chúng tôi tạo một pseudo-transcript chứa các token "<sound>" mỗi cái có độ dài 1 giây điền vào độ dài của audio. Chúng tôi bổ sung điều kiện hóa trên một utterance khác từ cùng người nói (voice prompt). Như được mô tả trong Mục 7.1.2, voice prompt được chọn theo cách đối kháng để cho phép kiểm soát tách biệt. Đối với audio thiếu prompt, chúng tôi đưa một pseudo voice prompt có độ dài 0.1s được điền bằng số không. Voice prompt được nhúng bởi một Transformer nhẹ. Sau đó chúng tôi nối đầu ra với caption description embedding cho cross-attention. Chúng tôi khởi tạo ngẫu nhiên các tham số cho cross-attention, description projection và character embedding weights. Tất cả các tham số khác được khởi tạo dựa trên Audiobox SSL trong Mục 4. Tương tự như huấn luyện mô hình âm thanh trong Mục 6, chúng tôi sử dụng fine-tuning nhiều giai đoạn như được mô tả tiếp theo.

Fine-tuning nhiều giai đoạn: Ngoại trừ 500 giờ caption lời nói chất lượng cao mà chúng tôi thu thập, phần còn lại của caption lời nói được tạo ra bằng các tag thuộc tính và một LLM. Hơn nữa hầu hết các bộ dữ liệu không cung cấp bất kỳ meta-data nào làm hạn chế thêm chất lượng của các caption. Để giảm thiểu vấn đề này, chúng tôi huấn luyện mô hình của mình trong hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi sử dụng tất cả các caption cho lời nói và audio. Để tránh under-fitting trên việc tạo sinh sự kiện audio, chúng tôi upsample dữ liệu audio sao cho tỷ lệ tổng dữ liệu lời nói và audio tính bằng giờ là khoảng 3:1. Trong giai đoạn thứ hai, chúng tôi khởi tạo mô hình từ trọng số giai đoạn đầu tiên và chỉ huấn luyện trên dữ liệu chất lượng cao bao gồm 500 giờ caption lời nói có annotation và một vài bộ dữ liệu khác với metadata cảm xúc và giọng cho caption LLM phong phú. Chúng tôi lại upsample dữ liệu audio sao cho tỷ lệ tổng dữ liệu lời nói và audio là khoảng 2.6:1.

7.3 Tác vụ và Đánh giá

Trong mô hình Audiobox thống nhất của chúng tôi, mô hình có khả năng thực hiện các tác vụ tạo sinh mới như TTS có hướng dẫn mô tả (transcript + mô tả) và tạo sinh TTS có hướng dẫn mô tả với điều kiện giọng nói bổ sung (transcript + mô tả + voice prompt). Ngoài ra, Audiobox cũng duy trì khả năng tạo sinh từ tất cả các phần trước bao gồm: lấy mẫu lời nói đa dạng (chỉ transcript), zero-shot TTS (transcript + context prompt) (xem Mục 5.2), tạo sinh text-to-sound (TTA) (chỉ mô tả) và infilling có hướng dẫn văn bản (TAI, mô tả + context prompt) (xem Mục 6.2). Trong Phụ lục C, chúng tôi mô tả các tác vụ và đầu vào chi tiết.

Đối với tất cả các tác vụ tạo lời nói, chúng tôi đo WER và độ tương tự của phong cách giọng nói nếu ngữ cảnh hoặc voice prompt được cung cấp. Ngoài ra, đối với bất kỳ tác vụ tạo sinh nào có điều kiện mô tả, chúng tôi đo độ tương tự giữa mô tả và audio được tạo ra bằng độ tương tự cosine giữa CLAP text và audio embedding.

Đối với TTS có hướng dẫn mô tả, ngoài metric khách quan, chúng tôi cũng tiến hành đánh giá chủ quan để đánh giá QMOS và REL. Dưới đây, chúng tôi cung cấp chi tiết về mô hình CLAP được sử dụng cho đánh giá lời nói.

--- TRANG 18 ---
7.3.1 Độ tương tự Joint-CLAP

Về mặt tác vụ, việc tạo ra lời nói được điều kiện hóa trên mô tả văn bản tương tự như tạo sinh âm thanh có hướng dẫn mô tả (TTA). Như thường thấy trong TTA, chúng tôi cũng sử dụng độ tương tự text-to-audio để đo mức độ phù hợp của audio được tạo ra với mô tả. Tuy nhiên, không giống như kịch bản TTA, các mô hình nhúng text-audio kết hợp như CLAP Wu et al. (2023) không thể được áp dụng một cách đơn giản cho miền lời nói. Các mô hình CLAP hiện tại được huấn luyện với mô tả thô ráp về lời nói, như "một người đang nói". Mô hình không thể phân biệt các phong cách nói chi tiết như giọng hoặc cảm xúc. Mặc dù tồn tại các mô hình CLAP công khai được huấn luyện với dữ liệu lời nói, hầu hết chúng được huấn luyện với các cặp (lời nói, transcript) trực giao với mô tả văn bản. Do đó, nhằm mục đích đánh giá các mô hình tạo lời nói có điều kiện mô tả, chúng tôi đề xuất mô hình Joint-CLAP, được thiết kế cho cả đánh giá lời nói và audio dựa trên mô tả.

Huấn luyện Tương tự như CLAP Wu et al. (2023), Joint-CLAP bao gồm một nhánh audio và text, mỗi nhánh chịu trách nhiệm mã hóa dạng sóng audio và các câu ngôn ngữ tự nhiên tương ứng. Cho trước một cặp lời nói-văn bản (xa,xt), nhánh audio và text fa và ft mã hóa nó thành cặp embedding (ea,et): ea=fa(xa), et=ft(xt). Chúng tôi sử dụng cùng contrastive loss để huấn luyện mô hình theo Wu et al. (2023); Radford et al. (2021), trong đó τ là một tham số có thể học.

L=1/(2N) ∑(i=1 to N) (log(exp(ea_i·et_i/τ)/∑(j=1 to N)exp(ea_i·et_j/τ)) + log(exp(et_i·ea_i/τ)/∑(j=1 to N)exp(et_i·ea_j/τ))) (2)

Trong thực tế, chúng tôi sử dụng RoBERTa được pre-train Liu et al. (2019) làm text encoder ft. Trái ngược với CLAP, sử dụng các audio tagger được pretrain (ví dụ: HSTAT Chen et al. (2022a)) để mã hóa audio, ở đây chúng tôi sử dụng WavLM Chen et al. (2022b) làm backbone để mã hóa. Các mô hình lời nói tự giám sát có thể nắm bắt tốt hơn thông tin chi tiết (ví dụ: phong cách nói) so với các classifier audio tổng quát. Cả RoBERTa và WavLM encoder đều được fine-tune trong huấn luyện mô hình.

Dữ liệu Dữ liệu huấn luyện của Speech-CLAP bao gồm SD-tag-6K, SD-cap-150, và 2K giờ bộ dữ liệu lời nói bao gồm cả caption con người và tự động. Tập huấn luyện bao gồm cả dữ liệu lời nói và không phải lời nói để trang bị cho mô hình khả năng phân biệt cho các trường hợp sử dụng lời nói với âm thanh môi trường (ví dụ: một người đàn ông nói trong khi chim hót và chó sủa). Phần lời nói là một tập con của lời nói có caption được mô tả trong Mục 7.1.1, được chọn để cân bằng tỷ lệ caption có annotation con người và được tăng cường LLM.

Mô hình được đánh giá trên các tập đánh giá của tập con âm thanh và lời nói tương ứng.

Chi tiết Triển khai Đối với audio và text encoder, chúng tôi sử dụng WavLM-base+ và RoBERTa base tương ứng. Việc sử dụng các speech encoder thay thế trong cùng họ như WavLM-large mang lại kết quả tương tự. Các audio và text embedding được chuẩn hóa trước khi tính toán loss (Phương trình (2)). Mô hình được huấn luyện bằng optimizer Adam Kingma and Ba (2014) với learning rate 5e−5. Chúng tôi sử dụng 64 volta32 GPU với batch size 75 trên mỗi GPU trong 200K update. Để ổn định huấn luyện, gradient được cắt đến 10 theo norm và độ chính xác floating point thô được sử dụng mà không có bất kỳ quantization nào. Chúng tôi theo dõi recall (A2T@10) trên tập validation ở cuối mỗi epoch và chọn checkpoint mô hình có giá trị cao nhất.

Hiệu suất Truy xuất Chúng tôi so sánh Joint-CLAP với các CLAP gốc được đề xuất bởi Wu et al. (2023), đo hiệu suất truy xuất text-to-audio và audio-to-text. Cụ thể, chúng tôi lấy hai mô hình CLAP công khai được huấn luyện trên audio tổng quát: CLAP (general audio)⁴, và audio tổng quát cộng lời nói: CLAP (w/ speech)⁵. Trên mỗi tác vụ truy xuất, chúng tôi báo cáo recall dưới ba ngưỡng: 1, 5 và 10. Như được trình bày trong Bảng 8, các CLAP công khai, bất kể dữ liệu lời nói có được sử dụng hay không, đạt hiệu suất thấp hơn đáng kể trên truy xuất lời nói dựa trên mô tả văn bản, với sự suy giảm hiệu suất ~30x so với benchmark âm thanh. Điều này có thể do sự mơ hồ tự nhiên lớn hơn trong tác vụ, nơi mô tả lời nói có thể thể hiện phương sai cao hơn. Ví dụ, những người khác nhau có thể có ý kiến khác nhau về điều gì tạo nên nói nhanh so với nói chậm. Mặc dù có sự mơ hồ như vậy, Joint-CLAP vẫn cải thiện đáng kể hiệu suất truy xuất dưới cùng thiết lập (T2A@10 trên lời nói: 2.29→22.01), trong khi duy trì hiệu suất cho audio tổng quát (T2A@10 trên âm thanh: 63.64→67.64). Lợi ích được quy cho việc fine-tuning với các bộ dữ liệu specific lời nói

⁴https://huggingface.co/lukewys/laion_clap/blob/main/630k-best.pt
⁵https://huggingface.co/lukewys/laion_clap/blob/main/music_speech_audioset_epoch_15_esc_89.98.pt

--- TRANG 19 ---
và sử dụng một speech encoder hiệu suất cao. Để ablate thêm tác động này, chúng tôi huấn luyện một mô hình CLAP mà không thay đổi kiến trúc mô hình bằng dữ liệu lời nói trong miền. Hiệu suất truy xuất thấp hơn đáng kể so với Joint-CLAP dựa trên WavLM (ví dụ: T2A@10 trên lời nói: 12.01 vs. 22.01).

Bảng 8 So sánh giữa Speech-CLAP và các mô hình CLAP công khai về hiệu suất truy xuất trong âm thanh và lời nói.

[Bảng dữ liệu được bảo toàn như trong gốc]

Mối tương quan giữa điểm Joint-CLAP và điểm ý kiến con người Trong thực tế, chúng tôi cũng nhận thấy mô hình Joint-CLAP tương quan chặt chẽ hơn với độ tương tự text-audio được cảm nhận bởi con người, trái ngược với mô hình CLAP công khai (xem Hình 3). Cụ thể, chúng tôi lấy sáu mô hình Audiobox có hiệu suất khác nhau và chạy đánh giá chủ quan với các mô hình này trên bốn tập đánh giá. Như được trình bày trong Hình 3, hệ số tương quan Pearson giữa độ tương tự text-audio và điểm REL tăng từ 0.028 đến 0.727 với mô hình joint CLAP, cho thấy rằng điểm độ tương tự text-audio của nó là một metric đáng tin cậy để đánh giá việc tạo lời nói có kiểm soát mô tả.

Hình 3 Mối tương quan giữa độ tương tự text-audio và điểm REL trong các mô hình CLAP khác nhau. r: Hệ số tương quan Pearson.

7.4 Thiết lập Thực nghiệm

Dữ liệu huấn luyện: Chúng tôi huấn luyện Audiobox thống nhất với sự kết hợp của (1) bộ dữ liệu lời nói tiếng Anh (SP-Multi-100K, xem Mục 5.3) với mô tả văn bản bổ sung và voice prompt cho mỗi utterance tương ứng và

--- TRANG 20 ---
(2) bộ dữ liệu âm thanh với mô tả văn bản hoặc tag (SD-TAG-6K và SD-CAP-150, xem Mục 6.3). Trong cả hai trường hợp, mỗi mô tả được tạo ra từ một LLM hoặc được annotation bởi con người. Chúng tôi sử dụng fine-tuning hai giai đoạn để cải thiện độ chính xác và chất lượng mô hình. Trong fine-tuning giai đoạn đầu tiên, chúng tôi kết hợp tất cả bộ dữ liệu lời nói (SP-Multi-100K) và âm thanh (SD-TAG-6K và SD-CAP-150) vào bộ dữ liệu huấn luyện của chúng tôi. Trong fine-tuning giai đoạn thứ hai, chúng tôi sử dụng một tập con của bộ dữ liệu fine-tuning giai đoạn đầu tiên bao gồm bộ dữ liệu chất lượng cao hơn với tổng cộng khoảng 2,310 giờ.

Chi tiết triển khai: Mô hình Audiobox thống nhất nhận bốn đầu vào khác nhau: 1) transcript căn chỉnh frame, 2) mô tả, 3) voice prompts, và 4) context prompt (các đặc trưng audio bị che). Đầu tiên, chúng tôi nhúng chuỗi ký tự đầu vào trong transcript căn chỉnh frame thành các đặc trưng 128 chiều. Chuỗi được nhúng sau đó được chiếu bằng một lớp tuyến tính và được thêm vào các đặc trưng audio bị che đã chiếu làm đầu vào cho Transformer. Tiếp theo, chúng tôi sử dụng T5-base để trích xuất embedding liên tục 512 chiều từ mô tả. Các tham số của T5-base được giữ đông lạnh trong quá trình huấn luyện. Chúng tôi thêm một lớp tuyến tính có thể huấn luyện để chiếu đầu ra từ 512 chiều để phù hợp với chiều embedding Transformer (1024). Đối với voice prompts, trước tiên chúng tôi trích xuất các đặc trưng dense bằng cùng mô hình Encodec được mô tả trong Mục 4. Các đặc trưng này sau đó được đưa vào một mô hình Transformer 3 lớp với 1024 embedding dimensions, 16 attention head, và chiều feed-forward là 4096. Sau đó chúng tôi nối time-step embedding, đầu ra voice prompt encoder, và description embedding tạo thành đầu vào cho cross-attention.

Trong quá trình huấn luyện, chúng tôi ngẫu nhiên bỏ voice prompt, caption và context với các xác suất được chỉ định trong Bảng 9:

Bảng 9 Xác suất drop-out cho context (ctx), voice prompt (vp), và caption (cap). "F" (false) / "T" (true) đề cập đến việc đầu vào có được sử dụng hay không.

[Bảng dữ liệu được bảo toàn như trong gốc]

Các xác suất này được thiết kế với các trường hợp sử dụng cụ thể đã thảo luận trước đây. Lưu ý rằng zero-shot TTS yêu cầu mô hình sao chép mọi thuộc tính từ lời nhắc audio trong khi restylization yêu cầu mô hình duy trì độ tương tự cao của phong cách giọng nói trong khi loại bỏ cảm xúc, môi trường và các thuộc tính khác. Điều này yêu cầu chúng tôi phân biệt ngữ cảnh từ voice prompt.

Việc đặt các xác suất dropout như được định nghĩa trong Bảng 9 dẫn đến các xác suất kết hợp được trình bày trong Bảng 10. Các xác suất kết hợp tương ứng với mỗi trường hợp sử dụng mà mô hình có thể hỗ trợ. Lưu ý rằng pre-training tạo sinh đã điều chỉnh mô hình cho các ứng dụng ZS-TTS và lấy mẫu lời nói đa dạng. Do đó, chúng tôi chọn các hyperparameter để thiên về mô hình hướng tới description-guided TTS có và không có điều kiện giọng nói.

Bảng 10 Các xác suất kết hợp được dẫn xuất cho context, voice prompt, và caption cho các trường hợp sử dụng khác nhau.

[Bảng dữ liệu được bảo toàn như trong gốc]

Trong fine-tuning giai đoạn đầu tiên, chúng tôi fine-tune tất cả tham số trong tối đa 600K update với 32 A100-80GB GPU. Chúng tôi dừng huấn luyện sau 350K bước vì chúng tôi không tìm thấy bất kỳ lợi ích nào trong hiệu suất mô hình sau điều này. Trong giai đoạn thứ hai, chúng tôi fine-tune thêm tham số mô hình với LoRA fine-tuning trên các tham số self-attention với r=64 và các lớp chiếu đầu vào cross attention trong 100K update với 16 A100-80GB GPU.

Đối với mô hình duration Audiobox thống nhất, chúng tôi sử dụng cả transcript và văn bản mô tả làm đầu vào. Chúng tôi sử dụng 12 lớp Transformers decoder với 8 head, 768/2048 embedding/FFN dimensions self-attention và lớp cross-attention để attend description embedding. Chúng tôi sử dụng 40 chiều cho character embedding. Trong quá trình huấn luyện, chúng tôi đặt xác suất bỏ description embedding là 0.3. Mô hình được huấn luyện với 600K update với flow-matching loss với 8 A100-80GB GPU. Để đánh giá, chúng tôi sử dụng checkpoint ở 200K bước.

Dữ liệu đánh giá: Chúng tôi đo hiệu quả của description-guided TTS và description-guided TTS với vocal prompts trên các tập kiểm tra sau. Đầu tiên, chúng tôi annotation một tập 1,946 bản ghi được lấy mẫu từ các nguồn đa dạng, bao gồm LibriTTS (Zen et al., 2019), Common Voice (Ardila et al., 2019), Switchboard (Godfrey et al., 1992), Fisher (Cieri, Christopher, et al., 2004,2005a,,), Spotify (Clifton et al., 2020), AudioSet (Gemmeke et al., 2017), Expresso (Nguyen et al., 2023) để đánh giá khả năng tổng quát hóa. Tập này được ký hiệu là SpCap (SC). Tập thứ hai là AC-filtered (AC-filt) (Lee et al., 2023) với 825 utterance. Nó được xây dựng từ tập kiểm tra AudioCaps bằng cách transcript và giữ các mẫu có transcript ASR đáng tin cậy.

Tập thứ ba là tập kiểm tra Expresso (Expr) với 999 utterance. Cuối cùng, tập thứ tư chứa các utterance từ tập Accent nội bộ. Chúng tôi áp dụng RIR và noise augmentation được lấy mẫu ngẫu nhiên để xây dựng tập này và ký hiệu là "Accent+" (500 utterance). Expr và Accent+ sử dụng caption lời nói được dẫn xuất từ LLM bằng các thuộc tính có sẵn. Đối với Accent+, chúng tôi bổ sung truyền các tag môi trường và background noise cho LLM để kết hợp thông tin vào các caption được tạo ra. Cùng nhau các tập này bao gồm nhiều loại sự kiện âm thanh, cảm xúc, giọng, môi trường và phong cách giọng nói.

Để đánh giá TTS dựa trên mô tả với vocal prompt, chúng tôi sử dụng các bộ dữ liệu Expr và Accent+ và chọn một utterance khác từ cùng người nói. Prompt được chọn sao cho khác với utterance mục tiêu về cảm xúc hoặc phong cách nói (enunciated, whisper, v.v.). Hơn nữa, chúng tôi cũng so sánh với Audiobox Sound và Audiobox Speech trên các ứng dụng lời nói và âm thanh bằng các tập đánh giá được mô tả trong Mục 5 và 6 tương ứng.

Suy luận: Chúng tôi sử dụng mô hình duration được mô tả trong phần này với việc lấy trung bình trên 5 mẫu. Đối với description-guided TTS (có hoặc không có voice prompt), chúng tôi bổ sung lấy mẫu một duration im lặng từ 0 đến 3 giây và pad nó vào cả hai đầu. Chúng tôi thấy điều này tạo ra audio mạch lạc với mô tả đặc biệt khi chúng cũng đề cập đến các sự kiện âm thanh. Ví dụ: một người đàn ông nói và xe hơi đi qua trong khi chó sủa. Tuy nhiên, điều này có thể khiến mô hình tạo ra âm thanh khi không có sự kiện âm thanh nào được mô tả. Để bao gồm tất cả các kịch bản liên quan đến description-guided TTS, chúng tôi tạo ra N=8 mẫu với stochastic silence padding và sau đó xuất mẫu tốt nhất dựa trên clap re-ranking bằng mô hình kết hợp. Chúng tôi sử dụng guidance weight 0.75 cho các ứng dụng description-guided TTS (có/không có voice prompt).

Đối với tạo sinh chỉ âm thanh, chúng tôi luôn tạo ra audio dài 10s với pseudo-transcript bằng guidance weight 1.33. Chúng tôi sử dụng clap reranking với N=16 mẫu bằng mô hình clap âm thanh. Đối với các ứng dụng zero-shot in-context TTS, chúng tôi cắt end-silence tương tự như mô hình Audiobox Speech và sử dụng guidance weight 1.0. Cho rằng ứng dụng này không liên quan đến bất kỳ mô tả nào, chúng tôi không sử dụng clap re-ranking. Trừ khi được chỉ định, cả mô hình Audiobox acoustic và duration đều sử dụng midpoint solver với step size 1/32, gọi hàm được tích phân 64 lần. Khi sử dụng classifier free guidance, mô hình thực hiện 2 lần forward pass, dẫn đến tổng cộng 128 lần gọi forward pass mô hình.

7.5 Kết quả Chính

Trong phần này, chúng tôi điều tra hiệu quả của mô hình Audiobox thống nhất trên một số trường hợp sử dụng. Trước tiên chúng tôi so sánh description-guided TTS có và không có voice prompt trong Bảng 11 và 12 tương ứng. Đối với tác vụ này, chúng tôi so sánh với các mô hình VoiceLDM Lee et al. (2023) và AudioLDM2 Liu et al. (2023c) làm baseline. Tiếp theo, trong Bảng 13 chúng tôi đánh giá mức độ Audiobox thực hiện các tác vụ lời nói so với mô hình chỉ lời nói không có mô tả, Audiobox Speech. Cuối cùng, trong Bảng 14 chúng tôi so sánh với mô hình chỉ âm thanh Audiobox Sound trên tác vụ TTA.

7.5.1 Kiểm soát dựa trên mô tả cho tạo lời nói

Bảng 11 so sánh Audiobox với các mô hình VoiceLDM Lee et al. (2023) và AudioLDM2 Liu et al. (2023c) trên các tác vụ description-guided TTS và description-guided TTS với voice prompt (voice restylization). Chúng tôi thấy rằng Audiobox vượt trội so với cả hai baseline trên tất cả các bộ dữ liệu và metric. Đặc biệt, Audiobox có thể

--- TRANG 21 ---
nhất quán tạo ra audio cho các mô tả phong phú trong SC, các sự kiện nền (AC-filt), audio biểu cảm (Expr), và audio có giọng với nền đa dạng (Accent+).

Bảng 11 Kiểm soát dựa trên mô tả cho tạo lời nói. Audiobox vượt trội so với cả AudioLDM2 và VoiceLDM trên tất cả các bộ dữ liệu và metric. Các mô hình VoiceLDM và AudioLDM2 đặc biệt gặp khó khăn với các bộ dữ liệu Expr và Accent+ với audio biểu cảm.

[Bảng dữ liệu được bảo toàn như trong gốc]

Chúng tôi cũng lưu ý rằng AudioLDM2 và VoiceLDM đặc biệt gặp khó khăn trên các bộ dữ liệu biểu cảm (Expr và Accent+). Đặc biệt, chúng tôi thấy rằng các utterance được tạo ra bởi các mô hình AudioLDM2 và VoiceLDM kém hơn đáng kể so với ground truth đặc biệt trong các kịch bản phức tạp liên quan đến mô tả của cả lời nói, môi trường (cathedral), và âm thanh nền. Điều này dẫn đến điểm số tệ hơn trên bộ dữ liệu Accent+. Hơn nữa, tập kiểm tra Expr chứa các giọng nói khám phá các phong cách biểu cảm như enunciation, whispering, giới tính không nhị phân nơi AudioLDM2 và VoiceLDM gặp khó khăn. Chúng tôi giả thuyết điều này có thể là do chúng là các trường hợp ngoài phân phối w.r.t huấn luyện. Cả mô hình VoiceLDM và AudioLDM2 có xu hướng gặp khó khăn trên các utterance như vậy dẫn đến điểm số thấp trên các metric khách quan.

Các đánh giá chủ quan của chúng tôi cũng căn chỉnh với các metric khách quan nơi chúng tôi thấy mô hình Audiobox vượt trội đáng kể so với các baseline đặc biệt về độ tương tự với mô tả. Điểm số tệ hơn trên bộ dữ liệu Accent+ và Expr cho mô hình AudioLDM2 và VoiceLDM tiếp tục xác nhận quan sát của chúng tôi.

Trong Bảng 12, chúng tôi trình bày kết quả cho description-guided TTS với voice prompt. Các mô hình VoiceLDM và AudioLDM2 không đồng thời hỗ trợ điều kiện hóa dựa trên mô tả giọng nói và văn bản cho một transcript. Hướng tới nỗ lực so sánh tốt nhất của chúng tôi, chúng tôi kết hợp embedding CLAP cho voice prompt audio và mô tả văn bản bằng cách lấy trung bình chúng và sử dụng nó làm đầu vào điều kiện. Chúng tôi thấy rằng Audiobox vượt trội so với cả hai baseline. Chúng tôi cũng nhận thấy rằng trong trường hợp không có voice-prompt, độ tương tự speaker của Audiobox giảm đáng kể vì mô tả không thể nắm bắt tất cả các khía cạnh của giọng nói. Các đánh giá chủ quan căn chỉnh với các metric khách quan cả cho độ tương tự mô tả và audio được tạo ra và độ tương tự speaker. Chúng tôi thấy rằng voice prompt cải thiện đáng kể độ tương tự speaker trong khi phù hợp với các mô tả.

7.5.2 So sánh với Audiobox Speech và Audiobox Sound

Bảng 13 so sánh các mô hình Audiobox thống nhất và chỉ lời nói Audiobox Speech cho zero-shot TTS trên 5 bộ dữ liệu khác nhau. Chúng tôi sử dụng cùng mô hình duration cho cả hai mô hình acoustic cho tác vụ này. Chúng tôi thấy rằng mô hình Audiobox thống nhất cho độ tương tự speaker cao hơn nhưng thực hiện kém một chút về word error rate. Điều này cũng được xác nhận bởi các đánh giá chủ quan nơi chúng tôi thấy chỉ có sự khác biệt nhỏ giữa các mô hình Audiobox và Audiobox Speech.

Trong Bảng 14, chúng tôi trình bày kết quả so sánh Audiobox thống nhất với Audiobox Sound, VoiceLDM, và các mô hình AudioLDM2 trên tác vụ TTA như được mô tả trong Mục 6.2. Chúng tôi thấy rằng Audiobox vượt trội đáng kể so với tất cả baseline đạt hiệu suất hiện đại cho các mô hình kết hợp và thậm chí vượt trội so với các mô hình chỉ âm thanh như TANGO. Audiobox chỉ thực hiện kém so với mô hình Audiobox Sound chuyên về tạo âm thanh. Các đánh giá chủ quan tiếp tục xác nhận rằng cả Audiobox và Audiobox Sound của chúng tôi đều vượt trội so với tất cả các baseline khác với một khoảng cách đáng kể.

--- TRANG 22 ---
Bảng 12 Kiểm soát dựa trên mô tả với điều kiện giọng nói bổ sung cho tạo lời nói
So sánh trên các metric khách quan.

[Bảng dữ liệu được bảo toàn như trong gốc]

Bảng 13 So sánh mô hình Audiobox và Audiobox Speech cho ứng dụng In-context TTS. Cả hai mô hình đều sử dụng cùng mô hình duration dựa trên regression

[Bảng dữ liệu được bảo toàn như trong gốc]

8 Tối ưu hóa Suy luận với Bespoke Solver

Để tạo ra các mẫu từ một mô hình flow-matching, một ODE solver được sử dụng tại thời gian suy luận để ước lượng tích phân. Có nhiều solver mà người ta có thể lựa chọn, như adaptive step-size dopri5 solver hoặc fixed step-size midpoint solver. Các solver này có thể được cấu hình để hoạt động ở các trade-off tốc độ-độ chính xác khác nhau (độ chính xác trong việc tính toán tích phân). Trong khi flow-matching với đường dẫn OT tạo ra các mẫu chất lượng cao hơn so với các mô hình diffusion (Lipman et al., 2023; Le et al., 2023) cho cùng số bước ODE và đạt trade-off tốt hơn, các thiết lập rất aggressive như midpoint chỉ với 4 bước vẫn có thể giảm đáng kể chất lượng mẫu.

Hiệu quả suy luận được định lượng bằng số lần đánh giá hàm (NFE), biểu thị số lần một ODE solver đánh giá đạo hàm. Để cải thiện tốc độ suy luận ở chế độ NFE cực thấp (tức là 4), chúng tôi áp dụng Bespoke Solvers Shaul et al. (2023) để khôi phục chất lượng mẫu tương tự như mô hình gốc với NFE thấp hơn nhiều.

Giả sử mẫu noise ban đầu x(0)=x0∼p(x0). Bespoke solver học các tham số bổ sung θ∈Rp trong đó p là

--- TRANG 23 ---
Bảng 14 So sánh Audiobox thống nhất cho kết quả tạo Text-to-audio trên tập đánh giá AudioCaps. Chúng tôi thấy rằng Audiobox vượt trội so với tất cả baseline ngoại trừ Audiobox Sound chỉ âm thanh. Đáng chú ý nhất là nó thậm chí vượt trội so với mô hình TANGO-full-FT trên hầu hết các metric với khoảng cách đáng kể.

[Bảng dữ liệu được bảo toàn như trong gốc]

rất nhỏ và tối thiểu hóa global truncation error (tổng của local truncation error) giữa mẫu ước lượng xθn và điểm dữ liệu ground truth x(1) trong công thức sau: Ex0∼p(x0)∥x(1)−xθn∥, trong đó xθn là đầu ra của solver step θ.

Ở mức độ cao, Bespoke solver nhằm học biến đổi cho các đường dẫn sao cho biến đổi có thể được ước tính chính xác hơn với số bước ODE mong muốn. Bespoke Solver hoạt động bằng cách biến đổi quỹ đạo mẫu x(t) bằng hai thành phần tr:[0,1]→[0,1] như time reparameterization và hàm khả nghịch φ:[0,1]×Rd→Rd, trong đó các hàm đó được tham số hóa bởi các tham số bổ sung θ. Để parametric solver là stepθ(t,x;ut). Đầu tiên chúng ta biến đổi đầu vào (t,x) thành (r,x̄)=(rt,φrt(x)). Tiếp theo, chúng ta thực hiện một bước trong không gian biến đổi như (rnext,x̄next)=step(r,x̄;ūr), sử dụng base solver đã chọn (ví dụ: midpoint), trong đó ūr là vector field trên quỹ đạo biến đổi. Để biến đổi trở lại không gian gốc, chúng ta tính (tnext,xnext)=stepθ(x,t;ut)=(trnext,φ−1rnext(x̄next)).

Để huấn luyện Bespoke solver, chúng ta tạo ra đường dẫn ground-truth x(t) tại các thời điểm ti trong đó i∈[N] bằng ODE solver chuẩn, và chúng ta tính local truncation error dθi=∥x(ti)−stepθx(ti−1,x(ti−1);u)∥ giữa ground truth và mẫu được dự đoán từ parametrized solver θ, và cuối cùng chúng ta tối thiểu hóa Bespoke loss L(θ)=Ex0∼p(x0)∑ni=1dθi.

Trong bài báo này, chúng tôi tạo ra các đường dẫn ground truth để huấn luyện Bespoke Solvers cho tạo lời nói bằng dopri5 ODE solver để ước tính N=200 bước với guidance weight (GW) là 0.7. Bảng 15 nửa trên cho thấy kết quả đánh giá trên zero-shot TTS với guidance weight phù hợp (0.7) so sánh hai ODE solver chuẩn: midpoint và dopri5 với Bespoke Solver. Như chúng ta có thể thấy, bằng cách sử dụng bespoke solver, chúng ta có thể giảm các bước ODE xuống 4 và vẫn giữ hiệu suất tương tự về độ tương tự phong cách và WER.

Ngoài ra, chúng tôi cũng nghiên cứu xem Bespoke Solver được huấn luyện cho một guidance weight cụ thể có tổng quát hóa đến guidance weight khác hay không, và trình bày so sánh giữa midpoint solver mặc định với bespoke solver sử dụng GW=0.0. Kết quả cho thấy nó có thể tổng quát hóa đến các thiết lập guidance khác nhau.

Bảng 15 So sánh giữa ODE solver chuẩn sử dụng midpoint, dopri5 và parametrized Bespoke solver về NFE, độ tương tự speaker và WER.

[Bảng dữ liệu được bảo toàn như trong gốc]

--- TRANG 24 ---
9 AI Có trách nhiệm

Để xây dựng một hệ thống có trách nhiệm, chúng tôi tiến hành các đánh giá để đo khía cạnh công bằng và nghiên cứu các phương pháp để phòng chống lạm dụng. Trong phần này, trước tiên chúng tôi phân tích xem mô hình của chúng tôi có tạo ra hiệu suất tương tự trên các nhóm khác nhau như giới tính và giọng hay không. Thứ hai, chúng tôi cũng thực hiện các thí nghiệm watermarking để đánh giá xem một hệ thống watermarking được đề xuất gần đây có tổng quát hóa đến các mô hình của chúng tôi sao cho các mẫu có watermark từ các mô hình của chúng tôi có thể được phát hiện một cách đáng tin cậy hay không.

9.1 Công bằng qua các nhóm

Chúng tôi huấn luyện mô hình của mình trên lượng lớn dữ liệu từ các nguồn khác nhau. Chúng tôi tin rằng thông qua việc mở rộng dữ liệu huấn luyện, mô hình của chúng tôi có thể hoạt động tốt qua nhiều nhóm khác nhau. Chúng tôi đánh giá khía cạnh này bằng cách đánh giá hiệu suất mô hình theo giới tính và theo giọng. Đặc biệt, chúng tôi xem xét gender bias hoặc accent bias được quan sát nếu có một nhóm có hiệu suất kém hơn đáng kể về tính chính xác nội dung (được đo bằng WER) và độ tương tự phong cách (được đo bằng độ tương tự cosine giữa các style embedding) so với toàn bộ dân số.

Để tiến hành thí nghiệm của chúng tôi, chúng tôi xem xét tác vụ zero-shot TTS được điều kiện hóa trên một context prompt. Chúng tôi sử dụng một bộ dữ liệu có nhãn quốc gia và giới tính cho thí nghiệm này. Đối với transcript TTS, chúng tôi lấy mẫu 20 transcript từ tập kiểm tra. Đối với các TTS prompt, chúng tôi đánh giá trên các giọng mà có ít nhất 5 người nói duy nhất trong bộ dữ liệu, để lại cho chúng tôi 64 giọng. Sau đó, chúng tôi lấy mẫu 20 utterance ngẫu nhiên (10 cho nam, 10 cho nữ) từ mỗi nhóm giọng. Tổng cộng, chúng tôi có 400 (20 transcript × 20 prompt) cho mỗi nhóm giọng và 12800 (20 transcript × 10 prompt × 64 giọng) cho mỗi nhóm giới tính.

(a) WER qua nhóm giới tính.
(b) Độ tương tự speaker qua nhóm giới tính (mean ± 1 stddev).

Hình 4a cho thấy WER trung bình và Hình 4b cho thấy độ tương tự speaker trung bình qua nhóm giới tính khác nhau. Chúng tôi quan sát rằng các con số rất tương tự và mean độ tương tự speaker rơi vào trong ±1 độ lệch chuẩn. Hình 5a cho thấy WER trung bình và Hình 5b cho thấy độ tương tự speaker trung bình qua nhóm giọng khác nhau. Tương tự với các nhóm giới tính, WER trên tất cả giọng vẫn tương tự và độ tương tự của mỗi nhóm rơi vào trong ±1 độ lệch chuẩn. Qua giới tính và giọng, WER vẫn rất thấp khoảng 1.5% có nghĩa là 1 lỗi cho mỗi 66 từ trong transcript. Chúng tôi kết luận rằng mô hình của chúng tôi không có sự khác biệt hiệu suất đáng kể cho các nhóm giới tính và giọng khác nhau.

9.2 Watermarking cho Phát hiện Audio Được tạo ra

Tiến bộ gần đây về chất lượng và độ chính xác trong mô hình tạo audio đã trao quyền cho các ứng dụng mới và trường hợp sử dụng trên mô hình. Tuy nhiên, đồng thời, có nhiều người có mối quan ngại ngày càng tăng về rủi ro lạm dụng. Do đó, khả năng nhận biết audio nào được tạo ra hoặc thực là rất quan trọng để ngăn chặn việc lạm dụng công nghệ và cho phép các nền tảng nhất định tuân thủ chính sách của họ Fernandez et al. (2023).

Trong phần này, chúng tôi sử dụng Seamless Watermark (Seamless Communication, 2023) để xem chúng tôi có thể đặt và phát hiện một cách đáng tin cậy một watermark không thể cảm nhận được trên audio được tạo ra bởi mô hình của chúng tôi hay không. Mô hình watermarking có các building block tương tự như Encodec Défossez et al. (2022). Các mục tiêu huấn luyện dựa trên sự kết hợp có trọng số của hai loss: 1) perceptual loss để đảm bảo watermark không thể cảm nhận được (Si-SNR và L1 loss), 2) localization loss dựa trên binary cross entropy để đảm bảo phát hiện cục bộ chính xác trên watermark ở mức frame.

--- TRANG 25 ---
(a) WER qua nhóm giọng.
(b) Độ tương tự speaker qua nhóm giọng (mean ± 1 stddev).

Ở đây, chúng tôi sử dụng đầu ra được tạo ra từ hầu hết các kịch bản như zero-shot TTS, description-based TTS, voice+description-based TTS, và tạo âm thanh và áp dụng các data augmentation khác nhau trên chúng. Chúng tôi đo hiệu suất phát hiện watermark bằng false positive rate (FPR) và false negative rate (FNR) của chúng.

Bảng 16 cho thấy FPR và FNR trung bình trên tất cả các tác vụ cho mỗi data augmentation. Chúng tôi quan sát FPR và FNR rất thấp, gần 0%, có nghĩa là watermark hoạt động rất mạnh mẽ chống lại nhiều loại audio và lời nói được tạo ra và data augmentation. Đồng thời, audio có watermark cũng có scale-invariant signal-to-noise ratio (SI-SNR) rất thấp -20.6db, có nghĩa là phần dư watermark không thể cảm nhận được từ góc độ con người.

10 Thảo luận

10.1 Hạn chế

Kiểm soát chi tiết: Với những tiến bộ gần đây trong các mô hình tạo sinh, hiệu suất về khả năng kiểm soát chủ yếu được xác định bởi phạm vi miền và số lượng dữ liệu huấn luyện. Chúng tôi đã chứng minh rằng đối với TTS in-context (kiểm soát dựa trên ví dụ), độ tương tự phong cách có thể được cải thiện đáng kể bằng cách mở rộng dữ liệu được sử dụng cho pre-training tự giám sát, học cách infill audio cho trước ngữ cảnh audio. Ngược lại, kiểm soát dựa trên mô tả yêu cầu mức độ giám sát cao hơn, sử dụng audio và mô tả được ghép đôi để căn chỉnh các khái niệm được mô tả trong văn bản với các biến thể quan sát được trong audio. Do đó, khó tổng quát hóa kiểm soát dựa trên mô tả hơn do sự khan hiếm của dữ liệu gán nhãn bao gồm các khái niệm khác nhau và các khái niệm có độ chi tiết khác nhau.

Để đưa ra một ví dụ cụ thể, dữ liệu huấn luyện của chúng tôi có thể chứa cả trường hợp chó chihuahua sủa và chó labrador sủa; tuy nhiên, tất cả các trường hợp đó có thể được caption là "một con chó sủa." Do đó, khi được nhắc với "một con chó chihuahua sủa," điều tốt nhất mà mô hình có thể làm là tạo ra một clip audio chó sủa nếu text embedding của "chihuahua" và "chó" gần nhau, nhưng nó sẽ không thể tạo ra âm thanh chó chihuahua sủa đúng nếu giám sát như vậy không được cung cấp trong quá trình huấn luyện. Ý tưởng tương tự cũng áp dụng cho các thuộc tính lời nói như giọng, nơi các giọng vùng không thể được tạo ra chính xác nếu bộ dữ liệu huấn luyện không bao gồm các ví dụ được ghép đôi đó.

Tạo dữ liệu: Cho rằng phạm vi bao gồm và số lượng dữ liệu được ghép đôi là chìa khóa để cải thiện kiểm soát dựa trên mô tả, việc xem xét các chiến lược để tạo ra dữ liệu như vậy là tự nhiên. Tuy nhiên, thực tế rất khó tạo ra các mô tả chi tiết cho trước audio. Trong khi dễ dàng cho các annotator phân biệt mèo kêu và chó sủa, việc gắn nhãn loài chó nào chỉ dựa trên audio là nhiệm vụ khó cho hầu hết mọi người. Các thách thức tương tự cũng tồn tại đối với việc gắn nhãn các thuộc tính lời nói như giọng. Hơn nữa, các annotator thường có thể không đồng ý về các thuộc tính như cảm xúc, tuổi cảm nhận và chất lượng audio. Do đó, khó tạo ra các bộ dữ liệu mô tả chi tiết quy mô lớn cho audio. Việc thiếu các bộ dữ liệu lớn như vậy cũng dẫn đến khó khăn trong việc phát triển các attribute tagger và mô hình captioning có thể tự động hóa việc tạo mô tả và được sử dụng để đánh giá.

10.2 Tác động Rộng hơn

Công trình này tiến bộ đáng kể về khả năng kiểm soát cho tạo lời nói và cải thiện phạm vi bao gồm các phong cách. Khả năng tạo ra lời nói với các phong cách giọng nói và âm thanh mong muốn bằng mô tả ngôn ngữ tự nhiên mở khóa vô số ứng dụng. Ví dụ, nó có thể được sử dụng để tạo ra giọng nói mới cho các nhân vật trong audiobook nhập vai, quảng cáo và kịch bản phim, nơi người sáng tạo có trong đầu phong cách giọng nói mà nhân vật nên có. So với kiểm soát dựa trên ví dụ (in-context TTS), kiểm soát dựa trên mô tả có thể tạo ra giọng nói mới mong muốn mà không cần clone từ một cá nhân hiện có và tiết kiệm thời gian người sáng tạo dành cho việc tìm kiếm giọng nói tham chiếu.

Khả năng tạo ra lời nói trong các điều kiện âm thanh đa dạng đặc biệt quan trọng cho các ứng dụng như làm phim và tạo audiobook nhập vai, nơi các nhân vật có thể được trình bày trong các môi trường khác nhau như hang động và việc tạo ra audio phản ánh các thuộc tính âm thanh của những cảnh đó là thiết yếu. Khả năng bảo tồn giọng nói trong khi thay đổi cảm xúc và cảnh âm thanh cũng quan trọng để tạo ra nội dung audio dài như truyện. Nhìn chung, Audiobox làm cho việc tạo ra nội dung với chất lượng cao hơn so với các mô hình trước đây trở nên dễ dàng hơn nhiều cho người sáng tạo.

Mặc dù Audiobox có thể giúp kích thích sự sáng tạo của mọi người và mang lại nhiều tác động xã hội tích cực, tương tự như các mô hình tạo sinh mạnh mẽ khác, nó cũng mang rủi ro bị lạm dụng và gây tác hại không mong muốn. Đặc biệt, tổng hợp lời nói có thể được sử dụng để truyền bá thông tin sai lệch và mạo danh. Chúng tôi đã trình bày các nghiên cứu về watermarking để giảm thiểu hiệu quả rủi ro này theo cách mạnh mẽ. Mặt khác, chúng tôi cũng chứng minh rằng

--- TRANG 26 ---
mô hình hoạt động tương tự tốt qua các biến thể nhóm nhân khẩu học, đảm bảo bias được giảm thông qua việc mở rộng dữ liệu.

Bảng 16 Danh sách kỹ thuật audio augmentation được áp dụng trên audio có watermark với hiệu suất phát hiện tương ứng được lấy trung bình trên tất cả các kịch bản.

[Bảng dữ liệu được bảo toàn như trong gốc]

11 Kết luận

Bài báo này trình bày Audiobox, một mô hình thống nhất cho tạo audio với tính linh hoạt, khả năng kiểm soát và chất lượng chưa từng có. Audiobox có khả năng tạo ra cả lời nói và âm thanh từ mô tả văn bản, ví dụ audio hoặc sự kết hợp của tham chiếu phong cách giọng nói và mô tả. Đặc biệt, đối với tạo lời nói, Audiobox có thể kiểm soát các phong cách giọng nói rất chi tiết như giọng, cảm xúc, timbre và tạo ra lời nói mô phỏng môi trường đa dạng hơn so với các mô hình trước đây. Ngoài việc cho thấy các khả năng mới, Audiobox vượt trội so với tất cả các mô hình tạo lời nói in-context và tạo âm thanh trước đây trên các benchmark được nghiên cứu kỹ lưỡng đánh giá các khả năng hiện có.

Quan trọng hơn, chúng tôi tin rằng công trình này tiên phong trong việc xây dựng các mô hình tạo audio universal với kiểm soát thống nhất và soi sáng cho nghiên cứu tương lai về mô hình hóa tạo audio. Về bản chất, chúng tôi chứng minh rằng với lượng lớn dữ liệu, có thể xây dựng một mô hình thống nhất vượt trội so với các mô hình specific modalité. Điều này chỉ ra con đường tương tự như sự tiến hóa của các mô hình tạo ngôn ngữ, nơi một mô hình quy mô lớn được huấn luyện với một mục tiêu đơn giản trên lượng lớn dữ liệu cuối cùng vượt qua các mô hình specific tác vụ hoặc ngôn ngữ với khả năng tổng quát hóa tốt hơn đáng kể và các khả năng nổi lên.

Lời cảm ơn

Các tác giả muốn cảm ơn Ricky Chen, Hady Elsahar, Ilia Kulikov, Hirofumi Inaguma, Jing Xu, và Yossi Adi, Alexander H. Liu, Chung-Ming Chien, Qing He, Thilo Koehler, Fuchun Peng, Xiaohui Zhang, Vimal Manohar, Po-Wei Chou, Kaustubh Kalgaonkar, Anurag Kumar, Yangyang Shi, Zhaoheng Ni, Gael Le Lan, và Varun Nagaraja cho cuộc thảo luận hữu ích về nghiên cứu, cảm ơn Juan Pino, Ian Stewart, Alexander Miller, và Joelle Pineau cho hỗ trợ tổ chức, cảm ơn Adina Williams, Christophe Ropers, Chloe Bakalar, Imanol Arrieta Ibarra, và Esteban Arcaute cho thảo luận về AI có trách nhiệm, cảm ơn Wei Zhu, Yichen Wang, Jiun-Ren Lin, Chao Zhou, Peter Weng, Stephen Fink, Ivan Evtimov, David Renardy, Sonia Kim cho triển khai AI có trách nhiệm và an toàn, cảm ơn Neil Seejoor, Somya Jain, Chandan Avdhut, Chris Henry, và KC Braunschweig cho hỗ trợ cơ sở hạ tầng, cảm ơn Carolyn Krol, Ernest Hammond, Mo Metanat, David Soofian, Ndidi Elue, Mallika Malhotra, Kelechi Ebi Kamanu, Maeve Ryan, Harrison Rudolph, Jennifer Okafor cho hỗ trợ đánh giá nghiên cứu và grant, cảm ọn Ana Paula Kirschner Mofarrej, Lydia Baillergeau, Steph Miles, Raghu Nayani, Michelle Restrepo, Tamara Piksa, Chris Wiltz, Orialis Valentin, Aiman Farooq, Gopika Jhala và Ashley Gabriel cho hỗ trợ đa chức năng

--- TRANG 28 ---
Đóng góp

Apoorv Vyas đề xuất và triển khai LLM caption, audio augmentation và pipeline kiểm soát chất lượng annotation, và triển khai voice prompting

Bowen Shi dẫn dắt các thí nghiệm Audiobox-Sound, triển khai và tiến hành thí nghiệm cho Joint-CLAP, đề xuất fine-tuning hai giai đoạn và dẫn dắt các nghiên cứu về đánh giá

Matthew Le triển khai và tiến hành thí nghiệm cho Audiobox-SSL, Audiobox-Speech, và Bespoke Solver, dẫn dắt tích hợp mô hình vào demo

Andros Tjandra triển khai speech attribute labeler và các nghiên cứu AI có trách nhiệm

Yi-Chiao Wu tạo ra kết quả baseline Audiobox và triển khai audio infilling cho baseline

Liang Tan khám phá biểu diễn lời nói và tiến hành thí nghiệm sơ bộ về forced aligner

Bowen Shi và Wei-Ning Hsu chuẩn bị dữ liệu âm thanh và triển khai, đề xuất Joint-CLAP và tiến hành thí nghiệm cho Audiobox-Sound

Andros Tjandra và Apoorv Vyas triển khai Audiobox

Andros Tjandra và Matthew Le tiến hành thí nghiệm cho các mô hình duration

Apoorv Vyas, Andros Tjandra và Bowen Shi lặp lại trên LLM prompting cho huấn luyện text-to-speech và âm thanh

Apoorv Vyas, Andros Tjandra, Matthew Le, Bowen Shi, Liang Tan và Wei-Ning Hsu chuẩn bị dữ liệu lời nói

Apoorv Vyas, Andros Tjandra, Matthew Le và Bowen Shi tiến hành thí nghiệm Audiobox

Wei-Ning Hsu, Bowen Shi, Apoorv Vyas, Andros Tjandra, Matthew Le viết bài báo

Baishan Guo, Apoorv Vyas và Andros Tjandra triển khai pipeline annotation con người

Baishan Guo chạy annotation con người và đánh giá chủ quan, và phân tích kết quả annotation và đánh giá

Bapi Akula khám phá tiền xử lý và biến đổi audio, hỗ trợ trong phát triển pipeline dữ liệu

Carleigh Wood điều phối và tạo điều kiện cho annotation dữ liệu

Jiemin Zhang dẫn dắt phát triển demo, thiết kế và triển khai demo infra, tích hợp mô hình, giảm thiểu demo sớm và kiểm tra năng lực

Xinyue Zhang thiết kế và triển khai demo backend, data logging, xác minh giảm thiểu và lọc nội dung độc hại.

Robbie Adkins thiết kế và triển khai demo frontend và hỗ trợ triển khai backend.

Akinniyi Akinyemi tiến hành triển khai demo, thiết lập demo và infra giảm thiểu.

Joshua Lane triển khai cấu trúc UI sớm.

William Ngan thiết kế trải nghiệm demo và triển khai giao diện demo front-end.

Brian Ellis tạo mẫu các khái niệm demo và tạo audio cho demo

Alice Rakotoarison, Chris Summers tiến hành nghiên cứu trải nghiệm người dùng demo

Yael Yungster cung cấp hỗ trợ quản lý thiết kế

Jeff Wang cung cấp hỗ trợ quản lý sản phẩm cho nhóm, đóng góp vào tầm nhìn nghiên cứu tổng thể, chiến lược, cột mốc dự án và thực thi.

Ivan Cruz cung cấp hỗ trợ quản lý chương trình kỹ thuật, điều phối nghiên cứu AI có trách nhiệm, red teaming, và hỗ trợ đa chức năng

Rashel Moritz cung cấp hỗ trợ quản lý chương trình, đóng góp vào lập kế hoạch dự án sớm, lập kế hoạch giảm thiểu, đánh giá, và hỗ trợ đa chức năng

--- TRANG 29 ---
Mary Williamson cung cấp hỗ trợ quản lý cho nhóm và đồng dẫn dắt dự án, đóng góp vào tầm nhìn nghiên cứu, và giám sát demo

Wei-Ning Hsu thiết kế và dẫn dắt dự án, tư vấn cho Apoorv, Bowen, Matthew, Andros, Yi-Chiao, và Liang về nghiên cứu, và điều phối các luồng nghiên cứu, demo và dữ liệu.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo được bảo toàn như trong gốc]

--- TRANG 35 ---
A Thuộc tính Lời nói cho Tạo Caption Lời nói

Như được mô tả trong Mục 7.1.1, chúng tôi trích xuất các thuộc tính để tạo caption lời nói. Chúng tôi thu thập các thuộc tính lời nói từ metadata liên quan hoặc bằng pseudo-labeling cho một tập con các thuộc tính có thể được gắn nhãn đáng tin cậy hơn. Chi tiết cho mỗi thuộc tính được liệt kê dưới đây

• Tuổi: Trước tiên chúng tôi chia tuổi thành 4 danh mục khác nhau là dưới hai mươi (<20), thanh niên (20-35), trung niên (40-60), và người già (>60). Sau đó chúng tôi fine-tune bộ dữ liệu của mình từ checkpoint WavLM-base được pre-train với 3200 giờ lời nói và metadata tuổi từ tập huấn luyện của chúng tôi (bao gồm lời nói đối thoại và đọc với chất lượng khác nhau).

• Giới tính: Chúng tôi fine-tune trên checkpoint WavLM-base với 4300 giờ lời nói và metadata giới tính từ tập huấn luyện của chúng tôi (bao gồm lời nói đối thoại và đọc với chất lượng khác nhau).

• Chất lượng Audio: Chúng tôi sử dụng thư viện TorchAudio-Squim Kumar et al. (2023) và trích xuất điểm Perceptual Evaluation of Speech Quality (PESQ) Rix et al. (2001). Sau đó chúng tôi chia điểm thành ba danh mục: Chất lượng thấp (0-2.39), Chất lượng bình thường (2.39-3.8) và Chất lượng Studio (>3.8).

• Cao độ: Chúng tôi sử dụng PyWorld vocoder⁶ để trích xuất tần số cơ bản (f0) và sau đó tính trung bình hình học qua tất cả vùng có giọng. Chúng tôi sử dụng ngưỡng phụ thuộc giới tính để chia cao độ thành ba danh mục khác nhau: thấp, bình thường, cao. Đối với giới tính nam, chúng tôi đặt cao độ thấp (0-40 percentile), cao độ bình thường (40-90 percentile) và cao độ cao (>90 percentile). Đối với giới tính nữ, chúng tôi đặt cao độ thấp (percentile 0-10), cao độ bình thường (10-60 percentile) và cao độ cao (>60 percentile). Logic đằng sau ngưỡng bất đối xứng là vì nói chung mọi người sẽ cảm nhận hầu hết giọng nam có cao độ thấp hơn và hầu hết giọng nữ có cao độ cao hơn.

• Tốc độ nói: Cho trước transcript và audio, trước tiên chúng tôi áp dụng VAD để loại bỏ các phân đoạn im lặng. Sau đó chúng tôi tính ký tự trên giây (CPS) và chia chúng thành 3 danh mục: chậm (<9.2 CPS), cao (>20.8 CPS) và bình thường (9.2 <= x <= 20.8 CPS).

• Giọng: Chúng tôi sử dụng giọng từ metadata khi có sẵn trong metadata, nếu không để trống.

• Cảm xúc: Chúng tôi sử dụng nhãn cảm xúc khi có sẵn trong metadata, nếu không chúng tôi để trống.

• Môi trường: Chúng tôi sử dụng các tag môi trường như trong phòng, ngoài trời khi có sẵn từ các bộ dữ liệu.

B Caption Tự động: Chất lượng

Để đảm bảo chúng tôi nhận được mô tả chất lượng cao, chúng tôi triển khai cách tiếp cận hai giai đoạn để lọc ứng viên annotator. Đầu tiên, chúng tôi chỉ giữ các annotator đã gắn nhãn thành công các mẫu vàng đã chọn trước với độ chính xác cao (> 73%). Sau đó, chúng tôi chấm điểm các caption đã nộp của họ bằng LLM và giữ annotator nếu điểm trung bình của họ trên một ngưỡng nhất định. Cụ thể hơn, đối với một phân đoạn lời nói, trước tiên chúng tôi sử dụng một LLM để tạo ra một caption dựa trên các thuộc tính audio đã annotation. Sau đó chúng tôi chạy một giai đoạn thứ hai trong đó chúng tôi yêu cầu một LLM khác so sánh caption do LLM tạo ra với caption do con người viết và đánh giá chất lượng của caption do con người viết từ 1 đến 5. Chúng tôi nhắc LLM này cho điểm thấp đối với caption do con người viết trong đó không có sự kiện audio thú vị nào được thêm vào bổ sung cho các thuộc tính audio đã annotation hoặc một số thuộc tính audio quan trọng bị thiếu. Các annotator có điểm caption trung bình dưới 3 đã bị loại bỏ. Điều này dẫn đến caption chất lượng cao và chi tiết bổ sung cho caption pseudo-labeled của chúng tôi ở trên. Đây là một số ví dụ caption được tuyển chọn bởi annotator con người của chúng tôi:

C Mô tả Tác vụ Audiobox Thống nhất

Dưới đây chúng tôi mô tả các tác vụ khác nhau mà mô hình Audiobox thống nhất có thể giải quyết cùng với các đầu vào yêu cầu.

⁶https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder

--- TRANG 36 ---
• Zero-shot TTS (in-context TTS): mô hình nhận làm đầu vào một transcript và một ví dụ audio và tạo ra lời nói giống với phong cách audio của ví dụ (được mô tả trong Mục 5.2). Đầu vào: (context, transcript).

• Description-TTS/TTA: mô hình nhận làm đầu vào một transcript/pseudo-transcript và một mô tả văn bản và tạo ra lời nói / audio phù hợp với mô tả. Đầu vào: (description, transcript/pseudo-transcript)

• Voice Restylization: mô hình nhận một transcript, một voice prompt và một mô tả. Đầu ra được tạo ra cần phù hợp với phong cách giọng nói của speaker và mô tả. Lưu ý rằng mô tả có thể chứa các thuộc tính khác với voice prompt. Đối với voice prompt có thể đã được ghi trong một phòng nhỏ với cảm xúc trung tính và mô tả có thể chỉ định cảm xúc vui vẻ trong một nhà thờ với tiếng chuông. Đầu vào: (voice, description, transcript)

• Sampling: Mô hình nhận một transcript làm đầu vào và lấy mẫu các giọng nói đa dạng. Đầu vào: (transcript)

• Speech Infilling/Editing: mô hình nhận làm đầu vào một lời nói bị che với transcript đi kèm và một mô tả tùy chọn và infill phần bị che. Đầu vào: (context, transcript, mô tả tùy chọn)

• Audio Infilling/Editing: mô hình nhận làm đầu vào một audio bị che với pseudo transcript và mô tả để infill phần bị che với mô tả phù hợp. Đầu vào: (context, pseudo transcript, description)

D Giao diện Đánh giá Chủ quan

Chúng tôi hiển thị giao diện annotation con người cho âm thanh trong Hình D1 và D2, cho lời nói trong Hình D3 đến D5.

--- TRANG 37 ---
Hình D1 Đánh giá OVL cho âm thanh

--- TRANG 38 ---
Hình D2 Đánh giá REL cho âm thanh

--- TRANG 39 ---
Hình D3 Đánh giá Quality MOS cho lời nói

--- TRANG 40 ---
Hình D4 Đánh giá Similarity MOS cho lời nói

--- TRANG 41 ---
Hình D5 Đánh giá REL cho lời nói
