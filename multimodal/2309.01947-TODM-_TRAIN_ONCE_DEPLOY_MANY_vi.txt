# 2309.01947.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.01947.pdf
# Kích thước tệp: 865245 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TODM: HUẤN LUYỆN MỘT LẦN TRIỂN KHAI NHIỀU LẦN
NÉN RNN-T DỰA TRÊN SUPERNET HIỆU QUẢ CHO CÁC MÔ HÌNH ASR TRÊN THIẾT BỊ
Yuan Shangguan, Haichuan Yang, Danni Li∗, Chunyang Wu, Yassir Fathullah†, Dilin Wang
Ayushi Dalmia, Raghuraman Krishnamoorthi, Ozlem Kalinli
Junteng Jia, Jay Mahadeokar, Xin Lei, Mike Seltzer, Vikas Chandra
Meta AI
TÓM TẮT
Các mô hình Nhận dạng Giọng nói Tự động (ASR) cần được tối ưu hóa cho phần cứng cụ thể trước khi có thể triển khai trên thiết bị. Điều này có thể được thực hiện bằng cách điều chỉnh các siêu tham số của mô hình hoặc khám phá các biến thể trong kiến trúc của nó. Việc huấn luyện lại và xác thực lại các mô hình sau khi thực hiện những thay đổi này có thể là một nhiệm vụ tốn nhiều tài nguyên. Bài báo này giới thiệu TODM (Huấn luyện Một lần Triển khai Nhiều lần), một phương pháp mới để huấn luyện hiệu quả nhiều kích thước mô hình ASR trên thiết bị thân thiện với phần cứng với số giờ GPU tương đương với một công việc huấn luyện đơn lẻ. TODM tận dụng các hiểu biết từ công trình trước về Supernet, trong đó các mô hình Recurrent Neural Network Transducer (RNN-T) chia sẻ trọng số trong một Supernet. Nó giảm kích thước và độ rộng lớp của Supernet để có được các mạng con, làm cho chúng trở thành các mô hình nhỏ hơn phù hợp cho tất cả các loại phần cứng. Chúng tôi giới thiệu một sự kết hợp mới của ba kỹ thuật để cải thiện kết quả của TODM Supernet: dropout thích ứng, một kỹ thuật chưng cất tri thức Alpha-divergence tại chỗ, và việc sử dụng bộ tối ưu hóa ScaledAdam. Chúng tôi xác thực phương pháp của mình bằng cách so sánh việc huấn luyện Supernet với các mô hình Multi-Head State Space Model (MH-SSM) RNN-T được điều chỉnh riêng lẻ sử dụng LibriSpeech. Kết quả cho thấy rằng TODM Supernet của chúng tôi có hiệu suất tương đương hoặc vượt trội hơn các mô hình được điều chỉnh thủ công lên đến 3% tương đối tốt hơn về tỷ lệ lỗi từ (WER), trong khi hiệu quả duy trì chi phí huấn luyện nhiều mô hình ở mức hằng số nhỏ.
Thuật ngữ chỉ mục —Supernet, RNN-T, trên thiết bị, hiệu quả, nén, chưng cất tri thức

1. GIỚI THIỆU
Nhận dạng giọng nói tự động (ASR) toàn thần kinh đầu cuối (E2E) đã thu hút sự chú ý về tính tương thích với các thiết bị biên [1, 2, 3]. Recurrent neural network transducer (RNN-T) là một trong những kiến trúc phổ biến nhất cho ASR E2E trên thiết bị [4, 5, 6, 7]. Các ứng dụng hiện đại của RNN-T ASR chạy trên nhiều loại phần cứng khác nhau, bao gồm các bộ xử lý trung tâm (CPU) trên điện thoại, các bộ xử lý tensor (TPU) [8], và các bộ tăng tốc thần kinh khác. Quá trình tối ưu hóa các mô hình ASR trên thiết bị một cách hiệu quả cho từng cấu hình phần cứng riêng biệt đòi hỏi việc tinh chỉnh các siêu tham số huấn luyện mô hình và hợp lý hóa kiến trúc của mô hình [9]. Điều này đòi hỏi một lượng tài nguyên tính toán đáng kể, chủ yếu dưới dạng giờ GPU hoặc chi phí năng lượng cụm GPU tính bằng MegaWatt-giờ (MWh). Các nhà nghiên cứu phải cân bằng giữa độ chính xác và kích thước trong khi quản lý hiệu quả các tài nguyên huấn luyện.

Một phương pháp tiềm năng là tận dụng Supernet [10, 11, 12, 13, 14]. Supernet là một đồ thị mạng thần kinh chia sẻ trọng số có thể tạo ra các mạng con nhỏ hơn được tùy chỉnh cho các ứng dụng cụ thể. Công trình trước đây đã cố gắng tạo điều kiện cho việc huấn luyện đồng thời một số cố định các mô hình bằng cách thực hiện khả năng chia sẻ trọng số. Các ví dụ bao gồm công trình về các bộ mã hóa tầng RNN-T [8, 15]. Tuy nhiên, những nỗ lực này thường chỉ liên quan đến một số lượng hạn chế các mạng, thường khoảng ba, chỉ chia sẻ các phần của bộ mã hóa RNN-T. Mục tiêu chính của chúng là hợp lý hóa việc quản lý cho một vài mô hình, trong khi sử dụng các bộ giải mã mô hình riêng biệt để cải thiện hiệu suất mô hình. Một số cũng kết hợp các bộ mã hóa không nhân quả và nhân quả để nâng cao độ chính xác mô hình. Việc huấn luyện các hệ thống bộ mã hóa tầng như vậy tiêu tốn nhiều tài nguyên hơn so với một nhiệm vụ huấn luyện mô hình đơn lẻ, và không thể mở rộng quy mô cho một số lượng lớn các mô hình có kích thước khác nhau. Ngoài các bộ mã hóa tầng, công trình trước đây của chúng tôi đã khám phá khái niệm Supernet trong khung DNN Omni-Sparsity [16], trong đó một số lượng lớn các mạng con ASR có thể được tạo ra từ một công việc huấn luyện Supernet. Hơn nữa, đã có những nỗ lực mở rộng việc sử dụng Omni-Sparsity DNN để huấn luyện các mô hình ASR trong khi phù hợp với các ràng buộc độ trễ khác nhau cho các mạng con. Điều này được chứng minh bằng cách huấn luyện một Supernet dày đặc không streaming với các mạng con streaming thưa thớt trong [17]. Tuy nhiên, sự phụ thuộc của các DNN Omni-Sparsity vào tính thưa thớt có cấu trúc làm hạn chế không cần thiết khả năng tương thích phần cứng của chúng.

Bài báo này giới thiệu Train Once Deploy Many (TODM), một phương pháp để huấn luyện hiệu quả nhiều mô hình ASR trên thiết bị thân thiện với phần cứng với số giờ GPU tương đương với một công việc huấn luyện đơn lẻ. TODM tận dụng các hiểu biết từ công trình trước về Omni-sparsity DNN Supernet, nhưng không dựa vào tính thưa thớt. Thay vào đó, nó loại bỏ các lớp và giảm độ rộng lớp trong Supernet để có được các mạng con được tối ưu hóa cho tất cả các loại phần cứng. Chúng tôi giải thích TODM Supernet trong Phần 2. Chúng tôi cải thiện kết quả huấn luyện của nó bằng cách giới thiệu dropout thích ứng, một cơ chế chưng cất tri thức Alpha-divergence lấy mẫu logit tại chỗ, và bộ tối ưu hóa ScaledAdam (Phần 3). Chúng tôi xác thực phương pháp của mình bằng cách tạo ra các mô hình con được huấn luyện Supernet sử dụng Evolutionary Search (được mô tả trong Phần 2.2) trên tập validation và so sánh hiệu suất của chúng với các mô hình ASR được tối ưu hóa riêng lẻ sử dụng LibriSpeech [18] trong Phần 4. Chúng tôi thảo luận về kết quả và tại sao Supernet có thể hoạt động hiệu quả trong Phần 5. Công trình này là công trình đầu tiên sử dụng huấn luyện Supernet để tạo ra một cách hiệu quả nhiều mô hình ASR trên thiết bị. Với tài nguyên huấn luyện tương tự như một mô hình, chúng ta có thể triển khai nhiều mô hình có kích thước khác nhau, mỗi mô hình có chất lượng tối ưu ở kích thước mong muốn.

2. NÉN RNN-T SUPERNET HIỆU QUẢ
Trong phần này, chúng tôi mô tả cách một Supernet RNN-T tạo ra các RNN-T mạng con có kích thước động bằng cách thay đổi độ rộng và độ cao của các bộ mã hóa. Sau đó nó sử dụng evolutionary search [16] để khám phá kiến trúc bộ mã hóa tốt nhất của RNN-T tại các ràng buộc kích thước khác nhau.

--- TRANG 2 ---
(a)
(b)
Hình 1 : (a) Lớp MH-SSM: trong quá trình huấn luyện Supernet, các mạng con được tạo ra bằng cách giảm kích thước kênh đầu ra của các mô-đun FFN (đường chấm) hoặc loại bỏ toàn bộ lớp. (b) Giảm kênh và lớp bộ mã hóa của một mạng con Pareto 49.6MB cho mô hình F. xanh dương: các kênh FFN còn lại, xám: các lớp và kênh bị giảm.

2.1. Giảm Lớp và Độ Rộng Có Ràng Buộc
Chúng tôi thực nghiệm với các mô hình ASR dựa trên RNN-T [5]. Huấn luyện Supernet độc lập với loại mô hình chúng tôi chọn, và có thể mở rộng một cách tầm thường cho các bộ mã hóa ASR dựa trên CTC hoặc các mô hình sequence-to-sequence E2E. Thiết kế không gian tìm kiếm mạng con của chúng tôi là then chốt cho sự thành công của huấn luyện Supernet. Thông qua khám phá thực nghiệm, chúng tôi phác thảo ba nguyên tắc chỉ đạo để thiết kế không gian tìm kiếm kiến trúc cho kết quả Supernet tốt nhất; không gian tìm kiếm nên:
1. đủ lớn để tìm thấy các mô hình tốt với bất kỳ kích thước mong muốn nào;
2. đủ linh hoạt để khám phá các kiến trúc mới;
3. nhận thức kiến trúc, tập trung vào sự dư thừa tham số chính trong mô hình.

Trong RNN-T, bộ mã hóa thường chiếm phần lớn các tham số. Ví dụ, trong mô hình baseline của chúng tôi, bộ mã hóa chiếm 86.9% tham số, tiếp theo là predictor (8.9%) và joiner (4.2%). Do đó, chúng tôi chỉ sử dụng Supernet để tìm các mạng con của các bộ mã hóa, trong khi giữ nguyên joiner và decoder.

Cho Θ là các tham số mô hình Supernet, và Ln∈Zn_L biểu thị danh sách các lớp trong bộ mã hóa RNN-T; bộ mã hóa Supernet RNN-T có tối đa n lớp. Chúng tôi định nghĩa độ rộng của mô-đun i, thường đề cập đến kích thước kênh đầu ra của các mạng feed-forward hoặc các mô-đun projection tuyến tính, là Ci∈ZC_mi, trong đó lớp này có kích thước đầu ra tối đa là mi. Một mạng con có thể được tạo ra bằng cách chọn một tập con các lớp và một tập con các kênh từ không gian tìm kiếm Z̄.

Z̄:=I[0Z^mi_C∪Z^n_L (1)

trong đó L hoạt động ở cấp độ lớp, và C hoạt động ở cấp độ mô-đun con. Để làm cho không gian tìm kiếm dễ quản lý hơn, chúng tôi tiếp tục ràng buộc việc cắt giảm lớp và độ rộng theo một mối quan hệ đơn điệu. Điều đó có nghĩa là, nếu li bị cắt giảm, thì các lớp tiếp theo li+1, ..., ln cũng đều bị loại bỏ; tương tự, đối với các kênh c0, ..., cmi, nếu kênh ci bị cắt giảm, tất cả các kênh đầu ra tiếp theo cũng bị loại bỏ.

Trong quá trình huấn luyện, chúng tôi thực hiện phương pháp lấy mẫu sandwich [16, 13]. Chúng tôi lấy mẫu bốn kích thước mạng con: tối đa (tức là toàn bộ Supernet), tối thiểu, và hai kích thước ngẫu nhiên (tức là lựa chọn ngẫu nhiên các Cis và Lis từ Z̄). Khác với công trình trước, chúng tôi cung cấp cho mạng con tối đa toàn bộ batch đầu vào trong bước forward, trong khi cho ba mạng con khác một mini-batch bằng 1/4 của batch. Điều này giữ cho chất lượng của mạng tối đa cao trong khi giữ cho số giờ GPU huấn luyện hợp lý. Chúng tôi tối ưu hóa hàm loss sau:

min_Θ E_{s~Z̄} E_{(x,y*)~D_train} L_rnnt(y*|x; Θ_s) (2)

trong đó s là không gian tìm kiếm của Supernet, L_rnnt(y*|x; Θ_s) là loss transducer [4] đối với chuỗi đầu ra đúng y* cho các đặc trưng đầu vào x từ dữ liệu huấn luyện D_train, được tính toán sử dụng mạng con Θ_s.

2.2. Tìm Kiếm Pareto Supernet Sau Huấn Luyện
Chúng tôi sử dụng evolutionary search để khám phá các mạng con có hiệu suất cao nhất từ một không gian tìm kiếm của các tùy chọn mạng con khác nhau. WER tập validation được sử dụng làm điểm số fitness [19]. Evolutionary search Supernet có thể lượng tử hóa các mạng con một cách động, đánh giá chúng trên CPU, và do đó tiêu thụ ít tài nguyên hơn 100 lần so với việc huấn luyện mô hình. Kết quả là một tập hợp các cấu hình mạng con, mỗi cấu hình tối ưu hóa độ chính xác trên các ràng buộc kích thước τ̄ = [τ1, τ2, ..., τt]:

{arg min_{si} WER_{si~Z̄,E_{(x,y*)~D_val}(y*,x,Θ_{si}),s.t.M(si)≤τi}} (3)

trong đó M(si) là kích thước mô hình của mạng con si, và loss ở đây là word error rate (WER) được tính toán sử dụng beam search decoding với beam size=5.

3. CẢI THIỆN HUẤN LUYỆN SUPERNET
Trong phần này, chúng tôi phác thảo ba chiến lược chính để cải thiện chất lượng của các mô hình được huấn luyện Supernet. Trong khi mỗi kỹ thuật đã được áp dụng trong các bối cảnh không phải Supernet, sự kết hợp của chúng là mới lạ.

1. Adaptive Dropout: các mạng con khác nhau đóng góp các độ lớn khác nhau của gradient L2-norm trong quá trình huấn luyện Supernet. Được truyền cảm hứng từ adaptive dropout được sử dụng trong huấn luyện Omni-sparsity DNN [16], chúng tôi điều chỉnh độ lớn của dropout trong các lớp FFN, ngay sau các mô-đun với kênh đầu ra bị giảm trong quá trình huấn luyện. Trực giác, một mô-đun với độ chiều giảm yêu cầu ít regularization hơn để tạo ra đầu ra và gradient norm tương tự. Do đó, dropout_ci = dropout_cmi × ci/mi, ngay sau kích thước kênh đầu ra bị giảm của mô-đun i, trong đó mi là kích thước kênh đầu ra tối đa, và ci là kích thước kênh hiện tại.

2. Sampled In-place Knowledge Distillation (KD): trong mỗi bước của huấn luyện Supernet, chúng tôi sử dụng toàn bộ Supernet làm mô hình "teacher", và buộc chưng cất tại chỗ các phân phối xác suất đầu ra của mạng max vào mạng con được lấy mẫu, tức là "student", với dữ liệu từ mỗi mini-batch. Chúng tôi thực nghiệm với hai loại hàm divergence xác suất đầu ra: (1) Kullback–Leibler divergence (KLD), được biết là cải thiện độ chính xác của ASR nén trong quá trình chưng cất tri thức [20, 21]; và (2) Alpha-divergence [14] (AlphaD), đã được chứng minh là nắm bắt tốt hơn sự không chắc chắn của mạng teacher trong các phân phối xác suất đầu ra so với KL trong bối cảnh huấn luyện Supernet. Chưng cất phân phối xác suất đầu ra của teacher trên toàn bộ lattice RNN-T tốn nhiều bộ nhớ và làm chậm quá trình huấn luyện do yêu cầu kích thước batch nhỏ hơn. Để khắc phục điều đó, chúng tôi cải thiện bộ nhớ huấn luyện bằng cách lấy mẫu phụ top j xác suất đầu ra từ teacher. Trong một nghiên cứu trước về chưng cất RNN-T hiệu quả [22], j được đặt thành 2, có nghĩa là chưng cất chỉ tập trung vào 3 chiều logit: token mục tiêu, token blank, và tổng tích lũy của các xác suất còn lại. Chúng tôi giả thuyết rằng điều này không đủ để nắm bắt sự không chắc chắn của các xác suất đầu ra của teacher. Do đó, trong công trình này, chúng tôi so sánh các giá trị j là [2,10,100] (số chiều xác suất chưng cất = j+1). Hàm loss bây giờ là:

L=λL_KD+L_rnnt (4)
L_KD(Θ_s; Θ)_{s∈Z̄}=E_{x∈D_mini-batch}[f(p_j(x; Θ)||q_j(x; Θ_s)] (5)

trong đó chúng tôi sử dụng λ mặc định = 1.0, f là KLD hoặc AlphaD, được tính toán trên xác suất được lấy mẫu của top j token, và tổng của phần còn lại (p_j là phân phối của teacher, q_j là của student). Đối với Alpha-divergence, chúng tôi sử dụng cài đặt mặc định α_- = -1, α_+ = 1 và β = 5.0.

3. ScaledAdam Optimizer: chúng tôi khám phá việc huấn luyện Supernet với bộ tối ưu hóa ScaledAdam [23]. Là một biến thể của bộ tối ưu hóa Adam, ScaledAdam điều chỉnh cập nhật của mỗi tham số dựa trên norm của nó. Trực giác, bộ tối ưu hóa ScaledAdam cải thiện tính ổn định gradient của huấn luyện Supernet, trong đó mỗi mạng con đóng góp các gradient norm khác nhau một cách đáng kể vào các tham số mô hình trong bước forward-backward mini-batch.

4. THIẾT LẬP THỰC NGHIỆM
Trong bài báo này, chúng tôi xác thực kết quả của TODM Supernet trên một Multi-Head State Space Model (MH-SSM) RNN-T không streaming. Bộ mã hóa của RNN-T được xây dựng với 16 lớp MH-SSM; các lớp này không có attention và đã được chứng minh đạt hiệu suất tương đương so với ASR dựa trên transformer [24]. Mỗi lớp MH-SSM bao gồm 2 lớp xếp chồng, 1 đầu MH-SSM, và 4096 chiều feed-forward net (FFN). Kiến trúc RNN-T của chúng tôi tuân theo ví dụ được đưa ra trong công trình trước [24]: ba lớp LSTM 512 chiều cộng với một lớp tuyến tính trong predictor; một lớp project tuyến tính và một cổng ReLU trong joiner. Tổng cộng, MH-SSM RNN-T không streaming của chúng tôi có 99.9 triệu tham số. Trong quá trình huấn luyện Supernet, chúng tôi giảm một cách có chọn lọc toàn bộ các lớp MH-SSM, hoặc thay đổi kích thước của mô-đun FFN thành Ci∈[512,1024,2048,4096]. FFN chiếm 77.8% tổng số tham số trong một lớp MH-SSM, và có tiềm năng cao về sự dư thừa tham số. Giảm lớp trong supernet MH-SSM được thực hiện bằng cách giảm top 0, 3, hoặc 7 lớp MH-SSM. Xem Hình 1a để minh họa. Tất cả các baseline MH-SSM RNN-T của chúng tôi (A1, A2, A3 trong Bảng 1) được huấn luyện với cùng một thiết lập siêu tham số: tổng cộng 180 epoch huấn luyện, dưới tỷ lệ học cố định 0.006, force-anneal tại 60 epoch với hệ số thu nhỏ 0.96; 0.1 weight decay; bộ tối ưu hóa Adam với β1 = 0.9, β2 = 0.999.

Chúng tôi huấn luyện các RNN-T với dữ liệu huấn luyện LibriSpeech 960 giờ [25]. Chúng tôi thu được các đặc trưng log Mel-filterbank 80 chiều từ mỗi cửa sổ âm thanh 25 ms, trượt cửa sổ tiến 10 ms mỗi lần. Chúng tôi sử dụng vocab sentence piece 4096 chiều đã được huấn luyện trước [26], cộng với một ký hiệu 'blank', làm mục tiêu RNN-T. Sau huấn luyện Supernet, chúng tôi sử dụng dữ liệu LibriSpeech dev-clean và dev-other 10.7h để xác định các kiến trúc mạng con tốt nhất trong quá trình evolutionary search. Tất cả việc huấn luyện mô hình được thực hiện với 32 GPU NVIDIA-A100; để ngăn chặn việc preemption hàng đợi GPU, thời gian chờ lập lịch CPU, và các thời gian down-time khác của data center làm ô nhiễm tính toán giờ GPU, chúng tôi báo cáo tổng chi phí năng lượng của mỗi việc huấn luyện mô hình theo MegaWatt-giờ (MWh), được đo bằng năng lượng tiêu thụ và ghi lại bởi cụm GPU của chúng tôi.

5. KẾT QUẢ VÀ THẢO LUẬN
Chúng tôi so sánh TODM Supernet của chúng tôi (Model F) với 4 baseline:
1. A1 đại diện cho một tập hợp các mô hình được huấn luyện và điều chỉnh riêng lẻ, có kích thước được xác định bằng cách giảm kích thước lớp, kích thước kênh, hoặc cả hai, từ mô hình MH-SSM lớn nhất. Năng lượng sử dụng trong huấn luyện tỷ lệ thuận với số lượng và kích thước của các mô hình, do đó chúng tôi chỉ có thể khám phá các kiến trúc hạn chế.
2. A2 đại diện cho kết quả của mô hình lớn nhất, được huấn luyện với auxiliary cross-entropy loss [27] tại các lớp 8, 12, và 16; huấn luyện auxiliary đã giới thiệu thêm 1.15 triệu tham số vào mô hình, nhưng cuối cùng không cải thiện hiệu suất mô hình trong Bảng 1.
3. A3 là kết quả của mô hình lớn nhất, được huấn luyện với auxiliary RNN-T loss tại các lớp 8, 12, và 16 (tức là layer-drop) – mỗi lớp này được đi kèm với một sự kết hợp độc nhất của lớp tuyến tính, cổng và layer-norm, sau đó được đưa vào một predictor và joiner được chia sẻ [27]. Đối với k mạng con trong huấn luyện auxiliary RNN-T loss, có sự gia tăng tài nguyên k× trong quá trình huấn luyện, và kích thước mô hình bổ sung là k×0.53 triệu tham số. Lưu ý rằng A2 và A3 thể hiện mức tiêu thụ năng lượng huấn luyện tương đương. Chúng tôi sử dụng chúng để điều tra các hiệu ứng regularization bổ sung khi các hàm loss được áp dụng cho các lớp trung gian của mô hình RNN-T.
4. B0 đại diện cho một Supernet chỉ giảm kênh. Đó là một phiên bản tổng quát của slimmable network [13], trong đó các mạng con có độ rộng theo lớp khác nhau cùng tồn tại trong một Supernet.

5.1. Về Năng Lượng Tiêu Thụ Trong Quá Trình Huấn Luyện
Trong Hình 2, chúng tôi hiển thị năng lượng tiêu thụ bởi việc huấn luyện tối đa X∈[3,6, ..30] số lượng mô hình. Chi phí năng lượng huấn luyện mỗi Supernet là không đổi; các chi phí năng lượng tỷ lệ tuyến tính với số lượng mô hình không phải Supernet (Model A1 và A3). Supernet với KD (Model F) tiêu thụ nhiều năng lượng hơn Supernet không có KD (Model B), do nhu cầu sử dụng kích thước batch nhỏ hơn và do đó thời gian huấn luyện lâu hơn, vì mức tiêu thụ bộ nhớ lưu trữ gradient và logit cho KD là không nhỏ.

--- TRANG 3 ---
Hình 2 : Năng lượng (MWh) tiêu thụ bởi việc huấn luyện các mô hình ở quy mô lớn.

5.2. Về Thiết Kế Không Gian Tìm Kiếm Supernet
Chúng tôi đưa ra ba nguyên tắc cho thiết kế không gian tìm kiếm Supernet trong Phần 2.1. Chúng tôi chứng minh điều này bằng cách sử dụng hai Supernet trong Hình 3. Các điểm vuông màu đỏ tía (Model B0) hiển thị kết quả Pareto của Supernet giảm kênh, còn được gọi là slimmable networks [13]; các điểm tròn màu đỏ (Model B1) là kết quả Pareto được huấn luyện với giảm lớp và kênh. Giảm lớp mở rộng không gian tìm kiếm, cho phép tìm kiếm Pareto Supernet khám phá các mô hình có kích thước trung bình hiệu quả hơn, do đó dẫn đến sự cân bằng tốt hơn giữa độ chính xác mô hình và kích thước mô hình.

5.3. Về Lấy Mẫu Xác Suất cho KD
Để kiểm tra hiệu quả của KD, chúng tôi đánh giá kết quả Supernet tại 120 epoch, 2/3 tổng thời gian huấn luyện. Chúng tôi ghi lại WER của các mô hình max (tức là toàn bộ Supernet), min, và một kiến trúc cố định (47MB) trong Bảng 2. Kiến trúc cố định không phải lúc nào cũng nằm trên front Pareto của mỗi Supernet. Do lỗi GPU hết bộ nhớ xảy ra trong quá trình huấn luyện với KD của logit kích thước đầy đủ 4096; "C+KLD 4096" do đó không có mặt trong Bảng 2. Chúng tôi thấy rằng KD giúp supernet hội tụ nhanh hơn. Mô hình G 120epoch với KD đã vượt qua WER của mô hình baseline C 120epoch lên đến 10.3%, 4.2%, và 2.4% tương đối trong các mạng max, min, và kích thước cố định.

Đáng ngạc nhiên, với KD, mạng max hội tụ nhanh hơn nhiều so với các mạng nhỏ hơn, mặc dù KD được thiết kế để cải thiện các biểu diễn của các mạng nhỏ hơn. Chúng tôi giả thuyết rằng đó là do các mạng con chất lượng cao đóng góp vào hiệu suất tổng thể của mạng max thông qua chia sẻ trọng số.

AlphaD và KLD tạo ra kết quả tương tự, với AlphaD vượt trội hơn KLD một chút 2-3%. Tuy nhiên, sự khác biệt không có ý nghĩa thống kê. AlphaD có thể ngăn chặn sự ước lượng quá mức và ước lượng thiếu sự không chắc chắn của teacher [14], nhưng việc lấy mẫu logit và giảm phân phối xác suất có thể đã ước lượng thiếu sự không chắc chắn của teacher, hạn chế hiệu quả của AlphaD.

5.4. Về Ablation của Các Chiến Lược Huấn Luyện
Trong kết quả Supernet trong Bảng 1, hai cột cuối cùng của các cặp WER-Size là kết quả của mô hình tốt nhất trên front Pareto khi tìm kiếm một mạng con RNN-T ~50 triệu tham số. Model B1 được huấn luyện với giảm kênh và lớp; Model C thêm adaptive dropout trong quá trình huấn luyện; Model D sử dụng cả adaptive dropout và KL-divergence với top 10 log-xác suất được lấy mẫu; Model E, tương tự như Model D, sử dụng Alpha-divergence thay thế; Model F sử dụng adaptive dropout, Alpha-divergence (được lấy mẫu trên 10 log-xác suất), và tinh chỉnh bộ tối ưu hóa ScaledAdam sau 120 epoch. Tất cả các mô hình được huấn luyện từ đầu và được đánh giá tại 180 epoch.

Model A3 trong Bảng 1, được huấn luyện với RNN-T loss trên các lớp trung gian, là một trường hợp đặc biệt của Supernet B1 – chia sẻ trọng số giữa 3 mạng con bị layer-drop. Tuy nhiên, Model A3 có hiệu suất kém hơn đáng kể tại 40MB so với Model B1. Regularization phụ trợ và layer-dropping đơn thuần không giải thích được hiệu suất vượt trội của Supernet so với A2 và A3.

So sánh Model B1 và C, chỉ riêng adaptive dropout cải thiện huấn luyện Supernet với một mức độ nhỏ. Mặc dù sự hội tụ sớm của D 120epoch và E 120epoch, các mô hình D và E sau đó hội tụ chậm hơn nhiều so với mô hình C. Chúng tôi quan sát thấy rằng trong khi in-place KD giúp huấn luyện Supernet hội tụ ở các giai đoạn đầu của quá trình huấn luyện, nó cũng gây ra loss validation phân kỳ vào khoảng 100 đến 120 epoch. Do đó chúng tôi chuyển sang sử dụng bộ tối ưu hóa ScaledAdam tại 120 epoch, và giảm trọng số KD loss trong Phương trình (4) xuống λ = 0.1. Mặc dù bộ tối ưu hóa ScaledAdam hữu ích cho 60 epoch cuối cùng của huấn luyện, chúng tôi quan sát thấy rằng huấn luyện TODM từ đầu với nó ngăn cản Supernet hội tụ hoàn toàn.

5.5. Các Quan Sát Khác
Chúng tôi thấy rằng các mô hình Supernet không được lợi từ thời gian huấn luyện lâu hơn. Trên thực tế, tiếp tục huấn luyện Supernet Model C thêm 60 epoch nữa dẫn đến sự tăng WER tương đối 16.4% và 8.0% cho các mô hình max và min tương ứng. Điều này cho thấy rằng TODM hội tụ trong số epoch tương tự như huấn luyện một trong các mô hình đơn lẻ. Chúng tôi cũng thấy rằng việc tăng tỷ lệ học hoặc khởi tạo Supernet với một mô hình đã được huấn luyện trước làm tổn hại đến độ chính xác mạng con Pareto. Điều sau cho thấy rằng Supernet chia sẻ trọng số tốt nhất có thể khác biệt đáng kể so với mạng max nếu nó được huấn luyện đơn lẻ.

Cuối cùng, kiến trúc mô hình front Pareto của Model F tại 49.6MB trong Hình 1b không có các mẫu kiến trúc rõ ràng. Điều này có thể giải thích tại sao rất khó để đạt được kiến trúc này thông qua điều chỉnh thủ công.

Trong suốt các thí nghiệm của chúng tôi, các xu hướng trong WER cho tập dữ liệu test-clean tương tự như những xu hướng cho test-other – ví dụ, các Model trong A1 có test-clean WER là 2.6 (99.9MB) và 3.5 (31.6MB); Supernet Model F có WER 2.5 (99.9MB) và 3.4 (29.1MB).

6. KẾT LUẬN
Bài báo này giới thiệu khung TODM, có thể huấn luyện và khám phá các RNN-T dày đặc được tối ưu hóa với các kích thước khác nhau từ một Supernet đơn lẻ, với tài nguyên huấn luyện tương đương với một công việc huấn luyện mô hình đơn lẻ. Chúng tôi giới thiệu ba chiến lược cho TODM để cải thiện kết quả Supernet: adaptive dropout, chưng cất tri thức được lấy mẫu tại chỗ, và tinh chỉnh bộ tối ưu hóa ScaledAdam. TODM khám phá nhiều mô hình dọc theo front Pareto của độ chính xác so với kích thước, điều mà sẽ tốn nhiều tài nguyên để tìm và huấn luyện thủ công thông qua thử-và-sai.

--- TRANG 4 ---
[Bảng 1 và 2 với kết quả WER và thông tin chi tiết]

--- TRANG 5 ---
7. TÀI LIỆU THAM KHẢO
[1] Alex Graves and Navdeep Jaitly, "Towards end-to-end speech recognition with recurrent neural networks," in International conference on machine learning PMLR, 2014.
[2] Tara N Sainath, Yanzhang He, Bo Li, Arun Narayanan, Ruoming Pang, Antoine Bruguier, Shuo-yiin Chang, Wei Li, Raziel Alvarez, Zhifeng Chen, et al., "A streaming on-device end-to-end model surpassing server-side conventional model quality and latency," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.
[3] Yuan Shangguan, Rohit Prabhavalkar, Hang Su, Jay Mahadeokar, Yangyang Shi, Jiatong Zhou, Chunyang Wu, Duc Le, Ozlem Kalinli, Christian Fuegen, et al., "Dissecting user-perceived latency of on-device e2e speech recognition," Proc. of Interspeech, 2021.
[4] Alex Graves, "Sequence transduction with recurrent neural networks," International Conference of Machine Learning (ICML) Workshop on Representation Learning, 2012.
[5] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, "Speech recognition with deep recurrent neural networks," in International conference on acoustics, speech and signal processing (ICASSP), 2013.
[6] Bo Li, Anmol Gulati, Jiahui Yu, Tara N Sainath, Chung-Cheng Chiu, Arun Narayanan, Shuo-Yiin Chang, Ruoming Pang, Yanzhang He, James Qin, et al., "A better and faster end-to-end model for streaming asr," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.
[7] Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, Ralf Schlüter, and Shinji Watanabe, "End-to-end speech recognition: A survey," 2023.
[8] Shaojin Ding, Weiran Wang, Ding Zhao, Tara N Sainath, Yanzhang He, Robert David, Rami Botros, Xin Wang, Rina Panigrahy, Qiao Liang, et al., "A unified cascaded encoder asr model for dynamic model sizes," Proc. of Interspeech, 2022.
[9] Yuan Shangguan, Jian Li, Qiao Liang, Raziel Alvarez, and Ian McGraw, "Optimizing speech recognition for the edge," in Machine Learning and Systems (MLSys), On-device Intelligence Workshop, 2019.
[10] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han, "Once-for-all: Train one network and specialize it for efficient deployment," The International Conference on Learning Representations (ICLR), 2020.
[11] Jiahui Yu and Thomas S Huang, "Universally slimmable networks and improved training techniques," in Proc. of IEEE/CVF international conference on computer vision, 2019.
[12] Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le, "Bignas: Scaling up neural architecture search with big single-stage models," in Computer Vision ECCV 2020, 2020.
[13] Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang, "Slimmable neural networks," in International Conference on Learning Representations (ICLR), 2018.
[14] Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra, "Alphanet: Improved training of supernets with alpha-divergence," in International Conference on Machine Learning (ICML), 2021.
[15] Arun Narayanan, Tara N Sainath, Ruoming Pang, Jiahui Yu, Chung-Cheng Chiu, Rohit Prabhavalkar, Ehsan Variani, and Trevor Strohman, "Cascaded encoders for unifying streaming and non-streaming asr," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.
[16] Haichuan Yang, Yuan Shangguan, Dilin Wang, Meng Li, Pierce Chuang, Xiaohui Zhang, Ganesh Venkatesh, Ozlem Kalinli, and Vikas Chandra, "Omni-sparsity dnn: Fast sparsity optimization for on-device streaming e2e asr via supernet," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.
[17] Chunxi Liu, Yuan Shangguan, Haichuan Yang, Yangyang Shi, Raghuraman Krishnamoorthi, and Ozlem Kalinli, "Learning a dual-mode speech recognition model via self-pruning," in Spoken Language Technology Workshop (SLT), 2023.
[18] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, "Librispeech: An asr corpus based on public domain audio books," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
[19] Mu Yang, Andros Tjandra, Chunxi Liu, David Zhang, Duc Le, and Ozlem Kalinli, "Learning asr pathways: A sparse multilingual asr model," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023.
[20] Ruoming Pang, Tara Sainath, Rohit Prabhavalkar, Suyog Gupta, Yonghui Wu, Shuyuan Zhang, and Chung-Cheng Chiu, "Compression of end-to-end models," Proc. of Interspeech, 2018.
[21] Ladislav Mošner, Minhua Wu, Anirudh Raju, Sree Hari Krishnan Parthasarathi, Kenichi Kumatani, Shiva Sundaram, Roland Maas, and Björn Hoffmeister, "Improving noise robustness of automatic speech recognition via parallel data and teacher-student learning," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019.
[22] Sankaran Panchapagesan, Daniel S Park, Chung-Cheng Chiu, Yuan Shangguan, Qiao Liang, and Alexander Gruenstein, "Efficient knowledge distillation for rnn-transducer models," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.
[23] Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, and Daniel Povey, "Zipformer: A faster and better encoder for automatic speech recognition," arXiv preprint arXiv:2310.11230, 2023.
[24] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al., "Multi-head state space model for speech recognition," Proc. Interspeech, 2023.
[25] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, "Librispeech: An asr corpus based on public domain audio books," in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.
[26] Taku Kudo and John Richardson, "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," in EMNLP, 2018.
[27] Chunxi Liu, Frank Zhang, Duc Le, Suyoun Kim, Yatharth Saraf, and Geoffrey Zweig, "Improving rnn transducer based asr with auxiliary tasks," in Spoken Language Technology Workshop (SLT), 2021.
