# 2306.05425.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2306.05425.pdf
# File size: 9805311 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MIMIC-IT: Multi-Modal In-Context
Instruction Tuning
Bo Li∗,♠,1Yuanhan Zhang∗,♠,1Liangyu Chen∗,1Jinghao Wang∗,1Fanyi Pu∗,1
Jingkang Yang1Chunyuan Li2Ziwei Liu1,B
1S-Lab, Nanyang Technological University, Singapore2Microsoft Research, Redmond
{libo0013, yuanhan002, lchen025, c190209, fpu001, ziwei.liu}@ntu.edu.sg
chunyl@microsoft.com
https://github.com/Luodian/Otter
Abstract
High-quality instructions and responses are essential for the zero-shot performance
of large language models on interactive natural language tasks. For interactive
vision-language tasks involving intricate visual scenes, a large quantity of di-
verse and creative instruction-response pairs should be imperative to tune vision-
language models (VLMs). Nevertheless, the current availability of vision-language
instruction-response pairs in terms of quantity, diversity, and creativity remains lim-
ited, posing challenges to the generalization of interactive VLMs. Here we present
MultI-ModalIn-Context Instruction Tuning ( MIMIC-IT ), a dataset comprising
2.8 million multimodal instruction-response pairs, with 2.2 million unique instruc-
tions derived from images and videos. Each pair is accompanied by multi-modal
in-context information, forming conversational contexts aimed at empowering
VLMs in perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus , is scaled using an automatic annotation pipeline that
combines human expertise with GPT’s capabilities. Using the MIMIC-IT dataset,
we train a large VLM named Otter . Based on extensive evaluations conducted
on vision-language benchmarks, it has been observed that Otter demonstrates re-
markable proficiency in multi-modal perception, reasoning, and in-context learning.
Human evaluation reveals it effectively aligns with the user’s intentions. We release
the MIMIC-IT dataset, instruction-response collection pipeline, benchmarks, and
the Otter model.
1 Introduction
The recent advancements in artificial intelligence have focused on conversational assistants [ 42,
31,30,13,17] that possess a strong ability to understand user intentions [ 35] and then execute
actions [ 5,51]. In addition to the strong generalization ability of large language models (LLMs), the
notable achievements of these conversational assistants can be attributed to the practice of instruction
tuning [ 47,14,46,45,42,13,34]. It involves fine-tuning LLMs on a range of tasks specified through
diverse and high-quality instructions [ 14,45]. By incorporating instruction tuning, LLMs acquire
a heightened comprehension of user intentions [ 35], enabling them to exhibit improved zero-shot
capabilities even in previously unseen tasks [ 47]. One potential reason for the zero-shot performance
gain by instruction tuning is that it internalizes the context [ 40], which is preferred in user interactions
especially when user input skips commonsense context.
Conversational assistants that excel in language tasks have achieved remarkable success. However, an
optimal conversational assistant should be able to address tasks involving multiple modalities. This
∗Equal Contribution♠Project LeadBCorresponding Author
Preprint. Under review.arXiv:2306.05425v1  [cs.CV]  8 Jun 2023

--- PAGE 2 ---
SubtleDifference
IndoorEventPlanningSocialReasoningStoryTellingPerceptionReasoning
Planning
ImageDescriptionWhat does the girl do after completing the routine?(DC)EpisodeMemoryWhat is the difference between the two images?(SD)
Can you suggest any recreational pursuits that can be enjoyed within this room? (IEP)What are the male and female characters holding while they are together?(TVC)TVDramaHow did the students advocate for immigration reform policies during the ceremony?(VIST)
AbductiveReasoningWrite a detailed description of the given image.(LA)Can you think of some reasons why cats might sit on someone‘s laptop?(LA)What might be the purpose of this gathering of people? (LA)Egocentric Video ReasoningIs it safe to walk on the floor while the woman is cleaning?(E4D)
What might this young boy learn from this experience?(LA)CommonsenseReasoningGeneralSceneDifferenceWhat is the difference between the two images?(GSD)
EgocentricPlanningCan you suggest any ways to brighten up the room I’m in?(E4D)
MIMIC-ITFigure 1: MIMIC-IT overview. The MIMIC-IT dataset comprises 2.8M multi-modal instruction-
response pairs spanning fundamental capabilities: perception, reasoning, and planning. Each instruc-
tion is accompanied by multi-modal conversational context, allowing VLMs trained on MIMIC-IT to
demonstrate strong proficiency in interactive instruction following with zero-shot generalization.
requires access to a diverse and high-quality multi-modal instruction-following dataset. The LLaV A-
Instruct-150K dataset [ 28], also known as LLaV A, is the pioneering vision-language instruction-
following dataset. It is constructed using COCO [ 27] images, instructions and responses obtained
from GPT-4 [30] based on image captions and object bounding boxes.
Although inspiring, LLaV A-Instruct-150K exhibits three limitations. (1) Limited visual diversity :
The dataset’s visual diversity is constrained due to its exclusive reliance on the COCO image.
(2) Single image as visual data : it utilizes a single image as visual data, while a multi-modal
conversational assistant should possess the capability to process multiple images or even extensive
videos. For instance, it should effectively provide answers when a user presents a collection of images
(or a sequence of images, such as a video) alongside the instruction: "Help me think of an album title
for these images." (3) Language-only in-context information : it depends solely on language for
in-context information, whereas a multi-modal conversational assistant should integrate multi-modal
in-context information to better comprehend user instructions. For example, an assistant could more
accurately align its description of an image with the tone, style, or other aspects if the human user
provides a concrete image example of the desired attributes.
Addressing these limitations, we introduce MultI-ModalIn-Context Instruction Tuning ( MIMIC-
IT). MIMIC-IT is characterized by: (1) Diverse visual scenes , incorporating images and videos
from general scenes, egocentric view scenes, and indoor RGB-D images across various datasets. (2)
Multiple images (or a video) as visual data , supporting instruction-response pairs accompanied
by any number of images or videos. (3) Multi-modal in-context information , featuring in-context
information formulated in multi-modal formats, including multiple instruction-response pairs and
multiple images or videos (see Fig. 2 for data format clarification). To efficiently generate instruction-
response pairs, we introduce Sythus , an automated pipeline for instruction-response annotation
inspired by the self-instruct method [ 45]. Sythus employs system message, visual annotation, and
in-context examples to direct the language model (GPT-4 or ChatGPT) in generating instruction-
response pairs based on visual context, including timestamps, captions, and object information,
targeting three fundamental capabilities of vision-language models: perception, reasoning, and
planning (refer to Fig. 1). Additionally, instructions and responses are translated from English into
seven languages to support multi-lingual usage.
2

--- PAGE 3 ---
OnMIMIC-IT , we train a multi-modal model Otter based on OpenFlamingo [ 6]. We evaluate
Otter’s multi-modal capabilities in two aspects: (1) ChatGPT evaluation on the MMAGIBench-
mark [ 43], comparing Otter’s perception and reasoning abilities with other recent vision-language
models (VLMs), where Otter demonstrates the strongest performance. (2) Human evaluation on
the Multi-Modality Arena [ 32], where Otter outperforms other VLMs, achieving the highest Elo
rating. Furthermore, we assess Otter’s few-shot in-context learning ability using the COCO Caption
dataset [ 12], with results showing Otter’s superior performance over OpenFlamingo in all few-shot
settings. In summary, our contributions include:
•MultI-ModalIn-Context Instruction Tuning ( MIMIC-IT ) dataset, a dataset comprising ~
2.8M multi-modal in-context instruction-response pairs, with 2.2 million unique instructions,
across various real-life scenes.
•Syphus , an automatic pipeline built with LLMs to generate high-quality and multi-lingual
instruction-response pairs based on visual context.
•Otter , a multi-modal model demonstrates robust multi-modal perception and reasoning ca-
pabilities, effectively following human intent while exhibiting adeptness in-context learning.
2 Related Work
2.1 Multi-modal Instruction Tuning Dataset
The notion of instruction tuning in multi-modal models was initially introduced in the work called
Multi-Instruct [ 50], which encompassed a wide range of multi-modal tasks [ 18,56,41,27,12]
involving visual understanding and multi-modal reasoning, such as Visual Question Answering [ 18,
56,23]. Similarly, Mini-GPT4 [ 54] created its instruction-based dataset by merging Conceptual
Caption [ 38,8], SBU [ 33], and LAION [ 36] with handwritten instruction templates. More recently,
LLaV A-Instruct-150K [ 28] has elevated the quality of instruction tuning datasets by utilizing self-
instruct and GPT-4 [ 30], along with handwritten seed instructions on COCO images [ 27]. While
these previous works on multi-modal instruction tuning primarily focused on general scene images,
our approach categorizes our data sources into indoor scenes, outdoor scenes, conversations, and
egocentric videos. Additionally, drawing inspiration from the image-text interleaved structure of
the MMC4 dataset [ 55], our approach further distinguishes itself by incorporating a multi-modal
in-context format into instruction tuning.
2.2 Multi-modal Foundation Models
With the recent success of ChatGPT [ 31], GPT-4 [ 30], and other LLMs [ 44,42,13], recent studies
start to explore incorporating information from other modalities into pretrained language models.
These studies extend the capabilities of LLM to more tasks and modalities and can be categorized
into two classes: (i) Multi-model Aggregation. These approaches [ 48,39,11,57,57,52] take an
LLM as a dispatch scheduler and connect different expert models through it to allow for different
tasks. Language serves as an interface to call expert visual-language models within their respective
task domains. However, this approach is limited that each model cannot be trained individually on
new tasks. (ii) End-to-End Trainable Models. These approaches [ 2,6,25,30,3,37,7,54,53,28]
connect models from different modalities into integrated end-to-end trainable models, also known as
multi-modal foundation models. Among them, based on large-scale image-text interleaved pretrained
model OpenFlamingo [ 6], Otter is the first open-sourced model to further demonstrate the power of
multi-modal in-context instruction tuning.
3 Multi-modal In-context Instruction Tuning Dataset
We aim to build MIMIC-IT dataset to support more VLMs in acquiring the ability to comprehend
the real world. In this section, we provide an overview of the MIMIC-IT dataset, starting with the
data format in Sec. 3.1 and our automatic instruction generation pipeline, Sythus , in Sec. 3.2.
3

--- PAGE 4 ---
MIMIC-IT:Language-onlyIn-contextUp->Down:Ego4D->Spot The DifferenceLLaVA-Instruct-150K:Language-onlyIn-contextInstruction:What type of traffic device can be seen in the image?Response:There is a street sign and a traffic light above a city road.
MIMIC-IT:Multi-modalIn-contextUp->Down:LLaVA-Interleaved->TVShowCaptions
Instruction: What is the difference between these two images?Response: The first one shows an outhouse with a toilet, while the second one shows…Instruction:What the reason for the dog's current expression and outfit?Response:…wearing a festive Christmas hat and possibly panting, …,making a cute and memorable moment withtheir pet during the holiday season.
Instruction:Whyteddy bear ornament on the Christmas tree?Response:The teddy bear ornament hanging on the Christmas tree conveys a sense of …children and adults during the festive season.# In-context:Instruction:What is the current colorof the traffic lights in the image?Response:All the traffic lights in the image are showing the colorredat the same time.# Query:
Instruction:What task is the woman performing?Response:The woman is vacuuming the rug area between chairs and couches.Instruction:At what time of day is?Response:Asthebedside lamisopeningandthelightinthisroomisdark,itisinthenight.
≈≈Instruction:Whatcolorofjerseydoesmyteammateswear?Response:Black,asyoupasstheballtoaplayerwearingblackjersey.Instruction:Doesourteamgoal?Response:Unfortunately,doesnot.TheThe goalkeeper saves the ball at the end
Instruction: Are there any objects in image two but not in image one?Response: Yes, there are additional objects in image two that are not present in image one. Image two has a white sink, a bathtub, a mirror, and other bathroom items, which are not mentioned in image one.
Figure 2: Data Format Comparison: LLaVA-Instruct-150K vs. MIMIC-IT . (a) LLaV A-Instruct-
150K consists of a single image with corresponding language-only in-context information (yellow
box). (b) MIMIC-IT accommodates multiple images or videos within the input data and supports
multi-modal in-context information, i.e., considering both images/videos and language inputs as
in-context information.
3.1 MIMIC-IT Data Format
Each instance iin the MIMIC-IT dataset comprises an instruction-response pair and a set of N
images. We regard it as query example with a tuple: (Iq, Rq, Xq), where
xN
j=1	
∈Xq. Here, Iq
denotes the q-th instruction in our dataset, Rqrepresents the response, and Xqrefers to the images or
videos1. Our primary objective is to develop a visual language model pθ(Rq|(Iq, Xq))parametrized
by trainable parameters θ, the model generates the response Rifor each query (Iq, Xq). With above
example denotes the standard instruction tuning process of a visual language model. Further, we
could define a set of in-context examples as (Ik, Rk, Xk)M
k=1, where Mis the number of the set.
We then define a context function Cψ: (Iq, Xq)7→ { (Ik, Xk)}M
k=1to represent the in-context
examples with current query example. In summary, all data in the MIMIC-IT dataset will be
represented in the following format, query example with its corresponding in-context examples.
dq= (Iq, Rq, Xq, Cψ(Iq, Xq)), d q∼DMIMIC-IT (1)
Now the visual language model that incorporates in-context examples can be denoted as pθ(Rq|
(Iq, Xq, Cψ(Iq, Xq))).Cψis task-dependent, we apply different approaches to organize the in-
1Videos can be viewed as ordered sequences of images.
4

--- PAGE 5 ---
Step1SystemMessage+visualannotation
Step 2Generate instruction-response pairsStep3Filtering
Step4TranslationChatGPT
Prompt
In-contextexamplesColdStartChatGPT
Figure 3: Sythus overview. We employ a cold-start stage to identify the optimal system message and
in-context example for querying instruction-response pairs in a given dataset. Subsequently, Sythus,
spanning steps 1 to 4, generates high-quality instruction-response pairs in eight languages.
Table 1: Comparison between MIMIC-IT and other multi-modal instruction datasets. MIMIC-
IT stands out with the following features: (1) The largest vision-language instruction dataset. (2) The
first instruction dataset including video data. (3) Supporting multi-modal in-context scenarios (see
Fig. 2 for the data format). (4) Supporting eight languages including: English, Chinese, Spanish,
Japanese, French, German, Korean, and Arabic. The data source of MIMIC-IT includes seven
datasets: COCO [ 27], Spot-the-diff [ 21] (SD), ScanNetV2 [ 15] (SN), VisualStorytelling [ 20] (VIST),
DenseCaption/Activity caption [ 22] (DC), TVCaption [ 24] (TVC), and Ego4D [ 19] (E4D). lang.
indicates language and vis.indicates vision.
Dataset Visual Data (Scenes) In-context Video #Clips/Images #Instruct. #Instance. Lang.
MiniGPT-4 [54] CC (General) -/- - / 134M 4 5K English
LLaV A [28] COCO (General) [27] lang./- - / 81K 261K 345K English
MIMIC-ITCOCO (General) [27] lang./vis. - / 81K 261K 345K
Multi.SD (Surveillance) [21] lang./vis. - / 9K 10K 15K
SN (Indoor Ego.) [15] lang./vis. - / 0.5K 4.8K 6K
DC (General)[22] lang./vis. 16K / 1M 40K 62K
VIST (Story)[20] lang./vis. - / 16K 32K 33K
TVC (TV)[24] lang./vis. 86K / 577K 86K 92K
E4D (General Ego.)[19] lang./vis. 400K / 6.4M 1.8M 2.4M
Total lang./vis. 502K / 8.1M 2.2M 2.8M
context examples with the current query example. The details will be presented in Sec. 3.3 and
illustrative examples will be showcased in Fig. 2.
3.2 Sythus: Automatic Instruction-Response Generation Pipeline
We present Sythus (see Figure 3), an automated pipeline for generating high-quality instruction-
response pairs in multiple languages. Building upon the framework proposed by LLaV A [ 28],
we utilize ChatGPT to generate instruction-response pairs based on visual content. To ensure the
quality of the generated instruction-response pairs, our pipeline incorporates system messages,
visual annotations, and in-context examples as prompts for ChatGPT. System messages define the
desired tone and style of the generated instruction-response pairs, while visual annotations provide
essential image information such as bounding boxes and image descriptions. In-context examples
assist ChatGPT in learning within the context. Since the quality of coreset impacts subsequent
data collection process [ 10], we employ a cold-start strategy to enhance in-context examples before
the large-scale query. During the cold-start stage, in-context examples are collected by prompting
ChatGPT solely through system messages and visual annotations, employing a heuristic approach.
This stage concludes only when satisfactory in-context examples are identified. In step 4, once the
instruction-response pairs are obtained, the pipeline expands them into Chinese (zh), Japanese (ja),
5

--- PAGE 6 ---
Spanish (es), German (de), French (fr), Korean (ko), and Arabic (ar). For further details, please refer
to Appendix C, and task-specific prompts can be found in Appendix D.
3.3 Visual Data Exploration
Acknowledging the importance of high-quality visual annotations and the need for diverse vision-
language instructions that align with the distribution of real-world visual content, we curate a
collection of seven image and video datasets spanning a wide spectrum of scenes, from general to
specific. Encompassing various topics, the MIMIC-IT dataset includes general scene understanding
and reasoning, spoting general and subtle differences, as well as facilitating egocentric view compre-
hension to assist VLMs in future AR headsets, etc. In the subsequent sections, we will present the
application scenarios of our dataset: General Scene Understanding in Sec. 3.3.1 and General Scene
Understanding in Sec. 3.3.2. In each sub-task, we elaborate on the process of organizing various data
into an in-context instruction tuning format, based on the previously established guidelines.
3.3.1 General Scene Understanding
For understanding the general scenes, we include four tasks: (1)LLaV A-Interleaved. (2)Spot The
Difference. (3)Visual Story Telling. (4)Dense Captions.
LLaVA-Interleaved (LA-I). Learning with in-context examples is essential for effective instruction
tuning. To achieve this, we refine the LLaV A-Instruct-150K [ 28] dataset by retrieving ten in-context
examples for each instruction-response pair in LLaV A-Instruct-150K, building LLaV A-Interleaved
(LA-I). We identify each data’s in-context examples based on instruction text-to-text similarity or
image-image similarity. Further details on locating in-context examples and the data sources for LA-I
can be found in the Appendix.
Spot The Difference (SD). Learning to discern differences between images is vital for understanding
real-world changes. Our study encompasses two interrelated task types in Scene Difference (SD),
addressing varying complexity levels in difference identification. The first type, General Scene
Difference, involves creating a pair of images by determining the most similar one to the current
image, utilizing image-to-image similarity relationships from the COCO2017 [ 27]. The second
type, Subtle Difference, features pairs of similar images with subtle distinctions sourced from the
Spot-the-Diff[ 21], extracted from surveillance footage. For the first type, we prompt ChatGPT using
original image captions and object detection annotations, while for the second type, we employ
natural language difference descriptions as annotations. The resulting instruction-response pairs
focus on identifying differences between the paired images.
Visual Story Telling (VIST). Beyond traditional scene understanding, the ability to generate coherent
and engaging narratives based on visual input expands the context comprehension of Visual Language
Models (VLMs). To enable this, we propose a task using the Visual Storytelling datase [ 20], which
includes event-based image sequences and corresponding inquiry questions. Given that image
annotations often contain narratives and timelines not directly observable, we instruct ChatGPT to act
as a viewer answering questions about the images. The prompts also incorporate thought-provoking
inquiries to promote creativity. Each task instance comprises multiple images and instruction-response
pairs, providing in-context examples.
Dense Captions (DC). Expanding the scope of video understanding, DC features dense captions
from [ 22] corresponding to clips within longer videos. The instructions pose a diverse set of questions,
addressing the general visual content of the video, human actions, and behaviors, the chronological
sequence of events, and causal relationships. This approach encourages VLMs to delve deeper into
the intricacies of video content.
TV Show Captions (TVC). The primary purpose of incorporating TV show clips with high-level
captions into the training process of VLMs is to enhance their social reasoning abilities and deepen
their understanding of complex character dynamics. By organizing drama clips from [ 24] to analyze
character relationships and motivations, we aim to challenge VLMs to move beyond mere perception
and demonstrate their reasoning capabilities within the context of TV show narratives. This focused
approach is crucial for fostering advanced VLMs capable of effectively handling diverse real-world
situations and user queries.
6

--- PAGE 7 ---
identifys ugges t
see
t ake
descri be provi dehave d of aceusegi veo b s e r v ep i c kp l a yr e c o m m e n dn o t i c ea f f e c te x p l a i np u tf i n d
object
item
type
hazar d
way
o p t i o n
r e c i p e
i t e m
object
item
s i g n
t o o l
action
pr ecaut i on
pl ace
pi ct ure
surroundi ng
appearance
at mosphere
obj ect
i nf orm
at i on
det ai l
i nsi ght
 t i p
conversat i on
i mpact
t roubl e
ki nd
char act er
act i vi t y
ki nd
per son
chal l enge
r i sk
di f f i cul t y
i ssue
t ool
t ype
equi pment
phone
advi ce
i ns t r uc t i on
i nf or mat i on
ov er v i ew
p e r s o n
o b j e c t
i t e m
u s e r
i t e m
k i n d
o b j e c t
t y p e
r o l e
g a m e
b o o k
w a y
o p t i o n
i t e m
p e r s o n
o b j e c t
i t e m
b o t t l e
p e r f o r m a n c e
e x p e r i e n c e
a t m o s p h e r e
d i f f e r e n c e
r o l e
i m p o r t a n c e
s i g n i f i c a n c e
i t e m
b o o k
book
p r o v i d eseedet erm
i nehavewear
t a k e
play
observe
hol d
f eat urei dent i f yuseshowdepictc a p t u r en o t i c ep i c kp l a c ep u ti n c l u d e
i nf ormat i on
vi ew
 det ai l
answer
obj ect
person
item
 book
 type
 l ocat i on
cont ent
pur pose
information
conver sat i on
i mpact
benef i t
shirt
g e a r
glove
j a c k e t
p l a c e
p r e c a u t i o n
p i c t u r e
p h o t o
g a m e
r o l e
c a r d
t enni s
per son
obj ect
pl ant
bot t l e
phone
book
cup
pi ece
group
man
t abl e
woman
t ype
obj ect
brand
hazard
hand
kni f e
t rowel
phone
man
per son
gr oup
woman
scene
gr oup
st r eet
man
s c ene
mo me n t
group
g a me
h a z a r d
o b j e c t
s i g n
i t e m
b o o k
p i e c e
b o t t l e
c a r d
h a n d
b o o k
t r a y
c a r d
h a n d
p h o n e
b o o k
p a p e r
b o t t l e
c u p
c h a i r
b o o k(a) Instructions (b) Responses
(c) Statistics of instructions and responses.
Figure 4: The data statistics of multi-modal in-context instruction-response pairs. (a) and (b),
the root verb-noun pairs of instruction and responses, where the inner circle of the plot represents
the root verb of the output response, and the outer circle represents the direct nouns. (c) Statistics of
instructions and responses, retaining 25% of Ego4D instructions for a more balanced distribution. #
Related instructions denotes the number of related instructions in an instance, given the same set of
visual input data.
3.3.2 Egocentric View Understanding
Indoor Event Planning (IEP). Emphasizing the planning capabilities of virtual assistants, we utilize
visual inputs consisting of a collection of 2D photos depicting a room. We gather indoor scene
RGB-D images from ScanNetv2 [ 15] and sample them into multiple 2D visual inputs, representing
a room’s layout from a first-person perspective. We prompt ChatGPT to generate instructions that
direct humans to perform various activities in indoor spaces. Initially, we have ChatGPT create a
personality for the room owner. Subsequently, the planning should be intimately related to the room’s
layout and the generated room owner, underlining the importance of context awareness in VLMs.
This approach ensures that models can effectively support users across diverse indoor scenarios.
Ego4D (E4D) [ 19].Utilizing E4D’s egocentric videos, we strive to enable VLMs to function
effectively as augmented reality (AR) assistants in real-life scenarios. By prompting ChatGPT to
generate instructions based on visual descriptions, our goal is to simulate practical interactions
between users and AR assistants. To this end, we devise assistant-related questions and tasks that
demand context-aware responses. For instance, Instruction: What should I do now? Response: Based
on my observation, you can now proceed to do... . This focused approach underscores the potential of
VLMs in providing valuable insights and assistance across a diverse range of daily life situations.
3.4 Dataset Statistics
Table 1 presents the essential statistics pertaining to the generated data. Our dataset comprises over
2.8 million instruction-response pairs, wherein each pair includes at least one multi-modal in-context
example and one language-only in-context example. Among these pairs, there are 2.2M unique
instructions. Furthermore, to examine the characteristics and diversity of the instructions (refer to
Fig. 4 (a)) and responses (refer to Fig. 4 (b)), we analyze the verb-noun structure present in them,
refering to [ 45]. Specifically, we employ spaCy for parsing the instructions, extracting the verb closest
7

--- PAGE 8 ---
(1st)Instruction:What could happen if drivers become confused by the traffic signals at this intersection?Otter:Confusion at traffic signals can negatively impact road safety, traffic flow, and driver well-being.(2nd)Instruction:Pleaselistafewpotentialrisks.Otter:It could lead to various potential riskyscenarios: 1. Red light: The red lightsignal is on, and the car is moving. In this case, the driver might continue driving, potentially…
Instruction:What’swrittenonthisimage?Otter:pepsi,ispepsiokay?
Instruction:What’swrittenonthisimage?Otter:SUBWAY,eatfresh.Instruction:What’swrittenonthisimage?(wo/ICE)Otter:“Think Different” is written on this image.(w/ICE)Otter:Think Different
Learning with In-context ExamplesEgocentricVisualAssistantSituationUnderstandingandReasoning
Figure 5: Otter’s response examples in different scenarios. Trained on the MIMIC-IT dataset,
Otter is able to serve for situation understanding and reasoning, learning with in-context examples,
and egocentric visual assistant.
to the root, and retrieving its first direct noun object2. We plot the top 20 most frequently occurring
root verbs alongside their top 4 direct noun objects. Our findings reveal that the sentence structure
of responses exhibits greater diversity compared to that of instructions. Moreover, we demonstrate
diversity in terms of the length of instructions/responses, the number of images per instruction, and
the number of in-context examples per instruction, as depicted in Fig. 4 (c).
4 Empricial Evaluation
In this section, we showcase the diverse applications of the MIMIC-IT dataset and the potential
capabilities of a vision-language model (VLM) trained on it. Firstly, in Sec. 4.1, we introduce Otter,
an in-context instruction-tuned model developed using the MIMIC-IT dataset. Next, in Sec. 4.2, we
explore various methods for training Otter on the MIMIC-IT dataset and discuss numerous scenarios
in which Otter can be effectively employed. Finally, in Sec. 4.3 to Sec. 4.5, we present a comparative
analysis of Otter’s performance against other VLMs across an array of benchmarks.
4.1 Otter: A Multi-Modal In-context Instruction Tuned Model
Otter is designed to support multi-modal in-context instruction tuning based on the OpenFlamingo [ 6]
model, which involves conditioning the language model on the corresponding media, such as an
image that corresponds to a caption or an instruction-response pair.
4.2 Usage Examples and Demonstrations
Scene Understanding and Reasoning. The MIMIC-IT dataset comprises approximately 2.8 million
in-context instruction-response pairs, which are structured into a cohesive template to facilitate various
tasks. The following template encompasses images, user instructions, and model-generated responses,
utilizing the Human andAssistant role labels to enable seamless user-assistant interactions.
<image>Human:{instruction} Assistant:<answer>{response}<endofchunk>
2https://github.com/explosion/spacy-models/releases/tag/en_core_web_
md-3.5.0
8

--- PAGE 9 ---
Table 2: MMAGIBench evaluation results. Otter outperforms all baseline models by achieving the
highest average accuracy in both perception and reasoning tasks.
Perception ReasoningModel Lang. Decoder Avg.Coarse Finegrained Attribute Relation Future Pred.
InstructBLIP [16] Vicuna-7B 50.4 67.8 52.2 43.8 38.2 50.0
MiniGPT-4 [54] Vicuna-7B 51.0 63.3 47.8 50.6 26.5 66.7
OpenFlamingo [6] LLaMA-7B 51.1 34.4 40.0 61.3 52.9 66.7
LLaV A [28] Vicuna-7B 62.7 44.4 54.2 71.9 76.5 66.7
Otter LLaMA-7B 65.5 68.9 47.3 66.3 61.8 83.3
Training the Otter model on the MIMIC-IT dataset allows it to acquire different capacities, as
demonstrated by the LA and SD tasks. Trained on the LA task, the model exhibits exceptional scene
comprehension, reasoning abilities, and multi-round conversation capabilities. Meanwhile, on the SD
task, the model can acquire the ability to adeptly spot general differences or subtle distinctions within
daily scenes.
We showcase response examples from the Otter after training on the MIMIC-IT dataset in Fig. 5,
highlighting its ability to understand situations and reasoning in a multi-round conversation style.
Learning with In-context Examples. As mentioned in Sec. 3.1, regarding the concept of organizing
visual-language in-context examples, we demonstrate here the acquired ability of the Otter model to
follow inter-contextual instructions after training on the LA-T2T task (refer to Appx. for other tasks).
The organized input data format is as follows:
# Multiple in-context example with similar instructions
<image>Human:{instruction} Assistant:<answer>{response}<|endofchunk|>
# ....
<image>Human:{instruction} Assistant:<answer>{response}<|endofchunk|>
# Query example
<image>Human:{instruction} Assistant:<answer>
The Otter model’s demonstration of regulating its expressions by referencing in-context examples is
illustrated in Fig. 5.
Egocentric Visual Assistant. A distinctive feature of the MIMIC-IT dataset is its inclusion of a
comprehensive collection of videos and sequential images in an egocentric view, derived from the
IEP, E4D scenarios. In the IEP scenario, the content emphasizes understanding and planning within
indoor environments, incorporating instructions and responses designed to guide the model in event
planning based on interior layouts.
The E4D scenario, on the other hand, tailors instructions and responses specifically for first-person
augmented reality (AR) headset assistant applications. These two datasets collectively serve to bolster
the model’s proficiency in perceiving scenes from a first-person viewpoint, strategizing for impending
tasks, and providing valuable insights and suggestions to AR headset users. Tailored this part of data,
we train an egocentric visual assistant, termed Otter-E , which is specifically designed for AR headset
applications. MIMIC-IT bolsters the model’s proficiency in perceiving scenes from a first-person
viewpoint, strategizing for impending tasks, and providing valuable insights and suggestions to
AR headset users. As a result, the Otter-E model emerges as an exceptional and visionary Visual
Language Model for AR headsets, paving the way for a groundbreaking and immersive experience.
In the bottom image of Fig. 5, Otter-E demonstrates its ability to perceive the first-person view and
respond to users’ questions, such as guiding users to land a small aircraft (In real-life scenarios, you
are not encouraged to consult visual assistants for such hazardous actions).
4.3 ChatGPT Evaluation
In Tab. 2, we utilize the MMAGIBench framework [ 43] to provide an extensive evaluation of the
perception and reasoning capabilities of vision-language models. The perception benchmark consists
of data derived from COCO images and social network images ( e.g., , Twitter), covering tasks such
as coarse scene and object recognition, fine-grained OCR, celebrity identification, and recognition
9

--- PAGE 10 ---
QA Captioning3840424446 Accuracy
38.440.145.2
42.9MSVD 0-shot
QA Captioning303540 Accuracy
27.834.535.339.5MSRVTT 0-shotVideoChatGPT Otter
MM-GPT Inst. BLIP LLaVA MiniGPT Otter9909951000100510101015Elo Rating
991.9996.31012.21013.21014.7
0-shot 4-shot 8-shot 16-shot5560657075808590CIDEr
60.872.479.381.8
61.575.581.884.7
Otter
OpenFlamingo(a) Video understanding (b) Vision-language model alignment (c) COCO caption
Figure 6: (a) ChatGPT evaluation for video understanding. Otter outperforms baseline models by
substantial margins in video understanding. (b) Human evaluation comparison. Otter demonstrates
superior usefulness and alignment. (c) Few-shot in-context learning evaluation. Otter outperforms
OpenFlamingo as a better in-context and zero-shot learner.
of well-known locations. The reasoning benchmark, on the other hand, is performed across three
dimensions: attribute reasoning, relation reasoning, and future prediction.
Current evaluation metrics for vision-language models, like VQAv2 [ 4], exhibit shortcomings in
terms of robustness. For instance, VQAv2 primarily assesses single-word or phrase responses, while
many modern models generate sentence outputs. To bridge this gap, we evaluate the models by
asking ChatGPT to compare their label predictions with the ground truth labels for each input. A
test sample is considered correct if ChatGPT’s response indicates that the prediction aligns with the
corresponding label. For a more in-depth understanding of MMAGIBench, we recommend referring
to the original source [ 43]. Fig. 6 (a) demonstrates that Otter outperforms VideoChatGPT [ 26]
by 6.8% accuracy and 1.8% on MSVD [ 9] 0-shot question answering and captioning benchmarks
respectively. Similar substantial margins are also observed on the MSRVTT [49] dataset.
4.4 Human Evaluation
Multi-Modality Arena [ 32] uses an Elo rating system to evaluate the usefulness and alignment of
VLM responses. The Elo rating system calculates the relative skill levels of players, as commonly
used in chess and other competitive games. The difference in Elo ratings between the two models
predicts the outcome if they were matched against each other. This system works well for evaluating
conversational AI models, because multiple models can have pairwise "battles" responding to the
same inputs in a user-blind evaluation. Fig. 6(b) shows that Otter demonstrates superior usefulness
and alignment, achieving the highest Elo rating among recent VLMs.
4.5 Few-shot In-context Learning Metric Evaluation
Otter is finetuned based on OpenFlamingo, an architecture designed for multi-modal in-context
learning. Finetuned with the MIMIC-IT dataset, Otter outperforms OpenFlamingo by a substantial
margin on COCO caption (CIDEr) [ 27] few-shot evaluation (see Fig. 6(c)). As expected, the
finetuning also brings marginal performance gain on zero-shot evaluation.
5 Discussion
Limitations. Though we have iteratively refined the system message and instruction-response
examples, ChatGPT is prone to language hallucinations therefore it might generate incorrect responses.
Generally, more trustworthy language models are desired for self-instruct data generation.
Future Works. In the future, we plan to support more embodied AI datasets such as Language-
Table [ 29] and SayCan [ 1]. We also consider improving the instruction collection with more trustwor-
thy language models or generation techniques.
Conclusion. In this work, we propose MIMIC-IT, a large-scale multi-modal in-context instruction
tuning dataset. We leverage an automatic pipeline, Syphus, to enable this dataset to cover a diverse
set of visual scenes and creative instructions in eight languages. MIMIC-IT empowers our model,
Otter, to achieve state-of-the-art performances in perception and reasoning benchmarks as well as
human evaluations.
10

--- PAGE 11 ---
Acknowledgments and Disclosure of Funding
This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2
(MOE-T2EP20221- 0012), NTU NAP, and under the RIE2020 Industry Alignment Fund – Industry
Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the
industry partner(s). We thank Peiyu Fu, Xuli Chen, and Mehdi Cherti for their professional advice on
the in-context example of the translation query of Japanese, French, German, Spanish, Korean, and
Arabic.
References
[1]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,
Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not
as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 , 2022.
10
[2]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in Neural Information Processing Systems ,
35:23716–23736, 2022. 3
[3] Alibaba. Tongyi qianwen. 2023. 3
[4]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence
Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE
international conference on computer vision , pages 2425–2433, 2015. 10
[5]Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel,
Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprint
arXiv:2211.09260 , 2022. 1
[6]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel
Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 3, 8, 9
[7] Baidu. Ernie bot: Enhanced representation through knowledge integration. 2023. 3
[8]Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing
web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3558–3568, 2021. 3
[9]David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies , pages 190–200, Portland, Oregon, USA, June 2011. Association
for Computational Linguistics. 10
[10] Liangyu Chen, Yutong Bai, Siyu Huang, Yongyi Lu, Bihan Wen, Alan L Yuille, and Zongwei
Zhou. Making your first choice: To address cold start problem in vision active learning. arXiv
preprint arXiv:2210.02442 , 2022. 5
[11] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,
and Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on
Mathematical and Empirical Understanding of Foundation Models , 2023. 3
[12] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325 , 2015. 3
[13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:
An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 3
11

--- PAGE 12 ---
[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
language models. arXiv preprint arXiv:2210.11416 , 2022. 1
[15] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pages 5828–5839, 2017. 5, 7, 15
[16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose
vision-language models with instruction tuning. CoRR , abs/2305.06500, 2023. 9
[17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,
Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied
multimodal language model. arXiv preprint arXiv:2303.03378 , 2023. 1
[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making
the v in vqa matter: Elevating the role of image understanding in visual question answering.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages
6904–6913, 2017. 3
[19] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit
Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world
in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18995–19012, 2022. 5, 7, 15
[20] Ting-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Aish-
warya Agrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual
storytelling. In 15th Annual Conference of the North American Chapter of the Association for
Computational Linguistics (NAACL 2016) , 2016. 5, 6, 15
[21] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs
of similar images. In EMNLP , pages 4024–4034. Association for Computational Linguistics,
2018. 5, 6, 15
[22] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-
captioning events in videos. In Proceedings of the IEEE international conference on computer
vision , pages 706–715, 2017. 5, 6, 15
[23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting
language and vision using crowdsourced dense image annotations. International journal of
computer vision , 123:32–73, 2017. 3
[24] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for
video-subtitle moment retrieval. In Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16 , pages 447–463. Springer, 2020.
5, 6, 15
[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2023. 3
[26] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang,
and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355 ,
2023. 10
[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13 , pages 740–755. Springer, 2014. 2, 3, 5, 6, 10, 15
[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023. 2, 3, 5, 6, 9
12

--- PAGE 13 ---
[29] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch,
Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. arXiv
preprint arXiv:2210.06407 , 2022. 10
[30] OpenAI. Gpt-4 technical report. 2023. 1, 2, 3
[31] OpenAI. Introducing chatgpt. 2023. 1, 3
[32] OpenGVLab. Multi-modality arena. https://github.com/OpenGVLab/
Multi-Modality-Arena , 2023. 3, 10
[33] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1
million captioned photographs. Advances in neural information processing systems , 24, 2011. 3
[34] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with GPT-4. arXiv preprint arXiv:2304.03277 , 2023. 1
[35] Andy Rosenbaum, Saleh Soltan, Wael Hamza, Yannick Versley, and Markus Boese. Linguist:
Language model instruction tuning to generate annotated utterances for intent classification and
slot tagging. arXiv preprint arXiv:2209.09900 , 2022. 1
[36] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton
Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open
dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 , 2021. 3
[37] SenseTime. Sense nova. 2023. 3
[38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pages 2556–2565, 2018. 3
[39] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint
arXiv:2303.17580 , 2023. 3
[40] Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context. arXiv preprint
arXiv:2209.15189 , 2022. 1
[41] Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. A corpus of natural language for visual
reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 217–223, 2017. 3
[42] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca , 2023. 1, 3
[43] MMAGIBench Team. Mmagibench: A universal multi-modal benchmark towards artificial
general intelligence. https://github.com/open-mmlab/mmagibench , 2023. 3, 9,
10
[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971 , 2023. 3
[45] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-
tions. arXiv preprint arXiv:2212.10560 , 2022. 1, 2, 7
[46] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,
Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al.
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing ,
pages 5085–5109, 2022. 1
13

--- PAGE 14 ---
[47] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan
Du, Andrew M. Dai, and Quoc V . Le. Finetuned language models are zero-shot learners. In
ICLR . OpenReview.net, 2022. 1
[48] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint
arXiv:2303.04671 , 2023. 3
[49] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for
bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 5288–5296, 2016. 10
[50] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot
learning via instruction tuning. arXiv preprint arXiv:2212.10773 , 2022. 3
[51] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools:
Teaching large language model to use tools via self-instruction. 2023. 1
[52] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,
Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for
multimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023. 3
[53] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,
Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init
attention. arXiv preprint arXiv:2303.16199 , 2023. 3
[54] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 , 2023. 3, 5, 9
[55] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang,
Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An open,
billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939 , 2023. 3
[56] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question
answering in images. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 4995–5004, 2016. 3
[57] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat
Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language.
arXiv preprint arXiv:2212.11270 , 2022. 3
14

--- PAGE 15 ---
A Total Cost and ChatGPT Version
We construct MIMIC-IT using the ChatGPT-0301 version. Overall, we query 1,006,746,240 tokens
(859,677,150 and 147,069,090 for input and output tokens respectively). The estimated total cost is
$20134.9248.3
B Content Copyright and License
The license of the datasets we used in this work is illustrated below.
Visual Data Image License Instruction-response license
MS-COCO [27] Custom CC BY-NC-SA
Spot-the-diff [21] Unknown CC BY-NC-SA
ScanNetv2 [15] non-commercial CC BY-NC-SA
ActivityNet Captions [22] Unknown CC BY-NC-SA
Visual Storytelling [20] Unknown CC BY-NC-SA
TV Captions [24] Unknown CC BY-NC-SA
Ego4D [19] non-exclusive, non-transferable CC BY-NC-SA
C Sythus: Automatic Instruction Generation Pipeline
Safety and Ethical Filtering Since we use GPT to generate instructions and responses, we
generally follow the GPT content policy for safe and ethical use. This policy eliminates output
that is suspicious for unfair opportunities, stereotyping, overrepresentation/underrepresentation,
explicit content, disinformation, or unreliable information.
Multi-lingual Support We enrich the datasets by translating the English instruction-response pairs
by GPT into 7 additional languages: Chinese, Japanese, Spanish, German, French, Korean, and
Arabic. See the prompt for multi-lingual translation query in Fig. 7.
D Annotation Prompt
In this section, we will present prompts for querying ChatGPT of all datasets in detail. Each prompt
contains system message ,in-context emample .
3https://openai.com/pricing
15

--- PAGE 16 ---
EnglishInput: Hey, bro, I feel like having a cup of tea here!Response: Having tea is a great choice! Right now, you have two options in front of you: green tea and oolong tea. Which one would you like to have? Of course, if you want to enhance the tea-drinking experience even more, you can dim the lights here and play some classic Chinese music on your phone!Chinese 中文Input: 嘿，兄弟，我想在这⾥喝⼜茶!Response: 嗨！喝茶是个不错的选择！现在你⾯前有绿茶和乌龙茶，你想喝哪种呢︖当然，如果你想有更好的品茶氛围的话，你可以把这⾥的灯光调暗，再⽤你的⼿机来点经典的中国⾳乐！Spanish españolInput: ¡eh, tío, me apetecetomarunatazade café aquí!Response: ¡Tomartées unamuybuenaelección! ¿Cuáltegustaríatomar? Por supuesto, siquieresdisfrutaraúnmásde la experienciade tomarté, puedesatenuarlas luces de aquíy reproducirmúsicaclásicachinadesdetuteléfonomóvil.Japanese日本語Input: ねえ、ここでお茶を飲みたいな！Response: お茶を飲むのはいい選択だよ！今、目の前には緑茶とウーロン茶がありますが、どちらが飲みたいですか？もちろん、もっと良いお茶の時間を過ごしたいなら、ここでの照明を暗くして、あなたの携帯で中国のクラシック音楽を流すといいよ！FrenchFrançaisInput: Hé, j'aienviede prendre unetasse de théici!Response: Prendre du théestun excellent choix! Maintenant, devanttoi, tuas le choixentre du thévert et du théoolong. Lequelaimerais-tuprendre ? Bien sûr, situveuxvivre uneexpériencede dégustationde théencore meilleure, tupeuxbaisserles lumièresiciet jouerde la musique chinoiseclassiquesur ton téléphone!Korean한국어Input: 어이, 여기서차한잔마시고싶어!Response: 차를마시는건좋은선택이야! 지금당신앞에는녹차와우롱차중에무슨차를마시고싶어? 물론, 차를더욱풍미있게즐기고싶다면여기조명을어둡게조절하고, 핸드폰으로중국의클래식음악을틀어봐도좋아!GermanDeutschInput: Hey, ich möchtehiergerne eineTasse Tee trinken!Response: Hallo! Tee zu trinken ist eine gute Wahl! Sie haben jetzt grünen Tee und Oolong-Tee vor sich, welchen möchten Sie trinken? Selbstverständlich können Sie auch das Licht dimmen und klassische chinesische Musik auf Ihrem Handy abspielen, wenn Sie eine bessere Atmosphäre einer Teeverkostung genießen möchten!Arabic ﺔﯾﺑرﻌﻟا Input:ا ﻲﺧأ ، ﺮﻌﺷأ ﻲﻨﻧﺄﻛو ﺐﻏرأ ﻲﻓ بﺮﺷ بﻮﻛ ﻦﻣ يﺎﺸﻟا ﺎﻨھ!Response: زﯾزﻌﺗ تدرأ اذإ ، ﻊﺑطﻟﺎﺑ ؟لﺿﻔﺗ ﺎﻣﮭﯾأ .ﻎﻧوﻟووﻷا يﺎﺷﻟاو رﺿﺧﻷا يﺎﺷﻟا :كﻣﺎﻣأ نارﺎﯾﺧ كﯾدﻟ ، نﻵا !ﻊﺋار رﺎﯾﺗﺧا وھ يﺎﺷﻟا برﺷ !كﻔﺗﺎھ ﻰﻠﻋ ﺔﯾﻛﯾﺳﻼﻛﻟا ﺔﯾﻧﯾﺻﻟا ﻰﻘﯾﺳوﻣﻟا ضﻌﺑ لﯾﻐﺷﺗو ﺎﻧھ راوﻧﻷا تﻔﺧ كﻧﻛﻣﯾ ، رﺛﻛأ يﺎﺷﻟا برﺷ ﺔﺑرﺟﺗ Figure 7: In-context examples for multi-lingual translation query.
16

--- PAGE 17 ---
System Message
You are an AI assistant that can understand video and reason about what happens in the provided
video. You can observe multiple frames in a video with the provided descriptions.
Design a conversation between you and a person asking about this video. The answers should be in
a tone that a visual AI assistant is seeing the video and answering the question.
Ask multiple diverse questions and give each question a corresponding answer.
In the questions you ask, try to incorporate speculations about the relationships between the characters
and their motivations. You need to design questions that require a certain degree of reasoning to
answer, prompting the need for logical deduction before arriving at a response.
When using the information from the description, directly explain the scene and do not mention
anything about the descriptions. When you are unsure about the answer, you can say you are not
certain and provide a reasonable guess.
Please always use pronouns when referring to individuals and avoid directly using the names
mentioned in the description. For example, Monica should be "the female character". Ross should
be "the male character".
Answer yes if you understand the above instructions.
In-context Examples
User:
1. Monica says something to Ross, who is standing next to the door, sadly look down.
2. Monica is talking to Ross then Rachel look at him.
3. Rachel says she is surprised by Ross’ actions.
4. Monica kneels on the floor and talks to Ross who’s standing by the door.
ChatGPT:
"Question": "How does the male character appear when the female character talks to him by the
door?"
"Answer": "The male character appears to be feeling sad, as he is looking down while the female
character speaks to him."
"Question": "What is another character’s reaction to the conversation between the female character
and the male character?"
"Answer": "The other character seems to be observing the conversation, as they look at the male
character while the female character is talking to him."
"Question": "How does the other character express their feelings about the male character’s actions?"
"Answer": "The other character expresses surprise at the male character’s actions, indicating that his
behavior might have been unexpected or out of character."
"Question": "What change in position does the female character make while speaking to the male
character near the door?"
"Answer": "The female character kneels on the floor while talking to the male character, possibly to
create a more intimate or empathetic connection with him during their conversation."
Table 3: System message and in-context exemplars for TV show Captions (TVC) query.
17

--- PAGE 18 ---
System Message
You are an AI assistant that can understand videos. You can observe multiple frames in the given
video. You are provided with the timestamps and corresponding descriptions in the following format:
timestamps: [[0, 19], [17, 60]]
sentences: ["A young woman is seen standing in a room and leads into her dancing.", "The girl
dances around the room while the camera captures her movements."]
The first description "A young woman is seen standing in a room and leads into her dancing." is
happening from 0 to 19 seconds, while the second description is happening from 17 to 60 seconds.
Design a conversation between you and a person asking about this video. The answers should be in
a tone that a visual AI assistant is seeing the video and answering the question.
Ask diverse questions and give corresponding answers.
Include questions asking about the visual content of the video. The questions you pose can include
the actions and behaviors of people or objects in the video, the chronological order of events, and
causal relationships. Only include questions that have definite answers. Do not ask any question that
cannot be answered confidently.
When using the information from the description, directly explain the scene and do not mention
anything about the descriptions. When you are unsure about the answer, you can say you are not
certain and provide a reasonable guess.
In-context Examples
User:
timestamps: [[0, 91], [73, 174], [168, 212]]
sentences: ["Several shots of boats are shown riding around as well as people riding on the boats and
speaking to one another.", "Several shots are then shown of people sitting on the water as well as life
under the water.", "The camera pans around old cars under water as well as people in the area."]
ChatGPT:
"Question": "What is the main theme of this video?"
"Answer": "The main theme of this video is centered around water activities, featuring scenes of
boats, people on boats, underwater life, and exploring underwater areas with old cars."
"Question": "What are people doing on the boats?"
"Answer": "People on the boats are riding around and engaging in conversations with one another."
"Question": "What happens after the boat scenes?"
"Answer": "After the boat scenes, the video shows several shots of people sitting on the water and
exploring life beneath the water’s surface."
"Question": "Is there any dialogue in the video?"
"Answer": "Yes, there is dialogue in the video as people are shown speaking to one another while on
the boats."
"Question": "What is the setting of this video?"
"Answer": "The setting of this video is primarily in and around a body of water, featuring scenes of
boats, people on the water, and underwater exploration."
"Question": "What type of boats are shown in the video?"
"Answer": "I am not certain about the exact types of boats shown in the video, but multiple boats are
featured, and people are enjoying their time riding around on them."
"Question": "Why are all the subsequent scenes underwater?"
"Answer": "Because the cameraperson is using a GoPro-like camera to enter the water and capture
the underwater scenes."
"Question": "Are there any safety measures observed in the video, such as life jackets or flotation
devices?"
"Answer": "I cannot confidently determine the specific safety measures employed in the video.
It is essential, however, that people engaging in water activities should prioritize safety and use
appropriate equipment, such as life jackets and flotation devices."
Table 4: System message and in-context exemplars for Dense Caption (DC) query .
18

--- PAGE 19 ---
System Message
As an augmented reality (AR) assistant integrated within a pair of smart glasses, your capabilities
extend to interpreting video footage from a first-person perspective. Each frame of the video is at
your disposal, complete with timestamps, object identifications, and descriptions of the scene. Use
this information to anticipate potential user queries and provide helpful and relevant responses.
Envision the user scenarios and questions that might arise based on the video, and provide assistance
accordingly. Generate user question and assistant answer pairs that encapsulate the visual content of
the video, simulate real-life user queries, and offer helpful insights for improved decision-making.
For instance, the user could be searching for a particular item or seeking advice on a prospective
purchase like a beverage. Utilize your understanding of the context, the objects involved, and their
properties to create responses that are plausible, actionable, and health-conscious when appropriate.
Before moving forward, ensure you understand the guidelines for question and answer formatting
and make the interaction as genuine as possible. Keep in mind, the quality and diversity of these
interactions directly contribute to the refinement of your capabilities as a visual assistant.
In-context Examples
User:
timestamp: 102 description: man Y stands beside C objects: white couch with pillows on it; a
woman with black hair; a clock on the wall; christmas tree in the corner of the room; black and white
picture frames; a bald-headed man; a window in the room;
timestamp: 103 description: The cameraman turns around objects: woman with long dark hair; a TV
turned on; a woman looking at a computer; a brightly lit sign; a set of keys on a keyboard; the TV is
on;
timestamp: 104 description: The cameraman walks around objects: the side of the bus; a dark room;
a small blue light; a phone screen is on;
timestamp: 105 description: The cameraman turns on a light switch objects: the side of the bus; a
dark room; a small blue light; a phone screen is on;
timestamp: 109 description: The cameraman turns around objects: a black and pink tie; a white tie
on a rack; a red tie; a group of ties; pink tie on black and white ties; a black and white striped tie; a
white ceramic vase; red and black ties; red fabric behind green ties; white tie on a suit; a yellow tie;
light green tie on the right; a white cloth draped over the left side of the chair; black and red striped
tie; red and black shoe;
...
ChatGPT:
Question: Where could my favorite dress be? I can’t seem to locate it.
Answer: Given the observed video footage, your favorite dress might be in the wardrobe or the
cabinet that you opened recently. Please inspect these areas carefully. Do you require further
assistance?
Question: I checked the wardrobe, but the dress isn’t there. Any other ideas on where it could be?
Answer: According to my observations, another plausible location could be draped over the chair
that is covered with a white cloth. I suggest you exit the room and inspect that chair. Shall I continue
to assist you with this?
Question: Where can I find my phone charger?
Answer: Based on the visual data available, the phone charger isn’t directly visible in any of the
provided frames. However, it could be in the cabinet or the wardrobe that you’ve recently opened,
or near the computer if there’s a charging station set up there. I suggest you check these locations.
What else can I help?
Question: Can you identify the time from the clock on the wall?
Answer: As a visual assistant, I can only provide information based on what I can directly see. In
the given frames, there is no detailed view of the clock, making it impossible for me to determine
the time. Would you like me to investigate further?
Table 5: System message and in-context exemplars for Ego4D (E4D) query.
19

--- PAGE 20 ---
In-context Examples
User:
sentences: this full size guitar is leaning against the wall between the bed and the bathroom. its
bottom is on the floor and its fretts are along the wall, where it will be tripped over when someone
goes to the bathroom at night. in front of a black office chair , there is a desk . above the desk , is a
curtain , the curtain is to the left of the refrigerator ,it is beige. this is a round sink. it is next to a
toilet. the tall cabinet. the cabinet is next to the desk. the long table. the table is above the stools. it
is a white pedestal sink. the sink is in the bathroom, right next to the bedroom area. the sink is to the
left as you enter the bathroom. ...
Candidate activity and the role who want to do this activity:Hold a party - Human role: A college
student. Yoga Session - Human role: Yoga Instructor Home Barista Course - Human role: Coffee
Lover
Conversation 1 - Hold a party Human role: A college student Round 1: Human: I want to take a
party in this room, what can i do? Assistant: Based on the descriptions provided, the room appears
to be a multi-purpose living space with features such as a bed, desk, office chair, kitchen area with a
refrigerator and microwave, and a bathroom area with a toilet and sink. Here’s what you can do to
prepare for a party in this space: 1. Clear the Space: First, make sure that the room is as spacious as
possible to accommodate your guests. Relocate the bike that’s currently behind the green curtain to
another room, if possible. You might want to consider relocating the guitar that’s leaning against
the wall between the bed and the bathroom to prevent any accidents. 2. Seating: Utilize the office
chair, bed, stools by the bar, and the couch for seating. Depending on the number of guests, you
might need to bring in additional chairs or cushions. ... The above material is an example for a
task. In this task, you are an AI visual assistant that can guide people to do several activities base
on your understanding of the layout of a indoor room. First, you will receive around 50 sentences,
each describing the same indoor scene you are observing, but focus on the various view (a part of
view in this scene). These sentences can help you to understand the layout. Second, you will receive
several indoor activities and their related human role. Third, based on your understanding of the
layout in this room. You should design several conversation between you (answer questions) and
a human (ask questions) in this room. Each conversation can guide the human to do the activities
you received. You should select activities from the candidates. The chosen activities should be the
most suitable considering the layout of the room. You and the human should be in a tone that you
are seeing the layout in this room.Please ensure a diverse range of sentence structures in the tone
of human and assistant. Again, do not ask about uncertain details. Provide detailed answers when
answering complex questions. For example, give detailed examples or reasoning steps to make the
content more convincing and well-organized. You can include multiple paragraphs if necessary.
Answer me ’understood’ if you are understood this task. And tell me what is this task.
ChatGPT:
Understood. This task involves acting as an AI visual assistant to guide humans in performing
various activities in an indoor room. I’ll be provided with around 50 sentences that describe the
layout of the room from various perspectives, which will help me understand the layout of the
room. Next, I’ll be given several indoor activities and their related human roles. My job is to design
conversations between the human and me (the AI), using my understanding of the room layout to
guide the human in performing the given activities. This could involve answering questions from
the human about how to go about the activities within the constraints of the room layout. The tone
of the conversation should reflect that both the AI and the human are observing the room layout.
The dialogue should vary in sentence structure and provide detailed answers to complex questions.
I need to ensure that my responses are detailed and well-reasoned to make them more convincing
and well-organized. In cases where the proposed activities might not be well-suited to the room
layout, I need to creatively figure out ways to help the human carry out the activities within the given
constraints. In other words, I’m expected to assist in navigating the room and coordinating activities
based on my understanding of the room’s layout.
Table 6: System message and in-context exemplars for Indoor Event Planning (IEP) query.
20

--- PAGE 21 ---
System Message
You are an AI assistant playing the spot the difference game. You will be presented with several
sentences that describe the differences between two images. The two images are frames extracted
from the video-surveillance footage.
Your response shall always contain a pair of question and answer. The question you generate should
have definite and concrete answer according to the description. When generating answer, you should
answer as if you are watching the images. You are not allowed to say "according to descrption" or
similar words in your outputs. You do not have to provide a specific answer and you should not ask
for more details.
Each description mentioned in each round of conversation is independent. You should not assume
that the descriptions in a new round of conversation are related to the previous context.
If there are no differences, you still have to follow the previous format.
In-context Examples
User:
the car in the upper left corner of the picture is gone
the two people in the upper part of the picture are gone
the person near the handicapped parking place is not in the second picture
there appears to be a figure maybe man that was near handicapped parking place straining near a
pole in the bottom of the picture
the two people who were on th sidewalk are not in the second picture
there are two people in the seconde picture that are near the handicapped parking place
ChatGPT:
Question: Could you summarize the differences between the two images?
Answer: The left image has five people and the right image has only three. The people are not the
same. The car in the upper left corner of the picture is gone. The remaining elements in the two
images are the same.
Question: Does the right image have more people than the left image?
Answer: Yes, the left image has two more people than the right image.
Question: Does the car in the upper left corner of the picture still exist in the second picture?
Answer: No, the car in the upper left corner of the picture is gone.
Question: Does the right image have more cars than the left image?
Answer: Yes, the right image has one car while the left image has no car.
Question: Does the people in the left image the same as the people in the right image?
Answer: No, the people in the left image are not the same as the people in the right image.
User:
There is no difference between the two images.
ChatGPT:
Question: Could you summarize the differences between the two images?
Answer: There is no difference between the two images.
Table 7: System message and in-context exemplars for Spot The Defference (SD) query.
21

--- PAGE 22 ---
System Message
You are an AI assistant equipped with the remarkable ability to understand albums and weave
captivating narratives based on their images. While you are unable to directly see the pictures, you
have access to the accompanying descriptions, allowing you to gather important context. Your task is
to design a conversation that immerses the person inquiring about the album in its captivating stories.
As you engage in this creative endeavor, envision yourself as a companion to the person exploring the
album. Craft questions that delve into the heart of the content, drawing inspiration from the provided
descriptions while avoiding questions that rely solely on visual details that are not accessible. Your
goal is to generate meaningful conversations based on the available information.
When formulating your responses, aim to express empathy and curiosity, channeling the emotions and
narratives conveyed by the images. Utilize the descriptions to vividly describe the scenes, creating
a sensory experience that transports the person into the album’s world. If there are uncertainties,
approach them with humility and offer thoughtful insights based on your accumulated knowledge.
Although the information originates from text, your aim is to create an engaging and authentic
experience that makes the person feel intimately connected to the album. Leverage your creativity
and learning capabilities to generate questions and answers that embody the next generation of
large-scale multimodal language models.
Now that you have a clear understanding, proceed with crafting the dialogue, ensuring your questions
derive from the given content while maintaining coherence and immersion for the person interacting
with the album.
In-context Examples
User:
title: Moreton Bay Fig 1877 description: Believed to be the largest Moreton Bay Fig Tree in the
united States
image: Moreton Bay Fig 1877 tags: santabarbara annotations: ["it turns out it is a popular attraction
here .", "there was an informational sign posted near it .", "the plack under it had some interesting
information ."]
image: Santa Barbara tags: santabarbara annotations: ["our landmark tree in town was about to be
destroyed and cleared for a new mall .", "the tree is very unusual , with its roots exposed .", "pictures
of a tree are taken .", "the roots were huge and spread out over the ground .", "there was a good view
of the water from the tree as well ."]
image: Santa Barbara tags: santabarbara hero winner thegame moretonbayfig contestentry big-
momma gamewinner babymomma superherochallenge challengeyouwinner pickyourpoison cyniner
challengeyouniner 15challenges challengefactorywinner thechallengefactory motherofallchallenge-
groups motherofallchallengegroupswinner superherochallengewinner thegamewinner annotations: []
...
ChatGPT:
Question: f this tree had a voice, what stories would it tell us?
Answer: This tree might tell us stories about the past, as it has witnessed over a hundred years of
history. It could narrate the identity of the people who planted it, their reasons for choosing this
location, and their motives and visions. It could describe changes in the surrounding environment,
such as urban expansion, population growth, and alterations in the cityscape. It might also share
stories about the local people and how they formed a connection with this tree, along with memories
of community activities and events from that time.
Question: What makes the Moreton Bay Fig tree in Santa Barbara unique compared to other trees?
Answer: The Moreton Bay Fig tree in Santa Barbara is unique because of its distinctive root structure.
The roots are exposed and spread out over the ground, creating a captivating and unusual visual
display. This feature sets it apart from other trees that typically have their roots buried beneath the
soil.
Question: Why did people consider the Moreton Bay Fig tree a popular attraction in Santa Barbara?
Answer: The Moreton Bay Fig tree became a popular attraction in Santa Barbara due to its remarkable
size and captivating appearance. Visitors were drawn to its immense presence and the fascinating
sight of its exposed roots. Its reputation as the largest Moreton Bay Fig tree in the United States also
contributed to its popularity.
...
Table 8: System message and in-context exemplars for Visual Storytelling (VIST) query.
22
