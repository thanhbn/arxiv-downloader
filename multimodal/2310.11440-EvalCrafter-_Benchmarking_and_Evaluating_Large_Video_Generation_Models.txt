# 2310.11440.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.11440.pdf
# File size: 45231416 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
EvalCrafter: Benchmarking and Evaluating Large Video Generation Models
Yaofang Liu1,2,*Xiaodong Cun1,*Xuebo Liu3Xintao Wang1
Yong Zhang1Haoxin Chen1Yang Liu4‚Ä†Tieyong Zeng4Raymond Chan2‚Ä†Ying Shan1
1Tencent AI Lab2City University of Hong Kong3University of Macau
4The Chinese University of Hong Kong
Project Page: http://evalcrafter.github.io
Abstract
The vision and language generative models have been
overgrown in recent years. For video generation, various
open-sourced models and public-available services have
been developed to generate high-quality videos. However,
these methods often use a few metrics, e.g., FVD [57] or
IS [46], to evaluate the performance. We argue that it is hard
to judge the large conditional generative models from the
simple metrics since these models are often trained on very
large datasets with multi-aspect abilities. Thus, we propose a
novel framework and pipeline for exhaustively evaluating the
performance of the generated videos. Our approach involves
generating a diverse and comprehensive list of 700 prompts
for text-to-video generation, which is based on an analysis of
real-world user data and generated with the assistance of a
large language model. Then, we evaluate the state-of-the-art
video generative models on our carefully designed bench-
mark, in terms of visual qualities, content qualities, motion
qualities, and text-video alignment with 17 well-selected ob-
jective metrics. To obtain the final leaderboard of the models,
we further fit a series of coefficients to align the objective
metrics to the users‚Äô opinions. Based on the proposed human
alignment method, our final score shows a higher correlation
than simply averaging the metrics, showing the effectiveness
of the proposed evaluation method.
1. Introduction
The charm of the large generative models is sweeping the
world, e.g., the well-known ChatGPT and GPT4 [38] have
shown human-level abilities in several aspects, including
coding, solving math problems, and even visual understand-
ing, which can be used to interact with our human beings
using any knowledge in a conversational way. As for the gen-
erative models for visual content creation, Stable Diffusion
*Equal Contribution.
‚Ä†Corresponding Author
EALCRAFTERStyleHumanActionsObjectAnimalweatherFoodvehicleCameramotionlandscapeTextplantelectronic
Figure 1. We propose EvalCrafter, a comprehensive framework for
benchmarking and evaluating the text-to-video models, including
the well-defined prompt types in grey and the multiple evaluation
aspects in black circles.
(SD) [44] and SDXL [41] play very important roles since
they are the most powerful publicly available models that
can generate high-quality images from any text prompts.
Beyond text-to-image (T2I), taming diffusion model
for video generation has also progressed rapidly. Early
works (Imagen-Viedo [23], Make-A-Video [50]) utilize the
cascaded models for video generation directly. Powered by
the image generation priors in SD, LVDM [21] and Mag-
icVideo [70] have been proposed to train the temporal layers
to efficiently generate videos. Apart from the academic pa-
pers, several commercial services also can generate videos
from text or images, e.g., Gen2 [18] and PikaLabs [5]. Al-
though we can not get the technique details of these services,
they are not evaluated and compared with other methods.
However, all current large text-to-video (T2V) models only
use previous GAN-based metrics like FVD [57] for evalua-
tion, which only concerns the distribution matching between
the generated video and the real videos, other than the pairs
between the text prompt and the generated video. Differently,
we argue that a good evaluation method should consider the
1arXiv:2310.11440v3  [cs.CV]  23 Mar 2024

--- PAGE 2 ---
metrics in different aspects, e.g., the motion quality and the
temporal consistency. Also, similar to the large language
models (LLMs), some models are not publicly available and
we can only get access to the generated videos, which fur-
ther increases the difficulties in evaluation. Although the
evaluation has progressed rapidly in the large generative
models, including the areas of LLM [38], MLLM [33], and
T2I [26], it is still hard to directly use these methods for
video generation. The main problem here is that different
from T2I or dialogue evaluation, motion and consistency are
very important to video generation which previous works
ignore.
We make the very first step to evaluate the general T2V
models. In detail, we first build a comprehensive prompt list
containing various everyday objects, attributes, and motions.
To achieve a balanced distribution of well-known concepts,
we start from the well-defined meta types of the real-world
knowledge and utilize the knowledge of the LLM, i.e., Chat-
GPT [38], to extend our meta-prompt to a wide range. Be-
sides the prompts generated by the model, we also select
the prompts from real-world users and T2I prompts. After
that, we obtain the metadata ( e.g., color, size, etc.) from the
prompt for further evaluation. Second, we assess the perfor-
mance of large T2V models from four aspects, i.e., video
quality, text-video alignment, motion quality, and temporal
consistency. For each aspect, we employ several objective
metrics as evaluation measures, and we conduct a user study
to human scores w.r.t. these four aspects. After that, we train
coefficients of the regression model for each aspect, aligning
evaluation scores with user preferences. This enables us to
obtain the final model scores and evaluate new videos using
the trained coefficients.
Overall, we summarize the contribution of our paper as:
‚Ä¢We make the first step of evaluating the large T2V
model and build a comprehensive prompt list with de-
tailed annotations for T2V evaluation.
‚Ä¢We consider the aspects of the video visual quality,
video motion quality, video temporal consistency, and
text-video alignment for the evaluation of video genera-
tion. For each aspect, we align the opinions of humans
and also verify the effectiveness of the proposed metric
by correlation analysis.
‚Ä¢During the evaluation, we also discuss several conclu-
sions and findings, which might also contribute to fur-
ther innovation and development of T2V models.
2. Related Work
2.1. Text-to-Video Generation and Evaluation
Text-to-video (T2V) generation aims to create videos
from text prompts. Early works used Variational AutoEn-coders (V AEs [29]) or generative adversarial networks
(GANs [20]) but often yielded low-quality or domain-
specific results, such as faces [67] or landscapes [51, 64].
Recent methods leverage advancements in diffusion mod-
els [24, 25, 61] and large-scale text-image pretraining [43]
to improve generation quality. Examples include Make-A-
Video [50], Imagen-Video [23], LVDM [21], Align Your
Latent [9], and MagicVideo [70]. Commercial and non-
commercial entities have also shown interest in T2V gen-
eration, with online services like Gen1 [18], Gen2 [18],
and open-source models such as ZeroScope [6], Mod-
elScope [58]. Discord-based servers like Pika-Lab [5] and
Morph Studio [4] have demonstrated competitive results.
However, a fair and detailed benchmark for evaluating
these methods is still lacking. Existing metrics like FVD [57],
IS [46], and CLIP similarity [43] may perform well on previ-
ous in-domain T2I generation methods but do not adequately
assess alignment with input text, motion quality, and tempo-
ral consistency, which are crucial for T2V .
2.2. Evaluations on Large Generative Models
Evaluating the large generative models [38, 41, 44, 55, 56]
is a big challenge for both the NLP and vision tasks. For
the LLMs, current methods design several metrics in terms
of different abilities, question types, and user platform [15,
22, 62, 69, 71]. More details of LLM evaluation and Multi-
model LLM evaluation can be found in recent surveys [11,
68]. Similarly, the evaluation of the multi-modal generative
model also draws the attention of the researchers [8, 63].
For example, Seed-Bench [33] generates the VQA for multi-
modal LLM evaluation.
For the models in visual generation tasks, Imagen [45]
only evaluates the model via user studies. DALL-Eval [14]
assesses the visual reasoning skills and social basis of the
T2I model via both user and object detection algorithm [10].
HRS-Bench [7] proposes a holistic and reliable benchmark
by generating the prompt with ChatGPT [38] and utiliz-
ing 17 metrics to evaluate the 13 skills of the T2I model.
TIFA [26] proposes a benchmark utilizing the visual ques-
tion answering (VQA). However, these methods still work
for T2I evaluation or language model evaluation. For T2V
evaluation, we further consider the quality of motion and
temporal consistency.
3. Benchmark Construction
Our benchmark aims to create a trustworthy prompt list
to evaluate the abilities of various T2V models fairly. To
this end, we first collect and analyze large-scale real-world
users‚Äô prompts. After that, we propose an automatic pipeline
to generate a prompt list with high diversity. Since video
generation is time-consuming, we collect 700 prompts as
our initial version for evaluation with careful annotation. In
2

--- PAGE 3 ---
Describeasceneabout{Count|humans},{motions|None},returninjsonformat,withtheattributeof[someattributes],thekeysofjsonis[attributesnames],thelengthofdescriptionis{length_of_prompt}words.HumanMetaclass:‚ÄúTwo people, a man and a woman, are sitting on a park bench. The man is middle-aged with balding, gray hair. The woman is young with long, black hair in a ponytail.‚ÄùDescrip4onMetadata{‚Äúgender‚Äù: [‚Äúmale‚Äù,‚Äúfemale‚Äù], ‚Äúrace‚Äù: [‚ÄúN/A‚Äù,‚ÄúN/A‚Äù], ‚Äúactivity‚Äù: ["sitting"], ‚Äúage‚Äù: [‚Äúmiddle-aged‚Äù,‚Äúyoung‚Äù],‚Ä¶.}MetaInfo
üé≤
you need to answer `yes` or `no` to identify if the input: ‚Äò{description}‚Äô can actually describe {attributekey}of the subject is {attributevalue}.GPT4GPT4A<ributeConsistencyCheckviaQ&AHumanValidation
‚úÖConsistencyofrecognitionlabels
‚úÖConsistencyofmetadataanddescription
‚úÖDiversityanddistribution
üèÜRealWorldPrompts
YesNoFigure 2. We aim to generate a trustworthy benchmark with detailed prompts for text-to-video evaluation by computer vision model and
users. We show the pipeline above.
Method Ver. Abilities‚Ä†Resolution FPS Open Source Length Speed‚àóMotion Camera
ModelScope 23.03 T2V 256√ó256 8 ‚úì 4s 0.5 min - -
VideoCrafter 23.04 T2V 256√ó256 8 ‚úì 2s 0.5 min - -
ZeroScope 23.06 T2V & V2V 1024 √ó576 8 ‚úì 4s 3 min - -
ModelScope-XL 23.08 I2V & V2V 1280 √ó720 8 ‚úì 4s 8 min+ - -
Show-1 23.10 T2V 576 √ó320 8 ‚úì 4s 10 min - -
Hotshot-XL 23.10 T2V 672 √ó384 8 ‚úì 1s 10 s - -
VideoCrafter1 23.10 I2V & T2V 1024 √ó576 8 ‚úì 2s 3 min - -
Floor33 Pictures 23.08 T2V 1280 √ó720 8 - 2s 4 min - -
PikaLab 23.09 I2V ORT2V 1088 √ó640 24 - 3s 1 min ‚úì ‚úì
Gen2 23.09 I2V ORT2V 896 √ó512 24 - 4s 1 min ‚úì ‚úì
Table 1. The difference in the available diffusion-based text-to-video models. ‚Ä†We majorly evaluate the method of text-to-video genera-
tion (T2V). For related image-to-video generation model (I2V), i.e., ModelScope-XL, we first generate the image by Stable Diffusion v2.1
and then perform image-to-video on the generated content.
this section, we introduce the details of the construction of
our benchmark.
Real-World Data Collection. To better understand the
types of prompts we should generate, we collect prompts
from real-world T2V generation discord users, including the
FullJourney [2] and PikaLab [5]. In total, we gather over
600k prompts with corresponding videos and filter them to
200k by removing repeated and meaningless prompts.
Through analyzing the collected data including aspects
like prompt length and word frequency, we get to know that
most of the prompts contain 3 to 40 words. Besides, we iden-
tify four meta-subject classes for T2V generation: human ,
animal ,object , and landscape . For each type, we
consider the motions and styles of each type, the relationship
between the current metaclass and other metaclasses, and
themotion andcamera motion to construct the bench-
mark. We give more details in the supplementary materials.
Figure 3. The analysis of the proposed benchmark. Each meta type
contains 3 sub-types to increase the generated videos‚Äô diversity.
General Recognizable Prompt Generation. Based on
the metaclasses identified in the previous step, we generate
3

--- PAGE 4 ---
the recognizable prompts with the help of a LLM and human
input. As shown in Fig 2, for each kind of metaclass, we
ask GPT-4 [38] to describe the scenes about this metaclass
and its attributes with randomly sampled meta information.
This way, we get the ground truth for the computer vision
models for evaluation. However, we find that GPT-4 is not
perfect for this task, as the generated attributes are not very
consistent with the generated description. Thus, we involve
a self-check in the benchmark building process where we
use GPT-4 to identify the similarities between the generated
description and each metadata. Finally, we filter the prompts
by ourselves to ensure each prompt is correct and meaningful
for T2V generation.
In addition to the automatically generated prompts, we
also integrate prompts from real-world users and avail-
able T2I evaluation prompts, such as DALL-Eval [14] and
Draw-Bench [45]. We filter and generate the metadata using
GPT-4, choose suitable prompts with corresponding meta-
information as shown in Fig. 2, and check the consistency of
the meta-information.
Benchmark Overview. Overall, we get over 700 prompts
in the metaclasses of human ,animal ,objects , and
landscape . Each class contains the natural scenes, the
stylized prompts, and the results with explicit camera motion
controls. We give a brief view of the benchmark in Fig. 3.
To increase the diversity of the prompts, our benchmark con-
tains 3 different sub-types, where we have a total of 50 styles
and 20 camera motion prompts. We randomly add them in
250 prompts of the whole benchmark. Our benchmark con-
tains an average length of 12.3 words per prompt, which is
similar to the real-world prompts we collected.
4. Evaluation Metrics
Different from previous FID [47] based evaluation met-
rics, we evaluate the T2V models in different aspects, includ-
ing the visual quality of the generated video, the text-video
alignment, the motion quality, and temporal consistency. Be-
low, we give the detailed metrics.
4.1. Overall Video Quality Assessment
We focus on the visual quality of the generated video,
which is crucial for user appeal. As distribution-based meth-
ods like FVD [57] require ground truth videos, we argue
they are unsuitable for general T2V generation cases.
Video Quality Assessment (VQA A, VQA T).We employ
the Dover [60] method to assess generated video quality in
terms of aesthetics and technicality. The technical rating mea-
sures common distortions like noise and artifacts. Dover [60]
is trained on a large-scale dataset with labels ranked by real
users. We denote the aesthetic and technical scores as VQA A
and VQA T, respectively.Inception Score (IS). We also use the inception score [46]
as a video quality assessment index, following previous T2V
generation papers. The inception score evaluates GAN [20]
performance using a pre-trained Inception Network [53] on
the ImageNet [17] dataset. A higher inception score indicates
more diverse generated content.
4.2. Text-Video Alignment
We evaluate the alignment of input text and generated
video in various aspects, including global text prompts, con-
tent correctness, and specific attributes. The details of each
score are as follows.
Text-Video Consistency (CLIP-Score). We use the CLIP-
Score to quantify the discrepancy between input text prompts
and generated videos. Using the pretrained ViT-B/32
CLIP model [43] as a feature extractor, we obtain frame-
wise image embeddings and text embeddings, and compute
their cosine similarity. The overall CLIP-Score is then de-
rived by averaging individual scores across all frames.
Image-Video Consistency (SD-Score). We propose a new
metric, SD-Score, to compare the generated quality with
frame-wise SD [44], considering that most current video
diffusion models are fine-tuned on a base SD with a larger
scale dataset. Using SDXL [41], we generate N1images
{dk}N1
k=1for every prompt and extract visual embeddings
in both generated images and video frames. We calculate
the embedding similarity between the generated videos and
SDXL images, which helps address the concept forgetting
problems when fine-tuning the T2I diffusion model to video
models. The final SD-Score is calculated as:
SSD=1
MMX
i=1(1
NNX
t=1(1
N1N1X
k=1C(emb(xi
t), emb (di
k)))).
(1)
where xi
tis the t-th frame of the i-th video, C(¬∑,¬∑)is the
cosine similarity function, emb(¬∑)means CLIP embedding,
Mis the total number of testing videos, and Nis the total
number of frames in each video, where N1= 5.
Text-Text Consistency (BLIP-BLEU). We also consider
the evaluation between the generated text descriptions of
the video and the input prompt. We utilize BLIP2 [35] for
caption generation and use BLEU [40] for evaluation of text
alignment:
SBB=1
MMX
i=1(1
N2N2X
k=1B(pi, li
k)), (2)
where piis the i-th prompt, B(¬∑,¬∑)is the BLEU similarity
scoring function, {li
k}N2
k=1are BLIP2 generated captions for
i-th video, and N2is set to 5 experimentally.
Object and Attributes Consistency (Detection-Score,
Count-Score and Color-Score). We employ SAM-
Track [13] to analyze the correctness of the video content.
4

--- PAGE 5 ---
We evaluate T2V models on the existence of objects, as
well as the correctness of color and count of objects in text
prompts. Specifically, we assess the Detection-Score, Count-
Score, and Color-Score as follows:
1.Detection-Score (SDet): Measures average object pres-
ence across videos, calculated as:
SDet=1
M1M1X
i=1 
1
KKX
k=1œÉi
tk!
, (3)
where M1is the number of prompts with objects, Kis the
number of frames where detection is performed, and œÉi
tk
is the detection result for frame tkin video i(1 if an ob-
ject is detected, 0 otherwise). In our approach, we perform
detection every I= 5frames. Therefore, K=N
I
.
2.Count-Score (SCount ): Evaluates average object count
difference, calculated as:
SCount =1
M2M2X
i=1 
1‚àí1
KKX
k=1ci
tk‚àíÀÜci
ÀÜci!
,(4)
where M2is the number of prompts with object counts, ci
tk
is the detected object count at frame tkin video i, and ÀÜciis
the ground truth object count for video i.
3.Color-Score (SColor ): Assesses average color accuracy,
calculated as:
SColor =1
M3M3X
i=1 
1
KKX
k=1si
tk!
, (5)
where M3is the number of prompts with object colors and
si
tkis the color accuracy result for frame tkin video i(1 if the
detected color matches the ground truth color, 0 otherwise).
Human Analysis (Celebrity ID Score). Human is important
for the generated videos as shown in our collected real-world
prompts. To this end, we evaluate the correctness of human
faces using DeepFace [48], a popular face analysis toolbox.
We calculate the distance between the generated celebrities‚Äô
faces and real images of the celebrities.
SCIS=1
M4M4X
i=1(1
NNX
t=1( min
k‚àà{1,...,N 3}D(xi
t, fi
k))),(6)
where M4is the number of prompts that contain celebri-
ties,D(¬∑,¬∑)is the Deepface‚Äôs distance function, {fi
k}N3
k=1are
collected celebrities images for i-th prompt, and N3= 3.
Text Recognition (OCR-Score) Another hard case for vi-
sual generation is to generate text in the input prompt. To ex-
amine the abilities of current T2V models for text generation,
we utilize the widely used toolbox PaddleOCR [39] to de-
tect the English text from generated videos. Then, similar to
HRS-Bench [7], we calculate Word Error Rate (WER) [30],
Normalized Edit Distance (NED) [52], Character Error Rate
(CER) [37], and get the average.4.3. Motion Quality
For video, we believe the motion quality is a major dif-
ference from other domains, such as image. To this end, we
consider the quality of motion as one of the main evaluation
metrics in our evaluation system. Here, we consider two
different motion qualities introduced below.
Action Recognition (Action-Score). For videos about hu-
mans, we can easily recognize the common actions via pre-
trained models. We use the MMAction2 toolbox [16] and
the pre-trained VideoMAE V2 model [59] to infer human
actions in generated videos. We take the classification accu-
racy as our Action-Score, focusing on Kinetics 400 action
classes [27].
Average Flow (Flow-Score). We also consider the gen-
eral motion information of the video. To this end, we use
RAFT [54], to extract the dense flows of the video in every
two frames. Then, we calculate the average flow on these
frames to obtain the average flow score of every specific
generated video clip since some methods are likely to gener-
ate still videos that are hard to be identified by the temporal
consistency metrics.
Amplitude Classification Score (Motion AC-Score). Based
on the average flow, we further identify whether the motion
amplitude in the generated video is consistent with the am-
plitude specified by the text prompt. To this end, we set
an average flow threshold œÅthat if surpasses œÅ, one video
will be considered large, and here œÅis set to 5 based on our
subjective observation.
4.4. Temporal Consistency
Temporal consistency is also a very valuable field in our
generated video. To this end, we involve several metrics for
calculation. We list them below.
Warping Error. We first consider the warping error, which
is widely used in previous blind temporal consistency meth-
ods [31, 32, 42]. In detail, we first obtain the optical flow of
each two frames using the pre-trained optical flow estimation
network [54], then, we calculate the pixel-wise differences
between the warped image and the predicted image. We cal-
culate the warp differences on every two frames and calculate
the final score using the average of all the pairs.
Semantic Consistency (CLIP-Temp). Besides pixel-wise
error, we also consider the semantic consistency between ev-
ery two frames, which is also used in previous video editing
works [18, 42]. Specifically, we consider the cosine simi-
larity of the embeddings of each two consecutive frames
(emb(xt), emb (xt+1))of the generated videos and then get
the averages on each two frames.
Face Consistency. Similar to CLIP-Temp, we evaluate the
human identity consistency of the generated videos. Specif-
ically, we select the first frame x1as the reference and cal-
culate the cosine similarity of emb(x1)with{emb(xt)}N
t=2.
Then, we average the similarities as the final score.
5

--- PAGE 6 ---
Figure 4. Overall comparison results on our EvalCrafter benchmark.
Visual Text-Video Motion Temporal
Quality Alignment Quality Consistency
ModelScope 53.09 (7) 54.46 (7) 52.47 (7) 57.80 (6)
ZeroScope 53.41 (6) 51.21 (8) 53.61 (4) 58.91 (5)
Floor33 Pictures 58.78 (5) 61.32 (4) 49.16 (8) 50.24 (8)
PikaLab 60.77 (3) 55.80 (6) 55.77 (2) 65.41 (1)
Gen2 62.51 (1) 60.98 (5) 56.43 (1) 64.41 (2)
VideoCrafter1 60.85 (2) 61.95 (2) 53.08 (5) 55.89 (7)
Show-1 52.19 (8) 62.07 (1) 53.74 (3) 60.83 (3)
Hotshot-XL 60.38 (4) 61.52 (3) 52.98 (6) 59.96 (4)
Table 2. Human-preference aligned results from four different as-
pects, with the rank of each aspect in the brackets.
4.5. User Opinion Alignments
Besides the above objective metrics, we evaluate user
opinions through studies focusing on five main aspects: (1)
Video Quality , indicating the quality of the generated video
where a higher score shows less blur, noise, or other vi-
sual degradation; (2) Text and Video Alignment , examining
the relationships between the generated video and the input
text-prompt, requiring users to evaluate the correctness of
generated motions; (3) Motion Quality , requiring users to
identify the correctness of the generated motions from the
video. (4) Temporal Consistency , assessing frame-wise con-
sistency, varying from Motion Quality , which needs users to
give a rank for high-quality movement; (5) Subjective like-
ness, similar to the aesthetic index, a higher value indicates
the generated video generally achieves human preference,
and we leave this metric used directly.
For evaluation, we generate videos using the provided
prompts benchmark on five state-of-the-art methods of Mod-
elScope [58], ZeroScope [6], Gen2 [18], Floor33 [1], and
PikaLab [5], getting 2.5k videos in total. For a fair compar-
ison, we change the aspect ratio of Gen2 and PikaLab to
16 : 9 to suitable other methods. Also, since PikaLab can not
generate the content without the visual watermark, we add
the watermark of PikaLab to all other methods for a fair com-
parison. We also consider that some users might not under-
stand the prompt well, for this purpose, we use SDXL [41]
Figure 5. The raw ratings from our user study.
to generate three reference images of each prompt to help the
users understand better, which also inspires us to design an
SD-Score to evaluate the models‚Äô text-video alignments. For
each metric, we ask 7 users to give opinions between 1 to 5,
where a large value indicates better alignments. The video
sequence has been randomly shuffled before being given
to users, and we get 8647 feedback scores in total. Finally,
after filtering, we keep 1024 most objective and professional
scores as illustrated in Fig. ??.
Upon collecting user data, we proceed to perform hu-
man alignment for our evaluation metrics, with the goal of
establishing a more reliable and robust assessment of T2V
algorithms. Initially, we conduct alignment on the data using
the mentioned individual metrics above to approximate hu-
man scores for the user‚Äôs opinion in specific aspects. Similar
to the works of the evaluation of natural language process-
ing [19, 34], we employ a linear regression model to fit the
parameters in each dimension. Specifically, we randomly
choose 80% samples from four different methods as the fit-
tings samples and left the rest 20% samples to verify the
effectiveness of the proposed method. The coefficient param-
eters are obtained by minimizing the residual sum of squares
between the human labels and the prediction from the linear
regression model. In the subsequent stage, we integrate the
aligned results of these four aspects and calculate the total
score to obtain a comprehensive final score.
5. Results
We conduct the evaluation on our benchmark prompts,
where each prompt has a metafile for additional information
as the answer of evaluation. We then generate the videos us-
ing all available high-resolution T2V models, including the
ModelScope [58], Floor33 Pictures [1], and ZeroScope [6],
Show-1 [66], Hotshot-XL [3], and VideoCrafter1 [12]. We
6

--- PAGE 7 ---
Figure 6. Raw results in different aspects. We consider 4 main meta types ( animal ,human ,landscape ,object ) to evaluate the
performance of the meta types of the generated video, where each type contains several prompts with fine-grained attribute labels. For each
prompt, we also consider the style of the video, yet more diverse prompts. as shown in realistic andstyle figure above. (The metrics
values are normalized for better visualization, the Warping Error, Celebrity ID Score, and OCR-Score by 1‚àíso that large values indicate
better performance.)
keep all the hyper-parameters, such as classifier-free guid-
ance, as the default value. For the service-based model, we
evaluate the performance of the representative works of
Gen2 [18] and PikaLab [5]. They generate at least 512p
videos with high-quality watermark-free videos. We run all
available models on an NVIDIA A100 for speed compar-
ison. We first show the overall human-aligned results in
Fig. 4, with also the different aspects of our benchmark in
Table 2, which gives us the final and the main metrics of
our benchmark. Finally, as in Fig. 6, we give the results
of each method on 4 different meta types ( i.e.,animal ,
human ,landscape ,object ) and two different types of
videos ( i.e.,realistic ,style ) in our benchmark.
5.1. Analysis on Human Preference Alignment
To demonstrate the effectiveness of our model in aligning
with human scores, we calculate Spearman‚Äôs rank correla-
tion coefficient [65] and Kendall‚Äôs rank correlation coeffi-
cient [28], both of which are non-parametric measures of
rank correlation. These coefficients provide insights intothe magnitude and direction of the association between our
method results and human scores, as listed in Table. 3. From
this table, the proposed weighting method shows a better cor-
relation on the unseen 20% samples than directly averaging.
5.2. Findings
Finding #1: Single dimension evaluation is insufficient
for nowadays T2V models. Models‚Äô rankings in Table. 2
vary significantly across different aspects, emphasizing the
importance of a multi-aspect evaluation approach for a com-
prehensive understanding of model performance.
Finding #2: Meta type evaluation is necessary. As shown
in Fig. 6, models perform differently in various meta types,
highlighting the importance of evaluating their abilities by
meta type. For example, Gen2 [18] behaves better than
Floor33 Pictures [1] w.r.t. VQA Ainhuman ,animal , and
style videos. Contrarily, it falls behind Floor33 Pictures
inlandscape ,object , andrealistic ones.
Finding #3: Users prioritize visual appeal over T2V align-
ment. As shown in ??, despite Gen2 [18] performing rela-
7

--- PAGE 8 ---
tively badly in T2V alignment, it surpasses all other models
inSubjective Likeness . We argue that it is because
users prefer videos with better visual appeal like good visual
quality and high temporal consistency.
Finding #4: All methods cannot perform camera mo-
tion control directly using text prompt. Although some
additional hyper-parameters can be set as additional control
handles for Gen2 [18] and PikaLab [5], all current T2V mod-
els still lack the understanding of open-world prompts, like
camera motion.
Finding #5: Resolution doesn‚Äôt correlate much with vi-
sual appeal. As shown in Table. 1 and Table. 2, Gen2 [18]
and Hotshot-XL [3] have small resolutions but are both com-
petitive in visual quality.
Finding #6: Larger motion amplitude doesn‚Äôt ensure user
preference. In our study, most videos that users are fond of
are with slight movements, such as those videos generated
by PikaLab [5] and Gen2 [18].
Finding #7: Generating text remains challenging. Most
methods struggle to generate high-quality and consistent text
from prompts, as evident from OCR-Scores. Raw results of
all metrics are given in supplementary materials.
Finding #8: Many models can sometimes generate com-
pletely wrong videos. From our study, we find quite a num-
ber of failure cases like severe noises and distortion from our
baseline models such as ZeroScope [6], ModelScope [58]
and Floor33 Pictures [1]. We argue that it could be viewed
as a catastrophic forgetting problem [49], as we know many
current T2V models are finetuned from base models like
SD [44]. We present our detailed qualitative results in sup-
plementary materials.
Finding #9: Effective metrics and not that effective
metrics. Metrics like Warp Error, CLIP-Temp, VQA T,
and VQA Aseem to perform well as they all have high
correlations with human scores shown in Table. 3. How-
ever, some metrics are not as good as we think. The Clip-
Score especially, which is a widely used metric in previous
works [18,23,50], only has Spearsman‚Äôs œÅ6.3 and Kendall‚Äôs
œï4.3 compared to BLIP-BLEU in the same aspects has 26.7
and 19.0. Detailed correlation results can be found in the
supplementary materials.
Finding #10: All current models are not satisfactory
enough. From our objective evaluation and subjective ob-
servation, we argue that T2V models nowadays still have
lots to improve. Even for the best model in our evaluation,
Gen2 [18] also has limitations like struggling with complex
scenes, instruction following, and entity details.
5.3. Limitation
Although we have already made a step in evaluating the
T2V generation, there are still many challenges. (i)Currently,
we only collect 700 prompts as the benchmark, where the
real-world situation is very complicated. More prompts willAspects MethodsSpearsman‚Äôs Kendall‚Äôs
œÅ œï
Visual
QualityVQA A 42.1 30.5
VQA T 53.6 39.1
Avg. 55.0 41.0
Ours 55.4 41.1
Motion
AmplitudeMotion AC -22.1 -16.4
Flow-Score -43.3 -30.1
Avg. -38.2 -27.7
Ours 45.0 32.4
Temporal
ConsistencyCLIP-Temp 49.8 35.7
Warping Error 69.0 51.7
Avg. 54.4 38.9
Ours 56.7 41.5
TV
AlignmentCLIP-Score 6.3 4.3
BLIP-BLEU 26.7 19.0
Avg. 31.9 22.7
Ours 32.3 22.5
Table 3. Correlation Analysis. Correlations between some objective
metrics and human judgment on text-to-video generations. We use
Spearsman‚Äôs œÅand Kendall‚Äôs œïfor correlation calculation.
show a more general benchmark. (ii)Evaluating the motion
quality of the general senses is also hard. However, in the era
of multi-model LLM and large video foundational models,
we believe better and larger video understanding models will
be released and we can use them as our metrics. (iii)The
labels used for alignment are collected from only fewer
human annotators, which may introduce some bias in the
results. To address this limitation, we plan to expand the
pool of annotators and collect more diverse scores to ensure
a more accurate and unbiased evaluation.
6. Conclusion
Exploring the capabilities of large generative models is
crucial for improving model design and utilization. In this
paper, we take the first step towards evaluating large, high-
quality T2V models by constructing a comprehensive prompt
benchmark for T2V assessment. We also provide several
objective evaluation metrics to measure T2V model per-
formance concerning video quality, text-video alignment,
temporal consistency, and motion quality. Furthermore, we
conduct human alignment to correlate user scores with objec-
tive metrics, resulting in accurate evaluation metrics for T2V
methods. Our experiments demonstrate that the proposed
methods effectively align with user opinions, thus provid-
ing a reliable assessment of T2V approaches. We believe
this comprehensive evaluation benchmark will serve as a
foundation and foster development for future research.
8

--- PAGE 9 ---
Acknowledgments
Xuebo Liu was sponsored by CCF-Tencent Rhino-Bird
Open Research Fund. We would like to thank the anonymous
reviewers and meta-reviewer for their insightful suggestions.
References
[1]Floor33 pictures discord server. https : / / www .
morphstudio.com/ . Accessed: 2023-08-30. 6, 7, 8, 12,
14
[2]Fulljourney discord server. https : / / www .
fulljourney.ai/ . Accessed: 2023-08-30. 3, 12
[3]Hotshot-xl. https : / / huggingface . co /
hotshotco/Hotshot- XL . Accessed: 2023-10-11.
6, 8, 12, 14
[4]Morph studio discord server. https : / / www .
morphstudio.com/ . Accessed: 2023-08-30. 2
[5]Pika Lab discord server. https://www.pika.art/ .
Accessed: 2023-08-30. 1, 2, 3, 6, 7, 8, 12, 13, 14
[6]Zeroscope. https : / / huggingface . co /
cerspense / zeroscope _ v2 _ 576w . Accessed:
2023-08-30. 2, 6, 8, 12, 14
[7]EslamMohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan-
Farooq Khan, LiErran Li, and Mohamed Elhoseiny. Hrs-
bench: Holistic, reliable and scalable benchmark for text-to-
image models. Apr 2023. 2, 5
[8]Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang
Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng
Yu, Willy Chung, et al. A multitask, multilingual, multimodal
evaluation of chatgpt on reasoning, hallucination, and interac-
tivity. arXiv preprint arXiv:2302.04023 , 2023. 2
[9]Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 2
[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-end object detection with transformers. In European con-
ference on computer vision , pages 213‚Äì229. Springer, 2020.
2
[11] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie
Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang,
Yidong Wang, et al. A survey on evaluation of large language
models. arXiv preprint arXiv:2307.03109 , 2023. 2
[12] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xi-
aodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng
Chen, Xintao Wang, et al. Videocrafter1: Open diffusion
models for high-quality video generation. arXiv preprint
arXiv:2310.19512 , 2023. 6, 12
[13] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin
Yang, Wenguan Wang, and Yi Yang. Segment and track
anything. arXiv preprint arXiv:2305.06558 , 2023. 4
[14] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Prob-
ing the reasoning skills and social biases of text-to-image
generative transformers. 2, 4[15] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David
Jurgens. Do llms understand social knowledge? evaluating the
sociability of large language models with socket benchmark.
arXiv preprint arXiv:2305.14938 , 2023. 2
[16] MMAction2 Contributors. Openmmlab‚Äôs next generation
video understanding toolbox and benchmark. https://
github.com/open-mmlab/mmaction2 , 2020. 5
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
Fei-Fei. Imagenet: A large-scale hierarchical image database.
In2009 IEEE Conference on Computer Vision and Pattern
Recognition , pages 248‚Äì255, 2009. 4
[18] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models.
arXiv preprint arXiv:2302.03011 , 2023. 1, 2, 5, 6, 7, 8, 12,
14
[19] Kallirroi Georgila, Carla Gordon, V olodymyr Yanov, and
David Traum. Predicting ratings of real dialogue participants
from artificial data and ratings of human dialogue observers.
InProceedings of the Twelfth Language Resources and Eval-
uation Conference , pages 726‚Äì734, 2020. 6
[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2, 4
[21] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
long video generation. 2022. 1, 2
[22] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
Measuring mathematical problem solving with the math
dataset. arXiv preprint arXiv:2103.03874 , 2021. 2
[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,
Mohammad Norouzi, David J Fleet, et al. Imagen video:
High definition video generation with diffusion models. arXiv
preprint arXiv:2210.02303 , 2022. 1, 2, 8
[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 2
[25] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,
Mohammad Norouzi, and David J. Fleet. Video diffusion
models, 2022. 2
[26] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari
Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate
and interpretable text-to-image faithfulness evaluation with
question answering. arXiv preprint arXiv:2303.11897 , 2023.
2
[27] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-
man action video dataset. arXiv preprint arXiv:1705.06950 ,
2017. 5
[28] Maurice George Kendall. Rank correlation methods. 1948. 7
[29] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 2
9

--- PAGE 10 ---
[30] Dietrich Klakow and Jochen Peters. Testing the correlation
of word error rate and perplexity. Speech Communication ,
38(1-2):19‚Äì28, 2002. 5
[31] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,
Ersin Yumer, and Ming-Hsuan Yang. Learning blind video
temporal consistency. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 170‚Äì185, 2018. 5,
15
[32] Chenyang Lei, Yazhou Xing, and Qifeng Chen. Blind video
temporal consistency via deep video prior. In Advances in
Neural Information Processing Systems , 2020. 5
[33] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge,
and Ying Shan. Seed-bench: Benchmarking multimodal llms
with generative comprehension. Jul 2023. 2
[34] Dingquan Li, Tingting Jiang, and Ming Jiang. Unified quality
assessment of in-the-wild videos with mixed datasets training.
International Journal of Computer Vision , 129:1238‚Äì1257,
2021. 6
[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-
2: Bootstrapping language-image pre-training with frozen
image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2023. 4
[36] George A Miller. Wordnet: a lexical database for english.
Communications of the ACM , 38(11):39‚Äì41, 1995. 12
[37] Andrew Cameron Morris, Viktoria Maier, and Phil Green.
From wer and ril to mer and wil: improved evaluation mea-
sures for connected speech recognition. In Eighth Interna-
tional Conference on Spoken Language Processing , 2004.
5
[38] OpenAI. Gpt-4 technical report, 2023. 1, 2, 4
[39] PaddlePaddle. Paddleocr. https://github.com/
PaddlePaddle/PaddleOCR , 2013. 5
[40] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics , pages 311‚Äì318,
Philadelphia, Pennsylvania, USA, July 2002. Association for
Computational Linguistics. 4
[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,
Tim Dockhorn, Jonas M ¬®uller, Joe Penna, and Robin Rombach.
Sdxl: improving latent diffusion models for high-resolution
image synthesis. arXiv preprint arXiv:2307.01952 , 2023. 1,
2, 4, 6, 14
[42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero:
Fusing attentions for zero-shot text-based video editing.
arXiv:2303.09535 , 2023. 5
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 2, 4
[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models, 2021. 1, 2, 4, 8, 14[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael
Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-
torealistic text-to-image diffusion models with deep language
understanding. Advances in Neural Information Processing
Systems , 35:36479‚Äì36494, 2022. 2, 4
[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. Advances in neural information processing
systems , 29, 2016. 1, 2, 4
[47] Maximilian Seitzer. pytorch-fid: FID Score for PyTorch.
https://github.com/mseitzer/pytorch-fid ,
August 2020. Version 0.3.0. 4
[48] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended light-
face: A facial attribute analysis framework. In 2021 Interna-
tional Conference on Engineering and Emerging Technolo-
gies (ICEET) , pages 1‚Äì4. IEEE, 2021. 5
[49] Chenze Shao and Yang Feng. Overcoming catastrophic forget-
ting beyond continual learning: Balanced training for neural
machine translation. arXiv preprint arXiv:2203.03910 , 2022.
8
[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran
Gafni, et al. Make-a-video: Text-to-video generation without
text-video data. arXiv preprint arXiv:2209.14792 , 2022. 1, 2,
8
[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-
seiny. Stylegan-v: A continuous video generator with the
price, image quality and perks of stylegan2. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3626‚Äì3636, 2022. 2
[52] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Can-
jie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu,
Dimosthenis Karatzas, et al. Icdar 2019 competition on large-
scale street view text with partial labeling-rrc-lsvt. In 2019
International Conference on Document Analysis and Recog-
nition (ICDAR) , pages 1557‚Äì1562. IEEE, 2019. 5
[53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1‚Äì9, 2015. 4
[54] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In Computer Vision‚ÄìECCV 2020:
16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part II 16 , pages 402‚Äì419. Springer, 2020.
5
[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ¬¥ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971 , 2023. 2
[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-
jad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya
Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open foundation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288 , 2023. 2
10

--- PAGE 11 ---
[57] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric &
challenges. arXiv preprint arXiv:1812.01717 , 2018. 1, 2, 4
[58] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Xiang Wang, and Shiwei Zhang. Modelscope text-to-video
technical report. arXiv preprint arXiv:2308.06571 , 2023. 2,
6, 8, 12, 14
[59] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan
He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling
video masked autoencoders with dual masking, 2023. 5
[60] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-
wen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,
and Weisi Lin. Exploring video quality assessment on user
generated contents from aesthetic and technical perspectives.
InInternational Conference on Computer Vision (ICCV) ,
2023. 4
[61] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang
Xu, Zuxuan Wu, and Yu-Gang Jiang. A survey on video
diffusion models. arXiv preprint arXiv:2310.10647 , 2023. 2
[62] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu
Chen, and Jian Zhang. On the tool manipulation capability of
open-source large language models, 2023. 2
[63] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 2
[64] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho
Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with
dynamics-aware implicit generative adversarial networks. In
International Conference on Learning Representations , 2022.
2
[65] Jerrold H Zar. Spearman rank correlation. Encyclopedia of
Biostatistics , 7, 2005. 7
[66] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui
Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng
Shou. Show-1: Marrying pixel and latent diffusion models for
text-to-video generation. arXiv preprint arXiv:2309.15818 ,
2023. 6, 12
[67] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,
Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learn-
ing realistic 3d motion coefficients for stylized audio-driven
single image talking face animation, 2022. 2
[68] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. A survey of large language models.
arXiv preprint arXiv:2303.18223 , 2023. 2
[69] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,
Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and
chatbot arena, 2023. 2
[70] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint
arXiv:2211.11018 , 2022. 1, 2[71] Gu Zhouhong, Zhu Xiaoxuan, Ye Haoning, Zhang Lin, Wang
Jianchen, Jiang Sihang, Xiong Zhuozhi, Li Zihan, He Qianyu,
Xu Rui, Huang Wenhao, Zheng Weiguo, Feng Hongwei, and
Xiao Yanghua. Xiezhi: An ever-updating benchmark for
holistic domain knowledge evaluation. arXiv:2304.11679 ,
2023. 2
11

--- PAGE 12 ---
Appendices
Appendix Contents
A . Detailed Analysis of Real-World User Data 12
A.1 . Prompt Length Distribution . . . . . . . . . 12
A.2 . Important Words in Prompts . . . . . . . . . 12
A.3 . Meta Classes in Prompts . . . . . . . . . . . 12
B . Quantitative Results 12
B.1. Raw Results of Every Metric for Every Model 12
B.2. Correlations Between Metrics and Human
Labels . . . . . . . . . . . . . . . . . . . . 12
C . Qualitative Results 13
C.1. Content Generation . . . . . . . . . . . . . 14
C.2. Motion Generation . . . . . . . . . . . . . . 14
C.3. Style Generation . . . . . . . . . . . . . . . 14
C.4. Camera Motion Control . . . . . . . . . . . 14
C.5. Task-Specific Generation . . . . . . . . . . . 14
D . Additional Analysis and Explanations 14
D.1 . Adequacy of 700 Prompts . . . . . . . . . . 14
D.2. User study‚Äôs Demographics, Interface, and
Adequacy . . . . . . . . . . . . . . . . . . 15
D.3 . Crowdsourced Prompts & Dataset Dynamics 15
D.4 . Motion Amplitude Metrics Discrepancy . . . 15
D.5 . Dependence on Pre-trained Models . . . . . 15
D.6 . Costs of Evaluation . . . . . . . . . . . . . . 15
D.7 . Benchmark Stability and Noise Variables . . 15
D.8 . Warping Error . . . . . . . . . . . . . . . . 15
D.9 . Discrepancy in User Study and Evaluation . 15
A. Detailed Analysis of Real-World User Data
In this section, we present a detailed analysis of the real-
world user data collected from text-to-video (T2V) gen-
eration discord users, including the FullJourney [2] and
PikaLab [5]. We provide insights into the distribution of
prompt lengths, important words, and meta classes.
A.1. Prompt Length Distribution
Fig. 7 (a) shows the distribution of prompt lengths in
the real-world user data. We find that 90% of the prompts
contain words in the range of [3,40]. This observation helps
us determine the appropriate length for the prompts in our
benchmark.
A.2. Important Words in Prompts
Fig. 7 (b) presents a word cloud of all words in the real-
world user data. From this word cloud, we can observe the
most frequent words in the prompts and gain insights into
the key concepts that users request in T2V generation.A.3. Meta Classes in Prompts
Fig. 7 (c) shows the distribution of noun types in the real-
world user data. We use WordNet [36] to identify the meta
classes. Excluding communication, attribute, and cognition
words, we find that artifacts (human-made objects), humans,
animals, and locations (landscapes) play important roles
in the prompts. We also include the most important word
style from Fig. 7 (b) in the meta classes.
Based on this analysis, we divide the T2V generation into
four meta-subject classes: human ,animal ,object , and
landscape . This classification helps us create a diverse
and representative benchmark for evaluating T2V models.
B. Quantitative Results
In this part, we present the quantitative results of our
evaluation benchmark. We have conducted experiments on
various state-of-the-art video generative models and assessed
their performance using 17 objective metrics. We provide
the raw results of every metric for each model and the cor-
relations between metrics and human labels. The results are
illustrated in two tables. The first table (Table 4) shows the
raw results of every metric for each model. The second ta-
ble (Table 5) displays the correlations between metrics and
human labels.
B.1. Raw Results of Every Metric for Every Model
Table 4 shows the raw results of all 17 introduced metrics
for each of the evaluated models. All metrics are expressed
as percentages, except for Warping Error and Flow-Score.
The table is organized as follows:
‚Ä¢ The first column lists the metrics used for evaluation.
‚Ä¢The following columns display the raw results for
each model, including ModelScope [58], Floor33
Pictures [1], and ZeroScope [6], Show-1 [66],
Hotshot-XL [3], VideoCrafter1 [12], Gen2 [18], and
PikaLab [5].
‚Ä¢Arrows next to the metric names indicate whether
higher ( ‚Üë) or lower ( ‚Üì) values are better for that par-
ticular metric. For Flow-Score, the arrow is replaced
with a rightwards arrow ( ‚Üí) as it is a neutral metric.
B.2. Correlations Between Metrics and Human La-
bels
In addition to the raw results, Table 5 presents the correla-
tion analysis between objective metrics and human judgment
on T2V generations. We use Spearman‚Äôs œÅand Kendall‚Äôs œï
for correlation calculation. The table is organized into four
sections, representing the four aspects of the evaluation: vi-
sual quality, motion amplitude, temporal consistency, and
12

--- PAGE 13 ---
(a) Prompt length distribution.
 (b) All words cloud
 (c) types of noun
Figure 7. The analysis of the real-world prompts from PikaLab Server [5].
Metrics Gen2 ModelScope Pika Floor33 ZeroScope VideoCrafter Show-1 Hotshot
VQA A‚Üë 59.44 40.06 59.09 58.7 34.02 66.18 23.19 71.54
VQA T‚Üë 76.51 32.93 64.96 52.64 39.94 58.93 44.24 50.52
IS‚Üë 14.53 17.64 14.81 17.01 14.48 16.43 17.65 17.29
CLIP-Temp ‚Üë 99.94 99.74 99.97 99.6 99.84 99.78 99.77 99.74
Warping Error ‚Üì 0.0008 0.0162 0.0006 0.0413 0.0193 0.0295 0.0067 0.0091
Face Consistency ‚Üë 99.06 98.94 99.62 99.08 99.33 99.48 99.32 99.48
Action-Score ‚Üë 62.53 72.12 71.81 71.66 67.56 68.06 81.56 66.8
Motion AC-Score ‚Üë 44.0 42.0 44.0 74.0 50.0 50.0 50.0 56.0
Flow-Score ‚Üí 0.7 6.99 0.5 9.26 4.5 5.44 2.07 5.06
CLIP-Score ‚Üë 20.53 20.36 20.46 21.02 20.2 21.33 20.66 20.33
BLIP-BLUE ‚Üë 22.24 22.54 21.14 22.73 21.2 22.17 23.24 23.59
SD-Score ‚Üë 68.58 67.93 68.57 68.7 67.79 68.73 68.42 67.65
Detection-Score ‚Üë 64.05 50.01 58.99 52.44 53.94 67.67 58.63 45.7
Color-Score ‚Üë 37.56 38.72 34.35 41.85 39.25 45.11 48.55 42.39
Count-Score ‚Üë 53.31 44.18 51.46 58.33 41.01 58.11 44.31 49.5
OCR-Score ‚Üì 75.0 71.32 84.31 87.48 82.58 88.04 58.97 63.66
Celebrity ID Score ‚Üì 41.25 44.56 45.21 40.07 46.93 40.18 37.93 38.58
Table 4. Raw results of 17 introduced metrics among the aspects of video quality, text-video alignment, motion quality, and temporal
consistency. All metrics are expressed as percentages, except for Warping Error and Flow-Score.
text-video alignment. In each section, we compare various
methods with our proposed evaluation method, which is
highlighted in bold.
As can be seen from the table, our method consistently
achieves higher correlation values compared to the average
of other methods. This shows the effectiveness of our pro-
posed evaluation method in aligning the objective metrics
to users‚Äô opinions. For instance, in the visual quality aspect,
our method obtains a Spearman‚Äôs œÅof 55.4 and a Kendall‚Äôs
œïof 41.1, which are both higher than the average values of
55.0 and 41.0, respectively. Similar improvements can be
observed in other aspects as well.
In addition to the findings mentioned earlier, we can ob-
serve that some metrics show negative correlations with
human judgment, such as Color-Score and OCR-Score in the
TV Alignment aspect. This indicates that these metrics may
not be reliable for evaluating the alignment between text and
video content in generative models. On the other hand, met-rics like Detection-Score and Count-Score exhibit relatively
higher correlations with human judgment, suggesting their
potential usefulness in evaluating T2V alignment.
Overall, the results in Table 5 provide a comprehensive
analysis of various objective metrics and their correlations
with human judgment. These results can be valuable for
researchers and practitioners in the field of T2V generation
to select appropriate metrics for evaluating their models
and to better understand the strengths and weaknesses of
different evaluation methods.
C. Qualitative Results
In this part, we present qualitative results of the evaluated
T2V models for various aspects of video generation, taking
into account the findings listed in the paper. The results are
visualized in Fig. 11 to Fig. 13. We discuss the performance
of each model in terms of camera motion control, content
generation, motion generation, style generation, and task-
13

--- PAGE 14 ---
Aspects Methods Spearman Kendall
Visual
QualityVQA A 47.8 35.5
VQA T 53.6 39.1
IS 9.9 4.3
Avg. 54.9 40.9
Ours 55.4 41.1
Motion
AmplitudeAction-Score -14.9 -10.4
Motion AC -22.1 -16.4
Flow-Score -43.3 -30.1
Avg. -38.2 -27.7
Ours 45.0 32.4
Temporal
ConsistencyCLIP-Temp 49.7 35.7
Warping Error 69.0 51.7
Face Consistency 25.8 17.8
Avg. 54.4 38.9
Ours 56.7 41.5
TV
AlignmentCLIP-Score 6.3 4.3
BLIP-BLEU 26.7 19.0
SD-Score -2.8 -2.3
Detection-Score 11.9 9.4
Color-Score -5.5 -3.9
Count-Score 28.9 22.2
OCR-Score -8.3 -6.7
Celebrity ID Score -26.0 -19.8
Avg. 31.9 22.7
Ours 32.3 22.5
Table 5. Correlation Analysis. Whole results of correlations be-
tween objective metrics and human judgment on T2V generations.
We use Spearman‚Äôs œÅand Kendall‚Äôs œïfor correlation calculation.
specific generation.
C.1. Content Generation
In Fig. 9, we present the qualitative results of T2V models
and SDXL [41] for four meta types of content generation:
human, object, landscape, and animal. Finding #5 shows that
resolution does not correlate much with visual appeal, as
demonstrated by Gen2 [18] and Hotshot-XL [3], which have
small resolutions but are both competitive in visual quality.
Besides, we can also find that Gen2 [18] and PikaLab [5] are
more distinguishable from SDXL [41] in both video content
and style compared with other methods.
C.2. Motion Generation
Fig. 10 displays the qualitative results of T2V models
with respect to motion generation. According to Finding #6,
larger motion amplitude does not ensure user preference. In
our study, most videos that users are fond of are those with
slight movements, such as those generated by PikaLab [5]
and Gen2 [18].C.3. Style Generation
The qualitative results of T2V models concerning style
generation are shown in Fig. 11. We can see from the figure
that most methods have the ability to generate videos with
specific styles, which may be inherited from base models.
However, various methods like ZeroScope [6] and Mod-
elScope [58] are also struggling to generate high-quality and
consistent styled content from prompts.
C.4. Camera Motion Control
Fig. 12 shows the qualitative results of T2V models in
terms of prompts with camera motion controls. As indicated
by Finding #4, all methodss cannot perform camera motion
control using text prompts, which indicates all T2V models
lack the understanding of camera motion.
C.5. Task-Specific Generation
Finally, Fig. 13 presents the qualitative results of T2V
models and SDXL [41] in terms of different tasks, i.e., face
generation, object generation with color, object generation
with count, text generation, and activity generation. Find-
ing #8 indicates that many models can sometimes generate
completely wrong videos, with severe noises and distor-
tions observed in baseline models like ZeroScope [6], Mod-
elScope [58], and Floor33 Pictures [1]. This could be viewed
as a catastrophic forgetting problem, as many current T2V
models are finetuned from base models like SD [44].
In conclusion, the qualitative results presented in this ap-
pendix provide valuable insights into the strengths and weak-
nesses of different T2V models in various aspects of video
generation. As stated in Finding #10, all current models are
not satisfactory enough, and T2V models still have signif-
icant room for improvement. Even the best model in our
evaluation, Gen2 [18], has limitations like struggling with
complex scenes, instruction following, and entity details.
These results, along with our proposed evaluation frame-
work and pipeline, enable a more exhaustive and reliable
assessment of the performance of large video generation
models.
D. Additional Analysis and Explanations
D.1. Adequacy of 700 Prompts
As an initial attempt, one important reason to use these
prompts is that we find the metrics tend to reach a plateau
with the sample increased as in Fig. 8. Besides, con-
current benchmarks ( e.g., FETV (619 prompts overall),
VBench (100 prompts per metric)) use a similar amount
of prompts. From a practical view, T2V models‚Äô sampling
speed is typically slow, a small but effective benchmark is
crucial for fast evaluation of different methods.
14

--- PAGE 15 ---
D.2. User study‚Äôs Demographics, Interface, and Ad-
equacy
The interface is shown below. We use our internal AI-
testing platform to find human raters. Each rater is asked
to perform 100 tasks as pre-labeling, the annotator who has
more than 90% accuracy will be marked as qualified. Other-
wise, we will consider another supplier. Then, the qualified
annotators will label the whole benchmark.
D.3. Crowdsourced Prompts & Dataset Dynamics
Our approach aims to capture a snapshot of current user
expectations and model abilities. Besides, the benchmark is
intended to be dynamic with periodic updates.
D.4. Motion Amplitude Metrics Discrepancy
It‚Äôs mainly caused by user preferences on favoring subtle
motions as stated by Finding #6, e.g., Gen2 (always generate
small motions) ranks 7th w.r.t. Motion AC-Score in Tab. 4,
but it ranks 1st w.r.t. motion quality in user study, which
resulted in Gen2 ranks 1st in Tab. 2.
D.5. Dependence on Pre-trained Models
As our initial attempt (also the whole community), pre-
trained models provide a reference to find meaningful objec-
tive metrics. We will actively explore more straightforward
metrics to avoid using pre-trained models, e.g., training an
end-to-end evaluation model for each aspect using more user
opinions.
D.6. Costs of Evaluation
We agree that online evaluation is vital for model training.
However, it is impossible to monitor T2V Models in the
running process (even using FVD) since each video sample
requires more than 2 minutes for generation. Our method
is designed for offline evaluation (plays a similar role to
previous FVD evaluation). The whole benchmark requires
around 2 hours on an A100 GPU and at last 16 GB memory
cost without any code optimization, which we think is more
demanding than traditional methods such as FVD. However,
FVD can only reflect one aspect of the T2V model and needs
real video datasets as a reference.
Figure 8. The stability of CLIP-Score with different prompts.
D.7. Benchmark Stability and Noise Variables
We give some stability analysis. First, as in Fig. 8, we
find the objective scores are stable among different methods
with the prompt increasing. Besides, we also try to introduce
noise to the prompts, i.e., adding, removing, or swapping
words/symbols in 100 randomly selected prompts. Notably,
the changes in all scores in Tab. 2 are marginal among the
methods, with most variations below 0.2 points. The ranks
remain consistent across all models.
D.8. Warping Error
The warping error assesses the discrepancy between the
actual subsequent frame and its prediction, generated by
warping the current frame using optical flow. The larger
warping error means each frame changes dramatically, which
is typically unwanted for real-world video. It also widely
used previous blind video consistency methods [31] for tem-
poral consistency metrics.
D.9. Discrepancy in User Study and Evaluation
This discrepancy arose from our endeavors to continu-
ously update and enhance our prompt list. Our initial bench-
mark contains 512 prompts for user study, and we further
expanded it to 700 prompts to make it more comprehensive
and balanced. However, similar to Fig. 8, there are no sig-
nificant changes in our results after the prompt increment.
Therefore, we use the same user study result to avoid wasting
resources as the initial version.
15

--- PAGE 16 ---
Figure 9. Qualitative results of T2V models in terms of four meta types (i.e., human, object, landscape, and animal)
16

--- PAGE 17 ---
Figure 10. Qualitative results of T2V models w.r.t. motion generation
17

--- PAGE 18 ---
Figure 11. Qualitative results of T2V models w.r.t. style generation
18

--- PAGE 19 ---
Figure 12. Qualitative results of T2V models in terms of prompts wtih camera motion controls
19

--- PAGE 20 ---
Figure 13. Qualitative results of T2V models in terms of different tasks (i.e., face generation, object generation with color, object generation
with count, text generation, and activity generation)
20
