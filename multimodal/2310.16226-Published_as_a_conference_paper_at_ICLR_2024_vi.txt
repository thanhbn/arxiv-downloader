# 2310.16226.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.16226.pdf
# Kích thước file: 4550926 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
	TIC-CLIP: HUẤN LUYỆN LIÊN TỤC CỦA CÁC MÔ HÌNH CLIP
Saurabh Garg;˚Mehrdad Farajtabar:Hadi Pouransari:Raviteja Vemulapalli:
Sachin Mehta:Oncel Tuzel:Vaishaal Shankar:Fartash Faghri:
:Apple;Carnegie Mellon University
sgarg2@andrew.cmu.edu ,fartash@apple.com
TÓM TẮT
Việc giữ cho các mô hình nền tảng lớn luôn cập nhật với dữ liệu mới nhất vốn rất tốn kém. Để tránh chi phí cấm đoán của việc liên tục huấn luyện lại, điều bắt buộc là phải huấn luyện liên tục các mô hình này. Vấn đề này trở nên trầm trọng hơn do thiếu bất kỳ benchmark học liên tục quy mô lớn hoặc baseline nào. Chúng tôi giới thiệu bộ benchmark Time-Continual (TiC) đầu tiên ở quy mô web để huấn luyện các mô hình ngôn ngữ-thị giác: TIC-DataComp, TIC-YFCC, và TIC-RedCaps. TIC-DataComp, bộ dữ liệu lớn nhất của chúng tôi, chứa hơn 12,7 tỷ cặp hình ảnh-văn bản có dấu thời gian trải dài 9 năm (2014–2022). Trước tiên, chúng tôi sử dụng các benchmark của mình để tuyển chọn các đánh giá động khác nhau để đo lường độ bền thời gian của các mô hình hiện tại. Chúng tôi chỉ ra rằng CLIP của OpenAI (được huấn luyện trên dữ liệu đến năm 2020) mất «8% độ chính xác zero-shot trên tác vụ truy xuất được tuyển chọn của chúng tôi từ 2021–2022 so với các mô hình được huấn luyện gần đây hơn trong kho lưu trữ OpenCLIP. Sau đó, chúng tôi nghiên cứu cách huấn luyện hiệu quả các mô hình trên dữ liệu liên tục theo thời gian. Chúng tôi chứng minh rằng một phương pháp dựa trên rehearsal đơn giản tiếp tục huấn luyện từ checkpoint cuối cùng và phát lại dữ liệu cũ giảm tính toán 2,5ˆ so với thực tiễn tiêu chuẩn của việc huấn luyện lại từ đầu1.

1 GIỚI THIỆU
Các mô hình nền tảng đa phương thức lớn (Bommasani et al., 2021) đã mang lại những tiến bộ chưa từng có trong việc tạo hình ảnh và khái quát hóa zero-shot, và đã dẫn đến một sự thay đổi mô hình trong học đa phương thức, ví dụ như CLIP (Radford et al., 2021), Flamingo (Alayrac et al., 2022), và Stable Diffusion (Rombach et al., 2022). Các mô hình nền tảng này thường được huấn luyện trên các bộ dữ liệu quy mô web lớn có tính chất cố định và tĩnh. Ví dụ, dữ liệu huấn luyện của CLIP chứa 400 triệu cặp hình ảnh-văn bản, và Stable Diffusion được huấn luyện trên bộ dữ liệu LAION-2B (Schuhmann et al., 2022).

Tuy nhiên, trong thực tế, các mô hình này phải hoạt động trong một môi trường động, nơi thế giới luôn trong trạng thái thay đổi liên tục. Chẳng hạn, internet liên tục phát triển, với hàng petabyte dữ liệu mới được thêm vào hàng ngày (Wenzek et al., 2019; Wiener & Bronson, 2014). Vẫn chưa rõ các mô hình cũ, ví dụ như các mô hình CLIP của OpenAI được huấn luyện trên dữ liệu quy mô internet cho đến năm 2020, hoạt động như thế nào trên dữ liệu tương lai và liệu chúng có cần huấn luyện lại để thích ứng với dữ liệu phát triển theo thời gian hay không.

Chúng tôi bắt đầu bằng việc so sánh độ bền của các mô hình CLIP của OpenAI với các mô hình khác trong kho lưu trữ OpenCLIP được huấn luyện trên các bộ dữ liệu web được tuyển chọn gần đây hơn (ví dụ, LAION-5B, DataComp) chứa dữ liệu đến năm 2022 (Ilharco et al., 2021). Vì không có benchmark hiện tại nào để hiểu độ bền đối với dữ liệu ngôn ngữ-thị giác phát triển theo thời gian, chúng tôi tuyển chọn các tác vụ phân loại và truy xuất động cho các năm 2014–2022 và đánh giá các mô hình CLIP khác nhau (xem Mục 2.2 cho các tác vụ đánh giá của chúng tôi). Chúng tôi đưa ra một quan sát thú vị rằng các mô hình OpenAI thể hiện khoảng cách hiệu suất đáng kể trong truy xuất trên dữ liệu từ 2021–2022 so với 2014–2016 trong khi các mô hình OpenCLIP duy trì hiệu suất của chúng. Ngược lại, các đánh giá tiêu chuẩn như độ chính xác trên các chuyển đổi phân phối ImageNet vẽ nên một bức tranh không đầy đủ rằng các mô hình CLIP của OpenAI bền vững hơn một chút so với các mô hình OpenCLIP (Hình 1). Những phát hiện của chúng tôi không chỉ chứng minh nhu cầu quan trọng để các mô hình thích ứng và phát triển cùng với các phân phối dữ liệu động, mà còn nhấn mạnh những hạn chế của việc chỉ dựa vào các benchmark tĩnh (ví dụ ImageNet).

Một thực tiễn ngây thơ nhưng phổ biến để thích ứng với dữ liệu phát triển theo thời gian là huấn luyện một mô hình CLIP mới từ đầu mỗi khi chúng ta có được một nhóm dữ liệu hình ảnh-văn bản mới. Thực tiễn này có lý do của nó:
˚Công việc được thực hiện trong thời gian thực tập tại Apple.
1Code có sẵn tại https://github.com/apple/ml-tic-clip .
1arXiv:2310.16226v3  [cs.CV]  21 Mar 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
25 50 75
Imagenet accuracy20406080Imagenet dist. shift accuracy
Paradigm Đánh giá Tiêu chuẩn
40 60 80
Retrieval recall on 2014–20164050607080Retrieval recall on 2021–2022
Khoảng cách
hiệu suấtParadigm Đánh giá Được Đề xuất của Chúng tôi
Mô hình OpenAI huấn luyện trên dữ liệu trước 2020 Mô hình OpenClip huấn luyện trên dữ liệu trước 2022
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Total Compute (MACs)×102020253035404550Imagenet and its dist. shifts accuracy
2.7xTiC-Datacomp (L)
Huấn luyện từ đầu với dữ liệu mới +
dữ liệu cũ (thực tiễn tiêu chuẩn)
Khởi đầu ấm + dữ liệu mới
+ phát lại dữ liệu cũ
Hình 1: (Trái, Giữa) Các mô hình OpenAI cho thấy ít độ bền zero-shot hơn trên tác vụ truy xuất từ 2021–2022. Các mô hình OpenCLIP và OpenAI có độ bền tương tự trên các benchmark tiêu chuẩn. Tuy nhiên, các mô hình OpenAI cho thấy ít độ bền hơn trên tác vụ truy xuất của chúng tôi khi so sánh với các mô hình gần đây trong kho lưu trữ OpenCLIP, làm nổi bật tính nhạy cảm đối với phân phối dữ liệu phát triển theo thời gian (Phải) Baseline huấn luyện liên tục đơn giản có tính toán hiệu quả và cạnh tranh với việc huấn luyện lại từ đầu. Các điểm khác nhau biểu thị các mô hình được huấn luyện tuần tự trên TIC-DataComp (L) của chúng tôi khi dữ liệu đến theo thời gian. Huấn luyện khởi đầu ấm với checkpoint trước đó và phát lại tất cả dữ liệu cũ, hoạt động tương tự như Oracle huấn luyện từ đầu mỗi khi dữ liệu mới đến, bằng cách sử dụng ít tính toán hơn 2,7ˆ.

việc khởi tạo huấn luyện từ một mô hình có sẵn có thể gây khó khăn cho việc thay đổi hành vi của mô hình dưới ánh sáng của dữ liệu mới (Ash & Adams, 2020; Achille et al., 2018; Liu et al., 2023). Tuy nhiên, việc huấn luyện các mô hình nền tảng từ đầu đòi hỏi tài nguyên tính toán đáng kể và thường không khả thi để lặp lại thường xuyên. Ví dụ, ViT-g-14 trong Schuhmann et al. (2022); Cherti et al. (2022) được huấn luyện trong 240K giờ GPU A100 tương đương khoảng một tháng trên 400 GPU. Các hướng dẫn huấn luyện thịnh hành tập trung vào các luật tỷ lệ cho huấn luyện CLIP chỉ tập trung vào huấn luyện từ đầu (Cherti et al., 2023). Điều này dẫn đến một câu hỏi then chốt: Làm thế nào chúng ta có thể liên tục cập nhật các mô hình khi phân phối dữ liệu phát triển theo thời gian với các ràng buộc tính toán?

Tồn tại một văn liệu rộng lớn về học liên tục, tập trung vào việc thích ứng các mô hình với môi trường động (Parisi et al., 2019; Hadsell et al., 2020; De Lange et al., 2021). Theo truyền thống, lĩnh vực này tập trung vào các benchmark tăng dần tổng hợp thiếu sự tiến hóa tự nhiên giữa các tác vụ, và do đó, các phương pháp học liên tục hiếm khi được sử dụng trong các kịch bản thế giới thực (Cossu et al., 2022; Lin et al., 2021). Ngược lại, các công trình gần đây tập trung vào các phương pháp học liên tục cho các mô hình CLIP, chủ yếu nhắm vào việc cải thiện hiệu suất trên một tác vụ downstream duy nhất hoặc một chuỗi các tác vụ downstream không liên kết (Ding et al., 2022; Zhou et al., 2023b; Zheng et al., 2023; Ilharco et al., 2022). Trong khi một số công trình gần đây đã bắt đầu giải quyết những vấn đề này, các benchmark hiện tại có quy mô nhỏ hơn nhiều, hoặc thiếu dữ liệu hình ảnh-văn bản được ghép nối (Ni et al., 2023; Lin et al., 2021). Nói đơn giản, có sự khan hiếm các công trình tập trung vào huấn luyện liên tục các mô hình CLIP trên dữ liệu phát triển tự nhiên theo thời gian ở quy mô web.

Chúng tôi thực hiện bước đầu tiên hướng tới huấn luyện Time-Continual (TIC) các mô hình CLIP nơi phân phối dữ liệu phát triển tự nhiên theo thời gian (tổng quan trong Hình 2). Chúng tôi giới thiệu TIC-DataComp, một benchmark mới cho huấn luyện Time-Continual các mô hình CLIP, mà chúng tôi tạo ra bằng cách thêm thông tin "thời gian crawl" vào bộ dữ liệu CommonPool hiện có (Gadre et al., 2023). Chúng tôi cũng tái sử dụng các bộ dữ liệu quy mô web khác được thu thập từ các nguồn đa dạng, chẳng hạn như Reddit và Flickr. Cụ thể, chúng tôi tuyển chọn TIC-YFCC và TIC-RedCaps bằng cách tận dụng thông tin thời gian có sẵn trong YFCC (Thomee et al., 2016) và Redcaps (Desai et al., 2021) tương ứng. Mục tiêu chính của nghiên cứu của chúng tôi trên benchmark này là phát triển các phương pháp học liên tục hoạt động trong ngân sách tính toán bị ràng buộc (giả sử C) mỗi khi một lô dữ liệu mới trở nên có sẵn. Các phương pháp này cạnh tranh với một Oracle, bắt đầu huấn luyện từ đầu mỗi khi dữ liệu mới đến, sử dụng ngân sách tính toán tích lũy.

Để đánh giá các mô hình được huấn luyện trong framework TIC-CLIP của chúng tôi, chúng tôi đánh giá các mô hình trên các tác vụ đánh giá động được đề xuất phát triển theo thời gian cùng với 28 tác vụ phân loại và truy xuất tiêu chuẩn bao gồm ImageNet (Krizhevsky et al., 2012), các chuyển đổi phân phối ImageNet, và Flickr (Plummer et al., 2015), theo cách zero-shot theo công trình của Gadre et al. (2023); Radford et al. (2021).

Cuối cùng, chúng tôi phát triển các phương pháp học liên tục trên các benchmark của mình và thực hiện hơn hai trăm thí nghiệm với các baseline khác nhau sử dụng các checkpoint trước đó (ví dụ, khởi đầu ấm, patching, và distillation), buffer phát lại, và các lịch trình tốc độ học. Những phát hiện của chúng tôi làm nổi bật một điểm mấu chốt: Phương pháp tích lũy bắt đầu huấn luyện ấm với checkpoint mới nhất và phát lại tất cả dữ liệu cũ, đạt hiệu suất cạnh tranh với Oracle trong khi hiệu quả tính toán hơn 2,7ˆ.

2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Mô hình tại thời điểm t -1 Dữ liệu tại thời điểm t
Dữ liệu tại thời điểm 1Dữ liệu tại thời điểm t - 2 Dữ liệu tại thời điểm t -1 …Buffer phát lại với các ràng buộc  Dữ liệu mới
Huấn luyện mô hình CLIP với ràng buộc ngân sách tính toán
Khởi tạo ngẫu nhiên
ORA. Dữ liệu Tổng hợpB. Huấn luyện Liên tụcC. Đánh giá Tĩnh và Động
Tác vụ Đánh giá Tĩnh Tiêu chuẩn
 Tác vụ Đánh giá Động Được Đề xuấtTác vụ tại Thời điểm 1Tác vụ tại Thời điểm t-1…Tác vụ tại Thời điểm tTác vụ tại Thời điểm t + 1Tác vụ tại Thời điểm T …
Chuyển giao Ngược lạiChuyển giao Tiến lênĐánh giá ID  ImageNet (IN), ObjectNet,  IN-v2, IN-R, IN-S, IN-A, IN-O, VTAB, Wilds, v.v.Flickr30k, MSCOCOPhân loại Truy xuất

Hình 2: Quy trình thí nghiệm trên các benchmark liên tục được đề xuất của chúng tôi. (A) Kết hợp dữ liệu mới và cũ với các ràng buộc buffer. (B) Huấn luyện liên tục một mô hình với ngân sách tính toán (giả sử C) bằng cách bắt đầu với checkpoint trước đó hoặc từ đầu. (C) Đánh giá các mô hình trên các bộ dữ liệu tiêu chuẩn và các bộ dữ liệu động được đề xuất của chúng tôi. So sánh với các benchmark khác trong Phụ lục A.

Ngoài ra, các thí nghiệm của chúng tôi chứng minh những đánh đổi thú vị giữa kích thước buffer cho hiệu suất tĩnh và động và cung cấp những hiểu biết có giá trị về các lịch trình tốc độ học cho huấn luyện tuần tự. Kết quả của chúng tôi trải dài trên các quy mô bộ dữ liệu khác nhau (từ 11M mẫu đến 3B) và làm nổi bật các xu hướng với các phương pháp khác nhau phần lớn nhất quán qua các quy mô.

Để làm cho các benchmark của chúng tôi dễ tiếp cận, chúng tôi công khai phát hành code và thông tin thời gian mà chúng tôi thu thập trên các bộ dữ liệu hiện có tại đây. Công trình của chúng tôi chỉ là một bước đầu tiên hướng tới huấn luyện liên tục các mô hình nền tảng, và chúng tôi tin rằng nghiên cứu của chúng tôi sẽ thúc đẩy sự chú ý nhiều hơn đến lĩnh vực chưa được nghiên cứu đủ này.

2 TIC-CLIP: BENCHMARK VÀ QUY TRÌNH THÍ NGHIỆM

Trong phần này, chúng tôi giới thiệu benchmark của mình (Hình 2) tập trung vào việc huấn luyện mô hình nền tảng ngôn ngữ-thị giác với mục tiêu Contrastive Language Image Pretraining (CLIP) (Radford et al., 2021). Đáng chú ý, chúng tôi huấn luyện trên dữ liệu hình ảnh-văn bản đến tuần tự khác với các bộ dữ liệu hình ảnh-văn bản thông thường có tính tĩnh (ví dụ WiT trong CLIP, DataComp trong Gadre et al. (2023)). Chúng tôi tuyển chọn TIC-DataComp, TIC-YFCC, và TIC-RedCaps là các cặp hình ảnh-văn bản có nguồn gốc từ internet mà chúng tôi bổ sung với thông tin thời gian phụ trợ. Chúng tôi cũng giới thiệu các tác vụ đánh giá động để đánh giá hiệu suất của các mô hình được huấn luyện liên tục trên dữ liệu phát triển theo thời gian. Mục tiêu của một learner là huấn luyện một mô hình có thể triển khai ở mỗi bước khi dữ liệu mới trở nên có sẵn với ngân sách tính toán cố định.

2.1 THIẾT KẾ BENCHMARK: CHÚNG TÔI TẠO BỘ DỮ LIỆU TIME-CONTINUAL NHƯ THẾ NÀO?

Để tạo ra huấn luyện liên tục của CLIP, chúng tôi mở rộng các bộ dữ liệu hình ảnh-văn bản hiện có với thông tin thời gian được thu thập từ nguồn gốc của các bộ dữ liệu. Bộ dữ liệu lớn nhất của chúng tôi là TIC-DataComp chứa 12,7 tỷ cặp hình ảnh-văn bản với metadata "thời gian crawl". Chúng tôi tạo bộ dữ liệu này trên cơ sở benchmark DataComp hiện có (Gadre et al., 2023). Chúng tôi cũng tạo TIC-YFCC và TIC-RedCaps trên cơ sở các bộ dữ liệu YFCC15M (Thomee et al., 2016; Radford et al., 2021) và Redcaps (Desai et al., 2021) hiện có để làm nổi bật rằng những phát hiện của chúng tôi có thể áp dụng rộng rãi cho các bộ dữ liệu được tuyển chọn cẩn thận từ các nguồn đa dạng như Reddit và Flickr. Trong khi metadata liên quan đến thời gian vắng mặt trong benchmark DataComp, nó có sẵn trong các bản phát hành gốc của YFCC và Redcaps. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, không có công trình nào trước đây sử dụng thông tin thời gian như vậy cho huấn luyện liên tục các mô hình CLIP.

Chúng tôi hiển thị thống kê bộ dữ liệu cho tất cả các bộ dữ liệu, ví dụ, số lượng ví dụ trong mỗi năm trong App. C.3.

TIC-DataComp Chúng tôi thu thập timestamp cho bộ dữ liệu CommonPool được giới thiệu trong DataComp chứa 12,7B cặp hình ảnh-văn bản (không bao gồm 0,1B cặp không thể truy cập). Bộ dữ liệu này đứng đầu như bộ dữ liệu hình ảnh-văn bản công khai lớn nhất cho đến nay. Nguồn của DataComp là Common Crawl, định kỳ phát hành các snapshot dữ liệu đã crawl web, thường theo tháng kể từ năm 2014 với các trang web mới và được cập nhật. Để xây dựng TIC-DataComp, chúng tôi bổ sung mỗi cặp hình ảnh-văn bản trong DataComp với timestamp đầu tiên của chúng. Chúng tôi đã theo quy trình xây dựng tương tự như DataComp nhưng chỉ giữ lại cặp hình ảnh-văn bản được tìm thấy trong snapshot sớm nhất trong giai đoạn loại bỏ trùng lặp. Quy trình này cung cấp timestamp ở độ chi tiết của tháng, trải dài các năm 2014–2022. Xem App. C.7 để biết chi tiết về quy trình xây dựng. Chúng tôi lưu ý rằng trong khi thông tin thời gian được bổ sung này có thể chứa một số nhiễu, trung bình, chúng tôi thấy nó là một proxy khá chính xác cho thời gian tải lên của các trang web (xem App. C.7).

3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Apple Confidential–Internal Use Only
20142022.  . .Các chủ đề chưa từng thấy trước đó, ví dụ COVID-19 xuất hiện theo thời gian Mount Rainier and Towers 
First Snow Storm of the Season
image of virus that causes sickness coronavirus covid-19A. Tác vụ Truy xuất ĐộngB. Tác vụ Phân loại Động
MaskSports CarPhoneComputer

Hình 3: Phân phối các ví dụ thay đổi từ 2014 đến 2022 trong các tác vụ đánh giá động của chúng tôi. (Trái) Các mẫu cho truy xuất văn bản thành hình ảnh. Đối với các timestamp mới, hình ảnh từ các khái niệm mới xuất hiện (ví dụ, COVID-19). (Phải) Các mẫu từ tác vụ phân loại của chúng tôi cho 4 danh mục. Chúng tôi quan sát thấy rằng không chỉ các vật thể phát triển theo thời gian mà hình ảnh từ các timestamp gần đây cũng được chụp nhiều hơn trong điều kiện thực tế.

Mặc dù benchmark của chúng tôi chứa thông tin thời gian ở độ chi tiết của tháng, chúng tôi giới hạn các thí nghiệm của mình ở độ chi tiết của năm bằng cách hợp nhất dữ liệu cho tất cả các tháng trong một năm. Tương tự như DataComp, benchmark của chúng tôi có thiết kế bao gồm, phù hợp với các tham gia viên có mức độ tài nguyên tính toán khác nhau. Cụ thể, chúng tôi thí nghiệm với các kích thước trung bình, lớn và cực lớn từ CommonPool. Gadre et al. (2023) tận dụng các chiến lược lọc khác nhau để chọn tập huấn luyện. Chúng tôi lo ngại rằng các kỹ thuật lọc làm thiên lệch dữ liệu huấn luyện được chọn. Trong App. C.1, chúng tôi cung cấp bằng chứng sơ bộ rằng việc lọc "Bestpool" sử dụng các mô hình CLIP sẵn có, thực sự làm thiên lệch dữ liệu được chọn theo các bước thời gian cũ. Tuy nhiên, để làm nổi bật tầm quan trọng của những phát hiện của chúng tôi ngay cả đối với các kỹ thuật lọc hiện đại, chúng tôi thí nghiệm với cả lọc Bestpool và lọc Basic (không có lọc CLIP) ở quy mô cực lớn. Đối với các quy mô lớn và trung bình, chúng tôi chỉ thí nghiệm với lọc Basic.

TIC-YFCC Chúng tôi thí nghiệm với tập con 15M của YFCC100M (Thomee et al., 2016), cụ thể là YFCC15M, được OpenAI chọn (Radford et al., 2021). Việc lọc này chỉ giữ lại những hình ảnh có văn bản tự nhiên trong chú thích. YFCC100M chứa dữ liệu từ các năm 2008–2014 và ban đầu được phát hành với timestamp tải lên. Chúng tôi sử dụng thông tin này để tạo các phân chia liên tục ở độ chi tiết của năm.

TIC-RedCaps RedCaps chứa 12M cặp hình ảnh-chú thích từ tập hợp các subreddit được tuyển chọn thủ công qua 2011–2020 (Desai et al., 2021). Chúng tôi sử dụng timestamp tạo của các bài đăng để tạo phân chia cho học liên tục. Tương tự như hai bộ dữ liệu khác, chúng tôi thí nghiệm ở độ chi tiết của năm.

2.2 BỘ THỬ NGHIỆM ĐÁNH GIÁ

Các tác vụ động Chúng tôi tận dụng thông tin thời gian trong các benchmark của mình để tạo các tác vụ đánh giá động. Ở đây, dữ liệu thử nghiệm bao gồm các mẫu thay đổi qua các năm khi thế giới phát triển. Đối với bộ dữ liệu lớn nhất của chúng tôi là TIC-DataComp, chúng tôi tạo các tác vụ động cho cả truy xuất và phân loại như mô tả dưới đây. (ví dụ trong Hình 3 và các ví dụ bổ sung trong App. C.5):

I. Tác vụ truy xuất động: Để tạo một tác vụ truy xuất, chúng tôi lấy mẫu một lô các cặp hình ảnh-văn bản IID từ các timestamp khác nhau và đánh giá hiệu suất truy xuất văn bản cho hình ảnh tương ứng (tương tự, truy xuất hình ảnh cho văn bản tương ứng). Chúng tôi gọi bộ dữ liệu là TIC-DataComp-Retrieval.

II. Tác vụ phân loại động: Chúng tôi cũng tạo một bộ dữ liệu phân loại TIC-DataComp-Net với các lớp ImageNet từ CommonPool và được bổ sung với timestamp. Được lấy cảm hứng từ LAIONNet (Shirali & Hardt, 2023), trước tiên chúng tôi lọc các ví dụ nơi chú thích tương ứng chứa một và chỉ một trong các synset của ImageNet. Sau đó chúng tôi chỉ giữ lại các ví dụ nơi độ tương tự giữa định nghĩa synset ImageNet và chú thích vượt quá ngưỡng 0,5. Chúng tôi đánh giá độ tương tự bằng cách sử dụng một mô hình embedding câu sẵn có (Reimers & Gurevych, 2019). Quan trọng, khác với LAIONNet, chúng tôi không lọc các cặp hình ảnh-văn bản bằng điểm tương tự CLIP để tránh làm thiên lệch quá trình lựa chọn. Chúng tôi mô tả quá trình xây dựng chi tiết hơn trong App. C.5. Trên TIC-DataComp-Net, chúng tôi báo cáo độ chính xác trung bình trên tất cả các lớp và trên các nút đã chọn (ví dụ, xe cơ giới) tại mỗi bước thời gian.

Tương tự, chúng tôi tạo các tác vụ truy xuất cho TIC-YFCC và TIC-RedCaps. Lưu ý rằng chúng tôi loại bỏ các cặp hình ảnh-văn bản được trích xuất cho các tác vụ truy xuất và phân loại động khỏi các tập huấn luyện. Các đánh giá trên các tác vụ động được thực hiện theo cách zero shot.

4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Các tác vụ tĩnh Chúng tôi cũng đánh giá các mô hình trên nhiều tác vụ phân loại và truy xuất theo cách zero-shot như trong Radford et al. (2021). Cụ thể, chúng tôi xem xét 28 tác vụ tiêu chuẩn: 27 tác vụ phân loại hình ảnh, ví dụ, ImageNet và 6 chuyển đổi phân phối của nó (ví dụ, ImageNetv2, ImageNet-R, ImageNet-Sketch, và Objectnet), các bộ dữ liệu từ VTAB và tác vụ truy xuất Flickr30k. Chúng tôi gọi những tác vụ này là tác vụ đánh giá tĩnh. Chúng tôi liệt kê tất cả các bộ dữ liệu trong App. C.2.

Các metric đánh giá Chúng tôi định nghĩa các metric cho các tác vụ phân loại và tác vụ truy xuất dựa trên độ chính xác và Recall@1, tương ứng. Gọi T đại diện cho số bước thời gian mà chúng tôi có dữ liệu. Đối với mỗi phương pháp huấn luyện, chúng tôi tạo ra tổng cộng T mô hình, mỗi mô hình tương ứng với cuối huấn luyện tại một bước thời gian cụ thể. Đối với các bộ dữ liệu tĩnh (ví dụ, ImageNet), chúng tôi báo cáo hiệu suất trung bình của T mô hình. Tuy nhiên, khi xử lý các bộ dữ liệu đánh giá động, chúng tôi đánh giá hiệu suất của mỗi mô hình trong T mô hình trên các bộ dữ liệu đánh giá được thu thập tại tất cả các bước thời gian. Do đó, đối với mỗi mô hình và một tác vụ đánh giá động, chúng tôi thu được T giá trị hiệu suất. Chúng tôi biểu diễn những giá trị này bằng cách sử dụng ma trận hiệu suất E, trong đó mỗi mục Ei,j biểu thị hiệu suất của mô hình thu được sau khi quan sát dữ liệu huấn luyện tại bước thời gian i khi được đánh giá trên bộ dữ liệu từ bước thời gian j. Ma trận hiệu suất E cũng có thể được tóm tắt ngắn gọn bằng cách sử dụng ba metric tiêu chuẩn thường được sử dụng trong các đánh giá học liên tục (Lin et al., 2021; Díaz-Rodríguez et al., 2018):

• Hiệu suất in-domain: hiệu suất trung bình tại mỗi bước thời gian huấn luyện (tức là đường chéo của E)
• Chuyển giao ngược: trung bình trên các bước thời gian trước mỗi bước huấn luyện (tức là tam giác dưới của E)
• Chuyển giao tiến: trung bình trên các bước thời gian sau mỗi bước huấn luyện (tức là tam giác trên của E)

Đôi khi, các metric được mô tả ở trên có thể gây ra metric chuyển giao ngược bị ảnh hưởng bởi các bước thời gian đánh giá sau này, làm thiên lệch metric chuyển giao ngược (tham khảo App. F để biết chi tiết). Do đó, trong App. F, chúng tôi trình bày kết quả sử dụng các metric được sửa đổi làm giảm thiểu vấn đề này.

Trong khi các tác vụ tĩnh nắm bắt hiệu suất trên các benchmark tiêu chuẩn, các tác vụ động nắm bắt các vấn đề do chuyển đổi phân phối (cho chuyển giao tiến) và quên lãng (cho chuyển giao ngược). Mục tiêu trong benchmark của chúng tôi là phát triển các phương pháp học liên tục tối đa hóa hiệu suất trên các tác vụ tĩnh đồng thời tối ưu hóa hiệu suất trên các tác vụ động.

2.3 QUY TRÌNH THÍ NGHIỆM CHO HUẤN LUYỆN

Quy trình streaming Chúng tôi tuân theo một quy trình streaming, nơi dữ liệu được tiết lộ dần dần cho learner trong các lô lớn với mục tiêu đạt được một mô hình có thể triển khai càng sớm càng tốt sau khi mỗi lô đến. Chúng tôi tiến hành thí nghiệm với việc streaming dữ liệu ở độ chi tiết của năm và benchmark của chúng tôi hỗ trợ nghiên cứu tương lai ở độ chi tiết của tháng. Ngoài ra, vì lượng dữ liệu từ các bước thời gian sớm hơn bị hạn chế (xem App. C.3), chúng tôi tổng hợp dữ liệu từ các bước thời gian sớm hơn thành một lô lớn duy nhất và đánh dấu thời gian bằng năm mới nhất trong phạm vi. Sau khi tổng hợp này, chúng tôi có 7 bước thời gian cho TIC-DataComp (2016–2022) và 4 cho cả TIC-YFCC (2011–2014) và TIC-RedCaps (2017–2020). Trong khi số lượng cặp hình ảnh-văn bản được tiết lộ tại mỗi bước thời gian có cùng bậc độ lớn, số lượng chính xác khác nhau giữa các bước và chúng tôi không thay đổi nhân tạo các kích thước.

Ngân sách bộ nhớ Chúng tôi cho phép các phương pháp sử dụng checkpoint mô hình cuối cùng tại mỗi bước vì chi phí giữ một checkpoint mỗi tháng thường không đáng kể. Ngược lại, chi phí giữ lại dữ liệu cũ có thể cao và có thể không được cho phép do các chính sách hết hạn dữ liệu. Do đó, cùng với việc nghiên cứu các phương pháp giữ lại tất cả dữ liệu cũ, chúng tôi cũng khám phá các chiến lược hạn chế tính bền vững dữ liệu (xem Mục 3 để biết chi tiết).

Ngân sách tính toán Để đảm bảo so sánh công bằng giữa các phương pháp, chúng tôi thiết lập một ngân sách tính toán tổng nhất quán, được đo bằng Multiply-Accumulate Operations (MACs), và phân bổ đều cho huấn luyện tại mỗi bước thời gian. Trừ khi được chỉ định khác, đối với tất cả các phương pháp ngoại trừ Oracle và LwF, chúng tôi sử dụng cùng ngân sách tính toán. Đối với các thí nghiệm trên TIC-DataComp, chúng tôi tham khảo các cấu hình tính toán trong DATACOMP cho tính toán tổng thể. Đối với TIC-RedCaps và TIC-YFCC, chúng tôi sử dụng tính toán bậc quy mô trung bình trong TIC-DataComp. Chi tiết ngân sách tính toán có trong App. C.4.

2.4 PHÂN TÍCH CÁC CHUYỂN ĐỔI PHÂN PHỐI TRONG CÁC BENCHMARK ĐƯỢC XÂY DỰNG

Phân tích TIC-DataComp thông qua lăng kính của các tác vụ đánh giá được xây dựng Đầu tiên, chúng tôi phân tích định tính các ví dụ trong tác vụ truy xuất và phân loại của chúng tôi (Hình 3). Chúng tôi quan sát thấy rằng theo thời gian, trong tác vụ truy xuất, các khái niệm mới như COVID-19 xuất hiện. Tương tự, một số lớp ImageNet nhất định phát triển, chẳng hạn như sự chuyển đổi từ khẩu trang "masquerade" sang khẩu trang "phẫu thuật/bảo vệ" trong định nghĩa của chúng. Hơn nữa, khi thời gian trôi qua, chúng tôi quan sát thấy rằng chất lượng hình ảnh cải thiện và nhiều hình ảnh có xu hướng xuất hiện trong điều kiện thực tế trái ngược với hình ảnh nền trắng tập trung. Tiếp theo, chúng tôi so sánh hiệu suất của các mô hình OpenAI và OpenCLIP trên các bộ dữ liệu của chúng tôi. Ở đây, chúng tôi chỉ trình bày những phát hiện chính và ủy quyền thảo luận chi tiết cho App. C.6. Chúng tôi quan sát một khoảng cách hiệu suất đáng kể giữa các mô hình OpenAI và OpenCLIP trên tác vụ truy xuất động của chúng tôi (Hình 1). Khoảng cách này mở rộng đáng kể trên các truy vấn truy xuất nơi chú thích đề cập đến COVID-19. Mặt khác, các mô hình OpenAI và OpenCLIP thể hiện độ bền tương tự cho truy xuất trên dữ liệu đến từ Flickr, làm nổi bật rằng dữ liệu từ một số miền không thể hiện các chuyển đổi gây ra sụt giảm hiệu suất. Đối với tác vụ phân loại của chúng tôi, chúng tôi quan sát một sự giảm rất nhỏ («1%) khi tính trung bình trên tất cả các danh mục. Tuy nhiên, chúng tôi quan sát một khoảng cách đáng kể trên các cây con cụ thể trong ImageNet. Ví dụ, các lớp trong cây con "xe cơ giới" cho thấy khoảng 4% sụt giảm hiệu suất, khi so sánh các mô hình OpenAI và OpenCLIP. Những phát hiện này làm nổi bật rằng trong khi các lớp ImageNet tổng thể có thể vượt thời gian, một số danh mục nhất định có xu hướng phát triển nhanh hơn những danh mục khác. Phân tích định tính và định lượng của chúng tôi trên TIC-DataComp rõ ràng làm nổi bật sự tiến hóa của các phân phối và nắm bắt các thuộc tính khác nhau so với các benchmark tiêu chuẩn.

Phân tích định lượng trên TIC-YFCC Chúng tôi phân tích TIC-YFCC bằng cách sử dụng các encoder câu và hình ảnh sẵn có. Trước tiên, chúng tôi nhúng hình ảnh từ các bước thời gian khác nhau với encoder OpenAI CLIP và sau đó tính toán Frechet Inception Distance (FID; Seitzer (2020)). Khi thời gian trôi qua, chúng tôi quan sát thấy rằng khoảng cách FID tăng lên đối với dữ liệu từ bước thời gian đầu tiên (Hình 18 trong App. C.6). Tương tự, chúng tôi sử dụng sentence transformer được huấn luyện trước để trích xuất top-5 danh mục từ Wordnet Nouns cho mỗi chú thích. Chúng tôi quan sát thấy rằng khoảng cách TV trên phân phối WordNet Nouns phát triển theo thời gian khi so sánh với dữ liệu từ bước thời gian đầu tiên. Thêm chi tiết trong App. C.6.

3 TIC-CLIP: CÁCH HUẤN LUYỆN LIÊN TỤC CÁC MÔ HÌNH CLIP?

Bảng 1: Bảng tóm tắt các phương pháp của chúng tôi. D: kích thước dữ liệu trong mỗi bước, T: tổng số bước thời gian, t: bước thời gian hiện tại, C: ngân sách tính toán (lần lặp).

Phương pháp Mỗi bước Tổng
Kích thước huấn luyện Khởi tạo Tính toán Tính toán
Cumulative-All tD Cuối cùng C TC
Cumulative-Exp 2D Cuối cùng C TC
Cumulative-Equal 2D Cuối cùng C TC
Sequential D Cuối cùng C TC
Restart tD Ngẫu nhiên C TC
Patching D Cuối cùng Patch C TC
LwF D Cuối cùng 1.2ˆC 1.2ˆTC
Oracle˚˚ tD Ngẫu nhiên tCpT`1qT/2C

Trong phần này, chúng tôi đặt ra các phương pháp khác nhau đặc biệt tập trung vào các câu hỏi sau (Bảng 1): (i) Cách sử dụng/phát lại dữ liệu từ các bước thời gian trước; (ii) Cách tận dụng các checkpoint mô hình được huấn luyện trước đó? (iii) Quy trình huấn luyện/tối ưu hóa nên như thế nào?

Các phương pháp phát lại dữ liệu được khởi tạo từ checkpoint cuối cùng chứng minh hiệu suất mạnh mẽ trên các benchmark học liên tục tiêu chuẩn (Mục 5). Chúng tôi xem xét các phương pháp phát lại có/không có khởi tạo từ checkpoint cuối cùng:

I. Oracle: Huấn luyện mô hình CLIP từ đầu (tức là khởi tạo ngẫu nhiên) trên tất cả dữ liệu hình ảnh-văn bản nhận được đến thời điểm t sử dụng ngân sách tính toán lớn tˆC. Oracle đại diện cho một phương pháp cấm đoán đắt đỏ là thực tiễn phổ biến nhất trong huấn luyện các mô hình nền tảng quy mô lớn. Mục tiêu của các phương pháp khác là hoạt động càng gần với Oracle càng tốt trong ngân sách hạn chế của chúng.

II. Cumulative: Huấn luyện mỗi mô hình được khởi tạo từ checkpoint cuối cùng trên hợp của tất cả dữ liệu đến t với ngân sách tính toán C. Phương pháp này tương tự như experience replay (Robins, 1995; Hayes et al., 2019) nhưng với buffer lớn hơn đáng kể so với phổ biến trong văn liệu học liên tục. Với kích thước buffer cố định cho mỗi bước trong quá khứ, chúng tôi quan sát sự khác biệt tối thiểu đến không có giữa lấy mẫu ngẫu nhiên và các chiến lược khác. Sau khi lấy mẫu dữ liệu phát lại, chúng tôi xáo trộn ngẫu nhiên nó cùng với dữ liệu mới để huấn luyện. Chúng tôi xem xét các chiến lược sau để lấy mẫu kích thước buffer mỗi bước:

• -All: Phát lại tất cả dữ liệu trước đó.
• -Exp: Phát lại buffer kích thước D và giảm lượng dữ liệu cũ đi một nửa tại mỗi bước. Ví dụ, tại bước thời gian thứ 3, chúng tôi giữ lại D/2, D/2 dữ liệu cũ và tại bước thứ 4, chúng tôi giữ lại D/4, D/4, D/2 dữ liệu cũ. Cùng với dữ liệu D từ bước hiện tại, phương pháp này huấn luyện tối đa 2D dữ liệu trong mỗi bước.
• -Equal: Phát lại buffer kích thước D nhưng chia buffer đều cho tất cả các năm trước đó. Ví dụ, tại bước thứ 4, chúng tôi giữ lại D/3, D/3, D/3 dữ liệu cũ. Cùng với dữ liệu D từ bước thời gian hiện tại, phương pháp này huấn luyện tối đa 2D dữ liệu trong mỗi bước.

III. Sequential: Huấn luyện chỉ trên dữ liệu mới bắt đầu từ checkpoint tốt nhất của bước thời gian trước đó. Sequential tương tự như Cumulative nhưng không có bất kỳ buffer phát lại nào.

IV. Restart: Huấn luyện mỗi mô hình từ đầu (tức là khởi tạo ngẫu nhiên) trên tất cả dữ liệu đến thời điểm t cho ngân sách tính toán C. Restart tương tự như Oracle nhưng với ngân sách tính toán C tại mỗi bước thời gian và tương tự như Sequential nhưng với khởi tạo ngẫu nhiên. Như vậy, Restart giúp chúng tôi hiểu chuyển giao tiến và mất tính dẻo dai trong benchmark của chúng tôi (Ash & Adams, 2020; Dohare et al., 2023).

6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 2: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi. ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán. Đối với các tác vụ tĩnh, chúng tôi lập bảng độ chính xác của các mô hình thu được trên timestamp cuối cùng. Đối với các tác vụ động, chúng tôi lập bảng chuyển giao tiến/chuyển giao ngược và hiệu suất ID trên các tác vụ truy xuất (Mục 2.3). Đối với TIC-DataComp (XL), chúng tôi bao gồm kết quả với lọc Bestpool (lọc cơ bản trong Bảng 5). Đối với tất cả các metric, cao hơn là tốt hơn.

Benchmark Phương pháp Tính toán (MACs) Các tác vụ tĩnh Các tác vụ truy xuất động
ImageNet ImageNet dist. shift Flickr30k Trung bình trên 28 bộ dữ liệu Chuyển giao ngược Hiệu suất ID Chuyển giao tiến

TIC-YFCC Restart 3.4ˆ1018 5.2 3.6 3.0 12.9 13.2 41.4 18.6
Sequential 3.4ˆ1018 17.3 10.5 15.9 21.9 42.2 48.4 23.7
Patching 3.4ˆ1018 18.9 11.3 18.5 23.3 44.7 53.4 24.5
Cumulative-Exp 3.4ˆ1018 24.1 14.3 20.4 25.9 60.4 60.1 27.1
Cumulative-Equal 3.4ˆ1018 23.9 13.8 20.5 26.3 60.4 60.4 27.1
Cumulative-All 3.4ˆ1018 29.3 17.6 26.8 29.6 66.4 60.2 27.6
LwF˚ 4.1ˆ1018 16.9 9.8 14.7 21.2 36.6 56.0 23.2
Cumulative-All˚ 3.6ˆ1018 29.2 17.5 27.4 29.3 66.8 60.3 27.6
Oracle˚˚ 8.5ˆ1018 29.2 17.0 25.9 29.0 66.1 61.8 26.9

TIC-RedCaps Restart 3.4ˆ1018 11.7 8.5 3.7 18.4 21.3 25.4 22.4
Sequential 3.4ˆ1018 19.3 13.7 6.2 25.8 33.0 33.6 27.5
Patching 3.4ˆ1018 21.3 15.2 7.7 26.8 34.8 34.8 27.8
Cumulative-Exp 3.4ˆ1018 27.3 19.1 10.5 30.0 44.5 42.0 32.6
Cumulative-Equal 3.4ˆ1018 27.8 19.4 10.0 30.5 44.4 42.0 32.6
Cumulative-All 3.4ˆ1018 32.2 18.7 14.5 31.7 48.9 43.2 33.4
LwF˚ 4.1ˆ1018 21.6 14.8 8.2 27.3 35.4 36.0 28.4
Cumulative-All˚ 3.6ˆ1018 32.9 23.7 14.1 32.9 49.0 43.4 33.4
Oracle˚˚ 8.5ˆ1018 32.7 22.7 14.3 32.3 48.5 43.1 33.4

TIC-DataComp (M) Sequential 3.0ˆ1018 19.2 16.4 16.4 26.0 25.7 26.4 14.9
Patching 3.0ˆ1018 19.3 16.8 18.5 26.4 26.9 25.4 14.5
Cumulative-Exp 3.0ˆ1018 22.1 18.4 20.4 28.8 31.7 27.1 15.2
Cumulative-Equal 3.0ˆ1018 22.1 18.4 19.2 28.0 31.8 26.8 15.1
Cumulative-All 3.0ˆ1018 24.0 20.2 20.9 30.0 33.8 26.4 15.1
LwF˚ 3.8ˆ1018 19.2 16.5 17.7 27.0 25.6 26.6 14.9
Cumulative-All˚ 3.9ˆ1018 30.0 25.0 28.6 35.1 36.7 28.3 15.5
Oracle˚˚ 1.2ˆ1019 25.5 21.2 23.3 30.8 34.9 27.8 15.6

TIC-DataComp (L) Sequential 2.7ˆ1019 44.7 37.4 48.4 45.7 52.6 58.4 41.1
Patching 2.7ˆ1019 45.8 38.9 49.7 46.9 55.2 57.5 40.9
Cumulative-Exp 2.7ˆ1019 47.3 39.6 50.8 47.6 60.4 58.4 41.4
Cumulative-Equal 2.7ˆ1019 47.7 40.3 51.8 47.7 60.9 58.2 41.4
Cumulative-All 2.7ˆ1019 48.9 41.3 50.9 48.0 62.1 57.3 41.2
Cumulative-All˚ 4.1ˆ1019 53.0 44.3 54.4 51.3 63.0 57.8 41.2
Oracle˚˚ 1.1ˆ1020 53.6 44.0 53.9 50.4 64.3 58.6 41.8

TIC-DataComp (XL) Sequential 2.7ˆ1020 66.5 54.2 61.2 61.0 63.1 68.9 56.8
Cumulative-All 2.7ˆ1020 71.6 58.8 65.1 64.8 70.7 68.5 57.1
Cumulative-All˚ 3.5ˆ1020 72.8 60.4 66.5 66.7 71.0 68.6 57.1
Oracle˚˚ 1.1ˆ1021 73.3 61.3 68.0 65.8 - - -

V. Patching: Chúng tôi sử dụng sequential patching từ Ilharco et al. (2022). Khởi tạo từ mô hình được patch của bước cuối cùng và huấn luyện chỉ trên dữ liệu mới. Để có được mô hình được patch tại mỗi bước thời gian, chúng tôi áp dụng nội suy trọng số với mô hình được patch (nếu có) được huấn luyện tại bước thời gian t-1 và mô hình được huấn luyện tại bước thời gian t. Chúng tôi điều chỉnh các hệ số trộn bằng cách tối ưu hóa hiệu suất truy xuất trung bình trên các tác vụ trước đó.

VI. LwF: Huấn luyện chỉ trên dữ liệu mới với phạt divergence KL giữa ma trận tương tự hình ảnh-văn bản của checkpoint cuối cùng và mô hình hiện tại trên mỗi batch (Li & Hoiem, 2017; Ding et al., 2022). Xem App. E cho kết quả với các phương pháp học liên tục khác, ví dụ EWC (Kirkpatrick et al., 2017).

Lịch trình tốc độ học Lịch trình Learning Rate (LR) tiêu chuẩn cho huấn luyện mô hình CLIP là tăng tuyến tính ban đầu đến giá trị tối đa, tức là warm up, theo sau là giảm cosine (Radford et al., 2021; Gadre et al., 2023). Chúng tôi mặc định sử dụng lịch trình LR cosine cho mỗi lần chạy tuần tự, dẫn đến lịch trình chu kỳ và quan sát sự tăng đáng kể trong loss huấn luyện sớm trong các lần chạy tiếp theo khi LR cao. Tuy nhiên, khi huấn luyện tiến triển, chúng tôi quan sát thấy rằng loss tăng giảm với tốc độ nhanh hơn (khi so sánh với huấn luyện từ đầu) cho phép chúng tôi huấn luyện với các lịch trình chu kỳ. Chúng tôi thảo luận điều này nhiều hơn và khám phá một lịch trình tốc độ học thay thế trong App. B.5.

Các chi tiết huấn luyện khác và siêu tham số Trừ khi được chỉ định khác, chúng tôi tuân thủ chặt chẽ công thức huấn luyện CLIP gốc (Radford et al., 2021). Chúng tôi huấn luyện biến thể CLIP với ViT-B/16 như encoder hình ảnh (Dosovitskiy et al., 2020). Tất cả huấn luyện và siêu tham số có thể được tìm thấy trong App. D.2.

4 THÍ NGHIỆM VÀ KẾT QUẢ CHÍNH

Kết quả chính của chúng tôi có trong Bảng 2 và các biểu đồ chi tiết hơn trên mỗi bộ dữ liệu có trong App. B.1. Nhớ lại, mục tiêu của chúng tôi là cạnh tranh với Oracle huấn luyện lại từ đầu mỗi khi dữ liệu mới được quan sát, cả trên các tác vụ động và tĩnh, trong khi có tính toán hiệu quả. Ở đây, chúng tôi tóm tắt các phát hiện chính:

7

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

0.0 0.5 1.0
Total Compute (MACs)×1020203040Imagenet (& dist. shifts) accuracy
Đánh giá Tĩnh Tương tự
0.0 0.5 1.0
Total Compute (MACs)×10203040506070Retrieval Recall@1 on 2021–2022
Khoảng cách Đánh giá Động
Cumulative-All Oracle

2016 2017 2018 2019 2020 2021 2022
Evaluation time step2016
2017
2018
2019
2020
2021
2022Training time step47.09 32.46 28.98 25.04 23.10 22.67 22.82
59.39 54.99 47.50 41.57 38.93 38.19 36.98
63.07 60.23 60.11 53.13 47.08 45.79 45.29
64.23 62.00 59.78 57.68 51.65 50.27 50.01
65.38 63.92 64.72 64.89 61.35 58.67 56.61
66.90 64.57 65.11 66.38 64.00 63.73 61.12
66.92 65.38 65.91 67.23 64.33 65.42 65.36Dynamic evaluation of Oracle
253035404550556065

Hình 4: (Trái) Đánh giá động và tĩnh xếp hạng các mô hình khác nhau. Các mô hình có hiệu suất tương tự trên bộ dữ liệu tĩnh, có sự khác biệt >6% trên tác vụ truy xuất từ 2021-2022 TIC-DataComp (L). Các điểm khác nhau biểu thị các mô hình được huấn luyện tuần tự theo thời gian. (Phải) Hiệu suất của Oracle trên các bước thời gian tương lai giảm làm nổi bật sự chuyển đổi phân phối trong bộ dữ liệu. Mỗi hàng đánh giá Oracle được huấn luyện trên TIC-DataComp (L) tại một bước thời gian cụ thể trên tất cả các tác vụ truy xuất động.

Cumulative-All tiết kiệm tới 4× chi phí. Trên các tác vụ đánh giá động, chúng tôi quan sát thấy rằng Cumulative-All nơi chúng tôi phát lại tất cả dữ liệu trong quá khứ, đạt hiệu suất gần với Oracle (trong vòng 1%) sử dụng ít tính toán hơn đáng kể (4× ít hơn trên TIC-DataComp và 2,5× ít hơn trên TIC-YFCC và TIC-RedCaps). Trên các tác vụ tĩnh, khoảng cách vẫn nhỏ ở các quy mô nhỏ nhưng tăng lên 4,7% trên large, 1,8% trên xlarge Bestpool, và 4% trên xlarge Basic (xem Bảng 2 và Bảng 5). Trong những trường hợp này, việc huấn luyện các mô hình Cumulative với tính toán hơi thêm sẽ thu hẹp khoảng cách trong khi vẫn hiệu quả tính toán hơn ít nhất 2,7× (xem các hàng có ˚ trong Bảng 2). Điều này làm nổi bật rằng với quyền truy cập không bị ràng buộc vào dữ liệu trong quá khứ, chúng ta có thể chỉ cần huấn luyện tuần tự và tiết kiệm tài nguyên tính toán đáng kể.

Ở quy mô lớn, Sequential có chuyển giao tiến mạnh mẽ nhưng thiếu trên các tác vụ tĩnh. Trên TIC-YFCC và TIC-RedCaps, ở quy mô nhỏ nhất, chúng tôi quan sát một khoảng cách đáng kể (>10%) giữa Sequential (không có phát lại dữ liệu) và Oracle trên tất cả các tác vụ. Mặt khác, trên tất cả các quy mô trong TIC-DataComp, Sequential cho thấy hiệu suất mạnh mẽ trên chuyển giao tiến và đánh giá động ID. Tuy nhiên, trên các tác vụ tĩnh và đánh giá chuyển giao ngược, Sequential hiệu suất kém hơn Oracle đáng kể.

Patching và LwF cải thiện hơn Sequential nhưng tụt hậu so với Cumulative-All. Trên các tác vụ tĩnh, LwF cải thiện hơn Sequential 2%, trong khi trên các tác vụ động, LwF cải thiện chuyển giao ngược 7% trên TIC-DataComp (M). Tuy nhiên, chi phí tính toán của nó cao hơn cả Cumulative-All˚ vượt trội hơn LwF trên tất cả các tác vụ. Patching cải thiện hơn Sequential trên chuyển giao ngược trên tất cả các bộ dữ liệu (ví dụ, tăng 5% trên TIC-DataComp L) làm nổi bật rằng Patching kết hợp lợi ích của mô hình được patch trước đó và mô hình Sequential mới mà không có chi phí tính toán bổ sung. Tuy nhiên, những lợi ích như vậy không xuất hiện trên các tác vụ tĩnh. Những kết quả này gợi ý rằng để liên tục cải thiện trên các tác vụ tĩnh theo thời gian, việc phát lại dữ liệu cũ như trong Cumulative-All đóng vai trò quan trọng.

-Exp và -Equal giảm đáng kể kích thước buffer phát lại và duy trì hiệu suất tác vụ tĩnh và chuyển giao ngược. Nhớ lại, -Exp và -Equal giảm kích thước buffer phát lại xuống tối đa 2D dữ liệu cũ. Cụ thể, tại bước thời gian cuối cùng, -Exp và -Equal giảm kích thước buffer 3,5× cho các bộ dữ liệu TIC-DataComp. Trong khi giảm kích thước buffer, các phương pháp này vẫn đạt hiệu suất gần với Cumulative-All (trong vòng 2%) trên cả tác vụ tĩnh và động, với -Equal nhất quán tốt hơn chiến lược -Exp. Khi chúng ta chuyển sang quy mô lớn, ví dụ từ medium sang large, khoảng cách giữa các phương pháp này và Cumulative-All giảm. Những phát hiện này chứng minh rằng ngay cả một lượng nhỏ dữ liệu phát lại từ các bước thời gian cũ vẫn cạnh tranh với việc phát lại tất cả dữ liệu và cải thiện đáng kể so với không phát lại gì cả.

Warm up giúp huấn luyện trên dữ liệu từ bước thời gian đầu tiên, nhưng gây tổn hại cho các bước tiếp theo. Cosine LR thường được kết hợp với warm-up ban đầu tăng tuyến tính LR từ không đến LR tối đa. Chúng tôi điều tra hiệu quả của warm-up trong bước đầu tiên so với các bước tiếp theo. Đáng ngạc nhiên, chúng tôi quan sát rằng không sử dụng warmup cho các lần chạy huấn luyện tiếp theo có lợi hơn nghiêm ngặt so với sử dụng warm up trên cả tác vụ tĩnh và động. Cụ thể, trên TIC-DataComp (L), chúng tôi quan sát khoảng 1,5% cải thiện trong độ chính xác ImageNet và 4,3% cải thiện trong truy xuất động ID khi không sử dụng warmup với Cumulative (xem App. B.3). Hơn nữa, chúng tôi cũng ablate việc không sử dụng warm up cho lần chạy huấn luyện đầu tiên và quan sát sự giảm khoảng 4,8% độ chính xác trong bước thời gian đầu tiên trên TIC-DataComp (L). Do đó, chúng tôi mặc định sử dụng warmup khi huấn luyện trên bước thời gian đầu tiên và không sử dụng nó trong các bước tiếp theo với tất cả các phương pháp ngoại trừ huấn luyện trên TIC-DataComp (XL) nơi chúng tôi thêm warm up nhỏ hơn (10% số lần lặp warm up được sử dụng trong bước đầu tiên) để ổn định huấn luyện.

8

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Cùng LR tối đa hoạt động tốt nhất trên tất cả các lần chạy khi sử dụng lịch trình cosine. Chúng tôi ablate trên TIC-DataComp (M) để điều tra cách thay đổi LR sau khi huấn luyện trên dữ liệu từ bước thời gian đầu tiên. Khác với các thiết lập pretraining và finetuning thông thường nơi LR thường giảm cho huấn luyện tiếp theo, chúng tôi quan sát thấy rằng việc giảm LR tối đa cho các bước tiếp theo trong thiết lập của chúng tôi gây tổn hại trên các tác vụ tĩnh và động và do đó, chúng tôi sử dụng cùng LR tối đa qua các lần chạy của chúng tôi (xem App. B.3).

Chiến lược lọc thay đổi thứ tự hiệu suất trên các tác vụ truy xuất tĩnh và động. Chúng tôi quan sát thấy rằng trong khi các mô hình lọc bestpool vượt trội hơn các mô hình lọc basic trên TIC-DataComp (XL) 6% trên các tác vụ tĩnh, chúng hiệu suất kém hơn 5% trên tác vụ truy xuất động (xem Hình 7).

Các tác vụ động cung cấp thông tin bổ sung để lựa chọn mô hình so với các tác vụ tĩnh. Việc chọn mô hình chỉ dựa trên hiệu suất tác vụ tĩnh có thể vô tình chọn các mô hình hiệu suất kém trên các tác vụ động. Ví dụ, các mô hình Cumulative cho thấy cải thiện tương đối khiêm tốn trên các tác vụ tĩnh tiếp tục cải thiện >6% cho truy xuất trên 2021-2022 (Hình 4).

Bảng 3: Huấn luyện liên tục ImageNet. Cumulative-All vẫn gần với Oracle.

Phương pháp Số lượng phân chia
1 (Oracle) 2 4 8
Cumulative-All 80.9 80.8 80.6 80.0

Cumulative-All vẫn cạnh tranh với Oracle ngay cả trên ImageNet với tới 8 phân chia. Các mô hình CLIP thường được huấn luyện với ít epoch hơn và thường không được huấn luyện đến khi đạt đến chế độ "overfitting". Ở đây, chúng tôi điều tra Cumulative-All hoạt động như thế nào khi so sánh với Oracle khi huấn luyện được thực hiện lâu hơn. Cụ thể, chúng tôi đánh giá Cumulative-All trên 2, 4 và 8 phân chia IID bao gồm toàn bộ bộ dữ liệu (xem App. D.1 cho chi tiết). Bảng 3 tóm tắt các phát hiện chính của chúng tôi. Đáng chú ý, ngay cả với tới 8 phân chia, sự khác biệt về độ chính xác giữa Oracle và Cumulative-All vẫn dưới 0,9%. Những kết quả này nhấn mạnh tính khả thi của huấn luyện liên tục với Cumulative-All ngay cả trên ImageNet.

5 CÔNG TRÌNH LIÊN QUAN

Benchmark cho học liên tục Theo truyền thống, cộng đồng học liên tục đã tập trung vào các benchmark domain, class, và task incremental (Hsu et al., 2018; Van de Ven & Tolias, 2019; Zhou et al., 2023a) với các ranh giới tác vụ nhân tạo (ví dụ, Split-CIFAR, Perm-MNIST). Những benchmark này thường dành riêng cho tác vụ và trình bày sự tiến hóa tối thiểu hoặc không có ý nghĩa giữa các tác vụ liền kề. Do đó, các phương pháp học liên tục thường bị giới hạn trong những benchmark này và hiếm khi mở rộng đến các kịch bản thế giới thực thực tế (Cossu et al., 2022; Lin et al., 2021). Mặt khác, các phương pháp học liên tục cho các mô hình CLIP chủ yếu nhằm mục đích fine-tuning để cải thiện hiệu suất trên một tác vụ downstream duy nhất hoặc trên một chuỗi các tác vụ downstream không liên kết (Thengane et al., 2022; Zheng et al., 2023; Ilharco et al., 2022). Các benchmark quy mô lớn hiện tại để huấn luyện mô hình CLIP, ví dụ Datacomp (Gadre et al., 2023) và LAION-5B (Schuhmann et al., 2022), được tuyển chọn để điều tra các phương pháp và luật tỷ lệ để huấn luyện các mô hình CLIP hiện đại trong một lần chạy huấn luyện duy nhất. Trong công trình của chúng tôi, chúng tôi bổ sung những bộ dữ liệu hiện tại này với thông tin thời gian để tạo benchmark cho pretraining liên tục các mô hình CLIP.

Các phương pháp học liên tục Các phương pháp phổ biến có thể được phân loại thành ba danh mục: i) regularization, ii) replay, và iii) các phương pháp dựa trên kiến trúc. Các phương pháp regularization thêm phạt để giữ mô hình được fine-tuned gần với khởi tạo của nó và thường phát sinh chi phí bộ nhớ/tính toán bổ sung (Kirkpatrick et al., 2017; Mirzadeh et al., 2020a;b; Farajtabar et al., 2020). Các phương pháp replay dữ liệu giữ lại tất cả hoặc một tập con dữ liệu trước để huấn luyện tiếp theo (Lopez-Paz & Ranzato, 2017; Rebuffi et al., 2017; Chaudhry et al., 2018). Các baseline replay đơn giản vượt qua các phương pháp khác nhau trên các benchmark tiêu chuẩn (Lomonaco et al., 2022; Balaji et al., 2020; Prabhu et al., 2020). Cuối cùng, các phương pháp dựa trên kiến trúc mở rộng mô hình khi các tác vụ mới đến, hạn chế khả năng áp dụng của chúng trong môi trường phát triển mà không có ranh giới tác vụ rõ ràng (Schwarz et al., 2018; Rusu et al., 2016). Trong công trình này, chúng tôi so sánh các phương pháp học liên tục phổ biến với các lựa chọn thay thế đơn giản cho pretraining liên tục CLIP.

6 KẾT LUẬN VÀ CÔNG TRÌNH TƯƠNG LAI

Chúng tôi xem TIC-DataComp như bước đầu tiên hướng tới huấn luyện liên tục các mô hình nền tảng ngôn ngữ-thị giác quy mô lớn. Chúng tôi tin rằng benchmark của chúng tôi, cùng với các kết quả sơ bộ thu được bằng cách sử dụng các baseline đơn giản sẽ thúc đẩy nghiên cứu tương lai cho học liên tục quy mô lớn. Có nhiều hướng then chốt cho công trình tương lai: (i) So sánh các baseline của chúng tôi trên dữ liệu streaming liên tục ở độ chi tiết mịn hơn, ví dụ streaming dữ liệu ở cấp độ hàng tháng; (ii) Điều tra các lịch trình tốc độ học thay thế (ví dụ Const-Cosine như trong App. B.5) có tầm nhìn xa, và phù hợp hơn cho học liên tục; (iii) Các kỹ thuật lọc dữ liệu tốt hơn bao gồm nhiều hơn dữ liệu tương lai; (iv) Mở rộng thiết lập vấn đề của chúng tôi để bao gồm huấn luyện các mô hình nền tảng quy mô lớn khác.

9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

TÀI LIỆU THAM KHẢO

Alessandro Achille, Matteo Rovere, và Stefano Soatto. Critical learning periods in deep networks. Trong International Conference on Learning Representations, 2018.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, và cộng sự. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.

Jordan Ash và Ryan P Adams. On warm-starting neural network training. Advances in neural information processing systems, 33:3884–3894, 2020.

Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott, và Ang Li. The effectiveness of memory replay in large scale continual learning. arXiv preprint arXiv:2010.02418, 2020.

Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, và cộng sự. From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. IEEE Transactions on Medical Imaging, 2018. https://pubmed.ncbi.nlm.nih.gov/30716025/.

Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, và Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Trong H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett (eds.), Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc., 2019. https://proceedings.neurips.cc/paper/2019/file/97af07a14cacba681feacf3012730892-Paper.pdf.

Sara Beery, Elijah Cole, và Arvi Gjoka. The iwildcam 2020 competition dataset, 2020. https://arxiv.org/abs/2004.10340.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, và cộng sự. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

Jorg Bornschein, Alexandre Galashov, Ross Hemsley, Amal Rannen-Triki, Yutian Chen, Arslan Chaudhry, Xu Owen He, Arthur Douillard, Massimo Caccia, Qixuang Feng, và cộng sự. Nevis'22: A stream of 100 tasks sampled from 30 years of computer vision research. arXiv preprint arXiv:2211.11747, 2022.

Lukas Bossard, Matthieu Guillaumin, và Luc Van Gool. Food-101–mining discriminative components with random forests. Trong European Conference on Computer Vision (ECCV), 2014. https://link.springer.com/chapter/10.1007/978-3-319-10599-4_29.

Zhipeng Cai, Ozan Sener, và Vladlen Koltun. Online continual learning with natural distribution shifts: An empirical study with visual data. Trong Proceedings of the IEEE/CVF international conference on computer vision, pp. 8281–8290, 2021.

Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, và Barbara Caputo. Modeling the background for incremental learning in semantic segmentation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9233–9242, 2020.

Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient lifelong learning with a-gem. arXiv preprint arXiv:1812.00420, 2018.

Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, và Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486, 2019.

Gong Cheng, Junwei Han, và Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the Institute of Electrical and Electronics Engineers (IEEE), 2017. https://ieeexplore.ieee.org/abstract/document/7891544.

10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, và Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022. https://arxiv.org/abs/2212.07143.

Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, và Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2818–2829, 2023.

Gordon Christie, Neil Fendley, James Wilson, và Ryan Mukherjee. Functional map of the world. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.07846.

Adam Coates, Andrew Ng, và Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. Trong International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. https://proceedings.mlr.press/v15/coates11a.html.

Andrea Cossu, Gabriele Graffieti, Lorenzo Pellegrini, Davide Maltoni, Davide Bacciu, Antonio Carta, và Vincenzo Lomonaco. Is class-incremental enough for continual learning? Frontiers in Artificial Intelligence, 5:829842, 2022.

Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, và Serge Belongie. Class-balanced loss based on effective number of samples. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9268–9277, 2019.

Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, và Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2009. https://ieeexplore.ieee.org/abstract/document/5206848.

Karan Desai, Gaurav Kaul, Zubin Aysola, và Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.

Natalia Díaz-Rodríguez, Vincenzo Lomonaco, David Filliat, và Davide Maltoni. Don't forget, there is more than forgetting: new metrics for continual learning. arXiv preprint arXiv:1810.13166, 2018.

Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, và Haoxuan Ding. Don't stop learning: Towards continual learning for the clip model. arXiv preprint arXiv:2207.09248, 2022.

Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, và A Rupam Mahmood. Loss of plasticity in deep continual learning. 2023.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, và cộng sự. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong International Conference on Learning Representations (ICLR), 2021. https://openreview.net/forum?id=YicbFdNTTy.

M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, và A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007. http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.

Mehrdad Farajtabar, Navid Azizan, Alex Mott, và Ang Li. Orthogonal gradient descent for continual learning. Trong International Conference on Artificial Intelligence and Statistics, pp. 3762–3773. PMLR, 2020.

11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, và cộng sự. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.

Andreas Geiger, Philip Lenz, và Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2012. https://ieeexplore.ieee.org/abstract/document/6248074.

Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, và Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211, 2013.

Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, và Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Raia Hadsell, Dushyant Rao, Andrei A Rusu, và Razvan Pascanu. Embracing change: Continual learning in deep neural networks. Trends in cognitive sciences, 24(12):1028–1040, 2020.

Tyler L Hayes, Nathan D Cahill, và Christopher Kanan. Memory efficient experience replay for streaming learning. Trong 2019 International Conference on Robotics and Automation (ICRA), pp. 9769–9776. IEEE, 2019.

Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, và Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a. https://arxiv.org/abs/2006.16241.

Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, và Dawn Song. Natural adversarial examples. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2021b. https://arxiv.org/abs/1907.07174.

Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, và Zsolt Kira. Re-evaluating continual learning scenarios: A categorization and case for strong baselines. arXiv preprint arXiv:1810.12488, 2018.

Huiyi Hu, Ang Li, Daniele Calandriello, và Dilan Gorur. One pass imagenet. arXiv preprint arXiv:2111.01956, 2021.

Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, và Ludwig Schmidt. Openclip, tháng 7 2021. URL https://doi.org/10.5281/zenodo.5143773. Nếu bạn sử dụng phần mềm này, xin vui lòng trích dẫn như bên dưới.

Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali Farhadi, và Ludwig Schmidt. Patching open-vocabulary models by interpolating weights. Advances in Neural Information Processing Systems, 35:29262–29277, 2022.

Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, và Minjoon Seo. Towards continual knowledge learning of language models. arXiv preprint arXiv:2110.03215, 2021.

Joel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang, Joongbo Shin, Janghoon Han, Gyeonghun Kim, và Minjoon Seo. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models. arXiv preprint arXiv:2204.14211, 2022.

Armand Joulin, Edouard Grave, Piotr Bojanowski, và Tomas Mikolov. Bag of tricks for efficient text classification. Trong Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 427–431. Association for Computational Linguistics, tháng 4 2017.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, và cộng sự. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.

12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, và Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. Trong International Conference on Machine Learning (ICML), 2021. https://arxiv.org/abs/2012.07421.

Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 3d object representations for fine-grained categorization. Trong International Conference on Computer Vision Workshops (ICML), 2013. https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/html/Krause_3D_Object_Representations_2013_ICCV_paper.html.

Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.

Yann LeCun. The MNIST database of handwritten digits, 1998. http://yann.lecun.com/exdb/mnist/.

Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, và Byoung-Tak Zhang. Overcoming catastrophic forgetting by incremental moment matching. Advances in neural information processing systems, 30, 2017.

Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017.

Zhiqiu Lin, Jia Shi, Deepak Pathak, và Deva Ramanan. The clear benchmark: Continual learning on real-world imagery. Trong Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2), 2021.

Adam Liška, Tomáš Kočiský, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien de Masson d'Autume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen Gilsenan-McMahon Sophia Austin, Phil Blunsom, và Angeliki Lazaridou. Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models. arXiv preprint arXiv:2205.11388, 2022.

Xingyu Liu, Alex Leonardi, Lu Yu, Chris Gilmer-Hill, Matthew Leavitt, và Jonathan Frankle. Knowledge distillation for efficient sequences of training runs. arXiv preprint arXiv:2303.06480, 2023.

Vincenzo Lomonaco và Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. Trong Conference on robot learning, pp. 17–26. PMLR, 2017.

Vincenzo Lomonaco, Lorenzo Pellegrini, Pau Rodriguez, Massimo Caccia, Qi She, Yu Chen, Quentin Jodelet, Ruiping Wang, Zheda Mai, David Vazquez, và cộng sự. Cvpr 2020 continual learning in computer vision competition: Approaches, results, current challenges and future directions. Artificial Intelligence, 303:103635, 2022.

David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30, 2017.

Umberto Michieli và Pietro Zanuttigh. Incremental learning techniques for semantic segmentation. Trong Proceedings of the IEEE/CVF international conference on computer vision workshops, pp. 0–0, 2019.

Seyed Iman Mirzadeh, Mehrdad Farajtabar, và Hassan Ghasemzadeh. Dropout as an implicit gating mechanism for continual learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 232–233, 2020a.

Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, và Hassan Ghasemzadeh. Understanding the role of training regimes in continual learning. Advances in Neural Information Processing Systems, 33:7308–7320, 2020b.

13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, và Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. Trong Advances in Neural Information Processing Systems (NeurIPS) Workshops, 2011. https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37648.pdf.

Zixuan Ni, Longhui Wei, Siliang Tang, Yueting Zhuang, và Qi Tian. Continual vision-language representaion learning with off-diagonal information. arXiv preprint arXiv:2305.07437, 2023.

Maria-Elena Nilsback và Andrew Zisserman. Automated flower classification over a large number of classes. Trong Indian Conference on Computer Vision, Graphics and Image Processing, 2008. https://ieeexplore.ieee.org/document/4756141.

German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, và Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural networks, 113:54–71, 2019.

Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, và C. V. Jawahar. Cats and dogs. Trong Conference on Computer Vision and Pattern Recognition (CVPR), 2012. https://ieeexplore.ieee.org/document/6248092.

Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, và Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. Trong Proceedings of the IEEE international conference on computer vision, pp. 2641–2649, 2015.

Ameya Prabhu, Philip HS Torr, và Puneet K Dokania. Gdumb: A simple approach that questions our progress in continual learning. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16, pp. 524–540. Springer, 2020.

Ameya Prabhu, Hasan Abed Al Kader Hammoud, Puneet K Dokania, Philip HS Torr, Ser-Nam Lim, Bernard Ghanem, và Adel Bibi. Computationally budgeted continual learning: What does matter? Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3698–3707, 2023.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, và cộng sự. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, pp. 8748–8763. PMLR, 2021.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, và Christoph H Lampert. icarl: Incremental classifier and representation learning. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2001–2010, 2017.

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, và Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? Trong International Conference on Machine Learning (ICML), 2019. http://proceedings.mlr.press/v97/recht19a.html.

Nils Reimers và Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. Trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. URL http://arxiv.org/abs/1908.10084.

Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2):123–146, 1995.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684–10695, 2022.

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.

14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, và cộng sự. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278–25294, 2022.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, và Raia Hadsell. Progress & compress: A scalable framework for continual learning. Trong International conference on machine learning, pp. 4528–4537. PMLR, 2018.

Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/pytorch-fid, tháng 8 2020. Version 0.3.0.

Ali Shirali và Moritz Hardt. What makes imagenet look unlike laion. arXiv preprint arXiv:2306.15769, 2023.

Tejas Srinivasan, Ting-Yun Chang, Leticia Pinto Alva, Georgios Chochlakis, Mohammad Rostami, và Jesse Thomason. Climb: A continual learning benchmark for vision-and-language tasks. Advances in Neural Information Processing Systems, 35:29440–29453, 2022.

Johannes Stallkamp, Marc Schlipsing, Jan Salmen, và Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. Trong International Joint Conference on Neural Networks (IJCNN), 2011. https://ieeexplore.ieee.org/document/6033395.

Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, và Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021.

Richard S Sutton. Two problems with backpropagation and other steepest-descent learning procedures for networks. Trong Proc. of Eightth Annual Conference of the Cognitive Science Society, pp. 823–831, 1986.

Vishal Thengane, Salman Khan, Munawar Hayat, và Fahad Khan. Clip model is an efficient continual learner. arXiv preprint arXiv:2210.03114, 2022.

Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, và Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.

Gido M Van de Ven và Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734, 2019.

Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, và Max Welling. Rotation equivariant CNNs for digital pathology, 2018. https://arxiv.org/abs/1806.03962.

Tom Veniat, Ludovic Denoyer, và Marc'Aurelio Ranzato. Efficient continual learning with modular networks and task-driven priors. arXiv preprint arXiv:2012.12631, 2020.

Eli Verwimp, Kuo Yang, Sarah Parisot, Lanqing Hong, Steven McDonagh, Eduardo Pérez-Pellitero, Matthias De Lange, và Tinne Tuytelaars. Clad: A realistic continual learning benchmark for autonomous driving. Neural Networks, 161:659–669, 2023.

Haohan Wang, Songwei Ge, Zachary Lipton, và Eric P Xing. Learning robust global representations by penalizing local predictive power. Trong Advances in Neural Information Processing Systems (NeurIPS), 2019. https://arxiv.org/abs/1905.13549.

Jianren Wang, Xin Wang, Yue Shang-Guan, và Abhinav Gupta. Wanderlust: Online continual object detection in the real world. Trong Proceedings of the IEEE/CVF international conference on computer vision, pp. 10829–10838, 2021.

Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, và cộng sự. Dualprompt: Complementary prompting for rehearsal-free continual learning. Trong European Conference on Computer Vision, pp. 631–648. Springer, 2022.

15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Yeming Wen, Dustin Tran, và Jimmy Ba. Batchensemble: an alternative approach to efficient ensemble and lifelong learning. arXiv preprint arXiv:2002.06715, 2020.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, và Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359, 2019.

Janet Wiener và Nathan Bronson. Facebook's top open data problems. https://research.facebook.com/blog/2014/10/facebook-s-top-open-data-problems/, 10 2014. Truy cập: 2023-09-28.

Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, và Aude Oliva. Sun database: Exploring a large collection of scene categories. International Journal of Computer Vision (IJCV), 2016. https://link.springer.com/article/10.1007/s11263-014-0748-y.

Peter Young, Alice Lai, Micah Hodosh, và Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2014. https://aclanthology.org/Q14-1006/.

Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. Trong International conference on machine learning, pp. 3987–3995. PMLR, 2017.

Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, André Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, và Neil Houlsby. The visual task adaptation benchmark, 2019. http://arxiv.org/abs/1910.04867.

Zangwei Zheng, Mingyuan Ma, Kai Wang, Ziheng Qin, Xiangyu Yue, và Yang You. Preventing zero-shot transfer degradation in continual learning of vision-language models. arXiv preprint arXiv:2303.06628, 2023.

Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, và De-Chuan Zhan. Pycil: A python toolbox for class-incremental learning, 2023a.

Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, và Ziwei Liu. Learning without forgetting for vision-language models. arXiv preprint arXiv:2305.19270, 2023b.

16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

A CÁC BENCHMARK VÀ PHƯƠNG PHÁP HỌC LIÊN TỤC

Chúng tôi giới thiệu một benchmark hình ảnh-văn bản quy mô lớn với các cặp hình ảnh văn bản streaming quy mô web được phát triển đặc biệt để nghiên cứu cách hiệu quả có thể có được một mô hình CLIP mới với các lô dữ liệu mới đến. Bảng 4 so sánh benchmark được đề xuất với các bộ dữ liệu hiện có cho học liên tục. Lưu ý rằng bảng này không nhằm mục đích là một danh sách đầy đủ của tất cả các bộ dữ liệu CL, mà là các benchmark phổ biến nhất trong mỗi miền. Đối với các tác vụ mô hình hóa ngôn ngữ, chúng tôi báo cáo số lượng ví dụ/tài liệu như số lượng mẫu và đối với các tác vụ phát hiện, chúng tôi báo cáo số lượng đối tượng/hộp giới hạn được gán nhãn.

Bảng 4: So sánh với các benchmark học liên tục.

[Bảng được duy trì như trong bản gốc với các tên benchmark, số lượng mẫu, năm, và các đặc tính khác]

A.1 CÔNG TRÌNH LIÊN QUAN MỞ RỘNG

Các mạng neural được huấn luyện trên dữ liệu mới chịu sự quên lãng thảm khốc kiến thức trước đó (Sutton, 1986; Goodfellow et al., 2013). Giải quyết thách thức học liên tục, các nhà nghiên cứu chủ yếu tập trung vào các phương pháp được thiết kế riêng cho các benchmark quy mô nhỏ, cụ thể tập trung vào các benchmark domain, class, hoặc task incremental (Hsu et al., 2018; Van de Ven & Tolias, 2019). Học liên tục của các mô hình nền tảng sẽ giảm đáng kể chi phí và tăng khả năng thích ứng nhanh chóng.

Trong khi một số công trình gần đây đã bắt đầu giới thiệu các benchmark học liên tục, chúng không phải là time-continual tự nhiên và có quy mô nhỏ hơn nhiều (Ni et al., 2023; Srinivasan et al., 2022). Trong khi các đánh giá trên những benchmark này thường bỏ qua việc xem xét "thời gian huấn luyện", nó trở thành một yếu tố then chốt khi mở rộng các phương pháp học liên tục đến các kịch bản liên quan đến việc huấn luyện các mô hình nền tảng như CLIP.

Trong nghiên cứu của chúng tôi, chúng tôi tránh so sánh với các phương pháp học liên tục làm kéo dài đáng kể "thời gian huấn luyện". Các phương pháp như GEM (Lopez-Paz & Ranzato, 2017; Chaudhry et al., 2018), và IMM (Lee et al., 2017), tính toán gradient cho hai mô hình trong mỗi lần lặp huấn luyện, về cơ bản làm tăng gấp đôi thời gian huấn luyện. Để đầy đủ, chúng tôi bao gồm so sánh với LWF (Li & Hoiem, 2017; Ding et al., 2022) và EWC (Kirkpatrick et al., 2017). Trong khi các phương pháp này tăng chi phí tính toán so với huấn luyện tiêu chuẩn do một forward pass bổ sung, việc tăng chi phí tính toán tương đối nhỏ hơn nhiều so với các phương pháp tính toán gradient bổ sung. Việc triển khai LWF của chúng tôi được thúc đẩy bởi Ding et al. (2022) tập trung vào fine-tuning liên tục mô hình CLIP trên các tác vụ phân loại bằng cách thích ứng LwF với mô hình CLIP. Thay vào đó, cho các thiết lập có tài nguyên tính toán bổ sung, chúng tôi chạy phương pháp Cumulative-All của mình trong thời gian hơi lâu hơn. Cumulative-All thu hẹp khoảng cách với Oracle (tham khảo Bảng 2). Cho rằng chi phí lưu trữ dữ liệu thấp hơn đáng kể so với chi phí tính toán ở quy mô lớn, chúng tôi ủng hộ việc tính đến hiệu quả tính toán trong các nỗ lực tương lai.

A.2 THẢO LUẬN VÀ SO SÁNH VỚI BENCHMARK CLOC

Cai et al. (2021) cung cấp thảo luận/phân tích thú vị cho học liên tục ở số lượng bước lớn. Tuy nhiên, nghiên cứu của chúng tôi khác với Cai et al. (2021) trong một số khía cạnh quan trọng: (i) Phương pháp Huấn luyện: Chúng tôi sử dụng giám sát nhiễu bằng cách sử dụng loss contrastive giữa các cặp hình ảnh-văn bản, trái ngược với loss cross-entropy được sử dụng bởi Cai et al. (2021). (ii) Quy mô Thí nghiệm: Các thí nghiệm của chúng tôi trên bộ dữ liệu TiC-DataComp lớn hơn bậc độ lớn, mở rộng lên 200×.

Những khác biệt này đưa ra những thách thức độc đáo. Việc sử dụng loss contrastive (i) đòi hỏi một phương pháp được thiết kế riêng để thiết kế các nghiên cứu đánh giá của chúng tôi. Quy mô lớn hơn đáng kể của các thí nghiệm của chúng tôi (ii) đặt ra thách thức trong việc thu thập dữ liệu có dấu thời gian và hiểu liệu và cách các chuyển đổi phân phối ảnh hưởng đến học tập ở quy mô này.

18

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

B KẾT QUẢ THÍ NGHIỆM BỔ SUNG

B.1 KẾT QUẢ CHI TIẾT TRÊN CÁC BENCHMARK CỦA CHÚNG TÔI

[Các hình vẽ và biểu đồ về hiệu suất được duy trì như trong bản gốc]

(a) TIC-YFCC.
(b) TIC-RedCaps.
(c) TIC-DataComp (M).
(d) TIC-DataComp (L).

Hình 5: Hiệu suất đánh giá tĩnh và động theo thời gian với các phương pháp được chọn trong bộ thử nghiệm của chúng tôi. Khi chúng tôi nhận được nhiều dữ liệu hơn, tất cả các phương pháp đều cải thiện trên cả tác vụ tĩnh và chuyển giao tiến trên các tác vụ động nhưng các phương pháp với buffer phát lại hạn chế bắt đầu hoạt động hơi kém hơn cho chuyển giao ngược.

19

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

[Các ma trận và biểu đồ đánh giá động được duy trì như trong bản gốc]

(a) TIC-YFCC.
(b) TIC-RedCaps.
(c) TIC-DataComp (M).
(d) TIC-DataComp (L).

Hình 6: Kết quả đánh giá truy xuất động trên các benchmark của chúng tôi với Sequential, Cumulative-Exp, Cumulative-All và Oracle. Những đánh giá này làm nổi bật sự quên lãng thảm khốc được quan sát với Sequential và Cumulative-Exp. Hơn nữa, bằng cách quan sát dữ liệu mới, chúng tôi không chỉ được hưởng lợi trên các tác vụ từ bước thời gian hiện tại mà còn cải thiện hiệu suất trên các tác vụ từ các bước thời gian cũ.

B.2 KẾT QUẢ VỚI LỌC BASIC TRÊN TIC-DATACOMP XL

Chiến lược lọc thay đổi thứ tự hiệu suất trên các tác vụ truy xuất tĩnh và động. Chúng tôi quan sát thấy rằng trong khi các mô hình lọc Bestpool vượt trội hơn các mô hình lọc basic trên TIC-DataComp (XL) 6% trên các tác vụ tĩnh, chúng hiệu suất kém hơn 5% trên tác vụ truy xuất động (xem Hình 7). Trong bài báo chính (Bảng 2), chúng tôi đã bao gồm kết quả TIC-DataComp (xlarge) với lọc Bestpool. Trong Bảng 5, chúng tôi bao gồm kết quả lọc basic. Chúng tôi quan sát thấy rằng trong khi các mô hình lọc Bestpool hoạt động tốt hơn các mô hình lọc basic trên các tác vụ tĩnh, thứ tự bị đảo ngược trên các tác vụ truy xuất động. Do đó, chúng tôi sử dụng kết quả với lọc Basic ở các quy mô nhỏ hơn, nhưng bao gồm kết quả Bestpool để hoàn chỉnh vì nó đạt kết quả tốt hơn trên các tác vụ tĩnh.

20

--- TRANG 21 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 5: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi (lọc Basic và Bestpool). ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán và sử dụng tính toán bổ sung. Đối với các tác vụ tĩnh, chúng tôi lập bảng độ chính xác của các mô hình thu được trên timestamp cuối cùng. Đối với các tác vụ động, chúng tôi lập bảng chuyển giao tiến, chuyển giao ngược và hiệu suất ID. Đối với tất cả các metric, cao hơn là tốt hơn. Kết quả lọc Bestpool được sao chép từ Bảng 2.

[Bảng được duy trì như trong bản gốc]

2016 2018 2020 2022
Evaluation Timestamp60657075Retrieval Performance
Đánh giá Động trên TiC-Datacomp (XL)
Lọc Bestpool
Lọc Basic

Hình 7: So sánh các mô hình Oracle được huấn luyện trên lọc Bestpool và Basic được huấn luyện trên dữ liệu từ tất cả các bước thời gian. Kết quả của chúng tôi rõ ràng làm nổi bật rằng lọc Basic hoạt động tốt hơn lọc Bestpool trên tác vụ truy xuất động. Tuy nhiên, trên các tác vụ tĩnh, thứ tự được đảo ngược. Hơn nữa, lọc Bestpool cho thấy sự giảm hiệu suất truy xuất từ 2016 đến 2022 khi so sánh với lọc Basic.

B.3 ABLATION VỚI WARM UP TỐC ĐỘ HỌC VÀ TỐC ĐỘ HỌC TỐI ĐA

Để huấn luyện liên tục các mô hình khi nhiều dữ liệu hơn đến tuần tự theo thời gian, chúng tôi sử dụng nhiều chu kỳ của lịch trình tốc độ học cosine (Hình 8). Có hai lựa chọn thiết kế quan trọng: (i) Chúng ta có nên warm up tốc độ học cho các lần chạy liên tục tiếp theo không? và (ii) Tốc độ học tối đa nên thay đổi như thế nào cho các lần chạy huấn luyện tuần tự?

Bảng 6: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi với và không có warm up LR ban đầu cho các lần chạy tiếp theo. Sử dụng warm up trong các lần chạy tuần tự sau khi huấn luyện trên bước thời gian đầu tiên hơi gây tổn hại khi so sánh với không sử dụng warm up trong các lần chạy tuần tự.

[Bảng được duy trì như trong bản gốc]

21

--- TRANG 22 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

0 0.5T T 1.5T 2T (N-1)T (N - 0.5)T NT
Iterations0.00000.00020.00040.00060.00080.0010Learning Rate
Warm up Warm up Warm upLịch trình học chu kỳ với warm up

(a) Nhiều chu kỳ của lịch trình tốc độ học cosine tiêu chuẩn bao gồm warm-up cho tất cả các lần chạy huấn luyện tiếp theo.

0 0.5T T 1.5T 2T (N-1)T (N - 0.5)T NT
Iterations0.00000.00020.00040.00060.00080.0010Learning Rate
Warm upLịch trình học chu kỳ không có warm up trong các lần lặp tiếp theo

(b) Lịch trình tốc độ học cosine được đề xuất của chúng tôi không có warm-up tốc độ học cho các lần chạy huấn luyện tiếp theo.

Hình 8: Ablation lịch trình tốc độ học. Các lịch trình khác nhau về cách thực hiện huấn luyện liên tục khi lần chạy huấn luyện được khởi tạo với mô hình tốt nhất trước đó. Khi huấn luyện với lịch trình cosine cho các lần chạy tiếp theo, chúng tôi quan sát thấy rằng giữ cùng tốc độ học tối đa như lần chạy đầu tiên hoạt động tốt nhất.

Bảng 7: Thí nghiệm tích lũy trên TIC-DataComp (M) với các tốc độ học tối đa khác nhau cho các lần chạy tiếp theo với lần chạy đầu tiên cố định tại LR 0.00025. Lựa chọn mặc định của chúng tôi cho các lần chạy tiếp theo là 0.00025. Hiệu suất được báo cáo trên ImageNet. Tại tốc độ học tối đa 0.001, các lần chạy bị crash với Nan trong loss.

[Bảng được duy trì như trong bản gốc]

Khi huấn luyện với các batch lớn, warm-up tốc độ học tuyến tính thường được sử dụng để ổn định đầu huấn luyện khi bắt đầu từ khởi tạo ngẫu nhiên (Goyal et al., 2017; Steiner et al., 2021). Tuy nhiên, khi huấn luyện tuần tự bằng cách khởi tạo các mô hình với checkpoint từ bước trước đó, vẫn chưa rõ liệu chúng ta có nên sử dụng warm up tốc độ học hay không. Các quan sát của chúng tôi làm nổi bật rằng trong khi warm up có lợi cho bước thời gian đầu tiên, không sử dụng warm up trong các lần chạy tiếp theo hoạt động tốt hơn. Cụ thể, chúng tôi quan sát thấy rằng việc loại bỏ warm up cho lần chạy huấn luyện đầu tiên gây tổn hại cho hiệu suất cuối cùng. Trên TIC-DataComp (large), chúng tôi quan sát thấy rằng huấn luyện ViT-B/16 với warm up trong bước thời gian đầu tiên (tức là 2016) đạt 29.9 zero-shot trên Imagenet, trong khi, không có warm up ViT-B/16 chỉ đạt 24.1 hiệu suất zero-shot trên Imagenet. Bảng 6 hiển thị hiệu suất cuối cùng của các mô hình được huấn luyện với và không có warmup trong các bước thời gian tiếp theo (sau khi huấn luyện trên bước thời gian đầu tiên với warmup). Cụ thể, trên TIC-DataComp (large), chúng tôi quan sát khoảng cách độ chính xác 1.5% trên Imagenet và khoảng cách độ chính xác 4.3% trên hiệu suất truy xuất động ID trên các mô hình được huấn luyện với và không có warm up.

Do đó, chúng tôi mặc định sử dụng warmup khi huấn luyện trên bước thời gian đầu tiên và không sử dụng nó trong các bước thời gian tiếp theo với tất cả các phương pháp ngoại trừ huấn luyện trên TIC-DataComp (XL) nơi chúng tôi thêm warm up nhỏ hơn (10% số lần lặp warm up được sử dụng trong bước đầu tiên) để ổn định huấn luyện.

Tiếp theo, chúng tôi thí nghiệm với tốc độ học tối đa khác nhau khi huấn luyện với lịch trình cosine. Chúng tôi ablate trên TIC-DataComp (M) để điều tra cách thay đổi LR sau khi huấn luyện trên dữ liệu từ bước thời gian đầu tiên. Khác với các thiết lập pretraining và finetuning thông thường nơi LR thường giảm cho huấn luyện tiếp theo, chúng tôi quan sát thấy rằng việc giảm LR tối đa cho các bước tiếp theo trong thiết lập của chúng tôi gây tổn hại trên các tác vụ tĩnh và động và do đó, chúng tôi sử dụng cùng LR tối đa qua các lần chạy của chúng tôi (xem Bảng 7).

B.4 THÍ NGHIỆM SƠ BỘ SO SÁNH LẤY MẪU NGẪU NHIÊN VỚI CÁC CHIẾN LƯỢC KHÁC ĐỂ GIẢM KÍCH THƯỚC BUFFER

Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi đã khám phá hiệu quả của việc lấy mẫu con dữ liệu cũ dựa trên sự liên kết giữa nội dung văn bản và hình ảnh từ các bước thời gian trước đó. Cụ thể, khi huấn luyện mô hình tại bước thời gian t+1, chúng tôi đã sử dụng mô hình từ cuối bước thời gian t để đánh giá sự liên kết này. Chúng tôi đã sử dụng hai phương pháp lấy mẫu con khác biệt:

1. Giữ lại một nửa dữ liệu với điểm liên kết thấp nhất, dựa trên tiền đề rằng những điểm dữ liệu này có thể khó học hơn và cần thêm các bước gradient.
2. Giữ lại một nửa dữ liệu với điểm liên kết cao nhất, dưới giả định rằng chúng đại diện cho dữ liệu chất lượng cao hơn, như được chỉ ra bởi sự liên kết mạnh hơn giữa các cặp văn bản và hình ảnh.

Chúng tôi đã áp dụng những phương pháp này cho bộ dữ liệu TiC-YFCC và đánh giá hiệu suất của chúng so với baseline của việc lấy mẫu ngẫu nhiên. Kết quả cho thấy sự khác biệt tối thiểu: dưới 0.2% biến thiên trong hiệu suất Imagenet và dưới 0.5% trong hiệu suất truy xuất động qua các bước thời gian khác nhau. Cho rằng những cải thiện nhỏ này đi kèm với chi phí tính toán đáng kể—yêu cầu một forward pass đầy đủ để tính toán sự liên kết sau mỗi epoch huấn luyện—chúng vượt quá các ràng buộc ngân sách tính toán của chúng tôi. Kết quả là, chúng tôi đã chọn lấy mẫu ngẫu nhiên trong nghiên cứu của mình. Chúng tôi để lại điều tra về các kỹ thuật lấy mẫu con được cải thiện cho công trình tương lai.

B.5 CONST-COSINE: MỘT LỊCH TRÌNH TỐC ĐỘ HỌC THAY THẾ

Lịch trình LR tiêu chuẩn cho huấn luyện mô hình CLIP là tăng tuyến tính ban đầu đến giá trị tối đa, tức là warm up, theo sau là giảm cosine (Radford et al., 2021; Gadre et al., 2023). Trong bài báo chính, chúng tôi mặc định sử dụng lịch trình LR cosine cho mỗi lần chạy tuần tự, dẫn đến lịch trình chu kỳ. Chúng tôi quan sát sự tăng đáng kể trong loss huấn luyện sớm trong các lần chạy tiếp theo khi LR cao. So sánh loss trên dữ liệu huấn luyện với các phương pháp Cumulative và Oracle, chúng tôi quan sát thấy rằng khi huấn luyện tiến triển, loss huấn luyện tăng mỗi khi tốc độ học được tăng lên LR tối đa (Hình 9).

Sẽ lý tưởng cho huấn luyện liên tục để sử dụng một lịch trình tốc độ học "có tầm nhìn xa", cho phép chúng ta liên tục huấn luyện từ checkpoint trước đó mà không trải qua sự tăng đáng kể trong loss huấn luyện. Một thuộc tính mong muốn của lịch trình tốc độ học như vậy sẽ là khả năng thích ứng mà không cần kiến thức trước về thời gian giảm.

23

--- TRANG 24 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

0 200 400 600 800 1000 1200 1400 1600
Iterations51015Training loss
Cumulative (All) Oracle200 400 600 800 1000 1200 14002.02.53.03.54.04.55.0

Hình 9: Loss huấn luyện tăng mỗi khi LR được reset về LR tối đa cho Cumulative. So sánh loss trên dữ liệu huấn luyện với phương pháp Cumulative và Oracle. Cumulative được huấn luyện với lịch trình cosine chu kỳ không có warm up cho các lần chạy huấn luyện tuần tự. Đối với Cumulative, chúng tôi vẽ loss trên dữ liệu huấn luyện, và khi huấn luyện tiến triển, các mẫu từ các bước thời gian mới được thêm vào pool huấn luyện. Đối với Oracle, dữ liệu huấn luyện là hợp của dữ liệu từ tất cả các bước thời gian và vẫn như vậy trong suốt quá trình huấn luyện.

0 0.5T T 1.5T 2T (N-1)T (N - 0.5)T NT
Iterations0.00000.00020.00040.00060.00080.0010Learning Rate
Warm up Decay DecayDecay
(0.2*N*T)Spawn a new run
with lr decayLịch trình học Constant + Cosine

Hình 10: Const-Cosine: Lịch trình tốc độ học có tầm nhìn xa thay thế được đề xuất của chúng tôi huấn luyện một mô hình với tốc độ học không đổi và giảm tốc độ học với lịch trình cosine chỉ cho một phần lần lặp trước khi thu được mô hình có thể triển khai. Lịch trình Const-Cosine sử dụng ngân sách tính toán bổ sung hơn so với lần chạy Oracle vì một lần chạy huấn luyện bổ sung được khởi chạy cho phần huấn luyện khi tốc độ học được giảm.

Trong công trình của chúng tôi, chúng tôi thực hiện các thí nghiệm sơ bộ với phương án thay thế đơn giản nhất, Const-Cosine nơi sau thời gian warm up, chúng tôi huấn luyện với tốc độ học không đổi và giảm tốc độ học chỉ cho một phần nhỏ huấn luyện về phía cuối khi chúng tôi muốn có mô hình có thể triển khai (Hình 10). Điều này cho phép chúng tôi tiếp tục huấn luyện cho các lần chạy tiếp theo từ checkpoint ở cuối lịch trình tốc độ học không đổi và giảm LR chỉ ở cuối. Đối với các thí nghiệm của chúng tôi, chúng tôi cố định thời gian giảm là 0.2 tổng số lần lặp huấn luyện. Do điều này, lịch trình Const-Cosine làm tăng nhẹ ngân sách huấn luyện tổng thể của các lần chạy Cumulative khi so sánh với lịch trình cosine chu kỳ.

Đối với Const-Cosine, chúng tôi chỉ ablate ở các bộ dữ liệu quy mô tương đối nhỏ hơn trong bộ thử nghiệm của chúng tôi (tức là TIC-YFCC, TIC-RedCaps, và TIC-DataComp (medium)). Để so sánh công bằng, chúng tôi cũng chạy lại các phương pháp Oracle với cùng lịch trình Const-Cosine. Lưu ý rằng cho các thí nghiệm Const-Cosine, chúng tôi sử dụng cùng LR tối đa như với lịch trình cosine.

Chúng tôi quan sát thấy rằng huấn luyện với lịch trình Const-Cosine cải thiện đáng kể cả Cumulative và Oracle so với các đối tác của chúng được huấn luyện với tốc độ học cosine2. Hơn nữa, như mong đợi, chúng tôi không quan sát thấy các bước nhảy trong loss huấn luyện khi huấn luyện Cumulative với lịch trình Const-Cosine. Tuy nhiên, khoảng cách giữa Oracle và Cumulative với Const-Cosine không giảm khi so sánh với khoảng cách giữa Oracle và Cumulative với lịch trình tốc độ học cosine. Điều này làm nổi bật rằng các bước nhảy trong loss huấn luyện được quan sát trong khi huấn luyện với lịch trình cosine chu kỳ có thể có tác động lành tính đối với hiệu suất cuối cùng.

Bảng 8: Hiệu suất zero shot trên Imagenet với lịch trình LR Const-Cosine. Chúng tôi quan sát thấy rằng Const-Cosine cải thiện hơn lịch trình LR cosine chu kỳ. Tuy nhiên, khoảng cách giữa lịch trình LR cosine chu kỳ và Const-Cosine cho các lịch trình LR khác nhau vẫn như vậy. ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán.

[Bảng được duy trì như trong bản gốc]

B.6 CÁC MÔ HÌNH OPENCLIP THU ĐƯỢC BẰNG CÁCH HUẤN LUYỆN LẠI SAU KHI LOẠI BỎ BẤT KỲ VÍ DỤ TRÙNG LẶP NÀO TỪ TẬP THỬ NGHIỆM

Các mô hình OpenCLIP (ví dụ, các mô hình được huấn luyện trên Datacomp và LAION-5B) đã được huấn luyện trên dữ liệu được tuyển chọn từ Common Crawl. Vì các tác vụ truy xuất mà chúng tôi xây dựng được xây dựng trên dữ liệu được tuyển chọn từ Common Crawl, có thể có sự chồng chéo train/test trong các đánh giá của chúng tôi về các mô hình OpenCLIP. Do đó, chúng tôi huấn luyện lại các mô hình OpenCLIP trên các bộ dữ liệu DataComp sau khi loại bỏ các mẫu trong các tập thử nghiệm của chúng tôi. Hình 11 cho thấy rằng các xu hướng được quan sát cho các mô hình OpenCLIP vẫn giữ nguyên đối với các mô hình được huấn luyện lại của chúng tôi.

40 50 60 70 80
Retrieval recall on 2014–20164050607080Retrieval recall on 2021–2022
Paradigm Đánh giá Được Đề xuất của Chúng tôi
Mô hình OpenAI được huấn luyện trên dữ liệu trước 2020
Mô hình OpenClip được huấn luyện trên dữ liệu trước 2022
Mô hình của chúng tôi

Hình 11: Chúng tôi sao chép các mô hình OpenCLIP bằng cách huấn luyện từ đầu và loại bỏ trùng lặp khỏi bộ dữ liệu đánh giá. Chúng tôi quan sát thấy rằng các xu hướng tiếp tục giữ nguyên.

B.7 KẾT QUẢ TRÊN TÁC VỤ PHÂN LOẠI ĐỘNG

Trong bài báo chính, chúng tôi bao gồm kết quả trên tác vụ truy xuất động của chúng tôi. Để đầy đủ, ở đây chúng tôi bao gồm kết quả trên các tác vụ phân loại động trên các phân chia TIC-DataComp (Bảng 9). Cùng với việc bao gồm kết quả trên tất cả các nút của ImageNet, chúng tôi cũng bao gồm kết quả trên tác vụ phân loại được hạn chế cho các lớp trong cây con "xe cơ giới" của hệ thống phân cấp ImageNet. Đối với tác vụ phân loại động, chúng tôi quan sát các xu hướng tương tự như tác vụ truy xuất động.

2Chúng tôi cũng thí nghiệm với lịch trình Const-Cosine cho huấn luyện Oracle trên TIC-DataComp (large) và TIC-DataComp (xlarge). Chúng tôi quan sát thấy rằng với phần giảm 0.2, Const-Cosine đạt kết quả tương tự...

25

--- TRANG 26 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 9: Hiệu suất zero shot trên tác vụ phân loại TIC-DataComp-Net của chúng tôi. ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán. Chúng tôi lập bảng chuyển giao tiến/chuyển giao ngược và hiệu suất ID trên các tác vụ phân loại (Mục 2.3). Đối với TIC-DataComp (XL), chúng tôi bao gồm kết quả với lọc Bestpool.

[Bảng được duy trì như trong bản gốc]

B.8 GIẢI QUYẾT CÁC KHÁC BIỆT GIỮA SEQUENTIAL VÀ CUMULATIVE-ALL GIỮA TIC-YFCC VÀ TIC-DATACOMP

Trong Bảng 2, chúng tôi quan sát các khác biệt trong hành vi của Sequential và Cumulative-All trên TIC-YFCC khi so sánh với TIC-DataComp. Chẳng hạn, sự khác biệt giữa hiệu suất ID giữa Sequential và Cumulative-All lớn hơn trong TIC-YFCC so với TIC-DataComp (M). Các quan sát tương tự đúng cho hiệu suất chuyển giao ngược. Trong phần này, chúng tôi giải thích các nguyên nhân cơ bản cho những khác biệt này.

Chúng tôi xác định hai lý do chính:
(i) bản chất của sự chuyển đổi phân phối được quan sát trong TIC-YFCC. Chúng tôi quan sát thấy rằng các mô hình được huấn luyện với Sequential trên TIC-YFCC chịu sự giảm tương đối lớn hơn trên các bước thời gian cũ so với TIC-DataComp (M) do quên lãng thảm khốc (xem Hình 6).
(ii) tính toán được sử dụng tại mỗi bước thời gian trên mỗi dữ liệu có sẵn tại mỗi bước thời gian khác nhau cho những benchmark này. Tổng thể YFCC nhỏ hơn 2× so với Tic-Datacomp (M) nhưng tính toán chúng tôi sử dụng trong cả thiết lập TiC-YFCC và TiC-Datacomp có bậc tương tự (thực tế, nó hơi cao hơn trong TiC-YFCC). Chúng tôi chạy lại các thí nghiệm cho Tic-YFCC bằng cách giảm tính toán. Trong các lần chạy được cập nhật, chúng tôi quan sát thấy rằng khoảng cách giữa hiệu suất ID của Sequential và Cumulative-All biến mất.

Bảng 10: Hiệu suất truy xuất zero shot trên TIC-YFCC với Sequential và Cumulative-All với tính toán giảm.

[Bảng được duy trì như trong bản gốc]

C CHI TIẾT BENCHMARK BỔ SUNG

C.1 ABLATION LỌC TRÊN TIC-DATACOMP

Đối với Lọc Cơ bản, Gadre et al. (2023) thực hiện ba bước sau: lọc theo ngôn ngữ tiếng Anh (sử dụng fasttext (Joulin et al., 2017)), lọc theo độ dài chú thích trên hai từ và 5 ký tự, và lọc theo kích thước hình ảnh với kích thước nhỏ nhất trên 200 pixel và tỷ lệ khung hình trên 3. Chúng tôi không mặc định các kỹ thuật lọc khác sử dụng các mô hình CLIP sẵn có từ Gadre et al. (2023) để...

26

--- TRANG 27 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

tránh làm thiên lệch việc lựa chọn dữ liệu huấn luyện từ mỗi bước thời gian. Trong Hình 13, chúng tôi cho thấy rằng việc lọc "Bestpool" (lọc các cặp hình ảnh-văn bản với điểm CLIP và embedding hình ảnh ImageNet) làm thiên lệch việc lựa chọn bộ dữ liệu để ưu tiên dữ liệu bước thời gian cũ hơn dữ liệu timestamp mới. Hơn nữa, chúng tôi cũng cho thấy rằng các mô hình được huấn luyện với lọc Bestpool ít bền vững hơn khi được đánh giá trên các tác vụ động của chúng tôi từ 2021-2022 (Hình 13). Tuy nhiên, để đầy đủ và để làm nổi bật tầm quan trọng của những phát hiện của chúng tôi ngay cả đối với các kỹ thuật lọc hiện đại, chúng tôi thực hiện các thí nghiệm học liên tục với lọc Bestpool ở quy mô xlarge được bao gồm trong bài báo chính. Trong App. B.2, chúng tôi bao gồm kết quả với lọc Basic ở xlarge.

60 65 70 75 80 85
Retrieval Performance on 2014-201660657075808590Retrieval Performance on 2021-2022
Mô hình OpenClip (lọc Bestpool/Image/CLIP)
Mô hình OpenClip (lọc basic/không lọc)

2014 2016 2018 2020 2022
Year0.10.20.30.4Fraction of samples retained
from each yearBasic
CLIP
BestPool

Hình 13: (Trái) Khoảng cách trong hiệu suất truy xuất cho các mô hình OpenCLIP khác nhau sử dụng các kỹ thuật lọc khác nhau. (Phải) Giảm dữ liệu TIC-DataComp tại các thời điểm khác nhau với các kỹ thuật lọc khác nhau. Điều này rõ ràng làm nổi bật rằng có sự thiên lệch lựa chọn hướng tới giữ lại nhiều dữ liệu cũ hơn cho lọc CLIP/BestPool. Không có sự thiên lệch như vậy tồn tại cho lọc basic.

C.2 CÁC BỘ DỮ LIỆU TĨNH ĐƯỢC XEM XÉT ĐỂ ĐÁNH GIÁ

Bảng 11: Các tác vụ đánh giá được mượn từ Gadre et al. (2023).

[Bảng được duy trì như trong bản gốc với các bộ dữ liệu, kích thước tập thử nghiệm, số lớp và metric chính]

C.3 THỐNG KÊ BENCHMARK CỦA CHÚNG TÔI

Trong phần này, chúng tôi thảo luận về thống kê của các benchmark được xây dựng của chúng tôi. Hình 14 tóm tắt kích thước bộ dữ liệu TIC-RedCaps, TIC-YFCC và TIC-DataComp. Hình 15 tóm tắt kích thước bộ dữ liệu YFCC gốc. Bảng 12, Bảng 13 và Bảng 14 trình bày các số chính xác cho các bộ dữ liệu này. Đối với TIC-DataComp, chúng tôi chỉ thảo luận về kích thước ở quy mô xlarge.

2004-2008 2009-2010 2011-2012 2012-2014
Timestamp024Sizes×106 Kích thước TiC-YFCC

2017 2018 2019 2020
Timestamp01234Sizes×106 Kích thước TiC-Redcaps

2014 2016 2018 2020 2022
Timestamp0.00.51.01.52.0Sizes×109
 Kích thước TiC-Datacomp (XL)
Không lọc
Lọc cơ bản
CLIP
BestPool

Hình 14: Số lượng ví dụ trong mỗi năm trong các benchmark của chúng tôi.

28

--- TRANG 29 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 15: Số lượng ví dụ trong mỗi năm trong YFCC 15M gốc. Trục X là tháng tải lên và trục y là số lượng ví dụ trong tháng đó.

Bảng 12: Số lượng ví dụ trong TIC-RedCaps trong mỗi năm.
[Bảng được duy trì như trong bản gốc]

Bảng 13: Số lượng ví dụ trong TIC-YFCC trong mỗi năm.
[Bảng được duy trì như trong bản gốc]

Bảng 14: Số lượng ví dụ trong TIC-DataComp trong mỗi năm trước khi lọc.
[Bảng được duy trì như trong bản gốc]

Tiếp theo, chúng tôi lập bảng số lượng ví dụ trong các bộ dữ liệu đánh giá truy xuất của chúng tôi. Vì kích thước bộ dữ liệu đánh giá khác nhau tại các bước thời gian khác nhau, chúng tôi lấy mẫu con bộ dữ liệu đến kích thước cố định trước khi thực hiện đánh giá truy xuất. Trên TIC-YFCC và TIC-RedCaps, chúng tôi lấy mẫu ngẫu nhiên 1000 cặp hình ảnh-văn bản từ các bộ dữ liệu đánh giá này. Đối với TIC-DataComp, chúng tôi lấy mẫu ngẫu nhiên 4000 cặp hình ảnh-văn bản. Chúng tôi lặp lại quy trình này cho 3 seed và báo cáo hiệu suất tổng hợp.

Bảng 15: Số lượng ví dụ đánh giá truy xuất trong TIC-RedCaps trong mỗi năm.
[Bảng được duy trì như trong bản gốc]

Bảng 16: Số lượng ví dụ đánh giá truy xuất trong TIC-YFCC trong mỗi năm.
[Bảng được duy trì như trong bản gốc]

Bảng 17: Số lượng ví dụ đánh giá truy xuất trong TIC-DataComp trong mỗi năm trước khi lọc.
[Bảng được duy trì như trong bản gốc]

29

--- TRANG 30 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

C.4 CÁC RÀNG BUỘC TÍNH TOÁN CHO CÁC BỘ DỮ LIỆU KHÁC NHAU

Chúng tôi tuân thủ chặt chẽ các ràng buộc ngân sách tính toán từ Gadre et al. (2023). Cụ thể, trên TIC-DataComp, chúng tôi hạn chế sử dụng chính xác cùng lượng tính toán tổng thể như được cố định trong Gadre et al. (2023). Dưới đây chúng tôi liệt kê tổng MAC chính xác trên mỗi bộ dữ liệu:

• TIC-YFCC: Tổng MAC: 3,4×10^18
• TIC-RedCaps: Tổng MAC: 3,4×10^18
• TIC-DataComp medium: Tổng MAC: 3,0×10^18
• TIC-DataComp large: Tổng MAC: 2,7×10^19
• TIC-DataComp xlarge: Tổng MAC: 2,7×10^20

Đối với kiến trúc ViT-B, những giá trị này tương ứng với 20k lần lặp trên TIC-YFCC (kích thước batch: 8192), TIC-RedCaps (kích thước batch: 8192), 35k lần lặp trên TIC-DataComp (M) (kích thước batch: 4096), 157k lần lặp trên TIC-DataComp (L) (kích thước batch: 8192), và 143,5k lần lặp trên TIC-DataComp (XL) (kích thước batch: 90100). Chúng tôi chia đều các lần lặp này giữa tất cả các bước thời gian.

C.5 PIPELINE TẠO CHO CÁC BỘ DỮ LIỆU ĐÁNH GIÁ

TIC-DataComp-Retrieval Để tạo một tác vụ truy xuất, chúng tôi lấy mẫu một lô các cặp hình ảnh-văn bản IID từ các timestamp khác nhau và đánh giá hiệu suất truy xuất văn bản cho hình ảnh tương ứng (tương tự, truy xuất hình ảnh cho văn bản tương ứng). Cùng với các đánh giá chung, chúng tôi cũng xây dựng các bộ dữ liệu từ các miền cụ thể, ví dụ tập con Covid-19 và tập con Flickr. Để tạo Covid-19, chúng tôi lọc bộ dữ liệu để chỉ giữ lại các cặp nơi chú thích chứa đề cập đến "covid". Quy trình tìm kiếm này giới hạn dữ liệu chỉ sau năm 2019. Đối với tập con Flickr, chúng tôi lọc bộ dữ liệu để chỉ giữ lại các cặp nơi "url" tương ứng chứa dữ liệu từ Flickr.

TIC-DataComp-Net Chúng tôi tạo bộ dữ liệu phân loại động TIC-DataComp-Net với các lớp ImageNet từ dữ liệu CommonPool được bổ sung thông tin thời gian. Quy trình xây dựng của chúng tôi lấy cảm hứng từ quy trình xây dựng LAIONet được mô tả trong Shirali & Hardt (2023). Cụ thể, trước tiên chúng tôi lọc các ví dụ nơi chú thích tương ứng chứa một và chỉ một trong các synset của ImageNet-1K. Chúng tôi cũng áp dụng lọc cơ bản bổ sung (Gadre et al., 2023) để đảm bảo rằng hình ảnh có ít nhất 200 kích thước trong chiều nhỏ nhất và chú thích chứa ít nhất 2 từ và 5 ký tự. Sau khi lọc các ví dụ với synset ImageNet, chúng tôi chỉ giữ lại các ví dụ nơi độ tương tự—được đánh giá bởi mô hình embedding câu sẵn có (Reimers & Gurevych, 2019)—giữa định nghĩa synset imagenet và chú thích vượt quá ngưỡng 0,5. Mục tiêu của bước lọc này là hạn chế các ví dụ với sự liên kết "cao" giữa chú thích và định nghĩa synset imagenet. Bước cuối cùng này khác với việc xây dựng LAIONet. Quan trọng, khác với LAIONet, chúng tôi không lọc các cặp hình ảnh-văn bản với điểm tương tự CLIP để tránh làm thiên lệch quy trình lựa chọn bộ dữ liệu.

C.6 PHÂN TÍCH CHUYỂN ĐỔI PHÂN PHỐI TRÊN CÁC BENCHMARK ĐƯỢC ĐỀ XUẤT

20 30 40 50 60
Retrieval Performance on Flickr 2014-20152030405060Retrieval Performance on Covid 2021-2022
Mô hình OpenAI được huấn luyện trên dữ liệu trước 2020
Mô hình OpenClip được huấn luyện trên dữ liệu trước 2022

10 20 30 40 50 60 70
Retrieval Performance on Flickr 2014-201510203040506070Retrieval Performance on Flickr 2021-2022
Mô hình OpenAI được huấn luyện trên dữ liệu trước 2020
Mô hình OpenClip được huấn luyện trên dữ liệu trước 2022

Hình 16: (Trái) So sánh hiệu suất truy xuất trên các truy vấn COVID so với truy vấn Flickr (xây dựng được mô tả trong App. C.5). (Phải) So sánh trên dữ liệu Flickr cũ so với dữ liệu Flickr mới. Rõ ràng, chúng tôi quan sát thấy rằng trong khi khoảng cách trên dữ liệu flickr cũ so với mới nhỏ, khoảng cách lớn hơn đáng kể trên các truy vấn Covid.

30

--- TRANG 31 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75
Tic-DataComp-Net Performance on 2014-20150.400.450.500.550.600.650.700.75Tic-DataComp-Net Performance on 2021-2022
Mô hình OpenAI được huấn luyện trên dữ liệu trước 2020
Mô hình OpenClip được huấn luyện trên dữ liệu trước 2022

0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800
Tic-DataComp-Net Performance on 2014-20150.550.600.650.700.750.80Tic-DataComp-Net Performance on 2021-2022
Cây con Xe cơ giới
Mô hình OpenAI được huấn luyện trên dữ liệu trước 2020
Mô hình OpenClip được huấn luyện trên dữ liệu trước 2022

Hình 17: (Trái) So sánh trên dữ liệu cũ so với mới từ TIC-DataComp-Net. (Phải) So sánh trên nút xe cơ giới từ TIC-DataComp-Net. Đối với tác vụ phân loại của chúng tôi, chúng tôi quan sát sự giảm rất nhỏ (≈1%) khi tính trung bình trên tất cả các danh mục. Tuy nhiên, chúng tôi quan sát khoảng cách đáng kể trên các lớp trong cây con "xe cơ giới", khi so sánh các mô hình OpenAI và OpenCLIP. Những phát hiện này làm nổi bật rằng trong khi các lớp ImageNet tổng thể có thể vượt thời gian, một số danh mục nhất định có xu hướng phát triển nhanh hơn những danh mục khác.

Phân tích TIC-DataComp thông qua lăng kính của các tác vụ đánh giá được xây dựng Ở đây, chúng tôi so sánh hiệu suất của các mô hình OpenAI và OpenCLIP trên các bộ dữ liệu của chúng tôi. Chúng tôi quan sát khoảng cách hiệu suất đáng kể giữa các mô hình OpenAI và OpenCLIP trên tác vụ truy xuất động của chúng tôi (Hình 1). Khoảng cách này mở rộng đáng kể trên các truy vấn truy xuất nơi chú thích đề cập đến COVID-19. Mặt khác, các mô hình OpenAI và OpenCLIP thể hiện độ bền tương tự cho truy xuất trên dữ liệu đến từ Flickr làm nổi bật rằng dữ liệu từ một số miền không thể hiện các chuyển đổi gây ra sụt giảm hiệu suất. Đối với tác vụ phân loại của chúng tôi, chúng tôi quan sát sự giảm rất nhỏ (≈1%) khi tính trung bình trên tất cả các danh mục. Tuy nhiên, chúng tôi quan sát khoảng cách đáng kể trên các cây con cụ thể trong ImageNet. Ví dụ, các lớp trong cây con "xe cơ giới" cho thấy khoảng 7% sụt giảm hiệu suất, khi so sánh các mô hình OpenAI và OpenCLIP. Những phát hiện này làm nổi bật rằng trong khi các lớp ImageNet tổng thể có thể vượt thời gian, một số danh mục nhất định có xu hướng phát triển nhanh hơn những danh mục khác. Phân tích định tính và định lượng của chúng tôi trên TIC-DataComp rõ ràng làm nổi bật sự tiến hóa của các phân phối và nắm bắt các thuộc tính khác nhau so với các benchmark tiêu chuẩn.

Phân tích định lượng trên TIC-YFCC Chúng tôi phân tích TIC-YFCC bằng cách sử dụng các encoder câu và hình ảnh sẵn có. Đối với encoder câu sẵn có, chúng tôi đã sử dụng sentence transformer hiện có từ Hugging Face (Reimers & Gurevych, 2019). Đối với encoder hình ảnh, chúng tôi sử dụng mô hình CLIP được huấn luyện trước ViT-B-16 (Radford et al., 2021; Ilharco et al., 2021).

Chúng tôi trước tiên nhúng hình ảnh từ các bước thời gian khác nhau với encoder OpenAI CLIP và sau đó tính toán Frechet Inception Distance (FID; Seitzer (2020)). Khi thời gian trôi qua, chúng tôi quan sát rằng khoảng cách FID tăng đối với dữ liệu từ bước thời gian đầu tiên (Hình 18). Tương tự, chúng tôi sử dụng sentence transformer được huấn luyện trước để trích xuất top-5 danh mục từ Wordnet Nouns cho mỗi chú thích. Sau đó chúng tôi thu được phân phối trên những Noun này cho mỗi bước thời gian. Chúng tôi quan sát thấy rằng khoảng cách TV trên phân phối WordNet noun phát triển theo thời gian khi so sánh với dữ liệu từ bước thời gian đầu tiên.

C.7 PIPELINE TẠO CHO TIC-DATACOMP

Chúng tôi thu thập timestamp cho bộ dữ liệu CommonPool được giới thiệu trong DataComp. Chúng tôi lặp lại quy trình crawling được mô tả trong Gadre et al. (2023) để tải xuống các file WARC từ Common Crawl. Cụ thể, chúng tôi tuân theo cùng quy trình nhiều bước bao gồm: (i) phân tích URL và alt-text từ các dump Common Crawl và tải xuống những hình ảnh này; (ii) gắn thẻ hình ảnh với meta data và id của batch common crawl; và (iii) tiến hành loại bỏ trùng lặp tập đánh giá và lọc nội dung an toàn.

Sau khi tải xuống các file WARC, chúng tôi thực hiện join với 12,8B ví dụ datacomp. Trong quá trình join này, chúng tôi mất khoảng 0,1B ví dụ không còn có sẵn trực tuyến. Hơn nữa, trong khi thực hiện join này, chúng tôi chỉ giữ lại các ví dụ với lần xuất hiện đầu tiên của chúng. Điều này được thực hiện trước khi chạy bất kỳ loại bỏ trùng lặp nào trên các cặp hình ảnh-văn bản cho các khớp chính xác như được thực hiện trong Gadre et al. (2023).

Nguồn của DataComp là Common Crawl, định kỳ phát hành các snapshot dữ liệu đã crawl web, thường theo tháng kể từ năm 2014 với các trang web mới và được cập nhật. Quy trình này cung cấp timestamp ở độ chi tiết của tháng, trải dài các năm 2014–2022.

31

--- TRANG 32 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

(a) TIC-YFCC
(b) TIC-DataComp (M)

Hình 18: Kết quả chuyển đổi phân phối. Phân tích trên TIC-YFCC và TIC-DataComp (M) sử dụng các encoder câu và hình ảnh sẵn có. Chúng tôi trước tiên nhúng hình ảnh từ các bước thời gian khác nhau với encoder OpenAI CLIP và sau đó tính toán Frechet Inception Distance (FID; Seitzer (2020)). Khi thời gian trôi qua, chúng tôi quan sát rằng khoảng cách FID tăng đối với dữ liệu từ bước thời gian đầu tiên. Tương tự, khoảng cách TV trên phân phối phân loại trên synset Wordnet Noun cũng tăng theo thời gian khi so sánh với phân phối phân loại trên timestep đầu tiên.

[Các ma trận và biểu đồ được duy trì như trong bản gốc]

Hình 19: Phân tích chuyển đổi phân phối trên TIC-DataComp (M) sử dụng các encoder câu và hình ảnh sẵn có. Chúng tôi trước tiên nhúng hình ảnh từ các bước thời gian khác nhau với encoder OpenAI CLIP và sau đó tính toán Frechet Inception Distance (FID; Seitzer (2020)). Khi thời gian trôi qua, chúng tôi quan sát rằng khoảng cách FID tăng đối với dữ liệu từ bước thời gian đầu tiên. Tương tự, khoảng cách TV trên phân phối phân loại trên synset Wordnet Noun cũng tăng theo thời gian khi so sánh với phân phối phân loại trên timestep đầu tiên.

Chúng tôi lưu ý rằng trong khi thông tin thời gian được bổ sung này có thể chứa một số nhiễu, trung bình, chúng tôi thấy nó là một proxy khá chính xác cho thời gian tải lên của các trang web. Để thực hiện kiểm tra ban đầu, chúng tôi lưu ý rằng dữ liệu của chúng tôi chứa hình ảnh từ flickr cung cấp API để truy vấn timestamp tải lên thực. Vì vậy, chúng tôi trích xuất 10k ví dụ từ benchmark TIC-DataComp của chúng tôi và truy vấn Flickr cho timestamp thực của chúng. Hình 20 tóm tắt timestamp thực với timestamp được trích xuất từ CC.

32

--- TRANG 33 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

2014 2015 2016 2017 2018 2019 2020 2021 2022
CC time step2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022True time step0.09 0.03 0.05 0.02 0.06 0.03 0.13 0.15 0.15
0.63 0.34 0.38 0.54 0.58 0.44 0.29 0.23 0.18
2.01 1.16 1.41 1.37 1.63 1.17 0.83 0.79 0.75
4.36 2.19 3.18 2.49 3.14 1.93 2.00 1.48 1.76
7.53 5.06 4.94 5.25 5.83 3.61 3.25 2.40 2.79
10.80 6.82 7.79 7.06 7.51 5.46 4.84 3.38 3.98
12.89 6.68 8.41 9.30 7.92 5.76 4.83 3.75 4.17
16.24 8.67 9.49 11.85 8.93 6.53 5.76 5.06 4.96
19.07 11.58 10.30 11.42 8.56 7.49 6.63 5.81 6.22
12.01 8.12 10.05 10.41 8.66 7.81 6.66 6.56 6.07
14.29 8.21 7.30 8.16 6.17 7.32 6.07 5.18 5.34
0.00 41.08 9.36 8.80 6.15 7.64 6.94 6.26 4.79
0.02 0.02 27.31 9.75 5.31 7.64 6.91 5.86 5.15
0.02 0.00 0.01 13.57 8.58 7.99 6.49 5.70 4.79
0.01 0.01 0.00 0.01 20.87 10.94 6.99 5.36 5.29
0.01 0.00 0.00 0.00 0.01 18.21 10.76 5.90 5.49
0.00 0.01 0.00 0.00 0.02 0.02 20.52 10.06 4.33
0.01 0.00 0.00 0.01 0.01 0.02 0.01 26.01 9.62
0.00 0.00 0.01 0.00 0.03 0.00 0.02 0.02 24.09
0510152025303540

Hình 20: So sánh timestamp được gán bởi Common Crawl và timestamp thực trên tập con 10k ví dụ chứa các cặp hình ảnh-văn bản từ Flickr. Chúng tôi quan sát xu hướng rõ ràng nơi timestamp CC tương quan với timestamp thực.

D CHI TIẾT THÍ NGHIỆM BỔ SUNG

D.1 CHI TIẾT BỔ SUNG VỀ THÍ NGHIỆM HỌC LIÊN TỤC PHÂN CHIA IID IMAGENET

Với dữ liệu ImageNet, chúng tôi xem xét 2, 4 và 8 phân chia bao gồm toàn bộ bộ dữ liệu. Thiết kế này được lấy cảm hứng từ Ash & Adams (2020). Chúng tôi xem xét kiến trúc ViT-B/16 được huấn luyện trong 300 epoch trên dữ liệu đầy đủ và chia các lần lặp tương ứng với 300 epoch đều cho k phân chia khi huấn luyện tuần tự. Chúng tôi giữ tất cả các siêu tham số khác, chẳng hạn như tốc độ học, optimizer, và kích thước batch, được đặt thành các giá trị tiêu chuẩn thường được sử dụng để huấn luyện ViT-B/16 trên bộ dữ liệu ImageNet (Dosovitskiy et al., 2020). Chúng tôi cũng sử dụng regularization ℓ2 và augmentation trên dữ liệu huấn luyện ImageNet. Chúng tôi đánh giá các mô hình trên tập thử nghiệm ImageNet IID.

Các thí nghiệm Imagenet của chúng tôi chủ yếu được lấy cảm hứng từ hiện tượng "mất tính dẻo dai" được mô tả trong Ash & Adams (2020). Nghiên cứu của họ chứng minh rằng các mô hình được huấn luyện tuần tự trên hai phân chia dữ liệu CIFAR-10 (ban đầu trên 50%, theo sau là 100% dữ liệu) thể hiện khái quát hóa kém hơn so với các mô hình được huấn luyện từ đầu trên toàn bộ bộ dữ liệu. Vì chúng tôi không quan sát hành vi này cho huấn luyện liên tục CLIP, chúng tôi đã điều tra sự tồn tại của các hành vi như vậy trên tới 8 phân chia Imagenet. Những phát hiện của chúng tôi tiết lộ rằng baseline tích lũy đơn giản (không có ngân sách bổ sung) vẫn cạnh tranh gần với mô hình Oracle (được hưởng lợi từ việc sử dụng ngân sách tính toán đầy đủ trên toàn bộ dữ liệu huấn luyện được gộp từ đầu).

Các công trình trước đây (Prabhu et al., 2023; Hu et al., 2021) đã thực hiện các thí nghiệm học liên tục trên Imagenet để so sánh các phương pháp khác nhau và làm nổi bật hiệu quả của huấn luyện liên tục trên các thiết lập học liên tục tổng hợp được dẫn xuất từ ImageNet. Trong khi những bài báo này bao gồm kết quả với phương pháp Oracle, sự khác biệt trong các thiết lập được xem xét trong những nghiên cứu này hạn chế so sánh trực tiếp. Cụ thể, chúng tôi hiển thị khoảng cách hiệu suất dưới 1% trong cùng thiết lập được sử dụng khác trong bài báo khi sử dụng các quy trình huấn luyện SOTA đạt 81% hiệu suất validation. So sánh với Hu et al. (2021) được tham chiếu không hiển thị liệu khoảng cách hiệu suất 65% đến 77% trong Bảng 1 của họ có thể được thu hẹp bằng cách tăng tính toán cho phương pháp của họ hay không. Thay vào đó, các tác giả cho thấy rằng nếu họ hạn chế tính toán cho Oracle trong Bảng 2, hiệu suất Oracle giảm xuống 68% (với khoảng cách ≈3%). Hơn nữa, trong Prabhu et al. (2023), các tác giả thực hiện thí nghiệm trên DI-Imagenet-2k nơi họ bắt đầu với bộ nhớ ban đầu của Imagenet-1k 1,2M mẫu và tuần tự quan sát dữ liệu cho cùng các lớp 1k từ pool Imagenet-21k. Điều này làm cho việc so sánh độ chính xác streaming (hoặc độ chính xác Imagenet-1k) cho các phương pháp khác nhau không thể so sánh với thiết lập của chúng tôi (với khoảng cách hơn 7% trong độ chính xác streaming ngay cả tại bước 8 so với dưới 1% trong thiết lập của chúng tôi).

33

--- TRANG 34 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

D.2 CHI TIẾT HUẤN LUYỆN VÀ SIÊU THAM SỐ

Chúng tôi tạo một thiết lập thí nghiệm chung bằng cách cố định quy trình huấn luyện cho các lần chạy tuần tự. Trừ khi được chỉ định khác, chúng tôi tuân thủ chặt chẽ công thức huấn luyện CLIP được đề xuất trong (Ilharco et al., 2021; Radford et al., 2021) nơi chúng tôi huấn luyện các mô hình với mục tiêu contrastive trên hình ảnh và chú thích. Với một tập hợp các cặp hình ảnh-văn bản, chúng tôi huấn luyện một encoder hình ảnh và một encoder văn bản sao cho độ tương tự giữa các biểu diễn của hình ảnh và văn bản tương ứng của chúng được tối đa hóa so với các cặp không liên kết. Chỉ LwF khác biệt với quy trình huấn luyện tiêu chuẩn này. Đối với mỗi benchmark, chúng tôi chọn Vision Transformers (ViT) làm encoder hình ảnh, cụ thể, chúng tôi cố định kiến trúc mô hình thành ViT-B/16 (Dosovitskiy et al., 2021). Chúng tôi cố định optimizer Adam và các siêu tham số của nó thành các giá trị được đề xuất trong (Ilharco et al., 2021).

Chúng tôi chủ yếu ablate chỉ trên hai thứ: tốc độ học tối đa với lịch trình tốc độ học cosine và lần lặp warm up cho huấn luyện tuần tự. Để chọn các siêu tham số khác, chúng tôi tuân theo thư viện OpenCLIP (Ilharco et al., 2021).

D.3 KÍCH THƯỚC PHÁT LẠI VỚI CÁC CHIẾN LƯỢC EXP VÀ EQUAL

Chúng tôi mặc định sử dụng kích thước dữ liệu 2D nơi D đại diện cho kích thước dữ liệu đến từ bước thời gian mới. Như được mô tả trong văn bản chính, đối với -Exp, chúng tôi giảm kích thước buffer xuống một nửa so với những gì chúng tôi đã sử dụng tại bước thời gian cũ và sử dụng phần còn lại làm dữ liệu từ bước thời gian trước đó. App. C.3 liệt kê kích thước bộ dữ liệu cho mỗi benchmark quy định kích thước buffer chính xác.

E KẾT QUẢ VỚI CÁC PHƯƠNG PHÁP HỌC LIÊN TỤC KHÁC

E.1 KẾT QUẢ VỚI PHƯƠNG PHÁP EWC

Như được đề xuất trong công trình gốc Kirkpatrick et al. (2017), chúng tôi triển khai phương pháp EWC nơi chúng tôi tối ưu hóa loss sau:

LEWCpθq"Lpθq`ÿiλEWC/2Fipθi´θt´1,iq2,

trong đó Lpθq là loss contrastive tiêu chuẩn trên dữ liệu từ bước thời gian t, Fi là mục thứ i đường chéo của ma trận thông tin fisher, và θt´1 là các tham số đông lạnh từ bước thời gian trước đó. Chúng tôi thực hiện thí nghiệm với các giá trị λEWC khác nhau ∈ {1,10,100,400} (xem Bảng 18).

Bảng 18: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi với EWC. ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán. Đối với các tác vụ tĩnh, chúng tôi lập bảng độ chính xác của các mô hình thu được trên timestamp cuối cùng. Đối với các tác vụ động, chúng tôi lập bảng chuyển giao tiến/chuyển giao ngược và hiệu suất ID trên các tác vụ truy xuất (Mục 2.3). Chúng tôi quan sát thấy rằng EWC hoạt động kém hơn Sequential, Patching và LwF.

[Bảng được duy trì như trong bản gốc]

E.2 KẾT QUẢ VỚI PHƯƠNG PHÁP LẤY MẪU DỰA TRÊN OVERSAMPLING + COUNTING

Trong phần này, chúng tôi thực hiện ablation trên Cumulative-Equal. Cụ thể, chúng tôi đã thực hiện hai sửa đổi sau: (i) Lấy mẫu dựa trên đếm: Thay vì lấy mẫu ngẫu nhiên, chúng tôi đã triển khai việc lấy mẫu con dựa trên đếm ưu tiên các ví dụ không/ít được sử dụng; (ii) Oversampling: Chúng tôi oversample dữ liệu từ các timestep cũ với tỷ lệ tỷ lệ nghịch với tỷ lệ ví dụ, tức là nếu dữ liệu cũ có kích thước D/2 và dữ liệu mới có kích thước D, thì chúng tôi upsample dữ liệu cũ với tỷ lệ 2:1.

Tuy nhiên, chúng tôi quan sát thấy rằng phương pháp này không cải thiện hiệu suất so với Cumulative-Equal và thực tế làm tổn hại hiệu suất một chút (xem Bảng 19). Chúng tôi giả thuyết rằng điều này có thể do tiện ích biên giảm của dữ liệu được gán nhãn như được làm nổi bật trong Cui et al. (2019). Công trình của họ lập luận rằng do sự chồng chéo thông tin giữa dữ liệu, khi số lượng mẫu tăng, lợi ích biên mà một mô hình có thể trích xuất từ dữ liệu giảm. Kết quả là, Cui et al. (2019) đề xuất sử dụng "kích thước mẫu hiệu quả" thay vì số lượng mẫu thực tế để thu được tỷ lệ được sử dụng để thực hiện re-sampling hoặc re-weighting. Cụ thể, biểu thức của "kích thước mẫu hiệu quả" được cho bởi En"1´βn/(1´β) trong đó n là kích thước mẫu gốc và β là một siêu tham số mà Cui et al. (2019) chọn từ β∈{0.9,0.99,0.999,0.9999}.

Đối với các bước thời gian khác nhau, chúng tôi tận dụng biểu thức En này để tính toán số lượng mẫu hiệu quả. Trong các thiết lập của chúng tôi (ngay cả ở quy mô nhỏ), các bộ dữ liệu của chúng tôi chứa bậc 100k cặp hình ảnh-văn bản ngay cả sau khi lấy mẫu con dữ liệu từ bước thời gian cũ. Ví dụ, với baseline -Equal, khi huấn luyện trên bước thời gian cuối cùng (tức là 2022), bộ dữ liệu nhỏ nhất (tức là 2016) có khoảng 400k mẫu. Thay vào biểu thức cho kích thước mẫu hiệu quả từ Cui et al. (2019), chúng tôi quan sát thấy rằng đối với tất cả β∈(0,0.99999), tỷ lệ kích thước mẫu hiệu quả cho các bước thời gian khác nhau vẫn gần với 1. Điều này có thể làm nổi bật lý do tại sao chiến lược over-sampling ngây thơ của chúng tôi không cải thiện so với không-oversampling.

Bảng 19: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi với oversampling và lấy mẫu dựa trên đếm. ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán. Đối với các tác vụ tĩnh, chúng tôi lập bảng độ chính xác của các mô hình thu được trên timestamp cuối cùng. Đối với các tác vụ động, chúng tôi lập bảng chuyển giao tiến/chuyển giao ngược và hiệu suất ID trên các tác vụ truy xuất (Mục 2.3).

[Bảng được duy trì như trong bản gốc]

F KẾT QUẢ VỚI CÁC METRIC ĐÁNH GIÁ MỚI TRÊN CÁC TÁC VỤ ĐỘNG

Nhớ lại, T đại diện cho số bước thời gian mà chúng tôi có dữ liệu. Đối với mỗi phương pháp huấn luyện, chúng tôi tạo ra tổng cộng T mô hình, mỗi mô hình tương ứng với cuối huấn luyện tại một bước thời gian cụ thể. Đối với mỗi mô hình và một tác vụ đánh giá động, chúng tôi thu được T giá trị hiệu suất. Chúng tôi biểu diễn những giá trị này bằng cách sử dụng ma trận hiệu suất E, trong đó mỗi mục Ei,j biểu thị hiệu suất của mô hình thu được sau khi quan sát dữ liệu huấn luyện tại bước thời gian i khi được đánh giá trên bộ dữ liệu từ bước thời gian j. Định nghĩa các metric ngược như trong Mục 2.2 liên quan đến việc tính trung bình các mục trong đường chéo trên và dưới của ma trận hiệu suất E của chúng tôi, tức là nó được tính toán như trung bình của các bước thời gian trước mỗi bước huấn luyện (tức là tam giác dưới của E), tức là ∑i≥j Eij/(T(T−1))/2. Metric chuyển giao ngược này đã được sử dụng trong các công trình trước đây Lin et al. (2021). Tuy nhiên, phương pháp này vô tình dẫn đến metric chuyển giao ngược bị ảnh hưởng bởi các bước thời gian đánh giá sau này dẫn đến số hiệu suất chuyển giao ngược hơi lớn hơn hiệu suất ID.

Để giải quyết vấn đề này, chúng tôi đã sửa đổi phương pháp tính toán metric của mình thành metric như trong Díaz-Rodríguez et al. (2018). Bây giờ, chúng tôi chuẩn hóa dữ liệu trong mỗi hàng, tương ứng với các bước thời gian đánh giá bằng cách trừ hiệu suất ID. Điều chỉnh này đảm bảo biểu diễn cân bằng và chính xác hơn trên tất cả các bước thời gian huấn luyện. Cụ thể, các metric chuyển giao tiến và ngược được cập nhật của chúng tôi có thể được tóm tắt như sau:

• Chuyển giao ngược: Gọi Bi biểu thị hiệu suất trung bình trên các tác vụ đánh giá trước thời gian i, thì chúng tôi định nghĩa chuyển giao ngược như trung bình của Bi qua mỗi bước huấn luyện, tức là ∑T(i=2)∑i≥j Eij−Eii/(T(T−1)/2)
• Chuyển giao tiến: Gọi Fi biểu thị hiệu suất trung bình trên các tác vụ đánh giá sau thời gian i, thì chúng tôi định nghĩa chuyển giao tiến như trung bình của Fi qua mỗi bước huấn luyện, tức là ∑T−1(i=1)∑i≤j Eij−Eii/(T(T−1)/2)

35

--- TRANG 36 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 20: Hiệu suất zero shot trên các benchmark time-continual của chúng tôi. ˚và ˚˚biểu thị các phương pháp vi phạm ngân sách tính toán. Đối với các tác vụ động, chúng tôi lập bảng chuyển giao tiến/chuyển giao ngược và hiệu suất ID trên các tác vụ truy xuất với các metric được cập nhật như được định nghĩa trong App. F.

[Bảng được duy trì như trong bản gốc với tất cả các số liệu hiệu suất]

36
