# 2210.07839.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2210.07839.pdf
# Kích thước tệp: 21913419 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
BỘ MÃ HÓA TỰ ĐỘNG ĐƯỢC CHE PHỦ ÂM THANH-HÌNH ẢNH ĐỐI CHIẾU
Yuan Gong1(yuangong@mit.edu) , Andrew Rouditchenko1
Alexander H. Liu1, David Harwath2, Leonid Karlinsky3;4, Hilde Kuehne4;5, James Glass1
1MIT CSAIL;2UT Austin;3IBM Research AI;4MIT-IBM Watson AI Lab;5Goethe University Frankfurt
TÓM TẮT
Trong bài báo này, chúng tôi đầu tiên mở rộng mô hình Bộ mã hóa tự động được che phủ (MAE) gần đây từ một phương thức duy nhất sang đa phương thức âm thanh-hình ảnh. Tiếp theo, chúng tôi đề xuất Bộ mã hóa tự động được che phủ âm thanh-hình ảnh đối chiếu (CA V-MAE) bằng cách kết hợp học tập đối chiếu và mô hình hóa dữ liệu được che phủ, hai khung công tác học tập tự giám sát chính, để học một biểu diễn âm thanh-hình ảnh kết hợp và phối hợp.
Các thí nghiệm của chúng tôi cho thấy mục tiêu học tập tương ứng âm thanh-hình ảnh đối chiếu không chỉ cho phép mô hình thực hiện các nhiệm vụ truy xuất âm thanh-hình ảnh, mà còn giúp mô hình học một biểu diễn kết hợp tốt hơn. Kết quả là, CA V-MAE được huấn luyện trước hoàn toàn tự giám sát của chúng tôi đạt được độ chính xác SOTA mới là 65.9% trên VGGSound và có thể so sánh với mô hình được huấn luyện trước có giám sát tốt nhất trước đây trên AudioSet trong nhiệm vụ phân loại sự kiện âm thanh-hình ảnh. Mã nguồn và mô hình được huấn luyện trước tại https://github.com/yuangongnd/cav-mae .

1 GIỚI THIỆU
Các phương thức âm thanh và hình ảnh có các đặc tính khác nhau, tuy nhiên con người có thể kết nối và tích hợp chúng một cách liền mạch để cảm nhận thế giới. Phát triển các thuật toán học tập để tái tạo những khả năng này, đặc biệt là cho việc hợp nhất và truy xuất âm thanh-hình ảnh đa phương thức là điều rất quan trọng. Vì việc gán nhãn thủ công âm thanh và video là tốn kém và khó mở rộng, cách thức sử dụng dữ liệu video không nhãn quy mô web theo cách tự giám sát đã trở thành một câu hỏi nghiên cứu cốt lõi.

Một hướng nghiên cứu chính của học tập tự giám sát âm thanh-hình ảnh là tận dụng các tương ứng âm thanh-hình ảnh tự nhiên được tìm thấy trong video. Trong số nhiều cách sử dụng những tương ứng như vậy, Học tập âm thanh-hình ảnh đối chiếu đã cho thấy là một phương pháp đơn giản nhưng hiệu quả (Arandjelovic & Zisserman, 2018; Morgado et al., 2021b; Rouditchenko et al., 2021). Nó học các biểu diễn phối hợp1 gần nhau hơn cho các mẫu âm thanh và hình ảnh được ghép cặp so với các mẫu không khớp. Những biểu diễn phối hợp như vậy đặc biệt hữu ích cho các nhiệm vụ như truy xuất đa phương thức.

Một khung công tác học tập tự giám sát thường được sử dụng khác là Mô hình hóa dữ liệu được che phủ (MDM), nó học một biểu diễn có ý nghĩa với nhiệm vụ đặt trước là khôi phục các đầu vào hoặc đặc trưng gốc từ những cái bị hỏng (Devlin et al., 2019).

Đặc biệt, dựa trên Audio Spectrogram Transformer (Gong et al., 2021a) và Vision Transformer (Dosovitskiy et al., 2020), Bộ mã hóa tự động được che phủ đơn phương thức (MAE) (He et al., 2022) đã đạt được hiệu suất tốt nhất (SOTA) trên các nhiệm vụ hình ảnh và âm thanh (Huang et al., 2022a) riêng lẻ. Được truyền cảm hứng bởi những tiến bộ này, chúng tôi đề xuất mở rộng MAE đơn phương thức thành Bộ mã hóa tự động được che phủ âm thanh-hình ảnh (A V-MAE), nhằm học một biểu diễn kết hợp hợp nhất các tín hiệu đơn phương thức.

Mặc dù hai khung công tác tự giám sát chính này đã được sử dụng rộng rãi riêng lẻ, theo hiểu biết tốt nhất của chúng tôi, chúng chưa bao giờ được kết hợp trong học tập âm thanh-hình ảnh. Thực tế, chúng tôi thấy chúng bổ sung cho nhau: Học tập âm thanh-hình ảnh đối chiếu tận dụng một cách rõ ràng thông tin cặp âm thanh-hình ảnh rất hữu ích, nhưng nó có thể loại bỏ thông tin độc nhất của phương thức hữu ích trong các nhiệm vụ xuôi dòng; Nhiệm vụ tái tạo của A V-MAE buộc biểu diễn của nó phải mã hóa phần lớn thông tin đầu vào trong việc hợp nhất, nhưng nó thiếu một mục tiêu tương ứng âm thanh-hình ảnh rõ ràng.

Điều này thúc đẩy chúng tôi thiết kế Bộ mã hóa tự động được che phủ âm thanh-hình ảnh đối chiếu (CAV-MAE) tích hợp học tập đối chiếu và mô hình hóa dữ liệu được che phủ học một biểu diễn âm thanh-hình ảnh kết hợp và phối hợp với một mô hình duy nhất.

Các thí nghiệm của chúng tôi hỗ trợ thiết kế của chúng tôi: về phân loại sự kiện âm thanh-hình ảnh, CA V-MAE vượt trội đáng kể so với các mô hình cơ sở được huấn luyện chỉ với mục tiêu đối chiếu hoặc mô hình hóa dữ liệu được che phủ, chứng minh rằng hai mục tiêu này bổ sung cho nhau trong việc học một biểu diễn âm thanh-hình ảnh kết hợp mạnh mẽ. Kết quả là, CA V-MAE đạt được độ chính xác SOTA mới là 65.9% trên VGGSound và có thể so sánh với mô hình được huấn luyện trước có giám sát tốt nhất trước đây trên AudioSet. Hơn nữa, khi đến truy xuất âm thanh-hình ảnh, CA V-MAE cũng hoạt động tốt như nhau hoặc thậm chí tốt hơn các mô hình được huấn luyện chỉ với mục tiêu đối chiếu, điều này chứng minh rằng CA V-MAE có thể học tốt cả biểu diễn kết hợp và phối hợp.

Cuối cùng, việc huấn luyện trước đa phương thức CA V-MAE cải thiện hiệu suất đơn phương thức, do đó, CA V-MAE đạt được SOTA mới cho phân loại sự kiện dựa trên âm thanh trên AudioSet-20K và VGGSound.

Tóm lại, các đóng góp của chúng tôi là: (1) Chúng tôi mở rộng MAE đơn phương thức thành A V-MAE đa phương thức, hợp nhất đầu vào âm thanh-hình ảnh cho học tập tự giám sát thông qua mô hình hóa dữ liệu được che phủ đa phương thức; (2) Quan trọng hơn, chúng tôi điều tra cách kết hợp tốt nhất học tập âm thanh-hình ảnh đối chiếu với mô hình hóa dữ liệu được che phủ và đề xuất CA V-MAE; (3) Chúng tôi chứng minh rằng các mục tiêu đối chiếu và mô hình hóa dữ liệu được che phủ là bổ sung cho nhau. Kết quả là, CA V-MAE khớp hoặc vượt trội so với các mô hình SOTA về phân loại âm thanh-hình ảnh.

2 BỘ MÃ HÓA TỰ ĐỘNG ĐƯỢC CHE PHỦ ÂM THANH-HÌNH ẢNH ĐỐI CHIẾU

2.1 KIẾN THỨC CHUẨN BỊ

2.1.1 XỬ LÝ TRƯỚC VÀ TOKENIZATION ÂM THANH VÀ HÌNH ẢNH

Như được mô tả trong Hình 1 (A), chúng tôi tuân theo xử lý trước và tokenization trong AST (Gong et al., 2021a) và ViT (Dosovitskiy et al., 2020) cho đầu vào âm thanh và hình ảnh tương ứng. Cụ thể, chúng tôi sử dụng video 10 giây (với âm thanh song song) trong AudioSet (Gemmeke et al., 2017) và VGGSound (Chen et al., 2020) để huấn luyện trước và tinh chỉnh mô hình. Đối với âm thanh, mỗi dạng sóng âm thanh 10 giây đầu tiên được chuyển đổi thành một chuỗi các đặc trưng ngân hàng lọc Mel log 128 chiều (fbank) được tính toán với cửa sổ Hanning 25ms mỗi 10ms. Điều này dẫn đến một phổ đồ 1024(thời gian)×128(tần số). Sau đó chúng tôi chia phổ đồ thành 512 miếng vuông 16×16 a=[a1;:::;a512] làm đầu vào của mô hình. Xử lý video với các mô hình Transformer tốn kém và thường đòi hỏi tài nguyên tính toán cấp công nghiệp. Để giảm chi phí tính toán và phù hợp với tài nguyên của chúng tôi, chúng tôi sử dụng chiến lược tổng hợp khung hình. Cụ thể, chúng tôi lấy mẫu đồng đều 10 khung hình RGB từ mỗi video 10 giây (tức là 1 FPS). Trong quá trình huấn luyện, chúng tôi chọn ngẫu nhiên một khung hình RGB làm đầu vào; trong quá trình suy luận, chúng tôi lấy trung bình dự đoán mô hình của mỗi khung hình RGB làm dự đoán video. So với việc nối nhiều khung hình RGB làm đầu vào của Transformer có độ phức tạp bậc hai (ví dụ, trong Nagrani et al. (2021)), tổng hợp khung hình hiệu quả hơn nhiều với độ phức tạp tuyến tính theo thời gian với chi phí không xem xét tương quan liên khung hình. Đối với mỗi khung hình RGB, chúng tôi thay đổi kích thước và cắt trung tâm thành 224×224, và sau đó chia thành 196 miếng vuông 16×16 v=[v1;:::;v196].

2.1.2 KIẾN TRÚC TRANSFORMER

Trong suốt bài báo này, chúng tôi sử dụng Transformer tiêu chuẩn (Vaswani et al., 2017) làm thành phần mô hình chính của chúng tôi. Mỗi lớp Transformer bao gồm khối attention tự đa đầu (MSA), chuẩn hóa lớp (LN), và perceptron đa lớp (MLP) với các kết nối dư. Cụ thể, chúng tôi ký hiệu một lớp Transformer y=Transformer(x;MSA;LN1;LN2;MLP) là:
x0=MSA(LN1(x))+x;y=MLP(LN2(x0))+x0 (1)
trong đó MSA tính toán dot-product attention của mỗi phần tử của x và do đó có độ phức tạp bậc hai w.r.t. kích thước của x. Vui lòng tham khảo Vaswani et al. (2017) để biết chi tiết thêm về Transformers.

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023

[Tiếp tục với bản dịch phần còn lại...]

Bộ mã hóa 
kết hợp
Che phủ 
và 
nối
Bộ giải mã 
kết hợp
A1
A3
V2
A1
A2
A3
A4
V1
V2
V3
Mất mát 
tái tạo
Bộ mã hóa tự động 
được che phủ 
âm thanh-hình ảnh 
thông thường
V4
V3
E
A1
E
A3
E
v2
E
v3
E
A1
[M]
E
A3
[M]
[M]
E
v2
E
v3
[M]
A1
A2
A3
A4
V1
V2
V3
V4
A1
A2
A3
A4
V1
V2
V3
V4
Bộ mã hóa 
âm thanh
Bộ mã hóa 
hình ảnh
E
A1
E
A2
E
A3
E
A4
E
v1
E
v2
E
v3
E
v4
C
A
C
v
Mất mát 
đối chiếu
...
...
...
...
64
8
1024
128
...
A1
A2
A512
224
224
14
...
...
...
14
...
V1
V2
V196
A) 
Tokenization 
âm thanh 
và 
hình ảnh
Che phủ 
Bộ giải mã 
kết hợp
A1
A3
V2
A1
A2
A3
A4
V1
V2
V3
V4
V3
E
A1
[M]
E
A3
[M]
[M]
E
v2
E
v3
[M]
A1
A2
A3
A4
V1
V2
V3
V4
Bộ mã hóa 
kết hợp
Bộ mã hóa 
âm thanh
Bộ mã hóa 
hình ảnh
E
A1
E
A3
E
v2
E
v3
E
A1
E
A3
E
v2
E
v3
E
A1
E
A3
E
v2
E
v3
Âm thanh
Hình ảnh
Nối
E
A1
E
A3
E
v2
E
v3
C
A1
C
A3
C
v2
C
v3
C
A
C
v
Mất mát 
tái tạo
Mất mát 
đối chiếu
Pooling
C) 
Bộ mã hóa tự động 
được che phủ 
âm thanh-hình ảnh 
đối chiếu
LN
a
LN
v
Học tập 
âm thanh-hình ảnh 
đối chiếu
Kết hợp
B)
Pooling
Pooling

Hình 1: Minh họa phương pháp của chúng tôi. A) Chúng tôi tokenize phổ đồ âm thanh và hình ảnh RGB thành các miếng vuông 16×16 và sử dụng chúng làm đầu vào cho tất cả các mô hình. B) Mô hình học tập âm thanh-hình ảnh đối chiếu thông thường (trên) và bộ mã hóa tự động được che phủ âm thanh-hình ảnh thông thường (dưới, cũng mới và được giới thiệu lần đầu trong bài báo này). C) Mô hình bộ mã hóa tự động được che phủ âm thanh-hình ảnh đối chiếu (CA V-MAE) được đề xuất của chúng tôi. CA V-MAE tích hợp hai khung công tác tự giám sát chính: học tập âm thanh-hình ảnh đối chiếu và mô hình hóa dữ liệu được che phủ đa phương thức, học các biểu diễn kết hợp và phối hợp và hoạt động tốt trên cả nhiệm vụ phân loại kết hợp đa phương thức và nhiệm vụ truy xuất đa phương thức.

2.1.3 HỌC TẬP ÂM THANH-HÌNH ẢNH ĐỐI CHIẾU (CAV)

Việc ghép cặp tự nhiên thông tin âm thanh và hình ảnh trong video là một tín hiệu hữu ích để học biểu diễn âm thanh-hình ảnh thông qua tự giám sát. Một mô hình CA V thông thường được hiển thị trong Hình 1.B (trên), đối với một mini-batch của N mẫu cặp âm thanh-hình ảnh, chúng tôi đầu tiên xử lý trước và tokenize âm thanh và hình ảnh và nhận được một chuỗi token âm thanh và hình ảnh {ai;vi} cho mỗi mẫu i. Sau đó chúng tôi đưa ai và vi vào các bộ mã hóa Transformer âm thanh và hình ảnh độc lập Ea() và Ev() tương ứng, và nhận được biểu diễn âm thanh và hình ảnh pooled trung bình ca_i và cv_i, tức là ca_i=MeanPool(Ea(Proja(ai)) và cv_i=MeanPool(Ev(Projv(vi)), trong đó Proja và Projv là các phép chiếu tuyến tính ánh xạ mỗi token âm thanh và hình ảnh thành R^768. Sau đó chúng tôi áp dụng mất mát đối chiếu (Phương trình 7) trên ca_i và cv_i.

2.1.4 BỘ MÃ HÓA TỰ ĐỘNG ĐƯỢC CHE PHỦ ĐƠN PHƯƠNG THỨC (MAE)

Một hướng khác của các khung công tác tự giám sát chính là mô hình hóa dữ liệu được che phủ (MDM). Trong số nhiều biến thể của MDM (ví dụ, Bao et al. (2021); Wei et al. (2022)), bộ mã hóa tự động được che phủ (MAE) là một phương pháp đơn giản nhưng hiệu quả. Đối với một mẫu đầu vào x có thể được tokenize thành x=[x1;x2;:::;xn], MAE che phủ một phần đầu vào x_mask và chỉ đưa các token không được che phủ x\x_mask vào một mô hình encoder-decoder dựa trên Transformer. Mô hình được yêu cầu tái tạo các token bị che phủ với mục tiêu tối thiểu hóa mất mát lỗi bình phương trung bình (MSE). Trong quá trình này, mô hình học một biểu diễn có ý nghĩa của dữ liệu đầu vào. Ưu điểm của MAE là đa dạng. Đầu tiên, MAE trực tiếp sử dụng đầu vào gốc làm mục tiêu dự đoán, điều này đơn giản hóa rất nhiều pipeline huấn luyện. Thứ hai, MAE chỉ đưa các token không được che phủ vào encoder, và kết hợp với tỷ lệ che phủ cao, MAE giảm đáng kể chi phí tính toán. Thứ ba, MAE đã chứng minh hiệu suất mạnh mẽ trong các nhiệm vụ đơn phương thức cho cả phương thức âm thanh và hình ảnh. Do giới hạn không gian, vui lòng tham khảo He et al. (2022); Huang et al. (2022a) cho các MAE đơn phương thức.

2.2 BỘ MÃ HÓA TỰ ĐỘNG ĐƯỢC CHE PHỦ ÂM THANH-HÌNH ẢNH THÔNG THƯỜNG (AV-MAE)

Trong khi MAE đã được áp dụng cho cả phương thức âm thanh và hình ảnh riêng lẻ, nó chưa bao giờ được áp dụng cho học tập đa phương thức âm thanh-hình ảnh. Là đóng góp đầu tiên của công trình này, chúng tôi mở rộng MAE từ một phương thức duy nhất sang đa phương thức âm thanh-hình ảnh và xây dựng một bộ mã hóa tự động âm thanh-hình ảnh "thông thường" (A V-MAE). Như được hiển thị trong Hình 1.B (dưới), đối với một cặp đầu vào âm thanh và hình ảnh, chúng tôi đầu tiên tokenize chúng thành a=[a1;:::;a512] và v=[v1;:::;v196] và chiếu chúng thành R^768 với hai lớp chiếu tuyến tính đặc trưng cho phương thức cũng như thêm nhúng loại phương thức Ea và Ev và nhúng vị trí hình sin 2-D đặc trưng cho phương thức Ep_a và Ep_v, tức là a'=Proja(a)+Ea+Ep_a và v'=Projv(v)+Ev+Ep_v. Chúng tôi nối a' và v' và xây dựng nhúng kết hợp x=[a';v'].

Sau đó chúng tôi che phủ một phần (75%) của x và chỉ đưa các token không được che phủ x_unmask=x\x_mask vào một bộ mã hóa kết hợp âm thanh-hình ảnh Ej() và nhận được đầu ra x'_unmask. Sau đó, chúng tôi đệm x'_unmask với các token được che phủ có thể huấn luyện tại vị trí gốc của chúng thành x'. Một lần nữa, chúng tôi cũng thêm nhúng loại phương thức E'_a và E'_v và nhúng vị trí hình sin 2-D đặc trưng cho phương thức Ep_a' và Ep_v' trước khi đưa x' vào bộ giải mã âm thanh-hình ảnh kết hợp Dj() để tái tạo đầu vào, tức là â,v̂=Dj(x'+[E'_a;E'_v]+[Ep_a';Ep_v'])

Cuối cùng, chúng tôi tối thiểu hóa lỗi bình phương trung bình (MSE) giữa â,v̂ và a,v đã chuẩn hóa.

So với các MAE đơn phương thức, A V-MAE có đặc trưng là một mục tiêu mô hình hóa dữ liệu được che phủ đa phương thức cho phép mô hình tái tạo một phương thức dựa trên thông tin của phương thức khác, có thể giúp mô hình học tương quan âm thanh-hình ảnh. Tuy nhiên, không có mục tiêu rõ ràng khuyến khích tương ứng âm thanh-hình ảnh được ghép cặp, A V-MAE thông thường thực sự không tận dụng hiệu quả thông tin ghép cặp âm thanh-hình ảnh (thảo luận trong Phụ lục J). Ngoài ra, việc sử dụng một bộ mã hóa kết hợp cho hai phương thức cho phép attention đa phương thức, nhưng nó cũng có nghĩa là hai phương thức rất khác nhau được xử lý với cùng trọng số, có thể dẫn đến một giải pháp không tối ưu.

2.3 BỘ MÃ HÓA TỰ ĐỘNG ĐƯỢC CHE PHỦ ÂM THANH-HÌNH ẢNH ĐỐI CHIẾU (CAV-MAE)

Như đã thảo luận trong Phần 2.1.3 và 2.2, học tập âm thanh-hình ảnh đối chiếu và A V-MAE mỗi cái đều có ưu điểm và nhược điểm riêng. Liệu chúng ta có thể tích hợp những ưu điểm bổ sung của CA V và A V-MAE không? Với mục tiêu này, chúng tôi thiết kế Bộ mã hóa tự động được che phủ âm thanh-hình ảnh đối chiếu (CA V-MAE) (được hiển thị trong Hình 1.C). Đối với một mini-batch của N mẫu cặp âm thanh-hình ảnh, chúng tôi đầu tiên xử lý trước và tokenize âm thanh và hình ảnh và nhận được một chuỗi token âm thanh và hình ảnh {ai;vi} cho mỗi mẫu i và chiếu chúng thành R^768 với hai lớp chiếu tuyến tính đặc trưng cho phương thức. Chúng tôi cũng thêm nhúng loại phương thức Ea và Ev và nhúng vị trí hình sin 2-D đặc trưng cho phương thức Ep_a và Ep_v. Sau đó, chúng tôi che phủ đồng đều 75% token của mỗi phương thức, tức là

a^unmask_i = Mask_0.75(Proja(ai) + Ea + Ep_a) (2)
v^unmask_i = Mask_0.75(Projv(vi) + Ev + Ep_v) (3)

Sau đó chúng tôi đưa a^unmask_i và v^unmask_i vào các bộ mã hóa Transformer âm thanh và hình ảnh độc lập Ea() và Ev() và nhận được a'_i và v'_i tương ứng. Sau đó, chúng tôi áp dụng các lượt truyền xuôi đa luồng để đưa a'_i, v'_i vào một bộ mã hóa âm thanh-hình ảnh kết hợp Ej(;MSA;LN1;LN2;MLP). Cụ thể, chúng tôi đưa token âm thanh a'_i, token video v'_i, và token âm thanh-hình ảnh được nối [a'_i;v'_i] trong ba lượt truyền xuôi độc lập đến Ej. Đối với mỗi luồng, chúng tôi sử dụng các lớp chuẩn hóa lớp khác nhau LN1_{a;v;av} và LN2_{a;v;av}, tất cả các trọng số khác (tức là trọng số của MSA và MLP) của Ej được chia sẻ cho tất cả ba luồng. Chính thức,

ca_i = MeanPool(Ej(Ea(a^unmask_i)); LN1_a;LN2_a)) (4)
cv_i = MeanPool(Ej(Ev(v^unmask_i)); LN1_v;LN2_v)) (5)
xi = Ej([Ea(a^unmask_i);Ev(v^unmask_i)]; LN1_av;LN2_av) (6)

Chúng tôi sử dụng đầu ra của luồng đơn phương thức âm thanh và hình ảnh ca_i và cv_i cho học tập đối chiếu và đầu ra của luồng đa phương thức âm thanh-hình ảnh xi cho nhiệm vụ tái tạo.

Đối với học tập âm thanh-hình ảnh đối chiếu, chúng tôi sử dụng mất mát đối chiếu Lc:

Lc = (1/N) * Σ(i=1 to N) log[exp(si,i/τ) / (Σ(k≠i) exp(si,k/τ) + exp(si,i/τ))] (7)

trong đó si,j = ||cv_i||^T * ||ca_j|| và τ là nhiệt độ.

Đối với nhiệm vụ tái tạo, chúng tôi đệm xi với các token được che phủ có thể huấn luyện tại vị trí gốc của chúng thành x'_i. Chúng tôi cũng thêm nhúng loại phương thức E'_a và E'_v và nhúng vị trí hình sin 2-D đặc trưng cho phương thức Ep_a' và Ep_v' trước khi đưa x'_i vào bộ giải mã âm thanh-hình ảnh kết hợp Dj() để tái tạo đầu vào âm thanh và hình ảnh. Dj() xử lý token âm thanh và hình ảnh với cùng một tập trọng số ngoại trừ lớp chiếu đặc trưng cho phương thức cuối cùng, nó xuất ra âi và v̂i. Sau đó chúng tôi áp dụng mất mát tái tạo lỗi bình phương trung bình Lr:

âi;v̂i = Dj(x'_i + [E'_a;E'_v] + [Ep_a';Ep_v']) (8)

Lr = (1/N) * Σ(i=1 to N) [Σ(â^mask_i - norm(a^mask_i))^2 / |a^mask_i| + Σ(v̂^mask_i - norm(v^mask_i))^2 / |v^mask_i|] (9)

trong đó N là kích thước mini-batch; a^mask, v^mask, â^mask, v̂^mask ký hiệu các miếng gốc và được dự đoán bị che phủ (chúng tôi chỉ tính toán mất mát dựa trên phần bị che phủ của đầu vào); |a^mask_i| và |v^mask_i| ký hiệu số lượng miếng âm thanh và hình ảnh bị che phủ tương ứng.

Cuối cùng, chúng tôi tổng hợp mất mát đối chiếu Lc (nhân với trọng số λc) và mất mát tái tạo Lr làm mất mát cho CA V-MAE, tức là L_CAVMAE = Lr + λc*Lc.

Sau khi huấn luyện trước, chúng tôi bỏ bộ giải mã và chỉ giữ lại các bộ mã hóa của mô hình cho các nhiệm vụ xuôi dòng. Chúng tôi có thể sử dụng tổng của đầu ra luồng đơn phương thức và đầu ra luồng đa phương thức, hoặc chỉ đầu ra luồng đa phương thức để tinh chỉnh. Chúng hoạt động tương tự trong các thí nghiệm của chúng tôi.

Thảo luận: chúng tôi tiếp theo thảo luận động lực của một số thiết kế chính của CA V-MAE:

1. Các lượt truyền xuôi đa luồng của bộ mã hóa kết hợp. Chúng tôi thấy việc hạn chế các biểu diễn được sử dụng cho học tập âm thanh-hình ảnh đối chiếu là quan trọng, để ca chỉ đến từ đầu vào âm thanh và cv chỉ đến từ đầu vào hình ảnh, nếu không mục tiêu đối chiếu sẽ sụp đổ. Đồng thời, chúng tôi hy vọng bộ mã hóa hợp nhất thông tin âm thanh và hình ảnh cho nhiệm vụ tái tạo và các nhiệm vụ xuôi dòng. Do đó, chúng tôi thiết kế chiến lược truyền xuôi đa luồng cho CA V-MAE.

2. Các bộ mã hóa và lớp LN đặc trưng cho phương thức. Trong khi có một số nỗ lực gần đây (Akbari et al., 2021; Dai et al., 2022) để xử lý các phương thức âm thanh và hình ảnh với mạng thống nhất, do bản chất rất khác nhau của các phương thức âm thanh và hình ảnh, kết luận chung là các mạng đặc trưng cho phương thức vẫn tối ưu về mặt hiệu suất. Do đó, chúng tôi chọn mã hóa đầu vào âm thanh và hình ảnh với các bộ mã hóa đặc trưng cho phương thức trước bộ mã hóa kết hợp. Vì cùng lý do, chúng tôi cũng sử dụng các thống kê chuẩn hóa khác nhau cho mỗi luồng của bộ mã hóa kết hợp.

Về hiệu quả, việc có hai bộ mã hóa đặc trưng cho phương thức tăng kích thước mô hình, nhưng giảm tính toán vì Transformer có độ phức tạp bậc hai w.r.t. độ dài chuỗi đầu vào.

3. Học tập âm thanh-hình ảnh đối chiếu được che phủ. Không giống như học tập đối chiếu đơn phương thức, học tập âm thanh-hình ảnh đối chiếu thông thường thường không áp dụng tăng cường hoặc che phủ. Trong công trình này, chúng tôi đề xuất sử dụng học tập âm thanh-hình ảnh đối chiếu được che phủ, tức là chúng tôi che phủ ngẫu nhiên một phần đầu vào trước khi tiến hành học tập đối chiếu. Thiết kế này không chỉ cho phép chúng tôi kết hợp CA V với A V-MAE, mà còn giúp tránh quá khớp. Trong thực tế, khi tỷ lệ che phủ là 75% và kích thước batch đối chiếu hiệu quả là 27 (108 trên 4 GPU), độ chính xác khớp âm thanh-hình ảnh trong quá trình huấn luyện trước trên tập đánh giá khoảng 72%, cho thấy nhiệm vụ không quá tầm thường cũng không quá khó khăn. Chúng tôi thảo luận tác động của che phủ đến học tập đối chiếu chi tiết trong Phụ lục F.

2.3.1 CHI TIẾT THỰC HIỆN

Theo mặc định, tất cả các lớp Transformer bộ mã hóa đều có 768 chiều và có 12 đầu attention. Bộ mã hóa kết hợp của A V-MAE Thông thường là một Transformer 12 lớp;

Các bộ mã hóa âm thanh và hình ảnh của CA V-MAE là các Transformer 11 lớp (mỗi cái là 768 chiều) và bộ mã hóa kết hợp là một Transformer đơn lớp. Tức là, chúng tôi kiểm soát tổng số lớp bộ mã hóa của tất cả các mô hình là 12, nhưng CA V và CA V-MAE là các mô hình lớn hơn do các bộ mã hóa đặc trưng cho phương thức. Bộ giải mã của A V-MAE và CA V-MAE là các Transformer 8 lớp với chiều nhúng là 512 và 16 đầu attention. Những cài đặt này giống hệt với MAE hình ảnh gốc He et al. (2022). Chúng tôi cố định nhiệt độ mất mát đối chiếu τ = 0.05. Đối với CA V-MAE, chúng tôi sử dụng λc = 0.01.

Lưu ý λc tương đối nhỏ là do quy mô gradient của Lc lớn hơn Lr, nó không có nghĩa là mục tiêu đối chiếu không quan trọng. Bộ mã hóa và bộ giải mã của mô hình CA V-MAE mặc định có khoảng 164M và 27M tham số tương ứng.

--- TRANG 3 ---

Theo thực hành thông thường của học tập âm thanh-hình ảnh, chúng tôi khởi tạo trọng số của tất cả các mô hình với trọng số được huấn luyện trước ImageNet. Cụ thể, chúng tôi sử dụng trọng số của MAE hình ảnh gốc He et al. (2022). Tuy nhiên, không giống như công trình trước đây sử dụng trọng số được huấn luyện trước có giám sát (ví dụ, Fayek & Kumar (2021) và Nagrani et al. (2021)), chúng tôi chỉ sử dụng trọng số được huấn luyện trước tự giám sát (tức là không tinh chỉnh), điều này không dẫn đến hiệu suất tốt nhất nhưng làm cho toàn bộ pipeline huấn luyện của chúng tôi tự giám sát. Tác động của chiến lược khởi tạo được thảo luận chi tiết trong Phụ lục E.

3 HUẤN LUYỆN TRƯỚC MÔ HÌNH TỰ GIÁM SÁT

Chúng tôi huấn luyện trước và so sánh hiệu suất của các mô hình sau:

1. Audio-MAE/Visual-MAE: Các mô hình bộ mã hóa tự động được che phủ đơn phương thức. Kiến trúc mô hình giống với A V-MAE Thông thường nhưng chúng chỉ được huấn luyện trước với dữ liệu của một phương thức duy nhất.

2. CA V: Mô hình học tập âm thanh-hình ảnh đối chiếu không có mục tiêu tái tạo. Để so sánh công bằng, chúng tôi thực hiện CA V sử dụng cùng kiến trúc bộ mã hóa (bộ mã hóa đặc trưng cho phương thức + bộ mã hóa kết hợp) với CA V-MAE nhưng loại bỏ mục tiêu tái tạo Lr.

3. A V-MAE Thông thường: Bộ mã hóa tự động được che phủ âm thanh-hình ảnh thông thường với bộ mã hóa kết hợp và không có mục tiêu đối chiếu như được mô tả trong Phần 2.2.

4. A V-MAE: Bộ mã hóa tự động được che phủ âm thanh-hình ảnh với hai bộ mã hóa đặc trưng cho phương thức và một bộ mã hóa kết hợp. Nó có cùng kiến trúc với CA V-MAE, nhưng λc được đặt thành 0 (không có mất mát đối chiếu). Chúng tôi sử dụng mô hình này để tách biệt tác động của các bộ mã hóa đặc trưng cho phương thức (khi so sánh với A V-MAE Thông thường) và mục tiêu đối chiếu (khi so sánh với CA V-MAE).

5. CA V-MAE: Bộ mã hóa tự động được che phủ đối chiếu được đề xuất của chúng tôi như được mô tả trong Phần 2.3.

6. CA V-MAE_scale+: Cùng mô hình với CA V-MAE, nhưng được huấn luyện với kích thước batch lớn hơn = 108 (kích thước batch đối chiếu hiệu quả = 27) và nhiều epoch hơn = 25. Chúng tôi huấn luyện mô hình này trên GPU tốt nhất của chúng tôi.

Để so sánh công bằng, tất cả các mô hình (ngoại trừ CA V-MAE_scale+) được huấn luyện trước với cùng pipeline với kích thước batch là 48 trong 12 epoch trên AudioSet-2M đầy đủ. Trong quá trình huấn luyện trước, chúng tôi cố ý không sử dụng lấy mẫu cân bằng lớp vì điều đó ngầm tận dụng thông tin nhãn. Quá trình huấn luyện trước của chúng tôi (bao gồm khởi tạo trọng số được huấn luyện trước ImageNet) hoàn toàn tự giám sát. Vui lòng tham khảo Phụ lục B cho tất cả chi tiết huấn luyện trước.

4 PHÂN LOẠI SỰ KIỆN ÂM THANH-HÌNH ẢNH

Chúng tôi đánh giá chất lượng biểu diễn trên nhiệm vụ phân loại sự kiện âm thanh-hình ảnh, một điểm chuẩn học tập âm thanh-hình ảnh chính. Cụ thể, chúng tôi tinh chỉnh các mô hình được huấn luyện trước trên ba tập dữ liệu: 1) AudioSet-20K (20K mẫu, cùng miền với dữ liệu huấn luyện trước); 2) AudioSet-2M (2 triệu mẫu, giống với dữ liệu huấn luyện trước); và 3) VGGSound (200K mẫu, miền khác với dữ liệu huấn luyện trước), bao gồm các tình huống khối lượng dữ liệu xuôi dòng và miền khác nhau.

Trong giai đoạn tinh chỉnh, chúng tôi chỉ giữ lại bộ mã hóa của các mô hình được huấn luyện trước và kết nối nó với một đầu phân loại tuyến tính được khởi tạo ngẫu nhiên. Để tránh ghi đè quá nhiều kiến thức đã học trong huấn luyện trước, chúng tôi sử dụng tốc độ học nhỏ hơn cho trọng số được huấn luyện trước và tốc độ học lớn hơn 10-100 lần cho đầu phân loại mới. Chúng tôi sử dụng pipeline huấn luyện tiêu chuẩn được sử dụng trong công trình phân loại sự kiện dựa trên âm thanh và âm thanh-hình ảnh trước đây Gong et al. (2021a;b); Nagrani et al. (2021) với mixup Zhang et al. (2018), lấy mẫu cân bằng, làm mịn nhãn, tăng cường nhãn (chỉ cho AudioSet-20K) và dịch chuyển thời gian ngẫu nhiên. Chúng tôi tinh chỉnh mô hình sử dụng dữ liệu chỉ âm thanh (A), dữ liệu chỉ video (V), và dữ liệu âm thanh-hình ảnh (A V) để đánh giá chất lượng biểu diễn đơn phương thức và đa phương thức. Chúng tôi hiển thị kết quả trong Bảng 1. Các phát hiện chính như sau:

1. Học tập đối chiếu và mô hình hóa dữ liệu được che phủ là bổ sung cho nhau. Trong khi cả A V-MAE (chỉ với mục tiêu mô hình hóa dữ liệu được che phủ) và CA V (chỉ với mục tiêu đối chiếu) đều hoạt động tốt hơn việc kết hợp hai MAE đơn phương thức, CA V-MAE được đề xuất kết hợp hai mục tiêu đã tăng cường đáng kể hiệu suất (ví dụ, tăng 2.0 và 3.1 mAP từ CA V và A V-MAE trên AudioSet-20K tương ứng). Lưu ý CA V-MAE, A V-MAE, và CA V có cùng kiến trúc trong quá trình tinh chỉnh, chỉ khác biệt duy nhất là mục tiêu trong giai đoạn huấn luyện trước. Điều này chứng minh rằng hai khung công tác học tập tự giám sát chính bổ sung cho nhau trong bối cảnh học tập âm thanh-hình ảnh và CA V-MAE là một cách hiệu quả để kết hợp ưu điểm của chúng.

--- TRANG 4 ---

[Tiếp tục với bản dịch phần còn lại của tài liệu...]
