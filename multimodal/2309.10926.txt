# 2309.10926.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.10926.pdf
# File size: 392793 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SEMI-AUTOREGRESSIVE STREAMING ASR WITH LABEL CONTEXT
Siddhant Arora1, George Saon2, Shinji Watanabe1, Brian Kingsbury2
1Carnegie Mellon University,2IBM Research, Yorktown Heights, USA
ABSTRACT
Non-autoregressive (NAR) modeling has gained significant interest
in speech processing since these models achieve dramatically lower
inference time than autoregressive (AR) models while also achieving
good transcription accuracy. Since NAR automatic speech recogni-
tion (ASR) models must wait for the completion of the entire utter-
ance before processing, some works explore streaming NAR models
based on blockwise attention for low-latency applications. However,
streaming NAR models significantly lag in accuracy compared to
streaming AR and non-streaming NAR models. To address this, we
propose a streaming “semi-autoregressive” ASR model that incor-
porates the labels emitted in previous blocks as additional context
using a Language Model (LM) subnetwork. We also introduce a
novel greedy decoding algorithm that addresses insertion and dele-
tion errors near block boundaries while not significantly increasing
the inference time. Experiments show that our method outperforms
the existing streaming NAR model by 19% relative on Tedlium2,
16%/8% on Librispeech-100 clean /other test sets, and 19% /8% on
the Switchboard(SWB) /Callhome(CH) test sets. It also reduced the
accuracy gap with streaming AR and non-streaming NAR models
while achieving 2.5x lower latency. We also demonstrate that our
approach can e ffectively utilize external text data to pre-train the
LM subnetwork to further improve streaming ASR accuracy.
Index Terms —ASR, Streaming, CTC, Semi-Autoregressive
1. INTRODUCTION
End-to-end Speech Recognition (E2E ASR) systems [1–6] have
been widely studied thanks to their many applications like voice as-
sistants and intelligent home devices. Most E2E architectures, like
Recurrent Neural Network Transducer (RNN-T) [5, 7, 8] and Atten-
tion Encoder Decoder (AED) [6, 9–12] systems, are Autoregressive
(AR) models that condition on previous labels to make their predic-
tions. While these AR models have achieved strong performance,
their inference time increases with output length, thereby a ffecting
user experience when they are deployed in interactive applications.
To address this, prior works have introduced Non-Autoregressive
(NAR) models [4, 13–18] that assume conditional independence on
previously predicted labels and, hence, can output tokens concur-
rently. The Connectionist Temporal Classification (CTC) [4] loss
is an NAR methodology that has been shown to achieve promising
results while drastically reducing inference time.
However, these models must wait for the utterance to end be-
fore they can start processing and hence cannot be deployed in low-
latency applications. Further, these approaches are mostly based on
the Transformer [19–21] architecture where the memory and compu-
tation requirement grows quadratically with the length of the input,
making them impractical for handling very long utterances. Some at-
tempts have been made to create framewise streaming ASR models
using unidirectional encoders for RNN-T or CTC models; however,their performance is suboptimal [22] due to no access to the future
context. Hence, there has been interest in blockwise processing en-
coders [23–26]. Prior work has tried to capture global information
by introducing an additional context embedding vector [27] in each
block. This approach paves the way for streaming ASR, achieved
through blockwise synchronous inference [28, 29] of AED. Similar
efforts [30–33] have developed low-latency RNN-T ASR systems.
Inspired by the success of NAR models for o ffline ASR, there
has been an e ffort to build streaming NAR [34] models that com-
bines blockwise attention with CTC models and proposes a dynamic
overlapping strategy during greedy decoding to address insertion and
deletion errors at block boundaries. While streaming NAR mod-
els achieve dramatically lower latency, their accuracy is significantly
worse than streaming AR and non-streaming NAR models. Prior
work on machine translation has shown promising results with a
semi-autoregressive (SAR) [35, 36] model that retains the AR prop-
erty globally but relaxes the AR property within local blocks. Moti-
vated by this, we ask if we can encode labels predicted by previous
blocks as additional context to improve streaming NAR accuracy
without drastically increasing the inference time.
To this end, we propose a streaming SAR ASR model that per-
forms greedy NAR decoding within a block but keeps the AR prop-
erty across blocks by encoding the labels emitted at previous blocks
using a Language Model (LM) subnetwork, e ffectively adding an ad-
ditional “label” context vector in the contextual block encoder. This
LM subnetwork is pretrained using a causal Language Modelling ob-
jective on text-only data, and hence provides text injection function-
ality. During training, we employ teacher forcing and generate con-
text embeddings using forced alignments [37] obtained from CTC-
based ASR models. Our entire model is trained using frame-wise
cross-entropy loss, with the forced alignments serving as a proxy
for ground truth alignments. We additionally experiment with inter-
mediate CTC [18] and “random block” [38–41] regularisation. Fi-
nally, we apply a simple decoding strategy that combines the last
few non-blank frames from the previous block with the frames in
the current block during inference. We evaluated our approach on
3 publicly available datasets, Tedlium-2 [42], Librispeech-100 [43]
and Switchboard [44] . Our results show that our proposed stream-
ing SAR model reduces the accuracy gap with streaming AR and
non-streaming NAR models while achieving 2.5x lower latency.
The key contributions of our work are (1) we introduce a novel
streaming SAR ASR model that incorporates labels predicted from
previous blocks as additional context, (2) we propose a novel decod-
ing algorithm that improves over existing streaming NAR decoding
strategies, and (3) we show that our approach can pretrain the LM
subnetwork with external text to boost streaming ASR accuracy.
2. PROBLEM FORMULATION
In ASR, the input is a T-length sequence of speech features, X=
{xt|t=1, . . . , T}, and the goal is to predict the corresponding O-arXiv:2309.10926v2  [cs.CL]  20 Feb 2024

--- PAGE 2 ---
Encoder LayerArgmax
Ab-1Downsampled audios
<latexit sha1_base64="DzYDhPmLt/912TPlY2zk+gJpBwM=">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMdSLx4r2A9IQtlsN+nSzSbsboQS+jO8eFDEq7/Gm//GTZuDtj4YeLw3w8y8IOVMadv+tiobm1vbO9Xd2t7+weFR/fikr5JMEtojCU/kMMCKciZoTzPN6TCVFMcBp4Ngelf4gycqFUvEo56l1I9xJFjICNZGcr0OiyKJvBzVRvWG3bQXQOvEKUkDSnRH9S9vnJAspkITjpVyHTvVfo6lZoTTec3LFE0xmeKIuoYKHFPl54uT5+jCKGMUJtKU0Gih/p7IcazULA5MZ4z1RK16hfif52Y6vPVzJtJMU0GWi8KMI52g4n80ZpISzWeGYCKZuRWRCZaYaJNSEYKz+vI66beaznWz9XDVaHfKOKpwBudwCQ7cQBvuoQs9IJDAM7zCm6WtF+vd+li2Vqxy5hT+wPr8AeEwkFc=</latexit>(<latexit sha1_base64="DzYDhPmLt/912TPlY2zk+gJpBwM=">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMdSLx4r2A9IQtlsN+nSzSbsboQS+jO8eFDEq7/Gm//GTZuDtj4YeLw3w8y8IOVMadv+tiobm1vbO9Xd2t7+weFR/fikr5JMEtojCU/kMMCKciZoTzPN6TCVFMcBp4Ngelf4gycqFUvEo56l1I9xJFjICNZGcr0OiyKJvBzVRvWG3bQXQOvEKUkDSnRH9S9vnJAspkITjpVyHTvVfo6lZoTTec3LFE0xmeKIuoYKHFPl54uT5+jCKGMUJtKU0Gih/p7IcazULA5MZ4z1RK16hfif52Y6vPVzJtJMU0GWi8KMI52g4n80ZpISzWeGYCKZuRWRCZaYaJNSEYKz+vI66beaznWz9XDVaHfKOKpwBudwCQ7cQBvuoQs9IJDAM7zCm6WtF+vd+li2Vqxy5hT+wPr8AeEwkFc=</latexit>(Ub-1UbEncoder Layer
LMPrevious Acoustic Context cb-1acEncoder Layers (x Ne)
<latexit sha1_base64="DzYDhPmLt/912TPlY2zk+gJpBwM=">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMdSLx4r2A9IQtlsN+nSzSbsboQS+jO8eFDEq7/Gm//GTZuDtj4YeLw3w8y8IOVMadv+tiobm1vbO9Xd2t7+weFR/fikr5JMEtojCU/kMMCKciZoTzPN6TCVFMcBp4Ngelf4gycqFUvEo56l1I9xJFjICNZGcr0OiyKJvBzVRvWG3bQXQOvEKUkDSnRH9S9vnJAspkITjpVyHTvVfo6lZoTTec3LFE0xmeKIuoYKHFPl54uT5+jCKGMUJtKU0Gih/p7IcazULA5MZ4z1RK16hfif52Y6vPVzJtJMU0GWi8KMI52g4n80ZpISzWeGYCKZuRWRCZaYaJNSEYKz+vI66beaznWz9XDVaHfKOKpwBudwCQ7cQBvuoQs9IJDAM7zCm6WtF+vd+li2Vqxy5hT+wPr8AeEwkFc=</latexit>(Next Acoustic Context cbacAbout
<latexit sha1_base64="DzYDhPmLt/912TPlY2zk+gJpBwM=">AAAB8nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mKqMdSLx4r2A9IQtlsN+nSzSbsboQS+jO8eFDEq7/Gm//GTZuDtj4YeLw3w8y8IOVMadv+tiobm1vbO9Xd2t7+weFR/fikr5JMEtojCU/kMMCKciZoTzPN6TCVFMcBp4Ngelf4gycqFUvEo56l1I9xJFjICNZGcr0OiyKJvBzVRvWG3bQXQOvEKUkDSnRH9S9vnJAspkITjpVyHTvVfo6lZoTTec3LFE0xmeKIuoYKHFPl54uT5+jCKGMUJtKU0Gih/p7IcazULA5MZ4z1RK16hfif52Y6vPVzJtJMU0GWi8KMI52g4n80ZpISzWeGYCKZuRWRCZaYaJNSEYKz+vI66beaznWz9XDVaHfKOKpwBudwCQ7cQBvuoQs9IJDAM7zCm6WtF+vd+li2Vqxy5hT+wPr8AeEwkFc=</latexit>(hbNext Label Context
Encodes prior labels as context using LMLinear Layer
cb-1lmInitial Context Embedding vectorFig. 1 : Schematics of our semi-autoregressive streaming ASR model
length transcript Y={yo|o=1, . . . , O}. Through the maximum
a posteriori (MAP) decision theory, the ASR model estimates the
transcript by maximizing posterior probability P(Y|X).
In the streaming NAR model [34], the input is represented by a
sequence of Boverlapping blocks, U={Ub|b=1, . . . , B}. As-
suming a block size of Lblockand a hop size of Lhop, the bthblock is
Ub=x(Ib−1+1):(Ib−1+Lblock )where Ib−1=(b−1)∗Lhop. Let A={at|t=
1, . . . , T}be the frame-level alignment where atcorresponds to the
aligned transcript token for speech frame xt. Since the blocks over-
lap, streaming models [29] use the central Lhopframes from each
block for prediction, excluding the first Nlframes as past frames and
the last Nrframes as future frames for lookahead. We can approxi-
mate [4] posterior probability P(Y|X)≈max AP(A|X) and represent
its logarithm as a sum of log-posteriors from overlapping blocks:
log(P(Y|X))≈BX
b=1Ib−1+Nl+LhopX
t=Ib−1+Nl+1log(P(at|a1:t−1,X)). (1)
With the NAR [4] conditional independence (C.I.) assumption, aty
a1:t−1|X, and the block processing [23–27] C.I. assumptions, aty
U<b|Ub1andatyU>b|Ub, Eq. 1 simplifies to
log(P(Y|X))=BX
b=1Ib−1+Nl+LhopX
t=Ib−1+Nl+1log(P(at|Ub)). (2)
We propose to preserve the AR property globally. We relax the
NAR C.I. assumption and modify Eq. 2 by conditioning on labels
emitted in previous blocks, Ab−1=a1:(Ib−1+Nl), using LM subnetwork:
log(P(Y|X))=BX
b=1Ib−1+Nl+LhopX
t=Ib−1+Nl+1log(P(at|Ab−1|{z}
LM Sub-Net,Ub)). (3)
By relaxing the NAR C.I. assumption, our formulation better ap-
proximates the original posterior distribution in Eq. 1. This formula-
tion enhances the streaming NAR accuracy using the “label context”
1[27] can potentially encode U<busing the acoustic context embeddingembedding from the LM subnetwork while maintaining low latency
by being able to output tokens concurrently within a block.
3. METHOD
To achieve the formulation described in Eq. 3, we propose the
streaming SAR ASR model shown in Figure 1. For the bthblock,
the output of the previous blocks Ab−1are passed through an LM
subnetwork to produce the label context embedding clm
b−1:
clm
b−1=LM(Normalize( Ab−1)), (4)
where “Normalize” refers to removing repeated and blank tokens
from alignments to make it similar to the transcripts used for training
LMs such that Normalize( A)=Y.
The input speech features Ubfor each block bare passed to the
contextual block encoder (CBE) along with the context embeddings:
hb,cac
b=CBE( Ub,clm
b−1,cac
b−1) (5)
where cac
b−1is the previous acoustic context embedding and cac
bis the
next acoustic context embedding [27]. To train the model, we can-
not employ an alignment-free CTC loss since we need to ensure that
the frame-level output of our model aligns with the labels used for
computing label context embeddings Ab−1in Eq. 4. This is crucial
to prevent a train-test mismatch since we train using teacher forcing
of alignments, while during inference the model utilizes labels pro-
duced in previous blocks. Consequently, we train our entire model
utilizing frame-level cross-entropy loss, denoted as Loss CE:
Loss =Loss CE(Softmax(Out(∪B
b=1hb)),A) (6)
where Out(·) denotes a linear layer that maps encoder output hbto
the vocabulary size |V|followed by a softmax function.
3.1. Training and Inference
Algorithm 1 Alignment greedy decoding
1:Yout=[∅];
2:A0=[∅];
3:Yprev=[∅];
4:U=Contextual Audio blocks iterators;
5:forb=0 to B do
6: clm
b−1=LM(Normalize( Ab−1))
7: Aout
b=Argmax(Out(CBE( Ub,clm
b−1)))
8: Ab←{Ab−1,Aout
b}
9: Aout
b←{Yprev,Aout
b}
10: ifAout
bends in non-blank token and is not final then
11: Yprev=frames with ending token in Aout
b;
12: Aout
b=remove frames with ending token in Aout
b;
13: else
14: Yprev=[∅];
15: end if
16: Yb=Normalize( Aout
b)
17: Yout←{Yout,Yb}
18:end for
19:return Yout
We use CTC forced alignments [37] from a pre-trained NAR
ASR model as a proxy to obtain frame-level alignment Afor train-
ing. We use Aboth for computing label context embeddings (see

--- PAGE 3 ---
Tedlium-2 Librispeech-100 SWB
ID Model Encoder Type Decode Mode WER Latency Clean WER Other WER Latency SWB WER CH WER Latency
A1TransformerFull Path 12.5 1102 12.2 30.1 1170 ✗ ✗ ✗
A2 Non Streaming NAR Greedy 12.9 391 12.3 30.3 408 ✗ ✗ ✗
A3 (topline)ConformerFull Path 8.8 1806 8.8 22.8 1531 9.1 16.7 409
A4 Greedy 8.9 872 8.8 22.9 752 9.1 16.5 270
B1Streaming ARTransformer Full Path 11.5 599 10.7 26.8 540 ✗ ✗ ✗
B2 Conformer Full Path 10.5 776 8.9 26.1 444 9.3 16.8 411
C1
Streaming NARTransformerFull Path 14.9 327 12.5 32.7 305 ✗ ✗ ✗
C2 Greedy 18.2 46 16.6 35.1 78 ✗ ✗ ✗
C3 Overlap Greedy [34] 16.0 53 13.4 33.2 80 ✗ ✗ ✗
C4
ConformerFull Path 11.0 482 9.4 27.0 453 10.7 18.4 304
C5 Greedy 15.1 124 16.0 31.6 146 14.6 20.3 115
C6 Overlap Greedy 12.5 124 10.8 28.1 144 12.4 19.1 116
D1 Streaming NAR
ConformerFull Path 12.0 487 9.8 28.1 454 10.5 17.7 303
D2 trained w /alignment Greedy 16.4 121 17.0 33.2 146 14.9 20.2 114
D3 Overlap Greedy 13.6 123 12.1 29.8 145 12.5 18.8 115
E1
Streaming SARConformerOverlap Greedy 11.7 143 12.3 29.2 188 12.4 18.8 133
E2 Alignment Greedy 10.5 140 9.6 26.9 188 10.4 17.7 137
E3 +InterCTC Alignment Greedy 10.2 144 9.6 26.6 193 10.3 17.6 136
E4 +Random block Alignment Greedy 10.1 149 9.1 26.1 188 10.1 17.5 136
Table 1 : Results presenting ASR accuracy and latency (in msec.) of our semi-autoregressive (SAR) ASR system on Tedlium-2, Librispeech-
100 and Switchboard dataset. Due to time constraints, we opted to report the performance of only conformer-based models for Switchboard,
given their superior performance over transformer-based models on Librispeech-100 and Tedlium-2 datasets.
Eq. 4) and as ground truth for Cross Entropy Loss (see Eq. 6). Fur-
ther, we pre-train the LM subnetwork using a causal LM objective
on training transcripts Y. This formulation further allows us to in-
corporate external text data to pre-train the LM subnetwork.
During inference, we use the frame level output from the previ-
ous blocks as Ab−1to produce label context embeddings using Eq. 4.
Prior works [34] have shown that the straightforward approach of
splitting input audio into smaller blocks and performing greedy de-
coding on each block results in significant accuracy degradation.
We similarly observed that most ASR errors occur when the seg-
ment boundary appears in the middle of a token, leading to repetitive
recognition in 2 consecutive blocks. Based on this observation, we
came up with a simple decoding strategy outlined in Algorithm 1,
which we refer to as “alignment greedy decoding”. Let the frame-
level output from each block be denoted by Aout
b, then the frame-
level alignment Ab−1is first updated to Abby appending it with Aout
b.
IfAout
bends with non-blank frames, this decoding removes the last
non-blank frames Yprevfrom the current block Aout
band instead adds
Yprevto the next block Aout
b+1.
4. EXPERIMENTS
4.1. Datasets
To demonstrate the e ffectiveness of our streaming SAR model
(see section 3), we conducted experiments on 3 publicly avail-
able English ASR datasets: Tedlium-2 [42], Librispeech-100 [43]
and Switchboard (SWB) [44] . We quantify ASR accuracy us-
ing Word Error Rate (WER). For non-streaming models, latency
is straightforwardly represented by the average decoding time
per utterance. For streaming models, we calculate latency as
Latency =P
utt(last token emitted −duration of speech)
#total number of utterance. Here, “last token emit-
ted” denotes the moment when the model emits the final token,
while “duration of speech” refers to the entire duration of the input
audio. When computing the timestamp at which the model outputs
the last token, we take into account both the lookahead time, which
encompasses the time required before processing the next block can
start, and the actual processing time.4.2. Baseline
We compare our proposed streaming SAR model with streaming
NAR and AR models. In addition, we include a non-streaming NAR
model as a topline, demonstrating how our approach can e ffectively
narrow the accuracy gap while maintaining significantly lower la-
tency. Furthermore, we present the results of a streaming NAR
model trained with frame-level cross-entropy loss (Eq. 6) using
alignments to gain insights into the impact of alignment-based train-
ing. We also incorporate intermediate CTC [45] and “random block”
regularization [38] techniques into our streaming SAR system.
4.3. Experimental Setups
For our NAR models, we employ a 12-layer transformer with a hid-
den dimension of 256 for Librispeech-100 and Tedlium2 datasets.
Our conformer NAR model comprises 12 layers with a hidden di-
mension of 256 for Tedlium2 and SWB, while for Librispeech-100,
it consists of 18 layers with a hidden dimension of 256. The stream-
ing AR models are constructed with a contextual block encoder us-
ingLblock=40,Lhop=16,Nl=8,Nr=16 and share the same
hyperparameters as the NAR models. The decoder for the streaming
AR models is a 6 layer transformer. Streaming NAR baselines con-
sist of the same hyperparameters as the streaming AR encoders. Our
streaming SAR model shares the same blockwise encoder architec-
ture as the streaming NAR model and incorporates an LM subnet-
work. The LM subnetwork consists of an LSTM with 2 layers and a
hidden dimension of 256 for Tedlium2, and 1 layer with a hidden di-
mension of 1024 for Librispeech-100 and SWB. We experiment with
the incorporation of intermediate CTC [45] after the 6th layer with
weight of 0.3. We also explore the e ffectiveness of “random block”
regularization [38], where we randomly select block sizes from the
range of [35, 45] during training, instead of using a fixed block size.
We use bpe size of 500 for all datasets.
The streaming AR models are trained using joint CTC-attention
training [10], while the streaming and non-streaming NAR mod-
els are trained using the CTC [4] loss. We train a streaming NAR
model and our proposed streaming SAR models with cross-entropy

--- PAGE 4 ---
Model WER
Streaming NAR trained w /alignment
streaming NAR alignment 13.6
non streaming NAR alignment 14.6
Streaming SAR
LM subnetwork 11.7
w/o finetuning 12.0
w/o LM pre-training 12.4
Cross Attention 13.6
Table 2 : Results presenting ablation on di fferent ways of using
alignment to incorporate label context. Results are shown for con-
former based models with overlap decoding on Tedlium 2 dataset.
loss (Eq. 6) using frame-level alignments. As ground truth align-
ments are unavailable, we utilize forced alignments from the stream-
ing conformer NAR model as a proxy. We train the LM subnetwork
on training transcripts. We also experiment with training on external
text data2, including the transcripts of the entire Librispeech for the
Librispeech-100 and the Fisher transcripts for the SWB SAR model.
We use both greedy and full-path decoding for the non-streaming
NAR model. We run full-path decoding for the streaming AR and
streaming NAR models via blockwise synchronous inference [28,
29]. Additionally, we employ greedy decoding along with dynamic
mapping for overlapping inference [34] (referred to as “overlap
decoding”) for our streaming NAR models. We perform inference
for streaming SAR model using both “overlap decoding” and our
proposed “alignment decoding” (see Algorithm 1). The inference of
our models are performed using 4 parallel CPU (AMD EPYC 7763
@ 2.55 GHz) jobs with 64 GB memory. All model, training and
inference parameters are selected based on validation accuracy.3
4.4. Results and Discussion
Table 1 presents the performance of our streaming SAR models
alongside topline and baseline models. We observe that the stream-
ing NAR model with greedy ( C5) and overlap greedy ( C6) decod-
ing [34] achieves impressively low latency but significantly degrades
accuracy compared to streaming AR ( B2) and non-streaming NAR
(A4) models. Our experiments reveal that training with alignments
using cross-entropy loss mostly yield inferior results compared to
training with CTC loss ( D3vsC6), possibly due to imperfections
in frame-level alignments. Notably, our streaming SAR model im-
proves accuracy ( E1vsD3) by incorporating the LM subnetwork
to encode the label context. Additionally, our proposed “align-
ment decoding” proves to be a valuable enhancement, delivering
a significant boost in accuracy ( E2vsE1) compared to the “over-
lap decoding” proposed in prior work [34]. Further improvements
are achieved by introducing intermediate CTC ( E3) and employing
“random block” ( E4) regularization. Our best-performing streaming
SAR model ( E4) surpasses the streaming NAR model with overlap
decoding ( C6) by a relative margin of 19% on Tedlium-2, 16% /8%
on the clean /other test sets of Librispeech-100, 19% /8% on the
Switchboard(SWB) /Callhome(CH) test sets of SWB with only a
slight increase in latency. We further conduct significance tests and
observe a p-value of less than 0.003 using Matched Pair, Signed
Paired, Wilcoxon and McNemar tests for all test datasets. Moreover,
our streaming SAR model outperforms the streaming NAR model
2http://www.openslr.org/resources/11/
librispeech-lm-norm.txt.gz
3Full details regarding models, configuration files, and data preparation
setup will be made publicly available as part of the ESPnet [3] toolkit.Tedlium-2 Librispeech-100
LSTM-LM Perplexity WER Perplexity Clean WER Other WER
2 layer, Dim 256 28.9 10.5 29.9 10.2 27.6
1 layer, Dim 1024 21.5 10.7 26.1 10.0 27.4
w/external data - - 16.4 9.6 26.9
Table 3 : Results presenting ablation on the impact of causal LM pre-
training. Results are shown for the conformer based streaming SAR
model with alignment decoding on Tedlium 2 and Librispeech-100
dataset along with perplexity of pre-trained LMs.
with full path decoding ( E4vsC4) by a relative 8%, 4%, 5% on
Tedlium-2, Librispeech-100 and SWB, respectively, while achieving
a 2.5x reduction in latency. Impressively, our proposed streaming
SAR model e ffectively bridges the accuracy gap with non-streaming
NAR models ( E4,C6 vsA4) and can even match or surpass stream-
ing AR models ( E4vsB2) on Tedlium2 and Librispeech-100 all
while delivering a 5x and 2.5x faster processing speed respectively.
Ablation study on incorporating label context : Table 2 shows
our findings for conformer based models with “overlap decoding”
inference on Tedlium2 dataset. First, we experiment with using
“forced alignment” from a non-streaming NAR model and observe
that forced alignments from a streaming NAR model leads to better
accuracy. In the case of the LM subnetwork, we explore not fine-
tuning the LM during training of the streaming SAR model, which
proves that proposed finetuning is e ffective. We also experiment with
not pre-training the subnetwork on a causal LM objective, and again
observe that our formulation of initialising the LM subnetwork is
helpful. Additionally, we conduct an ablation study where, instead of
using an LM subnetwork, we experiment with incorporating previ-
ously predicted labels using the multi-sequence cross-attention [46,
47] in the decoder and observe that our proposed formulation of hav-
ing a LM subnetwork achieves better accuracy.
Ablation study on impact of causal LM pre-training : Table 3
shows the perplexity of di fferent pre-trained language models (LMs)
and their impact on streaming SAR accuracy. Our observations re-
veal that, for Tedlium2, there is no significant accuracy di fference
associated with a stronger LM. However, in the case of Librispeech-
100, using a slightly superior LM results in an accuracy boost. No-
tably, the use of external text data further enhances accuracy, under-
scoring the e ffectiveness of our approach.
5. CONCLUSION
We introduce a novel streaming SAR model that performs NAR de-
coding within a block while maintaining the AR property across
blocks by encoding labels emitted in previous blocks using an LM
subnetwork. Our approach includes a simple alignment decoding al-
gorithm that helps to mitigate recognition errors due to block bound-
aries. We demonstrate the e ffectiveness of pre-training the LM sub-
network with external text data. We also incorporate intermediate
CTC and “random block” regularization. Our experiments reveal su-
perior accuracy compared to streaming NAR models and also com-
petitive accuracy with streaming AR models while achieving a 2.5x
reduction in latency. Future work will explore the use of external
text data via advanced LM [48] or CTC-based text injection [49].
6. ACKNOWLEDGEMENTS
This work used NCSA Delta through allocation CIS210014 from the
Advanced Cyberinfrastructure Coordination Ecosystem: Services &
Support (ACCESS) program, which is supported by NSF grants
#2138259, #2138286, #2138307, #2137603, #2138296.

--- PAGE 5 ---
7. REFERENCES
[1] R. Prabhavalkar et al. , “End-to-end speech recognition: A survey,”
arXiv preprint arXiv:2303.03329 , 2023.
[2] J. Li et al. , “Recent advances in end-to-end automatic speech recog-
nition,” APSIPA Transactions on Signal and Information Processing ,
vol. 11, no. 1,
[3] S. Watanabe et al. , “ESPnet: End-to-end speech processing toolkit,”
inProc. Interspeech , 2018.
[4] A. Graves et al. , “Connectionist temporal classification: Labelling un-
segmented sequence data with recurrent neural networks,” in ICML
2006 , vol. 148, 2006, pp. 369–376.
[5] A. Graves, “Sequence transduction with recurrent neural networks,”
CoRR , vol. abs /1211.3711, 2012.
[6] J. Chorowski et al. , “Attention-based models for speech recognition,”
inProc. NeurIPS , 2015, pp. 577–585.
[7] A. Graves, A. Mohamed, and G. E. Hinton, “Speech recognition with
deep recurrent neural networks,” in Proc. ICASSP , 2013.
[8] G. Saon et al. , “Advancing RNN transducer technology for speech
recognition,” in Proc. ICASSP , 2021, pp. 5654–5658.
[9] W. Chan et al. , “Listen, attend and spell: A neural network for large
vocabulary conversational speech recognition,” in Proc. ICASSP ,
2016, pp. 4960–4964.
[10] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-to-
end speech recognition using multi-task learning,” in Proc. ICASSP ,
2017.
[11] T. Hori, S. Watanabe, and J. R. Hershey, “Joint CTC /attention decod-
ing for end-to-end speech recognition,” in Proc. ACL , 2017, pp. 518–
529.
[12] Z. T ¨uske, K. Audhkhasi, and G. Saon, “Advancing sequence-
to-sequence based speech recognition,” Proc. Interspeech 2019 ,
pp. 3780–3784, 2019.
[13] N. Chen et al. , “Listen and fill in the missing letters: Non-
autoregressive transformer for speech recognition,” CoRR ,
vol. abs /1911.04908, 2019.
[14] Z. Tian et al. , “Spike-triggered non-autoregressive transformer for
end-to-end speech recognition,” in Proc. Interspeech , 2020.
[15] Y . Higuchi et al. , “Mask CTC: non-autoregressive end-to-end ASR
with CTC and mask predict,” in Proc. Interspeech , 2020.
[16] Z. Gao et al. , “Paraformer: Fast and accurate parallel transformer
for non-autoregressive end-to-end speech recognition,” in Proc. In-
terspeech , 2022, pp. 2063–2067.
[17] J. Nozaki and T. Komatsu, “Relaxing the conditional independence
assumption of ctc-based ASR by conditioning on intermediate pre-
dictions,” in Proc. Interspeech , 2021, pp. 3735–3739.
[18] J. Lee and S. Watanabe, “Intermediate loss regularization for ctc-
based speech recognition,” CoRR , vol. abs /2102.03216, 2021.
[19] A. Vaswani et al. , “Attention is all you need,” Proc. NeurIPS , vol. 30,
pp. 5998–6008, 2017.
[20] A. Gulati et al. , “Conformer: Convolution-augmented transformer for
speech recognition,” in Proc. Interspeech , 2020, pp. 5036–5040.
[21] P. Guo et al. , “Recent developments on espnet toolkit boosted by con-
former,” in Proc. ICASSP , 2021, pp. 5874–5878.
[22] J. Li et al. , “On the comparison of popular end-to-end models for large
scale speech recognition,” in Proc. Interspeech , H. Meng, B. Xu, and
T. F. Zheng, Eds., 2020, pp. 1–5.
[23] N. Moritz, T. Hori, and J. L. Roux, “Streaming automatic speech
recognition with the transformer model,” in Proc. ICASSP , 2020,
pp. 6074–6078.
[24] D. Povey et al. , “A time-restricted self-attention layer for ASR,” in
Proc. ICASSP , 2018, pp. 5874–5878.[25] N. Jaitly et al. , “An online sequence-to-sequence model using partial
conditioning,” in Proc. NeurIPS , 2016, pp. 5067–5075.
[26] L. Dong, F. Wang, and B. Xu, “Self-attention aligner: A latency-
control end-to-end model for ASR using self-attention network and
chunk-hopping,” in Proc. ICASSP , 2019, pp. 5656–5660.
[27] E. Tsunoo et al. , “Transformer ASR with contextual block process-
ing,” in Proc. ASRU , 2019, pp. 427–433.
[28] E. Tsunoo, Y . Kashiwagi, and S. Watanabe, “Streaming trans-
former ASR with blockwise synchronous inference,” CoRR ,
vol. abs /2006.14941, 2020.
[29] E. Tsunoo, Y . Kashiwagi, and S. Watanabe, “Streaming transformer
asr with blockwise synchronous beam search,” in Proc. SLT , 2021,
pp. 22–29.
[30] A. Tripathi et al. , “Transformer transducer: One model uni-
fying streaming and non-streaming speech recognition,” CoRR ,
vol. abs /2010.03192, 2020.
[31] M. Jain et al. , “RNN-T for latency controlled ASR with improved
beam search,” CoRR , vol. abs /1911.01629, 2019.
[32] E. Battenberg et al. , “Exploring neural transducers for end-to-end
speech recognition,” in Proc. ASRU , 2017, pp. 206–213.
[33] W. Wang, K. Hu, and T. N. Sainath, “Deliberation of streaming rnn-
transducer by non-autoregressive decoding,” in Proc. ICASSP , 2022,
pp. 7452–7456.
[34] T. Wang et al. , “Streaming end-to-end ASR based on blockwise non-
autoregressive models,” in Proc. Interspeech , 2021, pp. 3755–3759.
[35] C. Wang, J. Zhang, and H. Chen, “Semi-autoregressive neural ma-
chine translation,” in Proc. EMNLP , 2018, pp. 479–488.
[36] Y . Zhou et al. , “Semi-autoregressive transformer for image caption-
ing,” in ICCVW 2021 , 2021, pp. 3132–3136.
[37] L. K ¨urzinger et al. , “Ctc-segmentation of large corpora for german
end-to-end speech recognition,” in SPECOM , ser. Lecture Notes in
Computer Science, vol. 12335, 2020, pp. 267–278.
[38] K. Audhkhasi et al. , “Forget a bit to learn better: Soft forgetting for
ctc-based automatic speech recognition,” in Proc. Interspeech , 2019,
pp. 2618–2622.
[39] S. Horiguchi et al. , “Online neural diarization of unlimited numbers of
speakers using global and local attractors,” TASLP , vol. 31, pp. 706–
720, 2023.
[40] Z. Yao et al. , “Wenet: Production oriented streaming and non-
streaming end-to-end speech recognition toolkit,” in Proc. Inter-
speech , H. Hermansky et al. , Eds., 2021, pp. 4054–4058.
[41] Y . Sudo et al. , “Time-synchronous one-pass Beam Search for Paral-
lel Online and O ffline Transducers with Dynamic Block Training,” in
Proc. INTERSPEECH 2023 , 2023, pp. 4479–4483.
[42] A. Rousseau, P. Del ´eglise, and Y . Est `eve, “Enhancing the TED-LIUM
corpus with selected data for language modeling and more TED
talks,” in LREC , 2014, pp. 3935–3939.
[43] V . Panayotov et al. , “Librispeech: An ASR corpus based on public
domain audio books,” in Proc. ICASSP , 2015, pp. 5206–5210.
[44] J. J. Godfrey, E. Holliman, and J. McDaniel, “SWITCHBOARD: tele-
phone speech corpus for research and development,” in Proc. ICASSP ,
1992, pp. 517–520.
[45] J. Lee and S. Watanabe, “Intermediate loss regularization for CTC-
based speech recognition,” in Proc. ICASSP , 2021, pp. 6224–6228.
[46] S. Arora et al. , “Token-level sequence labeling for spoken language
understanding using compositional end-to-end models,” in Findings
of the EMNLP , 2022, pp. 5419–5429.
[47] J. Helcl, J. Libovick ´y, and D. Varis, “CUNI system for the WMT18
multimodal translation task,” in WMT , 2018, pp. 616–623.
[48] H. Touvron et al. , “Llama 2: Open foundation and fine-tuned chat
models,” CoRR , vol. abs /2307.09288, 2023.
[49] H. Sato et al. , “Text-only domain adaptation based on intermediate
CTC,” in Proc. Interspeech , 2022, pp. 2208–2212.
