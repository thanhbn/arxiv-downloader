# 2311.07919.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.07919.pdf
# File size: 1798895 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Qwen-Audio: Advancing Universal Audio Understanding
via Unified Large-Scale Audio-Language Models
Yunfei Chu∗Jin Xu∗Xiaohuan Zhou∗Qian Yang
Shiliang Zhang Zhijie Yan Chang Zhou†Jingren Zhou
Alibaba Group
Code & Demo & Models: https://github.com/QwenLM/Qwen-Audio
Abstract
Recently, instruction-following audio-language models have received broad attention
foraudiointeractionwithhumans. However,theabsenceofpre-trainedaudiomodels
capable of handling diverse audio types and tasks has hindered progress in this field.
Consequently,mostexistingworkshaveonlybeenabletosupportalimitedrangeofinter-
action capabilities. In this paper, we develop the Qwen-Audio model and address this
limitationbyscalingupaudio-languagepre-trainingtocoverover30tasksandvarious
audiotypes,suchashumanspeech,naturalsounds,music,andsongs,tofacilitateuni-
versal audio understanding abilities. However, directly co-training all tasks and datasets
can lead to interference issues, as the textual labels associated with different datasets
exhibit considerable variations due to differences in task focus, language, granularity of
annotation,andtextstructure. Toovercometheone-to-manyinterference,wecarefully
designamulti-tasktrainingframeworkbyconditioningonasequenceofhierarchicaltags
to the decoder for encouraging knowledge sharing and avoiding interference through
shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive per-
formance across diverse benchmark tasks without requiring any task-specific fine-tuning,
surpassingitscounterparts. BuildinguponthecapabilitiesofQwen-Audio,wefurther
developQwen-Audio-Chat,whichallowsforinputfromvariousaudiosandtextinputs,
enabling multi-turn dialogues and supporting various audio-central scenarios.
1 Introduction
Large language models (LLMs) (Brown et al., 2020; OpenAI, 2022, 2023; Chowdhery et al., 2022; Anil et al.,
2023; Touvron et al., 2023a,c; Qwen, 2023) have greatly propelled advancements in the field of general
artificialintelligence(AGI)duetotheirstrongknowledgeretention,complexreasoningandproblem-solving
capabilities. However,languagemodelslackthecapabilitytoperceivenon-textualmodalitieslikeimagesand
audioin the samemannerashumansdo. Speech, asanimportantmodality, providesdiverseand complex
signals beyond texts such as emotions, tones, and intentions in human voice, train whistle, clock chime and
thunderinnaturalsounds,andmelodyinmusic. EnablingLLMstoperceiveandcomprehendrichaudio
signals foraudio interactionhas receivedbroad attention(Huang et al.,2023; Shenet al.,2023; Wanget al.,
2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b; Wang et al., 2023c; Shu et al., 2023).
Priorworksforinstructionfollowingmainlyinheritthecapabilitiesfromlarge(multimodal)LLMsandadopt
light-weightsupervisedfine-tuningtoactivatetheabilitiesofthemodeltoalignwithuserintent(Ouyang
et al., 2022; Wang et al., 2023a; Gong et al., 2023b). However, most works have been constrained in terms
∗Equal contribution,†Corresponding author
1arXiv:2311.07919v2  [eess.AS]  21 Dec 2023

--- PAGE 2 ---
Librispeech
Aishell1
Aishell2
CoVoST2
Clotho
CochlScene
TUT2017MeldClothoAQAVocalSoundNS.QualitiesNS.Instrucment
91.593.094.5
97.4497.8898.31
91.7593.595.25
8.75 17.5 26.25
11.5
23.0
34.565.0
70.0
75.017.5
35.0
52.537.0
44.0
51.020.0
40.0
60.046.25 62.5 78.7512.525.037.5
20.040.060.0
N/A
Previous T op-tiers
Qwen-AudioFigure 1: Performance of Qwen-Audio and previous top-tiers from multi-task audio-text learning mod-
els such as SpeechT5 (Ao et al., 2021), SpeechNet (Chen et al., 2021), SpeechLLaMA (Wu et al., 2023a),
SALMONN (Anonymous, 2023) and Pengi (Deshmukh et al., 2023). We demonstrate the test set results
acrossthe12datasetscoveringAutomaticSpeechRecognition(ASR),Speech-to-TextTranslation(S2TT), Au-
tomatic Audio Captioning (AAC), Acoustic Scene Classification (ASC), Speech Emotion Recognition (SER),
AudioQuestionandAnswering(AQA),VocalSoundClassification(VSC),andMusicNoteAnalysis(MNA).
The results of ASR datasets, such as Librispeech, Aishell1 and Aishell2 refer to 1 - WER%. The results of
CoVoST2istheaverageBLEUscoreofseventranslationdirections(en-de,de-en,en-zh,zh-en,es-en,fr-en
andit-en). Qwen-Audioachievesremarkableperformancewithoutrequiringanytask-specificfine-tuning,
surpassing its counterparts.
of audio interaction capabilities due to the lack of pre-trained audio-language models that can handle
diverse audio types and tasks. Existing representative audio-language multi-tasks language models, such as
SpeechNet(Chenetal.,2021),SpeechT5(Aoetal.,2021),VIOLA(Wangetal.,2023d),Whisper(Radford
et al., 2023) and Pengi (Deshmukh et al., 2023) are limited to processing specific audio types, such as human
speech or natural sounds.
Topromotethegrowthanddevelopmentoftheaudio-textmultimodalcommunity,weintroduceQwen-Audio,
a large-scale audio-language model. Qwen-Audio is a multi-task language model conditioning on audio and
text inputs, that extends the Qwen-7B (Bai et al., 2023a) language model to effectively perceive audio signals
bytheconnectionofasingleaudioencoder. Differentfrompreviousworksthatprimarilycatertoasingle
typeofaudiosuchashumanspeech,orfocusonspecifictaskslikespeechrecognitionandcaptioning,or
limitmodelstoasinglelanguage(Wangetal.,2023a;Lyuetal.,2023;Wuetal.,2023b;Gongetal.,2023b;Shu
etal.,2023),wescaleupthetrainingtodozensofdatasetscoveringover30tasks,eightlanguagesandvarious
typesofaudioforadvancinguniversalaudiounderstandingabilities. Asignificantchallengeofmulti-task
2

--- PAGE 3 ---
(a) Multilingual ASR and Translation
(f) Speech Editing with SWRT(d) Audio-Motivated Creative Writing(e) Music Appreciation(c) Sound Understanding and Reasoning(b) Multiple Audio Analysis
OK. The sentence “⼤家好我们来讨论⼀下公司周末团建的地点吧⼤家有什么意⻅呢” translated into English is “Hello, let’s discuss the location for our company’s weekend team building. What are your opinions?”
recognize the speech, and translate it into English
recognize the speech, and translate it into EnglishOK. The sentence “I think we can go to the countryside and find a beautiful natural area.” translated into English is “I think we can go to the countryside and find a beautiful natural area.
recognize the speech, and translate it into English
OK. The sentence “わかりました。⽥舎に⾏くことに同意します。⽥舎暮らしを体験してみるのもいいかもしれません。” translated into English is “I understand. I agree to go to the countryside. It’s also good to experience country life.”
What are the emotions in audio 1 and audio 2, respectively?The tone of the woman in audio 1 is cheerful, possibly talking about something pleasant. The tone of the woman in audio 2 is fearful, possibly discussing something terrifying.
How to respond to the questions in audio 1 and audio 2?In audio 1, when the woman asks if you're okay, you can respond with, "Thank you for your concern, I'm fine now, just felt a bit uncomfortable all of a sudden."In audio 2, considering the woman's fearful tone and discussing something terrifying, you can respond to her with either, "I'm scared, I don't know what to do" or "I'm feeling really upset, I don't know how to face this situation."
Recognize the sound information and assume that the sound is happening around the user. Please provide the user with some handling suggestions.The sound is of breaking glass. Here are some suggestions for handling the situation:1. Turn off the source of the sound: If the sound is coming from a broken glass item, turn off the source of the sound as soon as possible to prevent further damage.2. Clean up the broken glass: Use gloves and a broom or vacuum cleaner to clean up the broken glass. Be careful not to cut yourself.3. Assess the damage: If the broken glass is in a location that is not easily accessible or if it poses a safety hazard, contact a professional for assistance.4. Cover the area: Cover the area where the broken glass is located to prevent injury or further damage.5. Avoid walking on broken glass: If the broken glass is on a floor, do not walk on it. Instead, use a broom or vacuum cleaner to clean it up or cover it with a piece of cardboard or tape.  
what is itDescribe this music in detailThis is a solo piano piece. It is fast tempo with a playful and humorous mood. The piece is in the key of C major and is in 4/4 time signature. The melody is played on the right hand with the left hand playing chords. The piece is a classic ragtime style.
A solo piano piece.
Give me some recommendations for similar music.
some similar music to “Solo Piano Ragtime” could be:1. “The Entertainer” by Scott Joplin2. “Maple Leaf Rag” by Scott Joplin3. “Ave Maria” by Franz Schubert4. “Für Elise” by Ludwig van Beethoven5. “The Blue Danube” by Johann Strauss II
Write a poem about the above audio
 Birds are singing in the trees,  A gentle breeze is blowing free, The rustling leaves are music,  As nature’s symphony goes on.  The sun is shining bright and warm,    A peaceful atmosphere is sown,  In the midst of nature’s beauty,  We can find serenity and duty.   The birds take flight with ease,  Their chirping voices fill the trees,  A symphony of nature’s grace,  A moment to pause and embrace. The wind whispers through the branches,  As the leaves dance in the branches,  A peaceful moment in time,  A chance to let go and unwind.   So let us take a moment here,  To appreciate the beauty so dear,  In the midst of nature’s wonder,  We can find a sense of wonder.
change “joyful” to “interesting”
ok, here is the edited audio
Edit out the word "what", so I can turn it into a meme
The word “what” starts at 3.04 seconds and ends at 3.23 seconds.
change the person name to “John”ok, here is the edited audio
Figure2: ExamplesofQwen-Audioshowcasingitsproficiencyinperceivingandcomprehendingvarious
types of audio. Qwen-Audio supports multiple-audio analysis, sound understanding and reasoning, music
appreciation,andtoolusageforspeechediting. Demosareavailableat https://qwen-audio.github.io/
Qwen-Audio/ .
andmulti-datasetco-trainingarisesfromtheconsiderablevariationintextuallabelsassociatedwithdifferent
datasets. This variation stems from differences in task objectives, languages, annotation granularity, and text
structure (structured or unstructured). To address this one-to-many challenge, we have carefully designed a
multi-tasktrainingframeworkthatconditionsthedecoderonasequenceofhierarchicaltags. Thisdesign
encouragesknowledgesharingandhelpsmitigateinterferencethroughsharedandspecifiedtags,respectively.
Furthermore, we incorporate the speech recognition with the word-level time-stamp prediction (SRWT) task
for training, which is usually ignored in previous multi-task learning research. We find the task not only
improvesthegroundingandgrounding-basedQAtasksbeyondspeechsignalssuchassoundandmusic,
butalsoimprovestheperformanceofASR.AsshowninFigure1,extensiveevaluationdemonstratesthat
Qwen-Audio, without any task-specific fine-tuning, outperforms previous multi-task training models across
a diverse range of tasks. A notable achievement of Qwen-Audio is its state-of-the-art performance on the test
setofAishell1,cochlscene,ClothoAQA,andVocalSound. LeveragingthecapabilitiesofQwen-Audio,we
introduceQwen-Audio-Chatviasupervisedinstructionfine-tuning,whichfacilitatesflexibleinputfromboth
audioandtextmodalitiesinmulti-turndialogues,enablingeffectiveinteractionfollowinghumaninstructions.
The contribution of the paper is summarized below:
•We introduce Qwen-Audio, a fundamental multi-task audio-language model that supports various
3

--- PAGE 4 ---
tasks,languages, andaudio types,servingas auniversalaudiounderstanding model. Buildingupon
Qwen-Audio, we develop Qwen-Audio-Chat through instruction fine-tuning, enabling multi-turn
dialogues and supporting diverse audio-oriented scenarios. Both Qwen-Audio and Qwen-Audio-
Chatmodelsareopen-source,promotingthegrowthanddevelopmentoftheaudio-textmultimodal
community.
•To scale up audio-language pre-training, we address the challenge of variation in textual labels associ-
atedwithdifferentdatasetsbyproposingamulti-tasktrainingframework,enablingknowledgesharing
and avoiding one-to-many interference. Our model incorporates more than 30 tasks and extensive
experiments show the model achieves strong performance.
•Topromoteaudio-languagepre-training,wedemonstratethatincorporatingtheSRWTtask,whichis
oftenoverlookedin theaudio multimodalresearch community,improvesgrounding andgrounding-
based question answering tasks beyond speech signals, as well as ASR performance.
•ExperimentalresultsshowthatQwen-Audioachievesimpressiveperformanceacrossdiversebench-
marktaskswithoutrequiringanytask-specificfine-tuning,surpassingitscounterparts. Specifically,
Qwen-Audioachievesstate-of-the-artresultsonthetestsetofAishell1,cochlscene,ClothoAQA,and
VocalSound.
2 Related Work
Multi-task Audio-Text Learning The goal of multi-task training is to transfer knowledge between different
tasks with unified model architectures and data format (Raffel et al., 2020; Ao et al., 2021; Chen et al., 2021).
In audio processing domains, it is challenging to unify all audio processing tasks since there are various
audiosignals,such ashumanspeech,naturalsounds, music,andsongs,andtheir labelingformatdiffersa
lot. SpeechNet(Chenetal.,2021)andSpeechT5(Aoetal.,2021)treathumanspeechtasksintoaspeech/text
input and speech/text output format, and leverage a shared encoder-decoder framework for pretraining.
Many works (Wang et al., 2023d; Maiti et al., 2023; Rubenstein et al.; Wang et al., 2023e; Nachmani et al.,
2023) unify data format and tasks by directly feeding speech representation (Nachmani et al., 2023) or
encodingcontinuousspeechsignalsasdiscretecodes(Défossezetal.,2022;Zeghidouretal.,2022;Zhang
etal.,2023c),andtreatingdifferenthumanspeechtasksasconditionalgenerativetasks. Fortraining,they
directly adopt a decoder-only Transformer model (Vaswani et al., 2017). VoiceBox (Le et al.,2023) employs a
non-autoregressivecontinuousnormalizingflowmodelforhumanspeechsynthesisandspeecheditingtasks.
Whisper(Radfordetal.,2023)proposesatemplateformulti-tasktraining,consideringthegranularityof
dataset annotations (with or without sentence-level timestamps) and task types (human speech recognition
andtranslation)forunifiedtraining. Previousworksmostlyfocusonlyonhumanspeechprocessingtasks
suchasspeechrecognitionandtranslationandignoreothertypesofaudiosuchasnaturalsoundsandmusic.
Pengi(Deshmukhetal.,2023)focusesonnaturalsoundunderstandingtasksandtreatsthesetasksastext
generationtasks. Specifically,Pengiunifiesdataformatwithtexttemplatesandthentrainsalltaskswithin
a Transformer decoder model. In this work, Qwen-Audio integrates diverse audio types, such as human
speech,naturalsounds,music,andsongs,andfacilitatesco-trainingondatasetssourcedfromheterogeneous
data andfeatured disparatelabeling granularities. This isachieved throughintroducing aunified learning
framework. Upon completion of the co-training process, the model demonstrates comprehensive capabilities
in speech perception, comprehension, and recognition tasks, eliminating the need for additional task-specific
architectural extensions.
Interact with LLMs through Multiple Modality Recently, large language models such as ChatGPT (Ope-
nAI, 2022) have demonstrated impressive capabilities for knowledge retention, reasoning, and coding
followed by human instructions. To extend to application scope of LLMs beyond pure text tasks, many LLM-
basedmultimodalmodelshavebeendeveloped. Forvisualmodality,GPT4(OpenAI,2023),Flamingo(Alayrac
etal.,2022),Kosmos(Pengetal.,2023),BLIP(Lietal.,2022),Shikra(Chenetal.,2023),Emu(Sunetal.,2023)
4

--- PAGE 5 ---
and Qwen-VL (Bai et al., 2023b) have proposed different integration method to enable image understanding
or generation capabilities for LLMs.
Fortheaudiomodality,therehavebeenattemptstoutilizewell-trainedaudiofoundationmodelsastools,
suchasAudioGPT(Huangetal.,2023)andHuggingGPT(Shenetal.,2023),whileleveragingLLMsasa
versatile interface. These endeavors involve instructing LLMs to generate commands for controlling external
tools or transcribing human speech to text before inputting into the LLMs. However, these approaches lack
the inclusion of crucial information like prosody and sentiment in human speech, and in certain cases, they
failtoconvertnon-textualaudio,suchasnaturalsound. Consequently,thetransferofknowledgefromLLMs
tothespeechmodalityencountersobstacles,andtheLLMslackthenecessarycapabilitiestoperceiveand
comprehend audio signals. Recent efforts explore training end-to-end audio-text LLMs for direct speech
interaction. SpeechGPT (Zhang et al., 2023a) first converts human speech into discrete HuBERT tokens (Hsu
et al., 2021), and then designs a three-stage training pipeline on paired speech data, speech instruction
dataandchain-of-modalityinstructiondataaccordingly. BLSP(Wangetal.,2023a)alignsrepresentation
byrequiringtheLLMtogeneratethesametextcontinuationgiventhehumanspeechandcorresponding
transcripts. LLaSM(Shuetal.,2023)createslargespeechinstructiondatasetsbygeneratingspeechquestions
usingMicrosoftTTSAPI,andthenconductstrainingtoenableend-to-endinteractionbetweenhumanspeech
and text.
LTU (Gong et al., 2023b) creates a 5M audio QA dataset, and conducts supervised finetuning (SFT) on
the audio module and LoRA adapters (Hu et al., 2021) of LLaMA (Touvron et al., 2023b) to enhance the
alignmentbetweensoundperceptionandreasoning. SALMMON(Anonymous,2023)utilizesbothatext
encoder and a speech encoder to extract the representation from different kinds of audio and text input,
andthenconnectstheinputstoawell-train LLMwithQ-former(Lietal.,2023) styleattentiontogenerate
response. In this work, Qwen-Audio aims at training a unified audio-text multi-task multilingual LLMs
capable of perceiving and understanding audio inputs while preserving the textual conversational abilities.
Qwen-Audioemploysasingleencoderforallaudios,andbridgesthegapofaudioandtextmodalitybylarge-
scaleend-to-endtrainingtosupportvarioustaskssuchasnaturalsounddetection,humanspeechrecognition
and grounding, and audio captions tasks. The resulting model demonstrates superior performance than
previous works across a diverse style of tasks.
3 Methodology
This section provides details of Qwen-Audio and Qwen-Audio-Chat, which are designed for universal audio
understandingandflexibleinteractionbasedonhumaninstructions. ThemodelstructureofQwen-Audioand
Qwen-Audio-Chat is first presented in Section 3.1. The training process of our models consists of two stages:
multitaskpretrainingandsupervisedfine-tuning. WedescribethetrainingofQwen-Audioviamultitask
learning in Section 3.2. Then, we describe Qwen-Audio-Chat with supervised fine-tuning in Section 3.3 ,
which enables flexible human interaction.
3.1 Model Architecture
The architecture of Qwen-Audio models is depicted in Figure 3. Qwen-Audio contains an audio encoder and
alargelanguagemodel. Giventhepaireddata (a,x),wherethe aandxdenotetheaudiosequencesand
text sequences, the training objective is to maximize the next text token probability as
Pθ(xt|x<t,Encoder ϕ(a)), (1)
conditioning on audio representations and previous text sequences x<t, where θandϕdenote the trainable
parameters of the LLM and audio encoder respectively.
Audio Encoder Qwen-Audio employs a single audio encoder to process various types of audio. The
initializationoftheaudioencoderisbasedontheWhisper-large-v2model(Radfordetal.,2023),whichis
5

--- PAGE 6 ---
QwenLM
Next Token Prediction0.32
SOT…ZHTRANS-CRIBEZHTIMES-TAMPSWLTMy0.15catAudio EncoderInput Training Data                       Multi-language transcription“What work are you doing now?”<|0.00|>What<|0.23|>work<|0.43|>are<|0.45|><|SIL|><|0.51|>…                     Any-to-any translation“我记得我当时队友的反应，跟我差不多同时间登顶的队友”I remember the reaction of my teammates, who reached …                      Audio caption analysis(Background birds chirping and music playing)A guitar tune is played with birds chirping in the background.                      Audio keyword analysis(A man speaks Chinese)Chinese, male speech                       Audio question and answer(Sound of a car)Q:Are the sounds outside? A: Yes. 
START OFTRANSCRIPTIONSTART OF ANALYSISAudio Language TagTask TagCAPTIONKEYWORDTRANSCRIBETRANSLATEQUESTIONText Language TagOutput Instruction
UNKNOWNZHENFRDEJAITESKOZHENFRDEJAITESKOWORD LEVEL TRANSCRIPTIONTRANSCRIPTION WITH ENTITY
question…OrTimestamps TagMulti-Task Training Format FrameworkTranscription TagTIMESTAMPSNO TIMESTAMPSEMOTIONSCENEINVERSE TEXT NORMALIZATIONSPEAKER INFOSONG INFOOrVOCAL…ZHTRANS-CRIBEZHTIMES-TAMPSWLTMy0.15cat0.00
0.00Figure 3: The overview of Qwen-Audio architecture and multitask-pretraining.
a 32-layer Transformer model that includes two convolution down-sampling layers as a stem. The audio
encoder iscomposed of 640Mparameters. Although Whisper issupervised trained forspeech recognition
and translation, its encoded representation still contains rich information, such as background noise (Gong
etal.,2023a),andcanevenbeusedforrecoveringtheoriginalspeech(Zhangetal.,2023b). Topreprocessthe
audiodata,Whisperresamplesittoafrequencyof16kHzandconvertstherawwaveforminto80-channelmel-
spectrogram using a window size of 25ms and a hop size of 10ms. Additionally, a pooling layer with a stride
oftwoisincorporatedtoreducethelengthoftheaudiorepresentation. Asaresult,eachframeoftheencoder
outputapproximatelycorrespondstoa40mssegmentoftheoriginalaudiosignal. SpecAugment(Parketal.)
is applied at the training time as data augmentation.
LargeLanguage Model Qwen-Audio incorporatesa largelanguagemodel asits foundational component.
Themodelisinitializedusingpre-trainedweightsderivedfromQwen-7B(Baietal.,2023a). Qwen-7Bisa
32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.
3.2 Multitask Pretraining
In the domain of audio processing, diverse audio datasets have been developed to address specific tasks,
as shown in Table 1. Qwen-Audio aims to perform co-training using a wide range of audio datasets. The
objective is to train a unified model capable of supporting all audio tasks, eliminating the need for laborious
model switching when dealing with different tasks. More importantly, during co-training, tasks can benefit
from each other since 1) similar tasks can benefit from knowledge sharing and collaborative learning, as
they share a common focus on the fundamental information embedded within the audio signal; 2) tasks that
rely on lower-level perceptual abilities can assist tasks that require higher-level understanding or reasoning
capabilities.
However, different datasets exhibit significant variations in textual labels due to differences in task focus,
6

--- PAGE 7 ---
Table 1: Multi-task pre-training dataset.
Types Task Description Hours
SpeechASR Automatic speech recognition (multiple languages) 30k
S2TT Speech-to-text translation 3.7k
OSR Overlapped speech recognition <1k
Dialect ASR Automatic dialect speech recognition 2k
SRWTEnglish speech recognition with word-level timestamps 10k
Mandarin speech recognition with word-level timestamps 11k
DID Dialect identification 2k
LID Spoken language identification 11.7k
SGC Speaker gender recognition (biologically) 4.8k
ER Emotion recognition <1k
SV Speaker verification 1.2k
SD Speaker diarization <1k
SER Speech entity recognition <1k
KS Keyword spotting <1k
IC Intent classification <1k
SF Slot filling <1k
SAP Speaker age prediction 4.8k
VSC Vocal sound classification <1k
SoundAAC Automatic audio caption 8.4k
SEC Sound event classification 5.4k
ASC Acoustic scene classification <1k
SED Sound event detection with timestamps <1k
AQA Audio question answering <1k
Music&SongSID Singer identification <1k
SMER Singer and music emotion recognition <1k
MC Music caption 25k
MIC Music instruments classification <1k
MNA Music note analysis such as pitch, velocity <1k
MGR Music genre recognition 9.5k
MR Music recognition <1k
MQA Music question answering <1k
language, granularity of annotation, and text structure (e.g., some data is structured while others are
unstructured). To train a network for different tasks, simply mixing these diverse datasets cannot lead
tomutualenhancement; instead,itintroducesinterference. Mostexistingmulti-tasktrainingapproaches
have either grouped similar tasks (e.g., audio captioning, transcription) or assigned a dataset ID to each
dataset (Wang et al., 2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b; Shu et al., 2023) to avoid
interference. Although these approaches have achieved certain effectiveness, there is still considerable
room for improvement. Whisper proposes a multitask training format by specifying tasks and condition
informationasasequenceofinputspecialtokenstothelanguagedecoder,suchasvoiceactivitydetection,
language identification, and sentence-level timestamp tags. However, Whisper focuses on speech translation
and recognition tasks only.
Multi-task Training Format Framework Motivated by Whisper (Radford et al., 2023), to incorporate
different kinds of audio, we propose a multitask training format framework as follows:
•Transcription Tag: The initiation of prediction is denoted using a transcription tag. The <|startof-
transcripts|>isemployedtoindicatethetasksinvolve accurately transcribing the spokenwords and
7

--- PAGE 8 ---
capturingthelinguisticcontentofaspeechrecording,suchasspeechrecognitionandspeechtranslation
tasks. For other tasks, the <|startofanalysis|> tag is utilized.
•Audio Language Tag: Then, we incorporate a language tag that indicates the spoken language in
the audio. This tag uses a unique token assigned to each language present in our training set, eight
languages in totally. In the case where an audio segment does not contain any speech, such as natural
sounds and music, the model is trained to predict a <|unknown|> token.
•Task Tag: The subsequent tokens specify the task. We categorize the collected audio tasks into five
categories: <|transcribe|>, <|translate|>, <|caption|>, <|analysis|>, and <|question-answer|> tasks.
For question-answer (QA) tasks, we append the corresponding questions after the tag.
•Text Language Tag: The tag token specifies the language of output text sequences.
•TimestampsTag: Thepresenceofa<|timestamps|>or<|notimestamps|>tokendetermineswhether
themodelneedstopredicttimestampsornot. Differentfromthesentence-leveltimestampsusedin
Whisper, the inclusion of the <|timestamps|> tag requires the model to perform fine-grained word-level
timestamp prediction, abbreviated as SRWT (Speech Recognition with Word-level Timestamps). The
prediction of these timestamps is interleaved with the transcription words: the start time token is
predicted before each transcription token, while the end time token is predicted after. According to our
experiments, SRWT improves the ability of the model to align audio signals with timestamps. This
improvedalignmentcontributestoacomprehensiveunderstandingofspeechsignalsbythemodel,
resulting in notable advancements across many tasks such as speech recognition and audio QA tasks.
•OutputInstruction: Lastly,weprovideoutputinstructiontofurtherspecifythetaskanddesiredformat
for different subtasks, and then the text output begins.
The guiding principle behind our framework is to maximize the sharing of knowledge among similar tasks
throughsharedtags,therebyimprovingtheirperformance. Meanwhile,weensurethatdifferenttasksand
outputformats canbedistinguishedto avoidtheone-to-manymappingproblemfor themodel. Pleasesee
Figure 3 for an overview of our multitask format of Qwen-Audio.
3.3 Supervised Fine-tuning
The extensive pretraining of multitask models has equipped them with a broad understanding of audio.
Buildinguponthis,weemployinstruction-basedfine-tuningtechniquestoimprovetheabilityofthemodelto
alignwithhumanintent,resultinginaninteractivechatmodel,termedQwen-Audio-Chat. Toaccomplishthis,
we manually create demonstrations for each task. These demonstrations consist of raw text labels, questions,
andanswers. WethenutilizeGPT-3.5(OpenAI,2022)togeneratefurtherquestionsandanswersbasedonthe
provided raw text labels. Additionally, we also create a dataset of audio-dialogue data by employing manual
annotation, model generation, and strategy concatenation. This dataset helps us incorporate reasoning, story
generation, and multi-image comprehension abilities into our model.
Tohandlemulti-audiodialogueandmultipleaudioinputseffectively,weintroducetheconventionoflabeling
different audios with "Audio id:", where idcorresponds to the order of the audio input dialogue. In terms of
dialogue format, we construct our instruction tuning dataset using the ChatML (Openai) format. In this
format, each interaction’s statement is marked with two special tokens ( <im_start >and<im_end >) to
facilitate dialogue termination.
8

--- PAGE 9 ---
Table 2: Summary of the evaluation benchmarks for Qwen-Audio.
Task Description Dataset Split Metric
ASR Automatic Speech RecognitionAishell1 (Bu et al., 2017) dev | test
WER Aishell2 (Du et al., 2018) test
Librispeech (Panayotov et al., 2015) dev | test
S2TT Speech-to-text translation CoVoST2 (Wang et al., 2020) test BLEU1(Papineni et al., 2002)
SRWT Speech Recognition with Word-level Timestamp Industrial Data (Gao et al., 2023) test AAS (Shi et al., 2023)
AAC Automatic Audio Caption Clotho (Drossos et al., 2020) testCIDEr | SPICE | SPIDEr
(Vedantam et al., 2015; Anderson et al., 2016)
ASC Acoustic Scene ClassficationCochlScene (Jeong and Park, 2022) testACCTUT2017 (Mesaros et al., 2017) eval
SER Speech Emotion Recognition Meld (Poria et al., 2019) test ACC
AQA Audio Question & Answer ClothoAQA (Lipping et al., 2022) test ACC
VSC Vocal Sound Classification VocalSound (Gong et al., 2022) test ACC
MNA Music Note Analysis NSynth (Engel et al., 2017) test ACC / MAP
The Data Format Example of Supervised Fine-Tuning.
<im_start >user
Audio 1: <audio >emov-db/141-168-0155.wav </audio >what does the speaker say? <im_end >
<im_start >assistant
The speaker says in English, "Won’t you draw up, gentlemen.". <im_end >
<im_start >user
What’s the mood of the person? <im_end >
<im_start >assistant
Based on the voice, the mood of the person is disgusted. <im_end >
Inordertofacilitateversatileinputfrombothaudioandpuretextmodalitieswithinmulti-turndialogues,we
usea combinationof audio-centricinstructiondata mentionedabove andpuretext instructiondata during
this training process. This approach allows the model to handle diverse forms of input seamlessly. The total
amount of instruction tuning data is 20k.
4 Experiments
4.1 Setup
For multi-task pre-training, we freeze the weights of LLM and only optimize the audio encoder. This trained
modelis referredtoasQwen-Audio. In thesubsequentsupervisedfine-tuningstage,we fixtheweightsof
theaudioencoderandonlyoptimizetheLLM.TheresultingmodelisdenotedasQwen-Audio-Chat. The
detailed training configurations of both stages are listed in Table 6.
4.2 Evaluation
In order to assess the universal understanding capabilities of Qwen-Audio, as shown in Table 2, we perform
acomprehensiveevaluationthatencompassesvarioustasks, namelyAutomaticSpeechRecognition(ASR),
Speech-to-TextTranslation(S2TT),AutomaticAudioCaptioning(AAC),AcousticSceneClassification(ASC),
Speech Emotion Recognition (SER), Audio Question and Answering (AQA), Vocal Sound Classification
(VSC), and Music Note Analysis (MNA). This evaluation is conducted across 12 datasets. The evalua-
tion datasets are rigorously excluded from the training data to avoid data leakage. The detailed training
configurations of both stages are listed in Table 6.
1https://github.com/mjpost/sacrebleu
9

--- PAGE 10 ---
Table 3: The results of Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Auto-
matic Audio Captioning (AAC), Speech Recognition with Word-level Timestamps (SRWT), Acoustic Scene
Classification(ASC),SpeechEmotionRecognition(SER),AudioQuestionandAnswering(AQA),Vocal
SoundClassification(VSC),andMusicNoteAnalysis(MNA)tasks. ForSRWTtask,theresultsofForced-
aligner (McAuliffe et al., 2017) are to predict the timestamps given the ground-truth transcripts, while
Paraformer-large-TP (Gao et al., 2023) and Qwen-audio tackle a more challenging scenario by directly gener-
ating sequences containing both transcriptions and timestamps.
Task Dataset ModelPerformance
Metrics Results
ASRLibrispeech
dev-clean |dev-other |
test-clean |test-otherSpeechT5 (Ao et al., 2021)
WER↓2.1 | 5.5 | 2.4 | 5.8
SpeechNet (Chen et al., 2021) - | - | 30.7 | -
SLM-FT (Wang et al., 2023b) - | - | 2.6 | 5.0
SALMONN (Anonymous, 2023) - | - | 2.1 | 4.9
Qwen-Audio 1.8|4.0|2.0|4.2
Aishell1
dev|testMMSpeech-base (Zhou et al., 2022)
WER↓2.0 | 2.1
MMSpeech-large (Zhou et al., 2022) 1.6 | 1.9
Paraformer-large (Gao et al., 2023) - | 2.0
Qwen-Audio 1.2|1.3
Aishell2
Mic|iOS|AndroidMMSpeech-base (Zhou et al., 2022)
WER↓4.5 | 3.9 | 4.0
Paraformer-large (Gao et al., 2023) - | 2.9| -
Qwen-Audio 3.3| 3.1 |3.3
S2TTCoVoST2
en-de|de-en|
en-zh|zh-enSALMONN (Anonymous, 2023)
BLEU ↑18.6 | - | 33.1 | -
SpeechLLaMA (Wu et al., 2023a) - | 27.1 | - | 12.3
BLSP (Wang et al., 2023a) 14.1 | - | - | -
Qwen-Audio 25.1|33.9|41.5|15.7
CoVoST2 SpeechLLaMA (Wu et al., 2023a)BLEU ↑27.9 | 25.2 | 25.9
es-en|fr-en|it-en Qwen-Audio 39.7|38.5|36.0
AAC ClothoPengi (Deshmukh et al., 2023) CIDEr |
SPICE | SPIDEr ↑0.416 | 0.126 | 0.271
Qwen-Audio 0.441|0.136|0.288
SRWT Industrial DataForce-aligner (McAuliffe et al., 2017)
AAS (ms) ↓60.3
Paraformer-large-TP (Gao et al., 2023) 65.3
Qwen-Audio 51.5
ASCCochlSceneCochlScene (Jeong and Park, 2022)ACC↑0.669
Qwen-Audio 0.795
TUT2017Pengi (Deshmukh et al., 2023)ACC↑0.353
Qwen-Audio 0.649
SER MeldWavLM-large (Chen et al., 2022)ACC↑0.542
Qwen-Audio 0.557
AQA ClothoAQAClothoAQA (Lipping et al., 2022)
ACC | ACC (binary) ↑0.542 | 0.627
Pengi (Deshmukh et al., 2023) - | 0.645
Qwen-Audio 0.579|0.749
VSC VocalSoundCLAP (Elizalde et al., 2022)
ACC↑0.4945
Pengi (Deshmukh et al., 2023) 0.6035
Qwen-Audio 0.9289
MNANS. QualitiesPengi (Deshmukh et al., 2023)MAP↑0.3860
Qwen-Audio 0.4742
NS. InstrumentPengi (Deshmukh et al., 2023)ACC↑0.5007
Qwen-Audio 0.7882
4.3 Main Results
In this section, we present a comprehensive evaluation of the Qwen-Audio model, assessing its performance
across various tasks without any task-specific fine-tuning. We begin by examining its English Automatic
10

--- PAGE 11 ---
Table 4: Results of ASR tasks with or without training word-level timestamps tasks.
MethodLibriSpeech AISHELL1
dev-clean dev-other test-clean test-other dev test
w/o SRWT 1.93 4.18 2.22 4.21 1.54 1.71
Qwen-Audio 1.79 4.00 2.04 4.19 1.22 1.29
Table 5: Results of AQA tasks with or without training word-level timestamps tasks.
MethodClothoAQA MusicAVQA
test test-binary audio question
w/o SRWT 0.5648 0.7418 0.7027
Qwen-Audio 0.5795 0.7491 0.7211
Speech Recognition (ASR) results, as depicted in Table 3, where Qwen-Audio exhibits superior perfor-
mance compared to previous multi-task learning models. Specifically, it achieves a 2.0% and 4.2% WER
on the librispeech test-clean and test-other datasets, respectively. Similarly, the Chinese Mandarin ASR
results demonstrate Qwen-Audio’s competitive performance against previous approaches. To the best of
our knowledge, Qwen-Audio achieves state-of-the-art results on the Aishell1 dev and test sets. Furthermore,
we evaluate Qwen-Audio’s speech translation performance on the CoVoST2 dataset. The results reveal that
Qwen-Audio outperforms the baselines by a substantial margin across all seven translation directions.
Lastly, we analyze the performance of Qwen-Audio on various audio analysis tasks, including AAC, SWRT
ASC,SER,AQA,VSC,andMNA,assummarizedinTable3. Acrossthesetasks,Qwen-Audioconsistently
outperforms the baselines by a significant margin. Notably, it achieves state-of-the-art results on CochlScene,
ClothoAQA, and VocalSound, thereby demonstrating the model’s robust audio understanding capabilities.
4.4 Results of Interactive Chat
We showcase the conversational capabilities of Qwen-Audio-Chat through illustrative cases depicted in
Figure 2. Furthermore, we intend to provide public access to the trained models for online chat interactions.
4.5 The Analysis of Word-level Timestamps Prediction
We propose the task of speech recognition with word-level timestamps (SRWT) by training Qwen-Audio to
notonlyrecognizespeechtranscriptsbutalsopredictthetimestampsforeachword. ThepurposeofSRWTis
twofold: firstly, to improve the model’s ability to align audio signals with fine-grained timestamps; secondly,
tosupportgroundingofspeechandaudio,andgrounding-basedQAtasksinQwen-Audio-Chat,suchas
findingthestartingandendingtimeofanaudiosegmentmentioningaperson’snameoridentifyingwhether
a sound occurs in the given audio2.
Inthissection,weexcludethetrainingofSRWTtasksfrommulti-taskpretrainingwhilemaintainingtheother
tasks unchanged. Notably, the removal of SRWT does not impact the coverage of audio datasets for training
since SRWT tasks share the same audio dataset as automatic speech recognition (ASR) tasks. The results are
shown in Table 4 and Table 5: models trained with SRWT achieve superior performance in automatic speech
recognition and audio question-answering tasks, including natural sounds QA and Music QA. These results
highlight the efficacy of incorporating fine-grained word-level timestamps to enhance the general audio
signal grounding ability and subsequently improve the performance of sound and music signal QA tasks.
2Audio event detection can be considered as a subtask of event timestamp prediction as the absence of an event’s timestamp implies
its non-occurrence in the audio.
11

--- PAGE 12 ---
5 Conclusion
Inthis paper,wepresent theQwen-Audioseries, asetof large-scale audio-languagemodelswith universal
audio understanding abilities. To incorporate different kinds of audios for co-training, we propose a unified
multi-tasklearningframeworkthatfacilitatesthesharingofknowledgeamongsimilartasksandavoidsone-to-
manymappingproblemcausedbydifferenttextformats. Withoutanytask-specificfine-tuning,theresulting
Qwen-Audiomodelsoutperformpreviousworksacrossdiversebenchmarks,demonstratingitsuniversal
audio understanding abilities. Through supervised instruction finetuning, Qwen-Audio-Chat showcases
robustcapabilitiesinaligningwithhumanintent,supportingmultilingualandmulti-turndialoguesfrom
both audio and text inputs.
6 Acknowledgements
We express our gratitude to Jinze Bai, Shuai Bai, Peng Wang, Sinan Tan, Shijie Wang for their insightful
discussion. WewouldliketothankJuanZhu,JunyangLin,SiqiZheng,JiamingWangandZhihaoDufor
their support of this project.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. NeurIPS, 2022.
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image
captionevaluation. In ComputerVision–ECCV2016: 14thEuropeanConference,Amsterdam,TheNetherlands,
October 11-14, 2016, Proceedings, Part V 14 . Springer, 2016.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv:2305.10403 ,
2023.
Anonymous. SALMONN:Towardsgenerichearing abilitiesforlarge languagemodels. In Submittedto The
Twelfth International Conference on Learning Representations , 2023. under review.
JunyiAo,RuiWang,LongZhou,ChengyiWang,ShuoRen,YuWu,ShujieLiu,TomKo,QingLi,YuZhang,etal.
Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing. arXiv:2110.07205 ,
2021.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023a.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jin-
grenZhou. Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities. CoRR,abs/2308.12966,
2023b.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
NeurIPS, 2020.
HuiBu,JiayuDu,XingyuNa,BenguWu,andHaoZheng. AISHELL-1: anopen-sourcemandarinspeech
corpus and a speech recognition baseline. In 20th Conference of the Oriental Chapter of the International
Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, O-COCOSDA 2017, Seoul,
South Korea, November 1-3, 2017 . IEEE, 2017.
12

--- PAGE 13 ---
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv:2306.15195 , 2023.
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda,
TakuyaYoshioka,XiongXiao, JianWu,Long Zhou,ShuoRen, YanminQian,Yao Qian,JianWu, Michael
Zeng, Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech
processing. IEEE J. Sel. Top. Signal Process. , 2022.
Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin, Sung-Feng Huang, Da-Rong
Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung-yi Lee. Speechnet: A universal modularized model for
speech processing tasks. arXiv:2105.03070 , 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. PaLM:Scalinglanguagemodeling
with pathways. arXiv:2204.02311 , 2022.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression.
arXiv:2210.13438 , 2022.
Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for
audio tasks. CoRR, 2023.
Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In 2020
IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020 . IEEE, 2020.
Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. AISHELL-2: transforming mandarin ASR research into
industrial scale. abs/1808.10583, 2018.
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP: learning audio
concepts from natural language supervision. abs/2206.04769, 2022.
JesseH.Engel,CinjonResnick,AdamRoberts,SanderDieleman,MohammadNorouzi,DouglasEck,and
KarenSimonyan. Neuralaudiosynthesisofmusicalnoteswithwavenetautoencoders. In Proceedingsof
the34thInternationalConferenceonMachineLearning, ICML2017,Sydney,NSW,Australia, 6-11August2017 ,
Proceedings of Machine Learning Research. PMLR, 2017.
Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao
Du,ZhangyuXiao,andShiliangZhang. Funasr: Afundamentalend-to-endspeechrecognitiontoolkit.
CoRR, abs/2305.11013, 2023.
YuanGong,JinYu,andJamesR.Glass. Vocalsound: Adatasetforimprovinghumanvocalsoundsrecognition.
InIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,ICASSP2022,VirtualandSingapore,
23-27May2022 ,pages151–155.IEEE,2022. doi: 10.1109/ICASSP43922.2022.9746828. URL https://doi.
org/10.1109/ICASSP43922.2022.9746828 .
YuanGong,SameerKhurana,LeonidKarlinsky,andJamesR.Glass. Whisper-at: Noise-robustautomatic
speech recognizers are also strong general audio event taggers. CoRR, abs/2307.03183, 2023a.
Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and
understand. CoRR, abs/2305.10790, 2023b.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Ab-
delrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymaskedpredictionof
hidden units. IEEE ACM Trans. Audio Speech Lang. Process. , 2021.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685 , 2021.
13

--- PAGE 14 ---
RongjieHuang,MingzeLi,DongchaoYang,JiatongShi,XuankaiChang,ZhenhuiYe,YuningWu,Zhiqing
Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and
generating speech, music, sound, and talking head. CoRR, abs/2304.12995, 2023.
Il-Young Jeong and Jeongsoo Park. Cochlscene: Acquisition of acoustic scene data using crowdsourcing.
abs/2211.02289, 2022.
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal
Manohar,YossiAdi,JayMahadeokar,andWei-NingHsu. Voicebox: Text-guidedmultilingualuniversal
speech generation at scale. CoRR, 2023.
JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. Blip: Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In ICML, 2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-
training with frozen image encoders and large language models. In International Conference on Machine
Learning,ICML2023,23-29July2023,Honolulu,Hawaii,USA ,ProceedingsofMachineLearningResearch.
PMLR, 2023.
Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clotho-aqa: A
crowdsourceddatasetforaudioquestionanswering. In 30thEuropeanSignalProcessingConference,EUSIPCO
2022, Belgrade, Serbia, August 29 - Sept. 2, 2022 . IEEE, 2022.
Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and
ZhaopengTu. Macaw-llm: Multi-modallanguagemodelingwithimage,audio,video,andtextintegration.
CoRR, abs/2306.09093, 2023.
SoumiMaiti,YifanPeng,ShukjaeChoi,Jee-weonJung,XuankaiChang,andShinjiWatanabe. Voxtlm: unified
decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.
arXiv:2309.07937 , 2023.
MichaelMcAuliffe,MichaelaSocolof,SarahMihuc,MichaelWagner,andMorganSonderegger. Montreal
forcedaligner: Trainabletext-speechalignmentusingkaldi. In Interspeech2017,18thAnnualConferenceof
the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017 , 2017.
AnnamariaMesaros, ToniHeittola,Aleksandr Diment,BenjaminElizalde, AnkitShah,Emmanuel Vincent,
Bhiksha Raj, and Tuomas Virtanen. DCASE2017 challenge setup: Tasks, datasets and baseline system. In
Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events, DCASE 2017, Munich,
Germany, November 16-17, 2017 , 2017.
Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, R. J.
Skerry-Ryan,andMichelleTadmorRamanovich. Lmswithavoice: Spokenlanguagemodelingbeyond
speech tokens. CoRR, 2023.
Openai. Chatml documents. URL https://github.com/openai/openai-python/blob/main/chatml.md .
OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt .
OpenAI. Gpt-4 technical report, 2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. NeurIPS, 2022.
VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur. Librispeech: AnASRcorpusbasedon
public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,
ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015 . IEEE, 2015.
14

--- PAGE 15 ---
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics ,
2002.
Daniel S.Park, WilliamChan, Yu Zhang, Chung-ChengChiu, Barret Zoph,Ekin D. Cubuk,and Quoc V.Le.
Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019,
20th AnnualConferenceof the InternationalSpeechCommunication Association, Graz, Austria, 15-19 September
2019.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world. arXiv:2306.14824 , 2023.
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea.
MELD:Amultimodalmulti-partydatasetforemotionrecognitioninconversations. In Proceedingsofthe
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers . Association for Computational Linguistics, 2019.
Qwen. Introducing qwen-7b: Openfoundationand human-alignedmodels(ofthestate-of-the-arts),2023.
URL https://github.com/QwenLM/Qwen-7B .
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust
speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. The
Journal of Machine Learning Research , 2020.
Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix
de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Mucken-
hirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara N. Sainath, Johan Schalkwyk, Matthew Sharifi,
MichelleTadmorRamanovich,MarcoTagliasacchi,AlexandruTudor,MihajloVelimirovic,DamienVincent,
Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and
Christian Havnø Frank. Audiopalm: A large language model that can speak and listen. CoRR.
YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang. Hugginggpt: Solving
AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023.
XianShi,YanniChen,ShiliangZhang,andZhijieYan. Achievingtimestamppredictionwhilerecognizing
withnon-autoregressiveend-to-endasrmodel. In NationalConferenceonMan-MachineSpeechCommunication .
Springer, 2023.
Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin
Shi. Llasm: Large language and speech model. arXiv:2308.15930 , 2023.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,
Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv:2307.05222 , 2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation
language models. arXiv:2302.13971 , 2023a.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
language models. arXiv:2302.13971 , 2023b.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer,
MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,Cynthia
15

--- PAGE 16 ---
Gao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,Marcin
Kardas,ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-Anne
Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,
BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023c.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
HannaM.Wallach,RobFergus,S.V.N.Vishwanathan,andRomanGarnett,editors, AdvancesinNeural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017 , 2017.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In CVPR, 2015.
Changhan Wang, Anne Wu, and Juan Miguel Pino. Covost 2: A massively multilingual speech-to-text
translation corpus. abs/2007.10310, 2020. URL https://arxiv.org/abs/2007.10310 .
ChenWang,MinpengLiao,ZhongqiangHuang,JinliangLu,JunhongWu,YuchenLiu,ChengqingZong,and
JiajunZhang. Blsp: Bootstrappinglanguage-speechpre-trainingviabehavioralignmentofcontinuation
writing. arXiv:2309.00916 , 2023a.
MingqiuWang,WeiHan,IzhakShafran,ZelinWu,Chung-ChengChiu,YuanCao,YongqiangWang,Nanxin
Chen,YuZhang,HagenSoltau,PaulK.Rubenstein,LukasZilka,DianYu,ZhongMeng,GolanPundak,
Nikhil Siddhartha, Johan Schalkwyk, and Yonghui Wu. SLM: bridge the thin gap between speech and text
foundation models. abs/2310.00230, 2023b.
MingqiuWang,WeiHan,IzhakShafran,ZelinWu,Chung-ChengChiu,YuanCao,YongqiangWang,Nanxin
Chen,YuZhang,HagenSoltau,etal. Slm: Bridgethethingapbetweenspeechandtextfoundationmodels.
arXiv:2310.00230 , 2023c.
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu
Wei. Viola: Unifiedcodeclanguagemodelsforspeechrecognition,synthesis,andtranslation. CoRR,2023d.
Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min
Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as a versatile
speech transformer. CoRR, 2023e.
JianWu,YasheshGaur,ZhuoChen,LongZhou,YimengZhu,TianruiWang,JinyuLi,ShujieLiu,BoRen,
Linquan Liu, and Yu Wu. On decoder-only architecture for speech-to-text and large language model
integration. abs/2307.03917, 2023a.
ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-SengChua. Next-gpt: Any-to-anymultimodalLLM.
CoRR, abs/2309.05519, 2023b.
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An
end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process. , 2022.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Em-
poweringlarge languagemodels withintrinsiccross-modal conversationalabilities. CoRR,abs/2305.11000,
2023a.
XinZhang,DongZhang,ShiminLi,YaqianZhou,andXipengQiu. Speechtokenizer: Unifiedspeechtokenizer
for speech large language models. CoRR, abs/2308.16692, 2023b.
16

--- PAGE 17 ---
YuZhang,WeiHan,JamesQin,YongqiangWang,AnkurBapna,ZhehuaiChen,NanxinChen,BoLi,Vera
Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa
Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara N.
Sainath,PedroJ.Moreno,Chung-ChengChiu,JohanSchalkwyk,FrançoiseBeaufays,andYonghuiWu.
Google usm: Scaling automatic speech recognition beyond 100 languages. CoRR, 2023c.
Xiaohuan Zhou, Jiaming Wang, Zeyu Cui, Shiliang Zhang, Zhijie Yan, Jingren Zhou, and Chang Zhou.
Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition. abs/2212.00500,
2022.
17

--- PAGE 18 ---
A Hyperparameters
We report the detailed training hyperparameter settings of Qwen-Audio in Table 6.
Table 6: Training hyperparameters of Qwen-Audio
Configuration Multi-task Pre-training Supervised Fine-tuning
Audio encoder init. Whisper-large-v2 Qwen-audio 1st-stage
LLM init. Qwen-7B Qwen-7B
SpecAugment Policy LibriSpeech Basic LibriSpeech Basic
Optimizer AdamW AdamW
Optimizer hyperparameter β1=0.9, β2=0.98, eps = 1e−6
Peak learning rate 5e−51e−5
Minimum learning rate 1e−51e−6
Audio encoder learning rate decay 0.95 0
Learning rate schedule cosine decay cosine decay
Weight decay 0.05 0.05
Gradient clip 1.0 1.0
Training steps 500k 8k
Warm-up steps 2000 3k
Global batch size 120 128
Gradient Acc. 1 8
Numerical precision bfloat16 bfloat16
Optimizer sharding ✓ ✓
Activation checkpointing ✗ ✗
Model parallelism ✗ 2
Pipeline parallelism ✗ ✗
18
