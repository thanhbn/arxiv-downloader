# Học Liên Tục cho Nhận Dạng Giọng Nói Tự Động Đầu-cuối-Đầu Đơn Ngữ

Steven Vander Eeckt và Hugo Van hamme
KU Leuven
Khoa Kỹ Thuật Điện ESAT-PSI
Kasteelpark Arenberg 10, Bus 2441, B-3001 Leuven Bỉ
{steven.vandereeckt, hugo.vanhamme}@esat.kuleuven.be

Tóm tắt—Việc thích ứng các mô hình Nhận Dạng Giọng Nói Tự Động (ASR) với các miền mới dẫn đến sự suy giảm hiệu suất trên (các) miền ban đầu, một hiện tượng được gọi là Quên Thảm Khốc (CF). Ngay cả các mô hình ASR đơn ngữ cũng không thể được mở rộng sang các giọng địa phương, phương ngữ, chủ đề mới, v.v. mà không gặp phải CF, khiến chúng không thể được cải thiện liên tục mà không lưu trữ toàn bộ dữ liệu quá khứ. May mắn thay, các phương pháp Học Liên Tục (CL), nhằm mục đích cho phép thích ứng liên tục trong khi khắc phục CF, có thể được sử dụng. Trong bài báo này, chúng tôi triển khai một số lượng lớn các phương pháp CL cho ASR Đầu-cuối-Đầu và kiểm tra và so sánh khả năng mở rộng của chúng đối với mô hình Hybrid CTC-Transformer đơn ngữ qua bốn nhiệm vụ mới. Chúng tôi thấy rằng phương pháp CL hoạt động tốt nhất đóng khoảng cách giữa mô hình được tinh chỉnh (cận dưới) và mô hình được huấn luyện chung trên tất cả các nhiệm vụ (cận trên) hơn 40%, trong khi chỉ cần truy cập vào 0,6% dữ liệu ban đầu.

Từ khóa—Nhận Dạng Giọng Nói Tự Động Đầu-cuối-Đầu, Học Liên Tục, Nhận Dạng Giọng Nói Đơn Ngữ

I. GIỚI THIỆU

Nhận Dạng Giọng Nói Tự Động (ASR) đã có tiến bộ lớn trong những năm gần đây, chuyển từ các mô hình Hidden Markov Model (HMM) sang các mô hình Đầu-cuối-Đầu (E2E). Tuy nhiên, giống như các Mạng Nơ-ron Nhân Tạo (ANN) mà chúng sử dụng, các mô hình E2E ASR gặp phải vấn đề Quên Thảm Khốc (CF) [1] khi được thích ứng với các nhiệm vụ mới, ngay cả đối với các nhiệm vụ đơn ngữ: chỉ cần phân phối dữ liệu của các nhiệm vụ cũ và mới khác nhau là CF xảy ra.

May mắn thay, nhiều phương pháp Học Liên Tục (CL) đã được đề xuất trong cộng đồng phân loại hình ảnh, cho phép các ANN học liên tục mà không gặp phải CF. Theo [2], các phương pháp CL được phân loại thành ba nhóm: i) các phương pháp dựa trên chính quy hóa sử dụng mất mát chính quy hóa để huấn luyện các nhiệm vụ mới sao cho không làm tổn hại hiệu suất của các nhiệm vụ trước đó. [3]–[5] thực hiện điều này bằng cách ước tính tầm quan trọng của từng tham số đối với các nhiệm vụ trước đó và sử dụng các trọng số tầm quan trọng này trong chính quy hóa L2 có trọng số để học các nhiệm vụ mới. [6] sử dụng chưng cất tri thức [7] trên dữ liệu của các nhiệm vụ mới để chuyển giao tri thức từ mô hình cũ sang mô hình mới; ii) các phương pháp dựa trên lặp lại lưu trữ một tập hợp các mẫu đại diện trong bộ nhớ để ôn tập các nhiệm vụ cũ khi học các nhiệm vụ mới. Đơn giản nhất là [8]–[10], huấn luyện chung trên nhiệm vụ mới và bộ nhớ. Ngoài ra, [11], [12] tập trung vào việc căn chỉnh gradient giữa các nhiệm vụ cũ và mới; iii) các phương pháp dựa trên kiến trúc tăng khả năng của mô hình khi học các nhiệm vụ mới. Nhóm cuối cùng không được xem xét trong bài báo này.

Về ASR và đặc biệt là E2E ASR, CL là một chủ đề rất mới và chưa được khám phá nhiều. [13], [14] áp dụng CL cho mô hình âm thanh học của mô hình ASR dựa trên HMM. [15] xem xét CL cho mô hình wav2vec2 được huấn luyện trước [16]. [17] kết hợp mô hình Text-to-Speech và ASR để ngăn chặn việc quên. [18] áp dụng Learning without Memorizing [19] cho E2E ASR, tập trung vào tình huống mà các nhiệm vụ tiếp theo nhỏ hơn nhiều so với nhiệm vụ ban đầu. Cuối cùng, [20] triển khai bốn phương pháp CL hiện có cho E2E ASR. So với [20], chúng tôi triển khai thêm bảy phương pháp CL cho E2E ASR. Chúng tôi kiểm tra và so sánh khả năng mở rộng và cải thiện liên tục của chúng đối với E2E ASR đơn ngữ bằng cách huấn luyện trên dữ liệu mới. Để làm cho các thí nghiệm thực tế hơn, chúng tôi chạy các phương pháp mà không giả định có quyền truy cập vào bộ xác thực của các nhiệm vụ trước đó để tối ưu hóa siêu tham số. Vì đối với nhiều phương pháp CL, cả dựa trên chính quy hóa và dựa trên ôn tập, trọng số của chính quy hóa là một siêu tham số quan trọng, chúng tôi đề xuất, dựa trên [2], một cách đơn giản và hiệu quả để xác định trọng số này.

II. HỌC LIÊN TỤC CHO E2E ASR

Trước tiên chúng tôi mô tả chi tiết về mô hình E2E ASR được xem xét cũng như về mục tiêu của Học Liên Tục cho E2E ASR.

Mô hình. Mô hình của chúng tôi là Hybrid CTC/Transformer từ [21]. Mất mát của nó trong quá trình huấn luyện được tính như sau:

L(X; y; θ) = λc Lc(X; y; θ) + (1 - λc)Ld(X; y; θ)  (1)

trong đó Lc(X; y; θ) và Ld(X; y; θ) lần lượt là mất mát CTC và Cross-Entropy (CE) của Decoder của mô hình với tham số θ trên câu nói X với nhãn thực y. Như trong [21], trọng số của CTC cho huấn luyện và giải mã là λc = 0,3. Không có Mô hình Ngôn Ngữ nào được sử dụng trong quá trình giải mã. Đầu ra của mô hình là 300 mảnh từ, được tạo ra bởi mô hình Sentence Piece [22] trên dữ liệu huấn luyện của nhiệm vụ đầu tiên.

Ký hiệu. Ký hiệu fc(X; θ) ∈ ℝ^(L×o) và fd(X; θ) ∈ ℝ^(W×o) lần lượt là đầu ra CTC và Decoder của mô hình với tham số θ, cho câu nói X, với L, W và o là độ dài câu nói, độ dài đầu ra và số lượng mảnh từ. Trong quá trình huấn luyện, fd(X; θ) được điều kiện hóa trên nhãn thực y.

Công thức bài toán. Gọi D₁, D₂, ..., Dₜ đại diện cho các tập dữ liệu huấn luyện có nhãn của T nhiệm vụ. Nếu θₜ là tham số của mô hình sau khi học t nhiệm vụ, thì mục tiêu của các phương pháp CL là học các nhiệm vụ 1, ..., T theo thứ tự sao cho sau T nhiệm vụ, θₜ, được thích ứng từ θₜ₋₁, thỏa mãn:

θₜ = arg min_θ ∑ᵢ₌₁ᵀ ∑_{(X,y)∈Dᵢ} L(X; y; θ)  (2)

Tuy nhiên, khi học nhiệm vụ T trên Dₜ, quyền truy cập vào D₁, ..., Dₜ₋₁ được giả định là bị mất (mặc dù việc lưu trữ một số lượng nhỏ câu nói cho mỗi nhiệm vụ trong bộ nhớ được cho phép đối với các phương pháp dựa trên ôn tập), do đó θₜ không thể được tính trực tiếp từ (2). Ngoài ra, chúng tôi giả định rằng chúng tôi không thể sử dụng các bộ xác thực của các nhiệm vụ trước đó (để tối ưu hóa siêu tham số của các phương pháp CL). Chúng tôi coi đây là một tình huống thực tế hơn.

III. CÁC PHƯƠNG PHÁP HỌC LIÊN TỤC

Chúng tôi xem xét cả các phương pháp dựa trên chính quy hóa và dựa trên ôn tập. Vì các mô hình E2E ASR có kiến trúc phức tạp và đòi hỏi tính toán lớn để huấn luyện, chúng tôi tập trung vào các phương pháp nhẹ dễ áp dụng cho bất kỳ kiến trúc ANN nào và đã được chứng minh là hoạt động tốt trong các miền khác.

A. Các Phương Pháp Dựa Trên Chính Quy Hóa

Các phương pháp dựa trên chính quy hóa tính toán mất mát chính quy hóa được thêm vào L(X; y; θ) từ (1) trong quá trình huấn luyện.

Elastic Weight Consolidation (EWC). Sau khi huấn luyện nhiệm vụ t, EWC [3] tính toán đường chéo của ma trận thông tin Fisher, ký hiệu là Λₜ. Λₜᵢᵢ được coi là trọng số tầm quan trọng của tham số i cho nhiệm vụ t. Tiếp theo, Λₜ được thêm vào Λ̂ₜ = Λ̂ₜ₋₁ + Λₜ như trong [23], và được sử dụng trong mất mát chính quy hóa để học nhiệm vụ t+1:

Lewc(θ) = λ/2 (θ - θₜ)ᵀ Λ̂ₜ (θ - θₜ)  (3)

Vì Λ̂ₜ là đường chéo, (3) được rút gọn thành chính quy hóa L2 có trọng số, với trọng số Λ̂ₜᵢᵢ cho tham số i.

Memory-Aware Synapses (MAS). MAS [4] hoạt động tương tự như EWC, nhưng tính toán (đường chéo của) Λₜ khác nhau. Cho rằng mô hình ASR có cả đầu ra CTC và Decoder, chúng tôi tính toán Λₜ cho MAS như sau:

Λₜᵢᵢ = 𝔼_{X∈Dₜ} [λc ∂||fc(X; θₜ)||₂²/∂θᵢ + (1-λc) ∂||fd(X; θₜ)||₂²/∂θᵢ]  (4)

Tiếp theo, mất mát hoàn toàn giống như EWC trong (3).

Continual learning with Sampled Quasi-Newton (CSQN). CSQN [24] được đề xuất để mở rộng EWC bằng cách xem xét tương tác giữa các tham số. Bắt đầu từ Λₜ của EWC, CSQN xem xét các phương pháp quasi-Newton để tính toán các xấp xỉ thứ hạng thấp của Hessian của mất mát, sau đó được sử dụng như trong (3) để chính quy hóa huấn luyện. Chúng tôi xem xét cả phiên bản tiêu chuẩn và phiên bản rút gọn, được gọi là BTREE trong [24] và ở đây chúng tôi ký hiệu là CSQN-BT.

Learning Without Forgetting (LWF). LWF [6], khi học nhiệm vụ t+1, sử dụng chưng cất tri thức [7] giữa mô hình cũ (với tham số θₜ) làm giáo viên và mô hình hiện tại (với tham số θ) làm học sinh, trên dữ liệu của nhiệm vụ mới:

Llwf(X; θ) = λ(λc ∑ᵢ₌₁ᴸ ∑ⱼ₌₁ᵒ fc_{i,j}(X; θₜ)/τ log fc_{i,j}(X; θ)/τ + (1-λc) ∑ᵢ₌₁ᵂ ∑ⱼ₌₁ᵒ fd_{i,j}(X; θₜ)/τ log fd_{i,j}(X; θ)/τ)  (5)

Với τ được gọi là nhiệt độ. Trong các thí nghiệm của chúng tôi, τ = 1.

B. Các Phương Pháp Dựa Trên Ôn Tập

Các phương pháp dựa trên ôn tập sử dụng một bộ nhớ nhỏ các mẫu đại diện của các nhiệm vụ trước đó để cho phép CL.

Experience Replay (ER). Chúng tôi xem xét ba biến thể của ER [8]. Trong biến thể tiêu chuẩn, mini-batch từ nhiệm vụ hiện tại được bổ sung với một mini-batch được lấy mẫu từ bộ nhớ và gửi qua mô hình để tính toán mất mát. Vì điều này có thể dẫn đến việc quá khớp trên bộ nhớ, mất mát của mini-batch được lấy mẫu từ bộ nhớ có thể được gán một trọng số α ∈ (0,1), ký hiệu là ER(α). Ngoài ra, như trong [9], tập huấn luyện và bộ nhớ có thể được hợp nhất để huấn luyện trên tập kết quả, được gọi là BER (Batch-level ER).

Average-Gradient Episodic Memory (A-GEM). Xem xét g = ∂L(X; y; θ)/∂θ với (X, y) là mini-batch từ nhiệm vụ hiện tại. Trước khi g được sử dụng để cập nhật mô hình, A-GEM [12] lấy mẫu một mini-batch (X̃, ỹ) từ bộ nhớ và tính toán gref = ∂L(X̃; ỹ; θ)/∂θ. Nếu g và gref gây nhiễu, tức là nếu gᵀgref < 0, nó cập nhật g với g - (gᵀgref)/(grefᵀgref) gref sao cho các gradient được căn chỉnh. Gradient kết quả được sử dụng để cập nhật mô hình. A-GEM là phiên bản hiệu quả hơn của GEM (Gradient-Episodic Memory) [11], là phương pháp tốt nhất trong [20].

Knowledge Distillation (KD). KD sử dụng cùng mất mát như LWF trong (5), không được tính toán trên mini-batch của nhiệm vụ mới, mà trên mini-batch được lấy mẫu từ bộ nhớ. Lưu ý rằng mất mát này được thêm vào (1), vì vậy nhiệm vụ mới vẫn được học bằng mất mát CE.

IV. THÍ NGHIỆM

Các thí nghiệm được thực hiện trong ESPnet [25]. Để biết thông tin chi tiết và kết quả mở rộng hơn, xem kho lưu trữ của chúng tôi¹.

Dữ liệu. Chúng tôi sử dụng bộ dữ liệu Corpus Gesproken Nederlands (CGN) [26], chứa 900 giờ lời nói tiếng Hà Lan từ cả Hà Lan (NL) và Bỉ (VL). Chúng tôi xem xét tất cả trừ lời nói tự phát hơn và, dựa trên phương ngữ của người nói, chia dữ liệu thành bốn nhiệm vụ: NL-main, VL-main, NL-rest, VL-rest (được học theo thứ tự này). Mỗi nhiệm vụ được chia thêm thành tập huấn luyện, xác thực và kiểm tra.

Huấn luyện. Chúng tôi sử dụng bộ tối ưu hóa từ [21], với tốc độ học 10.0 cho nhiệm vụ đầu tiên và 1.0 cho các nhiệm vụ tiếp theo. Chúng tôi cho phép các mô hình chạy trong 230 epoch, nhưng dừng sớm khi Token Error Rate (TER) ở mức mảnh từ trên tập xác thực của nhiệm vụ mới không cải thiện trong 10 epoch. Như trong [27], chúng tôi lấy trung bình 10 snapshot cuối cùng để có được mô hình cuối cùng.

¹https://github.com/StevenVdEeckt/CGN_CLDialect

Xác định λ. Nhiều phương pháp CL yêu cầu thiết lập siêu tham số λ, trọng số của chính quy hóa. Dựa trên [2], chúng tôi đề xuất một cách đơn giản và hiệu quả để xác định λ cho E2E ASR. Trước tiên, chúng tôi xem xét TERinit, TER (trên tập xác thực của nhiệm vụ mới) của mô hình ban đầu. Tiếp theo, chúng tôi thích ứng mô hình trong năm epoch mà không có chính quy hóa và tính toán TER của nó, thu được TERnoreg. Sau đó, chúng tôi thiết lập λ về một giá trị cao và chạy mô hình trong năm epoch với chính quy hóa có trọng số λ. Chúng tôi tính toán TER và thu được TERλ. Nếu (TERinit - TERλ)/(TERnoreg - TERinit) > a, tức là nếu khoảng cách giữa TERinit và TERnoreg được đóng ít nhất 100a%, chúng tôi trả về λ; ngược lại, chúng tôi thiết lập λ ← λ × p với p ∈ (0,1) và lặp lại quá trình. Như vậy, việc xác định λ được thực hiện một cách nhanh chóng và hiệu quả và không yêu cầu quyền truy cập vào tập xác thực của các nhiệm vụ trước đó. Chúng tôi chỉ xác định λ cho lần thích ứng đầu tiên và sau đó cố định nó. Trong các thí nghiệm của chúng tôi, chúng tôi thiết lập a = 0,85 và p = 0,10. Hơn nữa, đối với mỗi phương pháp, giá trị ban đầu của λ là lũy thừa của 10.

Bộ nhớ. Sau khi học một nhiệm vụ, chúng tôi lấy mẫu 500 câu nói từ tập huấn luyện để thêm vào bộ nhớ. Trong khi lấy mẫu đồng đều, chúng tôi chỉ xem xét các câu nói có độ dài đầu ra (tức là số lượng mảnh từ trong đầu ra) vượt quá 0,40 × meanlength, trong đó meanlength là độ dài đầu ra trung bình của các câu nói trong tập huấn luyện, để đảm bảo rằng tất cả 500 câu nói đều chứa các câu có nghĩa.

Chuẩn. Các chuẩn sau được xem xét: (i) Fine-Tuning (FT): mô hình được thích ứng mà không có phương pháp CL (cận dưới); (ii) Joint (JT): được huấn luyện từ đầu trên tất cả các nhiệm vụ cùng nhau; (iii) Continued Joint (CJT): được thích ứng từ nhiệm vụ trước đó và được huấn luyện trên nhiệm vụ hiện tại và các nhiệm vụ trước đó cùng nhau (cận trên).

Thước đo. Đối với mỗi phương pháp, chúng tôi báo cáo Average WER (AWER), Backward Transfer (BWT) và Forward Transfer (FWT) [11], và Coverage (COV) [13]. Giả sử T nhiệm vụ đã được học và Ri,j là WER trên nhiệm vụ j sau khi học đến nhiệm vụ i, AWER = (1/T) ∑ᵢ₌₁ᵀ RT,i, trong khi BWT là:

BWT = (1/(T-1)) ∑ᵢ₌₁ᵀ⁻¹ (RT,i - Ri,i)  (6)

Lưu ý rằng sử dụng định nghĩa này, BWT âm cho thấy việc quên. Hơn nữa, chúng tôi định nghĩa FWT như:

FWT = (1/(T-1)) ∑ᵢ₌₂ᵀ (Ri,i - Rᶠᵀᵢ,ᵢ)  (7)

trong đó Rᶠᵀᵢ,ⱼ là WER trên nhiệm vụ j sau khi học đến nhiệm vụ i với FT. FWT đo lường mức độ mà mô hình có thể khai thác tri thức đã có trước đó để học các nhiệm vụ mới tốt hơn. FWT dương cho thấy việc học tốt hơn so với FT. Cuối cùng, COV đo lường mức độ mà phương pháp đã cho đóng khoảng cách giữa FT (cận dưới) và CJT (cận trên) về mặt AWER. Nó là 0% khi phương pháp hoạt động kém như FT, và 100% khi phương pháp hoạt động tốt như CJT. Ngoài AWER, BWT, FWT và COV, chúng tôi báo cáo các yêu cầu lưu trữ (Storage), được biểu thị bằng số lượng mô hình tương đương (một mô hình yêu cầu 105 MB).

Ý nghĩa thống kê. Chúng tôi sử dụng kiểm định Wilcoxon signed-rank trên số lượng lỗi trên mỗi câu nói [28] để kiểm tra ý nghĩa thống kê của các kết quả, xem xét các mức ý nghĩa α = 0,05 (★), α = 0,01 (★★) và α = 0,001 (★★★).

V. KẾT QUẢ

Bảng I cho thấy kết quả sau khi học bốn nhiệm vụ theo thứ tự. Trước tiên, chúng tôi lưu ý rằng FT thực sự gặp phải CF, trong khi cả JT và CJT đều có thể học các nhiệm vụ tốt, với CJT đạt được BWT và FWT dương.

Xem xét các phương pháp dựa trên chính quy hóa, chúng tôi thấy rằng các phương pháp ước tính tham số nào quan trọng gặp khó khăn trong việc học bốn nhiệm vụ. Điều này đặc biệt đúng đối với EWC và MAS, cả hai đều hoạt động kém hơn FT. Trong khi CSQN và CSQN-BT, bằng cách xem xét tương tác giữa các tham số, hoạt động tốt hơn một chút, chúng vẫn kém hiệu quả hơn FT. Chúng tôi đưa ra giả thuyết rằng hiệu suất kém của các phương pháp này là do các nhiệm vụ (rất giống nhau) có cùng các tham số quan trọng, điều này cho mô hình hai lựa chọn: hoặc nó cập nhật các tham số này, dẫn đến CF của các nhiệm vụ trước đó; hoặc nó để chúng không thay đổi, dẫn đến việc học kém các nhiệm vụ mới. Trong thí nghiệm này, EWC, MAS và CSQN giảm việc quên nhưng không học tốt các nhiệm vụ mới. Trong khi EWC đạt được BWT tốt nhất trong tất cả các phương pháp, nó cũng đạt được FWT tệ nhất. Lưu ý rằng hiệu suất của EWC phù hợp với [20], cũng thấy EWC kém hiệu quả hơn FT. So với EWC, MAS và CSQN, LWF hoạt động tốt hơn nhiều (và có ý nghĩa thống kê, với α = 0,001), đạt được FWT cao nhất (cao hơn FT). Tuy nhiên, COV của nó chỉ là 12,4%, vì nó chỉ giảm việc quên (BWT) của FT 21%.

So sánh LWF với KD, sử dụng cùng chính quy hóa nhưng được tính toán trên bộ nhớ thay vì trên dữ liệu của nhiệm vụ mới, chúng tôi thấy rằng việc có quyền truy cập vào bộ nhớ, mặc dù nó chỉ là 0,6% dữ liệu ban đầu khi học nhiệm vụ thứ tư, mang lại những cải thiện lớn (với mức ý nghĩa α = 0,001). KD đạt được COV hơn 40% và học các nhiệm vụ mới tốt như FT, trong khi giảm việc quên của FT hơn 70%. Nó vượt trội hơn các phương pháp dựa trên ôn tập khác với biên độ lớn (với mức ý nghĩa α = 0,001). A-GEM, trong khi nó học các nhiệm vụ mới tốt, vẫn gặp phải việc quên nghiêm trọng, đạt COV 22%. Điều này một lần nữa phù hợp với [20], thấy GEM (mà A-GEM là biến thể hiệu quả hơn) vượt trội hơn LWF, trong khi cả hai đều cải thiện hiệu suất của FT. Cuối cùng, ER hoạt động kém hơn FT, vì nó gặp phải CF và không thể học tốt nhiệm vụ mới. Cả BER và đặc biệt là ER(α) đều hoạt động tốt hơn nhiều, đạt COV lần lượt là 16,7% và 27,2%.

Bảng II cho chúng tôi cái nhìn sâu sắc hơn về cách các phương pháp dựa trên ôn tập hoạt động. Nó cho thấy WER trên bộ nhớ và tập kiểm tra của nhiệm vụ ban đầu NL-main của các mô hình được thích ứng với VL-main. Đối với ER, chúng tôi lưu ý rằng nó ghi nhớ hoàn toàn bộ nhớ, đạt WER 0,0, và điều này khái quát rất kém cho tập kiểm tra. ER(α) giảm nhẹ điều này, mặc dù nó vẫn ghi nhớ gần như hoàn hảo bộ nhớ. A-GEM cũng có WER rất thấp trên bộ nhớ. Mặt khác, KD chỉ cải thiện rất ít trên bộ nhớ, nhưng nó có thể trích xuất tri thức 'tổng quát' hơn nhiều từ nó, giới hạn việc quên trên tập kiểm tra tốt hơn nhiều so với ER, ER(α) hoặc A-GEM.

Hình 1 cho thấy COV sau khi học mỗi nhiệm vụ. Chúng tôi thấy rằng sau hai nhiệm vụ, LWF hoạt động tốt như A-GEM và ER(α). Tuy nhiên, khi thêm nhiều nhiệm vụ, khoảng cách giữa LWF và A-GEM và ER(α) mở rộng. Hơn nữa, trong khi BER sau hai nhiệm vụ chỉ vượt trội hơn FT một chút, hiệu suất của nó, so với FT và các phương pháp CL khác, cải thiện khi thêm nhiều nhiệm vụ. Điều này như mong đợi, vì BER, học trên sự hợp nhất của tập huấn luyện và bộ nhớ, rõ ràng được hưởng lợi từ việc có bộ nhớ lớn hơn. Cuối cùng, lưu ý cách hiệu suất của nhiều phương pháp CL giảm khi học NL-rest. Đó là bởi vì NL-main và NL-rest rất giống nhau, vì vậy các phương pháp CL nên cho phép mô hình học NL-rest, vì điều này cũng sẽ có lợi cho NL-main, trong khi bảo vệ VL-main. Trong khi đây là tình huống thực tế khi mở rộng mô hình ASR đơn ngữ, nó hóa ra là tình huống rất thách thức, đặc biệt đối với EWC, MAS và CSQN, bằng cách bảo vệ các tham số quan trọng của NL-main, không thể khai thác NL-rest để cải thiện thêm các tham số này.

A. Bộ Nhớ Tăng Dần vs Cố Định

Các phương pháp dựa trên ôn tập từ Bảng I có quyền truy cập vào bộ nhớ với 500 câu nói mỗi nhiệm vụ. Trong thực tế, có thể mong muốn và/hoặc khả thi hơn khi có bộ nhớ với kích thước cố định, đặc biệt khi số lượng nhiệm vụ trở nên lớn. Để làm điều này, chúng tôi cố định kích thước bộ nhớ ở 500. Bảng III cho thấy kết quả đối với A-GEM, ER(α) và KD.

Chúng tôi thấy rằng sự khác biệt với Bảng I là nhỏ. Đối với A-GEM và KD, chúng tôi quan sát thấy sự suy giảm nhẹ (mặc dù có ý nghĩa thống kê, với α = 0,05) của AWER. Mặt khác, đối với ER(α), thậm chí còn có sự cải thiện nhỏ, không có ý nghĩa thống kê, được cho là do tình cờ. Ngay cả với bộ nhớ rất nhỏ và cố định (chỉ với 0,2% dữ liệu huấn luyện ban đầu sau bốn nhiệm vụ), A-GEM và đặc biệt là ER(α) và KD do đó rất hiệu quả trong việc cho phép CL.

B. Yêu Cầu Lưu Trữ

Bảng I cho thấy yêu cầu lưu trữ của các phương pháp CL để học nhiệm vụ thứ tư. Đối với tất cả các phương pháp trừ JT, cái mà đối với mỗi nhiệm vụ mới bắt đầu từ đầu, điều này yêu cầu, trước hết, việc lưu trữ chính mô hình. Ngoài ra, các phương pháp dựa trên ôn tập yêu cầu lưu trữ tương đương 2,24 mô hình, vì chúng cần lưu trữ các câu nói trong bộ nhớ.

So với các phương pháp dựa trên ôn tập, các phương pháp dựa trên chính quy hóa hiệu quả hơn về lưu trữ (ngoài việc không yêu cầu lưu trữ dữ liệu từ các nhiệm vụ trước đó trong bộ nhớ, điều này có thể, do các mối quan tâm về quyền riêng tư, không luôn được phép). Trong khi LWF chỉ yêu cầu lưu trữ mô hình trước đó, EWC và MAS, ngoài ra, cần lưu trữ các trọng số tầm quan trọng. So với các phương pháp sau, CSQN và CSQN-BT kém hiệu quả hơn về lưu trữ, do các xấp xỉ Hessian.

Với bộ nhớ tăng dần, như trong Bảng I, yêu cầu lưu trữ của các phương pháp dựa trên ôn tập cũng tăng tuyến tính với số lượng nhiệm vụ. Tuy nhiên, như chúng tôi đã thấy trong Bảng III, điều này có thể được khắc phục bằng cách cố định kích thước bộ nhớ, với chỉ sự suy giảm không đáng kể về hiệu suất, cho phép A-GEM và đặc biệt là ER(α) và KD đạt được hiệu suất xuất sắc trong khi rất hiệu quả về lưu trữ, yêu cầu lưu trữ tương đương chỉ 1,72 mô hình (độc lập với số lượng nhiệm vụ). Cuối cùng, lưu ý cách JT và CJT, cần quyền truy cập vào tất cả dữ liệu mà mô hình từng được huấn luyện, yêu cầu lưu trữ tương đương lần lượt là 260,54 và 261,54 mô hình, khiến chúng rõ ràng không phải là giải pháp thực tế để khắc phục CF.

VI. KẾT LUẬN

Trong bài báo này, chúng tôi đã triển khai một số lượng lớn các phương pháp CL và kiểm tra và so sánh khả năng mở rộng mô hình E2E ASR đơn ngữ qua bốn nhiệm vụ. Việc có quyền truy cập vào bộ nhớ, mặc dù rất nhỏ so với tập huấn luyện ban đầu, đã chứng minh là rất có lợi, vì các phương pháp dựa trên ôn tập thường hoạt động tốt hơn nhiều so với các phương pháp dựa trên chính quy hóa. Để đảm bảo yêu cầu lưu trữ của các phương pháp trước không tăng theo số lượng nhiệm vụ, kích thước bộ nhớ có thể được cố định với chỉ sự suy giảm không đáng kể về hiệu suất. Nói chung, do đó, các phương pháp dựa trên ôn tập dường như là cách tốt nhất và thực tế nhất để hiện tại khắc phục CF trong các mô hình E2E ASR đơn ngữ; đặc biệt là KD, đóng khoảng cách giữa mô hình Fine-Tuned (cận dưới) và mô hình Continued Joint (cận trên) lần lượt là 41,7% và 38,3% trong khi chỉ có quyền truy cập vào 0,6% và 0,2% dữ liệu ban đầu. Trong trường hợp không được phép lưu trữ các câu nói từ các nhiệm vụ trước đó, LWF dường như là lựa chọn tốt nhất, vì các phương pháp dựa trên chính quy hóa khác, có yêu cầu lưu trữ cao hơn, không thể cải thiện cận dưới Fine-Tuning. Tuy nhiên, ngay cả trong trường hợp chỉ có thể lưu trữ một số lượng rất nhỏ câu nói mỗi nhiệm vụ, vẫn nên làm như vậy.
