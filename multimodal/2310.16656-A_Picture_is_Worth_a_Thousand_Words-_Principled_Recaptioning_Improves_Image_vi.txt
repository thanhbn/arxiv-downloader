# 2310.16656.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.16656.pdf
# Kích thước tệp: 8013067 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Hình Ảnh Đáng Giá Ngàn Lời: Tái Tạo Chú Thích Có Nguyên Tắc Cải Thiện Việc Tạo Hình Ảnh
Eyal Segalis
Google Research
eyalis@google.comDani Valevski
Google Research
daniv@google.comDanny Lumen
Google Research
dwasserman@google.comYossi Matias
Google Research
yossi@google.com
Yaniv Leviathan
Google Research
leviathan@google.com
Hình 1. Các ví dụ sinh ra không chọn lọc từ mô hình Stable Diffusion cơ bản (trái) và mô hình của chúng tôi được huấn luyện trên tập dữ liệu được tái tạo chú thích (phải), trên cùng một tập hạt giống ngẫu nhiên.
1arXiv:2310.16656v1  [cs.CV]  25 Oct 2023

--- TRANG 2 ---
1. Tóm tắt
Các mô hình khuếch tán văn bản-thành-hình ảnh đã đạt được một bước tiến đáng kể về khả năng trong vài năm qua, cho phép tổng hợp chất lượng cao và đa dạng các hình ảnh từ một lời nhắc văn bản. Tuy nhiên, ngay cả các mô hình tiên tiến nhất cũng thường gặp khó khăn trong việc tuân theo chính xác tất cả các hướng dẫn trong lời nhắc của họ. Phần lớn các mô hình này được huấn luyện trên các tập dữ liệu bao gồm các cặp (hình ảnh, chú thích) trong đó các hình ảnh thường đến từ web, và các chú thích là văn bản thay thế HTML của chúng. Một ví dụ đáng chú ý là tập dữ liệu LAION, được sử dụng bởi Stable Diffusion và các mô hình khác. Trong công trình này, chúng tôi quan sát thấy rằng những chú thích này thường có chất lượng thấp, và lập luận rằng điều này ảnh hưởng đáng kể đến khả năng của mô hình trong việc hiểu các ngữ nghĩa tinh tế trong các lời nhắc văn bản. Chúng tôi chỉ ra rằng bằng cách gắn lại nhãn kho ngữ liệu với một mô hình tạo chú thích tự động chuyên biệt và huấn luyện một mô hình văn bản-thành-hình ảnh trên tập dữ liệu được tái tạo chú thích, mô hình được cải thiện đáng kể trên toàn bộ. Đầu tiên, về chất lượng hình ảnh tổng thể: ví dụ FID 14.84 so với đường cơ sở 17.87, và cải thiện 64.3% trong việc tạo hình ảnh trung thực theo đánh giá con người. Thứ hai, về căn chỉnh ngữ nghĩa, ví dụ độ chính xác đối tượng ngữ nghĩa 84.34 so với 78.90, lỗi căn chỉnh đếm 1.32 so với 1.44 và căn chỉnh vị trí 62.42 so với 57.60. Chúng tôi phân tích các cách khác nhau để gắn lại nhãn kho ngữ liệu và cung cấp bằng chứng rằng kỹ thuật này, mà chúng tôi gọi là RECAP, vừa giảm sự khác biệt giữa huấn luyện và suy luận vừa cung cấp cho mô hình nhiều thông tin hơn mỗi ví dụ, tăng hiệu quả mẫu và cho phép mô hình hiểu rõ hơn mối quan hệ giữa chú thích và hình ảnh.

2. Giới thiệu
Trong những năm gần đây, các mô hình tạo văn bản-thành-hình ảnh (T2I) như Imagen [1], Muse [2], Dall-E [3], Dall-E 2 [4], Parti [5], và Stable Diffusion [6] đã trải qua những tiến bộ đáng kể. Sự tiến bộ này đã cho phép tạo ra các hình ảnh chất lượng cao và đa dạng một cách đáng kể bằng cách điều kiện hóa trên các đầu vào văn bản. Tuy nhiên, dù mang tính cách mạng, ngay cả các mô hình văn bản-thành-hình ảnh hiện đại tối tân cũng có thể thất bại trong việc tạo ra hình ảnh truyền tải đầy đủ ngữ nghĩa và sắc thái từ các lời nhắc văn bản đã cho. Các chế độ lỗi bao gồm: thiếu một hoặc nhiều chủ thể từ lời nhắc đầu vào [5,7]; ràng buộc không chính xác các thực thể và bộ điều chỉnh [5,7,8]; và vị trí không chính xác và cấu trúc không gian của các thực thể [5, 9, 10].

Trong công trình này, trước tiên chúng tôi quan sát thấy rằng các tập dữ liệu web mở được sử dụng để huấn luyện các mô hình văn bản-thành-hình ảnh mở gặp phải các vấn đề đáng kể. Ví dụ, các chú thích trong tập dữ liệu LAION [11], được sử dụng để huấn luyện Stable Diffusion, đến từ các thẻ alt HTML (Alttext). Theo hướng dẫn khả năng tiếp cận nội dung web của W3C¹, thuộc tính alt được sử dụng để truyền tải ý nghĩa và ý định của hình ảnh, và không nhất thiết phải là mô tả theo nghĩa đen của chính hình ảnh đó. Thực sự, chúng tôi quan sát thấy rằng thường Alttext chỉ mô tả một khía cạnh hẹp của hình ảnh, bỏ qua các chi tiết thị giác quan trọng. Ví dụ, một hình ảnh của một người có thể có Alttext là tên của người đó và tên của nhiếp ảnh gia, nhưng không có mô tả về ngoại hình, quần áo, vị trí của họ, hoặc bối cảnh. Ngoài ra, đôi khi các thẻ Alttext chứa các thông tin không chính xác, sai lầm và ngoài ngữ cảnh. Xem Hình 4 để có ví dụ.

Chúng tôi tiếp tục quan sát thấy rằng trong khi được huấn luyện chủ yếu trên các tập dữ liệu tương tự của các cặp (hình ảnh, chú thích) mở, các hệ thống tạo chú thích tự động gần đây, như PaLI [12], tạo ra các chú thích có độ chính xác cao. Xem ví dụ trong Hình 3. Điều này có thể do thực tế là bài toán nghịch đảo của hình ảnh-thành-văn bản (I2T) dễ dàng hơn, hoặc do thực tế là những mô hình tạo chú thích này lớn hơn, được huấn luyện lâu hơn các mô hình T2I, hoặc tận dụng các thành phần ngôn ngữ được huấn luyện trước lớn.

Với những quan sát này, chúng tôi đề xuất một phương pháp mới để cải thiện theo chiều ngang các mô hình T2I bằng cách huấn luyện chúng trên các chú thích được cải thiện, được tự động tạo ra bởi một mô hình I2T tùy chỉnh. Chúng tôi gọi phương pháp của mình là RECAP, và chỉ ra rằng việc áp dụng nó cho Stable Diffusion dẫn đến một mô hình tốt hơn đường cơ sở trên toàn bộ, với một loạt các chỉ số tiêu chuẩn được cải thiện đáng kể, ví dụ FID 17.87→14.84, cũng như trong đánh giá con người về việc tạo hình ảnh thành công 29.25%→48.06% (xem Phần 5).

Trong Phần 4, chúng tôi cung cấp chi tiết về phương pháp của mình, RECAP. Phần 5 thảo luận về kết quả của chúng tôi và chỉ ra các cải thiện theo chiều ngang cả về chất lượng hình ảnh cũng như độ trung thực ngữ nghĩa với các lời nhắc. Trong Phần 6, chúng tôi phân tích các vấn đề với các chú thích gốc, chứng minh rằng các cải thiện thực sự là do các chú thích mới, và chúng phát sinh do vừa giảm thiểu sự sai lệch huấn luyện-kiểm tra vừa tăng hiệu quả mẫu.

Tóm lại, các đóng góp chính của chúng tôi là:
• Một phương pháp mới mà chúng tôi gọi là RECAP, tận dụng tạo chú thích tự động để cải thiện chất lượng của mô hình văn bản-thành-hình ảnh một cách đáng kể theo chiều ngang, cả về độ trung thực và ngữ nghĩa, được đo lường trên một tập 7 chỉ số tiêu chuẩn cũng như với các đánh giá con người.
• Một phân tích chỉ ra cách các chú thích Alttext được sử dụng bởi các phương pháp huấn luyện hiện tại bị sai lệch huấn luyện-suy luận và thiếu chi tiết ngữ nghĩa, dẫn đến các mô hình văn bản-thành-hình ảnh thường thất bại trong độ trung thực và căn chỉnh ngữ nghĩa, và cách các chú thích khác nhau giảm thiểu cả hai vấn đề.

3. Công trình liên quan
Mô hình Văn bản-thành-Hình ảnh. Các mô hình sinh sâu để tạo hình ảnh từ văn bản đã cho thấy tiến bộ đáng chú ý trong những năm gần đây, chuyển từ sử dụng các phương pháp dựa trên GAN [13] sang sử dụng các transformer tự hồi quy [2,3,5] và các mô hình khuếch tán [1,4,14]. Một lĩnh vực cải tiến quan trọng là trong việc cải thiện khả năng của mô hình để căn chỉnh với văn bản đầu vào một cách hiệu quả. Các phương pháp điều kiện hóa hình ảnh trên đầu ra

--- TRANG 3 ---
1a. Tạo chú thích 
thủ công cho 
một mẫu
Mô hình Tạo Chú thích 
Hình ảnh Chuyên biệt 1b. Tinh chỉnh 
Mô hình Tạo Chú thích 2. Tạo 
chú thích
RECAP Mô hình 
Tạo Hình ảnh 3. Huấn luyện Mô hình 
Tạo Hình ảnh

Hình 2. Sơ đồ phương pháp RECAP của chúng tôi. Trong các bước (1a) và (1b), chúng tôi tinh chỉnh một mô hình tạo chú thích hình ảnh-thành-văn bản trên một tập nhỏ các chú thích chi tiết của con người. Trong bước (2), chúng tôi sử dụng mô hình đã tinh chỉnh này để tái tạo chú thích cho các hình ảnh trong tập dữ liệu huấn luyện của một mô hình văn bản-thành-hình ảnh, và với tập dữ liệu này, trong bước (3) chúng tôi huấn luyện một mô hình tạo hình ảnh với tập dữ liệu được tái tạo chú thích.

của một bộ nhúng văn bản được huấn luyện trước, thường là CLIP [15]. Imagen [1] chỉ ra rằng việc sử dụng một bộ mã hóa T5 [16] mạnh cải thiện đáng kể việc căn chỉnh văn bản-hình ảnh. Parti [5] sử dụng một mô hình SimVLM [17] để chú thích một số hình ảnh trong tập huấn luyện.

Tạo chú thích hình ảnh. Tạo chú thích hình ảnh là một vấn đề cơ bản trong đồ họa máy tính. Các mô hình gần đây [12,18-21] đã đạt được tiến bộ đáng kể trong nhiệm vụ này, nhờ vào dữ liệu huấn luyện quy mô lớn và việc sử dụng các mô hình hình ảnh và văn bản được huấn luyện trước, cho phép chúng giải quyết nhiều nhiệm vụ đa phương thức khác nhau. Trong công trình này, chúng tôi tinh chỉnh PaLI [12] để tái tạo chú thích cho tập dữ liệu huấn luyện của một mô hình văn bản-thành-hình ảnh.

Tập dữ liệu đa phương thức tổng hợp. Một số công trình đồng thời tăng cường tập dữ liệu đa phương thức thông qua các phương tiện tự động để cải thiện khả năng của các mô hình tạo chú thích hình ảnh và nhúng. Nguyen et al. [22] chỉ ra rằng việc huấn luyện CLIP trên dữ liệu với các chú thích được tạo ra cải thiện hiệu suất của nó. Li et al. [23] chỉ ra cách cải thiện mô hình BLIP bằng cách lặp lại loại bỏ các chú thích với mất mát cao, thay thế chúng bằng các chú thích từ epoch trước hoặc tạo một hình ảnh mới cho chúng bằng Stable Diffusion. Ma et al. [24] tạo ra một mô hình văn bản-thành-hình ảnh tổng hợp bằng Stable Diffusion và sau đó sử dụng nó để huấn luyện một thuật toán tạo chú thích.

Cải thiện căn chỉnh văn bản của các mô hình khuếch tán. Một hướng công trình quan trọng cố gắng khắc phục các vấn đề căn chỉnh văn bản-thành-hình ảnh trong các mô hình khuếch tán. Điều này thường được thực hiện bằng phân tích từ vựng của lời nhắc đầu vào và sửa đổi các bản đồ chú ý trong suốt quá trình lấy mẫu. Chefer et al. [7] cố gắng ngăn chặn các mô hình khuếch tán bỏ qua các token nhất định bằng cách hướng dẫn quá trình lấy mẫu để tái cân bằng các bản đồ chú ý token. Rassin et al. [8] phân tích lời nhắc để xác định các bộ điều chỉnh, và sau đó sử dụng hướng dẫn thời gian lấy mẫu để ràng buộc các bản đồ chú ý của chúng với những bản đồ của các thực thể. Wu et al. [9] cô lập các cụm từ mô tả các thực thể riêng lẻ và chú ý đến chúng. Phung et al. [10] cho phép đặt các đối tượng trong vùng nhất định bằng cách tăng cường sự chú ý của chúng trong những vùng đó.

Đồng thời với công trình của chúng tôi, Dall-E 3 [25] đề xuất sử dụng một hệ thống tạo chú thích tự động để tái tạo các chú thích được sử dụng để huấn luyện một mô hình T2I. Công trình của chúng tôi sử dụng một mô hình mở (Stable Diffusion) và chúng tôi cung cấp nhiều chi tiết hơn và tập trung nhiều hơn vào phân tích và đánh giá, nhưng ngoài ra các ý tưởng chính rất tương tự.

4. Phương pháp
Phương pháp của chúng tôi, RECAP bao gồm 3 bước: (1) tinh chỉnh một hệ thống tạo chú thích tự động để tạo ra các nhãn mong muốn; (2) gắn lại nhãn cho các hình ảnh từ tập huấn luyện văn bản-thành-hình ảnh của chúng tôi với hệ thống tái tạo chú thích tự động này; và (3) huấn luyện mô hình văn bản-thành-hình ảnh trên tập dữ liệu bao gồm các hình ảnh và chú thích mới. Hình 2 trực quan hóa phương pháp tổng thể.

4.1. Tập dữ liệu huấn luyện
Chúng tôi đã chọn một tập con 10 triệu ảnh từ tập dữ liệu LAION-2B-en improved Aesthetics. Chúng tôi đã tuân theo các bộ lọc dữ liệu được sử dụng để huấn luyện các phiên bản Stable Diffusion 1.2-1.4², như sau: điểm thẩm mỹ ≥5.0, pwatermark <0.5, nsfw isUNLIKELY và cả chiều cao & chiều rộng ≥512. Chúng tôi lưu ý rằng hoạt động lọc này có thể khuếch đại các thiên lệch trong tập dữ liệu [26]. Chúng tôi tiếp tục loại trừ một tập con tùy ý 10K ảnh từ tập huấn luyện để được sử dụng làm tập xác thực nội bộ.

4.2. Mô hình tạo chú thích
Chúng tôi đã sử dụng một mô hình tạo chú thích I2T được huấn luyện trước (PaLI [12]). Vì các đầu ra của mô hình tương đối ngắn gọn và thiếu chi tiết, trước tiên chúng tôi đã thu thập một tập nhỏ 100 chú thích thủ công từ các người đánh giá con người và tinh chỉnh mô hình tạo chú thích trên tập đó.

Để thử nghiệm với hiệu ứng của các phân phối tạo chú thích khác nhau, các người đánh giá được yêu cầu cung cấp hai loại

²https://huggingface.co/CompVis/stable-diffusion

--- TRANG 4 ---
chú thích. Đầu tiên, một chú thích chi tiết cho mỗi hình ảnh, với các hướng dẫn này: "Mô tả những gì bạn thấy trong mỗi hình ảnh bằng 1-2 câu chi tiết". Lưu ý rằng các hướng dẫn giới hạn độ dài chỉ tối đa 1-2 câu chi tiết, do kích thước ngữ cảnh của CLIP (bộ mã hóa văn bản downstream) chỉ có 77 token, mà các chú thích dài hơn vượt quá. Thứ hai, chúng tôi đã thu thập một chú thích ngắn và ít chi tiết hơn cho mỗi hình ảnh với hướng dẫn này: "Mô tả những gì bạn thấy trong mỗi hình ảnh bằng một câu ngắn đơn". Chúng tôi không lặp lại về chất lượng của những chú thích thủ công này. Hình 3 cung cấp các chú thích ví dụ được đưa ra bởi các người đánh giá con người, cũng như những chú thích được tạo ra bởi mô hình PaLI không tinh chỉnh.

Với tập dữ liệu nhỏ này, chúng tôi đã tinh chỉnh PaLI trong 300 bước, sử dụng tốc độ học 4e-5, tỷ lệ dropout 0.1 và kích thước batch 64, trộn 50% chú thích ngắn và 50% chú thích dài cho nhiều bản sao của 100 hình ảnh, sử dụng một tiền tố điều kiện cố định khác nhau cho chú thích ngắn so với chú thích dài. Khi tạo chú thích từ mô hình đã tinh chỉnh, chúng tôi sử dụng các thuật ngữ RECAP Short và RECAP Long để chỉ các thế hệ được điều kiện trên các tiền tố ngắn và dài tương ứng.

Các đầu ra ví dụ, cũng như so sánh với các chú thích Alttext gốc có thể được tìm thấy trong Hình 4. Xem Phụ lục C để có thêm hàng chục ví dụ ngẫu nhiên về các chú thích RECAP được tạo ra.

Nhìn chung, các chú thích được tạo ra bởi mô hình tùy chỉnh này cải thiện cả hai vấn đề trên - phân phối của chúng phù hợp hơn với các lời nhắc thời gian suy luận và chúng chứa nhiều chi tiết hơn để cải thiện hiệu quả mẫu (xem Phần 6.1.1).

4.3. Mô hình tạo hình ảnh
Tiếp theo, chúng tôi đã tinh chỉnh Stable Diffusion v1.4 thêm 250k³ bước với tốc độ học 1e-5, kích thước batch 512, và tỷ lệ dropout lời nhắc 0.1⁴. Chúng tôi đã tinh chỉnh mô hình huấn luyện cả trọng số UNet và CLIP và sử dụng hỗn hợp 50%-50% của chú thích RECAP Short và RECAP Long (RECAP Mix) vì điều này hoạt động tốt nhất. Các kết quả trong văn bản chính đều từ cấu hình này.

RECAP độc lập với phương pháp lấy mẫu, vì vậy chúng tôi có thể sử dụng bất kỳ phương pháp lấy mẫu nào với nó. Điều đó nói, đối với tất cả các thí nghiệm trong công trình này, chúng tôi đã sử dụng lấy mẫu DDIM với 50 bước suy luận và thang đo hướng dẫn 7.5.

5. Kết quả
Chúng tôi so sánh mô hình RECAP với hai mô hình: Baseline và Alttext. Baseline là mô hình Stable Diffusion v1.4. Alttext là mô hình baseline được tinh chỉnh cho cùng số bước và trên cùng tập hình ảnh như RECAP, nhưng với các chú thích gốc (Alttext) thay vì các chú thích RECAP.

³Khi tinh chỉnh thêm mô hình trong 1M bước, chúng tôi quan sát thấy kết quả hình ảnh tốt hơn cùng với lợi ích giảm dần trong các chỉ số tự động tính toán. Tất cả kết quả trong bài báo là cho 250K bước ngoại trừ Hình 1 và đánh giá con người, nơi chúng tôi đã sử dụng một mô hình được tinh chỉnh trong 1M bước.

⁴Các lời nhắc với hơn 77 token (giới hạn trên của CLIP) đã bị loại bỏ. Điều này xảy ra cho <1% dữ liệu trong mỗi tập huấn luyện.

Mô hình Alttext giải quyết mối quan tâm về contamination, vì nó bao gồm chính xác cùng tập hình ảnh. Trong tất cả các so sánh, chúng tôi đã sử dụng cùng các hạt giống ngẫu nhiên trên các mô hình.

Chúng tôi so sánh các mô hình bằng cách sử dụng nhiều chỉ số tự động, đánh giá con người và đánh giá định tính các ví dụ. Chúng tôi quan sát thấy cải thiện trong tất cả các chỉ số (xem Bảng 1 và 2).

5.1. Chỉ số tự động
Chúng tôi đã đánh giá hiệu suất và khả năng ngữ nghĩa của mô hình RECAP bằng cách sử dụng một loạt chỉ số được đề xuất bởi [27] (sử dụng mã công khai của họ) trên tập xác thực MS-COCO. Bảng 1 chứa tóm tắt các kết quả.

Để đánh giá chất lượng tạo tổng thể, chúng tôi sử dụng chỉ số FID tiêu chuẩn và quan sát thấy rằng các hình ảnh được tạo ra với mô hình RECAP có điểm số tốt hơn đáng kể (17.87→14.84).

Ngoài ra, chúng tôi đánh giá khả năng ngữ nghĩa của mô hình: để kiểm tra rằng mô hình tạo ra một cách trung thực các đối tượng được yêu cầu, chúng tôi sử dụng Semantic Object Accuracy (80.80→86.17), để kiểm tra số lượng đối tượng được tạo ra, chúng tôi sử dụng Counting Alignment errors (1.44→1.32), để kiểm tra rằng vị trí của các đối tượng là chính xác, chúng tôi sử dụng Positional Alignment (57.60→62.42), và cuối cùng để kiểm tra sự tuân thủ tổng thể với lời nhắc, chúng tôi đo lường điểm Clip (92.78→93.80).

Trong tất cả các chỉ số, chúng tôi không thấy cải thiện nào trong mô hình Alttext so với baseline, chứng minh rằng các cải thiện bắt nguồn từ chính các chú thích và không phải từ việc huấn luyện bổ sung.

Lưu ý rằng theo [27], chúng tôi bỏ qua các chỉ số IS và O-IS, vì tất cả các mô hình Stable Diffusion cho điểm số cao hơn hình ảnh thực. Như được lưu ý bởi [27], IS phù hợp hơn cho các tập dữ liệu có một đối tượng duy nhất và không hoạt động tốt cho tập dữ liệu MS-COCO, chứa nhiều đối tượng mỗi hình ảnh.

Chi tiết bổ sung về các chỉ số tự động có trong Phụ lục A.1 đến A.4.

5.2. Đánh giá con người
Để đánh giá bổ sung về hiệu suất mô hình, chúng tôi đã sử dụng các người đánh giá con người. Kết quả được tóm tắt trong Bảng 2.

Các người đánh giá được yêu cầu chọn các hình ảnh được tạo ra từ mỗi mô hình, chỉ khi chúng thành công tuân theo một lời nhắc đã cho. Chúng tôi đánh giá một lần trên 200 lời nhắc ngẫu nhiên từ tập xác thực MS-COCO, và riêng biệt trên tập dữ liệu DrawBench [1] đầy thách thức. Chúng tôi trình bày bốn hình ảnh (từ các hạt giống khác nhau) cho mỗi lời nhắc, sử dụng cùng các hạt giống trên các mô hình.

Chúng tôi tính toán hai chỉ số: tỷ lệ phần trăm tạo hình ảnh thành công trên tất cả các lời nhắc và hạt giống (tức là với một lời nhắc và một hạt giống, cơ hội tạo hình ảnh thành công); và tỷ lệ phần trăm ít nhất một lần tạo hình ảnh thành công cho một lời nhắc đã cho, trong bốn hạt giống (tức là với một lời nhắc, cơ hội tạo hình ảnh thành công cho nó, trong bốn lần thử). Chúng tôi thấy cải thiện tương đối 64.3% trong tạo hình ảnh thành công trên MS-COCO, và cải thiện 41.7% trên DrawBench. Chúng tôi cũng thấy cải thiện tương đối 42.1% trong tạo lời nhắc thành công trên MS-COCO và cải thiện 37.5% trên DrawBench. Mô hình Alttext cho thấy cải thiện nhỏ trên tập dữ liệu MS-COCO (12%-13%) và không cải thiện tập dữ liệu DrawBench. Chi tiết thêm có thể được tìm thấy trong Phụ lục A.5.

5.3. Kết quả định tính
Hình 1 cung cấp các ví dụ đại diện trong đó RECAP vượt trội hơn mô hình Stable Diffusion cơ bản (và đôi khi cả các mô hình lớn hơn, xem Phụ lục B).

Nhìn chung, chúng tôi quan sát thấy rằng RECAP có thể diễn giải tốt hơn các mối quan hệ giữa các thực thể. Các giới từ như "near", "through", "made off" thường bị mô hình Stable Diffusion cơ bản bỏ qua. Mô hình RECAP áp dụng các giới từ một cách chính xác, trong khi mô hình cơ bản thường dựa vào mối quan hệ phổ biến nhất giữa các thực thể (dựa trên phân phối dữ liệu huấn luyện). Ví dụ, trong lời nhắc "A pizza near a pineapple", mô hình cơ bản đặt dứa trên pizza, như mối quan hệ có thể xảy ra giữa hai thứ, trong khi RECAP tạo ra một quả dứa gần nó, như được yêu cầu.

RECAP cũng xử lý tốt hơn các trường hợp mà các bộ điều chỉnh khác nhau được áp dụng cho nhiều thực thể (ví dụ "A red bench and a yellow clock"). Mô hình cơ bản sẽ coi câu như một túi từ, áp dụng tất cả các bộ điều chỉnh cho tất cả các thực thể hoặc bỏ qua một số trong số chúng. RECAP cũng có thể diễn giải các bộ điều chỉnh phức tạp như đại từ. Ví dụ, trong lời nhắc "Two flowers, one is blue and the other one is green", nó hiểu rằng "one" đề cập đến một bông hoa.

6. Phân tích
Chúng tôi đưa ra giả thuyết rằng việc cải thiện cơ bản trong chất lượng tạo hình ảnh, như được đo lường trong các kết quả trên, bắt nguồn từ hai cải thiện trong các chú thích huấn luyện: (1) giảm sự khác biệt giữa các lời nhắc huấn luyện và suy luận, và (2) cung cấp cho mô hình nhiều thông tin văn bản hơn mỗi hình ảnh, do đó cải thiện hiệu quả mẫu huấn luyện. Dưới đây chúng tôi cung cấp phân tích ablation cho thấy các chú thích RECAP đạt được cả hai tính chất, và mô hình kết quả được lợi từ cả hai.

6.1. So sánh các loại chú thích khác nhau
6.1.1 Chú thích được tạo ra
Bảng 3 so sánh các chú thích được tạo ra trong tập huấn luyện, với chú thích trong tập xác thực MS-COCO, trên nhiều

--- TRANG 5 ---
Hình 3. Các ví dụ về chú thích được đưa ra bởi các người đánh giá con người, và chú thích được tạo tự động từ mô hình PaLI không tinh chỉnh. Ảnh được lấy từ LAION.

chỉ số ngôn ngữ. Chúng tôi đã sử dụng MS-COCO để đại diện cho các lời nhắc mà chúng tôi mong đợi sẽ thấy trong thời gian suy luận. Chúng tôi quan sát thấy rằng các chú thích được tạo ra của chúng tôi gần hơn trong phân phối với MS-COCO theo nhiều nghĩa.

Đầu tiên, chúng tôi so sánh chúng bằng cách sử dụng các chỉ số ngôn ngữ tiêu chuẩn từ gói textstat python⁵ được tính toán trên 10 triệu ví dụ trong mỗi tập dữ liệu huấn luyện. Điểm Flesch Reading Ease là một thước đo tiêu chuẩn cho việc đọc một văn bản trong một ngôn ngữ nhất định khó khăn như thế nào. Chúng tôi quan sát thấy rằng RECAP có thể tạo ra các câu dễ đọc, trong khi Alttext gốc thường khó đọc. Tương tự, điểm text_standard là một điểm số đồng thuận dựa trên một loạt chỉ số để ước tính lớp nhỏ nhất của một người đọc nói tiếng mẹ đẻ để có thể đọc văn bản. RECAP tạo ra các văn bản mà học sinh lớp 4 (thấp nhất có thể) có thể đọc, trong khi Alttext gốc được ước tính yêu cầu học sinh lớp 8 để hiểu đầy đủ.

Để đo lường trực tiếp hơn việc giảm sai lệch huấn luyện-suy luận, chúng tôi so sánh phân phối của các văn bản được nhúng. Vì Stable Diffusion đang sử dụng CLIP để mã hóa văn bản, chúng tôi tính toán khoảng cách Fréchet giữa các embedding CLIP của các tập dữ liệu khác nhau⁶. Kết quả được tóm tắt trong Bảng 3. Như nghi ngờ, các chú thích được tạo ra bởi RECAP gần hơn trong phân phối với các chú thích MS-COCO so với các chú thích Alttext. Hơn nữa, các chú thích RECAP Short gần hơn với MS-COCO so với các chú thích RECAP Long. Những kết quả này phù hợp với việc cải thiện FID của các hình ảnh từ các mô hình được huấn luyện với các tập dữ liệu tương ứng, như được nêu chi tiết trong Phần 6.1.2.

Tiếp theo, chúng tôi đo lường mức độ mô tả tốt của các chú thích được tạo tự động đối với các hình ảnh, bằng cách sử dụng đánh giá con người của 100 hình ảnh ngẫu nhiên, yêu cầu các người đánh giá chấm điểm mỗi chú thích xem xét cả độ trung thực với hình ảnh, và tính đầy đủ trong việc mô tả hình ảnh, trên thang điểm 1-5. Các chú thích được tạo ra bởi RECAP được đánh giá là trung thực và đầy đủ hơn (với điểm số trung bình 3.58 cho RECAP Short và 4.3 cho RECAP Long) so với các chú thích Alttext (chấm điểm trung bình 2.9). Điều này hỗ trợ giả thuyết của chúng tôi rằng các chú thích tự động làm cho việc huấn luyện hiệu quả hơn bằng cách cung cấp nhiều thông tin văn bản hơn.

6.1.2 Hình ảnh được tạo ra
Tiếp theo, chúng tôi so sánh kết quả của việc tinh chỉnh Stable Diffusion trên hai loại chú thích của chúng tôi, RECAP Long và RECAP Short. Kết quả được tóm tắt trong Bảng 4. Chúng tôi quan sát thấy rằng việc huấn luyện trên các chú thích RECAP Short đạt được điểm số FID tốt hơn và nhanh hơn, nhưng với ít cải thiện ngữ nghĩa, trong khi các chú thích RECAP Long thể hiện cải thiện ngữ nghĩa đáng kể (xem các chỉ số đại diện trong Hình 5). Việc trộn các tập chú thích (RECAP Mix) cung cấp tốt nhất của cả hai thế giới.

Lưu ý rằng việc cải thiện FID trên tập xác thực MS-COCO có tương quan với việc cải thiện khoảng cách Fréchet của các chú thích được tạo ra, như được nêu chi tiết trong Phần 6.1.1. Điều này chỉ ra rằng việc cải thiện ngữ nghĩa trong mô hình RECAP Long bắt nguồn từ hiệu quả mẫu huấn luyện được cải thiện, và không chỉ từ việc giảm sai lệch huấn luyện-suy luận.

6.2. Huấn luyện các trọng số mô hình khác nhau
Để khám phá thêm đóng góp của các ví dụ được cải thiện cho mỗi phần của mô hình Stable Diffusion, chúng tôi so sánh việc huấn luyện chỉ trọng số UNet, so với trọng số CLIP, so với cả hai, sử dụng cùng quy trình huấn luyện và đánh giá. Để đơn giản, chúng tôi chỉ báo cáo kết quả cho RECAP Mix so với các mô hình Alttext và baseline.

Kết quả được tóm tắt trong Bảng 5. Nhìn chung, như mong đợi, việc huấn luyện nhiều trọng số hơn đạt được hiệu suất tốt hơn.

--- TRANG 6 ---
Hình 4. Các ví dụ về chú thích được tạo ra bởi mô hình RECAP được điều kiện trên các tiền tố ngắn hoặc dài, mô hình PaLI gốc, và các chú thích Alttext gốc. Ảnh được lấy từ LAION.

--- TRANG 7 ---
[Bảng 1 đã được dịch đầy đủ trong phần trước]

[Bảng 2 đã được dịch đầy đủ trong phần trước]

Thật thú vị, việc huấn luyện chỉ trọng số CLIP (chiếm ∽12% tổng trọng số) đạt được FID tốt hơn, với ít bước huấn luyện hơn, nhưng với ít cải thiện ngữ nghĩa. Việc huấn luyện cả trọng số CLIP và UNet dẫn đến cải thiện đáng kể cao hơn cho điểm số ngữ nghĩa so với việc huấn luyện chỉ trên một trong số chúng. Xem Phụ lục A.6 để biết thêm chi tiết.

Chúng tôi tin rằng việc huấn luyện trọng số CLIP chủ yếu giảm sai lệch trong phân phối văn bản giữa tập huấn luyện và tập đánh giá, trong khi việc huấn luyện trọng số UNet chủ yếu cải thiện việc căn chỉnh văn bản với hình ảnh.

7. Tóm tắt và thảo luận
Trong bài báo này, chúng tôi chỉ ra cách các mô hình văn bản-thành-hình ảnh có thể được cải thiện trên toàn bộ bằng cách huấn luyện trên các chú thích được tạo ra tổng hợp. Chúng tôi đã thực hiện phân tích sâu chứng minh rằng các mô tả ngắn thu hẹp khoảng cách huấn luyện-suy luận là hữu ích, cũng như các mô tả dài và chi tiết cải thiện hiệu quả mẫu mặc dù khác với tập suy luận. Chúng tôi tiếp tục chứng minh rằng việc trộn những mô tả này trong tập huấn luyện cải thiện đồng thời tất cả các mặt.

Có một số hướng thú vị cho nghiên cứu tương lai. Sẽ thú vị nếu kiểm tra liệu bằng cách điều chỉnh mô hình tạo chú thích để tạo ra chi tiết dồi dào trong các lĩnh vực hẹp, cùng một công thức có thể được sử dụng để cải thiện khả năng ngữ nghĩa trong các miền mới (ví dụ, liệu chúng ta có thể tạo ra các mô hình có thể tạo chính xác kiểu tóc, thiết kế phòng, biểu cảm khuôn mặt, quần áo, v.v. dựa trên các mô tả chi tiết?). Tương tự, có thể sử dụng RECAP để huấn luyện các mô hình T2I trong các miền thiếu chú thích văn bản hoàn toàn (ví dụ, album ảnh cá nhân hoặc ảnh chụp màn hình của các chương trình truyền hình). Chúng tôi đã tiến hành các thí nghiệm ban đầu ở đây, và chúng cho thấy rất nhiều hứa hẹn.

Chúng tôi đã thử nghiệm với việc tinh chỉnh một mô hình với các chú thích RECAP, nhưng sẽ thú vị nếu so sánh điều đó với một mô hình được huấn luyện trước trên các chú thích RECAP từ đầu. Liên quan đến điều này, sẽ thú vị nếu thử nghiệm thêm với các hỗn hợp khác nhau của ba loại chú thích mà chúng tôi có (RECAP Short, RECAP Long, và Alttext). Thậm chí tổng quát hơn, chúng ta có thể tưởng tượng việc tạo ra và trộn với nhau một số hương vị khác của các mô hình tái tạo chú thích. Điều này có thể cho phép chúng ta vượt qua giới hạn token (bằng cách huấn luyện trên cùng một hình ảnh nhiều lần với một tập con khác nhau của chú thích mỗi lần). Bất kể giới hạn token, có thể thú vị khi khám phá hiệu ứng của việc huấn luyện trên một số chú thích ngắn hơn mỗi hình ảnh thay vì một chú thích dài duy nhất. Cũng sẽ thú vị khi khám phá hiệu ứng của RECAP trên các mô hình lớn hơn được huấn luyện trên các tập dữ liệu lớn hơn.

Cuối cùng, RECAP chỉ ra tầm quan trọng của các tập dữ liệu chất lượng cao, và rằng nó có thể được cải thiện bằng dữ liệu tổng hợp, chúng tôi hy vọng rằng điều này cung cấp thêm một khuyến khích khác để áp dụng những kỹ thuật như vậy ngay cả ngoài miền T2I.

8. Lời cảm ơn
Chúng tôi muốn gửi lời cảm ơn tới Eyal Molad, Matan Kalman, Jason Baldridge, và nhóm Theta Labs tại Google, vì những đánh giá tuyệt vời, gợi ý, và hỗ trợ cho bài báo này.

--- TRANG 8 ---
[Tiếp tục với các bảng và biểu đồ đã được dịch]

--- TRANG 9 ---
[Hình 5 và các nội dung khác đã được dịch]

--- TRANG 10 ---
Tài liệu tham khảo
[1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.

[2] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers, 2023.

[3] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021.

[4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.

[5] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation, 2022.

[6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2022.

[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models, 2023.

[8] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment, 2023.

[9] Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, and Shiyu Chang. Harnessing the spatial-temporal attention of diffusion models for high-fidelity text-to-image synthesis, 2023.

[10] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing, 2023.

[11] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.

[12] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.

[13] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.

[14] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021.

[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.

[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.

[17] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision, 2022.

[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.

[19] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.

[20] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models, 2022.

[21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.

[22] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning, 2023.

[23] Wenyan Li, Jonas F. Lotz, Chen Qiu, and Desmond Elliott. Data curation for image captioning with text-to-image generative models, 2023.

[24] Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, and Xiaoyan Sun. Text-only image captioning with multi-context data generation, 2023.

[25] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Lia, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. https://cdn.openai.com/papers/dall-e-3.pdf, 2023.

[26] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornography, and malignant stereotypes, 2021.

[27] Tan M. Dinh, Rang Nguyen, and Binh-Son Hua. Tise: Bag of metrics for text-to-image synthesis evaluation, 2022.

[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.

[29] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1552–1565, mar 2022.

[30] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1316–1324, Los Alamitos, CA, USA, jun 2018. IEEE Computer Society.

--- TRANG 11 ---
[Tiếp tục với các tài liệu tham khảo còn lại]

--- TRANG 12 ---
A. Kết quả chi tiết
Các phần phụ sau đây giải thích chi tiết hơn về các chỉ số được sử dụng để đánh giá mô hình của chúng tôi, cũng như cung cấp các biểu đồ của một số chỉ số theo số bước huấn luyện.

A.1. Tính thực tế của hình ảnh
FID [28] là một thước đo mức độ gần nhau của hai tập dữ liệu hình ảnh về phân phối nội dung ngữ nghĩa trên một tập lớn ảnh trong mỗi tập dữ liệu. Nó thường được sử dụng để đánh giá các mô hình văn bản-thành-hình ảnh, bằng cách lấy một tập cặp văn bản và hình ảnh, và so sánh tập hình ảnh gốc với một tập hình ảnh mới được tạo ra từ các văn bản. Điểm số thấp hơn có nghĩa là các phân phối giống nhau hơn.

O-FID [27] là một biến thể của FID trong đó bộ phát hiện đối tượng sẵn có cắt các đối tượng từ hình ảnh trước, và FID được tính trên các tập hình ảnh đã cắt, cung cấp phép đo phân phối đối tượng phần nào chi tiết hơn.

Hình 6 cho thấy các điểm số FID và O-FID được tính toán cho một số checkpoint trong suốt quá trình huấn luyện. Rõ ràng là mô hình RECAP cải thiện đáng kể điểm số FID và O-FID trên tập dữ liệu MS-COCO, trong khi việc tinh chỉnh cùng số bước trên cùng tập hình ảnh, nhưng với các chú thích Alttext gốc, chỉ tạo ra cải thiện nhỏ.

A.2. Độ chính xác đối tượng ngữ nghĩa
Chỉ số SOA được đề xuất trong [29] và được sử dụng để đánh giá độ chính xác của mô hình tạo văn bản-thành-hình ảnh, bằng cách đo lường mức độ tuân theo các hướng dẫn để tạo ra các đối tượng cụ thể như một phần của lời nhắc. Để làm điều này, nó sử dụng các bộ phát hiện đối tượng chuyên biệt sẵn có của 80 lớp khác nhau (ví dụ xe máy hoặc bàn phím) trên dữ liệu được gắn nhãn. Có hai biến thể của chỉ số này, một cái trung bình trên các hình ảnh (SOA-I) và một cái khác trung bình trên các lớp (SOA-C).

Kết quả có thể được tìm thấy trong Hình 7. Chúng tôi thấy cải thiện rất đáng kể cho mô hình RECAP so với baseline, trong khi mô hình Alttext không cho thấy bất kỳ cải thiện nào.

A.3. Căn chỉnh đếm và vị trí
Các mô hình tạo văn bản thành hình ảnh được biết là gặp khó khăn trong việc đếm đối tượng, tức là nếu một lời nhắc chỉ định một số cụ thể các đối tượng (ví dụ "3 con chim") mô hình thường tạo ra một số khác các đối tượng. Trong chỉ số Counting Alignment (CA) [27], MS-COCO đã được lọc thành các lời nhắc chứa hướng dẫn cụ thể để tạo ra một số đối tượng đã biết. Một mô hình đếm sẵn có được sử dụng để đếm số lượng đối tượng mong đợi trong hình ảnh được tạo ra (theo loại đối tượng). Điểm số thấp hơn thì tốt hơn.

Tương tự, các mô hình tạo văn bản thành hình ảnh thường gặp khó khăn trong việc tuân theo các gợi ý vị trí, ví dụ "một cô gái ở phía trước một cậu bé" hoặc "một đĩa bơ dưới bàn". Trong chỉ số Positional Alignment (PA) [27], MS-COCO đã được lọc thành các lời nhắc với các gợi ý vị trí cụ thể. Mỗi hình ảnh được tạo ra bởi lời nhắc như vậy nhận được một điểm số CLIP so với lời nhắc gốc, và cũng so với mỗi sự thay thế của gợi ý vị trí trong lời nhắc gốc bằng một gợi ý khác (sai) (ví dụ "dưới" thay vì "ở phía trước").

Kết quả cho cả hai chỉ số có thể được tìm thấy trong Hình 8. Một lần nữa, mô hình RECAP cải thiện baseline trong khi mô hình Alttext thì không.

A.4. Căn chỉnh văn bản
Chỉ số R-Precision (RP), còn được gọi là điểm số CLIP, [30] là một thước đo phổ biến về mức độ gần nhau của một lời nhắc với một hình ảnh được tạo ra từ nó, bằng cách sử dụng khoảng cách embedding CLIP giữa mỗi lời nhắc và hình ảnh được tạo ra từ nó. Tuy nhiên, CLIP cũng được Stable Diffusion sử dụng làm bộ mã hóa văn bản, tạo ra thiên lệch hướng tới các hình ảnh được tạo ra bởi mô hình Stable Diffusion, và đặc biệt là cho điểm số RP cao hơn cho các hình ảnh được tạo ra so với các hình ảnh thực gốc. Tuy nhiên, chúng tôi thấy rằng so với mô hình cơ sở (chấm điểm 92.78), mô hình Alttext đạt được điểm số thấp hơn (91.31, có thể do kích thước tập dữ liệu nhỏ làm giảm sự biến thiên dữ liệu huấn luyện), trong khi mô hình RECAP cải thiện nó (93.8, bất chấp kích thước tập dữ liệu nhỏ).

A.5. Đánh giá con người
Chúng tôi đã gửi các mẫu từ các mô hình cơ sở, Alttext và RECAP để đánh giá con người. Các người đánh giá được trình bày bốn hình ảnh cho mỗi mô hình cùng với lời nhắc được sử dụng để tạo ra hình ảnh. Các hướng dẫn là chỉ chọn những hình ảnh tuân theo nghiêm ngặt lời nhắc và không chứa bất kỳ biến dạng lớn nào (biến dạng nhỏ khá phổ biến với Stable Diffusion 1.4). Kết quả được trình bày là trung bình trên các người đánh giá.

Chúng tôi đánh giá tất cả các mô hình này trên 200 lời nhắc được lấy mẫu ngẫu nhiên từ MS-COCO và tập dữ liệu DrawBench [1]⁷, mà Stable Diffusion 1.4 được biết là gặp khó khăn. Mô hình RECAP tạo ra 64.3% (29.25%→48.06%) hình ảnh hợp lệ hơn, và có khả năng 42.1% (53.5%→76%) cao hơn để có thể tạo ra ít nhất một hình ảnh hợp lệ (trong bốn hạt giống) so với mô hình cơ sở. Điều này chỉ ra rằng RECAP vừa tốt hơn trong việc tạo ra hình ảnh vừa có khả năng tuân theo các lời nhắc khó hơn (Bảng 2).

A.6. Trọng số mô hình
Chúng tôi cung cấp các biểu đồ bổ sung cho các mô hình khác nhau được huấn luyện với tập con khác nhau của các trọng số không đóng băng, như được mô tả trong Phần 6.2, trong Hình 9 và 10.

⁷2 lời nhắc có >77 token CLIP đã bị loại bỏ

--- TRANG 13 ---
[Các hình 6, 7, 8 với chú thích đã được dịch]

--- TRANG 14 ---
[Các hình 9, 10 với chú thích đã được dịch]

--- TRANG 15 ---
B. Các mô hình tạo hình ảnh khác
Hình 11 cho thấy các hình ảnh ví dụ được tạo ra bởi SDXL và Midjourney cho các lời nhắc trong hình trên cùng.

Hình 11. Các ví dụ về hình ảnh được tạo ra bởi SDXL 1.0 và Midjourney 5.1 cho các lời nhắc trong hình trên cùng.

--- TRANG 16 ---
C. Các ví dụ tạo chú thích bổ sung
Hình 12 và Hình 13 cho thấy các ví dụ chú thích bổ sung được tạo ra bởi RECAP so với Alttext gốc.

Hình 12. Các ví dụ về chú thích được tạo ra bởi các biến thể RECAP so với Alttext gốc. Ảnh được lấy từ LAION.

--- TRANG 17 ---
Hình 13. Thêm các ví dụ về chú thích được tạo ra bởi các biến thể RECAP so với Alttext gốc. Ảnh được lấy từ LAION.
