# 2307.10802.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2307.10802.pdf
# File size: 1395430 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Meta-Transformer: A Unified Framework for
Multimodal Learning
Yiyuan Zhang1,2∗Kaixiong Gong1,2∗Kaipeng Zhang2†
Hongsheng Li1Yu Qiao2Wanli Ouyang2Xiangyu Yue1†‡
1Multimedia Lab, The Chinese University of Hong Kong2Shanghai AI Lab
yiyuanzhang.ai@gmail.com, kaixionggong@gmail.com, xyyue@ie.cuhk.edu.hk
https://kxgong.github.io/meta_transformer/
TextNatural Language
3D Vision
Point Cloud
X-ray
Medical Application
Graph
Meta -Transformer
Molecular
VideoSpatial -TemporalInfrared
Nighttime/Thermal
Figure 1: Unified Multimodal Learning . Meta-Transformer utilizes the same backbone to encode
natural language, image, point cloud, audio, video, infrared, hyperspectral, X-ray, time-series, tabular,
Inertial Measurement Unit (IMU), and graph data. It reveals the potential of transformer architectures
for unified multi-modal intelligence.
Abstract
Multimodal learning aims to build models that can process and relate information
from multiple modalities. Despite years of development in this field, it still re-
mains challenging to design a unified network for processing various modalities
(e.g. natural language, 2D images, 3D point clouds, audio, video, time series,
tabular data) due to the inherent gaps among them. In this work, we propose
a framework, named Meta-Transformer, that leverages a frozen encoder to per-
∗Equal contribution
†Corresponding authors
‡Project leader
Preprint. Under review.arXiv:2307.10802v1  [cs.CV]  20 Jul 2023

--- PAGE 2 ---
form multimodal perception without any paired multimodal training data. In
Meta-Transformer, the raw input data from various modalities are mapped into
a shared token space, allowing a subsequent encoder with frozen parameters to
extract high-level semantic features of the input data. Composed of three main
components: a unified data tokenizer, a modality-shared encoder, and task-specific
heads for downstream tasks, Meta-Transformer is the first framework to perform
unified learning across 12 modalities with unpaired data. Experiments on differ-
ent benchmarks reveal that Meta-Transformer can handle a wide range of tasks
including fundamental perception (text, image, point cloud, audio, video), practical
application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,
tabular, and time-series). Meta-Transformer indicates a promising future for devel-
oping unified multimodal intelligence with transformers. Code will be available at
https://github.com/invictus717/MetaTransformer .
1 Introduction
The human brain, which is considered as the inspiration for neural network models, processes
information from various sensory inputs, e.g.visual, auditory, and tactile signals, simultaneously.
Moreover, knowledge from one source can benefit the comprehension of another. However, in
deep learning, designing a unified network capable of processing a wide range of data formats is a
non-trivial task due to the significant modality gap [1–3].
Each data modality presents unique data patterns, which makes it difficult to adapt models trained
on one modality to another. For instance, images exhibit a high degree of information redundancy
due to densely packed pixels, which is not the case with natural language [ 4]. Point clouds, on
the other hand, have a sparse distribution in 3D space, making them more susceptible to noise and
challenging to represent [ 5]. Audio spectrograms are time-varying and non-stationary data patterns
consisting of combinations of waves across frequency domains [ 6]. Video data contains a sequence of
image frames, which gives it the unique capability to capture both spatial information and temporal
dynamics [ 7]. Graph data represents entities as nodes and relationships as edges in a graph, modeling
complex, many-to-many relationships between entities [ 8]. Owing to the substantial differences
inherent to various data modalities, it is common practice to utilize distinct network architectures to
encode each modality separately. For instance, Point Transformer [ 9] leverages vector-level position
attention to extract structural information from 3D coordinates, but it cannot encode an image, a
natural language paragraph, or an audio spectrogram slice. Therefore, designing a unified framework
capable of utilizing a modality-shared parameter space to encode multiple data modalities remains a
significant challenge. Recently, the development of unified frameworks such as VLMO [ 2], OFA [ 10],
and BEiT-3 [ 3] have improved the ability of the network for multimodal understanding, through
large-scale multimodal pretraining on paired data [ 3,10,2], but they are more focused on vision and
language, and unable to share the whole encoder across modalities
The transformer architecture and attention mechanism, proposed by Vaswani et al. in 2017 [ 11]
for natural language processing (NLP), have made a significant difference in deep learning [ 11–16].
These advancements have been instrumental in enhancing perception across different modalities
such as 2D vision (including ViT [ 17,18] and Swin Transformer [ 19]), 3D vision (such as Point
Transformer [ 9] and Point-ViT [ 20,21]), and audio signal processing ( AST [ 6]),etc. These works
have demonstrated the versatility of transformer-based architectures, inspiring researchers to explore
whether it’s possible to develop foundation models capable of unifying multiple modalities, ultimately
achieving human-level perception across all modalities.
Table 1: Comparison between Meta-Transformer and related works on perception tasks.
Method Modalities Share Parameters Unpaired Data
Transformer [11]
 ✘ ✘
ViT [13], Swin Transformer [19], MAE [4]
 ✘ ✘
Point Transformer[9], PCT [22], Point ViT [21]
 ✘ ✘
AST [6], SSAST [23]
 ✘ ✘
CLIP [24], Flamingo [25], VLMO [2], OFA [10]
 ✘ ✘
BEiT-3 [3]
 Several Layers ✘
ImageBind [26]
 ✘ ✘
Meta-Transformer [ours]
 Whole Backbone ✔
2

--- PAGE 3 ---
In this paper, We explore the potential of transformer architecture to process 12 modalities including
images, natural language, point cloud, audio spectrogram, video, infrared, hyperspectral, X-Ray,
IMU, tabular, graph, and time-series data, as shown in Figure 1. We discuss the learning process
with transformers for each modality and address the challenges associated with unifying them into a
single framework. Consequently, we propose a novel unified framework named Meta-Transformer
for multimodal learning. Meta-Transformer is the first framework to simultaneously encode
data from a dozen of modalities using the same set of parameters , allowing a more cohesive
approach to multimodal learning (as shown in Table 1). Meta-Transformer incorporates three simple
and effective components: a modality-specialist (§ 3.2) for data-to-sequence tokenization, a modality-
shared encoder (§ 3.3) for extracting representations across modalities, and task-specific heads
for downstream tasks. Specifically, Meta-Transformer first transforms multimodal data into token
sequences that share a common manifold space. Then, a modality-shared encoder with frozen
parameters extracts representations, which are further adapted to individual tasks by updating the
parameters of downstream task heads and lightweight tokenizers only. Finally, task-specific and
modality-generic representations can be effectively learned by this simple framework.
We conduct extensive experiments on various benchmarks of 12 modalities. By utilizing images
of LAION-2B [ 24] dataset for pretraining exclusively, Meta-Transformer demonstrates remarkable
performance in processing data from multiple modalities, achieving consistently superior outcomes
over state-of-the-art methodologies in different multimodal learning tasks. More detailed experimental
settings can be found in § D.
In conclusion, our contributions can be summarized as follows:
•For multimodal research, we propose a novel framework, Meta-Transformer, which enables
a unified encoder to simultaneously extract representations from multiple modalities with
the same set of parameters.
•For multimodal network design, we comprehensively examine the functions of transformer
components such as embeddings, tokenization, and encoders in processing various modali-
ties. Meta-Transformer provides valuable insights and sparks a promising new direction in
developing a modality-agnostic framework capable of unifying all modalities.
•Experimentally, Meta-Transformer achieves outstanding performance on various datasets
regarding 12 modalities, which validates the further potential of Meta-Transformer for
unified multimodal learning.
2 Related Work
2.1 Single-Modality Perception
The development of various neural networks facilitates the perception of machine intelligence [ 27–
29, 11].
Multi-Layer Perceptron for pattern recognition. At the beginning, support vector machine (SVM)
and multi-layer perceptron (MLP) are applied to text [ 30], image [ 31], point cloud [ 32], and audio [ 33]
classification. These innovative works merit the feasibility of introducing AI to pattern recognition.
Recurrent & Convolutional Neural Network. Hopfield Network [ 34] is the original form of
recurrent networks, then LSTM [ 35] and GRU [ 36] further explore the advantages of RNNs in
sequence modeling and application in NLP tasks [ 37–39], which is also widely applied in audio
synthesis [ 40]. Meanwhile, the success of CNNs including LeNet [ 41], AlexNet [ 42], VGG [ 43],
GoogleNet [ 44] and ResNet [ 29] in image recognition greatly promote the application of CNNs
in other fields such as text classification [ 45,46], point cloud understanding [ 47–49], and speech
classification [50].
Transformer. Recently, transformer architecture [ 11] has been adopted in various tasks such as
text understanding [ 51] and generation [ 52] in NLP, classification [ 13], detection [ 53] and segmenta-
tion [15] in images, point cloud understanding [22, 9], and audio recognition [6, 23].
However, similar to applications of CNNs and RNNs, these networks are modified according to dis-
tinct properties of modalities. There is no common architecture for modality-agnostic learning. More
importantly, information from different modalities can be complementary [ 54–56], it’s significant
3

--- PAGE 4 ---
to design a framework that can encode data from different modalities and bridge these complicated
representations via a shared parameter space.
2.2 Transformed-based Multimodal Perception
The advantages of transformers for perception are the global receptive field and similarity modeling,
which prominently facilitate the development of multimodal perception. MCAN [ 57] proposes the
deep modular co-attention networks between vision and language, which performs the cross-modal
alignment by concisely maximizing the cross-attention. Then it becomes a consensus [ 2,1,10,3]
to utilize a cross-attention mechanism to bridge different modalities. With the success of pretrain-
finetune paradigm, more works are getting focused on how to effectively align representations
extracted across modalities by pretraining. VL-BERT [ 58] pioneers modality-aligned representations
for generic vision-language understanding with the MLM paradigm. Then Oscar [59] described the
object semantics in both visual and textural contents. Frameworks such as Vinvl [ 60], Simvlm [ 1],
VLMO [ 2], ALBEF [ 61], and Florence [ 62] further explore the advantages of joint representations
across vision-language modalities in terms of semantic consistency.
Multimodal models are also utilized for few-shot learning [ 25], sequence-to-sequence learning [ 10],
contrastive learning [ 63]. BEiT-v3 [ 3] proposes to take images as a foreign language with a more fine-
grained cross-modal mask-and-reconstruction process, sharing partial parameters. And MoMo [ 64]
further explores the training strategy and objective functions while using the same encoder for images
and texts.
Despite these advances, there remain significant obstacles to designing unified multimodal networks
due to differences between modalities. Additionally, most research in this area has focused on
vision and language tasks, and may not directly contribute to challenges such as 3D point cloud
understanding, audio recognition, or other modalities. The Flamingo model [ 25] represents a powerful
few-shot learner, but its transferability to point clouds is limited, and it remains a challenge to leverage
prior knowledge from one modality to benefit the others. In other means, existing multimodal methods
have limited extensibility on more modalities, although they have taken expensive training costs.
Addressing these discrepancies is dependent on bridging different modalities using the same set of
parameters, akin to how a bridge connects multiple river banks.
3 Meta-Transformer
In this section, we depict the proposed framework, Meta-Transformer, in detail. Meta-Transformer
unifies the multiple pipelines of processing data from different modalities and fulfills encoding texts,
images, point clouds, audio, and the other 8 modalities with a shared encoder. To achieve this,
Meta-Transformer is composed of a data-to-sequence tokenizer to project data to a shared embedding
space, a modality-agnostic encoder to encode the embedding of different modalities, and task-specific
heads to perform downstream predictions, as shown in Fig. 2.
3.1 Preliminary
Formally, we denote the input space of nmodalities as {X1,X2,···,Xn}, while {Y1,Y2,···,Yn}
are the corresponding label spaces. In addition, we assume there exists an effective parameter space
Θifor each modality, where any parameter θi∈Θican be utilized for processing data xi∈ Xifrom
that modality. We say that the essence of Meta-Transformer is to find a shared θ∗that satisfies:
θ∗∈Θ1∩Θ2∩Θ3∩ ··· Θn, (1)
with the hypothesis:
Θ1∩Θ2∩Θ3∩ ··· Θn̸=∅. (2)
The multimodal neural networks can be formulated as a unified mapping function F:x∈ X →
ˆy∈ Y, where xis the input data coming from any modality {X1,X2,···,Xn}andˆydenotes the
prediction of the network. Let’s denote yas the ground truth labels, the multimodal pipeline can be
formulated as:
ˆy=F(x;θ∗), θ∗= arg min
x∈X[L(ˆy, y)].(3)
4

--- PAGE 5 ---
Shared Token Space
Point Cloud
Images“The answer 
is blowing in the wind.”
Natural Language
Audio Spectrogram…L1*1
… 0*1 L2
0*1 L3…
0*1 L4…Data -to-Sequence Tokenizer0
Word Piece
Image Patches
Skeleton Adjacency
Spectrograms
Segmentation Detection
Scene SegmentationClassification
Classification
Speech Classification
“sentences have the 
same semantics?”“sentiment positive 
or negative?”“determine
statements is 
entailed.”
Paraphrase Sentiment Inference
Unified Multimodal Model 
 Part Segmentation
Parameter Frozen
Parameter Trainable
Figure 2: Meta-Transformer consists of data-to-sequence tokenization, unified feature encoding, and
down-stream task learning. The framework is illustrated with text, image, point cloud, and audio.
3.2 Data-to-Sequence Tokenization
We propose a novel meta-tokenization scheme designed to transform data across various modalities
into token embeddings, all within a shared manifold space. This approach is then applied to
tokenization, taking into account the practical characteristics of modality, as illustrated in Figure 3.
We take text, images, point clouds, and audio as examples. More details can be found in supplementary
materials. In specific, we use xT,xI,xP, andxAto denote a data sample of text, image, point cloud,
and audio spectrogram.
Natural Language . Following the common practice [ 51,65], we use WordPiece embeddings [ 66]
with a 30,000 token vocabulary. WordPiece segments original words into subwords. For example, the
original sentence: “The supermarket is hosting a sale”, could be converted by WordPiece to: “_The
_super market _is _host ing _a _sale”.
In this case, the word “supermarket” is divided into two subwords “_super” and “market” and the
word “hosting” is divided into “_host” and “ing”, while the rest words are unchanged and still single
units. The front of the first character of each original word will be stacked with a special character
“_”, indicating the beginning of a natural word. Each subword is corresponding to a unique token in a
vocabulary, then is projected to a high-dimensional feature space with word embedding layers. As a
result, each input text is transformed to a set of token embeddings x∈Rn×D, where nis the number
of tokens and Dis the dimension of embedding.
Images . To accommodate 2D images, we reshape the image x∈RH×W×Cinto a sequence of
flattened 2D patches xp∈RNs×(S2·C), where (H, W )represents the original image resolution, C
denotes the number of channels; Sis the patch size, and Ns= (HW/S2)is the resulting number of
patches. After that, a projection layer is utilized to project the embedding dimension to D:
xI∈RC×H×W→x′
I∈RNs×(S2·C)→x′′
I∈RNs×D. (4)
Note that we use the same operation for infrared images but the linear projection for hyperspectral
images. In addition, we simply replace 2D convolution layers with 3D convolution for video
recognition. More details can be found in B.1 and B.3.
Point Cloud . To learn 3D patterns with transformers, we convert point clouds from raw input space
to the token embedding space. X={xi}P
i=1denotes a point cloud of Ppoints, where xi= (pi,fi),
pi∈R3represents the 3D coordinates, and fi∈Rcis feature of the i-th point. Generally, ficontains
visual hints such as color, viewpoint, normal, etc. We employ the Farthest Point Sampling ( FPS)
operation to sample a representative skeleton of original point clouds with a fixed sampling ratio
(1/4). Then we employ K-Nearest Neighbor ( KNN) to group neighboring points. Based on grouped
sets containing local geometric prior, we construct the adjacency matrix with center points of grouped
subsets to further undercover the comprehensive structural information of 3D objects and 3D scenes.
5

--- PAGE 6 ---
Parsing
1×1 Conv
Flatten
(b) Text Tokenization
TESentences
ProjectionSub    words
C
(a) Meta Scheme
x
xETransformationConvolutionGrouping
Local   Data
Local   SemanticsPatchify
S×S Conv
Flatten
(c) Image Tokenization
C H W
Ix
IE
C S S
C H W
Patches
PatchFPS & KNN
Flatten
(d) Point Tokenization
(3 )Pc
px+
PESubsets
(3 )4Pc+
Adjacency1×1 Conv
(3 )c S S+  
Patchify
Flatten
(e) Audio Tokenization
TF
Ax
AE
1TF
Patches
S×S Conv
Spectrum
1SS
Figure 3: Illustration of Data-to-Sequence Tokenization 3.2. We propose the meta scheme in (a)
containing grouping, convolution, and transformation progress. Then (b)-(e) represents the building
blocks applied with our meta scheme on texts, images, point clouds, and audio spectrograms.
Finally, we aggregate the structural representations from Ksubsets. We obtain point embeddings as:
xP∈RP×(3+c)→x′
P∈RP
4×D
2→x′′
P∈RP
16×D. (5)
Audio Spectrogram . Initially, we pre-process the audio waveform with the duration of tseconds
with log Mel filterbank [ 67]. Then we employ the Hamming window with a stride of tson the
frequency of fsto split the original wave into l= (t/ts)intervals and further transform the original
wave into l-dimensional filterbank.
Subsequently, we split the spectrogram into patches from time and frequency dimensions with
the same patch size of S. Different from image patches, audio patches overlap on spectrograms.
Following AST [ 6], we also choose to split whole spectrograms into Ns= 12[(100 t−16)/10]
patches by S×Sconvolution, then we flatten patches into token sequences. Finally, we summarize
the process:
xA∈RT×F→x′
A∈RNs×S×S→x′′
A∈R(Ns·D/S2)×D, (6)
where TandFdenote time and frequency dimensions.
3.3 Unified Encoder
After transforming the raw inputs to token embedding space, we leverage a unified transformer en-
coder with frozen parameters to encode the sequences of token embeddings from different modalities.
Pretraining . We utilize ViT [ 13] as the backbone network and pre-train it on the LAION-2B dataset
with contrastive learning, which reinforces the ability for generic token encoding. After pretraining,
we freeze the parameters of the backbone network. In addition, for text understanding, we utilize the
pretrained text tokenizer of CLIP [ 24] to segment sentences into subwords and transform subwords
into word embeddings.
Modality-Agnostic Learning . Following common practice [ 51,13], we prepend a learnable token
xCLSto the sequence of token embeddings, and the final hidden state of xCLStoken ( z0
L) serves as the
summary representation of the input sequence, which is usually utilized for performing recognition.
To reinforce positional information, we incorporate position embeddings into the token embeddings.
Recall that we tokenize the input data to 1D embeddings, thus, we opt for standard learnable 1D
position embeddings. In addition, we do not observe substantial performance improvements using
more sophisticated 2D-aware position embeddings on image recognition. We simply fuse the position
embeddings and the content embeddings with an element-wise addition operation, and the resulting
embedding sequences are then fed into the encoder.
The transformer encoder with a depth of Lcompromises multiple stacked multi-head self-attention
(MSA) layers and MLP blocks. The input token embeddings are fed into an MSA layer first, then
6

--- PAGE 7 ---
an MLP block. Then the output of (ℓ−1)-th MLP block serves as the input of ℓ-th MSA layer.
Layer Normalization ( LN) is appended before each layer and the residual connection is applied after
each layer. The MLP contains two linear FC layers along with a GELU non-linear activation. The
formulation of the transformer is:
z0= [xCLS;Ex1;Ex2;···;Exn] +Epos, E∈Rn×D,Epos∈R(n+1)×D(7)
z′
ℓ= MSA( LN(zℓ−1)) +zℓ−1, ℓ = 1. . . L (8)
zℓ= MLP( LN(z′
ℓ)) +z′
ℓ, ℓ = 1. . . L (9)
y=LN(z0
L) (10)
where Exdenotes the token embeddings from proposed tokenizer and ndenotes the number of
tokens. We augment patch embeddings and learnable embedding with position embeddings Epos.
3.4 Task-Specific Heads
After obtaining learning representations, we feed representations to the task-specific heads h(·;θh),
which consists mainly of MLPs and varies from modalities and tasks. The learning objective of
Meta-Transformer can be summarized as:
ˆy=F(x;θ∗) =h◦g◦f(x), θ∗= arg min
θL(ˆy, y), (11)
where f(·),g(·), andh(·)denote the function of tokenizer, backbone, and heads, respectively.
4 Experiments
In this section, we perform experiments on each of the 12 modalities. We demonstrate the potential
of Meta-Transformer for multimodal perception. A summary of our experimental design is shown in
Table 2 and more experimental details can be found in § C.1.
4.1 Experimental Setups
Text understanding . For text understanding evaluation, we employ the General Language Under-
standing Evaluation (GLUE) benchmark [ 68] which incorporates several different datasets, covering
a wide range of natural language understanding tasks.
Image understanding . 1) Classification: we conduct experiments on ImageNet-1K [ 69] which
contains approximately 1.3 million images with 1000 categories. Following common practices [ 70,
19,71], base-scale models are trained for 300 epochs, while large models are pre-trained on ImageNet-
22K (14.2 million images) for 90 epochs and fine-tuned on ImageNet-1K for another 20 epochs. 2)
Object Detection: we conduct experiments on the MS COCO dataset [ 72] using Mask R-CNN [ 73]
as the detector and training each model for 12 epochs. 3) Semantic Segmentation: we train the
segmentation head UperNet [ 74] on ADE20K [ 75] for 160k iterations, providing a fair comparison
with previous CNN-based and transformer-based backbones.
Infrared, X-Ray, and Hyperspectral data understanding . We conduct experiments on infrared
image, X-Ray scan, and hyperspectral data recognition with RegDB [ 76], Chest X-Ray [ 77], and
Indian Pine4datasets, respectively.
Point cloud understanding . 1) Classification: to assess the performance of Meta-Transformer in 3D
object classification, we use the ModelNet-40 [ 78] benchmark, consisting of CAD models across
40 classes, with 9,843 training samples and 2,468 validation samples. 2) Semantic segmentation: to
evaluate performance in 3D point cloud segmentation, we assess the model on both S3DIS [ 79] and
ShapeNetPart [ 80] datasets. The S3DIS dataset encompasses 6 large indoor areas and 13 semantic
classes, comprising 271 rooms. The ShapeNetPart dataset includes 16,880 object models across 16
shape categories.
Audio recognition . For audio recognition, we utilize the Speech Commands V2 [ 81] dataset, which
consists of 105,829 one-second recordings of 35 common speech commands.
4https://github.com/danfenghong/IEEE_TGRS_SpectralFormer/blob/main/data/
IndianPine.mat
7

--- PAGE 8 ---
Video recognition . For video understanding, we conduct experiments on the UCF101 [ 82] dataset
for action recognition, with more details presented in § B.1.
Time-series forecasting . For time-series forecasting, we conduct experiments on ETTh1 [ 83],
Traffic5, Weather6, and Exchange [84] datasets. We use the tokenizer of Autoformer [85].
Graph understanding . We conduct experiments on the PCQM4M-LSC dataset [ 86], which is a
large-scale dataset consisting of 4.4 million organic molecules with up to 23 heavy atoms with their
corresponding quantum-mechanical properties. With the target of predicting molecular properties
using machine learning, it has plenty of applications in drug discovery, and material science.
Tabular analysis . We conduct experiments on adult and bank marketing from UCI repository7. We
use the tokenizer of TabTransformer [87] to encode raw tabular data.
IMU recognition . To evaluate the ability of Meta-Transformer to understand the inertial motion
systems, we conduct experiments of IMU sensor classification on the Ego4D [88] dataset.
Table 2: Summary of experimental settings across different modalities. We report the task, dataset,
and data scale for each modality.
Modalities Tasks Datasets Data Scale
Text Classification GLUE Benchmark 330K
ImageClassification ImageNet-1K 1.3M
Detection MS COCO 118K
Segmentation ADE-20K 20K
Point CloudShape Classification ModelNet-40 9K
Scene Segmentation S3DIS 400M Points
Object Segmentation ShapeNetPart 16K
Audio Classification Speech commands v2 105K
Video Action Recognition UCF101 14K
Infrared Classification RegDB 40K
Hyper-spectrum Classification Indian Pine 10K
X-Ray Classification Chest X-Ray 112K
IMU Classification Ego4D 193K
Tabular data Prediction Adult & Bank 32K-45K
Graph data Prediction PCQM4M-LSC 47M
Time-series Forecasting Exchange, Traffic, etc 5-36K
Settings of Networks : We follow the default settings of ViT [ 13].Meta-Transformer-B16 Fdenotes
Meta-Transformer with a base-scale encoder which contains 12 transformer blocks and 12 attention
heads, and the image patch size is 16. For the base-scale encoder, the embedding dimension is 768
and the output dimension of MLP is 3,072. ‘F’ and ‘T’ denotes that parameters of the encoder are
Frozen and further Tuned , respectively.
Table 3: Experimental results for text understanding on the GLUE benchmark. We compare
existing advanced methods from paraphrasing, sentiment, duplication, inference, and answering tasks,
and we report the pre-training settings and performances.
MethodPretraining Settings GLUE Benchmark
Modality Data SizeSST-2 MRPC QQP MNLI QNLI
Sentiment Paraphrase Duplication Inference Answering
BiLSTM+ELMo+Attn - - - 90.4 84.9 64.8 76.4 79.8
OpenAI GPT [89]
LanguageBook 0.8B 91.3 82.3 70.3 82.1 87.4
BERT BASE [51]Wiki+Book 3.3B88.0 88.9 71.2 84.6 90.5
RoBERTa BASE [65] 96.0 90.0 84.0 84.0 92.0
ChatGPT Various 4,5000B 92.0 66.0 78.0 89.3 84.0
Meta-Transformer-B16 F[ours]Image LAION-2B [24] 2B54.6 81.1 66.0 63.4 56.3
Meta-Transformer-B16 T[ours] 81.3 81.8 78.0 70.0 60.3
5https://pems.dot.ca.gov/
6https://www.bgc-jena.mpg.de/wetter/
7http://archive.ics.uci.edu/ml/
8

--- PAGE 9 ---
Table 4: Experimental results for image understanding . We conduct experiments in classification,
object detection, and instance segmentation tasks on the ImageNet [ 69], MSCOCO [ 72], and ADE-
20K [75] datasets, where Bold and underline indicate best and second best results.
MethodClassification Object Detection Semantic Segmentation
Res #Params #FLOPs Acc (%) #Params #FLOPs AP (%) #Params #FLOPs mIoU (%)
PVT-L [70] 224261.4M 9.8G 81.7 81.0M - 42.9 65.1M 79.6G 44.8
Swin-L‡[19] 3842197M 104G 87.3 253M 1382G 51.8 234M 2468G 52.1
CoAtNet-3‡[90] 3842168M 107G 87.6 - - - - - -
CoAtNet-4‡[90] 3842275M 190G 87.9 - - - - - -
DeiT III-L‡[91] 3842304M 191G 87.7 - - - 353.6M 2231G 51.5
SwinV2-L/24‡[92] 3842197M 115G 87.6 - - 58.8 - - 55.9
RepLKNet-31L‡[93] 3842172M 96G 86.6 229M 1321G 53.9 207M 2404G 52.4
HorNet-L‡[94] 3842202M 102G 87.7 259M 1358G 56.0 232M 2473G 54.1
ConvNeXt-L‡[95] 3842198M 101G 87.5 255M 1354G 53.5 235M 2458G 53.2
ConvNeXt-XL‡[95] 3842350M 179G 87.8 407M 1898G 53.6 391M 3335G 53.6
InternImage-L‡[96] 3842223M 108G 87.7 277M 1399G 54.9 256M 2526G 53.9
InternImage-XL‡[96] 3842335M 163G 88.0 387M 1782G 55.3 368M 3142G 55.0
Meta-Transformer-B16 F[ours]224286.6M 17.5G 69.3∗
143M 1126G 31.7 164M 135G 33.4224286.6M 17.5G 79.3†
Meta-Transformer-L14 F[ours]3362191.1M 190.6G 75.3∗
364M 2143G 43.5 314M 683G 41.23362191.1M 190.6G 83.1†
Meta-Transformer-B16 T[ours] 224286.6M 17.5G 85.4 143M 1126G 46.4 164M 135G 48.3
Meta-Transformer-L14 T[ours] 3362191.1M 190.6G 88.1 364M 2143G 56.3 314M 683G 55.0
∗: zero-shot classification†: linear probing for classification‡: models pre-trained on ImageNet-22K
Table 5: Experimental results for infrared and hyperspectral data understanding . We conduct
experiments on classification tasks over the SYSU-MM01 and Indian Pine datasets. We report Rank-1
(R@1), mean Average Precision (mAP), Overall Accuracy (OA), Average Accuracy (AA), and the
number of trainable parameters (Params).
Method R@1 (%) mAP (%) Params
AGW [97] [TPAMI’21] 70.49 65.90 25M
SMCL [98] [ICCV’21] 83.05 78.57 40M
MSCLNet [99] [ECCV’22] 83.86 78.31 50M
Meta-Transformer-B16 F 73.50 65.19 1.8M
(a) Infrared data understadingMethod OA (%) AA (%) Params
ViT [13] [ICLR’21] 71.86 78.97 85.2M
SpectralFormer [100] [TGRS’21] (Pixel) 78.55 84.68 85.2M
SpectralFormer [100] [TGRS’21] (Patch) 81.76 87.81 85.2M
Meta-Transformer-B16 F 67.62 78.09 0.17M
(b) Hyperspectral data understanding
4.2 Results on Natural Language Understanding
Table 3 illustrates the experimental results on the GLUE benchmark for text understanding tasks,
comparing various state-of-the-art methods such as BERT [ 51], RoBERTa [ 65], and ChatGPT. The
comparison centers on paraphrasing, sentiment, duplication, inference, and answering tasks. When
using frozen parameters pretrained on images, Meta-Transformer-B16 Fachieves scores of 54.6% in
sentiment (SST-2), 81.1% in paraphrase (MRPC), 66.0% in duplication (QQP), 63.4% in inference
(MNLI), and 56.3% in answering (QNLI) tasks. After finetuning, Meta-Transformer-B16T exhibits
improved performance, with 81.3% in sentiment, 81.8% in paraphrase, 78.0% in duplication, 70.0%
in inference, and 60.3% in answering tasks. Although the Meta-Transformer’s performance on
the GLUE benchmark might not be as impressive as that of BERT, RoBERTa, or ChatGPT, it still
demonstrates competitive performance, adaptability, and potential for understanding natural language.
4.3 Results on Image Understanding
As shown in Table 4, Meta-Transformer exhibits outstanding performance when compared with
Swin Transformer series [ 19,107] and InternImage [ 96] on image understanding tasks. On image
classification, with the help of CLIP [ 24] text encoder, Meta-Transformer delivers great performances
under zero-shot classification with the Meta-Transformer-B16 FandMeta-Transformer-L14 F, achiev-
ing 69.3% and 75.3%, respectively. At the same time, when the pretrained parameters are further
tuned, Meta-Transformer can outperform existing advanced methods, with Meta-Transformer-B16 T
andMeta-Transformer-L14 Tachieving 85.4% and 88.1% accuracy, respectively. The latter outper-
forms both SwinV2-L/24‡[107] (87.6%) and InternImage-XL [ 96]‡(88.0%) on ImageNet [ 69]
classification.
9

--- PAGE 10 ---
Table 6: Experimental results for point cloud understanding . We conduct experiments on the
ModelNet-40 [ 78], S3DIS [ 79], and ShapeNetPart [ 80] datasets. We compare existing advanced
methods from classification, semantic, and object part segmentation tasks, and we report the pre-
training modality (Pre-train) and trainable parameters number (Params) of each method.
Method Pre-trainModelNet-40 S3DIS Area-5 ShapeNetPart
mAcc (%) OA (%) Params mIoU (%) mAcc (%) Params mIoU I(%) mIoU C(%) Params
PointNet [CVPR’17] [32] N/A 86.0 89.2 3.5M 41.1 49.0 3.6M 83.7 80.4 3.6M
PointNet++ [NeurIPS’17] [5] N/A - 91.9 1.5M 53.5 - 1.0M 85.1 81.9 1.0
PointCNN [NeurIPS’18] [47] N/A 88.1 92.5 0.6M 57.3 - 0.6M
KPConv [ICCV’19] [49] N/A - 92.9 14.3M 67.1 72.8 15.0M 86.4 85.1 -
DGCNN [TOG’19] [101] N/A 90.2 92.9 1.8M 52.5 - 1.3M 85.2 82.3 1.3
Point Transformer [ICCV’21] [9] N/A 90.6 93.7 7.8M 70.4 - 7.8M 86.6 83.7 7.8
PointNeXt [NeurIPS’22] [102] N/A 90.8 93.2 1.4M 67.3 73.9 3.8M 86.7 84.4 1.0
Point-MLP [ICLR’22] [103] N/A 90.9 93.6 0.68M - - - 86.1 84.6 -
PointMixer [ECCV’22] [104] N/A 91.4 93.6 3.6M 71.4 77.4 6.5M - - -
Point-BERT [CVPR’22] [20] 3D - 93.2 21.1M 60.8 69.9 21.1M 85.6 84.1 21.1M
Point-MAE [ECCV’22] [105] 3D - 93.8 21.1M - - - 86.1 84.2 21.1M
P2P [NeurIPS’22] [56] 2D - 93.1 1.2M - - - 86.5 84.1 -
ACT [ICLR’23] [106] 2D - 93.5 21.1M 61.2 71.1 21.1M 86.1 84.7 21.2M
Meta-Transformer-B16 F[ours] 2D 90.5 93.6 0.6M 72.3 83.5 2.3M 87.0 85.2 2.3M
When it comes to object detection and semantic segmentation, Meta-Transformer also delivers
excellent performances, which further proves its generic ability on image understanding. On object
detection, Meta-Transformer-B16 FandMeta-Transformer-L14 Fachieve APs of 31.7% and 43.5%,
while Meta-Transformer-B16 TandMeta-Transformer-L14 Treach 46.4% and 56.3% AP, respectively.
In semantic segmentation, the mIoUs for Meta-Transformer-B16 FandMeta-Transformer-L14 Fare
33.4% and 41.2%, while Meta-Transformer-B16 TandMeta-Transformer-L14 Tachieve 51.0% and
55.0%, respectively. In comparison, SwinV2-L/24‡outperforms the Meta-Transformer in both
object detection (58.8% AP) and semantic segmentation (55.9% mIoU). The Meta-Transformer-L14 T
model has a similar performance to InternImage-XL‡[96] in semantic segmentation (both achieving
55.0% mIoU), but outperforms it in object detection (56.3% AP compared to 55.3% AP). These
results highlight that Meta-Transformer demonstrates a competitive performance in various image
understanding tasks even compared to Swin Transformer [19] and InternImage.
4.4 Results on Infrared, Hyperspectral, and X-Ray data
Table 5a presents the performance comparison of Meta-Transformer and other advanced methods
on the RegDB dataset [ 76] for infrared image recognition. Meta-Transformer-B16 Fdemonstrates
competitive results with a Rank-1 accuracy of 73.50% and an mAP of 65.19%. While it may not out-
perform the top-performing methods, Meta-Transformer proves to be a simple transferable approach
for infrared image recognition tasks. These results indicate the potential of Meta-Transformer in
handling the challenges associated with infrared images and contribute to advancements in this field.
Table 7: X-ray image recognition with Meta-
Transformer . We conduct experiments on the Chest
X-Ray dataset, we report the Accuracy (%) and the
number of trainable parameters.
Method Accuracy (%) Params
ViT [13] 96.3 86.9M
SEViT [108] 94.6 85.8M
Meta-Transformer-B16 F 94.1 0.75MIn addition, Table 5b presents the perfor-
mance of Meta-Transformer on the Indian
Pine dataset for hyperspectral image recog-
nition. SpectralFormer [ 100] achieves im-
pressive accuracy scores, with a patch-wise
approach. Plain vision transformer also per-
forms well in comparison when fully tun-
ing all parameters. Meta-Transformer-B16 F
demonstrates competitive results on hyper-
spectral image recognition with lower over-
all accuracy. However, Meta-Transformer
stands out for its significantly fewer trainable parameters (only 0.17M) compared to other methods.
This reveals a promising development direction of applying the Meta-Transformer to remote sensing,
environmental monitoring, and mineral exploration. For X-Ray images, similar to dealing with
infrared images, we take the same image tokenizer as common visible images. From Table 7, we can
observe that Meta-Transformer can achieve a competitive performance of 94.1% accuracy.
4.5 Results on 3D Point Cloud Understanding
Table 6 showcases the experimental results for point cloud understanding, comparing the perfor-
mance of Meta-Transformer with other state-of-the-art methods on the ModelNet-40 [ 78], S3DIS [ 79],
10

--- PAGE 11 ---
and ShapeNetPart [ 80] datasets. The tasks include classification, semantic segmentation, and object
part segmentation. When pretrained on 2D data, Meta-Transformer-B16 Fdemonstrates competi-
tive performance, achieving an overall accuracy (OA) of 93.6% on ModelNet-40 with only 0.6M
trainable parameters, which is comparable to the best-performing models. On the S3DIS Area-5
dataset, Meta-Transformer outperforms other methods with a mean IoU (mIoU) of 72.3% and a
mean accuracy (mAcc) of 83.5%, using 2.3M parameters. Moreover, Meta-Transformer excels
in the ShapeNetPart dataset, achieving the highest scores on both instances mIoU ( mIoU I) and
category mIoU ( mIoU C) with 87.0% and 85.2%, respectively, using 2.3M parameters. In summary,
Meta-Transformer demonstrates remarkable advantages in point cloud understanding tasks, offering
competitive performance with fewer trainable parameters compared to other state-of-the-art methods.
4.6 Results on Audio Recognition
In order to fairly compare Meta-Transformer with existing audio transformer series [ 6,23]
of similar scale , we conduct experiments on audio recognition using Meta-Transformer-B32 .
Table 8: Audio understanding with Meta-Transformer . We
conduct experiments on the Speech Commands V2 dataset and
report the accuracy score and the number of trainable and all
parameters.
Method Pre-train Acc(%) A-Params Params
AST [6] (Supervised) N/A 92.6 86.9M 86.9M
AST [6] (Supervised) AudioSet-20K 96.2 86.9M 86.9M
AST [6] (Supervised) ImageNet+KD 98.1 86.9M 86.9M
SSAST [23] (Self-Supervised) AudioSet-2M 97.8 89.3M 89.3M
SSAST [23] (Self-Supervised) Librispeech 97.8 89.3M 89.3M
SSAST [23] (Self-Supervised) Joint Pretraining 98.0 89.3M 89.3M
Meta-Transformer-B32 F[ours] 2D 78.3 86.6M 1.1M
Meta-Transformer-B32 T[ours] 2D 97.0 86.6M 86.3MTable 8 showcases the performance
of Meta-Transformer in the audio
domain. These models are com-
pared to existing methods such
as AST [ 6] and SSAST [ 23] in
terms of accuracy, all parameters
(A-Params), and trainable param-
eters (T-Params). With frozen pa-
rameters, Meta-Transformer-B32F
achieves an accuracy of 78.3%
while requiring only 1.1M param-
eters for tuning. On the other
hand, the Meta-Transformer-B32T
model exhibits a significantly higher accuracy of 97.0% when tuning the parameters, whereas the AST
model only reaches an accuracy of 92.6%. When AST is pre-trained on ImageNet and supplemented
with additional Knowledge Distillation (KD), it achieves an improved performance of 98.1%, but
with a higher number of trainable parameters of 86.9M. SSAST models display accuracy scores
ranging from 97.8% to 98.0% while requiring 89.3M parameters. These results highlight that the
Meta-Transformer performs competitively in the audio domain, demonstrating its versatility and
effectiveness across different fields.
4.7 Results on Video Recognition
Table 9: Video understanding with Meta-
Transformer . We conduct experiments on the
UCF101 [ 82] dataset and report the accuracy score
and the number of trainable parameters, where "V"
denotes video clips only.
Method Modality UCF101 Params
OPN [109] V 59.6 -
SimCLR [110] V 88.9 86.9M
VideoMAE V1 [111] V 96.1 86.9M
VideoMAE V2 [112] V 99.6 86.9M
ViT [13] (from scratch) V 51.4 86.9M
Meta-Transformer-B16 F V 46.6 1.1MTable 9 presents the performance compar-
ison of the Meta-Transformer and existing
advanced methods on the UCF101 dataset
for video understanding. Several state-of-the-
art video-tailored methods achieve accura-
cies of over 90%. Meta-Transformer only
contains a negligible amount of trainable pa-
rameters of 1.1 million to obtain an accuracy
of 46.6% while other methods have to train
around 86.9 million parameters. Though
Meta-Transformer is not able to beat other
state-of-the-art video understanding models,
Meta-Transformer stands out for its signifi-
cantly reduced trainable parameter count, suggesting the potential benefit of unified multi-modal
learning and less architectural complexity.
4.8 Results on Time-series Forecasting
To explore the ability of Meta-Transformer for time-series forecasting, we conduct experiments on
several widely-adopted benchmarks for Long-term forecasting tasks including ETTh1 [ 83], Traffic,
Weather, and Exchange [84], with results shown in Table 10.
11

--- PAGE 12 ---
Table 10: Time-series Forecasting with Meta-Transformer . Following TimesNet, we report the
number of trainable parameters and average performances from 4 different prediction lengths, which
is{96,192,336,720}.
ModelsMeta-Transformer TimesNet [113] ETSformer [114] FEDformer [115] Stationary [116] Autoformer [85] Pyraformer [117] Informer [83] LogTrans [118] Reformer [119]
[Ours] [ICLR’23] [Arxiv’22] [ICML’22] [NeurIPS’22] [NeurIPS’21] [ICLR’21] [AAAI’21] [NeurIPS’19] [ICLR’20]
Metric MSE MAE Param MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ETTh1 0.994 0.797 19K 0.458 0.450 0.542 0.510 0.440 0.460 0.570 0.537 0.496 0.487 0.827 0.703 1.040 0.795 1.072 0.837 1.029 0.805
Traffic 0.694 0.372 2.0M 0.620 0.336 0.621 0.396 0.610 0.376 0.624 0.340 0.628 0.379 0.878 0.469 0.764 0.416 0.705 0.395 0.741 0.422
Weather 0.797 0.640 51K 0.259 0.287 0.271 0.334 0.309 0.360 0.288 0.314 0.338 0.382 0.946 0.717 0.634 0.548 0.696 0.602 0.803 0.656
Exchange 1.430 0.961 22K 0.416 0.443 0.410 0.427 0.519 0.500 0.461 0.454 0.613 0.539 1.913 1.159 1.550 0.998 1.402 0.968 1.280 0.932
From Table 10, we can have the following observations. 1) With most of the model parameters
being fixed, Meta-Transformer can still outperform existing methods including Pyraformer [ 117],
Informer [ 83], LogTrans [ 118], and Reformer [ 119] on these datasets. 2) The number of trainable
parameters of Meta-Transformer is very few. With only 19K trainable parameters, Meta-Transformer
can still outperform Informer [ 83]. When 2M parameters are trained, Meta-Transformer can directly
outperform Pyraformer [ 117]. Therefore, Meta-Transformers pretrained on perception tasks can also
be applied to time-series forecasting tasks, which is inspiring for this area.
4.9 Results on Tabular Data Understanding
Table 11: Tabular data understanding with
Meta-Transformer . We report Accuracy (%) and
F1 score.
MethodAdult Bank Marketing
Accuracy (%) Accuracy (%) F1
LightGBM 87.8 - 0.39
Tabmlp 87.2 - 0.39
Tabnet 87.0 - 0.31
Tabtransformer 87.1 93.4 0.42
Meta-Transformer-B16 F 85.9 90.1 0.41Table 11 provides the comparison results about
the performances of different methods for tab-
ular data understanding on Adult Census and
Bank Marketing datasets.
Meta-Transformer-B16 Fachieves a slightly
lower accuracy than other methods on Adult
Census but performs better than all other meth-
ods on Bank Marketing dataset in terms of ac-
curacy and F1 scores. It suggests that Meta-
Transformer is also advantageous for tabular
data understanding, especially on complex datasets such as Bank Marketing.
Table 12: Graph data understanding with Meta-Transformer . We conduct experiments on the
PCQM4M-LSC dataset, and we report the evaluation metrics of train and validation MAE scores and
the number of trainable parameters.
Method Param. train MAE validate MAE
GCN [120] 2.0M 0.1318 0.1691
GIN [121] 3.8M 0.1203 0.1537
GCN- VN[120, 8] 4.9M 0.1225 0.1485
GIN- VN[121, 8] 6.7M 0.1150 0.1395
GINE- VN[122, 8] 13.2M 0.1248 0.1430
DeeperGCN- VN[123, 8] 25.5M 0.1059 0.1398
Graph Transformer [124] 0.6M 0.0944 0.1400
Graph Transformer- Wide [124] 83.2M 0.0955 0.1408
Graphormer SMALL [125] 12.5M 0.0778 0.1264
Graphormer [125] 47.1M 0.0582 0.1234
Meta-Transformer-B16 F 1.1M 0.8034 0.8863
4.10 Results on Graph and IMU Data Understanding
We report the performance of utilizing Meta-Transformer for graph understanding in Table 12.
We compare Meta-Transformer-B16 Fwith various graph neural network models for graph data
understanding on the PCQM4M-LSC dataset [ 86]. Among all the methods, Graphormer shows the
best performance with the lowest train and validation MAE scores of 0.0582 and 0.1234, respectively.
12

--- PAGE 13 ---
In contrast, Meta-Transformer-B16 Fdelivers the train and validation MAE scores of 0.8034 and
0.8863, which reveals the limited ability of current Meta-Transformer architecture for structural data
learning. We will further improve this in the future. Besides, following ImageBind [ 26], we conduct
classification on the Ego4D dataset [88], with input data, Meta-Transformer delivers an accuracy of
73.9%.
5 Limitation
From the perspectives of complexity, methodology, and further application, the limitations of the
Meta-Transformer are summarized as follows:
Complexity : Meta-Transformer requires O(n2×D)computation dealing with token embeddings
[E1,···,En]. High memory cost and heavy computation burden make it difficult to scale up.
Methodology : Compared with Axial Attention mechanism in TimeSformer [ 7] and
Graphormer [ 125], Meta-Transformer lacks temporal and structural awareness. This limitation
may affect the overall performance of Meta-Transformer in tasks where temporal and structural
modeling plays a critical role, such as video understanding, visual tracking, or social network
prediction.
Application : Meta-Transformer primarily delivers its advantages in multimodal perception. It’s still
unknown about its ability for cross-modal generation. We will work on this in the future.
6 Conclusion
In the early stages of artificial intelligence development, pioneers introduced the Multi-Layer
Perceptron (MLP) to address prediction tasks in machine learning. Later, recurrent and convolutional
networks expanded AI capabilities in multimedia data processing, achieving significant success in ex-
tracting representations from texts, images, point clouds, and audio. MLPs have since been integrated
into deep convolutional networks. In this paper, we explore the potential of plain transformers for
unified multimodal learning, highlighting a promising trend toward developing unified multimodal
intelligence with a transformer backbone. To some extent, this paper supports the dominant position
of transformers in next-generation networks. Importantly, CNNs and MLPs are not left behind. They
play essential roles in data tokenization and representation projection. This process exemplifies the
law of succession in neural networks and the ongoing evolution of artificial intelligence.
13

--- PAGE 14 ---
References
[1]Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint
arXiv:2108.10904 , 2021.
[2]Wenhui Wang, Hangbo Bao, Li Dong, and Furu Wei. Vlmo: Unified vision-language pre-
training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358 , 2021.
[3]Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:
Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442 ,
2022.
[4]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 16000–16009, 2022.
[5]Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. In NeurIPS , 2017.
[6]Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. arXiv
preprint arXiv:2104.01778 , 2021.
[7]Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for
video understanding? In Proceedings of the International Conference on Machine Learning
(ICML) , July 2021.
[8]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.
Neural message passing for quantum chemistry. In International Conference on Machine
Learning , pages 1263–1272. PMLR, 2017.
[9]Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer.
InICCV , pages 16259–16268, 2021.
[10] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang
Zhou, Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through
a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052 , 2022.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems , 30, 2017.
[12] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. ICLR , 2021.
[14] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision trans-
formers. In CVPR , pages 12104–12113, 2022.
[15] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo.
Segformer: Simple and efficient design for semantic segmentation with transformers. Advances
in Neural Information Processing Systems , 34:12077–12090, 2021.
[16] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
Ping Luo, and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.
arXiv:2106.13797 , 2021.
14

--- PAGE 15 ---
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In ICLR , 2021.
[18] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision
transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534 , 2022.
[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV ,
pages 10012–10022, 2021.
[20] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert:
Pre-training 3d point cloud transformers with masked point modeling. In CVPR , 2022.
[21] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, and Bernard Ghanem. Pix4point: Image
pretrained transformers for 3d point cloud understanding. arXiv preprint arXiv:2208.12259 ,
2022.
[22] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, and Shi-Min
Hu. Pct: Point cloud transformer. Computational Visual Media , 7(2):187–199, 2021.
[23] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio
spectrogram transformer. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 36, pages 10699–10709, 2022.
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning ,
pages 8748–8763. PMLR, 2021.
[25] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. arXiv preprint arXiv:2204.14198 , 2022.
[26] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
15180–15190, 2023.
[27] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous
activity. The bulletin of mathematical biophysics , 5:115–133, 1943.
[28] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support
vector machines. IEEE Intelligent Systems and their applications , 13(4):18–28, 1998.
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR , pages 770–778, 2016.
[30] Zhao Xu, Kai Yu, V olker Tresp, Xiaowei Xu, and Jizhi Wang. Representative sampling for
text classification using support vector machines. In Advances in Information Retrieval: 25th
European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings
25, pages 393–407. Springer, 2003.
[31] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne
Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation network.
Advances in neural information processing systems , 2, 1989.
[32] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep
learning on point sets for 3d classification and segmentation. In CVPR , 2017.
[33] P Dhanalakshmi, S Palanivel, and Vennila Ramalingam. Classification of audio signals using
svm and rbfnn. Expert systems with applications , 36(3):6069–6075, 2009.
15

--- PAGE 16 ---
[34] John J Hopfield. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the national academy of sciences , 79(8):2554–2558, 1982.
[35] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,
9(8):1735–1780, 1997.
[36] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555 ,
2014.
[37] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text sum-
marization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023 ,
2016.
[38] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 , 2014.
[39] Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network
for sentiment classification. In Proceedings of the 2015 conference on empirical methods in
natural language processing , pages 1422–1432, 2015.
[40] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward
Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient
neural audio synthesis. In International Conference on Machine Learning , pages 2410–2419.
PMLR, 2018.
[41] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
[42] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. Communications of the ACM , 60(6):84–90, 2017.
[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. In ICLR , 2015.
[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9,
2015.
[45] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text
classification. Advances in neural information processing systems , 28, 2015.
[46] Ye Zhang and Byron Wallace. A sensitivity analysis of (and practitioners’ guide to) con-
volutional neural networks for sentence classification. arXiv preprint arXiv:1510.03820 ,
2015.
[47] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn:
Convolution on x-transformed points. Advances in neural information processing systems , 31,
2018.
[48] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d convolutional neural network for
real-time object recognition. In IROS , 2015.
[49] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François
Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point
clouds. In ICCV , 2019.
[50] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, and Dong
Yu. Convolutional neural networks for speech recognition. IEEE/ACM Transactions on audio,
speech, and language processing , 22(10):1533–1545, 2014.
[51] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. In NAACL-HLT , 2019.
16

--- PAGE 17 ---
[52] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems , 33:1877–
1901, 2020.
[53] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and
Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16 ,
pages 213–229. Springer, 2020.
[54] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song
Han. Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye view representation.
arXiv preprint arXiv:2205.13542 , 2022.
[55] Feihu Zhang, Jin Fang, Benjamin Wah, and Philip Torr. Deep fusionnet for point cloud
semantic segmentation. In ECCV , 2020.
[56] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. P2p: Tuning pre-trained
image models for point cloud analysis with point-to-pixel prompting. arXiv preprint
arXiv:2208.02812 , 2022.
[57] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks
for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 6281–6290, 2019.
[58] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert:
Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 ,
2019.
[59] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,
Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for
vision-language tasks. In European Conference on Computer Vision , pages 121–137. Springer,
2020.
[60] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin
Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 5579–5588, 2021.
[61] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and
Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with
momentum distillation. Advances in neural information processing systems , 34:9694–9705,
2021.
[62] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for
computer vision. arXiv preprint arXiv:2111.11432 , 2021.
[63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui
Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 , 2022.
[64] Rakesh Chada, Zhaoheng Zheng, and Pradeep Natarajan. Momo: A shared encoder model for
text, image and multi-modal representations. arXiv preprint arXiv:2304.05523 , 2023.
[65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[66] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural
machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144 , 2016.
17

--- PAGE 18 ---
[67] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsuper-
vised pre-training for speech recognition. arXiv preprint arXiv:1904.05862 , 2019.
[68] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 , 2018.
[69] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR , pages 248–255. Ieee, 2009.
[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions. In ICCV , 2021.
[71] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545 , 2022.
[72] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV ,
2014.
[73] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV , pages
2961–2969, 2017.
[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing
for scene understanding. In ECCV , pages 418–434, 2018.
[75] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 633–641, 2017.
[76] Dat Tien Nguyen, Hyung Gil Hong, Ki Wan Kim, and Kang Ryoung Park. Person recognition
system based on a combination of body images from visible light and thermal cameras. Sensors ,
17(3):605, 2017.
[77] Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker Rejaul Islam,
Khandakar F Islam, Rashid Mazhar, Tahir Hamid, Mohammad Tariqul Islam, Saad Kashem,
Zaid Bin Mahbub, et al. Reliable tuberculosis detection using chest x-ray with deep learning,
segmentation and visualization. IEEE Access , 8:191586–191601, 2020.
[78] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and
Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR , 2015.
[79] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and
Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR , pages 1534–1543,
2016.
[80] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao Su, ARCewu Lu, Qixing
Huang, Alla Sheffer, Leonidas Guibas, et al. A scalable active framework for region annotation
in 3d shape collections. ACM TOG , 35(6):210, 2016.
[81] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv
preprint arXiv:1804.03209 , 2018.
[82] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402 , 2012.
[83] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai
Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In
AAAI , 2021.
[84] Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu. Modeling long-and short-
term temporal patterns with deep neural networks. In The 41st international ACM SIGIR
conference on research & development in information retrieval , pages 95–104, 2018.
18

--- PAGE 19 ---
[85] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition
transformers with Auto-Correlation for long-term series forecasting. In NeurIPS , 2021.
[86] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-
lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430 ,
2021.
[87] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data
modeling using contextual embeddings. arXiv preprint arXiv:2012.06678 , 2020.
[88] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit
Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the
world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 18995–19012, 2022.
[89] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
[90] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. Advances in Neural Information Processing Systems , 34:3965–3977,
2021.
[91] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXIV , pages 516–533. Springer, 2022.
[92] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao,
Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages
12009–12019, 2022.
[93] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and Jian Sun.
Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. In CVPR , 2022.
[94] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, and Jiwen Lu. Hornet:
Efficient high-order spatial interactions with recursive gated convolutions. Advances in Neural
Information Processing Systems , 35:10353–10366, 2022.
[95] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining
Xie. A convnet for the 2020s. In CVPR , 2022.
[96] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu,
Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation
models with deformable convolutions. arXiv preprint arXiv:2211.05778 , 2022.
[97] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, and Steven C. H. Hoi. Deep
learning for person re-identification: A survey and outlook. arXiv preprint arXiv:2001.04193 ,
2020.
[98] Ziyu Wei, Xi Yang, Nannan Wang, and Xinbo Gao. Syncretic modality collaborative learning
for visible infrared person re-identification. In ICCV , pages 225–234, October 2021.
[99] Yiyuan Zhang, Sanyuan Zhao, Yuhao Kang, and Jianbing Shen. Modality synergy com-
plement learning with cascaded aggregation for visible-infrared person re-identification. In
Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part XIV , pages 462–479. Springer, 2022.
[100] Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, and Jocelyn
Chanussot. Spectralformer: Rethinking hyperspectral image classification with transformers.
IEEE Transactions on Geoscience and Remote Sensing , 60:1–15, 2021.
[101] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M
Solomon. Dynamic graph cnn for learning on point clouds. TOG , 2019.
19

--- PAGE 20 ---
[102] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny,
and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling
strategies. In Advances in Neural Information Processing Systems (NeurIPS) , 2022.
[103] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu. Rethinking network design and local
geometry in point cloud: A simple residual mlp framework. ICLR , 2022.
[104] Jaesung Choe, Chunghyun Park, Francois Rameau, Jaesik Park, and In So Kweon. Pointmixer:
Mlp-mixer for point cloud understanding. In European Conference on Computer Vision , pages
620–640. Springer, 2022.
[105] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked
autoencoders for point cloud self-supervised learning. arXiv preprint arXiv:2203.06604 , 2022.
[106] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and
Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers
help 3d representation learning? arXiv preprint arXiv:2212.08320 , 2022.
[107] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao,
Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In
CVPR , 2022.
[108] Faris Almalik, Mohammad Yaqub, and Karthik Nandakumar. Self-ensembling vision trans-
former (sevit) for robust medical image classification. In Medical Image Computing and
Computer Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore,
September 18–22, 2022, Proceedings, Part III , pages 376–386. Springer, 2022.
[109] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised repre-
sentation learning by sorting sequence. In ICCV , 2017.
[110] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale
study on unsupervised spatiotemporal representation learning. In CVPR , 2021.
[111] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are
data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602 ,
2022.
[112] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and
Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. arXiv preprint
arXiv:2303.16727 , 2023.
[113] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times-
net: Temporal 2d-variation modeling for general time series analysis. arXiv preprint
arXiv:2210.02186 , 2022.
[114] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven C. H. Hoi. Ets-
former: Exponential smoothing transformers for time-series forecasting. arXiv preprint
arXiv:2202.01381 , 2022.
[115] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. FEDformer:
Frequency enhanced decomposed transformer for long-term series forecasting. In ICML , 2022.
[116] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers:
Rethinking the stationarity in time series forecasting. In NeurIPS , 2022.
[117] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.
Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and
forecasting. In ICLR , 2021.
[118] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng
Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series
forecasting. In NeurIPS , 2019.
[119] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In
ICLR , 2020.
20

--- PAGE 21 ---
[120] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In ICLR . OpenReview.net, 2017.
[121] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations , 2019.
[122] Rémy Brossard, Oriel Frigo, and David Dehaene. Graph convolutions that can finally model
local structure. arXiv preprint arXiv:2011.15069 , 2020.
[123] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. arXiv preprint arXiv:2006.07739 , 2020.
[124] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to
graphs. AAAI Workshop on Deep Learning on Graphs: Methods and Applications , 2021.
[125] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming
Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In
Thirty-Fifth Conference on Neural Information Processing Systems , 2021.
[126] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan
Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio–visual segmentation. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXVII , pages 386–403. Springer, 2022.
21

--- PAGE 22 ---
Appendix
A Summary
The appendix is organized as the following:
•We first validate and discuss the potential of the Meta-Transformer on more modalities
(video, infrared, X-Ray, and hyperspectral images) in addition to the modalities shown in
the main paper, and we provide surprising experimental results on these modalities in § B.
•Then we further demonstrate the performance and merits of Meta-Transformer in dealing
with multi-modal tasks (involving inputs from more than one modality to perform predic-
tions) in § C.
•In addition, we introduce more details of experiments on text, image, point cloud, and audio
in § D.
•Last but not least, we discuss the impact of Meta-Transformer on the machine learning and
computer vision community in § E.
B Extensibility on Single-Modality Perception
In the main body of this paper, we illustrate that Meta-Transformer can simultaneously uncover
the underlying patterns of natural language, 2D images, 3D point clouds, and audio spectrograms
with the same network architecture and network parameters. Furthermore, we explore its ability
in perceiving other modalities, like video recognition, infrared, X-Ray, and hyperspectral image
recognition. In specific, we conduct experiments on UCF101 [ 82] (video ), RegDB [ 76] (infrared
images), Chest X-Ray [77], and Indian Pine ( hyperspectral images) datasets.
B.1 Video Recognition
For video recognition, we follow VideoMAE [ 111] to modify the tokenizer by replacing the
2D embedding layer with a 3D embedding layer to simultaneously encode the spatial-temporal
information from input frames. After tokenization, by leveraging the modality-shared encoder and
task-specific heads, Meta-Transformer is able to extract high-level semantic features from videos and
achieve favorable performance in the action recognition task of the UCF101 dataset.
Dataset . The UCF101 [ 82] dataset is a common-used benchmark dataset for action recognition tasks.
It is an extended version of UCF50 and contains 13,320 video clips of 101 categories. These 101
categories can be divided into 5 groups: Body motion, Human-human interactions, Human-object
interactions, Playing musical instruments and Sports. All the input frames are with a resolution of
320×240 and a fixed frame rate of 25 FPS, collected from YouTube.
B.2 Infrared Image Recognition
Infrared and hyperspectral image recognition poses unique challenges due to their specific char-
acteristics. For infrared images, the Meta-Transformer framework could be adapted to capture
thermal information by encoding temperature values alongside visual features, where the tokenizer
for infrared images is the same as common RGB images.
Dataset . The RegDB [ 76] dataset focuses on evaluating the performance of infrared recognition
algorithms in unconstrained and realistic scenarios. It includes variations in pose, expression, illumi-
nation, and occlusion. We conduct experiments on the RegDB dataset to evaluate the performance of
Meta-Transformer on infrared recognition.
B.3 Hyperspectral Image Recognition
Similarly, for hyperspectral images, we expect that Meta-Transformer can also handle the high-
dimensional spectral information by representing each spectral band in token embeddings. Compared
22

--- PAGE 23 ---
with dealing with RGB images, the only modification is that we employ the new linear projection
layer to replace the existing 2D convolution layer.
Dataset . The Indian Pine dataset is widely used in remote sensing and hyperspectral image analysis.
It consists of 145×145pixels with 145 spectral bands, which are captured in Indiana.
B.4 X-Ray Image Recognition
In addition, we explore the potential of the Meta-Transformer in medical image analysis. We
leverage the tokenizer for RGB images here to encode raw medical images. Specifically, we conduct
experiments regarding X-ray image analysis on the Chest X-Ray [ 77] dataset. It is a collection
of medical images commonly used for the analysis and diagnosis of various thoracic conditions.
It comprises 7,000 X-ray images of the chest. The dataset is annotated with labels indicating the
presence or absence of abnormalities such as lung diseases, fractures, and heart conditions.
C Extensibility on Multi-Modality Perception
Since the modalities of text, image, point cloud, and audio are all involved in this paper, we did
not conduct comprehensive multi-modal experiments as common practice such as Flamingo [ 25],
OFA [ 10], or BEiT-3 [ 3]. Instead, we conduct multi-modal experiments on a new and challenging
task of Audio-Visual Segmentation [ 126], which is mainly focused on building an intelligent listener
to align with fundamental visual tasks.
C.1 Audio-Visual Segmentation
Audio-visual segmentation [ 126] refers to the task of segmenting objects from different audio
sources within a referring image. It aims to develop algorithms that analyze both audio and visual
signals simultaneously to identify and delineate distinct sources or events. It finds applications in
fields like video conferencing, surveillance, multimedia analysis, and augmented reality.
We conduct experiments on the A VSS [126] dataset, which is recently released in the field of audio-
visual research. It provides a comprehensive collection of audio and visual data captured in real-world
scenarios. The dataset includes synchronized audio and visual recordings, featuring various events
of human actions and natural sounds. In contrast to introducing multi-modal fusion modules as
existing methods, Meta-Transformer directly concatenates visual and audio embeddings after Data-to-
Sequence tokenization. After extracting representation, we employ a simple global average pooling
layer to obtain the final representations of two modalities. Table 13 illustrates the performance of
Table 13: Audio-Visual Segmentation with Meta-Transformer . We conduct experiments on the
A VSS [126] dataset, we report mIou (%) and F-score.
Method mIou (%) F-score Params
A VSS [126] (ResNet-50) 20.18 0.252 ˜80M
A VSS [126] (ASPP) 28.94 - ˜180M
A VSS [126] (PVT-v2) 29.77 0.352 ˜180M
Meta-Transformer 31.33 0.387 86.5M
Meta-Transformer and existing methods on the A VSS dataset for audio-visual segmentation. The
evaluation metrics reported in this task are mIou and F-score. In comparison, Meta-Transformer
outperforms all other methods with the highest mIou of 31.33% and the highest F-score of 0.387.
It also stands out for its significantly lower parameter count, with only 86.5 million parameters
compared to the approximate 80M to 180M parameters of other methods.
Meta-Transformer offers several advantages over other methods in the field.
•Unified architecture . It relieves modality-specific encoders and reduces computation by
leveraging a unified encode to process both audio and images, resulting in a more efficient
and streamlined process.
23

--- PAGE 24 ---
•Faster convergence . Thanks to the unified architecture for processing both audio and
images, the encoder can deeply align the two modalities instead of only at the output end,
which leads to faster convergence. Meta-Transformer only needs 4 training epochs to reach
31.33% of mIou.
•Superior performance . Meta-Transformer achieves a significant improvement of 10%
compared to other methods of a similar parameter scale.
•Efficiency . Despite its enhanced performance, Meta-Transformer achieves this with much
fewer parameters, requiring only 1/3of the parameter amount, which makes forward and
backward progress ease.
In summary, the benefits of employing the Meta-Transformer to deal with multi-modal tasks are
appealing due to computational efficiency, rapid convergence, improved performance, and parameter
efficiency. It reveals the significantly promising direction to apply Meta-Transformer to more multi-
modal tasks.
D Experimental Details
Our code is built on open-source projects including MMClassification8, MMDetection9, MMseg-
mentation10, OpenPoints11, Time-Series-Library12, Graphomer13.
We sincerely thank their great contributions. More implementation details can be found in our source
code.
E Further Impact Discussion
E.1 Modality-Free Perception
We hope that Meta-Transformer can introduce new insight into both multi-modal learning and multi-
modal generation fields. Meta-Transformer enables the usage of a shared encoder to encode diverse
modalities, e.g. natural language, 2D images, 3D point clouds, as well as audio spectrograms., and
project them into a shared representation space. This naturally reduces the modality gap across
modalities and mitigates the burden of cross-modal alignment. In addition, Meta-Transformer
removes the need for paired training data (such as image-text pairs), thus endowing multi-modal
learning with more training flexibility.
E.2 Application Prospects
We investigate the application of Meta-Transformer on a wide range of modalities including RGB
images, text, point clouds, video understanding, remote sensing (hyper-spectral images), nighttime
surveillance (infrared images), and medical analysis (X-Ray images).
In video understanding , Meta-Transformer reveals the potential of enhancing the analysis and
interpretation of videos by integrating information from text, audio, and image with the shared
encoder. This benefits tasks such as action recognition, event detection, and video summarization.
Meta-Transformer’s capability to handle video-related modalities paves the way for improved video
understanding applications in areas like video surveillance, video indexing, and content-based video
retrieval.
In hyperspectral imaging for remote sensing , Meta-Transformer enables the analysis and under-
standing of hyperspectral data by extracting high-level semantic features. It enhances tasks such as
8https://github.com/open-mmlab/mmpretrain/tree/mmcls-1.x
9https://github.com/open-mmlab/mmdetection
10https://github.com/open-mmlab/mmsegmentation
11https://github.com/guochengqian/openpoints
12https://github.com/thuml/Time-Series-Library
13https://github.com/microsoft/Graphormer
24

--- PAGE 25 ---
classification, target detection, and land cover mapping, improving the accuracy and efficiency of
remote sensing applications. The ability to process hyperspectral images using Meta-Transformer
opens doors for advancements in environmental monitoring, agriculture, urban planning, and disaster
management.
In medical applications , particularly X-ray image analysis, Meta-Transformer offers a promising
approach to improving diagnostic accuracy and efficiency with multi-modal information. It can
effectively capture and fuse information from X-ray images, clinical data, and other modalities to
aid in disease detection, anomaly identification, and treatment planning by leveraging its unified
learning framework. Meta-Transformer’s capability to handle multi-modal data enhances the potential
for more accurate and comprehensive medical imaging analysis, leading to better patient care and
outcomes.
For infrared images used in nighttime recognition and surveillance , Meta-Transformer’s ability to
process infrared data helps extract crucial information for object detection, tracking, and recognition
in low-light conditions, which opens an avenue for advancements in nighttime surveillance, security
systems, and autonomous navigation in challenging environments with the cooperation between
infrared cameras with RGB cameras.
E.3 Conclusion
In summary, we think that the ability of Meta-Transformer to unify multi-modal learning comes
from that neural network architectures can learn modality-invariant patterns . The architecture of
Meta-Transformer illustrates the advantages of length-variable token embeddings in multi-modal
learning, which provides flexible but unified forms of multi-modal semantics. Then it’s time to
think about designing algorithms to train networks that generalize on unseen modalities. Meanwhile,
it’s also intriguing to design the architecture of a unified multi-modal decoder, which can decode
representations into any form of a specific modality.
Although Meta-Transformer presents a surprising performance and shows a new promising direction
in multi-modal perception, we are not sure whether the proposed architectures are also effective in
generative tasks. And it remains mysterious how to develop modality-invariant generative models.
We hope that this can inspire future research.
25
