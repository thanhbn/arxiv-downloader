# 2211.11315.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2211.11315.pdf
# Kích thước tệp: 1223000 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Vượt Ra Ngoài Các Token Chú Ý: Kết Hợp Tầm Quan Trọng và Tính Đa Dạng của Token cho Vision Transformer Hiệu Quả
Sifan Long1,2*Zhen Zhao3,2*Jimin Pi2Shengsheng Wang1†Jingdong Wang2†
1Đại học Jilin2Baidu VIS3Đại học Sydney
longsf22@mails.jlu.edu.cn zhen.zhao@sydney.edu.au
wss@jlu.edu.cn fpijimin01, wangjingdong g@baidu.com
Tóm tắt
Vision transformer đã đạt được những cải thiện đáng kể trên các tác vụ thị giác khác nhau nhưng sự tương tác bậc hai giữa các token làm giảm đáng kể hiệu quả tính toán. Nhiều phương pháp cắt tỉa đã được đề xuất để loại bỏ các token dư thừa cho vision transformer hiệu quả gần đây. Tuy nhiên, các nghiên cứu hiện tại chủ yếu tập trung vào tầm quan trọng của token để bảo tồn các token chú ý cục bộ nhưng hoàn toàn bỏ qua tính đa dạng token toàn cục. Trong bài báo này, chúng tôi nhấn mạnh tính quan trọng của ngữ nghĩa toàn cục đa dạng và đề xuất một phương pháp tách và gộp token hiệu quả có thể đồng thời xem xét tầm quan trọng và tính đa dạng của token cho việc cắt tỉa token. Theo sự chú ý của class token, chúng tôi tách các token chú ý và không chú ý. Ngoài việc bảo tồn các token cục bộ phân biệt nhất, chúng tôi gộp các token không chú ý tương tự và ghép các token chú ý đồng nhất để tối đa hóa tính đa dạng của token. Mặc dù đơn giản, phương pháp của chúng tôi đạt được sự cân bằng đầy hứa hẹn giữa độ phức tạp mô hình và độ chính xác phân loại. Trên DeiT-S, phương pháp của chúng tôi giảm FLOPs 35% chỉ với độ giảm độ chính xác 0.2%. Đáng chú ý, nhờ vào việc duy trì tính đa dạng token, phương pháp của chúng tôi thậm chí có thể cải thiện độ chính xác của DeiT-T 0.1% sau khi giảm FLOPs 40%.

1. Giới thiệu
Transformer [29] đã trở thành kiến trúc phổ biến nhất trong cả cộng đồng xử lý ngôn ngữ tự nhiên và thị giác máy tính. Vision transformer (ViTs) [8] đã đạt được hiệu suất vượt trội và vượt qua các CNN tiêu chuẩn trong các tác vụ thị giác khác nhau như phân loại hình ảnh [10, 28, 31, 38], phân đoạn ngữ nghĩa [17, 19, 30, 33], và phát hiện đối tượng [1, 5]. Ưu điểm nổi bật nhất của transformer là khả năng nắm bắt hiệu quả các phụ thuộc tầm xa giữa các patch trong hình ảnh đầu vào thông qua

*Đóng góp ngang nhau.
†Tác giả liên hệ.

Token
Gộp Thiểu Số Cụm Dựa trên Đa dạng Token
Gộp Thiểu Số Dựa trên Quan trọng
Bảo tồn
Loại bỏ
Token
Gộp Thiểu Số Kết hợp cả hai
Cụm Ghép
(a)
(b)
(c)

Hình 1. Độ chính xác ImageNet và tỷ lệ giữ của DeiT-S đã cắt tỉa. (a) Phương pháp dựa trên quan trọng bảo tồn token chú ý dựa trên sự chú ý của class token và che tất cả token không chú ý; (b) Phương pháp dựa trên đa dạng gom các token tương tự thành một nhóm và sau đó kết hợp các token từ cùng nhóm thành một token mới. (c) Phương pháp kết hợp tách và gộp token để xem xét đồng thời tầm quan trọng và tính đa dạng của token.

cơ chế self-attention [23]. Tuy nhiên, sự tương tác bậc hai giữa các token làm giảm đáng kể hiệu quả tính toán [36], điều này thúc đẩy nhiều nghiên cứu về việc khám phá các transformer hiệu quả.

Là một trong những cách trực tiếp và hiệu quả nhất để giảm độ phức tạp tính toán, việc cắt tỉa token đã được nghiên cứu rộng rãi gần đây. Các nghiên cứu hiện tại chủ yếu tập trung vào việc thiết kế các chiến lược đánh giá tầm quan trọng khác nhau để giữ lại các token chú ý và cắt tỉa các token không chú ý [18, 21, 23, 35, 37]. Trong các công trình dựa trên tầm quan trọng này, DyViT [23] giới thiệu một mô-đun bổ sung để ước tính tầm quan trọng của mỗi token trong khi EViT [18] tổ chức lại các token hình ảnh dựa trên điểm số tầm quan trọng chú ý của class. Tuy nhiên, được truyền cảm hứng từ các nghiên cứu bảo tồn đa dạng gần đây trong các biến thể ViT [9, 11–13, 25,27], chúng tôi cho rằng việc thúc đẩy tính đa dạng token cũng rất quan trọng

1arXiv:2211.11315v1  [cs.CV]  21 Nov 2022

--- TRANG 2 ---
cho việc cắt tỉa token. Mặc dù các token không chú ý như nền hình ảnh và các kết cấu cấp thấp không liên quan trực tiếp đến các đối tượng phân loại, chúng có thể tăng tính đa dạng token và cải thiện khả năng biểu đạt của mô hình. Như đã thảo luận trong [32], nền hình ảnh (ví dụ, cỏ và lá trong Hình 2) có thể cải thiện độ chính xác phân loại do mối quan hệ tiềm năng của chúng với các đối tượng nền trước. Để đạt được mục tiêu này, chúng tôi đầu tiên khảo sát một chiến lược cắt tỉa dựa trên đa dạng trên DeiT-S [28] với các tỷ lệ giữ khác nhau. Cụ thể, thay vì làm nổi bật tầm quan trọng của token, nó trực tiếp gom cụm và kết hợp các token tương tự thành một token duy nhất, từ đó tối đa hóa tính đa dạng token. Đáng ngạc nhiên, như thể hiện trong Hình 1, một chiến lược trực quan như vậy có thể đạt được hiệu suất tương đương và thậm chí tốt hơn so với các phương pháp cắt tỉa dựa trên tầm quan trọng SOTA, đặc biệt ở tỷ lệ giữ thấp.

Mặc dù có hiệu suất đầy hứa hẹn, chiến lược dựa trên đa dạng không thể giữ lại các token chú ý ban đầu và do đó có thể làm suy yếu khả năng phân biệt của mô hình. Như thể hiện trong Hình 2 (c), các token đại diện nhất, ví dụ như mắt và tai của con chó hoặc mỏ của hai con chim, chứa thông tin ngữ nghĩa quan trọng cho các tác vụ phân loại nhưng không thể được bảo tồn bởi chiến lược dựa trên đa dạng. Để giải quyết vấn đề này, chúng tôi tự nhiên có xu hướng giữ tất cả các token chi phối này trong khi duy trì tính đa dạng token, như thể hiện trong Hình 2 (d). Nói tóm lại, một phương pháp cắt tỉa thỏa mãn nên đồng thời tính đến tầm quan trọng và tính đa dạng của token, sao cho thông tin cục bộ quan trọng nhất và thông tin toàn cục đa dạng có thể được bảo tồn đồng thời.

Được thúc đẩy bởi những quan sát trên, trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa mới kết hợp tầm quan trọng và tính đa dạng của token thông qua việc tách và gộp token hiệu quả. Như thể hiện trong Hình 1 (c), chúng tôi đầu tiên tách chuỗi token gốc thành các phần chú ý và không chú ý dựa trên sự chú ý của class token. Thay vì loại bỏ hoàn toàn các token không chú ý, chúng tôi áp dụng một thuật toán gom cụm density peak đơn giản hóa [24] để gom cụm hiệu quả các token không chú ý tương tự và kết hợp các token từ cùng nhóm thành một token mới. Ngoài ra, không giống như các phương pháp hiện tại bảo tồn tất cả token chú ý, chúng tôi thiết kế một thuật toán ghép đơn giản để gộp các token chú ý đồng nhất và cải thiện hiệu quả tính toán hơn nữa. Bằng cách này, chúng tôi có thể cắt tỉa token hiệu quả trong khi tối đa hóa việc bảo tồn tính đa dạng token. Chúng tôi tiến hành các thí nghiệm cắt tỉa token mở rộng để xác nhận hiệu quả của phương pháp. Mặc dù đơn giản, phương pháp của chúng tôi đạt được hiệu suất cắt tỉa vượt trội trên ImageNet [6] cho hai vision transformer khác nhau, DeiT [28] và LV-ViT [15]. Những đóng góp chính của chúng tôi được tóm tắt như sau:

• Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên nhấn mạnh tính đa dạng token cho việc cắt tỉa ViT. Chúng tôi cũng chứng minh tính quan trọng của nó thông qua phân tích số và thực nghiệm.

(a) (b) (c) (d)

Hình 2. Trực quan hóa kết quả cắt tỉa của các phương pháp khác nhau trên ImageNet với DeiT-S. (a) Hình ảnh gốc. (b) Phương pháp dựa trên quan trọng che các token không chú ý. (c) Phương pháp dựa trên đa dạng gom cụm các token tương tự và trực quan hóa cùng nhóm token thành một màu. (d) Phương pháp của chúng tôi bảo tồn các token phân biệt nhất, ví dụ như đầu của chim và chó. Ngoài ra, chúng tôi gộp các token không chú ý tương tự và ghép các token chú ý đồng nhất, ví dụ như cỏ và lá.

• Chúng tôi đề xuất một phương pháp tách và gộp đơn giản nhưng hiệu quả có thể đồng thời bảo tồn các token cục bộ chú ý nhất và ngữ nghĩa toàn cục đa dạng mà không áp đặt thêm tham số.

• Nhờ vào việc kết hợp tầm quan trọng và tính đa dạng token, phương pháp của chúng tôi đạt được hiệu suất SOTA mới về sự cân bằng giữa độ chính xác và FLOPs. Nó cũng có thể được triển khai cho các phương pháp cắt tỉa token khác, đạt được cải thiện hiệu suất xuất sắc.

2. Công trình liên quan

Vision Transformer. Khác với mạng tích chập, transformer có khả năng đáng kể trong việc mô hình hóa các phụ thuộc tầm xa và thiên kiến quy nạp tối thiểu [35]. Những tiến bộ gần đây cho thấy rằng các biến thể của transformer có thể là một lựa chọn thay thế cạnh tranh cho CNN. Visual transformer (ViT) [8] là công trình đầu tiên áp dụng kiến trúc transformer để đạt được hiệu suất STOA, nhưng nó chỉ thay thế tích chập tiêu chuẩn trong mạng nơ-ron sâu trên các tập dữ liệu hình ảnh quy mô lớn. Để giải phóng ViT khỏi sự phụ thuộc vào các tập dữ liệu lớn, DeiT [28] kết hợp một token bổ sung cho việc chưng cất kiến thức để cải thiện hiệu quả huấn luyện của vision transformer. LV-ViT [15] tiếp tục cải thiện hiệu suất bằng cách sử dụng tất cả các token patch hình ảnh để tính toán loss huấn luyện một cách chuyên sâu. Nó tương đương với việc chuyển đổi bài toán phân loại hình ảnh thành bài toán nhận dạng từng token. Trong khi ViT và các công trình tiếp theo đạt được hiệu suất xuất sắc, độ phức tạp bậc hai với số lượng token gây ra chi phí tính toán cao. Việc cắt tỉa token nhằm mục đích giảm các token dư thừa và cải thiện hiệu quả suy luận của các backbone ViT khác nhau.

2

--- TRANG 3 ---
Hình 3. Minh họa phương pháp của chúng tôi. (trên) Sử dụng phương pháp của chúng tôi tại các lớp 4, 7 và 10 của mô hình DeiT-S. (dưới) Cấu trúc mô hình trong một khối transformer đơn lẻ. Chúng tôi tách các token chú ý và không chú ý theo sự chú ý của class token. Sau đó, chúng tôi gom cụm các token không chú ý và kết hợp các token từ cùng nhóm thành một token mới. Đồng thời, chúng tôi ghép các token chú ý đồng nhất và kết hợp cùng cặp token.

Cắt tỉa token ViT. Mặc dù ViT đã đạt được độ chính xác cạnh tranh trong các tác vụ thị giác [1, 5, 10, 28, 31, 38], nó cần bộ nhớ và tài nguyên tính toán rất lớn. Do đó, làm thế nào để xây dựng một transformer hiệu quả hơn thu hút sự quan tâm của các nhà nghiên cứu. So với CNN, chi phí tính toán cao hơn của transformer chủ yếu do độ phức tạp thời gian bậc hai của multi-head self-attention (MHSA). Theo đó, một số công trình [18, 21, 23, 35] cố gắng cắt tỉa token dựa trên điểm số quan trọng trong transformer. Dựa trên việc có cần giới thiệu thêm tham số vào mô hình hay không, chúng tôi chia các phương pháp cắt tỉa token hiện tại thành hai nhóm sau. Một nhóm thực hiện cắt tỉa token bằng cách chèn các mô-đun dự đoán. DyViT [23] thiết kế một mô-đun dự đoán nhẹ được chèn vào các lớp khác nhau để ước tính điểm số quan trọng của mỗi token để cắt tỉa các token dư thừa với các đặc trưng hiện tại. IA-RED2 [21] giới thiệu các mô-đun có thể diễn giải để xóa động các patch dư thừa không liên quan đến đầu vào. AdaViT [20] kết nối một mạng quyết định nhẹ với backbone để tạo ra các quyết định một cách động. Nhóm khác tận dụng sự chú ý của class token để giữ các token chú ý. EViT [18] chia các token hình ảnh thành token chú ý và không chú ý theo sự chú ý của class token, giữ lại các token chú ý và loại bỏ các token hình ảnh không chú ý để tổ chức lại các token hình ảnh. Evo-ViT [35] phân biệt các token thông tin và không thông tin thông qua sự chú ý class toàn cục cho các cập nhật chậm và nhanh tương ứng. A-ViT [37] thiết kế một cơ chế cắt tỉa token thích ứng dựa trên sự chú ý của class token, điều chỉnh động chi phí tính toán của hình ảnh với độ phức tạp khác nhau. Không giống như các phương pháp cắt tỉa token này, chỉ tập trung vào tầm quan trọng của token, phương pháp của chúng tôi cũng xem xét tính đa dạng của thông tin ngữ nghĩa token. Do đó phương pháp của chúng tôi đạt được hiệu suất đáng kinh ngạc.

3. Kiến thức cơ bản

Trong vision transformer tiêu chuẩn [8], mỗi hình ảnh đầu vào I∈R^(H×W×C) đầu tiên được chuyển đổi thành một chuỗi patch một chiều X∈R^(N×P²×C). Sau đó tất cả các patch được ánh xạ thành token embedding D chiều thông qua một lớp tuyến tính có thể huấn luyện. Ngoài ra, một position embedding có thể học E_pos∈R^((N+1)×D) được thêm vào token embedding để giữ lại thông tin vị trí. Chính thức, chuỗi patch đầu vào có thể được biểu diễn như:

3

--- TRANG 4 ---
X = [x_cls; x_1; :::; x_N] + E_pos; (1)

trong đó x_cls biểu thị class token có thể học được phục vụ như biểu diễn hình ảnh, và x_i biểu thị token của patch thứ i với i≥0. Sau đó, chuỗi token này được đưa vào một mô hình ViT với L khối transformer xếp chồng, mỗi khối bao gồm một mô-đun multi-head self-attention (MHSA) và một mạng feed forward (FFN).

3.1. MHSA & FFN

Trong MHSA, chuỗi token đầu vào được ánh xạ tuyến tính thành ba ma trận khác nhau query Q, key K và value V tương ứng. MHSA có thể được công thức hóa như:

MHSA(Z) = Concat[softmax(Q_h K_h^T / √d) V_h]_{h=1}^H; (2)

trong đó Z là chuỗi token của N + 1 token. Concat[] xuất ra sự nối đặc trưng của H heads. Q_h, K_h và V_h là các ma trận chiếu của Q, K và V trong head thứ h tương ứng. d là chiều đặc trưng của head đơn lẻ.

FFN thường bao gồm hai lớp fully-connected và một lớp ánh xạ phi tuyến, có thể được biểu diễn như:

FFN(Z) = Sigmoid(Linear(GeLU(Linear(Z)))); (3)

trong đó Linear biểu thị các lớp fully-connected và GeLU là một hàm kích hoạt phi tuyến.

3.2. Độ phức tạp tính toán

Chiều của chuỗi token đầu vào là N×D, trong đó N là số lượng token và D là chiều embedding của mỗi token. Do đó chi phí tính toán của các mô-đun MSHA và FFN là O(4ND² + 2N²D) và O(8ND²) tương ứng. Rõ ràng, vision transformer yêu cầu chi phí tính toán rất thâm dụng, với tổng độ phức tạp tính toán O(12ND² + 2N²D). Vì việc giảm chiều kênh D chỉ ảnh hưởng đến tính toán của phép nhân ma trận hiện tại, hầu hết các công trình liên quan có xu hướng cắt tỉa token, ví dụ giảm số lượng N, để giảm tất cả các phép toán một cách tuyến tính hoặc thậm chí bậc hai.

4. Phương pháp luận

4.1. Tổng quan

Khác với các công trình hiện tại chỉ tập trung vào các token chú ý, phương pháp của chúng tôi kết hợp tầm quan trọng và tính đa dạng token để có được vision transformer hiệu quả và chính xác. Để đạt được mục tiêu này, chúng tôi đề xuất phương pháp tách và gộp token, đạt được sự cân bằng đầy hứa hẹn giữa FLOPs và độ chính xác. Như thể hiện trong Hình 3, chúng tôi chèn phương pháp của chúng tôi tại các lớp 4, 7 và 10 của mô hình DeiT-S. Phương pháp có hai thành phần chính: bộ tách token và bộ gộp token. Bộ tách chia chuỗi token gốc thành các phần chú ý và không chú ý dựa trên sự chú ý của class token. Sau đó bộ gộp gom cụm các token không chú ý tương tự và ghép các token chú ý đồng nhất. Trong phần này, chúng tôi đầu tiên chứng minh làm thế nào việc bảo tồn tính đa dạng token có lợi cho việc cắt tỉa token và sau đó trình bày chi tiết hai thành phần chính.

4.2. Tính đa dạng token quan trọng

Trong tài liệu, hầu hết công trình chỉ nhấn mạnh việc giữ lại các token quan trọng nhưng trực tiếp loại bỏ tất cả những token còn lại để đạt được tỷ lệ giữ token thỏa mãn. Tuy nhiên, được truyền cảm hứng từ các quan sát trong [32], rằng ngay cả nền hình ảnh cũng có thể giúp cải thiện phân loại instance nền trước, chúng tôi cho rằng các token ít quan trọng hơn cũng có thể chứa thông tin ngữ nghĩa hữu ích và là một bổ sung hiệu quả cho tính đa dạng thông tin. Ngoài ra, như đã thảo luận trong [9, 11–13, 25, 27], tính đa dạng token rất quan trọng để tối ưu hóa cấu trúc transformer. Do đó, việc bảo tồn thích hợp các token không chú ý này tăng cường tính đa dạng của thông tin ngữ nghĩa, có thể có lợi cho việc cắt tỉa token. Ngược lại, việc loại bỏ token một cách mù quáng sẽ gây ra mất mát thông tin ngữ nghĩa không thể phục hồi, đặc biệt ở tỷ lệ giữ thấp. Tham khảo [7, 11, 26, 27], chúng tôi tận dụng sự khác biệt giữa token và ma trận rank-1 để đo tính đa dạng của chuỗi token Z. Điểm số đa dạng sr(Z) có thể được tính như:

r(Z) = ‖Z - 1z^T‖, trong đó z = argmin_{z'} ‖Z - 1z'^T‖; (4)

trong đó ‖·‖ biểu thị chuẩn l1. Z∈R^{N×C} là chuỗi token của N token và z, z'∈R^C là một trong các token. z^T là ma trận chuyển vị của z và 1 là vector toàn số một. Rank của ma trận 1z^T là 1. Điểm số đa dạng lớn hơn cho thấy chuỗi token đa dạng hơn.

Chúng tôi khảo sát làm thế nào điểm số đa dạng ảnh hưởng đến hiệu suất cắt tỉa token. Trong Hình 4, chúng tôi kiểm tra độ chính xác phân loại và điểm số đa dạng ở các tỷ lệ giữ khác nhau. Rõ ràng, chúng ta có thể thấy rằng điểm số đa dạng chuỗi token có tương quan dương với độ chính xác phân loại. Trong cả phương pháp EViT hoặc phương pháp đề xuất của chúng tôi, điểm số đa dạng cao hơn nhất quán dẫn đến độ chính xác cao hơn. Ngoài ra, cũng có thể quan sát thấy rằng, đối với phương pháp EViT, điểm số đa dạng và độ chính xác phân loại giảm nhanh chóng khi tỷ lệ giữ giảm. Khác biệt, nhờ vào chiến lược gộp token bảo tồn đa dạng của chúng tôi, phương pháp của chúng tôi có thể duy trì điểm số đa dạng tương đối cao hơn ở các tỷ lệ giữ khác nhau và do đó nhất quán vượt trội hơn phương pháp EViT, đặc biệt ở tỷ lệ giữ thấp. Do đó, việc duy trì tính đa dạng token cao hơn là rất quan trọng để cải thiện độ chính xác phân loại.

4.3. Bộ tách Token

Để đầy đủ xem xét tầm quan trọng token trong khi duy trì đa dạng, chúng tôi ưu tiên các token chú ý để bảo tồn thông tin ngữ nghĩa quan trọng nhất. Do đó, chúng tôi tách chuỗi token gốc thành các token chú ý và không chú ý để chúng tôi duy trì tính đa dạng và tầm quan trọng token đồng thời. Giống như [29], chúng tôi chia token thành hai nhóm bằng cách so sánh độ tương tự của chúng với class token. Về mặt toán học, điểm số tương tự Attn_cls giữa class token và các token khác như sự chú ý của class token có thể được tính bằng

Attn_cls = Softmax(q_cls K^T / √d); (5)

trong đó q_cls biểu thị class token của vector query. Với N token tổng cộng và tỷ lệ giữ ρ, chúng tôi chọn top-K token làm token chú ý theo điểm số chú ý. N-K token còn lại được xác định là token không chú ý chứa ít thông tin hơn. Hơn nữa, trong lớp multi-head self-attention, chúng tôi tính trung bình của điểm số chú ý của tất cả các head.

4.4. Bộ gộp Token

Chúng tôi áp dụng các chiến lược khác nhau cho token chú ý và không chú ý khi gộp token. Đối với token không chú ý, chúng tôi đầu tiên áp dụng thuật toán gom cụm density peak để gom cụm token không chú ý và sau đó kết hợp các token từ cùng nhóm thành token mới bằng tổng có trọng số. Bằng cách này, chúng tôi có thể tích hợp một chuỗi token không chú ý mới T = [t_1; :::; t_n]. Đối với token chú ý, chúng tôi điều chỉnh một thuật toán ghép đơn giản để gộp các token chú ý đồng nhất. Chuỗi token đã gộp là P = [p_1; :::; p_m]. Chúng tôi nối T và P để có được chuỗi token đã cắt tỉa Z = [z_cls; p_1; :::; p_m; t_1; :::; t_n].

Gom cụm Token không chú ý. Thuật toán gom cụm K-means thông thường yêu cầu nhiều lần lặp để có được kết quả thỏa mãn, giảm thông lượng trong thực tế và đánh bại ý định tăng tốc mô hình. Sau khi nghiên cứu, chúng tôi phát hiện rằng thuật toán gom cụm mật độ có thể nhanh chóng tìm thấy các lớp có hình dạng tùy ý bằng cách khai thác tính kết nối mật độ của các lớp. Do đó, chúng tôi đơn giản hóa một thuật toán gom cụm density peak hiệu quả (DPC) không có quá trình lặp cũng như không có thêm tham số. Chúng tôi tuân theo thuật toán DPC được đề xuất trong [24]. Nó giả định rằng tâm cụm được bao quanh bởi các hàng xóm mật độ thấp, và khoảng cách giữa tâm cụm và bất kỳ điểm mật độ cao nào tương đối lớn. Chúng tôi tính hai biến cho mỗi token i: mật độ và khoảng cách tối thiểu từ token mật độ cao hơn. Cho một tập hợp token x, chúng tôi tính mật độ của mỗi token bằng

ρ_i = exp(-∑_{z_j∈Z} ‖z_i - z_j‖_2^2 / 2σ^2); (6)

trong đó Z biểu thị tập hợp token. z_i và z_j là các đặc trưng token tương ứng.

Đối với token có mật độ cao nhất, khoảng cách tối thiểu của nó được đặt thành khoảng cách tối đa giữa nó và bất kỳ token khác. Chúng tôi định nghĩa δ_i là khoảng cách tối thiểu giữa token i và bất kỳ token khác có mật độ cao hơn. Khoảng cách tối thiểu của mỗi token là:

δ_i = {min_{j:ρ_j>ρ_i} ‖z_i - z_j‖_2, nếu ∃j sao cho ρ_j > ρ_i
      max_j ‖z_i - z_j‖_2,         ngược lại.     (7)

Chúng tôi ký hiệu điểm số tâm cụm của token thứ i là ρ_i δ_i. Điểm số cao hơn có nghĩa là tiềm năng cao hơn để trở thành tâm cụm. Chúng tôi chọn top-K token có điểm số cao làm tâm cụm. Thuật toán DPC gán mỗi token còn lại cho cụm có tâm cụm gần nhất với token và có mật độ cao hơn.

Ghép Token chú ý. Xem hình ảnh ví dụ trong Hình 5. Token đồng nhất cũng hiện diện trong các đối tượng nền trước (token chú ý), chẳng hạn như má của động vật. Sự dư thừa này làm cho việc gộp các token chú ý đồng nhất có thể giảm số lượng token trong khi duy trì độ chính xác. Chúng tôi có thể áp dụng cùng chiến lược gom cụm token như đã làm cho token không chú ý. Tuy nhiên, vì

5

--- TRANG 6 ---
[THIS IS TABLE: Comparison table showing Model, Method, Top-1 Acc.(%), Params (M), FLOPs (G), FLOPs ↓(%), and Throughput (img/s) for various DeiT models and methods]

Bảng 1. So sánh với các phương pháp cắt tỉa token hiện tại trên DeiT. Chúng tôi báo cáo độ chính xác phân loại top-1, FLOPs và thông lượng trên ImageNet. 'FLOPs ↓' biểu thị tỷ lệ giảm của FLOPs.

các token chú ý chứa thông tin ngữ nghĩa quan trọng cho tác vụ phân loại cuối cùng, tốt nhất là nếu chúng tôi có thể bảo tồn các token gốc. Để giải quyết vấn đề này, chúng tôi tùy chỉnh một thuật toán ghép đơn giản giữ các token quan trọng nhất trong khi gộp các token đồng nhất. Cụ thể, chúng tôi định nghĩa độ đo tương tự cosine để xác định sự tương tự giữa các token khác nhau và tính điểm số tương tự cosine giữa các token chú ý. Sau đó chúng tôi chọn top-K cặp token tương tự nhất làm token đồng nhất. Cuối cùng, chúng tôi kết hợp mỗi cặp token thành một token mới và liên hệ các token chú ý còn lại.

Mặc dù các token trong cùng tập hợp có thông tin ngữ nghĩa tương tự, tầm quan trọng ngữ nghĩa của mỗi token không nhất thiết phải giống nhau. Thay vì trung bình mù quáng các token trong cùng tập hợp, chúng tôi kết hợp các token này bằng tổng có trọng số. Bằng cách giới thiệu sự chú ý của class token để biểu diễn tầm quan trọng, chúng tôi kết hợp cùng tập hợp token thành một token mới bằng

p_i = ∑_{j∈C_i} s_j z_j; (8)

trong đó s_j biểu thị điểm số quan trọng của token z_j, và C_i biểu thị tập hợp thứ i.

5. Thí nghiệm

5.1. Thiết lập

Tập dữ liệu và thước đo đánh giá. Thí nghiệm của chúng tôi được tiến hành trên ImageNet-1K [6] với 1.28 triệu hình ảnh huấn luyện và 50000 hình ảnh xác thực. Chúng tôi báo cáo độ chính xác phân loại top-1 và số phép toán dấu phẩy động (FLOPs) để đánh giá hiệu quả mô hình. Ngoài ra, chúng tôi đo thông lượng của các mô hình trên một GPU NVIDIA V100 duy nhất với kích thước batch cố định là 256.

Chi tiết triển khai. Để chứng minh tính tổng quát của phương pháp, chúng tôi tiến hành cắt tỉa token trên các mô hình ViT khác nhau bao gồm DeiT-T, DeiT-S, DeiT-B [28] và LV-ViT-S [15]. Theo [18], chúng tôi sử dụng phương pháp của chúng tôi tại các lớp 4, 7 và 10 của mô hình DeiT-T/S/B và tại các lớp 4, 8 và 12 cho LV-ViT-S [15]. Chúng tôi sử dụng cùng cài đặt huấn luyện như các bài báo gốc của DeiT [28] và LV-ViT [15]. Theo [38], chúng tôi kết hợp một scheduler cosine vào chiến lược học tập của chúng tôi trong đó tỷ lệ giữ giảm dần từ 1 xuống giá trị mục tiêu trong 100 epoch. Để so sánh công bằng, tất cả mô hình của chúng tôi được huấn luyện từ đầu trong 300 epoch trên 8 NVIDIA V100.

6

--- TRANG 7 ---
Hình 5. Trực quan hóa kết quả bộ gộp token trên DeiT-S. Các vùng che màu khác nhau biểu thị các token không chú ý được chia thành các nhóm token khác biệt. Phương pháp của chúng tôi gom cụm các token không chú ý tương tự thành một nhóm và ghép các token chú ý đồng nhất. Chúng tôi trực quan hóa cùng nhóm/cặp token thành cùng màu.

[THIS IS TABLE: So sánh với các vision transformer tiên tiến trên ImageNet, showing various methods with their Top-1 Acc(%), FLOPs(G), and Params(M)]

Bảng 2. So sánh với các vision transformer tiên tiến trên ImageNet. Chúng tôi cắt tỉa LV-ViT-S làm mô hình cơ sở.

5.2. Kết quả chính

So sánh với các phương pháp tiên tiến. Chúng tôi so sánh phương pháp của chúng tôi với các phương pháp cắt tỉa token SOTA, kết quả được thể hiện trong Bảng 1. Chúng tôi tận dụng chỉ số biểu thị tỷ lệ giữ. Chúng tôi báo cáo độ chính xác top-1, FLOPs và thông lượng cho mỗi mô hình. So với công trình trước đây, phương pháp của chúng tôi đạt được hiệu suất SOTA mới với chi phí tính toán tương tự. Cụ thể, trên mô hình cổ điển DeiT-S [28], sự suy giảm độ chính xác top-1 của các mô hình đã cắt tỉa của chúng tôi được kiểm soát trong vòng 0.2% khi chi phí tính toán giảm 35%. Ngoài ra, sự vượt trội của phương pháp chúng tôi rõ ràng hơn ở tỷ lệ giữ thấp hơn. Khi tỷ lệ nén của DeiT-S tăng lên 50%, chúng tôi cải thiện 0.5% so với đối thủ tốt nhất. Đặc biệt, tỷ lệ nén của phương pháp chúng tôi gần 40% trên mô hình DeiT-T [28], và độ chính xác thậm chí còn tốt hơn mô hình gốc.

Do tầm quan trọng và tính đa dạng token là trực giao cho việc cắt tỉa token, phương pháp của chúng tôi có thể được cắm vào EViT và đạt được cải thiện hiệu suất 0.1%. Như thể hiện trong Bảng 2, chúng tôi tiếp tục tiến hành thí nghiệm trên transformer sâu-hẹp LV-ViT [15]. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi đạt được sự cân bằng độ chính xác-độ phức tạp tốt hơn ở các tỷ lệ giữ khác nhau so với các kiến trúc CNN và ViT hàng đầu hiện tại.

Hiệu suất của các phương pháp hiện tại trên mỗi tỷ lệ giữ. Như thể hiện trong Hình 6, phương pháp của chúng tôi nhất quán đạt được hiệu suất tốt nhất trong khi hai phương pháp khác đạt được hiệu suất gần nhau. Ngoài ra, với việc giảm tỷ lệ giữ,

[THIS IS FIGURE: Biểu đồ so sánh hiệu suất của DyViT, EViT và phương pháp của chúng tôi với FLOPs khác nhau]

Hình 6. So sánh hiệu suất của DyViT, EViT và phương pháp của chúng tôi với FLOPs khác nhau.

7

--- TRANG 8 ---
[THIS IS TABLE: Shows Strategy, Acc (%), and FLOPs (G) for different methods on DeiT-S with keep rates of 0.7 and 0.5]

Bảng 3. Nghiên cứu loại bỏ về phương pháp của chúng tôi với tỷ lệ giữ khác nhau ρ.

độ chính xác phân loại của các phương pháp cắt tỉa token hiện tại giảm mạnh. May mắn thay, phương pháp của chúng tôi làm giảm hiện tượng này bằng cách bảo tồn tính đa dạng của thông tin ngữ nghĩa token. Đặc biệt khi FLOPs của DyViT giảm xuống 1.6G, độ chính xác phân loại giảm hơn 10%. Điều này là do việc loại bỏ hoàn toàn các token không chú ý làm giảm đáng kể tính đa dạng token, dẫn đến việc giảm thông tin ngữ nghĩa của chuỗi token gốc. Chúng tôi áp dụng việc tách và gộp token để đồng thời xem xét tầm quan trọng và tính đa dạng token, đạt được độ chính xác đáng kinh ngạc ở tỷ lệ giữ thấp. Trực quan, khi chúng tôi chỉ giữ một vài token, việc gộp token rõ ràng có ý nghĩa hơn so với việc chỉ giữ top-K token chú ý.

Trực quan hóa kết quả bộ gộp token. Để tiếp tục thể hiện khả năng diễn giải của phương pháp, chúng tôi trực quan hóa kết quả bộ gộp token cuối cùng trở lại các patch đầu vào gốc của nó trong Hình 5. Chúng tôi nhận thấy rằng phương pháp của chúng tôi chú ý đến các vùng đóng góp nhiều hơn cho dự đoán hình ảnh thay vì nền không mang thông tin. ví dụ, năm cơ quan giác quan của động vật được bảo tồn. Điều này chứng minh rằng phương pháp của chúng tôi hiệu quả tách các token chú ý và không chú ý. Không giống như các phương pháp khác che tất cả token không chú ý, phương pháp của chúng tôi kết hợp các patch nền có ngữ nghĩa tương tự. ví dụ, lông động vật được gộp thành một token duy nhất. Điều này ngụ ý rằng phương pháp của chúng tôi không chỉ tập trung vào token chú ý mà còn duy trì tính đa dạng của ngữ nghĩa token. Cũng đáng chú ý rằng cặp mắt được ghép và kết hợp thành một token trong hình thứ năm và thứ sáu. Điều này cho thấy rằng phương pháp của chúng tôi hiệu quả gộp các token chú ý đồng nhất và giảm sự dư thừa tiềm năng.

5.3. Phân tích loại bỏ

Hiệu quả của từng mô-đun. Như thể hiện trong Bảng 3, chúng tôi thêm từng mô-đun con một để đánh giá hiệu quả của từng mô-đun. i) Bảo tồn token chú ý. Loại bỏ token không chú ý dựa trên sự chú ý của class token trong các lớp cắt tỉa; ii) Đóng gói token không chú ý. Đóng gói tất cả token không chú ý thành một token; iii) Gom cụm token không chú ý. Gom cụm token không chú ý và kết hợp các token của cùng nhóm thành một token mới; iv) Ghép token chú ý. Ghép token chú ý và kết hợp các token của cùng cặp thành một token mới; Rõ ràng là vì mô-đun gom cụm bảo tồn tính đa dạng token, độ chính xác được cải thiện 0.2% và 0.8% ở tỷ lệ giữ 0.7 và 0.5 tương ứng. Đáng chú ý, tỷ lệ giữ càng thấp, phương pháp của chúng tôi hoạt động càng tốt. Ngoài ra, mô-đun ghép token chú ý tiếp tục giảm FLOPs của mô hình trong khi duy trì độ chính xác bằng cách gộp các token chú ý đồng nhất.

Chiến lược Bộ gộp Token khác nhau. Như trình bày trong Bảng 4, chúng tôi so sánh một số chiến lược gộp token thông thường để đánh giá hiệu quả của phương pháp. i) Chiến lược pooling. Sử dụng phép toán pooling để giảm số lượng token. ii) Chiến lược sub-sampling. Thêm một loạt lớp tích chập giữa MHSA và FFN để giảm chiều token. iii) Chiến lược cụm. Gom cụm các token và kết hợp các token của cùng nhóm thành một token mới. Chúng tôi quan sát thấy rằng chiến lược gom cụm nói chung cải thiện độ chính xác 0.4% so với các chiến lược gộp token khác. Một lý do có thể là chiến lược gom cụm có được nhiều thiên kiến quy nạp hơn với cùng chi phí tính toán. Tuy nhiên, chúng tôi phát hiện rằng thông lượng của thuật toán K-means thấp hơn, cho thấy rằng nó không hoạt động tốt về mặt tăng tốc mô hình trong thực tế. Hơn nữa, chúng tôi khám phá rằng thông lượng của thuật toán K-means giảm theo số lần lặp. Do đó chúng tôi đơn giản hóa một thuật toán DPC hiệu quả không có quá trình lặp cũng như không có thêm tham số, vượt trội hơn các chiến lược khác về cả độ chính xác và hiệu quả.

[THIS IS TABLE: Shows different token merger strategies on DeiT-S with their Acc(%), FLOPs(G), and Throughput(img/s)]

Bảng 4. Các chiến lược bộ gộp token khác nhau trên DeiT-S.

8

--- TRANG 9 ---
6. Kết luận

Trong bài báo này, chúng tôi đề xuất một phương pháp tách và gộp token để đồng thời xem xét tầm quan trọng và tính đa dạng token. Vì tầm quan trọng và tính đa dạng token là trực giao cho việc cắt tỉa token, phương pháp của chúng tôi có thể được sử dụng trong các phương pháp cắt tỉa token hiện tại để tiếp tục cải thiện hiệu suất. Chúng tôi chứng minh rằng phương pháp của chúng tôi đạt được sự cân bằng hiệu suất SOTA giữa độ chính xác và FLOPs mà không áp đặt thêm tham số. Chúng tôi hy vọng rằng bài báo này, kết hợp tầm quan trọng và tính đa dạng token, sẽ cung cấp những hiểu biết sâu sắc cho công trình tương lai về cắt tỉa visual transformer.

Tài liệu tham khảo
[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và Sergey Zagoruyko. End-to-end object detection with transformers. Trong European conference on computer vision, trang 213–229. Springer, 2020. 1, 3

[2] Chun-Fu Richard Chen, Quanfu Fan, và Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer for image classification. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 357–366, 2021. 7

[3] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, và Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems, 34:19974–19988, 2021. 6

[4] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, và Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. 7

[5] Zhigang Dai, Bolun Cai, Yugeng Lin, và Junying Chen. Up-detr: Unsupervised pre-training for object detection with transformers. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 1601–1610, 2021. 1, 3

[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. Trong 2009 IEEE conference on computer vision and pattern recognition, trang 248–255. Ieee, 2009. 2, 6

[7] Yihe Dong, Jean-Baptiste Cordonnier, và Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. Trong International Conference on Machine Learning, trang 2793–2803. PMLR, 2021. 4

[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2, 3, 7

[9] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, và Qiang Liu. Vision transformers with patch diversification. arXiv preprint arXiv:2104.12753, 2021. 1, 4

[10] Benjamin Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervé Jégou, và Matthijs Douze. Levit: a vision transformer in convnet's clothing for faster inference. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 12259–12269, 2021. 1, 3

[11] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, và Enhua Wu. Vision gnn: An image is worth graph of nodes. arXiv preprint arXiv:2206.00272, 2022. 1, 4

[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, và Chang Xu. Ghostnet: More features from cheap operations. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 1580–1589, 2020. 1, 4

[13] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, và Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908–15919, 2021. 1, 4

[14] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie Jin, Anran Wang, và Jiashi Feng. Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv preprint arXiv:2104.10858, 3(6):7, 2021. 7

[15] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, và Jiashi Feng. All tokens matter: Token labeling for training better vision transformers. Advances in Neural Information Processing Systems, 34:18590–18602, 2021. 2, 6, 7

[16] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, và Yanzhi Wang. Spvit: Enabling faster vision transformers via soft token pruning. arXiv preprint arXiv:2112.13890, 2021. 6

[17] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Ping Luo, và Tong Lu. Panoptic segformer: Delving deeper into panoptic segmentation with transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 1280–1289, 2022. 1

[18] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, và Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. 1, 3, 6

[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 10012–10022, 2021. 1, 7

[20] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan, Zuxuan Wu, Yu-Gang Jiang, và Ser-Nam Lim. Adavit: Adaptive vision transformers for efficient image recognition. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 12309–12318, 2022. 3

[21] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, và Aude Oliva. Ia-red2: Interpretability-aware redundancy reduction for vision transformers. Advances in Neural Information Processing Systems, 34:24898–24911, 2021. 1, 3, 6

[22] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, và Piotr Dollár. Designing network design spaces. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang 10428–10436, 2020. 7

[23] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, và Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:13937–13949, 2021. 1, 3, 6

[24] Alex Rodriguez và Alessandro Laio. Clustering by fast search and find of density peaks. science, 344(6191):1492–1496, 2014. 2, 5

[25] Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, và Anelia Angelova. Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021. 1, 4

[26] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, và Dacheng Tao. Patch slimming for efficient vision transformers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 12165–12174, 2022. 4, 6

[27] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, và Yunhe Wang. Augmented shortcuts for vision transformers. Advances in Neural Information Processing Systems, 34:15316–15327, 2021. 1, 4

[28] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. Trong International Conference on Machine Learning, trang 10347–10357. PMLR, 2021. 1, 2, 3, 6, 7

[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1, 5

[30] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, và Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 568–578, 2021. 1, 7

[31] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, và Lei Zhang. Cvt: Introducing convolutions to vision transformers. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 22–31, 2021. 1, 3

[32] Kai Xiao, Logan Engstrom, Andrew Ilyas, và Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. arXiv preprint arXiv:2006.09994, 2020. 2, 4

[33] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, và Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34:12077–12090, 2021. 1

[34] Weijian Xu, Yifan Xu, Tyler Chang, và Zhuowen Tu. Co-scale conv-attentional image transformers. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 9981–9990, 2021. 7

[35] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, và Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. Trong Proceedings of the AAAI Conference on Artificial Intelligence, 2022. 1, 2, 3, 6

[36] Huanrui Yang, Hongxu Yin, Pavlo Molchanov, Hai Li, và Jan Kautz. Nvit: Vision transformer compression and parameter redistribution. arXiv preprint arXiv:2110.04869, 2021. 1

[37] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, và Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 10809–10818, 2022. 1, 3, 6

[38] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, và Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 558–567, 2021. 1, 3, 6, 7

10
