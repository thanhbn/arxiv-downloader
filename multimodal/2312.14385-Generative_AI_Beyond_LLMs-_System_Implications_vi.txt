# AI tạo sinh vượt xa LLM: Ảnh hưởng hệ thống của việc tạo sinh đa phương thức

Alicia Golden1,2Samuel Hsia1,2Fei Sun3Bilge Acun1Basil Hosmer1Yejin Lee1
Zachary DeVito1Jeff Johnson1Gu-Yeon Wei2David Brooks2Carole-Jean Wu1
1FAIR tại Meta2Đại học Harvard3Meta

Tóm tắt - Khi việc phát triển các mô hình AI tạo sinh quy mô lớn phát triển vượt ra ngoài việc tạo sinh văn bản (1D) để bao gồm việc tạo sinh hình ảnh (2D) và video (3D), việc xử lý thông tin không gian và thời gian đặt ra những thách thức độc đáo đối với chất lượng, hiệu suất và hiệu quả. Chúng tôi trình bày nghiên cứu đầu tiên hướng tới việc hiểu không gian thiết kế hệ thống mới này cho các mô hình tạo sinh văn bản thành hình ảnh (TTI) và văn bản thành video (TTV) đa phương thức. Các thiết kế kiến trúc mô hình hiện tại được phân đôi thành 2 loại: các mô hình dựa trên Diffusion và Transformer. Đặc điểm hiệu suất hệ thống có tính hệ thống của chúng tôi trên một bộ tám mô hình TTI/TTV đại diện cho thấy rằng sau khi áp dụng các kỹ thuật tối ưu hóa tiên tiến như Flash Attention, Convolution chiếm tới 44% thời gian thực thi cho các mô hình TTI dựa trên Diffusion, trong khi các lớp Linear tiêu thụ tới 49% thời gian thực thi cho các mô hình dựa trên Transformer. Chúng tôi cũng quan sát thấy rằng các mô hình TTI dựa trên Diffusion giống với giai đoạn Prefill của suy luận LLM, và được hưởng lợi từ việc tăng tốc 1.1-2.5x lớn hơn từ Flash Attention so với các mô hình TTI dựa trên Transformer giống với giai đoạn Decode. Vì các tối ưu hóa được thiết kế cho LLM không ánh xạ trực tiếp lên các mô hình TTI/TTV, chúng tôi phải thực hiện đặc điểm hóa toàn diện các khối lượng công việc này để có được thông tin chi tiết cho các cơ hội tối ưu hóa mới. Khi làm như vậy, chúng tôi định nghĩa độ dài chuỗi trong bối cảnh của các mô hình TTI/TTV và quan sát thấy độ dài chuỗi có thể thay đổi tới 4x trong suy luận mô hình Diffusion. Chúng tôi cũng quan sát thấy các khía cạnh thời gian của khối lượng công việc TTV đặt ra các nút thắt hệ thống độc đáo, với Temporal Attention chiếm hơn 60% tổng thời gian Attention. Nhìn chung, đặc điểm hóa hiệu suất hệ thống sâu sắc của chúng tôi là bước đầu tiên quan trọng hướng tới việc thiết kế các hệ thống hiệu quả và có thể triển khai cho các khối lượng công việc TTI/TTV mới nổi.

Từ khóa chỉ mục - AI tạo sinh, đa phương thức, mô hình Diffusion, Transformer, độ dài chuỗi, Attention

I. GIỚI THIỆU

Những tiến bộ gần đây trong AI tạo sinh đã thúc đẩy nỗ lực to lớn vào việc phát triển các mô hình hiệu quả và có thể mở rộng cho việc tạo sinh văn bản [1]-[3]. Sự xuất hiện của các mô hình ngôn ngữ lớn (LLM) đã thúc đẩy vô số ứng dụng bao gồm chatbot, như ChatGPT [4], trợ lý email [5] và công cụ mã hóa [6]. Nỗ lực đáng kể đã được đầu tư vào việc tăng hiệu quả của các mô hình này khi được triển khai ở quy mô lớn, giúp cho phép chỉ riêng ChatGPT phục vụ hơn 100 triệu người dùng tích cực mỗi tuần [7]. Tuy nhiên, tạo sinh văn bản chỉ là phần nổi của tảng băng trôi. Một biểu diễn một chiều của thông tin thiếu khả năng truyền tải thông tin không gian và thời gian, cả hai đều quan trọng để hiểu thế giới xung quanh chúng ta. Sự tiến triển tự nhiên của các mô hình AI tạo sinh quy mô lớn này do đó là phát triển từ văn bản (1D) sang hình ảnh (2D) sang video (3D). Tuy nhiên, chuyển sang các biểu diễn chiều cao hơn đặt ra nhiều thách thức đối với chất lượng, hiệu suất và hiệu quả. Trong khi các hệ thống hiện tại chủ yếu đã được tối ưu hóa cho LLM thông qua các kỹ thuật như Flash Attention [8], các thuộc tính riêng biệt của các mô hình văn bản thành hình ảnh (TTI) và văn bản thành video (TTV) cho thấy những khối lượng công việc mới nổi này có thể không thấy lợi ích bằng nhau - do đó đòi hỏi một phân tích sâu sắc để xác định các cơ hội tối ưu hóa TTI/TTV. Bài báo này kiểm tra cẩn thận các khối lượng công việc TTI/TTV mới nổi và làm nổi bật các thuộc tính khác biệt lớn so với các LLM dựa trên văn bản tiền nhiệm.

Trong khi các mô hình tạo sinh hình ảnh và video đã có những tiến bộ thuật toán đáng kể trong những năm gần đây, tương đối ít đã được thực hiện để tối ưu hóa việc triển khai các mô hình này từ góc độ hệ thống. Các tối ưu hóa hệ thống mới được thiết kế riêng hướng tới nút thắt hiệu suất hệ thống của các mô hình TTI/TTV có tiềm năng thay thế việc tạo sinh các clip video ngắn trong khoảng vài giây bằng các bộ phim và phim truyện đầy đủ. Trong bài báo này, chúng tôi đề cập đến nút thắt hiệu suất như các toán tử chiếm ưu thế trong thời gian thực thi của khối lượng công việc. Các tác vụ tạo sinh hình ảnh khác như tạo sticker [9], tài liệu giáo dục [10], và thậm chí các khám phá khoa học [11], cũng có thể được hưởng lợi từ các tối ưu hóa hệ thống cho phép tăng tốc độ và độ phân giải. Vượt qua các thách thức hệ thống là quan trọng để cho phép các ứng dụng trong tương lai.

Để đánh giá việc tạo sinh hình ảnh/văn bản tiên tiến hiện tại, trước tiên chúng tôi kiểm tra các tác vụ học sâu tạo sinh quy mô công nghiệp ở cấp độ toàn hạm đội. Chúng tôi thấy rằng trong khi các mô hình TTI/TTV nhỏ hơn một bậc độ lớn so với LLM về số lượng tham số mô hình, số lượng GPU được sử dụng để huấn luyện gần như cùng bậc độ lớn. Thực tế, tỷ lệ số lượng GPU trên mỗi tham số mô hình cao hơn 14x đối với các mô hình TTI so với LLM, nhấn mạnh tầm quan trọng của việc chạy các mô hình này một cách hiệu quả (Hình 1). Đặc điểm hóa toàn hạm đội tiếp theo tiết lộ rằng lớp khối lượng công việc AI mới nổi này có các yêu cầu hệ thống riêng biệt - việc sử dụng bộ nhớ trung bình cho các mô hình TTI/TTV cao hơn khoảng 35% so với LLM.

Sau đó, chúng tôi sử dụng cách tiếp cận định lượng để đặc điểm hóa các mô hình TTI/TTV tiên tiến, so sánh không gian thiết kế đa chiều qua độ trễ và cường độ tính toán. Chúng tôi xây dựng một bộ mô hình gồm tám tác vụ tạo sinh văn bản thành hình ảnh và video đại diện và chứng minh cách các mô hình này khác biệt so với các mô hình ngôn ngữ được sử dụng rộng rãi, tức là LLaMA [12]. Chúng tôi thấy rằng các nút thắt hiệu suất hệ thống mới xuất hiện sau khi áp dụng các tối ưu hóa hiệu suất tiên tiến, như Flash Attention [8], với Convolution chiếm tới 44% thời gian thực thi trong các mô hình TTI dựa trên Diffusion, và các lớp Linear tiêu thụ tới 49% thời gian thực thi trong các mô hình TTI dựa trên Transformer.

Chúng tôi cũng quan sát thấy rằng các mô hình LLM truyền thống như Prefill/Decode và Độ dài chuỗi không ánh xạ 1:1 lên các khối lượng công việc TTI/TTV. Chúng tôi profile độ dài chuỗi trong suốt quá trình suy luận và thấy rằng ngược lại với LLM, độ dài chuỗi thay đổi tới 4x trong suốt quá trình suy luận mô hình Diffusion. Nói chung, độ dài chuỗi tăng theo hàm bậc hai với kích thước hình ảnh trong các mô hình Diffusion. Hơn nữa, chúng tôi điều tra nút thắt hiệu suất hệ thống được trình bày bởi Temporal Attention, cho phép tạo sinh khung hình gắn kết qua thời gian trong các mô hình TTV. Điều này trái ngược với Spatial Attention, tập trung qua một hình ảnh 2D. Chúng tôi thấy rằng Temporal Attention mất 2x thời gian thực thi so với Spatial Attention, nhưng tiêu thụ 9x số lượng FLOP. Chúng tôi quan sát thêm rằng nút thắt Temporal Attention tăng theo hàm bậc hai với số lượng khung hình, gợi ý nhu cầu về các tối ưu hóa trong tương lai.

Phân tích tiếp theo của chúng tôi về các trường hợp sử dụng AI tạo sinh quy mô công nghiệp cung cấp những hiểu biết thú vị cho thiết kế hệ thống tương lai. Các mô hình TTI/TTV thể hiện các thuộc tính hệ thống độc đáo sau đây, khác biệt so với LLM truyền thống:

• Cường độ số học cao (Phần II). Các mô hình TTI, và đặc biệt là các mô hình dựa trên diffusion thể hiện cường độ số học cao hơn so với LLM tới 100x. Điều này bắt nguồn từ việc tái sử dụng tham số cao xảy ra trong UNet, nơi hàng chục hoặc hàng trăm bước khử nhiễu gây ra các lần lặp trên cùng một số lượng tham số.

• Convolution như một nút thắt hiệu suất hệ thống (Phần IV-A). Sau khi áp dụng các tối ưu hóa tiên tiến như Flash Attention, nút thắt hiệu suất hệ thống chuyển sang các toán tử khác như Convolution, chiếm tới 44% thời gian thực thi.

• Sự tương ứng prefill/decode độc đáo (Phần IV-B). Suy luận mô hình Diffusion giống với giai đoạn Prefill của suy luận LLM, trong khi các mô hình TTI dựa trên Transformer giống với giai đoạn Decode. Chúng tôi thấy rằng tăng tốc kernel Attention khi sử dụng Flash Attention lớn hơn 1.1-2.5x đối với các mô hình Diffusion so với các mô hình TTI dựa trên Transformer. Điều này cho thấy rằng các mô hình TTI dựa trên Diffusion và Transformer yêu cầu các kỹ thuật tối ưu hóa khác nhau.

• Độ dài chuỗi thay đổi cao (Phần V-A). Chúng tôi profile độ dài chuỗi trong suốt quá trình suy luận và thấy độ dài chuỗi trong các mô hình Diffusion như Stable Diffusion có thể thay đổi tới 4x. Độ dài chuỗi thay đổi này tác động đến cường độ tính toán trong suốt quá trình suy luận mô hình Diffusion, và tạo ra cơ hội để điều chỉnh thiết kế hệ thống.

• Phụ thuộc tỷ lệ với kích thước hình ảnh (Phần V-B). Chúng tôi thấy rằng yêu cầu bộ nhớ Attention tỷ lệ là O(L4), trong đó L là kích thước hình ảnh/latent. Kích thước hình ảnh lớn hơn thấy yêu cầu bộ nhớ cao hơn đáng kể. Hơn nữa, khi chúng tôi tăng kích thước hình ảnh lên độ phân giải cao hơn, chúng tôi thấy rằng thời gian thực thi Convolution trong các mô hình Diffusion tăng nhanh hơn Attention sau khi áp dụng các kỹ thuật tiên tiến như Flash Attention.

• Chiều thời gian (Phần VI). Chiều thời gian vốn có trong các mô hình TTV trình bày một nút thắt hệ thống mới so với các mô hình TTI mà chúng được xây dựng từ đó. Thông qua profiling, chúng tôi thấy rằng Temporal Attention bị thời gian thực thi chậm hơn 2x so với Spatial Attention, ngay cả khi nó yêu cầu 9x số lượng FLOP. Điều này cho thấy temporal attention là một nút thắt quan trọng cho thiết kế hệ thống.

II. HIỂU CÁC TÁC VỤ HỌC MÁY ĐA PHƯƠNG THỨC

Trước tiên, chúng tôi trình bày một phân loại được thông tin hệ thống về bối cảnh hiện tại của các mô hình tạo sinh văn bản thành hình ảnh (TTI) và văn bản thành video (TTV). Phát triển mô hình gần đây đã thấy một sự phân đôi của kiến trúc, mỗi loại được đặc trưng bởi quy trình tạo sinh hình ảnh và đặc điểm hệ thống riêng của chúng. Hình 2 minh họa pipeline tạo sinh hình ảnh cho hai lớp khối lượng công việc này: (i) Mô hình Diffusion, bao gồm cả mô hình dựa trên pixel và latent, và (ii) Mô hình dựa trên Transformer. Hai lớp khối lượng công việc tương tự này cũng tạo thành các khối xây dựng cơ bản của mô hình TTV - vì các mô hình tạo sinh video thường tạo ra một loạt hình ảnh (tức là khung hình) sử dụng mô hình TTI được huấn luyện trước, và sau đó đảm bảo các khung hình nhất quán về thời gian thông qua các lớp attention/convolutional thời gian bổ sung. Thảo luận và phân tích tiếp theo đề cập đến pipeline Suy luận của tạo sinh hình ảnh/văn bản. Không giống như LLM, các mô hình TTI/TTV bao gồm một số thành phần mô hình khác nhau được huấn luyện riêng biệt và sau đó được nối lại với nhau tại thời điểm suy luận.

A. Mô hình tạo sinh văn bản thành hình ảnh

1) Mô hình Diffusion: Mô hình Diffusion tạo ra hình ảnh trong một quy trình khử nhiễu lặp, trong đó một nhóm pixel ngẫu nhiên dần dần được biến đổi thành một hình ảnh tương tự như đầu vào văn bản đã cho [13]-[15]. Trong pipeline tạo sinh hình ảnh (Hình 2), một đầu vào văn bản đầu tiên được mã hóa thành một biểu diễn embedding trước khi được truyền vào mô hình diffusion, cùng với một tập hợp các pixel nhiễu ngẫu nhiên. Như được hiển thị trong Hình 3, kiến trúc mô hình diffusion tuân theo cấu trúc UNet, dần dần downsample và upsample một hình ảnh nhiễu thông qua các khối Attention và Resnet xen kẽ với các kích thước khác nhau [16], [17]. Lưu ý các khối Resnet xen kẽ với (i) các khối Self-Attention, điều kiện hóa việc tạo sinh trên chính hình ảnh đó, và (ii) Cross-Attention, tập trung vào đầu vào văn bản. Trước khi tạo ra đầu ra cuối cùng, hình ảnh đi qua UNet hàng chục hoặc hàng trăm lần như một phần của quy trình khử nhiễu, dẫn đến cường độ tính toán cao, tái sử dụng tham số thường xuyên và độ trễ dài (xem Phần II-C). Lưu ý có một sự đánh đổi vốn có giữa số lượng bước khử nhiễu và chất lượng hình ảnh.

Như được hiển thị trong Hình 2, có hai biến thể riêng biệt của mô hình diffusion từ góc độ hệ thống - dựa trên pixel và latent. Mô hình Pixel vs Latent được phân biệt bởi không gian tham số của mô hình diffusion và việc xử lý sau tiếp theo cần thiết một khi hình ảnh ban đầu được tạo ra. Trong khi các mô hình dựa trên pixel vận hành quy trình khử nhiễu trên các pixel hình ảnh tiêu chuẩn, các mô hình dựa trên latent biến đổi hình ảnh thành một biểu diễn embedding, làm cho nó hiệu quả hơn cho tính toán [17]. Kết quả là, các mô hình dựa trên latent có thể biểu diễn hình ảnh độ phân giải cao mà không cần đưa hình ảnh qua các mạng SuperResolution (SR) tiếp theo như trong trường hợp pixel. Điều này đi kèm với chi phí của một bộ giải mã dựa trên VAE hoặc GAN để chuyển đổi không gian latent trở lại không gian pixel một khi hoàn thành.

2) Mô hình Transformer: Ngược lại với bản chất lặp của mô hình Diffusion, các mô hình TTI dựa trên Transformer tạo ra hình ảnh tuần tự. Như được hiển thị trong Hình 2, các kiến trúc dựa trên transformer mô hình hóa việc tạo sinh hình ảnh như một tác vụ sequence-to-sequence, trong đó việc dự đoán pixel tiếp theo (hoặc patch hình ảnh) được điều kiện hóa trên tất cả các patch trước đó [18]. Lưu ý rằng các token hình ảnh được tạo ra từ mô hình transformer sau đó phải được giải mã thông qua bộ giải mã GAN hoặc tương đương để chuyển đổi trở lại biểu diễn hình ảnh. Hình 3 hiển thị một chế độ xem chi tiết của kiến trúc transformer cơ bản, bao gồm hai lớp attention đa đầu và một lớp feedforward, và không thay đổi so với LLM. Tuy nhiên, số lượng và sự sắp xếp của các khối transformer này thay đổi. So với GPT-3, có 96 lớp và một chiều mô hình là 12,288, Parti có 80 lớp và một chiều mô hình là 4096 [19], [20]. Các mô hình TTI dựa trên transformer khác như Muse có 48 lớp và một chiều ẩn là 2048 [21].

B. Mô hình văn bản thành video

Mô hình văn bản thành video mở rộng kiến trúc mô hình TTI truyền thống, và thường sử dụng các mô hình TTI được huấn luyện trước để tạo ra các khung hình ảnh riêng lẻ trước khi kết nối chúng với attention hoặc convolution thời gian. Giống như mô hình TTI, mô hình TTV có thể tuân theo cấu trúc mô hình dựa trên diffusion [22] hoặc dựa trên transformer [23]. Tuy nhiên, việc tạo ra một video gắn kết thời gian là thách thức từ góc độ hệ thống. Ví dụ, các lớp Temporal Attention thường được chèn sau các lớp Spatial Attention hiện có trong kiến trúc UNet (Hình 3), vì việc thêm một chiều thời gian bổ sung vào cuộc gọi Attention hiện có không khả thi từ góc độ bộ nhớ. Ngoài ra, các cuộc gọi Attention đôi khi được thay thế bằng các lớp Convolutional để giữ chi phí tính toán/bộ nhớ xuống, đặc biệt là trong các mô hình với độ phân giải cao hơn [24].

C. Không gian thiết kế hệ thống của mô hình văn bản thành hình ảnh/video

Để hiểu thêm về không gian thiết kế hệ thống cho các công nghệ AI tạo sinh đa phương thức mới nổi, Hình 4 minh họa các công nghệ tạo sinh TTI tiên tiến giữa các chiều thiết kế chính của độ chính xác mô hình (trục x) và số lượng tham số có thể huấn luyện (trục y). Các mô hình được gán nhãn là số trích dẫn của chúng [17]-[19], [21], [25]-[35]. Độ chính xác được đo thông qua các giá trị điểm FID được báo cáo trước đó [36] trên tập dữ liệu COCO [37]. Chúng tôi bỏ qua các mô hình không mã nguồn mở vì thiếu các triển khai công khai ngăn cản phân tích hệ thống sâu hơn. Như được hiển thị, các mô hình tối ưu nhất có xu hướng hướng về góc dưới bên trái với điểm FID thấp nhất và ít tham số nhất. Nói chung, mô hình Diffusion có xu hướng có chất lượng mô hình cao hơn cho cùng số lượng tham số như mô hình Transformer. Tuy nhiên, một tập hợp đa dạng các mô hình nằm trên đường cong Pareto-Optimal, bao gồm Imagen [25] (dựa trên pixel), Stable Diffusion [17] (dựa trên latent) và Parti (transformer), mô hình cuối cùng cung cấp chất lượng mô hình cao hơn so với các mô hình diffusion nhưng với số lượng tham số gấp 4 lần. Điều này minh họa tầm quan trọng của việc hiểu ý nghĩa hệ thống cho các kiến trúc khác nhau này.

Vượt ra ngoài số lượng tham số, tiếp theo chúng tôi phân loại các kiến trúc mô hình này theo các trục thiết kế hệ thống chính là tính toán, bộ nhớ và độ trễ. Bảng I làm nổi bật các thông số kỹ thuật và yêu cầu hệ thống của bốn mô hình từ đường cong Pareto-Optimal được hiển thị trong Hình 4, trong khi Hình 5 vẽ các mô hình này trên một roofline tương ứng với GPU A100. Lưu ý cường độ số học được tính như tỷ lệ FLOPs với dung lượng mô hình yêu cầu. Chúng tôi đầu tiên thấy rằng mô hình Diffusion có cường độ số học cao hơn so với mô hình TTI dựa trên Transformer. Điều này là do quy trình khử nhiễu vốn có trong mô hình Diffusion. Số lượng lớn các lần lặp qua UNet phát sinh số lượng FLOP cao trên một số lượng nhỏ tham số, dẫn đến việc tái sử dụng tham số cao. Quy trình khử nhiễu cũng phát sinh độ trễ cao, do số lượng lớn các lần lặp được yêu cầu. Ngược lại, mô hình TTI dựa trên Transformer có xu hướng bị giới hạn bởi bộ nhớ cho trường hợp kích thước batch thấp được hiển thị trong Hình 5. Kích thước batch thấp là phù hợp cho mô hình TTI. Mô hình TTI dựa trên Transformer yêu cầu ít tính toán và thường yêu cầu bộ nhớ cao hơn, đặc biệt là trong trường hợp của Parti [19]. Tuy nhiên, mô hình TTI transformer nói chung có độ trễ nhanh hơn so với quy trình diffusion lặp. Điều này đặc biệt đúng với các kỹ thuật được giới thiệu gần đây như parallel decoding [21].

III. PHƯƠNG PHÁP THỰC NGHIỆM

Chúng tôi xây dựng một bộ mô hình gồm tám khối lượng công việc đại diện cho các kiến trúc mô hình được giới thiệu trong Phần II, bao gồm so sánh với một mô hình tạo sinh văn bản thành văn bản tiên tiến, có sẵn công khai - LLaMA2 [12]. Ngoài bốn mô hình mã nguồn mở được làm nổi bật trong Phần II, chúng tôi tiếp tục mở rộng bộ mô hình của mình để cung cấp cái nhìn thực tế về yêu cầu hệ thống để triển khai ở quy mô lớn bằng cách bao gồm một mô hình TTI sản xuất. Chúng tôi đánh giá tất cả các mô hình trên phần cứng hệ thống thực và đo lường yêu cầu hệ thống của chúng.

Khối lượng công việc. Để xây dựng bộ mô hình, chúng tôi chọn các mô hình đại diện cho mỗi loại kiến trúc mô hình văn bản thành hình ảnh (Phần II). Cụ thể, chúng tôi bao gồm các mô hình nằm trên Đường cong Pareto-Optimal giữa chất lượng mô hình và số lượng tham số (xem Hình 4). Chúng tôi chọn Imagen như một mô hình diffusion dựa trên pixel đại diện, do sự hiện diện của nó trên đường cong Pareto Optimal. Imagen chứa một T5 Encoder để mã hóa thông tin văn bản và bao gồm ba mô hình diffusion: một để tạo ra hình ảnh 64x64, và hai mô hình super-resolution khác phục vụ để upsample hình ảnh lên 768x768 và 1024x1024, tương ứng. Ngoài ra, để đại diện cho một mô hình dựa trên latent, chúng tôi chọn một mô hình sử dụng kiến trúc Stable Diffusion được huấn luyện lại trên dữ liệu có giấy phép, bao gồm một bộ mã hóa văn bản CLIP, mô hình diffusion và một mô hình VQ VGAN. Đối với mô hình TTI dựa trên transformer, chúng tôi chọn Muse và Parti để thể hiện sự đa dạng của các kiến trúc transformer này, vì Muse là một mô hình transformer chỉ decoder sử dụng parallel decoding tại thời điểm suy luận, trong khi Parti là một mô hình encoder-decoder dự đoán token hình ảnh tự hồi quy. Chúng tôi cũng bao gồm hai mô hình TTV: Make-a-Video [22] được xây dựng dựa trên kiến trúc mô hình diffusion, và Phenaki [23] được phái sinh từ transformer.

Hệ thống phần cứng. Chúng tôi đánh giá huấn luyện và suy luận sử dụng GPU NVIDIA A100 80GB. Để phân tích suy luận, chúng tôi profile suy luận mô hình TTI/TTV trên một GPU duy nhất, vì các tham số mô hình có thể vừa trong các ràng buộc bộ nhớ 80 GB. Khi profiling huấn luyện mô hình, chúng tôi sử dụng Fully Sharded Data Parallelism (FSDP) để huấn luyện trên nhiều nút tính toán, trong đó mỗi nút bao gồm 8 GPU A100. Hình 6 hiển thị cách tính toán và giao tiếp tỷ lệ khi huấn luyện được phân phối trên số lượng nút tăng dần.

Công cụ. Để thực hiện đặc điểm hóa và phân tích tiếp theo của chúng tôi, chúng tôi sử dụng PyTorch Profiler để ghi lại các timeline thực thi. Chúng tôi đo thời gian kernel GPU và chú thích mô hình để theo dõi toán tử CPU nào tương ứng với một kernel khởi chạy GPU nhất định. Chúng tôi phát triển một framework profiling để tự động hóa quy trình này, thông qua việc chèn hook vào forward pass. Sau đó, chúng tôi phát triển script để phân tích hiệu quả đầu ra PyTorch Profiler kết quả và liên kết các kernel GPU với chú thích tương ứng của chúng để xác định sự phân tích toán tử, tăng tốc, v.v. Chúng tôi xây dựng một framework tương tự để tính toán FLOP một cách phân tích. Chúng tôi sử dụng công cụ NVIDIA Nsight Compute để kiểm tra các sự phân tích kernel và phân tích tỷ lệ hit cache, sử dụng bộ nhớ, v.v.

IV. ĐẶC ĐIỂM HÓA HIỆU SUẤT HỆ THỐNG

Hình 7 hiển thị sự phân tích thời gian toán tử (trục y) qua bộ mô hình được giới thiệu trong Phần III. Trên trục y, chúng tôi so sánh thời gian thực thi của pipeline forward pass (suy luận) được hiển thị trong Hình 2. Chúng tôi ghi lại sự phân tích thời gian thực thi của mô hình sử dụng Attention cơ bản (thanh trái), và vẽ thời gian thực thi được chuẩn hóa tương ứng sau khi Flash Attention V2 được áp dụng (thanh phải).

A. Phân tích nút thắt hiệu suất hệ thống

Chúng tôi đầu tiên kiểm tra sự phân tích thời gian thực thi của các mô hình cơ bản (trái). Chúng tôi quan sát sự đa dạng của các toán tử mô hình trong các mô hình TTI/TTV dựa trên Diffusion so với LLM truyền thống. Trong khi LLM và mô hình TTI dựa trên transformer chủ yếu bao gồm các lớp Attention và Linear, Convolution chiếm tới 36% thời gian trong các mô hình Diffusion cơ bản. Mô hình Diffusion cũng có một loạt các toán tử lớn hơn, với 4-11% thời gian thực thi được quy cho GroupNorm. Chúng tôi cũng quan sát thấy rằng các mô hình dựa trên pixel dành nhiều thời gian hơn 15% cho convolution so với các mô hình dựa trên latent. Điều này là do tần suất cao hơn của các toán tử convolution, vì các mô hình dựa trên pixel chứa các mạng super-resolution (SR) tuân theo kiến trúc UNet (Hình 2), nhưng thường hoán đổi các lớp attention cho convolution do yêu cầu bộ nhớ cấm đoán ở độ phân giải cao.

Đặc điểm hóa thời gian toán tử chi tiết tiết lộ Attention là một nút thắt hiệu suất hệ thống quan trọng trong các mô hình cơ bản - do Attention tiêu thụ khoảng 41.3% thời gian thực thi khi được tính trung bình qua bộ mô hình TTI/TTV. Để tăng tốc thời gian thực thi của Attention, Flash Attention được đề xuất gần đây cho thấy tiềm năng hiệu suất đáng kể trên GPU [8]. Kỹ thuật này về cơ bản cho phép các ma trận trung gian lớn được tạo ra trong tính toán Attention được chia ô, do đó giảm truy cập bộ nhớ và mang lại tăng tốc đáng kể. Lưu ý rằng trong khi Flash Attention ban đầu được thiết kế như một tối ưu hóa huấn luyện, công việc trước đây bao gồm DeepSpeed Inference Framework và các công việc khác đã cho thấy lợi ích của nó cho suy luận cũng như [38]. Ở đây chúng tôi kiểm tra tác động của việc áp dụng Flash Attention V2 qua các khối lượng công việc suy luận TTI/TTV.

Hình 7 hiển thị sự phân tích toán tử kết quả sau khi Flash Attention V2 được áp dụng, được chuẩn hóa theo thời gian thực thi cơ bản của một mô hình nhất định. Lưu ý rằng sau khi Flash Attention được áp dụng cho LLaMA hoặc mô hình TTI dựa trên transformer, Attention vẫn chiếm 37-45% tổng thời gian thực thi. Ngược lại, đối với mô hình Diffusion, Attention chỉ tiêu thụ 13-25% thời gian thực thi sau khi Flash Attention được áp dụng, với Convolution vẫn là khối toán tử đơn lẻ lớn nhất. Chúng tôi thấy rằng sau khi áp dụng Flash Attention cho mô hình Diffusion, nút thắt hiệu suất hệ thống chuyển sang các toán tử khác như Convolution thay vì Attention.

B. Đánh giá hiệu quả của Flash Attention qua bộ mô hình TTI/TTV

Chúng tôi cũng quan sát thấy rằng tăng tốc end-to-end của Flash Attention thay đổi từ 4-67% qua bộ mô hình. Theo Định luật Amdahl, tăng tốc tiềm năng của một tối ưu hóa như vậy bị tác động bởi hai yếu tố: (i) phần trăm thời gian dành cho Attention, và (ii) tăng tốc của chính mô-đun Attention. Trong khi phần trăm thời gian dành cho Attention thay đổi qua các kiến trúc mô hình, như được minh họa trong Phần IV-A, ở đây chúng tôi tập trung vào tăng tốc mô-đun Attention. Bằng cách kiểm tra Hình 7 và so sánh tăng tốc cô lập của mô-đun Attention (tức là thanh màu đỏ), chúng tôi thấy rằng tăng tốc mô-đun Attention từ Flash Attention lớn hơn 1.1-2.5x đối với mô hình Diffusion so với mô hình TTI dựa trên Transformer.

Để hiểu sự khác biệt quan sát được trong tăng tốc Attention qua bộ mô hình, chúng tôi lưu ý rằng tăng tốc kernel Attention thay đổi như một yếu tố của kích thước ma trận. Sau đó, chúng tôi phân tích tăng tốc mô hình TTI trong bối cảnh suy luận LLM truyền thống, bao gồm hai giai đoạn riêng biệt: Prefill và Decode. Prefill (tức là xử lý ban đầu của prompt) cho phép song song hóa lớn hơn, vì nó được đặc trưng bởi việc xử lý một vector truy vấn Nxd lớn, trong đó N là độ dài chuỗi và d là chiều mô hình. Một ma trận tương tự NxN lớn được tạo ra trong quá trình tính toán Attention, có lợi từ việc chia ô của Flash Attention do giảm truy cập bộ nhớ HBM [8]. Ngược lại, giai đoạn Decode tạo ra token tự hồi quy, với các truy vấn có kích thước 1xN. Điều này tạo ra một ma trận tương tự Nx1 nhỏ hơn, yêu cầu ít truy cập bộ nhớ hơn ngay từ đầu, và do đó thấy tăng tốc nhỏ hơn. Nói chung, Prefill tóm lại thành các tính toán ma trận-ma trận lớn trong khi Decode bao gồm các tính toán ma trận-vector.

Bảng III minh họa cách khái niệm Prefill và Decode này cho suy luận LLM ánh xạ lên các khối lượng công việc TTI. Đối với mô hình TTI dựa trên diffusion, tất cả pixel của một hình ảnh được tạo ra cùng lúc (xem Hình 2), do đó tạo ra các ma trận lớn giống với giai đoạn prefill của suy luận LLM. Điều này cho thấy Flash Attention có lợi cho suy luận mô hình Diffusion. Ngược lại, pixel hình ảnh (patch) được dự đoán tuần tự trong mô hình TTI dựa trên transformer do bản chất tự hồi quy của chúng, giống với việc giải mã. Lưu ý cũng rằng mô hình TTI transformer thấy ít tăng tốc từ Flash Attention so với LLM, do độ dài chuỗi nhỏ hơn và kích thước ma trận của chúng (như được thảo luận trong Phần V). Vì các mô hình LLM truyền thống như prefill/decode không áp dụng trong các khối lượng công việc TTI/TTV, điều này thúc đẩy nhu cầu hiểu các tính năng mô hình trong bối cảnh tạo sinh TTI/TTV để thiết kế các hệ thống tối ưu.

V. TÁC ĐỘNG CỦA ĐỘ DÀI CHUỖI

Như được minh họa bởi hiệu quả khác nhau của Flash Attention, các mô hình LLM truyền thống như prefill/decode không áp dụng cho các khối lượng công việc TTI và TTV dựa trên Diffusion. Để hiểu cách các mô hình TTI/TTV này hoạt động, chúng tôi phải dịch các khái niệm LLM khác, như độ dài chuỗi, vào bối cảnh tạo sinh hình ảnh/video. Điều này sẽ cho phép thiết kế hệ thống hiệu quả hơn.

LLM được đặc trưng bởi một chuỗi đại diện cho thông tin mà một mô hình có thể chú ý đến, tức là số lượng từ nó có thể tham chiếu khi tạo ra từ tiếp theo [39]. Tuy nhiên, độ dài chuỗi trong các mô hình TTI/TTV tiên tiến bị tác động trực tiếp bởi kích thước hình ảnh. Đặc biệt, độ dài chuỗi của mô hình diffusion tỷ lệ thuận với (kích thước hình ảnh)2, vì attention được tính toán giữa một phiên bản của hình ảnh và phiên bản tiếp theo. Trong mô hình TTI dựa trên transformer, độ dài chuỗi bị tác động bởi kích thước hình ảnh và kích thước embedding văn bản cùng nhau.

A. Profiling độ dài chuỗi

Hình 8 hiển thị độ dài chuỗi được profile trong suốt quá trình gọi suy luận cho các mô hình TTI khác nhau. Mỗi điểm trên trục x đại diện cho một thời điểm riêng biệt mà mô-đun Attention được gọi trong quá trình suy luận, trong khi trục y ghi lại độ dài chuỗi tương ứng. Mỗi biểu đồ được cắt để hiển thị chu kỳ cơ bản của mỗi mô hình, hoặc mẫu lặp lại tối thiểu. Lưu ý rằng đối với Parti, độ dài chuỗi tăng tuyến tính trong suốt quá trình suy luận. Điều này là do bản chất tự hồi quy của việc tạo sinh, nơi mỗi token được tạo ra trở thành một phần của prompt cho việc tạo sinh token tiếp theo. Muse sử dụng parallel decoding thay vì tạo sinh tự hồi quy, đó là lý do tại sao độ dài chuỗi là hằng số.

Ngược lại, độ dài chuỗi trong mô hình Diffusion (i) rất thay đổi, thường thể hiện một mẫu tuần hoàn, và (ii) có thể nhỏ hơn tới một bậc độ lớn so với LLM tương ứng. Lưu ý rằng bản chất hình chữ U của độ dài chuỗi là kết quả của các khối upsampling/downsampling trong mô hình diffusion, thay đổi kích thước của biểu diễn hình ảnh (Hình 3). Chiều rộng của hình dạng UNet được điều chỉnh bởi số lượng khối convolutional trong kiến trúc mô hình tương ứng, trong khi chiều cao phụ thuộc vào độ dài chuỗi bắt đầu được điều chỉnh bởi kích thước hình ảnh đầu ra mong muốn.

Lưu ý rằng chúng tôi cũng định lượng việc sử dụng băng thông bộ nhớ và tỷ lệ miss cache cấp cuối (LLC) cho mô hình Imagen và Parti trong cùng ROI như Hình 8. Chúng tôi thấy rằng việc sử dụng băng thông bộ nhớ và tỷ lệ miss LLC tuân theo cùng mẫu như độ dài chuỗi được minh họa. Điều này làm nổi bật tầm quan trọng của sự biến đổi độ dài chuỗi trong suốt quá trình suy luận mô hình và tác động của nó lên hệ thống bộ nhớ.

Chúng tôi phát triển một framework phân tích để mô hình hóa yêu cầu bộ nhớ và FLOP thay đổi trong suốt quá trình forward pass của mô hình Diffusion. Mô hình được xây dựng bằng cách kiểm tra tĩnh tính toán mô hình để phân tích yêu cầu bộ nhớ lý thuyết. Sau đó, chúng tôi thực hiện các lần chạy profiling bổ sung để đảm bảo độ chính xác của mô hình, và định lượng kích thước ma trận trong suốt quá trình thực thi mô hình để xác minh kích thước của các ma trận mà công thức của chúng tôi dự đoán.

Chúng tôi bắt đầu với kích thước hình ảnh mong muốn HO×WO, sau đó được downsample xuống không gian latent, HL×WL. Chúng tôi định nghĩa textencode là độ dài của prompt văn bản được mã hóa. Lưu ý rằng độ dài chuỗi bình phương cho các khối Self-Attention trong UNet dựa trên kích thước của biểu diễn latent, và được điều chỉnh bởi:

(HL·WL)·(HL·WL)

trong khi độ dài chuỗi bình phương cho các khối Cross-Attention cũng dựa trên mã hóa văn bản như được hiển thị:

HL·WL·(textencode)

Chúng tôi mô hình hóa bộ nhớ yêu cầu cho ma trận tương tự của một tính toán Attention trong công thức sau đây. Lưu ý rằng chúng tôi bỏ qua kích thước batch và giả sử 1 attention head cho đơn giản. Chúng tôi cũng giả sử FP16 (tức là 2 byte/param).

2·(HL·WL)·(HL·WL) + 2·(HL·WL)·(textencode)

2HLWL[HLWL+textencode]

Để nắm bắt tác động của sự biến đổi độ dài chuỗi đến từ các khối convolutional downsampling/upsampling trong UNet, chúng tôi mô hình hóa hệ số downsampling là dn, trong đó d là hệ số mà biểu diễn hình ảnh/latent được giảm, và n là giai đoạn diffusion. Độ dài chuỗi của một tính toán attention cụ thể phụ thuộc vào giai đoạn của UNet và hệ số downsampling tương ứng. Điều này tạo ra bản chất tuần hoàn được quan sát trong Hình 8. Sau đó, chúng tôi có công thức sau cho yêu cầu bộ nhớ tích lũy của ma trận tương tự trong suốt quá trình huấn luyện:

2·[∑(n=0 to 2unetdepth-1) HLWL/d2n[HLWL/d2n+textencode] + HLWL/d2unetdepth[HLWL/d2unetdepth+textencode]]

Phân tích của chúng tôi tiết lộ rằng vì độ dài chuỗi thay đổi trong suốt quá trình suy luận cho các mô hình dựa trên Diffusion này, yêu cầu bộ nhớ cũng vậy. Thực tế, có một mối quan hệ bậc hai giữa mã hóa latent (hoặc tổng quát hơn, kích thước hình ảnh) và độ dài chuỗi. Sau đó, do thực tế rằng bộ nhớ yêu cầu cho ma trận tương tự tỷ lệ theo hàm mũ với độ dài chuỗi, mối quan hệ giữa mã hóa latent và bộ nhớ là bậc bốn. Như các nhà thiết kế hệ thống, chúng tôi phải nhận thức rằng việc tăng độ phân giải hình ảnh hoặc kích thước latent có mối quan hệ O(L4) với yêu cầu bộ nhớ. Điều này đặt ra thách thức, đặc biệt là cho các mạng super-resolution hoạt động trên độ phân giải cao. Các mô hình này thường chỉnh sửa kiến trúc UNet để loại bỏ Attention vì yêu cầu bộ nhớ trở nên quá lớn.

Mối quan hệ giữa độ dài chuỗi và yêu cầu bộ nhớ dẫn đến các tối ưu hóa hệ thống tiềm năng. Ví dụ, các bước khử nhiễu khác nhau của quy trình diffusion có thể được sắp xếp lệch để cho phép sử dụng băng thông bộ nhớ tối đa tại bất kỳ thời điểm nào. Mặc dù các bước khử nhiễu theo truyền thống là tuần tự, một số bước nhất định có thể được nhóm lại với nhau thành các pod để cho phép tối ưu hóa hệ thống này mà không ảnh hưởng đáng kể đến dòng khử nhiễu.

B. Ý nghĩa của việc mở rộng kích thước hình ảnh

Sau đó, chúng tôi xây dựng một nghiên cứu trường hợp về mô hình Stable Diffusion, để hiểu cụ thể hơn về tác động của việc mở rộng kích thước hình ảnh (tức là HO,WO). Hình 9 quét các kích thước hình ảnh đầu vào khác nhau, và minh họa phân phối độ dài chuỗi tương ứng cho suy luận Stable Diffusion. Lưu ý rằng khi kích thước hình ảnh tăng, phân phối độ dài chuỗi dịch chuyển sang phải. Bằng cách kiểm tra trường hợp kích thước hình ảnh 512 x 512, chúng tôi lưu ý phân phối trên độ dài chuỗi tương đối bằng nhau, điều này xác nhận bản chất đối xứng của profile độ dài chuỗi cho Stable Diffusion được hiển thị trong Hình 8. Như được hiển thị, độ dài chuỗi tự giới hạn trong các bucket riêng biệt, có thể cho phép các hệ thống tương lai điều chỉnh phần cứng hướng tới độ dài chuỗi quan tâm.

Ngoài ra, chúng tôi tăng cường phân tích kernel Attention của mình bằng cách so sánh với cách mà các kernel Convolution tỷ lệ với kích thước hình ảnh. Chúng tôi một lần nữa sử dụng mô hình Stable Diffusion như một nghiên cứu trường hợp đại diện, và quét kích thước hình ảnh đầu ra mong muốn từ 64 x 64 đến 512 x 512, ghi lại Tổng thời gian thực thi. Biểu đồ kết quả được hiển thị trong Hình 10. Chúng tôi thấy rằng sau khi áp dụng các kỹ thuật như Flash Attention, Convolution thực sự có phụ thuộc tỷ lệ lớn hơn với kích thước hình ảnh so với Attention.

VI. Ý NGHĨA HỆ THỐNG CỦA CHIỀU THỜI GIAN

A. Temporal versus Spatial Attention

Mô hình văn bản thành video (TTV) thường tăng cường backbone TTI của chúng với temporal attention để kết nối các khung hình được tạo ra riêng lẻ từ mô hình văn bản thành hình ảnh. Chúng tôi thấy rằng Temporal Attention này thể hiện các nút thắt hệ thống độc đáo so với Spatial Attention truyền thống và khám phá những ý nghĩa này về thiết kế hệ thống tương lai.

Như được hiển thị trong Hình 11, các kích thước của các ma trận Q/K/V trong tính toán Attention [8] được sắp xếp lại cho Temporal Attention, với chiều mong muốn để chú ý được chuyển vào vị trí độ dài chuỗi, trong khi các chiều khác được chuyển vào kích thước batch. Cụ thể, trong Temporal Attention, độ dài chuỗi tương đương trở thành số lượng khung hình của việc tạo sinh video, vì mục tiêu của temporal attention là tạo ra một video gắn kết trong thời gian. Ngược lại, độ dài chuỗi của Spatial Attention được điều chỉnh bởi kích thước hình ảnh.

Sau đó, chúng tôi sử dụng mô hình Make-A-Video như một nghiên cứu trường hợp cho khối lượng công việc TTV, và profile các mô-đun Temporal và Spatial Attention. Chúng tôi thấy rằng Temporal Attention mất 2x thời gian thực thi của Spatial Attention (Hình 12a), mặc dù yêu cầu ít FLOP hơn 9x (Hình 12b) khi được profile trong Make-A-Video. Điều này cho thấy rằng tồn tại một nút thắt độc đáo trong Temporal Attention gây ra thời gian thực thi dài hơn đáng kể này mặc dù ít tính toán hơn.

Chúng tôi profile một loạt bộ đếm hiệu suất để so sánh thêm Temporal và Spatial Attention, bao gồm Misses Per Thousand Instructions (MPKI), Average Memory Access Time (AMAT), và Memory Bandwidth. Như Hình 13 hiển thị, chúng tôi thấy rằng các kernel GEMM và elementwise có L2 MPKI cao hơn 1.96x và 1.22x cho Temporal attention so với Spatial Attention. Chúng tôi cũng thấy AMAT cho các kernel GEMM, elementwise và softmax dài hơn 28, 4 và 18 chu kỳ cho Temporal Attention, tương ứng. Bằng cách thu thập một tập hợp các bộ đếm hiệu suất, chúng tôi tiến bộ hướng tới việc hiểu nút thắt hệ thống này, tuy nhiên còn nhiều công việc cần làm để hiểu đầy đủ bản chất của Temporal Attention.

B. Xu hướng trong Temporal Attention

Do Temporal Attention là một nút thắt hiệu suất hệ thống, chúng tôi tạo ra một benchmark dựa trên [40] để dự đoán yêu cầu tính toán trong tương lai. Hình 14 quét số lượng khung hình (trục x) và đo số lượng FLOP tương ứng (trục y), được tính toán bởi hai phép toán matmul chính trong Attention cho đơn giản. Trong khi số lượng FLOP của Spatial Attention tỷ lệ tuyến tính với số lượng khung hình tăng, số lượng FLOP của Temporal Attention tỷ lệ theo thời gian đa thức với phương trình y=AX2, do số lượng khung hình là độ dài chuỗi hiệu quả. Đối với số lượng khung hình nhỏ, Temporal Attention ít tốn kém tính toán hơn Spatial Attention. Tuy nhiên, khi số lượng khung hình tăng, Temporal Attention trở thành nút thắt hệ thống chiếm ưu thế. Lưu ý rằng việc tăng độ phân giải hình ảnh kéo dài điểm giao cắt, vì tính toán Spatial Attention bị tác động mạnh bởi độ phân giải hình ảnh.

Chúng tôi kết luận bằng cách phác thảo rằng để thiết kế các hệ thống hiệu quả cho mô hình TTV, chúng tôi phải dự đoán các xu hướng hướng tới (i) nhiều khung hình hơn, và (ii) độ phân giải cao hơn. Đầu tiên, các video hiện tại thường chỉ dài vài giây, với MakeA Video tạo ra các clip chỉ 2-3 giây. Trong khi nội suy khung hình có thể giúp mở rộng độ dài video với một số lượng khung hình cốt lõi nhất định, việc tạo sinh phim sẽ yêu cầu nhiều khung hình độc đáo hơn đáng kể để đại diện cho các khái niệm riêng biệt. Thứ hai, chúng tôi thấy xu hướng hướng tới độ phân giải cao hơn. Mô hình TTV hiện tại ngừng sử dụng Attention khi xử lý độ phân giải cao vì quá tốn kém bộ nhớ. Điều này thúc đẩy nhu cầu về các tối ưu hóa hệ thống TTV cho phép tạo sinh video dài, gắn kết.

VII. CÔNG VIỆC LIÊN QUAN

Công việc gần đây đã tập trung vào việc đặc trưng hóa các đặc điểm hệ thống của LLM, và đặc biệt là khối transformer. [1] phân tích độ trễ và việc sử dụng FLOPS mô hình (MFU) để hiểu cách thực hiện suy luận transformer hiệu quả. [41] phân tích tính không đồng nhất của các tính toán GEMM trong BERT. Flash Attention [8] đề xuất một kỹ thuật tiling để giảm truy cập HBM và do đó cải thiện nút thắt Attention. Flash Attention V2 [42] và Flash Decoding [43] tiếp tục giới thiệu thêm song song và tối ưu hóa suy luận, tương ứng.

Một lớp công việc khác đã tập trung vào việc tối ưu hóa mô hình TTI/TTV từ góc độ thuật toán. [3] cung cấp một tổng quan về tiến bộ gần đây trong nghiên cứu mô hình diffusion, và tập trung vào việc đánh giá hiệu quả tính toán của các mô hình này. [4] đề xuất một mô hình diffusion Multi-Architecture Multi-Expert để điều chỉnh tốt hơn các bước khác nhau của quy trình diffusion theo chức năng của chúng. Những người khác đề xuất sử dụng các kỹ thuật Retrieval Augment Generation (RAG) để bổ sung cho các kiến trúc mô hình [30].

VIII. KẾT LUẬN

Trong công việc này, chúng tôi trình bày một đặc trưng hóa hệ thống chi tiết của một lớp khối lượng công việc đa phương thức mới nổi. Chúng tôi thấy rằng các mô hình TTI dựa trên Diffusion thể hiện các nút thắt hệ thống độc đáo như Convolution sau khi Flash Attention được áp dụng. Chúng tôi cũng thấy rằng không giống như LLM, độ dài chuỗi thay đổi trong suốt quá trình suy luận cho mô hình Diffusion, làm phức tạp nhu cầu thiết kế hệ thống cho độ dài chuỗi tối ưu hơn là độ dài chuỗi lớn nhất có thể. Cuối cùng, chúng tôi điều tra mô hình TTV và thấy rằng temporal attention có thể sẽ trở thành một nút thắt ngày càng tăng khi chúng ta trưởng thành trong việc tạo sinh TTV. Thông qua phân tích hiệu suất hệ thống sâu sắc của chúng tôi, chúng tôi thực hiện bước đầu tiên quan trọng hướng tới việc thiết kế các hệ thống hiệu quả và có thể triển khai cho các khối lượng công việc TTI/TTV mới nổi.

LỜI CẢM ƠN

Nghiên cứu này không thể thực hiện được nếu không có các đồng nghiệp sau đây tại Meta. Chúng tôi muốn cảm ơn Uriel Singer, Adam Polyak, Yipin Zhou để hiểu các yêu cầu mô hình cho văn bản thành hình ảnh và mô hình ngôn ngữ lớn và cơ sở mã, Driss Guessous cho việc triển khai Scaled Dot Product Attention của PyTorch, và Henry Estela cho sự hỗ trợ của anh ấy với cơ sở hạ tầng huấn luyện.

TÀI LIỆU THAM KHẢO

[1] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, A. Levskaya, J. Heek, K. Xiao, S. Agrawal, và J. Dean, "Efficiently scaling transformer inference," trong Proceedings of Machine Learning and Systems, 2023.

[2] H. Shen, H. Chang, B. Dong, Y. Luo, và H. Meng, "Efficient llm inference on cpus," 2023.

[3] Z. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu, Q. Zhang, M. Chowdhury, và M. Zhang, "Efficient large language models: A survey," 2023.

[4] OpenAI, "Chatgpt," https://chat.openai.com/, 2023.

[5] Google, "Write with ai in google docs," 2023. [Online]. Available: https://support.google.com/docs/answer/13447609?hl=en

[6] Github, "Maximize developer velocity with ai," 2023. [Online]. Available: https://resources.github.com/copilot-for-business/

[7] A. Malik, "Openai's chatgpt now has 100 million weekly active users," 2023. [Online]. Available: https://techcrunch.com/2023/11/06/openais-chatgpt-now-has-100-million-weekly-active-users/

[8] T. Dao, D. Fu, S. Ermon, A. Rudra, và C. Ré, "Flashattention: Fast and memory-efficient exact attention with io-awareness," trong Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, và A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022, pp. 16 344–16 359.

[9] Meta, "https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/," Meta, 2023. [Online]. Available: https://about.fb.com/news/2023/09/introducing-ai-powered-assistants-characters-and-creative-tools/

[10] E. Chesbrough, "Turbocharge k–12 creativity with new generative ai features in adobe express for education," Adobe Express, 2023. [Online]. Available: https://www.adobe.com/express/learn/blog/edu-gen-ai-features#:~:text=Students%20can%20further%20express%20their,image%20that%20matches%20their%20vision.

[11] C. Zeni, R. Pinsler, D. Zügner, A. Fowler, M. Horton, X. Fu, S. Shysheya, J. Crabbé, L. Sun, J. Smith, R. Tomioka, và T. Xie, "Mattergen: a generative model for inorganic materials design," Tháng 12 năm 2023. [Online]. Available: https://www.microsoft.com/en-us/research/publication/mattergen-a-generative-model-for-inorganic-materials-design/

[12] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, và T. Scialom, "Llama 2: Open foundation and fine-tuned chat models," 2023.

[13] J. Ho, A. Jain, và P. Abbeel, "Denoising diffusion probabilistic models," trong Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, và H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 6840–6851.

[14] J. Song, C. Meng, và S. Ermon, "Denoising diffusion implicit models," 2021.

[15] Y. Song và S. Ermon, "Generative modeling by estimating gradients of the data distribution," trong Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, và R. Garnett, Eds., vol. 32. Curran Associates, Inc., 2019.

[16] O. Ronneberger, P. Fischer, và T. Brox, "U-net: Convolutional networks for biomedical image segmentation," trong Medical Image Computing and Computer-Assisted Intervention (MICCAI), ser. LNCS, vol. 9351. Springer, 2015, pp. 234–241, (available on arXiv:1505.04597 [cs.CV]).

[17] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, và B. Ommer, "High-resolution image synthesis with latent diffusion models," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Tháng 6 năm 2022, pp. 10 684–10 695.

[18] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, và I. Sutskever, "Zero-shot text-to-image generation," trong Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, M. Meila và T. Zhang, Eds., vol. 139. PMLR, 18–24 Jul 2021, pp. 8821–8831.

[19] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, B. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang, J. Baldridge, và Y. Wu, "Scaling autoregressive models for content-rich text-to-image generation," 2022.

[20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language Models are Few-Shot Learners," trong Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 1877–1901.

[21] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein, Y. Li, và D. Krishnan, "Muse: Text-to-image generation via masked generative transformers," 2023.

[22] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, và Y. Taigman, "Make-a-video: Text-to-video generation without text-video data," 2022.

[23] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, và D. Erhan, "Phenaki: Variable length video generation from open domain textual description," 2022.

[24] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, và T. Salimans, "Imagen video: High definition video generation with diffusion models," 2022.

[25] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, và M. Norouzi, "Photorealistic text-to-image diffusion models with deep language understanding," 2022.

[26] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, và Y. Taigman, "Make-a-scene: Scene-based text-to-image generation with human priors," trong Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XV. Berlin, Heidelberg: Springer-Verlag, 2022, p. 89–106.

[27] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, và J. Tang, "Cogview: Mastering text-to-image generation via transformers," trong Advances in Neural Information Processing Systems, M. Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, và J. W. Vaughan, Eds., vol. 34. Curran Associates, Inc., 2021, pp. 19 822–19 835.

[28] M. Ding, W. Zheng, W. Hong, và J. Tang, "Cogview2: Faster and better text-to-image generation via hierarchical transformers," 2022.

[29] A. Aghajanyan, B. Huang, C. Ross, V. Karpukhin, H. Xu, N. Goyal, D. Okhonko, M. Joshi, G. Ghosh, M. Lewis, và L. Zettlemoyer, "Cm3: A causal masked multimodal model of the internet," 2022.

[30] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, và W.-t. Yih, "Retrieval-augmented multimodal language modeling," trong Proceedings of the 40th International Conference on Machine Learning, ser. ICML'23. JMLR.org, 2023.

[31] C. Wu, J. Liang, L. Ji, F. Yang, Y. Fang, D. Jiang, và N. Duan, "Nüwa: Visual synthesis pre-training for neural visual world creation," 2021.

[32] Z. Feng, Z. Zhang, X. Yu, Y. Fang, L. Li, X. Chen, Y. Lu, J. Liu, W. Yin, S. Feng, Y. Sun, L. Chen, H. Tian, H. Wu, và H. Wang, "Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts," 2023.

[33] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, và M. Chen, "GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models," trong Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, và S. Sabato, Eds., vol. 162. PMLR, 17–23 Jul 2022, pp. 16 784–16 804. [Online]. Available: https://proceedings.mlr.press/v162/nichol22a.html

[34] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, và M. Chen, "Hierarchical text-conditional image generation with clip latents," 2022.

[35] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, và B. Guo, "Vector quantized diffusion model for text-to-image synthesis," 2022.

[36] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, và S. Hochreiter, "Gans trained by a two time-scale update rule converge to a local nash equilibrium," trong Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper/files/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf

[37] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, và P. Dollár, "Microsoft coco: Common objects in context," 2015.

[38] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, và Y. He, "Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale," trong Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, ser. SC '22. IEEE Press, 2022.

[39] Z. Zheng, X. Ren, F. Xue, Y. Luo, X. Jiang, và Y. You, "Response length perception and sequence scheduling: An llm-empowered llm inference pipeline," arXiv preprint arXiv:2305.13144, 2023.

[40] G. Bertasius, H. Wang, và L. Torresani, "Is space-time attention all you need for video understanding?" trong Proceedings of the International Conference on Machine Learning (ICML), Tháng 7 năm 2021.

[41] S. Pati, S. Aga, N. Jayasena, và M. D. Sinclair, "Demystifying bert: System design implications," trong 2022 IEEE International Symposium on Workload Characterization (IISWC), 2022, pp. 296–309.

[42] T. Dao, "Flashattention-2: Faster attention with better parallelism and work partitioning," 2023.

[43] T. Dao, D. Haziza, F. Massa, và G. Sizov, "Flash-decoding for long-context inference," 2023. [Online]. Available: https://crfm.stanford.edu/2023/10/12/flashdecoding.html
