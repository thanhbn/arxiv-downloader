# 2308.12966.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.12966.pdf
# Kích thước tệp: 6329953 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Qwen-VL: Một Mô hình Thị giác-Ngôn ngữ Đa năng cho
Hiểu biết, Định vị, Đọc văn bản và Hơn thế nữa
Jinze Bai∗Shuai Bai∗Shusheng Yang∗Shijie Wang Sinan Tan
Peng Wang Junyang Lin Chang Zhou†Jingren Zhou
Alibaba Group
Mã & Demo & Mô hình: https://github.com/QwenLM/Qwen-VL
Tóm tắt
Trong công trình này, chúng tôi giới thiệu chuỗi Qwen-VL, một tập hợp các mô hình thị giác-ngôn ngữ quy mô lớn (LVLMs) được thiết kế để nhận biết và hiểu cả văn bản và hình ảnh. Xuất phát từ Qwen-LM làm nền tảng, chúng tôi trang bị cho nó khả năng thị giác thông qua việc thiết kế tỉ mỉ (i) bộ thu thị giác, (ii) giao diện đầu vào-đầu ra, (iii) quy trình huấn luyện 3 giai đoạn, và (iv) tập dữ liệu đa phương tiện đa ngôn ngữ được làm sạch. Ngoài việc mô tả hình ảnh và hỏi đáp thông thường, chúng tôi thực hiện khả năng định vị và đọc văn bản của Qwen-VL bằng cách căn chỉnh các bộ ba hình ảnh-chú thích-khung. Các mô hình thu được, bao gồm Qwen-VL và Qwen-VL-Chat, thiết lập kỷ lục mới cho các mô hình tổng quát ở quy mô mô hình tương tự trên một loạt rộng các điểm chuẩn tập trung vào thị giác (ví dụ: chú thích hình ảnh, hỏi đáp, định vị thị giác) và các cài đặt khác nhau (ví dụ: zero-shot, few-shot). Hơn nữa, trên các điểm chuẩn hội thoại thực tế, Qwen-VL-Chat được điều chỉnh hướng dẫn của chúng tôi cũng thể hiện sự vượt trội so với các chatbot thị giác-ngôn ngữ hiện có. Tất cả các mô hình đều được công khai để hỗ trợ nghiên cứu tương lai.

Hình 1: Qwen-VL đạt hiệu suất tiên tiến trên một loạt rộng các tác vụ so với các mô hình tổng quát khác.
∗Đóng góp ngang nhau, †Tác giả liên hệ
1arXiv:2308.12966v3 [cs.CV] 13 Oct 2023

--- TRANG 2 ---
Hình 2: Một số ví dụ định tính được tạo bởi Qwen-VL-Chat của chúng tôi. Qwen-VL-Chat hỗ trợ nhiều đầu vào hình ảnh, hội thoại nhiều vòng, cuộc trò chuyện đa ngôn ngữ, đọc văn bản, định vị, khả năng nhận diện và hiểu tinh tế.

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Gao et al., 2023; Qwen, 2023) đã thu hút sự chú ý rộng rãi do khả năng mạnh mẽ của chúng trong việc tạo và hiểu văn bản. Các mô hình này có thể được căn chỉnh thêm với ý định của người dùng thông qua việc điều chỉnh hướng dẫn, thể hiện khả năng tương tác mạnh mẽ và tiềm năng tăng cường năng suất như các trợ lý thông minh. Tuy nhiên, các mô hình ngôn ngữ lớn bản địa chỉ sống trong thế giới văn bản thuần túy, thiếu khả năng xử lý các phương tiện khác thông thường (như hình ảnh, lời nói và video), dẫn đến hạn chế lớn trong phạm vi ứng dụng của chúng. Được thúc đẩy bởi điều này, một nhóm các Mô hình Ngôn ngữ Thị giác Lớn (LVLMs) (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2023c; Dai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al., 2023; Ye et al., 2023b,a; Chen et al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023; OpenAI, 2023) đã được phát triển để tăng cường các mô hình ngôn ngữ lớn với khả năng nhận biết và hiểu tín hiệu thị giác. Các mô hình thị giác-ngôn ngữ quy mô lớn này thể hiện tiềm năng đầy hứa hẹn trong việc giải quyết các vấn đề trung tâm thị giác trong thế giới thực.

Tuy nhiên, mặc dù đã có rất nhiều công trình được thực hiện để khám phá hạn chế và tiềm năng của LVLMs, các LVLMs mã nguồn mở hiện tại luôn gặp phải tình trạng huấn luyện và tối ưu hóa không đầy đủ, do đó tụt hậu xa so với các mô hình độc quyền (Chen et al., 2022, 2023b; OpenAI, 2023), điều này cản trở việc khám phá và ứng dụng LVLMs trong cộng đồng mã nguồn mở. Hơn nữa, vì các tình huống thị giác trong thế giới thực khá phức tạp, hiểu biết thị giác tinh tế đóng vai trò quan trọng để LVLMs hỗ trợ con người một cách hiệu quả và chính xác. Nhưng chỉ có một vài nỗ lực đã được thực hiện theo hướng này (Peng et al., 2023; Chen et al., 2023a), phần lớn các LVLMs mã nguồn mở vẫn nhận biết hình ảnh theo cách thô và thiếu khả năng thực hiện nhận biết tinh tế như định vị đối tượng hoặc đọc văn bản.

--- TRANG 3 ---
Trong bài báo này, chúng tôi khám phá một con đường và trình bày những thành viên mới nhất của gia đình Qwen mã nguồn mở: chuỗi Qwen-VL. Qwen-VL là một chuỗi các mô hình nền tảng thị giác-ngôn ngữ có hiệu suất cao và linh hoạt dựa trên mô hình ngôn ngữ Qwen-7B (Qwen, 2023). Chúng tôi trang bị cho nền tảng LLM khả năng thị giác bằng cách giới thiệu bộ thu thị giác mới bao gồm một bộ mã hóa thị giác được căn chỉnh ngôn ngữ và một bộ chuyển đổi nhận biết vị trí. Kiến trúc mô hình tổng thể cũng như giao diện đầu vào-đầu ra khá súc tích và chúng tôi đã thiết kế một cách tinh tế quy trình huấn luyện 3 giai đoạn để tối ưu hóa toàn bộ mô hình trên một bộ sưu tập khổng lồ các tập dữ liệu hình ảnh-văn bản.

Checkpoint tiền huấn luyện của chúng tôi, được gọi là Qwen-VL, có khả năng nhận biết và hiểu đầu vào thị giác, tạo ra các phản hồi mong muốn theo lời nhắc đã cho, và hoàn thành các tác vụ thị giác-ngôn ngữ khác nhau như chú thích hình ảnh, hỏi đáp, hỏi đáp định hướng văn bản, và định vị thị giác. Qwen-VL-Chat là chatbot thị giác-ngôn ngữ được điều chỉnh hướng dẫn dựa trên Qwen-VL. Như được thể hiện trong Hình 2, Qwen-VL-Chat có thể tương tác với người dùng và nhận biết hình ảnh đầu vào theo ý định của người dùng. Cụ thể, các tính năng của các mô hình chuỗi Qwen-VL bao gồm:

• Hiệu suất hàng đầu: Qwen-VL đạt độ chính xác hàng đầu trên rất nhiều điểm chuẩn hiểu biết tập trung vào thị giác so với các đối thủ có quy mô tương tự. Ngoài ra, hiệu suất điều chỉnh của Qwen-VL bao gồm không chỉ các điểm chuẩn thông thường như chú thích, hỏi đáp, định vị), mà còn một số điểm chuẩn hội thoại được giới thiệu gần đây.

• Đa ngôn ngữ: Tương tự như Qwen-LM, Qwen-VL được huấn luyện trên dữ liệu hình ảnh-văn bản đa ngôn ngữ với một lượng đáng kể tập dữ liệu bằng tiếng Anh và tiếng Trung. Theo cách này, Qwen-VL tự nhiên hỗ trợ hướng dẫn bằng tiếng Anh, tiếng Trung và đa ngôn ngữ.

• Nhiều hình ảnh: Trong giai đoạn huấn luyện, chúng tôi cho phép dữ liệu hình ảnh-văn bản xen kẽ tùy ý làm đầu vào của Qwen-VL. Tính năng này cho phép Qwen-Chat-VL của chúng tôi so sánh, hiểu và phân tích bối cảnh khi được cung cấp nhiều hình ảnh.

• Hiểu biết thị giác tinh tế: Nhờ kích thước đầu vào độ phân giải cao hơn và tập dữ liệu tinh tế mà chúng tôi sử dụng trong huấn luyện, Qwen-VL thể hiện khả năng hiểu biết thị giác tinh tế có tính cạnh tranh cao. So với các mô hình tổng quát thị giác-ngôn ngữ hiện có, Qwen-VL của chúng tôi sở hữu khả năng định vị, đọc văn bản, hỏi đáp định hướng văn bản, và hiệu suất hội thoại tinh tế tốt hơn nhiều.

2 Phương pháp
2.1 Kiến trúc Mô hình
Kiến trúc mạng tổng thể của Qwen-VL bao gồm ba thành phần và chi tiết các tham số mô hình được thể hiện trong Bảng 1:

Mô hình Ngôn ngữ Lớn: Qwen-VL áp dụng một mô hình ngôn ngữ lớn làm thành phần nền tảng. Mô hình được khởi tạo với trọng số tiền huấn luyện từ Qwen-7B (Qwen, 2023).

Bộ mã hóa Thị giác: Bộ mã hóa thị giác của Qwen-VL sử dụng kiến trúc Vision Transformer (ViT) (Dosovitskiy et al., 2021), được khởi tạo với trọng số tiền huấn luyện từ ViT-bigG của Openclip (Ilharco et al., 2021). Trong cả quá trình huấn luyện và suy luận, hình ảnh đầu vào được thay đổi kích thước theo độ phân giải cụ thể. Bộ mã hóa thị giác xử lý hình ảnh bằng cách chia chúng thành các patch với bước nhảy 14, tạo ra một tập hợp các đặc trưng hình ảnh.

Bộ chuyển đổi Thị giác-Ngôn ngữ Nhận biết Vị trí: Để giảm thiểu các vấn đề về hiệu quả phát sinh từ chuỗi đặc trưng hình ảnh dài, Qwen-VL giới thiệu một bộ chuyển đổi thị giác-ngôn ngữ nén các đặc trưng hình ảnh. Bộ chuyển đổi này bao gồm một mô-đun cross-attention một lớp được khởi tạo ngẫu nhiên. Mô-đun sử dụng một nhóm vector có thể huấn luyện (Embeddings) làm vector truy vấn và các đặc trưng hình ảnh từ bộ mã hóa thị giác làm khóa cho các thao tác cross-attention. Cơ chế này nén chuỗi đặc trưng thị giác thành độ dài cố định là 256. Việc ablation về số lượng truy vấn được thể hiện trong Phụ lục E.2. Ngoài ra, xem xét tầm quan trọng của thông tin vị trí đối với sự hiểu biết hình ảnh tinh tế, mã hóa vị trí tuyệt đối 2D được kết hợp vào các cặp truy vấn-khóa của cơ chế cross-attention để giảm thiểu khả năng mất chi tiết vị trí trong quá trình nén. Chuỗi đặc trưng hình ảnh được nén có độ dài 256 sau đó được đưa vào mô hình ngôn ngữ lớn.

Bảng 1: Chi tiết tham số mô hình Qwen-VL.
Bộ mã hóa Thị giác   Bộ chuyển đổi VL   LLM    Tổng
1.9B              0.08B            7.7B    9.6B

--- TRANG 4 ---
[Phần còn lại được dịch tiếp theo cùng định dạng và phong cách]

QwenLM
ViT 
Giai đoạn 1: Tiền huấn luyện
Cặp Hình ảnh-Văn bản
QwenLM Giai đoạn 2: Tiền huấn luyện Đa tác vụ
Dữ liệu VL Đa tác vụ và Xen kẽ
Giai đoạn 3: Điều chỉnh có Giám sát
ViT 
QwenLM
ViT 
Dữ liệu VL Xen kẽ Chat
Độ phân giải Thấp
Độ phân giải Cao
Độ phân giải Cao
CrossAttn LearnableQueryEmbs
CrossAttn LearnableQueryEmbs
CrossAttn LearnableQueryEmbs

Hình 3: Quy trình huấn luyện của chuỗi Qwen-VL.

2.2 Đầu vào và Đầu ra
Đầu vào Hình ảnh: Hình ảnh được xử lý qua bộ mã hóa thị giác và bộ chuyển đổi, tạo ra các chuỗi đặc trưng hình ảnh có độ dài cố định. Để phân biệt giữa đầu vào đặc trưng hình ảnh và đầu vào đặc trưng văn bản, hai token đặc biệt (<img> và </img>) được thêm vào đầu và cuối chuỗi đặc trưng hình ảnh tương ứng, báo hiệu sự bắt đầu và kết thúc của nội dung hình ảnh.

Đầu vào và Đầu ra Bounding Box: Để tăng cường khả năng hiểu biết thị giác tinh tế và định vị của mô hình, việc huấn luyện Qwen-VL bao gồm dữ liệu dưới dạng mô tả vùng, câu hỏi và phát hiện. Khác với các tác vụ thông thường bao gồm mô tả hoặc câu hỏi hình ảnh-văn bản, tác vụ này đòi hỏi mô hình phải hiểu và tạo ra mô tả vùng một cách chính xác theo định dạng được chỉ định. Đối với bất kỳ bounding box nào, quá trình chuẩn hóa được áp dụng (trong phạm vi [0, 1000)) và chuyển đổi thành định dạng chuỗi được chỉ định: "(Xtopleft, Ytopleft),(Xbottomright, Ybottomright)". Chuỗi này được token hóa như văn bản và không yêu cầu từ vựng vị trí bổ sung. Để phân biệt giữa chuỗi phát hiện và chuỗi văn bản thông thường, hai token đặc biệt (<box> và </box>) được thêm vào đầu và cuối chuỗi bounding box. Ngoài ra, để liên kết bounding box một cách thích hợp với các từ hoặc câu mô tả tương ứng, một tập token đặc biệt khác (<ref> và </ref>) được giới thiệu, đánh dấu nội dung được tham chiếu bởi bounding box.

--- TRANG 5 ---
3 Huấn luyện
Như được minh họa trong Hình 3, quá trình huấn luyện mô hình Qwen-VL gồm ba giai đoạn: hai giai đoạn tiền huấn luyện và giai đoạn cuối cùng là huấn luyện điều chỉnh hướng dẫn.

3.1 Tiền huấn luyện
Trong giai đoạn đầu tiên của tiền huấn luyện, chúng tôi chủ yếu sử dụng một tập hợp quy mô lớn các cặp hình ảnh-văn bản được thu thập từ web với nhãn yếu. Tập dữ liệu tiền huấn luyện của chúng tôi được cấu thành từ nhiều nguồn có thể truy cập công khai và một số dữ liệu nội bộ. Chúng tôi đã nỗ lực làm sạch tập dữ liệu khỏi các mẫu nhất định. Như được tóm tắt trong Bảng 2, tập dữ liệu gốc chứa tổng cộng 5 tỷ cặp hình ảnh-văn bản, và sau khi làm sạch, còn lại 1.4 tỷ dữ liệu, với 77.3% dữ liệu (văn bản) tiếng Anh và 22.7% dữ liệu (văn bản) tiếng Trung.

Bảng 2: Chi tiết dữ liệu tiền huấn luyện Qwen-VL. LAION-en và LAION-zh là tập con ngôn ngữ tiếng Anh và tiếng Trung của LAION-5B (Schuhmann et al., 2022a). LAION-COCO (Schuhmann et al., 2022b) là tập dữ liệu tổng hợp được tạo từ LAION-en. DataComp (Gadre et al., 2023) và Coyo (Byeon et al., 2022) là các bộ sưu tập cặp hình ảnh-văn bản. CC12M (Changpinyo et al., 2021), CC3M (Sharma et al., 2018), SBU (Ordonez et al., 2011) và COCO Caption (Chen et al., 2015) là các tập dữ liệu chú thích học thuật.

[Bảng với dữ liệu về ngôn ngữ, tập dữ liệu, số lượng gốc, đã làm sạch, và phần trăm còn lại]

Chúng tôi đóng băng mô hình ngôn ngữ lớn và chỉ tối ưu hóa bộ mã hóa thị giác và bộ chuyển đổi VL trong giai đoạn này. Hình ảnh đầu vào được thay đổi kích thước thành 224×224. Mục tiêu huấn luyện là tối thiểu hóa entropy chéo của các token văn bản. Tốc độ học tối đa là 2e−4 và quá trình huấn luyện sử dụng kích thước batch 30720 cho các cặp hình ảnh-văn bản, và toàn bộ giai đoạn tiền huấn luyện đầu tiên kéo dài 50,000 bước, tiêu thụ khoảng 1.5 tỷ mẫu hình ảnh-văn bản. Các siêu tham số khác được chi tiết trong Phụ lục C và đường cong hội tụ của giai đoạn này được thể hiện trong Hình 6.

3.2 Tiền huấn luyện Đa tác vụ
Trong giai đoạn thứ hai của tiền huấn luyện đa tác vụ, chúng tôi giới thiệu dữ liệu chú thích VL chất lượng cao và tinh tế với độ phân giải đầu vào lớn hơn và dữ liệu hình ảnh-văn bản xen kẽ. Như được tóm tắt trong Bảng 3, chúng tôi đã huấn luyện Qwen-VL trên 7 tác vụ đồng thời. Đối với việc tạo văn bản, chúng tôi sử dụng tập dữ liệu thu thập nội bộ để duy trì khả năng của LLM. Dữ liệu chú thích giống như Bảng 2 ngoại trừ ít mẫu hơn và loại trừ LAION-COCO. Chúng tôi sử dụng hỗn hợp dữ liệu có sẵn công khai cho tác vụ VQA bao gồm GQA (Hudson và Manning, 2019), VGQA (Krishna et al., 2017), VQAv2 (Goyal et al., 2017), DVQA (Kafle et al., 2018), OCR-VQA (Mishra et al., 2019) và DocVQA (Mathew et al., 2021). Chúng tôi tuân theo Kosmos-2 để sử dụng tập dữ liệu GRIT (Peng et al., 2023) cho tác vụ định vị với các sửa đổi nhỏ. Đối với tác vụ định vị tham chiếu và chú thích định vị kép, chúng tôi xây dựng mẫu huấn luyện từ GRIT (Peng et al., 2023), Visual Genome (Krishna et al., 2017), RefCOCO (Kazemzadeh et al., 2014), RefCOCO+ và RefCOCOg (Mao et al., 2016).

--- TRANG 6 ---
[Tiếp tục dịch nội dung còn lại với cùng phong cách và định dạng]

Bảng 3: Chi tiết dữ liệu tiền huấn luyện đa tác vụ Qwen-VL.
Tác vụ                      # Mẫu    Tập dữ liệu
Chú thích                   19.7M    LAION-en & zh, DataComp, Coyo, CC12M & 3M, SBU, COCO, Dữ liệu Nội bộ
VQA                         3.6M     GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D
Định vị                     23.5M    GRIT
Định vị Tham chiếu          8.7M     GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg
Chú thích Định vị           8.7M     GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg
OCR                         24.8M    SynthDoG-en & zh, Common Crawl pdf & HTML
Tự hồi quy Văn bản Thuần    7.8M     Dữ liệu Nội bộ

Chúng tôi tăng độ phân giải đầu vào của bộ mã hóa thị giác từ 224×224 lên 448×448, giảm mất mát thông tin do thu nhỏ hình ảnh gây ra. Ngoài ra, chúng tôi thực hiện ablation window attention và global attention cho độ phân giải cao hơn của vision transformer trong Phụ lục E.3. Chúng tôi mở khóa mô hình ngôn ngữ lớn và huấn luyện toàn bộ mô hình. Mục tiêu huấn luyện giống như giai đoạn tiền huấn luyện.

3.3 Điều chỉnh có Giám sát
Trong giai đoạn này, chúng tôi điều chỉnh mô hình tiền huấn luyện Qwen-VL thông qua điều chỉnh hướng dẫn để tăng cường khả năng tuân theo hướng dẫn và hội thoại, tạo ra mô hình Qwen-VL-Chat tương tác. Dữ liệu điều chỉnh hướng dẫn đa phương tiện chủ yếu đến từ dữ liệu chú thích hoặc dữ liệu hội thoại được tạo thông qua tự hướng dẫn LLM, thường chỉ đề cập đến hội thoại và lý luận một hình ảnh và bị giới hạn trong việc hiểu nội dung hình ảnh. Chúng tôi xây dựng một tập hợp dữ liệu hội thoại bổ sung thông qua chú thích thủ công, tạo mô hình và nối chuỗi chiến lược để kết hợp khả năng định vị và hiểu nhiều hình ảnh vào mô hình Qwen-VL. Chúng tôi xác nhận rằng mô hình chuyển giao hiệu quả các khả năng này sang một loạt rộng hơn các ngôn ngữ và loại câu hỏi. Ngoài ra, chúng tôi trộn dữ liệu hội thoại đa phương tiện và văn bản thuần túy trong quá trình huấn luyện để đảm bảo tính toàn diện của mô hình trong khả năng hội thoại. Dữ liệu điều chỉnh hướng dẫn có số lượng 350k. Trong giai đoạn này, chúng tôi đóng băng bộ mã hóa thị giác và tối ưu hóa mô hình ngôn ngữ và mô-đun chuyển đổi. Chúng tôi trình bày định dạng dữ liệu của giai đoạn này trong Phụ lục B.2.

4 Đánh giá
Trong phần này, chúng tôi tiến hành đánh giá tổng thể trên các tác vụ đa phương tiện khác nhau để đánh giá toàn diện khả năng hiểu biết thị giác của mô hình. Trong phần tiếp theo, Qwen-VL biểu thị mô hình sau giai đoạn huấn luyện đa tác vụ, và Qwen-VL-Chat biểu thị mô hình sau giai đoạn điều chỉnh có giám sát (SFT).

Bảng 9 cung cấp tóm tắt chi tiết về các điểm chuẩn đánh giá được sử dụng và các số liệu tương ứng.

4.1 Chú thích Hình ảnh và Hỏi đáp Thị giác Tổng quát
Chú thích hình ảnh và hỏi đáp thị giác tổng quát (VQA) là hai tác vụ thông thường cho các mô hình thị giác-ngôn ngữ. Cụ thể, chú thích hình ảnh yêu cầu mô hình tạo ra mô tả cho hình ảnh đã cho và VQA tổng quát yêu cầu mô hình tạo ra câu trả lời cho cặp hình ảnh-câu hỏi đã cho.

[Tiếp tục dịch phần còn lại theo cùng phong cách]
