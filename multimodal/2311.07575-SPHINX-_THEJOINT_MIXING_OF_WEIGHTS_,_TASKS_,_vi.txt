# 2311.07575.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.07575.pdf
# Kích thước tệp: 8907983 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SPHINX: SỰ KẾT HỢP CHUNG CỦA TRỌNG SỐ, NHIỆM VỤ,
VÀ NHÚNG TRỰC QUAN CHO CÁC MÔ HÌNH NGÔN NGỮ
LỚN ĐA PHƯƠNG THỨC
Ziyi Lin1,2∗, Chris Liu1∗, Renrui Zhang1,2∗, Peng Gao1∗†‡, Longtian Qiu1,3∗
Han Xiao1, Han Qiu1, Chen Lin1, Wenqi Shao1, Keqin Chen1, Jiaming Han1,2
Siyuan Huang1, Yichi Zhang1, Xuming He3, Hongsheng Li1,2†, Yu Qiao1†
1Phòng thí nghiệm AI Thượng Hải,2MMLab, CUHK,3Đại học Công nghệ Thượng Hải
TÓM TẮT
Chúng tôi giới thiệu SPHINX, một mô hình ngôn ngữ lớn đa phương thức (MLLM) đa năng với sự kết hợp chung của trọng số mô hình, nhiệm vụ điều chỉnh, và nhúng trực quan. Đầu tiên, để căn chỉnh thị giác-ngôn ngữ mạnh mẽ hơn, chúng tôi bỏ đóng băng mô hình ngôn ngữ lớn (LLM) trong quá trình tiền huấn luyện, và giới thiệu chiến lược kết hợp trọng số giữa các LLM được huấn luyện bằng dữ liệu thực tế và tổng hợp. Bằng cách tích hợp trực tiếp các trọng số từ hai lĩnh vực, LLM kết hợp có thể kết hợp hiệu quả các ngữ nghĩa đa dạng với độ bền thuận lợi. Sau đó, để kích hoạt khả năng đa mục đích, chúng tôi kết hợp nhiều nhiệm vụ khác nhau để điều chỉnh hướng dẫn thị giác chung, và thiết kế các hướng dẫn cụ thể theo nhiệm vụ để tránh xung đột giữa các nhiệm vụ. Ngoài việc trả lời câu hỏi thị giác cơ bản, chúng tôi bao gồm các nhiệm vụ thách thức hơn như hiểu cấp vùng, nền tảng chú thích, phát hiện bố cục tài liệu, và ước tính tư thế con người, góp phần nâng cao lẫn nhau qua các tình huống khác nhau. Ngoài ra, chúng tôi đề xuất trích xuất nhúng trực quan toàn diện từ các kiến trúc mạng khác nhau, mô hình tiền huấn luyện, và độ chi tiết thông tin, cung cấp cho các mô hình ngôn ngữ các biểu diễn hình ảnh mạnh mẽ hơn. Dựa trên sự kết hợp chung được đề xuất, SPHINX thể hiện khả năng hiểu đa phương thức vượt trội trên nhiều ứng dụng. Trên cơ sở này, chúng tôi tiếp tục đề xuất một chiến lược hiệu quả nhằm nắm bắt tốt hơn các diện mạo chi tiết của hình ảnh độ phân giải cao. Với sự kết hợp của các tỷ lệ khác nhau và hình ảnh phụ độ phân giải cao, SPHINX đạt được hiệu suất phân tích và lý luận thị giác đặc biệt trên các tiêu chuẩn đánh giá hiện có. Chúng tôi hy vọng công trình của chúng tôi có thể làm sáng tỏ việc khám phá sự kết hợp chung trong nghiên cứu MLLM tương lai. Mã nguồn được phát hành tại https://github.com/Alpha-VLLM/
LLaMA2-Accessory.

1 GIỚI THIỆU
Từ kỷ nguyên dữ liệu lớn, các mô hình ngôn ngữ lớn (LLM) đã đạt được những bước tiến to lớn (OpenAI, 2023a;b; Brown et al., 2020; Touvron et al., 2023a; Zhang et al., 2022), thể hiện các tình huống ứng dụng và khả năng tổng quát hóa chưa từng có. Để mở rộng thêm trần khả năng của chúng, các hình ảnh thị giác cũng được giới thiệu như đầu vào để phát triển các mô hình ngôn ngữ lớn đa phương thức (MLLM) mạnh mẽ (Zhang et al., 2023a; Li et al., 2023d; Liu et al., 2023d; Zhu et al., 2023; Zhao et al., 2023). Các phương pháp này không chỉ có thể tạo ra các phản hồi ngôn ngữ được tổ chức tốt được kế thừa từ LLM, mà còn mở khóa khả năng hiểu đa phương thức cho nhiều ứng dụng, như cung cấp chú thích hình ảnh chi tiết, trả lời câu hỏi thị giác, định vị các đối tượng khác nhau trên hình ảnh, v.v.

Các MLLM hiện có đã khám phá nhiều chiến lược khác nhau để trang bị cho LLM khả năng tuân theo hướng dẫn thị giác. 1) Đóng băng LLM trong quá trình tiền huấn luyện, và chỉ học một mạng chiếu để căn chỉnh thị giác-ngôn ngữ, ví dụ, một lớp MLP đơn giản của LLaMA-Adapter V2 (Gao et al., 2023b) và một abstractor thị giác dựa trên attention của mPLUG-Owl (Ye et al., 2023). 2) Xây dựng dữ liệu huấn luyện của các nhiệm vụ mới để trang bị cho MLLM khả năng hiểu thị giác mới, ví dụ, các cuộc đối thoại tham chiếu của Kosmos-2 (Peng et al., 2023b) và nền tảng cấp vùng của Shikra (Chen et al., 2023b). 3) Sử dụng các bộ mã hóa hình ảnh tiên tiến để trích xuất nhúng thị giác, ví dụ, bộ mã hóa CLIP (Radford et al., 2021) trong LLaVA (Liu et al., 2023c) và Q-Former (Li et al., 2023d) trong MiniGPT-4 (Zhu et al., 2023).

Trong bài báo này, chúng tôi đề xuất một MLLM đa năng, SPHINX, với sự kết hợp của bốn khía cạnh quan trọng: trọng số mô hình, nhiệm vụ điều chỉnh, nhúng thị giác, và hình ảnh phụ độ phân giải cao. Các đặc điểm chính và phát hiện của phương pháp chúng tôi được minh họa như sau:

•Bỏ đóng băng LLM để tiền huấn luyện. Mặc dù LLM đóng băng có thể bảo tồn hiệu quả khả năng tạo câu dài, nó hạn chế tiềm năng căn chỉnh đa phương thức tốt hơn thông qua việc tiền huấn luyện thêm trên dữ liệu thị giác-ngôn ngữ. Do đó, chúng tôi bỏ đóng băng toàn bộ LLM, và kết hợp các bộ dữ liệu thị giác-ngôn ngữ (Schuhmann et al., 2021) để căn chỉnh đa phương thức và RefinedWeb (Penedo et al., 2023) để điều chỉnh cụ thể ngôn ngữ. Chiến lược tiền huấn luyện này không chỉ cho phép LLM học thêm kiến thức đa phương thức, mà còn giảm thiểu vấn đề quên để tạo ra các phản hồi ngôn ngữ chi tiết.

•Trọng số mô hình kết hợp. Dữ liệu thị giác-ngôn ngữ từ các lĩnh vực cụ thể có thể chứa ngữ nghĩa đặc biệt, ví dụ, chú thích tổng hợp (Schuhmann et al., 2022) so với chú thích thế giới thực (Schuhmann et al., 2021). Xem xét rằng việc kết hợp trực tiếp dữ liệu như vậy có thể gây nhầm lẫn cho MLLM, chúng tôi giới thiệu chiến lược kết hợp trọng số để kết hợp hiệu quả kiến thức cụ thể theo lĩnh vực. Dựa trên MLLM được tiền huấn luyện trên dữ liệu thế giới thực, chúng tôi tinh chỉnh nó trên dữ liệu tổng hợp, và sau đó kết hợp tuyến tính trọng số LLM được tinh chỉnh với trọng số thế giới thực. Bằng cách này, hai loại mô hình sẽ không bị ảnh hưởng bởi dữ liệu mâu thuẫn và SPHINX cuối cùng của chúng tôi có thể tích hợp hiệu quả kiến thức từ cả lĩnh vực tổng hợp và thế giới thực.

•Nhiệm vụ điều chỉnh kết hợp. Khác với các mô hình MLLM cụ thể theo nhiệm vụ hiện có (Ye et al., 2023; Peng et al., 2023b; Chen et al., 2023b; Liu et al., 2023d; Gao et al., 2023b), chúng tôi tích hợp một tập hợp đa dạng các nhiệm vụ hướng dẫn thị giác để điều chỉnh mô hình được tiền huấn luyện, nhằm mục đích có được nhiều khả năng. Sự kết hợp nhiệm vụ của chúng tôi bao gồm trả lời câu hỏi thị giác cơ bản (VQA), hiểu/tạo biểu thức tham chiếu cấp vùng (REC/REG), phát hiện đa đối tượng và lý luận mối quan hệ, VQA biểu đồ/tài liệu hướng văn bản, ước tính tư thế con người, v.v. Bằng mô hình huấn luyện đa nhiệm vụ toàn diện như vậy, SPHINX của chúng tôi là một mô hình tổng quát hoạt động tốt để tuân theo hướng dẫn thị giác.

•Nhúng thị giác kết hợp. Để tận dụng lợi thế của các bộ mã hóa khác nhau, chúng tôi đề xuất kết hợp nhúng thị giác từ nhiều backbone thị giác khác nhau (Oquab et al., 2023; Li et al., 2023d; Radford et al., 2021) với các kiến trúc mạng khác nhau (CNN vs. ViT), mô hình tiền huấn luyện (có giám sát vs. tự giám sát), và độ chi tiết thông tin (toàn cục vs. cục bộ). Bằng cách kết hợp các token hình ảnh khác nhau theo kênh và theo chuỗi, SPHINX thu được biểu diễn thị giác mạnh mẽ hơn và dẫn đến hiệu quả căn chỉnh thị giác-ngôn ngữ tốt hơn.

Trên cơ sở này, chúng tôi tiếp tục điều tra một vấn đề thách thức khác trong các MLLM hiện có, tức là độ phân giải hạn chế của hình ảnh đầu vào. Vì các bộ mã hóa hình ảnh được tiền huấn luyện thường áp dụng độ phân giải hình ảnh tương đối thấp, ví dụ, 224×224, nó cản trở nghiêm trọng việc hiểu và lý luận thị giác chi tiết cho MLLM. Tuy nhiên, việc đơn giản tăng mẫu hình ảnh cho các bộ mã hóa sẽ làm hại tiên nghiệm vị trí được tiền huấn luyện, và quan trọng hơn, dẫn đến chi phí tính toán đắt đỏ (độ phức tạp tăng theo bình phương kích thước hình ảnh trong cơ chế tự chú ý). Do đó, chúng tôi đề xuất trang bị cho SPHINX một chuỗi dài hơn các nhúng thị giác của sự kết hợp các tỷ lệ khác nhau và hình ảnh phụ độ phân giải cao.

•Tỷ lệ kết hợp và hình ảnh phụ độ phân giải cao. chúng tôi đầu tiên chia không gian hình ảnh đầu vào độ phân giải cao thành nhiều hình ảnh phụ, và cũng giảm mẫu nó thành một hình ảnh độ phân giải thấp. Sau đó, chúng tôi đưa tất cả các hình ảnh đồng thời vào các bộ mã hóa thị giác kết hợp, và nối các nhóm token được trích xuất nhiều để biểu diễn toàn bộ các đặc trưng thị giác độ phân giải cao. Bằng cách kết hợp nhúng thị giác của các tỷ lệ và hình ảnh phụ khác nhau, SPHINX của chúng tôi có thể khám phá thích ứng ngữ nghĩa thị giác chi tiết hơn từ độ phân giải cao và biểu diễn hình ảnh đa tỷ lệ, trong khi duy trì hiệu quả mã hóa.

--- TRANG 2 ---
Người dùng: Cung cấp chú thích một câu 
cho hình ảnh được cung cấp.
Người dùng:
Cảm giác [0.05,0.32,0.54,0.83] 
như thế nào trong hình ảnh? 
SPHINX :
 Lý luận thị giác với phát hiện
Ước tính tư thế con ngườiChú thích
 Tạo biểu thức 
tham chiếu
Các điểm quan trọng được phát hiện là 
mũi[0.42,0.15], 
mắt trái[0.43,0.13], 
tai trái[0.50,0.13], 
vai trái[0.55,0.20], 
vai phải[0.50,0.18], 
khuỷu tay trái[0.47,0.35], 
khuỷu tay phải[0.47,0.33], 
cổ tay trái[0.39,0.44], 
cổ tay phải[0.39,0.43], 
hông trái[0.65,0.45], 
hông phải[0.60,0.44], 
đầu gối trái[0.52,0.56], 
đầu gối phải[0.47,0.46], 
mắt cá chân trái[0.59,0.82], 
và mắt cá chân phải[0.61,0.63].SPHINX : 
Người dùng:
Phát hiện các điểm quan trọng của người 
trong vùng [0.34,0.06,0.73,0.89].Trả lời câu hỏi 
thị giác
Hiểu biểu thức 
tham chiếu
Bìa sách dành cho trẻ em có tựa đề 
If You Give a Dog a Donut.SPHINX : 
Người dùng:
Mô tả hình ảnh một cách súc tích. 
Bao gồm hộp giới hạn cho mỗi 
đối tượng được đề cập.
SPHINX : 
Hình ảnh là một bức tranh hoạt hình chính trị châm biếm có hai người 
đàn ông mặc đồng phục quân sự, một trong số họ đang cắt một quả 
địa cầu bằng kiếm. Bức tranh có thể đang bình luận về một vấn đề 
chính trị hoặc xã hội, như việc phân chia một lãnh thổ hoặc chia sẻ 
tài nguyên. Đồng phục quân sự của những người đàn ông và hành 
động cắt quả địa cầu cho thấy cảm giác quyền lực và kiểm soát, 
trong khi thanh kiếm tượng trưng cho việc sử dụng vũ lực hoặc 
xâm lược. Ý nghĩa sâu sắc của bức tranh là truyền đạt thông điệp 
về hậu quả của những hành động như vậy, có thể cảnh báo về nguy 
hiểm của việc phân chia hoặc chia rẽ.
Người dùng:
Ý nghĩa sâu sắc của bức 
tranh này là gì?
Dựa trên hình ảnh, rất khó 
để xác định chính xác cảm 
xúc của người đàn ông. Tuy 
nhiên, anh ta đang ngồi trên 
ghế và có vẻ như đang tham 
gia cuộc trò chuyện với nhóm 
người xung quanh. Người đàn 
ông đang cầm một cuốn sách, 
điều này cho thấy anh ta có 
thể đang thảo luận hoặc dạy 
một chủ đề liên quan đến cuốn 
sách. Sự hiện diện của nhiều 
người trong phòng chỉ ra rằng 
người đàn ông có thể đang 
trong một môi trường xã hội 
hoặc giáo dục, nơi anh ta 
đang chia sẻ kiến thức hoặc 
ý kiến với người khác. Cảm 
xúc của người đàn ông có thể 
từ bình tĩnh và tập trung đến 
đam mê và tham gia vào cuộc 
thảo luận.Người dùng:
Vui lòng cung cấp tọa độ 
hộp giới hạn của vùng mà 
câu này mô tả: 
Con mèo.
SPHINX : 
[0.37,0.32,0.90,0.58]
Phát hiện bố cục tài liệu<p>+</p >[0.14, 0.56, 0.16, 0.59].<p>+</p >[0.86, 0.47, 0.87, 0.5].<p>+</p 
>[0.94, 0.62, 0.95, 0.65].<p>Text</p >[0.45, 0.33, 0.48, 
0.34].<p>Start</p >[0.37, 0.39, 0.4, 0.41].<p>Start</p >[0.37, 0.46, 0.4, 
0.48].<p>Start</p >[0.37, 0.51, 0.4, 0.53].<p>Start</p >[0.37, 0.58, 0.4, 
0.59].<p>Start</p >[0.37, 0.62, 0.4, 0.64].<p>Start</p >[0.37, 0.67, 0.4, 
0.69].<p>Start</p >[0.37, 0.32, 0.4, 0.34].<p>Task </p >[......
Người dùng:
Vui lòng phát hiện tất cả các văn bản và cung cấp tọa độ hộp giới 
hạn của chúng.
SPHINX : 
SPHINX :
 Một con hươu[0.39,0.19,1.00,0.71] đang ăn một 
quả bí ngô[0.38,0.41,0.66,0.70] .
Người dùng:
Tại sao hình ảnh này hài hước?
SPHINX :
 Hình ảnh hài hước vì một 
con hươu[0.39,0.19,1.00,0.71] đang ăn một 
quả bí ngô[0.38,0.41,0.66,0.70] , đây là 
cảnh tượng hài hước vì 
hươu[0.39,0.19,1.00,0.71] thường ăn 
lá[0.00,0.19,1.00,0.81] và 
cỏ[0.00,0.19,1.00,0.81] , không phải 
bí ngô[0.38,0.41,0.66,0.70] .
Hình 1: Các ví dụ về lý luận thị giác đa nhiệm vụ bởi SPHINX được đề xuất của chúng tôi, xuất sắc trong các nhiệm vụ hiểu và nhận thức thị giác đa dạng, như phát hiện đối tượng, nền tảng chú thích, và mô tả cấp vùng.

--- TRANG 3 ---
SPHINX : Có một con thuyền[0.91,0.53,0.96,0.54] trong 
hình ảnh.
Người dùng:
Phát hiện tất cả thuyền được hiển thị trong hình ảnh.
SPHINX : Có hai 
người[0.96,0.25,0.97,0.28;0.98,0.25,0.99,0.28] trong 
hình ảnh.
Người dùng:
Phát hiện tất cả người được hiển thị trong hình ảnh.
Hình 2: Các ví dụ về SPHINX cho nhận thức thị giác chi tiết. Với một chuỗi dài hơn các token thị giác, mô hình của chúng tôi thể hiện khả năng hiểu chi tiết vượt trội.

Lưu ý rằng, vì các hình ảnh phụ khác nhau của hình ảnh độ phân giải cao không tương tác với nhau trong bộ mã hóa thị giác, chúng buộc phải trao đổi thông tin trong các lớp chú ý của LLM, điều này thúc đẩy LLM xử lý điều kiện thị giác kỹ lưỡng và sâu sắc hơn. Bằng bộ trộn ba chiều được đề xuất cùng với chuỗi token thị giác dài hơn, SPHINX tinh chỉnh LLM, ví dụ, LLaMA-2 (Touvron et al., 2023b), để trở thành một MLLM mạnh mẽ với khả năng tuân theo hướng dẫn thị giác vượt trội. Như được thể hiện bởi các ví dụ trong Hình 1, mô hình của chúng tôi xuất sắc trong nhiều nhiệm vụ thị giác, ví dụ, phát hiện các đối tượng khác nhau với độ chính xác đáng chú ý và phân tích mối quan hệ của chúng, hoặc diễn giải chính xác nội dung trong các con số phức tạp. Quan trọng là, như được thể hiện trong Hình 2, SPHINX có thể đạt được nhận thức thị giác chi tiết ấn tượng cho hình ảnh độ phân giải cao, thể hiện hiệu suất tối tân trên các tiêu chuẩn đánh giá mở rộng, ví dụ, MMBench (Liu et al., 2023f), MME (Fu et al., 2023a), và POPE (Li et al., 2023e).

2 CÔNG TRÌNH LIÊN QUAN

Mô hình ngôn ngữ lớn (LLM). Lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) đã chứng kiến tiến bộ đáng kể qua các năm, đặc biệt với sự ra đời của LLM. Với Transformer (Vaswani et al., 2017) là kiến trúc cơ bản, LLM (OpenAI, 2023a; Radford et al., 2019; OpenAI, 2023b) đã thể hiện hiệu suất chưa từng có trong việc mô hình hóa các mẫu ngôn ngữ phức tạp trên ngữ cảnh rộng lớn. Trong đó, BERT (Devlin et al., 2018) thể hiện lợi ích của việc tiền huấn luyện trên tập dữ liệu văn bản lớn và tinh chỉnh trên các nhiệm vụ cụ thể, thiết lập tiêu chuẩn mới trên nhiều tiêu chuẩn khác nhau. Chuỗi GPT của OpenAI (Radford & Narasimhan, 2018; Radford et al., 2019; OpenAI, 2023a;b), đặc biệt là GPT-3 (Brown et al., 2020), khai thác sức mạnh của việc mở rộng mô hình lớn, với hàng tỷ và thậm chí hàng nghìn tỷ tham số. Để có khả năng tuân theo hướng dẫn tốt hơn, InstructGPT (Ouyang et al., 2022) và ChatGPT (OpenAI, 2023a) được trình bày để thể hiện sự trôi chảy và đa dạng đặc biệt trong các nhiệm vụ trò chuyện miền mở, từ tạo văn bản đến trả lời câu hỏi. Gần đây, việc điều chỉnh hướng dẫn dựa trên LLaMA (Touvron et al., 2023a) và LLaMA-2 (Touvron et al., 2023b) đã trở nên rất phổ biến như LLM nguồn mở trong cộng đồng. Trong đó, Alpaca (Taori et al., 2023) và LLaMA-Adapter (Zhang et al., 2023a) tương ứng áp dụng tinh chỉnh đầy đủ và hiệu quả tham số để có được LLM tuân theo hướng dẫn thuận lợi. Vicuna (Chiang et al., 2023) và GPT-4-LLM (Peng et al., 2023a) tiếp tục thể hiện cải thiện mang lại bởi bộ dữ liệu hướng dẫn chất lượng cao hơn.

Các nỗ lực khác cũng mở rộng LLM để giải quyết vấn đề toán học (Wang et al., 2023a; Zhou et al., 2023), hệ thống mô hình thị giác (Wu et al., 2023; Yang et al., 2023), và nhận dạng thế giới mở (Zhang et al., 2023b; Zhu et al., 2022). Trong bài báo này, chúng tôi phát triển SPHINX dựa trên khả năng hiểu ngôn ngữ vượt trội của LLaMA-2 (Touvron et al., 2023b) và kinh nghiệm điều chỉnh hướng dẫn của chuỗi LLaMA-Adapter (Zhang et al., 2023a; Gao et al., 2023b), giới thiệu bộ trộn ba chiều để mở rộng trần khả năng của LLM tuân theo hướng dẫn cho đầu vào đa phương thức.

Mô hình ngôn ngữ lớn đa phương thức (MLLM). Ngoài việc tuân theo hướng dẫn ngôn ngữ, nhiều nỗ lực đã được thực hiện để chèn điều kiện đa phương thức vào LLM cho các tình huống ứng dụng rộng hơn. Như những nỗ lực trước đây, VisualGPT (Chen et al., 2022) và chuỗi BLIP (Li et al., 2023d; 2022; Dai et al., 2023) chỉ ra tiềm năng của việc căn chỉnh LLM với đầu vào thị giác để chú thích hình ảnh và trả lời câu hỏi. Flamingo (Alayrac et al., 2022) và Kosmos-1 (Huang et al., 2023) tiếp tục thể hiện hiệu suất hiểu đa phương thức đầy hứa hẹn cho ngữ cảnh hình ảnh-văn bản xen kẽ. Với việc tiền huấn luyện quy mô lớn và kích thước mô hình, GPT-4 (OpenAI, 2023b) và Bard (Google, 2023) cả hai đều thể hiện khả năng đáng chú ý trong hiểu và lý luận thị giác-ngôn ngữ qua các nhiệm vụ đa phương thức đa dạng. Song song, một loạt công trình đã được đề xuất để căn chỉnh LLaMA với phương thức thị giác cho khả năng tuân theo hướng dẫn thị giác tiên tiến. LLaVA (Liu et al., 2023d) và MiniGPT-4 (Zhu et al., 2023) sử dụng một lớp chiếu đơn giản để kết nối các bộ mã hóa thị giác (Li et al., 2023d; Radford et al., 2021) với LLM. LLaMA-Adapter V2 (Gao et al., 2023a) giới thiệu các cơ chế chú ý được khởi tạo bằng không để điều chỉnh hướng dẫn thị giác hiệu quả, và mPLUG-Owl (Ye et al., 2023) áp dụng các mạng trung gian được thiết kế tinh tế để căn chỉnh đa phương thức. Đối với đầu vào đa phương thức hơn, ImageBind-LLM (Han et al., 2023) và PandaGPT (Su et al., 2023) tiếp tục kết hợp điều kiện âm thanh và video được hướng dẫn bởi ImageBind (Girdhar et al., 2023). Bên cạnh đó, các MLLM gần đây cũng được mở rộng để phân tích cấp vùng (Chen et al., 2023b; Peng et al., 2023b), học trong ngữ cảnh (Li et al., 2023a;b), độ phân giải hình ảnh tùy ý (Bavishi et al., 2023), tạo văn bản thành hình ảnh (Wen et al., 2023; Dong et al., 2023), và trả lời câu hỏi 3D (Xu et al., 2023; Guo et al., 2023; Hong et al., 2023). Khác với các công trình trước đây, SPHINX của chúng tôi nhằm mục đích cho MLLM có điều kiện hình ảnh, và đề xuất bộ trộn ba chiều, tức là trọng số mô hình, nhiệm vụ điều chỉnh, và nhúng thị giác, đạt được khả năng tổng quát hóa vượt trội cho học đa phương thức.

3 SPHINX

Trong phần này, chúng tôi giới thiệu một MLLM đa năng, SPHINX, với sự kết hợp chung của trọng số mô hình, nhiệm vụ điều chỉnh, nhúng thị giác, và token hình ảnh phụ độ phân giải cao trong Phần 3.1 và Phần 3.2. Cuối cùng, trong Phần 3.3, chúng tôi giới thiệu một số ứng dụng mở rộng của SPHINX.

3.1 SỰ KẾT HỢP CHUNG CỦA TRỌNG SỐ MÔ HÌNH, NHIỆM VỤ ĐIỀU CHỈNH, VÀ NHÚNG THỊ GIÁC

Mô hình kết hợp tổng thể của SPHINX được thể hiện trong Hình 3. Chúng tôi áp dụng mô hình huấn luyện hai giai đoạn: giai đoạn tiền huấn luyện đầu tiên để căn chỉnh thị giác-ngôn ngữ, và giai đoạn tinh chỉnh thứ hai để học tuân theo hướng dẫn thị giác. Trong hai giai đoạn, chúng tôi áp dụng sự kết hợp được đề xuất của trọng số mô hình và nhiệm vụ điều chỉnh, tương ứng. Mô hình được cấu thành từ một LLM, ví dụ, LLaMA-2 (Touvron et al., 2023b), sự kết hợp các bộ mã hóa thị giác, và hai lớp chiếu tuyến tính.

Bỏ đóng băng LLM cho tiền huấn luyện giai đoạn 1. Các MLLM hiện có (Zhu et al., 2023; Li et al., 2023d; Dai et al., 2023) thường đóng băng toàn bộ LLM trong quá trình tiền huấn luyện bằng dữ liệu hình ảnh-chú thích, và chỉ huấn luyện các lớp chiếu trung gian để căn chỉnh thị giác-ngôn ngữ. Chiến lược này có thể ngăn LLM khỏi quá khớp để chỉ tạo ra các câu ngắn, vì dữ liệu chú thích tiền huấn luyện chủ yếu chứa mô tả ngắn gọn về hình ảnh. Tuy nhiên, trọng số đóng băng hạn chế lớn tiềm năng học đa phương thức của LLM với dữ liệu thị giác-ngôn ngữ quy mô lớn. Do đó, chúng tôi đề xuất bỏ đóng băng toàn bộ LLM cùng với các lớp chiếu tuyến tính có thể học được, để thích ứng thị giác-ngôn ngữ đầy đủ hơn. Mặt khác, các bộ mã hóa thị giác được giữ đóng băng để biểu diễn hình ảnh chất lượng cao. Để đặc biệt bảo tồn khả năng tạo câu dài của LLM, chúng tôi bổ sung dữ liệu tiền huấn luyện thị giác-ngôn ngữ hiện có với dữ liệu tập văn bản bổ sung Penedo et al. (2023) để điều chỉnh chỉ ngôn ngữ. Cụ thể hơn, trong mỗi lần lặp, chúng tôi lấy mẫu một văn bản và một số dữ liệu hình ảnh-chú thích tương ứng từ các bộ dữ liệu ngôn ngữ và thị giác-ngôn ngữ.

Trọng số mô hình kết hợp của các lĩnh vực khác nhau. Một số dữ liệu thị giác-ngôn ngữ từ các lĩnh vực cụ thể chứa kiến thức ngữ nghĩa khác biệt, như chú thích tổng hợp của LAION-COCO (Schuhmann et al., 2022) so với mô tả thế giới thực của LAION-400M (Schuhmann et al., 2021). Chúng tôi đề xuất chiến lược kết hợp trọng số của các LLM được điều chỉnh cụ thể theo lĩnh vực để tích hợp kiến thức tương ứng từ dữ liệu thế giới thực và tổng hợp. Chúng tôi đầu tiên sử dụng dữ liệu lĩnh vực phổ biến nhất (LAION-400M (Schuhmann et al., 2021)) để tiền huấn luyện, trang bị cho MLLM khả năng hiểu thị giác cơ bản. Sau đó, chúng tôi coi mô hình được tiền huấn luyện như vậy là điểm kiểm tra ban đầu để tiếp tục tinh chỉnh LLM trên các lĩnh vực tổng hợp, ví dụ, LAION-COCO (Schuhmann et al., 2022). Cuối cùng, để tận dụng các lĩnh vực dữ liệu tốt nhất, chúng tôi trực tiếp tiến hành kết hợp có trọng số của trọng số hai LLM để tổng hợp ngữ nghĩa. Chi tiết, chúng tôi ký hiệu các tham số của LLM cơ bản là θreal, và các tham số được tinh chỉnh bởi dữ liệu tổng hợp là θsyn. Quá trình kết hợp được công thức hóa như

θmix = β·θreal + (1-β)·θsyn,                    (1)

trong đó β biểu thị hệ số kết hợp, và θmix đại diện cho trọng số LLM kết hợp với ngữ nghĩa tổng hợp. So với việc kết hợp dữ liệu lĩnh vực khác nhau để tiền huấn luyện chung, chiến lược kết hợp trọng số của chúng tôi có thể khuyến khích mọi MLLM học tốt hơn kiến thức duy nhất theo lĩnh vực, và thể hiện tính mở rộng linh hoạt cho bất kỳ lĩnh vực dữ liệu mới nào.

Nhiệm vụ điều chỉnh kết hợp cho tinh chỉnh giai đoạn 2. Sau khi tiền huấn luyện và kết hợp trọng số mô hình, MLLM đã đạt được sự căn chỉnh thỏa mãn giữa dữ liệu thị giác và ngôn ngữ. Để nâng cao thêm khả năng tuân theo hướng dẫn, chúng tôi thu thập dữ liệu hướng dẫn từ một loạt các nhiệm vụ đa phương thức, và tinh chỉnh chung mô hình để học một nhà tổng quát thị giác, thay vì một chuyên gia cho các tình huống cụ thể. Các MLLM nguồn mở trước đây chỉ có thể thực hiện trả lời câu hỏi thị giác đơn giản (VQA) và tham chiếu đối tượng lớn đơn. Ngược lại, chúng tôi cho phép SPHINX được tinh chỉnh chung với một loạt các nhiệm vụ, và thiết kế một bộ hướng dẫn cụ thể theo nhiệm vụ để tránh xung đột giữa các nhiệm vụ. Các nhiệm vụ kết hợp bao gồm VQA chung, hiểu/tạo biểu thức tham chiếu cấp vùng (REC/REG), phát hiện đa đối tượng và lý luận mối quan hệ, VQA biểu đồ/tài liệu hướng văn bản, và ước tính tư thế con người. Ví dụ, chúng tôi áp dụng "Phát hiện tất cả đối tượng được hiển thị trong hình ảnh" cho phát hiện đối tượng chung, và "Phát hiện tất cả văn bản và cung cấp tọa độ hộp giới hạn của chúng" cho phát hiện bố cục tài liệu. Vui lòng tham khảo Bảng 1 để biết hướng dẫn chi tiết về các tiêu chuẩn khác nhau. Nhờ khả năng lý luận vượt trội của LLM và thiết kế phù hợp của các lời nhắc nhiệm vụ, SPHINX, lần đầu tiên, thể hiện khả năng đa mục đích của hiểu và nhận thức thị giác, xuất sắc trong nhiều tình huống ứng dụng khác nhau.

Nhúng kết hợp cho mã hóa thị giác. Để nắm bắt biểu diễn thị giác mạnh mẽ từ các khía cạnh khác nhau, chúng tôi đề xuất tập hợp nhiều backbone thị giác khác nhau để mã hóa hình ảnh. Các backbone thị giác với các đặc điểm khác nhau được chọn như sau. 1) Các kiến trúc mạng khác nhau. Vì CNN (He et al., 2016a) và ViT (Dosovitskiy et al., 2020) chủ yếu tổng hợp các loại xuất hiện thị giác khác nhau, tức là phụ thuộc lân cận và tương tác tầm xa, chúng tôi áp dụng các mô hình CLIP (Radford et al., 2021) tương ứng với các bộ mã hóa hình ảnh ConvNeXt (Woo et al., 2023) và ViT. 2) Các mô hình tiền huấn luyện khác nhau. Huấn luyện có giám sát có thể áp đặt thông tin ngữ nghĩa rõ ràng từ chú thích văn bản hoặc nhãn danh mục, trong khi học tự giám sát buộc mô hình khám phá tín hiệu nhiệm vụ tiền văn bản ngầm. Do đó, chúng tôi tiếp tục sử dụng ViT tự giám sát bởi DINOv2 (Oquab et al., 2023) cũng như các bộ mã hóa thị giác được giám sát bằng văn bản, CLIP. 3) Độ chi tiết thông tin khác nhau. Các bộ mã hóa thị giác nói trên đều tạo ra token thị giác ở cấp độ bản vá. Để nắm bắt tốt hơn các đặc trưng toàn cục, chúng tôi cũng áp dụng Q-Former (Li et al., 2023d) để tóm tắt nhúng thị giác thông qua truy vấn từ ngữ cảnh toàn cục. Sau tất cả việc mã hóa nói trên, chúng tôi đầu tiên nối theo kênh các token thị giác cấp bản vá. Sau đó, bằng cách sử dụng hai lớp chiếu để căn chỉnh chiều, chúng tôi nối theo không gian các biểu diễn giữa những của Q-Former và các đặc trưng cấp bản vá khác. Các token hình ảnh thu được được đặt trực tiếp phía trước hướng dẫn ngôn ngữ, cung cấp ngữ cảnh thị giác cho các hướng dẫn ngôn ngữ.

--- TRANG 4 ---
Hình ảnh đầu vào độ phân giải cao
… … … … … \n \n \n \n \n Một con sphinx hoạt hình đangMô hình ngôn ngữ lớn (LLM)
mặcMột con sphinx hoạt hình mặc một chiếc
Giảm mẫuChia
Hình ảnh độ phân giải thấp
Bộ mã hóa thị giác kết hợp
Hình ảnh phụHình 4: Đường ống của SPHINX cho hình ảnh độ phân giải cao. Chúng tôi đề xuất kết hợp thêm các tỷ lệ khác nhau và hình ảnh phụ để nắm bắt tốt hơn ngữ nghĩa chi tiết trên hình ảnh độ phân giải cao.

3.2 SỰ KẾT HỢP CÁC TỶ LỆ VÀ HÌNH ẢNH PHỤ ĐỘ PHÂN GIẢI CAO

Với chiến lược kết hợp chung nói trên, SPHINX đã thể hiện hiệu suất vượt trội cho các nhiệm vụ nhận thức và lý luận thị giác đa dạng. Tuy nhiên, một thách thức chính vẫn còn, tức là độ phân giải hạn chế của hình ảnh đầu vào. Để giải quyết vấn đề này, chúng tôi tiếp tục đề xuất sử dụng các token thị giác kết hợp của hình ảnh phụ độ phân giải cao, như được thể hiện trong Hình 4.

Hạn chế độ phân giải thấp của MLLM. Các MLLM nguồn mở tối tân (Li et al., 2023d; Liu et al., 2023d; Gao et al., 2023b; Chen et al., 2023b; Peng et al., 2023b; Chen et al., 2023a) áp dụng các bộ mã hóa hình ảnh đóng băng trong tất cả các giai đoạn huấn luyện, để bảo tồn ngữ nghĩa thị giác được tiền huấn luyện. Do đó, độ phân giải hình ảnh của MLLM thường được đặt là 224×224, cản trở nghiêm trọng hiệu quả của chúng cho nhận thức thị giác chi tiết, đặc biệt là nền tảng và mô tả cấp vùng. Tuy nhiên, việc xử lý trực tiếp hình ảnh được tăng mẫu không tối ưu vì hai lý do. Thứ nhất, để căn chỉnh kích thước hình ảnh, các vector mã hóa vị trí được tiền huấn luyện trong ViT cũng cần được tăng mẫu tương ứng, điều này sẽ làm hại các gợi ý không gian tiên nghiệm. Thứ hai, độ phức tạp tính toán của ViT tăng theo bình phương kích thước hình ảnh đầu vào. Do đó, việc tăng mẫu hình ảnh một cách ngây thơ dẫn đến thời gian suy luận lâu và tiêu thụ bộ nhớ GPU lớn.

Tỷ lệ kết hợp và hình ảnh phụ độ phân giải cao. Trong SPHINX của chúng tôi, chúng tôi mở rộng sự kết hợp nhúng thị giác sang nhiều tỷ lệ hơn và hình ảnh phụ độ phân giải cao, cho phép mã hóa hình ảnh độ phân giải cao hiệu quả. Đối với một hình ảnh đầu vào độ phân giải cao, ví dụ, 448×448, chúng tôi xây dựng năm hình ảnh tương ứng của 224×224, và đưa chúng như các hình ảnh độc lập vào các bộ mã hóa thị giác kết hợp của chúng tôi. Cụ thể, chúng tôi đầu tiên giảm mẫu hình ảnh đầu vào xuống 224×224 như một biểu diễn trừu tượng, và cũng giảm mẫu hình ảnh đầu vào xuống 448×448 và cắt bốn hình ảnh phụ của 224×224 từ bốn góc của hình ảnh 448×448, bảo tồn thông tin thị giác chi tiết. Bằng cách này, chúng tôi cho phép MLLM không chỉ nắm bắt xuất hiện thị giác chi tiết với mã hóa vị trí 224×224, mà còn đạt được hiệu quả tính toán thuận lợi. Sau đó, năm nhóm token hình ảnh được mã hóa và nối như một chuỗi dài để đưa vào LLM, trong đó nhóm đầu tiên mã hóa ngữ nghĩa toàn cục, và bốn nhóm khác ghi lại các đặc trưng cục bộ chi tiết. Quan trọng là, vì các token hình ảnh của các bản vá khác nhau không có tương tác thông qua các bộ mã hóa thị giác, chúng buộc phải tương tác trong LLM để có được thông tin thị giác hoàn chỉnh. Chiến lược như vậy, ngược lại, thúc đẩy LLM phân tích các mối quan hệ trong điều kiện thị giác để học đa phương thức tốt hơn. Từ góc độ này, SPHINX của chúng tôi có thể được coi là một mô hình mới tương tự như ViT (Dosovitskiy et al., 2020), trong đó các bộ mã hóa thị giác kết hợp phục vụ như một lớp nhúng bản vá, và LLM đóng vai trò cho tương tác bản vá như một bộ giải mã thị giác. Trên các nhiệm vụ hiểu thị giác yêu cầu độ phân giải cao hơn, SPHINX đạt được cải thiện đáng kể với các biểu diễn thị giác kết hợp của tỷ lệ và hình ảnh phụ độ phân giải cao.

3.3 MỞ RỘNG CHO CÁC ỨNG DỤNG RỘNG HƠN

Trong phần này, chúng tôi tương ứng giới thiệu một số ứng dụng mở rộng được phát triển từ SPHINX.

3.3.1 TÍCH HỢP VỚI SAM VÀ STABLE DIFFUSION

Ngoài việc tuân theo hướng dẫn thị giác đa mục đích, chúng tôi cũng có thể tích hợp SPHINX với các mô hình nền tảng thị giác khác để giải quyết các nhiệm vụ thị giác thách thức hơn. Hình 5 và 6 tương ứng thể hiện hai ứng dụng cho phân đoạn tham chiếu ngôn ngữ và chỉnh sửa hình ảnh.

Phân đoạn tham chiếu ngôn ngữ. Cho rằng MLLM của chúng tôi có thể xuất ra các hộp phát hiện chính xác với mô tả hoặc danh mục ngữ nghĩa do người dùng cung cấp, chúng tôi có thể xâu chuỗi Mô hình Phân đoạn Bất cứ điều gì (SAM) (Kirillov et al., 2023) để phân đoạn thể hiện tham chiếu ngôn ngữ hoặc ngữ nghĩa. Chi tiết, chúng tôi coi các hộp giới hạn được dự đoán từ SPHINX như lời nhắc hộp, và đưa chúng vào SAM để phân đoạn các thể hiện tương ứng. Bằng cách này, chúng tôi kết hợp hiệu quả khả năng lý luận ngữ nghĩa của LLM và phân đoạn không biết lớp của SAM.

Sơn lại và chỉnh sửa hình ảnh. Dựa trên kết quả phân đoạn từ SAM, chúng tôi tham khảo Inpaint Anything (Yu et al., 2023a) để tích hợp các mô hình sơn lại hình ảnh (LaMa (Suvorov et al., 2021)) và các mô hình tạo văn bản thành hình ảnh (Stable Diffusion (Rombach et al., 2021)) để sơn lại và chỉnh sửa hình ảnh chất lượng cao. Cụ thể, chúng tôi đầu tiên phát hiện và phân đoạn các đối tượng do người dùng chỉ định thông qua SPHINX và SAM như được minh họa trong đoạn trước. Sau đó, chúng tôi đưa mặt nạ phân đoạn vào LaMa (Suvorov et al., 2021) để loại bỏ các đối tượng tương ứng với dữ liệu ngữ cảnh. Sau này, người dùng có thể nhắc Stable Diffusion (Rombach et al., 2021) để tiếp tục tạo ra nội dung thị giác mới để thay thế những cái ban đầu. Cài đặt này tích hợp SPHINX, SAM, LaMa, và Stable Diffusion của chúng tôi để đạt được sơn lại và chỉnh sửa hình ảnh được điều khiển bởi ngôn ngữ.

--- TRANG 5 ---
SPHINX：Phát hiện biển báo đường trong bức 
tranh này
Stable Diffusion：Một ánh sáng ở giữa 
bức tranh
SPHINX：Phát hiện chó trong bức 
tranh nàySPHINX：Phát hiện xe máy trong bức 
tranh này
Stable Diffusion：Một con gấu teddy ngồi 
trong rổStable Diffusion：Một người đàn ông đi xe đạp

Hình 6: Các ví dụ về sơn lại và chỉnh sửa hình ảnh bằng cách tích hợp SPHINX và Stable Diffusion (Rombach et al., 2021).

3.3.2 TINH CHỈNH SPHINX CHO NHẬN DẠNG THỊ GIÁC

Được trao quyền bởi sự kết hợp chung của trọng số, nhiệm vụ và nhúng thị giác, SPHINX của chúng tôi có thể hiểu ngữ nghĩa danh mục thị giác mạnh mẽ và đa dạng. Chúng tôi đề xuất coi SPHINX như một khởi tạo phổ quát cho các nhiệm vụ nhận dạng thị giác truyền thống. Ví dụ, cho một nhiệm vụ phân loại của ImageNet-1K (Russakovsky et al., 2015), chúng tôi chuyển đổi nhiệm vụ thành định dạng cuộc trò chuyện một lượt của "Phân loại hình ảnh." như hướng dẫn và sử dụng "Đây là một [LỚP]" như phản hồi. Bằng cách thực hiện tinh chỉnh có giám sát trên bộ dữ liệu được chuyển đổi văn bản, chúng tôi quan sát hội tụ huấn luyện nhanh trên ImageNet-1K. Đáng ngạc nhiên, chỉ với một epoch, SPHINX có thể đạt được độ chính xác phân loại 70.8% mà không có bất kỳ tăng cường dữ liệu nào. Tốc độ hội tụ này nhanh hơn nhiều so với các phương pháp truyền thống, như ResNet (He et al., 2016b) và ViT (Dosovitskiy et al., 2020) thường mất khoảng 300 epoch huấn luyện và yêu cầu tăng cường dữ liệu mạnh.

4 THỰC NGHIỆM

4.1 CHI TIẾT HUẤN LUYỆN

Như đã đề cập trong Phần 3.1, đường ống huấn luyện của chúng tôi bao gồm hai giai đoạn. Trong giai đoạn 1, hoặc giai đoạn Tiền huấn luyện, chúng tôi bắt đầu từ một LLM chỉ văn bản, và xây dựng khả năng đa phương thức từ đầu với các bộ dữ liệu nhiễu quy mô lớn. Trong giai đoạn 2, hoặc giai đoạn tinh chỉnh, chúng tôi trích xuất các khả năng mạnh mẽ được học trong giai đoạn 1 trên các nhiệm vụ thực tế bằng cách huấn luyện thêm với các bộ dữ liệu tuân theo hướng dẫn đa dạng và chất lượng cao. Việc xây dựng các bộ dữ liệu và cấu hình huấn luyện cho cả hai giai đoạn được chi tiết như sau.

Bộ dữ liệu tiền huấn luyện. Chúng tôi sử dụng hai bộ dữ liệu chú thích hình ảnh LAION-400M (Schuhmann et al., 2021) và LAION-COCO (Schuhmann et al., 2022) để căn chỉnh đa phương thức. Vì chúng tôi tinh chỉnh đầy đủ backbone mô hình ngôn ngữ trong nhiều bước dài, chúng tôi cũng huấn luyện chung với bộ dữ liệu chỉ văn bản RefinedWeb (Penedo et al., 2023) để tránh làm hại khả năng lý luận văn bản do quên thảm khốc.

Cấu hình tiền huấn luyện. Chúng tôi tinh chỉnh trọng số của mô hình ngôn ngữ lớn và các chiếu thị giác trong giai đoạn tiền huấn luyện, trong đó trọng số của mô hình ngôn ngữ lớn được khởi tạo từ trọng số nguồn mở sẵn có như LLaMA-2 (Touvron et al., 2023b) và các chiếu thị giác được khởi tạo ngẫu nhiên. Bản thân các bộ mã hóa thị giác được giữ đóng băng với trọng số được tiền huấn luyện ban đầu trong suốt quá trình huấn luyện. Chúng tôi sử dụng bộ tối ưu hóa AdamW (Kingma & Ba, 2014) với (β1, β2) = (0.9, 0.95), lịch trình tỷ lệ học cosine annealing cho 180,000 bước từ 5×10^-5 đến 5×10^-6 với 2,000 bước đầu tiên là khởi động tuyến tính từ 0 đến 5×10^-5, và suy giảm trọng số không đổi là 0.1. Đối với việc huấn luyện chung trên cả hình ảnh và văn bản, chúng tôi tạo thành mỗi lô với 640 cặp hình ảnh-văn bản từ LAION-400M hoặc LAION-COCO và 65,536 token văn bản từ RefinedWeb. Vì các chú thích trong LAION-400M và LAION-COCO dựa trên dữ liệu thu thập web và thường không chứa nhiều thông tin chi tiết, chúng tôi chỉ sử dụng một góc nhìn toàn cục của mỗi hình ảnh, tức là độ phân giải thấp 224×224, để huấn luyện nhanh hơn. Chúng tôi không áp dụng bất kỳ hình thức lời nhắc ngôn ngữ nào trong quá trình tiền huấn luyện. Thời gian tiền huấn luyện khoảng 125 giờ trên 32 GPU A100 với mô hình ngôn ngữ 7B và khoảng gấp đôi thời gian với mô hình ngôn ngữ 13B.

Bộ dữ liệu tinh chỉnh. Trong giai đoạn tinh chỉnh đa nhiệm vụ, mục tiêu của chúng tôi là trang bị cho MLLM các nhu cầu đa dạng của các nhiệm vụ downstream. Dựa trên những hiểu biết từ nghiên cứu trước (Liu et al., 2023d; Dai et al., 2023; Chen et al., 2023b; Zhu et al., 2023; Liu et al., 2023b), chúng tôi bao gồm dữ liệu tuân theo hướng dẫn như LLaVA (Liu et al., 2023d) và ShareGPT (ShareGPT, 2023), đưa mô hình tiếp xúc với các nhiệm vụ yêu cầu chỉ thị rõ ràng. Đối với Trả lời câu hỏi thị giác chung (VQA), chúng tôi tận dụng các bộ dữ liệu như VQA V2 (Agrawal et al., 2015) và GQA (Hudson & Manning, 2019). Mở rộng phạm vi đến kiến thức ngoài lĩnh vực, chúng tôi tích hợp các bộ dữ liệu như OKVQA (Marino et al., 2019) và A-OKVQA (Schwenk et al., 2022), cung cấp cho mô hình thông tin vượt ra ngoài dữ liệu huấn luyện. Các bộ dữ liệu Nhận dạng ký tự quang học (OCR), như OCRVQA (Mishra et al., 2019) và TextCaps (Sidorov et al., 2020) được sử dụng để tăng khả năng hiểu văn bản của SPHINX. Chúng tôi giới thiệu các bộ dữ liệu phát hiện đối tượng chung và ước tính tư thế phong phú, như COCO (Lin et al., 2014) và LVIS (Gupta et al., 2019) để truyền cảm hứng cho khả năng định vị, phân loại và ước tính tư thế con người của mô hình. Để giải quyết các nhiệm vụ nền tảng, chúng tôi kết hợp các bộ dữ liệu RefCOCO (Kazemzadeh et al., 2014) và VG (Krishna et al., 2017), huấn luyện mô hình xử lý định vị đối tượng tham chiếu. Ngoài ra, các bộ dữ liệu Chú thích nền tảng, như những từ Flickr30k (Plummer et al., 2015), tiếp tục tinh chỉnh hiểu biết về mô tả trong ngữ cảnh của các vùng hình ảnh. Mặc dù đa dạng về nguồn dữ liệu, chúng tôi hợp lý hóa việc huấn luyện bằng cách chuyển đổi tất cả các bộ dữ liệu thành định dạng cuộc trò chuyện nhiều lượt. Điều này không chỉ giảm chi phí huấn luyện mà còn nâng cao hiệu quả tổng thể.

Cấu hình tinh chỉnh. Các thành phần mạng được huấn luyện và đóng băng giống hệt như giai đoạn tiền huấn luyện. Cài đặt tối ưu hóa tương tự như giai đoạn tiền huấn luyện, ngoại trừ việc chúng tôi sử dụng kích thước lô 128, tỷ lệ học tối đa 2×10^-5, tỷ lệ học tối thiểu 0, và khởi động tuyến tính cho 0.03 epoch trong quá trình tinh chỉnh. Dữ liệu huấn luyện được lấy mẫu từ hỗn hợp các bộ dữ liệu theo tần suất tự nhiên của chúng, tức là, cơ hội của một bộ dữ liệu được lấy mẫu từ tỷ lệ thuận với kích thước ban đầu của nó. Chúng tôi làm theo các bước tiền xử lý hình ảnh của (Chen et al., 2023b; Liu et al., 2023b), tức là, đệm hình ảnh dọc theo cạnh ngắn hơn để tạo thành hình vuông trước khi thay đổi kích thước, để xử lý tốt hơn các hình ảnh có tỷ lệ khía cạnh cực đoan. Việc tinh chỉnh mất khoảng 38 giờ với 16 GPU A100 với mô hình ngôn ngữ 13B. Chiều dài chuỗi huấn luyện tối đa được đặt là 3072.

--- TRANG 6 ---
0 2500 5000 7500 10000 12500 15000 17500 20000
bước huấn luyện1.52.02.53.03.54.0lossCurve tổn thất huấn luyện có và không có RefinedWeb
tổn thất chỉ văn bản, với RefinedWeb
tổn thất chỉ văn bản, không có RefinedWeb
tổn thất hình ảnh-chú thích, với RefinedWeb
tổn thất hình ảnh-chú thích, không có RefinedWebHình 7: Đường cong tổn thất trong giai đoạn tiền huấn luyện có và không có tối ưu hóa trên RefinedWeb.
Tổn thất chỉ văn bản tương ứng với việc huấn luyện chỉ trên huấn luyện chỉ RefinedWeb và tổn thất hình ảnh-chú thích tương ứng với việc huấn luyện chỉ trên LAION-400M. Không có huấn luyện chung trên RefinedWeb, tổn thất hình ảnh-chú thích giảm tương tự nhưng tổn thất chỉ văn bản tăng đáng kể ngay cả trong ít hơn 1/10 lịch trình huấn luyện. Chúng tôi dừng sớm các thí nghiệm không có RefinedWeb sau khi hiện tượng quên rõ ràng.

4.2 ĐÁNH GIÁ ĐỊNH LƯỢNG

Trong phần này, chúng tôi cung cấp một đánh giá toàn diện về SPHINX và thể hiện kết quả trên nhiều tiêu chuẩn. Đánh giá của chúng tôi bao gồm cả các số liệu định lượng và đánh giá định tính, cung cấp hiểu biết toàn diện về hiệu suất mô hình VLM của chúng tôi.

Tiền huấn luyện chung hình ảnh-văn bản. Chúng tôi thể hiện trong Hình 7 hiệu quả của việc giới thiệu bộ dữ liệu chỉ văn bản (tức là, RefinedWeb) để huấn luyện chung với chú thích hình ảnh trong giai đoạn tiền huấn luyện. Chúng tôi thiết kế một thí nghiệm chỉ sử dụng dữ liệu thị giác-ngôn ngữ và không sử dụng RefinedWeb. Chúng tôi quan sát rằng tổn thất chỉ văn bản tăng lên nếu mô hình không được huấn luyện với RefinedWeb, cho thấy rằng lịch trình huấn luyện chung của chúng tôi có hiệu quả trong việc bảo tồn khả năng mô hình hóa văn bản trong khi thích ứng cho hiểu đa phương thức.

Thiết kế lời nhắc đánh giá. Trong đánh giá mô hình của chúng tôi, chúng tôi ưu tiên căn chỉnh với định dạng đầu ra mong muốn của mỗi tiêu chuẩn. Để đạt được điều này, chúng tôi sử dụng các lời nhắc khác biệt được thiết kế riêng cho các tiêu chuẩn cần câu trả lời dài, câu trả lời ngắn, và phản hồi trắc nghiệm. Thông tin chi tiết được cung cấp trong Bảng 1. Phương pháp này đảm bảo rằng mô hình của chúng tôi có khả năng xử lý các tình huống đa dạng.

--- TRANG 7 ---
Hướng dẫn Tiêu chuẩn
- LLaVA-Bench, MM-Vet, MathVista
Trả lời câu hỏi bằng một từ hoặc cụm từ duy nhất. VQA V2, GQA, OKVQA, VSR, MME, OCR-VQA
Trả lời bằng chữ cái tùy chọn từ các lựa chọn đã cho trực tiếp. SeedBench, ScienceQA, IconVQA
Vui lòng cung cấp tọa độ hộp giới hạn của vùng mà câu này mô tả: {mô tả}. RefCOCO, RefCOCO+, RefCOCOg
Tham chiếu token OCR: {OCR}
Trả lời câu hỏi bằng một từ hoặc cụm từ duy nhất. TextVQA
Khi thông tin được cung cấp không đủ, trả lời với 'Không thể trả lời'. Trả lời câu hỏi bằng một từ hoặc cụm từ duy nhất. VizWiz
Có một số tùy chọn: {tùy chọn} CCBench, MMBench
Phát hiện tất cả đối tượng được hiển thị trong hình ảnh.
phát hiện tất cả {tên danh mục} được hiển thị trong hình ảnh. Phát hiện đối tượng
Phát hiện tất cả người được hiển thị trong hình ảnh.
Phát hiện các điểm quan trọng của người trong vùng {tọa độ}. Phát hiện tư thế con người
Phát hiện tất cả văn bản và cung cấp tọa độ hộp giới hạn của chúng. Bố cục tài liệu
Mô tả hình ảnh một cách súc tích.
Bao gồm hộp giới hạn cho mỗi đối tượng được đề cập. Chú thích nền tảng
Mối quan hệ giữa đối tượng trong {tọa độ} và đối tượng trong {tọa độ} là gì? Phát hiện mối quan hệ
Vui lòng cung cấp tọa độ hộp giới hạn của vùng mà câu này mô tả: {mô tả} Mối quan hệ tham chiếu

Bảng 1: Hướng dẫn cụ thể theo nhiệm vụ trên các tiêu chuẩn khác nhau cho SPHINX.

Phương pháp POPE MMEPMMECMMB MMBCNSEED LLavaWMM-Vet CCbench MathVista Tiny LVLM Touchstone
BLIP-2 (Li et al., 2023d) 85.3 1293.8 - - - 46.4 38.1 22.4 - - 284.7 -
InstructBLIP-7B (Dai et al., 2023) - - - 36 23.7 53.4 60.9 26.2 12.1 25.3 300.6 552.4
InstructBLIP-13B (Dai et al., 2023) 78.9 1212.8 - - - - 58.2 25.6 - - - -
Shikra (Chen et al., 2023b) - - - 58.8 - - - - - - - -
LLaMA-AdapterV2 (Gao et al., 2023a) - 1328.40 356.43 - - - - - - - 229.2 590.1
Qwen-VL-7B (Bai et al., 2023a) - - - 38.2 7.4 56.3 - - 5.5 - - -
Qwen-VL-7B-Chat (Bai et al., 2023a) - 1487.58 360.71 60.6 56.7 58.2 - - 39.3 - 316.8 645.2
LLaVA1.5-7B (Liu et al., 2023b) 85.9 1510.7 - 64.3 58.3 58.6 63.4 30.5 16.4 - - -
LLaVA1.5-13B (Liu et al., 2023b) 85.9 1531.3 295.36 67.7 63.6 61.6 70.7 35.4 26.5 - - -
SPHINX 80.7 1476.1 322.2 66.9 56.2 69.14 73.5 36.0 25.6 27.0 - 632.4
SPHINX-1k 90.8 1560.2 310.0 67.1 59.5 71.6 74.3 36.6 27.9 27.5 288.9 645.0
SPHINX-2k 87.2 1470.6 326.8 65.9 57.9 71.6 76.9 40.2 27.4 27.8 - 659.5

Bảng 2: So sánh với các phương pháp SoTA trên 10 tiêu chuẩn MLLM.

--- TRANG 8 ---
VQA chung VQA hướng văn bảnPhương pháp OKVQA VQA V2 VizWiz GQA VSR ScienceQA IconVQA TextVQA OCR-VQA
BLIP-2 (Li et al., 2023d) 45.9 - 19.6 41.0 50.9 - 40.6 - 40.6
InstructBLIP (Dai et al., 2023) - - 33.4 49.5 52.1 - 44.8 - 44.8
LLaMA-AdapterV2 (Gao et al., 2023a) 49.6 70.7 39.8 45.1 - - - 37.4 -
Shikra (Chen et al., 2023b) 47.2 77.4 - - - - - - -
Fuyu-8B (Bavishi et al., 2023) 60.6 74.2 - - - - - - -
MiniGPT-v2 (Chen et al., 2023a) 57.8 - 53.6 60.1 62.9 - 51.5 - -
Qwen-VL-7B (Bai et al., 2023a) 58.6 79.5 35.2 59.3 63.8 67.1 - 63.8 75.7
Qwen-VL-7B-Chat (Bai et al., 2023a) 56.6 78.2 38.9 57.5 61.5 68.2 - 61.5 70.5
LLaVA1.5-7B (Liu et al., 2023b) - 78.5 50.0 62.0 - 66.8 - 58.2 -
LLaVA1.5-13B (Liu et al., 2023b) - 80.0 53.6 63.3 - 71.6 - 61.3 -
SPHINX 62.1 78.1 39.9 62.6 58.5 69.3 50.4 51.63 66.0
SPHINX-1k 62.2 80.2 46.8 62.9 65.4 69.1 52.7 58.78 70.0
SPHINX-2k 62.6 80.7 44.9 63.1 57.1 70.6 50.5 61.19 67.8

Bảng 3: So sánh hiệu suất trên 10 tiêu chuẩn hướng nhiệm vụ học thuật.

Định nghĩa biến thể mô hình. Chúng tôi ký hiệu biến thể cơ bản của MLLM của chúng tôi là SPHINX, nhận đầu vào là hình ảnh độ phân giải thấp 224×224, và tạo ra 289 token thị giác (257 từ CLIP (Radford et al., 2021) và DINOv2 (Oquab et al., 2023) kết hợp, và 32 từ Q-Former (Li et al., 2023d)). Sau đó, chúng tôi ký hiệu biến thể độ phân giải cao của chúng tôi là SPHINX-1k và SPHINX-2k. SPHINX-1k xử lý độ phân giải hình ảnh 448×448 bằng cách chia đều bốn hình ảnh phụ với 1,445 token thị giác, tức là năm nhóm 289 token (một nhóm cho hình ảnh giảm mẫu và bốn nhóm cho hình ảnh phụ). SPHINX-2k tiếp tục xử lý độ phân giải cao hơn 762×762 với chín hình ảnh phụ được chia đều của 2,890 token thị giác, tức là mười nhóm 289 token.

Tiêu chuẩn về mô hình ngôn ngữ lớn đa phương thức. Chúng tôi thử nghiệm mô hình của chúng tôi trên các tiêu chuẩn MLLM được đề xuất gần đây để đánh giá toàn diện đặc điểm của mô hình như MME (Fu et al., 2023b), Seedbench (Li et al., 2023c), POPE (Li et al., 2023e), LLaVA-Bench (In-the-Wild) (Liu et al., 2023d), MM-Vet (Yu et al., 2023b), MathVista (Lu et al., 2023), MMbench (Liu et al., 2023g), CCbench (Contributors, 2023), Tiny LVLM (Shao et al., 2023) và Touchstone (Bai et al., 2023b). Chúng tôi thể hiện kết quả trong Bảng 2. Chúng tôi quan sát rằng SPHINX vượt qua hiệu suất MLLM tối tân trước đây trên 6 trong 10 tiêu chuẩn. Chúng tôi so sánh mô hình của chúng tôi với các đường cơ sở mạnh bao gồm BLIP-2 (Li et al., 2023d), InstructBLIP (Dai et al., 2023), Shikra (Chen et al., 2023b), Qwen (Bai et al., 2023a), Fuyu (Bavishi et al., 2023) và LLaVA1.5 (Liu et al., 2023b). Khoảng cách giữa SPHINX và SPHINX-1k trên POPE cho thấy rằng việc giới thiệu hình ảnh phụ độ phân giải cao có thể cải thiện đáng kể vấn đề ảo giác thị giác.

Trả lời câu hỏi thị giác. Hơn nữa, chúng tôi đánh giá các tiêu chuẩn VQA chung, như VQA V2 (Agrawal et al., 2015), OKVQA (Marino et al., 2019), GQA (Hudson & Manning, 2019), vizwiz (Gurari et al., 2018), ScienceQA (Lu et al., 2022), lý luận không gian thị giác (VSR) (Liu et al., 2023a), IconQA (Lu et al., 2021). Ngoài ra, chúng tôi tiến hành thí nghiệm trên VQA hướng văn bản như TextVQA (Singh et al., 2019), OCR-VQA (Mishra et al., 2019). Chúng tôi cung cấp kết quả trong Bảng 3. SPHINX đạt được kết quả so sánh trên tất cả các tiêu chuẩn. Chúng tôi quan sát rằng SPHINX-1k và SPHINX-2k vượt trội đáng kể so với SPHINX trong bộ dữ liệu VQAv2 và VQA hướng văn bản đòi hỏi thông tin thị giác chi tiết, thể hiện hiệu quả của phương pháp trộn thị giác được đề xuất của chúng tôi để đạt được độ phân giải cao mà không cần dựa vào bộ mã hóa thị giác được huấn luyện đặc biệt trên hình ảnh độ phân giải cao. Mặc dù hiệu suất của SPHINX trên VQA hướng văn bản vượt qua các đường cơ sở mạnh, như BLIP-2 và InstructBLIP, nó vẫn dưới Qwen-VL-7B do thiếu dữ liệu tiền huấn luyện liên quan đến văn bản. Trong tương lai, chúng tôi sẽ giới thiệu nhiều bộ dữ liệu tiền huấn luyện liên quan đến văn bản hơn.

Nền tảng thị giác. Bảng 4 đánh giá SPHINX trên các tiêu chuẩn REC với các bộ dữ liệu RefCOCO (Kazemzadeh et al., 2014), RefCOCO+ (Mao et al., 2015), và RefCOCOg (Mao et al., 2015). SPHINX vượt trội hơn hầu hết các mô hình tối tân, bao gồm mô hình chuyên gia G-DINO-L Liu et al. (2023e) và các mô hình tổng quát thị giác-ngôn ngữ khác. So với đường cơ sở mạnh gần đây Qwen-VL-7B (Bai et al., 2023a), cũng tận dụng mô hình ngôn ngữ lớn để hiểu thị giác, mô hình của chúng tôi vẫn đạt được kết quả tốt hơn trên tất cả các phần chia với biên độ lớn. Hơn nữa, SPHINX-1k và SPHINX-2k cho phép sử dụng hình ảnh đầu vào độ phân giải cao, dẫn đến cải thiện liên tục so với SPHINX và thu hẹp khoảng cách với mô hình chuyên gia mạnh UNINEXT, áp dụng kích thước hình ảnh đầu vào lớn hơn. Những kết quả này chứng minh khả năng cạnh tranh của SPHINX cho nền tảng thị giác.

--- TRANG 9 ---
RefCOCO+ RefCOCO RefCOCOgPhương pháp val test-A test-B val test-A test-B val-u test-u
Mô hình chuyên gia
UNINEXT (Yan et al., 2023) 85.24 89.63 79.79 92.64 94.33 91.46 88.73 89.37
G-DINO-L (Liu et al., 2023e) 82.75 88.95 75.92 90.56 93.19 88.24 86.13 87.02
Mô hình tổng quát
VisionLLM-H (Wang et al., 2023b) - - - - 86.70 - - -
OFA-L (Wang et al., 2022) 68.29 76.00 61.75 79.96 83.67 76.39 67.57 67.58
Shikra 7B (Chen et al., 2023b) 81.60 87.36 72.12 87.01 90.61 80.24 82.27 82.19
Shikra 13B (Chen et al., 2023b) 82.89 87.79 74.41 87.83 91.11 81.81 82.64 83.16
MiniGPT-v2 7B (Chen et al., 2023a) 79.97 85.12 74.45 88.69 91.65 85.33 84.44 84.66
MiniGPT-v2 7B-chat (Chen et al., 2023a) 79.58 85.52 73.32 88.06 91.29 84.30 84.19 84.31
Qwen-VL-7B (Bai et al., 2023a) 83.12 88.25 77.21 89.36 92.26 85.34 85.58 85.48
Qwen-VL-7B-Chat (Bai et al., 2023a) 82.82 88.59 76.79 88.55 92.27 84.51 85.96 86.32
SPHINX 82.77 87.29 76.85 89.15 91.37 85.13 84.87 83.65
SPHINX-1k 86.64 91.08 80.35 91.05 92.65 86.56 88.19 88.35
SPHINX-2k 85.51 90.62 80.45 91.10 92.88 87.07 88.07 88.65

Bảng 4: So sánh hiệu suất (Top-1 Accuracy@0.5) trên nhiệm vụ hiểu biểu thức tham chiếu. Kết quả tốt nhất trong các mô hình tổng quát được đánh dấu in đậm.

4.3 MINH HỌA

Trong phần này, chúng tôi trình bày kết quả định tính của SPHINX, thể hiện khả năng của nó trong phân đoạn hỗ trợ SAM, phát hiện đối tượng chung, ước tính tư thế con người, phát hiện bố cục tài liệu, phát hiện bất thường, v.v. Đáng ngạc nhiên, SPHINX cũng thể hiện hiệu suất cải thiện trên chuỗi suy nghĩ và có được khả năng nổi lên giữa các nhiệm vụ.

Phân đoạn thể hiện tăng cường SAM. Chúng tôi tích hợp SPHINX với SAM để nâng cao khả năng phân đoạn. Tích hợp này bao gồm việc phát hiện các hộp giới hạn cho các đối tượng mục tiêu và sau đó cung cấp tọa độ hộp giới hạn cho SAM để tạo ra các mặt nạ phân đoạn. Kết quả, được mô tả trong Hình 8, thể hiện cải thiện hiệu suất đáng chú ý đạt được thông qua sự hợp tác của SPHINX và SAM. Đáng ngạc nhiên, chúng tôi quan sát rằng các mặt nạ được dự đoán cho các đối tượng nhỏ cực kỳ chính xác như điện thoại di động ở hàng cuối. Việc áp dụng tương hỗ của SPHINX và SAM nhấn mạnh tiềm năng đáng kể vốn có trong phương pháp của chúng tôi.

Hiểu cấp vùng. Trong Hình 9, hiệu suất của khả năng phát hiện của SPHINX được thể hiện. Hàng trên hiển thị việc nhảy đồng bộ của năm thiếu niên, mỗi người đảm nhận các tư thế khác biệt. Đáng chú ý, SPHINX chính xác dự đoán tư thế với các điểm quan trọng cho mỗi cá nhân, không bỏ qua người tham gia nào. Hàng giữa minh họa khả năng lý luận của SPHINX để tập trung vào một vùng được chỉ định. Chúng tôi quan sát rằng SPHINX thành công nhận ra các đối tượng mong muốn và trả lời chi tiết cho câu hỏi. Hàng dưới chỉ ra khả năng hiểu sơ đồ vượt trội của SPHINX, tạo ra phát hiện bố cục chính xác và hiểu nội dung.

Lý luận thị giác tốt hơn với phát hiện đối tượng. Khả năng lý luận thị giác nâng cao của mô hình chúng tôi với phát hiện đối tượng được thể hiện trong Hình 10. Đáng chú ý, SPHINX tận dụng phản hồi phát hiện đối tượng bằng cách ban đầu hướng dẫn SPHINX tạo ra kết quả phát hiện đối tượng và sau đó yêu cầu nó trả lời câu hỏi dựa trên kết quả định vị. Mô hình sẽ ưu tiên lựa chọn các đối tượng liên quan nhất để phản hồi tọa độ dựa trên nội dung truy vấn, thay vì tất cả các đối tượng được phát hiện. Điều này nhấn mạnh ý tưởng rằng trong huấn luyện đa nhiệm vụ, sự tương hỗ giữa các nhiệm vụ khác nhau có thể nâng cao đáng kể hiệu suất tổng thể. Hơn nữa, mô hình thể hiện Hiểu ngữ cảnh (COT) đáng khen ngợi bằng cách tích hợp hiệu quả thông tin từ các yếu tố đa dạng trong hình ảnh, dẫn đến khả năng lý luận mạnh mẽ hơn.

Phát hiện đối tượng cải thiện thông qua gợi ý. Chúng tôi nhấn mạnh khả năng thành thạo của SPHINX trong việc hiểu gợi ý của người dùng. Như được mô tả trong Hình 10, ban đầu yêu cầu mô hình dự đoán tất cả chó trong hình ảnh dẫn đến việc nhận dạng sai các đối tượng khác. Tuy nhiên, khi đưa ra gợi ý bổ sung về đối tượng mong muốn, SPHINX thể hiện sự hiểu biết cải thiện về hướng dẫn và chính xác dự đoán tất cả chó trong hình ảnh.

Ước tính tư thế tham chiếu nổi lên. Hiểu đối tượng tham chiếu ban đầu và ước tính tư thế là hai nhiệm vụ khác nhau, trong đó nhiệm vụ trước phát hiện hộp giới hạn đối tượng theo mô tả văn bản, và nhiệm vụ sau xuất ra các điểm quan trọng con người từ hộp giới hạn đã cho. Thú vị, như được thể hiện trong Hình 11 (Trên), bằng sự kết hợp của hai nhiệm vụ điều chỉnh, SPHINX của chúng tôi có được khả năng nổi lên cho ước tính tư thế tham chiếu, tức là tạo ra các điểm quan trọng con người trực tiếp từ mô tả văn bản. Quan sát như vậy chỉ ra rằng SPHINX của chúng tôi hoàn toàn hiểu ngữ nghĩa qua các nhiệm vụ thị giác-ngôn ngữ khác nhau, và kết nối chúng một cách ngầm định thông qua sức mạnh lý luận vượt trội.

Phát hiện bất thường. Điều quan trọng cho giám sát công nghiệp và chăm sóc sức khỏe là phát hiện các sự kiện hiếm hoặc ngoại lệ có thể chỉ ra hành vi bất thường hoặc đáng ngờ. Như được thể hiện trong Hình 11 (Dưới), SPHINX của chúng tôi cũng xuất sắc trong phát hiện bất thường. Mặc dù chúng tôi không rõ ràng bao gồm dữ liệu huấn luyện liên quan, MLLM của chúng tôi vẫn thể hiện độ chính xác định vị vượt trội cho các khuyết tật không sắc nét. Điều này chỉ ra tiềm năng rộng lớn của SPHINX trong các ứng dụng thế giới thực.

Chú thích dày đặc đa cấp. Được trang bị với tiền huấn luyện đa nhiệm vụ đa dạng, SPHINX có thể thực hiện chú thích dày đặc đa cấp bằng cách thúc đẩy bản thân lặp lại. Cho một hình ảnh đầu vào, nhắc SPHINX với "Phát hiện tất cả đối tượng được hiển thị trong hình ảnh" có thể định vị vị trí của tất cả đối tượng. Sau đó, chúng tôi lặp lại nhắc mỗi vùng được phát hiện với "Vui lòng cung cấp mô tả ngắn cho vùng này: [x1, y1, x2, y2]" để trích xuất thuộc tính đơn giản trên vùng được định vị. Để có hiểu biết sâu sắc hơn về các vùng được phát hiện, chúng tôi cắt tất cả hình ảnh dựa trên kết quả phát hiện. Mỗi góc nhìn được cắt được đưa

--- TRANG 10 ---
Người dùng:
 SPHINX : 
Có 11 
người[0.52,0.31,0.71,0.76;0.18,0.54,0.49,0.82;0.04,0.45,0.25,0.81;..... trong hình ảnh
Phân đoạn thể hiện tăng cường SAM
Một bàn[0.00,0.63,0.97,0.83] với bốn 
laptop[0.05,0.55,0.32,0.76;0.60,0.48,0.80,0.67;0.11,0.45,0.30,0.58;0.07,0.55,0.30,0.70] 
và một máy tính để bàn[0.36,0.43,0.62,0.62] .
Có ba đối tượng được hiển thị trong hình ảnh, 
bao gồm một laptop[0.31,0.13,1.00,0.87], một 
cuốn sách[0.00,0.54,0.34,0.87], và một điện 
thoại di động[0.00,0.51,0.14,0.63].Phát hiện tất cả người được hiển thị trong hình ảnh
Phát hiện tất cả đối tượng được hiển thị trong hình ảnh
Phát hiện tất cả đối tượng được hiển thị trong hình ảnhHình 8: Các ví dụ về SPHINX tích hợp với Mô hình Phân đoạn Bất cứ điều gì (SAM) (Kirillov et al., 2023) cho phân đoạn tham chiếu ngôn ngữ.

độc lập vào SPHINX với hai lời nhắc, cụ thể, "Cung cấp chú thích một câu cho hình ảnh được cung cấp." và "Tạo mô tả chi tiết về hình ảnh.". Bằng cách này, chúng tôi có thể phát hiện tất cả đối tượng được hiển thị trong hình ảnh và dán nhãn dày đặc tất cả các hộp với thuộc tính, chú thích đơn giản, và chú thích chi tiết. Kết quả chú thích dày đặc đa cấp được minh họa trong Hình 12.

--- TRANG 11 ---
Các điểm quan trọng được phát hiện là mũi[0.46,0.44], mắt trái[0.47,0.43], mắt 
phải[0.46,0.43], tai trái[0.48,0.43], tai phải[0.45,0.43], vai 
trái[0.50,0.45], vai phải[0.44,0.45], khuỷu tay trái[0.53,0.43], 
khuỷu tay phải[0.41,0.43], cổ tay trái[0.55,0.41], cổ tay phải[0.39,0.40], hông 
trái[0.50,0.53], hông phải[0.47,0.53], đầu gối trái[0.54,0.59], đầu gối 
phải[0.48,0.60], mắt cá chân trái[0.50,0.62], và mắt cá chân phải[0.42,0.61].......SPHINX : 
Người dùng:
Phát hiện các điểm quan trọng của năm người trong vùng 
[0.38,0.39,0.57,0.64];[0.63,0.37,0.81,0.65];[0.82,0.36,0.94,0.65];[0.23,0.35,0.34,0.64];[0.04,0.38,0.16,0.64].
Ước tính tư thế con người
Phát hiện mối quan hệ
Người dùng:
[0.10,0.44,0.59,0.76] đang cố gắng làm gì?
SPHINX : Con chim đang cố gắng ăn thức ăn, cụ thể là 
chiếc bánh sandwich, mà em bé đang cầm.
Người dùng:
[0.28,0.44,0.51,0.53] có ý định làm gì?
SPHINX : Con chó, là một con chó chăn cừu đen trắng, có thể 
có ý định chăn cừu trong cánh đồng. Chó chăn cừu được 
huấn luyện để làm việc với cừu và chịu trách nhiệm hướng dẫn, 
bảo vệ và kiểm soát đàn. Vị trí của con chó trong cánh đồng, 
cùng với sự hiện diện của đàn cừu, cho thấy rằng nó 
đang thực hiện nhiệm vụ chăn cừu.
Hiểu sơ đồ chính xác
Người dùng:
Vui lòng phát hiện tất cả các văn bản và cung cấp tọa độ hộp giới hạn của chúng.
SPHINX : Hình ảnh mô tả một sơ đồ minh họa một khung học nhúng ngầm truyền thống và một khung căn chỉnh thị giác-ngôn ngữ rõ ràng. Khung truyền thống bao gồm một loạt các bước, bao gồm mã hóa hình ảnh, chiếu, và học ma trận hoặc phân loại. Mặt khác, khung rõ ràng bao gồm một bộ mã hóa văn bản, một tích vô hướng, và học tương phản. Sơ đồ cũng bao gồm nhiều hình ảnh khác nhau, như một đồng hồ, một ghế sofa, và một cái ghế, cũng như một máy bay và một chiếc xe hơi.
SPHINX : lớp[0.37, 0.24, 0.43, 0.27].Hình ảnh [0.25, 0.32, 0.33, 
0.35].ghế sofa[0.04, 0.77, 0.1, 0.8].Lời nhắc[0.11, 0.85, 0.19, 
0.88].máy bay[0.04, 0.74, 0.12, 0.77].bộ mã hóa[0.25, 0.35, 0.34, 
0.38].đồng hồ báo thức[0.04, 0.8, 0.16, 0.83].Tích vô hướng[0.41, 0.6, 0.53, 
0.63].phân loại[0.73, 0.29, 0.88, 0.32].Chiếu [0.37, 0.21, 0.49, 
0.24].Bộ mã hóa văn bản[0.21, 0.88, 0.37, 0.91].Thị giác-ngôn ngữ [0.54, 0.81, 
0.72, 0.84].Bộ mã hóa hình ảnh[0.2, 0.68, 0.38, 0.71].điểm tương tự[0.54, 
0.84, 0.72, 0.87].Thông tin phụ[0.49, 0.1, 0.67, 0.13].Học ma trận hoặc [0.73, 0.25, 0.94, 0.28].Học tương phản[0.73, 0.68, 0.96, 0.71].a): 
Khung học nhúng ngầm truyền thống[0.31, 0.43, 0.94, 
0.46].b): Khung căn chỉnh thị giác-ngôn ngữ rõ ràng của chúng tôi[0.31, 0.49, 
0.94, 0.52].
Người dùng:
Tạo mô tả chi tiết về hình ảnh.

Hình 9: Các ví dụ về SPHINX cho ước tính tư thế con người (Trên), lý luận mối quan hệ (Giữa), và hiểu sơ đồ (Dưới).

--- TRANG 12 ---
Người dùng:
Mô tả hình ảnh một cách súc tích. Bao gồm hộp giới hạn cho mỗi 
đối tượng được đề cập.
SPHINX : Một chiếc xe hơi[0.04,0.34,0.99,0.71] với các bộ phận gỗ[0.26,0.41,0.94,0.68] 
được đỗ bên lề đường[0.00,0.50,1.00,0.71] .
Lý luận thị giác tốt hơn với phát hiện đối tượng
Người dùng:
Tại sao bức tranh này hài hước?
SPHINX : Bức tranh hài hước vì việc nhìn thấy một 
chiếc xe hơi[0.04,0.34,0.99,0.71] với các bộ phận gỗ[0.26,0.41,0.94,0.68] là bất thường . Các bộ phận gỗ mang lại cho chiếc xe một diện mạo độc đáo và kỳ quặc, làm cho nó nổi bật và 
gây ra tiếng cười từ người xem.
Người dùng:
Phát hiện tất cả đối tượng được hiển thị trong hình ảnh.
SPHINX : Có bảy đối tượng được hiển thị trong hình ảnh, bao gồm một 
ô[0.77,0.46,0.80,0.63], hai 
bò[0.38,0.38,0.61,0.54;0.27,0.41,0.38,0.64], một người[0.53,0.49,0.71,0.67], 
và ba chó[0.76,0.50,0.84,0.57;0.49,0.55,0.57,0.62;0.75,0.44,0.84,0.47].
Cải thiện phát hiện đối tượng thông qua gợi ý
Người dùng:
Có bốn con chó được hiển thị trong hình ảnh. Phát hiện tất cả chó được hiển thị trong 
hình ảnh.
SPHINX : Có bốn 
chó[0.76,0.51,0.85,0.57;0.76,0.44,0.84,0.47;0.43,0.66,0.57,0.74;0.50,0.55,0.57,0.62] trong hình ảnh.
Người dùng:
Mô tả hình ảnh một cách súc tích. Bao gồm hộp giới hạn cho mỗi 
đối tượng được đề cập.
SPHINX : Một người đàn ông[0.43,0.25,0.65,0.65] mặc áo sơ mi vàng[0.43,0.29,0.63,0.46] đang 
là quần áo[0.34,0.44,0.65,0.59] trên thùng xe 
hơi[0.46,0.37,1.00,0.75] .
Người dùng:
Tại sao bức tranh này hài hước?
SPHINX : Bức tranh này hài hước vì việc nhìn thấy ai đó là 
quần áo[0.34,0.44,0.65,0.59] trên thùng xe 
hơi[0.46,0.37,1.00,0.75] là bất thường và không theo quy ước . Việc là thường được thực hiện trong nhà, trong một không gian dành riêng như phòng giặt là hoặc phòng ngủ. Việc người đàn ông đang là 
quần áo[0.34,0.44,0.65,0.59] trên thùng xe 
hơi[0.46,0.37,1.00,0.75] tạo thêm yếu tố hài hước và kỳ quặc cho cảnh tượng.

Hình 10: Các ví dụ về SPHINX cho phát hiện đối tượng cải thiện thông qua gợi ý (Trên) và lý luận thị giác tốt hơn với phát hiện đối tượng (Dưới).

--- TRANG 13 ---
SPHINX : Có, có một khuyết tật trên hình ảnh. Khuyết tật là một vết xước 
trên đối tượng kim loại. Hộp trên hình ảnh là [0.45,0.46,0.50,0.57], đó 
là vết xước trên đối tượng kim loại.
Người dùng:
Có khuyết tật trên 
hình ảnh không? nếu có hãy phát hiện nó và 
vẽ hộp trên hình ảnh
Phát hiện bất thườngƯớc tính tư thế tham chiếu nổi lên
Người dùng:
Phát hiện các điểm quan trọng của người mang ba lô xanh.
SPHINX : Các điểm quan trọng được phát hiện là tai 
trái[0.76,0.22], vai trái[0.78,0.32], vai 
phải[0.88,0.30], khuỷu tay trái[0.74,0.49], khuỷu tay 
phải[0.90,0.47], cổ tay trái[0.68,0.52], cổ tay 
phải[0.85,0.53], hông trái[0.80,0.65], hông phải[0.90,0.64], 
đầu gối trái[0.71,0.88], và đầu gối phải[0.84,0.88].

Hình 11: Các ví dụ về SPHINX cho ước tính tư thế tham chiếu nổi lên (Trên) và phát hiện bất thường thách thức (Dưới).

--- TRANG 14 ---
Người dùng: Vui lòng cung cấp mô tả ngắn cho vùng 
này:[0.37,0.44,0.97,0.80]
SPHINX : Một con bò đang liếm mặt một người đàn ông
Người dùng: Cung cấp chú thích một câu cho 
hình ảnh được cung cấp
SPHINX : Một con bò đang ngửi bánh xe đạp.
Người dùng: Tạo mô tả chi tiết về 
hình ảnh.
SPHINX : Hình ảnh có một con bò lớn màu nâu và 
trắng đang đứng trong cánh đồng cỏ. Con bò được 
đặt về phía bên trái của cánh đồng, và nó có vẻ 
như đang ngửi lốp xe đạp. Một con bò khác có thể 
được nhìn thấy ở phía sau, hơi về phía bên phải 
của con bò chính. Cảnh tượng nắm bắt bản chất 
tò mò của con bò khi nó tương tác với 
lốp xe đạp.SPHINX : Một chiếc xe đạp xanh
Người dùng: Cung cấp chú thích một câu cho 
hình ảnh được cung cấp
SPHINX : Một người đàn ông đi xe đạp trong cánh đồng cỏ..
Người dùng: Tạo mô tả chi tiết về 
hình ảnh.
SPHINX : Hình ảnh thể hiện một người đàn ông đứng 
cạnh một chiếc xe đạp leo núi màu xanh trong khu 
vực cỏ. Anh ta được đặt ở phía bên phải của chiếc 
xe đạp, với chân phải trên bàn đạp. Người đàn ông 
có vẻ như đang trong quá trình leo lên xe đạp, khi 
anh ta đang đứng trên bàn đạp và cầm xe đạp.
Có một số bông hoa vàng rải rác khắp khu vực cỏ, 
thêm một chút màu sắc cho cảnh tượng. Người đàn 
ông mặc quần short, có thể nhìn thấy khi anh ta 
đứng cạnh xe đạp.Người dùng: Vui lòng cung cấp mô tả ngắn cho 
vùng này:[0.03,0.53,0.43,0.76]
SPHINX : Một người đàn ông mặc áo sơ mi xanh
Người dùng: Cung cấp chú thích một câu cho hình ảnh được cung cấp
SPHINX : Một người đàn ông đi xe đạp trong khi cầm điện thoại di động.
Người dùng: Tạo mô tả chi tiết về hình ảnh.
SPHINX : Hình ảnh thể hiện một người đàn ông ngồi trên xe đạp trong khu 
vực cỏ, cầm điện thoại di động. Anh ta có vẻ như đang chụp ảnh tự sướng 
hoặc ghi video trong khi ngồi trên xe đạp. Người đàn ông đang đội mũ bảo 
hiểm, cho thấy rằng anh ta đã chuẩn bị để đi xe an toàn.
Có một số chiếc xe đạp khác trong cảnh, với một chiếc nằm bên trái người 
đàn ông, một chiếc khác ở bên phải, và một chiếc thứ ba ở xa hơn phía 
sau. Khu vực cỏ được rải rác với những bông hoa vàng, thêm một chút 
màu sắc cho cảnh tượng.Người dùng: Vui lòng cung cấp mô tả ngắn cho vùng này:
[0.21,0.39,0.37,0.76]SPHINX : Có 10 đối tượng được hiển thị trong 
hình ảnh, bao gồm một người[0.21,0.39,0.37,0.76], một 
mũ bảo hiểm[0.30,0.39,0.36,0.43], hai giày thể thao
[0.24,0.73,0.29,0.76;0.29,0.74,0.31,0.76], một 
xe đạp[0.03,0.53,0.43,0.76], một điện 
thoại di động[0.22,0.40,0.23,0.42], và bốn 
bò[0.37,0.44,0.97,0.80;0.89,0.50,0.99,0.69;0.51,0.42,0.64,0.48;0.01,0.43,0.14,0.59].Người dùng: Phát hiện tất cả mục tiêu trong hình ảnh này.

Hình 12: Các ví dụ về chú thích dày đặc đa cấp bởi SPHINX.

5 KẾT LUẬN

Trong bài báo này, chúng tôi đề xuất SPHINX, một mô hình ngôn ngữ lớn đa phương thức (MLLM) đa năng với khả năng tuân theo hướng dẫn thị giác đa mục đích. Trong MLLM của chúng tôi, chúng tôi giới thiệu sự kết hợp chung của ba khía cạnh khác nhau: trọng số mô hình của các LLM được tiền huấn luyện bởi dữ liệu thế giới thực và tổng hợp, điều chỉnh

--- TRANG 15 ---
nhiệm vụ cho các nhiệm vụ nhận thức và lý luận thị giác đa dạng, và nhúng thị giác từ các loại backbone thị giác khác nhau. Trên cơ sở này, chúng tôi tiếp tục thiết kế để trang bị cho SPHINX khả năng xử lý hình ảnh độ phân giải cao bằng cách kết hợp các tỷ lệ thị giác khác nhau và hình ảnh phụ, thể hiện hiệu suất hiểu thị giác chi tiết vượt trội. Thông qua chiến lược kết hợp ba chiều được đề xuất, SPHINX đạt được hiệu suất ấn tượng trên một loạt các tiêu chuẩn đánh giá đa phương thức, và có thể phục vụ như một nhà tổng quát thị giác mạnh mẽ để giải quyết phát hiện đối tượng, chú thích cấp vùng, và ước tính tư thế con người, v.v. MLLM của chúng tôi cũng có thể được tích hợp với các mô hình nền tảng thị giác khác cho chức năng rộng hơn, ví dụ, SAM (Kirillov et al., 2023) cho phân đoạn tham chiếu ngôn ngữ và Stable Diffusion (Rombach et al., 2021) cho chỉnh sửa hình ảnh. Công việc tương lai của chúng tôi sẽ tập trung vào việc kết hợp một loạt các nhiệm vụ thị giác-ngôn ngữ rộng hơn vào SPHINX để có khả năng toàn diện.

TÀI LIỆU THAM KHẢO

Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, và Dhruv Batra. Vqa: Trả lời câu hỏi thị giác. Tạp chí Quốc tế về Thị giác Máy tính, 123:4 – 31, 2015.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: một mô hình ngôn ngữ thị giác cho học few-shot. Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 35:23716–23736, 2022.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren Zhou. Qwen-vl: Một mô hình ngôn ngữ thị giác biên giới với khả năng đa dạng. ArXiv, abs/2308.12966, 2023a.

Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, và Jingren Zhou. Touchstone: Đánh giá các mô hình thị giác-ngôn ngữ bằng mô hình ngôn ngữ, 2023b.

Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, và Sağnak Taşırlar. Giới thiệu các mô hình đa phương thức của chúng tôi, 2023. URL https://www.adept.ai/blog/fuyu-8b.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Mô hình ngôn ngữ là học viên few-shot. Tiến bộ trong hệ thống xử lý thông tin thần kinh, 33:1877–1901, 2020.

Jun Chen, Han Guo, Kai Yi, Boyang Li, và Mohamed Elhoseiny. Visualgpt: Thích ứng hiệu quả dữ liệu của mô hình ngôn ngữ được tiền huấn luyện cho chú thích hình ảnh. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, trang 18030–18040, 2022.

Jun Chen, Deyao Zhu1 Xiaoqian Shen1 Xiang Li, Zechun Liu2 Pengchuan Zhang, Raghuraman Krishnamoorthi2 Vikas Chandra2 Yunyang Xiong, và Mohamed Elhoseiny. Minigpt-v2: Mô hình ngôn ngữ lớn như một giao diện thống nhất cho học đa nhiệm vụ thị giác-ngôn ngữ. bài báo trước arXiv arXiv:2310.09478, 2023a.

Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, và Rui Zhao. Shikra: Mở khóa phép thuật đối thoại tham chiếu của llm đa phương thức. bài báo trước arXiv arXiv:2306.15195, 2023b.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. Vicuna: Một chatbot nguồn mở gây ấn tượng với gpt-4 với 90%* chất lượng chatgpt. https://lmsys.org/blog/2023-03-30-vicuna/, Tháng 3 2023.

Các cộng tác viên OpenCompass. Opencompass: Một nền tảng đánh giá phổ quát cho các mô hình nền tảng. https://github.com/open-compass/opencompass, 2023.

Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, và Steven C. H. Hoi. Instructblip: Hướng tới các mô hình thị giác-ngôn ngữ đa mục đích với điều chỉnh hướng dẫn. ArXiv, abs/2305.06500, 2023.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Tiền huấn luyện các transformer hai chiều sâu để hiểu ngôn ngữ. bài báo trước arXiv arXiv:1810.04805, 2018.

--- TRANG 16 ---
Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Hiểu và sáng tạo đa phương thức tương hỗ. bài báo trước arXiv arXiv:2309.11499, 2023.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. Một hình ảnh đáng giá 16x16 từ: Transformer cho nhận dạng hình ảnh ở quy mô. bài báo trước arXiv arXiv:2010.11929, 2020.

Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: Một tiêu chuẩn đánh giá toàn diện cho mô hình ngôn ngữ lớn đa phương thức. bài báo trước arXiv arXiv:2306.13394, 2023a.

Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: Một tiêu chuẩn đánh giá toàn diện cho mô hình ngôn ngữ lớn đa phương thức. bài báo trước arXiv arXiv:2306.13394, 2023b.

Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, và Yu Qiao. Llama-adapter v2: Mô hình hướng dẫn thị giác hiệu quả tham số, 2023a.

Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Mô hình hướng dẫn thị giác hiệu quả tham số. bài báo trước arXiv arXiv:2304.15010, 2023b.

Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, và Ishan Misra. Imagebind một không gian nhúng để kết nối tất cả chúng. Hội nghị IEEE/CVF 2023 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 15180–15190, 2023.

Google. Bard. https://bard.google.com/, 2023.

Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Ke Chen, Peng Gao, Xianzhi Li, Hongsheng Li, và Pheng-Ann Heng. Point-bind & point-llm: Căn chỉnh đám mây điểm với đa phương thức để hiểu, tạo ra và tuân theo hướng dẫn 3D. ArXiv, abs/2309.00615, 2023.

Agrim Gupta, Piotr Dollar, và Ross Girshick. Lvis: Một bộ dữ liệu cho phân đoạn thể hiện từ vựng lớn. Trong Kỷ yếu hội nghị IEEE/CVF về thị giác máy tính và nhận dạng mẫu, trang 5356–5364, 2019.

Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, và Jeffrey P. Bigham. Thách thức lớn vizwiz: Trả lời câu hỏi thị giác từ người mù. Hội nghị IEEE/CVF 2018 về Thị giác Máy tính và Nhận dạng Mẫu, trang 3608–3617, 2018.

Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Điều chỉnh hướng dẫn đa phương thức. bài báo trước arXiv arXiv:2309.03905, 2023.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học tàn dư sâu cho nhận dạng hình ảnh. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu, trang 770–778, 2016a.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Học tàn dư sâu cho nhận dạng hình ảnh. Trong Kỷ yếu hội nghị IEEE về thị giác máy tính và nhận dạng mẫu, trang 770–778, 2016b.

Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, và Chuang Gan. 3d-llm: Tiêm thế giới 3d vào mô hình ngôn ngữ lớn. bài báo trước arXiv arXiv:2307.12981, 2023.

Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Ngôn ngữ không phải là tất cả những gì bạn cần: Căn chỉnh nhận thức với mô hình ngôn ngữ. bài báo trước arXiv arXiv:2302.14045, 2023.

--- TRANG 17 ---
Drew A. Hudson và Christopher D. Manning. Gqa: Một bộ dữ liệu mới cho lý luận thị giác thế giới thực và trả lời câu hỏi kết hợp. Hội nghị IEEE/CVF 2019 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 6693–6702, 2019.

Sahar Kazemzadeh, Vicente Ordonez, Marc andre Matten, và Tamara L. Berg. Referitgame: Tham chiếu đến các đối tượng trong ảnh chụp cảnh tự nhiên. Trong Hội nghị về Phương pháp Thực nghiệm trong Xử lý Ngôn ngữ Tự nhiên, 2014.

Diederik P. Kingma và Jimmy Ba. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. CoRR, abs/1412.6980, 2014.

Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Phân đoạn bất cứ điều gì. bài báo trước arXiv arXiv:2304.02643, 2023.

Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Bộ gen thị giác: Kết nối ngôn ngữ và thị giác bằng chú thích hình ảnh dày đặc có nguồn gốc từ đám đông. Tạp chí quốc tế về thị giác máy tính, 123:32–73, 2017.

Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, C. Li, và Ziwei Liu. Mimic-it: Điều chỉnh hướng dẫn trong ngữ cảnh đa phương thức. ArXiv, abs/2306.05425, 2023a.

Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, và Ziwei Liu. Otter: Một mô hình đa phương thức với điều chỉnh hướng dẫn trong ngữ cảnh. ArXiv, abs/2305.03726, 2023b.

Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, và Ying Shan. Seed-bench: Đánh giá mllm đa phương thức với hiểu biết tạo sinh. ArXiv, abs/2307.16125, 2023c.

Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi. Blip: Khởi động tiền huấn luyện hình ảnh-ngôn ngữ để hiểu và tạo ra thị giác-ngôn ngữ thống nhất. Trong Hội nghị Quốc tế về Học máy, trang 12888–12900. PMLR, 2022.

Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Khởi động tiền huấn luyện hình ảnh-ngôn ngữ với bộ mã hóa hình ảnh đóng băng và mô hình ngôn ngữ lớn. bài báo trước arXiv arXiv:2301.12597, 2023d.

Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, và Ji-Rong Wen. Đánh giá ảo giác đối tượng trong mô hình thị giác-ngôn ngữ lớn. bài báo trước arXiv arXiv:2305.10355, 2023e.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. Microsoft coco: Các đối tượng chung trong ngữ cảnh. Trong Thị giác Máy tính–ECCV 2014: Hội nghị Châu Âu lần thứ 13, Zurich, Thụy Sĩ, 6-12 tháng 9, 2014, Kỷ yếu, Phần V 13, trang 740–755. Springer, 2014.

Fangyu Liu, Guy Edward Toh Emerson, và Nigel Collier. Lý luận không gian thị giác. Giao dịch của Hiệp hội Ngôn ngữ Tính toán, 2023a.

Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee. Đường cơ sở cải thiện với điều chỉnh hướng dẫn thị giác. ArXiv, abs/2310.03744, 2023b.

Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Điều chỉnh hướng dẫn thị giác. bài báo trước arXiv arXiv:2304.08485, 2023c.

Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Điều chỉnh hướng dẫn thị giác. bài báo trước arXiv arXiv:2304.08485, 2023d.

Siyi Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chun yue Li, Jianwei Yang, Hang Su, Jun-Juan Zhu, và Lei Zhang. Grounding dino: Kết hôn dino với tiền huấn luyện nền tảng cho phát hiện đối tượng tập mở. ArXiv, abs/2303.05499, 2023e.

Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Mô hình đa phương thức của bạn có phải là người chơi toàn diện không? bài báo trước arXiv arXiv:2307.06281, 2023f.

--- TRANG 18 ---
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Mô hình đa phương thức của bạn có phải là người chơi toàn diện không? bài báo trước arXiv arXiv:2307.06281, 2023g.

Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, và Song-Chun Zhu. Iconqa: Một tiêu chuẩn mới cho hiểu sơ đồ trừu tượng và lý luận ngôn ngữ thị giác. Trong Hội nghị lần thứ 35 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS) Track về Bộ dữ liệu và Tiêu chuẩn, 2021.

Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, và Ashwin Kalyan. Học để giải thích: Lý luận đa phương thức thông qua chuỗi suy nghĩ để trả lời câu hỏi khoa học. Trong Hội nghị lần thứ 36 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS), 2022.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun yue Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, và Jianfeng Gao. Mathvista: Đánh giá lý luận toán học trong ngữ cảnh thị giác với gpt-4v, bard, và các mô hình đa phương thức lớn khác. ArXiv, abs/2310.02255, 2023.

Junhua Mao, Jonathan Huang, Alexander Toshev, Oana-Maria Camburu, Alan Loddon Yuille, và Kevin P. Murphy. Tạo ra và hiểu các mô tả đối tượng rõ ràng. Hội nghị IEEE 2016 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 11–20, 2015.

Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và Roozbeh Mottaghi. Ok-vqa: Một tiêu chuẩn trả lời câu hỏi thị giác yêu cầu kiến thức bên ngoài. Hội nghị IEEE/CVF 2019 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 3190–3199, 2019.

Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, và Anirban Chakraborty. Ocr-vqa: Trả lời câu hỏi thị giác bằng cách đọc văn bản trong hình ảnh. Hội nghị Quốc tế 2019 về Phân tích và Nhận dạng Tài liệu (ICDAR), trang 947–952, 2019.

OpenAI. Chatgpt. https://chat.openai.com, 2023a.

OpenAI. Báo cáo kỹ thuật gpt-4. ArXiv, abs/2303.08774, 2023b.

Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Học các đặc trưng thị giác mạnh mẽ mà không cần giám sát. bài báo trước arXiv arXiv:2304.07193, 2023.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Huấn luyện mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi của con người. Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 35:27730–27744, 2022.

Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, và Julien Launay. Bộ dữ liệu refinedweb cho falcon llm: vượt trội hơn các tập dữ liệu được tuyển chọn với dữ liệu web, và chỉ dữ liệu web. bài báo trước arXiv arXiv:2306.01116, 2023.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Điều chỉnh hướng dẫn với gpt-4. bài báo trước arXiv arXiv:2304.03277, 2023a.

Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, và Furu Wei. Kosmos-2: Nền tảng mô hình ngôn ngữ lớn đa phương thức cho thế giới. bài báo trước arXiv arXiv:2306.14824, 2023b.

Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, J. Hockenmaier, và Svetlana Lazebnik. Các thực thể flickr30k: Thu thập tương ứng vùng-cụm từ cho mô hình hình ảnh-câu phong phú hơn. Tạp chí Quốc tế về Thị giác Máy tính, 123:74 – 93, 2015.

Alec Radford và Karthik Narasimhan. Cải thiện hiểu biết ngôn ngữ bằng tiền huấn luyện tạo sinh. 2018.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Mô hình ngôn ngữ là học viên đa nhiệm vụ không giám sát. Blog OpenAI, 1(8):9, 2019.

--- TRANG 19 ---
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Học các mô hình thị giác có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong hội nghị quốc tế về học máy, trang 8748–8763. PMLR, 2021.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. Tổng hợp hình ảnh độ phân giải cao với mô hình khuếch tán tiềm ẩn, 2021.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Thách thức nhận dạng thị giác quy mô lớn imagenet. Tạp chí quốc tế về thị giác máy tính, 115:211–252, 2015.

Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, và Aran Komatsuzaki. Laion-400m: Bộ dữ liệu mở gồm 400 triệu cặp hình ảnh-văn bản được lọc clip. bài báo trước arXiv arXiv:2111.02114, 2021.

Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes, và Romain Beaumont. Laion-coco. https://laion.ai/blog/laion-coco/, 2022.

Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, và Roozbeh Mottaghi. A-okvqa: Một tiêu chuẩn cho trả lời câu hỏi thị giác sử dụng kiến thức thế giới. Trong Hội nghị Châu Âu về Thị giác Máy tính, 2022.

Wenqi Shao, Yutao Hu, Peng Gao, Meng Lei, Kaipeng Zhang, Fanqing Meng, Peng Xu, Siyuan Huang, Hongsheng Li, Yu Qiao, et al. Tiny lvlm-ehub: Thí nghiệm đa phương thức sớm với bard. bài báo trước arXiv arXiv:2308.03729, 2023.

ShareGPT. Sharegpt. https://sharegpt.com/, 2023.

Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, và Amanpreet Singh. Textcaps: một bộ dữ liệu cho chú thích hình ảnh với hiểu đọc. ArXiv, abs/2003.12462, 2020.

Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, và Marcus Rohrbach. Hướng tới các mô hình vqa có thể đọc. Hội nghị IEEE/CVF 2019 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 8309–8318, 2019.

Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, và Deng Cai. Pandagpt: Một mô hình để tuân theo hướng dẫn tất cả chúng. ArXiv, abs/2305.16355, 2023.

Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, và Victor Lempitsky. Sơn lại mặt nạ lớn bền vững độ phân giải với tích chập fourier. bài báo trước arXiv arXiv:2109.07161, 2021.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. Llama: Mô hình ngôn ngữ nền tảng mở và hiệu quả. bài báo trước arXiv arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Mô hình nền tảng mở và chat được tinh chỉnh. bài báo trước arXiv arXiv:2307.09288, 2023b.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Tiến bộ trong hệ thống xử lý thông tin thần kinh, 30, 2017.

Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, và Hongsheng Li. Mathcoder: Tích hợp mã liền mạch trong llm để tăng cường lý luận toán học. bài báo trước arXiv arXiv:2310.03731, 2023a.

--- TRANG 20 ---
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, và Hongxia Yang. Ofa: Thống nhất kiến trúc, nhiệm vụ, và phương thức thông qua một khung học chuỗi-đến-chuỗi đơn giản. Trong Hội nghị Quốc tế về Học máy, trang 23318–23340. PMLR, 2022.

Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Mô hình ngôn ngữ lớn cũng là bộ giải mã cuối mở cho các nhiệm vụ tập trung thị giác. bài báo trước arXiv arXiv:2305.11175, 2023b.

Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, và Dimitris Metaxas. Cải thiện tạo ra văn bản-thành-hình ảnh tổng hợp với mô hình thị giác-ngôn ngữ lớn. bài báo trước arXiv arXiv:2310.06311, 2023.

Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, và Saining Xie. Convnext v2: Đồng thiết kế và mở rộng convnet với masked autoencoder. Trong Kỷ yếu Hội nghị IEEE/CVF về Thị giác Máy tính và Nhận dạng Mẫu, trang 16133–16142, 2023.

Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, và Nan Duan. Visual chatgpt: Nói chuyện, vẽ và chỉnh sửa với mô hình nền tảng thị giác. bài báo trước arXiv arXiv:2303.04671, 2023.

Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, và Dahua Lin. Pointllm: Trao quyền cho mô hình ngôn ngữ lớn để hiểu đám mây điểm. ArXiv, abs/2308.16911, 2023.

B. Yan, Yi Jiang, Jiannan Wu, D. Wang, Ping Luo, Zehuan Yuan, và Huchuan Lu. Nhận thức thể hiện phổ quát như khám phá và truy xuất đối tượng. Hội nghị IEEE/CVF 2023 về Thị giác Máy tính và Nhận dạng Mẫu (CVPR), trang 15325–15336, 2023.

Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, và Lijuan Wang. Mm-react: Nhắc chatgpt cho lý luận và hành động đa phương thức. bài báo trước arXiv arXiv:2303.11381, 2023.

Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Mô-đun hóa trao quyền cho mô hình ngôn ngữ lớn với đa phương thức. bài báo trước arXiv arXiv:2304.14178, 2023.

Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, và Zhibo Chen. Inpaint anything: Phân đoạn bất cứ điều gì gặp sơn lại hình ảnh. bài báo trước arXiv arXiv:2304.06790, 2023a.

Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, và Lijuan Wang. Mm-vet: Đánh giá mô hình đa phương thức lớn cho khả năng tích hợp. ArXiv, abs/2308.02490, 2023b.

Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, và Yu Qiao. Llama-adapter: Tinh chỉnh hiệu quả mô hình ngôn ngữ với attention khởi tạo bằng không. bài báo trước arXiv arXiv:2303.16199, 2023a.

Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Hongsheng Li, Yu Qiao, và Peng Gao. Prompt, generate, then cache: Cascade của mô hình nền tảng tạo ra học viên few-shot mạnh mẽ. CVPR 2023, 2023b.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Mô hình ngôn ngữ transformer tiền huấn luyện mở. bài báo trước arXiv arXiv:2205.01068, 2022.

Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, và Baobao Chang. Mmicl: Trao quyền cho mô hình thị giác-ngôn ngữ với học trong ngữ cảnh đa phương thức. bài báo trước arXiv arXiv:2309.07915, 2023.

Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Giải quyết bài toán từ toán học thách thức sử dụng trình thông dịch mã gpt-4 với tự xác minh dựa trên mã. bài báo trước arXiv arXiv:2308.07921, 2023.

--- TRANG 21 ---
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Nâng cao hiểu biết thị giác-ngôn ngữ với mô hình ngôn ngữ lớn tiên tiến. bài báo trước arXiv arXiv:2304.10592, 2023.

Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng, Shanghang Zhang, và Peng Gao. Pointclip v2: Thích ứng clip cho học thế giới mở 3d mạnh mẽ. bài báo trước arXiv arXiv:2211.11682, 2022.
