# 2309.10926.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.10926.pdf
# Kích thước tệp: 392793 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
NHẬN DẠNG GIỌNG NÓI STREAMING BÁN TỰ HỒI QUY VỚI NGỮ CẢNH NHÃN
Siddhant Arora1, George Saon2, Shinji Watanabe1, Brian Kingsbury2
1Carnegie Mellon University, 2IBM Research, Yorktown Heights, USA
TÓM TẮT
Mô hình hóa không tự hồi quy (NAR) đã thu hút sự quan tâm đáng kể trong xử lý giọng nói vì các mô hình này đạt được thời gian suy luận thấp hơn đáng kể so với các mô hình tự hồi quy (AR) đồng thời cũng đạt được độ chính xác phiên âm tốt. Vì các mô hình nhận dạng giọng nói tự động (ASR) NAR phải chờ hoàn thành toàn bộ câu phát biểu trước khi xử lý, một số nghiên cứu khám phá các mô hình NAR streaming dựa trên attention blockwise cho các ứng dụng độ trễ thấp. Tuy nhiên, các mô hình NAR streaming tụt hậu đáng kể về độ chính xác so với các mô hình AR streaming và NAR không streaming. Để giải quyết vấn đề này, chúng tôi đề xuất mô hình ASR streaming "bán tự hồi quy" kết hợp các nhãn được phát ra trong các khối trước đó như ngữ cảnh bổ sung bằng cách sử dụng mạng con Mô hình Ngôn ngữ (LM). Chúng tôi cũng giới thiệu một thuật toán giải mã tham lam mới giải quyết các lỗi chèn và xóa gần ranh giới khối mà không tăng đáng kể thời gian suy luận. Các thí nghiệm cho thấy phương pháp của chúng tôi vượt trội hơn mô hình NAR streaming hiện có 19% tương đối trên Tedlium2, 16%/8% trên các bộ kiểm tra clean/other Librispeech-100, và 19%/8% trên các bộ kiểm tra Switchboard(SWB)/Callhome(CH). Nó cũng giảm khoảng cách độ chính xác với các mô hình AR streaming và NAR không streaming trong khi đạt được độ trễ thấp hơn 2,5 lần. Chúng tôi cũng chứng minh rằng phương pháp của chúng tôi có thể sử dụng hiệu quả dữ liệu văn bản bên ngoài để tiền huấn luyện mạng con LM nhằm cải thiện thêm độ chính xác ASR streaming.
Thuật ngữ chỉ mục —ASR, Streaming, CTC, Bán tự hồi quy

1. GIỚI THIỆU
Các hệ thống Nhận dạng Giọng nói Đầu cuối tới Đầu cuối (E2E ASR) [1–6] đã được nghiên cứu rộng rãi nhờ nhiều ứng dụng như trợ lý giọng nói và thiết bị nhà thông minh. Hầu hết các kiến trúc E2E, như hệ thống Recurrent Neural Network Transducer (RNN-T) [5, 7, 8] và Attention Encoder Decoder (AED) [6, 9–12], là các mô hình Tự hồi quy (AR) có điều kiện trên các nhãn trước đó để đưa ra dự đoán. Mặc dù các mô hình AR này đã đạt được hiệu suất mạnh mẽ, thời gian suy luận của chúng tăng theo chiều dài đầu ra, do đó ảnh hưởng đến trải nghiệm người dùng khi chúng được triển khai trong các ứng dụng tương tác.
Để giải quyết vấn đề này, các nghiên cứu trước đây đã giới thiệu các mô hình Không tự hồi quy (NAR) [4, 13–18] giả định độc lập có điều kiện đối với các nhãn được dự đoán trước đó và do đó có thể xuất token đồng thời. Mất mát Connectionist Temporal Classification (CTC) [4] là một phương pháp NAR đã được chứng minh đạt kết quả hứa hẹn trong khi giảm đáng kể thời gian suy luận.
Tuy nhiên, các mô hình này phải chờ câu phát biểu kết thúc trước khi có thể bắt đầu xử lý và do đó không thể được triển khai trong các ứng dụng độ trễ thấp. Hơn nữa, các phương pháp này chủ yếu dựa trên kiến trúc Transformer [19–21] nơi yêu cầu bộ nhớ và tính toán tăng bậc hai với độ dài đầu vào, khiến chúng không thực tế để xử lý các câu phát biểu rất dài. Một số nỗ lực đã được thực hiện để tạo ra các mô hình ASR streaming theo khung hình bằng cách sử dụng bộ mã hóa một chiều cho các mô hình RNN-T hoặc CTC; tuy nhiên, hiệu suất của chúng không tối ưu [22] do không có quyền truy cập vào ngữ cảnh tương lai. Do đó, đã có sự quan tâm đến các bộ mã hóa xử lý theo khối [23–26]. Nghiên cứu trước đây đã cố gắng nắm bắt thông tin toàn cục bằng cách giới thiệu một vector nhúng ngữ cảnh bổ sung [27] trong mỗi khối. Phương pháp này mở đường cho ASR streaming, đạt được thông qua suy luận đồng bộ theo khối [28, 29] của AED. Các nỗ lực tương tự [30–33] đã phát triển các hệ thống ASR RNN-T độ trễ thấp.
Được truyền cảm hứng bởi sự thành công của các mô hình NAR cho ASR offline, đã có nỗ lực xây dựng các mô hình NAR streaming [34] kết hợp attention blockwise với các mô hình CTC và đề xuất chiến lược chồng lấp động trong quá trình giải mã tham lam để giải quyết các lỗi chèn và xóa tại ranh giới khối. Trong khi các mô hình NAR streaming đạt được độ trễ thấp hơn đáng kể, độ chính xác của chúng kém hơn đáng kể so với các mô hình AR streaming và NAR không streaming. Nghiên cứu trước đây về dịch máy đã cho thấy kết quả hứa hẹn với mô hình bán tự hồi quy (SAR) [35, 36] giữ lại tính chất AR toàn cục nhưng nới lỏng tính chất AR trong các khối cục bộ. Được thúc đẩy bởi điều này, chúng tôi đặt câu hỏi liệu chúng ta có thể mã hóa các nhãn được dự đoán bởi các khối trước đó như ngữ cảnh bổ sung để cải thiện độ chính xác NAR streaming mà không tăng đáng kể thời gian suy luận.
Vì vậy, chúng tôi đề xuất mô hình ASR SAR streaming thực hiện giải mã NAR tham lam trong một khối nhưng giữ tính chất AR qua các khối bằng cách mã hóa các nhãn được phát ra tại các khối trước đó bằng cách sử dụng mạng con Mô hình Ngôn ngữ (LM), một cách hiệu quả thêm một vector ngữ cảnh "nhãn" bổ sung trong bộ mã hóa khối ngữ cảnh. Mạng con LM này được tiền huấn luyện bằng cách sử dụng mục tiêu Mô hình hóa Ngôn ngữ nhân quả trên dữ liệu chỉ văn bản, và do đó cung cấp chức năng tiêm văn bản. Trong quá trình huấn luyện, chúng tôi sử dụng teacher forcing và tạo ra các nhúng ngữ cảnh bằng cách sử dụng căn chỉnh bắt buộc [37] thu được từ các mô hình ASR dựa trên CTC. Toàn bộ mô hình của chúng tôi được huấn luyện bằng cách sử dụng mất mát cross-entropy theo khung hình, với các căn chỉnh bắt buộc đóng vai trò như một proxy cho các căn chỉnh sự thật cơ bản. Chúng tôi cũng thí nghiệm với CTC trung gian [18] và điều chỉnh "khối ngẫu nhiên" [38–41]. Cuối cùng, chúng tôi áp dụng một chiến lược giải mã đơn giản kết hợp một số khung hình không trống cuối cùng từ khối trước đó với các khung hình trong khối hiện tại trong quá trình suy luận. Chúng tôi đánh giá phương pháp của mình trên 3 bộ dữ liệu có sẵn công khai, Tedlium-2 [42], Librispeech-100 [43] và Switchboard [44]. Kết quả của chúng tôi cho thấy rằng mô hình SAR streaming được đề xuất giảm khoảng cách độ chính xác với các mô hình AR streaming và NAR không streaming trong khi đạt được độ trễ thấp hơn 2,5 lần.
Các đóng góp chính của công trình chúng tôi là (1) chúng tôi giới thiệu một mô hình ASR SAR streaming mới kết hợp các nhãn được dự đoán từ các khối trước đó như ngữ cảnh bổ sung, (2) chúng tôi đề xuất một thuật toán giải mã mới cải thiện so với các chiến lược giải mã NAR streaming hiện có, và (3) chúng tôi cho thấy rằng phương pháp của chúng tôi có thể tiền huấn luyện mạng con LM với văn bản bên ngoài để tăng cường độ chính xác ASR streaming.

2. CÔNG THỨC HÓA VẤN ĐỀ
Trong ASR, đầu vào là một chuỗi có độ dài T của các đặc trưng giọng nói, X = {xt|t=1, . . . , T}, và mục tiêu là dự đoán bản phiên âm có độ dài O tương ứng Y={yo|o=1, . . . , O}. Thông qua lý thuyết quyết định maximum a posteriori (MAP), mô hình ASR ước lượng bản phiên âm bằng cách tối đa hóa xác suất hậu nghiệm P(Y|X).
Trong mô hình NAR streaming [34], đầu vào được biểu diễn bởi một chuỗi B khối chồng lấp, U={Ub|b=1, . . . , B}. Giả định kích thước khối Lblock và kích thước bước nhảy Lhop, khối thứ b là Ub=x(Ib−1+1):(Ib−1+Lblock) trong đó Ib−1=(b−1)∗Lhop. Đặt A={at|t=1, . . . , T} là căn chỉnh cấp khung hình nơi at tương ứng với token phiên âm được căn chỉnh cho khung hình giọng nói xt. Vì các khối chồng lấp, các mô hình streaming [29] sử dụng các khung hình Lhop trung tâm từ mỗi khối để dự đoán, loại trừ Nl khung hình đầu tiên như các khung hình quá khứ và Nr khung hình cuối cùng như các khung hình tương lai để nhìn trước. Chúng ta có thể xấp xỉ [4] xác suất hậu nghiệm P(Y|X)≈max AP(A|X) và biểu diễn logarithm của nó như một tổng của các log-hậu nghiệm từ các khối chồng lấp:
log(P(Y|X))≈∑B[b=1]∑[t=Ib−1+Nl+1]^[Ib−1+Nl+Lhop] log(P(at|a1:t−1,X)). (1)
Với giả định độc lập có điều kiện (C.I.) NAR [4], at⊥a1:t−1|X, và giả định C.I. xử lý khối [23–27], at⊥U<b|Ub và at⊥U>b|Ub, Phương trình 1 đơn giản hóa thành
log(P(Y|X))=∑B[b=1]∑[t=Ib−1+Nl+1]^[Ib−1+Nl+Lhop] log(P(at|Ub)). (2)
Chúng tôi đề xuất bảo tồn tính chất AR toàn cục. Chúng tôi nới lỏng giả định C.I. NAR và sửa đổi Phương trình 2 bằng cách có điều kiện trên các nhãn được phát ra trong các khối trước đó, Ab−1=a1:(Ib−1+Nl), sử dụng mạng con LM:
log(P(Y|X))=∑B[b=1]∑[t=Ib−1+Nl+1]^[Ib−1+Nl+Lhop] log(P(at|Ab−1[z: Mạng con LM],Ub)). (3)
Bằng cách nới lỏng giả định C.I. NAR, công thức của chúng tôi xấp xỉ tốt hơn phân phối hậu nghiệm gốc trong Phương trình 1. Công thức này tăng cường độ chính xác NAR streaming bằng cách sử dụng nhúng ngữ cảnh "nhãn" từ mạng con LM trong khi duy trì độ trễ thấp bằng cách có thể xuất token đồng thời trong một khối.

3. PHƯƠNG PHÁP
Để đạt được công thức được mô tả trong Phương trình 3, chúng tôi đề xuất mô hình ASR SAR streaming được hiển thị trong Hình 1. Đối với khối thứ b, đầu ra của các khối trước đó Ab−1 được truyền qua mạng con LM để tạo ra nhúng ngữ cảnh nhãn clm[b−1]:
clm[b−1]=LM(Normalize(Ab−1)), (4)
trong đó "Normalize" đề cập đến việc loại bỏ các token lặp lại và trống từ căn chỉnh để làm cho nó tương tự như các bản phiên âm được sử dụng để huấn luyện LM sao cho Normalize(A)=Y.
Các đặc trưng giọng nói đầu vào Ub cho mỗi khối b được truyền đến bộ mã hóa khối ngữ cảnh (CBE) cùng với các nhúng ngữ cảnh:
hb,cac[b]=CBE(Ub,clm[b−1],cac[b−1]) (5)
trong đó cac[b−1] là nhúng ngữ cảnh âm thanh trước đó và cac[b] là nhúng ngữ cảnh âm thanh tiếp theo [27]. Để huấn luyện mô hình, chúng ta không thể sử dụng mất mát CTC không có căn chỉnh vì chúng ta cần đảm bảo rằng đầu ra cấp khung hình của mô hình của chúng ta căn chỉnh với các nhãn được sử dụng để tính toán các nhúng ngữ cảnh nhãn Ab−1 trong Phương trình 4. Điều này rất quan trọng để ngăn chặn sự không khớp huấn luyện-kiểm tra vì chúng ta huấn luyện bằng cách sử dụng teacher forcing của căn chỉnh, trong khi trong quá trình suy luận mô hình sử dụng các nhãn được tạo ra trong các khối trước đó. Do đó, chúng tôi huấn luyện toàn bộ mô hình của chúng tôi bằng cách sử dụng mất mát cross-entropy cấp khung hình, được ký hiệu là Loss CE:
Loss =Loss CE(Softmax(Out(∪B[b=1]hb)),A) (6)
trong đó Out(·) biểu thị một lớp tuyến tính ánh xạ đầu ra bộ mã hóa hb đến kích thước từ vựng |V| theo sau bởi một hàm softmax.

3.1. Huấn luyện và Suy luận
Thuật toán 1 Giải mã tham lam căn chỉnh
1:Yout=[∅];
2:A0=[∅];
3:Yprev=[∅];
4:U=Iterator khối âm thanh ngữ cảnh;
5:for b=0 to B do
6: clm[b−1]=LM(Normalize(Ab−1))
7: Aout[b]=Argmax(Out(CBE(Ub,clm[b−1])))
8: Ab←{Ab−1,Aout[b]}
9: Aout[b]←{Yprev,Aout[b]}
10: if Aout[b] kết thúc bằng token không trống và không phải cuối cùng then
11: Yprev=khung hình với token kết thúc trong Aout[b];
12: Aout[b]=loại bỏ khung hình với token kết thúc trong Aout[b];
13: else
14: Yprev=[∅];
15: end if
16: Yb=Normalize(Aout[b])
17: Yout←{Yout,Yb}
18:end for
19:return Yout

Chúng tôi sử dụng căn chỉnh bắt buộc CTC [37] từ một mô hình ASR NAR được huấn luyện trước như một proxy để có được căn chỉnh cấp khung hình A để huấn luyện. Chúng tôi sử dụng A cả để tính toán các nhúng ngữ cảnh nhãn (xem Phương trình 4) và như sự thật cơ bản cho Mất mát Cross Entropy (xem Phương trình 6). Hơn nữa, chúng tôi tiền huấn luyện mạng con LM bằng cách sử dụng mục tiêu LM nhân quả trên các bản phiên âm huấn luyện Y. Công thức này cho phép chúng tôi kết hợp dữ liệu văn bản bên ngoài để tiền huấn luyện mạng con LM.
Trong quá trình suy luận, chúng tôi sử dụng đầu ra cấp khung hình từ các khối trước đó như Ab−1 để tạo ra các nhúng ngữ cảnh nhãn bằng cách sử dụng Phương trình 4. Các nghiên cứu trước đây [34] đã chỉ ra rằng phương pháp đơn giản của việc chia đầu vào âm thanh thành các khối nhỏ hơn và thực hiện giải mã tham lam trên mỗi khối dẫn đến sự suy giảm độ chính xác đáng kể. Chúng tôi cũng quan sát thấy rằng hầu hết các lỗi ASR xảy ra khi ranh giới đoạn xuất hiện ở giữa một token, dẫn đến nhận dạng lặp lại trong 2 khối liên tiếp. Dựa trên quan sát này, chúng tôi đưa ra một chiến lược giải mã đơn giản được nêu trong Thuật toán 1, mà chúng tôi gọi là "giải mã tham lam căn chỉnh". Đặt đầu ra cấp khung hình từ mỗi khối được ký hiệu bởi Aout[b], sau đó căn chỉnh cấp khung hình Ab−1 đầu tiên được cập nhật thành Ab bằng cách nối nó với Aout[b]. Nếu Aout[b] kết thúc bằng các khung hình không trống, việc giải mã này loại bỏ các khung hình không trống cuối cùng Yprev từ khối hiện tại Aout[b] và thay vào đó thêm Yprev vào khối tiếp theo Aout[b+1].

4. THÍ NGHIỆM
4.1. Bộ dữ liệu
Để chứng minh tính hiệu quả của mô hình SAR streaming của chúng tôi (xem phần 3), chúng tôi đã tiến hành thí nghiệm trên 3 bộ dữ liệu ASR tiếng Anh có sẵn công khai: Tedlium-2 [42], Librispeech-100 [43] và Switchboard (SWB) [44]. Chúng tôi định lượng độ chính xác ASR bằng cách sử dụng Tỷ lệ Lỗi Từ (WER). Đối với các mô hình không streaming, độ trễ được biểu diễn một cách đơn giản bằng thời gian giải mã trung bình mỗi câu phát biểu. Đối với các mô hình streaming, chúng tôi tính toán độ trễ như Độ trễ = ∑utt(token cuối cùng được phát ra − thời lượng giọng nói) / #tổng số câu phát biểu. Ở đây, "token cuối cùng được phát ra" biểu thị thời điểm mô hình phát ra token cuối cùng, trong khi "thời lượng giọng nói" đề cập đến toàn bộ thời lượng của âm thanh đầu vào. Khi tính toán mốc thời gian mà mô hình xuất token cuối cùng, chúng tôi tính cả thời gian nhìn trước, bao gồm thời gian cần thiết trước khi việc xử lý khối tiếp theo có thể bắt đầu, và thời gian xử lý thực tế.

4.2. Baseline
Chúng tôi so sánh mô hình SAR streaming được đề xuất với các mô hình NAR và AR streaming. Ngoài ra, chúng tôi bao gồm một mô hình NAR không streaming như topline, chứng minh cách phương pháp của chúng tôi có thể thu hẹp hiệu quả khoảng cách độ chính xác trong khi duy trì độ trễ thấp hơn đáng kể. Hơn nữa, chúng tôi trình bày kết quả của một mô hình NAR streaming được huấn luyện với mất mát cross-entropy cấp khung hình (Phương trình 6) sử dụng căn chỉnh để có được hiểu biết về tác động của việc huấn luyện dựa trên căn chỉnh. Chúng tôi cũng kết hợp các kỹ thuật CTC trung gian [45] và điều chỉnh "khối ngẫu nhiên" [38] vào hệ thống SAR streaming của chúng tôi.

4.3. Thiết lập Thí nghiệm
Đối với các mô hình NAR của chúng tôi, chúng tôi sử dụng một transformer 12 lớp với chiều ẩn là 256 cho các bộ dữ liệu Librispeech-100 và Tedlium2. Mô hình NAR conformer của chúng tôi bao gồm 12 lớp với chiều ẩn là 256 cho Tedlium2 và SWB, trong khi đối với Librispeech-100, nó bao gồm 18 lớp với chiều ẩn là 256. Các mô hình AR streaming được xây dựng với một bộ mã hóa khối ngữ cảnh sử dụng Lblock=40, Lhop=16, Nl=8, Nr=16 và chia sẻ cùng các siêu tham số như các mô hình NAR. Bộ giải mã cho các mô hình AR streaming là một transformer 6 lớp. Các baseline NAR streaming bao gồm cùng các siêu tham số như các bộ mã hóa AR streaming. Mô hình SAR streaming của chúng tôi chia sẻ cùng kiến trúc bộ mã hóa blockwise như mô hình NAR streaming và kết hợp một mạng con LM. Mạng con LM bao gồm một LSTM với 2 lớp và chiều ẩn là 256 cho Tedlium2, và 1 lớp với chiều ẩn là 1024 cho Librispeech-100 và SWB. Chúng tôi thí nghiệm với việc kết hợp CTC trung gian [45] sau lớp thứ 6 với trọng số 0,3. Chúng tôi cũng khám phá tính hiệu quả của điều chỉnh "khối ngẫu nhiên" [38], nơi chúng tôi chọn ngẫu nhiên các kích thước khối từ phạm vi [35, 45] trong quá trình huấn luyện, thay vì sử dụng kích thước khối cố định. Chúng tôi sử dụng kích thước bpe là 500 cho tất cả các bộ dữ liệu.
Các mô hình AR streaming được huấn luyện bằng cách sử dụng huấn luyện kết hợp CTC-attention [10], trong khi các mô hình NAR streaming và không streaming được huấn luyện bằng cách sử dụng mất mát CTC [4]. Chúng tôi huấn luyện một mô hình NAR streaming và các mô hình SAR streaming được đề xuất với mất mát cross-entropy (Phương trình 6) sử dụng căn chỉnh cấp khung hình. Vì các căn chỉnh sự thật cơ bản không có sẵn, chúng tôi sử dụng căn chỉnh bắt buộc từ mô hình NAR conformer streaming như một proxy. Chúng tôi huấn luyện mạng con LM trên các bản phiên âm huấn luyện. Chúng tôi cũng thí nghiệm với việc huấn luyện trên dữ liệu văn bản bên ngoài, bao gồm các bản phiên âm của toàn bộ Librispeech cho Librispeech-100 và các bản phiên âm Fisher cho mô hình SAR SWB.
Chúng tôi sử dụng cả giải mã tham lam và full-path cho mô hình NAR không streaming. Chúng tôi chạy giải mã full-path cho các mô hình AR streaming và NAR streaming thông qua suy luận đồng bộ blockwise [28, 29]. Ngoài ra, chúng tôi sử dụng giải mã tham lam cùng với ánh xạ động cho suy luận chồng lấp [34] (được gọi là "giải mã chồng lấp") cho các mô hình NAR streaming của chúng tôi. Chúng tôi thực hiện suy luận cho mô hình SAR streaming bằng cách sử dụng cả "giải mã chồng lấp" và "giải mã căn chỉnh" được đề xuất của chúng tôi (xem Thuật toán 1). Suy luận của các mô hình của chúng tôi được thực hiện bằng cách sử dụng 4 công việc CPU song song (AMD EPYC 7763 @ 2.55 GHz) với bộ nhớ 64 GB. Tất cả các tham số mô hình, huấn luyện và suy luận được chọn dựa trên độ chính xác xác thực.

4.4. Kết quả và Thảo luận
Bảng 1 trình bày hiệu suất của các mô hình SAR streaming của chúng tôi cùng với các mô hình topline và baseline. Chúng tôi quan sát thấy rằng mô hình NAR streaming với giải mã tham lam (C5) và giải mã tham lam chồng lấp (C6) [34] đạt được độ trễ thấp ấn tượng nhưng suy giảm đáng kể về độ chính xác so với các mô hình AR streaming (B2) và NAR không streaming (A4). Các thí nghiệm của chúng tôi tiết lộ rằng việc huấn luyện với căn chỉnh bằng cách sử dụng mất mát cross-entropy hầu hết mang lại kết quả kém hơn so với việc huấn luyện với mất mát CTC (D3 so với C6), có thể do các khiếm khuyết trong căn chỉnh cấp khung hình. Đáng chú ý, mô hình SAR streaming của chúng tôi cải thiện độ chính xác (E1 so với D3) bằng cách kết hợp mạng con LM để mã hóa ngữ cảnh nhãn. Ngoài ra, "giải mã căn chỉnh" được đề xuất của chúng tôi chứng minh là một cải tiến có giá trị, mang lại một sự tăng cường đáng kể về độ chính xác (E2 so với E1) so với "giải mã chồng lấp" được đề xuất trong nghiên cứu trước đây [34]. Các cải tiến thêm được đạt được bằng cách giới thiệu CTC trung gian (E3) và sử dụng điều chỉnh "khối ngẫu nhiên" (E4). Mô hình SAR streaming hiệu suất tốt nhất của chúng tôi (E4) vượt trội hơn mô hình NAR streaming với giải mã chồng lấp (C6) với biên độ tương đối 19% trên Tedlium-2, 16%/8% trên các bộ kiểm tra clean/other của Librispeech-100, 19%/8% trên các bộ kiểm tra Switchboard(SWB)/Callhome(CH) của SWB chỉ với một sự gia tăng nhỏ về độ trễ. Chúng tôi cũng tiến hành kiểm tra ý nghĩa và quan sát thấy giá trị p nhỏ hơn 0,003 bằng cách sử dụng các kiểm tra Matched Pair, Signed Paired, Wilcoxon và McNemar cho tất cả các bộ dữ liệu kiểm tra. Hơn nữa, mô hình SAR streaming của chúng tôi vượt trội hơn mô hình NAR streaming với giải mã full path (E4 so với C4) với 8%, 4%, 5% tương đối trên Tedlium-2, Librispeech-100 và SWB, tương ứng, trong khi đạt được sự giảm 2,5 lần về độ trễ. Ấn tượng hơn, mô hình SAR streaming được đề xuất của chúng tôi hiệu quả thu hẹp khoảng cách độ chính xác với các mô hình NAR không streaming (E4, C6 so với A4) và thậm chí có thể bằng hoặc vượt trội hơn các mô hình AR streaming (E4 so với B2) trên Tedlium2 và Librispeech-100 tất cả trong khi mang lại tốc độ xử lý nhanh hơn 5 lần và 2,5 lần tương ứng.

Nghiên cứu phân tích về việc kết hợp ngữ cảnh nhãn: Bảng 2 hiển thị các phát hiện của chúng tôi cho các mô hình dựa trên conformer với suy luận "giải mã chồng lấp" trên bộ dữ liệu Tedlium2. Đầu tiên, chúng tôi thí nghiệm với việc sử dụng "căn chỉnh bắt buộc" từ một mô hình NAR không streaming và quan sát thấy rằng các căn chỉnh bắt buộc từ một mô hình NAR streaming dẫn đến độ chính xác tốt hơn. Trong trường hợp của mạng con LM, chúng tôi khám phá việc không tinh chỉnh LM trong quá trình huấn luyện mô hình SAR streaming, điều này chứng minh rằng việc tinh chỉnh được đề xuất là hiệu quả. Chúng tôi cũng thí nghiệm với việc không tiền huấn luyện mạng con trên mục tiêu LM nhân quả, và một lần nữa quan sát thấy rằng công thức khởi tạo mạng con LM của chúng tôi là hữu ích. Ngoài ra, chúng tôi tiến hành một nghiên cứu phân tích nơi, thay vì sử dụng mạng con LM, chúng tôi thí nghiệm với việc kết hợp các nhãn được dự đoán trước đó bằng cách sử dụng multi-sequence cross-attention [46, 47] trong bộ giải mã và quan sát thấy rằng công thức được đề xuất của chúng tôi về việc có một mạng con LM đạt được độ chính xác tốt hơn.

Nghiên cứu phân tích về tác động của tiền huấn luyện LM nhân quả: Bảng 3 hiển thị perplexity của các mô hình ngôn ngữ (LM) được tiền huấn luyện khác nhau và tác động của chúng lên độ chính xác SAR streaming. Các quan sát của chúng tôi tiết lộ rằng, đối với Tedlium2, không có sự khác biệt độ chính xác đáng kể liên quan đến một LM mạnh hơn. Tuy nhiên, trong trường hợp của Librispeech-100, việc sử dụng một LM hơi tốt hơn dẫn đến một sự tăng cường độ chính xác. Đáng chú ý, việc sử dụng dữ liệu văn bản bên ngoài càng tăng cường độ chính xác, nhấn mạnh tính hiệu quả của phương pháp của chúng tôi.

5. KẾT LUẬN
Chúng tôi giới thiệu một mô hình SAR streaming mới thực hiện giải mã NAR trong một khối trong khi duy trì tính chất AR qua các khối bằng cách mã hóa các nhãn được phát ra trong các khối trước đó bằng cách sử dụng mạng con LM. Phương pháp của chúng tôi bao gồm một thuật toán giải mã căn chỉnh đơn giản giúp giảm thiểu các lỗi nhận dạng do ranh giới khối. Chúng tôi chứng minh tính hiệu quả của việc tiền huấn luyện mạng con LM với dữ liệu văn bản bên ngoài. Chúng tôi cũng kết hợp CTC trung gian và điều chỉnh "khối ngẫu nhiên". Các thí nghiệm của chúng tôi tiết lộ độ chính xác vượt trội so với các mô hình NAR streaming và cũng có độ chính xác cạnh tranh với các mô hình AR streaming trong khi đạt được sự giảm 2,5 lần về độ trễ. Công việc tương lai sẽ khám phá việc sử dụng dữ liệu văn bản bên ngoài thông qua LM tiên tiến [48] hoặc tiêm văn bản dựa trên CTC [49].

6. LỜI CẢM ƠN
Công trình này đã sử dụng NCSA Delta thông qua cấp phát CIS210014 từ chương trình Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS), được hỗ trợ bởi các khoản tài trợ NSF #2138259, #2138286, #2138307, #2137603, #2138296.

--- TRANG 5 ---
7. TÀI LIỆU THAM KHẢO
[1] R. Prabhavalkar et al., "End-to-end speech recognition: A survey," arXiv preprint arXiv:2303.03329, 2023.
[2] J. Li et al., "Recent advances in end-to-end automatic speech recognition," APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1,
[3] S. Watanabe et al., "ESPnet: End-to-end speech processing toolkit," in Proc. Interspeech, 2018.
[4] A. Graves et al., "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks," in ICML 2006, vol. 148, 2006, pp. 369–376.
[5] A. Graves, "Sequence transduction with recurrent neural networks," CoRR, vol. abs/1211.3711, 2012.
[6] J. Chorowski et al., "Attention-based models for speech recognition," in Proc. NeurIPS, 2015, pp. 577–585.
[7] A. Graves, A. Mohamed, and G. E. Hinton, "Speech recognition with deep recurrent neural networks," in Proc. ICASSP, 2013.
[8] G. Saon et al., "Advancing RNN transducer technology for speech recognition," in Proc. ICASSP, 2021, pp. 5654–5658.
[9] W. Chan et al., "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition," in Proc. ICASSP, 2016, pp. 4960–4964.
[10] S. Kim, T. Hori, and S. Watanabe, "Joint CTC-attention based end-to-end speech recognition using multi-task learning," in Proc. ICASSP, 2017.
[11] T. Hori, S. Watanabe, and J. R. Hershey, "Joint CTC/attention decoding for end-to-end speech recognition," in Proc. ACL, 2017, pp. 518–529.
[12] Z. Tüske, K. Audhkhasi, and G. Saon, "Advancing sequence-to-sequence based speech recognition," Proc. Interspeech 2019, pp. 3780–3784, 2019.
[13] N. Chen et al., "Listen and fill in the missing letters: Non-autoregressive transformer for speech recognition," CoRR, vol. abs/1911.04908, 2019.
[14] Z. Tian et al., "Spike-triggered non-autoregressive transformer for end-to-end speech recognition," in Proc. Interspeech, 2020.
[15] Y. Higuchi et al., "Mask CTC: non-autoregressive end-to-end ASR with CTC and mask predict," in Proc. Interspeech, 2020.
[16] Z. Gao et al., "Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition," in Proc. Interspeech, 2022, pp. 2063–2067.
[17] J. Nozaki and T. Komatsu, "Relaxing the conditional independence assumption of ctc-based ASR by conditioning on intermediate predictions," in Proc. Interspeech, 2021, pp. 3735–3739.
[18] J. Lee and S. Watanabe, "Intermediate loss regularization for ctc-based speech recognition," CoRR, vol. abs/2102.03216, 2021.
[19] A. Vaswani et al., "Attention is all you need," Proc. NeurIPS, vol. 30, pp. 5998–6008, 2017.
[20] A. Gulati et al., "Conformer: Convolution-augmented transformer for speech recognition," in Proc. Interspeech, 2020, pp. 5036–5040.
[21] P. Guo et al., "Recent developments on espnet toolkit boosted by conformer," in Proc. ICASSP, 2021, pp. 5874–5878.
[22] J. Li et al., "On the comparison of popular end-to-end models for large scale speech recognition," in Proc. Interspeech, H. Meng, B. Xu, and T. F. Zheng, Eds., 2020, pp. 1–5.
[23] N. Moritz, T. Hori, and J. L. Roux, "Streaming automatic speech recognition with the transformer model," in Proc. ICASSP, 2020, pp. 6074–6078.
[24] D. Povey et al., "A time-restricted self-attention layer for ASR," in Proc. ICASSP, 2018, pp. 5874–5878.
[25] N. Jaitly et al., "An online sequence-to-sequence model using partial conditioning," in Proc. NeurIPS, 2016, pp. 5067–5075.
[26] L. Dong, F. Wang, and B. Xu, "Self-attention aligner: A latency-control end-to-end model for ASR using self-attention network and chunk-hopping," in Proc. ICASSP, 2019, pp. 5656–5660.
[27] E. Tsunoo et al., "Transformer ASR with contextual block processing," in Proc. ASRU, 2019, pp. 427–433.
[28] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, "Streaming transformer ASR with blockwise synchronous inference," CoRR, vol. abs/2006.14941, 2020.
[29] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, "Streaming transformer asr with blockwise synchronous beam search," in Proc. SLT, 2021, pp. 22–29.
[30] A. Tripathi et al., "Transformer transducer: One model unifying streaming and non-streaming speech recognition," CoRR, vol. abs/2010.03192, 2020.
[31] M. Jain et al., "RNN-T for latency controlled ASR with improved beam search," CoRR, vol. abs/1911.01629, 2019.
[32] E. Battenberg et al., "Exploring neural transducers for end-to-end speech recognition," in Proc. ASRU, 2017, pp. 206–213.
[33] W. Wang, K. Hu, and T. N. Sainath, "Deliberation of streaming rnn-transducer by non-autoregressive decoding," in Proc. ICASSP, 2022, pp. 7452–7456.
[34] T. Wang et al., "Streaming end-to-end ASR based on blockwise non-autoregressive models," in Proc. Interspeech, 2021, pp. 3755–3759.
[35] C. Wang, J. Zhang, and H. Chen, "Semi-autoregressive neural machine translation," in Proc. EMNLP, 2018, pp. 479–488.
[36] Y. Zhou et al., "Semi-autoregressive transformer for image captioning," in ICCVW 2021, 2021, pp. 3132–3136.
[37] L. Kürzinger et al., "Ctc-segmentation of large corpora for german end-to-end speech recognition," in SPECOM, ser. Lecture Notes in Computer Science, vol. 12335, 2020, pp. 267–278.
[38] K. Audhkhasi et al., "Forget a bit to learn better: Soft forgetting for ctc-based automatic speech recognition," in Proc. Interspeech, 2019, pp. 2618–2622.
[39] S. Horiguchi et al., "Online neural diarization of unlimited numbers of speakers using global and local attractors," TASLP, vol. 31, pp. 706–720, 2023.
[40] Z. Yao et al., "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit," in Proc. Interspeech, H. Hermansky et al., Eds., 2021, pp. 4054–4058.
[41] Y. Sudo et al., "Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training," in Proc. INTERSPEECH 2023, 2023, pp. 4479–4483.
[42] A. Rousseau, P. Deléglise, and Y. Estève, "Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks," in LREC, 2014, pp. 3935–3939.
[43] V. Panayotov et al., "Librispeech: An ASR corpus based on public domain audio books," in Proc. ICASSP, 2015, pp. 5206–5210.
[44] J. J. Godfrey, E. Holliman, and J. McDaniel, "SWITCHBOARD: telephone speech corpus for research and development," in Proc. ICASSP, 1992, pp. 517–520.
[45] J. Lee and S. Watanabe, "Intermediate loss regularization for CTC-based speech recognition," in Proc. ICASSP, 2021, pp. 6224–6228.
[46] S. Arora et al., "Token-level sequence labeling for spoken language understanding using compositional end-to-end models," in Findings of the EMNLP, 2022, pp. 5419–5429.
[47] J. Helcl, J. Libovický, and D. Varis, "CUNI system for the WMT18 multimodal translation task," in WMT, 2018, pp. 616–623.
[48] H. Touvron et al., "Llama 2: Open foundation and fine-tuned chat models," CoRR, vol. abs/2307.09288, 2023.
[49] H. Sato et al., "Text-only domain adaptation based on intermediate CTC," in Proc. Interspeech, 2022, pp. 2208–2212.
