# 2311.04219.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.04219.pdf
# Kích thước tệp: 7581317 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OtterHD: Một Mô hình Đa phương thức Độ phân giải Cao
Bo Li*Peiyuan Zhang*
Jingkang Yang†Yuanhan Zhang†Fanyi Pu†Ziwei LiuB
S-Lab, Đại học Công nghệ Nanyang, Singapore
{libo0013, peiyuan.zhang, ziwei.liu}@ntu.edu.sg
https://github.com/Luodian/Otter
https://huggingface.co/datasets/Otter-AI/MagnifierBench
Tóm tắt
Trong bài báo này, chúng tôi giới thiệu OtterHD-8B, một mô hình đa phương thức sáng tạo được phát triển từ Fuyu-8B, được thiết kế đặc biệt để diễn giải các đầu vào thị giác độ phân giải cao với độ chính xác chi tiết. Không giống như các mô hình thông thường bị giới hạn bởi các bộ mã hóa thị giác có kích thước cố định, OtterHD-8B có khả năng xử lý các chiều đầu vào linh hoạt, đảm bảo tính linh hoạt của nó qua các yêu cầu suy luận khác nhau. Cùng với mô hình này, chúng tôi giới thiệu MagnifierBench, một khung đánh giá được thiết kế để kiểm tra khả năng của các mô hình trong việc phân biệt các chi tiết nhỏ và mối quan hệ không gian của các đối tượng nhỏ. Phân tích so sánh của chúng tôi cho thấy trong khi các mô hình hàng đầu hiện tại thất bại trên benchmark này, OtterHD-8B, đặc biệt khi xử lý trực tiếp các đầu vào độ phân giải cao, vượt trội so với các đối thủ với một khoảng cách đáng kể. Các phát hiện làm sáng tỏ sự khác biệt cấu trúc trong xử lý thông tin thị giác giữa các mô hình khác nhau và ảnh hưởng mà sự khác biệt độ phân giải tiền huấn luyện của các bộ mã hóa thị giác có lên hiệu quả của mô hình trong các benchmark như vậy. Nghiên cứu của chúng tôi nhighlight vai trò quan trọng của tính linh hoạt và khả năng đầu vào độ phân giải cao trong các mô hình đa phương thức lớn và cũng minh họa tiềm năng vốn có trong sự đơn giản của kiến trúc Fuyu để xử lý dữ liệu thị giác phức tạp.
*Đóng góp ngang nhau,†Đánh giá ngang nhau về hỗ trợ,BTác giả liên hệ.
Báo cáo Kỹ thuật.arXiv:2311.04219v1 [cs.CV] 7 Nov 2023

--- TRANG 2 ---
1 Giới thiệu & Động lực
Thành công đáng kể đạt được từ việc mở rộng quy mô các mô hình ngôn ngữ [44,6,12,55] đã khơi dậy sự quan tâm đến việc mở rộng tương tự các Mô hình Đa phương thức Lớn (LMM) [31,3,25,13,24]. Thú vị là, hầu hết các nghiên cứu gần đây về LMM đã chủ yếu tập trung vào việc mở rộng các bộ giải mã văn bản. Ví dụ, series Llava [31] và Qwen-VL [4] phát hành các mô hình với kích thước khác nhau của các mô hình ngôn ngữ tích hợp, nhưng vẫn duy trì bộ mã hóa thị giác và độ phân giải đầu vào nhất quán. Có tương đối ít nỗ lực hướng vào việc khuếch đại thành phần hình ảnh của LMM. Series PaLI [9,7,8] nổi bật là một trong số ít nghiên cứu tập trung vào mở rộng quy mô bộ mã hóa thị giác. Họ cũng khám phá việc tăng độ phân giải đầu vào và đánh giá mô hình trên các nhiệm vụ OCR chi tiết. Những nghiên cứu như vậy nhấn mạnh rằng việc mở rộng quy mô đồng thời cả thành phần thị giác và ngôn ngữ cho hiệu suất nâng cao trên một loạt rộng các nhiệm vụ.

Xu hướng hiện tại trong các Mô hình Đa phương thức Lớn (LMM) có xu hướng dựa vào kiến trúc hai tháp, được cấu thành từ bộ mã hóa thị giác, bộ giải mã ngôn ngữ và cơ chế kết nối. Bộ mã hóa thị giác, được minh họa bởi các mô hình như ViT [18] và CLIP [42], thường tuân theo các độ phân giải cố định như 224×224 hoặc 336×336 trong quá trình huấn luyện. Mặc dù có thể giới thiệu hình ảnh độ phân giải cao hơn trong giai đoạn tinh chỉnh, như được chứng minh bởi các mô hình như PaLI, độ phân giải suy luận vẫn không thay đổi, hạn chế khả năng thích ứng của mô hình với các độ phân giải khác nhau và giảm tính linh hoạt thời gian suy luận. Sự cứng nhắc này có thể cản trở khả năng của mô hình trong việc xử lý và nhận diện đầu vào ở độ phân giải cao hơn, bất chấp kiến thức trước của bộ mã hóa thị giác về hình ảnh. Hơn nữa, việc tích hợp hiệu quả các mô hình thị giác và ngôn ngữ có kích thước khác nhau vào một hệ thống gắn kết là một thách thức liên tục và phức tạp đối với các nhà nghiên cứu trong lĩnh vực này.

Công việc của chúng tôi được thúc đẩy bởi mô hình Fuyu-8B [5], vốn khéo léo tránh được những hạn chế này bằng cách loại bỏ hoàn toàn bộ mã hóa thị giác và trực tiếp kết hợp thông tin cấp độ pixel vào bộ giải mã ngôn ngữ. Mô hình tận dụng embedding vị trí gốc của nó để hiểu các kích thước hình ảnh khác nhau, loại bỏ nhu cầu về các giai đoạn huấn luyện độ phân giải cao và thấp riêng biệt như được thấy trong series PaLI.

Dựa trên Fuyu, chúng tôi giới thiệu OtterHD-8B, một mô hình tuned theo hướng dẫn tiên tiến để xử lý các độ phân giải hình ảnh lớn hơn và đa dạng. OtterHD-8B là mã nguồn mở và quá trình điều chỉnh hướng dẫn được thiết kế đặc biệt để phù hợp với một loạt rộng các độ phân giải hình ảnh lên đến 1024×1024 pixel. Tính linh hoạt như vậy cho phép người dùng chọn độ phân giải đầu vào dựa trên ngân sách suy luận và bản chất nhiệm vụ của họ. Chúng tôi đánh giá OtterHD trên một loạt rộng các benchmark, bao gồm MagnifierBench: một benchmark mới mà chúng tôi phát triển tập trung vào việc đánh giá khả năng của LMM trong việc phát hiện các chi tiết nhỏ trong hình ảnh độ phân giải cao. Các hình ảnh trong MagnifierBench trưng bày những cảnh phức tạp được đông đúc bởi các đối tượng nhỏ, chủ yếu được tìm thấy trong video góc nhìn thứ nhất của các hoạt động gia đình. Quá trình biên soạn dataset yêu cầu các chú thích viên phải tỉ mỉ phóng to và tập trung vào những đối tượng nhỏ bé này, chiếm khoảng 1% kích thước hình ảnh. Trong đánh giá của chúng tôi, chúng tôi quan sát thấy rằng các mô hình độ phân giải cố định thông thường thể hiện hiệu quả hạn chế trên benchmark này, cho kết quả độ chính xác tương tự như đoán ngẫu nhiên. Ngược lại, OtterHD, khi được cung cấp đầu vào độ phân giải cao, vượt trội đáng kể so với các đối thủ. Nghiên cứu này nhấn mạnh tầm quan trọng then chốt của các đầu vào độ phân giải cao có thể thích ứng cho LMM và nêu bật điểm mạnh của thiết kế kiến trúc đơn giản của Fuyu. Đóng góp của chúng tôi có thể được tóm tắt như sau:

•Chúng tôi giới thiệu OtterHD-8B, một mô hình mới dựa trên kiến trúc Fuyu-8B, được tối ưu hóa cho các độ phân giải đầu vào khác nhau. Các đánh giá thực nghiệm của chúng tôi cho thấy mô hình thể hiện hiệu suất tiên tiến trên nhiều nhiệm vụ khi được điều chỉnh hướng dẫn với độ phân giải cao hơn.
•Chúng tôi giới thiệu MagnifierBench, một benchmark độc đáo tập trung vào đánh giá khả năng của LMM hiện đại trong việc nhận diện các thuộc tính nhỏ và mối quan hệ giữa các đối tượng trong hình ảnh lớn.

2 Kiến trúc Thống nhất cho Thị giác và Ngôn ngữ
Trong phần này, chúng tôi trước tiên giới thiệu bối cảnh của Fuyu-8B [5] và mô hình ngôn ngữ nền tảng của Fuyu, Persimmon-8B [19]. Sau đó chúng tôi trình bày chi tiết mô hình theo hướng dẫn đa phương thức được đề xuất của chúng tôi, OtterHD.

--- TRANG 3 ---
Câu hỏi Truy vấn: Có gì bên trong bức tranh này? Nhận thức & Nhận dạng Minh họa Phản hồi Mô hình: Hình ảnh mô tả một cảnh Đông Á truyền thống chi tiết, có thể có nguồn gốc Trung Quốc. Một tòa nhà lớn, được thiết kế phức tạp với nhiều tầng và mái dốc nổi bật ở trung tâm-trái. Kiến trúc, với các dầm gỗ, chạm khắc phức tạp và cấu trúc cao, gợi nhớ đến các đền hoặc cung điện Trung Quốc cổ.

Câu hỏi Truy vấn: Có bao nhiêu con lạc đà trong hình ảnh này? Phản hồi Mô hình: Khó để kiểm tra các con lạc đà trong hình ảnh phức tạp như vậy. Tuy nhiên, tôi thấy có ba con lạc đà nhìn thấy được trong hình ảnh.

Hình 1: Minh họa nhận thức và nhận dạng của OtterHD-8B. Hình ảnh là một bức tranh truyền thống Trung Quốc từ triều đại Tống, Dọc theo sông trong lễ hội Thanh Minh. Đây là một phần của tác phẩm nghệ thuật toàn bộ, với độ phân giải 2466×1766.

Bảng 1: So sánh hiệu suất của OtterHD-8B với các LMM mã nguồn mở nổi bật, chi tiết các cặp dữ liệu hướng dẫn/phản hồi, huấn luyện và độ phân giải đánh giá. Thuật ngữ Dynamic đề cập đến việc huấn luyện với các độ phân giải khác nhau như được trình bày trong Mục 4.2. Thuật ngữ Original cho biết các đánh giá sử dụng độ phân giải của mỗi hình ảnh mà không có bất kỳ thao tác thay đổi kích thước nào, trong khi các mô hình khác trải qua đánh giá với hình ảnh được thay đổi kích thước thành độ phân giải vuông nhất quán được biểu thị trong Eval Res. Chi tiết về các chỉ số được cung cấp trong Mục 4.1.

[Bảng dữ liệu so sánh các mô hình với các thông số về số cặp I/R, độ phân giải huấn luyện, độ phân giải đánh giá và các kết quả benchmark khác nhau]

2.1 Thiết kế Mô hình
Persimmon-8B [19] Persimmon-8B là một transformer chỉ giải mã với các sửa đổi như kích hoạt squared ReLU [49], mã hóa vị trí xoay [50], và embedding đầu vào\đầu ra tách rời. Nó cũng bao gồm một layernorm cho các embedding Q và K trước khi tính toán attention [16]. Mô hình có kích thước ẩn 4096, 64 head, và 36 layer, và đã thấy 737 tỷ token trong quá trình huấn luyện. Checkpoint được phát hành có khoảng 9.3B tham số, làm cho nó hơi lớn hơn Llama-7B [54], và chi phí suy luận của nó có thể so sánh với mô hình 8B tham số với embedding kết hợp.

Fuyu-8B [5] Fuyu-8B phản ánh Persimmon-8B trong thiết kế của nó như một transformer chỉ giải mã được điều chỉnh cho cả đầu vào hình ảnh và văn bản mà không có bộ mã hóa hình ảnh. Hình ảnh được chia thành các patch 30x30 và được xử lý tương tự như văn bản sử dụng causal attention. Các patch này được token hóa theo thứ tự quét raster, với một ký tự "image-newline" độc đáo chỉ ra ngắt dòng của mỗi hàng. Mô hình sử dụng embedding vị trí vốn có của nó để hiểu các kích thước hình ảnh khác nhau, loại bỏ sự cần thiết cho các giai đoạn huấn luyện độ phân giải cao và thấp riêng biệt như series PaLI.

OtterHD-8B OtterHD-8B của chúng tôi là một mô hình được điều chỉnh hướng dẫn từ Fuyu-8B, nhằm kiểm tra tác động của việc tăng độ phân giải lên hiệu suất của các nhiệm vụ downstream. Chúng tôi đã sử dụng định dạng hướng dẫn sau và sử dụng \x04 được định nghĩa bản địa của Fuyu làm token bắt đầu của câu trả lời.

{image tokens} User: {instruction} Assistant: \x04 {answer} \eos

Tương tự như Fuyu-8B, hình ảnh trước tiên được thay đổi kích thước thành kích thước mục tiêu được chỉ định và sau đó được phân đoạn thành các patch có kích thước 30x30, với padding được áp dụng cho các cạnh dưới và phải. Đối với các nghiên cứu ablation và phân tích so sánh, kích thước mục tiêu có thể được đặt thành độ phân giải cố định hoặc được lấy mẫu ngẫu nhiên trong khoảng từ 448×448 đến 1024×1024, như được trình bày trong Mục 4. Chúng tôi đã không khám phá các phương pháp tăng cường hình ảnh như cắt ngẫu nhiên. Bằng cách mở rộng hình ảnh gốc đến độ phân giải lớn hơn trong khi duy trì kích thước patch cố định, các patch hiệu quả nắm bắt các chi tiết tinh tế hơn với trường thụ cảm nhỏ hơn. Đáng chú ý, OtterHD đại diện cho LMM được điều chỉnh hướng dẫn mã nguồn mở đầu tiên được huấn luyện trên đầu vào lên đến 1024×1024. Như được chứng minh trong Mục 4, nó tiếp tục tổng quát hóa đến các độ phân giải thậm chí lớn hơn (ví dụ: 1440×1440) trong quá trình suy luận.

2.2 Chi tiết Huấn luyện
Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng mô hình Fuyu thể hiện hạn chế trong việc phản hồi các hướng dẫn cụ thể trong một số benchmark, như không thể phản hồi tốt với các chữ cái tùy chọn và có hoặc không. Điều này dẫn đến hiệu suất rất yếu trên MME [21] và MMBench [34]. Để giải quyết những thiếu sót này, chúng tôi bắt tay vào việc điều chỉnh hướng dẫn mô hình Fuyu trên hỗn hợp dữ liệu của chúng tôi và sử dụng một mẫu hướng dẫn mới. Tuy nhiên, lượng huấn luyện điều chỉnh hướng dẫn của chúng tôi tương đối nhỏ so với các LMM tiên tiến [31,4], có khả năng rằng khả năng ban đầu của Fuyu có thể bị ảnh hưởng ở một mức độ nào đó.

Hỗn hợp Dữ liệu Chúng tôi đã biên soạn tổng cộng 370K cặp hướng dẫn/phản hồi được lấy từ các dataset công cộng sau: LLaVA-Instruct [30], VQAv2 [2], GQA [23], OKVQA [36], OCRVQA [38], A-OKVQA [45], COCO-GOI [33], COCO-Caption [10], TextQA [48], RefCOCO [58], COCO-ITM [28], ImageNet [17], và LLaVA-RLHF [51]. Hỗn hợp dữ liệu và các chiến lược prompt cụ thể được thúc đẩy bởi LLaVA-1.5 [30] và Idefics-Instruct [24] để đạt được kiểm soát định dạng văn bản tốt hơn. Tất cả các dataset đều được tổ chức thành các cặp hướng dẫn/phản hồi, được tổng hợp vào một dataloader duy nhất và được lấy mẫu đồng đều trong giai đoạn huấn luyện để đảm bảo tính toàn vẹn đại diện. Trung bình, mỗi cặp hướng dẫn/phản hồi tạo ra khoảng 200 token văn bản và 342 token hình ảnh bao gồm token |NEWLINE|, khi độ phân giải đầu vào được đặt thành 512×512. Chi tiết thêm, bao gồm kích thước trung bình của hình ảnh trong mỗi dataset, có thể được tìm thấy trong Phụ lục A.1.

Triển khai & Tối ưu hóa
Các thí nghiệm của chúng tôi sử dụng thư viện PyTorch kết hợp với framework HuggingFace transformers [56]. Chúng tôi thấy rằng việc triển khai HuggingFace gốc của Fuyu-8B rất chưa được tối ưu hóa. Do đó chúng tôi tăng cường mã modeling với FlashAttention-2 [15] và các toán tử fused khác bao gồm fused layernorm, fused square ReLU, và fused rotary positional embedding từ repository FlashAttention [15]. Kiến trúc đơn giản của Fuyu tạo điều kiện cho chúng tôi làm điều này một cách khá thuận tiện. Như được minh họa trong Hình 2, các sửa đổi tăng cường đáng kể việc sử dụng GPU và throughput.

Trong các cấu hình, OB đề cập đến finetune với đầy đủ tham số, trong khi OB-Light chỉ ra LoRA finetuning với r=32 và α=32. Các module được nhắm mục tiêu để sửa đổi bao gồm tất cả attention và linear layer, bao gồm head layer.

Triển khai của chúng tôi cho phép hoàn thành huấn luyện đầy đủ tham số trong vòng 3 giờ mỗi epoch trên 8×A100 GPU. Ngoài ra, LoRA finetuning chỉ yêu cầu 1 giờ mỗi epoch. Mô hình được huấn luyện với batch size 64 sử dụng optimizer AdamW, được đặt với learning rate 1×10^-5 và weight decay 0.1. Một thảo luận về điều chỉnh đầy đủ tham số và LoRA được cung cấp trong Phụ lục A.3 và chi tiết thêm được cung cấp trong Phụ lục A.2.

--- TRANG 4 ---
[Hình 2: Đánh giá so sánh throughput qua các mô hình khác nhau. Chỉ số throughput huấn luyện, được biểu thị là token per second per GPU, được xác định bằng cách ghi lại giá trị cho mỗi batch và sau đó tính trung bình trong khoảng thời gian 30 phút. Token bao gồm cả token hình ảnh và văn bản.]

Nhận dạng Màu sắc                    Định vị Đối tượng

Câu hỏi: Laptop ở đâu? Gợi ý tùy chọn: A. Trên sofa xanh lá cây B. Trên bàn ăn vuông C. Trên ghế đẩu tròn màu xám D. Trên bàn tròn màu nâu
Đa lựa chọn. Trả lời: C
Dạng tự do
Trả lời: Trên ghế đẩu tròn màu xám

Đếm Đối tượng
Câu hỏi: Có bao nhiêu nón giao thông trên cỏ?
Gợi ý tùy chọn: A. 3 B. 4 C. 5 D. 6
Đa lựa chọn. Trả lời: C
Dạng tự do
Trả lời: 5

Câu hỏi: Nắp của chai nhỏ gần nhất với nắp xanh lá cây của lọ gia vị có màu gì?
Gợi ý tùy chọn: A. Xanh lá cây, B. Vàng, C. Trắng, D. Tím
Đa lựa chọn. Trả lời: D
Dạng tự do. Trả lời: Tím

Hình 3: Minh họa mẫu của ba loại câu hỏi trong MagnifierBench. Mỗi câu hỏi được liên kết với hai loại câu hỏi và câu trả lời. Độ phân giải là 1080×1920 pixel cho cả hình ảnh trái và phải, trong khi hình ảnh trung tâm có 640×480 pixel.

3 MagnifierBench
Hệ thống thị giác con người có thể tự nhiên nhận thức các chi tiết của các đối tượng nhỏ trong một trường nhìn rộng, nhưng các benchmark hiện tại để kiểm tra LMM chưa tập trung cụ thể vào việc đánh giá khả năng này. Điều này có thể là do kích thước đầu vào của các mô hình Thị giác-Ngôn ngữ chính được giới hạn ở các độ phân giải tương đối nhỏ. Với sự xuất hiện của các mô hình Fuyu và OtterHD, chúng ta có thể, lần đầu tiên, mở rộng độ phân giải đầu vào đến một phạm vi lớn hơn nhiều. Do đó, có nhu cầu cấp thiết cho một benchmark có thể kiểm tra khả năng phân biệt các chi tiết của các đối tượng nhỏ trong hình ảnh đầu vào độ phân giải cao. Trong bài báo này, chúng tôi giới thiệu MagnifierBench để lấp đầy khoảng trống này.

3.1 Chi tiết Xây dựng
Các hình ảnh của MagnifierBench được lấy từ dataset Panoptic Scene Graph Generation (PVSG) [57], bao gồm dữ liệu video có nhiều cảnh phức tạp đông đúc với các đối tượng lạ lẫm, đặc biệt trong video góc nhìn thứ nhất về việc nhà. Để sử dụng dataset PVSG, nhóm chú thích của chúng tôi được hướng dẫn đầu tiên kiểm tra video để xác định các khung hình phức tạp đặc biệt, được đặc trưng bởi sự hiện diện của nhiều đối tượng nhỏ. Một hình vuông nhỏ, tương đương 1% kích thước hình ảnh, được đặt bên cạnh mỗi video để hỗ trợ chú thích viên trong việc đánh giá quy mô của các vật phẩm nhỏ. Khi các khung hình phù hợp được xác định và ghi chú, nhiệm vụ tiếp theo của chú thích viên là phát triển các cặp câu hỏi-câu trả lời cho những đối tượng nhỏ đó. Như được mô tả trong Hình 3, mỗi câu hỏi đi kèm với chính truy vấn và bốn câu trả lời tiềm năng. Dataset của chúng tôi cung cấp hai định dạng trả lời: tùy chọn đa lựa chọn và phản hồi dạng tự do. Trong giai đoạn hậu chú thích tiếp theo, nhóm tác giả của chúng tôi đã xem xét tỉ mỉ từng mục câu hỏi-câu trả lời trong dataset. Chúng tôi đã loại bỏ bất kỳ câu hỏi nào đề cập đến các đối tượng quá lớn hoặc những câu hỏi có thể dễ dàng trả lời bằng kiến thức thường thức. Ví dụ, câu hỏi về màu sắc của điều khiển từ xa đã bị loại bỏ, vì hầu hết điều khiển từ xa đều màu đen, làm cho nó trở thành một phỏng đoán dễ dàng và loại trừ các màu như đỏ hoặc vàng.

Dataset MagnifierBench kết quả tổng hợp 283 cặp câu hỏi-câu trả lời (QA) được lấy từ 166 hình ảnh từ dataset PVSG [57]. Cụ thể, bộ sưu tập bao gồm 172 cặp QA từ 108 hình ảnh trong EpicKitchen [14], 80 QA từ 38 hình ảnh trong Ego4D [22], và 31 cặp QA từ 20 hình ảnh trong VidOR [46]. Độ phân giải điển hình của hình ảnh từ EpicKitchen và Ego4D là 1920×1080 pixel, trong khi VidOR thường là 640×480 pixel.

Hình 3 hiển thị các ví dụ từ MagnifierBench. Các loại câu hỏi được tạo ra bao gồm nhận dạng, số lượng, câu hỏi liên quan đến màu sắc, và hơn thế nữa. Chúng tôi nhấn mạnh tầm quan trọng của việc tạo ra các câu trả lời gây nhầm lẫn có thể gây nhầm lẫn hợp lý, nhưng vẫn đảm bảo rằng câu trả lời đúng vẫn rõ ràng và duy nhất, như được minh họa trong hình đính kèm. Một tiêu chí quan trọng cho dataset này là các câu hỏi đủ phức tạp để yêu cầu chú thích viên phải ở gần màn hình, phóng to và ở chế độ toàn màn hình trên máy tính để có thể phản hồi chính xác. Dataset dễ dàng truy cập và có thể được tải xuống từ Otter-AI/MagnifierBench.

3.2 Phương pháp Đánh giá
Các LMM gần đây ngày càng được điều chỉnh để tạo ra các phản hồi mở rộng trong các cài đặt trò chuyện thay vì câu trả lời ngắn. Dựa trên các kỹ thuật đánh giá trước [34], chúng tôi chia đánh giá của mình thành hai giao thức riêng biệt, mỗi giao thức được thiết kế để định lượng hiệu suất của mô hình theo cách khác nhau.

Đa lựa chọn: Trong giao thức này, mô hình đối mặt với một câu hỏi đi kèm với một số tùy chọn trả lời. Để hướng dẫn mô hình phản hồi bằng một chữ cái duy nhất (ví dụ: A, B, C), chúng tôi thêm hướng dẫn Trả lời bằng chữ cái tùy chọn từ các lựa chọn đã cho trực tiếp làm gợi ý trước câu hỏi để thúc đẩy mô hình phản hồi theo định dạng mong muốn. Trong tình huống này, chỉ những câu trả lời khớp chính xác với lựa chọn đúng mới được coi là chính xác.

Trả lời Tự do: Việc cung cấp các tùy chọn đa lựa chọn có thể đơn giản hóa nhiệm vụ, vì một phỏng đoán ngẫu nhiên có 25% cơ hội đúng. Hơn nữa, nó không phản ánh các tình huống thực tế mà trợ lý chat phải đối mặt, nơi người dùng thường không trình bày cho mô hình các tùy chọn được định nghĩa trước. Để loại bỏ thiên vị tiềm năng này, chúng tôi cũng trình bày câu hỏi cho mô hình một cách đơn giản, mở mà không có bất kỳ tùy chọn gợi ý nào. Chúng tôi sử dụng GPT-4 để đánh giá phản hồi của mô hình so với câu trả lời benchmark, cho kết quả đúng hoặc sai để tính toán độ chính xác. Các mẫu prompt cho GPT-4, cùng với các phản hồi mẫu từ cả hai loại đánh giá, có thể được tìm thấy trong Phụ lục A.4.

4 Thí nghiệm & Phân tích
Trong phần này, chúng tôi phân tích hiệu suất của OtterHD được đánh giá trên cả MagnifierBench được đề xuất của chúng tôi và một số benchmark LMM đã được thiết lập, như được nêu trong Mục 4.1. Tiếp theo, trong Mục 4.2, chúng tôi chia sẻ những hiểu biết thu được trong quá trình thí nghiệm. Cuối cùng, chúng tôi trình bày cách hiệu suất của OtterHD so sánh với các mô hình tiên tiến trong các tình huống thực tế khác nhau trong Mục 4.3.

4.1 Kết quả Đánh giá Benchmark
Trong Bảng 1, chúng tôi trình bày một so sánh toàn diện giữa OtterHD-8B và các LMM tiên tiến khác trên nhiều benchmark khác nhau. Chúng tôi trình bày hiệu suất về độ chính xác trên các benchmark bao gồm POPE [29], MM-Vet [59], MMBench [34], MathVista [35], và MagnifierBench mới được phát triển của chúng tôi dưới cả giao thức đa lựa chọn và giao thức trả lời tự do. Trên MMBench, chúng tôi báo cáo kết quả trên test set. Đối với MME [21], chúng tôi báo cáo điểm số tổng hợp trong nhận thức và perception để tuân theo quy ước đánh giá của nó. Chúng tôi bao gồm ba cài đặt khác nhau cho OtterHD: (1) huấn luyện và kiểm tra với độ phân giải cố định ở 512² hoặc 1024². (2) sử dụng phương pháp huấn luyện động trong đó hình ảnh được thay đổi kích thước ngẫu nhiên đến các độ phân giải từ tập hợp [448², 512², 768², 1024²] trong khi kiểm tra được thực hiện ở độ phân giải gốc của hình ảnh trong test set. Các phát hiện của chúng tôi cho thấy trong khi nhiều mô hình đạt điểm số cao trên các benchmark đã được thiết lập như MME và POPE, hiệu suất của họ thường không đạt yêu cầu trên MagnifierBench của chúng tôi, chứng minh sự cần thiết của những benchmark như vậy để đánh giá tổng thể hơn

--- TRANG 5 ---
[Hình 4: So sánh hiệu suất của OtterHD ở các độ phân giải đánh giá khác nhau. Ý nghĩa của cố định và động được giải thích trong Mục 4.2.]

khả năng nhận thức của LMM về các chi tiết tinh tế. Mặt khác, OtterHD-8B thể hiện hiệu suất xuất sắc trên MagnifierBench. Đáng chú ý, độ chính xác của nó cải thiện với độ phân giải cao hơn. OtterHD-8B cũng có khả năng điều chỉnh với các độ phân giải hình ảnh và tỷ lệ khung hình khác nhau trong test set khi quá trình huấn luyện bao gồm việc thay đổi kích thước động các hình ảnh. Kết quả tổng thể của chúng tôi làm nổi bật tính linh hoạt và khả năng vượt trội của OtterHD-8B trong việc xử lý một loạt rộng các nhiệm vụ và độ phân giải, làm cho nó trở thành một lựa chọn mẫu mực cho một loạt rộng các ứng dụng đa phương thức.

4.2 Hiểu biết Thực nghiệm
Tăng Độ phân giải và Tỷ lệ Hình ảnh-Văn bản Để khám phá thêm hiệu ứng của việc tăng độ phân giải và khả năng của OtterHD để tổng quát hóa đến các độ phân giải khác nhau, có thể lớn hơn, chúng tôi huấn luyện Otter8B với độ phân giải cố định hoặc động và trình bày kết quả trong Hình 4. Trục x cho thấy rằng, khi độ phân giải tăng trong quá trình đánh giá, nhiều token hình ảnh hơn được gửi đến bộ giải mã ngôn ngữ, cung cấp thêm chi tiết của hình ảnh. Chúng tôi so sánh hiệu suất trên MagnifieBench khi đánh giá trên các độ phân giải khác nhau dưới hai chiến lược huấn luyện. Cố định đại diện cho việc sử dụng cùng một độ phân giải để thay đổi kích thước vuông hình ảnh trong quá trình huấn luyện. Động có nghĩa là hình ảnh được thay đổi kích thước đến các kích thước khác nhau được lấy mẫu đồng đều từ [448,512,768,1024] trong quá trình huấn luyện. Chúng tôi đánh giá hai chiến lược trên các độ phân giải khác nhau, bao gồm 1440 để kiểm tra thêm xem mô hình có thể tổng quát hóa đến các độ phân giải thậm chí lớn hơn không.

Bảng 3 tiếp tục hiển thị các token hình ảnh, token newline hình ảnh, và token văn bản trung bình của cặp câu hỏi-câu trả lời của MagnificerBench của mỗi cài đặt.

Bảng 2: Số lượng token hình ảnh và văn bản ở các độ phân giải khác nhau.
Độ phân giải 448 512 768 1024
Token hình ảnh 225 324 676 1225
Token Newline 15 18 26 35
Token văn bản (Trung bình) 200 200 200 200

Kết quả cho thấy rằng việc tăng độ phân giải dẫn đến hiệu suất tốt hơn trên MagnifierBench. Vì token văn bản trung bình vẫn không thay đổi, tỷ lệ token hình ảnh với văn bản dần tăng, và nó cho thấy sự tập trung chi tiết vào hình ảnh, nhấn mạnh tầm quan trọng tiềm năng của các nhiệm vụ yêu cầu nhận dạng thị giác chi tiết. Sự tiến triển này làm nổi bật tầm quan trọng của độ phân giải trong LMM, đặc biệt đối với các nhiệm vụ đòi hỏi suy luận thị giác phức tạp. Ngoài ra, sự khác biệt hiệu suất giữa các phương pháp huấn luyện cố định và động làm nổi bật lợi ích của việc thay đổi kích thước động, đặc biệt trong việc ngăn ngừa overfitting với các độ phân giải cụ thể. Chiến lược động tiếp tục cho phép mô hình tổng quát hóa đến độ phân giải lớn hơn (1440) không thấy trong quá trình huấn luyện.

4.3 Minh họa Định tính
Chúng tôi củng cố các phát hiện của mình bằng các minh họa định tính được trình bày trong Hình 5 đến 8. Những minh họa này làm sáng tỏ hiệu suất của OtterHD-8B so với các LMM khác trong các tình huống thực tế, bao gồm đếm đối tượng, hiểu văn bản cảnh chi tiết, và hiểu screenshot.

--- TRANG 6 ---
Câu hỏi Truy vấn: Có bao nhiêu quả táo trong hình ảnh này?
Đếm Đối tượng
Phản hồi Mô hình:
Fuyu-8B: 9.
LLaVA-v1.5-13B: Có sáu quả táo trong hình ảnh này.
OtterHD-8B: Có tổng cộng 11 quả táo nhìn thấy được trong hình ảnh.

Câu hỏi Truy vấn: Có gì được viết trên hình ảnh này?
OCR Chi tiết
Phản hồi Mô hình:
Fuyu-8B: Congratulations Kate & Luke on your upcoming arrival!
LLaVA-1.5-13B: Hình ảnh có một chiếc bánh với thông điệp được viết trên đó, ghi "Congratulations Kate & Luke on your upcoming wedding."
OtterHD-8B: Chữ viết trên hình ảnh là "Congratulations Kate & Luke on your upcoming arrival."

Hình 5: So sánh khả năng đếm đối tượng và hiểu văn bản cảnh chi tiết giữa các LMM. Các phần không chính xác được ghi chú.

Câu hỏi Truy vấn: Tất cả văn bản cảnh trong hình ảnh là gì?
OCR Chi tiết
Phản hồi Mô hình:
Fuyu-8B: 3,642,039,905,005,000,0(lặp lại vô tận)
LLaVA-1.5-13B: Văn bản cảnh trong hình ảnh là 36423636363636363636363636363636363636363636363636363636363636363636363636...(lặp lại vô tận)
OtterHD-8B: Văn bản cảnh trong hình ảnh là "3,642,039,031,0055. Có vẻ là một hình ảnh của các số, dường như là sự kết hợp của số, chữ cái và ký hiệu.

Câu hỏi Truy vấn: Món ăn này là gì? Nó đến từ thành phố nào của Trung Quốc?
Văn bản Cảnh + Kiến thức + Suy luận
Phản hồi Mô hình:
Fuyu-8B: 面, 中国*
LLaVA-1.5-13B: Món ăn là một tô mì với rau, có thể là món mì bò, được phục vụ trong tô màu nâu. Đây là một món ăn Trung Quốc phổ biến, và nó đến từ thành phố Trùng Khánh.
OtterHD-8B: Món ăn được hiển thị trong hình ảnh được gọi là "热干面", được dịch là "Hot dry noodles". Đây là một món ăn sáng truyền thống từ Vũ Hán, thủ đô của tỉnh Hồ Bắc ở Trung Quốc...

Hình 6: So sánh khả năng hiểu màn hình máy tính giữa các LMM. Các phần không chính xác được ghi chú.

--- TRANG 7 ---
Câu hỏi Truy vấn: Tôi có thể làm gì trong trang web này?
Hiểu Màn hình máy tính
Phản hồi Mô hình:
Fuyu-8B: Mô tả hình ảnh là gì?\n\nMô tả hình ảnh là "Một hình ảnh của trang web có tên là "Hacker News."".
LLaVA-1.5-13B: Trong trang web này, bạn có thể truy cập thông tin về các dịch vụ được cung cấp bởi tổ chức LVL (Langue, Langage et Vision). Trang web được thiết kế để hỗ trợ những người khiếm thị, cung cấp cho họ hỗ trợ và tài nguyên để cải thiện kỹ năng ngôn ngữ và thị giác.
OtterHD-8B: Trong trang web này, bạn có thể tìm kiếm và tải lên nội dung thị giác của mình. Đây là nơi tuyệt vời để chia sẻ video, ảnh, nhạc và các phương tiện thị giác khác với người khác.

Câu hỏi Truy vấn: Tôi đang ở trang web nào?
Hiểu Màn hình máy tính
Phản hồi Mô hình:
Fuyu-8B: "HBR3E" memory (manas.tech)
LLaVA-1.5-13B: Bạn đang ở trong trang web có tên "Hacker News."
OtterHD-8B: Theo thông tin được cung cấp, bạn đang ở trong chương "Hacker News View | comments'

Hình 7: So sánh khả năng hiểu và suy luận văn bản cảnh chi tiết (định hướng màn hình máy tính) giữa các LMM. Các phần không chính xác được ghi chú.

Câu hỏi Truy vấn: Có gì trong screenshot màn hình máy tính hiện tại của tôi?
Hiểu Màn hình máy tính
Phản hồi Mô hình:
Fuyu-8B: Tùy chọn "Model Response: Fayou-BR" được hiển thị.
LLaVA-1.5-13B: Trong screenshot màn hình máy tính hiện tại, có một màn hình máy tính với tài liệu mở, hiển thị một slide show về táo. Slide show bao gồm hai hình ảnh của táo, một trong số đó là hình ảnh của quả táo xanh. Các hình ảnh đi kèm với văn bản, có thể cung cấp thông tin hoặc bối cảnh về táo.
OtterHD-8B: Screenshot màn hình máy tính hiện tại hiển thị hai thiệp chúc mừng với văn bản trên đó.

Câu hỏi Truy vấn: Có gì quan trọng được viết trong hình ảnh này?
Hiểu Màn hình máy tính
Phản hồi Mô hình:
Fuyu-8B: Adept –Fuyu-8B: A Multimodal Architecture for AI Agents
LLaVA-1.5-13B: Hình ảnh có một trang web với tiêu đề "Fuyu-8B: A Multimodal Architecture for AI Agents." Trang này đang quảng bá một sản phẩm hoặc dịch vụ liên quan đến trí tuệ nhân tạo và kiến trúc đa phương thức...
OtterHD-8B: Hình ảnh chứa các thông tin quan trọng sau:
Tin tức từ ADEPT
Ngày: 17 tháng 10, 2023
Tiêu đề: "Fuyu-8B Multimodal Architecture for AI Agents"

Hình 8: So sánh khả năng hiểu và suy luận văn bản cảnh chi tiết (định hướng màn hình máy tính) giữa các LMM. Các phần không chính xác được ghi chú.

--- TRANG 8 ---
5 Công trình Liên quan
5.1 Mô hình Đa phương thức Lớn
Thành công ngày càng tăng của các mô hình lớn tập trung vào một phương thức duy nhất, chẳng hạn như mô hình ngôn ngữ [41,40,54,52,11] và mô hình thị giác [43,20], đã khơi dậy làn sóng nghiên cứu gần đây khám phá sự kết hợp của các mô hình này. Mục tiêu là tích hợp các mô hình từ các phương thức khác nhau thành các cấu trúc gắn kết, có thể huấn luyện từ đầu đến cuối, được gọi là Mô hình Đa phương thức Lớn (LMM). Như được phân định bởi Zhang et al. [60], kiến trúc của LMM hiện tại có thể được phân đoạn thành ba thành phần: bộ mã hóa thị giác, bộ chiếu, và mô hình ngôn ngữ lớn (LLM). Tùy thuộc vào các biến thể trong cài đặt VPG và bộ chiếu, thiết kế của LMM đương đại có thể được phân loại thành bốn loại: (1) bộ mã hóa thị giác + resampler + cross-gated attention layer: Danh mục này bao gồm các mô hình như Flamingo [1,3] và Otter [25]. Đáng kể, Otter là một phiên bản nâng cao của OpenFlamingo [3] với các hướng dẫn được tối ưu hóa. Ở đây, resampler xử lý số lượng đặc trưng hình ảnh hoặc video khác nhau từ bộ mã hóa thị giác, tạo ra số lượng token thị giác cố định, do đó giảm cường độ tính toán của cross-attention thị giác-văn bản. Khối cross-gated attention layer được xây dựng bằng cách chèn một lớp cross-attention mới được khởi tạo trước khối self-attention đông lạnh trong lớp cross-attention ban đầu của LLM. (2) bộ mã hóa thị giác + Q-former + linear layer: Các mô hình như BLIP-2 [27] là đại diện của cấu hình này, với instructBLIP [13] là biến thể tối ưu hóa hướng dẫn của nó. Thiết kế này loại bỏ khối cross-gated attention layer phức tạp được tìm thấy trong Flamingo và áp dụng một linear layer đơn giản làm bộ chiếu đa phương thức. Q-former là một transformer nhỏ sử dụng tập hợp các vector truy vấn có thể học để thu thập đặc trưng thị giác từ bộ mã hóa hình ảnh cố định. (3) bộ mã hóa thị giác + linear layer: LLaVA [31] tiêu biểu cho cài đặt này. Trong cấu hình này, LLaVA giữ lại tất cả token thị giác để thúc đẩy LLM, bảo toàn toàn bộ thông tin thị giác. (4) chỉ linear layer: Các mô hình trong danh mục này, chẳng hạn như Fuyu, hoạt động như transformer chỉ giải mã cơ bản mà không có bộ mã hóa thị giác chuyên dụng. Trong danh mục này, các patch hình ảnh được chuyển đổi trực tiếp bởi một linear layer và chiếu vào các lớp giải mã ngôn ngữ. Lợi thế của thiết kế này nằm ở sự độc lập của nó khỏi các bộ mã hóa thị giác được tiền huấn luyện để xử lý thông tin. Do đó, mô hình không bị ràng buộc bởi các độ phân giải cố định được thích ứng bởi các bộ mã hóa thị giác được tiền huấn luyện, cho phép thích ứng tự nhiên hơn với các đầu vào hình ảnh độ phân giải cao hơn. Mô hình OtterHD của chúng tôi cũng sử dụng phương pháp thiết kế này.

5.2 Benchmarking Nhận thức Chi tiết
Nắm bắt các chi tiết thị giác phức tạp, đặc biệt là những chi tiết của các đối tượng nhỏ hơn, là quan trọng để các mô hình computer vision được áp dụng hiệu quả trong các tình huống thực tế như lái xe tự động và robot [32,53]. Tuy nhiên, trong lĩnh vực Mô hình Đa phương thức Lớn (LMM), các mô hình và benchmark hiện tại đã không đáp ứng đầy đủ yêu cầu này. Các benchmark như MME [21], MMBench [34], và SEED-Bench [26] đánh giá khả năng nhận thức của LMM, nhưng chúng không tập trung đầy đủ vào nhận thức tinh tế của các đối tượng nhỏ hơn. Trong khi các nhiệm vụ liên quan đến Nhận dạng Ký tự Quang học (OCR) [39,38,37,47] có thể xuất hiện phù hợp để đánh giá các chi tiết tinh tế, chúng chủ yếu quan tâm đến nhận dạng văn bản. Trong công việc này, chúng tôi nhấn mạnh nhu cầu quan trọng để nâng cao hiệu suất của LMM trong nhận thức chi tiết, đặc biệt liên quan đến các đối tượng nhỏ hơn. Chúng tôi nhấn mạnh tầm quan trọng của các benchmark chuyên dụng như MagnifierBench, nhằm thu hẹp khoảng cách hiện có và mở rộng khả năng của LMM về nhận thức và hiểu biết.

6 Kết luận
Trong nghiên cứu này, chúng tôi trình bày mô hình OtterHD-8B, được xây dựng trên kiến trúc sáng tạo của Fuyu-8B. Mô hình này xử lý hiệu quả các hình ảnh có độ phân giải khác nhau, thoát khỏi hạn chế truyền thống của đầu vào độ phân giải cố định được thấy trong hầu hết LMM. Được thiết kế đặc biệt để tuân theo hướng dẫn, OtterHD-8B xuất sắc trong việc xử lý hình ảnh độ phân giải cao. Điều này trở nên đặc biệt rõ ràng khi được kiểm tra với benchmark MagnifierBench mới được thiết kế để đánh giá khả năng của LMM trong việc phân biệt các chi tiết tinh tế trong các cảnh phức tạp, làm nổi bật vai trò quan trọng của tính linh hoạt độ phân giải trong LMM đương đại. Kết quả của chúng tôi không chỉ làm nổi bật lời hứa của kiến trúc giống Fuyu cho các nghiên cứu tương lai mà còn nhấn mạnh nhu cầu về các benchmark như MagnifierBench để kiểm tra nghiêm ngặt nhận thức tinh tế của LLM.

--- TRANG 9 ---
A Chi tiết Mở rộng
A.1 Hỗn hợp Dữ liệu & Độ phân giải Trung bình
Bảng 3 cung cấp một so sánh chi tiết về các độ phân giải hình ảnh trung bình (chiều rộng và chiều cao tính bằng pixel) và số lượng cặp hướng dẫn/phản hồi trong nhiều dataset. Bảng này cung cấp những hiểu biết thiết yếu về tính không đồng nhất và quy mô dữ liệu, phục vụ như một tài liệu tham khảo quan trọng để hiểu các đặc tính tính toán và thống kê của các dataset liên quan đến việc huấn luyện mô hình của chúng tôi.

Bảng 3: Tóm tắt chiều rộng, chiều cao trung bình và số lượng cặp hướng dẫn/phản hồi trên các dataset khác nhau trong hỗn hợp dữ liệu của chúng tôi. Chiều rộng và chiều cao được đo bằng pixel.

[Bảng với các dataset LLaVA-DD/CR, VQAv2, GQA, v.v. với chiều rộng, chiều cao trung bình và số cặp]

A.2 Siêu tham số
Bảng 4 cung cấp tổng quan so sánh về các siêu tham số được sử dụng trong hai phương pháp điều chỉnh hướng dẫn khác nhau: LoRA và Full-finetune. So sánh này phục vụ để làm rõ các yêu cầu tính toán và cài đặt mang lại hiệu suất tối ưu cho mỗi phương pháp. Tuy nhiên, vì các cài đặt tối ưu có thể khác nhau dựa trên tài nguyên tính toán có sẵn và độ phức tạp của vấn đề được giải quyết.

Bảng 4: So sánh cài đặt siêu tham số giữa các phương pháp LoRA và Full-finetune.

[Bảng với các thông số Batch Size, LR, LR Schedule, v.v. cho LoRA và Full-finetune]

A.3 Điều chỉnh đầy đủ tham số so với LoRA
Trong việc đánh giá hiệu quả của Low-Rank Adaptation (LoRA) trên hiệu suất mô hình trong quá trình finetuning, chúng tôi quan sát thấy các hành vi huấn luyện riêng biệt như được phân định trong Hình 9. Biểu đồ bên trái của hình làm rõ rằng việc tích hợp LoRA dẫn đến việc giảm loss huấn luyện ổn định và nhất quán hơn theo các bước batch, chỉ ra hiệu quả học tập được nâng cao so với phương pháp finetuning tham số đầy đủ thông thường. Hơn nữa, biểu đồ bên phải trong Hình 9 thể hiện tốc độ xử lý token cao hơn đáng kể trên mỗi GPU khi sử dụng LoRA, làm nổi bật đóng góp của nó vào hiệu quả tính toán được cải thiện.

Để đánh giá hiệu suất định lượng, Bảng 5 tương phản kết quả của các kỹ thuật full-finetuning và LoRA-finetuning. Sử dụng cùng một chế độ huấn luyện trên dataset LLaVA-Instruct-150K [31] trong một epoch, phương pháp LoRA-SFT đạt được sự giảm đáng kể trong thời gian huấn luyện ước tính, giảm từ ba giờ xuống chỉ một giờ. Sự giảm đáng kể này trong thời gian huấn luyện đi kèm với chỉ một sự giảm nhỏ trong các chỉ số hiệu suất trên các benchmark MagBench và MM-Vet. Những quan sát này làm nổi bật lợi ích thực tế của LoRA, cung cấp một sự đánh đổi hấp dẫn giữa hiệu quả và hiệu suất mô hình, làm cho nó trở thành một lựa chọn hấp dẫn cho các môi trường hạn chế tài nguyên hoặc các tình huống yêu cầu lặp mô hình nhanh.

Những hiểu biết thu được từ các thí nghiệm của chúng tôi cho thấy rằng việc sử dụng LoRA cho phép các nhà nghiên cứu và thực hành viên cắt giảm đáng kể tài nguyên tính toán và thời gian, vốn thường là những rào cản đáng kể trong việc finetuning các mô hình ngôn ngữ lớn. Mặc dù có một sự đánh đổi có thể quan sát được về mặt giảm nhẹ hiệu suất benchmark, sự giảm này tương đối nhỏ khi cân nhắc với lợi ích của thời gian huấn luyện giảm và tải tính toán. Sự cân bằng này định vị LoRA-finetuning như một phương pháp khả thi về mặt chiến lược, đặc biệt khi triển khai mô hình nhanh được ưu tiên, và hiệu quả tính toán có tầm quan trọng tối quan trọng.

--- TRANG 10 ---
[Hình 9: So sánh loss huấn luyện và hiệu quả xử lý token. Trái: Quỹ đạo loss huấn luyện cho các mô hình có và không có Low-Rank Adaptation (LoRA) theo các bước batch trong quá trình finetuning. Phải: Tốc độ xử lý token trên mỗi GPU cho các mô hình được finetuned với đầy đủ tham số và LoRA.]

Bảng 5: So sánh hiệu suất mô hình giữa các phương pháp Full-finetune và LoRA-finetune. Bảng hiển thị số lượng cặp hướng dẫn/phản hồi (I/R Pairs), số epoch được sử dụng trong quá trình huấn luyện, thời gian ước tính để hoàn thành, và hiệu suất trên các benchmark MagBench và MM-Vet.

[Bảng với các mô hình Fuyu-8B, Full-params., LoRA và kết quả của chúng]

A.4 Chi tiết Đánh giá MagnifierBench
Trong Hình 10, chúng tôi trình bày phân tích so sánh các LMM khác nhau khi được đánh giá trên Magnifier Benchmark, bao gồm hai loại câu hỏi chính: Đa lựa chọn và Trả lời Tự do. Ở bảng bên trái, chúng tôi quan sát các câu hỏi đa lựa chọn nơi các mô hình được mong đợi chọn tùy chọn đúng dựa trên nội dung thị giác. Ở bảng bên phải, câu hỏi trả lời tự do yêu cầu các mô hình tạo ra mô tả văn bản tương ứng với truy vấn thị giác. Sự thật cơ bản làm nổi bật các phản hồi mong muốn.

Một quan sát đáng chú ý là sự biến đổi trong các câu trả lời được cung cấp bởi các mô hình khác nhau, nhấn mạnh độ phức tạp và thách thức liên quan đến việc tích hợp thông tin thị giác và văn bản. Từ ví dụ trên được cung cấp trong Hình 10, rõ ràng là khi các tùy chọn được đưa ra, tức là được đánh giá với Đa lựa chọn, cả Fuyu và OtterHD đều chọn câu trả lời đúng D. Tuy nhiên, khi không có tùy chọn được cung cấp làm gợi ý, câu trả lời của họ không chính xác. Do đó, hai phương pháp đánh giá này có thể được sử dụng để xác minh các hành vi khác nhau của các mô hình trên MagnifierBench. Hơn nữa, chúng tôi đã mở mã nguồn tất cả các nhật ký câu hỏi và câu trả lời của các mô hình để cộng đồng phân tích thêm.

--- TRANG 11 ---
Đa lựa chọn
Câu hỏi: Trả lời bằng chữ cái tùy chọn từ các lựa chọn đã cho trực tiếp. Có gì trên lò vi sóng? A. Đĩa, B. Cốc, C. Tô, D. Lọ
Trả lời Mô hình:
Fuyu: D
Qwen: A. Đĩa
LVA-13B: A
OtterHD: D
Sự thật Cơ bản: D

Trả lời Tự do
Câu hỏi: Có gì trên lò vi sóng?
Trả lời Mô hình:
Fuyu: Một đĩa bay màu vàng trên lò vi sóng.
Qwen: Có một đĩa màu vàng trên lò vi sóng.
LVA-13B: Có túi trên lò vi sóng.
OtterHD: Có bánh mì trên lò vi sóng.
Sự thật Cơ bản: Lọ

Đa lựa chọn
Câu hỏi: Trả lời bằng chữ cái tùy chọn từ các lựa chọn đã cho trực tiếp. Có gì trên lưng ghế bên phải? A. Tất, B. Khăn, C. Mũ, D. Khăn tay
Trả lời Mô hình:
Fuyu: C
Qwen: A. Tất
LVA-13B: A
OtterHD: D
Sự thật Cơ bản: B

Trả lời Tự do
Câu hỏi: Có gì trên lưng ghế bên phải?
Trả lời Mô hình:
Fuyu: Một đĩa thức ăn.
Qwen: Có một túi màu đỏ trên lưng ghế bên phải.
LVA-13B: Có một đĩa thức ăn trên lưng ghế bên phải.
OtterHD: Có một chiếc khăn màu xanh trên lưng ghế.
Sự thật Cơ bản: Khăn

Hình 10: So sánh phản hồi của các LLM khác nhau trên MagnifierBench về hai loại câu hỏi. Các phản hồi đúng được gạch chân.

--- TRANG 12 ---
Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 10

[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425–2433, 2015. 4

[3] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 10

[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 2, 3, 4

[5] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023. 2, 3

[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 2

[7] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023. 2

[8] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut. Pali-3 vision language models: Smaller, faster, stronger, 2023. 2

[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 2

[10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 4

[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 10

[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. 2

[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. 2, 3, 10

[14] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):4125–4141, 2020. 6

[15] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023. 4

[16] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters, 2023. 3

[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 4

[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 2

[19] Erich Elsen, Augustus Odena, Maxwell Nye, Sağnak Taşırlar, Tri Dao, Curtis Hawthorne, Deepak Moparthi, and Arushi Somani. Releasing Persimmon-8B, 2023. 2, 3

[20] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358–19369, 2023. 10

[21] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 4, 6, 10

[22] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995–19012, 2022. 6

[23] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700–6709, 2019. 4

[24] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527, 2023. 2, 3, 4

[25] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning, 2023. 2, 3, 10

[26] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023. 10

[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 10

[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888–12900. PMLR, 2022. 4

[29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 6

[30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 3, 4

[31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 2, 4, 10, 11

[32] Yang Liu, Peng Sun, Nickolas Wergeles, and Yi Shang. A survey and performance evaluation of deep learning methods for small object detection. Expert Systems with Applications, 172:114602, 2021. 10

[33] Yen-Cheng Liu, Chih-Yao Ma, Xiaoliang Dai, Junjiao Tian, Peter Vajda, Zijian He, and Zsolt Kira. Open-set semi-supervised object detection. In European Conference on Computer Vision, pages 143–159. Springer, 2022. 4

[34] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 4, 6, 10

[35] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 6

[36] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195–3204, 2019. 4

[37] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for vqa on document images, 2021. 10

[38] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 4, 10

[39] Shunji Mori, Hirobumi Nishida, and Hiromitsu Yamada. Optical character recognition. John Wiley & Sons, Inc., 1999. 10

[40] OpenAI. Gpt-4 technical report. 2023. 10

[41] OpenAI. Introducing chatgpt. 2023. 10

[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2

[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 10

[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2023. 2

[45] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146–162. Springer, 2022. 4

[46] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating objects and relations in user-generated videos. In Proceedings of the 2019 on International Conference on Multimedia Retrieval, pages 279–287, 2019. 6

[47] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension, 2020. 10

[48] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317–8326, 2019. 4

[49] David R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling, 2022. 3

[50] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022. 3

[51] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 4

[52] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 10

[53] Kang Tong, Yiquan Wu, and Fei Zhou. Recent advances in small object detection based on deep learning: A review. Image and Vision Computing, 97:103910, 2020. 10

[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 3, 10

[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 2

[56] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics. 4

[57] Jingkang Yang, Wenxuan Peng, Xiangtai Li, Zujin Guo, Liangyu Chen, Bo Li, Zheng Ma, Kaiyang Zhou, Wayne Zhang, Chen Change Loy, et al. Panoptic video scene graph generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18675–18685, 2023. 5, 6

[58] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69–85. Springer, 2016. 4

[59] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 6

[60] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt generator across llms. arXiv preprint arXiv:2305.01278, 2023. 10
