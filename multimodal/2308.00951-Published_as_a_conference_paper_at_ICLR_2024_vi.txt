<<<<<<< Updated upstream
TỪ HỖN HỢP CHUYÊN GIA THƯA THỚT ĐẾN MỀM MƯỢT

Các kiến trúc hỗn hợp chuyên gia thưa thớt (MoEs) mở rộng quy mô khả năng mô hình mà không cần tăng đáng kể chi phí huấn luyện hoặc suy luận. Mặc dù thành công, MoEs gặp phải một số vấn đề: bất ổn định huấn luyện, bỏ qua token, không thể mở rộng quy mô số chuyên gia, hoặc tinh chỉnh không hiệu quả. Trong nghiên cứu này, chúng tôi đề xuất Soft MoE, một Transformer thưa thớt hoàn toàn khả vi giải quyết những thách thức này, đồng thời duy trì lợi ích của MoEs. Soft MoE thực hiện phân bổ mềm ngầm định bằng cách truyền các kết hợp có trọng số khác nhau của tất cả token đầu vào đến từng chuyên gia. Như trong các MoE khác, các chuyên gia trong Soft MoE chỉ xử lý một tập con của các token (kết hợp), cho phép khả năng mô hình lớn hơn (và hiệu suất) với chi phí suy luận thấp hơn. Trong bối cảnh nhận dạng thị giác, Soft MoE vượt trội đáng kể so với Transformers dày đặc (ViTs) và các MoEs phổ biến (Tokens Choice và Experts Choice). Hơn nữa, Soft MoE mở rộng quy mô tốt: Soft MoE Huge/14 với 128 chuyên gia trong 16 lớp MoE có hơn 40× số tham số nhiều hơn ViT Huge/14, chỉ tăng 2% thời gian suy luận, và chất lượng tốt hơn đáng kể.

1 GIỚI THIỆU

Các Transformer lớn hơn cải thiện hiệu suất với chi phí tính toán tăng lên. Các nghiên cứu gần đây cho thấy rằng kích thước mô hình và dữ liệu huấn luyện phải được mở rộng cùng nhau để sử dụng tối ưu bất kỳ ngân sách tính toán huấn luyện nào (Kaplan et al., 2020; Hoffmann et al., 2022; Zhai et al., 2022a). Một giải pháp thay thế đầy hứa hẹn cho phép mở rộng quy mô mô hình về kích thước mà không phải trả toàn bộ chi phí tính toán của chúng là hỗn hợp chuyên gia thưa thớt (MoEs). Gần đây, một số phương pháp thành công đã đề xuất cách kích hoạt thưa thớt đường dẫn token qua mạng trong ngôn ngữ (Lepikhin et al., 2020; Fedus et al., 2022), thị giác (Riquelme et al., 2021), và các mô hình đa phương thức (Mustafa et al., 2022).

Các Transformer MoE thưa thớt liên quan đến bài toán tối ưu rời rạc để quyết định module nào nên được áp dụng cho mỗi token. Các module này thường được gọi là chuyên gia và thường là MLPs. Nhiều kỹ thuật đã được phát triển để tìm sự khớp tốt giữa token và chuyên gia: chương trình tuyến tính (Lewis et al., 2021), học tăng cường (Bengio et al., 2015), quy tắc cố định xác định (Roller et al., 2021), vận chuyển tối ưu (Liu et al., 2022), top-k chuyên gia tham lam mỗi token (Shazeer et al., 2017), hoặc top-k token tham lam mỗi chuyên gia (Zhou et al., 2022). Thường cần các hàm mất mát phụ trợ thể thức để cân bằng việc sử dụng chuyên gia và giảm thiểu token chưa được phân bổ. Những thách thức này có thể lớn hơn trong cài đặt ngoài phân phối: kích thước batch suy luận nhỏ, đầu vào mới lạ, hoặc trong học chuyển giao.

Chúng tôi giới thiệu Soft MoE, vượt qua nhiều thách thức này. Thay vì sử dụng bộ định tuyến thưa thớt và rời rạc cố gắng tìm phân bổ cứng tốt giữa token và chuyên gia, Soft MoEs thay vào đó thực hiện phân bổ mềm bằng cách trộn token. Cụ thể, chúng tôi tính toán nhiều trung bình có trọng số của tất cả token—với trọng số phụ thuộc vào cả token và chuyên gia—và sau đó chúng tôi xử lý mỗi trung bình có trọng số bằng chuyên gia tương ứng.

Soft MoE L/16 vượt trội hơn ViT H/14 về upstream, few-shot và fine-tuning trong khi yêu cầu gần như một nửa thời gian huấn luyện, và nhanh hơn 2× khi suy luận. Hơn nữa, Soft MoE B/16 có hiệu suất tương đương ViT H/14 về few-shot và fine-tuning và vượt trội về các chỉ số upstream sau lượng huấn luyện tương đương. Đáng chú ý, Soft MoE B/16 nhanh hơn 5.7× khi suy luận mặc dù có 5.5× số tham số của ViT H/14 (xem Bảng 1 và Hình 5 để biết chi tiết). Phần 4 chứng minh tiềm năng của Soft MoE để mở rộng sang các tác vụ khác: chúng tôi huấn luyện một tháp văn bản mô hình tương phản với tháp thị giác cố định, cho thấy rằng các biểu diễn học được thông qua định tuyến mềm bảo tồn lợi ích của chúng cho việc căn chỉnh hình ảnh-văn bản.

2 HỖN HỢP CHUYÊN GIA MỀM

2.1 MÔ TẢ THUẬT TOÁN

Thuật toán định tuyến Soft MoE được mô tả trong Hình 2. Chúng tôi ký hiệu các token đầu vào cho một chuỗi bằng X∈Rm×d, trong đó m là số token và d là chiều của chúng. Mỗi lớp MoE sử dụng một tập n hàm chuyên gia được áp dụng trên từng token, cụ thể là {fi:Rd→Rd}1:n.

Mỗi chuyên gia xử lý p slot, và mỗi slot có một vector tham số d chiều tương ứng, Φ∈Rd×(n·p).

Cụ thể, các slot đầu vào ˜X∈R(n·p)×d là kết quả của tổ hợp lồi của tất cả m token đầu vào, X:

Dij=exp((XΦ)ij)∑m i′=1exp((XΦ)i′j), ˜X=D⊤X. (1)

Lưu ý rằng D, mà chúng tôi gọi là trọng số dispatch, đơn giản là kết quả của việc áp dụng softmax trên các cột của XΦ. Sau đó, như đã đề cập ở trên, hàm chuyên gia tương ứng được áp dụng trên mỗi slot (tức là trên các hàng của ˜X) để thu được các slot đầu ra: ˜Yi=f⌊i/p⌋(˜Xi).

Cuối cùng, các token đầu ra Y được tính toán như một tổ hợp lồi của tất cả (n·p) slot đầu ra, ˜Y, có trọng số được tính toán tương tự như trước:

Cij=exp((XΦ)ij)∑n·p j′=1exp((XΦ)ij′),Y=C˜Y. (2)

Chúng tôi gọi C là trọng số combine, và nó là kết quả của việc áp dụng softmax trên các hàng của XΦ.

Theo thiết kế thông thường cho Sparse MoEs, chúng tôi thay thế một tập con các khối MLP của Transformer bằng các khối Soft MoE. Chúng tôi thường thay thế nửa sau của các khối MLP. Tổng số slot là một siêu tham số chính của các lớp Soft MoE vì độ phức tạp thời gian phụ thuộc vào số slot chứ không phải số chuyên gia. Có thể đặt số slot bằng độ dài chuỗi đầu vào để khớp với FLOPs của Transformer dày đặc tương đương.

2.2 TÍNH CHẤT CỦA SOFT MOE VÀ KẾT NỐI VỚI SPARSE MOES

Các thuật toán Sparse MoE hoàn toàn khả vi liên quan đến bài toán phân bổ giữa token và chuyên gia, chịu ràng buộc về dung lượng và cân bằng tải. Các thuật toán khác nhau xấp xỉ giải pháp theo những cách khác nhau: ví dụ, bộ định tuyến top-k hoặc "Token Choice" (Shazeer et al., 2017; Lepikhin et al., 2020; Riquelme et al., 2021) chọn k chuyên gia được điểm cao nhất cho mỗi token, trong khi có slot khả dụng trong chuyên gia đó (tức là chuyên gia chưa đầy dung lượng). Bộ định tuyến "Expert Choice" (Zhou et al., 2022) chọn các token được điểm capacity-cao nhất cho mỗi chuyên gia. Các nghiên cứu khác đề xuất thuật toán tiên tiến hơn (và thường tốn kém hơn) để tính toán phân bổ, như phương pháp dựa trên thuật toán Lập trình Tuyến tính (Lewis et al., 2021), Vận chuyển Tối ưu (Liu et al., 2022; Clark et al., 2022) hoặc Học Tăng cường (Clark et al., 2022). Tuy nhiên, hầu như tất cả các phương pháp này đều có tính chất rời rạc và do đó không khả vi. Ngược lại, tất cả các phép toán trong lớp Soft MoE đều liên tục và hoàn toàn khả vi. Chúng ta có thể hiểu các trung bình có trọng số với điểm softmax như phân bổ mềm, thay vì phân bổ cứng được sử dụng trong Sparse MoE.

Không bỏ token và mất cân bằng chuyên gia Các cơ chế định tuyến cổ điển có xu hướng gặp phải các vấn đề như "bỏ token" (tức là một số token không được phân bổ cho bất kỳ chuyên gia nào), hoặc "mất cân bằng chuyên gia" (tức là một số chuyên gia nhận được nhiều token hơn các chuyên gia khác). Thật không may, hiệu suất có thể bị ảnh hưởng nghiêm trọng do hậu quả. Ví dụ, bộ định tuyến top-k hoặc "Token Choice" phổ biến (Shazeer et al., 2017) gặp phải cả hai vấn đề, trong khi bộ định tuyến "Expert Choice" (Zhou et al., 2022) chỉ gặp phải vấn đề trước (xem Phụ lục B để biết một số thí nghiệm về việc bỏ). Soft MoEs miễn nhiễm với việc bỏ token và mất cân bằng chuyên gia vì mọi slot đều được điền bằng trung bình có trọng số của tất cả token.

Nhanh Tổng số slot quyết định chi phí của lớp Soft MoE. Mọi đầu vào áp dụng số MLP đó. Tổng số chuyên gia không liên quan trong tính toán này: ít chuyên gia với nhiều slot mỗi chuyên gia hoặc nhiều chuyên gia với ít slot mỗi chuyên gia sẽ có chi phí khớp nhau nếu tổng số slot giống nhau. Ràng buộc duy nhất chúng ta phải đáp ứng là số slot phải lớn hơn hoặc bằng số chuyên gia (vì mỗi chuyên gia phải xử lý ít nhất một slot). Lợi thế chính của Soft MoE là hoàn toàn tránh được các phép toán sort hoặc top-k chậm và thường không phù hợp với bộ tăng tốc phần cứng. Kết quả là, Soft MoE nhanh hơn đáng kể so với hầu hết sparse MoEs (Hình 6). Xem Phần 2.3 để biết chi tiết về độ phức tạp thời gian.

Đặc điểm của cả sparse và dense Sự thưa thớt trong Sparse MoEs đến từ việc tham số chuyên gia chỉ được áp dụng cho một tập con của các token đầu vào. Tuy nhiên, Soft MoEs về mặt kỹ thuật không thưa thớt, vì mọi slot đều là trung bình có trọng số của tất cả token đầu vào. Mọi token đầu vào kích hoạt một phần tất cả tham số mô hình. Tương tự, tất cả token đầu ra đều phụ thuộc một phần vào tất cả slot (và chuyên gia). Cuối cùng, lưu ý rằng Soft MoEs cũng không phải Dense MoEs, nơi mọi chuyên gia xử lý tất cả token đầu vào, vì mọi chuyên gia chỉ xử lý một tập con của các slot.

Tính xác định theo chuỗi Dưới ràng buộc dung lượng, tất cả phương pháp Sparse MoE định tuyến token theo nhóm có kích thước cố định và thực thi (hoặc khuyến khích) cân bằng trong nhóm. Khi nhóm chứa token từ các chuỗi hoặc đầu vào khác nhau, những token này cạnh tranh cho các chỗ có sẵn trong bộ đệm chuyên gia. Do đó, mô hình không còn xác định ở cấp chuỗi, mà chỉ ở cấp batch. Các mô hình sử dụng nhóm lớn hơn có xu hướng cung cấp nhiều tự do hơn cho thuật toán định tuyến và thường hoạt động tốt hơn, nhưng chi phí tính toán của chúng cũng cao hơn.

2.3 TRIỂN KHAI

Độ phức tạp thời gian Giả sử chi phí mỗi token của một hàm chuyên gia đơn là O(k). Độ phức tạp thời gian của lớp Soft MoE sau đó là O(mnpd + npk). Bằng cách chọn p=O(m/n) slot mỗi chuyên gia, tức là số token chia cho số chuyên gia, chi phí giảm xuống O(m²d + mk). Cho rằng mỗi hàm chuyên gia có bộ tham số riêng, việc tăng số chuyên gia n và điều chỉnh p tương ứng, cho phép chúng ta tăng tổng số tham số mà không ảnh hưởng đến độ phức tạp thời gian. Hơn nữa, khi chi phí áp dụng chuyên gia lớn, số hạng mk chiếm ưu thế so với m²d, và chi phí tổng thể của lớp Soft MoE trở nên tương đương với việc áp dụng một chuyên gia đơn trên tất cả token đầu vào. Cuối cùng, ngay cả khi m²d không chiếm ưu thế, đây cũng giống như chi phí self-attention (đơn đầu), do đó nó không trở thành nút thắt cổ chai trong các mô hình Transformer. Điều này có thể thấy trong biểu đồ dưới của Hình 6 nơi thông lượng của Soft MoE hầu như không thay đổi khi số chuyên gia tăng từ 8 đến 4096 chuyên gia, trong khi Sparse MoEs bị ảnh hưởng đáng kể.

Chuẩn hóa Trong Transformers, các lớp MoE thường được sử dụng để thay thế lớp feedforward trong mỗi khối encoder. Do đó, khi sử dụng pre-normalization như hầu hết kiến trúc Transformer hiện đại (Domhan, 2018; Xiong et al., 2020; Riquelme et al., 2021; Fedus et al., 2022), đầu vào của lớp MoE được "layer normalized". Điều này gây ra vấn đề ổn định khi mở rộng chiều mô hình d, vì softmax tiến đến vector one-hot khi d→∞ (xem Phụ lục E). Do đó, trong Dòng 3 của thuật toán 1, chúng tôi thay thế X và Phi bằng l2_normalize(X, axis=1) và scale * l2_normalize(Phi, axis=0), tương ứng; trong đó scale là một số vô hướng có thể huấn luyện, và l2_normalize chuẩn hóa trục tương ứng để có chuẩn (L2) đơn vị, như Thuật toán 2 cho thấy.

Đối với các giá trị d tương đối nhỏ, chuẩn hóa có ít tác động đến chất lượng mô hình. Tuy nhiên, với chuẩn hóa đề xuất trong lớp Soft MoE, chúng ta có thể làm cho chiều mô hình lớn hơn và/hoặc tăng tốc độ học (xem Phụ lục E).

Mô hình phân tán Khi số chuyên gia tăng đáng kể, không thể vừa toàn bộ mô hình trong bộ nhớ trên một thiết bị đơn, đặc biệt trong quá trình huấn luyện hoặc khi sử dụng MoEs trên các backbone mô hình lớn. Trong những trường hợp này, chúng tôi sử dụng các kỹ thuật tiêu chuẩn để phân phối mô hình trên nhiều thiết bị, như trong (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022) và các nghiên cứu khác huấn luyện mô hình MoE lớn. Phân phối mô hình thường thêm overhead vào chi phí của mô hình, không được phân tích độ phức tạp thời gian dựa trên FLOPs mà chúng tôi đưa ra ở trên ghi nhận. Để tính đến sự khác biệt này, trong tất cả thí nghiệm, chúng tôi đo không chỉ FLOPs mà còn thời gian thực tế bằng TPUv3-chip-hours.

3 THÍ NGHIỆM PHÂN LOẠI HÌNH ẢNH

Biên Pareto huấn luyện. Trong Phần 3.3, chúng tôi so sánh các mô hình ViT dày đặc ở kích thước Small, Base, Large và Huge với các đối tác dày đặc và thưa thớt dựa trên cả định tuyến thưa thớt Tokens Choice và Experts Choice. Chúng tôi nghiên cứu hiệu suất ở các ngân sách huấn luyện khác nhau và cho thấy Soft MoE chiếm ưu thế so với các mô hình khác về hiệu suất ở ngân sách chi phí hoặc thời gian huấn luyện nhất định.

Các mô hình tối ưu hóa thời gian suy luận. Trong Phần 3.4, chúng tôi trình bày các lần chạy huấn luyện dài hơn ("overtraining"). So với ViT, Soft MoE mang lại cải thiện lớn về tốc độ suy luận cho mức hiệu suất cố định (mô hình nhỏ hơn: S, B) và hiệu suất tuyệt đối (mô hình lớn hơn: L, H).

Ablation mô hình. Trong Phần 3.5 và 3.6, chúng tôi điều tra tác động của việc thay đổi số slot và chuyên gia, và thực hiện ablation trên thuật toán định tuyến Soft MoE.

3.1 DỮ LIỆU HUẤN LUYỆN VÀ ĐÁNH GIÁ

Chúng tôi pre-train các mô hình trên JFT-4B (Zhai et al., 2022a), một bộ dữ liệu độc quyền chứa hơn 4B hình ảnh, bao gồm 29k lớp. Trong quá trình pre-training, chúng tôi đánh giá các mô hình trên hai chỉ số: precision-at-1 validation upstream trên JFT-4B, và độ chính xác ImageNet 10-shot. Chỉ số sau được tính bằng cách đóng băng trọng số mô hình và thay thế head bằng một head mới chỉ được huấn luyện trên bộ dữ liệu chứa 10 hình ảnh mỗi lớp từ ImageNet-1k (Deng et al., 2009). Cuối cùng, chúng tôi cung cấp độ chính xác trên tập validation của ImageNet-1k sau khi fine-tune trên tập huấn luyện của ImageNet-1k (1.3 triệu hình ảnh) ở độ phân giải 384.

3.2 THUẬT TOÁN ĐỊNH TUYẾN THƯA THỚT

Tokens Choice. Mỗi token chọn K chuyên gia hàng đầu có điểm định tuyến cao nhất cho token (Shazeer et al., 2017). Tăng K thường dẫn đến hiệu suất tốt hơn với chi phí tính toán tăng. Batch Priority Routing (BPR) (Riquelme et al., 2021) cải thiện đáng kể hiệu suất mô hình, đặc biệt trong trường hợp K=1 (Phụ lục F, Bảng 7). Theo đó, chúng tôi sử dụng định tuyến Top-K với BPR và K∈{1,2}. Chúng tôi cũng tối ưu số chuyên gia (Phụ lục F, Hình 11).

Experts Choice. Ngoài ra, chuyên gia có thể chọn C token hàng đầu về điểm định tuyến (Zhou et al., 2022). C là kích thước bộ đệm, và chúng tôi đặt E·C=c·T trong đó E là số chuyên gia, T là tổng số token trong nhóm, và c là hệ số dung lượng. Khi c=1, tất cả token có thể được xử lý thông qua hợp của các chuyên gia. Với định tuyến Experts Choice, thường thấy một số token được chọn đồng thời bởi nhiều chuyên gia trong khi một số token khác hoàn toàn không được chọn. Hình 10, Phụ lục B minh họa hiện tượng này. Chúng tôi thí nghiệm với c=0.5,1,2.

3.3 HUẤN LUYỆN CÁC MÔ HÌNH PARETO-TỐI ƯU

Chúng tôi huấn luyện các mô hình ViT-{S/8, S/16, S/32, B/16, B/32, L/16, L/32, H/14} và các đối tác thưa thớt của chúng. Chúng tôi huấn luyện nhiều biến thể (thay đổi K, C và số chuyên gia), tổng cộng 106 mô hình. Chúng tôi huấn luyện trong 300k bước với kích thước batch 4096, độ phân giải 224, sử dụng lịch trình tốc độ học căn bậc hai nghịch đảo.

Hình 3a và 3b cho thấy kết quả cho các mô hình trong mỗi lớp nằm trên biên Pareto chi phí/hiệu suất huấn luyện tương ứng. Trên cả hai chỉ số, Soft MoE vượt trội mạnh mẽ so với các phương pháp dày đặc và thưa thớt khác ở bất kỳ ngân sách FLOPs hoặc thời gian nhất định.

3.4 THỜI LƯỢNG HUẤN LUYỆN DÀI

Chúng tôi huấn luyện một số mô hình trong thời gian dài hơn nhiều, lên đến 4M bước. Chúng tôi huấn luyện một số Soft MoEs trên JFT, theo cài đặt tương tự Zhai et al. (2022a). Chúng tôi thay thế nửa cuối của các khối trong ViT S/16, B/16, L/16, và H/14 bằng các lớp Soft MoE với 128 chuyên gia, sử dụng một slot mỗi chuyên gia. Chúng tôi huấn luyện các mô hình từ 1B đến 54B tham số. Tất cả mô hình được huấn luyện trong 4M bước, ngoại trừ H/14 được huấn luyện trong 2M bước vì lý do chi phí.

Hình 4 cho thấy độ chính xác JFT-4B, độ chính xác ImageNet 10-shot, và độ chính xác fine-tuning ImageNet cho Soft MoE và ViT theo chi phí huấn luyện. Phụ lục F, Bảng 8 chứa kết quả số và Hình 16 cho thấy hiệu suất theo core-hours, từ đó có thể rút ra cùng kết luận. Soft MoE vượt trội đáng kể so với các mô hình ViT dày đặc cho ngân sách tính toán nhất định. Ví dụ, Soft MoE S/16 hoạt động tốt hơn ViT B/16 trên JFT và 10-shot ImageNet, và nó cũng cải thiện điểm fine-tuning trên dữ liệu ImageNet đầy đủ, mặc dù chi phí huấn luyện (và suy luận) của nó nhỏ hơn đáng kể. Tương tự, Soft MoE B/16 vượt trội ViT L/16 upstream, và chỉ kém 0.5 sau fine-tuning trong khi nhanh hơn 3x và yêu cầu gần như ít hơn 4x FLOPs. Cuối cùng, mô hình Soft MoE L/16 vượt trội mô hình H/14 dày đặc trong khi lại nhanh hơn khoảng 3x về thời gian bước huấn luyện và suy luận.

Chúng tôi tiếp tục huấn luyện các backbone nhỏ lên đến 9M bước để có được các mô hình chất lượng cao với chi phí suy luận thấp. Ngay cả sau khi huấn luyện bổ sung (quá mức), tổng thời gian huấn luyện so với các mô hình ViT lớn hơn là tương tự hoặc nhỏ hơn. Đối với những lần chạy này, cooldown dài hơn (giảm tốc độ học tuyến tính) hoạt động tốt cho Soft MoE. Do đó, chúng tôi tăng cooldown từ 50k bước lên 500k bước.

Hình 5 và Bảng 1 trình bày kết quả. Soft MoE B/16 được huấn luyện trong 1k ngày TPUv3 có hiệu suất tương đương hoặc vượt trội ViT H/14 được huấn luyện trên ngân sách tương tự, và rẻ hơn 10× khi suy luận về FLOPs (32 vs. 334 GFLOPS/img) và >5× rẻ hơn về thời gian thực tế (1.5 vs. 8.6 ms/img). Soft MoE B/16 có hiệu suất tương đương mô hình ViT H/14 khi chúng ta tăng gấp đôi ngân sách huấn luyện của ViT-H/14 (lên 2k ngày TPU). Soft MoE L/16 vượt trội tất cả mô hình ViT trong khi gần như nhanh hơn 2× khi suy luận so với ViT H/14 (4.8 vs. 8.6 ms/img).

3.5 SỐ SLOT VÀ CHUYÊN GIA

Chúng tôi nghiên cứu tác động của việc thay đổi số slot và chuyên gia trong Sparse và Soft MoEs. Hình 6 cho thấy chất lượng và tốc độ của MoEs với số chuyên gia khác nhau, và số slot mỗi token; sau này tương đương với số chuyên gia trung bình được phân bổ mỗi token cho Sparse MoEs. Khi thay đổi số chuyên gia, FLOPs backbone của mô hình không đổi, vì vậy thay đổi tốc độ là do chi phí định tuyến. Khi thay đổi slot-per-expert, số token được xử lý trong các lớp chuyên gia tăng, vì vậy thông lượng giảm. Đầu tiên, quan sát rằng đối với Soft MoE, mô hình hoạt động tốt nhất ở mỗi số slot-per-token là mô hình có nhiều chuyên gia nhất (tức là một slot mỗi chuyên gia). Đối với hai Sparse MoEs, có một điểm mà khó khăn huấn luyện vượt qua lợi ích của dung lượng bổ sung, dẫn đến số chuyên gia tối ưu khiêm tốn. Thứ hai, thông lượng của Soft MoE gần như không đổi khi thêm nhiều chuyên gia hơn. Tuy nhiên, thông lượng của Sparse MoEs giảm mạnh từ 1k chuyên gia, xem thảo luận trong Phần 2.2.

3.6 ABLATIONS

Chúng tôi nghiên cứu tác động của các thành phần của lớp định tuyến Soft MoE bằng cách chạy các ablation sau: Định tuyến đồng nhất: Token không được trộn: token đầu tiên đi đến chuyên gia đầu tiên, token thứ hai đi đến chuyên gia thứ hai, v.v. Trộn đồng nhất: Mỗi slot trộn tất cả token đầu vào theo cùng một cách: bằng cách lấy trung bình chúng, cả cho dispatch và combine. Sự đa dạng chuyên gia phát sinh từ các khởi tạo khác nhau của trọng số. Soft / Uniform: Chúng tôi học trộn token trên đầu vào của chuyên gia để tạo slot (trọng số dispatch), nhưng chúng tôi lấy trung bình đầu ra chuyên gia. Điều này có nghĩa là mỗi token đầu vào được cập nhật giống nhau trước kết nối residual. Uniform / Soft: Tất cả slot được điền bằng trung bình đồng nhất của token đầu vào. Chúng tôi học trộn slot của đầu ra chuyên gia tùy thuộc vào token đầu vào. Bảng 2 cho thấy rằng có slot là quan trọng; định tuyến Identity và Uniform kém hiệu suất đáng kể so với Soft MoE, mặc dù chúng vượt trội ViT. Trộn dispatch có vẻ quan trọng hơn một chút so với trộn combine. Xem Phụ lục A để biết chi tiết bổ sung.

4 HỌC TƯƠNG PHẢN

Chúng tôi kiểm tra liệu các biểu diễn của Soft MoE có tốt hơn cho các tác vụ khác không. Cho việc này, chúng tôi thử học tương phản hình ảnh-văn bản. Theo Zhai et al. (2022b), tháp hình ảnh được pre-train trên phân loại hình ảnh, và sau đó được đóng băng trong khi huấn luyện bộ mã hóa văn bản trên bộ dữ liệu các cặp hình ảnh-văn bản. Chúng tôi tái sử dụng các mô hình được huấn luyện trên JFT trong phần trước và so sánh hiệu suất zero-shot của chúng trên các bộ dữ liệu downstream. Cho học tương phản, chúng tôi huấn luyện trên WebLI (Chen et al., 2022), một bộ dữ liệu độc quyền gồm 10B hình ảnh và alt-text. Bộ mã hóa hình ảnh được đóng băng, trong khi bộ mã hóa văn bản được huấn luyện từ đầu.

Bảng 3 cho thấy kết quả. Nhìn chung, lợi ích chúng tôi quan sát được về phân loại hình ảnh cũng có trong cài đặt này. Ví dụ, Soft MoE-L/16 vượt trội ViT-L/16 hơn 1% và 2% trên ImageNet và Cifar-100 zero-shot, tương ứng. Tuy nhiên, cải thiện trên COCO retrieval khiêm tốn, và có thể phản ánh sự căn chỉnh kém giữa các đặc trưng học được trên JFT từ vựng đóng và tác vụ từ vựng mở này.

Cuối cùng, trong Phụ lục F.1, chúng tôi cho thấy Soft MoEs cũng vượt qua vanilla ViT và bộ định tuyến Experts Choice khi được huấn luyện từ đầu trên LAION-400M có sẵn công khai (Schuhmann et al., 2021). Với pre-training này, Soft MoEs cũng hưởng lợi từ data augmentation, nhưng cả ViT và Experts Choice đều không có vẻ hưởng lợi từ nó, điều này phù hợp với quan sát của chúng tôi trong Phần 3.5, rằng Soft MoEs tận dụng tốt hơn các tham số chuyên gia bổ sung.

5 NGHIÊN CỨU LIÊN QUAN

Nhiều nghiên cứu hiện tại hợp nhất, trộn hoặc kết hợp token đầu vào để giảm độ dài chuỗi đầu vào (Jaegle et al., 2021; Ryoo et al., 2021; Renggli et al., 2022; Wang et al., 2022), thường sử dụng trung bình có trọng số giống attention với key cố định, để cố gắng giảm bớt chi phí bậc hai của self-attention so với độ dài chuỗi. Mặc dù trọng số dispatch và combine của chúng tôi được tính toán theo cách tương tự với các phương pháp này, mục tiêu của chúng tôi không phải là giảm độ dài chuỗi (mặc dù có thể), và chúng tôi thực sự khôi phục độ dài chuỗi ban đầu sau khi tính trọng số đầu ra của chuyên gia với trọng số combine, ở cuối mỗi lớp Soft MoE.

Multi-headed attention cũng cho thấy một số điểm tương đồng với Soft MoE, ngoài việc sử dụng softmax trong trung bình có trọng số: h head khác nhau có thể được hiểu như các chuyên gia (tuyến tính) khác nhau. Sự khác biệt là, nếu m là độ dài chuỗi và mỗi token đầu vào có chiều d, mỗi trong h head xử lý m vector có kích thước d/h. m vector kết quả được kết hợp sử dụng trọng số khác nhau cho mỗi trong m' token đầu ra (tức là trọng số attention), trên mỗi head độc lập, và sau đó các vector (d/h)-chiều kết quả từ mỗi head được nối thành một có chiều d. Các chuyên gia của chúng tôi là phi tuyến và kết hợp các vector có kích thước d, ở đầu vào và đầu ra của các chuyên gia đó.

Các nghiên cứu MoE khác sử dụng tổ hợp có trọng số của các tham số chuyên gia, thay vì thực hiện định tuyến thưa thớt của các ví dụ (Yang et al., 2019; Tian et al., 2020; Muqeeth et al., 2023). Các phương pháp này cũng hoàn toàn khả vi, nhưng chúng có thể có chi phí cao hơn, vì 1) chúng phải lấy trung bình tham số của các chuyên gia, có thể trở thành nút thắt cổ chai về thời gian và/hoặc bộ nhớ khi sử dụng chuyên gia với nhiều tham số; và 2) chúng không thể tận dụng các phép toán vector hóa rộng rãi như Soft (và Sparse) MoEs, vì mọi đầu vào sử dụng tổ hợp có trọng số khác nhau của các tham số.

6 HẠN CHỾ HIỆN TẠI

Giải mã tự hồi quy Một trong những khía cạnh chính của Soft MoE bao gồm học sự hợp nhất của tất cả token trong đầu vào. Điều này làm cho việc sử dụng Soft MoEs trong bộ giải mã tự hồi quy khó khăn, vì tính nhân quả giữa token quá khứ và tương lai phải được bảo tồn trong quá trình huấn luyện. Mặc dù mask nhân quả được sử dụng trong các lớp attention có thể được sử dụng, người ta phải cẩn thận không đưa ra bất kỳ tương quan nào giữa chỉ số token và slot, vì điều này có thể thiên vị chỉ số token nào mỗi chuyên gia được huấn luyện. Việc sử dụng Soft MoE trong bộ giải mã tự hồi quy là một hướng nghiên cứu đầy hứa hẹn mà chúng tôi để dành cho nghiên cứu tương lai.

Chuyên gia lười và tiêu thụ bộ nhớ Chúng tôi cho thấy trong Phần 3 rằng một slot mỗi chuyên gia có xu hướng là sự lựa chọn tối ưu. Nói cách khác, thay vì cung cấp một chuyên gia với hai slot, việc sử dụng hai chuyên gia với một slot mỗi cái hiệu quả hơn. Chúng tôi giả thuyết các slot sử dụng cùng chuyên gia có xu hướng căn chỉnh và cung cấp ít lợi ích thông tin, và một chuyên gia có thể thiếu tính linh hoạt để phù hợp với các phép chiếu slot rất khác nhau. Chúng tôi cho thấy điều này trong Phụ lục I. Do đó, Soft MoE có thể tận dụng một số lượng lớn chuyên gia và—trong khi chi phí của nó vẫn tương tự với backbone dày đặc—yêu cầu bộ nhớ của mô hình có thể tăng lớn.
=======
# 2308.00951.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.00951.pdf
# Kích thước file: 6980585 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TỪ HỖN HỢP CHUYÊN GIA THƯA THỚT ĐẾN MỀM MẠI
Joan Puigcerver∗
Google DeepMindCarlos Riquelme∗
Google DeepMindBasil Mustafa
Google DeepMindNeil Houlsby
Google DeepMind
TÓM TẮT
Kiến trúc hỗn hợp chuyên gia thưa thớt (MoE) mở rộng quy mô dung lượng mô hình mà không làm tăng đáng kể chi phí huấn luyện hoặc suy luận. Mặc dù thành công, MoE vẫn gặp phải một số vấn đề: bất ổn định trong huấn luyện, loại bỏ token, không thể mở rộng số lượng chuyên gia, hoặc tinh chỉnh không hiệu quả. Trong công trình này, chúng tôi đề xuất Soft MoE, một Transformer thưa thớt hoàn toàn khả vi giải quyết những thách thức này, đồng thời duy trì lợi ích của MoE. Soft MoE thực hiện một phép gán mềm ngầm định bằng cách truyền các tổ hợp trọng số khác nhau của tất cả token đầu vào đến mỗi chuyên gia. Như trong các MoE khác, các chuyên gia trong Soft MoE chỉ xử lý một tập con của các token (kết hợp), cho phép dung lượng mô hình lớn hơn (và hiệu suất) với chi phí suy luận thấp hơn. Trong bối cảnh nhận dạng thị giác, Soft MoE vượt trội hơn đáng kể so với Transformer dày đặc (ViT) và MoE phổ biến (Tokens Choice và Experts Choice). Hơn nữa, Soft MoE mở rộng tốt: Soft MoE Huge/14 với 128 chuyên gia trong 16 lớp MoE có hơn 40× nhiều tham số hơn ViT Huge/14, chỉ tăng 2% thời gian suy luận, và chất lượng tốt hơn đáng kể.

1 GIỚI THIỆU
Transformer lớn hơn cải thiện hiệu suất với chi phí tính toán gia tăng. Các nghiên cứu gần đây cho thấy rằng kích thước mô hình và dữ liệu huấn luyện phải được mở rộng cùng nhau để sử dụng tối ưu bất kỳ ngân sách tính toán huấn luyện nào (Kaplan et al., 2020; Hoffmann et al., 2022; Zhai et al., 2022a). Một giải pháp thay thế đầy hứa hẹn cho phép mở rộng mô hình về kích thước mà không phải trả toàn bộ chi phí tính toán của chúng là hỗn hợp chuyên gia thưa thớt (MoE). Gần đây, một số cách tiếp cận thành công đã đề xuất các cách kích hoạt thưa thớt đường dẫn token qua mạng trong ngôn ngữ (Lepikhin et al., 2020; Fedus et al., 2022), thị giác (Riquelme et al., 2021), và mô hình đa phương thức (Mustafa et al., 2022).

Transformer MoE thưa thớt liên quan đến một bài toán tối ưu rời rạc để quyết định những mô-đun nào nên được áp dụng cho mỗi token. Những mô-đun này thường được gọi là chuyên gia và thường là MLP. Nhiều kỹ thuật đã được phát triển để tìm ra sự phù hợp tốt giữa token và chuyên gia: chương trình tuyến tính (Lewis et al., 2021), học tăng cường (Bengio et al., 2015), quy tắc cố định xác định (Roller et al., 2021), vận chuyển tối ưu (Liu et al., 2022), top-k chuyên gia tham lam mỗi token (Shazeer et al., 2017), hoặc top-k token tham lam mỗi chuyên gia (Zhou et al., 2022). Thường thì cần có những hàm mất mát phụ trợ thực nghiệm để cân bằng việc sử dụng chuyên gia và giảm thiểu token không được gán. Những thách thức này có thể lớn hơn trong các cài đặt ngoài phân phối: kích thước batch suy luận nhỏ, đầu vào mới, hoặc trong học chuyển giao.

Chúng tôi giới thiệu Soft MoE, vượt qua nhiều thách thức này. Thay vì sử dụng một router thưa thớt và rời rạc cố gắng tìm một phép gán cứng tốt giữa token và chuyên gia, Soft MoE thay vào đó thực hiện một phép gán mềm bằng cách trộn token. Cụ thể, chúng tôi tính toán một số trung bình trọng số của tất cả token—với trọng số phụ thuộc vào cả token và chuyên gia—và sau đó chúng tôi xử lý mỗi trung bình trọng số bằng chuyên gia tương ứng.

Soft MoE L/16 vượt trội hơn ViT H/14 trên upstream, few-shot và finetuning trong khi yêu cầu gần một nửa thời gian huấn luyện, và nhanh hơn 2× trong suy luận. Hơn nữa, Soft MoE B/16 phù hợp với ViT H/14 trên few-shot và finetuning và vượt trội hơn trên các chỉ số upstream sau một lượng huấn luyện tương đương. Đáng chú ý, Soft MoE B/16 nhanh hơn 5.7× trong suy luận mặc dù có 5.5× số lượng tham số của ViT H/14 (xem Bảng 1 và Hình 5 để biết chi tiết). Phần 4 chứng minh tiềm năng của Soft MoE mở rộng sang các nhiệm vụ khác: chúng tôi huấn luyện một mô hình contrastive text tower với frozen vision tower, cho thấy biểu diễn học được thông qua soft routing giữ nguyên lợi ích cho việc căn chỉnh hình ảnh-văn bản.

∗Đóng góp bằng nhau. Thứ tự được quyết định bằng tung đồng xu.
1arXiv:2308.00951v2 [cs.LG] 27 May 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Sparse MoE
...
...
...
Expert 1
Expert 2
Expert n
Assign
Slots
Soft MoE
...
...
...
Expert 1
Expert 2
Expert n
Weighted average
0.01
0.01
0.01
0.01
0.02
0.09
0.09
0.30
0.40
0.01
0.03
0.01
0.01
Slots

Hình 1: Lớp Sparse và Soft MoE. Trong khi router trong lớp Sparse MoE (trái) học để gán từng token đầu vào cho mỗi slot có sẵn, trong lớp Soft MoE (phải) mỗi slot là kết quả của một trung bình trọng số (khác nhau) của tất cả token đầu vào. Học để tạo ra các phép gán rời rạc đưa ra một số vấn đề tối ưu hóa và triển khai mà Soft MoE tránh được. Phụ lục G trực quan hóa các phân phối đã học của soft-assignments bởi Soft MoE.

tower, cho thấy biểu diễn học được thông qua soft routing giữ nguyên lợi ích cho việc căn chỉnh hình ảnh-văn bản.

2 HỖN HỢP CHUYÊN GIA MỀM
2.1 MÔ TẢ THUẬT TOÁN
Thuật toán định tuyến Soft MoE được mô tả trong Hình 2. Chúng tôi ký hiệu các token đầu vào cho một chuỗi bằng X∈Rm×d, trong đó m là số lượng token và d là chiều của chúng. Mỗi lớp MoE sử dụng một tập hợp n hàm chuyên gia¹ được áp dụng trên từng token riêng lẻ, cụ thể là {fi:Rd→Rd}1:n.

Mỗi chuyên gia xử lý p slot, và mỗi slot có một vector tham số d-chiều tương ứng, Φ∈Rd×(n·p).

Cụ thể, các slot đầu vào ˜X∈R(n·p)×d là kết quả của các tổ hợp lồi của tất cả m token đầu vào, X:

Dij=exp((XΦ)ij)/∑m_{i'=1}exp((XΦ)i'j), ˜X=D⊤X. (1)

Chú ý rằng D, mà chúng tôi gọi là trọng số dispatch, chỉ đơn giản là kết quả của việc áp dụng softmax trên các cột của XΦ. Sau đó, như đã đề cập ở trên, hàm chuyên gia tương ứng được áp dụng trên mỗi slot (tức là trên các hàng của ˜X) để thu được các slot đầu ra: ˜Yi=f⌊i/p⌋(˜Xi).

Cuối cùng, các token đầu ra Y được tính toán như một tổ hợp lồi của tất cả (n·p) slot đầu ra, ˜Y, có trọng số được tính toán tương tự như trước:

Cij=exp((XΦ)ij)/∑_{j'=1}^{n·p}exp((XΦ)ij'), Y=C˜Y. (2)

Chúng tôi gọi C là trọng số combine, và nó là kết quả của việc áp dụng softmax trên các hàng của XΦ.

Theo thiết kế thông thường cho Sparse MoE, chúng tôi thay thế một tập con của các khối MLP của Transformer bằng các khối Soft MoE. Chúng tôi thường thay thế nửa cuối của các khối MLP. Tổng số slot là một siêu tham số quan trọng của các lớp Soft MoE vì độ phức tạp thời gian phụ thuộc vào số lượng slot hơn là số lượng chuyên gia. Ta có thể đặt số lượng slot bằng độ dài chuỗi đầu vào để khớp với FLOP của Transformer dày đặc tương đương.

2.2 TÍNH CHẤT CỦA SOFT MOE VÀ KẾT NỐI VỚI SPARSE MOE
Thuật toán Sparse MoE hoàn toàn khả vi liên quan đến một bài toán gán giữa token và chuyên gia, chịu ràng buộc dung lượng và cân bằng tải. Các thuật toán khác nhau xấp xỉ giải pháp theo các cách khác nhau: ví dụ, router top-k hoặc "Token Choice" (Shazeer et al., 2017;

¹Trong thực tế, tất cả chuyên gia áp dụng cùng một hàm với các tham số khác nhau, thường là một MLP.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Soft MoE
Weighting
Logits
Expert 1
Expert 2
Expert E
Slot 1
Slot 2
Slot 3
Slot 4
Slot S
Slot 1
Slot 2
Slot 3
Slot 4
Slot S
slots S
input tokens N
Token 1
Token 2
Token 3
Token N
Dispatch
Weights
softmax per slot
(normalized per column)
Combine
Weights
softmax per token
(normalized per row)
Token 1
Token 2
Token 3
Token N
Per-slot
Learnable
Parameters
...
...
...
...
...
slot linear
combination
output linear
combination
Soft MoE Layer

Hình 2: Chi tiết định tuyến Soft MoE. Soft MoE tính toán điểm số hoặc logit cho mọi cặp token đầu vào và slot. Từ đó nó tính toán một ma trận logit slots × tokens, được chuẩn hóa thích hợp để tính toán cả trọng số dispatch và combine. Các slot được phân bổ cho chuyên gia theo round-robin.

Lepikhin et al., 2020; Riquelme et al., 2021) chọn top-k chuyên gia có điểm cao nhất cho mỗi token, trong khi có slot khả dụng trong chuyên gia đó (tức là chuyên gia chưa lấp đầy dung lượng). Router "Expert Choice" (Zhou et al., 2022) chọn top-capacity token có điểm cao nhất cho mỗi chuyên gia. Các công trình khác đề xuất các thuật toán tiên tiến hơn (và thường tốn kém) để tính toán các phép gán, như các cách tiếp cận dựa trên thuật toán Linear Programming (Lewis et al., 2021), Optimal Transport (Liu et al., 2022; Clark et al., 2022) hoặc Reinforcement Learning (Clark et al., 2022). Tuy nhiên, hầu như tất cả các cách tiếp cận này đều có bản chất rời rạc, và do đó không khả vi. Ngược lại, tất cả các phép toán trong lớp Soft MoE đều liên tục và hoàn toàn khả vi. Chúng ta có thể hiểu các trung bình trọng số với điểm softmax như là các phép gán mềm, thay vì các phép gán cứng được sử dụng trong Sparse MoE.

1def soft_moe_layer(X, Phi, experts):
2# Tính toán trọng số dispatch và combine.
3logits = jnp.einsum('md,dnp->mnp', X, Phi)
4D = jax.nn.softmax(logits, axis=(0,))
5C = jax.nn.softmax(logits, axis=(1, 2))
6# Các slot đầu vào là trung bình trọng số của tất cả token đầu vào,
7# được cho bởi trọng số dispatch.
8Xs = jnp.einsum('md,mnp->npd', X, D)
9# Áp dụng hàm chuyên gia tương ứng cho mỗi slot đầu vào.
10 Ys = jnp.stack([
11 f_i(Xs[i, :, :]) for i, f_i in enumerate(experts)],
12 axis=0)
13 # Các token đầu ra là trung bình trọng số của tất cả slot đầu ra,
14 # được cho bởi trọng số combine.
15 Y = jnp.einsum('npd,mnp->md', Ys, C)
16 return Y

Thuật toán 1: Triển khai JAX đơn giản (Bradbury et al., 2018) của lớp Soft MoE. Mã đầy đủ có sẵn tại https://github.com/google-research/vmoe .

Không loại bỏ token và mất cân bằng chuyên gia Các cơ chế định tuyến cổ điển có xu hướng gặp phải những vấn đề như "token dropping" (tức là một số token không được gán cho chuyên gia nào), hoặc "expert unbalance" (tức là một số chuyên gia nhận được nhiều token hơn các chuyên gia khác). Thật không may, hiệu suất có thể bị ảnh hưởng nghiêm trọng do hậu quả. Ví dụ, router top-k hoặc "Token Choice" phổ biến (Shazeer et al., 2017) gặp phải cả hai vấn đề, trong khi router "Expert Choice" (Zhou et al., 2022) chỉ gặp phải vấn đề đầu tiên (xem Phụ lục B để biết một số thí nghiệm về dropping). Soft MoE miễn nhiễm với token dropping và expert unbalance vì mọi slot đều được lấp đầy bằng trung bình trọng số của tất cả token.

Nhanh Tổng số slot xác định chi phí của lớp Soft MoE. Mỗi đầu vào áp dụng số lượng MLP như vậy. Tổng số chuyên gia không liên quan trong tính toán này: ít chuyên gia với nhiều slot mỗi chuyên gia hoặc nhiều chuyên gia với ít slot mỗi chuyên gia sẽ có chi phí khớp nhau nếu tổng số slot giống nhau. Ràng buộc duy nhất chúng ta phải đáp ứng là số lượng slot phải lớn hơn hoặc bằng số lượng chuyên gia (vì mỗi chuyên gia phải xử lý ít nhất một slot). Ưu điểm chính của Soft MoE là hoàn toàn tránh được các phép toán sort hoặc top-k chậm và thường không phù hợp với bộ tăng tốc phần cứng. Kết quả là, Soft MoE nhanh hơn đáng kể so với hầu hết Sparse MoE (Hình 6). Xem Phần 2.3 để biết chi tiết về độ phức tạp thời gian.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Đặc điểm của cả sparse và dense Độ thưa thớt trong Sparse MoE đến từ việc tham số chuyên gia chỉ được áp dụng cho một tập con của token đầu vào. Tuy nhiên, Soft MoE về mặt kỹ thuật không thưa thớt, vì mỗi slot là trung bình trọng số của tất cả token đầu vào. Mỗi token đầu vào kích hoạt phân số tất cả tham số mô hình. Tương tự, tất cả token đầu ra phụ thuộc phân số vào tất cả slot (và chuyên gia). Cuối cùng, cũng chú ý rằng Soft MoE không phải là Dense MoE, nơi mỗi chuyên gia xử lý tất cả token đầu vào, vì mỗi chuyên gia chỉ xử lý một tập con của các slot.

Tính xác định theo chuỗi Dưới ràng buộc dung lượng, tất cả các cách tiếp cận Sparse MoE định tuyến token theo nhóm có kích thước cố định và thực thi (hoặc khuyến khích) sự cân bằng trong nhóm. Khi nhóm chứa token từ các chuỗi hoặc đầu vào khác nhau, những token này cạnh tranh cho các chỗ trống có sẵn trong buffer chuyên gia. Do đó, mô hình không còn xác định ở mức chuỗi, mà chỉ ở mức batch. Mô hình sử dụng nhóm lớn hơn có xu hướng cung cấp nhiều tự do hơn cho thuật toán định tuyến và thường hoạt động tốt hơn, nhưng chi phí tính toán của chúng cũng cao hơn.

2.3 TRIỂN KHAI
Độ phức tạp thời gian Giả sử chi phí mỗi token của một hàm chuyên gia là O(k). Độ phức tạp thời gian của lớp Soft MoE khi đó là O(mnpd + npk). Bằng cách chọn p=O(m/n) slot mỗi chuyên gia, tức là số token trên số chuyên gia, chi phí giảm xuống O(m²d + mk). Cho rằng mỗi hàm chuyên gia có tập tham số riêng, việc tăng số lượng chuyên gia n và điều chỉnh p tương ứng, cho phép chúng ta tăng tổng số tham số mà không có bất kỳ tác động nào đến độ phức tạp thời gian. Hơn nữa, khi chi phí áp dụng một chuyên gia lớn, thuật ngữ mk chiếm ưu thế hơn m²d, và chi phí tổng thể của lớp Soft MoE trở nên tương đương với việc áp dụng một chuyên gia duy nhất trên tất cả token đầu vào. Cuối cùng, ngay cả khi m²d không bị chi phối, điều này cũng giống như chi phí self-attention (đơn đầu), do đó nó không trở thành nút thắt cổ chai trong mô hình Transformer. Điều này có thể được thấy trong biểu đồ dưới của Hình 6 nơi throughput của Soft MoE hầu như không thay đổi khi số lượng chuyên gia tăng từ 8 lên 4096 chuyên gia, trong khi Sparse MoE có giảm đáng kể.

Chuẩn hóa Trong Transformer, lớp MoE thường được sử dụng để thay thế lớp feedforward trong mỗi khối encoder. Do đó, khi sử dụng pre-normalization như hầu hết kiến trúc Transformer hiện đại (Domhan, 2018; Xiong et al., 2020; Riquelme et al., 2021; Fedus et al., 2022), đầu vào cho lớp MoE được "layer normalized". Điều này gây ra vấn đề ổn định khi mở rộng chiều mô hình d, vì softmax tiến gần đến vector một-nóng khi d→∞ (xem Phụ lục E). Do đó, trong Dòng 3 của thuật toán 1, chúng tôi thay thế X và Phi bằng l2_normalize(X, axis=1) và scale *l2_normalize(Phi, axis=0), tương ứng; trong đó scale là một vô hướng có thể huấn luyện, và l2_normalize chuẩn hóa trục tương ứng để có chuẩn (L2) đơn vị, như Thuật toán 2 cho thấy.

1def l2_normalize(x, axis, eps=1e-6):
2norm = jnp.sqrt(jnp.square(x).sum(axis=axis, keepdims=True))
3return x *jnp.reciprocal(norm + eps)

Thuật toán 2: Triển khai JAX của chuẩn hóa L2 được sử dụng trong lớp Soft MoE.

Đối với giá trị d tương đối nhỏ, việc chuẩn hóa có ít tác động đến chất lượng mô hình. Tuy nhiên, với việc chuẩn hóa được đề xuất trong lớp Soft MoE, chúng ta có thể làm cho chiều mô hình lớn hơn và/hoặc tăng tỷ lệ học (xem Phụ lục E).

Mô hình phân tán Khi số lượng chuyên gia tăng đáng kể, không thể đưa toàn bộ mô hình vào bộ nhớ trên một thiết bị duy nhất, đặc biệt là trong quá trình huấn luyện hoặc khi sử dụng MoE trên các backbone mô hình lớn. Trong những trường hợp này, chúng tôi sử dụng các kỹ thuật tiêu chuẩn để phân tán mô hình qua nhiều thiết bị, như trong (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022) và các công trình khác huấn luyện mô hình MoE lớn. Phân tán mô hình thường thêm một overhead vào chi phí của mô hình, không được ghi nhận bởi phân tích độ phức tạp thời gian dựa trên FLOP mà chúng tôi đưa ra ở trên. Để tính đến sự khác biệt này, trong tất cả các thí nghiệm, chúng tôi đo không chỉ FLOP mà còn thời gian wall-clock tính bằng TPUv3-chip-hours.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
3 THÍ NGHIỆM PHÂN LOẠI HÌNH ẢNH
Pareto frontiers huấn luyện. Trong Phần 3.3, chúng tôi so sánh các mô hình ViT dày đặc ở kích thước Small, Base, Large và Huge với các đối tác dày đặc và thưa thớt của chúng dựa trên cả định tuyến thưa thớt Tokens Choice và Experts Choice. Chúng tôi nghiên cứu hiệu suất ở các ngân sách huấn luyện khác nhau và cho thấy rằng Soft MoE chiếm ưu thế so với các mô hình khác về hiệu suất ở một chi phí hoặc thời gian huấn luyện nhất định.

Mô hình tối ưu hóa thời gian suy luận. Trong Phần 3.4, chúng tôi trình bày các lần chạy huấn luyện dài hơn ("overtraining"). So với ViT, Soft MoE mang lại cải thiện lớn về tốc độ suy luận cho một mức hiệu suất cố định (mô hình nhỏ hơn: S, B) và hiệu suất tuyệt đối (mô hình lớn hơn: L, H).

Ablation mô hình. Trong Phần 3.5 và 3.6, chúng tôi điều tra tác động của việc thay đổi số lượng slot và chuyên gia, và thực hiện ablation trên thuật toán định tuyến Soft MoE.

3.1 DỮ LIỆU HUẤN LUYỆN VÀ ĐÁNH GIÁ
Chúng tôi pretrain mô hình trên JFT-4B (Zhai et al., 2022a), một tập dữ liệu độc quyền chứa hơn 4B hình ảnh, bao gồm 29k lớp. Trong quá trình pretraining, chúng tôi đánh giá mô hình trên hai chỉ số: upstream validation precision-at-1 trên JFT-4B, và ImageNet 10-shot accuracy. Chỉ số sau được tính toán bằng cách đóng băng trọng số mô hình và thay thế head bằng một cái mới chỉ được huấn luyện trên tập dữ liệu chứa 10 hình ảnh mỗi lớp từ ImageNet-1k (Deng et al., 2009). Cuối cùng, chúng tôi cung cấp độ chính xác trên tập validation của ImageNet-1k sau khi finetuning trên tập training của ImageNet-1k (1.3 triệu hình ảnh) ở độ phân giải 384.

3.2 THUẬT TOÁN ĐỊNH TUYẾN THƯA THỚT
Tokens Choice. Mỗi token chọn top-K chuyên gia có điểm định tuyến cao nhất cho token (Shazeer et al., 2017). Tăng K thường dẫn đến hiệu suất tốt hơn với chi phí tính toán gia tăng. Batch Priority Routing (BPR) (Riquelme et al., 2021) cải thiện đáng kể hiệu suất mô hình, đặc biệt trong trường hợp K=1 (Phụ lục F, Bảng 7). Theo đó, chúng tôi sử dụng định tuyến Top-K với BPR và K∈{1,2}. Chúng tôi cũng tối ưu hóa số lượng chuyên gia (Phụ lục F, Hình 11).

Experts Choice. Thay vào đó, chuyên gia có thể chọn top-C token về điểm định tuyến (Zhou et al., 2022). C là kích thước buffer, và chúng tôi đặt E·C=c·T trong đó E là số lượng chuyên gia, T là tổng số token trong nhóm, và c là hệ số dung lượng. Khi c=1, tất cả token có thể được xử lý thông qua hợp của các chuyên gia. Với định tuyến Experts Choice, thường thấy một số token được nhiều chuyên gia chọn đồng thời trong khi một số token khác không được chọn. Hình 10, Phụ lục B minh họa hiện tượng này. Chúng tôi thí nghiệm với c=0.5,1,2.

3.3 HUẤN LUYỆN CÁC MÔ HÌNH PARETO-OPTIMAL
Chúng tôi huấn luyện các mô hình ViT-{S/8, S/16, S/32, B/16, B/32, L/16, L/32, H/14} và các đối tác thưa thớt của chúng. Chúng tôi huấn luyện một số biến thể (thay đổi K, C và số lượng chuyên gia), tổng cộng 106 mô hình. Chúng tôi huấn luyện trong 300k bước với batch size 4096, độ phân giải 224, sử dụng lịch tỷ lệ học căn bậc hai nghịch đảo.

Hình 3a và 3b cho thấy kết quả cho các mô hình trong mỗi lớp nằm trên Pareto frontiers chi phí/hiệu suất huấn luyện tương ứng. Trên cả hai chỉ số, Soft MoE vượt trội mạnh mẽ so với các cách tiếp cận dày đặc và thưa thớt khác với bất kỳ ngân sách FLOP hoặc thời gian nhất định. Bảng 9, Phụ lục J, liệt kê tất cả mô hình, với tham số, hiệu suất và chi phí của chúng, tất cả đều được hiển thị trong Hình 19.

3.4 THỜI GIAN HUẤN LUYỆN DÀI
Chúng tôi huấn luyện một số mô hình trong thời gian dài hơn nhiều, lên đến 4M bước. Chúng tôi huấn luyện một số Soft MoE trên JFT, theo cài đặt tương tự như Zhai et al. (2022a). Chúng tôi thay thế nửa cuối của các khối trong ViT S/16, B/16, L/16, và H/14 bằng lớp Soft MoE với 128 chuyên gia, sử dụng một slot mỗi chuyên gia. Chúng tôi huấn luyện mô hình từ 1B đến 54B tham số. Tất cả mô hình được huấn luyện trong 4M bước, ngoại trừ H/14, được huấn luyện trong 2M bước vì lý do chi phí.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10¹10²
Total Training TPUv3-days0.440.460.480.500.520.540.560.58JFT-4B Precision-at-1
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice
Tokens Choice
Dense
(a) JFT-4B Precision-at-1
10¹10²
Total Training TPUv3-days0.500.550.600.650.700.750.80ImageNet 10-shot Accuracy
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice
Tokens Choice
Dense (b) ImageNet 10-shot Accuracy

Hình 3: Pareto frontiers huấn luyện. Soft MoE chiếm ưu thế so với cả ViT (dày đặc) và MoE phổ biến (Experts và Tokens Choice) trên Pareto frontier chi phí huấn luyện / hiệu suất. Kích thước marker lớn hơn chỉ ra mô hình lớn hơn, từ S/32 đến H/14. Chi phí được báo cáo theo FLOP và thời gian huấn luyện TPU-v3. Chỉ có mô hình trên Pareto frontier của chúng được hiển thị, Phụ lục F cho thấy tất cả mô hình được huấn luyện.

10³
Total Training ExaFLOPs67%73%77%81%84%ImageNet 10-shot AccuracyS/16B/16L/16H/14
10³
Total Training ExaFLOPs51%54%57%59%JFT-4B Precision-at-1S/16B/16L/16H/14
10³
Total Training ExaFLOPs84%85%87%88%89%ImageNet Finetune AccuracyS/16B/16L/16H/14
Soft MoE
Dense

Hình 4: Mô hình với thời gian huấn luyện dài. Mô hình được huấn luyện trong 4M bước (H/14 chỉ huấn luyện 2M bước). Các lớp mô hình tương đương (S/16, B/16, v.v.) có chi phí huấn luyện tương tự, nhưng Soft MoE vượt trội hơn ViT trên tất cả chỉ số với ngân sách huấn luyện cố định.

Hình 4 cho thấy độ chính xác JFT-4B, độ chính xác ImageNet 10-shot, và độ chính xác finetuning ImageNet cho Soft MoE và ViT so với chi phí huấn luyện. Phụ lục F, Bảng 8 chứa kết quả số, và Hình 16 cho thấy hiệu suất so với core-hours, từ đó có thể rút ra cùng kết luận. Soft MoE vượt trội đáng kể so với mô hình ViT dày đặc với ngân sách tính toán nhất định. Ví dụ, Soft MoE S/16 hoạt động tốt hơn ViT B/16 trên JFT và 10-shot ImageNet, và nó cũng cải thiện điểm finetuning trên dữ liệu ImageNet đầy đủ, mặc dù chi phí huấn luyện (và suy luận) của nó nhỏ hơn đáng kể. Tương tự, Soft MoE B/16 vượt trội hơn ViT L/16 upstream, và chỉ chậm 0.5 sau finetuning trong khi nhanh hơn 3x và yêu cầu ít hơn gần 4x FLOP. Cuối cùng, mô hình Soft MoE L/16 vượt trội hơn mô hình dày đặc H/14 trong khi lại nhanh hơn khoảng 3x về thời gian bước huấn luyện và suy luận.

Chúng tôi tiếp tục huấn luyện các backbone nhỏ lên đến 9M bước để có được các mô hình chất lượng cao với chi phí suy luận thấp. Ngay cả sau khi huấn luyện thêm (quá mức), tổng thời gian huấn luyện so với mô hình ViT lớn hơn là tương tự hoặc nhỏ hơn. Đối với các lần chạy này, cooldown dài hơn (giảm tỷ lệ học tuyến tính) hoạt động tốt cho Soft MoE. Do đó, chúng tôi tăng cooldown từ 50k bước lên 500k bước.

Hình 5 và Bảng 1 trình bày kết quả. Soft MoE B/16 được huấn luyện trong 1k TPUv3 ngày khớp hoặc vượt trội hơn ViT H/14 được huấn luyện với ngân sách tương tự, và rẻ hơn 10× trong suy luận về FLOP (32 so với 334 GFLOP/img) và >5× rẻ hơn về thời gian wall-clock (1.5 so với 8.6 ms/img). Soft MoE B/16 khớp hiệu suất của mô hình ViT H/14 khi chúng tôi tăng gấp đôi ngân sách huấn luyện của ViT-H/14 (lên 2k TPU-ngày). Soft MoE L/16 vượt trội hơn tất cả mô hình ViT trong khi gần như nhanh hơn 2× trong suy luận so với ViT H/14 (4.8 so với 8.6 ms/img).

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10¹10²
Evaluation cost (GFLOP/img)70%75%80%85%ImageNet 10-shot Accuracy
S/16B/16L/16 H/14 S/14B/1610.4×
10¹10²
Evaluation cost (GFLOP/img)52%55%57%60%62%65%JFT-4B Precision-at-1
S/16B/16L/16
H/1410.4×
10³
Training cost (exaFLOP)70%75%80%85%ImageNet 10-shot Accuracy
S/16B/16L/16H/14
10³
Training cost (exaFLOP)52%55%57%60%62%65%JFT-4B Precision-at-1
S/16B/16L/16
H/14S/14B/16L/16ViT Soft MoE Soft MoE (long)
10⁰ 10¹
Evaluation time (TPUv3 ms/img)70%75%80%85%ImageNet 10-shot Accuracy
S/16B/16L/16 H/14 S/14B/165.7×
10⁰ 10¹
Evaluation time (TPUv3 ms/img)52%55%57%60%62%65%JFT-4B Precision-at-1
S/16B/16L/16
H/145.7×
10³
Training time (TPUv3 days)70%75%80%85%ImageNet 10-shot Accuracy
S/16B/16L/16H/14
10³
Training time (TPUv3 days)52%55%57%60%62%65%JFT-4B Precision-at-1
S/16B/16L/16
H/14S/14B/16L/16

Hình 5: Mô hình được tối ưu hóa cho tốc độ suy luận. Hiệu suất của mô hình được huấn luyện trong nhiều bước hơn, do đó được tối ưu hóa cho hiệu suất với chi phí suy luận nhất định (thời gian TPUv3 hoặc FLOP).

Bảng 1: Mô hình được huấn luyện trong thời gian dài hơn (bước cooldown trong ngoặc đơn).
Model Params Train Train Train Eval Eval JFT @1 INet INet
steps (cd) TPU-days exaFLOP ms/img GFLOP/img P@1 10shot finetune
ViT S/16 33M 4M (50k) 153.5 227.1 0.5 9.2 51.3 67.6 84.0
ViT B/16 108M 4M (50k) 410.1 864.1 1.3 35.1 56.2 76.8 86.6
ViT L/16 333M 4M (50k) 1290.1 3025.4 4.9 122.9 59.8 81.5 88.5
ViT H/14 669M 1M (50k) 1019.9 2060.2 8.6 334.2 58.8 82.7 88.6
ViT H/14 669M 2M (50k) 2039.8 4120.3 8.6 334.2 59.7 83.3 88.9
Soft MoE S/14 256E 1.8B 10M (50k) 494.7 814.2 0.9 13.2 60.1 80.6 87.5
Soft MoE B/16 128E 3.7B 9M (500k) 1011.4 1769.5 1.5 32.0 62.4 82.9 88.5
Soft MoE L/16 128E 13.1B 4M (500k) 1355.4 2734.1 4.8 111.1 63.0 84.3 89.2

3.5 SỐ LƯỢNG SLOT VÀ CHUYÊN GIA
Chúng tôi nghiên cứu tác động của việc thay đổi số lượng slot và chuyên gia trong Sparse và Soft MoE. Hình 6 cho thấy chất lượng và tốc độ của MoE với số lượng chuyên gia khác nhau, và số lượng slot mỗi token; cái sau tương đương với số lượng chuyên gia trung bình được gán mỗi token cho Sparse MoE. Khi thay đổi số lượng chuyên gia, FLOP backbone của mô hình vẫn không đổi, vì vậy thay đổi tốc độ là do chi phí định tuyến. Khi thay đổi slot-per-expert, số lượng token được xử lý trong lớp chuyên gia tăng lên, vì vậy throughput giảm. Đầu tiên, quan sát rằng đối với Soft MoE, mô hình hoạt động tốt nhất ở mỗi số lượng slot-per-token là mô hình có nhiều chuyên gia nhất (tức là một slot mỗi chuyên gia). Đối với hai Sparse MoE, có một điểm mà khó khăn huấn luyện vượt qua lợi ích của dung lượng bổ sung, dẫn đến một số lượng chuyên gia tối ưu khiêm tốn. Thứ hai, throughput của Soft MoE gần như không đổi khi thêm nhiều chuyên gia hơn. Tuy nhiên, throughput của Sparse MoE giảm mạnh từ 1k chuyên gia, xem thảo luận trong Phần 2.2.

3.6 ABLATION
Chúng tôi nghiên cứu tác động của các thành phần của lớp định tuyến Soft MoE bằng cách chạy các ablation sau: Identity routing: Token không được trộn: token đầu tiên đến chuyên gia đầu tiên, token thứ hai đến chuyên gia thứ hai, v.v. Uniform Mixing: Mỗi slot trộn tất cả token đầu vào theo cùng một cách: bằng cách lấy trung bình chúng, cả cho dispatching và combining. Sự đa dạng chuyên gia phát sinh từ việc khởi tạo khác nhau của trọng số của chúng. Soft / Uniform: Chúng tôi học trộn token trên đầu vào cho chuyên gia để tạo slot (trọng số dispatch), nhưng chúng tôi lấy trung bình đầu ra chuyên gia. Điều này có nghĩa là mỗi token đầu vào được cập nhật giống hệt nhau trước kết nối dư. Uniform / Soft: Tất cả slot được lấp đầy bằng trung bình đồng nhất của token đầu vào. Chúng tôi học trộn slot của đầu ra chuyên gia phụ thuộc vào token đầu vào. Bảng 2 cho thấy rằng có slot là quan trọng; Identity và Uniform routing hoạt động kém hơn đáng kể so với Soft MoE, mặc dù chúng vượt trội hơn ViT. Dispatch mixing dường như quan trọng hơn một chút so với combine mixing. Xem Phụ lục A để biết chi tiết bổ sung.

4 HỌC CONTRASTIVE
Chúng tôi kiểm tra xem biểu diễn của Soft MoE có tốt hơn cho các nhiệm vụ khác hay không. Để làm điều này, chúng tôi thử học contrastive hình ảnh-văn bản. Theo Zhai et al. (2022b), image tower được pretrain trên phân loại hình ảnh, và sau đó được đóng băng trong khi huấn luyện text encoder trên tập dữ liệu các cặp hình ảnh-văn bản. Chúng tôi tái sử dụng các mô hình được huấn luyện trên JFT trong phần trước và so sánh hiệu suất zero-shot của chúng trên các tập dữ liệu downstream. Để học contrastive, chúng tôi huấn luyện trên WebLI (Chen et al., 2022), một tập dữ liệu độc quyền gồm 10B hình ảnh và alt-text. Image encoder được đóng băng, trong khi text encoder được huấn luyện từ đầu.

Bảng 3 cho thấy kết quả. Nhìn chung, những lợi ích chúng tôi quan sát được trên phân loại hình ảnh cũng có trong cài đặt này. Ví dụ, Soft MoE-L/16 vượt trội hơn ViT-L/16 hơn 1% và 2% trên ImageNet và Cifar-100 zero-shot, tương ứng. Tuy nhiên, cải thiện trên COCO retrieval khiêm tốn, và có thể phản ánh sự căn chỉnh kém giữa đặc trưng học được trên JFT từ vựng đóng và nhiệm vụ từ vựng mở này.

Cuối cùng, trong Phụ lục F.1, chúng tôi cho thấy rằng Soft MoE cũng vượt qua vanilla ViT và router Experts Choice khi được huấn luyện từ đầu trên LAION-400M có sẵn công khai (Schuhmann et al., 2021). Với pretraining này, Soft MoE cũng hưởng lợi từ data augmentation, nhưng cả ViT và Experts Choice đều dường như không hưởng lợi từ nó, điều này nhất quán với quan sát của chúng tôi trong Phần 3.5, rằng Soft MoE sử dụng tốt hơn các tham số chuyên gia bổ sung.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Number of Experts1 2 4 8 16Number of Slots per Token66.6 68.7 70.6 71.6 72.0 72.3
66.8 68.6 70.6 71.6 72.5 72.8 73.8
67.4 69.3 70.6 72.0 72.9 73.6 74.0 74.4
67.1 69.2 71.2 72.2 73.1 73.7 74.4 74.9 74.6
67.4 69.6 70.9 72.1 73.0 74.2 74.8 75.1 75.3 75.4Soft MoE
Number of Experts65.4 66.6 67.1 68.2 68.2 68.4
66.4 67.6 68.8 69.3 69.7 69.3 69.0
67.1 68.8 69.8 70.4 70.6 70.2 70.1 69.7
67.5 69.1 70.4 71.4 71.8 71.3 71.4 70.9 70.0
69.4 71.1 71.8 72.5 72.1 72.2 71.7 71.1 71.0Experts Choice
Number of Experts64.7 66.2 66.8 64.3 55.0
65.5 66.3 67.3 66.5 59.4 59.2 58.3
65.4 66.5 67.5 67.7 67.8 57.7 54.9
65.4 66.3 68.2 68.5 68.8 64.9 58.4 58.4 53.5
65.8 66.6 67.9 69.1 69.1 68.3 62.1 58.0 53.4 32.1Tokens Choice8
16
32
64
128
256
512
1024
2048
4096
Number of Experts1 2 4 8 16Number of Slots per Token379 378 375 371 367 365
342 341 338 335 335 322 331
285 285 282 278 282 267 268 274
214 214 212 208 212 201 202 199 202
142 142 141 140 141 131 137 134 129 132Soft MoE
8
16
32
64
128
256
512
1024
2048
4096
Number of Experts374 373 369 361 360 344
334 333 329 321 322 308 288
275 274 271 268 267 255 242 222
203 202 201 199 199 189 183 172 150
133 132 131 131 125 122 119 107 91Experts Choice
8
16
32
64
128
256
512
1024
2048
4096
Number of Experts366 359 347 324 355
326 321 310 290 316 295 261
270 266 258 245 261 243 216
200 198 193 185 193 179 161 134 97
131 132 131 129 128 117 107 92 69 45Tokens Choice
40506070
ImageNet 10shot Accuracy
100200300
Training Throughput (img/s)

Hình 6: Trên: Hiệu suất (ImageNet) cho MoE với số lượng chuyên gia khác nhau (cột) và slot-per-token / assignment-per-token (hàng). Dưới: Throughput huấn luyện của cùng các mô hình. Qua các cột, số lượng tham số tăng lên, tuy nhiên, chi phí lý thuyết (FLOP) cho mô hình (không bao gồm chi phí định tuyến) vẫn không đổi. Đi xuống các hàng, lớp chuyên gia trở nên tính toán chuyên sâu hơn khi nhiều token/slot được xử lý trong lớp MoE.

Bảng 2: Ablation sử dụng Soft MoE-S/14 với 256 chuyên gia được huấn luyện trong 300k bước.
Method Experts Mixing Learned Dispatch Learned Combine JFT p@1 IN/10shot
Soft MoE ✓ ✓ ✓ ✓ 54.3% 74.8%
Soft / Uniform ✓ ✓ ✓ 53.6% 72.0%
Uniform / Soft ✓ ✓ ✓ 52.6% 71.8%
Uniform ✓ ✓ 51.8% 70.0%
Identity ✓ 51.5% 69.1%
ViT 48.3% 62.3%

identically updated trước kết nối dư. Uniform / Soft: Tất cả slot được lấp đầy bằng trung bình đồng nhất của token đầu vào. Chúng tôi học trộn slot của đầu ra chuyên gia phụ thuộc vào token đầu vào. Bảng 2 cho thấy rằng có slot là quan trọng; Identity và Uniform routing hoạt động kém hơn đáng kể so với Soft MoE, mặc dù chúng vượt trội hơn ViT. Dispatch mixing dường như quan trọng hơn một chút so với combine mixing. Xem Phụ lục A để biết chi tiết bổ sung.

4 HỌC CONTRASTIVE
Chúng tôi kiểm tra xem biểu diễn của Soft MoE có tốt hơn cho các nhiệm vụ khác hay không. Để làm điều này, chúng tôi thử học contrastive hình ảnh-văn bản. Theo Zhai et al. (2022b), image tower được pretrain trên phân loại hình ảnh, và sau đó được đóng băng trong khi huấn luyện text encoder trên tập dữ liệu các cặp hình ảnh-văn bản. Chúng tôi tái sử dụng các mô hình được huấn luyện trên JFT trong phần trước và so sánh hiệu suất zero-shot của chúng trên các tập dữ liệu downstream. Để học contrastive, chúng tôi huấn luyện trên WebLI (Chen et al., 2022), một tập dữ liệu độc quyền gồm 10B hình ảnh và alt-text. Image encoder được đóng băng, trong khi text encoder được huấn luyện từ đầu.

Bảng 3 cho thấy kết quả. Nhìn chung, những lợi ích chúng tôi quan sát được trên phân loại hình ảnh cũng có trong cài đặt này. Ví dụ, Soft MoE-L/16 vượt trội hơn ViT-L/16 hơn 1% và 2% trên ImageNet và Cifar-100 zero-shot, tương ứng. Tuy nhiên, cải thiện trên COCO retrieval khiêm tốn, và có thể phản ánh sự căn chỉnh kém giữa đặc trưng học được trên JFT từ vựng đóng và nhiệm vụ từ vựng mở này.

Cuối cùng, trong Phụ lục F.1, chúng tôi cho thấy rằng Soft MoE cũng vượt qua vanilla ViT và router Experts Choice khi được huấn luyện từ đầu trên LAION-400M có sẵn công khai (Schuhmann et al., 2021). Với pretraining này, Soft MoE cũng hưởng lợi từ data augmentation, nhưng cả ViT và Experts Choice đều dường như không hưởng lợi từ nó, điều này nhất quán với quan sát của chúng tôi trong Phần 3.5, rằng Soft MoE sử dụng tốt hơn các tham số chuyên gia bổ sung.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 3: Đánh giá kiểu LIT với text tower ViT-g được huấn luyện cho 18B hình ảnh đầu vào (~5 epoch).
Model Experts IN/0shot Cifar100/0shot Pet/0shot Coco Img2Text Coco Text2Img
ViT-S/16 – 74.2% 56.6% 94.8% 53.6% 37.0%
Soft MoE-S/16 128 81.2% 67.2% 96.6% 56.0% 39.0%
Soft MoE-S/14 256 82.0% 75.1% 97.1% 56.5% 39.4%
ViT-B/16 – 79.6% 71.0% 96.4% 58.2% 41.5%
Soft MoE-B/16 128 82.5% 74.4% 97.6% 58.3% 41.6%
ViT-L/16 – 82.7% 77.5% 97.1% 60.7% 43.3%
Soft MoE-L/16 128 83.8% 79.9% 97.3% 60.9% 43.4%
Souped Soft MoE-L/16 128 84.3% 81.3% 97.2% 61.1% 44.5%
ViT-H/14 – 83.8% 84.7% 97.5% 62.7% 45.2%
Soft MoE-H/14 256 84.6% 86.3% 97.4% 61.0% 44.8%

5 CÔNG TRÌNH LIÊN QUAN
Nhiều công trình hiện tại merge, mix hoặc fuse token đầu vào để giảm độ dài chuỗi đầu vào (Jaegle et al., 2021; Ryoo et al., 2021; Renggli et al., 2022; Wang et al., 2022), thường sử dụng trung bình trọng số giống attention với key cố định, để cố gắng giảm bớt chi phí bậc hai của self-attention đối với độ dài chuỗi. Mặc dù trọng số dispatch và combine của chúng tôi được tính toán theo cách tương tự như các cách tiếp cận này, mục tiêu của chúng tôi không phải là giảm độ dài chuỗi (mặc dù có thể), và chúng tôi thực sự khôi phục độ dài chuỗi gốc sau khi cân trọng đầu ra của chuyên gia với trọng số combine, ở cuối mỗi lớp Soft MoE.

Multi-headed attention cũng cho thấy một số điểm tương đồng với Soft MoE, ngoài việc sử dụng softmax trong trung bình trọng số: h head khác nhau có thể được hiểu là các chuyên gia (tuyến tính) khác nhau. Sự khác biệt là, nếu m là độ dài chuỗi và mỗi token đầu vào có chiều d, mỗi trong số h head xử lý m vector có kích thước d/h. M vector kết quả được kết hợp sử dụng trọng số khác nhau cho mỗi trong m' token đầu ra (tức là trọng số attention), trên mỗi head độc lập, và sau đó các vector (d/h)-chiều kết quả từ mỗi head được nối lại thành một vector chiều d. Các chuyên gia của chúng tôi là phi tuyến và kết hợp vector có kích thước d, ở đầu vào và đầu ra của các chuyên gia đó.

Các công trình MoE khác sử dụng tổ hợp trọng số của tham số chuyên gia, thay vì thực hiện định tuyến thưa thớt của các ví dụ (Yang et al., 2019; Tian et al., 2020; Muqeeth et al., 2023). Các cách tiếp cận này cũng hoàn toàn khả vi, nhưng chúng có thể có chi phí cao hơn, vì 1) chúng phải lấy trung bình tham số của các chuyên gia, có thể trở thành nút thắt cổ chai về thời gian và/hoặc bộ nhớ khi sử dụng chuyên gia với nhiều tham số; và 2) chúng không thể tận dụng các phép toán vector hóa rộng rãi như Soft (và Sparse) MoE, vì mỗi đầu vào sử dụng một tổ hợp trọng số khác nhau của tham số. Chúng tôi khuyến nghị thảo luận "chi phí tính toán" trong Muqeeth et al. (2023).

6 HẠN CHẾ HIỆN TẠI
Giải mã tự hồi quy Một trong những khía cạnh chính của Soft MoE bao gồm việc học trộn tất cả token trong đầu vào. Điều này làm cho việc sử dụng Soft MoE trong decoder tự hồi quy khó khăn, vì tính nhân quả giữa token quá khứ và tương lai phải được bảo toàn trong quá trình huấn luyện. Mặc dù có thể sử dụng causal mask được sử dụng trong lớp attention, người ta phải cẩn thận để không đưa ra bất kỳ tương quan nào giữa chỉ số token và slot, vì điều này có thể thiên lệch chỉ số token nào mà mỗi chuyên gia được huấn luyện. Việc sử dụng Soft MoE trong decoder tự hồi quy là một hướng nghiên cứu đầy hứa hẹn mà chúng tôi để lại cho công việc tương lai.

Chuyên gia lười biếng & tiêu thụ bộ nhớ Chúng tôi cho thấy trong Phần 3 rằng một slot mỗi chuyên gia có xu hướng là lựa chọn tối ưu. Nói cách khác, thay vì cung cấp cho một chuyên gia hai slot, việc sử dụng hai chuyên gia với một slot mỗi cái hiệu quả hơn. Chúng tôi giả thuyết rằng các slot sử dụng cùng một chuyên gia có xu hướng căn chỉnh và cung cấp ít lợi ích thông tin, và một chuyên gia có thể thiếu tính linh hoạt để điều chỉnh các phép chiếu slot rất khác nhau. Chúng tôi cho thấy điều này trong Phụ lục I. Do đó, Soft MoE có thể tận dụng một số lượng lớn chuyên gia và—trong khi chi phí của nó vẫn tương tự như backbone dày đặc—yêu cầu bộ nhớ của mô hình có thể tăng lớn.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TÀI LIỆU THAM KHẢO
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. JAX: composable transformations of Python+ NumPy programs, 2018.
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.
Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057–4086. PMLR, 2022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.
Tobias Domhan. How much attention do you need? a granular analysis of neural machine translation architectures. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1799–1808, 2018.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1): 5232–5270, 2022.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026–1034, 2015.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 4651–4664. PMLR, 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing neural networks. Advances in neural information processing systems, 30, 2017.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 6265–6274. PMLR, 2021.
Tianlin Liu, Joan Puigcerver, and Mathieu Blondel. Sparsity-constrained optimal transport. arXiv preprint arXiv:2209.15466, 2022.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing, 2023.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770, 2022.
Cedric Renggli, André Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, and Carlos Riquelme. Learning to merge tokens in vision transformers. arXiv preprint arXiv:2202.12015, 2022.
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583–8595, 2021.
Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555–17566, 2021.
Michael S Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021.
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400m: Open dataset of CLIP-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 282–298. Springer, 2020.
Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe Wang. Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12186–12195, June 2022.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524–10533. PMLR, 2020.
Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. Condconv: Conditionally parameterized convolutions for efficient inference. Advances in Neural Information Processing Systems, 32, 2019.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–12113, 2022a.
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123–18133, 2022b.
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103–7114, 2022.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A SO SÁNH TRỌNG SỐ DISPATCH VÀ COMBINE SOFT VS. UNIFORM VS. IDENTITY
Trong phần này, chúng tôi so sánh Soft MoE (tức là thuật toán sử dụng trọng số dispatch và combine được tính toán bởi Soft MoE trong eq. (1) và eq. (2)) với các lựa chọn "định tuyến cố định" khác nhau, trong đó cả chuyên gia được chọn và trọng số của các tổ hợp lồi đều không phụ thuộc vào nội dung của token.

Chúng tôi xem xét các sửa đổi đơn giản sau của Soft MoE:
Identity. Token đầu tiên trong chuỗi được xử lý bởi chuyên gia đầu tiên, token thứ hai bởi chuyên gia thứ hai, và cứ thế tiếp theo theo kiểu round robin. Khi độ dài chuỗi bằng số lượng slot và chuyên gia, điều này tương đương với việc thay thế ma trận D trong eq. (1) (resp. C trong eq. (2)) bằng ma trận đồng nhất.

Uniform. Mỗi slot đầu vào được lấp đầy bằng trung bình đồng nhất của tất cả token đầu vào, và mỗi token đầu ra là trung bình đồng nhất của tất cả slot đầu ra. Điều này tương đương với việc thay thế ma trận D từ eq. (1) bằng giá trị 1/m trong tất cả phần tử, và ma trận C từ eq. (2) bằng giá trị 1/np trong tất cả phần tử. Chúng tôi khởi tạo ngẫu nhiên và độc lập mỗi chuyên gia.

Uniform / Soft. Mỗi slot đầu vào được lấp đầy bằng trung bình đồng nhất của tất cả token đầu vào, nhưng chúng tôi giữ định nghĩa của C từ eq. (2).

Soft / Uniform. Mỗi token đầu ra là trung bình đồng nhất của tất cả slot đầu ra, nhưng chúng tôi giữ định nghĩa của D trong eq. (1).

Hình 7 và Bảng 2 cho thấy kết quả từ thí nghiệm này, huấn luyện một mô hình backbone S/14 với MoE trên 6 lớp cuối. Vì độ dài chuỗi là 256, chúng tôi chọn 256 chuyên gia và slot (tức là 1 slot mỗi chuyên gia), để các ma trận D và C là hình vuông. Như được thể hiện trong hình, Soft MoE tốt hơn nhiều so với tất cả các lựa chọn khác. Để tham khảo, chúng tôi cũng thêm ViT S/14 dày đặc vào so sánh.

0123
Train steps×10⁵0.350.400.450.500.55JFT-4B Precision-at-1
0123
Train steps×10⁵0.450.500.550.600.650.700.75ImageNet 10shot AccuracyIdentity
Uniform
Uniform / Soft
Soft / Uniform
Soft
Dense

Hình 7: Soft MoE được so sánh với các chiến lược "định tuyến cố định" khác nhau. Identity xử lý token thứ i với chuyên gia thứ i; Uniform thay thế cả ma trận dispatch và combine bằng trung bình đồng nhất; Uniform / Soft thay thế trọng số dispatch bằng trung bình đồng nhất, nhưng trọng số combine được tính toán như trong Soft MoE; Soft / Uniform thực hiện phép thay thế ngược lại; và Soft sử dụng thuật toán chúng tôi trình bày trong Phần 2.

B LOẠI BỎ TOKEN
Trong phụ lục này, chúng tôi khám phá ngắn gọn việc loại bỏ token cho thuật toán Experts Choose và Tokens Choose. Với Tokens Choose, mỗi token chọn K chuyên gia. Khi chuyên gia đầy, một số token được gán cho chuyên gia đó sẽ không được xử lý. Một token bị "loại bỏ" khi không có lựa chọn nào của nó đi qua, và không có chuyên gia nào xử lý token đó. Thuật toán Expert Choose dẫn đến lượng xử lý không đều mỗi token: một số token đầu vào được nhiều chuyên gia chọn, trong khi một số khác không được chọn bởi ai. Chúng tôi thường định nghĩa số lượng token được xử lý bởi mỗi chuyên gia theo cách mà dung lượng kết hợp của tất cả chuyên gia tương ứng với số lượng token đầu vào (hoặc bội số C của chúng). Nếu chúng tôi sử dụng hệ số C cao hơn một (chẳng hạn, 2x hoặc 3x), lượng loại bỏ sẽ giảm nhưng chúng tôi sẽ trả chi phí tính toán gia tăng. Do đó, chúng tôi chủ yếu khám phá cài đặt K=1 và C=1, nơi không có slack trong buffer.

Trong tất cả các trường hợp sau, chúng tôi thấy một xu hướng chung: cố định mọi thứ không đổi, tăng số lượng chuyên gia dẫn đến ngày càng nhiều việc loại bỏ cả trong Experts Choose và Tokens Choose.

Hình 8 so sánh Experts Choose và Tokens Choose với cùng hệ số C=1. Đây là cài đặt rẻ nhất nơi mỗi token có thể được gán cho một chuyên gia với định tuyến cân bằng. Chúng tôi thấy rằng trong cả hai trường hợp, lượng loại bỏ nhanh chóng tăng với số lượng chuyên gia. Hơn nữa, mặc dù Experts Choose có mức độ loại bỏ cao hơn (đặc biệt là với số lượng lớn chuyên gia), nó vẫn hoạt động tốt hơn Tokens Choose. Lưu ý có một sự khác biệt cơ bản: khi Tokens Choose loại bỏ một token, mô hình lãng phí lượng tính toán tiềm năng đó. Mặt khác, đối với Experts Choose, loại bỏ chỉ có nghĩa là một token khác đã nhận được chỗ đó trong buffer chuyên gia, do đó mô hình chỉ chuyển tính toán từ một token kém may mắn sang một token may mắn khác.

Trong cài đặt này, với số lượng nhỏ chuyên gia (16-32), thường quan sát thấy tỷ lệ loại bỏ ~15%. Mặt khác, chúng tôi cũng thí nghiệm với số lượng lớn chuyên gia (100-1000) nơi mỗi chuyên gia chọn rất ít token. Trong trường hợp này, tỷ lệ loại bỏ cho Experts Choose có thể tăng trên 40-50% trong một số lớp: hầu hết chuyên gia chọn cùng những token. Tokens Choose dường như hoàn toàn loại bỏ lên đến ~25% token.

Trong Hình 9 và 10, chúng tôi nghiên cứu một chút buffer slack (C=1.125) có thể giúp ích như thế nào về hiệu suất và loại bỏ cho Experts Choose và Tokens Choose, tương ứng. Cả hai biểu đồ đều tương tự: lượng loại bỏ giảm khoảng ~5% và hiệu suất tăng nhẹ khi số lượng chuyên gia lớn. Lưu ý rằng thời gian bước cũng tăng trong những trường hợp này.

Cuối cùng, Hình 11 cho thấy tác động của Batch Priority Routing (Riquelme et al., 2021) đối với Tokens Choose. Bằng cách chọn thông minh token nào để loại bỏ, chúng tôi không chỉ giảm đồng đều lượng loại bỏ mà còn tăng đáng kể hiệu suất.

C SOFT MOE TĂNG SLOT
Trong phần này, chúng tôi khám phá câu hỏi sau: đối với một số lượng chuyên gia cố định, Soft MoE routing hưởng lợi bao nhiều từ việc có thêm slot mỗi chuyên gia? Hình 12 cho thấy kết quả cho Soft MoE S/16 với 32 chuyên gia. Chúng tôi cũng cho thấy Experts Choice với kích thước nhóm một và tám hình ảnh. Khi tăng số lượng slot, hiệu suất chỉ tăng khiêm tốn, trong khi chi phí tăng nhanh. Experts Choice hưởng lợi nhiều hơn từ việc tăng slot, bắt kịp ở kích thước nhóm lớn, nhưng với chi phí rất lớn.

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
4 816 32 64128 256 5121024
total number of experts0.5500.5750.6000.6250.6500.6750.7000.7250.750ImageNet 10-shot Accuracy
4 816 32 64128 256 5121024
total number of experts0.460.480.500.520.54JFT-4B Precision-at-1
4 816 32 64128 256 5121024
total number of experts0.00.10.20.30.40.5Maximum Dropping Ratio across LayersExperts Choose C=1
Tokens Choose K=1 C=1

Hình 8: S/14. Hiệu suất và lượng loại bỏ token cho số lượng chuyên gia tăng cho Experts Choose (C=1) và Tokens Choose (K=1 và C=1).

4 816 32 64128 256 5121024
total number of experts0.5500.5750.6000.6250.6500.6750.7000.7250.750ImageNet 10-shot Accuracy
4 816 32 64128 256 5121024
total number of experts0.460.480.500.520.54JFT-4B Precision-at-1
4 816 32 64128 256 5121024
total number of experts0.00.10.20.30.40.5Maximum Dropping Ratio across LayersTokens Choose K=1 C=1
Tokens Choose K=1 C=1.125

Hình 9: S/14. Hiệu suất và lượng loại bỏ token cho số lượng chuyên gia tăng cho Tokens Choose với buffer chặt (K=1 và C=1) và một chút buffer slack (K=1 và C=1.125).

4 816 32 64128 256 5121024
total number of experts0.5500.5750.6000.6250.6500.6750.7000.7250.750ImageNet 10-shot Accuracy
4 816 32 64128 256 5121024
total number of experts0.460.480.500.520.54JFT-4B Precision-at-1
4 816 32 64128 256 5121024
total number of experts0.00.10.20.30.40.5Maximum Dropping Ratio across LayersExperts Choose C=1
Experts Choose C=1.125

Hình 10: S/14. Hiệu suất và lượng loại bỏ token cho số lượng chuyên gia tăng cho Experts Choose với buffer chặt (C=1) và buffer hơi lớn hơn (C=1.125).

D VỊ TRÍ LỚP SPARSE
Soft MoE thực sự mở khóa việc sử dụng hiệu quả số lượng lớn chuyên gia. Một lựa chọn thiết kế quan trọng cho mô hình sparse là số lượng và vị trí của lớp sparse, cùng với số lượng chuyên gia mỗi lớp. Thật không may, số lượng lớn bậc tự do trong những lựa chọn này thường làm cho ablation và tối ưu hóa toàn diện trở nên không khả thi. Trong phần này, chúng tôi cung cấp kết quả của một thí nghiệm đơn giản có thể giúp thiết kế tốt hơn cấu hình của mô hình sparse. Chúng tôi cố định tổng số chuyên gia (E=512) với một slot mỗi chuyên gia, do đó dẫn đến số lượng tham số khớp (lưu ý trong trường hợp này FLOP có thể thay đổi rất nhiều tùy thuộc vào số lượng lớp sparse). Sau đó, cho một kiến trúc backbone S/16

481632641282565121024
total number of experts0.5500.5750.6000.6250.6500.6750.7000.7250.750ImageNet 10-shot Accuracy
481632641282565121024
total number of experts0.460.480.500.520.54JFT-4B Precision-at-1
481632641282565121024
total number of experts0.00.10.20.30.40.5Maximum Dropping Ratio across LayersTokens Choose K=1 C=1
Tokens Choose K=1 C=1 No BPR

Hình 11: S/14. Hiệu suất và lượng loại bỏ token cho số lượng chuyên gia tăng có và không có BPR cho Tokens Choose.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
1 2 34 8 16 32
slots per expert0.490.500.510.520.53JFT-4B Precision-at-1
1 2 34 8 16 32
slots per expert0.640.660.680.700.72ImageNet 10-shot Accuracy
1 2 34 8 16 32
slots per expert100125150175200225250Train step time in millisecondsSoft-MoE Experts Choice (gs = 1 img) Experts Choice (gs = 8 img)

Hình 12: Hiệu suất (trái, giữa) và thời gian bước (phải) của các mô hình với 32 chuyên gia, nhưng slot tăng, tất cả được huấn luyện trong cùng số bước (300k). Tăng số lượng slot mỗi chuyên gia chỉ tăng hiệu suất của Soft MoE một lượng nhỏ, trong khi tăng chi phí đáng kể.

backbone architecture, chúng tôi phân phối những chuyên gia đó theo nhiều cách khác nhau (tất cả trong một lớp, một nửa trong hai lớp, v.v.) và so sánh hiệu suất của chúng sau 300k bước huấn luyện. Bảng 4 cho thấy kết quả. Một lần nữa, chúng tôi quan sát thấy rằng số lượng chuyên gia gần với số lượng token đầu vào (có 196 token, với kích thước patch 16x16 cho hình ảnh 224x224) được chia trên vài lớp cuối hoạt động tốt nhất. Hơn nữa, lưu ý những mô hình này thực sự rẻ hơn những mô hình trong so sánh với 512 hoặc 256 chuyên gia mỗi lớp.

Bảng 5 đưa ra kết quả cho định tuyến Tokens Choose với K=1 và BPR Riquelme et al. (2021). Trong trường hợp này, tất cả thuật toán sử dụng số lượng FLOP tương đương (bỏ qua chi phí định tuyến tăng nhẹ với nhiều chuyên gia hơn). Kết quả cơ bản tương tự, do đó đề xuất rằng vị trí chuyên gia tối ưu (bao gồm số lượng chuyên gia và vị trí) có thể không phụ thuộc mạnh vào thuật toán định tuyến.

Bảng 4: Ablation vị trí chuyên gia với Soft MoE S/16 với 12 lớp (chỉ số từ 0 đến 11).
Sparse Layers Experts per Layer Total Experts IN/10shot JFT prec1
11 512 512 70.0% 51.5%
10 512 512 70.1% 52.0%
10, 11 256 512 71.7% 52.2%
5, 11 256 512 70.4% 52.1%
8, 9, 10, 11 128 512 72.8% 53.2%
2, 5, 8, 11 128 512 71.1% 52.5%
4:11 64 512 72.1% 53.1%
1:4, 8:11 64 512 70.5% 52.1%

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 5: Ablation vị trí chuyên gia với V-MoE S/16 Tokens Choose K=1 với 12 lớp (chỉ số là 0:11).
Sparse Layers Experts per Layer Total Experts IN/10shot JFT prec1
11 512 512 64.4% 50.1%
10 512 512 67.2% 51.9%
10, 11 256 512 68.6% 51.3%
5, 11 256 512 65.3% 50.6%
8, 9, 10, 11 128 512 69.1% 52.3%
2, 5, 8, 11 128 512 67.3% 51.1%
4:11 64 512 69.9% 52.2%
1:4, 8:11 64 512 68.0% 51.2%

Bảng 6: Ablation vị trí chuyên gia với V-MoE S/16 Experts Choose C=1 với 12 lớp (chỉ số là 0:11).
Sparse Layers Experts per Layer Total Experts IN/10shot JFT prec1
11 512 512 65.3% 50.3%
10 512 512 66.5% 51.7%
10, 11 256 512 68.8% 51.8%
5, 11 256 512 65.9% 51.1%
8, 9, 10, 11 128 512 69.4% 52.2%
2, 5, 8, 11 128 512 68.0% 51.7%
4:11 64 512 69.0% 52.2%
1:4, 8:11 64 512 67.4% 51.1%

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
E SỰ SỤP ĐỔ CỦA LỚP SOFTMAX ĐƯỢC ÁP DỤNG SAU LAYER NORMALIZATION
E.1 PHÂN TÍCH LÝ THUYẾT
Một lớp softmax với tham số Θ∈Rn×d biến đổi vector x∈Rd thành vector softmax(Θx)∈Rn, với các phần tử:

softmax(Θx)i=exp((Θx)i)/∑j=1nexp((Θx)j)=exp(∑k=1dθikxk)/∑j=1nexp(∑k=1dθjkxk) (3)

Layer normalization áp dụng phép toán sau trên x∈Rd.

LN(x)i=αi(xi−μ(x))/σ(x)+βi; trong đó μ(x)=1/d∑i=1dxi và σ(x)=√(1/d∑i=1d(xi−μ(xi))²) (4)

Chú ý rằng LN(x)=LN(x−μ(x)), do đó chúng ta có thể viết lại LayerNorm đối với vector tâm ˜x=x−μ(x), và vector tâm được điều chỉnh để có chuẩn đơn vị ˆxi=˜xi/‖˜x‖:

LN(˜x)i=αi˜xi/√(1/d∑j=1d˜xj²)+βi=√dαi˜xi/‖˜x‖+βi=√dαiˆxi+βi (5)

Khi một lớp softmax được áp dụng cho đầu ra của layer normalization, đầu ra của softmax được cho bởi phương trình:

softmax(ΘLN(x))i=exp(∑k=1dθik(√dαkˆxk+βk))/∑j=1nexp(∑k=1dθjk(√dαkˆxk+βk)) (6)

Bằng cách đặt ϑi=∑k=1dθikαkˆxk, và δi=∑k=1dθikβk, phương trình trước có thể được viết lại thành:

softmax(ΘLN(x))i=exp(√dϑi+δi)/∑j=1nexp(√dϑj+δj) (7)

Định nghĩa m=maxi∈[n]√dϑi−δi, M={i∈[n]:√dϑi−δi=m}. Khi đó, đẳng thức sau thành lập:

softmax(ΘLN(x))i=exp(√dϑi+δi−m)/∑j=1nexp(√dϑj+δj−m) (8)

Cho rằng limd→∞exp(√dϑi+δi−m)={1:i∈M; 0:i∉M}, đầu ra của softmax tiến về:

limd→∞softmax(ΘLN(x))i={(1/|M|):i∈M; 0:i∉M} (9)

Cụ thể, khi giá trị cực đại chỉ được đạt bởi một trong các thành phần (tức là |M|=1), softmax sụp đổ thành vector một-nóng (vector với tất cả phần tử bằng 0 trừ một phần tử).

E.2 PHÂN TÍCH THỰC NGHIỆM
Phân tích lý thuyết trước giả định rằng tham số của lớp softmax là hằng số, hay cụ thể hơn là chúng không phụ thuộc vào d. Người ta có thể tranh luận rằng việc sử dụng các kỹ thuật khởi tạo tham số hiện đại, tính đến 1/√d trong độ lệch chuẩn của việc khởi tạo Glorot and Bengio (2010); He et al. (2015); Klambauer et al. (2017), có thể khắc phục vấn đề này. Chúng tôi thấy rằng chúng không (cụ thể, chúng tôi sử dụng khởi tạo từ Glorot and Bengio (2010)).

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
0123
Train steps×10⁵0.00.10.20.30.40.50.60.70.8ImageNet 10shot Accuracyd = 384
0123
Train steps×10⁵d = 768
0123
Train steps×10⁵d = 1024
0123
Train steps×10⁵d = 1280
0123
Train steps×10⁵d = 1664
Normalization
No
Yes
(a) Độ chính xác ImageNet 10shot.

0123
Train steps×10⁵0.00.10.20.30.40.5Average maximum dispatch weightd = 384
0123
Train steps×10⁵d = 768
0123
Train steps×10⁵d = 1024
0123
Train steps×10⁵d = 1280
0123
Train steps×10⁵d = 1664
Normalization
No
Yes
(b) Giá trị trung bình của trọng số dispatch tối đa mỗi slot.

0123
Train steps×10⁵0.00.10.20.30.40.50.60.70.8Average maximum combine weightd = 384
0123
Train steps×10⁵d = 768
0123
Train steps×10⁵d = 1024
0123
Train steps×10⁵d = 1280
0123
Train steps×10⁵d = 1664
Normalization
No
Yes
(c) Giá trị trung bình của trọng số combine tối đa mỗi token.

Hình 13: Biểu đồ huấn luyện của độ chính xác ImageNet 10shot (trên), giá trị trung bình của trọng số dispatch tối đa mỗi slot (giữa) và giá trị trung bình của trọng số combine tối đa mỗi token (dưới) cho các chiều mô hình d khác nhau. Quan sát thấy rằng giá trị tối đa của trọng số combine và (đặc biệt) dispatch tăng khi chiều mô hình tăng trong quá trình huấn luyện, như phân tích lý thuyết của chúng tôi dự đoán. Mặc dù độ chính xác ImageNet 10shot tương tự với chiều mô hình nhỏ, việc áp dụng lớp softmax trực tiếp trên đầu ra của layer normalization, không có bất kỳ chuẩn hóa nào khác, làm tổn hại độ chính xác khi chiều mô hình d tăng. Bằng cách chuẩn hóa đầu vào cho softmax như đề xuất trong Phần 2.3 cải thiện hiệu suất cho giá trị d lớn.

Hình 13 cho thấy các đường cong chỉ số khác nhau trong quá trình huấn luyện của một mô hình SoftMoE nhỏ với các chiều mô hình khác nhau. Các chiều mô hình là những chiều tương ứng với các backbone tiêu chuẩn khác nhau: S (384), B (768), L (1024), H (1280) và G (1664). Các tham số kiến trúc còn lại được cố định: 6 lớp (3 lớp dày đặc theo sau bởi 3 lớp MoE với 256 chuyên gia), patch 14x14, và chiều MLP 1536. Khi chiều mô hình d tăng, hình cho thấy rằng, nếu đầu vào cho softmax trong lớp SoftMoE không được chuẩn hóa, giá trị tối đa trung bình của trọng số dispatch và combine có xu hướng tăng (đặc biệt là cái đầu tiên). Khi d đủ lớn, độ chính xác ImageNet 10shot tệ hơn đáng kể so với việc chuẩn hóa đầu vào đúng cách.

Trong thí nghiệm trước, chúng tôi huấn luyện mô hình với lịch giảm tuyến tính và giá trị đỉnh 10⁻³. Ngoài ra, chúng tôi cũng thấy rằng việc áp dụng lớp softmax trực tiếp trên đầu ra của layer normalization cũng rất nhạy cảm với cấu hình tỷ lệ học. Một lần nữa, công thức của chúng tôi đề xuất trong Phần 2.3 cho chất lượng bằng nhau hoặc tốt hơn, và nói chung ổn định hơn. Hình 14 cho thấy các đường cong chỉ số khác nhau trong quá trình huấn luyện của cùng mô hình SoftMoE nhỏ như trước, với chiều mô hình d=1664, sử dụng lịch tỷ lệ học căn bậc hai nghịch đảo, với timescale cố định

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
0123
Train steps×10⁵0.00.10.20.30.40.50.60.70.8ImageNet 10shot AccuracyLearning rate = 0.0003
0123
Train steps×10⁵Learning rate = 0.0006
0123
Train steps×10⁵Learning rate = 0.001
0123
Train steps×10⁵Learning rate = 0.003
0123
Train steps×10⁵Learning rate = 0.006
Normalization
No
Yes
(a) Độ chính xác ImageNet 10shot.

0123
Train steps×10⁵0.00.20.40.60.8Average maximum dispatch weightLearning rate = 0.0003
0123
Train steps×10⁵Learning rate = 0.0006
0123
Train steps×10⁵Learning rate = 0.001
0123
Train steps×10⁵Learning rate = 0.003
0123
Train steps×10⁵Learning rate = 0.006
Normalization
No
Yes
(b) Giá trị trung bình của trọng số dispatch tối đa mỗi slot.

0123
Train steps×10⁵0.00.20.40.60.81.0Average maximum combine weightLearning rate = 0.0003
0123
Train steps×10⁵Learning rate = 0.0006
0123
Train steps×10⁵Learning rate = 0.001
0123
Train steps×10⁵Learning rate = 0.003
0123
Train steps×10⁵Learning rate = 0.006
Normalization
No
Yes
(c) Giá trị trung bình của trọng số combine tối đa mỗi token.

Hình 14: Biểu đồ huấn luyện của độ chính xác ImageNet 10shot (trên), giá trị trung bình của trọng số dispatch tối đa mỗi slot (giữa) và giá trị trung bình của trọng số combine tối đa mỗi token (dưới) cho các giá trị đỉnh khác nhau của tỷ lệ học, sử dụng chiều mô hình d=1664 (tức là của backbone G).

10⁵, một giai đoạn warmup tuyến tính 10⁵ bước, và cooldown tuyến tính 5·10⁵ bước, thay đổi giá trị tỷ lệ học đỉnh. Trong hình này, tương tự như kết quả từ thí nghiệm trước, giá trị tối đa trung bình của trọng số dispatch và combine tăng lên giá trị tiến gần 1.0 (chỉ ra sự sụp đổ trong lớp softmax thành vector một-nóng), khi đầu vào cho softmax trong lớp SoftMoE không được chuẩn hóa, cuối cùng làm tổn hại nghiêm trọng độ chính xác của mô hình. Tuy nhiên, việc sử dụng chuẩn hóa trong Phần 2.3 cho độ chính xác tốt hơn và làm cho mô hình ít nhạy cảm hơn với việc lựa chọn giá trị đỉnh của tỷ lệ học.

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F KẾT QUẢ BỔ SUNG
F.1 HỌC CONTRASTIVE TRÊN LAION-400M
Chúng tôi bổ sung huấn luyện một mô hình contrastive, tương tự như mô tả trong Phần 4, nhưng huấn luyện cả vision và language tower từ đầu trên tập dữ liệu LAION-400M² có sẵn công khai (Schuhmann et al., 2021). Kiến trúc backbone của vision tower là B/16. Chúng tôi huấn luyện một mô hình sử dụng Vision Transformer thuần túy, một mô hình khác với lớp MoE trên nửa cuối mạng với 128 chuyên gia trên mỗi lớp, sử dụng router Experts Choice, và một mô hình thứ ba sử dụng Soft MoE. Tất cả mô hình sử dụng cùng kiến trúc text tower không có MoE. Tất cả mã cần thiết để tái tạo thí nghiệm, bao gồm siêu tham số huấn luyện được sử dụng, có sẵn tại https://github.com/google-research/vmoe.

0.0 2.5 5.0 7.5
Training steps 1e50.400.450.500.550.600.650.70ImageNet 10-shot Accuracy
ViT
Experts Choice
Soft MoE
0.0 2.5 5.0 7.5
Training steps 1e50.500.550.600.650.700.75ImageNet 0-shot Accuracy
ViT
Experts Choice
Soft MoE
0.0 2.5 5.0 7.5
Training steps 1e50.600.650.700.750.800.850.90Oxford IIIT Pet 0-shot Accuracy
ViT
Experts Choice
Soft MoE

Hình 15: Kết quả sau khi pretraining trên LAION-400M. Đường đứt khúc tương ứng với mô hình được huấn luyện với data augmentation "Inception crop", trong khi đường liền tương ứng với mô hình được huấn luyện không có data augmentation. Soft MoE hoạt động tốt hơn vanilla Vision Transformer (ViT) và Sparse MoE với router Experts Choice, và hưởng lợi từ data augmentation.

Hình 15 cho thấy rằng, như với pretraining JFT-4B và WebLI (xem Phần 3 và 4 tương ứng), khi pretraining trên tập dữ liệu LAION-400M có sẵn công khai, Soft MoE hoạt động tốt hơn đáng kể so với cả vanilla Vision Transformer và ViT với lớp Sparse MoE sử dụng router Experts Choice, trên các chỉ số downstream khác nhau. Ngoài ra, chúng tôi cũng có thể thấy rằng Soft MoE hưởng lợi từ data augmentation, trong khi cả vanilla ViT và Expert Choice đều không. Theo quan sát từ Hình 6 trong Phần 3, chúng tôi giả thuyết rằng điều này là do Soft MoE có thể sử dụng tốt hơn các tham số chuyên gia.

F.2 TÁC ĐỘNG CỦA BATCH PRIORITY ROUTING TRÊN TOKENS CHOICE ROUTING
Bảng 7 cho thấy JFT Precision-at-1 và ImageNet 10-shot Accuracy của mô hình S/16 MoE với router Tokens Choice, có/không sử dụng Batch Priority Routing (BPR), cho số lượng chuyên gia tổng khác nhau và chuyên gia được chọn mỗi token. Bảng này cho thấy rằng BPR đặc biệt hữu ích cho K=1.

F.3 CÁC BẢNG VÀ BIỂU ĐỒ BỔ SUNG BỔ SUNG CHO PHẦN 3

²Chúng tôi thực sự sử dụng một tập con 275M hình ảnh và cặp văn bản, vì nhiều trong số 400M ví dụ ban đầu không thể tải xuống hoặc chứa hình ảnh bị hỏng (tức là việc giải mã những hình ảnh đó thất bại).

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 7: So sánh giữa Top-K có và không có BPR.
Model Number of Experts K BPR JFT prec@1 IN/10shot
V-MoE S/16 32 1 No 50.1% 64.5%
V-MoE S/16 32 1 Yes 51.2% 68.9%
V-MoE S/16 32 2 No 52.5% 71.0%
V-MoE S/16 32 2 Yes 52.8% 71.4%
V-MoE S/16 64 1 No 50.0% 64.4%
V-MoE S/16 64 1 Yes 51.5% 69.1%
V-MoE S/16 64 2 No 52.9% 70.9%
V-MoE S/16 64 2 Yes 52.9% 71.4%

Bảng 8: Kết quả huấn luyện và finetuning cho Soft MoE và mô hình dày đặc. Kết quả finetuning trên ImageNet ở độ phân giải 384. Chúng tôi sử dụng một slot mỗi chuyên gia và không tăng số này trong quá trình finetuning, do đó Soft MoE trở nên rẻ hơn ViT, khi số lượng token đầu vào tăng lên 576 (kích thước patch 16x16) và 752 (kích thước patch 14x14) nhưng số lượng slot được cố định ở một số nhỏ hơn nhiều (128 hoặc 256).

Model Params Train steps Train days & exaFLOP Eval Ms/img & GFLOP/img JFT P@1 IN/10s IN/ft
ViT S/16 33M 4M (50k) 153.5 227.1 0.5 9.2 51.3 67.6 84.0
Soft MoE S/16 128E 933M 4M (50k) 175.1 211.9 0.7 8.6 58.1 78.8 86.8
Soft MoE S/16 128E 933M 10M (50k) 437.7 529.8 0.7 8.6 59.2 79.8 87.1
Soft MoE S/14 256E 1.8B 4M (50k) 197.9 325.7 0.9 13.2 58.9 80.0 87.2
Soft MoE S/14 256E 1.8B 10M (500k) 494.7 814.2 0.9 13.2 60.9 80.7 87.7
ViT B/16 108M 4M (50k) 410.1 864.1 1.3 35.1 56.2 76.8 86.6
Soft MoE B/16 128E 3.7B 4M (50k) 449.5 786.4 1.5 32.0 60.0 82.0 88.0
ViT L/16 333M 4M (50k) 1290.1 3025.4 4.9 122.9 59.8 81.5 88.5
Soft MoE L/16 128E 13.1B 1M (50k) 338.9 683.5 4.8 111.1 60.2 82.9 88.4
Soft MoE L/16 128E 13.1B 2M (50k) 677.7 1367.0 4.8 111.1 61.3 83.3 88.9
Soft MoE L/16 128E 13.1B 4M (50k) 1355.4 2734.1 4.8 111.1 61.3 83.7 88.9
ViT H/14 669M 1M (50k) 1019.9 2060.2 8.6 334.2 58.8 82.7 88.6
ViT H/14 669M 2M (50k) 2039.8 4120.3 8.6 334.2 59.7 83.3 88.9
Soft MoE H/14 128E 27.3B 1M (50k) 1112.7 1754.6 8.8 284.6 61.0 83.7 88.9
Soft MoE H/14 128E 27.3B 2M (50k) 2225.4 3509.2 8.8 284.6 61.7 84.2 89.1
Soft MoE H/14 256E 54.1B 1M (50k) 1276.9 2110.1 10.9 342.4 60.8 83.6 88.9
Soft MoE H/14 256E 54.1B 2M (50k) 2553.7 4220.3 10.9 342.4 62.1 84.3 89.1

10³
Total Training TPUv3-days67%73%77%81%84%ImageNet 10-shot AccuracyS/16B/16L/16H/14
10³
Total Training TPUv3-days51%54%57%59%JFT-4B Precision-at-1S/16B/16L/16H/14
10³
Total Training TPUv3-days84%85%87%88%89%ImageNet Finetune AccuracyS/16B/16L/16H/14
Soft MoE
Dense

Hình 16: Chạy dài. Mô hình Soft MoE và ViT được huấn luyện trong 4 triệu bước với batch size 4096 (mô hình H/14 được huấn luyện trong 2 triệu bước thay vào đó). Các lớp mô hình tương đương (S/16, B/16, L/16, H/14) có chi phí huấn luyện tương tự, nhưng Soft MoE vượt trội hơn ViT trên tất cả chỉ số. Chúng tôi cho thấy ImageNet 10-shot (trái), JFT precision at 1 (giữa) và độ chính xác ImageNet sau finetuning (phải), so với tổng FLOP huấn luyện. Xem Bảng 8. Chúng tôi báo cáo FLOP huấn luyện trong Hình 4.

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
32 64 128 256 512 1024 2048 4096
Number of Experts0.5150.5200.5250.5300.5350.5400.545JFT-4B Precision-at-1
32 64 128 256 512 1024 2048 4096
Number of Experts0.690.700.710.720.730.740.75ImageNet 10-shot Accuracy
32 64 128 256 512 1024 2048 4096
Number of Experts1.001.251.501.752.002.50(normalized) Train Step Time
Soft MoE Experts Choice (gs=1 img) Experts Choice (gs=8 img) Experts Choice (gs=32 img)

Hình 17: JFT precision-at-1, ImageNet 10-shot accuracy, và thời gian bước huấn luyện chuẩn hóa khi tăng tổng số chuyên gia trong khi giữ tổng số slot cố định. Soft MoE đạt kết quả tốt hơn một cách nhất quán với nhiều chuyên gia hơn, trong khi chi phí được giữ gần như không đổi. Thêm quá nhiều chuyên gia vào Experts Choice làm tổn hại hiệu suất và tăng đáng kể chi phí. Experts Choice có thể hoạt động tốt với nhiều chuyên gia nếu chúng ta tăng kích thước nhóm lên 32 hình ảnh mỗi nhóm. Thời gian bước huấn luyện chuẩn hóa được tính toán so với Soft MoE với 32 chuyên gia. Experts Choice với 32 hình ảnh mỗi nhóm và 4096 chuyên gia yêu cầu hơn 2.5x chi phí của nó.

32 64 128 256 512 1024 2048 4096
Number of Experts0.5150.5200.5250.5300.5350.5400.545JFT-4B Precision-at-1
32 64 128 256 512 1024 2048 4096
Number of Experts0.700.710.720.730.740.75ImageNet 10-shot Accuracy
32 64 128 256 512 1024 2048 4096
Number of Experts1.001.251.501.752.002.503.003.504.00(normalized) Train Step Time
Soft MoE Tokens Choice (gs=8 img) Tokens Choice (gs=16 img)

Hình 18: JFT precision-at-1, ImageNet 10-shot accuracy, và thời gian bước huấn luyện chuẩn hóa khi tăng tổng số chuyên gia trong khi giữ tổng số slot cố định. Soft MoE đạt kết quả tốt hơn một cách nhất quán với nhiều chuyên gia hơn, trong khi chi phí được giữ gần như không đổi. Thêm quá nhiều chuyên gia vào Tokens Choice làm tổn hại hiệu suất và tăng đáng kể chi phí. Ngay cả với kích thước nhóm lớn (16 hình ảnh), Tokens Choice gặp khó khăn để hoạt động tốt với vài nghìn chuyên gia. Thời gian bước huấn luyện chuẩn hóa được tính toán so với Soft MoE với 32 chuyên gia. Tokens Choice với 8 hoặc 16 hình ảnh mỗi nhóm và 4096 chuyên gia yêu cầu gần 4x chi phí của nó.

--- TRANG 22 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10¹10²
Total Training TPUv3-days0.440.460.480.500.520.540.560.58JFT-4B Precision-at-1
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice
Tokens Choice
Dense
(a) JFT-4B Precision-at-1
10¹10²
Total Training TPUv3-days0.500.550.600.650.700.750.80ImageNet 10-shot Accuracy
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice
Tokens Choice
Dense (b) ImageNet 10-shot Accuracy

Hình 19: JFT-4B Precision-at-1 và ImageNet 10-shot accuracy trên chạy ngắn (300k bước). Kích thước marker phụ thuộc vào kích thước backbone: S/32, S/16, B/32, B/16, L/16 và H/14. Màu sắc đại diện cho các phương pháp khác nhau: Soft MoE (xanh), Sparse MoE với Experts Choice (cam) và Tokens Choice routing (xanh lá), và mô hình Dense (đỏ). Chạy MoE bao gồm các cấu hình khác nhau.

10¹10²
Total Training TPUv3-days0.440.460.480.500.520.540.560.58JFT-4B Precision-at-1
10¹10²
Total Training ExaFLOPSoft MoE
Dense
(a) JFT-4B Precision-at-1
10¹10²
Total Training TPUv3-days0.500.550.600.650.700.750.80ImageNet 10-shot Accuracy
10¹10²
Total Training ExaFLOPSoft MoE
Dense (b) ImageNet 10-shot Accuracy

Hình 20: JFT-4B Precision-at-1 và ImageNet 10-shot accuracy trên chạy ngắn (300k bước huấn luyện). Kích thước marker phụ thuộc vào kích thước backbone: S/32, S/16, B/32, B/16, L/16 và H/14. Màu sắc đại diện cho các phương pháp khác nhau: Soft MoE (xanh) và mô hình Dense (đỏ). Chạy MoE bao gồm các cấu hình khác nhau. Chúng tôi chỉ cho thấy các lần chạy không bị chi phối bởi mô hình khác sử dụng cùng phương pháp (S/8 và L/32 luôn bị chi phối).

10¹10²
Total Training TPUv3-days0.460.480.500.520.540.560.58JFT-4B Precision-at-1
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice
(a) JFT-4B Precision-at-1
10¹10²
Total Training TPUv3-days0.600.650.700.750.80ImageNet 10-shot Accuracy
10¹10²10³
Total Training ExaFLOPSoft MoE
Experts Choice (b) ImageNet 10-shot Accuracy

Hình 21: JFT-4B Precision-at-1 và ImageNet 10-shot accuracy trên chạy ngắn (300k bước huấn luyện). Kích thước marker phụ thuộc vào kích thước backbone: S/32, S/16, B/32, B/16, L/16 và H/14. Màu sắc đại diện cho các phương pháp khác nhau: Soft MoE (xanh) và Sparse MoE với Experts Choice (cam). Chạy MoE bao gồm các cấu hình khác nhau. Chúng tôi chỉ cho thấy các lần chạy không bị chi phối bởi mô hình khác sử dụng cùng phương pháp (S/8 và L/32 luôn bị chi phối).

--- TRANG 23 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
10¹10²
Total Training TPUv3-days0.460.480.500.520.540.560.58JFT-4B Precision-at-1
10¹10²10³
Total Training ExaFLOPSoft MoE
Tokens Choice
(a) JFT-4B Precision-at-1
10¹10²
Total Training TPUv3-days0.600.650.700.750.80ImageNet 10-shot Accuracy
10¹10²10³
Total Training ExaFLOPSoft MoE
Tokens Choice (b) ImageNet 10-shot Accuracy

Hình 22: JFT-4B Precision-at-1 và ImageNet 10-shot accuracy trên chạy ngắn (300k bước huấn luyện). Kích thước marker phụ thuộc vào kích thước backbone: S/32, S/16, B/32, B/16, L/16 và H/14. Màu sắc đại diện cho các phương pháp khác nhau: Soft MoE (xanh) và Sparse MoE với Tokens Choice (xanh lá). Chạy MoE bao gồm các cấu hình khác nhau. Chúng tôi chỉ cho thấy các lần chạy không bị chi phối bởi mô hình khác sử dụng cùng phương pháp (S/8 và L/32 luôn bị chi phối).

248163264128256512102420484096
Number of Experts0.480.490.500.510.520.530.54JFT-4B Precision-at-1
248163264128256512102420484096
Number of Experts0.620.640.660.680.700.720.74ImageNet 10-shot Accuracy
248163264128256512102420484096
Number of Experts0.900.951.001.051.101.15(normalized) Train Step Time
Soft MoE

Hình 23: JFT Precision-at-1, ImageNet 10-shot Accuracy, và thời gian Bước huấn luyện chuẩn hóa khi tăng tổng số chuyên gia trong khi giữ tổng số slot cố định (4096). Soft MoE đạt kết quả tốt hơn một cách nhất quán với nhiều chuyên gia hơn, trong khi chi phí được giữ gần như không đổi (cùng FLOP nhưng chi phí giao tiếp thay đổi do cần topology cao hơn cho mô hình lớn hơn). Thời gian bước huấn luyện chuẩn hóa được tính toán so với Soft MoE với 32 chuyên gia. Kích thước mô hình từ 38M (2 chuyên gia) đến 9.7B tham số (4096 chuyên gia).

--- TRANG 24 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
G KIỂM TRA MÔ HÌNH
Trong phần này, chúng tôi xem xét các khía cạnh khác nhau của việc định tuyến mà mô hình học được.

Đóng góp của token vào slot. Mặc dù không có loại bỏ trong Soft MoE, vẫn có thể một số token đóng góp ít cho tất cả slot nếu logit của chúng thấp hơn nhiều so với token khác. Chúng tôi muốn xem liệu một số token đóng góp vào slot theo cách không cân xứng. Hình 24 (trái) cho thấy phân phối qua token cho tổng trọng số mà mỗi token cung cấp cho slot (tức là tổng trên tất cả slot). Điều này được tính toán trên một batch với 256 hình ảnh với 196 token mỗi hình trên Soft MoE S/16 được finetuning trên ImageNet. Chúng tôi thấy có một đuôi nặng của token cung cấp đóng góp tổng mạnh hơn cho slot, và hình dạng hơi tương tự qua các lớp. Khoảng 2-5% token cung cấp trọng số tổng trên 2. Ngoài ra, giữa 15% và 20% token chỉ đóng góp lên đến 0.25 trong tổng trọng số. Lớp cuối hơi khác, nơi đóng góp token có đuôi mềm hơn. Phụ lục H khám phá thêm điều này.

Đóng góp của chuyên gia vào đầu ra. Tương tự, chúng tôi muốn hiểu bao nhiều slot khác nhau cuối cùng đóng góp vào token đầu ra. Chúng tôi tập trung vào trường hợp một slot mỗi chuyên gia. Chúng tôi có thể ước tính tổng đóng góp của mỗi chuyên gia (tương đương, slot) bằng cách lấy trung bình các hệ số tương ứng của chúng trong tổ hợp tuyến tính cho tất cả token đầu ra trong một batch. Hình 24 (giữa) cho thấy tầm quan trọng (chuẩn hóa) như vậy qua chuyên gia cho các lớp MoE khác nhau. Chúng tôi thấy rằng, tùy thuộc vào lớp, một số chuyên gia có thể tác động đến token đầu ra từ 3x đến 14x nhiều hơn các chuyên gia khác.

Số lượng token đầu vào mỗi slot. Đối với mỗi slot, Hình 24 (phải) cho thấy bao nhiêu token đầu vào được yêu cầu để đạt một trọng số tích lũy nhất định trong tổ hợp tuyến tính của nó. Phân phối thay đổi đáng kể qua slot. Đối với một vài slot, 20-25 token hàng đầu chiếm 90% trọng số slot, trong khi đối với slot khác, phân phối đồng đều hơn và nhiều token đóng góp để lấp đầy slot. Nói chung, chúng tôi thấy rằng slot có xu hướng trộn một số lượng lớn token không giống như trong Sparse MoE tiêu chuẩn.

Kiểm tra trực quan. Để cung cấp một số trực giác về cách slot lấy trung bình token đầu vào, Hình 25 cho thấy đồ họa các tổ hợp tuyến tính cho 8 slot khác nhau cho hình ảnh được hiển thị trong Hình 1. Chúng tôi tô bóng patch tỷ lệ nghịch với trọng số của chúng trong slot; lưu ý rằng tất cả biểu diễn token cuối cùng được kết hợp thành một (với chiều ẩn h) trước khi được chuyển đến chuyên gia (không giống như trong biểu đồ của chúng tôi, nơi chúng được sắp xếp theo cách thông thường). Những biểu đồ này tương ứng với Soft MoE S/16 với 128 chuyên gia và một slot mỗi chuyên gia, và chúng tôi chọn 8 trong số 128 slot để làm nổi bật cách các slot khác nhau có xu hướng tập trung vào các phần tử khác nhau của hình ảnh.

0.25 1 2 3 4 5
per-token sum of its dispatch weights0.050.100.150.200.300.400.500.600.700.800.850.900.951.00cumulative percentage of tokensMoE Layer 6
MoE Layer 7
MoE Layer 8
MoE Layer 9
MoE Layer 10
MoE Layer 11
0 20 40 60 80 100 120
(sorted) expert id12468101214(normalized) average combine weight across tokensMoE Layer 6
MoE Layer 7
MoE Layer 8
MoE Layer 9
MoE Layer 10
MoE Layer 11
0 25 50 75 100 125 150 175 200
top-k tokens for slot s0.00.10.20.30.40.50.60.70.80.91.0cumulative weight for top-k tokens

Hình 24: (Trái) Phân phối trọng số dispatch tổng mỗi token cho các lớp MoE khác nhau. Ví dụ, trong lớp 11, trọng số dispatch cho 90-95% token đầu vào tổng trên tất cả slot nhiều nhất là 1. Chỉ một phần nhỏ token đóng góp vào slot bằng cách tổng hơn 3. (Giữa) Phân phối trọng số combine mỗi slot (hoặc chuyên gia, vì chúng tôi sử dụng một slot mỗi chuyên gia) tổng qua tất cả token đầu vào. Chúng tôi chuẩn hóa tổng bằng giá trị tối thiểu của nó qua chuyên gia. (Phải) Mỗi đường cong tương ứng với một slot. Trọng số dispatch từ tất cả token đến mỗi slot cộng lại bằng 1. Phân phối về số lượng token đầu vào cần thiết để đạt một phần nhất định của tổng trọng số cho slot.

--- TRANG 25 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Slot / Expert 1
Slot / Expert 2
Slot / Expert 3
Slot / Expert 4
Slot / Expert 5
Slot / Expert 6
Slot / Expert 7
Slot / Expert 8

Hình 25: Tổ hợp tuyến tính cho 8 slot khi sử dụng hình ảnh đầu vào trong Hình 1. Mô hình là Soft MoE S/16 với 128 chuyên gia và một slot mỗi chuyên gia, và nó được finetuning trên ImageNet. Chúng tôi cho thấy kết quả cho lớp MoE đầu tiên (khối thứ bảy). Các slot được chọn (trong số 128) được chọn kỹ để làm nổi bật sự khác biệt qua slot.

H PHÂN TÍCH BỔ SUNG
H.1 TỔNG TÍCH LŨY CỦA TRỌNG SỐ DISPATCH VÀ COMBINE
Hình 26 cho thấy phân phối trên slot của tổng tích lũy (trên token) của trọng số dispatch tương ứng của chúng. Đối với mỗi slot, chúng tôi tính tổng tích lũy của trọng số dispatch trên token được sắp xếp theo thứ tự giảm dần. Điều này chỉ ra bao nhiêu token cần thiết để bao phủ một tỷ lệ phần trăm nhất định của tổng khối lượng của trung bình trọng số. Chúng tôi tính tổng tích lũy này cho tất cả slot trên tất cả 50.000 hình ảnh validation ImageNet, qua tất cả lớp của mô hình Soft MoE H/16 sau finetuning.

Trong biểu đồ, chúng tôi biểu diễn bằng đường liền nét tổng tích lũy trung bình (trên tất cả slot và hình ảnh), và các khu vực màu khác nhau đại diện cho 60%, 80%, 90%, 95% và 99% trung tâm của phân phối (từ màu tối hơn đến sáng hơn) của tổng tích lũy.

Điều này cho chúng tôi biết, chẳng hạn, trung bình trọng số trên token được sử dụng để tính mỗi slot đầu vào đồng đều như thế nào. Cụ thể, mỗi slot trong hai lớp cuối gần với trung bình đồng đều của tất cả token (trung bình hoàn toàn đồng đều sẽ được biểu diễn bằng đường thẳng). Điều này cho chúng tôi biết rằng trong những lớp này, mỗi chuyên gia xử lý gần như cùng đầu vào, ít nhất là sau khi mô hình được huấn luyện. Tuy nhiên, trung bình trọng số này khác xa đồng đều trong phần còn lại của các lớp, có nghĩa là có token đóng góp nhiều hơn các token khác. Ví dụ, trong lớp 28, vài chục token đã bao phủ 80% khối lượng trung bình trọng số. Cuối cùng, với độ rộng của các khu vực màu, chúng tôi cũng có thể thấy rằng có sự khác biệt đáng kể về trung bình trọng số tùy thuộc vào slot, qua tất cả lớp (ngoại trừ có thể hai lớp cuối). Điều này chỉ ra rằng trọng số dispatch thay đổi qua slot và hình ảnh khác nhau.

Tương tự, Hình 27 cho thấy các biểu đồ tương ứng cho tổng tích lũy của trọng số combine. Trong trường hợp này, đối với mỗi token đầu ra, chúng tôi tính tổng tích lũy của trọng số combine trên slot được sắp xếp theo thứ tự giảm dần. Lưu ý rằng, mặc dù trọng số dispatch trong hai lớp cuối gần như đồng đều, trọng số combine thì không. Điều này chỉ ra rằng một số slot (và do đó, chuyên gia) quan trọng hơn các slot khác trong việc tính toán token đầu ra, và do đó tham số chuyên gia tương ứng của chúng không dư thừa. Tất nhiên, danh tính của các slot "quan trọng" có thể thay đổi tùy thuộc vào token đầu vào.

--- TRANG 26 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
020406080100Cumulative dispatch weights (%)Block 16 Block 17 Block 18 Block 19
020406080100Cumulative dispatch weights (%)Block 20 Block 21 Block 22 Block 23
020406080100Cumulative dispatch weights (%)Block 24 Block 25 Block 26 Block 27
0 250 500 750
Number of tokens020406080100Cumulative dispatch weights (%)Block 28
0 250 500 750
Number of tokensBlock 29
0 250 500 750
Number of tokensBlock 30
0 250 500 750
Number of tokensBlock 31

Hình 26: Phân phối tổng tích lũy của trọng số dispatch. Đối với mỗi slot đầu vào, chúng tôi tính tổng tích lũy của trọng số dispatch tương ứng (được sắp xếp theo giá trị giảm dần). Điều này chỉ ra trên bao nhiêu token đầu vào một trọng số tích lũy nhất định được phân phối. Đường trong mỗi biểu đồ biểu diễn trung bình được tính trên tất cả slot và hình ảnh validation ImageNet của khối đã cho trong mô hình SoftMoE H/14. Các khu vực màu biểu diễn 60%, 80%, 90%, 95% và 99% trung tâm của phân phối (từ tối hơn đến sáng hơn, thấy rõ hơn khi có màu).

--- TRANG 27 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
020406080100Cumulative combine weights (%)Block 16 Block 17 Block 18 Block 19
020406080100Cumulative combine weights (%)Block 20 Block 21 Block 22 Block 23
020406080100Cumulative combine weights (%)Block 24 Block 25 Block 26 Block 27
0 100 200
Number of slots020406080100Cumulative combine weights (%)Block 28
0 100 200
Number of slotsBlock 29
0 100 200
Number of slotsBlock 30
0 100 200
Number of slotsBlock 31

Hình 27: Phân phối tổng tích lũy của trọng số combine. Đối với mỗi token đầu ra, chúng tôi tính tổng tích lũy của trọng số combine tương ứng (được sắp xếp theo giá trị giảm dần). Điều này chỉ ra trên bao nhiêu slot đầu ra một trọng số tích lũy nhất định được phân phối. Đường trong mỗi biểu đồ biểu diễn trung bình được tính trên tất cả token và hình ảnh validation ImageNet của khối đã cho trong mô hình SoftMoE H/14. Các khu vực màu biểu diễn 60%, 80%, 90%, 95% và 99% trung tâm của phân phối (từ tối hơn đến sáng hơn, thấy rõ hơn khi có màu).

--- TRANG 28 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
I TƯƠNG QUAN SLOT
Trong phần này, chúng tôi khám phá tương quan giữa các tham số slot khác nhau mà Soft MoE học được, và mối quan hệ của nó với số lượng slot mỗi chuyên gia. Hình 28 đến 30 cho thấy đối với mỗi trong 6 lớp trong Soft MoE S/16 tích vô hướng giữa mỗi cặp vector tham số slot (chuẩn hóa).

Trong khi Hình 28 không cho thấy mối quan hệ rõ ràng giữa slot từ các chuyên gia khác nhau (vì mỗi chuyên gia chỉ có một slot), chúng tôi quan sát trong Hình 29 và 30 cách các slot liên tiếp (tương ứng với cùng một chuyên gia) căn chỉnh cực kỳ chặt chẽ. Điều này xác nhận giả thuyết của chúng tôi rằng thêm nhiều slot cho chuyên gia không hoạt động tốt vì những slot này kết thúc bằng việc căn chỉnh giá trị của chúng, và tính toán các tổ hợp tuyến tính hơi tương tự. Do đó, những phép chiếu này không thêm quá nhiều thông tin hữu ích cho các token khác nhau được xử lý bởi chuyên gia (trong trường hợp cực đoan, những slot này sẽ giống hệt nhau).

14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 6
0.2
0.00.20.40.60.81.0
14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 7
0.00.20.40.60.81.0
14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 8
0.00.20.40.60.81.0
14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 9
0.2
0.00.20.40.60.81.0
14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 10
0.2
0.00.20.40.60.81.0
14 8 12 16 20 24 28 32
slots1
4
8
12
16
20
24
28
32slotsSoft MoE Layer 11
0.2
0.00.20.40.60.81.0

Hình 28: Soft MoE S/16 với 1 slot mỗi chuyên gia.

0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 6
0.4
0.2
0.00.20.40.60.81.0
0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 7
0.2
0.00.20.40.60.81.0
0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 8
0.2
0.00.20.40.60.81.0
0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 9
0.2
0.00.20.40.60.81.0
0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 10
0.2
0.00.20.40.60.81.0
0 20 40 60 80 100 120
slots0
20
40
60
80
100
120slotsSoft MoE Layer 11
0.2
0.00.20.40.60.81.0

Hình 29: Soft MoE S/16 với 4 slot mỗi chuyên gia.

--- TRANG 29 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 6
0.4
0.2
0.00.20.40.60.81.0
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 7
0.4
0.2
0.00.20.40.60.81.0
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 8
0.4
0.2
0.00.20.40.60.81.0
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 9
0.2
0.00.20.40.60.81.0
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 10
0.2
0.00.20.40.60.81.0
0 100 200 300 400 500
Slots0
100
200
300
400
500SlotsSoft MoE Layer 11
0.2
0.00.20.40.60.81.0

Hình 30: Soft MoE S/16 với 16 slot mỗi chuyên gia.

--- TRANG 30 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
J MÔ HÌNH PARETO
Bảng 9: Chạy mô hình từ Phần 3.3 (hiển thị trong biểu đồ Pareto) được huấn luyện trong 300k bước trên JFT với giảm căn bậc hai nghịch đảo và 50k bước cooldown. Chúng tôi huấn luyện mô hình dày đặc và MoE (Soft MoE, Tokens Choice, Experts Choice) với kích thước S/32, S/16, S/8, B/32, B/16, L/32, L/16 và H/14. Được sắp xếp theo TPUv3 ngày huấn luyện tăng dần.

Ref Model Routing Experts Group Size K C JFT P@1 IN/10shot Train exaFLOP Train Days
1 S/32 Dense – – – – 42.8 51.0 4.2 6.3
2 S/32 Experts Choice 32 392 – 0.5 45.5 56.4 3.7 6.9
3 S/32 Soft MoE 32 49 – – 48.2 62.3 3.8 7.0
4 S/32 Experts Choice 32 49 – 0.5 44.3 54.8 3.8 7.0
5 S/32 Experts Choice 32 392 – 1.0 46.6 58.2 4.4 7.6
6 S/32 Experts Choice 64 392 – 1.0 46.7 58.4 4.4 7.8
7 S/32 Soft MoE 64 49 – – 49.4 65.1 4.6 7.8
8 S/32 Experts Choice 64 49 – 1.0 45.4 56.3 4.6 7.9
9 S/32 Experts Choice 32 49 – 1.0 45.5 56.9 4.6 7.9
10 S/32 Tokens Choice 32 392 1 1.0 46.3 58.7 4.4 8.0
11 S/32 Tokens Choice 64 392 1 1.0 47.1 59.4 4.4 8.1
12 S/32 Tokens Choice 32 392 2 1.0 47.4 60.3 6.0 9.3
13 S/32 Tokens Choice 64 392 2 1.0 48.0 61.9 6.0 9.8
14 B/32 Dense – – – – 48.0 63.4 16.0 11.7
15 B/32 Soft MoE 32 49 – – 50.9 69.7 14.3 11.7
16 S/32 Tokens Choice 64 392 2 2.0 48.9 63.9 9.0 12.0
17 S/32 Tokens Choice 32 392 2 2.0 48.5 62.8 9.1 12.1
18 S/16 Soft MoE 16 196 – – 50.9 68.1 12.4 14.2
19 S/16 Soft MoE 32 196 – – 52.0 70.8 12.9 14.8
20 S/16 Dense – – – – 47.9 60.8 17.0 15.3
21 S/16 Soft MoE 64 196 – – 52.9 72.3 13.9 15.7
22 S/16 Soft MoE 128 196 – – 53.6 73.0 15.9 17.6
23 B/32 Experts Choice 32 392 – 0.5 49.0 64.3 13.7 18.0
24 B/32 Experts Choice 32 49 – 0.5 47.8 62.4 14.3 18.2
25 S/16 Experts Choice 128 196 – 0.5 50.2 66.7 15.8 18.5
26 S/16 Experts Choice 32 196 – 1.0 50.5 67.4 17.5 18.8
27 S/16 Experts Choice 128 1568 – 0.5 51.4 67.7 16.8 19.7
28 B/32 Experts Choice 32 392 – 1.0 49.9 66.0 16.5 19.7
29 B/32 Experts Choice 64 392 – 1.0 49.9 65.5 16.5 19.8
30 B/32 Tokens Choice 32 392 1 1.0 49.7 66.2 16.5 20.0
31 B/32 Tokens Choice 64 392 1 1.0 49.8 65.6 16.5 20.2
32 B/32 Soft MoE 64 49 – – 51.8 70.7 17.8 20.3
33 B/32 Experts Choice 64 49 – 1.0 48.6 64.0 17.7 20.3
34 B/32 Experts Choice 32 49 – 1.0 48.4 63.8 17.7 20.5
35 S/16 Experts Choice 32 1568 – 1.0 51.3 68.7 21.5 21.5
36 S/16 Soft MoE 256 196 – – 53.8 73.7 19.9 22.1
37 S/16 Tokens Choice 32 1568 1 1.0 51.2 68.9 21.5 23.2
38 S/16 Experts Choice 256 196 – 1.0 50.7 67.7 19.8 23.3
39 S/16 Experts Choice 32 196 – 2.0 51.0 68.3 23.1 23.5
40 B/32 Tokens Choice 32 392 2 1.0 50.2 67.4 22.0 23.6
41 B/32 Tokens Choice 64 392 2 1.0 50.8 68.0 22.1 23.8
42 S/16 Tokens Choice 64 1568 1 1.0 51.5 69.1 21.3 24.9
43 S/16 Experts Choice 256 1568 – 1.0 52.3 69.7 21.7 25.5
44 S/16 Experts Choice 32 1568 – 2.0 52.4 70.3 31.0 27.8
45 S/16 Tokens Choice 32 1568 2 1.0 52.1 70.3 31.0 30.0
46 B/32 Tokens Choice 64 392 2 2.0 51.2 70.0 33.2 30.4
47 B/32 Tokens Choice 32 392 2 2.0 51.0 69.5 33.6 31.1
48 S/16 Tokens Choice 64 1568 2 1.0 52.3 70.4 31.1 32.0
49 S/16 Tokens Choice 32 1568 2 2.0 52.8 71.4 50.0 42.5

--- TRANG 31 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 9: Chạy mô hình từ Phần 3.3 (hiển thị trong biểu đồ Pareto) được huấn luyện trong 300k bước trên JFT với giảm căn bậc hai nghịch đảo và 50k bước cooldown. Chúng tôi huấn luyện mô hình dày đặc và MoE (Soft MoE, Tokens Choice, Experts Choice) với kích thước S/32, S/16, S/8, B/32, B/16, L/32, L/16 và H/14. Được sắp xếp theo TPUv3 ngày huấn luyện tăng dần.

Ref Model Routing Experts Group Size K C JFT P@1 IN/10shot Train exaFLOP Train Days
50 S/16 Tokens Choice 64 1568 2 2.0 52.9 71.4 50.1 45.1
51 B/16 Dense – – – – 52.0 71.8 64.8 45.2
52 B/16 Soft MoE 128 196 – – 55.3 77.0 59.0 46.8
53 B/16 Experts Choice 128 1568 – 0.5 53.7 73.0 59.0 48.2
54 B/16 Experts Choice 32 196 – 1.0 53.3 73.0 65.6 51.0
55 B/16 Experts Choice 128 196 – 0.5 52.5 72.2 58.8 52.6
56 L/32 Dense – – – – 51.3 70.9 55.9 54.9
57 L/32 Experts Choice 32 392 – 0.5 52.3 71.2 47.4 55.2
58 L/32 Experts Choice 32 49 – 0.5 51.1 70.6 49.8 55.7
59 L/32 Soft MoE 32 49 – – 53.5 75.0 49.8 56.0
60 B/16 Experts Choice 32 1568 – 1.0 54.2 74.5 73.6 56.2
61 B/16 Tokens Choice 32 1568 1 1.0 53.7 74.4 73.6 57.8
62 B/16 Experts Choice 256 196 – 1.0 52.7 72.7 73.4 58.1
63 B/16 Soft MoE 256 196 – – 55.8 78.0 73.7 58.2
64 B/16 Tokens Choice 64 1568 1 1.0 54.0 74.8 73.2 58.7
65 L/32 Experts Choice 64 392 – 1.0 52.7 72.1 56.9 60.4
66 B/16 Experts Choice 256 1568 – 1.0 53.9 73.5 73.8 60.5
67 L/32 Experts Choice 32 392 – 1.0 52.7 71.7 56.8 60.6
68 L/32 Tokens Choice 64 392 1 1.0 51.9 71.4 56.9 61.0
69 L/32 Tokens Choice 32 392 1 1.0 52.3 71.7 57.1 61.6
70 L/32 Experts Choice 64 49 – 1.0 51.1 70.7 61.6 62.6
71 L/32 Soft MoE 64 49 – – 54.0 75.2 61.7 62.8
72 L/32 Experts Choice 32 49 – 1.0 51.4 70.3 61.5 63.2
73 B/16 Experts Choice 32 196 – 2.0 53.1 73.9 86.8 64.2
74 L/32 Tokens Choice 32 392 2 1.0 51.5 70.7 76.0 72.2
75 B/16 Experts Choice 32 1568 – 2.0 54.6 75.6 102.9 72.5
76 L/32 Tokens Choice 64 392 2 1.0 52.0 71.8 76.0 72.5
77 B/16 Tokens Choice 32 1568 2 1.0 53.9 74.7 102.9 74.7
78 B/16 Soft MoE 512 196 – – 56.1 78.5 103.1 76.5
79 B/16 Tokens Choice 64 1568 2 1.0 54.3 74.8 103.0 76.5
80 S/8 Dense – – – – 49.9 66.7 82.7 77.7
81 S/8 Soft MoE 512 784 – – 56.1 78.0 85.6 88.5
82 S/8 Experts Choice 32 784 – 1.0 52.9 72.6 91.3 93.0
83 L/32 Tokens Choice 64 392 2 2.0 52.9 72.9 114.3 93.2
84 L/32 Tokens Choice 32 392 2 2.0 52.5 72.5 115.7 95.8
85 L/16 Dense – – – – 54.8 77.8 226.9 100.9
86 L/16 Experts Choice 128 196 – 0.5 54.0 76.7 204.6 104.9
87 L/16 Soft MoE 128 196 – – 57.2 80.3 205.0 106.0
88 B/16 Tokens Choice 32 1568 2 2.0 54.4 75.7 161.4 108.4
89 B/16 Tokens Choice 64 1568 2 2.0 54.8 76.0 161.5 110.5
90 L/16 Experts Choice 32 196 – 1.0 55.1 77.5 228.6 113.6
91 L/16 Tokens Choice 32 1568 1 1.0 55.9 78.5 250.4 125.1
92 L/16 Tokens Choice 64 1568 1 1.0 56.2 78.6 248.8 125.7
93 S/8 Experts Choice 32 6272 – 1.0 53.6 73.4 160.6 126.6
94 S/8 Experts Choice 512 784 – 1.0 53.4 72.4 104.1 129.0
95 L/16 Soft MoE 256 196 – – 57.4 80.2 256.0 129.6
96 S/8 Tokens Choice 32 6272 1 1.0 53.8 73.7 162.5 129.8
97 L/16 Experts Choice 256 196 – 1.0 54.1 76.7 255.2 130.1
98 L/16 Experts Choice 32 196 – 2.0 55.2 77.8 301.0 140.3
99 S/8 Experts Choice 512 6272 – 1.0 54.8 74.6 149.3 161.9
100 S/8 Tokens Choice 32 6272 2 1.0 54.2 74.6 243.4 166.6
101 H/14 Soft MoE 128 256 – – 58.0 81.6 599.2 170.5

--- TRANG 32 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 9: Chạy mô hình từ Phần 3.3 (hiển thị trong biểu đồ Pareto) được huấn luyện trong 300k bước trên JFT với giảm căn bậc hai nghịch đảo và 50k bước cooldown. Chúng tôi huấn luyện mô hình dày đặc và MoE (Soft MoE, Tokens Choice, Experts Choice) với kích thước S/32, S/16, S/8, B/32, B/16, L/32, L/16 và H/14. Được sắp xếp theo TPUv3 ngày huấn luyện tăng dần.

Ref Model Routing Experts Group Size K C JFT P@1 IN/10shot Train exaFLOP Train Days
102 H/14 Dense – – – – 56.5 80.1 680.5 196.2
103 H/14 Experts Choice 64 2048 – 1.25 57.3 80.4 855.9 210.9
104 L/16 Tokens Choice 32 1568 2 2.0 53.5 74.6 534.5 218.5
105 L/16 Tokens Choice 64 1568 2 2.0 53.3 73.3 535.1 226.9
106 H/14 Tokens Choice 64 2048 1 1.25 56.7 79.8 857.0 230.7
107 S/8 Tokens Choice 32 6272 2 2.0 54.1 74.8 424.4 255.4

--- TRANG 33 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
>>>>>>> Stashed changes
