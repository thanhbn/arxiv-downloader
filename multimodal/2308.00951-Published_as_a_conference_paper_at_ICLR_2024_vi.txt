TỪ HỖN HỢP CHUYÊN GIA THƯA THỚT ĐẾN MỀM MƯỢT

Các kiến trúc hỗn hợp chuyên gia thưa thớt (MoEs) mở rộng quy mô khả năng mô hình mà không cần tăng đáng kể chi phí huấn luyện hoặc suy luận. Mặc dù thành công, MoEs gặp phải một số vấn đề: bất ổn định huấn luyện, bỏ qua token, không thể mở rộng quy mô số chuyên gia, hoặc tinh chỉnh không hiệu quả. Trong nghiên cứu này, chúng tôi đề xuất Soft MoE, một Transformer thưa thớt hoàn toàn khả vi giải quyết những thách thức này, đồng thời duy trì lợi ích của MoEs. Soft MoE thực hiện phân bổ mềm ngầm định bằng cách truyền các kết hợp có trọng số khác nhau của tất cả token đầu vào đến từng chuyên gia. Như trong các MoE khác, các chuyên gia trong Soft MoE chỉ xử lý một tập con của các token (kết hợp), cho phép khả năng mô hình lớn hơn (và hiệu suất) với chi phí suy luận thấp hơn. Trong bối cảnh nhận dạng thị giác, Soft MoE vượt trội đáng kể so với Transformers dày đặc (ViTs) và các MoEs phổ biến (Tokens Choice và Experts Choice). Hơn nữa, Soft MoE mở rộng quy mô tốt: Soft MoE Huge/14 với 128 chuyên gia trong 16 lớp MoE có hơn 40× số tham số nhiều hơn ViT Huge/14, chỉ tăng 2% thời gian suy luận, và chất lượng tốt hơn đáng kể.

1 GIỚI THIỆU

Các Transformer lớn hơn cải thiện hiệu suất với chi phí tính toán tăng lên. Các nghiên cứu gần đây cho thấy rằng kích thước mô hình và dữ liệu huấn luyện phải được mở rộng cùng nhau để sử dụng tối ưu bất kỳ ngân sách tính toán huấn luyện nào (Kaplan et al., 2020; Hoffmann et al., 2022; Zhai et al., 2022a). Một giải pháp thay thế đầy hứa hẹn cho phép mở rộng quy mô mô hình về kích thước mà không phải trả toàn bộ chi phí tính toán của chúng là hỗn hợp chuyên gia thưa thớt (MoEs). Gần đây, một số phương pháp thành công đã đề xuất cách kích hoạt thưa thớt đường dẫn token qua mạng trong ngôn ngữ (Lepikhin et al., 2020; Fedus et al., 2022), thị giác (Riquelme et al., 2021), và các mô hình đa phương thức (Mustafa et al., 2022).

Các Transformer MoE thưa thớt liên quan đến bài toán tối ưu rời rạc để quyết định module nào nên được áp dụng cho mỗi token. Các module này thường được gọi là chuyên gia và thường là MLPs. Nhiều kỹ thuật đã được phát triển để tìm sự khớp tốt giữa token và chuyên gia: chương trình tuyến tính (Lewis et al., 2021), học tăng cường (Bengio et al., 2015), quy tắc cố định xác định (Roller et al., 2021), vận chuyển tối ưu (Liu et al., 2022), top-k chuyên gia tham lam mỗi token (Shazeer et al., 2017), hoặc top-k token tham lam mỗi chuyên gia (Zhou et al., 2022). Thường cần các hàm mất mát phụ trợ thể thức để cân bằng việc sử dụng chuyên gia và giảm thiểu token chưa được phân bổ. Những thách thức này có thể lớn hơn trong cài đặt ngoài phân phối: kích thước batch suy luận nhỏ, đầu vào mới lạ, hoặc trong học chuyển giao.

Chúng tôi giới thiệu Soft MoE, vượt qua nhiều thách thức này. Thay vì sử dụng bộ định tuyến thưa thớt và rời rạc cố gắng tìm phân bổ cứng tốt giữa token và chuyên gia, Soft MoEs thay vào đó thực hiện phân bổ mềm bằng cách trộn token. Cụ thể, chúng tôi tính toán nhiều trung bình có trọng số của tất cả token—với trọng số phụ thuộc vào cả token và chuyên gia—và sau đó chúng tôi xử lý mỗi trung bình có trọng số bằng chuyên gia tương ứng.

Soft MoE L/16 vượt trội hơn ViT H/14 về upstream, few-shot và fine-tuning trong khi yêu cầu gần như một nửa thời gian huấn luyện, và nhanh hơn 2× khi suy luận. Hơn nữa, Soft MoE B/16 có hiệu suất tương đương ViT H/14 về few-shot và fine-tuning và vượt trội về các chỉ số upstream sau lượng huấn luyện tương đương. Đáng chú ý, Soft MoE B/16 nhanh hơn 5.7× khi suy luận mặc dù có 5.5× số tham số của ViT H/14 (xem Bảng 1 và Hình 5 để biết chi tiết). Phần 4 chứng minh tiềm năng của Soft MoE để mở rộng sang các tác vụ khác: chúng tôi huấn luyện một tháp văn bản mô hình tương phản với tháp thị giác cố định, cho thấy rằng các biểu diễn học được thông qua định tuyến mềm bảo tồn lợi ích của chúng cho việc căn chỉnh hình ảnh-văn bản.

2 HỖN HỢP CHUYÊN GIA MỀM

2.1 MÔ TẢ THUẬT TOÁN

Thuật toán định tuyến Soft MoE được mô tả trong Hình 2. Chúng tôi ký hiệu các token đầu vào cho một chuỗi bằng X∈Rm×d, trong đó m là số token và d là chiều của chúng. Mỗi lớp MoE sử dụng một tập n hàm chuyên gia được áp dụng trên từng token, cụ thể là {fi:Rd→Rd}1:n.

Mỗi chuyên gia xử lý p slot, và mỗi slot có một vector tham số d chiều tương ứng, Φ∈Rd×(n·p).

Cụ thể, các slot đầu vào ˜X∈R(n·p)×d là kết quả của tổ hợp lồi của tất cả m token đầu vào, X:

Dij=exp((XΦ)ij)∑m i′=1exp((XΦ)i′j), ˜X=D⊤X. (1)

Lưu ý rằng D, mà chúng tôi gọi là trọng số dispatch, đơn giản là kết quả của việc áp dụng softmax trên các cột của XΦ. Sau đó, như đã đề cập ở trên, hàm chuyên gia tương ứng được áp dụng trên mỗi slot (tức là trên các hàng của ˜X) để thu được các slot đầu ra: ˜Yi=f⌊i/p⌋(˜Xi).

Cuối cùng, các token đầu ra Y được tính toán như một tổ hợp lồi của tất cả (n·p) slot đầu ra, ˜Y, có trọng số được tính toán tương tự như trước:

Cij=exp((XΦ)ij)∑n·p j′=1exp((XΦ)ij′),Y=C˜Y. (2)

Chúng tôi gọi C là trọng số combine, và nó là kết quả của việc áp dụng softmax trên các hàng của XΦ.

Theo thiết kế thông thường cho Sparse MoEs, chúng tôi thay thế một tập con các khối MLP của Transformer bằng các khối Soft MoE. Chúng tôi thường thay thế nửa sau của các khối MLP. Tổng số slot là một siêu tham số chính của các lớp Soft MoE vì độ phức tạp thời gian phụ thuộc vào số slot chứ không phải số chuyên gia. Có thể đặt số slot bằng độ dài chuỗi đầu vào để khớp với FLOPs của Transformer dày đặc tương đương.

2.2 TÍNH CHẤT CỦA SOFT MOE VÀ KẾT NỐI VỚI SPARSE MOES

Các thuật toán Sparse MoE hoàn toàn khả vi liên quan đến bài toán phân bổ giữa token và chuyên gia, chịu ràng buộc về dung lượng và cân bằng tải. Các thuật toán khác nhau xấp xỉ giải pháp theo những cách khác nhau: ví dụ, bộ định tuyến top-k hoặc "Token Choice" (Shazeer et al., 2017; Lepikhin et al., 2020; Riquelme et al., 2021) chọn k chuyên gia được điểm cao nhất cho mỗi token, trong khi có slot khả dụng trong chuyên gia đó (tức là chuyên gia chưa đầy dung lượng). Bộ định tuyến "Expert Choice" (Zhou et al., 2022) chọn các token được điểm capacity-cao nhất cho mỗi chuyên gia. Các nghiên cứu khác đề xuất thuật toán tiên tiến hơn (và thường tốn kém hơn) để tính toán phân bổ, như phương pháp dựa trên thuật toán Lập trình Tuyến tính (Lewis et al., 2021), Vận chuyển Tối ưu (Liu et al., 2022; Clark et al., 2022) hoặc Học Tăng cường (Clark et al., 2022). Tuy nhiên, hầu như tất cả các phương pháp này đều có tính chất rời rạc và do đó không khả vi. Ngược lại, tất cả các phép toán trong lớp Soft MoE đều liên tục và hoàn toàn khả vi. Chúng ta có thể hiểu các trung bình có trọng số với điểm softmax như phân bổ mềm, thay vì phân bổ cứng được sử dụng trong Sparse MoE.

Không bỏ token và mất cân bằng chuyên gia Các cơ chế định tuyến cổ điển có xu hướng gặp phải các vấn đề như "bỏ token" (tức là một số token không được phân bổ cho bất kỳ chuyên gia nào), hoặc "mất cân bằng chuyên gia" (tức là một số chuyên gia nhận được nhiều token hơn các chuyên gia khác). Thật không may, hiệu suất có thể bị ảnh hưởng nghiêm trọng do hậu quả. Ví dụ, bộ định tuyến top-k hoặc "Token Choice" phổ biến (Shazeer et al., 2017) gặp phải cả hai vấn đề, trong khi bộ định tuyến "Expert Choice" (Zhou et al., 2022) chỉ gặp phải vấn đề trước (xem Phụ lục B để biết một số thí nghiệm về việc bỏ). Soft MoEs miễn nhiễm với việc bỏ token và mất cân bằng chuyên gia vì mọi slot đều được điền bằng trung bình có trọng số của tất cả token.

Nhanh Tổng số slot quyết định chi phí của lớp Soft MoE. Mọi đầu vào áp dụng số MLP đó. Tổng số chuyên gia không liên quan trong tính toán này: ít chuyên gia với nhiều slot mỗi chuyên gia hoặc nhiều chuyên gia với ít slot mỗi chuyên gia sẽ có chi phí khớp nhau nếu tổng số slot giống nhau. Ràng buộc duy nhất chúng ta phải đáp ứng là số slot phải lớn hơn hoặc bằng số chuyên gia (vì mỗi chuyên gia phải xử lý ít nhất một slot). Lợi thế chính của Soft MoE là hoàn toàn tránh được các phép toán sort hoặc top-k chậm và thường không phù hợp với bộ tăng tốc phần cứng. Kết quả là, Soft MoE nhanh hơn đáng kể so với hầu hết sparse MoEs (Hình 6). Xem Phần 2.3 để biết chi tiết về độ phức tạp thời gian.

Đặc điểm của cả sparse và dense Sự thưa thớt trong Sparse MoEs đến từ việc tham số chuyên gia chỉ được áp dụng cho một tập con của các token đầu vào. Tuy nhiên, Soft MoEs về mặt kỹ thuật không thưa thớt, vì mọi slot đều là trung bình có trọng số của tất cả token đầu vào. Mọi token đầu vào kích hoạt một phần tất cả tham số mô hình. Tương tự, tất cả token đầu ra đều phụ thuộc một phần vào tất cả slot (và chuyên gia). Cuối cùng, lưu ý rằng Soft MoEs cũng không phải Dense MoEs, nơi mọi chuyên gia xử lý tất cả token đầu vào, vì mọi chuyên gia chỉ xử lý một tập con của các slot.

Tính xác định theo chuỗi Dưới ràng buộc dung lượng, tất cả phương pháp Sparse MoE định tuyến token theo nhóm có kích thước cố định và thực thi (hoặc khuyến khích) cân bằng trong nhóm. Khi nhóm chứa token từ các chuỗi hoặc đầu vào khác nhau, những token này cạnh tranh cho các chỗ có sẵn trong bộ đệm chuyên gia. Do đó, mô hình không còn xác định ở cấp chuỗi, mà chỉ ở cấp batch. Các mô hình sử dụng nhóm lớn hơn có xu hướng cung cấp nhiều tự do hơn cho thuật toán định tuyến và thường hoạt động tốt hơn, nhưng chi phí tính toán của chúng cũng cao hơn.

2.3 TRIỂN KHAI

Độ phức tạp thời gian Giả sử chi phí mỗi token của một hàm chuyên gia đơn là O(k). Độ phức tạp thời gian của lớp Soft MoE sau đó là O(mnpd + npk). Bằng cách chọn p=O(m/n) slot mỗi chuyên gia, tức là số token chia cho số chuyên gia, chi phí giảm xuống O(m²d + mk). Cho rằng mỗi hàm chuyên gia có bộ tham số riêng, việc tăng số chuyên gia n và điều chỉnh p tương ứng, cho phép chúng ta tăng tổng số tham số mà không ảnh hưởng đến độ phức tạp thời gian. Hơn nữa, khi chi phí áp dụng chuyên gia lớn, số hạng mk chiếm ưu thế so với m²d, và chi phí tổng thể của lớp Soft MoE trở nên tương đương với việc áp dụng một chuyên gia đơn trên tất cả token đầu vào. Cuối cùng, ngay cả khi m²d không chiếm ưu thế, đây cũng giống như chi phí self-attention (đơn đầu), do đó nó không trở thành nút thắt cổ chai trong các mô hình Transformer. Điều này có thể thấy trong biểu đồ dưới của Hình 6 nơi thông lượng của Soft MoE hầu như không thay đổi khi số chuyên gia tăng từ 8 đến 4096 chuyên gia, trong khi Sparse MoEs bị ảnh hưởng đáng kể.

Chuẩn hóa Trong Transformers, các lớp MoE thường được sử dụng để thay thế lớp feedforward trong mỗi khối encoder. Do đó, khi sử dụng pre-normalization như hầu hết kiến trúc Transformer hiện đại (Domhan, 2018; Xiong et al., 2020; Riquelme et al., 2021; Fedus et al., 2022), đầu vào của lớp MoE được "layer normalized". Điều này gây ra vấn đề ổn định khi mở rộng chiều mô hình d, vì softmax tiến đến vector one-hot khi d→∞ (xem Phụ lục E). Do đó, trong Dòng 3 của thuật toán 1, chúng tôi thay thế X và Phi bằng l2_normalize(X, axis=1) và scale * l2_normalize(Phi, axis=0), tương ứng; trong đó scale là một số vô hướng có thể huấn luyện, và l2_normalize chuẩn hóa trục tương ứng để có chuẩn (L2) đơn vị, như Thuật toán 2 cho thấy.

Đối với các giá trị d tương đối nhỏ, chuẩn hóa có ít tác động đến chất lượng mô hình. Tuy nhiên, với chuẩn hóa đề xuất trong lớp Soft MoE, chúng ta có thể làm cho chiều mô hình lớn hơn và/hoặc tăng tốc độ học (xem Phụ lục E).

Mô hình phân tán Khi số chuyên gia tăng đáng kể, không thể vừa toàn bộ mô hình trong bộ nhớ trên một thiết bị đơn, đặc biệt trong quá trình huấn luyện hoặc khi sử dụng MoEs trên các backbone mô hình lớn. Trong những trường hợp này, chúng tôi sử dụng các kỹ thuật tiêu chuẩn để phân phối mô hình trên nhiều thiết bị, như trong (Lepikhin et al., 2020; Riquelme et al., 2021; Fedus et al., 2022) và các nghiên cứu khác huấn luyện mô hình MoE lớn. Phân phối mô hình thường thêm overhead vào chi phí của mô hình, không được phân tích độ phức tạp thời gian dựa trên FLOPs mà chúng tôi đưa ra ở trên ghi nhận. Để tính đến sự khác biệt này, trong tất cả thí nghiệm, chúng tôi đo không chỉ FLOPs mà còn thời gian thực tế bằng TPUv3-chip-hours.

3 THÍ NGHIỆM PHÂN LOẠI HÌNH ẢNH

Biên Pareto huấn luyện. Trong Phần 3.3, chúng tôi so sánh các mô hình ViT dày đặc ở kích thước Small, Base, Large và Huge với các đối tác dày đặc và thưa thớt dựa trên cả định tuyến thưa thớt Tokens Choice và Experts Choice. Chúng tôi nghiên cứu hiệu suất ở các ngân sách huấn luyện khác nhau và cho thấy Soft MoE chiếm ưu thế so với các mô hình khác về hiệu suất ở ngân sách chi phí hoặc thời gian huấn luyện nhất định.

Các mô hình tối ưu hóa thời gian suy luận. Trong Phần 3.4, chúng tôi trình bày các lần chạy huấn luyện dài hơn ("overtraining"). So với ViT, Soft MoE mang lại cải thiện lớn về tốc độ suy luận cho mức hiệu suất cố định (mô hình nhỏ hơn: S, B) và hiệu suất tuyệt đối (mô hình lớn hơn: L, H).

Ablation mô hình. Trong Phần 3.5 và 3.6, chúng tôi điều tra tác động của việc thay đổi số slot và chuyên gia, và thực hiện ablation trên thuật toán định tuyến Soft MoE.

3.1 DỮ LIỆU HUẤN LUYỆN VÀ ĐÁNH GIÁ

Chúng tôi pre-train các mô hình trên JFT-4B (Zhai et al., 2022a), một bộ dữ liệu độc quyền chứa hơn 4B hình ảnh, bao gồm 29k lớp. Trong quá trình pre-training, chúng tôi đánh giá các mô hình trên hai chỉ số: precision-at-1 validation upstream trên JFT-4B, và độ chính xác ImageNet 10-shot. Chỉ số sau được tính bằng cách đóng băng trọng số mô hình và thay thế head bằng một head mới chỉ được huấn luyện trên bộ dữ liệu chứa 10 hình ảnh mỗi lớp từ ImageNet-1k (Deng et al., 2009). Cuối cùng, chúng tôi cung cấp độ chính xác trên tập validation của ImageNet-1k sau khi fine-tune trên tập huấn luyện của ImageNet-1k (1.3 triệu hình ảnh) ở độ phân giải 384.

3.2 THUẬT TOÁN ĐỊNH TUYẾN THƯA THỚT

Tokens Choice. Mỗi token chọn K chuyên gia hàng đầu có điểm định tuyến cao nhất cho token (Shazeer et al., 2017). Tăng K thường dẫn đến hiệu suất tốt hơn với chi phí tính toán tăng. Batch Priority Routing (BPR) (Riquelme et al., 2021) cải thiện đáng kể hiệu suất mô hình, đặc biệt trong trường hợp K=1 (Phụ lục F, Bảng 7). Theo đó, chúng tôi sử dụng định tuyến Top-K với BPR và K∈{1,2}. Chúng tôi cũng tối ưu số chuyên gia (Phụ lục F, Hình 11).

Experts Choice. Ngoài ra, chuyên gia có thể chọn C token hàng đầu về điểm định tuyến (Zhou et al., 2022). C là kích thước bộ đệm, và chúng tôi đặt E·C=c·T trong đó E là số chuyên gia, T là tổng số token trong nhóm, và c là hệ số dung lượng. Khi c=1, tất cả token có thể được xử lý thông qua hợp của các chuyên gia. Với định tuyến Experts Choice, thường thấy một số token được chọn đồng thời bởi nhiều chuyên gia trong khi một số token khác hoàn toàn không được chọn. Hình 10, Phụ lục B minh họa hiện tượng này. Chúng tôi thí nghiệm với c=0.5,1,2.

3.3 HUẤN LUYỆN CÁC MÔ HÌNH PARETO-TỐI ƯU

Chúng tôi huấn luyện các mô hình ViT-{S/8, S/16, S/32, B/16, B/32, L/16, L/32, H/14} và các đối tác thưa thớt của chúng. Chúng tôi huấn luyện nhiều biến thể (thay đổi K, C và số chuyên gia), tổng cộng 106 mô hình. Chúng tôi huấn luyện trong 300k bước với kích thước batch 4096, độ phân giải 224, sử dụng lịch trình tốc độ học căn bậc hai nghịch đảo.

Hình 3a và 3b cho thấy kết quả cho các mô hình trong mỗi lớp nằm trên biên Pareto chi phí/hiệu suất huấn luyện tương ứng. Trên cả hai chỉ số, Soft MoE vượt trội mạnh mẽ so với các phương pháp dày đặc và thưa thớt khác ở bất kỳ ngân sách FLOPs hoặc thời gian nhất định.

3.4 THỜI LƯỢNG HUẤN LUYỆN DÀI

Chúng tôi huấn luyện một số mô hình trong thời gian dài hơn nhiều, lên đến 4M bước. Chúng tôi huấn luyện một số Soft MoEs trên JFT, theo cài đặt tương tự Zhai et al. (2022a). Chúng tôi thay thế nửa cuối của các khối trong ViT S/16, B/16, L/16, và H/14 bằng các lớp Soft MoE với 128 chuyên gia, sử dụng một slot mỗi chuyên gia. Chúng tôi huấn luyện các mô hình từ 1B đến 54B tham số. Tất cả mô hình được huấn luyện trong 4M bước, ngoại trừ H/14 được huấn luyện trong 2M bước vì lý do chi phí.

Hình 4 cho thấy độ chính xác JFT-4B, độ chính xác ImageNet 10-shot, và độ chính xác fine-tuning ImageNet cho Soft MoE và ViT theo chi phí huấn luyện. Phụ lục F, Bảng 8 chứa kết quả số và Hình 16 cho thấy hiệu suất theo core-hours, từ đó có thể rút ra cùng kết luận. Soft MoE vượt trội đáng kể so với các mô hình ViT dày đặc cho ngân sách tính toán nhất định. Ví dụ, Soft MoE S/16 hoạt động tốt hơn ViT B/16 trên JFT và 10-shot ImageNet, và nó cũng cải thiện điểm fine-tuning trên dữ liệu ImageNet đầy đủ, mặc dù chi phí huấn luyện (và suy luận) của nó nhỏ hơn đáng kể. Tương tự, Soft MoE B/16 vượt trội ViT L/16 upstream, và chỉ kém 0.5 sau fine-tuning trong khi nhanh hơn 3x và yêu cầu gần như ít hơn 4x FLOPs. Cuối cùng, mô hình Soft MoE L/16 vượt trội mô hình H/14 dày đặc trong khi lại nhanh hơn khoảng 3x về thời gian bước huấn luyện và suy luận.

Chúng tôi tiếp tục huấn luyện các backbone nhỏ lên đến 9M bước để có được các mô hình chất lượng cao với chi phí suy luận thấp. Ngay cả sau khi huấn luyện bổ sung (quá mức), tổng thời gian huấn luyện so với các mô hình ViT lớn hơn là tương tự hoặc nhỏ hơn. Đối với những lần chạy này, cooldown dài hơn (giảm tốc độ học tuyến tính) hoạt động tốt cho Soft MoE. Do đó, chúng tôi tăng cooldown từ 50k bước lên 500k bước.

Hình 5 và Bảng 1 trình bày kết quả. Soft MoE B/16 được huấn luyện trong 1k ngày TPUv3 có hiệu suất tương đương hoặc vượt trội ViT H/14 được huấn luyện trên ngân sách tương tự, và rẻ hơn 10× khi suy luận về FLOPs (32 vs. 334 GFLOPS/img) và >5× rẻ hơn về thời gian thực tế (1.5 vs. 8.6 ms/img). Soft MoE B/16 có hiệu suất tương đương mô hình ViT H/14 khi chúng ta tăng gấp đôi ngân sách huấn luyện của ViT-H/14 (lên 2k ngày TPU). Soft MoE L/16 vượt trội tất cả mô hình ViT trong khi gần như nhanh hơn 2× khi suy luận so với ViT H/14 (4.8 vs. 8.6 ms/img).

3.5 SỐ SLOT VÀ CHUYÊN GIA

Chúng tôi nghiên cứu tác động của việc thay đổi số slot và chuyên gia trong Sparse và Soft MoEs. Hình 6 cho thấy chất lượng và tốc độ của MoEs với số chuyên gia khác nhau, và số slot mỗi token; sau này tương đương với số chuyên gia trung bình được phân bổ mỗi token cho Sparse MoEs. Khi thay đổi số chuyên gia, FLOPs backbone của mô hình không đổi, vì vậy thay đổi tốc độ là do chi phí định tuyến. Khi thay đổi slot-per-expert, số token được xử lý trong các lớp chuyên gia tăng, vì vậy thông lượng giảm. Đầu tiên, quan sát rằng đối với Soft MoE, mô hình hoạt động tốt nhất ở mỗi số slot-per-token là mô hình có nhiều chuyên gia nhất (tức là một slot mỗi chuyên gia). Đối với hai Sparse MoEs, có một điểm mà khó khăn huấn luyện vượt qua lợi ích của dung lượng bổ sung, dẫn đến số chuyên gia tối ưu khiêm tốn. Thứ hai, thông lượng của Soft MoE gần như không đổi khi thêm nhiều chuyên gia hơn. Tuy nhiên, thông lượng của Sparse MoEs giảm mạnh từ 1k chuyên gia, xem thảo luận trong Phần 2.2.

3.6 ABLATIONS

Chúng tôi nghiên cứu tác động của các thành phần của lớp định tuyến Soft MoE bằng cách chạy các ablation sau: Định tuyến đồng nhất: Token không được trộn: token đầu tiên đi đến chuyên gia đầu tiên, token thứ hai đi đến chuyên gia thứ hai, v.v. Trộn đồng nhất: Mỗi slot trộn tất cả token đầu vào theo cùng một cách: bằng cách lấy trung bình chúng, cả cho dispatch và combine. Sự đa dạng chuyên gia phát sinh từ các khởi tạo khác nhau của trọng số. Soft / Uniform: Chúng tôi học trộn token trên đầu vào của chuyên gia để tạo slot (trọng số dispatch), nhưng chúng tôi lấy trung bình đầu ra chuyên gia. Điều này có nghĩa là mỗi token đầu vào được cập nhật giống nhau trước kết nối residual. Uniform / Soft: Tất cả slot được điền bằng trung bình đồng nhất của token đầu vào. Chúng tôi học trộn slot của đầu ra chuyên gia tùy thuộc vào token đầu vào. Bảng 2 cho thấy rằng có slot là quan trọng; định tuyến Identity và Uniform kém hiệu suất đáng kể so với Soft MoE, mặc dù chúng vượt trội ViT. Trộn dispatch có vẻ quan trọng hơn một chút so với trộn combine. Xem Phụ lục A để biết chi tiết bổ sung.

4 HỌC TƯƠNG PHẢN

Chúng tôi kiểm tra liệu các biểu diễn của Soft MoE có tốt hơn cho các tác vụ khác không. Cho việc này, chúng tôi thử học tương phản hình ảnh-văn bản. Theo Zhai et al. (2022b), tháp hình ảnh được pre-train trên phân loại hình ảnh, và sau đó được đóng băng trong khi huấn luyện bộ mã hóa văn bản trên bộ dữ liệu các cặp hình ảnh-văn bản. Chúng tôi tái sử dụng các mô hình được huấn luyện trên JFT trong phần trước và so sánh hiệu suất zero-shot của chúng trên các bộ dữ liệu downstream. Cho học tương phản, chúng tôi huấn luyện trên WebLI (Chen et al., 2022), một bộ dữ liệu độc quyền gồm 10B hình ảnh và alt-text. Bộ mã hóa hình ảnh được đóng băng, trong khi bộ mã hóa văn bản được huấn luyện từ đầu.

Bảng 3 cho thấy kết quả. Nhìn chung, lợi ích chúng tôi quan sát được về phân loại hình ảnh cũng có trong cài đặt này. Ví dụ, Soft MoE-L/16 vượt trội ViT-L/16 hơn 1% và 2% trên ImageNet và Cifar-100 zero-shot, tương ứng. Tuy nhiên, cải thiện trên COCO retrieval khiêm tốn, và có thể phản ánh sự căn chỉnh kém giữa các đặc trưng học được trên JFT từ vựng đóng và tác vụ từ vựng mở này.

Cuối cùng, trong Phụ lục F.1, chúng tôi cho thấy Soft MoEs cũng vượt qua vanilla ViT và bộ định tuyến Experts Choice khi được huấn luyện từ đầu trên LAION-400M có sẵn công khai (Schuhmann et al., 2021). Với pre-training này, Soft MoEs cũng hưởng lợi từ data augmentation, nhưng cả ViT và Experts Choice đều không có vẻ hưởng lợi từ nó, điều này phù hợp với quan sát của chúng tôi trong Phần 3.5, rằng Soft MoEs tận dụng tốt hơn các tham số chuyên gia bổ sung.

5 NGHIÊN CỨU LIÊN QUAN

Nhiều nghiên cứu hiện tại hợp nhất, trộn hoặc kết hợp token đầu vào để giảm độ dài chuỗi đầu vào (Jaegle et al., 2021; Ryoo et al., 2021; Renggli et al., 2022; Wang et al., 2022), thường sử dụng trung bình có trọng số giống attention với key cố định, để cố gắng giảm bớt chi phí bậc hai của self-attention so với độ dài chuỗi. Mặc dù trọng số dispatch và combine của chúng tôi được tính toán theo cách tương tự với các phương pháp này, mục tiêu của chúng tôi không phải là giảm độ dài chuỗi (mặc dù có thể), và chúng tôi thực sự khôi phục độ dài chuỗi ban đầu sau khi tính trọng số đầu ra của chuyên gia với trọng số combine, ở cuối mỗi lớp Soft MoE.

Multi-headed attention cũng cho thấy một số điểm tương đồng với Soft MoE, ngoài việc sử dụng softmax trong trung bình có trọng số: h head khác nhau có thể được hiểu như các chuyên gia (tuyến tính) khác nhau. Sự khác biệt là, nếu m là độ dài chuỗi và mỗi token đầu vào có chiều d, mỗi trong h head xử lý m vector có kích thước d/h. m vector kết quả được kết hợp sử dụng trọng số khác nhau cho mỗi trong m' token đầu ra (tức là trọng số attention), trên mỗi head độc lập, và sau đó các vector (d/h)-chiều kết quả từ mỗi head được nối thành một có chiều d. Các chuyên gia của chúng tôi là phi tuyến và kết hợp các vector có kích thước d, ở đầu vào và đầu ra của các chuyên gia đó.

Các nghiên cứu MoE khác sử dụng tổ hợp có trọng số của các tham số chuyên gia, thay vì thực hiện định tuyến thưa thớt của các ví dụ (Yang et al., 2019; Tian et al., 2020; Muqeeth et al., 2023). Các phương pháp này cũng hoàn toàn khả vi, nhưng chúng có thể có chi phí cao hơn, vì 1) chúng phải lấy trung bình tham số của các chuyên gia, có thể trở thành nút thắt cổ chai về thời gian và/hoặc bộ nhớ khi sử dụng chuyên gia với nhiều tham số; và 2) chúng không thể tận dụng các phép toán vector hóa rộng rãi như Soft (và Sparse) MoEs, vì mọi đầu vào sử dụng tổ hợp có trọng số khác nhau của các tham số.

6 HẠN CHỾ HIỆN TẠI

Giải mã tự hồi quy Một trong những khía cạnh chính của Soft MoE bao gồm học sự hợp nhất của tất cả token trong đầu vào. Điều này làm cho việc sử dụng Soft MoEs trong bộ giải mã tự hồi quy khó khăn, vì tính nhân quả giữa token quá khứ và tương lai phải được bảo tồn trong quá trình huấn luyện. Mặc dù mask nhân quả được sử dụng trong các lớp attention có thể được sử dụng, người ta phải cẩn thận không đưa ra bất kỳ tương quan nào giữa chỉ số token và slot, vì điều này có thể thiên vị chỉ số token nào mỗi chuyên gia được huấn luyện. Việc sử dụng Soft MoE trong bộ giải mã tự hồi quy là một hướng nghiên cứu đầy hứa hẹn mà chúng tôi để dành cho nghiên cứu tương lai.

Chuyên gia lười và tiêu thụ bộ nhớ Chúng tôi cho thấy trong Phần 3 rằng một slot mỗi chuyên gia có xu hướng là sự lựa chọn tối ưu. Nói cách khác, thay vì cung cấp một chuyên gia với hai slot, việc sử dụng hai chuyên gia với một slot mỗi cái hiệu quả hơn. Chúng tôi giả thuyết các slot sử dụng cùng chuyên gia có xu hướng căn chỉnh và cung cấp ít lợi ích thông tin, và một chuyên gia có thể thiếu tính linh hoạt để phù hợp với các phép chiếu slot rất khác nhau. Chúng tôi cho thấy điều này trong Phụ lục I. Do đó, Soft MoE có thể tận dụng một số lượng lớn chuyên gia và—trong khi chi phí của nó vẫn tương tự với backbone dày đặc—yêu cầu bộ nhớ của mô hình có thể tăng lớn.
