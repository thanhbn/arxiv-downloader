# 2401.11053.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2401.11053.pdf
# File size: 1304836 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
StreamVoice: Streamable Context-Aware Language Modeling for Real-time
Zero-Shot Voice Conversion
Zhichao Wang1, Yuanzhe Chen2, Xinsheng Wang1, Lei Xie1*, Yuping Wang2
1Audio, Speech and Language Processing Group (ASLP@NPU)
School of Computer Science, Northwestern Polytechnical University, Xi’an, China
2Douyin Vision Co., Ltd.
Abstract
Recent language model (LM) advancements
have showcased impressive zero-shot voice
conversion (VC) performance. However, exist-
ing LM-based VC models usually apply offline
conversion from source semantics to acous-
tic features, demanding the complete source
speech and limiting their deployment to real-
time applications. In this paper, we introduce
StreamV oice, a novel streaming LM-based
model for zero-shot VC, facilitating real-time
conversion given arbitrary speaker prompts
and source speech. Specifically, to enable
streaming capability, StreamV oice employs a
fully causal context-aware LM with a temporal-
independent acoustic predictor, while alter-
nately processing semantic and acoustic fea-
tures at each time step of autoregression which
eliminates the dependence on complete source
speech. To address the potential performance
degradation from the incomplete context in
streaming processing, we enhance the context-
awareness of the LM through two strategies:
1) teacher-guided context foresight, using a
teacher model to summarize the present and fu-
ture semantic context during training to guide
the model’s forecasting for missing context; 2)
semantic masking strategy, promoting acoustic
prediction from preceding corrupted semantic
and acoustic input, enhancing context-learning
ability. Notably, StreamV oice is the first LM-
based streaming zero-shot VC model without
any future look-ahead. Experiments demon-
strate StreamV oice’s streaming conversion ca-
pability while achieving zero-shot performance
comparable to non-streaming VC systems.
1 Introduction
V oice conversion (VC) aims to transfer a speaker’s
voice to that of another speaker without changing
the linguistic content. This technique has been de-
ployed in many real-world applications, such as
*Corresponding author
Recognition:
Synthesis: 
Conversion  &
Reconstruction
Streaming Zero-shot VC (Recognition-Synthesis)
User AUser B
Desired Speaker 
Timbre...
This system can convert my voice  to arbitrary speakers  in 
real time ! And only one utterance  of the speaker is needed.Synthesis: 
Streaming Zero-shot VC
User AUser B
Speaker 
Timbre...
This system can convert my voice  to arbitrary speakers  in 
real time ! And only one utterance  of the speaker is needed.Wow! It's Shinichi's voice. And 
there is almost no delay.
...
Streaming Zero-shot VC (Recognition-Synthesis)
User AUser B
Speaker 
Timbre...
This system can convert my voice  to arbitrary speakers  in 
real time ! And only one utterance  of the speaker is needed.Wow! It's Shinichi's voice. And 
there is almost no delay.
...
Shinichi
Wow! There is al most no delay.Figure 1: The concept of the streaming zero-shot VC
employing the widely used recognition-synthesis frame-
work (Sun et al., 2016), where only the encoder of
ASR is involved. StreamV oice is built on this popu-
lar paradigm.
movie dubbing, privacy protection, pronunciation
correction, etc. With the help of neural semantic
features, such as the bottleneck feature (BNF) from
an automatic speech recognition (ASR) system,
converting source speech from arbitrary speakers in
the wild has been successfully achieved (Sun et al.,
2016). Meanwhile, converting to an arbitrary tar-
get speaker with only one utterance of this speaker,
called zero-shot VC , has also been researched re-
cently (Qian et al., 2019; Wang et al., 2023c). How-
ever, most existing zero-shot VC models are de-
signed for offline systems, which are insufficient
to meet the recent growing demands of streaming
capability in real-time VC applications, such as live
broadcasting and real-time communication (RTC).
In this study, we focus on the streaming zero-shot
VCas illuminated in Fig. 1.
Disentangling speech into different components,
e.g., semantic content and speaker timbre, plays
an important role in the zero-shot VC task (Chou
and Lee, 2019; Wang et al., 2023d, 2021; Qian
et al., 2019). Recently, benefiting from the power-
ful LM framework and the scaling up of training
data, LM-based VC models (Wang et al., 2023c;arXiv:2401.11053v5  [eess.AS]  19 Jul 2024

--- PAGE 2 ---
Yang et al., 2023; Zhu et al., 2023) with built-in
in-context learning ability can learn the context
relations between source and target speaker’s ut-
terances to capture fine-grained speaker timbre,
achieving impressive zero-shot VC performance.
However, demanding the complete source speech
utterance limits these LM-based VC models to real-
time scenarios; thus, they can only be used in of-
fline applications. While several non-LM-based
methods (Yang et al., 2022; Wang et al., 2023a)
have been proposed for streaming zero-shot VC,
the performance fails to generalize well to unseen
speakers with high speaker similarity and speech
naturalness, mainly due to the limited model ca-
pacity to scale up training data, and also the per-
formance degradation caused by the missing future
information in streaming scenario.
Inspired by the success of LM-based models in
zero-shot VC, we aim to explore the feasibility of
LMs for the streaming VC scenario. An intuitive
way is to follow the popular recognize-synthesis
framework shown in Fig. 1, in which speech is
represented in semantic BNF and acoustic features
respectively extracted by a streaming ASR and an
audio codec. Then, the LM-based VC model under-
takes the transformation of semantic information
into acoustic features with the target speaker’s tim-
bre. However, the development of the LM-based
model in streaming zero-shot VC is hampered by
two primary challenges.
•Streamable architecture: streaming models
typically produce immediate output upon receiv-
ing current input without reliance on future time
steps. Current LM-based VC models perform the
conversion only when they get a full-utterance of
source speech, which fails to meet the demands
of streaming applications. The widely adopted
multi-stage language modeling for multi-layer
codec prediction introduces complexity to sys-
tem design, posing a potential risk of cumulative
errors. Additionally, the dependency models of
the streaming pipeline also impact the design
and performance of the VC model.
•Performance gap: unlike non-streaming mod-
els, streaming models must process frame-wise
or chunked input causally on the fly without fu-
ture information, facing missing context and po-
tential performance degradation. This missing
hinders the streaming VC model from achieving
high-quality conversion. In addition, as shown
in Fig. 1, the VC model relies on the semanticfeature BNF from ASR to achieve conversion,
which makes semantic features very important.
However, streaming ASR exhibits inferior per-
formance compared to its non-streaming coun-
terpart, leading to the BNF carrying low-quality
semantic information but more speaker infor-
mation. In addition to the inherent unavailable
future reception, this low-quality semantic input
makes achieving high-quality conversion more
difficult. The goal of zero-shot VC amplifies the
challenges faced by our streaming VC model.
In this work, we propose StreamVoice , a stream-
ing LM-based model for high-quality zero-shot
VC. Specifically, StreamV oice has a streamable ar-
chitecture that integrates a single-stage language
model that casually generates acoustic codecs with
the collaboration of an acoustic predictor. Alter-
nating input of semantic and acoustic features at
each time step ensures seamless streaming behav-
ior. Two methods are introduced to enhance the
context-awareness of the LM to mitigate the per-
formance gap caused by missing contextual infor-
mation. 1) We incorporate a teacher-guided con-
text foresight, where the VC model is taught by a
teacher non-streaming ASR to infer the present and
future semantic information summarized by the
teacher, which is then used to enhance the acoustic
prediction. 2) To enhance the context learning from
the input history, semantic masking encourages
acoustic prediction from the preceding acoustic
and corrupted semantic input, which also implic-
itly creates an information bottleneck to reduce the
source speaker’s information.
Experiments demonstrate StreamV oice’s ability
to convert speech in a streaming manner with high
speaker similarity for both seen and unseen speak-
ers while maintaining performance comparable to
non-streaming VC systems. As the first LM-based
zero-shot VC model without any future look-ahead,
the total pipeline only has 124 ms latency to per-
form the conversion, 2.4x faster than real-time on
a single A100 GPU without engineering optimiza-
tions. Converted samples can be found in https:
//kerwinchao.github.io/StreamVoice/ .
2 Related Works
Zero-shot Voice Conversion. Zero-shot VC im-
poses stringent demands on speech decoupling and
capturing speaker timbre. Many studies specifically
design many disentanglement approaches, incor-
porating intricate structures (Chou and Lee, 2019),

--- PAGE 3 ---
loss functions (Wang et al., 2021), and training
strategies (Ebbers et al., 2021), to achieve speech
decoupling. Rather than embedding explicit dis-
entanglement designs in VC training, some ap-
proaches (Gu et al., 2021) leverage a speaker ver-
ification (SV) model for speaker representation,
while linguistic content is extracted using ASR or
self-supervised learning (SSL) models (Sun et al.,
2016; Choi et al., 2021). To enhance speaker tim-
bre capturing, some fine-grained speaker model-
ing methods have also been explored (Yin et al.,
2021; Wang et al., 2023d). Recent successes of lan-
guage models in generative tasks have prompted
the exploration of LM-based models in zero-shot
VC, yielding impressive results. Using the pre-
trained model to decouple speech, the LM-based
VC model (Wang et al., 2023c; Yang et al., 2023;
Zhu et al., 2023) captures fine-grained speaker tim-
bre from the speaker prompt and then performs
the conversion. However, current LM-based VC
models are inapplicable to streaming scenarios,
constraining their real-world utility. This paper
addresses this gap by investigating the zero-shot ca-
pabilities of language models specifically tailored
for streaming scenarios.
Streaming Voice Conversion. Despite the high-
quality conversion achieved by non-streaming VC
models, their non-streamable structure and reliance
on full-utterance input hamper them for real-time
streaming applications. For streaming, causal pro-
cessing and the structure of the streaming pipeline
are crucial considerations. Streaming models are
compelled to process frame-wise or chunked in-
put on the fly, devoid of access to future informa-
tion, leading to performance degradation compared
to non-streaming counterparts. To address this, a
common approach (Hayashi et al., 2022; Kameoka
et al., 2021; Ning et al., 2023, 2024) involves the
integration of a teacher model to guide the train-
ing of the streaming model or the distillation of
knowledge from a non-streaming model. Chen et
al. (2023b) focus on selecting BNF with minimal
semantic information loss through layer-wise analy-
sis, while Chen et al. (2022) incorporate adversarial
training to enhance the quality of semantic features.
Beyond streaming VC, some efforts have recently
been towards streaming zero-shot VC. For instance,
VQMIVC (Wang et al., 2021), designed for the non-
streaming application, is modified to be streamable
by Yang et al. (2022). ALO-VC (2023a) constructs
a streaming system using an SV model, a stream-
ing PPG extractor, and a pitch extractor. However,current streaming zero-shot VC, designed for low-
resource devices, has limited model capacity with
poor generalization to unseen speakers, leading
to inferior similarity and naturalness. Motivated
by LM’s successes in zero-shot VC, we design a
streamable LM in streaming scenarios. To tackle
distinctive challenges in streaming VC, we enhance
the context awareness of the LM to improve con-
version quality.
Language Model-based Speech Generation. Re-
cent advancements in LMs within natural language
processing have showcased potent generation ca-
pabilities, influencing the development of LMs in
speech generation. By employing codec (Zeghi-
dour et al., 2021) or other SSL models (Chung
et al., 2021), speech and audio can be efficiently to-
kenized into discrete units, facilitating low-bitrate
audio representation and semantic extraction. This
progress allows speech generation to utilize LM
frameworks seamlessly. Taking audio generation
as a conditional language modeling task, Audi-
oLM (2023) employs hierarchical language mod-
eling for acoustic prediction from coarse to fine
units. V ALL-E (2023b) and SpearTTS (2023) ex-
tend LMs for zero shot-TTS, which can clone a
human’s voice with prompt tokens from a short
recording. For zero-shot VC, LM-VC (2023c) em-
ploys task-oriented optimizations to this task. And
some studies (Zhu et al., 2023; Yang et al., 2023)
leverage multitask objectives and datasets, achiev-
ing high-quality conversion. Despite this progress,
existing LM-based VC models usually apply offline
processing, demanding complete utterance from
the source speech, which hinders their suitability
for real-time streaming applications. In contrast
to prior studies, we explore the zero-shot capabil-
ity of the LM-based VC for streaming scenarios.
With the enhancement of context awareness, the
proposed LM-based VC model achieves results
comparable to non-streaming LM-based VC.
3 StreamVoice
3.1 Overview
As shown in Fig. 2, the development of
StreamV oice follows the recognition-synthesis
framework. In this framework, speech is first rep-
resented as semantic features s={s1, s2, ...sTs}
and acoustic features a={a1, a2, ..., a Ta}by a
pre-trained streaming ASR model and a speech
codec model respectively. Here, TsandTade-
note the sequence length. Before inputting to

--- PAGE 4 ---
Source Speech Target Speaker Speech
Semantic Extraction
(Streaming ASR)Codec Encoding
(Streaming Codec)Codec Decoding 
(Streaming Codec)
s~s a~1s~
1a~
2s~
2a~
3s~
3a~Speaker Prompt
1s1aˆ2s1aˆ2aˆ
2aˆ3s ...3aˆ
Timestep...
StreamVoicePredicted Feature
Semantic Feature
Acoustic Features
Converted Speech
aFigure 2: The overall architecture for StreamV oice.
StreamV oice, sandaare aligned to the same length
T. StreamV oice incorporates a context-aware lan-
guage model and an acoustic predictor to perform a
single language modeling process. With the seman-
tic and acoustic features {˜ s,˜ a}of speech from the
target speaker as speaker prompt, the LM leverages
the semantic information s1:tof source speech to
autoregressively predict the hidden outputch. In
each autoregression time-step of the LM, the acous-
tic predictor transforms the hidden outputchto the
codec feature ˆ aof the converted speech. Finally,
the codec model reconstructs the waveform from
the predicted codec feature. In the following sec-
tions, we will introduce how to build a streamable
LM for VC and how to ensure the high-quality
conversation of this streaming VC.
Bottleneck Regulator1h2h3h4h5h...
1a2a3a4a5a
...
1s2s5sMasked  Semantic Feature (Steaming ASR )...Cross EmbeddingAutoregression Output
skip skip skip skip
Unidirectional AttentionLinear 
Prediction
Teacher  Semantic Feature 
(Non-steaming ASR)1h2h3h4h5h ...
1c2c3c
1s......Context Feature
2s3s4s5sts...
...
Add
Context-enhanced Output
1hcTFL
2hc
3hc
4hc
5hc...Context-masked
 Autoregressive PredictionTeacher-guided 
Context Foresight
MMAcoustic FeatureBottleneck
Figure 3: The architecture for context-aware LM.
3.2 Streamable Architecture
To perform streaming VC, a streamable architec-
ture is necessary. In StreamV oice, the language
model is carefully designed to perform full causalprocessing in the VC task, and the acoustic predic-
tor is designed to achieve frame-wise prediction
without dependency on temporal information.
3.2.1 Fully Casual Language Model
As shown in Fig. 3, inspired by the success of the
LM-based VC model, we intend to achieve stream-
ing zero-shot VC by language models. In previ-
ous LM-based VC models (Wang et al., 2023c),
the demand of the complete semantic feature s
from source speech to achieve conversion hinders
the deployment for real-time application, which
can be formulated as p(at|s1:Ts,a1:t−1)for each
time step. To achieve streaming, any components
of the LM cannot rely on future information. As
shown in Fig. 3, decoder-only LM with unidirec-
tional attention can easily fit the requirement of
casual generation. To eliminate the dependency of
the complete semantic input, semantic and acous-
tic features {s,a}are first aligned with each other
to the same sequence length Tand then they are
alternatively inputted to the LM, forming a cross-
embedding like {s1, a1, s2, a2, ..., s T, aT}. With
these modifications, the LM can achieve streaming
processing, modeling p(at|s1:t,a1:t−1).
To be specific, the semantic feature sobtained
via an ASR model comprises a sequence of em-
beddings, denoted as {s1, s2, ..., s T}. On the other
hand, the codec tokens obtained from an L-layer
codec are discrete units represented by a∈ RT×L.
To obtain the acoustic embedding sequence, the
codec tokens from each layer undergo separate
embedding into the embedding space, and then
they are concatenated along the embedding dimen-
sion, resulting in the fused acoustic embedding.
Both the fused acoustic embedding and semantic
features are transformed into the same dimension
using linear layers. Subsequently, they are alter-
nately inputted into the language model, forming
the cross-embedding.
Continuous Codec
1a2a3a4a5a ...
Continuous Projection
Linear PredictionLinear PredictionLinear PredictionDiscrete ProjectionL-layer Discrete Codec
Transformer Block
tch1
ta1
ta2
ta
2
ta
In t-th step......L
ta1
1aLa1...La2La3La4La5...
1
2a1
3a1
4a1
5a
.........
...
hc
From Autoregressive ModelContinuous
DiscreteContL DiscL
Figure 4: The architecture for acoustic predictor. Our
system can support continuous or discrete projection.

--- PAGE 5 ---
3.2.2 Acoustic Predictor
As the preceding LM has essentially encoded con-
tent and speaker into its outputch, the acoustic
predictor can be designed in temporal irrelevant
to transformchinto acoustic codec space, which
means the predictor can be easily applied in the
streaming scenario. Given that the speech can be
represented in acoustic features by neural codec in
either continuous or discrete forms, we investigate
the incorporation of both features in StreamV oice,
which are performed by continuous projection and
discrete projection, respectively.
Continuous Projection. Following Shen et
al. (2024), the D-dimensional quantized latent vec-
tora∈ RT×Dencoded by the codec model is used
as the continuous acoustic representation. The pre-
diction of the continuous representation involves
employing a stack of linear layers, as shown in
Fig. 4. The continuous projection loss is calculated
as the L2 distance between the predicted acoustic
feature ˆaand the ground-truth acoustic feature a,
which is defined as:
LCont=||a−ˆa||2
2. (1)
Discrete Projection. In general, the codec
is designed with multi-layer quantizers to com-
press original speech into L-layer discrete indices
a∈ RT×Lat a low bitrate. Most LM-based
work (Wang et al., 2023b,c) stacks multiple LMs
to predict discrete features, making the pipeline
complicated and unsuitable for the streaming sce-
nario. In contrast, StreamV oice adopts a stream-
lined multi-layer codec prediction method inspired
by MQTTS (Chen et al., 2023a). This method, free
from temporal dependencies, can seamlessly inte-
grate into the streaming process of the language
model. Specifically, a single-layer transformer is
used to model the hierarchical conditional distribu-
tion of codecs. As depicted in the right of Fig. 4, at
timet, the transformer employs thechas the start-
ing condition and sequentially generates al
tfrom
layer 1 to L. Remarkably, this generation process is
independent of the preceding or the futurech, ren-
dering it well-suited for the demands of a streaming
scenario. Notably, in the proposed StreamV oice,
we mainly incorporate the discrete projection to
achieve acoustic prediction. The discrete projec-
tion loss can be described as:
LDisc =−logTY
t=1LY
l=1p(al
t|a1:t−1,ms1:t, t, a1:l−1
t ).(2)3.3 Context-aware Enhancement
Due to the disadvantage of the causality in the
streaming framework, streaming models face miss-
ing future reception and potential performance
degradation compared to the non-streaming model,
while the low-quality semantic input from the
streaming ASR, as we mentioned in Section 1,
makes achieving high-quality conversion more
challenging. To address these issues, a context-
aware enhancement method is proposed, which can
alleviate incomplete contextual information arising
from the semantic input and the absence of future
information. Specifically, we introduce context-
masked autoregressive prediction in the LM to en-
hance the capture of historical context from the
given semantic input. Meanwhile, a teacher-guided
context foresight is proposed to ensure the model
can imagine the future context based on that of its
historical context.
Context-masked Autoregressive Prediction.
As shown in the left of Fig. 3, the LM is achieved
by the multi-layer Transformer with unidirec-
tional attention, following the implementation of
LLaMA (Touvron et al., 2023). To enhance con-
textual awareness from the given semantic input,
semantic masking is introduced in the LM to en-
courage acoustic prediction from the corrupted se-
mantic. Specifically, within a sequence of seman-
tic tokens s={s1, s2, ...sT}, we randomly se-
lect several indices as start indices at a ratio r,
and spans of lsteps are masked by [M]. After
masking, LM takes the corrupted semantic feature
msas input and performs autoregression. With
this method, an information bottleneck is also im-
plicitly created in the semantic feature to reduce
speaker information. Moreover, during training, we
do not explicitly use a speech clip as the speaker
prompt. Instead, LM leverages the previous se-
quence {s1:t−1,a1:t−1, st}as prompts to autore-
gressively generate hidden representation htfor
further acoustic prediction. Notably, when the cur-
rent input is at, the corresponding output is skipped
and does not involve further steps.
Teacher-guided Context Foresight. As previ-
ously discussed, the absence of future information
resulting in the loss of contextual information leads
to a decline in the conversion performance. In-
spired by the effective representation learning ex-
hibited by autoregressive predictive coding (2019)
(APC), we introduce teacher-guided context fore-
sight guided by a non-streaming ASR to enhance

--- PAGE 6 ---
the autoregression output, as presented in the right
of Fig. 3. This allows the model to learn a context
vector containing envisioned future information.
Specifically, the context representation cis first de-
rived by linear prediction from the hidden features
h, which is generated by the LM through historical
context. Subsequently, this ctis encouraged to dis-
cover more general future context information by
minimizing the L2 distance not only with kseman-
tic features from future time steps st+1, ...,st+k
but also with the current semantic st. This dual
minimization approach contributes to precise con-
tent delivery and enhances the ability to forecast
future context. The loss can be summarized as
LTF=1
T−kT−kX
1∥ct−Concat (st,st+1, ...,st+k)∥2
2
(3)
where Concat (·)denotes the concatenation of fea-
tures along the dimensional axe. Unlike the original
APC, which operates between the input and output
of an autoregressive model, our approach employs
a non-streaming ASR model as a teacher to provide
semantic information sfor guiding this foresight
process. This is done to tackle the inherent chal-
lenge of obtaining high-quality semantic features
from the streaming ASR. After dimensional trans-
formations, the context representation cis then
combined with hto form the context-enhancedch,
which is then fed into the acoustic predictor.
Furthermore, since the semantic feature {s,s}
still may contain speaker-related information. To
further ensure the speech decoupling, the bottle-
neck regulator (Qian et al., 2019), which squeezes
out speaker information by reducing dimension
size with a linear layer, is applied in sandc.
3.4 Training & Inference Procedure
Training. During training, the context-enhanced
language model and acoustic predictor are trained
together. The total loss can be described as
Ltotal=LTF+LCont for continuous codec or
Ltotal=LTF+LDisc for discrete version.
Streaming Inference. We use the semantic and
acoustic features from a short speech clip of the
target speaker as the speaker prompt. Since this
clip is randomly selected, which may contain un-
finished pronunciation at the end of the clip, we
pad a silence clip after the speaker recording before
the conversion process to prevent the unexpected
continuation. With this prompt, StreamV oice can
stream convert the source speech. In discrete pro-
jection, we use greedy decoding to choose thecodec token with the highest probability. Be-
sides, to ensure the real-time streaming inference
of StreamV oice, we employ the key-value cache in
LM to reduce redundant calculations. In practice,
since the beginning and end of the source speech
can be determined by ASR or voice activity de-
tection (V AD), we don’t employ techniques, such
as window attention or slide attention, to handle
the input. StreamV oice’s performance decreases
when the long input exceeds the maximum training
length. Notably, these techniques can be easily in-
tegrated into our framework, providing flexibility
for future extensions.
4 Experiments
4.1 Experimental Setup
Corpus. A mixed dataset comprising 1,500 hours
of Aishell3 (Shi et al., 2021) and an internal Chi-
nese dataset are used to train StreamV oice and Au-
diodec (Wu et al., 2023). The internal dataset con-
tains recordings from 2679 Chinese speakers, while
we use utterances from 200 speakers in Aishell3.
To extract semantic features, we incorporate a
streaming ASR Fast-U2++ (Liang et al., 2023),
which is implemented by WeNet (Yao et al., 2021)
and trained on WenetSpeech (Zhang et al., 2022).
For zero-shot testing, a set of 400 testing pairs is
selected from DIDISpeech (Guo et al., 2021) and
EMIME (Wester, 2010), each with a source and
target speaker utterance. For evaluation of seen
speakers, eight speakers from Aishell3 are selected
to form 160 conversion pairs. And 3s speech utter-
ance is used as a speaker prompt in inference. The
duration of testing utterances is between 3s and 7s.
Implement Details. We use open-sourced code1
of Audiodec, which has 4 quantizer layers with a
1024 codebook size and 64 codebook dimension,
representing a 24kHz waveform in 20ms frame
length. The Fast-U2++ uses an 80ms chunk size
to perform streaming inference and compresses a
16kHz waveform into a semantic feature with a
40ms frame length. StreamV oice contains 101M
parameters. For context-enhanced LM, we employ
the variant of Transformer, LLaMA (Touvron et al.,
2023), with 6 layers and 8 heads. The hidden and
intermediate sizes are 1024 and 4096. We use the
officially released code2to implement the acoustic
predictor, which uses a layer Transformer decoder
with a hidden size 256, feed-forward hidden size
1https://github.com/facebookresearch/AudioDec
2https://github.com/b04901014/MQTTS

--- PAGE 7 ---
1024, and 4 heads. In semantic masking, mask ratio
rranges from 0.01to0.02, and span lis set to 10.
The foresight step kis set to 4. The bottleneck reg-
ulator compresses feature dimensions by 6 times.
During training, the max training length is set to
12s. StreamV oice is trained using 8 V100 GPUs
with a batch size of 7 utterances per GPU for 700k
steps. We use the AdamW optimizer with a learn-
ing rate of 5×10−4. Exponential decay updates
the learning rate after each epoch, using a decay
ratio of 0.986.
Evaluation Metrics. The mean opinion score
(MOS) subjectively measures speech naturalness
(NMOS) and speaker similarity (SMOS), which
are calculated with 95 %confidence intervals. We
randomly select 120 testing pairs for subjective
evaluations involving a group of 15 listeners. For
objective evaluations, a neural network-based sys-
tem with open-source implementation3is used to
measure speech quality (WV-MOS). Character er-
ror rate (CER) measured by an ASR model4indi-
cates the speech intelligibility. Speaker similarity
(SSIM) is calculated by an SV model (Desplanques
et al., 2020) to determine if the converted speech
matches the target speaker. Real-time factor (RTF)
and latency indicate the streaming performance.
MethodQuality Similarity
NMOS ↑WVMOS ↑CER ↓SMOS ↑SSIM ↑
GT (origin) - 3.61 6.29 - 0.803
Non-streaming Topline
LM-VC 3.80 ±0.09 3.74 8.93 3.78 ±0.08 0.742
NS-StreamV oice 3.87 ±0.07 3.68 8.51 3.73 ±0.11 0.755
Streaming Model
C-StreamV oice 3.72 ±0.10 3.49 10.2 3.67 ±0.09 0.729
StreamV oice 3.83±0.09 3.63 9.43 3.74 ±0.08 0.740
Table 1: Zero-shot performance (unseen speakers)
4.2 Experiments Results
4.2.1 Zero-shot Evaluation
To evaluate the zero-shot VC performance, one
recent LM-based zero-shot VC system, LM-
VC(Wang et al., 2023c), is selected as the topline
system. Besides, a variant of StreamV oice, referred
to as NS-StreamVoice , using a non-streaming ASR
for semantic extraction, is also compared. We
implement the proposed system StreamVoice in-
tegration discrete projection, while C-StreamVoice
3https://github.com/AndreevP/wvmos
4https://github.com/wenet-
e2e/wenet/tree/main/examples/wenetspeechalso involves the evaluation since speech can repre-
sented in continuous form by codec model. Table. 1
presents both subjective and objective results. Com-
pared with the non-streaming topline LM-VC, our
proposed StreamV oice can achieve close results
regarding subjective NMOS and SMOS, while a
performance gap still exists. Similar results are also
observed in objective results. The non-streaming
StreamV oice even surpasses the topline model in
certain aspects, indicating the effectiveness of our
streamable architecture for zero-shot VC. Addition-
ally, C-StreamV oice exhibits inferior performance
compared to the discrete version, which can con-
tribute to the over-smoothness in speech genera-
tion (Ren et al., 2022) and the mismatch between
the ground truth and predicted features.
As illustrated in Table. 2, the RTF of the entire
pipeline is below 1, which meets the real-time re-
quirement. Consisting of chunk-waiting latency
(80ms) and model inference latency, the overall
pipeline latency is 124.3 ms. If using a V100 GPU,
StreamV oice can obtain an RTF of 0.56, and the
overall latency reaches 137.2 ms. Importantly, un-
like previous streaming VC, our VC model is en-
tirely causal without any future look-ahead, high-
lighting its powerful modeling capability. These
results show that StreamV oice can achieve high-
quality zero-shot VC in streaming scenarios.
RTF Latency (ms)
ASR Encoder 0.13 10.4
Codec Decoder 0.004 0.3
StreamV oice 0.42 33.6
Overall 0.554 44.3+80=124.3
Table 2: Speed tested on an A100 80G GPU. Latency is
obtained by multiplying RTF by 80ms chunk size.
MethodQuality Similarity
NMOS ↑WVMOS ↑CER ↓SMOS ↑SSIM ↑
GT (origin) - 3.65 6.29 - 0.729
Non-streaming Topline
NS-VC 3.85 ±0.09 3.71 8.39 3.92 ±0.08 0.744
Streaming Model
IBF-VC 3.71 ±0.09 3.48 9.52 3.67 ±0.10 0.687
DualVC2 3.80 ±0.10 3.57 10.2 3.85 ±0.09 0.703
StreamV oice 3.82 ±0.09 3.50 10.0 3.82 ±0.10 0.694
+Tuning 3.78 ±0.08 3.52 10.4 3.87 ±0.10 0.714
Table 3: In-dataset performance (seen speakers)
4.2.2 In-dataset Evaluation
To get further insight into StreamV oice, we con-
ducted an in-dataset evaluation on eight seen speak-

--- PAGE 8 ---
ers, as shown in Table. 3. A non-streaming VC
system (Tian et al., 2020) achieving any2many
VC, is selected, referred to as NS-VC . Also, IBF-
VC(Chen et al., 2022) and DualVC2 (Ning et al.,
2024) are recently proposed streaming models for
any2many VC. As observed, a performance gap ex-
ists between the strong non-streaming topline and
streaming models. Among the streaming models,
StreamV oice, designed for the zero-shot scenario,
delivers similar results to systems designed for in-
dataset speakers, even though StreamV oice uses
a smaller chunk size of 80ms in streaming ASR,
achieving lower ASR performance. In contrast,
IBF-VC and DualVC2 employ 160ms chunk size of
ASR for streaming VC. It indicates StreamV oice’s
good conversion ability. With available utterances
of target speakers, fine-tuning yields superior per-
formance. This indicates our system can be easily
applied to various scenarios with or without the
utterances of target speakers.
Method WVMOS ↑CER↓SSIM↑
StreamV oice 3.63 9.43 0.740
w/o Teacher-guided Context Foresight
w/oLTF(st) 2.56 76.8 0.59
w/oLTF(st+1:t+k) 3.39 13.7 0.728
w/o Semantic Masking 3.47 13.0 0.715
w/o Bottleneck Regulator 3.59 9.21 0.718
Table 4: Results of ablation studies.
4.3 Ablation Study
As presented in Table 4, we conducted several ab-
lations studies. In w/oteacher-guided context fore-
sight, we discard the prediction of current and fu-
ture semantic information, forming two ablations
w/oLTF(st)andw/oLTF(st+1:t+k). As can be
seen, a noticeable decrease occurs in all evaluation
metrics when the LTF(st+1:t+k)is discarded, espe-
cially in WVMOS and CER. This indicates that this
foresight improves performance in capturing the
linguistic content. But when only integrating con-
text from future semantics, the model w/oLTF(st)
faces severe performance loss. It shows that only
using future information interferes with delivering
current linguistic content. In w/osemantic masking,
we observe a performance decrease in all evaluation
metrics when the semantic masking is discarded.
This indicates that StreamV oice, trained with se-
mantic masking, effectively enhances contextual
learning from the preceding input while improving
speaker timbre capturing. Furthermore, the resultsof dropping the bottleneck regulator show that its
integration effectively prevents the source speaker
information contained in the semantic feature from
leaking into the converted speech, with little effect
on speech quality.
Type of ASR WVMOS ↑CER↓SSIM↑
Non-streaming ASR 3.68 8.51 0.755
Streaming ASR (Moritz et al., 2019)
+0ms Future Look-ahead 3.19 91.7 0.674
+160ms Future Look-ahead 3.48 10.6 0.727
Streaming ASR (Fast-U2++ (Liang et al., 2023))
Chunk (80ms) 3.63 9.43 0.740
Chunk (160ms) 3.69 9.16 0.744
Table 5: Analysis of dependency on different ASR.
4.4 Discussion: Dependency Analysis
In this section, we will explore the dependency
relations between the selection of ASR and codec
and the performance of StreamV oice.
ASR. To investigate the impact of ASR on
StreamV oice, three representative ASR systems,
including non-streaming ASR4, widely used CTC-
based streaming ASR (Moritz et al., 2019), and
the recently proposed streaming Fast-U2++ (Liang
et al., 2023), are selected to perform semantic ex-
traction. As can be seen in Table 5, StreamV oice
using semantic features of non-streaming ASR out-
performs those using streaming ASR. This discrep-
ancy may be attributed to the inherent performance
gap between non-streaming and streaming ASR
models, resulting in different semantic extraction
abilities. Besides, without future look-ahead in
StreamV oice, using semantic features from (Moritz
et al., 2019) cannot achieve reasonable conversion,
while we introduce a 160ms future look-ahead in
StreamV oice, i.e., modeling p(at|a1:t−1,s1:t+m, t)
withmfuture look-ahead, yield good conversion re-
sults. This issue may arise from delayed CTC spike
distributions and token emission latency existing
in streaming ASR (Liang et al., 2023), leading to
semantic information shifting. Benefiting from the
low emission latency of Fast-U2++, StreamV oice
can perform conversion without future look-ahead.
With a longer chunk size employed in Fast-U2++,
StreamV oice can obtain better results while reach-
ing a larger latency of 270ms. A trade-off still
exists between performance and speed.
Codec. In StreamV oice, we employ a low-
latency streaming codec Audiodec (Wu et al.,
2023). As presented in Table. 6, we validate the

--- PAGE 9 ---
Type of Audiodec WVMOS ↑CER↓SSIM↑RTF
w/2kbps Audiodec 3.63 9.43 0.740 0.42
w/8kbps Audiodec 3.61 9.38 0.738 0.61
Large w/ 8kbps Audiodec 3.68 9.12 0.751 0.90
Table 6: Analysis of dependency on Audiodec with
various bitrate.
performance of StreamV oice using codecs with dif-
ferent bitrates, including 2kbps and 8kbps, where
higher bitrate codecs achieve superior reconstruc-
tion quality to lower bitrate ones. The 2kbps
Audiodec utilizes 4 layers of quantization and
represents audio with a frame length of 20ms,
while the 8kbps Audiodec employs 8 layers with a
frame length of 10ms. Using the configuration of
StreamV oice mentioned in Section 4.1, the results
in different bitrates of codec models show no ob-
vious differences. When increasing the number of
transformer layers in the codec predictor, forming
Large w/ 8kbps Audiodec , conversion performance
using 8kbps codec improves noticeably, but result-
ing in slower inference. This result shows that the
design of StreamV oice depends on the codec con-
figuration, affecting both conversion quality and
inference speed.
5 Conclusions
This paper introduces StreamV oice, a novel LM-
based zero-shot VC system designed for streaming
scenarios. Specifically, StreamV oice employs a
single-stage framework encompassing a context-
aware LM and an acoustic predictor. The casual
design of the model’s input and structure ensures
compliance with streaming behavior. To address
performance degradation caused by missing com-
plete contextual information in streaming scenarios,
context-aware LM adopts teacher-guided context
foresight to make the model able to forecast the
current and future information given by a teacher.
Besides, semantic masking is introduced in LM
to enhance context learning from historical input
and facilitate better disentanglement. Finally, an
acoustic predictor collaborates with the LM to gen-
erate the target speech. Experiments demonstrate
that StreamV oice achieves streaming zero-shot VC
while maintaining performance comparable to non-
streaming VC systems.
6 Limitations
We have to point out that StreamV oice still has
limitations. In our configuration, StreamV oiceneeds a GPU, such as V100 and A100, to achieve
real-time streaming inference. The design of
streaming VC heavily relies on the ASR and the
speech codec as mentioned in Section 4.4. Be-
sides, StreamV oice also faces the out-of-domain
problem, which causes performance degradation
for utterances with accents, strong emotions, or un-
seen recording environments. Our future work will
first use more training data with diversity coverage
to explore StreamV oice’s modeling ability. Also,
we will focus on optimizing our streaming pipeline,
such as high-fidelity codec with low bitrate and
unified streaming model.
7 Ethics Statement
Since StreamV oice can convert source speech to
desired speakers, it may carry potential risks of mis-
use for various purposes, such as spreading fake
information or phone fraud. To prevent the abuse of
the VC technology, many studies have focused on
synthetic speech detection (Yi et al., 2022). Mean-
while, we also encourage the public to report the
illegal usage of VC to the appropriate authorities.
References
Zalán Borsos, Raphaël Marinier, Damien Vincent, Eu-
gene Kharitonov, Olivier Pietquin, Matt Sharifi,
Dominik Roblek, Olivier Teboul, David Grangier,
Marco Tagliasacchi, and Neil Zeghidour. 2023. Au-
diolm: A language modeling approach to audio gen-
eration. IEEE/ACM Transactions on Audio, Speech,
and Language Processing , 31:2523–2533.
Li-Wei Chen, Shinji Watanabe, and Alexander Rud-
nicky. 2023a. A vector quantized approach for text to
speech synthesis on real-world spontaneous speech.
InAssociation for the Advancement of Artificial In-
telligence (AAAI) .
Yuanzhe Chen, Ming Tu, Tang Li, Xin Li, Qiuqiang
Kong, Jiaxin Li, Zhichao Wang, Qiao Tian, Yuping
Wang, and Yuxuan Wang. 2023b. Streaming voice
conversion via intermediate bottleneck features and
non-streaming teacher guidance. In International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 1–5.
Ziyi Chen, Haoran Miao, and Pengyuan Zhang. 2022.
Streaming non-autoregressive model for any-to-many
voice conversion. Arxiv .
Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee,
Hoon Heo, and Kyogu Lee. 2021. Neural analy-
sis and synthesis: Reconstructing speech from self-
supervised representations. In Neural Information
Processing Systems(NeurIPS) , pages 16251–16265.

--- PAGE 10 ---
Juchieh Chou and Hungyi Lee. 2019. One-shot voice
conversion by separating speaker and content rep-
resentations with instance normalization. In Inter-
national Speech Communication Association (Inter-
speech) , pages 664–668.
Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James
Glass. 2019. An Unsupervised Autoregressive Model
for Speech Representation Learning. In International
Speech Communication Association (Interspeech) ,
pages 146–150.
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng
Chiu, James Qin, Ruoming Pang, and Yonghui Wu.
2021. w2v-bert: Combining contrastive learning
and masked language modeling for self-supervised
speech pre-training. In Automatic Speech Recogni-
tion and Understanding Workshop (ASRU) , pages
244–250.
Brecht Desplanques, Jenthe Thienpondt, and Kris De-
muynck. 2020. ECAPA-TDNN: Emphasized chan-
nel attention, propagation and aggregation in TDNN
based speaker verification. In International Speech
Communication Association (Interspeech) , pages
3830–3834.
Janek Ebbers, Michael Kuhlmann, Tobias Cord-
Landwehr, and Reinhold Haeb-Umbach. 2021. Con-
trastive predictive coding supported factorized varia-
tional autoencoder for unsupervised learning of dis-
entangled speech representations. In International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP) , pages 3860–3864.
Yewei Gu, Zhenyu Zhang, Xiaowei Yi, and Xianfeng
Zhao. 2021. MediumVC: Any-to-any voice conver-
sion using synthetic specific-speaker speeches as in-
termedium features. Arxiv .
Tingwei Guo, Cheng Wen, Dongwei Jiang, Ne Luo,
Ruixiong Zhang, Shuaijiang Zhao, Wubo Li, Cheng
Gong, Wei Zou, Kun Han, and Xiangang Li. 2021.
Didispeech: A large scale mandarin speech corpus.
InInternational Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 6968–6972.
Tomoki Hayashi, Kazuhiro Kobayashi, and Tomoki
Toda. 2022. An investigation of streaming non-
autoregressive sequence-to-sequence voice conver-
sion. In International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , pages 6802–
6806.
Hirokazu Kameoka, Kou Tanaka, and Takuhiro Kaneko.
2021. Fasts2s-vc: Streaming non-autoregressive
sequence-to-sequence voice conversion. Arxiv .
Eugene Kharitonov, Damien Vincent, Zalán Borsos,
Raphaël Marinier, Sertan Girgin, Olivier Pietquin,
Matt Sharifi, Marco Tagliasacchi, and Neil Zeghi-
dour. 2023. Speak, Read and Prompt: High-fidelity
text-to-speech with minimal supervision. Transac-
tions of the Association for Computational Linguis-
tics, 11:1703–1718.Chengdong Liang, Xiao-Lei Zhang, BinBin Zhang,
Di Wu, Shengqiang Li, Xingchen Song, Zhendong
Peng, and Fuping Pan. 2023. Fast-u2++: Fast
and accurate end-to-end speech recognition in joint
ctc/attention frames. In International Conference on
Acoustics, Speech and Signal Processing (ICASSP) ,
pages 1–5.
Niko Moritz, Takaaki Hori, and Jonathan Le Roux. 2019.
Streaming end-to-end speech recognition with joint
ctc-attention based models. In Automatic Speech
Recognition and Understanding Workshop (ASRU) ,
pages 936–943.
Ziqian Ning, Yuepeng Jiang, Pengcheng Zhu, Shuai
Wang, Jixun Yao, Lei Xie, and Mengxiao Bi. 2024.
Dualvc 2: Dynamic masked convolution for unified
streaming and non-streaming voice conversion. In
International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 11106–11110.
Ziqian Ning, Yuepeng Jiang, Pengcheng Zhu, Jixun Yao,
Shuai Wang, Lei Xie, and Mengxiao Bi. 2023. Du-
alvc: Dual-mode voice conversion using intra-model
knowledge distillation and hybrid predictive coding.
InInternational Speech Communication Association
(Interspeech) , pages 2063–2067.
Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang,
and Mark Hasegawa-Johnson. 2019. Autovc: Zero-
shot voice style transfer with only autoencoder loss.
InInternational Conference on Machine Learning
(ICML) , pages 5210–5219.
Yi Ren, Xu Tan, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
2022. Revisiting over-smoothness in text to speech.
InAssociation for Computational Linguistics(ACL) ,
pages 8197–8213.
Kai Shen, Zeqian Ju, Xu Tan, Eric Liu, Yichong Leng,
Lei He, Tao Qin, sheng zhao, and Jiang Bian. 2024.
NaturalSpeech 2: Latent diffusion models are natural
and zero-shot speech and singing synthesizers. In In-
ternational Conference on Learning Representations
(ICLR) .
Yao Shi, Hui Bu, Xin Xu, Shaoji Zhang, and Ming
Li. 2021. AISHELL-3: A multi-speaker mandarin
tts corpus. In International Speech Communication
Association (Interspeech) , pages 2756–2760.
Lifa Sun, K. Li, Hao Wang, Shiyin Kang, and H. Meng.
2016. Phonetic posteriorgrams for many-to-one
voice conversion without parallel data training. In
International Conference on Multimedia and Expo
(ICME) , pages 1–6.
Xiaohai Tian, Zhichao Wang, Shan Yang, Xinyong
Zhou, Hongqiang Du, Yi Zhou, Mingyang Zhang,
Kun Zhou, Berrak Sisman, Lei Xie, and Haizhou Li.
2020. The NUS & NWPU system for V oice Conver-
sion Challenge 2020. In Proc. Joint Workshop for the
Blizzard Challenge and Voice Conversion Challenge
2020 , pages 170–174.

--- PAGE 11 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023. Llama: Open and efficient foun-
dation language models. Arxiv .
Bohan Wang, Damien Ronssin, and Milos Cernak.
2023a. Alo-vc: Any-to-any low-latency one-shot
voice conversion. In International Speech Communi-
cation Association (Interspeech) , pages 2073–2077.
Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,
Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,
Huaming Wang, Jinyu Li, et al. 2023b. Neural codec
language models are zero-shot text to speech synthe-
sizers. Arxiv .
Disong Wang, Liqun Deng, Yu Ting Yeung, Xiao Chen,
Xunying Liu, and Helen Meng. 2021. Vqmivc: Vec-
tor quantization and mutual information-based un-
supervised speech representation disentanglement
for one-shot voice conversion. In International
Speech Communication Association (Interspeech) ,
pages 1344–1348.
Zhichao Wang, Yuanzhe Chen, Lei Xie, Qiao Tian,
and Yuping Wang. 2023c. Lm-vc: Zero-shot voice
conversion via speech generation based on language
models. IEEE Signal Processing Letters , pages 1157–
1161.
Zhichao Wang, Liumeng Xue, Qiuqiang Kong, Lei Xie,
Yuanzhe Chen, Qiao Tian, and Yuping Wang. 2023d.
Multi-level temporal-channel speaker retrieval for
robust zero-shot voice conversion. Arxiv .
Mirjam Wester. 2010. The EMIME bilingual database.
Technical report, The University of Edinburgh.
Yi-Chiao Wu, Israel D Gebru, Dejan Markovi ´c, and
Alexander Richard. 2023. Audiodec: An open-
source streaming high-fidelity neural audio codec.
InInternational Conference on Acoustics, Speech
and Signal Processing (ICASSP) , pages 1–5.
Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang,
Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng
Zhao, Jiang Bian, Xixin Wu, et al. 2023. Uniaudio:
An audio foundation model toward universal audio
generation. Arxiv .
Haoquan Yang, Liqun Deng, Yu Ting Yeung, Nianzu
Zheng, and Yong Xu. 2022. Streamable speech rep-
resentation disentanglement and multi-level prosody
modeling for live one-shot voice conversion. In In-
ternational Speech Communication Association (In-
terspeech) , pages 2578–2582.
Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang,
Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen,
Lei Xie, and Xin Lei. 2021. WeNet: Production
oriented streaming and non-streaming end-to-end
speech recognition toolkit. In International Speech
Communication Association (Interspeech) , pages
4054–4058.Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin
Ma, Chenglong Wang, Tao Wang, Zhengkun Tian,
Ye Bai, Cunhan Fan, Shan Liang, Shiming Wang,
Shuai Zhang, Xinrui Yan, Le Xu, Zhengqi Wen, and
Haizhou Li. 2022. Add 2022: the first audio deep
synthesis detection challenge. In International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP) .
Dacheng Yin, Xuanchi Ren, Chong Luo, Yuwang Wang,
Zhiwei Xiong, and Wenjun Zeng. 2021. Retriever:
Learning content-style representation as a token-level
bipartite graph. In International Conference on
Learning Representations (ICLR) .
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan
Skoglund, and Marco Tagliasacchi. 2021. Sound-
Stream: An end-to-end neural audio codec. Trans-
actions on Audio, Speech, and Language Processing ,
30:495–507.
Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao,
Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen,
Chenchen Zeng, et al. 2022. Wenetspeech: A 10000+
hours multi-domain mandarin corpus for speech
recognition. In International Conference on Acous-
tics, Speech and Signal Processing (ICASSP) , pages
6182–6186.
Xinfa Zhu, Yuanjun Lv, Yi Lei, Tao Li, Wendi He, Hong-
bin Zhou, Heng Lu, and Lei Xie. 2023. Vec-tok
speech: speech vectorization and tokenization for
neural speech generation. Arxiv .
