# 2306.01385.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2306.01385.pdf
# Kích thước tệp: 185142 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
arXiv:2306.01385v2 [eess.AS] 9 Jul 2023 Cắt tỉa có cấu trúc bất khả tri tác vụ của các mô hình biểu diễn giọng nói
Haoyu Wang1, Siyuan Wang1, Wei-Qiang Zhang1∗, Hongbin Suo2, Yulong Wan2
1Khoa Kỹ thuật Điện tử, Đại học Tsinghua, Bắc Kinh 100084, Trung Quốc
2Hệ thống Kỹ thuật Dữ liệu & AI, OPPO, Bắc Kinh 100026, Trung Quốc
w-hy21@mails.tsinghua.edu.cn, wq-zhang@tsinghua.edu.cn

Tóm tắt
Các mô hình tiền huấn luyện tự giám sát như Wav2vec2, Hubert, và WavLM đã được chứng minh là cải thiện đáng kể nhiều tác vụ giọng nói. Tuy nhiên, yêu cầu bộ nhớ lớn và tính toán mạnh của chúng cản trở khả năng ứng dụng công nghiệp. Cắt tỉa có cấu trúc là một kỹ thuật nén mô hình thân thiện với phần cứng nhưng thường dẫn đến mất mát độ chính xác lớn hơn. Trong bài báo này, chúng tôi đề xuất một phương pháp cắt tỉa đầu chú ý chi tiết để bù đắp cho sự suy giảm hiệu suất. Ngoài ra, chúng tôi cũng giới thiệu bộ ước lượng thẳng vào chính quy hóa L0 để thêm tăng tốc cho mô hình đã cắt tỉa. Thí nghiệm trên benchmark SUPERB cho thấy mô hình của chúng tôi có thể đạt được hiệu suất tương đương với mô hình dày đặc trong nhiều tác vụ và vượt trội hơn mô hình cơ sở Wav2vec 2.0 trung bình, với ít hơn 72% tham số và tốc độ suy luận nhanh hơn 2 lần.

Từ khóa: Cắt tỉa mô hình, chưng cất kiến thức, nén mô hình, học biểu diễn

1. Giới thiệu
Gần đây, tiền huấn luyện tự giám sát đã trở thành một trong những chủ đề hấp dẫn nhất trong lĩnh vực giọng nói [1, 2]. Với phương pháp này, một lượng lớn dữ liệu không nhãn có thể được sử dụng để huấn luyện một mô hình sâu nhằm trích xuất các biểu diễn mức cao từ âm thanh thô, có thể mang lại cải thiện đáng kể cho nhiều tác vụ xuôi dòng.

Trong khi các mô hình tiền huấn luyện mang lại cải thiện hiệu suất to lớn, chúng cũng đòi hỏi lượng lớn bộ nhớ và sức mạnh tính toán. Các mô hình giọng nói tiền huấn luyện tự giám sát lớn như Wav2vec2 [3], Hubert [4], và WavLM [5] thường có hàng trăm triệu tham số, khiến chúng không phù hợp để sử dụng trên các sản phẩm tiêu dùng như laptop và điện thoại thông minh. Đây là một trở ngại cho việc ứng dụng các mô hình này trong nhiều tình huống thực tế. Do đó, nén mô hình đã trở thành một mối quan tâm chính đối với những mô hình tự giám sát lớn này.

Chưng cất kiến thức thường sử dụng một mô hình giáo viên để hướng dẫn một mô hình sinh viên nhỏ hơn, và cấu trúc của mô hình sinh viên phải được thiết kế cẩn thận để đạt được hiệu suất tốt hơn. DistilHubert [6] chưng cất một mô hình dựa trên Hubert 12 tầng để có được một mô hình sinh viên 2 tầng và giảm đáng kể kích thước mô hình. FitHubert [7], được lấy cảm hứng từ FitNets [8], thiết kế một mạng sinh viên mỏng nhưng sâu để cung cấp khả năng biểu diễn tốt hơn.

Cắt tỉa mô hình cố gắng loại bỏ các trọng số không quan trọng và có được một mạng con từ mô hình tiền huấn luyện. Trong cắt tỉa không có cấu trúc, những trọng số bị loại bỏ này được phân phối ngẫu nhiên trong các ma trận; trong cắt tỉa có cấu trúc, các đơn vị mạng như đầu chú ý hoặc các tầng feed-forward được loại bỏ hoàn toàn. Các mô hình được cắt tỉa có cấu trúc không cần phần cứng được thiết kế đặc biệt để tăng tốc, có thể phù hợp hơn cho các thiết bị tiêu dùng. LightHubert coi việc cắt tỉa mô hình như một bài toán tìm kiếm kiến trúc mạng và giảm đáng kể sự suy giảm hiệu suất, nhưng quá trình tìm kiếm vẫn đòi hỏi một số lựa chọn thủ công tốn thời gian [9]. Peng et al. đề xuất một phương pháp linh hoạt hơn bằng cách áp dụng phương pháp cắt tỉa dựa trên chính quy hóa L0 [10] cho mô hình Wav2vec 2.0, nhưng phương pháp của họ có tính chất đặc thù tác vụ và đi kèm với một số chi phí bổ sung khi áp dụng cho các tác vụ xuôi dòng [11].

Chúng tôi cố gắng sử dụng một phương pháp tương tự dựa trên chính quy hóa L0 để có được một mô hình nén bất khả tri tác vụ. Tuy nhiên, việc học các mặt nạ cắt tỉa sử dụng chính quy hóa L0 trên các tác vụ tiền huấn luyện không giám sát như mã hóa dự đoán tương phản [12] đòi hỏi tài nguyên tính toán lớn. Sự kết hợp giữa chưng cất và cắt tỉa là một giải pháp đầy hứa hẹn [13, 14]. Biểu diễn được cung cấp bởi mô hình tiền huấn luyện không chỉ giảm nỗ lực huấn luyện của các mô hình xuôi dòng mà còn cung cấp thông tin độc lập với tác vụ cho việc cắt tỉa mô hình.

So với các phương pháp cắt tỉa không có cấu trúc hiện tại của các mô hình giọng nói tiền huấn luyện [15, 16], cắt tỉa có cấu trúc thường chịu sự suy giảm hiệu suất lớn hơn [17]. Nút thắt của vấn đề này là việc sử dụng cấu trúc thay vì các trọng số riêng lẻ như đơn vị cơ bản của việc cắt tỉa giảm bậc tự do, dẫn đến việc loại bỏ một số trọng số quan trọng. Để bù đắp cho sự suy giảm hiệu suất, chúng tôi giới thiệu một phương pháp cắt tỉa đầu chú ý chi tiết để cắt tỉa từng đầu chú ý một cách riêng biệt. Để thúc đẩy việc cắt tỉa các cấu trúc thô và tăng tốc thêm cho mô hình đã cắt tỉa, chúng tôi cũng giới thiệu bộ ước lượng thẳng (STE) [18] vào phương pháp cắt tỉa có cấu trúc đa thang [13] dựa trên chính quy hóa L0.

Thí nghiệm trên benchmark SUPERB cho thấy khả năng tổng quát hóa của mô hình đề xuất trên các tác vụ xuôi dòng khác nhau. Với sự hỗ trợ của giáo viên tiền huấn luyện, mô hình đề xuất là bất khả tri tác vụ và có thể được tinh chỉnh trực tiếp cho nhiều tác vụ xuôi dòng. Các thí nghiệm đối chiếu sâu hơn chứng minh hiệu quả của việc cắt tỉa đầu chú ý chi tiết và STE. Mô hình của chúng tôi vượt trội hơn các baseline được chưng cất và đạt được kết quả tương đương với mô hình giáo viên trên nhiều tác vụ, với ít hơn 72% tham số và nhanh hơn 2 lần về tốc độ.

2. Kiến thức nền tảng
2.1. Các mô hình biểu diễn giọng nói tiền huấn luyện
Thí nghiệm của chúng tôi chủ yếu được thực hiện trên WavLM [5], nhưng phương pháp có thể dễ dàng mở rộng cho Wav2vec 2.0 [3], data2vec [19], Hubert [4], và các mô hình khác có cấu trúc tương tự dựa trên transformer.

WavLM là một tập hợp các mô hình tiền huấn luyện tự giám sát tiên tiến nhất. Trong quá trình tiền huấn luyện, các đơn vị phân cụm ngoại tuyến được sử dụng làm mục tiêu huấn luyện và các mô hình học cách biểu diễn các đầu vào liên tục bằng một số đơn vị ẩn rời rạc. WavLM cũng giới thiệu khử nhiễu giọng nói có mặt nạ và thiên lệch vị trí tương đối có cổng để cải thiện hiệu suất.

2.2. Cắt tỉa dựa trên chính quy hóa L0
Cắt tỉa dựa trên chính quy hóa L0 là một trong những phương pháp học mặt nạ. Trong một số phương pháp cắt tỉa, các tham số được loại bỏ theo một số tiêu chí được đặt ra nhân tạo, chẳng hạn như độ lớn của trọng số hoặc gradient. Mặt khác, các phương pháp học mặt nạ có xu hướng coi việc cắt tỉa như một bài toán tối ưu hóa [10].

Như tên gọi, cắt tỉa dựa trên chính quy hóa L0 thêm một mặt nạ vào các tham số (hoặc nhóm tham số) và sử dụng chuẩn L0 của những mặt nạ cắt tỉa này làm số hạng chính quy hóa của hàm mất mát. Ví dụ, trong thí nghiệm của chúng tôi, mục tiêu huấn luyện là:

R(θ,π) = Ez∼q(π)[1/N ∑(i=1 to N) L(fs(xi,θ̃),ft(xi))+λ||θ̃||0],

trong đó fs và ft là các mô hình sinh viên và giáo viên để chưng cất kiến thức, xi là dữ liệu đầu vào thứ i, θ là tập tham số của mô hình sinh viên, z∈{0,1} là tập mặt nạ cắt tỉa, θ̃=θ⊙z là tập tham số sau khi che mặt nạ. Biến ngẫu nhiên rời rạc z tuân theo phân phối Bernoulli q(π).

Tuy nhiên, hàm mục tiêu này không thể được tối ưu hóa bằng các phương pháp gradient descent vì quá trình lấy mẫu z cho q(π) không thể vi phân. Louizos et al. giới thiệu một thủ thuật tái tham số hóa để giải quyết vấn đề này [10]. Sau khi tái tham số hóa, z trở thành một biến liên tục, được xác định bởi một tham số có thể học α và một biến ngẫu nhiên bổ sung u "thu thập" tính ngẫu nhiên từ z. Nói một cách chính thức, z được tính bởi:

u∼U(0,1), s=sigmoid(1/β log(u/(1-u))+log α)
s̄=s(ζ-γ)+γ, z=hardtanh(s̄),

trong đó u được lấy mẫu từ phân phối đều U(0,1), ζ=1.1, γ=-0.1 là 2 hằng số để mở rộng s ra một khoảng lớn hơn và đảm bảo z có thể chính xác là 0 hoặc 1. β kiểm soát nhiệt độ, và α là tham số có thể học.

Hình 1a cho thấy phân phối xác suất của z và s̄, trong khi hình 1b cho thấy giá trị của chúng như hàm của log α. Chúng ta có thể thấy rằng thủ thuật tái tham số hóa biến các mặt nạ rời rạc z thành các biến liên tục trong khi vẫn cho phép chúng chính xác là 0 hoặc 1.

2.3. Cắt tỉa có cấu trúc đa thang
Chính quy hóa L0 không giới hạn độ chi tiết của việc cắt tỉa. Nếu z che mặt nạ một cấu trúc nào đó, chính quy hóa L0 có thể được sử dụng cho cắt tỉa có cấu trúc. Độ chi tiết có thể lớn như một toàn bộ tầng hoặc nhỏ như một chiều nhất định của ma trận trọng số. Gần đây, Xia et al. giới thiệu một phương pháp cắt tỉa đa thang loại bỏ các cấu trúc chi tiết và thô song song để thúc đẩy việc loại bỏ các cấu trúc lớn và đạt được tăng tốc thêm [13]. Chúng tôi đã giới thiệu phương pháp này để tăng khả năng loại bỏ các cấu trúc thô nhằm bù đắp cho những tác động tiêu cực tiềm tàng của phương pháp cắt tỉa đầu chú ý chi tiết đối với tốc độ suy luận của mô hình.

3. Phương pháp
3.1. Cắt tỉa đầu chú ý chi tiết
Trong các công trình trước đây [11, 13], các đầu chú ý được sử dụng làm đơn vị nhỏ nhất để cắt tỉa. Điều này có thể giảm bậc tự do của việc cắt tỉa và dẫn đến sự suy giảm hiệu suất nhiều hơn. Để làm cho cắt tỉa có cấu trúc linh hoạt hơn, chúng tôi đề xuất một phương pháp chú ý chi tiết để cắt tỉa riêng biệt từng chiều của các ma trận trong tầng chú ý dựa trên phương pháp cắt tỉa có cấu trúc đa thang của Xia et al. [13]. Nói một cách chính thức, một khối transformer được che mặt nạ như sau:

fMHA(X) = zMHA · concat(fATT(X))
fATT(X) = Sc·(XWiV)·diag(zivo)
Sc = softmax((XWiQ)·diag(ziqk)·(XWiK)T)
fFFN(X) = zFFN·gelu(XWU)·diag(zint)·WD,

trong đó X là dữ liệu đầu vào, WiQ, WiK, WiV, WO là các ma trận truy vấn, khóa, giá trị và đầu ra, tương ứng. zMHA, ziqk, zivo, zFFN, zint ký hiệu mặt nạ cắt tỉa cho các tầng chú ý đa đầu, ma trận chú ý, tầng feed-forward, và các chiều trung gian. Chúng tôi bỏ qua các yếu tố tỷ lệ trong fATT(X) để rõ ràng, và xin lưu ý rằng WO cũng nên được cắt tỉa theo zivo. Đối với WQ, WV∈Rdhidden×dhead, ziqk và zivo sẽ có dhead biến.

3.2. Tối ưu hóa mặt nạ cắt tỉa với STE
Mặc dù thủ thuật tái tham số hóa làm cho z có thể vi phân, việc giới thiệu hardtanh trong Eq. 2 tạo ra một trở ngại mới cho việc tối ưu hóa. Như được thể hiện trong Hình 1b, khi log α lấy giá trị trong vùng có bóng, sự hiện diện của hardtanh làm cho ∂z/∂s = 0, và tham số có thể học α không thể được cập nhật. Nghĩa là, mô hình quyết định giữ một cấu trúc khi z là 1, nhưng nó không thể đánh giá quyết định đó.

Vấn đề này trở nên rõ ràng hơn đối với cắt tỉa có cấu trúc đa thang. Hình 3a cho thấy giá trị trung bình của zFFN không thay đổi trong quá trình huấn luyện, khiến cắt tỉa đa thang trở nên không hiệu quả. Lý do có thể là trong giai đoạn đầu của việc huấn luyện, việc cắt tỉa toàn bộ tầng FFN có thể dẫn đến sự suy giảm hiệu suất to lớn, vì vậy α có thể được tối ưu hóa thành một giá trị dương lớn, và khó cập nhật trong các bước huấn luyện còn lại.

Việc thất bại trong việc cắt các cấu trúc quy mô thô sẽ khiến trọng số thưa thớt của mô hình cắt tỉa quá phân tán, dẫn đến tỷ lệ tăng tốc thấp hơn. Để giải quyết vấn đề này, chúng tôi áp dụng bộ ước lượng thẳng [18] để đảm bảo rằng gradient có thể đi qua hàm hardtanh trong Eq. 2. Vì các gradient từ STE không phải là gradient cho hàm mất mát, việc tối ưu hóa theo hướng này có thể không dẫn đến sinh viên chính xác nhất và có thể gây ra bất ổn gần một số cực tiểu cục bộ [20]. Để tính ổn định của việc huấn luyện, chúng tôi định nghĩa gradient của STE như sau:

∂L/∂s̄ = {
1, nếu ∂L/∂z >= 1;
-1, nếu ∂L/∂z < -1;
∂L/∂z, ngược lại.
}

3.3. Mục tiêu huấn luyện
Các trạng thái ẩn của các tầng khác nhau chứa các loại thông tin khác nhau [6, 21]. Do đó, chúng tôi theo Xia et al. [13] để sử dụng chưng cất kiến thức đa tác vụ có thể học để học biểu diễn của các tầng khác nhau. Chúng tôi cũng theo Wang et al. để thay đổi số hạng thứ 2 ở vế phải của eq. 1 thành một số hạng Lagrangian để kiểm soát tính thưa thớt tốt hơn [22]. Mục tiêu huấn luyện của chúng tôi như sau:

L = 1/N ∑(i=0 to N) ∑((j,k)∈D) LMSE(hji, ĥki) + λ1(p̂-p) + λ2(p̂-p)²,

trong đó p̂ là tính thưa thớt mô hình xấp xỉ, p là tính thưa thớt mục tiêu. λ1 và λ2 là các tham số có thể học cho chính quy hóa Lagrangian. D là mối quan hệ ghép cặp tầng giáo viên-sinh viên được học trong quá trình huấn luyện [13], đối với mẫu i, hji và ĥki là đầu ra của tầng j/k của mô hình sinh viên và giáo viên, tương ứng.

4. Thí nghiệm
4.1. SUPERB
SUPERB (Speech processing Universal PERformance Benchmark) là một benchmark để đánh giá hiệu suất của các mô hình tiền huấn luyện giọng nói [23]. SUPERB cung cấp 10 tác vụ giọng nói được định nghĩa trước từ các góc độ khác nhau, trong đó các mô hình tiền huấn luyện được sử dụng như bộ trích xuất đặc trưng upstream. Những tác vụ này bao gồm nhận dạng âm vị (PR), nhận dạng giọng nói tự động (ASR), phát hiện từ khóa (KS), phát hiện thuật ngữ nói theo mẫu (QbE), nhận dạng người nói (SID), xác thực người nói tự động (SV), phân tách người nói (SD), phân loại ý định (IC), điền slot (SF), và nhận dạng cảm xúc (ER).

4.2. Thiết lập cắt tỉa
Mô hình. Mô hình của chúng tôi được khởi tạo từ mô hình cơ sở WavLM, bao gồm một bộ trích xuất đặc trưng CNN 7 tầng và một bộ mã hóa transformer 12 tầng. Đối với các ma trận trong Eq. 3, WiQ, WiK, WiV ∈ R768×64, WO ∈ R768×768, WU ∈ R768×3072, và WD ∈ R3072×768. Đối với mỗi khối transformer, chúng tôi có 12 đầu chú ý, dẫn đến 12*64 = 768 phần tử trong zqk và zvo. Chúng tôi cũng có 3072 phần tử trong zint cho mỗi chiều trong tầng FFN, và 1 phần tử trong zMHA và zFFN để che mặt nạ toàn bộ tầng. Tính thưa thớt cắt tỉa mục tiêu được đặt ở 80%. Mô hình giáo viên của chưng cất kiến thức cũng là mô hình cơ sở WavLM.

Dữ liệu. Chúng tôi sử dụng corpus Librispeech [24] 960 giờ để cắt tỉa. Đối với các tác vụ SUPERB, chúng tôi sử dụng tập dữ liệu theo hướng dẫn chính thức.

Cắt tỉa. Cắt tỉa được thực hiện trên GPU RTX 3090 trong 200k bước và mất khoảng 36 giờ. Các siêu tham số huấn luyện của chúng tôi được chọn theo DistilHuBERT [6] và Xia et al. [13]. Tốc độ học tăng tuyến tính đến 2.0e-4 trong 7% bước đầu tiên và giảm tuyến tính về 0 trong các bước còn lại, và tính thưa thớt mục tiêu tăng tuyến tính đến 80% trong 7% bước đầu tiên và giữ nguyên cho phần còn lại.

5. Kết quả
Bảng 1 cho thấy kết quả đánh giá trên các tác vụ xuôi dòng SUPERB. Mô hình của chúng tôi có hiệu suất tương đương với mô hình giáo viên trong các tác vụ KS, IC, ER, SV, và SD, chứng minh hiệu quả của phương pháp chúng tôi. Sự suy giảm hiệu suất chủ yếu xảy ra trong các tác vụ PR, ASR, và SF. Những tác vụ này đòi hỏi thông tin liên quan đến nội dung phức tạp hơn, có khả năng bị mất trong quá trình cắt tỉa. Sử dụng cùng mô hình giáo viên cơ sở WavLM, phương pháp của chúng tôi vượt trội hơn các mô hình được chưng cất trong hầu hết các tác vụ, đặc biệt là trong các tác vụ liên quan đến nội dung như ASR, cho thấy rằng mô hình của chúng tôi bảo tồn tốt hơn hiệu suất của mô hình giáo viên.

Ngoài các thước đo cụ thể theo tác vụ, chúng tôi cũng sử dụng điểm SUPERB (superbs) để cung cấp một đánh giá tổng thể. Điểm SUPERB là trung bình của các phép biến đổi tuyến tính của tất cả các thước đo cụ thể theo tác vụ, và được xác định bởi mô hình SOTA trên benchmark và một baseline FBANK được định nghĩa trước. Tại thời điểm viết bài, mô hình SOTA là WavLM-Large. Nói một cách chính thức, điểm SUPERB được định nghĩa là:

superbs = 1/T ∑(t∈T) 1000/(msota_t - mfbank_t) * (mu_t - mfbank_t),

trong đó mu_t là thước đo của tác vụ t và mô hình u, superbs(sota) ≡ 1000, superbs(fbank) ≡ 0.

Hình 2 cho thấy mối quan hệ giữa điểm SUPERB và số lượng tham số. Mô hình của chúng tôi vượt trội đáng kể so với các mô hình chưng cất có số lượng tham số tương tự, và thậm chí có hiệu suất vượt trội so với mô hình cơ sở Wav2vec 2.0. Những kết quả này cho thấy rằng phương pháp đề xuất đạt được sự cân bằng tốt hơn giữa hiệu suất và số lượng tham số so với phương pháp dựa trên chưng cất.

Chúng tôi cũng so sánh phương pháp của mình với phương pháp cắt tỉa trước đây loại bỏ trực tiếp các đầu chú ý (w/o FAHP trong Bảng 1). Một lần nữa, sự cải thiện chủ yếu được phản ánh trong các tác vụ liên quan đến nội dung như ASR, cho thấy rằng cắt tỉa đầu chú ý chi tiết có thể giúp bù đắp cho việc mất thông tin phức tạp trong cắt tỉa có cấu trúc.

Hình 3a cho thấy giá trị trung bình của các mặt nạ cắt tỉa zFFN và zMHA trong quá trình cắt tỉa. Bằng cách giới thiệu STE, các mặt nạ cắt tỉa của các cấu trúc thô thay đổi thường xuyên hơn và cuối cùng giảm xuống giá trị thấp hơn, điều này chứng minh hiệu quả của STE. Hình 3b cho thấy sự phân bố của các trọng số còn lại của mỗi tầng sau khi cắt tỉa. Vì các cấu trúc thô có thể được loại bỏ hoàn toàn, các tham số còn lại có xu hướng tập trung, dẫn đến tăng tốc thêm.

Ngoài ra, trọng số còn lại tập trung ở đầu mạng. Vì thông tin liên quan đến nội dung nổi bật hơn trong các đặc trưng của các tầng trên, sự phân bố này của trọng số còn lại có thể là một trong những lý do cho sự cải thiện của mạng trong các tác vụ liên quan đến nội dung.

Chúng tôi cũng đo thời gian suy luận của 2 mô hình trên. Bảng 2 cho thấy tác động tốc độ của STE. Có thể thấy rằng sự phân bố trọng số tập trung do STE mang lại cải thiện đáng kể tốc độ suy luận của mô hình. Với STE, mô hình được cắt tỉa nhanh hơn 1.4 lần với số lượng tham số tương tự.

Hơn nữa, chúng tôi cho thấy tác động của STE lên độ chính xác. Trong số 4 tác vụ này, STE mang lại cải thiện trong ASR và IC, trong khi gây ra suy giảm trong ER và SID, nhưng cả ảnh hưởng tích cực và tiêu cực đều không đáng kể. Sự suy giảm trong ER và SID có thể do các tham số bị loại bỏ khỏi các tầng thấp hơn liên quan đến thông tin người nói hoặc cảm xúc.

6. Kết luận
Trong bài báo này, chúng tôi trình bày một phương pháp cắt tỉa có cấu trúc bất khả tri tác vụ của các mô hình biểu diễn giọng nói tiền huấn luyện. Bằng cách sử dụng cắt tỉa đầu chú ý chi tiết, chúng tôi duy trì khả năng biểu diễn thông tin mức nội dung và giảm sự suy giảm hiệu suất do cắt tỉa có cấu trúc. Chúng tôi giới thiệu STE vào cắt tỉa có cấu trúc đa thang để tăng tốc thêm cho mô hình. Các thí nghiệm của chúng tôi chứng minh rằng mô hình đề xuất giảm 72% tham số trong khi có hiệu suất tương đương với mô hình dày đặc trong nhiều tác vụ, và vượt trội hơn mô hình cơ sở Wav2vec2 về hiệu suất trung bình.

--- TRANG 2 ---
[Nội dung hình ảnh và biểu đồ]

--- TRANG 3 ---
[Nội dung hình ảnh và biểu đồ]

--- TRANG 4 ---
[Bảng kết quả và biểu đồ]

--- TRANG 5 ---
7. Tài liệu tham khảo
[1] A. Mohamed, H.-y. Lee, L. Borgholt, J. D. Havtorn, J. Edin, C. Igel, K. Kirchhoff, S.-W. Li, K. Livescu, L. Maaløe, T. N. Sainath, và S. Watanabe, "Self-supervised speech representation learning: A review," IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179–1210, Oct. 2022.

[2] J. Zhao và W.-Q. Zhang, "Improving Automatic Speech Recognition Performance for Low-Resource Languages With Self-Supervised Models," IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1227–1241, Oct. 2022.

[3] A. Baevski, Y. Zhou, A. Mohamed, và M. Auli, "Wav2vec 2.0: A framework for self-supervised learning of speech representations," Advances in Neural Information Processing Systems, vol. 33, pp. 12449–12460, 2020.

[4] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, và A. Mohamed, "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.

[5] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., "WavLM: Large-scale self-supervised pre-training for full stack speech processing," IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022.

[6] H.-J. Chang, S.-w. Yang, và H.-y. Lee, "DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit bert," trong ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7087–7091.

[7] Y. Lee, K. JANG, J. Goo, Y. Jung, và H.-R. Kim, "FitHuBERT: Going thinner and deeper for knowledge distillation of speech self-supervised learning," trong 23rd Annual Conference of the International Speech Communication Association, INTERSPEECH 2022. ISCA, 2022, pp. 3588–3592.

[8] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, và Y. Bengio, "Fitnets: Hints for thin deep nets," arXiv preprint arXiv:1412.6550, 2014.

[9] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, và H. Li, "LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT," trong Interspeech 2022. ISCA, Sep. 2022, pp. 1686–1690.

[10] C. Louizos, M. Welling, và D. Kingma, "Learning sparse neural networks through l0 regularization." trong Sixth International Conference on Learning Representations, 2018, 2018.

[11] Y. Peng, K. Kim, F. Wu, P. Sridhar, và S. Watanabe, "Structured pruning of self-supervised pre-trained models for speech recognition and understanding," trong 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Jun. 2023, pp. 1–5.

[12] A. v. d. Oord, Y. Li, và O. Vinyals, "Representation learning with contrastive predictive coding," arXiv preprint arXiv:1807.03748, 2018.

[13] M. Xia, Z. Zhong, và D. Chen, "Structured pruning learns compact and accurate models," trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 1513–1528.

[14] V. Sanh, T. Wolf, và A. Rush, "Movement pruning: Adaptive sparsity by fine-tuning," Advances in Neural Information Processing Systems, vol. 33, pp. 20378–20389, 2020.

[15] M. Yang, A. Tjandra, C. Liu, D. Zhang, D. Le, và O. Kalinli, "Learning ASR pathways: A sparse multilingual ASR model," trong 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Jun. 2023, pp. 1–5.

[16] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, và J. Glass, "PARP: Prune, adjust and re-prune for self-supervised speech recognition," Oct. 2021, arXiv:2106.05933 [cs, eess].

[17] Z. Liu, M. Sun, T. Zhou, G. Huang, và T. Darrell, "Rethinking the value of network pruning," trong International Conference on Learning Representations, 2018.

[18] Y. Bengio, N. Léonard, và A. Courville, "Estimating or propagating gradients through stochastic neurons for conditional computation," arXiv preprint arXiv:1308.3432, 2013.

[19] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, và M. Auli, "Data2vec: A general framework for self-supervised learning in speech, vision and language," trong International Conference on Machine Learning. PMLR, 2022, pp. 1298–1312.

[20] P. Yin, J. Lyu, S. Zhang, S. J. Osher, Y. Qi, và J. Xin, "Understanding straight-through estimator in training activation quantized neural nets," trong International Conference on Learning Representations, 2019.

[21] L. Chen, M. Asgari, và H. H. Dodge, "Optimize Wav2vec2s architecture for small training set through analyzing its pre-trained models attention pattern," trong 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2022, pp. 7112–7116.

[22] Z. Wang, J. Wohlwend, và T. Lei, "Structured pruning of large language models," trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 6151–6162.

[23] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, và H. yi Lee, "SUPERB: Speech Processing Universal PERformance Benchmark," trong Proc. Interspeech 2021, 2021, pp. 1194–1198.

[24] V. Panayotov, G. Chen, D. Povey, và S. Khudanpur, "Librispeech: an ASR corpus based on public domain audio books," trong 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206–5210.

[25] S. Schneider, A. Baevski, R. Collobert, và M. Auli, "wav2vec: Unsupervised Pre-Training for Speech Recognition," trong Proc. Interspeech 2019, 2019, pp. 3465–3469.
