# 2306.17165.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2306.17165.pdf
# File size: 3215497 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
An Efficient General-Purpose Modular Vision Model
via Multi-Task Heterogeneous Training
Zitian Chen1, Mingyu Ding2, Yikang Shen3, Wei Zhan2,
Masayoshi Tomizuka2, Erik Learned-Miller1, Chuang Gan1,3
1University of Massachusetts Amherst,2University of California Berkeley,3MIT-IBM Watson AI Lab
Abstract
We present a model that can perform multiple vision tasks and can be adapted to
other downstream tasks efficiently. Despite considerable progress in multi-task
learning, most efforts focus on learning from multi-label data : a single image
set with multiple task labels. Such multi-label data sets are rare, small, and ex-
pensive. We say heterogeneous to refer to image sets with different task labels,
or to combinations of single-task datasets. Few have explored training on such
heterogeneous datasets. General-purpose vision models are still dominated by
single-task pretraining, and it remains unclear how to scale up multi-task models
by leveraging mainstream vision datasets designed for different purposes. The
challenges lie in managing large intrinsic differences among vision tasks, including
data distribution, architectures, task-specific modules, dataset scales, and sam-
pling strategies. To address these challenges, we propose to modify and scale up
mixture-of-experts (MoE) vision transformers, so that they can simultaneously learn
classification, detection, and segmentation on diverse mainstream vision datasets
including ImageNet, COCO, and ADE20K. Our approach achieves comparable
results to single-task state-of-the-art models and demonstrates strong generalization
on downstream tasks. Due to its emergent modularity, this general-purpose model
decomposes into high-performing components, efficiently adapting to downstream
tasks. We can fine-tune it with fewer training parameters, fewer model parameters,
and less computation. Additionally, its modularity allows for easy expansion in
continual-learning-without-forgetting scenarios. Finally, these functions can be
controlled and combined to meet various demands of downstream tasks.
1 Introduction
Comprehensive visual understanding demands a general-purpose model capable of performing
diverse vision tasks. With a similar goal, multitask learning (MTL), which enables the simultaneous
training of models on multiple tasks and allows them to leverage shared information, has been
explored extensively. Most MTL efforts [ 3,37,26] have been made by learning from multi-label
datasets, where each input has multiple different types of annotations. However, such data sets with
multiple annotations are often impractical to obtain. And the mainstream classification, detection,
and segmentation datasets (ImageNet [ 5], COCO [ 22], and ADE20K [ 47]) have no overlapping
images. Hence the current paradigm for general-purpose vision models is still dominated by single-
task pretraining ( e.g., image classification [ 23], self-distillation [ 2], or multi-modal contrastive
learning [ 41]) and then fine-tuning on downstream tasks. A detailed demonstration of different
schemes of pre-training is shown in Fig. 1.
The previous work Mod-Squad [ 3] proposes to use a mixture-of-experts (MoE) and a mutual infor-
mation loss to address task conflict in MTL. However, it oversimplifies some task-specific network
designs and the success of this model heavily relies on multi-label datasets, which are difficult to
Preprint. Under review.arXiv:2306.17165v1  [cs.CV]  29 Jun 2023

--- PAGE 2 ---
1. Train from Scratch
Model 1
Cat
Model 2
Model 3ImageNetCOCOADE20K
3. Multi-task Multi-label Training 
Taskonomy
Model
2. Pre-train then Finetune
Model
CatImageNet
Model
COCOInit4. Multi-task Heterogeneous Training 
Cat
Model 
ImageNetCOCOADE20K
Figure 1: Different ways of training. (1) Train from Scratch: Train a model for a single task from
scratch. (2) Pre-train then Finetune: Pre-train a model on one dataset and later fine-tune the model on
other datasets. (3) Multi-task Multi-label Training: Train a model that can produce multiple types
of outputs simultaneously. The dataset is expected to have multiple annotations for different tasks
on each training image. (4) Multi-task Heterogeneous Training (MTHT): Train a model that can
produce different types of outputs corresponding to each task. The model can make use of training
data designed for any single task. It can use these in combination to achieve multi-task training.
obtain and scale up. Therefore, it remains unclear: 1) How to scale up this MTL model for multi-task
heterogeneous training on conventional computer vision datasets; 2) Whether this model can be
utilized as a general-purpose vision backbone that can be easily adapted to many downstream tasks;
3) Whether we can leverage the success of single-task methods instead of removing complicated
modules and simplifying the task-specific sub-network.
Another issue is that previous large-scale vision models [ 3,23,41,2] do not consider fast adaptation
on downstream tasks. One common limitation of large-scale models is that adapting to downstream
tasks requires updating all parameters, which can be prohibitively expensive in terms of time and
computational resources. For example, large models like GPT-3 [ 1] with 175B parameters, can take
months to train, making adapting the whole model for a tiny new task impractical. Therefore, efficient
adaptation is an important practical feature for successful model deployment.
To address these problems, we build a large-scale multi-task heterogeneous training framework
based on a modular vision transformer that can simultaneously do three fundamental vision tasks:
classification, detection, and segmentation. We refer to this framework as Multi-task Heterogeneous
Learner (MTHL) . Benefiting from a more diverse training set designed for multiple purposes, the
framework generalizes better and is semantically rich enough for rapid downstream adaptation, which
is often hard to obtain from a single (homogeneous) pre-training task/dataset.
We also address efficient adaptation by leveraging the strong modularity in our model. As shown in
Fig. 2, MTHL can adapt efficiently in several aspects including reducing training parameters, model
parameters, and computational cost. The mixture-of-experts module enables the model to select the
most semantically meaningful part for faster transferring to downstream tasks by simply learning
new routers. Further, the model can easily expand by adding experts to address continual learning.
Our main contributions can be summarized as follows:
•Large-scale multi-task heterogenous training. We explore heterogeneous training on three
fundamental computer vision tasks: classification, detection, and segmentation with mainstream
vision datasets. We demonstrate that one model to perform three tasks can be on par with
single-task state-of-the-art.
•Strong generalization on downstream datasets. Heterogeneous training provides the advan-
tage of diverse perception and the ability to handle a wider range of scenarios, leading to better
generalization to downstream datasets.
•Modular adaptation with efficiency. The emergence of modularity allows for flexible control
of architecture and provides several straightforward and efficient ways to adapt the architecture.
•Continual learning without forgetting. The model can effortlessly leverage existing experts to
adapt to new tasks by learning new routers. Additionally, it can incorporate new experts without
disrupting the current architecture, thus avoiding catastrophic forgetting.
2

--- PAGE 3 ---
MoE
Module
Expert
Router D
1
Router D
2
Router 
D
n
A. Multi
-
task heterogenous training 
with dataset
-
specific routers
B. Modular adaptation on 
downstream datasets
MoE
Module
Expert
New Router
Reduce training parameters
Freeze most 
of the experts
MoE
Module
Expert
New Router
Reduce model parameters
Remove experts with 
low using frequency
MoE
Module
Expert
New Router
Reduce computation cost
Set Top
-
K=2 
instead of Top
-
K=3
MoE
Module
Expert
New Router
Simple model expansion
Add new experts
FreezeCat
Dataset 1Dataset 2Dataset nClassificationDetectionSegmentationFigure 2: Efficient modular adaption. Strong modularity facilitates efficient adaptation to new
datasets: 1) Reduce training parameters by only learning new routers and a few optional experts
while freezing other parameters. 2) Reduce model parameters via learning new routers and removing
rarely selected experts. 3) Reduce computation cost via learning new routers with a smaller Top-K
that makes fewer experts chosen in one forward pass. 4) Simple model expansion via adding and
learning a few new experts per MoE module while freezing old experts. Above ways of adaptation
can be combined to suit specific needs.
2 Related Work
Multi-task Learning. Multi-task learning [ 15] jointly learns multiple related tasks with a sin-
gle model. Recently, transformer-based MTL architectures [ 37] have gained popularity. Some
works [ 14,25] attempt to unify the input and output space for different tasks. Some works [ 3,37,26]
remove complicated task-specific module for simplicity and conducts multi-task learning on a multi-
label dataset. However, these works either rely on a single multi-label dataset or lose some perceptual
feature when removing the task-specific modules and unifying the input/output [ 14,25]. While
Ubernet[17] adapts a CNN-based framework that can learn from multiple datasets, it struggles with
task conflicts and tends to have lower performance when learning from more than one dataset, which
makes it hard to generalize to downstream applications. In contrast, MTHL can learn from diverse
datasets and still achieve comparable performance to state-of-the-art single-task methods. Addition-
ally, it can leverage the success of single-task methods by adopting similar designs, including unique
task-specific modules (e.g., anchor generator), data pre-processing, and complicated engineering
techniques (e.g., non-maximum suppression). These designs are non-trivial but necessary to achieve
the best model.
Mixture of Experts (MoE). Jacobs et al. [ 13] introduced the MoE as a method to merge sub-models
and perform conditional computation. Recently, this technique has been commonly used to decrease
computation costs while maintaining a large model capacity [ 32]. Some studies [ 19,8,30,27] have
leveraged MoE to train massive models with trillions of parameters at a relatively low computation
cost. In contrast, we utilize this technique primarily to manage the sub-models and conduct the
modular adaptation on downstream tasks.
Parameter-efficient transfer learning. The Adapter technique was proposed as a standalone layer
that can be integrated into an existing neural network for efficient transfer. LoRA [ 11] utilizes a
bottleneck structure to enforce a low-rank constraint on the weight updates. Other approaches integrate
CLIP-based adapters [ 9,38,45], upsampling and downsampling modules [ 20], and additional bias
parameters [ 42] to reduce training parameters during fine-tuning. Our work focuses on selecting
the most semantically related part of the model and adapting to downstream tasks efficiently. No
additional newly designed module is required.
Continual learning. Continual learning involves handling a diverse set of tasks and accumulating
knowledge through a series of training. Recent efforts have been made to address catastrophic
forgetting, including imposing regularization [ 16,44,31] and retaining a small buffer of data for
replay [ 24,28]. Some approaches [ 39,12] dynamically expand the network by adding neurons to each
MLP or convolution layer. In contrast, our modular design enables straightforward well-organized
expansion by adding experts and learning new routers. Moreover, since each dataset has its router,
3

--- PAGE 4 ---
the experts added will not be chosen by the previous dataset. Unlike other expansion techniques, our
approach does not suffer from catastrophic forgetting.
3 Method
We start with the definition of multi-task heterogeneous training. Suppose we have Mdatasets D1,
D2, ...,DM. Each dataset contains a set of training pair {I;Ti(I)}andTiis the task on dataset Di
that map images ItoTi(I). Here, we assume each dataset only has one task to do for simplicity.
Multi-task heterogeneous training is to learn a joint model on the Mdatasets at once.
3.1 Preliminary
Mixture-of-Experts (MoE). A MoE layer contains a group of expert networks E1, E2, ..., E Nand
a routing network G. The routing network Gcalculates the weight Gk(x)for each expert Ekgiven
input xand the output of an MoE layer is the weighted sum of the output of every expert Ek(x).
Formally, the output of an MoE layer is
y=NX
k=1Gk(x)Ek(x). (1)
The routing network Gis a Top- KRouting network [ 32] that only Kexperts with the highest weight
contribute to the final output:
G(x) = TopK(Softmax( xWg), k) (2)
where TopK( ·, k)sets all elements in the vector to zero except the elements with the largest K
values.
Mutual information loss. Mod-Squad [ 3] proposes a mutual information loss as an auxiliary loss to
better assign experts to tasks so that each expert is more likely to be used for a fixed set of tasks. In
contrast, the key motivation in MTHL is to encourage experts to specialize on datasets and then when
adapting to downstream tasks, the downstream datasets are more likely to activate a small subset of
experts. So we have Mdataset-specific routing networks and modify the loss so that the experts are
assigned to datasets instead of tasks:
LMI=−MX
i=1KX
j=1P(Di, Ej) logP(Di, Ej) +MX
i=1P(Di) logP(Di) +KX
j=1P(Ej) logP(Ej).
(3)
As in [ 3], we assume that P(Di) =1
Mas we want all datasets to be considered equally important.
We have P(Ej|Di) =P
x∈DiGj
i(x)where Gj
iis the weight of expert Ejfor dataset Di. With
P(Ej|Di), we can get P(Di, Ej) =P(Ej|Di)P(Di)andP(Ej) =PM
i=1P(Di, Ej).
3.2 Multi-Task Heterogeneous Training
Backbone architecture. Our multi-task heterogeneous training is a general framework that is
orthogonal to model architecture design. All Transformer or MLP-based structures are applicable. In
this work, we choose two recent state-of-the-art transformer architectures for both image classification
and dense prediction tasks as our backbone: Swin-Transformer [ 23] and DaviT [ 6]. We replace the
MLP layers in these two models with MoE MLP layers.
Task-specific module. Vision tasks require specific designs of modules to process the data and
different ways of perception have a huge impact on performance. While recent studies [ 25,14] tend
to use a shared task module for all tasks, we believe that the inherent differences in vision tasks
make it difficult for a shared module to capture the essential information for all tasks. Thus, MTHL
incorporates all task-specific designed modules (e.g., feature pyramid network), with only a backbone
transformer shared among all tasks.
Sampling strategy. Data sampling plays a crucial role in heterogeneous training. Datasets could have
varying scale levels with huge gaps in batch size. For example, while a single GPU may work with
4

--- PAGE 5 ---
128 samples on image classification, it could only afford 2 samples on detection and segmentation.
Most multi-task frameworks [ 3,37,17] tend to update the network after forwarding for all tasks.
However, such approaches are impractical as the GPU memory is heavily consumed when activating
all dense vision modules, e.g., detection head and segmentation head. Also, forwarding samples from
all tasks in one batch is not scalable when having more tasks.
To address the above issue, MTHL adopts a two-step sampling. We first apply weighted sampling to
select one out of the Mdatasets, then randomly sample a batch of data from the chosen dataset. The
weight assigned to each dataset Difor sampling is denoted as wsample i, which can be pre-defined by
the total number of iterations required for convergence in single dataset training, with some empirical
tuning. Note that for relatively small datasets, a sufficiently large weight should be assigned to
prevent degeneration caused by overtraining on other datasets.
Optimization and convergence. Each task in our framework is associated with its unique module
designed to process the data and its own loss. The losses on datasets Diare weighted and alternately
optimized with a predetermined weight wlifor each dataset. One challenge in optimization is
the presence of gradient conflicts between different tasks. These conflicts interfere with the joint
optimization and slow down the convergence. It is also not uncommon to observe that one task
dominates the training process while others lag behind. We find that well-defined loss weights and
sampling weights contribute to the stabilization of training, and the large batch optimizer Lamb [ 40]
works well in heterogeneous training. Effective convergence in heterogeneous training requires
approximately 50 percent more iterations than the combined number of iterations for each individual
single-task training. These additional training iterations account for the complexity introduced by
joint optimization over diverse vision tasks.
New mutual information loss for multi-task heterogeneous training. In Mod-Squad [ 3], the
mutual information loss in Equ. 3 can be calculated in each batch as all tasks are contained in one
batch. However, calculating P(D, E )andP(E)within a sampled batch from one random dataset in
heterogeneous training leads to heavy bias. To deal with this, we use an approximation inspired by
the following idea:
∂
∂x[xlogx] = 1 + log x=∂
∂x[(1 + log c)x]|c=x. (4)
This suggests that if we replace xlogxwith(1 + log c)x, and cis a good approximation of x, then
we will still have a similar gradient. In our case, we will approximate a running estimate of the joint
distribution of P(D, E )with a buffer B(D, E ). The running estimate B(D, E )avoids the heavy
bias caused by estimating P(D, E )from a single task data set. In each forward pass when we sample
dataset Di, we momentum update B(Di, E)with a momentum of 0.98. This keeps the estimate of B
close to that of the desired joint distribution. Using this idea, we rewrite Eq. 3 and use the resulting
equation as the loss function to calculate the gradient. The equation is given by:
LMI=−MX
i=1KX
j=1[1 + log B(Di, Ej)]P(Di, Ej) +KX
j=1[1 + log(MX
i=1B(Di, Ej))]P(Ej).(5)
Here, P(Di, Ej), P(Ej)is calculated in each forward pass backpropping gradients. If Diis not
sampled in the current forward pass, P(Di, Ej)is set to 0. Note that P(Di) logP(Di)is ignored as
a constant. When adapting to new downstream datasets, the buffer still memorizes P(D, E )for old
datasets. Therefore, the MI loss can still be computed to balance experts on new datasets, which is
not applicable in [3].
3.3 Efficient Adaptation on Downstream Tasks
Mod-squad [ 3] explores modular design in multi-task learning, which helps mitigate task conflicts and
enables the extraction of sub-models for specific tasks. However, learning from a single multi-label
dataset, its applicability is limited to scenarios similar to the pre-trained dataset, thereby restricting its
generalizability across diverse vision datasets. Consequently, it’s hard for the method to gain enough
benefit from multi-task training, and its downstream performance is somehow restrained.
In comparison, we scale up multi-task learning to mainstream vision datasets, leading to better
generalizations on downstream tasks. Benefiting from the strong modularity, MTHL can be easily
decomposed into high-performing components and also allows for a more flexible selection of
5

--- PAGE 6 ---
semantically meaningful components when transferring to downstream tasks, ensuring efficient
adaptation capabilities.
MTHL has two appealing advantages: 1) Downstream applications can select experts that best
match the downstream scenario. This can be done by learning a new router in each MoE module to
find good experts for the downstream task. We consider an expert as a good expert if it is chosen with
a high frequency by the router on the downstream dataset. The routers are very lightweight (0.4M in
parameters) and can quickly converge to the optimum while freezing all other parameters. 2) We can
easily control the architecture within each small component (a small MoE module). It is easy to
expand or prune the model by simply adding or removing experts. This flexibility enables efficient
customization of the model based on the specific requirements of the task at hand.
With these two advantages, we can do efficient fine-tuning in the following aspects as shown in Fig. 2:
1) fewer training parameters . The model only needs to learn a new router for the downstream
dataset and optionally fine-tune a few experts in each MoE module. 2) fewer model parameters .
After learning a new router for the new downstream dataset, we can rank experts according to the
frequency chosen by the routers. Then, we can remove some of the experts rarely being used. 3) lower
computation cost . The new router for the downstream dataset can be learned with a smaller Top-K.
So that fewer experts are chosen during one forward pass and can greatly reduce the computation
cost and inference latency. Note that all these ways of efficient adaptation can be combined together
to meet the demands of downstream datasets.
3.4 Continual Learning
The strong modularity also enables simple model expansion and helps conduct continual learning.
Specifically, we directly add Cexperts in each MoE module along with new task-specific routers
every time learning a new task. We train on the new task but freeze all parameters except for the newly
added part. There are three main advantages of this approach: 1) No catastrophic forgetting. As all
the experts are unchanged after learning and the newly added experts will not be chosen by the router
of previous tasks, there is no catastrophic forgetting. 2) Well-organized architecture and knowledge
reuse. The model still keeps an elegant modularized design. The routers select experts to reuse
knowledge related to the new task and ignore experts with unrelated expertise. 3) The computation
cost is constant. Other expanding methods [ 39,12] add both computation cost and capacity to the
existing model, while our approach only adds capacity. This makes our approach expandable with a
large number of tasks.
4 Experiments
4.1 Multi-task heterogeneous training.
We conduct three fundamental vision tasks (classification, detection, and segmentation) on three
datasets: ImageNet-1K [ 5], COCO [ 22], and ADE20K [ 47]. For the downstream datasets, we
evaluate classification on the scene dataset Places-365 [ 46] (P365), the popular fine-grained dataset
iNaturalist-2018 [ 34] (iNat18), the pet dataset Pets [ 29], the fine-grained bird dataset CUB [ 35], and
the car dataset Cars [ 18]. We evaluate downstream detection on PASCAL VOC [ 7] and downstream
segmentation on Cityscapes [4] and NYU [33].
Models and baselines. We utilize Swin Transformer [ 23] and DaViT [ 6] as our backbone trans-
formers, with results reported on three different sizes: tiny (T), small (S), and base (B). Each task
has its own task-specific head. For classification, we use a single linear layer. For detection, we
use the retina head [ 21]. For segmentation, we use the UperNet [ 36]. Each task follows its own
input and output format based on single-task methods. We implement our methods and baselines
as the following: 1) Train from scratch (Scratch): a vanilla single-task learning baseline that trains
models from scratch. 2) Pre-train then fine-tune (Pre. & FT.): pre-training on ImageNet followed by
fine-tuning on the target dataset. 3) MTHL.D: our multi-task heterogeneous learner using a dense
model (no MoE). 4) MTHL: our multi-task heterogeneous learner using a sparse model (with MoE).
Configs. We employ 12 experts with Top-K as 4 for all MoE modules, following [ 3]. For base-size
transformers, we replace the MLP with MoE MLP every 2 transformer layers. For small and tiny
transformers, we use MoE Mlp in every transformer layer. All models are trained for 240,000
iterations on 96 Tesla V100 GPUs with Lamb [ 40] as the optimizer for large batch training. Weight
6

--- PAGE 7 ---
Table 1: Multi-task heterogeneous training. We compare it with training from scratch (scratch)
and pre-training then fine-tuning (pre. & ft.). Note that on COCO and ADE20K, pre. & ft. would
initialize the backbone with an IN-1K pre-trained model. The numbers of parameters and FLOPs
of the backbone are measured. Note that all classifications have input resolution 224×224. The
single-crop test is used for semantic segmentation.
Backbone ModelParams FLOPs IN-1K COCO ADE20K
(M) (G) top-1 top-5 mAP mAP 50mAP 75mIoU mAcc aAcc
Swin-TScratch 27.5×3 4.4 80.6 95.2 34.9 54.3 36.6 32.0 41.4 75.8
Pre. & FT. 27.5×3 4.4 – – 42.0 64.7 45.9 44.3 55.8 81.0
MTHL.D 27.5 4.4 79.7 95.1 43.8 65.7 46.8 44.4 54.8 80.5
MTHL 50.9 5.1 80.3 94.7 45.0 66.5 48.2 44.6 55.0 81.0
Swin-SScratch 48.9×3 8.5 82.6 96.1 36.3 55.6 38.4 34.5 43.9 77.1
Pre. & FT. 48.9×3 8.5 – – 46.0 68.0 49.9 47.0 56.9 81.7
MTHL.D 48.9 8.5 80.7 95.5 45.8 67.8 48.7 47.7 58.4 81.8
MTHL 89.1 9.2 82.0 95.9 45.7 66.8 49.1 46.7 57.1 81.8
Swin-BScratch 86.7×3 15.1 83.1 96.4 35.5 54.7 37.4 35.4 44.8 77.6
Pre. & FT. 86.7×3 15.1 – – 47.3 69.0 51.2 47.7 58.7 82.3
MTHL.D 86.7 15.1 82.2 96.2 47.5 69.2 51.0 48.8 59.7 82.5
MTHL 158.3 16.2 82.3 96.2 47.6 69.1 50.9 48.2 59.0 82.5
DaViT-TScratch 27.6×3 4.4 82.5 96.2 37.7 57.1 40.0 36.4 46.4 77.8
Pre. & FT. 27.6×3 4.4 – – 45.4 66.9 48.4 45.8 56.0 81.8
MTHL.D 27.6 4.4 81.3 95.8 44.6 66.6 47.5 46.2 56.4 81.6
MTHL 51.2 5.1 82.0 95.8 45.1 67.5 48.1 47.4 57.1 82.1
DaViT-SScratch 49.0×3 8.6 83.8 96.8 37.8 56.7 40.5 38.2 48.4 78.8
Pre. & FT. 49.0×3 8.6 – – 47.2 68.9 50.7 48.3 60.2 82.3
MTHL.D 49.0 8.6 82.6 96.5 47.3 69.2 50.6 48.7 59.1 82.7
MTHL 88.9 9.2 83.3 96.5 46.4 67.7 49.5 47.6 57.9 82.6
DaViT-BScratch 86.9×3 15.2 84.2 96.9 38.0 57.2 40.5 38.5 48.7 78.9
Pre. & FT. 86.9×3 15.2 – – 48.1 69.7 51.3 49.3 60.2 83.0
MTHL.D 86.8 15.2 83.9 96.9 48.3 70.4 51.8 50.0 60.3 83.1
MTHL 158.7 16.3 83.6 96.8 47.8 69.5 51.5 49.6 60.1 83.1
Table 2: Comparisons of different pre-training schemes on downstream performance. We
compare with IN-1K pre-trained model (IN-1K Pre.), and multi-task multi-label pre-training (Mod-
Squad [ 3]) on Taskonomy [ 43]. To calculate the mean, we first average the performance on classifica-
tion, detection, and segmentation separately. Afterward, we average the results across all tasks.
Backbone MethodP365 iNat18 Pets CUB Cars PASC. City. NYUMeantop-1 top-1 top-1 top-1 top-1 mAP mIoU mIoU
Swin-BIN-1K Pre. 58.7 72.9 94.0 83.9 94.0 76.9 80.6 76.2 78.7
Mod-Squad [3] 56.4 69.4 92.3 79.8 93.7 77.2 81.1 77.5 78.1
MTHL.D 59.1 73.3 94.2 84.3 94.2 78.7 82.1 78.0 79.9
MTHL 59.4 73.6 94.6 84.7 94.9 79.1 82.5 78.7 80.4
Davit-BIN-1K pre. 59.2 73.4 94.4 88.4 94.9 77.4 81.5 76.7 79.5
MTHL.D 59.6 73.5 94.8 89.0 95.0 78.8 82.7 78.6 80.6
MTHL 60.1 73.9 94.9 89.4 95.0 79.5 83.4 79.3 81.2
decay is set to 0.05 and the maximal gradient norm is clipped to 0.1. We use a simple triangular
learning rate schedule with a maximum learning rate of 0.004, as in [ 6]. Data augmentations for each
task follow the common practice in [ 23,6]. During multi-task heterogeneous training, data sampling
weight is set to {3, 2, 1}, loss weight is set to {1.0, 0.6, 0.2}, and batch size is set to {64, 2, 2} for
classification, detection, and segmentation, respectively. For a fair comparison, all results of our
method and baselines are obtained from our implementations with the same settings. More details of
the training settings, models, and datasets can be found in the Appendix.
Multi-task heterogeneous training. We compare different training schemes as shown in Tab. 1.
Across all three datasets with varying backbones, we observe that: 1) Heterogeneous training
performs on par with the state-of-the-art pre-train then fine-tune learning scheme, indicating the
gradient conflicts between different tasks are alleviated by our modular design. 2) Notably, for the
segmentation task, MTHL consistently outperforms the previous state-of-the-art across all backbone
choices, suggesting that joint training with classification and detection tasks improves segmentation.
7

--- PAGE 8 ---
Table 3: Efficient adaptation. All experiments use MTHL as the pre-trained model with Davit-S as
the backbone. The ratio calculates the percentage of efficiency metric compared to the fully fine-tuned
baseline. Notations: ‘Ro.’ for Router, ‘Ex.’ for expert(s), θis a threshold on the frequency used for
an expert. We have two hybrid models: 1) ‘Hybrid-A’ directly combines ‘Ro. w/ 1 Ex.’, ‘Prune 2/3
Ex.’, and ‘Top-K=2’. 2) ‘Hybrid-B’ combines ‘Ro. w/ 2 Ex.’, ‘Prune 2/3 Ex.’, and ‘Top-K=3’.
MethodTrain. Model FLOPsRatioP365 iNat18 Pets CUB Cars PASC. City. NYUMeanPar.(M) Par.(M) (G) top-1 top-1 top-1 top-1 top-1 mAP mIoU mIoU
FT-Full 88.9 88.9 9.2 - 59.0 72.9 94.0 88.2 95.0 78.6 81.4 77.4 79.9
Adapter [10] 14.8 - - 16.6% 50.7 62.4 81.1 75.8 80.8 67.7 69.9 66.8 68.7
Ro. Only 0.4 - - 0.4% 52.1 64.2 83.3 77.9 78.2 69.6 71.8 68.7 70.3
Ro. w/ 1 Ex. 5.4 - - 6.1% 57.4 70.7 91.3 85.8 94.7 76.5 78.8 75.2 77.8
Ro. w/ 2 Ex. 10.4 - - 11.7% 58.8 72.7 94.0 87.8 95.0 77.9 80.7 76.7 79.4
Prune θ= 1% - 60.2 - 67.7% 58.9 72.8 93.9 88.1 95.0 78.6 81.4 77.3 79.9
Prune θ= 5% - 54.4 - 61.2% 58.8 72.7 93.8 88.0 94. 78.4 81.4 77.2 79.7
Prune 1/2 Ex. - 59.9 - 67.3% 58.8 72.8 93.9 88.0 93.9 78.6 81.4 77.3 79.8
Prune 2/3 Ex. - 49.9 - 56.1% 58.8 72.6 93.6 87.8 93.8 78.6 81.3 77.2 79.7
Top-K=3 - - 7.7 83.7% 58.8 72.5 93.3 87.3 94.9 77.3 80.1 76.3 79.0
Top-K=2 - - 6.2 67.4% 58.1 70.7 91.9 86.2 92.0 74.9 77.6 73.7 76.8
Top-K=1 - - 4.7 51.0% 48.5 59.9 77.3 72.4 77.4 64.3 66.6 63.3 65.4
Hybrid-A 5.4 49.9 6.2 - 58.0 70.6 91.1 85.8 94.7 76.3 78.5 73.2 77.4
Hybrid-B 10.4 49.9 7.7 - 58.8 72.4 93.3 87.2 94.9 77.1 79.9 76.2 78.8
3) MTHL also works pretty well on image detection and is superior to previous arts in most cases. 4)
The MTHL and MTHL.D generally exhibit similar performance on small and base models and MTHL
consistently outperforms MTHL.D on tiny models, likely influenced by the relationship between
model capacity and dataset scale.
Downstream performance. As shown in Tab. 2, we compare different training schemes on the
downstream datasets. MTHL outperforms the single-task pre-trained model IN-1K Pre. and multi-
task multi-label pre-trained model Mod-Squad, particularly on detection and segmentation tasks. We
also note that the sparse model MTHL consistently outperforms the dense model MTHL.D, indicating
that additional experts for selection could be beneficial for downstream tasks.
4.2 Efficient Adapters
In this section, we highlight the potential of MTHL as an efficient adapter.
Efficient in training parameters. MTHL can adapt quickly to a new task or dataset by tuning
the router with a few optional experts and learning a new task head. During this process, all other
parameters are frozen. The optional few experts to be fine-tuned are randomly selected. We find that
randomly selected experts perform similarly to selecting the expert with the highest or lowest use
frequency on the downstream dataset. Please refer to the supplementary for more details.
In Tab. 3, our method is referred to as ’Ro. Only’, ’Ro. w/ 1 Ex.’, and ’Ro. w/ 2 Ex.’ that denotes
only tuning routers, routers with one expert per MoE module, and routers with two experts per MoE
module, respectively. We compare our efficiency in training parameters with the commonly used
adapter [ 10], which adds an adapter module after each MoE MLP block. In contrast, we only need
new lightweight routers (0.4M) and one or two additional experts per MoE module. Even updating
only new routers outperforms the adapter baseline, and Ro. w/2 Ex. has a very close performance
(0.5 points lower in mean) to the fully fine-tuned baseline. For a clearer comparison, please see Fig. 3.
Efficient in model capacity. In terms of model capacity, MTHL can remove experts after learning a
new router on the new task. This can be achieved by removing experts with the least use frequency,
followed by fine-tuning the entire model.
We explore two methods of pruning: 1) Removing a few experts from each MoE layer. In Tab. 3,
we attempt to remove 1/2 experts and 2/3 experts. 2) Removing all experts whose use frequency is
lower than a threshold θon the downstream dataset. This approach may result in a different number
of experts in each MoE layer, but it has comparable efficiency to the first pruning method. Results
and a clear comparison can be referred to Tab. 3 and Fig. 3.
8

--- PAGE 9 ---
0 25 50 75
Training Parameters (M)707274767880Mean Performance (%)
Fully 
 finetune
AdapterRo. OnlyRo. w/1 Ex.Ro. w/2 Ex.Ro. w/3 Ex.
Hybrid-AHybrid-B
50 60 70 80 90
Model Parameters (M)77.578.078.579.079.580.0
Fully 
 finetunePrune 2/3 Ex.Prune 5%Prune 1/2 Ex.Prune 1%
Hybrid-AHybrid-B
5 6 7 8 9
FLOPs (G)65.067.570.072.575.077.580.0
Fully 
 finetuneTop-K=3
Top-K=2
Top-K=1
Hybrid-A
Hybrid-BFigure 3: Trade-off between efficiency and performance. We visualize the trade-off between
performance and training parameters, model parameters, and computation cost respectively.
Table 4: Continual learning. We conduct continual learning on these datasets one by one after
heterogenous pre-training and report the final performance. All experiments use MTHL as the
pre-trained model with DaviT-S as the backbone. The number of training parameters and newly
added parameters in the backbone per task are measured. Here the average is the average performance
on all datasets.
MethodNew params Train. params P365 iNat18 Pets CUB Cars PASC. City. NYUAverageper task (M) per task (M) top-1 top-1 top-1 top-1 top-1 mAP mIoU mIoU
LWF [16] 0 88.9 46.2 57.0 73.5 70.6 75.5 62.7 71.1 68.9 65.7
Rou. only 0.4 0.4 52.1 64.2 83.3 77.9 78.2 69.6 71.8 68.7 70.7
Rou. w/ 1Ex. 5.4 5.4 57.6 70.8 91.3 85.9 94.7 76.8 79.0 75.6 79.0
Rou. w/ 2Ex. 10.4 10.4 58.8 72.8 94.5 88.0 95.0 78.1 80.7 76.9 80.6
FT-Full – – 59.0 72.9 94.0 88.2 95.0 78.6 81.4 77.4 80.8
Efficient in computation cost. Most pre-training may use a relatively large backbone, but the
downstream tasks/datasets may not require such a large model capacity. MTHT.S can regulate the
computation cost by learning new routers with a reduced Top-K. This would result in a trade-off
between performance and computation cost, as illustrated in Fig. 3. For some datasets ( e.g., P365),
it can achieve a relatively low computation cost ( e.g., 67.4%) while maintaining the same level of
performance ( e.g., <1% drop).
Combine all efficient adapting. To further improve efficiency, the efficient adapting techniques
mentioned above can be combined. In Tab. 3, for Hybrid-B, we first learn a new router and remove
2/3 experts. Then, we fine-tune the router with Top-K as 3 along with two experts per module. This
approach achieves a mean performance of 78.8, which is only 1 point lower than fine-tuning the entire
model. Moreover, this method reduces training parameters, model parameters, and computation cost
simultaneously.
4.3 Continual learning.
Continual learning without any forgetting is achievable with MTHL by learning new routers (0.4M)
and a few optional experts on the new dataset. We compared it with the common regularization-based
continual learning baseline LWF[ 16]. As demonstrated in Tab. 4, our method has three significant
advantages: 1) No forgetting on the learned datasets. 2) Only a smart part of the model needs to be
trained on new datasets, requiring only 10.4M training parameters, while LWF needs to tune the
whole model (88.9M). 3) Comparable performance to fully fine-tuning the whole model on every
dataset.
5 Conclusion
Our study focuses on multi-task heterogeneous training and its adaptation ability on downstream
datasets. MTHL can achieve outcomes comparable to the previous single-task state-of-the-art on
all tasks. Furthermore, we investigate various methods of utilizing modularity to efficiently adapt
to downstream tasks. Modularity also allows model expansion easily for continual learning. The
broader impact of our work could be significant in terms of advancing general-purpose vision model
pre-training and effective adaptation of large-scale models. One limitation of MTHL is model may
be biased toward certain datasets and require more training iterations for convergence.
9

--- PAGE 10 ---
References
[1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems (Neurips) , 33:1877–1901, 2020.
[2]M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging
properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international
conference on computer vision (ICCV) , pages 9650–9660, 2021.
[3]Z. Chen, Y . Shen, M. Ding, Z. Chen, H. Zhao, E. Learned-Miller, and C. Gan. Mod-squad: De-
signing mixtures of experts as modular multi-task learners. Proceedings of the IEEE conference
on computer vision and pattern recognition (CVPR) , 2023.
[4]M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth,
and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings
of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 3213–3223,
2016.
[5]J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , 2009.
[6]M. Ding, B. Xiao, N. Codella, P. Luo, J. Wang, and L. Yuan. Davit: Dual attention vision
transformers. In ECCV , pages 74–92, 2022.
[7]M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual
object classes (voc) challenge. International journal of computer vision , 88(2):303–338, 2010.
[8]W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39, 2022.
[9]P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y . Zhang, H. Li, and Y . Qiao. Clip-adapter: Better
vision-language models with feature adapters. arXiv preprint arXiv:2110.04544 , 2021.
[10] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. At-
tariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. In International Conference
on Machine Learning (ICML) , pages 2790–2799. PMLR, 2019.
[11] E. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, L. Wang, and W. Chen. Lora: Low-rank
adaptation of large language models, 2021.
[12] C.-Y . Hung, C.-H. Tu, C.-E. Wu, C.-H. Chen, Y .-M. Chan, and C.-S. Chen. Compacting, picking
and growing for unforgetting continual learning. Advances in Neural Information Processing
Systems (Nerurips) , 32, 2019.
[13] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural computation , 3(1):79–87, 1991.
[14] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General
perception with iterative attention. In International conference on machine learning (ICML) ,
pages 4651–4664. PMLR, 2021.
[15] A. Kendall, Y . Gal, and R. Cipolla. Multi-task learning using uncertainty to weigh losses for
scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and
pattern recognition (CVPR) , pages 7482–7491, 2018.
[16] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan,
J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017.
[17] I. Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and
high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE
conference on computer vision and pattern recognition (CVPR) , pages 6129–6138, 2017.
[18] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained
categorization. In Proceedings of the IEEE international conference on computer vision
workshops , pages 554–561, 2013.
[19] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen.
{GS}hard: Scaling giant models with conditional computation and automatic sharding. In
International Conference on Learning Representations , 2021.
10

--- PAGE 11 ---
[20] Y . Li, H. Mao, R. B. Girshick, and K. He. Exploring plain vision transformer backbones for
object detection. ArXiv , abs/2203.16527, 2022.
[21] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Dollár. Focal loss for dense object detection. In
ICCV , pages 2980–2988, 2017.
[22] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.
Microsoft coco: Common objects in context. In European conference on computer vision , pages
740–755. Springer, 2014.
[23] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In ICCV , 2021.
[24] D. Lopez-Paz and M. Ranzato. Gradient episodic memory for continual learning. Advances in
neural information processing systems , 30, 2017.
[25] J. Lu, C. Clark, R. Zellers, R. Mottaghi, and A. Kembhavi. UNIFIED-IO: A unified model for
vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning
Representations (ICLR) , 2023.
[26] K.-K. Maninis, I. Radosavovic, and I. Kokkinos. Attentive single-tasking of multiple tasks. In
Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR) , pages
1851–1860, 2019.
[27] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby. Multimodal contrastive
learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770 ,
2022.
[28] C. V . Nguyen, Y . Li, T. D. Bui, and R. E. Turner. Variational continual learning. In International
Conference on Learning Representations (ICLR) , 2018.
[29] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE
conference on computer vision and pattern recognition (CVPR) , pages 3498–3505. IEEE, 2012.
[30] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers,
and N. Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information
Processing Systems (NeurIPS) , 34:8583–8595, 2021.
[31] H. Ritter, A. Botev, and D. Barber. Online structured laplace approximations for overcoming
catastrophic forgetting. Advances in Neural Information Processing Systems , 31, 2018.
[32] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously
large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference
on Learning Representations , 2017.
[33] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference
from rgbd images. ECCV (5) , 7576:746–760, 2012.
[34] G. Van Horn, O. Mac Aodha, Y . Song, Y . Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and
S. Belongie. The inaturalist species classification and detection dataset. In Proceedings of the
IEEE conference on computer vision and pattern recognition (CVPR) , pages 8769–8778, 2018.
[35] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011
dataset. 2011.
[36] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun. Unified perceptual parsing for scene understanding.
InProceedings of the European conference on computer vision (ECCV) , pages 418–434, 2018.
[37] X. Xu, H. Zhao, V . Vineet, S.-N. Lim, and A. Torralba. Mtformer: Multi-task learning via
transformer and cross-task reasoning. In Proceedings of the European Conference on Computer
Vision (ECCV) , 2022.
[38] M. B. Yi-Lin Sung, Jaemin Cho. Vl-adapter: Parameter-efficient transfer learning for vision-
and-language tasks. In CVPR , 2022.
[39] J. Yoon, E. Yang, J. Lee, and S. J. Hwang. Lifelong learning with dynamically expandable
networks. 2018.
[40] Y . You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer,
and C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv
preprint arXiv:1904.00962 , 2019.
11

--- PAGE 12 ---
[41] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu, X. Huang, B. Li, C. Li, et al.
Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 ,
2021.
[42] E. B. Zaken, S. Ravfogel, and Y . Goldberg. Bitfit: Simple parameter-efficient fine-tuning for
transformer-based masked language-models. Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (ACL) , 2022.
[43] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling
task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern
recognition (CVPR) , pages 3712–3722, 2018.
[44] F. Zenke, B. Poole, and S. Ganguli. Continual learning through synaptic intelligence. In
International conference on machine learning , pages 3987–3995. PMLR, 2017.
[45] R. Zhang, R. Fang, P. Gao, W. Zhang, K. Li, J. Dai, Y . Qiao, and H. Li. Tip-adapter: Training-
free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930 , 2021.
[46] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image
database for scene recognition. IEEE transactions on pattern analysis and machine intelligence ,
40(6):1452–1464, 2017.
[47] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba. Scene parsing through
ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , 2017.
12

--- PAGE 13 ---
Appendix
Table A1: We compare three ways of selecting a subset of experts to fine-tune, while freezing the
remaining experts. We first learn new routers on the new downstream to determine each expert’s
frequency of being chosen. Random represents randomly choosing experts. Best represents choosing
the experts with the highest frequency. Worse represents choosing the experts with the lowest
frequency. We report mean top-1 accuracy on CUB, Cars, and Pets. Other settings are the same as in
Tab. 3 in the paper.
Random Best Worse
Ro. w/1 Ex. 90.6 90.5 90.6
Ro. w/2 Ex. 92.3 92.3 92.2
A1 Different ways to select experts to be fine-tuned.
Tab. A1 compares various methods of selecting experts to fine-tune while freezing the rest. We
compare random selecting experts and selecting experts that are more or less likely to be chosen by
routers. We find out that the selection method does not significantly affect the fine-tuning performance.
Therefore, we use random selection for simplicity.
A2 Ablation on Top- K.
As shown in Tab. A2, we explore the effect on Top- Kin MoE module. The experiment setting is the
same as in Table.1 in the paper with 12 experts per MoE module. We report the mean performance
on pre-train and downstream datasets of our MHTL with Davit-T as the backbone. To control the
FLOPs to be the same for different Top- K, the hidden dimension of MLP experts is divided by K.
All experiments have the same parameter size and the same FLOPs. We find that Top- K= 4has the
best performance.
A3 Ablation on the number of experts.
As shown in Tab. A3, we explore the effect on the number of experts Efor the MoE MLP layer. The
settings are the same as in appendix A2 with a Top- Kas 4.
Table A2: Ablation study of Top- Kon MoE MLP layer.
FLOPs(G) Params(M) Hidden Dim Pre-train mean Downstream mean
K=2 5.1 51.2 768 58.1 80.3
K=4 5.1 51.2 384 58.2 80.4
K=6 5.1 51.2 256 57.9 80.0
Table A3: Ablation study of expert number Eon MoE MLP layer.
FLOPs(G) Params(M) Pre-train mean Downstream mean
E=6 5.1 33.4 57.2 78.5
E=9 5.1 42.3 57.9 80.0
E=12 5.1 51.2 58.2 80.4
E=15 5.1 60.1 58.2 80.5
13
