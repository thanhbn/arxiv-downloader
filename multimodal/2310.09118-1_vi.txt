DSG: Một Trình Tạo Cấu Trúc Tài Liệu Đầu Cuối

Johannes Rausch†, Gentiana Rashiti†, Maxim Gusev†, Ce Zhang†, Stefan Feuerriegel‡
†Khoa Khoa học Máy tính, ETH Zurich‡Trung tâm Học máy Munich, LMU Munich

Tóm tắt—Thông tin trong công nghiệp, nghiên cứu và khu vực công được lưu trữ rộng rãi dưới dạng tài liệu được hiển thị (ví dụ: file PDF, bản quét). Do đó, để kích hoạt các tác vụ xuôi dòng, cần có các hệ thống ánh xạ tài liệu được hiển thị thành định dạng phân cấp có cấu trúc. Tuy nhiên, các hệ thống hiện có cho tác vụ này bị giới hạn bởi suy luận và không thể huấn luyện đầu cuối. Trong công trình này, chúng tôi giới thiệu Document Structure Generator (DSG), một hệ thống mới cho phân tích cú pháp tài liệu có thể huấn luyện đầu cuối hoàn toàn. DSG kết hợp một mạng nơ-ron sâu để phân tích cú pháp (i) các thực thể trong tài liệu (ví dụ: hình ảnh, khối văn bản, tiêu đề, v.v.) và (ii) các quan hệ nắm bắt cấu trúc tuần tự và lồng nhau giữa các thực thể. Khác với các hệ thống hiện có dựa trên suy luận, DSG của chúng tôi được huấn luyện đầu cuối, giúp nó hiệu quả và linh hoạt cho các ứng dụng thực tế. Chúng tôi tiếp tục đóng góp một bộ dữ liệu mới quy mô lớn có tên E-Periodica bao gồm các tạp chí thực tế với cấu trúc tài liệu phức tạp để đánh giá. Kết quả của chúng tôi chứng minh rằng DSG vượt trội hơn các công cụ OCR thương mại và đạt được hiệu suất tốt nhất. Theo hiểu biết tốt nhất của chúng tôi, hệ thống DSG là hệ thống có thể huấn luyện đầu cuối đầu tiên cho phân tích cú pháp tài liệu phân cấp.

I. GIỚI THIỆU

Lượng lớn thông tin được tạo ra hàng ngày trong công nghiệp, nghiên cứu và khu vực công. Tuy nhiên, dữ liệu đó thường được lưu trữ dưới dạng hiển thị tài liệu (ví dụ: file PDF, bản quét) chứ không phải định dạng phân cấp có cấu trúc. Đây là rào cản quan trọng trong thực tế. Một mặt, định dạng có cấu trúc trái ngược với hiển thị tài liệu là cần thiết cho việc lưu trữ hiệu quả trong cơ sở dữ liệu, vì cơ sở dữ liệu yêu cầu các định dạng được chuẩn hóa. Mặt khác, hiển thị tài liệu không thể được xử lý trong các tác vụ xuôi dòng vì các tác vụ xuôi dòng thường yêu cầu các tài liệu ở định dạng có thể phân tích cú pháp. Các ví dụ là truy vấn và truy xuất và xây dựng cơ sở tri thức. Để đạt được mục tiêu này, có nhu cầu trực tiếp trong thực tế về các hệ thống ánh xạ hiển thị tài liệu thành định dạng phân cấp có cấu trúc.

Tác vụ phân tích cú pháp cấu trúc tài liệu đề cập đến việc tạo ra cấu trúc tài liệu phân cấp, với đầu vào là một hiển thị tài liệu (xem Hình 1). Để thực hiện điều này, nội dung văn bản phải được trích xuất từ hiển thị tài liệu trong khi vẫn bảo toàn cấu trúc ngữ nghĩa và phân cấp của các file nguồn. Để đạt được điều này, các hệ thống hiện đại thường đầu tiên phát hiện tất cả các thực thể tài liệu (ví dụ: hình ảnh, khối văn bản, tiêu đề) và sau đó suy luận các mối quan hệ phân cấp giữa các thực thể (ví dụ: cấu trúc tuần tự và lồng nhau của chúng) để tạo thành cấu trúc tài liệu phân cấp. Tuy nhiên, tác vụ này rất khó khăn do cấu trúc phức tạp, lồng nhau trong các tài liệu thực tế.

Một giải pháp tiêu chuẩn để ánh xạ hiển thị tài liệu thành tài liệu có thể phân tích cú pháp là các hệ thống nhận dạng ký tự quang học (OCR). Các hệ thống OCR hiện tại rất hiệu quả trong việc truy xuất nội dung văn bản ở cấp độ từ từ các tài liệu được hiển thị. Tuy nhiên, các hệ thống OCR thường chỉ tập trung vào nội dung văn bản mà gặp khó khăn trong việc suy luận cấu trúc phân cấp. Các hệ thống OCR thường dựa vào bước trước đó để phân tích cú pháp cấu trúc tài liệu, nhưng điều này vẫn rất thách thức và dễ xảy ra lỗi. Kết quả là, các hệ thống OCR gặp phải những lỗi lớn, đặc biệt nếu các lỗi trong bước phân tích cú pháp cấu trúc xảy ra. Để giải quyết vấn đề này, nghiên cứu trước đây tập trung vào các hệ thống tùy chỉnh để phân tích cú pháp các thực thể cụ thể trong tài liệu như cấu trúc bảng nhưng không phân tích cú pháp cấu trúc phân cấp hoàn chỉnh trong tài liệu. Thậm chí nghiên cứu khác nhằm mục đích xác định các thực thể tài liệu nhưng không thực sự tạo ra cấu trúc tài liệu phân cấp. Chỉ có một công trình được thiết kế riêng để tạo ra cấu trúc tài liệu phân cấp từ hiển thị tài liệu. Tuy nhiên, công trình này dựa trên suy luận và do đó không thể huấn luyện đầu cuối, vì vậy tính linh hoạt của nó bị hạn chế. Theo hiểu biết tốt nhất của chúng tôi, không có hệ thống nào để phân tích cú pháp cấu trúc tài liệu phân cấp có thể huấn luyện đầu cuối.

Hệ thống DSG của chúng tôi: Chúng tôi phát triển Document Structure Generator (DSG), một hệ thống mới để tạo ra cấu trúc tài liệu phân cấp từ hiển thị tài liệu trong đó hệ thống có thể huấn luyện đầu cuối hoàn toàn. DSG của chúng tôi xây dựng dựa trên mạng nơ-ron sâu để phân tích cú pháp (i) các thực thể trong tài liệu (ví dụ: hình ảnh, khối văn bản, tiêu đề, v.v.) và (ii) các quan hệ nắm bắt cấu trúc tuần tự và lồng nhau giữa các thực thể. Trái ngược với các hệ thống hiện có để tạo ra cấu trúc tài liệu, DSG của chúng tôi sử dụng một thành phần có thể huấn luyện để phân loại quan hệ và do đó tránh việc sử dụng suy luận. Kết quả là, DSG của chúng tôi dự đoán toàn bộ cấu trúc tài liệu và do đó có thể huấn luyện đầu cuối hoàn toàn. Điều này làm cho hệ thống của chúng tôi rất linh hoạt để xử lý nhiều loại tài liệu có thể phát sinh trong thực tế. Cuối cùng, DSG của chúng tôi có một công cụ chuyển đổi tùy chỉnh để tạo ra các file đầu ra tài liệu có cấu trúc bằng ngôn ngữ đánh dấu hOCR, cho phép tích hợp liền mạch vào các quy trình lưu trữ và xử lý tài liệu hiện có.

Chúng tôi tiếp tục đóng góp một bộ dữ liệu mới quy mô lớn để tạo ra và đánh giá cấu trúc tài liệu phân cấp có tên E-Periodica. E-Periodica dựa trên các tạp chí thực tế từ các ngôn ngữ nguồn khác nhau (ví dụ: tiếng Anh, tiếng Đức, tiếng Pháp, tiếng Ý). Chúng tôi đã gắn nhãn thủ công cấu trúc tài liệu phân cấp cho vài trăm trang tạp chí. Tổng thể, E-Periodica chứa 542 tài liệu với hơn 11.000 thực thể được gắn nhãn. Qua đó, chúng tôi mở rộng so với các bộ dữ liệu trước đây chủ yếu giới hạn ở các bài báo khoa học. Tuy nhiên, một hạn chế của các bài báo khoa học là chúng tuân theo một cấu trúc khá tương tự, trong khi tạp chí được đặc trưng bởi sự đa dạng lớn trong cách trình bày và do đó có cấu trúc tài liệu phức tạp. Do đó, E-Periodica cung cấp một bối cảnh mới và thách thức, thực tế để đánh giá.

Đóng góp chính của chúng tôi như sau:
1) Chúng tôi phát triển một hệ thống mới để tạo ra cấu trúc tài liệu phân cấp từ hiển thị tài liệu có tên DSG. Theo hiểu biết tốt nhất của chúng tôi, DSG của chúng tôi là hệ thống đầu tiên cho tác vụ này có thể huấn luyện đầu cuối.
2) Chúng tôi đóng góp một bộ dữ liệu mới quy mô lớn có tên E-Periodica với các chú thích thủ công để đánh giá.
3) Chúng tôi cho thấy rằng hệ thống DSG của chúng tôi đạt được hiệu suất tốt nhất. Chúng tôi tiếp tục chứng minh hiệu quả của việc huấn luyện đầu cuối.

II. CÔNG TRÌNH LIÊN QUAN

Phân tích cú pháp cấu trúc tài liệu: Các hệ thống hiện có có hai nhược điểm là chúng hoặc (i) giới hạn ở nhận dạng thực thể và do đó không tạo ra cấu trúc phân cấp, hoặc (ii) dựa trên suy luận và do đó không thể huấn luyện đầu cuối. Chúng tôi cung cấp một tổng quan chi tiết trong phần sau.

Hệ thống OCR: Trích xuất văn bản từ hình ảnh tài liệu đã được nghiên cứu rộng rãi như một phần của hệ thống OCR. Như vậy, các công trình trích xuất nội dung văn bản, nhưng không phải cấu trúc tài liệu phân cấp, đây là mục tiêu của nghiên cứu của chúng tôi.

Bộ phân tích cú pháp thực thể cụ thể: Một số hệ thống tập trung vào các thực thể ngữ nghĩa cụ thể, cụ thể là phát hiện bảng và phân tích cú pháp cấu trúc bảng. Trong phát hiện bảng, tác vụ là dự đoán các hộp giới hạn của bảng trong hiển thị tài liệu, thay vì tạo ra cấu trúc bảng thực tế. Trong phân tích cú pháp cấu trúc bảng, mục tiêu là nhận dạng cấu trúc (ví dụ: hàng, ô) trong bảng. Ở đây, đầu vào được cung cấp dưới dạng văn bản hoặc thông qua hiển thị tài liệu. Tuy nhiên, các công trình bị giới hạn ở một thực thể duy nhất (bảng). Do đó, những công trình này không thể xác định các thực thể khác và do đó không thể xác định cấu trúc tài liệu đầy đủ. Các công trình khác thực hiện phát hiện thực thể trong tài liệu bằng cách định vị các yếu tố cụ thể, nhưng một lần nữa không trích xuất cấu trúc phân cấp. Thậm chí những công trình khác chỉ tập trung vào việc phân đoạn các dòng riêng lẻ.

Bộ phân tích cú pháp tài liệu phân cấp: Một công trình phân loại các dòng và dòng cha trực tiếp của chúng nhưng sử dụng công cụ OCR bên thứ ba dẫn đến sự lan truyền lỗi trong toàn bộ hệ thống. Quan trọng hơn, công trình này bị giới hạn ở một vài thực thể cấp cao (ví dụ: cấu trúc bảng, danh sách và hình phụ bị thiếu) và hầu hết cấu trúc phân cấp bị mất (ví dụ: thứ tự). Do đó, công trình này không thể tạo ra cấu trúc tài liệu toàn diện như yêu cầu trong tác vụ của chúng tôi.

Gần nhất với công trình của chúng tôi là một hệ thống có tên DocParser, được thiết kế đặc biệt để nắm bắt cấu trúc tài liệu phân cấp. DocParser bao gồm năm thành phần (chuyển đổi hình ảnh, phát hiện thực thể, phân loại quan hệ, tinh chỉnh dựa trên cấu trúc và giám sát yếu có thể mở rộng) để tạo ra cả thực thể và quan hệ. Tuy nhiên, trong DocParser, các quan hệ được phát hiện dựa trên suy luận thủ công và không thể huấn luyện. Do đó, theo hiểu biết tốt nhất của chúng tôi, các hệ thống phân tích cú pháp tài liệu phân cấp có thể huấn luyện đầu cuối đang thiếu.

Tạo đồ thị cảnh: Tạo đồ thị cảnh là một tác vụ thị giác máy tính kết hợp tác vụ phát hiện thực thể với phân loại quan hệ bổ sung. Nhiều phương pháp gần đây cho tạo đồ thị cảnh dựa trên quy trình huấn luyện hai giai đoạn trong đó các thành phần phát hiện xây dựng trên Faster R-CNN. Tuy nhiên, các hệ thống tạo đồ thị cảnh chủ yếu được sử dụng để phân tích cú pháp hình ảnh thực tế và theo hiểu biết tốt nhất của chúng tôi, chưa được điều chỉnh để phân tích cú pháp cấu trúc tài liệu. Đây là đóng góp của chúng tôi.

Khoảng trống nghiên cứu: Các hệ thống hiện có để tạo ra cấu trúc tài liệu phân cấp dựa trên suy luận và do đó không thể huấn luyện đầu cuối, điều này hạn chế tính linh hoạt của chúng. Như một giải pháp, chúng tôi phát triển DSG của chúng tôi, hệ thống đầu tiên cho phân tích cú pháp tài liệu phân cấp có thể huấn luyện đầu cuối.

III. MÔ TẢ VẤN ĐỀ

Mục tiêu: Mục tiêu của hệ thống của chúng tôi là tạo ra cấu trúc tài liệu phân cấp từ hiển thị tài liệu (ví dụ: file PDF, hình ảnh quét). Chính thức, đầu vào được đưa ra bởi hiển thị tài liệu D1, ..., Dn. Đầu ra là các tài liệu có cấu trúc phân cấp được đưa ra bởi các cặp (H1, T1), ..., (Hn, Tn), trong đó Hi, i = 1...n, nắm bắt cấu trúc phân cấp và Ti, i = 1...n, các văn bản. Cấu trúc phân cấp được định nghĩa bởi một tập hợp (i) các thực thể trong tài liệu (ví dụ: hình ảnh, khối văn bản, tiêu đề, v.v.) và (ii) các quan hệ nắm bắt cấu trúc tuần tự và lồng nhau giữa các thực thể. Chính thức, các thực thể được đưa ra bởi Ej, j = 1,...,m, và các quan hệ bởi Rj, j = 1,...,k. Cả hai đều được định nghĩa dưới đây. Tác vụ trên do đó tương tự với nghiên cứu trước đây về phân tích cú pháp cấu trúc tài liệu phân cấp.

Thực thể: Thực thể nắm bắt các yếu tố cấu trúc khác nhau trong tài liệu, như hình ảnh, bảng, chú thích, khối văn bản, v.v. Mỗi thực thể Ej, j = 1,...,m, được mô tả bởi ba thuộc tính: (1) một danh mục ngữ nghĩa cj ∈ C = {C1,...,Cl} (ví dụ: nó là hình ảnh, bảng, tiêu đề, v.v.); (2) một hộp giới hạn hình chữ nhật Bj trong hiển thị tài liệu, được định nghĩa bởi tọa độ x và y của các điểm góc của hộp giới hạn; và (3) một điểm tin cậy Pj đi kèm với dự đoán của danh mục ngữ nghĩa cj.

Quan hệ: Quan hệ nắm bắt cấu trúc lồng nhau giữa các thực thể. Quan hệ Rj, j = 1,...,k được định nghĩa bởi các bộ ba (Esubj, Eobj, Ψ) bao gồm một chủ thể Esubj, một đối tượng Eobj và một loại quan hệ Ψ ∈ {parent of, followed by, null}. Hơn nữa, các quan hệ Rj được liên kết với một điểm tin cậy PΨj cho loại quan hệ dự đoán Ψ.

Đầu ra hOCR: Như đầu ra bổ sung, các tài liệu văn bản có cấu trúc phân cấp được đưa ra bởi các cặp (H1, T1), ..., (Hn, Tn), trong đó Ti, i = 1...n, nắm bắt các văn bản cũng nên được cung cấp trong định dạng hOCR chuẩn hóa để tạo điều kiện cho các tác vụ xử lý xuôi dòng. hOCR là một ngôn ngữ đánh dấu để đại diện và lưu trữ tài liệu có cấu trúc trong một định dạng thống nhất. Do đó, điều này sẽ đảm bảo rằng đầu ra có thể được sử dụng trực tiếp bởi các công cụ phổ biến cho quy trình xử lý và lưu trữ tài liệu được sử dụng rộng rãi trong thực tế.

IV. HỆ THỐNG DSG CỦA CHÚNG TÔI

Tổng quan: Một tổng quan về hệ thống DSG của chúng tôi được thể hiện trong Hình 2. Mục tiêu của DSG là tạo ra cấu trúc tài liệu phân cấp từ hiển thị tài liệu trong thiết lập huấn luyện đầu cuối. Để thực hiện điều này, hệ thống của chúng tôi xây dựng trên một mạng nơ-ron sâu bao gồm các thành phần có thể huấn luyện hoàn toàn để phân tích cú pháp cả thực thể và sau đó các quan hệ đại diện cho cấu trúc phân cấp. Hệ thống của chúng tôi xử lý tài liệu dọc theo năm thành phần: (C1) tiền xử lý hình ảnh, (C2) phát hiện thực thể, (C3) phân loại quan hệ và tinh chỉnh thực thể, (C4) hậu xử lý dựa trên ngữ pháp và (C5) công cụ chuyển đổi hOCR. Các thành phần được mô tả trong phần sau.

A. Tiền xử lý hình ảnh (C1)

Hệ thống của chúng tôi xử lý tất cả tài liệu đầu vào dưới dạng hình ảnh được hiển thị. Đối với các định dạng nguồn như PDF, chúng tôi đầu tiên tạo ra hình ảnh cho mỗi trang tài liệu, sau đó được sử dụng làm đầu vào cho hệ thống của chúng tôi.

Hình ảnh được thay đổi kích thước song tuyến tính để cạnh nhỏ nhất có kích thước tối đa φmax s. Nếu cạnh dài nhất vượt quá kích thước tối đa được định nghĩa trước φmax l sau bước này, hình ảnh được thay đổi kích thước để độ dài cạnh dài nhất là φmax l. Trong quá trình huấn luyện, kích thước hình ảnh có thể được thay đổi cho mục đích tăng cường dữ liệu. Để thực hiện điều này, φmax s và φmax l được chọn ngẫu nhiên từ một tập hợp các kích thước khác nhau. Hình ảnh sau đó được chuẩn hóa theo quy trình trong [35]. Cụ thể, chúng tôi trừ các giá trị pixel trung bình theo kênh của bộ dữ liệu huấn luyện trước cơ bản từ các đầu vào.

B. Phát hiện thực thể (C2)

Thành phần thứ hai xây dựng trên kiến trúc Faster R-CNN cho phát hiện thực thể. Ở đây, các bản đồ đặc trưng thị giác ở các tỷ lệ khác nhau được trích xuất thông qua một mạng nơ-ron tích chập. Các bản đồ đặc trưng thị giác sau đó được chuyển đến một thành phần mạng khác, gọi là mạng đề xuất vùng (RPN), tạo ra một tập hợp các đề xuất vùng thực thể ứng viên hình chữ nhật trong hình ảnh. Đối với mỗi đề xuất vùng, một mạng dự đoán danh mục được áp dụng để dự đoán danh mục ngữ nghĩa c'j của một thực thể Ej. Nếu điểm tin cậy P'j của một vùng ứng viên vượt quá ngưỡng được định nghĩa trước, nó được chấp nhận như một thực thể (và bị loại bỏ trong trường hợp ngược lại). Sau đó, một mạng nơ-ron bổ sung được sử dụng để dự đoán kích thước và vị trí của vùng hình chữ nhật ban đầu Bj (dựa trên các đề xuất vùng thực thể ứng viên hình chữ nhật từ RPN). Sau đó, các thực thể được chuyển đến thành phần C3, chịu trách nhiệm về phân loại quan hệ.

C. Phân loại quan hệ và tinh chỉnh thực thể (C3)

Phân loại quan hệ trong DSG xây dựng trên kiến trúc neural motifs. Kiến trúc này mở rộng kiến trúc phát hiện thực thể với hai đầu mạng nơ-ron bổ sung. Cụ thể, các thực thể được phát hiện được chuyển đến các đầu mạng nơ-ron cho phân loại quan hệ và tinh chỉnh thực thể. Trong phần sau, chúng tôi gọi hai đầu này là đầu quan hệ và đầu tinh chỉnh, tương ứng.

Cả đầu quan hệ và đầu tinh chỉnh đều xây dựng trên các mạng bộ nhớ dài-ngắn hạn hai chiều (LSTM) nhận các thực thể từ thành phần C2 làm đầu vào. Cả hai tiến hành theo cách hơi khác nhau. Đầu quan hệ được cung cấp với các cặp thực thể chủ thể và thực thể đối tượng, (Esubj, Eobj), để phân loại xem chúng có tạo thành một bộ ba quan hệ (Esubj, Eobj, Ψ) loại Ψ ∈ {parent of, followed by, null} hay không. Đầu tinh chỉnh ánh xạ các nhãn phân loại c'j và điểm tin cậy P'j từ thành phần C2 thành các danh mục đã tinh chỉnh cj và điểm tin cậy Pj bằng cách tính đến thông tin ngữ cảnh của tất cả các thực thể được dự đoán. Chính thức, cả đầu quan hệ và đầu tinh chỉnh đều được thực hiện như sau:

(i) Đầu quan hệ: Phân loại quan hệ trả về một điểm tin cậy PΨj cho tất cả các cặp thực thể được xem xét (Esubj, Eobj). Nếu điểm tin cậy tương ứng vượt quá ngưỡng được định nghĩa trước τ, một quan hệ Rj loại Ψ ∈ {parent of, followed by, null} được chấp nhận. Ở đây, loại quan hệ Ψ = null được sử dụng để chỉ ra sự vắng mặt của một quan hệ phân cấp.

(ii) Đầu tinh chỉnh: Đầu tinh chỉnh được cung cấp với các đặc trưng bổ sung ρrefin i, i ∈ {vis, cat, pos}, như sau. Đầu tiên, ρrefin vis đề cập đến bản đồ đặc trưng thị giác được trích xuất bởi kiến trúc Faster R-CNN cơ bản và tương ứng với vùng hình ảnh của thực thể trong tài liệu được hiển thị. Thứ hai, ρrefin cat là một embedding danh mục, dựa trên từ điển embedding từ được huấn luyện trước và được chọn theo danh mục ngữ nghĩa được dự đoán của thực thể. Thứ ba, ρrefin pos là một embedding vị trí để đại diện cho kích thước và vị trí của hộp giới hạn thực thể. Cụ thể, embedding vị trí kết hợp chiều rộng, chiều cao và vị trí của hộp giới hạn Bj. Chúng tôi sau đó gọi các đặc trưng ρrefin i, i ∈ {vis, cat, pos} là các đặc trưng đầu vào ngữ cảnh tinh chỉnh.

Các đặc trưng đầu vào ngữ cảnh tinh chỉnh được chuyển đến LSTM từ đầu tinh chỉnh và sau đó một lớp kết nối đầy đủ để tạo ra cái gọi là các đặc trưng đầu ra ngữ cảnh tinh chỉnh ρrefout. Các đặc trưng này sau đó được sử dụng để dự đoán các danh mục thực thể đã tinh chỉnh cj.

Sau đó, đầu quan hệ được cung cấp với ba đặc trưng đầu vào ngữ cảnh quan hệ ρrelin i, i ∈ {vis, cat, ref} như sau. Đầu tiên, ρrelin vis là bản đồ đặc trưng thị giác, giống hệt với trong ρrefin vis. Thứ hai, ρrelin cat là embedding danh mục, tương tự như embedding danh mục ρrefin cat để đại diện cho danh mục thực thể đã tinh chỉnh. Thứ ba, ρrelin ref là các đặc trưng đầu ra ngữ cảnh tinh chỉnh, tức là ρrelin ref = ρrefout.

Tiếp theo, các cặp hai thực thể được xử lý bởi đầu quan hệ. Quan trọng là, không giống như các hệ thống hiện có như DocParser, bước này có thể huấn luyện hoàn toàn. Để thực hiện điều này, đầu quan hệ tạo thành ba cái gọi là đặc trưng theo cặp ρpair 1, ρpair 2 và ρpair 3 như được chỉ định trong phần sau. Sau đó, các đặc trưng theo cặp được sử dụng để dự đoán điểm tin cậy PΨj cho tất cả các cặp thực thể được xem xét (Esubj, Eobj). Cụ thể, các đặc trưng theo cặp là: Đầu tiên, ρpair 1 là bản đồ đặc trưng thị giác được trích xuất bởi kiến trúc Faster R-CNN. Nó tương ứng với vùng hình ảnh của cặp thực thể Bpair = Union(Esubj, Eobj). Thứ hai, ρpair 2 được tạo thành cho các cặp thực thể-thực thể (Esubj, Eobj) bằng cách nối các đặc trưng đầu ra ngữ cảnh tinh chỉnh tương ứng (ρrelout subj, ρrelout obj). Thứ ba, một số hạng thiên vị tần số ρpair 3 được tính toán từ các danh mục đã tinh chỉnh c của các thực thể được xem xét theo cặp. Số hạng thiên vị tần số dựa trên phân phối thực nghiệm trên các quan hệ (Esubj, Eobj, Ψ) trong tập huấn luyện. Số hạng thiên vị tần số do đó phản ánh rằng, đối với các cặp nhất định của danh mục thực thể (csubj, cobj), các loại quan hệ Ψ ∈ {parent of, followed by, null} có khả năng xảy ra nhiều hơn hoặc ít hơn. Cuối cùng, các đặc trưng theo cặp ρpair 1, ρpair 2 và ρpair 3 sau đó được kết hợp thành một đặc trưng đầu ra theo cặp ρpair out được sử dụng để dự đoán PΨj cho tất cả các cặp thực thể.

Kết quả là, toàn bộ thành phần cho phân loại quan hệ có thể huấn luyện đầu cuối. Đây là một sự khác biệt quan trọng của DSG so với các hệ thống hiện có.

D. Hậu xử lý dựa trên ngữ pháp (C4)

Thành phần này của hệ thống chuyển đổi các cấu trúc tài liệu phân cấp Hi, i = 1,...,n, bao gồm các thực thể được dự đoán E và quan hệ R, thành một cấu trúc tài liệu được hậu xử lý H'. Ở đây, mục tiêu là đảm bảo một định dạng có cấu trúc cây hợp lệ có thể được sử dụng sau đó để tạo ra các định dạng đầu ra khác nhau như hOCR. Để thực hiện điều này, chúng tôi đảm bảo rằng tất cả các thực thể tạo thành một cấu trúc cây w.r.t. các quan hệ phân cấp của chúng loại Ψ = parent of và được kết nối với một thực thể gốc với c = DOC.ROOT. Chúng tôi lưu ý rằng hậu xử lý của chúng tôi không đưa ra bất kỳ giả định nào về sự chồng lấp hình học hoặc bố cục tài liệu. Để đạt được mục tiêu này, nó hoàn toàn dựa trên ngữ pháp tài liệu và các điểm tin cậy được dự đoán PΨ. Cụ thể, chúng tôi áp dụng hậu xử lý dựa trên ngữ pháp trong các bước tuần tự để giải quyết các thực thể gốc (grt), các thực thể bất hợp pháp (gilg) và các quan hệ còn thiếu (gmis) như sau:

• Thực thể gốc (grt): Chúng tôi thêm các thực thể bổ sung để xây dựng một khung sườn cơ bản cho các file tài liệu. Cụ thể, chúng tôi thêm các thực thể gốc DOC.ROOT, ARTICLE và META. Để cho phép huấn luyện đầu cuối hoàn toàn, chúng tôi cho phép dự đoán các thực thể này trong quá trình huấn luyện.

• Quan hệ bất hợp pháp (gilg): Trong quá trình huấn luyện và suy luận của phân loại quan hệ, không có hạn chế nào được đưa ra về các kết hợp có thể có của các cặp thực thể. Lựa chọn này được thực hiện để cho phép tính linh hoạt trong quá trình huấn luyện đầu cuối (ví dụ: trong trường hợp các thực thể không được dự đoán chính xác bởi thành phần phát hiện của DSG). Tuy nhiên, tính linh hoạt như vậy có thể dẫn đến các quan hệ vi phạm ngữ pháp tài liệu của chúng tôi và do đó các xung đột phải được giải quyết. Ví dụ, chúng tôi loại bỏ các chu trình tiềm năng để cấu trúc tài liệu phân cấp tạo thành một cấu trúc cây.

• Quan hệ còn thiếu (gmis): Quan hệ được thêm vào để đảm bảo một cấu trúc cây hợp lệ để mỗi thực thể có một quan hệ hợp lệ loại Ψ = parent of. Cụ thể, mỗi thực thể phải có đúng một cha, ngoại trừ thực thể với c = DOC.ROOT, không có cha. Nếu một thực thể E không có cha, chúng tôi thêm một quan hệ tương ứng (Esubj, Eobj, Ψ) với Eobj = E và Ψ = parent of và Esubj dựa trên các điểm tin cậy được dự đoán.

E. Công cụ chuyển đổi hOCR (C5)

Trong DSG, thành phần cuối cùng là một công cụ chuyển đổi hOCR. Nó nhận một cấu trúc tài liệu được hậu xử lý H' làm đầu vào và sau đó chuyển đổi nó thành một file hOCR tương thích với các công cụ mã nguồn mở phổ biến cho quy trình xử lý tài liệu. Chúng tôi mở rộng định dạng hOCR phổ biến để bổ sung chứa các cấu trúc phân cấp.

F. Chi tiết thực hiện

Hệ thống DSG của chúng tôi dựa trên kiến trúc neural motifs sử dụng các triển khai từ [35], [32]. Tuy nhiên, chúng tôi thực hiện các điều chỉnh không tầm thường để phù hợp với tác vụ tạo ra cấu trúc tài liệu phân cấp của chúng tôi, như được nêu chi tiết trong phần sau.

Tiền xử lý hình ảnh (thành phần C1): Hình ảnh được thay đổi kích thước song tuyến tính để cạnh nhỏ nhất có kích thước tối đa φmax s. Nếu cạnh dài nhất vượt quá kích thước tối đa được định nghĩa trước φmax l sau bước này, việc thay đổi kích thước được thực hiện để độ dài cạnh dài nhất là φmax l. Trong quá trình huấn luyện, kích thước hình ảnh được thay đổi cho mục đích tăng cường. Để thực hiện điều này, φmax s được chọn ngẫu nhiên từ một tập hợp các kích thước khác nhau. Chúng tôi đặt φmax l ở 600, trong khi φmax s được chọn ngẫu nhiên từ phạm vi [250,...,550] sử dụng gia số 50. Trong quá trình huấn luyện, hình ảnh được thay đổi kích thước ngẫu nhiên bằng cách áp dụng sơ đồ thay đổi kích thước nói trên. Để kiểm tra, chúng tôi đặt φmax s = 400 và φmax l = 600.

Phát hiện thực thể (thành phần C2): Thành phần phát hiện thực thể dựa trên kiến trúc Faster R-CNN. Chúng tôi sử dụng backbone ResNet độ sâu 50 trong hệ thống của chúng tôi. Huấn luyện thành phần C2 sử dụng số hạng mất mát LC2 = Lcls RPN + Lloc RPN + Lcls E + Lloc E. Các mất mát Lloc RPN và Lcls RPN phạt các lỗi định vị và phân loại cho các vùng ứng viên được tạo bởi mạng đề xuất vùng (RPN). Hơn nữa, mục tiêu định vị và phân loại chính xác các thực thể được dự đoán được công thức hóa thông qua các mất mát Lloc E và Lcls E, tương ứng.

Trong quá trình huấn luyện, tối đa 50 thực thể được chuyển từ thành phần phát hiện thực thể (C2) đến thành phần chịu trách nhiệm về phân loại quan hệ và tinh chỉnh thực thể (C3).

Phân loại quan hệ (thành phần C3(i)): Đầu quan hệ sử dụng một LSTM hai chiều với một lớp lặp lại, kích thước lớp ẩn 512 và dropout 0.2. Các đặc trưng đầu vào ngữ cảnh quan hệ ρrelin 1 được trích xuất thông qua kiến trúc Faster R-CNN cơ bản cho hộp giới hạn Bj của mỗi thực thể. Cụ thể, mạng nơ-ron tích chập của kiến trúc Faster R-CNN xử lý hình ảnh đầu vào trong nhiều bước tuần tự với độ phân giải không gian giảm dần. Các đặc trưng đầu ra của Faster R-CNN sau đó được đưa vào một mạng kim tự tháp đặc trưng (FPN), tạo ra các bản đồ đặc trưng thị giác đa tỷ lệ. Bốn bản đồ đặc trưng thị giác đa tỷ lệ tương ứng với vùng hình ảnh của Bj được lọc bởi một lớp căn chỉnh, được nối và được chuyển qua hai lớp kết nối đầy đủ với kích hoạt ReLu để tạo ra các vector đặc trưng có chiều 1024. Các vector đặc trưng này sau đó được sử dụng như đặc trưng đầu vào ngữ cảnh quan hệ ρrelin vis.

Theo [31], chúng tôi trích xuất bản đồ đặc trưng thị giác ρpair 1 cho hộp giới hạn hợp nhất Bpair của các cặp chủ thể-đối tượng (Esubj, Eobj). Việc trích xuất tiến hành tương tự như ρrelin vis nhưng sử dụng vùng Bpair để lọc các bản đồ đặc trưng thị giác đa tỷ lệ, dẫn đến các đặc trưng theo cặp ρpair 1. ρpair 1 và ρpair 2 được đưa vào các lớp kết nối đầy đủ Wpair 1 và Wpair 2 với chiều đầu ra 4096 và sau đó được kết hợp bằng phép nhân từng phần tử. Vector đặc trưng kết quả cuối cùng được đưa vào một lớp kết nối đầy đủ Wpair rel với chiều đầu ra bằng số loại quan hệ và được thêm vào số hạng thiên vị tần số ρpair 3, dẫn đến đặc trưng đầu ra theo cặp ρpair out = Wpair rel((Wpair 1 ρpair 1) ◦ (Wpair 2 ρpair 2))) + ρpair 3. ρpair out sau đó được sử dụng để dự đoán xác suất lớp cho các quan hệ. Để huấn luyện và đánh giá, các bộ ba quan hệ ground-truth được khớp với các bộ ba ứng viên bằng cách tính toán các điểm IoU (xem Sec. VI-A) Ssubj = IoU(Esubj, EGT subj) và Sobj = IoU(Eobj, EGT obj) của các thực thể chủ thể và đối tượng trong quan hệ, tương ứng. Phù hợp với mục tiêu khớp duy nhất tất cả các quan hệ ground-truth, chúng tôi chỉ cho phép một quan hệ ứng viên được xem xét cho mỗi quan hệ ground-truth.

Tinh chỉnh thực thể (thành phần C3(ii)): Đầu tinh chỉnh dựa trên một LSTM hai chiều với một lớp lặp lại, kích thước lớp ẩn 512 và dropout 0.2. Các đầu vào cho LSTM được sắp xếp theo tọa độ x của chúng (điểm trung tâm) từ trái sang phải. Các đặc trưng đầu vào ngữ cảnh tinh chỉnh bổ sung được tính toán như sau. Bản đồ đặc trưng thị giác ρrefin vis được tính toán tương tự như ρrelin vis. Embedding danh mục ρrefin cat được tính toán bằng cách ánh xạ tên của danh mục ngữ nghĩa trực tiếp lên embedding từ GloVe với tên giống hệt nhau. Đối với một số danh mục gặp phải trong bộ dữ liệu của chúng tôi, không có sự khớp trực tiếp. Đối với những danh mục này, chúng tôi sử dụng ánh xạ sau: (BIBLIOGRAPHY BLOCK 7→ bibliography), (TEXT BLOCK 7→ paragraph), (FIGURE CAPTION 7→ caption), (FIGURE GRAPHIC 7→ graphic), (PAGE NR. 7→ numbering), (TABLE CAPTION 7→ caption). Chiều của embedding từ được đặt thành 200 trong thí nghiệm của chúng tôi. Đối với embedding vị trí ρrefin pos, chiều rộng hộp giới hạn và tọa độ x được chuẩn hóa theo chiều rộng của hình ảnh kích thước đầy đủ. Tương tự, chúng tôi chuẩn hóa chiều cao hộp và tọa độ y theo chiều cao của hình ảnh kích thước đầy đủ.

Huấn luyện thành phần C3 sử dụng số hạng mất mát LC3 = Lref + Lrel, bao gồm các mất mát của phân loại quan hệ, Lrel và tinh chỉnh lớp, Lref.

V. BỘ DỮ LIỆU

Chúng tôi so sánh hệ thống của chúng tôi sử dụng hai bộ dữ liệu khác nhau. Các bộ dữ liệu phù hợp để đánh giá phải chứa chú thích cho cấu trúc tài liệu phân cấp đầy đủ, và do đó các bộ dữ liệu hiện có cho đến nay bị giới hạn ở các bài báo khoa học. Tuy nhiên, các bài báo khoa học tuân theo một cấu trúc khá tương tự. Để đạt được mục tiêu này, chúng tôi cũng giới thiệu một bộ dữ liệu mới có tên E-Periodica chứa các tạp chí thực tế, ngoại tuyến. Điều này có lợi cho việc đánh giá của chúng tôi vì nó cung cấp một bộ dữ liệu với sự đa dạng lớn trong cách trình bày và do đó có cấu trúc tài liệu phức tạp. Các bộ dữ liệu được mô tả trong phần sau.

A. arXivdocs-target

Bộ dữ liệu hiện có để huấn luyện và đánh giá, arXivdocs-target, chứa 362 tài liệu được gắn nhãn thủ công. Trước đây, bộ dữ liệu chỉ được sử dụng để huấn luyện phát hiện thực thể chứ không phải phân loại quan hệ. Để đạt được mục tiêu này, chúng tôi thực hiện các bước xử lý sau để cho phép huấn luyện đầu cuối. Chúng tôi đầu tiên chia bất kỳ tài liệu nhiều trang nào thành các hình ảnh một trang riêng biệt. Sau đó chúng tôi chuyển đổi bộ dữ liệu thành một định dạng chuẩn hóa để cho phép xử lý với các thư viện benchmark chuẩn hóa và codebase phân tích cú pháp tài liệu.

B. E-Periodica

E-Periodica là một dự án nhằm mục đích số hóa một loạt các tạp chí lịch sử và đương đại cho các thế hệ tương lai. Nó bao gồm các tạp chí từ các ngôn ngữ nguồn khác nhau như tiếng Anh, tiếng Đức, tiếng Pháp và tiếng Ý. Các trang tạp chí thường có cấu trúc phức tạp và ít nhất quán trong các quy tắc định dạng so với tài liệu khoa học. Toàn bộ E-Periodica chứa hơn 8 triệu trang từ hơn 400 tạp chí. Chúng tôi đã gắn nhãn thủ công 542 tài liệu bao gồm 11.446 thực thể được gắn nhãn. Cụ thể, chúng tôi lấy mẫu một tập con các trang tài liệu từ các số tạp chí của sáu thập kỷ qua. Hơn nữa, phân phối của các trang cho mỗi tạp chí rất bất thường, và vì lý do này, chúng tôi chỉ xem xét năm trang cho mỗi số của một tạp chí nhất định trong một năm nhất định. Sau đó chúng tôi gắn nhãn các thực thể và quan hệ giữa tất cả các thực thể để tạo thành cấu trúc tài liệu phân cấp. Chi tiết về quy trình gắn nhãn được cung cấp dưới đây. Chúng tôi chia bộ dữ liệu thành các tập huấn luyện, xác thực và kiểm tra gồm 270, 135 và 137 mẫu, tương ứng.

Quy trình gắn nhãn: Việc gắn nhãn thủ công của chúng tôi tuân theo một quy trình hai bước: (1) gắn nhãn thực thể và (2) gắn nhãn quan hệ. Trong bước gắn nhãn thực thể, một hộp giới hạn được vẽ xung quanh các thực thể trên một trang và một danh mục ngữ nghĩa được gán cho mỗi thực thể. Đối với bước gắn nhãn quan hệ, chúng tôi đầu tiên gắn nhãn quan hệ để xác định thứ tự đọc của một trang bằng cách tập trung vào Ψ = followed by. Nếu các thực thể lồng nhau, chúng tôi bổ sung gắn nhãn quan hệ đặc trưng cho cấu trúc lồng nhau được đưa ra bởi Ψ = parent of (ví dụ: (FIGURE, FIGURE CAPTION, parent of)).

Các cân nhắc cụ thể được thực hiện cho việc gắn nhãn cấu trúc phân cấp trong E-Periodica. Không giống như tài liệu khoa học, nhiều trang tạp chí thiếu thứ tự đọc chuẩn hóa (ví dụ: các bài báo riêng biệt trong một tạp chí có thể được đọc theo thứ tự tùy ý). Để mô hình hóa điều này, chúng tôi chỉ định hai danh mục ngữ nghĩa cho mục đích này: UNORDERED GROUP và ORDERED GROUP. Một UNORDERED GROUP đề cập đến các phần không thuộc về thứ tự đọc cấp độ tài liệu chung (ví dụ: quảng cáo). Một ORDERED GROUP đề cập đến các phần thuộc về thứ tự đọc thường xuyên (ví dụ: một cột duy nhất với một bài báo riêng biệt). Chi tiết thêm và thống kê tóm tắt được bao gồm trong GitHub của chúng tôi.

VI. THIẾT LẬP THỰC NGHIỆM

A. Các chỉ số hiệu suất

Chúng tôi đánh giá riêng biệt hiệu suất của hệ thống cho (i) phát hiện thực thể và (ii) tạo ra cấu trúc trong đó các quan hệ phân cấp được xem xét. Để đạt được mục tiêu này, chúng tôi điều chỉnh benchmarking trong các tác vụ liên quan từ tạo đồ thị cảnh cho mục đích phân tích cú pháp cấu trúc tài liệu của chúng tôi.

Phát hiện thực thể: Chúng tôi tuân theo thực hành phổ biến trong benchmarking phát hiện thực thể. Cụ thể, chúng tôi xác định có bao nhiêu thực thể được dự đoán chính xác trong tổng số các thực thể ground-truth. Chúng tôi so sánh các thực thể được dự đoán Ej = (cj, Bj, Pj) với các thực thể ground-truth, bao gồm danh mục thực ĉcj và hộp giới hạn thực ĉBj. Ở đây, chúng tôi đo lường sự chồng lấp giữa các hộp giới hạn của cùng một danh mục. Cụ thể, chúng tôi tính toán giao điểm trên hợp nhất (IoU) được gọi là

IoU = area(Bj ∩ ĉBj) / area(Bj ∪ ĉBj). (1)

Các thực thể được dự đoán được coi là đúng dương nếu IoU của chúng cao hơn ngưỡng được định nghĩa trước. Nếu nhiều hơn một thực thể vượt quá ngưỡng đó cho cùng một thực thể ground-truth, thực thể có IoU cao nhất được coi là đúng dương. Bất kỳ thực thể được dự đoán và thực thể ground-truth không được khớp nào được coi là sai dương và sai âm, tương ứng. Chúng tôi so sánh các ngưỡng IoU 0.5 và 0.75. Tất cả các tính toán IoU dựa trên API trong [44].

Chúng tôi tiếp tục tính toán độ chính xác trung bình (AP) cho mỗi danh mục ngữ nghĩa ck ∈ C. Hiệu suất tổng thể trên tất cả các danh mục được đưa ra bởi độ chính xác trung bình của trung bình (mAP) với (0: tệ nhất, 100: tốt nhất).

B. Huấn luyện và điều chỉnh siêu tham số

Khởi tạo: Chúng tôi khởi tạo hệ thống với hai bước huấn luyện trước. Đầu tiên, các hệ thống được khởi tạo với trọng số của kiến trúc Faster R-CNN được huấn luyện trên bộ dữ liệu COCO với tăng cường copy-paste. Thứ hai, vì bộ dữ liệu COCO không chứa tài liệu, chúng tôi sau đó tiến hành huấn luyện trước tất cả các hệ thống trên arXivdocs-weak. Giống như arXivdocs-target, bộ dữ liệu này được tạo ra cho các bài báo khoa học, nhưng nó chỉ được gắn nhãn bằng cơ chế giám sát yếu. Điều này cho phép chuẩn bị tốt hơn cho các tác vụ phân tích cú pháp tài liệu. Trọng số hệ thống kết quả sau đó được sử dụng làm điểm bắt đầu cho các thí nghiệm của chúng tôi.

Huấn luyện: Chúng tôi đầu tiên lấy mẫu 128 từ tất cả các cặp thực thể-thực thể có thể có mỗi lần lặp huấn luyện để phục vụ như đầu vào cho phân loại quan hệ của đầu quan hệ. Điều này giảm độ phức tạp tính toán của việc huấn luyện và cho phép chúng tôi lấy mẫu một tập hợp mẫu dương và âm cân bằng hơn, vì phần lớn các cặp thực thể-thực thể tương ứng với các quan hệ loại Ψ = null. Không giống như các quy trình huấn luyện phổ biến từ tạo đồ thị cảnh, chúng tôi không áp dụng các ràng buộc hình học trên các cặp ứng viên (Esubj, Eobj). Cụ thể, điều này có nghĩa là các cặp thực thể-thực thể không có sự chồng lấp hình học được xem xét cho dự đoán quan hệ. Điều này đặc biệt quan trọng đối với các quan hệ loại Ψ = followed by, nơi các hộp giới hạn (Bsubj và Bobj) của (Esubj và Eobj) không giao nhau. Để cho phép huấn luyện đầu tinh chỉnh và quan hệ trong toàn bộ quy trình huấn luyện, chúng tôi thêm bất kỳ thực thể ground-truth còn thiếu nào vào tập hợp các thực thể được chuyển đến đầu tinh chỉnh. Điều này là để tránh các trường hợp không có thực thể nào hoặc chỉ có các thực thể sai lầm được phát hiện và do đó không có mẫu học tập dương nào có thể được cung cấp. Điều này xảy ra, ví dụ, ở đầu quy trình huấn luyện.

Chúng tôi huấn luyện hệ thống DSG đầu cuối thông qua một mục tiêu chung bao gồm (i) thành phần phát hiện thực thể dựa trên thành phần Faster R-CNN và (ii) thành phần phân loại quan hệ và tinh chỉnh thực thể. Các mất mát LC2 và LC3 cho cả hai thành phần được kết hợp để huấn luyện hệ thống DSG của chúng tôi. Hệ thống của chúng tôi được huấn luyện trong tối đa 200.000 lần lặp với kích thước batch 4 và tốc độ học 0.001. Chúng tôi áp dụng dừng sớm dựa trên hiệu suất cho phát hiện thực thể trên tập xác thực.

Hiệu suất tính toán: Chúng tôi đo lường hiệu suất tính toán của hệ thống trên một máy có một GPU NVIDIA Titan Xp duy nhất với kích thước bộ nhớ 12GB. Ở đây, thời gian trung bình để xử lý một mẫu, được đo trên tập xác thực với kích thước batch 1, là ~0.1616 giây. Việc tích hợp hậu xử lý dựa trên ngữ pháp của chúng tôi đi kèm với chỉ một chi phí nhỏ và dẫn đến thời gian xử lý chung trung bình ~0.1776 giây cho cùng một thiết lập tính toán.

C. Baseline

Chúng tôi đo lường DSG được đề xuất của chúng tôi so với các hệ thống hiện đại cho phân tích cú pháp cấu trúc tài liệu.

• DocParser: Chúng tôi triển khai lại DocParser, một hệ thống hiện đại cho phân tích cú pháp cấu trúc phân cấp. Theo hiểu biết tốt nhất của chúng tôi, đây là baseline duy nhất phù hợp có thể phân tích cú pháp toàn bộ cấu trúc tài liệu (xem Sec. II). Để đảm bảo tính so sánh giữa DocParser và hệ thống của chúng tôi, chúng tôi sử dụng kiến trúc Faster R-CNN với backbone ResNet độ sâu 50 cho phát hiện thực thể. Cần lưu ý, DocParser sử dụng suy luận cho phân loại quan hệ và do đó không thể huấn luyện đầu cuối. Do đó, chúng tôi đánh giá DocParser sử dụng các suy luận ban đầu trong [11] cho phân loại quan hệ. Chúng tôi mở rộng các suy luận để phù hợp với thực thể gốc bổ sung ARTICLE (tức là ánh xạ nó lên danh mục thực thể DOCUMENT).

Chúng tôi tiếp tục so sánh các biến thể khác nhau của DSG hoạt động như các nghiên cứu loại bỏ để chứng minh tầm quan trọng của việc huấn luyện đầu cuối. Qua đó, chúng tôi có thể đánh giá tầm quan trọng của việc huấn luyện đầu cuối so với huấn luyện 2 giai đoạn.

• DSG 2-stage (C2 frozen): Trong giai đoạn đầu tiên, thành phần C2 của DSG được huấn luyện độc quyền theo dự đoán chính xác của các thực thể. Trong giai đoạn thứ hai, thành phần C3 được thêm vào huấn luyện DSG. Tuy nhiên, trọng số của thành phần C2 bị đóng băng trong giai đoạn thứ hai. Do đó, mất mát để cập nhật C3 chỉ dựa trên các dự đoán của đầu quan hệ và đầu tinh chỉnh.

• DSG 2-stage (C2 unfrozen): Trong giai đoạn đầu tiên, thành phần C2 của DSG được huấn luyện. Ở đây, một mất mát được sử dụng chỉ học so với dự đoán chính xác của các thực thể. Trong giai đoạn thứ hai, chúng tôi cho phép cập nhật trọng số tham số cho cả C2 và C3 của hệ thống. Ở đây, một mất mát được sử dụng chỉ học so với các dự đoán của đầu quan hệ và đầu tinh chỉnh của thành phần C3, nhưng không so với dự đoán của các thực thể bởi thành phần C2.

• DSG end-to-end (w/o postprocessing): Đây là hệ thống DSG từ trên được huấn luyện theo cách đầu cuối nhưng không có hậu xử lý từ thành phần C4.

• DSG end-to-end (w/ postprocessing): Đây là hệ thống DSG từ trên được huấn luyện theo cách đầu cuối.

Quy trình huấn luyện cho baseline: Chúng tôi sử dụng các thiết lập siêu tham số giống hệt nhau để huấn luyện thành phần phát hiện thực thể được sử dụng cho các hệ thống baseline DocParser, DSG 2-stage và DSG end-to-end. Tất cả các hệ thống được huấn luyện trong tối đa 100.000 lần lặp với tốc độ học 0.01 và kích thước batch 8. Việc huấn luyện này chỉ bao gồm dự đoán của các thực thể tài liệu thông qua thành phần C2 và sử dụng số hạng mất mát LC2. Đối với DocParser, chúng tôi áp dụng dừng sớm dựa trên điểm mAP cho ngưỡng IoU 0.5 trên tập xác thực.

Sau khi huấn luyện thành phần C2 cho phát hiện thực thể, chúng tôi tiếp tục với giai đoạn huấn luyện thứ hai của DSG 2-stage (C2 frozen) và DSG 2-stage (C2 unfrozen), trong đó các hệ thống được huấn luyện với một mục tiêu chung cho phân loại quan hệ và tinh chỉnh thực thể sử dụng số hạng mất mát LC3. Chúng tôi thực hiện giai đoạn thứ hai với tốc độ học 0.001 và kích thước batch 8 trong tối đa 100.000 lần lặp. Không giống như DSG, các hệ thống trong nghiên cứu loại bỏ không sử dụng mất mát bổ sung LC2 cho phát hiện thực thể thông qua thành phần C2 trong giai đoạn này. Chúng tôi áp dụng dừng sớm dựa trên điểm mAP cho ngưỡng IoU 0.5 trên tập xác thực trong giai đoạn thứ hai.

Tạo ra cấu trúc: Để đánh giá cấu trúc phân cấp được tạo ra tốt như thế nào, chúng tôi thực hiện một đánh giá về các bộ ba cho các quan hệ. Cụ thể, chúng tôi đo lường các trận đấu chính xác của các quan hệ được dự đoán (Esubj, Eobj, Ψ) so với các quan sát ground-truth và báo cáo điểm F1 tương ứng. Điểm F1 là trung bình điều hòa của độ chính xác và độ nhạy cho việc dự đoán các bộ ba này, tức là F1 = 2precision · recall / (precision + recall) với (0: tệ nhất, 1: tốt nhất). Để thực hiện điều này, chúng tôi đầu tiên xác định tất cả các trận đấu của các hộp giới hạn của các thực thể. Sau đó, một quan hệ được coi là một trận đấu nếu loại quan hệ được dự đoán và cả hai hộp giới hạn khớp với một bộ ba ground-truth.

Các thước đo hiệu suất trên của chúng tôi khá nghiêm ngặt khi so sánh với các đánh giá trong tạo đồ thị cảnh thông thường (ví dụ: [31], [32]). Nhớ lại rằng chúng tôi coi một trận đấu nếu và chỉ nếu loại quan hệ và cả hai hộp giới hạn khớp với một bộ ba ground-truth duy nhất. Ngược lại, tạo đồ thị cảnh thông thường sử dụng một định nghĩa nới lỏng trong đó một bộ ba quan hệ được dự đoán (Ep 1, Ep 2, φp) được coi là một trận đấu với một bộ ba ground-truth (Eg 1, Eg 2, φp) nếu φp = φg và nếu một sự chồng lấp IoU được tìm thấy giữa các thực thể ground-truth và cả hai thực thể được dự đoán. Tuy nhiên, định nghĩa này cho phép nhiều hơn một thực thể được dự đoán có thể được khớp với một thực thể ground-truth trong quá trình đánh giá dự đoán quan hệ. Trong đánh giá của chúng tôi, chúng tôi áp dụng một hiệu suất nghiêm ngặt hơn xem xét tối đa một trận đấu duy nhất với thực thể ground-truth, tuân theo cùng một quy trình như trong đánh giá phát hiện thực thể.

Lựa chọn các chỉ số hiệu suất của chúng tôi rất quan trọng để phân biệt hiệu quả giữa các thực thể lồng nhau chặt chẽ. Điều này có liên quan, ví dụ, để phân biệt giữa một hình được bao quanh một hình phụ. Sử dụng khớp IoU đơn giản, một thực thể được dự đoán có thể được khớp với bất kỳ thực thể ground-truth ứng viên nào trong hai thực thể (tức là hình và hình phụ). Tuy nhiên, để tái tạo cấu trúc tài liệu phân cấp, việc xác định chính xác các quan hệ phân cấp chính xác giữa các thực thể là rất quan trọng để đạt được một cấu trúc cây duy nhất và hợp lệ.

VII. KẾT QUẢ

A. Thí nghiệm số

Mục tiêu của các thí nghiệm của chúng tôi là xác nhận hiệu quả của DSG trong việc tạo ra cấu trúc tài liệu phân cấp. Do đó, chúng tôi tiến hành hai mặt: (1) Chúng tôi đầu tiên đo lường hiệu suất trong phát hiện thực thể, và (2) chúng tôi đo lường hiệu suất tạo ra cấu trúc phân cấp một cách chính xác.

Phát hiện thực thể: Chúng tôi báo cáo hiệu suất cho phát hiện thực thể trong Bảng II (arXivdocs-target) và Bảng III (E-Periodica). Ở đây, chúng tôi báo cáo kết quả cho DSG của chúng tôi (hàng cuối). Chúng tôi tiếp tục nêu kết quả cho DocParser [11], một baseline hiện đại và là hệ thống hiện có duy nhất cho tác vụ của chúng tôi. Chúng tôi tiếp tục so sánh các biến thể khác nhau của hệ thống để đánh giá tầm quan trọng của việc huấn luyện đầu cuối so với huấn luyện 2 giai đoạn.

Chúng tôi có những quan sát sau. (1) Hiệu suất trong phát hiện thực thể thường tốt hơn cho arXivdocs-target (bài báo khoa học) so với E-Periodica (tạp chí). Điều này chứng minh rằng bộ dữ liệu mới của chúng tôi là một bối cảnh thách thức, thực tế để đánh giá. Cụ thể, sự khác biệt về hiệu suất có thể được giải thích bởi việc định dạng của tạp chí được đặc trưng bởi sự biến đổi khá lớn so với các bài báo khoa học. (2) DSG của chúng tôi với việc huấn luyện đầu cuối liên tục hoạt động tốt nhất. Cụ thể, nó vượt trội hơn DocParser hiện đại từ [11]. Ví dụ, với ngưỡng IoU 0.5, mAP của DSG đầu cuối của chúng tôi tốt hơn DocParser 1.91 điểm phần trăm cho arXivdocs-target và tốt hơn 7.03 điểm phần trăm cho E-Periodica. Điều này dịch thành một cải thiện tương đối 2.46% và 12.71%, tương ứng. (3) Cải thiện hiệu suất tương đối lớn hơn cho E-Periodica so với arXivdocs-target có thể là do hệ thống của chúng tôi có thể tận dụng trực tiếp các quan hệ được gắn nhãn và học từ chúng, trong khi DocParser bị giới hạn ở suy luận đơn giản. (4) DSG của chúng tôi sử dụng huấn luyện đầu cuối vượt trội hơn phương pháp huấn luyện 2 giai đoạn. Do đó, một trong những lý do cho hiệu suất mạnh mẽ của phương pháp của chúng tôi là quy trình học tập kết hợp, trong đó các thành phần C2 và C3 cho phép cập nhật tham số toàn hệ thống. Tóm lại, các kết quả chứng minh hiệu quả của DSG của chúng tôi.

Bảng I cung cấp một phân tích theo các danh mục ngữ nghĩa khác nhau trên bộ dữ liệu E-Periodica. Rõ ràng, DSG của chúng tôi liên tục tốt hơn cho đại đa số các danh mục ngữ nghĩa. Ví dụ, với ngưỡng IoU 0.5, nó đạt được cải thiện 9.34 điểm phần trăm cho danh mục ARTICLE. Các thực thể của danh mục này rất quan trọng trong quá trình phân loại quan hệ, vì chúng phản ánh sự phân đoạn cấp cao của các trang tài liệu và do đó được sử dụng trong một số lượng lớn các quan hệ phân cấp. Rõ ràng, mục tiêu huấn luyện đầu cuối của DSG kết hợp các mất mát cấp độ quan hệ cung cấp cho hệ thống các tín hiệu giám sát hữu ích cho danh mục này. DocParser có điểm số hơi tốt hơn hệ thống của chúng tôi cho danh mục HEADER ở ngưỡng IoU 0.5. Chúng tôi đưa ra giả thuyết rằng điều này có thể là do các thực thể HEADER chỉ chiếm 1.38% tất cả các thực thể và ít liên quan đến việc phân tích cú pháp cấu trúc tài liệu, vì chúng thường không phải là một phần của thứ tự đọc trong các bài báo tạp chí. Như vậy, DSG ít được khuyến khích tối ưu hóa cho danh mục thực thể này thông qua mục tiêu huấn luyện đầu cuối của nó.

Bảng I: Hiệu suất phát hiện thực thể theo danh mục trên E-Periodica. Báo cáo: độ chính xác trung bình (AP) cho mỗi danh mục ngữ nghĩa. So sánh: DocParser và DSG end-to-end.

Bảng II: Hiệu suất (mAP) của phát hiện thực thể trên arXivdocs-target.

Bảng III: Hiệu suất phát hiện thực thể trên E-Periodica.

Tạo ra cấu trúc: Bây giờ chúng tôi đánh giá độ chính xác mà các quan hệ phân cấp được tạo ra một cách chính xác. Để thực hiện điều này, chúng tôi một lần nữa báo cáo hiệu suất cho cả hai bộ dữ liệu, cụ thể là arXivdocs-target (Bảng IV) và E-Periodica (Bảng V).

Chúng tôi có những quan sát sau. (1) Chúng tôi một lần nữa đo lường hiệu suất tổng thể tốt hơn cho arXivdocs-target so với E-Periodica. Điều này được mong đợi do định dạng phức tạp của các bài báo tạp chí. (2) Chúng tôi thấy rằng DSG của chúng tôi hoạt động tốt nhất. Cụ thể, nó vượt trội hơn DocParser hiện đại [11] từ tài liệu với một biên độ rõ ràng. Hệ thống của chúng tôi cải thiện so với F1 từ DocParser 7.63% (arXivdocs-target) và 183.44% (E-Periodica). (3) Chúng tôi một lần nữa tìm thấy cải thiện lớn hơn cho E-Periodica so với arXivdocs-target. Điều này có thể được giải thích bởi hệ thống của chúng tôi có thể tận dụng trực tiếp các quan hệ được gắn nhãn và học từ chúng, trong khi DocParser bị giới hạn ở suy luận đơn giản. (4) DSG của chúng tôi hưởng lợi từ việc huấn luyện đầu cuối. Như có thể thấy trong các nghiên cứu loại bỏ của chúng tôi, việc huấn luyện đầu cuối vượt trội hơn việc huấn luyện 2 giai đoạn. (5) Chúng tôi quan sát thấy một sự giảm nhỏ trong điểm F1 sau khi áp dụng hậu xử lý các cấu trúc phân cấp H được tạo ra bởi DSG. Một lý do cho điều này nằm trong quy trình đánh giá nghiêm ngặt của chúng tôi, kết hợp với động lực tạo ra các cấu trúc cây hợp lệ như một kết quả của việc hậu xử lý của chúng tôi. Để minh họa điều này, hãy xem xét một thực thể bị hệ thống của chúng tôi bỏ lỡ. Nếu nút này thường sẽ được định vị như một nút trung gian trong tài liệu, việc hậu xử lý của chúng tôi có thể kết nối các thực thể kế tiếp và tiền nhiệm của nó để tạo thành một cấu trúc tài liệu hợp lệ. Tuy nhiên, điều này sẽ có tác động tiêu cực đến hiệu suất tổng thể, nhưng tạo điều kiện cho mục tiêu tạo ra các cấu trúc cây hợp lệ của chúng tôi.

Đánh giá trên cũng có một hàm ý quan trọng. DocParser xây dựng dựa trên các suy luận được điều chỉnh đặc biệt cho các bài báo khoa học trong bộ dữ liệu arXivdocs-target. Vì lý do này, DocParser không trực tiếp hiệu quả cho các bộ dữ liệu khác như E-Periodica mà không có kỹ thuật thủ công lại.

Chúng tôi nhắc nhở rằng chúng tôi thực thi một đánh giá nghiêm ngặt trong đó tuple hoàn chỉnh bao gồm cả hai thực thể phải đúng. Do đó, tác vụ phân tích cú pháp cấu trúc của chúng tôi dựa trên việc xác định chính xác mọi thực thể và loại quan hệ trong một bộ ba nhất định. Vì điều này, điểm F1 cao yêu cầu độ chính xác phát hiện cao trong nhận dạng thực thể. Tuy nhiên, hiệu suất của hệ thống của chúng tôi rất hiệu quả trong thực tế nơi mục tiêu là khôi phục cấu trúc tài liệu tổng thể.

Đánh giá định tính: Chúng tôi đã thực hiện một đánh giá định tính (xem GitHub của chúng tôi tại https://github.com/j-rausch/DSG). Qua đó, chúng tôi chứng minh rằng chúng tôi tạo ra các cấu trúc tài liệu có ý nghĩa và hiệu quả trong thực tế.

Bảng IV: Hiệu suất phân tích cú pháp cấu trúc trên arXivdocs-target.

Bảng V: Hiệu suất phân tích cú pháp cấu trúc trên E-Periodica.

VIII. THẢO LUẬN

Hệ thống mới: Hệ thống của chúng tôi có liên quan đến một số tác vụ xuôi dòng mà hiển thị tài liệu (ví dụ: file PDF, bản quét) phải được ánh xạ lên định dạng có thể phân tích cú pháp. Các ví dụ là [4], [5], [6], [7], [8], [9]. Các công trình gần đây (ví dụ: [18], [47]) đã giới thiệu các hệ thống dựa trên transformer cho việc huấn luyện trước quy mô lớn trên dữ liệu tài liệu nhưng cho các tác vụ khác như phát hiện thực thể và do đó không trích xuất cấu trúc phân cấp. Do đó, hệ thống của chúng tôi trực giao với các công trình như vậy và tạo ra một đóng góp quan trọng, không tầm thường. Quan trọng là, lợi thế chính của DSG của chúng tôi là nó có thể tạo ra cấu trúc tài liệu phân cấp hoàn chỉnh thông qua việc huấn luyện đầu cuối.

So sánh với hệ thống OCR: Nghiên cứu trước đây (ví dụ: [48]) đã nhiều lần chứng minh những thách thức trong các hệ thống OCR hiện có. Các hệ thống OCR thường không được thiết kế để tạo ra cấu trúc tài liệu phân cấp mà chủ yếu để suy luận nội dung văn bản từ hiển thị tài liệu. Kết quả là, các hệ thống OCR thường gặp khó khăn trong việc nhận dạng các cấu trúc chi tiết như hình phụ và thứ tự của chúng (xem phân tích định tính của chúng tôi ở trên). Hệ thống của chúng tôi làm giảm những thách thức này và do đó được thiết kế đặc biệt để tạo ra chính xác các cấu trúc tài liệu phân cấp với độ chi tiết cao để kích hoạt các tác vụ xuôi dòng. Để đạt được mục tiêu này, chúng tôi lựa chọn các chỉ số hiệu suất tương đối nghiêm ngặt để đảm bảo rằng các cấu trúc chi tiết được nhận dạng chính xác.

Điểm mạnh thực tiễn: Một điểm mạnh quan trọng của hệ thống của chúng tôi là nó có thể huấn luyện đầu cuối. Điều này cho phép hệ thống của chúng tôi tận dụng đầy đủ dữ liệu huấn luyện hiện có, bao gồm thông tin về các quan hệ phân cấp nắm bắt cấu trúc tuần tự và lồng nhau trong tài liệu. Ngược lại, các hệ thống trước đây [11] không thể huấn luyện đầu cuối mà suy luận quan hệ thông qua suy luận, do đó về cơ bản bỏ qua thông tin tương ứng trong dữ liệu huấn luyện. Kết quả là, hệ thống của chúng tôi giảm chi phí gắn nhãn cấu trúc tài liệu phân cấp một cách đáng kể. Tóm lại, hệ thống của chúng tôi đáp ứng một nhu cầu chính trong thực tế nơi việc tạo ra cấu trúc tài liệu thường phụ thuộc vào dữ liệu khan hiếm và nơi các hệ thống nên có thể tùy chỉnh một cách linh hoạt.

Bộ dữ liệu mới: Chúng tôi đóng góp một bộ dữ liệu mới quy mô lớn dựa trên tạp chí để tạo ra cấu trúc tài liệu phân cấp. Cụ thể, bộ dữ liệu của chúng tôi cung cấp một bối cảnh thực tế thách thức để đánh giá do sự đa dạng lớn trong bố cục của tạp chí. Chìa khóa cho bộ dữ liệu của chúng tôi là độ chi tiết lớn của các chú thích về cả các thực thể chi tiết và quan hệ giữa chúng. Điều này khác với các bộ dữ liệu khác, thường thô [17] và không có thông tin phân cấp [49].

Kết luận: Trong bài báo này, chúng tôi giới thiệu Document Structure Generator (DSG), một hệ thống mới để phân tích cú pháp cấu trúc tài liệu phân cấp có thể huấn luyện đầu cuối. Chúng tôi cho thấy rằng hệ thống của chúng tôi vượt trội hơn các hệ thống hiện đại. Bằng cách có thể huấn luyện đầu cuối, DSG của chúng tôi có giá trị trực tiếp trong thực tế ở chỗ nó có thể được điều chỉnh cho các tài liệu mới một cách đơn giản mà không cần kỹ thuật thủ công lại.
