# 2211.11559.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2211.11559.pdf
# Kích thước tệp: 22028448 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lập trình trực quan: Suy luận trực quan tổng hợp không cần huấn luyện
Tanmay Gupta, Aniruddha Kembhavi
PRIOR @ Allen Institute for AI
https://prior.allenai.org/projects/visprog
Hình 1. VISPROG là một hệ thống neuro-symbolic mô-đun và có thể diễn giải cho suy luận trực quan tổng hợp. Với một vài ví dụ về hướng dẫn ngôn ngữ tự nhiên và các chương trình cấp cao mong muốn, VISPROG tạo ra một chương trình cho bất kỳ hướng dẫn mới nào bằng cách sử dụng học trong ngữ cảnh trong GPT-3 và sau đó thực thi chương trình trên (các) hình ảnh đầu vào để có được dự đoán. VISPROG cũng tóm tắt các đầu ra trung gian thành một lý luận trực quan có thể diễn giải (Hình 4). Chúng tôi trình bày VISPROG trên các tác vụ yêu cầu kết hợp một tập hợp đa dạng các mô-đun để hiểu và thao tác hình ảnh, truy xuất kiến thức, và các phép toán số học và logic.

Tóm tắt
Chúng tôi trình bày VISPROG, một phương pháp neuro-symbolic để giải quyết các tác vụ trực quan phức tạp và tổng hợp dựa trên hướng dẫn ngôn ngữ tự nhiên. VISPROG tránh nhu cầu huấn luyện đặc thù cho tác vụ. Thay vào đó, nó sử dụng khả năng học trong ngữ cảnh của các mô hình ngôn ngữ lớn để tạo ra các chương trình mô-đun giống python, sau đó được thực thi để có được cả giải pháp và một lý luận toàn diện và có thể diễn giải. Mỗi dòng của chương trình được tạo ra có thể gọi một trong số các mô hình thị giác máy tính có sẵn, các chương trình con xử lý hình ảnh, hoặc các hàm python để tạo ra các đầu ra trung gian có thể được tiêu thụ bởi các phần tiếp theo của chương trình. Chúng tôi chứng minh tính linh hoạt của VISPROG trên 4 tác vụ đa dạng - trả lời câu hỏi trực quan tổng hợp, suy luận zero-shot trên các cặp hình ảnh, gắn thẻ đối tượng kiến thức thực tế, và chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Chúng tôi tin rằng các phương pháp neuro-symbolic như VISPROG là một hướng thú vị để dễ dàng và hiệu quả mở rộng phạm vi của các hệ thống AI để phục vụ đuôi dài của các tác vụ phức tạp mà con người có thể muốn thực hiện.

--- TRANG 2 ---
1. Giới thiệu
Việc theo đuổi các hệ thống AI đa năng đã dẫn đến sự phát triển của các mô hình có thể huấn luyện end-to-end có khả năng [1, 5, 8, 13, 19, 25, 27], nhiều mô hình trong số này khao khát cung cấp một giao diện ngôn ngữ tự nhiên đơn giản để người dùng tương tác với mô hình. Phương pháp chủ đạo để xây dựng các hệ thống này là tiền huấn luyện không giám sát quy mô lớn tiếp theo là huấn luyện đa tác vụ có giám sát. Tuy nhiên, phương pháp này yêu cầu một tập dữ liệu được tuyển chọn tốt cho mỗi tác vụ khiến việc mở rộng quy mô trở nên thách thức cho đuôi dài vô hạn của các tác vụ phức tạp mà chúng ta cuối cùng muốn các hệ thống này thực hiện. Trong công trình này, chúng tôi khám phá việc sử dụng các mô hình ngôn ngữ lớn để giải quyết đuôi dài của các tác vụ phức tạp bằng cách phân tách các tác vụ này được mô tả bằng ngôn ngữ tự nhiên thành các bước đơn giản hơn có thể được xử lý bởi các mô hình end-to-end chuyên biệt hoặc các chương trình khác.

Hãy tưởng tượng hướng dẫn một hệ thống thị giác "Gắn thẻ 7 nhân vật chính trong chương trình TV Big Bang Theory trong hình ảnh này." Để thực hiện tác vụ này, hệ thống trước tiên cần hiểu ý định của hướng dẫn và sau đó thực hiện một chuỗi các bước - phát hiện khuôn mặt, truy xuất danh sách các nhân vật chính trong Big Bang Theory từ cơ sở kiến thức, phân loại khuôn mặt sử dụng danh sách các nhân vật, và gắn thẻ hình ảnh với khuôn mặt và tên của nhân vật được nhận dạng. Trong khi các hệ thống thị giác và ngôn ngữ khác nhau tồn tại để thực hiện từng bước này, việc thực thi tác vụ được mô tả bằng ngôn ngữ tự nhiên này vượt ra ngoài phạm vi của các hệ thống huấn luyện end-to-end.

Chúng tôi giới thiệu VISPROG nhận đầu vào là dữ liệu trực quan (một hình ảnh đơn lẻ hoặc một tập hợp hình ảnh) cùng với một hướng dẫn ngôn ngữ tự nhiên, tạo ra một chuỗi các bước, một chương trình trực quan nếu bạn muốn, và sau đó thực thi các bước này để tạo ra đầu ra mong muốn. Mỗi dòng trong một chương trình trực quan gọi một trong số nhiều mô-đun được hệ thống hiện tại hỗ trợ. Các mô-đun có thể là các mô hình thị giác máy tính có sẵn, mô hình ngôn ngữ, các chương trình con xử lý hình ảnh trong OpenCV [4], hoặc các toán tử số học và logic. Các mô-đun tiêu thụ đầu vào được tạo ra bằng cách thực thi các dòng mã trước đó và xuất ra các kết quả trung gian có thể được tiêu thụ ở phía sau. Trong ví dụ trên, chương trình trực quan được tạo ra bởi VISPROG gọi một bộ phát hiện khuôn mặt [18], GPT-3 [5] như một hệ thống truy xuất kiến thức, và CLIP [23] như một bộ phân loại hình ảnh từ vựng mở để tạo ra đầu ra mong muốn (xem Hình 1).

VISPROG cải thiện so với các phương pháp trước đây để tạo ra và thực thi các chương trình cho các ứng dụng thị giác. Đối với tác vụ trả lời câu hỏi trực quan (VQA), Neural Module Networks (NMN) [2,9,10,12] kết hợp một mạng có thể huấn luyện end-to-end đặc thù cho câu hỏi từ các mô-đun neural chuyên biệt, có thể vi phân. Các phương pháp này hoặc sử dụng các trình phân tích cú pháp ngữ nghĩa có sẵn dễ vỡ để tính toán một cách tất định bố cục của các mô-đun, hoặc học một bộ tạo bố cục thông qua giám sát đáp án yếu qua REINFORCE [33]. Ngược lại, VISPROG sử dụng một mô hình ngôn ngữ mạnh mẽ (GPT-3)

Hình 2. Các mô-đun hiện được hỗ trợ trong VISPROG. Các mô-đun màu đỏ sử dụng các mô hình neural (OWL-ViT [21], DSFD [18], MaskFormer [6], CLIP [23], ViLT [16], và Stable Diffusion [28]). Các mô-đun màu xanh sử dụng xử lý hình ảnh và các chương trình con python khác. Các mô-đun này được gọi trong các chương trình được tạo ra từ hướng dẫn ngôn ngữ tự nhiên. Việc thêm các mô-đun mới để mở rộng khả năng của VISPROG rất đơn giản (Code. 1).

và một số lượng nhỏ các ví dụ trong ngữ cảnh để tạo ra các chương trình phức tạp mà không yêu cầu bất kỳ huấn luyện nào¹. Các chương trình được tạo ra bởi VISPROG cũng sử dụng một mức độ trừu tượng cao hơn so với NMNs và gọi các mô hình có hiệu suất cao được huấn luyện và các chương trình con python không neural (Hình 2). Những ưu điểm này làm cho VISPROG trở thành một hệ thống neuro-symbolic dễ sử dụng, có hiệu suất cao và mô-đun.

VISPROG cũng có khả năng diễn giải cao. Thứ nhất, VISPROG tạo ra các chương trình dễ hiểu mà người dùng có thể xác minh tính đúng đắn logic. Thứ hai, bằng cách chia nhỏ dự đoán thành các bước đơn giản, VISPROG cho phép người dùng kiểm tra các đầu ra của các bước trung gian để chẩn đoán lỗi và nếu cần, can thiệp vào quá trình suy luận. Tổng thể, một chương trình được thực thi với các kết quả bước trung gian (ví dụ: văn bản, hộp giới hạn, mặt nạ phân đoạn, hình ảnh được tạo ra, v.v.) được liên kết với nhau để mô tả luồng thông tin phục vụ như một lý luận trực quan cho dự đoán.

Để chứng minh tính linh hoạt của nó, chúng tôi sử dụng VISPROG cho 4 tác vụ khác nhau chia sẻ một số kỹ năng chung (ví dụ: để phân tích hình ảnh) trong khi cũng yêu cầu một mức độ khả năng suy luận và thao tác trực quan chuyên biệt. Các tác vụ này là - (i) trả lời câu hỏi trực quan tổng hợp; (ii) suy luận ngôn ngữ tự nhiên trực quan zero-shot (NLVR) trên các cặp hình ảnh; (iii) gắn thẻ đối tượng kiến thức thực tế từ hướng dẫn ngôn ngữ tự nhiên; và (iv) chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Chúng tôi nhấn mạnh rằng không mô hình ngôn ngữ nào cũng như bất kỳ mô-đun nào được tinh chỉnh theo bất kỳ cách nào. Việc điều chỉnh VISPROG cho bất kỳ tác vụ nào đơn giản như cung cấp một vài ví dụ trong ngữ cảnh bao gồm hướng dẫn ngôn ngữ tự nhiên và các chương trình tương ứng. Trong khi dễ sử dụng, VISPROG cho thấy một sự cải thiện ấn tượng 2,7 điểm so với mô hình VQA cơ sở trên tác vụ VQA tổng hợp, độ chính xác zero-shot mạnh mẽ 62,4% trên NLVR mà không bao giờ huấn luyện trên các cặp hình ảnh, và các kết quả định tính và định lượng thú vị trên các tác vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh.

¹Chúng tôi sử dụng "huấn luyện" để chỉ học dựa trên gradient để phân biệt nó với học trong ngữ cảnh chỉ liên quan đến một lần truyền xuôi.

--- TRANG 3 ---
Các đóng góp chính của chúng tôi bao gồm - (i) VISPROG - một hệ thống sử dụng khả năng học trong ngữ cảnh của mô hình ngôn ngữ để tạo ra các chương trình trực quan từ hướng dẫn ngôn ngữ tự nhiên cho các tác vụ trực quan tổng hợp (Mục 3); (ii) chứng minh tính linh hoạt của VISPROG trên các tác vụ trực quan phức tạp như gắn thẻ đối tượng kiến thức thực tế và chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ (Mục 4.3 và 4.4) đã tránh hoặc thấy thành công hạn chế với một mô hình end-to-end duy nhất; và (iii) tạo ra lý luận trực quan cho các tác vụ này và cho thấy tính hữu ích của chúng cho phân tích lỗi và điều chỉnh hướng dẫn do người dùng thúc đẩy để cải thiện hiệu suất của VISPROG một cách đáng kể (Mục 5.3).

2. Công trình liên quan
Các phương pháp neuro-symbolic đã thấy động lực đổi mới nhờ khả năng hiểu biết, tạo sinh và học trong ngữ cảnh đáng kinh ngạc của các mô hình ngôn ngữ lớn (LLMs). Chúng tôi hiện thảo luận về các phương pháp tạo và thực thi chương trình trước đây cho các tác vụ trực quan, công trình gần đây trong việc sử dụng LLMs cho thị giác, và những tiến bộ trong các phương pháp suy luận cho các tác vụ ngôn ngữ.

Tạo và thực thi chương trình cho các tác vụ trực quan.
Neural module networks (NMN) [2] đi tiên phong trong các phương pháp mô-đun và tổng hợp cho tác vụ trả lời câu hỏi trực quan (VQA). NMNs kết hợp các mô-đun neural thành một mạng có thể vi phân end-to-end. Trong khi các nỗ lực đầu sử dụng các trình phân tích cú pháp có sẵn [2], các phương pháp gần đây [9,10,12] học mô hình tạo bố cục cùng với các mô-đun neural sử dụng REINFORCE [33] và giám sát đáp án yếu. Trong khi tương tự về tinh thần với NMNs, VISPROG có một số ưu điểm so với NMNs. Thứ nhất, VISPROG tạo ra các chương trình cấp cao gọi các mô hình neural có hiệu suất cao được huấn luyện và các hàm python khác ở các bước trung gian trái ngược với việc tạo ra các mạng neural end-to-end. Điều này làm cho việc kết hợp các mô-đun tượng trưng, không thể vi phân trở nên dễ dàng. Thứ hai, VISPROG tận dụng khả năng học trong ngữ cảnh của LLMs [5] để tạo ra các chương trình bằng cách nhắc LLM (GPT-3) với một hướng dẫn ngôn ngữ tự nhiên (hoặc một câu hỏi trực quan hoặc một phát biểu cần được xác minh) cùng với một vài ví dụ về các hướng dẫn tương tự và các chương trình tương ứng của chúng do đó loại bỏ nhu cầu huấn luyện các bộ tạo chương trình chuyên biệt cho mỗi tác vụ.

LLMs cho các tác vụ trực quan. LLMs và học trong ngữ cảnh đã được áp dụng cho các tác vụ trực quan. PICa [34] sử dụng LLMs cho một tác vụ VQA dựa trên kiến thức [20]. PICa biểu diễn thông tin trực quan trong hình ảnh dưới dạng văn bản thông qua chú thích, đối tượng và thuộc tính và đưa biểu diễn văn bản này vào GPT-3 cùng với câu hỏi và các ví dụ trong ngữ cảnh để trực tiếp tạo ra đáp án. Socratic models (SMs) [36], kết hợp các mô hình được tiền huấn luyện từ các phương thức khác nhau như ngôn ngữ (BERT [7], GPT-2 [24]), thị giác-ngôn ngữ (CLIP [23]), và âm thanh-ngôn ngữ (mSLAM [3]), để thực hiện một số tác vụ zero-shot, bao gồm tạo chú thích hình ảnh, truy xuất video-to-text, và lập kế hoạch robot. Tuy nhiên, trong SMs việc kết hợp được xác định trước và cố định cho mỗi tác vụ. Ngược lại, VISPROG xác định cách kết hợp các mô hình cho mỗi thể hiện bằng cách tạo ra các chương trình dựa trên hướng dẫn, câu hỏi, hoặc phát biểu. Chúng tôi chứng minh khả năng của VISPROG để xử lý các hướng dẫn phức tạp liên quan đến các khả năng đa dạng (20 mô-đun) và đầu vào đa dạng (văn bản, hình ảnh, và cặp hình ảnh), trung gian (văn bản, hình ảnh, hộp giới hạn, mặt nạ phân đoạn), và các phương thức đầu ra (văn bản và hình ảnh). Tương tự như VISPROG, ProgPrompt [29] là một công trình đồng thời chứng minh khả năng của LLMs để tạo ra các kế hoạch hành động robot được đặt trong tình huống giống python từ hướng dẫn ngôn ngữ tự nhiên. Trong khi các mô-đun ProgPrompt (như "tìm" hoặc "nắm") nhận chuỗi (thường là tên đối tượng) làm đầu vào, các chương trình VISPROG tổng quát hơn. Trong mỗi bước trong một chương trình VISPROG, một mô-đun có thể chấp nhận nhiều đối số bao gồm chuỗi, số, biểu thức số học và logic, hoặc các đối tượng python tùy ý (như các thể hiện list() hoặc dict() chứa hộp giới hạn hoặc mặt nạ phân đoạn) được tạo ra bởi các bước trước đó.

Suy luận thông qua Prompting trong NLP. Có một cơ sở văn học đang phát triển [14, 17] về việc sử dụng LLMs cho các tác vụ suy luận ngôn ngữ thông qua prompting. Chain-of-Thought (CoT) prompting [32], nơi một mô hình ngôn ngữ được nhắc với các ví dụ trong ngữ cảnh về đầu vào, lý luận chuỗi suy nghĩ (một loạt các bước suy luận trung gian), và đầu ra, đã cho thấy khả năng ấn tượng để giải quyết các vấn đề suy luận toán học. Trong khi CoT dựa vào khả năng của LLMs để cả tạo ra một đường suy luận và thực thi nó, các phương pháp tương tự như VISPROG đã được áp dụng cho các tác vụ ngôn ngữ, nơi một decomposer prompt [15] được sử dụng trước tiên để tạo ra một chuỗi các tiểu tác vụ sau đó được xử lý bởi các bộ xử lý tiểu tác vụ.

3. Lập trình trực quan
Trong vài năm qua, cộng đồng AI đã tạo ra các mô hình có hiệu suất cao, đặc thù cho tác vụ cho nhiều tác vụ thị giác và ngôn ngữ như phát hiện đối tượng, phân đoạn, VQA, tạo chú thích, và tạo văn bản thành hình ảnh. Trong khi mỗi mô hình này giải quyết một vấn đề được định nghĩa rõ ràng nhưng hẹp, các tác vụ chúng ta thường muốn giải quyết trong thế giới thực thường rộng hơn và được định nghĩa một cách lỏng lẻo.

Để giải quyết các tác vụ thực tế như vậy, người ta phải hoặc thu thập một tập dữ liệu đặc thù cho tác vụ mới, có thể tốn kém, hoặc tỉ mỉ soạn một chương trình gọi nhiều mô hình neural, các chương trình con xử lý hình ảnh (ví dụ: thay đổi kích thước hình ảnh, cắt, lọc, và chuyển đổi không gian màu), và tính toán khác (ví dụ: tra cứu cơ sở dữ liệu, hoặc các phép toán số học và logic). Việc tạo ra thủ công các chương trình này cho đuôi dài vô hạn của các tác vụ phức tạp mà chúng ta gặp phải

--- TRANG 4 ---
Hình 3. Tạo chương trình trong VISPROG.

hàng ngày không chỉ yêu cầu chuyên môn lập trình mà còn chậm, tốn nhiều công sức, và cuối cùng không đủ để bao phủ không gian của tất cả các tác vụ. Điều gì sẽ xảy ra nếu chúng ta có thể mô tả tác vụ bằng ngôn ngữ tự nhiên và có một hệ thống AI tạo ra và thực thi chương trình trực quan tương ứng mà không cần bất kỳ huấn luyện nào?

Các mô hình ngôn ngữ lớn cho lập trình trực quan. Các mô hình ngôn ngữ lớn như GPT-3 đã cho thấy khả năng đáng chú ý để tổng quát hóa cho các mẫu mới cho một tác vụ sau khi đã thấy một số ít các minh họa đầu vào và đầu ra trong ngữ cảnh. Ví dụ, việc nhắc GPT-3 với hai ví dụ dịch thuật Anh-Pháp và một cụm từ tiếng Anh mới
good morning -> bonjour
good day -> bonne journée
good evening ->
tạo ra bản dịch tiếng Pháp "bonsoir". Lưu ý rằng chúng tôi không phải tinh chỉnh GPT-3 để thực hiện tác vụ dịch thuật trên cụm từ thứ ba. VISPROG sử dụng khả năng học trong ngữ cảnh này của GPT-3 để xuất ra các chương trình trực quan cho hướng dẫn ngôn ngữ tự nhiên.

Tương tự như các cặp dịch thuật Anh và Pháp trong ví dụ trên, chúng tôi nhắc GPT-3 với các cặp hướng dẫn và chương trình cấp cao mong muốn. Hình 3 cho thấy một prompt như vậy cho một tác vụ chỉnh sửa hình ảnh. Các chương trình trong các ví dụ trong ngữ cảnh được viết thủ công và thường có thể được xây dựng mà không cần hình ảnh đi kèm. Mỗi dòng của một chương trình VISPROG, hoặc một bước chương trình, bao gồm tên của một mô-đun, tên đối số đầu vào của mô-đun và giá trị của chúng, và tên biến đầu ra. Các chương trình VISPROG thường sử dụng các biến đầu ra từ các bước trước đó làm đầu vào cho các bước tương lai. Chúng tôi sử dụng tên mô-đun mô tả (ví dụ: "Select", "ColorPop", "Replace"), tên đối số (ví dụ: "image", "object", "query"), và tên biến (ví dụ: "IMAGE", "OBJ") để cho phép GPT-3 hiểu loại đầu vào và đầu ra, và chức năng của mỗi mô-đun. Trong quá trình thực thi, các biến đầu ra có thể được sử dụng để lưu trữ các loại dữ liệu tùy ý. Ví dụ "OBJ" là danh sách các đối tượng trong hình ảnh, với mặt nạ, hộp giới hạn, và văn bản (ví dụ: tên danh mục) liên kết với mỗi đối tượng.

Các ví dụ trong ngữ cảnh này được đưa vào GPT-3 cùng với một hướng dẫn ngôn ngữ tự nhiên mới. Mà không quan sát hình ảnh hoặc nội dung của nó, VISPROG tạo ra một chương trình (dưới cùng của Hình 3) có thể được thực thi trên (các) hình ảnh đầu vào để thực hiện tác vụ được mô tả.

class VisProgModule():
def __init__(self):
# tải một mô hình đã huấn luyện; chuyển sang GPU
def html(self,inputs: List,output: Any):
# trả về một chuỗi html trực quan hóa bước I/O
def parse(self,step: str):
# phân tích bước và trả về danh sách các giá trị đầu vào
# và biến, và tên biến đầu ra
def execute(self,step: str,state: Dict):
inputs, input_var_names, output_var_name = \
self.parse(step)
# lấy giá trị của các biến đầu vào từ state
for var_name in input_var_names:
inputs.append(state[var_name])
# thực hiện tính toán sử dụng mô hình đã tải
output = some_computation(inputs)
# cập nhật state
state[output_var_name] = output
# tóm tắt trực quan của tính toán bước
step_html = self.html(inputs,output)
return output, step_html

Code 1. Triển khai một mô-đun VISPROG.

Các mô-đun. VISPROG hiện hỗ trợ 20 mô-đun (Hình 2) để kích hoạt các khả năng như hiểu hình ảnh, thao tác hình ảnh (bao gồm tạo sinh), truy xuất kiến thức, và thực hiện các phép toán số học và logic. Trong VISPROG, mỗi mô-đun được triển khai như một lớp Python (Code. 1) có các phương thức để: (i) phân tích dòng để trích xuất tên và giá trị đối số đầu vào, và tên biến đầu ra; (ii) thực thi tính toán cần thiết có thể liên quan đến các mô hình neural đã huấn luyện và cập nhật trạng thái chương trình với tên và giá trị biến đầu ra; và (iii) tóm tắt tính toán của bước một cách trực quan bằng html (được sử dụng sau này để tạo ra một lý luận trực quan). Việc thêm các mô-đun mới vào VISPROG chỉ đơn giản là triển khai và đăng ký một

--- TRANG 5 ---
Hình 4. Lý luận trực quan được tạo ra bởi VISPROG. Những lý luận này tóm tắt trực quan đầu vào và đầu ra của mỗi bước tính toán trong chương trình được tạo ra trong quá trình suy luận cho một tác vụ chỉnh sửa hình ảnh (trên) và tác vụ NLVR (dưới).

lớp mô-đun, trong khi việc thực thi các chương trình sử dụng mô-đun này được xử lý tự động bởi trình thông dịch VISPROG, được mô tả tiếp theo.

Thực thi chương trình. Việc thực thi chương trình được xử lý bởi một trình thông dịch. Trình thông dịch khởi tạo trạng thái chương trình (một từ điển ánh xạ tên biến với giá trị của chúng) với các đầu vào, và bước qua chương trình từng dòng một trong khi gọi mô-đun đúng với các đầu vào được chỉ định trong dòng đó. Sau khi thực thi mỗi bước, trạng thái chương trình được cập nhật với tên và giá trị của đầu ra của bước.

Lý luận trực quan. Ngoài việc thực hiện tính toán cần thiết, mỗi lớp mô-đun cũng triển khai một phương thức gọi là html() để tóm tắt trực quan các đầu vào và đầu ra của mô-đun trong một đoạn HTML. Trình thông dịch chỉ đơn giản ghép tóm tắt HTML của tất cả các bước chương trình thành một lý luận trực quan (Hình 4) có thể được sử dụng để phân tích tính đúng đắn logic của chương trình cũng như kiểm tra các đầu ra trung gian. Các lý luận trực quan cũng cho phép người dùng hiểu lý do thất bại và điều chỉnh hướng dẫn ngôn ngữ tự nhiên một cách tối thiểu để cải thiện hiệu suất. Xem Mục 5.3 để biết thêm chi tiết.

Hình 5. Chúng tôi đánh giá VISPROG trên một tập hợp các tác vụ đa dạng. Các tác vụ bao trùm nhiều đầu vào và đầu ra và tái sử dụng các mô-đun (Loc, FaceDet, VQA) bất cứ khi nào có thể.

4. Các tác vụ
VISPROG cung cấp một khung linh hoạt có thể được áp dụng cho một phạm vi đa dạng các tác vụ trực quan phức tạp. Chúng tôi đánh giá VISPROG trên 4 tác vụ yêu cầu các khả năng từ suy luận không gian, suy luận về nhiều hình ảnh, truy xuất kiến thức, và tạo sinh và thao tác hình ảnh. Hình 5 tóm tắt các đầu vào, đầu ra, và mô-đun được sử dụng cho các tác vụ này. Chúng tôi hiện mô tả các tác vụ này, cài đặt đánh giá của chúng, và lựa chọn các ví dụ trong ngữ cảnh.

4.1. Trả lời câu hỏi trực quan tổng hợp
VISPROG có tính tổng hợp theo thiết kế khiến nó phù hợp cho tác vụ trả lời câu hỏi trực quan tổng hợp, đa bước: GQA [11]. Các mô-đun cho tác vụ GQA bao gồm những mô-đun để định vị từ vựng mở, một mô-đun VQA, các hàm để cắt vùng hình ảnh cho trước tọa độ hộp giới hạn hoặc giới từ không gian (như above, left, v.v.), mô-đun để đếm hộp, và một mô-đun để đánh giá các biểu thức Python. Ví dụ, xem xét câu hỏi: "Chiếc xe tải nhỏ ở bên trái hay bên phải của những người đang đội mũ bảo hiểm?". VISPROG trước tiên định vị "những người đội mũ bảo hiểm", cắt vùng bên trái (hoặc bên phải) của những người này, kiểm tra xem có một "chiếc xe tải nhỏ" ở phía đó không, và trả về "left" nếu có và "right" nếu không. VISPROG sử dụng mô-đun trả lời câu hỏi dựa trên ViLT [16], nhưng thay vì chỉ đơn giản truyền câu hỏi phức tạp gốc cho ViLT, VISPROG gọi nó cho các tác vụ đơn giản hơn như xác định nội dung trong một vùng hình ảnh. Do đó, VISPROG kết quả của chúng tôi cho GQA không chỉ có thể diễn giải hơn ViLT mà còn chính xác hơn (Bảng 1). Thay vào đó, người ta có thể hoàn toàn loại bỏ nhu cầu cho một mô hình QA như ViLT và sử dụng các hệ thống khác như CLIP và bộ phát hiện đối tượng, nhưng chúng tôi để lại điều đó cho nghiên cứu tương lai.

Đánh giá. Để hạn chế số tiền chi cho việc tạo ra chương trình với GPT-3, chúng tôi tạo ra một tập con của GQA để đánh giá. Mỗi câu hỏi trong GQA được chú thích với một loại câu hỏi. Để đánh giá trên một tập đa dạng các loại câu hỏi (100 loại chi tiết), chúng tôi lấy mẫu ngẫu nhiên tối đa k mẫu trên mỗi loại câu hỏi từ các bộ balanced val (k = 5) và testdev (k = 20).

--- TRANG 6 ---
Hình 6. Kết quả định tính cho chỉnh sửa hình ảnh (trên) và các tác vụ gắn thẻ kiến thức (dưới).

Prompts. Chúng tôi chú thích thủ công 31 câu hỏi ngẫu nhiên từ bộ balanced train với các chương trình VISPROG mong muốn. Việc chú thích câu hỏi với chương trình rất dễ dàng và yêu cầu viết ra chuỗi suy luận cần thiết để trả lời câu hỏi cụ thể đó. Chúng tôi cung cấp một tập con nhỏ hơn các ví dụ trong ngữ cảnh cho GPT-3, được lấy mẫu ngẫu nhiên từ danh sách này để giảm chi phí trả lời mỗi câu hỏi GQA.

4.2. Suy luận Zero-Shot trên các cặp hình ảnh
Các mô hình VQA được huấn luyện để trả lời câu hỏi về một hình ảnh duy nhất. Trong thực tế, người ta có thể yêu cầu một hệ thống trả lời câu hỏi về một bộ sưu tập hình ảnh. Ví dụ, một người dùng có thể yêu cầu một hệ thống phân tích album ảnh du lịch của họ và trả lời câu hỏi: "Chúng ta đã đến thăm địa danh nào, vào ngày sau khi chúng ta nhìn thấy Tháp Eiffel?". Thay vì lắp ráp một tập dữ liệu đắt tiền và huấn luyện một mô hình đa hình ảnh, chúng tôi chứng minh khả năng của VISPROG để sử dụng một hệ thống VQA hình ảnh đơn lẻ để giải quyết một tác vụ liên quan đến nhiều hình ảnh mà không huấn luyện trên các ví dụ đa hình ảnh.

Chúng tôi trình bày khả năng này trên benchmark NLVR V2 [30], liên quan đến việc xác minh các phát biểu về các cặp hình ảnh. Thông thường, việc giải quyết thách thức NLVR V2 yêu cầu huấn luyện các kiến trúc tùy chỉnh nhận các cặp hình ảnh làm đầu vào trên bộ train của NLVR V2. Thay vào đó, VISPROG đạt được điều này bằng cách phân tách một phát biểu phức tạp thành các câu hỏi đơn giản hơn về các hình ảnh riêng lẻ và một biểu thức python liên quan đến các toán tử số học và logic và câu trả lời cho các câu hỏi cấp hình ảnh. Mô hình VQA ViLT-VQA được sử dụng để có được câu trả lời cấp hình ảnh, và biểu thức python được đánh giá để xác minh phát biểu.

Đánh giá. Chúng tôi tạo ra một tập xác thực nhỏ bằng cách lấy mẫu 250 mẫu ngẫu nhiên từ bộ dev của NLVR V2 để hướng dẫn lựa chọn prompt, và kiểm tra tổng quát hóa trên bộ test công khai đầy đủ của NLVR V2.

Prompts. Chúng tôi lấy mẫu và chú thích các chương trình VISPROG cho 16 phát biểu ngẫu nhiên trong bộ train của NLVR V2. Vì một số ví dụ này dư thừa (cấu trúc chương trình tương tự) chúng tôi cũng tạo ra một tập con được tuyển chọn gồm 12 ví dụ bằng cách loại bỏ 4 ví dụ dư thừa.

4.3. Gắn thẻ đối tượng kiến thức thực tế
Chúng ta thường muốn xác định con người và đối tượng trong hình ảnh có tên không rõ đối với chúng ta. Ví dụ, chúng ta có thể muốn xác định các nghệ sĩ, chính trị gia, nhân vật trong chương trình TV, cờ của các quốc gia, logo của các tập đoàn, ô tô phổ biến và nhà sản xuất của chúng, loài sinh vật, và như vậy. Việc giải quyết tác vụ này yêu cầu không chỉ định vị con người, khuôn mặt, và đối tượng mà còn tra cứu kiến thức thực tế trong một cơ sở kiến thức bên ngoài để xây dựng một tập hợp các danh mục cho phân loại, như tên của các nhân vật trong một chương trình TV. Chúng tôi gọi tác vụ này là Gắn thẻ đối tượng kiến thức thực tế hoặc Gắn thẻ kiến thức cho ngắn gọn.

Để giải quyết Gắn thẻ kiến thức, VISPROG sử dụng GPT-3 như một cơ sở kiến thức ngầm có thể được truy vấn với các prompt ngôn ngữ tự nhiên như "Liệt kê các nhân vật chính trong chương trình TV Big Bang Theory được phân tách bằng dấu phẩy." Danh sách danh mục được tạo ra này sau đó có thể được sử dụng bởi một mô-đun phân loại hình ảnh CLIP phân loại các vùng hình ảnh được tạo ra bởi các mô-đun định vị và phát hiện khuôn mặt. Bộ tạo chương trình của VISPROG tự động xác định xem sử dụng

--- TRANG 7 ---
một bộ phát hiện khuôn mặt hay một bộ định vị từ vựng mở tùy thuộc vào ngữ cảnh trong hướng dẫn ngôn ngữ tự nhiên. VISPROG cũng ước tính kích thước tối đa của danh sách danh mục được truy xuất. Ví dụ, "Gắn thẻ logo của 5 công ty ô tô hàng đầu của Đức" tạo ra một danh sách 5 danh mục, trong khi "Gắn thẻ logo của các công ty ô tô Đức" tạo ra một danh sách có độ dài tùy ý được xác định bởi GPT-3 với một ngưỡng cắt ở 20. Điều này cho phép người dùng dễ dàng kiểm soát nhiễu trong quá trình phân loại bằng cách điều chỉnh hướng dẫn của họ.

Đánh giá. Để đánh giá VISPROG trên tác vụ này, chúng tôi chú thích 100 hướng dẫn gắn thẻ trên 46 hình ảnh yêu cầu kiến thức bên ngoài để gắn thẻ 253 thể hiện đối tượng bao gồm các nhân vật nổi tiếng trên văn hóa đại chúng, chính trị, thể thao, và nghệ thuật, cũng như nhiều loại đối tượng (ví dụ: ô tô, cờ, trái cây, thiết bị, đồ nội thất, v.v.). Đối với mỗi hướng dẫn, chúng tôi đo cả hiệu suất định vị và gắn thẻ thông qua precision (phần của các hộp dự đoán đúng) và recall (phần của các đối tượng ground truth được dự đoán đúng). Các chỉ số gắn thẻ yêu cầu cả hộp giới hạn dự đoán và nhãn hoặc nhãn lớp liên kết phải đúng, trong khi định vị bỏ qua nhãn. Để xác định tính đúng đắn của định vị, chúng tôi sử dụng ngưỡng IoU là 0,5. Chúng tôi tóm tắt hiệu suất định vị và gắn thẻ bằng điểm F1 (trung bình điều hòa của precision và recall trung bình trên các hướng dẫn).

Prompts. Chúng tôi tạo ra 14 ví dụ trong ngữ cảnh cho tác vụ này. Lưu ý rằng các hướng dẫn cho các ví dụ này được tưởng tượng tức là không có hình ảnh nào được liên kết với các ví dụ này.

4.4. Chỉnh sửa hình ảnh với ngôn ngữ tự nhiên
Tạo văn bản thành hình ảnh đã có những tiến bộ ấn tượng trong vài năm qua với các mô hình như DALL-E [26], Parti [35], và Stable Diffusion [28]. Tuy nhiên, vẫn vượt ra ngoài khả năng của các mô hình này để xử lý các prompt như "Che giấu khuôn mặt của Daniel Craig bằng :p" (bỏ nhận dạng hoặc bảo vệ quyền riêng tư), hoặc "Tạo một color pop của Daniel Craig và làm mờ nền" (làm nổi bật đối tượng) mặc dù những điều này tương đối đơn giản để đạt được theo chương trình sử dụng kết hợp phát hiện khuôn mặt, phân đoạn và các mô-đun xử lý hình ảnh. Việc đạt được một chỉnh sửa tinh vi như "Thay thế Barack Obama bằng Barack Obama đeo kính râm" (thay thế đối tượng), trước tiên yêu cầu xác định đối tượng quan tâm, tạo ra một mặt nạ của đối tượng cần thay thế và sau đó gọi một mô hình inpainting hình ảnh (chúng tôi sử dụng Stable Diffusion) với hình ảnh gốc, mặt nạ chỉ định các pixel cần thay thế, và mô tả của các pixel mới để tạo ra tại vị trí đó. VISPROG, khi được trang bị các mô-đun cần thiết và các chương trình ví dụ, có thể xử lý các hướng dẫn rất phức tạp một cách dễ dàng.

Đánh giá. Để kiểm tra VISPROG trên các hướng dẫn chỉnh sửa hình ảnh cho bỏ nhận dạng, làm nổi bật đối tượng, và thay thế đối tượng, chúng tôi thu thập 107 hướng dẫn trên 65 hình ảnh. Chúng tôi chấm điểm thủ công các dự đoán về tính đúng đắn và báo cáo độ chính xác. Lưu ý rằng chúng tôi không phạt các artifact trực quan

Độ chính xác # ví dụ ngữ cảnh # ví dụ ngữ cảnh GQA NLVR v2
Hình 7. Hiệu suất cải thiện với số lượng ví dụ trong ngữ cảnh trên các bộ xác thực GQA và NLVR V2. Các thanh lỗi đại diện cho khoảng tin cậy 95% trên 5 lần chạy. Các dự đoán từ cùng các lần chạy được sử dụng cho bầu chọn đa số. (Mục 5.1)

cho tiểu tác vụ thay thế đối tượng sử dụng Stable Diffusion miễn là hình ảnh được tạo ra về mặt ngữ nghĩa đúng.

Prompts. Tương tự như gắn thẻ kiến thức, chúng tôi tạo ra 10 ví dụ trong ngữ cảnh cho tác vụ này không có hình ảnh liên kết.

5. Thí nghiệm và phân tích
Các thí nghiệm của chúng tôi đánh giá ảnh hưởng của số lượng prompt trên hiệu suất GQA và NLVR (Mục 5.1), tổng quát hóa của VISPROG trên bốn tác vụ so sánh các chiến lược prompting khác nhau (Mục 5.2), phân tích các nguồn lỗi cho mỗi tác vụ (Hình 8), và nghiên cứu tính hữu ích của lý luận trực quan để chẩn đoán lỗi và cải thiện hiệu suất của VISPROG thông qua điều chỉnh hướng dẫn (Mục 5.3).

5.1. Ảnh hưởng của kích thước prompt
Hình 7 cho thấy hiệu suất xác thực tăng dần với số lượng ví dụ trong ngữ cảnh được sử dụng trong các prompt cho cả GQA và NLVR. Mỗi lần chạy chọn ngẫu nhiên một tập con của các ví dụ trong ngữ cảnh được chú thích dựa trên một seed ngẫu nhiên. Chúng tôi cũng thấy rằng bầu chọn đa số trên các seed ngẫu nhiên dẫn đến hiệu suất tốt hơn một cách nhất quán so với hiệu suất trung bình trên các lần chạy. Điều này phù hợp với các phát hiện trong văn học suy luận Chain-of-Thought [32] cho các bài toán suy luận toán học [31]. Trên NLVR, hiệu suất của VISPROG bão hòa với ít prompt hơn so với GQA. Chúng tôi tin rằng điều này là do các chương trình NLVR V2 yêu cầu ít mô-đun hơn và do đó ít minh họa hơn để sử dụng các mô-đun đó so với GQA.

5.2. Tổng quát hóa
GQA. Trong Bảng 1 chúng tôi đánh giá các chiến lược prompting khác nhau trên bộ testdev GQA. Đối với kích thước prompt lớn nhất được đánh giá trên valset (24 ví dụ trong ngữ cảnh), chúng tôi so sánh chiến lược ngẫu nhiên bao gồm prompt tốt nhất của VISPROG được chọn trong số 5 lần chạy trên tập xác thực (mỗi lần chạy lấy mẫu ngẫu nhiên các ví dụ trong ngữ cảnh từ 31 ví dụ được chú thích) và chiến lược bầu chọn đa số lấy dự đoán đồng thuận tối đa cho mỗi câu hỏi

--- TRANG 8 ---
Phương pháp Chiến lược prompting Lần chạy Ví dụ ngữ cảnh trên mỗi lần chạy Độ chính xác
VILT-VQA - 1 - 47.8
VISPROG tuyển chọn 1 20 50.0
VISPROG ngẫu nhiên 1 24 48.2
VISPROG bầu chọn 5 24 50.5

Bảng 1. Kết quả GQA testdev. Chúng tôi báo cáo hiệu suất trên một tập con của bộ testdev GQA gốc như được mô tả trong Mục 4.1.

Phương pháp Chiến lược prompting Tinh chỉnh Lần chạy Ví dụ ngữ cảnh trên mỗi lần chạy Độ chính xác
VILT-NLVR - 3 1 - 76.3
VISPROG tuyển chọn 7 1 12 61.8
VISPROG ngẫu nhiên 7 1 16 61.3
VISPROG bầu chọn 7 5 16 62.4

Bảng 2. Kết quả NLVR V2 test. VISPROG thực hiện NLVR zero-shot tức là không huấn luyện bất kỳ mô-đun nào trên các cặp hình ảnh. ViLT-NLVR, một mô hình VILT được tinh chỉnh trên NLVR V2, phục vụ như một giới hạn trên.

Hướng dẫn Gắn thẻ Định vị
precision recall F1 precision recall F1
Gốc 69.0 59.1 63.7 87.2 74.9 80.6
Đã sửa đổi 77.6 73.9 75.7 87.4 82.5 84.9

Bảng 3. Kết quả gắn thẻ kiến thức. Bảng cho thấy hiệu suất trên các hướng dẫn gốc cũng như các hướng dẫn được sửa đổi được tạo ra sau khi kiểm tra lý luận trực quan để hiểu các nguồn lỗi đặc thù cho thể hiện.

Gốc Đã sửa đổi
Độ chính xác 59.8 66.4

Bảng 4. Kết quả chỉnh sửa hình ảnh. Chúng tôi đánh giá thủ công mỗi dự đoán về tính đúng đắn ngữ nghĩa.

trên 5 lần chạy. Trong khi các prompt "ngẫu nhiên" chỉ vượt trội ViLT-VQA một chút, bầu chọn dẫn đến một sự cải thiện đáng kể 2,7 điểm. Điều này là do bầu chọn trên nhiều lần chạy, mỗi lần với một tập hợp ví dụ trong ngữ cảnh khác nhau, hiệu quả tăng tổng số ví dụ trong ngữ cảnh được thấy cho mỗi dự đoán. Chúng tôi cũng đánh giá một prompt được tuyển chọn thủ công bao gồm 20 ví dụ - 16 từ 31 ví dụ được chú thích, và 4 ví dụ tưởng tượng bổ sung nhằm cung cấp phạm vi bao phủ tốt hơn cho các trường hợp thất bại được quan sát trong tập xác thực. Prompt tuyển chọn hoạt động tốt như chiến lược bầu chọn trong khi sử dụng ít tính toán hơn 5 lần, làm nổi bật triển vọng của kỹ thuật prompt.

NLVR. Bảng 2 cho thấy hiệu suất của VISPROG trên bộ test NLVR V2 và so sánh các chiến lược prompting ngẫu nhiên, bầu chọn, và tuyển chọn như được thực hiện với GQA. Trong khi VISPROG thực hiện tác vụ NLVR zero-shot mà không bao giờ huấn luyện trên các cặp hình ảnh, chúng tôi báo cáo ViLT-NLVR, một mô hình VILT được tinh chỉnh trên NLVR V2 như một giới hạn trên về hiệu suất. Trong khi kém giới hạn trên vài điểm, VISPROG cho thấy hiệu suất zero-shot mạnh mẽ chỉ sử dụng một mô hình VQA hình ảnh đơn lẻ để hiểu hình ảnh, và một LLM để suy luận. Lưu ý rằng, VISPROG sử dụng ViLT-VQA cho mô-đun VQA của nó được huấn luyện trên VQA V2 một tác vụ trả lời câu hỏi hình ảnh đơn lẻ, nhưng không phải NLVR V2.

Gắn thẻ kiến thức. Bảng 3 cho thấy hiệu suất định vị và gắn thẻ cho tác vụ Gắn thẻ kiến thức. Tất cả các hướng dẫn cho tác vụ này không chỉ yêu cầu định vị từ vựng mở mà còn truy vấn cơ sở kiến thức để lấy các danh mục để gắn thẻ các đối tượng được định vị. Điều này làm cho nó trở thành một tác vụ không thể thực hiện chỉ với bộ phát hiện đối tượng. Với các hướng dẫn gốc, VISPROG đạt được điểm F1 ấn tượng 63,7% cho gắn thẻ, liên quan đến cả định vị đúng và đặt tên đối tượng, và điểm F1 80,6% chỉ cho định vị. Lý luận trực quan trong VISPROG cho phép cải thiện hiệu suất thêm bằng cách sửa đổi hướng dẫn. Xem Hình 6 để biết các ví dụ định tính và Mục 5.3 để biết thêm chi tiết về điều chỉnh hướng dẫn.

Chỉnh sửa hình ảnh. Bảng 4 cho thấy hiệu suất trên tác vụ chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ. Hình 6 cho thấy phạm vi rộng của các thao tác có thể thực hiện với tập mô-đun hiện tại trong VISPROG bao gồm thao tác khuôn mặt, làm nổi bật một hoặc nhiều đối tượng trong hình ảnh thông qua các hiệu ứng phong cách như color popping và làm mờ nền, và thay đổi ngữ cảnh cảnh bằng cách thay thế các yếu tố chính trong cảnh (ví dụ: sa mạc).

5.3. Tính hữu ích của lý luận trực quan
Phân tích lỗi. Lý luận được tạo ra bởi VISPROG cho phép phân tích kỹ lưỡng các chế độ thất bại như được hiển thị trong

GQA NLVR
Gắn thẻ kiến thức Chỉnh sửa hình ảnh
Hình 8. Các nguồn lỗi trong VISPROG.

Hình 8. Đối với mỗi tác vụ, chúng tôi kiểm tra thủ công lý luận cho 100 mẫu để phân tích các nguồn lỗi. Phân tích như vậy cung cấp một con đường rõ ràng hướng tới việc cải thiện

--- TRANG 9 ---
Gốc: Gắn thẻ các nhà vô địch giải Triwizard Tournament Lý do thất bại: # List hạn chế độ dài đầu ra xuống 3 LIST0 = List(query='Triwizard Tournament Champions', max=3)
Lý do thành công: # List xuất ra tất cả 4 nhà vô địch LIST0 = List(query='Triwizard Tournament Champions', max=4) Đã sửa đổi: Gắn thẻ 4 nhà vô địch giải Triwizard Tournament

Gốc: Thay thế bàn cà phê bằng một bàn cà phê hiện đại mặt kính
Đã sửa đổi: Thay thế bàn cà phê (table-merged) bằng một bàn cà phê hiện đại mặt kính
Lý do thất bại: # Mô-đun selection chọn vùng sai (thảm) OBJ1 = Select(query='coffee table', category=None) Lý do thành công: # Category hạn chế không gian tìm kiếm OBJ1 = Select(query='coffee table', category='table-merged')

Gốc: Gắn thẻ CEO của IBM
Đã sửa đổi: Gắn thẻ CEO gần đây nhất của IBM Lý do thất bại: # Truy vấn kiến thức trả về một trong những CEO trước đây của IBM LIST0 = List(query='CEO of IBM',max=1) Lý do thành công: # Truy vấn kiến thức trả về CEO hiện tại của IBM LIST0 = List(query='most recent CEO of IBM',max=1)

Gốc: Gắn thẻ vật phẩm được sử dụng để pha cà phê
Lý do thất bại: # Mô-đun định vị thất bại trong việc phát hiện bất kỳ đối tượng nào OBJ0 = Loc(image=IMAGE, object='item')
Lý do thành công: # Mô-đun định vị phát hiện nhiều thiết bị sau đó được lọc bởi Select OBJ0 = Loc(image=IMAGE,object='kitchen appliance that makes coffee') Đã sửa đổi: Gắn thẻ thiết bị nhà bếp được sử dụng để pha cà phê

Hình 9. Điều chỉnh hướng dẫn sử dụng lý luận trực quan. Bằng cách tiết lộ lý do thất bại, VISPROG cho phép người dùng sửa đổi hướng dẫn gốc để cải thiện hiệu suất.

hiệu suất của VISPROG trên các tác vụ khác nhau. Ví dụ, vì các chương trình không đúng là nguồn lỗi hàng đầu trên GQA ảnh hưởng đến 16% mẫu, hiệu suất trên GQA có thể được cải thiện bằng cách cung cấp thêm ví dụ trong ngữ cảnh tương tự như các hướng dẫn mà VISPROG hiện thất bại. Hiệu suất cũng có thể được cải thiện bằng cách nâng cấp các mô hình được sử dụng để triển khai các mô-đun có lỗi cao lên những mô hình có hiệu suất tốt hơn. Ví dụ, thay thế mô hình ViLT-VQA bằng một mô hình VQA tốt hơn cho NLVR có thể cải thiện hiệu suất lên đến 24%. Tương tự, cải thiện các mô hình được sử dụng để triển khai các mô-đun "List" và "Select", các nguồn lỗi chính cho các tác vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh, có thể giảm đáng kể lỗi.

Điều chỉnh hướng dẫn. Để hữu ích, một lý luận trực quan cuối cùng phải cho phép người dùng cải thiện hiệu suất của hệ thống trên tác vụ của họ. Đối với các tác vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh, chúng tôi nghiên cứu xem lý luận trực quan có thể giúp người dùng sửa đổi hoặc điều chỉnh hướng dẫn để đạt được hiệu suất tốt hơn hay không. Hình 9 cho thấy các hướng dẫn đã sửa đổi: (i) dẫn đến một truy vấn tốt hơn cho mô-đun định vị (ví dụ: "thiết bị nhà bếp" thay vì "vật phẩm"); (ii) cung cấp một truy vấn nhiều thông tin hơn cho truy xuất kiến thức (ví dụ: "CEO gần đây nhất của IBM" thay vì "CEO của IBM"); (iii) cung cấp tên danh mục (ví dụ: "table-merged") cho mô-đun Select để hạn chế tìm kiếm đến các vùng phân đoạn thuộc danh mục được chỉ định; hoặc (iv) kiểm soát số lượng danh mục phân loại cho gắn thẻ kiến thức thông qua đối số max trong mô-đun List. Bảng 3 và 4 cho thấy điều chỉnh hướng dẫn dẫn đến những cải thiện đáng kể cho các tác vụ gắn thẻ kiến thức và chỉnh sửa hình ảnh.

6. Kết luận
VISPROG đề xuất lập trình trực quan như một cách đơn giản và hiệu quả để mang khả năng suy luận của LLMs áp dụng cho các tác vụ trực quan phức tạp. VISPROG chứng minh hiệu suất mạnh mẽ trong khi tạo ra lý luận trực quan có khả năng diễn giải cao. Việc điều tra các chiến lược prompting tốt hơn và khám phá những cách mới để kết hợp phản hồi của người dùng để cải thiện hiệu suất của các hệ thống neuro-symbolic như VISPROG là một hướng thú vị để xây dựng thế hệ tiếp theo của các hệ thống thị giác đa năng.

7. Lời cảm ơn
Chúng tôi cảm ơn Kanchan Aggarwal đã giúp đỡ với quá trình chú thích cho các tác vụ chỉnh sửa hình ảnh và gắn thẻ kiến thức. Chúng tôi cũng biết ơn hệ sinh thái Hugging Face tuyệt vời đã đơn giản hóa việc sử dụng các mô hình neural hiện đại để triển khai các mô-đun VISPROG.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, và Karen Simonyan.
Flamingo: một mô hình ngôn ngữ trực quan cho học few-shot.
ArXiv, abs/2204.14198, 2022. 2

--- TRANG 10 ---
[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, và Dan
Klein. Neural module networks. 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), trang
39–48, 2016. 2, 3
[3] Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin John-
son, Yong Cheng, Simran Khanuja, Jason Riesa, và Alexis
Conneau. mslam: Tiền huấn luyện đồng thời đa ngôn ngữ quy mô lớn cho giọng nói và văn bản. ArXiv, abs/2202.01374, 2022. 3
[4] Gary Bradski. Thư viện opencv. Dr. Dobb's Journal: Software Tools for the Professional Programmer, 25(11):120–
123, 2000. 2
[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J.
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, và Dario Amodei. Các mô hình ngôn ngữ là những người học few-shot. ArXiv, abs/2005.14165, 2020. 2, 3
[6] Bowen Cheng, Alexander G. Schwing, và Alexander Kir-
illov. Phân loại theo pixel không phải tất cả những gì bạn cần cho phân đoạn ngữ nghĩa. Trong NeurIPS, 2021. 2
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina
Toutanova. Bert: Tiền huấn luyện của transformers hai chiều sâu cho hiểu biết ngôn ngữ. ArXiv, abs/1810.04805,
2019. 3
[8] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, và
Derek Hoiem. Hướng tới các hệ thống thị giác đa năng.
ArXiv, abs/2104.00743, 2021. 2
[9] Ronghang Hu, Jacob Andreas, Trevor Darrell, và Kate
Saenko. Tính toán neural có thể giải thích thông qua stack neural module networks. Trong ECCV, 2018. 2, 3
[10] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor
Darrell, và Kate Saenko. Học suy luận: End-to-end module networks cho trả lời câu hỏi trực quan. 2017 IEEE
International Conference on Computer Vision (ICCV), trang
804–813, 2017. 2, 3
[11] Drew A. Hudson và Christopher D. Manning. Gqa: Một tập dữ liệu mới cho suy luận trực quan thế giới thực và trả lời câu hỏi tổng hợp. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), trang 6693–
6702, 2019. 5
[12] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, và Ross B.
Girshick. Suy luận và thực thi các chương trình cho suy luận trực quan. 2017 IEEE International Conference on Computer
Vision (ICCV), trang 3008–3017, 2017. 2, 3
[13] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric
Kolve, Derek Hoiem, và Aniruddha Kembhavi. Mở rộng khái niệm có giám sát web cho các mô hình thị giác đa năng. Trong ECCV, 2022. 2
[14] Tushar Khot, Kyle Richardson, Daniel Khashabi, và Ashish
Sabharwal. Học giải quyết các tác vụ phức tạp bằng cách nói chuyện với các agent. ArXiv, abs/2110.08542, 2021. 3

[15] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle
Richardson, Peter Clark, và Ashish Sabharwal. Decomposed prompting: Một phương pháp mô-đun để giải quyết các tác vụ phức tạp. ArXiv, abs/2210.02406, 2022. 3
[16] Wonjae Kim, Bokyung Son, và Ildoo Kim. Vilt: Vision-
and-language transformer không có convolution hoặc giám sát vùng. Trong ICML, 2021. 2, 5
[17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
Matsuo, và Yusuke Iwasawa. Các mô hình ngôn ngữ lớn là những người suy luận zero-shot. ArXiv, abs/2205.11916, 2022. 3
[18] Jian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun
Qian, Jian Yang, Chengjie Wang, Jilin Li, và Feiyue Huang.
Dsfd: Dual shot face detector. 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), trang
5055–5064, 2019. 2
[19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, và Aniruddha Kembhavi. Unified-io: Một mô hình thống nhất cho thị giác, ngôn ngữ, và các tác vụ đa phương thức. ArXiv,
abs/2206.08916, 2022. 2
[20] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, và
Roozbeh Mottaghi. Ok-vqa: Một benchmark trả lời câu hỏi trực quan yêu cầu kiến thức bên ngoài. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), trang 3190–3199, 2019. 3
[21] Matthias Minderer, Alexey A. Gritsenko, Austin Stone,
Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy,
Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani,
Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, và
Neil Houlsby. Phát hiện đối tượng từ vựng mở đơn giản với vision transformers. ArXiv, abs/2205.06230, 2022. 2
[22] Zoe Papakipos và Joanna Bitton. Augly: Tăng cường dữ liệu cho tính mạnh mẽ. ArXiv, abs/2201.06494, 2022. 12
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, và các cộng sự. Học các mô hình trực quan có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong International Conference on Machine Learning,
trang 8748–8763. PMLR, 2021. 2, 3, 12
[24] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
Amodei, và Ilya Sutskever. Các mô hình ngôn ngữ là những người học đa tác vụ không giám sát. 2019. 3
[25] Colin Raffel, Noam M. Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, và Peter J. Liu. Khám phá giới hạn của học chuyển giao với một transformer text-to-text thống nhất. ArXiv,
abs/1910.10683, 2020. 2
[26] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, và Ilya Sutskever.
Tạo sinh hình ảnh từ văn bản zero-shot. ArXiv, abs/2102.12092,
2021. 7
[27] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez
Colmenarejo, Alexander Novikov, Gabriel Barth-Maron,
Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-
genberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Ed-
wards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Had-
sell, Oriol Vinyals, Mahyar Bordbar, và Nando de Freitas.
Một agent tổng quát. ArXiv, abs/2205.06175, 2022. 2

--- TRANG 11 ---
[28] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick
Esser, và Björn Ommer. Tổng hợp hình ảnh độ phân giải cao với các mô hình khuếch tán tiềm ẩn. 2022 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), trang
10674–10685, 2022. 2, 7
[29] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thoma-
son, và Animesh Garg. Progprompt: Tạo ra các kế hoạch tác vụ robot được đặt trong tình huống sử dụng các mô hình ngôn ngữ lớn. ArXiv,
abs/2209.11302, 2022. 3
[30] Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai, và
Yoav Artzi. Một corpus để suy luận về ngôn ngữ tự nhiên dựa trên ảnh. ArXiv, abs/1811.00491, 2019. 6
[31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, và Denny Zhou. Tự nhất quán cải thiện suy luận chuỗi suy nghĩ trong các mô hình ngôn ngữ. ArXiv,
abs/2203.11171, 2022. 7
[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, và Denny Zhou. Chain of thought
prompting gợi ra suy luận trong các mô hình ngôn ngữ lớn. ArXiv,
abs/2201.11903, 2022. 3, 7
[33] Ronald J Williams. Các thuật toán gradient-following thống kê đơn giản cho học tăng cường connectionist. Machine
learning, 8(3):229–256, 1992. 2, 3
[34] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-
mao Lu, Zicheng Liu, và Lijuan Wang. Một nghiên cứu thực nghiệm về gpt-3 cho vqa dựa trên kiến thức few-shot. Trong AAAI, 2022.
3
[35] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei
Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge,
và Yonghui Wu. Mở rộng quy mô các mô hình tự hồi qui cho tạo sinh văn bản thành hình ảnh phong phú nội dung. ArXiv, abs/2206.10789, 2022.
7
[36] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny
Lee, Vincent Vanhoucke, và Pete Florence. Socratic models: Kết hợp suy luận đa phương thức zero-shot với ngôn ngữ. arXiv, 2022. 3

--- TRANG 12 ---
A. Phụ lục
Phụ lục này bao gồm
• Prompts tác vụ cho VISPROG (Mục A.1)
• Chi tiết triển khai mô-đun (Mục A.2)
• Nhiều kết quả định tính hơn với lý luận trực quan cho cả trường hợp thành công và thất bại có thể được tìm thấy tại https://prior.allenai.org/projects/visprog.

A.1. Prompts tác vụ
Chúng tôi hiển thị cấu trúc prompt cho GQA (Hình 10), NLVR (Hình 11), gắn thẻ kiến thức (Hình 13), và các tác vụ chỉnh sửa hình ảnh được hướng dẫn bằng ngôn ngữ (Hình 12) với mỗi tác vụ 3 ví dụ trong ngữ cảnh.

Hình 10. Prompt GQA

A.2. Chi tiết mô-đun
Để giúp hiểu các chương trình được tạo ra tốt hơn, chúng tôi hiện cung cấp một vài chi tiết triển khai về một số mô-đun.

Select. Mô-đun nhận một đối số query và category. Khi category được cung cấp, việc lựa chọn chỉ được thực hiện trên các vùng đã được xác định là thuộc danh mục đó bởi một mô-đun trước đó trong chương trình (thường là mô-đun Seg). Nếu category là None, việc lựa chọn được thực hiện trên tất cả các vùng. Query là văn bản được sử dụng để chấm điểm vùng-văn bản để thực hiện lựa chọn. Chúng tôi sử dụng CLIP-ViT [23] để chọn vùng có điểm số tối đa cho query. Khi query chứa nhiều cụm từ được phân tách bằng dấu phẩy, vùng có điểm số cao nhất được chọn cho mỗi cụm từ.

Hình 11. Prompt NLVR

Hình 12. Prompt chỉnh sửa hình ảnh. Lưu ý rằng prompt bao gồm một ánh xạ của emoji với tên của chúng trong thư viện AugLy [22] được sử dụng để triển khai mô-đun Emoji. Ví dụ thứ ba cho thấy cách cung cấp giá trị category cho mô-đun Select.

Classify. Mô-đun Classify nhận danh sách các vùng đối tượng và danh mục và cố gắng gán một trong các danh mục cho mỗi vùng. Để đơn giản, chúng tôi giả định các hình ảnh trong tác vụ gắn thẻ có nhiều nhất 1 thể hiện của mỗi danh mục. Mô-đun Classify hoạt động khác nhau dựa trên việc danh sách danh mục có 1 hay nhiều phần tử. Nếu danh sách danh mục chỉ có 1 phần tử, danh mục được gán cho vùng có điểm CLIP cao nhất, tương tự như mô-đun Select. Khi có nhiều hơn một danh mục được cung cấp, trước tiên, mỗi vùng được gán danh mục có điểm số tốt nhất. Do lỗi phân loại, điều này có thể dẫn đến nhiều vùng được gán cùng một danh mục. Do đó, đối với mỗi danh mục được gán (trừ những danh mục không được gán cho bất kỳ vùng nào), chúng tôi thực hiện một bước khử trùng lặp chỉ giữ lại vùng có điểm số tối đa cho mỗi danh mục.

List. Mô-đun List sử dụng GPT3 để tạo ra một bộ truy xuất kiến thức linh hoạt và mạnh mẽ. Hình 14 cho thấy prompt được cung cấp cho GPT3 để truy xuất kiến thức thực tế.

Tạo danh sách được phân tách bằng dấu phẩy dựa trên truy vấn. Query: Liệt kê nhiều nhất 3 màu chính được phân tách bằng dấu phẩy List: đỏ, xanh lam, xanh lục Query: Liệt kê nhiều nhất 2 bang Bắc Mỹ được phân tách bằng dấu phẩy List: California, Washington Query: Liệt kê nhiều nhất {list_max} {new_query} được phân tách bằng dấu phẩy List:

Hình 14. Prompt cho mô-đun List. list_max biểu thị độ dài danh sách tối đa mặc định và new_query là chỗ dành cho truy vấn truy xuất mới

--- TRANG 13 ---
Hình 13. Prompt gắn thẻ kiến thức. Lưu ý rằng prompt có một chỗ dành bổ sung để cấu hình giá trị max mặc định cho mô-đun List. Trong khi ví dụ đầu tiên suy luận max từ một hướng dẫn tự nhiên, ví dụ thứ ba chứng minh cách người dùng có thể tăng cường tối thiểu một hướng dẫn tự nhiên để cung cấp giá trị đối số.
