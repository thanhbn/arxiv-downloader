# 2205.06126.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2205.06126.pdf
# Kích thước tệp: 3010848 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Một Mô Hình, Nhiều Phương Thức: Một Cách Tiếp Cận
Kích Hoạt Thưa Thớt cho Văn Bản, Âm Thanh, Hình Ảnh, Video
và Mã

Yong Dai, Duyu Tang, Liangxin Liu, Minghuan Tan, Cong Zhou, Jingquan Wang,
Zhangyin Feng, Fan Zhang, Xueyu Hu, Shuming Shi
Tencent AI Lab

Tóm tắt
Con người cảm nhận thế giới bằng nhiều giác quan (ví dụ: thông qua việc nghe âm thanh,
đọc từ ngữ và nhìn thấy vật thể). Tuy nhiên, hầu hết các hệ thống AI hiện có chỉ xử lý
một phương thức riêng lẻ. Bài báo này trình bày một cách tiếp cận xuất sắc trong việc xử lý
nhiều phương thức thông tin với một mô hình duy nhất. Trong mô hình "SkillNet" của chúng tôi,
các phần khác nhau của tham số được chuyên biệt hóa để xử lý các phương thức khác nhau.
Không giống như các mô hình dày đặc truyền thống luôn kích hoạt tất cả tham số của mô hình,
mô hình của chúng tôi kích hoạt thưa thớt các phần tham số có kỹ năng liên quan đến nhiệm vụ.
Thiết kế mô hình như vậy cho phép SkillNet học các kỹ năng theo cách dễ hiểu hơn. Chúng tôi
phát triển mô hình cho năm phương thức bao gồm văn bản, hình ảnh, âm thanh, video và mã.
Kết quả cho thấy, SkillNet hoạt động tương đương với năm mô hình tinh chỉnh chuyên biệt cho
từng phương thức. Hơn nữa, mô hình của chúng tôi hỗ trợ tiền huấn luyện tự giám sát với cùng
cách kích hoạt thưa thớt, dẫn đến các tham số khởi tạo tốt hơn cho các phương thức khác nhau.
Chúng tôi phát hiện rằng tiền huấn luyện cải thiện đáng kể hiệu suất của SkillNet trên năm
phương thức, ngang bằng hoặc thậm chí tốt hơn so với các baseline với tiền huấn luyện chuyên
biệt cho từng phương thức. Trên nhiệm vụ truy xuất hình ảnh từ văn bản tiếng Trung, hệ thống
cuối cùng của chúng tôi đạt độ chính xác cao hơn so với các hệ thống hàng đầu hiện có bao gồm
Wukong ViT-B và Wenlan 2.0 trong khi sử dụng ít tham số được kích hoạt hơn.

1 Giới thiệu

Trong những năm gần đây, Transformer [40] và các mô hình tiền huấn luyện dựa trên Transformer [12,35] đã
cách mạng hóa xử lý ngôn ngữ tự nhiên [33] và đã có sự quan tâm ngày càng tăng trong việc mở rộng
paradigm thành công này sang các lĩnh vực trí tuệ nhân tạo rộng lớn hơn bao gồm thị giác máy tính [8,23,32],
xử lý giọng nói [4] và phân tích chương trình [18]. Các nhà nghiên cứu từ các cộng đồng khác nhau không
có rào cản giao tiếp và thường lặp lại cùng một quy trình: tiền huấn luyện cho từng phương thức và
tinh chỉnh tất cả tham số mô hình cho từng nhiệm vụ.

Mặc dù đã có những tiến bộ đáng kể trong trí tuệ nhân tạo, các phương pháp hiện có khác biệt so với
việc học của con người trong ba khía cạnh sau [11]. Thứ nhất, chúng ta con người cảm nhận thế giới bằng
nhiều giác quan. Chúng ta biết rằng từ "chó", tiếng sủa của chó và hình ảnh/video của chó đều đề cập đến
cùng một khái niệm. Tuy nhiên, hầu hết các phương pháp hiện có chỉ xử lý một phương thức thông tin.
Thứ hai, não bộ con người có khoảng 100 tỷ tế bào thần kinh, trong đó các phần khác nhau được chuyên
biệt hóa cho các kỹ năng khác nhau. Khi chúng ta hoàn thành một nhiệm vụ, chúng ta chỉ gọi một phần nhỏ
các tế bào thần kinh có liên quan đến nhiệm vụ đó. Tuy nhiên, hầu hết các phương pháp hiện có kích hoạt
tất cả tham số mô hình. Thứ ba, khi chúng ta giải quyết một vấn đề mới hoặc học một kỹ năng mới, chúng ta
không học từ con số không mà kết hợp các kỹ năng cũ để học những điều mới một cách nhanh chóng.
Tuy nhiên, các phương pháp hiện có thường học cho từng nhiệm vụ từ đầu (hoặc từ một mô hình tổng quát
hoặc nền tảng), dẫn đến hàng trăm mô hình cho hàng trăm nhiệm vụ.

Trong công trình này, chúng tôi đề xuất một cách tiếp cận đa nhiệm vụ đa phương thức gọi là SkillNet.
Chúng tôi sử dụng một mô hình duy nhất để xử lý nhiều nhiệm vụ đòi hỏi sự hiểu biết về các phương thức
thông tin khác nhau. Trong SkillNet, các phần khác nhau của tham số được chuyên biệt hóa cho các kỹ năng
khác nhau. Khi mô hình được áp dụng cho một nhiệm vụ xuôi dòng, không giống như các mô hình "dày đặc"
truyền thống luôn kích hoạt tất cả tham số mô hình, nó "thưa thớt" kích hoạt các phần tham số có kỹ năng
liên quan đến nhiệm vụ mục tiêu. Ví dụ, chúng ta có thể định nghĩa năm kỹ năng liên quan đến phương thức
{stext, simage, ssound, svideo, scode}, được chuyên biệt hóa để hiểu văn bản, hình ảnh, âm thanh, video và mã
tương ứng. Xem xét nhiệm vụ nhận dạng giọng nói tự động (ASR), chỉ liên quan đến kỹ năng hiểu thính giác
(tức là ssound). Khi SkillNet được áp dụng cho ASR, các tham số mô hình liên quan đến bốn kỹ năng khác
(tức là {stext, simage, svideo, scode}) được tắt. Tương tự, đối với truy xuất văn bản-hình ảnh, đó là tìm kiếm
hình ảnh có liên quan ngữ nghĩa với văn bản đã cho, chỉ stext và simage được kích hoạt. Hình 1 đưa ra các
minh họa cấp cao về các tình huống nói trên. Có nhiều cách khác nhau để triển khai SkillNet. Trong công
trình này, chúng tôi cung cấp một triển khai đơn giản dựa trên Transformer [40]. Thay vì tạo ra các vector
K=Q=V tổng quát cho mỗi token, chúng tôi kích hoạt các tham số chuyên biệt cho phương thức khác nhau
để tạo ra các vector K=Q=V chuyên biệt cho phương thức khác nhau trước khi thực hiện attention đa đầu.
Trực giác là chúng tôi mong đợi mô hình gọi các phần khác nhau khi cần thiết để xử lý các loại tín hiệu
khác nhau và kết hợp thông tin từ nhiều giác quan để hình thành sự hiểu biết của chúng ta về một khái niệm
(như ví dụ nói trên về khái niệm chó).

Chúng tôi tiến hành thí nghiệm trên các nhiệm vụ của năm phương thức, bao gồm phân loại văn bản,
nhận dạng giọng nói tự động, truy xuất văn bản-hình ảnh, truy xuất văn bản-video và truy xuất văn bản-mã.
Kết quả cho thấy, SkillNet hoạt động tương đương với năm mô hình chuyên biệt cho phương thức với chỉ
một tệp mô hình. Hơn nữa, sau khi được tiền huấn luyện, SkillNet hoạt động tốt hơn so với các hệ thống
có tiền huấn luyện chuyên biệt cho phương thức trên ba trong số năm phương thức. Trên nhiệm vụ truy xuất
hình ảnh từ văn bản tiếng Trung, SkillNet đạt được độ chính xác cao hơn so với các hệ thống hiện có
(ví dụ: Wukong ViT-B và Wenlan 2.0) trong khi sử dụng ít tham số được kích hoạt hơn. Công trình của
chúng tôi chứng minh tính khả thi của việc phát triển một mô hình tổng quát vừa chính xác vừa hiệu quả
để giải quyết nhiều nhiệm vụ của các phương thức khác nhau.

2 So sánh với Các Phương pháp Hiện có

Chúng tôi mô tả các kết nối và sự khác biệt của công trình này với các phương pháp đa phương thức,
đa nhiệm vụ và hỗn hợp các chuyên gia liên quan.

Đa phương thức Vì có lượng lớn các công trình đa phương thức, chúng tôi chỉ mô tả những công trình
có liên quan chặt chẽ. Omnivore [19] sử dụng một mô hình duy nhất để xử lý nhiều phương thức hình ảnh,
bao gồm dữ liệu 3D đơn góc nhìn, hình ảnh và video. VATT [1] học các biểu diễn đa phương thức trên
các tín hiệu thô cho video, âm thanh và văn bản. So với Omnivore và VATT, công trình của chúng tôi
nghiên cứu nhiều phương thức hơn và cách tiếp cận của chúng tôi là thưa thớt. Data2vec [5] là một mục tiêu
học tổng quát thao tác trên các biểu diễn tiềm ẩn thay vì các token chuyên biệt cho phương thức. Cùng một
mục tiêu học được sử dụng để học cho văn bản, giọng nói và thị giác. Tuy nhiên, họ không thực hiện huấn
luyện đa nhiệm vụ. Công trình của chúng tôi trực giao với Data2vec và thật thú vị khi kết hợp các ưu điểm
của Data2vec và SkillNet.

Đa nhiệm vụ Công trình này cũng liên quan đến các phương pháp học đa nhiệm vụ. Các hệ thống được
xây dựng dựa trên Transformer thường sử dụng bộ mã hóa đặc trưng chung cộng với các lớp dự đoán
chuyên biệt cho nhiệm vụ để hiểu các nhiệm vụ [29] và sử dụng các prompt ngôn ngữ tự nhiên để điều
khiển mô hình mã hóa-giải mã cho các nhiệm vụ sinh [37]. Hầu hết các phương pháp đa nhiệm vụ hiện có
đều dày đặc - tất cả tham số mô hình đều được kích hoạt. Một ngoại lệ là SkillNet-NLU và SkillNet-NLG
[28,46], các mô hình thưa thớt được giới thiệu gần đây thực hiện học đa nhiệm vụ trên văn bản. Công trình
này có thể được xem như một mở rộng sang tình huống đa phương thức.

Hỗn hợp Chuyên gia (MoE) Các phương pháp MoE dựa trên Transformer thường bao gồm nhiều mạng
thần kinh đồng nhất (gọi là chuyên gia), có thể được kích hoạt đầy đủ hoặc một phần được hướng dẫn bởi
một hàm cổng bổ sung [15,16,27,38]. Tuy nhiên, không rõ loại kiến thức nào được học trong mỗi chuyên
gia và tại sao một chuyên gia được kích hoạt. Từ quan điểm này, cách tiếp cận của chúng tôi có thể được
xem như một MoE đa phương thức thưa thớt. Không giống như các phương pháp MoE truyền thống, mỗi
chuyên gia trong mô hình của chúng tôi có một định nghĩa rõ ràng và việc kích hoạt mỗi chuyên gia có
một lý do rõ ràng (được đánh giá bởi các chuyên gia con người).

--- TRANG 3 ---
Ví dụ, chuyên gia tương ứng với stext chịu trách nhiệm hiểu tín hiệu văn bản và nó chỉ được kích hoạt
nếu tín hiệu đầu vào là văn bản.

3 Phương pháp

Phần này trình bày mô hình kích hoạt thưa thớt SkillNet của chúng tôi. Trước tiên chúng tôi đưa ra
nền tảng ngắn gọn về Transformer (§3.1). Sau đó, chúng tôi mô tả kiến trúc mô hình của SkillNet (§3.2).
Cuối cùng, chúng tôi mô tả cách tạo ra các embedding cho các phương thức khác nhau (§3.3).

3.1 Nền tảng về Transformer

Để làm cho bài báo của chúng tôi độc lập, chúng tôi mô tả ngắn gọn Transformer ở đây. Transformer [40]
là một kiến trúc mô hình được sử dụng phổ biến với nhiều lớp, mỗi lớp bao gồm một lớp attention đa đầu
theo sau là một mạng truyền thẳng (FFN). Cơ chế attention đa đầu nối các vector đầu ra của H đầu khác
nhau và sau đó chiếu tuyến tính chúng bằng WO:

Multi-Head(Q; K; V) = Concat(head1; :::; headH)WO; (1)

trong đó Q(Query), K(Key), V(Value) là các biểu diễn ẩn của lớp trước.

Trong mỗi đầu, Q, K và V được biến đổi với các ma trận chiếu trước khi được đưa vào hàm attention:

headi = Attention(QWQi; KWKi; VWVi); (2)

trong đó WQi, WKi và WVi là các tham số mô hình, và i biểu thị đầu thứ i. Hàm attention tính tích vô hướng
của query với tất cả các key, và sử dụng softmax để thu được trọng số trên các value:

Attention(Q; K; V) = softmax(QKT/√dk)V; (3)

trong đó dk là chiều của key. Cuối cùng, lớp FNN được áp dụng để thu được các biểu diễn cuối cùng.
Kết nối dư [22] và chuẩn hóa lớp [3] được áp dụng cho cả lớp attention đa đầu và lớp FFN. Vì Transformer
phổ biến, chúng tôi loại trừ các chi tiết và hướng độc giả đến bài báo gốc.

Chúng tôi sử dụng tìm kiếm hình ảnh qua mạng Siamese làm ví dụ chạy để cho thấy cách áp dụng
Transformer cho các nhiệm vụ xuôi dòng. Như được hiển thị trong Hình 2, hai Transformer được sử dụng
để mã hóa văn bản và hình ảnh tương ứng. Đối với mỗi bên, chúng tôi lấy vector của token đầu tiên ([CLS])
để đại diện cho đầu vào. Độ tương tự ngữ nghĩa giữa văn bản và hình ảnh được tính toán bằng tích vô hướng
hoặc cosine.

3.2 Kiến trúc của SkillNet

Chúng tôi xây dựng mô hình SkillNet bằng cách sử dụng Transformer [40] làm khung xương. Cụ thể,
chúng tôi sửa đổi attention đa đầu của mỗi lớp Transformer như sau. Thay vì tạo ra các vector K=Q=V
tổng quát cho mỗi token, chúng tôi kích hoạt các tham số chuyên biệt cho phương thức khác nhau để
tạo ra các vector K=Q=V chuyên biệt cho phương thức khác nhau trước khi thực hiện attention đa đầu.
Lấy Q làm ví dụ. Thay vì chỉ có một ma trận chiếu WQi cho tất cả các query, chúng tôi có năm ma trận
tham số chiếu {WQtexti, WQimagei, WQsoundi, WQvideoi, WQcodei}, trong đó mỗi mục đại diện cho
một kỹ năng hiểu thông tin của một phương thức cụ thể. Khi mô hình được áp dụng cho một nhiệm vụ,
chúng tôi chỉ kích hoạt các ma trận chiếu tương ứng của các kỹ năng liên quan. Các sửa đổi tương tự
được thực hiện cho key và value. Việc tính toán một đầu được sửa đổi như sau.

headskilli = Attention(QWQActivatedi; KWKActivatedi; VWVActivatedi) (4)

WQActivatedi = {
WQtexti nếu stext được kích hoạt
WQimagei nếu simage được kích hoạt
WQsoundi nếu ssound được kích hoạt
WQvideoi nếu svideo được kích hoạt
WQcodei nếu scode được kích hoạt
} (5)

Như được hiển thị trong Hình 4, chúng tôi chỉ cần một mô hình để xử lý nhiệm vụ truy xuất hình ảnh,
trong đó chúng tôi kích hoạt stext và simage cho bộ mã hóa văn bản và bộ mã hóa hình ảnh tương ứng.

3.3 Embedding

Chúng tôi mô tả cách tạo ra các embedding cho các phương thức khác nhau.

Văn bản Theo BERT [12], chúng tôi chia nhỏ một văn bản thành một chuỗi các token wordpiece [45] và
xây dựng embedding của mỗi wordpiece bằng cách cộng embedding token, embedding vị trí và embedding
đoạn của nó. Chúng tôi cũng thêm một token phân loại đặc biệt [CLStext] ở đầu chuỗi để tạo ra biểu diễn
của chuỗi. Nếu đầu vào bao gồm hai đoạn, chúng tôi thêm một token đặc biệt [SEP] giữa hai đoạn.

Âm thanh Cho một dạng sóng thô làm đầu vào, chúng tôi theo wav2vec [4] và sử dụng mạng tích chập
để tạo ra một chuỗi vector như các embedding. Cụ thể, chúng tôi sử dụng bảy phép tích chập với 512 kênh,
stride của (5,2,2,2,2,2,2) và độ rộng kernel của (10,3,3,3,3,2,2) để tạo ra một chuỗi vector từ tần số
khung hình 20ms được lấy mẫu ở 16KHz. Sau đó, chúng tôi áp dụng một mạng tích chập 1D để biến đổi
chuỗi vector thành embedding 768 chiều, được cộng với các embedding vị trí tương ứng của chúng làm
embedding âm thanh cuối cùng.

Hình ảnh Theo Vision Transformer (ViT) [13], chúng tôi xây dựng các embedding patch cho mỗi hình ảnh.
Trước tiên chúng tôi định hình lại mỗi hình ảnh x ∈ RH×W×C thành các patch 2D xp ∈ RN×(P²×C),
trong đó (H,W) là độ phân giải hình ảnh, (P,P) là độ phân giải của mỗi patch, N là số lượng patch và C
là số kênh hình ảnh (ví dụ: 3 cho RGB). Sau đó, một mạng tích chập 2D được áp dụng để biến đổi pixel
patch thành embedding 768 chiều, được cộng với các embedding vị trí tương ứng làm embedding patch
cuối cùng. Chúng tôi thêm một token đặc biệt [CLSimage] ở đầu mỗi chuỗi để tạo ra biểu diễn của hình ảnh.

Video Chúng tôi theo Vivit [2], một mở rộng của ViT cho video, để tạo ra embedding video. Cho một
video V ∈ RT×H×W×C, trong đó T là số khung hình được lấy mẫu, chúng tôi trích xuất [T/t]×[H/h]×[W/w]
"tube" không giao nhau theo thời gian-không gian và sử dụng tích chập 3D để tạo ra một biểu diễn cho
mỗi tube. Chúng tôi thêm [T/t] + [H/h] + [W/w] embedding vị trí và nối một token đặc biệt [CLSvideo]
ở đầu mỗi chuỗi để đại diện cho toàn bộ đầu vào video.

Mã Chúng tôi theo CodeBERT [18] để tạo ra embedding mã. Chúng tôi chia nhỏ một đoạn mã thành một
chuỗi các token wordpiece chuyên biệt cho mã. Embedding cuối cùng của mỗi token là tổng của embedding
token, embedding vị trí và embedding đoạn. Một token đặc biệt [CLScode] được thêm vào đầu mỗi chuỗi
để tạo ra embedding của toàn bộ mã.

4 Nhiệm vụ

Trong phần này, trước tiên chúng tôi mô tả các nhiệm vụ xuôi dòng liên quan đến năm phương thức
trong §4.1. Mỗi phương thức liên quan đến một lĩnh vực nghiên cứu tích cực bao gồm nhiều nhiệm vụ.
Chúng tôi chọn một nhiệm vụ cho mỗi phương thức với sự ưu tiên cho các nhiệm vụ được công nhận rộng
rãi (ví dụ: ASR) và các nhiệm vụ liên quan đến nhiều phương thức (ví dụ: truy xuất video/mã). Vì khung
làm việc của chúng tôi cũng hỗ trợ tiền huấn luyện kích hoạt thưa thớt, chúng tôi tiến hành tiền huấn
luyện đa phương thức để khởi tạo các tham số mô hình. Các nhiệm vụ tiền huấn luyện được mô tả trong §4.2.

4.1 Nhiệm vụ Xuôi dòng

Văn bản Phân loại văn bản là một nhiệm vụ hiểu văn bản cổ điển và cơ bản [34]. Cho một câu làm đầu
vào, nhiệm vụ là dự đoán câu đó thuộc về loại nào. Theo BERT [12], chúng tôi thêm một token [CLStext]
ở đầu mỗi câu để đại diện cho ý nghĩa của toàn bộ câu. Đối với nhiệm vụ phân loại văn bản, chỉ các tham
số liên quan đến stext được kích hoạt.

Âm thanh Nhận dạng giọng nói tự động (ASR) là chuyển đổi giọng nói thành văn bản [24]. Theo wave2vec
[4], chúng tôi tạo ra các đặc trưng giọng nói và sinh ra một bản chuyển đổi bằng cách thực hiện phân loại
cấp token. Mất mát phân loại thời gian kết nối [20] được áp dụng cho việc huấn luyện mô hình. Đối với
nhiệm vụ ASR, chỉ các tham số liên quan đến ssound được kích hoạt.

Hình ảnh Chúng tôi xem xét truy xuất văn bản-hình ảnh. Cho một văn bản làm truy vấn, nhiệm vụ là
tìm hình ảnh mục tiêu từ một tập hợp các ứng cử viên. Xem xét hiệu quả của giai đoạn suy luận, chúng
tôi sử dụng hai lượt truyền riêng biệt (như Mạng Siamese) để tạo ra các vector văn bản và hình ảnh riêng
biệt mà không có attention xuyên phương thức. Đáng chú ý, chúng tôi sử dụng cùng một mô hình với các
cấu hình kích hoạt khác nhau (tức là stext được kích hoạt cho văn bản và simage được kích hoạt cho hình
ảnh) để tạo ra các vector văn bản và hình ảnh. Độ tương tự ngữ nghĩa giữa một văn bản và một hình ảnh
được tính toán bằng hàm tích vô hướng hoặc cosine.

Video Chúng tôi xem xét truy xuất văn bản-video. Cho một văn bản làm truy vấn, nhiệm vụ là tìm video
mục tiêu từ một tập hợp các ứng cử viên. Khung làm việc tương tự như truy xuất hình ảnh nói trên. Chúng
tôi sử dụng cùng một mô hình với các tham số được kích hoạt khác nhau (tức là stext được kích hoạt cho
văn bản và svideo được kích hoạt cho video) để tạo ra các vector văn bản và video riêng biệt.

Mã Chúng tôi xem xét truy xuất mã ngôn ngữ tự nhiên. Cho một văn bản làm truy vấn, nhiệm vụ là tìm
mã có liên quan nhất từ một tập hợp các ứng cử viên. Chúng tôi sử dụng cùng một mô hình với các tham
số được kích hoạt khác nhau (tức là stext cho văn bản và scode cho mã) để tạo ra các vector văn bản và
mã riêng biệt. Khung làm việc tương tự như truy xuất hình ảnh.

4.2 Nhiệm vụ Tiền huấn luyện

Nhắc lại rằng cách tiếp cận của chúng tôi cũng hỗ trợ tiền huấn luyện đa phương thức với kích hoạt
thưa thớt. Chúng tôi mô tả các nhiệm vụ tiền huấn luyện cho mỗi phương thức ở đây.

Văn bản Chúng tôi áp dụng mô hình hóa ngôn ngữ có mặt nạ (MLM) làm nhiệm vụ tiền huấn luyện [12,30].
Cho một văn bản, chúng tôi che ngẫu nhiên 15% token. Mỗi token bị che được thay thế bằng một token
đặc biệt [MASK] 80% thời gian, một token ngẫu nhiên 10% thời gian, và để nguyên cho 10% thời gian
còn lại.

Âm thanh Chúng tôi phát triển một phiên bản đơn giản hóa của HuBERT [25] và tiền huấn luyện thông
qua việc dự đoán các loại của các token âm thanh bị che, có nhãn mục tiêu được tạo ra bằng một quy
trình phân cụm ngoại tuyến. Chúng tôi đặt số lượng cụm là 100 và sử dụng phân cụm k-mean với các
đặc trưng âm học Mel-Frequency Cepstral Coefficients (MFCCs). Chúng tôi sử dụng cùng chiến lược
che của wav2vec2 [4], trong đó khoảng 5% các bước thời gian được lấy mẫu ngẫu nhiên làm chỉ số bắt
đầu và 10 bước thời gian tiếp theo bị che.

Hình ảnh Chúng tôi theo CLIP [36] và sử dụng các mục tiêu đối tỷ cho tiền huấn luyện. Chúng tôi sử
dụng cùng kiến trúc cho truy xuất hình ảnh như được minh họa trong §4.1 và áp dụng lấy mẫu âm trong
batch.

Video Tương tự như cấu hình tiền huấn luyện hình ảnh, chúng tôi xem xét một nhiệm vụ tiền huấn luyện
đối tỷ của khớp văn bản-video. Lấy mẫu âm trong batch được áp dụng.

Mã Như CodeBERT [18], chúng tôi nối mã và văn bản, tách chúng bằng [SEP] và che ngẫu nhiên 15%
token. Nhiệm vụ tiền huấn luyện là dự đoán các token bị che.

5 Thí nghiệm

5.1 Thiết lập

Chúng tôi so sánh với các baseline sau.

Các mô hình chuyên biệt cho phương thức. Chúng tôi huấn luyện năm mô hình khác nhau cho các phương
thức khác nhau. Kiến trúc mô hình cho mỗi phương thức là Transformer tiêu chuẩn.

Baseline đa phương thức dày đặc. Chúng tôi huấn luyện một mô hình đa phương thức học chung cho năm
phương thức. Đây là một mô hình dày đặc trong đó tất cả các phương thức này chia sẻ một kiến trúc
Transformer tiêu chuẩn chung, tương đương với SkillNet chỉ với một kỹ năng và kỹ năng đó luôn được
kích hoạt.

Baseline đa phương thức MoE. Chúng tôi huấn luyện một baseline Mixture-of-Expert (MoE) [27] và đặt
số lượng chuyên gia bằng số lượng kỹ năng của SkillNet (tức là 5). Có một hàm cổng để kích hoạt
chọn lọc top-2 chuyên gia cho mỗi token.

Chúng tôi triển khai SkillNet dựa trên Transformers của HuggingFace [42]. Chúng tôi tiến hành thí
nghiệm với 12 lớp mã hóa Transformer và 768 chiều trạng thái ẩn và để việc mở rộng sang quy mô mô
hình lớn hơn cho tương lai. Vì các tham số của SkillNet có thể được tiền huấn luyện (như được mô tả
trong §4.2), chúng tôi có hai cấu hình mô hình, tùy thuộc vào việc các tham số có được tiền huấn luyện
theo cùng cách kích hoạt thưa thớt hay không. Chúng tôi cũng so sánh với các baseline có tiền huấn
luyện chuyên biệt cho phương thức. Đối với văn bản, chúng tôi so sánh với [46], sử dụng tập con của
corpus tiền huấn luyện văn bản của chúng tôi để tiền huấn luyện BERT. Đối với hình ảnh, chúng tôi so
sánh với Wukong ViT-B [21], có quy mô mô hình tương tự (với 12 lớp Transformer) và được tiền huấn
luyện với tập con của dữ liệu tiền huấn luyện hình ảnh của chúng tôi. Đối với giọng nói, video và mã,
chúng tôi tiền huấn luyện các mô hình chuyên biệt cho phương thức với cùng lượng dữ liệu tiền huấn
luyện như SkillNet.

Chi tiết về các tập dữ liệu và quy trình huấn luyện được đưa ra trong Phụ lục.

5.2 Kết quả và Phân tích

Bảng 2 đưa ra kết quả trên năm nhiệm vụ. Các hệ thống trong nhóm đầu tiên không được tiền huấn
luyện. Chúng ta có thể thấy rằng SkillNet hoạt động tương đương với các mô hình chuyên biệt cho phương
thức. Một phát hiện thú vị là mô hình kết hợp với bộ mã hóa dày đặc không thân thiện với nhiệm vụ
ít tài nguyên như văn bản-video, nhưng hiện tượng này không tồn tại trong hệ thống MoE hoặc SkillNet.
Nhóm thứ hai bao gồm các hệ thống với tiền huấn luyện chuyên biệt cho phương thức hoặc tiền huấn
luyện kết hợp. Chúng ta có thể thấy rằng tiền huấn luyện cải thiện nhất quán hiệu suất của SkillNet trên
tất cả năm nhiệm vụ, thậm chí tốt hơn tiền huấn luyện chuyên biệt cho phương thức trên hình ảnh,
video và mã.

Trên nhiệm vụ truy xuất văn bản-hình ảnh, SkillNet đạt được độ chính xác tốt hơn so với các hệ thống
hàng đầu hiện có nhưng sử dụng ít tham số được kích hoạt hơn. Các số được đưa ra trong Bảng 3. Vì
các tham số của Wenlan 2.0 và Wukong ViT-B không được báo cáo trong các bài báo của họ, chúng tôi
tính toán tham số của họ dựa trên mô tả mô hình của họ. Các tham số của Wenlan 2.0 [17] bao gồm ba
phần, một bộ mã hóa hình ảnh bao gồm EfficientNet-B7 [39] (66M) và bốn lớp mã hóa Transformer (50M),
một bộ mã hóa văn bản RoBERTa-Large [10] (326M) và một lớp chiếu xuyên phương thức với hai lớp
được kết nối đầy đủ (3M). Wukong ViT-B [21] bao gồm một Vision Transformer (ViT) [14] (86M) làm
bộ mã hóa hình ảnh, một transformer chỉ giải mã tiêu chuẩn (110M) làm bộ mã hóa văn bản và một
lớp chiếu xuyên phương thức tuyến tính (0.6M).

Hình 5 hiển thị các đường cong học của SkillNet có hoặc không có tiền huấn luyện trên các nhiệm vụ
khác nhau. Chúng ta có thể thấy rằng nói chung tiền huấn luyện cho mô hình một điểm khởi đầu tốt
và dẫn đến độ chính xác tốt hơn.

Hình 6, 7 và 8 đưa ra các nghiên cứu trường hợp về truy xuất hình ảnh, video và mã tương ứng.

6 Kết luận

Bài báo này trình bày một cách tiếp cận đa nhiệm vụ đa phương thức kích hoạt thưa thớt gọi là SkillNet.
Chúng tôi chứng minh tính khả thi của việc sử dụng một mô hình để đạt được hiệu suất tương đương so
với nhiều mô hình chuyên biệt cho phương thức. Chúng tôi tiếp tục cho thấy rằng tiền huấn luyện thưa
thớt cho các tham số khởi tạo tốt hơn dẫn đến độ chính xác được cải thiện, thậm chí tốt hơn tiền huấn
luyện chuyên biệt cho phương thức trên ba trong số năm phương thức. Trên truy xuất văn bản-hình ảnh
tiếng Trung, hệ thống cuối cùng của chúng tôi mang lại độ chính xác tốt hơn với ít tham số được kích
hoạt hơn so với các hệ thống hàng đầu hiện có. Cách tiếp cận của chúng tôi không phụ thuộc vào phương
thức và không phụ thuộc vào nhiệm vụ. Chúng tôi để việc mở rộng sang số lượng lớn hơn các phương
thức và nhiệm vụ cho tương lai.
