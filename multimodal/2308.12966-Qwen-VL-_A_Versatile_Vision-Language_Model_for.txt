# 2308.12966.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2308.12966.pdf
# File size: 6329953 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Qwen-VL: A Versatile Vision-Language Model for
Understanding, Localization, Text Reading, and Beyond
Jinze Bai∗Shuai Bai∗Shusheng Yang∗Shijie Wang Sinan Tan
Peng Wang Junyang Lin Chang Zhou†Jingren Zhou
Alibaba Group
Code & Demo & Models: https://github.com/QwenLM/Qwen-VL
Abstract
Inthiswork,weintroducetheQwen-VLseries,asetoflarge-scalevision-languagemodels
(LVLMs) designed to perceive and understand both texts and images. Starting from the
Qwen-LM as a foundation, we endow it with visual capacity by the meticulously de-
signed(i) visual receptor ,(ii) input-output interface ,(iii) 3-stage training pipeline ,and
(iv) multilingual multimodal cleaned corpus . Beyond the conventional image descrip-
tion and question-answering, we implement the grounding and text-reading ability of
Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-
VL and Qwen-VL-Chat, set new records for generalist models under similar model scales
onabroadrangeofvisual-centricbenchmarks( e.g.,imagecaptioning,questionanswer-
ing, visual grounding) and different settings ( e.g., zero-shot, few-shot). Moreover, on
real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates
superiority compared to existing vision-language chatbots. All models are public to
facilitate future research.
Figure 1: Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other
generalist models.
∗Equal contribution,†Corresponding author
1arXiv:2308.12966v3  [cs.CV]  13 Oct 2023

--- PAGE 2 ---
Figure2: SomequalitativeexamplesgeneratedbyourQwen-VL-Chat. Qwen-VL-Chatsupportsmultiple
image inputs, multi-round dialogue, multilingual conversation, text-reading, localization, fine-grained
recognition and understanding ability.
1 Introduction
Recently, Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Anil et al., 2023; Gao et al.,
2023; Qwen, 2023) have attracted wide attention due to their powerful capabilities in text generation and
comprehension. These models can be further aligned with user intent through fine-tuning instructions,
showcasing strong interactive capabilities and the potential to enhance productivity as intelligent assistants.
However,nativelargelanguagemodelsonlyliveinthepure-textworld,lackingtheabilitytohandleother
common modalities (such as images, speech, and videos), resulting in great restrictions on their application
scope. Motivatedbythis,agroupofLargeVisionLanguageModels(LVLMs)(Alayracetal.,2022;Chen
et al., 2022; Li et al., 2023c; Dai et al., 2023; Huang et al., 2023; Peng et al., 2023; Zhu et al., 2023; Liu et al.,
2023;Yeetal.,2023b,a;Chenetal.,2023a;Lietal.,2023a;Zhangetal.,2023;Sunetal.,2023;OpenAI,2023)
have been developed toenhance large language modelswith theability to perceive andunderstand visual
signals. Theselarge-scalevision-languagemodelsdemonstratepromisingpotentialinsolvingreal-world
vision-central problems.
Nevertheless,despitethatlotsofworkshavebeenconductedtoexplorethelimitationandpotencyofLVLMs,
currentopen-sourceLVLMsalwayssufferfrominadequatetrainingandoptimization,thuslagfarbehind
the proprietary models (Chen et al., 2022, 2023b; OpenAI, 2023), which hinders further exploration and
application of LVLMs in open-source community. What’s more, as real-world visual scenarios are quite
complicated,fine-grainedvisualunderstandingplaysacrucialroleforLVLMstoassistpeopleeffectively
andprecisely. Butonlyafewattemptshadbeenmadetowardthisdirection(Pengetal.,2023;Chenetal.,
2023a), the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and
lacking the ability to execute fine-grained perception such as object grounding or text reading.
2

--- PAGE 3 ---
In this paper, we explore a way out and present the newest members of the open-sourced Qwen families:
Qwen-VL series. Qwen-VLs are a series of highly performant and versatile vision-language foundation
models based on Qwen-7B (Qwen, 2023) language model. We empower the LLM basement with visual
capacity by introducing a new visual receptor including a language-aligned visual encoder and a position-
aware adapter. The overall model architecture as well as the input-output interface are quite concise and
we elaboratedly design a 3-stage training pipeline to optimize the whole model upon a vast collection of
image-text corpus.
Our pre-trained checkpoint, termed Qwen-VL, is capable of perceiving and understanding visual inputs,
generating desired responses according to given prompts, and accomplishing various vision-language tasks
such as image captioning, question answering, text-oriented question answering, and visual grounding.
Qwen-VL-Chat is the instruction-tuned vision-language chatbot based on Qwen-VL. As shown in Fig. 2,
Qwen-VL-Chat is able to interact with users and perceive the input images following the intention of users.
Specifically, the features of the Qwen-VL series models include:
•Leadingperformance: Qwen-VLsachievetop-tieraccuracyonavastofvision-centricunderstanding
benchmarkscomparedtocounterpartswithsimilarscales. Besides,Qwen-VL’sstuningperformance
covers not only the conventional benchmarks e.g., captioning, question-answering, grounding), but
also some recently introduced dialogue benchmarks.
•Multi-lingual: SimilartoQwen-LM,Qwen-VLsaretraineduponmultilingualimage-textdatawitha
considerable amount of corpus being in English and Chinese. In this way, Qwen-VLs naturally support
English, Chinese, and multilingual instructions.
•Multi-image: Inthetrainingphase, weallowarbitraryinterleavedimage-textdataasQwen-VL’sinputs.
ThisfeatureallowsourQwen-Chat-VLtocompare,understand,andanalyzethecontextwhenmultiple
images are given.
•Fine-grained visual understanding: Thanks to the higher-resolution input size and fine-grained corpus
weusedintraining,Qwen-VLsexhibithighlycompetitivefine-grainedvisualunderstandingability.
Compared to existing vision-language generalists, our Qwen-VLs possess much better grounding,
text-reading, text-oriented question answering, and fine-grained dialog performance.
2 Methodology
2.1 Model Architecture
TheoverallnetworkarchitectureofQwen-VLconsistsofthreecomponentsandthedetailsofmodelparameters
are shown in Table 1:
Large Language Model : Qwen-VL adopts a large language model as its foundation component. The model
is initialized with pre-trained weights from Qwen-7B (Qwen, 2023).
Visual Encoder : ThevisualencoderofQwen-VLusestheVisionTransformer(ViT)(Dosovitskiyetal.,2021)
architecture, initialized with pre-trained weights from Openclip’s ViT-bigG (Ilharco et al., 2021). During
both training and inference, input imagesare resized to a specific resolution. Thevisual encoder processes
images by splitting them into patches with a stride of 14, generating a set of image features.
Position-aware Vision-Language Adapter : To alleviate the efficiency issues arising from long image feature
sequences, Qwen-VL introduces a vision-language adapter that compresses the image features. This adapter
comprises a single-layer cross-attention module initialized randomly. The module uses a group of trainable
vectors (Embeddings) as query vectors and the image features from the visual encoder as keys for cross-
attention operations. Thismechanism compresses thevisual feature sequence toa fixed lengthof 256. The
ablationaboutthenumberofqueriesisshowninAppendixE.2. Additionally,consideringthesignificance
3

--- PAGE 4 ---
of positional information for fine-grained image comprehension, 2D absolute positional encodings are
incorporated into the cross-attention mechanism’s query-key pairs to mitigate the potential loss of positional
details during compression. The compressed image feature sequence of length 256 is subsequently fed into
the large language model.
Table 1: Details of Qwen-VL model parameters.
Vision Encoder VL Adapter LLM Total
1.9B 0.08B 7.7B 9.6B
QwenLM
ViT 
Stage1: Pretraining
Image-Text Pairs
QwenLMStage2:Multi-task Pretraining
Multi-task and Interleaved VL Data
Stage3: Supervised Finetuning
ViT 
QwenLM
ViT 
Chat Interleaved VL Data
Low Resolution
High Resolution
High Resolution
CrossAttnLearnableQueryEmbs
CrossAttnLearnableQueryEmbs
CrossAttnLearnableQueryEmbs
Figure 3: The training pipeline of the Qwen-VL series.
2.2 Inputs and Outputs
Image Input : Images are processed through the visual encoder and adapter, yielding fixed-length sequences
ofimagefeatures. Todifferentiatebetweenimagefeatureinputandtextfeatureinput,twospecialtokens
(<img>and</img >)areappendedtothebeginningandendoftheimagefeaturesequencerespectively,
signifying the start and end of image content.
Bounding Box Input and Output : Toenhancethemodel’scapacityforfine-grainedvisualunderstandingand
grounding,Qwen-VL’straininginvolvesdataintheformofregiondescriptions,questions,anddetections.
Differingfromconventionaltasksinvolvingimage-textdescriptionsorquestions,thistasknecessitatesthe
model’s accurate understanding and generation of region descriptions in a designated format. For any
given bounding box, a normalization process is applied (within the range [0, 1000)) and transformed into a
specifiedstringformat: " (Xtopleft , Ytopleft ),(Xbottomright , Ybottomright )". Thestringistokenizedastextand
does not require an additional positionalvocabulary. To distinguish between detection stringsandregular
textstrings,twospecialtokens( <box>and</box>areaddedatthebeginningandendofthebounding
box string. Additionally, to appropriately associate bounding boxes with their corresponding descriptive
wordsorsentences,anothersetofspecialtokens( <ref>and</ref>)isintroduced,markingthecontent
referred to by the bounding box.
4

--- PAGE 5 ---
3 Training
As illustrated in Fig. 3, the training process of the Qwen-VL model consists of three stages: two stages of
pre-training and a final stage of instruction fine-tuning training.
3.1 Pre-training
Inthefirststageofpre-training,wemainlyutilizealarge-scale,weaklylabeled,web-crawledsetofimage-text
pairs. Ourpre-trainingdatasetiscomposedofseveralpubliclyaccessiblesourcesandsomein-housedata.
Wemadeanefforttocleanthedatasetofcertainpatterns. AssummarizedinTable2,theoriginaldataset
containsatotalof5billionimage-textpairs,andaftercleaning,1.4billiondataremain,with77.3%English
(text) data and 22.7% Chinese (text) data.
Table 2: Details of Qwen-VL pre-training data. LAION-en and LAION-zh are the English and Chinese
language subset of LAION-5B (Schuhmann et al., 2022a). LAION-COCO (Schuhmann et al., 2022b) is a
syntheticdatasetgeneratedfromLAION-en. DataComp(Gadreetal.,2023)andCoyo(Byeonetal.,2022)are
collectionsofimage-textpairs. CC12M(Changpinyoetal.,2021),CC3M(Sharmaetal.,2018),SBU(Ordonez
et al., 2011) and COCO Caption (Chen et al., 2015) are academic caption datasets.
Language Dataset Original Cleaned Remaining%
EnglishLAION-en 2B 280M 14%
LAION-COCO 600M 300M 50%
DataComp 1.4B 300M 21%
Coyo 700M 200M 28%
CC12M 12M 8M 66%
CC3M 3M 3M 100%
SBU 1M 0.8M 80%
COCO Caption 0.6M 0.6M 100%
ChineseLAION-zh 108M 105M 97%
In-house Data 220M 220M 100%
Total 5B 1.4B 28%
We freeze the large language model and only optimize the vision encoder and VL adapter in this stage.
The input images are resized to 224×224. The training objective is to minimize the cross-entropy of the
texttokens. Themaximumlearningrateis 2e−4andthetrainingprocessusesabatchsizeof30720forthe
image-text pairs, and the entire first stage of pre-training lasts for 50,000 steps, consuming approximately 1.5
billion image-text samples. More hyperparameters are detailed in Appendix C and the convergence curve of
this stage is shown in Figure 6.
3.2 Multi-task Pre-training
Inthesecondstageofmulti-taskpre-training,weintroducehigh-qualityandfine-grainedVLannotation
datawithalargerinputresolutionandinterleavedimage-textdata. AssummarizedinTable3,wetrained
Qwen-VLon7tasks simultaneously. Fortextgeneration, weusethe in-housecollectedcorpustomaintain
the LLM’s ability. Captioning data is the same with Table 2 except for far fewer samples and excluding
LAION-COCO. We use a mixture of publicly available data for the VQA task which includes GQA (Hudson
andManning,2019),VGQA(Krishnaetal.,2017),VQAv2(Goyaletal.,2017),DVQA(Kafleetal.,2018),OCR-
VQA (Mishra et al., 2019) and DocVQA (Mathew et al., 2021). We follow Kosmos-2 to use the GRIT (Peng
et al., 2023) dataset for the grounding task with minor modifications. For the reference grounding and
grounded captioning duality tasks, we construct training samples from GRIT (Peng et al., 2023), Visual
Genome(Krishnaetal.,2017),RefCOCO(Kazemzadehetal.,2014),RefCOCO+,andRefCOCOg(Maoetal.,
5

--- PAGE 6 ---
2016). In order to improve the text-oriented tasks, we collect pdf and HTML format data from Common
Crawl1and generate synthetic OCR data in English and Chinese language with natural scenery background,
following(Kimetal.,2022). Finally,wesimplyconstructinterleavedimage-textdatabypackingthesame
task data into sequences of length 2048.
Table 3: Details of Qwen-VL multi-task pre-training data.
Task # Samples Dataset
Captioning 19.7MLAION-en & zh, DataComp, Coyo, CC12M & 3M, SBU,
COCO, In-house Data
VQA 3.6MGQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA,
TextVQA, ChartQA, AI2D
Grounding23.5M GRIT
Ref Grounding 8.7M GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg
Grounded Cap. 8.7M GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg
OCR 24.8M SynthDoG-en & zh, Common Crawl pdf & HTML
Pure-text Autoregression 7.8M In-house Data
We increase the input resolution of the visual encoder from 224×224to448×448, reducing the information
loss caused by image down-sampling. Besides, we ablate the window attention and global attention for
higherresolutionsofthevisiontransformerinAppendixE.3. Weunlockedthelargelanguagemodeland
trained the whole model. The training objective is the same as the pre-training stage.
3.3 Supervised Fine-tuning
During this stage, we finetuned the Qwen-VL pre-trained model through instruction fine-tuning to enhance
itsinstructionfollowinganddialoguecapabilities,resultingintheinteractiveQwen-VL-Chatmodel. The
multi-modal instruction tuning data primarily comes from caption data or dialogue data generated through
LLM self-instruction, which often only addresses single-image dialogue and reasoning and is limited to
image content comprehension. We construct an additional set of dialogue data through manual annotation,
modelgeneration,andstrategyconcatenationtoincorporatelocalizationandmulti-imagecomprehension
abilities into the Qwen-VL model. We confirm that the model effectively transfers these capabilities to a
wider range of languages and question types. Additionally, we mix multi-modal and pure text dialogue
data during training to ensure the model’s universality in dialogue capabilities. The instruction tuning data
amounts to 350k. In this stage, we freeze the visual encoder and optimize the language model and adapter
module. We demonstrate the data format of this stage in Appendix B.2.
4 Evaluation
Inthissection,weconductanoverallevaluationonvariousmulti-modaltaskstocomprehensivelyassess
our models’ visual understanding ability. In the following, Qwen-VL denotes the model after the multi-task
training, and Qwen-VL-Chat denotes the model after supervised fine-tuning (SFT) stage.
Table 9 provides a detailed summary of the used evaluation benchmarks and corresponding metrics.
4.1 Image Caption and General Visual Question Answering
Image caption and general visual question answering (VQA) are two conventional tasks for vision-language
models. Specifically,imagecaptionrequiresthemodeltogenerateadescriptionforagivenimageandgeneral
VQA requires the model to generate an answer for a given image-question pair.
1https://digitalcorpora.org/corpora/file-corpora/cc-main-2021-31-pdf-untruncated
2This task is to generate noun/phrase grounded captions (Peng et al., 2023).
6

--- PAGE 7 ---
Table 4: Results on Image Captioning and General VQA.
Model Type ModelImage Caption General VQA
Nocaps
(0-shot)Flickr30K
(0-shot)VQAv2 OKVQA GQASciQA-Img
(0-shot)VizWiz
(0-shot)
Generalist
ModelsFlamingo-9B - 61.5 51.8 44.7 - - 28.8
Flamingo-80B - 67.2 56.3 50.6 - - 31.6
Unified-IO-XL 100.0 - 77.9 54.0 - - -
Kosmos-1 - 67.1 51.0 - - - 29.2
Kosmos-2 - 80.5 51.1 - - - -
BLIP-2 (Vicuna-13B) 103.9 71.6 65.0 45.9 32.3 61.0 19.6
InstructBLIP (Vicuna-13B) 121.9 82.8 - - 49.5 63.1 33.4
Shikra (Vicuna-13B) - 73.9 77.36 47.16 - - -
Qwen-VL (Qwen-7B) 121.4 85.8 79.5 58.6 59.3 67.1 35.2
Qwen-VL-Chat 120.2 81.0 78.2 56.6 57.5 68.2 38.9
Specialist
SOTAs-127.0
(PALI-17B)84.5
(InstructBLIP
-FlanT5-XL)86.1
(PALI-X
-55B)66.1
(PALI-X
-55B)72.1
(CFR)92.53
(LLaVa+
GPT-4)70.9
(PALI-X
-55B)
Fortheimagecaptiontask,wechooseNocaps(Agrawaletal.,2019)andFlickr30K(Youngetal.,2014)as
benchmarks and report CIDEr score (Vedantam et al., 2015) as metric. We utilize greedy search for caption
generation with a prompt of "Descripe the image in English:" .
For general VQA, we utilize five benchmarks including VQAv2 (Goyal et al., 2017), OKVQA (Marino et al.,
2019),GQA(HudsonandManning,2019),ScienceQA(ImageSet)(Luetal.,2022b)andVizWizVQA(Gurari
et al., 2018). For VQAv2, OKVQA, GQA and VizWiz VQA, we employ open-ended answer generation with
greedydecodingstrategyandapromptof "{question} Answer:" ,withoutanyconstrainonmodel’soutput
space. However,forScienceQA,weconstrainthemodel’soutputtopossibleoptions(insteadofopen-ended),
choose the option with highest confidence as model’s prediction, and report the Top- 1accuracy.
The overall performance on image caption and general VQA tasks are reported in Table 4. As the results
shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous
generalistmodelsintermsofbothtwotasks. Specifically,onzero-shotimagecaptiontask,Qwen-VLachieves
state-of-the-artperformance( i.e.,85.8CIDErscore)ontheFlickr30Kkarpathy-testsplit,evenoutperforms
previous generalist models with much more parameters ( e.g., Flamingo-80B with 80B parameters).
On general VQA benchmarks, our models also exhibit distinct advantages compared to others. On VQAv2,
OKVQAandGQAbenchmarks,Qwen-VLachieves79.5,58.6and59.3accuracyrespectively,whichsurpasses
recent proposed LVLMs by a large margin. It’s worth noting that Qwen-VL also shows strong zero-shot
performance on ScienceQA and VizWiz datasets.
4.2 Text-oriented Visual Question Answering
Text-oriented visual understanding has a broad application prospect in real-world scenarios. We assess our
models’abilitytowardtext-orientedvisualquestionansweringonseveralbenchmarksincludingTextVQA(Sidorov
etal.,2020),DocVQA(Mathewetal.,2021),ChartQA(Masryetal.,2022),AI2Diagram(Kembhavietal.,
2016),andOCR-VQA(Mishraetal.,2019). Similarly,theresultsareshowninTable5. Comparedtoprevious
generalist models and recent LVLMs, our models show better performance on most benchmarks, frequently
by a large margin.
4.3 Refer Expression Comprehension
Weshowourmodels’fine-grainedimageunderstandingandlocalizationabilitybyevaluatingonasortof
refer expression comprehension benchmarks such as RefCOCO (Kazemzadeh et al., 2014), RefCOCOg (Mao
etal.,2016),RefCOCO+(Maoetal.,2016)andGRIT(Guptaetal.,2022). Specifically,thereferexpression
comprehension task requires the model to localize the target object under the guidance of a description. The
7

--- PAGE 8 ---
Table 5: Results on Text-oriented VQA.
Model type Model TextVQA DocVQA ChartQA AI2D OCR-VQA
Generalist ModelsBLIP-2 (Vicuna-13B) 42.4 - - - -
InstructBLIP (Vicuna-13B) 50.7 - - - -
mPLUG-DocOwl (LLaMA-7B) 52.6 62.2 57.4 - -
Pix2Struct-Large (1.3B) - 76.6 58.6 42.1 71.3
Qwen-VL (Qwen-7B) 63.8 65.1 65.7 62.3 75.7
Qwen-VL-Chat 61.5 62.6 66.3 57.7 70.5
Specialist SOTAsPALI-X-55B (Single-task fine-
tuning, without OCR Pipeline)71.44 80.0 70.0 81.2 75.0
Table 6: Results on Referring Expression Comprehension task.
Model type ModelRefCOCO RefCOCO+ RefCOCOg GRIT
val test-A test-B val test-A test-B val test refexp
Generalist ModelsGPV-2 - - - - - - - - 51.50
OFA-L* 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.58 61.70
Unified-IO - - - - - - - - 78.61
VisionLLM-H 86.70 - - - - - - -
Shikra-7B 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 69.34
Shikra-13B 87.83 91.11 81.81 82.89 87.79 74.41 82.64 83.16 69.03
Qwen-VL-7B 89.3692.26 85.34 83.12 88.25 77.2185.58 85.48 78.22
Qwen-VL-7B-Chat 88.55 92.2784.51 82.82 88.5976.79 85.96 86.32 -
Specialist SOTAsG-DINO-L 90.56 93.19 88.24 82.75 88.95 75.92 86.13 87.02 -
UNINEXT-H 92.64 94.33 91.46 85.24 89.63 79.79 88.73 89.37 -
ONE-PEACE 92.58 94.18 89.26 88.77 92.21 83.23 89.22 89.27 -
results areshown in Table 6. Compared to previous generalistmodels or recentLVLMs,our models obtain
top-tier results on all benchmarks.
4.4 Few-shot Learning on Vision-Language Tasks
Our model also exhibits satisfactory in-context learning ( a.k.a., few-shot learning) ability. As shown in
Figure 4,Qwen-VLachieves betterperformance through in-contextfew-shot learning onOKVQA (Marino
et al., 2019), Vizwiz (Gurari et al., 2018), TextVQA (Sidorov et al., 2020), and Flickr30k (Young et al.,
2014)whencomparedwithmodelswithsimilarnumberofparameters(Flamingo-9B(Alayracetal.,2022),
OpenFlamingo-9B( ?) and IDEFICS-9B ?). Qwen-VL’s performance is even comparable with much larger
models(Flamingo-80BandIDEFICS-80B).Notethatweadoptnaïverandomsampletoconstructthefew-shot
exemplars, sophisticated few-shot exemplar construction methods such as RICES (Yang et al., 2022b) are not
used despite better results would be achieved.
Figure 4: Few-shot learning results of Qwen-VL in comparison with other models.
8

--- PAGE 9 ---
Table 7: Results on Instruction-following benchmarks.
ModelTouchStone SEED-Bench MME
En Cn All Img Video Perception Cognition
VisualGLM - 247.1 - - - 705.31 181.79
PandaGPT 488.5 - - - - 642.59 228.57
MiniGPT4 531.7 - 42.8 47.4 29.9 581.67 144.29
InstructBLIP 552.4 - 53.4 58.8 38.1 1212.82 291.79
LLaMA-AdapterV2 590.1 - 32.7 35.2 25.8 972.67 248.93
LLaVA 602.7 - 33.5 37.0 23.8 502.82 214.64
mPLUG-Owl 605.4 - 34.0 37.9 23.0 967.34 276.07
Qwen-VL - - 56.3 62.3 39.1 - -
Qwen-VL-Chat 645.2 401.2 58.2 65.4 37.8 1487.58 360.71
4.5 Instruction Following in Real-world User Behavior
Inadditiontopreviousconventionalvision-languageevaluations,toevaluateourQwen-VL-Chatmodel’s
capacityunderreal-worlduserbehavior,wefurtherconducttheevaluationsontheTouchStone(Baietal.,
2023), SEED-Bench (Li et al., 2023b), and MME (Fu et al., 2023). TouchStone is an open-ended vision-
language instruction-following benchmark. We compare the instruction-following ability of Qwen-VL-Chat
withotherinstruction-tunedLVLMsinbothEnglishandChineseontheTouchStonebenchmark. SEED-Bench
consistsof19Kmultiple-choicequestionswithaccuratehumanannotationsforevaluatingMultimodalLLMs,
covering 12 evaluation dimensions including both the spatial and temporal understanding. MME measures
both perception and cognition abilities on a total of 14 subtasks.
The results on three benchmarks are shown in Table 7. Qwen-VL-Chat has achieved obvious advantages
over other LVLMs on all three datasets, indicating that our model performs better in understanding and
answering diverse user instructions. In SEED-Bench, we have found that our model’s visual capabilities
canbeeffectivelytransferredtovideotasksbysimplysamplingfourframes. Intermsoftheoverallscores
presentedinTouchStone,ourmodeldemonstratesaclearadvantagecomparedtootherLVLMs,especially
intermsofitsChinesecapabilities. Intermsofthebroadcategoriesofabilities,ourmodelexhibitsamore
pronounced advantage in understanding and recognition, particularly in areas such as text recognition and
chart analysis. For more detailed information, please refer to the TouchStone dataset.
5 Related Work
In recent years, researchers have shown considerable interest in vision-language learning (Su et al., 2019;
Chen et al., 2020; Li et al., 2020; Zhang et al., 2021; Li et al., 2021b; Lin et al., 2021; Kim et al., 2021; Dou
et al., 2022; Zeng et al., 2021; Li et al., 2021a, 2022), especially in the development of multi-task generalist
models(HuandSingh,2021;Singhetal.,2022;Zhuetal.,2022;Yuetal.,2022;Wangetal.,2022a;Luetal.,
2022a; Bai et al., 2022). CoCa (Yu et al., 2022) proposes an encoder-decoder structure to address image-text
retrieval and vision-language generation tasks simultaneously. OFA (Wang et al., 2022a) transforms specific
vision-language tasksinto sequence-to-sequence tasksusing customized taskinstructions. UnifiedI/O (Lu
etal.,2022a)furtherintroducesmoretaskslikesegmentationanddepthestimationintoaunifiedframework.
Another category of research focuses on building vision-language representation models (Radford et al.,
2021; Jia et al., 2021; Zhai et al., 2022; Yuan et al., 2021; Yang et al., 2022a). CLIP (Radford et al., 2021)
leverages contrastive learningand large amounts of datato align images andlanguagein a semantic space,
resulting in strong generalization capabilities across a wide range of downstream tasks. BEIT-3 (Wang
et al., 2022b) employs a mixture-of-experts (MOE) structure and unified masked token prediction objective,
achieving state-of-the-art results on various visual-language tasks. In addition to vision-language learning,
ImageBind (Girdhar et al., 2023) and ONE-PEACE (Wang et al., 2023) align more modalities such as speech
into a unified semantic space, thus creating more general representation models.
Despite achieving significant progress, previous vision-language models still have several limitations such
9

--- PAGE 10 ---
aspoorrobustnessininstructionfollowing,limitedgeneralizationcapabilitiesinunseentasks,andalack
ofin-context abilities. Withtherapiddevelopment oflargelanguagemodels (LLMs)(Brownet al.,2020;
OpenAI,2023;Aniletal.,2023;Gaoetal.,2023;Qwen,2023),researchershavestartedbuildingmorepowerful
large vision-language models (LVLMs) based on LLMs (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2023c;
Daiet al.,2023;Huang etal.,2023; Pengetal., 2023;Zhuetal., 2023;Liuet al.,2023;Ye etal.,2023b,a; Chen
et al., 2023a; Li et al., 2023a; Zhang et al., 2023; Sun et al., 2023). BLIP-2 (Li et al., 2023c) proposes Q-Former
to align the frozen vision foundation models and LLMs. Meanwhile, LLAVA (Liu et al., 2023) and Mini-
GPT4(Zhuetal.,2023)introducevisualinstructiontuningtoenhanceinstructionfollowingcapabilitiesin
LVLMs. Additionally, mPLUG-DocOwl (Ye et al., 2023a) incorporates document understanding capabilities
into LVLMs by introducing digital documents data. Kosmos2 (Peng et al., 2023), Shikra (Chen et al., 2023a),
andBuboGPT(Zhaoetal.,2023)furtherenhanceLVLMswithvisualgroundingabilities,enablingregion
descriptionandlocalization. Inthiswork,weintegrateimagecaptioning,visualquestionanswering,OCR,
documentunderstanding,andvisualgroundingcapabilitiesintoQwen-VL.Theresultingmodelachieves
outstanding performance on these diverse style tasks.
6 Conclusion and Future Work
WereleasetheQwen-VLseries,asetoflarge-scalemultilingualvision-languagemodelsthataimstofacili-
tate multimodal research. Qwen-VL outperforms similar models across various benchmarks, supporting
multilingual conversations, multi-image interleaved conversations, grounding in Chinese, and fine-grained
recognition. Movingforward,wearededicatedtofurtherenhancingQwen-VL’scapabilitiesinseveralkey
dimensions:
•Integrating Qwen-VL with more modalities, such as speech and video.
•Augmenting Qwen-VL by scaling up the model size, training data and higher resolution, enabling it to
handle more complex and intricate relationships within multimodal data.
•Expanding Qwen-VL’s prowess in multi-modal generation, specifically in generating high-fidelity
images and fluent speech.
References
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi
Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. In NeurIPS, 2022.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2technicalreport. arXiv:2305.10403 ,2023.
JinzeBai,RuiMen,HaoYang,XuanchengRen,KaiDang,YichangZhang,XiaohuanZhou,PengWang,Sinan
Tan, An Yang, et al. Ofasys: A multi-modal multi-task learning system for building generalist models.
arXiv:2212.04408 , 2022.
ShuaiBai,ShushengYang,JinzeBai,PengWang,XingxuanZhang,JunyangLin,XinggangWang,ChangZhou,
and Jingren Zhou. Touchstone: Evaluating vision-language models by language models. arXiv:2308.16890 ,
2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
InNeurIPS, 2020.
10

--- PAGE 11 ---
MinwooByeon,BeomheePark,HaecheonKim,SungjunLee,WoonhyukBaek,andSaehoonKim. Coyo-700m:
Image-text pair dataset, 2022. URL https://github.com/kakaobrain/coyo-dataset .
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv:2306.15195 , 2023a.
XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,SebastianGoodman,
Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.
arXiv:2209.06794 , 2022.
XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelmeRuiz,
SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupamultilingualvisionandlanguage
model. arXiv preprint arXiv:2305.18565 , 2023b.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325 , 2015.
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
Uniter: Universal image-text representation learning. In ECCV, 2020.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang
Li,PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instruction tuning. arXiv:2305.06500 , 2023.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil
Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.
Zi-Yi* Dou, Aishwarya* Kamath, Zhe* Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu,
YannLeCun,NanyunPeng,JianfengGao,andLijuanWang. Coarse-to-finevision-languagepre-training
with fusion in the backbone. In NeurIPS, 2022.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui
Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language
models. arXiv:2306.13394 , 2023.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan
Marten,MitchellWortsman,DhrubaGhosh,JieyuZhang,etal. Datacomp: Insearchofthenextgeneration
of multimodal datasets. arXiv:2304.14108 , 2023.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,
Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv:2304.15010 , 2023.
RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,ArmandJoulin,
and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023.
Google. Puppeteer, 2023. URL https://github.com/puppeteer/puppeteer .
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:
Elevating the role of image understanding in visual question answering. In CVPR, 2017.
Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and Derek Hoiem. Grit: General robust image task
benchmark. arXiv:2204.13653 , 2022.
DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,andJeffreyP
Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018.
11

--- PAGE 12 ---
RonghangHuandAmanpreetSingh. Unit: Multimodalmultitasklearningwithaunifiedtransformer. In
ICCV, 2021.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui,OwaisKhanMohammed,QiangLiu,etal. Languageisnotallyouneed: Aligningperceptionwith
language models. arXiv:2302.14045 , 2023.
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and
compositional question answering. In CVPR, 2019.
GabrielIlharco,MitchellWortsman,RossWightman,CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,JohnMiller,HannanehHajishirzi,AliFarhadi,andLudwig
Schmidt. Openclip, 2021. URL https://doi.org/10.5281/zenodo.5143773 .
ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocVLe,YunhsuanSung,ZhenLi,
andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytextsupervision.
arXiv:2102.05918 , 2021.
KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa: Understandingdatavisualizations
via question answering. In CVPR, 2018.
SaharKazemzadeh, VicenteOrdonez, MarkMatten,and TamaraBerg. Referitgame: Referringto objectsin
photographs of natural scenes. In EMNLP, 2014.
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A
diagram is worth a dozen images. In ECCV, 2016.
GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,WonseokHwang,
SangdooYun,DongyoonHan,andSeunghyunPark. Ocr-freedocumentunderstandingtransformer. In
ECCV, 2022.
Wonjae Kim, Bokyung Son, andIldoo Kim. Vilt: Vision-and-language transformer withoutconvolutionor
region supervision. In ICML, 2021.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis
Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using
crowdsourced dense image annotations. In IJCV, 2017.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv:2305.03726 , 2023a.
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking
multimodal llms with generative comprehension. arXiv:2307.16125 , 2023b.
Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi.
Align before fuse: Vision and language representation learning with momentum distillation. In NeurIPS,
2021a.
JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. Blip: Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In ICML, 2022.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-training
with frozen image encoders and large language models. arXiv:2301.12597 , 2023c.
Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. UNIMO:
towards unified-modal understanding and generation via cross-modal contrastive learning. In ACL, 2021b.
Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong Hu,
Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for
vision-language tasks. In ECCV, 2020.
12

--- PAGE 13 ---
Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang,
Xianyan Jia, et al. M6: A chinese multimodal pretrainer. In KDD, 2021.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv:2304.08485 ,
2023.
JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi. Unified-io: A
unified model for vision, language, and multi-modal tasks. arXiv:2206.08916 , 2022a.
PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathoughtchainsforsciencequestion
answering. In NeurIPS, 2022b.
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Gener-
ation and comprehension of unambiguous object descriptions. In CVPR, 2016.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question
answering benchmark requiring external knowledge. In CVPR, 2019.
Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark for
question answering about charts with visual and logical reasoning. arXiv:2203.10244 , 2022.
MineshMathew,DimosthenisKaratzas, andCVJawahar. Docvqa: Adatasetforvqaon documentimages.
InWACV, 2021.
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question
answering by reading text in images. In ICDAR, 2019.
Openai. Chatml documents. URL https://github.com/openai/openai-python/blob/main/chatml.md .
OpenAI. Gpt-4 technical report, 2023.
VicenteOrdonez, GirishKulkarni, andTamaraBerg. Im2text: Describingimages using1million captioned
photographs. In NeurIPS, 2011.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world. arXiv:2306.14824 , 2023.
Qwen. Introducing qwen-7b: Openfoundationand human-alignedmodels(ofthestate-of-the-arts),2023.
URL https://github.com/QwenLM/Qwen-7B .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In ICML, 2021.
ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-5b: Anopenlarge-scale
dataset for training next generation image-text models. arXiv:2210.08402 , 2022a.
Christoph Schuhmann, Andreas Köpf, Richard Vencu, Theo Coombes,and Romain Beaumont. Laion coco:
600m synthetic captions from laion2b-en. https://laion.ai/blog/laion-coco/ , 2022b.
PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions: Acleaned,hyper-
nymed, image alt-text dataset for automatic image captioning. In ACL, 2018.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image
captioning with reading comprehension. In ECCV, 2020.
13

--- PAGE 14 ---
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus
Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model. In CVPR, 2022.
Artifex Software. Pymupdf, 2015. URL https://github.com/pymupdf/PyMuPDF .
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic
visual-linguistic representations. In ICLR, 2019.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,
Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv:2307.05222 , 2023.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In CVPR, 2015.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,
and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-
sequence learning framework. In ICML, 2022a.
Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xiaohuan Zhou, Jingren Zhou, Xinggang Wang, and
Chang Zhou. One-peace: Exploring one general representation model toward unlimited modalities.
arXiv:2305.11172 , 2023.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan
Mohammed,SakshamSinghal,SubhojitSom,etal. Imageasaforeignlanguage: Beitpretrainingforall
vision and vision-language tasks. arXiv:2208.10442 , 2022b.
An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip:
Contrastive vision-language pretraining in chinese. arXiv:2211.01335 , 2022a.
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An
empirical study of gpt-3 for few-shot knowledge-based vqa. In AAAI, 2022b.
Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang
Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document
understanding. arXiv:2307.02499 , 2023a.
QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,Pengcheng
Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality.
arXiv:2304.14178 , 2023b.
PeterYoung,AliceLai,MicahHodosh,andJuliaHockenmaier. Fromimagedescriptionstovisualdenotations:
New similarity metrics for semantic inference over event descriptions. In ACL, 2014.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastive captioners are image-text foundation models. arXiv:2205.01917 , 2022.
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel C. F. Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,
XuedongHuang,BoxinLi,ChunyuanLi,CeLiu,MengchenLiu,ZichengLiu,YumaoLu,YuShi,Lijuan
Wang,JianfengWang,BinXiao,ZhenXiao,JianweiYang,MichaelZeng,LuoweiZhou,andPengchuan
Zhang. Florence: A new foundation model for computer vision. arXiv:2111.11432 , 2021.
YanZeng,XinsongZhang,andHangLi. Multi-grainedvisionlanguagepre-training: Aligningtextswith
visual concepts. arXiv:2111.08276 , 2021.
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas
Beyer. Lit: Zero-shot transfer with locked-image text tuning. In CVPR, 2022.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for
video understanding. arXiv:2306.02858 , 2023.
14

--- PAGE 15 ---
PengchuanZhang,XiujunLi,XiaoweiHu,JianweiYang,LeiZhang,LijuanWang,YejinChoi,andJianfeng
Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.
YangZhao,ZhijieLin,DaquanZhou,ZilongHuang,JiashiFeng,andBingyiKang. Bubogpt: Enablingvisual
grounding in multi-modal llms. arXiv:2307.08581 , 2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv:2304.10592 , 2023.
Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver:
Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In CVPR, 2022.
15

--- PAGE 16 ---
A Dataset details
A.1 Image-text pairs
We use web-crawled image-text pairs dataset for pre-training, which includes LAION-en (Schuhmann et al.,
2022a), LAION-zh (Schuhmann et al., 2022a), LAION-COCO (Schuhmann et al., 2022b), DataComp (Gadre
et al., 2023) and Coyo (Byeon et al., 2022). We clean these noisy data by several steps:
1. Removing pairs with too large aspect ratio of the image
2. Removing pairs with too small image
3. Removing pairs with a harsh CLIP score (dataset-specific)
4. Removing pairs with text containing non-English or non-Chinese characters
5. Removing pairs with text containing emoji characters
6. Removing pairs with text length too short or too long
7. Cleaning the text’s HTML-tagged part
8. Cleaning the text with certain unregular patterns
For academic caption datasets, we remove pairs whose text contains the special tags in CC12M (Changpinyo
et al., 2021) and SBU (Ordonez et al., 2011). If there is more than one text matching the same image, we
select the longest one.
A.2 VQA
FortheVQAv2(Goyaletal.,2017)dataset,weselecttheanswerannotationbasedonthemaximumconfidence.
For other VQA datasets, we didn’t do anything special.
A.3 Grounding
For the GRIT (Peng et al., 2023) dataset, we found that there are many recursive grounding box labels in one
caption. Weusethegreedyalgorithmtocleanthecaptiontomakesureeachimagecontainsthemostbox
labelswithnorecursiveboxlabels. Forothergroundingdatasets,wesimplyconcatenatethenoun/phrase
with respective bounding box coordinates.
A.4 OCR
WegeneratedthesyntheticOCRdatasetusingSynthdog(Kimetal.,2022). Specifically,weusetheCOCO(Lin
et al., 2014) train2017 and unlabeld2017 dataset split as the natural scenery background. Then we selected 41
English fonts and 11 Chinese fonts to generate text. We use the default hyperparameters as in Synthdog. We
trackthegeneratedtextlocationsintheimageandconvertthemtoquadrilateralcoordinatesandwealsouse
these coordinates as training labels. The visualization example is illustrated in the second row of Fig 5.
ForallthePDFdatawecollected,wefollowthestepsbelowtopre-processthedatausingPyMuPDF(Software,
2015) to get the rendering results of each page in a PDF file as well as all the text annotations with their
bounding boxes.
1. Extracting all texts and their bounding boxes for each page.
16

--- PAGE 17 ---
Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL
17

--- PAGE 18 ---
2. Rendering each page and save them as an image file.
3. Removing too small image.
4. Removing images with too many or too few characters.
5.RemovingimagescontainingUnicodecharactersinthe“LatinExtended-A”and“LatinExtended-B”
blocks.
6. Removing images containing Unicode characters in the “Private Use Area (PUA)” block.
For all HTML web pages we collected, we pre-process them in a similar approach to all the PDF data we
collected,butweusePuppeteer(Google,2023)insteadofPyMuPDFtorendertheseHTMLpagesandget
the ground truth annotation. We follow the steps below to pre-process the data.
1. Extracting all texts for each webpage.
2. Rendering each page and save them as an image file.
3. Removing too small image.
4. Removing images with too many or too few characters.
5. Removing images containing Unicode characters in the “Private Use Area (PUA)” block.
B Data Format Details of Training
B.1 Data Format of Multi-Task Pre-training
We visualize the Multi-Task Pre-training data format in Box B.1. The Box contains all 7 tasks with the
black-colored text as the prefix sequence without loss and blue-colored text as the ground truth labels with
loss.
18

--- PAGE 19 ---
Image Captioning
<img>cc3m/01581435.jpg </img >Generate the caption in English: the beautiful flowers for
design. <eos>
Vision Question Answering
<img>VG_100K_2/1.jpg </img >Does the bandage have a different color than the wrist band?
Answer: No, both the bandage and the wrist band are white. <eos>
OCR VQA
<img>ocr_vqa/1.jpg </img >Whatisthe titleofthisbook? Answer: AsiSe Dice!,Volume 2: Work-
book And Audio Activities (Glencoe Spanish) (Spanish Edition) <eos>
Caption with Grounding
<img>coyo700m/1.jpg </img >GeneratethecaptioninEnglishwithgrounding: Beautifulshotof
<ref>bees</ref><box>(661,612),(833,812) </box><box>(120,555),(265,770) </box>gathering
nectars from <ref>an apricot flower </ref><box>(224,13),(399,313) </box><eos>
Referring Grounding
<img>VG_100K_2/3.jpg </img ><ref>the ear on a giraffe </ref><box>(176,106),(232,160)
</box><eos>
Grounded Captioning
<img>VG_100K_2/4.jpg </img><ref>This</ref><box>(360,542),(476,705) </box>isYellowcross
country ski racing gloves <eos>
OCR
<img>synthdog/1.jpg </img>OCR with grounding: <ref>It is managed </ref> <quad >(568,121),
(625,131), (624,182), (567,172) </quad >...<eos>
B.2 Data Format of Supervised Fine-tuning
Tobetteraccommodatemulti-imagedialogueandmultipleimageinputs,weaddthestring"Picture id:"before
differentimages,wherethe idcorrespondstotheorderofimageinputdialogue. Intermsofdialogueformat,
weconstructourinstructiontuningdatasetusingtheChatML(Openai)format, whereeachinteraction’s
statement is marked with two special tokens ( <im_start >and<im_end >) to facilitate dialogue termination.
The Dataset Format Example of ChatML
<im_start >user
Picture 1: <img>vg/VG_100K_2/649.jpg </img >What is the sign in the picture? <im_end >
<im_start >assistant
The sign is a road closure with an orange rhombus. <im_end >
<im_start >user
How is the weather in the picture? <im_end >
<im_start >assistant
The shape of the road closure sign is an orange rhombus. <im_end >
Duringtraining,weensuretheconsistencybetweenpredictionandtrainingdistributionsbyonlysupervising
answers and special tokens (blue in the example), and not supervising role names or question prompts.
19

--- PAGE 20 ---
C Hyperparameters
We report the detailed training hyperparameter settings of Qwen-VL in Table 8.
Table 8: Training hyperparameters of Qwen-VL
Configuration Pre-training Multi-task Pre-training Supervised Fine-tuning
ViT init. Open-CLIP-bigG Qwen-VL 1st-stage Qwen-VL 2nd-stage
LLM init. Qwen-7B Qwen-7B Qwen-VL 2nd-stage
VL Adapter init. random Qwen-VL 1st-stage Qwen-VL 2nd-stage
Image resolution 224244824482
ViT sequence length 256 1024 1024
LLM sequence length 512 2048 2048
Learnable query numbers 256 256 256
Optimizer AdamW
Optimizer hyperparameter β1= 0.9, β2= 0.98, eps = 1e−6
Peak learning rate 2e−45e−51e−5
Minimum learning rate 1e−61e−51e−6
ViT learning rate decay 0.95 0.95 0
ViT Drop path rate 0
Learning rate schedule cosine decay
Weight decay 0.05
Gradient clip 1.0
Training steps 50k 19k 8k
Warm-up steps 500 400 3k
Global batch size 30720 4096 128
Gradient Acc. 6 8 8
Numerical precision bfloat16
Optimizer sharding ✓
Activation checkpointing ✗
Model parallelism ✗ 2 2
Pipeline parallelism ✗
Inthefirstpre-trainingstage,themodelistrainedusingAdamWoptimizerwith β1= 0.9, β2= 0.98, eps =
1e−6. We use the cosine learning rate schedule and set the maximum learning rate of 2e−4and minimum of
1e−6withalinearwarm-upof500steps. Weuse aweightdecayof 5e−2andagradientclippingof 1.0. For
theViTimageencoder,weapplyalayer-wiselearningratedecaystrategywithadecayfactorof 0.95. The
trainingprocessusesabatchsizeof30720fortheimage-textpairs,andtheentirefirststageofpre-training
lasts for50,000steps, consumingapproximately 1.5billionimage-text samplesand 500billionimage-text
tokens.
Inthesecondmulti-task training stage, weincreasetheinputresolutionof thevisual encoderfrom 224×224
to448×448,reducingtheinformationlosscausedbyimagedown-sampling. Weunlockedthelargelanguage
model and trained the whole model. The training objective is the same as the pre-training stage. We use
AdamW optimizer with β1= 0.9, β2= 0.98, eps = 1e−6. We trained for 19000 steps with 400 warm-up steps
and a cosine learning rate schedule. Specifically, we use the model parallelism techniques for ViT and LLM.
D Summary of the evaluation benchmarks
We provide a detailed summary of the used evaluation benchmarks and corresponding metrics in Table 9.
20

--- PAGE 21 ---
Table 9: Summary of the evaluation benchmarks.
Task Dataset Description Split Metric
Image CaptionNocaps Captioning of natural images val CIDEr( ↑)
Flickr30K Captioning of natural images karpathy-test CIDEr( ↑)
General VQAVQAv2 VQA on natural images test-dev VQA Score( ↑)
OKVQA VQA on natural images requiring outside knowledge val VQA Score( ↑)
GQA VQA on scene understanding and reasoning test-balanced EM(↑)
ScienceQA-Img Multi-choice VQA on a diverse set of science topics test Accuracy( ↑)
VizWiz VQA on photos taken by people who are blind test-dev VQA Score( ↑)
Text-oriented VQATextVQA VQA on natural images containing text val VQA Score( ↑)
DocVQA VQA on images of scanned documents test ANLS(↑)
ChartQA VQA on images of charts test Relaxed EM( ↑)
OCRVQA VQA on images of book covers test EM(↑)
AI2Diagram VQA on images of scientific diagrams test EM(↑)
RefCOCO Refer grounding on natural images val & testA & testB Accuracy( ↑)
Refer Expression RefCOCO+ Refer grounding on natural images val & testA & testB Accuracy( ↑)
Comprehension RefCOCOg Refer grounding on natural images val & test Accuracy( ↑)
GRiT Refer grounding on natural images test Accuracy( ↑)
Instruction FollowingTouchStone Open-ended VL instruction following benchmark English & Chinese GPT- 4Score (↑)
MME Open-ended VL Benchmark by yes/no questions Perception & Cognition Accuracy ( ↑)
Seed-Bench Open-ended VL Benchmark by Multi-choice VQA Image & Video Accuracy ( ↑)
E Additional experimental details
E.1 Convergence of the Pre-training Stage
InFigure6,weshowtheconvergenceofthePre-trainingStage(stageone). Thewholemodelsaretrained
using BFloat16mixedprecision, thebatch size is30720, and the learningrateis 2e−4. All images areonly
trainedonce(oneepoch). Thetraininglossdecreasessteadilywiththeincreaseofthenumberoftraining
pictures. Note that, the pre-training stage (Stage one) has no VQA data being added, but the Zero-shot VQA
score increases amidst fluctuations.
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
#Images(B)1.61.82.02.22.42.62.83.0Loss
a. Pre-training Loss0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
#Images(B)6264666870727476CIDEr
b. Caption (Flickr)0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
#Images(B)4850525456Accuracy
c. Zero-shot VQA (VQAv2)
Figure 6: Visualization of the Convergence of the Pre-training Stage
E.2 Number of Learnable Queries in the Vision-Language Adapter
The vision-language adapter usescross-attention to compress the visual featuresequence bya set of learning
queriesoflength. Toofewqueriescanleadtothelossofsomevisualinformation,whiletoomanyqueries
may reduce in greater convergence difficulty and computational cost.
An ablation experiment is conducted on the number of learnable queries in the vision-language adapter. We
21

--- PAGE 22 ---
10 20 30 40 50
Steps34567891011LossL64
L144
L256
L400
1000 1500 2000 2500 3000 3500 4000 4500
Steps2.402.452.502.552.602.652.70LossL64
L144
L256
L400Figure7: Visualizationofthetraininglosswhenusingdifferentcompressedfeaturelengthsofthevision-
language adapter. The left depicts the initial training loss (within 50 steps), and the right depicts the loss in
convergence(1k-5ksteps). Inthelegend, L64denotesthattheadapteruses64queriestocompressthevisual
featuresequencetoafixedlengthof64,andsoon. Thelosscurveshavebeensmoothedtoavoidshading
owing to fluctuations.
usedViT-L/14asthevisualencoderandthe 224×224resolutionpictureasinput,sothesequencelength
of ViT’s output is (224/14)2= 256. As shown in the left part of Figure 7, the fewer queries used at the
beginning of training, the lower the initial loss. However, with convergence, too many or too few queries
willcauseconvergencetoslowdown,asshownintherightpartofFigure7. Consideringthatthesecond
training stage (Multi-task Pre-train) applies 448*448 resolution, where the sequence length of ViT’s output
is(448/14)2= 1024. Toofewqueriescanresultinmoreinformationbeinglost. Wefinallychosetouse256
queries for the vision-language adapter in Qwen-VL.
E.3 Window Attention vs Global Attention for Vision Transformer
Using ahigh-resolution Vision Transformer inthe model will significantlyincrease the computational cost.
OnepossiblesolutiontoreducethecomputationalcostofthemodelistouseWindowAttentionintheVision
Transformer,i.e.,toperformAttentiononlyinawindowof 224×224inmostlayersoftheViTpartofthe
model, and to perform Attention for the full 448×448or896×896image in a small number of layers (e.g. 1
out of every 4 layers) of the ViT part of the model.
Tothisend,weconductedablationexperimentstocomparetheperformanceofthemodelwhenusingGlobal
AttentionandWindowAttentionforViT.Wecomparetheexperimentalresultsforanalysingthetrade-off
between computational efficiency and convergence of the model.
Table 10: Training speed of Window Attention vs Global Attention for different input image resolutions
Model input resolution & Attention type Training speed
448×448, Global Attention 10s / iter
448×448, Window Attention 9s / iter
896×896, Global Attention 60s / iter
896×896, Window Attention 25s / iter
As shown in Figure 8 and Table 10, the loss of the model is significantly higher when Window Attention
instead of Vanilla Attention is used. And the training speeds for both of them are similar. Therefore, we
decided to use Vanilla Attention instead of Window Attention for the Vision Transformer when training
Qwen-VL.
22

--- PAGE 23 ---
Figure 8: Visualization of the Loss when using Window Attention vs Global Attention
Thereason wedon’tuse WindowAttentionwith 896×896resolutionis thatits training speedistooslowfor
us. Although itreaches a loss value similar to model with 448×448resolution input at 5000steps. It takes
almost 2.5 times longer to train than the model with 448×448resolution input.
E.4 Performance on Pure-text Tasks
Inordertostudytheeffectofmulti-modaltrainingonpure-textability,weshowtheperformanceofpure-text
tasks of Qwen-VL compared to open-source LLM in Table 11.
Qwen-VLusesanintermediatecheckpointofQwen-7BastheLLMinitialization. Thereasonwhywedid
notusethefinalreleasedcheckpointofQwen-7BisthatQwen-VLandQwen-7Bweredevelopedatavery
similarperiod. BecauseQwen-VLhasagoodinitializationonLLMbyQwen-7B,itiscomparabletomany
text-only LLMs on pure-text tasks.
Table 11: Performance on Pure-text Benchmarks of Qwen-VL compared to open-source LLM. Due to the
introductionofpure-textdatainthemulti-tasktrainingandSFTstage,Qwen-VLdonotcompromiseany
pure-text ability.
Model MMLU CMMLU C-Eval
LLaMA-7B 35.1 26.8 -
LLaMA2-7B 46.8 31.8 32.5
Baichuan-7B 42.3 44.4 42.8
Baichuan2-7B 54.2 57.1 54.0
ChatGLM2-6B 47.9 48.8 51.7
InternLM-7B 51.0 51.8 52.8
Qwen-7B (final released) 58.2 62.2 63.5
Qwen-7B (intermediate, use as Qwen-VL’s LLM initialization) 49.9 - 48.5
Qwen-VL 50.7 49.5 51.1
Furthermore,inthemulti-tasktrainingandSFTstages,Qwen-VLnotonlyutilizesvisualandlanguage-related
data but also incorporates pure-text data for training. The purpose of this is to prevent the catastrophic
23

--- PAGE 24 ---
forgetting of text comprehension by leveraging the information from pure-text data. The results in Table 11
indicatethattheQwen-VLmodeldoesnotexhibitanydegradationintermsofitspuretextcapabilityand
even demonstrates improvement after multi-task training.
24
