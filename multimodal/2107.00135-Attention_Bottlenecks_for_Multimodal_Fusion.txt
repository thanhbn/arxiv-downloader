# 2107.00135.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2107.00135.pdf
# File size: 839715 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Attention Bottlenecks for Multimodal Fusion
Arsha Nagrani Shan Yang Anurag Arnab Aren Jansen
Cordelia Schmid Chen Sun
{anagrani, shanyang, aarnab, arenjansen, cordelias, chensun}@google.com
Google Research
Abstract
Humans perceive the world by concurrently processing and fusing high-
dimensional inputs from multiple modalities such as vision and audio. Machine
perception models, in stark contrast, are typically modality-speciﬁc and optimised
for unimodal benchmarks, and hence late-stage fusion of ﬁnal representations or
predictions from each modality ( ‘late-fusion’ ) is still a dominant paradigm for
multimodal video classiﬁcation. Instead, we introduce a novel transformer based
architecture that uses ‘fusion bottlenecks’ for modality fusion at multiple layers.
Compared to traditional pairwise self-attention, our model forces information be-
tween different modalities to pass through a small number of bottleneck latents,
requiring the model to collate and condense relevant information in each modality
and share what is necessary. We ﬁnd that such a strategy improves fusion per-
formance, at the same time reducing computational cost. We conduct thorough
ablation studies, and achieve state-of-the-art results on multiple audio-visual classi-
ﬁcation benchmarks including Audioset, Epic-Kitchens and VGGSound. All code
and models will be released.
1 Introduction
Simultaneous multimodal sensations are a crucial enabler of human perceptual learning [ 57]. For
artiﬁcial learning systems, however, designing a uniﬁed model for modality fusion is challenging
due to a number of factors: (i) variations in learning dynamics between modalities [ 63], (ii) different
noise topologies, with some modality streams containing more information for the task at hand
than others, as well as (iii) specialised input representations. The difference in input representations
between audio and vision is particularly stark – many state of the art audio classiﬁcation methods
rely on short term Fourier analysis to produce log-mel spectrograms, often using them as inputs to
CNN architectures designed for images [ 29,55]. These time-frequency representations have different
distributions to images – multiple acoustic objects can have energy at the same frequency, and the
translation invariances of CNNs may no longer be a desired property (while an acoustic object can be
shifted in time, a shift in frequency could alter the meaning entirely). In contrast, the visual stream in
a video is three-dimensional (two spatial and one temporal), and while different spatial regions of
the image correspond to different objects, there is the unique challenge of high redundancy across
multiple frames. Hence input representations, and consequently neural network architectures and
benchmarks tend to vary wildly for different modalities. For simplicity, the dominant paradigm for
multimodal fusion therefore often consists of an ad-hoc scheme that involves integrating separate
audio and visual networks via their output representations or scores i.e. ‘late-fusion’ [25, 49].
In this work, we present a new transformer based model for audiovisual fusion in video. Despite
originally being proposed for NLP tasks, there has been recent interest in transformers [ 61] as
universal perceptual models [ 32], due to their ability to model dense correlations between tokens,
at the same time making few assumptions about their inputs (and because continuous perceptual
inputs can be tokenised). By dividing dense continuous signals into patches and rasterising them
35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2107.00135v3  [cs.CV]  30 Nov 2022

--- PAGE 2 ---
%RWWOHQHFN$XGLR9LGHR
/DWH)XVLRQ0LG)XVLRQ%RWWOHQHFN)XVLRQ%RWWOHQHFN0LG)XVLRQ7\SHRIWRNHQ/D\HUFigure 1: Cross-modal Fusion . Unlike late fusion (left), where no cross-modal information is
exchanged in the model until after the classiﬁer, we investigate two pathways for the exchange of
cross-modal information. The ﬁrst is via standard pairwise self attention across all hidden units in
a layer, but applied only to later layers in the model – mid fusion (middle, left). We also propose
the use of ‘fusion bottlenecks’ (middle, right) that restrict attention ﬂow within a layer through tight
latent units. Both forms of restriction can be applied in conjunction (Bottleneck Mid Fusion) for
optimal performance (right). We show B= 2bottleneck units and 3 hidden units per modality. Grey
boxes indicate tokens that receive attention ﬂow from both audio and video tokens.
to 1D tokens, transformers have been shown to perform competitively for image (ViT [ 18]) and
video classiﬁcation (ViViT [ 6]), and more recently, audio classiﬁcation (AST [ 26]). Because these
models are able to elegantly handle variable length sequences, a natural ﬁrst extension would be to
feed in a sequence of both visual and auditory patches to a transformer, with minimal changes to
the architecture. This ‘early fusion’ model allows free attention ﬂow between different spatial and
temporal regions in the image, as well as across frequency and time in the audio spectrogram. While
theoretically appealing, we hypothesise that full pairwise attention at all layers of the model is not
necessary because audio and visual inputs contain dense, ﬁne-grained information, much of which is
redundant. This is particularly the case for video , as shown by the performance of ‘factorised’ versions
of [6]. Such a model would also not scale well to longer videos due to the quadratic complexity of
pairwise attention with token sequence length. To mitigate this, we propose two methods to restrict
the ﬂow of attention in our model. The ﬁrst follows from a common paradigm in multimodal learning,
which is to restrict cross-modal ﬂow to later layers of the network, allowing early layers to specialise
in learning and extracting unimodal patterns. Henceforth this is is referred to as ‘mid fusion’ (Fig. 1,
middle left), where the layer at which cross-modal interactions are introduced is called the ‘fusion
layer’. The two extreme versions of this are ‘early fusion’ (all layers are cross-modal) and ‘late fusion’
(all are unimodal) which we compare to as a baselines. Our second idea (and main contribution),
is to restrict cross-modal attention ﬂow between tokens within a layer. We do this by allowing free
attention ﬂow within a modality, but force our model to collate and ‘condense’ information from each
modality before sharing it with the other. The core idea is to introduce a small set of latent fusion units
that form an ‘attention bottleneck’, through which cross-modal interactions within a layer must pass.
We demonstrate that this ‘bottlenecked’ version, which we name Multimodal Bottleneck Transformer
(MBT ), outperforms or matches its unrestricted counterpart, but with lower computational cost.
Concretely, we make the following contributions: (i) We propose a new architecture ( MBT ) for
audiovisual fusion. Our model restricts the ﬂow of cross-modal information between latent units
through tight fusion ‘bottlenecks’, that force the model to collect and ‘condense’ the most relevant
inputs in each modality (and therefore share only that which is necessary with the other modality).
This avoids the quadratic scaling cost of full pairwise attention, and leads to performance gains with
less compute; (ii) We apply MBT to image and spectogram patches (Fig. 2), and explore a number
of ablations related to the fusion layer, the sampling of inputs and data size; and ﬁnally (iii) We set
the new state-of-the-art for video classiﬁcation across a number of popular audio-visual benchmarks,
including AudioSet [24], Epic-Kitchens100 [14] and VGGSound [12]. On the Audioset dataset, we
outperform the current state of the art by 5.9 mAP (12.7% relative improvement).
2 Related work
Audiovisual learning: Audiovisual multimodal learning has a rich history, both before and during
the deep learning era [ 53]. Given the limited available data and computational resources, early
work focused on relatively simple early-stage (e.g. stacking hand-designed features) and late-stage
2

--- PAGE 3 ---
Audio spectrogram patches RGB frame patchesAudio Projection Video Projection  CLS12FSNFSN12   Multimodal Bottleneck Transformer 
Multimodal Video ......
...1BclassifierclassifierMultimodal BottlenecksCLSavg logitsFigure 2: A Multimodal Fusion Transformer applied to audiovisual inputs. The input sequence
consists of image and spectrogram patches. These are then projected into tokens and appended to
special CLS (classiﬁcation) and FSN (fusion bottleneck) tokens. Our transformer encoder then uses
self attention to model unimodal information, and restricts cross-modal information ﬂow via cross
attention with the bottleneck tokens at multiple layers of the network.
(e.g. score fusion) techniques [ 13]. Deep learning has allowed more sophisticated strategies in
which modality-speciﬁc or joint latents are implicitly learned to mediate the fusion. The result has
enabled major advances in a range of downstream supervised audiovisual tasks [ 48,38,19]. In the
supervised setting, multiple modality-speciﬁc convolution networks can be jointly trained, whose
intermediate activations are then combined by summation [ 36] or via ‘lateral connections’ [ 64]. In the
unsupervised setting, audiovisual learning is commonly used to learn good unimodal representations,
with a popular pretraining task being to synchronise signals from different modalities via a contrastive
loss [4, 5, 7, 49, 33, 2, 3], however each modality is usually encoded separately under this setup.
Multimodal transformers: The self attention operation of transformers provides a natural mecha-
nism to connect multimodal signals. Multimodal transformers have been applied to various tasks
including audio enhancement [ 19,60], speech recognition [ 27], image segmentation [ 66,60], cross-
modal sequence generation [ 43,41,56], image and video retrieval [ 28,23,8], visual navigation [ 51]
and image/video captioning/classiﬁcation [ 46,59,58,40,31]. For many works, the inputs to trans-
formers are the output representations of single modality CNNs [ 39,23] – unlike these works we
use transformer blocks throughout, using only a single convolutional layer to rasterise 2D patches.
The tokens from different modalities are usually combined directly as inputs to the transformers [ 42],
for example, the recently released Perceiver model [ 32] introduces an iterative attention mechanism
which takes concatenated raw multimodal signals as inputs, which corresponds to our ‘early fusion’
baseline. In contrast, we carefully examine the impact of different modality fusion strategies, includ-
ing limiting cross-modal attention ﬂow to later layers of our model, and ‘channeling’ cross-modal
connections through bottlenecks in our proposed Multimodal Bottleneck Transformer (MBT).
3 Multimodal fusion transformers
In this section we describe our proposed Multimodal Bottleneck Transformer (MBT). We begin
by summarising the recently proposed Vision Transformer (ViT) [ 18] and Audio Spectrogram
Transformer (AST) [ 26], developed for image and audio classiﬁcation respectively, in Sec. 3.1. We
then describe our extension to the audio-visual fusion case. We discuss three different token fusion
strategies (Sec. 3.2), and ﬁnally discuss the fusion pathway in the entire model (Sec. 3.3), which
involves restricting multimodal fusion to certain layers of the model.
3.1 The ViT and AST architectures
Vision Transformer (ViT) [ 18] (and a recent extension to audio – Audio Spectrogram Transformer
(AST) [ 26]) adapts the Transformer architecture [ 61], originally designed for natural language
processing, to process 2D inputs with minimal changes. The key insight is to extract Nnon-
overlapping patches from the RGB image (or the audio spectrogram), xi2Rhw, and convert them
into a series of 1D tokens zi2Rd, as follows:
z=g(x;E;zcls) = [zcls;Ex1;Ex2;:::;ExN] +p: (1)
3

--- PAGE 4 ---
Here, Eis a linear projection mapping each token to Rd,zclsis a special token prepended to this
sequence so that its representation at the ﬁnal layer can be passed to a classiﬁer for classiﬁcation
tasks [ 17], and p2R(N+1)dis a learned positional embedding added to the tokens to retain
positional information (as all subsequent self-attention operations are permutation invariant).
The tokens are then passed through an encoder consisting of a sequence of Ltransformer layers. Each
transformer layer consists of Multi-Headed Self-Attention (MSA), Layer Normalisation (LN) and
Multilayer Perceptron (MLP) blocks applied using residual connections. We denote a transformer
layer, zl+1= Transformer( zl)as
yl= MSA(LN( zl)) +zl(2)
zl+1= MLP(LN( yl)) +yl: (3)
Here, the MSA operation [ 61] computes dot-product attention [ 61] where the queries, keys and
values are all linear projections of the same tensor, MSA( X) = Attention( WQX;WKX;WVX).
We further deﬁne Multi-Headed Cross Attention (MCA) between two tensors, XandY, where
Xforms the query and Yforms the keys and values which are used to reweight the query as
MCA( X;Y) = Attention( WQX;WKY;WVY). This will be used in our multimodal case, as
described next.
3.2 Multimodal transformer
We now describe our extension to the multimodal case. We begin by discussing three different token
fusion strategies.
3.2.1 Fusion via vanilla self-attention
We begin by describing a ‘vanilla’ fusion model, which simply consists of the regular transformer
applied to multimodal inputs. Our method of tokenising video is straightforward – given a video clip
of lengthtseconds, we uniformly sample FRGB frames and convert the audio waveform into a
single spectrogram. We then embed each frame and the spectrogram independently following the
encoding proposed in ViT [18], and concatenate all tokens together into a single sequence.
Formally, if we have extracted a total of NvRGB patches from all Fsampled frames, xrgb2RNvd,
andNaspectrogram patches, xspec2RNad, our sequence of tokens is
z= [zrgbjjzspec]where zrgb=g(xrgb;Ergb;zcls-rgb)and zspec=g(xspec;Espec;zcls-spec ):(4)
Here, [zrgbjjzspec]denotes the concatenation of the tokens for each modality. We use different
projections ErgbandEspecfor RGB and spectrogram patches respectively, and prepend a separate
classiﬁcation token for each modality.
Our multimodal encoder then applies a series of transformer layers in the same manner as above.
Attention is allowed to ﬂow freely through the network, i.e. each RGB token can attend to all other
RGB and spectrogram tokens as follows: zl+1= Transformer( zl;)with model parameters . Here
Transformer refers to a standard transformer layer with vanilla self-attention blocks.
3.2.2 Fusion with modality-speciﬁc parameters
We can generalise this model by allowing each modality to have its own dedicated parameters rgb
andspec, but still exchange information via the attention mechanism. For this purpose, we deﬁne a
Cross-Transformer layer:
zl+1
rgb= Cross -Transformer( zl
rgb;zl;rgb) (5)
zl+1
spec= Cross -Transformer( zl
spec;zl;spec);
where the Cross-Transformer employs the generalised cross-attention operation that takes two sets
of inputs z1andz2that are not necessarily overlapping. This layer follows the original transformer
layer with the difference being that Eq. 2 becomes
yl= MCA(LN( zl
1);LN(zl
2)) +zl
1: (6)
Finally, note that we have explicitly deﬁned the parameters, rgbandspecof the cross-transformer
layers in Eq. 5 as they are different for each modality. However, when rgbandspecare equal,
(rgb=spec=), the computation deﬁned in Eq. 5 is equivalent to Sec. 3.2.1.
4

--- PAGE 5 ---
3.2.3 Fusion via attention bottlenecks
In order to tame the quadratic complexity of pairwise attention, we next introduce a small set of
Bfusion bottleneck tokens zfsn= [z1
fsn;z2
fsn;:::;zB
fsn]to our input sequence (see Fig. 2). The input
sequence is now
z= [zrgbjjzfsnjjzspec]: (7)
We then restrict all cross-modal attention ﬂow in our model to be via these bottleneck tokens. More
formally for layer l, we compute token representations as follows:
[zl+1
ijj^ zl+1
fsni] = Transformer([ zl
ijjzl
fsn];i) (8)
zl+1
fsn= Avg i(^ zl+1
fsni) (9)
Hereiindexes each modality, in this case RGB and Spec, and zrgbandzspeccan only exchange
information via the bottleneck zfsnwithin a transformer layer. We ﬁrst create modality speciﬁc
temporary bottleneck fusion tokens ^ zfsni, which are updated separately and simultaneously with audio
and visual information (Equation 8). The ﬁnal fusion tokens from each cross-modal update are then
averaged in Equation 9. We also experiment with asymmetric updates for the bottleneck tokens (see
appendix) and found performance was robust to this choice. We keep the number of bottleneck tokens
in the network to be much smaller than the total number of latent units per modality ( BNvand
BNa). Because all cross-modal attention ﬂow must pass through these units, these tight ‘fusion’
bottlenecks force the model to condense information from each modality and share that which is
necessary. As we show in the experiments, this increases or maintains performance for multimodal
fusion, at the same time reducing computational complexity. We also note that our formulation is
generic to the type and the number of modalities.
3.3 Where to fuse: early, mid and late
The above strategies discuss fusion within a layer, and in most transformer architectures (such as
ViT), every layer consists of an identical set of operations. A common paradigm in multimodal
learning, however, is to restrict early layers of a network to focus on unimodal processing, and only
introduce cross-modal connections at later layers. This is conceptually intuitive if we believe lower
layers are involved in processing low level features, while higher layers are focused on learning
semantic concepts – low-level visual features such as edges and corners in images might not have a
particular sound signature, and therefore might not beneﬁt from early fusion with audio [64].
This can be implemented with our model as follows: We initially perform vanilla self-attention among
tokens from a single modality for Lflayers. Thereafter, we concatenate all latent tokens together,
zLf= [zLf
rgbjjzLf
spec]and pass them through the remaining L Lflayers where the tokens are fused
according to Sec. 3.2. Here, Lf= 0corresponds to an ‘early-fusion’ model, Lf=La ‘late-fusion’
model, and 0<Lf<L a ‘mid-fusion’ one. More formally, this can be denoted as
zl+1
rgb= Transformer( zl
rgb;rgb);zl+1
spec= Transformer( zl
spec;spec) ifl<L f
zl= [zl
rgbjjzl
spec]; zl+1= Multimodal -Transformer( zl;spec;rgb)otherwise
where Multimodal -Transformer()can refer to either of the 3 fusion strategies described in Sec 3.2.
3.4 Classiﬁcation
For all model variants described above, we pass output representations of the CLS tokens zL
cls-rgb and
zL
cls-spec to the same linear classiﬁer and average the pre-softmax logits.
4 Experiments
We apply MBT to the task of video classiﬁcation. In this section we ﬁrst describe the datasets used to
train and test multimodal fusion and their respective evaluation protocols (Sec. 4.1), then discuss
implementation details (Sec. 4.2). We then ablate the key design choices in our model (Sec. 4.3),
before ﬁnally comparing our model to the state of the art (Sec. 4.4).
5

--- PAGE 6 ---
4.1 Datasets and evaluation protocol
We experiment with three video classiﬁcation datasets – AudioSet [ 24], Epic-Kitchens-100 [ 14] and
VGGSound [ 12], described in more detail below. Results on two additional datasets Moments in
Time [47] and Kinetics [35] are provided in the appendix.
AudioSet [24] consists of almost 2 million 10-second video clips from YouTube, annotated with 527
classes. Like other YouTube datasets, this is a dynamic dataset (we only use the clips still available
online). This gives us 20,361 clips for the balanced train set (henceforth referred to as mini-AudioSet
or miniAS) and 18,589 clips for the test set. This test set is exactly the same as recent works we
compare to, including Perceiver [ 32]. Instead of using the 2M unbalanced training set, we train on a
(slightly more) balanced subset consisting of 500K samples (AS-500K). Details are provided in the
appendix. Because each sample has multiple labels, we train with a binary cross-entropy (BCE) loss
and report mean average precision (mAP) over all classes, following standard practice.
Epic-Kitchens 100 [14] consists of egocentric videos capturing daily kitchen activities. The dataset
consists of 90,000 variable length clips spanning 100 hours. We report results for action recognition
following standard protocol [ 14] - each action label is a combination of a verb and noun, and we
predict both using a single network with two ‘heads’, both trained with a cross-entropy loss. The
top scoring verb and action pair predicted by the network are used, and Top-1 action accuracy is the
primary metric. Actions are mainly short-term (average length is 2.6s with minimum length 0.25s).
VGGSound [12] contains almost 200K video clips of length 10s, annotated with 309 sound classes
consisting of human actions, sound-emitting objects and human-object interactions. Unlike AudioSet,
the sound source for each clip is ‘visually present’ in the video. This was ensured during dataset
creation through the use of image classiﬁers. After ﬁltering clips that are no longer available on
YouTube, we end up with 172,427 training and 14,448 test clips. We train with a standard cross-
entropy loss for classiﬁcation and report Top-1 and Top-5 classiﬁcation accuracy.
4.2 Implementation details
Our backbone architecture follows that of ViT [ 18] identically, speciﬁcally we use ViT-Base (ViT-B,
L= 12 ,NH= 12 ,d= 3072 )1initialised from ImageNet-21K [ 16], however we note that our
method is agnostic to transformer backbone. Unless otherwise specialised, we use B= 4bottleneck
tokens for all experiments with bottleneck fusion. Bottleneck tokens are initialized using a Gaussian
with mean of 0 and standard deviation of 0.02, similar to the positional embeddings in the public
ViT [ 18] code. We randomly sample clips of tseconds for training. RGB frames for all datasets
are extracted at 25 fps. For AudioSet and VGGSound we sample 8 RGB frames over the sampling
window of length twith a uniform stride of length (t25)=8. We extract 1616patches from
each frame of size 224224, giving us a total of 81414 = 1568 patches per video. For
Epic-Kitchens (because the segments are shorter), we sample 32 frames with stride 1. Audio for all
datasets is sampled at 16kHz and converted to mono channel. Similar to [ 26], we extract log mel
spectrograms with a frequency dimension of 128 computed using a 25ms Hamming window with
hop length 10ms. This gives us an input of size 128100tfortseconds of audio. Spectrogram
patches are extracted with size 1616, giving us 508 = 400 patches for 8 seconds of audio.
For images we apply the standard data augmentations used in [ 6] (random crop, ﬂip, colour jitter),
and for spectrograms we use SpecAugment [ 50] with a max time mask length of 192 frames and
max frequency mask length of 48 bins following AST [26]. We set the base learning rate to 0:5and
train for 50epochs, using Mixup [ 67] with= 0:3and stochastic depth regularisation [ 30] with
probabilityp= 0:3. All models (across datasets) are trained with a batch size of 64, synchronous
SGD with momentum of 0:9, and a cosine learning rate schedule with warmup of 2:5epochs on TPU
accelerators using the Scenic library [15].
Inference: Following standard practice, we uniformly sample multiple temporal crops from the clip
and average per-view logits to obtain the ﬁnal result. The number of test crops is set to 4.
4.3 Ablation analysis
In this section we investigate the impact of the different architectural choices in MBT. Unless
otherwise speciﬁed, we use the mini-AudioSet split for training and report results on the AudioSet
eval split. More ablations on backbone size and pretraining initalisation can be found in the appendix.
1Lis the number of transformer layers, NHis the number of self-attention heads with hidden dimension d.
6

--- PAGE 7 ---
4.3.1 Fusion strategies
We implement all the three fusion strategies described in Sec. 3.2:
(i)Vanilla self-attention – Unrestricted pairwise attention between all latent units within a layer;
(ii)Vanilla cross-attention with separate weights: Same as above, but we now have separate
weights for each modality. The latent units are updated via pairwise attention with all other latent
units from both modalities; and ﬁnally
(iii)Bottleneck fusion: Here all cross-modal attention must pass through bottleneck fusion latents.
Note that these three fusion strategies only describe attention ﬂow between tokens within a layer. For
strategies (ii) and (iii), we also conduct experiments showing the impact of restricting cross-modal
attention to layers after a ﬁxed fusion layer Lf. We investigate models with different fusion layers,
Lf= 0;2;4;6;8;10;12, and present the results in Fig. 3.2
Sharing weights for both modalities: We ﬁrst investigate the impact of sharing the encoder weights
for both modalities (strategy (i) vs (ii)). The results can be found in Fig. 7 in the appendix. When
modalities are fused at earlier layers, using separate encoders improves performance. For models
with later fusion layers, performance is similar for both models. We hence use separate modality
weights for further experiments.
Fusion layer: We then investigate the impact of varying the fusion layer Lf, for the latter two
strategies: (ii) Vanilla Cross-Attention and (iii) Bottleneck Fusion. We conduct experiments with
Lf= 0;2;4;6;8;10;12. We ﬁx the input span tto 4s and the number of bottleneck tokens Bto
4. We conduct 3 runs for each experiment and report mean and std deviation. As can be seen from
Fig. 3 (left), ‘mid fusion’ outperforms both early ( Lf= 0) and late fusion ( Lf= 12 ), with optimal
performance obtained by using fusion layer Lf= 10 for vanilla cross-attention and Lf= 8 for
bottleneck attention. This suggests that the model beneﬁts from restricting cross-modal connections to
later layers, allowing earlier layers to specialise to learning unimodal features, however still beneﬁts
from multiple layers of cross-modal information ﬂow. In appendix D , we conﬁrm that mid fusion
outperforms late fusion across a number of different datasets.
Attention bottlenecks: In Fig. 3, we also examine the effect of bottleneck attention vs vanilla
cross-attention for multimodal fusion. We ﬁnd that for all values of Lfrestricting ﬂow to bottlenecks
improves or maintains performance, with improvements more prominent at lower values of Lf. At
Lf= 10 , both perform similarly, note that at this stage we only have 3 fusion layers in the model.
Our best performing model uses attention bottlenecks with Lf= 8, and we ﬁx this for all further
experiments. We also compare the amount of computation, measured in GFLOPs, for both fusion
strategies (Fig. 3, right). Using a small number of bottleneck tokens (in our experiments B= 4) adds
negligible extra computation over a late fusion model, with computation remaining largely constant
with varying fusion layer Lf. This is in contrast to vanilla cross-fusion, which has a non-negligible
computational cost for every layer it is applied to. We note that for early fusion ( Lf= 0), bottleneck
fusion outperforms vanilla cross-attention by over 2 mAP, with less than half the computational cost.
Number of bottleneck tokens B:We experiment with B= 4;36;64;256and1024 , and ﬁnd that
performance is relatively consistent (all within 0.5 mAP). We hence ﬁx the number of tokens to
B= 4for all experiments. It is interesting that with such a small number of cross-modal connections
through only 4 hidden units ( B= 4) at each cross-modal layer, we get large performance gains
over late fusion (Fig. 3), highlighting the importance of allowing cross-modal information to ﬂow at
multiple layers of the model.
4.3.2 Input sampling and dataset size
In this section we investigate the impact of different modality sampling strategies. We also compare to
single modality baselines – the visual-only and audio-only baselines consist of a vanilla transformer
model applied to only the RGB or spectrogram patches respectively.
Sampling window size t:An advantage of our transformer based model is that we can easily input
variable length token sequences. We experiment with varying the sampling window twith the
following values t= 2;4;6and8seconds (note that all videos in AudioSet are 10s), and show results
in Fig. 43. At inference, we uniformly sample multiple windows covering the entire video. While
the number of spectrogram patches Nachanges with t, we keep the number of RGB patches Nv
2Note that Lf= 12 refers to late fusion, where logits are only aggregated after the classiﬁers, and neither
fusion strategy (ii) nor (iii) is applied, but we show results on the same plot for convenience.
3Averaged over 3 runs. Because error bars are small in the plot we also provide them in Table 6 in the
appendix.
7

--- PAGE 8 ---
0 2 4 6 8 10 12
Fusion Layer Lf384042mAP
0 2 4 6 8 10 12
Fusion Layer Lf100150200GFLOPs
Attention Bottlenecks Vanilla Cross-AttentionFigure 3: The impact of using attention bottlenecks for fusion on performance (left) and compute
(right) at different fusion layers Lfon AudioSet, using clip span t= 4andB= 4bottleneck tokens.
Attention bottlenecks improve performance at lower computational cost.
2 3 4 5 6 7 8
Video clip span (s)25303540mAP
Visual-only Audio-only Audio-visual
Figure 4: The effect of varying input clip span t
on the AudioSet test set.
0 1 2 3 4 5
Number of Training Samples 1e53035404550mAP
Visual-only Audio-only Audio-visualFigure 5: The effect of training data size on the
AudioSet test set.
ﬁxed by changing the stride of frames (to avoid running out of memory). Our results indicate that
the performance of both the audio and audio-visual fusion model increases with input span, however
the performance of the visual-only model slightly decreases (we hypothesize that this is due to the
increased ﬁxed stride, meaning fewer frames are randomly sampled during training). We ﬁx t= 8s
in all further experiments.
Synchronous vs asynchronous sampling: Given that auditory and visual events may not always be
perfected aligned in videos [ 36], we also investigate asynchronous sampling of different modalities.
Here input windows are sampled independently from the entire video clip for each modality. Results
are provided in Fig. 8 in the appendix. We ﬁnd performance to be largely robust to either case, and
so for simplicity we use synchronised sampling for all further experiments.
Modality MixUp: While applying Mixup regularization [ 67] to training, we note that there are
two different ways to apply it for multimodal inputs – the standard approach is to sample one set
of mixup weights from a Beta distribution using the parameter , and use it to generate all virtual
modality-label pairs [ 67]. We also explore a modiﬁed version which we call modality mixup , which
samples an independent weight for each modality. Modality mixup imposes stronger augmentation
than standard mixup, leading to a slight improvement (42.6 mAP to 43.9 mAP) on AudioSet.
Impact of dataset size: We show the impact of varying the number of training samples in Fig. 5,
and ﬁnd a monotonic increase with dataset size (more steeply for audio-only than visual-only).
4.4 Results
Comparison to single modality performance: We compare MBT to visual-only and audio-only
baselines on AudioSet (Table 1), Epic-Kitchens (Table 2) and VGGSound (Table 3). Note we use the
best parameters obtained via the ablations above, i.e. bottleneck fusion with t= 8,B= 4,Fl= 8
and modality mixup. For all datasets, multimodal fusion outperforms the higher-performing single
modality baseline, demonstrating the value of complementary information. The relative importance
of modalities for the classiﬁcation labels varies (audio-only has higher relative performance for
AudioSet and lower for Epic-Kitchens, while both audio and visual baselines are equally strong for
VGGSound). This is (unsurprisingly) largely a function of the dataset annotation procedure and
positions VGGSound as a uniquely suitable dataset for fusion. We also show that audio-visual fusion
8

--- PAGE 9 ---
Model Training Set A only V only A V Fusion
GBlend [63] MiniAS 29.1 22.1 37.8
GBlend [63] FullAS-2M 32.4 18.8 41.8
Attn Audio-Visual [21] FullAS-2M 38.4 25.7 46.2
Perceiver [32] FullAS-2M 38.4 25.8 44.2
MBT MiniAS 31.3 27.7 43.9
MBT AS-500K 41.5 31.3 49.6
Table 1: Comparison to SOTA on AudioSet [24]. We report mean average precision (mAP). We
outperform works that train on the full Audioset (2M samples), while we train on only 500K samples.
Model Modalities Verb Noun Action
Damen et al. [14] A 42.1 21.5 14.8
AudioSlowFast [37] yA 46.5 22.78 15.4
TSN [62] V , F 60.2 46.0 33.2
TRN [68] V , F 65.9 45.4 35.3
TBN [36] A, V , F 66.0 47.2 36.7
TSM [45] V , F 67.9 49.0 38.3
SlowFast [22] V 65.6 50.0 38.5
MBT A 44.3 22.4 13.0
MBT V 62.0 56.4 40.7
MBT A, V 64.8 58.0 43.4
Table 2: Comparison to SOTA on EpicKitchens-100 [14] . Modalities are A:Audio, V:Visual, F:
Optical ﬂow.yUses pretraining on VGGSound.
provides slight performance gains for traditionally video only datasets such as Kinetics and Moments
in Time (details provided in Appendix C ). We also examine per-class performance on the Audioset
dataset (Figures 9 and 10 in the Appendix), and ﬁnd that for the top 60 classes (ranked by overall
performance), audio-visual fusion improves performance over audio only or visual only for almost
all (57 out of 60) classes, except for ‘bagpiping’, ‘emergency vehicle’ and ‘didgeridoo’ which have
strong audio signatures. For classes such as ‘bicycle’ and ‘shufﬂing cards’ where audio signals are
weaker, fusion improves over the audio-only baseline by over 60% in absolute AP.
Comparison to state of the art: We compare MBT to previous fusion methods on AudioSet in
Table 1. We outperform all previous works on fusion (even though we only train on a quarter of the
training set – 500K samples), including the recently introduced Perceiver [ 32] which uses early fusion
followed by multiple self attention layers, and Attn Audio-Visual [ 21] which uses self-attention
fusion on top of individual modality CNNs. We compare to previous video classiﬁcation methods on
Epic-Kitchens in Table 2, and note that our model outperforms all previous works that use vision only,
as well as TBN [ 36] which uses three modalities - RGB, audio and optical ﬂow. Given VGGSound is
Model Modalities Top-1 Acc Top-5 Acc
Chen et alz[12] A 48.8 76.5
AudioSlowFastz[37] A 50.1 77.9
MBT A 52.3 78.1
MBT V 51.2 72.6
MBT A,V 64.1 85.6
Table 3: Comparison to the state of the art on VGGSound [12] . Modalities are A:Audio, V:
Visual, F:Optical ﬂow.zWe calculate metrics on our test set for a fair comparison using the scores
provided by the authors.
9

--- PAGE 10 ---
Mid FrameMid FrameVanilla FusionVanilla FusionMBTMBT
Baby cry
Piano, musicyodelingString instrumentFigure 6: Attention Maps. We compute maps of the attention from the output CLS tokens to the
RGB image input space for a vanilla self-attention model and MBT on the Audioset test set. For each
video clip, we show the original middle frame on the left with the ground truth labels overlayed at
the bottom. The attention is particularly focused on sound source regions in the video that contain
motion, eg. the ﬁngertips on the piano, the hands on the string instrument, faces of humans. The
bottlenecks in MBT further force the attention to be localised to smaller regions of the images (i.e the
mouth of the baby on the top left and the mouth of the woman singing on the bottom right).
a relatively new dataset, we compare to two existing audio-only works4(Table 3), and set the ﬁrst
audiovisual benchmark (that we are aware of) on this dataset.
Visualisation of attention maps Finally, we compute maps of the attention from the output CLS
tokens to the RGB image input space using Attention Rollout [ 1]. Results on test images for both a
vanilla fusion model and MBT trained on Audioset-mini (fusion layer Lf= 8) are shown in Figure 6.
We show the attention maps summed over all the frames in the video clip. We note that ﬁrst, the model
focuses on semantically salient regions in the video for audio classiﬁcation, particularly regions where
there is motion that creates or modiﬁes sound, i.e. the mouth of humans making sounds, ﬁngertips on
a piano, hands and instruments. This is unlike state of the art sound source localisation techniques
trained with images [ 11], which tend to highlight the entire object. We further note that the attention
maps for MBT are more localised to these regions, showing that the tight bottlenecks do force the
model to focus only on the image patches that are actually relevant for the audio classiﬁcation task
and which beneﬁt from early fusion with audio.
5 Conclusion
We propose a new transformer architecture ( MBT ) for audiovisual fusion, and explore a number of
different fusion strategies using cross-attention between latent tokens. We propose a novel strategy
to restrict cross-modal attention via a small set of fusion ‘bottlenecks’, and demonstrate that this
improves performance over vanilla cross-attention at lower computational cost, achieving state of the
art results on a number of benchmarks. Future work will involve extending MBT to other modalities
such as text and optical ﬂow.
Limitations: The fusion layer is a hyperparameter and may need to be tuned speciﬁcally for
different tasks and datasets. We also only explore fully supervised fusion, and future work will tackle
extensions to a self-supervised learning framework.
Broader impact: Multimodal fusion strategies are important for machine learning, as fusing
complementary information from different modalities can increase robustness when applied to real
world applications. We also note that transformers are in general compute-heavy, which can have
adverse environmental effects. We propose a token fusion method via bottlenecks that helps reduce
computational complexity when applying transformers for multimodal fusion. Finally, we observe
that training datasets contain biases that may render models trained on them unsuitable for certain
applications. It is thus possible that people use classiﬁcation models (intentionally or not) to make
decisions that impact different groups in society differently, and it is important to keep this in mind
4To fairly compare to these works, we obtain the scores on the full VGGSound test set from the authors, and
compute accuracy metrics on our slightly smaller test set as described in Sec. 4.1.
10

--- PAGE 11 ---
when deploying, analysing and building upon these models.
Acknowledgements: We would like to thank Joao Carreira for helpful discussions on the
Perceiver [32].
References
[1]Samira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. arXiv preprint
arXiv:2005.00928 , 2020.
[2]Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and
Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video,
audio and text. NeurIPS , 2021.
[3]Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovi ´c, Jason Ramapu-
ram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervised
multimodal versatile networks. In NeurIPS , 2020.
[4] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In ICCV , 2017.
[5] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In ECCV , 2018.
[6]Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu ˇci´c, and Cordelia
Schmid. Vivit: A video vision transformer. ICCV , 2021.
[7]Yusuf Aytar, Carl V ondrick, and Antonio Torralba. Soundnet: Learning sound representations
from unlabeled video. In NeurIPS , 2016.
[8]Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video
and image encoder for end-to-end retrieval. ICCV , 2021.
[9]Yunlong Bian, Chuang Gan, Xiao Liu, Fu Li, Xiang Long, Yandong Li, Heng Qi, Jie Zhou,
Shilei Wen, and Yuanqing Lin. Revisiting the effectiveness of off-the-shelf temporal modeling
approaches for large-scale video classiﬁcation. arXiv preprint arXiv:1708.03805 , 2017.
[10] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the
kinetics dataset. In CVPR , 2017.
[11] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew
Zisserman. Localizing visual sounds the hard way. In CVPR , 2021.
[12] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. VGGSound: A large-scale
audio-visual dataset. In ICASSP , 2020.
[13] Tsuhan Chen and Ram R Rao. Audio-visual integration in multimodal communication. Pro-
ceedings of the IEEE , 86(5):837–852, 1998.
[14] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos,
Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric
vision. arXiv preprint arXiv:2006.13256 , 2020.
[15] Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A
JAX library for computer vision research and beyond. arXiv preprint arXiv:2110.11403 , 2021.
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR , pages 248–255. Ieee, 2009.
[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. In NAACL , 2019.
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
11

--- PAGE 12 ---
[19] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim,
William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: a
speaker-independent audio-visual model for speech separation. ACM Transactions on Graphics
(TOG) , 37(4):1–11, 2018.
[20] Quanfu Fan, Chun-Fu Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More is less: Learn-
ing efﬁcient video representations by big-little network and depthwise temporal aggregation.
arXiv preprint arXiv:1912.00869 , 2019.
[21] Haytham M Fayek and Anurag Kumar. Large scale audiovisual learning of sounds with weakly
labeled data. arXiv preprint arXiv:2006.01595 , 2020.
[22] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for
video recognition. In ICCV , pages 6202–6211, 2019.
[23] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer
for video retrieval. In ECCV , volume 5. Springer, 2020.
[24] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing
Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset
for audio events. In ICASSP , pages 776–780. IEEE, 2017.
[25] Bernard Ghanem, Juan Carlos Niebles, Cees Snoek, Fabian Caba Heilbron, Humam Alwassel,
Victor Escorcia, Ranjay Krishna, Shyamal Buch, and Cuong Duc Dao. The activitynet large-
scale activity recognition challenge 2018 summary. arXiv preprint arXiv:1808.03766 , 2018.
[26] Yuan Gong, Yu-An Chung, and James Glass. AST: audio spectrogram transformer. arXiv
preprint arXiv:2104.01778 , 2021.
[27] David Harwath, Antonio Torralba, and James R Glass. Unsupervised learning of spoken
language with visual context. NeurIPS , 2017.
[28] Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and Aida Ne-
matzadeh. Decoupling the role of data, attention, and losses in multimodal transformers. arXiv
preprint arXiv:2102.00529 , 2021.
[29] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Chan-
ning Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. CNN architectures
for large-scale audio classiﬁcation. In ICASSP , pages 131–135. IEEE, 2017.
[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In ECCV , 2016.
[31] Vladimir Iashin and Esa Rahtu. Multi-modal dense video captioning. In CVPR Workshops ,
pages 958–959, 2020.
[32] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Car-
reira. Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206 ,
2021.
[33] Aren Jansen, Daniel PW Ellis, Shawn Hershey, R Channing Moore, Manoj Plakal, Ashok C
Popat, and Rif A Saurous. Coincidence, categorization, and consolidation: Learning to recognize
sounds with minimal supervision. In ICASSP , 2020.
[34] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal
and motion encoding for action recognition. In ICCV , pages 2000–2009, 2019.
[35] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human
action video dataset. arXiv preprint arXiv:1705.06950 , 2017.
[36] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-
visual temporal binding for egocentric action recognition. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 5492–5501, 2019.
12

--- PAGE 13 ---
[37] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Slow-fast auditory
streams for audio recognition. In ICASSP , pages 855–859. IEEE, 2021.
[38] Yelin Kim, Honglak Lee, and Emily Mower Provost. Deep learning for robust feature generation
in audiovisual emotion recognition. In ICASSP . IEEE, 2013.
[39] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, and Yale Song. Pa-
rameter efﬁcient multimodal transformers for video representation learning. arXiv preprint
arXiv:2012.04124 , 2020.
[40] Guang Li, Linchao Zhu, Ping Liu, and Yi Yang. Entangled transformer for image captioning.
InICCV , pages 8928–8937, 2019.
[41] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, and Hao Li. Learning
to generate diverse dance motions with transformer. arXiv preprint arXiv:2008.08171 , 2020.
[42] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A
simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 ,
2019.
[43] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Learn to dance with aist++:
Music conditioned 3d dance generation. arXiv preprint arXiv:2101.08779 , 2021.
[44] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal
excitation and aggregation for action recognition. In CVPR , pages 909–918, 2020.
[45] Ji Lin, Chuang Gan, and Song Han. Temporal shift module for efﬁcient video understanding.
2019 ieee. In ICCV , pages 7082–7092, 2019.
[46] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks. In NeurIPS , 2019.
[47] Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah Adel Bargal, Tom
Yan, Lisa Brown, Quanfu Fan, Dan Gutfreund, Carl V ondrick, et al. Moments in time dataset:
one million videos for event understanding. IEEE transactions on pattern analysis and machine
intelligence , 42(2):502–508, 2019.
[48] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng.
Multimodal deep learning. In ICML , 2011.
[49] Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisen-
sory features. In ECCV , 2018.
[50] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk,
and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech
recognition. arXiv preprint arXiv:1904.08779 , 2019.
[51] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-
language navigation. In ICCV , 2021.
[52] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal
representation with local and global diffusion. In CVPR , 2019.
[53] Dhanesh Ramachandram and Graham W Taylor. Deep multimodal learning: A survey on recent
advances and trends. IEEE Signal Processing Magazine , 34(6):96–108, 2017.
[54] Michael S Ryoo, AJ Piergiovanni, Mingxing Tan, and Anelia Angelova. Assemblenet: Searching
for multi-stream neural connectivity in video architectures. arXiv preprint arXiv:1905.13209 ,
2019.
[55] Justin Salamon and Juan Pablo Bello. Deep convolutional neural networks and data augmenta-
tion for environmental sound classiﬁcation. IEEE Signal Processing Letters , 24(3):279–283,
2017.
13

--- PAGE 14 ---
[56] Paul Hongsuck Seo, Arsha Nagrani, and Cordelia Schmid. Look before you speak: Visually
contextualized utterances. In CVPR , 2021.
[57] Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from
babies. Artiﬁcial life , 11(1-2):13–29, 2005.
[58] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations
using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743 , 2019.
[59] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A
joint model for video and language representation learning. In ICCV , 2019.
[60] Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel PW Ellis,
and John R Hershey. Into the wild with audioscope: Unsupervised audio-visual separation of
on-screen sounds. arXiv preprint arXiv:2011.01143 , 2020.
[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 ,
2017.
[62] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool.
Temporal segment networks: Towards good practices for deep action recognition. In ECCV .
Springer, 2016.
[63] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classiﬁcation
networks hard? In CVPR , pages 12695–12705, 2020.
[64] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer.
Audiovisual slowfast networks for video recognition. arXiv preprint arXiv:2001.08740 , 2020.
[65] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-
tiotemporal feature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV ,
2018.
[66] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang. Cross-modal self-attention network for
referring image segmentation. In CVPR , 2019.
[67] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
empirical risk minimization. arXiv preprint arXiv:1710.09412 , 2017.
[68] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning
in videos. In ECCV , pages 803–818, 2018.
Here we provide additional ablation results on mini-Audioset (Sec. A) as well as analyse the per-class
average precision of fusion over single modality baselines (Sec. B). We then provide results on two
additional datasets, Moments in Time and Kinetics in Sec. C and perform some preliminary transfer
learning experiments in Sec. E. Finally we provide details on the AS-500K split.
A Ablations on mini-Audioset
In this section we expand on the ablations provided in Sec. 4.3 of the main paper. Unless otherwise
speciﬁed, ablations are performed using Audioset-mini as the training set and the Audioset test set
for evaluation. For most experiments we conduct 3 runs and report mean and standard deviation.
A.1 Symmetric vs asymmetric bottleneck updates
We also experiment with an asymmetric bottleneck update. This involves replacing Eq. 8 and 9 with
the following:
[zl+1
rgbjj^ zl+1
fsn] = Transformer([ zl
rgbjjzl
fsn];rgb) (10)
[zl+1
specjjzl+1
fsn] = Transformer([ zl
specjj^ zl+1
fsn];spec) (11)
14

--- PAGE 15 ---
Here the bottleneck tokens are updated twice, ﬁrst with visual information (Equation 10), and then
with audio information (Equation 11). We also experimented with updating the bottlenecks with
audio information ﬁrst and compare both variations to the symmetric update in Table 4. We ﬁnd
performance is robust to all variations.
RGB ﬁrst Spec ﬁrst Symmetric updates
43.420.19 43.230.12 43.660.26
Table 4: Asymmetric vs symmetric bottleneck updates.
A.2 Backbone architecture
We experiment with three standard ViT [ 18] backbones, ViT-Small, ViT-Base and ViT-Large on both
Audioset-mini and VGGSound. We report results in Table 5 for audiovisual fusion with our best
MBT model. We ﬁnd that performance increases from ViT-Small to ViT-Base, but then drops for
ViT-Large. This could be due to the fact that these datasets are on the smaller side, and more data
might be required to take advantage of larger models.
Backbone AS-mini VGGSound
ViT-Small 38.2 59.0
ViT-Base 43.3 64.1
ViT-Large 42.2 61.4
Table 5: Performance with varying backbones on AS-mini and VGGSound.
A.3 The impact of weight sharing
We investigate the impact of sharing the encoder weights for both modalities (strategy (i) vs (ii))
as described in Sec. 4.3.1 . Results are provided in Fig. 7 for different fusion layers Lf. When
modalities are fused at earlier layers, using separate encoders improves performance. For models
with later fusion layers, performance is similar for both models.
A.4 Input sampling
Here we investigate asynchronous sampling of different modalities (where input windows are sampled
independently from the entire video clip for each modality) as compared to synchronous sampling.
Results are provided in Fig. 8 for different input span lengths t. Over multiple runs we ﬁnd that
performance is largely robust to either sampling choice. We hypothesise that asynchronous sampling
provides the following trade-off: while it introduces a misalignment between the two modality inputs,
slight shifts are also a good source of temporal augmentation. As the video clip span length grows,
the possible options for misalignment between inputs are less severe, while the impact of additional
augmentation is more evident.
In Table 6, we provide the results in numerical form used to create Fig. 4 . We perform 3 runs per
experiment and report mean and standard deviation. All segments in AudioSet are 10 seconds long.
Span Length t 2s 4s 6s 8s
Visual only 26.23 0.16 25.740.18 25.680.02 25.430.02
Audio only 27.10 0.54 29.910.21 30.080.21 30.550.22
Audio-Visual 37.95 0.51 40.320.20 41.510.24 42.370.44
Table 6: The effect of varying input clip span ton performance.
15

--- PAGE 16 ---
B Per class performance
We also examine per-class average precision (AP) results for our best model trained on the mini-
Audioset (note that this dataset has 527 classes). We ﬁrst show the results for the 60 top ranked
classes in Audioset (by audio-visual mAP performance) in Fig. 9. We show the per class AP using
our best fusion model (MBT), as well as the performance of audio only and visual only baselines.
Audio-visual fusion improves performance over audio only or visual only for almost all (57 out of
60) classes, except for ‘bagpiping’, ‘emergency vehicle’ and ‘didgeridoo’ which have strong audio
signatures. We then analyse the top 60 classes for which fusion has the largest improvement over
single modality performance, over audio-only (Figure 10, top) and visual-only (Figure 10, bottom).
For some classes such as ‘bicycle’ and ‘shufﬂing cards’, fusion improves over the audio-only baseline
by over 60% in absolute AP. The class that beneﬁts most from audio-visual fusion over a visual-only
baseline is ‘Whistling’ (almost 80% improvement in absolute AP).
C Additional Datasets
In this section we report results on 2 additional datasets, Moments in Time [47] and Kinetics [35].
C.1 Moments In Time
Moments In Time [ 47] consists of 800,000, 3-second clips from YouTube videos. The videos are
diverse and capture dynamic scenes involving animals, objects, people, or natural phenomena. The
videos are labelled with 330 verb classes, each associated with over 1,000 videos. We show results
for MBT compared to single modality baselines in Table 7. Our ﬁrst observation is that audio-only
performance is much lower than visual-only. This is largely a function of the annotation procedure
for the dataset, however we also note that clips are only 3 seconds long, and as shown in Fig. 4 ,
audio-only performance is heavily dependant on the span length ton Audioset, suggesting that it
may be difﬁcult to recognise audio events from shorter inputs. Our fusion model provides a further
modest 1% boost to performance over the visual-only baseline.
C.2 Kinetics
Kinetics [ 35] consists of 10-second videos sampled at 25fps from YouTube. We evaluate on both
Kinetics 400 [ 35] and a commonly used subset Kinetics-Sound [ 4], containing 400 and 36 classes
respectively. As these are dynamic datasets (videos may be removed from YouTube), we train and
test on 209,552 and 17,069 videos respectively for Kinetics and report results on 1,165 videos for
Kinetics-Sound. Results for MBT compared to single modality baselines are shown in Table 8. We
note that on the entire Kinetics test set, our fusion model outperforms the visual only baseline by
about 1% in top 1 accuracy (in line with other works [ 64] that demonstrate that audio for the large
part does not improve performance for most Kinetics classes). This gap is widened, however, for the
0 2 4 6 8 10 12
Fusion Layer Lf3839404142mAP
separate encoders shared encoders
Figure 7: The effect of sharing weights for vanilla
fusion.
2 3 4 5 6 7 8
Video clip span length t (s)3839404142mAP
synchronous asynchronousFigure 8: Asynchronous vs synchronous sam-
pling of RGB and spectrogram inputs.
16

--- PAGE 17 ---
Wind instrument
Snoring
Arrow
Brass instrument
Fireworks
Frying 
Theremin
Battle cry
Mechanical fan
Fowl
Boat
Harp
Harmonica
Banjo
Tuning fork
Sailboat
Howl
Hair dryer
Moo
Goose
Cattle
Bagpipes
Gong
Crow
Fire engine
Music
Whale vocalization
Machine gun
Blender
Emergency vehicle
Chopping 
Didgeridoo
Bicycle
Subway
Harpsichord
Owl
Ice cream truck
Police car 
Hoot
French horn
Vacuum cleaner
Sink 
Turkey
Gobble
Fire alarm
Sanding
Whistling
Railroad car
Rail transport
Siren
Train
Shofar
Singing bowl
Accordion
Artillery fire
Shuffling cards
Cash register
Change ringing 
Sewing machine
Civil defense siren00.20.40.60.81APaudio-visual visual-only audio-onlyFigure 9: Per-class average precision for the top 60 classes in Audioset ranked by mAP. Best viewed
in colour and zoomed in. Note how audio-visual fusion helps improve performance over audio only
for almost all classes. The visual only model performs well for classes that have a stronger visual
signature than audio, eg ‘bicycle’, ‘mechanical fan’, ‘boat’ and ‘arrow’.
Arrow
Truck
Domestic animals
Whip
Microwave oven
Dishes
Horse
Ratchet
Wind noise 
Clock
Hoot
Wood
Splinter
Eruption
Owl
Cash register
Wild animals
Lawn mower
Toilet flush
Waterfall
Wind
Rowboat
Roaring cats 
Artillery fire
Clickety-clack
Patter
Writing
Subway
Sawing
Zither
Bus
Mechanisms
Goose
Caw
Dog
Pulleys
Crow
Coin 
Toothbrush
Ship
Run
Whale vocalization
Vacuum cleaner
Pig
Helicopter
Boat
Blender
Gears
Rodents
Sailboat
Caterwaul
Sink 
Mechanical fan
Fire
Sewing machine
Shuffle
Clip-clop
Motorboat
Shuffling cards
Bicycle00.20.40.6APDifference between fusion and audio-only performanceCough
Dishes
Thunderstorm
Music of Africa
Doorbell
Stomach rumble
Fireworks
Trance music
Drum kit
Cutlery
Harmonica
Finger snapping
Singing bowl
Thunder
Howl
Bicycle bell
Rain on surface
Siren
Saxophone
Hair dryer
Fly
Throbbing
Tuning fork
Electronic dance music
Crumpling
Telephone dialing
Chime
Opera
Chatter
Emergency vehicle
Techno
Pink noise
Mallet percussion
Burping
Bagpipes
Wind chime
Environmental noise
Hiccup
Rain
Clicking
Train horn
Applause
Telephone bell ringing
Pour
Dial tone
Snoring
Sink 
Dubstep
Mosquito
String section
Air horn
Electronic tuner
Timpani
Car alarm
Knock
Wood block
Sanding
Angry music
Police car 
Whistling00.20.40.60.8 APDifference between fusion and visual-only performance
Figure 10: Top 60 classes that have the highest gain with fusion over a audio only (top) and visual
only (bottom) baseline. Note how fusion improves the per class AP for certain classes by over
50% over a unimodal model. As expected, the classes that beneﬁt most from visual information are
‘bicycle’ and ‘shufﬂing cards’ and the class that beneﬁts most from audio is ‘Whistling’.
Kinetics-Sound subset of the dataset (over 4%), as expected because this subset consists of classes in
Kinetics selected to have a strong audio signature [4].
D Dataset Variations for MBT vs Late Fusion
In this section we further analyse the signiﬁcance of our method across all the popular video
classiﬁcation datasets used in the paper (most ablations results are only shown for mini-Audioset
in the main paper). We note that the gap between MBT and late-fusion is highly dataset dependant
(see Table 9), with our method providing an even greater advantage for Epic-Kitchens (almost 6%
difference in Top 1 action accuracy).
17

--- PAGE 18 ---
Model Top-1 acc Top-5 acc
I3D [10] 29.5 56.1
blVNet [20] 31.4 59.3
AssembleNet-101 [54] 34.3 62.7
ViViT-Base [6] 37.3 64.2
Ours (Audio-only) 8.2 18.2
Ours (Visual-only) 36.3 59.3
MBT (A V) 37.3 61.2
Table 7: Comparison to state of the art on Moments in Time [ 47]. We report top 1 and top 5
classiﬁcation accuracy. A V:Refers to audio-visual fusion.
Model Kinetics Kinetics-Sounds
Top-1 Top-5 Top-1 Top-5
blVNet [20] 73.5 91.2 - -
STM[34] 73.7 91.6 - -
TEA [44] 76.1 92.5 - -
TS S3D-G [65] 77.2 93.0 - -
3-stream SATT [9] 77.7 93.2 - -
A VSlowFast, R101 [64] 78.8 93.6 85.0 y -
LGD-3D R101 [52] 79.4 94.4 - -
SlowFast R101-NL [22] 79.8 93.9 - -
ViViT-Base [6] 80.0 94.0 - -
Ours (Audio-only) 25.0 43.9 52.6 71.5
Ours (Visual-only) 79.4 94.0 80.7 94.9
MBT (A V) 80.8 94.6 85.0 96.8
Table 8: Comparison to state of the art on Kinetics [ 35] and Kinetics Sound [ 4]. We report top-1 and
top-5 classiﬁcation accuracy. A V:Refers to audio-visual fusion. yNote the Kinetics-Sound test set
has reduced since this work as videos have been removed from YouTube, hence this is not a direct
comparison.
E Transfer learning
We use checkpoints pretrained on VGGSound, Kinetics400 and AS-500K and ﬁnetune them on
Audioset-mini and VGGSound (note we use a ViT-B backbone for these experiments, and report
results for audiovisual fusion with our best MBT model). Results are provided in Table 10. While
Kinetics400 pretraining gives a slight 0.7% mAP boost on AS-mini, VGGSound initialisation gives a
substantial 3% mAP boost over Imagenet Initialisation. On VGGSound, AS500K pretraining gives
a more modest boost of 1.2% Top 1 Acc, while Kinetics pretraining does not help (expected as
VGGSound is a larger dataset).
F AS-500K details
The original unbalanced AudioSet training set consists of almost 2M samples, and is extremely
unbalanced with most samples either labelled as speech or music. To improve training efﬁciency,
we create a slightly more balanced subset called AudioSet-500K. The main issue is that AudioSet
is multilabel, and this makes balancing difﬁcult. We create AS-500K by greedily restricting the
maximum number of samples per class to be 200K. Given the distribution of labels, this gives us a
total size of 508,994 samples. We provide the full histogram of labels in Fig. 11 (note the number of
samples is on a log10scale).
18

--- PAGE 19 ---
Dataset mini-Audioset Epic-Kitchens VGGSound Moments in Time Kinetics
Late Fusion 41.80 37.90 63.3 36.48 77.0
MBT 43.92 43.40 64.1 37.26 80.8
Table 9: MBT vs late Fusion for different datasets. For each dataset we report the widely used primary
metric, i.e. Audioset: mAP, Epic-Kitchens: Top-1 action accuracy, VGGSound, Moments in Time
and Kinetics: Top-1 classiﬁcation accuracy.
Initialisation Checkpoint AS-mini VGGSound
ImageNet init. 43.3 64.1
VGGSound init. 46.6 N/A
K400 init. 44.0 64.0
AS-500K init. N/A 65.3
Table 10: Transfer learning on Audioset-mini and VGGSound.
Classes in Audioset0246810Log (# samples)
Figure 11: Class label histogram in the AudioSet-500K split.
19
