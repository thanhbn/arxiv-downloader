# 2307.10802.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2307.10802.pdf
# Kích thước tệp: 1395430 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Meta-Transformer: Một Khung Thống Nhất cho
Học Đa Phương Thức
Yiyuan Zhang1,2∗Kaixiong Gong1,2∗Kaipeng Zhang2†
Hongsheng Li1Yu Qiao2Wanli Ouyang2Xiangyu Yue1†‡
1Phòng thí nghiệm Đa phương tiện, Đại học Trung văn Hồng Kông2Phòng thí nghiệm AI Thượng Hải
yiyuanzhang.ai@gmail.com, kaixionggong@gmail.com, xyyue@ie.cuhk.edu.hk
https://kxgong.github.io/meta_transformer/
Văn bản Ngôn ngữ Tự nhiên
Thị giác 3D
Đám mây Điểm
Tia X
Ứng dụng Y tế
Đồ thị
Meta -Transformer
Phân tử
Video Không gian-Thời gian Hồng ngoại
Ban đêm/Nhiệt
Hình 1: Học Đa Phương Thức Thống Nhất. Meta-Transformer sử dụng cùng một backbone để mã hóa
ngôn ngữ tự nhiên, hình ảnh, đám mây điểm, âm thanh, video, hồng ngoại, siêu phổ, tia X, chuỗi thời gian, dạng bảng,
Đơn vị Đo Quán tính (IMU), và dữ liệu đồ thị. Nó tiết lộ tiềm năng của kiến trúc transformer cho
trí tuệ đa phương thức thống nhất.

Tóm tắt
Học đa phương thức nhằm xây dựng các mô hình có thể xử lý và liên kết thông tin
từ nhiều phương thức. Mặc dù đã có nhiều năm phát triển trong lĩnh vực này, việc thiết kế một mạng thống nhất để xử lý các phương thức khác nhau
(ví dụ: ngôn ngữ tự nhiên, hình ảnh 2D, đám mây điểm 3D, âm thanh, video, chuỗi thời gian,
dữ liệu dạng bảng) vẫn còn là thách thức do khoảng cách vốn có giữa chúng. Trong công trình này, chúng tôi đề xuất
một khung, có tên Meta-Transformer, tận dụng một bộ mã hóa đông lạnh để thực
∗Đóng góp ngang nhau
†Tác giả liên hệ
‡Trưởng dự án
Bản thảo. Đang được xem xét.arXiv:2307.10802v1 [cs.CV] 20 Jul 2023

--- TRANG 2 ---
hiện nhận thức đa phương thức mà không cần bất kỳ dữ liệu huấn luyện đa phương thức được ghép đôi nào. Trong
Meta-Transformer, dữ liệu đầu vào thô từ các phương thức khác nhau được ánh xạ vào
một không gian token chung, cho phép một bộ mã hóa tiếp theo với các tham số đông lạnh
trích xuất các đặc trưng ngữ nghĩa cao cấp của dữ liệu đầu vào. Bao gồm ba thành phần chính:
một tokenizer dữ liệu thống nhất, một bộ mã hóa chia sẻ phương thức, và các đầu cụ thể cho nhiệm vụ
cho các nhiệm vụ downstream, Meta-Transformer là khung đầu tiên thực hiện
học thống nhất trên 12 phương thức với dữ liệu không được ghép đôi. Các thí nghiệm trên
các benchmark khác nhau cho thấy Meta-Transformer có thể xử lý một loạt rộng các nhiệm vụ
bao gồm nhận thức cơ bản (văn bản, hình ảnh, đám mây điểm, âm thanh, video), ứng dụng thực tế
(tia X, hồng ngoại, siêu phổ, và IMU), và khai thác dữ liệu (đồ thị, dạng bảng, và chuỗi thời gian).
Meta-Transformer chỉ ra một tương lai hứa hẹn cho việc phát triển trí tuệ đa phương thức thống nhất với transformer.
Mã nguồn sẽ có sẵn tại https://github.com/invictus717/MetaTransformer.

1 Giới thiệu
Bộ não con người, được coi là nguồn cảm hứng cho các mô hình mạng nơ-ron, xử lý
thông tin từ các đầu vào cảm giác khác nhau, ví dụ như tín hiệu thị giác, thính giác và xúc giác, đồng thời.
Hơn nữa, kiến thức từ một nguồn có thể có lợi cho việc hiểu nguồn khác. Tuy nhiên, trong
học sâu, việc thiết kế một mạng thống nhất có khả năng xử lý một loạt rộng các định dạng dữ liệu là một
nhiệm vụ không tầm thường do khoảng cách phương thức đáng kể [1–3].

Mỗi phương thức dữ liệu có các mẫu dữ liệu độc đáo, điều này làm cho việc thích ứng các mô hình được huấn luyện
trên một phương thức sang phương thức khác trở nên khó khăn. Ví dụ, hình ảnh thể hiện mức độ dư thừa thông tin cao
do các pixel được đóng gói dày đặc, điều này không xảy ra với ngôn ngữ tự nhiên [4]. Đám mây điểm, mặt khác,
có phân bố thưa thớt trong không gian 3D, làm cho chúng dễ bị ảnh hưởng bởi nhiễu và
khó biểu diễn [5]. Phổ đồ âm thanh là các mẫu dữ liệu thay đổi theo thời gian và không dừng
bao gồm các kết hợp sóng trên các miền tần số [6]. Dữ liệu video chứa một chuỗi
khung hình ảnh, mang lại cho nó khả năng độc đáo để nắm bắt cả thông tin không gian và động lực thời gian [7].
Dữ liệu đồ thị biểu diễn các thực thể như nút và mối quan hệ như cạnh trong một đồ thị, mô hình hóa
các mối quan hệ phức tạp, nhiều-đến-nhiều giữa các thực thể [8]. Do những khác biệt đáng kể
vốn có của các phương thức dữ liệu khác nhau, việc sử dụng các kiến trúc mạng ribiệt biệt để
mã hóa từng phương thức riêng lẻ là thông lệ phổ biến. Ví dụ, Point Transformer [9] tận dụng
sự chú ý vị trí ở cấp độ vector để trích xuất thông tin cấu trúc từ tọa độ 3D, nhưng nó không thể
mã hóa một hình ảnh, một đoạn văn ngôn ngữ tự nhiên, hoặc một lát phổ đồ âm thanh. Do đó, việc thiết kế
một khung thống nhất có khả năng sử dụng không gian tham số chia sẻ phương thức để mã hóa nhiều phương thức dữ liệu
vẫn là một thách thức đáng kể. Gần đây, sự phát triển của các khung thống nhất như VLMO [2], OFA [10],
và BEiT-3 [3] đã cải thiện khả năng của mạng cho hiểu biết đa phương thức, thông qua
tiền huấn luyện đa phương thức quy mô lớn trên dữ liệu được ghép đôi [3,10,2], nhưng chúng tập trung hơn vào thị giác và
ngôn ngữ, và không thể chia sẻ toàn bộ bộ mã hóa trên các phương thức

Kiến trúc transformer và cơ chế chú ý, được đề xuất bởi Vaswani và cộng sự vào năm 2017 [11]
cho xử lý ngôn ngữ tự nhiên (NLP), đã tạo ra sự khác biệt đáng kể trong học sâu [11–16].
Những tiến bộ này đã đóng vai trò quan trọng trong việc tăng cường nhận thức trên các phương thức khác nhau
như thị giác 2D (bao gồm ViT [17,18] và Swin Transformer [19]), thị giác 3D (như
Point Transformer [9] và Point-ViT [20,21]), và xử lý tín hiệu âm thanh (AST [6]), v.v.
Những công trình này đã chứng minh tính linh hoạt của các kiến trúc dựa trên transformer, truyền cảm hứng cho các nhà nghiên cứu
khám phá liệu có thể phát triển các mô hình nền tảng có khả năng thống nhất nhiều phương thức,
cuối cùng đạt được nhận thức ở cấp độ con người trên tất cả các phương thức.

Bảng 1: So sánh giữa Meta-Transformer và các công trình liên quan về các nhiệm vụ nhận thức.
Phương pháp Phương thức Chia sẻ Tham số Dữ liệu Không ghép đôi
Transformer [11] ✘ ✘
ViT [13], Swin Transformer [19], MAE [4] ✘ ✘
Point Transformer[9], PCT [22], Point ViT [21] ✘ ✘
AST [6], SSAST [23] ✘ ✘
CLIP [24], Flamingo [25], VLMO [2], OFA [10] ✘ ✘
BEiT-3 [3] Một số Lớp ✘
ImageBind [26] ✘ ✘
Meta-Transformer [của chúng tôi] Toàn bộ Backbone ✔

--- TRANG 3 ---
Trong bài báo này, chúng tôi khám phá tiềm năng của kiến trúc transformer để xử lý 12 phương thức bao gồm
hình ảnh, ngôn ngữ tự nhiên, đám mây điểm, phổ đồ âm thanh, video, hồng ngoại, siêu phổ, tia X,
IMU, dạng bảng, đồ thị, và dữ liệu chuỗi thời gian, như được hiển thị trong Hình 1. Chúng tôi thảo luận về quá trình học
với transformer cho từng phương thức và giải quyết các thách thức liên quan đến việc thống nhất chúng thành
một khung duy nhất. Do đó, chúng tôi đề xuất một khung thống nhất mới có tên Meta-Transformer
cho học đa phương thức. Meta-Transformer là khung đầu tiên đồng thời mã hóa
dữ liệu từ hàng chục phương thức sử dụng cùng một bộ tham số, cho phép một cách tiếp cận
gắn kết hơn cho học đa phương thức (như được hiển thị trong Bảng 1). Meta-Transformer kết hợp ba thành phần
đơn giản và hiệu quả: một chuyên gia phương thức (§ 3.2) cho tokenization dữ liệu-thành-chuỗi, một
bộ mã hóa chia sẻ phương thức (§ 3.3) để trích xuất biểu diễn trên các phương thức, và các đầu cụ thể nhiệm vụ
cho các nhiệm vụ downstream. Cụ thể, Meta-Transformer đầu tiên chuyển đổi dữ liệu đa phương thức thành các chuỗi token
chia sẻ một không gian manifold chung. Sau đó, một bộ mã hóa chia sẻ phương thức với các tham số đông lạnh
trích xuất biểu diễn, được điều chỉnh thêm cho các nhiệm vụ cá nhân bằng cách cập nhật
các tham số của các đầu nhiệm vụ downstream và các tokenizer nhẹ mà thôi. Cuối cùng, các biểu diễn
cụ thể nhiệm vụ và chung phương thức có thể được học hiệu quả bởi khung đơn giản này.

Chúng tôi tiến hành các thí nghiệm rộng rãi trên các benchmark khác nhau của 12 phương thức. Bằng cách sử dụng hình ảnh
của bộ dữ liệu LAION-2B [24] chỉ để tiền huấn luyện, Meta-Transformer chứng minh hiệu suất đáng chú ý
trong việc xử lý dữ liệu từ nhiều phương thức, đạt được kết quả vượt trội nhất quán so với
các phương pháp tiên tiến trong các nhiệm vụ học đa phương thức khác nhau. Cài đặt thí nghiệm chi tiết hơn
có thể được tìm thấy trong § D.

Tóm lại, đóng góp của chúng tôi có thể được tóm tắt như sau:
•Đối với nghiên cứu đa phương thức, chúng tôi đề xuất một khung mới, Meta-Transformer, cho phép
một bộ mã hóa thống nhất đồng thời trích xuất biểu diễn từ nhiều phương thức với
cùng một bộ tham số.
•Đối với thiết kế mạng đa phương thức, chúng tôi kiểm tra toàn diện các chức năng của các thành phần transformer
như embedding, tokenization, và bộ mã hóa trong việc xử lý các phương thức khác nhau.
Meta-Transformer cung cấp những hiểu biết có giá trị và khơi dậy một hướng mới hứa hẹn trong
việc phát triển một khung bất khả tri phương thức có khả năng thống nhất tất cả các phương thức.
•Về mặt thí nghiệm, Meta-Transformer đạt được hiệu suất xuất sắc trên các bộ dữ liệu khác nhau
liên quan đến 12 phương thức, điều này xác nhận tiềm năng tiếp theo của Meta-Transformer cho
học đa phương thức thống nhất.

2 Công trình Liên quan

2.1 Nhận thức Đơn Phương thức
Sự phát triển của các mạng nơ-ron khác nhau tạo điều kiện cho nhận thức của trí tuệ máy [27–
29, 11].

Perceptron Đa Lớp cho nhận dạng mẫu. Ban đầu, máy vector hỗ trợ (SVM)
và perceptron đa lớp (MLP) được áp dụng cho văn bản [30], hình ảnh [31], đám mây điểm [32], và âm thanh [33]
phân loại. Những công trình sáng tạo này xứng đáng với tính khả thi của việc giới thiệu AI vào nhận dạng mẫu.

Mạng Nơ-ron Hồi quy & Tích chập. Mạng Hopfield [34] là dạng gốc của
mạng hồi quy, sau đó LSTM [35] và GRU [36] tiếp tục khám phá những ưu điểm của RNN trong
mô hình hóa chuỗi và ứng dụng trong các nhiệm vụ NLP [37–39], cũng được áp dụng rộng rãi trong tổng hợp âm thanh [40].
Trong khi đó, thành công của CNN bao gồm LeNet [41], AlexNet [42], VGG [43],
GoogleNet [44] và ResNet [29] trong nhận dạng hình ảnh đã thúc đẩy mạnh mẽ việc áp dụng CNN
trong các lĩnh vực khác như phân loại văn bản [45,46], hiểu biết đám mây điểm [47–49], và phân loại giọng nói [50].

Transformer. Gần đây, kiến trúc transformer [11] đã được áp dụng trong các nhiệm vụ khác nhau như
hiểu biết văn bản [51] và tạo sinh [52] trong NLP, phân loại [13], phát hiện [53] và phân đoạn [15]
trong hình ảnh, hiểu biết đám mây điểm [22, 9], và nhận dạng âm thanh [6, 23].

Tuy nhiên, tương tự như các ứng dụng của CNN và RNN, những mạng này được sửa đổi theo
các thuộc tính riêng biệt của các phương thức. Không có kiến trúc chung cho học bất khả tri phương thức. Quan trọng hơn,
thông tin từ các phương thức khác nhau có thể bổ sung [54–56], việc

--- TRANG 4 ---
thiết kế một khung có thể mã hóa dữ liệu từ các phương thức khác nhau và kết nối những biểu diễn phức tạp này
thông qua một không gian tham số chia sẻ là rất quan trọng.

2.2 Nhận thức Đa Phương thức dựa trên Transformer
Những ưu điểm của transformer cho nhận thức là trường tiếp nhận toàn cục và mô hình hóa độ tương tự,
điều này nổi bật tạo điều kiện cho sự phát triển của nhận thức đa phương thức. MCAN [57] đề xuất
các mạng đồng chú ý mô-đun sâu giữa thị giác và ngôn ngữ, thực hiện căn chỉnh chéo phương thức
bằng cách tối đa hóa chú ý chéo một cách súc tích. Sau đó nó trở thành một sự đồng thuận [2,1,10,3]
sử dụng cơ chế chú ý chéo để kết nối các phương thức khác nhau. Với thành công của paradigm tiền huấn luyện-
tinh chỉnh, nhiều công trình tập trung vào cách căn chỉnh hiệu quả các biểu diễn
được trích xuất trên các phương thức bằng tiền huấn luyện. VL-BERT [58] tiên phong trong biểu diễn căn chỉnh phương thức
cho hiểu biết thị giác-ngôn ngữ chung với paradigm MLM. Sau đó Oscar [59] mô tả
ngữ nghĩa đối tượng trong cả nội dung thị giác và văn bản. Các khung như Vinvl [60], Simvlm [1],
VLMO [2], ALBEF [61], và Florence [62] tiếp tục khám phá những ưu điểm của biểu diễn chung
trên các phương thức thị giác-ngôn ngữ về tính nhất quán ngữ nghĩa.

Các mô hình đa phương thức cũng được sử dụng cho học few-shot [25], học chuỗi-thành-chuỗi [10],
học tương phản [63]. BEiT-v3 [3] đề xuất coi hình ảnh như một ngôn ngữ nước ngoài với
quá trình che-và-tái-tạo chéo phương thức chi tiết hơn, chia sẻ một phần tham số. Và MoMo [64]
tiếp tục khám phá chiến lược huấn luyện và các hàm mục tiêu trong khi sử dụng cùng bộ mã hóa cho hình ảnh
và văn bản.

Mặc dù có những tiến bộ này, vẫn còn những rào cản đáng kể đối với việc thiết kế các mạng đa phương thức thống nhất
do sự khác biệt giữa các phương thức. Ngoài ra, hầu hết nghiên cứu trong lĩnh vực này tập trung vào
các nhiệm vụ thị giác và ngôn ngữ, và có thể không đóng góp trực tiếp cho các thách thức như hiểu biết đám mây điểm 3D,
nhận dạng âm thanh, hoặc các phương thức khác. Mô hình Flamingo [25] đại diện cho một learner few-shot mạnh mẽ,
nhưng khả năng chuyển giao của nó sang đám mây điểm bị hạn chế, và việc tận dụng kiến thức có sẵn
từ một phương thức để có lợi cho các phương thức khác vẫn là một thách thức. Nói cách khác, các phương pháp đa phương thức hiện tại
có khả năng mở rộng hạn chế trên nhiều phương thức hơn, mặc dù chúng đã tốn chi phí huấn luyện đắt đỏ.
Việc giải quyết những khác biệt này phụ thuộc vào việc kết nối các phương thức khác nhau bằng cùng một bộ
tham số, tương tự như cách một cây cầu kết nối nhiều bờ sông.

3 Meta-Transformer

Trong phần này, chúng tôi mô tả chi tiết khung được đề xuất, Meta-Transformer. Meta-Transformer
thống nhất nhiều đường ống xử lý dữ liệu từ các phương thức khác nhau và hoàn thành việc mã hóa văn bản,
hình ảnh, đám mây điểm, âm thanh, và 8 phương thức khác với một bộ mã hóa chia sẻ. Để đạt được điều này,
Meta-Transformer bao gồm một tokenizer dữ liệu-thành-chuỗi để chiếu dữ liệu vào một không gian embedding
chia sẻ, một bộ mã hóa bất khả tri phương thức để mã hóa embedding của các phương thức khác nhau, và các đầu
cụ thể nhiệm vụ để thực hiện dự đoán downstream, như được hiển thị trong Hình 2.

3.1 Khái niệm Cơ bản
Chính thức, chúng tôi ký hiệu không gian đầu vào của n phương thức là {X1,X2,···,Xn}, trong khi {Y1,Y2,···,Yn}
là các không gian nhãn tương ứng. Ngoài ra, chúng tôi giả định tồn tại một không gian tham số hiệu quả
Θi cho mỗi phương thức, trong đó bất kỳ tham số θi∈Θi nào có thể được sử dụng để xử lý dữ liệu xi∈ Xi từ
phương thức đó. Chúng tôi nói rằng bản chất của Meta-Transformer là tìm một θ∗ chia sẻ thỏa mãn:
θ∗∈Θ1∩Θ2∩Θ3∩ ··· Θn, (1)
với giả thuyết:
Θ1∩Θ2∩Θ3∩ ··· Θn̸=∅. (2)
Các mạng nơ-ron đa phương thức có thể được công thức hóa như một hàm ánh xạ thống nhất F:x∈ X →
ˆy∈ Y, trong đó x là dữ liệu đầu vào đến từ bất kỳ phương thức nào {X1,X2,···,Xn} và ˆy biểu thị
dự đoán của mạng. Hãy ký hiệu y là nhãn sự thật, đường ống đa phương thức có thể được
công thức hóa như:
ˆy=F(x;θ∗), θ∗= arg min
x∈X[L(ˆy, y)].(3)

--- TRANG 5 ---
Không gian Token Chia sẻ
Đám mây Điểm
Hình ảnh"Câu trả lời 
đang bay trong gió."
Ngôn ngữ Tự nhiên
Phổ đồ Âm thanh…L1*1
… 0*1 L2
0*1 L3…
0*1 L4…Tokenizer Dữ liệu-thành-Chuỗi0
Word Piece
Patch Hình ảnh
Kề cận Khung xương
Phổ đồ
Phân đoạn Phát hiện
Phân đoạn Cảnh Phân loại
Phân loại
Phân loại Giọng nói
"câu có cùng 
ngữ nghĩa không?""cảm xúc tích cực 
hay tiêu cực?""xác định
phát biểu được
hàm ý."
Paraphrase Cảm xúc Suy luận
Mô hình Đa Phương thức Thống nhất 
 Phân đoạn Phần
Tham số Đông lạnh
Tham số Có thể Huấn luyện
Hình 2: Meta-Transformer bao gồm tokenization dữ liệu-thành-chuỗi, mã hóa đặc trưng thống nhất, và
học nhiệm vụ downstream. Khung được minh họa với văn bản, hình ảnh, đám mây điểm, và âm thanh.

3.2 Tokenization Dữ liệu-thành-Chuỗi
Chúng tôi đề xuất một sơ đồ meta-tokenization mới được thiết kế để chuyển đổi dữ liệu trên các phương thức khác nhau
thành embedding token, tất cả trong một không gian manifold chia sẻ. Cách tiếp cận này sau đó được áp dụng cho
tokenization, có tính đến các đặc điểm thực tế của phương thức, như được minh họa trong Hình 3.
Chúng tôi lấy văn bản, hình ảnh, đám mây điểm, và âm thanh làm ví dụ. Chi tiết hơn có thể được tìm thấy trong tài liệu bổ sung.
Cụ thể, chúng tôi sử dụng xT,xI,xP, và xA để ký hiệu một mẫu dữ liệu của văn bản, hình ảnh, đám mây điểm,
và phổ đồ âm thanh.

Ngôn ngữ Tự nhiên. Theo thực hành phổ biến [51,65], chúng tôi sử dụng embedding WordPiece [66]
với từ vựng 30.000 token. WordPiece phân đoạn các từ gốc thành từ phụ. Ví dụ,
câu gốc: "The supermarket is hosting a sale", có thể được chuyển đổi bởi WordPiece thành: "_The
_super market _is _host ing _a _sale".

Trong trường hợp này, từ "supermarket" được chia thành hai từ phụ "_super" và "market" và từ "hosting"
được chia thành "_host" và "ing", trong khi các từ còn lại không thay đổi và vẫn là đơn vị đơn.
Phía trước ký tự đầu tiên của mỗi từ gốc sẽ được thêm một ký tự đặc biệt "_", chỉ ra sự bắt đầu
của một từ tự nhiên. Mỗi từ phụ tương ứng với một token duy nhất trong từ vựng, sau đó được chiếu
vào một không gian đặc trưng chiều cao với các lớp embedding từ. Kết quả là, mỗi văn bản đầu vào được chuyển đổi
thành một tập hợp embedding token x∈Rn×D, trong đó n là số lượng token và D là chiều của embedding.

Hình ảnh. Để phù hợp với hình ảnh 2D, chúng tôi định hình lại hình ảnh x∈RH×W×C thành một chuỗi
các patch 2D được làm phẳng xp∈RNs×(S2·C), trong đó (H, W) đại diện cho độ phân giải hình ảnh gốc, C
biểu thị số lượng kênh; S là kích thước patch, và Ns= (HW/S2) là số lượng patch kết quả.
Sau đó, một lớp chiếu được sử dụng để chiếu chiều embedding thành D:
xI∈RC×H×W→x′I∈RNs×(S2·C)→x′′I∈RNs×D. (4)
Lưu ý rằng chúng tôi sử dụng cùng một thao tác cho hình ảnh hồng ngoại nhưng chiếu tuyến tính cho hình ảnh siêu phổ.
Ngoài ra, chúng tôi đơn giản thay thế các lớp tích chập 2D bằng tích chập 3D cho nhận dạng video.
Chi tiết hơn có thể được tìm thấy trong B.1 và B.3.

Đám mây Điểm. Để học các mẫu 3D với transformer, chúng tôi chuyển đổi đám mây điểm từ không gian đầu vào thô
sang không gian embedding token. X={xi}Pi=1 ký hiệu một đám mây điểm của P điểm, trong đó xi= (pi,fi),
pi∈R3 đại diện cho tọa độ 3D, và fi∈Rc là đặc trưng của điểm thứ i. Nói chung, fi chứa
các gợi ý thị giác như màu sắc, góc nhìn, pháp tuyến, v.v. Chúng tôi sử dụng thao tác Lấy mẫu Điểm Xa nhất (FPS)
để lấy mẫu một khung xương đại diện của đám mây điểm gốc với tỉ lệ lấy mẫu cố định
(1/4). Sau đó chúng tôi sử dụng K-Nearest Neighbor (KNN) để nhóm các điểm lân cận. Dựa trên các tập hợp nhóm
chứa tiên nghiệm hình học cục bộ, chúng tôi xây dựng ma trận kề với các điểm trung tâm của các tập con nhóm
để tiếp tục khám phá thông tin cấu trúc toàn diện của các đối tượng 3D và cảnh 3D.

--- TRANG 6 ---
Phân tích
Conv 1×1
Làm phẳng
(b) Tokenization Văn bản
TE Câu
ChiếuTừ phụ
C
(a) Sơ đồ Meta
x
xE Chuyển đổi Tích chập Nhóm
Dữ liệu Cục bộ
Ngữ nghĩa Cục bộ Patchify
Conv S×S
Làm phẳng
(c) Tokenization Hình ảnh
C H W
Ix
IE
C S S
C H W
Patch
Patch FPS & KNN
Làm phẳng
(d) Tokenization Điểm
(3 )Pc
px+
PE Tập con
(3 )4Pc+
Kề cận Conv 1×1
(3 )c S S+  
Patchify
Làm phẳng
(e) Tokenization Âm thanh
TF
Ax
AE
1TF
Patch
Conv S×S
Phổ
1SS
Hình 3: Minh họa Tokenization Dữ liệu-thành-Chuỗi 3.2. Chúng tôi đề xuất sơ đồ meta trong (a)
chứa quá trình nhóm, tích chập, và chuyển đổi. Sau đó (b)-(e) đại diện cho các khối xây dựng
được áp dụng với sơ đồ meta của chúng tôi trên văn bản, hình ảnh, đám mây điểm, và phổ đồ âm thanh.

Cuối cùng, chúng tôi tổng hợp các biểu diễn cấu trúc từ K tập con. Chúng tôi thu được embedding điểm như:
xP∈RP×(3+c)→x′P∈RP4×D2→x′′P∈RP16×D. (5)

Phổ đồ Âm thanh. Ban đầu, chúng tôi tiền xử lý sóng âm thanh với thời lượng t giây
với bộ lọc log Mel [67]. Sau đó chúng tôi sử dụng cửa sổ Hamming với bước nhảy ts trên
tần số fs để chia sóng gốc thành l= (t/ts) khoảng thời gian và tiếp tục chuyển đổi sóng gốc
thành filterbank l-chiều.

Tiếp theo, chúng tôi chia phổ đồ thành các patch từ chiều thời gian và tần số với
cùng kích thước patch là S. Khác với patch hình ảnh, patch âm thanh chồng lên nhau trên phổ đồ.
Theo AST [6], chúng tôi cũng chọn chia toàn bộ phổ đồ thành Ns= 12[(100 t−16)/10]
patch bằng tích chập S×S, sau đó chúng tôi làm phẳng các patch thành chuỗi token. Cuối cùng, chúng tôi tóm tắt
quá trình:
xA∈RT×F→x′A∈RNs×S×S→x′′A∈R(Ns·D/S2)×D, (6)
trong đó T và F biểu thị chiều thời gian và tần số.

3.3 Bộ Mã hóa Thống nhất
Sau khi chuyển đổi các đầu vào thô thành không gian embedding token, chúng tôi tận dụng một bộ mã hóa transformer thống nhất
với các tham số đông lạnh để mã hóa các chuỗi embedding token từ các phương thức khác nhau.

Tiền huấn luyện. Chúng tôi sử dụng ViT [13] làm mạng backbone và tiền huấn luyện nó trên bộ dữ liệu LAION-2B
với học tương phản, điều này củng cố khả năng mã hóa token chung. Sau tiền huấn luyện,
chúng tôi đông lạnh các tham số của mạng backbone. Ngoài ra, đối với hiểu biết văn bản, chúng tôi sử dụng
tokenizer văn bản được tiền huấn luyện của CLIP [24] để phân đoạn câu thành từ phụ và chuyển đổi từ phụ
thành embedding từ.

Học Bất khả tri Phương thức. Theo thực hành phổ biến [51,13], chúng tôi thêm một token có thể học
xCLS vào đầu chuỗi embedding token, và trạng thái ẩn cuối cùng của token xCLS (z0L) phục vụ như
biểu diễn tóm tắt của chuỗi đầu vào, thường được sử dụng để thực hiện nhận dạng.
Để tăng cường thông tin vị trí, chúng tôi kết hợp embedding vị trí vào embedding token.
Nhớ lại rằng chúng tôi tokenize dữ liệu đầu vào thành embedding 1D, do đó, chúng tôi chọn embedding vị trí 1D
có thể học tiêu chuẩn. Ngoài ra, chúng tôi không quan sát thấy cải thiện hiệu suất đáng kể khi sử dụng
embedding vị trí nhận biết 2D phức tạp hơn trên nhận dạng hình ảnh. Chúng tôi đơn giản kết hợp
embedding vị trí và embedding nội dung với phép cộng theo từng phần tử, và các chuỗi embedding
kết quả sau đó được đưa vào bộ mã hóa.

Bộ mã hóa transformer với độ sâu L bao gồm nhiều lớp tự-chú-ý đa đầu (MSA) xếp chồng
và các khối MLP. Embedding token đầu vào được đưa vào lớp MSA trước, sau đó
một khối MLP. Sau đó đầu ra của khối MLP thứ (ℓ−1) phục vụ như đầu vào của lớp MSA thứ ℓ.
Chuẩn hóa Lớp (LN) được thêm trước mỗi lớp và kết nối dư được áp dụng sau
mỗi lớp. MLP chứa hai lớp FC tuyến tính cùng với kích hoạt phi tuyến GELU. Công thức
của transformer là:
z0= [xCLS;Ex1;Ex2;···;Exn] +Epos, E∈Rn×D,Epos∈R(n+1)×D(7)
z′ℓ= MSA( LN(zℓ−1)) +zℓ−1, ℓ = 1. . . L (8)
zℓ= MLP( LN(z′ℓ)) +z′ℓ, ℓ = 1. . . L (9)
y=LN(z0L) (10)
trong đó Ex biểu thị embedding token từ tokenizer được đề xuất và n biểu thị số lượng
token. Chúng tôi tăng cường embedding patch và embedding có thể học với embedding vị trí Epos.

3.4 Đầu Cụ thể Nhiệm vụ
Sau khi thu được biểu diễn học, chúng tôi đưa biểu diễn vào các đầu cụ thể nhiệm vụ h(·;θh),
chủ yếu bao gồm MLP và thay đổi từ phương thức và nhiệm vụ. Mục tiêu học của
Meta-Transformer có thể được tóm tắt như:
ˆy=F(x;θ∗) =h◦g◦f(x), θ∗= arg min
θL(ˆy, y), (11)
trong đó f(·),g(·), và h(·) biểu thị hàm của tokenizer, backbone, và đầu, tương ứng.

4 Thí nghiệm

Trong phần này, chúng tôi thực hiện thí nghiệm trên từng phương thức trong số 12 phương thức. Chúng tôi chứng minh tiềm năng
của Meta-Transformer cho nhận thức đa phương thức. Tóm tắt thiết kế thí nghiệm của chúng tôi được hiển thị trong
Bảng 2 và chi tiết thí nghiệm hơn có thể được tìm thấy trong § C.1.

4.1 Thiết lập Thí nghiệm
Hiểu biết văn bản. Để đánh giá hiểu biết văn bản, chúng tôi sử dụng benchmark Đánh giá Hiểu biết Ngôn ngữ Chung (GLUE) [68]
bao gồm một số bộ dữ liệu khác nhau, bao quát một loạt rộng các nhiệm vụ hiểu biết ngôn ngữ tự nhiên.

Hiểu biết hình ảnh. 1) Phân loại: chúng tôi tiến hành thí nghiệm trên ImageNet-1K [69] chứa
khoảng 1.3 triệu hình ảnh với 1000 danh mục. Theo thực hành phổ biến [70,
19,71], các mô hình quy mô cơ sở được huấn luyện trong 300 epoch, trong khi các mô hình lớn được tiền huấn luyện trên ImageNet-
22K (14.2 triệu hình ảnh) trong 90 epoch và tinh chỉnh trên ImageNet-1K trong 20 epoch nữa. 2)
Phát hiện Đối tượng: chúng tôi tiến hành thí nghiệm trên bộ dữ liệu MS COCO [72] sử dụng Mask R-CNN [73]
làm detector và huấn luyện mỗi mô hình trong 12 epoch. 3) Phân đoạn Ngữ nghĩa: chúng tôi huấn luyện
đầu phân đoạn UperNet [74] trên ADE20K [75] trong 160k iteration, cung cấp so sánh công bằng
với các backbone dựa trên CNN và transformer trước đây.

Hiểu biết dữ liệu Hồng ngoại, Tia X, và Siêu phổ. Chúng tôi tiến hành thí nghiệm trên
nhận dạng hình ảnh hồng ngoại, quét tia X, và dữ liệu siêu phổ với RegDB [76], Chest X-Ray [77], và
bộ dữ liệu Indian Pine4, tương ứng.

Hiểu biết đám mây điểm. 1) Phân loại: để đánh giá hiệu suất của Meta-Transformer trong
phân loại đối tượng 3D, chúng tôi sử dụng benchmark ModelNet-40 [78], bao gồm các mô hình CAD trên
40 lớp, với 9,843 mẫu huấn luyện và 2,468 mẫu validation. 2) Phân đoạn ngữ nghĩa: để
đánh giá hiệu suất trong phân đoạn đám mây điểm 3D, chúng tôi đánh giá mô hình trên cả bộ dữ liệu S3DIS [79] và
ShapeNetPart [80]. Bộ dữ liệu S3DIS bao gồm 6 khu vực trong nhà lớn và 13 lớp ngữ nghĩa,
bao gồm 271 phòng. Bộ dữ liệu ShapeNetPart bao gồm 16,880 mô hình đối tượng trên 16
danh mục hình dạng.

Nhận dạng âm thanh. Đối với nhận dạng âm thanh, chúng tôi sử dụng bộ dữ liệu Speech Commands V2 [81],
bao gồm 105,829 bản ghi một giây của 35 lệnh giọng nói phổ biến.

4https://github.com/danfenghong/IEEE_TGRS_SpectralFormer/blob/main/data/
IndianPine.mat

--- TRANG 8 ---
Nhận dạng video. Đối với hiểu biết video, chúng tôi tiến hành thí nghiệm trên bộ dữ liệu UCF101 [82]
cho nhận dạng hành động, với chi tiết hơn được trình bày trong § B.1.

Dự báo chuỗi thời gian. Đối với dự báo chuỗi thời gian, chúng tôi tiến hành thí nghiệm trên các bộ dữ liệu ETTh1 [83],
Traffic5, Weather6, và Exchange [84]. Chúng tôi sử dụng tokenizer của Autoformer [85].

Hiểu biết đồ thị. Chúng tôi tiến hành thí nghiệm trên bộ dữ liệu PCQM4M-LSC [86], là
bộ dữ liệu quy mô lớn bao gồm 4.4 triệu phân tử hữu cơ với tối đa 23 nguyên tử nặng với
các tính chất cơ học lượng tử tương ứng. Với mục tiêu dự đoán tính chất phân tử
bằng machine learning, nó có nhiều ứng dụng trong khám phá thuốc và khoa học vật liệu.

Phân tích dạng bảng. Chúng tôi tiến hành thí nghiệm trên adult và bank marketing từ kho UCI7. Chúng tôi
sử dụng tokenizer của TabTransformer [87] để mã hóa dữ liệu dạng bảng thô.

Nhận dạng IMU. Để đánh giá khả năng của Meta-Transformer trong hiểu biết hệ thống chuyển động quán tính,
chúng tôi tiến hành thí nghiệm phân loại cảm biến IMU trên bộ dữ liệu Ego4D [88].

Bảng 2: Tóm tắt thiết lập thí nghiệm trên các phương thức khác nhau. Chúng tôi báo cáo nhiệm vụ, bộ dữ liệu,
và quy mô dữ liệu cho mỗi phương thức.
Phương thức Nhiệm vụ Bộ dữ liệu Quy mô Dữ liệu
Văn bản Phân loại GLUE Benchmark 330K
Hình ảnh Phân loại ImageNet-1K 1.3M
Phát hiện MS COCO 118K
Phân đoạn ADE-20K 20K
Đám mây Điểm Phân loại Hình dạng ModelNet-40 9K
Phân đoạn Cảnh S3DIS 400M Điểm
Phân đoạn Đối tượng ShapeNetPart 16K
Âm thanh Phân loại Speech commands v2 105K
Video Nhận dạng Hành động UCF101 14K
Hồng ngoại Phân loại RegDB 40K
Siêu phổ Phân loại Indian Pine 10K
Tia X Phân loại Chest X-Ray 112K
IMU Phân loại Ego4D 193K
Dữ liệu dạng bảng Dự đoán Adult & Bank 32K-45K
Dữ liệu đồ thị Dự đoán PCQM4M-LSC 47M
Chuỗi thời gian Dự báo Exchange, Traffic, v.v. 5-36K

Thiết lập Mạng: Chúng tôi tuân theo thiết lập mặc định của ViT [13]. Meta-Transformer-B16 F biểu thị
Meta-Transformer với bộ mã hóa quy mô cơ sở chứa 12 khối transformer và 12 đầu chú ý,
và kích thước patch hình ảnh là 16. Đối với bộ mã hóa quy mô cơ sở, chiều embedding là 768
và chiều đầu ra của MLP là 3,072. 'F' và 'T' biểu thị rằng các tham số của bộ mã hóa được
Đông lạnh và tiếp tục Điều chỉnh, tương ứng.

Bảng 3: Kết quả thí nghiệm cho hiểu biết văn bản trên benchmark GLUE. Chúng tôi so sánh
các phương pháp tiên tiến hiện tại từ các nhiệm vụ paraphrasing, sentiment, duplication, inference, và answering,
và chúng tôi báo cáo thiết lập tiền huấn luyện và hiệu suất.
Phương pháp Thiết lập Tiền huấn luyện GLUE Benchmark
Phương thức Kích thước Dữ liệu SST-2 MRPC QQP MNLI QNLI
Cảm xúc Paraphrase Trùng lặp Suy luận Trả lời
BiLSTM+ELMo+Attn - - - 90.4 84.9 64.8 76.4 79.8
OpenAI GPT [89] Ngôn ngữ Book 0.8B 91.3 82.3 70.3 82.1 87.4
BERT BASE [51] Wiki+Book 3.3B 88.0 88.9 71.2 84.6 90.5
RoBERTa BASE [65] 96.0 90.0 84.0 84.0 92.0
ChatGPT Khác nhau 4,5000B 92.0 66.0 78.0 89.3 84.0
Meta-Transformer-B16 F[của chúng tôi] Hình ảnh LAION-2B [24] 2B 54.6 81.1 66.0 63.4 56.3
Meta-Transformer-B16 T[của chúng tôi] 81.3 81.8 78.0 70.0 60.3

5https://pems.dot.ca.gov/
6https://www.bgc-jena.mpg.de/wetter/
7http://archive.ics.uci.edu/ml/

--- TRANG 9 ---
Bảng 4: Kết quả thí nghiệm cho hiểu biết hình ảnh. Chúng tôi tiến hành thí nghiệm trong các nhiệm vụ phân loại,
phát hiện đối tượng, và phân đoạn thực thể trên các bộ dữ liệu ImageNet [69], MSCOCO [72], và ADE-
20K [75], trong đó Đậm và gạch chân biểu thị kết quả tốt nhất và tốt thứ hai.
Phương pháp Phân loại Phát hiện Đối tượng Phân đoạn Ngữ nghĩa
Res #Params #FLOPs Acc (%) #Params #FLOPs AP (%) #Params #FLOPs mIoU (%)
PVT-L [70] 224 261.4M 9.8G 81.7 81.0M - 42.9 65.1M 79.6G 44.8
Swin-L‡[19] 384 2197M 104G 87.3 253M 1382G 51.8 234M 2468G 52.1
CoAtNet-3‡[90] 384 2168M 107G 87.6 - - - - - -
CoAtNet-4‡[90] 384 2275M 190G 87.9 - - - - - -
DeiT III-L‡[91] 384 2304M 191G 87.7 - - - 353.6M 2231G 51.5
SwinV2-L/24‡[92] 384 2197M 115G 87.6 - - 58.8 - - 55.9
RepLKNet-31L‡[93] 384 2172M 96G 86.6 229M 1321G 53.9 207M 2404G 52.4
HorNet-L‡[94] 384 2202M 102G 87.7 259M 1358G 56.0 232M 2473G 54.1
ConvNeXt-L‡[95] 384 2198M 101G 87.5 255M 1354G 53.5 235M 2458G 53.2
ConvNeXt-XL‡[95] 384 2350M 179G 87.8 407M 1898G 53.6 391M 3335G 53.6
InternImage-L‡[96] 384 2223M 108G 87.7 277M 1399G 54.9 256M 2526G 53.9
InternImage-XL‡[96] 384 2335M 163G 88.0 387M 1782G 55.3 368M 3142G 55.0
Meta-Transformer-B16 F[của chúng tôi] 224 286.6M 17.5G 69.3∗ 143M 1126G 31.7 164M 135G 33.4
224 286.6M 17.5G 79.3†
Meta-Transformer-L14 F[của chúng tôi] 336 2191.1M 190.6G 75.3∗ 364M 2143G 43.5 314M 683G 41.2
336 2191.1M 190.6G 83.1†
Meta-Transformer-B16 T[của chúng tôi] 224 286.6M 17.5G 85.4 143M 1126G 46.4 164M 135G 48.3
Meta-Transformer-L14 T[của chúng tôi] 336 2191.1M 190.6G 88.1 364M 2143G 56.3 314M 683G 55.0
∗: phân loại zero-shot †: linear probing cho phân loại ‡: các mô hình được tiền huấn luyện trên ImageNet-22K

Bảng 5: Kết quả thí nghiệm cho hiểu biết dữ liệu hồng ngoại và siêu phổ. Chúng tôi tiến hành
thí nghiệm về các nhiệm vụ phân loại trên các bộ dữ liệu SYSU-MM01 và Indian Pine. Chúng tôi báo cáo Rank-1
(R@1), mean Average Precision (mAP), Overall Accuracy (OA), Average Accuracy (AA), và số
tham số có thể huấn luyện (Params).
Phương pháp R@1 (%) mAP (%) Params
AGW [97] [TPAMI'21] 70.49 65.90 25M
SMCL [98] [ICCV'21] 83.05 78.57 40M
MSCLNet [99] [ECCV'22] 83.86 78.31 50M
Meta-Transformer-B16 F 73.50 65.19 1.8M
(a) Hiểu biết dữ liệu hồng ngoại
Phương pháp OA (%) AA (%) Params
ViT [13] [ICLR'21] 71.86 78.97 85.2M
SpectralFormer [100] [TGRS'21] (Pixel) 78.55 84.68 85.2M
SpectralFormer [100] [TGRS'21] (Patch) 81.76 87.81 85.2M
Meta-Transformer-B16 F 67.62 78.09 0.17M
(b) Hiểu biết dữ liệu siêu phổ

4.2 Kết quả về Hiểu biết Ngôn ngữ Tự nhiên
Bảng 3 minh họa kết quả thí nghiệm trên benchmark GLUE cho các nhiệm vụ hiểu biết văn bản,
so sánh các phương pháp tiên tiến khác nhau như BERT [51], RoBERTa [65], và ChatGPT. So sánh
tập trung vào các nhiệm vụ paraphrasing, sentiment, duplication, inference, và answering. Khi
sử dụng các tham số đông lạnh được tiền huấn luyện trên hình ảnh, Meta-Transformer-B16 F đạt điểm số
54.6% trong sentiment (SST-2), 81.1% trong paraphrase (MRPC), 66.0% trong duplication (QQP), 63.4% trong inference
(MNLI), và 56.3% trong các nhiệm vụ answering (QNLI). Sau khi tinh chỉnh, Meta-Transformer-B16T thể hiện
hiệu suất cải thiện, với 81.3% trong sentiment, 81.8% trong paraphrase, 78.0% trong duplication, 70.0% trong
inference, và 60.3% trong các nhiệm vụ answering. Mặc dù hiệu suất của Meta-Transformer trên
benchmark GLUE có thể không ấn tượng bằng BERT, RoBERTa, hoặc ChatGPT, nó vẫn
chứng minh hiệu suất cạnh tranh, khả năng thích ứng, và tiềm năng cho hiểu biết ngôn ngữ tự nhiên.

4.3 Kết quả về Hiểu biết Hình ảnh
Như được hiển thị trong Bảng 4, Meta-Transformer thể hiện hiệu suất xuất sắc khi so sánh với
loạt Swin Transformer [19,107] và InternImage [96] trong các nhiệm vụ hiểu biết hình ảnh. Trong phân loại hình ảnh,
với sự trợ giúp của bộ mã hóa văn bản CLIP [24], Meta-Transformer mang lại hiệu suất tuyệt vời
dưới phân loại zero-shot với Meta-Transformer-B16 F và Meta-Transformer-L14 F, đạt
69.3% và 75.3%, tương ứng. Đồng thời, khi các tham số tiền huấn luyện được tinh chỉnh thêm,
Meta-Transformer có thể vượt qua các phương pháp tiên tiến hiện tại, với Meta-Transformer-B16 T
và Meta-Transformer-L14 T đạt độ chính xác 85.4% và 88.1%, tương ứng. Phương pháp sau vượt qua
cả SwinV2-L/24‡[107] (87.6%) và InternImage-XL [96]‡(88.0%) trong phân loại ImageNet [69].

--- TRANG 10 ---
Bảng 6: Kết quả thí nghiệm cho hiểu biết đám mây điểm. Chúng tôi tiến hành thí nghiệm trên các
bộ dữ liệu ModelNet-40 [78], S3DIS [79], và ShapeNetPart [80]. Chúng tôi so sánh các phương pháp tiên tiến hiện tại
từ các nhiệm vụ phân loại, ngữ nghĩa, và phân đoạn phần đối tượng, và chúng tôi báo cáo
phương thức tiền huấn luyện (Pre-train) và số tham số có thể huấn luyện (Params) của mỗi phương pháp.
Phương pháp Pre-train ModelNet-40 S3DIS Area-5 ShapeNetPart
mAcc (%) OA (%) Params mIoU (%) mAcc (%) Params mIoU I(%) mIoU C(%) Params
PointNet [CVPR'17] [32] N/A 86.0 89.2 3.5M 41.1 49.0 3.6M 83.7 80.4 3.6M
PointNet++ [NeurIPS'17] [5] N/A - 91.9 1.5M 53.5 - 1.0M 85.1 81.9 1.0
PointCNN [NeurIPS'18] [47] N/A 88.1 92.5 0.6M 57.3 - 0.6M
KPConv [ICCV'19] [49] N/A - 92.9 14.3M 67.1 72.8 15.0M 86.4 85.1 -
DGCNN [TOG'19] [101] N/A 90.2 92.9 1.8M 52.5 - 1.3M 85.2 82.3 1.3
Point Transformer [ICCV'21] [9] N/A 90.6 93.7 7.8M 70.4 - 7.8M 86.6 83.7 7.8
PointNeXt [NeurIPS'22] [102] N/A 90.8 93.2 1.4M 67.3 73.9 3.8M 86.7 84.4 1.0
Point-MLP [ICLR'22] [103] N/A 90.9 93.6 0.68M - - - 86.1 84.6 -
PointMixer [ECCV'22] [104] N/A 91.4 93.6 3.6M 71.4 77.4 6.5M - - -
Point-BERT [CVPR'22] [20] 3D - 93.2 21.1M 60.8 69.9 21.1M 85.6 84.1 21.1M
Point-MAE [ECCV'22] [105] 3D - 93.8 21.1M - - - 86.1 84.2 21.1M
P2P [NeurIPS'22] [56] 2D - 93.1 1.2M - - - 86.5 84.1 -
ACT [ICLR'23] [106] 2D - 93.5 21.1M 61.2 71.1 21.1M 86.1 84.7 21.2M
Meta-Transformer-B16 F[của chúng tôi] 2D 90.5 93.6 0.6M 72.3 83.5 2.3M 87.0 85.2 2.3M

Khi nói đến phát hiện đối tượng và phân đoạn ngữ nghĩa, Meta-Transformer cũng mang lại
hiệu suất xuất sắc, điều này tiếp tục chứng minh khả năng chung của nó trong hiểu biết hình ảnh. Trong phát hiện đối tượng,
Meta-Transformer-B16 F và Meta-Transformer-L14 F đạt AP 31.7% và 43.5%,
trong khi Meta-Transformer-B16 T và Meta-Transformer-L14 T đạt AP 46.4% và 56.3%, tương ứng.
Trong phân đoạn ngữ nghĩa, mIoU cho Meta-Transformer-B16 F và Meta-Transformer-L14 F là
33.4% và 41.2%, trong khi Meta-Transformer-B16 T và Meta-Transformer-L14 T đạt 51.0% và
55.0%, tương ứng. So sánh, SwinV2-L/24‡ vượt qua Meta-Transformer trong cả
phát hiện đối tượng (58.8% AP) và phân đoạn ngữ nghĩa (55.9% mIoU). Mô hình Meta-Transformer-L14 T
có hiệu suất tương tự với InternImage-XL‡[96] trong phân đoạn ngữ nghĩa (cả hai đạt
55.0% mIoU), nhưng vượt qua nó trong phát hiện đối tượng (56.3% AP so với 55.3% AP). Những kết quả này
nhấn mạnh rằng Meta-Transformer chứng minh hiệu suất cạnh tranh trong các nhiệm vụ hiểu biết hình ảnh khác nhau
ngay cả khi so sánh với Swin Transformer [19] và InternImage.

4.4 Kết quả về Dữ liệu Hồng ngoại, Siêu phổ, và Tia X
Bảng 5a trình bày so sánh hiệu suất của Meta-Transformer và các phương pháp tiên tiến khác
trên bộ dữ liệu RegDB [76] cho nhận dạng hình ảnh hồng ngoại. Meta-Transformer-B16 F chứng minh
kết quả cạnh tranh với độ chính xác Rank-1 là 73.50% và mAP là 65.19%. Mặc dù có thể không vượt qua
các phương pháp có hiệu suất cao nhất, Meta-Transformer chứng minh là một cách tiếp cận có thể chuyển giao đơn giản
cho các nhiệm vụ nhận dạng hình ảnh hồng ngoại. Những kết quả này chỉ ra tiềm năng của Meta-Transformer trong
việc xử lý các thách thức liên quan đến hình ảnh hồng ngoại và đóng góp cho những tiến bộ trong lĩnh vực này.

Bảng 7: Nhận dạng hình ảnh tia X với Meta-
Transformer. Chúng tôi tiến hành thí nghiệm trên bộ dữ liệu Chest
X-Ray, chúng tôi báo cáo Độ chính xác (%) và số
tham số có thể huấn luyện.
Phương pháp Độ chính xác (%) Params
ViT [13] 96.3 86.9M
SEViT [108] 94.6 85.8M
Meta-Transformer-B16 F 94.1 0.75M

Ngoài ra, Bảng 5b trình bày hiệu suất của Meta-Transformer trên bộ dữ liệu Indian
Pine cho nhận dạng hình ảnh siêu phổ. SpectralFormer [100] đạt
điểm độ chính xác ấn tượng, với cách tiếp cận theo patch.
Vision transformer thông thường cũng hoạt động tốt trong so sánh khi tinh chỉnh hoàn toàn
tất cả các tham số. Meta-Transformer-B16 F
chứng minh kết quả cạnh tranh trong nhận dạng hình ảnh siêu phổ với độ chính xác tổng thể thấp hơn.
Tuy nhiên, Meta-Transformer
nổi bật với số lượng tham số có thể huấn luyện ít hơn đáng kể (chỉ 0.17M) so với các phương pháp khác.
Điều này tiết lộ một hướng phát triển hứa hẹn của việc áp dụng Meta-Transformer cho viễn thám,
giám sát môi trường, và thăm dò khoáng sản. Đối với hình ảnh tia X, tương tự như xử lý
hình ảnh hồng ngoại, chúng tôi sử dụng cùng tokenizer hình ảnh như hình ảnh thông thường có thể nhìn thấy. Từ Bảng 7, chúng tôi có thể
quan sát rằng Meta-Transformer có thể đạt được hiệu suất cạnh tranh 94.1% độ chính xác.

4.5 Kết quả về Hiểu biết Đám mây Điểm 3D
Bảng 6 trình bày kết quả thí nghiệm cho hiểu biết đám mây điểm, so sánh hiệu suất
của Meta-Transformer với các phương pháp tiên tiến khác trên các bộ dữ liệu ModelNet-40 [78], S3DIS [79],

--- TRANG 11 ---
và ShapeNetPart [80]. Các nhiệm vụ bao gồm phân loại, phân đoạn ngữ nghĩa, và phân đoạn phần đối tượng.
Khi được tiền huấn luyện trên dữ liệu 2D, Meta-Transformer-B16 F chứng minh
hiệu suất cạnh tranh, đạt độ chính xác tổng thể (OA) 93.6% trên ModelNet-40 với chỉ 0.6M
tham số có thể huấn luyện, có thể so sánh với các mô hình có hiệu suất tốt nhất. Trên bộ dữ liệu S3DIS Area-5,
Meta-Transformer vượt qua các phương pháp khác với mean IoU (mIoU) 72.3% và
mean accuracy (mAcc) 83.5%, sử dụng 2.3M tham số. Hơn nữa, Meta-Transformer xuất sắc
trong bộ dữ liệu ShapeNetPart, đạt điểm số cao nhất trên cả instances mIoU (mIoU I) và
category mIoU (mIoU C) với 87.0% và 85.2%, tương ứng, sử dụng 2.3M tham số. Tóm lại,
Meta-Transformer chứng minh những ưu điểm đáng chú ý trong các nhiệm vụ hiểu biết đám mây điểm, cung cấp
hiệu suất cạnh tranh với ít tham số có thể huấn luyện hơn so với các phương pháp tiên tiến khác.

4.6 Kết quả về Nhận dạng Âm thanh
Để so sánh công bằng Meta-Transformer với loạt transformer âm thanh hiện tại [6,23]
có quy mô tương tự, chúng tôi tiến hành thí nghiệm về nhận dạng âm thanh bằng Meta-Transformer-B32.

Bảng 8: Hiểu biết âm thanh với Meta-Transformer. Chúng tôi
tiến hành thí nghiệm trên bộ dữ liệu Speech Commands V2 và
báo cáo điểm độ chính xác và số lượng tham số có thể huấn luyện và tất cả.
Phương pháp Pre-train Acc(%) A-Params Params
AST [6] (Supervised) N/A 92.6 86.9M 86.9M
AST [6] (Supervised) AudioSet-20K 96.2 86.9M 86.9M
AST [6] (Supervised) ImageNet+KD 98.1 86.9M 86.9M
SSAST [23] (Self-Supervised) AudioSet-2M 97.8 89.3M 89.3M
SSAST [23] (Self-Supervised) Librispeech 97.8 89.3M 89.3M
SSAST [23] (Self-Supervised) Joint Pretraining 98.0 89.3M 89.3M
Meta-Transformer-B32 F[của chúng tôi] 2D 78.3 86.6M 1.1M
Meta-Transformer-B32 T[của chúng tôi] 2D 97.0 86.6M 86.3M

Bảng 8 trình bày hiệu suất của Meta-Transformer trong lĩnh vực âm thanh. Những mô hình này được
so sánh với các phương pháp hiện tại như AST [6] và SSAST [23] về
độ chính xác, tất cả tham số (A-Params), và tham số có thể huấn luyện (T-Params). Với
tham số đông lạnh, Meta-Transformer-B32F đạt độ chính xác 78.3%
trong khi chỉ yêu cầu 1.1M tham số để tinh chỉnh. Mặt khác, mô hình Meta-Transformer-B32T
thể hiện độ chính xác cao hơn đáng kể là 97.0% khi tinh chỉnh các tham số, trong khi mô hình AST
chỉ đạt độ chính xác 92.6%. Khi AST được tiền huấn luyện trên ImageNet và bổ sung
thêm Knowledge Distillation (KD), nó đạt hiệu suất cải thiện 98.1%, nhưng
với số lượng tham số có thể huấn luyện cao hơn là 86.9M. Các mô hình SSAST hiển thị điểm độ chính xác
từ 97.8% đến 98.0% trong khi yêu cầu 89.3M tham số. Những kết quả này nhấn mạnh rằng
Meta-Transformer hoạt động cạnh tranh trong lĩnh vực âm thanh, chứng minh tính linh hoạt và
hiệu quả của nó trên các lĩnh vực khác nhau.

4.7 Kết quả về Nhận dạng Video
Bảng 9: Hiểu biết video với Meta-
Transformer. Chúng tôi tiến hành thí nghiệm trên
bộ dữ liệu UCF101 [82] và báo cáo điểm độ chính xác
và số tham số có thể huấn luyện, trong đó "V"
biểu thị chỉ các clip video.
Phương pháp Phương thức UCF101 Params
OPN [109] V 59.6 -
SimCLR [110] V 88.9 86.9M
VideoMAE V1 [111] V 96.1 86.9M
VideoMAE V2 [112] V 99.6 86.9M
ViT [13] (từ đầu) V 51.4 86.9M
Meta-Transformer-B16 F V 46.6 1.1M

Bảng 9 trình bày so sánh hiệu suất của Meta-Transformer và
các phương pháp tiên tiến hiện tại trên bộ dữ liệu UCF101 cho hiểu biết video.
Một số phương pháp được thiết kế riêng cho video tiên tiến đạt
độ chính xác trên 90%. Meta-Transformer chỉ chứa
một lượng tham số có thể huấn luyện không đáng kể là 1.1 triệu để
đạt độ chính xác 46.6% trong khi các phương pháp khác phải huấn luyện
khoảng 86.9 triệu tham số. Mặc dù Meta-Transformer không thể đánh bại
các mô hình hiểu biết video tiên tiến khác, Meta-Transformer nổi bật với
số lượng tham số có thể huấn luyện giảm đáng kể, gợi ý lợi ích tiềm năng của học đa phương thức thống nhất
và độ phức tạp kiến trúc thấp hơn.

4.8 Kết quả về Dự báo Chuỗi thời gian
Để khám phá khả năng của Meta-Transformer cho dự báo chuỗi thời gian, chúng tôi tiến hành thí nghiệm
trên một số benchmark được áp dụng rộng rãi cho các nhiệm vụ dự báo dài hạn bao gồm ETTh1 [83], Traffic,
Weather, và Exchange [84], với kết quả được hiển thị trong Bảng 10.

--- TRANG 12 ---
Bảng 10: Dự báo Chuỗi thời gian với Meta-Transformer. Theo TimesNet, chúng tôi báo cáo
số tham số có thể huấn luyện và hiệu suất trung bình từ 4 độ dài dự đoán khác nhau, là
{96,192,336,720}.
Mô hình Meta-Transformer TimesNet [113] ETSformer [114] FEDformer [115] Stationary [116] Autoformer [85] Pyraformer [117] Informer [83] LogTrans [118] Reformer [119]
[Của chúng tôi] [ICLR'23] [Arxiv'22] [ICML'22] [NeurIPS'22] [NeurIPS'21] [ICLR'21] [AAAI'21] [NeurIPS'19] [ICLR'20]
Metric MSE MAE Param MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE
ETTh1 0.994 0.797 19K 0.458 0.450 0.542 0.510 0.440 0.460 0.570 0.537 0.496 0.487 0.827 0.703 1.040 0.795 1.072 0.837 1.029 0.805
Traffic 0.694 0.372 2.0M 0.620 0.336 0.621 0.396 0.610 0.376 0.624 0.340 0.628 0.379 0.878 0.469 0.764 0.416 0.705 0.395 0.741 0.422
Weather 0.797 0.640 51K 0.259 0.287 0.271 0.334 0.309 0.360 0.288 0.314 0.338 0.382 0.946 0.717 0.634 0.548 0.696 0.602 0.803 0.656
Exchange 1.430 0.961 22K 0.416 0.443 0.410 0.427 0.519 0.500 0.461 0.454 0.613 0.539 1.913 1.159 1.550 0.998 1.402 0.968 1.280 0.932

Từ Bảng 10, chúng ta có thể có những quan sát sau. 1) Với hầu hết các tham số mô hình
được cố định, Meta-Transformer vẫn có thể vượt qua các phương pháp hiện tại bao gồm Pyraformer [117],
Informer [83], LogTrans [118], và Reformer [119] trên các bộ dữ liệu này. 2) Số lượng tham số có thể huấn luyện
của Meta-Transformer rất ít. Với chỉ 19K tham số có thể huấn luyện, Meta-Transformer
vẫn có thể vượt qua Informer [83]. Khi 2M tham số được huấn luyện, Meta-Transformer có thể trực tiếp
vượt qua Pyraformer [117]. Do đó, Meta-Transformer được tiền huấn luyện trên các nhiệm vụ nhận thức cũng có thể
được áp dụng cho các nhiệm vụ dự báo chuỗi thời gian, điều này rất truyền cảm hứng cho lĩnh vực này.

4.9 Kết quả về Hiểu biết Dữ liệu Dạng bảng
Bảng 11: Hiểu biết dữ liệu dạng bảng với
Meta-Transformer. Chúng tôi báo cáo Độ chính xác (%) và
điểm F1.
Phương pháp Adult Bank Marketing
Độ chính xác (%) Độ chính xác (%) F1
LightGBM 87.8 - 0.39
Tabmlp 87.2 - 0.39
Tabnet 87.0 - 0.31
Tabtransformer 87.1 93.4 0.42
Meta-Transformer-B16 F 85.9 90.1 0.41

Bảng 11 cung cấp kết quả so sánh về hiệu suất của các phương pháp khác nhau cho
hiểu biết dữ liệu dạng bảng trên các bộ dữ liệu Adult Census và Bank Marketing.

Meta-Transformer-B16 F đạt độ chính xác thấp hơn một chút so với các phương pháp khác trên Adult
Census nhưng hoạt động tốt hơn tất cả các phương pháp khác trên bộ dữ liệu Bank Marketing về
độ chính xác và điểm F1. Điều này gợi ý rằng Meta-Transformer cũng có lợi thế cho
hiểu biết dữ liệu dạng bảng, đặc biệt trên các bộ dữ liệu phức tạp như Bank Marketing.

Bảng 12: Hiểu biết dữ liệu đồ thị với Meta-Transformer. Chúng tôi tiến hành thí nghiệm trên
bộ dữ liệu PCQM4M-LSC, và chúng tôi báo cáo các metric đánh giá của điểm MAE huấn luyện và validation
và số tham số có thể huấn luyện.
Phương pháp Param. train MAE validate MAE
GCN [120] 2.0M 0.1318 0.1691
GIN [121] 3.8M 0.1203 0.1537
GCN- VN[120, 8] 4.9M 0.1225 0.1485
GIN- VN[121, 8] 6.7M 0.1150 0.1395
GINE- VN[122, 8] 13.2M 0.1248 0.1430
DeeperGCN- VN[123, 8] 25.5M 0.1059 0.1398
Graph Transformer [124] 0.6M 0.0944 0.1400
Graph Transformer- Wide [124] 83.2M 0.0955 0.1408
Graphormer SMALL [125] 12.5M 0.0778 0.1264
Graphormer [125] 47.1M 0.0582 0.1234
Meta-Transformer-B16 F 1.1M 0.8034 0.8863

4.10 Kết quả về Hiểu biết Dữ liệu Đồ thị và IMU
Chúng tôi báo cáo hiệu suất của việc sử dụng Meta-Transformer cho hiểu biết đồ thị trong Bảng 12.
Chúng tôi so sánh Meta-Transformer-B16 F với các mô hình mạng nơ-ron đồ thị khác nhau cho hiểu biết dữ liệu đồ thị
trên bộ dữ liệu PCQM4M-LSC [86]. Trong tất cả các phương pháp, Graphormer cho thấy
hiệu suất tốt nhất với điểm MAE huấn luyện và validation thấp nhất là 0.0582 và 0.1234, tương ứng.

--- TRANG 13 ---
Ngược lại, Meta-Transformer-B16 F mang lại điểm MAE huấn luyện và validation là 0.8034 và
0.8863, điều này tiết lộ khả năng hạn chế của kiến trúc Meta-Transformer hiện tại cho học dữ liệu cấu trúc.
Chúng tôi sẽ cải thiện điều này hơn nữa trong tương lai. Bên cạnh đó, theo ImageBind [26], chúng tôi tiến hành
phân loại trên bộ dữ liệu Ego4D [88], với dữ liệu đầu vào, Meta-Transformer mang lại độ chính xác
73.9%.

5 Hạn chế
Từ quan điểm độ phức tạp, phương pháp, và ứng dụng tiếp theo, các hạn chế của
Meta-Transformer được tóm tắt như sau:

Độ phức tạp: Meta-Transformer yêu cầu tính toán O(n2×D) xử lý embedding token
[E1,···,En]. Chi phí bộ nhớ cao và gánh nặng tính toán nặng khiến việc mở rộng quy mô trở nên khó khăn.

Phương pháp: So với cơ chế Chú ý Trục trong TimeSformer [7] và
Graphormer [125], Meta-Transformer thiếu nhận thức thời gian và cấu trúc. Hạn chế này
có thể ảnh hưởng đến hiệu suất tổng thể của Meta-Transformer trong các nhiệm vụ mà mô hình hóa thời gian và cấu trúc
đóng vai trò quan trọng, như hiểu biết video, theo dõi thị giác, hoặc dự đoán mạng xã hội.

Ứng dụng: Meta-Transformer chủ yếu mang lại lợi thế trong nhận thức đa phương thức. Vẫn chưa biết
về khả năng tạo sinh chéo phương thức của nó. Chúng tôi sẽ làm việc về điều này trong tương lai.

6 Kết luận
Trong giai đoạn đầu phát triển trí tuệ nhân tạo, các nhà tiên phong đã giới thiệu Perceptron Đa Lớp
(MLP) để giải quyết các nhiệm vụ dự đoán trong machine learning. Sau đó, các mạng hồi quy và tích chập
mở rộng khả năng AI trong xử lý dữ liệu đa phương tiện, đạt được thành công đáng kể trong
trích xuất biểu diễn từ văn bản, hình ảnh, đám mây điểm, và âm thanh. MLP đã được tích hợp
vào các mạng tích chập sâu. Trong bài báo này, chúng tôi khám phá tiềm năng của transformer thuần túy cho
học đa phương thức thống nhất, nhấn mạnh một xu hướng hứa hẹn hướng tới phát triển trí tuệ đa phương thức thống nhất
với backbone transformer. Ở một mức độ nào đó, bài báo này hỗ trợ vị trí thống trị
của transformer trong các mạng thế hệ tiếp theo. Quan trọng là, CNN và MLP không bị bỏ lại phía sau. Chúng
đóng vai trò thiết yếu trong tokenization dữ liệu và chiếu biểu diễn. Quá trình này minh họa
luật kế thừa trong mạng nơ-ron và sự tiến hóa liên tục của trí tuệ nhân tạo.

--- TRANG 14 ---
Tài liệu tham khảo
[1]Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, và Yuan Cao.
Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint
arXiv:2108.10904, 2021.
[2]Wenhui Wang, Hangbo Bao, Li Dong, và Furu Wei. Vlmo: Unified vision-language pre-
training with mixture-of-modality-experts. arXiv preprint arXiv:2111.02358, 2021.
[3]Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:
Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442,
2022.
[4]Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, và Ross Girshick. Masked
autoencoders are scalable vision learners. Trong Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, trang 16000–16009, 2022.
[5]Charles R Qi, Li Yi, Hao Su, và Leonidas J Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. Trong NeurIPS, 2017.
[6]Yuan Gong, Yu-An Chung, và James Glass. Ast: Audio spectrogram transformer. arXiv
preprint arXiv:2104.01778, 2021.
[7]Gedas Bertasius, Heng Wang, và Lorenzo Torresani. Is space-time attention all you need for
video understanding? Trong Proceedings of the International Conference on Machine Learning
(ICML), Tháng 7 2021.
[8]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, và George E Dahl.
Neural message passing for quantum chemistry. Trong International Conference on Machine
Learning, trang 1263–1272. PMLR, 2017.
[9]Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, và Vladlen Koltun. Point transformer.
Trong ICCV, trang 16259–16268, 2021.
[10] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang
Zhou, Jingren Zhou, và Hongxia Yang. Unifying architectures, tasks, and modalities through
a simple sequence-to-sequence learning framework. arXiv preprint arXiv:2202.03052, 2022.
[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information
processing systems, 30, 2017.
[12] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và
Sergey Zagoruyko. End-to-end object detection with transformers. Trong ECCV, 2020.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. ICLR, 2021.
[14] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, và Lucas Beyer. Scaling vision trans-
formers. Trong CVPR, trang 12104–12113, 2022.
[15] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, và Ping Luo.
Segformer: Simple and efficient design for semantic segmentation with transformers. Advances
in Neural Information Processing Systems, 34:12077–12090, 2021.
[16] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu,
Ping Luo, và Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer.
arXiv:2106.13797, 2021.

--- TRANG 15 ---
[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. Trong ICLR, 2021.
[18] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, và Yu Qiao. Vision
transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534, 2022.
[19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining
Guo. Swin transformer: Hierarchical vision transformer using shifted windows. Trong ICCV,
trang 10012–10022, 2021.
[20] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, và Jiwen Lu. Point-bert:
Pre-training 3d point cloud transformers with masked point modeling. Trong CVPR, 2022.
[21] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, và Bernard Ghanem. Pix4point: Image
pretrained transformers for 3d point cloud understanding. arXiv preprint arXiv:2208.12259,
2022.
[22] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang Mu, Ralph R Martin, và Shi-Min
Hu. Pct: Point cloud transformer. Computational Visual Media, 7(2):187–199, 2021.
[23] Yuan Gong, Cheng-I Lai, Yu-An Chung, và James Glass. Ssast: Self-supervised audio
spectrogram transformer. Trong Proceedings of the AAAI Conference on Artificial Intelligence,
tập 36, trang 10699–10709, 2022.
[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. Trong International Conference on Machine Learning,
trang 8748–8763. PMLR, 2021.
[25] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.
[26] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, và Ishan Misra. Imagebind: One embedding space to bind them all. Trong
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang
15180–15190, 2023.
[27] Warren S McCulloch và Walter Pitts. A logical calculus of the ideas immanent in nervous
activity. The bulletin of mathematical biophysics, 5:115–133, 1943.
[28] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, và Bernhard Scholkopf. Support
vector machines. IEEE Intelligent Systems and their applications, 13(4):18–28, 1998.
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image
recognition. Trong CVPR, trang 770–778, 2016.
[30] Zhao Xu, Kai Yu, Volker Tresp, Xiaowei Xu, và Jizhi Wang. Representative sampling for
text classification using support vector machines. Trong Advances in Information Retrieval: 25th
European Conference on IR Research, ECIR 2003, Pisa, Italy, April 14–16, 2003. Proceedings
25, trang 393–407. Springer, 2003.
[31] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne
Hubbard, và Lawrence Jackel. Handwritten digit recognition with a back-propagation network.
Advances in neural information processing systems, 2, 1989.
[32] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, và Leonidas J. Guibas. Pointnet: Deep
learning on point sets for 3d classification and segmentation. Trong CVPR, 2017.
[33] P Dhanalakshmi, S Palanivel, và Vennila Ramalingam. Classification of audio signals using
svm and rbfnn. Expert systems with applications, 36(3):6069–6075, 2009.

--- TRANG 16 ---
[34] John J Hopfield. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.
[35] Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997.
[36] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, và Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555,
2014.
[37] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text sum-
marization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023,
2016.
[38] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, và Yoshua Bengio. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
[39] Duyu Tang, Bing Qin, và Ting Liu. Document modeling with gated recurrent neural network
for sentiment classification. Trong Proceedings of the 2015 conference on empirical methods in
natural language processing, trang 1422–1432, 2015.
[40] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward
Lockhart, Florian Stimberg, Aaron Oord, Sander Dieleman, và Koray Kavukcuoglu. Efficient
neural audio synthesis. Trong International Conference on Machine Learning, trang 2410–2419.
PMLR, 2018.
[41] Yann LeCun, Léon Bottou, Yoshua Bengio, và Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
[42] Alex Krizhevsky, Ilya Sutskever, và Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.
[43] Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale
image recognition. Trong ICLR, 2015.
[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, và Andrew Rabinovich. Going deeper with convolutions.
Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 1–9,
2015.
[45] Xiang Zhang, Junbo Zhao, và Yann LeCun. Character-level convolutional networks for text
classification. Advances in neural information processing systems, 28, 2015.
[46] Ye Zhang và Byron Wallace. A sensitivity analysis of (and practitioners' guide to) con-
volutional neural networks for sentence classification. arXiv preprint arXiv:1510.03820,
2015.
[47] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, và Baoquan Chen. Pointcnn:
Convolution on x-transformed points. Advances in neural information processing systems, 31,
2018.
[48] Daniel Maturana và Sebastian Scherer. Voxnet: A 3d convolutional neural network for
real-time object recognition. Trong IROS, 2015.
[49] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, François
Goulette, và Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point
clouds. Trong ICCV, 2019.
[50] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, Li Deng, Gerald Penn, và Dong
Yu. Convolutional neural networks for speech recognition. IEEE/ACM Transactions on audio,
speech, and language processing, 22(10):1533–1545, 2014.
[51] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: Pre-training of
deep bidirectional transformers for language understanding. Trong NAACL-HLT, 2019.

--- TRANG 17 ---
[52] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
models are few-shot learners. Advances in neural information processing systems, 33:1877–
1901, 2020.
[53] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và
Sergey Zagoruyko. End-to-end object detection with transformers. Trong Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16,
trang 213–229. Springer, 2020.
[54] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, và Song
Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation.
arXiv preprint arXiv:2205.13542, 2022.
[55] Feihu Zhang, Jin Fang, Benjamin Wah, và Philip Torr. Deep fusionnet for point cloud
semantic segmentation. Trong ECCV, 2020.
[56] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, và Jiwen Lu. P2p: Tuning pre-trained
image models for point cloud analysis with point-to-pixel prompting. arXiv preprint
arXiv:2208.02812, 2022.
[57] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, và Qi Tian. Deep modular co-attention networks
for visual question answering. Trong Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition, trang 6281–6290, 2019.
[58] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, và Jifeng Dai. Vl-bert:
Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530,
2019.
[59] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,
Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for
vision-language tasks. Trong European Conference on Computer Vision, trang 121–137. Springer,
2020.
[60] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin
Choi, và Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models.
Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
trang 5579–5588, 2021.
[61] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, và
Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with
momentum distillation. Advances in neural information processing systems, 34:9694–9705,
2021.
[62] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong
Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for
computer vision. arXiv preprint arXiv:2111.11432, 2021.
[63] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, và Yonghui
Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917, 2022.
[64] Rakesh Chada, Zhaoheng Zheng, và Pradeep Natarajan. Momo: A shared encoder model for
text, image and multi-modal representations. arXiv preprint arXiv:2304.05523, 2023.
[65] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert
pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
[66] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural
machine translation system: Bridging the gap between human and machine translation. arXiv
preprint arXiv:1609.08144, 2016.

--- TRANG 18 ---
[67] Steffen Schneider, Alexei Baevski, Ronan Collobert, và Michael Auli. wav2vec: Unsuper-
vised pre-training for speech recognition. arXiv preprint arXiv:1904.05862, 2019.
[68] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018.
[69] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. Trong CVPR, trang 248–255. Ieee, 2009.
[70] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
Luo, và Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
without convolutions. Trong ICCV, 2021.
[71] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, và Saining
Xie. A convnet for the 2020s. arXiv preprint arXiv:2201.03545, 2022.
[72] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr Dollár, và C. Lawrence Zitnick. Microsoft coco: Common objects in context. Trong ECCV,
2014.
[73] Kaiming He, Georgia Gkioxari, Piotr Dollár, và Ross Girshick. Mask r-cnn. Trong ICCV, trang
2961–2969, 2017.
[74] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, và Jian Sun. Unified perceptual parsing
for scene understanding. Trong ECCV, trang 418–434, 2018.
[75] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, và Antonio Torralba.
Scene parsing through ade20k dataset. Trong Proceedings of the IEEE conference on computer
vision and pattern recognition, trang 633–641, 2017.
[76] Dat Tien Nguyen, Hyung Gil Hong, Ki Wan Kim, và Kang Ryoung Park. Person recognition
system based on a combination of body images from visible light and thermal cameras. Sensors,
17(3):605, 2017.
[77] Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker Rejaul Islam,
Khandakar F Islam, Rashid Mazhar, Tahir Hamid, Mohammad Tariqul Islam, Saad Kashem,
Zaid Bin Mahbub, et al. Reliable tuberculosis detection using chest x-ray with deep learning,
segmentation and visualization. IEEE Access, 8:191586–191601, 2020.
[78] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, và
Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. Trong CVPR, 2015.
[79] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, và
Silvio Savarese. 3d semantic parsing of large-scale indoor spaces. Trong CVPR, trang 1534–1543,
2016.
[80] Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao Su, ARCewu Lu, Qixing
Huang, Alla Sheffer, Leonidas Guibas, et al. A scalable active framework for region annotation
in 3d shape collections. ACM TOG, 35(6):210, 2016.
[81] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv
preprint arXiv:1804.03209, 2018.
[82] Khurram Soomro, Amir Roshan Zamir, và Mubarak Shah. Ucf101: A dataset of 101 human
actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
[83] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, và Wancai
Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. Trong
AAAI, 2021.
[84] Guokun Lai, Wei-Cheng Chang, Yiming Yang, và Hanxiao Liu. Modeling long-and short-
term temporal patterns with deep neural networks. Trong The 41st international ACM SIGIR
conference on research & development in information retrieval, trang 95–104, 2018.

--- TRANG 19 ---
[85] Haixu Wu, Jiehui Xu, Jianmin Wang, và Mingsheng Long. Autoformer: Decomposition
transformers with Auto-Correlation for long-term series forecasting. Trong NeurIPS, 2021.
[86] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, và Jure Leskovec. Ogb-
lsc: A large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430,
2021.
[87] Xin Huang, Ashish Khetan, Milan Cvitkovic, và Zohar Karnin. Tabtransformer: Tabular data
modeling using contextual embeddings. arXiv preprint arXiv:2012.06678, 2020.
[88] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit
Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the
world in 3,000 hours of egocentric video. Trong Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, trang 18995–19012, 2022.
[89] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language
understanding by generative pre-training. 2018.
[90] Zihang Dai, Hanxiao Liu, Quoc V Le, và Mingxing Tan. Coatnet: Marrying convolution and
attention for all data sizes. Advances in Neural Information Processing Systems, 34:3965–3977,
2021.
[91] Hugo Touvron, Matthieu Cord, và Hervé Jégou. Deit iii: Revenge of the vit. Trong Com-
puter Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXIV, trang 516–533. Springer, 2022.
[92] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao,
Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Trong
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, trang
12009–12019, 2022.
[93] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, và Jian Sun.
Scaling up your kernels to 31x31: Revisiting large kernel design in cnns. Trong CVPR, 2022.
[94] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser Nam Lim, và Jiwen Lu. Hornet:
Efficient high-order spatial interactions with recursive gated convolutions. Advances in Neural
Information Processing Systems, 35:10353–10366, 2022.
[95] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, và Saining
Xie. A convnet for the 2020s. Trong CVPR, 2022.
[96] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu,
Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation
models with deformable convolutions. arXiv preprint arXiv:2211.05778, 2022.
[97] Mang Ye, Jianbing Shen, Gaojie Lin, Tao Xiang, Ling Shao, và Steven C. H. Hoi. Deep
learning for person re-identification: A survey and outlook. arXiv preprint arXiv:2001.04193,
2020.
[98] Ziyu Wei, Xi Yang, Nannan Wang, và Xinbo Gao. Syncretic modality collaborative learning
for visible infrared person re-identification. Trong ICCV, trang 225–234, Tháng 10 2021.
[99] Yiyuan Zhang, Sanyuan Zhao, Yuhao Kang, và Jianbing Shen. Modality synergy com-
plement learning with cascaded aggregation for visible-infrared person re-identification. Trong
Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part XIV, trang 462–479. Springer, 2022.
[100] Danfeng Hong, Zhu Han, Jing Yao, Lianru Gao, Bing Zhang, Antonio Plaza, và Jocelyn
Chanussot. Spectralformer: Rethinking hyperspectral image classification with transformers.
IEEE Transactions on Geoscience and Remote Sensing, 60:1–15, 2021.
[101] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, và Justin M
Solomon. Dynamic graph cnn for learning on point clouds. TOG, 2019.

--- TRANG 20 ---
[102] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny,
và Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling
strategies. Trong Advances in Neural Information Processing Systems (NeurIPS), 2022.
[103] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, và Yun Fu. Rethinking network design and local
geometry in point cloud: A simple residual mlp framework. ICLR, 2022.
[104] Jaesung Choe, Chunghyun Park, Francois Rameau, Jaesik Park, và In So Kweon. Pointmixer:
Mlp-mixer for point cloud understanding. Trong European Conference on Computer Vision, trang
620–640. Springer, 2022.
[105] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, và Li Yuan. Masked
autoencoders for point cloud self-supervised learning. arXiv preprint arXiv:2203.06604, 2022.
[106] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, và
Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers
help 3d representation learning? arXiv preprint arXiv:2212.08320, 2022.
[107] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao,
Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. Trong
CVPR, 2022.
[108] Faris Almalik, Mohammad Yaqub, và Karthik Nandakumar. Self-ensembling vision trans-
former (sevit) for robust medical image classification. Trong Medical Image Computing and
Computer Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore,
September 18–22, 2022, Proceedings, Part III, trang 376–386. Springer, 2022.
[109] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, và Ming-Hsuan Yang. Unsupervised repre-
sentation learning by sorting sequence. Trong ICCV, 2017.
[110] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, và Kaiming He. A large-scale
study on unsupervised spatiotemporal representation learning. Trong CVPR, 2021.
[111] Zhan Tong, Yibing Song, Jue Wang, và Limin Wang. Videomae: Masked autoencoders are
data-efficient learners for self-supervised video pre-training. arXiv preprint arXiv:2203.12602,
2022.
[112] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, và
Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. arXiv preprint
arXiv:2303.16727, 2023.
[113] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, và Mingsheng Long. Times-
net: Temporal 2d-variation modeling for general time series analysis. arXiv preprint
arXiv:2210.02186, 2022.
[114] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, và Steven C. H. Hoi. Ets-
former: Exponential smoothing transformers for time-series forecasting. arXiv preprint
arXiv:2202.01381, 2022.
[115] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, và Rong Jin. FEDformer:
Frequency enhanced decomposed transformer for long-term series forecasting. Trong ICML, 2022.
[116] Yong Liu, Haixu Wu, Jianmin Wang, và Mingsheng Long. Non-stationary transformers:
Rethinking the stationarity in time series forecasting. Trong NeurIPS, 2022.
[117] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, và Schahram Dustdar.
Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and
forecasting. Trong ICLR, 2021.
[118] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, và Xifeng
Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series
forecasting. Trong NeurIPS, 2019.
[119] Nikita Kitaev, Lukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer. Trong
ICLR, 2020.

--- TRANG 21 ---
[120] Thomas N. Kipf và Max Welling. Semi-supervised classification with graph convolutional
networks. Trong ICLR. OpenReview.net, 2017.
[121] Keyulu Xu, Weihua Hu, Jure Leskovec, và Stefanie Jegelka. How powerful are graph neural
networks? Trong International Conference on Learning Representations, 2019.
[122] Rémy Brossard, Oriel Frigo, và David Dehaene. Graph convolutions that can finally model
local structure. arXiv preprint arXiv:2011.15069, 2020.
[123] Guohao Li, Chenxin Xiong, Ali Thabet, và Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. arXiv preprint arXiv:2006.07739, 2020.
[124] Vijay Prakash Dwivedi và Xavier Bresson. A generalization of transformer networks to
graphs. AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.
[125] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming
Shen, và Tie-Yan Liu. Do transformers really perform badly for graph representation? Trong
Thirty-Fifth Conference on Neural Information Processing Systems, 2021.
[126] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan
Guo, Lingpeng Kong, Meng Wang, và Yiran Zhong. Audio–visual segmentation. Trong Com-
puter Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXVII, trang 386–403. Springer, 2022.

--- TRANG 22 ---
Phụ lục
A Tóm tắt
Phụ lục được tổ chức như sau:
•Chúng tôi đầu tiên xác nhận và thảo luận về tiềm năng của Meta-Transformer trên nhiều phương thức hơn
(video, hồng ngoại, tia X, và hình ảnh siêu phổ) ngoài các phương thức được hiển thị trong
bài báo chính, và chúng tôi cung cấp kết quả thí nghiệm đáng ngạc nhiên trên các phương thức này trong § B.
•Sau đó chúng tôi tiếp tục chứng minh hiệu suất và ưu điểm của Meta-Transformer trong việc xử lý
các nhiệm vụ đa phương thức (liên quan đến đầu vào từ nhiều hơn một phương thức để thực hiện dự
đoán) trong § C.
•Ngoài ra, chúng tôi giới thiệu chi tiết hơn về các thí nghiệm trên văn bản, hình ảnh, đám mây điểm, và âm thanh
trong § D.
•Cuối cùng nhưng không kém phần quan trọng, chúng tôi thảo luận về tác động của Meta-Transformer đối với cộng đồng machine learning và
computer vision trong § E.

B Khả năng Mở rộng về Nhận thức Đơn Phương thức
Trong phần chính của bài báo này, chúng tôi minh họa rằng Meta-Transformer có thể đồng thời khám phá
các mẫu cơ bản của ngôn ngữ tự nhiên, hình ảnh 2D, đám mây điểm 3D, và phổ đồ âm thanh
với cùng kiến trúc mạng và tham số mạng. Hơn nữa, chúng tôi khám phá khả năng của nó
trong việc nhận thức các phương thức khác, như nhận dạng video, hồng ngoại, tia X, và nhận dạng hình ảnh siêu phổ.
Cụ thể, chúng tôi tiến hành thí nghiệm trên UCF101 [82] (video), RegDB [76] (hình ảnh hồng ngoại),
Chest X-Ray [77], và Indian Pine (hình ảnh siêu phổ) datasets.

B.1 Nhận dạng Video
Đối với nhận dạng video, chúng tôi theo VideoMAE [111] để sửa đổi tokenizer bằng cách thay thế
lớp embedding 2D bằng lớp embedding 3D để đồng thời mã hóa thông tin không gian-thời gian
từ các khung hình đầu vào. Sau tokenization, bằng cách tận dụng bộ mã hóa chia sẻ phương thức và các đầu cụ thể nhiệm vụ,
Meta-Transformer có thể trích xuất các đặc trưng ngữ nghĩa cao cấp từ video và
đạt được hiệu suất thuận lợi trong nhiệm vụ nhận dạng hành động của bộ dữ liệu UCF101.

Bộ dữ liệu. Bộ dữ liệu UCF101 [82] là một benchmark thường được sử dụng cho các nhiệm vụ nhận dạng hành động.
Nó là phiên bản mở rộng của UCF50 và chứa 13,320 clip video của 101 danh mục. 101 danh mục này
có thể được chia thành 5 nhóm: Chuyển động cơ thể, Tương tác người-người, Tương tác người-đối tượng,
Chơi nhạc cụ và Thể thao. Tất cả các khung hình đầu vào có độ phân giải
320×240 và tốc độ khung hình cố định 25 FPS, được thu thập từ YouTube.

B.2 Nhận dạng Hình ảnh Hồng ngoại
Nhận dạng hình ảnh hồng ngoại và siêu phổ đặt ra những thách thức độc đáo do các đặc điểm cụ thể của chúng.
Đối với hình ảnh hồng ngoại, khung Meta-Transformer có thể được điều chỉnh để nắm bắt
thông tin nhiệt bằng cách mã hóa giá trị nhiệt độ cùng với các đặc trưng thị giác, trong đó tokenizer
cho hình ảnh hồng ngoại giống như hình ảnh RGB thông thường.

Bộ dữ liệu. Bộ dữ liệu RegDB [76] tập trung vào việc đánh giá hiệu suất của các thuật toán nhận dạng hồng ngoại
trong các tình huống không bị ràng buộc và thực tế. Nó bao gồm các biến đổi về tư thế, biểu cảm, chiếu sáng,
và che khuất. Chúng tôi tiến hành thí nghiệm trên bộ dữ liệu RegDB để đánh giá hiệu suất của
Meta-Transformer trong nhận dạng hồng ngoại.

B.3 Nhận dạng Hình ảnh Siêu phổ
Tương tự, đối với hình ảnh siêu phổ, chúng tôi mong đợi rằng Meta-Transformer cũng có thể xử lý
thông tin phổ chiều cao bằng cách biểu diễn mỗi dải phổ trong embedding token. So với
việc xử lý hình ảnh RGB, sự sửa đổi duy nhất là chúng tôi sử dụng lớp chiếu tuyến tính mới
để thay thế lớp tích chập 2D hiện có.

Bộ dữ liệu. Bộ dữ liệu Indian Pine được sử dụng rộng rãi trong viễn thám và phân tích hình ảnh siêu phổ.
Nó bao gồm 145×145 pixel với 145 dải phổ, được chụp ở Indiana.

B.4 Nhận dạng Hình ảnh Tia X
Ngoài ra, chúng tôi khám phá tiềm năng của Meta-Transformer trong phân tích hình ảnh y tế. Chúng tôi
tận dụng tokenizer cho hình ảnh RGB ở đây để mã hóa hình ảnh y tế thô. Cụ thể, chúng tôi tiến hành
thí nghiệm liên quan đến phân tích hình ảnh tia X trên bộ dữ liệu Chest X-Ray [77]. Nó là một bộ sưu tập
hình ảnh y tế thường được sử dụng để phân tích và chẩn đoán các tình trạng ngực khác nhau.
Nó bao gồm 7,000 hình ảnh tia X của ngực. Bộ dữ liệu được chú thích với nhãn chỉ ra
sự hiện diện hoặc vắng mặt của các bất thường như bệnh phổi, gãy xương, và tình trạng tim.

C Khả năng Mở rộng về Nhận thức Đa Phương thức
Vì các phương thức văn bản, hình ảnh, đám mây điểm, và âm thanh đều có liên quan trong bài báo này, chúng tôi không
tiến hành các thí nghiệm đa phương thức toàn diện như thực hành phổ biến như Flamingo [25],
OFA [10], hoặc BEiT-3 [3]. Thay vào đó, chúng tôi tiến hành thí nghiệm đa phương thức trên một nhiệm vụ mới và thử thách
của Phân đoạn Âm thanh-Thị giác [126], chủ yếu tập trung vào việc xây dựng một người nghe thông minh
để căn chỉnh với các nhiệm vụ thị giác cơ bản.

C.1 Phân đoạn Âm thanh-Thị giác
Phân đoạn âm thanh-thị giác [126] đề cập đến nhiệm vụ phân đoạn các đối tượng từ các nguồn âm thanh khác nhau
trong một hình ảnh tham chiếu. Nó nhằm phát triển các thuật toán phân tích cả tín hiệu âm thanh và thị giác
đồng thời để xác định và phân định các nguồn hoặc sự kiện riêng biệt. Nó tìm thấy ứng dụng trong
các lĩnh vực như hội nghị truyền hình, giám sát, phân tích đa phương tiện, và thực tế tăng cường.

Chúng tôi tiến hành thí nghiệm trên bộ dữ liệu AVSS [126], được phát hành gần đây trong lĩnh vực nghiên cứu âm thanh-thị giác.
Nó cung cấp một bộ sưu tập toàn diện về dữ liệu âm thanh và thị giác được thu thập trong các tình huống thực tế.
Bộ dữ liệu bao gồm các bản ghi âm thanh và thị giác được đồng bộ hóa, có các sự kiện khác nhau
của hành động con người và âm thanh tự nhiên. Trái ngược với việc giới thiệu các mô-đun kết hợp đa phương thức như
các phương pháp hiện tại, Meta-Transformer trực tiếp nối các embedding thị giác và âm thanh sau Data-to-
Sequence tokenization. Sau khi trích xuất biểu diễn, chúng tôi sử dụng một lớp global average pooling đơn giản
để thu được biểu diễn cuối cùng của hai phương thức. Bảng 13 minh họa hiệu suất của

Bảng 13: Phân đoạn Âm thanh-Thị giác với Meta-Transformer. Chúng tôi tiến hành thí nghiệm trên
bộ dữ liệu AVSS [126], chúng tôi báo cáo mIou (%) và F-score.
Phương pháp mIou (%) F-score Params
AVSS [126] (ResNet-50) 20.18 0.252 ˜80M
AVSS [126] (ASPP) 28.94 - ˜180M
AVSS [126] (PVT-v2) 29.77 0.352 ˜180M
Meta-Transformer 31.33 0.387 86.5M

Meta-Transformer và các phương pháp hiện tại trên bộ dữ liệu AVSS cho phân đoạn âm thanh-thị giác. Các
metric đánh giá được báo cáo trong nhiệm vụ này là mIou và F-score. So sánh, Meta-Transformer
vượt qua tất cả các phương pháp khác với mIou cao nhất là 31.33% và F-score cao nhất là 0.387.
Nó cũng nổi bật với số lượng tham số thấp hơn đáng kể, chỉ với 86.5 triệu tham số
so với khoảng 80M đến 180M tham số của các phương pháp khác.

Meta-Transformer cung cấp một số ưu điểm so với các phương pháp khác trong lĩnh vực này.
•Kiến trúc thống nhất. Nó giảm bớt các bộ mã hóa cụ thể phương thức và giảm tính toán bằng cách
tận dụng một bộ mã hóa thống nhất để xử lý cả âm thanh và hình ảnh, dẫn đến một
quá trình hiệu quả và hợp lý hơn.

--- TRANG 23 ---
•Hội tụ nhanh hơn. Nhờ kiến trúc thống nhất để xử lý cả âm thanh và hình ảnh, bộ mã hóa có thể
căn chỉnh sâu hai phương thức thay vì chỉ ở đầu ra, dẫn đến hội tụ nhanh hơn. Meta-Transformer
chỉ cần 4 epoch huấn luyện để đạt 31.33% mIou.
•Hiệu suất vượt trội. Meta-Transformer đạt được cải thiện đáng kể 10%
so với các phương pháp khác có quy mô tham số tương tự.
•Hiệu quả. Mặc dù có hiệu suất tăng cường, Meta-Transformer đạt được điều này với ít tham số hơn nhiều,
chỉ yêu cầu 1/3 số lượng tham số, điều này làm cho quá trình forward và backward dễ dàng hơn.

Tóm lại, lợi ích của việc sử dụng Meta-Transformer để xử lý các nhiệm vụ đa phương thức là
hấp dẫn do hiệu quả tính toán, hội tụ nhanh, hiệu suất cải thiện, và hiệu quả tham số. Nó tiết lộ
hướng hứa hẹn đáng kể để áp dụng Meta-Transformer cho nhiều nhiệm vụ đa phương thức hơn.

D Chi tiết Thí nghiệm
Mã nguồn của chúng tôi được xây dựng trên các dự án mã nguồn mở bao gồm MMClassification8, MMDetection9, MMseg-
mentation10, OpenPoints11, Time-Series-Library12, Graphomer13.
Chúng tôi chân thành cảm ơn những đóng góp tuyệt vời của họ. Chi tiết triển khai hơn có thể được tìm thấy trong mã nguồn của chúng tôi.

E Thảo luận Tác động Tiếp theo

E.1 Nhận thức Không phương thức
Chúng tôi hy vọng rằng Meta-Transformer có thể giới thiệu cái nhìn mới vào cả lĩnh vực học đa phương thức và
tạo sinh đa phương thức. Meta-Transformer cho phép sử dụng một bộ mã hóa chia sẻ để mã hóa các
phương thức đa dạng, ví dụ như ngôn ngữ tự nhiên, hình ảnh 2D, đám mây điểm 3D, cũng như phổ đồ âm thanh,
và chiếu chúng vào một không gian biểu diễn chia sẻ. Điều này tự nhiên giảm khoảng cách phương thức giữa
các phương thức và giảm bớt gánh nặng căn chỉnh chéo phương thức. Ngoài ra, Meta-Transformer
loại bỏ nhu cầu về dữ liệu huấn luyện được ghép đôi (như cặp hình ảnh-văn bản), do đó trao cho học đa phương thức
tính linh hoạt huấn luyện hơn.

E.2 Triển vọng Ứng dụng
Chúng tôi điều tra việc áp dụng Meta-Transformer trên một loạt rộng các phương thức bao gồm hình ảnh RGB,
văn bản, đám mây điểm, hiểu biết video, viễn thám (hình ảnh siêu phổ), giám sát ban đêm (hình ảnh hồng ngoại),
và phân tích y tế (hình ảnh tia X).

Trong hiểu biết video, Meta-Transformer tiết lộ tiềm năng tăng cường phân tích và diễn giải video
bằng cách tích hợp thông tin từ văn bản, âm thanh, và hình ảnh với bộ mã hóa chia sẻ. Điều này có lợi cho
các nhiệm vụ như nhận dạng hành động, phát hiện sự kiện, và tóm tắt video. Khả năng của Meta-Transformer
xử lý các phương thức liên quan đến video mở đường cho các ứng dụng hiểu biết video cải thiện trong các lĩnh vực
như giám sát video, lập chỉ mục video, và truy xuất video dựa trên nội dung.

Trong hình ảnh siêu phổ cho viễn thám, Meta-Transformer cho phép phân tích và hiểu biết
dữ liệu siêu phổ bằng cách trích xuất các đặc trưng ngữ nghĩa cao cấp. Nó tăng cường các nhiệm vụ như

8https://github.com/open-mmlab/mmpretrain/tree/mmcls-1.x
9https://github.com/open-mmlab/mmdetection
10https://github.com/open-mmlab/mmsegmentation
11https://github.com/guochengqian/openpoints
12https://github.com/thuml/Time-Series-Library
13https://github.com/microsoft/Graphormer

--- TRANG 24 ---
phân loại, phát hiện mục tiêu, và lập bản đồ lớp phủ đất, cải thiện độ chính xác và hiệu quả của
các ứng dụng viễn thám. Khả năng xử lý hình ảnh siêu phổ bằng Meta-Transformer
mở ra cánh cửa cho những tiến bộ trong giám sát môi trường, nông nghiệp, quy hoạch đô thị, và quản lý thảm họa.

Trong các ứng dụng y tế, đặc biệt là phân tích hình ảnh tia X, Meta-Transformer cung cấp một cách tiếp cận hứa hẹn
để cải thiện độ chính xác chẩn đoán và hiệu quả với thông tin đa phương thức. Nó có thể
hiệu quả nắm bắt và kết hợp thông tin từ hình ảnh tia X, dữ liệu lâm sàng, và các phương thức khác để
hỗ trợ phát hiện bệnh, xác định bất thường, và lập kế hoạch điều trị bằng cách tận dụng khung học thống nhất của nó.
Khả năng của Meta-Transformer xử lý dữ liệu đa phương thức tăng cường tiềm năng cho
phân tích hình ảnh y tế chính xác và toàn diện hơn, dẫn đến chăm sóc bệnh nhân và kết quả tốt hơn.

Đối với hình ảnh hồng ngoại được sử dụng trong nhận dạng ban đêm và giám sát, khả năng của Meta-Transformer
xử lý dữ liệu hồng ngoại giúp trích xuất thông tin quan trọng cho phát hiện đối tượng, theo dõi, và nhận dạng
trong điều kiện ánh sáng yếu, điều này mở ra một con đường cho những tiến bộ trong giám sát ban đêm, hệ thống an ninh,
và điều hướng tự động trong các môi trường thử thách với sự hợp tác giữa
camera hồng ngoại với camera RGB.

E.3 Kết luận
Tóm lại, chúng tôi nghĩ rằng khả năng của Meta-Transformer thống nhất học đa phương thức đến từ
việc các kiến trúc mạng nơ-ron có thể học các mẫu bất biến phương thức. Kiến trúc của
Meta-Transformer minh họa những ưu điểm của embedding token có độ dài thay đổi trong học đa phương thức,
cung cấp các dạng linh hoạt nhưng thống nhất của ngữ nghĩa đa phương thức. Sau đó đã đến lúc
suy nghĩ về việc thiết kế các thuật toán để huấn luyện mạng tổng quát hóa trên các phương thức chưa thấy. Trong khi đó,
việc thiết kế kiến trúc của một bộ giải mã đa phương thức thống nhất cũng thú vị, có thể giải mã
biểu diễn thành bất kỳ dạng nào của một phương thức cụ thể.

Mặc dù Meta-Transformer trình bày hiệu suất đáng ngạc nhiên và cho thấy một hướng mới hứa hẹn
trong nhận thức đa phương thức, chúng tôi không chắc chắn liệu các kiến trúc được đề xuất có hiệu quả
trong các nhiệm vụ tạo sinh hay không. Và vẫn còn bí ẩn về cách phát triển các mô hình tạo sinh bất biến phương thức.
Chúng tôi hy vọng rằng điều này có thể truyền cảm hứng cho nghiên cứu tương lai.
