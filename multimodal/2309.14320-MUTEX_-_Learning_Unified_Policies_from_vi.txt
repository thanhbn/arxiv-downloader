# 2309.14320.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.14320.pdf
# Kích thước tệp: 18055014 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MUTEX: Học Các Chính Sách Thống Nhất từ
Đặc Tả Tác Vụ Đa Phương Thức
Rutav Shah1
rutavms@cs.utexas.eduRoberto Martín Martín1∗
robertomm@cs.utexas.eduYuke Zhu1∗
yukez@cs.utexas.edu
Tóm tắt: Con người sử dụng các phương thức khác nhau, chẳng hạn như giọng nói, văn bản, hình ảnh, video,
v.v., để truyền đạt ý định và mục tiêu của họ với các thành viên trong nhóm. Để robot trở thành những trợ lý
tốt hơn, chúng tôi hướng tới việc trao cho chúng khả năng tuân theo hướng dẫn và hiểu các tác vụ được chỉ
định bởi các đối tác con người. Hầu hết các phương pháp học chính sách robot đã tập trung vào một phương
thức duy nhất của đặc tả tác vụ trong khi bỏ qua thông tin liên phương thức phong phú. Chúng tôi trình bày
MUTEX, một cách tiếp cận thống nhất để học chính sách từ các đặc tả tác vụ đa phương thức. Nó đào tạo một
kiến trúc dựa trên transformer để tạo điều kiện cho việc lý luận liên phương thức, kết hợp các mục tiêu mô
hình hóa có mặt nạ và khớp liên phương thức trong một quy trình đào tạo hai giai đoạn. Sau khi đào tạo,
MUTEX có thể tuân theo một đặc tả tác vụ trong bất kỳ phương thức nào trong số sáu phương thức đã học
(minh họa video, hình ảnh mục tiêu, mô tả mục tiêu văn bản, hướng dẫn văn bản, mô tả mục tiêu giọng nói,
và hướng dẫn giọng nói) hoặc kết hợp chúng. Chúng tôi đánh giá một cách có hệ thống các lợi ích của
MUTEX trong một bộ dữ liệu được thiết kế mới với 100 tác vụ trong mô phỏng và 50 tác vụ trong thế giới
thực, được chú thích với nhiều trường hợp đặc tả tác vụ theo các phương thức khác nhau, và quan sát thấy
hiệu suất được cải thiện so với các phương pháp được đào tạo đặc biệt cho bất kỳ phương thức đơn lẻ nào.
Thông tin thêm tại https://ut-austin-rpl.github.io/MUTEX/
Từ khóa: Học Đa Phương Thức, Đặc Tả Tác Vụ, Thao Tác Robot

Minh Họa 
Video
Hành Động RobotTransformer
Chính Sách
Mutex
Mô-đun Kết Hợp
Liên Phương ThứcToken Đặc Tả 
Tác Vụ
Hướng Dẫn 
Giọng Nói"Xin vui lòng, di chuyển tay kẹp
của bạn về phía bánh mì và
nắm lấy nó nhẹ nhàng. Sau đó,
di chuyển nó lên trên đĩa trắng
và nhả ra nhẹ nhàng."
Hướng Dẫn
Văn Bản
Di chuyển về phía bánh mì
Giữ nó bằng tay của bạn
Đặt nó lên khay lò nướng
Đẩy khay lò nướng vào trong
Thả tay khỏi khay.
Mục Tiêu Giọng Nói"Xin vui lòng, đặt bánh mì
lên khay lò nướng."
Bánh mì được
đặt trên khay
lò nướng.Mục Tiêu Văn Bản
Quan Sát Robot
Hình Ảnh Mục Tiêu

Hình 1: Tổng quan. Chúng tôi giới thiệu MUTEX, một chính sách thống nhất học cách thực hiện các tác vụ có điều kiện
trên các đặc tả tác vụ từ nhiều phương thức (hình ảnh, video, văn bản, và giọng nói) dưới dạng hướng dẫn và mô tả mục
tiêu. MUTEX tận dụng thông tin bổ sung qua các phương thức để trở nên có khả năng hoàn thành các tác vụ được chỉ
định bởi bất kỳ phương thức đơn lẻ nào hơn so với các phương pháp được đào tạo đặc biệt cho từng phương thức.

1 Giới thiệu
Khi làm việc trong một nhóm, con người thường xuyên sử dụng các phương thức khác nhau để chỉ định tác vụ và
cải thiện giao tiếp, ví dụ, chia sẻ các mục tiêu tác vụ cấp cao ("Hãy nấu một bữa ăn!"), hướng dẫn bằng lời nói
("Chúng ta sẽ đi đến nhà bếp, lấy nồi từ tủ, rồi đặt nó lên bếp."), hoặc
∗Tư vấn ngang hàng,1Đại học Texas tại Austin
Hội nghị Robot Learning lần thứ 7 (CoRL 2023), Atlanta, USA.arXiv:2309.14320v1 [cs.RO] 25 Sep 2023

--- TRANG 2 ---
các minh họa trực quan chi tiết (hiển thị video nấu ăn). Các nhóm robot-con người nên hướng tới một mức độ hiểu biết tương tự. Trong khi nghiên cứu gần đây về học robot đã nghiên cứu các phương thức khác nhau để chỉ định tác vụ robot, bao gồm văn bản, hình ảnh, giọng nói, và video, hầu hết các nghiên cứu trước đây đã xem xét các phương thức riêng lẻ này như các vấn đề riêng biệt, chẳng hạn như học chính sách có điều kiện ngôn ngữ [1, 2, 3, 4], tuân theo hướng dẫn [5], đạt mục tiêu trực quan [6, 7, 8, 9], và bắt chước từ minh họa video [10, 11, 12]. Do đó, những cách tiếp cận này dẫn đến các hệ thống riêng biệt được thiết kế riêng cho từng phương thức đặc tả tác vụ.

Một nhóm nghiên cứu AI liên ngành đang phát triển đã đề xuất rằng học kết hợp qua nhiều phương thức, chẳng hạn như hình ảnh và văn bản [13, 14, 15, 16], video và văn bản [17, 18, 19] và thị giác và xúc giác [20, 21, 22], tạo ra những biểu diễn phong phú và hiệu quả hơn để cải thiện hiểu biết về các phương thức riêng lẻ. Những kết quả này phù hợp với các phát hiện trong khoa học nhận thức và tâm lý học, cho thấy rằng việc kết hợp các tín hiệu đa phương thức (ví dụ, trực quan và bằng lời) vào các quá trình học tập của con người nâng cao chất lượng học tập so với việc sử dụng các phương thức riêng lẻ [23, 24]. Lấy cảm hứng từ hiệu quả của học liên phương thức trong công việc trước đây, mục tiêu của chúng tôi là phát triển các chính sách thống nhất có khả năng lý luận về các đặc tả tác vụ đa phương thức cho các tác vụ thao tác đa dạng, trong đó mỗi tác vụ sẽ được định nghĩa trong một phương thức đơn lẻ thay đổi từ tác vụ này sang tác vụ khác. Chúng tôi tìm cách khai thác các điểm mạnh bổ sung của các phương thức khác nhau — một số cung cấp thông tin nhỏ gọn và cấp cao như mô tả mục tiêu, trong khi những phương thức khác cung cấp thông tin chi tiết như minh họa video từng bước — do đó cải thiện khả năng của mô hình để thực hiện các tác vụ từ các đặc tả tác vụ của các phương thức khác nhau.

Các công trình trước đây chủ yếu tập trung vào việc nâng cao hiểu biết ngôn ngữ bằng cách sử dụng dữ liệu trực quan [25, 26, 27, 28] hoặc trên một tập con các phương thức như ngôn ngữ và minh họa robot [29], ngôn ngữ và hình ảnh [30]. Những cách tiếp cận này thường bao gồm một hoặc hai phương thức, không thể bao quát các cách đa dạng mà con người thể hiện mục tiêu và ý định của họ. Thách thức chính liên quan đến việc học qua các phương thức đa dạng là tận dụng hiệu quả thông tin liên phương thức để củng cố lẫn nhau và có một kiến trúc có tính linh hoạt cao để bao quát sự biến đổi được đưa ra bởi nhiều phương thức.

Để học hiệu quả từ các phương thức đặc tả tác vụ khác nhau, chúng tôi cải thiện hai kỹ thuật học biểu diễn, mô hình hóa có mặt nạ [13, 18, 31] và khớp liên phương thức [14, 32], để thúc đẩy tương tác liên phương thức thông qua một không gian nhúng chia sẻ. Thứ nhất, chúng tôi khai thác các điểm mạnh bổ sung của mỗi phương thức — các đặc tả văn bản và giọng nói cung cấp hướng dẫn cho mô hình để trích xuất các đặc trưng liên quan đến tác vụ từ các đặc tả trực quan (mục tiêu hình ảnh và minh họa video), và các đặc tả trực quan, ngược lại, giúp gắn kết văn bản và giọng nói với các quan sát thế giới thực. Mô hình được đào tạo trên các mục tiêu học biểu diễn cùng với mục tiêu học chính sách (tức là, bắt chước hành vi) sao cho biểu diễn của đặc tả tác vụ cũng nắm bắt thông tin liên quan đến hành động. Sau khi chúng tôi xây dựng các biểu diễn phong phú hơn, thông tin hơn cho mỗi phương thức, chúng tôi đưa chúng vào một không gian chung [13, 14, 18, 31]. Không giống như công việc trước đây ánh xạ các đặc tả trực quan vào các nhúng ngôn ngữ [33], chúng tôi khai thác thực tế rằng các minh họa video của con người chứa thông tin chi tiết hơn về tác vụ. Chúng tôi làm phong phú các biểu diễn của các phương thức khác bằng cách khớp chúng với các biểu diễn video giàu thông tin. Việc khớp liên phương thức này dẫn đến các biểu diễn đa phương thức nhỏ gọn nhưng thông tin được sử dụng để thực hiện các tác vụ được chỉ định bởi bất kỳ phương thức nào.

Để mô hình thực hiện một tác vụ được chỉ định bởi bất kỳ phương thức nào, nó phải xử lý độ dài đầu vào biến đổi của các token đặc tả tác vụ. Đồng thời, nó phải dự đoán các hành động robot cùng với một số lượng biến đổi các tín hiệu có mặt nạ cho việc mô hình hóa có mặt nạ của các phương thức khác nhau. Để đạt được điều này, chúng tôi thiết kế một bộ mã hóa kiểu Perceiver [34] trong đó một số lượng biến đổi các token đặc tả tác vụ được chú ý với một lịch sử cố định các quan sát robot sử dụng các cơ chế chú ý chéo. Các nhúng thu được từ bộ mã hóa sau đó được truyền qua bộ giải mã kiểu Perceiver [35] để dự đoán các hành động robot và tín hiệu có mặt nạ cho các phương thức riêng lẻ. Thiết kế kiến trúc này cho phép mô hình học một chính sách có thể thực hiện các tác vụ được chỉ định bởi bất kỳ phương thức đơn lẻ nào hoặc tổng hợp tùy ý của nhiều phương thức.

Tóm lại, chúng tôi giới thiệu MUTEX (Đặc tả Tác vụ đa phương thức cho việc thực hiện robot), một chính sách thống nhất có khả năng thực hiện các tác vụ được chỉ định như mô tả mục tiêu hoặc hướng dẫn chi tiết hơn bằng văn bản, giọng nói, hoặc phương thức trực quan. MUTEX linh hoạt và hiệu quả. Nó không chỉ có thể hiểu các đặc tả tác vụ đa phương thức mà còn cải thiện tính mạnh mẽ của việc thực hiện tác vụ cho mỗi phương thức đơn lẻ. Chúng tôi đã chứng minh điều này với một điểm chuẩn đánh giá toàn diện, bao gồm 100 tác vụ thao tác đa dạng trong mô phỏng và 50 tác vụ trong thế giới thực, dẫn đến 6000 thử nghiệm đánh giá (mỗi phương pháp) trong mô phỏng và 600 thử nghiệm đánh giá trong thế giới thực. Đáng chú ý, đánh giá thế giới thực cho thấy MUTEX có thể diễn giải hiệu quả các minh họa video của con người và thực hiện tác vụ thành công với robot, mặc dù có sự khác biệt về hình thái. Như một phần của nỗ lực này, chúng tôi cung cấp một bộ dữ liệu thế giới thực lớn gồm 50 tác vụ với danh sách đầy đủ các tác vụ thế giới thực với 30 quỹ đạo cho mỗi tác vụ chứa các tác vụ như "đặt bánh mì lên khay lò vi sóng và đóng nó", "mở lò chiên không khí và bỏ xúc xích vào", hoặc "đặt cuốn sách vào ngăn phía trước của giỏ", được chỉ định với nhiều đặc tả tác vụ đa phương thức phong phú: mô tả mục tiêu văn bản, hướng dẫn văn bản, hình ảnh mục tiêu, đặc tả video, mô tả mục tiêu giọng nói, và hướng dẫn giọng nói [Hình 3], hỗ trợ nghiên cứu tương lai về đặc tả tác vụ đa phương thức.

2 Công Trình Liên Quan
Đặc Tả Tác Vụ trong Thao Tác Robot. Một cách để truyền đạt tác vụ cho một chính sách đa tác vụ là thông qua các vector mã hóa một-nóng [36]. Tuy nhiên, cách tiếp cận này bị hạn chế với một tập hợp tác vụ được định nghĩa trước và không thể mở rộng cho những tác vụ mới. Mặt khác, các phương pháp sử dụng ngôn ngữ để chỉ định tác vụ [1, 33, 2, 4, 37, 38] đã cho thấy việc cải thiện khái quát đa tác vụ nhờ biểu diễn tác vụ ngữ nghĩa phong phú hơn. Tuy nhiên, học với đặc tả ngôn ngữ có thể không hiệu quả vì nó yêu cầu gắn kết ngôn ngữ với không gian quan sát và hành động của robot [39, 40]. Hơn nữa, bản chất nhỏ gọn của ngôn ngữ đặt ra thách thức cho các tác vụ cần mô tả chi tiết hoặc chính xác hơn [30]. Các đặc tả tác vụ trực quan (hình ảnh [7, 9, 8, 41] hoặc video [10, 11, 33, 12]) cung cấp thông tin dày đặc khiến các chính sách phụ thuộc sai vào thông tin không liên quan đến tác vụ (ví dụ, một minh họa trực quan về việc di chuyển vật thể cũng hiển thị vị trí và chuyển động của các vật thể nền trong cảnh [42]), gây ra hành vi khái quát kém. Sự đặc biệt của các phương thức riêng lẻ hạn chế các phương pháp tập trung vào đặc tả đơn phương thức và không thể tận dụng thông tin bổ sung qua các phương thức.

Học Biểu Diễn Liên Phương Thức. Trong những năm qua, đã có một khối lượng lớn tài liệu về học biểu diễn phong phú từ dữ liệu đa phương thức với các mục tiêu học liên phương thức [16, 43, 44]. Những công trình này đã cung cấp bằng chứng thuyết phục rằng học qua nhiều phương thức có thể tăng cường đáng kể hiệu suất mô hình trên các tác vụ nhận dạng thị giác thông thường (chẳng hạn như phân loại hình ảnh [15, 45, 46, 47], phát hiện vật thể [48, 49, 50], phân đoạn [51, 46], nhận dạng hoạt động [52]), các tác vụ hiểu ngôn ngữ (chẳng hạn như phân tích tình cảm, diễn giải lại [53, 54]), và các tác vụ lý luận đa phương thức (chẳng hạn như QA trực quan [55, 19, 56] và truy xuất liên phương thức [13, 14]). Trong robot học, các quan sát đa cảm biến đã được chứng minh là cải thiện hiệu suất của các tác vụ thao tác [20, 21, 22]. Lấy cảm hứng từ những thành công này, MUTEX học một biểu diễn liên phương thức của các đặc tả tác vụ cho học bắt chước đa tác vụ cho thao tác robot.

Học Bắt Chước Đa Tác Vụ trong Thao Tác: Học bắt chước đa tác vụ cho thao tác robot đã được nghiên cứu rộng rãi với các đặc tả tác vụ ngôn ngữ [1, 37, 33, 4] và minh họa trực quan [12, 9], tương ứng. Một dòng công việc gần hơn với chúng tôi bao gồm các phương pháp khai thác các đặc tả tác vụ đa phương thức. Một số tận dụng dữ liệu đa phương thức để đào tạo mô hình, nhưng các chính sách cuối cùng được triển khai chỉ hoạt động trên một loại phương thức duy nhất [25, 26, 27, 33]. Những phương pháp khác chứng minh rằng các chính sách tiêu thụ các đặc tả đa phương thức có thể khái quát cho các trường hợp tác vụ mới trong một lần, nhưng tác vụ mới phải được chỉ định trong tất cả các phương thức [29]. Các công trình gần đây hơn đã khám phá việc chỉ định tác vụ với các gợi ý đa phương thức (token văn bản và hình ảnh) được định nghĩa trong một kết hợp các phương thức [30], cung cấp một giao diện linh hoạt hơn và một phần giảm bớt các vấn đề gắn kết. Tuy nhiên, không có công trình nào trước đây đã cung cấp sự linh hoạt để chỉ định tác vụ sử dụng bất kỳ phương thức riêng lẻ nào, cũng như không hỗ trợ nhiều phương thức khác nhau như MUTEX.

3 Mô Hình và Bộ Dữ Liệu MUTEX
Mục tiêu của chúng tôi là học một chính sách thống nhất thực hiện các tác vụ đa dạng dựa trên một bộ dữ liệu minh họa được chú thích với các đặc tả tác vụ đa phương thức, bao gồm các đặc tả ngôn ngữ, giọng nói, và trực quan. Chúng tôi giả định rằng, trong thời gian kiểm tra, tác vụ cần thực hiện sẽ được chỉ định trong một hoặc

--- TRANG 4 ---
một tập con các phương thức có thể thay đổi từ tác vụ này sang tác vụ khác. Đối với mỗi tác vụ, Ti∈ {T1, T2, . . . , Tn}, chúng tôi giả định rằng tác tử học từ một tập hợp các minh họa của con người thu được thông qua điều khiển từ xa, Di={d1i, d2i, . . . , dmi}, trong đó m là số lượng minh họa cho tác vụ Ti, tạo thành toàn bộ bộ dữ liệu minh họa, D. Mỗi minh họa dji trình bày dưới dạng một chuỗi các quan sát và hành động chuyên gia, dji= [(o1, a1)ji, . . .(oT, aT)ji].

Mục tiêu của mỗi tác vụ Ti được chỉ định bởi k mô tả tác vụ thay thế (t) trong sáu dạng khác nhau trong ba phương thức: hướng dẫn văn bản t∈ {L1i, L2i, . . . , Lki}, mô tả mục tiêu văn bản t∈ {l1i, l2i, . . . , lki}, minh họa video t∈ {V1i, V2i, . . . , Vki}, hình ảnh mục tiêu t∈ {v1i, v2i, . . . , vki}, hướng dẫn giọng nói t∈ {S1i, S2i, . . . , Ski}, và mô tả mục tiêu giọng nói t∈ {s1i, s2i, . . . , ski}. Lưu ý rằng chúng tôi đặc trưng hóa các phương thức với các chữ cái L/l, V/v, và S/s, và sử dụng chữ hoa để biểu thị hướng dẫn chi tiết và chữ thường để biểu thị đặc tả trạng thái mục tiêu. Mục tiêu của chúng tôi là học một chính sách thống nhất, π(a|t, o), xuất ra các hành động liên tục, a∈A, cho các quan sát hiện tại, o∈O, có điều kiện trên một đặc tả tác vụ trong một hoặc nhiều phương thức có thể, t∈L|l|V|v|S|s. Chúng tôi hướng tới việc tạo ra một chính sách không chỉ thực hiện n tác vụ trong bộ dữ liệu đào tạo D (tác vụ đã thấy) trong các điều kiện ban đầu mới (ví dụ vị trí của các vật thể) mà còn khái quát cho các mô tả tác vụ chưa thấy trước đây.

3.1 Quy Trình Đào Tạo MUTEX
Mục tiêu của chúng tôi không chỉ là thu được một chính sách thống nhất hiểu các đặc tả tác vụ trong các phương thức khác nhau mà còn cải thiện hiệu suất chính sách khi một tác vụ được chỉ định trên mỗi phương thức đơn lẻ bằng cách khai thác các tương tác liên phương thức trong quá trình đào tạo. Để đạt được mục tiêu đó, chúng tôi tận dụng hai kỹ thuật học biểu diễn mà chúng tôi tích hợp tuần tự trong quy trình đào tạo của MUTEX: 1) Mô hình Hóa Mặt Nạ để thúc đẩy các tương tác liên phương thức của tất cả các phương thức vào một không gian ẩn được học chia sẻ, và 2) Khớp Liên Phương Thức để làm phong phú mỗi phương thức với thông tin của phương thức dày đặc thông tin hơn. Cả hai giai đoạn của quy trình của chúng tôi được kết hợp với một mục tiêu bắt chước hành vi cho việc học chính sách để đảm bảo rằng biểu diễn được học chứa thông tin liên quan cho tác tử thực hiện tác vụ thao tác. Tổng quan về cách tiếp cận được đề xuất được hiển thị trong Hình 2.

Mô hình Hóa Mặt Nạ cho Học Liên Phương Thức: Trong giai đoạn đầu [Bước 1 trong PseudoCode 1], MUTEX thúc đẩy các tương tác liên phương thức giữa các thành phần mô hình diễn giải các phương thức khác nhau: hướng dẫn văn bản (L), mục tiêu văn bản (l), minh họa video (V), hình ảnh mục tiêu (v), hướng dẫn giọng nói (S), và mục tiêu giọng nói (s). Lấy cảm hứng từ thành công trong các tác vụ học liên phương thức khác [53, 19, 43, 44], chúng tôi che giấu các token hoặc đặc trưng nhất định của mỗi phương thức và học dự đoán chúng với sự giúp đỡ của các phương thức khác. Điều này buộc mô hình sử dụng thông tin từ các phương thức khác để nâng cao biểu diễn của từng phương thức. Một cách trực quan, dự đoán mặt nạ văn bản và giọng nói giúp tập trung thông tin liên quan từ các phương thức trực quan, trong khi dự đoán mặt nạ hình ảnh và video giúp gắn kết các phương thức khác. Trong quá trình kiểm tra, một đặc tả tác vụ chỉ trong một hoặc một tập con các phương thức được sử dụng. Do đó, để có được các biểu diễn phương thức đơn lẻ mạnh mẽ [Bảng 1, 2], chúng tôi tái tạo các điều kiện này trong quy trình đào tạo của chúng tôi bằng cách lấy mẫu ngẫu nhiên các phương thức trong mỗi lần lặp.

Cụ thể, tại mỗi lần lặp của quá trình đào tạo, chúng tôi chọn ngẫu nhiên các đặc tả tác vụ của một hoặc một tập con các phương thức. Nếu nhiều hơn một phương thức được chọn, chúng tôi che giấu một số phần nhất định của mỗi phương thức. Chúng tôi yêu cầu MUTEX dự đoán các phần bị che giấu cùng với các giá trị hành động mà chuyên gia đã minh họa tại mỗi bước. Các phương thức khác nhau sẽ che giấu các đặc trưng đầu vào hoặc trung gian và sử dụng một mất mát metric khác nhau (ℓ1 hoặc ℓ2). Đối với mô hình hóa văn bản có mặt nạ (che giấu các yếu tố của mô tả mục tiêu văn bản hoặc hướng dẫn văn bản), chúng tôi che giấu các từ [58] sau đó được truyền qua mô hình ngôn ngữ CLIP được đào tạo trước [15] để trích xuất các đặc trưng. Chúng tôi sử dụng mất mát entropy chéo chuẩn giữa các token được dự đoán (ŷ) và sự thật cơ bản (y), tức là, LCE(y,ŷ) =−∑Ni=1yi log(ŷi), trong đó N là số lượng token trong từ vựng. Đối với mô hình hóa trực quan có mặt nạ (che giấu các yếu tố của hình ảnh mục tiêu hoặc hướng dẫn video), chúng tôi che giấu các vùng trung gian của các đặc trưng thu được từ mô hình CLIP được đào tạo trước [15] và, theo các công trình trước trong mô hình hóa thị giác-ngôn ngữ [59], chúng tôi sử dụng mất mát hồi quy ℓ1 đơn giản giữa các đặc trưng được dự đoán và sự thật cơ bản, Lℓ1=|y−ŷ|. Đối với mô hình hóa giọng nói có mặt nạ (che giấu các yếu tố của mô tả mục tiêu giọng nói hoặc hướng dẫn giọng nói), chúng tôi sử dụng cách tiếp cận tương tự với mô hình hóa trực quan nhưng sử dụng các đặc trưng từ mô hình Whisper được đào tạo trước [57] thay vì với mất mát hồi quy ℓ1 (Tham khảo Phụ lục 6.2).

Khớp Liên Phương Thức cho Biểu Diễn Phong Phú Hơn: Trong giai đoạn thứ hai của quy trình đào tạo của MUTEX [Bước 2 trong PseudoCode 1], chúng tôi làm phong phú không gian nhúng chung cho mỗi phương thức tác vụ bằng cách liên kết nó với các đặc trưng của phương thức giàu thông tin hơn. Để đạt được mục tiêu đó, trái ngược với các công trình trước sử dụng mất mát đối lập liên phương thức để học một không gian nhúng chung [14, 16], chúng tôi sử dụng mất mát ℓ2 đơn giản để kéo các biểu diễn của tất cả các phương thức về phía phương thức có nhiều thông tin hơn và hiệu suất tốt hơn. Các đặc tả video chứa nhiều thông tin nhất, dẫn đến các đặc trưng minh họa và mạnh mẽ hơn; do đó, chúng tôi làm phong phú các phương thức khác với thông tin từ không gian biểu diễn video thu được sau khi học liên phương thức.

Cụ thể, gọi fL,fl,fv,fs,fS là các biểu diễn đặc trưng của hướng dẫn văn bản, mục tiêu văn bản, hình ảnh mục tiêu, mục tiêu giọng nói, và hướng dẫn giọng nói, và fV là biểu diễn đặc trưng của minh họa video cho cùng một tác vụ. Mất mát khớp liên phương thức của chúng tôi được cho bởi:
Lmatch =Lℓ2(fL, fV) +Lℓ2(fl, fV) +Lℓ2(fS, fV) +Lℓ2(fs, fV) +Lℓ2(fs, fV) (1)
trong đó Lℓ2 là mất mát hồi quy ℓ2. Độ dốc từ mất mát này không được lan truyền ngược đến phần của MUTEX mã hóa phương thức video, để chúng không thay đổi.

3.2 Kiến Trúc MUTEX
Quá trình đào tạo được phác thảo ở trên yêu cầu một kiến trúc mô hình có khả năng truyền và mã hóa thông tin liên phương thức từ nhiều phương thức. Mô hình của MUTEX bao gồm ba thành phần chính (xem Hình 2): 1) Bộ Mã Hóa Đặc Trưng Phương Thức ánh xạ các phương thức đầu vào thành các token đặc trưng tác vụ, 2) một Bộ Mã Hóa Chính Sách nhận các token đặc trưng tác vụ và quan sát robot và xuất ra các trạng thái ẩn, và 3) một Bộ Giải Mã Chính Sách nhận các trạng thái ẩn cùng với các truy vấn giải mã và xuất ra các đặc trưng tương ứng với các truy vấn.

Các bộ mã hóa đặc trưng phương thức của MUTEX trích xuất các token từ đặc tả tác vụ đầu vào sử dụng một mô hình lớn được đào tạo trước, cố định, điều này giúp chúng tôi trích xuất biểu diễn có ý nghĩa ngữ nghĩa từ phương thức đầu vào. Để học các biểu diễn được gắn kết với không gian quan sát và hành động, các đặc trưng này được truyền qua một lớp chiếu bao gồm một MLP đơn giản hoặc khối chú ý đơn lẻ

--- TRANG 5 ---
trước khi chuyển đến bộ mã hóa chính sách. Bộ mã hóa chính sách của MUTEX kết hợp hiệu quả thông tin thu được từ nhiều phương thức đặc tả tác vụ và quan sát robot, sử dụng một kiến trúc dựa trên transformer với các lớp chú ý chéo và tự chú ý được xếp chồng. Trong các lớp chú ý chéo, các truy vấn được tạo ra từ quan sát robot, trong khi các khóa và giá trị từ các token đặc tả tác vụ. Đầu ra của bộ mã hóa sau đó được chuyển đến bộ giải mã chính sách. Mặc dù đầu ra của bộ mã hóa chính sách được làm phong phú với thông tin thu được từ các phương thức đặc tả tác vụ khác nhau, MUTEX yêu cầu chính sách xuất ra các đặc trưng để dự đoán giá trị hành động và một số lượng biến đổi các token có mặt nạ. Điều này thúc đẩy chúng tôi áp dụng kiến trúc Perceiver Decoder [35] làm bộ giải mã chính sách của MUTEX để tận dụng các truy vấn có thể học và chỉ xuất ra thông tin tương ứng với các truy vấn đầu vào. Các đặc trưng giải mã cho dự đoán hành động được chuyển qua một MLP để ước tính Mô hình Hỗn hợp Gaussian cho các giá trị hành động liên tục. Tương tự, các MLP riêng biệt được sử dụng để dự đoán giá trị token hoặc đặc trưng cho các truy vấn token có mặt nạ. Thông tin chi tiết hơn có thể được tìm thấy trong Phụ lục 6.3.

3.3 Bộ Dữ Liệu Đặc Tả Tác Vụ Đa Phương Thức

Minh Họa 
Video
Hướng Dẫn
Văn Bản
Mục Tiêu Giọng Nói
Hình Ảnh Mục Tiêu
Mục Tiêu Văn Bản
Hướng Dẫn 
Giọng Nói
Nắm tay cầm hoặc cạnh của ngăn kéo trên.
Từ từ và cẩn thận kéo ngăn kéo mở.
Cầm thanh granola bằng tay kẹp của bạn.
Đặt thanh granola vào bên trong ngăn kéo
đã mở.
Thả thanh granola.Vui lòng giữ chặt núm điều khiển và xoay
nó theo cách đặt lò về "bật."
Tiến về phía chảo, nắm lấy tay cầm.
Đặt chảo chiên lên trên bề mặt bếp,
hạ xuống dần
Thả tay cầm chảo chiên.Thanh granola nằm trong ngăn kéo
trên đã mở của tủ.Chảo chiên trên bếp và
nhiệt độ được bật lên."Giữ tay cầm của ngăn kéo trên. Từ từ
và cẩn thận mở ngăn kéo. Tìm và
cầm thanh dinh dưỡng bằng tay của bạn. Đặt
thanh dinh dưỡng vào không gian trống bên trong
ngăn kéo. Thả thanh dinh dưỡng.""Nhẹ nhàng nắm núm điều chỉnh, căn chỉnh nó với
cài đặt bật, và đi đến chảo chiên.
Cầm tay cầm, nhấc chảo khỏi vị trí của nó,
đặt nó lên trên bếp, và nhẹ nhàng đặt xuống.
Thả tay cầm khi nó đã cố định.""Trong ngăn kéo trên đã mở của tủ,
thanh granola đã được
đặt cẩn thận.""Lúc này, bếp đang
bật với một chảo chiên trên đó."

Hướng dẫn tay kẹp của bạn về phía ổ bánh mì
Cẩn thận giữ bánh mì bằng tay kẹp của bạn
Đặt tay kẹp lên trên đĩa trắng
Nhẹ nhàng đặt bánh mì lên đĩa trắng.Ổ bánh mì được đặt trên
đĩa trắng."Vui lòng di chuyển tay kẹp của bạn gần bánh mì.
Nhẹ nhàng nắm bánh mì và lơ lửng trên
đĩa sứ. Sau đó, đặt bánh mì nhẹ nhàng
lên đĩa.""Bánh mì được sắp xếp trên
đĩa trắng."

Hình 3: Bộ Dữ Liệu Đặc Tả Tác Vụ Đa Phương Thức MUTEX. Chúng tôi cung cấp một bộ dữ liệu gồm 100 tác vụ mô phỏng (ví dụ trong cột đầu tiên) dựa trên LIBERO-100 [60] và 50 tác vụ thế giới thực (ví dụ trong cột thứ hai và thứ ba), được chú thích với 50 và 30 minh họa mỗi tác vụ, tương ứng. Chúng tôi chú thích mỗi tác vụ với 11 đặc tả tác vụ thay thế trong mỗi phương thức sau đây (các hàng từ trên xuống dưới): minh họa video, hình ảnh mục tiêu, hướng dẫn văn bản, mục tiêu văn bản, hướng dẫn giọng nói, và mục tiêu giọng nói.

Như một phần của nỗ lực phát triển một chính sách học bắt chước đa tác vụ thống nhất, chúng tôi xây dựng một bộ dữ liệu mới về các tác vụ với nhiều đặc tả tác vụ mỗi phương thức trong cả tác vụ mô phỏng và thế giới thực (xem Hình 3). Trong mô phỏng, chúng tôi mở rộng điểm chuẩn LIBERO-100 [60] bao gồm các tương tác vật thể đa dạng và tác vụ linh hoạt (n= 100) như "bật bếp và đặt chảo chiên lên đó." Mỗi tác vụ được chú thích với m= 50 quỹ đạo của con người (được cung cấp bởi các tác giả) và k= 11 đặc tả tác vụ mỗi phương thức. Do khoảng cách sim2real không thể tránh khỏi, các minh họa video trong mô phỏng được thu thập bằng cách điều khiển từ xa robot mô phỏng thay vì trực tiếp bởi con người thực hiện các tác vụ. Trong thế giới thực, chúng tôi thu thập một bộ dữ liệu mới với n= 50 tác vụ (Hình 5) từ các tác vụ nhặt và đặt như "đặt bánh mì lên đĩa trắng", "nhặt cái bát ở phía sau cảnh và đặt nó vào bên trong ngăn kéo trên" đến các tác vụ tiếp xúc phong phú hơn như "mở lò chiên không khí và đặt bát có xúc xích vào", "lấy khay của lò nướng ra, và đặt bánh mì lên đó." Tất cả các tác vụ được thu thập trong cùng một môi trường nhưng liên quan đến các vật thể khác nhau từ một tập hợp 17 vật thể. Mỗi tác vụ được minh họa với m= 30 quỹ đạo do con người thu thập [61] sử dụng một thiết bị điều khiển từ xa 3D spacemouse. Mỗi tác vụ được chú thích với k= 11 đặc tả tác vụ khác nhau

--- TRANG 6 ---
cho mỗi phương thức. Một con người thực hiện các minh họa video chỉ định các tác vụ bằng tay của họ. Để tạo ra 11 mô tả mục tiêu và hướng dẫn văn bản đa dạng, chúng tôi sử dụng ChatGPT với các gợi ý để tạo ra các mô tả thay thế mà chúng tôi lọc thủ công để tránh sử dụng các từ đồng nghĩa không khớp với các vật thể liên quan đến tác vụ đúng (ví dụ, sử dụng đĩa như một từ đồng nghĩa cho bát khi có các đĩa khác trong cảnh). Chúng tôi yêu cầu tín hiệu giọng nói từ một số nhân vật của dịch vụ Amazon Polly để tạo ra các mô tả giọng nói đa dạng.

4 Đánh Giá Thực Nghiệm
Thiết Lập Thực Nghiệm: Chúng tôi thực hiện đánh giá trên bộ dữ liệu đặc tả tác vụ đa phương thức mới được xây dựng, với tỷ lệ chia 80%/20% (8/3) tác vụ để đào tạo và kiểm tra. Để đánh giá, chúng tôi đưa robot vào cả các đặc tả tác vụ chưa thấy (từ tập kiểm tra) và các điều kiện ban đầu mới (tức là, vị trí của các vật thể). Đối với mỗi tác vụ được kiểm tra, chúng tôi đánh giá 20 thử nghiệm trong mô phỏng và 10 thử nghiệm trong thế giới thực. Kết quả báo cáo cho mỗi phương thức đánh giá là trung bình cho tất cả 20 thử nghiệm × 100 tác vụ × 3 seed trong mô phỏng và 10 thử nghiệm × 5 tác vụ trong thế giới thực. Trong đánh giá của chúng tôi, chúng tôi so sánh MUTEX với các mô hình được đào tạo đặc biệt cho mỗi phương thức chỉ sử dụng các đặc tả tác vụ trong phương thức đó. Điều này giống với hầu hết các công trình trước đây hiện có trong học bắt chước dựa trên video [11, 12], và học bắt chước có điều kiện mục tiêu [6, 9, 33, 4]. Vui lòng tham khảo Phụ lục 6.5 để biết chi tiết.

Thực Nghiệm: Trong đánh giá thực nghiệm của chúng tôi, chúng tôi hướng tới trả lời những câu hỏi sau:
1) Liệu một chính sách thống nhất có khả năng thực hiện tác vụ qua nhiều phương thức có vượt trội hơn các phương pháp được đào tạo đặc biệt từ và cho mỗi phương thức riêng lẻ không? Bảng 1 và Bảng 2 tóm tắt kết quả đánh giá của chúng tôi trong mô phỏng và thế giới thực, tương ứng. Trong cả hai trường hợp, chúng tôi quan sát thấy một sự cải thiện đáng kể (+10.3%, +14.3% trong mô phỏng và thế giới thực tương ứng) từ việc sử dụng chính sách thống nhất MUTEX của chúng tôi so với các mô hình đặc trưng phương thức, cho thấy rằng quy trình học liên phương thức từ MUTEX có thể tận dụng nhiều thông tin hơn từ các phương thức khác.
Ngoài ra, chúng tôi phân tích các lỗi trong đánh giá thế giới thực của chúng tôi và thấy rằng các biểu diễn được học bằng MUTEX khái quát tốt hơn cho các đặc tả tác vụ mới so với các biểu diễn đơn phương thức. Cụ thể, khi xử lý các đặc tả tác vụ chưa thấy, các thử nghiệm thất bại không được quy cho lỗi ghép BC, nơi chính sách hoàn thành chính xác một tác vụ có ý nghĩa ngữ nghĩa trong môi trường, mặc dù không phải tác vụ dự định (ví dụ, nhặt và đặt một vật thể khác). Chúng tôi thấy rằng trong các đường cơ sở đơn phương thức, 85 trong số 240 thử nghiệm (35.4%) là do thất bại hiểu các tác vụ được chỉ định, trong khi nó giảm xuống 40 trong số 240 (16.7%) với MUTEX.

2) Tầm quan trọng của hai giai đoạn trong quy trình đào tạo MUTEX là gì? Có quan trọng không khi thực hiện các giai đoạn liên tiếp? Bảng 1 bao gồm một so sánh kết quả thu được với quy trình đào tạo hai giai đoạn liên tiếp (MUTEX) so với đào tạo với cả hai giai đoạn đồng thời (đào tạo kết hợp), đào tạo không có giai đoạn đầu (không có mô hình hóa mặt nạ) hoặc không có giai đoạn thứ hai (không có khớp liên phương thức). Chúng tôi quan sát thấy sự giảm hiệu suất lớn nhất đến từ đào tạo không có mô hình hóa mặt nạ, cho thấy rằng bước này là quan trọng để học thông tin liên phương thức từ các đặc tả tác vụ khác nhau. Khớp liên phương thức cung cấp một sự thúc đẩy ngoại trừ minh họa video (chúng tôi khớp với phương thức đó) và quan sát một sự giảm nhỏ trong các đặc tả giọng nói.

3) Liệu hiệu suất có tăng đáng kể khi các tác vụ được chỉ định với nhiều phương thức không? Một trong những ưu điểm của MUTEX là nó có thể thực hiện tác vụ với một đặc tả duy nhất trong bất kỳ phương thức nào đã học hoặc với nhiều đặc tả trong một số phương thức. Chúng tôi đánh giá bằng cách sử dụng các kết hợp Mục Tiêu Văn Bản + Mục Tiêu Giọng Nói, Mục Tiêu Văn Bản + Hình Ảnh Mục Tiêu và Hướng Dẫn Giọng Nói + Minh Họa Video và đạt được tỷ lệ thành công 50.1, 59.2, và 59.6, tương ứng. Những giá trị này gần với hiệu suất sử dụng một đặc tả duy nhất trong tốt nhất của hai phương thức, cho thấy rằng phương thức bổ sung không cung cấp thông tin thêm nào. Chúng tôi đưa ra giả thuyết rằng đó là bởi vì tất cả thông tin liên phương thức có thể đã được học bởi MUTEX. Thú vị là, khi sử dụng đặc tả trong tất cả các phương thức, tỷ lệ thành công là 60.1, thấp hơn khi sử dụng Mục tiêu Hình ảnh hoặc Minh Họa Video, có thể do sự phức tạp tăng lên của việc diễn giải nhiều đặc tả tác vụ.

4) Liệu các biểu diễn đặc tả tác vụ được học bởi MUTEX có tốt hơn các mô hình đặc tả tác vụ tiên tiến không? Để đánh giá thêm hiệu quả của các biểu diễn MUTEX, chúng tôi so sánh

Phương Pháp Mục Tiêu Văn Bản Hướng Dẫn Văn Bản Hình Ảnh Mục Tiêu Minh Họa Video Hình Ảnh Mục Tiêu + Mục Tiêu Văn Bản
T5 [62] 40.0 44.0 - - -
R3M [63] - - 59.5 44.7 -
VIMA [30] - - - - 47.0
MUTEX 50.1 53.0 61.6 63.2 59.2

Bảng 3: Tỷ Lệ Thành Công trên Bộ Dữ Liệu Đặc Tả Tác Vụ Đa Phương Thức trong Mô Phỏng.

nó với các mô hình đặc tả tác vụ khác trong Bảng 3. Mặc dù các mô hình đơn phương thức, T5 và R3M, đạt kết quả tốt hơn CLIP (Bảng 1) trong Hướng Dẫn Văn Bản (+4.1%) và Hình Ảnh Mục Tiêu (+2%), tương ứng, MUTEX luôn luôn vượt trội hơn các mô hình này qua tất cả các phương thức. Sự cải thiện nhất quán qua các phương thức chứng minh giá trị của việc tận dụng nhiều phương thức trong quá trình đào tạo. Hơn nữa, MUTEX vượt trội đáng kể so với VIMA, một phương pháp gần đây sử dụng cả mục tiêu văn bản và hình ảnh vật thể để đặc tả tác vụ, nhấn mạnh rằng MUTEX không chỉ có hiệu suất cao hơn so với các đối tác đơn phương thức mà còn có thể sử dụng hiệu quả nhiều phương thức trong quá trình suy luận.

5 Kết Luận, Hạn Chế, và Công Việc Tương Lai
Chúng tôi chứng minh với các thực nghiệm toàn diện trong mô phỏng và thế giới thực rằng học đa tác vụ, khi được đào tạo trên các đặc tả tác vụ qua nhiều phương thức, tạo ra một chính sách mạnh mẽ và linh hoạt hơn trong mỗi phương thức. Chúng tôi rất được khuyến khích bởi các kết quả thực nghiệm và tiềm năng của MUTEX để thiết kế một giao diện giao tiếp robot-con người đa phương thức linh hoạt hơn.

Chúng tôi hướng tới việc cải thiện MUTEX trong công việc tương lai bằng cách giải quyết một số hạn chế. MUTEX giả định truy cập ghép nối đến tất cả các phương thức đặc tả tác vụ, điều này có thể khó khăn để thu được một cách có thể mở rộng. MUTEX sử dụng các tín hiệu giọng nói sạch được tổng hợp bởi Amazon Polly có thể không đại diện chính xác cho giọng nói thế giới thực, thường ồn ào hơn và khó hiểu hơn. MUTEX sử dụng mục tiêu video và hình ảnh được cung cấp từ cùng một không gian làm việc như tác vụ cần được thực hiện. Có một chính sách có thể thực hiện tác vụ được chỉ định bằng mục tiêu hoặc minh họa trực quan "trong tự nhiên" sẽ mời thêm các thách thức. Chúng tôi cũng dự định khám phá cách thúc đẩy khái quát mạnh mẽ hơn bằng cách đào tạo qua các môi trường đa dạng, điều này có thể mở ra cánh cửa cho việc sử dụng các bộ dữ liệu video của con người lớn hơn. Cuối cùng, MUTEX sử dụng bắt chước hành vi vanilla để học chính sách, có các vấn đề như chuyển đổi hiệp biến và lỗi ghép. Để giảm thiểu hạn chế này, việc kết hợp các kỹ thuật học bắt chước tương tác và học tăng cường là một hướng nghiên cứu thú vị cho tương lai.

Lời Cảm Ơn
Chúng tôi cảm ơn Yifeng Zhu và Huihan Liu cho việc phát triển cơ sở hạ tầng hệ thống robot thực. Chúng tôi cảm ơn Zhenyu Jiang, Jake Grigsby, và Hanwen Jiang vì đã cung cấp phản hồi hữu ích cho bản thảo này. Chúng tôi ghi nhận sự hỗ trợ của Quỹ Khoa học Quốc gia (1955523, 2145283), Văn phòng Nghiên cứu Hải quân (N00014-22-1-2204), UT Good Systems, và Phòng thí nghiệm Học Máy.

--- TRANG 9 ---
Tài Liệu Tham Khảo
[1] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, và H. Ben Amor. Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33:13139–13150, 2020.

[2] S. Nair, E. Mitchell, K. Chen, S. Savarese, C. Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. Trong Conference on Robot Learning, trang 1303–1315. PMLR, 2022.

[3] L. Shao, T. Migimatsu, Q. Zhang, K. Yang, và J. Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. The International Journal of Robotics Research, 40(12-14):1419–1434, 2021.

[4] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.

[5] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, và P. Florence. Interactive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.

[6] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik, A. A. Efros, và T. Darrell. Zero-shot visual imitation. Trong Proceedings of the IEEE conference on computer vision and pattern recognition workshops, trang 2050–2053, 2018.

[7] S. Nasiriany, V. Pong, S. Lin, và S. Levine. Planning with goal-conditioned policies. Advances in Neural Information Processing Systems, 32, 2019.

[8] Y. Cui, S. Niekum, A. Gupta, V. Kumar, và A. Rajeswaran. Can foundation models perform zero-shot task specification for robot manipulation? Trong Learning for Dynamics and Control Conference, trang 893–905. PMLR, 2022.

[9] Z. J. Cui, Y. Wang, N. Muhammad, L. Pinto, et al. From play to policy: Conditional behavior generation from uncurated robot data. arXiv preprint arXiv:2210.10047, 2022.

[10] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, và S. Levine. One-shot imitation from observing humans via domain-adaptive meta-learning. arXiv preprint arXiv:1802.01557, 2018.

[11] A. Bonardi, S. James, và A. J. Davison. Learning one-shot imitation from humans without humans. IEEE Robotics and Automation Letters, 5(2):3533–3539, 2020.

[12] S. James, M. Bloesch, và A. J. Davison. Task-embedded control networks for few-shot imitation learning. Trong Conference on robot learning, trang 783–795. PMLR, 2018.

[13] Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, và J. Liu. Uniter: Universal image-text representation learning. Trong Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX, trang 104–120. Springer, 2020.

[14] H. Tan và M. Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490, 2019.

[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, trang 8748–8763. PMLR, 2021.

[16] A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, và D. Kiela. Flava: A foundational language and vision alignment model. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 15638–15650, 2022.

--- TRANG 10 ---
[17] H. Xu, G. Ghosh, P.-Y. Huang, P. Arora, M. Aminzadeh, C. Feichtenhofer, F. Metze, và L. Zettlemoyer. Vlm: Task-agnostic video-language model pre-training for video understanding, 2021.

[18] H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti, và M. Zhou. Univl: A unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020.

[19] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, và Y. Choi. Merlot reserve: Neural script knowledge through vision and language and sound. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 16375–16387, 2022.

[20] M. A. Lee, Y. Zhu, K. Srinivasan, P. Shah, S. Savarese, L. Fei-Fei, A. Garg, và J. Bohg. Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks, 2019.

[21] Y. Li, J.-Y. Zhu, R. Tedrake, và A. Torralba. Connecting touch and vision via cross-modal prediction. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 10609–10618, 2019.

[22] I. Guzey, B. Evans, S. Chintala, và L. Pinto. Dexterity from touch: Self-supervised pre-training of tactile representations with robotic play, 2023.

[23] J. M. Clark và A. Paivio. Dual coding theory and education. Educational psychology review, 3:149–210, 1991.

[24] L. E. Bahrick và R. Lickliter. Intersensory redundancy guides attentional selectivity and perceptual learning in infancy. Developmental psychology, 36(2):190, 2000.

[25] C. Lynch và P. Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020.

[26] O. Mees, L. Hermann, và W. Burgard. What matters in language conditioned robotic imitation learning. arXiv preprint arXiv:2204.06252, 2022.

[27] V. Myers, A. He, K. Fang, H. Walke, P. Hansen-Estruch, C.-A. Cheng, M. Jalobeanu, A. Kolobov, A. Dragan, và S. Levine. Goal representations for instruction following: A semi-supervised language interface to control, 2023.

[28] H. Liu, L. Lee, K. Lee, và P. Abbeel. Instruction-following agents with jointly pre-trained vision-language models. arXiv preprint arXiv:2210.13431, 2022.

[29] A. Yu và R. J. Mooney. Using both demonstrations and language instructions to efficiently learn robotic tasks. arXiv preprint arXiv:2210.04476, 2022.

[30] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. Fei-Fei, A. Anandkumar, Y. Zhu, và L. Fan. Vima: General robot manipulation with multimodal prompts, 2023.

[31] G. Kwon, Z. Cai, A. Ravichandran, E. Bas, R. Bhotika, và S. Soatto. Masked vision and language modeling for multi-modal representation learning. arXiv preprint arXiv:2208.02131, 2022.

[32] L. Xue, M. Gao, C. Xing, R. Martín-Martín, J. Wu, C. Xiong, R. Xu, J. C. Niebles, và S. Savarese. Ulip: Learning unified representation of language, image and point cloud for 3d understanding. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.

--- TRANG 11 ---
[33] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, và C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. Trong Conference on Robot Learning, trang 991–1002. PMLR, 2022.

[34] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, và J. Carreira. Perceiver: General perception with iterative attention, 2021.

[35] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.

[36] R. Rahmatizadeh, P. Abolghasemi, L. Bölöni, và S. Levine. Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration. Trong 2018 IEEE international conference on robotics and automation (ICRA), trang 3758–3765. IEEE, 2018.

[37] P.-L. Guhur, S. Chen, R. G. Pinel, M. Tapaswi, I. Laptev, và C. Schmid. Instruction-driven history-aware policies for robotic manipulations. Trong Conference on Robot Learning, trang 175–187. PMLR, 2023.

[38] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.

[39] S. Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(1-3):335–346, 1990.

[40] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, et al. Experience grounds language. arXiv preprint arXiv:2004.10151, 2020.

[41] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, và A. Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training, 2023.

[42] P. Agrawal. The task specification problem. Trong Conference on Robot Learning, trang 1745–1751. PMLR, 2022.

[43] L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu, X. Huang, B. Li, C. Li, C. Liu, M. Liu, Z. Liu, Y. Lu, Y. Shi, L. Wang, J. Wang, B. Xiao, Z. Xiao, J. Yang, M. Zeng, L. Zhou, và P. Zhang. Florence: A new foundation model for computer vision, 2021.

[44] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, và I. Misra. Imagebind: One embedding space to bind them all, 2023.

[45] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. V. Le, Y. Sung, Z. Li, và T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021.

[46] R. Bachmann, D. Mizrahi, A. Atanov, và A. Zamir. Multimae: Multi-modal multi-task masked autoencoders, 2022.

[47] Z. Lin, S. Yu, Z. Kuang, D. Pathak, và D. Ramanan. Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models, 2023.

[48] Y. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou, X. Dai, L. Yuan, Y. Li, và J. Gao. Regionclip: Region-based language-image pretraining, 2021.

[49] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, K.-W. Chang, và J. Gao. Grounded language-image pre-training, 2022.

--- TRANG 12 ---
[50] X. Geng, H. Liu, L. Lee, D. Schuurmans, S. Levine, và P. Abbeel. Multimodal masked autoencoders learn transferable representations, 2022.

[51] J. Xu, S. D. Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, và X. Wang. Groupvit: Semantic segmentation emerges from text supervision, 2022.

[52] M. Li, L. Chen, Y. Duan, Z. Hu, J. Feng, J. Zhou, và J. Lu. Bridge-prompt: Towards ordinal action understanding in instructional videos, 2022.

[53] H. Tan và M. Bansal. Vokenization: Improving language understanding with contextualized, visual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.

[54] Z. Tang, J. Cho, H. Tan, và M. Bansal. Vidlankd: Improving language understanding via video-distilled knowledge transfer. Advances in Neural Information Processing Systems, 34: 24468–24481, 2021.

[55] A. J. Wang, Y. Ge, R. Yan, Y. Ge, X. Lin, G. Cai, J. Wu, Y. Shan, X. Qie, và M. Z. Shou. All in one: Exploring unified video-language pre-training, 2022.

[56] L. Li, Z. Gan, K. Lin, C.-C. Lin, Z. Liu, C. Liu, và L. Wang. Lavender: Unifying video-language understanding as masked language modeling, 2022.

[57] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, và I. Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.

[58] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[59] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, và H. Hu. Simmim: A simple framework for masked image modeling. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 9653–9663, 2022.

[60] B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, và P. Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning, 2023.

[61] Y. Zhu, A. Joshi, P. Stone, và Y. Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. 6th Annual Conference on Robot Learning, 2022.

[62] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, và P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer, 2020.

[63] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, và A. Gupta. R3m: A universal visual representation for robot manipulation, 2022.

[64] I. Loshchilov và F. Hutter. Decoupled weight decay regularization, 2019.

[65] I. Loshchilov và F. Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.

[66] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, và S. Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.

[67] K. He, X. Zhang, S. Ren, và J. Sun. Deep residual learning for image recognition, 2015.

[68] K. He, G. Gkioxari, P. Dollár, và R. Girshick. Mask r-cnn, 2018.

--- TRANG 13 ---
Phương Pháp Mục Tiêu Văn Bản Hướng Dẫn Văn Bản Hình Ảnh Mục Tiêu Minh Họa Video Mục Tiêu Giọng Nói Hướng Dẫn Giọng Nói
MUTEX (với biểu diễn đào tạo trước) 42.3 46.8 46.0 48.0 34.4 32.0
MUTEX (với tinh chỉnh mô hình đầy đủ) 37.1 40.2 44.7 46.3 31.6 34.2
MUTEX (với tinh chỉnh bộ mã hóa video) 49.6 53.4 57.6 61.3 38.4 46.0
MUTEX 50.1 53.0 61.6 63.2 40.9 46.0

Bảng 4: Thử Nghiệm Đào Tạo Mô Hình của các biến thể MUTEX.

6 Phụ Lục
6.1 Thử Nghiệm Đào Tạo Mô Hình
Trong đào tạo MUTEX, các biểu diễn đặc tả tác vụ được học cùng với mục tiêu học chính sách, tức là, bắt chước hành vi. Mục tiêu học chính sách giúp nắm bắt thông tin cần thiết cho dự đoán hành động robot, thiếu trong bộ dữ liệu đặc tả tác vụ. Để xác minh điều này một cách thực nghiệm, chúng tôi đào tạo trước các biểu diễn đặc tả tác vụ sử dụng quy trình đào tạo hai giai đoạn mà không sử dụng mất mát BC. Sau đó, chúng tôi học chính sách sử dụng các biểu diễn đặc tả tác vụ đóng băng (Được gọi là MUTEX (với biểu diễn đào tạo trước) trong Bảng 4). MUTEX có hiệu suất tốt hơn đáng kể (+10.9%) so với MUTEX (với biểu diễn đào tạo trước).

Khớp liên phương thức trong MUTEX nhằm làm phong phú các phương thức khác với đặc tả video được thông tin hơn thu được sau mô hình hóa mặt nạ. Để tránh cực tiểu cục bộ, nơi đặc tả video mất thông tin để khớp với các biểu diễn của các phương thức khác, chúng tôi đóng băng các lớp Chính sách MUTEX và Bộ Mã hóa Video. Để xác thực giả thuyết, chúng tôi đánh giá hiệu suất của hai biến thể MUTEX với tinh chỉnh mô hình đầy đủ (bao gồm bộ mã hóa video)- MUTEX (với tinh chỉnh mô hình đầy đủ) và tinh chỉnh bộ mã hóa video- MUTEX (với tinh chỉnh bộ mã hóa video). Kết quả được nhấn mạnh trong Bảng 4. Chúng tôi quan sát thấy rằng chiến lược đào tạo được áp dụng bởi MUTEX luôn luôn vượt trội hơn các biến thể khác, củng cố giả thuyết ban đầu của chúng tôi rằng lan truyền ngược độ dốc thông qua bộ mã hóa video trong khớp liên phương thức có thể dẫn đến hiệu suất dưới tối ưu.

6.2 Chi Tiết Đào Tạo
Để đào tạo MUTEX 3.2, theo thực hành tiêu chuẩn, chúng tôi sử dụng bộ tối ưu AdamW [64] cùng với Bộ Lập Lịch LR Cosine Annealing [65]. Để tạo điều kiện học một mô hình mạnh mẽ, chúng tôi áp dụng tăng cường dữ liệu như ColorJitter và tăng cường dịch chuyển trên các hình ảnh quan sát RGB. Các chi tiết siêu tham số được áp dụng từ đường cơ sở transformer của LIBERO [60], và các chi tiết được cung cấp trong Bảng 5. Để mô hình hóa mặt nạ, chúng tôi đào tạo MUTEX trong 50 epoch end-to-end, trong khi để khớp liên phương thức, các lớp chiếu [Hình 2] được tinh chỉnh trong 20 epoch. Mô hình được đào tạo sử dụng 2 GPU NVIDIA RTX A5000 (24 GB) với kích thước batch 64 sử dụng framework PyTorch [66] cho tất cả các đường cơ sở, và MUTEX với tổng cộng khoảng năm ngày cho mỗi thực nghiệm.

Mô hình Hóa Mặt Nạ: Chúng tôi sử dụng mô hình hóa mặt nạ để tạo điều kiện tương tác liên phương thức giữa các phương thức khác nhau [Phần 3.1]. Đối với phương thức văn bản, chúng tôi che giấu các từ cụ thể từ đầu vào và dự đoán các token từ tương ứng. Liên quan đến mục tiêu văn bản, chúng tôi chọn ngẫu nhiên một từ từ đặc tả đầu vào. Đối với hướng dẫn văn bản, chúng tôi liên tục che giấu hai từ ngẫu nhiên. Đáng chú ý là các từ dừng (ví dụ, 'a', 'the') không được che giấu, vì dự đoán các từ dừng không cung cấp thông tin hữu ích liên quan đến tác vụ. Liên quan đến các đặc tả trực quan, chúng tôi che giấu các đặc trưng được trích xuất từ CLIP thay vì trực tiếp dự đoán các giá trị pixel nhiễu. Chúng tôi sử dụng mất mát hồi quy L1 để đào tạo. Chúng tôi che giấu đặc trưng vùng trung gian thu được sau khối transformer thứ 22 trong CLIP cho mục tiêu hình ảnh. Đối với minh họa video, chúng tôi che giấu các đặc trưng khung hình riêng lẻ. Tương tự, đối với các đặc tả giọng nói, chúng tôi che giấu và dự đoán các đặc trưng trực tiếp thu được từ mô hình bộ mã hóa Whisper-Small được đào tạo trước cho cả mục tiêu giọng nói và hướng dẫn giọng nói.

--- TRANG 14 ---
Chi Tiết Đào Tạo
Kích Thước Batch 64
Cắt Gradient 100
Epoch (Mô hình Hóa Mặt Nạ) 50
Epoch (Khớp Liên Phương Thức) 20
Tỷ Lệ Học Ban Đầu 1e-4
Beta (0.9, 0.999)
Giảm Trọng Lượng 1e-4
Tỷ Lệ Học Tối Thiểu 1e-5
Trọng Lượng Mất Mát Mô Hình Hóa Mặt Nạ 0.5
Trọng Lượng Mất Mát Khớp Liên Phương Thức 0.5
Tăng Cường Dữ Liệu
Độ Sáng 0.3
Độ Tương Phản 0.3
Độ Bão Hòa 0.3
Màu Sắc 0.3
Dịch Chuyển 8 pixel

Bảng 5: Chi tiết siêu tham số để đào tạo MUTEX

6.3 Kiến Trúc Mô Hình
MUTEX tận dụng nhiều phương thức trong quá trình đào tạo để học hiệu quả các biểu diễn phong phú, thông tin đầy đủ của mỗi phương thức, cho phép chỉ định tác vụ sử dụng đơn lẻ hoặc bất kỳ kết hợp nào của các phương thức đặc tả tác vụ trong quá trình đánh giá. Để cho phép sự linh hoạt như vậy, chúng tôi áp dụng kiến trúc transformer (Chi tiết trong văn bản chính 3.2).

Các Bộ Mã Hóa Đặc Trưng Phương Thức trích xuất một nhúng duy nhất cho mỗi phương thức đặc tả tác vụ đầu vào. Để trích xuất các biểu diễn có ý nghĩa ngữ nghĩa, được gắn kết, mỗi phương thức được truyền qua một mô hình lớn được đào tạo trước, và sau đó qua một lớp chiếu [Hình 2]. Đối với (a) mô hình lớn được đào tạo trước, MUTEX sử dụng mô hình CLIP Large ViT-L/14[15] cho các phương thức văn bản và trực quan trong khi Whisper-Small [57] bộ mã hóa cho các phương thức giọng nói cung cấp các nhúng có ý nghĩa ngữ nghĩa. Các (b) lớp chiếu cho mỗi phương thức cho phép MUTEX học một nhúng chung được gắn kết cho tất cả các phương thức. Đối với các phương thức có nhúng đơn lẻ từ mô hình được đào tạo trước, chúng tôi sử dụng một lớp chiếu MLP [Tham khảo 1], trong khi đối với các phương thức có nhiều nhúng từ mô hình được đào tạo trước, chúng tôi sử dụng một khối transformer đơn lẻ với trung bình pooling [Tham khảo 2] để tổng hợp thông tin thành một nhúng. Các chi tiết của bộ mã hóa đặc trưng phương thức được tóm tắt trong Bảng 6.

Danh Sách 1: Kiến Trúc Mô Hình cho MLP trong Lớp Chiếu
MLPBlock(
nn.Linear(768, 512),
nn.ReLU()
nn.Dropout(0.1)
nn.Linear(512, 768)
nn.Dropout(0.1)
)

Danh Sách 2: Kiến Trúc Mô Hình cho Khối Transformer với Mean Pooling trong Lớp Chiếu
TransformerPoolBlock(
nn.LayerNorm(768),
nn.MultiheadAttention(
embed_dim = 768,
num_heads = 4,
kdim = 256,
vdim = 256
dropout = 0.1,

--- TRANG 15 ---
Phương Thức Mô Hình Lớn Được Đào Tạo Trước Hình Dạng Nhúng Được Đào Tạo Trước Lớp Chiếu
Mục Tiêu Văn Bản CLIP (1,768) MLPBlock[1] +
Khớp Liên Phương Thức MLPBlock[1] +
Chia Sẻ MLPBlock[1]
Hướng Dẫn Văn Bản CLIP (L,768) TransformerPoolBlock[2] +
Khớp Liên Phương Thức MLPBlock[1] +
L = số hướng dẫn Chia Sẻ MLPBlock[1]
Hình Ảnh Mục Tiêu CLIP (1,768) MLPBlock[1] +
Khớp Liên Phương Thức MLPBlock[1] +
Chia Sẻ MLPBlock[1]
Minh Họa Video CLIP (16,768) TransformerPoolBlock[2] +
Chia Sẻ MLPBlock[1]
Mục Tiêu Giọng Nói Whisper (4,768) TransformerPoolBlock[2] +
Khớp Liên Phương Thức MLPBlock[1] +
Chia Sẻ MLPBlock[1]
Hướng Dẫn Giọng Nói Whisper (4,768) TransformerPoolBlock[2] +
Khớp Liên Phương Thức MLPBlock[1] +
Chia Sẻ MLPBlock[1]

Bảng 6: Bộ Mã Hóa Đặc Trưng Phương Thức của MUTEX bao gồm một mô hình lớn được đào tạo trước và một lớp chiếu cho mỗi phương thức. Các nhúng được trích xuất từ mô hình được đào tạo trước được biến đổi thành các nhúng đơn lẻ sử dụng các lớp chiếu. Các lớp chiếu bao gồm một MLP hoặc khối Transformer đặc trưng phương thức, một khối MLP được thêm vào trong khớp liên phương thức, và một khối MLP được chia sẻ giữa tất cả các phương thức.

)
nn.LayerNorm(768),
TransformerFeedForwardNN(
nn.Linear(768, 256),
nn.GELU(),
nn.Dropout(0.1),
nn.Linear(256, 768),
nn.Dropout(0.1)
)
nn.AvgPool1d(kernel_size = num_embeddings)
)

Bộ Mã Hóa Chính Sách kết hợp các quan sát robot với các nhúng đặc tả tác vụ nhận được từ các bộ mã hóa đặc trưng phương thức. Các quan sát robot bao gồm một lịch sử (T= 10) của Hình Ảnh RGB từ hai góc nhìn camera (agentview, eyeinhand) và thông tin proprioceptive. Các hình ảnh RGB được chuyển đổi thành các nhúng token sử dụng bộ mã hóa ResNet [67] trong khi thông tin proprioceptive được mã hóa sử dụng một MLP [Tham khảo [60] để biết thêm chi tiết]. Các nhúng quan sát robot và nhúng đặc tả tác vụ được kết hợp sử dụng các lớp chú ý chéo và tự chú ý được xếp chồng như được hiển thị trong Hình 4, và các siêu tham số được tóm tắt trong Bảng 7.

Bộ Giải Mã Chính Sách nhận đầu vào từ bộ mã hóa chính sách và các truy vấn có thể học, cho phép MUTEX xuất ra một số lượng biến đổi các token. Những đầu ra này được sử dụng để dự đoán các giá trị token bị che giấu trong quá trình mô hình hóa mặt nạ và các giá trị hành động. Các vector truy vấn có thể học chỉ ra loại đầu ra tương ứng - {Hành Động, Mục Tiêu Văn Bản, Hướng Dẫn Văn Bản, Hình Ảnh Mục Tiêu, Minh Họa Video, Mục Tiêu Giọng Nói, Hướng Dẫn Giọng Nói}. Những vector này được thêm với các vector vị trí sinusoidal chỉ ra chỉ số hành động và vị trí của các token bị che giấu trong các truy vấn hành động và truy vấn mô hình hóa mặt nạ, tương ứng. Tương tự với bộ mã hóa chính sách, nó bao gồm các lớp chú ý chéo và tự chú ý được xếp chồng như được hiển thị trong Hình 4 và các siêu tham số được tóm tắt trong Bảng 7.

--- TRANG 16 ---
Chú Ý ChéoxN'Tự Chú ÝVQKChú Ý ChéoNhúng Quan Sát RobotNhúng Đặc Tả Tác VụVKKQQ
Bộ Mã Hóa Chính SáchBộ Giải Mã Chính SáchTruy Vấn Hành ĐộngTruy Vấn Token Bị Che Giấu
VChú Ý ChéoxNTự Chú ÝVQKVKQToken Được Dự Đoán

Hình 4: Kiến Trúc Bộ Mã Hóa và Bộ Giải Mã Chính Sách MUTEX

Bộ Mã Hóa Chính Sách
Kích Thước Nhúng Đầu Vào 768
Kích Thước Nhúng Truy Vấn Đầu Vào 64
Kích Thước Đầu Transformer 64
Số Đầu Transformer 6
Kích Thước Đầu Ra Transformer 64
Tỷ Lệ MLP Transformer 4.0
Dropout 0.1
Attention Dropout 0.1
Số Lớp Tự Chú Ý 3
Số Lớp Chú Ý Chéo 4
Bộ Giải Mã Chính Sách
Kích Thước Nhúng Đầu Vào 64
Kích Thước Nhúng Truy Vấn Đầu Vào 64
Kích Thước Đầu Transformer 16
Số Đầu Transformer 4
Kích Thước Đầu Ra Transformer 64
Tỷ Lệ MLP Transformer 4.0
Dropout 0.1
Attention Dropout 0.1
Số Lớp Tự Chú Ý 4
Số Lớp Chú Ý Chéo 4

Bảng 7: Chi tiết siêu tham số của kiến trúc Bộ Mã Hóa và Bộ Giải Mã Chính Sách của MUTEX.

Đầu Dự Đoán [Không được hiển thị trong Hình 2 vì đơn giản] sử dụng đầu ra của bộ giải mã chính sách được sử dụng để dự đoán các giá trị token bị che giấu trong quá trình mô hình hóa mặt nạ và các giá trị hành động. Đối với mô hình hóa văn bản, các nhúng được ánh xạ đến kích thước từ vựng sử dụng một MLP hai lớp và được đào tạo sử dụng mất mát entropy chéo. Đối với mô hình hóa trực quan và giọng nói, các đặc trưng trung gian bị che giấu được dự đoán sử dụng một MLP một lớp và được đào tạo sử dụng mất mát L1. Đối với mô hình hóa hành động, các giá trị hành động được dự đoán sử dụng một MLP hai lớp. Phân phối hành động được mô hình hóa như một Mô hình Hỗn hợp Gaussian với năm thành phần để kết hợp tính đa phương thức trong các minh họa do con người thu thập và được đào tạo sử dụng mất mát negative log-likelihood.

--- TRANG 17 ---
6.4 Chi Tiết Bộ Dữ Liệu Đa Phương Thức
Dữ Liệu Robot MUTEX: MUTEX sử dụng bắt chước hành vi để học từ các bộ dữ liệu chuyên gia được thu thập bởi con người sử dụng điều khiển từ xa. Chúng tôi đánh giá trên hai môi trường: mô phỏng với 100 tác vụ và tác vụ thao tác thế giới thực [Tham khảo 3.3]. Đối với mô phỏng, chúng tôi sử dụng bộ thử nghiệm thao tác LIBERO 100 [60] bao gồm các tác vụ đại diện tốt hơn cho thế giới thực. Chúng tôi sử dụng bộ dữ liệu được cung cấp bởi các tác giả gốc, bao gồm 50 quỹ đạo mỗi tác vụ được thu thập sử dụng SpaceMouse. Đối với đánh giá thế giới thực, chúng tôi thiết kế 50 tác vụ trong thế giới thực bao gồm các tác vụ nhặt và đặt như "Đặt bánh mì lên đĩa trắng" đến các tác vụ phức tạp hơn như "Mở lò chiên không khí và đặt bát có xúc xích vào". 50 tác vụ được chia thành tám cảnh khác nhau, với độ dài đường chân trời trung bình là 242 cho mỗi tác vụ. Danh sách đầy đủ các tác vụ, cùng với hình ảnh hóa cảnh, được cung cấp trong Hình 5. Chúng tôi sử dụng robot Franka Panda với camera workspace Kinect Azure và camera Intel RealSense cho góc nhìn eye-in-hand. Chúng tôi sử dụng SpaceMouse để thu thập 30 quỹ đạo mỗi tác vụ, cho phép chúng tôi thu thập 1,500 quỹ đạo. Bộ dữ liệu được công khai tại https://ut-austin-rpl.github.io/MUTEX/, và chúng tôi hy vọng nó sẽ phục vụ như một tài nguyên cho nghiên cứu tương lai trong các tác vụ thao tác thế giới thực. Cơ sở hạ tầng robot thực được áp dụng từ [61].

Danh Sách 3: Gợi Ý GPT4 để tạo ra các chú thích của tác vụ
# Gợi ý GPT4 để tạo ra đặc tả văn bản đơn lẻ
# Để tạo ra example_paragraph cho hướng dẫn văn bản
"Tôi sẽ đưa bạn hướng dẫn; bạn nên chia nhỏ nó thành các hướng dẫn nhỏ hơn
cho robot. Vui lòng làm điều đó mà không thêm bất kỳ vật thể nào khác trong môi trường
trong các hướng dẫn. Chia nhỏ thành 3-4 hướng dẫn, ví dụ, 'nhặt
quả táo': 'Di chuyển tay kẹp của bạn về phía quả táo. Giữ quả táo nhẹ nhàng bằng
tay kẹp của bạn. Nâng quả táo khỏi bề mặt.' Một số điểm cần xem xét khi
tạo hướng dẫn: Robot chỉ có thể nắm một vật thể tại một thời điểm. Tất cả
vật thể đều ở phía trước robot, và robot không cần di chuyển.
Hãy cho tôi một ví dụ như vậy cho mọi hướng dẫn. Mỗi ví dụ phải hoàn thành tác vụ.
Đây là hướng dẫn tác vụ {task_instruction}"

# Gợi ý để tạo ra nhiều đặc tả văn bản sử dụng đặc tả văn bản đơn lẻ được tạo ra trước
"Thay thế các từ bằng từ đồng nghĩa hoặc diễn đạt lại câu. Sử dụng các từ mà người
nói tiếng Anh sẽ sử dụng trong thói quen hàng ngày và tiếng Anh đơn giản. Bạn cũng có thể
thêm các từ để làm cho nó lịch sự hơn, như xin vui lòng, cẩn thận, thận trọng,
vv. Hãy cho tôi mười ví dụ như vậy về các đoạn văn được diễn đạt lại. Đây là đoạn văn
cần diễn đạt lại: {example_paragraph}"

# Gợi ý để tạo ra chú thích giọng nói sử dụng chú thích văn bản
"Bạn là một người đang nói chuyện với robot. Viết lại mỗi đoạn văn bằng tiếng Anh đơn giản,
tự nhiên, và lịch sự. Đảm bảo đầu vào là câu khẳng định; giữ nó
khẳng định. Nếu đầu vào là mệnh lệnh, giữ nó mệnh lệnh. Đây là {
text_annotations} đoạn văn như vậy, viết lại mỗi đoạn văn riêng biệt:\n"

Mã Diễn Giả Tên Diễn Giả
Diễn Giả Đào Tạo
en-IN Aditi, Raveena
en-AU Nicole, Russell
en-US Joanna, Matthew
en-GB Amy, Brian
Diễn Giả Đánh Giá
en-US Salli
en-GB Emma

Bảng 8: Mã Amazon Polly cho các diễn giả trong chú thích giọng nói.

Chú Thích Đặc Tả Tác Vụ MUTEX: MUTEX tận dụng nhiều phương thức để chỉ định tác vụ, bao gồm mục tiêu văn bản, hướng dẫn văn bản, minh họa video, hình ảnh mục tiêu, mục tiêu giọng nói, và hướng dẫn

--- TRANG 18 ---
Kích Thước Mô Hình Baseline Đặc Trưng Phương Thức
Mô hình Mục Tiêu Văn Bản Hướng Dẫn Văn Bản Hình Ảnh Mục Tiêu Minh Họa Video Mục Tiêu Giọng Nói Hướng Dẫn Giọng Nói
S (13M) 47 47 54 66 34 32
M (14M) 30 30 62 67 20 25
L (15M) 46 33 56 61 14 21

Bảng 9: Thử nghiệm kích thước mô hình cho các baseline: Chúng tôi lặp lại ba kích thước mô hình khác nhau cho mỗi baseline đặc trưng phương thức để tránh thiên vị thực nghiệm trong việc lựa chọn kích thước mô hình và báo cáo kết quả sử dụng mô hình tốt nhất trong Bảng 1. Lưu ý rằng kết quả được thu thập sử dụng một seed và được tính trung bình trên 100 tác vụ × 20 thử nghiệm.

giọng nói. Trong quá trình đào tạo, chúng tôi giả định truy cập vào tất cả các phương thức của đặc tả tác vụ cho mỗi tác vụ, trong đó mỗi phương thức giúp củng cố các phương thức khác của đặc tả. Do đó, chúng tôi có thể chỉ định tác vụ sử dụng bất kỳ phương thức riêng lẻ nào trong quá trình đánh giá hiệu quả hơn. Chúng tôi chú thích với 11 đặc tả tác vụ mỗi phương thức mỗi tác vụ [Tham khảo Hình 3 để biết ví dụ về đặc tả tác vụ]. Chúng tôi tuân theo giao thức tiêu chuẩn của việc chia 80%/20% train/eval cho các chú thích đặc tả tác vụ. Tất cả kết quả được báo cáo trên tập đánh giá đặc tả tác vụ chưa thấy. Đối với mô phỏng, để tránh khoảng cách real2sim, chúng tôi thu thập minh họa video sử dụng điều khiển từ xa robot trong mô phỏng [chỉ thông tin RGB được sử dụng để chỉ định tác vụ]. Đối với tác vụ thế giới thực, minh họa của con người được sử dụng để chỉ định tác vụ, cho phép một cách trực quan hơn để chỉ định tác vụ cho người dùng so với điều khiển từ xa. Đối với đặc tả văn bản, chúng tôi tận dụng GPT4 để tạo ra đặc tả tác vụ ban đầu cho các tác vụ khác nhau với các biến thể đa dạng. Các đặc tả tác vụ được tạo ra được con người sửa để đảm bảo chất lượng của các chú thích đặc tả tác vụ. Đối với đặc tả giọng nói, chúng tôi tuân theo một quy trình tương tự với chú thích văn bản và, cuối cùng, sử dụng dịch vụ Amazon Polly để tạo ra đặc tả giọng nói. Gợi ý được sử dụng để tạo ra chú thích cho đặc tả văn bản và giọng nói được cung cấp trong Phụ lục 3, và các chi tiết của các nhân vật Amazon Polly được cung cấp trong Bảng 8.

6.5 Chi Tiết Baseline
Chúng tôi so sánh MUTEX với các baseline đặc trưng phương thức cho mỗi phương thức đặc tả tác vụ, tức là, Mục Tiêu Văn Bản, Hướng Dẫn Văn Bản, Hình Ảnh Mục Tiêu, Minh Họa Video, Mục Tiêu Giọng Nói, và Hướng Dẫn Giọng Nói. Để nhấn mạnh tầm quan trọng của đào tạo đa phương thức được đề xuất trong công trình của chúng tôi, chúng tôi không thay đổi kiến trúc chính sách [Tham khảo 6.3], siêu tham số đào tạo BC [Tham khảo 6.2] trong các baseline. Hơn nữa, để tránh thiên vị thực nghiệm do kích thước mô hình, chúng tôi lặp lại ba kích thước mô hình khác nhau cho mỗi baseline đặc trưng phương thức với việc tăng các lớp tự chú ý và chú ý chéo và sử dụng kích thước mô hình có hiệu suất tốt nhất cho baseline phương thức tương ứng. Các baseline được thử nghiệm được nhấn mạnh trong Bảng 9. Các baseline Đặc Trưng Phương Thức trong Bảng 1 sử dụng cùng mô hình được đào tạo trước như phương thức tương ứng trong MUTEX, tức là, CLIP [15] cho Mục Tiêu Văn Bản, Hướng Dẫn Văn Bản, Hình Ảnh Mục Tiêu, Minh Họa Video và WHISPER [57] cho Mục Tiêu Giọng Nói, Hướng Dẫn Giọng Nói. Trong Bảng 3, đối với T5 (với Mục Tiêu Văn Bản) và T5 (với Hướng Dẫn Văn Bản), chúng tôi sử dụng mô hình T5-small [62] để mã hóa các đặc tả văn bản tương tự như các công trình học bắt chước có điều kiện ngôn ngữ [2, 33]. Tương tự, chúng tôi sử dụng đặc trưng R3M [63] trong R3M (với Hình Ảnh Mục Tiêu) và R3M (với Minh Họa Video). Chúng tôi cũng so sánh với VIMA [30], sử dụng hình ảnh vật thể và mục tiêu văn bản để chỉ định tác vụ. Đối với baseline VIMA, chúng tôi nối các đặc trưng mục tiêu văn bản với sáu vị trí cắt vật thể hàng đầu (đệm bằng số không nếu khác) thu được từ Hình Ảnh Mục Tiêu sử dụng Mask-RCNN [68]. Chúng tôi quan sát thấy rằng một mô hình duy nhất, MUTEX, luôn luôn vượt trội hơn tất cả các baseline trong mỗi phương thức đánh giá. Điều này chứng minh rằng đào tạo MUTEX trên đặc tả tác vụ đa phương thức có hiệu suất tốt hơn so với đào tạo với các phương thức riêng lẻ hoặc một tập con các phương thức.

--- TRANG 19 ---
Thuật Toán 1 Mã Giả cho đào tạo MUTEX.
đầu vào: Dữ liệu đào tạo: quỹ đạo robot chuyên gia, đặc tả tác vụ ti={Li, li, Vi, vi, Si, si} cho
mỗi tác vụ Ti, trong đó
L:Hướng Dẫn Văn Bản
l:Mục Tiêu Văn Bản
V:Minh Họa Video
v:Hình Ảnh Mục Tiêu
S:Hướng Dẫn Giọng Nói
s:Mục Tiêu Giọng Nói
θ:Trọng Số Chính Sách
θx:Trọng số lớp chiếu của phương thức x∈ {L, l, V, v, S, s };θx⊂θ
đầu ra: Mạng chính sách với trọng số đã cập nhật

Bước 1:
cho epoch = 1 đến n làm
# Quan Sát (o), Hành Động Chuyên Gia (ae), Đặc Tả Tác Vụ (t∈ {L, l, V, v, S, s })
cho mỗi batch (o, ae, t) làm
Lấy mẫu k phương thức đặc tả tác vụ: t′⊆t
t′=áp dụng che giấu (t′) nếu k >1
# Dự đoán: (hành động, token bị che giấu)
â,t̂=πθ(o, t′)
# Tính toán mất mát và cập nhật trọng số chính sách θ
LBC=−∑i ae,i log(âi)
Lmô hình hóa mặt nạ =L(t̂, t) nếu k >1
L1 mất mát hồi quy cho {V, v, S, s }
mất mát entropy chéo cho {L, l}
kết thúc
kết thúc
# Đóng băng tất cả trọng số chính sách ngoại trừ lớp chiếu [Tham khảo hình 2] của L,l,v,S,s

Bước 2:
cho epoch = 1 đến n′ làm
# Quan Sát (o), Hành Động Chuyên Gia (ae), Đặc Tả Tác Vụ (t∈ {L, l, V, v, S, s })
cho mỗi batch (o, ae, t) làm
Lấy mẫu k phương thức đặc tả tác vụ: t′⊆t
# Dự đoán: (hành động)
â=πθ(o, t′)
# Tính toán mất mát và cập nhật trọng số chính sách θL, θl, θv, θS, θs
LBC=−∑i ae,i log(âi)
Lkhớp liên phương thức =L1⟨πθV(V), πθt′(t′)⟩
kết thúc
kết thúc

--- TRANG 20 ---
Hình Ảnh Hóa Cảnh Mô Tả Tác Vụ Độ Dài Đường Chân Trời Trung Bình
mở ngăn kéo dưới 171
mở ngăn kéo trên 180
mở ngăn kéo trên và đặt cái bát ở phía sau cảnh vào bên trong nó 415
mở ngăn kéo trên và đặt thanh granola vào bên trong nó 426
đặt thanh granola vào ngăn sau của giỏ 185
đặt thanh granola vào ngăn trước của giỏ 197
kéo khay của lò nướng ra và đặt cốc xanh lên đó 398
kéo khay của lò nướng ra và đặt bát đỏ lên đó 398
đặt cốc xanh vào giỏ 143
đặt hộp mac and cheese vào giỏ 129
kéo khay của lò nướng ra 188
đặt bát đỏ vào giỏ 143
đặt thanh granola vào bên trong giỏ 135
đặt thanh granola vào bên trong ngăn kéo trên 163
đặt cốc hồng vào bên trong giỏ 176
đặt bát đỏ vào bên trong giỏ 150
đặt cốc hồng vào bên trong ngăn kéo trên và đóng ngăn kéo 375
đặt bát đỏ vào bên trong ngăn kéo trên và đóng ngăn kéo 410
đóng ngăn kéo trên và mở ngăn kéo dưới 277
kéo khay của lò nướng ra 182
kéo khay của lò nướng ra và đặt bát có xúc xích lên khay 436
kéo khay của lò nướng ra và đặt cốc đỏ lên khay 403
đặt cuốn sách vào ngăn sau của giỏ 193
đặt cuốn sách vào ngăn trước của giỏ 199
đặt cốc đỏ vào ngăn sau của giỏ 161
đẩy khay của lò nướng vào 174
đặt cốc xanh lên khay lò nướng 167
đặt cốc xanh lên đĩa trắng 146
đặt bánh mì lên khay lò nướng và đẩy nó vào lò 429
đặt bánh mì lên đĩa trắng 148
đặt bát vàng lên khay lò nướng và đẩy nó vào lò 418
đặt bát vàng lên đĩa trắng 143
mở lò chiên không khí 208
mở lò chiên không khí và đặt bát có xúc xích vào đó 410
mở lò chiên không khí và đặt bánh mì vào đó 426
đặt bát có xúc xích lên đĩa trắng 155
đặt bánh mì lên đĩa trắng 145
đặt cốc hồng lên đĩa trắng 156
mở lò chiên không khí và đặt bát xanh vào bên trong nó 388
mở lò chiên 182
đặt cuốn sách vào ngăn sau của giỏ 194
đặt cuốn sách vào ngăn trước của giỏ 193
đặt cốc đỏ vào ngăn trước của giỏ 158
đặt bát xanh vào ngăn sau của giỏ 146
mở ngăn kéo dưới 157
mở ngăn kéo trên 172
mở ngăn kéo trên và đặt cốc xanh vào bên trong nó 404
mở ngăn kéo trên và đặt cốc hồng vào bên trong nó 432
đặt cốc xanh lên đĩa trắng 153
đặt bánh mì lên đĩa trắng 158
Tổng số tác vụ = 50 241.9

Hình 5: Hình ảnh hóa tác vụ thế giới thực MUTEX: Tác vụ thế giới thực MUTEX bao gồm các tác vụ đa dạng như "mở ngăn kéo trên", "đặt bánh mì lên khay lò nướng", "đặt bát có xúc xích vào bên trong lò chiên không khí sau khi mở nó" thể hiện độ phức tạp của các tác vụ trong môi trường nhà bếp gia đình thực.
