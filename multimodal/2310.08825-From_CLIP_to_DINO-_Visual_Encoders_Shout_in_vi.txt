# Từ CLIP đến DINO: Các Bộ Mã Hóa Thị Giác Gọi To Trong Các Mô Hình Ngôn Ngữ Lớn Đa Phương Thức

Dongsheng Jiang1⋆, Yuchen Liu2∗, Songlin Liu1∗, Jin'e Zhao3, Hao Zhang3,
Zhen Gao3, Xiaopeng Zhang1, Jin Li2, Hongkai Xiong2.
1Huawei Cloud2Shanghai Jiao Tong University3Yunding Technology
dongsheng_jiang@outlook.com, wspolsl@gmail.com, zxphistory@gmail.com
{liuyuchen6666, deserve_lj, xionghongkai}@sjtu.edu.cn

**Tóm tắt.** Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) đã đạt được những bước tiến đáng kể trong việc mở rộng khả năng của các Mô hình Ngôn ngữ Lớn (LLMs) thông qua việc kết hợp các giao diện nhận thức thị giác. Bất chấp sự xuất hiện của các ứng dụng thú vị và tính khả dụng của dữ liệu điều chỉnh hướng dẫn đa dạng, các phương pháp hiện tại thường dựa vào CLIP hoặc các biến thể của nó làm nhánh thị giác, và chỉ trích xuất đặc trưng từ các tầng sâu. Tuy nhiên, các phương pháp này thiếu phân tích toàn diện về các bộ mã hóa thị giác trong MLLMs. Trong bài báo này, chúng tôi tiến hành một cuộc điều tra sâu rộng về hiệu quả của các bộ mã hóa thị giác khác nhau trong MLLMs. Các phát hiện của chúng tôi cho thấy rằng các đặc trưng tầng nông của CLIP mang lại lợi thế đặc biệt cho các tác vụ chi tiết như định vị và hiểu biết vùng. Đáng ngạc nhiên, mô hình chỉ có thị giác DINO, không được tiền huấn luyện với sự liên kết văn bản-hình ảnh, cho thấy hiệu suất đầy hứa hẹn như một nhánh thị giác trong MLLMs. Bằng cách đơn giản trang bị nó với một tầng MLP để liên kết, DINO vượt trội hơn CLIP trong các tác vụ nhận thức liên quan đến chi tiết. Dựa trên những quan sát này, chúng tôi đề xuất một chiến lược hợp nhất đặc trưng đơn giản nhưng hiệu quả, được đặt tên là COMM, tích hợp CLIP và DINO với Hợp nhất đặc trưng Đa tầng, để tăng cường khả năng thị giác của MLLMs. Chúng tôi đánh giá COMM thông qua các thí nghiệm toàn diện trên một loạt các tiêu chuẩn, bao gồm chú thích hình ảnh, trả lời câu hỏi thị giác, định vị thị giác, và ảo giác đối tượng. Kết quả thí nghiệm chứng minh hiệu suất vượt trội của COMM so với các phương pháp hiện tại, thể hiện khả năng thị giác được tăng cường của nó trong MLLMs.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLMs) đã đạt được những bước tiến đáng kể trong các lĩnh vực hiểu biết và tạo sinh ngôn ngữ, đạt được tiến bộ đáng chú ý gần đây. Thông qua điều chỉnh hướng dẫn, các LLMs hiện tại thể hiện tính linh hoạt của chúng như các mô hình đa năng có khả năng xử lý một loạt các tác vụ. Khả năng này mở ra tiềm năng học tập zero-shot của chúng, cho phép chuyển đổi tác vụ một cách liền mạch được hướng dẫn bởi các chỉ dẫn. Dựa trên hiệu suất đầy hứa hẹn của LLMs, các nhà nghiên cứu hiện được thúc đẩy để tăng cường khả năng của chúng bằng cách kết hợp các tín hiệu thị giác như đầu vào. Sự mở rộng này cho phép tạo ra các đầu ra văn bản có liên quan chặt chẽ đến nội dung thị giác, mở ra những khả năng thú vị trong lĩnh vực hiểu biết thị giác-ngôn ngữ.

Để đạt được mục tiêu này, Flamingo và BLIP2 liên kết các LLMs mạnh mẽ với một bộ mã hóa thị giác được đóng băng để hiểu đầu vào thị giác và thực hiện các tác vụ thị giác-ngôn ngữ khác nhau. Một loạt các nghiên cứu tiếp theo, LLaVA, InstructBLIP, MiniGPT-4 và mPLUG-OWL tiếp tục cải thiện khả năng tuân theo hướng dẫn của con người bằng cách xây dựng các bộ dữ liệu hướng dẫn đa phương thức để huấn luyện. Tuy nhiên, các phương pháp này được xây dựng dựa trên sự liên kết cấp hình ảnh, gặp phải hạn chế trong hiểu biết chi tiết (như mô tả vùng và suy luận) và vấn đề ảo giác đối tượng nghiêm trọng. Để giải quyết điều này, GPT4ROI đề xuất điều chỉnh hướng dẫn trên vùng quan tâm và mở khóa các khả năng đa phương thức cấp vùng. Kosmos-2 và Shikra tiếp tục tích hợp khả năng định vị vào LLMs và mở khóa khả năng tham chiếu trong hội thoại, tức là cho phép người dùng chỉ đến đối tượng hoặc vùng như đầu vào và mô hình phản hồi với tọa độ không gian của các hộp bao quanh. Khả năng định vị như vậy có thể hoàn thành nhiều tác vụ thị giác-ngôn ngữ, đây là một tiến bộ lớn trong MLLMs.

Bất chấp sự đa dạng của các phương pháp và ứng dụng thú vị, hầu hết các MLLM đa phương thức hiện tại sử dụng CLIP hoặc các biến thể của nó làm nhánh thị giác, trong đó các đặc trưng đầu ra từ các tầng sâu (ví dụ: tầng áp chót) thường được sử dụng làm đầu vào cho các bộ giải mã ngôn ngữ. Tuy nhiên, vẫn thiếu phân tích rằng: Liệu việc sử dụng các đặc trưng CLIP thông thường làm bộ mã hóa thị giác có phải là cách tốt nhất cho MLLMs? Mặc dù bộ mã hóa thị giác của CLIP rõ ràng được liên kết tốt với không gian nhúng từ bằng học tập tương phản hình ảnh-văn bản, nó không thể học được thông tin cấp pixel chi tiết hơn như màu sắc và định vị do giám sát toàn cục của chú thích hình ảnh, điều này có thể cản trở khả năng nhận thức chi tiết trong MLLMs. Bên cạnh đó, các MLLMs hiện tại có các bộ mã hóa thị giác và ngôn ngữ khá mất cân bằng (ví dụ: ViT-Large-300M so với Vicuna-7B/13B). Vì các mô hình ngôn ngữ đã thành công trong việc mở rộng quy mô mô hình với khả năng ngôn ngữ mạnh mẽ dần dần, tấm ngắn của Hiệu ứng Thùng cho MLLMs nằm ở các mô hình thị giác, không thể thể hiện khả năng nổi bật và gặp phải khoảng cách miền và khả năng zero-shot hạn chế. Do đó, việc tăng cường khả năng thị giác là rất quan trọng để thúc đẩy MLLMs.

Bài báo này trình bày một cuộc điều tra sâu rộng về các bộ mã hóa thị giác khác nhau cho MLLMs. Bốn mô hình nền tảng thị giác điển hình được xem xét, tức là: học tập tương phản hình ảnh-văn bản CLIP, học tập tương phản chỉ hình ảnh DINOv2, mô hình hóa hình ảnh có mặt nạ MAE và học tập có giám sát DeiT. Chúng tôi đánh giá hiệu suất trên các tác vụ thị giác-ngôn ngữ thường được sử dụng bao gồm định vị thị giác, ảo giác đối tượng, trả lời câu hỏi thị giác, chú thích hình ảnh và tiêu chuẩn MME. Phân tích của chúng tôi tiết lộ rằng các tầng đặc trưng khác nhau thể hiện các thiên hướng khác nhau đối với các mẫu cục bộ và toàn cục. Các đặc trưng tầng nông chứa thông tin chi tiết cấp thấp chứng minh có lợi cho các tác vụ nhận thức chi tiết như khả năng định vị và định vị, trong khi các đặc trưng tầng sâu vượt trội trong hiểu biết toàn cục. Để tăng cường biểu diễn, chúng tôi đề xuất một chiến lược hợp nhất đặc trưng đa tầng kết hợp cả đặc trưng cấp thấp và cấp cao. Đáng ngạc nhiên, khi được trang bị một tầng MLP để liên kết, mô hình chỉ có thị giác DINOv2 cho thấy tiềm năng như một nhánh thị giác cho MLLMs. Chúng tôi gán điều này cho thông tin định vị chi tiết được DINOv2 nắm bắt. Ngược lại, MAE và DeiT hoạt động kém hơn như các nhánh thị giác cho MLLMs. MAE học được thông tin ngữ nghĩa hạn chế, trong khi huấn luyện có giám sát mạnh mẽ của DeiT làm cho việc liên kết với không gian văn bản trở nên thách thức. Dựa trên các quan sát trên, chúng tôi đề xuất một chiến lược hợp nhất tích hợp CLIP và DINO với Hợp nhất đặc trưng Đa tầng, được gọi là COMM, để thúc đẩy các nhánh thị giác của MLLMs. Kết quả thí nghiệm chứng minh lợi thế rõ ràng của mô hình được đề xuất so với các phương pháp hiện tại và làm nổi bật khả năng thị giác được tăng cường do COMM mang lại. Tóm lại, những đóng góp của bài báo này được tóm tắt như sau:

- Chúng tôi là những người đầu tiên điều tra sâu rộng về hiệu quả của các bộ mã hóa thị giác khác nhau cho MLLMs. Dựa trên phân tích rằng các đặc trưng tầng nông chứa thông tin chi tiết cấp thấp hữu ích cho các tác vụ chi tiết, chúng tôi đề xuất một chiến lược hợp nhất đặc trưng đa tầng để kết hợp các đặc trưng cấp thấp và cấp cao để cải thiện biểu diễn.

- Phân tích của chúng tôi chỉ ra rằng DINOv2 chỉ có thị giác đạt được kết quả đầy hứa hẹn trong MLLMs chỉ với một tầng MLP để liên kết. Xem xét thông tin pixel chi tiết trong DINOv2 và thông tin ngữ nghĩa toàn cục trong CLIP, chúng tôi đề xuất COMM để hợp nhất các nhúng thị giác của hai mô hình này để tăng cường khả năng thị giác cho việc thúc đẩy MLLMs.

- Các thí nghiệm sâu rộng trên một loạt các tác vụ bao gồm định vị thị giác, tạo biểu thức tham chiếu, ảo giác đối tượng, trả lời câu hỏi thị giác và chú thích hình ảnh chứng minh sự vượt trội của COMM so với các nghiên cứu hiện tại.

## 2 Nghiên cứu Liên quan

**Mô hình Ngôn ngữ Lớn Đa phương thức.** LLMs đã thu hút sự chú ý đáng kể từ cả học viện và công nghiệp do khả năng hiểu biết và tạo sinh đáng chú ý của chúng. Thành công của LLMs đã thúc đẩy các nhà nghiên cứu khám phá việc tích hợp thị giác vào các mô hình này, dẫn đến sự phát triển của các MLLM đa phương thức mạnh mẽ. Flamingo sử dụng một mô-đun chú ý chéo để trích xuất bối cảnh thị giác, được nối với token văn bản như đầu vào cho LLMs. LLaVA và FROMAGe tận dụng bộ mã hóa thị giác của CLIP để trích xuất các đặc trưng thị giác, được liên kết với các đặc trưng văn bản bằng một tầng tuyến tính duy nhất và sau đó đầu vào cho LLMs. Các mô hình như BLIP-2, mPLUG-OWL, MiniGPT-4 và InstructBLIP sử dụng Q-former để trích xuất các đặc trưng thị giác được liên kết văn bản cho LLMs. Gần đây, một số nghiên cứu thú vị mở rộng LLMs sang truy xuất hình ảnh, hiểu biết video, âm thanh, phân tích y sinh học, hệ thống điều khiển.

Trong các nghiên cứu gần đây, đã có sự quan tâm ngày càng tăng trong việc mở rộng MLLMs để cải thiện khả năng hiểu biết chi tiết của chúng thông qua liên kết hình ảnh-văn bản cấp vùng. Kosmos-2 giải quyết điều này bằng cách xây dựng một bộ dữ liệu quy mô lớn của các cặp văn bản-vùng được định vị, cho phép tích hợp khả năng định vị vào LLMs. GPT4RoI tái cấu trúc hộp bao quanh thành định dạng hướng dẫn không gian và trích xuất các đặc trưng thị giác dựa trên vùng quan tâm, tạo điều kiện cho hiểu biết đa phương thức cấp vùng. Shikra đề xuất một mô hình thống nhất xử lý tọa độ không gian để sở hữu khả năng tham chiếu trong bối cảnh hội thoại. Ferret và ViP-LLaVA tiếp tục mở rộng với một loạt các hình dạng tự do rộng hơn để tham chiếu, bao gồm điểm, hộp, phác thảo và vẽ nguệch ngoạc. Ngoài ra, Qwen trình bày một tập hợp các MLLMs thể hiện hiệu suất đáng chú ý trên các tác vụ khác nhau. Tuy nhiên, các nghiên cứu trước đây chủ yếu tập trung vào việc trích xuất các đặc trưng thị giác chỉ từ vài tầng cuối của mô hình CLIP, dẫn đến việc nhấn mạnh vào các thuộc tính hình ảnh toàn cục. Trong nghiên cứu này, chúng tôi chú ý đến thực tế là các đặc trưng được trích xuất từ các tầng nông hơn thể hiện sự tập trung mạnh mẽ hơn vào các thuộc tính cục bộ, mà chúng tôi cho rằng có thể mạnh mẽ hơn trong việc hiểu vị trí đối tượng và chi tiết hình ảnh. Ngoài ra, trong khi CLIP chủ yếu học các đặc trưng được liên kết toàn cục, các mô hình thị giác tiên tiến chỉ có thị giác như DINOv2 xuất sắc trong việc nắm bắt các đặc trưng thị giác chi tiết hơn. Chúng tôi cho rằng việc tận dụng các đặc trưng thị giác chi tiết này có thể nâng cao hiệu quả khả năng của MLLMs, như được chứng minh trong phân tích của chúng tôi. Để tiếp tục phát triển hướng nghiên cứu này, chúng tôi giới thiệu một mô-đun hợp nhất mới mở rộng và tăng cường các nhánh thị giác, từ đó nhằm cải thiện đáng kể hiệu suất của MLLMs.

**Mô hình Nền tảng Thị giác Lớn.** Những tiến bộ gần đây trong việc huấn luyện các mô hình nền tảng thị giác với dữ liệu hình ảnh quy mô lớn tập trung vào học tập tương phản, mô hình hóa hình ảnh có mặt nạ và huấn luyện có giám sát. Một mặt, học tập tương phản có thể được tiến hành theo cách chỉ hình ảnh hoặc hình ảnh-văn bản. DINOv2 tiền huấn luyện bộ mã hóa hình ảnh trên dữ liệu hình ảnh được tuyển chọn lớn, cho thấy hiểu biết vượt trội về các phần đối tượng và hình học cảnh trên các miền hình ảnh. Học tập tương phản hình ảnh-văn bản như CLIP và EVA-CLIP sử dụng ngôn ngữ tự nhiên làm giám sát yếu để hướng dẫn việc học các đặc trưng thị giác. Mặt khác, BEiT dự đoán các token rời rạc dựa trên một tokenizer hình ảnh được tiền huấn luyện trong khi iBOT đề xuất một tokenizer hình ảnh trực tuyến. MAE đề xuất một bộ mã hóa tự động có mặt nạ để tái tạo các pixel hình ảnh. Bên cạnh đó, DeiT III đề xuất một công thức huấn luyện để đạt được hiệu suất đầy hứa hẹn. Các MLLMs gần đây sử dụng bộ mã hóa thị giác của CLIP/EVA-CLIP mà không xem xét các thuộc tính của các mô hình thị giác cụ thể. Trong bài báo này, chúng tôi là những người đầu tiên xem xét lại hiệu quả của các mô hình thị giác hiện tại trong MLLMs và đề xuất một chiến lược hợp nhất đơn giản nhưng hiệu quả để thúc đẩy khả năng thị giác.

## 3 Phân tích Nhánh Thị giác trong MLLMs

Các MLLMs trước đây thường sử dụng bộ mã hóa thị giác của CLIP làm nhánh thị giác của chúng. Thông thường, các mô hình này trích xuất các đặc trưng từ vài tầng cuối, như tầng áp chót, sau đó được đưa vào một mạng liên kết. Tiếp theo, các đặc trưng được liên kết được nối với các token văn bản để phục vụ như đầu vào cho LLMs. Trong khi việc tiền huấn luyện hình ảnh-văn bản của CLIP liên kết tốt với mô hình ngôn ngữ, nó chủ yếu học các đặc trưng cấp hình ảnh nhưng bỏ qua các đặc trưng cấp pixel phong phú hơn do hạn chế của thông tin chi tiết hạn chế trong các chú thích hướng dẫn. Hơn nữa, các đặc trưng tầng sâu chủ yếu tập trung vào các thuộc tính hình ảnh toàn cục và không khám phá đầy đủ các chi tiết phức tạp của các phần đối tượng cục bộ. Như được mô tả trong Hình 1, các đặc trưng thị giác được trích xuất từ các tầng nông của CLIP và các đặc trưng thị giác sâu thu được từ mô hình chỉ có thị giác DINOv2 chứa thông tin chi tiết hơn về các đối tượng cục bộ, như hình dạng hoặc kết cấu. Việc tận dụng các đặc trưng chi tiết này có thể tăng cường khả năng nhận thức chi tiết của MLLMs.

**Cài đặt Đánh giá.** Để phân tích sâu hơn, chúng tôi tiến hành một loạt các thí nghiệm định lượng sử dụng các loại mô hình thị giác khác nhau, tức là: học tập tương phản hình ảnh-văn bản CLIP, học tập tương phản chỉ hình ảnh DINOv2, mô hình hóa hình ảnh có mặt nạ MAE và học tập có giám sát DeiT. Cụ thể, các đặc trưng thị giác được trích xuất từ các tầng khác nhau của các mô hình thị giác (dựa trên ViT-Large) được liên kết bằng một tầng chiếu tuyến tính và sau đó được nối với các token văn bản làm đầu vào cho LLMs (ở đây chúng tôi sử dụng Vicuna-7B). Kiến trúc tổng thể và quy trình huấn luyện tuân theo Shikra nhưng với ít lần lặp hơn (9400 lần lặp, kích thước batch 16 trên 4 A800) để tiết kiệm chi phí tính toán. Sau đó, chúng tôi đo lường khả năng của các MLLMs đã được huấn luyện trên hiểu biết biểu thức tham chiếu (REC), tạo biểu thức tham chiếu (REG) và tiêu chuẩn ảo giác đối tượng (POPE). Mô tả chi tiết về các tác vụ này có thể tham khảo Phần 5.

**CLIP làm Nhánh Thị giác của MLLMs.** Như được mô tả trong Hình 2, chúng tôi quan sát thấy rằng các tầng đặc trưng khác nhau thể hiện các thiên hướng khác nhau đối với khả năng định vị và hiểu biết. Ví dụ, các đặc trưng nông thể hiện độ chính xác tương đối cao hơn về REC và đạt giá trị tối ưu ở tầng 12. Ngược lại, các đặc trưng sâu đạt được độ chính xác cao hơn về POPE, cho thấy khả năng hiểu biết vượt trội. Đáng chú ý, các đặc trưng tương đối sâu (tầng 16) hiển thị điểm CIDEr REG tốt nhất, thể hiện khả năng hiểu biết vùng đầy hứa hẹn. Do đó, thay vì chỉ dựa vào các đặc trưng sâu như đã làm trong các nghiên cứu trước, chúng tôi cho rằng việc tích hợp cả các đặc trưng nông và sâu là rất quan trọng cho MLLMs với hiệu suất tổng thể được cải thiện.

Chúng tôi tiếp tục khám phá các chế độ hợp nhất khác nhau của các đặc trưng cấp thấp và cấp cao. Ký hiệu các đặc trưng đầu ra từ mỗi tầng transformer của ViT với độ sâu N là z = [z1, .., zi, ..., zN], chúng tôi thảo luận về một số chiến lược hợp nhất đặc trưng đa tầng (MFM) để kết hợp các đặc trưng nông và sâu, cụ thể là:

• **Mean(half)**: lấy trung bình các đặc trưng token patch đầu ra trong nửa thứ hai của backbone như z = (zN/2+···+zN)/(N/2).
• **Mean(all)**: lấy trung bình các đặc trưng đầu ra bởi tất cả các tầng như z = (z1+···+zN)/N.
• **Layerscale(all)**: học một tham số tỷ lệ như trọng số để tính tổng các đặc trưng đầu ra bởi tất cả các tầng như z = w1z1+···+wNzN, trong đó wi đề cập đến trọng số được gán cho đặc trưng tầng thứ i và tất cả các trọng số này được cập nhật động và tổng lên thành 1.
• **LLN-Layerscale(all)**: sử dụng một mô-đun linear-layernorm để liên kết không gian đặc trưng giữa các đặc trưng của các tầng khác nhau và sau đó được tính tổng bởi Layerscale như z = w1LLN(z1) +···+wNLLN(zN).
• **Conv-Layerscale(all)**: sử dụng một mô-đun convolution và bn để liên kết không gian đặc trưng giữa các đặc trưng của các tầng khác nhau và sau đó được tính tổng bởi Layerscale như z = w1Conv(z1) +···+wNConv(zN).

Hình 3(a) và (b) cho thấy rằng việc đơn giản lấy trung bình tất cả các đặc trưng nông và sâu của CLIP thực sự có thể đạt được độ chính xác thỏa mãn và chiến lược LLN-Layerscale tiếp tục cải thiện hiệu suất. Với LLN-Layerscale làm mô-đun MFM, hiệu suất của CLIP có thể được cải thiện rõ rệt trên các tác vụ thị giác-ngôn ngữ thường được sử dụng như được thể hiện trong Bảng 1.

**DINOv2 làm Nhánh Thị giác của MLLMs.** Để tận dụng thông tin thị giác chi tiết phong phú có trong DINOv2, nhưng không được liên kết tự nhiên với văn bản, chúng tôi sử dụng một mô-đun Perceptron Đa tầng (MLP) phi tuyến để liên kết các đặc trưng hình ảnh với không gian nhúng từ. Hình 2 chứng minh rằng các đặc trưng tầng sâu của DINOv2 thể hiện khả năng định vị vượt trội, như được chứng minh bởi độ chính xác REC cao hơn, và hiển thị khả năng hiểu biết thỏa mãn, như được chỉ ra bởi kết quả POPE và REG tốt. Ngoài ra, chúng tôi khám phá hiệu quả của hợp nhất đặc trưng đa tầng để tăng cường hiệu suất. Trái ngược với CLIP, việc hợp nhất các đặc trưng nông từ DINOv2 dẫn đến sự suy giảm hiệu suất đáng kể. Cụ thể, trong Hình 3(c) và (d), rõ ràng là Mean(all) hoạt động kém hơn đáng kể so với Mean(19-24) về cả độ chính xác REC và POPE, cho thấy rằng các biểu diễn nông thiếu thông tin ngữ nghĩa đầy đủ. Dựa trên phương pháp LLN-Layerscale, việc kết hợp mô-đun MLP để tạo kết nối mạnh mẽ hơn giữa các không gian thị giác và văn bản thể hiện sự cải thiện hiệu suất rõ ràng. Bảng 1 thể hiện những cải thiện hiệu suất đáng kể đạt được bằng cách sử dụng LLN-Layerscale-MLP làm mô-đun Hợp nhất Đặc trưng Đa tầng (MFM) trên các tác vụ ngôn ngữ thị giác khác nhau. Các nghiên cứu phân tích chi tiết hơn về mô-đun MLP có trong Phần 5.5.

**MAE và DeiT làm Nhánh Thị giác của MLLMs.** Hình 2 cho thấy rằng các đặc trưng MAE đạt được độ chính xác REC chấp nhận được, nhưng gặp sự suy giảm hiệu suất lớn trong đánh giá POPE và REG. Điều này là do các đặc trưng MAE thiếu thông tin ngữ nghĩa đầy đủ cho hiểu biết toàn cục hoặc vùng. Do đó, MAE không phù hợp làm nhánh thị giác cho MLLMs. DeiT hoạt động thậm chí còn tệ hơn MAE (chi tiết trong Phần 5.5). Chúng tôi suy đoán rằng điều này là do huấn luyện có giám sát quá mạnh, học được một không gian thị giác chuyên biệt khó liên kết với không gian nhúng từ.

## 4 COMM: Kết hợp CLIP và DINO với Hợp nhất Đặc trưng Đa tầng

**Tổng quan Kiến trúc.** Trong phần này, chúng tôi giới thiệu COMM được đề xuất, tích hợp CLIP và DINO với Hợp nhất đặc trưng Đa tầng để tăng cường khả năng thị giác của MLLMs. Khung tổng thể được minh họa trong Hình 4, COMM được tích hợp vào một mô hình tuân theo hướng dẫn thị giác-ngôn ngữ được xây dựng dựa trên các mô hình nền tảng ngôn ngữ và thị giác-ngôn ngữ tiên tiến gần đây. Tuân theo các hướng dẫn đầu vào, mô hình của chúng tôi nhận thị giác và ngôn ngữ làm đầu vào để tạo ra các phản hồi văn bản tuân theo các hướng dẫn đầu vào. Cụ thể, chúng tôi áp dụng bộ mã hóa thị giác của CLIP và DINOv2 (dựa trên ViT-Large) với chiến lược hợp nhất được đề xuất của chúng tôi làm nhánh thị giác, và Vicuna (7B/13B) làm bộ giải mã ngôn ngữ. Bộ mã hóa thị giác được lấy mẫu xuống với tỷ lệ 14, có nghĩa là một hình ảnh với độ phân giải H×W sẽ được biểu diễn bởi một chuỗi của H/14×W/14 token. Các đặc trưng token được hợp nhất được chiếu bằng một tầng tuyến tính và sau đó được nối với các token hướng dẫn làm đầu vào cho bộ giải mã ngôn ngữ, là một giao diện chung để thống nhất các tác vụ thị giác-ngôn ngữ khác nhau như tác vụ tạo văn bản.

Cụ thể, ký hiệu bộ mã hóa thị giác của CLIP và DINOv2 (ViT Large được sử dụng) lần lượt là f1 và f2. Cho một hình ảnh đầu vào x, chúng tôi trích xuất các đặc trưng token patch đầu ra bởi tất cả các tầng của CLIP như f1(x) = [v1¹, ..., vⁱ¹, ..., v²⁴¹], trong đó vⁱ¹ ∈ R^(N×D), N là số lượng token patch và D là kích thước nhúng. Các đặc trưng đầu ra bởi các tầng sâu của DINOv2 là f2(x) = [v¹⁹², ..., vⁱ², ..., v²⁴²]. Sau đó chúng tôi nối các đặc trưng đầu ra bởi hai mô hình này như v = [v1¹, ..., v²⁴¹, v¹⁹², ..., v²⁴²]. Một mô-đun linear-layernorm được sử dụng để liên kết không gian đặc trưng giữa các đặc trưng của các tầng khác nhau và layerscale được sử dụng để hợp nhất các đặc trưng đa tầng như:

v¹ = Σ(i=1 to 24) αᵢ · Linear(LN(vⁱ¹)), v² = Σ(j=19 to 24) βⱼ · Linear(LN(vⱼ²)) (1)

trong đó α và β là tham số tỷ lệ có thể học được. Sau đó, chúng tôi sử dụng một tầng MLP để chiếu các đặc trưng của DINOv2 và nối các đặc trưng đầu ra với đặc trưng của CLIP như v = [v¹, MLP(v²)]. Sau đó, một tầng tuyến tính được sử dụng để khớp kích thước của các đặc trưng thị giác với kích thước của các đặc trưng văn bản như v̂ = Linear(v). Cuối cùng, các đặc trưng thị giác được hợp nhất v̂ được nối với các token văn bản làm đầu vào cho LLMs.

## 5 Thí nghiệm

Trong phần này, chúng tôi tiến hành đánh giá sâu rộng trên bốn loại tác vụ thị giác-ngôn ngữ để đánh giá toàn diện khả năng hiểu biết thị giác của mô hình chúng tôi, cụ thể là: Hiểu biết Biểu thức Tham chiếu, Tạo Biểu thức Tham chiếu, Tiêu chuẩn Ảo giác Đối tượng, và Trả lời Câu hỏi Thị giác và Chú thích Hình ảnh.

**Chi tiết Huấn luyện.** Tương tự như các phương pháp MLLM trước đây, COMM được huấn luyện trong hai giai đoạn. Trong giai đoạn tiền huấn luyện đầu tiên, chúng tôi huấn luyện mô hình trên bộ dữ liệu thị giác-ngôn ngữ được tái tổ chức như [8], bao gồm bộ dữ liệu VQA, Chú thích Hình ảnh công cộng và một số bộ dữ liệu chứa chú thích vị trí RefCOCO, visual gemone [17] và Visual-7W [28]. Giai đoạn tiền huấn luyện đầu tiên được tiến hành trong 100K bước. Trong giai đoạn điều chỉnh hướng dẫn thứ hai, chúng tôi đặt tỷ lệ lấy mẫu là 50% trên LLaVA-Instruct-150K [23] và Shikra-RD [8]. Thay vì độ phân giải 224×224 hiện đang được sử dụng bởi các MLLMs hiện tại, chúng tôi sử dụng độ phân giải 336×336 để giảm mất thông tin do lấy mẫu xuống hình ảnh và thúc đẩy khả năng nhận thức chi tiết. Trong cả hai giai đoạn, chúng tôi đóng băng bộ mã hóa thị giác và điều chỉnh tất cả các tham số trong LLMs, tầng liên kết và mô-đun hợp nhất đặc trưng đa tầng. Chúng tôi áp dụng AdamW [27] làm bộ tối ưu hóa và bộ lập lịch cosine annealing [26] làm bộ lập lịch tốc độ học với tốc độ học ban đầu là 2e-5 và kích thước batch toàn cục là 64. Tất cả việc huấn luyện chạy trên 8 GPU NVIDIA A800. Mất khoảng 100h cho huấn luyện giai đoạn một và 20h cho giai đoạn hai.

### 5.1 Hiểu biết Biểu thức Tham chiếu

Để đánh giá khả năng hiểu biết và định vị chi tiết của mô hình chúng tôi, chúng tôi điều tra tác vụ hiểu biết biểu thức tham chiếu trên các tiêu chuẩn như RefCOCO [15], RefCOCO+ [29] và RefCOCOg [29], trong đó các mô hình được yêu cầu định vị đối tượng được mô tả bằng một biểu thức. Như được thể hiện trong Bảng 2, so với các mô hình VL tổng quát và các MLLMs SOTA trước đây, COMM đạt được cải thiện hiệu suất đáng kể trên tất cả các tiêu chuẩn, tức là COMM-7B vượt trội hơn Shikra-13B và Qwen-VL-7B-Chat lần lượt 4,87% và 3,10% độ chính xác trung bình. Với khả năng thị giác mạnh mẽ hơn của mô hình hợp nhất được đề xuất của chúng tôi, chúng tôi có thể vượt trội rõ rệt các MLLMs SOTA gần đây theo cách hiệu quả hơn, ví dụ: sử dụng LLM nhỏ hơn so với Shikra (7B so với 13B) và ít dữ liệu huấn luyện hơn so với Qwen (3.6M so với 1.4B). Bên cạnh đó, mô hình tổng quát của chúng tôi thậm chí đạt được kết quả tương đương với các phương pháp SOTA chuyên biệt, cho thấy khả năng định vị vượt trội của MLLMs chúng tôi.

### 5.2 Tạo Biểu thức Tham chiếu

Hơn nữa, chúng tôi đánh giá khả năng hiểu biết các vùng hình ảnh hoặc đối tượng được tham chiếu thông qua việc nhập các hộp bao quanh. Thay vì tham chiếu các vùng hình ảnh hoặc đối tượng thông qua mô tả văn bản chi tiết, việc tham chiếu trực tiếp đến các vùng hình ảnh thông qua hộp bao quanh của nó hiệu quả hơn và có thể giảm sự mơ hồ. Các thí nghiệm được tiến hành trên tác vụ tạo biểu thức tham chiếu với RefCOCO, RefCOCO+ và RefCOCOg, nhằm tạo ra các mô tả văn bản về các vùng cụ thể trong hộp bao quanh. Bảng 3 cho thấy rằng mô hình của chúng tôi vượt trội hơn Shikra và Kosmos-2 với biên độ đáng kể là 16,51 CIDEr và 16,92 CIDEr tăng trên RefCOCOg, chứng minh hiệu quả của mô hình chúng tôi cho hiểu biết chi tiết. Bên cạnh đó, COMM thậm chí vượt trội hơn SLR được điều chỉnh tinh trên RefCOCO+ và RefCOCOg.

### 5.3 Tiêu chuẩn Ảo giác Đối tượng

Chúng tôi so sánh mô hình của chúng tôi với các mô hình cơ sở trên bộ dữ liệu đánh giá ảo giác được giới thiệu gần đây bởi POPE [21], chọn ngẫu nhiên 500 hình ảnh từ COCO [6]. Bảng 4 cho thấy rằng COMM vượt trội hơn các MLLMs phổ biến gần đây với độ chính xác cao hơn 1,44% và 4,95% trung bình so với Shikra và InstrutBLIP. Bằng cách tăng cường khả năng thị giác chi tiết, COMM có thể giảm thiểu hiệu quả vấn đề ảo giác đối tượng.

### 5.4 Trả lời Câu hỏi Thị giác và Chú thích Hình ảnh

Chúng tôi đánh giá COMM trên các tác vụ VL thông thường là VQA và Chú thích Hình ảnh. Cụ thể, chú thích hình ảnh yêu cầu mô hình tạo ra mô tả cho hình ảnh đã cho và VQA yêu cầu mô hình tạo ra câu trả lời cho cặp hình ảnh-câu hỏi đã cho. Đối với chú thích hình ảnh, chúng tôi chọn COCO [9] và Flickr30K [35] làm tiêu chuẩn và báo cáo điểm CIDEr. Đối với tác vụ VQA, chúng tôi thí nghiệm trên VQAv2 [2] và OK-VQA [30]. Như được thể hiện trong Bảng 5, COMM đạt được hiệu suất tốt nhất trên tác vụ chú thích hình ảnh, tức là 88,2 điểm CIDEr trên Flickr30K và 132,7 điểm CIDEr trên COCO, thậm chí vượt trội hơn các mô hình SOTA trước đây với nhiều tham số hơn (ví dụ: Shikra-13B với 13B tham số) hoặc nhiều dữ liệu huấn luyện hơn (ví dụ: Qwen với 1.4B dữ liệu). Đối với tác vụ VQA, mô hình của chúng tôi cũng cho thấy lợi thế đáng kể so với các MLLMs khác. Trên VQAv2 val, dev và std, mô hình của chúng tôi đạt được độ chính xác 79,05, 81,04 và 81,17 tương ứng, vượt trội hơn Shikra được đề xuất gần đây với cùng dữ liệu huấn luyện và quy trình với biên độ lớn, chứng minh hiệu quả của việc hợp nhất các nhúng thị giác của DINOv2 và CLIP để tăng cường khả năng thị giác. Bên cạnh đó, mô hình COMM của chúng tôi vượt trội hơn Qwen với cải thiện độ chính xác 1,54 và 0,58 trên VQAv2 dev và OK-VQA tương ứng với ít dữ liệu huấn luyện VQA hơn, tức là chúng tôi sử dụng 0,6M và Qwen với 3,6M. Huấn luyện với nhiều dữ liệu VQA hơn có thể cải thiện thêm hiệu suất và chúng tôi để lại điều này như công việc tương lai.

### 5.5 Nghiên cứu Phân tích

**Phân tích về MLP của DINOv2.** Chúng tôi tiến hành nghiên cứu phân tích về thiết kế của mô-đun MLP trong DINOv2 để liên kết không gian nhúng thị giác và văn bản. Chúng tôi phân tích về số lượng và tỷ lệ mở rộng của mô-đun MLP. Bảng 6 cho thấy rằng tăng số lượng MLP lên 2 có thể cải thiện hiệu suất rõ rệt, chứng minh hiệu quả của việc sử dụng một mạng mạnh mẽ hơn để liên kết mô hình chỉ có thị giác DINOv2 với không gian nhúng từ. Tuy nhiên, tăng số lượng vượt quá 2 gặp phải hiệu suất suy giảm. Đối với tỷ lệ mở rộng, tăng lên 8 có thể cải thiện hiệu suất, trong khi tăng lên 16 không đạt được cải thiện hiệu suất đáng kể. Hơn nữa, chúng tôi thí nghiệm với một tầng tuyến tính, gặp phải sự suy giảm hiệu suất nghiêm trọng. Do đó, MLP phi tuyến là cần thiết để liên kết các đặc trưng của DINOv2 chỉ có thị giác với không gian nhúng từ.

**Phân tích về mô hình thị giác của MAE và DeiT.** Như được thể hiện trong Bảng 7, MAE và DeiT gặp phải sự suy giảm hiệu suất rõ rệt. Một mặt, các đặc trưng thị giác của MAE thiếu thông tin ngữ nghĩa đầy đủ cho hiểu biết toàn cục hoặc vùng. Mặt khác, huấn luyện có giám sát của DeiT quá mạnh đến mức nó học được không gian thị giác chuyên biệt, làm cho việc liên kết với không gian nhúng từ trở nên khó khăn.

### 5.6 Minh họa

Như được thể hiện trong Hình 7, mô hình COMM của chúng tôi thể hiện nhiều khả năng đầy hứa hẹn bao gồm định vị thị giác, hiểu biết vùng chi tiết và khả năng chống lại ảo giác đối tượng. Ví dụ đầu tiên thể hiện khả năng nhận thức chi tiết mạnh mẽ của chúng tôi, nhận dạng dâu tây ngầm trong máy xay sinh tố. Ví dụ thứ hai thể hiện khả năng định vị thị giác mạnh mẽ của chúng tôi để định vị thành công đối tượng đường. Trường hợp thứ ba chứng minh khả năng chống lại ảo giác đối tượng của chúng tôi. Trái ngược, Shikra thất bại trong những trường hợp thách thức này, cho thấy khả năng vượt trội của mô hình chúng tôi. Chúng tôi cung cấp các minh họa bổ sung về mô hình COMM của chúng tôi trong phần này để chứng minh nhiều khả năng đầy hứa hẹn bao gồm định vị thị giác, hiểu biết vùng chi tiết và khả năng chống lại ảo giác đối tượng. Ví dụ, chúng tôi thể hiện Hiểu biết Biểu thức Tham chiếu trong Hình 5 và Ảo giác Đối tượng trong Hình 6.

## 6 Kết luận

Bài báo này trình bày một cuộc điều tra sâu rộng về hiệu quả của các mô hình thị giác khác nhau khi được sử dụng làm nhánh thị giác trong MLLMs. Thông qua phân tích có hệ thống, chúng tôi làm nổi bật tầm quan trọng của các đặc trưng tầng nông, nắm bắt các chi tiết cấp thấp chứng minh có lợi cho các tác vụ định vị và định vị. Hơn nữa, chúng tôi nhận ra tiềm năng của mô hình chỉ có thị giác DINOv2, tận dụng thông tin cấp pixel chi tiết vốn có của nó để tăng cường nhận thức chi tiết trong MLLMs khi kết hợp với một tầng MLP cho mục đích liên kết. Được thúc đẩy bởi phân tích của chúng tôi, chúng tôi giới thiệu một phương pháp hợp nhất để kết hợp các đặc trưng thị giác thu được từ CLIP và DINOv2, từ đó tăng cường hơn nữa khả năng thị giác và hiệu suất của MLLMs. Thông qua phân tích định tính và các thí nghiệm định lượng sâu rộng, chúng tôi chứng minh hiệu quả của phương pháp được đề xuất, vượt trội hơn hiệu suất của các mô hình MLLM hiện tại trên các bộ dữ liệu tiêu chuẩn đa dạng. Nhìn về tương lai, chúng tôi khuyến khích nghiên cứu trong tương lai khám phá việc tích hợp các mô hình thị giác mạnh mẽ hơn để tăng cường khả năng của các nhánh thị giác trong MLLMs. Chúng tôi tin rằng hướng điều tra này nắm giữ chìa khóa để mở khóa tiềm năng của thế hệ MLLMs tiếp theo.
