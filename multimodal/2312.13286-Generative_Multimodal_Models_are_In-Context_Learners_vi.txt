# Các Mô hình Đa phương tiện Sinh tạo là Những Người học Trong bối cảnh

Quan Sun1∗Yufeng Cui1∗Xiaosong Zhang1∗Fan Zhang1∗Qiying Yu2,1∗Zhengxiong Luo1
Yueze Wang1Yongming Rao1Jingjing Liu2Tiejun Huang1,3Xinlong Wang1†
1Học viện Trí tuệ Nhân tạo Bắc Kinh 2Đại học Thanh Hoa 3Đại học Bắc Kinh
∗đóng góp ngang bằng †trưởng nhóm dự án

mã nguồn & mô hình: https://github.com/baaivision/Emu

## Tóm tắt

Khả năng của con người giải quyết dễ dàng các tác vụ đa phương tiện trong bối cảnh (tức là chỉ với một vài minh họa hoặc hướng dẫn đơn giản), chính là điều mà các hệ thống đa phương tiện hiện tại đã gặp nhiều khó khăn để mô phỏng. Trong nghiên cứu này, chúng tôi chứng minh rằng khả năng học trong bối cảnh không phụ thuộc vào tác vụ của các mô hình đa phương tiện lớn có thể được tăng cường đáng kể bằng việc mở rộng quy mô hiệu quả. Chúng tôi giới thiệu Emu2, một mô hình đa phương tiện sinh tạo với 37 tỷ tham số, được huấn luyện trên các chuỗi đa phương tiện quy mô lớn với mục tiêu tự hồi quy thống nhất. Emu2 thể hiện khả năng học đa phương tiện trong bối cảnh mạnh mẽ, thậm chí có khả năng giải quyết các tác vụ đòi hỏi suy luận tức thời, chẳng hạn như gợi ý trực quan và tạo sinh dựa trên đối tượng. Mô hình đạt kỷ lục mới trên nhiều tác vụ hiểu biết đa phương tiện trong cài đặt few-shot. Khi được điều chỉnh hướng dẫn để tuân theo các chỉ dẫn cụ thể, Emu2 tiếp tục đạt được state-of-the-art mới trên các tác vụ thử thách như các tiêu chuẩn đánh giá hỏi đáp cho các mô hình đa phương tiện lớn và tạo sinh hướng chủ đề mở. Những thành tựu này chứng minh rằng Emu2 có thể phục vụ như một mô hình cơ sở và giao diện đa năng cho nhiều tác vụ đa phương tiện. Mã nguồn và mô hình được công bố công khai để tạo điều kiện cho nghiên cứu tương lai.

## 1. Giới thiệu

Các tác vụ đa phương tiện bao gồm mọi thứ liên quan đến hiểu biết và tạo sinh trong một hoặc nhiều phương tiện, có thể rất đa dạng và thuộc nhóm long-tail. Các hệ thống đa phương tiện trước đây chủ yếu dựa vào việc thiết kế kiến trúc cụ thể cho từng tác vụ và thu thập bộ dữ liệu huấn luyện có giám sát đáng kể, cả hai đều khó mở rộng quy mô, đặc biệt khi quá trình này cần được lặp lại cho mỗi tác vụ mới gặp phải. Ngược lại, con người có thể giải quyết một tác vụ mới trong bối cảnh, tức là chỉ với một vài minh họa hoặc hướng dẫn đơn giản - một khả năng mà các mô hình đa phương tiện hiện tại vẫn chưa học được.

Gần đây, các mô hình ngôn ngữ sinh tạo được huấn luyện trước đã chứng minh khả năng học trong bối cảnh mạnh mẽ. Bằng cách huấn luyện một mô hình 37 tỷ tham số Emu2 và đánh giá kỹ lưỡng trên các tác vụ đa phương tiện đa dạng, chúng tôi chứng minh rằng một mô hình sinh tạo đa phương tiện được huấn luyện trước với quy mô lớn có thể khai thác các khả năng học trong bối cảnh tương tự và tổng quát hóa hiệu quả cho các tác vụ đa phương tiện chưa thấy.

Emu2 được huấn luyện với mục tiêu tự hồi quy thống nhất: dự đoán-phần-tử-đa-phương-tiện-tiếp-theo (có thể là embedding hình ảnh hoặc token văn bản). Trong quá trình huấn luyện trước sinh tạo thống nhất này, các chuỗi đa phương tiện quy mô lớn (ví dụ: văn bản, cặp hình ảnh-văn bản, và hình ảnh-văn bản-video xen kẽ) được sử dụng để huấn luyện mô hình.

Chúng tôi đo lường khả năng của Emu2 trong việc học từ một vài ví dụ hoặc hướng dẫn trên các bộ dữ liệu đa phương tiện chuẩn, cũng như các tác vụ mới chưa thấy trong tập huấn luyện. Cụ thể, Emu2 được đánh giá dưới hai kịch bản: (a) cài đặt few-shot, nơi chúng tôi cho phép nhiều ví dụ nhất có thể để vừa với cửa sổ ngữ cảnh của mô hình; và (b) điều chỉnh hướng dẫn, nơi mô hình được điều chỉnh để tuân theo các chỉ dẫn cụ thể.

Emu2 đạt kết quả khả quan trong cài đặt few-shot trên một loạt rộng các tác vụ thị giác-ngôn ngữ. Ví dụ, nó chứng minh hiệu suất few-shot state-of-the-art trên nhiều bộ dữ liệu hỏi đáp thị giác. Chúng tôi quan sát thấy cải thiện hiệu suất khi số lượng ví dụ trong ngữ cảnh tăng lên. Hình 1 minh họa khả năng suy luận đa phương tiện mạnh mẽ của Emu2 cho các tác vụ thực tế, ví dụ như nhận dạng và đếm theo định dạng cụ thể. Emu2 cũng học cách tuân theo gợi ý trực quan trong ngữ cảnh (ví dụ: các vòng tròn được đặt trên hình ảnh trong Hình 1), mặc dù nó gặp khó khăn ở quy mô nhỏ hơn hoặc ở zero shot.

Vì Emu2 vốn được trang bị để xử lý văn bản-hình ảnh-video xen kẽ ở cả đầu vào và đầu ra, nó phục vụ như một mô hình cơ sở mạnh mẽ và linh hoạt cho các tác vụ đa phương tiện đa dạng, bằng cách tuân theo các hướng dẫn tác vụ cụ thể. Ví dụ, sau khi điều chỉnh hướng dẫn với dữ liệu đối thoại, Emu2 đạt kết quả state-of-the-art trên các tác vụ hỏi đáp thị giác, và vượt qua các mô hình trước đây có thiết kế phức tạp hơn. Ngoài ra, Emu2 có thể được tinh chỉnh để hoạt động như một mô hình tạo sinh thị giác có thể kiểm soát với chất lượng cao. Nó có khả năng chấp nhận hỗn hợp văn bản, vị trí và hình ảnh làm điều kiện, và tạo sinh hình ảnh được căn cứ như chỉ định.

Với phạm vi rộng lớn của các khả năng được thể hiện bởi Emu2, chúng tôi tiến hành phân tích kỹ lưỡng về những tác động xã hội tiềm ẩn của nó và thảo luận chi tiết về những lo ngại tiềm ẩn về việc lạm dụng. Bằng cách xác định các tác vụ khác nơi khả năng học trong bối cảnh của Emu2 có thể được cải thiện thêm, chúng tôi nhấn mạnh sự cần thiết cho việc liên tục nâng cao mô hình và tầm quan trọng của việc triển khai Emu2 một cách có trách nhiệm.

## 2. Phương pháp tiếp cận

### 2.1. Kiến trúc mô hình

Emu2 là một mô hình đa phương tiện sinh tạo học với mục tiêu dự đoán-phần-tử-tiếp-theo trong ngữ cảnh đa phương tiện. Như được minh họa trong Hình 2, kiến trúc của Emu2 bao gồm ba thành phần: Bộ mã hóa thị giác, Mô hình hóa đa phương tiện, và Bộ giải mã thị giác. Mỗi hình ảnh trong chuỗi đa phương tiện đầu vào được token hóa thành các embedding liên tục qua Bộ mã hóa thị giác và sau đó xen kẽ với các token văn bản để thực hiện Mô hình hóa đa phương tiện tự hồi quy. Các embedding thị giác được hồi quy sau đó được giải mã thành một hình ảnh hoặc một video bởi Bộ giải mã thị giác. Cụ thể, chúng tôi tận dụng EVA-02-CLIP-E-plus được huấn luyện trước, LLaMA-33B và SDXL để khởi tạo Bộ mã hóa thị giác, Mô hình hóa đa phương tiện, và Bộ giải mã thị giác tương ứng. So với Emu, Emu2 áp dụng một khung đơn giản hơn kết nối Bộ mã hóa thị giác và Mô hình hóa đa phương tiện thông qua việc tính trung bình pooling mỗi hình ảnh thành các patch hình ảnh 8×8, theo sau bởi một phép chiếu tuyến tính, thay vì sử dụng thêm một C-Former.

### 2.2. Huấn luyện trước

#### 2.2.1 Dữ liệu

Dữ liệu huấn luyện trước cho Emu2 bao gồm một số bộ dữ liệu có thể truy cập công khai, bao gồm các cặp hình ảnh-văn bản từ LAION-2B và CapsFusion-120M, các cặp video-văn bản từ WebVid-10M, dữ liệu hình ảnh-văn bản xen kẽ từ Multimodal-C4 (MMC4), dữ liệu video-văn bản xen kẽ từ YT-Storyboard-1B, các cặp hình ảnh-văn bản có căn cứ từ GRIT-20M được giới thiệu bởi Kosmos-2 và CapsFusion-grounded-100M được tuyển chọn bởi CapsFusion-120M. Ngoài ra, dữ liệu chỉ có ngôn ngữ từ Pile được bao gồm để duy trì khả năng suy luận văn bản.

#### 2.2.2 Huấn luyện

Tương tự như Emu, Emu2 học với mục tiêu dự đoán-phần-tử-tiếp-theo trong một chuỗi đa phương tiện. Mỗi hình ảnh được mã hóa thành N=64 embedding thị giác có kích thước cố định và sau đó xen kẽ với các token văn bản để xây dựng một chuỗi đa phương tiện. Chuỗi xen kẽ sau đó được đưa vào một decoder Transformer để mô hình hóa tự hồi quy.

Emu2 đầu tiên được huấn luyện trước trên dữ liệu cặp hình ảnh-văn bản và video-văn bản chỉ với tổn thất chú thích trên các token văn bản. Các hình ảnh đầu vào được thay đổi kích thước thành 224×224. Chúng tôi áp dụng bộ tối ưu hóa AdamW với β₁=0.9, β₂=0.95, ε=1×10⁻⁶. Tốc độ học tối đa là 1×10⁻⁴ cho lớp chiếu tuyến tính, 3×10⁻⁵ cho Mô hình hóa đa phương tiện, và 5×10⁻⁵ cho Bộ mã hóa thị giác. Chúng tôi huấn luyện trước Emu2 trên 162 triệu mẫu hình ảnh-văn bản và 7 triệu mẫu video-văn bản trong 35.200 lần lặp. Kích thước batch toàn cục là 6.144 cho các cặp hình ảnh-văn bản và 768 cho các cặp video-văn bản. Quá trình huấn luyện sau đó được khởi động lại ở độ phân giải 448 pixel cao hơn trong 4.000 lần lặp bổ sung.

Sau đó, chúng tôi đóng băng Bộ mã hóa thị giác và chỉ tối ưu hóa lớp chiếu tuyến tính và Mô hình hóa đa phương tiện với cả tổn thất phân loại văn bản và tổn thất hồi quy hình ảnh. Các bộ dữ liệu bổ sung bao gồm dữ liệu hình ảnh-văn bản xen kẽ, dữ liệu video-văn bản xen kẽ, dữ liệu cặp hình ảnh-văn bản có căn cứ, và dữ liệu chỉ có ngôn ngữ được sử dụng trong việc huấn luyện. Tất cả hình ảnh được thay đổi kích thước thành 448×448, và tốc độ học tối đa là 1×10⁻⁵. Chúng tôi sử dụng kích thước batch toàn cục là 12.800 cho dữ liệu cặp hình ảnh-văn bản, 6.400 cho dữ liệu cặp video-văn bản, 3.200 cho dữ liệu hình ảnh-văn bản và video-văn bản xen kẽ, và 800 cho dữ liệu chỉ có ngôn ngữ. Quá trình huấn luyện kéo dài 20.350 lần lặp và tiêu thụ khoảng 160 triệu mẫu dữ liệu hình ảnh-văn bản và 3.8 tỷ token dữ liệu chỉ có ngôn ngữ.

#### 2.2.3 Giải mã thị giác

Chúng tôi huấn luyện Bộ giải mã thị giác để giải mã trực tiếp các embedding thị giác được tạo ra bởi Bộ mã hóa thị giác thành hình ảnh. Chúng tôi sử dụng SDXL-base làm khởi tạo cho Bộ giải mã thị giác của chúng tôi, được huấn luyện đầy đủ để giải quyết tác vụ mới của việc tự mã hóa. Cụ thể, chúng tôi sử dụng N embedding thị giác làm đầu vào điều kiện cho Bộ giải mã thị giác và điều chỉnh kích thước của các lớp chiếu trong mô-đun chú ý chéo để phù hợp với kích thước của các embedding thị giác.

Không giống như Emu nơi mỗi bước tối ưu hóa của Bộ giải mã thị giác yêu cầu một suy luận tự hồi quy của mô hình ngôn ngữ, việc giải mã thị giác của Emu2 có thể được coi là huấn luyện một bộ de-tokenizer, có thể được huấn luyện sẵn mà không cần mô hình ngôn ngữ. Một khi được huấn luyện, Bộ giải mã thị giác cùng với Bộ mã hóa thị giác hoạt động như một bộ tự mã hóa hình ảnh có thể token hóa một hình ảnh thành các embedding và de-tokenize ngược lại. Trong quá trình suy luận Emu2, nó tạo ra N embedding hình ảnh và giải mã thành một hình ảnh ngay lập tức.

Để giải mã dữ liệu video, chúng tôi huấn luyện một bộ giải mã dựa trên khuếch tán. Tương tự như các nghiên cứu trước, chúng tôi điều chỉnh một U-Net khử nhiễu 2D thành kiểu 3D bằng cách chèn một phép tích chập thời gian 1D sau mỗi lớp tích chập không gian 2D và mở rộng sự chú ý không gian thành sự chú ý không gian-thời gian. Bộ giải mã video này được khởi tạo qua Stable Diffusion 2.1 và được huấn luyện đầy đủ để tạo ra các clip video có điều kiện trên các embedding thị giác từ Emu2.

**Thiết lập huấn luyện.** Chúng tôi sử dụng các hình ảnh trong LAION-COCO và LAION-Aesthetics để huấn luyện Bộ giải mã thị giác dưới tác vụ tự mã hóa hình ảnh. Bộ mã hóa thị giác và VAE trong SDXL được đóng băng, và chỉ có U-Net được cập nhật trong quá trình huấn luyện. Chúng tôi áp dụng bộ tối ưu hóa AdamW với β₁=0.9, β₂=0.999 và độ suy giảm trọng số 0.01. Chúng tôi sử dụng khởi động tốc độ học log và suy giảm tốc độ học tuyến tính với tốc độ học đỉnh 1×10⁻⁴ cho 2.000 và 6.000 bước tương ứng. Chúng tôi lọc ra các hình ảnh có độ phân giải thấp hơn 512×512. Đầu vào cho Bộ mã hóa thị giác được đặt thành 448×448, trong khi đầu ra của Bộ giải mã thị giác được đặt thành 1024×1024. Chúng tôi cũng sử dụng hướng dẫn không có phân loại, loại bỏ ngẫu nhiên các embedding hình ảnh với xác suất 10%. Kích thước batch được đặt thành 2.048 tổng cộng.

### 2.3. Điều chỉnh hướng dẫn

Emu2 có thể được căn chỉnh hiệu quả để tuân theo các hướng dẫn tác vụ cụ thể. Chúng tôi tinh chỉnh mô hình cơ sở với dữ liệu đối thoại để tạo ra Emu2-Chat, có khả năng tuân theo các câu hỏi đa phương tiện và đưa ra phản hồi trong cuộc đối thoại. Tương tự, chúng tôi tạo ra một mô hình tạo sinh thị giác có thể kiểm soát Emu2-Gen, có khả năng chấp nhận hỗn hợp văn bản, vị trí, và hình ảnh làm điều kiện, và tạo sinh hình ảnh được căn cứ trong văn bản hoặc chủ đề được chỉ định.

#### 2.3.1 Chat tuân theo hướng dẫn

**Dữ liệu huấn luyện.** Chúng tôi áp dụng một phương pháp đồng nhất để huấn luyện trên cả các bộ dữ liệu hướng tác vụ học thuật và dữ liệu chat đa phương tiện để trang bị cho Emu2-Chat khả năng tuân theo hướng dẫn trong khi vẫn duy trì kiến thức thị giác phong phú. Vì các bộ dữ liệu hướng tác vụ học thuật có chú thích ngắn gọn hạn chế khả năng của mô hình trong việc cung cấp phản hồi toàn diện và hữu ích hơn, chúng tôi phân biệt giữa hai loại dữ liệu này bằng cách sử dụng các thông điệp hệ thống khác nhau và bao gồm hướng dẫn với thông tin kiểm soát định dạng đầu ra như được sử dụng trong nghiên cứu trước. Một tóm tắt dữ liệu được sử dụng như sau: (a) Dữ liệu hướng tác vụ học thuật: chú thích hình ảnh, hỏi đáp thị giác, hỏi đáp có kiến thức, phân loại đa phương tiện, và hiểu biết biểu thức tham chiếu. (b) Dữ liệu chat đa phương tiện: hướng dẫn thị giác được hỗ trợ bởi GPT, hướng dẫn ngôn ngữ, đọc đồng hồ, và chat video.

**Mục tiêu huấn luyện.** Trong việc điều chỉnh hướng dẫn của Emu2-Chat, hai token đặc biệt, [USER] và [ASSISTANT], được kết hợp vào mô hình để biểu thị vai trò. Các token này giúp tổ chức các loại dữ liệu khác nhau theo định dạng sau: "<Sys.Msg.> [USER]: <Instruction> [ASSISTANT]: <Answer>". Ở đây <Sys.Msg.> đại diện cho thông điệp hệ thống và thay đổi giữa hai danh mục tác vụ chính (hướng tác vụ học thuật và chat đa phương tiện).

Phần <Instruction> bao gồm các token đa phương tiện, bao gồm hình ảnh, video, và văn bản. Chỉ có các token trong phần <Answer> sẽ được giám sát bởi tổn thất cross-entropy trong quá trình huấn luyện.

**Thiết lập huấn luyện.** Chúng tôi sử dụng kích thước batch toàn cục là 768 và huấn luyện trong 8k bước. Tốc độ học tăng tuyến tính đến 1×10⁻⁵ trong 100 bước đầu tiên, sau đó giảm về không với lịch trình cosin. Mô hình được huấn luyện bằng bộ tối ưu hóa AdamW với β₁=0.9, β₂=0.98, ε=1×10⁻⁶, và cắt gradient là 5.0. Độ dài chuỗi trong quá trình huấn luyện được giới hạn ở 2048, và bất kỳ phần nào vượt quá sẽ được cắt bỏ trực tiếp. Chúng tôi sử dụng nhất quán độ phân giải hình ảnh/video đầu vào là 448×448. Đối với dữ liệu video, chúng tôi lấy mẫu các khung hình đồng đều theo thời gian làm đầu vào cho mô hình. Số lượng khung hình được lấy mẫu cho mỗi video được chọn ngẫu nhiên từ 8, 12, và 16. Để nắm bắt các chi tiết không gian phức tạp hơn, theo sau giai đoạn bộ mã hóa thị giác, chúng tôi áp dụng mean-pooling cho mỗi hình ảnh tĩnh, chia nó thành 16×16 token trong quá trình tinh chỉnh hướng dẫn. Điều này khác với giai đoạn huấn luyện trước, nơi 8×8 token được sử dụng.

#### 2.3.2 Tạo sinh thị giác có thể kiểm soát

**Dữ liệu huấn luyện.** Chúng tôi tận dụng hỗn hợp các bộ dữ liệu chất lượng cao để khai thác tiềm năng của việc tạo sinh có thể kiểm soát trong bối cảnh. Chúng tôi sử dụng bộ dữ liệu cặp hình ảnh-văn bản có căn cứ CapsFusion-grounded-100M và GRIT cho việc tạo sinh hình ảnh từ văn bản có căn cứ. Để giảm thiểu tác động của nền hình ảnh đối với hiệu quả của việc tạo sinh hướng chủ đề đa thực thể, chúng tôi sử dụng SAM để tiền xử lý dữ liệu căn cứ, tạo ra một tập con khoảng 5 triệu mẫu với kết quả phân đoạn. Ngoài ra, chúng tôi tận dụng InstructPix2Pix được xây dựng bởi nghiên cứu trước cho các tác vụ chỉnh sửa hình ảnh. Đối với tác vụ văn bản-thành-hình ảnh, chúng tôi sử dụng một tập con được lọc của CapsFusion, LAION-Aesthetics, SA-1B, và LAION-High-Resolution.

Chúng tôi cũng thu thập dữ liệu từ các nguồn cao cấp (ví dụ: Unsplash) và đầu ra từ các hệ thống văn bản-thành-hình ảnh tiên tiến (ví dụ: Midjourney-V5 và DALL-E-3) để tinh chỉnh chất lượng. Bộ dữ liệu đa dạng này bao gồm khoảng 500k cặp hình ảnh-văn bản chất lượng cao. Đối với tất cả dữ liệu trên, trong quá trình huấn luyện, chỉ các mẫu có độ phân giải hình ảnh cao hơn 448×448 được giữ lại để đảm bảo chất lượng tạo sinh. Thêm chi tiết có thể được tìm thấy trong phần bổ sung.

**Mục tiêu huấn luyện.** Chúng tôi sử dụng cùng một mục tiêu huấn luyện trước sinh tạo thống nhất để thích ứng với các tác vụ tạo sinh đa dạng trong bối cảnh. Cụ thể, một mẫu huấn luyện để tạo sinh được công thức hóa như: "<s>A photo of <p>a man</p><coor>image embedding of object localization image</coor>[IMG]image embedding of man[/IMG]sitting next to <p>a dog</p><coor>image embedding of object localization image</coor>[IMG]image embedding of dog[/IMG][IMG]image embedding of the whole image[/IMG]</s>". Chúng tôi đại diện cho tọa độ của mỗi đối tượng trực tiếp dưới dạng hình ảnh bằng cách vẽ hộp giới hạn của mỗi đối tượng tại vị trí được chỉ định trên một hình ảnh đen. Emu2-Gen của chúng tôi tiến hành mô hình hóa đa phương tiện thống nhất của văn bản, hình ảnh đối tượng, và hình ảnh định vị đối tượng tương ứng. Tổn thất hồi quy chỉ áp dụng cho các embedding thị giác của hình ảnh cuối cùng. Chúng tôi đóng băng Bộ mã hóa thị giác trong quá trình tinh chỉnh. Chúng tôi loại bỏ ngẫu nhiên các token của thực thể và hình ảnh định vị đối tượng để tăng cường khả năng thích ứng và tính mạnh mẽ của mô hình. Ngoài ra, chúng tôi áp dụng tăng cường dữ liệu cho mỗi hình ảnh đối tượng, kết hợp các biến thể nền ngẫu nhiên và cắt ngẫu nhiên, nhằm giảm sự phụ thuộc vào nền hình ảnh.

**Thiết lập huấn luyện.** Chúng tôi sử dụng kích thước batch toàn cục là 4.096 và huấn luyện trong 3k bước. Tốc độ học tăng tuyến tính đến 5×10⁻⁵ trong 100 bước đầu tiên, sau đó giảm về không với lịch trình cosin. Chúng tôi tinh chỉnh thêm 900 bước sử dụng 500k cặp chất lượng cao với kích thước batch là 2048.

## 3. Đánh giá

### 3.1. Mô hình cơ sở được huấn luyện trước

Chúng tôi đánh giá khả năng zero-shot và few-shot của Emu2 trên các tác vụ OKVQA, VQAv2, VizWiz, TextVQA, và HatefulMemes. Chi tiết về các bộ dữ liệu và gợi ý có thể được tìm thấy trong tài liệu bổ sung. Kết quả được trình bày trong Bảng 1. Emu2 chứng minh khả năng trong bối cảnh đáng chú ý, thể hiện hiệu suất cải thiện với nhiều mẫu trong bối cảnh hơn. Cụ thể, trên các bộ dữ liệu VQAv2, VizWiz và TextVQA, Emu2 vượt trội so với Flamingo-80B và IDEFICS-80B trong tất cả các cài đặt few-shot với quy mô mô hình nhỏ hơn nhiều (37B).

Hình 1 chứng minh khả năng few-shot của Emu2 trong thực tế. Ví dụ, mô hình học cách phân loại và đếm đồng thời theo một định dạng cụ thể thông qua một vài ví dụ (hàng 1). Ngoài ra, Emu2 có khả năng tuân theo các gợi ý thị giác trong bối cảnh, ví dụ như các vòng tròn đỏ được đặt trên hình ảnh (hàng 2 và 3).

### 3.2. Chat tuân theo hướng dẫn

Emu2-Chat của chúng tôi được đánh giá trên các tiêu chuẩn hướng tác vụ học thuật bao gồm các bộ dữ liệu hỏi đáp hình ảnh (VQAv2, OKVQA, GQA, VizWiz, TextVQA) và các bộ dữ liệu hỏi đáp video (MSVD và MSRVTT). Việc đánh giá cũng bao gồm các tiêu chuẩn gần đây cho các mô hình đa phương tiện lớn, bao gồm SEED-Bench, MM-Vet, TouchStone và MMMU. Khi được đánh giá trên SEED-Bench, chúng tôi tuân theo thiết lập của LLaVa-1.5 bằng cách trình bày các tùy chọn cho mô hình để hoàn thành các tác vụ trắc nghiệm.

Như được thể hiện trong Bảng 2, Emu2-Chat liên tục vượt trội so với các mô hình khác trong các tác vụ hỏi đáp hình ảnh, bao gồm các tiêu chuẩn được thiết lập tốt như VQAv2 và GQA. Đáng chú ý, nó thể hiện cải thiện đáng kể trong tác vụ OKVQA, đòi hỏi việc sử dụng kiến thức bên ngoài, thể hiện ưu thế của mô hình chúng tôi trong việc nắm vững kiến thức thế giới thực. Đối với hỏi đáp video, Emu2-Chat thể hiện ưu thế mặc dù không sử dụng dữ liệu hỏi đáp video để huấn luyện. Nó đạt độ chính xác 49.0 và 31.4 trên các tác vụ MSVD-QA và MSRVTT-QA tương ứng, vượt qua InstructBLIP và Flamingo-80B lớn hơn. Quan trọng hơn, mô hình của chúng tôi cũng đạt kết quả tốt hơn trên các tiêu chuẩn LMM. Các tiêu chuẩn LMM như MM-Vet cung cấp đánh giá toàn diện hơn về khả năng của mô hình, bao gồm việc giải quyết các tác vụ phức tạp. Emu2-Chat đạt điểm 48.5 trong MM-Vet và 703.8 trong TouchStone, xác nhận khả năng vượt trội trong việc hiểu và giải quyết các vấn đề đa phương tiện.

Ngoài ra, chúng tôi chứng minh khả năng căn cứ thị giác của mô hình bằng cách sử dụng các tiêu chuẩn hiểu biết biểu thức tham chiếu. Trong Bảng 3, Emu2-Chat đạt kết quả tốt nhất trong số các mô hình tổng quát trên RefCOCO, RefCOCO+ và RefCOCOg. Ưu thế đáng chú ý nhất được quan sát thấy trong RefCOCO+, tập trung hoàn toàn vào các mô tả dựa trên ngoại hình thuần túy mà không cho phép sử dụng tham chiếu vị trí. Điều này làm nổi bật khả năng tri giác mạnh mẽ của mô hình trong việc nắm bắt các chi tiết phức tạp.

### 3.3. Tạo sinh thị giác có thể kiểm soát

**Kết quả định tính.** Hình 3 trình bày một hình ảnh minh họa của kết quả tự mã hóa của Emu2. Với Bộ mã hóa thị giác và Bộ giải mã thị giác của Emu2, chúng ta có thể token hóa một hình ảnh thành các embedding thị giác và de-tokenize chúng ngược lại. So với SEED và Emu, Emu2 thể hiện kết quả vượt trội đáng kể. Chúng tôi cũng đánh giá kết quả tự mã hóa hình ảnh trên MS-COCO và đạt điểm CLIP-I mạnh mẽ 0.907. Thêm kết quả có trong phần bổ sung.

Như được mô tả trong Hình 4, Emu2-Gen có khả năng chấp nhận hỗn hợp văn bản, vị trí và hình ảnh làm đầu vào, và tạo sinh hình ảnh trong bối cảnh. Mô hình khéo léo tham gia vào các tác vụ tạo sinh thị giác có thể kiểm soát khác nhau trong cài đặt zero-shot, tận dụng khả năng học trong bối cảnh trong đa phương tiện. Các ví dụ trong Hình 4 thể hiện hình ảnh được tạo sinh của ba con chó có điều kiện trên các chủ đề, vị trí và kịch bản khác nhau. Các mẫu thị giác được trình bày chứng minh khả năng thành thạo của mô hình trong các tác vụ như tái ngữ cảnh hóa, phong cách hóa, sửa đổi, tạo sinh có thể kiểm soát theo vùng, và tổng hợp đa thực thể.

**Tạo sinh hình ảnh từ văn bản zero-shot.** Chúng tôi đánh giá khả năng tạo sinh hình ảnh từ văn bản zero-shot trên 30k dữ liệu được lấy mẫu ngẫu nhiên từ bộ xác nhận MS-COCO. Chúng tôi sử dụng CLIP-ViT-B, theo cách tiếp cận trong DALL-E 3, để tính điểm CLIP-T nhằm đánh giá khả năng tuân theo gợi ý. Ngoài ra, chúng tôi sử dụng CLIP-ViT-L, như trong GILL, để tính điểm CLIP-I cho việc đo lường độ tương tự hình ảnh. Điểm cao hơn có nghĩa là hình ảnh được tạo sinh tương tự hơn với gợi ý hoặc hình ảnh thực.

Bảng 4 cho thấy Emu2-Gen đạt hiệu suất state-of-the-art về cả điểm CLIP-I và CLIP-T so với các mô hình tạo sinh đơn phương tiện và mô hình đa phương tiện khác nhau. Thêm các trường hợp tạo sinh văn bản-thành-hình ảnh có thể được tìm thấy trong phần bổ sung.

**Tạo sinh hướng chủ đề zero-shot.** Theo Kosmos-G, chúng tôi cũng đánh giá khả năng chỉnh sửa hình ảnh hướng chủ đề của mô hình trên DreamBench. Chúng tôi tạo sinh bốn hình ảnh cho mỗi gợi ý, tạo ra tổng cộng 3.000 hình ảnh để đánh giá toàn diện. Chúng tôi sử dụng DINO và CLIP-I để đánh giá độ trung thực chủ đề, và CLIP-T để đánh giá độ trung thực văn bản, phù hợp với phương pháp được thiết lập bởi DreamBooth. Đáng chú ý, Emu2-Gen xuất sắc trong độ trung thực chủ đề, như được chứng minh bởi hiệu suất vượt trội trên các chỉ số DINO và CLIP-I so với các phương pháp như BLIP-Diffusion và Kosmos-G. Emu2-Gen tái tạo chủ đề một cách ấn tượng chỉ với một hình ảnh đầu vào trong cài đặt zero-shot, chứng minh độ trung thực chủ đề vượt trội thông qua việc giải mã thị giác mạnh mẽ. Các trường hợp minh họa thêm được cung cấp trong phần bổ sung, thể hiện khả năng thành thạo của Emu2-Gen trong việc tạo sinh đa thực thể.

## 4. Nghiên cứu liên quan

**Mô hình đa phương tiện lớn.** Những năm gần đây đã chứng kiến sự phát triển nhanh chóng của các mô hình đa phương tiện lớn. CLIP tiên phong trong việc học các LMM với mục tiêu học tương phản trên dữ liệu cặp hình ảnh-văn bản khổng lồ. Flamingo và Kosmos thể hiện hiệu suất hiểu biết đa phương tiện zero-shot và few-shot đầy hứa hẹn bằng cách huấn luyện trên dữ liệu hình ảnh-văn bản xen kẽ quy mô lớn. Với tiến bộ đáng kể trong các LLM mã nguồn mở, nhiều nghiên cứu cho thấy kết quả đầy hứa hẹn bằng cách kết nối các bộ mã hóa thị giác và LLM với một mô hình trung gian nhỏ. Một trường phái nỗ lực liên tiếp tiếp tục cải thiện việc điều chỉnh hướng dẫn thị giác với các quy trình huấn luyện tổng thể tốt hơn, chú thích căn cứ, và các tác vụ bổ sung. Có những nghiên cứu sớm về việc huấn luyện các mô hình đa phương tiện lớn thống nhất hơn có khả năng thực hiện hiểu biết và tạo sinh thị giác đồng thời. Trong bài báo này, chúng tôi tiếp tục khám phá giải pháp riêng biệt được đề xuất trong Emu: học các mô hình đa phương tiện lớn với mục tiêu sinh tạo trên cả văn bản và hình ảnh.

**Học trong bối cảnh.** Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn nhấn mạnh khả năng học trong bối cảnh của chúng. Hiện tượng này, đặc biệt rõ ràng khi các LLM mở rộng quy mô và dữ liệu, đã được khai thác cho các thử thách phức tạp như suy luận toán học, báo hiệu khả năng nổi bật mới trong hành vi mô hình. Flamingo tích hợp đầu vào thị giác với các LLM, cho phép học trong bối cảnh của các tác vụ thị giác-ngôn ngữ như chú thích hình ảnh và OCR thông qua giao diện dựa trên ngôn ngữ. Painter và SegGPT tiến hành nghiên cứu sớm về học trong bối cảnh thị giác. Lấy cảm hứng từ các khả năng nổi bật của các mô hình ngôn ngữ lớn, trong nghiên cứu này chúng tôi nghiên cứu vấn đề học trong bối cảnh đa phương tiện bằng cách mở rộng quy mô các mô hình đa phương tiện sinh tạo và chứng minh kết quả mạnh mẽ trong các tác vụ hiểu biết và tạo sinh rộng lớn.

## 5. Kết luận

Chúng tôi trình bày một mô hình đa phương tiện sinh tạo 37 tỷ tham số Emu2 thể hiện hiệu suất mạnh mẽ và tính linh hoạt trên nhiều tác vụ đa phương tiện trong các cài đặt trong bối cảnh. Emu2 phục vụ như một mô hình cơ sở và giao diện đa năng cho nhiều tác vụ đa phương tiện. Chúng tôi chứng minh kết quả state-of-the-art trên một loạt rộng các tiêu chuẩn đánh giá hiểu biết và tạo sinh đa phương tiện. Cụ thể, mô hình của chúng tôi vượt trội đáng kể so với các nghiên cứu trước đây trên các tiêu chuẩn LMM được đề xuất gần đây đòi hỏi khả năng tiên tiến hơn so với các tiêu chuẩn học thuật cổ điển. Emu2 cũng thể hiện khả năng đáng chú ý của việc tạo sinh thị giác có thể kiểm soát trong bối cảnh đa phương tiện, ví dụ như tạo sinh có căn cứ chủ đề/văn bản. Ngoài ra, chúng tôi xem xét các hạn chế và tác động xã hội rộng lớn của Emu2. Mặc dù có những điểm yếu được thảo luận, những kết quả này cho thấy rằng mô hình đa phương tiện sinh tạo ở quy mô lớn có thể là một bước quan trọng hướng tới việc phát triển các hệ thống đa phương tiện tổng quát có khả năng thích ứng.
