# Self-slimmed Vision Transformer
Zhuofan Zong1⋆, Kunchang Li3,4⋆, Guanglu Song2, Yali Wang3,5, Yu Qiao3,6,
Biao Leng1, Yu Liu2†
1Khoa Khoa học và Kỹ thuật Máy tính, Đại học Beihang
2SenseTime Research
3Phòng thí nghiệm chính Thị giác máy tính và Nhận dạng mẫu ShenZhen, Phòng thí nghiệm chung SIAT-SenseTime, Viện Công nghệ Tiên tiến Thâm Quyến, Viện Hàn lâm Khoa học Trung Quốc
4Đại học Viện Hàn lâm Khoa học Trung Quốc
5Chi nhánh SIAT, Viện Trí tuệ nhân tạo và Robot cho Xã hội Thâm Quyến
6Phòng thí nghiệm AI Thượng Hải
Tóm tắt. Vision transformer (ViT) đã trở thành các cấu trúc phổ biến và vượt trội hơn mạng nơ-ron tích chập (CNN) trong nhiều tác vụ thị giác. Tuy nhiên, những transformer mạnh mẽ này mang lại gánh nặng tính toán khổng lồ do việc so sánh token-với-token tốn kém. Các nghiên cứu trước đây tập trung vào việc loại bỏ các token không quan trọng để giảm chi phí tính toán của ViT. Nhưng khi tỷ lệ loại bỏ tăng lên, cách tiếp cận cứng nhắc này sẽ không thể tránh khỏi việc loại bỏ các token quan trọng, điều này hạn chế hiệu quả của nó. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp học tự-thu gọn chung cho ViT gốc, gọi là SiT. Cụ thể, chúng tôi đầu tiên thiết kế một Token Slimming Module (TSM) mới, có thể tăng hiệu suất suy luận của ViT thông qua tổng hợp token động. Như một phương pháp chung của việc loại bỏ token cứng, TSM của chúng tôi tích hợp mềm mại các token dư thừa thành ít token thông tin hơn. Nó có thể động điều chỉnh sự chú ý thị giác mà không cắt đứt các mối quan hệ token phân biệt trong hình ảnh, ngay cả với tỷ lệ thu gọn cao. Hơn nữa, chúng tôi giới thiệu một khung Feature Recalibration Distillation (FRD) ngắn gọn, trong đó chúng tôi thiết kế một phiên bản ngược của TSM (RTSM) để hiệu chỉnh lại token không có cấu trúc theo cách auto-encoder linh hoạt. Do cấu trúc tương tự giữa thầy và học sinh, FRD của chúng tôi có thể tận dụng hiệu quả kiến thức cấu trúc để hội tụ tốt hơn. Cuối cùng, chúng tôi thực hiện các thí nghiệm rộng rãi để đánh giá SiT của mình. Nó chứng minh rằng phương pháp của chúng tôi có thể tăng tốc ViT lên 1.7× với độ sụt giảm độ chính xác không đáng kể, và thậm chí tăng tốc ViT lên 3.6× trong khi duy trì 97% hiệu suất của chúng. Đáng ngạc nhiên, bằng cách chỉ trang bị LV-ViT với SiT của chúng tôi, chúng tôi đạt được hiệu suất tối tân mới trên ImageNet. Mã nguồn có tại https://github.com/Sense-X/SiT.
Từ khóa: Transformer, Token Slimming, Knowledge Distillation

1 Giới thiệu
Kể từ khi vision transformer (ViT) [10] bắt đầu kỷ nguyên cấu trúc transformer trong các tác vụ thị giác máy tính cơ bản [3,36,5], các biến thể transformer đã được

⋆Z. Zong và K. Li đóng góp như nhau trong thời gian thực tập tại SenseTime.
†Tác giả liên hệ.

2 Z. Zong, K. Li, et al.
Bảng 1: So sánh với các phương pháp cắt tỉa mô hình gần đây cho ViT. SiT của chúng tôi vượt qua tất cả các phương pháp khác dựa trên cắt tỉa cấu trúc hoặc loại bỏ cứng.

[THIS IS TABLE: Comparison table showing different methods with their type, reference, ImageNet Top-1 accuracy, and throughput performance. The table includes baseline DeiT and various approaches like structure pruning, token hard dropping, and token soft slimming.]

được thiết kế để thách thức sự thống trị của mạng nơ-ron tích chập (CNN). Khác với CNN chồng các tích chập để mã hóa các đặc trưng cục bộ một cách tuần tự, ViT trực tiếp nắm bắt các phụ thuộc token dài hạn. Tuy nhiên, do việc so sánh token-với-token tốn kém, các transformer mạnh mẽ hiện tại yêu cầu tính toán khổng lồ, hạn chế ứng dụng rộng rãi của chúng trong thực tế [12]. Do đó, trong bài báo này, chúng tôi nhằm thiết kế một khung học tổng quát để tăng hiệu suất của các vision transformer gốc.

Để làm cho ViT hiệu quả hơn, chúng tôi đã cố gắng khám phá các tính chất vốn có của việc so sánh token-với-token. Chúng tôi thực hiện một loạt thí nghiệm dựa trên LV-ViT, điều này cho thấy rằng sự chú ý thưa thớt với độ tương tự token cao tồn tại trong ViT. Hình 1a cho thấy độ tương tự token trở nên cao hơn ở các lớp sâu hơn, đặc biệt là ở ba lớp cuối. Bên cạnh đó, sự chú ý có xu hướng tập trung vào các token cụ thể ở các lớp sâu hơn (Hình 1b), điều này cho thấy số lượng token liên quan đến quyết định trở nên ít hơn. Những quan sát này chứng minh rằng chỉ một vài ứng viên token chỉ ra thông tin có ý nghĩa. Điều này gợi ý cho chúng tôi một chiều không phụ thuộc cấu trúc khả thi, số lượng token, để giảm chi phí tính toán. Trực quan, chúng ta có thể loại bỏ tuần tự các token dư thừa khi mạng đi sâu hơn.

Các nghiên cứu gần đây đã cố gắng nén token thông qua loại bỏ độc lập dữ liệu với việc tối thiểu hóa lỗi tái tạo [28], hoặc loại bỏ phụ thuộc dữ liệu với việc chấm điểm khả vi [24]. Tuy nhiên, loại bỏ độc lập dữ liệu yêu cầu tối ưu hóa từng lớp, điều này khó tổng quát hóa. Hơn nữa, việc loại bỏ token cứng sẽ không thể tránh khỏi việc loại bỏ các token quan trọng khi tỷ lệ loại bỏ tăng lên, ví dụ, hình dạng của chó otterhound bị phá hủy ở lớp sâu (Hình 1c), do đó hạn chế hiệu suất của nó như được thể hiện trong Bảng 1.

Chúng ta có thể thiết kế một phương pháp thu gọn token linh hoạt, do đó thông tin liên quan đến quyết định có thể được tổng hợp động vào một tập token thu gọn không? Để trả lời câu hỏi này, chúng tôi đề xuất thu gọn token mềm. Nó chứa một Token Slimming Module (TSM) ngắn gọn, tạo ra các token liên quan đến quyết định thông qua một ma trận trọng số phụ thuộc dữ liệu. Như được thể hiện trong Hình 1c, bằng cách chỉ đơn giản chèn nhiều TSM vào LV-ViT, mạng của chúng tôi có thể học cách định vị các token đối tượng chính. Quan trọng hơn, phạm vi chú ý có thể được phóng to tự động mà không cắt đứt các mối quan hệ token phân biệt, ví dụ, mạng của chúng tôi có thể thích ứng tập trung vào các phần thông tin nhất của chó otterhound theo cách mềm mại hơn, trong khi mặt nạ oxy theo cách cứng nhắc hơn.

Trong DynamicViT [24], tự chưng cất được giới thiệu ở lớp cuối để tối thiểu hóa sự sụt giảm hiệu suất do thưa thớt token. Tuy nhiên, nó bỏ qua kiến thức gợi ý ở các lớp trung gian, dẫn đến mất mát kiến thức không thể tránh khỏi. Để giải quyết vấn đề này, chúng tôi giới thiệu một Feature Recalibration Distillation (FRD) ngắn gọn để đạt được tối ưu hóa thu gọn mô hình ổn định và hiệu quả. Lưu ý rằng các phương pháp chưng cất kiến thức gợi ý trước đây [25,42,39,17] được thiết kế cho token có cấu trúc không gian. Vì thông tin token lân cận là liên tục, chúng có thể áp dụng phép nâng mẫu liên tục (ví dụ, deconvolution và nội suy) để tìm tương ứng giữa token của thầy và học sinh. Ngược lại, TSM của chúng tôi tạo ra tập token không có cấu trúc, không thể được phân bổ giám sát tương ứng trực tiếp. Để căn chỉnh các mối quan hệ token giữa các token không có cấu trúc, chúng tôi thiết kế một phiên bản ngược của token slimming module (RTSM) theo cách auto-encoder linh hoạt. Cách hiệu quả như vậy giúp FRD của chúng tôi truyền dày đặc tất cả thông tin token từ khối này sang khối khác. Được hưởng lợi từ việc kế thừa kiến thức bẩm sinh (kiến thức cấu trúc), FRD của chúng tôi phù hợp hơn để tự dạy chính mình. Như được thể hiện trong Bảng 1, SiT của chúng tôi có thể cải thiện thông lượng 43.2% mà không có bất kỳ sụt giảm hiệu suất nào, và tăng tốc suy luận trên 100% với sụt giảm độ chính xác không đáng kể.

Phương pháp học tự thu gọn của chúng tôi là linh hoạt và dễ tổng quát hóa cho tất cả các vision transformer gốc (SiT), ví dụ, DeiT [29], LV-ViT [16], v.v. Chúng tôi thực hiện các thí nghiệm rộng rãi trên ImageNet [8] để xác minh hiệu quả và hiệu suất. Thú vị là, phương pháp của chúng tôi không có tự chưng cất thậm chí có thể hoạt động tốt hơn DynamicViT [24] với chưng cất. Bên cạnh đó, SiT-XS đạt 81.8% độ chính xác top-1 với tốc độ suy luận 3.6× và SiT-L đạt 85.6% độ chính xác top-1 cạnh tranh trong khi chạy nhanh hơn 1.7×. Quan trọng hơn, SiT của chúng tôi dựa trên LV-ViT

4 Z. Zong, K. Li, et al.

đạt được hiệu suất tối tân mới trên ImageNet, vượt qua các CNN và ViT gần đây.

2 Công trình liên quan

2.1 Vision Transformer

Kiến trúc Transformer [31] lần đầu tiên được đề xuất cho dịch máy. Thành công của transformer trong NLP truyền cảm hứng cho việc ứng dụng transformer trong các tác vụ thị giác khác nhau, ví dụ, DETR [3] cho phát hiện đối tượng và ViT [10] cho nhận dạng hình ảnh. ViT là transformer thuần đầu tiên đạt được hiệu suất tối tân trên ImageNet [8]. Các biến thể ViT gần đây chủ yếu tập trung vào tối ưu hóa tốt hơn và hiệu suất mạnh mẽ hơn [43,41,40,34,13,4,9,35,11,7,38,15,19,18]. Tuy nhiên, ít trong số chúng khám phá để cải thiện hiệu suất của vision transformer [12]. Trong bài báo này, chúng tôi nhằm thiết kế một khung tối ưu hóa tổng quát có tên là học tự thu gọn để thúc đẩy hiệu suất của ViT.

2.2 Thu gọn Transformer

Việc tính toán lớn của tự chú ý cản trở ứng dụng rộng rãi của ViT, chẳng hạn như phát hiện và phân đoạn với hình ảnh đầu vào độ phân giải cao. Để giải quyết vấn đề này, một số công trình trước đây tập trung vào việc thiết kế chú ý thưa thớt [33,21] hoặc cắt tỉa cấu trúc [6]. S2ViTE [6] động trích xuất và huấn luyện các mạng con thưa thớt của ViT, trong khi duy trì ngân sách tham số nhỏ cố định. Tuy nhiên, cắt tỉa cấu trúc mô hình gặp khó khăn trong việc giảm độ trễ suy luận.

Các công trình khác cố gắng giảm sự dư thừa token [24,28,22,37] bằng cách hoàn toàn loại bỏ các token không quan trọng, mang lại nhiều cải thiện hơn về thông lượng so với cắt tỉa cấu trúc. Khác với các công trình trên, SiT của chúng tôi tổng hợp tất cả token thành ít token thông tin hơn theo cách mềm mại bằng một module thu gọn ngắn gọn. Nó có thể tự động phóng to phạm vi chú ý để định vị đối tượng chính để nhận dạng tốt hơn.

3 Phương pháp

Trong phần này, chúng tôi mô tả chi tiết phương pháp học tự thu gọn cho vision transformer (SiT). Đầu tiên, chúng tôi giới thiệu kiến trúc tổng thể của SiT. Sau đó, chúng tôi giải thích thiết kế quan trọng của SiT, tức là token slimming module (TSM) và feature recalibration distillation (FRD). Cuối cùng, chúng tôi so sánh kỹ lưỡng TSM và FRD của chúng tôi với các đối tác khác.

3.1 Tổng quan về học tự thu gọn

Khung tổng thể được minh họa trong Hình 2. Chúng tôi đầu tiên thiết kế một Token Slimming Module (TSM) nhẹ cho các ViT thông thường để thực hiện thu gọn token, và phiên bản ngược của nó (RTSM) để hiệu chỉnh lại các token không có cấu trúc cho chưng cất kiến thức gợi ý. Chúng tôi chia vision transformer thu gọn thành nhiều giai đoạn (ví dụ, 4 giai đoạn như trong các công trình trước [12,21]), trong đó số lượng token khác nhau tham gia vào tính toán feed-forward. Để giảm mất mát thông tin, chúng tôi đề xuất một feature recalibration distillation (FRD) từ khối tới khối, trong đó vision transformer gốc có thể phục vụ như một giáo viên để tối thiểu hóa sự khác biệt giữa chính nó và học sinh thu gọn. Cuối cùng, chúng tôi tích hợp TSM và FRD để tạo thành một phương pháp học tự thu gọn tổng quát cho tất cả ViT gốc.

3.2 Token Slimming Module

Cho một chuỗi N token đầu vào với C kênh X = [x1;x2;⋯;xN] ∈ RN×C, (class token bị bỏ qua vì nó sẽ không bao giờ bị cắt tỉa), thu gọn token nhằm tổng hợp động các token dư thừa để tạo ra N̂ token thông tin X̂ = [x̂1;x̂2;⋯;x̂N̂]:

X̂ = ÂX,                    (1)

trong đó Â ∈ RN̂×N là một ma trận trọng số chuẩn hóa:

∑(i=1 to N̂) Âi,j = 1, trong đó j = 1...N.     (2)

Phép toán như vậy có thể vi phân và thân thiện với huấn luyện đầu-cuối. Chúng tôi theo mẫu thiết kế của tự chú ý [32] và đề xuất một token slimming module (TSM) nhẹ được thể hiện trong Hình 3a:

Â = Softmax(Wqσ(XWk)T/τ),     (3)

6 Z. Zong, K. Li, et al.

trong đó Wk ∈ RC×C/2 và Wq ∈ RN̂×C/2 đều là các tham số có thể học được. σ và τ đại diện cho hàm phi tuyến (GELU) và hệ số tỷ lệ tương ứng. Tương tự như tự chú ý, TSM tạo ra một ma trận chú ý toàn cục, nhưng nó yêu cầu ít chi phí hơn nhiều về thông lượng và sử dụng bộ nhớ trong cả huấn luyện và suy luận. Hình 3c cho thấy các khối TSM chỉ yêu cầu chi phí không đáng kể. Nhờ hệ số tỷ lệ có thể học τ, sự chú ý có xu hướng thưa thớt trong các thí nghiệm của chúng tôi, có nghĩa là nó học cách tập trung vào các token thông tin nhất.

Loại bỏ cứng vs thu gọn mềm. Các công trình trước đây đã cố gắng nén token thông qua loại bỏ cứng [28,24], trong đó trọng số thu gọn Âi,j ∈ {0,1} là ma trận quyết định nhị phân, tức là loại bỏ hoặc giữ token tương ứng. Tuy nhiên, cách tiếp cận này với quyết định nhị phân dẫn đến mất mát thông tin nghiêm trọng nếu nhiều token bị loại bỏ. Điểm yếu như vậy hạn chế hiệu suất cao của chúng trên ImageNet [8], trong đó các đối tượng thường chiếm một phần lớn trong hình ảnh. Ngược lại, chúng tôi thiết kế thu gọn mềm với trọng số chuẩn hóa có thể học Âi,j ∈ (0,1), có thể phân biệt các token có ý nghĩa trong góc nhìn toàn cục. Như được thể hiện trong Hình 1c, thu gọn mềm của chúng tôi có thể động điều chỉnh phạm vi chú ý để bao phủ các vùng quan trọng để phân loại. Nó cho thấy rằng Â có thể thích ứng trở thành ma trận one-hot để giúp SiT của chúng tôi tập trung vào phần thông tin nhất.

3.3 Feature Recalibration Distillation

Hiệu chỉnh đặc trưng. Mặc dù thu gọn token giảm đáng kể độ trễ suy luận, khi sử dụng tỷ lệ thu gọn lớn, nó không thể tránh khỏi giảm độ chính xác do mất mát thông tin. Chưng cất kiến thức gợi ý là phương pháp phổ biến để duy trì thông tin có ý nghĩa ở các lớp trung gian, trong đó thử thách là căn chỉnh ngữ nghĩa đặc trưng giữa học sinh và giáo viên. Các công trình trước đây [25,39] áp dụng deconvolution không gian hoặc nội suy để đối phó với sự sai lệch này giữa các đặc trưng liên tục không gian. Tuy nhiên, nó không phù hợp cho các token thu gọn không có cấu trúc với ngữ nghĩa rời rạc không gian. Để giải quyết vấn đề này, chúng tôi thiết kế một phiên bản ngược của token slimming module (RTSM) để hiệu chỉnh lại các token không có cấu trúc theo cách auto-encoder linh hoạt (Hình 3b). Do đó, tất cả thông tin token có thể được truyền một cách liền mạch từ giáo viên. Lưu ý rằng chúng tôi chỉ thực hiện RTSM khi huấn luyện, do đó không có tính toán bổ sung nào được giới thiệu trong suy luận. Chúng tôi đầu tiên biến đổi tuyến tính các token thông tin thành nhiều ứng viên token, do đó sử dụng hàm phi tuyến (GELU) để lọc các biểu diễn quan trọng. Cuối cùng, một phép biến đổi tuyến tính khác được thực hiện để nén các ứng viên token:

X̂' = A2(σ(A1X̂)),                    (4)

trong đó A1 ∈ R4N×N̂ và A2 ∈ RN×4N trong các thí nghiệm của chúng tôi. Để tăng cường hơn nữa các biểu diễn token, chúng tôi giới thiệu một khối perception đa lớp (MLP) bổ sung [32] với kết nối dư (residual connection) [14]:

X' = X̂' + MLP(X̂').                    (5)

Các đặc trưng được hiệu chỉnh lại X' sẽ được buộc phải nhất quán với các đặc trưng giáo viên trong FRD, đảm bảo thông tin đầy đủ của các token thu gọn X̂.

Chưng cất kiến thức. Do cấu trúc mô hình không thay đổi, chúng tôi thiết kế một chưng cất kiến thức từ khối tới khối cho các đặc trưng được hiệu chỉnh lại:

Ltoken = (1/LN) ∑(i=1 to L) ∑(j=1 to N) (Xs i,j - Xt i,j)2,     (6)

trong đó Xs i,j và Xt i,j tham chiếu đến embedding token thứ j tại lớp thứ i của học sinh và giáo viên tương ứng. L có nghĩa là số lượng lớp. Lưu ý rằng Xs tham chiếu đến các token được hiệu chỉnh lại X' trong Eq. 5. Với loss tái tạo như vậy, mô hình học sinh sẽ được buộc duy trì càng nhiều kiến thức trong các token thông tin X̂. Bên cạnh đó, để giảm thiểu hơn nữa sự suy giảm hiệu suất phân loại do thu gọn token, chúng tôi giới thiệu chưng cất logits để tối thiểu hóa sự khác biệt dự đoán giữa học sinh và giáo viên:

Llogits = KL(ψ(Zs), ψ(Zt)),                    (7)

trong đó KL biểu thị loss phân kỳ Kullback-Leibler và ψ là hàm softmax. Zs và Zt tương ứng là các dự đoán của mô hình học sinh và giáo viên. Hơn nữa, FRD trên bổ sung cho chưng cất cứng trong [29]:

Lhard = CrossEntropy(ψ(Zd), yc),                    (8)

trong đó Zd chỉ ra dự đoán của đầu chưng cất và yc là quyết định cứng của giáo viên CNN bổ sung. Nó có thể cải thiện thêm hiệu suất với số epoch huấn luyện lâu hơn. Mục tiêu cuối cùng của chúng tôi về chưng cất cho học tự thu gọn là:

Ldist = λtoken Ltoken + λlogits Llogits + λhard Lhard,     (9)

8 Z. Zong, K. Li, et al.

trong đó λ là hệ số cân bằng ba loss chưng cất. Chúng tôi đặt λlogits = 2, λtoken = 2 theo mặc định. λhard được đặt thành 1 khi giáo viên CNN tham gia. Đối với mục tiêu huấn luyện của học tự thu gọn, chúng tôi đối xử như nhau với tác vụ phân loại và tác vụ chưng cất:

Lcls = CrossEntropy(ψ(Zs), y),                    (10)
Lglobal = Lcls + Ldist,                    (11)

trong đó y có nghĩa là ground truth, tức là nhãn one-hot.

FRD vs chưng cất kiến thức khác. Thứ nhất, các vision transformer hiện tại [29,30] đơn giản chọn một mạng giáo viên mạnh với kiến trúc hoàn toàn khác biệt, chẳng hạn như RegNet cho DeiT. Chỉ một ít kiến thức (ví dụ, dự đoán cuối cùng) có thể được kế thừa, do đó thông tin ngữ nghĩa ở lớp trung gian bị bỏ qua. Trong FRD, nhờ tính nhất quán giữa giáo viên và học sinh, chúng tôi tự nhiên thực hiện giám sát ở mức token dày đặc cho mỗi khối, điều này cải thiện đáng kể sự ổn định của việc bắt chước mô hình. Bên cạnh đó, phương pháp chưng cất kiến thức gợi ý phổ biến [25,39] chủ yếu được thiết kế cho các token có cấu trúc không gian. Như được thể hiện trong Hình 4a, chúng có thể đơn giản áp dụng phép nâng mẫu cục bộ và liên tục để tái tạo token. Tuy nhiên, như được thể hiện trong Hình 4b, thu gọn token tạo ra một tập token không có cấu trúc. Mỗi token chứa thông tin một phần của các token trước đó. Để hiệu chỉnh lại các đặc trưng không có cấu trúc, chúng tôi thiết kế một RTSM ngắn gọn theo cách auto-encoder linh hoạt. Do đó thông qua loss tái tạo, FRD của chúng tôi có thể buộc mô hình học sinh duy trì kiến thức đầy đủ trong các token thông tin.

4 Thí nghiệm

4.1 Chi tiết triển khai

Trong phần này, chúng tôi thực hiện các thí nghiệm toàn diện để phân tích thực nghiệm hiệu quả của phương pháp học tự thu gọn cho vision transformer (SiT) được đề xuất. Tất cả các mô hình được đánh giá trên bộ dữ liệu ImageNet [8]. Đối với các mô hình giáo viên của chúng tôi,

chúng tôi huấn luyện LV-ViT [16] theo các cài đặt gốc, nhưng chúng tôi thay thế module patch embedding bằng các tích chập chồng nhẹ được lấy cảm hứng từ LeViT [12]. Đối với các mô hình học sinh, tất cả các siêu tham số huấn luyện đều giống như DeiT [29] theo mặc định. Để khởi tạo, chúng tôi tải tất cả trọng số từ các mô hình giáo viên tương ứng để tăng tốc hội tụ và huấn luyện chúng trong 125 epoch. Nếu sử dụng giáo viên CNN bổ sung, chúng tôi mở rộng thời gian huấn luyện đến 300 epoch để cải thiện tốt hơn. Hơn nữa, chúng tôi đặt tốc độ học khởi tạo khác nhau cho backbone và nhánh hiệu chỉnh đặc trưng, lần lượt là 0.0002 × batch_size/1024 và 0.001 × batch_size/1024. Đối với thu gọn token, chúng tôi chèn TSM ba lần, do đó có bốn giai đoạn trong SiT. Tỷ lệ giữ mặc định N̂/N được đặt thành 0.5, có nghĩa là số lượng token bị giảm một nửa sau khi thu gọn.

4.2 Kết quả chính

Chúng tôi tiến hành học tự thu gọn cho LV-ViT [16], là vision transformer gốc tối tân. Bảng 2 cho thấy các cài đặt chi tiết của chúng tôi cho các biến thể SiT khác nhau. Đối với SiT-Ti và SiT-XS, chúng tôi khám phá khả năng suy luận nhanh của chúng, do đó chúng tôi chèn TSM vào các lớp đầu. Nó chứng minh rằng phương pháp tự thu gọn của chúng tôi có thể tăng tốc các vision transformer gốc lên 3.6×, trong khi duy trì ít nhất 97% độ chính xác của chúng. Bên cạnh đó, chúng tôi áp dụng một giáo viên CNN khác để cung cấp nhãn cứng như trong DeiT [29]. Kết quả cho thấy giám sát dự đoán bổ sung có thể cải thiện thêm hiệu suất. Đối với các biến thể khác,

chúng tôi chèn TSM vào các lớp sâu hơn. Đáng ngạc nhiên, với sự sụt giảm độ chính xác không đáng kể, SiT của chúng tôi nhanh hơn tới 1.7× so với các mô hình giáo viên của chúng. Đáng chú ý rằng, chưng cất CNN bổ sung mang lại ít cải thiện, chủ yếu vì giáo viên CNN kém hơn giáo viên transformer gốc (82.9% so với 83.3%/84.2%).

4.3 Hiệu quả và độ bền vững

So sánh với DynamicViT. Trong Hình 5a, chúng tôi so sánh phương pháp của mình với DynamicViT [24] trên DeiT-S [29]. Khi loại bỏ quá nhiều token, hiệu suất của DynamicViT suy giảm đáng kể. Mặc dù nó sử dụng chưng cất kiến thức để tối thiểu hóa khoảng cách, SiT của chúng tôi không có chưng cất vẫn vượt trội nhất quán dưới các tỷ lệ FLOP khác nhau, đặc biệt là dưới tỷ lệ nhỏ nhất. Bên cạnh đó, khi được trang bị FRD, SiT của chúng tôi có thể duy trì hiệu suất tốt hơn.

Vị trí TSM và tỷ lệ giữ. Để xác minh độ bền vững của phương pháp, chúng tôi thực hiện thí nghiệm trên SiT-Ti như được thể hiện trong Hình 5b. Nó cho thấy rõ ràng rằng tất cả các mô hình được lấy mẫu ngẫu nhiên đều vượt trội hơn các ViT phổ biến với chưng cất kiến thức, ví dụ, DeiT [29] và XCiT [1]. Bên cạnh đó, so với các đối tác khác dựa trên loại bỏ token cứng [24,28] và cắt tỉa cấu trúc [6], các mô hình của chúng tôi vượt trội hơn với biên độ lớn. Những kết quả này chứng minh SiT của chúng tôi không nhạy cảm với cài đặt vị trí TSM và tỷ lệ giữ. Để so sánh công bằng với các ViT tối tân, chúng tôi đặt các siêu tham số này theo FLOP.

Bảng 3: So sánh với tối tân trên ImageNet. Các mô hình được đánh dấu màu xám được huấn luyện với giám sát chưng cất từ CNN mạnh mẽ trong 300 epoch. SiT của chúng tôi đạt được sự cân bằng hiệu suất tốt nhất.

[Tiếp tục với bảng và nội dung còn lại...]

4.4 So sánh với tối tân

Trong Bảng 3, chúng tôi so sánh SiT với các CNN và ViT cạnh tranh khác. Để so sánh công bằng, chúng tôi nhóm các phương pháp này theo độ chính xác top-1 của chúng. Thông lượng được đo trên một GPU V100 16GB duy nhất dưới cùng cài đặt như LeViT [12]. SiT-Ti của chúng tôi cạnh tranh với LeViT, trong khi thông lượng gấp 3.2× so với EfficientNet [26]. Lưu ý rằng EfficientNet được thiết kế thông qua tìm kiếm kiến trúc mạng thần kinh rộng rãi và LeViT được thiết kế tỉ mỉ cho suy luận nhanh. Đối với các biến thể mô hình lớn hơn của chúng tôi, chúng hoạt động tốt hơn EfficientNetV2 [27] với các chiến lược huấn luyện đơn giản. So với LV-ViT gốc [16], SiT của chúng tôi nhanh hơn 1.5× so với những mô hình có độ chính xác tương tự.

Chúng tôi tiếp tục trực quan hóa các so sánh với giới hạn trên của CNN và ViT trong Hình 6a và 6b. Nó cho thấy rõ ràng rằng SiT của chúng tôi đạt được sự cân bằng tốt nhất giữa thông lượng và độ chính xác, vượt qua các CNN và ViT tối tân gần đây.

4.5 Nghiên cứu loại bỏ

Nếu không được chỉ định khác, tất cả các thí nghiệm cho việc loại bỏ được thực hiện trên SiT-Ti và chạy chỉ với 125 epoch huấn luyện dưới sự giám sát của mô hình giáo viên gốc. "Token-MLP" đề cập đến các lớp tuyến tính đôi dọc theo chiều token.

Thu gọn token có vượt trội hơn thu nhỏ mô hình không? Trong Bảng 4, chúng tôi so sánh thu gọn token với quy tắc thu nhỏ mô hình dưới cùng giới hạn tính toán. Đối với thu nhỏ mô hình, chúng tôi thích ứng kênh và độ sâu riêng lẻ. Lưu ý rằng hai mô hình trên được huấn luyện từ đầu trong 300 epoch với token labeling [16]. Đối với thu gọn token, chúng tôi chỉ đơn giản chèn TSM mà không có FRD. Chúng tôi cũng loại bỏ token và huấn luyện nó với chưng cất bổ sung như trong DynamicViT [24]. Nó cho thấy rằng tỷ lệ theo kênh đạt độ chính xác cao hơn so với tỷ lệ theo độ sâu nhưng với thông lượng thấp hơn. Bên cạnh đó, thu gọn token có thể cải thiện đáng kể thông lượng với hiệu suất cao hơn. Tuy nhiên, DynamicViT hoạt động kém hơn SiT của chúng tôi mà không có chưng cất, vì loại bỏ token cứng mất nhiều thông tin phân biệt với tỷ lệ thu gọn lớn. Những kết quả như vậy chứng minh việc chỉ đơn giản chèn TSM của chúng tôi vào ViT gốc có thể đạt được hiệu suất tuyệt vời.

Kiến thức cấu trúc có quan trọng với học tự thu gọn không? Chúng tôi tiếp tục điều tra xem liệu kiến thức cấu trúc có mang lại lợi ích cho hiệu suất như được thể hiện trong Bảng 5. Đối với các mô hình giáo viên, chúng tôi áp dụng các kiến trúc khác nhau (LV-ViT-S[16], CaiT-S24[30], và RegNetY-16GF[23]) nhưng độ chính xác tương tự để so sánh công bằng. Nó cho thấy rằng huấn luyện với trọng số đã được huấn luyện trước trong 125 epoch hội tụ đến kết quả cao hơn so với những mô hình được huấn luyện từ đầu trong 300 epoch. Hơn nữa, chúng tôi sử dụng kiến thức cấu trúc thông qua bắt chước từ khối tới khối, có thể tăng thêm hiệu suất. Nó cũng cho thấy rằng sự tương tự cao hơn giữa học sinh và giáo viên có thể mang lại cải thiện lớn hơn.

Học tự thu gọn có bền vững với các tỷ lệ FLOP khác nhau không? Trong Bảng 6, chúng tôi thực nghiệm huấn luyện các mô hình với tỷ lệ FLOP khác nhau. Khi tỷ lệ lớn hơn 0.5, FRD và chưng cất CNN của chúng tôi đều hữu ích để duy trì hiệu suất. Tuy nhiên, khi tỷ lệ nhỏ, chưng cất CNN dẫn đến sụt giảm hiệu suất cao hơn, trong khi FRD của chúng tôi chỉ làm giảm độ chính xác 2.0%. Những kết quả này chứng minh rằng phương pháp của chúng tôi bền vững với các tỷ lệ FLOP khác nhau.

Động vs Tĩnh: Cách tổng hợp nào hoạt động tốt hơn cho thu gọn token? Để khám phá xem liệu tổng hợp động có tốt hơn cho thu gọn token, chúng tôi thực hiện thí nghiệm loại bỏ như được thể hiện trong Bảng 7. Đối với tổng hợp tĩnh, chúng tôi chọn các phép toán độc lập dữ liệu khác nhau và duy trì tính toán tương tự: average pooling/convolution 3×3 với stride 2×2, và các lớp tuyến tính đôi với hàm GELU ("Token-MLP"). Nó cho thấy rằng các tham số có thể học là quan trọng cho thu gọn token vì average pooling dẫn đến sụt giảm độ chính xác nghiêm trọng. Bên cạnh đó, các phương pháp tổng hợp tĩnh với trọng số độc lập dữ liệu cho kết quả tương tự nhưng kém hơn TSM của chúng tôi (79.3% vs 80.1%). Những so sánh như vậy chứng minh rằng TSM của chúng tôi có thể tạo ra các token thông tin hơn.

Phép nâng mẫu liên tục có thể hiệu chỉnh lại các đặc trưng không? Chúng tôi đầu tiên hiệu chỉnh lại các token gốc bằng phương pháp nâng mẫu liên tục, ví dụ, nội suy song tuyến tính và deconvolution. Như được thể hiện trong Bảng 8, hai phương pháp liên tục không gian này sắp xếp sai các mối quan hệ token và làm tổn hại khả năng so với baseline (không có bắt chước từ khối tới khối). Ngược lại, "Token-MLP" không làm tổn hại biểu diễn token, và độ chính xác của nó có thể được tăng thêm lên 80.1% bằng cách chèn MLP.

Mỗi giám sát chưng cất có giúp ích không? Bảng 9 thể hiện rằng giám sát logits mềm Llogits mang lại 1.4% tăng độ chính xác. Khi tiếp tục giới thiệu giám sát kiến thức từ khối tới khối, mô hình của chúng tôi cải thiện độ chính xác 1.1%. Cuối cùng, kết hợp giám sát nhãn cứng bổ sung, độ chính xác đạt 80.6% với số epoch huấn luyện dài hơn.

Trọng số loss thích hợp là gì? Bảng 10 cho thấy các cài đặt trọng số loss bền vững trong SiT của chúng tôi (được huấn luyện trong 100 epoch). Trên thực tế, chúng tôi chỉ đơn giản chọn trọng số 2:2:1 để đảm bảo các giá trị loss khác nhau gần nhau trong huấn luyện sớm.

4.6 Trực quan hóa

Trực quan hóa thu gọn token định tính. Hình 7 cho thấy các hình ảnh gốc và quy trình thu gọn token của SiT-Ti. Chúng tôi quan sát thấy rằng các token có điểm số cao hơn, tức là token sáng hơn, được tập trung và có xu hướng bao phủ các đối tượng chính trong hình ảnh. Nó chứng minh TSM được đề xuất của chúng tôi có thể định vị các vùng quan trọng và dự đoán điểm số chính xác cho các token thông tin nhất.

5 Kết luận

Trong bài báo này, chúng tôi đề xuất một phương pháp học tự thu gọn chung cho vision transformer gốc (SiT), có thể tăng tốc ViT với sự sụt giảm độ chính xác không đáng kể. TSM ngắn gọn của chúng tôi tích hợp mềm mại các token dư thừa thành ít token thông tin hơn. Để huấn luyện ổn định và hiệu quả, chúng tôi giới thiệu khung FRD mới để tận dụng kiến thức cấu trúc, có thể truyền dày đặc thông tin token theo cách auto-encoder linh hoạt. Các thí nghiệm rộng rãi chứng minh hiệu quả của SiT. Bằng cách chỉ đơn giản trang bị LV-ViT với SiT của chúng tôi, chúng tôi đạt được hiệu suất tối tân mới trên ImageNet, vượt qua các CNN và ViT gần đây.

Lời cảm ơn. Công trình này được hỗ trợ một phần bởi Chương trình R&D Chính quốc gia của Trung Quốc theo Grant 2019YFB2102400, Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (61876176), Phòng thí nghiệm chung CAS-HK, Viện Trí tuệ nhân tạo và Robot cho Xã hội Thâm Quyến, Ủy ban Khoa học và Công nghệ Thượng Hải (Grant No. 21DZ1100100).
