# 2310.05295.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.05295.pdf
# File size: 2480202 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Visual Storytelling with Question-Answer Plans
Danyang Liu, Mirella Lapata, Frank Keller
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB
danyang.liu@ed.ac.uk, {mlap, keller}@inf.ed.ac.uk
Abstract
Visual storytelling aims to generate compelling
narratives from image sequences. Existing
models often focus on enhancing the represen-
tation of the image sequence, e.g., with external
knowledge sources or advanced graph struc-
tures. Despite recent progress, the stories are
often repetitive, illogical, and lacking in de-
tail. To mitigate these issues, we present a
novel framework which integrates visual rep-
resentations with pretrained language models
and planning. Our model translates the image
sequence into a visual prefix , a sequence of
continuous embeddings which language mod-
els can interpret. It also leverages a sequence
of question-answer pairs as a blueprint plan
for selecting salient visual concepts and deter-
mining how they should be assembled into a
narrative. Automatic and human evaluation
on the VIST benchmark (Huang et al., 2016)
demonstrates that blueprint-based models gen-
erate stories that are more coherent, interesting,
and natural compared to competitive baselines
and state-of-the-art systems.
1 Introduction
Visual storytelling involves narrating an engaging
and logically coherent story based on a sequence
of images (see the example in Figure 1). The task
lies at the intersection of natural language process-
ing and computer vision and has recently attracted
increasing interest from both communities (Wang
et al., 2022; Hsu et al., 2021; Xu et al., 2021; Chen
et al., 2021; Hsu et al., 2020; Wang et al., 2020;
Huang et al., 2016). Visual storytelling differs from
image captioning, which typically focuses on gen-
erating descriptive text, e.g., by identifying and
depicting objects within an image. It requires a
deeper understanding of how images and the events
they illustrate relate to each other in order to create
a convincing narrative.
Visual storytelling is commonly modeled as a
two-stage process. The image sequence is first
A small indie rock groupperformedforamusicstoreinourcity.The girlsdoingharmoniesinthesongswerephenomenal.Thebandhadthree guitarists.One of the guitaristsresembledJamesMorrison.More independent bandsplayedforthegrandopeningofthemusicstore.
Q0:Whatgroupperformedforamusicstoreinourcity?A0: a small indie rock groupQ1:Whatkindofmusicdidthegirlsdo?A1:harmoniesQ2: Who did the harmonies in the songs?A2: the girlsQ3:Howmanyguitaristsdidthegrouphave?A3:three guitaristsQ4: Who resembled James Morrison?A4: one of the guitaristsQ5: Who played for the grand opening of the music store?A5: more independent bandsFigure 1: Blueprint annotation for a visual story. Color-
coded answers are extracted from the gold story. Ques-
tions are generated by feeding the answers and the gold
story as context to a pretrained question generator.
encoded into a representation which typically in-
cludes image embeddings and detected objects.
Subsequently, a decoder generates a story token by
token based on the encoding of the image sequence.
Recent work has mainly focused on enhancing the
first stage of the generation process e.g., by leverag-
ing external knowledge sources (Hsu et al., 2021;
Chen et al., 2021; Hsu et al., 2020; Yang et al.,
2019). Advanced representations for image se-
quences have also been explored, such as scene
graphs (Hong et al., 2020) and story graphs (Hsu
et al., 2021). Despite recent progress, these meth-
ods struggle to produce meaningful narratives, are
prone to hallucination and repetition, often gener-
ate vague sentences, and have difficulty identifying
salient visual concepts.
We attribute the lack of story quality to at least
two reasons. Previous work on text-based genera-arXiv:2310.05295v2  [cs.CL]  17 Oct 2023

--- PAGE 2 ---
tion has demonstrated that planning can improve
story coherence, allowing to control the trajectory
of events, the characters described and their ac-
tions (Yao et al., 2019; Xu et al., 2018; Rashkin
et al., 2020; Goldfarb-Tarrant et al., 2020a; Fan
et al., 2019; Yang et al., 2022). However, plan-
ning has not been considered in the context of vi-
sual storytelling, existing models adopt black-box
architectures which are not particularly control-
lable or interpretable. Another limitation concerns
the nature of current models which are essentially
trained from scratch, and as a result have limited
language modelling and generalization capabilities
(they only see multimodal training samples; see top
of Figure 1). Although pretrained language mod-
els (Raffel et al., 2020; Lewis et al., 2020; Brown
et al., 2020) have been widely adopted for general-
purpose story generation, their potential for visual
storytelling remains unexplored.
In this work we propose an approach to visual
storytelling which integrates pretrained language
models with visual representations and incorpo-
rates an intermediate planning step before gener-
ating the full story. Our encoder translates the im-
age sequence into a visual prefix , a sequence of
continuous embeddings which language models
can interpret. Following Narayan et al. (2022), we
represent plans as a sequence of question-answer
pairs, called blueprints , which serve as a proxy for
content selection (i.e., what to say) and planning
(i.e., in what order). Blueprints are loosely related
to the Question-under-Discussion (QUD) theory of
discourse (Larsson, 2002; Roberts, 2012; De Kuthy
et al., 2020), which posits that text structure can be
analyzed by identifying implicit questions raised
and answered by subsequent spans of text. We
augment visual storytelling training data with story
blueprints (see Figure 1), which we obtain automat-
ically thanks to state-of-the-art question generation
technology.
We fine-tune pretrained language models to gen-
erate blueprints from image sequences andthe sto-
ries based on them. We showcase two types of
storytelling models, which vary in how the plan-
ning mechanism is implemented. A top-down
model generates the blueprint first and then contin-
ues to generate the corresponding story in one go,
whereas an integrated model interleaves planning
with text generation rather than determining a plan
in advance; generation is iteratively conditioned on
the image input, the blueprint and the story gener-ated so far. Experiments on the VIST benchmark
(Huang et al., 2016) show that blueprint-based mod-
els generate more coherent, interesting, and human-
like stories compared to the state of the art and large
language models (LLMs) like GPT-3.5, according
to automatic and human evaluation.
2 Related Work
Visual Storytelling Huang et al. (2016) intro-
duced visual storytelling as a vehicle for devel-
oping AI tools with human-like understanding
of grounded event structure and linguistic abili-
ties that go beyond descriptive language. While
earlier work (Gonzalez-Rico and Fuentes-Pineda,
2018; Kim et al., 2018) employed simple encoder-
decoder architectures (using CNNs to extract visual
features and RNNs to generate text), more recent
methods (Xu et al., 2021; Chen et al., 2021; Hsu
et al., 2020; Yang et al., 2019) leverage external
resources (e.g., ConceptNet) as a way of instilling
commonsense reasoning skills. Sometimes, scene
graphs are also used to model relations between
objects (Lu et al., 2016; Hong et al., 2020; Wang
et al., 2020). To our knowledge, none of these ap-
proaches make use of plan-based decoding. Hsu
et al. (2021) construct a graph representing the im-
age sequence (based on training data and external
resources) and identify the highest scoring path as
the best storyline encapsulated therein. The story-
line can be viewed as a form of planning, however,
on the encoder side.
Most existing approaches (Xu et al., 2021; Hsu
et al., 2020; Wang et al., 2020; Yang et al., 2019)
train Transformer models from scratch, with the ex-
ception of Chen et al. (2021), who employ a vanilla
BART model as a baseline without task-specific
adaptation. In contrast, our work leverages the lan-
guage modeling and generalization capabilities of
pretrained language models for visual storytelling.
Planning and Generation In the domain of au-
tomatic story generation, planning has been effec-
tive at capturing the content and structure of sto-
ries. The generation process is often decomposed
into two stages, namely planning an outline and
then elaborating on it, e.g., by filling in specific
details of a story. Plans have been represented
as a sequence of event or phrase keywords (Yao
et al., 2019; Xu et al., 2018; Rashkin et al., 2020),
character actions (Liu et al., 2020), plot structures
(Goldfarb-Tarrant et al., 2020a), and more elabo-
rate descriptions including details about the setting

--- PAGE 3 ---
of the story, its characters, and main plot points
(Yang et al., 2022).
The idea of having a separate planning stage has
also been explored for other text generation tasks
including summarization (Narayan et al., 2021;
Liu and Chen, 2021) and data-to-text generation
(Moryossef et al., 2019; Puduppully et al., 2022).
Our work is closest to Narayan et al. (2022) who
propose the use of question-answer pairs as inter-
mediate plans for summarization. However, their
approach is designed for descriptive text. Our work
extends their framework to a multimodal setting,
where the input consists of image sequences, and
the output are narratives characterized by more ab-
stract and figurative language.
3 Blueprint-based Visual Storytelling
LetIrepresent a sequence of kimages, denoted
as{v1, v2..., v k}. Given this input, our goal is to
generate a blueprint plan B(i.e., an ordered set of
question-answer pairs) and a story Sbased on it.
Most generation datasets do not include blueprint
annotations, and visual storytelling is no excep-
tion. We first describe how we automatically obtain
{Ii, Bi, Si}N
i=1training data samples (Section 3.1),
and then introduce our story generation models
(Section 3.2).
3.1 Blueprint Annotation
Let{Ii, Si}N
i=1denote a dataset consisting of pairs
of image sequences and their corresponding sto-
ries. We automatically create blueprint Bibased
on story Siusing state-of-the-art question genera-
tors (Romero, 2021; Raffel et al., 2020), coupled
with a filtering procedure to remove repetitions and
ill-formed questions.
The generation of question-answer pairs involves
two steps, namely answer extraction and question
generation. In the context of storytelling, captur-
ing key events is crucial for a compelling narra-
tive. While noun phrases and named entities are
commonly recognized as significant content units
in other tasks such as summarization (Narayan
et al., 2022; Deutsch and Roth, 2023), verb phrases
also play a vital role in conveying story dynamics,
actions, and relationships (Trabasso et al., 1989;
Eisenberg and Finlayson, 2017; Liu et al., 2020).
Therefore, in addition to named entities and noun
phrases, we also extract verb phrases as answer
candidates using the spaCy library.
We then generate questions for answer candi-
…VisualPrefix
Image Encoder
MappingNetworkFFφFFφCD
ELθConceptEmbedderFigure 2: Visual prefix construction. The pretrained
image encoder and concept detector are frozen. FF
refers to a feed-forward layer, CDandELdenote a
concept detector and embedding layer, respectively.
dates with a T5 model (Raffel et al., 2020; Romero,
2021) fine-tuned on the SQuAD reading compre-
hension dataset (Rajpurkar et al., 2018). The an-
swer and the story are provided as context to pre-
dict the corresponding question. We decontextu-
alize stories by replacing pronouns with their cor-
responding head mentions, using a state-of-the-art
coreference resolution model (Dobrovolskii, 2021).
Question-answer pairs are subsequently filtered
to eliminate noise which is unavoidable due to the
automatic preprocessing steps mentioned above.
We thus remove any question-answer pairs where
the answer is already present in the question. We
also employ a round-trip consistency check (Alberti
et al., 2019) which discards questions if they yield
answers different from those used to generate them.
3.2 Blueprint Models
Our approach leverages the generation capabili-
ties of pre-trained sequence-to-sequence models.
As our backbone model, we employ BART-base
(Lewis et al., 2020) which has been fine-tuned for
text generation. We adapt this model to our visual
storytelling task in two ways. Aside from enabling
the generation of blueprints, we convert the image
sequence to a visual prefix which the pretrained lan-
guage model can interpret. The pretrained language
model is prompted with this prefix to generate the
blueprint, and eventually the story.
Visual Prefix Construction Our model needs to
grasp what the image sequence is about, e.g., the
depicted objects, actions, and their associations.
Drawing inspiration from recent advances in vision
and language research (Mokady et al., 2021; Tsim-
poukelli et al., 2021; Alayrac et al., 2022; Zhai
et al., 2022; Liu et al., 2023; Huang et al., 2023),
we translate the input sequence of images into a

--- PAGE 4 ---
Visual PrefixPretrained Language ModelθQ0: What game did the drum line prepare for?A0: the high school football gameThe big drumline prepared for the High School football game. The big drumline prepared for the High School football game. The boys all walked out onto the field, ready for the game.Q0: Who walked out onto the field?A0: the boysQ1: How many boys walked out to the field?A1: all Iter.1
ContextEmbeddingLayerθBlueprintStory
Iter. 2Figure 3: Iterative blueprint model: in the first iteration, the embedding layer uses <START> as context; in the
second iteration, the context includes the blueprint and previously generated story (enclosed by blue dashed line). In
contrast, the top-down model takes only the visual prefix as input and predicts a global blueprint and corresponding
story in one go. For details on the visual prefix, see Figure 2.
sequence of continuous embeddings, aka a visual
prefix (see Figure 2).
Following previous work (Wang et al., 2018; Xu
et al., 2021; Chen et al., 2021), we use ResNet-152
(He et al., 2016) to extract visual features from im-
ages. We next employ a lightweight linear mapping
network that consists of a series of feedforward
layers, denoted as Fϕ, to map image features to
kvisual clues:
p1, . . . , p k=Fϕ(ResNet ( v1, v2, . . . , v k))(1)
where kis the image sequence length, and each
visual clue pihas the same dimensionality as a
token embedding of the pretrained language model.
To further instil world knowledge in our visual
prefix, we employ a concept detector. The latter
identifies specific objects within images, but also
actions, scenes, and attributes. For each image vi,
we retain the Kconcepts {c1
i, c2
i, ..., cK
i}with the
highest confidence score:
c1
i, . . . , cK
i= Concept (ResNet ( vi)) (2)
Concepts for each image are then concatenated
with a ⟨SEP⟩token and serve as input to the em-
bedding layer of the pretrained model. The visual
clues and concept embeddings are concatenated
to form the visual prefix V. The image encoder
and the concept detector remain frozen during the
training phase. Only the parameters in the mapping
network Fϕare updated (see Figure 2).
Top-down Planning This model takes image se-
quence Ias input and generates blueprint Band
storySin one go. More precisely, during decod-
ing, the model first generates the blueprint, which
then serves as a prompt guiding subsequent story
generation. Our training objective maximizes thelog-likelihood of the joint distribution:
max
θ,ϕNX
i=1logpθ,ϕ(Bi,Si|Ii) (3)
where (B,S)refers to the concatenation of
blueprint Band story S.θrepresents the param-
eters of the pretrained language model, ϕare the
parameters of the mapping visual network Fϕ, and
Ndenotes the size of the dataset.
We introduce special tokens Story: andPlan:
preceding the story and blueprint, respectively.
In experiments, our blueprints consist of answer-
question pairs {a1, q1, . . . , a m, qm}(rather than
question-answer pairs). We place the answer be-
fore its question to encourage the model to zoom
in on salient visual concepts depicted in the image
sequence. This ordering is intuitive for our story-
telling task: We first decide on what the story is
about and then elaborate on key concepts. Inciden-
tally, Narayan et al. (2022) also find that generating
the answer before the question performs better for
their summarization tasks. Finally, the model is
trained with the standard maximum likelihood ob-
jective to generate the joint target.
Iterative Planning This model employs an in-
cremental generation strategy to create the story.
Rather than generating in one step a global
blueprint and the story, planning and generation
are interleaved. At each time step, the iterative
model considers the image sequence andcontext
from previous steps, including the blueprint and
story generated so far. We gradually construct the
blueprint and its corresponding story sentence-by-
sentence; our planning is informed by generation
and vice versa, which we argue should be mutually
beneficial (they are conditioned on each other).
LetS={s1, s2, . . . , s k}, denote a target story
andB={b1, b2, . . . , b k}its blueprint where

--- PAGE 5 ---
Length of image sequences 5.0
Number of sentences in the story 5.0
Number of tokens per story 52.3
Number of QA pairs per story 11.1
Number of tokens per QA pair 10.3
Number of tokens per story plus QA pair 166.2
Table 1: VIST dataset Statistics (average values).
sirepresents the i-th sentence in the story, and bi
its associated blueprint. Each biconsists of answer-
question pairs, denoted as {ai
1, qi
1, . . . , ai
l(i), qi
l(i)},
where l(i)is the number of pairs in the i-th
blueprint. The training objective for the iterative
model is defined as follows:
max
θ,ϕNX
j=1kX
i=1logpθ,ϕ(bi+1, si+1|b1:i, s1:i,Ij)(4)
where b1:iands1:irefer to the blueprint and sen-
tences generated so far, from step 1to step i.
At each time step i, the encoder takes image
sequence Ias input, and the decoder takes the
context (i.e., blueprint and sentences generated so
far{b1, b2, . . . , b i;s1, s2, . . . , s i}) as a prompt to
predict the next blueprint bi+1and sentence si+1.
Therefore, the iterative model is trained on samples
{I,(b1:i, s1:i), bi+1, si+1}. We prefix (b1:i, s1:i),
bi+1, andsi+1with Context: ,Plan: , and Next Sen-
tence , respectively. To handle the first time step,
we introduce special token ⟨START ⟩as context to
predict b1ands1. We also use ⟨END⟩to indicate
the completion of an iteration (see Figure 3 for an
illustration). It is important to note that (b1:i, s1:i)
are masked out when computing the loss because
they serve as prompts to the decoder. We want to
avoid the model repeatedly predicting and overly
optimizing the blueprints and sentences that appear
at the beginning of the output.
4 Experimental Setting
4.1 Dataset
We performed experiments on the widely used
VIST dataset (Huang et al., 2016), which contains
10,117 Flickr albums and 210,819 unique photos.
Each training sample consists of k= 5images and
a corresponding story of k= 5sentences. As de-
scribed in Section 3.1, we augment each story with
an automatically generated blueprint.4.2 Implementation Details
Our models are built on top of BART-base (Lewis
et al., 2020) and finetuned with a learning rate
of3e-5, batch size of 64, and warm-up ratio of 0.05.
We select the best checkpoint on the validation set
using a QA-based metric which quantifies the ex-
tent to which the output story follows its blueprint
(see Section 4.4). During inference, we employ
beam search (size 5). For our visual prefix, we
employed the Clarifai Concept Detector which was
trained on a dataset containing 9,098 concepts and
20 million images (multiple concepts are assigned
to each image), and is integrated with the Incep-
tionV2 architecture (Szegedy et al., 2016) .
4.3 Comparison Systems
We compared our models against several base-
lines and state-of-the-art visual storytelling mod-
els. These included a vanilla BART-base model
with the same encoder and visual prefix as ours
but no planning ( VP-BART ; it generates the story
directly in an autoregressive manner without the
blueprint). KG-Story (Hsu et al., 2020) predicts a
set of words representative of the image sequence,
enriches them using external knowledge graphs,
and generates stories based on the enriched word
set.PR-VIST (Hsu et al., 2021) is a state-of-the-art
model which constructs a graph representing the
relations between elements in the image sequence,
identifies the best storyline captured therein, and
proceeds to generate a story based on it. The pro-
cess of constructing the story graph can be viewed
as a form of planning. Along similar lines, Chen
et al. (2021) build a common sense knowledge
graph capturing concepts in the image sequence,
and use MCSM , a Maximal Clique Selection Mod-
ule to identify which ones to write a story about.
They use BART-large to generate the story based
on selected concepts (and image features).
We also compared against an LLM that gener-
ates stories via prompting. We provide GPT-3.5
with a visual prefix, namely the concepts identified
in the image sequence, and a prompt which ex-
plains how to create the blueprint and generate the
story together with examples (in-context learning).
Details on the prompt can be found in Appendix A.
4.4 Automatic Evaluation
We evaluated our stories using BLEU, ROUGE,
METEOR, and CIDER, mainly to compare to pre-
vious work. Several studies (Hsu et al., 2022, 2021,

--- PAGE 6 ---
2020; Hu et al., 2020; Yang et al., 2019; Modi and
Parde, 2019) have demonstrated the inadequacy
of lexical matching metrics: they correlate poorly
with human judgments, and not do effectively mea-
sure the semantic similarity to human-written sto-
ries or the lexical richness of the generated stories.
We further employ story-specific metrics to as-
sess story quality aspects such as diversity, fluency,
naturalness, and grounding. Specifically, we use
two types of trigram repetition metrics (Yao et al.,
2019; Goldfarb-Tarrant et al., 2020b). Intra-story
repetition is a fluency metric, it measures the pro-
portion of trigrams repeated within a story. Inter-
story repetition examines trigram repetition across
stories. This metric evaluates diversity, high intra-
story repetition suggests that the model tends to
generate the same story even when conditioned on
different image sequences. We also use MAUVE
(Pillutla et al., 2021) to measure the naturalness of
the generated stories. MAUVE is a recently intro-
duced automatic metric for open-ended generation
which has high correlation with human judgements.
It computes the similarity of the distribution of
human-written text and machine-generated text.
To quantify the extent to which the generated
story is grounded , i.e., whether it accurately repre-
sents the content of the image sequence, we mea-
sure concept precision andrecall . Precision mea-
sures the number of words in the generated story
that align with the detected concept set, while re-
call assesses the number of words in the detected
concept set that are present in the generated story.
Finally, for our own models we also evaluate
whether the generated stories are faithful to their
blueprint. Drawing inspiration from recent stud-
ies on summary evaluation (Deutsch et al., 2021;
Fabbri et al., 2022), we measure how well the gen-
erated story answers questions from the predicted
blueprint. We utilize a RoBERTa-based (Liu et al.,
2019) QA model finetuned on the SQuAD dataset.
5 Results
Our results are summarized in Table 2. The
first block presents the performance of state-of-
the-art storytelling systems. The second block
presents variants of our approach: a vanilla BART
model, enhanced with a visual prefix (VP), and
two blueprint models which vary in the way plans
are generated, i.e., in a top-down fashion or itera-
tively. The third block contains GPT-3.5 models
with (+BP) and without blueprints.Pretrained Language Models Produce Better
Stories We observe that models based on pre-
trained language models (i.e., our models and
MCSM, outperform models trained from scratch
(i.e., KG-Story and PR-VIST) in terms of trigram-
repetition scores and MAUVE. This indicates that
we can maintain strong language modeling capabil-
ities while enabling pretrained language models to
process visual signals effectively.
The Visual Prefix is an Effective Interface be-
tween Image and Text MCSM is the only ex-
isting model that utilizes a pretrained language
model for visual storytelling. However, our base-
line model (VP-BART) demonstrates superior per-
formance in most story-specific metrics. Remark-
ably, this is achieved using a smaller pretrained
model (BART-base, 140M parameters); MCSM
is built on top of BART-large (400M parameters).
This highlights the effectiveness of our visual pre-
fix, indicating it successfully translates the image
sequence into a space that BART can understand.
Blueprint Models are Most Grounded Our
models outperform comparison systems in terms of
concept grounding. This confirms that an interme-
diate planning step allows the model to effectively
select salient concepts based on the visual prefix.
The top-down model in particular achieves the high-
est concept grounding recall, it stays close to the
image sequence, accurately describing the informa-
tion conveyed therein. The higher lexical matching
scores further support this observation. The iter-
ative blueprint model achieves the best concept
grounding precision (excluding GPT-3.5 models)
which in turn suggests that the stories generated
by this model exhibit a stronger grounding to the
images with fewer hallucinations.
The Iterative Model Generates Most Natural
and Faithful Stories Despite not achieving the
highest scores in lexical matching metrics, the itera-
tive blueprint model stands out in terms of MAUVE
evaluation. Compared to other models, it generates
more natural stories, closer to those written by hu-
mans. This finding suggests that humans might em-
ploy a similar iterative planning strategy, at least for
the short stories considered here; they construct a
narrative gradually rather than a global plan which
they subsequently convert into a story.
With regard to faithfulness, we observe that both
blueprint models achieve scores higher than 40%,
indicating effective translation of blueprints into

--- PAGE 7 ---
ModelRepetition ( ↓) Grounding ( ↑)MAUVE (↑)Faithful ( ↑)N-gram-based Metrics ( ↑)
Intra Inter Precis. Recall B-4 RLSum METEOR CIDER
KG-Story 1.03 88.72 4.55 3.46 3.86 — 9.8 27.3 32.3 7.9
PR-VIST 1.19 83.80 3.76 3.28 2.31 — 7.5 26.1 31.4 7.6
MCSM 2.85 77.48 5.12 5.89 11.01 — 8.1 27.7 31.4 7.6
VP-BART 0.22 83.70 4.31 3.23 11.31 — 8.6 26.6 31.0 6.8
+ BP (top-down) 0.08 81.51 5.17 11.56 8.32 44.73 9.9 28.5 33.6 7.2
+ BP (iterative) 0.29 72.70 5.22 3.59 28.25 51.66 7.0 26.1 30.3 5.5
+ BP (gold) 0.12 18.61 6.81 2.97 52.24 — 29.4 52.0 58.4 36.3
GPT-3.5 0.47 40.61 10.80 7.90 2.30 — 5.0 24.4 27.3 1.9
GPT-3.5 + BP 1.52 31.19 14.70 10.30 2.10 34.56 4.2 23.3 25.1 2.3
Table 2: Automatic evaluation results. We report intra- and inter-story trigram Repetition (lower is better), precision
and recall for concept grounding, MAUVE, Faithfulness, and a suite of commonly used metrics which rely on
lexical similarity between system stories and references. Best results are highlighted in bold font.
stories. Notably, the iterative model performs best
in terms of faithfulness, which suggests that trans-
lating the entire global blueprint into a story is more
challenging, whereas breaking down planning into
individual steps is more effective. To get an idea
of the upper bound performance for blueprint mod-
els, we ran the top-down model with silver standard
blueprints extracted from the human-written stories
(see row +BP (gold) in Table 2). As can be seen,
the MAUVE score jumps to 52.24, edging closer to
human-written stories (their MAUVE score is 69.6).
This further supports our hypothesis that our model
successfully leverages the blueprints and retains
the information captured in them.
GPT-3.5 Struggles with Blueprints We also
compared our approach to GPT-3.5, which we
adapted to our task with in-context learning. A
GPT-3.5 model enhanced with blueprints performs
well at concept grounding, i.e., it generates stories
which talk about what is depicted in the image.
However, these stories are neither human-like (see
the very low MAUVE score) nor faithful to the
intermediate blueprints (in fact they are 10% less
faithful compared to our iterative model). This sug-
gests that GPT-3.5 tends to ignore the plan, despite
being explicitly prompted with blueprints.
6 Human Evaluation
We conducted a judgment elicitation study to fur-
ther evaluate the stories generated by our mod-
els. Specifically, we compared the best perform-
ing blueprint model (iterative) and three other sys-
tems: (a) PR-VIST, which represents the current
planning-based state of the art; (b) VP-BART, our
proposed model without blueprints; and (c) ground
truth stories written by humans. Raters were shown
an image sequence, alongside two stories and wereasked to provide pairwise references along the fol-
lowing dimensions: Relevance ,Fluency ,Coher-
ence,Interestingness , and Overall . The full instruc-
tions are given in Appendix A. We recruited nine
native English speakers and elicited preferences for
100 stories (three judgments per story).
Our human evaluation results are summarized in
Table 3. The iterative blueprint model outperforms
PR-VIST across metrics. Our participants perceive
VP-BART stories as marginally more fluent and
coherent compared to those created by the iterative
model (even though they prefer iterative stories
overall). This discrepancy is likely due to the gen-
eration process introduced by the iterative model
which requires the decoder to produce a mix of
questions, answers, and corresponding sentences,
deviating from the traditional BART pretraining
pattern. This added complexity might result in
minor grammatical errors and pose challenges for
coherence, given that story generation is broken
down into separate steps instead of being a contin-
uous process. Nonetheless, the coherence scores
are fairly close.
The blueprint model excels in terms of interest-
ingness and grounding, indicating its effectiveness
in creating engaging and memorable stories. Our
model’s superior grounding performance aligns
with our hypothesis that blueprints serve not only
as a planning strategy but also as a visual concept
selector. This is due to the way blueprints are struc-
tured (as answer-question pairs), which explicitly
forces the model to first identify salient visual con-
cepts and then generate questions based on them.
Figure 4 shows example stories created by the
models used in our human evaluation study and
GPT-3.5+BP . The story generated by the iterative
model is coherent, rich in detail, and fluent. VP-

--- PAGE 8 ---
Choices(%)Iterative vs. PR-VIST Iterative vs. VP-BART Iterative vs. Human
Win Lose Tie Win Lose Tie Win Lose Tie
Fluency 47.0 37.6 15.4 40.5 42.9 16.6 10.9 84.3 4.8
Coherence 56.0 24.8 19.2 41.2 41.6 17.2 15.2 75.7 9.1
Interestingness 70.5 19.2 10.3 55.5 38.1 6.4 23.0 70.0 7.0
Grounding 50.9 40.6 8.5 45.1 41.7 13.2 7.8 79.6 12.6
Overall 58.1 25.6 16.2 42.6 41.5 15.9 12.6 80.0 7.4
Table 3: Human evaluation results. Raters provide pairwise story preferences in terms of fluency, coherence,
interestingness, grounding, and overall. VP-BART is a BART model enhanced with a visual prefix (VP) but no
planning; Iterative is our best blueprint model and PR-VIST is a state-of-the-art visual storytelling model. We report
the percentage of times the Iterative model Wins, Loses or is in a Tie with a comparison system. Unless underlined ,
differences between systems are statistically significant ( p <0.05; using the Wilcoxon signed-rank test).
PR-VIST:Iboughtacarttothemarketwithsomefood.Itlookedgreatinthemarket.Thereweresomanyoftheirenthusiasm.Theyevenhadasalewiththeirbumper.Themarketwassoldoutoffreshvegetables.the[location]carisveryniceandbeautiful.VP-BART:Iwenttothemarketyesterday.Thereweremanydifferentkindsoffruitsthere.Iboughtalotofthem.Theywereveryexpensive.AfterwardIwentbackhome.Iterative:TodaywedecidedtotakeasmallshoppingtriptotheMarket.Themarkethadsomanywonderfulthingstochoosefromthatwelookedatandboughtsomanyofthem.TherewasanarrayofdifferentfruitsthatIcouldhardlyresistbuying.Therewasalsoahugearrayofvarioustypesofpepper.Finallythedayhadcometoanendandwepiledinthetaxibacktoheadhome.GPT-3.5+BP:OnSaturdaymorning,Ivisitedthelocalmarket.Ithadalotoffruitsandvegetablesbeingsold.Theatmospherewasfriendlyandbuzzingwithmanyvendorsreadytohelpyou.Thefreshproducewasthebestthingaboutthemarket.Youcanalwaysfindfriendlyvendorsthatsellthemostdeliciousproduce.Thekeytofindingthebestbargainsatthemarketisbargaining.Themarketisalwaysagreatplacetoshopforfreshfruitsandvegetables.Human:Shoppersarrivedearlyforthemarket.Therewasavarietyofgoodssold.Somecarriedawaylargebundles.Everyitemwasfreshandcolorful.Thespotwasafavoriteamongproduceshoppers.
Figure 4: Examples of system output and human-written story for an image sequence.
BART generates a grounded and accurate story
without hallucination and semantic errors. How-
ever, it is a relatively plain narrative, offering lim-
ited detail about the market or the experience of
the characters. Compared to GPT-3.5+BP , the it-
erative model’s story follows the image sequence
more closely, mentioning details like an array of
different fruits andvarious types of pepper , which
significantly enhances storytelling.
7 Controllable Generation
In this section we showcase how the blueprint plan
allows us to control the content and length of model
output without additional mechanisms or training.
For example, in cases where the generated story
contains entities which do not appear in the image
sequence, it is possible to refine the story gener-
ation process, mitigating hallucinations. Specif-ically, we apply a filtering step which removes
non-grounded entities (and corresponding QA pair)
from the blueprint before generating the story.
We consider as non-grounded any blueprint entity
which is not included in the output of the concept
detector (see Section 3.2).
Figure 5 shows how this refinement approach
can be used to adjust the model’s output. In the first
example, we observe that the story generated with
a refined blueprint effectively avoids hallucinations
(highlighted in blue) and is overall more faithful.
However, it is important to note that imagination
plays a crucial role in crafting an engaging story, es-
pecially when the image sequence provides limited
information. Therefore, employing the refinement
method may result in shorter and less detailed sto-
ries, as illustrated in the second example. While
the refined blueprint successfully eliminates all hal-

--- PAGE 9 ---
Iterative+Grounded-Ent:Weweresoexcitedtoseewhereourvacationwasgoing.Wearrivedatthehotelanditwasquitebusy.Wecouldn'tbelievehowcrowdeditwasgetting.Someofthearchitecturewasverycool.Thisstorewasourfavoritesinceitwasclosetothecitycenter.Iterative:WetookatriptoBrazil.WewenttothebigCitywherewestayin,andtookalotsofpictures.Whenwearrived,therewereoompa pompa loompa hompa,andthepeoplewerewalkingaround.Wewereatthealtar,anditwasstanding.Weendedthedaygettinghungry,sowedecidedtoeatatalittleelegantrestaurant.
Iterative+Grounded-Ent:Therewasaquestionandanswersession.Everyonewaslistening.Thespeakerwasveryinteresting.Everyonewastakingnotes.Thespeakertalkedverylong.Iterative:Iwenttomycolleaguestheotherday.My bosswastheonethatinvitedmetoattendthemeeting.Hetalkedabouthowtoimprovethe companyforthefuture.Afterthepresentation,Ilookedandlistenedtomycoworkersonhowthecompanywouldbedoing.MybosswaspleasedthatIinvitedthemtotalkforhours.Wetalkedforhoursandwewereverypleasedwhenmybossasked.Faithful Story GenerationFigure 5: Comparison of stories generated by original
blueprint and refined blueprint models. Hallucinated
words are highlighted in blue.
lucinated entities, the resulting story appears plain
and lacks depth. Our blueprint method seems to
strike the right balance between accurate and capti-
vating story generation, prioritizing faithfulness to
the image sequence and creativity in storytelling.
Most visual storytelling systems generate
5-sentence stories, following the predefined story
structure of the VIST dataset (Huang et al., 2016).
Nevertheless, our iterative blueprint model can flex-
ibly modulate the length of the story by controlling
the number of iterative steps, thereby overcom-
ing the conventional sentence limitation. Figure 6
presents stories generated by this model with a max-
imum of 10 iterations. Despite the increased length,
the stories maintain coherence and are engaging.
8 Conclusion
In this work, we have introduced a novel approach
to visual storytelling which integrates visual rep-
resentations with pretrained language models and
a blueprint-based planning method for story gen-
Wetookacaroutontothestreetthatday.Herewearelookingatthecarthatthewinneroftheracewasin.Thecarwiththewinnerinbluewasveryclosethroughoutthewholerace.Itwasneckandneckastheoneinblueheldtheleadthroughouttheentirerace.Finally,wecrossedthelapandthewinnerpulledinforfirstplaceatthelandingline.Wewereveryclosetowinningtheraceandfinishinginthefinalplaceline.Itwasaveryvery,veryclosefinish!
Lastnight,thebandplayedanawesomeconcert.Theguitaristhadanincredibleperformance.Thekeyboardplayerwasincredibleaswell.Theguitaristwassuperb.Heplayedhisguitarverywell.Thenextsonghesangwasverygood.Theyhadsuchagoodperformance,andI'mgladtheywent.Theaudiencelovedthemsomuch.Thenightwasoverwhenthebandwroteandputtogethertheirmusic.Itwasafantasticwaytofinishthenight.
Long Story GenerationFigure 6: Stories generated within 10 iterations.
eration. Blueprint models leverage a sequence of
question-answer pairs as intermediate plans, en-
abling better selection of salient concepts from the
image sequence and guiding the construction of the
final narrative. Specifically, we have showcased
two model variants: a top-down model which re-
lies on a global plan, and an iterative model, which
interleaves planning with sentence generation. Our
experiments have shown that blueprint models ex-
cel in concept grounding and their ability to create
human-like stories. Additionally, they are control-
lable: Blueprints can be made shorter or longer
and their details can be refined (e.g., by emphasis-
ing specific entities or characters), thus enabling
human-in-the-loop and personalized storytelling.
We showcase examples of controllability in Sec-
tion 7. In the future, we would like to explore visual
storytelling with grounded characters and entities,
as well as tackle the generation of more complex
narratives, such as long-form stories.
Acknowledgments The authors gratefully ac-
knowledge the support of the UK Engineering
and Physical Sciences Research Council (grant
EP/W002876/1). Liu was supported by the UKRI
Centre for Doctoral Training in Natural Lan-
guage Processing, funded by the UKRI (grant
EP/S022481/1) and the University of Edinburgh.

--- PAGE 10 ---
Limitations
While our proposed model demonstrates effective
story generation, it has certain limitations. Firstly,
the grounding relation between the visual concepts
and the corresponding text may not always be clear,
leading to potential ambiguity in the generated sto-
ries. Furthermore, the model can sometimes suffer
from hallucinations due to falsely detected visual
concepts.
It is worth noting that our model was built on top
of BART-base (Lewis et al., 2020). It would be ben-
eficial to investigate the performance of larger mod-
els, as they could potentially enhance the quality
of the planning component and overall storytelling
capability.
Ethics Statement
Large Language Models This paper uses large
pretrained language models, which have been
shown to be subject to a variety of biases, to occa-
sionally generate toxic language, and to hallucinate
content. Model output used for the human evalua-
tion study (Section 6) was screened by the authors
for harmful content.
Experimental Participants The departmental
ethics panel judged our human evaluation study
to be exempt from ethical approval, as all partici-
pants were employees of the University of X, and
as such were protected by employment law. Par-
ticipants were paid at the standard hourly rate for
tutors and demonstrators at the university.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35:23716–23736.
Chris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin,
and Michael Collins. 2019. Synthetic QA corpora
generation with roundtrip consistency. In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 6168–6173, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.Hong Chen, Yifei Huang, Hiroya Takamura, and Hideki
Nakayama. 2021. Commonsense knowledge aware
concept selection for diverse and informative visual
storytelling. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence , volume 35, pages 999–
1008.
Kordula De Kuthy, Madeeswaran Kannan, Hae-
manth Santhi Ponnusamy, and Detmar Meurers. 2020.
Towards automatically generating questions under
discussion to link information and discourse structure.
InProceedings of the 28th International Conference
on Computational Linguistics , pages 5786–5798.
Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth.
2021. Towards question-answering as an automatic
metric for evaluating the content quality of a sum-
mary. Transactions of the Association for Computa-
tional Linguistics , 9:774–789.
Daniel Deutsch and Dan Roth. 2023. Incorporating
question answering-based signals into abstractive
summarization via salient span selection. In Pro-
ceedings of the 17th Conference of the European
Chapter of the Association for Computational Lin-
guistics , pages 575–588.
Vladimir Dobrovolskii. 2021. Word-level coreference
resolution. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7670–7675, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Joshua Eisenberg and Mark Finlayson. 2017. A sim-
pler and more generalizable story detector using verb
and character features. In Proceedings of the 2017
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 2708–2715, Copenhagen,
Denmark. Association for Computational Linguis-
tics.
Alexander Richard Fabbri, Chien-Sheng Wu, Wenhao
Liu, and Caiming Xiong. 2022. Qafacteval: Im-
proved qa-based factual consistency evaluation for
summarization. In Proceedings of the 2022 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies , pages 2587–2601.
Angela Fan, Mike Lewis, and Yann Dauphin. 2019.
Strategies for structuring story generation. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 2650–
2660.
Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph
Weischedel, and Nanyun Peng. 2020a. Content plan-
ning for neural story generation with aristotelian
rescoring. arXiv preprint arXiv:2009.09870 .
Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph
Weischedel, and Nanyun Peng. 2020b. Content plan-
ning for neural story generation with aristotelian
rescoring. In Proceedings of the 2020 Conference on

--- PAGE 11 ---
Empirical Methods in Natural Language Processing
(EMNLP) , pages 4319–4338, Online. Association for
Computational Linguistics.
Diana Gonzalez-Rico and Gibran Fuentes-Pineda. 2018.
Contextualize, show and tell: A neural visual story-
teller. arXiv preprint arXiv:1806.00738 .
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 770–
778.
Xudong Hong, Rakshith Shetty, Asad Sayeed, Khush-
boo Mehra, Vera Demberg, and Bernt Schiele. 2020.
Diverse and relevant visual storytelling with scene
graph embeddings. In Proceedings of the 24th Con-
ference on Computational Natural Language Learn-
ing, pages 420–430.
Chao-Chun Hsu, Zi-Yuan Chen, Chi-Yang Hsu, Chih-
Chia Li, Tzu-Yuan Lin, Ting-Hao Huang, and Lun-
Wei Ku. 2020. Knowledge-enriched visual story-
telling. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 34, pages 7952–7960.
Chi-Yang Hsu, Yun-Wei Chu, Vincent Chen, Kuan-
Chieh Lo, Chacha Chen, Ting-Hao Huang, and Lun-
Wei Ku. 2022. Learning to rank visual stories from
human ranking data. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 6365–
6378.
Chi-yang Hsu, Yun-Wei Chu, Ting-Hao Huang, and
Lun-Wei Ku. 2021. Plot and rework: Modeling story-
lines for visual storytelling. In Findings of the Asso-
ciation for Computational Linguistics: ACL-IJCNLP
2021 , pages 4443–4453.
Junjie Hu, Yu Cheng, Zhe Gan, Jingjing Liu, Jianfeng
Gao, and Graham Neubig. 2020. What makes a good
story? designing composite rewards for visual story-
telling. In Proceedings of the AAAI Conference on
Artificial Intelligence , volume 34, pages 7969–7976.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui, Owais Khan Mohammed, Qiang Liu, et al.
2023. Language is not all you need: Aligning
perception with language models. arXiv preprint
arXiv:2302.14045 .
Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,
Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross
Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Ba-
tra, et al. 2016. Visual storytelling. In Proceedings
of the 2016 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 1233–1239.
Taehyeong Kim, Min-Oh Heo, Seonil Son, Kyoung-
Wha Park, and Byoung-Tak Zhang. 2018. Glac
net: Glocal attention cascading networks for multi-
image cued story generation. arXiv preprint
arXiv:1805.10973 .Staffan Larsson. 2002. Issue-based dialogue manage-
ment . Citeseer.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:
Denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and comprehen-
sion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
7871–7880.
Danyang Liu, Juntao Li, Meng-Hsuan Yu, Ziming
Huang, Gongshen Liu, Dongyan Zhao, and Rui Yan.
2020. A character-centric neural model for auto-
mated story generation. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 34,
pages 1725–1732.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023. Visual instruction tuning.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Zhengyuan Liu and Nancy Chen. 2021. Controllable
neural dialogue summarization with personal named
entity planning. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 92–106, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Cewu Lu, Ranjay Krishna, Michael Bernstein, and
Li Fei-Fei. 2016. Visual relationship detection with
language priors. In European conference on com-
puter vision , pages 852–869. Springer.
Yatri Modi and Natalie Parde. 2019. The steep road
to happily ever after: an analysis of current visual
storytelling models. In Proceedings of the Second
Workshop on Shortcomings in Vision and Language ,
pages 47–57.
Ron Mokady, Amir Hertz, and Amit H Bermano. 2021.
Clipcap: Clip prefix for image captioning. arXiv
preprint arXiv:2111.09734 .
Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019.
Step-by-step: Separating planning from realization
in neural data-to-text generation. In Proceedings of
the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 2267–2277, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Shashi Narayan, Joshua Maynez, Reinald Kim Am-
playo, Kuzman Ganchev, Annie Louis, Fantine Huot,
Dipanjan Das, and Mirella Lapata. 2022. Condi-
tional generation with a question-answering blueprint.
arXiv preprint arXiv:2207.00397 .

--- PAGE 12 ---
Shashi Narayan, Yao Zhao, Joshua Maynez, Gonçalo
Simões, Vitaly Nikolaev, and Ryan McDonald. 2021.
Planning with learned entity prompts for abstractive
summarization. Transactions of the Association for
Computational Linguistics , 9:1475–1492.
Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaid
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. In NeurIPS .
Ratish Puduppully, Yao Fu, and Mirella Lapata. 2022.
Data-to-text generation with variational sequential
planning. Transactions of the Association for Com-
putational Linguistics , 10:697–715.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable questions
for squad. arXiv preprint arXiv:1806.03822 .
Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and
Jianfeng Gao. 2020. PlotMachines: Outline-
conditioned generation with dynamic plot state track-
ing. In Proceedings of the 2020 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP) , pages 4274–4295, Online. Association
for Computational Linguistics.
Craige Roberts. 2012. Information structure: Towards
an integrated formal theory of pragmatics. Semantics
and pragmatics , 5:6–1.
Manuel Romero. 2021. T5 (base)
fine-tuned on squad for qg via ap.
https://huggingface.co/mrm8488/
t5-base-finetuned-question-generation-ap .
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision. In
Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2818–2826.
Tom Trabasso, Paul Van den Broek, and So Young Suh.
1989. Logical necessity and transitivity of causal
relations in stories. Discourse processes , 12(1):1–25.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-
timodal few-shot learning with frozen language mod-
els.Advances in Neural Information Processing Sys-
tems, 34:200–212.
Eileen Wang, Caren Han, and Josiah Poon. 2022. Ro-
vist: Learning robust metrics for visual storytelling.
InFindings of the Association for Computational
Linguistics: NAACL 2022 , pages 2691–2702.Ruize Wang, Zhongyu Wei, Piji Li, Qi Zhang, and Xuan-
jing Huang. 2020. Storytelling from an image stream
using scene graphs. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 34, pages
9185–9192.
Xin Wang, Wenhu Chen, Yuan-Fang Wang, and
William Yang Wang. 2018. No metrics are perfect:
Adversarial reward learning for visual storytelling.
arXiv preprint arXiv:1804.09160 .
Chunpu Xu, Min Yang, Chengming Li, Ying Shen, Xi-
ang Ao, and Ruifeng Xu. 2021. Imagine, reason
and write: Visual storytelling with graph knowl-
edge and relational reasoning. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 35, pages 3022–3029.
Jingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-
aoyan Cai, and Xu Sun. 2018. A skeleton-based
model for promoting coherence among sentences in
narrative story generation. In Proceedings of the
2018 Conference on Empirical Methods in Natural
Language Processing , pages 4306–4315, Brussels,
Belgium. Association for Computational Linguistics.
Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan
Klein. 2022. Re3: Generating longer stories with
recursive reprompting and revision. arXiv preprint
arXiv:2210.06774 .
Pengcheng Yang, Fuli Luo, Peng Chen, Lei Li, Zhiyi
Yin, Xiaodong He, and Xu Sun. 2019. Knowledge-
able storyteller: A commonsense-driven generative
model for visual storytelling. In IJCAI , pages 5356–
5362.
Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin
Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-
and-write: Towards better automatic storytelling. In
Proceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pages 7378–7385.
Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas
Steiner, Daniel Keysers, Alexander Kolesnikov, and
Lucas Beyer. 2022. Lit: Zero-shot transfer with
locked-image text tuning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 18123–18133.

--- PAGE 13 ---
A GPT-3.5 Experimental Setting
We designed the following prompts for GPT-3.5:
Prompt (w/o Blueprint): I want you to act as
a visual storyteller by creating a story based on a
given sequence of images. For each image, I’ll pro-
vide the key concepts. The concepts from different
images will be separated by <SEP>. \n Concepts:
..., \n Story: ..., ..., \n Concepts: ..., \n Story:
Prompt (w/ Blueprint): I’d like you to act as a
visual storyteller by creating a story based on a
given sequence of images. For each image, I’ll pro-
vide the key concepts separated by <SEP>. Your
task is to first generate a series of question-answer
pairs for each image as part of the planning pro-
cess, and then use these pairs to create the final
story. \n Concepts: ..., \n Plan: ..., \n Story: ..., ...,
\n Concepts: ..., \n Plan: ..., \n Story:
In-context learning examples in the prompts are
not shown for brevity. To make the best of in-
context learning, we employed max-shot learning,
while adhering to the token limit of 4,096.
B Human Evaluation Study
As mentioned in Section 6 we conducted a judg-
ment elicitation study to further evaluate the stories
generated by our models. Specifically, we com-
pared the best performing blueprint model (itera-
tive) and three other systems: (a) PR-VIST, which
represents the current planning-based state of the
art; (b) VP-BART, our proposed model without
blueprints; and (c) ground truth stories written by
humans. Raters were shown an image sequence,
alongside two stories and were asked to provide
pairwise preferences along various dimensions of
story quality. We describe below our evaluation
procedure and reproduce the instructions given to
our raters.
B.1 Evaluation Procedure
We initially conducted a pilot study, based on which
we devised our instructions. Subsequently, 100 im-
age sequences were randomly selected from the test
set, leading to a total of 300 pairwise comparisons.
We employed the expertise of 9 native speakers
who triple-annotated the 300 pairwise comparisons,
resulting in a total of 900 judgments.
B.2 Experimental Instructions
Human raters were asked to compare and evaluate
stories generated by different systems using pair-wise judgments. The evaluation focused on the
following dimensions of story quality: Relevance ,
Fluency ,Coherence ,Interestingness , and an Over-
alljudgment. We provide their definitions below.
Relevance Relevance captures whether the sen-
tences in the stories relate to the input images. For
each sentence, if the sentence accurately describes
the content of a specific image (i.e., not imagining
something that does not exist in the image), it will
be marked as relevance. Otherwise, if the sentence
does not correspond to an image or is too vague, it
will be not marked as relevance.
Fluency Fluency evaluates the grammatical cor-
rectness of the text. A story is fluent if it has few
or no grammatical errors and is easy to understand.
Coherence Coherence assesses whether the story
makes sense. A coherent story flows well, the sen-
tences are related, and logically connected. In con-
trast, an incoherent story would be more or less
incomprehensible, without any logical connection
between its sentences.
Interestingness This evaluates whether the story
contains unique, possibly unexpected elements.
For example, a memorable storyline. Below are
examples of a dull story and an interesting story.
Overall Taking all the aforementioned criteria
into consideration, the annotators selected their
preferred story for the given set of five images.
B.3 Annotation Interface
We designed an annotation interface using Python
Flask. Figure 7 shows a screenshot of the interface.

--- PAGE 14 ---
Figure 7: Screenshot of the annotation interface.
