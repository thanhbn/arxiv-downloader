# 2309.07117.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.07117.pdf
# Kích thước tệp: 361557 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
KHOA HỌC TRUNG QUỐC
Khoa học Thông tin
2025, Tập 68, Số , 000000:1–000000:2
https://doi.org/10.1007/s11432-024-4276-4
©Science China Press 2025 info.scichina.com link.springer.com.TIN TỨC & QUAN ĐIỂM .

PILOT: Một Bộ Công Cụ Học Liên Tục
Dựa trên Mô hình Đã Huấn luyện Trước
Hai-Long Sun1,2, Da-Wei Zhou1,2*, De-Chuan Zhan1,2& Han-Jia Ye1,2*
1Trường Trí tuệ Nhân tạo, Đại học Nam Kinh, Trung Quốc
2Phòng thí nghiệm Quốc gia Trọng điểm về Công nghệ Phần mềm Mới, Đại học Nam Kinh, Trung Quốc
Nhận 9 tháng 4 năm 2024/Sửa đổi 12 tháng 7 năm 2024/Chấp nhận 17 tháng 1 năm 2025
Trích dẫn Sun H-L, Zhou D-W, Zhan D-C, et al. PILOT: Một Bộ Công Cụ Học Liên Tục Dựa trên Mô hình Đã Huấn luyện Trước. Sci China
Inf Sci, 2025, 68(): 000000, https://doi.org/10.1007/s11432-024-4276-4

Những tiến bộ nhanh chóng trong học sâu đã dẫn đến
những thành tựu đáng kể trong nhiều lĩnh vực khác nhau. Tuy nhiên,
thế giới luôn thay đổi của chúng ta thường trình bày dữ liệu huấn luyện dưới
dạng luồng từ một môi trường mở. Ví dụ,
trong khi ChatGPT thể hiện khả năng suy luận đặc biệt,
nó gặp khó khăn trong việc cung cấp cho người dùng thông tin
cập nhật nhất. Thách thức này phát sinh từ chi phí cao liên
quan đến việc huấn luyện lại mô hình GPT trên dữ liệu mới hàng ngày.
Do đó, khả năng cập nhật mô hình liên tục là
cực kỳ quan trọng. Học liên tục đã được đề xuất
như một giải pháp cho thách thức này, cho phép các mô hình học
từ dữ liệu luồng. Một mối quan tâm chính trong học liên tục
là quên thảm khốc, khi các mô hình quên thông tin
đã học trước đó khi thu thập kiến thức mới. Nhiều
phương pháp đã được phát triển để giải quyết vấn đề này và
cho phép các mô hình học từ dữ liệu mới mà không quên
kiến thức cũ. Trong bài báo này, chúng tôi tập trung vào
thiết lập Học Tăng dần Lớp (CIL), đây là một kịch bản phổ biến
trong học liên tục.

Các phương pháp truyền thống giả định rằng các mô hình được "huấn luyện
từ đầu". Tuy nhiên, với sự tiến hóa nhanh chóng của các
kỹ thuật huấn luyện trước, Các Mô hình Đã Huấn luyện Trước (PTM) đã
trở nên được sử dụng rộng rãi cho các nhiệm vụ hạ nguồn. Những PTM này
thường được huấn luyện trên các tập dữ liệu rộng lớn hoặc các bộ dữ liệu hình ảnh
khổng lồ, dẫn đến khả năng tổng quát hóa mạnh mẽ. Do đó,
nghiên cứu về CIL đang chuyển từ huấn luyện mô hình từ đầu
sang tận dụng sức mạnh của PTM. Theo một khảo sát gần đây [1],
các phương pháp dựa trên PTM thể hiện hiệu suất vượt trội
đáng kể so với các phương pháp truyền thống dựa vào
khởi tạo ngẫu nhiên. Điều này đặt ra một câu hỏi quan trọng:
Liệu vẫn còn cần thiết phải nghiên cứu CIL truyền thống?
Để giải quyết câu hỏi này, chúng tôi không chỉ tái tạo các
phương pháp tiên tiến trong CIL dựa trên PTM mà còn
sửa đổi một số phương pháp truyền thống để tương thích với PTM.
Điều này cho phép so sánh công bằng giữa các phương pháp dựa trên PTM
và các phương pháp truyền thống.

Chúng tôi mở mã nguồn Bộ Công Cụ Học Liên Tục dựa trên
Mô hình Đã Huấn luyện Trước (PILOT) cho cộng đồng
học máy. Nó bao gồm một số phương pháp CIL truyền thống
được sửa đổi bởi PTM và cung cấp các thuật toán tiên tiến
để thúc đẩy nghiên cứu CIL dựa trên PTM. Mã nguồn
của PILOT có sẵn tại https://github.com/sun-hailong/
LAMDA-PILOT.

So sánh với Các Bộ Công Cụ Khác. Vì cộng đồng học máy
hiện tại thiếu một bộ công cụ bao gồm nhiều phương pháp
dựa trên PTM, có nhu cầu cấp bách để phát triển một
bộ công cụ dựa trên PTM chuyên dụng. Bộ công cụ này sẽ
tạo điều kiện cho nghiên cứu tiên tiến và cho phép so sánh
công bằng giữa các phương pháp truyền thống và các phương pháp dựa trên PTM
sử dụng cùng một backbone. Chúng tôi chủ yếu so sánh PILOT và
các bộ công cụ khác trong ba khía cạnh sau:

Tích hợp PTM. PILOT không chỉ bao gồm các phương pháp
CIL truyền thống mà còn mở rộng hỗ trợ cho các phương pháp
CIL dựa trên PTM mới nhất. Ngược lại, các bộ công cụ khác
chủ yếu tập trung vào các phương pháp CIL thông thường và
chưa khám phá việc tích hợp PTM.

Kiến trúc Mạng và Điều chỉnh Tham số. Bằng cách
chuyển đổi từ backbone ResNet điển hình sang sử dụng
PTM, chúng tôi thiết kế một cài đặt tham số và phương pháp
điều chỉnh độc đáo. Trong khi các bộ công cụ truyền thống
có thể mở rộng để phù hợp với PTM, chúng chủ yếu được
thiết kế với Mạng Nơ-ron Tích chập (CNN). Do đó, các
tham số và siêu tham số phù hợp với CNN có thể không
tối ưu cho PTM.

Điểm chuẩn và Bộ dữ liệu. Chúng tôi cung cấp các điểm chuẩn
và bộ dữ liệu được tuyển chọn đặc biệt cho các kịch bản
liên quan đến PTM. Những tài nguyên chuyên dụng này có thể
đóng vai trò then chốt trong việc đạt được các chỉ số hiệu suất
chính xác và đánh giá phù hợp với CIL dựa trên PTM.

Các Thuật toán Được Triển khai. Trong PILOT, chúng tôi
triển khai 15 thuật toán điển hình cho CIL, bao gồm các
phương pháp truyền thống được sửa đổi bởi PTM và các phương pháp
dựa trên PTM. Dưới đây, chúng tôi liệt kê các phương pháp
dựa trên PTM mới nhất: Finetune liên quan đến việc huấn luyện
liên tục một mô hình đã được huấn luyện trước trên các nhiệm vụ mới.
Nó cập nhật tất cả các tham số và dễ bị quên thảm khốc nghiêm trọng.
SimpleCIL [2] xây dựng các bộ phân loại liên tục bằng cách
trích xuất các đặc trưng nguyên mẫu sử dụng PTM, mà không cần
huấn luyện bổ sung trên nhiệm vụ hạ nguồn. L2P [3] tích hợp
điều chỉnh gợi ý hình ảnh vào CIL sử dụng một bộ chuyển đổi
thị giác đã được huấn luyện trước và thiết lập một

* Tác giả liên hệ (email: zhoudw@lamda.nju.edu.cn, yehj@lamda.nju.edu.cn)arXiv:2309.07117v3 [cs.LG] 8 tháng 3 năm 2025

--- TRANG 2 ---
Sun H-L, et al. Sci China Inf Sci 2025, Tập 68, Số, 000000:2

[Hình 1 - Biểu đồ hiệu suất tái tạo trên CIFAR100, ImageNet-R, và VTAB với các backbone khác nhau]

Hình 1 Độ chính xác tăng dần được tái tạo trên CIFAR100, ImageNet-R, và VTAB. Các hình con (a) và (b) sử dụng backbone ViT-B/16-IN1K, trong khi các hình con (c) và (d) sử dụng backbone ViT-B/16-IN21K.

bộ gợi ý để chọn các gợi ý cụ thể cho từng trường hợp.
DualPrompt [4] đề xuất hai loại gợi ý dựa trên L2P,
tức là gợi ý chung và gợi ý chuyên gia. CODA-Prompt [5]
cải thiện quá trình lựa chọn gợi ý với cơ chế chú ý.
APER [2], dựa trên SimpleCIL, sử dụng điều chỉnh tham số
hiệu quả để có được mô hình thích ứng. Sau đó, nó nối
mô hình thích ứng với mô hình gốc để có được các đặc trưng
tăng cường cho việc xây dựng bộ phân loại dựa trên nguyên mẫu.
RanPAC [6] tiêm một lớp chiếu ngẫu nhiên đông lạnh với
kích hoạt phi tuyến để nắm bắt các tương tác giữa các đặc trưng
với chiều mở rộng. SLCA [7] cải thiện lớp phân loại bằng
cách mô hình hóa các phân phối theo lớp và căn chỉnh các lớp
phân loại theo cách hậu hoc. LAE [8] định nghĩa giao thức
học trực tuyến và ngoại tuyến, nơi mô hình trực tuyến được
cập nhật với mất mát entropy chéo, nhằm thu thập kiến thức
mới trong các nhiệm vụ mới. EASE [9] thiết kế một phương pháp
tập hợp không gian con mở rộng cho CIL dựa trên PTM.

Các Bộ dữ liệu Được Hỗ trợ. Do sự chồng chéo trong dữ liệu
giữa các điểm chuẩn dựa trên ImageNet và bộ dữ liệu được
huấn luyện trước, ImageNet không phải là lựa chọn thích hợp
để đánh giá các phương pháp CIL dựa trên PTM, chúng tôi
cung cấp một số điểm chuẩn mới cho CIL mà: 1) hoàn toàn
khác biệt với bộ dữ liệu ImageNet, 2) thể hiện khoảng cách
miền đáng kể từ ImageNet, do đó thách thức khả năng tổng quát
hóa của PTM, và 3) bao gồm các bộ dữ liệu quy mô lớn từ
nhiều miền khác nhau để thiết lập một điểm chuẩn tăng dần
lớp đa miền. Mặt khác, vì các mô hình đã được huấn luyện trước
có thể sở hữu kiến thức rộng lớn về các nhiệm vụ thượng nguồn,
chúng tôi đánh giá hiệu suất trên CIFAR100, CUB200, ImageNet-R,
ImageNet-A, ObjectNet, OmniBenchmark, và VTAB. Những bộ dữ liệu
này đại diện cho các điểm chuẩn CIL điển hình và bao gồm
các bộ dữ liệu ngoài phân phối thể hiện khoảng cách miền
đáng kể với ImageNet (tức là bộ dữ liệu được huấn luyện trước).
Cụ thể, có 50 lớp trong VTAB, 100 lớp trong CIFAR100,
200 lớp trong CUB, ImageNet-R, ImageNet-A, và ObjectNet,
và 300 lớp trong OmniBenchmark.

Phương pháp Đánh giá. Trong CIL, một chỉ số hiệu suất
được sử dụng rộng rãi là độ chính xác kiểm tra tại mỗi
giai đoạn tăng dần, được ký hiệu là Ab, trong đó b đại diện
cho chỉ số giai đoạn. Một chỉ số quan trọng khác là độ
chính xác trung bình trên tất cả các giai đoạn, được cho
bởi ¯A=1/B∑B b=1 Ab. Trong công trình này, chúng tôi đánh giá
hiệu suất tăng dần (độ chính xác Top-1) tại mỗi giai đoạn,
với kết quả được thể hiện trong Hình 1. Chúng tôi sử dụng
các bộ dữ liệu như CIFAR100, ImageNet-R, ObjectNet, và VTAB,
chia tất cả các lớp thành nhiều giai đoạn tăng dần. Do
thiếu một số tham số trong một số bài báo (ví dụ, L2P),
chúng tôi đã tối ưu hóa một bộ tham số phù hợp cho những
phương pháp này. Thật khích lệ khi quan sát thấy rằng
hầu hết các thuật toán được triển khai lại đều khớp hoặc
vượt quá các điểm chuẩn hiệu suất của bài báo gốc. Hơn nữa,
chúng tôi thấy rằng mặc dù các phương pháp truyền thống sử dụng
backbone PTM và bảo tồn một số mẫu để phát lại, hiệu suất
của chúng nói chung thấp hơn so với các phương pháp dựa trên PTM.
Điều này nhấn mạnh tầm quan trọng của việc tận dụng các
kỹ thuật huấn luyện trước để thiết kế các phương pháp CIL hiệu quả.

Kết luận. Chúng tôi đã giới thiệu PILOT, một bộ công cụ
học liên tục dựa trên mô hình đã huấn luyện trước. Nó bao gồm
một tập hợp các phương pháp CIL dựa trên PTM được tái tạo
và cung cấp các thuật toán tiên tiến cho nghiên cứu nâng cao.
PILOT nhằm tạo điều kiện cho nghiên cứu và phát triển sáng tạo
trong lĩnh vực học liên tục. Trong tương lai, chúng tôi sẽ
tiếp tục cập nhật bộ công cụ của mình, mở rộng nó để bao gồm
nhiều thuật toán và bộ dữ liệu hơn, và áp dụng nó cho
phạm vi thiết lập rộng hơn.

Lời cảm ơn Công trình này được hỗ trợ một phần bởi
Chương trình R&D Quốc gia Trọng điểm của Trung Quốc (2022ZD0114805),
NSFC (62476123, 62376118, 62006112, 62250069, 61921006),
Quỹ Nghiên cứu Cơ bản cho các Đại học Trung ương
(2024300373, 14380021), Chương trình Trọng điểm của Quỹ Khoa học
Giang Tô (BK20243012), Quỹ Nghiên cứu Mở CCF-Tencent Rhino-Bird
RAGR20240101, Dự án AI & AI cho Khoa học của Đại học
Nam Kinh, Trung tâm Hợp tác Đổi mới Công nghệ Phần mềm
Mới và Công nghiệp hóa.

Thông tin hỗ trợ Phụ lục A. Thông tin hỗ trợ có sẵn
trực tuyến tại info.scichina.com và link.springer.com.
Các tài liệu hỗ trợ được xuất bản như đã gửi, không
sắp chữ hoặc chỉnh sửa. Trách nhiệm về tính chính xác
khoa học và nội dung hoàn toàn thuộc về các tác giả.

Tài liệu tham khảo
1 Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, và
De-Chuan Zhan. Học liên tục với các mô hình đã huấn luyện
trước: Một khảo sát. Trong IJCAI, trang 8363–8371, 2024.
2 Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, và
Ziwei Liu. Xem lại học tăng dần lớp với các mô hình đã
huấn luyện trước: Khả năng tổng quát hóa và thích ứng
là tất cả những gì bạn cần. Tạp chí Quốc tế về Thị giác
Máy tính, 2024.
3 Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, và Tomas Pfister. Học gợi ý cho học liên tục.
Trong CVPR, trang 139–149, 2022.
4 Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent
Perot, Jennifer Dy, et al. Dualprompt: Gợi ý bổ sung
cho học liên tục không cần luyện tập lại. Trong ECCV,
trang 631–648. Springer, 2022.
5 James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta,
Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,
Rameswar Panda, Rogerio Feris, và Zsolt Kira. Coda-
prompt: Gợi ý dựa trên chú ý phân tách liên tục
cho học liên tục không cần luyện tập lại. Trong CVPR,
trang 11909–11919, 2023.
6 Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Ab-
basnejad, và Anton van den Hengel. Ranpac: Chiếu ngẫu
nhiên và các mô hình đã huấn luyện trước cho học liên tục.
NeurIPS, 36, 2024.
7 Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen,
và Yunchao Wei. Slca: Người học chậm với căn chỉnh
bộ phân loại cho học liên tục trên mô hình đã huấn luyện
trước. Trong ICCV, trang 19148–19158, 2023.
8 Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang,
Bernard Ghanem, và Jian Zhang. Một khung học liên tục
thống nhất với điều chỉnh tham số hiệu quả tổng quát.
Trong ICCV, trang 11483–11493, 2023.
9 Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, và De-Chuan
Zhan. Tập hợp không gian con mở rộng cho học tăng dần
lớp dựa trên mô hình đã huấn luyện trước. Trong CVPR,
trang 23554–23564, 2024.
