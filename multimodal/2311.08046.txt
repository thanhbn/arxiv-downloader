# 2311.08046.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.08046.pdf
# File size: 9902651 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Chat-UniVi: Unified Visual Representation Empowers Large Language Models
with Image and Video Understanding
Peng Jin1,2,3Ryuichi Takanobu Wancai Zhang4Xiaochun Cao5Li Yuan1,2,3*
1School of Electronic and Computer Engineering, Peking University, Shenzhen, China2Peng Cheng Laboratory, Shenzhen, China
3AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, Shenzhen, China
4Nari Technology Co.,Ltd., China5School of Cyber Science and Tech., Shenzhen Campus of Sun Yat-sen University, Shenzhen, China
jp21@stu.pku.edu.cn yuanli-ece@pku.edu.cn
https://github.com/PKU-YuanGroup/Chat-UniVi
Abstract
Large language models have demonstrated impressive
universal capabilities across a wide range of open-ended
tasks and have extended their utility to encompass multi-
modal conversations. However, existing methods encounter
challenges in effectively handling both image and video un-
derstanding, particularly with limited visual tokens. In this
work, we introduce Chat-UniVi, a UnifiedVision-language
model capable of comprehending and engaging in conver-
sations involving images and videos through a unified visual
representation. Specifically, we employ a set of dynamic
visual tokens to uniformly represent images and videos.
This representation framework empowers the model to ef-
ficiently utilize a limited number of visual tokens to simul-
taneously capture the spatial details necessary for images
and the comprehensive temporal relationship required for
videos. Moreover, we leverage a multi-scale representa-
tion, enabling the model to perceive both high-level seman-
tic concepts and low-level visual details. Notably, Chat-
UniVi is trained on a mixed dataset containing both images
and videos, allowing direct application to tasks involving
both mediums without requiring any modifications. Exten-
sive experimental results demonstrate that Chat-UniVi con-
sistently outperforms even existing methods exclusively de-
signed for either images or videos. Code is available at
https://github.com/PKU-YuanGroup/Chat-UniVi.
1. Introduction
Large language models (LLMs), such as GPT-3 [7] and
LLaMA [63, 64], showcase substantial universal capabil-
ities that pave the way for achieving general artificial in-
telligence. However, language represents just one facet of
*Corresponding author: Li Yuan.
Input Vanilla tokens Dynamic tokens
(1, 3, H, W) (1, L, D) (1, C, D)
(M, 3, H, W) (M, L, D) (E, C, D)
Image Video
Figure 1. The unified representation framework for images
and videos utilizing a collection of dynamic visual tokens. “H”
and “W” represent the height and width of the input, respectively.
“L”, “D”, “M”, “C”, and “ E” denote the number of vanilla vi-
sual tokens, the feature dimension, the frame length, the number
of dynamic visual tokens, and the number of events, respectively.
communication. Visual information serves to augment and
enhance our comprehension of the world. Therefore, there
exists a burgeoning interest in developing a multimodal
conversation model that can accommodate various input
modalities simultaneously, including images and videos.
Recent advances in multimodal conversation models,
such as MiniGPT-4 [84], LLaV A [39, 40], and mPLUG-
Owl [73], focus on integrating visual tokens into LLMs.
Despite their commendable progress, existing methods of-
ten specialize in either image or video inputs. For instance,
methods [39, 40] that prioritize image inputs typically em-
ploy a larger number of visual tokens to attain finer spatial
understanding. Conversely, methods [45] concentrating on
video inputs often compromise spatial comprehension per
1arXiv:2311.08046v3  [cs.CV]  5 Apr 2024

--- PAGE 2 ---
frame to accommodate more frames for modeling temporal
relationships. Although some methods, e.g., Flamingo [1],
can extract a fixed number of tokens for each image and
video using a query transformer, their primary emphasis re-
mains on image understanding, lacking the capability to ef-
fectively model temporal comprehension, thus resulting in
a limited understanding of videos. Therefore, it is crucial
and challenging to enable LLMs for both image and video
comprehension within a unified framework.
In this paper, we introduce Chat-UniVi, a Unified
Vision-language model designed to proficiently compre-
hend and engage in conversations about both images and
videos. Chat-UniVi uniformly represents images and videos
using a collection of dynamic visual tokens, enabling it to
concurrently capture the spatial details of images and the
comprehensive temporal relationship of videos. As illus-
trated in Fig. 1, images can be depicted through visual to-
kens of diverse sizes. For example, the primary object, i.e.,
the sheep in Fig. 1, necessitates a fine-grained representa-
tion with numerous visual tokens, while the background,
i.e., the snow-capped mountain, can be sufficiently mod-
eled with only one visual token. In the case of videos,
the video is initially divided into several events, and sub-
sequently, these visual tokens expand over frames within
each event to encapsulate frame-level dynamics. Such uni-
fied representation for both images and videos significantly
reduces the number of visual tokens while maintaining the
expressive capabilities of the model. It is worth noting
that longer videos are assigned more visual tokens in our
method. Therefore, our method is better suited for variable-
length video understanding than existing methods.
To obtain these dynamic visual tokens, we propose
a token merging method for progressively merging vi-
sual tokens with similar semantic meanings. Specifically,
starting with visual tokens initialized by the vision trans-
former [15], we gradually group them by applying the k-
nearest-neighbor based density peaks clustering algorithm,
i.e., DPC-KNN [16], on the token features. When it comes
to videos, we also utilize DPC-KNN on the frame features
to get events. At each merging step, visual tokens assigned
to the same cluster are merged by averaging their token fea-
tures. Finally, we provide a multi-scale representation to
the LLMs, where the upper layers of the multi-scale rep-
resentation encompass high-level semantic concepts, while
the lower layers emphasize visual details representations.
The proposed Chat-UniVi has two compelling advan-
tages: First , its unified image and video modeling method
allows training on the mixed dataset of image and video,
enabling direct application to both image and video tasks
without any modifications. Second , the multi-scale repre-
sentation contributes to the comprehensive understanding
of images and videos, empowering Chat-UniVi to adapt to
various tasks, including employing high-level representa-
Image
Hallucination
Video84.2Figure 2. The proposed Chat-UniVi, designed as a unified
model, consistently outperforms even existing methods ex-
clusively designed for either images or videos. These results
demonstrate the advantages of the proposed method.
tion for semantic understanding and low-level representa-
tion for generating detailed descriptions. We evaluate Chat-
UniVi on both image and video understanding tasks. As
shown in Fig. 2, compared to other methods focused ex-
clusively on either images or videos, Chat-UniVi consis-
tently demonstrates superiority in comprehending images
and videos. Moreover, we also provide evidence of the ad-
vantages of joint training of images and videos for multi-
modal large language models. The main contributions are
summarized as follows:
• We propose a unified visual representation for LLMs, en-
abling LLMs to comprehend both images and videos.
• We uniformly represent images and videos using multi-
scale dynamic visual tokens and propose a token merging
method to obtain these dynamic visual tokens.
• Without fine-tuning, Chat-UniVi attains competitive per-
formance in both image and video tasks and achieves im-
pressive results in the object hallucination benchmark.
2. Related Work
Large Language Models. Large language models [50,
53, 65] have made disruptive progress, primarily attributed
to the expansion of training data and the substantial increase
in model parameters. Inspired by the success of GPT-3 [7],
numerous LLMs have subsequently been developed, includ-
ing PaLM [13], OPT [78], BLOOM [57], InstructGPT [48],
and ChatGPT [46]. However, language represents just one
facet of communication. Visual information serves to aug-
ment and enhance our comprehension of the world [5, 24–
27, 29, 66, 82]. In this work, we introduce Chat-UniVi,
designed to comprehend both image and video inputs.
Large-scale Multimodal Models. Existing large-scale
multimodal models [4, 9–11, 17–19, 31, 36–38, 43, 56, 68,
2

--- PAGE 3 ---
Visual 
Encoder
Pre-trained Large Language Model 
Projection W
 User Query:  What color is the boy's hair in the image and the pot 
used to cook the pasta in the video?
Chat-UniVi Response: The boy in the image hasblonde hair, and the pot used to cook the 
pasta in the video is red.
Visual 
EncoderFine-tune
 Frozen Concatenate
Visual inputs Original visual tokens Step one Step two Step three
Temporal MergingSpatial  MergingSpatial  MergingSpatial  Merging
Event 1
Event 2Image
Video
Figure 3. The overview of the proposed Chat-UniVi for conversations containing both images and videos. Chat-UniVi uniformly
represents images and videos using a collection of dynamic visual tokens and provides a multi-scale representation that equips large
language models to perceive both high-level semantic concepts and low-level visual details.
81, 83] can be broadly categorized into two classes. The
first class of methods [59, 61, 67, 72] involves using LLMs
as a dispatch scheduler, facilitating connections between
various expert models to handle different vision tasks. The
second class of methods [32, 32, 47] emphasizes the inte-
gration of models from different modalities into end-to-end
trainable models. More recently, there have also been sev-
eral dedicated multimodal models tailored for video pro-
cessing, such as Video-LLaV A [35], Video-ChatGPT [45],
VideoChat [33], and Video-LLaMA [76]. Despite their
commendable progress, existing methods often focus exclu-
sively on either image or video inputs. In this work, we fo-
cus on developing an end-to-end trained multimodal model
for both image and video tasks. Although Flamingo also
supports both image and video inputs, it can only extract a
fixed number of tokens for videos of varying lengths with
a query transformer. Recent works [9, 68] have explored
the use of separately pre-trained image and video encoders
for processing, but these methods introduce model redun-
dancy and prove challenging to train together. Hence, it
does not align with our focus on achieving a unified vision-
language model. In contrast to the previous works, the pro-
posed method uniformly represents images and videos us-
ing multi-scale dynamic visual tokens.
Dynamic Visual Token. There have also been recentmethods [6, 44, 54, 55, 70, 75] to explore the role of dy-
namic tokens within the transformer framework. However,
none of these methods can be directly extended to video.
We summarize the advantages of our method as follows:
(i)Supporting video input. In contrast to other methods,
Chat-UniVi extends the dynamic token method to incorpo-
rate video inputs, achieving the integration of image and
video representations for the first time. Our work is the first
to demonstrate that this unified representation can reconcile
the intricate spatial details of images with the broader tem-
poral understanding required for videos. (ii) Without pa-
rameters. Our clustering method is parameter-free. Inter-
estingly, we find that this parameter-free clustering method
serves as the linchpin to the success of our model. We at-
tribute this phenomenon to the gradient instability in multi-
modal conversation training, which hinders the convergence
of parameterized methods. Comparisons of Chat-UniVi and
other dynamic token methods are provided in the appendix.
3. Methodology
Chat-UniVi aims to model images and videos concurrently
within a language sequence that can be comprehended by
Large Language Models (LLMs) in a unified framework.
Chat-UniVi achieves this by uniformly representing images
3

--- PAGE 4 ---
and videos through a set of dynamic visual tokens, bridging
the intricate spatial details of images with the broader tem-
poral comprehension needed for videos. The overview of
the proposed Chat-UniVi is shown in Fig. 3.
3.1. Dynamic Visual Tokens for Image and Video
Building upon the vanilla Vision Transformer, most meth-
ods generate equally important visual tokens by dividing
the image into regular and fixed grids. However, it is evi-
dent that not all regions hold equal significance in vision-
language tasks. For example, capturing the background
may require only a single visual token. Drawing inspira-
tion from this insight, We amalgamate non-essential tokens
to derive dynamic vision regions as input for LLMs.
Spatial Visual Token Merging. For an input image, we
adopt the vision encoder of CLIP [51] to provide the orig-
inal visual tokens Z={zi}L
i=1, where Lis the number of
visual tokens each image is divided into. To amalgamate
non-essential visual tokens, we utilize DPC-KNN [16], a k-
nearest-neighbor based density peaks clustering algorithm,
to cluster the visual tokens. Starting with visual tokens
Z={zi}L
i=1initialized by the vision transformer, we first
compute the local density ρiof each token ziaccording to
itsK-nearest neighbors, which is formulated as:
ρi=exp 
−1
KX
zk∈KNN(zi,Z)∥zk−zi∥2
, (1)
where KNN (zi,Z)is the K-nearest neighbors of ziin
Z\{zi}. “Z\{zi}” denotes removing {zi}fromZ. Intu-
itively, ρidenotes the local density of token zi. Then, we
compute the distance index δiof the token zi:
δi=

min
j:ρj>ρi∥zj−zi∥2,if∃js.t.ρj> ρi.
max
j∥zj−zi∥2,otherwise.(2)
In essence, δirepresents the distance between the given to-
kenzifrom other high-density tokens. We identify those to-
kens with relatively high ρi×δias cluster centers and then
allocate other tokens to their nearest cluster center accord-
ing to the Euclidean distances. Finally, we utilize the aver-
age token within each cluster to represent the corresponding
cluster. The vision region of the merged token is the union
of the vision regions within the corresponding cluster.
Temporal Visual Token Merging. To adapt the dynamic
visual tokens to video inputs, we extend the visual tokens
across frames. However, directly consolidating all frames
into a limited number of visual tokens may lead to the loss
of temporal information within the video. For example,
in Fig. 3, the video demonstrates the process of cooking
pasta before preparing the sauce. Simply merging all frames
would pose challenges for the model in determining the cor-
rect sequence, such as whether to prepare the sauce first,cook the pasta first, or simultaneously cook the pasta while
preparing the sauce. Therefore, we propose temporal visual
token merging to first divide the video into several critical
events. Subsequently, we make the visual tokens only ex-
pand over frames within the same event.
Given the mthframeZm={zm
i}L
i=1of a video, we
first apply mean-pooling over all tokens to obtain the frame-
level representation fm. Similar to the spatial visual token
merging method, we leverage DPC-KNN to amalgamate
non-essential frames. Specifically, we first compute the lo-
cal density ρmand the distance index δmof each frame fm.
Frames with relatively high ρm×δmare identified as cluster
centers, and other frames are then assigned to their nearest
cluster center based on Euclidean distances. We treat each
cluster as a critical event and denote the set of indexes of
the frames in the cluster as F. Therefore, the set of visual
tokens within the ntheventFncan be formulated as:
˜Zn=
zm
i|m∈Fn, i∈ {1,2, ..., L}	
. (3)
After completing the temporal visual token merging, we ob-
tain the set of visual tokens within the event, i.e.,˜Z. To
make the visual tokens expand over frames within the event,
we adjust Eq. (1) and Eq. (2) in the spatial visual token
merging method to the following form:
˜ρi=exp 
−1
KX
zk∈KNN(zi,˜Z)∥zk−zi∥2
,
˜δi=

min
j: ˜ρj>˜ρi∥zj−zi∥2,if∃js.t.˜ρj>˜ρi.
max
j∥zj−zi∥2,otherwise.(4)
Finally, we concatenate the expanded dynamic visual to-
kens together in order of events to ensure the broader tem-
poral understanding required for videos.
Multi-scale Representation. To further enhance the ca-
pabilities of our model, we propose a multi-step aggregation
method designed to provide multi-scale visual features for
LLMs. Specifically, in Chat-UniVi, the initial visual tokens
at the first merging step are derived from the vision encoder
of CLIP. Then, we progressively merge visual tokens with
similar semantic meanings and obtain different numbers of
tokens in different steps. The higher-level features encom-
pass abstract semantic concepts, while the lower levels em-
phasize representations of visual details. In practice, we
execute a three-step aggregation process for each input im-
age or video. Finally, we concatenate the outputs from each
merging step and utilize a trainable projection matrix W
to transform these multi-scale visual features into language
embedding tokens, which serve as inputs for LLMs.
It is worth noting that despite the concatenation, the
number of visual tokens in our method remains significantly
lower than the original visual tokens generated by the vision
transformer. For example, while LLaV A [40] uses 256 vi-
sual tokens, our method utilizes only 112 visual tokens.
4

--- PAGE 5 ---
MethodsLLM VisualConversation Detail Reason AllSize Tokens
LLaV A [40] 13B 256 83.1 75.3 96.5 85.1
LLaV A [40] 7B 256 70.3 56.6 83.3 70.1
LLaV A [40]†7B 256 78.8 70.2 91.8 80.4
Chat-UniVi 7B 112 84.1 74.2 93.7 84.2
Table 1. GPT-based evaluation for image understanding.
“†” denotes our own re-implementation of LLaV A under our
training settings (same foundation model, same image data,
and same training scheme) for a fair comparison.MethodsLLMCorrect Detail Context Temporal ConsistencySize
Video-LLaMA [76] 7B 39.2 43.6 43.2 36.4 35.8
LLaMA-Adapter [77] 7B 40.6 46.4 46.0 39.6 43.0
VideoChat [33] 7B 44.6 50.0 50.6 38.8 44.8
Video-ChatGPT [45] 7B 48.0 50.4 52.4 39.6 47.4
Chat-UniVi 7B 57.8 58.2 69.2 47.9 56.2
Table 2. GPT-based evaluation for video understanding. The results
reported in Maaz et al. [45] span a range from 0 to 5. To standardize the
metrics, we normalize all scores to a scale of 0 to 100.
Methods LLM SizeSubject Context Modality GradeAverage
NAT SOC LAN TXT IMG NO G1-6 G7-12
Random Choice [42] - 40.28 46.13 29.25 47.45 40.08 33.66 39.35 40.67 39.83
Human [42] - 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40
Zero-shot Question Answering Accuracy (%)
GPT-4 [40] 1T+ 84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69
GPT-3 [42] 175B 75.04 66.59 78.00 74.24 65.74 79.58 76.36 69.87 74.04
LLaV A [40]†7B 47.78 41.96 53.64 47.90 44.03 51.92 49.63 45.29 48.08
Chat-UniVi 7B 58.61 61.08 61.82 57.33 58.25 61.39 62.04 56.23 59.96
Fine-tuning Question Answering Accuracy (%)
LLaV A [40] 13B 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92
LLaV A [40]†7B 79.71 91.68 82.82 80.94 83.24 81.46 83.74 81.74 83.02
LLaMA-Adapter [77] 7B 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19
LLaMA-SciTune [20] 7B 84.50 94.15 82.91 88.35 83.64 88.74 85.05 85.60 86.11
Chat-UniVi 7B 88.50 93.03 85.91 88.51 85.97 88.15 88.88 88.60 88.78
Table 3. Zero-shot and fine-tuning question answering accuracy on the ScienceQA test set. Question classes: NAT = natural science,
SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 =
grades 7-12. “†” denotes our own re-implementation of LLaV A under our training settings for a fair comparison.
3.2. Multimodal Training Scheme
Multimodal Pre-training. Following the approach of
previous works [40], our training is divided into two stages.
In the first stage, we pre-train the projection matrix W
while freezing both the LLM and the vision encoder. This
strategic freezing of the LLM empowers our method to ef-
fectively capture semantic visual information without any
discernible compromise in the performance of LLMs.
Joint Instruction Tuning. After completing the first
stage, the model is able to understand human queries but
still fails to generate reasonable and coherent linguistic re-
sponses. In the second stage, we fully fine-tune the large
language model and the projection matrix Won a multi-
modal instruction-following dataset. This dataset is a com-
posite of multi-turn conversations and single-turn conversa-
tions presented in a conversational format, alongside sin-
gle images, multiple images, and videos as visual input.
Through joint training on the mixture dataset, Chat-UniVi
achieves a superior comprehension of various directives and
produces more natural and dependable output. Moreover, it
exhibits the distinctive ability to seamlessly process both
images and videos without requiring any realignment.4. Experiments
4.1. Experimental Setup
Model Settings. We adopt the vision encoder of
CLIP (ViT-L/14) [51] as the visual foundation model. Be-
sides, we chose the Vicuna-v1.5 model [62], which consists
of 7B parameters, as our language foundation model.
Data and Training Details. For the multimodal pre-
training stage, we utilize the image-caption pairs from
various datasets, including COCO [12] and CC3M-595K
screened from CC3M [58] by LLaV A [40]. We pre-train
Chat-UniVi for one epoch with a batch size of 128, em-
ploying the AdamW [28, 41] optimizer with a cosine sched-
ule. The learning rate is set to 2e-3, and the warm-
up rate is 0.03. For the joint instruction tuning stage,
we incorporate multimodal instruction data from multiple
sources: (i) multimodal in-context instruction datasets, such
as MIMIC-IT [2, 22, 30], (ii) visual instruction datasets,
such as LLaV A, (iii) video instruction data from Video-
ChatGPT [45]. All input images or frames are resized to
224×224. We train Chat-UniVi for 2 epochs with a batch
size of 128, and the learning rate is set to 2e-5.
5

--- PAGE 6 ---
Methods LLM SizeMSRVTT-QA MSVD-QA TGIF-QA ActivityNet-QA
Accuracy Score Accuracy Score Accuracy Score Accuracy Score
FrozenBiLM [71] 1B 16.8 - 32.2 - 41.0 - 24.7 -
Video-LLaMA [76] 7B 29.6 1.8 51.6 2.5 - - 12.4 1.1
LLaMA-Adapter [77] 7B 43.8 2.7 54.9 3.1 - - 34.2 2.7
VideoChat [33] 7B 45.0 2.5 56.3 2.8 34.4 2.3 26.5 2.2
Video-ChatGPT [45] 7B 49.3 2.8 64.9 3.3 51.4 3.0 35.2 2.7
Chat-UniVi 7B 55.0 3.1 69.3 3.7 69.0 3.8 46.1 3.3
Table 4. Zero-shot video question answering accuracy. We follow the evaluation protocol in Maaz et al. [45], i.e., employing GPT-
assisted evaluation to assess the capabilities of models. “Score” denotes the confidence score from 0 to 5 assigned by the GPT model.
Methods LLM SizeRandom (POPE-R) Popular (POPE-P) Adversarial (POPE-A)
Accuracy F1-Score Yes Accuracy F1-Score Yes Accuracy F1-Score Yes
LLaV A [40] 13B 64.12 73.38 83.26 63.90 72.63 81.93 58.91 69.95 86.76
MiniGPT-4 [84] 13B 79.67 80.17 52.53 69.73 73.02 62.20 65.17 70.42 67.77
InstructBLIP [14] 13B 88.57 89.27 56.57 82.77 84.66 62.37 72.10 77.32 73.03
MultiModal-GPT [19] 7B 50.10 66.71 99.90 50.00 66.67 100.00 50.00 66.67 100.00
mPLUG-Owl [73] 7B 53.97 68.39 95.63 50.90 66.94 98.57 50.67 66.82 98.67
LLaV A [40]†7B 72.16 78.22 76.29 61.37 71.52 85.63 58.67 70.12 88.33
Chat-UniVi w/o multi-scale 7B 73.88 79.30 74.63 56.36 69.01 90.83 55.63 68.67 91.63
Chat-UniVi w/ multi-scale 7B 85.19 86.05 54.67 69.50 74.39 69.10 64.97 71.54 73.10
Table 5. Zero-shot object hallucination evaluation on the COCO validation set. We report the results of the polling-based object probing
evaluation (POPE). “Yes” represents the proportion of positive answers that the model outputs. “†” denotes our own re-implementation of
LLaV A under our training settings (same foundation model, same image data, and same training scheme) for a fair comparison.
4.2. GPT-based evaluation
Image Understanding. To quantitatively measure the
image understanding capability, we report the GPT-4 eval-
uation results in Tab. 1. Following Liu et al. [40], Zhang
et al. [79], we employ 90 questions based on 30 COCO val-
idation images, covering various aspects, including conver-
sation, detail description (Detail), and complex reasoning
(Reason). We utilize the GPT-4 model to evaluate the out-
puts of the model in these three aspects, as well as provide
an overall score. For a comprehensive description of im-
age understanding metrics, please refer to the appendix. As
shown in Tab. 1, Chat-UniVi uses fewer visual tokens while
achieving superior performance. Notably, our method, even
as a 7B model, can achieve the performance level of a 13B
model, demonstrating the effectiveness of our method.
Video Understanding. To quantitatively measure the
video understanding capability, we report the GPT evalua-
tion results in Tab. 2. Following Maaz et al. [45], we employ
a test set based on the ActivityNet dataset [8] and utilize the
GPT-3.5 model to assign a relative score to the outputs of
the model in the following five aspects: Correctness of In-
formation (Correct), Detail Orientation (Detail), Contextual
Understanding (Context), Temporal Understanding (Tem-
poral), and Consistency. Please refer to the appendix for
more details. As shown in Tab. 2, Chat-UniVi, even as a uni-
fied model, significantly surpasses recently proposed state-
of-the-art methods that exclusively focus on video, whichdemonstrates the effectiveness of our method.
4.3. Question-Answer Evaluation
ScienceQA Performance. ScienceQA [42] is a multi-
modal science question-answering dataset comprising 21k
multiple-choice questions. Each example in ScienceQA
contains a visual context, a textual context, a question, and
multiple options. We report both zero-shot and fine-tuning
results in Tab. 3. As shown in Tab. 3, Chat-UniVi shows
competitive performance across all metrics. Notably, Chat-
UniVi outperforms LLaMA-SciTune [20], a model specif-
ically tailored for science question answering, which fully
demonstrates the superiority of our method.
Zero-shot Video-question Answering Performance. In
Tab. 4, we show the zero-shot video-question an-
swering performance on several commonly used open-
ended question-answer datasets, including MSRVTT-
QA [69], MSVD-QA [69], TGIF-QA FrameQA [23], and
ActivityNet-QA [74]. Our evaluation protocol follows that
of Maaz et al. [45], utilizing GPT-assisted evaluation to as-
sess the capabilities of models. As shown in Tab. 4, Chat-
UniVi outperforms the recently proposed state-of-the-art
methods, e.g., FrozenBiLM [71], across various datasets.
4.4. Object Hallucination Evaluation
In Tab. 5, we report the results of the polling-based ob-
ject probing evaluation [34] (POPE). For details of the
polling-based object probing evaluation, please refer to the
6

--- PAGE 7 ---
MethodsImage Understanding Video Understanding
Conversation Detail Reason All Correct Detail Context Temporal Consistency
Only Image 84.0 69.3 89.3 81.5 43.4 48.6 56.8 36.6 46.2
Only Video 72.7 55.8 71.5 66.8 57.4 58.8 69.0 47.0 56.0
Image + Video 45.5 31.3 76.1 50.9 51.2 55.6 64.8 40.3 50.4
Video + Image 79.0 69.2 88.5 79.1 45.6 49.8 58.2 38.8 47.8
Image & Video 84.1 74.2 93.7 84.2 57.8 58.2 69.2 47.9 56.2
Table 6. Ablation study about instruction tuning scheme. “Only Image” indicates training solely on image data. “Image + Video”
means training on image data followed by fine-tuning on video data. “Image & Video” denotes training on a mixed dataset.
C1C2C3Visual Tokens Conversation Detail Reason All
16 8 4 28 78.6 69.0 95.1 81.1
32 16 8 56 82.7 67.2 94.5 81.6
64 32 16 112 84.1 74.2 93.7 84.2
128 64 32 224 79.8 68.7 83.8 79.8
Table 7. Ablation study about the number of spatial visual clus-
ters. “C1”, “C2”, and “ C3” denote the number of clusters at the
first step, the second step, and the last step, respectively.Clustering Ratio Correct Detail Context Temporal Consistency
1/M 51.2 41.8 47.6 28.0 42.2
1/32 57.2 58.0 69.6 45.8 54.2
1/16 57.8 58.2 69.2 47.9 56.2
1/8 56.8 58.2 68.0 46.2 57.8
Table 8. Ablation study about the number of temporal visual clus-
ters. “M” is the frame length. “ 1/M” denotes that the model directly
consolidates all frames into a single event.
(a) Comparison of image-based conversations (b) Comparison of video-based conversations
PerformancePerformance
Figure 4. Human evaluations. In 30 image conversation scenarios and 30 video conversation scenarios, the evaluators rate the model on
a scale of 0 to 10 based on its multimodal conversation performance. Finally, we use the average score as the final model score.
appendix. As shown in Tab. 5, Chat-UniVi outperforms
the recently proposed state-of-the-art methods. Moreover,
we find that multi-scale representation improves the abil-
ity to resist hallucinations. It is worth noting that, as a 7B
model, our method even outperforms the 13B model, such
as MiniGPT-4. We attribute this success to the multi-scale
representation that equips our method to perceive both high-
level semantic concepts and low-level visual appearance.
4.5. Ablative Analysis
Effect of the Tuning Scheme. In Tab. 6, we provide
the ablation study on the instruction tuning scheme. We
find that visual instruction tuning using only one type of
medium, such as images, results in a decrease in compre-
hension of another medium, such as videos. However, pre-
training on one medium and fine-tuning on another leads to
knowledge degradation from the pre-training stage. In con-
trast, our joint training strategy, which involves training on a
mixed dataset of images and videos, endows the model with
the capability to process both types of visual inputs. Among
all tuning schemes, joint training consistently achieves the
highest performance, confirming its effectiveness.
Effect of the Number of Spatial Visual Clusters. To
explore the influence of the number of spatial visual clus-ters, we provide the ablation results in Tab. 7. We find that
a smaller number of visual clusters may decrease the ca-
pacity to grasp fine visual details, whereas a larger number
of visual clusters may introduce redundancy and potentially
reduce the overall performance of the model. To strike a
balance between detailed understanding and model learn-
ing complexity, we set the number of clusters at the three
levels to 64, 32, and 16 respectively in practice.
Effect of the Number of Temporal Visual Clusters.
Videos vary in length, with longer videos typically con-
taining more events. Therefore, in Chat-UniVi, the num-
ber of temporal visual clusters is determined proportionally
based on the number of input video frames. As shown in
Tab. 8, we find that a smaller clustering ratio may result in
the loss of crucial temporal information within the video.
Conversely, a larger clustering ratio increases the computa-
tional overhead of the model. We observe that the model
performs optimally when the clustering ratio is set to 1/16.
Therefore, in practice, we adopt a default temporal cluster-
ing ratio of 1/16for optimal performance.
4.6. Qualitative Analysis
Human Evaluation. In our evaluation, we manually as-
sess the performance of Chat-UniVi and baselines in 30
7

--- PAGE 8 ---
Input image Step one Step two Step three Input image Step one Step two Step three Step three Step two Step one Input image Input video Step one Step two Step three
Event 1 Event 2 Event 3
Figure 5. Visualization of the dynamic visual tokens. For clarity in observation, we map the dynamic visual tokens of the video back to
each frame for visualization. Please refer to the appendix for additional visualizations and conversation examples of our model.
image conversation scenarios and 30 video conversation
scenarios. The results are presented in Fig. 4. Open-
Flamingo [3], derived from Flamingo [1], and Otter [30],
an in-context instruction tuning variant of OpenFlamingo,
are also included in our comparison. As shown in Fig. 4,
we find that methods based on Flamingo exhibit limita-
tions in their ability to comprehend videos. This limita-
tion is attributed to their use of a query transformer to ex-
tract a fixed number of visual tokens from videos of vary-
ing lengths, which hinders their effectiveness in modeling
temporal comprehension. In contrast, Chat-UniVi, func-
tioning as a unified model, not only outperforms methods
built upon the Flamingo but also surpasses models specifi-
cally designed for image and video.
Visualization of the Dynamic Visual Tokens. We pro-
vide the visualization in Fig. 5 and invite readers to explore
more visualizations in the appendix. It is important to em-
phasize that our proposed token merging method operates
without the need for object outline labels. As shown in
Fig. 5, the proposed dynamic visual tokens effectively gen-
eralize objects and backgrounds. This capability enables
Chat-UniVi to reconcile the intricate spatial nuances of im-
ages with the broader temporal understanding required forvideos with a limited number of visual tokens.
5. Conclusion
In this paper, we introduce Chat-UniVi, a unified multi-
modal large language model designed to comprehend and
engage in conversations about both images and videos.
To seamlessly bridge the intricate spatial nuances of im-
ages with the broader temporal understanding required for
videos, we propose a unified representation framework em-
ploying dynamic visual tokens. This representation lever-
ages DPC-KNN to progressively cluster visual tokens and
provides multi-scale features. More encouragingly, Chat-
UniVi is trained on a mixed dataset encompassing both im-
ages and videos, enabling it to be directly applicable to tasks
involving both media types without requiring any modifi-
cations. Extensive experimental results demonstrate that
Chat-UniVi, as a unified model, consistently surpasses even
methods exclusively designed for images or videos.
Acknowledgements. This work was supported
by the National Key R&D Program of China
(2022ZD0118101), Nature Science Foundation of China
(No.62202014), and Shenzhen Basic Research Program
(No.JCYJ20220813151736001).
8

--- PAGE 9 ---
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. In NeurIPS ,
pages 23716–23736, 2022. 2, 8, 13
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.
VQA: Visual question answering. In ICCV , pages 2425–
2433, 2015. 5, 14
[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-
source framework for training large autoregressive vision-
language models. arXiv preprint arXiv:2308.01390 , 2023.
8, 13
[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-VL: A frontier large vision-language model
with versatile abilities. arXiv preprint arXiv:2308.12966 ,
2023. 2
[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In ICCV , pages 1728–1738, 2021. 2
[6] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao
Zhang, Christoph Feichtenhofer, and Judy Hoffman. To-
ken merging: Your vit but faster. arXiv preprint
arXiv:2210.09461 , 2022. 3
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. In NeurIPS , pages
1877–1901, 2020. 1, 2, 14
[8] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. ActivityNet: A large-scale video
benchmark for human activity understanding. In CVPR ,
pages 961–970, 2015. 6, 18
[9] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang,
Jing Shi, Shuang Xu, and Bo Xu. X-LLM: Bootstrapping ad-
vanced large language models by treating multi-modalities as
foreign languages. arXiv preprint arXiv:2305.04160 , 2023.
2, 3, 13
[10] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
MiniGPT-v2: large language model as a unified interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478 , 2023.
[11] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
modal llm’s referential dialogue magic. arXiv preprint
arXiv:2306.15195 , 2023. 2
[12] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-
tam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence Zitnick.
Microsoft COCO Captions: Data collection and evaluation
server. arXiv preprint arXiv:1504.00325 , 2015. 5, 14[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. PaLM: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 2
[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. InstructBLIP: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500 , 2023. 6
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In ICLR , 2021. 2
[16] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on den-
sity peaks clustering based on k-nearest neighbors and prin-
cipal component analysis. Knowledge-Based Systems , 99:
135–145, 2016. 2, 4
[17] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-
angyu Yue, et al. LLaMA-Adapter V2: Parameter-efficient
visual instruction model. arXiv preprint arXiv:2304.15010 ,
2023. 2
[18] Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan
Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin,
Peng Jin, et al. SPHINX-X: Scaling data and parameters
for a family of multi-modal large language models. arXiv
preprint arXiv:2402.05935 , 2024.
[19] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
Luo, and Kai Chen. MultiModal-GPT: A vision and lan-
guage model for dialogue with humans. arXiv preprint
arXiv:2305.04790 , 2023. 2, 6
[20] Sameera Horawalavithana, Sai Munikoti, Ian Stewart, and
Henry Kvinge. Scitune: Aligning large language mod-
els with scientific multimodal instructions. arXiv preprint
arXiv:2307.01139 , 2023. 5, 6
[21] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In
ICLR , 2022. 15
[22] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In CVPR , pages 6700–6709, 2019. 5,
14
[23] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and
Gunhee Kim. TGIF-QA: Toward spatio-temporal reasoning
in visual question answering. In CVPR , pages 2758–2766,
2017. 6
[24] Peng Jin, Jinfa Huang, Fenglin Liu, Xian Wu, Shen Ge,
Guoli Song, David Clifton, and Jie Chen. Expectation-
maximization contrastive learning for compact video-and-
language representations. In NeurIPS , pages 30291–30306,
2022. 2
[25] Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian,
Chang Liu, Xiangyang Ji, Li Yuan, and Jie Chen. Video-text
9

--- PAGE 10 ---
as game players: Hierarchical banzhaf interaction for cross-
modal representation learning. In CVPR , pages 2472–2482,
2023. 13
[26] Peng Jin, Hao Li, Zesen Cheng, Jinfa Huang, Zhennan
Wang, Li Yuan, Chang Liu, and Jie Chen. Text-video
retrieval with disentangled conceptualization and set-to-set
alignment. In IJCAI , pages 938–946, 2023.
[27] Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji,
Chang Liu, Li Yuan, and Jie Chen. Diffusionret: Generative
text-video retrieval with diffusion model. In ICCV , pages
2470–2481, 2023. 2
[28] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[29] Nathan Labiosa, Dat Huynh, and Ser-Nam Lim. Visual in-
formation and large language models: A deeper analysis. 2
[30] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint
arXiv:2305.03726 , 2023. 5, 8, 13, 14
[31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
BLIP: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In ICML ,
pages 12888–12900, 2022. 2
[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 3
[33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355 , 2023. 3, 5, 6
[34] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
tion in large vision-language models. arXiv preprint
arXiv:2305.10355 , 2023. 6, 16, 20, 21
[35] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and
Li Yuan. Video-LLaV A: Learning united visual repre-
sentation by alignment before projection. arXiv preprint
arXiv:2311.10122 , 2023. 3
[36] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng
Jin, Junwu Zhang, Munan Ning, and Li Yuan. MoE-LLaV A:
Mixture of experts for large vision-language models. arXiv
preprint arXiv:2401.15947 , 2024. 2
[37] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-
bench: You see what you think? or you think what you see?
an image-context reasoning benchmark challenging for gpt-
4v (ision), llava-1.5, and other multi-modality models. arXiv
preprint arXiv:2310.14566 , 2023.
[38] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Ya-
coob, and Lijuan Wang. Mitigating hallucination in large
multi-modal models via robust instruction tuning. arXiv
preprint arXiv:2306.14565 , 2023. 2
[39] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 1[40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 1, 4, 5, 6, 14, 15, 18
[41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[42] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to explain: Multimodal reasoning via
thought chains for science question answering. In NeurIPS ,
pages 2507–2521, 2022. 5, 6
[43] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai
Sun, and Rongrong Ji. Cheap and quick: Efficient vision-
language instruction tuning for large language models. In
NeurIPS , 2023. 2
[44] Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang
Liu, and Yun Fu. Image as set of points. In ICLR , 2023. 3,
13, 14
[45] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-
had Shahbaz Khan. Video-ChatGPT: Towards detailed video
understanding via large vision and language models. arXiv
preprint arXiv:2306.05424 , 2023. 1, 3, 5, 6, 14, 18, 19
[46] OpenAI. Introducing chatgpt. CoRR , 2022. 2
[47] OpenAI. GPT-4 technical report. CoRR , 2023. 3
[48] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
InNeurIPS , pages 27730–27744, 2022. 2
[49] Ofir Press, Noah A Smith, and Mike Lewis. Train short,
test long: Attention with linear biases enables input length
extrapolation. In ICLR , 2022. 14
[50] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog , 1(8):9, 2019. 2
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision. In ICML , pages
8748–8763, 2021. 4, 5, 15
[52] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-
can, Jordan Hoffmann, Francis Song, John Aslanides, Sarah
Henderson, Roman Ring, Susannah Young, et al. Scaling
language models: Methods, analysis & insights from train-
ing gopher. arXiv preprint arXiv:2112.11446 , 2021. 14
[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485–5551, 2020. 2
[54] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie
Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient vision
transformers with dynamic token sparsification. In NeurIPS ,
pages 13937–13949, 2021. 3
[55] Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and
Lu Hou. TESTA: Temporal-spatial token aggregation for
long-form video-language understanding. arXiv preprint
arXiv:2310.19060 , 2023. 3
10

--- PAGE 11 ---
[56] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu
Hou. TimeChat: A time-sensitive multimodal large lan-
guage model for long video understanding. arXiv preprint
arXiv:2312.02051 , 2023. 2
[57] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman Castagn ´e,
Alexandra Sasha Luccioni, Franc ¸ois Yvon, Matthias Gall ´e,
et al. BLOOM: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 , 2022. 2
[58] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual Captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In ACL,
pages 2556–2565, 2018. 5, 14
[59] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. HuggingGPT: Solving
ai tasks with chatgpt and its friends in huggingface. arXiv
preprint arXiv:2303.17580 , 2023. 3
[60] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. Eva-clip: Improved training techniques for clip at scale.
arXiv preprint arXiv:2303.15389 , 2023. 15
[61] D ´ıdac Sur ´ıs, Sachit Menon, and Carl V ondrick. ViperGPT:
Visual inference via python execution for reasoning. arXiv
preprint arXiv:2303.08128 , 2023. 3
[62] Vicuna Team. Vicuna: An open chatbot impressing gpt-4
with 90% chatgpt quality. 2023. 5, 15
[63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
LLaMA: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 1
[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
LLaMA 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023. 1, 15
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 2
[66] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Lu-
owei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang
Jiang, and Lu Yuan. OmniVL: One foundation model for
image-language and video-language tasks. In NeurIPS ,
pages 5696–5710, 2022. 2
[67] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,
Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking,
drawing and editing with visual foundation models. arXiv
preprint arXiv:2303.04671 , 2023. 3
[68] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-
Seng Chua. NExT-GPT: Any-to-any multimodal llm. arXiv
preprint arXiv:2309.05519 , 2023. 2, 3, 13
[69] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video question answer-
ing via gradually refined attention over appearance and mo-
tion. In ACM MM , pages 1645–1653, 2017. 6
[70] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT:
Semantic segmentation emerges from text supervision. In
CVPR , pages 18134–18144, 2022. 3, 13[71] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. In NeurIPS , pages
124–141, 2022. 6
[72] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,
Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,
Michael Zeng, and Lijuan Wang. MM-REACT: Prompting
chatgpt for multimodal reasoning and action. arXiv preprint
arXiv:2303.11381 , 2023. 3
[73] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mPLUG-Owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 1, 6
[74] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting
Zhuang, and Dacheng Tao. ActivityNet-QA: A dataset for
understanding complex web videos via question answering.
InAAAI , pages 9127–9134, 2019. 6
[75] Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo,
Wanli Ouyang, and Xiaogang Wang. Not all tokens are
equal: Human-centric visual analysis via token clustering
transformer. In CVPR , pages 11101–11111, 2022. 3, 13
[76] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An
instruction-tuned audio-visual language model for video un-
derstanding. arXiv preprint arXiv:2306.02858 , 2023. 3, 5,
6
[77] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.
LLaMA-Adapter: Efficient fine-tuning of language models
with zero-init attention. arXiv preprint arXiv:2303.16199 ,
2023. 5, 6
[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,
Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained trans-
former language models. arXiv preprint arXiv:2205.01068 ,
2022. 2
[79] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,
Nedim Lipka, Diyi Yang, and Tong Sun. LLaV AR: Enhanced
visual instruction tuning for text-rich image understanding.
arXiv preprint arXiv:2306.17107 , 2023. 6, 18
[80] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
Singh. Calibrate before use: Improving few-shot perfor-
mance of language models. In ICML , pages 12697–12706,
2021. 14
[81] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. MiniGPT-
5: Interleaved vision-and-language generation via generative
vokens. arXiv preprint arXiv:2310.02239 , 2023. 3
[82] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa
Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei
Li, et al. LanguageBind: Extending video-language pretrain-
ing to n-modality by language-based semantic alignment.
arXiv preprint arXiv:2310.01852 , 2023. 2
[83] Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang,
Qi Song, Mingjun Pan, and Li Yuan. LLMBind: A uni-
fied modality-task integration framework. arXiv preprint
arXiv:2402.14891 , 2024. 3
[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. MiniGPT-4: Enhancing vision-language
11

--- PAGE 12 ---
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 1, 6
12

--- PAGE 13 ---
Abstract This appendix provides additional discus-
sions (Appendix A), implementation details (Appendix B),
several additional experiments (Appendix C), additional vi-
sualization results (Appendix D), more qualitative anal-
ysis (Appendix E), and details of quantitative evalua-
tions (Appendix F).
A. Additional Discussions
A.1. Comparison of Chat-UniVi and Other Multi-
modal Methods
Existing methods often focus exclusively on either image
or video inputs. Recently, there have also been some meth-
ods [1, 9, 68] that support both images and videos, and they
can be broadly divided into two classes.
•Q-former based methods. The first class of methods
uses a query transformer to extract a fixed number of to-
kens for each image and video. These methods are ex-
emplified by Flamingo [1], OpenFlamingo [3], and Ot-
ter [30]. However, videos vary in length, posing a chal-
lenge for these methods, as they extract a fixed number
of visual tokens from each video, limiting their ability
to effectively capture temporal comprehension. Human
evaluation results (see Fig. 4) also substantiate that these
methods struggle to strike a balance between image and
video comprehension.
•Multi-encoder methods. The second category of meth-
ods employs separate pre-trained image and video en-
coders to process images and videos independently.
Prominent examples of this approach include X-LLM [9]
and NExT-GPT [68]. However, these methods intro-
duce redundancy within the model and present difficul-
ties when trained jointly. Most importantly, this approach
does not leverage the advantages of joint training with
both image and video data. Consequently, they do not
align with our primary objective of developing a unified
vision-language model.
In contrast to the previous works, Chat-UniVi uniformly
represents images and videos using multi-scale dynamic vi-
sual tokens. The proposed Chat-UniVi has two compelling
advantages:
•Variable length video features. In Chat-UniVi, the num-
ber of temporal visual clusters is determined proportion-
ally based on the number of input video frames. In con-
trast to the Q-former based methods, Chat-UniVi allo-
cates a greater number of visual tokens to longer videos.
Therefore, our method is better suited for variable-length
video understanding.
•Unified visual encoder. Chat-UniVi employs a shared
visual encoder to consistently process both images and
videos. In contrast to multi-encoder methods, our method
eliminates the need for introducing redundant parameters
and streamlines the training process.•Benefit from joint training. Due to the unified represen-
tation framework for both images and videos, Chat-UniVi
can be trained on mixed datasets that include both images
and videos. This allows for direct application to tasks in-
volving both images and videos. Most importantly, we
find that this joint training strategy can simultaneously
enhance the model’s understanding of both images and
videos. Experimental results are shown in Tab. 6.
In Tab. A, we show the comparison of Chat-UniVi and
other methods. For Q-former based methods, the advan-
tages of joint training are not shown, and even the perfor-
mance of the model may affect each other when multiple
datasets are mixed [1]. However, the potential to benefit
from joint training cannot be ruled out. In addition, the
multi-encoder method can also select a video encoder that
can encode dynamic length features.
A.2. Comparison of Chat-UniVi and Other Cluster-
ing Transformer Methods
There have also been recent methods [25, 44, 70, 75] to
explore the role of token clustering within the transformer
framework. However, none of these methods can be di-
rectly extended to video, and additional parameters need to
be trained. We summarize the advantages of our method as
follows:
•Supporting video input. In contrast to other methods,
Chat-UniVi extends the tokens clustering method to in-
corporate video inputs, achieving the integration of im-
age and video representations for the first time. Our work
is the first to demonstrate that this unified representation
can reconcile the intricate spatial details of images with
the broader temporal understanding required for videos.
•Without parameters. Our clustering method is
parameter-free and therefore requires no training. In-
terestingly, we find that this parameter-free clustering
method serves as the linchpin to the success of our model.
As shown in Tab. B, the performance of the clustering
method with training parameters is significantly inferior
to the parameter-free clustering method we propose. We
attribute this phenomenon to the gradient instability in
multimodal conversation training, which hinders the con-
vergence of parameterized methods.
A.3. Runtime and Memory Complexity
As shown in Tab. C, the time and memory costs of our
clustering algorithm are negligible compared to those of the
large language model.
A.4. Limitations and Future Work
In this section, we delineate the limitations of our work and
outline avenues for future research.
The Enduring Impact of Large Language Models. Our
method leverages the strength of pre-trained Large Lan-
13

--- PAGE 14 ---
Type MethodsVariable Unified Benefit from
Length Features Visual Encoder Joint Training
Q-former based methodsFlamingo✘ ✔ –OpenFlamingo, Otter
Multi-encoder methods X-LLM, NExT-GPT – ✘ ✘
Unified methods Chat-UniVi ✔ ✔ ✔
Table A. Comparison with other methods. “✘” denotes that the model does not have this property. “ ✔” denotes that the model has this
property. “ –” indicates a temporary lack of experimental evidence.
Methods Parameter-free Video InputImage Understanding
Conversation Detail Reason All
Ma et al. [44] ✘ ✘ 71.8 60.9 91.6 75.0
Chat-UniVi ✔ ✔ 84.1 74.2 93.7 84.2
Table B. Comparison of Chat-UniVi and another token clustering method. “✘” denotes that the model does not have this property.
“✔” denotes that the model has this property.
guage Models, and as a consequence, also inherits their vul-
nerabilities.
•Hallucination. While our experiments (see Tab. 5)
demonstrate the effectiveness of our method in address-
ing hallucinations, it is important to acknowledge that
the issue of hallucinations in LLMs remains a challenge
yet to be fully resolved. The phenomenon of illusory re-
sponses in LLMs can result in unsupported conjectures
during open multimodal conversations, and addressing
this issue has the potential to significantly expedite ad-
vancements in the field. For a more in-depth exploration
of common weaknesses observed in large LLMs, please
refer to Brown et al. [7], Rae et al. [52].
•Long sequence processing. Transformer-based language
models often exhibit suboptimal generalization when
confronted with test sequences considerably longer than
their training data [49]. This becomes particularly evident
in multi-turn conversations, where the model may exhibit
forgetfulness of prior conversational context, resulting in
erroneous responses. Simultaneously, we find a decline
in model performance when multiple videos are inputted,
which could also be attributed to constraints associated
with sequence length.
•Prompt sensitivity. In-context learning has demonstrated
disconcerting sensitivity to various aspects of demonstra-
tions, including prompt formats [80]. Notably, different
prompt formats can yield entirely contradictory output re-
sults. Finding a solution to this issue holds the potential
to greatly accelerate progress in the field.
Natural Language Output. Natural language serves as
a robust and adaptable input/output interface for describ-
ing visual tasks to the model, facilitating the generation ofoutputs, or estimating conditional probabilities for potential
outcomes. However, it may prove to be a less convenient
interface for tasks that require conditioning on or predicting
more structured outputs, such as bounding boxes, as well as
for generating dense pixel predictions. Besides, the flexibil-
ity of the natural language output also makes it difficult to
evaluate the performance of the model.
More Modalities. Future work can explore alternative
modalities, such as audio, in addition to visual inputs. The
incorporation of multiple modalities holds the promise of
broadening the spectrum of tasks that the model can ad-
dress, and it has the potential to enhance their performance
by leveraging synergies among these various modalities.
For example, contemplating audio information alongside
video processing can significantly augment the video un-
derstanding of the model.
B. Implementation Details
Data Details. For the multimodal pre-training stage,
we utilize the image-caption pairs from various datasets,
including COCO [12] and CC3M-595K screened from
CC3M [58] by LLaV A [40]. All input images are re-
sized to 224×224. For the joint instruction tuning stage,
we incorporate multimodal instruction data from multiple
sources: (i) multimodal in-context instruction datasets, such
as MIMIC-IT [2, 22, 30], (ii) visual instruction datasets,
such as LLaV A, (iii) video instruction data from Video-
ChatGPT [45]. In order to further filter the training data,
we delete the duplicate data in LLaV A-instruct-150K and
MIMIC-IT, and delete the video data in MIMIC-IT. This
dataset is a composite of multi-turn conversations and
single-turn conversations presented in a conversational for-
14

--- PAGE 15 ---
MethodsTime Complexity Image Inference Video Inference
Spatial Temporal Merging (s) All (s) Memory (M) Merging (s) All (s) Memory (M)
LLaV A - - 0 2.3116 15673 ✘ ✘ ✘
Ours O(L2D)O(M2D) 0.0027 2.2722 15443 0.0174 4.4040 16533
Table C. Runtime and memory complexity analysis. L,D, andMdenote the number of vanilla visual tokens, the feature dimension, the
frame length, respectively. “ ✘” denotes that the method does not have this property.
Datasets Image Inputs Video InputsMulti-turn Number of
Conversations Conversations
Multimodal Pre-training Stage
CC3M-595K ✔ ✘ ✘ 595K
COCO ✔ ✘ ✘ 956K
Joint Instruction Tuning Stage
LLaV A-instruct-150K ✔ ✘ ✔ 150K
MIMIC-IT-399K‡✔ ✘ ✘ 399K
Video-ChatGPT-instruct ✘ ✔ ✘ 100K
Table D. Description of training data. “✘” denotes that the dataset does not have this property. “ ✔” denotes that the dataset has this
property. “‡” represents the dataset filtered from MIMIC-IT, containing exclusively image data. In order to further filter the training data,
we also delete the duplicate data in LLaV A-instruct-150K and MIMIC-IT.
mat, alongside single images, multiple images, and videos
as visual input. For each video, we select 64 frames as in-
put for the model. All input images or frames are resized to
224×224. We provide a detailed description of the training
data in Tab. D.
Model Settings. Following previous works [40], we
adopt the vision encoder of CLIP (ViT-L/14) [51] as the
visual foundation model. We chose an instruction-tuned
variant of LLaMA2 [64], i.e., Vicuna [62], as our language
foundation model. Specifically, we utilize the Vicuna-v1.5
model, comprised of 7B parameters.
Training Hyperparameters. For the multimodal pre-
training stage, we pre-train Chat-UniVi for one epoch with
a batch size of 128, employing the AdamW optimizer with
a cosine schedule. The learning rate is set to 2e-3, and the
warm-up rate is 0.03. For the joint instruction tuning stage,
we train Chat-UniVi for 2 epochs with a batch size of 128,
and the learning rate is set to 2e-5, employing the AdamW
optimizer with a cosine schedule. The warm-up rate is set
to 0.03.
ScienceQA Fine-tuning Settings. We start with a pre-
trained model to fine-tune. We fine-tune the model for 9
epochs with a batch size of 32, employing the AdamW op-
timizer with a cosine schedule. The learning rate is set to
2e-5, and the warm-up rate is 0.03.
C. Additional Experiments
Comparison between the LoRA and Full Fine-tuning.
When the number of model parameters is too large, fullfine-tuning of retraining all model parameters becomes ex-
pensive, so many recent methods freeze most of the model
parameters and train the model with LoRA [21]. We pro-
vide the results of the comparison between the LoRA and
full fine-tuning in Tab. E. We find that LoRA can achieve
competitive performance with full fine-tuning while saving
more than half the GPU memory required for training. Fu-
ture work can use LoRA to extend our method on larger
LLMs and vision encoders to achieve better performance.
Analysis of the Vision Encoder. EV A-CLIP [60] is a re-
cently developed multimodal model with performance com-
parable to Openai-CLIP [51]. We provide the results of
the comparison between EV A-CLIP and Openai-CLIP in
Tab. F. We find that the performance of EV A-CLIP is com-
parable to that of Openai-CLIP when the number of param-
eters is equal. However, EV A-CLIP offers a larger version
of the model with a parameter count of 1.8B, so we think
it might be better to adopt a larger EV A-CLIP than Openai-
CLIP when using larger LLMs.
Effect of the Multi-scale Representation. To investigate
the impact of the multi-scale representation of our method,
we provide the ablation results in Tab. G. Multi-scale rep-
resentation improves both image understanding and video
understanding of the model. These results provide evidence
for the benefits of employing a multi-scale representation in
multimodal large language models.
Effect of the Multi-scale Representation on Object Hal-
lucination. As shown in Tab. 5, Chat-UniVi, as a 7B
model, even outperforms the 13B model, e.g., MiniGPT-
15

--- PAGE 16 ---
MethodsImage Understanding Video Understanding
Conversation Detail Reason All Correct Detail Context Temporal Consistency .
LoRA 76.1 68.6 82.4 75.8 52.8 55.0 63.8 42.6 53.8
Full fine-tuning 84.1 74.2 93.7 84.2 57.8 58.2 69.2 47.9 56.2
Table E. Comparison between the LoRA and full fine-tuning. “Detail” denotes the “Detail Description” in the context of image un-
derstanding or “Detail Orientation” in the context of video understanding. For image understanding, “Reason” denotes the “Complex
Reasoning”. For video understanding, “Correct”, “Context”, and “Temporal” stand for “Correctness of Information”, “Contextual Under-
standing”, and “Temporal Understanding”, respectively.
MethodsImage Understanding Video Understanding
Conversation Detail Reason All Correct Detail Context Temporal Consistency
EV A-CLIP 80.0 74.7 91.2 82.1 57.2 58.8 67.8 45.7 54.6
Openai-CLIP 84.1 74.2 93.7 84.2 57.8 58.2 69.2 47.9 56.2
Table F. Comparison between the EV A CLIP and the Openai CLIP. We choose EV A-CLIP (ViT-G), which has a similar number of
parameters as Openai-CLIP (ViT-L/14), for the experiment.
MethodsImage Understanding Video Understanding
Conversation Detail Reason All Correct Detail Context Temporal Consistency
Single-scale 70.5 63.4 88.3 74.2 54.6 56.4 65.8 42.1 52.2
Multi-scale 84.1 74.2 93.7 84.2 57.8 58.2 69.2 47.9 56.2
Table G. Ablation study about the multi-scale representation. These results provide evidence for the benefits of employing a multi-scale
representation in multimodal large language models.
4, in the object hallucination evaluation. We attribute this
success to the multi-scale representation that equips our
method to perceive both high-level semantic concepts and
low-level visual appearance. In Tab. H, we show the results
of ablation experiments on object hallucination evaluation
for the multi-scale representation. We find that multi-scale
representation improves the ability to resist hallucinations.
Therefore, multi-scale representation is beneficial for mul-
timodal LLMs.
Ablation of Training Data. We provide comparisons
of our method with LLaV A under different conditions
in Tab. I. Our method achieves better performance than
LLaV A, which we explain in the following two aspects.
Multi-scale Representation. In contrast to LLaV A, which
focuses on low-level visual features, our method perceives
both high-level semantic concepts and low-level visual de-
tails by multi-scale representation. Therefore, our method
outperforms LLaV A in conversation, reasoning, and hallu-
cinations. Scalability. Our framework supports video in-
put, and by fine-tuning with high-quality video instruction
data, the visual capabilities of our models have been signif-
icantly enhanced, especially in terms of detailed captioning
and reasoning.Besides, we draw the following two conclusions: (1) In-
struction tuning data has a greater impact on performance
than pre-training data. (2) High-quality instruction tuning
data can significantly enhance model performance. Espe-
cially after training on high-quality video data, the perfor-
mance of the model is greatly improved.
Detailed Results on Object Hallucination Evaluation.
In Tab. J, we report the detailed results of the polling-based
object probing evaluation [34]. As shown in Tab. J, Chat-
UniVi outperforms the recently proposed state-of-the-art
methods. Notably, as a 7B model, our method even outper-
forms the 13B model, e.g., MiniGPT-4, in the object hal-
lucination evaluation. These results demonstrate the effec-
tiveness of our method.
D. Additional Visualization Results
Visualization of the dynamic visual tokens for the im-
age inputs. To gain a deeper insight into the function-
ality of our proposed dynamic visual tokens, we present
the additional visualization results for the image inputs in
Fig. A. In Fig. A, we provide a diverse range of visual-
izations encompassing various image categories, including
portraits, sports, wildlife, art, architecture, and food. It
16

--- PAGE 17 ---
POPE Methods LLM Size Accuracy Precision Recall F1-Score Yes
RandomSingle-scale 7B 73.88 67.03 97.06 79.30 74.63
Multi-scale 7B 85.19 83.59 88.66 86.05 54.67
PopularSingle-scale 7B 56.36 53.50 97.20 69.01 90.83
Multi-scale 7B 69.50 64.10 88.60 74.39 69.10
AdversarialSingle-scale 7B 55.63 53.07 97.26 68.67 91.63
Multi-scale 7B 64.97 60.23 88.06 71.54 73.10
Table H. Effect of the multi-scale representation on object hallucination. “Yes” represents the proportion of positive answers that the
model outputs.
MethodsMultimodal Pre-training Instruction Tuning Image Understanding
POPE-RVideo
Datasets Datasets Conv Detail Reason All Inputs
LLaV ACC3M-595K LLaV A-instruct-150K82.3 70.2 87.9 80.4 66.83 ✘
Chat-UniVi 82.9 68.8 89.8 80.7 82.26 ✘
LLaV A CC3M-595K,LLaV A-instruct-150K82.7 68.8 88.8 80.8 72.02 ✘
Chat-UniVi COCO 83.3 72.6 89.0 81.5 82.33 ✘
LLaV A CC3M-595K, LLaV A-instruct-150K, 78.8 70.2 91.8 80.4 74.53 ✘
Chat-UniVi COCO MIMIC-IT-399K 84.0 69.3 89.3 81.5 83.53 ✘
Chat-UniVi CC3M-595K, LLaV A-instruct-150K, MIMIC-IT-399K,84.1 74.2 93.7 84.2 85.19 ✔w/ video data COCO Video-ChatGPT-instruct
Table I. Ablation of structure and training data. “✘” denotes that the method does not have this property. “ ✔” denotes that the method
has this property.
is crucial to underscore that our proposed token merging
method operates without the need for object outline labels
and is parameter-free. As shown in Fig. A, the proposed dy-
namic visual tokens effectively generalize objects and back-
grounds, empowering Chat-UniVi to capture the spatial nu-
ances of images using a limited number of visual tokens.
Visualization of the dynamic visual tokens for the video
inputs. To gain a more comprehensive understanding of
our proposed dynamic visual tokens, we also present addi-
tional visualization results for the video inputs in Fig. B. In
the case of videos, the video is initially divided into sev-
eral events, and subsequently, these visual tokens expand
over frames within each event to encapsulate frame-level
dynamics. Notably, our method imposes no restrictions on
the number of frames per event, showcasing the remarkable
flexibility and generalization ability of our methodology. As
shown in Fig. B, the proposed dynamic visual tokens sig-
nificantly reduce the number of visual tokens while main-
taining the expressive capabilities of the model. This em-
powerment equips Chat-UniVi with the capacity to capture
the broader temporal understanding required for videos, all
within the confines of a limited number of visual tokens.
E. Additional Qualitative Analysis
The conversation includes both the image and the video.
In Fig. C and Fig. D, we present examples of conversationsthat encompass both the image and the video. As shown
in Fig. C and Fig. D, Chat-UniVi offers detailed and con-
textually appropriate responses aligned with user prompts.
These illustrative examples showcase the remarkable ability
of Chat-UniVi to comprehend both image and video con-
texts across multiple conversational turns.
The conversation includes multiple videos. Fig. E il-
lustrates a conversation example including multiple videos.
As shown in Fig. E, Chat-UniVi can use the information of
multiple videos in the context, and provide appropriate and
coherent responses based on user prompts. The illustrative
example showcases the remarkable ability of Chat-UniVi to
comprehend multiple video contexts across multiple con-
versational turns.
The conversation includes multiple images. Fig. F pro-
vides an illustrative conversation example including multi-
ple images. As shown in Fig. F, Chat-UniVi adeptly lever-
ages information from multiple images within the context,
enabling it to make choices among various images. This
illustrative example highlights the impressive capacity of
Chat-UniVi to grasp multiple image contexts seamlessly
throughout various conversational exchanges.
The conversation includes the image. Fig. G features
an example of a conversation that incorporates an image.
As shown in Fig. G, Chat-UniVi excels at providing de-
tailed descriptions and can even craft compelling narratives
17

--- PAGE 18 ---
POPE Methods LLM Size Accuracy Precision Recall F1-Score Yes
RandomLLaV A 13B 64.12 59.38 95.99 73.38 83.26
MiniGPT-4 13B 79.67 78.24 82.20 80.17 52.53
InstructBLIP 13B 88.57 84.09 95.13 89.27 56.57
MultiModal-GPT 7B 50.10 50.05 100.00 66.71 99.90
mPLUG-Owl 7B 53.97 52.07 99.60 68.39 95.63
LLaV A†7B 72.16 78.22 76.29 78.22 76.29
Chat-UniVi 7B 85.19 83.59 88.66 86.05 54.67
PopularLLaV A 13B 63.90 58.46 95.86 72.63 81.93
MiniGPT-4 13B 69.73 65.86 81.93 73.02 62.20
InstructBLIP 13B 82.77 76.27 95.13 84.66 62.37
MultiModal-GPT 7B 50.00 50.00 100.00 66.67 100.00
mPLUG-Owl 7B 50.90 50.46 99.40 66.94 98.57
LLaV A†7B 61.37 56.63 97.00 71.52 85.63
Chat-UniVi 7B 69.50 64.10 88.60 74.39 69.10
AdversarialLLaV A 13B 58.91 55.11 95.72 69.95 86.76
MiniGPT-4 13B 65.17 61.19 82.93 70.42 67.77
InstructBLIP 13B 72.10 65.13 95.13 77.32 73.03
MultiModal-GPT 7B 50.00 50.00 100.00 66.67 100.00
mPLUG-Owl 7B 50.67 50.34 99.33 66.82 98.67
LLaV A†7B 58.67 54.90 97.00 70.12 88.33
Chat-UniVi 7B 64.97 60.23 88.06 71.54 73.10
Table J. Detailed results on object hallucination evaluation. “†” denotes our own re-implementation of LLaV A under our training
settings (excluding video data) for a fair comparison.
inspired by the image. The illustrative example showcases
the remarkable ability of Chat-UniVi in the realms of rea-
soning and creative expression.
The conversation includes the video. In Fig. H and
Fig. I, we offer examples of conversations that incorporate
the video. As shown in Fig. H and Fig. I, Chat-UniVi ex-
hibits a remarkable proficiency in comprehending videos
and is adept at offering valuable insights inspired by the
video content. These illustrative examples showcase the re-
markable ability of Chat-UniVi to grasp video contexts and
engage in reasoned responses.
F. Details of Quantitative Evaluations
GPT-based Evaluation For Image Understanding. Our
quantitative evaluation protocol follows that of Liu et al.
[40]. Following Liu et al. [40], Zhang et al. [79], we em-
ploy 90 questions based on 30 COCO validation images,
covering various aspects, including conversation, detail de-
scription (Detail), and complex reasoning (Reason). These
images are randomly selected by Liu et al. [40]. We uti-
lize the GPT-4 model to generate reference responses based
on the question, and the ground-truth bounding boxes and
captions. During the model evaluation process, the model
predicts answers based on both the question and input im-
age. After obtaining the response from the model, we feedthe question, visual information (in the format of captions
and bounding boxes), the generated response, and the refer-
ence response to GPT-4. GPT-4 evaluates the helpfulness,
relevance, accuracy, and level of detail of the responses, as-
signing an overall score on a scale of 1 to 10, where a higher
score indicates better overall performance. Besides, we also
ask GPT-4 to provide a comprehensive explanation of the
evaluation to enhance our understanding of the models.
GPT-based Evaluation For Video Understanding. The
quantitative evaluation protocol for video understanding
follows the methodology introduced by Maaz et al. [45].
Specifically, Maaz et al. [45] curates a test set based on
the ActivityNet-200 dataset [8], which includes videos with
rich, dense descriptive captions and associated question-
answer pairs from human annotations. During the model
evaluation process, we employ the GPT-3.5 model to assign
a relative score to the generated predictions on a scale of
1-5, across five critical aspects: (1) Correctness of informa-
tion (Correct). (2) Detail orientation (Detail). (3) Contex-
tual understanding (Context). (4) Temporal understanding
(Temporal). (5) Consistency. It is worth noting that the re-
sults reported in Maaz et al. [45] span a range from 0 to
5. To standardize the metrics, we normalize all scores to a
scale of 0 to 100.
Zero-shot Video Question Evaluation. Our evaluation
18

--- PAGE 19 ---
Input image Step one Step two Step three Input image Step one Step two Step three Step three Step two Step one Input image 
Figure A. Visualization of the dynamic visual tokens for the image inputs. We provide a diverse range of visualizations encompassing
various image categories, including portraits, sports, wildlife, art, architecture, and food. It is important to emphasize that our proposed
token merging method is parameter-free and operates without the need for object outline labels.
protocol follows that of Maaz et al. [45], utilizing GPT-
assisted evaluation to assess the capabilities of models. Dur-
ing the model evaluation process, we feed the question,the ground-truth answer, and the generated response to the
GPT-3.5 model. GPT-3.5 evaluates whether the generated
responses are correct and assigns a matching score on a
19

--- PAGE 20 ---
Input video Step one Step two Step three
Event 1Event 2 Event 3Input video Step one Step two Step threeEvent 1 Event 2 Event 3Input video Step one Step two Step three
Event 1Event 2 Event 3
Figure B. Visualization of the dynamic visual tokens for the video inputs. It is important to emphasize that our proposed token merging
method is parameter-free and operates without the need for object outline labels. Our method imposes no restrictions on the number of
frames per event, showcasing the remarkable flexibility and generalization ability of our methodology.
scale of 0 to 5, where a higher score indicates better overall
performance.
Zero-shot Object Hallucination Evaluation. To quanti-tatively evaluate the hallucination problem of the model, we
adopt the polling-based object probing evaluation (POPE)
process proposed by Li et al. [34]. Specifically, POPE for-
20

--- PAGE 21 ---
mulates the evaluation of object hallucination as a binary
classification task, where the model is prompted to respond
with either “Yes” or “No” to queries like “Is there a chair
in the image?”. Li et al. [34] randomly selects 500 images
from the COCO validation set. Each image contains more
than three ground-truth objects in the annotations, and six
questions are generated for each image. The annotations
of objects in images directly construct the questions with
the answer “Yes”. For the questions with the answer “No”,
three different strategies are employed for sampling their
probing objects as follows:
•Random Sampling. Randomly sampling objects that do
not exist in the image.
•Popular Sampling. Selecting the top-3 most frequently
occurring objects in the COCO dataset that are absent
from the image.
•Adversarial Sampling. Initially, Li et al. [34] rank all
objects based on their co-occurring frequencies with the
ground-truth objects, and subsequently select the top-3
most frequent objects from this list that are not present
in the image.
21

--- PAGE 22 ---
Figure C. A conversation with both image and video. The blue box shows the user input. The gray box shows the model output.
22

--- PAGE 23 ---
Figure D. A conversation with both image and video. The blue box shows the user input. The gray box shows the model output.
23

--- PAGE 24 ---
Figure E. A conversation includes multiple videos. The blue box shows the user input. The gray box shows the model output.
24

--- PAGE 25 ---
Figure F. A conversation includes multiple images. The blue box shows the user input. The gray box shows the model output.
25

--- PAGE 26 ---
Figure G. A conversation includes the image. The blue box shows the user input. The gray box shows the model output.
26

--- PAGE 27 ---
Figure H. A conversation includes the video. The blue box shows the user input. The gray box shows the model output.
27

--- PAGE 28 ---
Figure I. A conversation includes the video. The blue box shows the user input. The gray box shows the model output.
28
