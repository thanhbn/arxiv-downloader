# 2310.15308.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.15308.pdf
# File size: 9819707 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SAM-CLIP : Merging Vision Foundation Models towards
Semantic and Spatial Understanding
Haoxiang Wang2‚Ä†, Pavan Kumar Anasosalu Vasu1, Fartash Faghri1, Raviteja Vemulapalli1
Mehrdad Farajtabar1, Sachin Mehta1, Mohammad Rastegari1, Oncel Tuzel1, Hadi Pouransari1‚Ä†
1Apple2University of Illinois Urbana-Champaign
Abstract
The landscape of publicly available vision foundation
models (VFMs), such as CLIP and Segment Anything Model
(SAM), is expanding rapidly. VFMs are endowed with dis-
tinct capabilities stemming from their pre-training objec-
tives. For instance, CLIP excels in semantic understand-
ing, while SAM specializes in spatial understanding for seg-
mentation. In this work, we introduce a simple recipe to
efficiently merge VFMs into a unified model that absorbs
their expertise. Our method integrates techniques of multi-
task learning, continual learning, and distillation. Fur-
ther, it demands significantly less computational cost com-
pared to traditional multi-task training from scratch, and it
only needs a small fraction of the pre-training datasets that
were initially used to train individual models. By apply-
ing our method to SAM and CLIP , we obtain SAM-CLIP :
a unified model that combines the capabilities of SAM and
CLIP into a single vision transformer. Compared with de-
ploying SAM and CLIP independently, our merged model,
SAM-CLIP , reduces storage and compute costs for infer-
ence, making it well-suited for edge device applications.
We show that SAM-CLIP not only retains the foundational
strengths of SAM and CLIP , but also introduces synergistic
functionalities, notably in zero-shot semantic segmentation,
where SAM-CLIP establishes new state-of-the-art results
on 5 benchmarks. It outperforms previous models that are
specifically designed for this task by a large margin, includ-
ing +6.8% and +5.9% mean IoU improvement on Pascal-
VOC and COCO-Stuff datasets, respectively.
1. Introduction
Vision Foundation Models (VFM) such as CLIP [68],
SAM [38], MAE [26], and DINOv2 [62] provide strong
backbones that work well for a wide range of vision tasks
when finetuned on domain-specific data. Additionally,
some of these models exhibit notable prompt-based
open-form (also known as zero-shot) capabilities, such
‚Ä†Work completed during internship of H. Wang at Apple. Correspon-
dence to: hwang264@illinois.edu, mpouransari@apple.comas classification from text prompts [68] and segmentation
from geometric prompts (e.g., points, bounding boxes, and
masks) [38]. Depending on their pre-training objectives,
VFMs can act as feature extractors suitable for diverse
downstream tasks. For instance, models that employ
contrastive losses during training [11, 62, 68], utilize low-
frequency signals, and generate features that can linearly
separate samples based on their semantic content [65]. Con-
versely, the pre-training objectives for MAE and SAM in-
volve denoising masked images and instance mask segmen-
tation, respectively. These objectives lead to the acquisition
of features utilizing high-frequency signals with localiza-
tion knowledge but limited semantic understanding (Fig. 4).
Maintaining and deploying separate vision models for
different downstream tasks is inefficient (high memory foot-
print and runtime, especially on edge devices) and lacks op-
portunity for cross-model learning [76]. Multitask learn-
ing[97] is a paradigm capable of addressing this issue.
However, it often requires costly training and simultane-
ous access to all tasks [20]. Training foundation models of-
ten relies on an unsupervised or semi-supervised approach,
requiring substantial computational resources. For exam-
ple, state-of-the-art CLIP models are trained on extensive
datasets, such as LAION [77] and DataComp [22], con-
suming a massive amount of computational power. Simi-
larly, SAM‚Äôs pre-training on 1.1 billion masks is computa-
tionally demanding. A multi-objective pre-training method
requires comparable or more data and compute power as
single objective VFM training. Additionally, there are
still challenges to be addressed, such as how to best mix
datasets, how to handle interfering gradients and instabil-
ities in multi-task training [15], and how to access VFM
pre-training datasets that are often proprietary [68], which
limit the scalability and feasibility of this approach.
To overcome these challenges, model merging has
emerged as a rapidly growing area of research [83, 90].
The majority of merging techniques focus on combining
multiple task-specific models into a single model without
requiring additional training. For instance, this can be
achieved through techniques such as model weights in-
1arXiv:2310.15308v4  [cs.CV]  10 Jun 2024

--- PAGE 2 ---
Figure 1. SAM-CLIP inherits most zero-shot capabilities of SAM (instance segmentation) and CLIP (classification) using a single shared
backbone ( left). Further, SAM-CLIP is capable of a new task, zero-shot semantic segmentation, and obtains state-of-the-art results on
several benchmarks, with a large margin compared to previous models specifically designed for this task ( right ). Detailed results are
provided in Tables 1 and 2.
terpolation [31], parameter importance analysis [54], or
leveraging invariances in the models [1]. These techniques,
on the other side, put too much stress on not using data or
not performing additional training/finetuning resulting in
decreased performance or lack of generalization to diverse
sets of tasks [83]. In this work, our goal is to merge VFMs
that are trained with fundamentally different objectives,
have distinct capabilities, and possibly interact with other
modalities. In this setup, naive merging approaches such as
weight interpolation result in significant forgetting [56], as
shown in Appendix D.
We aim to fill the gap between training-free model merg-
ing and multitask training by drawing techniques from con-
tinual learning [46, 64] and knowledge distillation [27]. We
treat model merging as a continual learning problem, where,
given a pretrained VFM, the knowledge of a second VFM is
merged without forgetting of the initial knowledge. On one
side, in contrast to weight averaging techniques, we allow
access to a small part of pretraining data or its surrogates to
be replayed during the merging process. We leverage multi-
task distillation on the replay data to avoid forgetting the
original knowledge of pretrained VFMs during the merging
process. On the other side, our merging process is signif-
icantly more efficient than traditional multitask training by
requiring less than 10% of the data and computational cost
compared to their original pretraining (Section 3).
We instantiate our proposed merging approach by com-
bining SAM and CLIP into a single multi-task model, called
SAM-CLIP , suitable for edge device deployment. Thismerged model inherits prompt-based zero-shot capabilities
from both CLIP and SAM with minimal forgetting: specif-
ically, zero-shot classification and image-text retrieval from
CLIP, and zero-shot instance segmentation from SAM (see
Figure 1 left). Further, we illustrate that SAM-CLIP learns
richer visual representations compared to SAM and CLIP,
endowed with both spatial and semantic features, result-
ing in improved head-probing performance on new tasks
(see Figure 4). Finally, SAM-CLIP shows an emerging
capability of zero-shot transfer to a new task: zero-shot
semantic segmentation thanks to combined skills inherited
from SAM and CLIP. This task involves generating a seg-
mentation mask based on a free-form text prompt. It re-
quires both semantic understanding from text and segmen-
tation capabilities, which are skills that SAM-CLIP learns
from CLIP and SAM, respectively. We demonstrate that
SAM-CLIP achieves state-of-the-art performance on zero-
shot semantic segmentation in a single-stage inference setup
over multiple datasets (Figure 1 right). With a compromise
of a negligible drop compared to the performance of indi-
vidual models on the original tasks (zero-shot classification
and instance segmentation), we get a single model that not
only masters both tasks, but also is capable of accomplish-
ing a new task.
2. Background
Vision-Language Models (VLMs) such as CLIP and
ALIGN [32] are trained on Billion-scale, often noisy,
image-text datasets. These models consist of modality-
2

--- PAGE 3 ---
specific (image and text) encoders that produce an embed-
ding for each modality. For a randomly sampled batch
of image-text pairs, these models are trained with a con-
trastive objective to maximize alignment between embed-
dings of positive pairs of image and text. A direct ap-
plication of such models is zero-shot image-text retrieval,
or zero-shot classification via text prompts [68]. Other
works such as ViLT [36], VLMo [4], and BLIP [42] ex-
plored shared or mixed architectures between image and
text modalities and enabled additional zero-shot capabilities
such as Visual Question Answering (VQA) and captioning.
Approaches such as LiT [95], APE [75], and BLIP-2 [43]
reduce the training cost of CLIP-like models by deploying
pre-trained single-modal models. This is similar to our ap-
proach in terms of harvesting knowledge of available pre-
trained models. However, we focus on merging vision back-
bones into a unified model in a multi-modal multi-encoder
setup. Further, on top of representation learning abilities,
we transfer zero-shot capabilities of the pre-trained models.
Segment Anything Model (SAM) [38] introduces a
large-scale dataset, a model, and a training recipe to en-
able segmentation given a prompt. The dataset consists of
triplets of an image, a geometric prompt, and a segmenta-
tion mask. SAM consists of an image encoder, a prompt
encoder, and a mask decoder. SAM‚Äôs image encoder is a
ViT-Det [45] pretrained with MAE [26] objective, which
is endowed with rich high-frequency localization knowl-
edge [65]. The prompt-encoder gets a geometric input in
the form of points, mask regions, or bounding boxes. The
mask decoder gets the output of both encoders and produces
a high-resolution segmentation mask. SAM is trained us-
ing a linear combination of Focal [48] and Dice [58] losses
and is capable of generating segmentation masks even when
the input prompt is ambiguous/low-quality. It is noteworthy
that Kirillov et al. [38] briefly discusses a possible multi-
task pre-training strategy to enable free-form text-to-mask
capability, but has not released the model.
There are a few follow-up works to SAM that we briefly
discuss here. HQ-SAM [34] adds an additional token and
a lightweight learnable layer to a frozen SAM model to en-
able high-quality segmentation using a small high-quality
annotated segmentation dataset. FastSAM [99] and Mobile-
SAM [96] employ CNN architecture and knowledge distil-
lation, respectively, to train smaller and faster variants of
the SAM model. Unlike our work, all these methods target
the same task as the original SAM and could potentially be
used as the base VFM in our proposed method. Semantic-
SAM [41] and SEEM [102] use semantic segmentation an-
notations for training to enable semantic-aware and multi-
granular segmentation, thus they are not zero-shot seman-
tic segmentation models. These works differ from our ap-
proach, which does not use any semantic segmentation an-
notations and instead gains semantic knowledge from distil-lation with CLIP. Besides, it has been shown that compos-
ing SAM and CLIP for semantic segmentation is feasible
by using SAM to generate all possible segmentation masks
and then using CLIP to provide labels [28]. However, this
approach requires loading two models simultaneously (2x
memory footprint) and, for each image, needs one forward
pass of the SAM backbone to generate Kobject segments,
followed by a forward pass of the CLIP model for each seg-
ment to filter (overall K+ 1forward passes)1.
Knowledge Distillation (KD) [5, 27] was originally
proposed to train a compressed classifier (student) us-
ing knowledge accumulated in a pretrained large model
(teacher). Related to our work, recent works explored dis-
tillation methods for VLMs such as EV A [17, 18], DIME-
FM [82], CLIPPING [67], and CLIP-KD [91]. They show
the transfer of the same zero-shot capability of the teacher
model to the student. Here, in a multi-task setup, we per-
form distillation and self-distillation [21], and demonstrate
the transfer of different zero-shot capabilities (from two
teachers) into a single model, as well as the emergence of
new zero-shot capability specific to the student model.
Continual Learning (CL) Our setup is also related to
Continual Learning [64], where new knowledge is added to
an existing model. The main challenge in continual learn-
ing is catastrophic forgetting [55, 56] referring to the loss
of previously learned knowledge due to learning new tasks.
Continual Learning algorithms usually alleviate forgetting
via regularization [39, 94], experience replay [25, 70], reg-
ularized replay [9, 19], dynamic expansion [78, 92], and
optimization based methods [59, 63], among them, replay
based methods proved to be simple yet very successful
ones [3, 51]. In this work, we propose a simple recipe
based on memory replay and distillation to merge VFMs
with minimal forgetting.
Zero-shot Semantic Segmentation task aims to predict
a dense segmentation mask given a text prompt in an open
form, without prior knowledge of specific object classes
of interest or any finetuning. Recent approaches to open-
vocabulary segmentation deploy image-text pairs datasets
and pretrained VLMs such as CLIP and their internal repre-
sentations to obtain dense segmentation masks, for example
GroupViT [88], ViewCo [72], CLIPpy [69], ViL-Seg [49],
OVS [89], TCL [7], and SegCLIP [53]. In this work, we
do not directly use any text data. Instead, all text semantic
knowledge is derived from a pretrained CLIP. An alternative
approach is to deploy existing models, without any training,
and generate segmentation masks using multiple backbones
in a multi-stage setup. For example, one can run SAM to
get several object proposals and run each through CLIP for
1With SAM-CLIP , only one ViT model needs to be loaded (lower
memory footprint), and a single forward pass of the ViT backbone is re-
quired for each image. Overall, our method offers significant efficiency
advantages over this model composition approach in terms of memory and
computational costs during inference.
3

--- PAGE 4 ---
Pooling
(Image)(Image, geometric prompt)
‚ÄúMotorbike‚Äù(Input text)(Input image)
(Output mask)[CLIP-HEAD](Output mask)[SAM-HEAD]
üî•
üî•
üî•
‚ùÑ
üî• Trainable module 
‚ùÑ Frozen module<latexit sha1_base64="U3azYbh5LvkdtajG4n785huRA0M=">AAACEHicbVDLSsNAFJ34rPFVdekm2IquSiKiLqsiuBEi2ge0pUymk3boTBJmbsQSsnYlgv/gF7hxoYhbl+78G6ePhbYeGDiccy9zz/EizhTY9rcxNT0zOzefWTAXl5ZXVrNr62UVxpLQEgl5KKseVpSzgJaAAafVSFIsPE4rXve071duqFQsDK6hF9GGwO2A+Yxg0FIzu5PPm3WBoSNF4spQRHAWkLSZ1IHeAkBydXyRpmY+38zm7II9gDVJnBHJFXMP/pN7Z7jN7Fe9FZJY0AAIx0rVHDuCRoIlMMJpatZjRSNMurhNa5oGWFDVSAaBUmtbKy3LD6V+AVgD9fdGgoVSPeHpyf7tatzri/95tRj8o0bCgigGqnMOPvJjbkFo9duxWkxSArynCSaS6Vst0sESE9AdmroEZzzyJCnvFZyDwv6lbuMEDZFBm2gL7SIHHaIiOkcuKiGC7tEzekVvxqPxYrwbH8PRKWO0s4H+wPj8AcIEn10=</latexit>PromptEncSAM
<latexit sha1_base64="SD7kC3NNVv5NaJ0QL0GRvBhq8JQ=">AAACD3icbVC7SgNBFJ2Nr7i+Vi1tFhPFxrAropbRICgoRDQPSEKYncwmQ2YfzNwVw7K1jSB+hD9gY6GIra2df+PkUWj0wMDhnHuZe44TcibBsr601MTk1PRMelafm19YXDKWV8oyiAShJRLwQFQdLClnPi0BA06roaDYczitON1C369cUyFZ4F9BL6QND7d95jKCQUlNYzOb1eseho7w4mOfJM24DvQGAOLLw/PtwtlpMUn0bLZpZKycNYD5l9gjksln7t3H4q1WbBqf9VZAIo/6QDiWsmZbITRiLIARThO9HkkaYtLFbVpT1McelY14kCcxN5TSMt1AqOeDOVB/bsTYk7LnOWqyf7oc9/rif14tAvegETM/jICqsIOP3IibEJj9cswWE5QA7ymCiWDqVpN0sMAEVIW6KsEej/yXlHdy9l5u90K1cYSGSKM1tI62kI32UR6doCIqIYLu0BN6Qa/ag/asvWnvw9GUNtpZRb+gfXwDvrmeJg==</latexit>EncSAM-CLIP<latexit sha1_base64="ObVZeNxxoSxjYfV2hw+5OF7nScg=">AAACC3icbVA9SwNBEN3z2/Pr1NLmSE6wCnciahm1sREimigkR9jbzOni3ge7c2I4rtZG/BW2NhaK2PoH7Pw3bhILTXww8Hhvhpl5QSq4Qtf9MsbGJyanpmdmzbn5hcUla3mloZJMMqizRCTyPKAKBI+hjhwFnKcSaBQIOAuuDnr+2TVIxZP4FLsp+BG9iHnIGUUtta2S45itiOKljPJDoJ2inbcQbhAxP9k7KgrTcdpW2a24fdijxPsh5Wr5IXys3Rq1tvXZ6iQsiyBGJqhSTc9N0c+pRM4EFGYrU5BSdkUvoKlpTCNQft7/pbDXtdKxw0TqitHuq78nchop1Y0C3dk7Ww17PfE/r5lhuOvnPE4zhJgNFoWZsDGxe8HYHS6BoehqQpnk+labXVJJGer4TB2CN/zyKGlsVrztytaxTmOfDDBD1kiJbBCP7JAqOSQ1UieM3JEn8kJejXvj2Xgz3getY8bPzCr5A+PjG34NnQU=</latexit>HeadSAM
<latexit sha1_base64="xVktCqupp0GTmJuwvpKM5N8A7mo=">AAACDHicbVC7SgNBFJ31GddX1NJmMRGswq6IWgbTRLCIYB6QhDA7uZsMzj6YuSuGZWux0a+wtrFQxNYPsPNvnE1S+DowcDjnXObe40aCK7TtT2Nmdm5+YTG3ZC6vrK6t5zc2GyqMJYM6C0UoWy5VIHgAdeQooBVJoL4roOleVjK/eQVS8TC4wFEEXZ8OAu5xRlFLvXyhWDQ7PsWh9JMq0H7aSzoI14iYVM5Oa2lqFos6ZZfsMay/xJmSQrlw7z3UboxaL//R6Ycs9iFAJqhSbceOsJtQiZwJSM1OrCCi7JIOoK1pQH1Q3WR8TGrtaqVveaHUL0BrrH6fSKiv1Mh3dTLbW/32MvE/rx2jd9xNeBDFCAGbfOTFwsLQypqx+lwCQzHShDLJ9a4WG1JJGer+TF2C8/vkv6SxX3IOSwfnuo0TMkGObJMdskccckTKpEpqpE4YuSWP5Jm8GHfGk/FqvE2iM8Z0Zov8gPH+BRsinVY=</latexit>HeadCLIP<latexit sha1_base64="hK/0z/JNAxiJj9rVp2qBcVGUoHQ=">AAACD3icbVC7SgNBFJ2Nr7i+opY2i4liFXZF1DIYBAWLCHlBdgmzk9lkyOyDmbtiWLa2EcSP8AdsLBSxtbXzb5w8Ck08MHA4517mnuNGnEkwzW8tMze/sLiUXdZXVtfWN3KbW3UZxoLQGgl5KJoulpSzgNaAAafNSFDsu5w23H556DduqJAsDKowiKjj427APEYwKKmd2y8UdNvH0BN+UqW3cB6QtJ3YoChAUr66rKSpXii0c3mzaI5gzBJrQvKl/IP3VLnTKu3cl90JSezTAAjHUrYsMwInwQIY4TTV7VjSCJM+7tKWogH2qXSSUZ7U2FNKx/BCoV4Axkj9vZFgX8qB76rJ4ely2huK/3mtGLxTJ2FBFANVQUcfeTE3IDSG5RgdJigBPlAEE8HUrQbpYYEJqAp1VYI1HXmW1A+L1nHx6Fq1cYbGyKIdtIsOkIVOUAldoAqqIYLu0TN6RW/ao/aivWsf49GMNtnZRn+gff4AqKuevQ==</latexit>TextEncCLIP<latexit sha1_base64="ObVZeNxxoSxjYfV2hw+5OF7nScg=">AAACC3icbVA9SwNBEN3z2/Pr1NLmSE6wCnciahm1sREimigkR9jbzOni3ge7c2I4rtZG/BW2NhaK2PoH7Pw3bhILTXww8Hhvhpl5QSq4Qtf9MsbGJyanpmdmzbn5hcUla3mloZJMMqizRCTyPKAKBI+hjhwFnKcSaBQIOAuuDnr+2TVIxZP4FLsp+BG9iHnIGUUtta2S45itiOKljPJDoJ2inbcQbhAxP9k7KgrTcdpW2a24fdijxPsh5Wr5IXys3Rq1tvXZ6iQsiyBGJqhSTc9N0c+pRM4EFGYrU5BSdkUvoKlpTCNQft7/pbDXtdKxw0TqitHuq78nchop1Y0C3dk7Ww17PfE/r5lhuOvnPE4zhJgNFoWZsDGxe8HYHS6BoehqQpnk+labXVJJGer4TB2CN/zyKGlsVrztytaxTmOfDDBD1kiJbBCP7JAqOSQ1UieM3JEn8kJejXvj2Xgz3getY8bPzCr5A+PjG34NnQU=</latexit>HeadSAM
<latexit sha1_base64="xVktCqupp0GTmJuwvpKM5N8A7mo=">AAACDHicbVC7SgNBFJ31GddX1NJmMRGswq6IWgbTRLCIYB6QhDA7uZsMzj6YuSuGZWux0a+wtrFQxNYPsPNvnE1S+DowcDjnXObe40aCK7TtT2Nmdm5+YTG3ZC6vrK6t5zc2GyqMJYM6C0UoWy5VIHgAdeQooBVJoL4roOleVjK/eQVS8TC4wFEEXZ8OAu5xRlFLvXyhWDQ7PsWh9JMq0H7aSzoI14iYVM5Oa2lqFos6ZZfsMay/xJmSQrlw7z3UboxaL//R6Ycs9iFAJqhSbceOsJtQiZwJSM1OrCCi7JIOoK1pQH1Q3WR8TGrtaqVveaHUL0BrrH6fSKiv1Mh3dTLbW/32MvE/rx2jd9xNeBDFCAGbfOTFwsLQypqx+lwCQzHShDLJ9a4WG1JJGer+TF2C8/vkv6SxX3IOSwfnuo0TMkGObJMdskccckTKpEpqpE4YuSWP5Jm8GHfGk/FqvE2iM8Z0Zov8gPH+BRsinVY=</latexit>HeadCLIP
<latexit sha1_base64="U3azYbh5LvkdtajG4n785huRA0M=">AAACEHicbVDLSsNAFJ34rPFVdekm2IquSiKiLqsiuBEi2ge0pUymk3boTBJmbsQSsnYlgv/gF7hxoYhbl+78G6ePhbYeGDiccy9zz/EizhTY9rcxNT0zOzefWTAXl5ZXVrNr62UVxpLQEgl5KKseVpSzgJaAAafVSFIsPE4rXve071duqFQsDK6hF9GGwO2A+Yxg0FIzu5PPm3WBoSNF4spQRHAWkLSZ1IHeAkBydXyRpmY+38zm7II9gDVJnBHJFXMP/pN7Z7jN7Fe9FZJY0AAIx0rVHDuCRoIlMMJpatZjRSNMurhNa5oGWFDVSAaBUmtbKy3LD6V+AVgD9fdGgoVSPeHpyf7tatzri/95tRj8o0bCgigGqnMOPvJjbkFo9duxWkxSArynCSaS6Vst0sESE9AdmroEZzzyJCnvFZyDwv6lbuMEDZFBm2gL7SIHHaIiOkcuKiGC7tEzekVvxqPxYrwbH8PRKWO0s4H+wPj8AcIEn10=</latexit>PromptEncSAM
(Input geometric prompt)
<latexit sha1_base64="SD7kC3NNVv5NaJ0QL0GRvBhq8JQ=">AAACD3icbVC7SgNBFJ2Nr7i+Vi1tFhPFxrAropbRICgoRDQPSEKYncwmQ2YfzNwVw7K1jSB+hD9gY6GIra2df+PkUWj0wMDhnHuZe44TcibBsr601MTk1PRMelafm19YXDKWV8oyiAShJRLwQFQdLClnPi0BA06roaDYczitON1C369cUyFZ4F9BL6QND7d95jKCQUlNYzOb1eseho7w4mOfJM24DvQGAOLLw/PtwtlpMUn0bLZpZKycNYD5l9gjksln7t3H4q1WbBqf9VZAIo/6QDiWsmZbITRiLIARThO9HkkaYtLFbVpT1McelY14kCcxN5TSMt1AqOeDOVB/bsTYk7LnOWqyf7oc9/rif14tAvegETM/jICqsIOP3IibEJj9cswWE5QA7ymCiWDqVpN0sMAEVIW6KsEej/yXlHdy9l5u90K1cYSGSKM1tI62kI32UR6doCIqIYLu0BN6Qa/ag/asvWnvw9GUNtpZRb+gfXwDvrmeJg==</latexit>EncSAM-CLIP(Training Pipeline)(Inference Pipeline)
<latexit sha1_base64="gOa4uvAOKd1xuOZ8Pre4RP7pnYA=">AAACCXicbVC7SgNBFJ2Nr7i+Vi1tBhPBKuyKqGXUxkIhonlAEsLsZDYZMvtg5q4Ylm21sPAr7GwsFLH1D+z8GyePQhMPXDiccy/33uNGgiuw7W8jMzM7N7+QXTSXlldW16z1jYoKY0lZmYYilDWXKCZ4wMrAQbBaJBnxXcGqbu904FdvmFQ8DK6hH7GmTzoB9zgloKWWhfN5s+ET6FIikvO0lTSA3QJAcnV8kaZmPt+ycnbBHgJPE2dMcsXco/dUujdKLeur0Q5p7LMAqCBK1R07gmZCJHAqWGo2YsUiQnukw+qaBsRnqpkMP0nxjlba2AulrgDwUP09kRBfqb7v6s7B0WrSG4j/efUYvKNmwoMoBhbQ0SIvFhhCPIgFt7lkFERfE0Il17di2iWSUNDhmToEZ/LlaVLZKzgHhf1LncYJGiGLttA22kUOOkRFdIZKqIwoukPP6BW9GQ/Gi/FufIxaM8Z4ZhP9gfH5A9denBw=</latexit>LSAM
<latexit sha1_base64="nU7Pzr9x/DoXHRG2l3N9ugqBrr0=">AAACCnicbVDLSsNAFJ3UV42vqEs30UZwVRIRdVnsRqGLCvYBbQmT6aQdOnkwcyOWkHU3gl/h0o0LRdz6Be78G6etC209cOFwzr3ce48XcybBtr+03MLi0vJKflVfW9/Y3DK2d+oySgShNRLxSDQ9LClnIa0BA06bsaA48DhteIPy2G/cUiFZFN7AMKadAPdC5jOCQUmusW9ZejvA0CeYp5XMTdtA7wAgLVeuqlmmW5ZrFOyiPYE5T5wfUigVHvzH6kirusZnuxuRJKAhEI6lbDl2DJ0UC2CE00xvJ5LGmAxwj7YUDXFAZSedvJKZh0rpmn4kVIVgTtTfEykOpBwGnuocXy1nvbH4n9dKwD/vpCyME6AhmS7yE25CZI5zMbtMUAJ8qAgmgqlbTdLHAhNQ6ekqBGf25XlSPy46p8WTa5XGBZoij/bQATpCDjpDJXSJqqiGCBqhJ/SCXrV77Vl7096nrTntZ2YX/YH28Q1zmpxt</latexit>LCLIP<latexit sha1_base64="pL/3z3rePY2zksUMDkg57mHi1Xw=">AAACCnicbVDLSsNAFJ3UV42vqks30VZwVRIRdVmsCwUXEewDmlAm00k7dPJg5kYsIetuBL/CpRsXirj1C9z5N05bF9p64MLhnHu59x4v5kyCaX5pubn5hcWl/LK+srq2vlHY3KrLKBGE1kjEI9H0sKSchbQGDDhtxoLiwOO04fWrI79xS4VkUXgDg5i6Ae6GzGcEg5Lahd1SSXcCDD2CeXqetVMH6B0ApNWrSzvL9FKpXSiaZXMMY5ZYP6RYKT74j/ZQs9uFT6cTkSSgIRCOpWxZZgxuigUwwmmmO4mkMSZ93KUtRUMcUOmm41cyY18pHcOPhKoQjLH6eyLFgZSDwFOdo6vltDcS//NaCfinbsrCOAEakskiP+EGRMYoF6PDBCXAB4pgIpi61SA9LDABlZ6uQrCmX54l9cOydVw+ulZpnKEJ8mgH7aEDZKETVEEXyEY1RNAQPaEX9Krda8/am/Y+ac1pPzPb6A+0j29m2pxl</latexit>DCLIP<latexit sha1_base64="sgeEOfoc3zun+DoYGqbA2xgYNJs=">AAACCXicbVC7SgNBFJ2Nr7i+Vi1tBhPBKuyKqGV8FDZCRPOAJITZyWwyZPbBzF0xLNtqYeFX2NlYKGLrH9j5N04ehSYeuHA4517uvceNBFdg299GZmZ2bn4hu2guLa+srlnrGxUVxpKyMg1FKGsuUUzwgJWBg2C1SDLiu4JV3d7pwK/eMKl4GFxDP2JNn3QC7nFKQEstC+fzZsMn0KVEJGdpK2kAuwWA5Or4Ik3NfL5l5eyCPQSeJs6Y5Iq5R++pdG+UWtZXox3S2GcBUEGUqjt2BM2ESOBUsNRsxIpFhPZIh9U1DYjPVDMZfpLiHa20sRdKXQHgofp7IiG+Un3f1Z2Do9WkNxD/8+oxeEfNhAdRDCygo0VeLDCEeBALbnPJKIi+JoRKrm/FtEskoaDDM3UIzuTL06SyV3AOCvuXOo0TNEIWbaFttIscdIiK6ByVUBlRdIee0St6Mx6MF+Pd+Bi1ZozxzCb6A+PzB8qmnBQ=</latexit>DSAM
Figure 2. Multi-head architecture of SAM-CLIP .Left: the training pipeline where we perform multi-task distillation from CLIP and
SAM teacher models on DCLIP andDSAMdatasets, respectively. Right : shows our inference pipeline where with a single backbone we
can perform multiple promptable tasks: classification, instance segmentation, and semantic segmentation. ‚äôdenotes the inner product
between text embedding and image patch embeddings.
semantic classification [50]. Some recent works [33, 85]
use internal attention maps of conditional vision generative
models such as StableDiffusion [74] to obtain segmentation
masks. While these approaches are training-free, they re-
quire several stages with complex processing, multiple vi-
sion encoders, and many forward passes, making their de-
ployment for edge devices limited.
Merging Models techniques aim to combine the capa-
bility of different models by simple interpolation operations
such as weight averaging [86] and task arithmetic [31]. Re-
cently there‚Äôs abundance of such techniques [2, 13, 30, 35,
54, 61, 80, 87] employing different weight schemes and
parameter sensitivity and importance. The way we train
SAM-CLIP , can be regarded as a data-dependent merging
approach where the knowledge of the models is combined
by repeatedly reminding them of their original behavior via
replay, while the optimization algorithm explores the pa-
rameter space to find an optimum.
3. Proposed Approach
In this section, we explain our approach for efficiently
merging pretrained VFMs. We start with a base VFM, then
transfer knowledge from other auxiliary VFMs to it with
minimal forgetting. We assume that each VFM possesses a
vision encoder, and potentially other modality encoders, as
well as task-specific decoders/heads. Our goal is to com-
bine the vision encoders into a single backbone such that
it can be used in conjunction with other modality encoders,
which remain frozen.
To focus our exposition, we constrain our discussion
to the specific case where SAM serves as the base VFM,
while a CLIP model serves as the auxiliary VFM. This
pair presents an intriguing combination, as both models
have been successfully deployed in diverse tasks and ex-
hibit complementary capabilities. SAM excels in localiza-
tion and high-resolution image segmentation but has lim-itations in semantic understanding. Conversely, CLIP of-
fers a powerful image backbone for semantic understand-
ing. We demonstrate it by several probing experiments (see
Figure 4). Potentially, one could start with CLIP as the base
VFM and merge knowledge of SAM to it. However, exist-
ing pretrained CLIP ViT models are inefficient in dealing
with high-resolution images that are used for SAM train-
ing. Hence, we choose SAM as the base model and inherit
its ViT-Det structure that can process high-resolution inputs
efficiently.
We assume access to limited subsets of datasets (or their
proxies) used to train the base and auxiliary VFMs, which
function as memory replay in our CL setup. These are de-
noted as DSAM andDCLIP , respectively with details pro-
vided in Section 4.1.
We employ a multi-head architecture, illustrated in Fig-
ure 2. Our base VFM, SAM, has an image encoder
(Enc SAM), a prompt encoder ( PromptEncSAM), and a
light mask decoder ( MaskDec SAM). The auxiliary VFM,
CLIP, has an image encoder ( Enc CLIP) and a text encoder
(TextEnc CLIP). Our goal is to merge both image encoders
to a single backbone called Enc SAM-CLIP which is initialized
byEnc SAM. Further, we consider lightweight heads corre-
sponding to each VFM, namely, Head SAMandHead CLIP .
Head SAMis initialized with MaskDec SAMandHead CLIP is
initialized with random weights (since CLIP does not come
with a head that we can deploy). We deploy other modality
encoders (i.e., PromptEncSAMandTextEnc CLIP) with no
change (frozen).
As a baseline merging approach, we perform KD on
DCLIP utilizing a cosine distillation loss [23]:
LCLIP =Ex‚àºD CLIP[ 1‚àí (1)
œïPooling(Head CLIP(Enc SAM-CLIP (x)))TEnc CLIP(x)]
where œïPoolingis a spatial pooling operator that gets patch-
level features from Head CLIP and produces a normalized
4

--- PAGE 5 ---
Table 1. Zero-shot evaluations on classification, text-to-image retrieval, and instance segmentation tasks, comparing SAM-CLIP with
state-of-the-art models that use the ViT-B architecture. SAM-CLIP demonstrates minimal forgetting compared to the baseline FMs on
their original tasks.
Model Training Data 0-Shot Classification (%) 0-Shot Image Retrieval (%) 0-Shot Instance Seg. (mAP)
ImageNet ImageNet-v2 Places-365 Flickr R@1 COCO R@1 COCO LVIS
SAM [38] SA-1B - - - - - 41.2 36.8
CLIP [68] OpenAI-400M 68.3 61.9 42.2 72.2 42.8 - -
CLIP [12] LAION-2B 70.2 62.3 43.4 78.1 50.9 - -
CLIP [22] DataComp-1B 73.5 65.6 43.0 76.3 48.8 - -
SAM-CLIP (Ours) Merged-41M 72.4 63.2 43.6 79.2 49.3 40.9 35.0
image-level embedding. In this setup, parameters of both
Head CLIP andEnc SAM-CLIP are learnable, while the CLIP
encoder, Enc CLIP , is frozen and used as a teacher. While
this infuses SAM with CLIP‚Äôs semantic abilities, it incurs at
the cost of catastrophic forgetting of SAM‚Äôs original capa-
bilities. Further, we show that training-free mitigative meth-
ods against catastrophic forgetting, such as Wise-FT [86],
to be ineffective in our context of VFM merging, as demon-
strated in section D.
To address these challenges, we propose a rehearsal-
based multi-task distillation. This serves two primary goals:
1) facilitate the efficient transfer of knowledge from the aux-
iliary VFM to the base model, and 2) preserve the original
capabilities of the base model. Inspired by Kumar et al.
[40], we consider a two-stage training: head-probing and
multi-task distillation. An optional stage of resolution adap-
tation can be appended if the multiple heads are trained un-
der different resolutions, which is the case in our experiment
of merging SAM and CLIP. See Section 4.1 for details about
resolution adaptation.
I. Head probing: In this stage, we first freeze the image
backbone, Enc SAM-CLIP , and only train Head CLIP with the
loss in Equation (1). Intuitively, with this approach, we first
learn some reasonable values for parameters of Head CLIP
(which is initialized randomly) before allowing any change
inEnc SAM-CLIP that is prone to forgetting.
II. Multi-task distillation: In this stage, we allow all
heads as well as our image encoder to be learnable. We
perform a multi-task training on LCLIP +ŒªLSAM, with:
LSAM=E(x,g)‚àºD SAMLFD(Head SAM(Enc SAM-CLIP (x),
PromptEncSAM(g)),z)(2)
where, xis a raw image, gis a geometric prompt, z=
MaskDec SAM(Enc SAM(x))is segmentation mask score pro-
duced by frozen SAM teacher, and LFDrefers to a lin-
ear combination of Focal [48] and Dice [58] used in the
original SAM training adapted for distillation. We train on
DSAM‚à™ D CLIP with total loss of LCLIP +ŒªLSAM. Dur-
ing training, each batch has some samples from DCLIP and
some form DSAM, which contribute to LCLIP andLSAM,respectively (i.e., samples from CLIP dataset do not con-
tribute to SAM loss and vice versa). To encourage less for-
getting, we use an order of magnitude smaller learning rate
for parameters of Enc SAM-CLIP andHead SAMcompared to
Head CLIP at this stage.
4. Experiments
4.1. Implementation Details
Our design choices, as explained below, aim to balance the
trade-off between learning from CLIP (zero-shot classifica-
tion) and retaining SAM‚Äôs knowledge (instance segmenta-
tion).
Model Architecture. We employ the ViT-B/16 version
of the Segment Anything Model (SAM) as our base archi-
tecture [38], comprising 12 transformer layers. To integrate
CLIP capabilities, we append a lightweight CLIP head con-
sisting of 3 transformer layers to the SAM backbone. The
patch token outputs from this CLIP head undergo a pool-
ing layer to produce an image-level embedding, akin to the
role of the CLS token output in ViT models. We adopt
max-pooling since we observe that it can lead to better zero-
shot classification and semantic segmentation performance
ofSAM-CLIP than average pooling. It is noteworthy that
max-pooling has been found to be able to encourage the
learning of spatial visual features [69]. With the pooling
layer, the CLIP head can output an embedding for the whole
image, which can be aligned with a text embedding just like
the original CLIP model [68].
Dataset Preparation. For CLIP distillation, we merge
images from several datasets: CC3M [79], CC12M [8],
YFCC-15M [68] (a curated subset of YFCC-100M [84] by
OpenAI) and ImageNet-21k [73]. This forms our DCLIP
containing 40.6M unlabeled images. For the SAM self-
distillation, we sample 5.7% subset from the SA-1B dataset
to form DSAM, which originally comprises 11M images
and 1.1B masks. We randomly select 1% of DCLIP and
DSAM as validation sets. Overall, we have 40.8M images
for training, which we term as Merged-41M in this work.
Training. As we discussed in Sec. 3, the training is con-
ducted in two phases to optimize convergence, in a ‚Äú prob-
5

--- PAGE 6 ---
(a) Input image
 (b) Ground-Truth
 (c)Head CLIP prediction
 (d)Head SAMrefined
Figure 3. Demo on zero-shot semantic segmentation. (a)(c) Passing an input image through the image encoder, Head CLIP can predict a
semantic segmentation mask (quantitative results provided in Table 2). (d)One can further refine it by passing the mask output of Head CLIP
and auto-generated point prompts to Head SAMto generate a more fine-grained semantic mask (quantitative results shown in Table 5).
ing then full finetuning ‚Äù style. The first stage of CLIP-head
probing takes 20 epochs on DCLIP , while the backbone is
kept frozen. Here, the teacher model is the OpenCLIP [29]
ViT-L/14 trained on the DataComp-1B dataset [22]. In
the second stage (16 epochs), we unfreeze the backbone
Enc SAM-CLIP and proceed with joint fine-tuning together
withHead CLIP andHead SAM, incorporating both CLIP and
SAM distillation losses at the ratio of 1:10. The original
SAM ViT-B model serves as the teacher in SAM loss. Fur-
ther, the learning rates applied to Enc SAM-CLIP andHead SAM
are 10 times smaller than that of Head CLIP in order to
reduce the forgetting of the original SAM abilities. Be-
sides, we adopt a mixed input resolution strategy for train-
ing. A notable difference between SAM and CLIP is their
pre-training resolution. SAM is trained and works best
on 1024px resolution while often lower resolutions (e.g.,
224/336/448px) are adopted for CLIP training and infer-
ence [12, 68, 81]. Hence, we employ variable resolutions
of 224/448px for the CLIP distillation via the variable batch
sampler approach of Mehta et al. [57], while SAM distilla-
tion utilizes a 1024px resolution in accordance with SAM‚Äôs
original training guidelines [38]. In every optimization step,
we form a batch of 2048 images from DCLIP and 32 images
(each with 32 mask annotations) from DSAM and perform
training in a multi-task fashion (see Appendix A for more
details).
Resolution Adaption. After the two training stages,
SAM-CLIP can accomplish CLIP tasks (e.g., zero-shot
classification) using the CLIP-head under 224/336/448px,
and run inference with the SAM-head under 1024px. How-
ever, if one wants to apply the two heads together on a single
input image for certain tasks (we present a demo of this in
Sec. 4.4), it would be inefficient to pass the image twice tothe image encoder with two resolutions for the two heads
respectively. To remedy this issue, we adapt the CLIP head
for 1024px input using a very short and efficient stage of
fine-tuning: freezing the image encoder and only finetuning
the CLIP-head with LCLIP for 3 epochs (it is the same as
the first stage of training, which is also CLIP-head probing)
under variable resolutions of 224/448/1024px. Note: res-
olution upscaling strategies are prevalent in CLIP training:
Li et al. [44], Radford et al. [68], Sun et al. [81] show it is
more efficient than training with high resolution from the
beginning.
More Details about implementation and training are pre-
sented in the Appendix A.
4.2. Zero-Shot Evaluations
CLIP Tasks: Zero-Shot Image Classification & Text-
to-Image Retrieval. To examine the CLIP-related capabil-
ities of SAM-CLIP , we evaluate it with zero-shot im-
age classification on ImageNet[14], ImageNet-v2 [71] and
Places365 [100], as well as zero-shot text-to-image retrieval
on Flickr30K [93] and COCO [47], under image resolution
of 336px. For classification, we use the text templates as
Radford et al. [68] utilizing the textual embeddings from the
text encoder of SAM-CLIP (which is kept frozen from our
CLIP teacher) to perform zero-shot classification without
any finetuning. For retrieval, we compute the cosine sim-
ilarity between the image and text embeddings to rank the
images for each text query and report the Recall@1 metric.
The evaluation results are presented in Table 1. Employing
a ViT-B architecture, our model achieves zero-shot accuracy
comparable to the state-of-the-art CLIP ViT-B models pre-
trained on LAION-2B [77] and DataComp-1B [22] (both
released by Ilharco et al. [29]), over the three classification
6

--- PAGE 7 ---
Table 2. Zero-shot semantic segmentation performance comparison with recent works. Note: The results of SAM-CLIP below are obtained
by using the CLIP-head only. The results with SAM-head refinement are provided in Table 5. (‚Ä†SegCLIP is trained on COCO data, so it is
not zero-shot transferred to COCO-Stuff.)
Model Arch Training Data 0-Shot Semantic Segmentation (mIoU %)
Pascal VOC Pascal-Context ADE20k COCO-Stuff COCO-Panoptic
GroupViT [88] ViT-S Merged-26M 52.3 22.4 - 24.3 -
ViewCo [72] ViT-S Merged-26M 52.4 23.0 - 23.5 -
ViL-Seg [49] ViT-B CC12M 37.3 18.9 - 18.0 -
OVS [89] ViT-B CC4M 53.8 20.4 - 25.1 -
CLIPpy [69] ViT-B HQITP-134M 52.2 - 13.5 - 25.5
TCL [7] ViT-B CC3M+CC12M 51.2 24.3 14.9 19.6 -
SegCLIP [53] ViT-B CC3M+COCO 52.6 24.7 8.7 26.5‚Ä†-
SAM-CLIP (CLIP-head) ViT-B Merged-41M 60.6 29.2 17.1 31.5 28.8
Table 3. Head probing evaluations on semantic segmentation datasets, comparing our model with SAM and CLIP that use the ViT-B
architecture. Avg is the average evaluation results of three heads.
Training Data Pascal VOC ADE20k
Model Linear DeepLabv3 PSPNet Avg Linear DeepLabv3 PSPNet Avg
SAM SA-1B 46.6 69.9 71.2 62.6 26.6 32.8 36.2 31.9
CLIP DataComp-1B 70.7 78.9 79.7 76.4 36.4 39.4 40.7 38.8
SAM-CLIP Merged-41M 75.0 80.3 81.3 78.8 38.4 41.1 41.7 40.4
datasets. Moreover, SAM-CLIP outperforms the CLIP ViT-
B/16 model trained on DataComp-1B on both Flickr30K
and COCO retrieval datasets. These results validate the
efficacy of our merging approach in inheriting CLIP‚Äôs ca-
pabilities. Note: We observe that SAM-CLIP benefits
from a 336px resolution for zero-shot image classification,
whereas the baseline CLIP models do not, as they were
trained at a 224px resolution (the reported results of base-
line CLIP models in Table 1 are evaluated at 224px). The
evaluation results of SAM-CLIP at 224px vs. 336px reso-
lutions are provided in Appendix A.
SAM Task: Zero-Shot Instance Segmentation. For the
SAM component of SAM-CLIP , we evaluate its perfor-
mance in instance segmentation, a task at which the origi-
nal SAM model excels [38], with COCO [47] and LVIS [24]
datasets. Following the original practices of Kirillov et al.
[38], we first generate object detection bounding boxes us-
ing a ViT-Det model (ViT-B version) [45]. These bounding
boxes act as geometric prompts for SAM‚Äôs prompt encoder,
which then predicts masks for each object instance. The
evaluation results of SAM-CLIP and the original SAM ViT-
B are provided in Table 1 (both under 1024px resolution),
showing that SAM-CLIP is very close to SAM on the two
benchmarks, not suffering from catastrophic forgetting dur-
ing training.
Zero-Shot Transfer to Semantic Segmentation. We
extend our evaluation to (text-prompted) zero-shot seman-
tic segmentation over 5 datasets, Pascal VOC [16], Pas-
cacl Context [60], ADE20k [101], COCO-Stuff [6] and
COCO-Panoptic [37]. We adopt a common evaluation pro-
tocol for this task: i) each input image is resized to 448√ó
448px and passed to the image encoder and CLIP-head of
SAM-CLIP to obtain 28√ó28patch features; ii) OpenAI‚Äôs80 pre-defined CLIP text templates are employed to gener-
ate textual embeddings for each semantic class, and these
embeddings act as mask prediction classifiers and operate
on the patch features from the CLIP head; iii) we linearly
upscale the mask prediction logits to match the dimensions
of the input image. Evaluation results of SAM-CLIP and
previous zero-shot models over the five datasets are demon-
strated in Fig. 2. Notably, SAM-CLIP establishes new
state-of-the-art performance on all 5 datasets, with a sig-
nificant margin over past works. More details are provided
in Appendix C.
4.3. Head-Probing Evaluations on Learned Repre-
sentations
By merging the SAM and CLIP models, we anticipate that
the resultant model will inherit advantages at the represen-
tation level from both parent models. Specifically, SAM
excels at capturing low-level spatial visual details pertinent
to segmentation tasks, while CLIP specializes in high-level
semantic visual information encompassing the entire im-
age. We hypothesize that the merged model combines these
strengths, thereby enhancing its utility in a broad range
of downstream vision tasks. To investigate this hypoth-
esis, we conduct head-probing (i.e., learn a task-specific
head with a frozen image backbone) evaluations on SAM,
CLIP, and SAM-CLIP , utilizing different segmentation
head structures (linear head, DeepLab-v3 [10] and PSP-
Net [98]) across two semantic segmentation datasets, Pascal
VOC and ADE20k. The results are presented in Table 3. We
observe that SAM representations do not perform as well as
those of CLIP for tasks that require semantic understanding,
even for semantic segmentation. However, SAM-CLIP out-
performs both SAM and CLIP across different head struc-
7

--- PAGE 8 ---
Figure 4. Representation learning comparison. Head-probing evalua-
tion of each vision backbone for classification and semantic segmen-
tation tasks. The results show that SAM-CLIP learns richer visual
features compared to SAM and CLIP.Table 4. Linear probing evaluations on image
classification datasets with ViT-B models.
Model Linear Probing
ImageNet Places365
SAM 41.2 41.5
CLIP (DataComp1B) 81.3 55.1
CLIP (LAION-2B) 79.6 55.2
SAM-CLIP 80.5 55.3
Table 5. Composing both CLIP and SAM heads
ofSAM-CLIP for zero-shot semantic segmenta-
tion on Pascal VOC.
Method Resolution mIoU
CLIP head only 448px 60.6
CLIP+SAM heads 1024px 66.0
tures and datasets, thereby confirming its superior visual
feature representation capabilities.
Besides, we apply linear probing to these models for
image classification tasks on two datasets, ImageNet and
Places365. Results in Table 4 show that SAM-CLIP at-
tains comparable performance with CLIP, implying that
the image-level representation of SAM-CLIP is also well-
learned. All head probing evaluation results are visualized
in Figure 4 to deliver messages more intuitively.
4.4. Composing Both CLIP and SAM Heads for
Better Segmentation
Given that SAM-CLIP is a multi-task model with SAM
and CLIP heads, one would naturally ask if the two heads
can work together towards better performance on some
tasks. Here, we showcase that a simple composition of the
CLIP and SAM heads can lead to better zero-shot seman-
tic segmentation. Specifically, we resize the input image to
1024px and pass it through Enc SAM-CLIP , and use the CLIP
head to generate low-resolution mask prediction ( 32√ó32)
using text prompts. Then, we generate some point prompts
from the mask prediction (importance sampling based on
the mask prediction confidence), and pass the mask predic-
tion and point prompts together to the prompt encoder mod-
ule as geometric prompts. Finally, Head SAM takes embed-
dings from both the prompt encoder and the image encoder
to generate high-resolution mask predictions ( 256√ó256)
as shown in Fig. 2 (right). Examples of this pipeline are
shown in Fig. 3. One can clearly observe that the refined
segmentation by the SAM-head is more fine-grained. The
implementation details are discussed in Appendix C.Note that this pipeline requires only one forward pass
onEnc SAM-CLIP with 1024px resolution. For fair compar-
ison, in Table 1 and Figure 1 we report SAM-CLIP zero-
shot segmentation performance with 448px resolution us-
ingHead CLIP only. Using our high-resolution pipeline, we
obtain further gain in zero-shot semantic segmentation as
shown in Table 5.
5. Conclusion
We discussed merging publicly available vision foundation
models, as digested sources of visual knowledge, into a
single unified architecture. We proposed a simple and ef-
ficient recipe based on multi-task distillation and memory
rehearsal. Specifically, we instantiated our proposed ap-
proach to merge SAM and CLIP vision foundation models,
and introduced SAM-CLIP . SAM and CLIP have comple-
mentary vision capabilities: one is good at spatial under-
standing, while the other excels at semantic understanding
of images. We demonstrate multiple benefits as a result of
our proposed approach: 1) We obtain a single vision back-
bone with minimal forgetting of zero-shot capabilities of
the original models, suitable for edge device deployment.
2) We demonstrate the merged model produces richer rep-
resentations utilizable for more diverse downstream tasks
when compared to original models in a head-probing evalu-
ation setup. 3) The merged model demonstrates synergistic
new zero-shot capability thanks to complementary inherited
skills from the parent models. Specifically, we show that
SAM-CLIP obtains state-of-the-art performance on zero-
shot semantic segmentation by combining semantic under-
standing of CLIP and localization knowledge of SAM.
8

--- PAGE 9 ---
References
[1] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha
Srinivasa. Git re-basin: Merging models modulo permu-
tation symmetries. arXiv preprint arXiv:2209.04836 , 2022.
2
[2] Jinze Bai, Rui Men, Hao Yang, Xuancheng Ren, Kai Dang,
Yichang Zhang, Xiaohuan Zhou, Peng Wang, Sinan Tan,
An Yang, et al. Ofasys: A multi-modal multi-task learn-
ing system for building generalist models. arXiv preprint
arXiv:2212.04408 , 2022. 4
[3] Yogesh Balaji, Mehrdad Farajtabar, Dong Yin, Alex Mott,
and Ang Li. The effectiveness of memory replay in large
scale continual learning. arXiv preprint arXiv:2010.02418 ,
2020. 3
[4] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu,
Owais Khan Mohammed, Kriti Aggarwal, Subhojit Som,
Songhao Piao, and Furu Wei. Vlmo: Unified vision-
language pre-training with mixture-of-modality-experts.
Advances in Neural Information Processing Systems , 35:
32897‚Äì32912, 2022. 3
[5] Cristian Bucilu Àáa, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery
and data mining , pages 535‚Äì541, 2006. 3
[6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1209‚Äì1218, 2018. 7
[7] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learn-
ing to generate text-grounded mask for open-world seman-
tic segmentation from only image-text pairs. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11165‚Äì11174, 2023. 3, 7
[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In
CVPR , 2021. 5
[9] Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus
Rohrbach, and Mohamed Elhoseiny. Efficient lifelong
learning with a-gem. arXiv preprint arXiv:1812.00420 ,
2018. 3
[10] Liang-Chieh Chen, George Papandreou, Florian Schroff,
and Hartwig Adam. Rethinking atrous convolution
for semantic image segmentation. arXiv preprint
arXiv:1706.05587 , 2017. 7
[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on
machine learning , pages 1597‚Äì1607. PMLR, 2020. 1
[12] Mehdi Cherti, Romain Beaumont, Ross Wightman,
Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,
Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
Reproducible scaling laws for contrastive language-image
learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2818‚Äì
2829, 2023. 5, 6, 16[13] Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav
Katz. Fusing finetuned models for better pretraining. arXiv
preprint arXiv:2204.03044 , 2022. 4
[14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR , 2009. 6
[15] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
Gradient descent provably optimizes over-parameterized
neural networks. In International Conference on Learning
Representations , 2019. 1
[16] Mark Everingham, Luc Van Gool, Christopher KI
Williams, John Winn, and Andrew Zisserman. The pascal
visual object classes (voc) challenge. International journal
of computer vision , 88:303‚Äì338, 2010. 7
[17] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang,
Xinlong Wang, and Yue Cao. Eva-02: A visual represen-
tation for neon genesis. arXiv preprint arXiv:2303.11331 ,
2023. 3
[18] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Cao. Eva: Exploring the limits of masked visual represen-
tation learning at scale. CVPR , 2023. 3
[19] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
Orthogonal gradient descent for continual learning. In In-
ternational Conference on Artificial Intelligence and Statis-
tics, pages 3762‚Äì3773. PMLR, 2020. 3
[20] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan
Anil, and Chelsea Finn. Efficiently identifying task group-
ings for multi-task learning. Advances in Neural Informa-
tion Processing Systems , 34:27503‚Äì27516, 2021. 1
[21] Tommaso Furlanello, Zachary Lipton, Michael Tschannen,
Laurent Itti, and Anima Anandkumar. Born again neural
networks. In International Conference on Machine Learn-
ing, pages 1607‚Äì1616. PMLR, 2018. 3
[22] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Dat-
acomp: In search of the next generation of multimodal
datasets. arXiv preprint arXiv:2304.14108 , 2023. 1, 5, 6
[23] Jean-Bastien Grill, Florian Strub, Florent Altch ¬¥e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Do-
ersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad
Gheshlaghi Azar, et al. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural
information processing systems , 33:21271‚Äì21284, 2020. 4
[24] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A
dataset for large vocabulary instance segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 5356‚Äì5364, 2019. 7
[25] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan.
Memory efficient experience replay for streaming learning.
In2019 International Conference on Robotics and Automa-
tion (ICRA) , pages 9769‚Äì9776. IEEE, 2019. 3
[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll¬¥ar, and Ross Girshick. Masked autoencoders are scal-
able vision learners. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
16000‚Äì16009, 2022. 1, 3
9

--- PAGE 10 ---
[27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 2, 3
[28] IDEA Research. Grounded-sam: Marrying grounding dino
with segment anything & stable diffusion & recognize any-
thing - automatically detect, segment and generate any-
thing, 2023. 3
[29] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 6, 14
[30] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-
man, Suchin Gururangan, Ludwig Schmidt, Hannaneh Ha-
jishirzi, and Ali Farhadi. Editing models with task arith-
metic. arXiv preprint arXiv:2212.04089 , 2022. 4
[31] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre,
Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali
Farhadi, and Ludwig Schmidt. Patching open-vocabulary
models by interpolating weights. Advances in Neural In-
formation Processing Systems , 35:29262‚Äì29277, 2022. 2,
4
[32] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig. Scaling up visual and vision-language
representation learning with noisy text supervision. In In-
ternational conference on machine learning , pages 4904‚Äì
4916. PMLR, 2021. 2
[33] Laurynas Karazija, Iro Laina, Andrea Vedaldi, and
Christian Rupprecht. Diffusion models for zero-
shot open-vocabulary segmentation. arXiv preprint
arXiv:2306.09316 , 2023. 4
[34] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-
Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment any-
thing in high quality. arXiv preprint arXiv:2306.01567 ,
2023. 3
[35] Simran Khanuja, Melvin Johnson, and Partha Talukdar.
Mergedistill: Merging pre-trained language models using
distillation. arXiv preprint arXiv:2106.02834 , 2021. 4
[36] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
and-language transformer without convolution or region su-
pervision. In International Conference on Machine Learn-
ing, pages 5583‚Äì5594. PMLR, 2021. 3
[37] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ¬¥ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 9404‚Äì9413, 2019. 7
[38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ¬¥ar,
and Ross Girshick. Segment anything. arXiv:2304.02643 ,
2023. 1, 3, 5, 6, 7, 14, 16
[39] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in
neural networks. Proceedings of the national academy of
sciences , 114(13):3521‚Äì3526, 2017. 3[40] Ananya Kumar, Aditi Raghunathan, Robbie Matthew
Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort
pretrained features and underperform out-of-distribution.
InInternational Conference on Learning Representations ,
2022. 5, 14
[41] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong
Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng
Gao. Semantic-sam: Segment and recognize anything at
any granularity. arXiv preprint arXiv:2307.04767 , 2023. 3
[42] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for uni-
fied vision-language understanding and generation. In
International Conference on Machine Learning , pages
12888‚Äì12900. PMLR, 2022. 3
[43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 3
[44] Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scal-
ing law for clip training. NeurIPS , 2023. 6
[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
Exploring plain vision transformer backbones for object
detection. In European Conference on Computer Vision ,
pages 280‚Äì296. Springer, 2022. 3, 7, 14
[46] Zhizhong Li and Derek Hoiem. Learning without forget-
ting. IEEE transactions on pattern analysis and machine
intelligence , 40(12):2935‚Äì2947, 2017. 2
[47] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ¬¥ar, and
C Lawrence Zitnick. Microsoft coco: Common objects in
context. In Computer Vision‚ÄìECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13 , pages 740‚Äì755. Springer, 2014. 6,
7
[48] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,
and Piotr Doll ¬¥ar. Focal loss for dense object detection. In
Proceedings of the IEEE international conference on com-
puter vision , pages 2980‚Äì2988, 2017. 3, 5
[49] Quande Liu, Youpeng Wen, Jianhua Han, Chunjing Xu,
Hang Xu, and Xiaodan Liang. Open-world semantic seg-
mentation via contrasting and clustering vision-language
embedding. In European Conference on Computer Vision ,
pages 275‚Äì292. Springer, 2022. 3, 7
[50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499 , 2023. 4
[51] Vincenzo Lomonaco, Lorenzo Pellegrini, Pau Rodriguez,
Massimo Caccia, Qi She, Yu Chen, Quentin Jodelet, Ruip-
ing Wang, Zheda Mai, David Vazquez, et al. Cvpr 2020
continual learning in computer vision competition: Ap-
proaches, results, current challenges and future directions.
Artificial Intelligence , 303:103635, 2022. 3
[52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 14
10

--- PAGE 11 ---
[53] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,
and Tianrui Li. Segclip: Patch aggregation with learn-
able centers for open-vocabulary semantic segmentation.
InInternational Conference on Machine Learning , pages
23033‚Äì23044. PMLR, 2023. 3, 7
[54] Michael S Matena and Colin A Raffel. Merging models
with fisher-weighted averaging. Advances in Neural Infor-
mation Processing Systems , 35:17703‚Äì17716, 2022. 2, 4
[55] James L McClelland, Bruce L McNaughton, and Randall C
O‚ÄôReilly. Why there are complementary learning systems
in the hippocampus and neocortex: insights from the suc-
cesses and failures of connectionist models of learning and
memory. Psychological review , 102(3):419, 1995. 3
[56] Michael McCloskey and Neal J Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. In Psychology of learning and motivation , pages
109‚Äì165. Elsevier, 1989. 2, 3
[57] Sachin Mehta, Farzad Abdolhosseini, and Mohammad
Rastegari. Cvnets: High performance library for computer
vision. In Proceedings of the 30th ACM International Con-
ference on Multimedia , 2022. 6, 14
[58] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV) , pages 565‚Äì571. Ieee, 2016.
3, 5
[59] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pas-
canu, and Hassan Ghasemzadeh. Understanding the role
of training regimes in continual learning. Advances in Neu-
ral Information Processing Systems , 33:7308‚Äì7320, 2020.
3
[60] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2014. 7
[61] Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft
merging of experts with adaptive routing. arXiv preprint
arXiv:2306.03745 , 2023. 4
[62] Maxime Oquab, Timoth ¬¥ee Darcet, Theo Moutakanni,
Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-
dez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma,
Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Ass-
ran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve
Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and
Piotr Bojanowski. Dinov2: Learning robust visual features
without supervision, 2023. 1
[63] Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa
Eschenhagen, Richard Turner, and Mohammad Emtiyaz E
Khan. Continual deep learning by functional regularisation
of memorable past. Advances in Neural Information Pro-
cessing Systems , 33:4453‚Äì4464, 2020. 3
[64] German I Parisi, Ronald Kemker, Jose L Part, Christopher
Kanan, and Stefan Wermter. Continual lifelong learning
with neural networks: A review. Neural networks , 113:54‚Äì
71, 2019. 2, 3[65] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim,
and Sangdoo Yun. What do self-supervised vision trans-
formers learn? In The Eleventh International Conference
on Learning Representations , 2022. 1, 3
[66] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library.
Advances in Neural Information Processing Systems , 32:
8026‚Äì8037, 2019. 14
[67] Renjing Pei, Jianzhuang Liu, Weimian Li, Bin Shao, Song-
cen Xu, Peng Dai, Juwei Lu, and Youliang Yan. Clipping:
Distilling clip-based models with a student base for video-
language retrieval. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18983‚Äì18992, 2023. 3
[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning ,
pages 8748‚Äì8763. PMLR, 2021. 1, 3, 5, 6, 16
[69] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi,
Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Per-
ceptual grouping in contrastive vision-language models.
ICCV , 2023. 3, 5, 7, 16
[70] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental clas-
sifier and representation learning. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 2001‚Äì2010, 2017. 3
[71] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classifiers generalize to im-
agenet? In International conference on machine learning ,
pages 5389‚Äì5400. PMLR, 2019. 6
[72] Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guan-
grun Wang, Jianzhuang Liu, Xiaojun Chang, and Xiaodan
Liang. Viewco: Discovering text-supervised segmentation
masks via multi-view semantic consistency. arXiv preprint
arXiv:2302.10307 , 2023. 3, 7
[73] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi
Zelnik-Manor. Imagenet-21k pretraining for the masses. In
Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track , 2021. 5
[74] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 4
[75] Elan Rosenfeld, Preetum Nakkiran, Hadi Pouransari, Oncel
Tuzel, and Fartash Faghri. Ape: Aligning pretrained en-
coders to quickly learn aligned multimodal representations.
InHas it Trained Yet? NeurIPS 2022 Workshop , 2022. 3,
14
[76] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,
Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-
naud Stiegler, Teven Le Scao, Arun Raja, et al. Multi-
11

--- PAGE 12 ---
task prompted training enables zero-shot task generaliza-
tion. arXiv preprint arXiv:2110.08207 , 2021. 1
[77] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa R Kundurthy, Kather-
ine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and
Jenia Jitsev. LAION-5b: An open large-scale dataset for
training next generation image-text models. In Thirty-
sixth Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track , 2022. 1, 6
[78] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina,
Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan
Pascanu, and Raia Hadsell. Progress & compress: A scal-
able framework for continual learning. In International
conference on machine learning , pages 4528‚Äì4537. PMLR,
2018. 3
[79] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In ACL,
2018. 5
[80] George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn,
and Judy Hoffman. Zipit! merging models from different
tasks without training. arXiv preprint arXiv:2305.03053 ,
2023. 4
[81] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. Eva-clip: Improved training techniques for clip at
scale. arXiv preprint arXiv:2303.15389 , 2023. 6
[82] Ximeng Sun, Pengchuan Zhang, Peizhao Zhang, Hardik
Shah, Kate Saenko, and Xide Xia. Dime-fm: Distilling
multimodal and efficient foundation models. arXiv preprint
arXiv:2303.18232 , 2023. 3
[83] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal,
and Lijuan Wang. An empirical study of multimodal model
merging. arXiv preprint arXiv:2304.14933 , 2023. 1, 2
[84] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,
and Li-Jia Li. Yfcc100m: The new data in multimedia re-
search. Communications of the ACM , 59(2):64‚Äì73, 2016.
5
[85] Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin
Zhou, Qian Yu, Lu Sheng, and Dong Xu. Diffusion model
is secretly a training-free open vocabulary semantic seg-
menter. arXiv preprint arXiv:2309.02773 , 2023. 4
[86] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, and Ludwig Schmidt. Robust fine-tuning of
zero-shot models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7959‚Äì7971, 2022. 4, 5, 17
[87] Chengyue Wu, Teng Wang, Yixiao Ge, Zeyu Lu, Ruisong
Zhou, Ying Shan, and Ping Luo. œÄ-tuning: Transfer-
ring multimodal foundation models with optimal multi-
task interpolation. In International Conference on Machine
Learning , pages 37713‚Äì37727. PMLR, 2023. 4
[88] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:Semantic segmentation emerges from text supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18134‚Äì18144, 2022.
3, 7
[89] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu
Qiao, and Weidi Xie. Learning open-vocabulary semantic
segmentation models from natural language supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2935‚Äì2944, 2023. 3,
7
[90] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel,
and Mohit Bansal. Resolving interference when merging
models. arXiv preprint arXiv:2306.01708 , 2023. 1
[91] Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi,
Xinqiang Yu, Han Yang, and Yongjun Xu. Clip-kd: An
empirical study of distilling clip models. arXiv preprint
arXiv:2307.12732 , 2023. 3
[92] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju
Hwang. Lifelong learning with dynamically expandable
networks. arXiv preprint arXiv:1708.01547 , 2017. 3
[93] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. From image descriptions to visual denotations:
New similarity metrics for semantic inference over event
descriptions. Transactions of the Association for Computa-
tional Linguistics , 2:67‚Äì78, 2014. 6
[94] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
conference on machine learning , pages 3987‚Äì3995. PMLR,
2017. 3
[95] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 18123‚Äì18133, 2022.
3
[96] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim,
Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.
Faster segment anything: Towards lightweight sam for mo-
bile applications. arXiv preprint arXiv:2306.14289 , 2023.
3, 17
[97] Yu Zhang and Qiang Yang. A survey on multi-task learning.
IEEE Transactions on Knowledge and Data Engineering ,
34(12):5586‚Äì5609, 2021. 1
[98] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 2881‚Äì2890, 2017. 7
[99] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao
Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment
anything. arXiv preprint arXiv:2306.12156 , 2023. 3
[100] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 million image database
for scene recognition. IEEE transactions on pattern analy-
sis and machine intelligence , 40(6):1452‚Äì1464, 2017. 6
[101] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-
dler, Adela Barriuso, and Antonio Torralba. Semantic un-
derstanding of scenes through the ade20k dataset. Inter-
12

--- PAGE 13 ---
national Journal of Computer Vision , 127:302‚Äì321, 2019.
7
[102] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie
Li, Jianfeng Gao, and Yong Jae Lee. Segment everything
everywhere all at once. arXiv preprint arXiv:2304.06718 ,
2023. 3
13

--- PAGE 14 ---
A. More Experimental Details
Software We built our codebase using PyTorch [66] and the CVNets framework [57]. The evaluation code for instance
segmentation relies on the publicly released codebases from Kirillov et al. [38] and Li et al. [45].
Hardware We conducted all experiments on servers equipped with 8√óA100 GPUs. For training our models, we most
employed multi-node training across four 8√óA100 servers. The local batch size per server is one-fourth of the global batch
size.
CLIP Head Structure We initialized each transformer layer of the CLIP head using parameters from the last transformer
layer of SAM ViT-B, as we found this approach to expedite training compared to random initialization. Following the
implementation of CLIP-ConvNeXt in Ilharco et al. [29] (the only OpenCLIP model that uses a pooling layer instead of
a CLS token), we incorporated a LayerNorm layer subsequent to the pooling layer. After applying LayerNorm, we use a
shallow MLP with two hidden layers to project the features into the text-embedding space, consistent with the approach in
Rosenfeld et al. [75].
Hyperparameters We employ AdamW optimizers [52] with a learning rate of 8√ó10‚àí4(consistent with SAM training
[38]) during the first training stage (head probing) for 20 epochs. This rate is reduced to 4√ó10‚àí5during the second stage
(joint distillation) for 16 epochs. It should be noted that we apply a learning rate multiplier of 0.1 to the backbone and SAM
head in the second stage to mitigate forgetting. The learning rate in the resolution adaptation stage (3 epochs) remains the
same as in the first stage. The global image batch size for CLIP distillation is 2048, and for SAM distillation, it is 32 (i.e., 32
images from the SA-1B dataset [38]). In the latter case, we randomly sample 32 masks for each image.
Multi-Task Distillation Our training process consists of two stages: 1) Head probing to learn parameters of Head CLIP
that are initialized randomly, and 2) Joint training of the Head SAM,Head CLIP, and the ViT backbone Enc SAM-CLIP using a
multi-task distillation loss.
In the first stage, only the Head CLIP is trainable, and it is trained using a single CLIP distillation loss (cosine distance
between embeddings as in Equation (1)). At this stage, all image batches are sampled only from DCLIP . This stage involves
training for a fixed duration of 20 epochs without early stopping. The motivation for this step is to have a warm start for the
Head CLIP in the next stage where we also allow modifying the backbone, similar to Kumar et al. [40].
In the second stage, the Head SAM and the ViT backbone Enc SAM-CLIP become also trainable, and we have a multi-
task objective: CLIP Distillation Equation (1) and SAM self-distillation Equation (2). The balance between the losses is
determined by the coefficient Œª, which we picked to optimize the trade-off between learning semantic knowledge from CLIP
and forgetting SAM‚Äôs segmentation knowledge. We experimented with Œª= 1,10,100, and found that Œª= 10 offers the best
trade-off between mitigating the forgetting of SAM‚Äôs ability and learning CLIP‚Äôs ability.
Each training step for the second stage is performed as follows:
‚Ä¢ Sample a batch of 2048 images from DCLIP . 2048 is determined based on available total GPU memory. Run the forward
pass, and compute gradients backward from LCLIP (note that only parameters of the Head CLIP andEnc SAM-CLIP will get
gradients after this step).
‚Ä¢ Sample a batch of 32 images from DSAM. 32 is determined based on available total GPU memory. Run the forward pass,
and compute gradients backward from LSAM(note that only parameters of the Head SAMandEnc SAM-CLIP will get gradients
after this step).
‚Ä¢ Apply one optimization step (note that at this point, the parameters of the Enc SAM-CLIP have accumulated gradients from
both of the above two steps).
We early-stop after 16 epochs (out of a full training length of 20 epochs) as we observed more forgetting (as measured by
instance segmentation performance on the COCO dataset) after the 16th epoch.
Loss Coefficients We empirically determined the loss coefficient ratio of 1:10 for the CLIP and SAM distillation losses
from three options: 1:1, 1:10, and 1:100. This ratio provides the best trade-off between mitigating SAM‚Äôs ability to forget
and fostering the learning of CLIP‚Äôs ability. Specifically, a ratio of 1:1 leads to greater forgetting of SAM‚Äôs original ability (as
measured by the performance drop in instance segmentation on COCO), while ratios of 1:10 and 1:100 maintain it relatively
well. However, a ratio of 1:100 impedes the learning of CLIP‚Äôs ability (as measured by zero-shot accuracy on ImageNet).
Therefore, we ultimately selected the ratio of 1:10.
14

--- PAGE 15 ---
(1a) SAM Output
 (1b) SAM-CLIP Output
(2a) SAM Output
 (2b) SAM-CLIP Output
Figure 5. Comparison of instance segmentation between SAM and SAM-CLIP . The same images, along with geometric prompts (bound-
ing box and point), are provided to both SAM and SAM-CLIP , and their respective model outputs are displayed above. While the outputs
of SAM and SAM-CLIP exhibit slight differences, they are overall quite similar.
(1a) Input image of three dogs
 (1b) SAM-CLIP Segmentation
 (1c) SAM Segmentation
(2a) Input image of a horse and
a humen
(2b) SAM-CLIP Segmentation
Mask
(2c) SAM Segmentation for the
horse
(2d) SAM Segmentation for the
human
Figure 6. Comparison of SAM vs. SAM-CLIP for semantic segmentation on two images. The segmentation of SAM-CLIP is obtained by:
i) using CLIP-head output (i.e., coarse-grained prediction masks) to generate point prompts automatically, and ii) passing the CLIP-head
output and point prompts to the SAM-head to generate final fine-grained prediction masks. For SAM, the same point prompts for each
class (‚Äúdog‚Äù, ‚Äúhuman‚Äù, ‚Äúhuman‚Äù) are passed to its prompt encoder to generate a segmentation mask.
Image Resolution for Zero-Shot Classification In Table 1, we report the evaluation results for both SAM-CLIP and CLIP
models using the 224px image resolution. However, we found that SAM-CLIP benefits from the 336px resolution, whereas
the performance of CLIP models deteriorates (they exhibit worse accuracy). The 336px results for SAM-CLIP are incorpo-
rated into the diagram in Figure 1. We provide a comparison between the 224px and 336px resolutions for SAM-CLIP in
Table 6.
Table 6. Different input resolutions for zero-shot image classification.
Resolution ImageNet ImageNet-v2 Places365
224px 71.7 63.2 43.4
336px 72.4 63.2 43.6
B. Visual Comparisons of SAM and SAM-CLIP in Segmentation Tasks
Comparison on Instance Segmentation Table 1 provides a quantitative comparison of SAM and SAM-CLIP on two
instance segmentation datasets (COCO and LVIS), showing that SAM-CLIP maintains comparable performance to SAM.
15

--- PAGE 16 ---
To give readers a more intuitive understanding of the segmentation quality of SAM versus SAM-CLIP , we present two
examples in Figure 5. These examples demonstrate that, given the same geometric prompts (bounding box and point prompt),
the segmentation masks predicted by SAM and SAM-CLIP are quite similar, with slight differences. This suggests that the
segmentation quality of SAM-CLIP is indeed comparable to that of SAM.
Comparison on Semantic Segmentation Figure 3 illustrates the semantic segmentation outputs of SAM-CLIP , featuring
both CLIP-head segmentation predictions and SAM-head refined segmentation predictions. Specifically, the SAM-head
refinement utilizes the CLIP-head output and some auto-generated point prompts from this output. The same point prompts
are fed to SAM ViT-B, with its segmentation prediction shown in Figure 6. It is evident that SAM‚Äôs prediction typically
segments only a sub-part of the object indicated by the point prompts, instead of segmenting the entire semantic object class
(e.g., ‚Äúdog,‚Äù ‚Äúhorse,‚Äù ‚Äúhuman‚Äù). This indicates that the CLIP-head of SAM-CLIP is essential for semantic segmentation, as
it provides semantic understanding to the SAM-head of SAM-CLIP . In contrast, the point prompting approach used in SAM
[38] is insufficient for semantic segmentation. Furthermore, point prompting requires human-provided points, making it not
qualified for zero-shot semantic segmentation. In contrast, SAM-CLIP requires only text prompts for each object class (e.g.,
‚Äúdog,‚Äù ‚Äúhorse,‚Äù ‚Äúhuman‚Äù) to automatically generate semantic segmentation masks (the point prompts are auto-generated from
the CLIP-head output in our pipeline).
C. Inference Experiments
CLIP and SAM Tasks The inference process for zero-shot classification is identical to that of the original CLIP [12, 68].
The evaluation of zero-shot instance segmentation also exactly follows the protocol outlined in Kirillov et al. [38]. The image
resolutions for classification and instance segmentation tasks are set at 224px and 1024px, respectively.
Zero-Shot Semantic Segmentation For zero-shot semantic segmentation, we largely adhere to the practices outlined by
Ranasinghe et al. [69]. We insert the class names into 80 prompt templates created by Radford et al. [68] and obtain text em-
beddings using the text encoder. Next, we compute the cosine similarity between each text embedding and the corresponding
patch feature (the output of the CLIP head). The class with the highest cosine similarity is selected as the predicted class for
each patch. We then resize the patch class predictions to match the original image dimensions and calculate mIoU scores.
The evaluation resolution is maintained at 448px for fair comparison with previous methods.
Composing CLIP and SAM Heads To combine both CLIP and SAM heads for zero-shot semantic segmentation, we first
resize the image to 1024px and run the CLIP head to obtain mask predictions (i.e., logits) for each class. Subsequently, we
pass the mask prediction corresponding to each class to the prompt encoder, along with 1-3 auto-generated points. These
points are randomly sampled from pixels where the mask prediction logits exceed a specific threshold (for Pascal VOC, we
find that a threshold of 0.5 is generally sufficient). The output from the prompt encoder is then fed to the SAM head (i.e.,
mask decoder) along with the patch token outputs from the ViT backbone. Finally, the mask decoder produces fine-grained
mask prediction logits for each class, and we designate the class with the highest logit value as the predicted class for each
pixel.
C.1.SAM-CLIP vs. SAM+CLIP
One may wonder if it is possible to compose pretrained SAM and CLIP in a pipeline for zero-shot semantic segmentation,
and how the results compare with SAM-CLIP . We implemented the SAM+CLIP pipeline that passes segmentation masks
predicted by SAM ViT-B (in the segment-everthing mode) to CLIP ViT-B/16 (DataComp-1B) for class prediction. From
Table 7, one can clearly observe that the results on Pascal VOC reveal the unsatisfactory performance of the SAM+CLIP
pipeline, which we attribute primarily to SAM‚Äôs limited semantic understanding. SAM often segments parts of objects rather
than the whole, and CLIP struggles to classify these segmented parts. See visualizations in Figure 7.
Table 7. Comparison of SAM-CLIP vs. SAM+CLIP
SAM+CLIP SAM-CLIP (CLIP-Head) SAM-CLIP (Both Heads)
Pascal VOC (mIoU) 27.2 60.6 66.0
16

--- PAGE 17 ---
(a) Input image ‚Üí
 (b) SAM outputs ‚Üí
 (c) CLIP prediction
Figure 7. Visualization of the SAM+CLIP pipeline (see descriptions in Sec. C.1)
(a) Zero-Shot Accuracy (%)
 (b) Zero-Shot Instance Segmentation (mAP)
Figure 8. Wise-FT [86] to a CLIP-distilled SAM ViT-B model. The red dashed line marks the performance of the CLIP teacher model.
D. Weight Averaging
Weight averaging is a straightforward post-processing method proven to mitigate forgetting across a variety of fine-tuning
tasks. Specifically, Wise-FT [86] proposes linearly interpolating the pretrained and fine-tuned parameters using a coefficient
Œ±. In this study, we explore the application of Wise-FT in our setup. We focus exclusively on CLIP distillation applied to
SAM ViT-B (serving as the student model), with a CLIP ViT-B/16 model acting as the teacher model. The model is trained on
ImageNet-21k for 20 epochs. It is evident that the fine-tuned student model ( Œ±= 1) gains zero-shot classification capabilities
at the expense of forgetting its original zero-shot instance segmentation abilities. Upon applying Wise-FT to the fine-tuned
model, we observe an inherent tradeoff between learning and forgetting. Notably, no optimal point exists where both high
classification accuracy ( >60% on ImageNet) and a high mAP ( >35mAP on COCO) are achieved simultaneously.
E. Limitations
Our proposed method for merging existing foundational vision models may inherit the limitations of the original models.
Specifically, our approach might carry over limitations from both the original SAM and CLIP models, including biases in
data distribution. We have not assessed the robustness and fairness of our method in this work. Another potential limitation is
the model size/architecture of the base VFM (SAM in this paper), which must be adopted from an existing model. However,
we believe this should not be a practical limitation. The original SAM model offers several sizes/architectures (ViT-B/L/H).
Moreover, follow-up works, such as MobileSAM [96], could be adopted as the base model in our proposed method to achieve
a suitable final merged model. Additionally, our merged image encoder for the auxiliary model (CLIP in this case) requires
an additional head (the CLIP-Head here). In this work, this increases the overall size by approximately 25% compared to a
single ViT-B.
17
