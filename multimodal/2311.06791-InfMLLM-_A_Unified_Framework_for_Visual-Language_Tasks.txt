# 2311.06791.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.06791.pdf
# File size: 3403203 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
InfMLLM: A Unified Framework for Visual-Language Tasks
Qiang Zhou†Zhibin Wang†Wei Chu Yinghui Xu Hao Li∗Yuan Qi∗
{zhouqiang, zhibin.waz, chuwei, xuyinghui, qiyuan }@inftech.ai
lihao hank@163.com
Abstract
Large language models (LLMs) have proven their re-
markable versatility in handling a comprehensive range of
language-centric applications. To expand LLMs’ capabil-
ities to a broader spectrum of modal inputs, multimodal
large language models (MLLMs) have attracted growing in-
terest. This work delves into enabling LLMs to tackle more
vision-language-related tasks, particularly image caption-
ing, visual question answering (VQA,) and visual ground-
ing. To this end, we implemented a three-stage train-
ing scheme: starting with lightweight alignment pretrain-
ing, then moderate-weight multitask hybrid training, and
finally, LLM fine-tuning to improve instruction following
capability. Throughout the training process, the require-
ments on GPU memory gradually increase. To effectively
manage the number of visual embeddings passed to the
LLM while preserving their positional information, we in-
troduce a straightforward visual adapter module dubbed
pool-adapter. Our experiments demonstrate that preserv-
ing the positional information of visual embeddings through
the pool-adapter is particularly beneficial for tasks like vi-
sual grounding. We name our proposed approach InfM-
LLM and have evaluated it extensively on various bench-
mark datasets. Our results demonstrate that InfMLLM
achieves either state-of-the-art (SOTA) performance or per-
formance comparable to recent MLLMs. The code and
model will be made open-source at: https://github.
com/infly-ai/INF-MLLM .
1. Introduction
As interest in large language models (LLMs) grows, so does
the research community’s focus on multimodal large lan-
guage models (MLLMs). MLLMs are essential in develop-
ing versatile general-purpose assistants, as everyday inter-
actions involve information of various modalities, such as
voice, text, images, and videos. This work extends LLMs
to handle more vision-language-related tasks, including im-
†Equal contribution.
∗Corresponding author.
(a) InfMLLM-7B
(b) InfMLLM-7B-Chat
Figure 1. InfMLLM achieves state-of-the-art or similar perfor-
mance in various vision-language tasks.
age captioning, VQA, and visual grounding. Developing an
MLLM from scratch is a challenging task. It necessitates
substantial training resources and high-quality training data
in large quantities. Recent MLLMs [2, 5, 10, 23, 26] in-
stead utilize finetuning strategies, which leverage pretrained
LLMs and finetuning them using multimodal data.
Effectively finetuning LLMs to extend their capabilitiesarXiv:2311.06791v2  [cs.CV]  6 Dec 2023

--- PAGE 2 ---
to multimodality remains an ongoing research challenge.
This work conducts a progressive training strategy to im-
porve the training efficiency. In the initial stage, we employ
an extensive collection of lower-quality image-text pair data
to train a randomly initialized visual adapter. This adapter
aligns image features extracted from the pretrained im-
age encoder with text embeddings, allowing the pretrained
LLM to process them effectively. During the second phase,
we utilize publicly available high-quality datasets spanning
various vision-language tasks and finetune the model to ex-
tend its ability to handle various tasks. To minimize the
training overhead during this stage, we freeze the param-
eters of the LLM except for the QV projection weights.
To further improve the model’s ability to follow instruc-
tions effectively, we finetune the entire LLM using a limited
amount of instruction data in the final stage.
Visual adapters are commonly used to transform visual
features into aligned visual embeddings, which can then be
processed by a pretrained LLM. They also help to reduce
the number of visual embeddings, which is especially im-
portant for large images. Two well-known visual adapters
are Q-Former [23] and Perceiver [1], which use cross-
attention modules to condense a large set of visual features
into a small number of visual embeddings. While these
adapters are good at multimodal tasks like image caption-
ing and Visual Question Answering (VQA), they struggle
with tasks that involve spatial relationships, such as visual
grounding. In this work, we propose a new visual adapter
called pool-adapter. It pools image features into a fixed
number of visual features and then converts them into vi-
sual embeddings using a two-layer MLP. Pool-adapter is
simple, can be used with different image sizes, and effec-
tively retains the positional information of visual embed-
dings. This makes it particularly effective for tasks such as
visual grounding.
In summary, our main contributions are:
• We present a new MLLM framework named InfMLLM
that trains in three stages, each with a distinct focus: pool-
adapter alignment pretraining, multitask finetuning, and
instruction tuning.
• We introduce a straightforward pool-adapter to align ViT
image features with LLMs, preserving positional infor-
mation while reducing the number of image embeddings.
• We comprehensively evaluated InfMLLM on various
benchmarks, achieving either state-of-the-art (SOTA) per-
formance or performance comparable to recent MLLMs.
2. Related Work
In this section, we survey related research on large lan-
guage models (LLMs) and multimodal large language mod-
els (MLLMs).Large Language Models (LMMs). The landscape of
LLMs has evolved significantly, with several noteworthy
contributions shaping the field. Early models, such as GPT-
2 [37] and BERT [11], laid the foundation by demonstrat-
ing the potential of training on vast web-scale text datasets.
These models marked a breakthrough in Natural Language
Processing (NLP) and set the stage for subsequent advance-
ments. One prominent advancement is GPT-3 [3], an un-
precedented scale and complexity model. GPT-3 showcased
the capabilities of massive neural networks, boasting 175
billion parameters and excelling in diverse language tasks.
Its release has spurred interest in exploring model size limits
and has opened new avenues for understanding the potential
applications and challenges associated with such colossal
language models. Megatron-turing NLG [44], PaLM [9],
Gopher [39], Chinchilla [15], OPT [52], and BLOOM [40]
represent subsequent models that have further pushed the
boundaries of LLMs. These models vary in architecture,
training methodologies, and applications, contributing to a
rich tapestry of research in the domain of large language
models. The diversity of approaches reflects ongoing ef-
forts to optimize performance, efficiency, and generaliza-
tion across different linguistic tasks. Recent endeavors in
the field have concentrated on refining LLMs to better align
with human instructions and feedback. InstructGPT [35],
ChatGPT [33] and GPT4 [34] stand out as exemplars in this
regard. These models can engage in dynamic and contex-
tually rich conversations, respond adeptly to user prompts,
and even demonstrate proficiency in complex tasks such as
code generation. The emphasis on aligning LLMs with hu-
man interaction and instruction represents a crucial step to-
ward their practical deployment and integration into real-
world applications.
In this work, we investigate a model that is trained end-
to-end, with a LLM at its core, aiming to enhance its capa-
bilities in vision-language tasks.
Multimodal Large Language Models (MLLMs). Uti-
lizing the impressive generalization capabilities ingrained
in LMMs, researchers have undertaken comprehensive in-
vestigations to extend their functionalities into multimodal
domains. As illustrated by VisualGPT [4] and Frozen [46],
initial undertakings train visual encoders to represent each
image through continuous embeddings. These image em-
beddings were subsequently fed into pretrained language
models, enhancing visual language capabilities and focus-
ing on tasks like image captioning and visual question an-
swering. This seminal exploration laid the foundation for
subsequent advancements in visual language research, as
exemplified by notable studies such as Flamingo [1], BLIP-
2 [23] and Kosmos-1 [17]. Recent studies, such as Vision-
LLM [48], Kosmos-2 [36], RegionBLIP [53], Shikra [6],
Qwen-VL [2], and miniGPT-v2 [5], are actively broad-

--- PAGE 3 ---
ening the spectrum of vision-language tasks that MLLMs
can adeptly tackle. These investigations underscore that
MLLMs excel in conventional tasks like image captioning
and VQA, and exhibit robust visual grounding capability.
Specifically, these models generate textual representations
of bounding boxes through language models. Beyond di-
versifying the tasks within the purview of MLLMs, there
is a considerable body of ongoing research in the realm of
multimodal instruction tuning. Notable studies in this do-
main include LLaV A [28], InstructBLIP [10], Otter [21],
LLaV A-1.5 [26], and others.
The design of the model architecture and training
pipeline of MLLMs is still an open problem. In this work,
we propose an efficient model named InfMLLM, which per-
forms appreciatively on various vision-language tasks.
3. Method
This section presents a comprehensive explanation of the
architecture 3.1 and training pipeline 3.2 of InfMLLM.
3.1. Model Architecture
As depicted in Figure 2, InfMLLM primarily comprises
three components: the image encoder, the pool-adapter, and
the large language model (LLM).
Image encoder. The image encoder is responsible for the
transformation of raw image input into compact and in-
formative image features. The Vision Transformer [12]
(ViT) model’s streamlined architecture makes it particu-
larly adept at handling diverse multimodal data types, such
as images and videos. Pioneering work like CLIP [38]
further popularized ViT models by providing pretrained
models for image-to-text matching. As a result, ViT has
become the de facto image encoder of choice in many
MLLMs [2, 5, 26, 27]. In this work, we employ ViT-g/14
from EV A-CLIP [45]. Following the practices in BLIP-
2 [23], we discard the final layer of the ViT and utilize the
output features of the second last layer. Furthermore, we
remove the output features associated with the class token
and retain only the patch features.
Pool-adapter. LLMs are typically pretrained using pure
text corpora and lack the inherent capability to process im-
age features obtained from ViT models. To bridge the
gap between image features and text embeddings, the Q-
Former module was introduced in BLIP-2 [23]. It serves
as a pivotal intermediary, facilitating the connection be-
tween ViT and LLMs in the realm of visual understanding.
Nonetheless, our initial experiments exposed shortcomings
in the performance of Q-Former in tasks related to visual
grounding. This discrepancy may be attributed to the ten-
dency of Q-Former’s queried features to lose positional in-formation inherent in image features, a critical element for
tasks such as visual grounding. To maximize the reten-
tion of positional information in image features, we propose
the lightweight pool-adapter. Pool-adapter operates by ini-
tially pooling the image features, reducing them to a fixed
number. Subsequently, a two-layer Multilayer Perceptron
(MLP) is employed to effectively align these pooled fea-
tures with text embeddings, enhancing the model’s capacity
for multimodal tasks. Our ablation results in Table 5 and Ta-
ble 6 reveal that the number of visual embeddings generated
by the pool-adapter substantially influences the model’s per-
formance on vision-language tasks. Methods of maintain-
ing a reduced number of visual embeddings while enhanc-
ing model performance warrant further investigation, as a
reduction in visual embeddings can considerably improve
the model inference speed.
LLM. The LLM plays a central role in multimodal large
language models. They receive instructions as well as
aligned image features as input and then generate corre-
sponding answers. In this work, we use the open source
Vicuna-7B [8], a powerful large language model. Vicuna
was initially finetuned using pure text corpora, and fur-
ther finetuning it with multimodal data will benefit the
performance of MLLMs on multimodal tasks. For exam-
ple, Qwen-VL [2] finetunes the full LLM in the multitask
pretraining phase, and achieves excellent performance on
tasks such as captioning, visual question answering (VQA),
text-oriented VQA, and visual grounding. As LLMs get
larger, the full finetuning approach becomes less feasible.
LoRA [16] finetuning is a more efficient approach and is
adopted in miniGPT-v2 [5]. In our initial experiments,
the LoRA finetuned model performed at a disadvantage
on vision-language tasks compared to the fully finetuned
model. To achieve an optimal balance between performance
and training efficiency, we opt to finetune the complete QV
projection weights within the LLM while maintaining the
remaining parameters frozen, as illustrated in the multitask
finetuning stage in Figure 2. Note that during the final in-
struction tuning stage, we freeze the ViT model and finetune
the entire LLM model.
3.2. Data, Prompts and Training
In this subsection, we provide the datasets, prompts, and
training details for each stage of InfMLLM.
Stage-1 Pretraining. In this phase, we employ weakly la-
beled image-text pairs to conduct pretraining of the pool-
adapter module. The pretrained pool-adapter will align the
image features derived from ViT to text embeddings, en-
abling recognition by pretrained LLMs. Throughout this
stage, all parameters are freezed, except for those of pool-
adapter. The data utilized for pretraining consist of publicly

--- PAGE 4 ---
Tokenizer&embedding
PoolMLP
ViT
LargeLanguageModel(Vicuna-7B)
Tokenizer&embedding
PoolMLP
ViTLargeLanguageModel(Vicuna-7B)
Stage1: PretrainingStage2:Multitaskfinetuning
QV
Tokenizer&embedding
PoolMLP
ViTLargeLanguageModel(Vicuna-7B)
Stage3:InstructionTuning
Figure 2. The framework and training pipeline of InfMLLM. InfMLLM consists of three modules: ViT [12] model, pool-adapter, and large
language model (LLM). To extend the tasks processed by InfMLLM and improve the instruction-following capability, InfMLLM adopts a
three-stage training pipeline.
Prompt
Stage 1 <image ><ImageHere >A short image caption: A child holding a flowered umbrella and petting a yak.
Stage 2VQA <image ><ImageHere >Question: What color is the bedspread? Short answer: white
Captioning <image ><ImageHere >A short image caption: A bath tub sitting next to a sink in a bathroom.
Grounding <image ><ImageHere ><ref>woman in top picture on the left </ref><box>(0.074,0.142),(0.390,0.468) </box>
Stage 3 LLaV A-1.5 [26]
Table 1. Prompts employed in InfMLLM. It’s worth noting that “ <ImageHere >” is substituted with the aligned visual embeddings from
the pool-adapter, and the text in red represents the ground-truth label.
Dataset
Stage 1 CC3M, CC12M, LAION-115M
Stage 2VQA: VQAv2, OK-VQA, AOK-VQA, GQA, OCR-VQA, TextVQA
Captioning: COCO,TextCaps
Grounding: RefCOCO, RefCOCO+, RefCOCOg
Stage 3 LLaV A 1.5665k
Table 2. Training dataset used in stage-1 pretraining and stage-2
multitask finetuning.
available datasets, including CC3M, CC12M, and LAION-
115M [22]. During this phase, we employ a straightforward
prompt, where LLMs are prompted to generate a concise
caption based on the aligned image features, as exemplified
in Table 1.
Stage-2 Multitask finetuning. By aligning image fea-
tures to text embeddings that are compatible with LLM
processing, various visual language tasks such as im-
age captioning, visual question answering (VQA), and
referring expression grounding can be integrated into a
unified MLLM model. In this work, we collect pub-
licly available datasets, encompassing image captioning
datasets (COCO [25], TextCaps [42]), VQA datasets
(VQAv2 [14], OK-VQA [31], AOK-VQA [41], GQA [18],OCR-VQA [32], TextVQA [42]), and referring expres-
sion grounding datasets (RefCOCO [19], RefCOCO+, Re-
fCOCOg), and subsequently finetune a general MLLM
model. In this phase, we conduct finetuning on the ViT
model, the pool-adapter, and the QV projection weights
within the LLM. We employ uniform sampling of training
data from three tasks: image captioning, visual question an-
swering, and visual grounding. The prompts utilized for the
data associated with these three tasks exhibit slight varia-
tions, as outlined in Table 1. Specifically, we employ identi-
cal prompts for VQA and text-oriented VQA, without mak-
ing any distinctions. For visual grounidng tasks, we utilize
the LLM to directly generate the normalized coordinates of
the upper left and lower right corners of the bounding box.
Stage-3 Instruction Tuning. Instruction tuning [49] is
frequently a crucial step in improving LMMs’ zero-shot
performance and ability to follow instructions, and its ef-
fectiveness has been well-established. In order to im-
prove InfMLLM’s instruction-following capability, we fur-
ther finetune the model with instruction data. Recent ef-
forts [7, 10, 26, 27], whether done manually or leveraging
GPT-4, involve the transformation of public datasets into
usable instruction tuning datasets. In this work, we employ
the instructional data provided by LLaV A-1.5 [26]. During

--- PAGE 5 ---
the instruction tuning stage, all parameters, except those of
ViT, undergo the finetuning process.
4. Experiments
In this section, we delve into the details of model training
and perform a comprehensive quantitative evaluation of the
model on public benchmarks. Additionally, we provide a
qualitative demonstration with visual examples to provide a
more complete understanding of the model’s capabilities.
Training hyperparameters. The training hyperparame-
ters are outlined in Table 7. In particular, all stage models
undergo training using the AdamW optimizer and a cosine
decay learning rate scheduler. For the initial stage-1 pre-
training, the model is trained using 32 ×A800 GPUs with a
global batch size of 1024 and a maximum learning rate of
2e-4. The image resolution is set to 224 during this stage.
In the subsequent stage-2 multitask finetuning, the model
is trained with 32 ×A800 GPUs, employing a global batch
size of 512 and a maximum learning rate of 2e-5. The image
resolution is increased to 448 for this stage. Lastly, in the in-
struction tuning stage, the model is trained using 32 ×A800
GPUs for 1 epoch, with a global batch size of 128 and a
maximum learning rate of 2e-5. The image resolution re-
mains at 448 for this stage.
4.1. Visualization
To provide a more intuitive illustration of the vision-
language tasks accomplished by InfMLLM, we have cu-
rated visual examples showcased in Figure 3. The examples
reveal InfMLLM’s ability to seamlessly integrate tasks such
as image captioning, VQA, OCR-related VQA, and visual
grounding into a unified model. In one word, InfMLLM
effectively extends the task boundaries of LMMs. Addi-
tionally, we offer a selection of multi-turn conversations in
Figure 4 utilizing the InfMLLM-7B-Chat model to show-
case its proficiency in following instructions.
4.2. Benchmark Evaluation
Following multitask finetuning, InfMLLM becomes a ver-
satile model capable of addressing various vision and lan-
guage tasks, such as Visual Question Answering (VQA),
text-oriented VQA, and visual grounding. To comprehen-
sively assess the capabilities of InfMLLM, we undertake
evaluations across all these tasks. The specifics regarding
the datasets, splits, and evaluation metrics can be found
in Table 8. The results presented in Table 3 highlight the
strong performance of InfMLLM, achieving state-of-the-art
results across nearly all visual grounding and VQA tasks.
To assess the instruction-following capability of
InfMLLM-Chat after stage-3 finetuning, we evaluated
InfMLLM-Chat on various benchmarks. As illustrated inTable 4, InfMLLM-Chat exhibited superior performance
compared to other MLLMs employing language models of
similar size.
4.3. Ablation
In this section, we conduct ablation studies on the
InfMLLM-7B and InfMLLM-7B-Chat models.
Effect of number of visual embeddings. LLMs possess
the ability to extract valuable insights from diverse and
complex text sequences. Similarly, MLLMs are expected
to be capable of gathering pertinent information from vi-
sual embeddings to effectively answer various questions.
Yet, the optimal number of visual embeddings required for
MLLMs remains uncertain. In this section, we explore how
the number of visual embeddings affects the performance of
InfMLLM-7B and InfMLLM-7B-Chat. Specifically, we ad-
just the value of the parameter pin the pool-adapter, which
regulates the number of image embeddings (equivalent to
p2) fed into the LLMs. As demonstrated in Table 5, there
is a clear trend observed: the performance of InfMLLM-7B
in visual-language tasks improves with an increase in the
number of visual embeddings. Across eight benchmarks,
setting pto 8 yields a mean performance of 75.23. Increas-
ingpto 32 results in a mean performance boost to 77.67.
Furthermore, concatenating image embeddings from three
settings—8, 16, and 32—further increases the mean perfor-
mance to 78.09. The same conclusion is reached in the chat
model of InfMLLM-7B-Chat, as shown in Table 6.
Adjust the quantity of visual embeddings online While
increasing visual embeddings improves performance, it
comes at the cost of slowing down inference. There is a
frequent requirement for swift, initial results with fewer vi-
sual embeddings. The process of training multiple models
for different values of pcan be quite burdensome. How-
ever, our experiments in Table 9 demonstrate that when a
model is trained with p= 32 + 16 + 8 , utilizing p= 32
orp= 16 during the inference phase unexpectedly sustains
strong performance without substantial degradation.
5. Limitations
In our experiments, we observed that during the multitask
finetuning phase, there exists a degree of optimization con-
flict in individual tasks. The performance is notably influ-
enced by the choice of loss weights or data distribution ra-
tios for different tasks, necessitating meticulous finetuning.
Future research will delve into discovering more effective
solutions for multitask finetuning.

--- PAGE 6 ---
grounding:person bottom left
grounding:guy flopping around on the right
grounding:snow board second from left
Who is the author of this book?A:Marty JeromeWhat is the title of this book?TheCompleteRunner'sDay-by-DayLog2016CalendarWhat is the genre of this book?Calendars
dakotadigitalwhat is the brand of this camera?
what brand liquor is on the right?bowmore
what time is it?10:10
How do you know the train is working?smoke
What are the walls of this building made of?brick
What are the sheep doing in the pasture?graze
What kind of device is on top of the desk?keyboard
What color are the pants?red
a man wearing a knitted orange and white hat.
amanandagirlarepracticingmartialarts.
describetheimageinshort:
describetheimageinshort:
a police car driving on a beach next to a body of water.describetheimageinshort:
amanstandingnexttoabicyclewithfruitonit.
describetheimageinshort:
agroupofpeopleonastagewithguitars.
describetheimageinshort:
Figure 3. Some qualitative examples generated by InfMLLM-7B. The capabilities of InfMLLM encompass support for image captioning,
visual question answering (VQA), text-oriented VQA, and visual grounding.
Model LLMBenchamrks
VQAv2 OK-VQA GQA TextVQA OCR-VQARefCOCO
val / test-A / test-BRefCOCO+
val / test-A / test-BRefCOCOg
val / test
Flamingo-9B [1] - 44.7 - - - - - -
BLIP-2 [23] Vicuna-13B - 45.9 32.3 42.4 - - - -
InstructBLIP [10] Vicuna-13B - - 45.9 50.7 - - - -
OFA-L [47] - - - - - 79.96 / 83.67 / 76.39 68.29 / 76.00 / 61.75 67.57 / 67.58
VisionLLM-H [48] - - - - - - / 86.70 / - - -
mPLUG-DocOwl [50] LLaMA-7B - - - 52.6 - - - -
Shikra-7B [6] Vicuna-7B - - - - - 87.01 / 90.61 / 80.24 81.60 / 87.36 / 72.12 82.27 / 82.19
Shikra-13B [6] Vicuna-13B - 47.16 - - - 87.83 / 91.11 / 81.81 82.89 / 87.79 / 74.41 82.64 / 83.16
MiniGPT-v2-7B [5] LLaMA2-7B - 56.9 60.3 - - 88.69 / 91.65 / 85.33 79.97 / 85.12 / 74.45 84.44 / 84.66
QWen-VL-7B [2] Qwen-7B 79.5 58.6 59.3 63.8 75.7 89.36 / 92.26 / 85.34 83.12 / 88.25 / 77.21 85.58 / 85.48
LLaV A-1.5-7B [26] Vicuna-7B 78.5 - 62.0 58.2 -
InfMLLM-7B Vicuna-7B 81.95 61.33 63.15 68.02 73.52 - / 94.62 /89.30 - / 92.36 /81.83 - / 89.78
Table 3. Comparison with SoTA MLLMs across tasks such as visual question answering (VQA), text-oriented VQA, and visual grounding.
InfMLLM-7B achieves the best performance on most benchmarks, and ranks the second on the other. . Benchmark names are abbreviated
due to space limits. VQAv2 [14]; OK-VQA [31]; GQA [18]; TextVQA [43]; OCR-VQA [32]; RefCOCO [19]; RefCOCO+ [19]; Ref-
COCOg [19].

--- PAGE 7 ---
Why this image is interesting ?Theimageisinterestingbecauseitfeaturesacatsittingonawindowsill,lookingoutofthewindow,whileabirdisvisibleoutside.Thiscreatesacaptivatingsceneasitshowcasestheinteractionbetweenthetwoanimals,whichareoftenconsiderednaturalenemies.Thecat'scuriosityandthebird'spresenceoutsidethewindowcreateasenseofintrigueandwonder,makingtheimageengagingandvisuallyappealing.
What is the cat watching ?
The cat is watching a bird outside the window.What is the scientific name of the bird in the picture?The bird in the picture is a parakeet.
How is the weather outside?The weather outside is sunny, as the cat is sitting by the window and enjoying the sunlight.what season is it now ?Itisnotpossibletodefinitivelydeterminetheseasonfromtheimagealone.However,sincethecatissittingbythewindowandenjoyingthesunlight,itcouldbeinferredthatitmightbeawarmerseason,suchasspringorsummer.How many person are there in the picture?Therearethreepeopleinthepicture:thebatter,thecatcher,andtheumpire.图⽚中⼈们在玩的是什么运动？图⽚中⼈们在玩的是美式棒球。Can you tell which team the batter is from?Yes,thebatterisfromtheOaklandA'steam.
What equipment can be seen used by the catcher and the umpire?Thecatcheriswearingabaseballglove,whichisusedtocatchtheballifthebattermisses.Theumpireisalsowearingabaseballglove,whichisusedtomakeaccuratecallsduringthegame.What part of the baseball field is shown in the image?The image shows the home plate area of the baseball field, where the batter, catcher, and umpire are positioned.How is the weather at the game, as suggested by the picture?The weather at the game appears to be sunny, as the image shows a sunny day at the baseball game.
Figure 4. Some qualitative examples generated by InfMLLM-7B-Chat.
Method LLM VQAv2 GQA SQAIVQATPOPE MME MMB MMBCNSEED MM-Vet
BLIP-2 Vicuna-13B 41.0 41 61 42.5 85.3 1294 – – 46.4 22.4
InstructBLIP Vicuna-7B - 49.2 60.5 50.1 - - 36 23.7 53.4 26.2
InstructBLIP Vicuna-13B - 49.5 63.1 50.7 78.9 1212 - - - 25.6
Shikra Vicuna-13B 77.4 - - - - - 58.8 - - -
IDEFICS-9B LLaMA-7B 50.9 38.4 – 25.9 - - 48.2 25.2 - -
IDEFICS-80B LLaMA-65B 60.0 45.2 – 30.9 – – 54.5 38.1 - -
Qwen-VL-Chat Qwen-7B 78.2 57.5 68.2 61.5 - 1487 60.6 56.7 58.2 -
LLaV A-1.5-7B Vicuna-7B 78.5 62.0 66.8 58.2 85.9 1510 64.3 58.3 58.6 30.5
InfMLLM-7B-Chat Vicuna-7B 82.3 65.1 68.7 64.1 86.5 1490 65.6 59.1 61.7 33.4
Table 4. Comparison with SoTA methods on 11 benchmarks. InfMLLM-7B-Chat achieves the best performance on most benchmarks,
and ranks the second on the other. Benchmark names are abbreviated due to space limits. VQAv2 [14]; GQA [18]; SQAI: ScienceQA-
IMG [30]; VQAT: TextVQA [43]; POPE [24]; MME [13]; MMB: MMBench [29]; MMBCN: MMBench-Chinese [29]; SEED: SEED-
Bench [20]; MM-Vet [51].

--- PAGE 8 ---
pBenchmarksMean ( ↑)
OKVQA GQA VQAv2 TextVQA OCR-VQA RefCOCO testA RefCOCO+ testA RefCOCOg test
8 59.51 61.70 78.75 63.78 70.81 92.59 88.71 86.04 75.23
16 59.22 61.75 79.85 62.19 71.26 93.49 89.76 87.20 75.59
32 60.26 63.53 81.54 66.89 72.46 94.87 92.35 89.50 77.67
8+16+32 61.33 63.15 81.95 68.02 73.52 94.62 92.36 89.78 78.09
Table 5. Ablation studies on the configuration of parameter pin pool-adapter. The model is InfMLLM-7B after stage-2 multitask finetuning.
p VQAv2 GQA SQAIVQATPOPE MME MMB MMBCNSEED MM-Vet
16 80.16±0.02 63.69 ±0.01 62.42 ±0.39 61.58 ±0.21 84.80±0.08 1434 ±9 64.45 ±0.17 54.46 ±0.01 59.36 ±0.08 32.90 ±1.50
32 81.50±0.17 64.36±0.40 66.63±0.99 63.57 ±1.72 86.71±1.13 1494±12 65.21 ±0.25 56.86 ±1.11 61.26 ±0.14 33.35 ±0.05
8+16+32 81.99±0.08 64.13 ±0.16 66.01 ±0.86 65.26±0.14 85.86±0.30 1476 ±7 65.73±0.09 57.94±0.30 61.77±0.02 34.65±0.05
Table 6. Ablation studies on the configuration of parameter pin pool-adapter. The model is InfMLLM-7B-Chat after stage-3 instruction
tuning. The chat model’s performance shows considerable variation across certain benchmarks; therefore, to ensure reliability, each
experimental setup was executed twice. The average performance and the standard deviation for each configuration are provided for a clear
comparison.
Hyperparameters Stage-1 Stage-2 Stage-3
batch size 1024 512 128
lr 2e-4 2e-5 2e-5
lr scheduler Cosine decay
optimizer AdamW
image resolution 224 448 448
DeepSpeed stage 2 2 2
Table 7. Training hyperparameters of InfMLLM.
Task Dataset Evaluation split Metric
Image Caption Flickr30K karpathy-test CIDEr( ↑)
General VQAOKVQA val VQA Score ( ↑)
GQA test-balanced EM(↑)
VQAv2 test-dev VQA Score ( ↑)
Text-oriented VQATextVQA val VQA Score ( ↑)
OCRVQA test EM(↑)
Refer Expression
ComprehensionRefCOCO testA & testB Accuracy( ↑)
RefCOCO+ testA & testB Accuracy( ↑)
RefCOCOg test Accuracy( ↑)
Table 8. Summary of benchmark datasets employed for evaluating
the stage-2 multitask finetuning model.
p GQA SQA-I TextVQA POPE MME SEED MM-Vet
16 61.3 66.2 55.9 82.5 1365 57.5
32 64.5 67.0 62.4 85.0 1444 60.9 36.1
32+16+8 65.1 68.7 64.1 86.5 1490 61.7 33.4
Table 9. Ablation of the impact of online adjustments to visual
embeddings. The model undergoes training with pconfigured as
“32+16+8” and is subsequently evaluated with varying psettings
during inference.
6. Conclusion
In this work, we introduce a novel MultiModal Large
Language Model framework named InfMLLM. Utilizinga straightforward pool-adapter, InfMLLM dynamically ad-
justs the number of image embeddings while retaining cru-
cial positional information. In contrast to other MLLMs,
InfMLLM attains state-of-the-art performance in visual
grounding and visual question answering, while also de-
livering competitive results in image captioning and text-
oriented VQA tasks. We will release the associated code
and models as open-source. We anticipate that InfMLLM
will contribute to the advancement of MLLM-related re-
search.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning,
2022. 2, 6
[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A versatile vision-language model for un-
derstanding, localization, text reading, and beyond. arXiv
preprint arXiv:2308.12966 , 2023. 1, 2, 3, 6
[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-

--- PAGE 9 ---
ford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners, 2020. 2
[4] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed El-
hoseiny. Visualgpt: Data-efficient adaptation of pretrained
language models for image captioning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 18030–18040, 2022. 2
[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
Minigpt-v2: large language model as a unified interface
for vision-language multi-task learning. arXiv preprint
arXiv:2310.09478 , 2023. 1, 2, 3, 6
[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng
Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s
referential dialogue magic, 2023. 2, 6
[7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui
He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:
Improving large multi-modal models with better captions,
2023. 4
[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, 2023. 3
[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, and
Maarten Bosma et al. Palm: Scaling language modeling with
pathways, 2022. 2
[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning,
2023. 1, 3, 4, 6
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional trans-
formers for language understanding, 2019. 2
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. ICLR , 2021. 3, 4
[13] Chaoyou Fu, Peixian Chen, Yunhang Shen, and Yulei Qin et
al. Mme: A comprehensive evaluation benchmark for multi-
modal large language models, 2023. 7
[14] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the V in VQA matter: El-
evating the role of image understanding in visual question
answering. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA,
July 21-26, 2017 , pages 6325–6334. IEEE Computer Soci-
ety, 2017. 4, 6, 7
[15] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, and
Elena Buchatskaya et al. Training compute-optimal large
language models, 2022. 2
[16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.Lora: Low-rank adaptation of large language models, 2021.
3
[17] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, and
Saksham Singhal et al. Language is not all you need: Align-
ing perception with language models, 2023. 2
[18] Drew A. Hudson and Christopher D. Manning. GQA: A new
dataset for real-world visual reasoning and compositional
question answering. In IEEE Conference on Computer Vi-
sion and Pattern Recognition, CVPR 2019, Long Beach, CA,
USA, June 16-20, 2019 , pages 6700–6709. Computer Vision
Foundation / IEEE, 2019. 4, 6, 7
[19] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara L. Berg. Referitgame: Referring to objects in pho-
tographs of natural scenes. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language Process-
ing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A
meeting of SIGDAT, a Special Interest Group of the ACL ,
pages 787–798. ACL, 2014. 4, 6
[20] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking
multimodal llms with generative comprehension. CoRR ,
abs/2307.16125, 2023. 7
[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model
with in-context instruction tuning, 2023. 3
[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML ,
2022. 4
[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML ,
2023. 1, 2, 3, 6
[24] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucination in
large vision-language models. CoRR , abs/2305.10355, 2023.
7
[25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C. Lawrence Zitnick. Microsoft COCO: common objects
in context. In Computer Vision - ECCV 2014 - 13th Eu-
ropean Conference, Zurich, Switzerland, September 6-12,
2014, Proceedings, Part V , pages 740–755. Springer, 2014.
4
[26] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning, 2023. 1,
3, 4, 6
[27] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning, 2023. 3, 4
[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning, 2023. 3
[29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your
multi-modal model an all-around player?, 2023. 7
[30] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and

--- PAGE 10 ---
Ashwin Kalyan. Learn to explain: Multimodal reasoning
via thought chains for science question answering. In The
36th Conference on Neural Information Processing Systems
(NeurIPS) , 2022. 7
[31] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. OK-VQA: A visual question answering
benchmark requiring external knowledge. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019 , pages 3195–
3204. Computer Vision Foundation / IEEE, 2019. 4, 6
[32] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. OCR-VQA: visual question answer-
ing by reading text in images. In 2019 International Confer-
ence on Document Analysis and Recognition, ICDAR 2019,
Sydney, Australia, September 20-25, 2019 , pages 947–952.
IEEE, 2019. 4, 6
[33] OpenAI. Introducing chatgpt. https://openai.com/
blog/chatgpt , 2022. 2
[34] OpenAI. Gpt-4 technical report, 2023. 2
[35] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, and Car-
roll L. Wainwright et al. Training language models to follow
instructions with human feedback, 2022. 2
[36] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding
multimodal large language models to the world, 2023. 2
[37] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
Amodei, and Ilya Sutskever. Language models are unsuper-
vised multitask learners. 2019. 2
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever. Learning transferable visual
models from natural language supervision, 2021. 3
[39] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Milli-
can, Jordan Hoffmann, Francis Song, and John Aslanides el
al. Scaling language models: Methods, analysis & insights
from training gopher, 2022. 2
[40] Teven Le Scao, Angela Fan, Christopher Akiki, and El-
lie Pavlick et al. Bloom: A 176b-parameter open-access
multilingual language model, 2023. 2
[41] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A
benchmark for visual question answering using world knowl-
edge. In Computer Vision - ECCV 2022 - 17th European
Conference, Tel Aviv, Israel, October 23-27, 2022, Proceed-
ings, Part VIII , pages 146–162. Springer, 2022. 4
[42] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
Amanpreet Singh. Textcaps: a dataset for image caption-
ingwith reading comprehension. 2020. 4
[43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards VQA models that can read. In IEEE
Conference on Computer Vision and Pattern Recognition,
CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages
8317–8326. Computer Vision Foundation / IEEE, 2019. 6, 7
[44] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick
LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu,Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti,
Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie
Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He,
Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.
Using deepspeed and megatron to train megatron-turing nlg
530b, a large-scale generative language model, 2022. 2
[45] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. Eva-clip: Improved training techniques for clip at scale.
arXiv preprint arXiv:2303.15389 , 2023. 3
[46] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali
Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models, 2021. 2
[47] Peng Wang, An Yang, Rui Men, Junyang Lin, and Shuai Bai
et al. Ofa: Unifying architectures, tasks, and modalities
through a simple sequence-to-sequence learning framework,
2022. 6
[48] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu
Qiao, and Jifeng Dai. Visionllm: Large language model is
also an open-ended decoder for vision-centric tasks, 2023.
2, 6
[49] Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and
Quoc V . Le. Finetuned language models are zero-shot learn-
ers, 2022. 4
[50] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, and
Ming Yan et al. mplug-docowl: Modularized multimodal
large language model for document understanding, 2023. 6
[51] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
Mm-vet: Evaluating large multimodal models for integrated
capabilities. CoRR , abs/2308.02490, 2023. 7
[52] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, and Shuohui Chen et al. Opt: Open pre-trained
transformer language models, 2022. 2
[53] Qiang Zhou, Chaohui Yu, Shaofeng Zhang, Sitong Wu,
Zhibing Wang, and Fan Wang. Regionblip: A unified multi-
modal pre-training framework for holistic and regional com-
prehension, 2023. 2
