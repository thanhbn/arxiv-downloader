# JARVIS-1: Tác nhân đa nhiệm thế giới mở với Mô hình Ngôn ngữ Đa phương thức Tăng cường Bộ nhớ

Zihao Wang1, Shaofei Cai1, Anji Liu2, Yonggang Jin3, Jinbing Hou3, Bowei Zhang1, Haowei Lin1,
Zhaofeng He3, Zilong Zheng4, Yaodong Yang1, Xiaojian Ma4và Yitao Liang1
1PKU,2UCLA,3BUPT,4BIGAI, Tất cả tác giả đều thuộc Team CraftJarvis,

Đạt được khả năng lập kế hoạch và điều khiển giống con người với quan sát đa phương thức trong thế giới mở là một cột mốc quan trọng cho các tác nhân tổng quát có chức năng hơn. Các phương pháp hiện tại có thể xử lý một số tác vụ dài hạn trong thế giới mở. Tuy nhiên, chúng vẫn gặp khó khăn khi số lượng tác vụ thế giới mở có thể là vô hạn và thiếu khả năng tăng cường tiến bộ hoàn thành tác vụ theo thời gian chơi game. Chúng tôi giới thiệu JARVIS-1, một tác nhân thế giới mở có thể nhận thức đầu vào đa phương thức (quan sát thị giác và hướng dẫn của con người), tạo ra các kế hoạch phức tạp, và thực hiện điều khiển thể hiện, tất cả trong vũ trụ Minecraft thế giới mở phổ biến nhưng đầy thách thức. Cụ thể, chúng tôi phát triển JARVIS-1 dựa trên các mô hình ngôn ngữ đa phương thức được huấn luyện trước, ánh xạ quan sát thị giác và hướng dẫn văn bản thành kế hoạch. Các kế hoạch cuối cùng sẽ được gửi đến các bộ điều khiển có điều kiện mục tiêu. Chúng tôi trang bị JARVIS-1 với bộ nhớ đa phương thức, tạo thuận lợi cho việc lập kế hoạch sử dụng cả kiến thức được huấn luyện trước và kinh nghiệm sinh tồn game thực tế của nó. JARVIS-1 là tác nhân tổng quát nhất hiện tại trong Minecraft, có khả năng hoàn thành hơn 200 tác vụ khác nhau sử dụng không gian điều khiển và quan sát tương tự con người. Các tác vụ này dao động từ tác vụ ngắn hạn, ví dụ: "chặt cây" đến tác vụ dài hạn, ví dụ: "lấy cuốc kim cương". JARVIS-1 thể hiện hiệu suất xuất sắc trong các tác vụ ngắn hạn, đạt hiệu suất gần như hoàn hảo. Trong tác vụ dài hạn cổ điển ObtainDiamondPickaxe, JARVIS-1 vượt qua độ tin cậy của các tác nhân tiên tiến hiện tại gấp 5 lần và có thể hoàn thành thành công các tác vụ dài hạn và thử thách hơn. Trang dự án có sẵn tại craftjarvis.org/JARVIS-1.

## 1. Giới thiệu

Tạo ra các tác nhân tinh vi có thể hoàn thành vô số tác vụ trong các lĩnh vực phức tạp vẫn là một cột mốc quan trọng hướng tới trí tuệ nhân tạo có khả năng tổng quát (Alayrac et al., 2022; Brohan et al., 2022a; Brown et al., 2020; Reed et al., 2022; Zhao et al., 2023). Những tiến bộ gần đây đã cho thấy xu hướng sử dụng kiến trúc thực thi mục tiêu phân cấp (Huang et al., 2022a,b; Wang et al., 2023b), và tận dụng các mô hình ngôn ngữ lớn (LLM) làm bộ lập kế hoạch cấp cao để tạo ra các kế hoạch hành động cuối cùng được thực thi bởi các bộ điều khiển theo hướng dẫn cấp thấp. Mặc dù tiến bộ thành quả mà chúng đã mang lại trong nhiều lĩnh vực robot (Huang et al., 2022b) và thậm chí các môi trường thế giới mở như Minecraft (Fan et al., 2022; Guss et al., 2019b), các tác nhân ngày nay được xây dựng với những phương pháp này vẫn đang gặp khó khăn với ba vấn đề chính: 1) nhận thức thế giới từ quan sát cảm giác đa phương thức, như hình ảnh, video ngoài hướng dẫn ngôn ngữ tự nhiên và phản hồi để lập kế hoạch; Điều này chủ yếu do sự bất lực của các bộ lập kế hoạch dựa trên LLM trong việc xử lý dữ liệu đa phương thức (Huang et al., 2022a; Yao et al., 2022); 2) thực hiện lập kế hoạch dài hạn nhất quán và chính xác. Điều này đòi hỏi các cuộc đối thoại đa vòng, thâm dụng kiến thức và lý luận, vẫn là những thách thức lớn đối với LLM (Huang et al., 2022b); 3) học hỏi và phát triển theo cách học suốt đời. Điều này kêu gọi nhu cầu cho các tác nhân đề xuất tác vụ riêng của họ và tự cải thiện. Giải quyết những vấn đề này sẽ giải phóng toàn bộ tiềm năng lập kế hoạch của các tác nhân dựa trên LLM, và đẩy nhanh việc phát triển các tác nhân tổng quát hơn.

Trong công trình này, chúng tôi giới thiệu JARVIS-1, một tác nhân hoàn toàn mới có thể tạo ra các kế hoạch một cách mạnh mẽ cho các tác vụ dài hạn từ đầu vào đa phương thức của người dùng và môi trường, và dịch chúng thành điều khiển vận động trong Minecraft, một testbed thế giới mở phổ biến nhưng đầy thách thức cho các tác nhân tổng quát. Cụ thể, chúng tôi kết nối một mô hình nền tảng đa phương thức MineCLIP (Fan et al., 2022) và một LLM (Brown et al., 2020) với nhau, mô hình ngôn ngữ đa phương thức (MLM) kết quả cho phép tác nhân của chúng tôi hiểu rõ hơn tác vụ, tình huống và phản hồi môi trường. Để tăng cường thêm tính đúng đắn và nhất quán của việc lập kế hoạch, đặc biệt là đối với các tác vụ dài hạn, chúng tôi đề xuất tăng cường tác nhân với bộ nhớ đa phương thức, lưu trữ cả kịch bản và kế hoạch thực tế của các kinh nghiệm lập kế hoạch thành công trong quá khứ. Bằng cách truy xuất các mục nhớ liên quan, kỹ năng lập kế hoạch của tác nhân dựa trên MLM của chúng tôi có thể được tăng cường từ tương tác riêng của tác nhân với môi trường theo cách trong ngữ cảnh. Cuối cùng, JARVIS-1 có thể phát triển xuyên suốt gameplay bằng cách đề xuất tác vụ của riêng mình (tức là tự hướng dẫn) như một phương tiện khám phá và lưu các kinh nghiệm thu được trong bộ nhớ đa phương thức, do đó tạo thuận lợi cho lý luận và lập kế hoạch tốt hơn. Khả năng tự cải thiện này khơi dậy tiềm năng của nó cho mức độ tự chủ cao hơn.

Các đánh giá chính của chúng tôi được thực hiện trong Minecraft, với hơn 200 tác vụ được chọn từ Minecraft Universe Benchmark (Lin et al., 2023a), không có minh họa nào được cung cấp. Các tác vụ bao gồm một phổ rộng từ giai đoạn đầu game (ví dụ: ObtainCraftingTable) đến các tác vụ dài hạn trung gian và thậm chí thử thách (ví dụ: ObtainDiamondPickaxe). Một cái nhìn thoáng qua về những gì JARVIS-1 có thể đạt được có thể tìm thấy trong Hình 1. JARVIS-1 thể hiện hiệu suất mạnh mẽ trên các tác vụ này, đại diện cho sự gia tăng lên đến 5× so với các kỷ lục trước đó. Phân tích loại bỏ của chúng tôi sau đó cung cấp một tài khoản chi tiết về cách JARVIS-1 tiếp cận tiến bộ đáng kể này và trở thành tác nhân đầu tiên có thể lấy được cuốc kim cương một cách mạnh mẽ với tỷ lệ thành công lên đến 12,5%. Điều thậm chí còn đáng ngạc nhiên hơn là, không cần huấn luyện bổ sung, JARVIS-1 thể hiện sự gia tăng liên tục về hiệu suất khi thời gian game tăng lên trong các tác vụ dài hạn. Hơn nữa, JARVIS-1 đã chứng minh tiềm năng tự cải thiện trong một thí nghiệm học suốt đời khám phá, nơi nó cần đề xuất tác vụ để tiến bộ khám phá thế giới, thu thập kinh nghiệm, và mài giũa kỹ năng lập kế hoạch sử dụng những kinh nghiệm này được lưu trữ trong bộ nhớ đa phương thức.

Tóm lại, JARVIS-1 dẫn dắt nỗ lực hướng tới một tác nhân đa nhiệm và tự chủ giống con người trong một môi trường thế giới mở, thể hiện như Minecraft. Chúng tôi muốn chia sẻ những điểm chính mà chúng tôi đã học được trong quá trình phát triển của nó như sau:

• Từ LLM đến MLM. Khả năng nhận thức đầu vào cảm giác đa phương thức là rất quan trọng cho việc lập kế hoạch trong một thế giới động và mở. JARVIS-1 cho phép điều này bằng cách kết nối một mô hình nền tảng đa phương thức với một LLM. So với LLM "mù quáng" tạo ra kế hoạch, MLM có thể hiểu tự nhiên tình huống hiện tại và lập kế hoạch phù hợp. Hơn nữa, phản hồi môi trường phong phú có thể được thu thập thông qua nhận thức đa phương thức, do đó giúp việc tự kiểm tra và tự giải thích của bộ lập kế hoạch phát hiện và sửa các lỗi có thể có trong kế hoạch, cho phép lập kế hoạch tương tác mạnh mẽ hơn.

• Bộ nhớ đa phương thức. Nghiên cứu ban đầu đã gợi ý vai trò quan trọng mà các cơ chế bộ nhớ có thể phục vụ trong hoạt động của các tác nhân tổng quát. Bằng cách trang bị JARVIS-1 với bộ nhớ đa phương thức, chúng tôi hiệu quả cho phép nó lập kế hoạch với cả kiến thức được huấn luyện trước và kinh nghiệm thực tế của nó trong thế giới, do đó mang lại cải thiện đáng kể cho tính đúng đắn và nhất quán của việc lập kế hoạch. So với các tác nhân RL hoặc lập kế hoạch chuẩn với khám phá, không cần cập nhật mô hình bổ sung vì MLM trong JARVIS-1 làm cho việc tận dụng những kinh nghiệm này theo cách trong ngữ cảnh trở nên khả thi.

• Tự hướng dẫn và tự cải thiện. Một dấu hiệu của các tác nhân tổng quát là khả năng chủ động thu thập kinh nghiệm mới và liên tục cải thiện bản thân. Chúng tôi đã chứng minh cách JARVIS-1 hiệu quả di chuyển qua môi trường bằng cách thực thi các tác vụ được tạo ra tự động thông qua cơ chế tự hướng dẫn của nó. Với bộ nhớ đa phương thức kết hợp với kinh nghiệm từ các cuộc khám phá, chúng tôi đã quan sát thấy sự cải thiện nhất quán, đặc biệt là trong việc hoàn thành các tác vụ phức tạp hơn. Cuối cùng, khía cạnh học tự chủ này trong JARVIS-1 biểu thị một bước tiến hóa hướng tới các tác nhân tổng quát có thể học, thích ứng và cải thiện theo thời gian với sự can thiệp bên ngoài tối thiểu.

## 2. Thách thức cho Tác nhân Thế giới Mở

So với các kịch bản chuẩn với quy mô tương đối nhỏ, động lực đơn giản và tác vụ hạn chế, các môi trường thế giới mở đặt ra những thách thức đáng kể cho việc xây dựng các tác nhân có thể hoàn thành một bộ tác vụ đa dạng (Cai et al., 2023a,b; Fan et al., 2022; Guss et al., 2019a, 2021; Kanervisto et al., 2022; Wang et al., 2023b). Trong phần này, chúng tôi sẽ xem xét ba thách thức chính mà chúng tôi đã xác định trong quá trình phát triển JARVIS-1.

### 2.1. Thách thức I: Lập kế hoạch Nhận thức Tình huống

Trong một thế giới mở, có thể có nhiều con đường khả thi hướng tới một mục tiêu thế giới mở. Tuy nhiên, không phải tất cả chúng đều hợp lý hoặc hiệu quả như nhau trong một tình huống nhất định (vị trí, trạng thái kho đồ, v.v.). Ví dụ, xây giường có thể được thực hiện thông qua việc thu thập len từ cừu, săn nhện để lấy dây, hoặc giao dịch với dân làng. Tùy thuộc vào vị trí hiện tại và sự gần gũi với các đối tượng này, một số lựa chọn có thể khả thi và hiệu quả hơn những lựa chọn khác. Hơn nữa, tình huống riêng của tác nhân cũng có thể thay đổi xuyên suốt tập phim, ví dụ: thay đổi ngày và đêm, điều kiện thời tiết (mang lại các loại nguy hiểm khác nhau), và sử dụng công cụ (nó có thể bị hỏng). Để làm điều này, kế hoạch cần được cập nhật liên tục dựa trên tình huống hiện tại. Hình 2 (trái) cho thấy rằng khi cố gắng tác vụ "ObtainDiamondPickaxe" với một bộ lập kế hoạch dựa trên GPT tạo ra kế hoạch chỉ ở đầu mà không nhìn vào tình huống hiện tại, tác nhân đã thất bại trong việc hoàn thành tác vụ trái ngược với người chơi và JARVIS-1, thực hiện lập kế hoạch nhận thức tình huống theo thời gian. Chúng tôi đã quan sát thấy rằng nhiều thất bại từ điều này được quy cho sự bất lực của tác nhân trong việc thích ứng với các tình huống thay đổi bao gồm bước vào một biome mới, công cụ đang sử dụng trở nên hỏng, v.v.

### 2.2. Thách thức II: Độ phức tạp Tác vụ

Thách thức thứ hai đến từ độ phức tạp tác vụ cao hơn trong các môi trường thế giới mở. Do sự phong phú của địa hình, đối tượng và không gian hành động, các tác vụ trong các lĩnh vực thế giới mở thường đòi hỏi chân trời lập kế hoạch dài đáng kể cũng như độ chính xác và precision tốt. Ví dụ, tác vụ ObtainEnchantingTable bao gồm hơn 20 tiểu mục tiêu khác nhau và do đó đòi hỏi các bước lý luận dài hơn đáng kể. Đồng thời, nhiều tiểu mục tiêu này phải được đạt chính xác với tên đối tượng chính xác, số lượng và điều kiện tiên quyết, ví dụ: khai thác 3 obsidian với cuốc kim cương, chế tạo 1 cuốc kim cương từ 3 kim cương và 2 que; nếu không, các tiểu mục tiêu tiếp theo sẽ không được thực thi do điều kiện tiên quyết không được đáp ứng. Để giải quyết điều này, chúng ta có thể tham khảo một số phương pháp trong lý luận LLM, ví dụ: tự debug (Chen et al., 2023) và biến việc lập kế hoạch thành một cách tương tác. Trong Hình 2 (Giữa), chúng tôi đã cho thấy rằng khi độ phức tạp của tác vụ tăng lên, JARVIS-1 của chúng tôi, sử dụng lập kế hoạch tương tác (Wang et al., 2023b) để giảm thiểu các vấn đề đã nêu (chi tiết có thể tìm thấy trong tiểu mục 3.2), gây ra lợi thế đáng kể hơn so với bộ lập kế hoạch cơ sở (GPT).

### 2.3. Thách thức III: Học suốt đời

Cuối cùng, việc là thế giới mở thường ngụ ý cung cấp một số lượng vô hạn các tác vụ. Rõ ràng, rất khó để một tác nhân thành thạo tất cả các tác vụ hoặc tổng quát hóa cho các tác vụ tùy ý mà không cần học thêm. Để làm điều này, các tác nhân trong thế giới mở nên có khả năng học các tác vụ mới trong khi hoàn thành các tác vụ hiện có, tức là học suốt đời. Hơn nữa, vì nhiều tác nhân thế giới mở sử dụng các mô hình lớn (Wang et al., 2023a,b; Yuan et al., 2023; Zhu et al., 2023), việc học dựa trên gradient chuẩn có thể cực kỳ không hiệu quả với số lượng tác vụ và kinh nghiệm mới cần học. JARVIS-1 dựa trên MLM của chúng tôi giải quyết điều này bằng cách áp dụng một bộ nhớ để lưu tất cả kinh nghiệm về các tác vụ trong quá khứ. Bằng cách truy xuất các mục nhớ liên quan đến tác vụ mới đến và đặt chúng vào ngữ cảnh như một tham chiếu, JARVIS-1 có thể tích lũy thêm kinh nghiệm khi game tiếp tục và tăng cường kỹ năng lập kế hoạch riêng của mình mà không cần cập nhật gradient. Như được minh họa trong Hình 2 (Phải), ví dụ, cả ObtainDiamondPickaxe và ObtainDiamondAxe đều yêu cầu thu thập các vật liệu gần như giống hệt nhau. Do đó, chúng có thể giúp nhau bằng cách sử dụng kinh nghiệm từ tác vụ khác. So với việc hoàn thành những tác vụ thử thách này mà không có kinh nghiệm trước đó nào, học suốt đời dựa trên bộ nhớ trong ngữ cảnh trong JARVIS-1 có thể mang lại lợi thế đáng kể.

## 3. Tác nhân Đa nhiệm với MLM Tăng cường Bộ nhớ

Phần này chi tiết kiến trúc của tác nhân JARVIS-1 được đề xuất. Chúng tôi bắt đầu với tổng quan về thiết kế tác nhân modular trong tiểu mục 3.1. Tiếp theo, chúng tôi trình bày chi tiết cách triển khai một sơ đồ lập kế hoạch tương tác với mô hình ngôn ngữ đa phương thức, giúp với các kế hoạch chính xác hơn, đặc biệt là đối với các tác vụ phức tạp và dài hạn trong tiểu mục 3.2. Cuối cùng, chúng tôi cho thấy cách tăng cường khung lập kế hoạch này với bộ nhớ đa phương thức để cho phép JARVIS-1 tăng cường kỹ năng lập kế hoạch của mình xuyên suốt tập phim bằng học suốt đời trong ngữ cảnh trong tiểu mục 3.3 và tiểu mục 3.4.

### 3.1. Tổng quan

Chúng tôi nhắm tới phát triển một tác nhân có khả năng giải quyết các tác vụ theo hướng dẫn dài hạn sử dụng quan sát hình ảnh và hành động được căn chỉnh với con người. Để hoàn thành điều này, chúng tôi đề xuất một tác nhân đa phương thức bao gồm một bộ lập kế hoạch tương tác, một bộ điều khiển có điều kiện mục tiêu, và một bộ nhớ đa phương thức của các kinh nghiệm đa phương thức. Khi nhận được một tác vụ và quan sát hiện tại, JARVIS-1 đầu tiên sử dụng MLM để tạo ra một truy vấn đa phương thức (query gen) truy xuất các kinh nghiệm lập kế hoạch liên quan từ bộ nhớ. Những kinh nghiệm này sau đó sẽ được sử dụng cùng với hướng dẫn lập kế hoạch để nhắc MLM-based planner. Tận dụng kiến thức được huấn luyện trước riêng của mình cũng như các kế hoạch tham chiếu được truy xuất, bộ lập kế hoạch cuối cùng sẽ tạo ra một chuỗi K mục tiêu ngắn hạn g1,...,gK để được thực thi bởi bộ điều khiển. Một khi kế hoạch được thực thi thành công, nó sẽ được lưu trữ trong bộ nhớ cùng với tác vụ và tình huống tác nhân khi nó được lập kế hoạch. Chúng tôi cũng trao quyền cho JARVIS-1 với học suốt đời bằng cách kết hợp tự hướng dẫn, nơi JARVIS-1 sẽ đề xuất một số tác vụ cho chính mình để hoàn thành như một phương tiện khám phá; và tự cải thiện, nơi nhiều tác nhân JARVIS-1 sẽ chạy song song để thu thập kinh nghiệm, do đó giúp với việc lập kế hoạch tốt hơn sau này. Chúng tôi cung cấp một minh họa trong Hình 3.

### 3.2. Lập kế hoạch Tương tác với MLM

Như chúng tôi đã đề cập trong tiểu mục 2.1 và tiểu mục 2.2, những thách thức chính cho việc lập kế hoạch trong Minecraft đến từ yêu cầu có thể lập kế hoạch cho các tác vụ dài hạn dưới quan sát động. Được xác nhận bởi nhiều tác phẩm trước đó (Wang et al., 2023a,b; Yuan et al., 2023), điều này làm cho việc sử dụng các bộ lập kế hoạch ký hiệu chuẩn trở nên đặc biệt khó khăn, có thể kém linh hoạt hơn nhiều. Để làm điều này, chúng tôi lấy một mô hình ngôn ngữ đa phương thức (MLM) làm bộ lập kế hoạch zero-shot và kết hợp nó với một khung lập kế hoạch tương tác để giải quyết những thách thức này.

Lập kế hoạch nhận thức tình huống với MLM. Để đạt được lập kế hoạch nhận thức tình huống, bộ lập kế hoạch phải tính đến quan sát hiện tại, ngoài hướng dẫn tác vụ (Huang et al., 2022a; Yao et al., 2022). Cụ thể, chúng tôi bắt đầu với việc dịch quan sát đa phương thức thành mô tả văn bản. Trái ngược với việc để MLM trực tiếp mô tả cảnh, chúng tôi đầu tiên trích xuất từ khóa của các vật phẩm Minecraft (ví dụ: "cây acacia", "cừu") từ Minecraft wiki và sử dụng GPT (Brown et al., 2020) để tạo ra các câu mô tả những quan sát này. Ví dụ, một câu được tạo ra có thể là "Tôi có thể thấy cừu ở đồng bằng acacia". Sau đó MLM sẽ truy xuất câu điều kiện theo quan sát thị giác hiện tại trong quá trình lập kế hoạch. Chi tiết tình huống bổ sung bao gồm biome và trạng thái kho đồ cũng được chuyển đổi thành văn bản sử dụng template. Cuối cùng, chúng tôi nhắc MLM một lần nữa (chỉ phần ngôn ngữ) thành một kế hoạch được đưa ra hướng dẫn tác vụ và tất cả các mô tả tình huống văn bản đã nêu. So với các lựa chọn thay thế end-to-end (Brohan et al., 2023; Huang et al., 2023), chúng tôi thấy việc sử dụng MLM có thể kết hợp của chúng tôi cung cấp mô tả tình huống chất lượng cao hơn và cuối cùng là các kế hoạch với ít ảo giác hơn nhiều.

Lập kế hoạch với tự kiểm tra. Lớp khiên đầu tiên của chúng tôi để đảm bảo tính đúng đắn của các kế hoạch liên quan đến tự kiểm tra. Tương tự như tự debug (Chen et al., 2023), với một kế hoạch ban đầu, chúng tôi yêu cầu JARVIS-1 tiến bộ mô phỏng việc thực thi kế hoạch, dự đoán trạng thái kết quả sau mỗi bước (chủ yếu là trạng thái của kho đồ), và đánh giá chúng. Bằng cách xác minh liệu những trạng thái này có thỏa mãn điều kiện tiên quyết của mục tiêu hay không, JARVIS-1 có thể chủ động xác định các lỗ hổng kế hoạch tiềm ẩn. So với bộ lập kế hoạch chuẩn nơi tác nhân phải gặp lỗi trước khi tạo ra biện pháp khắc phục, việc xác minh kế hoạch trước này có thể giảm thiểu nhu cầu cho tác nhân phục hồi (lập kế hoạch lại) từ các tình huống thử thách hơn do thất bại kế hoạch. Ví dụ, nếu một tác nhân bắt đầu đào ngầm mà không có đủ gỗ, nó thường sẽ phải quay lại bề mặt, điều này làm giảm đáng kể cơ hội hoàn thành tác vụ.

Lập kế hoạch với phản hồi môi trường. Tiếp theo, khung lập kế hoạch tương tác của chúng tôi mạo hiểm cho phép JARVIS-1 nhanh chóng phục hồi từ thất bại bằng cách tận dụng phản hồi môi trường theo cách vòng kín. Quá trình được minh họa trong Hình 4. Trong quá trình thực thi kế hoạch, chúng tôi cung cấp phản hồi cho MLM của JARVIS-1 trong trường hợp có bất kỳ thất bại thực thi nào (có thể do kế hoạch có lỗ hổng) và sử dụng cơ chế tự giải thích của nó (Shinn et al., 2023) để giải thích lỗi và định vị các lỗi trong kế hoạch gốc (chúng tôi gọi điều này là giải thích lỗi). Cuối cùng, bộ lập kế hoạch MLM của JARVIS-1 sẽ tạo ra một kế hoạch cải thiện dựa trên cả phản hồi môi trường bên ngoài và nội quan hồi tưởng. So với các tác nhân khác dựa vào sự can thiệp của con người hoặc thông tin môi trường đặc quyền (Huang et al., 2022b; Zhu et al., 2023), JARVIS-1 có khả năng suy đoán về lý do tại sao các mục tiêu hiện tại không thể đạt được, mà không cần thông tin hoặc thiết kế bổ sung.

### 3.3. Lập kế hoạch với Bộ nhớ Đa phương thức trong Vòng lặp

Để giải quyết thách thức học suốt đời được đề cập trong tiểu mục 2.3, chúng tôi trang bị JARVIS-1 với bộ nhớ đa phương thức để cho phép học từ kinh nghiệm quá khứ của chính mình. Chúng tôi sẽ chi tiết công thức của việc lập kế hoạch tăng cường truy xuất, tạo truy vấn và bố cục bộ nhớ dưới đây.

Lập kế hoạch tăng cường truy xuất. Thế hệ tăng cường truy xuất (RAG) (Lewis et al., 2020; Mao et al., 2020) tăng cường chất lượng các phản hồi được tạo ra bởi LLM bằng cách kết hợp các nguồn kiến thức bên ngoài để bổ sung cho đại diện nội bộ của mô hình. Chúng tôi cũng sử dụng RAG để tăng cường khả năng lập kế hoạch dài hạn của JARVIS-1. So với các phương pháp RAG chính thức tận dụng thư viện kiến thức bên ngoài, chúng tôi lấy bộ nhớ đa phương thức được thu thập làm thư viện kiến thức và truy xuất các kinh nghiệm tương tác làm nhắc minh họa để tăng cường kết quả lập kế hoạch. Công thức như sau:

p(y|x) ~= Σ_{z in top-k(p(·|x))} p_η(z|x)p_θ(y|x,z), (1)

trong đó x, y, và z biểu thị hướng dẫn, kế hoạch, và các mục nhớ được truy xuất tương ứng, và p_η và p_θ được biểu thị là các mô hình truy xuất và lập kế hoạch. Phương pháp lập kế hoạch tăng cường truy xuất như vậy giúp JARVIS-1 gắn kiến thức nội bộ vào các môi trường mở một cách hiệu quả và tận dụng phản hồi tương tác lịch sử để giải quyết ảo giác trong LLM và tạo ra các kế hoạch chính xác hơn.

Bộ nhớ đa phương thức. Chúng tôi đã chứng minh bố cục của bộ nhớ đa phương thức của chúng tôi ở phía bên phải của Hình 5. Từ cấp độ cao, đây là một bộ nhớ key-value nơi các key là đa phương thức, bao gồm cả tác vụ và quan sát (hoặc tình huống) được thực hiện khi mục nhớ này được tạo ra. Các giá trị là các kế hoạch đã được thực thi thành công. Lưu ý rằng, vì các kế hoạch trong môi trường thế giới mở như Minecraft được định vị (xem tiểu mục 2.1), có thể có nhiều mục với cùng tác vụ nhưng các quan sát và kế hoạch khác nhau. Kết quả là, JARVIS-1 cần tạo ra các truy vấn đa phương thức dựa trên tác vụ và tình huống hiện tại để truy xuất các mục nhớ liên quan.

Tạo truy vấn thông qua lý luận. Khi được trình bày với một hướng dẫn như một tác vụ, chúng tôi sử dụng tạo truy vấn thông qua lý luận LLM để phân tách hướng dẫn thành các tiểu tác vụ hoặc tác vụ liên quan, sau đó sẽ được sử dụng làm truy vấn văn bản để truy xuất các kinh nghiệm lập kế hoạch liên quan làm tham chiếu để giải quyết tác vụ hiện tại. Ví dụ, xem xét hướng dẫn "chế tạo 1 bàn phù phép với kho đồ trống" như được hiển thị trong Hình 5. JARVIS-1 truy vấn MLM để xác định các tác vụ cần thiết để đạt được tác vụ chính theo cách tìm kiếm ngược, ví dụ: "lấy sách/kim cương/obsidian với kho đồ trống". Độ sâu tìm kiếm bị giới hạn cho hiệu quả. Hơn nữa, thay vì chỉ dựa vào truy xuất dựa trên truy vấn văn bản (Wang et al., 2023a; Zhu et al., 2023), chúng tôi cũng đề xuất thêm quan sát thị giác hiện tại của tác nhân vào truy vấn văn bản, dẫn đến một truy vấn đa phương thức để tính đến tình huống trong quá trình truy xuất bộ nhớ.

Truy xuất đa phương thức. Sau khi thu được truy vấn văn bản và thị giác, chúng tôi tính toán sự căn chỉnh giữa truy vấn và mỗi quỹ đạo trong bộ nhớ đa phương thức. Chúng tôi đầu tiên sử dụng bộ mã hóa văn bản của mô hình CLIP để tính toán embedding của truy vấn và key tác vụ của mỗi mục trong bộ nhớ. Chúng tôi chọn các mục nhớ có độ tương tự cao hơn ngưỡng tin cậy làm các mục ứng viên. Sau đó chúng tôi sẽ tính toán embedding trạng thái thị giác của truy vấn và trạng thái trong các mục ứng viên. Sau đó chúng tôi sắp xếp các mục ứng viên với độ tương tự embedding thị giác, có thể được hình thành như:

p_η(z|x) ∝ CLIP_v(s_z)^T CLIP_v(s_x), (2)

trong đó s_z và s_x là key thị giác của các mục nhớ và truy vấn thị giác, tương ứng. Cuối cùng, chúng tôi truy xuất kế hoạch của các mục ứng viên top-k làm nhắc tham chiếu z.

### 3.4. Tác nhân Tự cải thiện

Học trong Minecraft với bộ nhớ. Vấn đề còn lại bây giờ là bộ nhớ đa phương thức đã nêu đến từ đâu. Lấy cảm hứng từ sơ đồ học suốt đời trong nhiều vấn đề học tăng cường thế giới đóng và mở (Abel et al., 2018a,b; Wang et al., 2023a), chúng tôi đề xuất phương pháp học sau đây để tăng cường bộ nhớ trong JARVIS-1: 1) Đầu tiên, chúng tôi tạo ra một bộ tác vụ, tạo thành một số chương trình giảng dạy cho các tác nhân hoàn thành như phương tiện khám phá thế giới. Trong quá trình này, JARVIS-1 tạo ra kế hoạch, tương tác với môi trường, đón nhận lỗi, và lưu trữ tất cả những kinh nghiệm này trong bộ nhớ; 2) Sau giai đoạn học này, chúng tôi đánh giá JARVIS-1 trên các tác vụ khác nhau. Do đó, JARVIS-1 có thể tạo ra kế hoạch tốt hơn với bộ nhớ kết hợp với các kinh nghiệm lập kế hoạch. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng điều này làm cài đặt mặc định cho tất cả các tác vụ.

Khám phá sử dụng tự hướng dẫn. Vấn đề quan trọng đối với sự thành công của việc học với bộ nhớ là cách thu thập hiệu quả các kinh nghiệm hữu ích với một lượng thời gian hạn chế. Chúng tôi đề xuất sử dụng tự hướng dẫn (Wang et al., 2022) để tạo ra chương trình giảng dạy động và hướng dẫn JARVIS-1 học từ các tương tác với môi trường. Trong mỗi vòng, chúng tôi nhắc MLM xem xét JARVIS-1 có khả năng như thế nào tại thời điểm này và sau đó chọn các tác vụ từ một nhóm tác vụ để khám phá. Chúng tôi thấy rằng chương trình giảng dạy gần như theo hướng phát triển cây kỹ thuật. Để tăng tốc quá trình học, chúng tôi tăng cường tự hướng dẫn tuyến tính thành học phân tán trong các môi trường phân tán với bộ nhớ chung, tức là thực thi suy đoán (Leviathan et al., 2023). Cụ thể, chúng tôi tạo ra nhiều tác vụ thực thi như các lô tác vụ ứng viên và cung cấp chúng cho các tác nhân có cùng bộ nhớ để xác minh và thực thi trong các môi trường khác nhau. Đồng thời, kinh nghiệm được thu thập vào một bộ nhớ tập trung chung. Khi tất cả các tác vụ khám phá đã được hoàn thành, chúng tôi chuyển sang vòng tiếp theo, cho đến khi bộ nhớ đạt một dung lượng nhất định.

Học suốt đời. Chúng tôi cũng đã quan sát thấy rằng việc học đã nêu (nơi bộ nhớ đang được lấp đầy) có thể được mở rộng xuyên suốt toàn bộ gameplay, nơi tác nhân dần dần thu thập ngày càng nhiều kỹ năng. Khi gameplay tiếp tục, ngày càng nhiều kinh nghiệm đổ vào, do đó JARVIS-1 có thể tìm thấy tham chiếu tốt hơn cho các tác vụ thử thách như ObtainDiamondPickaxe, dẫn đến tỷ lệ thành công cải thiện trên những tác vụ này. Hơn nữa, không có cập nhật gradient trong điều này nhờ MLM tăng cường bộ nhớ, tức là chúng ta có thể thực hiện học suốt đời trong ngữ cảnh. Trong Phần 4.3, chúng tôi cung cấp các thí nghiệm khám phá để cho thấy tiềm năng của khả năng như vậy của JARVIS-1.

## 4. Thí nghiệm

Trong các thí nghiệm, mục tiêu của chúng tôi là 1) đánh giá hiệu suất chung của JARVIS-1 trên các tác vụ Minecraft thử thách, đặc biệt là về lợi thế của nó so với các baseline không (hoàn toàn) giải quyết các vấn đề đã nêu trong các tác nhân thế giới mở; 2) hiểu các yếu tố góp phần vào kết quả chung; 3) khám phá tiềm năng của JARVIS-1 về mặt học suốt đời và lợi ích của nó đối với các tác vụ dài hạn. Để làm điều này, chúng tôi sẽ đầu tiên giới thiệu ngắn gọn các cài đặt đánh giá, sau đó bao gồm kết quả so sánh chính và nghiên cứu loại bỏ, và kết thúc với một thử nghiệm khám phá về các tác vụ dài hạn.

### 4.1. Thiết lập Thí nghiệm

Chúng tôi đánh giá JARVIS-1 trong Minecraft, với các tác vụ được chọn từ Minecraft Universe Benchmark được giới thiệu gần đây (Lin et al., 2023a). Để thuận tiện cho người đọc, chúng tôi cung cấp chi tiết về các thiết lập cơ bản dưới đây.

Cài đặt môi trường. Để đảm bảo gameplay thực tế, tác nhân cần sử dụng không gian quan sát và hành động tương tự như những gì con người sử dụng. Thay vì thiết kế thủ công một giao diện tùy chỉnh cho các mô hình tương tác với môi trường, như được thực hiện trong các phương pháp trước đó như MineDojo (Fan et al., 2022), GITM (Zhu et al., 2023), và Voyager (Wang et al., 2023a), chúng tôi chọn sử dụng giao diện con người bản địa được cung cấp bởi Minecraft. Điều này áp dụng cho cả không gian quan sát và hành động. Mô hình hoạt động ở tốc độ 20 khung hình mỗi giây và được yêu cầu sử dụng giao diện chuột và bàn phím khi tương tác với GUI con người. Để biết thêm thông tin về mô tả chi tiết của không gian quan sát và hành động, vui lòng tham khảo Phụ lục.

Cài đặt tác vụ. Trong Minecraft, người chơi có quyền truy cập vào hàng nghìn vật phẩm, mỗi vật phẩm có yêu cầu thu thập hoặc công thức cụ thể. Ví dụ, các vật phẩm loại đá chỉ có thể được lấy bằng cuốc, và hai ván có thể được chế tạo thành bốn que (những yêu cầu này có sẵn trên Minecraft Wiki). Trong chế độ sinh tồn, người chơi phải lấy từng loại vật phẩm từ môi trường hoặc chế tạo/nung vật phẩm đối tượng từ vật liệu. Chúng tôi chọn hơn 200 tác vụ từ Minecraft Universe Benchmark (Lin et al., 2023a) để đánh giá. Những tác vụ này liên quan đến các vật phẩm có thể được lấy trong overworld Minecraft. Để thuận tiện cho thống kê, chúng tôi đã phân loại chúng thành 11 nhóm theo các danh mục được khuyến nghị trong Minecraft (xem Bảng 1). Do độ phức tạp khác nhau của những tác vụ này, chúng tôi áp dụng thời lượng gameplay tối đa khác nhau (Max. Steps) cho mỗi tác vụ. Giới hạn được xác định bởi thời gian trung bình mà người chơi cần để hoàn thành tác vụ tương ứng. Các chi tiết khác về mỗi tác vụ, như hướng dẫn ngôn ngữ, bước tối đa, thời gian đánh giá, biome, và kho đồ ban đầu khi tác nhân được sinh ra trong thế giới có thể tìm thấy trong Phụ lục Bảng 5-14.

Chỉ số đánh giá. Theo mặc định, tác nhân luôn bắt đầu trong chế độ sinh tồn, với kho đồ trống. Một tác vụ được coi là thành công khi đối tượng mục tiêu được lấy trong thời gian quy định. Do bản chất thế giới mở của Minecraft, thế giới và vị trí ban đầu mà tác nhân được sinh ra có thể khác nhau rất nhiều. Do đó, chúng tôi đã tiến hành ít nhất 30 thử nghiệm cho mỗi tác vụ sử dụng các seed khác nhau và báo cáo tỷ lệ thành công trung bình để đảm bảo đánh giá kỹ lưỡng. Hơn nữa, vì chúng tôi phân loại các tác vụ thành nhóm, chúng tôi cũng báo cáo giá trị trung bình và phương sai cho mỗi nhóm để dễ trình bày.

### 4.2. Kết quả Chính

Chúng tôi so sánh JARVIS-1 với các tác nhân theo hướng dẫn đa nhiệm khác dựa trên LLM, bao gồm Instruct GPT (Huang et al., 2022a; Ouyang et al., 2022), ReAct (Yao et al., 2022), Inner Monologue (Huang et al., 2022b), DEPS (Wang et al., 2023b). Vì một số phương pháp không được thí nghiệm ban đầu trong Minecraft, chúng tôi tái tạo chúng để phù hợp với đặc điểm kỹ thuật Minecraft dựa trên thiết kế template nhắc và phản hồi. Tất cả các phương pháp dựa trên LLM truy cập mô hình LLM thông qua OpenAI API. Và tất cả các siêu tham số của LLM bao gồm nhiệt độ được giữ như mặc định.

Tỷ lệ thành công trung bình cho mỗi nhóm tác vụ được liệt kê trong Bảng 2. JARVIS-1 đạt hiệu suất tốt nhất với tất cả các meta tác vụ. Quan trọng cần lưu ý rằng trong Minecraft, cây công nghệ có thể được hình thành bởi Nhóm Gỗ, Đá, Sắt, Vàng, và Kim cương. Các tác vụ trở nên ngày càng khó khăn khi bạn tiến qua cây. Đối với các tác vụ khó khăn hơn như lấy thỏi vàng hoặc kim cương, các tác nhân thường cần thực hiện nhiều hành động hơn và chuỗi mục tiêu dài hơn để hoàn thành tác vụ. Kết quả là, tỷ lệ thành công của tất cả tác nhân giảm khi mức độ khó tăng lên. Rõ ràng là các phương pháp lý luận (ReAct (Yao et al., 2022) so với GPT (Huang et al., 2022a; Ouyang et al., 2022)) và lập kế hoạch lại tương tác với phản hồi (Inner Monologue (Huang et al., 2022b) so với GPT) hiệu quả tăng cường hiệu suất tác vụ của tác nhân trong thế giới mở. Tuy nhiên, những phương pháp này vẫn gặp thách thức khi xử lý các tác vụ dài hạn, cụ thể là trong nhóm Sắt và Kim cương. DEPS (Wang et al., 2023b), mặt khác, cho phép các tác nhân hoàn thành các tác vụ liên quan đến kim cương thông qua lập kế hoạch dài hạn tương tác kèm theo mô tả và giải thích. Tuy nhiên, độ tin cậy của nó vẫn rất thấp ở khoảng 2,5%.

So với DEPS (Wang et al., 2023b) không có bộ nhớ, JARVIS-1 thể hiện hiệu suất vượt trội ngay cả trong các tác vụ thử thách do kinh nghiệm phong phú của nó. Trong các tác vụ liên quan đến kim cương cụ thể, tỷ lệ thành công đã tăng gần 3 lần (8,99% so với 2,42%). Và JARVIS-1 thường chỉ cần 2-3 vòng lập kế hoạch lại để tạo ra kế hoạch thực thi đúng, trong khi DEPS cần hơn 6 vòng. Điều này có nghĩa là JARVIS-1 tiết kiệm một lượng đáng kể token LLM và thời gian suy nghĩ, cho phép thực thi kế hoạch hiệu quả hơn và cung cấp thêm bước và token để xử lý sự không chắc chắn trong môi trường.

Dựa trên quan sát của chúng tôi, chúng tôi đã thấy rằng điểm nghẽn cho JARVIS-1 trong các tác vụ liên quan đến kim cương thường nằm ở sự bất lực của Bộ điều khiển trong việc thực thi hoàn hảo các hướng dẫn văn bản ngắn hạn được tạo ra bởi LLM. Do đó, đáng khám phá các phương pháp tạo ra kế hoạch dễ thực thi hơn cho bộ điều khiển hoặc cải thiện khả năng theo hướng dẫn của bộ điều khiển.

#### 4.2.1. JARVIS-1 dựa trên các LM khác nhau

Chúng tôi đã tiến hành thí nghiệm loại bỏ trên các Mô hình Ngôn ngữ khác nhau, bao gồm ChatGPT của OpenAI (Ouyang et al., 2022) và GPT-4 OpenAI (2023). Trong số những mô hình này, GPT-4 có nhiều tham số hơn và đã được chứng minh vượt trội hơn ChatGPT trong nghiên cứu rộng rãi Wang et al. (2023a). Chúng tôi cũng chọn mô hình LLaMA2 70B được huấn luyện trước mã nguồn mở Touvron et al. (2023). Ngoài ra, chúng tôi đã thu thập một lượng lớn văn bản liên quan đến Minecraft từ internet làm dữ liệu huấn luyện và tiến hành fine-tune thêm LLaMA2 13B. Các thí nghiệm được tiến hành trên một tập con của các tác vụ Minecraft sử dụng các mô hình ngôn ngữ khác nhau. Mỗi JARVIS-1 học trong 4 epoch tương tác với tất cả bộ tác vụ và đánh giá trên tập con tác vụ qua ít nhất 20 seed. Kết quả thí nghiệm được trình bày trong Hình 6.

Bảng 6 chứng minh rằng ChatGPT, mặc dù có ít tham số hơn, đạt tỷ lệ thành công gần như giống hệt GPT-4. Điều này gợi ý rằng các mô hình ngôn ngữ được trang bị bộ nhớ có thể tăng cường đáng kể khả năng lập kế hoạch. Trong các tác vụ liên quan đến Minecraft, LLaMA2 70B được huấn luyện trước mã nguồn mở thể hiện khoảng cách hiệu suất đáng chú ý so với các mô hình OpenAI, đặc biệt là trong các tác vụ dài hạn. Tuy nhiên, bằng cách fine-tune LLaMA2 với ít tham số hơn, hiệu suất của nó trên các tác vụ Minecraft cải thiện đáng kể. Điều này cho thấy rằng mô hình mã nguồn mở thiếu kiến thức cụ thể về Minecraft và cần fine-tune thêm để hoàn thành thành công các tác vụ như vậy.

#### 4.2.2. Loại bỏ về Bộ nhớ

Chúng tôi cũng tiến hành thí nghiệm loại bỏ trên module bộ nhớ đa phương thức và các phương pháp truy xuất. Chúng tôi đặt JARVIS-1 w/o module bộ nhớ làm tác nhân cơ sở. Chúng tôi đầu tiên đánh giá hiệu suất của JARVIS-1 với các kích thước bộ nhớ khác nhau (đại diện cho các giai đoạn học khác nhau) như được hiển thị trong Hình 7, chứng minh hiệu quả của việc tự cải thiện trong JARVIS-1. Chúng tôi tiến hành thêm các thí nghiệm trên một tập con của các tác vụ Minecraft sử dụng ba phương pháp truy xuất khác nhau: truy xuất chỉ với embedding hướng dẫn văn bản (Text Memory), kết hợp lý luận và truy xuất với embedding văn bản (Text Memory+Reasoning), và kết hợp lý luận và truy xuất với embedding đa phương thức (Multimodal Memory+Reasoning). Ngoại trừ các phương pháp bộ nhớ và truy xuất, tất cả những thứ khác được giữ như nhau. Kết quả được liệt kê trong Hình 8.

Các thí nghiệm cho thấy lý luận trước khi truy xuất có thể cải thiện hiệu quả độ chính xác truy xuất. Truy xuất dựa trên trạng thái đa phương thức bao gồm quan sát thị giác và thông tin ký hiệu (ví dụ: kho đồ, vị trí, v.v.) tốt hơn chỉ xem xét embedding văn bản.

### 4.3. Thách thức Dài hạn

Hầu hết các tác nhân đa nhiệm đồng thời trong Minecraft chỉ có thể xử lý các tác vụ ngắn hạn và gặp khó khăn với các tác vụ dài hạn như CraftingDiamondPickaxe. Mô hình nền tảng VPT (Baker et al., 2022) có khả năng hoàn thành nhiều tác vụ trong Minecraft nhưng thiếu khả năng thực thi hướng dẫn của con người. Để giải quyết hạn chế này, Học Tăng cường được yêu cầu để fine-tune mô hình nền tảng VPT cho việc hoàn thành tác vụ cụ thể. Tuy nhiên, sau khi fine-tune, VPT có thể trải qua sự suy giảm hiệu suất cho các tác vụ khác trong khi tập trung vào tác vụ được chỉ định. Ngược lại, Steve-1 (Lifshitz et al., 2023) đã triển khai fine-tune có điều kiện mục tiêu trên VPT, cho phép nó theo hướng dẫn văn bản của con người trong khi duy trì khả năng đa nhiệm. Tuy nhiên, Steve-1 chủ yếu tập trung vào các tác vụ cấp thấp như lấy đất, thu thập hoa và chặt cây. Khi nói đến các tác vụ dài hạn như bắt đầu từ đầu bằng cách lấy cuốc gỗ, Steve-1 vẫn gặp khó khăn.

DEPS (Wang et al., 2023b) cũng sử dụng LLM làm bộ lập kế hoạch, nhưng nó thiếu khả năng học từ kinh nghiệm trong các tác vụ khác nhau và áp dụng kiến thức đó cho những tác vụ mới. Ngoài ra, DEPS bị hạn chế trong các vòng lập kế hoạch lại do ràng buộc ngữ cảnh của LM. Các thí nghiệm tiết lộ rằng DEPS có tỷ lệ thành công dưới 50% trong việc tạo ra các kế hoạch chính xác và thực thi được để thu thập kim cương. Xác suất DEPS thành công lấy kim cương trong môi trường là khoảng 0,59%. Do đó, DEPS tiếp tục gặp thách thức khi cố gắng hoàn thành các tác vụ dài hạn trong thế giới Minecraft.

Ngay cả những người chơi đã thành thạo mẫu phân bố kim cương cũng đạt tỷ lệ thành công lấy kim cương và chế tạo cuốc kim cương (yêu cầu ít nhất ba kim cương) trong vòng 10 phút lần lượt là khoảng 15% và 12%. JARVIS-1 thể hiện tốt hơn trong thử thách ObtainDiamondPickaxe. So với mô hình tiên tiến, đã trải qua RL-finetuned VPT, JARVIS-1 đã tăng hơn gấp đôi tỷ lệ thành công lấy cuốc kim cương (6,22% so với 2,5% trong vòng 20 phút).

Để tăng cơ hội lấy kim cương, chúng tôi đã mở rộng thời gian chơi game lên 60 phút (72000 bước chơi game, như được hiển thị trong Hình 9). Kết quả là, tỷ lệ thành công của JARVIS-1 trong việc thu thập cuốc kim cương cải thiện từ 6,2% lên 12,5%. Đồ thị ở phía bên phải của Hình 7 minh họa cách tỷ lệ thành công của các vật phẩm mốc trung gian thay đổi theo thời gian, cho thấy rằng JARVIS-1 có xu hướng cải thiện với thời gian chơi game dài hơn. Chúng tôi cũng tiến hành hai biến thể của JARVIS-1 với các chương trình giảng dạy tự cải thiện khác nhau: do con người viết và được tạo ra ngẫu nhiên. Cả ba JARVIS-1 đều đã thu thập kinh nghiệm vào bộ nhớ với chương trình giảng dạy trong 4 epoch trước khi đánh giá trong 60 phút. Kết quả cho thấy JARVIS-1 với chương trình giảng dạy được tạo ra bởi GPT có thể hoàn thành tác vụ trong các bước chơi game ngắn nhất và đạt hiệu suất tốt nhất trong 60 phút.

Ngược lại, tỷ lệ thành công của VPT hầu như không thay đổi khi chúng tôi tăng thời gian từ 20 phút lên 60 phút (từ 2,5% lên 3%). Điều này có thể được quy cho hệ thống độ bền của Minecraft nơi việc khám phá ngầm kéo dài thường dẫn đến hư hỏng cuốc. Khi cuốc của JARVIS-1 bị hỏng, nó lập kế hoạch lại một cách động dựa trên kho đồ hiện tại và chế tạo một cái mới. Tuy nhiên, VPT-RL thể hiện hành vi khó hiểu ở giai đoạn này bằng cách sử dụng công cụ không phù hợp để khai thác đá hoặc chế tạo các vật phẩm không cần thiết. So sánh này chứng minh rằng JARVIS-1 sở hữu khả năng tổng quát hóa và lập kế hoạch vượt trội cho các tác vụ dài hạn.

Lưu ý rằng phương pháp của chúng tôi được thiết kế để đa nhiệm về bản chất và không được fine-tune thông qua học bắt chước trên các bộ dữ liệu cụ thể hoặc học tăng cường.

## 5. Công trình Liên quan

### 5.1. Lập kế hoạch với LLM

Đã có một số phương pháp tận dụng mô hình ngôn ngữ lớn để tạo ra kế hoạch hành động cho các tác vụ cấp cao trong các môi trường thể hiện (Dasgupta et al., 2022; Gong et al., 2023b; Liu et al., 2023; Mai et al., 2023; Zeng et al., 2022; Zhang et al., 2023; Zhang and Lu, 2023). Huang et al. (2022a) phân tách các lệnh ngôn ngữ tự nhiên thành chuỗi các hành động thực thi được bằng hoàn thành văn bản và dịch nghĩa, trong khi SayCan tạo ra các kế hoạch khả thi cho robot bằng cách giải mã chung một LLM được cân bằng bởi affordances kỹ năng từ các hàm giá trị (Brohan et al., 2022b). Một số phương pháp cũng tận dụng LLM để tạo ra mã chương trình làm kế hoạch để thực thi tốt hơn (Liang et al., 2022; Lin et al., 2023b; Singh et al., 2022). Tuy nhiên, các phương pháp trên giả định rằng kế hoạch ban đầu từ LLM là đúng. Khi có lỗi trong kế hoạch ban đầu, tác nhân khó có thể hoàn thành tác vụ thành công. Nghiên cứu gần đây thường sử dụng LLM làm bộ lập kế hoạch tương tác, khai thác khả năng tự cập nhật của nó để tăng cường khả năng thực thi của kế hoạch theo thời gian (Shinn et al., 2023; Sun et al., 2023; Wang et al., 2023b). Inner Monologue (Huang et al., 2022b) dẫn đầu việc lập kế hoạch tương tác với LLM, giới thiệu phản hồi (bao gồm phát hiện thành công và mô tả cảnh) cho bộ lập kế hoạch. Tuy nhiên, chúng tôi thấy nó vẫn có thể gặp phải lỗi lập kế hoạch tích lũy, đặc biệt là trong các tác vụ dài hạn thế giới mở. ReAct (Yao et al., 2022) sẽ lý luận về trạng thái tác nhân trước khi hành động, cho thấy rằng các phương pháp lý luận khác nhau (Wei et al., 2022; Wu et al., 2023; Yao et al., 2023) có lợi cho việc lập kế hoạch. Các phương pháp lập kế hoạch dựa trên LLM thường sử dụng LLM được huấn luyện trước cố định làm tác nhân, trong khi chúng tôi tập trung nhiều hơn vào học suốt đời và liên tục cho các tác nhân trong các môi trường thế giới mở (Ke et al., 2022a,b; Wang et al., 2023a). Để tận dụng tốt hơn tương tác lịch sử giữa tác nhân và môi trường, một bộ nhớ rõ ràng (Park et al., 2023; Zhu et al., 2023) cho lưu trữ lớn hơn các kinh nghiệm tác nhân đã được tận dụng. Tuy nhiên, các phương pháp trên thường chỉ dựa vào môi trường dựa trên văn bản và gặp khó khăn trong việc thực thi kế hoạch trong các môi trường thế giới mở thị giác quan sát từng phần.

### 5.2. Tác nhân Minecraft

Phát triển các tác nhân có khả năng tổng quát trong Minecraft để giải quyết các tác vụ thế giới mở đã thu hút sự quan tâm ngày càng tăng (Baker et al., 2022; Cai et al., 2023a,b; Ding et al., 2023; Fan et al., 2022; Yuan et al., 2023; Zhang and Lu, 2023; Zhu et al., 2023). Như một nỗ lực ban đầu, Oh et al. (2017) nghiên cứu tổng quát hóa tác vụ trong một biến thể môi trường Minecraft đơn giản. Nó thiết kế một pipeline hai giai đoạn, đầu tiên thành thạo các kỹ năng tiên quyết với thủ thuật tham số hóa, và sau đó học một bộ điều khiển meta để thực thi hướng dẫn. Chuyển sang giải quyết các tác vụ dài hạn phức tạp trong Minecraft, các công trình (Lin et al., 2021; Mao et al., 2022; Oh et al., 2017) khám phá kiến trúc phân cấp. Trong những năm gần đây, bị ảnh hưởng bởi xu hướng của các mô hình huấn luyện trước quy mô lớn, một nhóm nhà nghiên cứu đã xuất hiện, những người đang sử dụng lượng lớn kiến thức internet để huấn luyện các tác nhân thông minh. Fan et al. (2022) huấn luyện một mô hình căn chỉnh thị giác-nghĩa, MineCLIP, sử dụng sự tương ứng giữa phụ đề và đoạn video có sẵn trên YouTube, và sử dụng nó để tạo ra phần thưởng nội tại để hướng dẫn học chính sách. (Baker et al., 2022) sử dụng một mô hình dynamic nghịch đảo được huấn luyện trước để gắn nhãn hành động trong video YouTube được sử dụng để học một chính sách nền tảng VPT thông qua học bắt chước. Bằng cách bắc cầu MineCLIP và VPT, Lifshitz et al. (2023) tạo ra một chính sách theo hướng dẫn hiệu suất Steve-1 để giải quyết các tác vụ ngắn hạn thế giới mở sử dụng relabeling hindsight và thủ thuật unCLIP. Tuy nhiên, Steve-1 không thể giải quyết các tác vụ hướng quy trình phức tạp do khả năng biểu đạt của không gian mục tiêu của nó. Cai et al. (2023b) học theo video tham chiếu làm hướng dẫn bằng cách chỉ xem video gameplay, cải thiện khả năng của không gian mục tiêu và giảm chi phí huấn luyện chính sách. Tất cả những phương pháp này tập trung vào việc cải thiện sự mượt mà và mạnh mẽ của tương tác giữa chính sách và môi trường. Lấy cảm hứng từ khả năng hiểu ngôn ngữ và lý luận mạnh mẽ của các mô hình ngôn ngữ lớn, các nhà nghiên cứu đã bắt đầu xây dựng các tác nhân Minecraft dựa trên LLM. Wang et al. (2023a) sử dụng LLM để hướng dẫn tác nhân khám phá thế giới Minecraft bằng cách thu thập các kỹ năng đa dạng, tạo ra những khám phá mới và tạo ra đề xuất mục tiêu. Zhu et al. (2023) tích hợp LLM với kiến thức và bộ nhớ dựa trên văn bản để trang bị cho tác nhân với kiến thức thông thường và kinh nghiệm trong quá khứ để hiệu quả lý luận cao hơn. Yuan et al. (2023) sử dụng LLM để hướng dẫn tác nhân khám phá thế giới Minecraft và tương tác với môi trường với các chính sách điều khiển học tăng cường.

## 6. Kết luận

Chúng tôi đề xuất một tác nhân đa nhiệm JARVIS-1 được thiết kế cho môi trường phức tạp của Minecraft, đánh dấu một tiến bộ đáng kể trong việc đạt được lập kế hoạch giống con người trong một bối cảnh thế giới mở. Bằng cách tận dụng các Mô hình Ngôn ngữ Đa phương thức được huấn luyện trước, JARVIS-1 không chỉ hiệu quả diễn giải đầu vào đa phương thức mà còn khéo léo dịch chúng thành hành động. Việc tích hợp bộ nhớ đa phương thức của nó, rút ra từ cả kiến thức ăn sâu và kinh nghiệm game thời gian thực, tăng cường khả năng ra quyết định của nó. Bằng chứng thực nghiệm về sức mạnh của nó rõ ràng trong hiệu suất ấn tượng của nó qua một loạt rộng các tác vụ trong Minecraft. Đáng chú ý, thành tích của nó trong tác vụ cuốc kim cương dài hạn, nơi nó đạt tỷ lệ hoàn thành vượt qua VPT lên đến năm lần, nhấn mạnh tiềm năng của nó và những bước tiến đã đạt được trong lĩnh vực này. Đột phá này tạo ra nền tảng cho tương lai của các tác nhân đa năng và thích ứng hơn trong các môi trường ảo phức tạp.

## Lời cảm ơn

Công trình này được tài trợ một phần bởi Chương trình R&D Chính Quốc gia Trung Quốc #2022ZD0160301, một khoản tài trợ từ CCF-Tencent Rhino-Bird Open Research Fund, các khoản tài trợ NSF #IIS-1943641, #IIS-1956441, #CCF-1837129, một SRA từ Meta và một món quà nghiên cứu từ Amazon Alexa AI, và một món quà từ RelationalAI. Các tác giả chân thành cảm ơn Tiến sĩ Rita Zhang, Zhixiang Dai tại NVIDIA về hỗ trợ kỹ thuật có giá trị cho tính toán GPU.

## Tài liệu Tham khảo

[Phần tài liệu tham khảo tiếp tục với các citation đầy đủ như trong bản gốc...]
