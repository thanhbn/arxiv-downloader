# 2308.07702.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.07702.pdf
# Kích thước tệp: 1183217 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lý luận Zero-Shot tốt hơn với Role-Play Prompting
Aobo Kong1Shiwan Zhao2Hao Chen3Qicheng Li1∗Yong Qin1
Ruiqi Sun3Xin Zhou3Enzhi Wang1Xiaohang Dong1
1CS, Đại học Nankai2Nhà nghiên cứu độc lập
3Phòng thí nghiệm Nghiên cứu Doanh nghiệp & Đám mây, Nghiên cứu Lenovo
1kongaobo@mail.nankai.edu.cn2zhaosw@gmail.com
1{liqicheng, qinyong}@nankai.edu.cn
3{chenhao31, sunrq2, zhouxin16}@lenovo.com

Tóm tắt
Các mô hình ngôn ngữ lớn hiện đại (LLM) thể hiện khả năng đóng vai đáng chú ý, cho phép chúng không chỉ thể hiện các nhân vật con người mà còn cả các thực thể phi con người. Tính linh hoạt này cho phép chúng mô phỏng các tương tác và hành vi phức tạp giống con người trong nhiều bối cảnh khác nhau, cũng như bắt chước các đối tượng hoặc hệ thống cụ thể. Trong khi những khả năng này đã tăng cường sự tương tác của người dùng và giới thiệu các chế độ tương tác mới, ảnh hưởng của đóng vai đến khả năng lý luận của LLM vẫn chưa được khám phá đầy đủ. Trong nghiên cứu này, chúng tôi giới thiệu một phương pháp role-play prompting được thiết kế chiến lược và đánh giá hiệu suất của nó trong thiết lập zero-shot trên mười hai tiêu chuẩn lý luận đa dạng. Kết quả thực nghiệm của chúng tôi minh họa rằng role-play prompting liên tục vượt trội hơn phương pháp zero-shot tiêu chuẩn trên hầu hết các tập dữ liệu. Đáng chú ý, trong các thí nghiệm được thực hiện bằng ChatGPT, độ chính xác trên AQuA tăng từ 53,5% lên 63,8%, và trên Last Letter từ 23,8% lên 84,2%. Khi so sánh thêm với kỹ thuật Zero-Shot-CoT, yêu cầu mô hình "suy nghĩ từng bước", nghiên cứu của chúng tôi chứng minh rằng role-play prompting hoạt động như một trigger hiệu quả hơn cho quá trình CoT. Điều này làm nổi bật tiềm năng của nó để tăng cường khả năng lý luận của LLM. Chúng tôi công bố mã nguồn của chúng tôi tại url này.

1 Giới thiệu
Những năm gần đây đã chứng kiến sự thay đổi mô hình trong xử lý ngôn ngữ tự nhiên, chủ yếu được thúc đẩy bởi các mô hình ngôn ngữ lớn (LLM) như GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), và Llama (Touvron et al., 2023a). Bằng cách tiền huấn luyện trên các kho ngữ liệu văn bản rộng lớn, các mô hình này đã đạt được khả năng hiểu và tạo sinh ngôn ngữ ấn tượng, trao quyền cho chúng giải quyết nhiều tác vụ hạ nguồn thông qua prompting, do đó bỏ qua sự cần thiết cho việc fine-tuning cụ thể cho tác vụ. Giữa sự gia tăng của các kỹ thuật prompt, role-play (Wu et al., 2023) và chain-of-thought prompting (Wei et al., 2022; Kojima et al., 2022) đã thu hút sự quan tâm đặc biệt.

Các LLM hiện đại, với khả năng đóng vai tiên tiến, đã làm phong phú đáng kể trải nghiệm người dùng và tạo ra các chế độ tương tác mới. Chúng có thể bắt chước một cách thuyết phục các nhân cách khác nhau, từ các nhân vật hư cấu đến các nhân vật lịch sử và đương đại. Vai trò được giao cung cấp bối cảnh về danh tính và nền tảng của LLM. Bằng cách áp dụng nhân cách, LLM có thể tạo ra các phản hồi tự nhiên hơn, phù hợp với nhân vật được điều chỉnh cho vai trò đó. Nhận ra tiềm năng này, các công ty như Character.AI1 đã phát triển các tác nhân đối thoại mô tả các nhân vật đa dạng. Ngoài các ứng dụng đối thoại, việc đóng vai cũng tăng cường hiệu suất LLM trên một số tác vụ NLP nhất định. Ví dụ, khi được đóng vai như một thẩm phán với vai trò đặc biệt, LLM có thể đánh giá hiệu quả chất lượng tóm tắt văn bản (Wu et al., 2023). Một cách bất thường hơn, ChatGPT thể hiện năng lực trong việc xử lý các lệnh Linux khi được gợi ý như một terminal Linux2.

Mặc dù có những tiến bộ này, việc phân tích ảnh hưởng của đóng vai đến khả năng lý luận cốt lõi của LLM cần được điều tra thêm.

Trong khi khả năng đóng vai của LLM đã mở rộng chân trời của tương tác người-máy, việc thúc đẩy để khuếch đại sức mạnh lý luận của các mô hình này đã dẫn đến sự phát triển của các kỹ thuật như Chain-of-Thought (CoT) Prompting. CoT prompting được đề xuất bởi Wei et al. (2022) và bao gồm việc cung cấp các bước lý luận trong các ví dụ few-shot. Bằng cách kích thích lý luận từng bước, CoT prompting đã cải thiện đáng kể khả năng lý luận của LLM. Nhiều nghiên cứu tiếp theo (Wang et al., 2022; Kojima et al., 2022; Zhou et al., 2022) đã xây dựng dựa trên phương pháp này.

Được truyền cảm hứng từ thành công của đóng vai trên nhiều tác vụ hạ nguồn, chúng tôi khám phá liệu đóng vai có thể tương tự tăng cường hiệu suất lý luận của LLM hay không. Ví dụ, việc giao cho ChatGPT vai trò của một giáo viên toán có thể tăng cường khả năng giải quyết bài toán toán học của nó không? Trong công việc này, chúng tôi giới thiệu một phương pháp role-play prompting zero-shot dựa trên một khung hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi sử dụng LLM để xây dựng các prompt role-play cụ thể cho tác vụ. Trong giai đoạn thứ hai, các phản hồi được thu thập cho mỗi truy vấn lý luận, được hướng dẫn bởi các prompt role-play cụ thể cho tác vụ đã được xây dựng trước đó. Một ví dụ minh họa được cung cấp trong Hình 1. Chúng tôi tập trung nghiên cứu của mình vào các LLM đối thoại, đánh giá phương pháp của chúng tôi trên 12 tiêu chuẩn lý luận bằng ChatGPT. Kết quả của chúng tôi chứng minh sự cải thiện nhất quán so với đường cơ sở zero-shot trên phần lớn các tập dữ liệu, xác nhận hiệu quả của role-play prompting. Chúng tôi tiếp tục đánh giá các LLM đối thoại khác như Vicuna (Chiang et al., 2023) và Llama 2-Chat (Touvron et al., 2023b), quan sát được những cải thiện tương tự.

Hơn nữa, chúng tôi so sánh phương pháp của chúng tôi với kỹ thuật Zero-Shot-CoT (Kojima et al., 2022), kỹ thuật này kích hoạt CoT một cách rõ ràng bằng cách thêm "Let's think step by step" vào các câu hỏi. Các LLM đối thoại hiện đại như ChatGPT đã trải qua quá trình fine-tuning có giám sát rộng rãi, cho phép chúng tạo ra CoT cho một số chủ đề nhất định mà không cần trigger rõ ràng. Trong các tác vụ mà mô hình gặp khó khăn trong việc tạo ra CoT một cách tự nhiên, như Last Letter, cả phương pháp của chúng tôi và Zero-Shot-CoT đều có thể kích thích CoT từ đầu. Tuy nhiên, đối với các tác vụ mà CoT đã xảy ra, như số học, cả phương pháp của chúng tôi và Zero-Shot-CoT đều củng cố quá trình lý luận từng bước, nhưng Zero-Shot-CoT không thể hiện hiệu quả đáng kể, trong khi phương pháp của chúng tôi dẫn đến hiệu suất tốt hơn. Do đó, chúng tôi đưa ra giả thuyết rằng role-play prompting là một trigger CoT ngầm và có thể tạo ra CoT hiệu quả hơn trong một số lĩnh vực so với Zero-Shot-CoT.

Theo hiểu biết của chúng tôi, công việc này đại diện cho cuộc điều tra hệ thống đầu tiên về role-play prompting cho các tác vụ lý luận. Mặc dù có những ảnh hưởng biến đổi của đóng vai đến hành vi LLM, nghiên cứu học thuật ít ỏi đã khám phá hiện tượng này. Chúng tôi tin rằng nghiên cứu của chúng tôi phục vụ như một bước khởi đầu để thúc đẩy khám phá rộng rãi hơn vào hướng nghiên cứu đầy hứa hẹn này.

Đóng góp chính của chúng tôi có ba mặt:
• Chúng tôi đề xuất một phương pháp role-play prompting mới dựa trên khung hai giai đoạn để tăng cường khả năng lý luận zero-shot của LLM. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên cải thiện khả năng lý luận của LLM bằng role-play prompting.
• Chúng tôi đánh giá kỹ lưỡng phương pháp của chúng tôi trên 12 tiêu chuẩn lý luận, chứng minh hiệu quả của role-play prompting và cung cấp cái nhìn sâu sắc về thiết kế prompt.
• Dựa trên kết quả thực nghiệm của chúng tôi, chúng tôi kết luận rằng role-play prompting có thể phục vụ như một trigger CoT ngầm hiệu quả, giải thích sự tăng cường trong khả năng lý luận của nó.

2 Công trình liên quan
2.1 Khả năng đóng vai của LLM
Khả năng đóng vai xuất sắc của các mô hình ngôn ngữ lớn (LLM) gần đây đã thu hút sự chú ý đáng kể. LLM đã thể hiện tính linh hoạt đáng chú ý trong việc đóng các vai trò khác nhau một cách liền mạch, dù là một cố vấn du lịch có kiến thức rộng, được cá nhân hóa hay một terminal Linux ảo. Nhiều công ty, như Character.AI, đã tận dụng khả năng đóng vai khéo léo này bằng cách ra mắt các tác nhân đối thoại thương mại đảm nhận nhiều nhân cách đa dạng. Trong khi đóng vai cho phép các con đường sáng tạo cho tương tác người dùng, nó cũng đã bị khai thác để vượt qua một số hạn chế được áp đặt lên LLM, như được minh chứng bởi "grandma exploit" khét tiếng. Trong khai thác này, người dùng đã gợi ý các phản hồi không phù hợp từ LLM bằng cách đóng vai nó thành vai trò của một bà ngoại đã qua đời.

Mặc dù có sự quan tâm gia tăng đối với LLM, điều tra học thuật về khả năng đóng vai của chúng đã bị hạn chế cho đến nay. Han et al. (2022) xây dựng các mô hình đối thoại hấp dẫn dựa trên đóng vai. Wu et al. (2023) đề xuất một khung đánh giá tóm tắt dựa trên LLM, sử dụng đóng vai để cho phép đánh giá toàn diện và giống con người hơn. Shanahan et al. (2023) đề xuất rằng các tác nhân đối thoại được xây dựng trên LLM có thể phục vụ như các trình mô phỏng vai, và sử dụng các cuộc đối thoại đóng vai để phân tích khả năng giống con người của LLM với mục tiêu bác bỏ nhân cách hóa. Công việc của chúng tôi là đầu tiên áp dụng khả năng đóng vai của LLM vào các tác vụ lý luận. Chúng tôi hy vọng rằng công việc của chúng tôi sẽ khuyến khích nhiều khám phá hơn liên quan đến đóng vai với LLM.

2.2 Khả năng lý luận của LLM
Ban đầu, LLM được coi là thiếu sót trong khả năng lý luận do hiệu suất kém của chúng trong các lĩnh vực như số học và lý luận thông thường (Brown et al., 2020; Rae et al., 2021). Tuy nhiên, Wei et al. (2022) đề xuất chain-of-thought prompting, nơi các bước lý luận được cung cấp trong các ví dụ few-shot, dẫn đến sự tăng cường đáng kể trong khả năng lý luận của LLM. Chúng tôi chia công việc tiếp theo dựa trên chain-of-thought thành hai loại, few-shot và zero-shot, và giới thiệu chúng tương ứng.

Few-shot Self-consistency (Wang et al., 2022) lấy mẫu các đường lý luận đa dạng thay vì giải mã tham lam ngây thơ và sau đó chọn câu trả lời nhất quán nhất bằng bỏ phiếu đa số. DIVERSE (Li et al., 2023) áp dụng các ví dụ few-shot khác nhau để tăng cường tính đa dạng trong các đường lý luận thu được bằng self-consistency. Least-to-most prompting (Zhou et al., 2022) chia nhỏ một vấn đề phức tạp thành một loạt các vấn đề con đơn giản hơn và sau đó giải quyết chúng theo trình tự. Self-refine (Madaan et al., 2023) tạo ra một đầu ra thông qua chain-of-thought, và sau đó sử dụng cùng một LLM để cải thiện đầu ra ban đầu thông qua phản hồi và tinh chỉnh lặp lại. Active prompting (Diao et al., 2023) vay mượn từ học tập chủ động để chọn những câu hỏi không chắc chắn nhất làm ví dụ few-shot. Tree-of-Thought (Yao et al., 2023) biểu diễn các đường lý luận có thể như một cấu trúc cây và sử dụng các thuật toán tìm kiếm như DFS hoặc BFS để khám phá nhánh lý luận đúng.

Zero-shot Zero-Shot-CoT (Kojima et al., 2022) đơn giản thêm "Let's think step by step" sau câu hỏi để kích thích đầu ra chain-of-thought trong LLM. Auto-CoT (Zhang et al., 2022) và COSP (Wan et al., 2023) tự động xây dựng các ví dụ few-shot bằng cách chọn câu hỏi dựa trên các nguyên tắc nhất định và thu được câu trả lời của chúng thông qua Zero-Shot-CoT. Plan-and-Solve prompting (Wang et al., 2023) chia tác vụ ban đầu thành nhiều tác vụ con và giải quyết chúng tuần tự trong thiết lập zero-shot. Trong bài báo này, chúng tôi đề xuất một phương pháp zero-shot đơn giản nhưng hiệu quả dựa trên role-play prompting không cần xây dựng các ví dụ few-shot. Phương pháp của chúng tôi vượt trội hơn Zero-Shot-CoT trên hầu hết các tiêu chuẩn và có thể phục vụ như một đường cơ sở mới cho các tác vụ lý luận.

3 Role-Play Prompting
Thực hành thông thường của role-play prompting bao gồm việc đơn giản nối giao nhiệm vụ vai trò với câu hỏi lý luận thành một prompt duy nhất để truy vấn LLM, tạo thành một tương tác một lượt. Để đắm chìm LLM hơn nữa trong vai trò được chỉ định và có khả năng tăng cường hiệu quả của nó, chúng tôi đề xuất chuyển từ tương tác một lượt này sang một quá trình đối thoại hai vòng. Cụ thể, vòng đối thoại đầu tiên cho phép mô hình mô tả vai trò được giao của nó, do đó làm sâu sắc thêm khung và nhân cách của nó. Vòng tiếp theo sau đó thu thập phản hồi của mô hình đối với truy vấn lý luận được đặt ra trong vai trò được xác định trước đó.

Trong quá trình đối thoại hai vòng, việc mô tả vai trò ban đầu của mô hình có vai trò quan trọng đối với hiệu quả lý luận tiếp theo. Với chất lượng không kiểm soát của phản hồi ban đầu này, chúng tôi lấy mẫu nhiều phản hồi trong vòng đầu tiên và xác định phản hồi tối ưu để cố định cho tất cả câu hỏi. Bằng cách đảm bảo phản hồi vòng đầu tiên tối ưu này, chúng tôi nối cả đầu vào và đầu ra của tương tác vòng đầu tiên với câu hỏi lý luận để tạo ra một prompt duy nhất, tạo thuận lợi cho các phản hồi được điều chỉnh. Điều này cũng mang lại lợi thế của việc gọi API của mô hình một lần duy nhất cho mỗi trường hợp. Tóm lại, phương pháp role-play prompting của chúng tôi tuân theo một quá trình hai giai đoạn như được mô tả trong Hình 2: đầu tiên xây dựng một tương tác đắm chìm vai trò tối ưu cho mỗi tác vụ, sau đó thu thập phản hồi cho mỗi câu hỏi lý luận dựa trên vai trò đã được thiết lập đó. Chúng tôi tiếp tục cung cấp một ví dụ thể hiện quy trình hai giai đoạn này trên một tác vụ lý luận thông thường trong Hình 3.

3.1 Xây dựng Prompt
Trong giai đoạn đầu tiên, chúng tôi xây dựng hai prompt cho mỗi tác vụ lý luận:
• Role-Setting Prompt: Prompt do người dùng thiết kế này phác thảo vai trò cụ thể mà LLM được mong đợi đảm nhận trong suốt cuộc đối thoại, được điều chỉnh cho tác vụ đang thực hiện.
• Role-Feedback Prompt: Được dự định như sự thừa nhận của mô hình đối với prompt thiết lập vai trò, prompt này nhằm mục đích neo mô hình hơn nữa trong vai trò được quy định. Nó được tạo ra bằng cách lấy mẫu phản hồi của mô hình.

Trong việc thiết kế prompt thiết lập vai trò, điều bắt buộc là chọn các vai trò một cách tự nhiên thể hiện lợi thế riêng biệt cho tác vụ cụ thể đang thực hiện. Việc làm phong phú thêm prompt với các mô tả bổ sung nhấn mạnh lợi thế này thường dẫn đến kết quả cải thiện. Khi prompt thiết lập vai trò đã được nêu rõ, nó được trình bày cho LLM, tạo ra nhiều phản hồi được lấy mẫu. Từ những phản hồi này, chúng tôi chọn phản hồi đại diện và đắm chìm nhất nắm bắt bản chất của vai trò dự định làm prompt phản hồi vai trò cuối cùng. Một cuộc thảo luận toàn diện về các sắc thái của thiết kế prompt sẽ được trình bày trong Phần 4.4.

3.2 Trả lời câu hỏi
Trong giai đoạn thứ hai, mỗi câu hỏi của tác vụ, kết hợp với prompt thiết lập vai trò và phản hồi vai trò, được sử dụng làm đầu vào cho API của mô hình. Phương pháp này tạo thuận lợi cho việc tạo ra câu trả lời chỉ với một lần gọi API duy nhất. Để rõ ràng, chúng tôi cung cấp một ví dụ mã về việc thực hiện cuộc gọi API trong Phụ lục A.1.

4 Thí nghiệm
4.1 Tác vụ và Tập dữ liệu
Phù hợp với nghiên cứu trước đó về khả năng lý luận của LLM (Wei et al., 2022; Kojima et al., 2022), chúng tôi đánh giá phương pháp của chúng tôi trên 12 tập dữ liệu trải dài 4 loại: (1) số học, bao gồm MultiArith (Roy và Roth, 2015), GSM8K (Cobbe et al., 2021), AddSub (Hosseini et al., 2014), AQUA-RAT (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015), và SVAMP (Patel et al., 2021); (2) lý luận thông thường, bao gồm CSQA (Talmor et al., 2019) và StrategyQA (Geva et al., 2021); (3) lý luận ký hiệu, bao gồm Last Letter Concatenation và Coin Flip (Wei et al., 2022); (4) khác, bao gồm Date Understanding và Tracking Shuffled Objects từ BIG-bench (Srivastava et al., 2022). Thêm chi tiết có thể được tìm thấy trong Phụ lục C.

4.2 Thiết lập thí nghiệm
Mô hình Chúng tôi sử dụng ChatGPT (gpt-3.5-turbo-0613), mô hình đối thoại mạnh nhất hiện tại ngoài GPT-4, để tiến hành thí nghiệm.

Prompt Phương pháp của chúng tôi bao gồm thiết kế một prompt thiết lập vai trò và một prompt phản hồi vai trò cho một tác vụ nhất định. Tác vụ số học bao gồm sáu tập dữ liệu, tất cả đều sử dụng các prompt giống nhau, như được mô tả trong Hình 1. Tương tự, tác vụ lý luận thông thường bao gồm hai tập dữ liệu, cũng sử dụng các prompt giống nhau như được thể hiện trong Hình 3. Đối với các tác vụ khác, các prompt được sử dụng được chi tiết trong Bảng 1.

Đường cơ sở Chúng tôi chọn prompting zero-shot tiêu chuẩn, Zero-Shot-CoT (Kojima et al., 2022), và Few-Shot-CoT (Wei et al., 2022) làm đường cơ sở. Theo công việc trước đó (Kojima et al., 2022; Zhang et al., 2022), chúng tôi sử dụng giải mã tham lam cho tất cả thí nghiệm bằng cách đặt temperature thành 0, làm cho kết quả trở nên tất định. Xem thêm chi tiết trong Phụ lục A.3.

4.3 Kết quả và Phân tích
Kết quả đánh giá toàn diện được trình bày trong Bảng 2. Metric đánh giá là độ chính xác.

So sánh với Zero-Shot tiêu chuẩn Như được thể hiện trong Bảng 2, phương pháp role-play prompting của chúng tôi thể hiện hiệu suất vượt trội, vượt qua đường cơ sở zero-shot trong 10 trên 12 tập dữ liệu, và đạt được hiệu suất tương đương trong 2 tập dữ liệu còn lại (SingleEq và MultiArith). Xem xét tính đơn giản tương đối của các tập dữ liệu SingleEq và MultiArith, có thể hiểu rằng hiệu suất của mô hình đã tiến đến điểm bão hòa (vượt quá 97%), do đó tạo ra thách thức đáng kể cho phương pháp của chúng tôi để tăng cường thêm độ chính xác ở mức cao như vậy. Trong khi đạt được hiệu suất tương đương trong những tập dữ liệu cụ thể này, điều quan trọng là nhấn mạnh tính chất cạnh tranh của role-play prompting trên một loạt các tập dữ liệu phức tạp hơn đa dạng. Điều này mạnh mẽ chứng minh hiệu quả của role-play prompting trong một phạm vi rộng các tình huống ứng dụng.

So sánh với Zero-Shot-CoT Zero-Shot-CoT thêm "Let's think step by step" vào câu hỏi để kích thích chuỗi suy nghĩ (CoT) trong LLM, làm cho nó trở thành một phương pháp đơn giản nhưng hiệu quả để tăng cường khả năng lý luận của LLM. Tuy nhiên, khác với các LLM được hướng dẫn trước đó (Ouyang et al., 2022), các LLM đối thoại hiện tại đã trải qua fine-tuning có giám sát rộng rãi, cho phép chúng tự nhiên tạo ra CoT trong một số lĩnh vực dưới thiết lập zero-shot. Trong bối cảnh này, chúng tôi tiến hành phân tích so sánh phương pháp role-play prompting của chúng tôi với Zero-Shot-CoT. Kết quả thí nghiệm, cùng với khả năng tự nhiên tạo ra CoT của mô hình được trình bày trong Bảng 2. Lưu ý rằng đầu ra trực tiếp của câu trả lời hoặc một quá trình lý luận nhẹ không được coi là CoT. Nhìn chung, phương pháp của chúng tôi vượt trội hơn Zero-Shot-CoT trên 9 trong 12 tập dữ liệu. Trong các tác vụ (Letter, Coin, Object) mà ChatGPT gặp khó khăn trong việc tạo ra CoT một cách tự nhiên, cả hai đều có được cải thiện lớn. Thông qua nghiên cứu tình huống, chúng tôi phát hiện rằng role-play prompting cũng kích thích CoT trong mô hình giống như Zero-Shot-CoT. Một ví dụ được cung cấp trong Bảng 3. Trong nhiều tác vụ hơn mà CoT đã xảy ra, cả phương pháp của chúng tôi và Zero-Shot-CoT đều củng cố quá trình lý luận từng bước (các ví dụ được cung cấp trong Phụ lục B.1). Tuy nhiên, Zero-Shot-CoT không thể hiện hiệu quả đáng kể trong khi role-play prompting dẫn đến kết quả tốt hơn. Do đó, chúng tôi đưa ra giả thuyết rằng role-play prompting phục vụ như một trigger CoT ngầm và có thể tạo ra CoT hiệu quả hơn.

So sánh với Few-Shot-CoT Mặc dù phương pháp role-play prompting của chúng tôi hoàn toàn zero-shot, sự cải thiện mà nó mang lại gần như tương đương với Few-Shot-CoT, thậm chí vượt qua Few-Shot-CoT trên 6 trong 12 tập dữ liệu.

Theo công việc trước đó (Kojima et al., 2022; Wang et al., 2023), chúng tôi kết hợp phương pháp của chúng tôi và các đường cơ sở với Self-Consistency để chứng minh thêm hiệu quả của role-play prompting. Kết quả liên quan và thảo luận được cung cấp trong Phụ lục B.2.

4.4 Tác động của Thiết kế Prompt
Cấu trúc Prompt Để xác định cấu trúc prompt tối ưu, chúng tôi chọn tập dữ liệu AQuA và giao cho mô hình vai trò của một giáo viên toán. Sau đó chúng tôi tiến hành nghiên cứu ablation về thiết lập này để đánh giá một cách hệ thống tác động của các lựa chọn thiết kế khác nhau. Chúng tôi giả thuyết rằng các prompt đắm chìm mô hình sâu hơn trong vai trò của nó sẽ cải thiện hiệu suất. Do đó, chúng tôi thiết kế bốn nhóm prompt với mức độ đắm chìm tăng dần, như được thể hiện trong Bảng 4. Prompt 1 và 2 được thiết kế như các đối thoại một vòng, nơi chúng tôi trực tiếp gắn câu hỏi vào prompt và nhập nó vào mô hình để có được câu trả lời. Prompt 1 chỉ chứa vai trò cần đóng, và nó đã đạt được kết quả vượt qua đường cơ sở zero-shot. Đối với Prompt 2, chúng tôi tiếp tục tăng cường đắm chìm bằng cách thêm các mô tả bổ sung của vai trò và chỉ định các vai trò liên quan cho người dùng. Sự tăng cường này cải thiện thêm hiệu suất. Prompt 3 và 4 đều được thiết kế như các đối thoại hai vòng, như được mô tả trong phần trước. Bằng cách cho phép mô hình phản hồi với việc thiết lập vai trò được đưa ra, việc đắm chìm được tăng cường thêm, dẫn đến hiệu suất tốt nhất. Chúng tôi tiến hành các thí nghiệm tương tự trên các tập dữ liệu Letter và Coin, cho kết quả nhất quán (xem thêm chi tiết trong Phụ lục B.3). Do đó, chúng tôi khuyến nghị sử dụng cấu trúc prompt hai vòng với các mô tả bổ sung để tối đa hóa việc đắm chìm của mô hình, do đó mở khóa toàn bộ tiềm năng lý luận của role-play prompting.

Lựa chọn Vai trò Để đánh giá tác động của lựa chọn vai trò, chúng tôi kiểm tra trên các tập dữ liệu số học AQuA và SVAMP bằng cách sử dụng prompt đối thoại hai vòng. Chúng tôi thiết kế 8 vai trò khác nhau, được phân loại là có lợi thế, không liên quan, hoặc bất lợi dựa trên việc mỗi vai trò có giữ lợi thế trong tác vụ đã cho hay không. Hiệu suất của những vai trò này được chi tiết trong Bảng 5, trong khi các thiết kế prompt cụ thể có thể được tìm thấy trong Phụ lục D. Phù hợp với trực giác, các vai trò có lợi thế (1,2) chắc chắn đạt được kết quả tốt nhất, theo sau là các vai trò không liên quan (3-6) (đáng ngạc nhiên, hầu hết chúng đều vượt qua đường cơ sở zero-shot mặc dù không có lợi thế trong các tác vụ số học), và các vai trò bất lợi (7,8) đạt được kết quả tồi tệ nhất, kém hơn đường cơ sở zero-shot. Do đó, chúng tôi khuyến nghị chọn một vai trò giữ lợi thế trong tác vụ đã cho cho role-play prompting.

4.5 Thí nghiệm trên nhiều LLM hơn
Để đánh giá tính tổng quát của phương pháp role-play prompting của chúng tôi, chúng tôi tiến hành thí nghiệm bổ sung bằng cách sử dụng một số LLM đối thoại mã nguồn mở, bao gồm Llama 2-Chat (Touvron et al., 2023b) và Vicuna (Chiang et al., 2023), trên nhiều tập dữ liệu khác nhau như GSM8K, MultiArith, SVAMP, CSQA, và Letter. Các prompt và chiến lược giải mã được sử dụng nhất quán với các thí nghiệm ChatGPT trước đó. Kết quả được thể hiện trong Bảng 6, cho thấy role-play prompting cũng vượt qua đường cơ sở zero-shot trong các LLM đối thoại mã nguồn mở, thể hiện khả năng tổng quát tốt của role-play prompting.

Hơn nữa, chúng tôi kiểm tra tác động của quy mô mô hình bằng cách kiểm tra loạt Llama 2-Chat (7B, 13B, 70B) trên các tập dữ liệu GSM8K, MultiArith, và Letter. Như Hình 4 minh họa, cả ba kích thước mô hình đều đạt được hiệu suất cải thiện từ role-play prompting. Những lợi ích nhất quán trên 7B đến 70B tham số cho thấy hiệu quả độc lập với quy mô, trong phạm vi này.

5 Kết luận
Trong bài báo này, chúng tôi đã đề xuất một phương pháp role-play prompting zero-shot mới bao gồm khung hai giai đoạn, nhằm tăng cường khả năng lý luận của LLM. Đánh giá rộng rãi trên mười hai tiêu chuẩn được sử dụng rộng rãi tiết lộ rằng phương pháp của chúng tôi vượt trội hơn cả đường cơ sở zero-shot tiêu chuẩn và Zero-Shot-CoT trên hầu hết các tập dữ liệu. Những kết quả này làm nổi bật tiềm năng của role-play prompting như một trigger CoT ngầm và hiệu quả, dẫn đến kết quả lý luận tăng cường. Nhìn chung, công việc này đặt nền móng ban đầu để thúc đẩy điều tra sâu hơn về giao điểm của đóng vai và lý luận trong cộng đồng LLM, một hướng nghiên cứu đầy hứa hẹn để phát triển kỹ năng lý luận.

Hạn chế
Cốt lõi của phương pháp role-play prompting của chúng tôi nằm ở việc thiết kế prompt thiết lập vai trò và phản hồi vai trò. Trong khi chúng tôi đã thiết kế thủ công và lấy mẫu một số prompt, cho kết quả vượt trội so với đường cơ sở zero-shot, quá trình này tốn thời gian và có thể không luôn đảm bảo kết quả tối ưu. Để giải quyết hạn chế này, nghiên cứu tương lai có thể tập trung vào việc cho phép LLM tự động chọn các vai trò phù hợp và thiết kế prompt dựa trên câu hỏi đã cho. Phương pháp này có thể mở rộng thêm ứng dụng của role-play prompting cho một phạm vi rộng hơn các lĩnh vực ngoài lý luận.

Lời cảm ơn
Công việc được hỗ trợ bởi Chương trình R&D Chủ chốt Quốc gia của Trung Quốc (Số 2022ZD0116307), Quỹ Khoa học Tự nhiên Quốc gia của Trung Quốc (Số 62271270) và Được tài trợ bởi Quỹ Nghiên cứu Đại dương Xanh CCF-Lenovo.

[Phần tài liệu tham khảo và phụ lục được dịch tiếp theo...]
