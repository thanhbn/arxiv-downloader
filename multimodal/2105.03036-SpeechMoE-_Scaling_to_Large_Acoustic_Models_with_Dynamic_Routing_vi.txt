# 2105.03036.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2105.03036.pdf
# Kích thước tệp: 358773 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SpeechMoE: Mở rộng quy mô đến các mô hình âm thanh lớn với Hỗn hợp chuyên gia định tuyến động
Zhao You1, Shulin Feng1, Dan Su1, Dong Yu2
1Phòng thí nghiệm AI Tencent, Thâm Quyến, Trung Quốc
2Phòng thí nghiệm AI Tencent, Bellevue, WA, Hoa Kỳ
{dennisyou, shulinfeng, dansu, dyu}@tencent.com
Tóm tắt
Gần đây, Transformer dựa trên Hỗn hợp Chuyên gia (MoE) đã cho thấy kết quả đầy hứa hẹn trong nhiều lĩnh vực. Điều này phần lớn là do các ưu điểm sau của kiến trúc này: thứ nhất, Transformer dựa trên MoE có thể tăng khả năng của mô hình mà không làm tăng chi phí tính toán cả ở thời gian huấn luyện và suy luận. Bên cạnh đó, Transformer dựa trên MoE là một mạng động có thể thích ứng với độ phức tạp khác nhau của các trường hợp đầu vào trong các ứng dụng thực tế. Trong nghiên cứu này, chúng tôi khám phá mô hình dựa trên MoE cho nhận dạng giọng nói, có tên là SpeechMoE. Để kiểm soát thêm tính thưa thớt của kích hoạt bộ định tuyến và cải thiện tính đa dạng của các giá trị cổng, chúng tôi đề xuất một hàm mất mát L1 thưa thớt và một hàm mất mát tầm quan trọng trung bình tương ứng. Ngoài ra, một kiến trúc bộ định tuyến mới được sử dụng trong SpeechMoE có thể đồng thời tận dụng thông tin từ một mạng nhúng chia sẻ và biểu diễn phân cấp của các lớp MoE khác nhau. Kết quả thực nghiệm cho thấy SpeechMoE có thể đạt được tỷ lệ lỗi ký tự (CER) thấp hơn với chi phí tính toán tương đương so với các mạng tĩnh truyền thống, cung cấp 7,0% đến 23,0% cải thiện CER tương đối trên bốn bộ dữ liệu đánh giá.
Từ khóa chỉ mục: hỗn hợp chuyên gia, định tuyến động, mô hình âm thanh, nhận dạng giọng nói

1. Giới thiệu
Nhờ vào khả năng biểu diễn mạnh mẽ, Mạng Nơ-ron Sâu (DNN) đã đạt được thành công lớn trong nhận dạng giọng nói [1, 2]. Nhiều loại kiến trúc mạng nơ-ron khác nhau đã được sử dụng trong các hệ thống ASR, như mạng nơ-ron tích chập (CNN) [3, 4], bộ nhớ dài-ngắn hạn (LSTM) [5], đơn vị hồi quy có cổng [6], mạng nơ-ron trễ thời gian [7], mạng bộ nhớ tuần tự truyền thẳng (FSMN) [8], v.v. Gần đây, các mô hình sâu mạnh mẽ hơn như Transformer [9], Emformer [10] và Conformer [11] đã chứng minh hiệu quả của chúng để cải thiện thêm hiệu suất nhận dạng giọng nói.

Việc tăng kích thước mô hình và dữ liệu huấn luyện đã được chứng minh là một cách hiệu quả để cải thiện hiệu suất hệ thống, điều này đặc biệt được thể hiện trong lĩnh vực mô hình hóa ngôn ngữ [12, 13]. Gần đây, các phương pháp dựa trên hỗn hợp chuyên gia sâu (MoE) [14, 15] đã được nghiên cứu tích cực và áp dụng trong các tác vụ khác nhau như mô hình hóa ngôn ngữ [16, 17] và phân loại hình ảnh [18, 19, 20, 21]. Lợi ích chủ yếu đến từ hai khía cạnh: Thứ nhất, MoE là một cách hiệu quả để tăng khả năng của mô hình. Thứ hai, với việc giới thiệu lớp hỗn hợp chuyên gia có cổng thưa thớt [22], một tính chất hấp dẫn của các mô hình MoE là định tuyến động thưa thớt, cho phép chúng ta đáp ứng hiệu quả huấn luyện và suy luận bằng cách có một mạng con được kích hoạt trên cơ sở từng ví dụ.

*Đóng góp bằng nhau.

Trong các ứng dụng thực tế, các hệ thống nhận dạng giọng nói cần phải mạnh mẽ với các điều kiện đầu vào khác nhau như người nói, kênh ghi âm và môi trường âm thanh. Các mô hình lớn hơn rất hấp dẫn trong khi việc tăng chi phí huấn luyện và suy luận không thể chấp nhận được. Vấn đề chính là chi phí tính toán của một mô hình tĩnh là cố định và không thể thích ứng với độ phức tạp khác nhau của các trường hợp đầu vào. Do đó, việc phát triển các mô hình hỗn hợp chuyên gia cho nhận dạng giọng nói với cơ chế định tuyến động là một khám phá đầy hứa hẹn.

Trong nghiên cứu này, chúng tôi khám phá phương pháp hỗn hợp chuyên gia cho nhận dạng giọng nói. Chúng tôi đề xuất một kiến trúc hỗn hợp chuyên gia định tuyến động mới, tương tự như [17], bao gồm một tập hợp các chuyên gia và một mạng bộ định tuyến. Bộ định tuyến lấy đầu ra của lớp trước làm đầu vào và định tuyến nó đến mạng chuyên gia được xác định tốt nhất. Chúng tôi thấy rằng hàm mất mát cân bằng được đề xuất trong [17] đạt được định tuyến cân bằng nhưng tính thưa thớt của kích hoạt bộ định tuyến không thể luôn được đảm bảo. Ở đây, chúng tôi đề xuất một hàm mất mát L1 thưa thớt để khuyến khích kích hoạt bộ định tuyến trở nên thưa thớt cho mỗi ví dụ. Bên cạnh đó, chúng tôi sử dụng một hàm mất mát tầm quan trọng trung bình để cải thiện thêm sự cân bằng của việc sử dụng chuyên gia. Hơn nữa, một mạng nhúng chia sẻ được sử dụng trong kiến trúc của chúng tôi để cải thiện các quyết định định tuyến, đầu ra của nó sẽ được kết hợp với đầu ra của các lớp trước đó làm đầu vào của các bộ định tuyến.

Phần còn lại của bài báo được tổ chức như sau. Phần 2 xem xét các công trình liên quan của MoE và Phần 3 trình bày phương pháp được đề xuất của chúng tôi SpeechMoE. Thiết lập thực nghiệm được mô tả trong Phần 4 và kết quả thực nghiệm được báo cáo trong Phần 5. Cuối cùng, chúng tôi kết luận bài báo này trong Phần 6.

2. Các công trình liên quan
Trong phần này, chúng tôi chủ yếu mô tả hai kiến trúc khác nhau của MoE.

2.1. DeepMoE
Kiến trúc DeepMoE được đề xuất trong [20] có thể đạt được chi phí tính toán thấp hơn và độ chính xác dự đoán cao hơn so với các mạng tích chập tiêu chuẩn. Kiến trúc thiết kế một mạng cổng thưa thớt có thể lựa chọn động và tái trọng số các kênh trong mỗi lớp của mạng tích chập cơ sở. Hình 1(a) cho thấy kiến trúc chi tiết của DeepMoE. DeepMoE bao gồm một mạng tích chập cơ sở, một mạng nhúng chia sẻ và một mạng cổng thưa thớt đa đầu. Mạng cổng chuyển đổi đầu ra của mạng nhúng chia sẻ thành trọng số hỗn hợp thưa thớt:

gl(e) = f(Wl_g e)                                    (1)

trong đó gl(e) là trọng số hỗn hợp thưa thớt của lớp tích chập thứ l, e là đầu ra của mạng nhúng chia sẻ, và f là phép toán kích hoạt (tức là Relu). Sau đó, đầu ra của lớp thứ l

arXiv:2105.03036v1 [cs.SD] 7 Tháng 5 2021

--- TRANG 2 ---
(a)      (b)      (c)

[Hình 1: (a), (b) và (c) đại diện cho kiến trúc của DeepMoE, Switch Transformer và SpeechMoE tương ứng. Tương tự như Switch Transformer, chỉ một chuyên gia có xác suất bộ định tuyến lớn nhất trong mỗi lớp MoE được sử dụng trong SpeechMoE, điều này khác với DeepMoE. Bên cạnh đó, SpeechMoE sử dụng một mạng nhúng chia sẻ và đầu ra của lớp trước đó làm đầu vào của mỗi bộ định tuyến.]

có thể được công thức hóa như sau:
yl = Σ(i=1 đến n) gl_i El_i                          (2)

trong đó n là số kênh đầu vào của lớp tích chập thứ l và El_i là kênh thứ i của lớp tích chập thứ l, được coi như chuyên gia thứ i trong lớp thứ l.

Hàm mất mát để huấn luyện DeepMoE được định nghĩa là:
L(x;y) = Lb(x;y) + Lg(x;y) + Le(x;y)               (3)

trong đó x và y lần lượt là đặc trưng hình ảnh đầu vào và nhãn mục tiêu. Lb là hàm mất mát phân loại, Lg là số hạng chính quy hóa L1 điều khiển tính thưa thớt của mạng cổng và Le là hàm mất mát phân loại bổ sung khuyến khích tính đa dạng của mạng nhúng chia sẻ.

2.2. Switch Transformer
Fedus và cộng sự đã đề xuất Switch Transformer [17] cho mô hình hóa ngôn ngữ, giảm thêm chi phí tính toán và truyền thông bằng cách đơn giản hóa thuật toán định tuyến MoE. Kiến trúc của Switch Transformer được mô tả trong Hình 1(b), trong đó các chuyên gia đề cập đến các mạng truyền thẳng và các lớp không phải chuyên gia đề cập đến các lớp tự chú ý. Mỗi lớp MoE bao gồm n chuyên gia và một lớp bộ định tuyến. Nó lấy đầu ra cho lớp trước đó làm đầu vào và định tuyến nó đến chuyên gia top-1 có xác suất bộ định tuyến lớn nhất. Gọi Wl_r và ol-1 lần lượt là trọng số bộ định tuyến của lớp thứ l và đầu ra của lớp trước đó, thì xác suất bộ định tuyến có thể được định nghĩa như sau:

rl = Wl_r ol-1                                      (4)
pl_i = exp(rl_i) / Σ(j=1 đến n) exp(rl_j)          (5)

Sau đó, đầu ra của chuyên gia được chọn cũng được cổng bởi xác suất bộ định tuyến để có đầu ra của lớp MoE,
yl = pl_i El_i                                      (6)

Vì chỉ có một chuyên gia hoạt động trong mỗi lớp, Switch Transformer có thể giữ chi phí tính toán không đổi trong khi mở rộng quy mô đến một mô hình rất lớn. Để khuyến khích một tải cân bằng trên các chuyên gia, hàm mất mát cân bằng [17] được thêm vào hàm mất mát và được định nghĩa là:

Lb = n Σ(i=1 đến n) si Pi                           (7)

trong đó si là phần của các mẫu được gửi đến chuyên gia i, Pi là phần của xác suất bộ định tuyến được phân bổ cho chuyên gia i.

3. SpeechMoE

3.1. Kiến trúc mô hình
Hình 1(c) cho thấy tổng quan về kiến trúc của SpeechMoE được đề xuất của chúng tôi. Đối với nhận dạng giọng nói, đầu vào của nó là các đặc trưng giọng nói (ví dụ: fbank) và các khung đầu vào sẽ được gửi đến các chuyên gia trong mỗi lớp. Tương tự như Switch Transformer, SpeechMoE chỉ chọn một chuyên gia trong mỗi lớp để giảm chi phí tính toán. So với Switch Transformer và DeepMoE, SpeechMoE nối chuỗi nhúng chia sẻ với đầu ra của lớp trước đó làm đầu vào của các bộ định tuyến, có thể được định nghĩa là:

rl = Wl_r Concat(e; ol-1)                           (8)

Cơ chế bộ định tuyến này đến từ hai suy nghĩ: (1) Tất cả các giá trị cổng trong DeepMoE đều được điều khiển bởi nhúng chia sẻ, có thể suy giảm thành các kết quả cổng tương tự trong mỗi lớp. Việc sử dụng biểu diễn phân cấp từ đầu ra của mỗi lớp có thể dẫn đến kết quả định tuyến đa dạng cho SpeechMoE. (2) Nhúng chia sẻ liên quan đến tác vụ mục tiêu có thể hữu ích để có được chiến lược định tuyến tốt hơn, cung cấp biểu diễn phân biệt cấp cao và làm cho các chuyên gia chuyên biệt để xử lý các khung đầu vào riêng biệt.

3.2. Mục tiêu huấn luyện

3.2.1. Hàm mất mát L1 thưa thớt
Trong nghiên cứu của chúng tôi, chúng tôi thấy rằng phân phối xác suất bộ định tuyến có xu hướng đồng đều khi chúng tôi chỉ sử dụng hàm mất mát cân bằng được đề xuất trong [17], dẫn đến hiệu suất kém. Để khuyến khích tính thưa thớt của kích hoạt bộ định tuyến, chúng tôi đề xuất một hàm mất mát L1 thưa thớt, được định nghĩa như sau:

Ls = (1/m) Σ(i=1 đến m) ||f̂i||1                    (9)

trong đó f̂i = fi/||fi||2, đại diện cho phân phối xác suất bộ định tuyến được chuẩn hóa đơn vị của mẫu i, và m là số lượng mẫu trong lô nhỏ này. Do chuẩn hóa đơn vị, việc tối thiểu hóa chuẩn L1 sẽ buộc phân phối gần với các trục không gian và đạt được tính thưa thớt.

3.2.2. Hàm mất mát tầm quan trọng trung bình
Chúng tôi cũng quan sát thấy rằng mô hình không đủ cân bằng khi tăng số lượng chuyên gia. Để giải quyết vấn đề này, chúng tôi sử dụng một hàm mất mát tầm quan trọng được sửa đổi [22] để thay thế hàm mất mát cân bằng, được định nghĩa như sau:

Imp = (1/m) Σ(i=1 đến m) pi                         (10)
Lm = Σ(j=1 đến n) (Impj)²                          (11)

Tầm quan trọng trung bình được định nghĩa là kích hoạt trung bình của các chuyên gia trên lô mẫu và hàm mất mát được định nghĩa là tổng bình phương của tầm quan trọng trung bình của mỗi chuyên gia. Rõ ràng là khi tầm quan trọng trung bình của mỗi chuyên gia được tính trung bình 1/n, hàm mất mát đạt mức tối thiểu. So với hàm mất mát cân bằng trong đó si không thể vi phân được, hàm mất mát tầm quan trọng trung bình mượt mà hơn, dẫn đến chiến lược định tuyến cân bằng hơn.

3.2.3. Hàm mất mát
Với đầu vào x và mục tiêu y, hàm mất mát đầy đủ của phương pháp chúng tôi được định nghĩa là

L(x;y) = Lr(x;y) + αLs(x) + βLm(x) + γLe(x;y)      (12)

Trong số các mục này, Lr là hàm mất mát CTC [23] cho nhận dạng giọng nói, Ls và Lm là hàm mất mát L1 thưa thớt và hàm mất mát tầm quan trọng trung bình đã đề cập, được sử dụng để khuyến khích tính thưa thớt và đa dạng của mô hình SpeechMoE. Tương tự như [20], chúng tôi giới thiệu một hàm mất mát nhúng bổ sung Le, cũng là hàm mất mát CTC. Nó chia sẻ cùng mục tiêu với mô hình SpeechMoE của chúng tôi và cung cấp các nhúng đáng tin cậy cho các bộ định tuyến. α, β và γ lần lượt là tỷ lệ cho Ls, Lm và Le.

4. Thiết lập thực nghiệm

4.1. Thiết lập huấn luyện
Các đặc trưng giọng nói được sử dụng trong tất cả các thực nghiệm là các đặc trưng ngân hàng lọc log-Mel 40 chiều được nối với đạo hàm bậc nhất và bậc hai. Các đặc trưng ngân hàng lọc log-mel được tính toán với cửa sổ 25ms và dịch chuyển mỗi 10ms. Chúng tôi xếp chồng 8 khung liên tiếp và lấy mẫu phụ các khung đầu vào với 3. Một chuẩn hóa trung bình và phương sai toàn cục được áp dụng cho mỗi khung. Tất cả các thực nghiệm đều dựa trên khung học tập CTC. Chúng tôi sử dụng phương pháp mô hình hóa âm thanh dựa trên âm tiết CI [24] cho học tập CTC. Các nhãn mục tiêu của học tập CTC được định nghĩa để bao gồm 1394 âm tiết tiếng Quan Thoại, 39 âm tiếng Anh và một khoảng trống. Kết quả tỷ lệ lỗi ký tự được đo trên các tập kiểm tra và các phép toán dấu phẩy động (FLOP) cho một ví dụ một giây được sử dụng để đánh giá chi phí tính toán suy luận. Chúng tôi sử dụng một mô hình ngôn ngữ 5-gram đã được cắt tỉa, pass đầu tiên. Tất cả các hệ thống sử dụng một từ vựng bao gồm hàng triệu từ. Giải mã được thực hiện với thuật toán tìm kiếm chùm tia bằng cách sử dụng các bộ chuyển đổi trạng thái hữu hạn có trọng số (WFST).

4.2. Bộ dữ liệu
Kho dữ liệu huấn luyện của chúng tôi là các bộ dữ liệu hỗn hợp được thu thập từ nhiều lĩnh vực ứng dụng khác nhau, tất cả đều bằng tiếng Quan Thoại. Để cải thiện tính mạnh mẽ của hệ thống, một tập hợp các phản ứng xung phòng mô phỏng (RIR) được tạo ra với các kích thước phòng hình chữ nhật khác nhau, vị trí người nói và vị trí micrô, như được đề xuất trong [25]. Tổng cộng, nó tạo thành một kho dữ liệu huấn luyện 10k giờ.

Để đánh giá hiệu suất của phương pháp được đề xuất của chúng tôi, chúng tôi báo cáo hiệu suất trên 3 loại tập kiểm tra bao gồm các phát ngôn ẩn danh được phiên âm thủ công được trích xuất từ giọng nói đọc (1001 phát ngôn), giọng nói đối thoại (1665 phát ngôn) và giọng nói tự phát (2952 phát ngôn). Chúng tôi gọi chúng lần lượt là Read, Chat và Spon. Ngoài ra, để cung cấp một điểm chuẩn công cộng, chúng tôi cũng sử dụng tập phát triển AISHELL-2 (2500 phát ngôn) được ghi bởi micrô độ trung thực cao làm tập kiểm tra.

4.3. Mô hình âm thanh
Các mô hình âm thanh của chúng tôi bao gồm bốn thành phần: lớp MoE, lớp bộ nhớ tuần tự [26], lớp tự chú ý [27] và lớp softmax đầu ra. Mỗi lớp MoE bao gồm một bộ định tuyến và một tập hợp các chuyên gia là một mạng truyền thẳng với một lớp ẩn kích thước 1024 được kích hoạt bởi ReLU và một lớp chiếu kích thước 512. Đối với lớp bộ nhớ tuần tự, thứ tự nhìn lại và thứ tự nhìn trước của mỗi khối bộ nhớ lần lượt là 5 và 1, và các bước nhảy lần lượt là 2 và 1. Đối với lớp tự chú ý, chúng tôi đặt chiều mô hình d = 512 và số đầu h = 8. Đối với mọi lớp ngoại trừ lớp softmax đầu ra, kết nối dư được áp dụng.

Xương sống của mô hình chúng tôi bao gồm 30 lớp MoE, 30 lớp bộ nhớ tuần tự và 3 lớp tự chú ý. Mỗi lớp MoE được theo sau bởi một lớp bộ nhớ tuần tự, và một lớp tự chú ý được chèn sau mỗi 10 lớp MoE và bộ nhớ tuần tự liên tiếp. Trong các thực nghiệm của chúng tôi, chúng tôi thay đổi số lượng chuyên gia của các lớp MoE là 2, 4 và 8, được đánh dấu lần lượt là MoE-2e, MoE-4e và MoE-8e. Mạng nhúng chia sẻ là một mô hình tĩnh không có lớp MoE nhưng có cấu trúc tương tự như xương sống.

Trong nghiên cứu của chúng tôi, chúng tôi đã xây dựng hai hệ thống cơ sở để đánh giá hiệu suất của phương pháp được đề xuất của chúng tôi:
- Cơ sở 1 (B1): Mô hình tĩnh không có lớp MoE nhưng có cấu trúc tương tự như xương sống của các mô hình SpeechMoE, cũng có thể được coi như MoE-1e. Vì phương pháp được đề xuất sử dụng một mạng nhúng bổ sung, mô hình B1 được thiết kế có 60 lớp để khớp FLOP với các mô hình MoE của chúng tôi.
- Cơ sở 2 (B2): Mô hình với 4 chuyên gia trong mỗi lớp MoE, không có mạng nhúng chia sẻ và được huấn luyện chỉ với hàm mất mát cân bằng phụ trợ được đề xuất trong Switch Transformer.

Đối với tất cả các thực nghiệm trên các mô hình MoE, chúng tôi đặt các siêu tham số α = 0,1, β = 0,1 và γ = 0,01.

--- TRANG 4 ---
Bảng 1: Kết quả của việc thêm hàm mất mát L1 thưa thớt.
[THIS IS TABLE: Shows model comparison with Params, FLOPs, and test results for Read, Chat, Spon, AISHELL]
Mô hình    Params  FLOPs   Tập kiểm tra
                          Read  Chat  Spon  AISHELL
B1         71M     2.3B    2.0   22.92 24.95 4.52
B2         134M    2.3B    1.81  22.49 24.90 4.50
MoE-L1     134M    2.3B    1.69  22.47 24.70 4.25

Bảng 2: Kết quả của việc tăng cường mạng nhúng chia sẻ và sử dụng hàm mất mát tầm quan trọng trung bình.
[THIS IS TABLE: Shows model comparison with parameters and test results]
Mô hình     Params  FLOPs   Tập kiểm tra
                           Read  Chat  Spon  AISHELL
MoE-L1      134M    2.3B    1.69  22.47 24.70 4.25
+emb        170M    2.3B    1.63  22.15 24.15 4.16
+imp loss   170M    2.3B    1.58  21.57 23.31 4.00

5. Kết quả thực nghiệm

5.1. Thêm hàm mất mát L1 thưa thớt
Trong phần này, chúng tôi điều tra hiệu suất của việc thêm hàm mất mát L1 thưa thớt trong huấn luyện. Chúng tôi đã huấn luyện hai hệ thống cơ sở cho đánh giá này. Hệ thống cơ sở đầu tiên (B1) là mô hình tĩnh được huấn luyện dựa trên hàm mất mát Lr và hệ thống kia (B2) được huấn luyện dựa trên hàm mất mát Lr và Lb đã đề cập ở trên. Kết quả của chúng tôi về việc thêm hàm mất mát L1 thưa thớt so với B2 được đánh dấu là MoE-L1.

Như thể hiện trong bảng 1, B2 hoạt động tốt hơn một chút so với B1 với nhiều tham số hơn và chi phí tính toán tương đương. Như mong đợi rằng MoE-L1 sử dụng cả hàm mất mát cân bằng và hàm mất mát L1 thưa thớt đạt được hiệu suất tốt nhất so với hai hệ thống cơ sở. Điều này cho thấy rằng hàm mất mát L1 thưa thớt bổ sung mang lại tính thưa thớt hơn cho phân phối xác suất bộ định tuyến. Các bộ định tuyến trở nên riêng biệt và chuyên biệt hơn cho các khung đầu vào khác nhau để mô hình có được hiệu suất tốt hơn.

5.2. Tăng cường mạng nhúng chia sẻ
Trong phần này, chúng tôi đánh giá hiệu suất của kiến trúc bộ định tuyến mới nối chuỗi nhúng chia sẻ với đầu ra của lớp trước đó làm đầu vào của bộ định tuyến. Như có thể quan sát trong bảng 2, kiến trúc bộ định tuyến được đề xuất đạt được tỷ lệ lỗi ký tự thấp hơn so với mô hình MoE-L1.

Đáng chú ý là chỉ sử dụng đầu ra của lớp trước đó làm đầu vào không hoạt động tốt lắm, điều này mâu thuẫn với phương pháp được sử dụng trong [17]. Một lời giải thích hợp lý là đối với mô hình hóa ngôn ngữ, đầu vào từ như biểu diễn cấp cao đã có sự phân biệt tốt, trong khi đối với nhận dạng giọng nói, đầu vào phổ là đặc trưng cấp thấp không thể cung cấp đủ thông tin phân biệt cho các bộ định tuyến, vì vậy mạng nhúng chia sẻ chuyển đổi đặc trưng cấp thấp thành nhúng cấp cao, là cần thiết để giúp bộ định tuyến đạt được hiệu ứng lựa chọn tốt hơn.

5.3. Sử dụng hàm mất mát tầm quan trọng trung bình
Dòng cuối của bảng 2 trình bày các hiệu ứng của hàm mất mát tầm quan trọng trung bình thay cho hàm mất mát cân bằng. Chúng tôi quan sát thấy rằng hàm mất mát được đề xuất có thể đạt được tỷ lệ lỗi ký tự thấp hơn nữa so với mô hình MoE-L1 với mạng nhúng trên bốn tập kiểm tra. Vì hàm mất mát tầm quan trọng trung bình khuyến khích tất cả các chuyên gia có tầm quan trọng bằng nhau, nó sẽ giúp các bộ định tuyến phân phối các khung đầu vào cho các chuyên gia một cách cân bằng, tránh tình huống một số chuyên gia không nhận được mẫu nào để huấn luyện. Do đó, các chuyên gia sẽ đa dạng hơn và dẫn đến hiệu suất tốt hơn.

[Hình 2: Hàm mất mát CTC xác thực cho việc tăng số lượng chuyên gia]

Bảng 3: Kết quả của việc tăng số lượng chuyên gia.
[THIS IS TABLE: Shows results for different numbers of experts]
Mô hình    Params  FLOPs   Tập kiểm tra
                          Read    Chat    Spon    AISHELL
B1         71M     2.3B    2.0     22.92   24.95   4.52
MoE-2e     105M    2.3B    1.62    21.82   23.52   4.08
MoE-4e     170M    2.3B    1.58    21.57   23.31   4.00
MoE-8e     297M    2.3B    1.54    21.31   22.97   3.98
                           (-23.0%) (-7.0%) (-7.9%) (-11.9%)

5.4. Tăng số lượng chuyên gia
Trong phần này, chúng tôi điều tra hiệu ứng của việc tăng số lượng chuyên gia. Bảng 3 cho thấy so sánh hiệu suất trên các số lượng chuyên gia khác nhau với SpeechMoE. Dòng 2 trình bày kết quả của hệ thống cơ sở (B1). Ba dòng tiếp theo trình bày kết quả của 3 số lượng chuyên gia khác nhau được đánh dấu lần lượt là MoE-2e, MoE-4e và MoE-8e. Kết quả cho thấy rõ ràng rằng hiệu suất trở nên tốt hơn khi số lượng chuyên gia tăng. Cụ thể, MoE-8e đạt được tới 23,0% cải thiện CER tương đối so với mô hình cơ sở trên tập kiểm tra Read, và mức tăng là từ 7,0% đến 11,9% cho các tập kiểm tra thực tế khác.

Hình 2 cho thấy hàm mất mát CTC xác thực của MoE với số lượng chuyên gia khác nhau và mô hình cơ sở. Như thể hiện, mô hình MoE-8e tạo ra hàm mất mát CTC thấp nhất so với cả mô hình cơ sở và các mô hình SpeechMoE khác. Hơn nữa, chúng tôi quan sát thấy rằng có nhiều chuyên gia hơn sẽ tăng tốc huấn luyện. Điều này cho thấy rằng việc tăng số lượng chuyên gia dẫn đến các mô hình mạnh mẽ hơn.

6. Kết luận và công việc tương lai
Trong bài báo này, chúng tôi khám phá một phương pháp hỗn hợp chuyên gia cho nhận dạng giọng nói. Chúng tôi đề xuất một kiến trúc mô hình âm thanh định tuyến động mới, mô-đun bộ định tuyến được tăng cường bằng cách kết hợp đầu ra của lớp trước đó và nhúng từ một mạng nhúng độc lập. Chúng tôi cũng cải thiện hàm mất mát huấn luyện có thể đạt được cả tính thưa thớt tốt hơn và cân bằng giữa các chuyên gia khác nhau. Các thực nghiệm toàn diện được tiến hành về huấn luyện với hàm mất mát khác nhau và số lượng chuyên gia thay đổi. Công việc tương lai bao gồm cả việc mở rộng quy mô dữ liệu huấn luyện và số lượng chuyên gia, tăng từ một đến hai bậc độ lớn, và khám phá mô hình SpeechMoE được đề xuất với khung huấn luyện đầu cuối khác như bộ chuyển đổi transformer.

--- TRANG 5 ---
7. Tài liệu tham khảo
[1] G. E. Dahl, D. Yu, L. Deng, and A. Acero, "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition," in IEEE Transactions on audio, speech, and language processing, vol. 20. IEEE, 2012, p. 30–42.

[2] D. Yu and J. Li, "Recent progresses in deep learning based acoustic models," in IEEE/CAA Journal of Automatica Sinica, vol. 4. IEEE, 2017, p. 396–409.

[3] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, "Deep convolutional neural networks for lvcsr," in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 8614–8618.

[4] Y. Qian and P. C. Woodland, "Very deep convolutional neural networks for robust speech recognition," in 2016 IEEE Spoken Language Technology Workshop (SLT), 2016, pp. 481–488.

[5] A. Graves, N. Jaitly, and A.-r. Mohamed, "Hybrid speech recognition with deep bidirectional lstm," in 2013 IEEE workshop on automatic speech recognition and understanding. IEEE, 2013, pp. 273–278.

[6] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio, "Light gated recurrent units for speech recognition," IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 2, no. 2, pp. 92–102, 2018.

[7] V. Peddinti, D. Povey, and S. Khudanpur, "A time delay neural network architecture for efficient modeling of long temporal contexts," in Sixteenth Annual Conference of the International Speech Communication Association, 2015.

[8] S. Zhang, M. Lei, Z. Yan, and L. Dai, "Deep-fsmn for large vocabulary continuous speech recognition," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5869–5873.

[9] L. Dong, S. Xu, and B. Xu, "Speech-transformer: A no-recurrence sequence-to-sequence model for speech recognition," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5884–5888.

[10] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, "Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition," arXiv e-prints, p. arXiv:2010.10759, Oct. 2020.

[11] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, "Conformer: Convolution-augmented transformer for speech recognition," 2020.

[12] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019.

[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.

[14] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, "Adaptive mixtures of local experts," Neural computation, vol. 3, no. 1, pp. 79–87, 1991.

[15] M. I. Jordan and R. A. Jacobs, "Hierarchical mixtures of experts and the em algorithm," Neural computation, vol. 6, no. 2, pp. 181–214, 1994.

[16] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020.

[17] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," arXiv preprint arXiv:2101.03961, 2021.

[18] S. Gross, M. Ranzato, and A. Szlam, "Hard mixtures of experts for large scale weakly supervised vision," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 6865–6873.

[19] K. Ahmed, M. H. Baig, and L. Torresani, "Network of experts for large-scale image categorization," in European Conference on Computer Vision. Springer, 2016, pp. 516–532.

[20] X. Wang, F. Yu, L. Dunlap, Y.-A. Ma, R. Wang, A. Mirhoseini, T. Darrell, and J. E. Gonzalez, "Deep mixture of experts via shallow embedding," in Uncertainty in Artificial Intelligence. PMLR, 2020, pp. 552–562.

[21] S. Cai, Y. Shu, and W. Wang, "Dynamic routing networks," in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 3588–3597.

[22] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.

[23] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376.

[24] Z. Qu, P. Haghani, E. Weinstein, and P. Moreno, "Syllable-based acoustic modeling with ctc-smbr-lstm," in Automatic Speech Recognition and Understanding Workshop (ASRU), 2017 IEEE. IEEE, 2017, pp. 173–177.

[25] I. Himawan, P. Motlicek, D. Imseng, B. Potard, N. Kim, and J. Lee, "Learning feature mapping using deep neural network bottleneck features for distant large vocabulary speech recognition," in International Conference on Acoustics, Speech and Signal Processing, 2015.

[26] S. Zhang, M. Lei, Z. Yan, and L. Dai, "Deep-fsmn for large vocabulary continuous speech recognition," in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5869–5873.

[27] Z. You, D. Su, J. Chen, C. Weng, and D. Yu, "Dfsmn-san with persistent memory model for automatic speech recognition," in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7704–7708.
