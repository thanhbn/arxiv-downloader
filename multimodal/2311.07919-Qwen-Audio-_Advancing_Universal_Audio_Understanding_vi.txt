Loại | Nhiệm vụ | Mô tả | Giờ
---|---|---|---
Giọng nói | ASR | Nhận dạng giọng nói tự động (đa ngôn ngữ) | 30k
| S2TT | Dịch giọng nói sang văn bản | 3.7k
| OSR | Nhận dạng giọng nói chồng lấp | <1k
| Dialect ASR | Nhận dạng giọng nói phương ngữ tự động | 2k
| SRWT | Nhận dạng giọng nói tiếng Anh với dấu thời gian cấp từ | 10k
| | Nhận dạng giọng nói tiếng Quan Thoại với dấu thời gian cấp từ | 11k
| DID | Nhận dạng phương ngữ | 2k
| LID | Nhận dạng ngôn ngữ nói | 11.7k
| SGC | Nhận dạng giới tính người nói (sinh học) | 4.8k
| ER | Nhận dạng cảm xúc | <1k
| SV | Xác minh người nói | 1.2k
| SD | Phân tách người nói | <1k
| SER | Nhận dạng thực thể giọng nói | <1k
| KS | Phát hiện từ khóa | <1k
| IC | Phân loại ý định | <1k
| SF | Điền khe | <1k
| SAP | Dự đoán tuổi người nói | 4.8k
| VSC | Phân loại âm thanh giọng | <1k
Âm thanh | AAC | Mô tả âm thanh tự động | 8.4k
| SEC | Phân loại sự kiện âm thanh | 5.4k
| ASC | Phân loại cảnh âm thanh | <1k
| SED | Phát hiện sự kiện âm thanh với dấu thời gian | <1k
| AQA | Hỏi đáp âm thanh | <1k
Âm nhạc & Bài hát | SID | Nhận dạng ca sĩ | <1k
| SMER | Nhận dạng cảm xúc ca sĩ và âm nhạc | <1k
| MC | Mô tả âm nhạc | 25k
| MIC | Phân loại nhạc cụ | <1k
| MNA | Phân tích nốt nhạc như cao độ, vận tốc | <1k
| MGR | Nhận dạng thể loại nhạc | 9.5k
| MR | Nhận dạng âm nhạc | <1k
| MQA | Hỏi đáp âm nhạc | <1k

ngôn ngữ, độ chi tiết của chú thích và cấu trúc văn bản (ví dụ, một số dữ liệu có cấu trúc trong khi những dữ liệu khác không có cấu trúc). Để huấn luyện một mạng cho các nhiệm vụ khác nhau, việc đơn giản trộn lẫn các bộ dữ liệu đa dạng này không thể dẫn đến
sự tăng cường lẫn nhau; thay vào đó, nó gây ra can thiệp. Hầu hết các phương pháp huấn luyện đa nhiệm vụ hiện tại
đã nhóm các nhiệm vụ tương tự (ví dụ, mô tả âm thanh, phiên âm) hoặc gán một ID bộ dữ liệu cho mỗi
bộ dữ liệu (Wang et al., 2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b; Shu et al., 2023) để tránh
can thiệp. Mặc dù những phương pháp này đã đạt được một hiệu quả nhất định, vẫn còn có
chỗ cho cải thiện đáng kể. Whisper đề xuất một định dạng huấn luyện đa nhiệm vụ bằng cách chỉ định các nhiệm vụ và thông tin điều kiện
như một chuỗi các token đặc biệt đầu vào đến bộ giải mã ngôn ngữ, như phát hiện hoạt động giọng nói,
nhận dạng ngôn ngữ và thẻ dấu thời gian cấp câu. Tuy nhiên, Whisper tập trung chỉ vào các nhiệm vụ dịch thuật giọng nói
và nhận dạng.

Khung Định dạng Huấn luyện Đa nhiệm vụ Được thúc đẩy bởi Whisper (Radford et al., 2023), để kết hợp
các loại âm thanh khác nhau, chúng tôi đề xuất một khung định dạng huấn luyện đa nhiệm vụ như sau:

•Thẻ Phiên âm: Việc bắt đầu dự đoán được biểu thị bằng một thẻ phiên âm. <|startof-
transcripts|> được sử dụng để chỉ ra các nhiệm vụ liên quan đến việc phiên âm chính xác các từ nói và

--- TRANG 8 ---
nắm bắt nội dung ngôn ngữ của một bản ghi giọng nói, như các nhiệm vụ nhận dạng giọng nói và dịch giọng nói. Đối với các nhiệm vụ khác, thẻ <|startofanalysis|> được sử dụng.

•Thẻ Ngôn ngữ Âm thanh: Sau đó, chúng tôi kết hợp một thẻ ngôn ngữ cho biết ngôn ngữ được nói trong
âm thanh. Thẻ này sử dụng một token duy nhất được gán cho mỗi ngôn ngữ có mặt trong tập huấn luyện của chúng tôi, tổng cộng tám ngôn ngữ. Trong trường hợp một đoạn âm thanh không chứa bất kỳ giọng nói nào, như âm thanh tự nhiên và âm nhạc, mô hình được huấn luyện để dự đoán một token <|unknown|>.

•Thẻ Nhiệm vụ: Các token tiếp theo chỉ định nhiệm vụ. Chúng tôi phân loại các nhiệm vụ âm thanh được thu thập thành năm
danh mục: <|transcribe|>, <|translate|>, <|caption|>, <|analysis|>, và <|question-answer|>. 
Đối với các nhiệm vụ hỏi đáp (QA), chúng tôi thêm các câu hỏi tương ứng sau thẻ.

•Thẻ Ngôn ngữ Văn bản: Token thẻ chỉ định ngôn ngữ của chuỗi văn bản đầu ra.

•Thẻ Dấu thời gian: Sự hiện diện của token <|timestamps|> hoặc <|notimestamps|> xác định liệu
mô hình có cần dự đoán dấu thời gian hay không. Khác với dấu thời gian cấp câu được sử dụng trong
Whisper, việc bao gồm thẻ <|timestamps|> yêu cầu mô hình thực hiện dự đoán dấu thời gian cấp từ chi tiết, viết tắt là SRWT (Speech Recognition with Word-level Timestamps). Việc dự đoán các dấu thời gian này được xen kẽ với các từ phiên âm: token thời gian bắt đầu được dự đoán trước mỗi token phiên âm, trong khi token thời gian kết thúc được dự đoán sau. Theo thí nghiệm của chúng tôi, SRWT cải thiện khả năng của mô hình để căn chỉnh tín hiệu âm thanh với dấu thời gian. Sự căn chỉnh được cải thiện này góp phần vào sự hiểu biết toàn diện về tín hiệu giọng nói của mô hình, dẫn đến những tiến bộ đáng chú ý trong nhiều nhiệm vụ như nhận dạng giọng nói và các nhiệm vụ QA âm thanh.

•Hướng dẫn Đầu ra: Cuối cùng, chúng tôi cung cấp hướng dẫn đầu ra để chỉ định thêm nhiệm vụ và định dạng mong muốn
cho các nhiệm vụ con khác nhau, và sau đó đầu ra văn bản bắt đầu.

Nguyên tắc hướng dẫn đằng sau khung của chúng tôi là tối đa hóa việc chia sẻ kiến thức giữa các nhiệm vụ tương tự
thông qua các thẻ được chia sẻ, từ đó cải thiện hiệu suất của chúng. Trong khi đó, chúng tôi đảm bảo rằng các nhiệm vụ khác nhau và
định dạng đầu ra có thể được phân biệt để tránh vấn đề ánh xạ một-đến-nhiều cho mô hình. Vui lòng xem
Hình 3 để có tổng quan về định dạng đa nhiệm vụ của Qwen-Audio.

3.3 Tinh chỉnh Có giám sát

Việc tiền huấn luyện mở rộng của các mô hình đa nhiệm vụ đã trang bị cho chúng sự hiểu biết rộng về âm thanh.
Dựa trên điều này, chúng tôi sử dụng các kỹ thuật tinh chỉnh dựa trên hướng dẫn để cải thiện khả năng của mô hình
căn chỉnh với ý định con người, dẫn đến một mô hình chat tương tác, được gọi là Qwen-Audio-Chat. Để thực hiện điều này,
chúng tôi tạo ra thủ công các cuộc trình diễn cho mỗi nhiệm vụ. Những cuộc trình diễn này bao gồm nhãn văn bản thô, câu hỏi,
và câu trả lời. Sau đó chúng tôi sử dụng GPT-3.5 (OpenAI, 2022) để tạo ra thêm các câu hỏi và câu trả lời dựa trên
các nhãn văn bản thô được cung cấp. Ngoài ra, chúng tôi cũng tạo ra một bộ dữ liệu dữ liệu đối thoại âm thanh bằng cách sử dụng chú thích thủ công, tạo mô hình và nối chuỗi chiến lược. Bộ dữ liệu này giúp chúng tôi kết hợp khả năng lý luận, tạo ra câu chuyện và hiểu đa hình ảnh vào mô hình của chúng tôi.

Để xử lý đối thoại đa âm thanh và nhiều đầu vào âm thanh một cách hiệu quả, chúng tôi giới thiệu quy ước gắn nhãn
các âm thanh khác nhau với "Audio id:", trong đó id tương ứng với thứ tự của đối thoại đầu vào âm thanh. Về mặt
định dạng đối thoại, chúng tôi xây dựng bộ dữ liệu tinh chỉnh hướng dẫn của mình sử dụng định dạng ChatML (Openai). Trong định dạng này, mỗi tuyên bố tương tác được đánh dấu bằng hai token đặc biệt (<im_start> và <im_end>) để
tạo điều kiện kết thúc đối thoại.

--- TRANG 9 ---
Bảng 2: Tóm tắt các benchmark đánh giá cho Qwen-Audio.

Nhiệm vụ | Mô tả | Bộ dữ liệu | Phân chia | Thang đo
---|---|---|---|---
ASR | Nhận dạng Giọng nói Tự động | Aishell1 (Bu et al., 2017) | dev \| test | WER
| | Aishell2 (Du et al., 2018) | test |
| | Librispeech (Panayotov et al., 2015) | dev \| test |
S2TT | Dịch giọng nói sang văn bản | CoVoST2 (Wang et al., 2020) | test | BLEU¹(Papineni et al., 2002)
SRWT | Nhận dạng Giọng nói với Dấu thời gian Cấp từ | Industrial Data (Gao et al., 2023) | test | AAS (Shi et al., 2023)
AAC | Mô tả Âm thanh Tự động | Clotho (Drossos et al., 2020) | test | CIDEr \| SPICE \| SPIDEr (Vedantam et al., 2015; Anderson et al., 2016)
ASC | Phân loại Cảnh Âm thanh | CochlScene (Jeong and Park, 2022) | test | ACC
| | TUT2017 (Mesaros et al., 2017) | eval |
SER | Nhận dạng Cảm xúc Giọng nói | Meld (Poria et al., 2019) | test | ACC
AQA | Hỏi đáp Âm thanh | ClothoAQA (Lipping et al., 2022) | test | ACC
VSC | Phân loại Âm thanh Giọng | VocalSound (Gong et al., 2022) | test | ACC
MNA | Phân tích Nốt nhạc | NSynth (Engel et al., 2017) | test | ACC / MAP

Ví dụ Định dạng Dữ liệu của Tinh chỉnh Có giám sát.

<im_start>user
Audio 1: <audio>emov-db/141-168-0155.wav</audio>người nói nói gì? <im_end>
<im_start>assistant
Người nói nói bằng tiếng Anh, "Won't you draw up, gentlemen.". <im_end>
<im_start>user
Tâm trạng của người đó như thế nào? <im_end>
<im_start>assistant
Dựa trên giọng nói, tâm trạng của người đó là ghê tởm. <im_end>

Để tạo điều kiện đầu vào đa dạng từ cả phương thức âm thanh và văn bản thuần túy trong đối thoại đa lượt, chúng tôi
sử dụng kết hợp dữ liệu hướng dẫn lấy âm thanh làm trung tâm được đề cập ở trên và dữ liệu hướng dẫn văn bản thuần túy trong
quá trình huấn luyện này. Phương pháp này cho phép mô hình xử lý các hình thức đầu vào đa dạng một cách liền mạch. Tổng
lượng dữ liệu tinh chỉnh hướng dẫn là 20k.

4 Thí nghiệm

4.1 Thiết lập

Đối với tiền huấn luyện đa nhiệm vụ, chúng tôi đóng băng trọng số của LLM và chỉ tối ưu hóa bộ mã hóa âm thanh. Mô hình được huấn luyện này
được gọi là Qwen-Audio. Trong giai đoạn tinh chỉnh có giám sát tiếp theo, chúng tôi cố định trọng số của
bộ mã hóa âm thanh và chỉ tối ưu hóa LLM. Mô hình kết quả được gọi là Qwen-Audio-Chat. Các
cấu hình huấn luyện chi tiết của cả hai giai đoạn được liệt kê trong Bảng 6.

4.2 Đánh giá

Để đánh giá khả năng hiểu phổ quát của Qwen-Audio, như thể hiện trong Bảng 2, chúng tôi thực hiện
một đánh giá toàn diện bao gồm các nhiệm vụ khác nhau, cụ thể là Nhận dạng Giọng nói Tự động (ASR),
Dịch Giọng nói sang Văn bản (S2TT), Mô tả Âm thanh Tự động (AAC), Phân loại Cảnh Âm thanh (ASC),
Nhận dạng Cảm xúc Giọng nói (SER), Hỏi đáp Âm thanh (AQA), Phân loại Âm thanh Giọng
(VSC), và Phân tích Nốt nhạc (MNA). Đánh giá này được tiến hành trên 12 bộ dữ liệu. Các bộ dữ liệu đánh giá được loại trừ nghiêm ngặt khỏi dữ liệu huấn luyện để tránh rò rỉ dữ liệu. Các
cấu hình huấn luyện chi tiết của cả hai giai đoạn được liệt kê trong Bảng 6.

¹https://github.com/mjpost/sacrebleu

--- TRANG 10 ---
Bảng 3: Kết quả của Nhận dạng Giọng nói Tự động (ASR), Dịch Giọng nói sang Văn bản (S2TT), Mô tả Âm thanh Tự động (AAC), Nhận dạng Giọng nói với Dấu thời gian Cấp từ (SRWT), Phân loại Cảnh Âm thanh (ASC), Nhận dạng Cảm xúc Giọng nói (SER), Hỏi đáp Âm thanh (AQA), Phân loại Âm thanh Giọng (VSC), và Phân tích Nốt nhạc (MNA). Đối với nhiệm vụ SRWT, kết quả của Forced-aligner (McAuliffe et al., 2017) là dự đoán dấu thời gian cho các bản ghi chính xác, trong khi Paraformer-large-TP (Gao et al., 2023) và Qwen-audio giải quyết một tình huống thách thức hơn bằng cách trực tiếp tạo ra chuỗi chứa cả phiên âm và dấu thời gian.

Nhiệm vụ | Bộ dữ liệu | Mô hình | Thang đo Hiệu suất | Kết quả
---|---|---|---|---
ASR | Librispeech | SpeechT5 (Ao et al., 2021) | WER↓ | 2.1 \| 5.5 \| 2.4 \| 5.8
| dev-clean \|dev-other \| test-clean \|test-other | SpeechNet (Chen et al., 2021) | | - \| - \| 30.7 \| -
| | SLM-FT (Wang et al., 2023b) | | - \| - \| 2.6 \| 5.0
| | SALMONN (Anonymous, 2023) | | - \| - \| 2.1 \| 4.9
| | Qwen-Audio | | 1.8\|4.0\|2.0\|4.2
| Aishell1 | MMSpeech-base (Zhou et al., 2022) | WER↓ | 2.0 \| 2.1
| dev\|test | MMSpeech-large (Zhou et al., 2022) | | 1.6 \| 1.9
| | Paraformer-large (Gao et al., 2023) | | - \| 2.0
| | Qwen-Audio | | 1.2\|1.3
| Aishell2 | MMSpeech-base (Zhou et al., 2022) | WER↓ | 4.5 \| 3.9 \| 4.0
| Mic\|iOS\|Android | Paraformer-large (Gao et al., 2023) | | - \| 2.9\| -
| | Qwen-Audio | | 3.3\| 3.1 \|3.3
S2TT | CoVoST2 | SALMONN (Anonymous, 2023) | BLEU ↑ | 18.6 \| - \| 33.1 \| -
| en-de\|de-en\| en-zh\|zh-en | SpeechLLaMA (Wu et al., 2023a) | | - \| 27.1 \| - \| 12.3
| | BLSP (Wang et al., 2023a) | | 14.1 \| - \| - \| -
| | Qwen-Audio | | 25.1\|33.9\|41.5\|15.7
| CoVoST2 | SpeechLLaMA (Wu et al., 2023a) | BLEU ↑ | 27.9 \| 25.2 \| 25.9
| es-en\|fr-en\|it-en | Qwen-Audio | | 39.7\|38.5\|36.0
AAC | Clotho | Pengi (Deshmukh et al., 2023) | CIDEr \| SPICE \| SPIDEr ↑ | 0.416 \| 0.126 \| 0.271
| | Qwen-Audio | | 0.441\|0.136\|0.288
SRWT | Industrial Data | Force-aligner (McAuliffe et al., 2017) | AAS (ms) ↓ | 60.3
| | Paraformer-large-TP (Gao et al., 2023) | | 65.3
| | Qwen-Audio | | 51.5
ASC | CochlScene | CochlScene (Jeong and Park, 2022) | ACC↑ | 0.669
| | Qwen-Audio | | 0.795
| TUT2017 | Pengi (Deshmukh et al., 2023) | ACC↑ | 0.353
| | Qwen-Audio | | 0.649
SER | Meld | WavLM-large (Chen et al., 2022) | ACC↑ | 0.542
| | Qwen-Audio | | 0.557
AQA | ClothoAQA | ClothoAQA (Lipping et al., 2022) | ACC \| ACC (binary) ↑ | 0.542 \| 0.627
| | Pengi (Deshmukh et al., 2023) | | - \| 0.645
| | Qwen-Audio | | 0.579\|0.749
VSC | VocalSound | CLAP (Elizalde et al., 2022) | ACC↑ | 0.4945
| | Pengi (Deshmukh et al., 2023) | | 0.6035
| | Qwen-Audio | | 0.9289
MNA | NS. Qualities | Pengi (Deshmukh et al., 2023) | MAP↑ | 0.3860
| | Qwen-Audio | | 0.4742
| NS. Instrument | Pengi (Deshmukh et al., 2023) | ACC↑ | 0.5007
| | Qwen-Audio | | 0.7882

4.3 Kết quả Chính

Trong phần này, chúng tôi trình bày đánh giá toàn diện về mô hình Qwen-Audio, đánh giá hiệu suất của nó
trên các nhiệm vụ khác nhau mà không cần bất kỳ tinh chỉnh cụ thể cho nhiệm vụ nào. Chúng tôi bắt đầu bằng cách kiểm tra kết quả Nhận dạng Giọng nói Tự động (ASR) tiếng Anh của nó, như mô tả trong Bảng 3, nơi Qwen-Audio thể hiện hiệu suất vượt trội so với các mô hình học đa nhiệm vụ trước đây. Cụ thể, nó đạt được 2.0% và 4.2% WER trên các bộ dữ liệu librispeech test-clean và test-other, tương ứng. Tương tự, kết quả ASR tiếng Quan Thoại Trung Quốc chứng minh hiệu suất cạnh tranh của Qwen-Audio so với các phương pháp trước đây. Theo hiểu biết của chúng tôi, Qwen-Audio đạt được kết quả tối tân trên các tập dev và test của Aishell1. Hơn nữa, chúng tôi đánh giá hiệu suất dịch giọng nói của Qwen-Audio trên bộ dữ liệu CoVoST2. Kết quả cho thấy Qwen-Audio vượt trội so với các baseline với một biên độ đáng kể trên tất cả bảy hướng dịch.

Cuối cùng, chúng tôi phân tích hiệu suất của Qwen-Audio trên các nhiệm vụ phân tích âm thanh khác nhau, bao gồm AAC, SWRT, ASC, SER, AQA, VSC, và MNA, như được tóm tắt trong Bảng 3. Trên các nhiệm vụ này, Qwen-Audio luôn vượt trội so với các baseline với một biên độ đáng kể. Đáng chú ý, nó đạt được kết quả tối tân trên CochlScene, ClothoAQA, và VocalSound, từ đó chứng minh khả năng hiểu âm thanh mạnh mẽ của mô hình.

4.4 Kết quả của Chat Tương tác

Chúng tôi trình bày khả năng đối thoại của Qwen-Audio-Chat thông qua các trường hợp minh họa được mô tả trong
Hình 2. Hơn nữa, chúng tôi dự định cung cấp quyền truy cập công khai vào các mô hình đã được huấn luyện cho các tương tác chat trực tuyến.

4.5 Phân tích Dự đoán Dấu thời gian Cấp từ

Chúng tôi đề xuất nhiệm vụ nhận dạng giọng nói với dấu thời gian cấp từ (SRWT) bằng cách huấn luyện Qwen-Audio
không chỉ nhận dạng bản ghi giọng nói mà còn dự đoán dấu thời gian cho mỗi từ. Mục đích của SRWT là
hai mặt: thứ nhất, để cải thiện khả năng của mô hình trong việc căn chỉnh tín hiệu âm thanh với dấu thời gian chi tiết; thứ hai,
để hỗ trợ việc căn cứ giọng nói và âm thanh, và các nhiệm vụ QA dựa trên căn cứ trong Qwen-Audio-Chat, như
tìm thời gian bắt đầu và kết thúc của một đoạn âm thanh đề cập đến tên một người hoặc nhận dạng liệu
một âm thanh có xảy ra trong âm thanh đã cho hay không².

Trong phần này, chúng tôi loại trừ việc huấn luyện các nhiệm vụ SRWT khỏi tiền huấn luyện đa nhiệm vụ trong khi duy trì các
nhiệm vụ khác không thay đổi. Đáng chú ý, việc loại bỏ SRWT không ảnh hưởng đến phạm vi bao phủ của các bộ dữ liệu âm thanh cho huấn luyện
vì các nhiệm vụ SRWT chia sẻ cùng bộ dữ liệu âm thanh như các nhiệm vụ nhận dạng giọng nói tự động (ASR). Kết quả được
thể hiện trong Bảng 4 và Bảng 5: các mô hình được huấn luyện với SRWT đạt được hiệu suất vượt trội trong nhận dạng giọng nói tự động và các nhiệm vụ hỏi đáp âm thanh, bao gồm QA âm thanh tự nhiên và QA âm nhạc. Những kết quả này
làm nổi bật hiệu quả của việc kết hợp dấu thời gian cấp từ chi tiết để tăng cường khả năng căn cứ tín hiệu âm thanh chung và sau đó cải thiện hiệu suất của các nhiệm vụ QA tín hiệu âm thanh và âm nhạc.

²Phát hiện sự kiện âm thanh có thể được xem như một nhiệm vụ con của dự đoán dấu thời gian sự kiện vì sự vắng mặt của dấu thời gian sự kiện ngụ ý sự không xảy ra của nó trong âm thanh.

--- TRANG 11 ---
Bảng 4: Kết quả của các nhiệm vụ ASR có hoặc không có huấn luyện nhiệm vụ dấu thời gian cấp từ.

Phương pháp | LibriSpeech | | | | AISHELL1 |
---|---|---|---|---|---
| dev-clean | dev-other | test-clean | test-other | dev | test
w/o SRWT | 1.93 | 4.18 | 2.22 | 4.21 | 1.54 | 1.71
Qwen-Audio | 1.79 | 4.00 | 2.04 | 4.19 | 1.22 | 1.29

Bảng 5: Kết quả của các nhiệm vụ AQA có hoặc không có huấn luyện nhiệm vụ dấu thời gian cấp từ.

Phương pháp | ClothoAQA | | MusicAVQA |
---|---|---|---
| test | test-binary | audio question
w/o SRWT | 0.5648 | 0.7418 | 0.7027
Qwen-Audio | 0.5795 | 0.7491 | 0.7211

5 Kết luận

Trong bài báo này, chúng tôi trình bày series Qwen-Audio, một tập hợp các mô hình âm thanh-ngôn ngữ quy mô lớn với khả năng hiểu âm thanh phổ quát. Để kết hợp các loại âm thanh khác nhau cho đồng huấn luyện, chúng tôi đề xuất một khung học đa nhiệm vụ thống nhất tạo điều kiện chia sẻ kiến thức giữa các nhiệm vụ tương tự và tránh vấn đề ánh xạ một-đến-nhiều gây ra bởi các định dạng văn bản khác nhau. Mà không cần bất kỳ tinh chỉnh cụ thể cho nhiệm vụ nào, các mô hình Qwen-Audio kết quả vượt trội so với các công trình trước đây trên các benchmark đa dạng, chứng minh khả năng hiểu âm thanh phổ quát của nó. Thông qua tinh chỉnh hướng dẫn có giám sát, Qwen-Audio-Chat thể hiện khả năng mạnh mẽ trong việc căn chỉnh với ý định con người, hỗ trợ đối thoại đa ngôn ngữ và đa lượt từ cả đầu vào âm thanh và văn bản.

6 Lời cảm ơn

Chúng tôi bày tỏ lòng biết ơn đến Jinze Bai, Shuai Bai, Peng Wang, Sinan Tan, Shijie Wang vì những thảo luận sâu sắc của họ. Chúng tôi muốn cảm ơn Juan Zhu, Junyang Lin, Siqi Zheng, Jiaming Wang và Zhihao Du vì sự hỗ trợ của họ cho dự án này.

Tài liệu tham khảo

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. NeurIPS, 2022.

Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image
captionevaluation. In ComputerVision-ECCV2016: 14thEuropeanConference,Amsterdam,TheNetherlands,
October 11-14, 2016, Proceedings, Part V 14 . Springer, 2016.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv:2305.10403 ,
2023.

Anonymous. SALMONN:Towardsgenerichearing abilitiesforlarge languagemodels. In Submittedto The
Twelfth International Conference on Learning Representations , 2023. under review.

JunyiAo,RuiWang,LongZhou,ChengyiWang,ShuoRen,YuWu,ShujieLiu,TomKo,QingLi,YuZhang,etal.
Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing. arXiv:2110.07205 ,
2021.

JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023a.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jin-
grenZhou. Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities. CoRR,abs/2308.12966,
2023b.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
NeurIPS, 2020.

HuiBu,JiayuDu,XingyuNa,BenguWu,andHaoZheng. AISHELL-1: anopen-sourcemandarinspeech
corpus and a speech recognition baseline. In 20th Conference of the Oriental Chapter of the International
Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, O-COCOSDA 2017, Seoul,
South Korea, November 1-3, 2017 . IEEE, 2017.

--- TRANG 12 ---
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm's referential dialogue magic. arXiv:2306.15195 , 2023.

Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda,
TakuyaYoshioka,XiongXiao, JianWu,Long Zhou,ShuoRen, YanminQian,Yao Qian,JianWu, Michael
Zeng, Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech
processing. IEEE J. Sel. Top. Signal Process. , 2022.

Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin, Sung-Feng Huang, Da-Rong
Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung-yi Lee. Speechnet: A universal modularized model for
speech processing tasks. arXiv:2105.03070 , 2021.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. PaLM:Scalinglanguagemodeling
with pathways. arXiv:2204.02311 , 2022.

Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression.
arXiv:2210.13438 , 2022.

Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for
audio tasks. CoRR, 2023.

Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In 2020
IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020 . IEEE, 2020.

Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. AISHELL-2: transforming mandarin ASR research into
industrial scale. abs/1808.10583, 2018.

Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP: learning audio
concepts from natural language supervision. abs/2206.04769, 2022.

JesseH.Engel,CinjonResnick,AdamRoberts,SanderDieleman,MohammadNorouzi,DouglasEck,and
KarenSimonyan. Neuralaudiosynthesisofmusicalnoteswithwavenetautoencoders. In Proceedingsof
the34thInternationalConferenceonMachineLearning, ICML2017,Sydney,NSW,Australia, 6-11August2017 ,
Proceedings of Machine Learning Research. PMLR, 2017.

Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao
Du,ZhangyuXiao,andShiliangZhang. Funasr: Afundamentalend-to-endspeechrecognitiontoolkit.
CoRR, abs/2305.11013, 2023.

YuanGong,JinYu,andJamesR.Glass. Vocalsound: Adatasetforimprovinghumanvocalsoundsrecognition.
InIEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing,ICASSP2022,VirtualandSingapore,
23-27May2022 ,pages151-155.IEEE,2022. doi: 10.1109/ICASSP43922.2022.9746828. URL https://doi.
org/10.1109/ICASSP43922.2022.9746828 .

YuanGong,SameerKhurana,LeonidKarlinsky,andJamesR.Glass. Whisper-at: Noise-robustautomatic
speech recognizers are also strong general audio event taggers. CoRR, abs/2307.03183, 2023a.

Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and
understand. CoRR, abs/2305.10790, 2023b.

Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Ab-
delrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearningbymaskedpredictionof
hidden units. IEEE ACM Trans. Audio Speech Lang. Process. , 2021.

EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685 , 2021.

--- TRANG 13 ---
RongjieHuang,MingzeLi,DongchaoYang,JiatongShi,XuankaiChang,ZhenhuiYe,YuningWu,Zhiqing
Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and
generating speech, music, sound, and talking head. CoRR, abs/2304.12995, 2023.

Il-Young Jeong and Jeongsoo Park. Cochlscene: Acquisition of acoustic scene data using crowdsourcing.
abs/2211.02289, 2022.

Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal
Manohar,YossiAdi,JayMahadeokar,andWei-NingHsu. Voicebox: Text-guidedmultilingualuniversal
speech generation at scale. CoRR, 2023.

JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. Blip: Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In ICML, 2022.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-
training with frozen image encoders and large language models. In International Conference on Machine
Learning,ICML2023,23-29July2023,Honolulu,Hawaii,USA ,ProceedingsofMachineLearningResearch.
PMLR, 2023.

Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clotho-aqa: A
crowdsourceddatasetforaudioquestionanswering. In 30thEuropeanSignalProcessingConference,EUSIPCO
2022, Belgrade, Serbia, August 29 - Sept. 2, 2022 . IEEE, 2022.

Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and
ZhaopengTu. Macaw-llm: Multi-modallanguagemodelingwithimage,audio,video,andtextintegration.
CoRR, abs/2306.09093, 2023.

SoumiMaiti,YifanPeng,ShukjaeChoi,Jee-weonJung,XuankaiChang,andShinjiWatanabe. Voxtlm: unified
decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.
arXiv:2309.07937 , 2023.

MichaelMcAuliffe,MichaelaSocolof,SarahMihuc,MichaelWagner,andMorganSonderegger. Montreal
forcedaligner: Trainabletext-speechalignmentusingkaldi. In Interspeech2017,18thAnnualConferenceof
the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017 , 2017.

AnnamariaMesaros, ToniHeittola,Aleksandr Diment,BenjaminElizalde, AnkitShah,Emmanuel Vincent,
Bhiksha Raj, and Tuomas Virtanen. DCASE2017 challenge setup: Tasks, datasets and baseline system. In
Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events, DCASE 2017, Munich,
Germany, November 16-17, 2017 , 2017.

Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, R. J.
Skerry-Ryan,andMichelleTadmorRamanovich. Lmswithavoice: Spokenlanguagemodelingbeyond
speech tokens. CoRR, 2023.

Openai. Chatml documents. URL https://github.com/openai/openai-python/blob/main/chatml.md .

OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt .

OpenAI. Gpt-4 technical report, 2023.

LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. NeurIPS, 2022.

VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur. Librispeech: AnASRcorpusbasedon
public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,
ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015 . IEEE, 2015.

--- TRANG 14 ---
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics ,
2002.

Daniel S.Park, WilliamChan, Yu Zhang, Chung-ChengChiu, Barret Zoph,Ekin D. Cubuk,and Quoc V.Le.
Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019,
20th AnnualConferenceof the InternationalSpeechCommunication Association, Graz, Austria, 15-19 September
2019.

Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world. arXiv:2306.14824 , 2023.

Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea.
MELD:Amultimodalmulti-partydatasetforemotionrecognitioninconversations. In Proceedingsofthe
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers . Association for Computational Linguistics, 2019.

Qwen. Introducing qwen-7b: Openfoundationand human-alignedmodels(ofthestate-of-the-arts),2023.
URL https://github.com/QwenLM/Qwen-7B .

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust
speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA , 2023.

ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. The
Journal of Machine Learning Research , 2020.

Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix
de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Mucken-
hirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara N. Sainath, Johan Schalkwyk, Matthew Sharifi,
MichelleTadmorRamanovich,MarcoTagliasacchi,AlexandruTudor,MihajloVelimirovic,DamienVincent,
Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and
Christian Havnø Frank. Audiopalm: A large language model that can speak and listen. CoRR.

YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang. Hugginggpt: Solving
AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023.

XianShi,YanniChen,ShiliangZhang,andZhijieYan. Achievingtimestamppredictionwhilerecognizing
withnon-autoregressiveend-to-endasrmodel. In NationalConferenceonMan-MachineSpeechCommunication .
Springer, 2023.

Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin
Shi. Llasm: Large language and speech model. arXiv:2308.15930 , 2023.

Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,
Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv:2307.05222 , 2023.

HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation
language models. arXiv:2302.13971 , 2023a.

HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
language models. arXiv:2302.13971 , 2023b.

HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer,
MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,Cynthia

--- TRANG 15 ---
Gao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,Marcin
Kardas,ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-Anne
Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,
BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023c.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
HannaM.Wallach,RobFergus,S.V.N.Vishwanathan,andRomanGarnett,editors, AdvancesinNeural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017 , 2017.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In CVPR, 2015.

Changhan Wang, Anne Wu, and Juan Miguel Pino. Covost 2: A massively multilingual speech-to-text
translation corpus. abs/2007.10310, 2020. URL https://arxiv.org/abs/2007.10310 .

ChenWang,MinpengLiao,ZhongqiangHuang,JinliangLu,JunhongWu,YuchenLiu,ChengqingZong,and
JiajunZhang. Blsp: Bootstrappinglanguage-speechpre-trainingviabehavioralignmentofcontinuation
writing. arXiv:2309.00916 , 2023a.

MingqiuWang,WeiHan,IzhakShafran,ZelinWu,Chung-ChengChiu,YuanCao,YongqiangWang,Nanxin
Chen,YuZhang,HagenSoltau,PaulK.Rubenstein,LukasZilka,DianYu,ZhongMeng,GolanPundak,
Nikhil Siddhartha, Johan Schalkwyk, and Yonghui Wu. SLM: bridge the thin gap between speech and text
foundation models. abs/2310.00230, 2023b.

MingqiuWang,WeiHan,IzhakShafran,ZelinWu,Chung-ChengChiu,YuanCao,YongqiangWang,Nanxin
Chen,YuZhang,HagenSoltau,etal. Slm: Bridgethethingapbetweenspeechandtextfoundationmodels.
arXiv:2310.00230 , 2023c.

Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu
Wei. Viola: Unifiedcodeclanguagemodelsforspeechrecognition,synthesis,andtranslation. CoRR,2023d.

Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min
Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as a versatile
speech transformer. CoRR, 2023e.

JianWu,YasheshGaur,ZhuoChen,LongZhou,YimengZhu,TianruiWang,JinyuLi,ShujieLiu,BoRen,
Linquan Liu, and Yu Wu. On decoder-only architecture for speech-to-text and large language model
integration. abs/2307.03917, 2023a.

ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-SengChua. Next-gpt: Any-to-anymultimodalLLM.
CoRR, abs/2309.05519, 2023b.

Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An
end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process. , 2022.

Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Em-
poweringlarge languagemodels withintrinsiccross-modal conversationalabilities. CoRR,abs/2305.11000,
2023a.

XinZhang,DongZhang,ShiminLi,YaqianZhou,andXipengQiu. Speechtokenizer: Unifiedspeechtokenizer
for speech large language models. CoRR, abs/2308.16692, 2023b.

--- TRANG 16 ---
YuZhang,WeiHan,JamesQin,YongqiangWang,AnkurBapna,ZhehuaiChen,NanxinChen,BoLi,Vera
Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa
Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara N.
Sainath,PedroJ.Moreno,Chung-ChengChiu,JohanSchalkwyk,FrançoiseBeaufays,andYonghuiWu.
Google usm: Scaling automatic speech recognition beyond 100 languages. CoRR, 2023c.

Xiaohuan Zhou, Jiaming Wang, Zeyu Cui, Shiliang Zhang, Zhijie Yan, Jingren Zhou, and Chang Zhou.
Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition. abs/2212.00500,
2022.

--- TRANG 17 ---
A Siêu tham số

Chúng tôi báo cáo các thiết lập siêu tham số huấn luyện chi tiết của Qwen-Audio trong Bảng 6.

Bảng 6: Siêu tham số huấn luyện của Qwen-Audio

Cấu hình | Tiền huấn luyện Đa nhiệm vụ | Tinh chỉnh Có giám sát
---|---|---
Khởi tạo bộ mã hóa âm thanh | Whisper-large-v2 | Qwen-audio giai đoạn 1
Khởi tạo LLM | Qwen-7B | Qwen-7B
Chính sách SpecAugment | LibriSpeech Basic | LibriSpeech Basic
Tối ưu hóa | AdamW | AdamW
Siêu tham số tối ưu hóa | beta1=0.9, beta2=0.98, eps = 1e−6
Tốc độ học đỉnh | 5e−5 | 1e−5
Tốc độ học tối thiểu | 1e−5 | 1e−6
Giảm tốc độ học bộ mã hóa âm thanh | 0.95 | 0
Lịch trình tốc độ học | cosine decay | cosine decay
Giảm trọng số | 0.05 | 0.05
Cắt gradient | 1.0 | 1.0
Bước huấn luyện | 500k | 8k
Bước khởi động | 2000 | 3k
Kích thước batch toàn cục | 120 | 128
Tích lũy Gradient | 1 | 8
Độ chính xác số | bfloat16 | bfloat16
Chia sẻ tối ưu hóa | ✓ | ✓
Kiểm tra điểm kích hoạt | ✗ | ✗
Song song mô hình | ✗ | 2
Song song pipeline | ✗ | ✗
