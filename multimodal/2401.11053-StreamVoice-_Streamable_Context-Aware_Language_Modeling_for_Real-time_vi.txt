# StreamVoice: Mô hình Ngôn ngữ Nhận biết Ngữ cảnh Có thể Truyền trực tiếp cho Chuyển đổi Giọng nói Zero-shot Thời gian Thực

Zhichao Wang1, Yuanzhe Chen2, Xinsheng Wang1, Lei Xie1*, Yuping Wang2
1Nhóm Xử lý Âm thanh, Giọng nói và Ngôn ngữ (ASLP@NPU)
Trường Khoa học Máy tính, Đại học Bách khoa Tây Bắc, Tây An, Trung Quốc
2Công ty Douyin Vision Co., Ltd.

## Tóm tắt

Những tiến bộ gần đây của mô hình ngôn ngữ (LM) đã thể hiện hiệu suất chuyển đổi giọng nói (VC) zero-shot ấn tượng. Tuy nhiên, các mô hình VC dựa trên LM hiện tại thường áp dụng chuyển đổi offline từ ngữ nghĩa nguồn sang đặc trưng âm thanh, đòi hỏi toàn bộ giọng nói nguồn và hạn chế triển khai trong các ứng dụng thời gian thực. Trong bài báo này, chúng tôi giới thiệu StreamVoice, một mô hình streaming dựa trên LM mới lạ cho VC zero-shot, tạo điều kiện chuyển đổi thời gian thực với các gợi ý người nói và giọng nói nguồn tùy ý. Cụ thể, để kích hoạt khả năng streaming, StreamVoice sử dụng LM nhận biết ngữ cảnh hoàn toàn nhân quả với bộ dự đoán âm thanh độc lập theo thời gian, trong khi xử lý luân phiên các đặc trưng ngữ nghĩa và âm thanh tại mỗi bước thời gian của tự hồi quy, loại bỏ phụ thuộc vào giọng nói nguồn hoàn chỉnh. Để giải quyết sự suy giảm hiệu suất tiềm ẩn từ ngữ cảnh không đầy đủ trong xử lý streaming, chúng tôi tăng cường nhận thức ngữ cảnh của LM thông qua hai chiến lược: 1) dự báo ngữ cảnh hướng dẫn bởi giáo viên, sử dụng mô hình giáo viên để tóm tắt ngữ cảnh ngữ nghĩa hiện tại và tương lai trong quá trình huấn luyện để hướng dẫn dự đoán mô hình cho ngữ cảnh còn thiếu; 2) chiến lược che giấu ngữ nghĩa, thúc đẩy dự đoán âm thanh từ đầu vào ngữ nghĩa và âm thanh bị hỏng trước đó, tăng cường khả năng học ngữ cảnh. Đáng chú ý, StreamVoice là mô hình VC zero-shot streaming đầu tiên dựa trên LM không có bất kỳ nhìn trước tương lai nào. Các thí nghiệm chứng minh khả năng chuyển đổi streaming của StreamVoice trong khi đạt được hiệu suất zero-shot có thể so sánh với các hệ thống VC không streaming.

## 1 Giới thiệu

Chuyển đổi giọng nói (VC) nhằm mục đích chuyển giọng nói của một người nói thành giọng nói của người nói khác mà không thay đổi nội dung ngôn ngữ. Kỹ thuật này đã được triển khai trong nhiều ứng dụng thực tế, như lồng tiếng phim, bảo vệ quyền riêng tư, sửa lỗi phát âm, v.v. Với sự trợ giúp của các đặc trưng ngữ nghĩa thần kinh, chẳng hạn như đặc trưng cổ chai (BNF) từ hệ thống nhận dạng giọng nói tự động (ASR), việc chuyển đổi giọng nói nguồn từ các người nói tùy ý trong thế giới thực đã được thực hiện thành công (Sun et al., 2016). Đồng thời, chuyển đổi thành người nói mục tiêu tùy ý chỉ với một câu nói của người nói này, được gọi là VC zero-shot, cũng đã được nghiên cứu gần đây (Qian et al., 2019; Wang et al., 2023c). Tuy nhiên, hầu hết các mô hình VC zero-shot hiện tại được thiết kế cho hệ thống offline, không đủ để đáp ứng nhu cầu ngày càng tăng gần đây về khả năng streaming trong các ứng dụng VC thời gian thực, như phát sóng trực tiếp và giao tiếp thời gian thực (RTC). Trong nghiên cứu này, chúng tôi tập trung vào VC zero-shot streaming như được minh họa trong Hình 1.

Tách riêng giọng nói thành các thành phần khác nhau, ví dụ như nội dung ngữ nghĩa và âm sắc người nói, đóng vai trò quan trọng trong nhiệm vụ VC zero-shot (Chou và Lee, 2019; Wang et al., 2023d, 2021; Qian et al., 2019). Gần đây, nhờ vào khung LM mạnh mẽ và việc mở rộng quy mô dữ liệu huấn luyện, các mô hình VC dựa trên LM (Wang et al., 2023c; Yang et al., 2023; Zhu et al., 2023) với khả năng học theo ngữ cảnh tích hợp sẵn có thể học các mối quan hệ ngữ cảnh giữa câu nói của người nói nguồn và mục tiêu để nắm bắt âm sắc người nói tinh tế, đạt được hiệu suất VC zero-shot ấn tượng. Tuy nhiên, việc đòi hỏi toàn bộ câu nói nguồn hoàn chỉnh hạn chế các mô hình VC dựa trên LM này trong các tình huống thời gian thực; do đó, chúng chỉ có thể được sử dụng trong các ứng dụng offline. Trong khi một số phương pháp không dựa trên LM (Yang et al., 2022; Wang et al., 2023a) đã được đề xuất cho VC zero-shot streaming, hiệu suất không thể tổng quát hóa tốt cho những người nói chưa nhìn thấy với độ tương tự người nói cao và tự nhiên của giọng nói, chủ yếu do khả năng mô hình hạn chế để mở rộng quy mô dữ liệu huấn luyện, và cũng do sự suy giảm hiệu suất được gây ra bởi thông tin tương lai bị thiếu trong tình huống streaming.

Được truyền cảm hứng bởi thành công của các mô hình dựa trên LM trong VC zero-shot, chúng tôi nhằm mục đích khám phá tính khả thi của LM cho tình huống VC streaming. Một cách trực quan là tuân theo khung nhận dạng-tổng hợp phổ biến được hiển thị trong Hình 1, trong đó giọng nói được biểu diễn trong BNF ngữ nghĩa và các đặc trưng âm thanh được trích xuất tương ứng bởi ASR streaming và codec âm thanh. Sau đó, mô hình VC dựa trên LM thực hiện việc chuyển đổi thông tin ngữ nghĩa thành các đặc trưng âm thanh với âm sắc của người nói mục tiêu. Tuy nhiên, việc phát triển mô hình dựa trên LM trong VC zero-shot streaming bị cản trở bởi hai thách thức chính.

• Kiến trúc có thể streaming: các mô hình streaming thường tạo ra đầu ra ngay lập tức khi nhận đầu vào hiện tại mà không phụ thuộc vào các bước thời gian tương lai. Các mô hình VC dựa trên LM hiện tại thực hiện chuyển đổi chỉ khi chúng nhận được toàn bộ câu nói nguồn, điều này không đáp ứng được nhu cầu của các ứng dụng streaming. Việc mô hình ngôn ngữ đa giai đoạn được áp dụng rộng rãi cho dự đoán codec đa lớp gây ra sự phức tạp cho thiết kế hệ thống, đặt ra rủi ro tiềm ẩn của các lỗi tích lũy. Ngoài ra, các mô hình phụ thuộc của pipeline streaming cũng ảnh hưởng đến thiết kế và hiệu suất của mô hình VC.

• Khoảng cách hiệu suất: không giống như các mô hình không streaming, các mô hình streaming phải xử lý đầu vào theo khung hoặc theo chunk một cách nhân quả ngay lập tức mà không có thông tin tương lai, đối mặt với ngữ cảnh bị thiếu và sự suy giảm hiệu suất tiềm ẩn. Điều này bị thiếu ngăn cản mô hình VC streaming đạt được chuyển đổi chất lượng cao. Ngoài ra, như được hiển thị trong Hình 1, mô hình VC dựa vào đặc trưng ngữ nghĩa BNF từ ASR để thực hiện chuyển đổi, điều này làm cho các đặc trưng ngữ nghĩa rất quan trọng. Tuy nhiên, ASR streaming thể hiện hiệu suất kém hơn so với đối tác không streaming của nó, dẫn đến BNF mang thông tin ngữ nghĩa chất lượng thấp nhưng nhiều thông tin người nói hơn. Ngoài việc không thể nhận thông tin tương lai vốn có, đầu vào ngữ nghĩa chất lượng thấp này làm cho việc đạt được chuyển đổi chất lượng cao khó khăn hơn. Mục tiêu của VC zero-shot làm tăng cường các thách thức mà mô hình VC streaming của chúng tôi đối mặt.

Trong công trình này, chúng tôi đề xuất StreamVoice, một mô hình streaming dựa trên LM cho VC zero-shot chất lượng cao. Cụ thể, StreamVoice có kiến trúc có thể streaming tích hợp mô hình ngôn ngữ một giai đoạn tạo ra các codec âm thanh một cách tình cờ với sự hợp tác của bộ dự đoán âm thanh. Đầu vào luân phiên của các đặc trưng ngữ nghĩa và âm thanh tại mỗi bước thời gian đảm bảo hành vi streaming liền mạch. Hai phương pháp được giới thiệu để tăng cường nhận thức ngữ cảnh của LM nhằm giảm thiểu khoảng cách hiệu suất được gây ra bởi thông tin ngữ cảnh bị thiếu. 1) Chúng tôi kết hợp dự báo ngữ cảnh hướng dẫn bởi giáo viên, nơi mô hình VC được dạy bởi ASR không streaming giáo viên để suy ra thông tin ngữ nghĩa hiện tại và tương lai được tóm tắt bởi giáo viên, sau đó được sử dụng để tăng cường dự đoán âm thanh. 2) Để tăng cường việc học ngữ cảnh từ lịch sử đầu vào, việc che giấu ngữ nghĩa khuyến khích dự đoán âm thanh từ đầu vào âm thanh và ngữ nghĩa bị hỏng trước đó, điều này cũng tạo ra một cổ chai thông tin ngầm định để giảm thông tin của người nói nguồn.

Các thí nghiệm chứng minh khả năng của StreamVoice trong việc chuyển đổi giọng nói theo cách streaming với độ tương tự người nói cao cho cả những người nói đã nhìn thấy và chưa nhìn thấy trong khi duy trì hiệu suất có thể so sánh với các hệ thống VC không streaming. Là mô hình VC zero-shot đầu tiên dựa trên LM không có bất kỳ nhìn trước tương lai nào, toàn bộ pipeline chỉ có độ trễ 124 ms để thực hiện chuyển đổi, nhanh hơn 2.4 lần so với thời gian thực trên một GPU A100 duy nhất mà không có tối ưu hóa kỹ thuật. Các mẫu được chuyển đổi có thể được tìm thấy tại https://kerwinchao.github.io/StreamVoice/.

## 2 Công trình Liên quan

**Chuyển đổi Giọng nói Zero-shot.** VC zero-shot đặt ra những yêu cầu nghiêm ngặt về tách riêng giọng nói và nắm bắt âm sắc người nói. Nhiều nghiên cứu đã thiết kế đặc biệt nhiều phương pháp tách riêng, kết hợp các cấu trúc phức tạp (Chou và Lee, 2019), các hàm mất mát (Wang et al., 2021), và các chiến lược huấn luyện (Ebbers et al., 2021), để đạt được việc tách riêng giọng nói. Thay vì nhúng các thiết kế tách riêng rõ ràng trong huấn luyện VC, một số phương pháp (Gu et al., 2021) tận dụng mô hình xác minh người nói (SV) cho biểu diễn người nói, trong khi nội dung ngôn ngữ được trích xuất bằng ASR hoặc các mô hình học tự giám sát (SSL) (Sun et al., 2016; Choi et al., 2021). Để tăng cường việc nắm bắt âm sắc người nói, một số phương pháp mô hình người nói tinh tế cũng đã được khám phá (Yin et al., 2021; Wang et al., 2023d). Những thành công gần đây của các mô hình ngôn ngữ trong các nhiệm vụ tạo sinh đã thúc đẩy việc khám phá các mô hình dựa trên LM trong VC zero-shot, mang lại kết quả ấn tượng. Sử dụng mô hình được huấn luyện trước để tách riêng giọng nói, mô hình VC dựa trên LM (Wang et al., 2023c; Yang et al., 2023; Zhu et al., 2023) nắm bắt âm sắc người nói tinh tế từ gợi ý người nói và sau đó thực hiện chuyển đổi. Tuy nhiên, các mô hình VC dựa trên LM hiện tại không thể áp dụng cho các tình huống streaming, hạn chế tiện ích thực tế của chúng. Bài báo này giải quyết khoảng cách này bằng cách điều tra các khả năng zero-shot của các mô hình ngôn ngữ được thiết kế riêng cho các tình huống streaming.

**Chuyển đổi Giọng nói Streaming.** Mặc dù chuyển đổi chất lượng cao đạt được bởi các mô hình VC không streaming, cấu trúc không thể streaming và sự phụ thuộc vào đầu vào toàn câu nói của chúng cản trở chúng cho các ứng dụng streaming thời gian thực. Đối với streaming, xử lý nhân quả và cấu trúc của pipeline streaming là những cân nhắc quan trọng. Các mô hình streaming bị buộc phải xử lý đầu vào theo khung hoặc theo chunk ngay lập tức, không có quyền truy cập vào thông tin tương lai, dẫn đến sự suy giảm hiệu suất so với các đối tác không streaming. Để giải quyết điều này, một phương pháp phổ biến (Hayashi et al., 2022; Kameoka et al., 2021; Ning et al., 2023, 2024) liên quan đến việc tích hợp mô hình giáo viên để hướng dẫn việc huấn luyện mô hình streaming hoặc chưng cất kiến thức từ mô hình không streaming. Chen et al. (2023b) tập trung vào việc chọn BNF với mất mát thông tin ngữ nghĩa tối thiểu thông qua phân tích theo lớp, trong khi Chen et al. (2022) kết hợp huấn luyện đối kháng để tăng cường chất lượng của các đặc trưng ngữ nghĩa. Ngoài VC streaming, một số nỗ lực gần đây đã hướng tới VC zero-shot streaming. Ví dụ, VQMIVC (Wang et al., 2021), được thiết kế cho ứng dụng không streaming, được Yang et al. (2022) sửa đổi để có thể streaming. ALO-VC (2023a) xây dựng hệ thống streaming sử dụng mô hình SV, bộ trích xuất PPG streaming và bộ trích xuất cao độ. Tuy nhiên, VC zero-shot streaming hiện tại, được thiết kế cho các thiết bị tài nguyên thấp, có khả năng mô hình hạn chế với khả năng tổng quát hóa kém cho những người nói chưa nhìn thấy, dẫn đến độ tương tự và tự nhiên kém. Được thúc đẩy bởi thành công của LM trong VC zero-shot, chúng tôi thiết kế LM có thể streaming trong các tình huống streaming. Để giải quyết những thách thức đặc biệt trong VC streaming, chúng tôi tăng cường nhận thức ngữ cảnh của LM để cải thiện chất lượng chuyển đổi.

**Tạo sinh Giọng nói dựa trên Mô hình Ngôn ngữ.** Những tiến bộ gần đây của LM trong xử lý ngôn ngữ tự nhiên đã thể hiện khả năng tạo sinh mạnh mẽ, ảnh hưởng đến việc phát triển LM trong tạo sinh giọng nói. Bằng cách sử dụng codec (Zeghidour et al., 2021) hoặc các mô hình SSL khác (Chung et al., 2021), giọng nói và âm thanh có thể được tokenized hiệu quả thành các đơn vị rời rạc, tạo điều kiện cho biểu diễn âm thanh bitrate thấp và trích xuất ngữ nghĩa. Tiến bộ này cho phép tạo sinh giọng nói sử dụng các khung LM một cách liền mạch. Lấy tạo sinh âm thanh như một nhiệm vụ mô hình ngôn ngữ có điều kiện, AudioLM (2023) sử dụng mô hình ngôn ngữ phân cấp cho dự đoán âm thanh từ các đơn vị thô đến tinh. VALL-E (2023b) và SpearTTS (2023) mở rộng LM cho TTS zero-shot, có thể nhân bản giọng nói của con người với các token gợi ý từ bản ghi âm ngắn. Đối với VC zero-shot, LM-VC (2023c) sử dụng tối ưu hóa hướng nhiệm vụ cho nhiệm vụ này. Và một số nghiên cứu (Zhu et al., 2023; Yang et al., 2023) tận dụng các mục tiêu và tập dữ liệu đa nhiệm vụ, đạt được chuyển đổi chất lượng cao. Mặc dù có tiến bộ này, các mô hình VC dựa trên LM hiện tại thường áp dụng xử lý offline, đòi hỏi câu nói hoàn chỉnh từ giọng nói nguồn, điều này cản trở tính phù hợp của chúng cho các ứng dụng streaming thời gian thực. Trái ngược với các nghiên cứu trước, chúng tôi khám phá khả năng zero-shot của VC dựa trên LM cho các tình huống streaming. Với việc tăng cường nhận thức ngữ cảnh, mô hình VC dựa trên LM được đề xuất đạt được kết quả có thể so sánh với VC dựa trên LM không streaming.

## 3 StreamVoice

### 3.1 Tổng quan

Như được hiển thị trong Hình 2, việc phát triển StreamVoice tuân theo khung nhận dạng-tổng hợp. Trong khung này, giọng nói trước tiên được biểu diễn dưới dạng các đặc trưng ngữ nghĩa s={s1, s2, ...sTs} và các đặc trưng âm thanh a={a1, a2, ..., aTa} bởi mô hình ASR streaming được huấn luyện trước và mô hình codec giọng nói tương ứng. Ở đây, Ts và Ta biểu thị độ dài chuỗi. Trước khi đưa vào StreamVoice, s và a được căn chỉnh với cùng độ dài T. StreamVoice kết hợp mô hình ngôn ngữ nhận biết ngữ cảnh và bộ dự đoán âm thanh để thực hiện quá trình mô hình ngôn ngữ duy nhất. Với các đặc trưng ngữ nghĩa và âm thanh {s̃, ã} của giọng nói từ người nói mục tiêu như gợi ý người nói, LM tận dụng thông tin ngữ nghĩa s1:t của giọng nói nguồn để dự đoán tự hồi quy đầu ra ẩn ch. Trong mỗi bước thời gian tự hồi quy của LM, bộ dự đoán âm thanh chuyển đổi đầu ra ẩn ch thành đặc trưng codec â của giọng nói được chuyển đổi. Cuối cùng, mô hình codec tái tạo dạng sóng từ đặc trưng codec được dự đoán. Trong các phần tiếp theo, chúng tôi sẽ giới thiệu cách xây dựng LM có thể streaming cho VC và cách đảm bảo cuộc trò chuyện chất lượng cao của VC streaming này.

### 3.2 Kiến trúc Có thể Streaming

Để thực hiện VC streaming, một kiến trúc có thể streaming là cần thiết. Trong StreamVoice, mô hình ngôn ngữ được thiết kế cẩn thận để thực hiện xử lý nhân quả đầy đủ trong nhiệm vụ VC, và bộ dự đoán âm thanh được thiết kế để đạt được dự đoán theo khung mà không phụ thuộc vào thông tin thời gian.

#### 3.2.1 Mô hình Ngôn ngữ Hoàn toàn Tình cờ

Như được hiển thị trong Hình 3, được truyền cảm hứng bởi thành công của mô hình VC dựa trên LM, chúng tôi có ý định đạt được VC zero-shot streaming bằng các mô hình ngôn ngữ. Trong các mô hình VC dựa trên LM trước đây (Wang et al., 2023c), nhu cầu của đặc trưng ngữ nghĩa hoàn chỉnh s từ giọng nói nguồn để đạt được chuyển đổi cản trở việc triển khai cho ứng dụng thời gian thực, có thể được hình thức hóa là p(at|s1:Ts,a1:t−1) cho mỗi bước thời gian. Để đạt được streaming, bất kỳ thành phần nào của LM không thể dựa vào thông tin tương lai. Như được hiển thị trong Hình 3, LM chỉ giải mã với attention đơn hướng có thể dễ dàng đáp ứng yêu cầu tạo sinh tình cờ. Để loại bỏ sự phụ thuộc của đầu vào ngữ nghĩa hoàn chỉnh, các đặc trưng ngữ nghĩa và âm thanh {s,a} trước tiên được căn chỉnh với nhau đến cùng độ dài chuỗi T và sau đó chúng được đưa vào LM một cách thay thế, tạo thành embedding chéo như {s1, a1, s2, a2, ..., sT, aT}. Với những sửa đổi này, LM có thể đạt được xử lý streaming, mô hình p(at|s1:t,a1:t−1).

Cụ thể, đặc trưng ngữ nghĩa s thu được qua mô hình ASR bao gồm một chuỗi embedding, được ký hiệu là {s1, s2, ..., sT}. Mặt khác, các token codec thu được từ codec L lớp là các đơn vị rời rạc được biểu diễn bởi a∈RT×L. Để thu được chuỗi embedding âm thanh, các token codec từ mỗi lớp trải qua embedding riêng biệt vào không gian embedding, và sau đó chúng được nối theo chiều embedding, tạo ra embedding âm thanh được hợp nhất. Cả embedding âm thanh được hợp nhất và các đặc trưng ngữ nghĩa đều được chuyển đổi sang cùng chiều bằng các lớp tuyến tính. Tiếp theo, chúng được đưa vào mô hình ngôn ngữ một cách thay thế, tạo thành embedding chéo.

#### 3.2.2 Bộ Dự đoán Âm thanh

Vì LM trước đó về cơ bản đã mã hóa nội dung và người nói vào đầu ra ch của nó, bộ dự đoán âm thanh có thể được thiết kế không liên quan đến thời gian để chuyển đổi ch thành không gian codec âm thanh, có nghĩa là bộ dự đoán có thể được áp dụng dễ dàng trong tình huống streaming. Cho rằng giọng nói có thể được biểu diễn trong các đặc trưng âm thanh bởi codec thần kinh ở dạng liên tục hoặc rời rạc, chúng tôi điều tra việc kết hợp cả hai đặc trưng trong StreamVoice, được thực hiện bởi phép chiếu liên tục và phép chiếu rời rạc tương ứng.

**Phép chiếu Liên tục.** Theo Shen et al. (2024), vector latent quantized D chiều a∈RT×D được mã hóa bởi mô hình codec được sử dụng như biểu diễn âm thanh liên tục. Việc dự đoán biểu diễn liên tục liên quan đến việc sử dụng một chồng các lớp tuyến tính, như được hiển thị trong Hình 4. Mất mát phép chiếu liên tục được tính như khoảng cách L2 giữa đặc trưng âm thanh được dự đoán â và đặc trưng âm thanh thật a, được định nghĩa là:

LCont=||a−â||²₂. (1)

**Phép chiếu Rời rạc.** Nói chung, codec được thiết kế với các quantizer đa lớp để nén giọng nói gốc thành các chỉ số rời rạc L lớp a∈RT×L với bitrate thấp. Hầu hết công việc dựa trên LM (Wang et al., 2023b,c) chồng nhiều LM để dự đoán các đặc trưng rời rạc, làm cho pipeline phức tạp và không phù hợp cho tình huống streaming. Ngược lại, StreamVoice áp dụng phương pháp dự đoán codec đa lớp được đơn giản hóa được lấy cảm hứng từ MQTTS (Chen et al., 2023a). Phương pháp này, không có phụ thuộc thời gian, có thể tích hợp liền mạch vào quá trình streaming của mô hình ngôn ngữ. Cụ thể, một transformer một lớp được sử dụng để mô hình phân phối có điều kiện phân cấp của các codec. Như được mô tả ở bên phải của Hình 4, tại thời điểm t, transformer sử dụng ch như điều kiện bắt đầu và tuần tự tạo ra alt từ lớp 1 đến L. Đáng chú ý, quá trình tạo sinh này độc lập với ch trước đó hoặc tương lai, làm cho nó rất phù hợp với nhu cầu của tình huống streaming. Đáng chú ý, trong StreamVoice được đề xuất, chúng tôi chủ yếu kết hợp phép chiếu rời rạc để đạt được dự đoán âm thanh. Mất mát phép chiếu rời rạc có thể được mô tả như:

LDisc = −log∏ᵀₜ₌₁∏ᴸₗ₌₁p(alt|a1:t−1,ms1:t,t,a1:l−1t). (2)

### 3.3 Tăng cường Nhận thức Ngữ cảnh

Do bất lợi của tính nhân quả trong khung streaming, các mô hình streaming đối mặt với việc thiếu tiếp nhận tương lai và sự suy giảm hiệu suất tiềm ẩn so với mô hình không streaming, trong khi đầu vào ngữ nghĩa chất lượng thấp từ ASR streaming, như chúng tôi đã đề cập trong Phần 1, làm cho việc đạt được chuyển đổi chất lượng cao thách thức hơn. Để giải quyết những vấn đề này, một phương pháp tăng cường nhận thức ngữ cảnh được đề xuất, có thể giảm thiểu thông tin ngữ cảnh không đầy đủ phát sinh từ đầu vào ngữ nghĩa và sự vắng mặt của thông tin tương lai. Cụ thể, chúng tôi giới thiệu dự đoán tự hồi quy che ngữ cảnh trong LM để tăng cường việc nắm bắt ngữ cảnh lịch sử từ đầu vào ngữ nghĩa đã cho. Đồng thời, một dự báo ngữ cảnh hướng dẫn bởi giáo viên được đề xuất để đảm bảo mô hình có thể tưởng tượng ngữ cảnh tương lai dựa trên ngữ cảnh lịch sử của nó.

**Dự đoán Tự hồi quy Che ngữ cảnh.** Như được hiển thị ở bên trái của Hình 3, LM được thực hiện bởi Transformer đa lớp với attention đơn hướng, theo việc triển khai của LLaMA (Touvron et al., 2023). Để tăng cường nhận thức ngữ cảnh từ đầu vào ngữ nghĩa đã cho, việc che ngữ nghĩa được giới thiệu trong LM để khuyến khích dự đoán âm thanh từ ngữ nghĩa bị hỏng. Cụ thể, trong một chuỗi các token ngữ nghĩa s={s1, s2, ...sT}, chúng tôi chọn ngẫu nhiên một số chỉ số như chỉ số bắt đầu với tỷ lệ r, và các khoảng l bước được che bởi [M]. Sau khi che, LM lấy đặc trưng ngữ nghĩa bị hỏng ms làm đầu vào và thực hiện tự hồi quy. Với phương pháp này, một cổ chai thông tin cũng được tạo ra ngầm định trong đặc trưng ngữ nghĩa để giảm thông tin người nói. Hơn nữa, trong quá trình huấn luyện, chúng tôi không sử dụng một cách rõ ràng clip giọng nói như gợi ý người nói. Thay vào đó, LM tận dụng chuỗi trước đó {s1:t−1,a1:t−1, st} như gợi ý để tự hồi quy tạo ra biểu diễn ẩn ht cho dự đoán âm thanh tiếp theo. Đáng chú ý, khi đầu vào hiện tại là at, đầu ra tương ứng được bỏ qua và không liên quan đến các bước tiếp theo.

**Dự báo Ngữ cảnh Hướng dẫn bởi Giáo viên.** Như đã thảo luận trước đây, sự vắng mặt của thông tin tương lai dẫn đến mất mát thông tin ngữ cảnh dẫn đến sự suy giảm hiệu suất chuyển đổi. Được lấy cảm hứng từ việc học biểu diễn hiệu quả được thể hiện bởi mã hóa dự đoán tự hồi quy (2019) (APC), chúng tôi giới thiệu dự báo ngữ cảnh hướng dẫn bởi giáo viên được hướng dẫn bởi ASR không streaming để tăng cường đầu ra tự hồi quy, như được trình bày ở bên phải của Hình 3. Điều này cho phép mô hình học một vector ngữ cảnh chứa thông tin tương lai được hình dung. Cụ thể, biểu diễn ngữ cảnh c trước tiên được dẫn xuất bởi dự đoán tuyến tính từ các đặc trưng ẩn h, được tạo ra bởi LM thông qua ngữ cảnh lịch sử. Tiếp theo, ct này được khuyến khích khám phá thông tin ngữ cảnh tương lai tổng quát hơn bằng cách giảm thiểu khoảng cách L2 không chỉ với k đặc trưng ngữ nghĩa từ các bước thời gian tương lai st+1, ...,st+k mà còn với ngữ nghĩa hiện tại st. Phương pháp giảm thiểu kép này góp phần vào việc phân phối nội dung chính xác và tăng cường khả năng dự báo ngữ cảnh tương lai. Mất mát có thể được tóm tắt như:

LTF=1/(T−k)∑ᵀ⁻ᵏ₁‖ct−Concat(st,st+1,...,st+k)‖²₂ (3)

trong đó Concat(·) biểu thị việc nối các đặc trưng dọc theo trục chiều. Không giống như APC gốc, hoạt động giữa đầu vào và đầu ra của mô hình tự hồi quy, phương pháp của chúng tôi sử dụng mô hình ASR không streaming như giáo viên để cung cấp thông tin ngữ nghĩa s để hướng dẫn quá trình dự báo này. Điều này được thực hiện để giải quyết thách thức vốn có của việc thu được các đặc trưng ngữ nghĩa chất lượng cao từ ASR streaming. Sau các chuyển đổi chiều, biểu diễn ngữ cảnh c sau đó được kết hợp với h để tạo thành ch tăng cường ngữ cảnh, sau đó được đưa vào bộ dự đoán âm thanh.

Hơn nữa, vì đặc trưng ngữ nghĩa {s,s} vẫn có thể chứa thông tin liên quan đến người nói. Để đảm bảo thêm việc tách riêng giọng nói, bộ điều chỉnh cổ chai (Qian et al., 2019), ép thông tin người nói ra bằng cách giảm kích thước chiều với lớp tuyến tính, được áp dụng trong s và c.

### 3.4 Quy trình Huấn luyện & Suy luận

**Huấn luyện.** Trong quá trình huấn luyện, mô hình ngôn ngữ tăng cường ngữ cảnh và bộ dự đoán âm thanh được huấn luyện cùng nhau. Tổng mất mát có thể được mô tả như Ltotal=LTF+LCont cho codec liên tục hoặc Ltotal=LTF+LDisc cho phiên bản rời rạc.

**Suy luận Streaming.** Chúng tôi sử dụng các đặc trưng ngữ nghĩa và âm thanh từ clip giọng nói ngắn của người nói mục tiêu như gợi ý người nói. Vì clip này được chọn ngẫu nhiên, có thể chứa phát âm chưa hoàn thành ở cuối clip, chúng tôi thêm clip im lặng sau bản ghi người nói trước quá trình chuyển đổi để ngăn chặn sự tiếp tục không mong muốn. Với gợi ý này, StreamVoice có thể chuyển đổi streaming giọng nói nguồn. Trong phép chiếu rời rạc, chúng tôi sử dụng giải mã tham lam để chọn token codec với xác suất cao nhất. Ngoài ra, để đảm bảo suy luận streaming thời gian thực của StreamVoice, chúng tôi sử dụng bộ nhớ đệm key-value trong LM để giảm tính toán dư thừa. Trong thực tế, vì phần đầu và cuối của giọng nói nguồn có thể được xác định bởi ASR hoặc phát hiện hoạt động giọng nói (VAD), chúng tôi không sử dụng các kỹ thuật như attention cửa sổ hoặc attention trượt để xử lý đầu vào. Hiệu suất của StreamVoice giảm khi đầu vào dài vượt quá độ dài huấn luyện tối đa. Đáng chú ý, những kỹ thuật này có thể được tích hợp dễ dàng vào khung của chúng tôi, cung cấp tính linh hoạt cho các mở rộng trong tương lai.

## 4 Thí nghiệm

### 4.1 Thiết lập Thí nghiệm

**Corpus.** Một tập dữ liệu hỗn hợp bao gồm 1,500 giờ Aishell3 (Shi et al., 2021) và một tập dữ liệu tiếng Trung nội bộ được sử dụng để huấn luyện StreamVoice và Audiodec (Wu et al., 2023). Tập dữ liệu nội bộ chứa các bản ghi từ 2679 người nói tiếng Trung, trong khi chúng tôi sử dụng các câu nói từ 200 người nói trong Aishell3. Để trích xuất các đặc trưng ngữ nghĩa, chúng tôi kết hợp ASR streaming Fast-U2++ (Liang et al., 2023), được triển khai bởi WeNet (Yao et al., 2021) và được huấn luyện trên WenetSpeech (Zhang et al., 2022). Để kiểm tra zero-shot, một tập 400 cặp kiểm tra được chọn từ DIDISpeech (Guo et al., 2021) và EMIME (Wester, 2010), mỗi cặp có một câu nói của người nói nguồn và mục tiêu. Để đánh giá những người nói đã nhìn thấy, tám người nói từ Aishell3 được chọn để tạo thành 160 cặp chuyển đổi. Và câu nói 3s được sử dụng như gợi ý người nói trong suy luận. Thời lượng của các câu nói kiểm tra là từ 3s đến 7s.

**Chi tiết Triển khai.** Chúng tôi sử dụng mã nguồn mở¹ của Audiodec, có 4 lớp quantizer với kích thước codebook 1024 và chiều codebook 64, biểu diễn dạng sóng 24kHz trong độ dài khung 20ms. Fast-U2++ sử dụng kích thước chunk 80ms để thực hiện suy luận streaming và nén dạng sóng 16kHz thành đặc trưng ngữ nghĩa với độ dài khung 40ms. StreamVoice chứa 101M tham số. Đối với LM tăng cường ngữ cảnh, chúng tôi sử dụng biến thể của Transformer, LLaMA (Touvron et al., 2023), với 6 lớp và 8 đầu. Các kích thước ẩn và trung gian là 1024 và 4096. Chúng tôi sử dụng mã chính thức² để triển khai bộ dự đoán âm thanh, sử dụng decoder Transformer một lớp với kích thước ẩn 256, kích thước ẩn feed-forward 1024, và 4 đầu. Trong việc che ngữ nghĩa, tỷ lệ che r dao động từ 0.01 đến 0.02, và khoảng l được đặt thành 10. Bước dự báo k được đặt thành 4. Bộ điều chỉnh cổ chai nén các chiều đặc trưng 6 lần. Trong quá trình huấn luyện, độ dài huấn luyện tối đa được đặt thành 12s. StreamVoice được huấn luyện bằng 8 GPU V100 với kích thước batch 7 câu nói mỗi GPU trong 700k bước. Chúng tôi sử dụng bộ tối ưu AdamW với tốc độ học 5×10⁻⁴. Suy giảm theo cấp số nhân cập nhật tốc độ học sau mỗi epoch, sử dụng tỷ lệ suy giảm 0.986.

**Chỉ số Đánh giá.** Điểm ý kiến trung bình (MOS) đo lường chủ quan tự nhiên của giọng nói (NMOS) và độ tương tự người nói (SMOS), được tính với khoảng tin cậy 95%. Chúng tôi chọn ngẫu nhiên 120 cặp kiểm tra cho đánh giá chủ quan liên quan đến một nhóm 15 người nghe. Đối với đánh giá khách quan, một hệ thống dựa trên mạng nơ-ron với triển khai nguồn mở³ được sử dụng để đo lường chất lượng giọng nói (WV-MOS). Tỷ lệ lỗi ký tự (CER) được đo bởi mô hình ASR⁴ cho biết tính dễ hiểu của giọng nói. Độ tương tự người nói (SSIM) được tính bởi mô hình SV (Desplanques et al., 2020) để xác định xem giọng nói được chuyển đổi có phù hợp với người nói mục tiêu hay không. Hệ số thời gian thực (RTF) và độ trễ cho biết hiệu suất streaming.

### 4.2 Kết quả Thí nghiệm

#### 4.2.1 Đánh giá Zero-shot

Để đánh giá hiệu suất VC zero-shot, một hệ thống VC zero-shot dựa trên LM gần đây, LM-VC(Wang et al., 2023c), được chọn làm hệ thống topline. Ngoài ra, một biến thể của StreamVoice, được gọi là NS-StreamVoice, sử dụng ASR không streaming để trích xuất ngữ nghĩa, cũng được so sánh. Chúng tôi triển khai hệ thống được đề xuất StreamVoice tích hợp phép chiếu rời rạc, trong khi C-StreamVoice cũng liên quan đến đánh giá vì giọng nói có thể được biểu diễn ở dạng liên tục bởi mô hình codec. Bảng 1 trình bày cả kết quả chủ quan và khách quan. So với topline không streaming LM-VC, StreamVoice được đề xuất của chúng tôi có thể đạt được kết quả gần tương tự về NMOS và SMOS chủ quan, trong khi vẫn tồn tại khoảng cách hiệu suất. Kết quả tương tự cũng được quan sát trong kết quả khách quan. StreamVoice không streaming thậm chí còn vượt qua mô hình topline trong một số khía cạnh nhất định, cho thấy tính hiệu quả của kiến trúc có thể streaming của chúng tôi cho VC zero-shot. Ngoài ra, C-StreamVoice thể hiện hiệu suất kém hơn so với phiên bản rời rạc, có thể góp phần vào sự quá trơn trong tạo sinh giọng nói (Ren et al., 2022) và sự không khớp giữa sự thật cơ bản và các đặc trưng được dự đoán.

Như được minh họa trong Bảng 2, RTF của toàn bộ pipeline dưới 1, đáp ứng yêu cầu thời gian thực. Bao gồm độ trễ chờ chunk (80ms) và độ trễ suy luận mô hình, độ trễ pipeline tổng thể là 124.3 ms. Nếu sử dụng GPU V100, StreamVoice có thể đạt được RTF 0.56, và độ trễ tổng thể đạt 137.2 ms. Quan trọng, không giống như VC streaming trước đây, mô hình VC của chúng tôi hoàn toàn nhân quả mà không có bất kỳ nhìn trước tương lai nào, làm nổi bật khả năng mô hình mạnh mẽ của nó. Những kết quả này cho thấy StreamVoice có thể đạt được VC zero-shot chất lượng cao trong các tình huống streaming.

#### 4.2.2 Đánh giá Trong tập dữ liệu

Để có cái nhìn sâu sắc hơn về StreamVoice, chúng tôi đã tiến hành đánh giá trong tập dữ liệu trên tám người nói đã nhìn thấy, như được hiển thị trong Bảng 3. Một hệ thống VC không streaming (Tian et al., 2020) đạt được VC any2many, được chọn, được gọi là NS-VC. Ngoài ra, IBF-VC(Chen et al., 2022) và DualVC2 (Ning et al., 2024) là các mô hình streaming được đề xuất gần đây cho VC any2many. Như quan sát được, một khoảng cách hiệu suất tồn tại giữa topline không streaming mạnh và các mô hình streaming. Trong số các mô hình streaming, StreamVoice, được thiết kế cho tình huống zero-shot, mang lại kết quả tương tự như các hệ thống được thiết kế cho những người nói trong tập dữ liệu, mặc dù StreamVoice sử dụng kích thước chunk nhỏ hơn 80ms trong ASR streaming, đạt được hiệu suất ASR thấp hơn. Ngược lại, IBF-VC và DualVC2 sử dụng kích thước chunk 160ms của ASR cho VC streaming. Điều này cho thấy khả năng chuyển đổi tốt của StreamVoice. Với các câu nói có sẵn của người nói mục tiêu, tinh chỉnh mang lại hiệu suất vượt trội. Điều này cho thấy hệ thống của chúng tôi có thể được áp dụng dễ dàng cho các tình huống khác nhau có hoặc không có câu nói của người nói mục tiêu.

### 4.3 Nghiên cứu Loại bỏ

Như được trình bày trong Bảng 4, chúng tôi đã tiến hành một số nghiên cứu loại bỏ. Trong w/o dự báo ngữ cảnh hướng dẫn bởi giáo viên, chúng tôi loại bỏ việc dự đoán thông tin ngữ nghĩa hiện tại và tương lai, tạo thành hai loại bỏ w/oLTF(st) và w/oLTF(st+1:t+k). Như có thể thấy, một sự giảm đáng chú ý xảy ra trong tất cả các chỉ số đánh giá khi LTF(st+1:t+k) bị loại bỏ, đặc biệt là trong WVMOS và CER. Điều này cho thấy rằng dự báo này cải thiện hiệu suất trong việc nắm bắt nội dung ngôn ngữ. Nhưng khi chỉ tích hợp ngữ cảnh từ ngữ nghĩa tương lai, mô hình w/oLTF(st) đối mặt với mất mát hiệu suất nghiêm trọng. Điều này cho thấy chỉ sử dụng thông tin tương lai can thiệp vào việc cung cấp nội dung ngôn ngữ hiện tại. Trong w/o che ngữ nghĩa, chúng tôi quan sát sự giảm hiệu suất trong tất cả các chỉ số đánh giá khi việc che ngữ nghĩa bị loại bỏ. Điều này cho thấy StreamVoice, được huấn luyện với việc che ngữ nghĩa, hiệu quả tăng cường việc học ngữ cảnh từ đầu vào trước đó trong khi cải thiện việc nắm bắt âm sắc người nói. Hơn nữa, kết quả của việc loại bỏ bộ điều chỉnh cổ chai cho thấy rằng việc tích hợp của nó hiệu quả ngăn chặn thông tin người nói nguồn có trong đặc trưng ngữ nghĩa khỏi rò rỉ vào giọng nói được chuyển đổi, với ít ảnh hưởng đến chất lượng giọng nói.

### 4.4 Thảo luận: Phân tích Phụ thuộc

Trong phần này, chúng tôi sẽ khám phá các mối quan hệ phụ thuộc giữa việc lựa chọn ASR và codec và hiệu suất của StreamVoice.

**ASR.** Để điều tra tác động của ASR đến StreamVoice, ba hệ thống ASR đại diện, bao gồm ASR không streaming⁴, ASR streaming dựa trên CTC được sử dụng rộng rãi (Moritz et al., 2019), và Fast-U2++ streaming được đề xuất gần đây (Liang et al., 2023), được chọn để thực hiện trích xuất ngữ nghĩa. Như có thể thấy trong Bảng 5, StreamVoice sử dụng các đặc trưng ngữ nghĩa của ASR không streaming vượt trội hơn những cái sử dụng ASR streaming. Sự khác biệt này có thể được cho là do khoảng cách hiệu suất vốn có giữa các mô hình ASR không streaming và streaming, dẫn đến các khả năng trích xuất ngữ nghĩa khác nhau. Ngoài ra, không có nhìn trước tương lai trong StreamVoice, việc sử dụng các đặc trưng ngữ nghĩa từ (Moritz et al., 2019) không thể đạt được chuyển đổi hợp lý, trong khi chúng tôi giới thiệu nhìn trước tương lai 160ms trong StreamVoice, tức là mô hình p(at|a1:t−1,s1:t+m,t) với m nhìn trước tương lai, mang lại kết quả chuyển đổi tốt. Vấn đề này có thể phát sinh từ sự phân phối spike CTC trì hoãn và độ trễ phát token tồn tại trong ASR streaming (Liang et al., 2023), dẫn đến sự dịch chuyển thông tin ngữ nghĩa. Nhờ vào độ trễ phát thấp của Fast-U2++, StreamVoice có thể thực hiện chuyển đổi mà không cần nhìn trước tương lai. Với kích thước chunk dài hơn được sử dụng trong Fast-U2++, StreamVoice có thể đạt được kết quả tốt hơn trong khi đạt đến độ trễ lớn hơn 270ms. Vẫn tồn tại sự đánh đổi giữa hiệu suất và tốc độ.

**Codec.** Trong StreamVoice, chúng tôi sử dụng codec streaming độ trễ thấp Audiodec (Wu et al., 2023). Như được trình bày trong Bảng 6, chúng tôi xác thực hiệu suất của StreamVoice sử dụng codec với các bitrate khác nhau, bao gồm 2kbps và 8kbps, trong đó codec bitrate cao hơn đạt được chất lượng tái tạo vượt trội hơn so với những cái bitrate thấp hơn. Audiodec 2kbps sử dụng 4 lớp quantization và biểu diễn âm thanh với độ dài khung 20ms, trong khi Audiodec 8kbps sử dụng 8 lớp với độ dài khung 10ms. Sử dụng cấu hình StreamVoice được đề cập trong Phần 4.1, kết quả trong các bitrate khác nhau của các mô hình codec không cho thấy sự khác biệt rõ ràng. Khi tăng số lượng lớp transformer trong bộ dự đoán codec, tạo thành Large w/ 8kbps Audiodec, hiệu suất chuyển đổi sử dụng codec 8kbps cải thiện đáng kể, nhưng dẫn đến suy luận chậm hơn. Kết quả này cho thấy thiết kế của StreamVoice phụ thuộc vào cấu hình codec, ảnh hưởng đến cả chất lượng chuyển đổi và tốc độ suy luận.

## 5 Kết luận

Bài báo này giới thiệu StreamVoice, một hệ thống VC zero-shot mới lạ dựa trên LM được thiết kế cho các tình huống streaming. Cụ thể, StreamVoice sử dụng khung một giai đoạn bao gồm LM nhận biết ngữ cảnh và bộ dự đoán âm thanh. Thiết kế tình cờ của đầu vào và cấu trúc mô hình đảm bảo tuân thủ hành vi streaming. Để giải quyết sự suy giảm hiệu suất được gây ra bởi thông tin ngữ cảnh hoàn chỉnh bị thiếu trong các tình huống streaming, LM nhận biết ngữ cảnh áp dụng dự báo ngữ cảnh hướng dẫn bởi giáo viên để làm cho mô hình có thể dự báo thông tin hiện tại và tương lai được đưa ra bởi giáo viên. Ngoài ra, việc che ngữ nghĩa được giới thiệu trong LM để tăng cường việc học ngữ cảnh từ đầu vào lịch sử và tạo điều kiện cho việc tách riêng tốt hơn. Cuối cùng, một bộ dự đoán âm thanh hợp tác với LM để tạo ra giọng nói mục tiêu. Các thí nghiệm chứng minh rằng StreamVoice đạt được VC zero-shot streaming trong khi duy trì hiệu suất có thể so sánh với các hệ thống VC không streaming.

## 6 Hạn chế

Chúng tôi phải chỉ ra rằng StreamVoice vẫn có hạn chế. Trong cấu hình của chúng tôi, StreamVoice cần GPU, như V100 và A100, để đạt được suy luận streaming thời gian thực. Thiết kế của VC streaming phụ thuộc rất nhiều vào ASR và codec giọng nói như đã đề cập trong Phần 4.4. Ngoài ra, StreamVoice cũng đối mặt với vấn đề ngoài miền, gây ra sự suy giảm hiệu suất cho các câu nói có giọng, cảm xúc mạnh, hoặc môi trường ghi âm chưa nhìn thấy. Công việc tương lai của chúng tôi sẽ đầu tiên sử dụng nhiều dữ liệu huấn luyện hơn với bao phủ đa dạng để khám phá khả năng mô hình của StreamVoice. Ngoài ra, chúng tôi sẽ tập trung vào việc tối ưu hóa pipeline streaming của chúng tôi, chẳng hạn như codec độ trung thực cao với bitrate thấp và mô hình streaming thống nhất.

## 7 Tuyên bố Đạo đức

Vì StreamVoice có thể chuyển đổi giọng nói nguồn thành những người nói mong muốn, nó có thể mang theo rủi ro tiềm ẩn của việc sử dụng sai cho các mục đích khác nhau, như lan truyền thông tin giả mạo hoặc lừa đảo qua điện thoại. Để ngăn chặn việc lạm dụng công nghệ VC, nhiều nghiên cứu đã tập trung vào phát hiện giọng nói tổng hợp (Yi et al., 2022). Đồng thời, chúng tôi cũng khuyến khích công chúng báo cáo việc sử dụng bất hợp pháp VC cho các cơ quan có thẩm quyền thích hợp.
