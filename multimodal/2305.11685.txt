# 2305.11685.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2305.11685.pdf
# File size: 370516 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Recycle-and-Distill: Universal Compression Strategy for Transformer-based
Speech SSL Models with Attention Map Reusing and Masking Distillation
Kangwook Jang1‚àó, Sungnyun Kim2‚àó, Se-Young Yun2, Hoirin Kim1
1School of Electrical Engineering, KAIST
2Graduate School of AI, KAIST
{dnrrkdwkd12, ksn4397, yunseyoung, hoirkim }@kaist.ac.kr
Abstract
Transformer-based speech self-supervised learning (SSL) mod-
els, such as HuBERT, show surprising performance in various
speech processing tasks. However, huge number of parameters
in speech SSL models necessitate the compression to a more
compact model for wider usage in academia or small compa-
nies. In this study, we suggest to reuse attention maps across
the Transformer layers, so as to remove key and query parame-
ters while retaining the number of layers. Furthermore, we pro-
pose a novel masking distillation strategy to improve the student
model‚Äôs speech representation quality. We extend the distilla-
tion loss to utilize both masked and unmasked speech frames to
fully leverage the teacher model‚Äôs high-quality representation.
Our universal compression strategy yields the student model
that achieves phoneme error rate (PER) of 7.72% and word error
rate (WER) of 9.96% on the SUPERB benchmark.
Index Terms : speech self-supervised learning, model compres-
sion, attention map reusing, masking distillation
1. Introduction
Transformer-based speech SSL models [1, 2, 3] have been ac-
tively studied in speech processing field [4] as SSL arises as a
successful representation learning approach in recent years [5,
6, 7, 8]. Especially for wav2vec 2.0 [9], HuBERT [10], and
wavLM [11], all of which are inherited from BERT [12], show
surprising performance in automatic speech recognition (ASR),
comparable to supervised learning approaches [13, 14]. Since
the versatility of speech SSL becomes also crucial, the above
models have been further explored in various applications in-
cluding automatic speaker verification (ASV) [15] or emotion
recognition (ER) [16].
However, these models have huge number of parameters
and are trained for very long time, which makes it hard for
the resource-limited groups to train their own models. For in-
stance, wav2vec 2.0 L ARGE with 317M parameters should be
pretrained for more than 290 days on a single V100 GPU [9] on
LibriSpeech dataset [17]. This necessitates us to build a com-
pressed model that allows much more parameter-efficient train-
ing and lower computational overhead.
Knowledge distillation (KD) [18] is a common model com-
pression technique where a smaller student model is being
trained by distilling the knowledge from a teacher model. Prior
efforts in distilling large-scale speech SSL models have been
made with reducing the number of Transformer layers or shrink-
ing their width. DistilHuBERT [19] is distilled in a way of
predicting multi-layer outputs of HuBERT, with most of the
Transformer layers removed. FitHuBERT [20], instead of re-
* These authors contributed equally to this work.moving the layers, suggests cutting down the width of atten-
tion and feed-forward network (FFN) in each Transformer layer.
LightHuBERT [21] creates a prunable supernet through distilla-
tion and conducts architecture search to make a small student.
Despite the effectiveness of previous approaches in mitigat-
ing the performance drop by compression, they still face sev-
eral issues. (1) Wide and shallow students [19, 22] still exhibit
degradation on content-related downstream tasks. (2) Layer-to-
layer (L2L) distillation is proved to be effective [20, 22], how-
ever, it is counter-intuitive in terms of compression since ev-
ery layer‚Äôs parameters are required. (3) Pruning by architecture
search [21] prepares an additional teacher-sized supernet using
32 GPUs, which is not end-to-end (E2E) and cannot be easily
trained by resource-limited groups.
We suggest reusing attention maps across the student‚Äôs
Transformer layers, which is inspired by previous works [23,
24] that claimed the similarity between attention maps. Atten-
tion map reusing enables us to remove key and query parameters
in certain Transformer layers, making it unnecessary to retain
all layer parameters for L2L distillation. Furthermore, we can
reinvest the saved parameters to other parts of Transformer.
We also propose a masking with L2L distillation for bet-
ter speech representation quality of our student model. Mask-
ing speech frames is a widely used technique in speech SSL
models [9, 10], trained by predicting the masked representa-
tion. This technique has been simply applied to distilling Hu-
BERT [21], but not in the L2L manner. Our novel masking
distillation scheme aims to fully leverage the teacher‚Äôs repre-
sentation by extending the distillation loss to both masked and
unmasked speech frames. We emphasize that our scheme is an
E2E fashion and enhances the general quality of speech repre-
sentation, especially in content- and semantics-related tasks.
Combining our two approaches described (Fig. 1), we rein-
vest the saved parameters from attention map reusing to FFN,
and create our flagship model, ARMHuBERT (Attention map
Reused M ask HuBERT ). As evaluated on the SUPERB bench-
mark [25], ARMHuBERT achieves overall score [11] of 78.1,
the state-of-the-art E2E distillation. It also reaches 7.72% PER
in phoneme recognition (PR), and 9.96% WER in ASR.
2. Preliminaries
2.1. Transformer-based Speech SSL Models
Recent dominant SSL models in speech field are wav2vec
2.0 [9], HuBERT [10], and wavLM [11], where these three
model structures are identical except for detailed level. Specif-
ically, they share 12 or 24 Transformer [26] layers and 7-layer
1D-CNN. Their pretraining schemes are based on masked pre-
diction, estimating the codewords by output representation of
the masked frames. Despite the superiority and scalabilityarXiv:2305.11685v2  [eess.AS]  26 Oct 2023

--- PAGE 2 ---
Layer 12 KD
MASK
MASK
Layer 1 KD
MASKTeacherStudent
(ùëÑ!,ùêæ!)‚Üíùë®ùüèùëâ!ùë®ùüè‚Üíùë®ùüêùëâ$Transformer Layer 12
Transformer Layer 1Transformer Layer 2‚ãÆLinear proj. 1Linear proj. 2‚ãÆ‚ãÆ
Attn. map reused‚ãÆ
MASKmasked loss ‚Ñí!,‚Ñì$%unmasked loss ‚Ñí&,‚Ñì$%‚úï‚úï
‚úïFigure 1: Our compression strategy involves reusing the attention map of the previous layer and extending the distillation process to
masked (red arrow) and unmasked (blue arrow) representations. The input masked frames are identical for both teacher and student.
of speech SSL models, large number of parameters and their
computational overhead make it difficult to train these models.
We thus implement the model compression on HuBERT and
wavLM, the two dominant SSL models in speech, to demon-
strate the effectiveness of our compression strategy.
2.2. SUPERB Benchmark
The beginning of speech SSL models focused on content-
related downstream tasks such as ASR or PR [27, 28], however,
their versatility to other tasks has been recognized as crucial re-
cently [11]. In this context, SUPERB benchmark [25] has been
proposed to evaluate the generalizability of speech SSL models,
covering the aspects of content, speaker, semantics, and paralin-
guistics. We evaluate our representation against the SUPERB
benchmark to verify the generalizability of our student model.
The SUPERB downstream tasks include PR, ASR, keyword
spotting (KS), query-by-example spoken term detection (QbE),
speaker identification (SID), ASV , speaker diarization (SD), in-
tent classification (IC), slot filling (SF), and ER.
3. Methodology
3.1. Attention Map Reusing
Attention map reusing is a technique for substituting the present
layer‚Äôs attention map with the previous one, which has been
covered in several domains [23, 29]. Prior works [23, 24] have
pointed out the similarity of the attention maps across heads and
layers in pretrained Transformer models, such as BERT [12]
and ViT [30]. We leverage this property by reusing the attention
maps to compress the student model. Alternatively, we can reas-
sign the amount of parameters saved by attention map reusing,
without increasing the total number of parameters.
In Transformer‚Äôs multi-head self-attention (MHSA) mod-
ule [26], the input x‚ààRn√ódwith the sequence length nis
transformed to Hindependent queries, keys, and values by
transformation matrices Wh,k, Wh,q‚ààRd√ódk, and Wh,v‚àà
Rd√ódv, respectively, for each head h. Here, dk,dv, and dare
the width of the keys, values, and model, respectively.Kh=Wh,kx, K h‚ààRn√ódk,
Qh=Wh,qx, Q h‚ààRn√ódk,
Vh=Wh,vx, V h‚ààRn√ódv(1)
Then, key and query are multiplied along the width axis to ob-
tain a scaled dot-product attention map, Ah‚ààRn√ón. Linear
combinations of the attention map and value for each head are
concatenated, and then projected to the original width.
Ah=softmax 
QhK‚ä§
h/p
dk
, (2)
MHSA (x) =
A1V1, . . . , A HVH
Wo, Wo‚ààRHdv√ód(3)
Attention map reusing is to replace Ahwith the previous
layer‚Äôs one. For instance, if we reuse the k-th previous attention
map on the current layer ‚Ñì, the ReuseMHSA module is
ReuseMHSA (x) =
A‚Ñì‚àík
1V‚Ñì
1, . . . , A‚Ñì‚àík
HV‚Ñì
H
W‚Ñì
o. (4)
Accordingly, computing KhandQhcan be omitted, reduc-
ing the number of multiplications and additions by (2nd2+
n2d). Assuming d/H =dv=dk, the omitted computation ac-
counts for half of the original computation for MHSA, which
is(4nd2+ 2n2d). As a result, less parameters and multiply-
accumulates (MACs) are required as more ReuseMHSA mod-
ules are employed (see Sec. 5.1).
3.2. Masking Distillation
Attention map reusing has reduced the number of parameters,
however, it may affect the representation quality of the stu-
dent model. To improve the student‚Äôs representation learning,
we offer a novel masking distillation scheme that leverages the
teacher‚Äôs representation knowledge in a more sophisticated way.
Speech frame masking involves learning representation
through masked prediction, where the model learns to represent
masked frames accurately based on other unmasked frames.
LightHuBERT [21], inspired by data2vec [8], has first applied
the masking strategy to distilling HuBERT. In this approach,
the teacher model guides the representation of masked frames.
Let¬µ(x)be the masked input, and ftandfsthe teacher and

--- PAGE 3 ---
Table 1: Evaluation results on SUPERB benchmark. Metrics include parameter size in million, PER%, WER% (w/o language model),
accuracy (Acc%), maximum term weighted value (MTWV), equal error rate (EER%), diarization error rate (DER%), F1 score (F1%),
and concept error rate (CER%). ‚ÄúOverall‚Äù denotes the average scoring of all tasks proposed in [11]. LightHuBERT [21] works by two-
stage training, where HuBERT-sized supernet needs to be trained first, thus not compared with E2E distillation models. ARMHuBERT-S
and ARMwavLM-S with 960h distillation are trained for 100 epochs.
Content Speaker Semantics Paral.
Params Overall PR ASR KS QbE SID ASV SD IC SF ER
Models Millions ‚Üì Score‚ÜëPER‚ÜìWER‚ÜìAcc‚ÜëMTWV ‚ÜëAcc‚ÜëEER‚ÜìDER‚ÜìAcc‚Üë F1‚Üë CER‚ÜìAcc‚Üë
Baselines
FBANK [25] 0 40.5 82.01 23.18 8.63 0.0058 8.5E-4 9.56 10.55 9.1 69.64 52.94 35.39
HuBERT B ASE [10] 94.70 80.8 5.41 6.42 96.30 0.0736 81.42 5.11 5.88 98.34 88.53 25.20 64.92
wavLM B ASE [11] 94.70 81.9 4.84 6.21 96.79 0.0870 84.51 4.69 4.55 98.63 89.38 22.86 65.94
LightHuBERT aSmall[21] 94.7 ‚Üí27.00 79.1 6.60 8.33 96.07 0.0764 69.70 5.42 5.85 98.23 87.58 26.90 64.12
960h distillation ‚Äì # params: 26.4M ‚àº31.6M
FitW2V2 [20] 31.63 76.5 12.22 11.44 96.04 0.0475 64.71 6.65 6.44 93.38 86.65 29.40 62.35
3-L O NE-Pred [22] 30.58 76.8 13.34 12.23 96.69 0.0489 75.71 6.48 6.56 94.15 82.89 34.65 63.95
12-L H ALF-L2L [22] 26.87 77.6 10.67 10.96 97.24 0.0604 69.52 6.13 6.81 96.97 86.11 30.93 63.24
MaskHuBERT (ours) 26.64 77.8 7.30 9.77 96.36 0.0664 62.83 5.38 6.79 97.05 87.31 27.10 62.37
ARMHuBERT (ours) 26.45 78.1 7.72 9.96 96.88 0.0635 65.03 5.68 7.10 97.07 87.59 26.06 62.86
960h distillation ‚Äì # params: 22.4M ‚àº23.5M
DistilHuBERT [19] 23.49 75.9 16.27 13.37 95.98 0.0511 73.54 8.55 6.19 94.99 82.57 35.59 63.02
FitHuBERT [20] 22.49 74.5 13.32 12.09 96.27 0.0489 55.71 8.00 6.84 91.25 84.06 32.46 59.82
ARMHuBERT-S (ours) 22.39 77.5 8.63 10.82 96.82 0.0720 63.76 5.58 7.01 97.02 86.34 29.02 62.96
ARMwavLM-S (ours) 22.39 78.9 7.42 10.03 97.01 0.0741 71.29 5.99 7.11 97.76 87.41 26.97 64.54
100h distillation
FitW2V2 [20] 22.49 73.1 16.50 14.77 94.68 0.0380 51.65 7.43 6.94 90.03 81.95 34.74 62.87
FitHuBERT [20] 22.49 74.5 14.05 12.66 96.23 0.0579 54.24 7.88 7.19 94.20 83.41 34.00 61.67
ARMHuBERT-S (ours) 22.39 76.8 9.17 11.83 96.01 0.0569 66.48 5.92 6.23 95.97 83.89 33.29 63.29
ARMwavLM-S (ours) 22.39 77.0 8.33 11.37 96.30 0.0579 65.40 6.38 7.41 96.76 84.89 31.95 63.41
student model. Then, the masked loss function becomes
L(x) =1
|M|X
i‚ààMft
i(x)‚àífs
i(¬µ(x))
2(5)
where fiis the i-th frame of the speech representation, and M
is the set of the masked frames.
In addition to the masked part loss (eq. 5), we suggest to
employ an unmasked loss since the teacher model can pro-
vide high-quality representation even on the unmasked frames.
However, if the masking process removes essential frames, dis-
tilling the intact form of ft(x)can leak such essential knowl-
edge that should have been removed. This induces biased pre-
dictions of the student, as it learns information that cannot be
inferred from the masked input.
To prevent this, we make the teacher model receive the
same masked input as the student does when distilling the un-
masked part. Hence, the entire distillation loss becomes
L(x) =X
‚ÑìŒ±‚Ñì
Lm,‚Ñì(x) +Lu,‚Ñì(x)
=X
‚ÑìŒ±‚Ñì
|M|X
i‚ààMft
i,‚Ñì(x)‚àífs
i,‚Ñì(¬µ(x))
2(6)
+X
‚ÑìŒ±‚Ñì
n‚àí |M|X
i/‚ààMft
i,‚Ñì(¬µ(x))‚àífs
i,‚Ñì(¬µ(x))
2
where Œ±‚Ñìis the layerwise coefficient. Lm,‚ÑìandLu,‚Ñìrepresent
masked loss and unmasked loss of the ‚Ñì-th layer, respectively.
In summary, our novel masking distillation strategy appro-
priately guides the student‚Äôs knowledge acquisition, by distill-
ing not only the masked representation of unmasked data but
also the unmasked representation of masked data (see Fig. 1).
In Sec. 5.2, we investigate the strength of our masking strategy
compared to other types of losses.4. Results
4.1. Implementation Details
We distilled the two dominant Transformer-based speech SSL
models, HuBERT B ASE[10] and wavLM B ASE[11], that are
pretrained on LibriSpeech 960 hours dataset [17]. Our stu-
dent model consists of 12 layers of Transformers as the teach-
ers, while the detailed design mostly follows FitHuBERT [20]:
width of attention and FFN reduced and linear projections
adopted at each layer. The layerwise coefficients Œ±‚Ñìare set to
0.1 except for the last layer, where it is set to 1. Unless spec-
ified, the LibriSpeech [17] dataset is distilled for 200 epochs
with effective batch size of 72 including gradient accumulation.
Reuse pattern We employ an alternating reuse pattern for
the attention maps, whereby the attention map of an even-
numbered Transformer layer is repeated by that of the previous
odd-numbered layer. We denote this pattern as 2by6 , our default
setting. We examine other reuse patterns in Sec. 5.1 in terms of
performance, number of parameters, and MACs.
Model description To verify our masking distillation strat-
egy, we first build a student model, MaskHuBERT, which em-
ploys masking distillation only. MaskHuBERT has the width
of (attention, FFN) as (480, 640). Then, 2by6 reuse pattern
is applied to MaskHuBERT, leading to 10.3% of parameter re-
duction. We extend this model to two options: ARMHuBERT
and ARMHuBERT-S. ARMHuBERT is a reinvested version of
MaskHuBERT, where the saved parameters from attention map
reusing are reassigned to FFN, resulting in increased width of
(480, 864). ARMHuBERT-S is a reduced version to match
the parameters with previous works, having the width of (432,
816). To establish the universality of our strategy, we introduce
ARMwavLM-S that is structurally identical to ARMHuBERT-
S, with the only change in teacher from HuBERT to wavLM.

--- PAGE 4 ---
Table 2: Performance comparisons of various reusing patterns.
Parameter size (M) and MACs (G) are additionally measured.
The width of (attention, FFN) for each model is (432, 816),
while ‚Äú-up‚Äù suffix denotes more parameters assigned to FFN
to match with 2by6. Masking is not applied here.
pattern reused layers params MACs WER ‚ÜìEER‚ÜìF1‚ÜëCER‚Üì
6by2 {1,7} 20.90 423 13.52 6.30 83.69 34.92
3by4 {1,4,7,10 } 21.65 437 12.37 5.67 83.29 33.60
6by2-up {1,7} 22.39 440 13.18 5.89 83.07 34.79
3by4-up {1,4,7,10 } 22.39 445 12.39 6.06 83.79 33.49
2by6 {1,3,5,7,9,11 }22.39 450 12.18 5.95 84.91 32.29
None - 24.64 490 11.94 5.87 84.78 31.38
4.2. SUPERB Benchmark Results
In Table 1, we evaluate our student models on SUPERB bench-
mark [25]. We follow the default fine-tuning recipes, includ-
ing a learning rate scheduler, with the learning rate scaled to
10√óin SID task. MaskHuBERT outperforms 12-L H ALF-L2L,
the previous state-of-the-art E2E distillation method, with less
parameters used. Our observation indicates that incorporating
our masking strategy into the L2L distillation [20, 22] results
in enhancing the student‚Äôs representation quality. Especially,
MaskHuBERT highly improves the performances in content-
and semantics-related tasks.
ARMHuBERT achieves a better overall score of 78.1 with
less parameters than MaskHuBERT. Despite the removal of cer-
tain attention parameters, increasing the FFN width contributes
to better quality of speech representation, achieving 7.72%
PER and 9.96% WER. We find out that ARMHuBERT shows
promising improvements when compared to MaskHuBERT in
SF and SID tasks, exhibiting a similar level of performance in
other tasks. In the end, the number of parameters and MACs in
ARMHuBERT have decreased to 28% and 30% of the teacher
model, HuBERT B ASE[10], respectively.
In a smaller parameter group, ARMHuBERT-S, the param-
eter-reduced version, outperforms DistilHuBERT and FitHu-
BERT by a large margin. Specifically, ARMHuBERT-S also
shows the outstanding results in content- and semantics-related
tasks, which means the consistency of the representations pro-
duced by MaskHuBERT and ARMHuBERT-S. In addition, the
result that ARMwavLM-S surpasses ARMHuBERT-S implies
the universality of our strategy: without any modifications of the
student model structure, replacing with a superior teacher model
creates a better student. The results of the LibriSpeech [17]
100h distillation are also consistent with the formerly demon-
strated results.
5. Discussions
In this section, we explore which layer‚Äôs attention map should
be reused in other layers and how to implement the masking
distillation. Unless specified, we have conducted the distil-
lations on 100-hour of LibriSpeech [17] and evaluated on the
ASR, ASV , and SF tasks of the SUPERB benchmark [25].
5.1. Where to Reuse
Table 2 summarizes the performance depending on various at-
tention map reusing patterns, and in general, the 2by6 pattern
performs the best. Other reuse patterns have reduced the Trans-
former‚Äôs representation capacity due to overly frequent reusing.
Assigning more parameters to FFN ( -up) still has limit in terms
of the performance gain. Comparing to no reuse pattern applied,Table 3: Ablation study on our masking strategy.
methods WER ‚ÜìEER‚Üì F1‚Üë CER‚Üì
MaskHuBERT-100h 11.56 5.87 84.31 32.28
[‚Äì] distil. unmasked part 13.23 7.96 82.78 33.53
[‚Äì] distil. from masked input 11.65 6.09 84.29 31.41
Table 4: Performance comparisons with different masking ra-
tios. ‚Äúsch‚Äù indicates linear scheduling of the ratio as 0.4 to 0.8.
models ratio WER ‚ÜìEER‚Üì F1‚Üë CER‚Üì
MaskHuBERT-960h 0.4 9.75 5.58 86.94 26.79
MaskHuBERT-960h 0.8 9.77 5.38 87.31 27.10
MaskHuBERT-100h 0.4 11.56 5.87 84.31 32.28
MaskHuBERT-100h 0.6 11.99 6.18 83.42 33.31
MaskHuBERT-100h 0.8 12.74 6.56 83.68 33.82
MaskHuBERT-100h sch 12.07 6.29 83.84 33.50
the performance decrease of 2by6 is small, but it takes advan-
tages in 9.13% and 8.16% reduction of parameters and MACs,
respectively. We note that the number of MACs in a single reuse
MHSA module (eq. 4) is reduced by half, from 13.2G to 6.6G.
5.2. How to Mask
Masking strategy Table 3 shows the efficacy of our masking
strategy. We first eliminated the loss function on the unmasked
frames ( Lu,‚Ñì), making it equivalent to the L2L version of the
LightHuBERT [21] distillation loss. This approach severely
damaged performances, particularly in the ASR and ASV tasks.
Next, we modified the unmasked loss function to distill from the
unmasked input, i.e., only ft(x)being distilled to the student.
This also led to degraded performance in most tasks, revealing
that our unmasked loss with masked input properly guides the
knowledge acquisition without imposing biased predictions.
Masking ratio High value of masking ratio can lead to a stu-
dent model producing good representation, as it has less infor-
mation to infer with [10, 31]. However, it can also make the
learning process more difficult. In Table 4, we examine the
optimal masking ratios for each training set. For LibriSpeech
960h [17], both ratios of 0.4 and 0.8 produce excellent results.
On the other hand, for the 100h dataset, ratio of 0.4 produces
the best results overall. This implies that lower masking ratio
is preferred in low-resource distillation setting. Accordingly, in
our main experiments, we have used the ratios of 0.8 and 0.4 for
the 960h and 100h distillation, respectively.
6. Conclusion and Future Work
In summary, we have proposed the universal compression strat-
egy which involves attention map reusing and novel masking
distillation. Our parameter-reinvested model, ARMHuBERT,
achieves great performance in content- and semantics-related
tasks. Our strategy can be applied to any Transformer-based
speech SSL models, and contributes to enhancing the general
quality of speech representation. Future work can focus on fur-
ther improving our model on speaker-related tasks.
7. Acknowledgements
The study was supported by Korea Health Technology R&D
Project through the Korea Health Industry Development Insti-
tute funded by the Ministry of Health and Welfare, Republic of
Korea (HR18C0016).

--- PAGE 5 ---
8. References
[1] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, ‚ÄúMock-
ingjay: Unsupervised speech representation learning with deep
bidirectional transformer encoders,‚Äù in IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP) ,
2020, pp. 6419‚Äì6423.
[2] P.-H. Chi, P.-H. Chung, T.-H. Wu, C.-C. Hsieh, Y .-H. Chen, S.-
W. Li, and H.-y. Lee, ‚ÄúAudio albert: A lite bert for self-supervised
learning of audio representation,‚Äù in Spoken Language Technology
Workshop (SLT) . IEEE, 2021, pp. 344‚Äì350.
[3] A. T. Liu, S.-W. Li, and H.-y. Lee, ‚ÄúTera: Self-supervised learn-
ing of transformer encoder representation for speech,‚Äù IEEE/ACM
Transactions on Audio, Speech, and Language Processing ,
vol. 29, pp. 2351‚Äì2366, 2021.
[4] S. Liu, A. Mallol-Ragolta, E. Parada-Cabaleiro, K. Qian, X. Jing,
A. Kathan, B. Hu, and B. W. Schuller, ‚ÄúAudio self-supervised
learning: A survey,‚Äù Patterns , vol. 3, no. 12, p. 100616, 2022.
[5] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfficient esti-
mation of word representations in vector space,‚Äù arXiv preprint
arXiv:1301.3781 , 2013.
[6] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, ‚ÄúDeep cluster-
ing for unsupervised learning of visual features,‚Äù in Proceedings
of the European conference on computer vision (ECCV) , 2018,
pp. 132‚Äì149.
[7] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple
framework for contrastive learning of visual representations,‚Äù in
International conference on machine learning . PMLR, 2020,
pp. 1597‚Äì1607.
[8] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli,
‚ÄúData2vec: A general framework for self-supervised learning in
speech, vision and language,‚Äù in International Conference on Ma-
chine Learning . PMLR, 2022, pp. 1298‚Äì1312.
[9] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, ‚Äúwav2vec
2.0: A framework for self-supervised learning of speech repre-
sentations,‚Äù Advances in Neural Information Processing Systems ,
vol. 33, pp. 12 449‚Äì12 460, 2020.
[10] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, K. Lakhotia, R. Salakhut-
dinov, and A. Mohamed, ‚ÄúHubert: Self-supervised speech rep-
resentation learning by masked prediction of hidden units,‚Äù
IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing , vol. 29, pp. 3451‚Äì3460, 2021.
[11] S. Chen, C. Wang, Z. Chen, Y . Wu, S. Liu, Z. Chen, J. Li,
N. Kanda, T. Yoshioka, X. Xiao et al. , ‚ÄúWavlm: Large-scale self-
supervised pre-training for full stack speech processing,‚Äù IEEE
Journal of Selected Topics in Signal Processing , vol. 16, no. 6,
pp. 1505‚Äì1518, 2022.
[12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-
training of deep bidirectional transformers for language under-
standing,‚Äù in Proceedings of NAACL-HLT , 2019, pp. 4171‚Äì4186.
[13] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y . Wu et al. , ‚ÄúConformer: Convolution-
augmented transformer for speech recognition,‚Äù in Proc. Inter-
speech , 2020.
[14] S. Kim, A. Gholami, A. E. Shaw, N. Lee, K. Mangalam, J. Ma-
lik, M. W. Mahoney, and K. Keutzer, ‚ÄúSqueezeformer: An effi-
cient transformer for automatic speech recognition,‚Äù in Advances
in Neural Information Processing Systems , 2022.
[15] Y . Wang, A. Boumadane, and A. Heba, ‚ÄúA fine-tuned wav2vec
2.0/hubert benchmark for speech emotion recognition, speaker
verification and spoken language understanding,‚Äù arXiv preprint
arXiv:2111.02735 , 2021.
[16] L. Pepino, P. Riera, and L. Ferrer, ‚ÄúEmotion Recognition from
Speech Using wav2vec 2.0 Embeddings,‚Äù in Proc. Interspeech ,
2021, pp. 3400‚Äì3404.
[17] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‚ÄúLib-
rispeech: an asr corpus based on public domain audio books,‚Äù
in2015 IEEE international conference on acoustics, speech and
signal processing (ICASSP) . IEEE, 2015, pp. 5206‚Äì5210.[18] G. Hinton, O. Vinyals, J. Dean et al. , ‚ÄúDistilling the knowledge in
a neural network,‚Äù arXiv preprint arXiv:1503.02531 , vol. 2, no. 7,
2015.
[19] H.-J. Chang, S.-w. Yang, and H.-y. Lee, ‚ÄúDistilhubert: Speech
representation learning by layer-wise distillation of hidden-unit
bert,‚Äù in IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , 2022, pp. 7087‚Äì7091.
[20] Y . Lee, K. Jang, J. Goo, Y . Jung, and H. Kim, ‚ÄúFithubert: Go-
ing thinner and deeper for knowledge distillation of speech self-
supervised learning,‚Äù in Proc. Interspeech , 2022.
[21] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y . Zhang,
T. Ko, and H. Li, ‚ÄúLighthubert: Lightweight and configurable
speech representation learning with once-for-all hidden-unit bert,‚Äù
inProc. Interspeech , 2022.
[22] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, ‚ÄúDeep ver-
sus wide: An analysis of student architectures for task-agnostic
knowledge distillation of self-supervised speech models,‚Äù in Proc.
Interspeech , 2022.
[23] T. Xiao, Y . Li, J. Zhu, Z. Yu, and T. Liu, ‚ÄúSharing attention
weights for fast transformer,‚Äù in Proceedings of the Twenty-Eighth
International Joint Conference on Artificial Intelligence (IJCAI) ,
2019, pp. 5292‚Äì5298.
[24] S. Bhojanapalli, A. Chakrabarti, A. Veit, M. Lukasik, H. Jain,
F. Liu, Y .-W. Chang, and S. Kumar, ‚ÄúLeveraging redun-
dancy in attention with reuse transformers,‚Äù arXiv preprint
arXiv:2110.06821 , 2021.
[25] S. wen Yang, P.-H. Chi, Y .-S. Chuang, C.-I. J. Lai, K. Lakhotia,
Y . Y . Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang,
W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W.
Li, S. Watanabe, A. Mohamed, and H. yi Lee, ‚ÄúSUPERB: Speech
Processing Universal PERformance Benchmark,‚Äù in Proc. Inter-
speech 2021 , 2021, pp. 1194‚Äì1198.
[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù
Advances in neural information processing systems , vol. 30, 2017.
[27] A. v. d. Oord, Y . Li, and O. Vinyals, ‚ÄúRepresentation learning with
contrastive predictive coding,‚Äù arXiv preprint arXiv:1807.03748 ,
2018.
[28] S. Schneider, A. Baevski, R. Collobert, and M. Auli, ‚Äúwav2vec:
Unsupervised pre-training for speech recognition,‚Äù in Proc. Inter-
speech , 2019.
[29] K. Shim, J. Choi, and W. Sung, ‚ÄúUnderstanding the role of self
attention for efficient speech recognition,‚Äù in International Con-
ference on Learning Representations , 2022.
[30] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,
S. Gelly et al. , ‚ÄúAn image is worth 16x16 words: Transformers
for image recognition at scale,‚Äù in International Conference on
Learning Representations , 2021.
[31] K. He, X. Chen, S. Xie, Y . Li, P. Doll ¬¥ar, and R. Girshick, ‚ÄúMasked
autoencoders are scalable vision learners,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, 2022, pp. 16 000‚Äì16 009.
