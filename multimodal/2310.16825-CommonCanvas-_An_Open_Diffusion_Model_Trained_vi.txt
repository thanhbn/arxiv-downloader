# 2310.16825.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.16825.pdf
# Kích thước tệp: 27109777 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CommonCanvas: Một Mô hình Khuếch tán Mở được Huấn luyện
với Hình ảnh Creative-Commons
Aaron Gokaslan1A. Feder Cooper1Jasmine Collins2Landan Seguin2
Austin Jacobson2Mihir Patel2Jonathan Frankle2Cory Stephenson2Volodymyr Kuleshov1
1Cornell Tech
{akg87,afc78,vk379 }@cornell.edu
2Databricks Mosaic
{firstname.lastname }@databricks.com

Tóm tắt
Chúng tôi thu thập một bộ dữ liệu hình ảnh được cấp phép Creative-Commons (CC), mà chúng tôi sử dụng để huấn luyện một tập hợp các mô hình khuếch tán mở có chất lượng tương đương với Stable Diffusion 2 (SD2). Nhiệm vụ này đặt ra hai thách thức: (1) hình ảnh CC có độ phân giải cao thiếu các chú thích cần thiết để huấn luyện các mô hình sinh văn bản thành hình ảnh; (2) hình ảnh CC tương đối khan hiếm. Để giải quyết những thách thức này, chúng tôi sử dụng một kỹ thuật học chuyển giao trực quan để tạo ra một tập hợp các chú thích tổng hợp chất lượng cao được ghép nối với hình ảnh CC được tuyển chọn. Sau đó chúng tôi phát triển một công thức huấn luyện hiệu quả về dữ liệu và tính toán chỉ yêu cầu ít nhất 3% dữ liệu LAION (tức là khoảng 70 triệu ví dụ) cần thiết để huấn luyện các mô hình SD2 hiện có, nhưng đạt được chất lượng tương tự. Những kết quả này cho thấy chúng tôi có đủ số lượng hình ảnh CC (cũng khoảng 70 triệu) để huấn luyện các mô hình chất lượng cao. Công thức huấn luyện của chúng tôi cũng triển khai nhiều tối ưu hóa khác nhau đạt được tăng tốc huấn luyện ~3X, và cho phép lặp mô hình nhanh chóng. Chúng tôi tận dụng công thức này để huấn luyện một số mô hình văn bản thành hình ảnh chất lượng cao, mà chúng tôi gọi là họ CommonCanvas. Mô hình lớn nhất của chúng tôi đạt được hiệu suất tương đương với SD2 trong đánh giá của con người, mặc dù chúng tôi chỉ sử dụng một bộ dữ liệu CC có kích thước <3% so với LAION và các chú thích tổng hợp để huấn luyện. Chúng tôi phát hành các mô hình, dữ liệu và mã của mình tại https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md.

1 Giới thiệu
Các phương pháp hiện tại huấn luyện các mô hình văn bản thành hình ảnh (T2I) chất lượng cao với. Việc thiếu các bộ dữ liệu được tuyển chọn đủ lớn cho nhiệm vụ này đã khiến các nhà nghiên cứu chuyển sang các giải pháp thu thập từ web [29, 30], như LAION-2B [26]. Việc sử dụng dữ liệu thu thập từ web là một thực tiễn rất phổ biến để huấn luyện các mô hình sinh tạo, tuy nhiên, các tòa án Hoa Kỳ vẫn chưa có phán quyết cuối cùng về việc liệu điều này có được phép theo luật bản quyền hay không [1, 13, 15, 20, 21, 60]. Để đáp ứng, các nghiên cứu gần đây đã bắt đầu điều tra các phương pháp thay thế để điều hướng các mối quan tâm về bản quyền trong sinh văn bản [39], hoàn thành mã [16, 51], và sinh hình ảnh [24]. Tuy nhiên, việc đạt được hiệu suất của các mô hình tiên tiến vẫn là một thách thức. Trong công trình này, chúng tôi nghiên cứu câu hỏi tự nhiên sau: Có thể hiệu quả tạo ra một mô hình T2I chất lượng cao bằng cách chỉ huấn luyện trên dữ liệu được cấp phép Creative-Commons không?

Chúng tôi đề xuất một con đường có thể thực hiện được, huấn luyện một bộ kiến trúc T2I chỉ sử dụng hình ảnh Creative-Commons (CC) được cấp phép mở (Hình 1 & 2). Nhiệm vụ này làm nổi bật hai thách thức đáng kể. Vấn đề đầu tiên là tính không đầy đủ của dữ liệu: hầu như tất cả hình ảnh CC thiếu các chú thích cần thiết để huấn luyện một mô hình T2I chất lượng cao. Thứ hai là sự khan hiếm dữ liệu: có tương đối ít hình ảnh CC có độ phân giải cao — khoảng 70 triệu, so với khoảng 2 tỷ của LAION-2B [26].

--- TRANG 2 ---
Lời nhắc SD2-base CommonCanvas-S-C CommonCanvas-S-NC CommonCanvas-L-NC
một chú mèo đen dễ thương
bên trong một quả bí ngô
một robot cầm
bảng màu vẽ
một bức tranh sơn dầu về
một con thuyền buồm cao
đi qua cánh đồng lúa mì
lúc hoàng hôn

Hình 1: Lựa chọn các lời nhắc văn bản. Sử dụng hoàn toàn hình ảnh Creative-Commons và phương pháp tạo chú thích tổng hợp của chúng tôi, chúng tôi đạt được hiệu suất định tính tương đương với Stable Diffusion 2 (SD2-base), như được thấy trong các thế hệ CommonCanvas, trong khi chỉ yêu cầu một phần nhỏ (<3%) lượng dữ liệu huấn luyện. Chúng tôi bao gồm kết quả cho hai kiến trúc CommonCanvas, nhỏ (S) và lớn (L) (Phần 6), và hai bộ dữ liệu hình ảnh CC, thương mại (C) và phi thương mại (NC) (Phần 4). Chúng tôi gán nhãn kết quả của mình tương ứng là CommonCanvas-<kiến trúc>-<bộ dữ liệu>.

Chúng tôi giải quyết vấn đề không đầy đủ dữ liệu bằng cách sử dụng một mô hình BLIP-2 được huấn luyện trước [34], mà chúng tôi sử dụng để tạo ra các chú thích tổng hợp chất lượng cao cho một tập hợp hình ảnh CC được cấp phép mở được tuyển chọn. Đây là một giải pháp học chuyển giao trực quan: tận dụng các mô hình sinh tạo được huấn luyện trước mạnh mẽ để tạo ra các nhãn tổng hợp cho một bộ dữ liệu không có nhãn, mà chúng tôi sau đó có thể sử dụng để huấn luyện một mô hình sinh tạo đa phương thức khác. Chúng tôi lưu ý rằng đây là một mô hình ngày càng phổ biến trong văn học, mà chúng tôi gọi tắt bằng tên telephoning.

Để đối phó với sự khan hiếm dữ liệu, chúng tôi đề xuất một công thức huấn luyện hiệu quả về dữ liệu và tính toán đạt được chất lượng tương tự như SD2, nhưng (có lẽ đáng ngạc nhiên) chỉ yêu cầu ít nhất 3% dữ liệu LAION-2B (tức là khoảng 70 triệu ví dụ) ban đầu được sử dụng để huấn luyện SD2. Chúng tôi gọi mô hình này là SD2-base. Những kết quả này cho thấy chúng tôi có đủ số lượng hình ảnh CC (cũng khoảng 70 triệu) để huấn luyện các mô hình chất lượng cao. Công thức huấn luyện của chúng tôi cũng triển khai nhiều tối ưu hóa khác nhau đạt được tăng tốc huấn luyện ~3X, và cho phép lặp mô hình nhanh chóng.

Các phương pháp trên cho phép chúng tôi tạo ra CommonCanvas, một bộ kiến trúc mô hình khuếch tán tiềm ẩn (LDM) được huấn luyện trên bộ dữ liệu được tuyển chọn của chúng tôi về hình ảnh CC và chú thích tổng hợp, mà chúng tôi ký hiệu là CommonCatalog. Đối với CommonCanvasL-NC, chúng tôi thay thế UNet của SD2 bằng SDXL để chứng minh cách thậm chí với ít dữ liệu hơn, các mô hình lớn hơn không bị quá khớp với bộ dữ liệu nhỏ hơn này. Mô hình lớn nhất của chúng tôi đạt được hiệu suất tương đương với SD2-base trong đánh giá của con người về Parti Prompts [66], mặc dù bộ dữ liệu huấn luyện CommonCatalog của chúng tôi có kích thước <3% so với LAION và có các chú thích được tạo tổng hợp. Hình 1 hiển thị các mẫu được chọn từ các mô hình CommonCanvas của chúng tôi so với các mẫu tương ứng từ SD2-base. Mặc dù mô hình này là một kiến trúc mô hình lớn hơn và - có khả năng - có khả năng hơn so với SD2, chúng tôi thấy điều đáng ngạc nhiên và quan trọng là có thể huấn luyện một mô hình chất lượng SD2 dựa trên một bộ dữ liệu hạn chế như vậy được ghép nối theo cách này. Điều này tiết lộ một con đường hứa hẹn cho nghiên cứu tương lai về các mô hình T2I mở có khả năng cao. Tóm lại, chúng tôi:

• Tổng hợp một tập hợp các chú thích chất lượng cao cho hình ảnh CC không có chú thích, mà chúng tôi sau đó có thể sử dụng cùng nhau để huấn luyện. Chúng tôi lưu ý rằng loại kỹ thuật học chuyển giao này ngày càng phổ biến, và chúng tôi đặt tên tắt cho nó là telephoning (Phần 3).

• Tuyển chọn CommonCatalog, một bộ dữ liệu khoảng 70 triệu hình ảnh CC được cấp phép mở, mà chúng tôi sử dụng telephoning để tạo ra các chú thích tổng hợp chất lượng cao đi kèm (Phần 4).

--- TRANG 3 ---
một hình ảnh của
elsa từ
frozen
(a) Lời nhắc
 (b) Đầu ra SD2
 (c) Đầu ra CommonCanvas
vua sư tử
(d) Lời nhắc
 (e) Đầu ra SD2
 (f) Đầu ra CommonCanvas

Hình 2: Khi được đưa ra các lời nhắc cho các khái niệm liên quan đến phim Disney (a,d), SD2-base tạo ra một hình ảnh có thể nhận ra của Elsa từ Frozen (b) và một hình ảnh giống poster với logo Disney bị biến dạng và các nhân vật giống với những nhân vật từ The Lion King (e), và CommonCanvas (-SC) thì không (c,f).

• Huấn luyện và đánh giá CommonCanvas, một bộ kiến trúc LDM được huấn luyện trên CommonCatalog. Chúng tôi chứng minh rằng những mô hình này tạo ra kết quả định tính và định lượng cạnh tranh so với đường cơ sở SD2-base (Phần 6). Để làm cho phân tích này dễ xử lý, chúng tôi triển khai nhiều tối ưu hóa huấn luyện khác nhau, đạt được tăng tốc ~3X trong việc huấn luyện SD2-base (Phần 5).

• Phát hành bộ dữ liệu CommonCatalog của chúng tôi về hình ảnh CC và chú thích tổng hợp cùng với mô hình CommonCanvas được huấn luyện của chúng tôi tại https://github.com/mosaicml/diffusion/blob/main/assets/common-canvas.md.

2 Sơ bộ và Động lực

Trong phần này, chúng tôi trình bày nền tảng về việc huấn luyện mô hình T2I Stable Diffusion, ban đầu được huấn luyện trên bộ dữ liệu LAION-2B được thu thập từ web. Sau đó chúng tôi thảo luận về bản quyền và khả năng tái tạo liên quan đến bộ dữ liệu LAION. Cuộc thảo luận này thúc đẩy việc tạo ra một bộ dữ liệu thay thế bao gồm hình ảnh CC được cấp phép mở với các chú thích tổng hợp, mà chúng tôi giới thiệu trong Phần 4.

2.1 Các mô hình sinh tạo văn bản thành hình ảnh

Các mô hình sinh tạo văn bản thành hình ảnh (T2I) đề cập đến các mạng thần kinh lớn được huấn luyện trên các ví dụ dữ liệu cặp hình ảnh-chú thích. Một họ mô hình T2I như vậy là Stable Diffusion (SD) [47]. SD là một mô hình khuếch tán tiềm ẩn (LDM) chuyển đổi hình ảnh thành biểu diễn tiềm ẩn và ngược lại bằng cách sử dụng Variational Autoencoders (VAEs) [23]; nó sử dụng một quy trình lấy mẫu lặp [57] và huấn luyện một UNet cơ bản [48]. Kiến trúc cũng bao gồm một bộ mã hóa văn bản, chẳng hạn như mô hình Contrastive Language-Image Pre-training (CLIP) [43] – hoặc CLIP ban đầu từ OpenAI [45] hoặc đối tác mã nguồn mở của nó, OpenCLIP [10, 18].

UNet của Stable Diffusion 2 (SD2) có khoảng 865 triệu tham số có thể huấn luyện; Stable Diffusion XL (SDXL) lớn hơn, với 2,6 tỷ tham số, và có các tiến bộ khác liên quan đến aspect ratio bucketing, micro-conditioning, và nhiều bộ mã hóa văn bản và tokenizer. Về mặt dữ liệu huấn luyện, họ mô hình SD và OpenCLIP đều được huấn luyện trên các tập con của bộ dữ liệu LAION-5B [3, 53]. Bộ dữ liệu huấn luyện chính xác cho CLIP không được biết, nhưng có khả năng là dữ liệu được thu thập từ web [45].

2.2 Bản quyền và khả năng tái tạo liên quan đến bộ dữ liệu LAION

LAION-5B là một bộ dữ liệu được tạo ra từ một snapshot của Common Crawl, một kho dữ liệu khổng lồ được thu thập từ web. Từ snapshot này, tổ chức LAION đã tuyển chọn các cặp URL hình ảnh và chú thích alt-text tương ứng của chúng để sử dụng dự định là huấn luyện các mô hình sinh tạo T2I và hình ảnh thành văn bản (I2T) [3, 53]. Trong thực tế, các mô hình T2I thường được huấn luyện trên các tập con được lọc của bộ dữ liệu LAION-5B đầy đủ (ví dụ LAION-2B [26]). Việc huấn luyện các mô hình T2I trên bộ dữ liệu này yêu cầu truy cập các URL và tải xuống các hình ảnh liên quan. Có hai yếu tố của bộ dữ liệu LAION có liên quan đến công việc của chúng tôi:

Bản quyền. Các hình ảnh liên quan đến bộ dữ liệu LAION có nguồn gốc không rõ ràng: thường không biết nguồn hình ảnh ban đầu là gì [29, 30]. Các tòa án vẫn chưa quyết định xem việc huấn luyện trên những bộ dữ liệu này có phải là "sử dụng hợp lý" hay không — một ngoại lệ quan trọng trong bản quyền [29, 33, 50, 56]. Trong thời gian tạm thời, có một số vụ kiện bản quyền về việc cáo buộc sử dụng các tập con LAION-5B để huấn luyện các mô hình sinh tạo [1, 15, 20, 61].

--- TRANG 4 ---
BLIP-2LAION-400MBLIP-2Hình ảnh CCCommonCatalogN=~70Mi=1Chú thíchN=~70Mi=1(a) BLIP-2 được huấn luyện trước.
BLIP-2LAION-400MBLIP-2Hình ảnh CCCommonCatalogN=~70Mi=1Chú thíchN=~70Mi=1CommonCanvas
(b) Tạo CommonCatalog để huấn luyện CommonCanvas.
một chú chó hoạt hình đen trắng với tai đen
BLIP-2CommonCanvas
(c) "Nén mất mát" qua BLIP-2 từ một hình ảnh đầu vào thành một chú thích tổng hợp. Khi chúng tôi sử dụng một mô hình T2I để tạo ra một hình ảnh với chú thích "mất mát" này (ví dụ, qua CommonCanvas), kết quả tạo ra không giống gì hình ảnh lời nhắc ban đầu đã tạo ra chú thích.

Hình 3: (a) Bộ dữ liệu khổng lồ của LAION về các cặp hình ảnh-chú thích được sử dụng để huấn luyện BLIP-2, một mô hình hình ảnh thành văn bản. (b) Chúng tôi tận dụng BLIP-2 để tạo ra các chú thích tổng hợp cho hình ảnh CC không có chú thích của chúng tôi, và sử dụng các cặp hình ảnh-chú thích tổng hợp kết quả (bộ dữ liệu CommonCatalog) để huấn luyện mô hình khuếch tán mở của chúng tôi, CommonCanvas. (c) Mặc dù BLIP-2 được huấn luyện trên LAION (ví dụ, bao gồm hình ảnh nhân vật Snoopy), các chú thích nó tạo ra hoạt động như một "nén mất mát" (ví dụ, một chú chó hoạt hình đen trắng với tai đen, không đề cập đến Snoopy). Khi chúng tôi cung cấp các chú thích "mất mát" như vậy cho một mô hình T2I, như một trò chơi điện thoại, nó tạo ra đầu ra không còn giống với hình ảnh ban đầu (ví dụ, chúng tôi cho thấy cách CommonCanvas tạo ra một hình ảnh phù hợp với chú thích, nhưng không giống Snoopy).

Khả năng tái tạo. Vì các bộ dữ liệu chỉ chứa các URL hình ảnh, chứ không phải bản thân các hình ảnh, chúng bị ảnh hưởng bởi link rot [27]. Khi truy cập LAION-5B, không có gì đảm bảo các hình ảnh vẫn tồn tại tại các URL của chúng, khiến việc tái tạo hoàn toàn bộ dữ liệu trở nên không thể và mở ra khả năng các cuộc tấn công đầu độc dữ liệu [8].

Một lựa chọn thay thế tự nhiên là không sử dụng bộ dữ liệu LAION để huấn luyện. Thay vào đó, người ta có thể độc lập tuyển chọn một bộ dữ liệu hình ảnh được cấp phép CC với nguồn gốc đã biết mà rõ ràng cho phép sao chép, thích ứng và sử dụng thương mại. Vì các hình ảnh thành phần có thể được lưu trữ và phân phối, điều này cũng sẽ giải quyết vấn đề link rot, từ đó cho phép khả năng tái tạo tốt hơn. Chúng tôi hoãn thảo luận về việc tìm nguồn hình ảnh được cấp phép CC đến Phần 4, nơi chúng tôi chi tiết CommonCatalog: bộ dữ liệu mở mới của chúng tôi.

Trong khi hình ảnh CC là một lựa chọn thay thế hấp dẫn cho LAION-5B, chúng tôi lưu ý rằng hình ảnh CC hiếm khi chứa các chú thích cần thiết để huấn luyện các mô hình T2I. Do đó, trước tiên chúng tôi cần một phương pháp để tạo chú thích cho hình ảnh CC, mà chúng tôi mô tả trong phần tiếp theo.

3 Telephoning: Một Phương pháp Tạo Chú thích Hình ảnh Dựa trên Học Chuyển giao

Giải pháp của chúng tôi để xử lý việc thiếu chú thích trong hình ảnh CC là một loại học chuyển giao trực quan để tạo ra các nhãn tổng hợp chất lượng cao. Chúng tôi mô tả phương pháp này, và sau đó lưu ý rằng có nhiều phương pháp tương tự trong văn hóa mô hình sinh tạo trước đây. Tổng thể, những phương pháp này cho thấy rằng loại học chuyển giao này để tạo ra các nhãn tổng hợp (để sau này phục vụ như đầu vào cho việc huấn luyện các mô hình sinh tạo khác) đã trở thành một mô hình ngày càng phổ biến. Do đó chúng tôi đặt tên cho phương pháp này: telephoning.

3.1 Mô tả telephoning

Telephoning (Hình 3) nhận đầu vào từ một phương thức chiều cao (ví dụ, hình ảnh), thực hiện hiệu quả một "nén mất mát" thành một phương thức chiều thấp (ví dụ, chú thích văn bản ngắn), và sau đó

--- TRANG 5 ---
giải nén ngược lại thành phương thức chiều cao. Bởi vì bước nén trung gian là "mất mát", đầu ra cuối cùng thường không giống chút nào với đầu vào ban đầu, giống như một trò chơi điện thoại [38]. Chúng tôi rút ra thuật ngữ telephoning từ trực giác trên, và sử dụng nó như một cách viết tắt hữu ích để biểu thị các trường hợp học chuyển giao giải quyết các vấn đề thiếu dữ liệu trong mô hình sinh tạo đa phương thức.

Trong công việc này, hình ảnh CC là các đầu vào chiều cao, và chúng tôi sử dụng một mô hình BLIP-2 được huấn luyện trước [34] cho "nén mất mát" thành các chú thích văn bản ngắn (Hình 3a). Cùng nhau, những cặp hình ảnh-chú thích CC này bao gồm bộ dữ liệu CommonCatalog, mà chúng tôi sử dụng để huấn luyện các mô hình T2I CommonCanvas của chúng tôi (Hình 3b). Mặc dù BLIP-2 được huấn luyện trước trên LAION-400M [52], CommonCatalog và CommonCanvas không bao giờ có quyền truy cập trực tiếp vào LAION-400M hoặc, quan trọng là, bất cứ thứ gì tương tự với hình ảnh mà BLIP-2 được huấn luyện. Thay vào đó, chúng tôi chỉ có quyền truy cập vào ánh xạ trong mô hình, mà khi được cho một đầu vào hình ảnh, tạo ra văn bản đầu ra mất mát mà vốn dĩ không giống theo nghĩa đen với đối tác hình ảnh của nó (Hình 3c).

Chúng tôi hoãn đến các chuyên gia về sử dụng hợp lý (Phần 2.2) — cụ thể là, liên quan đến các mô hình như BLIP-2, và hình ảnh và chú thích alt-text của LAION-5B. Nói chung, những chuyên gia này dường như nghĩ rằng nhiều trường hợp sẽ rơi vào sử dụng hợp lý [29, 32, 50], đặc biệt là khi đầu ra mô hình không giống với đầu vào của chúng, đó là trường hợp với BLIP-2.

3.2 Công việc liên quan về telephoning

Công việc của chúng tôi phù hợp với xu hướng sử dụng các mô hình sinh tạo tiên tiến để giải quyết sự khan hiếm dữ liệu. Điều này rõ ràng trong nhiều phương thức khác nhau, chẳng hạn như tạo ra chú thích âm thanh từ các cặp hình ảnh-văn bản [64] và văn bản từ âm thanh [46]. Các phương pháp tương tự cũng đã được sử dụng để tạo ra bộ dữ liệu điều chỉnh hướng dẫn cho cả văn bản và hình ảnh [35, 37]. Công việc đồng thời đã sử dụng các mô hình câu hỏi-trả lời thị giác như LLava [37] để tăng cường các chú thích hiện có như DALLE·3 [4] và Chen et al. [9]. Tuy nhiên, mô hình của chúng tôi là một trong những công việc đầu tiên huấn luyện trên một bộ dữ liệu không có chú thích thật nào, và một trong những công việc đầu tiên phát hành bộ dữ liệu chú thích tổng hợp của chúng tôi cùng với một mô hình khuếch tán được huấn luyện đầy đủ. Hơn nữa, các phương pháp tăng cường chú thích được mô tả trong những công việc này có thể được sử dụng để cải thiện thêm các chú thích của CommonCatalogue trong công việc tương lai. Các mô hình tạo chú thích đã được sử dụng trước đây để tạo ra các chú thích mô tả trước để hướng dẫn một mô hình khuếch tán tạo ra một hình ảnh tương tự về mặt hình ảnh với một hình ảnh cụ thể. Công việc đồng thời SynthCap [6] tạo ra một bộ dữ liệu chú thích tổng hợp sử dụng một mô hình khuếch tán để tạo ra hình ảnh từ chú thích, giải quyết nghịch đảo của tuyên bố vấn đề của chúng tôi.

Chúng tôi đặt tên thuật ngữ telephoning để viết tắt các quy trình như vậy, bao gồm công việc của chúng tôi và công việc trước đây, và mà chúng tôi tin sẽ trở nên phổ biến hơn khi các mô hình sinh tạo tiến bộ.

4 CommonCatalog: Một Bộ dữ liệu Hình ảnh CC & Chú thích Tổng hợp

Trong phần này, chúng tôi giới thiệu bộ dữ liệu mở của chúng tôi, CommonCatalog. Đầu tiên, chúng tôi mô tả quy trình thu thập và tuyển chọn cho các hình ảnh CC được cấp phép mở. Quy trình này làm nổi bật hai thách thức: tính không đầy đủ của dữ liệu chú thích và sự khan hiếm dữ liệu hình ảnh. Để giải quyết việc thiếu chú thích CC, chúng tôi hiển thị cụ thể cách chúng tôi sử dụng telephoning để tạo ra các chú thích tổng hợp chất lượng cao để đi kèm với tập hợp hình ảnh được tuyển chọn của chúng tôi. Chúng tôi điều tra chủ đề về sự khan hiếm dữ liệu trong phần tiếp theo, nơi chúng tôi cũng thảo luận về các tối ưu hóa cấp hệ thống cần thiết cho phép chúng tôi lặp mô hình SD hiệu quả.

4.1 Tìm nguồn hình ảnh được cấp phép, có nguồn gốc cho CommonCatalog

Chúng tôi tập trung vào việc xác định vị trí các hình ảnh Creative-Commons có độ phân giải cao có giấy phép mở. Chúng tôi bắt đầu với bộ dữ liệu YFCC100M, bao gồm 100 triệu hình ảnh và tệp đa phương tiện được cấp phép CC, cũng như các ID Flickr liên kết đến dữ liệu gốc [59]. Các hình ảnh trong bộ dữ liệu liên quan đến bài báo gốc thể hiện hai vấn đề khiến nó không phù hợp để sử dụng trực tiếp để huấn luyện Stable Diffusion: chúng có độ phân giải thấp, và nhiều trong số chúng có giấy phép không rõ ràng cho phép phân phối các tác phẩm phái sinh, đó là một lĩnh vực luật bản quyền chưa được giải quyết trong bối cảnh huấn luyện mô hình. Do đó chúng tôi thu thập lại những hình ảnh này từ Flickr, dựa trên các ID được cung cấp trong metadata YFCC100M. Các hình ảnh được thu thập của chúng tôi có độ phân giải rất cao (vượt quá 4K), điều này làm cho chúng phù hợp hơn để huấn luyện T2I.

--- TRANG 6 ---
Chú thích Nguồn
Alt-Text (LAION-2B) Latest 1PC Transparent Gradient Color Voile Window Curtain
BLIP2-OPT-2.7B Một phòng khách với ghế sofa trắng và rèm cửa

Hình 5: Chú thích gốc so với chú thích được tạo bởi BLIP-2 cho một hình ảnh từ LAION-2B. BLIP-2 tạo ra một chú thích phù hợp hơn với những gì một con người sẽ viết. Xem Hình 14 để biết thêm ví dụ.

Hình 4: CommonCatalog-C chứa hình ảnh chỉ được cấp phép để sử dụng thương mại; -NC chứa -C cũng như hình ảnh được cấp phép để sử dụng phi thương mại.

Bộ dữ liệu # Hình ảnh % Alt Text
CommonCatalog-C 26,232,417 30.76%
CommonCatalog-NC 67,015,331 31.22%

Chúng tôi loại trừ hình ảnh với giấy phép không phái sinh (ND). Các hình ảnh còn lại có thể được chia thêm thành những hình ảnh có thể được sử dụng cho mục đích thương mại (C) và những hình ảnh không thể (phi thương mại/NC). Như được hiển thị trong Bảng 4, chúng tôi tương ứng xây dựng hai bộ dữ liệu, CommonCatalog-C và CommonCatalog-NC. Chúng tôi hoãn các chi tiết bổ sung về giấy phép đến Phụ lục B.1.1, nhưng nhấn mạnh rằng tất cả các hình ảnh được bao gồm đều có giấy phép mở: các cá nhân tự do sử dụng, thích ứng và remix hình ảnh, miễn là họ ghi công. Tổng cộng, CommonCatalog chứa khoảng 70 triệu hình ảnh CC NC, trong đó một tập con khoảng 25 triệu hình ảnh cũng có thể được sử dụng thương mại.

Việc tìm nguồn trực tiếp CommonCatalog tránh được một số mối quan tâm (Phần 2.2); tuy nhiên, nó cũng đi kèm với những thách thức riêng của nó. Một là, hình ảnh CC hiếm khi có các chú thích alt-text cần thiết để huấn luyện một mô hình T2I như Stable Diffusion (Hình 4); những hình ảnh có văn bản liên quan thường chỉ bao gồm tiêu đề hình ảnh hoặc một URL. Một khác, chúng tôi chỉ có thể tìm thấy khoảng 70 triệu hình ảnh CC có thể sử dụng được, điều này tương đối ít so với hàng tỷ hình ảnh trong LAION được sử dụng để huấn luyện SD2 (Phần 5). Chúng tôi giải quyết từng thách thức này một cách lần lượt. Đầu tiên, trong phần con tiếp theo, chúng tôi cho thấy cách chúng tôi thể hiện telephoning (Phần 3) để tạo ra các chú thích tổng hợp chất lượng cao cho hình ảnh CC.

4.2 Tổng hợp chú thích với telephoning

Chúng tôi so sánh một số mô hình tạo chú thích và, dựa trên phân tích định tính và hiệu suất tiên tiến của nó trên MS COCO, đã chọn sử dụng mô hình BLIP-2 OPT2.5B được huấn luyện trước để tổng hợp các chú thích của CommonCatalog [34]. BLIP-2 bao gồm ba thành phần: một bộ mã hóa thị giác được huấn luyện trước, đóng băng (tức là, cố định), một mạng transformer đã học chuyển đổi các embedding thị giác thành một lời nhắc văn bản, và một mô hình ngôn ngữ lớn (LLM) đóng băng nhận lời nhắc. Các biến số có thể huấn luyện duy nhất trong transformer là giữa bộ mã hóa thị giác đóng băng và các lớp LLM đóng băng.

Khi được cho một hình ảnh LAION-2B làm đầu vào, chúng tôi thấy rằng chú thích BLIP-2 kết quả thường có chất lượng mô tả tốt hơn so với chú thích alt-text thật của LAION-2B tương ứng. Các chú thích LAION-2B thường chứa tên sản phẩm, chi tiết không liên quan, hoặc ngữ pháp và cú pháp kém (Hình 5). Phát hiện này được củng cố bởi Nguyen et al. [42], cho thấy một cách định lượng (về CLIP Score) rằng các chú thích BLIP-2 có chất lượng cao hơn các chú thích thật, với chi phí là sự đa dạng chú thích.

Dựa trên những kết quả sơ bộ này, chúng tôi đã tạo chú thích cho tất cả hình ảnh Creative-Commons YFCC100M, điều này yêu cầu khoảng 1.120 giờ GPU A100. Để làm như vậy, chúng tôi cắt trung tâm và thay đổi kích thước tất cả hình ảnh đến kích thước tối đa 512x512 pixel. Chúng tôi thực hiện những biến đổi này vì việc tạo chú thích cho hình ảnh ở độ phân giải gốc sẽ rất tốn kém. Tại thời điểm huấn luyện mô hình khuếch tán, tất cả hình ảnh vẫn ở độ phân giải gốc của chúng. Chúng tôi phát hành các bộ dữ liệu hình ảnh CC và chú thích tổng hợp thương mại (CommonCatalog-C) và phi thương mại (CommonCatalog-NC) của chúng tôi trên HuggingFace tại [REDACTED] với các thẻ dữ liệu liên quan. Như một tập đánh giá, chúng tôi cũng phát hành các chú thích BLIP-2 mà chúng tôi đã tạo ra cho các hình ảnh CC không phái sinh (ND) mà chúng tôi không sử dụng để huấn luyện.

5 Tối ưu hóa Hiệu quả Huấn luyện và Phân tích Thiếu Dữ liệu

Hình ảnh CC có độ phân giải cao thực sự ít phong phú hơn nhiều so với những hình ảnh được thu thập từ web tùy ý, nhưng lượng dữ liệu cần thiết để huấn luyện các mô hình SD2 chất lượng cao chưa được nghiên cứu kỹ lưỡng. Chúng tôi đặt mục tiêu định lượng số lượng này bằng cách huấn luyện nhiều mô hình SD2 trên các tập con có kích thước khác nhau của LAION-2B. Tuy nhiên, việc huấn luyện một mô hình SD2 duy nhất, ngay cả với hàng trăm GPU, có thể mất vài ngày. Để làm cho phân tích thiếu dữ liệu của chúng tôi dễ xử lý hơn, trước tiên chúng tôi triển khai một số tối ưu hóa hiệu quả.

--- TRANG 7 ---
5.1 Tăng tốc phần mềm và phần cứng

Stability AI báo cáo ước tính 200.000 giờ A100 để huấn luyện SD2 [58]. Tùy thuộc vào phần cứng có sẵn, một lần chạy SD2 duy nhất có thể mất từ vài tuần đến hơn một tháng để huấn luyện. Chúng tôi tìm kiếm nhiều cách để giảm ràng buộc thời gian huấn luyện này. Cuối cùng chúng tôi đã có thể đạt được tăng tốc 2.71X so với triển khai SD2 ban đầu.

Đầu tiên, chúng tôi áp dụng Flash Attention [11] với thư viện xFormers [31]. Chúng tôi cũng tính toán trước các latent VAE và bộ mã hóa văn bản trên toàn bộ bộ dữ liệu huấn luyện, chuyển tất cả GroupNorm [63] và LayerNorm [2] thành độ chính xác float16, và áp dụng fully-sharded data parallelism (FSDP) cho lần chạy huấn luyện của chúng tôi. Cuối cùng chúng tôi lựa chọn chỉ giữ một exponential moving average của các trọng số cho 3.5% cuối của huấn luyện. Chi tiết hơn về từng cải tiến này có thể tìm thấy trong Phụ lục D.

Khi áp dụng tất cả các chiến lược nêu trên cùng nhau, chúng tôi có thể đạt được tăng tốc 2.71X trong giờ A100 so với triển khai đường cơ sở SD2 của chúng tôi. Chúng tôi thấy rằng tính toán trước latent giúp ích nhất ở độ phân giải thấp, trong khi FSDP cũng cung cấp lợi ích đáng kể, đặc biệt là ở quy mô. Các tối ưu hóa khác giúp giảm tổng sử dụng bộ nhớ, cho phép chúng tôi tăng kích thước microbatch để sử dụng phần cứng tốt hơn. Hình 6 tóm tắt từng phương pháp được đề xuất và tăng tốc tích lũy kết quả từ việc áp dụng nó. Được trang bị một thiết lập huấn luyện tối ưu, chúng tôi có thể dễ dàng nghiên cứu tác động của việc thay đổi kích thước bộ dữ liệu huấn luyện.

5.2 Điều tra thiếu dữ liệu: Bão hòa đánh giá SD2 với <3% LAION-2B

YFCC100M chứa 100 triệu hình ảnh, khoảng 10% kích thước của 1.1B ví dụ LAION mà chúng tôi có thể truy cập, do đó khoảng 5% của bộ dữ liệu LAION-2B ban đầu. Một câu hỏi thú vị vẫn chưa được trả lời là cần bao nhiêu dữ liệu thực sự để huấn luyện các mô hình khuếch tán này một cách hiệu quả.

Chúng tôi hỏi liệu có cần thiết phải huấn luyện trên 1+ tỷ hình ảnh để có được kết quả tốt như SD2 được huấn luyện trên LAION ban đầu hay không. Kết quả của chúng tôi cho thấy, đáng ngạc nhiên, đây không phải là trường hợp với một mô hình hơi lớn hơn (CommonCanvas-L); mô hình này thay thế U-Net của SD2 bằng U-Net lớn hơn của SDXL [43]. Hơn nữa, mô hình lớn hơn của chúng tôi đạt được kết quả tương đương với SD2-base trong đánh giá của con người, sử dụng ít hơn 33X dữ liệu huấn luyện. Chúng tôi huấn luyện trên các tập con ngẫu nhiên ngày càng nhỏ hơn của dữ liệu từ mô hình LAION-1.1B của chúng tôi và thấy rằng chúng tôi có thể đạt được kết quả tương tự trên các số MS COCO được báo cáo thường xuyên, nhưng với <3% lượng dữ liệu huấn luyện của SD2 (Hình 8). Thực tế, chúng tôi chạy thí nghiệm xuống còn 1 triệu hình ảnh LAION-1.1B, và thấy rằng chỉ cần 10 triệu hình ảnh là đủ để có hành vi huấn luyện ổn định (Phụ lục, Hình 15).

5.3 Điều tra hiệu suất của mô hình được huấn luyện CC

Những phát hiện này cho thấy rằng các mô hình SD2 có thể thiếu tham số. Thực tế, khi chúng tôi sử dụng CommonCanvas-LNC, chúng tôi đạt được hiệu suất cạnh tranh với SD2 về sở thích người dùng, mặc dù huấn luyện trên ít dữ liệu hơn đáng kể (Phần 7). Hơn nữa, bất chấp việc giảm drastically kích thước bộ dữ liệu, chúng tôi quan sát thấy rằng mô hình lớn hơn (CommonCanvas-LNC) vượt trội hơn mô hình nhỏ hơn (CommonCanvas-SNC), phù hợp với quan niệm rằng những mô hình này vẫn thiếu tham số. Chúng tôi đưa ra giả thuyết về lý do tại sao điều này có thể xảy ra và cần bao nhiêu dữ liệu thực sự để bão hòa mô hình trong Phụ lục A.1.

Đường cơ sở
+ Microbatch 4->8
 + Latent được tính toán trước + Low Precision LN/GN+ Microbatch 8->16+ FSDP+ không EMA01000200030004000Throughput (mẫu/s)

Hình 6: Tác động tích lũy của các tăng tốc khác nhau trong pipeline huấn luyện SD2 của chúng tôi trên 128 Throughput được đánh giá trên 128 A100.

CommonCanvas-SC CommonCanvas-SNC CommonCanvas-LNC0.00.10.20.30.40.5Tỷ lệ sở thích443401437

Hình 7: Nghiên cứu sở thích người dùng sử dụng lời nhắc Parti. Mô hình CommonCanvas-LNC đạt hiệu suất của SD2 mặc dù được huấn luyện với <3% lượng dữ liệu.

--- TRANG 8 ---
29 30 31 32
CLIP Score81012141618FID10m chú thích LAION
90m chú thích LAION
10m chú thích BLIP2
29 30 31 32
CLIP-Score0.0020.0040.0060.008KID
29 30 31 32
CLIP-Score8910111213CLIP-FIDHình 8: FID, KID, và CLIP-FID so với CLIP-Score được tính toán trên 30K mẫu từ COCO2014 cho các mô hình SD2 khác nhau được huấn luyện trên các tập con nhỏ hơn của LAION (10M, 90M, sử dụng chú thích gốc hoặc chú thích tổng hợp BLIP2. Thú vị là việc tăng lượng dữ liệu huấn luyện từ 10M lên 90M mẫu không dẫn đến cải thiện các chỉ số định lượng qua các thang hướng dẫn 1 đến 8. FID thấp hơn là tốt hơn; điểm CLIP cao hơn là tốt hơn.

tranh vẽ nhiếp ảnh COCO 2014 người mặt0.02.55.07.510.012.515.017.5CLIP-FID1.13B LAION
10M LAION
90M LAION
90M BLIP2
30M CommonCanvas-SC
70M CommonCanvas-SNC

Hình 9: CLIP-FID cho các mô hình khác nhau. Chúng ta có thể thấy sự thay đổi miền giữa chú thích MS COCO và chú thích khái niệm được thu thập từ web. CLIP-FID có khả năng ưu tiên SD2, vì CLIP được huấn luyện trên kiểu văn bản tương tự như LAION. Biểu đồ này chỉ bao gồm giai đoạn đầu tiên của huấn luyện ở độ phân giải 256x256. Chúng tôi

6 Thí nghiệm

Được trang bị các bộ dữ liệu thương mại (CommonCatalog-C) và phi thương mại (CommonCatalog-NC), chúng tôi huấn luyện hai mô hình CommonCanvas khác nhau. Chúng tôi cũng huấn luyện một biến thể lớn hơn của CommonCanvas-NC (CommonCanvas-LNC) mà, như chúng tôi lưu ý ở trên (Phần 5.2), có một U-Net lớn hơn đáng kể. Hình 1 hiển thị kết quả định tính từ mỗi biến thể mô hình này. Chi tiết hơn về kiến trúc CommonCanvas-LNC có thể tìm thấy trong Phụ lục A.2.

6.1 Chỉ số chất lượng tự động để đánh giá mô hình

Chúng tôi đo hiệu suất với ba chỉ số chất lượng hình ảnh tự động trên bộ dữ liệu MS COCO [36]: Frechet Inception Distance (FID) [17], Kernal Inception Distance (KID) [5], và CLIP-FID [25]. Ngoài ra, CLIP Score được đánh giá để hiểu sự liên kết giữa chú thích và hình ảnh tương ứng của chúng. Mô hình của chúng tôi đã chứng minh hiệu suất tương đương so với đường cơ sở của SD2 trên benchmark MS COCO phổ biến.

Tuy nhiên, như bất kỳ mô hình nào, mô hình của chúng tôi có những hạn chế. Nó kém hiệu quả trong một số danh mục, bao gồm mặt, nhiếp ảnh tổng quát, và tranh vẽ. Những danh mục này có nguồn gốc từ bộ dữ liệu Conceptual Captions [55], dựa trên dữ liệu được thu thập từ web. Những chú thích có nguồn gốc từ web này, mặc dù phong phú, có thể không luôn phù hợp với các sắc thái ngôn ngữ do con người tạo ra.

Sự khác biệt này nhấn mạnh tầm quan trọng của việc kết hợp dữ liệu chú thích do con người tạo ra quy mô lớn. Mặc dù việc chuyển sang chú thích tổng hợp đưa ra một số thách thức hiệu suất, sự sụt giảm hiệu suất không drastically như người ta có thể giả định. Hơn nữa, chúng tôi suy đoán rằng điều đó sẽ xảy ra nếu người dùng bổ sung với bộ dữ liệu riêng của họ, như FFHQ [22], nếu họ tìm cách tinh chỉnh mô hình cho các danh mục cụ thể.

6.2 Đánh giá của con người

Trong khi các chỉ số chất lượng tự động hữu ích, cho mức độ chi tiết và độ rộng của phân phối mà các T2I lớn dự định tạo ra, không có gì thay thế được việc đánh giá bởi người đánh giá con người. Xếp hạng sở thích cặp đôi của con người cho ba mô hình CommonCanvas có độ phân giải 512x512 so với SD2-base có thể được thấy trong Hình 7.

Trong thí nghiệm này, người đánh giá con người được hiển thị một lời nhắc (được chọn ngẫu nhiên từ tập lời nhắc PartiPrompts [66]) cùng với hai hình ảnh được tạo ra theo thứ tự ngẫu nhiên, một từ mô hình tham chiếu (SD2-base) và một từ mô hình CommonCanvas. Người dùng được hỏi họ thích hình ảnh được tạo ra nào hơn. Chúng tôi báo cáo phần trăm thời gian người dùng chọn hình ảnh được tạo ra bởi mô hình CommonCanvas so với thế hệ tương ứng từ SD2 làm tỷ lệ sở thích người dùng cho mô hình đó. Phù hợp với các chỉ số chất lượng tự động của chúng tôi, chúng tôi thấy rằng hai mô hình CommonCanvas nhỏ ít được ưa thích hơn SD2-base, với tỷ lệ sở thích 37% cho CommonCanvas-SC và 38% cho CommonCanvas-SNC, mà chúng tôi thấy đáng ngạc nhiên cao xem xét tính chất nhỏ hơn và tổng hợp của bộ dữ liệu. Đối với mô hình lớn nhất, CommonCanvas-LNC, chúng tôi không đo được sự khác biệt có ý nghĩa thống kê trong sở thích người dùng giữa mô hình này và SD2-base. Trong khi SDXL là một mô hình lớn hơn đáng kể, phát hiện này đại diện cho một kết quả hiện hữu, cho thấy rằng chúng tôi có khả năng đạt được hiệu suất của một mô hình được huấn luyện trên nhiều độ lớn dữ liệu hơn.

6.3 Lợi ích và thách thức của chú thích tổng hợp

Thú vị là, chúng tôi quan sát thấy rằng chú thích tổng hợp có thể tăng cường sự liên kết của mô hình chúng tôi. Ví dụ, CLIP Score cho chú thích tổng hợp vượt quá chú thích thật như được thấy trong Hình 8.

Chúng tôi cũng quan sát thấy sự đa dạng giảm của n-gram trong chú thích tổng hợp của chúng tôi, một mô hình đã được lưu ý trước đây bởi Nguyen et al. [42]. Hiệu ứng này có thể được hình dung thông qua sự giảm số trigram duy nhất.

Mặc dù chúng tôi huấn luyện trên hình ảnh Creative-Commons, vẫn có thể để một lời nhắc đối kháng tạo ra nội dung, ví dụ, bao gồm các nhân vật biểu tượng. Trong Hình 10, chúng tôi thử nghiệm mô hình của mình với các lời nhắc mơ hồ gợi ý về những nhân vật như vậy. Các ví dụ bao gồm hình ảnh giống chặt chẽ với Elsa từ Frozen, Indiana Jones giống Harrison Ford, và thậm chí một sự giống nhau với Harry Potter (Hình 10). Định tính, mô hình của chúng tôi lệch khỏi những nhân vật này nhiều hơn SD2.

--- TRANG 9 ---
Của chúng tôi SD2 Của chúng tôi SD2 Của chúng tôi SD2
công chúa băng Snoopy một nhà khảo cổ học phiêu lưu với
một cái roi và một chiếc mũ phớt
Một pháp sư tuổi teen với kính
tròn
một chú chó beagle hoạt hình trong
ngôi nhà chó đỏ
stencil đen trắng cô gái nhỏ
với tay với cho một quả bóng đỏ

Hình 10: Chúng tôi so sánh CommonCanvas-SNC (Của chúng tôi) với SD2. Mô hình của chúng tôi ít có khả năng tạo ra các nhân vật biểu tượng khi được đưa ra các lời nhắc gợi ý (lấy từ Lee et al. [29]).

--- TRANG 10 ---
Của chúng tôi SD2 Của chúng tôi SD2 Của chúng tôi SD2
Bill Gates Elon Musk Kim Kardashian
Barack Obama Hillary Clinton Richard Feynman

Hình 11: Sử dụng CommonCanvas-SNC (Của chúng tôi) để tạo ra người nổi tiếng. Mô hình của chúng tôi kém hơn trong việc tổng hợp các cá nhân so với SD2, nhưng có khả năng tạo ra một số nhân vật công chúng đáng chú ý.

7 Thảo luận và Công việc Liên quan

Trong bài báo này, chúng tôi huấn luyện họ mô hình khuếch tán tiềm ẩn văn bản thành hình ảnh CommonCanvas chỉ trên hình ảnh Creative-Commons và chú thích tổng hợp. Chúng tôi thảo luận về các vấn đề không đầy đủ và thiếu dữ liệu liên quan đến hình ảnh CC, và cách chúng tôi giải quyết từng vấn đề này một cách lần lượt. Đối với việc không đầy đủ dữ liệu, chúng tôi đề xuất telephoning, một loại học chuyển giao trực quan (Phần 3), mà chúng tôi thể hiện với BLIP-2 để tạo ra chú thích tổng hợp cho hình ảnh CC — cùng nhau, bộ dữ liệu CommonCatalog (Phần 4). Liên quan đến thiếu dữ liệu, chúng tôi đưa ra giả thuyết rằng cần ít dữ liệu hơn nhiều so với những gì có trong LAION-2B để bão hòa SD2, và rằng CommonCatalog nên đủ để huấn luyện. Để làm cho việc kiểm tra giả thuyết này hiệu quả hơn, chúng tôi triển khai nhiều tối ưu hóa hệ thống ML khác nhau, đạt được tăng tốc 2.7X so với đường cơ sở SD2 của chúng tôi. Cuối cùng, chúng tôi thấy rằng chúng tôi có thể huấn luyện SD2 trên <3% LAION-2B (Phần 5), điều này khuyến khích chúng tôi huấn luyện trên các ví dụ thương mại (khoảng 70 triệu) và phi thương mại (khoảng 25 triệu) của CommonCatalog. Các mô hình CommonCanvas của chúng tôi kém hiệu quả trong một số danh mục, như mặt, nhưng CommonCanvas-LNC chứng minh hiệu suất tương đương thống kê với SD2 trong đánh giá của con người (Phần 6).

Chúng tôi lưu ý rằng một số công việc gần đây nghiên cứu bản quyền. Công việc này có xu hướng liên quan đến dữ liệu huấn luyện văn bản thành văn bản [39], chủ yếu mang tính lý thuyết [51, 62], liên quan đến các nghiên cứu ablation [24], hoặc chỉ xử lý việc ghi nhớ nguyên văn [7] thông qua việc sử dụng các bộ lọc nội dung thời gian tạo [16], điều này đã được chứng minh là một giải pháp không đầy đủ [19]. Theo hiểu biết tốt nhất của chúng tôi, không có công việc mở trước đây nào cố gắng huấn luyện các mô hình T2I chỉ trên dữ liệu được cấp phép mở.

Hầu hết công việc trước đây về tạo bộ dữ liệu chú thích văn bản đã tập trung vào việc trích xuất dữ liệu chú thích từ Common Crawl [12, 14, 28]. Thay vào đó chúng tôi tập trung vào việc tổng hợp chú thích trực tiếp bằng cách sử dụng một mô hình BLIP-2 được huấn luyện trước. [42] chứng minh rằng các bộ dữ liệu chú thích hiện có có thể được cải thiện bằng cách sử dụng BLIP2 để tái tạo chú thích các chú thích chất lượng thấp trong các bộ dữ liệu lớn như Datacomp, nhưng không tập trung vào việc tạo ra một bộ dữ liệu mới các chú thích tổng hợp, như chúng tôi làm ở đây.

Một vấn đề, mà chúng tôi không giải quyết, là dữ liệu YFCC100M cũ khoảng một thập kỷ; các hình ảnh CC của nó không hiện tại như những hình ảnh trong LAION-2B. Cho thành công của kết quả chúng tôi, trong tương lai, chúng tôi dự định bổ sung CommonCatalog với hình ảnh Creative-Commons từ các nguồn khác, cũng như kiểm tra các kiến trúc mô hình CommonCanvas lớn hơn.

Lời cảm ơn
Chúng tôi muốn cảm ơn Christopher De Sa về phản hồi về các bản thảo trước của công việc này. A. Feder Cooper được tài trợ bởi giải thưởng NSF RI-CAREER 2046760 của Giáo sư Christopher De Sa. Công việc này cũng được tài trợ bởi giải thưởng CAREER của Volodymyr Kuleshov: #2145577. Chúng tôi cũng muốn cảm ơn Apolinário Passos vì đã giúp chúng tôi lưu trữ dữ liệu + mô hình và cho các cuộc thảo luận sâu sắc dọc theo con đường.

--- TRANG 11 ---
Tài liệu tham khảo
[1] Anderson v. Stability AI, Ltd., 2023. No. 3:23-cv-00201 (N.D. Cal. Jan. 13, 2023).
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[3] Romain Beaumont. LAION-5B: A New Era of Large-Scale Multi-Modal Datasets. LAION Blog, March 2022. URL https://laion.ai/blog/laion-5b/.
[4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. 2023.
[5] Mikołaj Bińkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. arXiv preprint arXiv:1801.01401, 2018.
[6] Davide Caffagni, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Synthcap: Augmenting transformers with synthetic data for image captioning. In International Conference on Image Analysis and Processing, pages 112–123. Springer, 2023.
[7] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting Training Data from Large Language Models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650. USENIX Association, August 2021. ISBN 978-1-939133-24-3.
[8] Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramèr. Poisoning Web-Scale Training Datasets is Practical, 2023.
[9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023.
[10] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, 2022.
[11] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022.
[12] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.
[13] Doe 1 v. GitHub, Inc., 2022. No. 4:22-cv-06823 (N.D. Cal. November 3, 2022).
[14] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. DataComp: In search of the next generation of multimodal datasets, 2023.
[15] Getty Images (US), Inc. v. Stability AI, Inc., 2023. No. 1:23-cv-00135 (D. Del. February 3, 2023).
[16] GitHub. Configuring github copilot in your environment, 2023. URL https://docs.github.com/en/copilot/configuring-github-copilot/configuring-github-copilot-in-your-environment.
[17] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.
[18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. OpenCLIP, July 2021. URL https://doi.org/10.5281/zenodo.5143773.
[19] Daphne Ippolito, Florian Tramèr, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A. Choquette-Choo, and Nicholas Carlini. Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy, 2023.
[20] J.L. v. Alphabet Inc., 2023. No. 3:23-cv-03440-LB (N.D. Cal July 11, 2023).

--- TRANG 12 ---
[21] Kadrey v. Meta Platforms, Inc., 2023. No. 3:23-cv-03417 (N.D. Cal. July 7, 2023).
[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401–4410, 2019.
[23] Dirk P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Conference on Learning Representations, 2014.
[24] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating Concepts in Text-to-Image Diffusion Models, 2023.
[25] Tuomas Kynkäänniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in fréchet inception distance. arXiv preprint arXiv:2203.06026, 2022.
[26] LAION-2Ben, 2022. URL https://huggingface.co/datasets/laion/laion2B-en. Accessed September 23, 2023.
[27] Viktor Lakic, Luca Rossetto, and Abraham Bernstein. Link-Rot In Web-Sourced Multimedia Datasets. In MultiMedia Modeling: 29th International Conference, MMM 2023, Bergen, Norway, January 9–12, 2023, Proceedings, Part I, page 476–488, Berlin, Heidelberg, 2023. Springer-Verlag. ISBN 978-3-031-27076-5. doi: 10.1007/978-3-031-27077-2_37. URL https://doi.org/10.1007/978-3-031-27077-2_37.
[28] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents, 2023.
[29] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin' 'Bout AI Generation: Copyright and the Generative-AI Supply Chain, 2023.
[30] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law: The Next Generation, 2023.
[31] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xFormers: A modular and hackable Transformer modelling library. https://github.com/facebookresearch/xformers, 2022.
[32] Mark A. Lemley. How Generative AI Turns Copyright Law on its Head, 2023. URL https://ssrn.com/abstract=4517702orhttp://dx.doi.org/10.2139/ssrn.4517702.
[33] Pierre N. Leval. Toward a Fair Use Standard. Harvard Law Review, 103(5):1105, 1990.
[34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
[35] Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023.
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014.
[37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.
[38] Susan Box Mann. The Telephone Game, 2019. URL https://icebreakerideas.com/telephone-game/. Accessed September 27, 2023.
[39] Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore, 2023.
[40] The Mosaic ML Team. composer. https://github.com/mosaicml/composer/, 2021.
[41] The Mosaic ML Team. streaming. <https://github.com/mosaicml/streaming/>, 2022.

--- TRANG 13 ---
[42] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. arXiv preprint arXiv:2307.10350, 2023.
[43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, 2023.
[44] Jacob Portes, Alexander R Trott, Sam Havens, Daniel King, Abhinav Venigalla, Moin Nadeem, Nikhil Sardana, Daya Khudia, and Jonathan Frankle. Mosaicbert: How to train bert with a lunch money budget. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023.
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021.
[46] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492–28518. PMLR, 2023.
[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In 2022 IEEE Conference on Computer Vision and Pattern Recognition, 2022.
[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. Medical Image Computing and Computer-Assisted Intervention, pages 234–241, 2015.
[49] Matthew Sag. Copyright Safety for Generative AI. Houston Law Review, 2023. Forthcoming.
[50] Pamela Samuelson. Generative AI meets copyright. Science, 381(6654):158–161, 2023. doi: 10.1126/science.adi0656. URL https://www.science.org/doi/abs/10.1126/science.adi0656.
[51] Sarah Scheffler, Eran Tromer, and Mayank Varia. Formalizing Human Ingenuity: A Quantitative Framework for Copyright Law's Substantial Similarity. In Proceedings of the Symposium on Computer Science and Law, pages 37–49, 2022.
[52] Christoph Schuhmann. LAION-400-Million Open Dataset, 2021. URL https://laion.ai/blog/laion-400-open-dataset/.
[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278–25294, 2022.
[54] Charles M. Schultz. Snoopy Peanuts, 2020. URL https://en.wikipedia.org/wiki/Snoopy#/media/File:Snoopy_Peanuts.png. Accessed September 26, 2023.
[55] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565, 2018.
[56] Benjamin L.W. Sobel. Artificial Intelligence's Fair Use Crisis. Columbia Journal of Law and The Arts, 41:45, 2017.
[57] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathany, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
[58] Stability AI. Stable Diffusion v2-base Model Card, 2022. URL https://huggingface.co/stabilityai/stable-diffusion-2-base.
[59] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.
[60] Tremblay v. OpenAI, Inc., 2023. No. 3:23-cv-03223 (N.D. Cal. June 28, 2023).

--- TRANG 14 ---
[61] James Vincent. Getty Images is suing the creators of AI art tool Stable Diffusion for scraping its content. The Verge, January 2023. URL https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit.
[62] Nikhil Vyas, Sham Kakade, and Boaz Barak. On Provable Copyright Protection for Generative Models, 2023.
[63] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3–19, 2018.
[64] Feiyang Xiao, Qiaoxi Zhu, Jian Guan, Xubo Liu, Haohe Liu, Kejia Zhang, and Wenwu Wang. Synth-ac: Enhancing audio captioning with synthetic supervision. arXiv preprint arXiv:2309.09705, 2023.
[65] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Hongjun Choi, Blake Hechtman, and Shibo Wang. Automatic cross-replica sharding of weight update in data-parallel training. arXiv preprint arXiv:2004.13336, 2020.
[66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.

A Chi tiết Bổ sung về Phân tích Thiếu Dữ liệu

A.1 Giả thuyết: Các mô hình khuếch tán quá nhỏ

Một tính toán thô cung cấp một số hiểu biết về lý do tại sao đây là trường hợp. Xem xét một bộ dữ liệu huấn luyện bao gồm N hình ảnh với độ phân giải H×W và c kênh. Để ghi nhớ hoàn toàn dữ liệu huấn luyện, mô hình phải có khả năng lưu trữ c×H×W×N số. Cho một số tham số có thể huấn luyện Np, thật tự nhiên khi giả định rằng trung bình mỗi tham số có khả năng lưu trữ đủ thông tin để tái tạo một số duy nhất từ bộ dữ liệu huấn luyện. Dưới giả định này, việc ghi nhớ hoàn toàn chỉ có thể nếu kích thước của bộ dữ liệu huấn luyện ở hoặc dưới một kích thước tới hạn Nc(N≤Nc) với Nc được cho bởi Nc=Np/(cHW). Lưu ý rằng kích thước tới hạn này giả định dữ liệu không thể được nén thêm, điều này rõ ràng không phải trường hợp cho hình ảnh tự nhiên. Tuy nhiên, SD2 và SDXL là các mô hình khuếch tán tiềm ẩn, trước tiên sử dụng một bộ mã hóa được huấn luyện trước để nén hình ảnh theo hệ số 8 trong cả H và W, và vì vậy khi chúng tôi huấn luyện LDM như SD2 và SDXL, chúng tôi đang huấn luyện trên dữ liệu đã được nén đáng kể.

Trong các thí nghiệm của chúng tôi, c=4 và H=W=32, tương ứng với hình ảnh RGB có độ phân giải 256×256 trong không gian tiềm ẩn SD2 và SDXL. UNet SD2 có Np=866×10⁶ tham số có thể huấn luyện, và UNet của SDXL có Np=2567×10⁶. Vì vậy chúng tôi tính Nc≈0.2×10⁶ cho SD2 và Nc≈0.6×10⁶ cho CommonCanvas-Large; cả hai số này đều thấp hơn vài bậc độ lớn so với kích thước của các bộ dữ liệu dẫn xuất YFCC của chúng tôi, và vì vậy ngay cả với việc nén dữ liệu bổ sung đáng kể, chúng tôi mong đợi rằng các bộ dữ liệu CommonCatalog của chúng tôi nên đủ để huấn luyện cả SD2 và SDXL. Ngoài ra, lập luận này dự đoán rằng chúng tôi chỉ nên bắt đầu thấy overfitting đáng kể trong những mô hình này cho các bộ dữ liệu có kích thước N∼10⁶. Những ước tính này phụ thuộc vào độ phân giải, và khi độ phân giải hình ảnh tăng, chúng tôi mong đợi rằng Nc sẽ giảm khi nhiều thông tin hơn được cung cấp cho mỗi hình ảnh.

A.2 Tăng khả năng mô hình với CommonCanvas-LNC

Chúng tôi cũng huấn luyện một biến thể của SD2 với nhiều tham số có thể huấn luyện hơn, lấy UNet từ SDXL. Chúng tôi gọi mô hình này là CommonCanvas-LNC. Chúng tôi điều chỉnh kiến trúc UNet SDXL với SD2 bằng cách thay đổi chiều attention chéo để phù hợp với chiều trạng thái ẩn của bộ mã hóa văn bản SD2 (1024 cho SD2 so với 2048 cho SDXL). SDXL cũng huấn luyện lại thành phần VAE trong mô hình của họ, và chúng tôi sử dụng VAE hiệu suất cải thiện này. Ngoại trừ những thay đổi này, kiến trúc giống hệt với SD2.

--- TRANG 15 ---
B Chi tiết Bộ dữ liệu Huấn luyện

B.1 LAION-2B

Thực tế là LAION không phải là một benchmark ổn định có thể dẫn đến nhiều vấn đề về khả năng tái tạo và bảo mật. Các cuộc tấn công đầu độc dữ liệu sẽ khó phát hiện ở quy mô 2 tỷ tham số. Trong khi điều này có thể được giảm thiểu bằng cách sử dụng giá trị hash của hình ảnh, thì bất kỳ lúc nào một trang web quyết định mã hóa lại hình ảnh, những hình ảnh đó giờ đây sẽ cần được loại trừ khỏi bộ dữ liệu. Hơn nữa, các cuộc tấn công đầu độc dữ liệu có mục tiêu cho các mô hình khuếch tán không còn chỉ là phỏng đoán học thuật. Năm ngoái sau khi phát hành Stable Diffusion, một cuộc biểu tình được khởi động trên ArtStation khiến người dùng tải lên hình ảnh có nội dung "NoAI" để làm hỏng dữ liệu huấn luyện tương lai cho các mô hình sinh tạo sau khi các nghệ sĩ cảm thấy như thể tác phẩm của họ đã được sử dụng một cách không công bằng để huấn luyện các mô hình. Với mức độ link rot cao, các cuộc tấn công có mục tiêu khá dễ dàng. Hơn nữa, việc tái tạo các thí nghiệm trở nên hầu như không thể. Điều này có nghĩa là bất kỳ benchmark nào sử dụng bản sao của LAION làm sự thật ground truth có khả năng sử dụng các tập con khác nhau của bộ dữ liệu đầy đủ.

B.1.1 Tìm nguồn hình ảnh Creative-Commons

Bảng 1: Giấy phép CC trong YFCC100M. ND có nghĩa là các tác phẩm phái sinh không được cấp phép hoặc giấy phép không cho phép người dùng tạo ra các tác phẩm phái sinh. NC có nghĩa là hình ảnh không thể được sử dụng trong bối cảnh thương mại. CommonCatalog-C chỉ chứa dữ liệu từ hai hàng dưới cùng (màu vàng), phản ánh hình ảnh được cấp phép cho bối cảnh thương mại (tức là, khoảng 25 triệu hình ảnh). CommonCatalog-NC chứa CommonCatalog-C, và bổ sung bao gồm hai hàng giữa (màu xanh), phản ánh hình ảnh được cấp phép cho mục đích phi thương mại. Chúng tôi không bao gồm khoảng 30 triệu hình ảnh trong hai hàng trên cùng (màu hồng) trong CommonCatalog, vì chúng là giấy phép không phái sinh. Chúng tôi không huấn luyện trên những hình ảnh này. Tuy nhiên, chúng tôi tạo ra các chú thích BLIP-2 cho chúng và phát hành những chú thích đó như một tập đánh giá.

Giấy phép CC # Hình ảnh % Có chú thích
CC-BY-NC-ND-2.0 25,790,117 33.52%
CC-BY-ND-2.0 4,827,970 30.23%
CC-BY-NC-2.0 12,468,229 31.39%
CC-BY-NC-SA-2.0 28,314,685 31.57%
CC-BY-SA 2.0 9,270,079 34.05%
CC-BY 2.0 16,962,338 28.96%

B.1.2 Phát hành và tài liệu

C Ví dụ Hình ảnh YFCC

Bảng 2: Hình ảnh được lấy mẫu ngẫu nhiên từ tập huấn luyện YFCC [59]. Các chú thích BLIP2 tổng hợp của chúng tôi cũng được cung cấp bên dưới.

một người đi xe đạp trên con đường đất các bức tranh trên tường một chiếc xe đua màu cam và xanh chạy trên đường đua

Kiến trúc Mô hình

--- TRANG 16 ---
Bảng 3: Top 10 chú thích có tần suất cao nhất trong bộ dữ liệu YFCC. Các chú thích phổ biến nhất không được người dùng tạo ra và không mô tả rõ ràng hình ảnh tương ứng.

Chú thích Gốc YFCC Số lượng
OLYMPUS+DIGITAL+CAMERA 184889
SONY+DSC 123128
Exif JPEG PICTURE 104480
Barclays+Center+Arena%0AAtlantic+Yards%0A6th+and+Atlantic+A 68832
Olympus+digital+camera 54805
Effortlessly+uploaded+by Eye-Fi 48388
. 43227
-+Camera+phone+upload+powered+by ShoZu 38856
Sony+dsc 32709
Photo+by @Kmeron —Facebook page is this way— 23754

Bảng 4: Số lượng chú thích có thể sử dụng từ bộ dữ liệu YFCC14M của OpenAI [45]. Bảng này thực tế là một tập con từ 1 mà hoặc mô tả người dùng hoặc tiêu đề hình ảnh được coi là có thể sử dụng. Những con số này cung cấp một ước tính về số lượng hình ảnh trong mỗi danh mục thực sự có tiềm năng có thể sử dụng làm chú thích.

Tên Giấy phép số lượng
CC-BY 2.0 2448002
CC-BY-ND 2.0 682273
CC-BY-NC 2.0 1925854
CC-BY-NC-ND 2.0 4058817
CC-BY-NC-SA 2.0 4146113
CC-BY-SA 2.0 1568336

Chúng tôi tuân theo kiến trúc mô hình và công thức huấn luyện của Stable Diffusion 2 càng sát càng tốt để tái tạo tốt nhất mô hình cho CC-Small. Mô hình có số lượng tham số và cấu trúc giống hệt với mô hình gốc. Thực tế, chúng tôi thậm chí có thể tải trọng số mô hình của SD2 vào framework của chúng tôi do kiến trúc và sơ đồ đặt tên giống hệt nhau. Chúng tôi có thể đạt được hiệu suất hầu như giống hệt với SD2 trong thời gian huấn luyện ngắn hơn nhiều với ít dữ liệu hơn. Chúng tôi sử dụng cùng VAE, tokenizer, và kiến trúc UNet như SD2 ngoại trừ việc giảm độ chính xác của các lớp normalization.

Mô hình CC-Large của chúng tôi lấy mô hình của SD2 và thay thế UNet bằng kiến trúc SDXL [43]. Giống như CC-Small, chúng tôi cũng thay thế các lớp normalization bằng phiên bản độ chính xác thấp của chúng. Việc thay thế tất cả các lớp normalization được xử lý tự động bởi thư viện Composer của MosaicML [40]. Chúng tôi thực hiện tất cả việc tải dữ liệu thông qua thư viện streaming của MosaicML [41].

D Chi tiết Bổ sung về Tối ưu hóa Hiệu quả

Trong phần này chúng tôi cung cấp chi tiết bổ sung về các tối ưu hóa mà chúng tôi triển khai để đạt được tăng tốc huấn luyện SD2. Chúng tôi cũng báo cáo chi phí gần đúng của việc huấn luyện triển khai SD2 của chúng tôi trên các cấu hình phần cứng khác nhau trong Bảng 5.

Flash Attention. Các hoạt động cross attention là một phần rất tốn kém của huấn luyện xảy ra trong hàng chục lớp trong UNet mô hình khuếch tán [47]. Flash Attention là một triển khai hiệu quả

Bảng 5: Hiệu suất (throughput) và chi phí gần đúng của việc huấn luyện UNet SD2 với các tối ưu hóa của chúng tôi. Tùy thuộc vào số lượng GPU được sử dụng, chi phí để huấn luyện cùng các mô hình mà không có những tối ưu hóa này dao động từ $90,000-$140,000

Số A100 256x256 (img/s) 512x512 (img/s) 512x512 với EMA (img/s) Ngày để Huấn luyện Chi phí ($)
8 1100 290 290 101.04 $38,800.00
16 2180 585 580 50.29 $38,630.00
32 4080 1195 1160 25.01 $38,420.00
64 8530 2340 2220 12.63 $38,800.00
128 11600 4590 3927 6.79 $41,710.00

--- TRANG 17 ---
Lời nhắc SD2CommonCanvas-
SCCommonCanvas-
SNCCommonCanvas-
LNC
một mô hình CAD 3D
của một máy bay
một con gấu và một con cáo trong rừng
một chai klein
một chiếc bánh sinh nhật
được cắt một phần với
kem phủ màu hồng và xanh
hai chim ruồi và một con sóc trong bồn tắm chim

Hình 12: Ví dụ định tính bổ sung so sánh SD2 với mô hình của chúng tôi được huấn luyện trên phần thương mại (CommonCanvas-SC), phần phi thương mại (CommonCanvas-SNC), và mô hình UNet lớn hơn được huấn luyện trên phi thương mại (CommonCanvas-LNC).

Hình 13: Ví dụ định tính bổ sung của các mô hình CommonCanvas của chúng tôi.

--- TRANG 18 ---
Đầu vào cho BLIP2 Chú thích BLIP2 SD2CommonCanvas-
SNCCommonCanvas-
SC
một hình ảnh của elsa từ frozen
pikachu pikachu pikachu pikachu pikachu pikachu pikachu pikachu pikachu
ba nhân vật mặc như gấu, đứng trong rừng

Hình 14: Ví dụ định tính bổ sung so sánh các mô hình CommonCanvas của chúng tôi với SD2, được đưa ra các chú thích tổng hợp BLIP2 làm lời nhắc. Mặc dù không hoàn hảo, các mô hình của chúng tôi tốt hơn trong việc tránh tạo ra dữ liệu có khả năng có vấn đề.

được tối ưu hóa để hoạt động tốt với độ chính xác giảm và phần cứng GPU [11], được triển khai sử dụng thư viện XFormers [31], cho phép chúng tôi tiết kiệm tính toán và sử dụng bộ nhớ.

Tính toán trước latent. Mỗi lần chuyển tiếp của SD2 yêu cầu tính toán biểu diễn tiềm ẩn của hình ảnh đầu vào, cũng như biến đổi chú thích thành embedding văn bản. Thay vì tính toán latent cho mỗi ví dụ trong quá trình huấn luyện, chúng tôi có thể tính toán trước latent cho toàn bộ bộ dữ liệu, phân bổ chi phí. Làm như vậy tăng tốc việc huấn luyện mô hình, đặc biệt ở độ phân giải thấp hơn, để đổi lấy chi phí cố định một lần của việc tính toán trước tất cả latent qua 1 epoch.

GroupNorm và LayerNorm độ chính xác giảm. Hầu hết các lớp trong SD2 được triển khai trong độ chính xác float16, nhưng GroupNorm và LayerNorm được triển khai trong float32, một phần vì được giả định là cần thiết cho sự ổn định huấn luyện. Việc upcast thường xuyên kết quả gây ra một nút thắt cổ chai lớn trong tốc độ huấn luyện. Công việc gần đây cho thấy rằng việc triển khai LayerNorm sử dụng độ chính xác float16 là an toàn [44], và chúng tôi thấy điều tương tự đúng với GroupNorm. Do đó chúng tôi chuyển tất cả các toán tử GroupNorm và LayerNorm thành float16 và có thể giảm thêm tổng tiêu thụ bộ nhớ và tăng tốc huấn luyện.

Fully-Sharded Data Parallelism (FSDP). FSDP là một biến thể của huấn luyện song song dữ liệu phân mảnh các tham số mô hình, gradient và trạng thái optimizer qua nhiều thiết bị. Khi huấn luyện batch dữ liệu không vừa vào bộ nhớ, chúng tôi thực hiện một số lần chuyển tiếp và ngược về trên các microbatch nhỏ hơn, theo sau là một cập nhật gradient duy nhất. Ở quy mô GPU, có thể chỉ có một microbatch duy nhất, vì vậy thời gian cho cập nhật gradient có thể trở thành một nút thắt cổ chai đáng kể. Trong huấn luyện phân phối dữ liệu tiêu chuẩn, mỗi GPU giao tiếp tất cả gradient của nó với mọi GPU khác, và sau đó mỗi GPU cập nhật bản sao cục bộ của mô hình. Thay vào đó, chúng tôi sử dụng một paradigm khác được lấy cảm hứng từ [65] nơi mỗi GPU chỉ nhận gradient và cập nhật trọng số cho một phần nhỏ của mô hình trước khi gửi trọng số cập nhật cho phần đó của mô hình đến tất cả GPU khác. Bằng cách chia bước cập nhật qua tất cả GPU, chúng tôi có thể đảm bảo rằng lượng công việc trên mỗi GPU giảm khi chúng tôi tăng số lượng GPU, giúp chúng tôi đạt được scaling tuyến tính. Để giải quyết vấn đề này, chúng tôi sử dụng hỗ trợ thử nghiệm của PyTorch cho Fully Sharded Data Parallelism (FSDP), cụ thể là chế độ SHARD GRAD OP của FSDP.

--- TRANG 19 ---
Hình 15: Việc giảm lượng dữ liệu huấn luyện ảnh hưởng như thế nào đến động lực huấn luyện? Chúng tôi thấy một sự cải thiện đáng chú ý khi huấn luyện với ít hơn 10 triệu mẫu.

Exponential Moving Average (EMA) có lịch trình. SD2 sử dụng EMA, duy trì một exponential moving average của trọng số tại mỗi cập nhật gradient cho toàn bộ thời kỳ huấn luyện. Điều này có thể chậm do các hoạt động bộ nhớ cần thiết để đọc và ghi tất cả trọng số tại mỗi bước. Vì các trọng số cũ bị phân rã bởi hệ số 0.9999 tại mỗi batch, các lần lặp đầu của huấn luyện chỉ đóng góp tối thiểu vào trung bình cuối cùng. Chúng tôi quyết định chỉ áp dụng EMA cho 50K bước cuối cùng (khoảng 3.5% của thời kỳ huấn luyện), và có thể tránh thêm overhead và vẫn đạt được một mô hình EMA gần như tương đương.

E Telephoning: Một Phương pháp Tạo Chú thích Hình ảnh Dựa trên Học Chuyển giao

Giải pháp của chúng tôi để xử lý việc thiếu chú thích trong hình ảnh CC được gọi là telephoning, một loại học chuyển giao (Hình 3). Telephoning giả định sự tồn tại của một bộ dữ liệu có nhãn lớn D₁={(x⁽ⁱ⁾, y⁽ⁱ⁾)}ⁿᵢ₌₁, bao gồm các cặp x⁽ⁱ⁾ chiều cao (ví dụ, hình ảnh, âm thanh) ánh xạ đến một nhãn có cấu trúc, compact y⁽ⁱ⁾ (ví dụ, chú thích, bản sao âm thanh). Telephoning huấn luyện một mô hình tiến q(y|x) trên D₁ để học ánh xạ của y cho x qua học maximum likelihood max q∈Q Σⁿᵢ₌₁ log q(y⁽ⁱ⁾|x⁽ⁱ⁾). Sau đó nó sử dụng q như tín hiệu huấn luyện cho một mô hình ngược p(x|y) được huấn luyện trên một bộ dữ liệu riêng D₂={x⁽ⁱ⁾}ᵐᵢ₌₁ bằng cách tối đa hóa Σᵐᵢ₌₁ E_y~q(y|x⁽ⁱ⁾)[log p(x⁽ⁱ⁾|y⁽ⁱ⁾)], khả năng của dữ liệu D₂ và nhãn được dự đoán y dưới q. Điều này tạo thành một loại chuyển giao kiến thức từ nhiệm vụ gán nhãn tiến được định nghĩa bởi D₁ đến nhiệm vụ ngược của việc đảo ngược x từ y trên một D₂ riêng biệt.

Trong khi telephoning có thể được xem như một loại gán nhãn tổng hợp, nó trở nên đặc biệt thú vị khi x là một loại phương thức được bảo vệ (ví dụ, một hình ảnh có bản quyền), trong khi y là một biểu diễn compact của x không mã hóa các khía cạnh nhạy cảm của y (ví dụ, một chú thích chung). Thực tế, telephoning thực hiện một loại "nén mất mát" hoặc "chưng cất" từ một x giàu chiều cao hoặc thông tin (ví dụ, một hình ảnh của Snoopy) thành một y nghèo chiều thấp hoặc thông tin mất đi nội dung nhạy cảm trong x (ví dụ, các đặc điểm hình ảnh của Snoopy). Bởi vì bước nén này là "mất mát", một tái tạo x' của x từ p(x|y) qua y thường không giống chút nào với đầu vào ban đầu, giống như trong một trò chơi điện thoại [38]. Chúng tôi rút ra thuật ngữ telephoning từ trực giác trên, và sử dụng nó như một cách viết tắt hữu ích để biểu thị các trường hợp học chuyển giao giải quyết các vấn đề thiếu dữ liệu trong mô hình sinh tạo đa phương thức.

Telephoning cho mô hình văn bản thành hình ảnh. Trong công việc này, chúng tôi áp dụng telephoning vào miền hình ảnh và văn bản, nơi hình ảnh CC là các đầu vào chiều cao x, và chúng tôi sử dụng một mô hình BLIP-2 được huấn luyện trước [34] cho "nén mất mát" thành các chú thích văn bản ngắn y (Hình 3a). Cùng nhau, những cặp hình ảnh-chú thích CC này bao gồm bộ dữ liệu CommonCatalog, mà chúng tôi sử dụng để huấn luyện các mô hình T2I CommonCanvas của chúng tôi (Hình 3b). Mặc dù BLIP-2 được huấn luyện trước trên LAION-400M [52], CommonCatalog và CommonCanvas không bao giờ có quyền truy cập trực tiếp vào LAION-400M hoặc, quan trọng là, bất cứ thứ gì tương tự với hình ảnh mà BLIP-2 được huấn luyện. Thay vào đó, chúng tôi chỉ có quyền truy cập vào ánh xạ trong mô hình, mà khi được cho một đầu vào hình ảnh, tạo ra văn bản đầu ra mất mát mà vốn dĩ không giống theo nghĩa đen với đối tác hình ảnh của nó (Hình 3c).³

³Chúng tôi dựa trên ví dụ của Snoopy từ [49]. Snoopy của Hình 3 được cấp phép CC [54].
