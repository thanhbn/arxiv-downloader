# 2310.01407.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.01407.pdf
# File size: 49093027 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CoDi: Conditional Diffusion Distillation
for Higher-Fidelity and Faster Image Generation
Kangfu Mei*1,2, Mauricio Delbracio1, Hossein Talebi1,
Zhengzhong Tu1, Vishal M. Patel2, Peyman Milanfar1
1Google Research,2Johns Hopkins University
https://fast-codi.github.io
(a) Our 4-step real-world super-resolution
(b) Our 1-step InstructPix2Pix with: “Make it lowkey ” and “Make it sunset ”
(c) Our 4-step generation from depth-map
(d) Our 4-step inpainting results with prompts: Shiba , Husky , Alpaca , Panda , Hawk , Dragon
(a) Our 4-step real-world super-resolution
(c) Our 4-step depth-map generation
(b) Our 4-step inpainting results with prompts: Shiba Inu , Husk , Chihuahua …
(c) Our 1-step InstructPix2Pix
Figure 1. Our proposed CoDi efficiently distills a conditional diffusion model from an unconditional one, enabling rapid generation of
high-quality images under various conditional settings. We demonstrate CoDi’s capabilities through generated results across various tasks.
Abstract
Large generative diffusion models have revolutionized
text-to-image generation and offer immense potential for
conditional generation tasks such as image enhancement,
restoration, editing, and compositing. However, their
widespread adoption is hindered by the high computational
cost, which limits their real-time application. To address
this challenge, we introduce a novel method dubbed CoDi,
that adapts a pre-trained latent diffusion model to accept
additional image conditioning inputs while significantly re-
ducing the sampling steps required to achieve high-quality
results. Our method can leverage architectures such as
ControlNet to incorporate conditioning inputs without com-
promising the model’s prior knowledge gained during large
scale pre-training. Additionally, a conditional consistency
loss enforces consistent predictions across diffusion steps,
effectively compelling the model to generate high-quality
images with conditions in a few steps. Our conditional-task learning and distillation approach outperforms previ-
ous distillation methods, achieving a new state-of-the-art
in producing high-quality images with very few steps (e.g.,
1-4) across multiple tasks, including super-resolution, text-
guided image editing, and depth-to-image generation.
1. Introduction
Text-to-image diffusion models [27, 29, 34] trained on
large-scale data [15, 38] have significantly dominated gen-
erative tasks by delivering impressive high-quality and di-
verse results. A newly emerging trend is to use the prior
of pre-trained text-to-image models such latent diffusion
models (LDMs) [29] to guide the generated results with ex-
ternal image conditions for image-to-image transformation
tasks such as image manipulation, enhancement, or super-
resolution [22, 53]. Among these transformation processes,
the diffusion prior introduced by pre-trained models is
1arXiv:2310.01407v2  [cs.CV]  17 Feb 2024

--- PAGE 2 ---
shown to be capable of greatly promoting the visual quality
of the conditional image generation results [3, 16, 26, 31].
However, diffusion models heavily rely on an iterative
refinement process [4, 33, 35, 43, 49] that often demands
a substantial number of iterations, which can be challeng-
ing to accomplish efficiently. Their reliance on the num-
ber of iterations further increases for high-resolution image
synthesis. For instance, in state-of-the-art text-to-image la-
tent diffusion models [29], achieving optimal visual quality
typically requires 20−200sampling steps (function evalua-
tions), even with advanced sampling methods [10, 17]. The
slow sampling time significantly impedes practical applica-
tions of the aforementioned conditional diffusion models.
Recent efforts to accelerate diffusion sampling predom-
inantly employ distillation methods [21, 36, 44]. These
methods achieve significantly faster sampling, completing
the process in just 4−8steps, with only a marginal decrease
in generative performance. Very recent works [14, 23] show
that these strategies are even applicable for distilling pre-
trained large-scale text-to-image diffusion models.
A very common application scenario is to incorporate
new conditions into these distilled diffusion models, such
as using low-resolution images for super-resoltion [35], or
instruction-tuning for image editing [3], where the most
straightforward way is to directly finetune the distilled text-
to-image pre-trained model with new conditional data. An
alternative common approach [23] is to first finetune the dif-
fusion model with the new conditional data, then conduct-
ing distillation on the already-finetuned conditional model.
While these two methods have been demonstrated to accel-
erate sampling, each has distinct disadvantages in terms of
result quality and cross-task flexibility, as discussed below.
In this paper, we introduce a new algorithm for
Conditional Distillation which we call CoDi for efficiently
adding new controls into distilled models. Unlike previ-
ous distillation methods that rely on finetuning, our method
directly distills a diffusion model from a text-to-image pre-
training ( e.g., StableDiffusion) and ends with a fully dis-
tilled conditional diffusion model. As depicted in Figure 1,
our distilled model is capable of predicting high-quality re-
sults in just 1−4sampling steps.
By design, our method eliminates the need for the orig-
inal text-to-image data [37, 38], a requirement in previous
distillation methods ( i.e., those that first distill the uncon-
ditional text-to-image model), thereby making our method
more practical. Additionally, our formulation avoids sac-
rificing the diffusion prior in the pre-trained model dur-
ing finetuning, a common drawback in the first stage of
the finetuning-first procedure. Our extensive experiments
show that our CoDi outperforms previous distillation meth-
ods in both visual quality and quantitative metrics, particu-
larly when operating under the same sampling time.
Parameter-efficient distillation methods are a relativelyunderstudied area. We demonstrate that our method also en-
ables a new Parameter- Efficient distillation paradigm ( PE-
CoDi ). It can transform an unconditional diffusion model
to conditional tasks by incorporating a small number of
additional learnable parameters. Specifically, our formula-
tion allows for integration with various existing parameter-
efficient tuning algorithms, e.g., ControlNet [53]. We show
that our distillation process that integrates the ControlNet
adapter can efficiently preserve the generative prior in pre-
training while adapting the model to new conditioned data.
This new paradigm significantly improves the practicality
of different conditional tasks.
Our contributions are summarized as follows:
• We propose a new method for image and image-text con-
ditioned generation. It can derive a conditional diffusion
model from pretrained text-to-image LDMs for generat-
ing high-quality results in only a few sampling steps.
• The proposed method’s efficiency and effectiveness arise
from a non-trivial consistency between the model’s pre-
dictions at different time steps. Enforcing this consis-
tency through learning enables the simultaneous reduc-
tion of required sampling steps and the integration of new
conditions into the model.
• We introduce the first parameter-efficient distillation
mechanism that can produce compelling results in just a
few steps, while requiring only a small number of addi-
tional parameters compared with the pretrained LDMs.
2. Related Work
Diffusion Distillation. To reduce the sampling time of
diffusion models, Luhman et al. [21] proposed to learn a
single-step student model from the output of the original
(teacher) model using multiple sampling steps. However,
this method requires to run the full inference with many
sampling steps during training which make it poorly scal-
able. Inspired by this, Progressive Distillation [36] and its
variants, including Guided Distillation [23] and SnapFu-
sion [14], use a progressive learning scheme for improving
the learning efficiency. A student model learns to predict the
output of two steps of the teacher model in one step. Then,
the teacher model is replaced by the student model, and
the procedure is repeated to progressively distill the mode
by halving the number of required steps. We demonstrate
our method by comparing these methods on the conditional
generation tasks. We note that strategies like classifier-free
guidance distillation [14, 23], or the different adopted sam-
pling techniques [51, 54], are orthogonal to our method, and
they could be incorporated in our formulation. Even though
some concurrent works [50, 52] find that tasks like super-
resolution requires less sampling steps, we later show that
distilling pre-trained diffusion models can still improve the
performance in such restoration tasks.
Consistency Distillation. A Consistency Model is a single-
2

--- PAGE 3 ---
step generative approach distilled from a pre-trained dif-
fusion model [44]. The learning is achieved by enforcing
a self-consistency in the predicted signal space. Based on
this idea, following work [7, 11, 19, 41] have focus on im-
proving the training techniques. However, learning con-
sistency models for conditional generation has yet to be
thoroughly studied. In this paper, we compare our method
against a baseline approach that enforces self-consistency
in an already fine-tuned conditional diffusion model. Our
results demonstrate that our conditional distilled model out-
performs the baseline approach, indicating the effectiveness
of our proposed distillation strategy.
Diffusion Models Adaptations. Leveraging the knowl-
edge of pre-trained models for new tasks, known as model
adaptation, has gained significant traction in NLP and
computer vision domains. This approach utilizes model
adapters [9, 28, 30, 45] and HyperNetworks [1, 6] to effec-
tively adapt pre-trained models to new domains and tasks.
In the context of diffusion models, model adapters have
been successfully employed to incorporate new conditions
into pre-trained models [24, 53]. Our proposed method
draws inspiration from these approaches and introduces a
novel application of model adapters: distilling the sampling
steps of diffusion models. Compared to fine-tuning the en-
tire model [36], our method offers enhanced efficiency and
flexibility. It enables the adaptation of multiple tasks using
the same backbone model.
3. Background
Continuous-time VP diffusion model. A continuous-time
variance-preserving (VP) diffusion model [8, 39] is a spe-
cial case of diffusion models1. It has latent variables {zt|t∈
[0, T]}specified by a noise schedule comprising differen-
tiable functions {αt, σt}withσ2
t= 1−α2
t. The clean data
x∼pdata is progressively perturbed in a (forward) Gaus-
sian process as in the following Markovian structure:
q(zt|x) =N(zt;αtx, σ2
tI), (1)
q(zt|zs) =N(zt;αt|szs, σ2
t|sI), (2)
where 0≤s < t≤1andα2
t|s=αt/αs. Here the latent zt
is sampled from the combination of the clean data and ran-
dom noise by using the reparameterization trick [13], which
haszt=αtx+σtϵ.
Deterministic sampling. The aforementioned diffusion
process that starts from z0∼pdata(x)and ends at zT∼
N(0,I)can be modeled as the solution of an stochastic
differential equation (SDE) [43]. The SDE is formed by
a vector-value function f(·,·) :Rd→Rd, a scalar function
1What we discussed based on the variance preserving (VP) form of
SDE [43] is equivalent to most general diffusion models like Denoising
Diffusion Probabilistic Models (DDPM) [8].g(·) :R→R, and the standard Wiener process was:
dzt=f(zt, t)dt+g(t)dw. (3)
The overall idea is that the reverse-time SDE that runs back-
wards in time, can generate samples of pdatafrom the prior
distribution N(0,I). This reverse SDE is given by
dzt= [f(zt, t)−g(t)2∇zlogpt(zt)]dt+g(t)d¯w,(4)
where the ¯wis a also standard Wiener process in reversed
time, and ∇zlogpt(zt)is the score of the marginal distri-
bution at time t. The score function can be estimated by
training a score-based model sθ(zt, t)≈ ∇ zlogpt(zt)with
score-matching [42] or a denoising network ˆxθ(zt, t)[8]:
sθ(zt, t) := ( αtˆxθ(zt, t)−zt)/σ2
t. (5)
Such backward SDE satisfies a special ordinary differential
equation (ODE) that allows deterministic sampling given
zT∼ N(0,I). This is known as the probability flow (PF)
ODE [43] and is given by
dzt= [f(zt, t)−1
2g2(t)sθ(zt, t)]dt, (6)
where f(zt, t) =d logαt
dtzt,g2(t) =dσ2
t
dt−2d logαt
dtσ2
t
with respect to {αt, σt}andtaccording to [12]. This
ODE can be solved numerically with diffusion samplers like
DDIM [40], where starting from ˆzT∼ N(0,I), we update
fors=t−∆t:
ˆzs:=αsˆxθ(ˆzt, t) +σs(ˆzt−αtˆxθ(ˆzt, t))/σt,(7)
till we reach ˆz0.
Diffusion models parametrizations. Leaving aside the
aforementioned way of parametrizing diffusion models with
a denoising network (signal prediction) or a score model
(noise prediction equation 5), in this work, we adopt a pa-
rameterization that mixes both the score (or noise) and the
signal prediction. Existing methods include either predict-
ing the noise ˆϵθ(xt, t)and the signal ˆxθ(zt, t)separately
using a single network [5], or predicting a combination of
noise and signal by expressing them in a new term, like the
velocity model ˆvθ(zt, t)≈αtϵ−σtx[36]. Note that one
can derive an estimation of the signal and the noise from the
velocity one,
ˆx=αtzt−σtˆvθ(zt, t),and ˆϵ=αtˆvθ(zt, t) +σtzt.(8)
Similarly, DDIM update rule (equation 7) can be rewritten
in terms of the velocity parametrization:
ˆzs:=αs(αtˆzt−σtˆvθ(ˆzt, t))+σs(αtˆvθ(ˆzt, t)+σtˆzt).(9)
Self-consistency property. To accelerate inference, [44]
introduced the idea of consistency models. Let sθ(·, t)
3

--- PAGE 4 ---
be a pre-trained diffusion model trained on data x∼
Odata. Then, a consistency function fϕ(zt, t)should sat-
isfy that [44] where fϕ(x,0) =xand
fϕ(zt, t) =fϕ(zt′, t′),∀t, t′∈[0, T], (10)
where {zt}t∈[0,T]is the solution trajectory of the probabil-
ity flow ODE (PF-ODE) (equation 6). A boundary condi-
tion, i.e.,fϕ(x,0) =xis parameterized with skip connec-
tions for ensuring continuous properties similar as done in
previous works [2, 10, 44]:
Fϕ(zt, t) =cskip(t)x+cout(t)fϕ(zt, t), (11)
where cskip(0) = 1 ,cout(0) = 0 . In practice, fϕ(zt, t)
is usually a denoising network that is distilled from a pre-
trained diffusion model. We later show that we can replace
the frozen PF-ODE with the distillation network and thus fit
the PF-ODE for new conditional data during distillation.
4. Method
4.1. From Unconditional to Conditional
In order to utilize the image generation prior encapsulated
by the pre-trained unconditional2diffusion model, we first
propose to adapt the unconditional diffusion model into a
conditional version for the conditional data (x, c)∼pdata.
Similar to the zero initialization technique used by control-
lable generation [25, 53], our method adapts the uncondi-
tional pre-trained architecture by using an additional condi-
tional encoder.
To elaborate, we take the widely used U-Net as the dif-
fusion network. Let us introduce the conditional-module
by duplicating the encoder layers of the pretrained network.
Then, let hθ(·)be the encoder features of the pretrained
network, and hη(·)be the features on the additional condi-
tional encoder. We define the new encoder features of the
adapted model by
hθ(zt)′= (1−µ)hθ(zt) +µhη(c), (12)
where µis a learnable scalar parameter, initialized to µ=
0. Starting from this zero initialization, we can adapt the
unconditional architecture into a conditional one. Thus,
our conditional diffusion model ˆwθ(zt, c, t)is the result
of adapting the pre-trained unconditional diffusion model
ˆvθ(zt, t)with the conditional features hη(c).
4.2. A New Conditional Diffusion Consistency
Our core idea is to optimize the adapted conditional diffu-
sion model ˆwθ(zt, c, t)from ˆvθ(zt, t), so it satisfies a con-
ditional diffusion consistency property:
ˆwθ(zt, c, t) =ˆwθ(ˆzs, c, s),∀t, s∈[0, T], (13)
2The discussed unconditional models include text-conditioned image
generation models, e.g., StableDiffusion [29] and Imagen [34], which are
only conditioned on text prompts.where the ˆzsbelong to the probability flow ODE (equa-
tion 6) of the adapted model. Note that this consistency
property differs from the one in consistency models [44] in
the probability flow ODE model used for sampling ˆzsand
the consistency loss space. To motivate this formulation, let
us introduce the following general remark.
Remark 1. If a diffusion model, parameterized by
ˆvθ(zt, t), satisfies the self-consistency property (equa-
tion 10) on the noise prediction ˆϵθ(zt, t) =αtˆvθ(zt, t) +
σtzt, then it also satisfies the self-consistency property on
the signal prediction ˆxθ(zt, t) =αtzt−σtˆvθ(zt, t).
The proof is a direct consequence of change of vari-
ables from noise into signal and is given in Appendix.
Based on this general remark, we claim that we can opti-
mize the conditional diffusion model ˆwθ(zt, c, t)to jointly
learn to enforce the self-consistency property on the noise
prediction ˆϵθ(zt, c, t)and the new conditional generation
(x, c)∼pdata with the signal prediction ˆxθ(zt, c, t). We
then impose the boundary condition for consistency distilla-
tion by parameterizing the noise prediction ˆϵθ(zt, c, t)with
the same skip connections of equation 17.
Prediction of ˆzs.In the distillation process given by equa-
tion 15, the latent variable ˆzsis achieved by running one
step of a numerical ODE solver. Consistency models [44]
solve the ODE using the Euler solver, while progressive dis-
tillation [36] and guided distillation [23] run two steps using
the DDIM sampler (equation 7).
We propose an alternative prediction for ˆzsthat lever-
ages the adapted diffusion model, ˆxθ(zt, c, t), as opposed
to the conventional frozen pretraining one. We then sample
ˆzsin the adapted diffusion model PF-ODE by
ˆzs=αsˆxθ(zt, c, t) +σsϵ,withzt=αtx+σtϵ,(14)
andϵ∼ N(0,I). This novel formulation effectively harmo-
nizes the conflicting optimization directions between con-
sistency distillation from pretrained data and conditional
guidance from conditional data.
Training scheme. Inspired by consistency models [44], we
use the exponential moving averaged parameters θ−as the
target network for stabilize training. Then, we can minimize
the following training loss for conditional distillation:
L(θ):=E[dϵ(ˆϵθ-(ˆzs,s,c),ˆϵθ(zt,t,c))) + dx(x,ˆxθ(zt, t, c)].
(15)
where dϵ(·,·)anddx(·,·)are two distance functions to mea-
sure difference in the noise space and in the signal space
respectively. Note that the total loss is a balance between
the conditional guidance given by dx, and the noise self-
consistency property given by dϵ.
The overall conditional distillation algorithm is pre-
sented in Appendix. In the following, we will detail how
4

--- PAGE 5 ---
0.250 0.275 0.300 0.325 0.350 0.375
LPIPS2025303540FID
Flpips( (x), ( x(zt,c))
||x ( (x(zt,c)))||2
2|| (x) ( x(zt,c))||2
2
||x x (zt)||2
2
Figure 2. Sampled results between distilled models learned with alternative conditional guidance. Left curves shows the quantitative
performance between the LPIPS and FID in {1,2,4,8}steps. Right part show the visual results where each result comes from the 1
sampling step (top) or 4 sampling steps (bottom). The distance function from the left to right is ∥x−E(D(ˆxθ(zt, c)))∥2
2,∥D(x)−
D(ˆxθ(zt, c))∥2
2,Flpips(D(x),D(ˆxθ(zt, c)), and our default ∥x−ˆxθ(zt)∥2
2, respectively.
we sample ˆzsand discuss other relevant hyperparameters in
our method (e.g., dx).
4.3. Effects of Different Conditional Guidance
To finetune the adapted diffusion model with the new con-
ditional data, our conditional diffusion distillation loss in
equation 15 penalizes the difference between the predicted
signal ˆxθ(zt, c, t)and the corresponding image xwith a dis-
tance function dx(·,·)for distillation learning.
Here we investigate the impact of the distance func-
tiondx(·,·)in the conditional guidance. According to
both qualitative and quantitative results, shown in Fig-
ure 2, different distance functions lead to different be-
haviours when doing multi-step sampling (inference). If
dx=∥ · ∥2in the pixel space or the encoded space, i.e.,
∥x−E(D(ˆxθ(zt, c, t)))∥2
2and∥D(x)−D(ˆxθ(zt, c, t))∥2
2,
multi-step sampling leads to more smooth and blurry re-
sults. If instead we adopt a perceptual distance in the pixel
space, i.e.,Flpips(D(x),D(ˆxθ(zt, c, t))), the iterative re-
finement in the multi-step sampling leads to over-saturated
results. Overall, by default we adopted the ℓ2distance in
the latent space since it leads to better visual quality and
achieve the optimal FID with 4 sampling steps in Figure 2.
4.4. Parameter-Efficient Conditional Distillation
Our method offers the flexibility to selectively update pa-
rameters pertinent to distillation and conditional finetun-
ing, leaving the remaining parameters frozen. This leads
us to introduce a new fashion of parameter-efficient condi-
tional distillation, aiming at unifying the distillation process
across commonly-used parameter-efficient diffusion model
finetuning, including ControlNet [53], T2I-Adapter [24],
etc. We highlight the ControlNet architecture illustrated in
Figure 3 as an example. This model duplicates the encoder
part of the denoising network, highlighted in the green
blocks, as the condition-related parameters. Our method
can then optimizes the conditional guidance and the consis-
tency by only updating the duplicated encoder.
Text
Noise
...
pretraining
Image
new conditional data ...zero-convzero-convNoise 
frozen target / online network diffusion latent variables attention layersSignal Noise 
Signal Figure 3. Network architecture illustration of our parameter-
efficient conditional distillation framework.
CM-I CM-II GD-I GD-II Ours
stage-1 distill finetune distill finetune conditional distill
stage-2 finetune distill finetune distill n.a.
✗ ✓ ✓ ✓ ✓
Table 1. We compare previous distillation methods by applying
them to a T2I LDMs and then finetuning the distilled models (CM-
X), and also distillation methods by directly applying them into the
finetuned LDMs (GD-X). Since fine-tuning a distilled consistency
model within the existing diffusion loss framework is not feasible,
we excluded it from our comparison.
5. Experiments
We demonstrate the efficacy of our method on represen-
tative conditional generation tasks, including, real-world
super-resolution [48], depth-to-image generation [53], and
instructed image editing [3]. We utilize a pre-trained text-
to-image latent diffusion models3and conduct conditional
distillation directly from the model. Each of the compared
methods, including the text-to-image pretraining, was inde-
pendently trained for 8 days on 64 TPU-v4 pods.
3We base our work on a version of Latent Diffusion Model trained on
internal text-to-image data. It is comparable with StableDiffusion v1.4.
5

--- PAGE 6 ---
LR CM-II CoDi (Ours) HR Ground Truth Mask CM-II PF-CoDi (Ours)
Figure 4. We show the results sampled in 4 steps by different models. Samples generated according to the low-resolution images (left) and
masks (right) respectively. Please see our supplement for many more examples such as visual comparisons with the other methods.
Super-resolution (DF2K)
Sampling Steps Methods FID ↓ LPIPS ↓
1 step RealESRGAN [48] 37.640 0.3112
200 steps StableSR [46] 24.440 0.3114
4 steps DiffIR [50] 31.719 0.3088
4 steps ControlNet [53] 34.56 0.3381
250 steps LDMs [29] 19.200 0.2639
50 steps LDMs [29] 19.231 0.2603
20 steps LDMs [29] 20.510 0.2627
8 steps LDMs [29] 24.493 0.2789
6 steps LDMs [29] 26.338 0.2873
4 steps LDMs [29] 29.266 0.3014
4 steps + DPM Solve [17] 28.936 0.3077
4 steps + DPM Solver++ [18] 28.937 0.3073
GD-I [23] 27.806 0.3202
GD-II [23] 23.675 0.2796
CM-II (frozen) [44] 28.088 0.3192
CM-II [44] 27.810 0.3172
4 steps PE-CoDi (Ours) 25.214 0.2941
CoDi (Ours) 19.637 0.2656Inpainting (ImageNet)
Sampling Steps Methods FID LPIPS
1000 steps Palette [33] 13.151 -
250 steps Repaint [20] - 0.2827
50 steps ControlNet [53] 14.895 0.2260
4 steps ControlNet [29] 20.205 0.2635
+ DPM Solver++ [18] 19.941 0.2644
CM-II [44] 17.710 0.2580
GD-II [23] 15.95 0.2452
4 steps PE-CoDi (Ours) 14.700 0.2231
Text-guided Depth-to-image (WebLI)
Sampling Steps Methods FID CLIP
250 steps ControlNet [53] 20.884 0.2910
4 steps ControlNet [53] 29.780 0.2854
+ DPM Solver++ [18] 32.208 0.2834
CM-II [44] 27.640 0.2869
GD-II [23] 26.51 0.2870
4 steps PE-CoDi (Ours) 23.047 0.2874
Table 2. Quantitative performance comparisons between the baselines and our methods. Our model can achieve comparable performance
in 4 steps than models sampled in 250 steps. The 4-step sampling results of our parameters-efficient distillation (PE-CoDi) is comparable
with the original 8-step sampling results, while PE-CoDi doesn’t sacrifice the original generative performance with frozen backbone.
5.1. Results
Baselines. We compare our method with two previous
SOTA diffusion distillation methods, i.e., consistency mod-
els (CM) [44] and guided-distillation (GD) [23]. We im-
plement CM with ControlNet without freezing denoisingU-Net, which leads to the same network architecture and
the same number of parameters as ours. For completeness,
we consider two different ways of applying the tested dis-
tillation techniques, by first making the model conditional
(fine-tuning first), or by first distilling the model and then
making it conditional (distill first). A summary of the tested
6

--- PAGE 7 ---
(a) Depth
 (b) ControlNet
 (c)CoDi (Ours)
Figure 5. Samples generated according to the depth image (left) from ControlNet sampled in 4 steps (middle), and ours from the uncondi-
tional pretraining sampled in 4 steps (right). Please see our supplement for many more examples.
Input IP2P (200 steps) CoDi (Ours) (1 step)
make it sunsetInput IP2P (200 steps) CoDi (Ours) (1 step)
make it long exposure
Input IP2P (200 steps) CoDi (Ours) (1 step)
make it low keyInput IP2P (200 steps) CoDi (Ours) (1 step)
make it sunny
Figure 6. Generated edited image according to the input image and the instruction (bottom) from Instructed Pix2Pix (IP2P) sampled in 200
steps and ours sampled in 1 step. Please see our supplement for many more examples.
configurations is shown in Table 1. Additionally, we com-
pare our method to recently introduced fast ODE solvers,
including DPM-Solver [17] and DPM-Solver++ [18].
Real-world super-resolution. We evaluate our method
on the challenging real-world super-resolution task,
where the degradation is simulated using Real-ESRGAN
pipeline [47]. Following StablSR [46], we compare all
methods on 3,000 randomly degraded image pairs. The
quantitative performance is shown in Table 2. The results
demonstrate that our distilled method leads to a signifi-
cant better performance than other distillation techniques.
Our method achieves better results than fine-tuned diffusion
models that requires 50 ×more sampling setps. Compared
with the distilled model by applying the guided-distillation,
our model outperforms it both quantitatively and qualita-
tively. The visual comparison presented in Figure. 4 also
demonstrates the superiority of our method.
Inpainting. Similar to the above super-resolution compar-
isons, we demonstrate our method on the inpainting taskthat conditioned on the masked image, as the quantitative
performance shown in Table 2. Similar to Palette [33], we
apply random masks into ImageNet data [32] for both train-
ing and testing. Note that we conduct experiments on the
up-scaled images in a 512×512resolution, which is differ-
ent than Palette in 256×256resolution. Even though we
evaluate their results in the same resoltuion, their number
can only be used for reference.
Depth-to-image generation. In order to demonstrate the
generality of our method on less informative conditions, we
apply our method in depth-to-image generation. The task
is usually conducted in parameter-efficient diffusion model
finetuning [24, 53], which can demonstrate the capability
of utilizing text-to-image generation priors. As Figure 5
illustrated, our distilled model from the unconditional pre-
training can effectively utilize the less informative condi-
tions and generate matched images with more details.
Instructed image editing. To demonstrate our conditional
distillation capability on text-to-image generation, here we
7

--- PAGE 8 ---
Methods Params FID LPIPS
LDMs 865M 29.266 0.3014
+ ControlNet 1.22B 28.951 0.3049
PE-CoDi (Ours) 364M 25.214 0.2941
CoDi (Ours) 1.22B 19.637 0.2656
- distilling PF-ODE 1.22B 20.307 0.2733
- noise-consistency 1.22B 25.728 0.3252
Table 3. Impact of the network architecture and con-
ditional distillation process, where all methods are
using the same 4-step sampling.
2 4 6 8
sampling steps0.30.40.5LPIPSrandom initialization
t2i pretraining
2 4 6 8
sampling steps255075100FIDrandom initialization
t2i pretraining
2 4 6 8
sampling steps0.300.35LPIPSmixed t
single t
2 4 6 8
sampling steps3040FIDmixed t
single t
2 4 6 8
sampling steps0.30.40.5LPIPSw/o. CG
w. CG
2 4 6 8
sampling steps20406080FIDw/o. CG
w. CGFigure 7. Ablations between alternative settings of our method.
apply our method on text-instructed image editing data [3]
and compare our conditional distilled model with the In-
structPix2Pix (IP2P) model. As the results shown in Fig-
ure 6, our single-step sampling result can achieve compara-
ble visual quality to 200 steps of the IP2P model. We ex-
perimentally find only small visual difference between the
results from our single-step sampling and the 200 steps sam-
pling. We believe this suggests that the effect of the condi-
tional guidance on distillation correlates with the similarity
between the conditions and the target data, further demon-
strating the effectiveness of our method.
5.2. Ablations
Here we compare the performance of the aforementioned
designs in our conditional distillation framework. Specifi-
cally we focus on the representative conditional generation
taski.e., real-world super-resolution [48] that conditions on
the low-resolution, noisy, blurry images.
Network architecture and distillation process. To elimi-
nate the impact of the architecture change, we compare our
method with a baseline given by adding a ControlNet mod-
ule trained on super-resolution without freezing the UNet.
As Table 3 shows, simply adopting a ControlNet mod-
ule for super-resolution has negligible impact on the per-
formance. To evaluate the proposed conditional diffusion
consistency, we removed the noise consistency term (equa-
tion 15) and employed the training model in the PF-ODE
instead of the frozen one as used in [44] formulation. As
shown in Table 3, adopting the distillation model PF-ODE
and noise-space consistency have positive effects on the fi-
nal results. These comparisons demonstrate the superiority
of our method without network architecture effects.
Pretraining. To validate the effectiveness of leveraging
pretraining in our model, we compare the results of random
initialization with initialization from the pre-trained text-to-
image model. As shown in Figure 7, our method outper-
forms the random initialized counterpart by a large margin,
thereby confirming that our strategy indeed utilizes the ad-
vantages of pretraining during distillation instead of simply
learning from scratch.
Sampling of zt.We empirically show that the way of sam-plingztplays a crucial role in the distillation learning pro-
cess. Compared with the previous protocol [23, 36] that
samples ztin different time tin a single batch, we show
that using a consistent time tacross different samples in a
single batch leads to a better performance in our targeted 1-
4 steps. As the comparisons shown in Figure 7, the model
trained with a single time t(in a single batch) achieves bet-
ter performance in both the visual quality ( i.e., FID) and the
accuracy ( i.e., LPIPS) when the number of evaluations is
increasing during inference.
Conditional guidance. In order to demonstrate the impor-
tance of our proposed conditional guidance (CG) for dis-
tillation, which is claimed to be capable of regularizing
the distillation process during training, we conduct compar-
isons between the setting of using the conditional guidance
asr=∥x−ˆxθ(zt, c)∥2
2and not using as r= 0. As the re-
sult shown in Figure 7, the conditional guidance improves
both the fidelity of the generated results and visual quality.
We further observed that the distillation process will con-
verge toward over-saturated direction without CG, which
thus lower the FID metric. In contrast, our model avoids
such a local minimum by using the proposed guidance loss.
6. Conclusion
We introduce a new framework for distilling an uncondi-
tional diffusion model into a conditional one that allows
sampling with very few steps. To the best of our knowledge,
this is the first method that distills the conditional diffusion
model from the unconditional pretraining in a single stage.
Compared with previous two-stage distillation and finetun-
ing techniques, our method leads to better quality given the
same number of (very few) sampling steps. Our method
also enables a new parameter-efficient distillation that al-
lows different distilled models, trained for different tasks,
to share most of their parameters. Only a few additional
parameters are needed for each different conditional gen-
eration task. We believe the method can serve as a strong
practical approach for accelerating large-scale conditional
diffusion models.
8

--- PAGE 9 ---
7. Acknowledgments.
The authors would like to thank our colleagues Keren Ye
and Chenyang Qi for reviewing the manuscript and pro-
viding valuable feedback. We also extend our gratitude to
Shlomi Fruchter, Kevin Murphy, Mohammad Babaeizadeh,
and Han Zhang for their instrumental contributions in fa-
cilitating the initial implementation of the latent diffusion
models.
References
[1] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit
Bermano. Hyperstyle: Stylegan inversion with hypernet-
works for real image editing. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR 2022, New
Orleans, LA, USA, June 18-24, 2022 , 2022. 3
[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-
image diffusion models with an ensemble of expert denois-
ers.ArXiv preprint , 2022. 4
[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023. 2, 5, 8
[4] Mauricio Delbracio and Peyman Milanfar. Inversion by di-
rect iteration: An alternative to denoising diffusion for image
restoration. Transactions on Machine Learning Research ,
2023. Featured Certification. 2
[5] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion
models beat gans on image synthesis. In Advances in Neu-
ral Information Processing Systems 34: Annual Conference
on Neural Information Processing Systems 2021, NeurIPS
2021, December 6-14, 2021, virtual , 2021. 3
[6] Tan M. Dinh, Anh Tuan Tran, Rang Nguyen, and Binh-Son
Hua. Hyperinverter: Improving stylegan inversion via hyper-
network. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition, CVPR 2022, New Orleans, LA, USA,
June 18-24, 2022 , 2022. 3
[7] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and
Joshua M Susskind. Boot: Data-free distillation of denoising
diffusion models with bootstrapping. In ICML 2023 Work-
shop on Structured Probabilistic Inference {\&}Generative
Modeling , 2023. 3
[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Informa-
tion Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual , 2020. 3
[9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for NLP. In Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA , 2019. 3
[10] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Elucidating the design space of diffusion-based generativemodels. Advances in Neural Information Processing Sys-
tems, 2022. 2, 4, 13
[11] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Mu-
rata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki
Mitsufuji, and Stefano Ermon. Consistency trajectory mod-
els: Learning probability flow ode trajectory of diffusion.
ArXiv preprint , 2023. 3
[12] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. Advances in neural infor-
mation processing systems , 2021. 3
[13] Diederik P. Kingma and Max Welling. Auto-encoding vari-
ational bayes. In 2nd International Conference on Learning
Representations, ICLR 2014, Banff, AB, Canada, April 14-
16, 2014, Conference Track Proceedings , 2014. 3
[14] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,
Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-
fusion: Text-to-image diffusion model on mobile devices
within two seconds. NeurIPS , 2023. 2, 12
[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV , 2014. 1
[16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
2023. 2
[17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion
probabilistic model sampling in around 10 steps. Advances
in Neural Information Processing Systems , 2022. 2, 6, 7
[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sam-
pling of diffusion probabilistic models. ArXiv preprint , 2022.
6, 7
[19] Haoye Lu, Yiwei Lu, Dihong Jiang, Spencer Ryan Szabados,
Sun Sun, and Yaoliang Yu. Cm-gan: Stabilizing gan train-
ing with consistency models. In ICML 2023 Workshop on
Structured Probabilistic Inference {\&}Generative Model-
ing, 2023. 3
[20] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and L Repaint Van Gool. Inpainting using
denoising diffusion probabilistic models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2022 . 6
[21] Eric Luhman and Troy Luhman. Knowledge distillation in
iterative generative models for improved sampling speed.
ArXiv preprint , 2021. 2
[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun
Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided im-
age synthesis and editing with stochastic differential equa-
tions. In The Tenth International Conference on Learn-
ing Representations, ICLR 2022, Virtual Event, April 25-29,
2022 , 2022. 1
[23] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik
Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
On distillation of guided diffusion models. In Proceedings
9

--- PAGE 10 ---
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , 2023. 2, 4, 6, 8
[24] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. ArXiv preprint , 2023. 3, 5, 7
[25] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In Proceedings
of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event , 2021. 4
[26] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun
Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image
translation. In ACM SIGGRAPH 2023 Conference Proceed-
ings, 2023. 2
[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. ArXiv preprint , 2022. 1
[28] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
Efficient parametrization of multi-domain deep neural net-
works. In 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018 , 2018. 3
[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , 2022. 1, 2, 4, 6
[30] Amir Rosenfeld and John K Tsotsos. Incremental learning
through deep adaptation. IEEE transactions on pattern anal-
ysis and machine intelligence , (3), 2018. 3
[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2023. 2
[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 2015. 7
[33] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In ACM
SIGGRAPH 2022 Conference Proceedings , 2022. 2, 6, 7
[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 2022. 1, 4
[35] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative refinement. IEEE Transactions on
Pattern Analysis and Machine Intelligence , (4), 2022. 2
[36] Tim Salimans and Jonathan Ho. Progressive distillation for
fast sampling of diffusion models. In The Tenth International
Conference on Learning Representations, ICLR 2022, Vir-
tual Event, April 25-29, 2022 , 2022. 2, 3, 4, 8[37] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs.
ArXiv preprint , 2021. 2
[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. Advances in Neural In-
formation Processing Systems , 2022. 1, 2
[39] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-
eswaranathan, and Surya Ganguli. Deep unsupervised
learning using nonequilibrium thermodynamics. In Pro-
ceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015 , 2015.
3
[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 , 2021. 3
[41] Yang Song and Prafulla Dhariwal. Improved techniques for
training consistency models. ArXiv preprint , 2023. 3
[42] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
Sliced score matching: A scalable approach to density and
score estimation. In Proceedings of the Thirty-Fifth Confer-
ence on Uncertainty in Artificial Intelligence, UAI 2019, Tel
Aviv, Israel, July 22-25, 2019 , 2019. 3
[43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In 9th International Conference on Learning Rep-
resentations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021 , 2021. 2, 3
[44] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. ICML , 2023. 2, 3, 4, 6,
8, 13
[45] Asa Cooper Stickland and Iain Murray. BERT and pals: Pro-
jected attention layers for efficient adaptation in multi-task
learning. In Proceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA , 2019. 3
[46] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK
Chan, and Chen Change Loy. Exploiting diffusion prior for
real-world image super-resolution. ArXiv preprint , 2023. 6,
7
[47] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Real-esrgan: Training real-world blind super-resolution with
pure synthetic data. In IEEE/CVF International Conference
on Computer Vision Workshops, ICCVW 2021, Montreal,
BC, Canada, October 11-17, 2021 , 2021. 7
[48] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan.
Realesrgan: Training real-world blind super-resolution with
pure synthetic data supplementary material. Computer Vi-
sion Foundation open access , 2022. 5, 6, 8
[49] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan
Saharia, Alexandros G Dimakis, and Peyman Milanfar. De-
blurring via stochastic refinement. In Proceedings of the
10

--- PAGE 11 ---
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2022. 2
[50] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xing-
long Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool.
Diffir: Efficient diffusion model for image restoration. ICCV ,
2023. 2, 6
[51] Yilun Xu, Mingyang Deng, Xiang Cheng, Yonglong Tian,
Ziming Liu, and Tommi Jaakkola. Restart sampling for im-
proving generative processes. ArXiv preprint , 2023. 2
[52] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.
Resshift: Efficient diffusion model for image super-
resolution by residual shifting. In Thirty-seventh Conference
on Neural Information Processing Systems , 2023. 2
[53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836–3847, 2023. 1, 2, 3, 4, 5, 6, 7
[54] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Aziz-
zadenesheli, and Anima Anandkumar. Fast sampling of dif-
fusion models via operator learning. In International Con-
ference on Machine Learning . PMLR, 2023. 2
11

--- PAGE 12 ---
A. Discussion
Limitations. We have shown image conditions benefit our distillation learning. However, the distillation learning depends
on the adapter architecture that introduces additional computation in our current framework. As a future work, we would
like to explore lightweight network architectures [14] in our distillation technique to further reduce the inference latency.
Nevertheless, CoDI’s significantly reduced sampling steps lead to lower latency. See the following table (measured in TPUv5)
for a detailed comparison:
Method CoDi (4step) ControlNet (4step) LDMs (4step) LDMs (50step)
Latency (ms) 107 ±3 107 ±3 103 ±2 977 ±1
Ethics statement. The diffusion distillation technique introduce in this work holds the promise of significantly enhancing
the practicality of diffusion models in everyday applications such as consumer photography and artistic creation. While we
are excited about the possibilities this model offers, we are also acutely aware of the possible risks and challenges associated
with its deployment. Our model’s ability to generate realistic scenes could be misused for generating deceptive content. We
encourage the research community and practitioners to prioritize privacy-preserving practices when using our method.
B. Proofs
B.1. Notations
We use ˆvθ(·,·)to denote a pre-trained diffusion model that learns the unconditional data distribution x∼pdatawith param-
etersθ. The signal prediction and the noise prediction transformed by equation 8 are denoted by ˆxθ(·,·)andˆϵθ(·,·), and they
share the same parameters θwithˆvθ(·,·).
B.2. Self-consistency in Noise Prediction
Remark. If a diffusion model, parameterized by ˆvθ(zt, t), satisfies the self-consistency property on the noise prediction
ˆϵθ(zt, t) =αtˆvθ(zt, t) +σtzt, then it also satisfies the self-consistency property on the signal prediction ˆxθ(zt, t) =αtzt−
σtˆvθ(zt, t).
Proof. The diffusion model that satisfies the self-consistency in the noise prediction implies:
ˆϵθ(zt′, t′) = ˆϵθ(zt, t),
αt′ˆvθ(zt′, t′) +σt′zt′=αtˆvθ(zt, t) +σtzt,
ˆvθ(zt′, t′) =αtˆvθ(zt, t) +σtzt−σt′zt′
αt′,(16)
Based on the above equivalence, the transformation between the signal prediction xθ(zt′, t′)andxθ(zt, t)by using the
update ruler in equation 7 and the reparameterization trick is:
xθ(zt′, t′) =αt′zt′−σt′ˆvθ(zt′, t′)
=αt′zt′−σt′αtˆvθ(zt, t) +σtzt−σt′zt′
αt′// integrating equation 16
=α2
t′zt′−σt′αtˆvθ(zt, t)−σt′σtzt+σ2
t′zt′
αt′
=(1−σ2
t′)zt′−σt′αtˆvθ(zt, t)−σt′σtzt+σ2
t′zt′
αt′
=zt′−σt′(αtˆvθ(zt, t) +σtzt)
αt′
=zt′−σt′(ˆϵθ(zt, t))
αt′// transformed with equation 8
=αt′xθ(zt, t) +σt′ˆϵθ(zt, t)−σt′(ˆϵθ(zt, t))
αt′// update ruler equation 9 of DDIM
=xθ(zt, t).
12

--- PAGE 13 ---
The derived equivalence shows that enforcing the self-consistency in the noise prediction, which is implemented by learning
to minimize our distillation loss in equation 15, enforces the self-consistency in the signal prediction and can distill the
pre-trained diffusion model.
C. Difference between Consisntecy Models
Algorithm 1 Conditional Diffusion Distillation (CDD)
Input: conditional data (x, c)∼pdata, adapted diffusion model ˆwθ(zt, c, t), learning rate η, distance functions dϵ(·,·)
anddx(·,·), and EMA γ
θ−←θ // target network initlization
repeat
Sample (x, c)∼pdataandt∼[∆t, T] // empirically ∆t= 1
Sample ϵ∼ N(0,I)
s←t−∆t
Sample zt←αtx+σtϵ
-ˆxt←αtzt−σtΦ(zt, c, t)
-ˆϵt←αtΦ(zt, c, t) +σtzt
+ˆxt←αtzt−σtˆwθ(zt, c, t) // signal prediction in equation 8
+ˆϵt←αtˆwθ(zt, c, t) +σtzt // noise prediction in equation 8
ˆzs←αsˆxt+σsˆϵt // update rule in equation 9
-ˆx′
t←αtwθ(zt, c, t) +σtzt
-ˆx′
s←αtwθ−(ˆzs, c, s) +σsˆzs
+ˆϵs←αswθ−(ˆzs, c, t) +σsˆzs // noise prediction in equation 8
-L(θ, θ−)←dx(ˆx′
t,ˆx′
s)
+L(θ, θ−)←dϵ(ˆϵt,ˆϵs) +dx(x,ˆxt)
θ←θ−η∇θL(θ,θ−)
θ−←stopgrad( γθ−+ (1−γ)θ) // exponential moving average
until convergence
D. Implementation Details
Skip Connections. We implement the skip connections as follows, which is same as the consistency models [44] and
EDMs [10] for satisfying the boundary condition but fϕcould be either the signal prediction or noise prediction:
f′
ϕ(zt, t) =cskip(t)x+cout(t)fϕ(zt, t), (17)
where
cskip(t) =σdata
t2+σ2
data, cout(t) =σdatatp
t2+σ2
data. (18)
We use σdata= 0.5.
13

--- PAGE 14 ---
E. Additional results
LR StableSR DiffIR LDMs (4 steps) GD-II (4 steps) CM-II (4 steps) CoDi (Ours) HR
Figure 8. Visual comparisons of various diffusion-based methods on the simulated real-world super-resolution benchmark. The input of
all methods is a ‘Bicubic’-upsampled image.
14

--- PAGE 15 ---
Input IP2P (200 steps) Ours (1 step) Ours (4 step)
make it sunny
make it sunset
Figure 9. Visual comparisons with the IP2P model and our conditional distilled model.
15

--- PAGE 16 ---
Input IP2P (200 steps) Ours (1 step) Ours (4 step)
make it long exposure
make it lowkey
Figure 10. Visual comparisons with the IP2P model and our conditional distilled model.
16
