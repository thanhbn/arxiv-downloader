# 2308.07891.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.07891.pdf
# Kích thước tệp: 13950932 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Liên Kết Ngữ Cảnh cho các LLM Đa Phương Thức
Yan Tai1, 2∗, Weichen Fan1∗†, Zhao Zhang1, Feng Zhu1, Rui Zhao1, Ziwei Liu3
1SenseTime Research
2Viện Tự Động Hóa, CAS
3S-Lab, Đại học Công nghệ Nanyang
Đây là hai hình ảnh: Hình đầu tiên 
là một RockFlock, và hình thứ hai 
là một Sheep. Bạn có thể cho tôi biết 
động vật nào có trong hai 
hình ảnh này không?
Hình ảnh đầu tiên là một con cừu đang 
đeo kính râm, trong khi hình thứ hai 
là một con cừu.Hình ảnh đầu tiên là một RockFlock, 
trong khi hình thứ hai là một Sheep.
Hình 1: Cuộc đối thoại demo của phương pháp học liên kết ngữ cảnh được đề xuất. Sau khi trình bày cho mô hình một cặp hình ảnh chưa từng thấy và các khái niệm mới, mô hình cải tiến của chúng tôi có được khả năng học và duy trì kiến thức thu được trong suốt cuộc trò chuyện trong khi các MLLM thông thường không thể đưa ra câu trả lời chính xác.

Tóm tắt
Khả năng học từ ngữ cảnh với các khái niệm mới và đưa ra phản hồi phù hợp là điều cần thiết trong các cuộc trò chuyện của con người. Mặc dù các Mô hình Ngôn ngữ Lớn Đa Phương thức (MLLM) và Mô hình Ngôn ngữ Lớn (LLM) hiện tại được huấn luyện trên các tập dữ liệu quy mô lớn, việc nhận dạng hình ảnh chưa từng thấy hoặc hiểu các khái niệm mới theo cách không cần huấn luyện vẫn là một thách thức. Học Trong Ngữ Cảnh (ICL) khám phá việc học few-shot không cần huấn luyện, nơi các mô hình được khuyến khích "học để học" từ các nhiệm vụ hạn chế và tổng quát hóa cho các nhiệm vụ chưa từng thấy. Trong công trình này, chúng tôi đề xuất học liên kết ngữ cảnh (LCL), nhấn mạnh "suy luận từ nguyên nhân và kết quả" để tăng cường khả năng học của MLLM. LCL vượt ra ngoài ICL truyền thống bằng cách tăng cường rõ ràng mối quan hệ nhân quả giữa tập hỗ trợ và tập truy vấn. Bằng cách cung cấp các minh chứng với liên kết nhân quả, LCL hướng dẫn mô hình phân biệt không chỉ sự tương tự mà còn các liên kết nhân quả cơ bản giữa các điểm dữ liệu, điều này trao quyền cho MLLM nhận dạng hình ảnh chưa từng thấy và hiểu các khái niệm mới hiệu quả hơn. Để tạo điều kiện cho việc đánh giá phương pháp mới này,
*Đóng góp Kỹ thuật Ngang bằng.
†Trưởng Dự án.chúng tôi giới thiệu tập dữ liệu ISEKAI, bao gồm hoàn toàn các cặp hình ảnh-nhãn được tạo ra chưa từng thấy được thiết kế cho việc học liên kết ngữ cảnh. Các thí nghiệm mở rộng cho thấy LCL-MLLM của chúng tôi thể hiện khả năng học liên kết ngữ cảnh mạnh mẽ đối với các khái niệm mới so với các MLLM thông thường. Mã và dữ liệu sẽ được phát hành tại https://github.com/isekai-portal/Link-Context-Learning.

1 Giới thiệu
(Trong tương lai gần, nhân loại cuối cùng có thể du hành liên sao và đến chòm sao Centaur.)
Con người và MLLM bước xuống tàu vũ trụ.
Con người: "Chúng ta đã đến rồi! Nhìn kìa! Người địa phương ở đây."
Người địa phương: Chào mừng, các bạn có thể gọi chúng tôi là 'RockFlock'.
MLLM: "Xin chào, cừu!"
Con người: " "

Cuộc trò chuyện ở trên giữa con người và MLLM phục vụ như một đại diện hài hước về cách MLLM khó khăn trong việc học từ minh chứng trong cuộc trò chuyện thực tế. 'RockFlock' là loài do chúng tôi tự tạo, sở hữu cảarXiv:2308.07891v1 [cs.CV] 15 Aug 2023

--- TRANG 2 ---
Minh chứngSuy luậnQ: Cái này là gì?A: Mèo.Q: Cái này là gì?A: Gấu trúc.Q: Cái này là gì?A: Hổ.Học trong ngữ cảnh
Minh chứngQ: Cái này là gì?A: Okapi.Q: Cái này là gì?A: Gấu trúc, không phải okapi.Q: Cái này là gì?A: Okapi.Học liên kết ngữ cảnhSuy luận
Liên quanKhông liên quanHình 2: Sự khác biệt giữa việc học liên kết ngữ cảnh của chúng tôi với học trong ngữ cảnh. Học trong ngữ cảnh bao gồm việc cung cấp các nhiệm vụ không liên quan cho minh chứng, trong khi có một mối quan hệ nhân quả trực tiếp giữa giai đoạn minh chứng và suy luận của học liên kết ngữ cảnh.

thân người và đầu cừu, như được hiển thị trong Hình 1. Các MLLM hiện tại không thể liên kết các cặp hình ảnh-nhãn chưa từng thấy để nhận dạng các đối tượng mới trong một cuộc trò chuyện duy nhất. Để giải quyết hạn chế này, việc trang bị cho mô hình khả năng học few-shot đã là một chủ đề lâu dài trong thị giác máy tính ngay cả trước kỷ nguyên MLLM. Phương pháp này cho phép mô hình học từ các ví dụ hạn chế và giảm thiểu vấn đề một cách hiệu quả.

Phương pháp chính cho MLLM học từ minh chứng được gọi là học trong ngữ cảnh, trong đó các mô hình cho thấy sự cải thiện đáng kể về các nhiệm vụ downstream sau khi được tiếp xúc với một vài cặp đầu vào-nhãn. Tuy nhiên, các MLLM hiện tại có lợi ích rất hạn chế từ học trong ngữ cảnh, vì trọng tâm chủ yếu là hướng dẫn mô hình có được khả năng xử lý các nhiệm vụ mới sau khi "học" từ các meta task. Tuy nhiên, hiệu suất của mô hình không bị ảnh hưởng ngay cả khi các câu trả lời được cung cấp trong meta-task đều sai. [1] Do đó, những gì MLLM đã "học" từ minh chứng vẫn là trả lời câu hỏi theo một định dạng cụ thể thay vì hiểu mối quan hệ nhân quả giữa các cặp hình ảnh-nhãn. Để cho phép MLLM tập trung nhiều hơn vào mối quan hệ nhân quả giữa các cặp hình ảnh và nhãn, phương pháp Frozen [2] gắn các nhãn khác nhau với các hình ảnh đã biết. Tuy nhiên, một thách thức đáng kể xuất hiện khi MLLM gặp phải các tình huống hoàn toàn mới nơi cả hình ảnh và nhãn đều chưa từng thấy. Trong những trường hợp như vậy, nhiệm vụ trích xuất nguyên nhân và kết quả cơ bản từ minh chứng và đưa ra dự đoán chính xác dựa trên kiến thức mới tìm thấy này vẫn là một câu đố chưa được giải quyết. 'RockFlock' (hình ảnh chưa từng thấy và khái niệm mới), được hiển thị trong Hình 1, sẽ bị nhận dạng sai bởi các phương pháp trước đó, trong khi mô hình của chúng tôi học khái niệm 'RockFlock' từ minh chứng và đưa ra phản hồi chính xác. Hơn nữa, việc thu nhận các khái niệm mới không cản trở kiến thức hiện có, cho phép mô hình phân biệt hiệu quả giữa các hình ảnh gốc và mới học.

Lấy cảm hứng từ học trong ngữ cảnh (sau đây gọi là ICL), chúng tôi đề xuất học liên kết ngữ cảnh (sau đây gọi là LCL), đòi hỏi MLLM phải có được kiến thức về các khái niệm mới từ cuộc trò chuyện và duy trì kiến thức hiện có của chúng để trả lời câu hỏi chính xác. Như được hiển thị trong Hình 2, học trong ngữ cảnh hiện tại trong MLLM nhấn mạnh lợi ích từ minh chứng không liên quan nhân quả. Tuy nhiên

wingeda
stripehopper
hornedelephant
thornbrawlers
aquarover
brawnger
mushroomhaven
shellfortress
lavasnake
pineapplepalace
rockflock
gildedfloatship
cactihog
airstone
blazefrost
skyquill0.00.20.40.60.81.0Độ chính xácPhương pháp của chúng tôi (7B)
OpenFlamingo (9B)
Otter (9B)Hình 3: Tổng quan kết quả trên một số danh mục của tập dữ liệu ISEKAI: Mô hình của chúng tôi vượt trội so với OpenFlamingo (9B) [6] và Otter (9B) [5] trên hầu như tất cả các danh mục, thể hiện hiệu suất vượt trội trong các tình huống liên quan đến hình ảnh hoàn toàn chưa từng thấy.

nhiên, đối với học liên kết ngữ cảnh, minh chứng và nhiệm vụ cuối cùng được liên kết nhân quả. (ví dụ: Nếu 'apple' được đổi tên thành 'orange' trong minh chứng, mô hình nên gọi apple là 'orange' trong quá trình suy luận.) Với khả năng này, MLLM có thể hỗ trợ học few-shot một cách linh hoạt.

Trong kỷ nguyên Mô hình Ngôn ngữ Lớn, việc đánh giá hiệu suất của mô hình trên học few-shot trở thành một thách thức, vì các mô hình này được huấn luyện rộng rãi trên lượng lớn dữ liệu thực tế. Để giải quyết vấn đề này và cung cấp một đánh giá toàn diện về học liên kết ngữ cảnh, chúng tôi giới thiệu tập dữ liệu ISEKAI. Tập dữ liệu này bao gồm các hình ảnh và khái niệm chưa từng thấy, hoàn toàn mới đối với MLLM, vì chúng vượt ra ngoài ranh giới của chủ nghĩa hiện thực. Tất cả hình ảnh trong tập dữ liệu được tạo bởi Stable Diffusion [3] và Midjourney [4], trong khi tất cả các nhãn hoặc khái niệm cũng được chế tạo. Hình 3 hiển thị so sánh giữa mô hình của chúng tôi và Otter [5], OpenFlamingo [6] trên tập dữ liệu ISEKAI.

Trong bài báo này, chúng tôi trình bày học liên kết ngữ cảnh (LCL), một cài đặt ban cho MLLM khả năng hiểu mối quan hệ nhân quả tiềm tàng trong cuộc trò chuyện và xử lý hình ảnh và khái niệm chưa từng thấy. Không như ICL chủ yếu tập trung vào việc truyền cảm hứng cho mô hình với nhiều loại nhiệm vụ khác nhau, LCL tiến thêm một bước bằng cách trao quyền cho mô hình thiết lập ánh xạ giữa nguồn và đích, từ đó nâng cao hiệu suất tổng thể. Các đóng góp của công trình này có thể được tóm tắt như sau:

•Học Liên kết Ngữ cảnh: Chúng tôi giới thiệu một cài đặt học few-shot liên quan nhân quả mới, nơi MLLM được thách thức để đồng hóa các khái niệm mới từ cuộc trò chuyện đang diễn ra và duy trì kiến thức này để trả lời câu hỏi chính xác. Dưới học liên kết ngữ cảnh, chúng tôi trao quyền cho MLLM nắm bắt mối quan hệ nhân quả giữa nguồn và đích từ minh chứng.

•Tập dữ liệu ISEKAI: Vì hầu hết dữ liệu thế giới thực không hoàn toàn chưa từng thấy đối với MLLM, chúng tôi phát hành một tập dữ liệu chế tạo thách thức cho công chúng, nơi các cặp hình ảnh-khái niệm mới được giới thiệu, để đánh giá hiệu suất của MLLM.

--- TRANG 3 ---
mance.

2 Các Công trình Liên quan
Các Mô hình Ngôn ngữ Lớn Đa Phương thức [7–11] đã thể hiện khả năng đáng kể trong các nhiệm vụ tạo sinh hoặc nhận dạng phổ quát. Theo mô hình mới của MLLM, các nhiệm vụ thị giác khác nhau có thể được thực hiện theo cách zero-shot không cần huấn luyện [12, 13], thoát khỏi quá trình pretrain-và-finetune nặng nề. Tuy nhiên, nhận dạng nội dung tùy ý thông qua một mô hình duy nhất thường được coi là cực kỳ khó khăn. Cách nâng cao khả năng nhận dạng của MLLM trong tự nhiên với chi phí thấp đã nổi lên như một trọng tâm nghiên cứu gần đây.

Điều chỉnh Prompt Đa Phương thức Điều chỉnh Prompt Đa Phương thức (M-PT) thường được sử dụng trong các mô hình lớn đa phương thức dựa trên học tương phản, như CLIP [12]. Trong quá trình huấn luyện, điều chỉnh prompt thường đóng băng hầu hết các tham số của mô hình và chỉ cập nhật một số lượng nhỏ tham số để đạt được kết quả tương tự như fine-tuning [14–17]. PT [14] thêm các embedding prompt có thể điều chỉnh vào mỗi lớp của encoder và decoder, chỉ các trọng số của các embedding được thêm sẽ được cập nhật trong quá trình huấn luyện. VPT [18] thêm một tập hợp các tham số có thể học ở các vị trí cụ thể để điều chỉnh mô hình. CoOp [15] và UPT [19] sử dụng CLIP như backbone và prompted nó để phù hợp với cài đặt few-shot. CoCoOp [16], POMP [20] và MaPLe [21] mở rộng điều chỉnh prompt cho các nhiệm vụ nhận dạng thị giác từ vựng mở. Tuy nhiên, các phương pháp điều chỉnh prompt truyền thống không phù hợp với các mô hình ngôn ngữ lớn đa phương thức sinh ra mạnh mẽ.

Điều chỉnh Hướng dẫn Đa Phương thức Điều chỉnh Hướng dẫn Đa Phương thức (M-IT) nâng cao khả năng zero-shot của MLLM trong các nhiệm vụ chưa từng thấy bằng cách fine-tune chúng trên tập dữ liệu dựa trên mô tả hướng dẫn [7, 8, 11, 22, 23]. MiniGPT-4 [24] và LLaVA [11] giữ encoder thị giác đóng băng và điều chỉnh mô hình ngôn ngữ, mở rộng điều chỉnh hướng dẫn cho đa phương thức. mPLUG-Owl [25] điều chỉnh encoder thị giác và văn bản riêng biệt trong hai giai đoạn, và đề xuất một tập dữ liệu đánh giá để đánh giá điều chỉnh hướng dẫn liên quan đến thị giác. InstructBLIP [26] nâng cao khả năng zero-shot bằng cách thực hiện điều chỉnh hướng dẫn trên nhiều tập dữ liệu. Shikra [27] và Kosmos-2 [28] mở rộng MLLM cho các nhiệm vụ grounding thị giác sử dụng hướng dẫn với tọa độ bounding box. Mặc dù các nghiên cứu này thể hiện khả năng zero-shot xuất sắc, chúng vẫn không thể nhận dạng các lớp không được thấy trong quá trình huấn luyện mô hình.

Học Trong Ngữ Cảnh Đa Phương thức Các Mô hình Ngôn ngữ Lớn (LLM) đã cho thấy khả năng xuất sắc trong việc học từ các mẫu ngữ cảnh. Trong cài đặt Học Trong Ngữ Cảnh Đa Phương thức (M-ICL), theo các mẫu hình ảnh đầu vào và hướng dẫn tùy chọn, MLLM có thể học các mẫu nhiệm vụ mới theo cách few-shot [29–32]. Flamingo [33] đưa học trong ngữ cảnh vào xem xét trong quá trình pretrain, cho phép mô hình sở hữu khả năng hỗ trợ học trong ngữ cảnh. Otter [5] theo Flamingo và đề xuất một tập dữ liệu học trong ngữ cảnh mới, tiến hành với khả năng ICL trong giai đoạn điều chỉnh hướng dẫn.

Khác với các phương pháp trước đó, học liên kết ngữ cảnh được đề xuất của chúng tôi có thể thiết lập liên kết nhân quả giữa tập hỗ trợ và tập truy vấn. Cụ thể, sử dụng hình ảnh cụ thể lớp few-shot và các prompt văn bản, LCL có thể liên kết prompt và các mẫu suy luận, và thậm chí liên kết các hình ảnh chưa từng thấy trước đó với các khái niệm mới.

3 Học Liên kết Ngữ cảnh
Trong phần này, trước tiên chúng tôi đưa ra một giới thiệu ngắn gọn về học trong ngữ cảnh và tiết lộ các hạn chế chính và sự khác biệt với học liên kết ngữ cảnh của chúng tôi trong Preliminary; tiếp theo, chúng tôi mang sức mạnh của học liên kết ngữ cảnh vào MLLM trong Bring Link-Context Learning to MLLMs.

3.1 Sơ bộ
Học Trong Ngữ Cảnh Chính thức, học trong ngữ cảnh [34] đề cập đến: mô hình nên chọn câu trả lời với điểm dự đoán cao nhất từ một tập các câu trả lời ứng viên Y={y1, y2, ..., yn}, cho một đầu vào truy vấn x, có điều kiện trên một tập hỗ trợ S, bao gồm nhiều cặp đầu vào-nhãn từ nhiều loại nhiệm vụ khác nhau, trong đó S={(x1, y1),(x2, y2), ...,(xn, yn)}. (Truy vấn và mẫu của S nên thuộc về các nhiệm vụ khác nhau.)

Từ một góc độ khác, học trong ngữ cảnh có thể được biểu thị như học few-shot không cần huấn luyện, vì nó biến đổi giai đoạn huấn luyện của học few-shot thành đầu vào minh chứng cho Mô hình Ngôn ngữ Lớn. Lưu ý rằng ICL [34] phù hợp với FSL, trong đó các nhiệm vụ trong giai đoạn minh chứng (huấn luyện) và trong giai đoạn suy luận (truy vấn) là khác nhau.

Học Liên kết Ngữ cảnh Về bản chất, học liên kết ngữ cảnh (LCL) đại diện cho một hình thức học few-shot không cần huấn luyện và liên kết nhân quả. Trong phương pháp này, một tập hỗ trợ S=(x1, y1),(x2, y2), ...,(xn, yn) được cung cấp, cùng với một mẫu truy vấn x từ tập truy vấn Q, trong đó các cặp dữ liệu từ tập hỗ trợ được liên kết nhân quả với tập truy vấn. Mô hình được giao nhiệm vụ dự đoán câu trả lời dựa trên mối quan hệ liên kết nhân quả giữa truy vấn và tập hỗ trợ.

Để cung cấp sự rõ ràng hơn, học liên kết ngữ cảnh tăng cường đáng kể mối quan hệ nhân quả giữa tập hỗ trợ và tập truy vấn. Ví dụ: 1). Quy tắc số học mới: Trong tình huống này, tập hỗ trợ bao gồm các biểu thức số học như (1<op> 2 = 3), (2<op> 3 = 5), với mẫu truy vấn là 4<op> 5 =?. Ở đây, "<op>" đại diện cho một quy tắc số học mới mà chúng tôi muốn dạy mô hình thông qua minh chứng; 2). Phân loại hình ảnh mới: Trong trường hợp này, tập hỗ trợ chứa các cặp như (<hình ảnh chưa thấy>:<lớp mới A>), (<hình ảnh chưa thấy>:<lớp mới B>), trong khi mẫu truy vấn là (<hình ảnh chưa thấy> thuộc về?). Ví dụ này thể hiện cách chúng tôi mong đợi mô hình phân loại chính xác hình ảnh chưa từng thấy vào một trong các lớp mới được chỉ định dựa trên minh chứng.

Về bản chất, học liên kết ngữ cảnh nâng cao khả năng của mô hình nắm bắt các khái niệm và mối quan hệ mới bằng cách hiệu quả thiết lập liên kết nhân quả giữa tập hỗ trợ và

--- TRANG 4 ---
tập truy vấn. Trong khi cài đặt này có thể áp dụng cho cả LLM và MLLM, trọng tâm chính của chúng tôi trong bài báo này là việc áp dụng học liên kết ngữ cảnh cụ thể trong MLLM. Bằng cách tập trung vào MLLM, chúng tôi nhằm thể hiện tiềm năng của phương pháp này trong các mô hình đa phương thức và ý nghĩa của nó đối với việc thúc đẩy khả năng học của chúng.

3.2 Mang Học Liên kết Ngữ cảnh đến MLLM
Trong phần này, mục tiêu chính của chúng tôi là giới thiệu Học Liên kết Ngữ cảnh (LCL) vào lĩnh vực MLLM. Nhận ra rằng các MLLM hiện tại được huấn luyện theo cách ICL có thể không xuất sắc trong các nhiệm vụ LCL, chúng tôi đề xuất một chiến lược huấn luyện mới để fine-tune MLLM. Phương pháp này nhằm trang bị cho các mô hình khả năng nắm bắt các liên kết nhân quả từ ngữ cảnh một cách hiệu quả. Bằng cách tận dụng chiến lược huấn luyện mới này, chúng tôi nhằm trao quyền cho MLLM để xuất sắc trong các nhiệm vụ đòi hỏi suy luận và hiểu các mối quan hệ nhân quả, từ đó mở rộng phạm vi khả năng của chúng và cải thiện hiệu suất tổng thể. Để cụ thể hơn, chúng tôi chọn Shikra [27] như baseline, và chúng tôi chia ImageNet1k thành ImageNet-900 và ImageNet-100 theo lớp, điều này sẽ được thảo luận chi tiết trong Training Dataset. Ngoài ra, chúng tôi kết hợp khái niệm học tương phản trong chiến lược huấn luyện của chúng tôi, như được thảo luận trong Training Strategy. Điều này giúp hướng dẫn mô hình hiểu các đặc điểm chung giữa các mẫu cùng loại và sự khác biệt giữa các mẫu khác loại.

3.2.1 Tập dữ liệu Huấn luyện
Không như các nhiệm vụ truyền thống đòi hỏi dữ liệu huấn luyện rộng rãi, LCL tập trung vào việc có được khả năng tìm liên kết giữa các cặp nguồn-đích trong minh chứng và tổng quát hóa cho các mẫu truy vấn. Do đó, đại diện đầy đủ của các danh mục hình ảnh đa dạng là cần thiết để cho phép MLLM nắm bắt các mối quan hệ nhân quả một cách hiệu quả và hiệu suất.

ImageNet1k [35] thường được sử dụng cho các nhiệm vụ phân loại hình ảnh, và thông thường là huấn luyện mô hình trên toàn bộ tập dữ liệu để nâng cao khả năng nhận dạng của chúng trên tất cả các danh mục. Ngược lại, trong cấu hình huấn luyện của LCL, chúng tôi chỉ chọn một số lượng hạn chế mẫu ngẫu nhiên từ mỗi danh mục. Sau đó chúng tôi sắp xếp một tập hợp các danh mục liên quan với độ tương tự giảm dần cho mỗi danh mục, được gọi là "neighbors". Cụ thể, chúng tôi áp dụng CLIP [12] để tính toán độ tương tự giữa các lớp khác nhau trong tập dữ liệu huấn luyện. Đầu tiên, chúng tôi chọn ngẫu nhiên 100 hình ảnh từ mỗi lớp và tính toán đặc trưng hình ảnh trung bình cho mỗi lớp. Sau đó, chúng tôi mã hóa tên văn bản của tất cả các lớp để có được các vector đặc trưng tương ứng. Cuối cùng, chúng tôi tính toán độ tương tự có trọng số trên các cặp lớp khác biệt, bao gồm tương quan hình ảnh-đến-hình ảnh, hình ảnh-đến-văn bản và văn bản-đến-văn bản. Đối với một danh mục cụ thể, chúng tôi sắp xếp tất cả các danh mục khác dựa trên độ tương tự và chia chúng thành N khoảng. Sau đó, trong mỗi khoảng, chúng tôi chọn ngẫu nhiên các danh mục để xây dựng một tập hợp "neighbors" với tổng số lượng là N.

3.2.2 Chiến lược Huấn luyện
Để làm cho MLLM hiểu liên kết nhân quả giữa tập hỗ trợ và mẫu truy vấn, cũng như mối quan hệ nhân quả giữa các cặp đầu vào-nhãn trong tập hỗ trợ, chúng tôi xây dựng các cặp dương-âm để thúc giục mô hình học từ so sánh. Gọi tập hỗ trợ được biểu thị là S={s1, s2, ..., sn}. Dựa trên tương quan giữa các mẫu của nó, chúng tôi có thể định nghĩa lại tập hỗ trợ là C={c1, c2, ..., cm}, trong đó mỗi cm phục vụ như một nguyên mẫu đại diện cho một cụm mẫu từ S. Các nguyên mẫu này nắm bắt các mối quan hệ và tương tự cần thiết giữa các mẫu trong S. Cho truy vấn x, chúng tôi huấn luyện θ để tối đa hóa khả năng:

logpθ(y|x) = Σl logpθ(yl|x, C, y1, y2, ..., yl−1), (1)

trong đó θ biểu thị các tham số của mô hình ngôn ngữ. Các tham số của encoder thị giác được đóng băng trong quá trình huấn luyện.

[2-way] strategy: Trong chiến lược này, chúng tôi huấn luyện MLLM cho phân loại hình ảnh nhị phân, trong đó C={c1, c2}. Để cụ thể hơn, c1 và c2 ở đây đại diện cho nguyên mẫu của hai lớp. Chúng tôi biểu thị tập lớp huấn luyện là T={t1, t2, ..., t100}, chúng tôi lấy mẫu ngẫu nhiên một lớp ti như lớp dương, trong đó tập lớp neighbor Nti={nti1, nti2, ..., nti100}(nti1 là lớp tương tự nhất với ti, trong khi nti100 là ít nhất). Sau đó chúng tôi áp dụng chiến lược hard-negative mining, trong đó chúng tôi lấy mẫu lớp âm ntij từ Nti với xác suất pj=101−j/Σ100m=1m. Lưu ý rằng cài đặt này được cố định để huấn luyện trên 16 shots.

[2-way-random] strategy: Trong chiến lược này, chúng tôi đầu tiên huấn luyện MLLM trên fixed-16 shots theo chiến lược [2-way], sau đó tiếp tục huấn luyện mô hình với shots được lấy mẫu trung bình từ 2-16 shots trong 10 epochs.

[2-way-weight] strategy: Trong chiến lược này, chúng tôi ban đầu huấn luyện MLLM sử dụng chế độ fixed-16 shot, tuân thủ phương pháp [2-way]. Sau đó, chúng tôi tinh chỉnh mô hình bằng huấn luyện bổ sung với shots được lấy mẫu từ phạm vi 2-16, với xác suất của mỗi shot được biểu thị là pj=ej/Σ16m=2em.

[mix] strategy: Để nâng cao khả năng tổng quát hóa của mô hình, chúng tôi thực hiện quá trình fine-tuning bao gồm cả nhiệm vụ [2-way] và các nhiệm vụ gốc của Shikra [27]. Trong mỗi lần lặp, các mẫu huấn luyện được lấy mẫu đều từ cả nhiệm vụ [2-way] và các nhiệm vụ gốc. Phương pháp cân bằng này đảm bảo rằng mô hình có được thành thạo trong cả các nhiệm vụ học liên kết ngữ cảnh mới được giới thiệu và các nhiệm vụ đã tồn tại trước từ Shikra [27].

4 Tập dữ liệu ISEKAI
Để đánh giá khách quan khả năng của MLLM học các khái niệm mới thông qua LCL, chúng tôi tạo ra tập dữ liệu ISEKAI, được hiển thị trong Hình 4. Các khái niệm liên quan là không thực, hiếm khi được thấy trong truyền thuyết, thần thoại, hoặc phương tiện truyền thông hư cấu. Do đó, việc tiếp xúc của MLLM với các khái niệm này là tối thiểu. Thuật ngữ "Isekai" có nguồn gốc

--- TRANG 5 ---
Thế giới ISEKAIThế giới Thực
ShellfortressPineapplePalaceOctoVacCrystalStagAirStoneCactihogHình 4: Tổng quan về Tập dữ liệu ISEKAI: Tập dữ liệu này bao gồm hoàn toàn các hình ảnh được tạo ra, trong đó các hình ảnh từ "Thế giới ISEKAI" không tồn tại trong đời thực, trong khi các hình ảnh từ "Thế giới Thực" có nguồn gốc từ thực tế.

từ một thể loại phụ giả tưởng trong anime. Cốt truyện thường liên quan đến các nhân vật được vận chuyển đến một thế giới khác, như một cõi giả tưởng hoặc vũ trụ ảo. Khán giả hiểu thế giới mới dần dần thông qua việc khám phá của nhân vật chính, tương tự như hành trình của MLLM vào một lĩnh vực kiến thức mới.

Các hình ảnh của tập dữ liệu được tạo bởi mô hình text-to-image của Midjourney [4] sử dụng các hướng dẫn được chế tác cẩn thận. Hình ảnh được chọn thủ công để đảm bảo tính nhất quán khái niệm cốt lõi. Tập dữ liệu hiện tại bao gồm 20 nhóm, và 40 danh mục tổng cộng (tiếp tục phát triển). Mỗi nhóm ghép một khái niệm mới với một khái niệm thế giới thực liên quan, như "octopus vacuum" và "octopus." Chúng có thể phục vụ như các mẫu âm thách thức cho nhau. Mỗi khái niệm có không dưới 32 hình ảnh, hỗ trợ các ví dụ multi-shot. Những đặc điểm này cho phép ISEKAI đánh giá toàn diện khả năng LCL của mô hình. Chúng tôi cũng cung cấp mô tả văn bản về ngoại hình và tên của mỗi khái niệm, đóng góp vào các đánh giá ngoài LCL.

Trong bài báo này, chúng tôi đánh giá hiệu suất của các mô hình khác nhau trên ISEKAI. Để biết chi tiết, tham khảo Results on ISEKAI.

5 Thí nghiệm
Trong phần này, chúng tôi trình bày kết quả thí nghiệm để thể hiện tính hiệu quả của phương pháp được đề xuất. Chúng tôi tiến hành so sánh toàn diện giữa phương pháp của chúng tôi (dựa trên học liên kết ngữ cảnh) và các MLLM dựa trên học trong ngữ cảnh khác.

5.1 Kết quả trên ISEKAI
Để đánh giá định lượng hiệu suất của học liên kết ngữ cảnh, chúng tôi so sánh các phương pháp của chúng tôi trong các chiến lược khác nhau với baseline (Shikra [27]) cũng như các phương pháp ICL (Otter và OpenFlamingo) trong hai tập dữ liệu thách thức: ISEKAI-10 và ISEKAI-pair.

Đánh giá ISEKAI-10: Bao gồm 10 lớp các cặp hình ảnh dương-âm thách thức, ISEKAI-10 trình bày một tình huống nơi lớp dương hoàn toàn không tồn tại trong thế giới thực nhưng chia sẻ một số đặc điểm với lớp âm, bao gồm động vật hoặc đối tượng phổ biến từ thực tế của chúng ta. Phần trên của Bảng 1 thể hiện kết quả trên tập dữ liệu ISEKAI-10, nơi vanilla-shikra [27] gặp khó khăn. Mô hình của chúng tôi thể hiện hiệu suất cạnh tranh so với OpenFlamingo [6] và Otter [5] trên tất cả số lượng shot.

Đánh giá ISEKAI-pair: Trong đánh giá ISEKAI-pair, các cặp dương và âm được xây dựng sử dụng tất cả danh mục hình ảnh

--- TRANG 6 ---
Cài đặt Phương pháp 2-shot 4-shot 6-shot 8-shot 10-shot 12-shot 14-shot 16-shot
ISEKAI-10OpenFlamingo [6] 0.46 0.44 0.46 0.48 0.50 0.50 0.48 0.46
Otter [5] 0.23 0.23 0.19 0.15 0.14 0.12 0.10 0.07
Vanilla-Shikra [27] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Ours-[2-way-random] 0.64 0.63 0.65 0.62 0.61 0.57 0.56 0.56
Ours-[mix] 0.68 0.70 0.73 0.69 0.63 0.62 0.65 0.62
ISEKAI-pairOpenFlamingo [6] 0.19 0.34 0.38 0.39 0.41 0.40 0.40 0.40
Otter [5] 0.01 0.04 0.04 0.03 0.03 0.02 0.02 0.01
Vanilla-Shikra [27] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Ours-[mix] 0.39 0.38 0.38 0.40 0.40 0.39 0.37 0.35
Ours-[2-way-random] 0.43 0.46 0.47 0.48 0.48 0.49 0.49 0.49

Bảng 1: Đánh giá định lượng trên ISEKAI từ zero-shot đến 16-shot, đo bằng độ chính xác. Chúng tôi đạt được kết quả tốt nhất so với Otter [5] và OpenFlamingo [6].

Phương pháp zero-shot 2-shot 4-shot 6-shot 8-shot 10-shot 12-shot 14-shot 16-shot
OpenFlamingo [6] 0.00 0.41 0.62 0.72 0.75 0.77 0.78 0.73 0.72
Otter [5] 0.13 0.18 0.21 0.24 0.25 0.26 0.24 0.23 0.23
Vanilla-Shikra [27] 0.05 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Ours-[mix] 0.16 0.73 0.78 0.83 0.73 0.71 0.72 0.65 0.57
Ours-[2-way] 0.02 0.51 0.61 0.68 0.73 0.77 0.78 0.78 0.79
Ours-[2-way-random] 0.0 0.77 0.78 0.77 0.79 0.77 0.77 0.77 0.75
Ours-[2-way-weight] 0.0 0.69 0.71 0.72 0.76 0.77 0.78 0.78 0.79

Bảng 2: Đánh giá định lượng trên ImageNet-100 từ zero-shot đến 16-shot, đo bằng độ chính xác. Chúng tôi đạt được kết quả tốt nhất so với Otter [5] và OpenFlamingo [6].

không tồn tại trong thế giới thực. Mỗi hình ảnh riêng lẻ được ghép với tất cả hình ảnh từ các danh mục khác, tạo điều kiện cho một đánh giá toàn diện. Đánh giá này cung cấp một thước đo thực tế về khả năng của mô hình xử lý những điều hoàn toàn không biết thông qua các kết hợp khác nhau. Phần dưới của Bảng 1 nhấn mạnh sự vượt trội của mô hình chúng tôi so với OpenFlamingo [6] và Otter [5] trong bối cảnh này.

Kết quả Định tính: Hình 1 cung cấp so sánh trực quan giữa mô hình của chúng tôi và OpenFlamingo [6], cũng như Otter [5]. Đáng chú ý, mô hình của chúng tôi thể hiện thành thạo trong việc hiểu chính xác các khái niệm mới và phân biệt hiệu quả các đối tượng không quen thuộc khỏi những đối tượng có sự giống nhau gần gũi. Quan sát này nhấn mạnh khả năng của mô hình chúng tôi nắm bắt mối quan hệ nhân quả giữa nguồn và đích từ minh chứng.

5.2 Kết quả trên ImageNet-100
Chúng tôi tiến hành đánh giá hiệu suất mô hình trên ImageNet-100, bao gồm 100 lớp hoàn toàn vắng mặt khỏi giai đoạn huấn luyện. Kết quả nhấn mạnh hiệu quả của chiến lược mix, đạt được độ chính xác cao nhất 83% ở 6-shot. Ngược lại, Otter đạt được độ chính xác đỉnh 25%, và hiệu suất của OpenFlamingo đạt 78%.
Không như tập dữ liệu ISEKAI, các hình ảnh từ ImageNet-100 tương ứng với các thực thể thế giới thực.

5.3 Nghiên cứu Ablation
Ánh xạ đầu vào-nhãn ground-truth có tồn tại không?
Chúng tôi tiến hành phân tích ablation về tính đúng đắn của nhãn trong minh chứng (tập hỗ trợ). Cho một tập các miền hình ảnh Xc∈RH×W×3 và miền nhãn C∈RN, một ánh xạ f:Xc→C tồn tại để liên kết mỗi hình ảnh với nhãn tương ứng. Chúng tôi sử dụng một số cặp hình ảnh-nhãn {(x1c1, c1),(x2c1, c1), ...,(xnc1, c1)}, trong đó xjci∈Xci, như tập hỗ trợ. Mô hình sẽ dự đoán câu trả lời đúng từ một tập ứng viên Y:

ŷ = arg max yi∈Y P(yi|x, f), (2)

trong đó dự đoán có điều kiện trên ánh xạ f.

Do đó, việc cố ý phá vỡ mối quan hệ ánh xạ trong tập hỗ trợ sẽ dẫn mô hình đưa ra câu trả lời không chính xác, vì nó dựa vào nhiều vào liên kết chính xác giữa các cặp hình ảnh-nhãn của tập hỗ trợ để đưa ra dự đoán chính xác. Như được hiển thị trong Hình 7, chúng tôi làm rối ánh xạ f bằng cách dần dần chèn nhãn sai vào tập hỗ trợ, và độ chính xác giảm từ 0.78 xuống 0.00 khi tính đúng đắn của nhãn giảm từ 100% xuống 0%. Những kết quả này rõ ràng cho thấy việc duy trì liên kết chính xác giữa các cặp hình ảnh-nhãn trong tập hỗ trợ đóng một vai trò quan trọng trong học liên kết ngữ cảnh.

Mô hình có được lợi ích từ việc sử dụng shot lớn hơn không?

--- TRANG 7 ---
Người dùng: Có gì trong hình ảnh?Trả lời: Cactihog.Trả lời: Hedgehog.Người dùng: Có gì trong hình ảnh?

Người dùng: Có gì trong hình ảnh?Người dùng: Có gì trong hình ảnh?Chúng tôi: Cactihog.Chúng tôi: Hedgehog.OpenFlamingo: Một hình ảnh của xương rồng.OpenFlamingo: Một hình ảnh của nhím.

Otter: Hình ảnh này là một động vật nhỏ, màu xanh lá và dễ thương.Otter: Một con nhím trong cánh đồng hoa.Tập hỗ trợTập truy vấn

Người dùng: Có gì trong hình ảnh?Trả lời: Mushroomhaven.Trả lời: Mushroom.Người dùng: Có gì trong hình ảnh?

Người dùng: Có gì trong hình ảnh?Người dùng: Có gì trong hình ảnh?Chúng tôi: Mushroomhaven.Chúng tôi: Mushroom.OpenFlamingo: Một hình ảnh của ngôi nhà nấm.OpenFlamingo: Một hình ảnh của nấm.Otter: Một cây nấm với mái màu xanh dương và một cánh cửa.Otter: Một cây nấm với giọt nước trên đó.Hình 5: So sánh định tính kết quả hiểu hình ảnh mới giữa chúng tôi và OpenFlamingo [6], Otter [5].
Tên "Cactihog" là sự kết hợp của "cactus" và "hedgehog", kết hợp các đặc điểm chính của hai sinh vật này. Tên "MushroomHaven" gợi ý một nơi ở được đặc trưng bởi những cây nấm khổng lồ

Giống như học có giám sát, độ chính xác của mô hình trải qua tăng trưởng ban đầu nhanh chóng với lượng dữ liệu huấn luyện tăng, cuối cùng đạt đến một cao nguyên. Trong giai đoạn này, việc lựa chọn các mẫu đại diện hơn trở nên quan trọng. Hình 6 trình bày hai kết quả: một mô tả độ chính xác mô hình từ huấn luyện riêng biệt ở shot cố định (thanh xám trong hình), trong khi cái khác thể hiện hiệu suất của mô hình thông qua lấy mẫu trên các shot khác nhau (đường đỏ trong hình). Kết quả tiết lộ những lợi ích nhỏ từ huấn luyện fixed-shot thấp hơn và hiệu suất nhất quán từ huấn luyện random-shot. Đáng chú ý, trong cả cài đặt ngẫu nhiên và cố định, độ chính xác cao nguyên hoặc trải qua tăng trưởng dần dần sau ngưỡng 8-shot.

Việc ra quyết định của mô hình trong trường hợp multi-shot phụ thuộc vào gì?
Như được hiển thị trong Hình 8, khi làm rối nhãn của các vị trí khác nhau, độ chính xác của mô hình với 16-shot giảm khác nhau, phản ánh mức độ mà mô hình ưa thích các vị trí khác nhau. Chúng tôi quan sát thấy mô hình dựa vào nhiều vào vị trí đầu và giữa. Từ một khía cạnh khác, nó cung cấp lời giải thích tại sao mô hình gặp cao nguyên ở số lượng shot cao hơn. Tương tự, hiện tượng này cũng tồn tại trong LLM [36], nơi mô hình ngôn ngữ có xu hướng "bị lạc ở giữa" khi xử lý ngữ cảnh dài. Họ cũng tiết lộ rằng hiệu suất của mô hình tiếp tục giảm khi ngữ cảnh phát triển dài hơn.

Sự khác biệt giữa các chiến lược huấn luyện khác nhau là gì?
Bảng 2 trình bày cái nhìn toàn diện về kết quả đạt được thông qua bốn chiến lược huấn luyện khác biệt của chúng tôi. Chiến lược mix nổi bật bằng cách nâng độ chính xác zero-shot từ 5% lên 16% và đạt được độ chính xác đáng kể 83% ở 6-shot; tuy nhiên, hiệu suất của nó giảm xuống 57% ở 16-shot. Ngược lại, chiến lược 2-way, được neo ở huấn luyện 16-shot, khởi đầu với độ chính xác 51% ở 2-shot và tiến triển lên 79% ở 16-shot. Thú vị, chúng tôi quan sát rằng xu hướng độ chính xác của chiến lược 2-way không chỉ do tăng shots, mà bắt nguồn từ sự sắp xếp gần hơn với mẫu được huấn luyện. Để xác thực điều này, chúng tôi giới thiệu hai cài đặt bổ sung: 2-way-random và 2-way-weight. Những cài đặt này trải qua huấn luyện fixed-shot để khởi tạo, theo sau bởi finetuning trên 2-16 shots với các phương pháp ngẫu nhiên và có trọng số, tương ứng. Cả hai thể hiện

--- TRANG 8 ---
2-shot 4-shot 8-shot 12-shot 16-shot0.400.450.500.550.600.650.700.750.80Độ chính xác0.690.710.760.780.79Lấy mẫu Cố địnhHình 6: Nghiên cứu ablation về số lượng shot. Các thanh xám minh họa độ chính xác cao nhất đạt được cho mỗi số lượng shot, biểu thị huấn luyện dựa trên shot cụ thể. Đường đỏ minh họa hiệu suất của mô hình được huấn luyện sử dụng chiến lược lấy mẫu. Đáng chú ý, cả hai tình huống đều thể hiện cao nguyên trong độ chính xác sau khi đạt đến mốc 8-shot.

0% 25% 50% 75% 100%
Tỷ lệ Sai0.00.10.20.30.40.50.60.70.8Độ chính xác0.38
0.20.78
0.64
0.48
0.35
0.0Chúng tôi OpenFlamingo OtterHình 7: Nghiên cứu ablation về tỷ lệ sai. Trái ngược với OpenFlamingo [6], duy trì độ chính xác 38% ở tỷ lệ sai 100%, mô hình của chúng tôi đạt 0% độ chính xác trong cùng điều kiện. Kết quả này nhấn mạnh khả năng của mô hình chúng tôi bảo tồn liên kết chính xác giữa tập hỗ trợ và truy vấn.

1st 4th 8th 12th 16th
Vị trí Được Sửa đổi0.6000.6250.6500.6750.7000.7250.7500.7750.800Độ chính xác
0.630.69
0.630.680.7Gốc Chúng tôiHình 8: Tác động của việc sửa đổi nhãn tại các vị trí khác biệt. Đường xanh dương nét đứt phục vụ như tham chiếu cho độ chính xác gốc, trong khi đường đỏ mô tả độ chính xác của mô hình chúng tôi sau khi nhãn được sửa đổi tại các vị trí cụ thể. Giảm độ chính xác đáng kể phản ánh sự phụ thuộc vị trí, trong khi thay đổi nhỏ cho thấy tính không quan trọng của vị trí trong việc ra quyết định của mô hình.

những cải thiện độ chính xác đáng kể trong shots thấp hơn. Đáng chú ý, trong khi độ chính xác của shots cao hơn, được finetuned với chiến lược ngẫu nhiên, giảm—một quan sát phản ánh hành vi của chiến lược mix. Những kết quả này nhấn mạnh hiệu quả của một phương pháp huấn luyện đều, bền vững và tổng quát trong việc khai thác tiềm năng của các mô hình ngôn ngữ lớn, tiết lộ sự xuất hiện của hiện tượng "lost-in-the-middle", phù hợp với các quan sát trước đó của chúng tôi.

Huấn luyện có làm hại hiệu suất zero-shot không?
Bảng 3 hiển thị so sánh giữa mô hình our-7B với shikra-13B [27] và một số phương pháp SOTA trước đó trên Imagenet-100 và VQAv2. Từ kết quả, chúng tôi kết luận rằng chiến lược huấn luyện mix của chúng tôi sẽ không làm hại hiệu suất zero-shot của mô hình.

6 Thảo luận
6.1 Hạn chế
Chúng tôi tin rằng công trình này giới thiệu một cài đặt thách thức và hứa hẹn cho cả MLLM và LLM. Tuy nhiên, trọng tâm chính trong bài báo này nằm ở học liên kết ngữ cảnh trong bối cảnh MLLM, cụ thể là xác thực các nhiệm vụ cơ bản như phân loại hình ảnh. Do đó, công trình này nên được coi như một baseline nền tảng để khám phá tiềm năng của học liên kết ngữ cảnh.

Nhìn về phía trước, các hướng nghiên cứu tương lai bao gồm phân tích lý thuyết sâu hơn đi sâu vào sự phức tạp của mối quan hệ nhân quả giữa các mẫu hỗ trợ và, quan trọng, giữa tập hỗ trợ và truy vấn. Hiểu và làm sáng tỏ sự phức tạp của những liên kết nhân quả này đại diện cho những con đường điều tra có ý nghĩa có thể dẫn đến những tiến bộ đáng kể trong khả năng của mô hình trong suy luận, học và thích nghi với các tình huống mới. Khi lĩnh vực tiến triển, chúng tôi dự đoán những điều tra và tinh chế thêm

Phương pháp ImageNet-100 VQAv2devVQAv2std
OpenFlamingo [6] 0.00 - -
Flamingo-80B [33] - 56.3 -
Flamingo-9B [33] - 51.8 -
BLIP2 [9] - 65.0 -
Otter [5] 0.13 - -
Shikra-13B [27] 0.05 77.3 77.5
Ours-7B-[mix] 0.16 75.1 75.3

Bảng 3: Đánh giá định lượng được tiến hành trên cả tập dữ liệu ImageNet-100 và VQAv2 sử dụng phương pháp zero-shot. Kết quả xác nhận rằng chiến lược huấn luyện của chúng tôi không thể hiện tác động có hại nào đến hiệu suất zero-shot.

sẽ không chỉ làm giàu sự hiểu biết của chúng ta về học liên kết ngữ cảnh mà còn thực hiện học trong ngữ cảnh cho MLLM và LLM theo cách thống nhất.

6.2 Kết luận
Tóm lại, bài báo này giới thiệu một mô hình đột phá của học few-shot liên quan nhân quả, mở rộng đáng kể khả năng của Mô hình Ngôn ngữ Lớn Đa Phương thức (MLLM) trong bối cảnh cuộc trò chuyện đơn lẻ. Thông qua thí nghiệm tỉ mỉ và chiến lược huấn luyện được thiết kế cẩn thận, chúng tôi chứng minh rằng MLLM có thể khéo léo thiết lập ánh xạ giữa các cặp đầu vào-nhãn ground-truth, từ đó có được thành thạo để tổng quát hóa khả năng này một cách liền mạch cho các hình ảnh chưa từng gặp trước đây và các khái niệm mới. Tiến bộ quan trọng này đẩy MLLM vào những lãnh thổ chưa được khám phá, cho phép chúng không chỉ có được mà còn áp dụng kiến thức theo cách giống với nhận thức con người hơn.

--- TRANG 9 ---
Tài liệu tham khảo
[1]Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, và Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.
[2]Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, và Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200–212, 2021.
[3]Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, và Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022.
[4]Midjourney. Midjourney. https://www.midjourney.com, 2023.
[5]Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, và Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. CoRR, abs/2305.03726, 2023.
[6]Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, và Ludwig Schmidt. Openflamingo, March 2023.
[7] OpenAI. Gpt-4 technical report, 2023.
[8]Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888–12900. PMLR, 2022.
[9]Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.
[10] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.
[11] Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.
[12] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.
[13] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965–10975, 2022.
[14] Hao Yang, Junyang Lin, An Yang, Peng Wang, Chang Zhou, và Hongxia Yang. Prompt tuning for generative multimodal pretrained models. arXiv preprint arXiv:2208.02532, 2022.
[15] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, và Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337–2348, 2022.
[16] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, và Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16816–16825, 2022.
[17] Xuejing Liu, Wei Tang, Jinghui Lu, Rui Zhao, Zhaojun Guo, và Fei Tan. Deeply coupled cross-modal prompt learning. arXiv preprint arXiv:2305.17903, 2023.
[18] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, và Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709–727. Springer, 2022.
[19] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, và Chen Change Loy. Unified vision and language prompt learning, 2022.
[20] Shuhuai Ren, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alex Smola, và Xu Sun. Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition, 2023.
[21] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, và Fahad Shahbaz Khan. Maple: Multi-modal prompt learning, 2023.
[22] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, và Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.
[23] OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2023.
[24] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.
[25] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, và Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023.
[26] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, và Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
[27] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, và Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic, 2023.
[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, và Furu Wei. Kosmos-2: Grounding multimodal large language models to the world, 2023.
[29] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, và Zhifang Sui. A survey on in-context learning, 2023.
[30] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, và Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023.
[31] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, và Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models, 2023.
[32] Tanmay Gupta và Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training, 2022.

--- TRANG 10 ---
[33] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikołaj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, và Karén Simonyan. Flamingo: a visual language model for few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, và A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 23716–23736. Curran Associates, Inc., 2022.
[34] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, và Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015.
[36] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, và Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023.
