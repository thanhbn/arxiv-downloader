# 2311.15759.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.15759.pdf
# Kích thước tệp: 4751907 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hướng tới Tầm nhìn Tăng cường LLM:
Trao quyền Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM
Yunxin Li1Baotian Hu1Wei Wang2Xiaochun Cao2Min Zhang1
Tóm tắt
Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn đa phương thức (MLLM) đã đạt được khả năng sinh tạo đa phương thức đáng kể, tương tự như GPT-4. Các mô hình này chủ yếu ánh xạ thông tin thị giác vào không gian biểu diễn ngôn ngữ, tận dụng kiến thức rộng lớn và khả năng sinh văn bản mạnh mẽ của LLM để tạo ra các phản hồi tuân theo hướng dẫn đa phương thức. Chúng ta có thể gọi phương pháp này là LLM cho Tầm nhìn vì việc sử dụng LLM để hiểu thị giác-ngôn ngữ, tuy nhiên quan sát thấy rằng các MLLM này bỏ qua tiềm năng khai thác kiến thức thị giác để tăng cường khả năng tổng thể của LLM, điều này có thể được coi là Tầm nhìn Tăng cường LLM. Trong bài báo này, chúng tôi đề xuất một phương pháp gọi là MKS2, nhằm tăng cường LLM thông qua việc trao quyền Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM. Cụ thể, chúng tôi giới thiệu Bộ nhớ Thị giác Mô-đun, một thành phần được tích hợp vào các khối nội bộ của LLM, được thiết kế để lưu trữ thông tin thị giác thế giới mở một cách hiệu quả. Ngoài ra, chúng tôi trình bày kiến trúc Hỗn hợp Chuyên gia Đa phương thức mềm trong LLM để kích hoạt sự hợp tác kiến thức đa phương thức trong quá trình sinh tạo. Các thí nghiệm toàn diện của chúng tôi chứng minh rằng MKS2 tăng cường đáng kể khả năng lý luận của LLM trong các bối cảnh cần thiết kiến thức vật lý hoặc thường thức. Nó cũng mang lại kết quả cạnh tranh trên các điểm chuẩn đa phương thức.

1. Giới thiệu
Những tiến bộ gần đây (Yin et al., 2023; Driess et al., 2023; Li et al., 2023c; Ye et al., 2023) về Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) đã mở mắt cho các mô hình ngôn ngữ lớn chỉ có văn bản (LLM, "mù" về thông tin thị giác), cho phép chúng hiểu và xử lý thông tin đa phương thức, từ đó thúc đẩy sự phát triển tiếp theo của Trí tuệ Nhân tạo Tổng quát (AGI) tập trung vào LLM. Trong dòng nghiên cứu này như MiniGPT-4 (Zhu et al., 2023), LLaVA (Liu et al., 2023a), và BLIP-2 (Li et al., 2023a), thông tin ngoài phương thức ngôn ngữ thường được căn chỉnh vào không gian ngôn ngữ, và sau đó kiến thức phong phú được lưu trữ trong LLM và khả năng sinh văn bản mạnh mẽ của nó được sử dụng để hiểu các thông tin đa phương thức khác nhau và tạo ra phản hồi tương ứng với hướng dẫn của con người. Họ đã thực hiện một bước quan trọng hướng tới việc xây dựng một mô hình ngôn ngữ thị giác lớn đa phương thức tương tự như GPT-4 (OpenAI, 2023), đóng góp rất nhiều dữ liệu tuân theo hướng dẫn đa phương thức (Zhang et al., 2023; Liu et al., 2023a;b) và kỹ thuật tinh chỉnh đa phương thức hiệu quả (Ye et al., 2023; Zhu et al., 2023). Những phương pháp này tập trung vào hiểu thông tin đa phương thức có thể được coi là "LLM cho tầm nhìn" vì chủ yếu sử dụng LLM để xử lý các vấn đề thị giác-ngôn ngữ.

1School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen2School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University. Liên hệ: Baotian Hu <hubaotian@hit.edu.cn >.
Công trình sơ bộ. Đang trong quá trình nghiên cứu.

Hình 1. So sánh giữa MKS2 được đề xuất và các mô hình LLM đa phương thức được tinh chỉnh có giám sát (SFT) trước đây. MKS2 tập trung vào cải thiện LLM với kiến thức thị giác. VMN đề cập đến mạng ánh xạ thị giác, chuyển mã hóa hình ảnh sang không gian ngôn ngữ. MVM và MoMEs đại diện cho bộ nhớ thị giác mô-đun được đề xuất và kiến trúc hỗn hợp chuyên gia đa phương thức mềm trong LLM, tương ứng.

--- TRANG 2 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

Tuy nhiên, các MLLM hiện tại, cả LLM được tiền huấn luyện và tinh chỉnh có giám sát (SFT) đều bỏ qua việc tăng cường khả năng của LLM trong việc khai thác kiến thức thị giác. Lý tưởng nhất, giống như não người giữ lại và sử dụng thông tin thị giác, MLLM hoặc LLM nên được trang bị để lưu trữ thông tin thị giác bên ngoài. Trong những tình huống cần thiết thị giác thông thường, ngay cả khi không có đầu vào thị giác trực tiếp, LLM nên có thể truy cập kiến thức thị giác-ngôn ngữ được lưu trữ này để lý luận kết hợp. Điều này vượt xa việc chỉ xử lý đầu vào đa phương thức, như "LLM cho Tầm nhìn" được mô tả trong Hình 1. Do đó, chúng tôi đưa ra thuật ngữ "Tầm nhìn Tăng cường LLM" để mô tả khả năng mong muốn cho LLM. Thông qua việc tăng cường này, các mô hình lớn sẽ lưu trữ và hiệu quả rút ra kiến thức đa phương thức và cơ sở kiến thức cũng như khả năng lý luận của chúng sẽ được tăng cường.

Để đạt được mục tiêu này, chúng tôi trình bày MKS2, một phương pháp sáng tạo được thiết kế để trao quyền Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM, bao gồm hai giai đoạn cốt lõi: Lưu trữ Thông tin Thị giác và Hợp tác Kiến thức Đa phương thức. Trong giai đoạn đầu tiên, chúng tôi giới thiệu Bộ nhớ Thị giác Mô-đun (MVM) trong các khối transformer nội bộ của LLM để lưu trữ thông tin thị giác. Cụ thể, được lấy cảm hứng từ các nghiên cứu trước đây (Kazemnejad et al., 2023; Wang et al., 2022b) tập trung vào đo lường kiến thức tham số của các mô hình ngôn ngữ được tiền huấn luyện và quan sát vai trò lưu trữ kiến thức của mạng nơ-ron truyền thẳng (FNN), chúng tôi kết hợp hai lớp FNN vào mỗi khối LLM để xây dựng bộ nhớ thị giác nhẹ. Tiếp theo, chúng tôi sử dụng một tập hợp các cặp hình ảnh-văn bản để độc quyền huấn luyện và cập nhật MVM sử dụng hai phương pháp học tập: sinh tạo từ hình ảnh sang văn bản và truy xuất từ văn bản sang hình ảnh. Trong cả hai cách, nhúng hình ảnh và token mềm đi qua bộ nhớ thị giác sau các tính toán chú ý. Những chiến lược này trao quyền cho LLM để hiểu, dịch và lưu trữ thông tin thị giác trong LLM thông qua khung ngôn ngữ học.

Để hợp tác kiến thức đa phương thức, chúng tôi giới thiệu kiến trúc Hỗn hợp Chuyên gia Đa phương thức mềm (MoMEs). Khung này tận dụng các chuyên gia chuyên biệt, bao gồm Bộ nhớ Thị giác Mô-đun (Chuyên gia Thị giác) và MLP gốc (Chuyên gia Văn bản) trong LLM trong quá trình sinh tạo. Để đạt được hiệu quả này, chúng tôi đóng băng tất cả tham số của LLM, áp dụng Thích ứng Hạng Thấp (LoRA (Hu et al., 2021)) cho mỗi mô-đun chuyên gia và tạo điều kiện tích hợp thông tin qua các khối LLM thông qua phương pháp trộn mềm ở mức token. Bằng cách làm như vậy, mô hình tổng thể trở nên thành thạo trong việc phù hợp với cả thông tin đa phương thức và chỉ văn bản, cho phép sự hợp tác liền mạch qua các hình thức đầu vào khác nhau. Trong quá trình huấn luyện, chúng tôi thu thập một tập dữ liệu hướng dẫn đa dạng, chứa các hướng dẫn chỉ văn bản và dữ liệu tuân theo hướng dẫn đa phương thức hình ảnh-văn bản, để đảm bảo tính hiệu quả của MoMEs trong việc xử lý cả các nhiệm vụ đa phương thức và chỉ văn bản.

Hình 2. MKS2-Llama-2-13b đạt hiệu suất zero-shot SOTA trên bảy nhiệm vụ lý luận ngôn ngữ tự nhiên. Nó cho thấy việc đạt được lưu trữ và chia sẻ kiến thức đa phương thức có hiệu quả trong việc cải thiện khả năng tổng thể của LLM.

Để xác nhận tính hiệu quả của phương pháp của chúng tôi, chúng tôi đánh giá MKS2 trên bảy điểm chuẩn xử lý ngôn ngữ tự nhiên (NLP) và sáu bộ dữ liệu hiểu hình ảnh-văn bản. Kết quả thí nghiệm mở rộng cho thấy MKS2 đạt được hiệu suất vượt trội trên các nhiệm vụ NLP đòi hỏi kiến thức thế giới vật lý hoặc thị giác, ví dụ, MKS2-Llama-2 vượt trội đáng kể so với Llama-2-chat như được hiển thị trong Hình 2. Nó cũng đạt được hiệu suất cạnh tranh trong các tình huống hiểu hình ảnh-văn bản so với các MLLM trước đây.

Những đóng góp chính của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi giới thiệu MKS2, một khung học tập tăng cường tầm nhìn cho LLM, được thiết kế để lưu trữ và chia sẻ kiến thức đa phương thức hiệu quả. Khung này xử lý hiệu quả cả đầu vào đa phương thức và chỉ văn bản.
• MKS2 chứng minh kết quả vượt trội trong các nhiệm vụ tập trung kiến thức so với LLM SFT truyền thống và LLM sử dụng Học tăng cường từ Phản hồi Con người (RLHF).
• Các nghiên cứu loại bỏ xác nhận hiệu quả của hỗn hợp chuyên gia đa phương thức kết hợp một chuyên gia kiến thức thị giác. Kiến trúc này cải thiện rõ rệt hiệu suất của LLM vượt ra ngoài khả năng của LLM tinh chỉnh có giám sát thông thường.
• Các thí nghiệm của chúng tôi cho thấy rằng dữ liệu tuân theo hướng dẫn đa phương thức tiếp tục tăng cường hiệu suất của LLM trong các nhiệm vụ lý luận ngôn ngữ tự nhiên đòi hỏi thường thức rộng rãi.

--- TRANG 3 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

Hình 3. Quy trình làm việc tổng thể của MKS2. Nó thực hiện lưu trữ thông tin thị giác và hợp tác kiến thức đa phương thức trong LLM. Trong giai đoạn đầu tiên, chúng tôi giới thiệu bộ nhớ thị giác mô-đun (MVM, được trình bày trong các khối màu xanh) và huấn luyện nó thông qua các chiến lược học tập tập trung vào ngôn ngữ. Chúng tôi cũng trình bày kiến trúc hỗn hợp chuyên gia đa phương thức mềm (MoMEs) để thực hiện hợp tác kiến thức đa phương thức trong quá trình sinh tạo.

2. Kiến thức cơ bản
Trước tiên chúng tôi xem xét phương pháp tinh chỉnh có giám sát và phương pháp tinh chỉnh tuân theo hướng dẫn đa phương thức được đề xuất gần đây cho LLM.

2.1. Tinh chỉnh có giám sát
Một LLM được tiền huấn luyện thuần túy được tinh chỉnh trên các bộ dữ liệu được gán nhãn chất lượng cao sử dụng giám sát ở mức token để tạo ra một mô hình Tinh chỉnh có Giám sát, được gọi là SFT-LLM. Các phương pháp phổ biến là sử dụng dữ liệu hướng dẫn được xây dựng tự động bởi GPT-4 (Wang et al., 2022c) và dữ liệu chất lượng cao được chú thích thủ công từ các nhiệm vụ downstream (Chung et al., 2022) để tinh chỉnh LLM thuần túy. Để giảm chi phí huấn luyện, các nghiên cứu gần đây đưa ra một số phương pháp tinh chỉnh hướng dẫn hiệu quả, ví dụ, LoRA (Hu et al., 2021), QLoRA (Dettmers et al., 2023), v.v. Những SFT-LLM này có khả năng tạo ra các phản hồi giống con người cho các hướng dẫn chỉ văn bản khác nhau, có tác động sâu sắc đến mọi tầng lớp xã hội.

2.2. Tinh chỉnh tuân theo hướng dẫn đa phương thức
So với các mô hình thị giác-ngôn ngữ truyền thống như Oscar (Li et al., 2020), Flamingo (Alayrac et al., 2022), OFA (Wang et al., 2022a), v.v., phương pháp tinh chỉnh tuân theo hướng dẫn đa phương thức đã khám phá việc mở rộng tinh chỉnh hướng dẫn chỉ văn bản trong LLM sang đa phương thức. Những MLLM này áp dụng LLM làm bộ xử lý thông tin đa phương thức đạt được hiệu suất zero-shot ấn tượng trên các nhiệm vụ chưa thấy. Nói chung, như phương pháp truyền thống được mô tả trong Hình 1, một bộ mã hóa thị giác đóng băng (ví dụ, bộ mã hóa thị giác của CLIP) được sử dụng để có được biểu diễn chuỗi của một hình ảnh và một mạng ánh xạ thị giác (VMN, một lớp chiếu tuyến tính hoặc Q-former từ BLIP-2) chiếu mã hóa hình ảnh thành nhúng hình ảnh mềm vào không gian ngôn ngữ của LLM. Sau đó, chúng ta có thể sử dụng một kỹ thuật tinh chỉnh hiệu quả để cho phép LLM xử lý thông tin đa phương thức, từ đó biến LLM thành MLLM.

Chính thức, một mẫu hướng dẫn hình ảnh-văn bản đa phương thức có thể được biểu diễn dưới dạng bộ ba sau, tức là (I, T, R), trong đó I, T, R đại diện cho hình ảnh đầu vào, mô tả văn bản (về nhu cầu con người hoặc tiền đề liên quan đến hình ảnh) và phản hồi thực tế, tương ứng. Trong quá trình huấn luyện, MLLM được xây dựng bị buộc phải dự đoán token tiếp theo của phản hồi thông qua mục tiêu tự hồi quy, có thể được trình bày như:
L(θ) = −∑(i=1 to N) log P(Ri|I, T, R<i; θ), (1)
trong đó N là độ dài của phản hồi và θ đề cập đến các tham số huấn luyện trong toàn bộ khung.

Tóm lại, chúng tôi thấy rằng hai phương pháp này bỏ qua việc giới thiệu kiến thức thị giác để cải thiện khả năng tổng thể của LLM trong việc xử lý các nhiệm vụ chỉ văn bản.

--- TRANG 4 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

3. Phương pháp luận
Trong các tiểu mục sau, chúng tôi sẽ trình bày chi tiết hai giai đoạn của MKS2: Lưu trữ Thông tin Thị giác và Hợp tác Kiến thức Đa phương thức.

3.1. Lưu trữ Thông tin Thị giác
Để thực hiện lưu trữ thông tin thị giác trong LLM, chúng tôi đề xuất tiêm Bộ nhớ Thị giác Mô-đun (MVM) vào các khối nội bộ của LLM và buộc MVM ghi nhớ thông tin thị giác thế giới mở thông qua các chiến lược học tập tập trung vào ngôn ngữ.

Bộ nhớ Thị giác Mô-đun (MVM). Mô-đun này là hai lớp mạng nơ-ron truyền thẳng (FFN) và được tiêm vào mỗi khối transformer của LLM. Như phần trên được hiển thị trong Hình 3, hình ảnh đầu vào I trước tiên được chiếu thành nhúng hình ảnh mềm hI thông qua bộ mã hóa thị giác được tiền huấn luyện, Q-former từ BLIP-2, và một lớp tuyến tính có thể học. Lấy khối đầu tiên làm ví dụ; quá trình tính toán có thể được trình bày như sau:

hTs = Self-Attention(hI),
hTF = hTs + MVM(layernorm(hTs)), (2)

trong đó Self-Attention là phép tính chú ý gốc trong LLM. Chúng tôi chỉ chèn MVM vào bên trong LLM gốc và không thay đổi các cấu trúc khác. Tất cả các trạng thái ẩn đi qua MVM sau khi có được đầu ra hTs của Self-Attention, và chúng tôi cũng đặt kích thước tổng thể của bộ nhớ thị giác bằng cách kiểm soát kích thước trạng thái ẩn của FFN.

Chiến lược Học tập Tập trung vào Ngôn ngữ. Vì chúng tôi coi LLM như là tương tự với não người, chúng tôi đã bắt đầu một nỗ lực tiên phong để tạo ra bộ nhớ lưu trữ thị giác trong LLM. Mục tiêu cuối cùng của chúng tôi là trao quyền cho LLM khả năng hiểu một hình ảnh đã cho và gợi lên các tình huống thị giác liên quan dựa trên đầu vào văn bản, tương tự như nhận thức con người. Để đạt được mục tiêu này, chúng tôi áp dụng hai mục tiêu học tập để huấn luyện MVM với một lượng lớn các cặp hình ảnh-văn bản. Như được hiển thị trong Hình 3, chúng tôi cho phép LLM tạo ra mô tả ngôn ngữ của một hình ảnh, điều này giống với việc hiểu và dịch một hình ảnh như não. Ngoài ra, được cho một câu với một số đối tượng thị giác, LLM nên gắn vào hình ảnh liên quan đến câu, điều này giống với trí tưởng tượng. Giả sử rằng mô tả ngắn (chú thích) của hình ảnh đầu vào I là D, tổn thất tạo mô tả là
Lc = −(1/N)∑(i=1 to N) lc(IMGi, Di), (3)
trong đó N là số lượng cặp hình ảnh-văn bản trong một batch và lc đề cập đến tổn thất cross-entropy.

Trong khi truy xuất hình ảnh liên quan, chúng tôi sử dụng trạng thái ẩn đầu ra he của token cuối ⟨/s⟩ của chú thích đầu vào để khớp với nhúng hình ảnh. Cụ thể, chúng tôi sử dụng một lớp tuyến tính có thể học để chiếu nó vào cùng kích thước với mã hóa toàn cục hình ảnh thu được bởi bộ mã hóa thị giác. Sau đó chúng tôi tính toán độ tương tự cosine giữa chúng và tối thiểu hóa tổn thất InfoNCE cho truy xuất từ văn bản sang hình ảnh (t2i) trên một batch gồm N mẫu. Các mẫu âm là các hình ảnh không liên quan khác trong một batch. Do đó, tổn thất học tập tập trung vào ngôn ngữ tổng thể là
LStage1 = Lc + Lt2i,
Lt2i = −(1/N)∑(i=1 to N) [log(exp(sim(Di, IMGi)/τ) / ∑(j=1 to N) exp(sim(Di, IMGj)/τ))], (4)
trong đó τ là một tham số nhiệt độ có thể học. Trong quá trình huấn luyện, chúng tôi đóng băng tất cả các tham số được tiền huấn luyện của LLM và chỉ cập nhật MVM. Ngoài cách truy xuất hình ảnh để đạt được liên kết thông tin thị giác, việc sử dụng công nghệ tạo hình ảnh để huấn luyện kết hợp cũng là một phương pháp thay thế.

3.2. Hợp tác Kiến thức Đa phương thức
Sau khi có được lưu trữ thông tin thị giác bên trong LLM, chúng tôi cần xem xét cách thực hiện hợp tác kiến thức đa phương thức trong quá trình sinh tạo. Coi MVM và MLP được tiền huấn luyện trong LLM như là chuyên gia thị giác và văn bản tương ứng, chúng tôi đề xuất phương pháp hỗn hợp chuyên gia đa phương thức mềm (MoMEs) để đạt được việc sử dụng kiến thức đa phương thức ở mức token.

Hỗn hợp Chuyên gia Đa phương thức (MoMEs). Để tăng tốc quá trình huấn luyện, như phần dưới được hiển thị trong Hình 3, chúng tôi đóng băng MVM và các tham số khác của LLM, áp dụng Thích ứng Hạng Thấp (Hu et al., 2021) (LoRA) cho chuyên gia hai phương thức: MVM và MLP. Chúng tôi ký hiệu các token đầu vào cho một chuỗi đầu vào đến MoMEs bằng X ∈ Rm×d, trong đó m là số lượng token và d là kích thước của chúng. Quá trình tính toán cho chuyên gia kiến thức thị giác và ngôn ngữ có thể được đưa ra trong
hVE = LoRA-MVM(X),
hTE = LoRA-MLP(X),
LoRA(W0) := W0X + ΔWX = W0x + BAX, (5)
trong đó B, A là các tham số có thể học được thêm vào cho mỗi trọng số được tiền huấn luyện của chuyên gia thị giác và văn bản. LoRA-MVM và LoRA-MLP(X) đại diện cho các chuyên gia kiến thức gốc được trang bị tính toán LoRA bổ sung. Bằng cách làm như vậy, quá trình huấn luyện hiệu quả vì không cập nhật các tham số tổng thể của chuyên gia.

Mỗi lớp MoE sử dụng các hàm chuyên gia (được hiển thị trong Phương trình 5) được áp dụng trên các token riêng lẻ, cụ thể là
fi: Rd → Rd để i = 1:2.
Mỗi chuyên gia sẽ xử lý p slot, và mỗi slot có một vectơ tham số d-chiều tương ứng. Như S→M được hiển thị trong Hình 3, sự kết hợp ở mức token cho các đầu ra chuyên gia có thể được trình bày như
S = Softmax(wsX + bs),
hM = S1hVE + S2hTE,
ho = hTs + hM, (6)
trong đó S ∈ RX×2 và kích thước cuối cùng được chuẩn hóa với tính toán Softmax. Đầu ra của mỗi khối trong LLM được ký hiệu là ho.

3.3. Huấn luyện
Trong giai đoạn đầu tiên, kích thước của các cặp hình ảnh-văn bản được sử dụng là khoảng 2,3M từ CC3M (Changpinyo et al., 2021), COCO Captioning (Chen et al., 2015), và Flickr-30k (Plummer et al., 2015). Để đạt được hợp tác kiến thức đa phương thức, như được hiển thị trong Hình 1, chúng tôi sử dụng dữ liệu tuân theo hướng dẫn chỉ văn bản và hình ảnh-văn bản để huấn luyện toàn bộ kiến trúc. Bộ nhớ thị giác mô-đun được thêm vào và LLM được đóng băng trong quá trình huấn luyện. Chúng tôi sử dụng dữ liệu hướng dẫn được sử dụng rộng rãi bao gồm: các nhiệm vụ xử lý ngôn ngữ tự nhiên chất lượng cao từ Flan-T5 (Chung et al., 2022), dữ liệu tinh chỉnh hướng dẫn phức tạp từ WizardLLMs (Xu et al., 2023), và dữ liệu hướng dẫn đa phương thức LLaVAR (Zhang et al., 2023), tổng cộng bao gồm 1,5M dữ liệu tinh chỉnh hướng dẫn chỉ văn bản và 166k hình ảnh-văn bản.

--- TRANG 5 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

4. Thí nghiệm
4.1. Bộ dữ liệu
Điểm chuẩn Xử lý Ngôn ngữ Tự nhiên. Chúng tôi sử dụng bảy bộ dữ liệu downstream chỉ văn bản để đánh giá toàn diện MKS2, bao gồm các bộ dữ liệu liên quan đến kiến thức thế giới vật lý và điểm chuẩn đánh giá khả năng cơ bản MMLU (Hendrycks et al., 2021). Chúng tôi sử dụng các nhiệm vụ trả lời câu hỏi trắc nghiệm có thể được hưởng lợi từ kiến thức thị giác: PIQA (Bisk et al., 2020) đòi hỏi lý luận thường thức vật lý, Commonsense QA (CSQA) (Talmor et al., 2019) để đánh giá khả năng lý luận thường thức của các mô hình, OpenBook QA (OBQA) (Mihaylov et al., 2018) đòi hỏi lý luận đa bước, sử dụng kiến thức thông thường và thường thức bổ sung, và hiểu văn bản phong phú, RiddleSense (RS) (Lin et al., 2021) để hiểu phức tạp về ngôn ngữ tượng trưng và kỹ năng lý luận phản thực tế, Social IQA (Sap et al., 2019) tập trung vào kiến thức vật lý hoặc phân loại để kiểm tra trí thông minh thường thức xã hội, StrategyQA (Geva et al., 2021) cần các bước lý luận phải được suy ra bằng cách sử dụng một chiến lược.

Điểm chuẩn Hiểu Hình ảnh-Văn bản. Để đánh giá khả năng đa phương thức của mô hình được đề xuất, chúng tôi giới thiệu sáu bộ dữ liệu Trả lời Câu hỏi Thị giác (VQA) cổ điển: VQAv2 (Antol et al., 2015), OK-VQA (Marino et al., 2019), ST-VQA (Biten et al., 2019), OCR-VQA (Mishra et al., 2019), TextVQA (Singh et al., 2019), và DocVQA (Mathew et al., 2021). VQAv2 là một bộ dữ liệu VQA thế giới mở cổ điển, chứa hơn 1 triệu mẫu. Scene Text Visual Question Answering (STVQA) bao gồm 31.000+ câu hỏi trên 23.000+ hình ảnh được thu thập từ các bộ dữ liệu công khai khác nhau. Bộ dữ liệu OCRVQA bao gồm hơn 1 triệu cặp câu hỏi-trả lời bao phủ hơn 207.000 hình ảnh bìa sách. Bộ dữ liệu TextVQA bao gồm hơn 45.000 câu hỏi liên quan đến văn bản trên hơn 28.000 hình ảnh được chọn từ các danh mục cụ thể của bộ dữ liệu OpenImages. DocVQA là một bộ dữ liệu toàn diện bao gồm 12.767 hình ảnh tài liệu với các loại và nội dung đa dạng, đi kèm với hơn 50.000 câu hỏi và câu trả lời. Đối với các bộ dữ liệu chứa nhiều hơn 5000 cặp hình ảnh-câu hỏi, chúng tôi đã chọn 5000 cặp đầu tiên để kiểm tra, tương tự như Liu et al. (2023c).

4.2. Các mô hình so sánh
Các mô hình so sánh chủ yếu bao gồm ba loại mô hình ngôn ngữ lớn mã nguồn mở Llama-2: SFT Llama-2, RLHF-tuned Llama-2, và MLLM được đề xuất gần đây. Để xác minh phương pháp tinh chỉnh có giám sát tăng cường tầm nhìn mới MKS2, chúng tôi trình bày biến thể tinh chỉnh hướng dẫn chỉ văn bản Llama-2-7b-INST-LoRA và Vicuna-Llama-2 (Penedo et al., 2023), trong đó chúng tôi áp dụng dữ liệu hướng dẫn văn bản giống hệt với phương pháp của chúng tôi và đặt r=16 cho LoRA để huấn luyện Llama2-7b-INST. Do đó, các tham số huấn luyện của Llama-2-7b-INST-LoRA tương tự như MKS2-Llama-2-7B được đề xuất, khoảng 14M. Các mô hình RLHF-tuned là các mô hình ngôn ngữ đã được huấn luyện sử dụng kết hợp các kỹ thuật phản hồi con người và học tăng cường, đạt được hiệu suất tốt hơn để hiểu hướng dẫn con người và tạo ra các phản hồi chất lượng cao. Chúng tôi chủ yếu so sánh với các biến thể Llama-2-7b-chat và Llama-2-13b-chat do Meta phát hành. Ngoài ra, để đánh giá khả năng xử lý thông tin đa phương thức của MKS2, chúng tôi cũng giới thiệu các MLLM được đề xuất gần đây như đường cơ sở. Flamingo (Alayrac et al., 2022) và OFA (Wang et al., 2022a) là các mô hình thị giác và ngôn ngữ được tiền huấn luyện truyền thống, đã thấy một lượng cặp hình ảnh-văn bản. BLIP-2 (Li et al., 2023a) là một mô hình thị giác và ngôn ngữ được sử dụng rộng rãi, đạt được hiệu suất zero-shot đáng chú ý trên các nhiệm vụ hiểu hình ảnh-văn bản downstream. MiniGPT-4 (Zhu et al., 2023), FROMAGe (Koh et al., 2023), mPLUG-Owl (Ye et al., 2023), LLaVR (Liu et al., 2023a) và InstructBLIP (Li et al., 2023a) là các MLLM được tinh chỉnh hướng dẫn đa phương thức, được huấn luyện với dữ liệu tuân theo hướng dẫn hình ảnh-văn bản khổng lồ.

4.3. Chi tiết triển khai
Chúng tôi lấy phiên bản Llama-2 được tiền huấn luyện (Touvron et al., 2023) làm xương sống của MKS2 và chạy tất cả các mô hình với Adam Optimizer (Kingma & Ba, 2014) trên 4 GPU A100-80G với môi trường python. Tất cả các mô hình được huấn luyện và kiểm tra với định dạng số thực dấu phẩy động Bfloat16. Kích thước của lớp giữa của mô-đun bộ nhớ thị giác được chèn là 1/4 của kích thước trạng thái ẩn của LLM. Đối với Llama-2-7b, tổng tham số của MVM là khoảng 410 triệu. Trong quá trình lưu trữ thông tin thị giác, chúng tôi lấy bộ mã hóa thị giác đóng băng và Q-former từ BLIP-2-FlanT5-xxl để có được mã hóa hình ảnh, do đó độ dài của nhúng hình ảnh mềm là 32. Ngoài ra, chúng tôi đặt tốc độ học ban đầu là 1e-4 và huấn luyện mô hình khoảng 2 epoch với các bước khởi động bằng 5000. Kích thước batch được đặt là 32 với tích lũy gradient bốn bước cho thiết bị GPU đơn. Trong khi thực hiện học tuân theo hướng dẫn, chúng tôi đặt kích thước batch, r trong LoRA lần lượt là 3 và 8, và độ dài tối đa của đầu vào được đặt là 1024. Để gắn thẻ vị trí của nhúng hình ảnh, chúng tôi giới thiệu hai token có thể học ⟨img-start⟩ và ⟨img-end⟩. Tương tự như Llama-2-chat, chúng tôi thêm [INST] và [/INST] ở đầu và cuối của hướng dẫn văn bản đầu vào, như " [INST] Vui lòng viết một câu chuyện ngắn về mèo và chó [/INST] ". Trong quá trình sinh tạo, chúng tôi đặt kích thước beam lần lượt là 1 và 4 cho các nhiệm vụ chỉ văn bản và VQA.

4.4. Hiệu suất tổng thể
Hiệu suất của LLM tăng cường tầm nhìn. Chúng tôi trình bày hiệu suất zero-shot của mô hình trong Bảng 1, nhằm đánh giá khả năng hiểu hướng dẫn và giải quyết vấn đề thế giới mở của LLM. Chúng tôi quan sát thấy rằng phương pháp MKS2-Llama-2-7B/13B được đề xuất đạt được hiệu suất tốt nhất trên hầu hết tất cả các bộ dữ liệu đánh giá, đặc biệt là vượt trội đáng kể so với Llama-2-7b/13b-chat. So với Llama-2-7b-INST-LoRA mạnh mẽ cùng cỡ, MKS2-Llama-2-7b có thể tăng khoảng 8% trên CommonsenseQA, 14,5% trên OpenBookQA, 16% trên PIQA, và 8,6% trên RS, tương ứng. Do đó, MKS2 có khả năng cải thiện đáng kể hiệu suất tổng thể trên các nhiệm vụ chỉ văn bản đòi hỏi kiến thức thế giới vật lý. So với các mô hình Vicuna-Llama-2 với tất cả các tham số của Llama-2 được cập nhật, phương pháp của chúng tôi nổi bật bằng việc yêu cầu tinh chỉnh chỉ trên một phần nhỏ tham số (<0,2% tham số LLM) trong khi vẫn đạt được hiệu suất vượt trội trên một số nhiệm vụ.

Hiệu suất cạnh tranh trên các điểm chuẩn đa phương thức. Chúng tôi cũng trình bày hiệu suất zero-shot của mô hình trên bộ dữ liệu VQA trong Bảng 2. Để có được nhúng hình ảnh phù hợp và mạnh mẽ, chúng tôi tiếp tục tinh chỉnh mạng ánh xạ thị giác trong một epoch và đóng băng tất cả các tham số khác, điều này không ảnh hưởng đến bất kỳ hiệu suất chỉ văn bản nào của LLM. Chúng ta có thể thấy rằng MKS2-Llama-2-7b có thể đạt được hiệu suất tương đương trên các bộ dữ liệu VQA thế giới mở và văn bản cảnh. Đáng chú ý là không có sự suy giảm hiệu suất rõ ràng khi so sánh hiệu suất MKS2 trên các nhiệm vụ đa phương thức với mô hình ngôn ngữ lớn gốc không có tăng cường thị giác. Điều này ngụ ý rằng việc thêm tăng cường thị giác trong LLM không dẫn đến mất kiến thức liên quan đến văn bản, trong khi thực hiện LLM cho tầm nhìn. Hơn nữa, việc kết hợp dữ liệu chỉ văn bản được chứng minh là một chiến lược có giá trị để tăng cường khả năng thành thạo của mô hình trong việc trả lời các câu hỏi thị giác mở. Trong khi dữ liệu hướng dẫn hỗn hợp dẫn đến cải thiện trong trả lời câu hỏi thế giới mở, nó có vẻ như có tác động có hại đến nhận dạng văn bản cảnh, có thể do sự thay đổi trong phân phối dữ liệu huấn luyện. Điều tra thêm về quá trình tinh chỉnh và phân phối dữ liệu có thể giúp tối ưu hóa hiệu suất MKS2 trên một phạm vi rộng hơn các nhiệm vụ liên quan đến cả văn bản và hình ảnh.

--- TRANG 6 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

Bảng 1. Hiệu suất zero-shot của mô hình trên các điểm chuẩn xử lý ngôn ngữ tự nhiên. Các mô hình có † cho thấy dữ liệu SFT hoặc RLHF được tinh chỉnh của họ không được biết hoặc không được sử dụng trong công việc của chúng tôi. "INST-LoRA" đề cập đến việc áp dụng kỹ thuật LoRA được sử dụng rộng rãi để tinh chỉnh LLM với cùng dữ liệu hướng dẫn chỉ văn bản. "Multimodal-SFT" đại diện cho dữ liệu tuân theo hướng dẫn đa phương thức. "Avg.Score" hiển thị điểm đánh giá trung bình trên tổng số nhiệm vụ. Số in đậm và gạch chân đề cập đến hiệu suất tốt nhất và tốt thứ hai cho các biến thể mô hình so sánh của Llama-2-7b/13b, tương ứng.

Bảng 2. Hiệu suất zero-shot trên một số bộ dữ liệu đa phương thức. "NumImg" đại diện cho tổng số hình ảnh chứa trong giai đoạn tiền huấn luyện. Kích thước của hình ảnh đầu vào luôn là 224² cho các mô hình sau. "‡" cho thấy rằng mô hình tương ứng sử dụng các mẫu huấn luyện của các điểm chuẩn đánh giá sau như VQAv2, OK-VQA, và OCR-VQA, dẫn đến so sánh không công bằng.

4.5. Nghiên cứu loại bỏ và phân tích
Tác động của MKS2. So sánh kết quả thí nghiệm của MKS2 w/o Multimodal-SFT, MKS2 w/o Multimodal-SFT & MoMEs, và Llama-2-7b-INST-LoRA trong Bảng 1, chúng tôi quan sát thấy rằng việc kết hợp thông tin thị giác vào MVM có tác động tích cực đến hiệu suất của mô hình trên các nhiệm vụ lý luận thường thức. Điều này chứng minh rằng MoMEs có thể tận dụng thông tin thị giác được lưu trữ để cải thiện khả năng hiểu và lý luận của nó trong các bối cảnh khác nhau. Ngoài ra, hiệu suất của các biến thể MKS2 w/o Text-SFT và w/o Text-SFT + MoMEs trên các nhiệm vụ đa phương thức tiếp tục nhấn mạnh khả năng truy cập kiến thức văn bản mà không có thỏa hiệp đáng kể. Việc tích hợp thông tin thị giác cùng với dữ liệu văn bản không cản trở khả năng của mô hình trong việc trích xuất và sử dụng kiến thức văn bản hiệu quả. Điều này cho thấy rằng các khả năng cốt lõi liên quan đến văn bản của LLM vẫn mạnh mẽ khi xử lý các đầu vào đa phương thức.

Tác động của dữ liệu hướng dẫn đa phương thức đối với hiệu suất MKS2. Kết quả thí nghiệm của chúng tôi trong Bảng 1 làm nổi bật tác động của dữ liệu hướng dẫn đa phương thức đối với hiệu suất của MKS2. Mặc dù nó hiệu quả tăng cường độ chính xác trong việc giải quyết các câu hỏi liên quan đến kiến thức thế giới vật lý, nó không cung cấp sự thúc đẩy đáng kể trong việc giải quyết các vấn đề phức tạp và phức tạp đòi hỏi lý luận tiên tiến và tư duy chiến lược. Những phát hiện này nhấn mạnh tầm quan trọng của việc điều chỉnh dữ liệu và phương pháp theo yêu cầu nhiệm vụ cụ thể khi tận dụng dữ liệu đa phương thức trong các mô hình ngôn ngữ lớn để có hiệu suất tối ưu. Nghiên cứu thêm có thể khám phá các chiến lược để thu hẹp khoảng cách này cho các nhiệm vụ giải quyết vấn đề phức tạp.

Bảng 3. Thí nghiệm loại bỏ trên năm bộ dữ liệu đòi hỏi thường thức. Chúng tôi khám phá các kích thước dữ liệu và bộ nhớ thị giác khác nhau để kiểm tra hiệu suất mô hình. Tất cả các mô hình được xây dựng trên Llama-2-7b, trong đó MKS2♣ biểu thị MKS2 w/o Multimodal-SFT. "-AM-BM" cho thấy rằng nó bao gồm kích thước bộ nhớ thị giác "A" và được huấn luyện sử dụng "B" số lượng cặp hình ảnh-văn bản.

Liệu dữ liệu hướng dẫn chỉ văn bản có thể hiệu quả để tăng cường hiệu suất đa phương thức khi xây dựng MLLM? Trong các thí nghiệm loại bỏ được thảo luận trong Bảng 2, dữ liệu hướng dẫn chỉ văn bản được phát hiện là tăng cường hiệu suất của LLM được tiền huấn luyện trên các vấn đề đa phương thức mở khác nhau, đặc biệt là các câu hỏi mở đòi hỏi sự hợp nhất của thông tin văn bản và thị giác. Tuy nhiên, việc giới thiệu dữ liệu hướng dẫn chỉ văn bản có thể dẫn đến sự đánh đổi, vì nó có thể tiềm ẩn làm giảm khả năng của LLM để trả lời các câu hỏi thị giác liên quan đến nhận dạng văn bản cảnh. Những phát hiện này nhấn mạnh tầm quan trọng của việc tạo ra sự cân bằng cẩn thận khi kết hợp các phương thức dữ liệu bổ sung vào LLM, với sự xem xét chu đáo về yêu cầu nhiệm vụ và đặc điểm dữ liệu. Sự tích hợp cẩn thận như vậy đảm bảo rằng hiệu suất mô hình tổng thể được tối ưu hóa mà không có hậu quả không mong muốn đối với khả năng xử lý các thách thức đa phương thức đa dạng của nó. Do đó, tối ưu hóa LLM cho các nhiệm vụ đa phương thức đòi hỏi một phương pháp tinh tế được điều chỉnh theo lĩnh vực vấn đề cụ thể.

Tác động của kích thước bộ nhớ thị giác và quy mô dữ liệu. Chúng tôi giới thiệu thêm các cặp hình ảnh-văn bản từ LAION-400M (Schuhmann et al., 2021) để phân tích tác động của kích thước bộ nhớ thị giác và dữ liệu, và kết quả thí nghiệm được hiển thị trong Bảng 3. Kết quả thí nghiệm của chúng tôi tiết lộ rằng việc mở rộng kích thước của dữ liệu hình ảnh-văn bản được tiền huấn luyện dẫn đến cải thiện hiệu suất của mô hình MKS2 trên các nhiệm vụ downstream khác nhau. Ngoài ra, chúng tôi quan sát thấy rằng việc mở rộng mô-đun lưu trữ thị giác của mô hình MKS2 không chỉ tăng cường hiệu suất mà còn đóng góp vào sự ổn định lớn hơn trong việc tăng cường này, khi kích thước của dữ liệu hình ảnh-văn bản tăng lên. Tóm lại, những phát hiện này đề xuất một chiến lược kép để tối ưu hóa LLM dựa trên MKS2 trong giai đoạn tinh chỉnh có giám sát: tăng kích thước của dữ liệu hình ảnh-văn bản và chọn kích thước lưu trữ thị giác tỷ lệ thuận lớn hơn. Phương pháp này rất quan trọng để đạt được những cải thiện hiệu suất nhất quán và ổn định hơn.

4.6. Nghiên cứu trường hợp
Chúng tôi trình bày một số trường hợp trong Hình 4 để tiếp tục cho thấy khả năng tổng thể của MKS2-Llama-2. Chúng ta có thể thấy rằng mô hình được đề xuất đạt được hiệu suất tốt hơn khi trả lời Q&A thường thức với kiến thức vật lý. Ngoài ra, chúng tôi cũng quan sát thấy rằng khả năng hiểu đa phương thức của MKS2-Llama-2 rất mạnh mẽ, chẳng hạn như nó có thể nhận ra sự hài hước của con mèo với đầu sư tử. Ngoài ra, nó có thể sử dụng kiến thức liên quan để làm phong phú thêm phản hồi dựa trên các manh mối thị giác, ví dụ, câu chuyện ngắn xung quanh nội dung được hiển thị trong hình ảnh thứ hai.

--- TRANG 7 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

Hình 4. Minh họa về các trường hợp được tạo ra bởi các mô hình so sánh và MKS2. Các từ màu xanh lá cây đề cập đến câu trả lời đúng trong các nhiệm vụ trả lời câu hỏi tự nhiên. Các phần màu vàng đại diện cho nội dung thú vị và đúng trong phản hồi.

5. Các nghiên cứu liên quan
Các phương pháp tăng cường kiến thức thị giác. Có một dòng công việc dài về việc sử dụng thông tin thị giác rõ ràng để cải thiện biểu diễn tưởng tượng của ngôn ngữ, do đó thúc đẩy khả năng sinh tạo đa dạng của LLM. Đặc biệt, Jin et al. (2022) tận dụng kiến thức thị giác trong các nhiệm vụ NLP, phát triển nhiều phương pháp tăng cường mô hình chéo để cải thiện khả năng biểu diễn của các mô hình ngôn ngữ được tiền huấn luyện. Một số nghiên cứu (Shi et al., 2019; Lu et al., 2022; Li et al., 2023b) đề xuất truy xuất hình ảnh tương ứng với văn bản từ kho hình ảnh và sử dụng kiến thức thị giác để cải thiện hiệu suất trên các nhiệm vụ downstream như hoàn thành văn bản (Zellers et al., 2019), sinh tạo câu chuyện (Fan et al., 2018), và concept-to-text (Barzilay & Lapata, 2005). Gần đây, một số nhà nghiên cứu (Long et al., 2021; Yang et al., 2021; Zhu et al., 2022) đề xuất sử dụng kỹ thuật text-to-image mạnh mẽ để có được biểu diễn tưởng tượng của ngôn ngữ và truyền chúng vào mô hình ngôn ngữ thông qua cách prefix-tuning. Trong bài báo này, chúng tôi trình bày lưu trữ thông tin thị giác trong LLM và đạt được kiến thức thị giác tăng cường LLM mà không cần nhập hình ảnh một cách rõ ràng vào các mô hình ngôn ngữ.

LLM cho tầm nhìn. Các nghiên cứu gần đây (Zhang et al., 2023; Zhu et al., 2023; Li et al., 2023a) hướng tới MLLM tập trung vào việc sử dụng kiến thức rộng lớn và khả năng sinh tạo ngôn ngữ của LLM để giải quyết các nhiệm vụ đa phương thức (Li et al., 2023d), đặc biệt là để hiểu và lý luận thị giác. Đầu tiên, các nghiên cứu này thường ánh xạ thông tin thị giác thu được bởi bộ mã hóa thị giác được tiền huấn luyện vào không gian biểu diễn của LLM, thông qua một lớp chiếu tuyến tính có thể học (Merullo et al., 2023), MLP, hoặc Q-Former (Li et al., 2023a). Giai đoạn này thường được gọi là căn chỉnh đặc trưng và chỉ cần vài trăm nghìn dữ liệu có thể làm tốt công việc (Liu et al., 2023a). Sau đó, các MLLM ban đầu sẽ được tinh chỉnh thông qua dữ liệu tuân theo hướng dẫn đa phương thức (Ye et al., 2023; Li et al., 2023c; Bai et al., 2023). Ở giai đoạn này, LLM và lớp chiếu thường được tinh chỉnh cùng nhau chỉ với dữ liệu hướng dẫn đa phương thức. Mô hình ngôn ngữ lớn được sử dụng phổ biến là SFT LLM và phương pháp tinh chỉnh áp dụng LoRA nhẹ được sử dụng rộng rãi (Hu et al., 2021). Tuy nhiên, các nghiên cứu này hiếm khi xem xét việc sử dụng kiến thức thị giác để tăng cường khả năng xử lý văn bản thuần túy của LLM, từ đó xây dựng một LLM hoặc MLLM mạnh mẽ hơn.

6. Kết luận
Trong bài báo này, chúng tôi trình bày một phương pháp mới MKS2 cho phép LLM ghi nhớ và sử dụng thông tin thị giác, đạt được lưu trữ và hợp tác kiến thức đa phương thức trong LLM. MKS2 bao gồm bộ nhớ thị giác mô-đun và hỗn hợp chuyên gia đa phương thức mềm, được sử dụng để lưu trữ thông tin thị giác và thực hiện hợp tác kiến thức đa phương thức, tương ứng. Chúng tôi tiến hành các thí nghiệm rộng rãi trên nhiều nhiệm vụ NLP và VQA và kết quả thí nghiệm cho thấy rằng MKS2 có khả năng tăng cường khả năng lý luận của LLM và được sử dụng để giải quyết các vấn đề đa phương thức.

--- TRANG 8 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

Tài liệu tham khảo

[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên như trong bản gốc]

--- TRANG 9 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

[Tiếp tục danh sách tài liệu tham khảo tiếng Anh]

--- TRANG 10 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

[Tiếp tục danh sách tài liệu tham khảo tiếng Anh]

--- TRANG 11 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

[Tiếp tục danh sách tài liệu tham khảo tiếng Anh]

--- TRANG 12 ---
Lưu trữ và Chia sẻ Kiến thức Đa phương thức trong LLM

[Kết thúc danh sách tài liệu tham khảo tiếng Anh]
