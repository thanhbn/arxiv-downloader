# 2309.10783.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.10783.pdf
# Kích thước tệp: 1167925 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Ngôn ngữ như Phương tiện: Phân loại Video Đa phương thức thông qua chỉ văn bản
Laura Hanu Anita L. Ver ˝o
Unitary
{laura,anita,james }@unitary.aiJames Thewlis
Tóm tắt
Bất chấp một làn sóng mới thú vị của các mô hình học máy đa phương thức, các phương pháp hiện tại vẫn gặp khó khăn trong việc diễn giải các mối quan hệ ngữ cảnh phức tạp giữa các phương thức khác nhau có trong video. Vượt ra ngoài các phương pháp hiện có nhấn mạnh vào các hoạt động hoặc đối tượng đơn giản, chúng tôi đề xuất một phương pháp mới không phụ thuộc vào mô hình để tạo ra các mô tả văn bản chi tiết có thể nắm bắt thông tin video đa phương thức. Phương pháp của chúng tôi tận dụng kiến thức rộng lớn được học bởi các mô hình ngôn ngữ lớn, như GPT-3.5 hoặc Llama2, để suy luận về các mô tả văn bản của các phương thức thị giác và thính giác, thu được từ BLIP-2, Whisper và ImageBind. Mà không cần điều chỉnh bổ sung các mô hình video-văn bản hoặc bộ dữ liệu, chúng tôi chứng minh rằng các LLM có sẵn có khả năng sử dụng các mô tả văn bản đa phương thức này như là các đại diện cho "thị giác" hoặc "thính giác" và thực hiện phân loại đa phương thức video zero-shot trong ngữ cảnh. Các đánh giá của chúng tôi trên các benchmark nhận dạng hành động phổ biến, như UCF-101 hoặc Kinetics, cho thấy các mô tả giàu ngữ cảnh này có thể được sử dụng thành công trong các tác vụ hiểu video. Phương pháp này hướng tới một hướng nghiên cứu mới đầy hứa hẹn trong phân loại đa phương thức, chứng minh cách sự tương tác giữa các mô hình học máy văn bản, thị giác và thính giác có thể cho phép hiểu video một cách toàn diện hơn.

1. Giới thiệu
Hãy tưởng tượng đây là năm 2008 và bạn vừa xem xong tập mới nhất của Breaking Bad - một trải nghiệm đa phương thức cao bao gồm hình ảnh chuyển động, lời nói và hiệu ứng âm thanh. Đột nhiên bạn nhận được một tin nhắn trên điện thoại di động của mình - đó là đồng nghiệp của bạn, người đang khẩn cấp yêu cầu mô tả về tập phim để họ có thể tham gia vào các cuộc thảo luận bên máy nước mà không gây nghi ngờ.

Bây giờ bạn phải truyền đạt cho đồng nghiệp của mình, chỉ sử dụng tin nhắn văn bản, một mô tả về tập phim sẽ đủ thuyết phục. Mặc dù việc rút gọn lượng lớn pixel và mẫu âm thanh mà bạn vừa tiêu thụ thành vài từ có vẻ như một nhiệm vụ khó khăn, bạn nhận ra rằng bằng cách kết hợp các mô tả súc tích về các hình ảnh, lời nói và âm thanh chính với khả năng lấp đầy khoảng trống bằng suy luận ngữ cảnh vốn có của đồng nghiệp, bạn sẽ có thể cung cấp một bản tường thuật toàn diện về tập phim mà không cần trải nghiệm trực tiếp. Trong công trình này, chúng tôi khám phá mức độ mà các Mô hình Ngôn ngữ Lớn (LLMs) có thể thực hiện một nhiệm vụ tương tự, cụ thể là phân loại hành động trong video khi chỉ nhận được các gợi ý văn bản về nội dung video từ các mô hình khác.

Những năm gần đây đã chứng kiến tiến bộ đáng kể trong các mô hình ngôn ngữ lớn cho văn bản, đã thể hiện khả năng và hiệu suất chưa từng có trên các tác vụ downstream [7, 17, 14, 18]. Điều này đã dẫn đến các phương pháp cố gắng thu hẹp khoảng cách giữa thị giác và ngôn ngữ. Các phương pháp tương phản như CLIP huấn luyện các biểu diễn ngôn ngữ thị giác kết hợp [15]. Perceiver IO [12] cung cấp một sơ đồ tổng quát để mã hóa các phương thức tùy ý. Kosmos [11] là một mô hình đa phương thức lớn được huấn luyện từ đầu trên dữ liệu hình ảnh và văn bản quy mô web. GPT-4 [14] chấp nhận đầu vào hình ảnh, nhưng tính năng này hiện không có sẵn công khai. Nhiều công trình thích nghi các LLM đã được huấn luyện trước để hiểu thông tin từ các phương thức khác nhau. Flamingo [5] tiêm các biểu diễn của hình ảnh và video ngắn vào backbone mô hình ngôn ngữ sử dụng các lớp Gated X-attn. BLIP-2 [13] giới thiệu một Q-Former cung cấp "soft visual prompts" để điều kiện hóa một LLM trên thông tin thị giác. Mini-GPT4 [19] tận dụng Q-Former để cung cấp một soft prompt cho mô hình dựa trên Llama. Trái ngược với các kỹ thuật này, chúng tôi chứng minh rằng chỉ sử dụng văn bản như phương tiện có thể truyền đạt thông tin đa phương thức đến các LLM downstream. Điều này có một số lợi thế chính. Thứ nhất, phương pháp này đảm bảo giao diện "plug and play" đơn giản để kết nối các mô hình mà không cần thích nghi thêm. Điều này đặc biệt liên quan với sự gia tăng của các mô hình ngôn ngữ dựa trên API cấm các sửa đổi. Thứ hai, giao tiếp giữa các mô hình trở nên minh bạch và có thể diễn giải bằng ngôn ngữ tự nhiên. Điều quan trọng, phương pháp này đơn giản hóa các tác vụ như phân loại video đa phương thức thành hai giai đoạn: giai đoạn "nhận thức" sử dụng các mô hình đơn phương thức hoặc đa phương thức như đại diện cho các giác quan khác nhau, tiếp theo là giai đoạn "suy luận" nơi một mô hình nền tảng hợp nhất các đầu vào đa dạng để tạo ra một câu chuyện video toàn diện.

Các phương pháp gần đây hơn như LENS [6] hoặc Video ChatCaptioner [8] khám phá các tương tác văn bản tương tự giữa các mô hình. Trong khi LENS chỉ khám phá khả năng của LLM để suy luận về các tác vụ trả lời câu hỏi thị giác được cung cấp các gợi ý thị giác về hình ảnh, Video ChatCaptioner đề xuất kết nối BLIP-2 và ChatGPT để có các cuộc trò chuyện về hình ảnh. Phương pháp của chúng tôi vượt ra ngoài các tác vụ trả lời câu hỏi, chứng minh rằng cả gợi ý thị giác và thính giác đều có thể được sử dụng bởi LLM để phân loại video.

Tóm lại, các đóng góp của chúng tôi là: 1) Chúng tôi giới thiệu một phương pháp phân loại đa phương thức mới bao gồm hai giai đoạn: giai đoạn "nhận thức" nơi các mô hình hoạt động như các đại diện cảm giác và giai đoạn "suy luận" hợp nhất các đầu vào văn bản đa phương thức thành một câu chuyện mạch lạc. 2) Chúng tôi chứng minh hiệu quả của văn bản như phương tiện chính để diễn giải dữ liệu đa phương thức. 3) Lần đầu tiên, chúng tôi giới thiệu rằng các biểu diễn văn bản của các gợi ý thị giác và thính giác riêng lẻ có thể phân loại hiệu quả các hành động trong video.

2. Phương pháp
Các mô hình nhận thức Để trích xuất các chú thích thị giác từ khung hình video, chúng tôi sử dụng mô hình BLIP-2 [13]. Chúng tôi chỉ xử lý 5 khung hình cách đều nhau mỗi video để đảm bảo lấy mẫu đa dạng về nội dung video. Chúng tôi sử dụng Whisper [16] để thu được các bản ghi âm thanh, cụ thể là phiên bản Faster Whisper [1] đã được tối ưu hóa cho suy luận nhanh. Chúng tôi sử dụng nhiệt độ 0, kích thước beam 5 và bộ lọc VAD để loại trừ các phần của video không có bất kỳ lời nói nào. Để tạo ra các thẻ âm thanh, chúng tôi tận dụng ImageBind [9] để có được các embedding âm thanh và tính toán độ tương tự với các embedding văn bản của các nhãn AudioSet. Sau đó chúng tôi chỉ chọn các nhãn có độ tương tự trên một ngưỡng nhất định, có thể được thu được bằng cách kiểm tra định tính một vài ví dụ.

Các mô hình suy luận Đối với mô-đun suy luận của chúng tôi, chúng tôi thử nghiệm 3 mô hình ngôn ngữ lớn tiên tiến khác nhau. Chúng tôi sử dụng GPT completion API, cụ thể là phiên bản GPT3.5-turbo [2]. Ngoài ra, chúng tôi sử dụng tính năng gọi hàm mới được ra mắt cho phép người dùng chỉ định một json schema cho đầu ra. LLM thứ hai mà chúng tôi đánh giá là Claude-instant-1 [3], được báo cáo có hiệu suất và khả năng tương tự như GPT3.5-turbo. Đối với Llama2, chúng tôi sử dụng biến thể Llama-2-13b-chat [18], có 13 tỷ tham số và chuyên dụng cho hội thoại. Chúng tôi sử dụng nhiệt độ 0 hoặc gần 0 cho tất cả các mô hình suy luận để đảm bảo đầu ra nhất quán hơn có thể tuân thủ tốt hơn các hướng dẫn đã cho. Các prompt chúng tôi sử dụng cho phân loại tuân theo mẫu đơn giản này với các biến thể nhỏ giữa các LLM khác nhau để phù hợp với các hướng dẫn prompt cụ thể: Cho {multimodal clues} và các nhãn nhận dạng hành động này: {labels} Vui lòng trả về 5 nhãn áp dụng nhất cho video trong định dạng json, từ khả năng cao nhất đến thấp nhất.

Đầu ra có cấu trúc LLM thường tạo ra các đầu ra ngôn ngữ tự nhiên trôi chảy, tuy nhiên, đối với tác vụ phân loại, chúng tôi muốn mô hình cung cấp cho chúng tôi 5 dự đoán được xếp hạng từ một tập hợp các tên lớp được định nghĩa trước. Để thực hiện điều này với GPT, chúng tôi sử dụng function calling API, cung cấp cho mô hình một JSON Schema của hàm để gọi, nơi schema chứa một enum của các tên lớp có thể.

Đối với Claude, chúng tôi cung cấp các tên lớp trong prompt và yêu cầu kết quả được trả về dưới dạng JSON, trong hầu hết các trường hợp, kết quả là một đối tượng JSON với khóa "labels" chứa danh sách các nhãn khả năng cao nhất, hoặc một đối tượng có các khóa là các lớp và giá trị là thứ hạng.

Đối với Llama2, chúng tôi cung cấp các tên lớp trong System prompt, và quan sát rằng các dự đoán thường được bao gồm như một danh sách được đánh số trong đầu ra, do đó chúng tôi chỉ đơn giản là phân tích các dòng bắt đầu bằng một số. Để so sánh với ground truth, chúng tôi chuẩn hóa để loại bỏ khoảng trắng và chuyển đổi thành chữ thường.

Đối với tất cả các mô hình, đôi khi đầu ra không thể được phân tích (như tên lớp bịa đặt hoặc ký tự thêm), và trong trường hợp này chúng tôi coi dự đoán là không chính xác.

3. Bộ dữ liệu đánh giá
UCF101 Tập test UCF-101 bao gồm 13.320 video clip ngắn từ YouTube trải rộng 101 danh mục hành động, cung cấp một tập hợp đa dạng các hành động con người hàng ngày, từ chơi nhạc cụ đến các hoạt động thể thao.

Kinetics400 Tập test Kinetics400 chứa các video clip Youtube 10s và 400 lớp hành động con người. Để tránh chi phí API, vì tập test chứa 38.685 video clip, chúng tôi xây dựng một tập con đại diện nhỏ hơn gồm 2000 video, lấy mẫu 5 video mỗi danh mục.

4. Thí nghiệm
Đầu tiên, chúng tôi chạy thí nghiệm để thấy vai trò của mỗi phương thức trong việc phân loại video trên tập test UCF-101 và tập con 2k của Kinetics400. Như Bảng 1 cho thấy, mô hình ngôn ngữ có thể hưởng lợi từ thông tin âm thanh bổ sung. Trong Bảng 2, chúng tôi so sánh mức độ tốt của 3 mô hình ngôn ngữ lớn khác nhau được sử dụng có thể diễn giải thông tin thị giác và thính giác đã cho. Chúng tôi thấy rằng cả GPT3.5-turbo và Claude-instant-1 đều vượt trội hơn Llama2, với Claude-instant-1 có được độ chính xác cao nhất trung bình. Trong Hình 2, chúng tôi thử nghiệm tác động của việc bao gồm nhiều hơn hoặc ít hơn các chú thích khung hình. Điều thú vị là, trong khi cả GPT3.5 và Claude-1 đều hưởng lợi từ việc "nhìn thấy" nhiều chú thích hơn, hiệu suất của LLama2 bị ảnh hưởng tiêu cực. Chúng tôi giả thuyết điều này là do mô hình trở nên quá tải với thông tin dư thừa, khiến nó có khả năng chọn một từ từ các chú thích thay vì danh sách nhãn.

5. Thảo luận và Công việc tương lai
Việc sử dụng các mô hình riêng biệt để dịch thị giác và lời nói thành văn bản có thể dẫn đến không khả năng mô hình hóa các tương tác giữa các phương thức, dẫn đến ngữ cảnh không đầy đủ. Phân tích hình ảnh trên cơ sở từng khung hình cũng thiếu mô hình hóa thời gian thích hợp về các danh tính và mối quan hệ bền vững theo thời gian. Các mô hình sinh có xu hướng ảo giác và đầu ra không đáng tin cậy, do đó việc dựa vào chúng cho các tác vụ phức tạp đòi hỏi đầu ra nhất quán có thể chứng minh là thách thức trong thực tế. Hơn nữa, bằng cách chỉ cung cấp văn bản của các tên lớp cho LLM, chúng tôi đang dựa vào các tên lớp đủ mô tả. Ví dụ, UCF-101 chứa các lớp liên quan đến các nhạc cụ rất cụ thể như PlayingDhol hoặc PlayingTabla, điều này sẽ yêu cầu LLM biết rằng đây là các nhạc cụ gõ và cũng sẽ yêu cầu mô hình chú thích có thể đưa ra đủ chi tiết cụ thể để phân biệt các nhạc cụ này. Như Hình 3 cho thấy, các mô hình gặp khó khăn với các hành động khó xác định chỉ từ thông tin khung hình, như headmassage hoặc các đối tượng khó hiểu hơn, như nunchuks hay yoyo. Các chế độ lỗi thường bao gồm không khả năng của mô hình chú thích thị giác nhận ra mức độ cụ thể cần thiết để phân biệt giữa các hành động tương tự. Cuối cùng, mặc dù phương pháp của chúng tôi không hoàn toàn cạnh tranh với hiệu suất zero-shot tiên tiến 91.5 Top-1 Accuracy trên UCF-101 và 76.8 trên Kinetics400 [4], nó tổng quát hóa hơn cho các tình huống hiểu video đòi hỏi suy luận ngữ cảnh phức tạp.

Công việc tương lai bao gồm tận dụng ngữ cảnh bổ sung, như bình luận video [10] hoặc sử dụng phương pháp dựa trên chat nơi mô-đun "suy luận" có thể hỏi mô-đun "nhận thức" để làm rõ để có thêm thông tin.

--- TRANG 4 ---
[Hình 3 hiển thị biểu đồ hiệu suất của các lớp UCF-101 tốt nhất và tệ nhất giữa 3 mô hình ngôn ngữ được sử dụng: Claude-instant-1, GPT3.5-turbo, Llama2-13B]

6. Kết luận
Trong công trình này, chúng tôi đã giới thiệu một khung mới cho phân loại video đa phương thức tận dụng văn bản như phương tiện chính để kết hợp tín hiệu qua các phương thức. Chúng tôi chứng minh lần đầu tiên rằng việc kết nối các mô hình nhận thức cho thị giác, lời nói và âm thanh với các mô hình ngôn ngữ lớn có thể cho phép phân loại video zero-shot chỉ sử dụng các biểu diễn văn bản của tín hiệu đa phương thức. Công trình của chúng tôi nhấn mạnh tiềm năng của việc sử dụng ngôn ngữ tự nhiên như một giao diện linh hoạt để tích hợp tín hiệu qua các phương thức.

Tài liệu tham khảo
[1] https://github.com/guillaumekln/faster-whisper. 2
[2] https://platform.openai.com/docs/guides/gpt/chat-completions-api. 2
[3] https://www.anthropic.com/index/introducing-claude. 2
[4] H. Akbari, D. Kondratyuk, Y. Cui, R. Hornung, H. Wang, and H. Adam. Alternating gradient descent and mixture-of-experts for integrated multimodal perception. arXiv preprint arXiv:2305.06324, 2023. 3
[5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022. 1
[6] W. Berrios, G. Mittal, T. Thrush, D. Kiela, and A. Singh. Towards language models that can see: Computer vision through the lens of natural language. arXiv:2306.16410, 2023. 1
[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 1
[8] J. Chen, D. Zhu, K. Haydarov, X. Li, and M. Elhoseiny. Video chatcaptioner: Towards the enriched spatiotemporal descriptions. arXiv:2304.04227, 2023. 2
[9] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 2
[10] L. Hanu, J. Thewlis, Y. M. Asano, and C. Rupprecht. Vtc: Improving video-text retrieval with user comments. In ECCV, 2022. 3
[11] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, Q. Liu, et al. Language is not all you need: Aligning perception with language models. arXiv:2302.14045, 2023. 1
[12] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C. Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, et al. Perceiver IO: A general architecture for structured inputs & outputs. arXiv:2107.14795, 2021. 1
[13] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv:2301.12597, 2023. 1, 2
[14] OpenAI. GPT-4 technical report, 2023. 1
[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1
[16] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. In ICML, 2023. 2
[17] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c, D. Hesslow, R. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv:2211.05100, 2022. 1
[18] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. 1, 2
[19] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv:2304.10592, 2023. 1
