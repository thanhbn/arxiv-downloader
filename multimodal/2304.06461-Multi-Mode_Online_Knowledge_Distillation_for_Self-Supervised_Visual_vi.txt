# Chưng cất tri thức trực tuyến đa phương thức cho việc học biểu diễn hình ảnh tự giám sát

**Tóm tắt**

Học tự giám sát (SSL) đã đạt được những tiến bộ đáng kể trong học biểu diễn hình ảnh. Một số nghiên cứu kết hợp SSL với chưng cất tri thức (SSL-KD) để tăng cường hiệu suất học biểu diễn của các mô hình nhỏ. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp chưng cất tri thức trực tuyến đa phương thức (MOKD) để tăng cường học biểu diễn hình ảnh tự giám sát. Khác với các phương pháp SSL-KD hiện có chuyển giao tri thức từ một giáo viên tĩnh đã được huấn luyện trước đến một học sinh, trong MOKD, hai mô hình khác nhau học cộng tác theo cách tự giám sát. Cụ thể, MOKD bao gồm hai phương thức chưng cất: phương thức tự chưng cất và phương thức chưng cất chéo. Trong đó, tự chưng cất thực hiện học tự giám sát cho mỗi mô hình độc lập, trong khi chưng cất chéo thực hiện tương tác tri thức giữa các mô hình khác nhau. Trong chưng cất chéo, một chiến lược tìm kiếm đặc trưng với sự chú ý chéo được đề xuất để tăng cường sự căn chỉnh đặc trưng ngữ nghĩa giữa các mô hình khác nhau. Kết quả là, hai mô hình có thể hấp thụ tri thức từ nhau để tăng cường hiệu suất học biểu diễn của chúng. Các kết quả thí nghiệm mở rộng trên các backbone và bộ dữ liệu khác nhau chứng minh rằng hai mô hình không đồng nhất có thể được hưởng lợi từ MOKD và vượt trội hơn so với đường cơ sở được huấn luyện độc lập. Ngoài ra, MOKD cũng vượt trội hơn các phương pháp SSL-KD hiện có cho cả mô hình học sinh và giáo viên.

**1. Giới thiệu**

Do hiệu suất hứa hẹn của học biểu diễn hình ảnh không giám sát trong nhiều tác vụ thị giác máy tính, học tự giám sát (SSL) đã thu hút sự chú ý rộng rãi từ cộng đồng thị giác máy tính. SSL nhằm học các biểu diễn tổng quát có thể được chuyển giao đến các tác vụ xuôi dòng bằng cách sử dụng dữ liệu không nhãn khổng lồ.

Trong số các phương pháp SSL khác nhau, học tương phản đã cho thấy tiến bộ đáng kể trong việc thu hẹp khoảng cách hiệu suất với các phương pháp có giám sát trong những năm gần đây. Nó nhằm tối đa hóa độ tương tự giữa các góc nhìn từ cùng một thực thể (cặp tích cực) trong khi tối thiểu hóa độ tương tự giữa các góc nhìn từ các thực thể khác nhau (cặp tiêu cực). MoCo và SimCLR sử dụng cả cặp tích cực và tiêu cực để tương phản. Chúng cải thiện hiệu suất đáng kể so với các phương pháp trước đó. Sau đó, nhiều phương pháp được đề xuất để giải quyết các hạn chế trong học tương phản, chẳng hạn như vấn đề tiêu cực sai, hạn chế của kích thước batch lớn, và vấn đề của các mẫu tăng cường khó. Cùng lúc đó, các nghiên cứu khác bỏ qua các mẫu tiêu cực trong quá trình học tương phản. Với các mô hình tương đối lớn, chẳng hạn như ResNet50 hoặc lớn hơn, các phương pháp này đạt được hiệu suất tương đương trên các tác vụ khác nhau so với các đối tác có giám sát. Tuy nhiên, như được tiết lộ trong các nghiên cứu trước đó, chúng không hoạt động tốt trên các mô hình nhỏ và có khoảng cách lớn so với các đối tác có giám sát.

Để giải quyết thách thức này trong học tương phản, một số nghiên cứu đề xuất kết hợp chưng cất tri thức với học tương phản (SSL-KD) để cải thiện hiệu suất của các mô hình nhỏ. Các phương pháp này đầu tiên huấn luyện một mô hình lớn hơn theo cách tự giám sát và sau đó chưng cất tri thức của mô hình giáo viên đã huấn luyện cho một mô hình học sinh nhỏ hơn. Có một hạn chế trong các phương pháp SSL-KD này, tức là tri thức được chưng cất cho mô hình học sinh từ mô hình giáo viên tĩnh theo cách một chiều. Mô hình giáo viên không thể hấp thụ tri thức từ mô hình học sinh để tăng cường hiệu suất của nó.

Trong nghiên cứu này, chúng tôi đề xuất một phương pháp chưng cất tri thức trực tuyến đa phương thức (MOKD), như được minh họa trong Hình 1, để tăng cường hiệu suất học biểu diễn của hai mô hình đồng thời. Khác với các phương pháp SSL-KD hiện có chuyển giao tri thức từ một giáo viên tĩnh đã được huấn luyện trước đến một học sinh, trong MOKD, hai mô hình khác nhau học cộng tác theo cách tự giám sát. Cụ thể, MOKD bao gồm một phương thức tự chưng cất và một phương thức chưng cất chéo. Trong đó, tự chưng cất thực hiện học tự giám sát cho mỗi mô hình độc lập, trong khi chưng cất chéo thực hiện tương tác tri thức giữa các mô hình khác nhau. Ngoài ra, một chiến lược tìm kiếm đặc trưng với sự chú ý chéo được đề xuất trong chưng cất chéo để tăng cường sự căn chỉnh đặc trưng ngữ nghĩa giữa các mô hình khác nhau. Các kết quả thí nghiệm mở rộng trên các backbone và bộ dữ liệu khác nhau chứng minh rằng các cặp mô hình đều có thể được hưởng lợi từ MOKD và vượt trội hơn so với đường cơ sở được huấn luyện độc lập. Ví dụ, khi được huấn luyện với ResNet và ViT, hai mô hình có thể hấp thụ tri thức từ nhau, và các biểu diễn của hai mô hình thể hiện đặc điểm của nhau. Ngoài ra, MOKD cũng vượt trội hơn các phương pháp SSL-KD hiện có cho cả mô hình học sinh và giáo viên.

Các đóng góp của nghiên cứu này gồm ba phần:

• Chúng tôi đề xuất một phương pháp chưng cất tri thức trực tuyến tự giám sát mới, tức là MOKD.

• MOKD có thể tăng cường hiệu suất của hai mô hình đồng thời, đạt được hiệu suất học tương phản tiên tiến nhất trên các mô hình khác nhau.

• MOKD đạt được hiệu suất SSL-KD tiên tiến nhất.

**2. Các công trình liên quan**

**2.1. Chưng cất tri thức**

Chưng cất tri thức nhằm chưng cất tri thức từ một mô hình giáo viên lớn hơn cho một mô hình học sinh nhỏ hơn để cải thiện hiệu suất của mô hình học sinh. Nhiều nghiên cứu đã được đề xuất trong những năm gần đây, có thể được chia thành ba nhóm, tức là dựa trên logit, dựa trên đặc trưng, và dựa trên quan hệ, theo các loại tri thức.

Chưng cất tri thức dựa trên logit sử dụng logit của mô hình giáo viên làm tri thức. Trong chưng cất tri thức vanilla, mô hình học sinh bắt chước logit của mô hình giáo viên bằng cách tối thiểu hóa phân kỳ KL của phân phối lớp. Các phương pháp dựa trên đặc trưng sử dụng đầu ra của các lớp trung gian, tức là bản đồ đặc trưng, làm tri thức để giám sát việc huấn luyện mô hình học sinh. Chưng cất tri thức dựa trên quan hệ chưng cất quan hệ giữa các mẫu thay vì một thực thể duy nhất.

Các phương pháp được đề cập ở trên thực hiện chưng cất ngoại tuyến. Một số nghiên cứu được phát triển để thực hiện chưng cất trực tuyến, tức là mô hình giáo viên và mô hình học sinh được huấn luyện đồng thời. Học tương hỗ sâu được đề xuất đầu tiên để huấn luyện nhiều mô hình cộng tác. Sau đó, các nghiên cứu được đề xuất để cải thiện học tương hỗ sâu về khả năng tổng quát hóa và hiệu quả tính toán. Tất cả các phương pháp này đều được huấn luyện theo cách có giám sát.

**2.2. Chưng cất tri thức tự giám sát**

Do cải thiện đáng kể cho các mô hình nhỏ, chưng cất tri thức được đưa vào học tự giám sát để cải thiện hiệu suất của các mô hình nhỏ. CRD kết hợp tổn thất tương phản với chưng cất tri thức để chuyển giao tri thức cấu trúc của mô hình giáo viên. SimCLR-v2 đề xuất huấn luyện một mô hình lớn hơn thông qua học tự giám sát trước và sử dụng mô hình lớn được tinh chỉnh có giám sát để chưng cất một mô hình nhỏ hơn thông qua học tự giám sát. SSKD kết hợp học tự giám sát với học có giám sát để chuyển giao tri thức phong phú hơn. Compress và SEED chuyển giao tri thức của phân phối xác suất theo cách tự giám sát bằng cách sử dụng ngân hàng bộ nhớ trong MoCo. SimReg trực tiếp tiến hành chưng cất đặc trưng bằng cách tối thiểu hóa khoảng cách Euclidean bình phương giữa các đặc trưng của giáo viên và học sinh. Trong khi ReKD chuyển giao tri thức quan hệ cho học sinh. DisCo đề xuất chuyển giao các nhúng cuối cùng của một giáo viên được huấn luyện trước tự giám sát. Có một hạn chế trong các phương pháp SSL-KD này, tức là tri thức được chưng cất cho một mô hình học sinh từ một mô hình giáo viên tĩnh theo cách một chiều. Mô hình giáo viên không thể hấp thụ tri thức từ mô hình học sinh. Gần đây, DoGo và MCL đã kết hợp MoCo với học tương hỗ cho SSL-KD trực tuyến. Tuy nhiên, chúng hoặc thiếu so sánh trực tiếp với các phương pháp SSL-KD trên các backbone và tác vụ chính thống hoặc không thể đảm bảo hiệu suất của các mô hình lớn hơn.

**3. Phương pháp**

Trong phần này, chúng tôi đầu tiên giới thiệu kiến trúc tổng thể của MOKD trong Phần 3.1. Sau đó, hai phương thức chưng cất của MOKD, tức là tự chưng cất và chưng cất chéo, được giới thiệu trong Phần 3.2 và Phần 3.3, tương ứng. Cuối cùng, quy trình huấn luyện và chi tiết thực hiện được giới thiệu trong Phần 3.4.

**3.1. Kiến trúc tổng thể**

Kiến trúc tổng thể của MOKD được thể hiện trong Hình 2. Trong MOKD, hai mô hình khác nhau fi (i = 1,2) được huấn luyện cộng tác theo cách tự giám sát. Có hai phương thức chưng cất tri thức: phương thức tự chưng cất và phương thức chưng cất chéo. Trong mỗi mô hình, một đầu perceptron đa lớp (MLP-Head) (Hình 2(a)) và một đầu Transformer (T-Head) (Hình 2(b)) được sử dụng để chiếu các biểu diễn đặc trưng Z được tạo ra bởi các bộ mã hóa thành các nhúng đầu ra m và t cho tự chưng cất và chưng cất chéo. Ở đây, T-Head, bao gồm một số khối Transformer, được thiết kế để tăng cường sự căn chỉnh ngữ nghĩa giữa hai mô hình. Tự chưng cất, được tiến hành giữa mỗi mô hình fi (như một học sinh) và phiên bản EMA f'i (như một giáo viên), thực hiện học tự giám sát cho mỗi mô hình độc lập. Các tổn thất tự chưng cất là Lsmi và Lsti cho MLP-Head và T-Head, tương ứng, sẽ được giới thiệu trong Phần 3.2. Trong khi chưng cất chéo, được tiến hành giữa hai mô hình, được sử dụng để tương tác tri thức giữa hai mô hình. Trong chưng cất chéo, bằng cách sử dụng cơ chế tự chú ý của T-Head, chúng tôi thiết kế một chiến lược tìm kiếm đặc trưng với sự chú ý chéo để tăng cường sự căn chỉnh ngữ nghĩa giữa các mô hình khác nhau. Các tổn thất chưng cất chéo là Lcmi và Lcti cho MLP-Head và T-Head, tương ứng, sẽ được giới thiệu trong Phần 3.3. Ở đây, chỉ số dưới s và c đại diện cho tự chưng cất và chưng cất chéo, tương ứng. Và chỉ số dưới m và t đại diện cho MLP-Head và T-Head, tương ứng.

**3.2. Tự chưng cất**

Tự chưng cất thực hiện tác vụ học tương phản cho mỗi mô hình độc lập. Trong nghiên cứu này, chúng tôi thiết kế tự chưng cất dựa trên phương pháp học tương phản DINO. Cụ thể, lấy mô hình1 làm ví dụ. Cho hai phép tăng cường (xa và xb) của một hình ảnh đầu vào x, bộ mã hóa backbone f1 và phiên bản EMA của nó (bộ mã hóa momentum f'1) mã hóa chúng thành các biểu diễn: Za1 = f1(xa), Z'b1 = f'1(xb). Các biểu diễn là các bản đồ đặc trưng (cho mạng nơ-ron tích chập (CNN)) hoặc token (cho vision transformer) trước khi pooling trung bình toàn cục. Sau đó, các biểu diễn được pooling trung bình toàn cục và đưa vào MLP-Head tương ứng để thu được các nhúng cuối cùng ma1 và m'b1. ma1, m'b1 ∈ RK, K là chiều đầu ra. Các nhúng được chuẩn hóa với hàm softmax:

mi a1 = exp(mi a1/τ) / ΣK k=1 exp(mk a1/τ)   (1)

trong đó τ > 0 là một tham số nhiệt độ kiểm soát độ nhọn của phân phối đầu ra. Lưu ý rằng m'b1 cũng được chuẩn hóa với hàm softmax tương tự với nhiệt độ τ'. xa và xb được đưa vào bộ mã hóa momentum và bộ mã hóa một cách đối xứng và m'a1 và mb1 được thu được tương ứng. Theo DINO, tổn thất entropy chéo được sử dụng làm tổn thất tương phản. Tác vụ này là một quy trình tự chưng cất động trong đó học sinh (bộ mã hóa) và giáo viên (bộ mã hóa momentum) có cùng kiến trúc. Tổn thất tự chưng cất tương tự có thể được tính toán cho mô hình2, như sau:

Lsm1 = -1/2(m'b1 log(ma1) + m'a1 log(mb1))
Lsm2 = -1/2(m'b2 log(ma2) + m'a2 log(mb2))   (2)

Theo DINO, chúng tôi cũng sử dụng cùng chiến lược whitening để tránh sự sụp đổ mô hình và multi-crop để làm phong phú phép tăng cường.

Như được thể hiện trong Hình 2(b), tự chưng cất cũng được tiến hành trên các nhúng đầu ra t của T-Head để ổn định việc huấn luyện T-Head. Một giải thích chi tiết sẽ được giới thiệu trong Phần 3.3. Cụ thể, các biểu diễn Z được đưa vào T-Head tương ứng để thu được các nhúng cuối cùng t. t ∈ RK, K là chiều đầu ra. Sau cùng hoạt động softmax trong Phương trình (1), tổn thất tự chưng cất của T-Head được tính toán:

Lst1 = -1/2(t'b1 log(ta1) + t'a1 log(tb1))
Lst2 = -1/2(t'b2 log(ta2) + t'a2 log(tb2))   (3)

Tổn thất tự chưng cất cho mỗi mô hình là tổng các tổn thất tự chưng cất của MLP-Head và T-Head:

Lself1 = Lsm1 + Lst1
Lself2 = Lsm2 + Lst2   (4)

**3.3. Chưng cất chéo**

Chưng cất chéo thực hiện học tương tác giữa hai mô hình. Chúng tôi thiết kế hai mục tiêu học tương tác, tức là chưng cất chéo sử dụng nhúng MLP-Head và chưng cất chéo sử dụng nhúng T-Head, để thực hiện chuyển giao tri thức giữa hai mô hình.

Đối với nhúng MLP-Head, nó chứa tri thức phong phú của mỗi mô hình. Do đó, chưng cất chéo được tiến hành giữa hai mô hình để tương tác tri thức. Cụ thể, mô hình1 học tri thức từ phiên bản momentum của mô hình2 và ngược lại. Chưng cất chéo có thể được tính toán như sau:

Lcm1 = -1/2(m'b2 log(ma1) + m'a2 log(mb1))
Lcm2 = -1/2(m'b1 log(ma2) + m'a1 log(mb2))   (5)

Chưng cất chéo được tiến hành giữa các góc nhìn khác nhau và các mô hình khác nhau (trực tuyến và một mô hình momentum khác), điều này có hai ưu điểm. Thứ nhất, chưng cất chéo giữa các góc nhìn khác nhau có thể nới lỏng ràng buộc cho cùng một góc nhìn và hữu ích để tránh sự đồng nhất hóa của hai mô hình. Thứ hai, chưng cất chéo giữa một mô hình trực tuyến và một mô hình momentum khác, thay vì hai mô hình trực tuyến, cung cấp việc huấn luyện ổn định hơn vì mô hình momentum ổn định hơn.

Trong quá trình chưng cất chéo, việc chuyển giao tri thức của các đặc trưng liên quan đến ngữ nghĩa từ các góc nhìn khác nhau cần được tăng cường trong khi các đặc trưng không liên quan cần được ức chế. Để làm điều này, tìm kiếm đặc trưng với sự chú ý chéo được đề xuất để tìm kiếm các đặc trưng liên quan đến ngữ nghĩa từ nhau để chuyển giao tri thức một cách thích ứng. Như được thể hiện trong Hình 3, T-Head được thiết kế để áp dụng cơ chế tự chú ý trong Transformer để thực hiện tìm kiếm đặc trưng.

Lấy tìm kiếm đặc trưng với sự chú ý chéo giữa mô hình1 và mô hình momentum2 làm ví dụ. Chúng tôi nhằm tìm kiếm các đặc trưng liên quan đến ngữ nghĩa giữa đặc trưng Za1 (Za1 ∈ RN1×C1, N1 biểu thị số lượng đặc trưng cục bộ, là tích của chiều rộng và chiều cao của bản đồ đặc trưng cho CNN và số lượng token cho vision transformer, và C1 biểu thị chiều của các đặc trưng cục bộ) của bộ mã hóa1 và đặc trưng Z'b2 (Z'b2 ∈ RN2×C2, N2 và C2 biểu thị thông tin tương tự cho bộ mã hóa2) của bộ mã hóa momentum2. Một hoạt động pooling trung bình toàn cục và một hoạt động tích chập 1×1 được tiến hành trên Za1 để thu được đặc trưng toàn cục của nó và thống nhất chiều của nó với Z'b2. Sau đó, za1 thu được (za1 ∈ RC2) được nối với Z'b2 và đưa vào T-Head:

[ẑa1, Ẑ'b2] = f't2([za1, Z'b2])   (6)

trong đó f't2(·) biểu thị hàm của các khối transformer trong T-Head của mô hình momentum2, và [·,·] đề cập đến hoạt động nối. Thông qua cơ chế tự chú ý trong T-Head, đặc trưng thu được Ẑ'b2 tăng cường thành phần nhất quán về ngữ nghĩa trong khi ức chế thành phần không liên quan với za1. Sau một pooling trung bình toàn cục, một lớp FC, và softmax trong T-Head, nhúng đầu ra Ẑ'b2 được sử dụng để học tương phản với nhúng ta1 của Za1. Một quy trình tìm kiếm đặc trưng với sự chú ý chéo tương tự được tiến hành giữa mô hình2 và mô hình momentum1. Tổn thất cho tìm kiếm đặc trưng với sự chú ý chéo có thể được tính toán:

Lct1 = -1/2(t̂'b2 log(ta1) + t̂'a2 log(tb1))
Lct2 = -1/2(t̂'b1 log(ta2) + t̂'a1 log(tb2))   (7)

T-Head của mô hình momentum không thể được cập nhật nếu chỉ có tổn thất tìm kiếm đặc trưng cho T-Head. Do đó, tự chưng cất cũng được tiến hành giữa các nhúng T-Head (Phương trình (3)) để cho phép cập nhật T-Head và cung cấp huấn luyện ổn định hơn. Tổn thất chưng cất chéo cho mỗi mô hình là tổng các tổn thất tương phản của MLP-Head và T-Head:

Lcross1 = Lcm1 + Lct1
Lcross2 = Lcm2 + Lct2   (8)

Tổn thất tổng thể cho mỗi mô hình là tổng có trọng số của tổn thất tự chưng cất và tổn thất chưng cất chéo:

L1 = Lself1 + λ1Lcross1
L2 = Lself2 + λ2Lcross2   (9)

trong đó 0 ≤ λ1 ≤ 1 và 0 ≤ λ2 ≤ 1 là các siêu tham số và biểu thị trọng số của tổn thất chưng cất chéo của mô hình1 và mô hình2, tương ứng.

**3.4. Chi tiết thực hiện**

**Quy trình huấn luyện.** Trong MOKD, hai mô hình được huấn luyện hợp tác. Thuật toán 1 tóm tắt quy trình huấn luyện của MOKD. Các bộ tối ưu SGD và AdamW được sử dụng cho CNN và ViT, tương ứng.

**Đầu chiếu.** MLP-Head bao gồm một MLP bốn lớp với cùng kiến trúc như DINO. T-Head bao gồm 3 khối transformer với cùng kiến trúc như ViT-Small và một lớp FC để chiếu. Chiều đầu ra của hai đầu là K = 65536.

**4. Thí nghiệm**

Trong phần này, chúng tôi tiến hành các thí nghiệm toàn diện để đánh giá hiệu quả của MOKD. Các kích thước khác nhau của CNN và vision transformer được sử dụng làm bộ mã hóa. Các mô hình không đồng nhất và đồng nhất được đánh giá. Đối với MOKD không đồng nhất, một ResNet và một ViT được sử dụng. Cụ thể, ResNet101 (R101)/ResNet50 (R50) được sử dụng cho CNN, và ViT-Base (ViT-B)/ViT-Small (ViT-S) được sử dụng cho vision transformer. Đối với MOKD đồng nhất, hai CNN hoặc hai ViT được sử dụng. Đối với hai CNN, R101/R50 được sử dụng cho mô hình lớn hơn, ResNet34 (R34)/ResNet18 (R18) được sử dụng cho mô hình nhỏ hơn. Đối với hai ViT, ViT-S và ViT-Tiny (ViT-T) được sử dụng. Ngoài ra, chúng tôi cũng tiến hành thí nghiệm cho hai mô hình có cùng kiến trúc, bao gồm R50, R18, và ViT-S. Tất cả các mô hình được huấn luyện trên tập huấn luyện ImageNet. Không có tuyên bố cụ thể, cài đặt mặc định là 256 batch size và 100 epoch. Chúng tôi theo hầu hết các cài đặt siêu tham số của DINO. Thêm chi tiết cho các thí nghiệm có thể được tìm thấy trong Tài liệu Bổ sung.

**4.1. Thí nghiệm trên ImageNet**

Sau khi huấn luyện trước, k-NN và linear probing được sử dụng để đánh giá hiệu suất biểu diễn. Đối với linear probing, một bộ phân loại tuyến tính được thêm vào backbone đã đóng băng được huấn luyện trong 100 epoch. Độ chính xác top-1 trên tập xác thực được áp dụng làm metric đánh giá.

**Độ chính xác k-NN và Linear Probing.** MOKD được so sánh với đường cơ sở trong đó hai mô hình được huấn luyện độc lập sử dụng DINO. Kết quả được thể hiện trong Bảng 1. Đối với các mô hình không đồng nhất (ResNet-ViT), MOKD cải thiện đáng kể hiệu suất của hai mô hình so với các mô hình được huấn luyện độc lập. Ví dụ, với R50-ViT-B, độ chính xác linear probing của hai mô hình cải thiện 3.5% (từ 72.1% lên 75.6%) và 1.0% (từ 77.0% lên 78.0%), tương ứng. Với các mô hình đồng nhất (hai ResNet và hai ViT), MOKD cải thiện hiệu suất của mô hình nhỏ hơn với biên độ lớn. Kết quả thí nghiệm chứng minh rằng MOKD có thể chuyển giao tri thức một cách hiệu quả giữa các mô hình khác nhau để tăng cường hiệu suất biểu diễn.

**So sánh với các phương pháp SSL.** Hiệu suất của MOKD cho các mô hình khác nhau được so sánh với các phương pháp SSL xuất sắc khác. Hầu hết các phương pháp học tương phản tiến hành thí nghiệm sử dụng R50, và một số phương pháp sử dụng ViT-S và ViT-B. Do đó, chúng tôi so sánh hiệu suất của ba mô hình. Như được thể hiện trong Bảng 2, MOKD đạt được hiệu suất tốt nhất cho R50, ViT-S, và ViT-B.

**So sánh với các phương pháp SSL-KD.** MOKD cũng được so sánh với các phương pháp SSL-KD khác, bao gồm SEED, ReKD, MCL, và DisCo. Theo SEED và DisCo, R101 và R50 được sử dụng làm mô hình lớn hơn (hoặc mô hình giáo viên), và R34 và R18 được sử dụng làm mô hình nhỏ hơn (hoặc mô hình học sinh). Độ chính xác linear probing được báo cáo. Kết quả được thể hiện trong Bảng 3. MOKD đạt được hiệu suất tốt nhất cho tất cả các mô hình, vượt trội hơn phương pháp tiên tiến nhất DisCo. Ví dụ, với R50-R34, MOKD đạt được 67.0% cho R34, vượt trội hơn DisCo với biên độ 4.5%.

**4.2. Học bán giám sát**

Trong phần này, chúng tôi đánh giá hiệu suất của MOKD trong thiết lập bán giám sát. Cụ thể, chúng tôi sử dụng các tập con 1% và 10% của tập huấn luyện ImageNet để tinh chỉnh, theo giao thức bán giám sát trong SimCLR. Các mô hình được tinh chỉnh với 1024 batch size trong 60 epoch và 30 epoch trên các tập con 1% và 10%, tương ứng. Độ chính xác top-1 được sử dụng. Kết quả được báo cáo trong Bảng 4. Tinh chỉnh sử dụng 1% và 10% dữ liệu huấn luyện, MOKD cải thiện hiệu suất của hai mô hình với biên độ lớn so với các mô hình được huấn luyện trước độc lập.

**4.3. Chuyển giao đến Cifar10/Cifar100**

Chúng tôi tiếp tục tinh chỉnh các mô hình được huấn luyện trước trên các bộ dữ liệu Cifar10 và Cifar100 để phân tích khả năng tổng quát hóa của các biểu diễn thu được bởi MOKD. Các mô hình được tinh chỉnh với 1024 batch size trong 100 epoch. Độ chính xác top-1 được sử dụng. Như được thể hiện trong Bảng 5, MOKD vượt trội hơn đường cơ sở huấn luyện trước độc lập với các mô hình khác nhau trên cả Cifar10 và Cifar100. Thí nghiệm này cho thấy khả năng tổng quát hóa tốt của MOKD.

**4.4. Chuyển giao đến phát hiện và phân đoạn**

Trong phần này, chúng tôi đánh giá biểu diễn của MOKD trên các tác vụ dự đoán dày đặc, tức là phát hiện đối tượng và phân đoạn thực thể, trên bộ dữ liệu MS COCO. Chúng tôi sử dụng tập train2017 để huấn luyện và đánh giá trên tập val2017. Theo SEED, Mask R-CNN dựa trên C4 được sử dụng cho phát hiện đối tượng và phân đoạn thực thể trên COCO. Và R34 được sử dụng làm backbone, được khởi tạo bởi các mô hình được huấn luyện trước. Triển khai của chúng tôi dựa trên detectron2. Kết quả thí nghiệm được thể hiện trong Bảng 6. Có thể thấy rằng MOKD đạt được hiệu suất tốt nhất, chứng minh rằng MOKD có khả năng tổng quát hóa tốt trên các tác vụ dự đoán dày đặc.

**4.5. Nghiên cứu loại bỏ**

Trong phần này, chúng tôi phân tích ảnh hưởng của mỗi thành phần trong MOKD. Bộ dữ liệu ImageNet100, chứa 100 danh mục được chọn ngẫu nhiên từ ImageNet, được áp dụng để tăng tốc thời gian huấn luyện. Tất cả các mô hình được huấn luyện trên tập huấn luyện ImageNet100 với 256 batch size và 200 epoch và được kiểm tra trên tập xác thực. Độ chính xác linear probing top-1 được sử dụng làm metric đánh giá.

**Hiệu quả của mỗi thành phần tổn thất.** Có bốn thành phần tổn thất cho mỗi mô hình trong MOKD, tức là Lsm, Lst, Lcm, và Lct trong Phương trình (2), Phương trình (3), Phương trình (5), và Phương trình (7), tương ứng. Chúng tôi sử dụng R50-ViT-S để phân tích ảnh hưởng của mỗi thành phần tổn thất. Kết quả được thể hiện trong Bảng 7. Lưu ý rằng kết quả với chỉ Lsm là đường cơ sở DINO huấn luyện hai mô hình độc lập. Với việc thêm Lcm, Lst, và Lct, hiệu suất cải thiện dần dần, điều này cho thấy rằng tự chưng cất và chưng cất chéo của MLP-head và T-Head có thể có lợi cho hiệu suất biểu diễn của MOKD.

**Ảnh hưởng của λ1 và λ2.** λ1 và λ2 trong Phương trình (9) là trọng số của tổn thất chưng cất chéo của mô hình1 và mô hình2, tương ứng. Bảng 8 thể hiện kết quả. Như được thể hiện trong hàng đầu tiên, khi λ2 = 1 cho ViT-S, hiệu suất của R50 trở nên tệ hơn với sự gia tăng của λ1, điều này cho thấy rằng tốt hơn là đặt một giá trị nhỏ cho mô hình có hiệu suất tốt hơn. Khi λ1 = 0, R50 được huấn luyện mà không có chưng cất chéo và đạt được cải thiện không đáng kể, điều này cho thấy rằng chưng cất chéo là cần thiết cho mô hình có hiệu suất tốt hơn. Như được thể hiện trong hàng thứ ba, khi λ1 = 0.1 cho R50, hiệu suất của ViT-S cải thiện với sự gia tăng của λ2, điều này chứng minh rằng chú trọng nhiều hơn vào chưng cất chéo có lợi cho mô hình có hiệu suất kém hơn. Trong nghiên cứu này, λ1 và λ2 được đặt là 1 và 0.1 cho các mô hình lớn hơn và nhỏ hơn, tương ứng. Trong khi đối với các cặp mô hình có cùng backbone, λ1 và λ2 đều được đặt là 1.

**4.6. Trực quan hóa và phân tích**

**MOKD có làm cho các mô hình trở nên tương tự hơn không?** Trong phần này, chúng tôi phân tích các biểu diễn được học bởi MOKD. Đầu tiên, chúng tôi phân tích liệu các biểu diễn của hai mô hình được huấn luyện bởi MOKD có xu hướng trở nên tương tự hơn hay không. Để làm điều này, tỷ lệ các mẫu mà hai cặp mô hình đưa ra cùng một dự đoán được tính toán. Cấu hình R50-ViT-S, được huấn luyện trước trên ImageNet, được sử dụng, và k-NN được áp dụng trên ImageNet100. Như được thể hiện trong Bảng 9, tỷ lệ nhất quán dự đoán của MOKD tăng so với kết quả huấn luyện độc lập trong cột thứ hai. Tuy nhiên, sự gia tăng này chủ yếu do cải thiện hiệu suất của hai mô hình so với tỷ lệ nhất quán trong cột cuối cùng, được thu được bằng cách thay thế mô hình R50 được huấn luyện bởi MOKD bằng một mô hình R50 với độ chính xác tương đối được huấn luyện bởi DINO. Kết quả xác minh rằng không có xu hướng đáng kể để các biểu diễn của hai mô hình được huấn luyện bởi MOKD trở nên tương tự hơn. Hình 4 trực quan hóa các phân phối đặc trưng của hai mô hình được huấn luyện bởi MOKD. Hai mô hình thể hiện các phân phối đặc trưng khác nhau, điều này tiếp tục xác minh rằng MOKD không làm cho các mô hình trở nên tương tự hơn. Thêm kết quả có thể được tìm thấy trong Hình S1 trong Tài liệu Bổ sung.

**Tri thức nào được học trong MOKD?** Chúng tôi điều tra đặc điểm của các đặc trưng trên mỗi lớp để phân tích sự khác biệt giữa các mô hình được huấn luyện độc lập và được huấn luyện bởi MOKD. Cụ thể, chúng tôi tính toán khoảng cách chú ý trung bình (MAD) cho các mô hình ViT. Đối với các mô hình CNN, một khoảng cách trung bình tương tự cũng có thể được tính toán trên bản đồ đặc trưng dựa trên trọng số tự chú ý của nó. Như được thể hiện trong Hình 5(a)(b), chúng tôi phát hiện rằng MAD trên các lớp sâu (lớp 10-12) của mô hình ViT-S được huấn luyện bởi MOKD (với R50) giảm so với những mô hình của mô hình ViT-S được huấn luyện độc lập, điều này cho thấy rằng mô hình ViT-S được huấn luyện bởi MOKD có xu hướng "cục bộ" hơn trên các lớp sâu. Hiện tượng ngược lại có thể được thấy trên các ResNet. Như được thể hiện trong Hình 5(c)(d), MAD trên mỗi lớp của các mô hình ResNet được huấn luyện bởi MOKD (với ViT) tăng so với những mô hình của các mô hình ResNet được huấn luyện độc lập, điều này cho thấy rằng các mô hình ResNet được huấn luyện bởi MOKD (với ViT) có xu hướng "toàn cục" hơn. Tuy nhiên, hiện tượng này không được thể hiện trong hai mô hình ResNet được huấn luyện bởi MOKD. Tức là, thông qua MOKD, hai mô hình không đồng nhất hấp thụ tri thức từ nhau, tức là mô hình ViT học được tính cục bộ hơn trong khi mô hình CNN học được thông tin toàn cục hơn. Thêm kết quả có thể được tìm thấy trong Hình S2 trong Tài liệu Bổ sung.

**5. Kết luận**

Trong nghiên cứu này, chúng tôi đề xuất phương pháp MOKD, trong đó hai mô hình khác nhau học cộng tác thông qua tự chưng cất và chưng cất chéo theo cách tự giám sát. Các thí nghiệm mở rộng trên các backbone và tác vụ khác nhau chứng minh rằng MOKD có thể tăng cường hiệu suất biểu diễn đặc trưng của các mô hình khác nhau. Nó đạt được hiệu suất tiên tiến nhất cho chưng cất tri thức tự giám sát. Chúng tôi hy vọng nghiên cứu này có thể truyền cảm hứng cho việc tăng cường hiệu suất học biểu diễn thông qua tương tác tri thức giữa các mô hình không đồng nhất.

Là một phương pháp chưng cất tri thức trực tuyến, hạn chế chính của MOKD là mô hình lớn hơn cần được huấn luyện lặp đi lặp lại cho các mô hình nhỏ hơn khác nhau, điều này đòi hỏi nhiều chi phí tính toán hơn so với chưng cất tri thức ngoại tuyến. Cách thiết kế một MOKD hiệu quả có thể được nghiên cứu thêm. Ví dụ, việc đưa các phương pháp tinh chỉnh hiệu quả vào MOKD có thể là một nghiên cứu tương lai có thể để thực hiện MOKD hiệu quả.
