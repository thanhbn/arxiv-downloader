# 2310.09478.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.09478.pdf
# Kích thước file: 4455546 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
MiniGPT-v2: Mô hình Ngôn ngữ Lớn như một Giao diện Thống nhất
cho Việc Học Đa nhiệm Thị giác-Ngôn ngữ
Jun Chen1,2∗Deyao Zhu1Xiaoqian Shen1Xiang Li1Zechun Liu2Pengchuan Zhang2
Raghuraman Krishnamoorthi2Vikas Chandra2Yunyang Xiong2†Mohamed Elhoseiny1†
1Đại học Khoa học và Công nghệ Vua Abdullah (KAUST)
2Meta AI Research
Tóm tắt
Các mô hình ngôn ngữ lớn đã cho thấy khả năng đáng chú ý của chúng như một giao diện chung cho các ứng dụng liên quan đến ngôn ngữ khác nhau. Được thúc đẩy bởi điều này, chúng tôi hướng đến việc xây dựng một giao diện thống nhất để hoàn thành nhiều nhiệm vụ thị giác-ngôn ngữ bao gồm mô tả hình ảnh, trả lời câu hỏi thị giác, và định vị thị giác, cùng các nhiệm vụ khác. Thách thức là sử dụng một mô hình duy nhất để thực hiện các nhiệm vụ thị giác-ngôn ngữ đa dạng một cách hiệu quả với các hướng dẫn đa phương thức đơn giản. Hướng đến mục tiêu này, chúng tôi giới thiệu MiniGPT-v2, một mô hình có thể được coi như một giao diện thống nhất để xử lý tốt hơn các nhiệm vụ thị giác-ngôn ngữ khác nhau. Chúng tôi đề xuất sử dụng các định danh duy nhất cho các nhiệm vụ khác nhau khi huấn luyện mô hình. Các định danh này cho phép mô hình của chúng tôi phân biệt tốt hơn từng hướng dẫn nhiệm vụ một cách dễ dàng và cũng cải thiện hiệu quả học tập của mô hình cho mỗi nhiệm vụ. Sau quá trình huấn luyện ba giai đoạn, kết quả thực nghiệm cho thấy MiniGPT-v2 đạt được hiệu suất mạnh mẽ trên nhiều bài đánh giá trả lời câu hỏi thị giác và định vị thị giác so với các mô hình tổng quát thị giác-ngôn ngữ khác. Mô hình và mã nguồn của chúng tôi có sẵn tại https://minigpt-v2.github.io/ .

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn Đa phương thức (LLM) đã nổi lên như một chủ đề nghiên cứu thú vị với một tập hợp phong phú các ứng dụng trong cộng đồng thị giác-ngôn ngữ, như trợ lý AI thị giác, chú thích hình ảnh, trả lời câu hỏi thị giác (VQA), và hiểu biểu thức tham chiếu (REC). Một đặc điểm chính của các mô hình ngôn ngữ lớn đa phương thức là chúng có thể kế thừa các khả năng tiên tiến (ví dụ, lập luận logic, tri thức thông thường, và khả năng biểu đạt ngôn ngữ mạnh mẽ) từ các LLM [32,49,50,8]. Khi được điều chỉnh với các hướng dẫn thị giác-ngôn ngữ phù hợp, các LLM đa phương thức, đặc biệt là các mô hình thị giác-ngôn ngữ, thể hiện khả năng mạnh mẽ như tạo ra các mô tả hình ảnh chi tiết, sinh mã, định vị các đối tượng thị giác trong hình ảnh, và thậm chí thực hiện lập luận đa phương thức để trả lời tốt hơn các câu hỏi thị giác phức tạp [59,26,55,53,7,10,58,6,60]. Sự phát triển này của các LLM cho phép tương tác giữa đầu vào thị giác và ngôn ngữ thông qua giao tiếp với cá nhân và đã được chứng minh là khá hiệu quả để xây dựng các chatbot thị giác.

Tuy nhiên, việc học thực hiện nhiều nhiệm vụ thị giác-ngôn ngữ một cách hiệu quả và xây dựng các hướng dẫn đa phương thức tương ứng của chúng đặt ra những thách thức đáng kể do sự phức tạp vốn có giữa các nhiệm vụ khác nhau. Ví dụ, với đầu vào của người dùng "hãy cho tôi biết vị trí của một người", có nhiều cách để diễn giải và phản hồi dựa trên nhiệm vụ cụ thể. Trong bối cảnh nhiệm vụ hiểu biểu thức tham chiếu, nó có thể được trả lời bằng một vị trí khung giới hạn của người đó. Đối với nhiệm vụ trả lời câu hỏi thị giác, mô hình có thể mô tả vị trí không gian của họ bằng ngôn ngữ tự nhiên của con người. Đối với nhiệm vụ phát hiện người, mô hình có thể xác định mọi vị trí không gian của mỗi người trong một hình ảnh cho trước. Để giảm thiểu vấn đề này và hướng tới một cách tiếp cận thống nhất, chúng tôi đề xuất một phương án huấn luyện hướng dẫn định hướng nhiệm vụ để giảm sự mơ hồ trong hướng dẫn đa phương thức, và một mô hình thị giác-ngôn ngữ, MiniGPT-v2. Cụ thể, chúng tôi cung cấp một token định danh nhiệm vụ duy nhất cho mỗi nhiệm vụ. Ví dụ, chúng tôi cung cấp một token định danh [vqa] để huấn luyện tất cả các mẫu dữ liệu từ các nhiệm vụ trả lời câu hỏi thị giác. Tổng cộng, chúng tôi cung cấp sáu định danh nhiệm vụ khác nhau trong các giai đoạn huấn luyện mô hình.

Mô hình của chúng tôi, MiniGPT-v2, có thiết kế kiến trúc đơn giản. Nó trực tiếp nhận các token thị giác từ một bộ mã hóa thị giác ViT [12] và chiếu chúng vào không gian đặc trưng của một mô hình ngôn ngữ lớn [50]. Để có nhận thức thị giác tốt hơn, chúng tôi sử dụng hình ảnh có độ phân giải cao hơn (448x448) trong quá trình huấn luyện. Nhưng điều này sẽ dẫn đến số lượng token thị giác lớn hơn. Để làm cho việc huấn luyện mô hình hiệu quả hơn, chúng tôi nối bốn token thị giác lân cận thành một token duy nhất, giảm tổng số lượng đi 75%. Ngoài ra, chúng tôi sử dụng một chiến lược huấn luyện ba giai đoạn để huấn luyện hiệu quả mô hình của chúng tôi với hỗn hợp các bộ dữ liệu hình ảnh-văn bản được gán nhãn yếu, chi tiết, và các bộ dữ liệu hướng dẫn đa phương thức, với trọng tâm huấn luyện khác nhau ở mỗi giai đoạn.

Để đánh giá hiệu suất của mô hình, chúng tôi đã tiến hành các thí nghiệm rộng rãi trên các nhiệm vụ thị giác-ngôn ngữ đa dạng, bao gồm chú thích hình ảnh (chi tiết)/có định vị, trả lời câu hỏi thị giác, và định vị thị giác. Kết quả cho thấy MiniGPT-v2 của chúng tôi có thể đạt được hiệu suất SOTA hoặc tương đương trên các bài đánh giá đa dạng so với các mô hình tổng quát thị giác-ngôn ngữ trước đây, như MiniGPT-4 [59], InstructBLIP [10], LLaVA [26] và Shikra [7]. Ví dụ, MiniGPT-v2 của chúng tôi vượt trội MiniGPT-4 21.3%, InstructBLIP 11.3%, và LLaVA 11.7% trên bài đánh giá VSR [25], và nó cũng hoạt động tốt hơn so với đường cơ sở mạnh mẽ được thiết lập trước đó, Shikra, trong hầu hết các xác thực trên RefCOCO, RefCOCO+, và RefCOCOg. Mô hình của chúng tôi thiết lập kết quả tiên tiến mới trên các bài đánh giá này trong số các mô hình tổng quát thị giác-ngôn ngữ, được thể hiện trong Hình 1.

2 Công trình liên quan
Chúng tôi xem xét ngắn gọn các công trình liên quan về các mô hình ngôn ngữ lớn tiên tiến và LLM đa phương thức để căn chỉnh thị giác.

Các Mô hình Ngôn ngữ Lớn Tiên tiến (LLM). Các mô hình giai đoạn đầu như GPT-2 [38] và BERT [11] là những mô hình nền tảng được huấn luyện trên các bộ dữ liệu văn bản quy mô web, đánh dấu một bước đột phá trong lĩnh vực NLP. Theo sau thành công của các mô hình nền tảng, các LLM với khả năng cao hơn và dữ liệu huấn luyện tăng được phát triển, bao gồm GPT-3 [4], Megatron-turing NLG [46], PaLM [9], Gopher [39], Chinchilla [16], OPT [57], và BLOOM [41]. Gần đây nhất, các nỗ lực đã tập trung vào việc tinh chỉnh các LLM để hoạt động hiệu quả với hướng dẫn và phản hồi của con người. Các công trình tiêu biểu theo hướng này là InstructGPT [34] và ChatGPT [32], thể hiện khả năng mạnh mẽ như trả lời một loạt câu hỏi ngôn ngữ đa dạng, tham gia vào cuộc trò chuyện với con người, và học thực hiện các nhiệm vụ phức tạp như tinh chỉnh viết và trợ lý lập trình.

Đồng thời với những tiến bộ này của các LLM là sự nổi lên của các mô hình ngôn ngữ LLaMA [49]. Để cho phép khả năng tuân theo hướng dẫn của con người tương tự như ChatGPT, một số công trình cố gắng tinh chỉnh mô hình LLaMA với các bộ dữ liệu hướng dẫn chất lượng cao bổ sung [1]. Ví dụ về các mô hình này bao gồm Alpaca [47], Vicuna [8], và MPT [48]. Một số mô hình ngôn ngữ mã nguồn mở khác đã học từ dữ liệu phản hồi của con người, như Falcon [35] và LLaMA-2 [50], cũng đã được giới thiệu cho cộng đồng NLP với hiệu suất ấn tượng.

Căn chỉnh Thị giác với LLM. Với khả năng tổng quát hóa đáng chú ý của các LLM, các nghiên cứu thú vị đã mở rộng LLM sang các lĩnh vực đa phương thức bằng cách căn chỉnh đầu vào thị giác với LLM. Các công trình đầu tiên như VisualGPT [5] và Frozen [51] đã sử dụng các mô hình ngôn ngữ được huấn luyện trước để cải thiện các mô hình thị giác-ngôn ngữ trên chú thích hình ảnh và trả lời câu hỏi thị giác. Khám phá ban đầu này đã mở đường cho nghiên cứu thị giác-ngôn ngữ tiếp theo như Flamingo [2] và BLIP-2 [22]. Gần đây hơn, GPT-4 đã được phát hành và thể hiện nhiều khả năng đa phương thức tiên tiến, ví dụ, tạo mã website dựa trên hướng dẫn văn bản viết tay. Những khả năng được thể hiện đó đã truyền cảm hứng cho các LLM thị giác-ngôn ngữ khác, bao gồm MiniGPT-4 [59] và LLaVA [26], căn chỉnh đầu vào hình ảnh với một mô hình ngôn ngữ lớn, Vicuna [8], sử dụng điều chỉnh hướng dẫn phù hợp. Các mô hình thị giác-ngôn ngữ này cũng thể hiện nhiều khả năng đa phương thức tiên tiến sau khi căn chỉnh. Các công trình gần đây, như Vision-LLM [53], Kosmos-2 [36], Shikra [7], và công trình đồng thời của chúng tôi, Qwen-VL [3], cũng chứng minh rằng các mô hình LLM đa phương thức có thể thực hiện định vị thị giác bằng cách tạo ra định dạng văn bản của các khung giới hạn thông qua mô hình ngôn ngữ.

3 Phương pháp
[Hình 2: Kiến trúc của MiniGPT-v2. Mô hình nhận một backbone thị giác ViT, được giữ nguyên trong tất cả các giai đoạn huấn luyện. Chúng tôi nối bốn token đầu ra thị giác liền kề từ backbone ViT và chiếu chúng vào không gian mô hình ngôn ngữ LLaMA-2 thông qua một lớp chiếu tuyến tính.]

Chúng tôi bắt đầu bằng việc giới thiệu mô hình thị giác-ngôn ngữ của chúng tôi, MiniGPT-v2, sau đó thảo luận về ý tưởng cơ bản của một mẫu hướng dẫn đa nhiệm vụ với các định danh nhiệm vụ để huấn luyện, và cuối cùng điều chỉnh ý tưởng định danh nhiệm vụ của chúng tôi để đạt được điều chỉnh hướng dẫn định hướng nhiệm vụ.

3.1 Kiến trúc Mô hình
Kiến trúc mô hình được đề xuất của chúng tôi, MiniGPT-v2, được hiển thị trong Hình 2. Nó bao gồm ba thành phần: một backbone thị giác, một lớp chiếu tuyến tính, và một mô hình ngôn ngữ lớn. Chúng tôi mô tả từng thành phần như sau:

Backbone thị giác. MiniGPT-v2 điều chỉnh EVA [12] làm backbone mô hình thị giác của chúng tôi. Chúng tôi đóng băng backbone thị giác trong toàn bộ quá trình huấn luyện mô hình. Chúng tôi huấn luyện mô hình với độ phân giải hình ảnh 448x448, và chúng tôi nội suy mã hóa vị trí để tỷ lệ với độ phân giải hình ảnh cao hơn.

Lớp chiếu tuyến tính. Chúng tôi nhằm mục đích chiếu tất cả các token thị giác từ backbone thị giác đóng băng vào không gian mô hình ngôn ngữ. Tuy nhiên, đối với hình ảnh có độ phân giải cao hơn như 448x448, việc chiếu tất cả các token hình ảnh dẫn đến đầu vào chuỗi rất dài (ví dụ, 1024 token) và giảm đáng kể hiệu quả huấn luyện và suy luận. Do đó, chúng tôi đơn giản nối 4 token thị giác liền kề trong không gian nhúng và chiếu chúng cùng nhau thành một nhúng duy nhất trong cùng không gian đặc trưng của mô hình ngôn ngữ lớn, do đó giảm số lượng token đầu vào thị giác đi 4 lần. Với thao tác này, MiniGPT-v2 của chúng tôi có thể xử lý hình ảnh có độ phân giải cao hiệu quả hơn nhiều trong giai đoạn huấn luyện và suy luận.

Mô hình ngôn ngữ lớn. MiniGPT-v2 áp dụng LLaMA2-chat (7B) [50] mã nguồn mở làm backbone mô hình ngôn ngữ. Trong công trình của chúng tôi, mô hình ngôn ngữ được coi như một giao diện thống nhất cho các đầu vào thị giác-ngôn ngữ khác nhau. Chúng tôi trực tiếp dựa vào các token ngôn ngữ LLaMA-2 để thực hiện các nhiệm vụ thị giác-ngôn ngữ khác nhau. Đối với các nhiệm vụ định vị thị giác cần thiết tạo ra vị trí không gian, chúng tôi trực tiếp yêu cầu mô hình ngôn ngữ tạo ra các biểu diễn văn bản của khung giới hạn để biểu thị vị trí không gian của chúng.

3.2 Mẫu Hướng dẫn Đa nhiệm vụ
Khi huấn luyện một mô hình thống nhất duy nhất cho nhiều nhiệm vụ khác nhau như trả lời câu hỏi thị giác, chú thích hình ảnh, biểu thức tham chiếu, chú thích hình ảnh có định vị, và nhận dạng vùng, mô hình đa phương thức có thể không phân biệt được từng nhiệm vụ chỉ bằng cách căn chỉnh token thị giác với mô hình ngôn ngữ. Ví dụ, khi bạn hỏi "Hãy cho tôi biết vị trí không gian của người mặc áo khoác đỏ?", mô hình có thể trả lời bạn vị trí theo định dạng khung giới hạn (ví dụ, <Xleft><Ytop><Xright><Ybottom>) hoặc mô tả vị trí đối tượng bằng ngôn ngữ tự nhiên (ví dụ, góc trên bên phải). Để giảm sự mơ hồ như vậy và làm cho mỗi nhiệm vụ dễ phân biệt, chúng tôi giới thiệu các token đặc thù nhiệm vụ trong mẫu hướng dẫn đa nhiệm vụ được thiết kế của chúng tôi để huấn luyện. Chúng tôi bây giờ mô tả chi tiết mẫu hướng dẫn đa nhiệm vụ của chúng tôi.

Định dạng đầu vào chung. Chúng tôi tuân theo thiết kế mẫu hội thoại LLaMA-2 và điều chỉnh nó cho mẫu hướng dẫn đa phương thức. Mẫu được biểu thị như sau:
[INST] <Img> < ImageFeature > </Img>[Task Identifier] Instruction [/INST]

Trong mẫu này, [INST] được coi là vai trò người dùng, và [/INST] được coi là vai trò trợ lý. Chúng tôi cấu trúc đầu vào người dùng thành ba phần. Phần đầu tiên là các đặc trưng hình ảnh, phần thứ hai là token định danh nhiệm vụ, và phần thứ ba là đầu vào hướng dẫn.

Token định danh nhiệm vụ. Mô hình của chúng tôi nhận một định danh riêng biệt cho mỗi nhiệm vụ để giảm sự mơ hồ giữa các nhiệm vụ khác nhau. Như được minh họa trong Bảng 1, chúng tôi đã đề xuất sáu định danh nhiệm vụ khác nhau cho trả lời câu hỏi thị giác, chú thích hình ảnh, chú thích hình ảnh có định vị, hiểu biểu thức tham chiếu, tạo biểu thức tham chiếu, và phân tích cụm từ và định vị tương ứng. Đối với các hướng dẫn không liên quan đến thị giác, mô hình của chúng tôi không sử dụng token định danh nhiệm vụ nào.

[Bảng 1: Token định danh nhiệm vụ cho 6 nhiệm vụ khác nhau, bao gồm trả lời câu hỏi thị giác, chú thích hình ảnh, chú thích hình ảnh có định vị, hiểu biểu thức tham chiếu (REC), tạo biểu thức tham chiếu (REG), và phân tích và định vị đối tượng (nơi mô hình trích xuất đối tượng từ văn bản đầu vào và xác định vị trí khung giới hạn của chúng).]

Biểu diễn vị trí không gian. Đối với các nhiệm vụ như hiểu biểu thức tham chiếu (REC), tạo biểu thức tham chiếu (REG), và chú thích hình ảnh có định vị, mô hình của chúng tôi được yêu cầu xác định chính xác vị trí không gian của các đối tượng được tham chiếu. Chúng tôi biểu diễn vị trí không gian thông qua định dạng văn bản của khung giới hạn trong thiết lập của chúng tôi, cụ thể: " {<Xleft><Ytop><Xright><Ybottom>}". Tọa độ cho X và Y được biểu diễn bằng các giá trị số nguyên được chuẩn hóa trong phạm vi [0,100]. <Xleft> và <Ytop> biểu thị tọa độ x và y góc trên-trái của khung giới hạn được tạo, và <Xright> và <Ybottom> biểu thị tọa độ x và y của góc dưới-phải.

3.3 Huấn luyện Hướng dẫn Đa nhiệm vụ
Chúng tôi bây giờ điều chỉnh mẫu hướng dẫn đa nhiệm vụ được thiết kế của chúng tôi cho huấn luyện hướng dẫn. Ý tưởng cơ bản là lấy hướng dẫn với token định danh đặc thù nhiệm vụ làm đầu vào cho huấn luyện hướng dẫn định hướng nhiệm vụ của MiniGPT-v2. Khi hướng dẫn đầu vào có token định danh nhiệm vụ, mô hình của chúng tôi sẽ trở nên có xu hướng hiểu đa nhiệm vụ hơn trong quá trình huấn luyện. Chúng tôi huấn luyện mô hình với hướng dẫn định danh nhiệm vụ để căn chỉnh thị giác tốt hơn trong ba giai đoạn. Giai đoạn đầu tiên là giúp MiniGPT-v2 xây dựng kiến thức thị giác-ngôn ngữ rộng rãi thông qua nhiều bộ dữ liệu hình ảnh-văn bản được gán nhãn yếu, và các bộ dữ liệu chú thích thị giác-ngôn ngữ chi tiết chất lượng cao cũng như (nơi chúng tôi sẽ gán tỷ lệ lấy mẫu dữ liệu cao cho các bộ dữ liệu hình ảnh-văn bản được gán nhãn yếu). Giai đoạn thứ hai là cải thiện mô hình chỉ với dữ liệu chi tiết cho nhiều nhiệm vụ. Giai đoạn thứ ba là tinh chỉnh mô hình của chúng tôi với nhiều bộ dữ liệu hướng dẫn đa phương thức và ngôn ngữ để trả lời tốt hơn các hướng dẫn đa phương thức đa dạng và hoạt động như một chatbot đa phương thức. Các bộ dữ liệu được sử dụng để huấn luyện ở mỗi giai đoạn được liệt kê trong Bảng 2.

[Bảng 2: Các bộ dữ liệu huấn luyện được sử dụng cho huấn luyện ba giai đoạn của mô hình.]

Giai đoạn 1: Huấn luyện trước. Để có kiến thức thị giác-ngôn ngữ rộng rãi, mô hình của chúng tôi được huấn luyện trên hỗn hợp các bộ dữ liệu được gán nhãn yếu và chi tiết. Chúng tôi đưa ra tỷ lệ lấy mẫu cao cho các bộ dữ liệu được gán nhãn yếu để có được kiến thức đa dạng hơn trong giai đoạn đầu tiên.

Đối với các bộ dữ liệu được gán nhãn yếu, chúng tôi sử dụng LAION [42], CC3M [44], SBU [33], và GRIT-20M từ Kosmos v2 [36] đã xây dựng bộ dữ liệu cho hiểu biểu thức tham chiếu (REC), tạo biểu thức tham chiếu (REG), và chú thích hình ảnh có định vị.

Đối với các bộ dữ liệu chi tiết, chúng tôi sử dụng các bộ dữ liệu như COCO caption [24] và Text Captions [45] cho chú thích hình ảnh, RefCOCO [20], RefCOCO+ [56], và RefCOCOg [29] cho REC. Đối với REG, chúng tôi tái cấu trúc dữ liệu từ ReferCOCO và các biến thể của nó, đảo ngược thứ tự từ cụm từ → khung giới hạn thành khung giới hạn → cụm từ. Đối với các bộ dữ liệu VQA, huấn luyện của chúng tôi sử dụng nhiều bộ dữ liệu khác nhau, như GQA [19], VQA-v2 [14], OCR-VQA [31], OK-VQA [30], và AOK-VQA [43].

Giai đoạn 2: Huấn luyện đa nhiệm vụ. Để cải thiện hiệu suất của MiniGPT-v2 trên mỗi nhiệm vụ, chúng tôi chỉ tập trung vào việc sử dụng các bộ dữ liệu chi tiết để huấn luyện mô hình của chúng tôi ở giai đoạn này. Chúng tôi loại trừ các bộ dữ liệu được giám sát yếu như GRIT-20M và LAION từ giai đoạn 1 và cập nhật tỷ lệ lấy mẫu dữ liệu theo tần suất của mỗi nhiệm vụ. Chiến lược này cho phép mô hình của chúng tôi ưu tiên dữ liệu hình ảnh-văn bản được căn chỉnh chất lượng cao để có hiệu suất vượt trội trên các nhiệm vụ khác nhau.

Giai đoạn 3: Điều chỉnh hướng dẫn đa phương thức. Tiếp theo, chúng tôi tập trung vào việc điều chỉnh mô hình của chúng tôi với nhiều bộ dữ liệu hướng dẫn đa phương thức hơn và tăng cường khả năng hội thoại của nó như một chatbot. Chúng tôi tiếp tục sử dụng các bộ dữ liệu từ giai đoạn thứ hai và thêm các bộ dữ liệu hướng dẫn, bao gồm LLaVA [26], bộ dữ liệu Flickr30k [37], bộ dữ liệu đa nhiệm vụ hỗn hợp được xây dựng của chúng tôi, và bộ dữ liệu ngôn ngữ, Unnatural Instruction [17]. Chúng tôi đưa ra tỷ lệ lấy mẫu dữ liệu thấp hơn cho các bộ dữ liệu chi tiết từ giai đoạn 2 và tỷ lệ lấy mẫu dữ liệu cao hơn cho các bộ dữ liệu hướng dẫn mới.

– Dữ liệu hướng dẫn LLaVA. Chúng tôi thêm các bộ dữ liệu điều chỉnh hướng dẫn đa phương thức, bao gồm các mô tả chi tiết và lập luận phức tạp từ LLaVA [26], với 23k và 58k ví dụ dữ liệu tương ứng.

– Flicker 30k. Sau huấn luyện giai đoạn thứ hai, MiniGPT-v2 của chúng tôi có thể tạo ra chú thích hình ảnh có định vị một cách hiệu quả. Tuy nhiên, những mô tả này có xu hướng ngắn và thường bao gồm rất ít đối tượng thị giác. Điều này là do bộ dữ liệu GRIT-20M từ KOSMOS-v2 [36] mà mô hình của chúng tôi được huấn luyện với, có đặc điểm là số lượng đối tượng thị giác có định vị hạn chế trong mỗi chú thích, và mô hình của chúng tôi thiếu điều chỉnh hướng dẫn đa phương thức phù hợp để dạy nó nhận ra nhiều đối tượng thị giác hơn. Để cải thiện điều này, chúng tôi tinh chỉnh mô hình của chúng tôi bằng cách sử dụng bộ dữ liệu Flickr30k [37], cung cấp nhiều định vị ngữ cảnh hơn của các thực thể trong chú thích của nó.

Chúng tôi chuẩn bị bộ dữ liệu Flickr30k theo hai định dạng riêng biệt để huấn luyện mô hình của chúng tôi thực hiện chú thích hình ảnh có định vị và một nhiệm vụ mới "phân tích và định vị đối tượng":

1)Chú thích hình ảnh có định vị. Chúng tôi chọn các chú thích có tối thiểu năm cụm từ có định vị, chứa khoảng 2.5k mẫu, và chúng tôi trực tiếp hướng dẫn mô hình tạo ra chú thích hình ảnh có định vị. ví dụ: một <p>bàn gỗ </p>{<Xleft><Ytop><Xright><Ybottom>} ở trung tâm phòng.

2)Phân tích và định vị đối tượng. Nhiệm vụ mới này là phân tích tất cả các đối tượng từ một chú thích đầu vào và sau đó định vị mỗi đối tượng. Để kích hoạt điều này, chúng tôi sử dụng định danh nhiệm vụ [detection] để phân biệt khả năng này với các nhiệm vụ khác. Ngoài ra, chúng tôi sử dụng Flickr30k để xây dựng hai loại bộ dữ liệu hướng dẫn: chú thích → cụm từ có định vị và cụm từ → cụm từ có định vị, mỗi loại chứa khoảng 2.5k và 3k mẫu. Sau đó chúng tôi nhắc mô hình của chúng tôi với hướng dẫn: [detection] description, mô hình sẽ trực tiếp phân tích các đối tượng từ mô tả hình ảnh đầu vào và cũng định vị các đối tượng thành khung giới hạn.

– Bộ dữ liệu đa nhiệm vụ hỗn hợp. Sau huấn luyện rộng rãi với các cặp hướng dẫn-trả lời một lượt, mô hình có thể không xử lý tốt nhiều nhiệm vụ trong các cuộc hội thoại nhiều lượt vì ngữ cảnh trở nên phức tạp hơn. Để giảm thiểu tình huống này, chúng tôi tạo ra một bộ dữ liệu hội thoại nhiều lượt mới bằng cách trộn dữ liệu từ các nhiệm vụ khác nhau. Chúng tôi bao gồm bộ dữ liệu này vào huấn luyện mô hình giai đoạn thứ ba của chúng tôi.

– Hướng dẫn không tự nhiên. Khả năng hội thoại của mô hình ngôn ngữ có thể bị giảm sau huấn luyện thị giác-ngôn ngữ rộng rãi. Để khắc phục điều này, chúng tôi thêm bộ dữ liệu ngôn ngữ, Unnatural Instruction [17] vào huấn luyện giai đoạn thứ ba của mô hình để giúp khôi phục khả năng tạo ngôn ngữ.

4 Thí nghiệm
Trong phần này, chúng tôi trình bày thiết lập thí nghiệm và kết quả. Chúng tôi chủ yếu tiến hành thí nghiệm trên chú thích hình ảnh (chi tiết)/có định vị, trả lời câu hỏi thị giác, và các nhiệm vụ định vị thị giác, bao gồm hiểu biểu thức tham chiếu. Chúng tôi trình bày cả kết quả định lượng và định tính.

Chi tiết triển khai. Trong toàn bộ quá trình huấn luyện, backbone thị giác của MiniGPT-v2 vẫn bị đóng băng. Chúng tôi tập trung vào việc huấn luyện lớp chiếu tuyến tính và tinh chỉnh hiệu quả mô hình ngôn ngữ bằng LoRA [18]. Với LoRA, chúng tôi tinh chỉnh Wq và Wv thông qua thích ứng thứ hạng thấp. Trong triển khai của chúng tôi, chúng tôi đặt thứ hạng, r = 64. Chúng tôi huấn luyện mô hình với độ phân giải hình ảnh 448x448 trong tất cả các giai đoạn. Trong mỗi giai đoạn, chúng tôi sử dụng các mẫu hướng dẫn đa phương thức được thiết kế của chúng tôi cho các nhiệm vụ thị giác-ngôn ngữ khác nhau trong quá trình huấn luyện mô hình.

Huấn luyện và siêu tham số. Chúng tôi sử dụng tối ưu hóa AdamW với bộ lập lịch tốc độ học cosine để huấn luyện mô hình của chúng tôi. Trong giai đoạn ban đầu, chúng tôi huấn luyện trên 8xA100 GPU trong 400,000 bước với kích thước batch toàn cầu là 96 và tốc độ học tối đa là 1e-4. Giai đoạn này mất khoảng 90 giờ. Trong giai đoạn thứ hai, mô hình được huấn luyện trong 50,000 bước trên 4xA100 GPU với tốc độ học tối đa là 1e-5, áp dụng kích thước batch toàn cầu là 64, và giai đoạn huấn luyện này kéo dài khoảng 20 giờ. Đối với giai đoạn cuối cùng, huấn luyện được thực hiện thêm 35,000 bước trên 4xA100 GPU, sử dụng kích thước batch toàn cầu là 24 và giai đoạn huấn luyện này mất khoảng 7 giờ, duy trì cùng tốc độ học tối đa là 1e-5.

4.1 Đánh giá Định lượng
Bộ dữ liệu và chỉ số đánh giá. Chúng tôi đánh giá mô hình của chúng tôi trên một loạt các bài đánh giá VQA và định vị thị giác. Đối với các bài đánh giá VQA, chúng tôi xem xét OKVQA [43], GQA [19], lập luận không gian thị giác (VSR) [25], IconVQA [28], VizWiz [15], HatefulMemes và (HM) [21]. Đối với định vị thị giác, chúng tôi đánh giá mô hình của chúng tôi trên các bài đánh giá RefCOCO [20] và RefCOCO+ [56], và RefCOCOg [29].

Để đánh giá các bài đánh giá VQA, chúng tôi sử dụng cách tiếp cận mở với chiến lược giải mã tham lam. Chúng tôi đánh giá mỗi câu hỏi VQA với mẫu hướng dẫn sau: "[vqa] question". Theo phương pháp trước đây [10], chúng tôi đánh giá hiệu suất bằng cách khớp phản hồi của mô hình với sự thật cơ bản và báo cáo độ chính xác top-1. Đối với các bài đánh giá định vị thị giác, chúng tôi sử dụng mẫu "[refer] give me the location of Referring expression" cho mỗi câu hỏi hiểu biểu thức tham chiếu, và một khung giới hạn dự đoán được coi là chính xác để báo cáo độ chính xác nếu IOU của nó giữa dự đoán và sự thật cơ bản cao hơn 0.5.

Kết quả trả lời câu hỏi thị giác. Bảng 3 trình bày kết quả thí nghiệm của chúng tôi trên nhiều bài đánh giá VQA. Kết quả của chúng tôi so sánh thuận lợi với các đường cơ sở bao gồm MiniGPT-4 [59], Shikra [7], LLaVA [26], và InstructBLIP [10] trên tất cả các nhiệm vụ VQA. Ví dụ, trên OKVQA, MiniGPT-v2 của chúng tôi vượt trội MiniGPT-4, Shikra, LLaVA, và BLIP-2 lần lượt 20.3%, 10.6%, 3.4%, và 11.9%. Những kết quả này cho thấy khả năng trả lời câu hỏi thị giác mạnh mẽ của mô hình chúng tôi. Hơn nữa, chúng tôi thấy rằng biến thể MiniGPT-v2 (chat) của chúng tôi cho thấy hiệu suất cao hơn so với phiên bản được huấn luyện sau giai đoạn thứ hai. Trên OKVQA, VSR, IconVQA, VizWiz, và HM, MiniGPT-v2 (chat) vượt trội MiniGPT-v2 lần lượt 0.9%, 2.3%, 4.2%, 20.7%, và 0.6%. Chúng tôi tin rằng hiệu suất tốt hơn có thể được quy cho các kỹ năng ngôn ngữ được cải thiện trong quá trình huấn luyện giai đoạn thứ ba, có thể mang lại lợi ích cho việc hiểu và phản hồi câu hỏi thị giác, đặc biệt là trên VizWiz với mức tăng 20.7% độ chính xác top-1.

Kết quả hiểu biểu thức tham chiếu. Bảng 4 so sánh mô hình của chúng tôi với các đường cơ sở trên các bài đánh giá REC. MiniGPT-v2 của chúng tôi cho thấy hiệu suất REC mạnh mẽ trên RefCOCO, RefCOCO+, và RefCOCOg, hoạt động tốt hơn so với các mô hình tổng quát thị giác-ngôn ngữ khác. MiniGPT-v2 vượt trội OFA-L [52] hơn 8% độ chính xác trên tất cả các nhiệm vụ của RefCOCO/RefCOCO+/RefCOCOg. So với một đường cơ sở mạnh mẽ, Shikra (13B) [7], mô hình của chúng tôi vẫn cho thấy kết quả tốt hơn, ví dụ, 84.29% so với 83.96% độ chính xác trung bình. Những kết quả này cung cấp bằng chứng trực tiếp cho khả năng định vị thị giác cạnh tranh của MiniGPT-v2. Mặc dù mô hình của chúng tôi hoạt động kém hơn các mô hình chuyên biệt, hiệu suất đầy hứa hẹn cho thấy năng lực ngày càng tăng của nó trong định vị thị giác.

Nghiên cứu cắt bỏ về định danh nhiệm vụ. Chúng tôi tiến hành các nghiên cứu cắt bỏ về tác động của định danh nhiệm vụ đối với hiệu suất của MiniGPT-v2. Chúng tôi so sánh mô hình của chúng tôi với biến thể không sử dụng định danh nhiệm vụ trên các bài đánh giá VQA. Cả hai mô hình đều được huấn luyện trên 4xA100 GPU trong 24 giờ với số bước huấn luyện bằng nhau cho nhiều nhiệm vụ thị giác-ngôn ngữ. Kết quả trong Bảng 5 cho thấy hiệu suất trên nhiều bài đánh giá VQA và nhất quán cho thấy rằng huấn luyện định danh token mang lại lợi ích cho hiệu suất tổng thể của MiniGPT-v2. Cụ thể, MiniGPT-v2 của chúng tôi với huấn luyện hướng dẫn định hướng nhiệm vụ đạt được cải thiện 1.2% độ chính xác top-1 trung bình. Những kết quả cắt bỏ này có thể xác nhận lợi thế rõ ràng của việc thêm token định danh nhiệm vụ và hỗ trợ việc sử dụng định danh đa nhiệm vụ cho hiệu quả học đa nhiệm vụ.

Ảo giác. Chúng tôi đo lường ảo giác của mô hình trên việc tạo mô tả hình ảnh và so sánh kết quả với các đường cơ sở thị giác-ngôn ngữ khác, bao gồm MiniGPT-4 [59], mPLUG-Owl [55], LLaVA [26], và MultiModal-GPT [13]. Theo phương pháp từ [23], chúng tôi sử dụng CHAIR [40] để đánh giá ảo giác ở cả cấp độ đối tượng và câu. Như được hiển thị trong Bảng 6, chúng tôi thấy rằng MiniGPT-v2 của chúng tôi có xu hướng tạo ra mô tả hình ảnh với ảo giác giảm so với các đường cơ sở khác. Chúng tôi đã đánh giá ba loại gợi ý trong MiniGPT-v2. Đầu tiên, chúng tôi sử dụng gợi ý "generate a brief description of the given image" mà không có định danh nhiệm vụ cụ thể nào có xu hướng tạo ra các mô tả hình ảnh chi tiết hơn. Sau đó chúng tôi cung cấp gợi ý hướng dẫn "[grounding] describe this image in as detailed as possible" để đánh giá chú thích hình ảnh có định vị. Cuối cùng, chúng tôi gợi ý mô hình của chúng tôi với "[caption] briefly describe the image". Với các định danh nhiệm vụ này, MiniGPT-v2 có thể tạo ra nhiều mô tả hình ảnh với các mức độ ảo giác khác nhau. Kết quả là, tất cả ba biến thể hướng dẫn này đều có ảo giác thấp hơn so với đường cơ sở của chúng tôi, đặc biệt là với các bộ xác định nhiệm vụ [caption] và [grounding].

4.2 Kết quả Định tính
Chúng tôi bây giờ cung cấp kết quả định tính để hiểu bổ sung về khả năng đa phương thức của mô hình. Một số ví dụ có thể được thấy trong Hình 3. Cụ thể, chúng tôi đã chứng minh các khả năng khác nhau trong các ví dụ bao gồm a) nhận dạng đối tượng; b) chú thích hình ảnh có định vị chi tiết; c) trả lời câu hỏi thị giác; d) hiểu biểu thức tham chiếu; e) trả lời câu hỏi thị giác dưới định danh nhiệm vụ; f) mô tả hình ảnh chi tiết; g) phân tích và định vị đối tượng từ văn bản đầu vào. Nhiều kết quả định tính hơn có thể được tìm thấy trong Phụ lục. Những kết quả này chứng minh rằng mô hình của chúng tôi có khả năng hiểu thị giác-ngôn ngữ cạnh tranh. Hơn nữa, lưu ý rằng chúng tôi chỉ huấn luyện mô hình của chúng tôi với vài nghìn mẫu hướng dẫn trên các nhiệm vụ phân tích và định vị đối tượng ở giai đoạn thứ ba, và mô hình của chúng tôi có thể hiệu quả tuân theo các hướng dẫn và tổng quát hóa trên nhiệm vụ mới. Điều này cho thấy rằng mô hình của chúng tôi có tính linh hoạt để thích ứng với nhiều nhiệm vụ mới.

Lưu ý rằng mô hình của chúng tôi vẫn thỉnh thoảng cho thấy ảo giác khi tạo mô tả hình ảnh hoặc định vị thị giác. ví dụ, mô hình của chúng tôi đôi khi có thể tạo ra mô tả về các đối tượng thị giác không tồn tại hoặc tạo ra vị trí thị giác không chính xác của các đối tượng có định vị. Chúng tôi tin rằng việc huấn luyện với nhiều dữ liệu hình ảnh-văn bản được căn chỉnh chất lượng cao hơn và tích hợp với backbone thị giác mạnh hơn hoặc mô hình ngôn ngữ lớn có tiềm năng giảm thiểu vấn đề này.

5 Kết luận
Trong bài báo này, chúng tôi giới thiệu MiniGPT-v2, một LLM đa phương thức có thể phục vụ như một giao diện thống nhất cho việc học đa nhiệm vụ thị giác-ngôn ngữ khác nhau. Để phát triển một mô hình duy nhất có khả năng xử lý nhiều nhiệm vụ thị giác-ngôn ngữ, chúng tôi đề xuất sử dụng các định danh riêng biệt cho mỗi nhiệm vụ trong quá trình huấn luyện và suy luận. Các định danh này giúp mô hình của chúng tôi dễ dàng phân biệt các nhiệm vụ khác nhau và cũng cải thiện hiệu quả học tập. MiniGPT-v2 của chúng tôi đạt được kết quả tiên tiến trên nhiều bài đánh giá trả lời câu hỏi thị giác và hiểu biểu thức tham chiếu. Chúng tôi cũng thấy rằng mô hình của chúng tôi có thể thích ứng hiệu quả với các nhiệm vụ thị giác-ngôn ngữ mới, cho thấy MiniGPT-v2 có nhiều ứng dụng tiềm năng trong cộng đồng thị giác-ngôn ngữ.
