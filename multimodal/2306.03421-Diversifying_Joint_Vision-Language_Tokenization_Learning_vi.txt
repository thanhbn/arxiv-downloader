<<<<<<< Updated upstream
# Đa dạng hóa việc học Token hóa chung Thị giác-Ngôn ngữ
Vardaan Pahuja1∗, AJ Piergiovanni2, Anelia Angelova2
1The Ohio State University2Google DeepMind
pahuja.9@osu.edu, {ajpiergi, anelia }@google.com

## Tóm tắt
Xây dựng biểu diễn chung trên hình ảnh và văn bản là một bước thiết yếu cho các tác vụ như Trả lời câu hỏi thị giác và Trả lời câu hỏi video. Trong công trình này, chúng tôi phát hiện rằng các biểu diễn không chỉ cần nắm bắt chung các đặc trưng từ cả hai phương thức mà còn cần đa dạng để có hiệu suất tổng quát hóa tốt hơn. Vì vậy, chúng tôi đề xuất việc học biểu diễn chung thị giác-ngôn ngữ bằng cách đa dạng hóa quá trình học token hóa, cho phép học được các token đủ tách biệt với nhau từ cả hai phương thức. Chúng tôi quan sát thấy phương pháp của chúng tôi vượt trội hơn các mô hình cơ sở trong hầu hết các cài đặt và cạnh tranh với các phương pháp hiện đại.

## 1. Giới thiệu
Trả lời câu hỏi thị giác (VQA) đã nhận được sự quan tâm đáng kể từ cộng đồng nghiên cứu trong những năm gần đây. Tác vụ này bao gồm việc dự đoán phản hồi văn bản cho câu hỏi ngôn ngữ tự nhiên dựa trên hình ảnh. Điều này có thể đạt được bằng cách phân loại phản hồi từ một tập hợp câu trả lời cố định hoặc sinh ra phản hồi văn bản tự do. Gần đây, trả lời câu hỏi video (VideoQA) đã trở nên phổ biến như một tác vụ phức tạp hơn trong AI đa phương thức. Nó thách thức hơn so với trả lời câu hỏi dựa trên hình ảnh vì nó bao gồm việc suy luận về nội dung các đối tượng và chuỗi hành động trong các khung hình khác nhau và liên kết chúng với ngôn ngữ tự nhiên được sử dụng trong câu hỏi.

Xem xét ví dụ được hiển thị trong Hình 1. Cho câu hỏi "Người mặc váy hồng đang chơi với ai?", mô hình được giao nhiệm vụ suy ra phản hồi chính xác - "trẻ em".

Mô hình Transformer [7] đã cách mạng hóa lĩnh vực NLP thông qua việc học tự giám sát, mang lại kết quả hiện đại trên các điểm chuẩn [7,24,27] và khả năng tổng quát hóa few-shot đáng chú ý [3]. Điều này đã thúc đẩy các nỗ lực tương tự cho các tác vụ thị giác-ngôn ngữ (VL), trong đó các mô hình như CLIP [21], ALIGN [14] và Florence [34] cho thấy tiềm năng hứa hẹn cho việc học chuyển giao trên các tác vụ hạ nguồn. Các mô hình thị giác-ngôn ngữ tự giám sát như LXMERT [26], ViLBERT [18] và VisualBERT [16] sử dụng mô hình này để tiền huấn luyện trên tập dữ liệu liên quan và tinh chỉnh trên các tác vụ hạ nguồn, bao gồm VQA.

Một hướng tiếp cận trong tài liệu đầu tiên trích xuất các đặc trưng cho từng phương thức riêng biệt rồi sau đó thử nghiệm kết hợp đa phương thức bằng cách nối đơn giản các token VL [16, 17, 25] hoặc bằng cách sử dụng cross-attention [18, 26]. Tuy nhiên, mô hình Co-tokenization [20] chứng minh rằng tương tác đa phương thức trong quá trình trích xuất đặc trưng là cách hiệu quả hơn nhiều để suy luận trên hai phương thức. Nó bao gồm việc học lặp lại một tập hợp biểu diễn token bằng TokenLearner [23] có điều kiện trên cả video và văn bản. Các biểu diễn token đã học như vậy cho phép các lớp Transformer tiếp theo chỉ xử lý một vài token, dẫn đến các mô hình hiệu quả hơn.

Học biểu diễn là một vấn đề trung tâm trong học máy hiện đại. Một biểu diễn tách biệt là biểu diễn mà mỗi đặc trưng chỉ nắm bắt thông tin về một yếu tố biến đổi nổi bật [2]. Các biểu diễn tách biệt có khả năng tổng quát hóa ngoài miền (OOD) vượt trội [9, 11, 28], khả năng diễn giải tốt hơn [1, 12], hiệu quả mẫu [12] và khả năng học chuyển giao [35]. Vì vậy, chúng tôi đề xuất một góc nhìn khác về việc học chung hình ảnh-ngôn ngữ, tập trung vào việc tách biệt các token cấu thành biểu diễn đặc trưng đa phương thức. Chúng tôi sử dụng hàm mất mát thúc đẩy đa dạng để khuyến khích các biểu diễn đặc trưng được tách biệt và do đó có tính đại diện theo cách kinh tế. Các biểu diễn của chúng tôi vượt trội hơn các mô hình cơ sở trong hầu hết các cài đặt và cho thấy hiệu suất cạnh tranh với các phương pháp hiện đại.

## 2. Nền tảng

**TokenLearner**: TokenLearner [23] nhằm mục đích học thích ứng một tập hợp biểu diễn token cố định tương ứng với một hoặc nhiều phương thức, ví dụ: hình ảnh, video và văn bản. Ý tưởng chính là chọn một chuỗi tổ hợp thông tin của các vị trí không gian trong hình ảnh/video có điều kiện trên tất cả các phương thức. Chính thức hơn, cho X∈RH×W×C là một hình ảnh đầu vào, trong đó H, W và C biểu thị kích thước hình ảnh và số kênh tương ứng. Đối với token thứ i là zi, nó học một bản đồ chú ý không gian αi(X) được nhân với đầu vào để tạo ra đầu ra token Ai(X),

zi=Ai(X) =ρ(X⊙γ(αi(X))),

trong đó ⊙ biểu thị tích Hadamard, γ(·) biểu thị hàm phát sóng và ρ(·) biểu thị pooling trung bình toàn cầu không gian.

**Mô hình co-tokenization lặp video-văn bản**: Mô hình co-tokenization lặp video-văn bản [20] (gọi tắt là Co-tokenization) có cách tiếp cận độc đáo đối với VideoQA bằng cách tích hợp tương tác giữa video và văn bản trực tiếp vào quá trình trích xuất đặc trưng thị giác thay vì coi chúng như một suy nghĩ sau. Các tương tác như vậy cho phép suy luận tốt hơn trên hai phương thức. Mô hình này sử dụng nhiều luồng video ở các tỷ lệ không gian-thời gian khác nhau để học biểu diễn đa phương thức. Độ phân giải thời gian tốt hơn có thể cần thiết để suy ra các hành động nhất định trong khi độ phân giải không gian tốt hơn hỗ trợ nhận dạng chính xác các đối tượng trong khung hình video, do đó dẫn đến sự đánh đổi. Điều này đòi hỏi việc sử dụng nhiều luồng có độ phân giải khác nhau để có hiệu suất vượt trội.

Mô hình Co-tokenization dựa trên kiến trúc Transformer encoder-decoder kiểu T5 [22]. Trong quá trình mã hóa, Transformer lặp đi lặp lại tạo ra các biểu diễn token đã học trong mỗi lớp bằng cách kết hợp thích ứng các token thị giác. Đầu vào cho mô hình Transformer là sự nối của các đặc trưng ngôn ngữ và các token thị giác được kết hợp từ lớp trước. Cuối cùng, bộ giải mã tạo ra đầu ra văn bản dựa trên các biểu diễn token từ lớp cuối cùng như phản hồi. Mô hình thu được các đặc trưng thị giác và ngôn ngữ ban đầu bằng cách sử dụng mô hình X3D [8] và bộ mã hóa T5 tương ứng. Chúng tôi sử dụng mô hình Co-tokenization làm mô hình cơ sở cho VideoQA.

## 3. Phương pháp đề xuất

### 3.1. Mô hình cơ sở

Mô hình cơ sở bao gồm một mô hình encoder-decoder dựa trên bộ mã hóa văn bản T5 [22] và bộ mã hóa thị giác ResNet-50 [10]. Hai phương thức được kết hợp với nhau để tạo thành không gian đặc trưng chung từ đó một tập nhỏ hơn các token được học. Đối với VideoQA, chúng tôi sử dụng mô hình Co-tokenization làm mô hình cơ sở. Đối với tác vụ VQA, chúng tôi sử dụng phiên bản đơn giản hóa của mô hình Co-tokenization tạo ra một tập hợp token đã học duy nhất thay vì tokenization lặp. Hình 2 hiển thị kiến trúc mô hình tổng thể.

### 3.2. Các biểu diễn tách biệt

Để khuyến khích mô hình học các biểu diễn token đa dạng, chúng tôi đề xuất một hàm mất mát mới - hàm mất mát đa dạng để sử dụng kết hợp với mục tiêu mô hình chính.

Ldiv=∑k=1^N ∑j=1,j≠i^M < αi^k, αj^k >^2    (1)

Ở đây, αi^k biểu thị trọng số chú ý không gian cho token thứ i trong ví dụ k. N và M biểu thị số lượng ví dụ và số lượng token được kết hợp tương ứng. Điều này được lấy cảm hứng từ một kỹ thuật tương tự để tách biệt phạm vi chú ý của nhiều tác nhân trong thao tác robot [36]. Các biểu diễn tách biệt hiệu quả thực thi các biểu diễn đã chọn trực giao với nhau. Mô hình cơ sở của chúng tôi được trình bày trong các thí nghiệm sử dụng các siêu tham số mô hình giống hệt và số lượng token đã học, để có thể so sánh trực tiếp với phương pháp đề xuất.

## 4. Thí nghiệm

### 4.1. Tập dữ liệu

Chúng tôi đánh giá phương pháp của chúng tôi trên các tập dữ liệu sau:

**MSRVTT-QA** [31]: Tập dữ liệu này được tạo bằng cách sử dụng tập dữ liệu mô tả video MSR-VTT [32]. Nó chứa 10K clip video và 243K cặp câu hỏi-câu trả lời.

**MSVD-QA** [31]: Tập dữ liệu này dựa trên Microsoft Research Video Description Corpus [5]. Nó chứa 50K câu hỏi dựa trên 1970 clip video.

**IVQA** [33]: Đây là tập dữ liệu bao gồm các video 'làm thế nào'. Nó có 10K clip video với một câu hỏi và 5 câu trả lời/clip.

**SNLI-VE** [30]: Tác vụ suy luận thị giác bao gồm việc dự đoán xem câu lệnh đã cho là suy luận/mâu thuẫn/trung tính trong ngữ cảnh của hình ảnh.

**GQA** [13]: Đây là điểm chuẩn phổ biến cho suy luận thị giác, được phát triển để giải quyết các thiên lệch của các tập dữ liệu VQA hiện có. Các câu hỏi trong tập dữ liệu này có tính thành phần và được căn cứ trên các đồ thị cảnh Visual Genome [15].

### 4.2. Chi tiết triển khai

Đối với cả mô hình VQA và VideoQA, chúng tôi sử dụng bộ tối ưu hóa Adam với độ suy giảm trọng số 1e-4 và L= 12, A= 12, H= 768 cho mô hình Transformer trong đó L, A và H biểu thị số lượng lớp, số lượng đầu chú ý mỗi lớp và kích thước ẩn tương ứng.

Các mô hình VideoQA sử dụng 16 khung hình và hình ảnh 224×224. Hai luồng là 8×224×224 và 16×112×112. Hơn nữa, để tiết kiệm tính toán, chúng tôi thể hiện hiệu suất cạnh tranh bằng cách chỉ sử dụng một phần ba số lớp Transformer (và các lần lặp huấn luyện) của mô hình Co-tokenization ban đầu, điều này cho phép mô hình của chúng tôi chạy với ít FLOPs hơn. Đối với các mô hình VideoQA, tiền huấn luyện được thực hiện trên tập con 10% của tập dữ liệu Howto69MVQA [33]. Đối với các mô hình VQA, tiền huấn luyện được thực hiện trên tập dữ liệu Conceptual Captions [4]. Chúng tôi sử dụng 8 và 16 token đã học cho các thí nghiệm VideoQA và VQA tương ứng.

### 4.3. Kết quả

**VideoQA**: Chúng tôi so sánh phương pháp của chúng tôi với mô hình cơ sở cả có và không có tiền huấn luyện (Bảng 1). Chúng tôi quan sát thấy phương pháp của chúng tôi liên tục vượt trội hơn các mô hình cơ sở cho cả hai cài đặt trên MSRVTT-QA, MSVD-QA và IVQA. Bảng 2 so sánh với mô hình Co-tokenization hiện đại, cho thấy hiệu suất cạnh tranh của mô hình chúng tôi. Chúng tôi lưu ý rằng mô hình của chúng tôi có cùng GFLOPS với mô hình cơ sở vì số lượng token đã học là giống nhau cho cả hai. Tuy nhiên, vì tokenization tách biệt có thể chọn các token theo cách kinh tế hơn (tức là một số token có thể 'trống'), nói chung, ít token hơn sẽ cần thiết cuối cùng, dẫn đến ít FLOPs hơn tổng thể.

**VQA**: Đối với cài đặt tiền huấn luyện, phương pháp của chúng tôi thu được những cải thiện nhất quán trên tập xác thực cho cả hai tập dữ liệu và trên tập kiểm tra cho SNLI-VE (Bảng 3). Tương tự, chúng tôi vượt trội hơn mô hình cơ sở cho tập dữ liệu SNLI-VE trong cài đặt không tiền huấn luyện (Bảng 4). Điều này cho thấy các token tách biệt được đề xuất cung cấp hiệu suất tốt hơn so với mô hình cơ sở cho cùng một hạn ngạch token được phân bổ. Bảng 5 so sánh với các mô hình hiện đại, và trong khi các mô hình của chúng tôi thực hiện tốt, chúng không vượt trội hơn các mô hình đặc biệt lớn.

### 4.4. Trực quan hóa

Hình 3 trực quan hóa các biểu diễn tách biệt đã học, cùng với các bản đồ chú ý tương ứng của chúng cho một ví dụ VQA. Chúng tôi quan sát thấy chúng tập trung sự chú ý vào các khu vực cụ thể hơn nhiều của hình ảnh, những khu vực quan trọng để trả lời câu hỏi. Hơn nữa, các token được chọn ưu tiên biểu diễn thị giác-ngôn ngữ chung, do đó nắm bắt các đặc trưng thiết yếu từ cả đầu vào thị giác và ngôn ngữ học. Các trực quan hóa bổ sung được hiển thị trong Phần A trong Phụ lục.

## 5. Kết luận

Trong công trình này, chúng tôi đề xuất học các biểu diễn tách biệt cho các token đã học trong các mô hình Transformer cho các tác vụ VQA và VideoQA. Phương pháp đơn giản nhưng hiệu quả này dẫn đến sự cải thiện hiệu suất trong phần lớn các cài đặt huấn luyện trên các tập dữ liệu. Công việc tương lai sẽ bao gồm việc đánh giá phương pháp này với các mô hình dung lượng cao hơn và nhiều tiền huấn luyện hơn để cải thiện hiệu suất. Một hướng tương lai hứa hẹn khác là sử dụng các biểu diễn token đã học cho các tác vụ hạ nguồn liên quan.
=======
# Đa dạng hóa Học biểu diễn Token hóa Thị giác-Ngôn ngữ Kết hợp
Vardaan Pahuja1∗, AJ Piergiovanni2, Anelia Angelova2
1Đại học Bang Ohio 2Google DeepMind
pahuja.9@osu.edu, {ajpiergi, anelia }@google.com
Tóm tắt
Xây dựng các biểu diễn kết hợp giữa hình ảnh và văn bản là
một bước thiết yếu cho các tác vụ như Trả lời Câu hỏi Thị giác
và Trả lời Câu hỏi Video. Trong công trình này, chúng tôi nhận
thấy rằng các biểu diễn không chỉ phải nắm bắt đồng thời các
đặc trưng từ cả hai phương thức mà còn phải đa dạng để có
hiệu suất khái quát hóa tốt hơn. Vì mục đích này, chúng tôi đề
xuất học biểu diễn thị giác-ngôn ngữ kết hợp bằng cách đa dạng
hóa quá trình học token hóa, cho phép học các token được tách
rời đủ mức với nhau từ cả hai phương thức. Chúng tôi quan sát
thấy phương pháp của chúng tôi vượt trội hơn các mô hình cơ
sở trong phần lớn các thiết lập và có khả năng cạnh tranh với
các phương pháp tiên tiến.
1. Giới thiệu
Trả lời Câu hỏi Thị giác (VQA) đã nhận được sự chú ý đáng
kể trong cộng đồng nghiên cứu trong những năm gần đây.
Tác vụ này bao gồm dự đoán một phản hồi văn bản cho một
câu hỏi ngôn ngữ tự nhiên dựa trên một hình ảnh. Điều này
có thể đạt được bằng cách phân loại phản hồi từ một tập hợp
câu trả lời cố định hoặc tạo ra một phản hồi văn bản dạng tự
do. Gần đây hơn, trả lời câu hỏi video (VideoQA) đã trở nên
phổ biến như một tác vụ phức tạp hơn trong AI đa phương
thức. Nó thách thức hơn so với trả lời câu hỏi dựa trên hình
ảnh vì nó liên quan đến việc suy luận về nội dung của các đối
tượng và chuỗi các hành động trong các khung hình khác nhau
và liên kết chúng với ngôn ngữ tự nhiên được sử dụng trong
câu hỏi. Xem xét ví dụ được hiển thị trong Hình 1. Cho một
câu hỏi - Ai là người trong váy hồng đang chơi cùng?, mô hình
được giao nhiệm vụ suy ra phản hồi chính xác - trẻ em.

Mô hình Transformer [7] đã cách mạng hóa lĩnh vực NLP
thông qua học tự giám sát, mang lại kết quả tiên tiến trên
các điểm chuẩn [7,24,27] và khả năng khái quát hóa few-shot
đáng chú ý [3]. Điều này đã thúc đẩy các nỗ lực tương tự cho
các tác vụ thị giác-ngôn ngữ (VL), nơi các mô hình như CLIP
[21], ALIGN [14], và Florence [34] cho thấy tiềm năng hứa
hẹn cho việc học chuyển đổi trên các tác vụ downstream. Các
mô hình thị giác-ngôn ngữ tự giám sát, như LXMERT [26],
ViLBERT [18], và VisualBERT [16], sử dụng mô hình này để
tiền huấn luyện trên một tập dữ liệu liên quan và tinh chỉnh
trên các tác vụ downstream, bao gồm VQA. Một dòng phương
pháp trong tài liệu trước tiên trích xuất các đặc trưng cho mỗi
hai phương thức riêng biệt và sau đó thực hiện fusion đa
phương thức bằng cách sử dụng concatenation đơn giản của
các token VL [16, 17, 25] hoặc bằng cách sử dụng cross-attention
[18, 26]. Tuy nhiên, mô hình Co-tokenization [20] chứng minh
rằng tương tác đa phương thức trong quá trình trích xuất đặc
trưng là một cách hiệu quả hơn nhiều để suy luận giữa hai
phương thức. Nó bao gồm việc học lặp lại một tập hợp các
biểu diễn token bằng cách sử dụng TokenLearner [23] có điều
kiện trên cả video và văn bản. Các biểu diễn token đã học như
vậy cho phép các lớp Transformer tiếp theo chỉ xử lý một vài
token, dẫn đến các mô hình hiệu quả hơn.

Học biểu diễn là một vấn đề trung tâm trong machine
learning hiện đại. Một biểu diễn tách rời là một biểu diễn
trong đó mỗi đặc trưng nắm bắt thông tin về chỉ một yếu tố
biến đổi nổi bật [2]. Các biểu diễn tách rời có khả năng khái
quát hóa ngoài miền (OOD) vượt trội [9, 11, 28], khả năng
diễn giải tốt hơn [1, 12], hiệu quả mẫu [12], và khả năng học
chuyển đổi [35]. Vì mục đích này, chúng tôi đề xuất một góc
nhìn khác về học hình ảnh-ngôn ngữ kết hợp, với trọng tâm
vào việc tách rời các token tạo thành biểu diễn đặc trưng đa
phương thức. Chúng tôi sử dụng một loss thúc đẩy đa dạng
để khuyến khích các biểu diễn đặc trưng được tách rời và do
đó có tính đại diện một cách tiết kiệm. Các biểu diễn của
chúng tôi vượt trội hơn các baseline trong hầu hết các thiết
lập và cho thấy hiệu suất cạnh tranh với các phương pháp
tiên tiến.

2. Nền tảng
TokenLearner: TokenLearner [23] nhằm mục đích học thích
ứng một tập hợp cố định các biểu diễn token tương ứng với
một hoặc nhiều phương thức, ví dụ hình ảnh, video, và văn
bản. Ý tưởng chính là chọn một chuỗi các kết hợp thông tin
của các vị trí không gian trong hình ảnh/video có điều kiện
trên tất cả các phương thức. Chính thức hơn, cho X∈RH×W×C
là một hình ảnh đầu vào, trong đó H, W, và C biểu thị kích
thước hình ảnh và số kênh, tương ứng. Đối với token thứ i zi,
nó học một bản đồ attention không gian αi(X) được nhân với
đầu vào để tạo ra một đầu ra token Ai(X),
zi=Ai(X) =ρ(X⊙γ(αi(X))),
trong đó ⊙ biểu thị tích Hadamard, γ(·) biểu thị hàm
broadcasting, và ρ(·) biểu thị pooling trung bình toàn cục
không gian.

Mô hình co-tokenization lặp video-văn bản: Mô hình Video-
text iterative co-tokenization [20] (Co-tokenization từ đây
trở đi) có một cách tiếp cận độc đáo đối với VideoQA bằng
cách tích hợp các tương tác giữa video và văn bản trực tiếp
vào quá trình trích xuất đặc trưng thị giác thay vì coi chúng
như một suy nghĩ sau. Các tương tác như vậy cho phép suy
luận tốt hơn giữa hai phương thức. Mô hình này sử dụng nhiều
luồng video ở các tỷ lệ không gian-thời gian khác nhau cho
việc học biểu diễn đa phương thức. Độ phân giải thời gian
tốt hơn có thể cần thiết để suy ra các hành động nhất định
trong khi độ phân giải không gian tốt hơn hỗ trợ nhận dạng
chính xác các đối tượng trong các khung video, do đó dẫn đến
một sự đánh đổi. Điều này đòi hỏi việc sử dụng nhiều luồng
với các độ phân giải khác nhau để có hiệu suất vượt trội.

Mô hình Co-tokenization dựa trên kiến trúc Transformer
encoder-decoder kiểu T5 [22]. Trong quá trình mã hóa,
Transformer lặp lại tạo ra các biểu diễn token đã học trong
mỗi lớp bằng cách fusion thích ứng các token thị giác. Đầu
vào của mô hình Transformer là một concatenation của các
đặc trưng ngôn ngữ và các token thị giác đã fusion từ lớp
trước. Cuối cùng, decoder tạo ra đầu ra văn bản dựa trên
các biểu diễn token từ lớp cuối cùng như phản hồi. Mô hình
thu được các đặc trưng thị giác và ngôn ngữ ban đầu bằng
cách sử dụng một mô hình X3D [8] và một encoder T5, tương
ứng. Chúng tôi sử dụng mô hình Co-tokenization như baseline
cho VideoQA.

3. Phương pháp đề xuất
3.1. Mô hình cơ sở
Mô hình baseline bao gồm một mô hình encoder-decoder
dựa trên encoder văn bản T5 [22] và một encoder thị giác
ResNet-50 [10]. Hai phương thức được fusion lại với nhau
để tạo thành một không gian đặc trưng kết hợp từ đó một tập
hợp nhỏ hơn các token được học. Đối với VideoQA, chúng
tôi sử dụng mô hình Co-tokenization như baseline. Đối với
tác vụ VQA, chúng tôi sử dụng một phiên bản đơn giản hóa
của mô hình Co-tokenization tạo ra một tập hợp duy nhất các
token đã học thay vì tokenization lặp. Hình 2 cho thấy kiến
trúc mô hình tổng thể.

3.2. Biểu diễn tách rời
Để khuyến khích mô hình học các biểu diễn token đa dạng,
chúng tôi đề xuất một hàm loss mới - diversity loss để được
sử dụng kết hợp với mục tiêu mô hình chính.
Ldiv=NX
k=1MX
j=1,j̸=i< αk
i, αk
j>2(1)
Ở đây, αk
i biểu thị các trọng số attention không gian cho token
thứ i trong ví dụ k. N và M biểu thị số lượng ví dụ và số
lượng token đã fusion tương ứng. Điều này được lấy cảm
hứng từ một kỹ thuật tương tự để tách rời sphere of attention
của nhiều agent trong thao tác robot [36]. Các biểu diễn tách
rời hiệu quả thúc đẩy các biểu diễn đã chọn trở nên trực giao
với nhau. Mô hình baseline của chúng tôi được trình bày
trong các thí nghiệm sử dụng các siêu tham số mô hình giống
hệt và số lượng token đã học, để có thể so sánh trực tiếp với
phương pháp đề xuất.

4. Thí nghiệm
4.1. Tập dữ liệu
Chúng tôi đánh giá phương pháp của mình trên các tập dữ
liệu sau:
MSRVTT-QA [31]: Tập dữ liệu này được tạo bằng cách sử
dụng tập dữ liệu mô tả video MSR-VTT [32]. Nó chứa 10K
clip video và 243K cặp câu hỏi-đáp án.
MSVD-QA [31]: Tập dữ liệu này dựa trên Microsoft Research
Video Description Corpus [5]. Nó chứa 50K câu hỏi dựa trên
1970 clip video.
IVQA [33]: Đây là một tập dữ liệu bao gồm các video 'how-to'.
Nó có 10K clip video với một câu hỏi và 5 câu trả lời/clip.
SNLI-VE [30]: Tác vụ entailment thị giác bao gồm dự đoán
liệu tuyên bố đã cho có phải là entailment/contradiction/neural
trong bối cảnh của hình ảnh.
GQA [13]: Đây là một điểm chuẩn phổ biến cho suy luận thị
giác, được phát triển để giải quyết các thiên lệch của các tập
dữ liệu VQA hiện có. Các câu hỏi trong tập dữ liệu này có
tính tổng hợp và được căn cứ trong các đồ thị scene Visual
Genome [15].

4.2. Chi tiết triển khai
Cho cả mô hình VQA và VideoQA, chúng tôi sử dụng optimizer
Adam với weight decay là 1e-4 và L= 12, A= 12, H= 768 cho
mô hình Transformer trong đó L, A, và H biểu thị số lượng
lớp, số lượng attention head mỗi lớp, và kích thước ẩn, tương
ứng.
Các mô hình VideoQA sử dụng 16 khung hình và hình ảnh
224×224. Hai luồng là 8×224×224 và 16×112×112. Hơn nữa,
để tiết kiệm tính toán, chúng tôi chứng minh hiệu suất cạnh
tranh bằng cách chỉ sử dụng một phần ba số lớp Transformer
(và vòng lặp huấn luyện) của mô hình Co-tokenization gốc,
cho phép mô hình của chúng tôi chạy với ít FLOPs hơn. Đối
với các mô hình VideoQA, tiền huấn luyện được thực hiện
trên một tập con 10% của tập dữ liệu Howto69MVQA [33].
Đối với các mô hình VQA, tiền huấn luyện được thực hiện
trên tập dữ liệu Conceptual Captions [4]. Chúng tôi sử dụng
8 và 16 token đã học cho các thí nghiệm VideoQA và VQA,
tương ứng.

4.3. Kết quả
VideoQA. Chúng tôi so sánh phương pháp của mình với mô
hình baseline cả có và không có tiền huấn luyện (Bảng 1).
Chúng tôi quan sát thấy phương pháp của chúng tôi liên tục
vượt trội hơn các mô hình baseline cho cả hai thiết lập trên
MSRVTT-QA, MSVD-QA, và IVQA. Bảng 2 so sánh nó với
mô hình Co-tokenization tiên tiến, cho thấy hiệu suất cạnh
tranh của mô hình chúng tôi. Chúng tôi lưu ý rằng mô hình
của chúng tôi có GFLOPS giống như mô hình baseline vì số
lượng token đã học là giống nhau cho cả hai. Tuy nhiên vì
tokenization tách rời có thể chọn các token theo cách tiết
kiệm hơn (tức là một số token có thể 'trống'), nói chung, ít
token hơn sẽ được cần thiết cuối cùng, dẫn đến ít FLOPs hơn
tổng thể.

VQA. Đối với thiết lập tiền huấn luyện, phương pháp của
chúng tôi đạt được những cải thiện nhất quán trên tập validation
cho cả hai tập dữ liệu và trên tập test cho SNLI-VE (Bảng 3).
Tương tự, chúng tôi vượt trội hơn baseline cho tập dữ liệu
SNLI-VE trong thiết lập không tiền huấn luyện (Bảng 4). Điều
này cho thấy các token tách rời đề xuất cung cấp hiệu suất
tốt hơn so với baseline cho cùng một quota token được phân
bổ. Bảng 5 so sánh với các mô hình tiên tiến, và trong khi
các mô hình của chúng tôi hoạt động tốt, chúng không vượt
trội đặc biệt so với các mô hình lớn.

4.4. Trực quan hóa
Hình 3 trực quan hóa các biểu diễn tách rời đã học, cùng với
các bản đồ attention tương ứng của chúng cho một ví dụ VQA.
Chúng tôi quan sát thấy chúng tập trung attention vào các
vùng cụ thể hơn nhiều của hình ảnh, những vùng quan trọng
để trả lời câu hỏi. Hơn nữa, các token được chọn ưu tiên
biểu diễn thị giác-ngôn ngữ kết hợp, do đó nắm bắt các đặc
trưng thiết yếu từ cả đầu vào thị giác và ngôn ngữ. Các trực
quan hóa bổ sung được hiển thị trong Phần A trong Phụ lục.

5. Kết luận
Trong công trình này, chúng tôi đề xuất học các biểu diễn
tách rời cho các token đã học trong các mô hình Transformer
cho các tác vụ VQA và VideoQA. Phương pháp đơn giản nhưng
hiệu quả này dẫn đến cải thiện hiệu suất trong phần lớn các
thiết lập huấn luyện trên các tập dữ liệu. Công việc tương lai
sẽ bao gồm việc benchmark phương pháp này với các mô hình
có năng lực cao hơn và nhiều tiền huấn luyện hơn để cải thiện
hiệu suất. Một hướng tương lai hứa hẹn khác là sử dụng các
biểu diễn token đã học cho các tác vụ downstream liên quan.
>>>>>>> Stashed changes
