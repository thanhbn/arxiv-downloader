# 2111.12624.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2111.12624.pdf
# File size: 7270168 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Self-slimmed Vision Transformer
Zhuofan Zong1⋆, Kunchang Li3,4⋆, Guanglu Song2, Yali Wang3,5, Yu Qiao3,6,
Biao Leng1, Yu Liu2†
1School of Computer Science and Engineering, Beihang University
2SenseTime Research
3ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime
Joint Lab,Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
4University of Chinese Academy of Sciences
5SIAT Branch, Shenzhen Institute of Artificial Intelligence and Robotics for Society
6Shanghai AI Laboratory
Abstract. Vision transformers (ViTs) have become the popular struc-
tures and outperformed convolutional neural networks (CNNs) on vari-
ous vision tasks. However, such powerful transformers bring a huge com-
putation burden, because of the exhausting token-to-token comparison.
The previous works focus on dropping insignificant tokens to reduce the
computational cost of ViTs. But when the dropping ratio increases, this
hard manner will inevitably discard the vital tokens, which limits its
efficiency. To solve the issue, we propose a generic self-slimmed learn-
ing approach for vanilla ViTs, namely SiT. Specifically, we first design
a novel Token Slimming Module (TSM), which can boost the inference
efficiency of ViTs by dynamic token aggregation. As a general method of
token hard dropping, our TSM softly integrates redundant tokens into
fewer informative ones. It can dynamically zoom visual attention with-
out cutting off discriminative token relations in the images, even with
a high slimming ratio. Furthermore, we introduce a concise Feature Re-
calibration Distillation (FRD) framework, wherein we design a reverse
version of TSM (RTSM) to recalibrate the unstructured token in a flex-
ible auto-encoder manner. Due to the similar structure between teacher
and student, our FRD can effectively leverage structure knowledge for
better convergence. Finally, we conduct extensive experiments to evalu-
ate our SiT. It demonstrates that our method can speed up ViTs by 1.7×
with negligible accuracy drop, and even speed up ViTs by 3.6×while
maintaining 97% of their performance. Surprisingly, by simply arming
LV-ViT with our SiT, we achieve new state-of-the-art performance on
ImageNet. Code is available at https://github.com/Sense-X/SiT .
Keywords: Transformer, Token Slimming, Knowledge Distillation
1 Introduction
Since vision transformer (ViT) [10] started the era of transformer structure in
the fundamental computer vision tasks [3,36,5], variant transformers have been
⋆Z. Zong and K. Li contribute equally during their internship at SenseTime.
†Corresponding author.arXiv:2111.12624v3  [cs.CV]  12 Sep 2022

--- PAGE 2 ---
2 Z. Zong, K. Li, et al.
Table 1: Comparison to recent model pruning methods for ViT. Our SiT
surpasses all the other methods based on structure pruning or hard dropping.
Type Method ReferenceImageNet Throughput
Top-1(%) (image/s) (%)
Baseline DeiT[29] ICML21 79.8 1637 0
Structure Pruning S2ViTE[6] NeurIPS21 79.2(−0.6) 2117 29.3
Token Hard DroppingPS-ViT[28] CVPR22 79.4(−0.5) 2351 43.6
IA-RED2[22] NeurIPS21 79.1(−0.7) 2369 44.7
Dynamic-ViT[24] NeurIPS21 79.3(−0.5) 2575 57.3
Evo-ViT[37] AAAI22 79.4(−0.4) 2629 60.6
EViT[20] ICLR22 79.1(−0.7) 2783 70.0
Token Soft Slimming Our SiTECCV22 79.8(−0.0) 2344 43.2
ECCV22 79.4(−0.4) 3308 102.1
Layer	indexProportion(%)
(a) Token similarity be-
comes higher in deeper
layers.
o]Iℎ^g]o]Iℎ^g]Layer	indexProportion(%)
Token	indexToken	index
jHsHk_HoHff]oℎHek_H|xì]ks^Ij^II^erfo7{r]√√××
0.50.750.875Slimming	ratiosoftsofthardhard
GFLOPsThroughput(image/s)(b) All tokens tend to fo-
cus on the same tokens in
deeper layers.
o]Iℎ^g]o]Iℎ^g]Layer	indexProportion(%)
Token	indexToken	index
jHsHk_HoHff]oℎHek_H|xì]ks^Ij^II^erfo7{r]√√××
0.50.750.875Slimming	ratiosoftsofthardhard
GFLOPsThroughput(image/s)(c) Our soft slimming can au-
tomatically zoom the attention
scope based on the object size.
Fig. 1: Our motivation. In Fig(a), we calculate the correlation coefficients
among tokens and count the proportion that is at least similar ( ≥0.7) to 4/8/16
tokens in different layers. As for Fig(b), we randomly select two tokens in the
tenth layer to show their attention. Moreover, we compare different token prun-
ing methods in Fig(c). Darker tokens get less attention.
designed to challenge the dominance of convolutional neural networks (CNNs).
Different from CNNs that stack convolutions to encode local features progres-
sively, ViTs directly capture the long-term token dependencies. However, because
of the exhausting token-to-token comparison, current powerful transformers re-
quire huge computation, limiting their wide application in reality [12]. Hence,
in this paper, we aim to design a generic learning framework for boosting the
efficiency of vanilla vision transformers.
To make ViTs more efficient, we tried to explore the inherent properties of
the token-to-token comparison. We conduct a series of experiments based on LV-
ViT, which reveals that sparse attention with high token similarity exists in ViTs.
Fig. 1a shows that token similarity becomes higher in deeper layers, especially in
the last three layers. Besides, the attention tends to focus on the specific tokens
in the deeper layers (Fig. 1b), which indicates the number of decision-relevant
tokens becomes fewer. These observations demonstrate that only a few token
candidates indicate meaningful information. It inspires us a feasible structure-

--- PAGE 3 ---
Self-slimmed Vision Transformer 3
agnostic dimension, token number, to reduce the computational cost. Intuitively,
we can progressively drop the redundant tokens as the network deepens.
Recent studies have tried to compress tokens via data-independent drop-
ping with minimizing reconstruction error [28], or data-dependent dropping with
differentiable scoring [24]. However, data-independent dropping requires layer-
by-layer optimization, which is hard to generalize. Moreover, the token hard
dropping will inevitably discard the vital tokens as the dropping ratio increases,
e.g., the shape of the otterhound is destroyed in the deep layer (Fig. 1c), thus
limiting its performance as shown in Table 1.
Can we design a flexible method of token slimming, thus decision-
relevant information can be dynamically aggregated into a slimmed
token set? To answer this question, we propose token soft slimming. It contains
a concise Token Slimming Module (TSM), which generates decision-relevant to-
kens via a data-dependent weight matrix. As shown in Fig. 1c, by simply in-
serting multiple TSMs in LV-ViT, our network can learn to localize the key
object tokens. More importantly, the attention scope can be zoomed automat-
ically without cutting off the discriminative token relations, e.g., our network
can adaptively concentrate on the most informative parts of the otterhound in
softer manner, while the oxygen mask in harder manner.
In DynamicViT [24], self-distillation is introduced in the last layer to mini-
mize the performance drop brought by token sparsification. However, it ignores
hint knowledge in the intermediate layers, leading to inevitable knowledge loss.
To solve this issue, we introduce a concise Feature Recalibration Distillation
(FRD) to achieve stable and efficient model slimming optimization. Note that
previous hint knowledge distillation methods [25,42,39,17] are designed for spa-
tial structured tokens. Since the neighbor token information is contiguous, they
can apply contiguous upsampling ( e.g., deconvolution and interpolation) to find
the correspondence between tokens of teacher and student. In contrast, our TSM
generates unstructured token set, which can not be allocated corresponding su-
pervision directly. To align the token relations among unstructured tokens, we
design a reverse version of the token slimming module (RTSM) in a flexible auto-
encoder manner. Such an effective way helps our FRD densely transfer all the
token information block to block. Benefiting from the innate knowledge inher-
itance (structure knowledge), our FRD is more suitable for teaching itself. As
shown in Table 1, our SiT is able to improve the throughput by 43.2% without
any performance drop, and accelerate the inference speed by over 100% with
negligible accuracy decrease.
Our self-slimmed learning method is flexible and easy to generalize to all
vanilla vision transformers (SiT), e.g., DeiT [29], LV-ViT [16] etc. We conduct
extensive experiments on ImageNet [8] to verify the effectiveness and efficiency.
Interestingly, our method without self-distillation can perform even better than
DynamicViT [24] with distillation. Besides, the SiT-XS achieves 81.8% top-1
accuracy with 3.6×inference speed and SiT-L achieves competitive 85.6% top-1
accuracy while running 1.7×faster. More importantly, our SiT based on LV-ViT

--- PAGE 4 ---
4 Z. Zong, K. Li, et al.
𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓𝐓
𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐒𝐓𝐓𝐒𝐒𝐒𝐒
𝐓𝐓𝐒𝐒𝐓𝐓ℒtoken
X𝑖𝑖𝑠𝑠X𝑖𝑖𝑡𝑡
X𝑖𝑖+1𝑠𝑠X𝑖𝑖+1𝑡𝑡
ℒtoken
ℒcls
ℒhardℒlogits
⋯DIST
HeadCLS
Head
Z𝑠𝑠
Z𝑑𝑑Z𝑡𝑡
Transformer
Encoder
𝐑𝐑𝐓𝐓𝐒𝐒𝐓𝐓
𝐓𝐓𝐒𝐒𝐓𝐓Transformer
Encoder
𝐑𝐑𝐓𝐓𝐒𝐒𝐓𝐓⋯CLS
HeadTransformer
EncoderTransformer
Encoder
Feature
Recalibration
Distillatio n
Fig. 2: The framework of our self-slimmed learning. We insert our token
slimming modules (TSM) into vanilla vision transformer. To reduce decision-
relevant information loss, we apply Feature Recalibration Distillation (FRD),
wherein the reverse version of TSM (RTSM) is utilized to recalibrate unstruc-
tured token. The dash lines indicate the prediction supervision from an extra
CNN teacher is optional and complementary to our method.
achieves the new state-of-the-art performance on ImageNet, surpassing recent
CNNs and ViTs.
2 Related Works
2.1 Vision Transformers
Transformer architecture [31] was first proposed for machine translation. The
success of transformer in NLP inspires the application of transformers in various
vision tasks, for example, DETR [3] for object detection and ViT [10] for image
recognition. ViT is the first pure transformer that achieves the state-of-the-art
performance on ImageNet [8]. Recent ViT variants mainly focus on better opti-
mization and more powerful performance [43,41,40,34,13,4,9,35,11,7,38,15,19,18].
However, few of them explore to improve the efficiency of vision transformers
[12]. In this paper, we aim to design a general optimization framework named
self-slimmed learning to promote the efficiency of ViTs.
2.2 Transformer Slimming
The large computation of self-attention hinders the wide application of ViTs,
such as detection and segmentation with the high-resolution input image. To
solve this problem, several prior works concentrate on designing sparse atten-
tion [33,21] or structure pruning [6]. S2ViTE [6] dynamically extracts and trains
sparse subnetworks of ViTs, while sticking to a fixed small parameter budget.
However, model structure pruning struggles to trim down the inference latency.

--- PAGE 5 ---
Self-slimmed Vision Transformer 5
Other works try to reduce the token redundancy [24,28,22,37] by entirely drop-
ping the unimportant tokens, which brings more improvements on throughput
compared to structure pruning. Different from the above works, our SiT ag-
gregates all tokens into fewer informative tokens in a soft manner by a concise
slimming module. It can automatically zoom the attention scope to localize the
key object for better recognition.
3 Method
In this section, we describe our self-slimmed learning for vision transformer (SiT)
in detail. First, we introduce the overall architecture of SiT. Then, we explain
the vital design of our SiT, i.e., token slimming module (TSM) and feature
recalibration distillation (FRD). Finally, we thoroughly compare our TSM and
FRD with other counterparts.
3.1 Overview of Self-slimmed Learning
The overall framework is illustrated in Fig. 2. We first design a lightweight Token
Slimming Module (TSM) for conventional ViTs to perform token slimming, and
its reverse version (RTSM) to recalibrate unstructured tokens for hint knowledge
distillation. We divide the slimmed vision transformer into multiple stages ( e.g.,
4 stages as in prior works [12,21]), where different numbers of tokens participate
in feed-forward computation. To decrease the information loss, we propose a
block-to-block feature recalibration distillation (FRD), wherein the original vi-
sion transformer can serve as a teacher to minimize the difference between itself
and the slimmed student. Finally, we integrate TSM and FRD to form a general
self-slimmed learning method for all vanilla ViTs.
3.2 Token Slimming Module
Given a sequence of Ninput tokens with Cchannels X= [x1;x2;···;xN]∈
RN×C, (class token is omitted as it will never be pruned), token slimming aims
to dynamically aggregate the redundant tokens to generate ˆNinformative tokens
ˆX= [ˆx1;ˆx2;···;ˆxˆN]:
ˆX=ˆAX, (1)
where ˆA∈RˆN×Nis a normalized weight matrix:
ˆNX
i=1ˆAi,j= 1,where j= 1. . . N. (2)
Such operation is differentiable and friendly to end-to-end training. We fol-
low the design paradigm of self-attention [32] and propose a lightweight token
slimming module (TSM) shown in Fig. 3a:
ˆA= Softmax(Wqσ(XWk)T
τ), (3)

--- PAGE 6 ---
6 Z. Zong, K. Li, et al.
TSM
1.0%
Backbone
99.0%
TSM
0.8%
Backbone
99.2%𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑃𝑃𝑚𝑚
𝐹𝐹𝐹𝐹𝐹𝐹𝑃𝑃𝑚𝑚(a)TSM
(b)RTSM (c)TSM cost𝑃𝑃𝑚𝑚𝑚𝑚𝑟𝑃𝑃𝑟𝑟𝑚𝑚
𝑚𝑚𝑠𝑠𝑃𝑃𝑠𝑠𝑚𝑚𝑁𝑁×𝐶𝐶 𝑁𝑁×𝐶𝐶
2𝑁𝑁×�𝑁𝑁 �𝑁𝑁×𝐶𝐶
0.02 0.8⋯ 0.1
⋮⋮
0.05 0.1⋱⋮
⋯ 0.8
SoftmaxNorm
Linear
GELU
Linear
�𝑁𝑁×𝐶𝐶 𝐶𝐶
×4𝑁𝑁𝐶𝐶×𝑁𝑁 𝑁𝑁×𝐶𝐶
𝑃𝑃𝑚𝑚𝑚𝑚𝑟𝑃𝑃𝑟𝑟𝑚𝑚 𝑃𝑃𝑚𝑚𝑚𝑚𝑟𝑃𝑃𝑟𝑟𝑚𝑚
FFNNorm
Linear
GELU
Linear
Norm
Fig. 3: The token slimming module (TSM) and its reverse version (RTSM).
where Wk∈RC×C
2andWq∈RˆN×C
2are both learnable parameters. σandτ
represents the nonlinear function (GELU) and scaling factor respectively. Similar
to self-attention, TSM generates a global attention matrix, but it requires much
fewer overhead in terms of throughput and memory usage during both train-
ing and inference. Fig. 3c shows that TSM blocks only require negligible cost.
Thanks to the learnable scaling factor τ, the attention tends to be sparse in our
experiments, which means it learns to focus on the most informative tokens.
Hard dropping vs. soft slimming. The prior works have tried to compress
tokens via hard dropping [28,24], in which the slimming weight ˆAi,j∈ {0,1}
is a binary decision matrix, i.e., dropping or keeping the corresponding token.
However, this approach with binary decision leads to severe information loss if
numerous tokens are discarded. Such weakness limits their high efficiency on
ImageNet [8], wherein the objects often occupy a large part in the pictures.
On the contrary, we design soft slimming with a learnable normalized weight
ˆAi,j∈(0,1), which is able to discriminate the meaningful tokens in a global view.
As shown in Fig. 1c, our soft slimming can dynamically zoom the attention scope
to cover the significant regions for classification. It reveals that ˆAcan adaptively
become a one-hot matrix to help our SiT focus on the most informative part.
3.3 Feature Recalibration Distillation
Feature recalibration. Though token slimming significantly reduces the in-
ference latency, when using a large slimming rate, it inevitably decreases the
accuracy because of the information loss. Hint knowledge distillation is the com-
mon method to maintain meaningful information in intermediate layers, wherein

--- PAGE 7 ---
Self-slimmed Vision Transformer 7
the challenge is to align the feature semantics between student and teacher. Pre-
vious works [25,39] adopt spatial deconvolution or interpolation to cope with
this misalignment between spatial contiguous features. However, it is not suit-
able for slimmed unstructured tokens with spatially discrete semantics. To solve
this problem, we design a reverse version of the token slimming module (RTSM)
to recalibrate the unstructured tokens in a flexible auto-encoder manner (Fig.
3b). Therefore, all the token information can be seamlessly transferred from the
teacher. Note that we only perform RTSM when training, thus no extra compu-
tation is introduced during inference. We first linearly transform the informa-
tive tokens into plenty of token candidates, thus utilizing a non-linear function
(GELU) to filter the vital representations. Finally, another linear transformation
is performed to compress the token candidates:
ˆX′=A2(σ(A1ˆX)), (4)
where A1∈R4N×ˆNandA2∈RN×4Nin our experiments. To further enhance
the token representations, we introduce an extra multi-layer perception (MLP)
block [32] with residual connection [14]:
X′=ˆX′+ MLP( ˆX′). (5)
The recalibrated features X′will be forced to be consistent with the teacher
features in FRD, ensuring the sufficient information of the slimmed tokens ˆX.
Knowledge distillation. Due to the invariant model structure, we design a
block-to-block knowledge distillation for the recalibrated features:
Ltoken =1
LNLX
i=1NX
j=1(Xs
i,j−Xt
i,j)2, (6)
where Xs
i,jandXt
i,jrefer to the j-th token embedding at the i-th layer of the
student and teacher, respectively. Lmeans the layer number. Note that Xsrefers
to the recalibrated tokens X′in Eq. 5. With such reconstruction loss, the student
model will be forced to maintain as much as knowledge in the informative to-
kens ˆX. Besides, to further alleviate the classification performance deterioration
caused by token slimming, we introduce the logits distillation to minimize the
predictions difference between the student and teacher:
Llogits = KL( ψ(Zs), ψ(Zt)), (7)
where KL denotes Kullback–Leibler divergence loss and ψis the softmax func-
tion. Zsand Ztare respectively the predictions of the student and teacher model.
Moreover, the above FRD is complementary to the hard distillation in [29]:
Lhard= CrossEntropy( ψ(Zd), yc), (8)
where Zdindicates the prediction of distillation head and ycis a hard decision
of the extra CNN teacher. It can further improve the performance with longer
training epochs. Our final objective of distillation for self-slimmed learning is:
Ldist=λtokenLtoken +λlogitsLlogits +λhardLhard, (9)

--- PAGE 8 ---
8 Z. Zong, K. Li, et al.
𝐹𝐹𝑆𝑆𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝑆𝑆 𝑇𝑇𝑒𝑒𝑇𝑇𝐹𝐹𝑒𝑒𝑜𝑜 𝑒𝑒𝑈𝑈𝑒𝑒𝑜𝑜𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝑆𝑆 𝑇𝑇𝑒𝑒𝑇𝑇𝐹𝐹𝑒𝑒𝑜𝑜
Teacher𝐶𝐶𝑒𝑒𝑒𝑒𝐹𝐹𝑒𝑒𝑈𝑈𝐹𝐹𝑒𝑒𝐹𝐹𝑜𝑜
𝑈𝑈𝑈𝑈𝑜𝑜𝐹𝐹𝑈𝑈𝑈𝑈𝑒𝑒𝑒𝑒𝑒𝑒𝑈𝑈
𝐾𝐾𝐷𝐷StudentBlock2,⋯,𝑛𝑛
Block2,⋯,𝑛𝑛
Teacher𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹
𝑅𝑅𝐹𝐹𝑒𝑒𝐹𝐹𝑒𝑒𝑒𝑒𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝑒𝑒𝑒𝑒
𝐾𝐾𝐷𝐷StudentBlock2,⋯,𝑛𝑛
Block2,⋯,𝑛𝑛
Block1 Block1
(a) Structured Tokens.
𝐹𝐹𝑆𝑆𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝑆𝑆 𝑇𝑇𝑒𝑒𝑇𝑇𝐹𝐹𝑒𝑒𝑜𝑜 𝑒𝑒𝑈𝑈𝑒𝑒𝑜𝑜𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝑆𝑆 𝑇𝑇𝑒𝑒𝑇𝑇𝐹𝐹𝑒𝑒𝑜𝑜
Teacher𝐶𝐶𝑒𝑒𝑒𝑒𝐹𝐹𝑒𝑒𝑈𝑈𝐹𝐹𝑒𝑒𝐹𝐹𝑜𝑜
𝑈𝑈𝑈𝑈𝑜𝑜𝐹𝐹𝑈𝑈𝑈𝑈𝑒𝑒𝑒𝑒𝑒𝑒𝑈𝑈
𝐾𝐾𝐷𝐷StudentBlock2,⋯,𝑛𝑛
Block2,⋯,𝑛𝑛
Teacher𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹𝐹
𝑅𝑅𝐹𝐹𝑒𝑒𝐹𝐹𝑒𝑒𝑒𝑒𝑒𝑒𝐹𝐹𝐹𝐹𝐹𝐹𝑒𝑒𝑒𝑒𝑒𝑒
𝐾𝐾𝐷𝐷StudentBlock2,⋯,𝑛𝑛
Block2,⋯,𝑛𝑛
Block1 Block1 (b) Unstructured Tokens.
Fig. 4: Hint knowledge distillation for structured and unstructured tokens.
where λis the coefficient balancing the three distillation losses. We set λlogits =
2, λtoken = 2 by default. λhardis set to 1 when the CNN teacher is involved.
As for the training objective of self-slimmed learning, we treat the classification
task and the distillation task equally:
Lcls= CrossEntropy( ψ(Zs),y), (10)
Lglobal =Lcls+Ldist, (11)
where ymeans the ground truth, i.e., one-hot label.
FRD vs. other knowledge distillation. Firstly, current vision transformers
[29,30] simply select a strong teacher network with totally different architectures,
such as RegNet for DeiT. Only a few knowledge ( e.g., final predictions) can be
inherited, thus the semantic information in the intermediate layer is ignored. In
FRD, thanks to the consistency between the teacher and student, we naturally
conduct densely token-level supervision for each block, which greatly improves
the stability of the model mimicking. Besides, the popular hint knowledge dis-
tillation method [25,39] are mainly designed for spatial structured tokens. As
shown in Fig. 4a, they can simply apply local and contiguous upsampling to
reconstruct tokens. However, as shown in Fig. 4b, the token slimming generates
an unstructured token set. Each token contains partial information of previous
tokens. To recalibrate the unstructured features, we design a concise RTSM in a
flexible auto-encoder manner. Thus via reconstruction loss, our FRD can force
the student model to maintain sufficient knowledge in the informative tokens.
4 Experiments
4.1 Implementation Details
In this section, we conduct comprehensive experiments to empirically analyze the
effectiveness of our proposed self-slimmed learning for vision transformer (SiT).
All the models are evaluated on the ImageNet dataset [8]. For our teacher models,

--- PAGE 9 ---
Self-slimmed Vision Transformer 9
Table 2: Main results on ImageNet. We apply our self-slimming learning on
the state-of-the-art vanilla vision transformer LV-ViT [16]. ‡means we adopt an
extra CNN teacher. Our SiT can speed up LV-ViT 1.7×with a slight accuracy
drop. For fast inference, our SiT can maintain 97% of the performance while
speeding up the original transformers by 3.6×.
Model Depth StageEmbed
DimHeads Resolution#Params FLOPs
(M) (G)
SiT-Ti 14{1,1,1,11 }320 5 224215.9 1.0
SiT-XS 16{1,1,1,13 }384 6 224225.6 1.5
SiT-S 16 {9,3,2,2 } 384 6 224225.6 4.0
SiT-M 20{10,4,3,3 }512 8 224255.6 8.1
SiT-L 24{10,4,3,7 }768 12 2882148.2 34.4
(a) Model architecture settings
ModelStudent Teacher
Throughput Top-1 Top-1 ‡ Throughput Top-1
(image/s) (%) (%) (image/s) (%)
SiT-Ti 5896 ( 3.2×) 80.1 ( −2.0) 80.6 ( −1.5) 1827 82.1
SiT-XS 4839 ( 3.6×) 81.1 ( −2.2) 81.8 ( −1.5) 1360 83.3
SiT-S 1892 ( 1.4×) 83.2 ( −0.1) 83.4 (+0.1) 1360 83.3
SiT-M 1197 ( 1.5×) 84.1 ( −0.1) 84.3 (+0.1) 804 84.2
SiT-L 346 (1.7×) 85.6 ( −0.1) - 204 85.7
(b) Efficiency comparisons
we train LV-ViTs [16] following the original settings, but we replace the patch
embedding module with lightweight stacked convolutions inspired by LeViT [12].
As for student models, all the training hyper-parameters are the same as DeiT
[29] by defaults. For initialization, we load all the weights from the corresponding
teacher models to accelerate the convergence and train them for 125 epochs. If
utilizing an extra CNN teacher, we extend the training time to 300 epochs for
better improvements. Moreover, we set different initial learning rates for the
backbone and the feature recalibration branch, which are 0 .0002×batch size
1024and
0.001×batch size
1024respectively. For token slimming, we insert TSM three times,
thus there are four stages in SiT. The default keeping ratio ˆN/N is set to 0.5,
which means the token number is halved after slimming.
4.2 Main Results
We conduct our self-slimmed learning for LV-ViT [16], which is the state-of-the-
art vanilla vision transformer. Table 2 shows our detailed settings for different
SiT variants. For SiT-Ti and SiT-XS, we explore their capacity for fast inference,
thus we insert TSMs in the early layers. It demonstrates that our self-slimmed
method is able to speed up the original vision transformers by 3.6×, while main-
taining at least 97% of their accuracy. Besides, we adopt another CNN teacher
to provide the hard label as in DeiT [29]. The results show that complementary
prediction supervision can further improve performance. As for other variants,

--- PAGE 10 ---
10 Z. Zong, K. Li, et al.
0.25 0.50 0.75
FLOPs ratio6668707274767880ImageNet Top-1 Accuracy (%)
 DeiT-Small
DynamicViT
SiT w/o FRD
SiT
(a) Our SiT achieves much higher ac-
curacy than DynamicViT even with-
out the distillation.
1.0 1.5 2.0 2.5 3.0
GFLOPs747576777879808182ImageNet Top-1 Accuracy (%)
DeiT-TXCiT-T12XCiT-T24
CaiT-XXS24DyViT-320/0.7PS-ViT-SS2ViTE-SSiT-TiSiT-XS
Random samples
Keeping ratio
Distilled ViTs
Token dropping
Model pruning(b) Our randomly sampled models
consistently outperform other distilled
and pruned ViTs.
Fig. 5: Effectiveness and robustness study. We compare our SiT with the
DynamicViT in Fig(a). To verify the robustness of our method, we randomly
change the numbers of blocks in each stage ( i.e., TSM location) and adjust the
keeping ratio of TSM from 0.3 to 0.7 to sample a series of SiT-Ti models in
Fig(b). All the models are trained for 125 epochs without a CNN teacher.
we insert TSMs in the deeper layers. Surprisingly, with negligible accuracy drop,
our SiTs are up to 1.7×faster than their teacher models. It is worth mentioning
that, extra CNN distillation brings little improvement, mainly because the CNN
teacher is inferior to the original transformer teacher (82 .9%vs.83.3%/84.2%).
4.3 Effectiveness and Robustness
Comparison to DynamicViT. In Fig. 5a, we compare our method with Dy-
namicViT [24] on DeiT-S [29]. When dropping too many tokens, the performance
of DynamicViT deteriorates dramatically. Though it utilizes knowledge distilla-
tion to minimize the gap, our SiT without the distillation consistently surpasses
it under different FLOPs ratios, especially under the smallest ratio. Besides,
when armed with FRD, our SiT can maintain performance better.
TSM locations and keeping ratio. To verify the robustness of our method, we
conduct experiments on SiT-Ti as shown in Fig. 5b. It clearly shows that all of the
randomly sampled models outperform popular ViTs with knowledge distillation,
e.g., DeiT [29] and XCiT [1]. Besides, compared with other counterparts based
on token hard dropping [24,28] and structure pruning [6], our models surpass
them by a large margin. These results demonstrate our SiT is insensitive to the
setting of TSM locations and keeping ratio. To make a fair comparison with the
state-of-the-art ViTs, we set these hyper-parameters according to the FLOPs.

--- PAGE 11 ---
Self-slimmed Vision Transformer 11
Table 3: Comparison to the state-of-the-art on ImageNet. The models
marked in gray color are trained with distillation supervision from a powerful
CNN for 300 epochs. Our SiT achieves the best performance trade-off.
Model Resolution#Params FLOPs Throughput ImageNet
(M) (G) (image/s) Top-1(%)
EfficientNet-B1 [26] 24027.8 0.7 2559 79.1
EfficientNet-B2 [26] 26029.1 1.1 1808 80.1
DeiT-T [29] 22425.9 1.3 3346 74.5
LeViT-256 [12] 224218.9 1.1 5802 80.1
SiT-Ti 224215.9 1.0 5896 80.1
SiT-Ti 224216.2 1.0 5833 80.6
EfficientNet-B3 [26] 300212.2 1.9 1062 81.6
Swin-T [21] 224228.3 4.5 1023 81.3
DeiT-S [29] 224222.4 4.6 1598 81.2
LeViT-384 [12] 224239.1 2.4 3876 81.6
SiT-XS 224225.6 1.5 4839 81.1
SiT-XS 224226.0 1.5 4798 81.8
EfficientNet-B4 [26] 380219.3 4.6 545 82.9
Swin-B [21] 224287.8 15.5 474 83.3
DeiT-B [29] 224287.3 17.7 718 83.4
LV-ViT-S [16] 224226.2 6.6 1270 83.3
SiT-S 224225.6 4.0 1892 83.2
SiT-S 224226.0 4.0 1873 83.4
EfficientNet-B6 [26] 528243.0 19.9 153 84.0
EfficientNetV2-S [27] 384221.5 8.5 742 83.9
CaiT-S36 [30] 224268.2 13.9 233 83.9
LV-ViT-M [16] 224255.8 12.7 768 84.1
SiT-M 224255.6 8.1 1197 84.1
SiT-M 224256.2 8.1 1185 84.3
EfficientNetV2-M [27] 480254.1 25.0 271 85.1
NFNet-F1 [2] 3202132.6 36.0 128 84.7
CaiT-M36 [30] 2242270.1 53.4 130 85.1
LV-ViT-L [16] 2882150.1 58.8 208 85.3
SiT-L 2882148.2 34.4 346 85.6
4.4 Comparison to state-of-the-art
In Table 3, we compare SiT with other competitive CNNs and ViTs. For a fair
comparison, we group these methods according to their top-1 accuracies. The
throughput is measured on a single 16GB V100 GPU under the same setting as
LeViT [12]. Our SiT-Ti is competitive with LeViT, while the throughput is 3.2×
than that of EfficientNet [26]. Note that EfficientNet is designed via extensive
neural architecture search and LeViT is elaborately designed for fast inference.
For our larger model variants, they perform better than EfficientNetV2 [27] with
simple training strategies. Compared with the original LV-ViT [16], our SiT is
1.5×faster than those with similar accuracy.
We further visualize the comparisons to the upper bounds of CNNs and ViTs
in Fig. 6a and 6b. It clearly shows that our SiT achieves the best balance between
throughput and accuracy, surpassing the recent state-of-the-art CNNs and ViTs.

--- PAGE 12 ---
12 Z. Zong, K. Li, et al.
6 7 8 9 10 11 12 13
log2(images/s)787980818283848586ImageNet Top-1 Accuracy (%)
SiT-TiSiT-XSSiT-SSiT-MSiT-L
EffNetV2-B0EffNetV2-B1EffNetV2-B3EffNetV2-SEffNetV2-M  
EffNet-B1EffNet-B2EffNet-B3EffNet-B4EffNet-B5EffNet-B6EffNet-B7
NFNet-F0NFNet-F1NFNet-F2
X101-64x4dSE-X101-64x4dSENet154SiT
CNNs upper bound
(a) Comparison to CNNs.
7 8 9 10 11 12 13
log2(images/s)7980818283848586ImageNet Top-1 Accuracy (%)
SiT-TiSiT-XSSiT-SSiT-MSiT-L
LeViT-256*LeViT-384*LVViT-S+LVViT-M+LVViT-L+
CaiT-XXS36CaiT-XS36CaiT-S36CaiT-M36
Swin-TSwin-SSwin-B
DeiT-SDeiT-BSiT
ViTs upper bound (b) Comparison to ViTs.
Fig. 6: Speed vs.Accuracy. We compare our SiT with the previous state-of-
the-art CNNs and ViTs in Fig(a) and Fig(b), respectively. “LV-ViT+” denotes
our improved LV-ViT teacher. Our SiT surpasses the SOTA methods by a large
margin, even the efficient models EfficientNetV2 [27] and LeViT [12].
Table 4: Efficiency comparison.
Method Top-1 Throughput
Structure-width 76.3 2947
Structure-depth 69.4 5652
DynamicViT[24] 75.7 5762
SiT w/o FRD 77.7 5896Table 5: Inherited knowledge.
KnowledgeSelf CaiT RegNet
83.3 83.5 82.9
Scratch 80.1 79.9 79.2
Fine-tuning 80.5 80.2 80.0
Fine-tuning+Structure 81.1 80.6 80.2
4.5 Ablation Studies
If not otherwise specified, all experiments for ablations are conducted on SiT-
Ti and run with only 125 training epochs under the supervision of the original
teacher model. “Token-MLP” refers double linear layers along the token dimen-
sion.
Does token slimming outperform model scaling down? In Table 4, we
compare token slimming with model scaling down rules under the same computa-
tion limit. For model scaling down, we adapt the channel and depth individually.
Note that the above two models are trained from scratch for 300 epochs with
token labeling [16]. For token slimming, we simply insert TSMs without FRD.
We also drop tokens and train it with extra distillation as in DynamicViT [24]. It
shows that scaling along the channel achieves higher accuracy than scaling along
the depth but with lower throughput. Besides, token slimming can largely im-
prove the throughput with higher performance. However, DynamicViT performs
worse than our SiT without distillation, since token hard dropping loses much
discriminative information with a large slimming ratio. Such results demonstrate
simply inserting our TSM into vanilla ViT is able to achieve great performance.
Does structure knowledge matter to self-slimmed learning? We further
investigate whether the structure knowledge benefits the performance as shown

--- PAGE 13 ---
Self-slimmed Vision Transformer 13
Table 6: Robustness of
slimming ratios.
Ratio Llogits+LtokenLhard
1 82.1 82.1
0.75 82.0 82.0
0.5 81.6 81.3
0.25 80.1 78.4Table 7: Slimming.
Method GFLOPs Top-1
Baseline 3.5 82.1
3×3 AvgPool 1.0 77.4
3×3 Conv 1.0 79.3
Token-Mixer 1.1 79.3
Our TSM 1.0 80.1Table 8: Recalibration.
Method Top-1
Baseline 79.0
Interpolation 78.3
Deconvolution 78.4
Token-MLP 79.0
Our RTSM 80.1
Table 9: Knowledge distillation.
Method Top-1
Baseline 77.7
+Llogits 79.0(+ 1.3)
+Llogits+Ltoken 80.1(+ 2.4)
+Llogits+Ltoken+Lhard 80.2(+ 2.5)
+Llogits+Ltoken+Lhard+Longer training 80.6(+ 2.9)Table 10: Loss weights.
λtoken:λlogit:λhard Top-1
1:1:1 79.3
1:2:1 79.4
1:2:2 79.5
2:1:1 79.6
2:2:1 79.6
in Table 5. For the teacher models, we adopt different architectures (LV-ViT-
S[16], CaiT-S24[30], and RegNetY-16GF[23]) but similar accuracies for a fair
comparison. It shows that training with the pre-trained weights for 125 epochs
converges to higher results than those trained from scratch for 300 epochs. More-
over, we utilize structure knowledge via block-to-block mimicking, which can
further boost the performance. It also reveals that higher similarity between
students and teachers can bring greater improvements.
Is self-slimmed learning robust to different FLOPs ratios? In Table
6, we empirically train models with different FLOPs ratios. When the ratio is
large than 0.5, our FRD and CNN distillation are both helpful for maintaining
performance. However, when the ratio is small, CNN distillation leads to a higher
performance drop, while our FRD only drops the accuracy by 2.0%. These results
demonstrate that our method is robust to different FLOPs ratios.
Dynamic vs.Static: Which aggregation manner works better for to-
ken slimming? To explore whether dynamic aggregation is better for token
slimming, we perform ablation experiments as shown in Table 7. For static ag-
gregation, we choose different data-independent operations and maintain simi-
lar computation: 3 ×3 average pooling/convolution with stride 2 ×2, and double
linear layers with GELU function (“Token-MLP”). It shows that learnable pa-
rameters are vital for token slimming since average pooling leads to a severe
accuracy drop. Besides, the static aggregation methods with data-independent
weights yield similar but inferior performance to our TSM (79.3% vs.80.1%).
Such comparisons prove that our TSM can generate more informative tokens.
Can contiguous upsampling recalibrate the features? We first recalibrate
the original tokens by contiguous upsampling methods, e.g., bilinear interpo-
lation and deconvolution. As shown in Table 8, these two spatial contiguous
methods misalign the token relations and hurt the capacity compared with the
baseline (without block-to-block mimicking). In contrast, “Token-MLP” does
not hurt the token representation, and its accuracy can be further boosted to
80.1% by the insertion of an MLP.

--- PAGE 14 ---
14 Z. Zong, K. Li, et al.
Stage 1 Stage 2 Stage 4 Stage 3
 Stage 1 Stage 2 Stage 4 Stage 3 Stage 1 Stage 2 Stage 4 Stage 3
Fig. 7: Visualizations of our progressive token slimming. The blue/red
tokens contribute less/more to the final informative tokens. Our method can
zoom the attention scope to cover the key object, even with only 12.5% of tokens.
Does each distillation supervision help? Table 9 presents that the soft
logits supervision Llogits brings 1.4% accuracy gain. When further introducing
block-to-block knowledge supervision, our model improves the accuracy by 1.1%.
Finally, combining complementary hard label supervision, the accuracy reaches
80.6% with longer training epochs.
What are the appropriate loss weights? Table 10 shows the settings of loss
wights are robust in our SiT (trained for 100 epochs). In fact, we simply choose
the weight of 2:2:1 to ensure different loss values are close in the early training.
4.6 Visualization
Qualitative token slimming visualization. Fig. 7 shows the original images
and the token slimming procedure of our SiT-Ti. We observe that the tokens of
higher scores, i.e., brighter tokens, are concentrated and tend to cover the key
objects in the image. It demonstrates our proposed TSM is able to localize the
significant regions and predict accurate scores for the most informative tokens.
5 Conclusions
In this paper, we propose a generic self-slimmed learning method for vanilla vi-
sion transformers (SiT), which can speed up the ViTs with negligible accuracy
drop. Our concise TSM softly integrates redundant tokens into fewer informative
ones. For stable and efficient training, we introduce a novel FRD framework to
leverage structure knowledge, which can densely transfer token information in
a flexible auto-encoder manner. Extensive experiments demonstrate the effec-
tiveness of our SiT. By simply arming LV-ViT with our SiT, we achieve new
state-of-the-art performance on ImageNet, surpassing recent CNNs and ViTs.
Acknowledgements. This work is partially supported by National Key R&D
Program of China under Grant 2019YFB2102400, National Natural Science
Foundation of China (61876176), the Joint Lab of CAS-HK, Shenzhen Insti-
tute of Artificial Intelligence and Robotics for Society, the Shanghai Committee
of Science and Technology (Grant No. 21DZ1100100).

--- PAGE 15 ---
Self-slimmed Vision Transformer 15
References
1. Ali, A., Touvron, H., Caron, M., Bojanowski, P., Douze, M., Joulin, A., Laptev,
I., Neverova, N., Synnaeve, G., Verbeek, J., et al.: Xcit: Cross-covariance image
transformers. Advances in neural information processing systems (2021)
2. Brock, A., De, S., Smith, S.L., Simonyan, K.: High-performance large-scale im-
age recognition without normalization. In: International Conference on Machine
Learning (2021)
3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-
to-end object detection with transformers. In: European conference on computer
vision (2020)
4. Chen, C.F.R., Fan, Q., Panda, R.: Crossvit: Cross-attention multi-scale vision
transformer for image classification. In: Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision (2021)
5. Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C., Gao,
W.: Pre-trained image processing transformer. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (2021)
6. Chen, T., Cheng, Y., Gan, Z., Yuan, L., Zhang, L., Wang, Z.: Chasing sparsity in
vision transformers: An end-to-end exploration. Advances in Neural Information
Processing Systems (2021)
7. Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.:
Twins: Revisiting the design of spatial attention in vision transformers. Advances
in Neural Information Processing Systems (2021)
8. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (2009)
9. Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin
transformer: A general vision transformer backbone with cross-shaped windows.
ArXiv abs/2107.00652 (2021)
10. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations (2020)
11. d’Ascoli, S., Touvron, H., Leavitt, M.L., Morcos, A.S., Biroli, G., Sagun, L.: Con-
vit: Improving vision transformers with soft convolutional inductive biases. In:
International Conference on Machine Learning (2021)
12. Graham, B., El-Nouby, A., Touvron, H., Stock, P., Joulin, A., J´ egou, H., Douze, M.:
Levit: a vision transformer in convnet’s clothing for faster inference. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision (2021)
13. Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer.
Advances in Neural Information Processing Systems (2021)
14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
IEEE Conference on Computer Vision and Pattern Recognition (2016)
15. Heo, B., Yun, S., Han, D., Chun, S., Choe, J., Oh, S.J.: Rethinking spatial di-
mensions of vision transformers. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision (2021)
16. Jiang, Z.H., Hou, Q., Yuan, L., Zhou, D., Shi, Y., Jin, X., Wang, A., Feng, J.: All
tokens matter: Token labeling for training better vision transformers. Advances in
Neural Information Processing Systems (2021)

--- PAGE 16 ---
16 Z. Zong, K. Li, et al.
17. Kim, J., Park, S., Kwak, N.: Paraphrasing complex network: Network compression
via factor transfer. Advances in neural information processing systems (2018)
18. Li, K., Wang, Y., Zhang, J., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.J.:
Uniformer: Unifying convolution and self-attention for visual recognition. ArXiv
abs/2201.09450 (2022)
19. Li, Y., Zhang, K., Cao, J., Timofte, R., Gool, L.V.: Localvit: Bringing locality to
vision transformers. ArXiv abs/2104.05707 (2021)
20. Liang, Y., GE, C., Tong, Z., Song, Y., Wang, J., Xie, P.: EVit: Expediting vision
transformers via token reorganizations. In: International Conference on Learning
Representations (2022)
21. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin
transformer: Hierarchical vision transformer using shifted windows. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision (2021)
22. Pan, B., Panda, R., Jiang, Y., Wang, Z., Feris, R., Oliva, A.: Ia-red2:
Interpretability-aware redundancy reduction for vision transformers. Advances in
Neural Information Processing Systems (2021)
23. Radosavovic, I., Kosaraju, R.P., Girshick, R.B., He, K., Doll´ ar, P.: Designing net-
work design spaces. 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2020)
24. Rao, Y., Zhao, W., Liu, B., Lu, J., Zhou, J., Hsieh, C.J.: Dynamicvit: Efficient
vision transformers with dynamic token sparsification. Advances in neural infor-
mation processing systems (2021)
25. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:
Hints for thin deep nets. CoRR abs/1412.6550 (2015)
26. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural
networks. In: International conference on machine learning (2019)
27. Tan, M., Le, Q.: Efficientnetv2: Smaller models and faster training. In: Interna-
tional Conference on Machine Learning (2021)
28. Tang, Y., Han, K., Wang, Y., Xu, C., Guo, J., Xu, C., Tao, D.: Patch slimming
for efficient vision transformers. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 12165–12174 (2022)
29. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training
data-efficient image transformers & distillation through attention. In: International
Conference on Machine Learning (2021)
30. Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., J´ egou, H.: Going deeper
with image transformers. In: Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (2021)
31. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
 L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems (2017)
32. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
 L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems (2017)
33. Wang, P., Wang, X., Wang, F., Lin, M., Chang, S., Xie, W., Li, H., Jin, R.: Kvt:
k-nn attention for boosting vision transformers. ArXiv abs/2106.00515 (2021)
34. Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao,
L.: Pyramid vision transformer: A versatile backbone for dense prediction with-
out convolutions. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision (2021)

--- PAGE 17 ---
Self-slimmed Vision Transformer 17
35. Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: In-
troducing convolutions to vision transformers. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (2021)
36. Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer:
Simple and efficient design for semantic segmentation with transformers. Advances
in Neural Information Processing Systems (2021)
37. Xu, Y., Zhang, Z., Zhang, M., Sheng, K., Li, K., Dong, W., Zhang, L., Xu, C., Sun,
X.: Evo-vit: Slow-fast token evolution for dynamic vision transformer. Proceedings
of the AAAI Conference on Artificial Intelligence (2022)
38. Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., Gao, J.: Focal self-attention
for local-global interactions in vision transformers. ArXiv abs/2107.00641 (2021)
39. Yim, J., Joo, D., Bae, J.H., Kim, J.: A gift from knowledge distillation: Fast op-
timization, network minimization and transfer learning. 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2017)
40. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.H., Tay, F.E., Feng, J., Yan,
S.: Tokens-to-token vit: Training vision transformers from scratch on imagenet.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(2021)
41. Yuan, L., Hou, Q., Jiang, Z., Feng, J., Yan, S.: Volo: Vision outlooker for visual
recognition. ArXiv abs/2106.13112 (2021)
42. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving
the performance of convolutional neural networks via attention transfer. ArXiv
abs/1612.03928 (2017)
43. Zhou, D., Kang, B., Jin, X., Yang, L., Lian, X., Hou, Q., Feng, J.: Deepvit: Towards
deeper vision transformer. ArXiv abs/2103.11886 (2021)

--- PAGE 18 ---
18 Z. Zong, K. Li, et al.
Table 11: Details of teacher models. The head dimensions of all the models are
set to 64. ‘ks’ means kernel size. ‘st’ means stride. ‘oc’ means output channel
number.
Teacher Student Resolution Depth HeadsPatch Stem
(ks, st, oc)
SiT-Ti 224214 5(3×3, 2×2, 40)
Our (3×3, 2×2, 80)
LV-ViT-Ti (3×3, 2×2, 160)
(3×3, 2×2, 320)
224216 6(3×3, 2×2, 48)
Our SiT-XS (3×3, 2×2, 96)
LV-ViT-S SiT-S (3×3, 2×2, 192)
(3×3, 2×2, 384)
SiT-M 224220 8(3×3, 2×2, 64)
Our (3×3, 2×2, 128)
LV-ViT-M (3×3, 2×2, 256)
(3×3, 2×2, 512)
SiT-L 288224 12(3×3, 2×2, 96)
(3×3, 1×1, 96)
Our (3×3, 1×1, 96)
LV-ViT-L (3×3, 2×2, 192)
(3×3, 2×2, 384)
(3×3, 2×2, 768)
Table 12: Robustness analysis based on our LV-ViT-S.
Ratio Llogits+Ltoken Lhard
1 83.3 83.3
0.75 83.2 83.0
0.5 82.6 82.2
0.25 80.9 80.0
A More details about teacher models
Table 11 shows more details about our teacher models. We elaborately design
different patch stems for our LV-ViT [16] models.
B More robustness analysis.
We conduct more analysis based on our LV-ViT-S in Table 12. It shows that
our self-slimmed learning is also robust to different FLOPs ratios on LV-ViT-
S. Moreover, our method still performs better than CNN distillation on larger
model.

--- PAGE 19 ---
Self-slimmed Vision Transformer 19
Table 13: More results on DeiT. “DeiT P” indicates the original DeiT and
“DeiT C” refers to the variant with lightweight convolutional patch embedding
stacked by four 3 ×3 convolutions (2 ×2 stride) and one point-wise convolution.
ModelFLOPs FLOPs LlogitsLhardThroughput ImageNet
ratio (G) +Ltoken (image/s) Top-1(%)
DeiT P-S0.251.1 % % 6413( 3.9×) 71.6(-8.2)
1.1 " % 6413( 3.9×) 75.9(-3.9)
1.1 % " 6286( 3.8×) 72.9(-6.9)
1.1 " " 6286( 3.8×) 75.3(-4.5)
0.52.3 % % 3308( 2.0×) 78.6( −1.3)
2.3 " % 3308( 2.0×) 79.4( −0.4)
2.3 % " 3262( 2.0×) 78.8( −1.0)
2.3 " " 3262( 2.0×) 79.8(+ 0.0)
1 4.6 % % 1637 79.8
DeiT C-S0.251.1 % % 5898( 3.7×) 76.1(-3.9)
1.1 " % 5898( 3.7×) 78.4(-1.6)
1.1 % " 5830( 3.7×) 77.5(-2.5)
1.1 " " 5830( 3.7×) 78.8(-1.2)
0.52.3 % % 3150( 2.0×) 79.1( −0.9)
2.3 " % 3150( 2.0×) 79.9( −0.1)
2.3 % " 3106( 1.9×) 80.3(+ 0.3)
2.3 " " 3106( 1.9×) 80.6(+ 0.6)
1 4.6 % % 1597 80.0
C More experiments on DeiT
We also verify the effectiveness of our self-slimmed learning on DeiT as illustrated
in Table 13. For the FLOPs ratio of 0.5 and 0.25, the stage numbers are {3,4,3,2 }
and{1,1,1,9 }respectively. Specifically, we conduct the experiments on the orig-
inal DeiT [29] and its variant with lightweight convolutional patch embedding.
Both models achieve similar accuracy with the same computational costs. How-
ever, we observe the performance of their students is quite different especially at
a small FLOPs ratio. DeiT Psuffers severe performance deterioration when 75%
computation is reduced, while DeiT Conly drops the accuracy by 2.5%. More
importantly, DeiT Cgenerally obtain higher accuracies than DeiT Pat a rela-
tively higher FLOPs ratio. It demonstrates that the models with convolutional
patch embedding are more redundant and friendly to slimming. In addition, we
also compare our DKD with the CNN distillation under different settings. The
layer-to-layer dense knowledge distillation consistently brings more performance
gains than CNN distillation. It is worth mentioning that, self-slimmed learning is
also complementary to the extra CNN distillation. Surprisingly, the best student
model of DeiT Ceven outperforms the teacher by 0 .6% top-1 accuracy while run-
ning 2 ×faster under the joint supervision. These results prove the effectiveness
and generalization ability of our self-slimmed learning.

--- PAGE 20 ---
20 Z. Zong, K. Li, et al.
Table 14: Comparisons between DynamicViT and our SiT on DeiT.
ModelFLOPs
ratio#FLOPs
(G)DynamicViT SiT
Throughput ImageNet Throughput ImageNet
(image/s) Top-1(%) (image/s) Top-1(%)
DeiT P-S0.25 1.1 6254( 3.8×) 65.6(-14.2) 6413 (3.9×)75.9(−3.9)
0.5 2.3 3248( 2.0×) 78.4(-1.4) 3308 (2.0×)79.4(−0.4)
1 4.6 1637 79.8 1637 79.8
DeiT C-S0.25 1.1 5689( 3.6×) 73.4(-6.6) 5898 (3.7×)78.4(−1.6)
0.5 2.3 3092( 1.9×) 79.2(-0.8) 3150 (2.0×)79.9(−0.1)
1 4.6 1597 80.0 1597 80.0
As described in Table 14, we further compare our self-slimmed learning with
the recent method, i.e., DynamicViT. We observe that our SiT runs slightly
faster than DynamicViT with the same FLOPs, which reveals our TSM presents
better inference efficiency than the prediction module of DynamicViT. More
importantly, thanks to the soft-slimming designs, SiT outperforms DynamicViT
by a large margin (5.3%-10.0%) at the FLOPs ratio of 0.25. For the large FLOPs
ratio, our SiT still obtains at least 0.7% higher accuracy than DynamicViT,
proving the soft slimming triumphs the hard dropping manner.
D More experiments on Swin Transformer
ModelBaseline Baseline+SiT
Throughput Top-1 Throughput Top-1
Swin-T 1023 81.2 1183 ( +15.6% ) 81.2
Swin-S 652 83.0 855 (+31.1% ) 83.0
Table 15: SiT for hierarchical networks.
Note that the recent slimming methods [24] only works for vanilla ViTs. Since
the hierarchical ViTs generally introduce structured operations like convolution
and relative position bias, it’s not suitable for arbitrary token dropping. To verify
the generality of our SiT, we adapt the typical hierarchical network (i.e., Swin)
with SiT, modifying some of the structured operations. Table 15 shows that
arming Swin with SiT, we can also improve its throughput without accuracy
drop. We will focus on more elegant token slimming method in the future.
E More visualizations
Qualitative token slimming visualization. We present more visualizations
of our progressive token slimming in Figure 9.

--- PAGE 21 ---
Self-slimmed Vision Transformer 21
LV-ViT-S RegNet -16GF CaiT -S24CKA mean: 0.85 CKA mean: 0.33 CKA mean: 0.38
 CKA mean: 0.75
scratch finetune finetune finetune
Fig. 8: Cross CKA heatmap between different student models and the
teacher models. We adopt LV-ViT-S [16] as student. Transfering knowledge
densely from same structure yields the largest similarity.
Qualitative FRD visualization. In Fig. 8, we compute the CKA [ ?] heatmap
by comparing all layers of the student models (LV-ViT-S) with all layers of
their teacher models. It shows that the CKA similarities between the similar
structures are generally higher than those between different structures (0.75/0.85
vs.0.33/0.38). Interestingly, we find the pre-trained weights inherited by the
student force itself to be similar to its teacher. Besides, for similar structures,
the CKA similarities in the shallow layers are higher than those in deep layers.
It is mainly because we slim a large number of tokens after the third layer,
leading to an inevitable information loss. As for different structures, the CKA
similarities in the deep layers are higher than those in shallow layers, which
is mainly because the logits distillation provides direct supervision for features
in the deeper layers. Note that the above observations are consistent with the
results in our experiments, which reveals that teachers with similar structures
can transfer structure knowledge better for higher performance.

--- PAGE 22 ---
22 Z. Zong, K. Li, et al.
Stage 1Stage 2Stage4Stage 3Stage 1Stage 2Stage4Stage 3Stage 1Stage 2Stage4Stage 3Stage1Stage2Stage3Stage4Stage1Stage2Stage3Stage4Stage1Stage2Stage3Stage4
Fig. 9: More visualizations of our SiT.
