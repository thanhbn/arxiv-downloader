# 2303.08128.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2303.08128.pdf
# Kích thước tệp: 33179583 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
ViperGPT : Suy luận Thị giác thông qua Thực thi Python để Lý luận
Dídac Surís*, Sachit Menon*, Carl V ondrick
Đại học Columbia
viper.cs.columbia.edu
Tóm tắt
Trả lời các truy vấn thị giác là một nhiệm vụ phức tạp đòi hỏi
cả xử lý thị giác và lý luận. Các mô hình end-to-end,
phương pháp thống trị cho nhiệm vụ này, không phân biệt rõ ràng
giữa hai khía cạnh này, hạn chế khả năng diễn giải và tổng quát hóa.
Học các chương trình modular đưa ra một giải pháp thay thế đầy hứa hẹn,
nhưng đã tỏ ra thách thức do sự khó khăn
của việc học đồng thời cả chương trình và module.
Chúng tôi giới thiệu ViperGPT, một framework tận dụng các mô hình
sinh mã để kết hợp các mô hình thị giác-và-ngôn ngữ
thành các chương trình con để tạo ra kết quả cho bất kỳ truy vấn nào. ViperGPT
sử dụng một API được cung cấp để truy cập các module có sẵn, và
kết hợp chúng bằng cách sinh mã Python sau đó được
thực thi. Phương pháp đơn giản này không yêu cầu huấn luyện thêm,
và đạt được kết quả state-of-the-art trên nhiều nhiệm vụ
thị giác phức tạp khác nhau.

1. Giới thiệu
Có bao nhiêu chiếc bánh muffin mà mỗi đứa trẻ trong Hình 1 (trên) có thể ăn để
công bằng? Để trả lời điều này, chúng ta có thể 1) tìm những đứa trẻ
và bánh muffin trong hình ảnh, 2) đếm có bao nhiêu
từng loại, và 3) lý luận rằng 'công bằng' có nghĩa là chia đều, do đó
chia. Con người thấy tự nhiên khi kết hợp từng bước riêng lẻ
với nhau để hiểu thế giới thị giác.
Tuy nhiên, phương pháp thống trị trong lĩnh vực thị giác máy tính
vẫn là các mô hình end-to-end, không tận dụng
lý luận kết hợp này một cách nội tại.

Mặc dù lĩnh vực này đã có tiến bộ lớn trong các
nhiệm vụ riêng lẻ như nhận dạng đối tượng và ước tính độ sâu, các phương pháp end-
to-end cho các nhiệm vụ phức tạp phải học để thực hiện
tất cả các nhiệm vụ trong quá trình forward pass của mạng neural.
Điều này không chỉ thất bại trong việc tận dụng những tiến bộ
trong các nhiệm vụ thị giác cơ bản ở các bước khác nhau, mà còn không
tận dụng thực tế rằng máy tính có thể thực hiện các phép toán
(ví dụ, phép chia) một cách dễ dàng mà không cần học máy.
Chúng ta không thể tin tưởng các mô hình neural sẽ tổng quát hóa một cách
có hệ thống với số lượng bánh muffin hoặc trẻ em khác nhau. Các mô hình End-
to-end cũng tạo ra các quyết định cơ bản không thể diễn giải
– không có cách nào để kiểm tra kết quả của từng bước
để chẩn đoán lỗi. Khi các mô hình ngày càng đói dữ liệu và
tính toán, phương pháp này ngày càng trở nên không bền vững.
Chúng tôi muốn thực hiện các nhiệm vụ mới mà không cần huấn luyện thêm
bằng cách kết hợp lại các mô hình hiện có theo những cách mới.

Điều gì hạn chế chúng ta không tạo ra các hệ thống modular như vậy cho
các nhiệm vụ phức tạp hơn? Trong những năm trước, các công trình tiên phong
của Neural Module Networks [2, 27, 19] đã cố gắng
phân tách các nhiệm vụ thành các module đơn giản hơn. Bằng cách huấn luyện end-to-
end với các module được sắp xếp lại theo những cách khác nhau cho các
vấn đề khác nhau, hy vọng là mỗi module sẽ học
chức năng phù hợp của chúng và do đó trở nên có thể tái sử dụng. Tuy nhiên,
nhiều vấn đề đã khiến phương pháp này khó mở rộng
ra thế giới thực. Đặc biệt, việc sinh chương trình dựa vào
các parser ngôn ngữ tự nhiên được điều chỉnh thủ công [2], hoặc
yêu cầu học tăng cường từ đầu và do đó khó
tối ưu hóa [19, 27]. Trong mỗi trường hợp, việc sinh chương trình
bị hạn chế rất nhiều về domain. Hơn nữa, việc học các
mô hình perceptual cùng với bộ sinh chương trình làm cho
việc huấn luyện còn khó khăn hơn, thường thất bại trong việc tạo ra
cấu trúc modular dự định [3, 48].

Trong công trình này, chúng tôi trình bày ViperGPT1, một framework
vượt qua những nút thắt này bằng cách tận dụng các mô hình
ngôn ngữ lớn sinh mã (ví dụ GPT-3 Codex [9]) để
kết hợp linh hoạt các mô hình thị giác dựa trên bất kỳ truy vấn văn bản nào
định nghĩa nhiệm vụ. Nó tạo ra các chương trình tùy chỉnh cho mỗi
truy vấn nhận hình ảnh hoặc video làm đối số và trả về
kết quả của truy vấn cho hình ảnh hoặc video đó. Chúng tôi cho thấy rằng
việc cung cấp cho Codex một API tiết lộ các khả năng thị giác khác nhau
(ví dụ find, compute_depth), giống như người ta có thể cung cấp
cho một kỹ sư, là đủ để tạo ra các chương trình này.
Việc huấn luyện trước của mô hình trên mã cho phép nó lý luận
về cách sử dụng các hàm này và thực hiện logic liên quan.
Kết quả của chúng tôi chứng minh rằng phương pháp đơn giản này mang lại
hiệu suất zero-shot đáng chú ý (tức là không bao giờ
huấn luyện trên hình ảnh cụ thể của nhiệm vụ).

Phương pháp đơn giản của chúng tôi có nhiều lợi ích: nó 1) có thể diễn giải,
vì tất cả các bước đều rõ ràng như các lời gọi hàm mã
1Chúng tôi đặt tên phương pháp theo con rắn vì nó thực thi mã Python.

--- TRANG 2 ---
Truy vấn: Bạn nhận được màu gì nếu kết hợp màu của con rắn độc và bông hoa?
Kết quả:"tím"
► viper_color='xanh dương'
► flower_color='đỏ'
► color='tím'► viper_patch=
► flower_patch=
Truy vấn: Hãy kể cho tôi về cuộc cạnh tranh giữa hai tòa nhà chọc trời trong hình.
► skyscraper_1_name='tòa nhà chrysler'
► skyscraper_2_name='tòa nhà empire state'
skyscraper_patches =
def execute_command(image):
    image_patch = ImagePatch(image)
    viper_patches = image_patch.find("viper")
    flower_patches = image_patch.find("flower")
    viper_patch = viper_patches[0]
    flower_patch = flower_patches[0]
    viper_color = viper_patch.simple_query("Con rắn độc có màu gì?")
    flower_color = flower_patch.simple_query("Bông hoa có màu gì?")
    color = llm_query(f"Bạn nhận được màu gì nếu kết hợp màu 
      {viper_color} và {flower_color}?")
    return color
def execute_command(image):
    image_patch = ImagePatch(image)
    skyscraper_patches = image_patch.find("skyscraper")
    skyscraper_patch_1 = skyscraper_patches[0]
    skyscraper_patch_2 = skyscraper_patches[1]
    skyscraper_1_name = skyscraper_patch_1.simple_query("Tên của tòa nhà chọc trời này là gì?")
    skyscraper_2_name = skyscraper_patch_2.simple_query("Tên của tòa nhà chọc trời này là gì?")
    return llm_query(f"Hãy kể cho tôi về cuộc cạnh tranh giữa {skyscraper_1_name} và {skyscraper_2_name}")Truy vấn: Mỗi đứa trẻ có thể có bao nhiêu chiếc bánh muffin để công bằng? Thực thi
Kết quả:4
► len(muffin_patches)=8
► len(kid_patches)=2
► 8//2 = 4
kid_patches = 
image_patch.find("kid")
muffin_patches = 
image_patch.find("muffin")
def execute_command(image):
    image_patch = ImagePatch(image)
    muffin_patches = image_patch.find("muffin")
    kid_patches = image_patch.find("kid")
    return str(len(muffin_patches) // len(kid_patches))
Truy vấn: Người sáng lập thương hiệu của chiếc xe bên trái sẽ nói gì với người sáng lập thương hiệu của chiếc xe bên phải?
Kết quả:"Ferruccio Lamborghini có thể sẽ nói, 'Thật vinh dự khi được làm đối thủ của bạn trong bao nhiêu năm, Enzo. 
Mong rằng những chiếc xe của chúng ta sẽ tiếp tục thúc đẩy nhau trở nên tốt hơn và nhanh hơn!'"
► left_car_founder='Ferruccio Lamborghini'
► right_car_founder='Enzo Ferrari'
car_patches = 
image_patch.find("car")
,
► left_car_brand='lamborghini'
► right_car_brand='ferrari'
car_patches.sort(...)
,
def execute_command(image):
     image_patch = ImagePatch(image)
    car_patches = image_patch.find("car")
    car_patches.sort(key=lambda car: car.horizontal_center)
    left_car = car_patches[0]
    right_car = car_patches[-1]
    left_car_brand = left_car.simple_query("Thương hiệu của chiếc xe này là gì?")
    right_car_brand = right_car.simple_query("Thương hiệu của chiếc xe này là gì?")
    left_car_founder = llm_query(f"Ai là người sáng lập {left_car_brand}?")
    right_car_founder = llm_query(f"Ai là người sáng lập {right_car_brand}?")
    return llm_query(f"{left_car_founder} sẽ nói gì với {right_car_founder}?")
Kết quả:"Cuộc cạnh tranh giữa Tòa nhà Chrysler và Tòa nhà Empire State là một trong những sự cạnh tranh nổi tiếng nhất trong 
lịch sử kiến trúc. Được xây dựng vào những năm 1930 tại Thành phố New York, hai tòa nhà chọc trời này từng là những tòa nhà cao nhất thế giới. Tòa nhà
Chrysler, được thiết kế bởi William Van Alen và hoàn thành vào năm 1930, là tòa nhà đầu tiên đạt độ cao 1.046 feet, tạm thời..."
Truy vấn: Trả về hai đứa trẻ ở xa người phụ nữ nhất ngay trước khi cô ấy ôm cô gái
def execute_command(video):
    video_segment = VideoSegment(video)
    hug_detected = False
    for i, frame in enumerate(video_segment.frame_iterator()):
        if frame.exists("woman") and frame.exists("girl") and \
                frame.simple_query("Người phụ nữ có đang ôm cô gái không?") == "yes":
            hug_detected = True
            break
    if hug_detected:
        index_frame = i - 1
    frame_of_interest = ImagePatch(video_segment, index_frame)
    woman_patches = frame_of_interest.find("woman")
    woman_patch = woman_patches[0]
    kid_patches = frame_of_interest.find("kid")
    kid_patches.sort(key=lambda kid: distance(kid, woman_patch))
    kid_patch_1 = kid_patches[-1]
    kid_patch_2 = kid_patches[-2]
    return [kid_patch_1, kid_patch_2]
► hug_detected=True
► frame=
► frame_of_interest=► kid_patches=
Kết quả:► kid_patches=sort(...distance...)
Mã được sinh
Truy vấn: Đồ uống không có cồn
► drink_name = 'gin'
► alcoholic = 'yes'
► drink_name = 'tullamore dew'
► alcoholic = 'yes'
► drink_name = 'bacardi'
► alcoholic = 'yes'
► drink_name = 'dr pepper'
► alcoholic = 'no'
► drink_patches=
Kết quả:
def execute_command(image):
    image_patch = ImagePatch(image)
    drink_patches = image_patch.find("drink")
    for drink_patch in drink_patches:
        drink_name = drink_patch.simple_query("Đây là gì?")
        alcoholic = llm_query(f"{drink_name} có chứa cồn không?")
        if alcoholic == "no":
            return drink_patch
    return NoneFigure 1. Kết quả ngoài thực tế. Cho một đầu vào thị giác và một truy vấn, ViperGPT tổng hợp một chương trình, sau đó thực thi nó với trình thông dịch Python
để tạo ra câu trả lời cuối cùng. Hình này hiển thị cả mã được tạo ra và kết quả của các biến trung gian trong
quá trình thực thi. Bằng cách kết hợp các module được huấn luyện trước, ViperGPT có được những câu trả lời vừa chính xác vừa có thể diễn giải cho các truy vấn thế giới mở.

--- TRANG 3 ---
với các giá trị trung gian có thể được kiểm tra; 2) logic, vì
nó sử dụng một cách rõ ràng các toán tử logic và toán học
tích hợp của Python; 3) linh hoạt, vì nó có thể dễ dàng kết hợp bất kỳ
module thị giác hoặc ngôn ngữ nào, chỉ yêu cầu đặc tả
của module liên quan được thêm vào API; 4) kết hợp,
phân tách các nhiệm vụ thành các nhiệm vụ con nhỏ hơn được thực hiện từng bước;
5) thích ứng với những tiến bộ trong lĩnh vực, vì những cải tiến
trong bất kỳ module nào được sử dụng sẽ dẫn đến cải thiện trực tiếp
trong hiệu suất của phương pháp chúng tôi; 6) không cần huấn luyện,
vì nó không yêu cầu huấn luyện lại (hoặc tinh chỉnh) một mô hình mới
cho mỗi nhiệm vụ mới; và cuối cùng, 7) tổng quát, vì nó thống nhất tất cả
các nhiệm vụ thành một hệ thống.

Tóm tắt, những đóng góp của chúng tôi là:
1. Chúng tôi đề xuất một framework đơn giản để giải quyết các truy vấn
thị giác phức tạp bằng cách tích hợp các mô hình sinh mã
vào thị giác với một API và trình thông dịch Python, với
những lợi ích như trên.
2. Chúng tôi đạt được kết quả zero-shot state-of-the-art trên
các nhiệm vụ định vị thị giác, trả lời câu hỏi hình ảnh,
và trả lời câu hỏi video, cho thấy khả năng diễn giải này
hỗ trợ hiệu suất thay vì cản trở nó.
3. Để thúc đẩy nghiên cứu theo hướng này, chúng tôi phát triển một
thư viện Python cho phép phát triển nhanh chóng cho
tổng hợp chương trình cho các nhiệm vụ thị giác, sẽ được mã nguồn mở
khi xuất bản.

2. Công trình liên quan
Thị giác Modular. Công trình của chúng tôi lấy cảm hứng từ Neural
Module Networks [2, 27], những người lập luận rằng các nhiệm vụ thị giác phức tạp
về cơ bản là kết hợp và đề xuất chia chúng thành các đơn vị
perceptual nguyên tử. Quy trình lý luận thị giác này đã được khám phá
bởi nhiều công trình [29, 57]. Những nỗ lực sau đó đã tập trung vào
lý luận một cách rõ ràng về sự kết hợp bằng cách tách biệt lý luận
khỏi perception, với các kết nối đến các phương pháp neuro-symbolic
[19, 27, 62]. Những phương pháp này tương tự về tinh thần với phương pháp của chúng tôi,
nhưng yêu cầu sự giám sát đắt đỏ dưới dạng chương trình
và huấn luyện end-to-end các module perception, điều này khiến chúng
không thể tổng quát hóa sang các domain khác.

Do khó khăn thực tế khi sử dụng các phương pháp này,
lĩnh vực này chủ yếu đã chuyển sang các mô hình end-to-end all-in-one
[1, 22, 23, 30]. Các mô hình như vậy hiện tại đạt được kết quả
state-of-the-art, và chúng tôi so sánh với chúng trong Mục 4.
Các công trình gần đây khác [63, 45, 55, 35, 37, 15] cho thấy các mô hình
được huấn luyện trước lớn có thể được sử dụng cùng nhau để đạt hiệu quả lớn,
nhưng chỉ định thủ công cách cụ thể các mô hình được kết hợp.

Trong quá trình thực hiện dự án này, một làn sóng quan tâm trong
lĩnh vực này đã dẫn đến một số bản thảo liên quan xuất hiện
trên arXiv sử dụng các mô hình ngôn ngữ lớn (LLM)
để tích hợp module tự động. Trong lĩnh vực xử lý ngôn ngữ tự nhiên,
chúng được nhắm đến sử dụng các công cụ bên ngoài [46, 40], hoặc cho
lý luận có cấu trúc sử dụng Codex [34, 54, 14, 10]. Công trình đồng thời [17] sinh một danh sách

Mã được sinh
Thực thi mã
Kết quả: "Shiba Inu" Đặc tả API
    """
def find(image, object_name) -> List[torch.Tensor]:
    """Trả về object_name trong image"""
def compute_depth(image) -> torch.Tensor:
    """Trả về độ sâu ước tính"""
def exists(image, object_name) -> bool:
    """Trả về True nếu object_name có trong image"""
def llm_query(text) -> text:
    """Trả về sự tương tự giữa text và image"""
def verify_property(image, object_name, property) -> bool:
    """Trả về True nếu object có property""" def process_query_function(image):
    image_patch = ImagePatch(image)
    pets = image_patch.find("pet")
    pets_sorted = ...
    ...
    return result
Trình thông dịch Python
+
Triển khai API"Thú cưng nào ở 
góc trên bên trái?"
Code LLM
Truy vấn Đầu vào thị giác
ViperGPTFigure 2. Phương pháp. ViperGPT là một framework để giải quyết các truy vấn
thị giác phức tạp một cách có chương trình.

các hướng dẫn pseudocode và diễn giải chúng như một 'chương trình
thị giác,' dựa vào học trong bối cảnh từ các ví dụ được cung cấp.
Không giống như chúng, chúng tôi trực tiếp sinh mã Python không giới hạn,
linh hoạt hơn nhiều và cho phép chúng tôi thể hiện các khả năng
emergent tiên tiến hơn, như control flow và toán học. Quan trọng là,
việc sử dụng Python cho phép chúng tôi tận dụng kiến thức tiên nghiệm mạnh mẽ
mà Codex học được bằng cách huấn luyện quy mô từ Internet. Ngoài ra,
chúng tôi đánh giá trên nhiều benchmark đã được thiết lập đo lường
hiểu biết thị giác và đạt được kết quả zero-shot hàng đầu.

Khả năng diễn giải. Lĩnh vực khả năng diễn giải cho các truy vấn
phức tạp trong thị giác rất rộng lớn. Nhiều phương pháp cung cấp
giải thích dưới dạng tầm quan trọng pixel, giống như Grad-CAM
[47, 65, 11, 41], một số cũng cung cấp giải thích bằng văn bản [41].
Những điều này thường là giải thích hậu nghiệm thay vì theo
thiết kế, và không đưa ra lý luận từng bước bao gồm
cắt hình ảnh và văn bản. Hard attention trong captioning [59]
nhằm mục tiêu tương tự liên quan đến cắt hình ảnh trung gian,
tương tự như module find của chúng tôi, nhưng đã tỏ ra khó khăn
để kết hợp vào các thuật toán học. Xem He et al. [18]
để có tổng quan đầy đủ.

Các mô hình được huấn luyện trước. Các module perception và kiến thức
bên ngoài được sử dụng bởi ViperGPT là GLIP [31] cho phát hiện đối tượng,
X-VLM [64] cho độ tương tự văn bản-hình ảnh (vì nó vượt qua
CLIP [43] trong phát hiện thuộc tính [5]), MiDaS [44] cho ước tính
độ sâu, GPT-3 [6] cho kiến thức bên ngoài, và BLIP-2 [30]
cho các truy vấn thị giác đơn giản.

3. Phương pháp
Chúng tôi sử dụng ký hiệu theo Johnson et al. [27]. Cho một
đầu vào thị giác x và một truy vấn văn bản q về nội dung của nó, trước tiên chúng tôi
tổng hợp một chương trình z=Π(q) với một bộ sinh chương trình
Π cho truy vấn. Sau đó chúng tôi áp dụng engine thực thi
r=E(x;z) để thực thi chương trình z trên đầu vào x và tạo ra

--- TRANG 4 ---
pizza.compute_depth()
0.30.7def execute_command(image):    
    image_patch = ImagePatch(image)
    pizza_patches = image_patch.find("pizza")
    pizza_patches.sort(key=lambda pizza: pizza.compute_depth())
    patch_return = pizza_patches[0]
    return patch_returnTruy vấn: pizza phía trước
pizza_patches = image_patch.find("pizza")
patch_return = pizza_patches[0]
return patch_return
pizza_patches.sort()
,
Kết quả:Mã được sinh
Thực thi
► pizza_patches = {List[ImagePatch]}Đầu vào:
,Figure 3. Định vị thị giác trên RefCOCO.

kết quả r. Framework của chúng tôi linh hoạt, hỗ trợ hình ảnh
hoặc video làm đầu vào x, câu hỏi hoặc mô tả làm truy vấn q,
và bất kỳ loại nào (ví dụ, văn bản hoặc cắt hình ảnh) làm đầu ra r.

Trong khi công trình trước đây biểu diễn các chương trình như đồ thị,
như cây cú pháp [27] hoặc đồ thị phụ thuộc [8], chúng tôi biểu diễn
lớp chương trình z∈Z trực tiếp thông qua mã Python,
cho phép các chương trình của chúng tôi tận dụng tính biểu cảm và
khả năng được cung cấp bởi các ngôn ngữ lập trình hiện đại.

3.1. Sinh chương trình
Johnson et al. [27] và công trình khác theo hướng này [19,
62, 25] thường triển khai với một mạng neural được huấn luyện
với học có giám sát hoặc học tăng cường để ước tính
các chương trình từ truy vấn. Tuy nhiên, những phương pháp này phần lớn
không thể mở rộng đến các môi trường in-the-wild vì a) sự giám sát
dưới dạng chương trình không thể được thu thập ở quy mô
hoặc b) việc tối ưu hóa cần thiết để tìm đồ thị tính toán
là cấm đoán.

Trong phương pháp của chúng tôi, thay vào đó chúng tôi tận dụng LLM cho
sinh mã để khởi tạo bộ sinh chương trình Π kết hợp
các module thị giác và ngôn ngữ lại với nhau. LLM nhận
đầu vào là một chuỗi mã được tokenize ("prompt") và dự đoán
các token tiếp theo một cách autoregressive. Chúng tôi sử dụng Codex [9],
đã cho thấy thành công đáng chú ý trong các nhiệm vụ sinh mã.
Vì chúng tôi thay thế việc tối ưu hóa Π bằng một LLM, phương pháp
của chúng tôi loại bỏ nhu cầu huấn luyện cụ thể cho nhiệm vụ để sinh chương trình.
Sử dụng Codex làm bộ sinh chương trình và sinh mã trực tiếp trong
Python cho phép chúng tôi tận dụng việc huấn luyện quy mô trên Internet,
nơi mã Python rất phong phú.

Để tận dụng LLM theo cách này, chúng ta cần định nghĩa một
prompt sẽ lấy mẫu các chương trình z kết hợp và gọi

Bảng 1. Kết quả RefCOCO. Chúng tôi báo cáo độ chính xác trên nhiệm vụ REC
và phân chia testA. ZS=zero shot, Sup.=có giám sát.
IoU (%) "
RefCOCO RefCOCO+Sup.MDETR [53] 90.4 85.5
OFA [53] 94.0 91.7ZSOWL-ViT [38] 30.3 29.4
GLIP [31] 55.0 52.2
ReCLIP [49] 58.6 60.5
ViperGPT (của chúng tôi) 72.0 67.0

các module này khi cần thiết. Prompt của chúng tôi bao gồm một giao diện
lập trình ứng dụng (API), được mô tả chi tiết trong phần
tiếp theo, mà chúng tôi cung cấp cho LLM như một phần của bối cảnh
đầu vào. Đầu vào cuối cùng cho LLM là một chuỗi văn bản mã
bao gồm đặc tả API theo sau bởi truy vấn cho mẫu đang
xem xét. Đầu ra mong đợi là một định nghĩa hàm Python dưới dạng
chuỗi, sau đó chúng tôi biên dịch và thực thi.

3.2. Module và API của chúng
Prompt của chúng tôi, được bao gồm trong Phụ lục B, cung cấp
API cho các module perceptual và kiến thức khác nhau, như
cho phát hiện đối tượng, ước tính độ sâu, hoặc truy vấn mô hình ngôn ngữ.
Từ prompt này, chúng tôi thấy rằng LLM có thể cảm ứng
các chương trình z chính xác từ truy vấn q.

API mà chúng tôi cung cấp định nghĩa hai lớp toàn cục
ImagePatch và VideoSegment, đại diện cho một patch hình ảnh
và một đoạn video tương ứng. Mỗi module được triển khai
như một phương thức lớp, gọi nội tại một mô hình được huấn luyện trước
để tính toán kết quả. Ví dụ, phương thức compute_depth của
ImagePatch trả về ước tính độ sâu trung vị (tương đối) của
các pixel trong patch hình ảnh; chúng tôi triển khai điều này với
các mô hình quy mô lớn state-of-the-art như MiDaS [44].
Chúng tôi cung cấp thêm chi tiết về các module được sử dụng trong Mục 4.

API chỉ định các loại đầu vào và đầu ra cho mỗi phương thức
mà nó định nghĩa, cũng như docstrings để giải thích mục đích
của những hàm này bằng ngôn ngữ tự nhiên. Giống như hầu hết các API,
nó cũng cung cấp các ví dụ cho thấy cách sử dụng các lớp
và hàm này, được chỉ định dưới dạng các cặp truy vấn-mã
tương tự như học trong bối cảnh [50, 6].

Đầu vào cho Codex không chứa triển khai đầy đủ của API.
Thay vào đó, nó được cung cấp đặc tả cho API, bao gồm
các chữ ký hàm và docstrings. Việc trừu tượng hóa các chi tiết
triển khai có lợi vì hai lý do. Đầu tiên, cửa sổ bối cảnh LLM
bị hạn chế về kích thước [6], khiến việc bao gồm toàn bộ
triển khai trở nên không khả thi. Ngoài ra, sự trừu tượng
làm cho việc sinh mã độc lập với các thay đổi được thực hiện
đối với triển khai module.

Các module perception end-to-end rất xuất sắc khi được sử dụng
đúng chỗ, và ViperGPT dựa mạnh vào chúng.

--- TRANG 5 ---
Mã được sinh
Kết quả:"yes"
def execute_command(image):    
    image_patch = ImagePatch(image)
    bookcase_patches = image_patch.find("bookcase")
    for bookcase_patch in bookcase_patches:
        is_wood = bookcase_patch.verify_property("bookcase", "wood")
        if is_wood:
            water_bottle_patches = image_patch.find("water bottle")
            for water_bottle_patch in water_bottle_patches:
                if water_bottle_patch.horizontal_center > \
                   bookcase_patch.horizontal_center:
                    return "yes"
            return "no"
    return "no"Truy vấn: Có chai nước ở bên phải 
             tủ sách được làm bằng gỗ không?Thực thi
water_bottle_patches = image_patch.
      find("water bottle")
► water_bottle_patches[0] 
   = {ImagePatches}
► water_bottle_patches[0] .
     horizontal_center = {float} 608.5
► water_bottle_patch.horizontal_center > 
bookcase_patch.horizontal_center  = 
{bool} True
bookcase_patches= image_patch.
       find("bookcase")
► bookcase_patches[0] = {ImagePatch}
► bookcase_patches[0] .
 h o r i z o n t a l _ c e n t e r  =  { f l o a t }  239.0
...verify_property("bookcase","wood")
► is_wood = {bool} True
Đầu vào:
def execute_command(image):    
    image_patch = ImagePatch(image)
    pancake_patches = image_patch.find("pancake")
    is_brown = pancake_patches[0].verify_property("pancake", "brown")
    is_round = pancake_patches[0].verify_property("pancake", "round")
    return bool_to_yesno(is_brown and is_round)Truy vấn : Chiếc bánh pancake đó có vẻ màu nâu              và tròn không?...verify_property("pancake", "brown")
► is_brown = {bool} True
...verify_property("pancake", round)
► is_round = {bool} True
► is_brown and is_round = {bool} True
Kết quả: "yes"Mã được sinh
pancake_patches = image_patch.
 f i n d ( " p a n c a k e " )
► pancake_patches[0] = {ImagePatch}Thực thi
Đầu vào:
Figure 4. Trả lời câu hỏi hình ảnh kết hợp trên GQA.

Tương tự như các mô hình hệ thống kép [28] trong khoa học nhận thức,
chúng tôi lập luận rằng các chương trình được sinh (Hệ thống 2 - phân tích)
nên được sử dụng để chia nhỏ các nhiệm vụ yêu cầu nhiều
bước lý luận thành các thành phần đơn giản hơn, nơi các module
perception end-to-end (Hệ thống 1 - nhận dạng mẫu)
là phương pháp hiệu quả nhất. Bằng cách kết hợp các module end-to-end
thành chương trình, ViperGPT mang khả năng Hệ thống 2
của xử lý tuần tự đến deep learning [4].

3.3. Thực thi chương trình
Tại thời điểm thực thi, chương trình được sinh z nhận một
hình ảnh hoặc video làm đầu vào và xuất ra kết quả r tương ứng
với truy vấn được cung cấp cho LLM. Để thực thi chương trình này,
công trình trước đây (ví dụ, [27]) học một engine thực thi như một
mạng module neural, kết hợp các module khác nhau được triển khai
bởi các mạng neural. Các module của họ chịu trách nhiệm không chỉ
cho các hàm perceptual như find, mà còn cho các hàm logic
như compare. Họ học tất cả các module neural cùng nhau
đồng thời end-to-end, điều này thất bại trong việc cho phép
tổng quát hóa có hệ thống [3] và dẫn đến các module không
trung thực với các nhiệm vụ dự định của chúng [48], làm tổn hại
khả năng diễn giải của mô hình.

Chúng tôi cung cấp một giải pháp thay thế đơn giản, hiệu quả bằng cách sử dụng
trình thông dịch Python kết hợp với các module được triển khai
bởi các mô hình được huấn luyện trước lớn. Trình thông dịch Python
cho phép các phép toán logic trong khi các mô hình được huấn luyện trước
cho phép các phép toán perceptual. Phương pháp của chúng tôi đảm bảo
tính trung thực theo thiết kế.

Chương trình được chạy với trình thông dịch Python; như vậy,
việc thực thi của nó là một lời gọi Python đơn giản. Điều này có nghĩa nó có thể
tận dụng tất cả các hàm Python tích hợp như sort; các công cụ control flow
như for hoặc if/else; và các module như datetime
hoặc math. Đáng chú ý, điều này không yêu cầu một trình thông dịch tùy chỉnh,
không giống như các phương pháp trước đây [17, 46]. Một lợi thế khác của

Bảng 2. Kết quả GQA. Chúng tôi báo cáo độ chính xác trên tập test-dev.
Độ chính xác (%) "Sup.LGCN [20] 55.8
LXMERT [51] 60.0
NSM [24] 63.0
CRF [39] 72.1ZSBLIP-2 [30] 44.7
ViperGPT (của chúng tôi) 48.1

việc triển khai hoàn toàn Pythonic là khả năng tương thích với một loạt
các công cụ hiện có, như PyTorch JIT [42].

Trong triển khai của chúng tôi, mỗi chương trình trong một batch
được sinh chạy đồng thời với multiprocessing.
Thiết kế producer-consumer [12] của chúng tôi cho phép batching GPU
hiệu quả, giảm chi phí bộ nhớ và tính toán. Mã của chúng tôi
được cung cấp tại viper.cs.columbia.edu/.

4. Đánh giá
ViperGPT có thể áp dụng cho bất kỳ nhiệm vụ nào truy vấn đầu vào thị giác
với văn bản. Không giống như công trình khác sử dụng các mô hình ngôn ngữ lớn
cho các nhiệm vụ thị giác, các giá trị trả về của các chương trình của chúng tôi có thể
có các loại tùy ý, như văn bản, lựa chọn nhiều lựa chọn,
hoặc các vùng hình ảnh. Chúng tôi chọn bốn cài đặt đánh giá khác nhau
để thể hiện khả năng đa dạng của mô hình trong các bối cảnh khác nhau
mà không cần huấn luyện thêm. Các nhiệm vụ mà chúng tôi xem xét
là: 1) định vị thị giác, 2) trả lời câu hỏi hình ảnh kết hợp,
3) trả lời câu hỏi hình ảnh phụ thuộc kiến thức bên ngoài, và
4) lý luận nhân quả và thời gian video. Chúng tôi xem xét các nhiệm vụ này
xây dựng lên nhau một cách sơ bộ, với định vị thị giác là
điều kiện tiên quyết cho trả lời câu hỏi hình ảnh kết hợp và tiếp tục.
Trong các phần sau, chúng tôi khám phá các khả năng mà ViperGPT thể hiện
để giải quyết từng nhiệm vụ.

--- TRANG 6 ---
def execute_command(image):
    image = ImagePatch(image)
    toy = image.simple_query("Đồ chơi này là gì?")
    result = llm_query("Phiên bản sống thật của 
                 {} làm gì vào mùa đông?", toy)
    return resultTruy vấn: Phiên bản sống thật của đồ chơi này 
             làm gì vào mùa đông?
Mã được sinh
► toy = {str} "gấu"
► guess = {str} "ngủ đông"  
Kết quả BLIP-2: "trượt tuyết"Kết quả: "ngủ đông"Thực thi
Đầu vào:Figure 5. Chuỗi suy nghĩ có chương trình với kiến thức bên ngoài
cho OK-VQA.

4.1. Định vị thị giác
Định vị thị giác là nhiệm vụ xác định hộp giới hạn trong hình ảnh
tương ứng tốt nhất với một truy vấn ngôn ngữ tự nhiên cho trước.
Các nhiệm vụ định vị thị giác đánh giá lý luận về các mối quan hệ
không gian và thuộc tính thị giác. Chúng tôi xem xét nhiệm vụ này
đầu tiên vì nó phục vụ như cầu nối đầu tiên giữa văn bản và thị giác:
nhiều nhiệm vụ yêu cầu định vị các truy vấn phức tạp vượt qua
việc định vị các đối tượng cụ thể.

Chúng tôi cung cấp cho ViperGPT API cho các module sau
(các mô hình được huấn luyện trước trong ngoặc đơn). find (GLIP
[31]) nhận đầu vào là một hình ảnh và một cụm danh từ ngắn
(ví dụ "xe hơi" hoặc "chó golden retriever"), và trả về một danh sách
các patch hình ảnh chứa cụm danh từ. exists (GLIP [31]) nhận
đầu vào là một hình ảnh và một cụm danh từ ngắn và trả về một
boolean cho biết liệu có một thể hiện của cụm danh từ đó
có mặt trong hình ảnh hay không. Tương tự, verify_property (X-
VLM [64]) nhận đầu vào là một hình ảnh, một cụm danh từ đại diện
cho một đối tượng, và một thuộc tính đại diện cho một tính chất của
đối tượng đó; nó trả về một boolean cho biết liệu tính chất đó có
hiện diện trong hình ảnh hay không. best_image_match (X-VLM [64]) nhận
đầu vào là một danh sách các patch hình ảnh và một cụm danh từ ngắn,
và trả về patch hình ảnh khớp tốt nhất với cụm danh từ.
Đối xứng với phép toán này, best_text_match nhận đầu vào
là một danh sách các cụm danh từ và một hình ảnh, và trả về
cụm danh từ khớp tốt nhất với hình ảnh. (Module này không
cần thiết cho định vị thị giác, mà cho các nhiệm vụ với đầu ra văn bản;
chúng tôi mô tả nó ở đây cho đơn giản.) Chúng được triển khai
sử dụng một mô hình độ tương tự hình ảnh-văn bản như trong CLIP [43].
Cuối cùng, compute_depth (MiDaS [44]) tính toán độ sâu trung vị
của patch hình ảnh. Chúng tôi cũng định nghĩa hàm distance,
tính toán khoảng cách pixel giữa hai patch, chỉ sử dụng
các công cụ Python tích hợp.

Để đánh giá, chúng tôi sử dụng các tập dữ liệu RefCOCO và RefCOCO+.
Tập đầu cho phép các quan hệ không gian trong khi tập sau không,
do đó cung cấp những hiểu biết khác nhau về khả năng của ViperGPT.
Chúng tôi so sánh ViperGPT với các phương pháp end-to-end,
và vượt trội hơn các phương pháp zero-shot khác trên cả hai tập dữ liệu
(xem Bảng 1). Chúng tôi hiển thị các ví dụ2 trong Hình 3.
Xem Phụ lục A để biết thêm chi tiết về thiết lập thí nghiệm.

2Các ví dụ trong bài báo đã được làm sạch về mặt thẩm mỹ bằng cách loại bỏ
các comment và xử lý lỗi, nhưng logic không thay đổi.

Bảng 3. Kết quả OK-VQA.
Độ chính xác (%) "Sup.TRiG [13] 50.5
KAT [16] 54.4
RA-VQA [32] 54.5
REVIVE [33] 58.0
PromptCap [21] 58.8ZSPNP-VQA [52] 35.9
PICa [60] 43.3
BLIP-2 [30] 45.9
Flamingo [1] 50.6
ViperGPT (của chúng tôi) 51.9

4.2. Trả lời câu hỏi hình ảnh kết hợp
Chúng tôi cũng đánh giá ViperGPT trên trả lời câu hỏi hình ảnh.
Chúng tôi tập trung vào trả lời câu hỏi kết hợp, yêu cầu
phân tách các câu hỏi phức tạp thành các nhiệm vụ đơn giản hơn.
Chúng tôi sử dụng tập dữ liệu GQA [26], được tạo ra để đo lường
hiệu suất trên các câu hỏi kết hợp phức tạp. Xem Hình 4
cho các câu hỏi ví dụ cũng như lý luận mà chúng tôi cung cấp.
Ngay cả khi một câu hỏi có thể được trả lời end-to-end, việc cung cấp
lý luận trung gian vừa có thể diễn giải hơn vừa phù hợp với con người hơn
thay vì yêu cầu mô hình nén tất cả các bước vào một forward pass;
vì kết quả cuối cùng của chúng tôi được xây dựng trực tiếp từ các giá trị
trung gian, chúng cung cấp một diễn giải hoàn toàn trung thực về cách
mô hình đi đến câu trả lời của mình.

Đối với GQA, chúng tôi kết hợp module simple_query
(BLIP-2 [31]), xử lý các truy vấn cơ bản không thể phân tách thêm,
như "Động vật này là gì?" Chúng tôi cũng thêm best_text_match
đã đề cập trước đây. Điều này dẫn chúng tôi đến độ chính xác tốt nhất
trên GQA trong số các mô hình zero-shot (Bảng 4).

4.3. Trả lời câu hỏi hình ảnh phụ thuộc kiến thức bên ngoài
Nhiều câu hỏi về hình ảnh chỉ có thể được trả lời chính xác
bằng cách tích hợp kiến thức bên ngoài về thế giới. Bằng cách
trang bị cho ViperGPT một module để truy vấn các cơ sở kiến thức
bên ngoài bằng ngôn ngữ tự nhiên, nó có thể kết hợp kiến thức
với lý luận thị giác để xử lý những câu hỏi như vậy. Chúng tôi thêm
một module mới llm_query (GPT-3 [6]), khai thác các mô hình văn bản
như các cơ sở kiến thức không có cấu trúc. Chúng tôi thấy rằng
sự kết hợp của lý luận từng bước từ Codex cùng với kiến thức
bên ngoài được truy vấn từ mô hình văn bản của GPT-3 đạt được
hiệu suất ấn tượng trong môi trường này.

Chúng tôi đánh giá trên tập dữ liệu OK-VQA [36], được thiết kế
để đánh giá khả năng của các mô hình trả lời câu hỏi về hình ảnh
yêu cầu kiến thức không thể tìm thấy trong hình ảnh. Các mục
trong tập dữ liệu này thường yêu cầu nhiều hơn một bước lý luận
để tạo ra câu trả lời chính xác. Ví dụ, trong Hình 5, người ta phải
trước tiên nhận thức từ hình ảnh rằng

--- TRANG 7 ---
Mã được sinh
def execute_command(video, question, possible_answers):    
    video_segment = VideoSegment(video)
    drop_detected = False
    for i, frame in enumerate(video_segment.frame_iterator()):
        if frame.exists("boy") and frame.exists("sparkles") and \
               frame.simple_query("cậu bé có đang thả tia lửa không?") == "yes":
            drop_detected = True
            break
    if drop_detected:
        index_frame = i + 1
    else:
        index_frame = video_segment.num_frames // 2
    frame_of_interest = ImagePatch(video_segment, index_frame)
    boys = frame_of_interest.find("boy")
    if len(boys) == 0:
        boys = [frame_of_interest]
    boy = boys[0]
    caption = boy.simple_query("Đây là gì?")
    info = {
        "Caption của frame sau khi thả tia lửa": caption,
    }
    answer = select_answer(info, question, possible_answers)
    return answerTruy vấn:  Cậu bé đã làm gì sau khi thả 
             tia lửa xuống sàn?
► i= {int} 25► frame = {ImagePatch}frame.exists("boy") and \ 
frame.exists("sparkles") and \
frame.simple_query("cậu bé có đang 
   thả tia lửa không?") == "yes":
►  answer = {str} "nhặt lên"► frame_of_interest = {ImagePatch}
► boy = {ImagePatch}
►  caption = {str} "một đứa trẻ chạy 
            với lửa trong tay"index_frame = i + 1
► index_frame = {int} 26
boys = frame_of_interest.find("boy")
Đầu vào:Thực thi
Kết quả: "Nhặt lên"
def execute_command(video, question, possible_answers):  
    video_segment = VideoSegment(video)
    last_frame = ImagePatch(video_segment, -1)
    last_caption = last_frame.simple_query("Đây là gì?")
    dogs = last_frame.find("dog")
    if len(dogs) == 0:
        dogs = [last_frame]
    dog = dogs[0]
    dog_action = dog.simple_query("Con chó đang làm gì?")
    info = {
        "Caption của frame cuối": last_caption,
        "Con chó có vẻ như đang": dog_action
    }
    answer = select_answer(info, question, possible_answers)
    return answerTruy vấn : Con chó đen đặt mình như thế nào 
             ở cuối?
► last_frame = {ImagePatch}last_frame = 
    ImagePatch(video_segment, -1)
►  answer = {str} "ngồi trên đất"► dog = {ImagePatch}
►  last_caption = {str} "một con chó đen 
      ngồi trên cỏ"
dogs = last_frame.find("dog")
dog_action = dog.simple_query(
    "Con chó đang làm gì?")
► dog_action= {str}  "ngồi"Mã được sinhThực thi
Đầu vào:
Kết quả: "Ngồi trên đất"Figure 6. Lý luận thời gian trên NeXT-QA.

"đồ chơi này" là "gấu," sau đó sử dụng kiến thức bên ngoài để trả lời
gấu làm gì vào mùa đông. Các mô hình end-to-end phải trực tiếp
tạo ra câu trả lời, và do đó có thể chọn những từ liên quan trực tiếp
đến hình ảnh hơn là ý định của câu hỏi. Trong trường hợp này,
mô hình end-to-end tốt nhất có sẵn đoán "trượt tuyết," có lẽ vì
đó là một hoạt động mùa đông phổ biến (tuy nhiên, không phải cho gấu).
Mặt khác, ViperGPT có thể sử dụng một dạng lý luận chuỗi suy nghĩ [56]
để chia nhỏ câu hỏi như đã mô tả trước đây, đầu tiên xác định
loại đồ chơi sử dụng các module perception và sau đó sử dụng
thông tin được nhận thức kết hợp với một module kiến thức bên ngoài
để tạo ra phản hồi chính xác.

ViperGPT vượt trội hơn tất cả các phương pháp zero-shot, và khi
so sánh với các mô hình sử dụng tài nguyên công khai, nó vượt qua
mô hình tốt nhất trước đây 6%, một khoảng cách lớn cho tập dữ liệu này
(xem Bảng 3).

4.4. Lý luận nhân quả/thời gian video
Chúng tôi cũng đánh giá cách ViperGPT mở rộng sang video và
các truy vấn yêu cầu lý luận nhân quả và thời gian. Để khám phá
điều này, chúng tôi sử dụng tập dữ liệu NExT-QA, được thiết kế để đánh giá
khả năng thực hiện loại lý luận này của các mô hình video.

Bảng 4. Kết quả NExT-QA. Phương pháp của chúng tôi đạt kết quả state-of-the-
art tổng thể (bao gồm cả các mô hình có giám sát) trên phần chia khó. "T" và
"C" đại diện cho câu hỏi "thời gian" và "nhân quả" tương ứng.
Độ chính xác (%) "
Phần chia khó - T Phần chia khó - C Tập đầy đủSup.ATP [7] 45.3 43.3 54.3
VGT [58] - - 56.9
HiTeA [61] 48.6 47.8 63.1ZSViperGPT (của chúng tôi) 49.8 56.4 60.0

Chúng tôi đánh giá sử dụng phiên bản lựa chọn nhiều đáp án của NExT-QA.
Chúng tôi cung cấp một module bổ sung select_answer
(GPT-3 [6]), với thông tin văn bản về một cảnh và danh sách
các câu trả lời có thể, trả về câu trả lời phù hợp nhất với thông tin.
Ngoài ra, nội dung bổ sung duy nhất được cung cấp trong API là
định nghĩa của lớp VideoSegment, chứa bytestream video cũng như
timestamp bắt đầu và kết thúc của đoạn video mà nó đại diện.
Nó cũng định nghĩa một iterator trên các frame, trả về một đối tượng
ImagePatch đại diện cho mỗi frame.

Chúng tôi thấy rằng mặc dù chỉ được cung cấp các module perception
cho hình ảnh, ViperGPT thể hiện lý luận nhân quả và

--- TRANG 8 ---
Figure 7. Can thiệp.
Chúng tôi phân tích tầm quan trọng
của các module thị giác và hàm Python
khác nhau trong các chương trình
được sinh như được đo bằng
sự giảm mIoU khi chúng được
làm cho không hoạt động.
find
exists
verify_property
best_image_match
compute_depth
distance
sort
>, <
+, -, *, /
-70-52.5-35 -17.5 0Giảm tương đối trong mIoU(%)

thời gian emergent khi được áp dụng cho video được cung cấp như
một danh sách có thứ tự các hình ảnh. Đặc biệt, chúng tôi quan sát nó sinh
các chương trình áp dụng perception để xác định frame nào liên quan
cho một truy vấn cho trước, sau đó lý luận về thông tin được trích xuất
từ các frame này cùng với số frame liên kết để tạo ra
câu trả lời cuối cùng.

Mặc dù không nhìn thấy dữ liệu video nào, ViperGPT đạt được
kết quả độ chính xác ngang bằng với mô hình có giám sát tốt nhất
(xem Bảng 4), và thậm chí vượt qua nó trên phần chia khó của NeXT-QA [7],
cả cho các truy vấn thời gian và nhân quả. Tất nhiên, framework của
ViperGPT cũng cho phép kết hợp các mô hình video, mà chúng tôi
kỳ vọng sẽ cải thiện hiệu suất xa hơn ngưỡng này.

Khả năng tính toán thể hiện thậm chí nhiều trở ngại hơn cho
hiểu biết video so với hình ảnh. Việc đưa mỗi frame của một video
có kích thước vừa phải vào bộ nhớ GPU là không khả thi ngay cả
trên phần cứng tốt nhất. ViperGPT có thể cung cấp một con đường
tiến cho hiểu biết video vượt qua các hạn chế của các hệ thống
cần thực hiện tính toán trên toàn bộ video đồng thời. Xem các ví dụ
trong Hình 6.

5. Khám phá các khả năng mới
Trong phần này, chúng tôi thể hiện các khả năng thú vị khác nhau
được kích hoạt bởi việc sử dụng ViperGPT.

5.1. Các truy vấn ngoài benchmark
Chúng tôi tin rằng sức mạnh rõ ràng của phương pháp này có thể
không được khám phá đầy đủ bởi các benchmark hiện có, được thiết kế
cho các mô hình end-to-end. Trong Hình 1, chúng tôi hiển thị các ví dụ
về các truy vấn thú vị mà thú vị trong thế giới thực nhưng sẽ không
xuất hiện trong các benchmark hiện có. Chúng tôi không thêm bất kỳ
đặc tả API mới nào ngoài những cái đã được sử dụng trong các benchmark.
Xem Phụ lục B để biết thêm chi tiết.

Những ví dụ này cho thấy rằng các module mà chúng tôi bao gồm
là tổng quát và bao phủ một loạt rộng các nhiệm vụ. Trong các môi trường
mà khả năng mới được yêu cầu, framework là tổng quát và cho phép
thêm bất kỳ module nào, như ocr, surface_normal_estimation,
segmentation, v.v.

5.2. Khả năng giải thích can thiệp
Phương pháp có chương trình của chúng tôi cho phép chẩn đoán tự động
module nào chịu trách nhiệm cho lỗi dự đoán,

# Bối cảnh: bức ảnh được chụp tại Mỹ
def execute_command(image):
    cars = image.find("car")
    for car in cars:
        if car.horizontal_center > image.horizontal_center:
            return car
    return NoneTruy vấn: Trả về chiếc xe ở làn đường đúng
# Bối cảnh: bức ảnh được chụp tại Anh
def execute_command(image):
    cars = image.find("car")
    for car in cars:
        if car.horizontal_center < image.horizontal_center:
            return car
    return NoneKết quả:None
Kết quả:
Figure 8. Các chương trình theo bối cảnh. ViperGPT dễ dàng kết hợp
bối cảnh bổ sung vào logic của các chương trình được sinh.

có khả năng thông báo loại mô hình nào cần cải thiện và nơi
thu thập thêm dữ liệu. Việc đánh giá đầu ra trung gian của mỗi module
là không thực tế do thiếu nhãn ground truth, và việc so sánh
độ chính xác một cách ngây thơ giữa các chương trình sử dụng một module
nhất định và những chương trình không sử dụng có thể bị confound
ví dụ bởi độ khó của vấn đề. Thay vào đó, chúng ta có thể thực hiện
can thiệp để hiểu rõ hơn hiệu suất của một module. Đối với mỗi module,
chúng ta có thể định nghĩa một giá trị mặc định không cung cấp thông tin,
và thay thế mô hình cơ bản bằng đầu ra mặc định này. Ví dụ,
find có thể luôn trả về toàn bộ hình ảnh đầu vào. Sau đó chúng ta có thể
xem xét hiệu suất giảm bao nhiều nếu đánh giá cùng mã cho
các ví dụ sử dụng module đó. Nếu can thiệp có tác động tối thiểu
đến hiệu suất, module có thể không hữu ích.

Chúng tôi hiển thị một ví dụ về phân tích này trong Hình 7 cho định vị
thị giác trên RefCOCO, nơi chúng tôi quan sát mức độ quan trọng tương tự
cho các module perception và các phép toán Python. Cả hai đều được
tích hợp chặt chẽ trong phương pháp của chúng tôi.

5.3. Điều kiện hóa trên thông tin bổ sung
Chúng tôi thấy ViperGPT dễ dàng chấp nhận việc sinh chương trình
dựa trên kiến thức bổ sung. Bối cảnh này có thể được cung cấp
như một comment trước khi sinh mã. Bối cảnh như vậy có thể quan trọng
để phản hồi chính xác một loạt rộng các truy vấn. Trong Hình 8
chúng tôi hiển thị một ví dụ như vậy. Phía đường đúng thay đổi
theo quốc gia, vì vậy truy vấn ban đầu không thể được trả lời.
Được cung cấp bối cảnh về nơi ảnh được chụp, mô hình tạo ra
logic khác nhau cho mỗi trường hợp, được điều chỉnh dựa trên
kiến thức tiên nghiệm liên quan.

6. Kết luận
Chúng tôi trình bày ViperGPT, một framework cho việc kết hợp
có chương trình các hàm thị giác, ngôn ngữ, toán học và logic
chuyên biệt cho các truy vấn thị giác phức tạp. ViperGPT có khả năng
kết nối các tiến bộ riêng lẻ trong thị giác và ngôn ngữ; nó cho phép
chúng thể hiện khả năng vượt quá những gì bất kỳ mô hình riêng lẻ nào
có thể làm được. Khi các mô hình triển khai những hàm này tiếp tục
cải thiện, chúng tôi kỳ vọng kết quả của ViperGPT cũng sẽ tiếp tục
cải thiện song song.

--- TRANG 9 ---
Lời cảm ơn: Nghiên cứu này dựa trên công trình được hỗ trợ một phần
bởi chương trình DARPA MCS theo Thỏa thuận Liên bang
Số N660011924032 và Giải thưởng NSF CAREER #2046910.
DS được hỗ trợ bởi Microsoft PhD Fellowship và SM được
hỗ trợ bởi NSF GRFP.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-
sch, Katherine Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and
Kyunghyun Cho, editors, Advances in Neural Information
Processing Systems, 2022.

[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan
Klein. Neural module networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2016.

[3] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch,
Thien Huu Nguyen, Harm de Vries, and Aaron Courville.
Systematic Generalization: What Is Required and Can It Be
Learned?, Apr. 2019. arXiv:1811.12889 [cs].

[4] Yoshua Bengio. The Consciousness Prior, Dec. 2019.
arXiv:1709.08568 [cs, stat].

[5] Maria A. Bravo, Sudhanshu Mittal, Simon Ging, and
Thomas Brox. Open-vocabulary attribute detection. arXiv
preprint arXiv:2211.12914, 2022.

[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-
ford, Ilya Sutskever, and Dario Amodei. Language Models
are Few-Shot Learners. arXiv:2005.14165 [cs], July 2020.
arXiv: 2005.14165.

[7] Shyamal Buch, Cristóbal Eyzaguirre, Adrien Gaidon, Jiajun
Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the"
video" in video-language understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2917–2927, 2022.

[8] Qingxing Cao, Xiaodan Liang, Bailin Li, and Liang Lin. In-
terpretable Visual Question Answering by Reasoning on De-
pendency Trees. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 43(3):887–901, Mar. 2021.

[9] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-
rique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda,
Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish

Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ry-
der, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Moham-
mad Bavarian, Clemens Winter, Philippe Tillet, Felipe Pet-
roski Such, David W. Cummings, Matthias Plappert, Fotios
Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H.
Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shan-
tanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant
Misra, Evan Morikawa, Alec Radford, Matthew M. Knight,
Miles Brundage, Mira Murati, Katie Mayer, Peter Welin-
der, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. Evaluating large lan-
guage models trained on code. ArXiv, abs/2107.03374, 2021.

[10] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W.
Cohen. Program of thoughts prompting: Disentangling com-
putation from reasoning for numerical reasoning tasks. arXiv
preprint arXiv:2211.12588, 2022.

[11] Chaorui Deng, Qi Wu, Qingyao Wu, Fuyuan Hu, Fan Lyu,
and Mingkui Tan. Visual Grounding via Accumulated At-
tention.

[12] E.W. Dijkstra. Information streams sharing a finite buffer.
Information Processing Letters, 1(5):179–180, 1972.

[13] Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti,
Ying Nian Wu, and Prem Natarajan. Transform-retrieve-
generate: Natural language-centric outside-knowledge vi-
sual question answering. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 5067–5077, 2022.

[14] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei
Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
Pal: Program-aided language models. arXiv preprint
arXiv:2211.10435, 2022.

[15] Prajwal Gatti, Abhirama Subramanyam Penamakuri, Revant
Teotia, Anand Mishra, Shubhashis Sengupta, and Roshni
Ramnani. Cofar: Commonsense and factual reasoning in
image search. In Proceedings of the 2nd Conference of the
Asia-Pacific Chapter of the Association for Computational
Linguistics and the 12th International Joint Conference on
Natural Language Processing, pages 1185–1199, 2022.

[16] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander
Hauptmann, Yonatan Bisk, and Jianfeng Gao. KAT: A
knowledge augmented transformer for vision-and-language.
In Proceedings of the 2022 Conference of the North Ameri-
can Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 956–968, Seat-
tle, United States, July 2022. Association for Computational
Linguistics.

[17] Tanmay Gupta and Aniruddha Kembhavi. Visual pro-
gramming: Compositional visual reasoning without training.
arXiv preprint arXiv:2211.11559, 2022.

[18] Feijuan He, Yaxian Wang, Xianglin Miao, and Xia Sun. In-
terpretable visual reasoning: A survey. Image and Vision
Computing, 112:104194, 2021.

[19] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor
Darrell, and Kate Saenko. Learning to Reason: End-to-
End Module Networks for Visual Question Answering. 2017
IEEE International Conference on Computer Vision (ICCV),
pages 804–813, Oct. 2017. Conference Name: 2017 IEEE

--- TRANG 10 ---
International Conference on Computer Vision (ICCV) ISBN:
9781538610329 Place: Venice Publisher: IEEE.

[20] Ronghang Hu, Anna Rohrbach, Trevor Darrell, and Kate
Saenko. Language-conditioned graph networks for relational
reasoning. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 10294–10303, 2019.

[21] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A
Smith, and Jiebo Luo. Promptcap: Prompt-guided task-
aware image captioning. arXiv preprint arXiv:2211.09699,
2022.

[22] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei
Chang, Yizhou Sun, Cordelia Schmid, David A Ross, and
Alireza Fathi. Reveal: Retrieval-augmented visual-language
pre-training with multi-source multimodal knowledge mem-
ory. arXiv preprint arXiv:2212.05221, 2022.

[23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, et al. Language is not
all you need: Aligning perception with language models.
arXiv preprint arXiv:2302.14045, 2023.

[24] Drew Hudson and Christopher D Manning. Learning by ab-
straction: The neural state machine. Advances in Neural In-
formation Processing Systems, 32, 2019.

[25] Drew A. Hudson and Christopher D. Manning. Composi-
tional Attention Networks for Machine Reasoning. ArXiv,
2018.

[26] Drew A. Hudson and Christopher D. Manning. GQA: A New
Dataset for Real-World Visual Reasoning and Compositional
Question Answering, May 2019. arXiv:1902.09506 [cs].

[27] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Judy Hoffman, Li Fei-Fei, C. Lawrence Zitnick, and Ross
Girshick. Inferring and Executing Programs for Visual Rea-
soning. pages 2989–2998, 2017.

[28] Daniel Kahneman. Thinking, fast and slow. macmillan,
2011.

[29] Seung Wook Kim, Makarand Tapaswi, and Sanja Fidler. Vi-
sual reasoning by progressive module networks. In Interna-
tional Conference on Learning Representations, 2019.

[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping Language-Image Pre-training with
Frozen Image Encoders and Large Language Models, Jan.
2023. arXiv:2301.12597 [cs].

[31] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10965–10975, 2022.

[32] Weizhe Lin and Bill Byrne. Retrieval augmented visual ques-
tion answering with outside knowledge. In Proceedings of
the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 11238–11254, Abu Dhabi, United
Arab Emirates, Dec. 2022. Association for Computational
Linguistics.

[33] Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chen-
guang Zhu, and Lu Yuan. REVIVE: Regional visual rep-
resentation matters in knowledge-based visual question an-
swering. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,

and Kyunghyun Cho, editors, Advances in Neural Informa-
tion Processing Systems, 2022.

[34] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and
Graham Neubig. Language models of code are few-shot
commonsense learners. arXiv preprint arXiv:2210.07128,
2022.

[35] Chengzhi Mao, Revant Teotia, Amrutha Sundar, Sachit
Menon, Junfeng Yang, Xin Wang, and Carl Vondrick. Dou-
bly Right Object Recognition: A Why Prompt for Visual Ra-
tionales, Dec. 2022. arXiv:2212.06202 [cs].

[36] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. OK-VQA: A Visual Question Answer-
ing Benchmark Requiring External Knowledge. May 2019.

[37] Sachit Menon and Carl Vondrick. Visual Classification
via Description from Large Language Models, Dec. 2022.
arXiv:2210.07183 [cs].

[38] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim
Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh
Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran
Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil
Houlsby. Simple open-vocabulary object detection with vi-
sion transformers. arXiv preprint arXiv:2205.06230, 2022.

[39] Binh X Nguyen, Tuong Do, Huy Tran, Erman Tjiputra,
Quang D Tran, and Anh Nguyen. Coarse-to-fine reason-
ing for visual question answering. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4558–4566, 2022.

[40] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool aug-
mented language models. arXiv preprint arXiv:2205.12255,
2022.

[41] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata,
Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus
Rohrbach. Multimodal Explanations: Justifying Decisions
and Pointing to the Evidence. In 2018 IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 8779–
8788, Salt Lake City, UT, June 2018. IEEE.

[42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An
imperative style, high-performance deep learning library.
In Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019.

[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International conference on machine learning,
pages 8748–8763. PMLR, 2021.

[44] René Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 44(3), 2022.

[45] Revant Gangi Reddy, Xilin Rui, Manling Li, Xudong Lin,
Haoyang Wen, Jaemin Cho, Lifu Huang, Mohit Bansal,

--- TRANG 11 ---
Avirup Sil, Shih-Fu Chang, et al. Mumuqa: Multimedia
multi-hop news question answering via cross-media knowl-
edge extraction and grounding. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages
11200–11208, 2022.

[46] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-
cedda, and Thomas Scialom. Toolformer: Language mod-
els can teach themselves to use tools. arXiv preprint
arXiv:2302.04761, 2023.

[47] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-
tra. Grad-CAM: Visual Explanations from Deep Net-
works via Gradient-based Localization. International Jour-
nal of Computer Vision, 128(2):336–359, Feb. 2020. arXiv:
1610.02391.

[48] Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolf-
son, Sameer Singh, Jonathan Berant, and Matt Gardner. Ob-
taining Faithful Interpretations from Compositional Neural
Networks, Sept. 2020. arXiv:2005.00724 [cs].

[49] Sanjay Subramanian, Will Merrill, Trevor Darrell, Matt
Gardner, Sameer Singh, and Anna Rohrbach. Reclip: A
strong zero-shot baseline for referring expression compre-
hension. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics, Dublin, Ireland,
May 2022. Association for Computational Linguistics.

[50] Dídac Surís, Dave Epstein, Heng Ji, Shih-Fu Chang, and
Carl. Vondrick. Learning to learn words from visual scenes.
European Conference on Computer Vision (ECCV), 2020.

[51] Hao Tan and Mohit Bansal. Lxmert: Learning cross-
modality encoder representations from transformers. arXiv
preprint arXiv:1908.07490, 2019.

[52] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio
Savarese, and Steven C.H. Hoi. Plug-and-play VQA: Zero-
shot VQA by conjoining large pretrained models with zero
training. In Findings of the Association for Computational
Linguistics: EMNLP 2022, pages 951–967, Abu Dhabi,
United Arab Emirates, Dec. 2022. Association for Computa-
tional Linguistics.

[53] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. CoRR, abs/2202.03052, 2022.

[54] Xingyao Wang, Sha Li, and Heng Ji. Code4struct: Code gen-
eration for few-shot structured prediction from natural lan-
guage. arXiv preprint arXiv:2210.12810, 2022.

[55] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-
guang Zhu, Derek Hoiem, et al. Language models with im-
age descriptors are strong few-shot video-language learners.
2022.

[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny
Zhou. Chain of Thought Prompting Elicits Reasoning in
Large Language Models, Oct. 2022. arXiv:2201.11903 [cs].

[57] Spencer Whitehead, Hui Wu, Heng Ji, Rogerio Feris, and
Kate Saenko. Separating skills and concepts for novel vi-

sual question answering. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5632–5641, June 2021.

[58] Junbin Xiao, Pan Zhou, Tat-Seng Chua, and Shuicheng Yan.
Video graph transformer for video question answering. In
European Conference on Computer Vision, pages 39–58.
Springer, 2022.

[59] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard Zemel,
and Yoshua Bengio. Show, Attend and Tell: Neural Im-
age Caption Generation with Visual Attention, Apr. 2016.
arXiv:1502.03044 [cs].

[60] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu-
mao Lu, Zicheng Liu, and Lijuan Wang. An empirical study
of gpt-3 for few-shot knowledge-based vqa. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 36,
pages 3081–3089, 2022.

[61] Qinghao Ye, Guohai Xu, Ming Yan, Haiyang Xu, Qi
Qian, Ji Zhang, and Fei Huang. Hitea: Hierarchical
temporal-aware video-language pre-training. arXiv preprint
arXiv:2212.14546, 2022.

[62] Kexin Yi, Jiajun Wu, Chuang Gan, A. Torralba, Pushmeet
Kohli, and J. Tenenbaum. Neural-Symbolic VQA: Disentan-
gling Reasoning from Vision and Language Understanding.
ArXiv, 2018.

[63] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choro-
manski, Adrian Wong, Stefan Welker, Federico Tombari,
Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny
Lee, Vincent Vanhoucke, and Pete Florence. Socratic mod-
els: Composing zero-shot multimodal reasoning with lan-
guage. arXiv, 2022.

[64] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vi-
sion language pre-training: Aligning texts with visual con-
cepts. arXiv preprint arXiv:2111.08276, 2021.

[65] Yundong Zhang, Juan Carlos Niebles, and Alvaro Soto. In-
terpretable Visual Question Answering by Visual Ground-
ing from Attention Supervision Mining, Aug. 2018.
arXiv:1808.00265 [cs].

--- TRANG 12 ---
A. Các mô hình được huấn luyện trước
Chúng tôi chỉ định chi tiết về tất cả các mô hình được huấn luyện trước được sử dụng, cũng như mô hình ngôn ngữ lớn sinh mã:

•GLIP [31]. Chúng tôi sử dụng triển khai từ kho GitHub chính thức3. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng phiên bản GLIP-L (large). Để thích ứng với các phiên bản PyTorch mới, chúng tôi phải sửa đổi triển khai CUDA của một số hàm, vì kho lưu trữ dựa vào các phiên bản PyTorch cũ. Chúng tôi cung cấp phiên bản GLIP được cập nhật trong mã của chúng tôi.

•MiDaS [44]. Chúng tôi sử dụng triển khai từ PyTorch hub4, và sử dụng phiên bản "DPT_Large".

•BLIP-2 [30]. Chúng tôi thử cả triển khai từ kho chính thức5 và Huggingface6, với ít khác biệt giữa hai cái, phiên bản đầu tiên hiệu suất cao hơn một chút và phiên bản sau nhanh hơn. Trong cả hai trường hợp, chúng tôi sử dụng phiên bản Flan-T5 XXL.

•X-VLM [64]. Chúng tôi sử dụng triển khai chính thức7, cụ thể là phiên bản được tinh chỉnh cho retrieval trên MSCOCO.

•GPT-3 cho llm_query. Mô hình GPT-3 mà chúng tôi sử dụng cho hàm LLM query là text-davinci-003. Chúng tôi sử dụng API Python chính thức của OpenAI8.

•Codex. Mô hình GPT-3 mà chúng tôi sử dụng cho sinh mã là code-davinci-002.

Xem mã để biết thêm chi tiết triển khai cụ thể.

B. API
Chúng tôi cung cấp API đầy đủ tiếp theo, trong Listing 1:

1class ImagePatch:
2 """Một lớp Python chứa một crop của hình ảnh tập trung xung quanh một đối tượng cụ thể, cũng như thông tin liên quan.
3 Thuộc tính
4 ----------
5 cropped_image : array_like
6 Một array-like của hình ảnh được cắt lấy từ hình ảnh gốc.
7 left : int
8 Một int mô tả vị trí của đường viền trái của hộp giới hạn crop trong hình ảnh gốc.
9 lower : int
10 Một int mô tả vị trí của đường viền dưới của hộp giới hạn crop trong hình ảnh gốc.
11 right : int
12 Một int mô tả vị trí của đường viền phải của hộp giới hạn crop trong hình ảnh gốc.
13 upper : int
14 Một int mô tả vị trí của đường viền trên của hộp giới hạn crop trong hình ảnh gốc.
15
16 Phương thức
17 -------
18 find(object_name: str)->List[ImagePatch]
19 Trả về một danh sách các đối tượng ImagePatch mới chứa các crop của hình ảnh tập trung xung quanh bất kỳ đối tượng nào được tìm thấy trong
20 hình ảnh khớp với object_name.
21 exists(object_name: str)->bool
22 Trả về True nếu đối tượng được chỉ định bởi object_name được tìm thấy trong hình ảnh, và False nếu không.
23 verify_property(property: str)->bool
24 Trả về True nếu thuộc tính được thỏa mãn, và False nếu không.
25 best_text_match(option_list: List[str], prefix: str)->str
26 Trả về chuỗi khớp tốt nhất với hình ảnh.
27 simple_query(question: str=None)->str
28 Trả về câu trả lời cho một câu hỏi cơ bản được hỏi về hình ảnh. Nếu không có câu hỏi nào được cung cấp, trả về câu trả lời
29 cho "Đây là gì?".
30 compute_depth()->float
31 Trả về độ sâu trung vị của crop hình ảnh.
32 crop(left: int, lower: int, right: int, upper: int)->ImagePatch
33 Trả về một đối tượng ImagePatch mới chứa một crop của hình ảnh tại các tọa độ cho trước.
34 """
35
36 def __init__(self, image, left: int=None, lower: int=None, right: int=None, upper: int=None):
37 """Khởi tạo một đối tượng ImagePatch bằng cách cắt hình ảnh tại các tọa độ cho trước và lưu trữ các tọa độ như thuộc tính.

3https://github.com/microsoft/GLIP
4https://pytorch.org/hub/intelisl_midas_v2/
5https://github.com/salesforce/LAVIS/tree/main/projects/blip2
6https://huggingface.co/Salesforce/blip2-flan-t5-xxl
7https://github.com/zengyan-97/X-VLM
8https://openai.com/blog/openai-api

--- TRANG 13 ---
38 Nếu không có tọa độ nào được cung cấp, hình ảnh được để nguyên không thay đổi, và các tọa độ được đặt theo kích thước của hình ảnh.
39 Tham số
40 -------
41 image : array_like
42 Một array-like của hình ảnh gốc.
43 left : int
44 Một int mô tả vị trí của đường viền trái của hộp giới hạn crop trong hình ảnh gốc.
45 lower : int
46 Một int mô tả vị trí của đường viền dưới của hộp giới hạn crop trong hình ảnh gốc.
47 right : int
48 Một int mô tả vị trí của đường viền phải của hộp giới hạn crop trong hình ảnh gốc.
49 upper : int
50 Một int mô tả vị trí của đường viền trên của hộp giới hạn crop trong hình ảnh gốc.
51
52 """
53 if left is None and right is None and upper is None and lower is None:
54 self.cropped_image = image
55 self.left = 0
56 self.lower = 0
57 self.right = image.shape[2] # width
58 self.upper = image.shape[1] # height
59 else:
60 self.cropped_image = image[:, lower:upper, left:right]
61 self.left = left
62 self.upper = upper
63 self.right = right
64 self.lower = lower
65
66 self.width = self.cropped_image.shape[2]
67 self.height = self.cropped_image.shape[1]
68
69 self.horizontal_center = (self.left + self.right) / 2
70 self.vertical_center = (self.lower + self.upper) / 2
71
72 def find(self, object_name: str) -> List[ImagePatch]:
73 """Trả về một danh sách các đối tượng ImagePatch khớp với object_name có trong crop nếu có bất kỳ cái nào được tìm thấy.
74 Nếu không, trả về một danh sách trống.
75 Tham số
76 ----------
77 object_name : str
78 tên của đối tượng cần tìm
79
80 Trả về
81 -------
82 List[ImagePatch]
83 một danh sách các đối tượng ImagePatch khớp với object_name có trong crop
84
85 Ví dụ
86 --------
87 >>> # trả về trẻ em
88 >>> def execute_command(image) -> List[ImagePatch]:
89 >>> image_patch = ImagePatch(image)
90 >>> children = image_patch.find("child")
91 >>> return children
92 """
93
94 def exists(self, object_name: str) -> bool:
95 """Trả về True nếu đối tượng được chỉ định bởi object_name được tìm thấy trong hình ảnh, và False nếu không.
96 Tham số
97 -------
98 object_name : str
99 Một chuỗi mô tả tên của đối tượng cần tìm trong hình ảnh.
100
101 Ví dụ
102 -------
103 >>> # Có cả bánh và gấu kẹo trong ảnh không?
104 >>> def execute_command(image)->str:
105 >>> image_patch = ImagePatch(image)
106 >>> is_cake = image_patch.exists("cake")
107 >>> is_gummy_bear = image_patch.exists("gummy bear")
108 >>> return bool_to_yesno(is_cake and is_gummy_bear)
109 """
110 return len(self.find(object_name)) > 0
111
112 def verify_property(self, object_name: str, property: str) -> bool:
113 """Trả về True nếu đối tượng sở hữu thuộc tính, và False nếu không.
114 Khác với 'exists' ở chỗ nó giả định sự tồn tại của đối tượng được chỉ định bởi object_name, thay vào đó kiểm tra xem đối tượng
có sở hữu thuộc tính hay không.
115 Tham số

--- TRANG 14 ---
116 -------
117 object_name : str
118 Một chuỗi mô tả tên của đối tượng cần tìm trong hình ảnh.
119 property : str
120 Một chuỗi mô tả thuộc tính cần kiểm tra.
121
122 Ví dụ
123 -------
124 >>> # Những chữ cái có màu xanh dương không?
125 >>> def execute_command(image) -> str:
126 >>> image_patch = ImagePatch(image)
127 >>> letters_patches = image_patch.find("letters")
128 >>> # Câu hỏi giả định chỉ có một letter patch
129 >>> if len(letters_patches) == 0:
130 >>> # Nếu không tìm thấy chữ cái nào, truy vấn hình ảnh trực tiếp
131 >>> return image_patch.simple_query("Những chữ cái có màu xanh dương không?")
132 >>> return bool_to_yesno(letters_patches[0].verify_property("letters", "blue"))
133 """
134 return verify_property(self.cropped_image, object_name, property)
135
136 def best_text_match(self, option_list: List[str]) -> str:
137 """Trả về chuỗi khớp tốt nhất với hình ảnh.
138 Tham số
139 -------
140 option_list : str
141 Một danh sách với tên của các tùy chọn khác nhau
142 prefix : str
143 Một chuỗi với tiền tố để nối vào các tùy chọn
144
145 Ví dụ
146 -------
147 >>> # Chiếc mũ có màu vàng hay trắng?
148 >>> def execute_command(image)->str:
149 >>> image_patch = ImagePatch(image)
150 >>> cap_patches = image_patch.find("cap")
151 >>> # Câu hỏi giả định một cap patch
152 >>> if len(cap_patches) == 0:
153 >>> # Nếu không tìm thấy mũ, truy vấn hình ảnh trực tiếp
154 >>> return image_patch.simple_query("Chiếc mũ có màu vàng hay trắng?")
155 >>> return cap_patches[0].best_text_match(["gold", "white"])
156 """
157 return best_text_match(self.cropped_image, option_list)
158
159 def simple_query(self, question: str = None) -> str:
160 """Trả về câu trả lời cho một câu hỏi cơ bản được hỏi về hình ảnh. Nếu không có câu hỏi nào được cung cấp, trả về câu trả lời cho "Đây là gì?".
161 Tham số
162 -------
163 question : str
164 Một chuỗi mô tả câu hỏi cần hỏi.
165
166 Ví dụ
167 -------
168
169 >>> # Loại động vật nào không ăn?
170 >>> def execute_command(image) -> str:
171 >>> image_patch = ImagePatch(image)
172 >>> animal_patches = image_patch.find("animal")
173 >>> for animal_patch in animal_patches:
174 >>> if not animal_patch.verify_property("animal", "eating"):
175 >>> return animal_patch.simple_query("Loại động vật nào đang ăn?") # crop sẽ bao gồm eating nên giữ nó trong truy vấn
176 >>> # Nếu không có động vật nào không ăn, truy vấn hình ảnh trực tiếp
177 >>> return image_patch.simple_query("Loại động vật nào không ăn?")
178
179 >>> # Có gì ở phía trước con ngựa?
180 >>> # chứa một quan hệ (xung quanh, bên cạnh, trên, gần, trên đỉnh, phía trước, phía sau, v.v.), nên hỏi trực tiếp
181 >>> return image_patch.simple_query("Có gì ở phía trước con ngựa?")
182 >>>
183 """
184 return simple_qa(self.cropped_image, question)
185
186 def compute_depth(self):
187 """Trả về độ sâu trung vị của crop hình ảnh
188 Tham số
189 ----------
190 Trả về
191 -------
192 float
193 độ sâu trung vị của crop hình ảnh
194

--- TRANG 15 ---
195 Ví dụ
196 --------
197 >>> # người ở xa nhất
198 >>> def execute_command(image)->ImagePatch:
199 >>> image_patch = ImagePatch(image)
200 >>> person_patches = image_patch.find("person")
201 >>> person_patches.sort(key=lambda person: person.compute_depth())
202 >>> return person_patches[-1]
203 """
204 depth_map = compute_depth(self.cropped_image)
205 return depth_map.median()
206
207 def crop(self, left: int, lower: int, right: int, upper: int) -> ImagePatch:
208 """Trả về một ImagePatch mới được cắt từ ImagePatch hiện tại.
209 Tham số
210 -------
211 left : int
212 Pixel trái nhất của hình ảnh được cắt.
213 lower : int
214 Pixel thấp nhất của hình ảnh được cắt.
215 right : int
216 Pixel phải nhất của hình ảnh được cắt.
217 upper : int
218 Pixel cao nhất của hình ảnh được cắt.
219 -------
220 """
221 return ImagePatch(self.cropped_image, left, lower, right, upper)
222
223 def overlaps_with(self, left, lower, right, upper):
224 """Trả về True nếu một crop với các tọa độ cho trước chồng lấp với cái này,
225 nếu không thì False.
226 Tham số
227 ----------
228 left : int
229 đường viền trái của crop cần kiểm tra
230 lower : int
231 đường viền dưới của crop cần kiểm tra
232 right : int
233 đường viền phải của crop cần kiểm tra
234 upper : int
235 đường viền trên của crop cần kiểm tra
236
237 Trả về
238 -------
239 bool
240 True nếu một crop với các tọa độ cho trước chồng lấp với cái này, nếu không thì False
241
242 Ví dụ
243 --------
244 >>> # cốc đen trên bàn
245 >>> def execute_command(image) -> ImagePatch:
246 >>> image_patch = ImagePatch(image)
247 >>> table_patches = image_patch.find("table")
248 >>> if len(table_patches) == 0:
249 >>> table_patches = [image_patch] # Nếu không tìm thấy bàn, giả định toàn bộ hình ảnh là một cái bàn
250 >>> table_patch = table_patches[0]
251 >>> cup_patches = image_patch.find("black cup")
252 >>> for cup in cup_patches:
253 >>> if cup.vertical_center > table_patch.vertical_center
254 >>> return cup
255 >>> return cup_patches[0] # Nếu không tìm thấy cốc trên bàn, trả về cốc đầu tiên được tìm thấy
256 """
257 return self.left <= right and self.right >= left and self.lower <= upper and self.upper >= lower
258
259
260def best_image_match(list_patches: List[ImagePatch], content: List[str], return_index=False) -> Union[ImagePatch, int]:
261 """Trả về patch có khả năng chứa nội dung cao nhất.
262 Tham số
263 ----------
264 list_patches : List[ImagePatch]
265 content : List[str]
266 đối tượng quan tâm
267 return_index : bool
268 nếu True, trả về chỉ số của patch có khả năng chứa đối tượng cao nhất
269
270 Trả về
271 -------
272 int
273 Patch có khả năng chứa đối tượng cao nhất

--- TRANG 16 ---
274
275 Ví dụ
276 --------
277 >>> # Trả về người đàn ông đội mũ
278 >>> def execute_command(image):
279 >>> image_patch = ImagePatch(image)
280 >>> man_patches = image_patch.find("man")
281 >>> if len(man_patches) == 0:
282 >>> return image_patch
283 >>> hat_man = best_image_match(list_patches=man_patches, content=["hat"])
284 >>> return hat_man
285
286 >>> # Trả về người phụ nữ với khăn quàng cổ hồng và quần xanh
287 >>> def execute_command(image):
288 >>> image_patch = ImagePatch(image)
289 >>> woman_patches = image_patch.find("woman")
290 >>> if len(woman_patches) == 0:
291 >>> return image_patch
292 >>> woman_most = best_image_match(list_patches=woman_patches, content=["pink scarf", "blue pants"])
293 >>> return woman_most
294 """
295 return best_image_match(list_patches, content, return_index)
296
297
298def distance(patch_a: ImagePatch, patch_b: ImagePatch) -> float:
299 """
300 Trả về khoảng cách giữa các cạnh của hai ImagePatch. Nếu các patch chồng lấp, nó trả về khoảng cách âm
301 tương ứng với intersection over union âm.
302 """
303 return distance(patch_a, patch_b)
304
305
306def bool_to_yesno(bool_answer: bool) -> str:
307 return "yes" if bool_answer else "no"
308
309
310def llm_query(question: str) -> str:
311 '''Trả lời một câu hỏi văn bản sử dụng GPT-3. Câu hỏi đầu vào luôn là một chuỗi được định dạng với một biến trong đó.
312
313 Tham số
314 ----------
315 question: str
316 câu hỏi văn bản cần hỏi. Không được chứa bất kỳ tham chiếu nào đến 'hình ảnh' hoặc 'ảnh', v.v.
317 '''
318 return llm_query(question)
319
320
321class VideoSegment:
322 """Một lớp Python chứa một tập hợp các frame được biểu diễn như các đối tượng ImagePatch, cũng như thông tin liên quan.
323 Thuộc tính
324 ----------
325 video : torch.Tensor
326 Một tensor của video gốc.
327 start : int
328 Một int mô tả frame bắt đầu trong đoạn video này đối với video gốc.
329 end : int
330 Một int mô tả frame kết thúc trong đoạn video này đối với video gốc.
331 num_frames->int
332 Một int chứa số lượng frame trong đoạn video.
333
334 Phương thức
335 -------
336 frame_iterator->Iterator[ImagePatch]
337 trim(start, end)->VideoSegment
338 Trả về một VideoSegment mới chứa phiên bản được cắt của video gốc tại đoạn [start, end].
339 select_answer(info, question, options)->str
340 Trả về câu trả lời cho câu hỏi được cung cấp các tùy chọn và thông tin bổ sung.
341 """
342
343 def __init__(self, video: torch.Tensor, start: int = None, end: int = None, parent_start=0, queues=None):
344 """Khởi tạo một đối tượng VideoSegment bằng cách cắt video tại các thời gian [start, end] cho trước và lưu trữ
345 thời gian bắt đầu và kết thúc như thuộc tính. Nếu không có thời gian nào được cung cấp, video được để nguyên không thay đổi, và thời gian được
346 đặt ở đầu và cuối video.
347
348 Tham số
349 -------
350 video : torch.Tensor
351 Một tensor của video gốc.
352 start : int

--- TRANG 17 ---
353 Một int mô tả frame bắt đầu trong đoạn video này đối với video gốc.
354 end : int
355 Một int mô tả frame kết thúc trong đoạn video này đối với video gốc.
356 """
357
358 if start is None and end is None:
359 self.trimmed_video = video
360 self.start = 0
361 self.end = video.shape[0] # duration
362 else:
363 self.trimmed_video = video[start:end]
364 if start is None:
365 start = 0
366 if end is None:
367 end = video.shape[0]
368 self.start = start + parent_start
369 self.end = end + parent_start
370
371 self.num_frames = self.trimmed_video.shape[0]
372
373 def frame_iterator(self) -> Iterator[ImagePatch]:
374 """Trả về một iterator trên các frame trong đoạn video."""
375 for i in range(self.num_frames):
376 yield ImagePatch(self.trimmed_video[i], self.start + i)
377
378 def trim(self, start: Union[int, None] = None, end: Union[int, None] = None) -> VideoSegment:
379 """Trả về một VideoSegment mới chứa phiên bản được cắt của video gốc tại đoạn [start, end].
380
381 Tham số
382 ----------
383 start : Union[int, None]
384 Một int mô tả frame bắt đầu trong đoạn video này đối với video gốc.
385 end : Union[int, None]
386 Một int mô tả frame kết thúc trong đoạn video này đối với video gốc.
387
388 Ví dụ
389 --------
390 >>> # Trả về nửa sau của video
391 >>> def execute_command(video):
392 >>> video_segment = VideoSegment(video)
393 >>> video_second_half = video_segment.trim(video_segment.num_frames // 2, video_segment.num_frames)
394 >>> return video_second_half
395 """
396 if start is not None:
397 start = max(start, 0)
398 if end is not None:
399 end = min(end, self.num_frames)
400
401 return VideoSegment(self.trimmed_video, start, end, self.start)
402
403 def select_answer(self, info: dict, question: str, options: List[str]) -> str:
404 return select_answer(self.trimmed_video, info, question, options)
405
406 def __repr__(self):
407 return "VideoSegment({}, {})".format(self.start, self.end)

Listing 1. API đầy đủ.

Không phải tất cả các phương thức đều được sử dụng trong tất cả các benchmark. Tiếp theo chúng tôi mô tả chi tiết hơn nội dung nào được sử dụng cho các đặc tả API cho mỗi benchmark.

•RefCOCO và RefCOCO+. Chúng tôi sử dụng tất cả các phương thức từ lớp ImagePatch ngoại trừ best_text_match và simple_query. Chúng tôi cũng sử dụng các hàm best_text_match và distance. Ngoài ra chúng tôi thêm các ví dụ sử dụng ImagePatch trong định nghĩa API đại diện cho tập dữ liệu RefCOCO, và trông như sau:

1# ghế ở phía trước
2def execute_command(image) -> ImagePatch:
3 # Trả về ghế
4 image_patch = ImagePatch(image)
5 chair_patches = image_patch.find("chair")
6 chair_patches.sort(key=lambda chair: chair.compute_depth())
7 chair_patch = chair_patches[0]
8 # Nhớ: trả về ghế
9 return chair_patch

Listing 2. Ví dụ RefCOCO.

--- TRANG 18 ---
•GQA. API GQA chứa tất cả nội dung trong API từ Listing 1 cho đến hàm llm_query, không được sử dụng. Các ví dụ sử dụng ImagePatch trông như sau:

1# Có ba lô ở bên phải người đàn ông không?
2def execute_command(image)->str:
3 image_patch = ImagePatch(image)
4 man_patches = image_patch.find("man")
5 # Câu hỏi giả định một man patch
6 if len(man_patches) == 0:
7 # Nếu không tìm thấy người đàn ông, truy vấn hình ảnh trực tiếp
8 return image_patch.simple_query("Có ba lô ở bên phải người đàn ông không?")
9 man_patch = man_patches[0]
10 backpack_patches = image_patch.find("backpack")
11 # Câu hỏi giả định một backpack patch
12 if len(backpack_patches) == 0:
13 return "no"
14 for backpack_patch in backpack_patches:
15 if backpack_patch.horizontal_center > man_patch.horizontal_center:
16 return "yes"
17 return "no"

Listing 3. Ví dụ GQA.

•OK-VQA. API chỉ sử dụng phương thức simple_query từ ImagePatch. Nó cũng sử dụng hàm llm_query. Các ví dụ sử dụng ImagePatch trông như sau:

1
2# Ai nổi tiếng vì được cho là đã làm điều này trong một cơn bão sét?
3def execute_command(image)->str:
4 # Câu hỏi không phải là perception trực tiếp, nên chúng ta cần hỏi hình ảnh để có thêm thông tin
5 # Thông tin nổi bật: điều gì đang được thực hiện?
6 image = ImagePatch(image)
7 guesses = []
8 action = image.simple_query("Điều gì đang được thực hiện?")
9 external_knowledge_query = "Ai nổi tiếng vì được cho là {} trong một cơn bão sét?".format(action)
10 step_by_step_guess = llm_query(external_knowledge_query)
11 guesses.append("điều đang được thực hiện là {}".format(action) + ", vậy " + step_by_step_guess)
12 direct_guess = image.simple_query("Ai nổi tiếng vì được cho là đã làm điều này trong một cơn bão sét?")
13 guesses.append(direct_guess)
14 return process_guesses("Ai nổi tiếng vì được cho là đã làm điều này trong một cơn bão sét?", guesses)

Listing 4. Ví dụ OK-VQA.

•NeXT-QA. Lớp VideoSegment được thêm vào định nghĩa API, và các phương thức ImagePatch có sẵn là find, exists, best_text_match và simple_query. Hàm best_image_match cũng được sử dụng. Các ví dụ sử dụng ImagePatch trông như:

1# tại sao người đàn ông đội mũ đỏ hạ tay xuống ở cuối video
2# các câu trả lời có thể: ['xem tivi', 'tìm kiếm thức ăn', 'di chuyển đầu', 'nhìn qua hộp carton', 'nhìn camera']
3def execute_command(video, possible_answers, question)->[str, dict]:
4 # Lý luận từng bước
5 video_segment = VideoSegment(video)
6 # Caption frame cuối của video (cuối video)
7 last_frame = ImagePatch(video_segment, -1)
8 last_caption = last_frame.simple_query("Đây là gì?")
9 men = last_frame.find("man")
10 if len(men) == 0:
11 men = [last_frame]
12 man = men[0]
13 man_action = man.simple_query("Người đàn ông đang làm gì?")
14 # Trả lời câu hỏi. Nhớ tạo dictionary info
15 info = {
16 "Caption của frame cuối": last_caption,
17 "Người đàn ông có vẻ như đang": man_action
18 }
19 answer = video_segment.select_answer(info, question, possible_answers)
20 return answer, info

Listing 5. Ví dụ NeXT-QA.

•Ngoài benchmark. Đối với các ví dụ trong Hình 1 chúng tôi sử dụng API giống như được sử dụng cho các benchmark, và các ví dụ sử dụng được lấy từ các API benchmark, kết hợp chúng để có tính tổng quát hơn. Chúng tôi không thêm bất kỳ ví dụ nào khác, ViperGPT tổng quát hóa đến các trường hợp phức tạp được hiển thị trong Hình 1 chỉ dựa trên API được cung cấp.

Lưu ý rằng trong một số ví dụ chúng tôi đã thêm comment, cũng như xử lý lỗi. Mã được sinh cũng chứa những dòng tương tự. Chúng tôi đã loại bỏ những điều đó để rõ ràng trong các hình được hiển thị trong bài báo chính.
