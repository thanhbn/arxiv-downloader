# 2211.01571.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2211.01571.pdf
# Kích thước tệp: 652263 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Đa-đơn vị Mục tiêu Hỗ trợ Ngữ âm để Cải thiện
Hệ thống ASR Conformer-Transducer
Li Li1, Dongxing Xu2, Haoran Wei3, Yanhua Long1∗
1Trung tâm Nghiên cứu Kỹ thuật Giáo dục Thông minh và Dữ liệu Lớn Shanghai,
Đại học Sư phạm Shanghai, Shanghai, Trung Quốc
2Công ty Công nghệ AI Unisound, Bắc Kinh, Trung Quốc
3Khoa ECE, Đại học Texas tại Dallas, Richardson, TX 75080, Hoa Kỳ
lili a0@163.com, xudongxing@unisound.com, haoran.wei@utdallas.edu, yanhua@shnu.edu.cn

Tóm tắt
Khai thác các đơn vị mô hình mục tiêu hiệu quả rất quan trọng và luôn là mối quan tâm trong nhận dạng giọng nói tự động đầu cuối đến cuối (ASR). Trong nghiên cứu này, chúng tôi đề xuất phương pháp mô hình đa-đơn vị mục tiêu hỗ trợ ngữ âm (PMU), để tăng cường hệ thống ASR Conformer-Transducer theo cách học biểu diễn tiến bộ. Cụ thể, PMU đầu tiên sử dụng mô hình từ con hỗ trợ phát âm (PASM) và mã hóa cặp byte (BPE) để tạo ra các đơn vị mục tiêu cảm ứng ngữ âm và cảm ứng văn bản một cách riêng biệt; Sau đó, ba khung mới được nghiên cứu để tăng cường bộ mã hóa âm thanh, bao gồm PMU cơ bản, paraCTC và paCTC, chúng tích hợp các đơn vị PASM và BPE ở các cấp độ khác nhau cho huấn luyện đa nhiệm CTC và transducer. Các thí nghiệm trên cả nhiệm vụ ASR LibriSpeech và ASR có giọng cho thấy PMU được đề xuất vượt trội đáng kể so với BPE truyền thống, nó giảm WER của LibriSpeech clean, other và sáu bộ dữ liệu kiểm tra ASR có giọng lần lượt 12.7%, 4.3% và 7.7% tương đối.

Từ khóa: đa-đơn vị mục tiêu, PMU, paCTC, Conformer-Transducer, ASR đầu cuối đến cuối

1. Giới thiệu
Conformer-Transducer (ConformerT) đã đạt được kết quả tốt nhất trong nhiều nhiệm vụ ASR [1–4] bởi vì việc kế thừa hoàn hảo các ưu điểm của conformer và transducer. Nó nắm bắt cả đặc trưng cục bộ và toàn cục bằng cách kết hợp mô-đun tích chập và transformer theo cách hiệu quả về tham số. Cùng với tính chất streaming tự nhiên của transducer, ConformerT đã trở nên ngày càng hấp dẫn trong các hệ thống ASR đầu cuối đến cuối (E2E) gần đây.

Như nhiều hệ thống ASR E2E, việc khám phá các đơn vị mô hình mục tiêu hiệu quả cho ConformerT cũng rất quan trọng. Các loại chính của đơn vị mục tiêu ASR E2E có thể được chia thành các đơn vị cảm ứng văn bản và các đơn vị cảm ứng ngữ âm. Ký tự, từ và từ con đều là các đơn vị cảm ứng văn bản và đã được nghiên cứu rộng rãi [5–10]. So với ký tự, từ con có thể tránh chuỗi đầu ra quá dài và phụ thuộc, điều này làm giảm độ khó của việc mô hình hóa và giải mã [7]. Nhiều kỹ thuật mô hình từ con đã được đề xuất: mã hóa cặp byte (BPE) [11], WordPieceModel (WPM) [12] và mô hình ngôn ngữ unigram (ULM) [13], v.v. Tuy nhiên, tất cả các kỹ thuật này đều hoàn toàn cảm ứng văn bản mà không có bất kỳ truy cập nào đến thông tin ngữ âm/phát âm cơ bản, điều này là chìa khóa của ASR.

Âm tiết, âm vị [5, 6, 14, 15] thuộc về các đơn vị mục tiêu cảm ứng ngữ âm, chúng cho phép mô hình học các mẫu ngữ âm tốt hơn của một ngôn ngữ, tuy nhiên, một từ điển phát âm bổ sung được yêu cầu trong cả quá trình huấn luyện mô hình và suy luận. Do đó, cách khai thác tốt thông tin trong cả đơn vị mục tiêu cảm ứng văn bản và cảm ứng ngữ âm trở nên rất quan trọng và cơ bản.

Trong tài liệu, một số nghiên cứu gần đây đã được đề xuất để kết hợp thông tin văn bản và ngữ âm để xây dựng hệ thống ASR E2E tốt hơn. Chẳng hạn, [16] đề xuất một đơn vị mục tiêu hybrid của âm tiết-ký tự-từ con trong học đa nhiệm CTC/Attention chung cho hệ thống ASR tiếng Quan Thoại; Trong khi đó [17], một mô hình từ con hỗ trợ phát âm (PASM) được giới thiệu để trích xuất các đơn vị mục tiêu ASR bằng cách khám phá cấu trúc âm thanh của chúng từ từ điển phát âm. Ngoài ra, [18] đã cố gắng khai thác thông tin văn bản và ngữ âm cơ bản trong âm thanh theo cách khác, các tác giả sử dụng một tập hợp các đơn vị văn bản tăng dần theo thứ bậc cho việc mô hình CTC của các lớp mã hóa Transformer trung gian. Tất cả các nghiên cứu này đã được xác minh là hiệu quả để cải thiện các hệ thống ASR E2E hiện tại.

Được thúc đẩy bởi PASM và nghiên cứu trong [18], nghiên cứu này nhằm cải thiện hệ thống ASR Conformer-Transducer bằng cách đề xuất phương pháp mô hình đa-đơn vị mục tiêu hỗ trợ ngữ âm (PMU). PMU được thiết kế để học thông tin từ cả đơn vị PASM cảm ứng ngữ âm và đơn vị BPE cảm ứng văn bản, sử dụng ba khung mô hình âm thanh mới như sau: 1) PMU Cơ bản. ConformerT được huấn luyện với cả mất mát CTC và transducer, nhưng gán đơn vị PASM và BPE cho chúng tương ứng; 2) paraCTC. Với cùng đơn vị BPE như trong 1) cho transducer, một mất mát CTC song song với cả PASM và BPE như các đơn vị mục tiêu của bộ mã hóa Conformer được lấy làm nhiệm vụ phụ trợ của việc huấn luyện mô hình ConformerT; 3) paCTC. Khác với 1) và 2), paCTC áp dụng một bộ mã hóa âm thanh có điều kiện ngữ âm, bằng cách sử dụng các đơn vị PASM và BPE theo cách tương tác với mất mát CTC của các bộ mã hóa Conformer trung gian khác nhau. Từ các thí nghiệm được thực hiện trên bộ dữ liệu LibriSpeech và CommonVoice, chúng ta thấy rằng ConformerT tiêu chuẩn được cải thiện đáng kể bởi PMU được đề xuất của chúng tôi, giảm WER tương đối lên đến 4.3% đến 12.7% được đạt được trên LibriSpeech clean và other, hoặc sáu bộ kiểm tra tiếng Anh có giọng của CommonVoice.

2. Conformer-Transducer
Conformer-Transducer (ConformerT) được đề xuất lần đầu trong [1,2]. Nó có thể được huấn luyện sử dụng mất mát RNN-T đầu cuối đến cuối [19] với một bộ mã hóa nhãn và một bộ mã hóa âm thanh dựa trên Conformer (AEncoder). Kiến trúc được minh họa trong Hình 1 (a). Cho một đặc trưng âm thanh đầu vào với T khung hình như x = (x1,...,xT), và chuỗi nhãn phiên âm tương ứng có độ dài U như y = (y1,...,yU). AEncoder đầu tiên biến đổi x thành một biểu diễn cao ht, t ≤ T, và bộ mã hóa nhãn, hoạt động như một mô hình ngôn ngữ,

--- TRANG 2 ---
Hình 1: Cấu trúc của (a) đơn vị đa-mục tiêu hỗ trợ ngữ âm được đề xuất (PMU) và (b) AEncoder có điều kiện ngữ âm của nó (paCTC).

tạo ra một biểu diễn hu cho trước chuỗi nhãn phát ra trước đó yu-1
1. Sau đó, ht và hu được kết hợp sử dụng mạng kết hợp gồm các lớp feed-forward và một hàm phi tuyến để tính toán logits đầu ra. Cuối cùng, bằng cách áp dụng Softmax cho logits đầu ra, chúng ta có thể tạo ra phân phối xác suất mục tiêu hiện tại như:

P(ŷt,u|x, yu-1
1) = Softmax (Joint (ht, hu)) (1)

Nhãn ŷt,u có thể tùy chọn là ký hiệu trống. Loại bỏ tất cả các ký hiệu trống trong chuỗi ŷt,u sẽ tạo ra y. Cho A, tập hợp tất cả các căn chỉnh có thể ŷ (với ký hiệu trống ϕ) giữa đầu vào x và đầu ra y, hàm mất mát ConformerT có thể được tính như logarit âm posterior sau:

Ltrans = -logP(y|x) = -log∑ŷ∈A P(ŷ|x) (2)

Bên cạnh mất mát transducer Ltrans, như trong [20], chúng tôi cũng huấn luyện ConformerT chung với một mất mát CTC phụ trợ LCTC [21] để học các biểu diễn âm thanh cấp khung hình và cung cấp giám sát cho AEncoder. Hàm mục tiêu ConformerT tổng thể được định nghĩa như:

Lobj = λtransLtrans + λctcLCTC (3)

trong đó λtrans, λctc ∈ [0,1] là các trọng số mất mát có thể điều chỉnh.

3. Phương pháp Được đề xuất

Mặc dù PASM [17] đã được đề xuất để tăng cường việc trích xuất các đơn vị mục tiêu ASR E2E, bằng cách tận dụng cấu trúc ngữ âm của âm thanh trong giọng nói sử dụng từ điển phát âm, hầu hết các hệ thống ASR ConformerT hiện tại vẫn chỉ sử dụng các từ con cảm ứng văn bản thuần túy, chẳng hạn như BPE, wordpieces như các đơn vị mô hình mục tiêu của chúng [2,3]. Điều này có thể do bị hạn chế bởi mẫu ngữ âm trong từ điển, PASM có xu hướng tạo ra các từ con ngắn và tránh mô hình các từ lớn hơn hoặc toàn bộ từ với các token đơn, kích thước từ vựng tương đối nhỏ kết quả hạn chế đáng kể giới hạn hiệu suất trên của PASM. Do đó, trong nghiên cứu này, chúng tôi đề xuất một mô hình đa-đơn vị mục tiêu hỗ trợ ngữ âm (PMU) để tích hợp các ưu điểm của cả đơn vị PASM và BPE để cải thiện ConformerT trong khung huấn luyện đa nhiệm CTC/transducer.

Toàn bộ kiến trúc của ConformerT với PMU được thể hiện trong Hình 1(a), trong đó chúng tôi sử dụng các loại đơn vị mục tiêu khác nhau cho nhánh CTC và transducer. BPE-trans có nghĩa là sử dụng các đơn vị BPE cảm ứng văn bản để căn chỉnh các đầu ra transducer trong quá trình huấn luyện ConformerT, trong khi đối với bộ mã hóa âm thanh chia sẻ (AEncoder) với nhánh CTC, chúng tôi nghiên cứu ba phương pháp mô hình đơn vị mục tiêu mới, như được minh họa trong phần bên trái của Hình 1(a) và Hình 1(b), phương pháp đầu tiên là PMU cơ bản với PASM-CTC, trong đó chỉ các đơn vị PASM được lấy làm mục tiêu CTC, hai phương pháp khác thay thế PASM-CTC bằng paraCTC và paCTC tương ứng. Tất cả PASM-CTC, BPE-CTC và BPE-trans đều được cấu thành bởi một lớp feed-forward được kết nối đầy đủ duy nhất với các đơn vị mục tiêu khác nhau theo sau bởi hàm Softmax. Cho cả đơn vị PASM và BPE, mất mát mục tiêu tổng thể của PMU cơ bản được định nghĩa như:

LPMU = λtransLBPE-trans + λctcLPASM-CTC (4)

Trong đó LBPE-trans và LPASM-CTC đại diện cho mất mát của transducer và CTC sử dụng đơn vị BPE và PASM làm mục tiêu tương ứng. Nếu chúng ta sử dụng paraCTC hoặc paCTC, mất mát của LPASM-CTC trong Eq.(4) sẽ được thay thế bởi mất mát CTC tương ứng LparaCTC và LpaCTC. Chi tiết về cách tạo ra các đơn vị PASM với từ điển phát âm và văn bản huấn luyện cho trước có thể được tìm thấy trong [17].

3.1. ParaCTC

Huấn luyện một mô hình với mất mát CTC được áp dụng song song với lớp cuối cùng gần đây đã đạt được thành công [22–25]. Trong paraCTC của chúng tôi, như được hiển thị trong Hình 1(a), chúng tôi sử dụng hai lớp tuyến tính khác nhau để biến đổi biểu diễn AEncoder thành đơn vị BPE và PASM với mất mát LCTC(yBPE|x) và LCTC(yPASM|x) tương ứng. Hàm mất mát tổng thể của paraCTC được định nghĩa như:

LparaCTC = αLCTC(yPASM|x) + (1 - α)LCTC(yBPE|x) (5)

trong đó α ∈ (0,1), yPASM và yBPE đại diện cho các đơn vị mục tiêu của CTC là PASM và BPE tương ứng. Với Eq.(5), thông tin cấu trúc ngữ âm và văn bản cơ bản trong PASM và BPE được khai thác và kết hợp hiệu quả để tăng cường AEncoder.

--- TRANG 3 ---

3.2. PaCTC

Khác với PMU cơ bản và paraCTC, paCTC được đề xuất của chúng tôi tăng cường AEncoder theo cách có điều kiện ngữ âm, bằng cách sử dụng các căn chỉnh CTC giữa yPASM hoặc yBPE và đầu ra của các lớp AEncoder trung gian. Cấu trúc tổng quan của paCTC được hiển thị trong Hình 1(b). Chúng tôi đầu tiên cắt toàn bộ AEncoder thành các lớp dưới N1 và trên N3. Sau đó, huấn luyện chung PASM-CTC và BPE-CTC được áp dụng cho hai khối AEncoder này để căn chỉnh các đầu ra cấp khung hình hN1 và hN3 tương ứng, sử dụng mất mát tương ứng LN1
PASM-CTC và LN3
BPE-CTC như:

LpaCTC = βLN1
PASM-CTC + (1-β)LN3
BPE-CTC (6)

Trong đó LpaCTC là tổng mất mát paCTC và β ∈ (0,1) là một tham số trọng số.

Hơn nữa, như được minh họa trong Hình 1(b), một cơ chế tự điều kiện (SC) [26] được áp dụng để cải thiện thêm AEncoder, bằng cách làm cho các lớp AEncoder tiếp theo có điều kiện trên cả biểu diễn lớp trước đó và các dự đoán CTC trung gian. Linear trong SC có nghĩa là sử dụng một lớp được kết nối đầy đủ để biến đổi tuyến tính chiều của các dự đoán CTC trung gian thành cùng chiều của các lớp AEncoder. Chúng tôi mong đợi paCTC có thể vượt trội hơn hai biến thể PMU khác, bởi vì nó tích hợp cả ưu điểm PASM và BPE theo cách hiệu quả hơn, bằng cách áp dụng PASM-CTC trên AEncoder dưới cho phép ConformerT học các biểu diễn âm thanh tốt hơn từ mô hình PASM cảm ứng ngữ âm, trong khi áp dụng BPE-CTC trên AEncoder trên giúp tạo ra các nhúng ngôn ngữ mạnh mẽ hơn.

Ngoài ra, được truyền cảm hứng bởi ý tưởng về các đơn vị từ con tăng dần theo thứ bậc trong [18], chúng tôi cũng thiết kế một cấu trúc tùy chọn (khối nét đứt trong Hình 1) trong paCTC, bằng cách chèn một căn chỉnh BPE-CTC trung gian tại khối AEncoder giữa với N2 lớp. Với cấu trúc tùy chọn này, Eq.(6) sau đó được sửa đổi như sau:

LpaCTC = β/2 LN1
PASM-CTC + LN2
BPE-CTC + (1-β)LN3
BPE-CTC (7)

trong đó LN2
BPE-CTC là mất mát BPE-CTC của khối AEncoder giữa trung gian. Đáng chú ý rằng BPE-CTC trung gian và PASM-CTC có cùng kích thước từ vựng nhỏ hơn nhiều so với BPE-CTC được áp dụng cho khối AEncoder trên, chẳng hạn như 194 so với 3000. paCTC với cấu trúc tùy chọn này không chỉ có thể tận dụng thông tin ngữ âm cấp thấp để tạo ra các mục tiêu ngôn ngữ cấp cao tốt hơn, mà còn đạt được quá trình học biểu diễn tiến bộ có thể tích hợp các loại từ con khác nhau theo cách từ tinh đến thô. Hơn nữa, chúng tôi khám phá hai biến thể khác nhau của paCTC với cấu trúc tùy chọn, cụ thể là paCTC-s và paCTC-us. paCTC-s có nghĩa là chúng tôi không chỉ chia sẻ hai lớp tuyến tính SC, mà còn chia sẻ các tham số lớp tuyến tính của cả PASM-CTC và BPE-CTC trung gian, trong khi paCTC-us có nghĩa là không.

4. Thí nghiệm và Kết quả

4.1. Bộ dữ liệu

Các thí nghiệm của chúng tôi được thực hiện trên hai nhiệm vụ ASR tiếng Anh mã nguồn mở, một là bộ dữ liệu LibriSpeech [27] với 100 giờ dữ liệu huấn luyện và các bộ kiểm tra clean và other của nó, bộ kia là nhiệm vụ ASR có giọng với dữ liệu được chọn từ kho CommonVoice [28]. Dữ liệu huấn luyện tiếng Anh có giọng của chúng tôi có 150 giờ (hrs) giọng nói, bao gồm giọng Ấn Độ, Mỹ và Anh và mỗi giọng có 50 giờ. Chúng tôi xây dựng sáu bộ kiểm tra để đánh giá các phương pháp được đề xuất cho ASR có giọng, bao gồm ba bộ kiểm tra trong miền với 2 giờ giọng nói Mỹ, 1.92 giờ Anh và 3.87 giờ giọng Ấn Độ, ba bộ kiểm tra ngoài miền với 2 giờ Singapore, 2.2 giờ Canada và 2 giờ giọng nói Úc.

4.2. Thiết lập Thí nghiệm

Tất cả các thí nghiệm của chúng tôi được thực hiện sử dụng thư viện từ bộ công cụ nhận dạng giọng nói đầu cuối đến cuối ESPnet [29]. Chúng tôi sử dụng 80 chiều log-mel filterbank được chuẩn hóa trung bình-phương sai toàn cục làm đặc trưng âm thanh đầu vào. Không có kỹ thuật tăng cường dữ liệu và không có mô hình ngôn ngữ bổ sung được áp dụng.

Đối với bộ mã hóa âm thanh của ConformerT, chúng tôi lấy mẫu con các đặc trưng đầu vào với hệ số 4 sử dụng hai lớp tích chập 2D, theo sau bởi 12 lớp mã hóa conformer với chiều feed-forward 2048 và chiều attention 512 với 8 đầu self-attention. Đối với bộ mã hóa nhãn, chúng tôi chỉ sử dụng một LSTM 512 chiều. Mạng kết hợp là một mạng feed-forward 640 chiều với hàm kích hoạt tanh. Warmup được đặt thành 25000, và cả trọng số label smoothing [30] và dropout đều được đặt thành 0.1 để điều chuẩn mô hình. Các đơn vị BPE được tạo bởi SentencePiece [31], và fast align [32] được sử dụng để tạo ra các đơn vị PASM với từ điển phát âm CMU1. Trong Bảng 1 và Bảng 2, β được đặt thành 0.5 và 0.7 tương ứng, α = 0.7 cho paraCTC, λtrans = λctc = 0.5 cho tất cả các hệ thống với paCTC. Tất cả hiệu suất hệ thống được đánh giá sử dụng tỷ lệ lỗi từ (WER (%)).

4.3. Kết quả

4.3.1. Kết quả trên Librispeech

Bảng 1: WER(%) trên các bộ kiểm tra clean và other của nhiệm vụ ASR Libri-100hrs. TUctc và TUtrans đại diện cho loại đơn vị mục tiêu cho CTC và transducer trong ConformerT tương ứng. Trong paCTC, hệ thống 9-10 sử dụng cấu trúc tùy chọn với N1=N2=N3= 4, trong khi hệ thống 8 thì không (N2= 0, N1= N3= 6).

ID Phương pháp TUctc TUtrans Đánh giá
Clean Other
1
ConformerT BPE-194 11.2 30.6
2 BPE-3000 11.0 29.9
3 PASM-194 10.5 30.5
4
PMU PASM-194 BPE-194 10.2 30.0
5 BPE-194 PASM-194 10.7 30.2
6 PASM-194 BPE-3000 10.1 28.4
7 paraCTC BPE-3000 9.8 28.4
8 paCTC BPE-3000 9.7 28.4
9 paCTC-s BPE-3000 9.7 28.3
10 paCTC-us BPE-3000 9.6 28.6

Chúng tôi đầu tiên kiểm tra các phương pháp được đề xuất trên các bộ kiểm tra clean và other của nhiệm vụ ASR Librispeech. Kết quả được hiển thị trong Bảng 1. Hệ thống 1 đến 3 là các baseline ConformerT của chúng tôi, mỗi hệ thống với cả nhánh CTC và transducer sử dụng một loại đơn vị mục tiêu duy nhất. 'BPE/PASM-*' có nghĩa là sử dụng đơn vị BPE hoặc PASM với kích thước từ vựng khác nhau. Trong các thí nghiệm rộng rãi của chúng tôi, chúng tôi thấy 194 và 3000 là các thiết lập tốt nhất cho PASM và BPE trên bộ dữ liệu Libri-100hrs tương ứng. 'BPE-194' được sử dụng để so sánh công bằng với 'PASM-194'. Hệ thống 4-10 là các mô hình ConformerT được huấn luyện sử dụng khung PMU được đề xuất của chúng tôi với ba biến thể khác nhau: PMU cơ bản (hệ thống 4-6), PMU với paraCTC (hệ thống 7) và PMU với cấu trúc khác nhau của paCTC (hệ thống 8-10).

1http://www.speech.cs.cmu.edu/cgi-bin/cmudict

--- TRANG 4 ---

Bảng 2: WER(%) trên các bộ kiểm tra trong miền và ngoài miền trên nhiệm vụ ASR CommonVoice có giọng. Trong paCTC, thiết lập 9-10 sử dụng cấu trúc tùy chọn, trong khi thiết lập 7-8 thì không.

ID Phương pháp TUctc TUtrans Trong miền Ngoài miền Tổng thể
Anh Ấn Độ Mỹ Úc Canada Singapore
1
ConformerT BPE-3000 21.9 26.8 18.7 24.1 17.1 35.4 24.4
2 BPE-205 21.7 25.0 17.6 24.5 16.2 33.8 23.4
3 PASM-205 21.6 24.7 17.8 24.6 16.6 35.1 23.9
4
PMU PASM-205 BPE-205 21.4 24.7 17.8 23.7 16.3 33.7 23.2
5 BPE-205 PASM-205 21.9 24.9 17.8 24.4 16.1 33.7 23.4
6 paraCTC BPE-205 21.3 24.7 17.3 24.0 16.2 33.1 23.1
7 paCTC BPE-205 20.5 24.1 16.5 23.3 15.3 32.6 22.4
8 paCTC BPE-3000 19.6 24.0 16.6 22.5 15.2 32.3 22.1
9 paCTC-s BPE-3000 19.9 23.6 16.4 22.0 15.4 31.3 21.8
10 paCTC-us BPE-3000 19.8 23.1 15.8 22.5 14.8 31.4 21.6

So sánh kết quả của hệ thống 1-3 trong Bảng 1, chúng ta thấy không có sự khác biệt hiệu suất lớn giữa việc sử dụng PASM cảm ứng ngữ âm và BPE cảm ứng văn bản như các đơn vị mục tiêu CTC/transducer của chúng. PASM đạt kết quả tốt nhất trên bộ kiểm tra clean, trong khi BPE đạt kết quả tốt nhất trên bộ kiểm tra other. Tuy nhiên, khi các phương pháp mô hình PMU được đề xuất được áp dụng, cả WER trên bộ kiểm tra clean và other đều được giảm đáng kể. Khi so sánh kết quả của ConformerT với BPE-3000 truyền thống (hệ thống 2), ngay cả với PMU cơ bản, hệ thống 6 vẫn đạt được giảm WER tương đối 8.2% và 5.0% trên bộ clean và other tương ứng. Đồng thời, bằng cách so sánh hệ thống 4 đến 6, rõ ràng rằng sử dụng PASM như căn chỉnh CTC, trong khi các đơn vị BPE lớn hơn như mục tiêu transducer là thiết lập tốt nhất cho PMU cơ bản, điều này có thể do thực tế rằng, sự tương ứng ngữ âm rõ ràng của các đơn vị mục tiêu là quan trọng đối với mô hình đồng bộ thời gian như vậy. Khi so sánh hệ thống 6 với 7-10, chúng ta thấy giảm WER liên tục trên bộ kiểm tra clean, ngay cả việc cải thiện hiệu suất trên bộ other bị hạn chế. Cuối cùng, paCTC-us đạt kết quả tốt nhất trên bộ kiểm tra clean. So với baseline tốt nhất (hệ thống 2), hệ thống 10 đạt được giảm WER tương đối 12.7% và 4.3% trên bộ kiểm tra clean và other tương ứng.

Bảng 3: WER(%) trên bộ kiểm tra clean và other Libri-100hrs cho PMU với paCTC (Hình 1 (b), Eq.(6)) không có cấu trúc tùy chọn dưới các điều kiện thiết lập khác nhau. Thiết lập 4 có nghĩa là thay thế PASM-CTC bằng BPE-194 CTC tại các lớp N1.

ID #lớp β Đánh giá
N1 N3 Clean Other
1 6 6 0.3 9.8 28.0
2 6 6 0.5 9.7 28.4
3 6 6 0.7 10.0 28.6
4 6 6 0.5, BPE-194 10.0 29.4
5 3 9 0.5 10.3 30.1
6 9 3 0.5 9.8 28.6

Trên thực tế, trước khi chúng tôi đề xuất paCTC với cấu trúc tùy chọn, chúng tôi thực hiện một tập hợp các thí nghiệm điều chỉnh tham số để xem chúng ảnh hưởng đến hiệu suất paCTC như thế nào. Kết quả được hiển thị trong Bảng 3. Thiết lập 1-3, 5-6 đều với PASM-194 tại khối AEncoder N1, và BPE-3000 tại khối trên. Chúng ta thấy rằng, N1=N3= 6 với β= 0.5 đạt kết quả tương đối ổn định. Hơn nữa, khi chúng tôi thay thế PASM-194 bằng BPE-194 để căn chỉnh các đầu ra lớp N1 đầu tiên, nó đạt WER tệ hơn so với thiết lập 2, tuy nhiên, khi chúng tôi so sánh nó với hệ thống 2 trong Bảng 1, chúng ta vẫn thấy cải thiện hiệu suất. Điều này cho chúng ta biết rằng PASM phù hợp hơn cho việc học thông tin âm thanh cấp thấp so với BPE, và việc giới thiệu học biểu diễn tiến bộ từ đơn vị mục tiêu nhỏ đến lớn sẽ hữu ích. Tất cả những quan sát này dẫn chúng tôi đề xuất toàn bộ cấu trúc paCTC được hiển thị trong Hình 1(b).

4.3.2. Kết quả trên ASR có giọng

Trong Bảng 2, hiệu quả của PMU với các biến thể khác nhau được kiểm tra trên nhiệm vụ ASR có giọng CommonVoice. Khác với nhiệm vụ Librispeech, chúng tôi thấy kích thước từ vựng tốt nhất của cả baseline PASM và BPE là 205, kích thước BPE lớn hơn không dẫn đến WER tốt hơn dưới ConformerT với các đơn vị mục tiêu loại đơn. Và nhất quán với các phát hiện trong Bảng 1, cả hiệu suất trong miền và ngoài miền đều được giảm liên tục bởi các phương pháp PMU được đề xuất, chẳng hạn như, hệ thống 4 hoạt động tốt hơn 5 bởi vì PASM được áp dụng trên CTC và trong khi BPE được áp dụng trên transducer; paraCTC đạt kết quả tốt hơn PMU cơ bản, và paCTC vượt trội đáng kể so với hai biến thể PMU khác, đặc biệt trên ba bộ kiểm tra trong miền. Đáng chú ý rằng, trong paCTC, cả BPE-CTC trung gian và PASM-CTC đều có cùng kích thước từ vựng 205. Bằng cách so sánh hệ thống 7 với 8, kết quả cũng chứng minh rằng việc giới thiệu học tiến bộ với các đơn vị mục tiêu từ nhỏ đến lớn hơn là hữu ích. Cuối cùng, paCTC với cấu trúc tùy chọn đạt WER tổng thể tốt nhất, so với baseline ConformerT tốt nhất hệ thống 2, hệ thống 10 tạo ra giảm WER tương đối 7.7% tổng thể trên các bộ kiểm tra ASR có giọng này. Cụ thể, giảm WER tương đối 8.8%, 7.6% và 10.2% cho các bộ kiểm tra trong miền Anh, Ấn Độ và Mỹ, giảm WER tương đối 8.2%, 8.6% và 7.1% cho bộ kiểm tra ngoài miền Úc, Canada và Singapore tương ứng.

5. Kết luận

Trong nghiên cứu này, chúng tôi đề xuất phương pháp mô hình đa-đơn vị mục tiêu hỗ trợ ngữ âm (PMU), để khai thác hiệu quả cả mô hình đơn vị mục tiêu PASM cảm ứng ngữ âm và BPE cảm ứng văn bản truyền thống để cải thiện hệ thống ASR đầu cuối đến cuối Conformer-Transducer tiên tiến. Ba cấu trúc PMU được đề xuất với việc thực hiện khác nhau của mô hình CTC/transducer đa mục tiêu, bao gồm PMU cơ bản với PASM và BPE được áp dụng cho CTC và transducer riêng biệt, PMU với paraCTC trong đó các đơn vị PASM và BPE cũng được tích hợp theo cách song song như các đơn vị mục tiêu của CTC, và PMU với paCTC sử dụng các đơn vị BPE có điều kiện trên PASM CTC theo cách học biểu diễn tiến bộ. Kết quả trên cả nhiệm vụ ASR LibriSpeech và tiếng Anh có giọng cho thấy PMU được đề xuất có thể vượt trội đáng kể so với hệ thống ASR E2E Conformer-Transducer dựa trên BPE truyền thống.

--- TRANG 5 ---

6. Tài liệu tham khảo

[1] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., "Conformer: Convolution-augmented transformer for speech recognition," in Proc. Interspeech, 2020, pp. 5036–5040.

[2] W. Huang, W. Hu, Y. T. Yeung, and X. Chen, "Conv-transformer transducer: Low latency, low frame rate, streamable end-to-end speech recognition," in Proc. Interspeech, 2020, pp. 5001–5005.

[3] J. Li, Y. Wu, Y. Gaur, C. Wang, R. Zhao, and S. Liu, "On the comparison of popular end-to-end models for large scale speech recognition," in Proc. Interspeech, 2020, pp. 1–5.

[4] F. Boyer, Y. Shinohara, T. Ishii, H. Inaguma, and S. Watanabe, "A study of transducer based end-to-end ASR with ESPnet: Architecture, auxiliary loss and decoding strategies," in IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 16–23.

[5] W. Zou, D. Jiang, S. Zhao, G. Yang, and X. Li, "Comparable study of modeling units for end-to-end mandarin speech recognition," in Proc. ISCSLP, 2018, pp. 369–373.

[6] S. Zhou, L. Dong, S. Xu, and B. Xu, "A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese," in Neural Information Processing, 2018, pp. 210–220.

[7] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., "State-of-the-art speech recognition with sequence-to-sequence models," in Proc. ICASSP, 2018, pp. 4774–4778.

[8] J. Li, G. Ye, A. Das, R. Zhao, and Y. Gong, "Advancing acoustic-to-word CTC model," in Proc. ICASSP, 2018, pp. 5794–5798.

[9] A. Zeyer, K. Irie, R. Schlüter, and H. Ney, "Improved training of end-to-end attention models for speech recognition," in Proc. Interspeech, 2018, pp. 7–11.

[10] K. Rao, H. Sak, and R. Prabhavalkar, "Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer," in IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017, pp. 193–199.

[11] R. Sennrich, B. Haddow, and A. Birch, "Neural machine translation of rare words with subword units," in Proc. ACL, 2016, pp. 1715–1725.

[12] M. Schuster and K. Nakajima, "Japanese and Korean voice search," in Proc. ICASSP, 2012, pp. 5149–5152.

[13] T. Kudo, "Subword regularization: Improving neural network translation models with multiple subword candidates," in Proc. ACL, 2018, pp. 66–75.

[14] W. Wang, G. Wang, A. Bhatnagar, Y. Zhou, C. Xiong, and R. Socher, "An investigation of phone-based subword units for end-to-end speech recognition," in Proc. Interspeech, 2020, pp. 1778–1782.

[15] M. Zeineldeen, A. Zeyer, W. Zhou, T. Ng, R. Schlüter, and H. Ney, "A systematic comparison of grapheme-based vs. phoneme-based label units for encoder-decoder-attention models," arXiv preprint arXiv:2005.09336, 2020.

[16] S. Chen, X. Hu, S. Li, and X. Xu, "An investigation of using hybrid modeling units for improving end-to-end speech recognition system," in Proc. ICASSP, 2021, pp. 6743–6747.

[17] H. Xu, S. Ding, and S. Watanabe, "Improving end-to-end speech recognition with pronunciation-assisted sub-word modeling," in Proc. ICASSP, 2019, pp. 7110–7114.

[18] Y. Higuchi, K. Karube, T. Ogawa, and T. Kobayashi, "Hierarchical conditional end-to-end ASR with CTC and multi-granular subword units," in Proc. ICASSP, 2022, pp. 7797–7801.

[19] A. Graves, "Sequence transduction with recurrent neural networks," in Proc. ICML, 2012.

[20] J.-J. Jeon and E. Kim, "Multitask learning and joint optimization for transformer-RNN-transducer speech recognition," in Proc. ICASSP, 2021, pp. 6793–6797.

[21] A. Graves, S. Fernández, F. Gomez, and J. Schmidhuber, "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in Proc. ICML, 2006, pp. 369–376.

[22] R. Sanabria and F. Metze, "Hierarchical multitask learning with CTC," in Proc. SLT, 2018, pp. 485–490.

[23] J. Li, G. Ye, R. Zhao, J. Droppo, and Y. Gong, "Acoustic-to-word model without OOV," in IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017, pp. 111–117.

[24] J. Kremer, L. Borgholt, and L. Maaløe, "On the inductive bias of word-character-level multi-task learning for speech recognition," arXiv preprint arXiv:1812.02308, 2018.

[25] A. Heba, T. Pellegrini, J.-P. Lorré, and R. Andre-Obrecht, "Char+ CV-CTC: combining graphemes and consonant/vowel units for CTC-based ASR using multitask learning," in Proc. Interspeech, 2019, pp. 1611–1615.

[26] J. Nozaki and T. Komatsu, "Relaxing the conditional independence assumption of CTC-based ASR by conditioning on intermediate predictions," in Proc. Interspeech, 2021, pp. 3735–3739.

[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, "LibriSpeech: an asr corpus based on public domain audio books," in Proc. ICASSP, 2015, pp. 5206–5210.

[28] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, "Common Voice: A massively-multilingual speech corpus," in Proc. LREC, 2020, pp. 4218–4222.

[29] S. Watanabe, T. Hori, S. Karita, T. Hayashi et al., "ESPnet: End-to-end speech processing toolkit," in Proc. Interspeech, 2018, pp. 2207–2211.

[30] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, "Rethinking the inception architecture for computer vision," in Proc. CVPR, 2016, pp. 2818–2826.

[31] T. Kudo and J. Richardson, "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," in Proc. The Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2018, pp. 66–71.

[32] C. Dyer, V. Chahuneau, and N. A. Smith, "A simple, fast, and effective reparameterization of IBM model 2," in Proc. NAACL, 2013, pp. 644–648.
