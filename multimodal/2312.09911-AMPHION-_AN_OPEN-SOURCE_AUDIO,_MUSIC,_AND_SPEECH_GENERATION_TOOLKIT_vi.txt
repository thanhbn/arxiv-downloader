# 2312.09911.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.09911.pdf
# Kích thước tệp: 500869 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
AMPHION: BỘ CÔNG CỤ MÃ NGUỒN MỞ SINH AUDIO, NHẠC VÀ GIỌNG NÓI
Xueyao Zhang1,⋆Liumeng Xue1,⋆Yicheng Gu1,⋆Yuancheng Wang1,⋆Jiaqi Li1,⋆
Haorui He3Chaoren Wang1Songting Liu3Xi Chen1Junan Zhang2
Zihao Fang1Haopeng Chen1Tze Ying Tang1Lexiao Zou3Mingxuan Wang1
Jun Han1Kai Chen2Haizhou Li1Zhizheng Wu1,2,3,‡
1Đại học Trung Hoa Hồng Kông, Thâm Quyến, Trung Quốc
2Phòng thí nghiệm AI Thượng Hải, Thượng Hải, Trung Quốc
3Viện Nghiên cứu Big Data Thâm Quyến, Thâm Quyến, Trung Quốc
TÓM TẮT
Amphion là một bộ công cụ mã nguồn mở cho Sinh Audio, Nhạc và Giọng nói, nhằm mục đích giúp các nhà nghiên cứu và kỹ sư mới dễ dàng tiếp cận các lĩnh vực này. Nó trình bày một khung thống nhất bao gồm các tác vụ và mô hình sinh đa dạng, với phần thưởng bổ sung là có thể dễ dàng mở rộng cho việc tích hợp mới. Bộ công cụ được thiết kế với quy trình làm việc thân thiện với người mới bắt đầu và các mô hình đã được huấn luyện trước, cho phép cả người mới bắt đầu và các nhà nghiên cứu có kinh nghiệm khởi động dự án của họ một cách tương đối dễ dàng. Phiên bản phát hành đầu tiên của Amphion v0.1 hỗ trợ một loạt các tác vụ bao gồm Text to Speech (TTS), Text to Audio (TTA), và Singing Voice Conversion (SVC), được bổ sung bởi các thành phần thiết yếu như tiền xử lý dữ liệu, vocoder tiên tiến và các chỉ số đánh giá. Bài báo này trình bày một cái nhìn tổng quan cấp cao về Amphion. Amphion được mã nguồn mở tại https://github.com/open-mmlab/Amphion .
Từ khóa chỉ mục —Sinh giọng nói, sinh audio, sinh nhạc, vocoder, phần mềm mã nguồn mở, bộ công cụ audio
1. GIỚI THIỆU
Sự phát triển của học sâu đã cải thiện đáng kể hiệu suất của các mô hình sinh. Tận dụng các mô hình này đã cho phép các nhà nghiên cứu và thực hành khám phá các khả năng đổi mới, dẫn đến những đột phá đáng chú ý trong nhiều lĩnh vực khác nhau, bao gồm thị giác máy tính và xử lý ngôn ngữ tự nhiên. Tiềm năng trong các tác vụ liên quan đến sinh audio, nhạc và giọng nói đã thúc đẩy cộng đồng khoa học tích cực công bố các mô hình và ý tưởng mới [1, 2].
Có sự hiện diện ngày càng tăng của cả các kho lưu trữ mã nguồn mở chính thức và do cộng đồng điều hành mà sao chép các mô hình này. Tuy nhiên, chất lượng của các kho lưu trữ thay đổi, và chúng thường bị phân tán, tập trung vào các bài báo cụ thể. Những kho lưu trữ phân tán này tạo ra nhiều trở ngại cho các nhà nghiên cứu hoặc kỹ sư mới mới tiếp cận khu vực nghiên cứu. Đầu tiên, các nỗ lực sao chép một thuật toán sử dụng các triển khai hoặc cấu hình khác nhau có thể dẫn đến chức năng hoặc hiệu suất mô hình không nhất quán. Thứ hai, trong khi nhiều kho lưu trữ tập trung vào kiến trúc mô hình, chúng thường bỏ qua các bước quan trọng như tiền xử lý dữ liệu chi tiết, trích xuất đặc trưng, huấn luyện mô hình và đánh giá có hệ thống. Việc thiếu hướng dẫn có hệ thống này tạo ra những thách thức đáng kể cho người mới bắt đầu, những người có thể có chuyên môn kỹ thuật và kinh nghiệm hạn chế trong việc huấn luyện các mô hình quy mô lớn. Tóm lại, tính chất phân tán của các kho lưu trữ này cản trở các nỗ lực hướng tới nghiên cứu có thể tái tạo và so sánh công bằng giữa các mô hình hoặc thuật toán.
Được thúc đẩy bởi điều đó, chúng tôi giới thiệu Amphion, một nền tảng mã nguồn mở dành riêng cho mục tiêu sao bắc của "Bất kỳ thứ gì sang Audio" (Hình 1). Các tính năng của Amphion được tóm tắt như sau:
•Khung thống nhất : Amphion cung cấp một khung thống nhất cho việc sinh và đánh giá audio, nhạc và giọng nói. Nó được thiết kế để có thể thích ứng, linh hoạt và có thể mở rộng, hỗ trợ tích hợp các mô hình mới.
•Quy trình làm việc thân thiện với người mới bắt đầu : Amphion cung cấp quy trình làm việc thân thiện với người mới bắt đầu với tài liệu và hướng dẫn đơn giản. Nó thiết lập chính nó như một nền tảng nghiên cứu một cửa dễ tiếp cận phù hợp cho cả người mới và nhà nghiên cứu có kinh nghiệm.
•Mô hình đã được huấn luyện trước chất lượng cao : Để thúc đẩy nghiên cứu có thể tái tạo, Amphion cam kết phát hành các mô hình đã được huấn luyện trước chất lượng cao. Trong đối tác với ngành công nghiệp, chúng tôi nhằm mục đích làm cho các mô hình đã được huấn luyện trước quy mô lớn có sẵn rộng rãi cho các ứng dụng khác nhau.arXiv:2312.09911v3  [cs.SD]  16 Sep 2024

--- TRANG 2 ---
Bộ công cụ Amphion v0.1¹, hiện có sẵn dưới giấy phép MIT, đã hỗ trợ một loạt các tác vụ sinh đa dạng. Bài báo này trình bày một cái nhìn tổng quan cấp cao về bộ công cụ.
2. KHUNG AMPHION
Mục tiêu sao bắc của Amphion là thống nhất các tác vụ sinh dạng sóng âm thanh khác nhau. Từ quan điểm đầu vào, chúng tôi xây dựng các tác vụ sinh audio thành ba loại,
1.Văn bản sang Dạng sóng : Đầu vào bao gồm các token văn bản rời rạc, chúng ràng buộc chặt chẽ nội dung của dạng sóng đầu ra. Các tác vụ đại diện bao gồm Text to Speech (TTS) và Singing Voice Synthesis (SVS)².
2.Văn bản Mô tả sang Dạng sóng : Đầu vào bao gồm các token văn bản rời rạc, chúng thường hướng dẫn nội dung hoặc phong cách của dạng sóng đầu ra. Các tác vụ đại diện bao gồm Text to Audio (TTA) và Text to Music (TTM).
3.Dạng sóng sang Dạng sóng : Cả đầu vào và đầu ra đều là tín hiệu dạng sóng liên tục. Các tác vụ đại diện bao gồm Voice Conversion (VC), Singing Voice Conversion (SVC), Emotion Conversion (EC), Accent Conversion (AC), và Speech Translation (ST).
2.1. Thiết kế Kiến trúc Hệ thống
Amphion được thiết kế để là một khung đơn hỗ trợ sinh audio, nhạc và giọng nói. Thiết kế kiến trúc hệ thống của nó được trình bày trong Hình 2. Từ dưới lên,
1. Xử lý dữ liệu ( Dataset ,Feature Extractor ,Sampler, và DataLoader ), thuật toán tối ưu hóa ( Optimizer ,Scheduler , và Trainer ), và các mô-đun mạng chung ( Module ) là những khối xây dựng được chia sẻ cho tất cả các tác vụ sinh audio.
2. Đối với mỗi tác vụ sinh cụ thể, chúng tôi thống nhất việc sử dụng dữ liệu/đặc trưng ( TaskLoader ), khung tác vụ ( TaskFramework ), và pipeline huấn luyện ( TaskTrainer ) của nó.
3. Trong mỗi tác vụ sinh, đối với mỗi mô hình cụ thể, chúng tôi xác định kiến trúc ( ModelArchitecture ) và pipeline huấn luyện ( ModelTrainer ) của nó.
4. Cuối cùng, chúng tôi cung cấp một công thức cho mỗi mô hình cho người dùng. Bên cạnh các mô hình đã được huấn luyện trước, chúng tôi cũng cung cấp các demo tương tác để người dùng khám phá. Amphion cũng có các hình ảnh hóa giáo dục về các mô hình máy học. Độc giả quan tâm có thể tham khảo [3] để có mô tả chi tiết.
¹https://github.com/open-mmlab/Amphion
²Đầu vào của SVS cũng có thể là các token âm nhạc như nốt MIDI. Chúng cũng rời rạc và hoạt động như đầu vào văn bản.
Hình 2 : Thiết kế kiến trúc hệ thống của Amphion.
2.2. Hỗ trợ Tác vụ Sinh Audio
Bộ công cụ Amphion v0.1 bao gồm một đại diện từ mỗi loại tác vụ sinh ba loại (cụ thể là TTS, TTA, và SVC) để tích hợp. Điều này đảm bảo rằng khung của Amphion có thể được thích ứng một cách thuận tiện với các tác vụ sinh audio khác trong quá trình phát triển tương lai.
Cụ thể, các pipeline của các tác vụ audio khác nhau được thiết kế như sau:
•Text to Speech : TTS nhằm chuyển đổi văn bản viết thành lời nói. TTS đa người nói thông thường chỉ được huấn luyện với các tập dữ liệu ít người nói được tuyển chọn cẩn thận và chỉ tạo ra lời nói từ nhóm người nói [1, 4]. Gần đây, TTS zero-shot thu hút nhiều sự chú ý từ cộng đồng nghiên cứu. Ngoài văn bản, TTS zero-shot yêu cầu một audio tham chiếu như một gợi ý. Bằng cách sử dụng các kỹ thuật học trong ngữ cảnh, nó có thể bắt chước âm sắc và phong cách nói của audio tham chiếu [2, 5].
•Text to Audio : TTA nhằm tạo ra âm thanh có ngữ nghĩa phù hợp với các mô tả. Nó thường yêu cầu một bộ mã hóa văn bản đã được huấn luyện trước để nắm bắt thông tin toàn cục của văn bản mô tả đầu vào, sau đó sử dụng một mô hình âm thanh, chẳng hạn như mô hình khuếch tán [6, 7], để tổng hợp các đặc trưng âm thanh.
•Singing Voice Conversion : SVC nhằm chuyển đổi giọng nói của tín hiệu hát thành giọng nói của một ca sĩ mục tiêu trong khi bảo tồn lời bài hát và giai điệu. Để trao quyền cho người nói tham chiếu hát audio nguồn, ý tưởng chính là trích xuất các biểu diễn cụ thể của người nói từ tham chiếu, trích xuất các biểu diễn không phụ thuộc vào người nói (bao gồm các đặc trưng ngữ nghĩa và giai điệu) từ nguồn, và sau đó tổng hợp các đặc trưng đã chuyển đổi sử dụng các mô hình âm thanh [8].

--- TRANG 3 ---
Bảng 1 : Tác vụ, mô hình và chỉ số được hỗ trợ trong Amphion v0.1.
Tác vụ, Mô hình Đã được huấn luyện trước Mở và Thuật toán Chỉ số Đánh giá
Text to Speech
•Dựa trên Transformer : FastSpeech 2 [1]
•Dựa trên Flow : VITS [4]
•Dựa trên Diffusion : NaturalSpeech 2 [2]
•Dựa trên Autoregressive : VALL-E [5]
Singing Voice Conversion
•Dựa trên Transformer : TransformerSVC
•Dựa trên Flow : VitsSVC
•Dựa trên Diffusion : DiffWaveNetSVC [8],
DiffComoSVC [9]
Text to Audio
• AudioLDM [6], PicoAudio [10]Vocoder
•Dựa trên Autoregressive : WaveNet [11],
WaveRNN [12]
•Dựa trên Diffusion : DiffWave [13]
•Dựa trên Flow : WaveGlow [14]
•Dựa trên GAN :
◦Generators : MelGAN [15], HiFi-
GAN [16], NSF-HiFiGAN [17], BigV-
GAN [18], APNet [19]
◦Discriminators : MSD [15],
MPD [16], MRD [20], MS-
STFTD [21], MS-SB-CQTD [22, 23]
Codec
• FACodec [24]•Mô hình hóa F0 : Hệ số
Pearson F0 (FPC), Điểm
F1 Có giọng/Không giọng
(V/UV F1), v.v.
•Biến dạng Spectrogram :
PESQ, STOI, FAD, MCD,
SI-SNR, SI-SDR
•Khả năng hiểu : Tỷ lệ
Lỗi Từ (WER) và Tỷ lệ
Lỗi Ký tự (CER)
•Độ tương tự Người nói :
Độ tương tự Cosine,
Resemblyzer, và WavLM.
Đáng chú ý, hầu hết các mô hình sinh audio thường áp dụng một quy trình sinh hai giai đoạn, trong đó chúng tạo ra một số đặc trưng âm thanh trung gian (ví dụ: Mel Spectrogram) trong giai đoạn đầu tiên, và sau đó tạo ra dạng sóng âm thanh cuối cùng sử dụng một vocoder hoặc codec audio trong giai đoạn thứ hai. Được thúc đẩy bởi điều đó, Amphion v0.1 cũng tích hợp nhiều mô hình vocoder và codec audio khác nhau. Tóm tắt về các tác vụ, mô hình và thuật toán hiện được hỗ trợ của Amphion được trình bày trong Bảng 1.
2.3. Mô hình Đã được huấn luyện trước Mở
Amphion đã phát hành nhiều mô hình cho TTS, TTA, SVC, và Vocoder.
Bảng 2 : Mô hình đã được huấn luyện trước được hỗ trợ trong Amphion v0.1.
Tác vụ Mô hình Kiến trúc Số tham số Tập huấn luyện
TTSVITS [4] 30M HiFi-TTS [25]
VALL-E [5] 250M MLS [26]
NaturalSpeech2 [2] 201M Libri-light [27]
TTA AudioLDM [7] 710M AudioCaps [28]
SVC DiffWaveNetSVC [8] 31M Hỗn hợp (xem Phần 3.3)
Codec FACodec [24] 140M Libri-light [27]
VocoderHiFi-GAN [16] 24M LibriTTS [29]
BigVGAN [18] 112M LibriTTS [29]
2.4. So sánh với Bộ công cụ Audio khác
Chúng tôi khảo sát một danh sách các bộ công cụ sinh audio, nhạc và giọng nói mã nguồn mở đại diện trong Bảng 3. So sánh các hệ thống này, chúng tôi thấy rằng Amphion có hỗ trợ tác vụ sinh audio toàn diện, bao gồm tổng hợp audio chung, tổng hợp nhạc/hát, TTS zero-shot và đa người nói. Từ các so sánh không chính thức, chúng tôi cũng thấy rằng Amphion thân thiện với người mới bắt đầu hơn với nhiều demo trực tuyến có thể truy cập hơn nhiều bộ công cụ khác³.
Bảng 3 : Bộ công cụ mã nguồn mở đại diện liên quan đến sinh audio, nhạc và giọng nói (sắp xếp theo thứ tự bảng chữ cái). Mỗi tên kho lưu trữ có một siêu liên kết tới nguồn web của nó.
Bộ công cụ Audio Nhạc/Hát TTS Zero-Shot TTS Đa người nói
Amphion ✓ ✓ ✓ ✓
AudioCraft ✓ ✓
Bark ✓ ✓ ✓
Coqui TTS ✓
EmotiVoice ✓
ESPnet ✓ ✓
Merlin ✓
Mocking Bird ✓
Muskits ✓
Muzic ✓
OpenVoice ✓
PaddleSpeech ✓ ✓ ✓
SoftVC VITS ✓
SpeechBrain ✓
TorToiSe ✓
WeTTS ✓
3. THỰC NGHIỆM
Trong phần này, chúng tôi so sánh hiệu suất của các mô hình được huấn luyện với khung Amphion v0.1 với các kho lưu trữ mở công cộng hoặc kết quả từ các bài báo học thuật gốc. Chúng tôi cũng đề cập ngắn gọn đến cấu hình huấn luyện của các mô hình đã được huấn luyện trước trong Amphion v0.1. Chúng tôi khuyến khích độc giả quan tâm tìm thêm thông tin trong kho lưu trữ của chúng tôi.
Chúng tôi sử dụng cả đánh giá khách quan và chủ quan để đánh giá. Các chỉ số đánh giá khách quan sẽ được xác định trong mỗi tác vụ. Đối với điểm chủ quan, bao gồm điểm Ý kiến Trung bình (MOS) về tính tự nhiên và điểm Ý kiến Trung bình Độ tương tự (SMOS), 10 người nghe có kinh nghiệm trong lĩnh vực này được yêu cầu chấm điểm từ 1 ("Kém") đến 5 ("Xuất sắc") trên 10 mẫu audio được chọn ngẫu nhiên trên mỗi điều kiện, để đánh giá chất lượng tổng thể và độ tương tự với người nói tham chiếu của mỗi audio.
3.1. Text to Speech
Bảng 4 : Kết quả đánh giá TTS đa người nói trong Amphion v0.1.
Hệ thống CER ↓ WER↓ FAD↓ MOS↑
Coqui TTS (VITS) 0.06 0.12 0.54 3.69
SpeechBrain (FastSpeech 2) 0.06 0.11 1.71 3.54
TorToiSe TTS 0.05 0.09 1.90 3.61
ESPnet (VITS) 0.07 0.11 1.28 3.57
Amphion v0.1 (VITS) 0.06 0.10 0.84 3.61
3.1.1. Kết quả TTS Đa người nói
Đối với TTS đa người nói, chúng tôi so sánh Amphion v0.1 với bốn bộ công cụ tổng hợp lời nói mã nguồn mở phổ biến khác, bao gồm Coqui TTS⁴, SpeechBrain⁵, TorToiSe⁶, và ESPnet⁷. Đối với mỗi hệ thống mã nguồn mở, chúng tôi chọn mô hình đa người nói có hiệu suất tốt nhất để so sánh. VITS [4] được chọn cho Coqui TTS, ESPnet và Amphion, và FastSpeech 2 [1] được chọn trong SpeechBrain, và mô hình TorToiSe TTS từ kho lưu trữ của nó. Chúng tôi đánh giá trên 100 phiên âm văn bản và sau đó tạo ra lời nói tương ứng sử dụng mỗi hệ thống. Kết quả được hiển thị trong Bảng 4, cho thấy rằng mô hình TTS đa người nói VITS được phát hành trong Amphion v0.1 có thể so sánh với các hệ thống mã nguồn mở hiện có.
3.1.2. Kết quả TTS Zero-Shot
Bảng 5 : Kết quả đánh giá tiếp tục của hệ thống TTS zero-shot VALL-E trong Amphion v0.1.
Hệ thống Tập dữ liệu huấn luyện Tập dữ liệu kiểm tra SIM-O ↑WER ↓
Proprietary (VALL-E) LibrilightLibrispeech0.51 0.038test-clean (4-10s)
Amphion v0.1(VALL-E) MLS (10-20s)Librispeech0.51 0.034test-clean (10-20s)
Đối với TTS zero-shot, chúng tôi so sánh mô hình VALL-E [5] được phát hành trong Amphion v0.1 với kết quả mô hình độc quyền từ bài báo chính thức. Chúng tôi kiểm tra điểm độ tương tự người nói khách quan SIM-O, và WER (Tỷ lệ Lỗi Từ), sử dụng cùng các công cụ đánh giá như bài báo chính thức [5]. Đối với SIM-O, chúng tôi sử dụng mô hình WavLM-TDNN⁸ để trích xuất các đặc trưng xác minh người nói, và sử dụng hệ thống ASR Hubert-large [30] để phiên âm lời nói. Vì tập huấn luyện của chúng tôi chỉ chứa lời nói 10-20s, chúng tôi kiểm tra kết quả trên một khoảng thời gian phù hợp của LibriSpeech test-clean (10-20s). Chúng tôi kiểm tra trong một thiết lập tiếp tục theo bài báo VALL-E [5], trong đó mô hình được đưa tiền tố 3 giây từ phát biểu thực tế và được yêu cầu tiếp tục lời nói. Kết quả cho thấy rằng trong một kịch bản khoảng thời gian huấn luyện-kiểm tra phù hợp, mô hình của chúng tôi đạt được kết quả SIM-O và WER có thể so sánh với bài báo chính thức.
Đối với huấn luyện mô hình, chúng tôi sử dụng tập dữ liệu MLS [26] chứa 45k giờ lời nói, gần với 60k giờ dữ liệu Libri-Light cho mô hình chính thức. Đáng chú ý, mô hình VALL-E được phát hành của chúng tôi đã sử dụng nhiều dữ liệu huấn luyện hơn các mô hình mã nguồn mở hiện có⁹, thường được huấn luyện trên hàng trăm giờ dữ liệu.
3.2. Text to Audio
Bảng 6 : Kết quả đánh giá Text to Audio trong Amphion v0.1.
Hệ thống FD ↓ IS↑ KL↓
Text-to-sound-synthesis (Diffsound) 47.68 4.01 2.52
AudioLDM (AudioLDM) 27.12 7.51 1.86
Amphion v0.1 (AudioLDM) 20.47 8.78 1.44
Chúng tôi so sánh các mô hình TTA trong các kho lưu trữ khác nhau: Kho lưu trữ Text-to-sound-synthesis¹⁰ với mô hình DiffSound [31], kho lưu trữ AudioLDM [6] chính thức¹¹, và mô hình AudioLDM được tái tạo sử dụng cơ sở hạ tầng của Amphion.
Để đánh giá mô hình text-to-audio của chúng tôi, chúng tôi sử dụng điểm inception (IS), Khoảng cách Fréchet (FD), và Phân kỳ Kullback–Leibler (KL). FD, IS, và KL dựa trên mô hình phân loại audio tiên tiến PANNs [32]. Chúng tôi sử dụng tập kiểm tra của AudioCaps làm tập kiểm tra của chúng tôi. Kết quả đánh giá TTA Amphion v0.1 được hiển thị trong Bảng 6. Kết quả cho thấy rằng hệ thống TTA Amphion v0.1 đạt được kết quả vượt trội so với các mô hình mã nguồn mở hiện có.
3.3. Singing Voice Conversion
Chúng tôi so sánh hệ thống SVC trong Amphion v0.1 ([8]) với bộ công cụ SoftVC¹². Để huấn luyện mô hình SVC của chúng tôi, chúng tôi sử dụng một loạt tập dữ liệu: Opencpop [33], dữ liệu huấn luyện SVCC¹³, VCTK¹⁴, OpenSinger [34], và M4Singer [35]. Có tổng cộng 83.1 giờ lời nói và 87.2 giờ dữ liệu hát.
Để đánh giá các mô hình, chúng tôi áp dụng tác vụ đánh giá trong miền của Thách thức Chuyển đổi Giọng nói Hát (SVCC) 2023¹³ với 48 phát biểu hát dưới kiểm tra. Tác vụ là chuyển đổi mỗi phát biểu hát thành hai ca sĩ mục tiêu (một nam và một nữ). Kết quả cho thấy rằng mô hình SVC Amphion v0.1 có hiệu suất tốt hơn trong cả tính tự nhiên và độ tương tự người nói so với SoftVC, và thu hẹp khoảng cách với các phát biểu thực tế.

--- TRANG 4 ---
Bảng 7 : Kết quả đánh giá Singing Voice Conversion trong Amphion v0.1.
Hệ thống MOS ↑SMOS ↑
Thực tế 4.67 3.96
SoftVC (VITS) 2.98 3.43
Amphion v0.1 (DiffWaveNetSVC) 3.52 3.69
Bảng 8 : Kết quả đánh giá Vocoder trong Amphion v0.1.
Hệ thống PESQ ↑ M-STFT ↓ F0RMSE ↓ FPC↑
Chính thức (HiFi-GAN) 3.43 1.98 177 0.88
ESPnet (HiFi-GAN) 3.55 1.12 188 0.86
Amphion v0.1 (HiFi-GAN) 3.55 1.09 188 0.88
3.4. Vocoder
Chúng tôi so sánh Vocoder Amphion v0.1 với hai điểm kiểm tra HiFi-GAN mã nguồn mở được sử dụng rộng rãi. Một cái từ kho lưu trữ HiFi-GAN chính thức¹⁵; cái khác từ ESPnet¹⁶. Tất cả các điểm kiểm tra đều được huấn luyện trên khoảng 600 giờ dữ liệu lời nói. Toàn bộ tập đánh giá và tập kiểm tra của LibriTTS được sử dụng để đánh giá, với tổng cộng 20306 phát biểu. Đánh giá khách quan được thực hiện với các chỉ số M-STFT, PESQ, F0RMSE, và FPC. Kết quả được minh họa trong bảng 8. Với sự hỗ trợ bổ sung của hướng dẫn từ Discriminator dựa trên Biểu diễn Tần số-Thời gian [22, 23], HiFi-GAN Amphion v0.1 đạt được hiệu suất vượt trội trong tái tạo spectrogram và mô hình hóa F0.
4. KẾT LUẬN
Bài báo này đã trình bày Amphion, một bộ công cụ mã nguồn mở dành riêng cho sinh audio, nhạc và giọng nói. Mục tiêu chính của Amphion là tạo thuận lợi cho nghiên cứu có thể tái tạo và phục vụ như một bước đệm cho các nhà nghiên cứu và kỹ sư mới tham gia vào lĩnh vực sinh audio, nhạc và giọng nói. Kể từ khi phát hành Amphion vào tháng 11 năm 2023, Amphion đã nhận được hơn 4.300 sao trên GitHub và nhận được một số lượng đáng kể các yêu cầu pull và phản hồi. Đối với các kế hoạch tương lai, Amphion đang phát hành một số tập dữ liệu quy mô lớn [36] trong lĩnh vực sinh audio, nhạc và giọng nói. Ngoài ra, chúng tôi dự định hợp tác với ngành công nghiệp để phát hành các mô hình đã được huấn luyện trước quy mô lớn và hướng sản xuất.
Tài liệu tham khảo
[1] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, và Tie-Yan Liu, "FastSpeech 2: Fast và high-quality end-to-end text to speech," trong ICLR , 2020.
[2] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, và Jiang Bian, "Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers," trong ICLR . 2024, OpenReview.net.
[3] Liumeng Xue và Chaoren Wang và Mingxuan Wang và Xueyao Zhang và Jun Han và Zhizheng Wu, "SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion," trong Computers & Graphics , 2024.
[4] Jaehyeon Kim, Jungil Kong, và Juhee Son, "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech," trong ICML , 2021.
[5] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al., "Neural codec language models are zero-shot text to speech synthesizers," arXiv preprint arXiv:2301.02111 , 2023.
[6] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, và Mark D. Plumbley, "Audioldm: Text-to-audio generation with latent diffusion models," trong ICML . 2023, vol. 202, pp. 21450–21474, PMLR.
[7] Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, và Sheng Zhao, "AUDIT: Audio editing by following instructions with latent diffusion models," trong NIPS , 2023.
[8] Xueyao Zhang, Zihao Fang, Yicheng Gu, Haopeng Chen, Lexiao Zou, Junan Zhang, Liumeng Xue, và Zhizheng Wu, "Leveraging diverse semantic-based audio pretrained models for singing voice conversion," trong SLT. 2024, IEEE.
[9] Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, và Yike Guo, "Comosvc: Consistency model-based singing voice conversion," arXiv preprint arXiv:2401.01792 , 2024.
[10] Zeyu Xie và Xuenan Xu và Zhizheng Wu và Mengyue Wu, "PicoAudio: Enabling Precise Timestamp and Frequency Controllability of Audio Events in Text-to-audio Generation," trong arXiv preprint arxiv:2407.02869 , 2024.
[11] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, và Koray Kavukcuoglu, "WaveNet: A generative model for raw audio," trong Speech Synthesis Workshop , 2016, p. 125.
[12] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aäron van den Oord, Sander Dieleman, và Koray Kavukcuoglu, "Efficient neural audio synthesis," trong ICML , 2018, vol. 80, pp. 2415–2424.

--- TRANG 5 ---
[13] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, và Bryan Catanzaro, "DiffWave: A versatile diffusion model for audio synthesis," trong ICLR , 2021.
[14] Ryan Prenger, Rafael Valle, và Bryan Catanzaro, "Waveglow: A flow-based generative network for speech synthesis," trong ICASSP , 2019, pp. 3617–3621.
[15] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brébisson, Yoshua Bengio, và Aaron C. Courville, "MelGAN: Generative adversarial networks for conditional waveform synthesis," trong NIPS , 2019.
[16] Jiaqi Su, Zeyu Jin, và Adam Finkelstein, "Hifi-gan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks," trong INTERSPEECH , 2020, pp. 4506–4510.
[17] Reo Yoneyama, Yi-Chiao Wu, và Tomoki Toda, "Source-filter hifi-gan: Fast and pitch controllable high-fidelity neural vocoder," trong ICASSP , 2023, pp. 1–5.
[18] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, và Sungroh Yoon, "Bigvgan: A universal neural vocoder with large-scale training," trong ICLR , 2023.
[19] Yang Ai và Zhen-Hua Ling, "APNet: An all-frame-level neural vocoder incorporating direct prediction of amplitude and phase spectra," IEEE/ACM TASLP , vol. 31, pp. 2145–2157, 2023.
[20] Won Jang, Dan Lim, và Jaesam Yoon, "Universal MelGAN: A robust neural vocoder for high-fidelity waveform generation in multiple domains," arXiv , vol. abs/2011.09631, 2020.
[21] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, và Yossi Adi, "High fidelity neural audio compression," arXiv , vol. abs/2210.13438, 2022.
[22] Yicheng Gu, Xueyao Zhang, Liumeng Xue, và Zhizheng Wu, "Multi-scale sub-band constant-q transform discriminator for high-fidelity vocoder," trong ICASSP , 2024.
[23] Yicheng Gu và Xueyao Zhang và Liumeng Xue và Haizhou Li và Zhizheng Wu, "An Investigation of Time-Frequency Representation Discriminators for High-Fidelity Vocoder," trong arXiv preprint arxiv:2404.17161 , 2024.
[24] Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, và Sheng Zhao, "Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models," trong ICML , 2024.
[25] Evelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg, và Yang Zhang, "Hi-Fi Multi-Speaker English TTS Dataset," INTERSPEECH , pp. 2776–2780, 2021.
[26] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, và Ronan Collobert, "MLS: A large-scale multilingual dataset for speech research," trong INTERSPEECH . 2020, pp. 2757–2761, ISCA.
[27] Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdelrahman Mohamed, và Emmanuel Dupoux, "Libri-light: A benchmark for ASR with limited or no supervision," trong ICASSP . 2020, pp. 7669–7673, IEEE.
[28] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, và Gunhee Kim, "AudioCaps: Generating captions for audios in the wild," trong NAACL-HLT , 2019, pp. 119–132.
[29] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, và Yonghui Wu, "LibriTTS: A corpus derived from librispeech for text-to-speech," trong INTERSPEECH , 2019, pp. 1526–1530.
[30] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, và Abdelrahman Mohamed, "Hubert: Self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM TASLP , vol. 29, pp. 3451–3460, 2021.
[31] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, và Dong Yu, "Diffsound: Discrete diffusion model for text-to-sound generation," IEEE/ACM TASLP , 2023.
[32] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, và Mark D Plumbley, "PANNs: Large-scale pretrained audio neural networks for audio pattern recognition," IEEE/ACM TASLP , vol. 28, pp. 2880–2894, 2020.
[33] Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie, và Mengxiao Bi, "Opencpop: A high-quality open source chinese popular song corpus for singing voice synthesis," trong INTERSPEECH , 2022, pp. 4242–4246.
[34] Rongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, và Zhou Zhao, "Multi-singer: Fast multi-singer singing voice vocoder with A large-scale corpus," trong ACM Multimedia , 2021, pp. 3945–3954.
[35] Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie Huang, Jieming Zhu, Xiao Chen, và Zhou Zhao, "M4singer: A multi-style, multi-singer and musical score provided mandarin singing corpus," trong NIPS , 2022.
[36] He, Haorui và Shang, Zengqiang và Wang, Chaoren và Li, Xuyuan và Gu, Yicheng và Hua, Hua và Liu, Liwei và Yang, Chen và Li, Jiaqi và Shi, Peiyang và Wang, Yuancheng và Chen, Kai và Zhang, Pengyuan và Wu, Zhizheng, "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation," trong SLT. 2024, IEEE.

--- TRANG 6 ---
⁴https://github.com/coqui-ai/TTS
⁵https://github.com/speechbrain/speechbrain
⁶https://github.com/neonbjb/tortoise-tts
⁷https://github.com/espnet/espnet
⁸https://github.com/microsoft/UniSpeech/tree/main/downstreams/
speaker verification
⁹https://github.com/Plachtaa/VALL-E-X
¹⁰https://github.com/yangdongchao/Text-to-sound-Synthesis
¹¹https://github.com/haoheliu/AudioLDM
¹²https://github.com/bshall/soft-vc
¹³http://vc-challenge.org/
¹⁴https://huggingface.co/datasets/CSTR-Edinburgh/vctk
¹⁵https://github.com/jik876/hifi-gan
¹⁶https://github.com/kan-bayashi/ParallelWaveGAN
³Đến tháng 9 năm 2024, Amphion đã phát hành 10 demo tương tác trên Hugging Face Spaces (https://huggingface.co/amphion) và OpenXLab (https://openxlab.org.cn/usercenter/Amphion)
