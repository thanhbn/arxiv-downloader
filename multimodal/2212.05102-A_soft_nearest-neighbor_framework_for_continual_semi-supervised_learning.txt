# 2212.05102.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2212.05102.pdf
# File size: 1038470 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A soft nearest-neighbor framework for continual semi-supervised learning
Zhiqi Kang∗1Enrico Fini∗2Moin Nabi3Elisa Ricci2,4Karteek Alahari1
1Inria† 2University of Trento3SAP AI Research4Fondazione Bruno Kessler
Abstract
Despite significant advances, the performance of state-
of-the-art continual learning approaches hinges on the un-
realistic scenario of fully labeled data. In this paper, we
tackle this challenge and propose an approach for contin-
ual semi-supervised learning—a setting where not all the
data samples are labeled. A primary issue in this scenario
is the model forgetting representations of unlabeled data
and overfitting the labeled samples. We leverage the power
of nearest-neighbor classifiers to nonlinearly partition the
feature space and flexibly model the underlying data dis-
tribution thanks to its non-parametric nature. This enables
the model to learn a strong representation for the current
task, and distill relevant information from previous tasks.
We perform a thorough experimental evaluation and show
that our method outperforms all the existing approaches by
large margins, setting a solid state of the art on the con-
tinual semi-supervised learning paradigm. For example, on
CIFAR-100 we surpass several others even when using at
least 30 times less supervision (0.8% vs. 25% of annota-
tions). Finally, our method works well on both low and
high resolution images and scales seamlessly to more com-
plex datasets such as ImageNet-100. Our source code is
publicly available at https://github.com/kangzhiq/NNCSL.
1. Introduction
Several efforts have been devoted to the continual learn-
ing (CL) [18] paradigm wherein training data samples ar-
rive sequentially. However, most of the state-of-the-art CL
methods [11, 12, 20] are based on a strong assumption: the
data is fully labeled. This is an unrealistic requirement as
labeling data is oftentimes expensive for the expertise re-
quired or the amount of annotations, hazardous due to pri-
vacy or safety concerns, or impractical in a real-time online
scenario. A natural way of tackling this issue is by leverag-
ing the semi-supervised learning framework, where not all
the data samples are labeled.
∗Zhiqi Kang and Enrico Fini contributed equally to this work
†Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France.
0.8% 5% 25%
Percentage05101520253035Average Accuracy
NNCSL
CCICER
iCaRLLwF
12345678910
T ask203040506070Average Accuracy
NNCSL
CSL + feat. dist.CSL
PAWSFigure 1: Left: The average accuracy with different percent-
ages of labeled data on CIFAR-100. Our method (NNCSL)
with 0.8% of the labels outperforms or matches the perfor-
mance of all other methods at 25%. Right: Comparison of
different versions of our approach and PAWS [3]. CSL is
equivalent to NNCSL without our NND loss.
In recent years, this stimulated the community to in-
vestigate a new line of research named continual semi-
supervised learning [8,44,52]. It refers to the setting where
each task in the sequence is semi-supervised. This learning
scenario brings novel challenges, as the models catastrophi-
cally forget the representations of unlabeled data while also
overfitting the labeled set. This is further exacerbated by an-
other well-studied phenomenon in CL: overfitting the expe-
rience replay buffer [10]. These challenges result in vanilla
CL methods underperforming, as they lack the ability to ex-
tract information from the unlabeled set, thus largely over-
fitting to the labeled set [53]. On the other hand, semi-
supervised learning approaches [15,29,46,58] balance well
the labeled and unlabeled sets but cannot handle the contin-
ual scenario, and suffer from forgetting even when paired
with well-known CL methods (see Fig. 1 (right) and Tab. 1).
A few recent approaches [8, 52] partially mitigate these
issues on small-scale datasets. The method in [52] relies on
generative models to replay previous classes, which leads to
a sizeable computational and memory overhead, making it
difficult to scale to larger datasets such as ImageNet. Bos-
chini et al. [8] proposed to contrast among samples of differ-
ent classes and tasks. However, [8] falls short with smaller
memory buffers, larger datasets, more tasks, or higher reso-
lution images (see Fig. 1 (left) and results in Sec. 6). There-arXiv:2212.05102v3  [cs.CV]  11 Sep 2023

--- PAGE 2 ---
fore, we argue that there is a clear need for more powerful
continual semi-supervised learning methods, with a more
suitable use of the labeled set, and efficient and stable rep-
resentation learning from the unlabeled set.
In this paper, we unleash the power of nearest-neighbors
in the context of continual semi-supervised learning. In
particular, we propose a new method, NNCSL ( Nearest-
Neighbor for Continual Semi-supervised Learning), that
leverages the ability of the nearest-neighbor classifier to
non-linearly partition the feature space in two ways: i) to
learn powerful and stable representations of the current task
using a self-supervised multi-view strategy, and ii) to dis-
till previous knowledge and transfer the local structure of
the feature space. The latter is achieved through our pro-
posed NND ( Nearest- Neighbor Distillation), a novel semi-
supervised distillation loss that mitigates forgetting in con-
tinual semi-supervised learning better than other competi-
tive distillation approaches. In contrast with knowledge dis-
tillation [11, 22, 30, 38] and feature distillation [19, 21, 24],
which focus exclusively on class-level and sample-level dis-
tributions respectively, NND simultaneously distills rela-
tionships between classes and samples by leveraging the
nearest-neighbor classifier. Overall, NNCSL outperforms
all related methods by very large margins on both small and
large scale datasets and both low and high resolution im-
ages. For instance, as shown in Fig. 1, NNCSL matches or
surpasses all others with more than 30 times less supervi-
sion (0.8% vs. 25% of annotations) on CIFAR100.
The main contributions of this work are as follows:
• We propose NNCSL, a novel nearest-neighbor-based
continual semi-supervised learning method that is, by
design, impacted less by the overfitting phenomenon
related to a small labeled buffer.
• We propose NND, a new distillation strategy that trans-
fers both representation-level and class-level knowl-
edge from the previously trained model using the out-
puts of a soft nearest-neighbor classifier, which effec-
tively helps alleviate forgetting.
• We show that NNCSL outperforms the existing meth-
ods on several benchmarks by large margins, setting a
new state of the art on continual semi-supervised learn-
ing. In contrast to previous approaches, our method
works well on both low and high resolution images and
scales seamlessly to more complex datasets.
2. Related work
Semi-supervised learning. Semi-supervised methods fo-
cus on learning models from large-scale datasets where
only a few samples have associated annotations [17]. Early
strategies for this learning paradigm applied to deep ar-
chitectures leveraged pseudo-labels and performed self-training based on them [29]. This scheme was later im-
proved with confidence thresholding [2] and adaptive con-
fidence thresholding [56, 59]. More sophisticated methods
for incorporating the confidence of the predictions and fil-
tering out spurious samples were also developed, such as
FixMatch [46], which employs a student-teacher architec-
ture. Other approaches demonstrated the benefit of co-
training [36] and distillation [55].
Another class of approaches was derived with the idea of
imposing similar predictions from the network for two sam-
ples obtained with different input perturbations [3,4,27,33,
35, 41, 47, 50, 60]. For example, [3] considered a consis-
tency loss and soft pseudo labels generated by comparing
the representations of the image views to those of a set of
randomly-sampled labeled images. Recently, sample mix-
ing techniques, such as MixUp, were also investigated in
the context of semi-supervised learning for improving the
model performance on low sample density regions [5,6,31].
However, none of the aforementioned works addressed the
problem of learning in an incremental setting.
Continual learning. Several CL approaches have been pro-
posed in the last few years to learn from data in an incre-
mental fashion. According to a recent survey [18], existing
CL methods can be roughly categorized into three groups.
The first category comprises regularization-based methods,
which address the problem of catastrophic forgetting by in-
troducing appropriate regularization terms in the objective
function [12, 19, 22, 24, 30] or identifying a set of param-
eters that are most relevant for certain tasks [13, 25, 54].
Replay-based methods correspond to the second group, and
they store a few samples from previous tasks [9, 14, 32, 38]
or generate them [34, 43] in order to rehearse knowledge
during the training phases for subsequent tasks. Finally,
the third category is parameter isolation methods [40, 42],
which operate by allocating task-specific parameters.
While the vast majority of these methods operate in a
supervised setting, recent works addressed the problem of
overcoming catastrophic forgetting in the challenging case
of limited [8, 28, 44, 52] or no supervision [1, 21, 37, 45].
However, most of them have default settings that are signifi-
cantly different, e.g., the use of external datasets, and the ac-
cessibility of labeled/unlabeled data during continual learn-
ing stages, leaving only a few [8, 52] to be comparable in
our desired realistic setting. Wang et al. [52] addressed the
continual semi-supervised learning problem and proposed
ORDisCo, a method that continually learns a conditional
generative adversarial network with a classifier from par-
tially labeled data. However, the method yields prohibitive
costs on higher resolution images, e.g., on ImageNet-100.
Contrastive continual interpolation consistency (CCIC) [8]
is another approach, which proposed to contrast samples
among different classes and tasks. However, task-level con-
trastive learning showed insignificant improvement in the

--- PAGE 3 ---
ablation study. We speculate it may confuse the model as it
gathers classes that are not necessarily related in the feature
space. This confusion leads to worse performance with a
small memory buffer, more samples, more tasks, or higher
resolution images as shown by our experiments. Our work
radically departs from these previous methods, as we design
NNCSL, a novel approach for continual semi-supervised
learning based on a soft nearest-neighbor classifier. Our em-
pirical evaluation demonstrates that NNCSL surpasses prior
works by a large margin.
3. Continual semi-supervised learning
We now formally define the problem of continual semi-
supervised learning. Let the training data arrive sequen-
tially, i.e., as a sequence of Ttasks. The dataset associated
to task tis denoted as Dt, with t∈ {1, ..., T}. Learning is
therefore performed task-wise, where only the current train-
ing data Dtis available during task t. When switching from
one task to the next one, previous data is systematically dis-
carded. Since the available dataset is not fully labeled, we
further divide it into two subsets such that Dt=Ut∪Lt.
Typically in a semi-supervised learning scenario, we have
|Lt| ≪ | Ut|, the ratio |Lt|/|Ut|is kept constant for all the
tasks. In addition, it is common practice in the CL liter-
ature [8, 38] to allow the retention of a memory buffer M
that stores and replays previously seen samples, as shown
in Fig. 2.
Letfθbe the model, parameterised by θ, and consist-
ing of three components: a backbone g, a projector hand a
classifier p. The backbone, here modeled as a convolutional
neural network, is used to extract representations z=g(x)
from an input image x. The classifier takes this representa-
tion to predict a set of logits p=p(z), while the projector
(implemented as a multi-layer perceptron) maps the back-
bone features to a lower-dimensional space h=h(z). In
addition, we use superscript to refer to the state at a certain
point in time, for instance for task tasft
θ, and for the previ-
ous task t−1asft−1
θ. Similarly, we use xt
uandxt
lto refer to
samples drawn from UtandLtrespectively. Apart from im-
ages, the labeled dataset also contains one-hot ground truth
annotations y.
In the following sections, we introduce the proposed
NNCSL method for continual semi-supervised learning.
We first present PAWS [3], which inspired our NNCSL,
(Sec. 4) and show why this method is not immediately ap-
plicable to the continual setting. Subsequently, we present
CSL, our base continual semi-supervised learner (Sec. 5.1),
which solves many of its issues. However, this base method
lacks a mechanism to counteract forgetting. Hence, we in-
troduce NND, our novel distillation approach based on the
soft nearest-neighbor classifier in Sec. 5.2. All these ele-
ments are harmoniously integrated into in our full method:
NNCSL, whose overall objective is summarized in Sec. 5.3.
Task t-1
Memor y Buff er
Labeled  
t-1 
Unlabeled  
t-1 t-3
t-2
Task t
t-3 
t-2
t-1Memor y Buff er
Model Model
Labeled  
t 
Unlabeled  
t 
Figure 2: Illustration of the learning process in continual
semi-supervised learning.
4. Nearest-neighbor meets continual learning:
strengths and weaknesses
We now discuss the use of nearest-neighbor tech-
niques [3] in the context of continual learning, and describe
its strengths and weaknesses in scenarios with data distri-
bution shifts. During training, the mini-batches that the
model receives are composed of labeled and unlabeled data,
withKandNas batch sizes for these two sets respectively.
Unlabeled images in the batch are augmented twice using
common data augmentation techniques to obtain two corre-
lated views of the same sample (x,ˆx). The model processes
the batch, producing the projected representations hland
(hu,ˆhu)for labeled and unlabeled samples respectively.
The main idea from [3] is to assign pseudo-labels for
unlabeled samples in a non-parametric manner by consid-
ering their relationship with labeled samples, i.e., nearest-
neighbor label assignments. Samples are compared in the
feature space using the cosine similarity of projected fea-
tures, and then the pseudo label is obtained by aggregating
labels according to the similarities. More formally, let the
superscript krepresent the index of the kthsample in the
labeled mini-batch, and δbe the cosine similarity. One can
apply a soft nearest-neighbor classifier to classify the aug-
mented unlabeled sample ˆxuas follows:
ˆv= SNN( ˆhu,S, ϵ) =KX
keckδ(ˆhu,hk
l)/ϵ
PK
ieciδ(ˆhu,hi
l)/ϵyk,(1)
whereS= [h1
l, ...,hK
l]are the features of the support sam-
ples and ϵis a sharpening parameter that controls the en-
tropy of the pseudo-label. Similarly, we can classify the
other view of the same sample v= SNN( hu,S, τ), with
the only difference that we use a more gentle sharpening
parameter τ > ϵ , referred to as the temperature. Now, we
can use ˆvas a target pseudo-label and train the network
through the cross-entropy loss:
LSNN=H(v,ˆv). (2)
The mechanism described above encourages the network

--- PAGE 4 ---
to output consistent representations for similar inputs, while
also accounting for the distribution of the classes in the fea-
ture space. However, one issue with this formulation is
that the network could output unbalanced or even degen-
erate predictions where some classes are predicted more
frequently than others. To avoid this, PAWS imposes the
distribution-wise likelihood of all the classes to be uniform
using a regularization term called Mean Entropy Maximiza-
tion (MEM) loss defined as:†
LMEM =H 
1
NNX
nˆvn!
. (3)
Given these two losses, the total loss for PAWS is a
weighted average of the two.
The advantage of this soft nearest-neighbor formulation
is that it utilizes labeled samples as support vectors, not as
training samples, which reduces overfitting. This property
is interesting from the point of view of continual learning,
since we would like to extract as much training signal as
possible from the memory buffer without overfitting to it.
However, PAWS is not designed to work under data dis-
tribution shifts. The key issue of PAWS in the CL setting
is the assumption that the labeled and unlabeled sets ex-
hibit the same distribution. This is untenable in CL, as the
memory buffer contains classes not in the current task’s un-
labeled set. MEM loss aggravates this problem, as it tries
to scatter the pseudo label over all the classes, even for the
ones whose unlabeled samples are unavailable. A simple
solution would be to use the labeled data of the current task
and discard the buffer, but this is sub-optimal as the buffer
is critical for CL.
Another important drawback of PAWS in the context of
CL is the ambiguity of two-stage methods. PAWS performs
best when the learned backbone is fine-tuned using the la-
beled set. This two-stage pipeline (pre-training and then
fine-tuning) is prone to a loss in generalization performance
as the labeled set is very small. Offline methods such as
PAWS can afford this loss to gain specialized knowledge
on the targeted tasks. However, CL requires the reuse of
the model for the next task. On one hand, using the fine-
tuned model leads to a noticeable loss in performance on
the subsequent tasks. On the other hand, using the pre-
trained checkpoint (before fine-tuning) preserves more gen-
eral properties but brings additional memory overhead, as
two models need to be stored, which is undesirable in CL. In
the following section, we describe our solution to overcome
these limitations. A detailed illustration of the ambiguity of
two-stage methods can be found in the supplementary ma-
terial.
†With a slight abuse of notation we refer to H(·)with one argument as
the entropy function, while when two arguments are passed we consider it
as the cross-entropy function H(·,·).
SNN Logits
SNN Logits
SNN
SNN
LIN
SNN
+
MEM
C
L
Snew classes
new classesL
t
t-1
Filterremove old
classes
Filter
remove old
classes
U
tFigure 3: Overview of the base learner component of our
method, which does not have a distillation loss. We refer to
this as CSL.
5. NNCSL: Our nearest-neighbor approach for
continual semi-supervised learning
5.1. Our continual semi-supervised learner
We now describe our proposed approach that leverages
the strengths of the nearest-neighbor approach described in
Sec. 4, while overcoming its weaknesses. The easiest way
to make the labeled and unlabeled distributions match is to
disregard the memory buffer. This is clearly undesirable
for CL. However, one could multi-task PAWS with another
objective that also takes into account the information of the
memory buffer. In particular, we suggest processing labeled
samples of all the classes seen so far, but filtering out sam-
ples from the previous tasks so they do not interfere in the
computation of Eq. 1. However, we can use the output of
the linear classifier pand optimize a standard cross-entropy
loss:
LLIN=JX
jH(pk,yj), (4)
on all the Jlabeled samples in the current batch (which also
contains Klabeled samples of the current task). The com-
plete loss for our base continual semi-supervised learner
(named CSL) is as follows:
LCSL=LSNN +λMEM· LMEM +λLIN· LLIN.(5)
This loss has several favorable effects; it: i) stimulates the
network to focus on the old classes while learning represen-
tations of the new ones through PAWS, ii) creates an en-
semble effect between the two classifiers, iii) completely
removes the need for fine-tuning, as the linear classifier is
trained online, and iv) enables us to control the trade-off
between fitting labeled or unlabeled data through the pa-
rameter λLIN. Interestingly, we found that very small val-
ues of λLINwork well in practice, while larger values in-

--- PAGE 5 ---
L
t-2
t-1
U 
t 
NND  
LossOld f eatur e
space
New f eatur e
spaceSNNSNNFigure 4: Illustration of our proposed NND loss, which
compares the predictions of ft
θandft−1
θwith the same un-
labeled samples and support samples.
crease overfitting. We believe that, due to its partially self-
supervised nature, LSNN learns improved representations,
that can be easily discriminated by the linear classifier. An
illustration of the architecture of CSL is shown in Fig. 3.
5.2. Soft nearest-neighbor distillation
Distilling information [23] is a common practice in CL,
which utilizes frozen models (or a bank of features and/or
probabilities) trained on previous tasks as a teacher to reg-
ularize the currently active model, which is a student. Let
tbe the index for the current task. The student model ft
θ
aims to mimic the outputs of the teacher ft−1
θ, while learn-
ing the new task. Previous works [22, 30] typically distill
either the logits of a linear classifier or the features of the
hidden layers of the network. However, in CSL, the main
driver for the network to learn representations is the loss ap-
plied to the soft nearest-neighbor classifier. As explained in
Sec. 5.1, this loss does not give any signal on previous data,
as old samples get filtered out and fed to the linear classi-
fier only. This is made worse by the fact that the nearest-
neighbor classifier is applied on a separated projection head
h, that has no incentive to remember previous knowledge.
To mitigate these issues, we devise a novel Nearest-
Neighbor Distillation (NND) loss that blends well with our
framework. The loss is based on the intuition that we can
evaluate the nearest-neighbor classifier on the old feature
space using the same support samples. This equates to com-
puting the following two vectors: w= SNN( hu,R, τ)and
wt−1= SNN( ht−1
u,Rt−1, τ), where ht−1
uis a feature vec-
tor output by the teacher for an unlabeled sample xu, while
RandRt−1represent the support set of previous classes
embedded in the old and new feature spaces respectively.
To mitigate forgetting, we use the probabilities predicted bythe teacher as a distillation target:
LNND=H(w,wt−1). (6)
Note that the output of the teacher is not sharpened as it is
done in Eq. 1. We apply the same temperature for both new
and old features. We emphasize that here we use an inverted
filter as that of Sec. 5.1, to distill knowledge about the previ-
ous classes only. See Fig. 4 for a visual intuition and supple-
mentary material for the visualization of its impact on deep
features. It is worth mentioning that the memory overhead
introduced by storing ft−1
θcan be easily reduced by stor-
ing instead the features, which are lightweight compared to
images. For instance, storing a 128-dimension feature takes
roughly 1/1000thmemory w.r.t. an RGB image of size 224
× 224.
Our NND loss is different from the standard distillation
loss. Knowledge distillation focuses on class-level distribu-
tions calculated by comparing samples of the new tasks with
the prototype of each class. This causes a loss of informa-
tion about representations of individual samples in the latent
space. On the contrary, NND distills the aggregated rela-
tionships of each unlabeled sample with respect to all the
labeled ones in the mini-batch. This encourages the model
to maintain more stable representations, by anchoring un-
labeled samples to labeled samples. In addition, NND can
capture and transfer non-linear sample-class relationships
by definition, unlike knowledge distillation which is limited
to linear boundaries. Also, the SNN classifier is computed
on a different support set sampled from the buffer at every
iteration, which introduces randomness that further regular-
izes the model. Furthermore, when distilling at very low
temperatures (e.g., 0.1), the softmax function from Eq. 1
behaves like an argmax (i.e., selecting the closest samples
in the feature space with high cosine similarity), leading to
the transfer of information mainly on the local neighbor-
hood from the teacher to the student. Finally, with respect
to feature distillation which lacks alignment with class la-
bels, NND carries more information about class distribu-
tions, which results in improved performance.
5.3. Overall loss
The overall NNCLS model, composed of two novel com-
ponents, i.e., CSL and NND, is trained with the loss:
LNNCSL =LCSL+λNND· LNND. (7)
6. Evaluation and analysis
6.1. Experimental settings
Datasets . We evaluate our method on three datasets.
CIFAR-10 [26] is a dataset of 10 classes with 50k train-
ing and 10k testing images. Each image is of size 32×32.
CIFAR-100 [26] is similar to CIFAR-10, except it has 100

--- PAGE 6 ---
MethodCIFAR-10 CIFAR-100
0.8% 5% 25% 0.8% 5% 25%
Fine-tuning 13.6 ±2.9 18.2 ±0.4 19.2 ±2.2 1.8 ±0.2 5.0 ±0.3 7.8 ±0.1
oEWC [25] 13.7 ±1.2 17.6 ±1.2 19.1 ±0.8 1.4 ±0.1 4.7 ±0.1 7.8 ±0.4
ER (500) [39] 36.3 ±1.1 51.9 ±4.5 60.9 ±5.7 8.2 ±0.1 13.7 ±0.6 17.1 ±0.7
iCaRL (500) [38] 24.7 ±2.3 35.8 ±3.2 51.4 ±8.4 3.6 ±0.1 11.3 ±0.3 27.6 ±0.4
FOSTER (500) [51] 43.3 ±0.7 51.9 ±1.3 57.1 ±2.0 4.7 ±0.6 14.1 ±0.6 21.7 ±0.7
X-DER (500) [7] 33.4 ±1.2 48.2 ±1.7 58.9 ±1.5 8.9 ±0.3 18.3 ±0.5 23.9 ±0.7
PseudoER (500) 50.5 ±0.1 56.5 ±0.6 57.0 ±0.6 8.7 ±0.4 11.4 ±0.5 12.3 ±0.2
CCIC [8] (500) 54.0 ±0.2 63.3 ±1.9 63.9 ±2.6 11.5 ±0.7 19.5 ±0.2 20.3 ±0.3
PAWS [3] (500) 51.8 ±1.6 64.6 ±0.6 65.9 ±0.3 16.1 ±0.4 21.2 ±0.4 19.2 ±0.4
CSL (500) 64.5 ±0.7 69.6 ±0.5 70.0 ±0.4 23.6 ±0.3 26.2 ±0.5 29.3 ±0.3
NNCSL (500) 73.2±0.1 77.2±0.2 77.3±0.1 27.4±0.5 31.4±0.4 35.3±0.3
PseudoER (5120) 55.4 ±0.5 70.0 ±0.3 71.5 ±0.2 15.1 ±0.2 24.9 ±0.5 30.1 ±0.7
CCIC [8] (5120) 55.2 ±1.4 74.3 ±1.7 84.7±0.9 12.0 ±0.3 29.5 ±0.4 44.3 ±0.1
ORDisCo [52] (12500) 41.7 ±1.2 59.9 ±1.4 67.6 ±1.8 - - -
CSL (5120) 64.3 ±0.7 73.1 ±0.3 73.9 ±0.1 23.7 ±0.5 41.8 ±0.4 50.3 ±0.8
NNCSL (5120) 73.7±0.4 79.3±0.3 81.0±0.2 27.5±0.7 46.0±0.2 56.4±0.5
Table 1: Average accuracy with standard deviation of different methods tested with 5-task CIFAR-10 and 10-task CIFAR-100
settings. The number between brackets indicates the size of the memory buffer for the labeled data.
classes containing 500 training images and 100 testing im-
ages per class. ImageNet-100 [48] is a 100-class subset of
the ImageNet-1k dataset from the ImageNet Large Scale Vi-
sual Recognition Challenge 2012, containing 1300 training
images and 50 test images per class.
Continual semi-supervised setting . For both CIFAR-
10 and CIFAR-100, we train the models with three dif-
ferent levels of supervision, i.e., λ∈ {0.8%,5%,25%}.
For instance, this corresponds to 4, 25, 125 labeled sam-
ples per class in CIFAR-100. As for ImageNet-100, we
opt for 1% labeled data. To build the continual datasets,
we use the standard setting in the literature [8, 52], and
divide the datasets into equally disjoint tasks: 5/10/20
tasks for CIFAR-10/CIFAR-100/ImageNet-100, i.e., 2/10/5
classes per task, respectively. We follow the standard class-
incremental learning setting in all our experiments: during
the CL stages, we assume that all the data of previous tasks
are discarded. A memory buffer can be eventually built, but
only for labeled data. Following [8], we set the buffer size
for labeled data as 500 or 5120, to ensure a fair comparison.
Metrics. We mainly evaluate the performance of different
methods considering the average accuracy over all the seen
classes after each task, as is common in CL methods [18].
Analysis with other metrics, such as forward and backward
transfer, can be found in the supplementary material.
Implementation details . As in [8], we use ResNet18 as
our backbone for all the datasets. We adopt the implemen-
tation of [3], unless explicitly stated. Specifically, we use
the LARS optimizer [57] with a momentum value of 0.9.We set the weight decay as 10−5, the batch-size as 256. The
learning rate is set to 0.4 for CIFAR-10 and 1.2 for CIFAR-
100 and ImageNet-100, respectively. We apply 10 epochs
warm-up and reduce it with a cosine scheduler. For the two
correlated views of the same sample, we generate two large
crops and two small crops of each sample. The large crops
serve as targets for each other, whereas they are both tar-
gets for the small crops. We apply data-augmentation as
in [16]. Label smoothing is applied with a smoothing fac-
tor of 0.1. The additional linear evaluation head is a simple
linear layer, which we use to predict labels at test time. We
choose λNND= 0.2andλLIN= 0.005. As for the memory
buffer, we utilize a simple random sampling strategy. We
run our experiments 3 times with different random seeds.
The standard deviation is also reported, if applicable. Fur-
ther implementation details and analysis of data augmenta-
tion can be found in the supplementary material.
Baselines. As baselines, we first consider traditional fully-
supervised CL methods. A straightforward way to con-
vert them into a continual semi-supervised setting is to
use only the labeled data during training and to discard
the unlabeled data. We consider two categories of meth-
ods: replay-less methods, namely, continually fine-tuning
the model on each new task, online elastic weight consol-
idation (oEWC) [25], and replay-based methods, i.e., ex-
perience replay (ER) [39], iCaRL [38], FOSTER [51] and
X-DER [7]. We denote PseudoER as an additional two-
stage baseline method, which is a combination of semi-
supervised learning (PAWS) and CL (ER) methods. More

--- PAGE 7 ---
precisely, we continually train a PAWS and use it to self-
label the unlabeled data. An ER method is applied afterward
on the labeled and pseudo-labeled data. We also consider
continual semi-supervised learning baseline methods, such
as CCIC [8] and ORDisCo [52]. While CCIC has an explicit
definition of the memory buffer for labeled data, which is
either 500 or 5120, ORDisCo directly stores all the labeled
data that the model receives. We thus denote its memory
buffer size with the largest value, which is M= 12500 ,
equivalent to 25% of CIFAR-10. As for the upper bound,
which is commonly shown in CL [18], it is obtained by
jointly training the model with all the available data. The
lower bound refers to continual fine-tuning. CSL is also
included as an ablation of NNCSL without NND loss.
6.2. Results
CIFAR-10 and CIFAR-100. We first report the perfor-
mance of different methods on CIFAR-10 and CIFAR-100
in Tab. 1. The upper bound on CIFAR-10 is 92.1 ±0.1%,
and that on CIFAR-100 is 67.7 ±0.9%. NNCSL outper-
forms all the competitors in all settings but one, with a sig-
nificant margin. For instance, when using a buffer of 5120
and 0.8 %of labeled data, NNCSL performs better than or
substantially matches almost all the others, even when they
use 25% labeled data, i.e., about 30 times more supervision.
It is also interesting to note that NNCSL has a very low
variance ( ≤0.7) across all the settings, indicating a better
convergence and representation learning during training.
From the results in Tab. 1, the memory buffer is shown to
be effective to alleviate forgetting, as replay-based methods
significantly outperform regularization-based ones. Pseu-
doER performs better than ER when labeled data is limited
(0.8%), but underperforms when the ratio is higher (5%,
25%). We conjecture this as a result of noisy pseudo-labeled
data replacing the ground truth in the memory buffer by the
sampling strategy of ER, which causes stronger forgetting.
In addition, PseudoER is upper-bounded by PAWS, since
ER is dependent on the accuracy of pseudo-labeling.
Methods such as CCIC and ORDisCo†benefit from their
design specific to the continual semi-supervised learning
scenario. Even though ORDisCo has a larger memory
buffer, its performance is inferior to that of CCIC. We be-
lieve that this is due to the difficulty in jointly training a
continual classifier with a GAN model. CCIC performs rea-
sonably well on CIFAR-10, especially with a large mem-
ory buffer. When the buffer size is 5120, CCIC performs
better than NNCSL in the 25 %setting. We suspect that
our method underfits in this setting, due to the very small
weight of the linear evaluation loss. In contrast, CCIC relies
more on labeled data, with equal or even higher importance
for the supervised loss. To validate our hypothesis, we in-
†Note that the results of ORDisCo are directly provided by the authors
of [52] as there is no open-source implementation of their approach.MethodImageNet-100
1% 5% 25%
Fine-tuning 1.5 ±0.2 2.7 ±0.1 4.1 ±0.2
ER [39] (5120) 12.2 ±0.8 26.3 ±0.7 38.8 ±1.0
FOSTER [51] (5120) 14.8 ±1.1 32.8 ±0.7 42.1 ±1.5
X-DER [7] (5120) 10.8 ±1.1 27.4 ±1.6 45.3 ±1.0
CCIC [8] (5120) 13.5 ±1.2 19.5 ±0.7 25.9 ±0.9
CSL (5120) 26.8 ±0.4 47.9 ±0.2 56.3 ±0.5
NNCSL (5120) 29.7±0.4 51.3±0.1 65.6±0.3
Table 2: Average accuracy with standard deviation of dif-
ferent methods tested with 20-task ImageNet-100 settings.
The number between brackets indicates the size of the mem-
ory buffer for the labeled data.
creased the weight λLINfor our linear classifier loss. We
obtained an accuracy of 84.5 ±0.4%, which is comparable
with CCIC in the same setting (84.7 ±0.9%). In the case
of a dataset with more classes such as CIFAR-100, the su-
periority of NNCSL is clearly evident. Further comparison
with the replay strategy of ORDisCo can be found in the
supplementary material.
ImageNet-100. We also evaluate on the more challenging
benchmark of ImageNet-100 in a 20-task continual semi-
supervised setting and buffer size 5120. As shown in Tab. 2,
NNCSL is the best-performing method. CCIC does not
show significant improvement with more labeled data (5%
& 25%) and also becomes inferior to vanilla CL methods,
e.g., ER. We suspect this is a limitation of its representation
learning for extracting information from images with higher
resolution and larger variance, as we observe its training
and validation accuracy in this setting is significantly lower.
In addition, Fig. 5 illustrates the evolution of average ac-
curacy. We note that the average accuracy of our NNCSL
stabilizes at around 30% after the 10thtask and has a clear
rebound between tasks 11 and 16, showing that NNCSL can
effectively retain knowledge acquired during the continual
learning steps. In contrast, previously competitive meth-
ods clearly suffer from forgetting in this challenging set-
ting, especially in the first several tasks. Despite the same
rebounding effect in some methods, e.g., FOSTER, they fail
to remain close to NNCSL.
6.3. Additional analyses
Ablation . We ablate the components of our framework in
Tab. 3 on both CIFAR-10 (C10 in the table), CIFAR-100
(C100) and ImageNet100 (IN100). The largest improve-
ment comes from filtering (10.8% and 6.2% improvements
on CIFAR-10 and CIFAR-100 respectively) and distillation
(9.4% and 4% improvements on CIFAR-10 and CIFAR-
100 respectively). This confirms the contributions of our
proposed components. Although the linear evaluation loss

--- PAGE 8 ---
1234567891011121314151617181920
T ask102030405060Average Accuracy
NNCSL
CCIC
ERX-DER
FOSTERFigure 5: Comparison of NNCSL with existing methods on
20-task ImagetNet-100 and 1% of labeled data. The average
accuracy after each learning step is shown.
MethodComponent Dataset
Distill Filter Linear C10 C100 IN100
PAWS 51.8 16.1 Collapse
CSL (w/o filter) ✓ 53.0 17.2 Collapse
CSL ✓ ✓ 63.8 23.4 27.1
NNCSL ✓ ✓ ✓ 73.2 26.8 29.3
Table 3: Ablation study of the effectiveness of the proposed
components on 5-task CIFAR-10 and 10-task CIFAR-100,
with M= 500 and 0.8% of labeled data, and 20-task
ImageNet-100 with M= 5120 and 1% of labeled data. We
use average accuracy as metrics.
Method & Distillation T1 T2 T3 T4 T5
CSL 25.2 22.9 24.6 37.3 37.0
CSL + Knowledge distill 27.8 24.3 22.9 35.4 31.7
CSL + Feature distill 26.5 25.8 26.3 43.9 37.1
CSL + NND 32.2 26.3 28.1 46.8 38.5
Table 4: Final accuracy on each task after training on 5-
task CIFAR-100. We use CSL as baseline to which we add
different distillations. NNCSL is equivalent to CSL + NND.
does not bring a significant improvement on the overall per-
formance, we find it useful for stabilizing the learning pro-
cess, especially for a small dataset with very limited super-
vision. Interestingly, PAWS and CSL (w/o filter) diverged
on ImageNet-100, as they cannot learn an effective repre-
sentation with increased scale, due to the instability intro-
duced by distribution drifts and forgetting of unlabeled data
(Sec. 4). We have included additional analysis on this col-
lapse behavior in the supplementary material.
Impact of distillation. Complementary to Fig. 1 (right),
which qualitatively shows the superior performance of our
proposed NND, we report in Tab. 4 the final accuracy of
each task after training on all tasks. An effective distillation
method should maintain the performance of old tasks (e.g.,
task 1 for NND vs. feature distillation) and efficiently learn
new tasks (e.g., task 5 for NND vs. knowledge distillation).
0 250 500 750 1000
Epoch020406080Training Accuracy
PAWS CSLFigure 6: Average training accuracy of unlabeled data on
5-task CIFAR-10. Comparison between vanilla PAWS and
our proposed CSL in the continual semi-supervised setting.
λLIN 1 0.05 0.005 0.001
Train (labeled) 99.9 99.9 97.9 96.5
Train (unlabeled) 74.7 76.0 76.3 75.2
Validation 77.1 78.4 78.9 77.3
Table 5: Training and validation accuracies on the current
task (i.e., evaluate on task twhile training on task t) with
different values of λLIN. Larger weights increase overfit-
ting on the labeled set and reduce generalization. These
experiments are conducted on CIFAR-10 with 5 tasks.
Evidence of effectiveness. We visualize in Fig. 6 the
learning curve of average accuracy for unlabeled training
data. As we illustrate in Sec. 5.1, the vanilla MEM loss is
detrimental to the learning as it forces the model to assign
incorrect pseudo-labels to the unlabeled data. Our proposed
CSL effectively resolves this issue and allows the model to
learn a better representation with the unlabeled data. Note
that we only use the labels of unlabeled data to monitor the
training, and no label information is leaked at train time.
Linear evaluation head. We report the train and valida-
tion accuracy in Tab. 5. Note that for the accuracy at train
time, we use the average of accuracy on the current task,
i.e., evaluation on task twhile training on task t. In general,
having a high training accuracy of unlabeled data means the
model learns a good representation, which leads to a good
validation accuracy. One can observe overfitting when λLIN
grows. This justifies our choice of i) having a small weight
for the linear evaluation head, and ii) choosing PAWS as the
basis, which does not directly use labeled data as training
samples to avoid overfitting. Moreover, we observe under-
fitting when λLINis smaller than 0.005. It confirms the need
for this linear classifier in the continual semi-supervised
scenario. We thus set λLINas0.005in our experiments.
Ablation of memory buffer. To verify the effectiveness
of the replay buffer, we evaluate our NNCSL without this
and observe a drastic decrease in performance (19.7% vs
73.2%) on 5-task CIFAR-10 with 0.8% of labeled data.
However, our method recovers 95% of the performance

--- PAGE 9 ---
(69.2% vs. 73.2%) while using only 10% of memory (50
vs. 500). We conjecture such data efficiency results from
the linear classifier that provides strong gradients in our
method. More details of this study can be found in the sup-
plementary material.
7. Conclusion
In this work, we studied continual semi-supervised
learning and proposed NNCSL, a novel approach based on
soft nearest-neighbors and distillation. Our extensive ex-
periments show the superior performance of NNCSL with
respect to existing methods, setting a new state of the art.
Previous work [3] showed that using a more powerful net-
work such as wider or deeper ResNet can further improve
performance. While this is not addressed in this work, we
consider it an interesting direction for future work. In this
paper, we considered a fixed ratio between labeled and un-
labeled samples across all tasks. A varying ratio would be
an even more challenging setting for future investigation.
Acknowledgements This work was funded in part by the
ANR grant A VENUE (ANR-18-CE23-0011). It was also
granted access to the HPC resources of IDRIS under the
allocation 2021-[AD011013084] made by GENCI.
Appendix
A. Implementation details
Although our model shares most of the hyper-parameters
across different datasets, there are few differences, in values
chosen empirically, to adapt to different scenarios. NNCSL,
as well as PAWS [3] and CSL for ablation study, are trained
with 250 epochs per task for CIFAR-10 and CIFAR-100,
and 100 epochs for ImageNet-100. For CIFAR-10, the
learning rate is initialized as 0.08, warmed up to 0.4, and
reduced to 0.032 with the cosine scheduler. For CIFAR-
100, a similar variation of learning rate is set from 0.08 to
1.2 to 0.032, and for ImageNet-100, it is 0.3 to 1.2 to 0.064.
The color distortion ratio is set to 1 for ImageNet-100 and
0.5 for CIFAR-10 and CIFAR-100. The size of the mini-
batch for labeled data is set to 5 for CIFAR-10 and 3 for
CIFAR-100 and ImageNet-100. The size of the mini-batch
for unlabeled data is set to 256 for CIFAR-10 and CIFAR-
100 and 64 for ImageNet-100. These hyper-parameters are
mostly based on the suggested default values of PAWS,
and we empirically update them after testing with a mod-
erate set of values variant around the default ones, based on
the validation performance. However, We do not perform
hyper-parameter tuning on ImageNet-100: we first adopt
the hyper-parameters for ImageNet from PAWS and update
them with the same changes we apply on CIFAR-100.
For the continual learning setting, we initialize a unified
linear evaluation head where the number of outputs is theMethod DatasetData Augmentation
Weak Strong
CCIC C10 72.8 69.4
CCIC C100 12.0 9.9
Table 6: Comparison of different data augmentation strate-
gies for CCIC on CIFAR-10 (denoted as C10 in the table)
and CIFAR-100 (denoted as C100 in the table).
Method Replay strategy Average Accuracy
NNCSL Labeled 76.7
NNCSL Labeled & Unlabeled 82.1
ORDisCo Labeled & GR 65.9
Table 7: Comparison of different strategies for the replay
buffer with 5-task CIFAR-10, using 3% of labeled data to
match the setting of [52].
total number of classes in the dataset. When a class is not
yet seen by the model, the corresponding output is masked.
To retain a copy of the previously trained model, we use the
deepcopy method from the copy package†
The copied model is in evaluation mode when training
the current model on the new classes.
We have included our source code as part of the sup-
plementary material. All the implementation details can be
found in the options files, for instance, random seeds, and
labeled samples on each dataset. We plan to open-source
our code upon acceptance of this submission.
B. Comparison of data augmentation
We note that the data augmentation of our proposed
framework is not the same as the one used in CCIC [8].
CCIC utilizes random cropping and horizontal flipping
(which we refer to as weak DA ), whereas our proposed CSL
and NNCSL include color distortion as an additional oper-
ation for data augmentation (referred to as strong DA ). To
verify the impact of this additional augmentation strategy,
we include color distortion in the data augmentation process
of CCIC and re-train it from scratch on CIFAR-10 (C10 in
the table) with 5 tasks, 5% labeled data and buffer size 5120,
and also on CIFAR-100 (C100 in the table) with 10 tasks,
0.8% labeled data and buffer size 5120. The results are re-
ported in Tab. 6. CCIC does not benefit from color distor-
tion on both datasets. We believe this is because CCIC does
not have the multiple-view consistency to be robust with re-
spect to strong data augmentation. Consequently, we chose
to report results using CCIC’s original (and more effective)
data augmentation.
†https://docs.python.org/3/library/copy.html

--- PAGE 10 ---
Buffer size 0 8 16 50 500 5120
NNCSL 19.7 37.7 53.2 69.2 73.2 73.7
Table 8: CIFAR-10 average accuracy wrt memory buffer
sizes with 5 tasks, 0.8% of labeled data.
C. Replay strategies
ORDisCo [52] utilizes a generative replay (GR) strategy
to replay unlabeled data. Given that the generative model
brings a memory overhead that is not negligible, it is rea-
sonable to equip our method with a memory buffer for unla-
beled data for a fair comparison. Specifically, we use 5000
samples, which is equivalent to the size of the generative
model of ORDisCo. Tab. 7 shows that having access to the
previously seen unlabeled data can indeed improve the per-
formance of our method, and our NNCSL performs better
with a simple memory buffer than ORDisCo with a sophisti-
cated generative-replay strategy. This experiment confirms
the ability of our method to exploit unlabeled data.
D. Ablation study of the memory buffer
We present the ablation study of the memory buffer in
a table, as shown in Tab. 8, for better readability. The use
of a memory buffer is a common practice in CL and most
replay-based methods would fail if the buffer is removed.
For instance, we observe a drastic decrease in performance
(19.7% vs. 73.2%) when no replay buffer is allowed. In
addition, increasing the memory buffer leads to improve-
ments in performance with a certain upper bound. The per-
formance plateaus when the memory buffer is larger than
500 (500 vs. 5000) because the buffer size is larger than the
total number of labeled data in this case (i.e., 400 labeled
samples).
E. Ambiguity of two-stage methods in CL
We show in Fig. 7 how two-stage methods (pre-training
and then fine-tuning) can be adapted to a CL setting. For
instance, after training on Task 1, one has to choose the
model to be utilized in the subsequent tasks. The left path
reuses the pre-trained model. It ensures the generalizability
for learning each new task but results in additional memory
overhead, as the fine-tuned model has to be saved for test-
ing. In contrast, the right path reuses the fine-tuned model,
which leads to a unified model for all tasks. However, the
overfitting caused by the small labeled set is detrimental to
the generalizability of the model. We explicitly show such
loss in Fig.7 by associating the size of the fine-tuned model
with its generalizability. On the right path, the fine-tuned
model is shrinking due to the overfitting issue.
TimeT1Fine-
tune Predict
T2 T2
T3 T3Task 1
Task 2
Task 3Pretrained
model
Fine-tuned
model
T1Test data
for T ask 1Fine-tune 
Representation
learning
Predict
Predict PredictPredictFigure 7: The ambiguity of two-stage methods in CL. On
the right path, the shrinking size of the fine-tuned model
after each task shows the loss of generalizability.
F. Training analysis on ImageNet-100
It is interesting to see that PAWS diverges in this setting,
as is shown in the Tab. 3 of the main paper. Our analysis
reveals that the vanilla MEM loss strongly impacts repre-
sentation learning on unlabeled data and makes the learning
procedure highly unstable, as shown in Fig. 8. Although
we do not observe the same collapse of PAWS on CIFAR-
10 or CIFAR-100, recall that in Fig. 6 of the main pa-
per, the training accuracy of unlabeled data for PAWS is
strongly constrained on CIFAR-10. This means the repre-
sentation learning of PAWS is already vulnerable. As im-
ages of ImageNet-100 have a much larger resolution than
that of CIFAR-10, learning a robust feature from the input
of ImageNet-100 is significantly more difficult. In such a
complex case, the model easily diverges but can hardly re-
cover, we suspect that it is because the gradient is very noisy
(due to the negative impact of MEM loss) and small (due to
the partial supervision and indirect use of labeled data). To
verify this assumption, we observe that adding the linear
head slightly alleviates the divergence. However, it cannot
prevent the collapse from happening. This means that the
MEM loss is the main cause of the collapse and is indeed
detrimental to representation learning.
Nevertheless, we believe it may be possible to resolve
this collapse without changing the framework, i.e., PAWS.
For example, one can conduct careful, extensive hyper-
parameter tuning to find an optimal set of parameters that
can stabilize the learning. However, it is not realistic given
the scale of the dataset. Hence, we did not conduct such
experiments.
G. Impact of λNND
In Tab. 9, we report the performance of our NNCSL with
respect to different values of λNND on CIFAR-100, with 5
tasks, 1% of labeled data and buffer size 5120. λNND con-
trols the importance of the distillation branch with respect
to the PAWS loss. The higher the value, the stronger con-
straint the model receives to retain the previous knowledge.
λNND= 0 means no distillation, which reduces the model
back to CSL. We can clearly see that distillation helps the

--- PAGE 11 ---
0 250 500 750 1000
Epoch02040Validation Accuracy(a) Validation accuracy of PAWS
0 250 500 750 1000
Epoch02040Training Accuracy
(b) Training accuracy for unlabeled data of PAWS
Figure 8: Learning curve of PAWS on ImageNet-100.
λNND 0 0.01 0.1 0.2 1
NNCSL 29.0 30.2 31.8 33.6 30.5
Table 9: Average Accuracy with different values of λNND.
These experiments are conducted on CIFAR-100 with 5
tasks, 1% of labeled data and buffer size 5120.
model perform better (e.g., λNND = 0 vs.λNND = 0.2)
and too much regularization from distillation can constraint
the model from learning new knowledge (e.g., λNND= 0.2
vs.λNND= 1).
H. Forward and backward transfer analysis
Forward transfer (FWT) and backward transfer (BWT)
are commonly used in continual learning literature [32,38].
The former measures the capacity of the model to gener-
alize to future tasks, whereas the latter shows the capacity
of the model to retain the previously acquired knowledge.
Specifically, they are defined as follows. Let Tagain be
the total number of tasks for the continual learning stages,
we therefore can divide the test set into Tsegments, each
one representing one task. After each task t, the model is
evaluated with respect to all Ttest sets. Consequently, we
obtain a matrix R∈RT×T, where the element Ri,jis the
test performance on task jwith the model on task i. Weuseclassification accuracy as our evaluation metrics. In ad-
dition, we define the random estimation as rj, which repre-
sents the test performance on task jusing a model with only
random initialization. We can define the FWT and BWT as:
FWT =1
T−1 TX
i=2Ri−1,i−ri!
. (8)
BWT =1
T−1 T−1X
i=1RT,i−Ri,i!
. (9)
Similarly, we can define the average accuracy (ACC) as:
ACC =1
T TX
i=1RT,i!
. (10)
It should be noticed that computing the backward transfer
for the first task or the forward transfer for the last task have
little utility and are excluded from Eq. 8 and Eq. 9.
We report the results in Tab. 10 a comparison of our pro-
posed components. Note that PAWS diverges in this setting,
leading to a low FWT. Instead, PAWS is better than CSL and
NNCSL if we look at BWT alone. It is simply because RT,i
andRi,iare all low after the divergence, having not much
room for the model to forget. That is, a model cannot forget
if it does not learn anything first. This observation confirms
the limitation of BWT, as it only shows a relative difference
with respect to its own performance, i.e., Eq. 9. Thus, BWT
is more suitable to be an additional indicator when the aver-
age accuracy of the two methods is close to each other, e.g.,
NNCSL vs. CSL. Comparing NNCSL and CSL, we no-
tice that the NND helps slightly improve the BWT. What is
more interesting is that NND significantly improves FWT.
We believe it is because NND stabilizes the representation
learning, allowing the model to generalize better to future
tasks.
We also notice that the absolute value of BWT is high
for both NNCSL and CSL. We suggest that it is because
the first task suffers the most from forgetting, as it is trained
with a simple task and without any regularization of distilla-
tion, but it goes through all continual stages. To verify this
assumption, we compute the BWT without the first task:
−11.3for NNCSL and −9.23for CSL, which are signifi-
cantly improved from the BWT scores in Tab. 10.
I. Visualization of the features
We use t-SNE [49] to project the learned features into a
lower-dimensional space and visualize them to qualitatively
verify the effectiveness of our proposed method. We apply
t-SNE on the deep features hu=h(zu)ofunlabeled data
and color them in the visualization with their ground-truth
label. Ideally, if the features are well learned, one can see

--- PAGE 12 ---
Method FWT ↑BWT↑
PAWS 1.1 -13.7
CSL 26.8 -18.25
NNCSL 31.7 -17.15
Table 10: Forward transfer (FWT) and backward transfer
(BWT) of PAWS, CSL and NNCSL in 20-task ImageNet-
100.
Figure 9: T-SNE visualization of deep features of 10 classes
of CIFAR-10, these experiments are conducted with 5 tasks.
Left: features from NNCSL after training on task 5, Right:
features from PAWS after training on task 5. Data points
are colored by their corresponding classes. A clear class
boundary after several tasks shows a robust representation
along the continual learning stages.
Figure 10: T-SNE visualization of deep features of the first
2 classes of CIFAR-10, these experiments are conducted
with 5 tasks. Left: features from NNCSL after training on
task 1, Middle: features from NNCSL after training on task
5, Right: features from PAWS after training on task 5. Data
points are colored by their corresponding classes. It is clear
that PAWS suffers from a blurry class boundary after sev-
eral continual learning stages.
different clusters representing different classes in the visual-
ization. Specifically, we choose 5-task CIFAR-10 to ensure
a distinguishable class boundary.
The result is shown in Fig. 9. The figure on the left shows
the features of all 10 classes after task 5, using NNCSL.
Recall that CIFAR-10 is divided into 5 tasks. We can see
a clear separation of different classes in the visualization.
Fig. 9 Right shows the features of the same 10 classes af-
ter task 5 using PAWS. We can clearly see that the vanilla
MEM loss of PAWS causes a blurry class boundary as it
tried to scatter over all classes with partially available unla-beled data.
To have a more detailed view on the feature space, we
select the first two classes as examples and visualize them
at different training stages using different methods. Fig. 10
confirms that PAWS leads to a blurry boundary and is prone
to severe forgetting due to this effect.
References
[1] Alessandro Achille, Tom Eccles, Loic Matthey, Christo-
pher P Burgess, Nick Watters, Alexander Lerchner, and Irina
Higgins. Life-long disentangled representation learning with
cross-domain latent homologies. NeurIPS , 2018. 2
[2] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor,
and Kevin McGuinness. Pseudo-labeling and confirmation
bias in deep semi-supervised learning. In IJCNN , 2020. 2
[3] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bo-
janowski, Armand Joulin, Nicolas Ballas, and Michael Rab-
bat. Semi-supervised learning of visual features by non-
parametrically predicting view assignments with support
samples. In ICCV , 2021. 1, 2, 3, 6, 9
[4] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and An-
drew Gordon Wilson. There are many consistent explana-
tions of unlabeled data: Why you should average. In ICLR ,
2019. 2
[5] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Ku-
rakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remix-
match: Semi-supervised learning with distribution alignment
and augmentation anchoring. In ICLR , 2020. 2
[6] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A
holistic approach to semi-supervised learning. NeurIPS , 32,
2019. 2
[7] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo
Porrello, and Simone Calderara. Class-incremental contin-
ual learning into the extended der-verse. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 45(5):5497–
5512, 2022. 6, 7
[8] Matteo Boschini, Pietro Buzzega, Lorenzo Bonicelli, Angelo
Porrello, and Simone Calderara. Continual semi-supervised
learning through contrastive interpolation consistency. Pat-
tern Recognition Letters , 162:9–14, 2022. 1, 2, 3, 6, 7, 9
[9] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for general
continual learning: a strong, simple baseline. In NeurIPS ,
2020. 2
[10] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-
mone Calderara. Rethinking experience replay: a bag of
tricks for continual learning. In ICPR , 2021. 1
[11] Francisco M Castro, Manuel J Marin-Jimenez, Nicolas Guil,
Cordelia Schmid, and Karteek Alahari. End-to-end incre-
mental learning. In ECCV , 2018. 1, 2
[12] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Con-
trastive continual learning. In CVPR , 2021. 1, 2
[13] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremen-
tal learning: Understanding forgetting and intransigence. In
ECCV , 2018. 2

--- PAGE 13 ---
[14] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with a-
gem. In ICLR , 2019. 2
[15] Baixu Chen, Junguang Jiang, Ximei Wang, Jianmin Wang,
and Mingsheng Long. Debiased pseudo labeling in self-
training. arXiv preprint arXiv:2202.07136 , 2022. 1
[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 6
[17] Yanbei Chen, Massimiliano Mancini, Xiatian Zhu, and
Zeynep Akata. Semi-supervised and unsupervised deep vi-
sual learning: A survey. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 2022. 2
[18] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ale ˇs Leonardis, Gregory Slabaugh, and
Tinne Tuytelaars. A continual learning survey: Defying for-
getting in classification tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 44(7):3366–3385, 2021.
1, 2, 6, 7
[19] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distil-
lation for small-tasks incremental learning. In ECCV , 2020.
2
[20] Arthur Douillard, Alexandre Ram ´e, Guillaume Couairon,
and Matthieu Cord. Dytox: Transformers for continual learn-
ing with dynamic token expansion. In CVPR , 2022. 1
[21] Enrico Fini, Victor G Turrisi da Costa, Xavier Alameda-
Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-
supervised models are continual learners. In CVPR , 2022. 2
[22] Enrico Fini, St `ephane Lathuili `ere, Enver Sangineto, Moin
Nabi, and Elisa Ricci. Online continual learning under ex-
treme memory constraints. In ECCV , 2020. 2, 5
[23] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2(7), 2015. 5
[24] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. In CVPR , 2019. 2
[25] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proc. of the national academy of sciences ,
2017. 2, 6
[26] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, Univ. Toronto, 2009. 5
[27] Samuli Laine and Timo Aila. Temporal ensembling for semi-
supervised learning. In ICLR , 2017. 2
[28] Alexis Lechat, St ´ephane Herbin, and Fr ´ed´eric Jurie. Semi-
supervised class incremental learning. In ICPR , 2021. 2
[29] Dong-Hyun Lee et al. Pseudo-label: The simple and effi-
cient semi-supervised learning method for deep neural net-
works. In Workshop on challenges in representation learn-
ing, ICML , 2013. 1, 2
[30] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 2018. 2, 5[31] Zicheng Liu, Siyuan Li, Ge Wang, Cheng Tan, Lirong Wu,
and Stan Z Li. Decoupled mixup for data-efficient learning.
arXiv preprint arXiv:2203.10761 , 2022. 2
[32] David Lopez-Paz and Marc-Aurelio Ranzato. Gradient
episodic memory for continual learning. In NeurIPS , 2017.
2, 11
[33] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken
Nakae, and Shin Ishii. Distributional smoothing with virtual
adversarial training. In ICLR , 2016. 2
[34] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jah-
nichen, and Moin Nabi. Learning to remember: A synaptic
plasticity driven framework for continual learning. In CVPR ,
2019.
[35] Sungrae Park, JunKeon Park, Su-Jin Shin, and Il-Chul Moon.
Adversarial dropout for supervised and semi-supervised
learning. In AAAI , 2018. 2
[36] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan
Yuille. Deep co-training for semi-supervised image recogni-
tion. In ECCV , 2018. 2
[37] Dushyant Rao, Francesco Visin, Andrei A Rusu, Yee Whye
Teh, Razvan Pascanu, and Raia Hadsell. Continual unsuper-
vised representation learning. NeurIPS , 2019. 2
[38] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H. Lampert. iCaRL: Incremental clas-
sifier and representation learning. In CVPR , 2017. 2, 3, 6,
11
[39] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. NeurIPS , 32, 2019. 6, 7
[40] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J.
Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell.
Progressive neural networks. arXiv 1606.04671 . 2
[41] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
Regularization with stochastic transformations and perturba-
tions for deep semi-supervised learning. NeurIPS , 29, 2016.
2
[42] Joan Serra, Didac Suris, Marius Miron, and Alexandros
Karatzoglou. Overcoming catastrophic forgetting with hard
attention to the task. ICML , 2018. 2
[43] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In NeurIPS ,
2017. 2
[44] James Smith, Jonathan Balloch, Yen-Chang Hsu, and Zsolt
Kira. Memory-efficient semi-supervised continual learning:
The world is its own replay buffer. In IJCNN , 2021. 1, 2
[45] James Smith, Cameron Taylor, Seth Baer, and Constantine
Dovrolis. Unsupervised progressive learning and the stam
architecture. arXiv preprint arXiv:1904.02021 , 2019. 2
[46] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence.
NeurIPS , 2020. 1, 2
[47] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. NeurIPS , 30, 2017. 2

--- PAGE 14 ---
[48] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. In ECCV , 2020. 6
[49] Laurens Van der Maaten and Geoffrey Hinton. Visualiz-
ing data using t-sne. Journal of machine learning research ,
9(11), 2008. 11
[50] Vikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala,
Yoshua Bengio, and David Lopez-Paz. Interpolation consis-
tency training for semi-supervised learning. In IJCAI , 2019.
2
[51] Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan
Zhan. Foster: Feature boosting and compression for class-
incremental learning. In ECCV . Springer, 2022. 6, 7
[52] Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong,
Zhenguo Li, and Jun Zhu. Ordisco: Effective and efficient
usage of incremental unlabeled data for semi-supervised
continual learning. In CVPR , 2021. 1, 2, 6, 7, 9, 10
[53] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A
comprehensive survey of continual learning: Theory, method
and application. arXiv preprint arXiv:2302.00487 , 2023. 1
[54] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. In CVPR , 2019. 2
[55] Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V
Le. Self-training with noisy student improves imagenet clas-
sification. In CVPR , 2020. 2
[56] Yi Xu, Lei Shang, Jinxing Ye, Qi Qian, Yu-Feng Li, Baigui
Sun, Hao Li, and Rong Jin. Dash: Semi-supervised learning
with dynamic thresholding. In ICML , 2021. 2
[57] Yang You, Igor Gitman, and Boris Ginsburg. Large
batch training of convolutional networks. arXiv preprint
arXiv:1708.03888 , 2017. 6
[58] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lu-
cas Beyer. S4l: Self-supervised semi-supervised learning. In
ICCV , 2019. 1
[59] Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jin-
dong Wang, Manabu Okumura, and Takahiro Shinozaki.
Flexmatch: Boosting semi-supervised learning with curricu-
lum pseudo labeling. NeurIPS , 2021. 2
[60] Liheng Zhang and Guo-Jun Qi. Wcp: Worst-case perturba-
tions for semi-supervised deep learning. In CVPR , 2020. 2
