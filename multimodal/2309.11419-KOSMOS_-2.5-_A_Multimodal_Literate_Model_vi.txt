# 2309.11419.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.11419.pdf
# Kích thước file: 6426197 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
KOSMOS-2.5: Một Mô hình Đa phương tiện Biết Đọc Viết
Tengchao Lv*, Yupan Huang*, Jingye Chen*, Yuzhong Zhao, Yilin Jia, Lei Cui†,
Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong,
Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei†
Microsoft
aka.ms/GeneralAI

Tóm tắt
Việc đọc tự động các hình ảnh có nhiều văn bản đại diện cho một bước tiến đáng kể hướng tới việc đạt được Trí tuệ Nhân tạo Tổng quát (AGI). Trong bài báo này, chúng tôi trình bày KOSMOS-2.5, một mô hình đa phương tiện biết đọc viết dành cho máy đọc các hình ảnh có nhiều văn bản. Được tiền huấn luyện trên một bộ dữ liệu quy mô lớn các hình ảnh có nhiều văn bản, KOSMOS-2.5 xuất sắc trong hai nhiệm vụ chuyển đổi riêng biệt nhưng bổ sung cho nhau: (1) tạo ra các khối văn bản nhận biết không gian, trong đó mỗi khối văn bản được gán tọa độ không gian trong hình ảnh, và (2) sản xuất đầu ra văn bản có cấu trúc nắm bắt cả kiểu dáng và cấu trúc ở định dạng markdown. Khả năng đọc viết đa phương tiện thống nhất này được đạt được thông qua một kiến trúc Transformer tự hồi quy chỉ có bộ giải mã chia sẻ và các lời nhắc cụ thể cho từng nhiệm vụ. Dựa trên nền tảng này, chúng tôi tinh chỉnh KOSMOS-2.5 cho các nhiệm vụ hiểu tài liệu, tạo ra một chuyên gia hiểu tài liệu có tên KOSMOS-2.5-CHAT. Ngoài ra, một bộ dữ liệu lớn gồm 357.4 triệu trang tài liệu trải rộng trên các lĩnh vực đa dạng đã được tuyển chọn để tiền huấn luyện. Chúng tôi đánh giá KOSMOS-2.5 trên hai bộ đánh giá mới được đề xuất, OCREval và MarkdownEval, cho nhận dạng văn bản cấp tài liệu và tạo hình ảnh thành markdown, thể hiện khả năng đọc viết ấn tượng có thể so sánh với GPT-4o. KOSMOS-2.5-CHAT đạt hiệu suất có thể so sánh với các chuyên gia tiên tiến khác lớn gấp năm lần (1.3B so với 7B) trên chín bộ đánh giá hỏi đáp hình ảnh nhiều văn bản. Các mô hình và mã đã được cung cấp tại https://aka.ms/kosmos25.

1 Giới thiệu
Các mô hình ngôn ngữ lớn đa phương tiện (MLLMs) mở rộng khả năng của các mô hình ngôn ngữ lớn (LLMs) cho các nhiệm vụ đa phương tiện, cho phép chúng xử lý và tạo ra các phản hồi từ cả đầu vào văn bản và hình ảnh (Zhang et al. 2023b; Liu et al. 2024b; ChatGPT 2022; Touvron et al. 2023). Tuy nhiên, trong khi các MLLMs hiện tại chủ yếu tập trung vào hình ảnh tự nhiên, thách thức về việc đọc và hiểu hiệu quả các hình ảnh có nhiều văn bản—như bài báo học thuật, hóa đơn, tài liệu thiết kế, và trang web—vẫn chưa được khám phá đầy đủ.

Các phương pháp Nhận dạng Ký tự Quang học (OCR) truyền thống chủ yếu được thiết kế để tạo ra nội dung văn bản cấp dòng và nắm bắt vị trí không gian của nó trong hình ảnh. Mặc dù các phương pháp này bảo tồn thông tin bố cục, chúng thường bỏ qua thứ tự đọc cấp tài liệu và tính toàn vẹn cấu trúc mà rất quan trọng cho việc hiểu tài liệu chính xác. Mặt khác, văn bản định dạng markdown mang lại lợi thế đáng kể so với văn bản thuần túy bằng cách phân biệt rõ ràng giữa các yếu tố cấu trúc khác nhau—như bảng, danh sách, và tiêu đề—thông qua các token cụ thể. Các cách tiếp cận hiện tại hoặc bị hạn chế ở việc nhận dạng văn bản cấp dòng (Ye et al. 2023a; Hu et al. 2024; Li et al. 2023c) hoặc tập trung vào việc phân tích cấu trúc trong một danh mục tài liệu cụ thể (Blecher et al. 2023), làm cho việc đạt được khả năng đọc và hiểu tài liệu cấp độ toàn diện trên các danh mục đa dạng trở nên khó khăn.

Được thúc đẩy bởi những quan sát này, chúng tôi trình bày KOSMOS-2.5, một mô hình đa phương tiện biết đọc viết được thiết kế để giải quyết những thách thức độc đáo của việc đọc và hiểu các tài liệu có nhiều văn bản, bao gồm việc nắm bắt thứ tự đọc và tính toàn vẹn cấu trúc của nội dung. Như được minh họa trong Hình 1, KOSMOS-2.5 được tiền huấn luyện trên hai nhiệm vụ tạo sinh riêng biệt nhưng bổ sung cho nhau: nhận dạng văn bản cấp tài liệu và tạo hình ảnh thành markdown. Nhiệm vụ đầu tiên bao gồm việc tạo ra các khối văn bản nhận biết không gian, gán các dòng văn bản với tọa độ không gian tương ứng trong hình ảnh có nhiều văn bản gốc. Nhiệm vụ thứ hai tập trung vào việc sản xuất đầu ra văn bản có cấu trúc nắm bắt cả kiểu dáng và cấu trúc ở định dạng markdown. Cả hai nhiệm vụ đều được thực hiện trong một khung thống nhất sử dụng các lời nhắc cụ thể cho từng nhiệm vụ, tận dụng một kiến trúc Transformer chia sẻ kết hợp bộ mã hóa thị giác dựa trên ViT và bộ giải mã ngôn ngữ dựa trên Transformer được kết nối bởi một mô-đun tái lấy mẫu (Dosovitskiy et al. 2021; Lee et al. 2023; Alayrac et al. 2022).

Để thực hiện tiềm năng của mô hình tiền huấn luyện và xác thực hiệu quả của nó trong các nhiệm vụ hiểu ở hạ nguồn, chúng tôi tiếp tục tinh chỉnh KOSMOS-2.5 cho các nhiệm vụ hiểu tài liệu, tạo ra KOSMOS-2.5-CHAT, có thể trả lời các câu hỏi do người dùng cung cấp về hình ảnh có nhiều văn bản. Mặc dù chỉ có 1.3B tham số, KOSMOS-2.5-CHAT đạt hiệu suất có thể so sánh với các chuyên gia tiên tiến khác có hơn 7B tham số trên các bộ đánh giá hỏi đáp hình ảnh nhiều văn bản đa dạng.

Do không có bộ dữ liệu đọc tài liệu toàn diện, chúng tôi đã tuyển chọn một bộ dữ liệu lớn gồm 357.4 triệu trang tài liệu, bao gồm tài liệu được quét, tài liệu chung, bài báo học thuật, trang web, hình ảnh thiết kế, văn bản viết tay, nội dung toán học, và tài liệu dự án. Mỗi tài liệu được chú thích với các dòng văn bản có hộp giới hạn hoặc định dạng markdown. Bộ dữ liệu này được xây dựng bằng cách sử dụng một quy trình tự động cho thu thập dữ liệu, lọc, và kiểm soát chất lượng, cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai.

Các bộ đánh giá đọc tài liệu hiện tại chủ yếu tập trung vào khả năng đọc văn bản cấp dòng (Liu et al. 2023b) hoặc bị hạn chế trong các lĩnh vực cụ thể, như chuyển đổi bài báo học thuật sang định dạng markdown (Blecher et al. 2023). Để đánh giá toàn diện khả năng của các mô hình trong nhận dạng văn bản cấp tài liệu và các nhiệm vụ tạo hình ảnh thành markdown, chúng tôi giới thiệu hai bộ đánh giá mở rộng: OCREval và MarkdownEval. Cụ thể, OCREval chứa 2,297 mẫu, trong khi MarkdownEval bao gồm 5,633 mẫu. Các bộ đánh giá bao phủ một phạm vi đa dạng các danh mục tài liệu, bao gồm văn bản viết tay, tài liệu thiết kế, hóa đơn, bài báo học thuật, trang web, nội dung toán học, bảng, và nhiều hơn nữa.

Kết quả thực nghiệm trên các bộ đánh giá này cho thấy KOSMOS-2.5 thể hiện khả năng đọc viết ấn tượng có thể so sánh với GPT-4o (GPT-4 2023).

Các đóng góp của công trình này được tóm tắt như sau:
• Chúng tôi đề xuất hai nhiệm vụ đọc tài liệu riêng biệt nhưng hợp tác để tiền huấn luyện một mô hình tài liệu cơ bản có khả năng đọc và hiểu thứ tự và cấu trúc của các tài liệu có nhiều văn bản. KOSMOS-2.5 được tiền huấn luyện thể hiện khả năng đọc viết đa phương tiện ấn tượng có thể so sánh với GPT-4o, và KOSMOS-2.5-CHAT được tinh chỉnh đạt kết quả cạnh tranh trên chín bộ đánh giá hiểu tài liệu.
• Chúng tôi đã tuyển chọn một bộ dữ liệu lớn và đa dạng bao gồm 357.4 triệu hình ảnh tài liệu có nhiều văn bản, với các dòng văn bản được chú thích bằng hộp giới hạn hoặc ở định dạng markdown. Quy trình tuyển chọn dữ liệu tự động cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai.
• Chúng tôi giới thiệu hai bộ đánh giá toàn diện, OCREval và MarkdownEval, để cung cấp các đánh giá kỹ lưỡng về khả năng đọc máy cấp tài liệu.

2 KOSMOS-2.5
2.1 Kiến trúc Mô hình
Kiến trúc của KOSMOS-2.5 bao gồm một bộ mã hóa thị giác và một bộ giải mã ngôn ngữ, được kết nối thông qua một mô-đun tái lấy mẫu để giảm độ dài chuỗi của hình ảnh (Alayrac et al. 2022), như được minh họa trong Hình 2. Bộ mã hóa thị giác được khởi tạo từ bộ mã hóa của mô hình Pix2Struct-Large (Lee et al. 2023), dựa trên Vision Transformer (ViT) (Dosovitskiy et al. 2021). Phù hợp với Pix2Struct (Lee et al. 2023), chúng tôi sử dụng chiến lược độ phân giải biến đổi và trích xuất số lượng tối đa các patch có kích thước cố định có thể vừa trong một độ dài chuỗi được định nghĩa trước.

Bộ tái lấy mẫu nén chuỗi hình ảnh thành một số token cố định ngắn hơn:
H0=f(I) (1)
H1=Attention(V,[V;H0],[V;H0]) (2)
trong đó I là hình ảnh đầu vào, f là hàm mã hóa, V đại diện cho một tập hợp các token mềm được định nghĩa trước, và [;] biểu thị toán tử nối. Bộ giải mã ngôn ngữ dựa trên kiến trúc Transformer và được thiết kế để điều kiện hóa trên cả ngữ cảnh hình ảnh và văn bản để dự đoán token tiếp theo. Chi tiết về các siêu tham số có thể tìm thấy trong Phụ lục A.1.

--- TRANG 2 ---
KOSMOS 2.5: Một Mô hình Đa phương tiện Biết Đọc Viết

[Hình minh họa kiến trúc và quy trình]

Hình 1: KOSMOS-2.5 là một mô hình nền tảng tài liệu đa phương tiện nhận hình ảnh văn bản làm đầu vào và tạo ra văn bản nhận biết không gian (tức là văn bản với hộp giới hạn) hoặc văn bản định dạng markdown (tức là văn bản với các yếu tố markdown), theo các lời nhắc nhiệm vụ khác nhau tương ứng. Mô hình sở hữu khả năng nhận thức toàn diện nội dung văn bản, ngữ cảnh không gian của nó, và các sắc thái về định dạng và kiểu dáng trong một khung thống nhất. KOSMOS-2.5-CHAT được tinh chỉnh từ KOSMOS-2.5. Đây là một chuyên gia hiểu tài liệu thị giác có thể trả lời các câu hỏi do người dùng cung cấp về hình ảnh có nhiều văn bản từ các lĩnh vực khác nhau.

[Tiếp tục với phần còn lại của trang 2...]

--- TRANG 3 ---
[Kiến trúc Vision Encoder, Resampler và shared decoder-only Transformer]

Hình 2: Kiến trúc mô hình của KOSMOS-2.5 tận dụng một kiến trúc Transformer chia sẻ kết hợp bộ mã hóa thị giác dựa trên ViT và bộ giải mã ngôn ngữ dựa trên Transformer được kết nối bởi một mô-đun tái lấy mẫu.

[Bảng 1 với các định nghĩa nhiệm vụ và định dạng]

Bảng 1: Các nhiệm vụ, lời nhắc, và định dạng chuỗi phản hồi được sử dụng để huấn luyện KOSMOS-2.5. Các token đặc biệt <s> và </s> biểu thị ranh giới chuỗi, trong khi <image> và </image> chỉ ra sự bắt đầu và kết thúc của embeddings hình ảnh. Đối với các nhiệm vụ nhận dạng văn bản cấp tài liệu, toán tử ⊕ đại diện cho việc nối dòng văn bản Tn và hộp giới hạn Bn của nó. Trong quá trình tiền huấn luyện, các token đặc biệt <ocr> và <md> biểu thị các nhiệm vụ nhận dạng văn bản cấp tài liệu và tạo văn bản thành markdown tương ứng. Đối với các nhiệm vụ hiểu tài liệu thị giác, chúng tôi sử dụng cùng định dạng như các nhiệm vụ tạo văn bản thành markdown vì chúng không yêu cầu đầu ra hộp giới hạn.

2.2 Biểu diễn Hình ảnh và Văn bản
Biểu diễn hình ảnh được suy ra từ bộ mã hóa hình ảnh và bộ tái lấy mẫu như được mô tả trong Phần 2.1. Biểu diễn văn bản được thu được thông qua tokenization và embedding văn bản. Đối với văn bản markdown, chúng tôi trực tiếp tokenize nó trong khi bảo tồn tất cả các ký tự đặc biệt và chỉ báo định dạng. Đối với các dòng văn bản có hộp giới hạn, chúng tôi chuyển đổi tọa độ thành các token vị trí rời rạc, tương tự như KOSMOS-2 (Peng et al. 2023).

Chúng tôi giới thiệu một tập hợp 2L + 2 token chuyên biệt: <x0>, <x1>, ..., <xL−1>, <y0>, ..., <yL−1>, <bbox>, và </bbox>, tương ứng với tọa độ và các điểm đánh dấu bắt đầu và kết thúc của hộp giới hạn. Tọa độ được thu được bằng cách làm tròn xuống các vị trí thực tế sau khi thay đổi kích thước hình ảnh.

Xét một tài liệu T với N dòng văn bản. Mỗi dòng được biểu diễn là Tn={w(n)1, w(n)2, ..., w(n)Mn}, trong đó Mn là số từ trong dòng văn bản thứ n. Hộp giới hạn cho Tn sau đó được biểu diễn là Bn=<bbox><x(n)tl><y(n)tl><x(n)br><y(n)br></bbox>, trong đó tọa độ đại diện cho các góc trên-trái và dưới-phải của hộp giới hạn.

2.3 Tiền huấn luyện trên Đọc Tài liệu
Nhiệm vụ Tiền huấn luyện. Các nhiệm vụ Nhận dạng Ký tự Quang học (OCR) truyền thống chủ yếu tập trung vào việc tạo ra nội dung văn bản cấp dòng và nắm bắt vị trí không gian của nó trong hình ảnh. Trong khi OCR bảo tồn vị trí bố cục của văn bản tài liệu, nó thường bỏ qua thứ tự đọc cấp tài liệu và tính toàn vẹn cấu trúc, cả hai đều rất quan trọng cho việc hiểu tài liệu toàn diện. Ngược lại, văn bản định dạng markdown cung cấp lợi thế so với văn bản thuần túy bằng cách phân biệt rõ ràng các yếu tố cấu trúc khác nhau, như bảng và danh sách, sử dụng các token cụ thể.

Để học hiệu quả bố cục và cấu trúc của tài liệu, chúng tôi đề xuất hai nhiệm vụ tạo sinh bổ sung cho việc tiền huấn luyện một mô hình tài liệu cơ bản: nhận dạng văn bản cấp tài liệu

--- TRANG 4 ---
[Bảng 2 với tóm tắt dữ liệu huấn luyện]

Bảng 2: Tóm tắt dữ liệu được sử dụng để tiền huấn luyện KOSMOS-2.5, bao gồm mô tả của mỗi danh mục tài liệu, số lượng trang, và tỷ lệ lấy mẫu tương ứng trong dữ liệu huấn luyện.

[Bảng 3 so sánh dữ liệu tiền huấn luyện]

Bảng 3: So sánh dữ liệu tiền huấn luyện được sử dụng bởi các mô hình đa phương tiện tài liệu.

và tạo hình ảnh thành markdown, như được trình bày chi tiết trong Bảng 1.

Mục tiêu và Định dạng Huấn luyện. Chúng tôi huấn luyện mô hình để dự đoán đầu ra dựa trên ngữ cảnh hình ảnh đầu vào và các lời nhắc cụ thể cho từng nhiệm vụ. Mục tiêu huấn luyện là tối thiểu hóa mất mát cross-entropy cho việc dự đoán token tiếp theo, thường được biết đến như mô hình ngôn ngữ tự hồi quy (Radford et al. 2018). Bảng 1 minh họa các định dạng cho lời nhắc huấn luyện mô hình và chuỗi phản hồi.

Lời nhắc được xây dựng bằng cách nối biểu diễn hình ảnh với token đặc biệt cụ thể cho từng nhiệm vụ. Phản hồi tương ứng với đầu ra văn bản của các nhiệm vụ: các dòng văn bản với hộp giới hạn cho nhận dạng văn bản cấp tài liệu và văn bản markdown cho nhiệm vụ tạo hình ảnh thành markdown. Một ví dụ định tính được cung cấp trong Phụ lục A.4 để minh họa đầu vào và đầu ra của mô hình.

Dữ liệu Tiền huấn luyện. Dữ liệu huấn luyện của chúng tôi được thu thập bằng cách sử dụng một quy trình tự động từ các nguồn đa dạng, tạo ra một bộ dữ liệu lớn gồm 357.4 triệu hình ảnh tài liệu, được chú thích với các dòng văn bản sử dụng hộp giới hạn hoặc ở định dạng markdown. Như được hiển thị trong Bảng 2, bộ dữ liệu tiền huấn luyện của chúng tôi bao gồm một phạm vi rộng các loại tài liệu, bao gồm tài liệu quét, bài báo học thuật, trang web, hình ảnh thiết kế, nội dung toán học, văn bản viết tay, và nhiều hơn nữa. So sánh với dữ liệu huấn luyện được sử dụng bởi các mô hình hiện tại trong Bảng 3, KOSMOS-2.5 tận dụng bộ dữ liệu lớn nhất và đa dạng nhất, điều này cải thiện đáng kể khả năng thích ứng và tổng quát hóa của mô hình trên các lĩnh vực khác nhau.

Chúng tôi áp dụng lọc và kiểm soát chất lượng trong quá trình tuyển chọn dữ liệu. Chúng tôi sử dụng fastText để xác định ngôn ngữ (với ngưỡng 0.5) để lọc ra các tài liệu không phải tiếng Anh từ toàn bộ bộ dữ liệu tiền huấn luyện. Để đảm bảo tính đa dạng nội dung trong mỗi nguồn, chúng tôi sử dụng MinHash (Broder 1997) để xác định và loại bỏ các trang dư thừa, áp dụng các tham số giống như (Lee et al. 2021), với các cặp tài liệu có điểm tương đồng 0.8 trở lên được đánh dấu là trùng lặp.

Đối với dữ liệu hình ảnh thành markdown có nguồn gốc từ README, DOCX, LATEX, và HTML, chúng tôi gặp phải sự khác biệt giữa nội dung trong hình ảnh văn bản và chuỗi markdown tương ứng của chúng do các vấn đề chuyển đổi. Để tinh chỉnh dữ liệu, chúng tôi đánh giá sự chồng lấp token giữa hình ảnh và tệp markdown, yêu cầu tỷ lệ giao điểm-hợp tập token lớn hơn 0.95 để bao gồm. Chi tiết về các quy trình xử lý cho mỗi danh mục tài liệu được cung cấp trong Phụ lục A.5, cùng với dữ liệu huấn luyện mẫu trong Phụ lục A.8, nhằm cung cấp các hướng dẫn minh bạch và có thể tái tạo cho nghiên cứu và ứng dụng tương lai.

--- TRANG 5 ---
[Bảng 4 và 5 với tóm tắt benchmarks]

Bảng 4: Tóm tắt các danh mục tài liệu, nguồn dữ liệu, và số lượng mẫu trong benchmark OCREval.

Bảng 5: Tóm tắt các danh mục tài liệu, nguồn dữ liệu, và số lượng mẫu trong benchmark MarkdownEval.

2.4 Tinh chỉnh trên Hiểu Tài liệu
Chúng tôi tinh chỉnh KOSMOS-2.5 trên các bộ dữ liệu hiểu tài liệu, gọi mô hình được tinh chỉnh là KOSMOS-2.5-CHAT. KOSMOS-2.5-CHAT được thiết kế để trả lời các câu hỏi đa dạng do người dùng cung cấp về hình ảnh có nhiều văn bản từ các lĩnh vực khác nhau. Để bảo tồn tốt hơn khả năng đọc của KOSMOS-2.5, chúng tôi đóng băng bộ mã hóa thị giác của mô hình tiền huấn luyện và tinh chỉnh bộ tái lấy mẫu và mô hình ngôn ngữ sử dụng lời nhắc nhiệm vụ hiểu tài liệu (Dòng 3 trong Bảng 1), trong đó [Question] và [Answer] đại diện cho một cặp câu hỏi-câu trả lời từ bộ dữ liệu.

3 Thí nghiệm
3.1 Cấu hình Mô hình và Huấn luyện
Theo Pix2Struct (Lee et al. 2023), chúng tôi sử dụng giai đoạn khởi động ngắn 20k bước để tạo điều kiện hội tụ nhanh hơn trong giai đoạn tiền huấn luyện. Trong giai đoạn này, mô hình học cách đọc các đoạn văn bản từ hình ảnh tổng hợp được hiển thị với màu sắc và phông chữ ngẫu nhiên. Do khối lượng dữ liệu dựa trên bố cục lớn hơn đáng kể so với dữ liệu dựa trên đánh dấu, chúng tôi ban đầu huấn luyện mô hình trong 100k bước chỉ sử dụng bộ dữ liệu dựa trên bố cục. Sau đó chúng tôi kết hợp hai bộ dữ liệu cho thêm 140k bước huấn luyện. Tổng cộng việc huấn luyện liên quan đến khoảng 260 tỷ token.

Tokenization văn bản của chúng tôi dựa trên tokenizer cl100k base tiktoken, với 8,194 token chuyên biệt được giới thiệu cho tọa độ và đánh dấu hộp giới hạn. Các word embedding mới được thêm cho các token vị trí được khởi tạo ngẫu nhiên, với tất cả các tham số được cập nhật trong quá trình huấn luyện. Chúng tôi cũng kết hợp các kỹ thuật tăng cường dữ liệu từ TrOCR (Li et al. 2022b) để tăng cường tính mạnh mẽ của mô hình.

KOSMOS-2.5 chứa tổng cộng 1.3 tỷ tham số. Chi tiết thêm về kiến trúc mô hình và siêu tham số huấn luyện được cung cấp trong Phụ lục A.1.

3.2 Đánh giá trên Đọc Tài liệu
Benchmarks. Để đánh giá toàn diện khả năng của các mô hình trong nhận dạng văn bản cấp tài liệu và các nhiệm vụ tạo hình ảnh thành markdown, chúng tôi đã thu thập các benchmark OCREval và MarkdownEval. Benchmark OCREval bao gồm 2,297 hình ảnh từ các tập thử nghiệm của 13 bộ dữ liệu, bao phủ các danh mục như nội dung toán học, hình ảnh viết tay, hình ảnh thiết kế, hóa đơn, tài liệu sinh số, và trang web. Benchmark MarkdownEval bao gồm 5,693 hình ảnh trải rộng trên các danh mục như phương trình toán học, bài báo học thuật, bảng, tài liệu chung, và tài liệu dự án. Các danh mục, nguồn dữ liệu, và số lượng mẫu tương ứng được chi tiết trong Bảng 4 và Bảng 5. Chi tiết xử lý dữ liệu thêm được cung cấp trong Phụ lục A.6.

Metrics. Các metrics cho OCREval bao gồm F1 cấp từ, IOU, và NED để đánh giá hiệu suất OCR cấp tài liệu. Các metrics cho MarkdownEval bao gồm Normalized Edit Distance (NED) và Normalized Tree Edit Distance (NTED) để đánh giá tạo hình ảnh thành markdown. NED là một metric so sánh dựa trên chuỗi, trong khi NTED đo khoảng cách chỉnh sửa cây được chuẩn hóa bởi số lượng nút, nắm bắt sự khác biệt cấu trúc trong cây phân tích. Khung đánh giá kép này xem xét cả tính chính xác từ vựng và việc bảo tồn cấu trúc phân cấp gốc vốn có trong định dạng Markdown. Chi tiết thêm về các metrics đánh giá được cung cấp trong Phụ lục A.2 và Phụ lục A.3.

Kết quả. KOSMOS-2.5 là một khung thống nhất tạo điều kiện đa nhiệm vụ với các nhiệm vụ được xác định bởi các lời nhắc được cung cấp. Chúng tôi so sánh KOSMOS-2.5 với các mô hình đọc tài liệu tiên tiến trên OCREval (Bảng 6) và MarkdownEval (Bảng 7). Đối với nhiệm vụ nhận dạng văn bản cấp tài liệu, KOSMOS-2.5 vượt trội hơn các mô hình hiện có trong việc đọc hình ảnh có nhiều văn bản. Ví dụ, KOSMOS-2.5 vượt trội hơn Vary Base với một biên độ đáng kể mặc dù có kích thước mô hình nhỏ hơn (1.3B so với 7B tham số). KOSMOS-2.5 cũng đạt hiệu suất tốt nhất trên tất cả các loại hình ảnh trên MarkdownEval. Đáng chú ý, việc bỏ qua các ký hiệu markdown của GPT-4o đã ảnh hưởng nhẹ đến điểm NTED của nó. Ví dụ, trong khi e² nên được biểu diễn là e<sup>2</sup> trong markdown, GPT-4o xuất ra e2 trực tiếp. Đối với các mô hình tuân thủ tiêu chuẩn markdown (ví dụ, Vary và Nougat), KOSMOS-2.5 liên tục vượt trội hơn chúng, hưởng lợi từ việc hiểu bố cục tốt hơn trong nhận dạng văn bản.

--- TRANG 6 ---
[Bảng 6 với kết quả thực nghiệm OCREval]

Bảng 6: Kết quả thực nghiệm cho nhiệm vụ nhận dạng văn bản cấp tài liệu trên OCREval. Các metrics được báo cáo như F1 ↑/ IOU↑/ NED↑. Vì Nougat và Vary chỉ tạo ra đầu ra văn bản mà không có hộp giới hạn, điểm IOU không khả dụng cho các mô hình này.

[Bảng 7 với kết quả MarkdownEval]

Bảng 7: Kết quả thực nghiệm cho tạo markdown cấp tài liệu trên MDEval. Các metrics được báo cáo như NED ↑/ NTED ↑.

3.3 Đánh giá trên Hiểu Tài liệu
Cài đặt. Được tinh chỉnh trên các bộ dữ liệu hạ nguồn, KOSMOS-2.5-CHAT có khả năng giải quyết một phạm vi rộng các nhiệm vụ hiểu tài liệu. Chúng tôi đã tinh chỉnh KOSMOS-2.5-CHAT trên các tập huấn luyện tiêu chuẩn của mười bộ dữ liệu hiểu tài liệu đa dạng. Các bộ dữ liệu này bao phủ tài liệu chung (DocVQA, InfoVQA, DeepForm, KLC), bảng (WTQ, TabFact), biểu đồ (ChartVQA), hình ảnh tự nhiên (TextVQA, TextCaps), và ảnh chụp màn hình trang web (VisualMRC). Đánh giá được thực hiện trên các tập thử nghiệm chính thức của chín benchmark hiểu tài liệu công khai. Chúng tôi không đánh giá trên TextCaps do không có sẵn máy chủ đánh giá chính thức vào thời điểm này.

Kết quả. Bảng 8 trình bày kết quả thực nghiệm so sánh với các mô hình OCR-free tiên tiến. Trong số các mô hình có ít hơn 2B tham số, KOSMOS-2.5-CHAT vượt trội hơn PixStruct LARGE và Donut trên các benchmark khác nhau mà không cần tinh chỉnh cụ thể cho từng nhiệm vụ. So sánh với các mô hình vượt quá 7B tham số, KOSMOS-2.5-CHAT mang lại hiệu suất cạnh tranh trên các benchmark bao phủ tài liệu, bảng, và biểu đồ, bao gồm DocVQA, InfoVQA, DeepForm, KLC, WTQ, và ChartVQA. Những kết quả này nêu bật hiệu quả của KOSMOS-2.5-CHAT trong việc xử lý các nhiệm vụ hiểu tài liệu phức tạp.

--- TRANG 7 ---
[Bảng 8 với kết quả benchmark hiểu tài liệu]

Bảng 8: Kết quả thực nghiệm trên các benchmark hiểu tài liệu. Các mô hình được liệt kê phía trên dòng có hơn 7B tham số, trong khi những mô hình phía dưới dòng là các mô hình nhỏ hơn. Dấu '∗' chỉ ra các mô hình được tinh chỉnh riêng biệt trên mỗi nhiệm vụ hạ nguồn. Trong số các mô hình có ít hơn 7B tham số, kết quả tốt nhất được đánh dấu in đậm.

4 Công trình Liên quan
4.1 Mô hình Ngôn ngữ Lớn Đa phương tiện
Các mô hình ngôn ngữ lớn đa phương tiện (MLLMs) có thể được phân loại rộng rãi thành các hệ thống lập lịch lấy LLM làm trung tâm và các hệ thống đa phương tiện có thể huấn luyện từ đầu đến cuối. Các hệ thống lập lịch lấy LLM làm trung tâm tận dụng các mô hình nền tảng thị giác khác nhau, phối hợp chúng theo cách lấy ngôn ngữ làm trung tâm. Mặt khác, các hệ thống đa phương tiện có thể huấn luyện từ đầu đến cuối tích hợp các mô hình thị giác và ngôn ngữ thành một khung thống nhất.

Mô hình của chúng tôi thuộc về danh mục sau, chia sẻ điểm tương đồng với các mô hình đa phương tiện có định vị như KOSMOS-2, Shikra, và ChatSpot, xuất ra vị trí đối tượng trong hình ảnh tự nhiên. Tuy nhiên, KOSMOS-2.5 tập trung độc đáo vào khả năng đọc và hiểu hình ảnh văn bản, giải quyết thách thức tạo ra bố cục tài liệu chất lượng cao trong khi duy trì tính toàn vẹn cấu trúc quan trọng cho việc hiểu tài liệu.

4.2 Đọc và Hiểu Tài liệu
Đọc và hiểu tài liệu tận dụng AI để tự động đọc, hiểu, và trích xuất thông tin từ tài liệu. Các mô hình nền tảng tài liệu đại diện như LayoutLMv3 tích hợp thông tin văn bản, bố cục, và hình ảnh trong quá trình tiền huấn luyện, xuất sắc trong các nhiệm vụ như trích xuất thông tin chính và hỏi đáp tài liệu. Donut giới thiệu một Transformer hiểu tài liệu không cần OCR, ánh xạ trực tiếp hình ảnh tài liệu đầu vào thành đầu ra mong muốn. Các mô hình như Pix2Struct, HRVDA, và series mPLUG-DocOwl tiền huấn luyện bộ mã hóa thị giác trên các nhiệm vụ đọc tài liệu, dẫn đến hiệu suất hiểu tài liệu ấn tượng. KOSMOS-2.5 mở rộng tiền huấn luyện tài liệu để bao gồm lên đến 357.4 triệu trang tài liệu và các nhiệm vụ đầy thách thức hơn, cải thiện đáng kể khả năng đọc và hiểu của mô hình.

Nougat tương tự phân tích tài liệu thành ngôn ngữ đánh dấu, nhưng trọng tâm của nó bị hạn chế ở tài liệu khoa học. Ngược lại, KOSMOS-2.5 xuất sắc trên một phạm vi rộng hơn các tài liệu và tổng quát hóa tốt cho các nhiệm vụ hiểu tài liệu. Các công trình gần đây như DocPedia tăng cường khả năng hiểu hình ảnh có nhiều văn bản của MLLMs bằng cách xử lý đầu vào thị giác trong miền tần số để có khả năng độ phân giải cao. Các cách tiếp cận như TextSquare, TRINS, và LLaVAR tăng cường khả năng đọc bằng cách sử dụng các công cụ OCR có sẵn công khai và MLLMs nguồn đóng để tạo ra dữ liệu tinh chỉnh hướng dẫn cho hình ảnh có nhiều văn bản. LLaVA-read tiếp tục sử dụng các công cụ OCR nguồn mở để trích xuất thông tin văn bản và bố cục cho các mô hình ngôn ngữ. UReader giới thiệu một mô-đun cắt thích ứng hình dạng để mã hóa hiệu quả các hình ảnh phụ độ phân giải thấp. Trong khi đó, Monkey thúc đẩy hiệu quả huấn luyện và độ phân giải, xuất sắc trong chú thích hình ảnh và xử lý tài liệu có nhiều văn bản. Tuy nhiên, các phương pháp này dựa vào bộ mã hóa thị giác tiền huấn luyện mà không có tiền huấn luyện cụ thể cho tài liệu, điều này hạn chế hiệu suất của chúng. Sau tiền huấn luyện rộng rãi, KOSMOS-2.5 đạt được hiệu suất hiểu tài liệu mạnh mẽ bằng cách tinh chỉnh chỉ trên các benchmark có sẵn công khai, mà không cần thiết kế mô-đun phức tạp, công cụ OCR, hoặc MLLMs nguồn đóng.

4.3 Benchmark Đọc Tài liệu
Các benchmark đánh giá OCR hiện tại như OCRBench hoặc DocLocal4K chủ yếu tập trung vào các nhiệm vụ nhận dạng dòng văn bản. Textmonkey đánh giá mô hình chỉ trên hình ảnh tự nhiên. Ngược lại, OCREval được đề xuất của chúng tôi là benchmark đầu tiên được thiết kế đặc biệt để đánh giá nhận dạng văn bản cấp tài liệu, đòi hỏi khả năng nhận dạng tiên tiến hơn. Đối với đánh giá markdown, Nougat hạn chế đánh giá hiệu suất của nó cho các bài báo học thuật từ ArXiv. Ngược lại, MarkdownEval của chúng tôi cung cấp một đánh giá toàn diện hơn bằng cách bao phủ một phạm vi rộng hơn các lĩnh vực hình ảnh, cung cấp đánh giá mạnh mẽ hơn về khả năng của mô hình.

5 Kết luận và Công việc Tương lai
Tóm lại, công trình này thúc đẩy việc đọc máy cấp tài liệu bằng cách giới thiệu một khung tiền huấn luyện mới và chứng minh hiệu quả của nó thông qua hiệu suất ấn tượng trên các benchmark đa dạng. Mô hình tiền huấn luyện của chúng tôi, KOSMOS-2.5, xuất sắc trong việc đọc tài liệu, trong khi mô hình được tinh chỉnh của chúng tôi, KOSMOS-2.5-CHAT, đạt kết quả cạnh tranh trong các benchmark hiểu tài liệu. Bộ dữ liệu rộng lớn gồm 357.4 triệu hình ảnh tài liệu được chú thích và việc phát triển các benchmark OCREval và MarkdownEval cung cấp các công cụ toàn diện để đánh giá và thúc đẩy nghiên cứu về trí tuệ tài liệu. Mặc dù có những kết quả đầy hứa hẹn này, mô hình hiện tại của chúng tôi đối mặt với một số hạn chế, mang lại các hướng nghiên cứu tương lai có giá trị. Ví dụ, các tài liệu trải rộng nhiều trang đặt ra thách thức vì chúng thường đòi hỏi xử lý và hiểu toàn diện. Trong khi đó, cũng khả thi rằng KOSMOS-2.5 cho phép nhiều trang hình ảnh xen kẽ với văn bản làm đầu vào; tuy nhiên, việc quản lý ngữ cảnh dài vẫn là một vấn đề quan trọng mà chúng tôi hướng tới giải quyết trong công việc tương lai. Trong bối cảnh nghiên cứu rộng lớn hơn, một hướng nghiên cứu quan trọng nằm ở việc thúc đẩy khả năng mở rộng mô hình. Với phạm vi nhiệm vụ và độ phức tạp ngày càng mở rộng, việc mở rộng mô hình để xử lý khối lượng dữ liệu lớn hơn là rất quan trọng cho các mô hình đọc viết đa phương tiện.

--- TRANG 8 ---
[Phần tài liệu tham khảo]

Tài liệu tham khảo
[Danh sách đầy đủ các tài liệu tham khảo được trích dẫn trong bài báo]

--- TRANG 9-24 ---
[Phần phụ lục với các bảng chi tiết, ví dụ, và thông tin bổ sung]

Phụ lục A
A.1 Siêu tham số Mô hình và Huấn luyện
[Các bảng chi tiết về cấu hình mô hình và huấn luyện]

A.2 Metrics Đánh giá OCR
[Giải thích chi tiết về các metrics F1, IoU, NED]

A.3 Metrics Đánh giá Tạo Hình ảnh thành Markdown
[Giải thích về NED và NTED]

A.4 Ví dụ Định tính
[Ví dụ minh họa đầu vào và đầu ra của mô hình]

A.5 Xử lý Dữ liệu Tiền huấn luyện
[Chi tiết về quy trình xử lý từng loại dữ liệu]

A.6 Xử lý Dữ liệu Đánh giá
[Chi tiết về cách xây dựng các benchmark]

A.7 Ví dụ Suy luận Mô hình
[Mã nguồn và kết quả đầu ra mẫu]

A.8 Ví dụ Dữ liệu Tiền huấn luyện
[Các mẫu dữ liệu huấn luyện từ các nguồn khác nhau]
