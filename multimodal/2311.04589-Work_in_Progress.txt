# 2311.04589.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.04589.pdf
# File size: 581250 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Work in Progress
TEAL :TOKENIZE AND EMBED ALL FOR MULTI -
MODAL LARGE LANGUAGE MODELS
Zhen Yang∗Yingxue Zhang∗Fandong Meng Jie Zhou
WeChat AI, Tencent Inc.
{zieenyang,yxuezhang,fandongmeng,withtomzhou }@tencent.com
ABSTRACT
Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the interactions among
multi-modal inputs and the generation in non-textual modalities. In this work, we
propose TEAL (Tokenize and Embed ALl) , an approach to treat the input from any
modality as a token sequence and learn a joint embedding space for all modalities.
Specifically, for the input from any modality, TEAL firstly discretizes it into a to-
ken sequence with the off-the-shelf tokenizer and embeds the token sequence into
a joint embedding space with a learnable embedding matrix. MM-LLMs just need
to predict the multi-modal tokens autoregressively as conventional textual LLMs
do. Finally, the corresponding de-tokenizer is applied to generate the output in
each modality based on the predicted token sequence. With the joint embedding
space, TEAL enables the frozen LLMs to perform both understanding and gen-
eration tasks involving non-textual modalities, such as image and audio. Thus,
the textual LLM can just work as an interface and maintain its high performance
in textual understanding and generation. Experiments show that TEAL achieves
substantial improvements in multi-modal understanding, and implements a simple
scheme for multi-modal generations.
1 I NTRODUCTION
Recently, Multi-Modal Large Language Models (MM-LLMs), which perform understanding and
generation tasks more than textual modalities, have made exciting strides and garnered significant
attention for their potential in Artificial Intelligence Generated Content (AIGC) (Cao et al., 2023).
MM-LLMs are considered a step closer to Artificial General Intelligence (AGI) (Goertzel & Pen-
nachin, 2007; Fei et al., 2022) due to their provision of more user-friendly interfaces and their ability
to perceive the world similarly to humans (Yin et al., 2023). Typically, there are two main different
branches in the realm of constructing MM-LLMs: One branch aims to construct a ‘real‘ multi-modal
model by training the model with multi-modal data from scratch, without relying on the pre-trained
textual LLMs (Borsos et al., 2023; Lu et al., 2022a; Barrault et al., 2023; Shukor et al., 2023; Chen
et al., 2023c; Copet et al., 2023); The other branch takes the textual LLMs as the backbone and
enables them to perform multi-modal understanding and generation tasks with instruction tuning.
With the rapid advancement of textual LLMs, researchers are keener on the second branch of ap-
proaches which empowers the pre-trained high-performance textual LLMs with multi-modal abil-
ities. In this line, some typical works, such as BLIP-2 (Li et al., 2023), Flamingo (Alayrac et al.,
2022), MiniGPT-4 (Zhu et al., 2023), LLama-Adapter (Gao et al., 2023; Zhang et al., 2023c), LLaV A
(Liu et al., 2023b;a), SpeechGPT (Zhang et al., 2023a), involve employing adapters that align pre-
trained encoders in other modalities to textual LLMs. As these works take the dense features from
the pre-trained encoders as additional non-textual information, they cannot efficiently model the in-
teractions among multi-modal inputs and falter in the nuanced art of generating non-textual content.
In order to compensate for this deficiency in the non-textual generation, some efforts, such as visual-
ChatGPT (Chen et al., 2023c), Hugging-GPT (Shen et al., 2023), Audio-GPT (Huang et al., 2023),
Next-GPT (Wu et al., 2023b), and MiniGPT-5 (Zheng et al., 2023) have sought to amalgamate the
textual LLMs with some external generation tools, e.g., Stable Diffusion (Rombach et al., 2022),
1These authors contribute equally to this work.
1arXiv:2311.04589v3  [cs.CL]  4 Jan 2024

--- PAGE 2 ---
Work in Progress
DALL-E (Ramesh et al., 2021), Whisper (Radford et al., 2023). Unfortunately, these systems suf-
fer from two critical challenges due to their complete pipeline architectures. First, the information
transfer between different modules is entirely based on generated textual tokens, where the process
may lose some multi-modal information and propagate errors (Wu et al., 2023b). Additionally, the
external tools usually make the models complex and heavy, which consequently results in inefficient
training and inference.
Based on the above observation, we conclude that the emerging challenges in the previous works
are mainly raised by their non-unified processing of the multi-modal inputs, where they encode the
non-textual inputs into a dense and high-level feature, but tokenize the textual input into a token
sequence. The non-unified processing introduces an extra burden for LLMs to model the interaction
between multi-modal inputs and generate the non-textual samples. In a nutshell, if we can tokenize
the interleaved multi-modal input into a token sequence and align the non-textual token embedding
into the textual embedding space, the original textual LLMs can be easily transformed to handle
non-textual understanding and generation tasks with parameters tuned as little as possible.
In pursuit of this goal and inspired by the recent advancement of multi-modal tokenizers (Yu et al.,
2023b; Chang et al., 2023; Peng et al., 2022; Borsos et al., 2023; Yu et al., 2023a), we propose
TEAL , a token-in-token-out MM-LLM designed to seamlessly handle the token input and output in
any combination of three modalities: text, image, and audio. Specifically, TEAL comprises three
tiers. First, we tokenize the input from any modality into a token sequence with the off-the-shelf
tokenizers, such as BEiT-V2 and a Whisper-based audio tokenizer. Second, we insert a non-textual
embedding matrix and output matrix into an open-source textual LLM, which enables the textual
LLM to process the non-textual inputs and outputs. To align the non-textual embedding matrices
with their textual counterparts, we equip them with a projection layer. Third, the generated tokens
are routed to the corresponding de-tokenizers, which transform the token sequences into samples in
different modalities. We conduct extensive experiments on the modalities of text, image, and audio.
Experimental results show that TEAL achieves substantial improvements over previous works on
multi-modal understanding and paves a simple way for the generation of non-textual modalities.
In summary, our contributions are three-fold:
1. We propose TEAL , an approach that treats the input from any modality as a token sequence
and learns a joint embedding space for all modalities. TEAL introduces a simple way to
enable the frozen LLMs to perform both understanding and generation tasks involving non-
textual modalities.
2. We conduct extensive experiments on the non-textual modalities of image and audio. Ex-
perimental results show that TEAL achieves substantial improvements over previous works
on multi-modal understanding and paves a simple way for the generation of non-textual
modalities. To the best of our knowledge, this is the first work that successfully empowers
the frozen LLM to perform tasks involving both the non-textual modalities of audio and
image.
3. By testing versatile tokenizers for image and audio, we find that the tokenizer is key to
the performance of MM-LLMs. Our extensive experiments have identified a new research
direction that devising a general semantic-aware tokenizer is very promising.
2 R ELATED WORK
2.1 MM-LLM S
Training a multi-modal large language model from scratch in an end-to-end manner incurs sub-
stantial costs. Therefore, most researchers choose to integrate multi-modal modules into existing
text-based large language models, allowing these models to acquire multi-modal capabilities. One
branch involves employing robust pre-trained vision or audio encoders to encode multi-modal infor-
mation into features and subsequently align it with the feature space of an LLM (Dai et al., 2023;
Chen et al., 2023a; Zhang et al., 2023b;c; Gao et al., 2023; Ling et al., 2023; Wu et al., 2023a; Hus-
sain et al., 2023). For example, Flamingo (Alayrac et al., 2022) utilizes vision encoders to obtain a
fixed number of visual tokens and use cross-attention layers to connect the pre-trained LLM layers.
BLIP-2 (Li et al., 2023) utilizes a Q-Former as a bridge between the input image and the LLMs.
2

--- PAGE 3 ---
Work in Progress
LauraGPT (Chen et al., 2023b) uses a pre-trained Conformer-based encoder to extract continuous
audio representations for the connected LLM. Furthermore, different projection layers are used to
reduce the modality gap, such as a simple Linear Layer (Liu et al., 2023a) or a two-layer Multi-layer
Perceptron (Zhang et al., 2023d). Moreover, LLaMa-Adapter (Zhang et al., 2023c; Gao et al., 2023)
integrates trainable adapter modules into LLMs, enabling effective parameter tuning for the fusion
of multi-modal information. Another branch involves using off-the-shelf expert models to convert
images or speech into natural language in an offline manner, such as Next-GPT (Wu et al., 2023b),
SpeechGPT (Zhang et al., 2023a) and AudioGPT (Huang et al., 2023).
Contrary to these works mentioned above, we tokenize the input from any modality into a token
sequence and train a token-in-token-out MM-LLM designed to seamlessly handle the token input
and output in any combination of three modalities: text, image, and audio.
2.2 N ON-TEXTUAL DISCRETIZATION
In addition to directly integrating multi-modal modules or using offline expert models, there are
also efforts focused on non-textual discretization, which employs tokenizers to convert continuous
images or audio into token sequences. This way, all modalities share the same form as tokens,
which can be better compatible with LLM. Next, we will introduce two mainstream methods of
Non-textual discretization.
VQ-VAEs Vector Quantised Variational AutoEncoder (VQ-V AE) (Van Den Oord et al., 2017) is a
seminal contribution in the field of non-textual tokenization, which incorporates vector quantization
(VQ) to learn discrete representations and converts images into a sequence of discrete codes. In the
vision domain, VQGAN (Esser et al., 2021) follows the idea, using a codebook to discretely encode
images, and employs Transformer as the encoder. ViT-VQGAN (Yu et al., 2021) introduces several
enhancements to the vanilla VQGAN, encompassing architectural modifications and advancements
in codebook learning. BEiT-V2 (Peng et al., 2022) proposes Vector-quantized Knowledge Distilla-
tion (VQ-KD) to train a semantic-rich visual tokenizer by reconstructing high-level features from the
teacher model. Ge et al. (2023) proposes SEED and claims two principles for the tokenizer architec-
ture and training that can ease the alignment with LLMs. Yu et al. (2023a) introduce SPAE, which
can convert between raw pixels and lexical tokens extracted from the LLM’s vocabulary, enabling
frozen LLMs to understand and generate images or videos. For the audio, Dieleman et al. (2018)
utilize autoregressive discrete autoencoders (ADAs) to capture correlations in waveforms. Jukebox
(Dhariwal et al., 2020) uses a multi-scale VQ-V AE to compress music to discrete codes and model
those using autoregressive Transformers, which can generate music with singing in the raw audio
domain. SoundStream (Zeghidour et al., 2021) employs a model architecture composed of a fully
convolutional encoder/decoder network and adopts a Residual Vector Quantizer (RVQ) to project
the audio embedding in a codebook of a given size. D ´efossez et al. (2022), Jiang et al. (2022) also
adopt RVQ to quantize the output of the encoder.
Clustering Except for those methods that use trained specialized vector quantization (VQ) mod-
ules as tokenizers, some works (Lakhotia et al., 2021; Kharitonov et al., 2022) apply the clustering
algorithms to the features, and the cluster indices are directly used as the discrete tokens for speech.
The cluster approach typically relies on self-supervised learning models, such as HuBERT (Hsu
et al., 2021), W2V-BERT (Chung et al., 2021; Borsos et al., 2023), USM (Zhang et al., 2023e;
Rubenstein et al., 2023), which are trained for discrimination or masking prediction and maintain
semantic information of the speech. Compared with neural VQ-based tokenizers, the clustering-
based approach provides enhanced flexibility as it can be applied to any pre-trained speech model
without altering its underlying model structure.
3 M ETHOD
The main goal of this paper is to enable the frozen textual LLMs to model sequences consisting of
multi-modal discrete tokens. Thus, the textual LLMs obtain the ability to perform both understand-
ing and generation tasks involving non-textual modalities and maintain their strong abilities in text.
The main architecture of our method is illustrated in Figure 1. Firstly, we discretize the interleaved
multi-modal input into a token sequence with the off-the-shelf tokenizers. Then, an open-sourced
3

--- PAGE 4 ---
Work in Progress
SentencePieceK-meansclusteringVectorQuantizationFeatureExtraction
…De-Tokenizer
Tokenizer
…TextualEmbeddingNon-TextualEmbeddingProjectionSelf-AttentionSelf-AttentionNon-TextualOutputMatrixProjectionTextualOutputMatrix…
…MM-LLM
Figure 1: The main architecture of TEAL . The modules in MM-LLM denoted with the color gray
make up the original textual LLM and most of them are frozen during training.
textual LLM is used to model the input and output token sequence by aligning the textual and non-
textual embedding space. Finally, the corresponding off-the-shelf decoder is utilized to generate the
output in each modality. In the remainder of this section, we will describe the model architecture in
Subsection 3.1. The tokenizer and de-tokenizer for non-textual modalities we used in this paper will
be presented in Subsection 3.2. Finally, we propose our two-stage training strategies in Subsection
3.3.
3.1 M ODEL ARCHITECTURE
TEAL is a general method that can be applied to any open-source LLMs. In this paper, the proposed
MM-LLM takes the most popular open-sourced textual LLM, i.e., LLaMA, as the backbone, which
makes it easy to compare fairly with previous works. To support the modeling of non-textual tokens,
the MM-LLM also incorporates a non-textual embedding layer and a non-textual output layer. Two
projection layers are applied after the non-textual embedding layer and before the output layer sep-
arately, which mainly serve two purposes: 1) make the output dimension of textual and non-textual
embedding the same; 2) align the non-textual embedding with the textual embedding space. To
ease the training process and solve the cold-start problem, we initialize the non-textual embedding
and output matrix with the codebook of the tokenizer, which will be described in Subsection 3.2 in
detail.
3.2 T OKENIZE AND DE-TOKENIZE
Tokenization is a very popular technique in the area of natural language processing, which is usually
used as a tool to split the input sentence into the granularity of sub-words. Most of the existing
textual LLMs take the sentence piece as the tokenizer for its universal processing of multi-lingual
texts. The de-tokenization for the sentence piece is very simple, which just works as a function to re-
place the meta-symbol ‘ ’ with the whitespace. Recently, tokenization (or denoted as discretization)
in non-textual modalities has gained much attention and achieved substantial improvements, which
makes it possible to build a fully token-in-token-out MM-LLM. The most widely used methods are
VQ-V AE and k-means clustering. In this paper, we take the encoder of the VQ-V AE models and
the k-means clustering as the tokenizers for the image and audio respectively. The decoders of the
VQ-V AE models are taken as the de-tokenizers for the image and audio. For the image, we test the
following typical tokenizers (and the corresponding de-tokenizers):
4

--- PAGE 5 ---
Work in Progress
• DALL-E (Ramesh et al., 2021): They train a discrete variational autoen-coder (dV AE) to
compress each 256×256 RGB image into a 32 × 32 grid of image tokens, each element of
which can assume 8192 possible values. We harness the open-source toolkit implemented
by DALLE-pytorch.1.
• VQ-GAN (Esser et al., 2021): They combine the efficiency of convolutional approaches
with the expressivity of transformers by introducing a convolutional VQGAN, which learns
a codebook of context-rich visual parts, whose composition is modeled with an autoregres-
sive transformer. We follow the open-source toolkit, Taming-Transformer, and directly use
their released pre-trained models.2
• BEiT-V2 (Peng et al., 2022): They propose vector-quantized knowledge distillation (VQ-
KD) to train the visual tokenizer, where the tokenizer is trained to reconstruct the semantic
features of a teacher model. We utilize the officially released toolkit and models.3
For the audio, we apply K-means Clustering on the intermediate features of the following typical
models, and the cluster indices are directly used as the discrete tokens for speech.
• HuBERT (Hsu et al., 2021): They incorporate an offline clustering step to generate aligned
target labels for a BERT-like prediction loss for self-supervised representation learning.
Through masked prediction, the model is forced to learn both acoustic and language models
from continuous inputs.
• Whisper (Radford et al., 2023): Whisper is a Transformer-based speech recognition model,
which is trained on many different speech processing tasks via large-scale weak mul-
tilingual and multitask supervision. In this paper, we conduct experiments with the
W hisper small to get discrete audio tokens.
3.3 T WO-STAGE SUPERVISED FINETUNING
The proposed TEAL model is initialized with the open-sourced textual LLM. To obtain the un-
derstanding and generation ability in non-textual modalities and maintain its high performance in
textual modality, we propose a two-stage supervised fine-tuning that trains the model with parame-
ters tuned as little as possible. In the following, we denote the two stages of supervised fine-tuning
as pre-training and fine-tuning separately.
Pre-training The goal of the pre-training is to align the non-textual and textual embedding space
by tuning the projection layer. Specifically, we freeze all parameters in the MM-LLM except the
parameter of the two projection layers. We generate the training samples from the vision-language
and audio-language pairs with very simple prompts. Taking the vision-language pair as an example,
we generate two training samples from each vision-language pair with the following format:
The image and text pair: [img ][text ]
The text and image pair: [text ][img ]
Fine-tuning In the stage of fine-tuning, we process the corpus of downstream tasks as the prompt
format in Zhang et al. (2023c). For each task, we use the GPT4 to generate 10 different prompts.4
We freeze the parameters of the textual LLM and tune all parameters related to the non-textual
modalities. Following Zhang et al. (2023c), we apply the bias-norm tuning where the bias and norm
parameters are inserted in each layer to enhance the fine-tuning performance. We also tested Lora
tuning, but we did not obtain further improvement.
4 E XPERIMENTS
We first test our method on the understanding tasks involving non-textual modalities, i.e., the task
of coco-caption, science-QA, and CoV oST 2. Then, we report our performance on the task of image
1https://github.com/lucidrains/DALLE-pytorch
2https://github.com/CompVis/taming-transformers
3https://github.com/microsoft/unilm
4For details of the prompt format, we refer the readers to the Appendix A.
5

--- PAGE 6 ---
Work in Progress
ModelData Scale COCO Caption
PT FT CiDER BLEU-4
LlaMA-Adapter v2 (Gao et al., 2023) 0 0.6M 122.2 36.2
BLIP (Li et al., 2022) 14M 0.6M 136.7 40.4
BLIP2 (Li et al., 2023) 129M 0.6M 145.3 43.7
TEAL 0 0.6M 128.8 38.1
Table 1: Model performance on the COCO2014 test set. The results of the baselines are cited from
their papers directly.
MethodSubject Conext Modality GradeAverageNAN SOC LAN TXT IMG NO G1-6 G7-12
LLaMA-Adapter 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19
Human 90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40
GPT-3.5 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97
GPT-3.5 w/ COT 75.44 70.87 78.09 76.48 67.43 79.93 78.23 69.68 75.17
MM-COT base 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91
MM-COT large 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68
LLaV A-7B - - - - - - - - 89.84
LLaV A-13B 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92
TEAL (Ours) 89.00 92.94 86.42 85.06 83.00 88.92 86.26 84.90 87.12
Table 2: Results on the ScienceQA test set. For the baselines, we directly cite the results from their
papers.
generation. The model is implemented based on the codebase of LLaMA-Adapter (Gao et al.,
2023).5If there is no specific explanation, all models are trained with two-stage supervised fine-
tuning with 8 A100 GPUs, and the main hyper-parameters are set the same with LlaMA-Adapter.
Following (Gao et al., 2023), we also adopt top-p sampling as the default decoding method with a
temperature of 0.1 and a top-p of 0.75.
4.1 COCO-C APTION
We utilize all image-caption pairs from the coco2014 dataset (Chen et al., 2015), which contains 83K
images for training. As there are at least five captions for each image in the coco2014 dataset, we
can construct at least five training examples for each image by pairing the image with its all captions
respectively. For a fair comparison, we report the CIDER, BLEU-4 on the Karpathy test split, which
is evaluated with the official toolkit, pycocoeval.6The result is presented in Table 1. From Table 1,
we can find that the proposed TEAL achieves substantial improvements compared to the baseline of
LLaMA-Adapter v2, which applies a frozen vision encoder to incorporate the vision information.
Specifically, we achieve 1.9 and 6.6 points improvement on the metrics of BLEU-4 and CiDER
respectively. Additionally, compared to the models that trained with large-scale corpora, such as
the BLIP and BLIP2, TEAL further narrows the performance gap without additional pre-training
corpus. The cases on the valid set are shown in Figure 2. We can find that the proposed TEAL is
able to understand the content of images well and can describe the details of the images clearly.
4.2 S CIENCE QA
ScienceQA (Lu et al., 2022b) is collected from elementary and high school science curricula and
contains 21,208 multimodal multiple-choice science questions. Out of the questions in ScienceQA,
10,332 (48.7%) have an image context, 10,220 (48.2%) have a text context, and 6,532 (30.8%)
have both. ScienceQA has rich domain diversity across 3 subjects, 26 topics, 127 categories, and
379 skills, and the benchmark dataset is split into training, validation, and test splits with 12726,
4241, and 4241 examples, respectively. The main baseline that can be used to make a fair com-
parison with our method is the LLaMA-Adapter (Zhang et al., 2023c). We also cite the results of
5https://github.com/Alpha-VLLM/LLaMA2-Accessory
6https://github.com/cocodataset/cocoapi
6

--- PAGE 7 ---
Work in Progress
Imageid:COCO_val2014_000000200959.jpgPrompt:The task at hand is to provide a caption for the image that has been provided.
Output:A man with a black jacket flying through a snow-covered slope while riding a snowboard.Reference:Person on snowboard jumping in air with mountains in the background.Imageid:COCO_val2014_000000384213.jpgPrompt:You have been given an image and your task is to write a caption for it.Output:A verysmallkitchenwithasink,twowindowswithcurtainsReference:A kitchen is shown with a variety of items on the counters.Imageid:COCO_val2014_000000466052.jpgPrompt:Please provide a caption for the image that has been given.Output:A coffee mug sits in the corner on a counter with several toothbrushesandpastesin it.Reference:A coffee cup filled with tooth paste and toothbrushes.
Figure 2: Some examples in the coco2014 validation set. For each case, we present the original
image ID, the prompt, the output of our model, and one reference caption randomly selected among
all five references.
two representation methods (GPT-3.5 and GPT-3.5 w/ COT) (Lu et al., 2022b), one multi-modal
COT method (MM-COT) (Zhang et al., 2023f), human evaluation (Lu et al., 2022b), and LLaV A
(Liu et al., 2023b) which tunes the full parameters of the vicuna with large-scale multi-modal pre-
training corpus. Table 2 presents the experimental results. As shown in Table 2, we can find that
the proposed TEAL achieves about 2 points improvement on average compared to the baseline of
LLaMA-Adapter.
4.3 C OVOST 2
Model WER
HuBERT large (Hsu et al., 2021) 31.77
Whispersmall (Radford et al., 2023) 18.8
Whispersmall + LLaMa-Adapter 26.96
TEAL (Ours) 24.22
Table 3: Results on the CoV oST 2 ASR test set.For audio, we conduct experiments on
the CoV oST 2 (Wang et al., 2020) ASR
English dataset, which contains 232976
audio-text training pairs, 15532 validation
pairs, and 15532 test pairs. We use the
word error rate (WER) as the metric. We
implement the audio tokenizer by apply-
ing k-means clustering on the 11th layer
of Whispersmall .7The number of cluster
centers is set as 8192 and the effect of the number of cluster centers will be investigated in Section
5.2. While training and inference, the audio and the corresponding prompt will be processed into
token sequences and fed into the MM-LLM directly. For a fair comparison, our main baseline is
also implemented based on LLaMa-Adapter and Whispersmall , where the Whispersmall is utilized
as an encoder to extract the dense audio features from the raw audio waves. We use the default
adapter architecture to integrate the audio features into the MM-LLM. As Table 3 shows, combin-
ing an audio tokenizer makes LLM possess better multi-modal understanding ability than explicitly
integrating an audio encoder, with a WER score improvement of 2.74. This may be because that
having modalities in the same token format makes it easier to integrate multi-modal information for
LLM.
7We tested different layers of Whispersmall and obtained the best performance on 11th layer.
7

--- PAGE 8 ---
Work in Progress
### Instruction:\n\nPleasegenerate handwritten images corresponding to the input.\n\n###Input:\n\nanimageof0PromptGeneration### Instruction:\n\nPleasegenerate handwritten images corresponding to the input.\n\n###Input:\n\nanimageofthelastdigitof3plus8### Instruction:\n\nPleasegenerate handwritten images corresponding to the input.\n\n###Input:\n\nanimageofthenumberofthecontinentsintheworld### Instruction:\n\nPleasegenerate handwritten images corresponding to the input.\n\n###Input:\n\nanimageofthenumberofthesquareof3
Figure 3: Some examples of the text-to-image generation on MNIST test set. We test with both
simple and complex questions for the proposed TEAL .
4.4 I MAGE GENERATION
Following (Yu et al., 2023a), we show several text-to-image generation examples on the MNIST
dataset (Deng, 2012) in Figure 3. Different from (Yu et al., 2023a), we do not use any prompt
example for in-context learning. As the BEiT-V2 is not good at image reconstruction, we apply the
VQGAN as the tokenizer for image generation.8From Figure 3, we can find that the proposed TEAL
empowers the frozen textual LLM with the ability to generate the image following the prompt query.
We also test with complex questions requiring mathematical reasoning or common sense knowledge,
and the model is able to give the right responses. These results show that TEAL not only learns how
to generate non-textual content but also maintains its previous ability in textual understanding. We
notice that the quality of the generated image is not so perfect, and we leave the work of polishing
the quality of generated images in the next version.
5 A NALYSIS AND DISCUSSION
5.1 D IFFERENT TOKENIZERS
ModelCOCO CaptionScienceQA (ave.)CiDER BLEU-4
DALLE 110.8 23.9 77.12
VQGAN 117.5 26.1 79.56
BEiT-V2 130.1 37.6 88.00
Table 4: The performance of different tokeniz-
ers on the validation sets of the COCO2014 and
ScienceQA. We keep all parameters and data the
same and only vary the tokenizers.We show how the tokenizer affects the perfor-
mance by testing different tokenizers for the
image and audio. For the image, we report the
performance on the validation set of COCO-
caption by varying the image tokenizers. Re-
sults are shown in Table 4. We find that differ-
ent tokenizers result in significant differences
in the final performance, and BEiT-V2 achieves
the best result. Compared to the baseline of
VQ-GAN, BEiT-v2 achieves 11.5 BLEU points
improvement on the task of COCO-caption and
8.5 accuracy points on ScienceQA. The signifi-
cant performance gap highlights the importance
8This is because the BEiT-V2 is not trained to reconstruct the image but to recover the prediction of its
teacher model.
8

--- PAGE 9 ---
Work in Progress
Tokenizer Type LLM LLM size WER
W2V-BERT(Chung et al., 2021) Cluster PaLM 8B 50.1
USM-v1(Zhang et al., 2023e) Cluster PaLM 8B 40.2
USM-v2(Zhang et al., 2023e) Cluster PaLM 8B 22.3
HuBERT(Hsu et al., 2021) Cluster LLaMa 7B 56.2
Whispersmall (Radford et al., 2023) Cluster LLaMa 7B 24.2
Table 5: The performance of different tokenizers on the test sets of the CoV oST 2.
of the tokenizer. We speculate that the main reason for BEiT-v2 achieving such a significant ad-
vantage is that BEiT-v2 has acquired much semantic information during its pre-training, and the
semantic information in the tokenizer is crucial for aligning different modalities.
We have similar observations in the modality of audio. We have tried different tokenizers such as
HuBERT Clustering, Whispersmall Clustering. Table 5 shows the comparison. We also list some
CoV oST2 ASR results with different tokenizers of AudioPaLM (Rubenstein et al., 2023) to make a
comparison. Both the experiments of AudioPaLM and TEAL demonstrate that different tokenizers
can have a significant impact on performance. A good tokenizer is crucial, and it is an area worth
exploring for future work.
5.2 K- MEANS CLUSTER ANALYSIS
V ocab Size 1024 2048 4096 8192
WER 40.22 30.85 25.31 21.49
Table 6: We randomly sample 500 audio-text
pairs from the development set of the CoV oST 2,
and the performance with different vocab sizes is
shown in the table.Table 6 shows the difference when adopting dif-
ferent audio vocab sizes. All the tokenizers are
trained based on the features of the 11th layer
ofW hisper small . We find out that the vocab
size has a substantial effect on performance.
Compared to clustering 1024 tokens, cluster-
ing 8192 tokens can result in a WER improve-
ment of over 18 percentage points. This makes
the clustering-based discretization approaches
more versatile than the VQ-based neural codecs for the audio. The former can adjust the vocabulary
size by tuning the number of clustering centers, while the latter needs to retrain a vector quantization
module.
5.3 A BLATION STUDY
ModelCOCO CaptionScienceQA (ave.)CiDER BLEU-4
TEAL (Ours) 130.1 37.6 88.00
w/o 1st-stage finetuning 127.8 35.4 86.19
w/o embedding initialization 129.1 36.2 86.82
w/o bias-norm tuning 126.9 35.7 85.74
Table 7: Ablation study on the proposed model. ‘w/o 1st-stage fine-
tuning’ indicates that the model is trained with the 2nd-stage finetun-
ing directly. ‘w/o embedding initialization’ means that we initialize
the word embedding and output matrix randomly. ‘w/o bias-tuning’
means that the parameters of bias and norm are not added during the
2nd stage finetuning.To investigate the signifi-
cance of each module in
our model and method, we
conduct an ablation study
by training multiple ver-
sions of our model with
some missing components,
i.e., the 1st-stage finetun-
ing, the embedding initial-
ization, and the bias-norm
tuning. We report the per-
formance on the validation
sets and Table 7 lists the
experimental results. From
Table 7, we can find that
the best performance is obtained with the simultaneous use of all the tested components. The most
critical components are the bias-norm tuning and the 1st-stage finetuning, which shows that the train-
ing strategies need to be carefully devised to ensure high performance. A surprising phenomenon is
that when we randomly initialize the word embedding (‘w/o embedding initialization’ in Table 7),
we do not observe a significant performance decrease. This result suggests that it is the way the to-
kenizer discretizes the image, rather than the word embedding preserved in the tokenizer, critical to
9

--- PAGE 10 ---
Work in Progress
the final performance. The reason why random initialization causes a certain degree of performance
decrease is likely due to the relatively small size of the training data. We speculate that when the
amount of training data reaches a certain level, the performance gap will disappear.
6 C ONCLUSION AND FUTURE WORK
In this paper, we propose TEAL , an approach to training a fully token-in-token-out MM-LLM by
treating the input from any modality as a token sequence and learning a joint embedding space for
all modalities. TEAL empowers the frozen textual LLM with the ability to perform understanding
and generation involving non-textual modalities. Extensive experiments show that, compared to the
baseline models which integrate non-textual encoders, our approach achieves superior performance
on non-textual understanding tasks, and paves a simple way for non-textual generation.
There are two main promising directions for the future work. Firstly, we are interested in construct-
ing an MM-LLM model that can handle more tasks and more modalities. The token-in-token-out
architecture has the potential to handle all tasks in AI within one model. Secondly, we want to de-
vise a general tokenizer, which can discretize the input from textual and non-textual modalities in a
unified way. With such a general tokenizer, aligning the samples from different modalities is simpler
and more straightforward.
10

--- PAGE 11 ---
Work in Progress
REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716–
23736, 2022.
Lo¨ıc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise
Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. Seamlessm4t-
massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596 ,
2023.
Zal´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shar-
ifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a
language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech,
and Language Processing , 2023.
Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and Lichao Sun. A com-
prehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt.
arXiv preprint arXiv:2303.04226 , 2023.
Xuankai Chang, Brian Yan, Kwanghee Choi, Jeeweon Jung, Yichen Lu, Soumi Maiti, Roshan
Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, et al. Exploring speech recognition,
translation, and understanding with discrete speech units: A comparative study. arXiv preprint
arXiv:2309.15800 , 2023.
Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu.
X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign
languages, 2023a.
Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen
Wang, Siqi Zheng, et al. Lauragpt: Listen, attend, understand, and regenerate audio with gpt.
arXiv preprint arXiv:2310.04673 , 2023b.
Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul V oigtlaender, Basil
Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, Daniel Salz, Xi Xiong,
Daniel Vlasic, Filip Pavetic, Keran Rong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu
Soricut. Pali-3 vision language models: Smaller, faster, stronger, 2023c.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll ´ar, and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv
preprint arXiv:1504.00325 , 2015.
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui
Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-
supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU) , pp. 244–250. IEEE, 2021.
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexan-
dre D ´efossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284 , 2023.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning, 2023.
Alexandre D ´efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio
compression. arXiv preprint arXiv:2210.13438 , 2022.
Li Deng. The mnist database of handwritten digit images for machine learning research [best of the
web]. IEEE signal processing magazine , 29(6):141–142, 2012.
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.
Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341 , 2020.
11

--- PAGE 12 ---
Work in Progress
Sander Dieleman, Aaron van den Oord, and Karen Simonyan. The challenge of realistic music
generation: modelling raw audio at scale. Advances in neural information processing systems ,
31, 2018.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recogni-
tion, pp. 12873–12883, 2021.
Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua
Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal founda-
tion model. Nature Communications , 13(1):3094, 2022.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,
Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010 , 2023.
Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large
language model. arXiv preprint arXiv:2307.08041 , 2023.
Ben Goertzel and Cassio Pennachin. Artificial general intelligence , volume 2. Springer, 2007.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,
and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked
prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
29:3451–3460, 2021.
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,
Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech,
music, sound, and talking head. arXiv preprint arXiv:2304.12995 , 2023.
Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, and Ying Shan. Mugen: Multi-modal mu-
sic understanding and generation with the power of large language models. arXiv preprint
arXiv:2311.11255 , 2023.
Xue Jiang, Xiulian Peng, Huaying Xue, Yuan Zhang, and Yan Lu. Cross-scale vector quantization
for scalable neural speech coding, 2022.
Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh
Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu.
Text-free prosody-aware generative spoken language modeling. In Smaranda Muresan, Preslav
Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) , pp. 8666–8681, Dublin, Ireland, May
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.593. URL
https://aclanthology.org/2022.acl-long.593 .
Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,
Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux.
On generative spoken language modeling from raw audio. Transactions of the Association for
Computational Linguistics , 9:1336–1354, 2021. doi: 10.1162/tacl a00430. URL https://
aclanthology.org/2021.tacl-1.79 .
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-
training for unified vision-language understanding and generation. In International Conference
on Machine Learning , pp. 12888–12900. PMLR, 2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2023.
Shaoshi Ling, Yuxuan Hu, Shuangbei Qian, Guoli Ye, Yao Qian, Yifan Gong, Ed Lin, and Michael
Zeng. Adapting large language model with speech for fully formatted end-to-end speech recog-
nition, 2023.
12

--- PAGE 13 ---
Work in Progress
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction
tuning. arXiv preprint arXiv:2310.03744 , 2023a.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023b.
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint
arXiv:2206.08916 , 2022a.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
science question answering. Advances in Neural Information Processing Systems , 35:2507–2521,
2022b.
Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling
with vector-quantized visual tokenizers. arXiv preprint arXiv:2208.06366 , 2022.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.
Robust speech recognition via large-scale weak supervision. In International Conference on Ma-
chine Learning , pp. 28492–28518. PMLR, 2023.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation, 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition , pp. 10684–10695, 2022.
Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal ´an Borsos,
F´elix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.
Audiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925 ,
2023.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint
arXiv:2303.17580 , 2023.
Mustafa Shukor, Corentin Dancette, Alexandre Rame, and Matthieu Cord. Unified model for image,
video, audio and language tasks. arXiv preprint arXiv:2307.16184 , 2023.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in
neural information processing systems , 30, 2017.
Changhan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text
translation. arXiv preprint arXiv:2007.10310 , 2020.
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu,
Bo Ren, Linquan Liu, and Yu Wu. On decoder-only architecture for speech-to-text and large
language model integration, 2023a.
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-
modal llm. arXiv preprint arXiv:2309.05519 , 2023b.
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549 , 2023.
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong
Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.
arXiv preprint arXiv:2110.04627 , 2021.
Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A
Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder
for multimodal generation with frozen llms. arXiv preprint arXiv:2306.17842 , 2023a.
13

--- PAGE 14 ---
Work in Progress
Lijun Yu, Jos ´e Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong
Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion–
tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737 , 2023b.
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-
stream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and
Language Processing , 30:495–507, 2021. URL https://api.semanticscholar.org/
CorpusID:236149944 .
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.
Speechgpt: Empowering large language models with intrinsic cross-modal conversational abil-
ities. arXiv preprint arXiv:2305.11000 , 2023a.
Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language
model for video understanding, 2023b.
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng
Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten-
tion. arXiv preprint arXiv:2303.16199 , 2023c.
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi
Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2023d.
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen,
Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar,
Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman,
Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk,
Franc ¸oise Beaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition be-
yond 100 languages, 2023e.
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal
chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 , 2023f.
Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language genera-
tion via generative vokens. arXiv preprint arXiv:2310.02239 , 2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 , 2023.
14

--- PAGE 15 ---
Work in Progress
Task Prompts
image captionPlease provide a caption for the image that has been given.
Your task is to write a caption for the provided image.
The objective is to come up with a caption for the image that has been provided.
You are required to write a caption for the provided image.
Your job is to create a caption for the image that has been given.
The challenge is to think of a caption for the provided image.
You have been given an image and your goal is to write a caption for it.
You have been given an image and your task is to write a caption for it.
The task at hand is to provide a caption for the image that has been provided.
Your assignment is to come up with a caption for the provided image.
ASR Write a response that appropriately completes the request based on the provided audio.
image generationCreate an image that perfectly matches the input sentence.
Generate an image that fits the input sentence perfectly.
Produce an image that seamlessly complements the input sentence.
Create a picture that perfectly corresponds to the input sentence.
Generate an image that perfectly aligns with the input sentence.
Create an image that perfectly harmonizes with the input sentence.
Produce an image that perfectly integrates with the input sentence.
Generate an image that perfectly suits the input sentence.
Create an image that perfectly matches the input sentence in every way.
Produce an image that perfectly corresponds to the input sentence in every aspect.
Table 8: The prompts generated by GPT4 for different tasks.
A P ROMPTS FOR DIFFERENT TASKS
We present the prompts we use for different tasks in Table 8, which are generated by GPT4 auto-
matically.
15
