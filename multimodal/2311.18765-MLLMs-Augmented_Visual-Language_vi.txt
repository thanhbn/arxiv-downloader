# Tăng cường Học biểu diễn Ngôn ngữ-Thị giác bằng MLLMs

Yanqing Liu1,2*, Kai Wang1*†, Wenqi Shao2, Ping Luo2,3, Yu Qiao2,
Mike Zheng Shou1, Kaipeng Zhang2‡, và Yang You1‡
1Đại học Quốc gia Singapore
2OpenGVLab, Phòng thí nghiệm AI Thượng Hải
3Đại học Hồng Kông

Tóm tắt. Việc tiền huấn luyện ngôn ngữ-thị giác đã đạt được thành công đáng kể trong nhiều nhiệm vụ đa phương thức, phần lớn nhờ vào sự sẵn có của các bộ dữ liệu hình ảnh-văn bản quy mô lớn. Trong nghiên cứu này, chúng tôi chứng minh rằng các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) có thể nâng cao việc học biểu diễn ngôn ngữ-thị giác bằng cách thiết lập các liên kết hình ảnh-văn bản phong phú hơn cho các bộ dữ liệu hình ảnh-văn bản. Phương pháp của chúng tôi đơn giản, sử dụng MLLMs để mở rộng nhiều chú thích đa dạng cho mỗi hình ảnh. Để ngăn chặn sự thiên vị được đưa ra bởi các ảo giác và phong cách ngôn ngữ đơn điệu của MLLMs, chúng tôi đề xuất "cắt tỉa văn bản" để duy trì chất lượng và tính khả dụng của các chú thích mở rộng. Trong truy xuất hình ảnh-văn bản, mà không cần đưa thêm chi phí huấn luyện, phương pháp của chúng tôi liên tục đạt được cải thiện 5.6 ∼35.0% và 16.8 ∼46.1% trên Recall@1 trong các thiết lập tinh chỉnh và zero-shot, tương ứng. Đáng chú ý, chúng tôi đạt được kết quả zero-shot có thể so sánh với việc tinh chỉnh trên các bộ dữ liệu mục tiêu, điều này khuyến khích thêm việc khám phá việc sử dụng đa năng của MLLMs. Các bộ dữ liệu và mã có sẵn tại: https://github.com/lyq312318224/MLLMs-Augmented.

Từ khóa: Tiền huấn luyện ngôn ngữ-thị giác · Học biểu diễn

1 Giới thiệu

Việc tiền huấn luyện ngôn ngữ-thị giác đã đạt được thành công đáng kể trong truy xuất hình ảnh-văn bản [25,26], phân loại hình ảnh [49,66], trả lời câu hỏi thị giác [3,32], và tạo chú thích hình ảnh [31,66]. Thành công này có thể được gán cho các bộ dữ liệu quy mô lớn được thu thập từ Internet, như CC3M [54], CC12M [8], YFCC100M [58], LAION400M [52], v.v. Tuy nhiên, hầu hết các bộ dữ liệu này bao gồm một phần không thể bỏ qua các cặp hình ảnh-văn bản nhiễu và không khớp [25,29,60], điều này ảnh hưởng lớn đến việc học biểu diễn ngôn ngữ-thị giác. Một trong những cách tiếp cận trực tiếp nhất là: sử dụng các mô hình đã được tiền huấn luyện để nhận biết và loại bỏ các cặp không khớp dựa trên các quy tắc heuristic [1,7,17,38].

Các phương pháp này thực sự làm giảm ảnh hưởng của các cặp không khớp. Tuy nhiên, việc đơn giản loại bỏ các cặp không khớp dẫn đến một vấn đề nghiêm trọng: số lượng cặp huấn luyện cũng bị giảm. Như minh họa trong Hình 1a, hiệu suất truy xuất hình ảnh-văn bản liên tục giảm mạnh khi tỷ lệ loại bỏ lớn. Gần đây nhất, một số nghiên cứu [17,42,70] chứng minh rằng LLM và MLLM có thể được sử dụng như các công cụ viết lại để cải thiện chất lượng chú thích mà không làm giảm số lượng cặp huấn luyện. Thật không may, các công cụ viết lại này không tránh khỏi việc đưa vào phong cách chú thích của chúng, tức là cấu trúc văn bản, điều này có thể làm gián đoạn phân phối của các chú thích gốc và dẫn đến khó khăn trong việc học các biểu diễn ngôn ngữ-thị giác tốt hơn.

Để điều tra các đặc điểm của các nghiên cứu viết lại trước đây [17,42,70], chúng tôi tiến hành thí nghiệm để đánh giá ảnh hưởng của chúng trong Hình 1b. Kết quả thí nghiệm xác nhận thực nghiệm rằng việc áp dụng MLLMs để nâng cao chất lượng của các bộ dữ liệu ngôn ngữ-thị giác là một cách tiếp cận đầy hứa hẹn, tức là cải thiện hiệu suất ban đầu rất nhiều. Tuy nhiên, việc viết lại quá mức dẫn đến sự giảm hiệu suất không tầm thường. Do đó, việc tìm ra nguyên nhân gây ra những sự giảm này và xác minh liệu có tồn tại sự thiên vị từ các chú thích tổng hợp hay không là có giá trị.

Chúng tôi khám phá các lý do cơ bản cho sự thiên vị bằng cách phân tích cấu trúc văn bản, trọng tâm chú ý, và tần suất từ của các chú thích được tạo ra. Chúng tôi sử dụng MiniGPT-4 và LLaVA để chú thích từng hình ảnh từ CC3M, tương ứng. Ở đây chúng tôi sử dụng cùng một câu hỏi {Mô tả ⟨hình ảnh⟩ bằng tiếng Anh:} để nhắc mô hình và đặt cùng số lượng token được tạo ra tối đa. Như được thể hiện trong Hình 1c và Hình 3, chúng tôi có những quan sát sau. 1). MLLMs thực sự có cấu trúc văn bản và trọng tâm vốn có của chúng. 2). Việc áp dụng một MLLM duy nhất khiến khó cung cấp các chú thích toàn diện. 3). Có sự đa dạng lớn giữa thống kê tần suất từ của các MLLMs khác nhau và chú thích thô.

Xem xét những hạn chế của một mô hình duy nhất trong việc nắm bắt các chú thích hình ảnh đa dạng, chúng tôi sử dụng nhiều MLLMs để làm phong phú các liên kết ngôn ngữ-thị giác từ các góc độ khác nhau, từ đó cải thiện việc học biểu diễn ngôn ngữ-thị giác trong quá trình tiền huấn luyện. Để cải thiện chất lượng và tính khả dụng của các chú thích tổng hợp, các thiết kế cụ thể sau được đề xuất. Đầu tiên, chúng tôi cắt các chú thích được tạo ra bởi MLLMs để khớp với độ dài của các chú thích gốc, được gọi là "cắt tỉa văn bản". Điều này không chỉ làm giảm sự xuất hiện lặp lại của các từ phổ biến trong chú thích tổng hợp, giảm bớt vấn đề sụp đổ chú thích được xác định trong [60], mà còn bảo tồn các khái niệm ngữ nghĩa gần nhất với hình ảnh, từ đó giảm thiểu tác động của các ảo giác thường xuất hiện ở phần sau của văn bản được tạo ra, như đã được xác minh trong [69]. Thứ hai, để có được một tập hợp các chú thích toàn diện cho mỗi hình ảnh, chúng tôi giữ đồng thời các chú thích thô và mở rộng từ MLLMs cho việc tiền huấn luyện ngôn ngữ-thị giác tiêu chuẩn.

Phương pháp của chúng tôi thể hiện các đặc điểm sau: 1). Nó tương thích với nhiều khung tiền huấn luyện ngôn ngữ-thị giác như CLIP [49] và BLIP [32], chứng minh cải thiện hiệu suất đáng kể trên các nhiệm vụ downstream khác nhau mà không cần đưa thêm chi phí huấn luyện. Ví dụ, trong truy xuất hình ảnh-văn bản zero-shot của MSCOCO và Flickr30K, phương pháp của chúng tôi đạt được cải thiện Recall@1 16.8 ∼46.1%. Trong phân loại hình ảnh zero-shot, phương pháp của chúng tôi đạt được cải thiện hiệu suất trung bình 13.4 trên 15 bộ dữ liệu phân loại thông thường và 13.1 trên ImageNet [14]. 2). Đối với truy xuất hình ảnh-văn bản, CLIP zero-shot của chúng tôi vượt trội hơn CLIP vanilla được tinh chỉnh trên MSCOCO và Flickr30K tương ứng 9.9% và 23.5%. (Tất cả các kết quả trên đều dựa trên tiền huấn luyện trên CC3M.) 3). Khi mở rộng quy mô lên các bộ dữ liệu lớn như CC12M và YFCC15M, phương pháp của chúng tôi tiếp tục mang lại cải thiện hiệu suất đáng kể.

2 Nghiên cứu liên quan

2.1 Cải thiện bộ dữ liệu hình ảnh-văn bản

Nhiều nghiên cứu [18,20,43] đã nhấn mạnh tầm quan trọng của các bộ dữ liệu hình ảnh-văn bản chất lượng cao trong việc ảnh hưởng đến hiệu suất chuyển giao của việc tiền huấn luyện ngôn ngữ-thị giác trong các nhiệm vụ downstream. SemDeDup [1] nâng cao hiệu quả dữ liệu và hiệu suất ngoài phân phối bằng cách xác định và loại bỏ các cặp dữ liệu trùng lặp về mặt ngữ nghĩa ở mức embedding. Cao et al. [7] giới thiệu một cách tiếp cận để cải thiện chất lượng bộ dữ liệu bằng cách loại bỏ các mẫu chứa văn bản trong hình ảnh. T-MARS [38] che vùng văn bản trong hình ảnh và sau đó lọc ra các mẫu có độ tương tự CLIP [49] thấp. Các phương pháp này không tránh khỏi mất nhiều thông tin thị giác khi lọc ra các mẫu, vì vậy một số phương pháp cố gắng có được dữ liệu chất lượng cao hơn bằng cách viết lại chú thích. Santurkar et al. [51] nhấn mạnh tầm quan trọng của các chú thích tổng hợp và sử dụng các mô hình ngôn ngữ được tiền huấn luyện để tăng cường nội dung văn bản. Gadre et al. [20] giới thiệu benchmark DataComp cho các bộ dữ liệu đa phương thức. Fan et al. [17] tận dụng khả năng học trong ngữ cảnh của các mô hình ngôn ngữ lớn để viết lại chú thích, làm phong phú cấu trúc ngôn ngữ trong khi bảo tồn ngữ nghĩa cốt lõi. Zhu et al. [70] sử dụng ChatGPT [45] và BLIP-2 [31] tương tác để tạo ra chú thích với thông tin thị giác phong phú. Nguyen et al. [42] sử dụng BLIP-2 [31] để viết lại chú thích cho các cặp hình ảnh-văn bản khớp thấp và kết hợp chú thích gốc và được tạo ra để huấn luyện. Lai et al. [29] đề xuất nâng cao thông tin thị giác trong chú thích bằng cách hợp nhất chú thích gốc với chú thích được tạo ra bởi LLaVA [36], tận dụng các mô hình ngôn ngữ lớn.

Trong khi các cách tiếp cận dựa trên việc viết lại chú thích đã mang lại kết quả hứa hẹn, việc phụ thuộc vào một mô hình duy nhất để viết lại đưa ra sự thiên vị vốn có của mô hình và đặt ra thách thức trong việc thiết lập một biểu diễn thống nhất giữa thị giác và ngôn ngữ. Ngược lại, phương pháp của chúng tôi giới thiệu các mô tả chính xác và đa dạng hơn cho một hình ảnh duy nhất trong khi giữ lại thông tin thị giác phong phú.

2.2 Mô hình ngôn ngữ lớn đa phương thức

Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) hiện có chủ yếu dựa vào ba công nghệ chính: Tinh chỉnh Hướng dẫn Đa phương thức (M-IT), Học Trong ngữ cảnh Đa phương thức (M-ICL), và Chuỗi Suy nghĩ Đa phương thức (M-CoT) [65]. M-IT tạo điều kiện cho hiệu suất chuyển giao mạnh mẽ bằng cách tinh chỉnh mô hình trên các bộ dữ liệu với định dạng hướng dẫn cụ thể. Các mô hình đáng chú ý sử dụng công nghệ này bao gồm LLaMA-Adapter [21,67], LLaVA [35,36], MiniGPT-4 [71], InstructBLIP [13], Qwen-VL [5], và NExT-GPT [62]. M-ICL là một loại học tương tự từ số lượng mẫu hạn chế. Các mô hình như Flamingo [3], Otter [30], và HuggingGPT [55] được phát triển sử dụng cách tiếp cận này. M-CoT yêu cầu các mô hình không chỉ cung cấp câu trả lời mà còn cả quá trình suy luận. Các mô hình đại diện bao gồm Multimodal-CoT [68] và Visual ChatGPT [61]. Ba công nghệ này không loại trừ lẫn nhau, và nhiều mô hình kết hợp hiệu quả nhiều công nghệ. Vì các MLLMs này thường được huấn luyện trên bộ dữ liệu tỷ mức, chúng có kiến thức cực kỳ phong phú và khả năng hiểu và biểu đạt thị giác xuất sắc, có thể được sử dụng để chú thích hình ảnh.

3 Phương pháp

3.1 Khái niệm cơ bản

Trong phần này, chúng tôi giới thiệu ngắn gọn các khái niệm cơ bản về tiền huấn luyện ngôn ngữ-thị giác. CLIP [49] là một nghiên cứu kinh điển sử dụng các cặp hình ảnh-văn bản để học tương phản. Nó sử dụng kiến trúc chỉ có encoder, tối ưu hóa encoder thông qua mất mát tương phản trên các cặp hình ảnh-văn bản. Trong quá trình huấn luyện, khi một batch gồm N cặp hình ảnh-văn bản {xI, xT} được lấy mẫu, mất mát tương phản cho một hình ảnh có thể được định nghĩa như sau:

LI = clip_contrast(xI, xT) (1)

Trong đó clip_contrast là mất mát tương phản hình ảnh-văn bản được giới thiệu trong [49]. Mất mát cho văn bản được tính toán theo cách tương tự và có thể được ký hiệu là LT. Tổng mất mát để huấn luyện là L = (LI + LT)/2.

BLIP [32] là một kiến trúc tiền huấn luyện ngôn ngữ-thị giác dựa trên encoder-decoder. Ba mục tiêu tiền huấn luyện chính của nó (mất mát tương phản hình ảnh-văn bản (ITC), mất mát khớp hình ảnh-văn bản (ITM), và mất mát mô hình ngôn ngữ (LM)) được tối ưu hóa đồng thời trong quá trình huấn luyện. Cụ thể, ITC tương tự như trong CLIP. ITM nhằm nắm bắt sự căn chỉnh chi tiết giữa thị giác và ngôn ngữ. LM được sử dụng để tối ưu hóa decoder.

Như được nhấn mạnh trong nhiều nghiên cứu [33,41,66], việc tiền huấn luyện ngôn ngữ-thị giác phụ thuộc mạnh vào các bộ dữ liệu hình ảnh-văn bản mở rộng. Sự hiện diện của dữ liệu hình ảnh-văn bản chất lượng thấp có thể làm tổn hại đáng kể hiệu suất của mô hình. Do đó, việc cải thiện bộ dữ liệu đa phương thức đã trở thành một hướng được mong đợi.

3.2 Tổng quan

Trong nghiên cứu này, chúng tôi sử dụng nhiều MLLMs tiên tiến để tăng cường việc học biểu diễn ngôn ngữ-thị giác từ góc độ tập trung vào dữ liệu. Chúng tôi chứng minh rằng các MLLMs khác nhau có thể tạo ra các chú thích chính xác và đa dạng trong Hình 2. Mục tiêu của cách tiếp cận này là sử dụng MLLMs để thiết lập các liên kết hình ảnh-văn bản phong phú hơn trong các bộ dữ liệu hiện tại. Phương pháp của chúng tôi bao gồm hai quy trình: trích xuất chú thích đa góc nhìn và cắt tỉa văn bản. Cụ thể, đối với mỗi hình ảnh từ một bộ dữ liệu hình ảnh-văn bản, đầu tiên chúng tôi giới thiệu nhiều MLLMs tiên tiến để tổng hợp các chú thích mở rộng. Sau đó, bằng cách phân tích toàn diện các đặc điểm của những chú thích mở rộng này, chúng tôi có một quan sát chính: MLLMs có phong cách chú thích của riêng chúng và có thể tạo ra nội dung ảo giác. Dựa trên quan sát này, chúng tôi đề xuất cắt tỉa văn bản để duy trì chất lượng và tính khả dụng của các chú thích mở rộng. Sau các quy trình này, các chú thích thô và mở rộng với các hình ảnh tương ứng được sử dụng đồng thời cho việc tiền huấn luyện ngôn ngữ-thị giác tiêu chuẩn.

3.3 Trích xuất chú thích đa góc nhìn

Cho một bộ dữ liệu T = {(xi
I, xi
T)}N
i=1, chứa N cặp hình ảnh xi
I và văn bản xi
T được ghép đôi. Chúng tôi định nghĩa một nhóm mô hình G = {g1···gk···gK} bao gồm K MLLMs tiên tiến.

Đối với mỗi hình ảnh xi
I, G được sử dụng để có được các chú thích phong phú. Thao tác có thể được công thức hóa như sau:

Ci = {ci
1···ci
n···ci
N} = G(xi
I, Q) (2)

Trong đó Ci đại diện cho các chú thích mở rộng cho hình ảnh xi
I, và Q biểu thị đầu vào câu hỏi cho MLLM. Đối với các mô hình khác nhau trong G, chúng tôi sử dụng cùng một mẫu câu hỏi đơn giản: {Mô tả ⟨hình ảnh⟩ bằng tiếng Anh:} để truy vấn các chú thích. Câu hỏi đơn giản có ít tác động đến tính đa dạng của câu trả lời, vì vậy chúng tôi có thể thu được các chú thích toàn diện của mỗi hình ảnh.

Làm thế nào để sử dụng những chú thích mở rộng này? Sau khi thu được những chú thích mở rộng này, một trong những ý tưởng trực tiếp nhất là thay thế một tập hợp các chú thích thô bằng chú thích mới để huấn luyện. Hiệu quả của nó đã được đánh giá trong nhiều nghiên cứu trước đây [42,70]. Tuy nhiên, chúng tôi có mối quan tâm về thao tác này về mặt sự khác biệt giữa chú thích thô và mới. Để điều tra điều này, chúng tôi đếm tần suất của các danh từ xuất hiện nhiều nhất của các chú thích mở rộng (được trích xuất từ hình ảnh trong CC3M bằng MiniGPT-4, Otter, Qwen-VL, và LLaVA-1.5, các ví dụ được thể hiện trong Hình 2) và trình bày kết quả trong Hình 3. Người ta có thể dễ dàng thấy rằng các MLLMs khác nhau đưa ra các từ phổ biến khác nhau trong các chú thích được tạo ra. Những từ đa dạng này có thể mở rộng rất nhiều các khái niệm ngôn ngữ của bộ dữ liệu.

Dựa trên phân tích trên, chúng tôi dự định sử dụng tất cả các chú thích mở rộng để huấn luyện nhằm cải thiện tính đa dạng của chú thích. Tuy nhiên, có một mối quan tâm khác về sự khác biệt về độ dài của chú thích thô và mở rộng. Chúng tôi trình bày các so sánh độ dài trung bình của những chú thích này trong Hình 4a. Những chú thích này được tạo ra bởi MLLMs dưới các cài đặt mặc định. Có thể thấy rằng các chú thích được tạo ra bởi MiniGPT-4 và LLaVA-1.5 dài hơn đáng kể, tức là 90 token. Trong khi chú thích thô ngắn hơn, tức là 15 token. Để điều tra ảnh hưởng của độ dài chú thích khác nhau đến tiền huấn luyện, chúng tôi hình dung chú thích dài hơn được tạo ra bởi MiniGPT-4 trong Hình 4b. Chúng tôi thấy rằng các chú thích dài hơn thực sự tạo ra các ảo giác không khớp với hình ảnh, và những ảo giác này thường xuất hiện ở phần sau của văn bản được tạo ra, điều này có tác động đến việc học biểu diễn. Kết luận tương tự cũng được xác minh trong [69]. Nó tuyên bố rằng khi việc tạo ra tiến triển, sự tích lũy của thông tin ảo giác trong quá khứ và sự không chắc chắn có thể làm mô hình lệch hướng và dẫn đến các ảo giác mới.

3.4 Cắt tỉa văn bản

Để loại bỏ tác động của độ dài chú thích quá mức, chúng tôi đề xuất cắt tỉa văn bản để làm cho độ dài của các chú thích mở rộng giống như các chú thích thô. Cụ thể, chúng tôi thêm một số token T như một giới hạn khi sử dụng MLLMs để tạo ra chú thích. Để ngăn chặn các chú thích được tạo ra có biểu đạt không đầy đủ, chúng tôi chặn mệnh đề hoàn chỉnh đầu tiên trong chú thích được tạo ra. Do đó, Phương trình 2 có thể được viết lại như:

C′
i = {ci′
1···ci′
n···ci′
N} = G(xi
I, Q, T) (3)

Đối với việc thiết lập T, chúng tôi sử dụng một chiến lược đơn giản nhưng hiệu quả. Chúng tôi đặt nó bằng độ dài trung bình của các chú thích gốc để bắt chước chú thích của con người. Bằng cách kiểm soát độ dài của các chú thích được tạo ra, chúng tôi hy vọng có thể giảm hiệu quả tác động của các ảo giác của MLLMs và phong cách ngôn ngữ đơn điệu của chúng.

Sau đó, chúng tôi sử dụng đồng thời chú thích gốc và chú thích mới được tạo ra để xây dựng một bộ dữ liệu mới. Các cặp hình ảnh-văn bản mới có thể được biểu thị là {xI, x′
T} = {xI, ci′
1}, {xI, ci′
2}···, {xI, ci′
k}, trong đó k đại diện cho số lượng MLLMs. Chúng tôi sử dụng bộ dữ liệu được tăng cường cho việc tiền huấn luyện ngôn ngữ-thị giác tiêu chuẩn tiếp theo. Lấy CLIP làm ví dụ, mất mát trên hình ảnh có thể được viết như:

LI = clip_contrast(xI, x′
T) (4)

Tổng mất mát L = (LI + LT)/2. Các tham số β cho CLIP được cập nhật bằng cách tối thiểu hóa L:

β ← arg min
β L. (5)

4 Thí nghiệm

4.1 Chi tiết tiền huấn luyện

Phương pháp của chúng tôi được triển khai trong Pytorch [47] và được huấn luyện trên một node được trang bị 8 GPU NVIDIA A100. Đối với tiền huấn luyện ngôn ngữ thị giác, chúng tôi tuân theo việc triển khai CLIP [49] và BLIP [32] tương ứng. Đối với CLIP, chúng tôi huấn luyện mô hình dựa trên mã nguồn mở OpenCLIP [24]. Trong quá trình huấn luyện, kích thước batch của chúng tôi được đặt là 320. Số epoch được đặt là 6 để căn chỉnh tốt hơn với chi phí huấn luyện. Kiến trúc mô hình được đặt là VIT-B-16. Các tham số khác giống với các giá trị mặc định trong OpenCLIP [24]. Đối với BLIP, chúng tôi sử dụng ViT-B-16 [16,59] được tiền huấn luyện trên ImageNet [14] làm encoder hình ảnh và Bert base [15] làm encoder văn bản. Về tiền xử lý hình ảnh, chúng tôi sử dụng cắt ngẫu nhiên để đạt được 224×224 trong tiền huấn luyện và tăng độ phân giải hình ảnh lên 384×384 trong tinh chỉnh. Chúng tôi áp dụng RandomAugment [12] để tăng cường hình ảnh. Để tối ưu hóa, chúng tôi sử dụng bộ tối ưu hóa AdamW [37] với tốc độ học khởi động lên 3e-4, kết hợp với suy giảm tuyến tính ở tốc độ 0.9. Kích thước batch huấn luyện của chúng tôi được đặt là 1280, và số epoch huấn luyện là 4.

Về việc sử dụng các mô hình ngôn ngữ lớn đa phương thức, chúng tôi sử dụng Mini-GPT4-Vicuna13B [71], Otter-Image-MPT7B [30], Qwen-VL-Chat [5], và LLaVA-v1.5-13B [35]. Trong quá trình tạo chú thích, chúng tôi áp dụng giới hạn token tối đa là 30 để kiểm soát độ dài chú thích. Để tạo chú thích tổng hợp, chúng tôi sử dụng beam search với số beam bằng 1.

Chúng tôi sử dụng CC3M [54], CC12M [8] và YFCC15M làm bộ dữ liệu tiền huấn luyện. Trong số đó, YFCC15M là một tập con của YFCC100M [58] chứa các mô tả tiếng Anh về hình ảnh. Do không thể truy cập một số hình ảnh, phiên bản bộ dữ liệu mà chúng tôi sử dụng chứa ít hình ảnh hơn so với bộ dữ liệu gốc. Trong số đó, trên CC3M, so với phiên bản 3.3M của bộ dữ liệu gốc, phiên bản chúng tôi sử dụng chứa 2.6M cặp hình ảnh-văn bản; trên CC12M, so với phiên bản 12.4M của bộ dữ liệu gốc, phiên bản chúng tôi sử dụng chứa 11.1M cặp hình ảnh-văn bản; Trên YFCC15M, so với 15M của bộ dữ liệu gốc, phiên bản của chúng tôi chứa 14.8M cặp hình ảnh-văn bản.

4.2 Đánh giá

Chúng tôi đánh giá hiệu suất của nhiệm vụ đa phương thức trong Bảng 1 và Bảng 4. Chúng tôi cũng cung cấp kết quả về nhiệm vụ phân loại hình ảnh trong Bảng 2 và Bảng 3.

Truy xuất hình ảnh-văn bản. Truy xuất hình ảnh-văn bản phục vụ như một chỉ số quan trọng để đánh giá khả năng nối cầu giữa các phương thức khác nhau trong một mô hình. Chúng tôi tiền huấn luyện CLIP và BLIP trên CC3M, tiến hành thử nghiệm truy xuất hình ảnh thành văn bản (TR) và văn bản thành hình ảnh (IR) trên các bộ dữ liệu MSCOCO [34] và Flickr30K [48]. Kết quả trong Bảng 1 chứng minh cải thiện hiệu suất đáng kể trong cả truy xuất zero-shot và tinh chỉnh cho cả CLIP và BLIP. Đáng chú ý, phương pháp của chúng tôi cho phép hiệu suất truy xuất zero-shot của CLIP vượt trội hơn các mô hình được tinh chỉnh trên bộ dữ liệu mục tiêu. Cụ thể, khi sử dụng CLIP để truy xuất zero-shot trên MSCOCO, R@1 của TR và IR tăng lần lượt 27.2 và 19.4. Để truy xuất zero-shot trên Flickr30K, R@1 của TR và IR cũng cải thiện lần lượt 46.1 và 35.4. Tương tự, phương pháp của chúng tôi cũng đạt được cải thiện 16.8 ∼23.4 khi sử dụng BLIP để truy xuất zero-shot. Những cải thiện hiệu suất đáng kể này chỉ ra rằng mô hình được tiền huấn luyện học được biểu diễn hình ảnh-văn bản tốt hơn.

Phân loại hình ảnh. Chúng tôi đánh giá hiệu suất của phương pháp chúng tôi trong nhiệm vụ phân loại hình ảnh. Trong Bảng 2, chúng tôi tiền huấn luyện CLIP với kiến trúc VIT-B-16 trên CC3M và CC12M tương ứng, sau đó thử nghiệm trên mười sáu bộ dữ liệu phân loại hình ảnh thông thường. Chúng tôi so sánh kết quả của mình với [17]. Phương pháp của chúng tôi thể hiện cải thiện hiệu suất đáng kể trên hầu hết các bộ dữ liệu, như đã chứng minh. Đối với CLIP được tiền huấn luyện trên CC3M, có cải thiện trung bình 13.4 trên 15 bộ dữ liệu và cải thiện 13.1 trên ImageNet [14]. Đối với CLIP được tiền huấn luyện trên CC12M, có cải thiện trung bình 11.1 và 10.2 tương ứng. Những kết quả này nêu bật việc nâng cao hiệu quả của việc học biểu diễn được đạt được bởi phương pháp của chúng tôi.

Linear-Probing. Chúng tôi cũng khám phá hiệu suất linear-probing trong phân loại hình ảnh. Chúng tôi sử dụng CLIP được tiền huấn luyện trên CC3M và CC12M để đánh giá tương ứng. Tương tự như phân loại hình ảnh zero-shot, chúng tôi so sánh phương pháp của mình với [17] trên 15 bộ dữ liệu thông thường. Kết quả trong Bảng 3 cho thấy phương pháp của chúng tôi cũng có những cải thiện nhất định cho linear probing. Điều này cho thấy các khái niệm văn bản phong phú đóng góp vào việc huấn luyện hiệu quả của các encoder thị giác. Khi encoder thị giác học từng hình ảnh, nó được hưởng lợi từ sự giám sát thông qua nhiều chú thích chính xác và đa dạng. Điều này cho phép nó có được một biểu diễn phổ quát của hình ảnh một cách hiệu quả, dẫn đến việc đặc trưng hóa hình ảnh và không gian embedding văn bản hiệu quả hơn.

Trả lời câu hỏi thị giác. Trả lời câu hỏi thị giác (VQA) [4] là nhiệm vụ cung cấp câu trả lời dựa trên hình ảnh và câu hỏi. Chúng tôi tiến hành thử nghiệm trên VQAv2 [22], A-OKVQA [53] và OK-VQA [40] sử dụng BLIP được tiền huấn luyện trên CC3M, CC12M và YFCC15M, tương ứng. Kết quả được trình bày trong Bảng 4. Tương tự như BLIP [32], chúng tôi cũng coi VQA là một nhiệm vụ tạo ra câu trả lời. Những cải thiện hiệu suất nhất quán thu được thông qua phương pháp của chúng tôi chỉ ra rằng mô hình đã có được một biểu diễn ngôn ngữ-thị giác mạnh mẽ hơn từ các bộ dữ liệu được làm phong phú bằng kiến thức của MLLMs. Bên cạnh đó, sự cải thiện trên A-OKVQA và OK-VQA chỉ ra rằng mô hình có nhiều thông thức và kiến thức về thế giới hơn. Những kết quả này cho thấy khả năng nâng cao trong hiểu biết thị giác và khả năng ngôn ngữ cùng nhau.

Suy luận thị giác. Trong nhiệm vụ Suy luận thị giác ngôn ngữ tự nhiên (NLVR2) [57], mô hình được yêu cầu thực hiện suy luận đa phương thức bằng cách phân tích hai hình ảnh và một câu hỏi ngôn ngữ tự nhiên. Chúng tôi tiến hành đánh giá sử dụng BLIP được tiền huấn luyện trên CC3M, CC12M, và YFCC15M, tương ứng. Như được mô tả trong Bảng 4, phương pháp của chúng tôi liên tục mang lại hiệu suất cải thiện. Điều này cho thấy rằng mô hình có tiến bộ nhất định trong hiểu biết ngôn ngữ tự nhiên, nhận dạng thị giác và suy luận logic.

Chú thích hình ảnh. Chú thích hình ảnh là nhiệm vụ tạo ra mô tả văn bản về nội dung hình ảnh được cho một hình ảnh. Chúng tôi tiến hành thử nghiệm trên COCO và Nocaps [2] sử dụng BLIP được tiền huấn luyện trên CC3M, CC12M, và YFCC15M, tương ứng. Theo cách tiếp cận trong [32], chúng tôi ban đầu tinh chỉnh mô hình được tiền huấn luyện với mất mát LM trên COCO. Để đạt được kết quả tốt hơn, chúng tôi cũng thêm "một bức tranh của" vào đầu prompt. Kết quả được trình bày trong Bảng 4, cho thấy cải thiện trong các chỉ số BLEU@4 và CIDEr. Những phát hiện này cho thấy rằng phương pháp của chúng tôi nâng cao hiểu biết của mô hình về mối quan hệ giữa văn bản và hình ảnh, dẫn đến độ tương tự cao hơn với các chú thích của con người trong quá trình chú thích.

Nhiệm vụ ngôn ngữ-video. Truy xuất từ văn bản sang video là một chỉ số để đánh giá khả năng tổng quát hóa của mô hình trong các nhiệm vụ ngôn ngữ-video. Chúng tôi đánh giá phương pháp của mình trên bộ dữ liệu MSRVTT. Theo [32], chúng tôi tinh chỉnh mô hình trên COCO. Đối với đầu vào video, chúng tôi lấy mẫu đều 8 khung hình từ nó để có được một chuỗi. Kết quả trong Bảng 4 chứng minh cải thiện hiệu suất ổn định được đạt bởi phương pháp của chúng tôi. Điều này cho thấy rằng việc học biểu diễn ngôn ngữ-thị giác mạnh mẽ có thể là chìa khóa cho truy xuất văn bản-video.

4.3 Phân tích

Khả năng mở rộng quy mô. Đáng chú ý, như được thể hiện trong Bảng 4, phương pháp của chúng tôi cho thấy những cải thiện nhất định khi tiền huấn luyện BLIP trên CC12M [8] và YFCC15M [58]. Điều này chỉ ra rằng phương pháp của chúng tôi có thể mở rộng quy mô lên các bộ dữ liệu lớn hơn ở một mức độ nào đó. Nó cũng nhấn mạnh tầm quan trọng của việc xây dựng các cặp hình ảnh-văn bản được biểu diễn tốt để nâng cao tiền huấn luyện ngôn ngữ-thị giác.

Chi phí huấn luyện. Để đảm bảo so sánh công bằng với các phương pháp cơ sở, chúng tôi cẩn thận xây dựng lịch trình huấn luyện. Xem xét rằng số lượng cặp hình ảnh-văn bản trong bộ dữ liệu được tăng cường là k lần số lượng cặp hình ảnh-văn bản trong bộ dữ liệu gốc, chúng tôi điều chỉnh các epoch tiền huấn luyện thành 1/k của các epoch gốc. Bằng cách làm điều này, chúng tôi tránh đưa thêm chi phí huấn luyện.

4.4 Nghiên cứu loại trừ

Độ dài chú thích. Độ dài của các chú thích được tạo ra là một trong những yếu tố ảnh hưởng đến tiền huấn luyện ngôn ngữ-thị giác. Chúng tôi sử dụng MiniGPT-4 để tạo chú thích cho CC3M với các giới hạn số token tối đa khác nhau. Bằng cách huấn luyện BLIP trên chú thích với độ dài khác nhau và tiến hành truy xuất trên MSCOCO, chúng tôi trình bày kết quả trong Hình 5a và quan sát thấy rằng khi độ dài chú thích tăng, hiệu suất của mô hình có xu hướng giảm. Điều này có thể là do các chú thích quá dài khiến các đặc trưng văn bản tương tự hơn giữa các chú thích của hình ảnh khác nhau. Chúng tôi minh họa hiện tượng này trong Hình 6. Khi độ dài chú thích tăng, các chú thích được tạo ra thường trở nên giống hệt nhau trong nhiều trường hợp. Điều này dẫn đến các hình ảnh khác nhau dễ dàng ánh xạ tới các đặc trưng văn bản giống nhau, tạo ra thách thức trong việc học một biểu diễn hiệu quả.

Kích thước batch. Chúng tôi khám phá tác động của kích thước batch đối với huấn luyện trong Hình 5b. Việc huấn luyện các mô hình trên chú thích tổng hợp đòi hỏi kích thước batch tương đối cao so với việc sử dụng chú thích được chú thích bởi con người. Chúng tôi cho rằng điều này là do với kích thước batch nhỏ, thông tin gradient chính thu được là nhiễu, khác biệt đáng kể so với gradient thực. Ngược lại, với kích thước batch tăng, gradient của phân phối được lấy mẫu trở nên tương tự hơn với gradient của phân phối thực, cho phép mô hình tổng quát hóa tốt hơn.

Số lượng MLLMs. Chúng tôi khám phá tác động của số lượng MLLMs đối với hiệu suất chuyển giao trong Hình 5c. Chúng tôi thêm bốn MLLMs {MiniGPT-4, Otter, Qwen-VL, và LLAVA-1.5} theo thứ tự. Sử dụng các mô hình được tiền huấn luyện cho truy xuất zero-shot trên MSCOCO, chúng tôi quan sát thấy rằng, khi số lượng MLLMs tăng, hiệu suất cải thiện dần dần. Tuy nhiên, mức độ cải thiện này đang giảm dần, cho thấy thông tin chú thích của một hình ảnh thu được từ MLLM đang đạt tới mức bão hòa.

Epochs huấn luyện. Chúng tôi hình dung đường cong mô tả số epochs huấn luyện so với hiệu suất của mô hình trong nhiệm vụ truy xuất trong Hình 5d. Rõ ràng rằng phương pháp của chúng tôi đạt được hiệu suất hứa hẹn chỉ với số epoch nhỏ. Trong thí nghiệm chính, chúng tôi chọn số epoch 4 để so sánh với cơ sở mà không tăng chi phí huấn luyện. Khi tăng thêm số epoch, phương pháp của chúng tôi tiếp tục cho thấy những cải thiện nhất định.

Tác động độc lập của MLLM. Chúng tôi cũng đánh giá hiệu suất của việc viết lại chú thích chỉ sử dụng một MLLM và sử dụng các chú thích được tạo ra để huấn luyện. Chú thích được viết lại sử dụng MiniGPT-4, Otter, Qwen-VL, và LLaVA-1.5 tương ứng, và kết quả được trình bày trong Bảng 6. Kết quả cho thấy rằng có giới hạn trên cho cải thiện hiệu suất có thể đạt được bằng cách viết lại chỉ với một MLLM. Bên cạnh đó, các MLLMs khác nhau cũng có điểm mạnh khác nhau trong kiến thức. Kết hợp kiến thức của nhiều MLLMs tỏ ra hiệu quả hơn trong việc nâng cao tiền huấn luyện ngôn ngữ-thị giác.

4.5 Hình dung

Hình dung chú thích hình ảnh. Trong Hình 7, chúng tôi minh họa sự khác biệt trong chú thích hình ảnh giữa các mô hình được tiền huấn luyện trên bộ dữ liệu CC3M gốc và bộ dữ liệu cải tiến của chúng tôi. Mà không cần bất kỳ tinh chỉnh nào, mô hình của chúng tôi thể hiện cải thiện đáng kể trong khả năng nhận dạng các khái niệm thị giác trong hình ảnh.

Hình dung phân phối hình ảnh-văn bản. Chúng tôi hình dung phân phối đặc trưng của các cặp hình ảnh-văn bản từ CC3M gốc và bộ dữ liệu của chúng tôi trong Hình 8a. Có thể quan sát thấy rằng bộ dữ liệu CC3M gốc chứa nhiều cặp hình ảnh-văn bản rời rạc và không khớp. Ngược lại, bộ dữ liệu của chúng tôi thể hiện một phân phối trong đó hầu như tất cả hình ảnh đều có văn bản tương ứng. Góc độ phân phối này giải thích cách phương pháp của chúng tôi căn chỉnh hình ảnh liên tục hơn với ngôn ngữ rời rạc, từ đó nâng cao việc học biểu diễn ngôn ngữ-thị giác.

Hình dung độ tương tự cosine. Chúng tôi hình dung so sánh độ tương tự cosine giữa bộ dữ liệu CC3M gốc và bộ dữ liệu CC3M được tăng cường của chúng tôi trong Hình 8b. Chúng tôi sử dụng CLIP được tiền huấn luyện để tính toán khoảng cách cosine giữa embeddings hình ảnh và embeddings văn bản. Phương pháp của chúng tôi thể hiện độ tương tự trung bình cao hơn, điều này chứng minh hiệu quả trong việc cải thiện mức độ khớp của các cặp hình ảnh-văn bản và thiết lập các liên kết hình ảnh-văn bản mạnh mẽ cho các bộ dữ liệu.

4.6 So sánh với nhiều phương pháp hơn

Chúng tôi cũng so sánh công trình của mình với công trình đương thời VeCLIP [29] trong cùng một thiết lập trong Bảng 7. Chúng tôi sử dụng CLIP được tiền huấn luyện để so sánh kết quả về truy xuất hình ảnh-văn bản và phân loại hình ảnh. Có thể thấy rằng phương pháp của chúng tôi đạt được cải thiện hiệu suất tốt hơn so với việc hợp nhất chú thích được sử dụng trong VeCLIP. Điều này cũng ngụ ý rằng việc thiết lập các tương ứng hình ảnh-văn bản liên tục hơn là chìa khóa để cải thiện tiền huấn luyện ngôn ngữ-thị giác trên các bộ dữ liệu quy mô nhỏ. Chúng tôi cũng so sánh phương pháp của mình với [42] trong phụ lục.

5 Kết luận và thảo luận

Trong bài báo này, chúng tôi đề xuất tăng cường việc học biểu diễn ngôn ngữ-thị giác bằng cách tận dụng nhiều MLLMs. Trong khi giữ lại thông tin thị giác phong phú của bộ dữ liệu gốc, chúng tôi sử dụng nhiều MLLMs để mở rộng các chú thích đa dạng cho mỗi hình ảnh. Ngoài ra, chúng tôi giới thiệu "cắt tỉa văn bản" để giải quyết các vấn đề về ảo giác và phong cách ngôn ngữ đơn điệu trong chú thích tổng hợp. Được xác thực trên các khung tiền huấn luyện ngôn ngữ-thị giác và bộ dữ liệu khác nhau, phương pháp của chúng tôi cải thiện đáng kể hiệu suất trên nhiều nhiệm vụ downstream. Điều này khuyến khích khám phá thêm về việc sử dụng MLLMs trong tương lai.

Hạn chế và nghiên cứu tương lai Mặc dù phương pháp của chúng tôi đã đạt được kết quả đáng kể trong việc nâng cao học biểu diễn ngôn ngữ-thị giác, một tỷ lệ nhất định của nhiễu vẫn tồn tại do đầu ra không đáng tin cậy của MLLMs. Những nhiễu này hạn chế việc cải thiện thêm hiệu suất của mô hình. Nghiên cứu tương lai có thể khám phá việc tận dụng các MLLMs mạnh mẽ hơn để tạo ra chú thích chính xác và mở rộng sang các bộ dữ liệu lớn hơn.
