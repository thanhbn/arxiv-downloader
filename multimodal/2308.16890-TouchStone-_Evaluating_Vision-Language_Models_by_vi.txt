# TouchStone: Đánh giá Mô hình Thị giác-Ngôn ngữ bằng Mô hình Ngôn ngữ

1Shuai Bai1,2Shusheng Yang1Jinze Bai1Peng Wang1,3Xingxuan Zhang
1Junyang Lin2Xinggang Wang1Chang Zhou† 1Jingren Zhou
1Alibaba Group,2Đại học Khoa học và Công nghệ Hoa Trung,3Đại học Thanh Hoa
6 tháng 9, 2023

## Tóm tắt

Các mô hình thị giác-ngôn ngữ lớn (LVLMs) gần đây đã chứng kiến những tiến bộ nhanh chóng, thể hiện khả năng đáng chú ý trong việc cảm nhận, hiểu và xử lý thông tin thị giác bằng cách kết nối bộ thu thị giác với các mô hình ngôn ngữ lớn (LLMs). Tuy nhiên, các đánh giá hiện tại chủ yếu tập trung vào khả năng nhận dạng và lập luận, thiếu đánh giá trực tiếp về kỹ năng đối thoại và bỏ qua khả năng kể chuyện bằng hình ảnh. Trong bài báo này, chúng tôi đề xuất một phương pháp đánh giá sử dụng các LLM mạnh làm giám khảo để đánh giá toàn diện các khả năng khác nhau của LVLMs. Đầu tiên, chúng tôi xây dựng một bộ dữ liệu đối thoại thị giác toàn diện TouchStone, bao gồm các hình ảnh và câu hỏi thế giới mở, bao phủ năm danh mục khả năng chính và 27 nhiệm vụ phụ. Bộ dữ liệu này không chỉ bao gồm nhận dạng và hiểu biết cơ bản mà còn mở rộng đến sáng tạo văn học. Thứ hai, bằng cách tích hợp các chú thích hình ảnh chi tiết, chúng tôi hiệu quả biến đổi nội dung đầu vào đa phương thức thành dạng có thể hiểu được bởi LLMs. Điều này cho phép chúng tôi sử dụng các LLM tiên tiến để đánh giá trực tiếp chất lượng của đối thoại đa phương thức mà không cần sự can thiệp của con người. Thông qua xác thực, chúng tôi chứng minh rằng các LVLM mạnh mẽ, như GPT-4, có thể chấm điểm hiệu quả chất lượng đối thoại bằng cách tận dụng khả năng văn bản của chúng, phù hợp với sở thích của con người. Chúng tôi hy vọng công việc của chúng tôi có thể phục vụ như một tiêu chuẩn đánh giá cho LVLMs và mở đường cho việc xây dựng các LVLM mạnh mẽ hơn. Mã đánh giá có sẵn tại https://github.com/OFA-Sys/TouchStone.

## 1 Giới thiệu

Việc sử dụng các mô hình ngôn ngữ lớn (LLMs) (Zhang et al., 2022; Gao et al., 2023b; Brown et al., 2020; OpenAI, 2023; Anil et al., 2023) trong lĩnh vực chatbot (Ouyang et al., 2022; Chiang et al., 2023) đã thể hiện sức mạnh đáng chú ý trong nhiều khía cạnh khác nhau như hiểu biết ngôn ngữ, tạo sinh và tương tác. Việc mở rộng GPT-4 (OpenAI, 2023) để bao gồm LLMs đã thúc đẩy hơn nữa sự phát triển nhanh chóng của các mô hình thị giác-ngôn ngữ lớn (LVLMs). Gần đây, một số LVLMs (Dai et al., 2023; Li et al., 2023a; Zhu et al., 2023; Su et al., 2023; Li et al., 2023b; Liu et al., 2023a; Ye et al., 2023; Gao et al., 2023a) đã được đề xuất với mục tiêu mở rộng khả năng của chatbot chỉ văn bản để bao gồm chatbot đa phương thức. Điều này được thực hiện thông qua việc liên kết các bộ mã hóa thị giác với LLMs và áp dụng các kỹ thuật điều chỉnh chỉ dẫn thị giác. Tuy nhiên, đáng chú ý là việc đánh giá những LVLM gần đây này chủ yếu tập trung vào đánh giá của con người về chất lượng tạo sinh trong một tập con hạn chế của câu hỏi, do đó thiếu đánh giá định lượng toàn diện.

Những phát triển gần đây trong phương pháp đánh giá LLM (Zheng et al., 2023), sử dụng đánh giá mô hình tự động, đã cho thấy tiềm năng đáng khích lệ về mặt hiệu quả và hiệu quả chi phí khi so sánh với đánh giá thủ công. Tuy nhiên, bất chấp những tiến bộ đáng kể này trong khả năng dựa trên văn bản, việc kết hợp đầu vào đa phương thức vào LLMs vẫn bị hạn chế và chưa được khám phá đầy đủ.

Hiện tại, các phương pháp đánh giá cho LVLMs chủ yếu liên quan đến việc so sánh các mô hình khác nhau dựa trên một tập nhỏ câu hỏi hoặc đánh giá hiệu suất của chúng trên các nhiệm vụ đa phương thức truyền thống như VQA (Goyal et al., 2017; Sidorov et al., 2020), mô tả hình ảnh (Agrawal et al., 2019; Chen et al., 2015a), và phân loại hình ảnh (Deng et al., 2009). Tuy nhiên, các số liệu và chú thích nhiệm vụ truyền thống thường có sở thích phong cách cụ thể vì mục đích đánh giá và so sánh. Những sở thích phong cách này (Agrawal et al., 2019; Chen et al., 2015a; Goyal et al., 2017) không nhất thiết phù hợp với sở thích của con người, như VQA và mô tả hình ảnh. Bên cạnh đó, việc có được xếp hạng hoặc so sánh của người chú thích cho đầu ra của các mô hình khác nhau là cấm đoán và khó để mở rộng thêm. Ngoài ra, ảo giác là một trở ngại quan trọng đối với việc ứng dụng rộng rãi hơn của LLMs hoặc LVLMs hiện tại, nhưng cách đánh giá mức độ ảo giác của LVLMs luôn bị bỏ qua trong hầu hết các đánh giá LVLM hiện tại và vẫn cần được khám phá. Do đó, có nhu cầu cấp thiết về các kỹ thuật đánh giá tự động có thể cung cấp đánh giá khách quan và hiệu quả về hiệu suất LVLM trong các cuộc đối thoại mở. Gần đây, MME (Fu et al., 2023) đã được đề xuất để biến đổi câu hỏi thành các câu phán đoán nhị phân để đánh giá mô hình lớn. MMBench (Liu et al., 2023b) đánh giá mô hình dựa trên độ chính xác của chúng trong việc chọn câu trả lời. Tuy nhiên, phán đoán nhị phân và khả năng chọn câu trả lời có thể không nắm bắt đầy đủ sự phức tạp trong các cuộc đối thoại thế giới thực mở, do đó hạn chế tính phù hợp của chúng như một phương pháp đánh giá toàn diện.

Để giải quyết những thách thức này, chúng tôi đề xuất một phương pháp đánh giá tự động được gọi là TouchStone, cung cấp đánh giá toàn diện về khả năng của các mô hình ngôn ngữ đa phương thức. Nguyên tắc thiết kế của chúng tôi có hai khía cạnh:

Đầu tiên, để đánh giá khả năng tổng thể của các mô hình, chúng tôi xây dựng một bộ dữ liệu đối thoại thị giác toàn diện, bao gồm năm danh mục khả năng chính và 27 nhiệm vụ phụ. Những danh mục này bao gồm khả năng mô tả cơ bản, khả năng nhận dạng thị giác, khả năng hiểu biết thị giác, khả năng kể chuyện thị giác, và khả năng phân tích đa hình ảnh. Điều này không chỉ kiểm tra khả năng nhận dạng và hiểu biết của mô hình mà còn kiểm tra khả năng sáng tạo văn học và phân tích của nó. Các hình ảnh và câu hỏi trong bộ dữ liệu của chúng tôi được tuyển chọn trong môi trường thế giới mở và đã được chú thích và xác minh thủ công.

Thứ hai, TouchStone liên quan đến việc chuyển đổi thông tin từ các phương thức khác, chẳng hạn như hình ảnh, thành dạng văn bản bằng cách sử dụng các chú thích và mô tả hình ảnh chi tiết. Điều này cho phép việc sử dụng các LLM tiên tiến để đánh giá trực tiếp chất lượng của các cuộc đối thoại mà không cần người đánh giá hoặc LLM được tăng cường thị giác. Để phản ánh hiệu suất của mô hình trong các tình huống thế giới thực, chúng tôi tiến hành đánh giá trực tiếp chất lượng đối thoại bằng cách so sánh tính đúng đắn, liên quan và hữu ích của nó. Để chấm điểm, chúng tôi sử dụng một mô hình ngôn ngữ hàng đầu làm giám khảo, so sánh các phản hồi của LVLM khác nhau với các câu trả lời được tạo bởi GPT-4 bằng cách sử dụng so sánh cặp đôi. Phản hồi được tạo bởi GPT-4 được thu được thông qua đầu vào của các chú thích hình ảnh chi tiết và câu hỏi, và được gọi là GPT4-HA (Human Assisted). Để giải quyết thiên vị vị trí, chúng tôi kết hợp cân bằng vị trí vào cơ chế chấm điểm của chúng tôi. Thông qua so sánh với đánh giá của con người, chúng tôi thấy rằng các LLM mạnh mẽ như GPT-4 (OpenAI, 2023) có thể chấm điểm hiệu quả chất lượng đối thoại dựa trên khả năng văn bản của chúng, đồng thời cũng có thể nhận biết các vấn đề ảo giác.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi tuyển chọn một bộ dữ liệu đối thoại thị giác đa dạng bao gồm năm danh mục khả năng và 27 nhiệm vụ phụ, bao gồm không chỉ nhận dạng và hiểu biết cơ bản mà còn mở rộng đến sáng tạo văn học.
• TouchStone chuyển đổi thông tin từ các phương thức khác, chẳng hạn như hình ảnh, thành dạng văn bản bằng cách sử dụng các chú thích chi tiết. Điều này cho phép các mô hình ngôn ngữ tiên tiến đánh giá trực tiếp chất lượng đối thoại mà không cần sự can thiệp thủ công.
• Chúng tôi cho thấy rằng GPT-4 có thể phục vụ như một người đánh giá hợp lý để đánh giá chất lượng phản hồi của LVLMs. Cụ thể, trong thí nghiệm của chúng tôi, chúng tôi thấy rằng GPT-4 cho thấy phán đoán nhất quán cao so với sở thích của con người.

## 2 Công việc liên quan

### 2.1 Mô hình Ngôn ngữ Lớn

Các mô hình được tiền huấn luyện ngôn ngữ như GPT (Radford et al., 2019; Brown et al., 2020), BERT (Devlin et al., 2018), và T5 (Raffel et al., 2020) đã chứng minh hiệu suất đặc biệt trong vô số nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP), nhờ vào việc tiền huấn luyện rộng rãi trên lượng dữ liệu khổng lồ. Đáng chú ý, mô hình GPT-3 (Brown et al., 2020), với kiến trúc chỉ bộ giải mã, đã thể hiện khả năng zero-shot ấn tượng khi kích thước mô hình và dữ liệu huấn luyện tăng lên. Hơn nữa, lĩnh vực này đã chứng kiến sự xuất hiện của những mô hình quy mô lớn ngày càng tinh vi như OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), và PaLM (Anil et al., 2023), được xây dựng tỉ mỉ để giải quyết các thách thức NLP phức tạp. InstructGPT (Ouyang et al., 2022) kết hợp điều chỉnh chỉ dẫn và học tăng cường, cho phép chúng phù hợp với sở thích của con người và thực thi hiệu quả các chỉ dẫn được chỉ định. Hơn nữa, ChatGPT vượt qua các tiền nhiệm bằng cách tham gia vào tương tác đối thoại và thành công trong việc thực hiện các lệnh người dùng đa dạng. Nó chứng minh tiềm năng giải quyết một loạt rộng các nhiệm vụ và yêu cầu thế giới thực.

### 2.2 Mô hình Thị giác-Ngôn ngữ

Nghiên cứu rộng rãi đã được thực hiện về Mô hình Thị giác-Ngôn ngữ (VLMs) liên phương thức, sử dụng các nhiệm vụ tiền huấn luyện khác nhau như dự đoán mặt nạ (He et al., 2022; Bao et al., 2021), dự đoán token tiếp theo (Chen et al., 2020), và học tương phản (Radford et al., 2021). BLIP đã sử dụng kết hợp mô hình linh hoạt để đạt được nhiều nhiệm vụ, trong khi OFA (Wang et al., 2022) đã tích hợp các nhiệm vụ văn bản và thị giác khác nhau vào một khung thống nhất. PaLI (Chen et al., 2022) đã cung cấp bằng chứng thực nghiệm hỗ trợ hiệu quả của các bộ mã hóa thị giác quy mô lớn hơn trong VLMs. OFA-sys (Bai et al., 2022) cố gắng xây dựng một khung đa nhiệm vụ thống nhất từ góc độ hệ thống. Flamingo (Alayrac et al., 2022) đã tận dụng sự chú ý chéo có cổng để thiết lập kết nối giữa các bộ mã hóa thị giác được tiền huấn luyện và Mô hình Ngôn ngữ Lớn (LLMs) cho khả năng few-shot ấn tượng. Ngoài ra, Kosmos (Huang et al., 2023) đã chứng minh khả năng nhận dạng và suy luận OCR zero-shot đặc biệt bằng cách huấn luyện LLMs sử dụng các đặc trưng thị giác đầu vào. GPT-4 (OpenAI, 2023) gần đây đã giới thiệu đầu vào thị giác, cho phép LVLMs bao gồm phạm vi chức năng rộng hơn. Nhiều phương pháp gần đây (Li et al., 2023a; Zhu et al., 2023; Su et al., 2023; Ye et al., 2023; Gao et al., 2023a) đã cố gắng tích hợp các bộ mã hóa thị giác được tiền huấn luyện với LLMs. BLIP-2 (Li et al., 2023b) đã phát triển mô hình Q-Former để liên kết bộ mã hóa thị giác và LLM, trong khi LLaVA (Liu et al., 2023a) đã xây dựng dữ liệu chỉ dẫn thị giác để điều chỉnh LLMs cho khả năng thị giác. Instructblip (Dai et al., 2023) đã giới thiệu chỉ dẫn văn bản vào Q-Former để nâng cao thêm hiệu suất. mPLUG-Owl (Ye et al., 2023) đã cố gắng huấn luyện một bộ mã hóa thị giác để cải thiện sự liên kết. Kosmos2 (Peng et al., 2023) và Shikra (Chen et al., 2023) khám phá khả năng định vị thị giác của LVLM. Qwen-VL được đề xuất gần đây (Bai et al., 2023) đạt được hiệu suất vượt trội trên một loạt rộng các nhiệm vụ tập trung vào thị giác như mô tả hình ảnh, trả lời câu hỏi thị giác, và các nhiệm vụ thị giác định hướng văn bản. Tuy nhiên, ít nỗ lực được thực hiện để đánh giá cách các LVLM này hoạt động dưới hành vi người dùng thế giới thực. Trong công việc này, chúng tôi cố gắng giải quyết vấn đề này.

### 2.3 Đánh giá Mô hình Thị giác-Ngôn ngữ

Các LVLM ban đầu (Bao et al., 2021; Wang et al., 2022; Chen et al., 2022; He et al., 2022) chủ yếu tập trung vào đánh giá hiệu suất của chúng trên các nhiệm vụ phụ khác nhau (Agrawal et al., 2019; Deng et al., 2009; Lin et al., 2014), thường thông qua điều chỉnh hoặc đánh giá zero-shot trên các nhiệm vụ liên phương thức khác nhau. Tuy nhiên, việc giới thiệu các LVLM đa năng hơn, như GPT-4, đã mở rộng phạm vi khả năng để bao gồm các tương tác dựa trên ngôn ngữ để hiểu đầu vào thị giác và thực thi chỉ dẫn. Những mô hình này (OpenAI, 2023) chứng minh tiềm năng đạt được trí tuệ nhân tạo tổng quát, vượt qua các hạn chế của phương pháp đánh giá thông thường. Các chú thích trong những nhiệm vụ này (Agrawal et al., 2019; Chen et al., 2015a), có xu hướng nhấn mạnh các định dạng và phong cách cụ thể, có thể không nắm bắt đầy đủ sở thích của con người. Ngoài ra, các số liệu đánh giá đa dạng (Chen et al., 2015b; Lin, 2004; Vedantam et al., 2015) và phương pháp được sử dụng trên các nhiệm vụ khác nhau làm cho việc thiết lập một tiêu chuẩn thống nhất và toàn diện trở nên thách thức. Hơn nữa, bất chấp khả năng tổng quát hóa ấn tượng được thể hiện bởi các LVLM hiện tại, chúng dễ bị các vấn đề ảo giác đáng chú ý (Li et al., 2023c), đòi hỏi đánh giá cẩn thận và hạn chế trong khía cạnh này. VisIT-Bench (Bitton et al., 2023) đánh giá khả năng tuân theo chỉ dẫn của LVLMs, phản ánh tốt hơn sở thích của con người so với các nhiệm vụ QA và mô tả truyền thống. Giải quyết những hạn chế này, nghiên cứu của chúng tôi nhằm phát triển một phương pháp đánh giá mới so sánh trực tiếp cuộc đối thoại để đánh giá hiệu suất của các mô hình thị giác-ngôn ngữ. Đồng thời với công việc của chúng tôi, VisIT-Bench (Bitton et al., 2023) cũng kết hợp các chú thích của con người và LLMs tiên tiến như người đánh giá. Tuy nhiên, họ tập trung cụ thể vào khả năng tuân theo chỉ dẫn bằng cách sử dụng mô tả có điều kiện chỉ dẫn. Ngược lại, phương pháp của chúng tôi liên quan đến đánh giá toàn diện bao gồm các khía cạnh khác nhau của hiệu suất mô hình đồng thời cũng xem xét ảo giác mô hình.

## 3 Phương pháp

### 3.1 Thu thập Dữ liệu và Thống kê

Để đánh giá khả năng của LVLMs, chúng tôi xây dựng một bộ dữ liệu đa dạng và toàn diện bao gồm năm khía cạnh chính: khả năng mô tả cơ bản, khả năng nhận dạng thị giác, khả năng hiểu biết thị giác, khả năng kể chuyện thị giác, và khả năng phân tích đa hình ảnh.

**Khả năng Mô tả Cơ bản.** Mô tả hình ảnh liên quan đến khả năng của mô hình mô tả thông tin có trong hình ảnh, bao gồm mô tả đơn giản và chi tiết. Mô tả đơn giản thường là những cụm từ ngắn mô tả chủ thể chính và hành động của hình ảnh, trong khi mô tả chi tiết cung cấp thông tin sâu hơn về cảnh hình ảnh, các thuộc tính và mối quan hệ của chúng.

**Khả năng Nhận dạng Thị giác.** Nhận dạng hình ảnh là nhiệm vụ nhận dạng các đối tượng hoặc cảnh trong hình ảnh và suy ra thông tin liên quan. Lĩnh vực này có thể được chia thành nhiều nhiệm vụ phụ, bao gồm QA thuộc tính, nhận dạng phim/TV, nhận dạng nghệ thuật, nhận dạng địa danh, nhận dạng người nổi tiếng, nhận dạng cảm xúc, nhận dạng văn bản, nhận dạng đối tượng, và nhận dạng nội dung cấu trúc. Những nhiệm vụ phụ này yêu cầu các kỹ thuật và phương pháp khác nhau, chẳng hạn như xác định số lượng, kích thước, màu sắc, chiều cao, và các thuộc tính khác của đối tượng trong hình ảnh, nhận dạng các địa danh nổi tiếng, núi non, và sông ngòi, hoặc hiểu cảm xúc của con người trong hình ảnh.

**Khả năng Hiểu biết Thị giác.** Hiểu biết hình ảnh liên quan đến khả năng của mô hình hiểu ý nghĩa của hình ảnh và các nhiệm vụ liên quan. Lĩnh vực này bao gồm nhiều nhiệm vụ phụ, như đánh giá phong cách, hiểu biết hình ảnh trừu tượng, hiểu biết meme, phân tích hình ảnh, phân tích biểu đồ, giải quyết vấn đề tổng quát, và QA lập luận. Những nhiệm vụ này yêu cầu mô hình phân tích nội dung của biểu đồ, PPT, hoặc sơ đồ luồng phức tạp, hiểu phép ẩn dụ và sự tương tự trong hình ảnh, hoặc phân tích nội dung của hướng dẫn sử dụng, bản đồ, và bài toán toán học.

**Khả năng Kể chuyện Thị giác.** Khả năng kể chuyện thị giác là quá trình sáng tạo văn học dựa trên nội dung thị giác, bao gồm viết email, thơ, truyện, quảng cáo/khuyến nghị hàng hóa, và động não. Những nhiệm vụ này yêu cầu mô hình tạo ra nội dung sáng tạo và độc đáo dựa trên hình ảnh.

**Khả năng Phân tích Đa Hình ảnh.** Phân tích đa hình ảnh là nhiệm vụ phân tích và so sánh nhiều hình ảnh. Lĩnh vực này bao gồm các nhiệm vụ như so sánh hai/nhiều hình ảnh, tóm tắt thông tin nhiều hình ảnh, so sánh hàng hóa, và phân tích từng bước của hình ảnh. Những nhiệm vụ này yêu cầu mô hình phân tích nội dung của nhiều hình ảnh và tóm tắt thông tin.

Nhìn chung, năm danh mục câu hỏi đánh giá toàn diện khả năng của mô hình. Như được thể hiện trong Hình 1, các ví dụ của 27 nhiệm vụ phụ được trình bày. Từ nhận thức đến nhận thức, và sau đó đến sáng tạo, khi độ khó tăng lên, nhu cầu đối với mô hình cũng trở nên cao hơn. Hiện tại, khả năng của LVLMs vẫn đang trong giai đoạn đầu. Bộ dữ liệu của chúng tôi hiện tại chú trọng hơn vào đánh giá khả năng cơ bản, trong đó tỷ lệ câu hỏi cao nhất liên quan đến nhận dạng, chiếm khoảng 44,1%, tiếp theo là câu hỏi hiểu biết ở 29,6%. Tỷ lệ của các danh mục khác là 15,3% cho khả năng mô tả cơ bản, 7,4% cho khả năng kể chuyện thị giác, và 3,6% cho khả năng phân tích đa hình ảnh. Có tổng cộng 908 câu hỏi.

### 3.2 Đánh giá

Đánh giá tự động và chính xác các LVLM trong bối cảnh đối thoại đa phương thức thế giới mở đặt ra một thách thức đáng kể. Tham khảo công việc Chiang et al. (2023); Zheng et al. (2023), chúng tôi áp dụng một LLM mạnh mẽ làm giám khảo để cho phép đánh giá tự động. Để hiểu hiệu quả nội dung của hình ảnh, chúng tôi thay thế thủ công đầu vào hình ảnh thực tế bằng các chú thích văn bản chi tiết. Bằng cách nhập những chú thích này và các câu hỏi tương ứng vào một LLM mạnh mẽ như GPT-4, chúng tôi thu được các câu trả lời tham khảo.

Để đánh giá LVLMs, chúng tôi cung cấp hình ảnh thực tế và câu hỏi làm đầu vào và thu được các câu trả lời tương ứng của chúng. Cuối cùng, chúng tôi sử dụng GPT-4 để chấm điểm các câu trả lời được tạo bởi LVLMs dựa trên các chú thích chi tiết và câu hỏi. Chỉ dẫn chấm điểm yêu cầu mô hình đánh giá tính hữu ích, liên quan và chính xác của các câu trả lời, xem xét các chú thích như nội dung của hình ảnh. Để đảm bảo công bằng trong đánh giá, câu trả lời của mỗi mô hình được so sánh với một câu trả lời tham khảo nhất quán từ GPT-4. Điểm trung bình của mô hình trong tất cả các câu hỏi được lấy làm điểm cuối cùng.

Để loại bỏ ảnh hưởng của vị trí câu trả lời, chúng tôi thực hiện vòng chấm điểm thứ hai bằng cách đổi chỗ các câu trả lời và sau đó tính trung bình của hai điểm thu được. Phương pháp này nhằm giảm thiểu bất kỳ thiên vị nào được gây ra bởi vị trí của các câu trả lời.

Ngoài ra, trong phần thử nghiệm, chúng tôi so sánh tính nhất quán của các kết quả thu được thông qua phương pháp được đề xuất với các kết quả được gán bởi người đánh giá. Sự so sánh này chứng minh tính khả thi của việc sử dụng các chú thích chi tiết của con người để đại diện cho nội dung của các phương thức khác. Nó cho phép LLM phục vụ như một giám khảo để đánh giá nội dung đa phương thức. Việc đánh giá LVLMs trong các cuộc đối thoại đa phương thức thế giới mở vẫn là một nhiệm vụ thách thức không có giải pháp xác định. Tuy nhiên, việc giới thiệu một LLM mạnh mẽ làm giám khảo, kết hợp với việc thay thế hình ảnh bằng các chú thích chi tiết, cho phép đánh giá hiệu quả hơn.

## 4 Kết quả và Phân tích

Trong phần này, chúng tôi trình bày thiết lập thử nghiệm được sử dụng để đánh giá hiệu suất của các LVLM. Chúng tôi xác thực hiệu quả của phương pháp đánh giá thông qua đánh giá tính nhất quán của con người. Hơn nữa, chúng tôi so sánh hiệu suất trên các nhiệm vụ khác nhau và cũng tiến hành phân tích vấn đề ảo giác mô hình. Ngoài ra, chúng tôi thảo luận về các hạn chế của phương pháp và các lĩnh vực tiềm năng để cải thiện.

### 4.1 Đánh giá tính nhất quán

Để đánh giá tính nhất quán giữa đánh giá mô hình và phán đoán của con người đối với GPT-4, chúng tôi so sánh kết quả của cả hai phương pháp. Chúng tôi lấy mẫu 200 câu hỏi dựa trên phân phối của chúng và chọn ba mô hình - InstructBLIP (Dai et al., 2023), LLaVA (Liu et al., 2023a), và Qwen-VL - với hiệu suất khác nhau trong đánh giá. Tổng cộng 600 câu hỏi và câu trả lời được đánh giá, với ba cá nhân cung cấp xếp hạng của họ dẫn đến 1,8k phiếu bầu. Phiếu bầu đa số của ba cá nhân được sử dụng làm kết quả cơ sở thực tế, và một cá nhân thứ tư được giới thiệu trong trường hợp có sự bất đồng. Sau đó chúng tôi tính toán tính nhất quán giữa kết quả dự đoán của mô hình và kết quả dự đoán của con người. Tính nhất quán được đo bằng tỷ lệ điểm số nhất quán so với tổng số điểm số. Điểm số nhất quán mô hình-con người là 72,2%, trong khi điểm số do con người tạo ra thể hiện 78,4% tính nhất quán, cho thấy tính nhất quán giữa phiếu bầu của mô hình và phiếu bầu của con người có sự khác biệt 6,2%, tương đối gần.

Như được thể hiện trong Hình 4, tính nhất quán khác nhau giữa các khả năng khác nhau, với tính nhất quán cao hơn được quan sát trong nhận dạng cơ bản. Khi độ khó của nhiệm vụ tăng lên, tính nhất quán của con người giảm dần. So sánh các mô hình khác nhau, chúng tôi thấy rằng các mô hình với điểm số thấp hơn có tính nhất quán cao hơn, trong khi các mô hình với điểm số cao hơn có tính nhất quán thấp hơn. Điều này cho thấy khi khả năng của mô hình cải thiện, cần một mô hình chấm điểm mạnh mẽ hơn để đánh giá.

### 4.2 So sánh Hiệu suất

Quan sát hiệu suất của các mô hình khác nhau trong Hình 5 và 6, hiện tại, các mô hình có sự khác biệt rõ ràng trong hiệu suất sáng tạo văn học, và vẫn còn chỗ để cải thiện trong nhận dạng, mô tả, và phân tích hiểu biết.

**Khả năng kể chuyện thị giác.** Có sự khác biệt đáng chú ý giữa các mô hình khác nhau, đặc biệt là MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), và PandaGPT (Su et al., 2023), hoạt động hơi kém trong khía cạnh này. Khi đối mặt với các chỉ dẫn như viết thơ hoặc truyện, những mô hình này có xu hướng cung cấp mô tả đơn giản thay vì sáng tạo văn học. Nhìn chung, các mô hình như LLaVA (Liu et al., 2023a) và mPLUG-Owl (Ye et al., 2023) xuất sắc trong khía cạnh này thường trải qua giai đoạn SFT (Supervised Fine Tuning), trong đó LLM được sử dụng để tham gia vào huấn luyện. Mặt khác, các mô hình khác được huấn luyện thông qua các phương pháp như huấn luyện tham số thấp, như LoRA (Hu et al., 2021) và Bias tuning (Gao et al., 2023a), hoặc bằng cách khóa tham số LLM. Điều này gợi ý rằng huấn luyện LLM để học nội dung thị giác có thể hữu ích hơn cho một số nhiệm vụ yêu cầu kết hợp nội dung mô hình và khả năng sáng tạo văn học.

**Khả năng nhận dạng thị giác.** Đối với các mô hình đóng băng bộ mã hóa thị giác trong quá trình tiền huấn luyện, khả năng nhận dạng không cho thấy mối tương quan mạnh với lượng dữ liệu tiền huấn luyện. Điều này gợi ý rằng việc liên kết bộ mã hóa thị giác được tiền huấn luyện với LLM không được hưởng lợi đáng kể từ tập dữ liệu lớn hơn. Tuy nhiên, các mô hình như mPLUG-Owl và Qwen-VL giải phóng bộ mã hóa thị giác có hiệu suất tốt hơn và được huấn luyện với tập dữ liệu lớn hơn. Sự khác biệt giữa các mô hình trong nhận dạng thuộc tính và nhận dạng cảm xúc tương đối nhỏ, nhưng đối với các nhiệm vụ nhận dạng tổng quát như người nổi tiếng, loài, và tác phẩm điện ảnh truyền hình, có nhiều sự khác biệt giữa các mô hình, mặc dù độ chính xác và độ tin cậy vẫn còn xa lý tưởng. Điều này có thể liên quan đến kho dữ liệu tiền huấn luyện. Hiện tại, hầu hết các mô hình có một số khả năng nhận dạng văn bản, nhưng độ chính xác vẫn tương đối thấp, đặc biệt là đối với các ký tự nhỏ, số, và chữ viết tay. Qwen-VL có lợi thế rõ ràng trong nhận dạng văn bản, và được gợi ý rằng việc huấn luyện mô hình chỉ thông qua việc liên kết hình ảnh và văn bản không thể cho phép nó thành thạo khả năng nhận dạng văn bản được đóng gói dày đặc.

**Khả năng hiểu biết thị giác.** Đặc biệt, một sự khác biệt đáng kể giữa các mô hình được quan sát trong các nhiệm vụ giải bài toán dựa trên hình ảnh và phân tích biểu đồ. Ngay cả khi mô tả câu hỏi toán được cung cấp bằng ngôn ngữ tự nhiên cho Mô hình Mô hình Ngôn ngữ tương ứng (LLM), khoảng cách hiệu suất tương tự vẫn tồn tại, cho thấy thiếu sót trong khả năng giải quyết vấn đề toán học hiệu quả của LLM. Hơn nữa, các mô hình thường gặp khó khăn với việc xác định chính xác và thiết lập mối quan hệ không chính xác trong biểu đồ, cản trở khả năng nhận dạng và diễn giải các phần tử biểu đồ một cách chính xác, dẫn đến câu trả lời sai. Qwen-VL thể hiện lợi thế rõ ràng trong phân tích biểu đồ, vì nó được hưởng lợi từ đầu vào độ phân giải cao hơn và các giai đoạn học đa nhiệm vụ bổ sung bao gồm nhiệm vụ nhận dạng văn bản dày đặc.

**Phân tích đa hình ảnh.** Để phù hợp với đầu vào từ các mô hình khác nhau, nhiều hình ảnh được nối lại thành một hình ảnh và nhập vào mô hình. Các mô hình có khả năng yếu trong việc phán đoán sự khác biệt hình ảnh và tóm tắt nội dung liên tục. Một mặt, đa hình ảnh ảnh hưởng đến độ chính xác nhận dạng, và mặt khác, có những thiếu sót trong việc hiểu mối quan hệ giữa nhiều nội dung, đặc biệt là trong trường hợp PandaGPT (Su et al., 2023), nơi khả năng nhận dạng giảm đáng kể khi nhiều hình ảnh được nhập vào.

**Mô tả cơ bản.** Các sai sót trong các thuộc tính của nội dung trong mô tả là yếu tố đóng góp. Hơn nữa, các mô hình hiện tại thể hiện các trường hợp ảo giác đáng kể, dẫn đến điểm số kém tổng thể trong đánh giá quan trọng nhất về khả năng mô tả. Chúng tôi sẽ cung cấp so sánh chi tiết về xu hướng ảo giác của các mô hình trong phần 4.2.

### 4.3 Phân tích Ảo giác Mô hình

Hầu hết các LVLM hiện tại thể hiện các vấn đề ảo giác, như dự đoán các đối tượng hoặc nội dung không tồn tại trong tín hiệu thị giác đầu vào. Như được minh họa trong Hình 2, thông qua phân tích so sánh với GPT-4, chúng tôi khám phá rằng GPT-4 có thể phát hiện ảo giác trong mô hình và phạt sự xuất hiện của những vấn đề này. Để so sánh ảo giác của các LVLM khác nhau, chúng tôi sử dụng các gợi ý khác nhau để yêu cầu mô hình mô tả hình ảnh. Sau đó chúng tôi nhập mô tả mô hình và chú thích chi tiết của con người vào GPT-4 để đánh giá mức độ ảo giác của mô hình.

Như được minh họa trong Bảng 2, các LVLM hiện tại thể hiện mức độ ảo giác cao trong nhiệm vụ mô tả. Trong số đó, PandaGPT (Su et al., 2023) có mức độ ảo giác cao nhất, có thể do đầu vào thị giác không đủ được cung cấp bởi ImageBind (Girdhar et al., 2023), chỉ nhập embedding cls vào LLM. Ngược lại, InstructBLIP (Dai et al., 2023) và Qwen-VL (Bai et al., 2023) đạt được điểm ảo giác thấp nhất bằng cách ưu tiên câu trả lời ngắn hơn, giảm cơ hội ảo giác. Cung cấp cho mô hình các gợi ý ngắn gọn hơn có thể là một chiến lược để ngăn ngừa ảo giác.

### 4.4 Hạn chế và Lĩnh vực Tiềm năng Cải thiện

Vẫn còn rất nhiều chỗ để cải thiện trong các LVLM dựa trên đánh giá và so sánh. Trong phần này, chúng tôi đề xuất một số hướng tiềm năng để nâng cao trong bối cảnh các hạn chế hiện tại.

**Hiểu biết không gian.** Những mô hình này hoạt động kém trong việc hiểu mối quan hệ vị trí và cấu trúc phức tạp. Một lý do là bản thân LLMs không học trực tiếp các khái niệm không gian, và việc biểu diễn và mô tả các mối quan hệ phức tạp trong dữ liệu cũng bị hạn chế. Một số phương pháp (Peng et al., 2023; Chen et al., 2023; Bai et al., 2023) đã được thử nghiệm để kết hợp các nhiệm vụ định vị nhất định vào LVLM, cho phép mô hình có được khả năng định vị bổ sung. Thêm nhiều dữ liệu chứa thông tin vị trí, như phát hiện, phân đoạn, và đồ thị cảnh, có thể giúp mô hình thiết lập một số khái niệm mối quan hệ không gian. Sự hiểu biết rộng hơn về mối quan hệ không gian này có thể góp phần cải thiện hiệu suất trong các nhiệm vụ như hiểu biết bố cục và quy hoạch không gian.

**Tiền huấn luyện đa hình ảnh.** Trong khi tiền huấn luyện hình ảnh đơn hiệu quả cho việc nhận dạng LLM, nó có tiện ích hạn chế trong việc so sánh và tóm tắt nhiều hình ảnh. Vì lý do này, cần thiết phải giới thiệu thêm dữ liệu hình ảnh-văn bản xen kẽ để học, như trang web, bài báo, và tin tức.

**Nâng cao LLM thông qua Nội dung Đa phương thức.** Trong khi việc liên kết các bộ mã hóa thị giác với LLMs nhanh chóng xây dựng LVLMs, khả năng của các mô hình cũng bị hạn chế trong một số nhiệm vụ, như hiểu biết không gian, nhận dạng văn bản dày đặc, và khả năng toán học. Khám phá thêm cách cải thiện khả năng của LLM thông qua nội dung đa phương thức là đáng giá.

**Vấn đề ảo giác.** Giải quyết vấn đề ảo giác thị giác, nơi các mô hình tạo ra nội dung không tồn tại trong hình ảnh đầu vào, là một khía cạnh quan trọng cần xem xét. Đầu vào thị giác không đủ có thể dễ dàng dẫn đến ảo giác. Một mặt, khám phá các kỹ thuật để tăng cường phán đoán của mô hình về nội dung không tồn tại là có thể. Mặt khác, tập trung nhiều hơn vào câu trả lời của mô hình đối với nội dung thị giác và tăng cường tính nhất quán giữa câu trả lời và nội dung thị giác có thể giúp giảm ảo giác thị giác.

**Độ phân giải cao hơn.** Hầu hết các LVLM nhập hình ảnh với độ phân giải 224×224, nhưng việc tăng độ phân giải của hình ảnh đầu vào có thể cải thiện khả năng nhận dạng các đối tượng nhỏ, văn bản dày đặc, và chi tiết tinh tế của các mô hình, dẫn đến đầu ra chính xác hơn.

## 5 Kết luận

Tóm lại, chúng tôi đề xuất một phương pháp đánh giá cho các mô hình thị giác-ngôn ngữ lớn (LVLMs) sử dụng các LLM mạnh làm giám khảo để đánh giá toàn diện các khả năng khác nhau của chúng. Bộ dữ liệu TouchStone của chúng tôi bao gồm năm danh mục khả năng chính và 27 nhiệm vụ phụ, không chỉ bao gồm nhận dạng và hiểu biết cơ bản mà còn mở rộng đến sáng tạo văn học. Nó tích hợp các chú thích và mô tả hình ảnh chi tiết để biến đổi nội dung đầu vào đa phương thức thành dạng có thể hiểu được bởi các mô hình ngôn ngữ. Thông qua xác thực, chúng tôi chứng minh rằng các LVLM mạnh mẽ, như GPT-4, có thể chấm điểm hiệu quả chất lượng đối thoại bằng cách tận dụng khả năng văn bản của chúng, phù hợp với sở thích của con người. Kết quả của chúng tôi cho thấy rằng vẫn còn nhiều chỗ để cải thiện trong các LVLM hiện tại, và xác định các lĩnh vực tiềm năng để phát triển thêm. Phương pháp của chúng tôi cung cấp một công cụ có giá trị để đánh giá LVLMs và nâng cao khả năng của chúng, cuối cùng thúc đẩy sự phát triển của các mô hình thị giác-ngôn ngữ hiệu quả và toàn diện hơn.
