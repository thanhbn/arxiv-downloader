# 2310.10616.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.10616.pdf
# File size: 6175264 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
How Do Transformers Learn In-Context Beyond Simple
Functions? A Case Study on Learning with Representations
Tianyu Guo∗ ∗Wei Hu† †Song Mei∗Huan Wang‡ ‡Caiming Xiong‡
Silvio Savarese‡Yu Bai‡
October 17, 2023
Abstract
While large language models based on the transformer architecture have demonstrated remarkable
in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where
existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple
function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by
studying learning with representations . Concretely, we construct synthetic in-context learning problems
withacompositionalstructure, wherethelabeldependsontheinputthroughapossiblycomplexbut fixed
representation function, composed with a linear function that differsin each instance. By construction,
the optimal ICL algorithm first transforms the inputs by the representation function, and then performs
linear ICL on top of the transformed dataset. We show theoretically the existence of transformers
that approximately implement such algorithms with mild depth and size. Empirically, we find trained
transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired
dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through
extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained
transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL
capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder
mixture setting. These observed mechanisms align well with our theory and may shed light on how
transformers perform ICL in more realistic scenarios.
1 Introduction
Large language models based on the transformer architecture have demonstrated remarkable in-context
learning(ICL)capabilities(Brownetal.,2020), wheretheycansolvenewlyencounteredtaskswhenprompted
with only a few training examples, without any parameter update to the model. Recent state-of-the-art
models further achieve impressive performance in context on sophisticated real-world tasks (OpenAI, 2023;
Bubeck et al., 2023; Touvron et al., 2023). Such remarkable capabilities call for better understandings, which
recent work tackles from various angles (Xie et al., 2021; Chan et al., 2022; Razeghi et al., 2022; Min et al.,
2022; Olsson et al., 2022; Wei et al., 2023).
A recent surge of work investigates ICL in a theoretically amenable setting where the context consists of
real-valued (input, label) pairs generated from a certain function class. They find that transformers can
learn many function classes in context, such as linear functions, shallow neural networks, and decision
trees (Garg et al., 2022; Akyürek et al., 2022; Li et al., 2023a), and further studies provide theoretical
justification on how transformers can implement and learn various learning algorithms in-context such as
ridge regression (Akyürek et al., 2022), gradient descent (von Oswald et al., 2022; Dai et al., 2022; Zhang
et al., 2023a; Ahn et al., 2023), algorithm selection (Bai et al., 2023), and Bayes model averaging (Zhang
et al., 2023b), to name a few. Despite the progress, an insufficiency of this line is that the settings and
∗UC Berkeley. Email: {tianyu_guo,songmei}@berkeley.edu
†University of Michigan. Email: vvh@umich.edu
‡Salesforce AI Research. Email: {huan.wang, cxiong, ssavarese, yu.bai}@salesforce.com
1arXiv:2310.10616v1  [cs.LG]  16 Oct 2023

--- PAGE 2 ---
(a) Illustration of our setting and theory
x(j)1y(j)1x(j)i−1x(j)iy(j)i−1?⋯⋯ICL problemOur settingw(j)Φ⋆fixedvaryingCompute Φ⋆(x(j)i)Linear ICL on {Φ⋆(x(j)i),yi}Transformer Mechanismx(j)iΦ⋆(x(j)i)y(j)i (b) ICL risks
0 10 20 30 40
in-context examples0.00.20.40.60.81.0square loss
Transformer
Optimal Φ⋆-ridge
Φ⋆-OLS (c) Linear probes
0 2 4 6 810 12
layer0.00.20.40.6mean squared error
Φ⋆(xi) at x tokens
Φ⋆(xi) at y tokens
̂yΦ⋆ridge
i at x tokens
Figure 1: An illustration of our setting and results. (a)We consider ICL problems with a fixed representation
composed with changing linear functions, and we construct transformers that first compute the representations and
then performs linear ICL. (b,c)Empirically, learned transformers can perform near-optimal ICL in this setting, and
exhibit mechanisms that align with our theory (detailed setups in Section 5.1).
results may not actually resemble ICL in real-world scenarios—For example, ICL in linear function classes
are well understood in theory with efficient transformer constructions (Bai et al., 2023), and transformers
indeed learn them well empirically (Garg et al., 2022); however, such linear functions in the raw input may
fail to capture real-world scenarios where prior knowledge can often aid learning.
This paper takes initial steps towards addressing this by studying ICL in the setting of learning with repre-
sentations , a more complex and perhaps more realistic setting than existing ones. We construct synthetic
ICL tasks where labels depend on inputs through a fixed representation function composed with a varying
linear function. We instantiate the representation as shallow neural networks (MLPs), and consider both a
supervised learning setting (with input-label pairs) and a dynamical systems setting (with inputs only) for
the in-context data. Our contributions can be summarized as follows.
•Theoretically,weconstructtransformersthatimplementin-contextridgeregressionontherepresentations
(which includes the Bayes-optimal algorithm) for both learning settings (Section 4). Our transformer
constructionsadmitmildsizes, andcanpredictateverytokenusingadecoderarchitecture, (non-trivially)
generalizing existing efficient constructions that predict at the last token only using an encoder architec-
ture.
•Empirically, we find that trained small transformers consistently achieve near-optimal ICL risk in both
learning settings (Section 5 & Figure 1b).
•Using linear probing techniques, we identify evidence for various mechanisms in the trained transformers.
Our high-level finding is that the lower layers transforms the data by the representation and prepares it
into a certain format, and the upper layers perform linear ICL on top of the transformed data (Figure
1c), with often a clear dissection between these two modules, consistent with our theory. See Figure 1a
for a pictorial illustration.
•We further observe several lower-level behaviors using linear probes that align well with our (and exist-
ing) theoretical constructions, such as copying (of both the input and the representations) where which
tokens are being copied are precisely identifiable (Section 5.2), and a post-ICL representation selection
mechanism in a harder setting (Section 5.1.1 & Appendix E).
•We perform a new pasting experiment and find that the upper layers within the trained transformer can
perform nearly-optimal linear ICL in (nearly-)isolation (Section 5.1), which provides stronger evidence
that the upper module alone can be a strong linear ICL learner.
2 Related work
In-context learning The in-context learning (ICL) capabilities of pretrained transformers have gained
significant attention since first demonstrated with GPT-3 (Brown et al., 2020). Subsequent empirical studies
have investigated the capabilities and limitations of ICL in large language models (Liu et al., 2021; Min
2

--- PAGE 3 ---
et al., 2021a,b; Lu et al., 2021; Zhao et al., 2021; Rubin et al., 2021; Razeghi et al., 2022; Elhage et al., 2021;
Kirsch et al., 2022; Wei et al., 2023).
A line of recent work investigates why and how pretrained transformers perform ICL from a theoretical
perspective (Garg et al., 2022; Li et al., 2023a; von Oswald et al., 2022; Akyürek et al., 2022; Xie et al., 2021;
Bai et al., 2023; Zhang et al., 2023a,b; Ahn et al., 2023; Raventós et al., 2023). In particular, Xie et al. (2021)
proposed a Bayesian inference framework explaining ICL. Garg et al. (2022) showed transformers could be
trained from scratch for ICL of simple function classes. Other studies found transformers can implement ICL
through in-context gradient descent (von Oswald et al., 2022; Akyürek et al., 2022) and in-context algorithm
selection (Bai et al., 2023). Zhang et al. (2023a) studied the training dynamics of a single attention layer on
linear ICL tasks. Li et al. (2023b) used the ICL framework to explain chain-of-thought reasoning (Wei et al.,
2022). Our work builds on and extends the work of (Garg et al., 2022; Akyürek et al., 2022; von Oswald et al.,
2022; Bai et al., 2023), where we study the more challenging setting of ICL with a representation function,
and also provide new efficient ICL constructions for predicting at every token using a decoder transformer,
as opposed to predicting only at the last token in most of these work.
In-weights learning versus in-context learning Recent work has investigated when transformers learn
a fixed input-label mapping versus when they perform ICL (Chan et al., 2022; Wei et al., 2023; Bietti et al.,
2023). Chan et al. (2022) refer to learning a fixed input-label mapping from the pre-training data as “in-
weights learning” (IWL), in contrast with ICL. Our problem setting assumes the pre-training data admits
a fixed representation function, which should be learned by IWL. In this perspective, unlike these existing
works where IWL and ICL are typically treated as competing mechanisms, we study a model in which IWL
(computing the fixed representation by transformer weights) and ICL (learning the changing linear function
in context) occur simultaneously.
Mechanistic understanding and probing techniques A line of work focuses on developing techniques
for understanding the mechanisms of neural networks, in particular transformers (Alain and Bengio, 2016;
Geiger et al., 2021; Meng et al., 2022; von Oswald et al., 2022; Akyürek et al., 2022; Wang et al., 2022;
Räuker et al., 2023). We adopted the linear probing technique of (Alain and Bengio, 2016) in a token-wise
fashion for interpreting the ICL mechanisms of transformers. Beyond probing, more convincing mechanistic
interpretations may require advanced approaches such as causal intervention (Geiger et al., 2021; Vig et al.,
2020; Wang et al., 2022); Our pasting experiment has a similar interventional flavor in that we feed input
sequences (ICL instances) from another distribution directly (through a trainable embedding layer) to the
upper module of a transformer.
3 Preliminaries
Transformers We consider sequence-to-sequence functions applied to Ninput vectors {hi}N
i=1⊂RDhidin
Dhiddimensions, which we write compactly as an input matrix H= [h1, . . . ,hN]∈RDhid×N, where each hi
is a column of H(also atoken).
We use a standard L-layer decoder-only (autoregressive) transformer, which consists of Lconsecutive blocks
each with a masked self-attention layer (henceforth “attention layer”) followed by an MLP layer. Each
attention layer computes
Attn θ(H):=H+PM
m=1(VmH)×σ 
MSK⊙((QmH)⊤(KmH))
∈RD×N,
where θ={(Qm,Km,Vm)⊂RDhid×Dhid}m∈[M]are the (query, key, value) matrices, Mis the number of
heads, MSK∈RN×Nis the decoder mask matrix with MSK ij= 1{i≤j}, and σis the activation function
which is typically chosen as the (column-wise) softmax: [σ(A)]:,j=softmax (aj)∈RNforA= [a1, . . . ,aN]∈
RN×N. Each MLP layer computes
MLP W1,W2(H):=H+W2σ(W1H),
where W{1,2}∈RDhid×Dhidare the weight matrices, and σ(t) = max {t,0}is the ReLU activation. We use
TFto denote a transformer, and typically use eH= TF( H)to denote its output on H.
3

--- PAGE 4 ---
In-context learning We consider in-context learning (ICL) on regression problems, where each ICL in-
stance is specified by a dataset D={(xi, yi)}i∈[N]iid∼P, with (xi, yi)∈Rd×R, and the model is required
to accurately predict yigiven all past observations Di−1:={(xj, yj)}j≤i−1and the test input xi. The
main difficulty of ICL compared with standard supervised learning is that each instance D(j)is in general
drawn from a different data distribution P=P(j)(for example, a linear model with a new w(j)
⋆∈Rd).
Accurate prediction requires learning Pin-context from the past observations Di−1(i.e. the context); merely
memorizing any fixed P(j)is not enough.
We consider using transformers to do ICL, where we feed a sequence of length 2Ninto the transformer TF
using the following input format:
H= [h1, . . . ,h2N] =
x10. . .xN0
0y1. . . 0yN
px
1py
1. . .px
Npy
N
∈RDhid×2N, (1)
where px
i,py
i∈RDhid−d−1are fixed positional encoding vectors consisting of zero paddings , followed by non-
zero entries containing information about the position index iand indicator of being an x-token (1 in px
i,
and0inpy
i); see (12) for our concrete choice. We refer to each odd token h2i−1as as an x-token (also the
xi-token), and each even token h2ias ay-token (also the yi-token).
After obtaining the transformer output eH= TF( H), for every index i∈[N], we extract the prediction byi
from the output token at position xi:byi:= (ehx
i)d+1.1Feeding input (1) into the transformer simultaneously
computes byi←TF(x1, y1, . . . ,xi−1, yi−1,xi)for all i∈[N].
In addition to the above setting, we also consider a dynamical system setting with D={xi}i∈[N]where
the transformer predicts bxifrom the preceding inputs x≤i−1. See Section 4.2 for details.
4 In-context learning with representations
4.1 Supervised learning with representation
We begin by considering ICL on regression problems with representation, where labels depend on the input
through linear functions of a fixed representation function. Formally, let Φ⋆:Rd→RDbe a fixed repre-
sentation function. We generate each in-context data distribution P=Pwby sampling a linear function
w∼N(0, τ2ID)from a Gaussian prior, and then generate the ICL instance D={(xi, yi)}i∈[N]∼Pwby a
linear model on Φ⋆with coefficient wand noise level σ >0:
yi=⟨w,Φ⋆(xi)⟩+σzi,xiiid∼Px, z iiid∼N(0,1), i∈[N]. (2)
Note that all D’s share the same representation Φ⋆, but each admits a unique linear function w.
The representation function Φ⋆can in principle be chosen arbitrarily. As a canonical and flexible choice for
both our theory and experiments, we choose Φ⋆to be a standard L-layer MLP:
Φ⋆(x) =σ⋆ 
B⋆
Lσ⋆ 
B⋆
L−1···σ⋆(B⋆
1x)···
,B⋆
1∈RD×d,(B⋆
ℓ)L
ℓ=2⊂RD×D(3)
where Dis the hidden and output dimension, and σ⋆is the activation function (applied entry-wise) which
we choose to be the leaky ReLU σ⋆(t) =σρ(t):= max {t, ρt}with slope ρ∈(0,1).
Theory AsΦ⋆is fixed and the wis changing in model (2), by construction, a good ICL algorithm should
compute the representations {Φ⋆(xi)}iand perform linear ICL on the transformed dataset {(Φ⋆(xi), yi)}ito
learn w. We consider the following class of Φ⋆-ridgeestimators:
bwΦ⋆,λ
i:= arg minw∈Rd1
2(i−1)Pi−1
j=1(⟨w,Φ⋆(xj)⟩ −yj)2+λ
2∥w∥2
2, (Φ⋆-Ridge)
1There is no information leakage, as the “prefix” property of decoder transformers ehx
i=eh2i−1= [TF( H:,1:(2i−1))]2i−1
ensures that ehx
i(and thus byi) only depends on (Di−1,xi).
4

--- PAGE 5 ---
and we understand bwΦ⋆,λ
1:=0. In words, bwΦ⋆,λ
iperforms ridge regression on the transformed dataset
{Φ(xj), yj}j≤i−1forall i∈[N]. Bystandardcalculations,theBayes-optimalpredictor2foryigiven (Di−1,xi)
is exactly the ridge predictor byΦ⋆,λ
i:=⟨bwΦ⋆,λ
i,Φ⋆(xi)⟩atλ=σ2/τ2.
We show that there exists a transformer that can approximately implement ( Φ⋆-Ridge) in-context at every
token i∈[N]. The proof can be found in Appendix B.
Theorem 1 (Transformer can implement Φ⋆-Ridge).For any representation function Φ⋆of form (3), any
λ > 0,BΦ, Bw, By>0,ε < B ΦBw/2, letting κ:= 1 + B2
Φ/λ, there exists a transformer TFwith L+
O(κlog(BΦBw/ε))layers, 5heads, Dhid= 2D+d+ 10such that the following holds.
For any dataset Dsuch that ∥Φ⋆(xi)∥2≤BΦ,|yi| ≤Byand the corresponding input H∈RDhid×2Nof
format (1), we have
(a) The first (L+ 2)layers of TFtransforms xito the representation Φ⋆(xi)at each xtoken, and copies
them into the succeeding ytoken:
TF(1:L+2)(H) =
Φ⋆(x1) Φ⋆(x1). . . Φ⋆(xN) Φ⋆(xN)
0 y1 . . . 0 yN
epx
1 epy
1 . . . epx
N epy
N
, (4)
whereepx
i,epy
ionly differ from px
i,py
iin the dimension of the zero paddings.
(b) For every index i∈[N], the transformer output eH= TF( H)contains prediction byi:= [eh2i−1]D+1that
is close to the ( Φ⋆-Ridge) predictor: |byi− ⟨Φ⋆(xi),bwΦ⋆,λ
i⟩| ≤ε.
The transformer construction in Theorem 1 consists of two “modules”: The lower layers computes the rep-
resentations and prepares the transformed dataset {(Φ⋆(xi), yi)}iinto form (4). In particular, each Φ⋆(xi)
appears both in the i-thx-token and is also copied into the succeeding ytoken. The upper layers perform
linear ICL (ridge regression) on top of the transformed dataset. We will test whether such mechanisms align
with trained transformers in reality in our experiments (Section 5.1).
Proof techniques The proof of Theorem 1 builds upon (1) implementing the MLP Φ⋆by transformers
(Lemma B.3), and (2) an efficient construction of in-context ridge regression (Theorem B.5), which to our
knowledge is the first efficient construction for predicting at every token using decoder transformers. The
latter requires several new construction techniques such as a copying layer (Lemma B.1), and an efficient
implementation of Nparallel in-context gradient descent algorithms at all tokens simultaneously using a
decoder transformer (Proposition B.4). These extend the related constructions of von Oswald et al. (2022);
Bai et al. (2023) who only consider predicting at the last token using encoder transformer, and could be of
independent interest.
Inaddition, theboundsonthenumberoflayers, heads, and DhidinTheorem1canimplyasamplecomplexity
guarantee for (pre-)training: A transformer with eε-excess risk (on the same ICL instance distribution) over
the one constructed in Theorem 1 can be found in eO 
(L+κ)2(D+d)2eε−2
training instances, by the
generalization analysis of (Bai et al., 2023, Theorem 20). We remark that the constructions in Theorem 1
& 2 choose σas the normalized ReLU instead of softmax, following (Bai et al., 2023) and in resonance with
recent empirical studies (Wortsman et al., 2023).
4.2 Dynamical system with representation
As a variant of model (2), we additionally consider a (nonlinear) dynamical system setting with data D=
(x1, . . . ,xN), where each xi+1depends on the kpreceding inputs [xi−k+1;. . .;xi]for some k≥1through
a linear function on top of a fixed representation function Φ⋆. Compared to the supervised learning setting
in Section 4.1, this setting better resembles some aspects of natural language, where the next token in general
depends on several preceding tokens.
2The predictor byi=byi(Di−1,xi)that minimizes the posterior square loss E[1
2(byi−yi)2|Di−1,xi].
5

--- PAGE 6 ---
Formally, let k≥1denote the number of input tokens that the next token depends on, and Φ⋆:Rkd→RD
denotes a representation function. Each ICL instance D={xi}i∈[N]is generated as follows: First sample
P=PWwhere W∈RD×dis sampled from a Gaussian prior: Wijiid∼N(0, τ2). Then sample the initial input
x1∼Pxand let
xi+1=W⊤Φ⋆([xi−k+1;. . .;xi]) +σzi,ziiid∼N(0,Id), i∈[N−1], (5)
where we understand xj:=0dforj≤0. We choose Φ⋆to be the same L-layer MLP as in (3), except that
the first weight matrix has size B⋆
1∈RD×kdto be consistent with the dimension of the augmented input
xi:= [xi−k+1;. . .;xi]. We remark that (5) substantially generalizes the setting of Li et al. (2023a) which only
considers lineardynamical systems (equivalent to Φ⋆≡id), a task arguably much easier for transformers to
learn in context.
Asxiacts as both inputs and labels in model (5), we use the following input format for transformers:
H:=
x1. . .xN
p1. . .pN
∈RDhid×N, (6)
where pi:= [0Dhid−d−4; 1;i;i2;i3], and we extract prediction bxi+1from the i-th output token.
Theory Similar as above, we consider the ridge predictor for the dynamical system setting
cWΦ⋆,λ
i:= arg minW∈RD×d1
2(i−1)Pi−1
j=1W⊤Φ⋆(xj)−xj+12
2+λ
2∥W∥2
Fr. (Φ⋆-Ridge-Dyn)
We understand cWΦ⋆,λ
0:=0D×d, and let ∥W∥2,∞:= max j∈[d]∥W:,j∥2for any W∈RD×d. Again, ( Φ⋆-
Ridge-Dyn) gives the Bayes-optimal predictor (cWΦ⋆,λ
i)⊤Φ⋆(xi)atλ=σ2/τ2.
The following result shows that ( Φ⋆-Ridge-Dyn) can also be implemented efficiently by a transformer. The
proof can be found in Appendix C.2.
Theorem 2 (Transformer can implement Φ⋆-Ridge for dynamical system) .For the dynamical system setting
where the L-layer representation function Φ⋆:Rkd→RDtakes form (3), but otherwise same settings
as Theorem 1, there exists a transformer TFwithL+ 2 +O(κlog(BΦBw/ε))layers, max{3d,5}heads, and
Dhid= max {2(k+ 1), D}d+ 3(D+d) + 5such that the following holds.
For any dataset Dsuch that ∥Φ⋆(xi)∥2≤BΦ,∥xi∥∞≤By, and∥cWΦ⋆,λ
i∥2,∞≤Bw/2(cf. ( Φ⋆-Ridge-Dyn))
for all i∈[N], and corresponding input H∈RDhid×Nof format (6), we have
(a) The first transformer layer copies the kprevious inputs into the current token, and computes the first
layer{σρ(B⋆
1xi)}i∈[N]within Φ⋆:
Attn(1)(H) =x1. . .xN
p1. . .pN
=
x1−k+1. . .xN−k+1
| |
x1 . . . xN
p1 . . . pN
; (7)
TF(1)(H) = MLP(1)
Attn(1)(H)
=
σρ(B⋆
1x1). . . σ ρ(B⋆
1xN)
x1 . . . xN
p′
1 . . . p′
N
. (8)
(b) The first (L+ 1)layers of TFtransforms each xitoΦ⋆(xi), and copies the preceding representation
Φ⋆(xi−1)onto the same token to form the (input, label) pair (Φ⋆(xi−1),xi):
TF(1:L+1)(H) =
Φ⋆(x1) Φ⋆(x2). . . Φ⋆(xN)
0d 0d . . . 0d
0D Φ⋆(x1). . . Φ⋆(xN−1)
x1 x2 . . . xN
ep1 ep2 . . . epN
. (9)
Above, pi,p′
i,epionly differs from piin the dimension of the zero paddings.
6

--- PAGE 7 ---
(a) Varying noise level
0 10 20 30 40
in-context examples0.00.20.40.60.81.0square loss
Transformer
opt Φ⋆-ridge
σ=0.0
σ=0.1
σ=0.5 (b) Varying rep hidden dimension
0 10 20 30 40
in-context examples0.00.20.40.60.81.0
Transformer
opt Φ⋆-ridge
D=5
D=20
D=80 (c) Varying depth of rep
0 10 20 30 40
in-context examples0.00.20.40.60.81.0
Transformer
opt Φ⋆-ridge
L=1
L=2
L=3
L=4
Figure 2: Test ICL risk for learning with representations. Each plot modifies a single problem parameter from the
base setting (L, D, σ ) = (2 ,20,0.1). Dotted lines plot the Bayes-optimal risks for each setting respectively.
(c) For every index i∈[N], the transformer output eH= TF( H)contains prediction bxi+1:= [ehi]1:dthat is
close to the ( Φ⋆-Ridge-Dyn) predictor: ∥bxi+1−(cWΦ⋆,λ
i)⊤Φ⋆(xi)∥∞≤ε.
To our best knowledge, Theorem 2 provides the first transformer construction for learning nonlinear dynam-
ical systems in context. Similar as for Theorem 1, the bounds on the transformer size here imply guarantees
eεexcess risk within eO 
(L+κ)2((k+D)d)2eε−2
(pre-)training instances.
In terms of the mechanisms, compared with Theorem 1, the main differences in Theorem 2 are (1) the
additional copying step (7) within the first layer, where the previous (k−1)tokens [xi−k+1;. . . ,xi−1]are
copied onto the xitoken, to prepare for computing of Φ⋆(xi); (2) the intermediate output (9), where relevant
information (for preparing for linear ICL) has form [Φ⋆(xi−1);xi; Φ⋆(xi)]and is gathered in the x-tokens,
different from (4) where the relevant information is [Φ⋆(xi);yi], gathered in the y-token. We will test these
in our experiments (Section 5.2).
5 Experiments
We now empirically investigate trained transformers under the two settings considered in Section 4.1 & 4.2.
In both cases, we choose the representation function Φ⋆to be a normalized version of the L-layer MLP (3):
Φ⋆(x):=eΦ⋆(x)/∥eΦ⋆(x)∥2, where eΦ⋆takes form (3), with weight matrices (B⋆
i)i∈[L]sampled as random
(column/row)-orthogonal matrices and held fixed in each experiment, and slope ρ= 0.01. We test L∈
{1,2,3,4}, hidden dimension D∈ {5,20,80}, and noise level σ∈ {0,0.1,0.5}. All experiments use Px=
N(0,Id),τ2= 1,d= 20, and N= 41.
WeuseasmallarchitecturewithintheGPT-2familywith12layers, 8heads, and Dhid= 256, following(Garg
et al., 2022; Li et al., 2023a; Bai et al., 2023). The (pre)-training objective for the transformer (for the
supervised learning setting) is the average prediction risk at all tokens:
min θEw,D∼Pwh
1
2NPN
i=1(byθ,i(Di−1,xi)−yi)2i
, (10)
where byθ,iis extracted from the (2i−1)-th output token of TFθ(H)(cf. Section 3). The objective for the
dynamical system setting is defined similarly. Additional experimental details can be found in Appendix D,
and ablation studies (e.g. along the training trajectory; cf. Figure 9) in Appendix F.
5.1 Supervised learning with representation
We first test ICL with supervised learning data as in Section 4.1, where for each configuration of (L, D, σ )
(which induces a Φ⋆) we train a transformer on ICL data distribution (2) and evaluate ICL on the same
distribution. Note that Figure 1c & 1b plots the results for (L, D, σ ) = (2 ,20,0.1).
ICL performance Figure 2 reports the test risk across various settings, where we observe that trained
transformers can consistently match the Bayes-optimal ridge predictor. This extends existing results which
7

--- PAGE 8 ---
(a) Probe Φ⋆(xi)atxitokens
0 2 4 6 8 10 12
layer10−210−1mean squared error
L=1
L=2
L=3
L=4 (b) Probe Φ⋆(xi)atyitokens
024681012
layer10−1
 (c) Probe byΦ⋆,λ
iatxitokens
024681012
layer10−210−1100
L=2,σ=0.1
L=2,σ=0.5
L=4,σ=0.1
L=4,σ=0.5
Figure 3: Probing errors for the learning with representation setting. Each setting modifies one or two problem
parameters from the base setting (L, D, σ ) = (2 ,20,0.1). Note that the orange curve corresponds to the same setting
(and thus the same transformer) across (a,b,c), as well as the red curve.
show that linear functions (without a representation) can be learned near-optimally in-context by transform-
ers(Gargetal.,2022;Akyüreketal.,2022), addingourmodel(2)tothislistof(empirically)nearly-optimally
learnable function classes. Among the complexity measures (L, D, σ ), observe that the noise level σand hid-
den dimension Dof the representation (Figure 2a & 2b) appears to have a larger effect on the (nearly
Bayes-optimal) risk than the depth L(Figure 2c).
Mechanismsvialinearprobing Weconductprobingexperimentstofurtherunderstandthemechanisms
of the trained transformers. In accordance with the theoretical construction in Theorem 1, our main question
here is: Does the trained transformer perform the following in order:
1. Computes Φ⋆(xi)atxitokens;
2. Copies them onto the following yitoken and obtains dataset {Φ⋆(xi), yi}iin the form of (4);
3. Performs linear ICL on top of {Φ⋆(xi), yi}i?
Whilesuchinternalmechanismsareingeneraldifficulttoquantifyexactly, weadaptthe linear probing (Alain
and Bengio, 2016) technique to the transformer setting to identify evidence. Linear probing allows us to test
whether intermediate layer outputs (tokens) {hx,(ℓ)
i}ℓ∈[12](ℓdenotes the layer) and {hy,(ℓ)
i}ℓ∈[12]“contains”
various quantities of interest, by linearly regressing these quantities (as the y) on the intermediate tokens
(as the x), pooled over the token index i∈[N]. For example, regressing Φ⋆(xi)onhx,(ℓ)
itests whether
thexitoken after the ℓ-th layer “contains” Φ⋆(xi), where a smaller error indicates a better containment.
See Appendix D.1 for further setups of linear probing.
Figure 3 reports the errors of three linear probes across all 12 layers: The representation Φ⋆(xi)in the xi
tokens and yitokens, and the optimal ridge prediction byΦ⋆,λ
iin the xitokens. Observe that the probing
errors for the representation decrease through lower layers and then increase through upper layers (Figure 3a
& 3b), whereas probing errors for the ridge prediction monotonically decrease through the layers (Figure 3c),
aligning with our construction that the transformer first computes the representations and then performs
ICL on top of the representation. Also note that deeper representations take more layers to compute (Figure
3a). Further, the representation shows up later in the y-tokens (layers 5-6) than in the x-tokens (layers
1,3,4,5), consistent with the copying mechanism, albeit the copying appears to be lossy (probe errors are
higher at y-tokens).
Finally, observe that the separation between the lower and upper modules seems to be strong in certain
runs—For example, the red transformer ( L= 4, σ= 0.1) computes the representation at layer 5, copies
them onto y-tokens at layer 6, and starts to perform iterative ICL from layer 7, which aligns fairly well with
our theoretical constructions at a high level.
Investigating upper module via pasting To further investigate upper module, we test whether it is
indeed a strong ICL learner on its own without relying on the lower module, which would provide stronger
8

--- PAGE 9 ---
(a) Illustration of the pasting experiment
Pasting
 ~ Linear- Model{(xi,yi)}Ni=1Φ⋆TF Lower Layerŝy1
Original ModelTF Upper LayerŝyN⋯
 ~ Linear Model{(x′ i,y′ i)}Ni=1TF Upper Layers (Frozen)Trainable linear/TF embed.̂yi≈yi ?̂y′ i≈y′ isame “containing” (hxi,hyi)(Φ⋆(xi),yi)̂yi⋯̂y1̂yN⋯̂yi⋯ (b) Linear ICL in TF_upper via pasting
0 10 20 30 40
in-context examples0.00.20.40.60.81.01.2square loss
TF_upper+1_layer_TF_embed
TF_upper+linear_copy_embed
TF_upper+linear_embed
fresh_1_layer_TF
optimal_ridge
Figure 4: (a)Illustration of our pasting experiment, which examines the linear ICL capability of the upper module
of a trained transformer. (b)Pasting results for the upper module of a trained transformer in setting (L, D, σ ) =
(3,20,0.1). It achieves nearly optimal linear ICL risk (in 20dimension with noise 0.1), using a 1-layer transformer
embedding, and also non-trivial performance using linear and linear copy embeddings.
evidence that the upper module performs linear ICL. However, a key challenge here is that it is unclear how
to feed raw inputs directly into the upper module, as they supposedly only admit input formats emitted
from the lower module—the part we wanted to exclude in the first place.
We address this by conducting a pastingexperiment, where we feed D-dimensional linear ICL problems
(y′
i=⟨w′,x′
i⟩withouta representation) with input format (1) directly to the upper module of the trans-
former trained on representation Φ⋆, by adding a trainable embedding layer in between; see Figure 4a for an
illustration of the pasting approach. This trainable embedding layer itself needs to be shallow without much
ICL power—we test the following three choices: (1) Linearembedding: hx
i=W[xi; 0]andhy
i=W[0D;yi];
(2)Linear-copy embedding, where the ytokens are instead hy
i=W[xi;yi], motivated by the format (4); (3)
One-layer transformer embedding TF, which computes H=TF(H). See Appendix D.2 for further setups
of pasting.
Figure 4b shows the pasting results on a trained transformer on (L, D, σ ) = (3 ,20,0.1)(an ablation in Figure
10b), where we dissect the lower and upper modules at layer 4 as suggested by the probing curve (Figure 3a
green). Perhaps surprisingly, the upper module of the transformer can indeed perform nearly optimal linear
ICL without representation when we use the one-layer transformer embedding. Note that a (freshly trained)
single-layer transformer itself performs badly, achieving about the trivial test risk 1.01, which is expected
due to our specific input format3(1). This suggests that the majority of the ICL is indeed carried by the
upper module, with the one-layer transformer embedding not doing much ICL itself. Also note that the
linear-copy and linear embeddings also yield reasonable (though suboptimal) performance, with linear-copy
performing slightly better.
5.1.1 Extension: Mixture of multiple representations
We aditionally investigate an harder scenario in which there exists multiple possible representation functions
(Φ⋆
j)j∈[K], and the ICL data distribution is a mixture of the Kdistributions of form (2) each induced by
Φ⋆
j(equivalent to using the concatenated representation Φ⋆= [Φ⋆
1, . . . , Φ⋆
K]with a group 1-sparse prior on
w∈RKD). We find that transformers still approach Bayes-optimal risks, though less so compared with
the single-representation setting. Using linear probes, we find that transformers sometimes implement the
post-ICL algorithm selection mechanism identified in Bai et al. (2023), depending on the setting. Details are
deferred to Appendix E due to the space limit.
3A one-layer transformer does not have much ICL power using input format (1)— xiandyiare stored in separate tokens
there, which makes “one-layer” mechanisms such as gradient descent (von Oswald et al., 2022; Akyürek et al., 2022; Bai et al.,
2023) unlikely to be implementable; see Appendix D.3 for a discussion.
9

--- PAGE 10 ---
(a) Risk
0 10 20 30 40
in-context examples0.00.20.40.60.81.0mean squared loss
Transformer
opt Φ⋆-ridge
D=20,σ=0.1
D=20,σ=0.5
D=80,σ=0.5(b) Probe past inputs at xitokens
0 2 4 6 8 10 12
layer10−310−210−1100mean squared error
B⋆
1,0xi
B⋆
1,−1xi−1
B⋆
1,−2xi−2
σ(B⋆
1̄xi)
B⋆
2σ(B⋆
1̄xi)
σ(B⋆
2σ(B⋆
1̄xi)) (c) Probe Φ⋆(xi−j)atxitokens
024681012
layer10−210−1
Φ⋆(̄xi)
Φ⋆(̄xi−1)
Φ⋆(̄xi−2)
Φ⋆(̄xi−3)
Φ⋆(̄xi−4)
Figure5: ICLrisksandprobingerrorsforthedynamicalsystemsetting. (a)Eachcurvemodifiesproblemparameters
from the base setting (k, L, D, σ ) = (3 ,2,80,0.5).(b,c)Results are with the same base setting.
5.2 Dynamical systems
WenowstudythedynamicalsystemssettinginSection4.2usingthesameapproachesasinSection5.1.Figure
5a shows that transformers can still consistently achieve nearly Bayes-optimal ICL risk. An ablation of the
risks and probing errors in alternative settings can be found in Appendix F.2.
Probing copying mechanisms The main mechanistic question we ask here is about the data preparation
phase, where the transformer construction in Theorem 2 performs copying twice:
i) A copying of [xi−k+1;. . .;xi−1]onto the xitoken as in (7), to prepare for the computation of Φ⋆(xi); As
copying may not be distinguishable from the consequent matrix multiplication step[xi−k+1;. . . ,xi−1;xi]
7→B⋆
1[xi−k+1;. . . ,xi−1;xi], we probe instead the result B⋆
1,−jxi−jafter matrix multiplication, where
B⋆
1,−j∈RD×ddenotes the block within B⋆
1hitting xi−j.
ii) A second copying of Φ⋆(xi−1)onto the xitoken to obtain (9), after {Φ⋆(xi)}iare computed.
We probe one transformer trained on the dynamical systems problem with k= 3(so that the useful preceding
inputsare xi−1andxi−2),andfindthatthetransformerindeedperformsthetwoconjecturedcopyings.Figure
5b demonstrates copying i) onto the current token, where the copying of xi−1happens earlier (at layer 3) and
is slightly more accurate than that of xi−2(at layer 4), as expected. Further observe that layer 4 (which we
recall contains an attention layer and an MLP layer) have seemingly also implemented the (unnormalized)
MLP representation eΦ⋆(xi) = σρ(B⋆
2σρ(B⋆
1xi)), though the probing error for the actual representation
Φ⋆(xi) =eΦ⋆(xi)/∥eΦ⋆(xi)∥2continues to drop in layer 4-6 (Figure 5c). Figure 5c further demonstrates
copying ii), where Φ⋆(xi−1)are indeed copied to the i-th token, whereas by sharp contrast Φ⋆(xi−k)for
k≥2arenotcopied at all into the xitoken, aligning with our conjectured intermediate output format (9).
6 Conclusion
This paper presents theoretical and mechanistic studies on the in-context learning ability of transformers on
learning tasks involving representation functions, where we give efficient transformer constructions for linear
ICL on top of representations for the supervised learning and dynamical system setting, and empirically
confirm the existence of various high-level mechanisms in trained transformers. We believe our work opens
up the investigation of ICL beyond simple function classes, and suggests open questions such as further
investigations of the mechanisms of the linear ICL modules, and theory for ICL in more complex function
classes. One limitation of our work is that the setting still consists of synthetic data with idealistic repre-
sentation functions; performing similar studies on more real-world data would be an important direction for
future work.
10

--- PAGE 11 ---
References
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement precon-
ditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297 , 2023.
Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm
is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661 , 2022.
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. arXiv
preprint arXiv:1610.01644 , 2016.
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable
in-context learning with in-context algorithm selection. arXiv preprint arXiv:2306.04637 , 2023.
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a transformer:
A memory viewpoint. arXiv preprint arXiv:2306.00802 , 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends ®in Machine
Learning , 8(3-4):231–357, 2015.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter
Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early
experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James
McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning in trans-
formers. Advances in Neural Information Processing Systems , 35:18878–18891, 2022.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn in-context?
language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559 ,
2022.
N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, et al.
A mathematical framework for transformer circuits. Transformer Circuits Thread , 2021.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-
context? a case study of simple function classes. Advances in Neural Information Processing Systems , 35:
30583–30598, 2022.
Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks.
Advances in Neural Information Processing Systems , 34:9574–9586, 2021.
Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning
by meta-learning transformers. arXiv preprint arXiv:2212.04458 , 2022.
Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms:
Generalizationandimplicitmodelselectioninin-contextlearning. arXiv preprint arXiv:2301.07067 , 2023a.
Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. Dissecting
chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint arXiv:2305.18869 ,
2023b.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes
good in-context examples for gpt- 3?arXiv preprint arXiv:2101.06804 , 2021.
11

--- PAGE 12 ---
YaoLu,MaxBartolo,AlastairMoore,SebastianRiedel,andPontusStenetorp. Fantasticallyorderedprompts
and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786 ,
2021.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations
in gpt.Advances in Neural Information Processing Systems , 35:17359–17372, 2022.
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompt-
ing for few-shot text classification. arXiv preprint arXiv:2108.04106 , 2021a.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context.
arXiv preprint arXiv:2110.15943 , 2021b.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint
arXiv:2202.12837 , 2022.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint
arXiv:2209.11895 , 2022.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.
Tilman Räuker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey
on interpreting the inner structures of deep neural networks. In 2023 IEEE Conference on Secure and
Trustworthy Machine Learning (SaTML) , pages 464–483. IEEE, 2023.
AllanRaventós, MansheejPaul, FengChen, andSuryaGanguli. Pretrainingtaskdiversityandtheemergence
of non-bayesian in-context learning for regression. arXiv preprint arXiv:2306.15063 , 2023.
Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term
frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206 , 2022.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning.
arXiv preprint arXiv:2112.08633 , 2021.
Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and softmax in
transformer. arXiv preprint arXiv:2302.06461 , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288 , 2023.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart
Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in neural
information processing systems , 33:12388–12401, 2020.
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, An-
drey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677 , 2022.
Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability
in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593 ,
2022.
JasonWei, XuezhiWang, DaleSchuurmans, MaartenBosma, FeiXia, EdChi, QuocVLe, DennyZhou, etal.
Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems , 35:24824–24837, 2022.
12

--- PAGE 13 ---
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint
arXiv:2303.03846 , 2023.
Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Replacing softmax with relu in vision
transformers. arXiv preprint arXiv:2309.08586 , 2023.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning
as implicit bayesian inference. arXiv preprint arXiv:2111.02080 , 2021.
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. arXiv
preprint arXiv:2306.09927 , 2023a.
Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning
learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420 ,
2023b.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot
performance of language models. In International Conference on Machine Learning , pages 12697–12706.
PMLR, 2021.
A Technical tools
The following convergence result for minimizing a smooth and strongly convex function is standard from
the convex optimization literature, e.g. by adapting the learning rate in (Bubeck, 2015, Theorem 3.10) from
η= 1/βto any η≤1/β.
Proposition A.1 (Gradient descent for smooth and strongly convex functions) .Suppose L:Rd→R
isα-strongly convex and β-smooth for some 0< α≤β. Then, the gradient descent iterates wt+1
GD:=
wt
GD−η∇L(wt
GD)with learning rate η≤1/βand initialization w0
GD∈Rdsatisfies for any t≥1,
wt
GD−w⋆2
2≤exp (−ηα·t)·w0
GD−w⋆2
2.
where w⋆:= arg minw∈RdL(w)is the minimizer of L.
B Proofs for Section 4.1
Throughout the rest of this and next section, we consider transformer architectures defined in Section 3
where we choose σto be the (entry-wise) ReLU activation normalized by sequence length, following (Bai
et al., 2023): For all A∈RN×Nandi, j∈[N],
[σ(A)]ij=1
jσ(Aij), (11)
where we recall σ(t) = max {t,0}denotes the standard ReLU. This activation is similar as the softmax in
that, for every (query index) j, the resulting attention weights {1
jσ(Aij)}i∈[j]is approximately a probabil-
ity distribution in typical scenarios, in the sense that they are non-negative and sum to O(1)when each
Aij=O(1). We remark that transformers with (normalized) ReLU activation is recently shown to achieve
comparable performance with softmax in larger-scale tasks (Shen et al., 2023; Wortsman et al., 2023).
Withactivationchosenas(11),a(decoder-only)attentionlayer eH= Attn θ(H)with θ= (Qm,Km,Vm)m∈[M]
takes the following form in vector notation:
ehi=hi+MX
m=11
iiX
j=1σ(⟨Qmhi,Kmhj⟩)·Vmhj.
13

--- PAGE 14 ---
Recall our input format (1):
H=
x10. . .xN0
0y1. . . 0yN
px
1py
1. . .px
Npy
N
∈RDhid×2N.
We will use (hk)k∈[2N]and(hx
i,hy
i)i∈[N]interchangeably to denote the tokens in (1), where hx
i:=h2i−1
andhy
i:=h2i. Similarly, we will use (px
i,py
i)i∈[N]and(pk)k∈[2N]interchangably to denote the positional
encoding vectors in (1), where p2i−1:=px
iandp2i:=py
i. Unless otherwise specified, we typically reserve
usei, jas (query, key) indices within [N]andk, ℓas (query, key) indices within [2N].
We use the following positional encoding vectors for all i∈[N]:
px
i= [0Dhid−d−9; 1; 2i−1; (2i−1)2; (2i−1)3;i;i2; 1;i],
py
i= [0Dhid−d−9; 1; 2i; (2i)2; (2i)3;i;i2; 0; 0].(12)
Note that pkcontains [1;k;k2;k3]for all k∈[2N];px
i,py
icontains [i;i2], an indicator of being an x-token,
and the product of the indicator and i.
B.1 Useful transformer constructions
LemmaB.1 (Copyingbyasingleattentionhead) .Thereexists asingle-head attentionlayer θ= (Q,K,V)⊂
RDhid×Dhidthat copies each xiinto the next token for every input Hof the form (1), i.e.
Attn θ(H) =
x1x1. . .xNxN
0y1. . . 0yN
px
1py
1. . .px
Npy
N
∈RDhid×2N.
Proof.By assumption of the positional encoding vectors, we can define matrices Q,K∈RDhid×Dhidsuch
that for all k, ℓ∈[2N],
Qhk= [k3;k2;k;0Dhid−3],Khℓ= [−1; 2ℓ+ 2;−ℓ2−2ℓ;0Dhid−3].
This gives that for all ℓ≤k,
σ(⟨Qhk,Khℓ⟩)
=σ 
−k3+k2(2ℓ+ 2)−k(ℓ2+ 2ℓ)
=σ 
k(1−(k−ℓ−1)2)
=k1{ℓ=k−1}.
Further defining Vsuch that Vhx
i= [xi;0]andVhy
i=0, we have for every k∈[2N]that
X
ℓ≤k1
kσ(⟨Qhk,Khℓ⟩)Vhℓ
=1
k·k1{ℓ=k−1} ·[x⌈ℓ/2⌉1{ℓis odd};0] = [x⌈ℓ/2⌉;0]·1{ℓ=k−1andℓis odd}.
By the residual structure of the attention layer, the above exactly gives the desired copying behavior, where
every xion the odd token His copied to the next token.
Lemma B.2 (Linear prediction layer) .For any Bx, Bw, By>0, there exists an attention layer θ=
{(Qm,Km,Vm)}m∈[M]with M= 2heads such that the following holds. For any input sequence H∈
RDhid×2Nthat takes form
hx
i= [xi; 0;wi;px
i],hy
i= [xi;yi;0d;py
i]
with∥xi∥2≤Bx,|yi| ≤By, and∥w∥2≤Bw, it gives output Attn θ(H) =eH∈RDhid×2Nwith
ehx
i=eh2i−1= [xi;byi;wi;px
i],where byi=⟨xi,wi⟩
for all i∈[N].
14

--- PAGE 15 ---
Proof.LetR:= max {BxBw, By}. Define matrices (Qm,Km,Vm)m=1,2as
Q1hx
i=
wi
i
R
0
,K1hx
j=K1hy
j=
xj
−2R
2j+ 1
0
,V1hℓ=
0d
ℓ
0Dhid−d−1
,
Q2hx
i=
i
R
0
,K2hx
j=K1hy
j=
−2R
2j+ 1
0
,V2hℓ=−
0d
ℓ
0Dhid−d−1

for all i, j∈[N]andℓ∈[2N]. For every i∈[N], we then have
2X
m=12i−1X
ℓ=11
2i−1σ(⟨Qmhx
i,Kmhℓ⟩)·Vmhℓ
=1
2i−1iX
j=1
σ 
w⊤
ixj+R(−2i+ 2j+ 1)
−σ(R(−2i+ 2j+ 1))
·[0d; 2j−1;0Dhid−d−1]
+i−1X
j=1
σ 
w⊤
ixj+R(−2i+ 2j−1)
−σ(R(−2i+ 2j+ 1))
·[0d; 2j;0Dhid−d−1]
=1
2i−1·w⊤
ixi·[0d; 2i−1;0Dhid−d−1] = [0d;w⊤
ixi;0Dhid−d−1].
By the residual structure of an attention layer, the above shows the desired result.
Lemma B.3 (Implementing MLP representation by transformers) .Fix any MLP representation function
Φ⋆of the form (3), suppose Dhid≥max{2D, D +d+ 10}, where Dis the hidden dimension within the
MLP (3). Then there exists a transformer TFθwith (L+ 1)layers and 5heads that exactly implements Φ⋆
in a token-wise fashion, i.e. for any input Hof form (1),
eH= TF θ(H) =
Φ⋆(x1)0. . . Φ⋆(xN)0
0 y1. . . 0 yN
epx
1epy
1. . . epx
Nepy
N
,
whereepx
i,epy
idiffers from px
i,py
ionly in the dimension of their zero paddings.
Proof.Recall that Φ⋆(x) =σρ(B⋆
L···σρ(B⋆
1x)···). We first show how to implement a single MLP layer
x7→σρ(B⋆
1x)by an (MLP-Attention) structure.
Consider any input token hx
i= [xi; 0;px
i]at an x-location. Define matrices W1,W2∈RDhid×Dhidsuch that
W1hx
i=
B⋆
1xi
−B⋆
1xi
0
, σ(W1hx
i) =
σ(B⋆
1xi)
σ(−B⋆
1xi)
0
,
W2σ(W1hx
i) =
0d
σ(B⋆
1xi)−ρσ(−B⋆
1xi)
0
=
0d
σρ(B⋆
1xi)
0
.
Therefore, the MLP layer (W1,W2)outputs
hx
i:= [MLP W1,W2(H)]x
i=hx
i+W2σ(W1hx
i) =
xi
σρ(B⋆
1xi)
0
px
i
,
and does not change the y-tokens.
15

--- PAGE 16 ---
We next define an attention layer that “moves” σρ(B1xi)to the beginning of the token, and removes xi.
Define three attention heads θ= (Qm,Km,Vm)m∈[3]as follows:
Q{1,2,3}hk=
k2
k
k1{kis odd}
0
,K{1,2,3}hℓ=
−1
ℓ
1
0
,
V1hx
j=
σρ(B⋆
1xj)
0d
0
,V2hx
j=
−xj
0D
0
,V3hx
j=
0d
−σρ(B⋆
1xj)
0
.
The values for V1,2,3hy
iare defined automatically by the same operations over the hy
itokens (which does
not matter to the proof, as we see shortly). For any ℓ≤kandm∈[3],
1
kσ 
Qmhk,Kmhℓ
=1
kσ(k(−k+ℓ+ 1{kis odd})) = 1{ℓ=k, kis odd}.
Therefore, these three attention heads are only active iff the query token k= 2i−1is odd (i.e. being an
x-token) and ℓ=k= 2i−1. At such tokens, the three value matrices (combined with the residual structure
of attention) would further remove the xipart, and move σρ(B⋆
1xi)to the beginning of the token, i.e.
ehx
i=
Attn θ(H)x
i=
σρ(B⋆
1xi)
0
px
i
,
andehy
i=hy
i. Additionally, we now add two more attention heads into θto move all yifrom entry d+ 1to
D+ 1, and leaves the x-tokens unchanged.
Repeating the above argument Ltimes, we obtain a structure (MLP-Attention- . . .-MLP-Attention) with
five heads in each attention layer that exactly implements the Φ⋆in a token-wise fashion. This structure can
be rewritten as an (L+ 1)-layer transformer by appending an identity {Attention, MLP} layer (with zero
weights) {before, after} the structure respectively, which completes the proof.
B.2 In-context ridge regression by decoder transformer
This section proves the existence of a decoder transformer that approximately implements in-context ridge
regression at every token i∈[N]simultaneously. For simplicity, we specialize our results to the ridge
regression problem; however, our construction can be directly generalized to any (generalized) linear models
with a sufficiently smooth loss, by approximating the gradient of the loss by sum of relus (Bai et al., 2023,
Section 3.5).
Denote the regularized empirical risk for ridge regression on dataset Di={(xj, yj)}j∈[i]by
bLλ
i(w):=1
2iiX
j=1 
w⊤xj−yj2+λ
2∥w∥2
2(13)
for all i∈[N]. Letbwλ
i:= arg minw∈RdbLλ
i−1(w)denote the minimizer of the above risk (solution of ridge
regression) for dataset Di−1. We further understand bLλ
0(w):= 0andbwλ
1:=0. LetbLi(w):=bL0
i(w)denote
the unregularized version of the above risk.
Proposition B.4 (Approximating a single GD step by a single attention layer) .For any η >0and any
Bx, Bw, By>0, there exists an attention layer θ={(Qm,Km,Vm)}m∈[M]withM= 3heads such that the
following holds. For any input sequence H∈RDhid×2Nthat takes form
hx
i= [xi; 0;wi;px
i],hy
i= [xi;yi;0d;py
i]
16

--- PAGE 17 ---
with∥xi∥2≤Bx,|yi| ≤By, and∥w∥2≤Bw, it gives output Attn θ(H) =eH∈RDhid×2Nwithehx
i=eh2i−1=
[xi; 0;ewi;px
i], where
ewi=wi−ηi∇bLλ
i−1(wi)
withηi=i−1
2i−1η, andehy
i=hy
i, for all i∈[N].
Proof.LetR:= max {BxBw, By}. By the form of the input (hk)k∈[2N]in (1), we can define two attention
heads {(Qm,Km,Vm)}m=1,2⊂RDhid×Dhidsuch that for all i, j∈[N],
Q1hx
i=
wi/2
−1
i
−3R
−R
0
,K1hy
j=
xj
yj
3R
j
1
0
,V1hx
j=V1hy
j=−η·
0d+1
xj
0Dhid−2d−1
,
Q2hx
i=Q2hy
i=
i
−3R
−R
0
,K2hx
j=K2hy
j=
3R
j
1
0
,V2hx
j=V2hy
j=η·
0d+1
xj
0Dhid−2d−1
.
Further, Q1hy
itakes the same form as Q1hx
iexcept for replacing the wi/2location with 0dand replacing
the−1location with 0(using the indicator for being an x-token within px
i,py
i);K1hx
jtakes the same form
asK1hy
jexcept for replacing the yjlocation with 0.
Fixing any i∈[N]. We have for all j≤i−1,
σ 
Q1hx
i,K1hy
j
−σ 
Q2hx
i,K2hy
j
=σ 
w⊤
ixj/2−yj+R(3i−3j−1)
−σ(R(3i−3j−1)) = w⊤
ixj/2−yj,
and for all j≤i,
σ 
Q1hx
i,K1hx
j
−σ 
Q2hx
i,K2hx
j
=σ 
w⊤
ixj/2 +R(3i−3j−1)
−σ(R(3i−3j−1)) = w⊤
ixj/2·1{j≤i−1}.
Above, we have usedw⊤
ixj/2−yj≤3R/2,w⊤
ixj/2≤R/2, and the fact that σ(z+M)−σ(M)equals
zforM≥ |z|and0forM≤ −| z|.
Therefore for all j≤i−1,
σ 
Q1hx
i,K1hy
j
V1hy
j+σ 
Q2hx
i,K2hy
j
V2hy
j
= 
σ 
Q1hx
i,K1hy
j
−σ 
Q2hx
i,K2hy
j
· −η[0d+1;xj;0Dhid−2d−1]
=−η 
w⊤
ixj/2−yj
·[0d+1;xj;0Dhid−2d−1],
and similarly for all j≤i,
σ 
Q1hx
i,K1hx
j
V1hx
j+σ 
Q2hx
i,K2hx
j
V2hx
j
=−η 
w⊤
ixj/2
1{j≤i−1} ·[0d+1;xj;0Dhid−2d−1]
Summing the above over all key tokens ℓ∈[2i−1], we obtain the combined output of the two heads at query
17

--- PAGE 18 ---
token 2i−1(i.e. the i-thx-token):
2i−1X
ℓ=1X
m=1,21
2i−1σ(⟨Qmh2i−1,Kmhℓ⟩)Vmhℓ
=i−1X
j=1X
m=1,21
2i−1σ 
Qmhx
i,Kmhy
j
Vmhy
j+iX
j=1X
m=1,21
2i−1σ 
Qmhx
i,Kmhx
j
Vmhx
j
=1
2i−1
i−1X
j=1−η 
w⊤
ixj/2−yj
+iX
j=1−η 
w⊤
ixj/2
1{j≤i−1}
·[0d+1;xj;0Dhid−2d−1]
=i−1
2i−1·h
0d+1;−η∇bLi−1(wi);0Dhid−2d−1i
.(14)
It is straightforward to see that, repeating the same operation at query token 2i(i.e. the i-thy-token) would
output 0Dhid, since the query vector Q1hy
icontains [0d; 0]instead of [wi/2;−1]as inQ1hx
i.
We now define one more attention head (Q3,K3,V3)⊂RD×Dsuch that for all k∈[2N],j∈[N],
Q3hk=
k2
k
1
0
,K3hℓ=
−1/2
(1−ℓ)/2
1−ℓ/2
0
,V3hx
j=
0d+1
−ηλwj
0Dhid−2d−1
,V3hy
j=0Dhid.
For any ℓ≤k, we have
σ(⟨Q3hk,K3hℓ⟩) =σ 
−k2/2 +k(1−ℓ)/2 + 1−ℓ/2
=k−1
2σ(−k+ℓ+ 1) =k−1
21{ℓ=k}.
Therefore, for query token k= 2i−1, the attention head outputs
kX
ℓ=11
kσ(⟨Q3hk,K3hℓ⟩)Vmhℓ=kX
ℓ=11
k·k−1
21{ℓ=k} ·Vmhℓ
=k−1
2k·Vmhk=i−1
2i−1·Vmhx
i=i−1
2i−1·[0d+1;−ηλwi;0Dhid−2d−1].(15)
It is straightforward to see that the same attention head at query token k= 2ioutputs 0Dhid, as the value
vector V3hk=V3hy
iis zero.
Combining (14) and (15), letting the full attention layer θ:={(Qm,Km,Vm)}m=1,2,3, we have Attn θ(H)
=eH, where for all i∈[N],
ehx
i=eh2i−1=h2i−1+3X
m=12i−1X
ℓ=11
2i−1σ(⟨Qmh2i−1,Kmhℓ⟩)·Vmhℓ
=
xi
0
wi
∗
+i−1
2i−1
0d+1
−η
∇bLi−1(wi) +λwi
0Dhid−2d−1
=
xi
0
wi−ηi∇bLλ
i−1(wi)
∗
,
where ηi:=i−1
2i−1wi, andehy
i=hy
i. This finishes the proof.
Theorem B.5 (In-context ridge regression by decoder-only transformer) .For any λ≥0,Bx, Bw, By>0
with κ:= 1 + B2
x/λ, and ε < B xBw/2, let Dhid≥2d+ 10, then there exists an L-layer transformer TFθ
withM= 3heads and hidden dimension Dhid, where
L=⌈3κlog(BxBw/(2ε))⌉+ 2, (16)
18

--- PAGE 19 ---
such that the following holds. On any input matrix Hof form (1) such that problem (13) has bounded inputs
and solution: for all i∈[N]
∥xi∥2≤Bx,|yi| ≤By,bwλ
i
2≤Bw/2, (17)
TFθapproximately implements the ridge regression algorithm (minimizer of risk (13)) at every token i∈[N]:
The prediction byi:= [TF θ(H)]d+1,2i−1satisfies
byi−
bwλ
i,xi≤ε. (18)
Proof.The proof consists of two steps.
Step 1 We analyze the convergence rate of gradient descent on bLλ
i−1simultaneously for all 2≤i≤N,
each with learning rate ηi=i−1
2i−1ηas implemented in Proposition B.4.
Fix2≤i≤N. Consider the ridge risk bLλ
i−1defined in (13), which is a convex quadratic function that is λ-
strongly convex and λmax 
X⊤
i−1Xi−1/(i−1)
+λ≤B2
x+λ=:βsmooth over Rd. Recall κ=β/λ= 1+ B2
x/λ.
Consider the following gradient descent algorithm on bLλ
i−1: Initialize w0
i:=0, and for every t≥0
wt+1
i=wt
i−ηi∇bLλ
i−1(wt
i), (19)
with ηi=i−1
2i−1η. Taking η:= 2/β, we have ηi∈[2/(3β),1/β], and thus ηiλ∈[2/(3κ),1/κ].
By standard convergence results for strongly convex and smooth functions (Proposition A.1), we have for
allt≥1that
wt
i−bwλ
i2
2≤exp (−ηiλt)w0
i−bwλ
i2
2= exp ( −ηiλt)bwλ
i2
2.
Further, taking the number of steps as
T:=
3κlogBxBw
2ε
so that ηiλT/2≥2/(3κ)·3κlog(BxBw/(2ε))/2 = log( BxBw/(2ε)), we have
wT
i−bwλ
i
2≤exp (−ηiλT/2)bwλ
i
2≤2ε
BxBw·Bw
2≤ε
Bx. (20)
Step 2 We construct a (T+2)-layer transformer TFθby concatenating the copying layer in Lemma B.1, T
identicalgradientdescentlayersasconstructedinPropositionB.4,andthelinearpredictionlayerinLemmaB.2.
Note that the transformer is attention only (all MLP layers being zero), and the number of heads within all
layers is at most 3.
The copying layer ensures that the output format is compatible with the input format required in Propo-
sition B.4, which in turn ensures that the Tgradient descent layers implement (19) simultaneously for all
1≤i≤N(wT
1:=0is not updated at token i= 1). Therefore, the final linear prediction layer ensures that,
the output matrix eH:= TF θ(H)contains the following prediction at every i∈[N]:
byi:= [ehx
i]d+1=
wT
i,xi
,
which satisfies
byi−
bwλ
i,xi=
wT
i−bwλ
i,xi≤(ε/Bx)·Bx=ε.
This finishes the proof.
19

--- PAGE 20 ---
B.3 Proof of Theorem 1
The result follows directly by concatenating the following two transformer constructions:
•The MLP implementation module in Lemma B.3, which has (L+1)-layers, 5 heads, and transforms every
xitoΦ⋆(xi)to give output matrix (4);
•The in-context ridge regression module in Theorem B.5 (with inputs being {Φ⋆(xi)}instead of xi) which
hasO(κlog(BΦBw/ε))layers, 3heads,andoutputsprediction byi:= [ehx
i]D+1where |byi−⟨Φ⋆(xi),bwΦ⋆,λ
i⟩| ≤
ε, where bwΦ⋆,λ
iis the ( Φ⋆-Ridge) predictor.
Claim (4) can be seen by concatenating the (L+ 1)-layer MLP module with the first layer in the ridge
regression module (Theorem B.5), which copies the Φ⋆(xi)in each xtoken to the same location in the
succeeding ytoken.
Further, the hidden dimension requirements are Dhid≥max{2D, D +d+ 10}for the first module and
Dhid≥2D+ 10for the second module, which is satisfied at our precondition Dhid= 2D+d+ 10. This
finishes the proof.
C Proofs for Section 4.2
Recall our input format (6) for the dynamical system setting:
H:=x1. . .xN
p1. . .pN
∈RDhid×N,
our choice of the positional encoding vectors pi= [0Dhid−d−4; 1;i;i2;i3]for all i∈[N], and that we under-
stand xi:=0for all i≤0.
C.1 Useful transformer constructions
Lemma C.1 (Copying for dynamical systems) .Suppose Dhid≥kd+ 4. For any k∈[N], there exists a
(k+ 1)-head attention layer θ={(Qm,Km,Vm)}m∈[k+1]⊂RDhid×Dhidsuch that for every input Hof the
form (6), we have
eH= Attn θ(H) =
x1−k+1. . .xi−k+1. . .xN−k+1
| | |
x1 . . . xi . . . xN
p1 . . . pi . . . pN
∈RDhid×N, (21)
where pionly differs from piin the dimension of the zero paddings. In words, Attn θcopies the k−1previous
tokens [xi−k+1;. . .;xi−1]onto the i-th token.
Proof.For every k′∈[k], we define an attention head (Qk′,Kk′,Vk′)⊂RDhid×Dhidsuch that for all j≤i∈
[N],
Qk′hi= [i3;i2;i;0Dhid−3],
Kk′hj= [−1; 2j+ 2(k′−1);−j2+ 2(k′−1)j+ 1−(k′−1′)2;0Dhid−3],
Vk′hj= [0(k−k′)D;xj;0].
Note that
σ(⟨Qk′hi,Kk′hj⟩) =σ 
−i3+ 2i2j+ 2(k′−1)i2−ij2+ 2ij(k′−1) +i−i(k′−1)2
=iσ 
1−(j−i+k′−1)2
=i1{j=i−k′+ 1}.
20

--- PAGE 21 ---
Therefore, at output token i∈[N], this attention head gives
1
iiX
j=1σ(⟨Qk′hi,Kk′hj⟩)Vk′hj=1
i·i·Vk′hi−k′+1= [0(k−k′)D;xi−k′+1;0]
when i−k′+1≥1,andzerootherwise. Combiningall kheads,anddefiningonemorehead (Qk+1,Kk+1,Vk+1)
to “remove” xiat its original location (similar as in the proof of Lemma B.3), we have
k+1X
m=11
iiX
j=1σ(⟨Qk′hi,Kk′hj⟩)Vk′hj=
xi−k+1−xi
xi−(k−1)+1
|
xi
0
.
By the residual structure of an attention layer, we have
[Attn θ(H)]i=xi
pi
+
xi−k+1−xi
xi−(k−1)+1
|
xi
0
=
xi−k+1
xi−(k−1)+1
|
xi
pi
.
(The precondition Dhid≥D+ 4guarantees that the xentries would not interfere with the non-zero entries
within pi.) This is the desired result.
Lemma C.2 (Implementing MLP representation for dynamical systems) .Fix any MLP representation
function Φ⋆:Rkd→RDof the form (3), suppose Dhid≥2(k+1)d+3D+2d+5. Then there exists a module
MLP-(Attention-MLP- . . .-Attention-MLP) with L+ 1(Attention-MLP) blocks (i.e. transformer layers) and
5heads in each attention layer (this is equivalent to an (L+2)-layer transformer without the initial attention
layer) that implements Φ⋆in the following fashion: For any input Hof form
H=x1. . .xN
p1. . .pN
where we recall xi= [xi−k+1;. . .;xi]∈Rkd, the following holds. The first MLP layer outputs
MLP(1)(H) =
σρ(B⋆
1x1). . . σ ρ(B⋆
1xi)
x1 . . . xi
p′
1 . . . p′
i
.
The full transformer outputs
eH= TF θ(H) =
Φ⋆(x1) Φ⋆(x2). . . Φ⋆(xi)
0d 0d . . . 0d
0D Φ⋆(x1). . . Φ⋆(xi−1)
x1 x2 . . . xi
ep1 ep2 . . . epi
. (22)
whereepi,epidiffers from pi,pionly in the dimension of their zero paddings.
Proof.We first construct the first MLP layer. Consider any input token hi= [xi;pi]. Define matrices
W1,W2∈RDhid×Dhidsuch that (below ±u:= [u;−u])
W1hi=
±B⋆
1xi
±xi
±xi
0
, σ(W1hi) =
σ(±B⋆
1xi)
σ(±xi)
σ(±xi)
0
,
21

--- PAGE 22 ---
W2σ(W1hi) =σ(B⋆
1xi)−ρσ(−B⋆
1xi)
0
+−σ(xi) +σ(−xi)
0
+
0D
σ(xi)−σ(−xi)
0
.
Therefore, the MLP layer (W1,W2)outputs
hi:= [MLP W1,W2(H)]i=hi+W2σ(W1hi) =
σρ(B⋆
1xi)
xi
pi
. (23)
The requirement for Dhidabove is Dhid≥max{2D+ 2(k+ 1)d, D+d+ 5}.
The rest of the proof follows by repeating the proof of Lemma B.3 (skipping the first (MLP-Attention)
block), with the following modifications:
•Save the xi∈xilocation within each token, and move it into the (2D+d+ 1 : 2 D+ 2d)block in the
final layer (instead of moving the label yiin Lemma B.3); this takes the same number (at most 2) of
attention heads in every layer, same as in Lemma B.3.
•Append one more copying layer with a single attention head (similar as the construction in Lemma C.1)
to copy each Φ⋆(xi) to the (D+d+ 1 : 2 D+d)block of the next token.
The above module has structure (L−1)×(MLP-Attention), followed by a single attention layer which can
be rewritten as an MLP-Attention-MLP module with identity MLP layers. Altogether, the module has
an MLP- L×(Attention-MLP) structure. The max number of attention heads within the above module is
5. The required hidden dimension here is Dhid≥max{kd+ 4,2D+d+ max {D, d}+ 5}, with Dhid≥
max{kd,3D+ 2d}+ 5being a sufficient condition.
Combining the above two parts, a sufficient condition for DhidisDhid≥2(k+ 1)d+ 3D+ 2d+ 5, as assumed
in the precondition. This finishes the proof.
Consider the following multi-output ridge regression problem:
cWλ
i:= arg min
W∈RD×d1
2(i−1)i−1X
j=1W⊤xj−yj2
2+λ
2∥W∥2
Fr. (24)
Theorem C.3 (In-context multi-output ridge regression with alternative input structure) .For any λ≥0,
Bx, Bw, By>0with κ:= 1 + B2
x/λ, and ε < B xBw/2, letDhid≥Dd+ 2(D+d) + 5, then there exists an
L-layer transformer TFθwithM= 3dheads and hidden dimension Dhid, where
L=O(κlog(BxBw/(ε))) (25)
such that the following holds. On any input matrix
H=
x1x2. . . xN
0d0d. . . 0d
0Dx1. . .xN−1
0dy1. . .yN−1
p1p2. . . pN

(where xi∈RD,yi∈Rd) such that problem (24) has bounded inputs and solution: for all i∈[N]
∥xi∥2≤Bx,∥yi∥∞≤By,∥cWλ
i∥2,∞≤Bw/2, (26)
TFθapproximately implements the ridge regression algorithm (24) at every token i∈[N]: The prediction
byi:= [TF θ(H)](D+1):( D+d),isatisfies
byi−(cWλ
i)⊤xi
∞≤ε. (27)
22

--- PAGE 23 ---
Proof.Observe that the multi-output ridge regression problem (24) is equivalent to dseparable single-output
ridge regression problems, one for each output dimension. Therefore, the proof follows by directly repeating
the same analysis as in Theorem B.5, with the adaptation that
•Omit the copying layer since each token already admits the previous (input, label) pair;
•UseaO(κlog(BxBw/(ε)))-layertransformerwith 3dheadstoperform dparallelridgeregressionproblems
(each with 3heads), using in-context gradient descent (Proposition B.4) as the internal optimization
algorithm, and with slightly different input structures that can be still accommodated by using relu to
implement the indicators. Further, by the precondition (26) and Dhid−2(D+d)−5≥Dd, we have
enough empty space to store the Wt
i∈RD×dwithin the zero-paddings in pi.
•Use a single-attention layer with dparallel linear prediction heads (Lemma B.2), one for each j∈[d], to
write prediction (byi)jinto location (i, D+j)with|(byi)j− ⟨(cWλ
i)j,xi⟩| ≤ε. Therefore,
byi−(cWλ
i)⊤xi
∞= max
j∈[d](byi)j−D
(cWλ
i)j,xiE≤ε.
This finishes the proof.
C.2 Proof of Theorem 2
Proof of Theorem 2. The proof is similar as that of Theorem 1. The result follows directly by concatenating
the following three transformer modules:
•The copying layer in Lemma C.1, which transforms the input to format (21), and thus verifies claim (7).
•The MLP representation module in Lemma C.2, which transforms (21) to (22). Together with the above
single attention layer, the module is now an (L+ 1)-layer transformer with 5heads. Claim (8) follows by
the intermediate output (23) within the proof of Lemma C.2.
•The in-context multi-output ridge regression construction in Theorem C.3 (with inputs being {Φ⋆(xi)}
and labels being {xi+1}). This TF has O(κlog(BΦBw/ε))layers, and 3dheads. It takes in input of
format (22), and outputs prediction byi:= [ehi]D+1:D+dwhere ∥byi−(cWΦ⋆,λ
i)⊤Φ⋆(xi)∥∞≤ε, where
cWΦ⋆,λ
iis the ( Φ⋆-Ridge-Dyn) predictor.
The resulting transformer has max{3d,5}heads, and the hidden dimension requirement is
Dhid≥max{kd+ 5,2(k+ 1)d+ 3D+ 2d+ 5, Dd+ 2(D+d) + 5}.
A sufficient condition is Dhid= max {2(k+ 1), D}d+ 3(D+d) + 5, as assumed in the precondition. This
finishes the proof.
D Details for experiments
Architecture and training details We train a 12-layer decoder model in GPT-2family with 8 heads and
hidden dimension Dhid= 256, with positional encoding. We use linear read-in and read-out layer before and
after the transformers respectively, both applying a same affine transform to all tokens in the sequence and
are trainable. The read-in layer maps any input vector to a Dhid-dimensional hidden state, and the read-out
layer maps a Dhid-dimensional hidden state to a 1-dimensional scalar for model (2) and to a d-dimensional
scalar for model (5).
Under the in-context learning with representation setting, we first generate and fix the representation Φ⋆.
For a single ICL instance, We generate new coefficients wandNtraining examples {(xi, yi)}i∈[N]and test
input (xN+1, yN+1). Before feeding into transformer, we re-format the sequence to HICL−rep, as shown in
equation (28).
HICL−rep=
x1,
y1
0d−1
, . . . ,xN,
yN
0d−1
∈Rd×2N(28)
23

--- PAGE 24 ---
We use the use the Adam optimizer with a fixed learning rate 10−4, which works well for all experiments.
We train the model for 300Ksteps, where each step consists of a (fresh) minibatch with batch size 64for
single representation experiments, except for the mixture settings in Appendix E where we train for 150K
iterations, each containing Kbatches one for each task.
Under ICL dynamic system setting, for a single ICL instance, we don’t need to reformat the input sequence.
We feed the original sequence HDynamic = [x1, . . . ,xN]∈Rd×Nto transformer.
All our plots show one-standard-deviation error bars, though some of those are not too visible.
D.1 Details for linear probing
Denote the ℓ−th hidden state of transformers as
H(ℓ)=h
hx,(ℓ)
1,hy,(ℓ)
1, . . . ,hx,(ℓ)
N,hy,(ℓ)
Ni
∈RDhid,2Nfor ℓ∈[12].
Denote the probing target as g({xj, yj}j∈[i])∈Rdprobefori∈[N]. Denote the linear probing parameter as
wx,(ℓ)andwy,(ℓ)that belong to RDhid×dprobe. Denote the best linear probing model as
wx,(ℓ)
⋆ = arg min
wx,(ℓ)EhNX
i=1n 
wx,ℓ⊤hx,(ℓ)
i−g
{xj, yj}j∈[i]o2i
and
wy,(ℓ)
⋆ = min
wy,(ℓ)EhNX
i=1n 
wy,ℓ⊤hy,(ℓ)
i−g
{xj, yj}j∈[i]o2i
.
To find them, we generate 2560ICL input sequences with length N, and obtain 12hidden states for each
input sequences. We leave 256sequences as test sample and use the remaining samples to estimate wx,(ℓ)
⋆
andwy,(ℓ)
⋆for each ℓwith ordinary least squares. We use the mean squared error to measure the probe
errors. In specific, define
Probe Errorx,(ℓ)
i(g) =Ehn 
wx,ℓ
⋆⊤hx,(ℓ)
i−g
{xj, yj}j∈[i]o2i
with
Probe Errorx,(ℓ)(g) =1
NNX
i=1Probe Errorx,(ℓ)
i(g),and
Probe Errory,(ℓ)
i(g) =Ehn 
wx,ℓ
⋆⊤hx,(ℓ)
i−g
{xj, yj}j∈[i]o2i
with
Probe Errory,(ℓ)(g) =1
NNX
i=1Probe Errory,(ℓ)
i(g).
When ℓ= 0, we let hx,(0)
i=hy,(0)
i=xias a control to the probe errors in the hidden layer. We normalize
each probe error with E[∥g(x, y)∥2
2]/dprobe. We use the 256 leaved-out samples to estimate these errors. We
replicate the above procedure for three times and take their mean to get the final probe errors.
D.2 Details for pasting
From the single fixed representation settings above, we pick a trained transformer trained on the represen-
tation with D=d= 20to avoid dimension mismatch between Φ⋆(x)andx. We choose L= 3and noise
level σ= 0.1.
We change the data generating procedure of yfrom Equation (2) to
yi=⟨w,xi⟩+σzi, i∈[N], (29)
which corresponds to a linear-ICL task. According to the results of probing Fig 3a, we conjecture that
transformer use the first 4layers to recover the representation, and implement in-context learning through
the5-th to the last layers. Therefore, we extract the 5−12layers as the transformer upper layers. Then
paste them with three kinds of embeddings:
24

--- PAGE 25 ---
1.Linearembedding W∈RDhid×(D+1)with re-formatted input HLinear:
HLinear =x1
0
,0D
y1
, . . . ,xN
0
,0D
yN
∈RD+1×2N
2.Linear copy embedding W∈RDhid×(D+1)with re-formatted input Hcopythat copies xitoyitokens in
advance:
Hcopy=x1
0
,x1
y1
, . . . ,xN
0
,xN
yN
∈RD+1×2N
3.Transformer embedding TFusing the same input format HICL−repwith normal settings, as shown in
(28). We extract the 4-th layer of the GPT-2model, its a complete transformer block with trainable
layer norm. We use a linear read-in matrix to map HICL−repto the Dhid-dimension hidden state, apply
one block of transformer to it to get the TF embedding H=TF(H).
Weapplytheupperlayerstothethreeembeddings, thenusetheoriginalread-outmatrixtogettheprediction
ofbyi. For comparison, we also train a one-layer transformer using the input sequence HICL−rep.
We use the same training objective as in (10). In the retraining process, we switch to task (29), fix the
parameters of upper layers of the transformer, and only retrain the embedding model. The training methods
are exact the same with the original transformer. We also find that using a random initialized transformer
block or extracting the 4-th layer of the transformer don’t make difference to the results.
D.3 Difficulty of linear ICL with a single-layer transformer with specific input
format
Recall the input format (1):
H=
x10. . .xN0
0y1. . . 0yN
px
1py
1. . .px
Npy
N
∈RDhid×2N.
Here we heuristically argue that a single attention layer alone (the only part in a single-layer transformer that
handles interaction across tokens) is unlikely to achieve good linear ICL performance on input format (1).
Consider a single attention head (Q,K,V). As we wish the transformer to do ICL prediction at every token,
the linear estimator wiused to predict byiis likely best stored in the xitoken (the only token that can attend
to all past data Di−1and the current input xi). In this case, the attention layer needs to use the following
(key, value) vectors to compute a good estimator wifrom the data Di−1:
{Vhx
j,Vhx
j}j∈[i],{Vhy
j,Vhy
j}j∈[i−1].
However (apart from position information), hx
jonly contains xj, and hy
jonly contains yj. Therefore, using
the normalized ReLU activation as in Appendix B & C.2, it is unlikely that an attention layer can implement
even simple ICL algorithms such as one step of gradient descent (von Oswald et al., 2022; Akyürek et al.,
2022):
wi=w0
i−η1
i−1X
j≤i−1 
w0
i,xj
−yj
xj,
which (importantly) involves term −yjxjthat is unlikely to be implementable by the above attention, where
each attention head at each key token can observe either xjoryjbut not both.
25

--- PAGE 26 ---
E Experiments on mixture of multiple representations
We train transformers on a mixture of multiple ICL tasks, where each task admits a different representation
function. This setting can be seen as a representation selection problem similar as the “algorithm selection”
setting of Bai et al. (2023). In specific, let K≥2denote the number of tasks. Given j, let
yi=
w,Φ⋆
j(xi)
+σzi, z i∼N(0,1), i∈[N],where
Φ⋆
j(x) =σ⋆
B⋆,(j)
Lσ⋆,(j)
B⋆
L−1···σ⋆,(j)
B⋆,(j)
1x
···
,B⋆,(j)
1∈RD×d,(B⋆,(j)
ℓ)L
ℓ=2⊂RD×D.
The generating distributions for w,{xi}i∈[N], and {B⋆,(j)
L}are same with previous setting. We generate
different Φ⋆
jforj∈[K]independently. We choose K∈ {3,6},σ∈ {0,0.1,0.5},L= 3, and noise σ∈
{0,0.1,0.5}.
At each training step, we generate Kindependent minibatches, with the j−th minimatch takes the repre-
sentation Φ⋆
jto generate {yi}i∈[N]. Due to multiple minibatches, we shorten the number of total training
steps to 150K. The other training details are the same with fixed single representation setting.
(a) Risk for K= 3
0 10 20 30 40
in-context examples0.00.20.40.60.81.0square loss
 (b) Risk for K= 6
0 10 20 30 40
in-context examples0.00.20.40.60.81.0square loss
noise = 0.1
noise = 0.5
Transformer
Baseline Bayes optimal
Oracle Φ⋆-ridge
Figure 6: ICL risks for multiple representations setting. Dotted lines plot two baseline risks. (a)The transformer
withlowerrisksistrainedwith (K, L, D, σ ) = (3 ,3,20,0.1). Theupperoneistrainedwith (K, L, D, σ ) = (3 ,3,20,0.5).
(b)The two transformers are trained with K= 6and same settings otherwise.
ICLperformance Wechooseonerepresentation Φ⋆
1fromtherepresentationsthattransformersaretrained
on. Figure6a&Figure6breportthetestrisk. Wevary K∈ {3,6}andnoiselevel σ∈ {0.1,0.5}. Weconsider
two baseline models.
1.The Bayes optimal algorithm : Note that the training distribution follows the Bayesian hierarchical
model:
j∼Unif([K]),xi∼N(0,Id),w∼N(0, τ2Id),and yi|xi, j,w∼N(⟨w,xi⟩, σ2).
This gives the Bayes optimal predictor
byi=KX
j=1η(j)
iby(j)
i,with (. . . , η(j)
i, . . .) =Softmaxnh
. . . ,iX
k=1(yk−by(j)
k)2/σ2, . . .io
(30)
withby(j)
ibeing ridge predictor with optimal λbased on 
Φ⋆
j(xr), yr	
r∈[i−1].
2.The oracle ridge algorithm: We use the ridge predictor by(1)
ibased on {(Φ⋆
1(xr), yr)}r∈[i−1], which is the
representation for test distribution. Note that this is an (improper) algorithm that relies on knowledge
of the ground truth task.
26

--- PAGE 27 ---
Comparable to those trained on single fixed representation, transformers consistently match the Bayes-
optimal ridge predictor. As expected, the oracle ridge algorithm is better than transformers and the Bayes
optimal algorithm and transformers. Increasing number of tasks Kcan slightly increase this gap. Increasing
the noise level has the same effect on transformers and baseline algorithms.
(a) Probe Φ⋆
1(xi)
024681012
layer10−1mean squared error
x tokens
y tokens
Φ⋆
1(xi)
Φ⋆
2(xi)
Φ⋆
3(xi) (b) Concatenated by(j)
iandη(j)
i
024681012
layer10−1100mean squared error
x tokens
y tokens
concatenated predictions
bayes weights(c) Last layer probe by(j)
i
T askRep function0.01 0.19 0.19
0.15 0.01 0.16
0.13 0.12 0.01
Figure 7: Probing errors for transformer trained with (K, L, D, σ ) = (3 ,3,20,0.1). Dotted lines plot probing errors
onytokens.
(a) Probe Φ⋆
1(xi)
024681012
layer10−210−1mean squared error
x tokens
y tokens
Φ⋆
1(xi)
Φ⋆
2(xi)
Φ⋆
3(xi) (b) Concatenated by(j)
iandη(j)
i
024681012
layer10−1mean squared error
x tokens
y tokens
concatenated predictions
bayes weights(c) Last layer probe by(j)
i
T askRep function0.01 0.03 0.03
0.02 0.01 0.02
0.02 0.02 0.01
Figure 8: Probing errors for transformer trained with σ= 0.5
Probesetup: Similartosinglefixedrepresentationsetting, weconductlinearprobingexperiments. Weare
wonderingtransformerimplementstheICL-learningonrepresentationswithalgorithmselectionsmechanism.
Weidentifythreesetsofprobingtargets: Φ⋆(xi),by(j)andη(j)
i. Allofthemareintermediatevaluestocompute
the Bayes optimal estimator (30). We generate different data for different probing targets:
1. To probe Φ⋆(xi)andby(j)for each j, we choose one representation from the representations that trans-
formers are trained on, then train and test our linear probing model. This is consistent with the training
and testing methods for probing transformers trained on a single representation.
2. To probe choose concatenated probing targets Yi= [by(1)
i, . . . ,by(K)
i]andBi= [η(1)
i, . . . , η(K)
i], we gener-
ate2560in-context sequences for each representation, and obtain 2560×Ksamples together. We use
ordinary linear square on 2560×K−256samples to get the linear probing models. Then test them on
the remaining 256samples to get the probing errors. We also repeat this process for three times and
take means to get the final probing errors.
Probe representations: Take the transformer trained on K= 3mixture representations with noise level
σ∈ {0.1,0.5}. Figure 7 show the probing errors for σ= 0.1: Figure 7a reports the errors of probing
Φ⋆
jj∈[3], with probing models trained on task Φ⋆
1. Echoing the results for transformers trained on
single representation, the probing errors for each representations decrease through lower layers and increase
through upper layers on xtokens. The probing errors on ytokens drop after xtokens, which suggests a
copy mechanism. Surprisingly, on x-tokens, the probing errors for all representations attain their minimum
27

--- PAGE 28 ---
at the 3-th layer, with transformers trained on single representation achieving their minimum on 4-th layer
(compare with Figure 3a).
More importantly, for both xandytokens, the probing errors for each representation are similar through
lower layers, but the probing errors for the true representation Φ⋆
1become the lowest through the upper
layers. The gap between the probing errors increases. At the last layer, the probing error for the other
representations go up to match the initial input.
Probe intermediate values for computing Bayes optimal predictor: Figure 7b shows the probing
errors for concatenated ridge predictors by(j)
iand Bayes weights η(j)
i, i.e., YiandBi. The probing errors
forYistart dropping at the 4−th layer, which suggest that transformer are implementing ICL using each
representations. Probing errors for Bihave a sudden drop at the 10−th layer. Figure 7c shows the probing
errors for probing by(j)
i. At(j, k)-th cell, we show the probing error of by(j)
iwith probing models trained on Φ⋆
k
at the xtokens of the last layer. The diagonal elements dominant. The results combined together suggest the
possibility that transformer compute in-context learning with three representations and implement algorithm
selections at the 10−th layer to drop some predictions.
In comparison, Figure 8 shows results of probing the same targets for transformer under σ= 0.5. Figure 8a
differs with Figure 7b at upper layers, where probing errors for different representations don’t have significant
gaps. Figure 8b is close to Figure 7b, also suggesting the algorithm selection mechanism. Figure 8c shows
that the last layer encodes the information of all ridge predictorsn
by(j)
io
, which is drastically different from
the results in Figure 7c.
Conjecture on two different algorithm selection mechanisms: Based on the empirical findings, we
conjecture two possible mechanisms of algorithm selection in transformer: (1) For small noise level data,
transformers implement “concurrent-ICL algorithm selection”, which means they concurrently implement
ICL with algorithm selection, then stop implementing the full ICL procedure for algorithms that not are not
likely to have good performance. (2) For large noise level data, transformers “post-ICL algorithm selection”,
which means they first implement ICL using each algorithm, then select and output the best one. However,
we need further experimental and theoretical to inspect this conjecture.
F Ablations
F.1 Supervised learning with representation
Probing results along training trajectory Figure 9a, Figure 9b, and Figure 9c show the probing error
forΦ⋆(xi)atxandytokens and byΦ⋆ridgeatxtokens. As expected, all probe errors reduce through training
steps, showingthattheprogressoflearning Φ⋆isconsistentwiththeprogressofthetrainingloss. Atthe2000
training steps, transformer cannot recover the representation. At the 5000 training steps, the transformer
starts memorizing the representation, starting showing differences between lower and upper layers. From
5000 training steps to 10000, the trend of probe errors varying with layers remains the same.
Additional results for probing and pasting Figure 10a plots the same probing errors as in Figure 3a
with (L, D, σ ) = (3 ,20,0.1)(the green line there), except that we separate the errors of the first 4 tokens
with the rest (token 5-41), but the probing training remains the same (pooled across all tokens). We observe
that lower layers compute the representation in pretty much the same ways, though later layers forget the
representations more for the beginning tokens (1-4) than the rest tokens.
Figure 10b plots the same pasting experiment as in Figure 4b, except that for noise level σ= 0.5as opposed
toσ= 0.1therein. The message is mostly the same as in Figure 4b.
28

--- PAGE 29 ---
(a) 2000 training steps
0 2 4 6 810 12
layer10−1mean squared error
Φ⋆(xi) at x tokens
Φ⋆(xi) at y tokens
̂yΦ⋆ridge
i at x tokens (b) 5000 training steps
0 2 4 6 810 12
layer10−1mean squared error
Φ⋆(xi) at x tokens
Φ⋆(xi) at y tokens
̂yΦ⋆ridge
i at x tokens (c) 10000 training steps
0 2 4 6 810 12
layer10−1mean squared error
Φ⋆(xi) at x tokens
Φ⋆(xi) at y tokens
̂yΦ⋆ridge
i at x tokens
Figure 9: Probing errors for transformer trained after 2000, 5000, and 10000 steps. All three plots are for the training
run on (L, D, σ ) = (2 ,10,0.1).
(a) Probe errors per token
0 2 4 6 8 10 12
layer10−210−1probe error
 point 1
 point 2
 point 3
 point 4
 points 5-41 (b) Pasting experiment
0 10 20 30 40
in-context examples0.20.40.60.81.01.21.4square loss
TF_upper+1layer_TF_embed
TF_upper+linear_copy_embed
TF_upper+linear_embed
fresh_1layer_TF
optimal_ridge
Figure 10: (a)Probing errors of Φ⋆(xi)inxitokens evaluated per-token. (b)Pasting results for the upper module
of a trained transformer in setting (L, D, σ ) = (3 ,20,0.5).
F.2 Dynamical systems
RiskFigure 11 gives ablation studies for the ICL risk in the dynamical systems setting in Section 4.2. In
all settings, the trained transformer achieves nearly Bayes-optimal risk. Note that the noise appears to have
a larger effect than the hidden dimension, or the number of input tokens.
Probing Figure 12a & 12b gives ablation studies for the probing errors in the dynamical systems setting
in Section 4.2, with D= 20instead of D= 80as in Figure 5b & 5c. The message is largely similar except
that in Figure 12a, all past inputs and intermediate steps in Φ⋆(xi)are simultaneously best implemented
after layer 4.
29

--- PAGE 30 ---
(a) Varying noise level
0 10 20 30 40
in-context examples0.00.20.40.60.81.0mean squared loss
Transformer
opt Φ⋆-ridge
σ=0.1
σ=0.5 (b) Varying rep hidden dim.
0 10 20 30 40
in-context examples0.00.20.40.60.81.0
Transformer
opt Φ⋆-ridge
D=20
D=80 (c) Varying number of input tokens
0 10 20 30 40
in-context examples0.00.20.40.60.81.0
Transformer
opt Φ⋆-ridge
k=2
k=3
k=4
Figure 11: Ablation studies for the risk for Risk for fixed rep setting. Each plot modifies a single problem parameter
from the base setting (k, L, D, σ ) = (3 ,2,20,0.1).
(a) Probe past inputs at xitokens
0 2 4 6 8 10 12
layer10−310−210−1100mean squared error
B⋆
1,0xi
B⋆
1,−1xi−1
B⋆
1,−2xi−2
σ(B⋆
1̄xi)
B⋆
2σ(B⋆
1̄xi)
σ(B⋆
2σ(B⋆
1̄xi)) (b) Probe Φ⋆(xi−j)atxitokens
024681012
layer10−210−1
Φ⋆(̄xi)
Φ⋆(̄xi−1)
Φ⋆(̄xi−2)
Φ⋆(̄xi−3)
Φ⋆(̄xi−4)
Figure 12: Ablation study for the probing errors in the dynamics setting. Here (k, L, D, σ ) = (3 ,2,20,0.5), different
from Figure 5 where D= 80.
30
