# 2206.02770.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2206.02770.pdf
# Kích thước tệp: 5542727 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Học Tương Phản Đa Phương Thức với LIMoE:
Hỗn Hợp Chuyên Gia Ngôn Ngữ-Hình Ảnh
Basil Mustafa, Carlos Riquelme*, Joan Puigcerver*, Rodolphe Jenatton, Neil Houlsby
Google Brain
{basilm, rikel, jpuigcerver, rjenatton, neilhoulsby}@google.com

Tóm tắt
Các mô hình kích hoạt thưa thớt lớn đã đạt được hiệu suất xuất sắc trong nhiều lĩnh vực. Tuy nhiên, các mô hình như vậy thường được huấn luyện trên một phương thức tại một thời điểm. Chúng tôi trình bày Hỗn Hợp Chuyên Gia Ngôn Ngữ-Hình Ảnh, LIMoE, một mô hình hỗn hợp chuyên gia thưa thớt có khả năng học đa phương thức. LIMoE chấp nhận cả hình ảnh và văn bản đồng thời, trong khi được huấn luyện bằng cách sử dụng tổn thất tương phản. MoE là sự phù hợp tự nhiên cho một xương sống đa phương thức, vì các lớp chuyên gia có thể học được sự phân vùng phù hợp của các phương thức. Tuy nhiên, những thách thức mới xuất hiện; đặc biệt là tính ổn định huấn luyện và việc sử dụng chuyên gia cân bằng, mà chúng tôi đề xuất một chương trình chính quy hóa dựa trên entropy. Trên nhiều quy mô, chúng tôi chứng minh sự cải thiện hiệu suất đáng kể so với các mô hình dày đặc có chi phí tính toán tương đương. LIMoE-L/16 được huấn luyện tương đương với CLIP-L/14 đạt 78.6% độ chính xác ImageNet zero-shot (so với 76.2%), và khi được mở rộng thêm lên H/14 (với dữ liệu bổ sung) nó đạt 84.1%, có thể so sánh với các phương pháp tối tân sử dụng xương sống tùy chỉnh lớn hơn cho mỗi phương thức và các chương trình tiền huấn luyện. Chúng tôi phân tích hành vi định lượng và định tính của LIMoE, và chứng minh các hiện tượng như việc xử lý khác biệt của các phương thức và sự xuất hiện tự nhiên của các chuyên gia đặc thù cho phương thức.

1 Giới thiệu
Các mô hình hỗn hợp chuyên gia (MoE) kích hoạt thưa thớt gần đây đã được sử dụng với hiệu quả lớn để mở rộng quy mô cho cả mô hình thị giác [1,2] và mô hình văn bản [3,4]. Động lực chính cho việc sử dụng MoE là mở rộng quy mô tham số mô hình trong khi giữ chi phí tính toán dưới kiểm soát. Tuy nhiên, những mô hình này có những lợi ích khác; ví dụ, tính thưa thớt bảo vệ chống lại việc quên thảm khốc trong học liên tục [5] và có thể cải thiện hiệu suất cho học đa nhiệm vụ [6] bằng cách cung cấp một thiên hướng quy nạp thuận tiện.

Với thành công trong từng lĩnh vực riêng lẻ, và trực quan rằng các mô hình thưa thớt có thể xử lý tốt hơn các nhiệm vụ khác biệt, chúng tôi khám phá việc ứng dụng MoE cho mô hình hóa đa phương thức. Chúng tôi thực hiện bước đầu tiên theo hướng này và nghiên cứu các mô hình xử lý cả hình ảnh và văn bản. Cụ thể, chúng tôi huấn luyện một kiến trúc đa phương thức duy nhất căn chỉnh các biểu diễn hình ảnh và văn bản thông qua học tương phản [7].

Khi sử dụng thiết lập được đề xuất trong các mô hình đơn phương thức trước đây [8,1], chúng tôi thấy rằng việc cung cấp nhiều phương thức cho một kiến trúc duy nhất dẫn đến các chế độ thất bại mới duy nhất đối với MoE. Để vượt qua những điều này, chúng tôi trình bày một tập hợp các bộ chính quy hóa dựa trên entropy để ổn định huấn luyện và cải thiện hiệu suất. Chúng tôi gọi mô hình kết quả là LIMoE (Language-Image MoE).

Chúng tôi huấn luyện một loạt các mô hình LIMoE vượt trội hơn đáng kể so với các đường cơ sở dày đặc phù hợp về tính toán. Chúng tôi mở rộng quy mô này lên một LIMoE-H/14 lớn có 5.6B tham số, áp dụng 675M tham số cho mỗi token. Khi được đánh giá zero-shot [7] trên ImageNet-2012 [9], nó đạt độ chính xác 84.1%, cạnh tranh với các mô hình hai tháp sử dụng tiền huấn luyện và trích xuất đặc trưng đặc thù cho phương thức, và áp dụng 3-4 lần nhiều tham số hơn cho mỗi token.

Các tác giả đóng góp bằng nhau.
Bản thảo. Đang được xem xét.arXiv:2206.02770v1 [cs.CV] 6 Jun 2022

--- TRANG 2 ---
Tra cứu nhúng Nhúng patch SelfAttnMLP
1MLP
2MLP
3MLP
...MLP
N···
Lớp Mã Hóa Dày ĐặcLớp Mã Hóa Dày ĐặcLớp Mã Hóa MoE
Lớp Mã Hóa 
MoE
AvgPool
Định tuyến
zi
zt
✖ ✖
✖ ✖
✖ ✖
✔
✔
✔
Tổn Thất
Tương Phản
Văn Bản Hình Ảnh

Hình 1: LIMoE, một mô hình đa phương thức kích hoạt thưa thớt, xử lý cả hình ảnh và văn bản, sử dụng tính toán có điều kiện để phân bổ token theo cách không phụ thuộc vào phương thức.

Tóm lại, những đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất LIMoE, các mô hình hỗn hợp chuyên gia đa phương thức quy mô lớn đầu tiên.
• Chúng tôi chứng minh chi tiết cách các phương pháp trước đây để chính quy hóa các mô hình hỗn hợp chuyên gia không đáp ứng được yêu cầu cho học đa phương thức, và đề xuất một chương trình chính quy hóa dựa trên entropy mới để ổn định huấn luyện.
• Chúng tôi cho thấy LIMoE tổng quát hóa trên các quy mô kiến trúc, với những cải thiện tương đối trong độ chính xác ImageNet zero-shot từ 7% đến 13% so với các mô hình dày đặc tương đương. Khi mở rộng thêm, LIMoE-H/14 đạt 84.1% độ chính xác ImageNet zero-shot, có thể so sánh với các mô hình tương phản SOTA với xương sống và tiền huấn luyện cho mỗi phương thức.
• Cuối cùng, chúng tôi trình bày các nghiên cứu cắt bỏ và phân tích để hiểu hành vi của mô hình và các quyết định thiết kế của chúng tôi.

2 Hỗn Hợp Chuyên Gia Đa Phương Thức
Học tương phản đa phương thức thường hoạt động với các mã hóa độc lập cho mỗi phương thức [7,10]. Tức là, các mô hình riêng biệt fm được huấn luyện để cung cấp một biểu diễn cuối cùng cho mỗi đầu vào từ phương thức tương ứng, m. Trong trường hợp một số đầu vào hình ảnh và văn bản, i và t, chúng ta có zi=fimage(i) và zt=ftext(t). Đối với học tương phản với hình ảnh và văn bản, phương pháp này tạo ra một kiến trúc "hai tháp", một cho mỗi phương thức. Thay vào đó, chúng tôi nghiên cứu thiết lập một tháp, trong đó một mô hình duy nhất được chia sẻ cho tất cả các phương thức, như được thể hiện trong Hình 1. Thiết kế một tháp cung cấp tính tổng quát và khả năng mở rộng tăng lên, và tiềm năng chuyển giao kiến thức xuyên phương thức và xuyên nhiệm vụ. Tiếp theo, chúng tôi mô tả kiến trúc LIMoE và quy trình huấn luyện.

2.1 Học tương phản đa phương thức
Cho n cặp hình ảnh và chú thích văn bản {(ij,tj)}nj=1, mô hình học các biểu diễn Zn={zij,ztj}nj=1 sao cho những biểu diễn tương ứng với các đầu vào được ghép đôi gần nhau trong không gian đặc trưng hơn so với những biểu diễn của các đầu vào không được ghép đôi. Mục tiêu huấn luyện tương phản [7, 11], với nhiệt độ học T, là:

Lj(Zn) = 1/2 log(e⟨zij,ztj⟩/T / ∑nk=1 e⟨zij,ztk⟩/T) + 1/2 log(e⟨zij,ztj⟩/T / ∑nk=1 e⟨zik,ztj⟩/T)
         |__________________|                    |__________________|
         tổn thất hình ảnh-thành-văn bản       tổn thất văn bản-thành-hình ảnh      (1)

2.2 Kiến trúc LIMoE
Chúng tôi sử dụng một kiến trúc duy nhất dựa trên Transformer cho cả phương thức hình ảnh và văn bản. Mô hình sử dụng một lớp tuyến tính cho mỗi phương thức để chiếu chiều dữ liệu nội tại lên chiều rộng mong muốn: đối với văn bản, mã hóa sentencepiece một-hot tiêu chuẩn và từ vựng đã học [12], và đối với hình ảnh, nhúng dựa trên patch theo kiểu ViT [13]. Sau đó, tất cả token được xử lý bởi một bộ mã hóa transformer được chia sẻ, không được điều kiện hóa một cách rõ ràng theo phương thức. Các biểu diễn token từ lớp cuối cùng được tổng hợp trung bình để tạo ra một vector biểu diễn duy nhất zm cho mỗi phương thức. Để tính toán tổn thất huấn luyện trong (1), các biểu diễn hình ảnh và văn bản được ghép đôi sau đó được chiếu tuyến tính bằng cách sử dụng ma trận trọng số Wm cho mỗi phương thức và Lj được áp dụng cho {(Wimagezik,Wtextztk)}nk=1.

Thiết lập một tháp này có thể được thực hiện với một Transformer dày đặc tiêu chuẩn (và chúng tôi huấn luyện nhiều mô hình như vậy làm đường cơ sở). Tiếp theo, chúng tôi mô tả cách chúng tôi giới thiệu MoE vào thiết lập này cho LIMoE.

Xương sống MoE thưa thớt: Các lớp MoE thưa thớt được giới thiệu theo thiết kế kiến trúc của [1,3]. Các chuyên gia — phần của mô hình được kích hoạt theo cách phụ thuộc vào đầu vào — là các MLP. LIMoE chứa nhiều lớp MoE. Trong những lớp đó, mỗi token x∈RD được xử lý thưa thớt bởi K trong số E chuyên gia có sẵn. Để chọn K nào, một bộ định tuyến nhẹ dự đoán trọng số cổng cho mỗi token:

--- TRANG 3 ---
Chuyên gia 2 (Thực vật)
Chuyên gia 7 (Mắt)
Chuyên gia 19 (Bánh xe)
Chuyên gia 8 (Tay)
Chuyên gia 9 (Kết cấu sọc)
Chuyên gia 17 (Kết cấu đặc)
Chuyên gia 4 (Từ ngữ)
Chuyên gia 18 (Tay nắm cửa)
Chuyên gia 12 (Thức ăn & Hoa quả)
Chuyên gia 6 (Biển & Trời)

Hình 2: Các ví dụ định tuyến token cho Coco. Các ví dụ hình ảnh về cách các patch được định tuyến tại lớp MoE được đặt trong khối mã hóa thứ 18 – tức là giữa mạng – cho mô hình LIMoE-H/14.

g(x) = softmax(Wgx) ∈ RE với Wg ∈ RD×E đã học. Các đầu ra của K chuyên gia được kích hoạt được kết hợp tuyến tính theo trọng số cổng: MoE(x) = ∑Ke=1 g(x)e MLPe(x).

Lưu ý rằng, để đạt hiệu quả tính toán và các ràng buộc thực hiện, các chuyên gia có dung lượng bộ đệm cố định. Số lượng token mà mỗi chuyên gia có thể xử lý được cố định trước và thường giả định rằng các token được phân bổ đều trên các chuyên gia. Nếu dung lượng bị vượt quá, một số token sẽ bị "loại bỏ"; chúng không được xử lý bởi chuyên gia, và đầu ra chuyên gia là tất cả số không cho những token đó. Tỷ lệ token được xử lý thành công (tức là không bị loại bỏ) được gọi là "tỷ lệ thành công". Đây là một chỉ số quan trọng của việc định tuyến lành mạnh và cân bằng và thường chỉ ra tính ổn định huấn luyện.

Chúng tôi phát hiện ra rằng việc định tuyến với token từ nhiều phương thức giới thiệu các chế độ thất bại mới; trong các phần tiếp theo, chúng tôi chứng minh hiện tượng này và mô tả các kỹ thuật của chúng tôi để giải quyết nó.

2.2.1 Những thách thức cho MoE đa phương thức
Như đã đề cập, các chuyên gia có dung lượng bộ đệm cố định. Nếu không can thiệp, Top-K MoE có xu hướng "sụp đổ", do đó chỉ sử dụng một chuyên gia. Điều này khiến hầu hết các token bị loại bỏ và dẫn đến hiệu suất kém [14]. Do đó, các công trình trước đây sử dụng các tổn thất phụ để khuyến khích định tuyến cân bằng [1,3,8].

Trong các thiết lập đa phương thức, những thách thức mới xuất hiện; một là sự mất cân bằng phương thức. Trong các thiết lập thực tế, có khả năng sẽ có nhiều dữ liệu loại này hơn loại khác. Theo đó, chúng tôi không giả định hoặc thực thi dữ liệu cân bằng giữa các phương thức, và các thí nghiệm của chúng tôi có token hình ảnh nhiều gấp 3÷7 lần token văn bản.

Các chuyên gia đặc thù cho phương thức có xu hướng xuất hiện tự nhiên. Trong bối cảnh mất cân bằng này, điều này dẫn đến một kịch bản trong đó tất cả các token từ phương thức thiểu số được gán cho một chuyên gia duy nhất, mà hết dung lượng. Ở mức độ toàn cầu, việc định tuyến vẫn xuất hiện cân bằng: các token từ phương thức đa số được phân bổ đẹp trên các chuyên gia, do đó thỏa mãn các tổn thất phụ không phụ thuộc vào phương thức. Ví dụ, trong thiết lập B/16 tiêu chuẩn của chúng tôi, bộ định tuyến có thể tối ưu hóa tổn thất quan trọng [14] đến trong vòng 0.5% giá trị tối thiểu của nó bằng cách cân bằng hoàn hảo các token hình ảnh nhưng loại bỏ tất cả các token văn bản. Tuy nhiên, điều này dẫn đến huấn luyện không ổn định và các mô hình không hoạt động tốt.

2.2.2 Các tổn thất phụ
Chúng tôi gọi các tổn thất phụ được sử dụng trong V-MoE [1] là các tổn thất phụ cổ điển. Chúng tôi thấy rằng chúng không tạo ra các mô hình MoE đa phương thức ổn định và có hiệu suất. Do đó, chúng tôi giới thiệu hai tổn thất mới: tổn thất entropy cục bộ và tổn thất entropy toàn cục, được áp dụng trên cơ sở từng phương thức. Chúng tôi kết hợp những tổn thất này với các tổn thất cổ điển; xem Phụ lục B để có tóm tắt về tất cả các tổn thất phụ.

Định nghĩa. Trong mỗi lớp MoE, đối với mỗi phương thức m, bộ định tuyến tính toán một ma trận cổng Gm ∈ Rnm×E. Mỗi hàng của Gm đại diện cho phân phối xác suất trên E chuyên gia cho một trong nm token của phương thức đó trong batch. Đối với token x, hàng tương ứng là pm(experts|x) ∈ RE; điều này sau đó quyết định chuyên gia nào xử lý x. Các tổn thất entropy cục bộ và toàn cục được định nghĩa bởi:

Llocal(Gm) := (1/nm) ∑nmi=1 H(pm(experts|xi)) và 
Lglobal(Gm) := H(p̄m(experts)); (2)

trong đó p̄m(experts) = (1/nm) ∑nmi=1 pm(experts|xi) là phân phối xác suất chuyên gia được tính trung bình trên các token và H(p) = -∑Ee=1 pe log(pe) biểu thị entropy. Lưu ý rằng p̄m(experts) ≈ pm(experts) vì chúng tôi xấp xỉ tỷ lệ thật từ các token trong batch. Chúng tôi sử dụng thuật ngữ cục bộ so với toàn cục để nhấn mạnh thực tế là Llocal áp dụng entropy cục bộ cho mỗi token trong khi Lglobal áp dụng entropy toàn cục sau khi đã biên hóa các token.

Tác động của các tổn thất. Hình 3 cho thấy tại sao những tổn thất này là cần thiết. Với các tổn thất mặc định, các chuyên gia đặc thù cho phương thức xuất hiện tự nhiên, nhưng bộ định tuyến thường thay đổi sở thích của nó. Điều này dẫn đến huấn luyện không ổn định và tỷ lệ thành công kém, đặc biệt là đối với phương thức văn bản. Tổn thất entropy cục bộ khuyến khích trọng số bộ định tuyến tập trung (ptext(experts|xi) có entropy thấp), nhưng với cái giá của tính đa dạng của các chuyên gia văn bản: cùng một chuyên gia được sử dụng cho tất cả các token văn bản (tỷ lệ p̄text(experts) cũng có entropy thấp), dẫn đến việc loại bỏ. Trong thiết lập này, nhiều lớp có tỷ lệ thành công văn bản kém.

Để giải quyết điều này, Lglobal khuyến khích tối đa hóa entropy biên, do đó đẩy p̄text(experts) hướng tới phân phối chuyên gia đều hơn. Kết quả là sử dụng chuyên gia đa dạng, định tuyến ổn định và tự tin, và tỷ lệ thành công cao. Đây do đó là các mô hình có hiệu suất nhất.

Một cách trực quan, mong muốn các token văn bản sử dụng nhiều chuyên gia, nhưng không phải tất cả. Để cho phép tính linh hoạt, chúng tôi thiết lập ngưỡng cho tổn thất entropy toàn cục là L'global(Gm) = max{0, α + Lglobal(Gm)}, sao cho mô hình được khuyến khích có một entropy tối thiểu nhất định, nhưng sau khi vượt quá đó, tổn thất không được áp dụng. Điều này tránh sự sụp đổ phân phối nhưng không áp dụng các tiên nghiệm quá hạn chế lên phân phối định tuyến, vì có nhiều giải pháp tối ưu. Điều này có thể được coi như một "tối thiểu mềm" S. Với α = -log(S), mô hình phải sử dụng ít nhất S chuyên gia để tối thiểu hóa tổn thất (hoặc một phân phối đều trên S chuyên gia - với entropy log(S) -, hoặc một phân phối không đều sử dụng nhiều hơn S). Hình 3b cho thấy điều sau xảy ra; tác động thực nghiệm của những ngưỡng này được phân tích trong Phần 4.1.

[Tiếp tục dịch phần còn lại...]

steps0%10%20%30%40%50%i1k 10shotHiệu suất
1: cổ điển
2: cổ điển + ent cục bộ
3: cổ điển + ent cục bộ + toàn cục
0%20%40%i1k 0shot
0 25k 50k 75k 100k
steps0.0%5.0%10.0%15.0%20.0%25.0%coco t2i
(a) Hiệu suất w.r.t. tổn thất phụ.

40%60%80%100%A: Tỷ lệ thành công định tuyến hình ảnh
L1
L7
L1140%60%80%100%B: Tỷ lệ thành công định tuyến văn bản
L1
L7
L11
0%20%40%60%80%100%C: Chuyên môn hóa văn bản lớp 5
40%60%80%100%% thành công định tuyến hình ảnh 40%60%80%100%% thành công định tuyến văn bản
0%20%40%60%80%100%% tất cả token văn bản đến chuyên gia
0 25k 50k 75k 100k
steps40%60%80%100%
0 25k 50k 75k 100k
steps40%60%80%100%
0 25k 50k 75k 100k
steps0%20%40%60%80%100%1: cổ điển 2: cổ điển + ent cục bộ3: cổ điển + ent cục bộ + ent toàn cục

(b) Phân tích hành vi định tuyến của các tổn thất phụ. Cột đầu tiên: Tỷ lệ thành công trung bình của định tuyến hình ảnh trong lớp 1/7/11. Cột thứ hai: Tương tự, cho văn bản. Cột thứ ba: Trong một số chuyên gia của lớp 5, phần trăm nào của tất cả token văn bản đến những chuyên gia đó

Hình 3: Điều gì làm cho các tổn thất entropy cần thiết? Cổ điển đề cập đến công thức tiêu chuẩn (tổn thất quan trọng + tải trọng [1]). Chúng tôi thêm tổn thất entropy cục bộ vào token văn bản (hàng giữa), theo sau là tổn thất entropy toàn cục (hàng dưới). Trái: Thiết lập "cổ điển" có hiệu suất thấp và không ổn định. Phải: Phân tích các entropy cho thấy tại sao: Nếu không có tổn thất cục bộ, mô hình dễ bị thay đổi không ổn định trong sở thích chuyên gia (C1), và tỷ lệ thành công định tuyến thấp (A1, B1). Tổn thất cục bộ khắc phục điều này nhưng gây ra sự sụp đổ phân phối cho một phương thức (C2), với tất cả token văn bản đến một chuyên gia (chuyên gia 11); điều này gây ra tỷ lệ thành công văn bản thậm chí còn kém hơn (B2). Điều này được giải quyết bằng tổn thất toàn cục, có phân bổ chuyên gia ổn định (C3) và tỷ lệ thành công luôn cao (A3, B3).

--- TRANG 4 ---

Kết nối với thông tin tương hỗ. Tổng Llocal(Gm) + L'global(Gm) tương ứng với thông tin tương hỗ (âm) [15] giữa các chuyên gia và token, có điều kiện trên phương thức m, mà chúng tôi viết là MIm(experts; x). Đối với mỗi phương thức được xem xét riêng biệt, chúng tôi đang khuyến khích một cách hiệu quả kiến thức về biểu diễn token để giảm sự không chắc chắn về việc lựa chọn chuyên gia. Chúng tôi cũng đã thử các biến thể khác của các tổn thất khai thác kết nối này, chẳng hạn như thông tin tương hỗ giữa các chuyên gia và phương thức, MI(experts; m), thu được bằng cách biên hóa các token trước.

2.2.3 Định tuyến ưu tiên
Với định tuyến Top-K, việc loại bỏ token hầu như không thể tránh khỏi. Định tuyến Ưu tiên Batch (BPR) [1] chủ động quyết định token nào sẽ bỏ qua dựa trên trọng số định tuyến của chúng. Nó giả định rằng các token có trọng số định tuyến lớn có khả năng mang thông tin và nên được ưu tiên. BPR chủ yếu được sử dụng tại thời điểm suy luận trong [1], cho phép bộ đệm dung lượng chuyên gia nhỏ hơn. Trong thiết lập này, người ta phải cẩn thận không ưu tiên một phương thức hơn phương thức khác một cách có hệ thống, ví dụ, bằng cách xác định token nào sẽ loại bỏ dựa trên thứ hạng của chúng trong batch, thường được nhóm theo phương thức token.

BPR cung cấp hiệu ứng ổn định cần thiết trong quá trình huấn luyện (Hình 6); chúng tôi cho thấy rằng nó không đơn giản xếp hạng một phương thức trên phương thức khác, và nó không thể được thay thế bằng các phương pháp khác để sắp xếp lại batch. Trong phụ lục, chúng tôi tiếp tục cho thấy cách các ưu tiên định tuyến so sánh giữa văn bản và hình ảnh.

3 Thí nghiệm
Chúng tôi nghiên cứu LIMoE trong bối cảnh học tương phản đa phương thức. Trước tiên, chúng tôi thực hiện so sánh có kiểm soát của LIMoE với một Transformer "tiêu chuẩn" dày đặc tương đương, trên một loạt kích thước mô hình. Sau đó, chúng tôi cho thấy khi được mở rộng quy mô, LIMoE có thể đạt mức hiệu suất cao. Cuối cùng, chúng tôi nghiên cứu cắt bỏ các quyết định thiết kế khác nhau dẫn đến LIMoE trong Phần 4.

Dữ liệu huấn luyện. Theo mặc định, tất cả các mô hình được huấn luyện trên dữ liệu hình ảnh-văn bản được ghép đôi được sử dụng trong [16], bao gồm 3.6B hình ảnh và văn bản thay thế được thu thập từ web. Đối với thí nghiệm LIMoE-H/14 lớn, chúng tôi cũng đồng huấn luyện với JFT-4B [17]. Chúng tôi xây dựng chú thích văn bản nhân tạo từ JFT bằng cách nối chuỗi các tên lớp được phân cách bằng dấu phẩy [18]. Phụ lục A chứa đầy đủ chi tiết về thiết lập huấn luyện của chúng tôi.

Đánh giá. Đánh giá chính của chúng tôi là "zero-shot": mô hình sử dụng biểu diễn văn bản của các lớp để đưa ra dự đoán về một nhiệm vụ mới mà không cần dữ liệu huấn luyện bổ sung [19,7]. Chúng tôi tập trung vào độ chính xác phân loại hình ảnh trên ImageNet [9] và truy xuất xuyên phương thức trên MS-COCO [20], theo giao thức trong [16]. Chúng tôi cũng đánh giá biểu diễn hình ảnh của LIMoE thông qua giao thức thích ứng tuyến tính [13], và báo cáo độ chính xác 10-shot trên ImageNet tương ứng. Khi có phạm vi, chúng báo cáo khoảng tin cậy 95% qua ba lần thử.

3.1 Nghiên cứu có kiểm soát trên các quy mô
Chúng tôi huấn luyện một loạt mô hình LIMoE với kích thước batch 16k trong 781k bước. Điều này khớp với số lượng ví dụ huấn luyện được sử dụng cho CLIP [7]. Do việc sử dụng dữ liệu huấn luyện khác nhau và các thủ thuật bổ sung, việc so sánh trực tiếp là khó khăn; do đó chúng tôi huấn luyện các mô hình một tháp dày đặc làm đường cơ sở. Tất cả các mô hình kích hoạt k=1 chuyên gia cho mỗi token, tương tự như Switch Transformer [8].

Hình 4 cho thấy hiệu suất của mỗi mô hình (dày đặc và thưa thớt) so với FLOP truyền tiến (để biết thời gian bước và thảo luận thêm về chi phí tính toán, xem Phụ lục D.2.). Biên Pareto chi phí-hiệu suất cho LIMoE vượt trội hơn các mô hình dày đặc với một khoảng cách lớn, chỉ ra rằng LIMoE mang lại những cải thiện mạnh mẽ trên tất cả các quy mô từ S/32, lên đến L/16. Hiệu ứng đặc biệt lớn trên phân loại ImageNet zero-shot và 10-shot, với cải thiện hiệu suất tuyệt đối 10.1% và 12.2% trung bình. Đối với truy xuất văn bản-thành-hình ảnh trên COCO, LIMoE mang lại một sự thúc đẩy mạnh mẽ ở quy mô nhỏ, trong khi ở quy mô lớn hơn, lợi ích khiêm tốn hơn nhưng vẫn đáng kể.

3.2 Mở rộng quy mô LIMoE
Chúng tôi tăng kích thước kiến trúc, thời gian huấn luyện và kích thước dữ liệu để đánh giá hiệu suất của LIMoE trong chế độ quy mô lớn. Cụ thể, chúng tôi huấn luyện LIMoE-H/14 32 lớp với 12 lớp chuyên gia; chúng được phân phối không đều, với 32 chuyên gia trên mỗi lớp, và K=1 được kích hoạt cho mỗi token. Nó được huấn luyện với kích thước batch 21k, giới thiệu 25% hình ảnh JFT-4B [17] vào mỗi batch (với tên lớp làm văn bản). Chúng tôi tính trung bình các checkpoint về cuối huấn luyện [21]; tham khảo Phụ lục A.3 để biết chi tiết.

--- TRANG 5 ---

101102
FLOPS (×109)50.0%60.0%70.0%80.0%Zero-shot ImageNetZero-shot ImageNet
S/32 S/16 B/32 B/16 L/32 L/16101102
FLOPS (×109)40.0%50.0%60.0%70.0%10-shot ImageNet10-shot ImageNet
LIMoE Dày đặc101102
FLOPS (×109)20.0%25.0%30.0%35.0%40.0%Coco T2I Recall@1Coco T2I Recall@1
Arch: Type:

Hình 4: LIMoE mở rộng quy mô tốt cho các mô hình lớn, với cải thiện hiệu suất nhất quán.

Mô hình chứa 5.6B tham số tổng cộng, nhưng chỉ áp dụng 675M tham số cho mỗi token. Tất cả các bộ định tuyến kết hợp chiếm dưới 0.5M tham số. Bảng 1 cho thấy hiệu suất của nó cùng với các mô hình tương phản tối tân hiện tại. LIMoE đạt 84.1% độ chính xác phân loại ImageNet zero-shot với kích thước kiến trúc khiêm tốn tương đối và số lượng huấn luyện. LIMoE được huấn luyện hoàn toàn từ đầu, mà không có bất kỳ thành phần được huấn luyện trước nào, và là mô hình cạnh tranh đầu tiên với xương sống được chia sẻ.

Trong bối cảnh phương pháp tiếp cận không phụ thuộc vào phương thức, kết quả này đáng ngạc nhiên mạnh mẽ. Các mô hình lớn xử lý hàng chục nhiệm vụ riêng biệt ngày càng phổ biến [22], nhưng chưa tiếp cận tối tân trong những nhiệm vụ này. Chúng tôi tin rằng khả năng xây dựng một mô hình tổng quát với các thành phần chuyên biệt, có thể quyết định cách các phương thức hoặc nhiệm vụ khác nhau nên tương tác, sẽ là chìa khóa để tạo ra các mô hình đa phương thức đa nhiệm vụ thực sự xuất sắc trong mọi việc chúng làm. LIMoE là một bước đầu tiên đầy hứa hẹn theo hướng đó.

Bảng 1: So sánh các mô hình phân loại zero-shot tối tân. Ở quy mô khiêm tốn tương đối, LIMoE-H/14 có thể so sánh với các mô hình hai tháp tốt nhất, và đây là mô hình một tháp có hiệu suất đầu tiên ở quy mô này. T-x đề cập đến Transformer [23] với các tham số tương đương của ViT-x [13].
Chú thích: Được huấn luyện trước PT Ví dụ được thấy trong quá trình huấn luyện trước y Sử dụng FixRes [24] x Mục tiêu huấn luyện khác không tương phản

[THIS IS TABLE: Comparison table showing different architectures, their parameters, and performance metrics across different datasets]

4 Nghiên cứu cắt bỏ
Chúng tôi sử dụng một thiết lập nhỏ hơn để nghiên cứu các khía cạnh khác nhau của LIMoE. Chúng tôi huấn luyện các mô hình B/16 với kích thước batch 8096 trong 100,000 bước (xem Phụ lục A.2 để biết thêm chi tiết). Bảng 2 cho thấy trung bình qua ba lần thử của thiết lập này cùng với các đường cơ sở một tháp dày đặc và hai tháp. LIMoE vượt trội hơn rất nhiều so với cả hai mô hình dày đặc trên ImageNet 0- và 10-shot, trong khi khoảng tin cậy trùng lặp cho truy xuất với hai tháp. Mô hình hai tháp lớn gấp đôi và đắt đỏ hơn, và vẫn thua mô hình thưa thớt.

4.1 Định tuyến và tổn thất phụ
Lựa chọn tổn thất phụ. Với việc giới thiệu các tổn thất dựa trên entropy ngoài các tổn thất cổ điển, có 7 tổn thất phụ có thể có. Chúng tôi nhắm đến tìm sự kết hợp đơn giản nhất của những tổn thất này để có hiệu suất tốt. Để nghiên cứu điều này, chúng tôi thực hiện một cuộc khảo sát lớn về các tổn thất phụ: với N ∈ [2,...,5], chúng tôi xem xét tất cả (7 choose N) kết hợp tổn thất có thể có. Bảng 3 cho thấy, đối với mỗi tổn thất, mô hình có hiệu suất cao nhất có và không có tổn thất đó. Một số kết luận nổi bật: Cả hai tổn thất entropy đều quan trọng đối với văn bản, nhưng đối với hình ảnh, tổn thất toàn cục không có tác động và tổn thất cục bộ là có hại. Sự kết hợp cuối cùng của các tổn thất được chọn dựa trên độ chính xác xác nhận cùng với các quan sát định tính về tính ổn định huấn luyện và tỷ lệ thành công định tuyến.

--- TRANG 6 ---

Bảng 2: Đường cơ sở cho các nghiên cứu cắt bỏ: B/16 với kích thước batch 8096 được huấn luyện trong 100,000 bước.
Cột 0shot và 10shot hiển thị độ chính xác (%), t2i và i2t hiển thị recall@1 (%).

[THIS IS TABLE: Performance comparison table showing different model configurations and their accuracy scores]

Bảng 3: Trên 121 kết hợp, mỗi hàng cho thấy độ chính xác tốt nhất (%) của tất cả các kết hợp đã bao gồm tổn thất phụ (3) so với những kết hợp không bao gồm (7). Tổn thất phụ in đậm chỉ ra chúng có trong LIMoE. Độ chính xác xác nhận là độ chính xác tương phản trung bình trong một minibatch kích thước 1024.

[THIS IS TABLE: Auxiliary loss comparison showing validation accuracy and performance metrics]

Ngưỡng cho tổn thất entropy toàn cục. Trong Phần 2.2.2, chúng tôi đã giới thiệu một ngưỡng để khuyến khích các phân phối chuyên gia cân bằng mà không ép buộc tất cả các phương thức sử dụng tất cả chuyên gia. Để hiểu tầm quan trọng của ngưỡng này, chúng tôi khảo sát nó cho cả tổn thất entropy toàn cục hình ảnh và văn bản. Phụ lục B.2 chứa phân tích đầy đủ; những kết luận quan trọng nhất là:

• αimage không ảnh hưởng đến số lượng chuyên gia được sử dụng cho hình ảnh, vì entropy toàn cục luôn cao. Ngoài những thí nghiệm ngưỡng này với αimage rất cao, tổn thất này thường không hoạt động. Nó được sử dụng trong các thí nghiệm chính của chúng tôi, nhưng có thể được loại bỏ trong công việc tương lai.

• Ngưỡng αtext hoạt động chính xác như một tối thiểu mềm cho các chuyên gia văn bản: Khảo sát αtext, chúng tôi thường quan sát khoảng S = e^(-αtext) chuyên gia văn bản.

• Hiệu suất bền vững với các giá trị αtext khác nhau, miễn là nó không quá thấp. Một αtext thấp có thể hữu ích để giới hạn số lượng chuyên gia văn bản, để cắt tỉa sau này, xem Phụ lục E.4.

Tổn thất phụ thông tin tương hỗ. Trong Phần 2.2.2, chúng tôi đã thảo luận về một tổn thất thay thế, đó là MI(experts; m), dựa trên thông tin tương hỗ giữa các chuyên gia và phương thức. Mặc dù có lợi thế là hợp nhất các tổn thất entropy cục bộ và toàn cục cho cả phương thức văn bản và hình ảnh thành một thuật ngữ duy nhất, không có tham số ngưỡng, nó dẫn đến kết quả hơi tệ hơn: trong một thiết lập tương đương, nó có hiệu suất zero-shot và 10-shot tệ hơn 1.5% và 0.1% so với Bảng 2.

[Continues with more sections...]

0 25k 50k 75k 100k
Các bước huấn luyện0%10%20%30%40%50%60%70%Độ chính xác xác nhận tương phản
cổ điển (nét đứt) so với entropy (nét liền)
Limg=9
img:txt=0.6:1
Limg=16
img:txt=1:1
Limg=49
img:txt=3.1:1
Limg=81
img:txt=5.1:1
Limg=196
img:txt=12.2:1
Limg=324
img:txt=20.2:1

Hình 5: Các tổn thất entropy không chỉ giải quyết sự mất cân bằng phương thức. Với sự cân bằng hình ảnh:văn bản khác nhau, bao gồm cả hoàn toàn cân bằng, các tổn thất entropy cải thiện đáng kể so với thiết lập cổ điển.

Tác động của việc cân bằng phương thức. Các mô hình của chúng tôi sử dụng độ dài chuỗi văn bản là 16, nhưng độ dài chuỗi hình ảnh từ 49 đến 400 (đối với các nghiên cứu cắt bỏ này, 196). Các nghiên cứu cắt bỏ của chúng tôi cho thấy rằng các tổn thất entropy quan trọng nhất khi áp dụng cho các token văn bản. Điều này dẫn đến giả thuyết rằng những tổn thất này chỉ cần thiết hoặc hữu ích trong trường hợp mất cân bằng. Để kiểm tra điều này, chúng tôi thay đổi sự cân bằng phương thức của LIMoE-B/16 bằng cách thay đổi kích thước patch; điều này cho phép chúng tôi kiểm soát số lượng token hình ảnh, và do đó sự cân bằng hình ảnh:văn bản, mà không thay đổi nội dung thông tin trong dữ liệu. Hình 5 cho thấy kết quả. Đầu tiên, chúng tôi quan sát rằng, với định tuyến entropy, độ dài chuỗi hình ảnh dài hơn luôn tốt hơn. Điều này cho thấy định tuyến entropy có thể xử lý hiệu quả các thiết lập mất cân bằng cao, và phản ánh quan sát rằng đối với Vision Transformer cổ điển: chuỗi dài hơn thì tốt hơn. Quan trọng, định tuyến entropy luôn vượt trội hơn rất nhiều so với thiết lập cổ điển với khoảng cách ngày càng tăng, ngay cả khi các phương thức cân bằng 1:1 (Limg = 16). Thí nghiệm này cũng xác nhận tính bền vững của định tuyến entropy đối với các thiết lập khác nhau.

Định tuyến ưu tiên batch như một bộ ổn định huấn luyện. Hình 6 cho thấy tác động của BPR trong quá trình huấn luyện. BPR không chỉ giảm thiểu việc loại bỏ token, mà còn cải thiện tính ổn định huấn luyện. Các mô hình không có can thiệp thứ tự gửi (vào trước-ra trước) hoạt động cực kỳ kém, dù chúng ta định tuyến hình ảnh trước hay văn bản trước. Những bộ định tuyến này có tỷ lệ thành công thấp. Việc trộn ngẫu nhiên các token (tức là quyết định ngẫu nhiên token nào sẽ loại bỏ khi một chuyên gia đầy) một phần cải thiện điều này, nhưng hiệu suất của nó vẫn tệ hơn nhiều so với các mô hình được huấn luyện với BPR. Chúng tôi phân tích thêm BPR trong Phụ lục F.5 và cho thấy rằng nó không đơn giản xếp hạng một phương thức trên phương thức khác.

i1k 0shot i1k 10shot coco t2i coco i2t
metric0%10%20%30%40%50%hiệu suất
0 25k 50k 75k 100k
step0%20%40%60%80%100%% định tuyến thành công Thành công hình ảnh
văn bản trước hình ảnh trước trộn bpr
0 25k 50k 75k 100k
step0%20%40%60%80%100%% định tuyến thành công Thành công văn bản

Hình 6: BPR ổn định huấn luyện và cho phép các mô hình có hiệu suất; biểu đồ đầu tiên cho thấy các thước đo hiệu suất khác nhau. Hai biểu đồ cuối cho thấy tỷ lệ thành công cho bộ định tuyến MoE trong Lớp 9.

4.2 Các nghiên cứu cắt bỏ khác
Chúng tôi tóm tắt các nghiên cứu cắt bỏ khác tại đây do hạn chế về không gian; chi tiết có thể được tìm thấy trong Phụ lục E.

Cấu trúc bộ định tuyến (Phụ lục E.3). Bộ định tuyến của chúng tôi không phụ thuộc vào phương thức; chúng tôi thí nghiệm với các bộ định tuyến cho mỗi phương thức, và các nhóm chuyên gia riêng biệt cho mỗi phương thức. Chúng tôi thấy rằng tất cả đều hoạt động tương đương với thiết lập chung, không phụ thuộc vào phương thức của chúng tôi, nhưng các nhóm chuyên gia riêng biệt theo thiết kế ổn định hơn và không yêu cầu tổn thất phụ để chính quy hóa—trong khi khó mở rộng quy mô cho nhiều phương thức và nhiệm vụ.

Tăng số chuyên gia được chọn cho mỗi token K (Phụ lục E.1). Chúng tôi đề xuất các sửa đổi cho BPR và tổn thất phụ cục bộ để tổng quát hóa cho K > 1; bằng cách làm như vậy, chúng tôi có thể tăng hiệu suất ổn định bằng cách tăng K, ví dụ từ 55.5% độ chính xác zero-shot với K = 1 lên 61.0% với K = 5.

Tổng số chuyên gia (Phụ lục E.2). Chúng tôi cho thấy rằng việc tăng nhóm chuyên gia có sẵn ở K cố định cải thiện hiệu suất (không giống như những gì đã được quan sát đối với các nhiệm vụ chỉ thị giác [1]).

Cắt tỉa chuyên gia (Phụ lục E.4). Chúng tôi cho thấy bằng cách sử dụng các heuristic đơn giản, chúng tôi có thể cắt tỉa xuống thành các chuyên gia đặc thù cho phương thức cho các lần truyền tiến đơn phương thức, do đó tránh sự sụp đổ chuyên gia dưới các batch đơn phương thức.

Huấn luyện trên dữ liệu công cộng (Phụ lục E.6). Phần lớn các mô hình LIMoE được huấn luyện trên dữ liệu độc quyền [16]. Chúng tôi cho thấy rằng LIMoE hoạt động tương tự tốt trên dữ liệu có sẵn công cộng, duy trì cải thiện hiệu suất so với mô hình dày đặc tương đương.

5 Phân tích mô hình
Trong phần này, chúng tôi khám phá một số hoạt động nội bộ của LIMoE. Chúng tôi sử dụng các mô hình B/32 và B/16 đơn giản với 8 chuyên gia, và H/14 lớn với 32. Xem Phụ lục F để biết thêm chi tiết và thí nghiệm.

Các chuyên gia đa phương thức xuất hiện (Phụ lục F.1). Ngoài việc khuyến khích đa dạng, chúng tôi không rõ ràng thực thi các chuyên gia chuyên môn hóa. Tuy nhiên, chúng tôi quan sát sự xuất hiện của cả các chuyên gia đặc thù cho phương thức, và các chuyên gia đa phương thức xử lý cả hình ảnh và văn bản (phân phối từng chuyên gia trong F.1).

--- TRANG 7 ---

Phân tích định tính (Phụ lục F.2). Chúng tôi phân tích một số dữ liệu ví dụ và cho thấy sự xuất hiện rõ ràng của các chuyên gia có ý nghĩa ngữ nghĩa. Với hình ảnh chẳng hạn, một số chuyên gia chuyên môn hóa về các đặc trưng mức thấp (màu sắc, đường nét) trong khi những chuyên gia khác về các đặc trưng phức tạp hơn (khuôn mặt và văn bản), xem Hình 2.

Xếp hạng BPR (Phụ lục F.5). Tổn thất cục bộ khuyến khích trọng số định tuyến tối đa cao cho văn bản, và BPR xếp hạng theo điều này. Tuy nhiên, chúng tôi cho thấy rằng điều này không có nghĩa là văn bản luôn được ưu tiên đầu tiên: Đặc biệt trong các lớp sau, mô hình thường ưu tiên các patch hình ảnh quan trọng hơn văn bản.

6 Công việc liên quan
Các mạng nơ-ron đơn phương thức, đặc thù cho nhiệm vụ đã được nghiên cứu từ lâu, với sự hội tụ ngày càng tăng về các kiến trúc dựa trên Transformer [23,26] cho cả NLP [27] và Thị giác máy tính [13,28,29]. Các mô hình đa phương thức nhằm xử lý nhiều loại dữ liệu bằng cách sử dụng một mạng nơ-ron duy nhất.

Nhiều phương pháp "kết hợp" các phương thức [30–33] để giải quyết các nhiệm vụ đa phương thức vốn có. LIMoE tương tự hơn với các phương pháp không làm điều đó, và vẫn hoạt động như các trích xuất đặc trưng đơn phương thức. Một số đồng huấn luyện trên các nhiệm vụ riêng biệt [34–36,22] mà không căn chỉnh hoặc kết hợp biểu diễn—hiệu quả chia sẻ trọng số qua các nhiệm vụ—trong khi những phương pháp khác bao gồm cả khía cạnh đơn phương thức và khía cạnh đa phương thức kết hợp để có chức năng trong cả hai ngữ cảnh [37].

Chúng tôi xây dựng dựa trên các mô hình Hỗn hợp Chuyên gia thưa thớt sâu, đã được nghiên cứu độc lập trong Thị giác máy tính [1,2] và NLP [14,3,8], thường trong bối cảnh học chuyển giao. Những mô hình này sử dụng cơ chế cổng đã học, trong đó chỉ một tập con K chuyên gia trong số E >> K được kích hoạt cho một đầu vào nhất định. Nhiều công việc nhằm cải thiện chính cơ chế cổng, bằng cách làm cho nó khả vi [38], tái công thức hóa như một nhiệm vụ gán tuyến tính [39] hoặc thậm chí thay thế nó bằng một thuật toán băm đơn giản [40]. Các mô hình MoE cũng đã được nghiên cứu cho học đa nhiệm vụ [38], với các bộ định tuyến cho mỗi nhiệm vụ [6] nhưng một nhóm chuyên gia được chia sẻ. Theo hiểu biết của chúng tôi, các mô hình thưa thớt chưa được khám phá cho học đa phương thức.

Một lượng lớn nghiên cứu tồn tại về học tương phản, thường trong các chế độ tự giám sát [41] nhưng cũng trong các chế độ có giám sát [42]. Học tương phản đa phương thức huấn luyện trên dữ liệu căn chỉnh từ nhiều phương thức. Ban đầu được nghiên cứu cho hình ảnh y tế và báo cáo [11], gần đây nó đã được mở rộng quy mô cho dữ liệu web nhiễu [7,10], nơi việc căn chỉnh hình ảnh-văn bản mạnh mẽ đã cho phép phân loại hình ảnh có hiệu suất và truy xuất hình ảnh-văn bản xuyên phương thức mà không cần tinh chỉnh trên dữ liệu hạ nguồn. Các công việc tiếp theo đã cải thiện điều này đáng kể bằng cách mở rộng quy mô và sử dụng các mô hình được huấn luyện trước [18,16] và huấn luyện đa nhiệm vụ với mô hình hóa sinh [25] hoặc các nhiệm vụ thị giác khác [43]. Những công việc này sử dụng các mô hình đơn phương thức xử lý riêng biệt dữ liệu hình ảnh và văn bản; chúng tôi không biết về nghiên cứu trước đây sử dụng một mô hình duy nhất để xử lý cả hình ảnh và văn bản cho học tương phản, không phải với các mô hình dày đặc cũng không phải với các mô hình thưa thớt.

7 Kết luận và Công việc tương lai
Chúng tôi đã trình bày LIMoE, mô hình hỗn hợp chuyên gia đa phương thức thưa thớt đầu tiên. Chúng tôi đã phát hiện ra các chế độ thất bại mới đặc thù cho thiết lập này và đề xuất các tổn thất phụ dựa trên entropy để ổn định huấn luyện và tạo ra các mô hình có hiệu suất cao. Nó hoạt động trên nhiều quy mô mô hình, với cải thiện trung bình so với các đường cơ sở dày đặc phù hợp về FLOP là +10.2% độ chính xác zero-shot. Khi mở rộng quy mô lên mô hình H/14 lớn, chúng tôi đạt 84.1% độ chính xác, cạnh tranh với các phương pháp SOTA hiện tại.

Tác động xã hội và hạn chế: Những tác hại tiềm tàng của các mô hình quy mô lớn [44], các mô hình tương phản [7] và dữ liệu đa phương thức quy mô web [45] cũng chuyển sang đây, vì LIMoE không giải quyết chúng một cách rõ ràng. Mặt khác, đã được chỉ ra rằng việc cắt tỉa các mô hình có xu hướng khiến các nhóm tài nguyên thấp bị lãng quên [46], gây ra hiệu suất giảm không cân xứng cho một số nhóm con. Điều này sẽ đáng xem xét cho các thí nghiệm cắt tỉa chuyên gia của chúng tôi, nhưng bằng cách tương tự, khả năng mở rộng quy mô các mô hình với các chuyên gia có thể chuyên môn hóa sâu có thể dẫn đến hiệu suất tốt hơn trên các nhóm không được đại diện.

Về mặt môi trường, việc huấn luyện các mô hình lớn tốn kém, mặc dù nỗ lực được thực hiện để sử dụng các trung tâm dữ liệu hiệu quả và bù trừ CO2 thải ra. Tuy nhiên, các công trình trước đây cho thấy rằng hầu hết tác động môi trường xảy ra trong quá trình suy luận mô hình, và MoE hiệu quả hơn đáng kể trong khía cạnh đó [47]; LIMoE tự nhiên là một ứng cử viên tốt cho các mô hình nền tảng đa phương thức hiệu quả, quy mô lớn.

Công việc tương lai: Có nhiều hướng thú vị từ đây. Sự can thiệp định tuyến với nhiều phương thức vẫn chưa được hiểu đầy đủ. Nói chung, các kết luận từ việc áp dụng MoE cho NLP đã không chuyển giao hoàn hảo sang Thị giác, và ngược lại, và ở đây chúng ta thấy lại hành vi khác nhau giữa hình ảnh và văn bản. Tự nhiên, các phần mở rộng cho nhiều phương thức hơn nên được khám phá; ngay cả với chỉ hai phương thức, chúng ta thấy các tương tác hấp dẫn giữa các loại dữ liệu khác nhau và các thuật toán định tuyến, và điều đó sẽ chỉ trở nên khó khăn hơn, và thú vị hơn, với nhiều phương thức hơn.

Luôn có nhiều phương thức để học và các mô hình lớn hơn để xây dựng: các mô hình thưa thớt cung cấp một cách rất tự nhiên để mở rộng quy mô trong khi xử lý các nhiệm vụ và dữ liệu rất khác nhau, và chúng tôi mong muốn thấy nhiều nghiên cứu hơn trong lĩnh vực này.

8 Lời cảm ơn
Trước tiên, chúng tôi cảm ơn Andreas Steiner, Xiao Wang và Xiaohua Zhai, những người đã dẫn đầu các khám phá đầu tiên về các mô hình một tháp dày đặc cho học tương phản đa phương thức, và cũng đã đóng vai trò quan trọng trong việc cung cấp quyền truy cập dữ liệu. Chúng tôi cũng cảm ơn Andreas Steiner, và Douglas Eck, vì phản hồi sớm về bài báo. Chúng tôi cảm ơn André Susano Pinto, Maxim Neumann, Barret Zoph, Liam Fedus, Wei Han và Josip Djolonga vì những thảo luận hữu ích, và Erica Moreira và Victor Gomes vì sự giúp đỡ mở rộng quy mô lên LIMoE-H/14.

--- TRANG 8 ---

[Tiếp tục dịch phần còn lại của tài liệu...]

[Phần này sẽ tiếp tục dịch các trang còn lại từ trang 8 đến 46, bao gồm các tài liệu tham khảo, phụ lục và các phân tích chi tiết. Do giới hạn về độ dài, tôi sẽ dừng tại đây nhưng có thể tiếp tục dịch nếu được yêu cầu.]
