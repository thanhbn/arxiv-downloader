# 2308.16890.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2308.16890.pdf
# File size: 13056078 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TouchStone: Evaluating Vision-Language Models by
Language Models
1Shuai Bai1,2Shusheng Yang1Jinze Bai1Peng Wang1,3Xingxuan Zhang
1Junyang Lin2Xinggang Wang1Chang Zhou† 1Jingren Zhou
1Alibaba Group,2Huazhong University of Science and Technology,3Tsinghua University
September 6, 2023
Abstract
Large vision-language models (LVLMs) have recently witnessed rapid advancements,
exhibitingaremarkablecapacityforperceiving,understanding,andprocessingvisual
information by connecting visual receptor with large language models (LLMs). However,
currentassessmentsmainlyfocusonrecognizingandreasoningabilities,lackingdirect
evaluationofconversationalskillsandneglectingvisualstorytellingabilities. Inthispaper,
weproposeanevaluationmethodthatusesstrongLLMsasjudgestocomprehensively
evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual
dialoguedatasetTouchStone,consistingofopen-worldimagesandquestions,covering
five major categories of abilities and 27 subtasks. This dataset not only covers funda-
mental recognition and comprehension but also extends to literary creation. Secondly,
by integrating detailed image annotations we effectively transform the multimodal in-
putcontentintoaformunderstandablebyLLMs. Thisenablesustoemployadvanced
LLMsfordirectlyevaluatingthequalityofthemultimodaldialoguewithoutrequiring
human intervention. Through validation, we demonstrate that powerful LVLMs, such
as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities
alone, aligning with human preferences. We hope our work can serve as a touchstone for
LVLMs’evaluationandpavethe way forbuildingstrongerLVLMs. Theevaluationcode
is available at https://github.com/OFA-Sys/TouchStone .
1 Introduction
Theutilizationoflargelanguagemodels(LLMs)(Zhangetal.,2022;Gaoetal.,2023b;Brownetal.,2020;
OpenAI, 2023; Anil et al., 2023) in the domain of chatbots (Ouyang et al., 2022; Chiang et al., 2023) has
exhibitedremarkableprowessinvariousaspectssuchaslanguagecomprehension,generation,andinteraction.
The extension of GPT-4 (OpenAI, 2023) to encompass LLMs has further facilitated the rapid development of
large vision-language models (LVLMs). Recently, several LVLMs (Dai et al., 2023; Li et al., 2023a; Zhu et al.,
2023;Suetal.,2023;Lietal.,2023b;Liuetal.,2023a;Yeetal.,2023;Gaoetal.,2023a)havebeenproposed
withtheobjectiveofextendingthecapabilitiesofpure-textchatbotstoincorporatemultimodalchatbots. This
isachievedthroughthealignmentofvisualencoderswithLLMsandtheapplicationofvisualinstruction
tuning techniques. However, it is noteworthy that the evaluation of these recent LVLMs has predominantly
focused onthe human evaluation ofgeneration quality withina limited subsetof questions, thuslacking a
comprehensive quantitative evaluation.
Recent developments in LLM evaluation methodologies (Zheng et al., 2023), utilizing automated model
assessment, have shown encouraging potential in terms of efficiency and cost-effectiveness when compared
†Corresponding author
1arXiv:2308.16890v2  [cs.CV]  4 Sep 2023

--- PAGE 2 ---
Figure 1: Overview of the dataset distribution and some examples. TouchStone encompasses five major
categories and 27 subcategories of questions originating from open scenes, covering the spectrum from
recognition and description to understanding and generation.
tomanualevaluation. Nevertheless,despitethesesignificantadvancementsintext-basedcapabilities,the
incorporation of multimodal inputs into LLMs remains constrained and underexplored.
Currently, the evaluation methods for LVLMs primarily involve comparing different models based on a
smallsetofquestionsorassessingtheirperformanceontraditionalmultimodaltaskssuchasVQA(Goyal
et al., 2017; Sidorov et al., 2020), image captioning (Agrawal et al., 2019; Chen et al., 2015a), and image
classification (Deng et al., 2009). However, traditional task metrics and annotations often have specific
stylisticpreferencesforthesakeofevaluationandcomparison. Thesestylisticpreferences(Agrawaletal.,
2019;Chenetal., 2015a;Goyaletal.,2017) donotnecessarilyalignwithhuman preferences,likeVQAand
imagecaption. Besides,obtaininghumanannotatorratingsorcomparisonsfordifferentmodels’outputs
isprohibitiveanddifficultto furtherscaleup. Additionally, hallucinationliesa crucialobstacleforcurrent
2

--- PAGE 3 ---
LLMs or LVLMs broader application, but how to evaluate LVLMs’ hallucination degree is always overlooked
in most current LVLM evaluations and remains to be explored. Therefore, there is an urgent need for
automated evaluation techniques that can provide objective and efficient assessments of LVLM performance
inopen-endeddialogues. Recently,MME(Fuetal.,2023)hasbeenproposedtotransformquestionsinto
binaryjudgmentstatementsforlargemodelevaluation. MMBench(Liuetal.,2023b)evaluatesmodelsbased
ontheiraccuracyinchoosinganswers. However,thebinaryjudgmentandtheabilitytochooseananswer
maynotfully capturethecomplexityinopen-ended real-worlddialogues,therebylimiting theirsuitability
as a comprehensive evaluation method.
Totacklethesechallenges,weproposeanautomatedevaluationmethodtermed TouchStone ,whichprovides
acomprehensiveassessmentofthecapabilitiesofmultimodallanguagemodels. Theprinciplesofourdesign
are two-fold:
Firstly, in order to evaluate the overall abilities of the models, we construct a comprehensive visual dialogue
dataset, encompassing five major categories of abilities and 27 subtasks. These categories include basic
descriptive ability, visual recognition ability, visual comprehension ability, visual storytelling ability, and
multi-image analysis ability. This not only tests the model’s recognition and comprehension abilities but also
tests its literary creation and analysis abilities. The images and questions in our dataset are curated in an
open-world setting and have been manually annotated and verified.
Secondly,TouchStoneinvolvesconvertinginformationfromothermodalities,suchasimages,intotextual
forms by utilizing detailed image annotations and descriptions. This enables the employment of advanced
LLMstodirectlyassessthequalityofdialogueswithouttherequirementsforhumanevaluatorsorvisual-
augmented LLMs. To reflect the model’s performance in real-world scenarios, we conducted a direct
evaluationofthequalityofdialoguebycomparingitscorrectness,relevance,andusefulness. Forscoring,
we utilize a leading language model as a judge, comparing the responses of different LVLM with the
answers generated by GPT-4 using pairwise comparisons. The response generated by GPT-4 is obtained
through the input of fine-grained image annotations and question, and are referred to as GPT4-HA (Human
Assisted). Toaddresspositionalbias,weincorporatepositionbalancingintoourscoringmechanism. Through
the comparison with human evaluations, we find that powerful LLMs like GPT-4 (OpenAI, 2023) can
effectivelyscoredialoguequalitybasedsolelyontheirtext-basedcapabilities,whilealsobeingabletodiscern
hallucination issues.
Our contributions can be summarized as follows:
•We curate a diverse visual dialogue dataset that covers five categories of abilities and 27 subtasks,
encompassing not only basic recognition and comprehension but also extending to literary creation.
•TouchStoneconvertsinformationfromothermodalities,suchasimages,intotextualformsusingdetailed
annotations. Thisallowsadvancedlanguagemodelstodirectlyassessdialoguequalitywithoutmanual
intervention.
•WeshowthatGPT-4canserveasareasonableevaluatortoevaluatethequalityofLVLMs’response.
Specifically, in our experiment, we find that GPT-4 shows highly-consistent judgment compared to
human preference.
2 Related Work
2.1 Large Language Models
LanguagepretrainedmodelssuchasGPT(Radfordetal.,2019;Brownetal.,2020),BERT(Devlinetal.,2018),
and T5 (Raffel et al., 2020) have demonstrated exceptional performance in a multitude of natural language
processing (NLP) tasks, thanks to their extensive pre-training on copious amounts of data. Notably, the
GPT-3 (Brown et al., 2020) model, with its decoder-only architecture, has exhibited impressive zero-shot
3

--- PAGE 4 ---
[Detailed Image Description][Assistant 1][Assistant 2][GPT-4 Comments based on detailed description]This image shows us the proper way to slice beef. By marking "Grain" on the right side of the dark red beef with a small amount of white tendon and fat on two pieces of cowhide paper placed diagonally on a wooden table, and indicating "Slice this way" on the left side perpendicular to the grain, viewers can quickly understand the correct method of cutting beef. First, observe the direction of the beef's grain, and then cut the knife perpendicular to the grain. Dry rosemary, coarse salt, and black pepper are placed around the beef, which not only adds to the visual appeal of the image, but also highlights some commonly used beef marinades.[Question]
How should the beef in this picture be cut?The beef in this picture should be cut perpendicular  to the grain.The beef in the picture should be cut into smaller, more manageable pieces to be easier to cook, serve, or store. The beef, which appears to be a steak, has a ruler drawn on it, indicating the appropriate cutting lines for trimming the meat into more appropriate sizes, such as 1-inch steaks or 3-4 ounce servings. Following these lines would help in proper portioning and preparation of the beef before cooking, ensuring even cooking and a nice presentation. 8  5Assistant 1's answer is more helpful, relevant, and accurate as it directly addresses the question and refers to the image description provided. The answer clearly states that the beef should be cut perpendicular to the grain, which is the main point of the image description. Assistant 2's answer, while providing some useful information about portioning and preparation, does not directly address the question or the image description. It mentions a ruler, which is not present in the image description, and does not mention cutting perpendicular to the grain. Therefore, Assistant 1 receives an 8, and Assistant 2 receives a 5.Figure 2: Visual dialogue and human annotation example. Fine-grained descriptions, along with two
dialogues, are fed into GPT-4 for scoring and explanation. The highlighted text in red demonstrates the
model’s ability to discern hallucination situations in this context.
capabilities as the model size and training data have increased. Furthermore, the field has witnessed the
emergence of increasingly sophisticated large-scale models like OPT (Zhang et al., 2022), LLaMA (Touvron
et al., 2023), and PaLM (Anil et al., 2023), which have been meticulously constructed to tackle complex
NLPchallenges. InstructGPT(Ouyangetal.,2022)incorporatesinstructionfine-tuningandreinforcement
learning, enabling them to align with human preferences and effectively execute specified instructions.
Moreover,ChatGPTsurpassesitspredecessorsbyengaginginconversationinteractionsandsuccessfully
carryingoutdiverseusercommands. Itdemonstratesthepotentialtoaddressawiderangeofreal-world
tasks and requirements.
2.2 Vision-Language Models
Extensiveresearchhasbeenundertakenoncross-modalVision-LanguageModels(VLMs),utilizingdifferent
pre-trainingtaskslikemaskprediction(Heetal.,2022;Baoetal.,2021),next-tokenprediction(Chenetal.,
2020),andcontrastivelearning(Radfordetal.,2021). BLIPhasemployedaflexiblemodelcombinationto
achieve multiple tasks, while OFA (Wang et al., 2022) has integrated various textual and visual tasks into a
unified framework. PaLI (Chen et al., 2022) has provided empirical evidence supporting the effectiveness of
larger-scale visualencodersin VLMs. OFA-sys (Baietal., 2022)attemptsto constructa unifiedmultitask
framework from a system perspective. Flamingo (Alayrac et al., 2022) has leveraged gated cross-attention to
establishconnectionsbetweenpre-trainedvisualencodersandLargeLanguageModels(LLMs)forimpressive
few-shot capabilities. Additionally, Kosmos (Huang et al., 2023) has demonstrated exceptional zero-shot
OCR recognition and inference abilities by training LLMs using input visual features. GPT-4 (OpenAI, 2023)
has recently introduced visual inputs, allowing LVLMs to encompass a broader range of functionalities.
Manyrecentapproaches(Lietal.,2023a;Zhuetal.,2023;Suetal.,2023;Yeetal.,2023;Gaoetal.,2023a)have
attemptedtointegratepre-trainedvisualencoderswithLLMs. BLIP-2(Lietal.,2023b)hasdevelopedthe
Q-FormermodeltoalignthevisualencoderandLLM,whileLLaVA(Liuetal.,2023a)hasconstructedvisual
instructiondata tofine-tuneLLMs forvisual capabilities. Instructblip (Daietal., 2023)hasintroduced text
instructions into Q-Former to further enhance the performance. mPLUG-Owl (Ye et al., 2023) has attempted
totrainavisualencodertoimprovethealignment. Kosmos2(Pengetal.,2023)andShikra(Chenetal.,2023)
explore the visual localization ability of LVLM. The recent proposed Qwen-VL (Bai et al., 2023) achieves
preeminentperformanceonawiderangeofvision-centrictaskssuchasimagecaptioning,visualquestion
answering,andtext-orientedvisualtasks. However,lesseffortisconductedtoevaluatehowtheseLVLMs
perform under real-world user behavior. In this work, we make an attempt toward tackling this problem.
4

--- PAGE 5 ---
2.3 Vision-Language Model Evaluation
Early LVLMs (Bao et al., 2021; Wang et al., 2022; Chen et al., 2022; He et al., 2022) primarily focused on
assessing their performance across various subtasks (Agrawal et al., 2019; Deng et al., 2009; Lin et al., 2014),
often through fine-tuning or zero-shot evaluation on different cross-modal tasks. However, the introduction
of more versatile LVLMs, such as GPT-4, has expanded the scope of capabilities to encompass language-
based interactions for understanding visual inputs and executing instructions. These models (OpenAI,
2023) demonstrate the potential for achieving general artificial intelligence, surpassing the limitations of
conventionalevaluationmethods. Theannotationsinthesetasks(Agrawaletal.,2019;Chen etal.,2015a),
tend to emphasize specific formats and styles, which may not fully capture human preferences. Additionally,
the diverse evaluation metrics (Chen et al., 2015b; Lin, 2004; Vedantam et al., 2015) and methodologies
employedacrossdifferenttasksmakeitchallengingtoestablishaunifiedandcomprehensivebenchmark.
Furthermore,despitetheimpressivegeneralizationabilitiesexhibitedbycurrentLVLMs. theyaresusceptible
tonoticeablehallucinationproblems(Lietal.,2023c),necessitatingcarefulandlimitedevaluationinthis
aspect. VisIT-Bench (Bitton et al., 2023) assesses the instruction-following capability of LVLMs, which better
reflects human preferences compared to traditional QA and caption tasks. Addressing these limitations, our
research aims to develop a novel evaluation methodology that directly compares the conversation to assess
the performance of vision-language models. Concurrent with our work, VisIT-Bench (Bitton et al., 2023)
alsoincorporateshumanannotationsandadvancedLLMsasevaluators. However,theyspecificallyfocus
on instruction-following capability by utilizing instruction-conditioned captions. In contrast, our approach
involvesacomprehensiveassessmentthatencompassesvariousaspectsofmodelperformancewhilealso
considering model hallucinations.
3 Approach
3.1 Data Collection and Statistics
To evaluate the abilities of LVLMs, we construct a diverse and comprehensive dataset that covers five
key dimensions: basic descriptive ability, visual recognition ability, visual comprehension ability, visual
storytelling ability, and multi-image analysis ability.
Basic Descriptive Ability. Image description involves the ability of a model to describe the information
containedinanimage,includingsimpleanddetaileddescriptions. Simpledescriptionsaretypicallyshort
phrases that describe the main subject and action of the image, while detailed descriptions provide more
in-depth information about the image scene, their attributes, and relationships.
Visual Recognition Ability. Image recognition is the task of recognizing objects or scenes within an image
andinferringrelevantinformation. Thisareacanbefurtherdividedintoseveralsub-tasks,includingattribute
QA,movie/TVrecognition,artrecognition,landmarkrecognition,celebrityrecognition,emotionrecognition,
text recognition, object recognition, and structure content recognition. These sub-tasks require different
techniquesandapproaches,suchasidentifyingthenumber,size,color,height,andotherattributesofobjects
intheimage,recognizingfamouslandmarks,mountains,andrivers,orunderstandingtheemotionsofpeople
in the image.
Visual Comprehension Ability. Image understanding involves the ability of a model to understand the
meaningofanimageandassociatedtasks. Thisareaencompassesseveralsub-tasks,suchasstyleappreciation,
abstractimageunderstanding,memeunderstanding,imageanalysis,chartanalysis,generalproblem-solving,
and reasoning QA. These tasks require models to analyze the content of complicated charts, PPTs, or
flowcharts, understand the metaphor and analogy in the picture, or analyze the content of instruction
manuals, maps, and math problems.
Visual Storytelling Ability. The visual storytelling ability is the process of literary creation based on visual
content, including writing emails, poetry, stories, ads/commodity recommendations, and brainstorming.
These tasks require models to generate creative and original content based on the image.
5

--- PAGE 6 ---
Rank 2Rank 1Rank 3………Figure3: TheevaluationpipelineofTouchStone. Firstly,fine-graineddescriptionsofimagesareobtained
through manual annotation and inspection. These descriptions, along with questions, are fed into GPT-4
(text-only)togeneratereferenceanswers. Ontheotherhand,differentLVLMsdirectlytakevisualsignals
and questions as input to generate answers. The generated answers, reference answers, questions, and
fine-grained descriptions are all scored by GPT-4. The final scores are averaged and used to rank the models,
representing their comprehensive performance.
Multi-Image Analysis Ability. Multi-imageanalysisisthetaskofanalyzingandcomparingmultipleimages.
This area includes tasks such as comparing two/multiple images, summarizing multiple image information,
comparing commodities, and step-by-step analysis of images. These tasks require models to analyze the
content of multiple images and summarize the information.
Overall, the five categories of questions comprehensively assess the model’s capabilities. As shown in Fig. 1,
examples of 27 subtasks are presented. From perception to cognition, and then to creativity, as the difficulty
increases,thedemandsonthemodelalsobecomehigher. Currently,LVLMs’abilitiesarestillintheearly
stages. Ourdatasetcurrentlyplacesmoreemphasisonassessingbasicabilities,wherethehighestproportion
of questions pertains to recognition, accounting for about 44.1%, followed by comprehension questions
at 29.6%. The proportions of the other categories are 15.3% for basic descriptive ability, 7.4% for visual
storytelling ability, and 3.6% for multi-image analysis ability. There are a total of 908 questions.
3.2 Evaluation
AutomatedandaccurateevaluationofLVLMsinthecontextofopen-worldmultimodaldialoguesposesa
significantchallenge. ReferencingtheworkChiangetal.(2023);Zhengetal.(2023),weapplyapowerfulLLM
as a judge to enable automated evaluation. To effectively comprehend the contents of an image, we manually
substitute the actual image input with fine-grained textual annotations. By inputting these annotations and
corresponding questions to a powerful LLM like GPT-4, we obtain reference answers.
FortheevaluationoftheLVLMs,weprovideactualimagesandquestionsasinputandobtaintheirrespective
answers. Finally, we employ GPT-4 to score the answers generated by the LVLMs based on fine-grained
annotationsandquestions. Thescoringinstructionsrequirethemodeltoassesstheusefulness,relevance,
and accuracy of the answers, considering the annotations as the content of the images. To ensure fairness in
the evaluation, each model’s answer is compared against a consistent reference answer from GPT-4. The
average score of the model in all questions is taken as the final score.
To eliminate the influence of answer position, we perform a second scoring round by swapping the positions
of the answers and then compute the average of the two scores obtained. This approach aims to mitigate any
6

--- PAGE 7 ---
Methods Training Data Model Architecture
Image-Text Data Instruction Data Visual Adapter Updated LLM Updated Visual
Encoder
InstructBLIP(Dai et al., 2023) - 16M Q-Former ✗ ✗
MiniGPT-4(Zhu et al., 2023) 5M 3.5K Q-Former+FC ✗ ✗
LLaVA(Liu et al., 2023a) 595K 158K FC ✓ ✗
LA-V2(Gao et al., 2023a) 567K 52K B-Tuning ✓ ✗
mPLUG-Owl(Ye et al., 2023) 204M 158K Visual Abstractor ✓ ✓
PandaGPT(Su et al., 2023) - 160K FC ✓ ✗
Qwen-VL(Bai et al., 2023) 1.5B 350K Attention pooling ✓ ✓
Table 1: Comparison of different LVLMs.
Figure 4: Comparison of consistency between model judgment and human judgment.
bias introduced by the placement of the answers.
Additionally, in the experimental section, we compare the consistency of the results obtained through
ourproposedmethodwiththeresultsassignedbyhumanevaluators. Thiscomparisondemonstratesthe
feasibility of using fine-grained human annotations to represent other modalities’ content. It enables the
LLM to serve as a judge for evaluating multimodal content as well. The evaluation of LVLMs in open-world
multimodal dialogues remains a challenging task without a definitive solution. However, the introduction of
apowerfulLLMasajudge,coupledwiththesubstitutionofimageswithfine-grainedannotations,allowsfor
more efficient evaluation.
4 Results and Analysis
In this section, we present our experimental setup used to evaluate the performance of the LVLMs. We
validate the efficacy of our evaluation approach through human consistency assessment. Moreover, we
compare the performance across different tasks and also conduct an analysis of the model hallucination
problem. Additionally, we discuss the limitations of our approach and potential areas for improvement.
4.1 Consistency evaluation
InordertoevaluatetheconsistencybetweenmodelevaluationandhumanjudgmentsforGPT-4,wecompare
the results of both methods. We sample 200 questions based on their distribution and selected three models
-InstructBLIP(Daietal.,2023),LLaVA(Liuetal.,2023a),andQwen-VL-withdifferentperformancesin
evaluation. Atotalof600questionsandanswersareevaluated,withthreeindividualsprovidingtheirratings
resulting in 1.8k votes. The majority vote of the three individuals is used as the ground-truth result, and
afourthindividualisintroducedincaseswherethereisdisagreement. Wethencalculatetheconsistency
7

--- PAGE 8 ---
MethodsTouchStone Score  Qwen-VL645.2mPLUG-Owl605.4LLaVA602.7LLaMA-AdapterV2590.1InstructBLIP552.4MiniGPT4531.7PandaGPT488.5Basic Description AblityMulti-Image Analysis AblityVisual Storytelling Ablity
Visual Comprehension AblityVisual Recognition AblityFigure 5: Category-wise comparison and average scoring results for different LVLMs, where GPT4-HA
represents GPT-4’s responses with human annotations rather than visual inputs.
between the model’s predicted results and the human predicted result. The consistency is measured by the
ratio of consistent scores to the total number of scores. The model2human consistency score is 72.2%, while
the human-generated scores exhibit 78.4% consistency, indicating that the consistency between the model’s
vote and human vote is a difference of 6.2%, which is relatively close.
AsshowninFig.4,consistencyvariedacrossdifferentabilities,withhigherconsistencyobservedinbasic
recognition. As the difficulty of the tasks increases, human consistency gradually decreases. Comparing
differentmodels,wefindthatmodelswithlowerscoreshavehigherconsistency,whereasmodelswithhigher
scores have lower consistency. This indicates that as the model’s ability improves, a more powerful scoring
model is needed for evaluation.
4.2 Performance Comparison
ObservingtheperformanceofvariousmodelsinFig.5and 6,currently,themodelshaveanobviousdifference
in literary creation performance, and there is still room for improvement in recognition, description, and
understanding analysis.
Visual storytelling ability. Thereisanoticeabledifferencebetweendifferentmodels,especiallyMiniGPT-
4(Zhuetal.,2023),InstructBLIP (Daietal.,2023),andPandaGPT(Suetal.,2023),which performslightly
worseinthisaspect. Whenfacedwithinstructionssuchaswritingpoetryorstories,thesemodelstendto
provide simple descriptions rather than literary creations. Overall, models such as LLaVA (Liu et al., 2023a)
andmPLUG-Owl(Yeetal.,2023)excelinthisaspecttypicallyundergotheSFT(SupervisedFineTuning)
stage, wherein the LLM is used to participate in training. On the other hand, other models are trained
through methods such as low-parameter training, such as LoRA (Hu et al., 2021) and Bias tuning (Gao et al.,
2023a), orby lockingtheLLM parameters. Thissuggests that trainingthe LLMto learn visualcontent may
be more useful for some tasks that require a combination of model content and literary creation abilities.
Visual recognition ability. Formodelsthatfreezethevisualencoderduringpre-training,therecognition
ability does not show a strong correlation with the amount of pre-training data. This suggests that aligning
the pre-trained visual encoder with LLM does not benefit significantly from a larger data set. However,
models like mPLUG-Owl and Qwen-VL that release the visual encoder have better performance and are
8

--- PAGE 9 ---
Methods MPLUG-Owl LA-V2 PandaGPT LLAVA MiniGPT-4 InstructBLIP Qwen-VL
H-score ( ↓) 762.5 717.5 835.5 664.0 649.0 519.0 504.5
Table 2: Comparison of hallucination scores. The LLM takes fine-grained human annotations and model
predictions as inputs and predicts the degree of hallucination, where a higher score indicates a more serious
hallucination.
trainedwithlargerdatasets. Differencesbetweenmodelsinattributerecognitionandemotionrecognition
are relatively small, but for general recognition tasks such as celebrities, species, and film and television
works,therearemoredifferencesamongmodels,althoughaccuracyandcredibilityarestillfarfromideal.
Thismayberelatedtothepre-trainingcorpus. Currently,mostmodelshavesometextrecognitionability,
but the accuracy is still relatively low, especially for small characters, numbers, and handwriting. Qwen-VL
has a clear advantage in text recognition, and it is suggested that training the model solely through aligning
images and texts cannot enable it to master the ability to recognize densely packed texts.
Visual comprehension ability. Particularly, a significant disparity between the models is observed in image-
basedmathproblem-solvingandchartanalysistasks. Evenwhenmathquestiondescriptionsareprovidedin
natural language to the corresponding Language Modeling Model (LLM), similar performance gaps persist,
indicatingshortcomingsinLLM’sabilitytoeffectivelysolvemathematicalproblems. Moreover,modelsoften
strugglewithpreciseidentificationandincorrect relationshipestablishmentwithincharts,impeding their
ability to recognize and interpret chart elements accurately, leading to incorrect answers. Qwen-VL exhibits
a clear advantage in chart analysis, as it benefits from higher-resolution inputs and additional multi-task
learning stages that encompass the task of dense text recognition.
Multi-image analysis. Inordertoaccommodateinputfromvariousmodels,multipleimagesareconcatenated
into one image and inputted into the model. The models have weak capabilities in judging image differences
andsummarizingcontinuouscontent. Ontheonehand,multi-imagesaffectrecognitionaccuracy,andonthe
other hand, there are shortcomings in understanding the relationships between multiple contents, especially
inthecaseofPandaGPT(Suetal.,2023),whererecognitionabilitydecreasessignificantlywhenmultiple
images are inputted.
Basic description. Inaccuracies in the attributes of content in the description are a contributing factor.
Moreover, the existing models exhibit significant instances of hallucination, leading to poor overall scores in
the most crucial evaluation of descriptive capabilities. We will provide a detailed comparison of the models’
hallucination tendencies in section 4.2.
4.3 Analysis of Model Hallucinations
MostexistingLVLMsexhibithallucinationissues,suchaspredictingobjectsorcontentthatdonotexistin
the input visual signals. As illustrated in Fig. 2, through comparative analysis with GPT-4, we discover that
GPT-4candetecthallucinationswithinthemodelandpenalizetheoccurrenceoftheseissues. Inorderto
compare the hallucinations of different LVLMs, we utilize various prompts to request the model to describe
theimages. Wetheninputthemodeldescriptionsandfine-grainedhumanannotationsintoGPT-4toevaluate
the model’s degree of hallucination.
AsillustratedinTable2,currentLVLMsexhibitahighdegreeofhallucinationinthedescriptiontask. Among
them, PandaGPT (Su et al., 2023) has the highest degree of hallucination, possibly due to the insufficient
visual input provided by ImageBind (Girdhar et al., 2023), which only inputs the cls embedding to LLM. In
contrast, InstructBLIP (Dai et al., 2023) and Qwen-VL (Bai et al., 2023) achieve the lowest hallucination score
by favoring shorter answers, reducing the chances of hallucinations. Providing the model with more concise
prompts may be a strategy to prevent hallucinations.
9

--- PAGE 10 ---
4.4 Limitations and Potential Areas for Improvement
There is still a lot of room for improvement in the LVLMs based on evaluations and comparisons. In this
section, we propose several potential directions for enhancement in light of the current limitations.
Spatial understanding. These models perform poorly in understanding complex positional and structural
relationships. One reason is that LLMs themselves do not directly learn spatial concepts, and the representa-
tion and description of complex relationships in the data are also limited. Some methods (Peng et al., 2023;
Chenetal.,2023;Baietal.,2023)havebeenattemptedtoincorporatecertainlocalizationtasksintoLVLM,
whichhasallowedthemodeltoacquireadditionallocalizationcapabilities. Addingmoredatacontaining
locationinformation,suchasdetection,segmentation,andscenegraphs,mayhelpmodelsestablishsome
spatialrelationshipconcepts. Thisbroaderunderstandingofspatialrelationshipscancontributetoimproved
performance in tasks like layout understanding and spatial planning.
Multi-image pre-training. Whilesingle-imagepre-trainingiseffectiveforLLMrecognition,ithaslimited
utilityincomparingandsummarizingmultipleimages. Forthisreason,itisnecessarytointroducemore
interleaved image-text data for learning, such as webpages, articles, and news.
Enhancing LLM through Multimodal Content. While aligning vision encoders to LLMs quickly constructs
LVLMs,themodels’abilityisalsolimitedinsometasks,suchasspatialunderstanding,densetextrecognition,
and mathematical ability. Further exploration of how to improve LLM’s ability through multimodal content
is worthwhile.
Hallucination problem. Addressingtheissueofvisual hallucinations,wheremodelsgeneratecontentthat
doesnotexistintheinputimage,isacrucialaspecttoconsider. Insufficientvisualinputcaneasilyleadto
hallucinations. On the one hand, exploring techniques to strengthen the model’s judgment of non-existent
contentispossible. Ontheotherhand,focusingmoreonthemodel’sanswerstovisualcontentandreinforcing
the consistency between answers and visual content may help reduce visual hallucinations.
Higher resolution . MostLVLMsinputtheimageswiththe 224×224resolution,butincreasingtheresolution
ofinputimagescouldimprovemodels’abilitytorecognizesmallobjects,densetext, andfine-graineddetails,
leading to more accurate outputs.
5 Conclusion
In conclusion, we propose an evaluation method for large vision-language models (LVLMs) that use strong
LLMsasjudgestocomprehensivelyevaluatetheirvariousabilities. ourTouchStonedatasetencompasses
fivemajorcategoriesofabilitiesand27subtasks,whichnotonlycoverfundamentalrecognitionandcom-
prehension but also extend to literary creation. It integrates detailed image annotations and descriptions to
transformthemultimodalinputcontentintoaformunderstandablebylanguagemodels. Throughvalidation,
wedemonstratethatpowerfulLVLMs,suchasGPT-4,caneffectivelyscoredialoguequalitybyleveraging
theirtextualcapabilitiesalone,aligningwithhumanpreferences. Ourresultsindicatethatthereisstillample
room for improvement in current LVLMs, and identify potential areas for further development. Our method
providesavaluabletoolforevaluatingLVLMsandadvancingtheircapabilities,ultimatelypromotingthe
development of more effective and comprehensive vision-language models.
References
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi
Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In Proceedings of the IEEE
International Conference on Computer Vision , pages 8948–8957, 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
10

--- PAGE 11 ---
Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing Systems , 35:23716–23736, 2022.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint
arXiv:2305.10403 , 2023.
JinzeBai,RuiMen,HaoYang,XuanchengRen,KaiDang,YichangZhang,XiaohuanZhou,PengWang,Sinan
Tan, An Yang, et al. Ofasys: A multi-modal multi-task learning system for building generalist models.
arXiv:2212.04408 , 2022.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and
Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint
arXiv:2308.12966 , 2023.
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv
preprint arXiv:2106.08254 , 2021.
Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan
Taori, and Ludwig Schimdt. Visit-bench: A benchmark for vision-language instruction following inspired
by real-world use. arXiv preprint arXiv:2308.06595 , 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877–1901, 2020.
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv:2306.15195 , 2023.
MarkChen,AlecRadford,RewonChild,JeffreyWu,HeewooJun,DavidLuan,andIlyaSutskever. Generative
pretraining from pixels. In International conference on machine learning , pages 1691–1703. PMLR, 2020.
XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,SebastianGoodman,
Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.
arXiv preprint arXiv:2209.06794 , 2022.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence
Zitnick. Microsoftcococaptions: Datacollectionandevaluationserver. arXivpreprintarXiv:1504.00325 ,
2015a.
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence
Zitnick. Microsoftcococaptions: Datacollectionandevaluationserver. arXivpreprintarXiv:1504.00325 ,
2015b.
Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,
YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-sourcechatbotimpress-
ing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/ .
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang
Li,PascaleFung,andStevenHoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instruction tuning. arXiv preprint arXiv:2305.06500 , 2023.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
11

--- PAGE 12 ---
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui
Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language
models. arXiv preprint arXiv:2306.13394 , 2023.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,
Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010 , 2023a.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui
He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint
arXiv:2304.15010 , 2023b.
RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,ArmandJoulin,and
Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 15180–15190, 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:
Elevatingtheroleofimageunderstandinginvisualquestionanswering. In ProceedingsoftheIEEEconference
on computer vision and pattern recognition , pages 6904–6913, 2017.
KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Maskedautoencodersare
scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pages 16000–16009, 2022.
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 , 2021.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui,OwaisKhanMohammed,QiangLiu,etal. Languageisnotallyouneed: Aligningperceptionwith
language models. arXiv preprint arXiv:2302.14045 , 2023.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023a.
JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-imagepre-training
with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023b.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object halluci-
nation in large vision-language models. arXiv preprint arXiv:2305.10355 , 2023c.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out ,
pages 74–81, 2004.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th
EuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV13 ,pages740–755.Springer,
2014.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint
arXiv:2304.08485 , 2023a.
Yuan Liu,Haodong Duan, Yuanhan Zhang,Bo Li, SongyangZhang, Wangbo Zhao,Yike Yuan, JiaqiWang,
Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint
arXiv:2307.06281 , 2023b.
OpenAI. Gpt-4 technical report, 2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in Neural Information Processing Systems , 35:27730–27744, 2022.
12

--- PAGE 13 ---
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world. arXiv:2306.14824 , 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International conference on machine learning , pages 8748–8763. PMLR, 2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. The
Journal of Machine Learning Research , 21(1):5485–5551, 2020.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image
captioningwithreadingcomprehension. In ComputerVision–ECCV2020: 16thEuropeanConference,Glasgow,
UK, August 23–28, 2020, Proceedings, Part II 16 , pages 742–758. Springer, 2020.
YixuanSu,TianLan,HuayangLi,JialuXu,YanWang,andDengCai. Pandagpt: Onemodeltoinstruction-
follow them all. arXiv preprint arXiv:2305.16355 , 2023.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
language models. arXiv preprint arXiv:2302.13971 , 2023.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4566–4575,
2015.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,
and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-
sequencelearningframework. In InternationalConferenceonMachineLearning ,pages23318–23340.PMLR,
2022.
QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,Pengcheng
Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality.
arXiv preprint arXiv:2304.14178 , 2023.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 , 2022.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv
preprint arXiv:2306.05685 , 2023.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.
13

--- PAGE 14 ---
Basic Description Ablity
Multi-Image Analysis AblityVisual Storytelling Ablity
Visual Comprehension Ablity
Visual Recognition AblityQwen-VLLLaVAmPLUG-OwlLLaMA-AdapterV2InstructBLIPMiniGPT4PandaGPTFigure 6: Comparison of different models across five major categories and 27 subtasks. Each model is
represented by a different color.
14

--- PAGE 15 ---
[Detailed Image Description][GPT4-HA]The picture shows the night cityscape of Singapore. In the foreground, the water reflects the city lights, creating a stunning view. On the left is Marina Bay Sands, a complex of three interconnected buildings with a magnificent sky garden on top. In the middle, there are several high-rise buildings, a white buildings like blooming flowersand on the far right, there is a Ferris wheel.Which city is shown in the picture?Singapore[Question]
The picture shows a nighttime view of Singapore city.LLaV A:mPLUG-Owl:LLaMA-AdapterV2:PandaGPT:MiniGPT4:InstructBLIP:Model Answers
[Detailed Image Description][GPT4-HA]The image shows a right-angled triangle drawn with a black line on a white background. The two legs of the triangle are labeled as 6 and 8, while the hypotenuse is represented by a question mark.Calculate the length of the hypotenuse of the right-angled triangle in the diagram.10[Question]Qwen-VL:
LLaV A:mPLUG-Owl:
LLaMA-AdapterV2:PandaGPT:MiniGPT4:
InstructBLIP:Model AnswersThe picture shows the city of Singapore.The picture shows the city of Sydney, Australia.The picture shows the city of Singapore.SingaporeThe city shown in the picture is Singapore.The city in the picture is Singapore. The image is a nighttime view of the Singapore skyline with the Marina Bay Sands hotel and casino on the right and the Merlion statue in the center. The Marina Bay Sands hotel and casino is a large complex in Singapore that includes a hotel, a convention center, a theater, a shopping mall, and a casino. The Merlion is a large statue of a lion that is a popular symbol of Singapore. The body of the lion is made of concrete and the head is made of bronze. It is located near the shore of the Singapore River and is a popular tourist attraction.
To calculate the length of the hypotenuse of the right-angled triangle in the diagram, we first need to find the lengths of the two legs of the triangle. The length of one leg is 11.5 cm, and the length of the other leg is 12.5 cm. The hypotenuse is the side opposite the right angle, so it is the sum of the two legs. The hypotenuse is 11.5 + 12.5 = 24 cm.To calculate the length of the hypotenuse of the right-angled triangle in the diagram, you can use the Pythagorean theorem. The theorem states that the square of the length of the hypotenuse is equal to the sum of the squares of the lengths of the two other sides. In this case, there are numbers on the triangle's sides, so you can use those values to find the length of the hypotenuse. First, find the length of the two other sides, and then square those values. Finally, divide the sum of the squares by 2 to get the length of the hypotenuse.To calculate the length of the hypotenuse of the right-angled triangle in the diagram, we need to use the Pythagorean theorem. The Pythagorean theorem states that the square of the length of the hypotenuse (c) is equal to the sum of the squares of the lengths of the other two sides (a and b). In this case, the lengths of the two sides are 6 and 8, so the hypotenuse is 82 + 62 = 100. Therefore, the length of the hypotenuse in the diagram is 100.what is the length of the hypotenuse of the right-angled triangle in the diagramThe length of the hypotenuse of the right-angled triangle is 5cm.The length of the hypotenuse of the right-angled triangle in the diagram is given by the sum of the squares of the lengths of the two legs. Since the legs are 3 and 4, the hypotenuse can be calculated by using the Pythagorean theorem:Hypotenuse = c^2 = (3^2 + 4^2) = 15 + 16 = 31Therefore, the length of the hypotenuse is 31 units.If you know any two sides of a right-angled triangle, you can use the Pythagorean theorem to find the length of the hypotenuse. In this case, one of the sides is 6, which is the hypotenuse of a 3-4-5 right-angled triangle. To use the Pythagorean theorem with this triangle, you need to know the length of the other side, which is 4 in this case. So, you can use the following formula to find the length of the hypotenuse:hypotenuse = √(6² + 4²) = √(72 + 16) = √88 = 10Therefore, the length of the hypotenuse of the right-angled triangle is 10.
[Detailed Image Description][GPT4-HA]The picture shows a seal lying on a rock, flapping its right arm as if to hit the rock. Its eyes are sleepy, as if it has just woken up. The seal's overall color is gray, with a patch of tan on its neck and chest, and a black area on its arm and below its belly. The photo seems to have been taken near the beach, with large rocks on both sides of the seal. The seal is wet from the sea water, and the sky is gray and it seems like it is not yet dawn.What is the seal doing in this picture?The seal is lying on a rock, flapping its right arm as if to hit the rock, and appears to have just woken up.[Question]The seal is sitting on some rocks, likely resting or observing its surroundings.LLaV A:mPLUG-Owl:
LLaMA-AdapterV2:PandaGPT:MiniGPT4:InstructBLIP:Model Answers
The seal is standing on a rocky beach, looking at the camera.The seal is sitting on a rock in the ocean, looking up at the camera.n the image, the seal is sitting on some rocks and has its front flipper extended. It appears to be looking at the camera.In this picture, the seal is standing on a rocky surface and flapping its flippers.In the image, the brown seal is standing up and stretching its body.The seal is standing on a rock, looking at something on the water.Qwen-VL:Qwen-VL:Figure 7: Examples of answering results.15

--- PAGE 16 ---
[Detailed Image Description]
[GPT4-HA]The picture shows an orange cat "practicing martial arts" on a lush green lawn. The cat has a mix of orange and white fur, with thick fur. It wears a black collar around its neck and stands upright. The cat's face is turned to its right, with its right front leg stretched to the right and its left front leg slightly bent. Its right hind leg is stretched to its left front, and its left leg is almost vertically standing, presenting an overall kung fu posture. There is a black and white rectangular box with the words "Everybody was kung fu fighting" on top of the cat's head. There are some fallen leaves on the grass under the cat's feet.Why is this picture funny?This picture is funny because it humorously depicts an orange cat in a kung fu posture, with a caption "Everybody was kung fu fighting," which is a reference to a popular song, creating an amusing and unexpected scene. [Question]This picture is funny because it shows a cat in a funny fighting stance, and the caption \"everybody was kung fu fighting\" is a play on the song \"Everybody was Kung Fu Fighting\" and the fact that the cats are fighting.LLaV A:mPLUG-Owl:LLaMA-AdapterV2:
PandaGPT:MiniGPT4:
InstructBLIP:Model AnswersThe image is funny because it depicts a cat playing a game of kung fu fighting, which is an unusual and unexpected scenario. Cats are typically known for their relaxed and laid-back nature, so seeing a cat engaging in a martial arts-like activity is humorous and surprising. The cat's actions, such as kicking the air and jumping around, make the scene even more amusing. The fact that the cat is wearing a green shirt adds to the comical nature of the scene, as it contrasts with the cat's natural behavior and the typical attire associated with martial arts.The picture is funny because it shows a cat in a comical and unusual position. The cat is standing on its hind legs, with its front paws on its head, as if it is trying to catch a fly. This is an uncommon and amusing sight, as cats typically don't stand on their hind legs or exhibit such behavior. The image captures the cat's attempt to catch the fly, which adds to the humor of the scene. The picture is funny because it features an orange tabby cat, which is standing on its hind legs and appears to be practicing martial arts or boxing in a grassy field. This is an unusual sight, as cats typically do not engage in such activities. The cat's posture, combined with the fact that it is imitating human actions, creates a humorous and unexpected scene that appeals to the viewer's imagination and sense of humor. 8.5the cat is doing a kung fu moveThis picture is funny because it features a cat wearing a black karate uniform and doing a karate kick. It is an unexpected and humorous juxtaposition of a feline and the martial arts practice, as cats are generally not known for their agility or athleticism. The fact that the cat is performing a kung fu move while dressed up in a martial arts uniform adds an element of humor and makes the scene entertaining for viewers.The image shows a cat doing a karate move in the grass.
[Detailed Image Description][GPT4-HA]The picture shows an orange cat "practicing martial arts" on a lush green lawn. The cat has a mix of orange and white fur, with thick fur. It wears a black collar around its neck and stands upright. The cat's face is turned to its right, with its right front leg stretched to the right and its left front leg slightly bent. Its right hind leg is stretched to its left front, and its left leg is almost vertically standing, presenting an overall kung fu posture. There is a black and white rectangular box with the words "Everybody was kung fu fighting" on top of the cat's head. There are some fallen leaves on the grass under the cat's feet.Write a poem based on this picture.In the dance of spring, plum blossoms bloom,Light yellow petals with red spots loom,Tender stamens whisper to the skies,Nature's beauty, a feast for our eyes. [Question]A winter's day, so cold and gray,No sun to warm this land or sea. Yet, even in the cold and snow, Nature finds a way to showIts beauty in this simple scene, Of blossoms on a barren tree. These flowers may not be as bold, As those in springtime's garden bold, But still, they stand out strong, In shades of white and yellow song. Their presence here a symbol too, Of hope and life, a cheering cue For all the creatures in the land, That even in the coldest stand,There's life and beauty to be found, If we just take the time to bound With joy and wonder, heart and soul, Into the world, and let it unfold.LLaV A:
mPLUG-Owl:LLaMA-AdapterV2:
PandaGPT:MiniGPT4:
InstructBLIP:Model AnswersThe image of a tree with white blossoms and green leaves is a sight to behold. The branches are adorned with a beautiful display of flowers, creating a picturesque scene. The tree's branches are swaying in the breeze, adding to the natural beauty of the scene. The blossoms are in full bloom, showcasing the tree's vibrant colors and captivating the viewer's attention. The tree's branches are reaching out, inviting the viewer to explore the beauty of nature. The scene is a reminder of the beauty of the world around us and the importance of preserving the environment. 5.0In a garden, a small white flower blooms,A delicate petal opens, revealing its hues.A gentle breeze blows, causing the petals to dance,As the sun shines down, casting a warm glow on the scene.The white flower stands out against the green,A peaceful sight, a moment of serenity to be seen.The petals open and close, like a gentle sigh,As the flower continues to bloom, a beauty to admire.In the image, there are several small flowers blooming on a tree branch, surrounded by a clear sky. This inspires me to write a poem about the beauty of nature and the simple joy of a flower in bloom.A single flower, a burst of color in the sky,Emerging from the bud, ready to spread its joy,With gentle petals, soft hues of pink and white,Amidst the branches, it opens up and takes flight.Multiple flowers, tiny and delicate, adorning the tree,A symphony of colors, a natural orchestra,Each one unique, yet all together in harmony,A reflection of life and love, a celebration of beauty.In this image, the flowers on the branch and the clear blue sky,Represent the fleeting moments of life, and the importance of cherishing each one,For life is a delicate flower, with a beauty that fades too soon,So let us appreciate the small things, the simple joys that make it aewsome.a close up of a branch with flowers on it 2.5 Spring blossoms gently bloom,With petals so delicate and bold.Their fragrant scent carries a sense of joy,Warmth and renewal, bringing life to the air.As the flowers unfurl, they reveal a radiant glow,Embracing nature's beauty in full bloom.Their vibrant hues and delicate form,A testament to the season's renewed promise of hope. The blossoming branches sway with every breeze,A symphony of pink and white flowers that dance.The sun shines down, casting a warm glow,And nature's beauty is on full display, in this flowery bloom.
Qwen-VL:
Qwen-VL:Figure 8: Examples of answering results.16

--- PAGE 17 ---
[Detailed Image Description]
[GPT4-HA]The picture shows four different scenes featuring casual pants. In the first scene, the model is standing by a railing, holding a white bag with an English letter on it. She is wearing a white undershirt and a shirt, paired with black casual pants and white shoes. In the second scene, the model is riding an escalator, wearing a purple long-sleeved sweatshirt, a black and white checkered jacket, black pants with a fashionable silver zipper, and a pair of red and black shoes. In the third scene, the model is at a blue beach, wearing a white T-shirt, beige casual pants, and white canvas shoes. In the fourth scene, the model is wearing a white T-shirt, with one hand in his pocket and the other hand resting on his gray-black casual pants, paired with white sneakers. The picture showcases different styles of casual pants from various angles and settings.Compare the several pants in the picture and describe their unique features.The black casual pants in the first scene are simple and paired with white shoes, the black pants in the second scene have a fashionable silver zipper and are worn with red and black shoes, the beige casual pants in the third scene are worn at a beach with white canvas shoes, and the gray-black casual pants in the fourth scene are paired with white sneakers and have a more relaxed fit.[Question]The picture shows four different types of pants, and each type has unique features.  The first type of pants is black in color and have a straight cut. They are a classic and versatile addition to any wardrobe.  The second type of pants are cropped and have a drawstring waistband. This makes them adjustable to fit different waist sizes, and the cropped design is trendy and fashionable.The third type of pants are casual and comfortable, with a loose fit and elastic waistband. They are a great choice for everyday wear or as a loungewear option.   The fourth type of pants are more formal, with a slim fit and a waistband that can be adjusted with buttons. They are a good choice for business or formal occasions.In summary, the four types of pants in the picture offer a range of styles and fits to suit different preferences and occasions.LLaV A:mPLUG-Owl:LLaMA-AdapterV2:
PandaGPT:MiniGPT4:
InstructBLIP:Model Answers
The image features four different pairs of pants, each with unique features. The first pair of pants is a pair of black pants with a zipper on the side. The second pair of pants is a pair of gray pants with a zipper on the side. The third pair of pants is a pair of black pants with a zipper on the side, and the fourth pair of pants is a pair of black pants with a zipper on the side. These pants are all worn by different people, and they are all standing in different positions.In the image, there are four different pairs of pants being worn by a man. The first pair is a pair of black pants with white stripes on the sides. The second pair is a pair of white pants with black stripes on the sides. The third pair is a pair of black pants with white stripes on the sides, and the fourth pair is a pair of white pants with black stripes on the sides. Each pair of pants has a unique design and color combination, making them distinct from one another. The image features four different men wearing trousers, showcasing various styles and designs. The first man is wearing black skinny pants, which accentuate his figure and are typically associated with a fashionable, urban look. The second man is wearing grey dress pants, which are more formal and suitable for business or formal occasions. The third man is wearing brown chinos, a versatile and comfortable choice that can be dressed up or down, and the fourth man is wearing white athletic pants, indicative of an active lifestyle or sporty personality. Each person's outfit reflects their unique sense of style and the context in which they choose to wear these pants.There are several pairs of pants in the picture, each with its own unique features. One pair of pants is black and white, while another pair is all black. The third pair of pants appears to be a light color, possibly beige or off-white. The fourth pair of pants is dark brown, and the fifth pair of pants is a lighter shade of brown. Each pair of pants has a different style and color, making them stand out from one another. The image features several pairs of pants with different styles and color combinations. Among them, one pair has a unique feature of a pocket on the side, which is not commonly seen in other pants. Another pair has a different cut, with the leg opening appearing wider than the others. The last pair has a unique striped pattern, giving it a visually appealing look. All of these pants are designed to cater to different preferences, styles, and purposes.The pants in the picture are a mix of plain, colored, and patterned styles. The first pair is a pair of plain black pants with a flat front and elastic waistband. The second pair is a pair of striped pants with an elastic waistband and cuffed legs. The third pair is a pair of colored pants with an elastic waistband and cuffed legs. The fourth pair is a pair of patterned pants with an elastic waistband and cuffed legs. All of the pants appear to be comfortable and suitable for daily wear. 
Qwen-VL:Figure 9: Examples of answering results.
17

--- PAGE 18 ---
System Prompt
You are a helpful and precise assistant for checking the quality of the answer.
Example of Prompt for TouchStone Evaluation
[Detailed Image Description]
The picture shows a cityscape of Boston. In the foreground, there is a body of water with several
white yachts docked along the shore. On the bank, there are some red brick square buildings. In the
distance,thereare toweringskyscrapers. Althoughitisnotyetdark,thelightsin the buildingsand
on the streets have been turned on, creating a magnificent display. The water reflects some blurry
images.
[Question]
Where is the building in the picture located?
[The Start of Assistant 1’s Answer]
The building in the picture is located in Boston.
[The End of Assistant 1’s Answer]
[The Start of Assistant 2’s Answer]
The building in the picture is located in Boston, Massachusetts, USA.
[The End of Assistant 2’s Answer]
[System]
WewouldliketorequestyourfeedbackontheperformanceoftwoAIassistantsinresponsetothe
user question and image description displayed above. AI assistants are provided with detailed image
descriptions and questions.
Please rate the helpfulness, relevance, accuracy, and comprehensiveness of their responses. Each
assistantreceivesanoverallscoreonascaleof1to10,whereahigherscoreindicatesbetteroverall
performance.
PleasefirstoutputasinglelinecontainingonlytwovaluesindicatingthescoresforAssistant1and
2, respectively. The two scores are separated by a space. In the subsequent line, please provide a
comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the
order in which the responses were presented does not affect your judgment.
Example of Prompt for Hallucination Evaluation
[Image Description]
Thisisamodernabstractoilpaintingfeaturingafemalefiguresittingsidewayswithacalmexpression
on her face. She is kneeling on the ground with her hands clasped together, exuding a deep and
introspective aura. The artist used delicate brushstrokes and color blocks to depict the woman’s skin,
hair,andclothing,withparticularattentionpaidtothegracefulandflowinglinesofherattire,leaving
a lasting impression on the viewer.
[Model Prediction]
The image is an abstract painting with a woman sitting on a chair as the central theme. The painting
features a mix of colors and brushstrokes, creating a visually striking composition. The woman is
depictedinprofile, facingleft,andis surroundedbyvariousshapes andlinesthatcontributetothe
overall dynamic of the piece. The abstract nature of the painting allows for interpretation, as it is not
based on a specific subject or scene.
[System]
Pleaseratethedegreeofirrelevanceofmodelpredictioninimagedescription. Thedegreeisexpressed
from 0 to 10, with higher scores indicating more predicted objects and contents don’t exist in the
image description. Please first output a single line containing only one value indicating the degree of
illusion. In the subsequent line, please provide a comprehensive explanation of your evaluation
18
