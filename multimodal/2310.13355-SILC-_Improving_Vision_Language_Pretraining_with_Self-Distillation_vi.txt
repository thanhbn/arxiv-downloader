# 2310.13355.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.13355.pdf
# Kích thước file: 4749493 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
SILC: Cải thiện Tiền huấn luyện Ngôn ngữ Thị giác với Tự chưng cất
Muhammad Ferjad Naeem1⋆Yongqin Xian2*Xiaohua Zhai3*
Lukas Hoyer1,2◦Luc Van Gool1Federico Tombari2,4
1ETH Zurich2Google3Google Deepmind4TU Munich
{mnaeem,lhoyer,vangool }@vision.ee.ethz.ch, {yxian, xzhai,tombari }@google.com

Tóm tắt
Tiền huấn luyện Hình ảnh-Văn bản trên các bộ dữ liệu chú thích hình ảnh quy mô web đã trở thành công thức mặc định cho các mô hình phân loại và tìm kiếm từ vựng mở nhờ vào thành công của CLIP và các biến thể của nó. Một số nghiên cứu cũng đã sử dụng các đặc trưng CLIP cho các tác vụ dự đoán dày đặc và đã cho thấy sự xuất hiện của các khả năng tập mở. Tuy nhiên, mục tiêu tương phản được sử dụng bởi các mô hình này chỉ tập trung vào việc căn chỉnh hình ảnh-văn bản và không khuyến khích việc học đặc trưng hình ảnh cho các tác vụ dự đoán dày đặc. Trong nghiên cứu này, chúng tôi giới thiệu SILC, một khung mới cho tiền huấn luyện ngôn ngữ thị giác. SILC cải thiện việc học tương phản hình ảnh-văn bản với việc bổ sung đơn giản là học tương ứng cục bộ-toàn cục bằng tự chưng cất. Chúng tôi chỉ ra rằng việc chưng cất các đặc trưng hình ảnh cục bộ từ một mô hình giáo viên trung bình động hàm mũ (EMA) cải thiện đáng kể hiệu suất mô hình trên các tác vụ dự đoán dày đặc như phát hiện và phân đoạn, đồng thời cũng mang lại cải thiện trên các tác vụ cấp độ hình ảnh như phân loại và tìm kiếm. Các mô hình SILC thiết lập một trạng thái nghệ thuật mới cho phân loại không-shot, phân loại few-shot, tìm kiếm hình ảnh và văn bản, phân đoạn không-shot, và phân đoạn từ vựng mở. Chúng tôi tiếp tục chỉ ra rằng các đặc trưng SILC rất có lợi cho phát hiện từ vựng mở, tạo chú thích và trả lời câu hỏi thị giác.

1. Giới thiệu.
Những tiến bộ gần đây trong học tự giám sát [8, 10, 20, 40] và học giám sát yếu trên dữ liệu web [23, 43, 60] đã dẫn đầu trong việc phát triển các mô hình ngôn ngữ [13, 42] và thị giác-ngôn ngữ cơ bản [23, 43, 60]. Các phương pháp này vượt qua thách thức lâu dài của việc thu thập bộ dữ liệu có nhãn lớn bằng cách phát triển các mục tiêu tự giám sát. Việc phát triển các mô hình thị giác máy tính từ vựng mở có thể lý luận vượt ra ngoài một tập hợp các lớp được xác định trước đã là một thách thức lâu dài. Việc giới thiệu các bộ dữ liệu hình ảnh-văn bản web và tiến bộ trong tính toán đã cho phép những tiến bộ đáng kể trong lĩnh vực này. Được phổ biến bởi CLIP [43], tiền huấn luyện tương phản sử dụng các bộ dữ liệu lớn với các cặp hình ảnh và văn bản từ web và huấn luyện một mô hình thị giác-ngôn ngữ (VLM) để nhúng chúng vào một không gian tiềm ẩn chung. Vì các mô hình này được huấn luyện trên một tập hợp các khái niệm rộng lớn, VLM học được cho phép suy luận từ vựng mở [43]. Tuy nhiên, việc phát triển các mô hình dự đoán dày đặc từ vựng mở cho phân đoạn và phát hiện vẫn là một thách thức mở, vì các bộ dữ liệu quy mô internet không có nhãn dày đặc ở cấp độ pixel. Một số nghiên cứu đã phát hiện ra rằng việc kết hợp VLM trong các mô hình phân đoạn và phát hiện có thể mở khóa một số khả năng từ vựng mở [12, 14, 24, 52, 57]. Vì CLIP không được huấn luyện cho các tác vụ này, các phương pháp này vượt qua các hạn chế của nó bằng cách điều chỉnh

--- TRANG 2 ---
mô hình học được với một số bộ dữ liệu có nhãn dự đoán dày đặc. Một nhóm phương pháp sử dụng một mô hình phân đoạn / phát hiện bình thường để suy luận bất tri lớp và sau đó dự đoán logits lớp với CLIP [12, 31]. Một nhóm phương pháp khác nhằm mục đích chưng cất VLM trực tiếp vào một mô hình dự đoán dày đặc và sử dụng transformer văn bản để tạo ra các trọng số lớp để dự đoán logits [18, 26]. Các nghiên cứu này đã có tác động cao trong việc mở rộng khả năng từ vựng mở của các mô hình dự đoán dày đặc. Tuy nhiên, vì mục tiêu tiền huấn luyện tương phản không khuyến khích một cách rõ ràng việc học các đặc trưng cục bộ tốt cho các tác vụ dự đoán dày đặc, các phương pháp này bị hạn chế bởi hiệu suất nội tại của VLM [40] như chúng tôi cũng chỉ ra sau này trong các thí nghiệm.

Trong văn liệu học tự giám sát, việc thực thi tính nhất quán cục bộ-toàn cục bằng tự chưng cất đã nổi lên như một mục tiêu tiền huấn luyện mạnh mẽ [8, 40, 65] để học các backbone thị giác có khả năng cạnh tranh trong phân loại cũng như các tác vụ dự đoán dày đặc, ví dụ như phân đoạn và phát hiện. Tuy nhiên, các backbone này không thể được sử dụng trực tiếp cho suy luận không-shot hoặc từ vựng mở vì chúng không chứa bất kỳ khái niệm nào về lớp hoặc ngôn ngữ trong mô hình. Trong nghiên cứu này, chúng tôi đề xuất SILC, kết hợp những ưu điểm của hai nhánh này và thống nhất tiền huấn luyện tương phản hình ảnh-văn bản và học tính nhất quán cục bộ-toàn cục. SILC sử dụng một bộ dữ liệu hình ảnh-văn bản web để học một mô hình cải thiện hiệu suất VLM trên các tác vụ phân loại và tìm kiếm hiện có trong khi đặc biệt cải thiện hiệu suất trên phân đoạn không-shot và từ vựng mở, phát hiện từ vựng mở, tạo chú thích và Trả lời Câu hỏi Thị giác (VQA).

Các đóng góp của chúng tôi như sau: 1. Chúng tôi đề xuất một khung huấn luyện mới cho VLM kết hợp tiền huấn luyện tương phản trên dữ liệu hình ảnh-văn bản với tự chưng cất trên hình ảnh web. 2. Mặc dù về mặt khái niệm rất đơn giản, chúng tôi chỉ ra rằng bằng cách học các đặc trưng thị giác mạnh hơn với hiểu biết cục bộ tốt hơn, các mô hình SILC mang lại cải thiện nhất quán trên nhiều tác vụ thị giác máy tính. Những cải thiện này đặc biệt rõ ràng trên các tác vụ đòi hỏi hiểu biết cục bộ tốt hơn bao gồm phân đoạn không-shot, phân đoạn từ vựng mở, phát hiện từ vựng mở, tạo chú thích và Trả lời Câu hỏi Thị giác (VQA). 3. Chúng tôi đóng góp một mô hình cơ sở mới thiết lập một trạng thái nghệ thuật mới về phân loại không-shot, phân loại few-shot, tìm kiếm hình ảnh-văn bản và văn bản-hình ảnh, phân đoạn ngữ nghĩa không-shot và phân đoạn ngữ nghĩa từ vựng mở.

2. Các nghiên cứu liên quan.
Tiền huấn luyện Hình ảnh-Văn bản. Tiền huấn luyện mô hình thị giác-ngôn ngữ (VLM) [11, 23, 28, 43] nhằm mục đích học các biểu diễn đa phương thức chung có thể tổng quát hóa cho một loạt rộng các tác vụ hạ nguồn. Tiến bộ đáng kể đã được thực hiện trong lĩnh vực này hướng tới các mục tiêu tiền huấn luyện tốt hơn [23, 50] và bộ dữ liệu hình ảnh-văn bản quy mô lớn tốt hơn [11, 43]. Một trong những hàm mục tiêu phổ biến nhất là học tương phản [23, 43] kéo các cặp hình ảnh và văn bản tích cực lại gần nhau và đẩy những cặp tiêu cực ra xa trong không gian nhúng chung. Nó có khả năng mở rộng quy mô tới một bộ dữ liệu tiền huấn luyện quy mô lớn và học các đặc trưng hình ảnh và văn bản có tính phân biệt cao. Nhiều nghiên cứu [17, 30, 37, 38, 47, 53, 59, 60] theo hướng này đã chứng minh cải thiện trên các tiêu chuẩn phân loại hình ảnh và tìm kiếm không-shot.

Một hướng nghiên cứu khác tập trung vào học tạo sinh thông qua tạo văn bản tự hồi quy [48–50]. So với học tương phản, học tạo sinh thường hoạt động tốt hơn trên các tác vụ tạo văn bản, ví dụ như tạo chú thích hình ảnh và VQA. Cuối cùng, có các phương pháp lai [1, 27, 28, 32, 46, 56] kết hợp nhiều hàm mục tiêu bao gồm tổn thất tạo sinh, tương phản và đa tác vụ. Trong khi nhiều VLM [43, 50] chủ yếu tập trung vào học căn chỉnh hình ảnh-văn bản toàn cục có lợi cho các tác vụ hạ nguồn cấp độ hình ảnh, nghiên cứu của chúng tôi nhằm mục đích phát triển một VLM mới có lợi cho cả tác vụ cấp độ hình ảnh và cấp độ pixel. Đã có một số nỗ lực [15, 16, 33, 63] để cải thiện VLM cho các tác vụ dự đoán dày đặc bao gồm phát hiện đối tượng và phân đoạn ngữ nghĩa. Tuy nhiên, chúng hoặc là mô hình hóa các tương tác patch-văn bản chi tiết không thể mở rộng [16, 33] hoặc dựa vào các chú thích hộp giới hạn bổ sung [29, 63]. Trong nghiên cứu này, chúng tôi đề xuất kết hợp học tương phản hình ảnh-văn bản với tự chưng cất để học một VLM.

Học Tự giám sát. Học tự giám sát là một mô hình tiền huấn luyện phổ biến khác nơi các đặc trưng được học từ chính dữ liệu hình ảnh. Một nhánh phương pháp tối ưu hóa mạng để giải quyết các tác vụ giả định, ví dụ như tô màu hình ảnh [62], vẽ [41], dự đoán biến đổi [19], và sắp xếp patch [35]. Một nhóm phương pháp khác áp dụng học phân biệt cấp độ thể hiện thông qua học tương phản [10, 21] và phân cụm [6, 7]. Gần đây, [22] chỉ ra rằng bộ tự động mã hóa có mặt nạ cũng là một học tự giám sát có thể mở rộng. Nghiên cứu của chúng tôi được lấy cảm hứng từ DINO [8] cho thấy rằng phân đoạn xuất hiện từ việc học tính nhất quán giữa các góc nhìn cục bộ và toàn cục. Tuy nhiên, DINO không thể được sử dụng trực tiếp cho suy luận không-shot và từ vựng mở vì nó chỉ học các đặc trưng hình ảnh. Ngược lại, phương pháp của chúng tôi được huấn luyện trên dữ liệu hình ảnh và văn bản cùng nhau. Chúng tôi chỉ ra rằng cùng với dữ liệu văn bản, mục tiêu DINO cho phép mô hình phát triển hiểu biết về các đặc trưng cục bộ và các lớp ngữ nghĩa của chúng. Do đó, mô hình của chúng tôi có thể trực tiếp có lợi cho nhiều ứng dụng thị giác máy tính hơn.

Phân đoạn Ngữ nghĩa Không-shot. Phân đoạn ngữ nghĩa không-shot nhằm mục đích phân đoạn các khái niệm thị giác tùy ý trong tự nhiên mà không có chú thích dày đặc [51]. Các phương pháp trong lĩnh vực này dựa vào các cặp hình ảnh-văn bản từ sự kết hợp của bộ dữ liệu tạo chú thích hình ảnh và hình ảnh-văn bản web. Vì các bộ dữ liệu này không có nhãn dày đặc, chúng sử dụng một tiêu chí chú ý vùng hình ảnh tới văn bản tự giám sát. Group-VIT [51] đề xuất giới thiệu các token nhóm gom các patch hình ảnh tương tự dưới mỗi token nhóm. MaskCLIP [64] và

--- TRANG 3 ---
CLIPpy [44] phát hiện ra rằng huấn luyện CLIP bình thường dẫn đến phân đoạn không-shot xuất hiện. ReCo [45] đề xuất một quy trình tinh chỉnh trên MaskCLIP bằng tìm kiếm và đồng phân đoạn. Cuối cùng, TCL [9] hiện tại là trạng thái nghệ thuật học một bộ giải mã để tăng mẫu các nhúng patch có căn cứ và học một sự chú ý vùng tới văn bản.

Phân đoạn và Phát hiện Từ vựng Mở. Các phương pháp phân đoạn ngữ nghĩa từ vựng mở nhằm mục đích phân đoạn hình ảnh theo một từ vựng của các danh mục lớp được cung cấp tại thời điểm kiểm tra chứa các lớp chưa thấy bổ sung. Trái ngược với phân đoạn không-shot, phân đoạn ngữ nghĩa từ vựng mở có quyền truy cập vào một bộ dữ liệu phân đoạn ngữ nghĩa với từ vựng hạn chế để huấn luyện. Các phương pháp gần đây chuyển giao khả năng từ vựng mở của CLIP từ dự đoán cấp độ hình ảnh sang cấp độ pixel. LSeg [25] học các nhúng thị giác từng pixel căn chỉnh với nhúng văn bản CLIP trong khi OpenSeg [18] học các đề xuất phân đoạn bất tri lớp để gom các đặc trưng thị giác cho việc căn cứ vùng-văn bản. ZegFormer [14] và ZSseg [52] giới thiệu một khung hai giai đoạn, trước tiên học dự đoán mặt nạ phân đoạn bất tri lớp và phân loại vùng tương ứng bằng CLIP đóng băng. OVSeg [31] tiếp tục điều chỉnh CLIP trên các cặp vùng-văn bản để bù đắp cho sự thay đổi diện mạo của các cắt có mặt nạ. Để tránh chi phí của hai giai đoạn, CAT-Seg [12] học tổng hợp của khối lượng chi phí giữa nhúng văn bản và nhúng hình ảnh dày đặc từ CLIP. Hướng tới phát hiện từ vựng mở, một nhóm phương pháp, ví dụ OWVLv2 [34], RegionCLIP [63], Detic [66], 3Ways [2], tạo nhãn giả các hộp cho dữ liệu chú thích hình ảnh để sử dụng cho tiền huấn luyện định vị. Một nhóm phương pháp trực giao bao gồm [29, 54, 55, 61] tiền huấn luyện các mô hình để căn chỉnh các hộp giả bất tri lớp với văn bản như tiền huấn luyện.

3. Phương pháp.
SILC được xây dựng trên khung tiền huấn luyện tương phản của CLIP [43] và SigLIP [60]. SILC bao gồm một mô hình transformer hai tháp với không gian nhúng chung. Chúng tôi sử dụng một bộ dữ liệu hình ảnh-văn bản kết hợp quy mô web và dựa vào tiền huấn luyện quy mô lớn để học các trọng số của mô hình. Thành phần đầu tiên của mục tiêu tiền huấn luyện của chúng tôi tập trung vào việc căn chỉnh các cặp hình ảnh-văn bản khớp gần nhau và xa khỏi các hình ảnh và văn bản khác trong lô. Mục tiêu này đã cực kỳ thành công trong văn liệu gần đây [43, 60]. Tuy nhiên, mục tiêu tương phản ở dạng hiện tại không tập trung vào việc nắm bắt ngữ nghĩa hình ảnh cục bộ phong phú cần thiết cho các tác vụ dự đoán dày đặc như phân đoạn và phát hiện. Do đó, chúng tôi đề xuất kết hợp mục tiêu tiền huấn luyện tương phản với một mục tiêu tính nhất quán cục bộ-toàn cục sử dụng tự chưng cất như được hiển thị trong Hình 2. SILC lấy tên từ hai mục tiêu huấn luyện bao gồm Tự chưng cất từ Hình ảnh và Căn chỉnh Tương phản Hình ảnh-Ngôn ngữ từ các cặp Hình ảnh-Văn bản.

Bộ mã hóa Hình ảnh
(Giáo viên) Một con mèo dễ thương
EMA
Bộ mã hóa Hình ảnh
(Học sinh) Bộ mã hóa Văn bản
Phép chiếu
Phép chiếu
& Căn giữa Dừng gradient
Góc nhìn toàn cục
Góc nhìn cục bộ Văn bản
Cắt cục bộ
Cắt toàn cục

Hình 2. SILC là một VLM dựa trên transformer hai tháp. Thành phần đầu tiên của mục tiêu huấn luyện của chúng tôi sử dụng góc nhìn toàn cục của hình ảnh bao phủ một khu vực rộng và chú thích kết hợp của nó để tối ưu hóa tổn thất tương phản theo lô cho hình ảnh và văn bản. Thành phần thứ hai của mục tiêu huấn luyện của chúng tôi thực thi tính nhất quán cục bộ-toàn cục bằng tự chưng cất giữa mô hình chính (học sinh) và giáo viên dựa trên Trung bình Động Hàm mũ (EMA). Tương ứng cục bộ-toàn cục này bổ sung cho phép mô hình học các đặc trưng thị giác tốt. Cùng nhau, hai mục tiêu cho phép mô hình xuất sắc trong cả các tác vụ VLM truyền thống cũng như các tác vụ đòi hỏi hiểu biết cục bộ như phân đoạn và phát hiện.

3.1. Căn chỉnh Hình ảnh và Văn bản.
Mục tiêu tiền huấn luyện tương phản dựa vào khung InfoNCE [39]. Nó sử dụng lượng lớn bộ dữ liệu hình ảnh-văn bản quy mô web để học căn chỉnh giữa hình ảnh và văn bản kết hợp. Cho một lô nhỏ B = {(I1, T1),(I2, T2), . . .}, nơi (Ii, Ti) biểu thị một cặp hình ảnh và văn bản khớp, mục tiêu tương phản khuyến khích các cặp hình ảnh và văn bản khớp nằm gần nhau trong không gian nhúng chung. Hình ảnh Ii được xử lý bởi một Vision Transformer F có thể học để có nhúng đặc trưng của nó. Tương tự, văn bản đã được tokenize Ti được xử lý bởi một Text Transformer G có thể học để có nhúng đặc trưng của nó. Các nhúng đặc trưng này được chuẩn hóa bởi chuẩn l2 của chúng để có fi=F(Ii)/∥F(Ii)∥2∈RJ cho hình ảnh Ii và gi=G(Ti)/∥G(Ti)∥2∈RJ cho văn bản kết hợp Ti nơi J là chiều đặc trưng của không gian nhúng chung. Tích vô hướng của fi và gi tính toán độ tương tự cosin của chúng và được tối ưu hóa với một cặp tổn thất cross-entropy như được đề xuất bởi CLIP [43] hoặc tổn thất sigmoid như được đề xuất bởi SigLIP [60]. Các tổn thất tương phản theo lô của CLIP/ SigLIP, được biểu thị là Limage−text, dựa vào kích thước lô lớn để căn chỉnh các cặp hình ảnh-văn bản. Mục tiêu này được điều chỉnh trên một lượng lớn dữ liệu học một không gian nhúng chung giữa hình ảnh và văn bản và do đó có thể được sử dụng cho chuyển giao không-shot cho nhiều tác vụ thị giác máy tính.

3.2. Chưng cất Đặc trưng Hình ảnh Cục bộ.
Tổn thất tương phản hình ảnh-văn bản đã được chỉ ra là rất thành công trong việc học các mô hình chuyển giao không-shot [23, 43]. Các mô hình học với mục tiêu này cũng đã được sử dụng để cải thiện các tác vụ dự đoán dày đặc như phân đoạn từ vựng mở

--- TRANG 4 ---
và phát hiện. Tuy nhiên, mục tiêu tương phản một mình không tập trung rõ ràng vào việc học các đặc trưng thị giác tốt cho các tác vụ dự đoán dày đặc. Các tác vụ này yêu cầu ngữ nghĩa hình ảnh cục bộ được mã hóa đủ trong các nhúng hình ảnh và patch đầu ra. Việc thực thi tính nhất quán cục bộ-toàn cục đã nổi lên như một kỹ thuật mạnh mẽ để hoàn thành điều này trên dữ liệu hình ảnh lớn không có nhãn [8, 40, 65] trong văn liệu tự giám sát. Tuy nhiên, các phương pháp này không thể được sử dụng trực tiếp cho các mô hình từ vựng mở vì chúng được huấn luyện mà không có bất kỳ thông tin ngôn ngữ nào. Trong thành phần thứ hai của khung huấn luyện của chúng tôi, chúng tôi lấy cảm hứng từ tập hợp con này của văn liệu và bổ sung thêm tính nhất quán cục bộ-toàn cục như một mục tiêu huấn luyện cho hình ảnh trong bộ dữ liệu hình ảnh-văn bản của chúng tôi.

Ý tưởng cơ bản của mục tiêu này như sau. Một mạng giáo viên nhận góc nhìn toàn cục của hình ảnh đại diện cho cảnh trong tổng thể và tạo ra một nhúng đặc trưng. Một mô hình học sinh nhận góc nhìn một phần của cùng hình ảnh và tạo ra một nhúng đặc trưng. Một mục tiêu tự chưng cất được giới thiệu nơi học sinh cần khớp dự đoán của giáo viên trong khi chỉ có thông tin một phần. Điều này thực thi mô hình học ngữ nghĩa cục bộ và mối quan hệ của chúng với ngữ nghĩa toàn cục của cảnh. Chúng tôi thêm tiêu chí này cho bộ mã hóa hình ảnh F. Chúng tôi thêm một phép chiếu như một MLP có thể học trên đầu bộ mã hóa hình ảnh để ánh xạ từ không gian nhúng chung ban đầu có chiều J tới K nơi K > J. Học sinh FS là bộ mã hóa hình ảnh chính với đầu phép chiếu có thể học. Vì chúng tôi dựa vào dữ liệu hình ảnh-văn bản quy mô web nhiễu, chúng tôi không có giáo viên oracle cho học sinh để khớp. Do đó, chúng tôi xây dựng giáo viên FT như một trung bình động hàm mũ của học sinh FS từ các lần lặp huấn luyện trước để thực hiện khung tự chưng cất của chúng tôi:

FT ←λFT + (1−λ)FS, (1)

nơi λ điều khiển bước cập nhật của giáo viên. Cho một hình ảnh Ii, giáo viên xử lý cắt toàn cục của nó để tạo ra pti∈RK và học sinh xử lý cắt cục bộ của nó để tạo ra psi∈RK. Để ngăn giáo viên sụp đổ thành một giải pháp tầm thường, chúng tôi áp dụng làm sắc nét trên đầu ra của giáo viên với τt và học sinh với τs. Để khuyến khích mỗi chiều đặc trưng đóng góp vào đặc trưng đầu ra, chúng tôi bổ sung giới thiệu một phép toán căn giữa trên dự đoán của giáo viên. Thuật ngữ căn giữa c∈RK được khởi tạo với 0 và được cập nhật bởi một cập nhật momentum với hệ số m với thống kê lô bậc nhất của dự đoán giáo viên tại mỗi bước như sau: c←mc+ (1−m)1/|B|∑|B|i=1pti.

Để học tương ứng cục bộ-toàn cục, học sinh phải đối mặt với bất đối xứng thông tin. Học sinh được cho góc nhìn cục bộ của hình ảnh được thực hiện như một cắt ngẫu nhiên trên một vùng nhỏ của hình ảnh. Giáo viên, tuy nhiên, có quyền truy cập vào góc nhìn toàn cục của hình ảnh chứa thông tin hơn về cảnh. Học sinh được giao nhiệm vụ khớp ngữ nghĩa của giáo viên trong khi chỉ có thông tin một phần. Do đó, cho một hình ảnh, mô hình cần học ngữ nghĩa cục bộ của hình ảnh và cách nó phù hợp trong bối cảnh toàn cục của hình ảnh này. Điều này được thực hiện như một tổn thất chưng cất kiến thức nơi các vector đặc trưng của học sinh và giáo viên trước tiên được chuyển đổi thành phân phối xác suất bằng cách áp dụng softmax trên dự đoán giáo viên Pt(Igli) =softmax ((pti−c)/τt) và dự đoán học sinh Ps(Ilci) =softmax (psi/τs). Học sinh được tối ưu hóa để khớp giáo viên với tổn thất cross-entropy,

Lself−dist=−Pt(Igli)⊺log(Ps(Ilci)). (2)

Mục tiêu tự chưng cất này khuyến khích bộ mã hóa hình ảnh học ngữ nghĩa cục bộ của hình ảnh trên bộ dữ liệu quy mô web lớn. Vì giáo viên được xây dựng với trọng số của học sinh, và các đặc trưng cấp độ hình ảnh được gom từ các nhúng patch trong một Vision Transformer, điều này cho phép nắm bắt ngữ nghĩa cục bộ phong phú hơn trong các đặc trưng cấp độ hình ảnh cũng như cấp độ patch.

Mặc dù mục tiêu này đã được khám phá trong học tự giám sát [8, 40], theo hiểu biết tốt nhất của chúng tôi, chúng tôi là nghiên cứu đầu tiên chỉ ra bản chất bổ sung của nó đối với học tương phản hình ảnh-văn bản trên bộ dữ liệu quy mô web. Chúng tôi chỉ ra rằng khi kết hợp với văn bản, mục tiêu này cho phép mô hình phát triển hiểu biết cục bộ về ngữ nghĩa của hình ảnh có căn cứ trong ngôn ngữ. Chúng tôi tìm thấy hai sửa đổi quan trọng so với các nghiên cứu trước cho phép nó bổ sung cho học tương phản hình ảnh-văn bản. 1. Mỗi góc nhìn toàn cục được sử dụng trong Lself−dist cần được căn chỉnh với văn bản, nếu không hai mục tiêu sẽ phân kỳ. Điều này được thực hiện bằng cách tính toán tổn thất tương phản hình ảnh-văn bản cho mỗi góc nhìn toàn cục trong khi duy trì cùng kích thước lô. 2. Bộ lập lịch momentum của EMA không nên hội tụ về 1.0. Nếu không, giáo viên ngừng học từ tổn thất hình ảnh-văn bản vì bước cập nhật trở nên quá nhỏ ở giai đoạn sau của huấn luyện. Do đó, chúng tôi sử dụng momentum cố định.

4. Thí nghiệm.
Chúng tôi so sánh khung tiền huấn luyện SILC của chúng tôi với cả CLIP [43] và SigLIP [60] trên cùng bàn thử nghiệm và thực hiện thí nghiệm rộng rãi. Các mô hình SILC dựa trên mục tiêu CLIP được biểu thị bằng SILC-C và các phiên bản SigLIP được biểu thị bằng SILC-S. Chúng tôi chỉ ra rằng SILC thiết lập một trạng thái nghệ thuật mới trên nhiều tác vụ: phân loại không-shot, phân loại few-shot, tìm kiếm, phân đoạn không-shot và phân đoạn từ vựng mở. Chúng tôi tiếp tục chỉ ra rằng các mô hình SILC cũng cải thiện các tác vụ hiểu biết ngữ nghĩa cục bộ khác bao gồm phát hiện từ vựng mở, tạo chú thích và VQA.

4.1. Chi tiết Triển khai.
Chúng tôi triển khai mô hình của chúng tôi bằng jax trong codebase bigvision [3, 4], theo các thiết lập tiền huấn luyện tương phản

--- TRANG 5 ---
Phân loại Không-Shot Phân loại Few-shot Tìm kiếm
Mô hình ImageNet CIFAR100 ImageNet CIFAR100 COCO
T1 T1 1shot 5shot 10shot 1shot 5shot 10shot I2T@1 T2I@1
CLIP (WebLI) [60] 74.1 68.4 42.8 63.2 67.3 39.4 59.6 64.6 61.7 43.9
SILC-C* (Của chúng tôi) 75.3 71.0 44.6 64.3 67.8 42.8 64.6 69.6 62.5 44.9
SILC-C (Của chúng tôi) 76.2 72.3 45.3 65.0 68.5 45.2 66.9 71.3 66.1 49.1
SigLIP [60] 75.1 69.8 44.0 64.2 68.4 39.0 61.7 66.3 62.6 44.9
SILC-S*(Của chúng tôi) 75.8 69.2 45.2 64.6 68.4 40.3 63.3 67.4 63.0 44.6
SILC-S(Của chúng tôi) 76.6 70.6 45.9 65.2 68.9 41.8 64.9 68.9 66.2 48.7

Bảng 1. So sánh SILC* với baseline, chúng tôi quan sát thấy rằng khung tiền huấn luyện của chúng tôi dẫn đến cải thiện đáng kể so với cả mục tiêu CLIP và SigLIP. Chúng tôi tái tạo cả CLIP và SigLIP trên cùng bộ dữ liệu WebLI [11] để định lượng các cải thiện từ mục tiêu huấn luyện được đề xuất. Chúng tôi tiếp tục điều chỉnh SILC* trên một tập hợp con sạch hơn để có mô hình cuối cùng SILC và thấy rằng nó mở khóa hiệu suất bổ sung mà không cần tái huấn luyện đáng kể thêm. Hiệu suất tốt nhất cho mỗi biến thể được in đậm, tốt thứ hai được gạch chân.

từ [60], và sử dụng bộ dữ liệu WebLI[11] cho các thí nghiệm của chúng tôi. Chúng tôi sử dụng hai góc nhìn toàn cục được cắt giữa (0.4−1.0) của diện tích hình ảnh ban đầu và tám góc nhìn cục bộ được cắt giữa (0.05−0.4) của diện tích hình ảnh ban đầu cho tổn thất tự chưng cất. Các góc nhìn toàn cục được thay đổi kích thước thành (256×256) và các góc nhìn cục bộ được thay đổi kích thước thành (96×96). Momentum giáo viên λ được giữ cố định tại 0.966 và momentum cập nhật trung tâm m được giữ cố định tại 0.9 trong suốt quá trình huấn luyện. Nhiệt độ giáo viên τt được cố định tại 0.04 và nhiệt độ học sinh τs được cố định tại 0.1. K là 65536. Chúng tôi thay đổi kích thước hình ảnh ban đầu thành (256×256) cho tổn thất tương phản giữa các cặp hình ảnh-văn bản. Chúng tôi huấn luyện với kích thước lô 16k trên Google TPU. Chúng tôi sử dụng example-seen để biểu thị có bao nhiêu cặp hình ảnh và văn bản được rút từ bộ dữ liệu trong suốt quá trình huấn luyện. Chúng tôi huấn luyện tất cả baseline trong các so sánh chính của chúng tôi trong Bảng 1 cho 20 Tỷ example-seen trên bộ dữ liệu WebLI [11] theo [60]. Các mô hình của chúng tôi được huấn luyện trên WebLI được đánh dấu là SILC*. Chúng tôi sử dụng bộ lập lịch học rsqrt [58] với tỷ lệ học cơ bản 0.001 với 50000 bước làm ấm và 50000 bước làm mát. Các chi tiết huấn luyện bổ sung bao gồm chi phí tính toán được thảo luận trong phần bổ sung.

Chúng tôi bổ sung điều chỉnh mô hình của chúng tôi bằng cách sử dụng một tập hợp con WebLI nhỏ hơn nhưng sạch hơn [11] cho 1 Tỷ example-seen bổ sung và biểu thị mô hình này là SILC. Tập hợp con WebLI nhỏ hơn chứa 100 triệu cặp hình ảnh-văn bản với các bộ lọc văn bản tinh vi hơn, v.v.

4.2. Phân loại và Tìm kiếm.
Chúng tôi so sánh khung tiền huấn luyện của chúng tôi với CLIP và SigLIP dưới cùng giao thức huấn luyện và đánh giá trong Bảng 1. Chúng tôi so sánh tại ViT/B16 và thấy rằng việc giới thiệu tự chưng cất cho cả hai đều cải thiện nhất quán hiệu suất của chúng trên phân loại không-shot, phân loại few-shot và tìm kiếm. Trên phân loại không-shot trên ImageNet, SILC-C* cải thiện CLIP (WebLI) 1.2 điểm, tương tự chúng tôi nhận thấy cải thiện 2.6 điểm trên CIFAR-100 cho thấy lợi ích của tự chưng cất đặc trưng cục bộ. Các cải thiện tương tự được ghi nhận cho phân loại few-shot nơi SILC-C* cải thiện so với CLIP (WebLI) 1.8, 1.1 và 0.5 điểm trên phân loại ImageNet 1 shot, 5 shot và 10 shot tương ứng. Chúng tôi có quan sát tương tự trên tìm kiếm nơi SILC-C* cho thấy cải thiện trên tìm kiếm hình ảnh sang văn bản cũng như văn bản sang hình ảnh. Chuyển sang các phiên bản SigLIP của mô hình, chúng tôi thấy xu hướng tương tự nơi việc giới thiệu mục tiêu tự chưng cất cho phép SILC-S* cải thiện nhất quán hầu hết tất cả các chỉ số trên các tác vụ được đánh giá. Do đó, chúng tôi kết luận rằng việc nắm bắt ngữ nghĩa cục bộ tốt hơn dẫn đến việc học các đặc trưng thị giác mạnh hơn cũng giúp các tác vụ đòi hỏi hiểu biết toàn cục về hình ảnh.

So sánh các mô hình SILC* với SILC, chúng tôi nhận thấy rằng việc điều chỉnh trên tập hợp con sạch hơn mở khóa hiệu suất bổ sung cho mô hình mà không cần huấn luyện thêm đáng kể. Cho SILC-C dựa trên CLIP, chúng tôi nhận thấy thêm 0.9 điểm cải thiện so với SILC-C* trên phân loại ImageNet không-shot. Chúng tôi quan sát cải thiện cùng độ lớn trên phân loại few-shot. So sánh hiệu suất tìm kiếm, chúng tôi thấy sự tăng đáng kể trong hiệu suất tìm kiếm trên COCO nơi SILC-C đạt được 3.6 và 4.2 điểm cải thiện trên Recall@1 Hình ảnh sang Văn bản và Văn bản sang Hình ảnh. SILC-S dựa trên SigLIP theo xu hướng tương tự và cải thiện nhất quán trên SILC-S* trên tất cả các chỉ số. Các mô hình SILC thiết lập trạng thái nghệ thuật mới cho các tác vụ này ở kích thước mô hình ViT/B16. Chúng tôi cũng so sánh với các biến thể CLIP mã nguồn mở trong phần bổ sung và cho thấy hiệu suất vượt trội của SILC.

4.3. Phân đoạn Ngữ nghĩa Không-Shot.
Phân đoạn ngữ nghĩa không-shot nhằm mục đích đo lường hiệu suất căn cứ của một VLM thường từ các nhúng patch của nó. MaskCLIP [64] và CLIPpy [44] phát hiện ra rằng căn cứ này tự nhiên xuất hiện như hệ quả của huấn luyện tương phản hình ảnh-văn bản. Chúng tôi sử dụng một Vision Transformer với đầu gom MAP [58]. Chúng tôi quan sát rằng căn cứ cho mô hình của chúng tôi xuất hiện trong các giá trị của đầu MAP thay vì khối mã hóa cuối cùng. Cho một tập hợp các lớp có thể có trong bộ dữ liệu phân đoạn, chúng tôi có các nhúng văn bản tương ứng bằng cách truy vấn bộ mã hóa văn bản của chúng tôi với một prompt tiêu chuẩn. Chúng tôi tính toán độ tương tự cosin giữa các nhúng patch hình ảnh và các đặc trưng văn bản của mỗi tên lớp để tạo ra một bản đồ phân đoạn theo cách không-shot. Chúng tôi báo cáo hiệu suất mean-IOU (mIOU) của mô hình trong Bảng 2 và so sánh với baseline tại ViT/B16 tương tự như các nghiên cứu trước. Chúng tôi theo giao thức đánh giá của TCL [9] mà không có lớp nền. Tuy nhiên, chúng tôi không sử dụng bất kỳ tinh chỉnh hậu xử lý nào, ví dụ PAMR vì chúng tôi cho rằng phân đoạn thô của một VLM là mô tả thực sự về hiệu suất không-shot của nó.

So sánh với CLIP và SigLIP, chúng tôi thấy rằng cả SILC-C* và SILC-S* đều cho thấy hiệu suất phân đoạn ngữ nghĩa không-shot vượt trội đáng kể. Thực tế, cả hai biến thể đều đạt được cải thiện nhiều điểm mIOU trên tất cả 5 bộ dữ liệu. Điều này xác nhận giả thuyết của chúng tôi rằng sự kết hợp của học tương phản hình ảnh-văn bản và học tương ứng cục bộ-toàn cục cho phép mô hình phát triển hiểu biết tốt hơn về ngữ nghĩa cục bộ của hình ảnh có căn cứ trong ngôn ngữ. Từ Bảng 2, chúng tôi quan sát rằng mục tiêu CLIP nói chung dẫn đến phân đoạn không-shot vượt trội hơn mục tiêu SigLIP. Điều này cũng rõ ràng khi chúng tôi so sánh SILC-C* với SILC-S*. Hơn nữa, chúng tôi quan sát rằng việc điều chỉnh trên tập hợp con sạch hơn tiếp tục cải thiện hiệu suất phân đoạn không-shot của cả SILC-C và SILC-S. Chúng tôi quan sát rằng biến thể CLIP SILC-C cũng vượt trội so với SILC-S ở đây. Chúng tôi cho thấy các cải thiện của SILC trên CLIP (WebLI) một cách định tính trong Hình 3. Chúng tôi có thể quan sát rằng SILC tốt hơn trong việc phân đoạn và gắn nhãn các lớp ngữ nghĩa trong hình ảnh. Chúng tôi muốn nhấn mạnh rằng SILC đạt được điều này mà không có bất kỳ sự thật phân đoạn nào.

So sánh với SOTA hiện tại, chúng tôi quan sát rằng SILC-C nhất quán đánh bại tất cả các baseline phân đoạn không-shot chuyên biệt trên 4/5 bộ dữ liệu để thiết lập trạng thái nghệ thuật mới (SOTA). So với trạng thái nghệ thuật trước đó TCL, SILC đạt được cải thiện đáng kể 4.3 điểm mIOU trên A-150, 2.9 điểm cải thiện trên PC-59, và 4.9 điểm cải thiện trên CityScapes. Các cải thiện tương tự được ghi nhận trên VOC-20 và COCO-Stuff, tuy nhiên Group-VIT duy trì kết quả tốt nhất trên VOC-20. Các phương pháp này sử dụng các bộ dữ liệu tạo chú thích hình ảnh tương đối sạch hơn cho các mục tiêu huấn luyện cụ thể phân đoạn của chúng. Chúng tôi nhận thấy rằng các cải thiện trong phân đoạn không-shot có thể đạt được chỉ bằng cách điều chỉnh trên tập hợp con sạch hơn của dữ liệu. Chúng tôi không quan sát hiệu suất vượt trội bằng cách học sự chú ý theo patch đắt đỏ như được đề xuất bởi PACL [36]. Chúng tôi chỉ ra trong phần bổ sung rằng các mô hình SILC cũng vượt trội so với PACL được huấn luyện trên WebLI. Các phương pháp như TCL và ReCo, được thiết kế để cải thiện hiệu suất phân đoạn không-shot của một VLM đóng băng, về mặt lý thuyết có thể tiếp tục cải thiện hiệu suất của mô hình chúng tôi. Tuy nhiên, vì chúng tôi nhằm mục đích cải thiện tiền huấn luyện thị giác-ngôn ngữ trên tất cả các tác vụ, điều này nằm ngoài phạm vi của nghiên cứu này.

4.4. Phân đoạn Ngữ nghĩa Từ vựng Mở.
Phân đoạn Ngữ nghĩa Từ vựng Mở nhằm mục đích phát triển các mô hình phân đoạn có thể phân đoạn các lớp mới vượt ra ngoài từ vựng huấn luyện. Hầu hết các phương pháp gần đây trong lĩnh vực này

--- TRANG 6 ---
VLM Phương pháp A-847 PC-459 A-150 PC-59 VOC-20 VOC-21
CLIP-B/16 ZegFormer [14] 5.6 10.4 18.0 45.5 89.5 65.5
CLIP-B/16 ZSseg [52] 7.0 - 20.5 47.7 88.4 -
CLIP-B/16 OVSeg [31] 7.1 11.0 24.8 53.3 92.6 -
CLIP-B/16 CAT-Seg [12] 8.4 16.6 27.2 57.5 93.7 78.3
SILC-C-B/16 CAT-Seg [12] 13.4 (+5.0) 22.0 (+5.4) 36.6 (+9.4) 61.2 (+3.7) 95.9 (+2.2) 80.4 (+2.1)
SILC-S-B/16 CAT-Seg [12] 13.5 (+5.1) 21.9 (+5.3) 37.0 (+9.8) 61.2 (+3.7) 96.1 (+2.4) 80.9 (+2.6)
CLIP-L/14 ZSseg [52] 7.1 10.2 21.7 52.2 92.3 -
CLIP-L/14 OVSeg [31] 9.0 12.4 29.6 55.7 94.5 -
CLIP-L/14 CAT-Seg [12] 10.8 20.4 31.5 62.0 96.6 81.8
SILC-C-L/16 CAT-Seg [12] 15.0 (+4.2) 25.8 (+5.4) 37.7 (+6.2) 63.5 (+1.5) 97.6 (+1.0) 82.5 (+0.7)
CLIP-G/14 CAT-Seg [12] 13.3 21.4 36.2 61.5 97.1 81.4

Bảng 3. So sánh hiệu suất Phân đoạn Ngữ nghĩa Từ vựng Mở, chúng tôi quan sát rằng các mô hình SILC cải thiện so với CLIP với biên độ đáng kể trên tất cả các tập kiểm tra chưa thấy. SILC đặc biệt cải thiện hiệu suất cho các tập kiểm tra thách thức với từ vựng lớn. SILC-L/16 thậm chí vượt trội so với CLIP-G/14 lớn hơn nhiều. Tất cả các mô hình được huấn luyện trên COCO-Stuff.

dựa vào một CLIP được tiền huấn luyện do khả năng từ vựng mở của nó và thích ứng nó cho tác vụ phân đoạn. Để đánh giá tiềm năng phân đoạn từ vựng mở của SILC, chúng tôi lấy mô hình trạng thái nghệ thuật hiện tại CAT-Seg [12] và thay thế mô hình CLIP được sử dụng bởi các tác giả bằng SILC. Các mô hình được huấn luyện trên COCO-Stuff-164k với 172 lớp và được kiểm tra trên các bộ dữ liệu chưa thấy với từ vựng khác nhau: ADE-20k với 847 hoặc 150 lớp (A-847/A-150), Pascal Context (PC-459/PC-59), và Pascal VOC (VOC-20/VOC-21).

Từ Bảng 3, chúng tôi quan sát rằng SILC cải thiện đáng kể so với CLIP [43]. Thực tế, SILC-C-B/16 hoạt động ngang bằng với CLIP-G/14 lớn hơn nhiều trên ba bộ dữ liệu kiểm tra thách thức nhất A-847, PC-459 và A-150. Hơn nữa, chúng tôi quan sát rằng trong khi SILC-S hoạt động hơi kém hơn SILC-C trong phân đoạn không-shot, nó đạt được hiệu suất hơi tốt hơn khi được huấn luyện cho phân đoạn từ vựng mở. SILC-S-B/16 tiếp tục cải thiện hiệu suất của SILC-C-B/16. Các cải thiện quan sát được của SILC-C cũng chuyển giao sang biến thể ViT-L lớn hơn, nơi CAT-Seg với SILC-C-L/16 vượt trội so với CAT-Seg với CLIP-L/14 trên tất cả các bộ dữ liệu với biên độ đáng kể. Cụ thể, nó đạt được hơn +4 cải thiện mIOU trên A-847, PC-459, và A-150 thách thức. SILC-L/16 thậm chí vượt trội đáng kể so với CLIP-G/14 lớn hơn nhiều trên tất cả các bộ dữ liệu được kiểm tra. Các cải thiện của SILC-C so với CLIP cũng được phản ánh trong các ví dụ định tính trong Hình 4. Chúng tôi

--- TRANG 7 ---
quan sát rằng SILC-C tốt hơn trong việc phân biệt các lớp ngữ nghĩa tương tự như grandstand/building, field/grass, runway/road và grandstand/chair. Hơn nữa, nó cải thiện phân đoạn trong các trường hợp khó khăn và xử lý tốt hơn các phân đoạn trong suốt như được hiển thị trong phần bổ sung. Kết quả trên các mô hình WebLI bổ sung cũng được cung cấp trong phần bổ sung với kết luận tương tự.

4.5. SILC cho Phát hiện Từ vựng Mở.
Chúng tôi sử dụng OWLv2 [34] như một khung để kiểm tra tiềm năng phát hiện từ vựng mở của các mô hình SILC. OWLv2 khởi tạo một mô hình phát hiện với trọng số của mô hình hình ảnh-văn bản tương phản và sử dụng các hộp được gắn nhãn giả từ WebLI (WebLI N-grams [34]) để học một mô hình phát hiện từ vựng mở. Chúng tôi sử dụng bàn kiểm tra của các tác giả và tái huấn luyện OWLv2 được khởi tạo từ các baseline VLM của chúng tôi để báo cáo kết quả trong Bảng 4. Chúng tôi đánh giá các mô hình này không-shot trên COCO và LVIS mà không thực hiện bất kỳ điều chỉnh nào trên bộ dữ liệu tương ứng để kiểm tra hiệu suất từ vựng mở của chúng.

Từ Bảng 4 chúng tôi quan sát rằng các mô hình SILC cũng có lợi cho phát hiện từ vựng mở nhờ vào việc học ngữ nghĩa cục bộ tốt hơn. SILC-S* đạt được cải thiện +1.8AP trên COCO. Các cải thiện cũng nhất quán trên tiêu chuẩn LVIS thách thức nơi SILC-S* đạt được cải thiện +1.4AP trên tất cả các lớp và đáng kể +2.0AP trên các lớp hiếm. Chúng tôi có quan sát tương tự khi so sánh SILC-C* với CLIP (WebLI) nơi SILC-C* mang lại cải thiện nhất quán. Điều này tiếp tục xác nhận rằng các mô hình SILC mang lại hiệu suất tốt hơn cho các tác vụ dày đặc.

4.6. Đánh giá đặc trưng SILC với LiT-Decoder.
LiT-Decoder [5] đề xuất sử dụng một bộ mã hóa hình ảnh đóng băng và huấn luyện một bộ giải mã tự hồi quy duy nhất để học một mô hình đa tác vụ cho Phân loại, Tạo chú thích và Trả lời Câu hỏi Thị giác. Chúng tôi sử dụng LiT-Decoder như một khung để đánh giá chất lượng biểu diễn thị giác được học bởi các mô hình SILC so với baseline CLIP (WebLI) và SigLIP. Chúng tôi sử dụng triển khai của các tác giả và chỉ thay thế ViT bằng các baseline tương ứng. Chúng tôi báo cáo kết quả trong Bảng 5. Chúng tôi quan sát rằng các mô hình SILC mang lại cải thiện nhất quán trong thiết lập đa tác vụ này. So với SigLIP, SILC-S* cải thiện phân loại +1.2 điểm trên ImageNet và +1.0 điểm trên SUN397. Các cải thiện thậm chí sâu sắc hơn trên tạo chú thích (+3.6 điểm CIDEr trên COCO) và Trả lời Câu hỏi Thị giác (+1.3 trên GQA và +1.5 trên VQAv2). Các tác vụ này được hưởng lợi rất nhiều từ khả năng mã hóa ngữ nghĩa cục bộ tốt hơn của các đặc trưng SILC. Các cải thiện tương tự được ghi nhận cho các baseline dựa trên CLIP. Điều này tiếp tục xác nhận rằng các đặc trưng SILC có thể đồng thời có lợi cho nhiều vấn đề thị giác máy tính.

4.7. Ablation trên Các Thành phần Mô hình.
Chúng tôi thực hiện ablation trên các lựa chọn thiết kế khác nhau của mô hình và tác động của chúng lên các tác vụ khác nhau. Chúng tôi huấn luyện tất cả các mô hình cho 5 Tỷ example-seen và báo cáo hiệu suất trong Bảng 6. Vì phương pháp của chúng tôi xử lý các phép biến đổi hình ảnh bổ sung trong tổn thất tương phản, trước tiên chúng tôi kiểm tra xem các cải thiện của chúng tôi có phải là hệ quả của việc xử lý nhiều phép biến đổi hơn hay không. Chúng tôi quan sát rằng việc giới thiệu các phép biến đổi hình ảnh bổ sung (hàng thứ hai) cải thiện các chỉ số phân loại và tìm kiếm nhưng tác động của chúng lên phân đoạn không-shot và phân đoạn từ vựng mở không đáng kể. Khi chúng tôi thêm EMA trên trọng số của mô hình này tương tự như mô hình của chúng tôi (hàng thứ ba), chúng tôi nhận thấy cải thiện nhỏ như đã thấy trong văn liệu SSL trước. Cuối cùng khi chúng tôi thêm tự chưng cất từ các cắt cục bộ, chúng tôi thấy cải thiện trên tất cả các tác vụ. Cụ thể, chúng tôi quan sát cải thiện mạnh nhất trên các tác vụ phân đoạn làm nổi bật tác động của đề xuất của chúng tôi lên chúng.

5. Kết luận.
Chúng tôi đề xuất tích hợp học tương ứng cục bộ-toàn cục bằng tự chưng cất như một mục tiêu bổ sung cho mục tiêu tương phản VLM phổ biến của CLIP [43] và SigLIP [60]. Chúng tôi chỉ ra rằng việc giới thiệu điều này dẫn đến cải thiện hiệu suất đáng kể trên một số tác vụ thị giác máy tính. Chúng tôi thấy cải thiện hiệu suất nhất quán trên phân loại không-shot, phân loại few-shot, và tìm kiếm. Chúng tôi tiếp tục kiểm tra VLM của chúng tôi trên phân đoạn không-shot và chỉ ra rằng khung huấn luyện của chúng tôi dẫn đến cải thiện đáng kể mà không sử dụng bất kỳ sự thật dày đặc nào. Cuối cùng chúng tôi chỉ ra rằng các mô hình SILC như các backbone được tiền huấn luyện cải thiện đáng kể hiệu suất của một mô hình trên phân đoạn từ vựng mở, phát hiện từ vựng mở, tạo chú thích và VQA. Các mô hình SILC thiết lập một trạng thái nghệ thuật mới trong Các Mô hình Cơ sở Thị giác-Ngôn ngữ.

Tài liệu tham khảo
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: một mô hình ngôn ngữ thị giác cho học few-shot. Trong NeurIPS, 2022. 2
[2] Relja Arandjelovi ´c, Alex Andonian, Arthur Mensch, Olivier J H ´enaff, Jean-Baptiste Alayrac, và Andrew Zisserman. Ba cách để cải thiện căn chỉnh đặc trưng cho phát hiện từ vựng mở. arXiv preprint arXiv:2303.13518, 2023.
[3] Lucas Beyer, Xiaohua Zhai, và Alexander Kolesnikov. Big vision. https://github.com/google-research/big_vision, 2022. 4
[4] Lucas Beyer, Xiaohua Zhai, và Alexander Kolesnikov. Baseline ViT đơn giản tốt hơn cho imagenet-1k, 2022. 4
[5] Lucas Beyer, Bo Wan, Gagan Madan, Filip Pavetic, Andreas Steiner, Alexander Kolesnikov, Andr ´e Susano Pinto, Emanuele Bugliarello, Xiao Wang, Qihang Yu, et al. Một nghiên cứu về bộ giải mã tự hồi quy cho đa tác vụ trong thị giác máy tính. arXiv preprint arXiv:2303.17376, 2023. 8
[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, và Matthijs Douze. Phân cụm sâu cho học không giám sát các đặc trưng thị giác. Trong ECCV, 2018. 2
[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, và Armand Joulin. Học không giám sát các đặc trưng thị giác bằng cách tương phản phân công cụm. Trong NeurIPS, 2020. 2
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, và Armand Joulin. Các tính chất nổi lên trong các transformer thị giác tự giám sát. Trong ICCV, 2021. 1, 2, 4
[9] Junbum Cha, Jonghwan Mun, và Byungseok Roh. Học để tạo ra mặt nạ có căn cứ văn bản cho phân đoạn ngữ nghĩa thế giới mở chỉ từ các cặp hình ảnh-văn bản. Trong CVPR, 2023. 3, 6
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, và Geoffrey Hinton. Một khung đơn giản cho học tương phản các biểu diễn thị giác. Trong ICML, 2020. 1, 2

--- TRANG 8 ---
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, và Radu Soricut. PaLI: Một mô hình ngôn ngữ-hình ảnh đa ngôn ngữ được mở rộng cùng nhau. Trong ICLR, 2023. 2, 5
[12] Seokju Cho, Heeseong Shin, Sunghwan Hong, Seungjun An, Seungjun Lee, Anurag Arnab, Paul Hongsuck Seo, và Seungryong Kim. Cat-seg: Tổng hợp chi phí cho phân đoạn ngữ nghĩa từ vựng mở. arXiv preprint arXiv:2303.11797, 2023. 1, 2, 3, 7
[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Mở rộng mô hình hóa ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311, 2022. 1
[14] Jian Ding, Nan Xue, Gui-Song Xia, và Dengxin Dai. Tách rời phân đoạn ngữ nghĩa không-shot. Trong CVPR, 2022. 1, 3, 7
[15] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Tự chưng cất có mặt nạ thúc đẩy tiền huấn luyện ngôn ngữ-hình ảnh tương phản. Trong CVPR, 2023. 2
[16] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Tiền huấn luyện thị giác-ngôn ngữ thô-tới-tinh với fusion trong backbone. Trong NeurIPS, 2022. 2
[17] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, và Vaishaal Shankar. Mạng lọc dữ liệu. arXiv preprint arXiv:2309.17425, 2023. 2
[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, và Tsung-Yi Lin. Mở rộng phân đoạn hình ảnh từ vựng mở với nhãn cấp độ hình ảnh. Trong ECCV. Springer, 2022. 2, 3
[19] Spyros Gidaris, Praveer Singh, và Nikos Komodakis. Học biểu diễn không giám sát bằng cách dự đoán xoay hình ảnh. Trong ICLR, 2018. 2
[20] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-một cách tiếp cận mới cho học tự giám sát. NeurIPS, 2020. 1
[21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, và Ross Girshick. Tương phản momentum cho học biểu diễn thị giác không giám sát. Trong CVPR, 2020. 2
[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, và Ross Girshick. Bộ tự động mã hóa có mặt nạ là những người học thị giác có thể mở rộng. Trong CVPR, 2022. 2
[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, và Tom Duerig. Mở rộng học biểu diễn thị giác và thị giác-ngôn ngữ với giám sát văn bản nhiễu. Trong ICML, 2021. 1, 2, 3
[24] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, và Anelia Angelova. F-vlm: Phát hiện đối tượng từ vựng mở trên các mô hình thị giác và ngôn ngữ đóng băng. ICLR, 2023. 1
[25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, và Rene Ranftl. Phân đoạn ngữ nghĩa dẫn dắt bởi ngôn ngữ. Trong International Conference on Learning Representations, 2022. 3
[26] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, và Rene Ranftl. Phân đoạn ngữ nghĩa dẫn dắt bởi ngôn ngữ. Trong International Conference on Learning Representations, 2022. 2
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, và Steven Chu Hong Hoi.

--- TRANG 9 ---
Căn chỉnh trước khi hòa trộn: Học biểu diễn thị giác và ngôn ngữ với chưng cất momentum. Trong NeurIPS, 2021. 2
[28] Junnan Li, Dongxu Li, Caiming Xiong, và Steven Hoi. Blip: Bootstrapping tiền huấn luyện ngôn ngữ-hình ảnh cho hiểu biết và tạo sinh thị giác-ngôn ngữ thống nhất. Trong ICML, 2022. 2
[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Tiền huấn luyện ngôn ngữ-hình ảnh có căn cứ. Trong CVPR, 2022. 2, 3
[30] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, và Junjie Yan. Giám sát tồn tại ở mọi nơi: Một mô hình tiền huấn luyện ngôn ngữ-hình ảnh tương phản hiệu quả dữ liệu. Trong ICML, 2021. 2
[31] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, và Diana Marculescu. Phân đoạn ngữ nghĩa từ vựng mở với clip thích ứng mặt nạ. Trong CVPR, 2023. 2, 3, 7
[32] Jiasen Lu, Dhruv Batra, Devi Parikh, và Stefan Lee. Vilbert: Tiền huấn luyện biểu diễn visiolinguistic bất tri tác vụ cho các tác vụ thị giác-và-ngôn ngữ. Trong NeurIPS, 2019. 2
[33] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, và Tianrui Li. Segclip: Tổng hợp patch với trung tâm có thể học cho phân đoạn ngữ nghĩa từ vựng mở. Trong ICLR, 2023. 2
[34] Matthias Minderer, Alexey Gritsenko, và Neil Houlsby. Mở rộng phát hiện đối tượng từ vựng mở. arXiv preprint arXiv:2306.09683, 2023. 3, 7, 8
[35] Ishan Misra và Laurens van der Maaten. Học tự giám sát các biểu diễn bất biến pretext. Trong CVPR, 2020. 2
[36] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, và Ser-Nam Lim. Phân đoạn ngữ nghĩa từ vựng mở với học tương phản căn chỉnh patch. Trong CVPR, 2023. 6
[37] Muhammad Ferjad Naeem, Yongqin Xian, Luc V Gool, và Federico Tombari. I2dformer: Học attention tài liệu từ hình ảnh cho phân loại hình ảnh không-shot. NeurIPS, 2022. 2
[38] Muhammad Ferjad Naeem, Muhammad Gul Zain Ali Khan, Yongqin Xian, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool, và Federico Tombari. I2mvformer: Giám sát tài liệu đa góc nhìn được tạo ra bởi mô hình ngôn ngữ lớn cho phân loại hình ảnh không-shot. Trong CVPR, 2023. 2
[39] Aaron van den Oord, Yazhe Li, và Oriol Vinyals. Học biểu diễn với mã hóa dự đoán tương phản. arXiv preprint arXiv:1807.03748, 2018. 3
[40] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Học các đặc trưng thị giác mạnh mẽ mà không có giám sát. arXiv preprint arXiv:2304.07193, 2023. 1, 2, 4
[41] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, và Alexei A Efros. Bộ mã hóa bối cảnh: Học đặc trưng bằng inpainting. Trong CVPR, 2016. 2
[42] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Cải thiện hiểu biết ngôn ngữ bằng tiền huấn luyện tạo sinh. Trong OpenAI, 2018. 1
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Học các mô hình thị giác có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong ICLR, 2021. 1, 2, 3, 4, 7, 8
[44] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, và Jonathon Shlens. Nhóm hóa nhận thức trong các mô hình thị giác-ngôn ngữ tương phản. Trong ICCV, 2023. 3, 5
[45] Gyungin Shin, Weidi Xie, và Samuel Albanie. Reco: Tìm kiếm và đồng phân đoạn cho chuyển giao không-shot. Trong NeurIPS, 2022. 3, 6
[46] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, và Douwe Kiela. Flava: Một mô hình căn chỉnh ngôn ngữ và thị giác cơ bản. Trong CVPR, 2022. 2
[47] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, và Yue Cao. Eva-clip: Các kỹ thuật huấn luyện được cải thiện cho clip ở quy mô lớn. arXiv preprint arXiv:2303.15389, 2023. 2
[48] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, và Lucas Beyer. Những người tạo chú thích hình ảnh cũng là những người học thị giác có thể mở rộng. arXiv preprint arXiv:2306.07915, 2023. 2
[49] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, và Lijuan Wang. Git: Một transformer tạo sinh hình ảnh-văn bản cho thị giác và ngôn ngữ. TMLR, 2022.
[50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, và Yuan Cao. Simvlm: Tiền huấn luyện mô hình ngôn ngữ thị giác đơn giản với giám sát yếu. Trong ICLR, 2022. 2
[51] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, và Xiaolong Wang. Groupvit: Phân đoạn ngữ nghĩa xuất hiện từ giám sát văn bản. Trong CVPR, 2022. 2, 6
[52] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, và Xiang Bai. Một baseline đơn giản cho phân đoạn ngữ nghĩa từ vựng mở với mô hình thị giác-ngôn ngữ được tiền huấn luyện. Trong ECCV. Springer, 2022. 1, 3, 7
[53] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, và Chunjing Xu. Filip: Tiền huấn luyện ngôn ngữ-hình ảnh tương tác tinh vi. Trong ICLR, 2021. 2
[54] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, và Hang Xu. Detclip: Tiền huấn luyện song song khái niệm thị giác được làm giàu từ điển cho phát hiện thế giới mở. NeurIPS, 2022. 3
[55] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, và Hang Xu. Detclipv2: Tiền huấn luyện phát hiện đối tượng từ vựng mở có thể mở rộng thông qua căn chỉnh từ-vùng. Trong CVPR, 2023. 3
[56] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, và Yonghui Wu. Coca: Những người tạo chú thích tương phản là các mô hình cơ sở hình ảnh-văn bản. Trong TMLR, 2022. 2
[57] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, và Liang-Chieh Chen. Convolution chết cứng: Phân đoạn từ vựng mở với clip tích chập đóng băng đơn lẻ. NeurIPS, 2023. 1

--- TRANG 10 ---
[58] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, và Lucas Beyer. Mở rộng vision transformer. Trong CVPR, 2022. 5
[59] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, và Lucas Beyer. Lit: Chuyển giao không-shot với điều chỉnh văn bản-hình ảnh khóa. Trong CVPR, 2022. 2
[60] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, và Lucas Beyer. Tổn thất sigmoid cho tiền huấn luyện hình ảnh ngôn ngữ. Trong ICCV, 2023. 1, 2, 3, 4, 5, 6, 8
[61] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, và Jianfeng Gao. Glipv2: Thống nhất định vị và hiểu biết thị giác-ngôn ngữ. NeurIPS, 2022. 3
[62] Richard Zhang, Phillip Isola, và Alexei A Efros. Tô màu hình ảnh đầy màu sắc. Trong ECCV, 2016. 2
[63] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Tiền huấn luyện ngôn ngữ-hình ảnh dựa trên vùng. Trong CVPR, 2022. 2, 3
[64] Chong Zhou, Chen Change Loy, và Bo Dai. Trích xuất nhãn dày đặc miễn phí từ clip. Trong ECCV, 2022. 2, 5, 6
[65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, và Tao Kong. Image BERT tiền huấn luyện với tokenizer trực tuyến. Trong ICLR, 2022. 2, 4
[66] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr¨ahenb ¨uhl, và Ishan Misra. Phát hiện hai mươi ngàn lớp bằng cách sử dụng giám sát cấp độ hình ảnh. Trong ECCV. Springer, 2022. 3

--- TRANG 11 ---
11
