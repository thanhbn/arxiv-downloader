# 2308.04152.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2308.04152.pdf
# File size: 9694110 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FINE-TUNING MULTIMODAL LLM S TO FOLLOW
ZERO-SHOT DEMONSTRATIVE INSTRUCTIONS
Juncheng Li1, 2‚àóKaihang Pan1‚àóZhiqi Ge1‚àóMinghe Gao1‚àóWei Ji2Wenqiao Zhang1‚Ä†
Tat-Seng Chua2Siliang Tang1‚Ä†Hanwang Zhang3Yueting Zhuang1‚Ä†
1Zhejiang University,2National University of Singapore,3Nanyang Technological University
ABSTRACT
Recent advancements in Multimodal Large Language Models (MLLMs) have
been utilizing Visual Prompt Generators (VPGs) to convert visual features into
tokens that LLMs can recognize. This is achieved by training the VPGs on
millions of image-caption pairs, where the VPG-generated tokens of images are
fed into a frozen LLM to generate the corresponding captions. However, this
image-captioning based training objective inherently biases the VPG to concen-
trate solely on the primary visual contents sufficient for caption generation, of-
ten neglecting other visual details. This shortcoming results in MLLMs‚Äô under-
performance in comprehending demonstrative instructions consisting of multi-
ple, interleaved, and multimodal instructions that demonstrate the required con-
text to complete a task. To address this issue, we introduce a generic and
lightweight Visual Prompt Generator Complete module ( VPG-C ), which can in-
fer and complete the missing details essential for comprehending demonstra-
tive instructions. Further, we propose a synthetic discriminative training strat-
egy to fine-tune VPG-C , eliminating the need for supervised demonstrative in-
structions. As for evaluation, we build DEMON , a comprehensive benchmark for
demonstrative instruction understanding. Synthetically trained with the proposed
strategy, VPG-C achieves significantly stronger zero-shot performance across all
tasks of DEMON . Further evaluation on the MME and OwlEval benchmarks also
demonstrate the superiority of VPG-C . The code and models are available at
https://github.com/DCDmllm/Cheetah .
1 I NTRODUCTION
Recent advances in Multimodal Large Language Models (MLLMs) (Li et al., 2023c; Liu et al., 2023;
Zhu et al., 2023a) have exhibited promising capabilities in processing single-image instructions,
such as producing detailed image descriptions and answering questions about the image. However,
they fall short in demonstrative instructions consisting of multiple, interleaved, and multimodal
instructions that demonstrate the required context to complete a task. For instance, the instruction
in Figure 1 contains interleaved visual and textual context, requiring the model to determine the
authenticity of the milk in the second image based on the official image provided in the first.
An MLLM should at least have the following two capabilities to comprehend demonstrative instruc-
tions effectively:
1) Not just the primary subject: Beyond focusing on the primary visual content, it should be able
to meticulously discern the details within the demonstrations. These details, complementing the
primary content, play a crucial role in semantically connecting the instructions. A case in point is
Figure 1, wherein accurate discernment relies on recognizing the logo detail on a milk carton.
2) Reasoning-aware details: How to decide what details are complementary to the reasoning? We
expect that an MLLM may ‚Äúthink twice‚Äù, that is, given a preliminary reasoning using the primary
contents, it would know what additional contents are needed as complementary details. For example,
in Figure 1, after preliminary reasoning, the model should re-attend details such as the logo and
‚àóEqual Contribution.‚Ä†Corresponding Authors.
1arXiv:2308.04152v4  [cs.CV]  25 May 2024

--- PAGE 2 ---
Ours-Answer: No,the cow icon within the red diamond-shaped box on the purchased milk packaging is noticeably different from that of the genuine milk.Compared to the genuine milk displayed in                             ,please examine the packaging design of  mypurchased milkshown in                         . Is the purchased milk  genuine?
Demonstrative Instruction
Stage2: attentionmapsafterpreliminary reasoning
Stage1: initialattentionmaps
InstructBLIP-Answer: Yes, the purchased milk is genuine.
VisualizedattentionmapsoftheInstructBLIP‚ÄôsVPG.
Figure 1: An example of InstructBLIP (Dai et al., 2023) and our MLLM enhanced by VPG-C .
brand name on the milk carton, thereby discerning its authenticity. However, to follow zero-shot
demonstrative instructions, this ‚Äúreasoning-aware‚Äù capability should be acquired without the need
for supervised demonstrative instructions.
Unfortunately, we find that the reason why existing MLLMs are not effective in demonstrative
instructions is due to the lack of the above capabilities. More specifically, the crux lies in the
Visual Prompt Generator (VPG) in MLLMs. VPG, such as Q-former (Li et al., 2023c) and Re-
sampler (Alayrac et al., 2022), translates visual features into tokens recognizable by LLMs, and
the translation is trained on millions of image-caption pairs by feeding the VPG-generated tokens
of images into a frozen LLM which generates the corresponding captions. However, this image
captioning training strategy inevitably introduces the inductive bias that VPG only focuses on the
primary visual contents which are just enough for the captioning task, but tends to omit other visual
details. For example in Figure 1, the averaged attention map of InstructBLIP (Dai et al., 2023) (Fig-
ure 1) shows a dominant focus on the primary contents, neglecting the logo detail, which is however
the key to answering the question.
ImageEncoderVisualPromptGenerator¬∑¬∑¬∑√ó	ùêø2√ó	ùêø2‚®Å¬∑¬∑¬∑
¬∑¬∑¬∑Residual Visual DetailsLLMLayerGuidanceLLMLayer
OutputLLM
Figure 2: An overview of VPG-C .To this end, we propose a lightweight Visual
Prompt Generator Complete module ( VPG-C ),
which can infer and complete the missing de-
tails essential for comprehending demonstra-
tive instructions (Section 2.1). As shown Fig-
ure 2, 1) VPG-C first derives the instruction-
specific guidance by intercepting the interme-
diate LLM‚Äôs output of the primary contents
extracted by a conventional VPG, and then
2) guides the VPG to recover the missing visual residual details. Finally, 3) these residual details are
then seamlessly reintegrated into the intermediate LLM‚Äôs layer via a skip connection. Together with
the original intermediate output, VPG-C is expected to provide an improved comprehension of the
demonstration instructions. Yet, VPG-C is not ready to follow zero-shot demonstrative instructions
because the ‚ÄúGuide‚Äù step requires fine-tuning to specialize in missing detail recovery. Therefore,
we propose a synthetic discriminative training strategy to fine-tune VPG-C, without the need for the
expensive data collection of ‚Äúdetail-caption‚Äù pairs (Section 2.2).
To evaluate VPG-C and diagnose existing MLLMs, we build DEMON , a comprehensive benchmark
for demonstrative instruction understanding, covering 31 diverse tasks across 7 categories, as shown
in Figure 4 (Section 3). Systematic evaluation on DEMON confirms the limitation of existing MLLMs
in demonstrative instructions. Without additional demonstrative instruction data, the lightweight
VPG-C module can be effectively tuned by the synthetic training strategy in several hours with a
single A100 GPU. While computation- and data- efficient, VPG-C significantly outperforms ex-
isting MLLMs on the DEMON benchmark. Zero-shot evaluation on other multimodal instruction
benchmarks (Fu et al., 2023; Ye et al., 2023) also indicates considerable improvement by VPG-C .
2 M ETHOD
2.1 V ISUAL PROMPT GENERATOR COMPLETE
As illustrated in Figure 2, VPG-C is built upon the frozen LLM (Vicuna-7B (Chiang et al., 2023))
and vision encoder (EV A-CLIP (Fang et al., 2023)). We adopt the widely used Q-Former from BLIP-
2

--- PAGE 3 ---
2 (Li et al., 2023c) as our visual prompt generator. VPG-C first uses the intermediate output of the
LLM to infer instruction-specific guidance. This then assists the VPG in attending to the missing
visual details from the images. By merging these residual details back via a skip connection, VPG-C
achieves a thorough grasp of the demonstrative instruction.
Given a demonstrative instruction, we first adopt the Q-former to generate general visual prompts
for each image in the instruction. Q-former takes a fixed number of Kquery vectors to in-
teract with image features by several cross-attention layers, and the output query representa-
tions are used as visual prompts, which are inserted into the position of their corresponding im-
ages in the instruction. We denote the input instruction for the language decoder as H0=
{h0
1,h0
2, ...,v0
11, ...,v0
1K, ...,h0
i, ...,v0
j1, ...,v0
jK, ...,h0
N}, where h0
irepresents the i-th text token
andV0
j={v0
j1, ...,v0
jK}represents the Kvisual prompts for the j-th interleaved image. Taking the
instruction as input to the L-layer language decoder, we then extract the hidden representation of
the last input token hL/2
Nat theL
2-th layer, which can fully perceive the whole multimodal context
during the firstL
2layers and contains comprehensive instruction-aware semantics. Next, we infer
the instruction-specific guidance gfromhL/2
Nvia a linear projection layer: g=Linear (hL/2
N).
After obtaining the instruction-specific guidance from the language decoder, we compose it with a
new set of learnable queries: g+Q, where Q ‚ààRK√ódandgis added to each query of Q. Then,
we reuse the same Q-former with the above conditionally generated queries to attend to residual
visual details, thus obtaining the visual prompts Vj={vj1, ...,vjK}for each image j, which
contains the complementary details missed by the original visual prompts. Finally, the transformed
Vjis reintegrated with the corresponding original intermediate representations of VL/2
j, via skip
connection: ÀúVL/2
j=VL/2
j+Linear (Vj), which is taken as the input to the (L
2+ 1) -th layer.
Efficient training. OurVPG-C module is parameter-efficient as the Q-former is frozen and only a
set of query embeddings and two linear projection layers need to be fine-tuned, which only account
for0.09% ( ‚àº6.3M) of the entire model. To stabilize the training process (Zhang & Agrawala, 2023),
we initialize the linear projection layers with zeros. Thus, at the early training stage, the input to the
(L
2+ 1) -th layer can be converted to: ÀúVL/2
j=VL/2
j, which will not cause any influence on LLMs.
Analysis on inserting VPG-C in the intermediate layer (L
2): 1) Guidance generation. Previous
studies have shown that features provided by the intermediate layer may suffice to preliminarily
understand the given input samples (Xin et al., 2020) and can serve as guidance hints to improve
training (Romero et al., 2014). Thus, generating guidance in the intermediate layer allows the model
to form a preliminary understanding of the given instruction. Generating guidance too early could
be problematic, as the model might not have gathered sufficient contextual information to generate
effective guidance. Conversely, generating guidance too late could result in the model‚Äôs attention
being narrowly focused on what it perceives as the final answer, hindering its ability to guide the
Q-former to extract relevant details from the images. Therefore, placing the guidance generation
step in the intermediate layer strikes a balance. 2) Detail reintegration. Intermediate-layer rein-
tegration of residual visual details preserves prior knowledge and allows subsequent layers to in-
tegrate new information effectively. Reintegrating residual details too early in the pipeline might
overwrite important context, while reintegrating it too late could limit the impact on the model‚Äôs
reasoning. Therefore, the intermediate layer offers a strategic position for residual details reinte-
gration, enabling the model to reason effectively and arrive at the correct answers by leveraging the
complemented visual residual details. We further provide quantitative analysis in Section 4.4.
2.2 S YNTHETIC DISCRIMINATIVE TRAINING STRATEGY
The proposed training strategy diagnoses the areas initially ignored by Q-former according to its
cross-attention maps between the queries and the image features, and generates a synthetic image
by performing several types of editing on the ignored areas. Then, an inter-image discriminative
task is formulated as describing the subtle difference between the original and the synthetic images.
Considering the edits are performed in the mostly ignored areas, VPG-C is challenged to recover
the missing details to describe the difference. An overview is illustrated in Figure 3.
Editing target identification. The Q-former takes the queries to interact with frozen image fea-
tures through several cross-attention layers and uses the output query representations as the visual
3

--- PAGE 4 ---
Q-FormerAttention MapInputImageSegment MapEditing Targetignoredareas
‚Äú red‚Ä¶‚Äù
‚Äú stones‚Ä¶‚Äù‚Äú horse‚Ä¶‚ÄùGenerate suitable editing description
Segment AnythingBlendedDiffusion
Deleting Object
Adding ObjectSwaping Object‚Äú remove‚Ä¶‚Äù
Cross-Attention Guided Synthetic Image EditingModifying Object
Inter-Image Discriminative TrainingDescribe the difference between the images.Instruction:
12OriginalImageEditedImageResponse:In the first image, the car on the road is yellow, whereas in the second image, it is red.MLLMFigure 3: Pipeline demonstration of synthetic discriminative training strategy for VPG-C .
prompts. Therefore, the cross-attention maps between queries and image features reflect the interest
of queries. We average the cross-attention maps across all layers and all queries to obtain the global
cross-attention map A, where the value Aijindicates the significance degree of the corresponding
image feature by the original task-agnostic Q-former queries. After that, we employ the advanced
vision foundation model (Kirillov et al., 2023) to obtain all the objects with segmentation masks
in the image. Then, the significance degree of each object Œ¶(oi)is computed based on the cross-
attention map Awith RoIAlign (He et al., 2017), where we average the values of Awithin the mask
miby interpolation. Œ¶(oi)reflects what degree the visual features of object oiis extracted by the
Q-former. Thus, we select the most ignored objects based on the Œ¶(oi)value.
Editing description generation. We define four types of editing: modifying objects, swapping
objects, deleting objects, and adding objects. Given the selected object, we instruct ChatGPT (Ope-
nAI, 2023a) to generate a suitable editing description that is in harmony with the context, where
ChatGPT is prompted with the corresponding image caption and detailed object information ( i.e.,
labels, positions). For adding objects, we only select BACKGROUND objects to add objects.
Synthetic image generation. After obtaining the editing description, we generate the synthetic
image using a text-to-image latent diffusion model ( i.e.,Blended Diffusion (Avrahami et al., 2022)).
Blended Diffusion performs local editing on the image according to the target object mask and the
editing description, thus rendering the synthetic image. To ensure quality, we filter the edited images
using CLIP similarity (Radford et al., 2021b).
Inter-image discriminative training. Given the original image and the synthetic image pair, along
with the task instruction ( ‚ÄúDescribe the difference between the images‚Äù ), the inter-image discrimi-
native training task is defined as generating sentences to describe the subtle difference between the
images. We convert the editing description to acquire the ground-truth sentences.
3 DEMON B ENCHMARK
Data format. All task instances are transformed into a unified instruction-response form for zero-
shot evaluation. Formally, each instance in DEMON consists of the following components:
‚Ä¢Task_Instruction : provides a complete natural language definition of a given task, includ-
ing the input/output format and the task objective.
‚Ä¢Task_Instance : is a concrete instance of a given task that consists of demonstrative image-
text sequential context ( e.g., visually-rich textbooks, specific questions about the context).
‚Ä¢Response : represents the target output in natural language for a given task instruction and task
instance. For classification tasks, we convert the class labels as options into the instruction and
ask the model to output the option index in natural language as the response.
Without any specific emphasis, we use the term ‚Äúinstruction‚Äù to refer to the combination
ofTask_Instruction andTask_Instance . For each task, we manually design 10
Task_Instruction templates in natural language to increase the diversity.
Task collection and categorization. To comprehensively benchmark the demonstrative instruc-
tion following ability, we extensively gather a wide variety of multimodal datasets from different
fields and scenarios. As illustrated in Figure 4, DEMON has three important properties: 1) Demon-
strative vision-language context, all the instructions contain sequences of inter-related images and
texts, such as storyboards with scripts, and textbooks with diagrams. 2) Diverse forms of complex
4

--- PAGE 5 ---
Category: Visual StorytellingFolks of all ages come out to peruse the crafts for sale.Response:Some of the crafters even dress up in unique costumes as part of their selling act.
4
5
1The local parish holds a craft show each year.
2Lots of folks come out and set up tables to sell their crafts.
3Some of these crafts are very uniqueand take a lot of talent to make.Instruction:Given the stories paired with the first several images, can you finish the story based on the last image?Folks of all ages come out to peruse the crafts for sale.Category: Visual Relation Inference
There are three people in the photo on the left and none in the photo on the right.What modifications can be observed between these two pictures? 
1
2
Category: Multi-Modal Cloze
1
2
3Instruction:With a sequence of comic panels provided, could you identify the most relevant next panel from the available choices?
ABC
DResponse:Panel C.Category: Text-Rich Images QAInstruction:Please analyze the data in these slides and accordingly answer the proposed question.
1
2
Question: How many profiles are viewed every day on the social website that has over 350 million members and where the average user has 240 connections?Response:Over 25 million.Category: Multi-Modal DialogueAs an intelligent assistant, your ultimate objective is to place a soap container on a counter. Start by carefully observing your surroundings and making decisions step by step.
1Turn left and walk to the right sink.
3
2
Take a step left.Open the bottom left cabinet and take out the closest soap container.Category: Multi-Image ReasoningInstruction:Instructions&Responses:
Response:
What should be done next?What‚Äôsthenextstep?
OK.Whatfollows?
‚Ä¶
1
3Are these three pictures of clothes similar in their style? 
Yes.Theyareinsimilarstyle.Instruction:
2
Category: KnowledgeGroundedQAFolks of all ages come out to peruse the crafts for sale.Response:A.a decrease in carbon dioxide, increase in pH, or a decrease in temperature.Thegraphshowsanoxygendissociationcurveforhemoglobin.Basedonthegraph,whatwouldlikelycausethecurvetoshifttotheleft,asshownbythedottedplotline?Instruction:Provided with a series of diagrams from a textbook, your responsibility is to correctly answer the following question.
1
3A.adecreaseincarbondioxide,increaseinpH,oradecreaseintemperatureB.anincreaseincarbondioxide,increaseinpH,oradecreaseintemperatureC.adecreaseincarbondioxide,decreaseinpH,oradecreaseintemperatureD.adecreaseincarbondioxide,increaseinpH,oranincreaseintemperature
2Response:Figure 4: Demonstrations and task taxonomy of the proposed DEMON benchmark.
instructions, the instructions range from designing panels for comics, to discovering differences
between surveillance images, and to conversational embodied tasks. 3) Vast range of instruction-
following scenarios, the benchmark covers multiple practical scenarios, including cartoons, indus-
trial visuals, driving recordings, recipes, etc.
Evaluation protocols. Thanks to the unified task format of DEMON , all tasks can be evaluated in a
zero-shot manner. For the open-ended generation tasks, we adopt ROUGE-L for evaluation. For the
tasks that require the models to output option indexes, we take Accuracy as the evaluation metric.
While well-formated options are provided, we empirically observe that many MLLMs struggle to
strictly follow instructions to output the option indexes but generate free-form text. Thus, when
models do not exactly output the required options, we match their outputs to one of the given options
based on the TF-IDF distance, which we find is more robust than model-based methods (OpenAI,
2023a; Reimers & Gurevych, 2019). Since we explore a large number of tasks, we take a maximum
of 500 instances per task for evaluation efficiency and exclude several datasets that are difficult to
obtain and are subject to strict copyright restrictions (referred to as DEMON-Core ). Meanwhile,
we report the full version of the benchmark to facilitate future research on large-scale multimodal
instruction tuning (referred to as DEMON-Full ). Unless specifically stated, we use DEMON to refer
toDEMON-Core in the following.
Table 1: Detailed statistics of DEMON benchmark.
Tasks Scenarios Images Instructions Avg. Images / Instruction Avg. Words / Instruction
DEMON-Core 29 19 62.81K 18.18K 3.46 92.69
DEMON-Full 31 20 1.77M 477.72K 3.70 97.58
Benchmark analysis. Table 1 details the statistics. DEMON benchmark covers 31 tasks of 7 cat-
egories across 20 scenarios. In total, DEMON-Full includes 477.72K instruction-response pairs,
5

--- PAGE 6 ---
Table 2: Average results of zero-shot evaluation on each task category of DEMON Benchmark.
Multimodal Visual Visual Relation Multimodal Knowledge Text-Rich Multi-Image
Dialogue Storytelling Inference Cloze Grounded QA Images QA Reasoning
BLIP-2 (Li et al., 2023c) 26.12 21.31 10.67 17.94 39.23 33.53 39.65
InstructBLIP (Dai et al., 2023) 33.58 24.41 11.49 21.20 47.40 44.40 48.55
LLaMA-Adapter V2 (Gao et al., 2023) 14.22 17.57 13.51 18.00 44.80 32.00 44.03
LLaV A (Liu et al., 2023) 7.79 10.70 8.27 15.85 36.20 28.33 41.53
MiniGPT-4 (Zhu et al., 2023a) 13.69 17.07 7.95 16.60 30.27 26.40 43.50
mPLUG-Owl (Ye et al., 2023) 12.67 19.33 5.40 16.25 33.27 32.47 42.50
OpenFlamingo (Awadalla et al., 2023) 16.88 24.22 13.85 21.65 32.00 30.60 41.63
Otter (Li et al., 2023a) 15.37 15.57 11.39 16.00 41.67 27.73 43.85
VPG-C 37.50 25.20 25.90 22.15 48.60 44.93 50.28
serving as a large-scale benchmark for demonstrative instruction following. On average, each in-
struction contains 3.70 images and 97.58 words. Please refer to Appendix B for more details.
4 E XPERIMENTS
4.1 Z ERO-SHOT EVALUATION ON DEMON B ENCHMARK
Comparison with advanced MLLMs. In this section, we conduct comprehensive evaluation of our
VPG-C and the recent advanced MLLMs on the proposed DEMON benchmark. For all methods, we
choose versions with parameter sizes less than 10B. Please refer to Appendix D, F for details. The
average results of each task category are summarized in Table 2, which indicates the following.
‚Ä¢VPG-C consistently outperforms existing models by a large margin across all categories, which
demonstrates the stronger generalizability to follow such complicated demonstrative instructions.
‚Ä¢ While previous works mainly fine-tune on massive multimodal instruction data, VPG-C still
achieves the highest performance using synthetic training data with much lower computation cost.
This validates the effectiveness of the proposed VPG-C module and its synthetic training strategy.
‚Ä¢ Compared with previous works that fine-tune the large-scale language decoder or visual en-
coder ( i.e.,LLaV A, mPLUG-Owl), our model only tunes the lightweight VPG-C module with
6.3M parameters and achieves significant performance gain.
‚Ä¢VPG-C exhibits significant superiority in several challenging tasks. For instance, VPG-C sur-
passes SOTA methods by 3.92% on multimodal dialogue, which requires models to effectively
associate the interleaved images mentioned in different turns of the dialogue.
Innovative findings. The extensive evaluation on DEMON benchmark reveals several key findings.
‚Ä¢Poor performance on demonstrative instructions. While several models ( e.g., OpenFlamingo,
Otter, mPLUG-owl) have been trained on interleaved vision-language data, such as mmc4 (Zhu
et al., 2023b), they still struggle to perform well on the demonstraive instructions. We suppose
that while mmc4 contains sequences of interleaved images as context, the web-crawled images
are often weakly related. In contrast, the images and text in demonstrative instructions are highly
related, requiring models to deeply associate them to comprehend the task intents.
‚Ä¢Limited instruction following ability. Despite existing vision-language models leveraging state-
of-the-art LLMs, which have demonstrated impressive ability in following language instructions,
this competence seems to falter when dealing with complex multimodal instructions. For instance,
when tasked with selecting the correct answer from a choice list given the context of images and
texts, we observed some models inclining more towards describing the contents of the images
instead of addressing the posed questions. This is perceived as a deficiency in the image-text
alignment training process, to which we attribute the discrepancy.
‚Ä¢Failing to process image-choice questions. When dealing with multimodal cloze tasks, all mod-
els are limited to processing instructions that involve images as options. We hope future work to
utilize the new benchmark to make progress on this type of demonstrative instructions.
4.2 Z ERO-SHOT EVALUATION ON MME B ENCHMARK
We evaluate our VPG-C on the concurrently proposed MME benchmark (Fu et al., 2023) to further
illustrate its strong generalizability to follow a diverse range of single-image instructions. MME
benchmark measures both perception and cognition abilities on a total of 14 subtasks. We report
6

--- PAGE 7 ---
Table 3: Zero-shot evaluation of perception and cognition abilities on MME benchmark.
BLIP-2 InstructBLIP LA-V2 LLaV A MiniGPT-4 mPLUG-Owl Otter VPG-C
Perception 1293.84 1212.82 972.67 502.82 866.57 967.34 1292.26 1299.24
Cognition 290.00 291.79 248.93 214.64 292.14 276.07 306.43 321.07
the averaged results of perception tasks and cognition tasks in Table 3, respectively. While we
do not use massive multimodal instruction data to fine-tune VPG-C ,VPG-C still achieves superior
performance, compared with the supervised instruction-tuned models. This indicates our method
effectively overcomes the inherent limitation of VPGs and the completed residual details are also
essential for single-image instructions. Please refer to Appendix E for detailed results.
4.3 H UMAN EVALUATION ON GENERAL -PURPOSE LANGUAGE GENERATION
OpenFlamingoInstructBLIPMiniGPT-4mPLUG-OwlVPG-C1.692.533.243.624.19
Figure 5: Human evaluation.We further conduct human evaluation on the OwlEval
benchmark (Ye et al., 2023), which contains 82 open-
ended questions including advertisement and poem cre-
ation, diagram and flowchart comprehension, and teach-
ing,etc. Specifically, we recruit 8 well-educated people
to rank the randomly shuffled responses from VPG-C ,
MiniGPT-4, mPLUG-Owl, OpenFlamingo, and Instruct-
BLIP. The scores range from 1 to 5 (5 means best) and are
allowed to be equal for comparable instances. As shown
in Figure 5, VPG-C also demonstrates better open-ended
language generation ability in various practical cases.
4.4 I N-DEPTH ANALYSIS
Effectiveness of individual components. We investigate the effectiveness of each component in
Table 4. We start with the backbone model that uses the Q-former to generate visual prompts.
1)Instead of applying VPG-C to capture missing details, we first attempt a simple heuristic-based
method that directly extracts the less attended visual features according to the cross-attention maps
of Q-former and reintegrates them to the intermediate layer of the LLM as ours. We fine-tune a linear
layer before reintegrating with 0.5 million image-caption pairs. The results of Row 2 show that such
a sample heuristic can bring some improvement. This validates the importance of re-extracting
missing visual features from images for comprehending demonstrative instructions .2)Then, we
replace it with VPG-C and train it only using the image-caption pairs without synthetic training. The
results of Row 3 demonstrate that VPG-C can more accurately complete the required missing details
by leveraging the intermediate inference results of the LLM. 3)However, solely using common
image-caption data can not fully unleash the power of VPG-C . Comparing Row 3 and Row 4, we
observe a significant improvement for all tasks, indicating that the proposed synthetic discriminative
training strategy can methodically empower VPG-C to extract missing visual details.
VPG-C can better guide VPGs. Since InstructBLIP can perform conditional visual feature ex-
traction, we implement a variant version that concatenates its initially generated answer with the
instruction as condition to re-extract features. The initial generated answer serves as an additional
heuristic from the LLM for guiding feature extraction. Then, the newly extracted visual prompts
are used to re-generate answers. For a fair comparison, we provide a zero-shot version (Row 5) and
a fine-tuned version (Row 6) using synthetic training as ours. As shown in Table 4, directly using
synthetic data and inferred answers as heuristic conditions fails to yield a notable improvement. In
contrast, VPG-C can better guide the VPG to complete the missing visual details by intercepting the
intermediate representations of the LLM. Further, VPG-C is more computation-efficient as it only
requires one full forward pass of the LLM, while the InstructBLIP variants require twice.
VPG-C works well on various language backbones. Table 4 also validates that our approach can
well cooperate with language backbones of different families (LLaMA2) and sizes (Vicuna-13B) .
VPG-C can be implemented with very simple VPG. As a generic method, VPG-C can be im-
plemented with different VPGs. Beyond the widely used Q-former that is composed of multiple
Transformer blocks, we further probe the effectiveness of VPG-C with a simpler VPG, i.e.,Linear
Projection, as used in LLaV A (please refer to Appendix C for implementation details). Table 4
Row 7 shows promising results. VPG-C can also significantly bolster the performance of a simple
7

--- PAGE 8 ---
Table 4: Ablation results on DEMON Benchmark.
Multimodal Visual Visual Relation Multimodal Knowledge Text-Rich Multi-Image
Dialogue Storytelling Inference Cloze Grounded QA Images QA Reasoning
1 Backbone 25.65 21.72 9.33 17.06 37.21 32.42 41.30
2 +Heuristic Details 28.13 22.76 12.69 18.81 38.75 34.14 43.26
3 +VPG-C 31.76 23.62 19.12 20.09 42.53 39.68 46.71
4 +Synthetic Training 37.50 25.20 25.90 22.15 48.60 44.93 50.28
InstructBLIP 33.58 24.41 11.48 21.20 47.40 44.40 48.55
5 +Answer Condition 32.10 23.76 11.02 21.86 47.94 42.08 49.01
6 +Synthetic Training 31.76 24.32 12.78 19.87 46.58 42.36 49.82
LLaV A 7.79 10.70 8.27 15.85 36.20 28.33 41.53
7 Linear VPG 16.43 19.48 14.75 18.54 41.32 36.87 46.02
8VPG-C-LLaMA2-7B 42.70 24.76 25.50 22.95 51.00 44.93 48.68
9VPG-C-Vicuna-13B 38.14 26.59 27.15 27.15 52.93 49.33 53.65
linear VPG, verifying the transferability of VPG-C . It is promising to adapt our generic VPG-C and
corresponding low-resource synthetic training strategy to different VPGs in the future.
4 8 12 16 20 24 28
Insertion Layer of VPG-C1520253035404550
Accuracy
ROUGE-L
Figure 6: Performance on DEMON with
different insertion layers.Analysis on the inserted layer of VPG-C .We investi-
gate the impact of inserting VPG-C into different layers
of LLMs. We report the averaged accuracy for multiple-
choice tasks and averaged ROUGE-L for open-ended
generation tasks in Figure 6. We observe that the per-
formance is low when we insert VPG-C too early ( i.e.,
4, 8) as the model might not have gathered sufficient con-
textual information to generate effective guidance. Mean-
while, inserting VPG-C too late ( i.e.,24, 28) degenerates
the performance. We speculate this is due to the gener-
ated guidance being too concentrated and there not being
enough layers to integrate the residual details.
Table 5: Efficiency analysis of
synthetic training.
Accuracy ROUGE-L
16K 38.93 25.67
32K 39.62 27.38
48K 40.45 28.81
64K 41.49 29.53
80K 41.62 29.73
96K 40.12 28.31Synthetic training is data-efficient. Since our proposed synthetic
training strategy can construct challenging discriminative tasks in
a targeted manner, enhancing VPG-C ‚Äôs ability to complete missing
details, it avoids the need for a large amount of supervised demon-
strative instruction data. We further investigate the impact of differ-
ent numbers of synthetic training data. As illustrated in Table 5, the
performance keeps increasing when the number of data is increased
from 16K to 64K. Beyond this, escalating the data count from 64K
to 80K yields only marginal enhancement. Further amplification of data eventually triggers a per-
formance dip as excessive data leads to model overfitting to the synthetic training task.
/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni00000030/uni00000052/uni00000047/uni00000044/uni0000004f
/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f
/uni00000036/uni00000057/uni00000052/uni00000055/uni0000005c
/uni00000037/uni00000048/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f
/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni00000030/uni00000052/uni00000047/uni00000044/uni0000004f
/uni00000026/uni0000004f/uni00000052/uni0000005d/uni00000048/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000048/uni00000047
/uni00000034/uni00000024/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000035/uni0000004c/uni00000046/uni0000004b
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056
/uni00000034/uni00000024/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048
/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000039/uni00000033/uni0000002a/uni00000010/uni00000026
/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000010/uni0000002a/uni00000033/uni00000037/uni00000017
/uni00000032/uni00000053/uni00000048/uni00000051
/uni00000029/uni0000004f/uni00000044/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000052
/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000025/uni0000004f/uni0000004c/uni00000053
/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013
Figure 7: Analysis on image order sensitivity.Image order sensitivity. The order of inter-
leaved images in demonstrative instructions is
pivotal for the compositional semantics of the
instruction. Intuitively, altering the order of
images within a demonstrative instruction can
significantly shift its semantics. Consequently,
variations in model performance can reveal the
model‚Äôs sensitivity to the instruction semantics.
An ideal model should keenly capture changes
in instruction semantics. Therefore, we visu-
alize the performance variations of models by
randomly shuffling the order of interleaved im-
ages within the demonstrative instructions. Ac-
cording to Figure 7, we surprisingly find that SOTA models are less sensitive to the image order.
In contrast, VPG-C can keenly capture the semantic changes caused by the shuffled image order.
Particularly, our performance varies dramatically in multimodal dialogue, as the order of images
within these tasks is closely intertwined with the dialogue content.
Qualitative examples. As illustrated in Figure 8, VPG-C demonstrates strong abilities to perform
reasoning over complicated demonstrative instructions. For instance, in (a),VPG-C can keenly
identify the connections between the images and thereby infer the reason that causes this unusual
phenomenon. In (b, c) ,VPG-C exhibits the ability to comprehend absurd objects through mul-
timodal conversations with humans. In (d, e) ,VPG-C can reasonably infer the relations among
8

--- PAGE 9 ---
Please explain the interesting point based on the three pictures.The third image reveals some peculiar tanned circles on a men‚Äòs feet, perfectly aligned with the holes in the sandals shown in the second image. It might be associated with his long-time outdoor activities since the first image indicates that the sun is intense. Therefore, the amusing sunburn occurs when the men‚Äôs feet are exposed to prolonged sunlight. Due to the sandals partially blocking the UV rays, the skin away from the holes remains untanned while the skin under the holes gets tanned. However, the sunlight still reaches the skin between the holes, resulting in the tanned circles.
(a)
The set of images is a creative advertisement for animal adoption, which encourages people to take a note with contact information that also serves as a cage bar for the animals behind it. As people take the notes, animals are more likely to be adopted and freed from their cages.What do these four pictures want to convey to us?
(e)
What is this in picture?This is an image of a fish. The fish is black and silvery, with a small mouth and eyes. It is lying on its side, with its tail and fins visible. The fish is about 5.51 inches long and 12.20 inches wide.
(c)
Is it still the fish you thought it was now?No, I don't think it's a fish now, there are many colored pens and a fish-like case in this picture, these pens should be packed in a pencil case, so this should be a black and white fish-looking pencil case.
What else can this pencil case be used for besides holding pens?It can also be used to hold stickers, erasers, or other stationery, but its fish-shaped appearance is too deceptive!
What changes have occurred to this wall in the pictures over time, and what does this set of pictures hope to show us through these changes?This set of pictures mainly shows the change of a wall. In the beginning, the wall is a metal wall with only one line of writing, which has a silver shine and is very clean. In the third picture, some rust marks appear on the wall. A man stops by and stares at the wall. In the third image, there are some cars and a rusted wall with the outline of a knife. The last picture is already a rusted wall and the outline of an unrustedknife. It demonstrates the stable quality of the knife by showing that it will not rust after a long time.(d)
What is the connection between these two pictures?The first image is a gray T-shirt with Muggle text printed on it, while the second image is Google's logo. The connection between these two images is that their font and color scheme are very similar.What makes this set of pictures look interesting?Because the word 'muggle' uses the same font and color scheme as Google's logo, and their pronunciation is very similar. Its humor lies in using a homophonic stem and making the person wearing this dress look like a muggle.
(f)What is it and how to use itÔºüThis is an image of a camera lens. It is a type of camera lens that is designed to capture images and videos. It is typically used in combination with a camera body to form a camera system. The lens is designed to focus light onto a film or digital sensor, which captures the image. 
(b)Are you sure this is an image of a camera lens?I‚Äôm sorry. The image shows a large silver cup with a handle, featuring a lens-like design. The cup is made of stainless steel and there is also a spoon resting inside the cup.Can this lens take photos?No, it's just designed to look like a camera lens, but it's not a real camera lens.
Figure 8: Qualitative examples generated by our VPG-C-Vicuna-7B model.
the images and understand the metaphorical implications they want to convey. In Appendix G, we
provide more practical examples as well as comparisons with other MLLMs, where we find that
baseline models fail to correctly associate multiple images and comprehend demonstrative context.
5 R ELATED WORK
MLLMs (Yin et al., 2023) aim to serve as a general-purpose assistant to perform various vision-
language tasks by free-text generation. Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023c)
bridge LLMs with powerful pre-trained visual encoders and demonstrate strong zero-shot ability
by aligning visual features with LLMs. Follow-up works of LLaV A (Liu et al., 2023), MiniGPT-
4 (Zhu et al., 2023a), InstructBLIP (Dai et al., 2023), Hallucidoctor (Yu et al., 2024), mPLUG-
Owl (Ye et al., 2023) propose to fine-tune MLLMs with multimodal instruction tuning data. To
effectively benchmark the recent progress in MLLMs, concurrent works of LVLM-eHub (Xu et al.,
2023) and MME Benchmark (Fu et al., 2023) are proposed, while they mainly focus on instructions
that only involve a single image with limited instruction diversity. In this paper, we propose the
first demonstrative instruction-following benchmark, covering various tasks of diverse scenarios.
Further, we propose a lightweight and generic VPG-C module to address the inherent limitation
of current VPGs. Our VPG-C is efficiently tuned by our synthetic discriminative training strategy,
which demonstrates powerful potentials of text-to-image diffusion models (He et al., 2022; Lin et al.,
2023; Prabhu et al., 2023; Bansal & Grover, 2023; Yu et al., 2023b) to facilitate vision-language
understanding (Radford et al., 2021b; Jia et al., 2021; Li et al., 2022b).
6 C ONCLUSION
In this paper, we propose VPG-C , a generic and parameter-efficient approach that infers and com-
pletes the missing visual details for MLLMs to comprehend demonstrative instructions with inter-
leaved multimodal context. Meanwhile, we present a synthetic discriminative training strategy to
fine-tune VPG-C , eliminating the need for supervised demonstrative instruction data. To foster the
research on demonstrative instruction understanding, we build DEMON , a comprehensive benchmark
for multimodal large language models, consisting of 31 tasks with complicated vision-language
demonstrative context, covering a wide range of scenarios. Through synthetic training, VPG-C
showcases notable zero-shot performance on the DEMON benchmark. Its superior performance on
other established benchmarks like MME and OwlEval further underscores its effectiveness.
Acknowledgment. This work was supported by the NSFC (No. 62272411), Key Research and
Development Projects in Zhejiang Province (No. 2024C01106), the National Key Research and De-
velopment Project of China (2018AAA0101900), the Tencent WeChat Rhino-Bird Special Research
Program (Tencent WXG-FR-2023-10), and Research funding from FinV olution Group.
9

--- PAGE 10 ---
REFERENCES
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716‚Äì
23736, 2022.
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-
nick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international
conference on computer vision , pp. 2425‚Äì2433, 2015.
Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of
natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 18208‚Äì18218, 2022.
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel
Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https:
//doi.org/10.5281/zenodo.7733589 .
Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis, Oncel Tuzel, Ping
Huang, Jiulong Shan, Jianjun Shi, and Meng Cao. Vision datasets: A benchmark for vision-based
industrial inspection. arXiv preprint arXiv:2306.07890 , 2023.
Ankan Bansal, Yuting Zhang, and Rama Chellappa. Visual question answering on image sets. In
Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28, 2020,
Proceedings, Part XXI 16 , pp. 51‚Äì67. Springer, 2020.
Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated
datasets. arXiv preprint arXiv:2302.02503 , 2023.
Nilavra Bhattacharya, Qing Li, and Danna Gurari. Why does a visual question have different an-
swers? In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.
4271‚Äì4280, 2019.
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 11621‚Äì11631, 2020.
Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk.
Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 16495‚Äì16504, 2022.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90% chatgpt quality, 2023. URL https://vicuna.lmsys.org .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-
els.arXiv preprint arXiv:2210.11416 , 2022.
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning. arXiv preprint arXiv:2305.06500 , 2023.
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong
Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
19358‚Äì19369, 2023.
Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge Belongie. Neural naturalist:
generating fine-grained image comparisons. arXiv preprint arXiv:1909.04101 , 2019.
10

--- PAGE 11 ---
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei
Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multi-
modal large language models. arXiv preprint arXiv:2306.13394 , 2023.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,
Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010 , 2023.
Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine
this! scripts to compositions to videos. In Proceedings of the European conference on computer
vision (ECCV) , pp. 598‚Äì613, 2018.
Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao,
and Larry S Davis. Automatic spatially-aware fashion concept discovery. In Proceedings of the
IEEE international conference on computer vision , pp. 1463‚Äì1471, 2017.
Darryl Hannan, Akshay Jain, and Mohit Bansal. Manymodalqa: Modality disambiguation and qa
over diverse inputs. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 34,
pp. 7879‚Äì7886, 2020.
Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Girshick. Mask r-cnn. In Proceedings of the
IEEE international conference on computer vision , pp. 2961‚Äì2969, 2017.
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiao-
juan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint
arXiv:2210.07574 , 2022.
Mehrdad Hosseinzadeh and Yang Wang. Image change captioning by learning from an auxiliary
task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 2725‚Äì2734, 2021.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 , 2021.
Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob
Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In
Proceedings of the 2016 conference of the North American chapter of the association for compu-
tational linguistics: Human language technologies , pp. 1233‚Äì1239, 2016.
Phillip Isola, Joseph J Lim, and Edward H Adelson. Discovering states and transformations in image
collections. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pp. 1383‚Äì1391, 2015.
Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daume,
and Larry S Davis. The amazing mysteries of the gutter: Drawing inferences between panels in
comic book narratives. In Proceedings of the IEEE Conference on Computer Vision and Pattern
recognition , pp. 7186‚Äì7195, 2017.
Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of
similar images. arXiv preprint arXiv:1808.10584 , 2018.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In International conference on machine learning , pp. 4904‚Äì4916.
PMLR, 2021.
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh
Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal
machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern
recognition , pp. 4999‚Äì5007, 2017.
11

--- PAGE 12 ---
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
preprint arXiv:2304.02643 , 2023.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan
Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint
arXiv:2306.05425 , 2023a.
Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven CH Hoi. Lavis: A
library for language-vision intelligence. arXiv preprint arXiv:2209.09019 , 2022a.
Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, and William Yang
Wang. Unsupervised reinforcement learning of transferable meta-skills for embodied navigation.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
12123‚Äì12132, 2020.
Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian,
and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in
neural information processing systems , 35:7290‚Äì7303, 2022b.
Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang,
and Xin Eric Wang. Compositional temporal grounding with structured variational cross-graph
correspondence learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 3032‚Äì3041, 2022c.
Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian,
Tat-Seng Chua, and Yueting Zhuang. Gradient-regulated meta-prompt learning for generalizable
vision-language models. 2023b.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2023c.
Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang,
Wenming Tan, Jin Wang, Peng Wang, et al. End-to-end modeling via information tree for one-
shot natural language spatial video grounding. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) , pp. 8707‚Äì8717, 2022d.
Mengze Li, Haoyu Zhang, Juncheng Li, Zhou Zhao, Wenqiao Zhang, Shengyu Zhang, Shiliang
Pu, Yueting Zhuang, and Fei Wu. Unsupervised domain adaptation for video object grounding
with cascaded debiasing learning. In Proceedings of the 31st ACM International Conference on
Multimedia , pp. 3807‚Äì3816, 2023d.
Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carl-
son, and Jianfeng Gao. Storygan: A sequential conditional gan for story visualization. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 6329‚Äì6338,
2019.
Yongqi Li, Wenjie Li, and Liqiang Nie. Mmcoqa: Conversational question answering over text,
tables, and images. In Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pp. 4220‚Äì4231, 2022e.
Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Explore the power of synthetic data on
few-shot object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 638‚Äì647, 2023.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485 , 2023.
Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-
image transformers for story continuation. In Computer Vision‚ÄìECCV 2022: 17th European Con-
ference, Tel Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXXVII , pp. 70‚Äì87. Springer,
2022.
12

--- PAGE 13 ---
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document
images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision ,
pp. 2200‚Äì2209, 2021.
Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual
question answering by reading text in images. In 2019 international conference on document
analysis and recognition (ICDAR) , pp. 947‚Äì952. IEEE, 2019.
OpenAI. Chatgpt: A language model for conversational ai. Technical report, OpenAI, 2023a. URL
https://www.openai.com/research/chatgpt .
OpenAI. Gpt-4 technical report. arXiv:2303.08774 , 2023b.
Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stress-
testing visual models by generating language-guided counterfactual images. arXiv preprint
arXiv:2305.19164 , 2023.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning , pp.
8748‚Äì8763. PMLR, 2021a.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning , pp.
8748‚Äì8763. PMLR, 2021b.
Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, and Mubbasir Kapadia. Aesop: Ab-
stract encoding of stories, objects, and pictures. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pp. 2052‚Äì2063, 2021.
Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , pp. 3982‚Äì3992, 2019.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 , 2014.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.
2556‚Äì2565, 2018.
Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pp. 10740‚Äì10749, 2020.
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for
reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491 ,
2018.
Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco,
Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over
text, tables and images. arXiv preprint arXiv:2104.06039 , 2021.
Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal. Expressing visual relation-
ships via language. arXiv preprint arXiv:1906.07689 , 2019.
Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito.
Slidevqa: A dataset for document visual question answering on multiple images. arXiv preprint
arXiv:2301.04883 , 2023.
13

--- PAGE 14 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ¬¥ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv:2302.13971 , 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXiv:2307.09288 , 2023b.
Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson
env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pp. 9068‚Äì9079, 2018.
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for
accelerating bert inference. arXiv preprint arXiv:2004.12993 , 2020.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning , pp. 2048‚Äì2057. PMLR, 2015.
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan
Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large
vision-language models. arXiv preprint arXiv:2306.09265 , 2023.
Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. Recipeqa: A challenge
dataset for multimodal comprehension of cooking recipes. arXiv preprint arXiv:1809.00812 ,
2018.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models
with multimodality. arXiv preprint arXiv:2304.14178 , 2023.
Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549 , 2023.
Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted
language model for fine-grained scene graph generation in an open world. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 21560‚Äì21571, 2023a.
Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, and Yueting Zhuang. Interactive data synthesis
for systematic vision adaptation via llms-aigcs collaboration. arXiv preprint arXiv:2305.12799 ,
2023b.
Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian,
and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2024.
Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.
arXiv preprint arXiv:2302.05543 , 2023.
Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang Pu, Fei Wu, and Yueting Zhuang. Frame
augmented alternating attention network for video question answering. IEEE Transactions on
Multimedia , 22(4):1032‚Äì1041, 2019.
14

--- PAGE 15 ---
Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu Zhang, Qingpeng Cai, Juncheng Li, Sihui
Luo, and Yueting Zhuang. Magic: Multimodal relational graph adversarial inference for diverse
and unpaired text-based image captioning. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 36, pp. 3335‚Äì3343, 2022.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 , 2023a.
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Young-
jae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-
scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939 , 2023b.
15

--- PAGE 16 ---
A O VERVIEW
In this appendix, we present:
‚Ä¢ Detailed information of the proposed DEMON benchmark (Section B).
‚Ä¢ Implementation details of our VPG-C (Section C).
‚Ä¢ Implementation details of existing MLLMs on the DEMON benchmark (Section D).
‚Ä¢ Detailed zero-shot performance on MME benchmark (Section E).
‚Ä¢ Detailed zero-shot performance on DEMON benchmark (Section F).
‚Ä¢ Qualitative comparison with existing MLLMs (Section G).
B B ENCHMARK DETAILS
Task Scenario Dataset Metirc
Multimodal Dialogue
Conversational Embodied Dialogue Embodied ALFRED (Shridhar et al., 2020) ROUGE-L
Multimodal Dialogue Conversation MMCoQA (Li et al., 2022e) ROUGE-L
Visual Storytelling
Animated Story Completion Cartoon AESOP (Ravi et al., 2021) ROUGE-L
Animated Story Completion Cartoon PororoSV (Li et al., 2019) ROUGE-L
Animated Story Completion Cartoon FlintstonesSV (Gupta et al., 2018) ROUGE-L
Sequential Photo Storytelling Album VIST (Huang et al., 2016) ROUGE-L
Sequential Photo Storytelling Cartoon DiDeMoSV (Maharana et al., 2022) ROUGE-L
Visual Relation Inference
Visual Change Captioning Surveillance Spot-the-Diff (Jhamtani & Berg-Kirkpatrick, 2018) ROUGE-L
Visual Change Captioning Synthetic CLEVR-Change (Hosseinzadeh & Wang, 2021) ROUGE-L
Visual Relationship Expressing General IEdit (Tan et al., 2019) ROUGE-L
Subtle Difference Expressing Fine-Grained Birds-to-Words (Forbes et al., 2019) ROUGE-L
Multimodal Cloze
Comic Dialogue Identification Cartoon COMICS-Dialogue (Iyyer et al., 2017) Accuracy
Comic Panel Identification Cartoon COMICS-Panel (Iyyer et al., 2017) Accuracy
Recipe Completion Recipe RecipeQA-TextCloze (Yagcioglu et al., 2018) Accuracy
Visual Step Cloze Recipe RecipeQA-VisualCloze (Yagcioglu et al., 2018) Accuracy
Knowledge Grounded QA
Webpage QA Webpage WebQA (Chang et al., 2022) Accuracy
Textbook QA Textbook TQA (Kembhavi et al., 2017) Accuracy
Complex Multimodal QA Wikipedia MMQA (Talmor et al., 2021) Accuracy
Complex Multimodal QA* Wikipedia MANYMODALQA (Hannan et al., 2020) Accuracy
Text-Rich Images QA
Slide QA Slide SlideVQA (Tanaka et al., 2023) Accuracy
OCR QA Book Cover OCR-VQA (Mishra et al., 2019) Accuracy
Document QA Document Image DocVQA (Mathew et al., 2021) Accuracy
Multi-Image Reasoning
Image-Set QA* Indoor Egocentric Gibson (Bansal et al., 2020; Xia et al., 2018) Accuracy
Image-Set QA Driving Recording nuScenes (Bansal et al., 2020; Caesar et al., 2020) Accuracy
Industrial Inspection Industrial VISION (Bai et al., 2023) Accuracy
Fashion QA Fashion Fashion200K (Han et al., 2017) Accuracy
Property Coherence General MIT-States-PropertyCoherence (Isola et al., 2015) Accuracy
State Transformation Coherence General MIT-States-StateCoherence (Isola et al., 2015) Accuracy
Visual Step Matching Recipe RecipeQA-ImageCoherence (Yagcioglu et al., 2018) Accuracy
Multi-Image Visual Entailment General NLVR2 (Suhr et al., 2018) Accuracy
Ambiguity Analysis Mobile Photo VizWiz (Bhattacharya et al., 2019) Accuracy
Table 6: Summary of the demonstrative instruction-following tasks in DEMON benchmark. * indi-
cates the tasks that are not included in DEMON-Core .
C I MPLEMENTATION DETAILS
Model. We choose ViT-G/14 from EV A-CLIP (Fang et al., 2023) as our visual encoder and pre-
trained Q-former from BLIP-2 without instruction tuning as the task-agnostic visual prompt gen-
erator. For the large language model, we implement three versions: LLaMA2-7B (Touvron et al.,
16

--- PAGE 17 ---
Q-Former¬∑¬∑¬∑LearnableQueriesFrozenQueriesStep1:Reasoning with Task-Agnostic VisualPrompts(VP)Step2:VisualPromptGeneratorComplete(VPG-C)......InterleavedVision-LanguageInstructionLLM Layer 1LLM Layer L/2Q-Former++the reused Q-Former takes the same input from theimage encoder*
*Step3:ReasoningwithInstruction-SpecificVPImageEncoderLLM Layer L/2 + 1LLM Layer L¬∑¬∑¬∑Linear
LinearInstruction-SpecificVisualPromptsLanguageResponse‚ë†Guidance Generation‚ë°ResidualDetailsExtraction‚ë¢reintegration	‚Ñé!"	‚Ñé#"	‚Ñé$"	‚Ñé%"............	ùë£!!"	ùë£!&"...	ùë£'!"	ùë£'&"...	‚Ñétext token	ùë£visualprompt	‚Ñé!(/#	‚Ñé#(/#			‚Ñé$(/#			‚Ñé%(/#............	ùë£'!(/#	ùë£'&(/#...	ùë£'!(/#	ùë£'&(/#...	‚Ñé!(/#	‚Ñé#(/#..................	‚Ñé%(/#			‚Ñé$(/#		ùë£$'!(/#ùë£$'&(/#ùë£$!&(/#ùë£$!!(/#ùë£ÃÖ!!ùë£ÃÖ!&...ùë£ÃÖ'!ùë£ÃÖ'&......
Figure 9: Detailed framework of our MLLM enhanced with VPG-C .
2023b), Vicuna-7B (Chiang et al., 2023), Vicuna-13B, with 32, 32, 48 Transformer layers, respec-
tively. We derive instruction-specific conditions from the 16th / 24th layer and re-inject the condi-
tional visual knowledge into the 17th / 25th layer. Furthermore, we provide detailed framework of
our MLLM enhanced with VPG-C in Figure 9.
Choice of Q-former. Recently, InstructBLIP (Dai et al., 2023) proposes to take the instruction as
additional input to the Q-former and fine-tune the Q-former to extract visual features according to in-
structions using 16M multimodal instruction tuning data. While achieving outstanding performance
on in-domain tasks, a recent study (Xu et al., 2023) indicates that fine-tuning on massive in-domain
data severely undermines its generalizability on open-world scenarios. Instead of directly relying
on the Q-former to achieve task-specific feature extraction by massive instruction tuning, we aim to
utilize the sophisticated reasoning ability of LLMs to guide the Q-former to conditionally attend to
residual visual details. Thus, we use the Q-former without instruction data tuning from BLIP-2 (Li
et al., 2023c), which extracts the task-agnostic primary visual contents at the first time.
Training. We implement VPG-C in LA VIS library (Li et al., 2022a). We keep the visual backbone,
visual prompt generator, and the language model frozen, and tune the VPG-C module using the
proposed training strategy. Since BLIP-2 models do not include pre-trained Q-former that matches
Vicuna and LLaMA2, we reuse the Q-former that matches FlanT5-XXL and fine-tune the last linear
projection layer with 5 million image-text pairs to align it with Vicuna/LLaMA2. All the tunable
parameters of our VPG-C module are a set of query embeddings and two linear projection layers,
which only accounts for 0.09% ( ‚àº6.3M) of the entire model. As for synthetic training, we select
about 30k images from CC3M (Sharma et al., 2018) that contain significantly ignored objects and
perform different types of editing on them. Totally, we generate approximately 64k synthetic images
with suitable modifications. To stablize the training and avoid overfitting, we use 500k image-
caption pairs from CC3M to jointly train the VPG-C module. We tune the VPG-C module for 18k
steps using a batch size of 24 for synthetic training and 64 for image captioning, which takes about
7 hours to complete with a single A100 GPU. Additionally, we adopt the AdamW optimizer with
Œ≤= (0.9,0.999) , and set the learning rate and weight decay to 0.00002 and 0.05, respectively. We
warm up the training with 2k warm-up steps, followed by a learning rate decay mechanism with the
cosine schedule.
Implementation of VPG-C with the linear VPG. As a generic method, VPG-C can be implemented
with different VPGs. Beyond widely used Q-former that is composed of multiple Transformer
blocks, we further probe the effectiveness of VPG-C with a simpler VPG, i.e., Linear Projection,
as used in LLaV A (Liu et al., 2023). LLaV A trains a simple linear layer as the VPG to connect
image features into the word embedding space. To implement VPG-C with the linear VPG, we
first linearly project the generated guidance gand then take it as a filter to perform element-wise
Hadamard product with the visual features XIfrom the image encoder:
V= (W1g1T)‚äô(W2XI) (1)
whereW1andW2are linear projection matrixes, 1Tis the transpose of an all-ones vector, and ‚äô
represents Hadamard product. The output Vrepresents the newly-extracted missing visual details
according to the inferred guidance. And Vis reintegrated into the LLM in the same manner.
17

--- PAGE 18 ---
D M ODEL DETAILS IN DEMON B ENCHMARK
Recent advancements in LLMs (OpenAI, 2023a;b) have heralded significant achievements across
various domains. Inspired by this success, many MLLMs (Li et al., 2023c; Liu et al., 2023; Zhu
et al., 2023a; Alayrac et al., 2022; Ye et al., 2023; Gao et al., 2023; Li et al., 2023a) have been
proposed to foster generalist vision-language reasoning (Xu et al., 2015; Li et al., 2023b; 2020; Yu
et al., 2023a; Li et al., 2022d; Zhang et al., 2022; Li et al., 2022c; 2023d; Zhang et al., 2019; Antol
et al., 2015). In our experiments, we conducted comparisons with some of the most recent and
representative MLLMs in the following.
‚Ä¢LLaV A (Liu et al., 2023) establishes a connection between the visual encoder ViT-L/14
from CLIP (Radford et al., 2021a) and the language decoder LLaMA (Touvron et al.,
2023a), utilizing a lightweight, fully-connected (FC) layer. Initially, the system trains this
FC layer using 595K image-text pairs, while keeping both the visual encoder and LLM
static. Following this, LLaV A fine-tunes both the FC layer and LLM using a dataset com-
prising 158K instructional vision-language pairs. The tested version is ‚ÄúLLaV A-7B-v0‚Äù.
‚Ä¢LLaMA-Adapter V2 (Gao et al., 2023) stands as a model of parameter efficiency within
the realm of visual instruction. Despite maintaining the visual encoder (ViT-L/14) and the
LLM in a static state, LA-V2 distributes the instruction-following capacity of the entire
LLaMA system via bias-tuning. This method allows for the refinement of scale, bias,
norm, and prompt parameters on diverse data sets. These include 200M image captioning
data, 158K visual instruction-following data, and an additional 52K language instruction-
following data, the latter of which was assembled by GPT-4 (OpenAI, 2023b). The tested
version is ‚ÄúLLaV A-7B‚Äù.
‚Ä¢MiniGPT-4 (Zhu et al., 2023a) bridges the gap between the visual encoder and text en-
coder using a fully-connected (FC) layer. Initially, this model trains the FC layer on a
dataset comprised of 5M image-text pairs before fine-tuning it on 3.5K instructional vision-
language data. Notwithstanding its simplicity, MiniGPT-4 requires the loading of a pre-
trained vision encoder from BLIP2, as well as a Vicuna LLM (Chiang et al., 2023). The
tested version is ‚Äúminigpt4-aligned-with-vicuna7b‚Äù.
‚Ä¢BLIP2 (Li et al., 2023c) employs a dual-stage strategy to seamlessly bridge the modal-
ity gap, utilizing a lean Q-Former pre-trained on 129 million image-text pairs. The initial
stage kick-starts the learning process of vision-language representation, leveraging a frozen
image encoder, the ViT-g/14 from EV A-CLIP (Fang et al., 2023). Subsequently, the sec-
ond stage harnesses a frozen LLM, the FlanT5 (Chung et al., 2022), to initiate the vision-
to-language generative learning. This innovative strategy effectively facilitates zero-shot
instructed image-to-text generation. The tested version is ‚Äúblip2-pretrain-flant5xl‚Äù.
‚Ä¢mPLUG-Owl (Ye et al., 2023) introduces a visual abstractor, fundamentally close the Per-
ceiver Resampler in Flamingo (Alayrac et al., 2022), as a bridge between the pre-trained vi-
sual encoder ViT-L/14 and the LLM (LLaMA (Touvron et al., 2023a)). This model adopts
a two-stage fine-tuning procedure. In the initial phase, both the visual encoder and the
visual abstractor undergo comprehensive fine-tuning using a dataset of 204M image-text
pairs. Subsequently, in the second phase, mPLUG-Owl applies the 158K LLaV A-Instruct
dataset to fine-tune the pre-trained LLM in a parameter-efficient manner through the use of
LoRA (Hu et al., 2021). The tested version is ‚Äúmplug-owl-llama-7b‚Äù.
‚Ä¢Otter (Li et al., 2023a) is a multimodal model that applies in-context instruction tuning
based on OpenFlamingo (Alayrac et al., 2022). This model integrates a LLaMA-7B (Tou-
vron et al., 2023a) language encoder and a CLIP ViT-L/14. While the visual and text
encoders remain static, Otter refines an additional 1.3 billion parameters. These parame-
ters are derived from adaptation modules and are trained using 158K instruction-following
data. The tested version is ‚ÄúOTTER-Image-LLaMA7B-LA-InContext‚Äù.
‚Ä¢InstructBLIP (Dai et al., 2023) originates from a pre-trained BLIP-2 model, which con-
sists of a ViT-g/14 image encoder, a Vicuna, and a Q-Former to act as the bridge between
these two components. During the process of vision-language instruction tuning, only the
Q-Former undergoes fine-tuning, with the training process leveraging data from 13 distinct
visual question-answering datasets. The tested version is ‚Äúblip2-vicuna-instruct-7b‚Äù.
18

--- PAGE 19 ---
‚Ä¢OpenFlamingo (Alayrac et al., 2022; Awadalla et al., 2023) represents one of the pioneer-
ing efforts to incorporate Language Model Learning (LLMs) into the domain of vision-
language pretraining. To optimize its conditioning on visual features, Flamingo strategi-
cally integrates a number of gated cross-attention dense blocks amidst the layers of the
pre-trained language encoder. OpenFlamingo offers an open-source rendition of this ad-
vanced model. The tested version is ‚Äúllama-7b‚Äù.
TheDEMON benchmark predominantly features interleaved vision-language instructions, distin-
guishing it from the traditional single-image datasets. While our innovative method, VPG-C , along
with OpenFlamingo and MiniGPT-4, inherently accommodates interleaved image-text sequences,
other models like BLIP-2, InstructBlip, LLaV A, mPLUG-Owl, Otter, and LLaMA-Adapter V2 do
not. For these, we employed a strategy where we concatenate the embeddings of all images. This
approach can be analogized to treating images as frames within a video. To maintain the positional
context of each image in an interleaved image-text instruction, we explicitly indicate the location of
each image within the context.
E D ETAILED ZERO-SHOT PERFORMANCE ON MME B ENCHMARK
In this section, we report the detailed performance on the 14 subtasks of MME benchmark in Table 7.
Table 7: Detailed zero-shot performance on MME benchmark.
BLIP-2 InstructBLIP LA-V2 LLaV A MiniGPT-4 mPLUG-Owl Otter VPG-C
Existence 160.00 185.00 120.00 50.00 115.00 120.00 195.00 180.00
Count 135.00 143.33 50.00 50.00 123.33 88.33 50.00 96.67
Position 73.33 66.67 48.33 50.00 81.67 50.00 86.67 80.00
Color 148.33 153.33 75.00 55.00 110.00 55.00 113.33 116.67
Poster 141.84 123.81 99.66 50.00 55.78 136.05 138.78 147.28
Celebrity 105.59 101.18 86.18 48.82 65.29 100.29 172.65 164.12
Scene 145.25 153.00 148.50 50.00 95.75 135.50 158.75 156.00
Landmark 138.00 79.75 150.25 50.00 69.00 159.25 137.25 145.00
Artwork 136.50 134.25 69.75 49.00 55.75 96.25 129.00 113.50
OCR 110.00 72.50 125.00 50.00 95.00 65.00 72.50 100.00
Perception 1293.84 1212.82 972.67 502.82 866.57 967.34 1292.26 1299.24
Commonsense 110.00 129.29 81.43 57.14 72.14 78.57 106.43 98.57
Numerical 40.00 40.00 62.50 50.00 55.00 60.00 72.50 77.50
Text Translation 65.00 65.00 50.00 57.50 55.00 80.00 57.50 57.50
Code Reasoning 75.00 57.50 55.00 50.00 110.00 57.50 70.00 87.50
Cognition 290.00 291.79 248.93 214.64 292.14 276.07 306.43 321.07
F D ETAILED ZERO-SHOT PERFORMANCE ON DEMON B ENCHMARK
Table 8: Zero-shot evaluation on multimodal dialogue.
Conversational Embodied Dialogue Multimodal Dialogue
BLIP-2 16.75 35.49
InstructBLIP 18.07 49.09
LLaMA-Adapter V2 19.04 9.40
LLaV A 10.19 5.39
MiniGPT-4 16.82 10.57
mPLUG-Owl 11.07 14.27
OpenFlamingo 24.27 9.49
Otter 16.06 14.68
VPG-C-LLaMA2-7B 48.31 37.04
VPG-C-Vicuna-7B 41.02 33.99
VPG-C-Vicuna-13B 42.25 34.02
19

--- PAGE 20 ---
Table 9: Zero-shot evaluation on visual storytelling.
Animated Story Animated Story Animated Story Sequential Photo Sequential Photo
Completion-AESOP Completion-PororoSV Completion-FlintstonesSV Storytelling-VIST Storytelling-DiDeMoSV
BLIP-2 21.64 26.24 29.61 13.16 24.2
InstructBLIP 18.80 28.20 33.32 16.92 24.80
LLaMA-Adapter V2 18.01 20.15 24.22 10.89 14.57
LLaV A 13.56 11.44 12.77 8.00 7.71
MiniGPT-4 12.23 16.00 26.48 14.82 15.81
mPLUG-Owl 18.28 20.49 32.12 10.82 14.94
OpenFlamingo 23.32 32.35 37.79 15.14 12.50
Otter 13.94 17.52 22.21 9.96 14.23
VPG-C-LLaMA2-7B 19.98 28.67 38.14 16.95 20.05
VPG-C-Vicuna-7B 19.93 28.36 39.19 17.34 21.27
VPG-C-Vicuna-13B 20.53 29.81 41.32 19.04 22.26
Table 10: Zero-shot evaluation on visual relation inference.
Visual Change Captioning Visual Change Captioning Visual Relationship Subtle Difference
-Spot-the-Diff -CLEVR-Change Expressing Expressing
BLIP-2 17.48 3.21 12.37 9.62
InstructBLIP 19.71 4.61 10.70 10.92
LLaMA-Adapter V2 16.72 15.52 7.88 13.92
LLaV A 8.50 8.76 6.72 9.11
MiniGPT-4 7.50 7.49 7.84 8.97
mPLUG-Owl 6.06 1.46 6.22 7.86
OpenFlamingo 13.01 11.90 12.57 17.90
Otter 12.69 11.63 8.85 12.38
VPG-C-LLaMA2-7B 21.02 42.05 14.10 24.81
VPG-C-Vicuna-7B 20.01 41.60 16.35 25.64
VPG-C-Vicuna-13B 21.56 40.67 20.27 26.08
Table 11: Zero-shot evaluation on multimodal cloze.
Comic Dialogue Identification Comic Panel Identification1Recipe Completion Visual Step Cloze1
BLIP-2 39.70 0.00 30.46 1.60
InstructBLIP 40.60 0.00 27.40 16.80
LLaMA-Adapter V2 24.40 0.40 38.20 9.00
LLaV A 30.60 0.00 32.80 0.00
MiniGPT-4 33.00 1.00 31.60 0.80
mPLUG-Owl 36.60 0.00 27.60 0.80
OpenFlamingo 38.40 1.20 29.00 18.00
Otter 29.00 0.00 35.00 0.00
VPG-C-LLaMA2-7B 36.80 1.80 51.80 1.40
VPG-C-Vicuna-7B 39.20 3.60 30.40 15.40
VPG-C-Vicuna-13B 42.20 8.20 39.80 18.40
1For tasks with images as options, only responses that begin with the correct answer will be evaluated as correct.
Table 12: Zero-shot evaluation on knowledge grounded QA.
Webpage QA Textbook QA Complex Multimodal QA
BLIP-2 47.60 29.73 40.36
InstructBLIP 45.20 30.20 66.80
LLaMA-Adapter V2 44.60 46.00 43.80
LLaV A 39.40 39.60 29.60
MiniGPT-4 27.40 28.60 34.80
mPLUG-Owl 34.20 30.00 35.60
OpenFlamingo 37.80 32.40 25.80
Otter 45.00 39.00 41.00
VPG-C-LLaMA2-7B 49.40 42.40 61.20
VPG-C-Vicuna-7B 50.00 33.40 62.40
VPG-C-Vicuna-13B 50.60 43.40 64.80
20

--- PAGE 21 ---
Table 13: Zero-shot evaluation on text-rich images QA.
Slide QA OCR QA Document QA
BLIP-2 43.80 10.40 46.40
InstructBLIP 42.00 44.20 47.00
LLaMA-Adapter V2 43.00 3.40 49.60
LLaV A 38.80 2.60 43.60
MiniGPT-4 35.20 7.20 36.80
mPLUG-Owl 35.60 22.60 39.20
OpenFlamingo 35.60 3.80 52.40
Otter 38.40 2.20 42.60
VPG-C-LLaMA2-7B 45.80 39.60 49.40
VPG-C-Vicuna-7B 46.80 39.40 48.60
VPG-C-Vicuna-13B 48.80 46.60 52.60
Table 14: Zero-shot evaluation on multi-image reasoning.
Image-Set Industrial Fashion Property State Transformation Visual Step Multi-Image Ambiguity
QA Inspection QA Coherence Coherence Matching1Visual Entailment Analysis
BLIP-2 34.60 42.80 43.20 59.00 38.20 0.20 53.40 45.80
instructblip7b 65.00 50.60 44.40 59.20 59.40 11.60 55.20 43.00
LLaMA-Adapter V2 41.60 55.00 45.60 48.80 63.00 0.00 54.80 43.40
LLaV A 29.60 53.00 45.20 50.40 59.20 0.80 50.80 43.20
MiniGPT-4 30.40 59.80 49.20 52.00 57.80 0.20 50.60 48.00
mPLUG-Owl 29.20 54.20 45.80 50.00 60.60 0.00 55.00 45.20
OpenFlamingo 25.80 52.20 44.20 59.60 51.40 2.20 53.60 44.00
Otter 44.80 69.80 47.00 51.40 46.40 0.00 49.00 42.40
VPG-C-LLaMA2-7B 62.60 61.40 46.00 56.60 57.80 0.00 53.80 51.20
VPG-C-Vicuna-7B 67.20 48.80 50.00 60.80 60.00 0.20 57.80 57.40
VPG-C-Vicuna-13B 73.40 54.00 51.00 63.20 63.40 2.60 60.20 61.40
1For tasks with images as options, only responses that begin with the correct answer will be evaluated as correct.
G Q UALITATIVE COMPARISON
In this section, we compare our model with existing MLLMs on some complicated demonstrative
instructions.
21

--- PAGE 22 ---
Please explain the interesting point based on the three pictures.The third image reveals some peculiar tanned circles on a men‚Äòs feet, perfectly aligned with the holes in the sandals shown in the second image. It might be associated with his long-time outdoor activities since the first image indicates that the sun is intense. Therefore, the amusing sunburn occurs when the men‚Äôs feet are exposed to prolonged sunlight. Due to the sandals partially blocking the UV rays, the skin away from the holes remains untanned while the skin under the holes gets tanned. However, the sunlight still reaches the skin between the holes, resulting in the tanned circles.
Please explain the interesting point based on the three pictures.The sun is shining on the person's feet.
Please explain the interesting point based on the three pictures.1.The picture of the blue sky and the clouds. 2.The picture of the blue sky and the clouds. 3.The picture of the blue sky and the clouds. 4.The picture of the blue sky and the clouds. 5.The picture of the blue sky and the clouds. 6.The picture of the blue sky and the clouds. 7.The picture of the blue sky and the clouds. 8.The picture of the blue sky and the clouds. 9.The picture of the blue sky and the clouds. 10.The picture of the blue sky and
Please explain the interesting point based on the three pictures.Please explain the interesting point based on the three pictures.
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4Figure 10: Qualitative comparison between our VPG-C and existing MLLMs.
22

--- PAGE 23 ---
The set of images is a creative advertisement for animal adoption, which encourages people to take a note with contact information that also serves as a cage bar for the animals behind it. As people take the notes, animals are more likely to be adopted and freed from their cages.What do these four pictures want to convey to us?
Don‚Äòt eat dog meat.What do these four pictures want to convey to us?
What is the message? What is the purpose of these pictures? What is the message? What is the purpose of these pictures? What is the message? What is the purpose of these pictures? What is the message? What is the purpose of these pictures? What is the messageWhat do these four pictures want to convey to us?
The poster is of a dog that has been attacked by a predator. It is lying on the ground with its eyes closed and its ears bent down. The poster is placed on a brick wall with a message saying \"please take one home.What do these four pictures want to convey to us?
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4Figure 11: Qualitative comparison between our VPG-C and existing MLLMs.
What changes have occurred to this wall in the pictures over time, and what does this set of pictures hope to show us through these changes?This set of pictures mainly shows the change of a wall. In the beginning, the wall is a metal wall with only one line of writing, which has a silver shine and is very clean. In the third picture, some rust marks appear on the wall. A man stops by and stares at the wall. In the third image, there are some cars and a rusted wall with the outline of a knife. The last picture is already a rusted wall and the outline of an unrustedknife. It demonstrates the stable quality of the knife by showing that it will not rust after a long time.
What changes have occurred to this wall in the pictures over time, and what does this set of pictures hope to show us through these changes?time lapse
What changes have occurred to this wall in the pictures over time, and what does this set of pictures hope to show us through these changes?The wall is a blank canvas, and the artist is the one who paints it. The artist is the one who paints the wall, and the wall is the one who paints the artist. The wall is the one who paints the artist, and the artist is the one who paints the wall. The wall is the one who paints the artist, and the artist is the one who paints the wall. The wall is the one who paints the artist, and the artist is the one who paints the wall. The wall is the one who paints the artist, and the artist is the one who
What changes have occurred to this wall in the pictures over time, and what does this set of pictures hope to show us through these changes?</Img> 
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4
Figure 12: Qualitative comparison between our VPG-C and existing MLLMs.
23

--- PAGE 24 ---
What is the connection between these two pictures?The first image is a gray T-shirt with Muggle text printed on it, while the second image is Google's logo. The connection between these two images is that their font and color scheme are very similar.What makes this set of pictures look interesting?Because the word 'muggle' uses the same font and color scheme as Google's logo, and their pronunciation is very similar. Its humor lies in using a homophonic stem and making the person wearing this dress look like a muggle.
What is the connection between these two pictures?googleWhat makes this set of pictures look interesting?This can be seen as a clever play on words or a subtle reference to the popular Harry Potter franchise, where the term \"Muggle\" is used to describe non-magical people in the wizarding world. 
What is the connection between these two pictures?Google, Muggle, Harry Potter, J.K. Rowling, Wizarding World, Wizard, Witch, Wizarding, Wizarding World of Harry Potter, Universal Orlando Resort, Universal Studios Florida, Universal Orlando,What makes this set of pictures look interesting?The first image is a gray T-shirt with Muggle text printed on it, while the second image is Google's logo. The connection between these two images is that their font and color scheme are very similar. 
What is the connection between these two pictures?The image shows the word \"google\" written in red, blue, and green letters on a white background. It is an example of how the color scheme can be changed in different ways. What makes this set of pictures look interesting?</img></s> 
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4Figure 13: Qualitative comparison between our VPG-C and existing MLLMs.
24

--- PAGE 25 ---
What is this in picture?This is an image of a fish. The fish is black and silvery, with a small mouth and eyes. It is lying on its side, with its tail and fins visible. The fish is about 5.51 inches long and 12.20 inches wide.
Is it still the fish you thought it was now?No, I don't think it's a fish now, there are many colored pens and a fish-like case in this picture, these pens should be packed in a pencil case, so this should be a black and white fish-looking pencil case.
What else can this pencil case be used for besides holding pens?It can also be used to hold stickers, erasers, or other stationery, but its fish-shaped appearance is too deceptive!
What is this in picture?fish
Is it still the fish you thought it was now?Yes, it is still the same fish.
What else can this pencil case be used for besides holding pens?Here are some possible uses:\n\n1. Desk accessory: The pencil case can serve as a decorative desk accessory, adding a touch of whimsy and personality to your workspace.
What is this in picture?1. 13 cm 2. 13 cm 3. 13 cm 4. 13 cm 5. 13 cm 6. 13 cm 7. 13 cm 8. 13 cm 9. 13 cm 10. 13 cm 11. 
Is it still the fish you thought it was now?This is an image of a fish. The fish is black and silvery, with a small mouth and eyes. It is lying on its side, with its tail and fins visible. The fish is about 5.51 inches long and 12.20 inches wide. 
What else can this pencil case be used for besides holding pens?It can be used to hold other things, like a pencil, a ruler, a pencil sharpener, a rubber, a pencil eraser, a pencil sharpener, a pencil sharpener, a pencil sharpener,
What is this in picture?"This is an image of a fish. It is lying on its side with its tail flipped up and its mouth open. The image is black and white and has a scale on the side of the fish. 
Is it still the fish you thought it was now?No, it's not a fish. It's a large, black catfish that has been placed on top of a pile of colored pens and pencils. 
What else can this pencil case be used for besides holding pens?The fish is black and silvery, with a small mouth and eyes. It is lying on its side, with its tail and fins visible. The fish is about 5.51 inches long and 12.20 inches wide. 
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4Figure 14: Qualitative comparison between our VPG-C and existing MLLMs.
25

--- PAGE 26 ---
What is it and how to use itÔºüThis is an image of a camera lens. It is a type of camera lens that is designed to capture images and videos. It is typically used in combination with a camera body to form a camera system. The lens is designed to focus light onto a film or digital sensor, which captures the image. Are you sure this is an image of a camera lens?I‚Äôm sorry. The image shows a large silver cup with a handle, featuring a lens-like design. The cup is made of stainless steel and there is also a spoon resting inside the cup.Can this lens take photos?No, it's just designed to look like a camera lens, but it's not a real camera lens.
What is it and how to use itÔºüCamera
Are you sure this is an image of a camera lens?Yes, I am sure this is an image of a camera lensCan this lens take photos?Yes, a camera lens can take photos. 
What is it and how to use itÔºüThe EOS 1000D is a 10.1 megapixelentry-level DSLR camera announced by Canon on 26 June 2008. It is the successor to the EOS 450D and the predecessor to the EOS 1100D. The EOS 1000D is the first DSLR camera to use the new EF-S 18-55mm f/3.5-5.6 IS lens. Are you sure this is an image of a camera lens?It looks more like a coffee mug. I'm not sure what you are trying to say. I'm not sure what you are trying to say. I'm not sure what you are trying to say. Can this lens take photos?Yes, it can. It is a camera lens. It is designed to capture images and videos. 
What is it and how to use itÔºüThis is an image of a camera lens. It is a type of camera lens that is designed to capture images and videos. It is typically used in combination with a camera body to form a camera system. The lens is designed to focus light onto a film or digital sensor, which captures the image. Are you sure this is an image of a camera lens?I‚Äôm sorry. The image shows a large silver cup with a handle, featuring a lens-like design. The cup is made of stainless steel and there is also a spoon resting inside the cup.Can this lens take photos?No, it's just designed to look like a camera lens, but it's not a real camera lens.
VPG-CInstructBLIP
OpenFlamingoMiniGPT-4Figure 15: Qualitative comparison between our VPG-C and existing MLLMs.
26
