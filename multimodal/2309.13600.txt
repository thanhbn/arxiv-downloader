# 2309.13600.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.13600.pdf
# File size: 18163645 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-Dimensional Hyena for Spatial Inductive Bias
Itamar Zimerman, Lior Wolf
Blavatnik School of Computer Science, Tel Aviv University
zimerman1@mail.tau.ac.il, wolf@mail.tau.ac.il
Abstract
In recent years, Vision Transformers have attracted increas-
ing interest from computer vision researchers. However, the
advantage of these transformers over CNNs is only fully man-
ifested when trained over a large dataset, mainly due to the re-
duced inductive bias towards spatial locality within the trans-
former’s self-attention mechanism. In this work, we present
a data-efficient vision transformer that does not rely on self-
attention. Instead, it employs a novel generalization to multi-
ple axes of the very recent Hyena layer. We propose several
alternative approaches for obtaining this generalization and
delve into their unique distinctions and considerations from
both empirical and theoretical perspectives.
Our empirical findings indicate that the proposed Hyena N-D
layer boosts the performance of various Vision Transformer
architectures, such as ViT, Swin, and DeiT across multi-
ple datasets. Furthermore, in the small dataset regime, our
Hyena-based ViT is favorable to ViT variants from the recent
literature that are specifically designed for solving the same
challenge, i.e., working with small datasets or incorporating
image-specific inductive bias into the self-attention mecha-
nism. Finally, we show that a hybrid approach that is based
on Hyena N-D for the first layers in ViT, followed by layers
that incorporate conventional attention, consistently boosts
the performance of various vision transformer architectures.
1 Introduction
Creating a versatile layer designed to effectively process N-
dimensional data within deep networks is an important re-
search direction, which has significant implications for key
application domains, such as computer vision and speech
processing. It is imperative that such a layer not only ex-
hibit strong inductive bias towards N-dimensional data,
but also retain the required capacity to exploit extensive
datasets. Currently, two primary types of layers dominate
N-dimensional data domains: transformers (Vaswani et al.
2017) and CNNs (He et al. 2016; Liu et al. 2022a).
Standard CNNs employ relatively small filters (He et al.
2016; LeCun et al. 1989, 1998a), entailing a high inductive
bias, particularly for N-D locality. However, they are less
efficient and effective at handling long contexts. Conversely,
transformers exhibit a lower inductive bias (Ma et al. 2022),
but when trained on enough data, they appear to handle N-
D data effectively, by processing it as a 1-D sequence withcorresponding positional encoding (Dosovitskiy et al. 2020;
Arnab et al. 2021; Liu et al. 2022b).
One advantage transformers hold over CNNs is their abil-
ity to deal with varying data lengths and provide a global
context at the layer level (Vaswani et al. 2017). Yet, their
quadratic complexity in sequence length presents obstacles
to processing long contexts, which are vital for many tasks.
This work aims to combine the relative strengths of both
CNN and transformers by developing a novel layer that pos-
sesses: (i) an inductive bias towards N-dimensional data,
(ii) sufficient expressiveness, (iii) a sub-quadratic depen-
dency on sequence length, and (iv) flexibility in processing
N-dimensional data of any N-D lengths, while maintaining
global context at the layer level.
As a foundation for this new layer we employ multi-axes
long convolutions, a recent family of layers proven effective
for N-dimensional data (Nguyen et al. 2022; Baron, Zimer-
man, and Wolf 2023). These layers employ the convolution
of the signal with a multi-axes implicit filter. Unlike prior
layers in this field, our implicit filters are not anchored in
linear recurrence (similarly to state-space layers (Gu et al.
2021; Gu, Goel, and R ´e 2021)). Instead, we extend the very
recent Hyena layer (Poli et al. 2023) to accommodate N-D
data. As our theoretical analysis reveals, this results in a sim-
pler, more expressive, and more efficient model.
Our main contribution is the Hyena N-D layer, which
generalizes the recent Hyena layer to multi-dimensional
data. We justify our design choices extensively, by empiri-
cally and theoretically considering several parametrizations,
several decaying structures for incorporating bias of two-
dimensional locality, and the first multi-directional variant of
Hyena, which has negligible additional computation. More-
over,we are the first to theoretically characterize a form of
inductive bias inherent in the family of the Hyena layer.
As a direct application, we demonstrate that our layer can
be used as a drop-in replacement within the ViT backbone
to derive a much more data- and memory-efficient model.
We also propose a hybrid model that combines attention and
Hyena 2-D layers in ViT, further improving performance.
2 Background and Notations
Implicit Global Convolution Layers Standard convolu-
tion layers are a fundamental building block of deep learn-
ing (Fukushima 1980; LeCun et al. 1998b; Ronneberger,arXiv:2309.13600v1  [cs.CV]  24 Sep 2023

--- PAGE 2 ---
Fischer, and Brox 2015). These layers parameterized a con-
volution filter of size L and C channels with L*C parame-
ters, where each element is defined explicitly. In contrast, an
emerging approach implicitly defined the convolution kernel
via a learnable function. Namely, the kernel kh
i(filter) at po-
sition iand channel his defined by a function fhsuch that
fh(i) =ki. These methods have three main advantages: (i)
These layers can operate over an unrestricted context, as op-
posed to fixed-size explicit filters. (ii) The layers have sub-
quadratic time dependency on sequence length, and (iii) As
the number of parameters is decoupled from the sequence
length, these kernels are regularized by design, which ap-
pears to be necessary for their effectiveness (Li et al. 2022;
Fu et al. 2023). S4 (Gu, Goel, and R ´e 2021) and state-space
layers (Gu et al. 2021) were the pioneers to show the ef-
fectiveness of this approach, by parameterizing convolution
kernels via the linear state-space model (SSM), which was
then simplified using diagonal and real SSMs (Gupta, Gu,
and Berant 2022; Gupta, Mehta, and Berant 2022). Simi-
lar approaches by Ma et al. (2022); Lutati, Zimerman, and
Wolf (2023), use learnable components, including EMA and
IIR filters, instead of SSMs to formulate the parameteriza-
tion. As an alternative, Hyena and CkConv (Romero et al.
2021) established the parameterization by applying standard
Feedforward neural network (FFN) layers that operate on
positional encoding. These approaches provide superior per-
formance in several areas, such as NLP (Mehta et al. 2022;
Wang et al. 2022; Dao et al. 2022), speech (Saon, Gupta, and
Cui 2023), RL (Lu et al. 2023; David et al. 2022), time series
analysis, and more, especially in tasks that require capturing
long-range dependencies.
Implicit N-D global convolution Recently, this ap-
proach extended into multi-dimensional data, using im-
plicit parametrization for N-dimensional filters, which is
shown to be an effective method for computer vision
tasks (Nguyen et al. 2022; Baron, Zimerman, and Wolf
2023). The S4ND (Nguyen et al. 2022) was the first to
present the effectiveness of such an approach. It parame-
terized N-D filters by composing independent SSM-based
filters per axis, and during the forward path the filters were
aggregated to create a N-D global filter, by taking the outer
product of the per-axis filters. This approach was very effi-
cient. However, in (Baron, Zimerman, and Wolf 2023) it was
shown that the approach of learning kernels separately per
axis can be limited in terms of expressiveness, which makes
it advisable to leverage it with more expressive mechanisms.
In this light, our layer is the first to construct N-D implicit
filters without relying on the SSM system.
Hyena The Hyena layer parameterized implicit scalar fil-
ters of size Lper channel c∈[C]byHc:=hc1,·, hcLby
employing FFN FFNc:Rd→Ron positional embed-
dingpe(l)∈Rdsuch that ∀l∈[L] :hcl=FFNc(PE(l)).
This mechanism can generate kernels of any size, enabling
the layer to process unrestricted context. Inspired by atten-
tion, Hyena implements an expressive data-controlled linear
operator, which relies on interleaving implicit long convo-
lutions with element-wise multiplication. In terms of perfor-
mance, the Hyena layer introduces exceptional performance,
reaching transformer quality with a more efficient model,and with sub-quadratic complexity in sequence length.
To add inductive bias towards 1-dimensional locality, the
Hyena layer multiplies the filters derived from the FFN with
a window function. This function is expressed as follows:
window (t) =exp(−αt) +γ (1)
Vision Transformers The Vision Transformer
(ViT) (Dosovitskiy et al. 2020) is an attention-based
model architecture for computer vision tasks. In contrast
to conventional Convolutional Neural Networks (CNNs)
that utilize local correlations via convolutional filters, the
ViT reshapes an image into a 1-D sequence of fixed-size
patches, which are processed by a stack of transformer
encoder layers. Since transformers are permutation invari-
ant, positional encoding is incorporated. The self-attention
mechanism within the transformer enables each patch to be
considered in relation to all others, thereby facilitating the
learning of both local and long-range dependencies within
the image. The output from the transformer is a sequence
of embedded patches, with a specific classification token
utilized for classification tasks, similar to BERT (Devlin
et al. 2018). In ViT, the architecture doesn’t impose any
explicit spatial locality bias, which results in a flexible
model that - given a sufficient amount of training data - can
capture complex dependencies across the image.
Over the years, the ViT model has seen numerous en-
hancements. For instance, DeiT (Touvron et al. 2021) opti-
mizes performance through data augmentation, token-based
distillation, and regularization, thereby achieving strong
benchmark results even with less data. The Swin Trans-
former (Liu et al. 2021) introduces a model with more spatial
inductive bias, which adapts a hierarchical structure by parti-
tioning images into non-overlapping windows and then pro-
cessing them hierarchically. Moreover, (Dai et al. 2021; Guo
et al. 2022; d’Ascoli et al. 2021) amplify ViT’s efficiency by
integrating convolutional layers into the ViT architecture.
Furthermore, approaches such as (Xie et al. 2021; Carion
et al. 2020; Chen et al. 2022) introduced specific modifica-
tions to ViT, enabling it to excel in detection and semantic
segmentation tasks.
Notation Our notation follows the Hyena literature as
closely as possible (Poli et al. 2023; Nguyen et al. 2023).
Specifically, we denote the number of channels by C, and
the filter on channel c∈[C]byHc. We denote the num-
ber of dimensions by N, and the sequence length at any di-
mension by Lnforn∈[N],L:= ΠN
n=1Lnas the total
sequence length, Lmax:= maxN
n=1Lnas the maximal se-
quence length, and ˆNas the depth of the Hyena recurrence.
For the notations of the Hyena architecture, we denote the
FFN network by FFN :RM→R(ˆN+1)C, the positional
encoding function by ( PE) :R→RM, where Mis the
size of the FFN input layer. Finally, we denote the window
function by window :R→R.
3 Method
Motivation A well-known drawback of self-attention is its
relatively weak inductive bias. This is even more relevant
when handling two-dimensional data. In order to design a

--- PAGE 3 ---
data-efficient ViT, we choose not to incorporate 2-D induc-
tive bias into the self-attention mechanism in ViT (as done
in (Liu et al. 2021; Xu et al. 2021)), and instead employ an
alternative sequence layer within the ViT engine. Recently,
several novel sequence layers showed impressive results in
1-D sequence modeling, specifically in improving complex-
ity (Peng et al. 2023; Poli et al. 2023; Dao et al. 2022). This
motivated us to explore the utility of such layers as a drop-in
replacement for ViT.
Among those layers, we focus on the Hyena (Poli et al.
2023) layer for two main reasons: (i) It is built on simple
mechanisms, such as multiplicative element-wise gating and
implicit global filters. Hence, it provides a flexible structure
that can be modified to incorporate image-specific inductive
bias. (ii) Given that traditional convolution layers are known
for their significant inductive bias in vision tasks, it is rea-
sonable to assume that the implicit global convolution layers
that are part of Hyena would possess similar capabilities. In
this light, our work can be considered as a further step to-
wards combining convolution layers and ViT. In contrast to
previous work (Dai et al. 2021; Guo et al. 2022; d’Ascoli
et al. 2021), we employ implicit global convolution layers,
rather than standard convolution layers, which do not fo-
cus exclusively on short-range dependencies. Furthermore,
since the Hyena layer has a sub-quadratic dependency on
sequence length, it can lead to a significantly more efficient
ViT. This is especially valuable for tasks that involve pro-
cessing high-resolution images, such as medical imaging.
The Hyena-ND Layer The Hyena layer is composed of
three main components: (i) implicit global filters (ii) a data
control mechanism in the form of gating (element-wise mul-
tiplications), and (iii) a short filter implemented by a 1-D
convolutional layer. The scalability of the last two compo-
nents to accommodate multidimensional data is straightfor-
ward, as the second component is dimension-agnostic, and
the extension of (iii) to handle multidimensional data can
be realized simply, via a standard 2-D convolutional layer.
Therefore, our main contribution in introducing the Hyena
N-D layer is the creation of N-D implicit filters, which can
be utilized for N-D convolution. In the following two sec-
tions, we list two alternative strategies for the construction
of these filters, and illustrate them in Fig. 1. For simplicity,
although the Hyena N-D formulation can naturally corre-
spond to Ndimensions, we assume N= 2.
Using N-Dimensional Hyena as a Composition The
most straightforward way employs multiple independent 1-
D filters, similarly to S4ND (Nguyen et al. 2022). To ob-
tain an N-D filter for each channel, a 1-D filter Hn:=
(hn
1, hn
2, . . . , hn
Ln)of length Lnis independently learned for
each axis n∈[N]. The N 1-D filters are then combined to
form a single global N-D filter via an outer product opera-
tion per channel:
H=H1⊗H2⊗. . . HN
a notable drawback of this method is that parameteriz-
ing each dimension independently is unnatural for several
modalities, for example images, and can result in poor in-
ductive bias. We denote the layer as Hyena N-D product .Using Implicit N-Dimensional Hyena Filter In contrast
to the previous approach for generalizing Hyena to N-
dimensional data, this approach attempts to keep the spirit
of the original Hyena in the design of N-D implicit filters
rather than build a N-dimensional layer on top of the 1-D
Hyena layer. To do so, N-D implicit filters and N-D win-
dows are defined.
N-D Implicit Filter The implicit filters of the conventional
(i.e., 1-D) Hyena are defined by:
ht=window (t)·FFN(PE(t)) (2)
where the window function is described in Eq. 1 and the FFN
denotes a simple feed-forward network. A simple extension
of Eq. 2 into multidimensional filters with Ndimensions
n1, n2,···, nN, can be described by:
Hi1,i2,...,iN=window (i1, i2, . . . , i N)FFN(PE(i1, i2, . . . , i N))
(3)
N-D Window The Hyena 1-D window is defined by
window (t) =exp(−αt) +γ (4)
where tis the time-stamp, αis a decaying parameter and
γis a bias term. The following two window functions are
considered for the 2-D case:
window symmetric (i, j) =exp(−α(i+j)) +γ (5)
window dimensional (i, j) =exp(−αi+βj) +γ (6)
In Hyena 1-D, the parameter αchanges to regulate ef-
fective filter lengths across separate channels. This is ac-
complished by generating a sequence of evenly spaced non-
learnable αvalues. For the 2-D case, we modified αandβ
across separate channels and shuffled them randomly to ob-
tain a diverse set of windows. We analyzed the two window
functions, as well as the decision to make α, β andγas con-
stants in Tab 5. We also ablate the empirical contribution of
the window mechanism by omitting it entirely.
4 Model Extension
Multi Directional Layer Since the Hyena layer is causal,
previous data elements will not be affected by subsequent
ones. While this property is essential for language modeling,
it is very unnatural for computer vision tasks, since it lim-
its the model’s capability and contradicts the ViT principles.
To address this limitation, we introduce a multi-directional
extension to our layer. The following two versions are ex-
plored: (a) a 4-directional version, in which before each
layer, the input is projected into 4 separate representations,
then each representation is rotated. For any representation,
the Hyena layer is applied, and then a channel-wise linear
layer aggregates the 4 signals. (b) a 2-directional version, in
which a rotation is applied between the Hyena recurrence
steps, or between the Hyena layers for order 2. These strate-
gies are compared empirically in Tab 4.
Combining with Attention Although the empirical anal-
ysis in Sec. 6 demonstrates that when dealing with smaller
datasets, Hyena 2-D surpasses attention as the core layer of
ViT, it is unclear whether attention can be used to boost the
model’s performance further. Perhaps, Hyena 2D provides

--- PAGE 4 ---
Figure 1: Method: (Left) An image can be organized as a 1-dimensional sequence of patches with a classification token, or as
a 2-dimensional sequence of patches. The original attention-based ViT processed the patches as a 1-D sequence; Hyena-based
ViT operates directly on a 2-D sequence of patches. (Right) The Hyena 2-D layer architecture is compared to the original
Hyena. The modifications are in the parametrization of implicit filters, the type of global convolution, and the short filter type.
Hyena 2-D product uses the implicit filters of Hyena 1-D as a black-box while Hyena 2-D extends the 1-D layer.
distinct benefits that are different from those of the attention
model and the two can be fused synergistically to create a
better hybrid model.
To delve deeper into this aspect, we suggest two main
strategies for integrating those layers: (i) Alternate : In this
approach, for each pair of self-attention layers in the ViT
backbone, we replace the first layer with Hyena 2-D. This
approach can be interpreted as using Hyena 2-D to add in-
ductive bias to the attention mechanism, similarly to (Ma
et al. 2022; Baron, Zimerman, and Wolf 2023). (ii) Hyena
First: Employing Hyena 2-D for the first half of the layers,
and attention for the rest. The motivation for using Hyena
first is that image-specific inductive bias is more important
at the lower layers of the architecture, while the top layers
integrate information from across the image.
Sec. 6.1 analyzes these methods empirically. To further
understand the potential of the Hyena-First approach, we
tried a similar version that employs self-attention for the first
layers, denoted as Attention First . We found that the Hyena
First approach outperformed the others. We also observed
that the Alternate approach is superior to attention-free mod-
els, which demonstrates that the layers are complementary.
This observation could potentially boost performance even
in larger models or larger datasets.
5 Model Analysis
5.1 Complexity
The Hyena forward path consists of three steps: (i) con-
structing implicit filters, (ii) Applying N-D convolution, and
(iii) computing the input and output projections, where the
complexity of the last step is minimal.
Under the assumption that the hidden FFN dimension issmaller than the number of channels ( M≤C), the time and
space complexity of creating implicit filters in Hyena 1-D,
Hyena N-D and Hyena N-D product areLCM . For all Hyena
variants, the computation of the kernel does not depend on
the batch size B, making it more efficient for large batches.
Next, we apply an N-dimensional convolution between
the kernel and the input. Since the convolution can be ef-
ficiently computed with FFT, the total time complexity is
O(BCL log(L)for any dimension N, and the total space
complexity is O(BLC ). As can be seen, the convolu-
tion complexity dominates the overall complexity for large
batches. An empirical analysis of this advantage in linear
space-complexity is given in 6.3.
5.2 Expressiveness and inductive bias
We next characterize the expressiveness of the Hyena N-D
layer variants, starting by introducing a theoretical analysis
of the expressiveness of the Hyena N-D layers and then com-
paring it to other methods for creating implicit N-D filters.
Assumptions In this section, every theorem assumes that
the FFN network uses sign activations and M > 1. Fur-
thermore, for simplicity, both the positional encoding and
window functions are considered to be identity functions.
Tensor rank as a criterion for expressiveness We start
by introducing our criteria for measuring the expressive-
ness of the Hyena N-D layer. Inspired by Cohen, Sharir, and
Shashua (2016), which employs tensor rank as a criteria for
expressiveness, we apply tensor rank for the N-D kernels
constructed in the Hyena N-D layer, and prove the follow-
ing theorems:
Theorem 5.1. A single channel of the Hyena N-D product im-
plicit filter can only express kernels of rank 1.

--- PAGE 5 ---
Theorem 5.2. Given a N-dimensional sequence such that
∀n∈[N] :Ln=r, a single channel of the Hyena N-
D implicit filter with hidden dimension F≥2Nrand at
least 2 hidden layers with sign activations can express N-D
kernels of tensor rank r′for any r′∈[2, . . . , r ].
These results are based on the unique structure of Hyena
filters, which are obtained by employing a learnable function
over positional encoding. Hence, we can represent an N-D
filter with Ndimensions of size Lnper dimension with an
equivalent N-dimensional tensor Asuch that:
Ai1,i2,...iN:=MLP(PE(i1, i2, . . . i N)), (7)
where∀j∈[N] :ij∈[Ln].
Equipped with this formulation, the proof of Theorem 5.2
is specified in Appendix A; the proof of 5.1 is trivial, and de-
rives from the fact that to compute a global multi-axis kernel
H, Hyena N-D product takes the outer product operation on the
per-axis kernels Hn∈HLn×1for all n∈[N]. Since each
kernel is a vector, it is clear that:
rank(H) =rank(H1⊗H2⊗. . .⊗HD) = 1 (8)
Inductive bias towards low rank Theorem 5.2 was orig-
inally designed to evaluate the expressiveness of the Hyena
N-D layer. Nevertheless, it offers valuable insights into the
implicit regularization and the inductive bias of the implicit
filter mechanism. Theorem 5.2 introduces a linear parame-
ter scaling type of regularization and it is evident that when
the hidden dimension of the FFN layer increases, the poten-
tial rank increases as well, and the filters are biased toward
low-rank tensors.
Since regularization is seen as a crucial attribute for the
effectiveness of global convolution layers (Li et al. 2022; Fu
et al. 2023), it is imperative to rigorously define the type
of regularization present in the Hyena filters. To the best of
our knowledge, this is the first time the inductive bias of the
Hyena layer has been formalized.
Comparison of complexity and expressiveness with other
layers Tab. 1 compares the expressiveness and complexities
of implicit N-dimensional convolution layers. The baseline
layers are S4ND (Nguyen et al. 2022) and 2D-SSM (Baron,
Zimerman, and Wolf 2023). As can be seen, Hyena N-D is
the first layer that can express full-rank kernels for any di-
mension. Moreover, it has the same complexity as the Hyena
1-D layer when given sequences with an equal number of el-
ements for any number of dimensions.
6 Experiments
We evaluated our method on image classification bench-
marks across several ViT backbones, including ViT, Swin,
and DeiT in Sec. 6.1, followed by empirically justifying the
choices made in the Hyena N-D layer design in Sec. 6.2.
Finally, in Sec. 6.3 we empirically analyzed the memory ef-
ficiency of our layer against standard ViT.
Experimental setup All experiments are conducted using
PyTorch. The results of all experiments were averaged over
3 seeds, and we set the FFN dimension at 32 for all datasets.
As a deliberate decision, we do not perform hyperparameter
tuning of the backbone and training procedure, apart fromCriteria: Complexity Express
Layer Time Space 2-D N-D
Hyena 1-D LMC LMC - -
Multi-dimensional layers
S4ND C(˜L+˜N′)C(L+N′) 1 1
2-D SSM LLnCN LL nCN 2 N.a
Hy. N-D prod LMC LMC 1 1
Hy. N-D LMC LMC 2 N
Table 1: Complexity and expressiveness (Express) for N-
dimensional implicit filters with equal length Lnper dimen-
sion. Tildes denote log factors. We compare our Hyena N-D
variants (Hy.) from Sec. 3, and the two baselines (i) S4ND
(Nguyen et al. 2022) and (ii) 2-D SSM (Baron, Zimerman,
and Wolf 2023). For the baselines, state size is denoted by
N′. Expressiveness is analyzed using the tensor rank of the
kernel. For simplicity, we assume that C≥M.
stochastic depth. All hyperparameters are copied from the
baseline, which is (Lee, Lee, and Song 2021) for ViT and
Swin, and the DeiT repository (Touvron et al. 2021) for ex-
periments on CelebA. Naturally, these parameters were op-
timized for the vanilla (attention-based) transformers.
6.1 Hyena N-D as the core layer of ViT
We evaluate our method on CIFAR-100, Tiny-ImageNet
and CelebA, three classification benchmarks with different
scales. We report results both for architectures in which the
self-attention mechanism is replaced with N-D hyena and
for the hybrid methods of Sec. 4. As we show below, there is
a clear advantage to the Hyena-first hybrid method over the
other variants, which can be considered as ablations.
Baselines We compare our models with vanilla attention
and two improved versions of attention for the ViT, Swin and
DeiT backbone: (i) SL-transformers (Lee, Lee, and Song
2021), which constitute a data-efficient version of ViT for
handling small datasets, and (ii) 2-D SSM (Baron, Zimer-
man, and Wolf 2023) that incorporates inductive bias into
the attention mechanism using a layer that is built on top of
a two-dimensional state-space model. Both layers are specif-
ically designed to improve the inductive bias of the self-
attention layer within the ViT backbone.
ViT experiments For the ViT backbone, we first remove
the class token, since Hyena N-D operates on an ordered
2-D sequence. Then we replace each attention layer with
Hyena, Hyena 2-D, or Hyena 2-D product . As can be seen
in the upper part of Tab. 2, employing Hyena 1-D instead
of attention improves the results by 1.44% on CIFAR-100
and 2.61% on Tiny-Imagenet. The empirical contribution of
using Hyena 2-D instead of Hyena on those two datasets
is 0.45% and 0.32% respectively. The hybrid models also
seem effective. The Hyena-2D First approach consistently
surpasses the other approaches, performing on average 1.2%
higher than the Alternate hybrid approach, 3.46 % higher
than the attention first hybrid approach, and 4.23 % higher
than the standard attention model.

--- PAGE 6 ---
Compared to the recent baselines 2-D SSM and SL-ViT,
we found empirically that the Hyena 2-D based ViT is su-
perior to those two variants by a significant margin. For
instance, on the ViT backbone, the Hyena 2-D based ViT
performs, on average, 3.83% higher than attention with 2D-
SSM and 0.385% higher than SL-ViT. The results of the hy-
brid model are even better, but we did not test hybrid models
for these variants.
Swin experiments The Swin backbone improves the ViT ar-
chitecture by adopting two principles: (i) using a hierarchi-
cal structure of decreasing size patches across layers, which
is implemented in the backbone level, and (ii) using shifted
windows for better capture of spatial dependencies, which
is implemented efficiently in the layer-level via a modified
attention mask. As we replace each attention layer with sev-
eral Hyena variants that do not support mask handling, we
omit the second principle.
The empirical results, presented in Tab 2(bottom) show
that Hyena-based ViT is favorable to the attention-based
models, even without leveraging this shifting strategy. In
CIFAR-100, using Hyena 1-D instead of attention improves
results by 1.26%, and in Tiny-Imagenet, by 1.34%. Using
Hyena 2-Dinstead of Hyena 1-D boosts results further, to
1.96% and 2.03%, respectively. We observed that the Hyena-
based model notably surpasses the baselines. For instance,
the performance advantage is 1.645% over attention with
2D-SSM and 1.17% above SL-ViT.
Similarly to ViT, in Swin, the Hyena-2D First hybrid
model approach consistently surpasses the other approaches,
performing on average 2.42% higher than the Alternate hy-
brid approach, 6.18 % higher than the Attention first hybrid
approach, and 5.45 % higher than the standard Swin model.
DeiT experiments Similarly to ViT, we first remove the
CLS token and measure performance for each layer. We con-
duct the experiments on the large-scale CelebA dataset. The
original image size is 178x218, and it is resized to 224x224
to match the standard DeiT patch size. The dataset includes
40-way multi-label attribute classification. We report the av-
erage accuracy for all 40 tasks, training the models for 20
epochs, similarly to the procedure of (Nguyen et al. 2022;
Baron, Zimerman, and Wolf 2023) on this dataset.
As can be seen in Tab. 3, contrary to the finding in Tab.
2, removing the classification token and replacing atten-
tion with Hyena 1-D impacts the results negatively. How-
ever, when we integrated Hyena 2-D, the results improved
by 6% over the Hyena 1-D baseline. Incorporating the bi-
directional Hyena 2-D variant boosted results by 1.35%,
matching the attention-based model (without a classification
token). However, the original DeiT (attention with classifi-
cation token) is still more accurate.
As before, the Hyena-2D First approach outdoes the other
approaches, performing 0.23% higher than the Alternate hy-
brid approach, 2.12 % higher than the Attention first hybrid
approach, and 0.66 % higher than the standard DeiT. It also
outperforms by 0.55% the 2-D SSM-base baseline, which is
slightly better than DeiT itself.
6.2 Model variants
In this section we justify our design choices.Dataset: CIFAR-100 Tiny-Imagenet
Layer Acc. # Params(M) Acc. # Params(M)
ViT variants:
ViT 72.72 2.71 55.14 2.75
ViT (no CLS) 75.27 2.71 59.34 2.75
ViT w. 2-D SSM 74.07 2.72 57.66 2.75
SL-ViT 76.92 2.90 61.07 2.92
Hyena 1-D 76.71 2.72 61.95 2.74
Hyena 2-D product 77.16 2.79 62.23 2.74
Hyena 2-D 76.82 2.73 62.27 2.74
Hybrid Hyena 2-D First78.42 2.72 64.66 2.74
Hybrid Attention First 74.97 2.72 61.2 2.74
Hybrid Alternate 77.35 2.72 63.32 2.74
Swin variants:
Swin 77.60 7.11 60.06 7.15
SL-Swin 79.99 10.2 64.95 10.4
Swin w/ 2-D SSM 80.12 7.15 65.77 7.18
Hyena 1-D 78.86 7.23 61.81 7.29
Hyena 2-D product 80.82 7.51 65.43 7.64
Hyena 2-D 81.31 7.28 66.92 7.31
Hybrid Hyena 2-D First81.50 7.19 67.06 7.23
Hybrid Attention First 76.49 7.19 59.70 7.23
Hybrid Alternate 79.8 7.19 63.92 7.23
Table 2: Variants of ViT (top half) and Swin (bottom half)
for small datasets
Multi-directional Tab. 4 explores two approaches for
efficiently modifying the Hyena layer to consider multi-
directional data. As expected, for both Hyena N-D and
Hyena N-D product , the multi-directional approach improves
the results by ∼1-1.5 %. We observe that the 2-D approach,
which rotates the input before each step on the Hyena re-
currence performed slightly better than the 4-directional ap-
proach. It is also important to note that the 2-directional
version has negligible additional computation than the 1-
directional variant.
Window function As detailed in Sec. 3, we explore several
window functions, which are evaluated in Tab. 5. First, we
compare the symmetric (Eq. 5) and the dimensional (Eq. 6)
windows functions within the Hyena 2-D layer. We found
that the dimensional function performs 1.04 % better, hence
we choose it as our standard window function. Next, we ab-
late the window mechanism by omitting it and observe a
degradation in accuracy of 0.54 % for Hyena 2-D and 0.26
% for Hyena 2-D product . Finally, we try to learn the window
function by parameterizing Eq. 6 separately for each chan-
nel. This decreases the results by 1.36% for Hyena 2-D and
by 0.93% for Hyena 2-D product .
6.3 Efficiency for large amounts of patches
One additional benefit of Hyena-based ViT compared to
Attention-based ViT is its enhanced complexity in terms of
time and memory, as detailed in Sec. 5.1. To evaluate the
memory efficiency of Hyena-ViT in comparison to the stan-
dard ViT, we conducted experiments using different patch

--- PAGE 7 ---
Layer Acc. # Params
DeiT 89.73 5.532
DeiT (w/o CLS token) 88.48 5.531
DeiT w/ 2-D SSM 89.84 5.541
DeiT w/ Hyena 1-D 80.93 5.81
DeiT w/ Hyena 2-D product 88.16 5.84
DeiT w/ Hyena 2-D 88.68 5.66
Hybrid DeiT Hyena 2-D First 90.39 5.61
Hybrid DeiT Attention First 88.27 5.61
Hybrid DeiT Alternate 90.16 5.61
Table 3: Variants of DeiT for the Celeb-A dataset.
Layer 1-Dir 2-Dir 4-Dir
Hyena 2-D product 86.76 88.16 88.09
Hyena 2-D 87.33 88.68 88.31
Table 4: Ablation results for multi-directional methods, on
Celeb-A and the DeiT-S backbone.
Layer Constant Learnable Symm. w/o
Hyena 2-D product 86.76 85.83 N/A 86.54
Hyena 2-D 87.33 85.97 86 .29 86 .79
Table 5: Ablation results for window methods (tested on 1-
Dir), on Celeb-A and the DeiT-S backbone.
sizes and measured the peak GPU memory consumption
during the forward pass. Fig. 2 demonstrates the signifi-
cantly improved memory consumption of Hyena-ViT.
Employing a large number of patches can be critical in
two main scenarios: (i) processing high-resolution images,
and (ii) working with smaller patches. Previous studies have
shown that overly large patches can negatively impact the
accuracy of ViT, and smaller patches generally tend to pro-
vide better image-specific inductive bias. We examined how
the patch size of Hyena-Hybrid ViT affects accuracy in
Fig. 3. The results indicate that Hybrid-ViT also benefits
from smaller patches, without a quadratic increase in mem-
ory consumption in half of the layers. Thus, Hyena-Hybrid
ViT and Hyena-ViT present an opportunity to develop cost-
effective ViT models with significantly smaller patches at
the same cost.
7 Limitations
The move to N-D Hyena-based pooling instead of attention
prevents us from using the CLS token, which could be use-
ful. As future work, we would like to add such tokens not as
a concatenation, but rather as a conditioning signal. Further-
more, as shown in Swin, self-attention can be easily modi-
fied with a domain-dependent mask that enforces a specific
shape of inductive bias. Our N-D Hyena lacks such a mech-
anism. As future work, we would like to investigate whether
the N-D window can be modified for similar purposes.
Figure 2: Peak Memory Consumption for Attention-Based
and Hyena-Based ViT per patch size
Figure 3: Impact of patch size on the accuracy of Attention-
based ViT and Hyena Hybrid ViT. For both Attention and
Hyena layers we use the best variants: a CLS token for the
Attention layer and a 2-directional layer for the Hyena 2-D.
8 Discussion and Future Work
In this work, we extend the recent Hyena layer into multi-
dimensional data and demonstrate that it can be leveraged
to create a data- and memory-efficient variant of ViT. We
show that a few design choices, such as (i) inserting induc-
tive bias of 2-dimensional locality via employing 2-D in-
stead of 1-D implicit filters, (ii) extending the layer to be
a multi-directional operator, and (iii) merging attention and
Hyena in a specific manner can notably improve the perfor-
mance of ViT across various benchmarks.
For future research, we plan on exploring the empirical
power of the Hyena 2-D layer in scenarios corresponding to
its advantages. As one clear advantage of Hyena-based ViT
over vanilla ViT is time- and memory complexity, employ-
ing Hyena-ViT in scenarios that require processing large
amounts of patches, as well as real-time or low-budget re-
strictions, is very promising. Finally, we are interested in
benchmarking Hyena-ViT on tasks beyond classification,
such as segmentation and generation, as well as applying
the layer directly to other N-dimensional modalities, such as
speech and video.

--- PAGE 8 ---
References
Arnab, A.; Dehghani, M.; Heigold, G.; Sun, C.; Lu ˇci´c, M.;
and Schmid, C. 2021. Vivit: A video vision transformer. In
Proceedings of the IEEE/CVF international conference on
computer vision , 6836–6846.
Baron, E.; Zimerman, I.; and Wolf, L. 2023. 2-D SSM:
A General Spatial Layer for Visual Transformers. arXiv
preprint arXiv:2306.06635 .
Bi, Y .; Lu, Y .; Long, Z.; Zhu, C.; and Liu, Y . 2022. Ten-
sor decompositions: computations, applications, and chal-
lenges. Tensors for Data Processing , 1–30.
Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,
A.; and Zagoruyko, S. 2020. End-to-end object detection
with transformers. In European conference on computer vi-
sion, 213–229. Springer.
Chen, X.; Wei, F.; Zeng, G.; and Wang, J. 2022. Conditional
detr v2: Efficient detection transformer with box queries.
arXiv preprint arXiv:2207.08914 .
Cohen, N.; Sharir, O.; and Shashua, A. 2016. On the expres-
sive power of deep learning: A tensor analysis. In Confer-
ence on learning theory , 698–728. PMLR.
Dai, Z.; Liu, H.; Le, Q. V .; and Tan, M. 2021. Coatnet: Mar-
rying convolution and attention for all data sizes. Advances
in neural information processing systems , 34: 3965–3977.
Dao, T.; Fu, D. Y .; Saab, K. K.; Thomas, A. W.; Rudra,
A.; and R ´e, C. 2022. Hungry hungry hippos: Towards lan-
guage modeling with state space models. arXiv preprint
arXiv:2212.14052 .
David, S. B.; Zimerman, I.; Nachmani, E.; and Wolf, L.
2022. Decision S4: Efficient Sequence-Based RL via State
Spaces Layers. In The Eleventh International Conference on
Learning Representations .
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 .
d’Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;
Biroli, G.; and Sagun, L. 2021. Convit: Improving vision
transformers with soft convolutional inductive biases. In In-
ternational Conference on Machine Learning , 2286–2296.
PMLR.
Fu, D. Y .; Epstein, E. L.; Nguyen, E.; Thomas, A. W.; Zhang,
M.; Dao, T.; Rudra, A.; and R ´e, C. 2023. Simple hardware-
efficient long convolutions for sequence modeling. arXiv
preprint arXiv:2302.06646 .
Fukushima, K. 1980. A self-organizing neural network
model for a mechanism of pattern recognition unaffected by
shift in position. Biol, Cybern , 36: 193–202.
Gu, A.; Goel, K.; and R ´e, C. 2021. Efficiently modeling
long sequences with structured state spaces. arXiv preprint
arXiv:2111.00396 .Gu, A.; Johnson, I.; Goel, K.; Saab, K.; Dao, T.; Rudra, A.;
and R ´e, C. 2021. Combining recurrent, convolutional, and
continuous-time models with linear state space layers. Ad-
vances in neural information processing systems , 34: 572–
585.
Guo, J.; Han, K.; Wu, H.; Tang, Y .; Chen, X.; Wang, Y .; and
Xu, C. 2022. Cmt: Convolutional neural networks meet vi-
sion transformers. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 12175–
12185.
Gupta, A.; Gu, A.; and Berant, J. 2022. Diagonal state
spaces are as effective as structured state spaces. Advances
in Neural Information Processing Systems , 35: 22982–
22994.
Gupta, A.; Mehta, H.; and Berant, J. 2022. Simplifying
and Understanding State Space Models with Diagonal Lin-
ear RNNs. arXiv preprint arXiv:2212.00768 .
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-
ual learning for image recognition. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, 770–778.
LeCun, Y .; Boser, B.; Denker, J. S.; Henderson, D.; Howard,
R. E.; Hubbard, W.; and Jackel, L. D. 1989. Backpropaga-
tion applied to handwritten zip code recognition. Neural
computation , 1(4): 541–551.
LeCun, Y .; Bottou, L.; Bengio, Y .; and Haffner, P. 1998a.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE , 86(11): 2278–2324.
LeCun, Y .; Bottou, L.; Bengio, Y .; and Haffner, P. 1998b.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE , 86(11): 2278–2324.
Lee, S. H.; Lee, S.; and Song, B. C. 2021. Vision transformer
for small-size datasets. arXiv preprint arXiv:2112.13492 .
Li, Y .; Cai, T.; Zhang, Y .; Chen, D.; and Dey, D. 2022.
What Makes Convolutional Models Great on Long Se-
quence Modeling? arXiv preprint arXiv:2210.09298 .
Liu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,
S.; and Guo, B. 2021. Swin transformer: Hierarchical vi-
sion transformer using shifted windows. In Proceedings of
the IEEE/CVF international conference on computer vision ,
10012–10022.
Liu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.;
and Xie, S. 2022a. A convnet for the 2020s. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , 11976–11986.
Liu, Z.; Ning, J.; Cao, Y .; Wei, Y .; Zhang, Z.; Lin, S.; and
Hu, H. 2022b. Video swin transformer. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , 3202–3211.
Lu, C.; Schroecker, Y .; Gu, A.; Parisotto, E.; Foerster, J.;
Singh, S.; and Behbahani, F. 2023. Structured state space
models for in-context reinforcement learning. arXiv preprint
arXiv:2303.03982 .
Lutati, S.; Zimerman, I.; and Wolf, L. 2023. Focus Your
Attention (with Adaptive IIR Filters). arXiv preprint
arXiv:2305.14952 .

--- PAGE 9 ---
Ma, X.; Zhou, C.; Kong, X.; He, J.; Gui, L.; Neubig, G.;
May, J.; and Zettlemoyer, L. 2022. Mega: moving average
equipped gated attention. arXiv preprint arXiv:2209.10655 .
Mehta, H.; Gupta, A.; Cutkosky, A.; and Neyshabur, B.
2022. Long range language modeling via gated state spaces.
arXiv preprint arXiv:2206.13947 .
Nguyen, E.; Goel, K.; Gu, A.; Downs, G.; Shah, P.; Dao, T.;
Baccus, S.; and R ´e, C. 2022. S4nd: Modeling images and
videos as multidimensional signals with state spaces. Ad-
vances in neural information processing systems , 35: 2846–
2861.
Nguyen, E.; Poli, M.; Faizi, M.; Thomas, A.; Birch-Sykes,
C.; Wornow, M.; Patel, A.; Rabideau, C.; Massaroli, S.;
Bengio, Y .; et al. 2023. Hyenadna: Long-range genomic
sequence modeling at single nucleotide resolution. arXiv
preprint arXiv:2306.15794 .
Peng, B.; Alcaide, E.; Anthony, Q.; Albalak, A.; Arcadinho,
S.; Cao, H.; Cheng, X.; Chung, M.; Grella, M.; GV , K. K.;
et al. 2023. RWKV: Reinventing RNNs for the Transformer
Era. arXiv preprint arXiv:2305.13048 .
Poli, M.; Massaroli, S.; Nguyen, E.; Fu, D. Y .; Dao, T.; Bac-
cus, S.; Bengio, Y .; Ermon, S.; and R ´e, C. 2023. Hyena hier-
archy: Towards larger convolutional language models. arXiv
preprint arXiv:2302.10866 .
Romero, D. W.; Kuzina, A.; Bekkers, E. J.; Tomczak,
J. M.; and Hoogendoorn, M. 2021. Ckconv: Continu-
ous kernel convolution for sequential data. arXiv preprint
arXiv:2102.02611 .
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:
Convolutional networks for biomedical image segmenta-
tion. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, 234–241. Springer.
Saon, G.; Gupta, A.; and Cui, X. 2023. Diagonal state space
augmented transformers for speech recognition. In ICASSP
2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) , 1–5. IEEE.
Touvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,
A.; and J ´egou, H. 2021. Training data-efficient image trans-
formers & distillation through attention. In International
conference on machine learning , 10347–10357. PMLR.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Wang, J.; Yan, J. N.; Gu, A.; and Rush, A. M. 2022. Pretrain-
ing without attention. arXiv preprint arXiv:2212.10544 .
Xie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;
and Luo, P. 2021. SegFormer: Simple and efficient design
for semantic segmentation with transformers. Advances in
Neural Information Processing Systems , 34: 12077–12090.
Xu, Y .; Zhang, Q.; Zhang, J.; and Tao, D. 2021. Vitae: Vi-
sion transformer advanced by exploring intrinsic inductive
bias. Advances in neural information processing systems ,
34: 28522–28535.

--- PAGE 10 ---
A Expressiveness
Theorem A.1. Given an N-dimensional sequence such that
∀d∈[N] :Ln=r, a single channel of the Hyena N-
D implicit filter with hidden dimension F≥2Nrand at
least 1 hidden layers with sign activations can express N-D
kernels of tensor rank r′for any r′∈[2, . . . , r ]
Proof. We prove the theorem by showing that a single chan-
nel of the Hyena N-D implicit filter can express the N-
dimensional identity tensor (Bi et al. 2022) for any dimen-
sionD > 1in Lemma. A.3. Thus, it is clear that a single
channel of the Hyena N-D implicit filter can express full-
rank kernels, and then we generalize the proof to kernels of
any rank r∈[D]
We start by introducing the identity tensor:
Definition A.2. Identity tensor (Bi et al. 2022) : The ele-
ments of the N-dimensional identity tensor Iare given by
Ij1,j2,···,jN=1ifj1=j2=···=jN
0otherwise
Lemma A.3 (Hyena N-D as the identity tensor) .The Hyena
N-D implicit filter with hidden dimension F≥2Nrand at
least 2 hidden layers with sign activations can express the
identity tensor of dimension r.
Generalization to any rank
Based on Lemma A.3, we can construct an FFN that em-
bodies the identity tensor.
A unique characteristic of this FFN construction is the
ability to adjust tensor rank through weight modifications
in the final layer. Specifically, the weights W3are defined
as:
w3
1,i=1ifi≤r′
0otherwise
By this configuration, only the first r’ neurons signifi-
cantly influence the output, effectively transforming the ten-
sor rank from r to r’
Following the weight adjustment specified above, the ten-
sor can be truncated to its initial r′elements across every
dimension. These elements inherently define an identity ten-
sor of rank r′. Any elements beyond this truncated set are
zeros. Given the properties of tensor rank, the introduction
of these zero elements does not augment the tensor rank.
Proof of Lemma A.3. We prove the lemma using a general
example. For simplicity, we assume that the positional en-
coding function is the identity function. Thus, we consider
the following FFN network:
FFN Definition Let the input layer have Nneurons, the
first hidden layer 2Nrneurons, the second hidden layer r
neurons, and the output layer 1 neuron.
Given an input vector x∈[r]Nthat represents the posi-
tional encoding:
The output of the hidden layers is:
h1=sign(W1x+b1),h2=sign(W2h1+b2)where W2∈Rh2×h1,b2∈Rh2,W1∈Rh1×nandb1∈
Rh1.
and the output of the network is:
y=sign(W3h2+b3)
where W3∈R1×h2andb3∈R1.
FFN Substitution We will substitute values in
W1,W2,W3andb1,b2,b3such that the FFN im-
plements the identity tensor. To achieve this, we use the
first layer to obtain a one-hot representation per dimension,
which the last two layers will convert into the desired
function.
Thus, we will substitute the values of the first hidden layer
W1which is denoted by w1i,jandb1as follows:
w1
i,j=

1 ifi≤Nrand floor (i/N) =j
−1ifi > Nr and floor (i/N) =j+Nr
0 otherwise
bi=

i−1
2ifi≤Nr
i+1
2ifi > Nr
0 otherwise
Given the output of the first layer
h1:= (h11, h12,···, h12Nr)
it is easy to see that the pair of neurons h1Ni+j, h1N(r+i)+j
are active if and only if xi=j.
Similarly, we define the second layer as follows:
w2
i,j=1ifj%r=i
0otherwise
b2= (−δ,−δ,···,−δ), δ =−2N+1
2
Given the output of the second layer
h3:= (h21, h22,···, h2r)
it easy to see that ∀i∈[N], j∈Ln:h2j= 1if and only if
∀i∈[N] :xi=j.
Hence, by using the last layer as an ”OR” gate, which can
be achieved by setting the last layer as follows:
w3
1,i=1ifi≤r
0otherwise
b3=−1
2
It is clear that the FFN network implements the identity ten-
sor.
