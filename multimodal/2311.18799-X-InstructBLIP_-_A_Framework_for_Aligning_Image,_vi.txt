# 2311.18799.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.18799.pdf
# Kích thước tệp: 5500342 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
X-InstructBLIP: Một Khung Làm Việc để Căn Chỉnh Hình Ảnh,
3D, Âm Thanh, Video với LLMs và Khả Năng Suy Luận
Đa Phương Thức Nổi Lên

Artemis Panagopoulou1⋆, Le Xue2,∗∗, Ning Yu2,∗∗, Junnan Li2, Dongxu Li2,
Shafiq Joty2, Ran Xu2, Silvio Savarese2, Caiming Xiong2, và Juan Carlos
Niebles2

1Đại học Pennsylvania
artemisp@seas.upenn.edu
2Salesforce AI Research

Tóm tắt. Nghiên cứu gần đây đã đạt được những tiến bộ đáng kể trong các tác vụ suy luận thị giác thông qua việc học các phép chiếu từ hình ảnh đến ngôn ngữ và tận dụng khả năng suy luận ấn tượng của các Mô hình Ngôn ngữ Lớn (LLM). Bài báo này giới thiệu một khung làm việc hiệu quả và hiệu suất cao tích hợp nhiều phương thức (hình ảnh, 3D, âm thanh và video) vào một LLM đông lạnh và chứng minh khả năng nổi lên cho suy luận đa phương thức (đầu vào 2+ phương thức). Phương pháp của chúng tôi khám phá hai cơ chế chiếu khác biệt: Q-Former và Phép Chiếu Tuyến Tính (LP). Thông qua thử nghiệm rộng rãi trên tất cả bốn phương thức trên 16 bộ đánh giá, chúng tôi khám phá cả hai phương pháp và đánh giá khả năng thích ứng của chúng trong suy luận đa phương thức tích hợp và tách biệt. Phép chiếu Q-Former cho thấy hiệu suất vượt trội trong các tình huống một phương thức và khả năng thích ứng trong suy luận kết hợp so với phân biệt liên quan đến hai hoặc nhiều phương thức hơn. Tuy nhiên, nó thể hiện khả năng tổng quát hóa thấp hơn so với phép chiếu tuyến tính trong bối cảnh dữ liệu tác vụ-phương thức bị hạn chế. Để kích hoạt khung làm việc này, chúng tôi thiết kế một pipeline có thể mở rộng tự động tạo ra các tập dữ liệu điều chỉnh hướng dẫn chất lượng cao từ dữ liệu chú thích có sẵn trên các phương thức khác nhau, và đóng góp 24K dữ liệu QA cho âm thanh và 250K dữ liệu QA cho 3D. Để tạo điều kiện cho nghiên cứu tiếp theo về suy luận đa phương thức, chúng tôi giới thiệu bộ đánh giá DisCRn (Suy luận Đa phương thức Phân biệt) bao gồm 9K mẫu QA âm thanh-video và 28K mẫu QA hình ảnh-3D yêu cầu mô hình suy luận một cách phân biệt trên các phương thức đầu vào khác nhau. Mã nguồn và dữ liệu có sẵn tại https://github.com/salesforce/LAVIS/tree/main/projects/xinstructblip.

Từ khóa: đa phương thức · căn chỉnh đa phương thức · suy luận đa phương thức

1 Giới thiệu

Con người vốn dĩ xử lý thông tin từ nhiều phương thức cảm giác để diễn giải môi trường xung quanh và đưa ra quyết định dựa trên cái nhìn toàn diện về⋆Công việc được thực hiện trong thời gian thực tập tại Salesforce Research,∗∗Đóng góp cố vấn bằng nhau.arXiv:2311.18799v2 [cs.CV] 9 Sep 2024

--- TRANG 2 ---
2 A. Panagopoulou et al.

môi trường của họ. Tuy nhiên, các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) chủ yếu tập trung vào các tác vụ thị giác, thường bỏ qua sự đa dạng phong phú của các phương thức phổ biến khác như Âm thanh, Video và 3D, và không khai thác được tiềm năng hiểu biết toàn diện về nhiều phương thức (>2) cùng lúc, điều này rất quan trọng cho các tác vụ nâng cao như suy luận đa phương thức3.

Việc tích hợp các phương thức khác nhau ngoài hình ảnh vào LLM vẫn là một lĩnh vực còn nhiều tiềm năng để khám phá, đặc biệt là về các khung tích hợp hiệu quả. Một thách thức đáng kể nằm ở việc thiếu các tập dữ liệu điều chỉnh hướng dẫn cho các phương thức khác như Âm thanh, 3D và Video, đặc biệt là cho dữ liệu liên quan đến hai hoặc nhiều phương thức cùng lúc, khiến việc huấn luyện phương thức kết hợp trở thành một cách tiếp cận khả thi nhưng tốn kém tài nguyên để kích hoạt suy luận đa phương thức.

Để đối phó với những thách thức trên, chúng tôi giới thiệu X-InstructBLIP, một khung làm việc có thể mở rộng - được minh họa trong Hình 1 và phân tích thêm trong Phần 3 - được thiết kế để căn chỉnh các phương thức khác nhau (hình ảnh, 3D, âm thanh, video) với LLM, đạt được các tác vụ suy luận một phương thức cho từng phương thức và kích hoạt suy luận đa phương thức trên ba hoặc nhiều phương thức hơn. Để tạo điều kiện cho việc khám phá này và do sự khan hiếm dữ liệu điều chỉnh hướng dẫn đơn nguyên cho một loạt các phương thức khác ngoài phương thức hình ảnh, chúng tôi giới thiệu một phương pháp đơn giản nhưng hiệu quả trong Phần 4.1: một kỹ thuật tăng cường dữ liệu truy vấn ba giai đoạn để tận dụng các LLM mã nguồn mở nhằm trích xuất dữ liệu điều chỉnh hướng dẫn từ các tập dữ liệu chú thích.

Khung làm việc của chúng tôi khám phá hai cơ chế chiếu tiên tiến trên các LLM đông lạnh - một điều kiện tiên quyết để duy trì việc huấn luyện phương thức riêng biệt - Q-Former nhận thức hướng dẫn [19] và phép chiếu tuyến tính [59]. Thông qua đánh giá mở rộng trên 13 bộ đánh giá trên 4 phương thức, chúng tôi thấy rằng Q-Former có xu hướng thể hiện hiệu suất cao hơn trên các tác vụ một phương thức và tính linh hoạt trong việc phân biệt khi nào nên suy luận theo cách kết hợp hoặc phân biệt khi có 2+ phương thức ngoài ngôn ngữ. Hình 2 cho thấy các kết quả minh họa, làm nổi bật khả năng của khung làm việc của chúng tôi. Để định lượng và thách thức khả năng nổi lên này, chúng tôi giới thiệu DisCRn trong Phần 4.2, một tập dữ liệu thách thức Suy luận Đa phương thức Phân biệt được tuyển chọn tự động yêu cầu các mô hình phân biệt giữa các kết hợp đa dạng của các phương thức, như âm thanh-video và 3D-hình ảnh.

Các đóng góp của chúng tôi được tóm tắt như sau:
(i) Chúng tôi giới thiệu một khung làm việc có thể mở rộng căn chỉnh Hình ảnh, 3D, Âm thanh và Video với LLM, và chúng tôi đánh giá khả năng suy luận đa phương thức nổi lên của nó qua hai cơ chế chiếu. Khung làm việc này không cần huấn luyện trước cụ thể được điều chỉnh cho từng phương thức. Theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên chứng minh rằng suy luận đa phương thức phân biệt xuất hiện một cách tự nhiên thông qua việc căn chỉnh phương thức cá nhân với LLM.

(ii) Chúng tôi giới thiệu một phương pháp tự động để tạo ra các tập dữ liệu điều chỉnh hướng dẫn cho nhiều phương thức khác nhau, chỉ tận dụng dữ liệu chú thích có sẵn và các mô hình ngôn ngữ mã nguồn mở. Đóng góp ~250k mẫu cho dữ liệu QA 3D và ~24k mẫu cho dữ liệu QA Âm thanh.

3Suy luận đa phương thức là khả năng tích hợp và phân biệt thông tin từ nhiều phương thức qua văn bản, trái ngược với "suy luận đa phương thức," thường dành cho các tác vụ thị giác-ngôn ngữ.

--- TRANG 3 ---
X-InstructBLIP 3

Hình 1: Mặc dù sử dụng các bộ mã hóa được huấn luyện trước khác biệt cho từng phương thức và căn chỉnh chúng độc lập với ngôn ngữ thông qua các Q-Former nhận thức hướng dẫn cá nhân, X-InstructBLIP chứng minh khả năng nổi lên trong hiểu biết đa phương thức.

(iii) Chúng tôi thu thập DisCRn, tập dữ liệu đầu tiên được thiết kế để đánh giá suy luận phân biệt đa phương thức dựa trên hướng dẫn. Bao gồm ~36k ví dụ trên các phương thức khác nhau như video, âm thanh, 3D và hình ảnh.

2 Nghiên cứu liên quan

Mô hình Thị giác Ngôn ngữ: Những năm gần đây đã chứng kiến sự gia tăng mạnh mẽ của các mô hình có khả năng thực hiện một loạt các tác vụ thị giác-ngôn ngữ, dẫn đến việc tạo ra các Mô hình Ngôn ngữ Đa phương thức (MLM). Những mô hình này căn chỉnh các biểu diễn thị giác và ngôn ngữ tĩnh thông qua các kỹ thuật khác nhau, như huấn luyện trước thống nhất [18,39,47,83,90,92–94,108], căn chỉnh thị giác-ngôn ngữ thông qua trích xuất đặc trưng văn bản [31,56,80,96,109], tối ưu hóa bộ mã hóa thị giác [86], và phép chiếu tuyến tính [21,45,59], dựa trên transformer [12,67,73], hoặc dựa trên bộ mã hóa tự động [58,111]. Liên quan hơn đến công trình này là các phương pháp học biểu diễn token ngôn ngữ thông tin thị giác trung gian hoặc xen kẽ trong các lớp LLM như trong Flamingo [3] và LLaMA adapter [114] hoặc chỉ đến lớp đầu vào như trong chuỗi BLIP [19,50,51] sử dụng phép chiếu dựa trên Q-Former, LLAVA [59], và MiniGPT4 [116] sử dụng phép chiếu tuyến tính.

Mô hình Ngôn ngữ Đa phương thức: Các phương pháp dựa trên phép chiếu, ban đầu tập trung vào hình ảnh, gần đây đã mở rộng để bao gồm các phép chiếu âm thanh [20,28,44,85], video [4,63,66,107], và 3D [32,37,104] vào các mô hình ngôn ngữ lớn được huấn luyện trước (LLM). Sự mở rộng này đã chứng kiến sự ra đời của các khung huấn luyện trước thống nhất như mPLUG2 [102] và OnePeace [91], cũng như các phương pháp dựa trên phép chiếu để tăng cường các LLM đông lạnh, như VideoLLaMA [113] và X-LLM [11], nhằm huấn luyện chung các bộ xử lý âm thanh và video. Đáng chú ý, X-LLM tập trung vào việc tích hợp này chủ yếu trong các giai đoạn sau của huấn luyện. Tương tự, ChatBridge [115] áp dụng phương pháp huấn luyện tương tự như X-LLM nhưng sử dụng phép chiếu dựa trên perceiver [41]. Audio-Visual LLM [81] tuân theo mô hình huấn luyện tương tự với giai đoạn tinh chỉnh chung cuối cùng, nhưng thay vì duy trì LLM đông lạnh, nó cập nhật nó bằng LoRA [38]. Phương pháp của chúng tôi được phân biệt bằng cách duy trì tinh chỉnh độc lập suốt quá trình và LLM đông lạnh tránh sự bất ổn trong huấn luyện do các phép chiếu phương thức được căn chỉnh khác nhau. Một hướng nghiên cứu khác, bao gồm ImageBind-LLM [35], PandaGPT [82] và PointLLM [32]

--- TRANG 4 ---
4 A. Panagopoulou et al.

[tiếng trẻ cười] H: Khả năng nghe thấy những âm thanh này ở địa điểm được mô tả trong hình ảnh như thế nào? Đ: rất có khả năng nghe thấy những âm thanh này ở địa điểm được mô tả trong hình ảnh

[đọc thơ] H: Đầu vào nào có nhiều khả năng diễn ra trong lớp học hơn? Đ: Âm thanh

H: Mô tả ngắn gọn về video.

[nhịp điệu âm nhạc] H: Mô tả mô hình 3D. Đ: Một hoạt hình 3D của màn hình máy tính H: Âm thanh nào đang phát trong đoạn âm thanh? H: Cho âm thanh, hãy mô tả video. Đ: Âm nhạc đang phát Đ: Đây là ảnh chụp màn hình phần mềm DJ của phần mềm DJ trên laptop

H: Mô tả mô hình 3D. Đ: Mô hình 3D của nhân vật hoạt hình với chân và tay dài, giống như quái vật

H: Mô tả hình ảnh.

[here comes the sun] H: Ban nhạc nào chơi bài hát nổi tiếng này? Đ: The Beatles H: Bạn nghe thấy nhạc cụ nào? Đ: Guitar

[giai điệu nutcracker] H: Tên của giai điệu nổi tiếng này là gì? Đ: Waltz of the Snowflakes từ Nutcracker Suite của Tchaikovsky H: Mô tả chi tiết hình ảnh Đ: Hình ảnh cho thấy mô hình 3d của một con vật màu trắng và đỏ, có vẻ như là một con mèo hoặc sói đang đứng bằng hai chân sau.

H: Mô tả chi tiết hình ảnh sử dụng mô hình 3D tương ứng. Đ: Mô hình 3D của con hổ trắng với mắt đỏ và đuôi dài, đứng bằng hai chân sau.

H: Cái nào diễn ra trong nhà bếp? Đ: Video

Đ: Mô hình 3D của khủng long triceratops với sừng và đuôi dài

[baa-] H: Con vật này là gì? Đ: Bò Đ: Cừu H: Sử dụng cả âm thanh và mô hình 3d, con vật này là gì?

[sōkyoku] H: Tôi đang ở quốc gia nào? Đ: Trung Quốc H: Tôi đang ở quốc gia nào? Đ: Nhật Bản +

Đ: Có bốn con vẹt màu vàng đang ngồi trên một cành trước hàng rào lưới, những con chim này đang đậu cùng nhau và có vẻ như đang tận hưởng sự đồng hành của nhau, chúng được bao quanh bởi cây xanh và tán lá.

Hình 2: Ví dụ Định tính: Khung làm việc X-InstructBLIP xử lý hiệu quả cả các tác vụ đơn phương thức và đa phương thức mà không cần huấn luyện trên dữ liệu kết hợp.

tận dụng các biểu diễn thống nhất như ImageBind [27] để chỉ căn chỉnh ngầm các phương thức bổ sung với LLM bằng cách chỉ huấn luyện trên các cặp hình ảnh-văn bản. Các công trình đương đại như AnyMAL [70] và OneLLM [34] đã đẩy ranh giới xa hơn bằng cách mở rộng ứng dụng của các phương pháp dựa trên phép chiếu cho các phương thức bổ sung, như 3D. Khác với các mô hình khác giữ LLM đông lạnh, cả hai đều chọn không đông lạnh LLM trong quá trình huấn luyện. OneLLM áp dụng chiến lược hỗn hợp chuyên gia dựa trên bộ định tuyến để học ánh xạ giữa các phương thức khác nhau. Ngược lại, AnyMAL tập trung vào việc học chung lớp Phép chiếu kiểu LLaVA cho từng phương thức trong một phần của quá trình huấn luyện.

Tác vụ Ngôn ngữ Đa phương thức Đa đầu vào: Những tiến bộ trong các tác vụ thị giác-ngôn ngữ đầu vào đơn đã mở đường cho việc phát triển các tác vụ đòi hỏi mô hình phải đồng thời suy luận về nhiều đầu vào phi ngôn ngữ, như tham gia vào suy luận không gian trên nhiều hình ảnh [5], suy ngẫm về một loạt slide [84], trả lời các truy vấn đòi hỏi suy luận đa phương thức trên hình ảnh và bảng [54], hoặc thực hiện một loạt tác vụ dựa trên hướng dẫn liên quan đến nhiều đầu vào hình ảnh [54]. Mặc dù phức tạp, những tác vụ này hoạt động chủ yếu trong phạm vi các phương thức hình ảnh-văn bản. Mặc dù các tác vụ đa phương thức tồn tại, chủ yếu yêu cầu mô hình suy luận về âm thanh kết hợp

--- TRANG 5 ---
X-InstructBLIP 5

"{Phương thức-X}:" LLM Điều chỉnh Hướng dẫn ❄ Phương thức-X Q-Former 🔥 Hướng dẫn Hướng dẫn Truy vấn ………… Tái sử dụng Tiền tố LLM Phép chiếu 🔥 … Phương thức-X Nhúng Mã hóa Phương thức-X ❄ LayerNorm 🔥

(a) Phép chiếu X-Instruct

"{Phương thức-X}:" LLM Điều chỉnh Hướng dẫn ❄ Hướng dẫn …… Tiền tố LLM Phép chiếu 🔥 Mã hóa Phương thức-X ❄ LayerNorm 🔥

(b) Phép chiếu Kiểu X-LLaVA

Hình 3: Các loại phép chiếu được khám phá trong Khung làm việc X-InstructBLIP. (a) là phép chiếu Q-Former nhận thức hướng dẫn [19] và (b) là phép chiếu tuyến tính [59].

và video [2,49], có một khoảng trống trong việc đánh giá khả năng sinh sản của mô hình trong việc suy luận về các đầu vào đa phương thức một cách tương phản. Trong khi các mô hình thường được tối ưu hóa trên các mục tiêu tương phản [15,16,36,42,50,52,53,77], ngay cả trong các thiết lập đa phương thức [27,33,72], việc đánh giá của chúng bị giới hạn trong các tác vụ phân loại hoặc sử dụng các biểu diễn được học tương phản cho các tác vụ phía sau. Để giải quyết khoảng trống này, chúng tôi giới thiệu DisCRn, một tác vụ yêu cầu suy luận tương phản trên các đầu vào đa phương thức trong một thiết lập sinh sản mở, đánh giá khả năng của mô hình trong việc dịch các đặc trưng của các phương thức khác nhau từ các biểu diễn nội tại của nó sang phân phối đầu ra sinh sản của nó.

3 Phương pháp

Tổng quan Khung làm việc: Hình 1 mô tả tổng quan về thiết lập của khung làm việc mở rộng tinh chỉnh hướng dẫn cho căn chỉnh hình ảnh [19,59] thành một số lượng tùy ý các phương thức thông qua tinh chỉnh độc lập các phép chiếu cụ thể cho phương thức tới một LLM đông lạnh, được phân tích chi tiết hơn trong Thuật toán 1. Khung căn chỉnh X-InstructBLIP bao gồm các bước sau: (1) Với mỗi phương thức, thu thập bộ dữ liệu điều chỉnh hướng dẫn (x, y) ∈ D_M sao cho x = (x_M, x_T) là một tuple của đầu vào phương thức và văn bản, và y là đầu ra văn bản mong đợi. (2) Cho Enc_M là bộ mã hóa phương thức đến R^{d_M} và Enc_T là ánh xạ từ văn bản đến không gian nhúng của LLM R^{d_L}. Tối ưu hóa một mô-đun phép chiếu riêng biệt f_M^θ: R^{d_M} → R^{k×d_L} cho mỗi phương thức M trên D_M trong khi duy trì các tham số của LLM đông lạnh, trong đó k là số lượng token đầu vào LLM tương ứng với đầu vào phi ngôn ngữ. Đối với dữ liệu tuần tự, như video và âm thanh, chúng tôi trích xuất N×k token truy vấn; mỗi khung hình được mã hóa và xử lý riêng biệt bởi mô-đun phép chiếu. (3) Mô hình được tối ưu hóa dưới mục tiêu mô hình hóa ngôn ngữ nhân quả [60]: min_θ L_{CE}^{LLM}(x_{LLM}), h(y) trong đó L_{CE} là hàm mất mát entropy chéo, θ là các tham số Q-Former, y là chuỗi mục tiêu, LLM(x_{LLM}) là dự đoán của LLM.

Phép chiếu X-Instruct: Hình 3a làm nổi bật tất cả các thành phần liên quan đến việc học các phép chiếu Q-Former nhận thức hướng dẫn [19] cho nhiều phương thức.

--- TRANG 6 ---
6 A. Panagopoulou et al.

Thuật toán 1 Khung Tối ưu hóa X-InstructBLIP
Yêu cầu: Tập hợp các phương thức M, mỗi phương thức liên kết với một tập dữ liệu D_M, và tập hợp các mẫu I={I_M^t: M∈M, t∈T} cho mỗi tác vụ t∈T
1: cho mỗi phương thức M trong M thực hiện
2: Khởi tạo bộ mã hóa cụ thể cho phương thức được huấn luyện trước Enc_M
3: Khởi tạo bộ mã hóa LLM Enc_T (token hóa và nhúng văn bản)
4: Khởi tạo phép chiếu f_M^θ: R^{d_M} → R^{k×d_L}
5: cho mỗi bước trong số lượng lần lặp thực hiện
6: Lấy mẫu (x, y) từ ∪D_M
7: Lấy mẫu i_M từ I_M^t trong đó t là ánh xạ tác vụ x tới y
8: z_M ← Enc_M(x) ▷ Mã hóa đầu vào đến không gian nhúng
9: w_M ← f_M^θ(z_M) ▷ Biến đổi đầu vào đã mã hóa đến không gian nhúng LLM
10: x_{LLM} ← w_M ∥ Enc_T(i_M) ∥ Enc_T(x_T)
11: Dự đoán ← LLM(x_{LLM}) ▷ Lấy dự đoán của LLM
12: Mất mát ← L_{CE}(Dự đoán, Enc_T(y)) ▷ Tính mất mát entropy chéo
13: θ ← θ - α∇_θ Mất mát ▷ Cập nhật tham số phép chiếu

Cho mã hóa phương thức M z_M = Enc_M(x_M) và hướng dẫn tác vụ i_M ∈ I_M^t, mô-đun Q-Former biến đổi một tập hợp k nhúng có thể học Q_M = {q_M^1...q_M^K} được gọi là các token truy vấn đầu vào thành các biểu diễn nhận thức hướng dẫn của Q'_M = QF_M(Q_M, z_M, i_M). Mô-đun Q-Former bao gồm hai mô-đun con transformer chia sẻ cùng các lớp tự chú ý: một mô-đun con tương tác với đầu ra của bộ mã hóa phương thức Enc_M và mô-đun con khác là transformer văn bản BERT cơ bản phục vụ như cả bộ mã hóa và bộ giải mã. Mỗi Q-Former được khởi tạo với các trọng số được huấn luyện trước từ BLIP-2 [50], không có các lớp chú ý chéo do sự không khớp kích thước giữa bộ mã hóa hình ảnh trong BLIP-2 và các bộ mã hóa phương thức khác. Nhúng phương thức z_M tương tác với văn bản hướng dẫn i_M và các token truy vấn đầu vào Q_M thông qua các lớp chú ý chéo được chèn vào mỗi khối transformer khác, tạo ra các token truy vấn đầu ra Q'_M sau đó được chiếu tuyến tính đến không gian của LLM đông lạnh thông qua lớp phép chiếu có thể học LP_M cụ thể cho mỗi phương thức. Cho pf_M là tiền tố phương thức, x là văn bản đầu vào ví dụ, và y là cụm từ mục tiêu. Với ∥ biểu thị phép nối, các token đầu vào LLM là: x_{LLM} = Enc_T(pf_M) ∥ LP_M(Q'_M) ∥ Enc_T(i_M) ∥ Enc_T(x_T)).

Phép chiếu Kiểu X-LLaVA: Chúng tôi thực hiện một sự thích ứng của kiến trúc LLaVA [59] để phục vụ nhiều phương thức, tương tự như Q-Former nhận thức hướng dẫn. Hình 3b mô tả kiến trúc của phép chiếu đơn giản này chuyển đổi tuyến tính các đầu ra của bộ mã hóa phương thức trực tiếp đến không gian nhúng đầu vào của LLM. Chính thức, mô hình bao gồm một lớp phép chiếu tuyến tính đơn LP_M: R^{d_M} → R^{k×d_{LLM}}, trong đó d_{LLM} là kích thước nhúng của LLM. Để so sánh hai loại phép chiếu, chúng tôi khớp số lượng tham số có thể huấn luyện cho mỗi phương thức, và duy trì thiết lập huấn luyện.

--- TRANG 7 ---
X-InstructBLIP 7

Chú thích
Hình ảnh 3D Âm thanh Video Held In Held Out Được tạo CC12M COCO CapFilt Visual Genome SBU Flickr30k NoCaps Cap3D MSRVTT MSVD AudioCaps WavCaps WebVid2m Hỏi đáp VGQ A VQA2 OK VQA OCR VQA GQA VizWiz Cap3D QA MSVD QA AudioCaps QA MSRVTT QA AOK VQA

LLaVa150k Phân loại ESC50 AudioSet Modelnet40 Suy luận Thực thể Đa phương thức Phân biệt VA T E X Đối thoại Video-Âm thanh Music AV Q A Hình ảnh-3D Clotho AQA Clotho

Hình 4: Tập dữ liệu Điều chỉnh Hướng dẫn và Đánh giá: Tập dữ liệu được bao quanh bởi hình oval và hình vuông lần lượt là tập dữ liệu điều chỉnh và đánh giá ngoài miền. Đường viền đứt nét được sử dụng cho các tập dữ liệu được tạo tự động như mô tả trong Phần 4.1.

4 Tập dữ liệu

X-InstructBLIP được tối ưu hóa và đánh giá trên một tập hợp các tập dữ liệu có sẵn trước và được tạo tự động được trình bày ngắn gọn trong Hình 4, được thảo luận trong Phần 4.1, với thêm chi tiết có sẵn trong tài liệu bổ sung. Phần 5.2 giới thiệu tập dữ liệu thách thức Suy luận Đa phương thức Phân biệt DisCRn⁴ được sử dụng để đánh giá khả năng nổi lên của X-InstructBLIP (Phần 4.2).

4.1 Tập dữ liệu Tinh chỉnh

Tập dữ liệu Hiện có: Hình 4 minh họa các tập dữ liệu được sử dụng cho cả điều chỉnh hướng dẫn và đánh giá. Một phân tích chi tiết về thống kê tập dữ liệu và định dạng có thể được tìm thấy trong tài liệu bổ sung. Đối với mỗi tập dữ liệu trong D_M, tập hợp các tập dữ liệu held-in cụ thể cho phương thức M, một chiến lược lấy mẫu được sửa đổi từ [19] được áp dụng để phù hợp với phạm vi rộng hơn của các phương thức. Xác suất lấy mẫu cho bất kỳ tập dữ liệu nào D_M^d ∈ D_M là √|D_M^d| / ∑_{d∈[1...|D_M|]} √|D_M^d|, với các điều chỉnh tối thiểu như được biện minh trong tài liệu bổ sung.

Tăng cường Dữ liệu Hướng dẫn: Việc trích xuất các biểu diễn nhận thức hướng dẫn đòi hỏi các tác vụ liên quan hướng dẫn đa dạng trên tất cả các phương thức. Đáng chú ý, các tập dữ liệu cho các phương thức 3D và âm thanh chủ yếu tập trung vào chú thích. Để giải quyết vấn đề này, chúng tôi tận dụng mô hình ngôn ngữ lớn mã nguồn mở google/flan-t5-xxl [97] để tự động tạo ra các cặp hỏi-đáp cho các phương thức 3D và âm thanh từ các chú thích tương ứng của chúng. Quá trình bắt đầu bằng việc nhắc mô hình với các chú thích để tạo ra các câu trả lời tiềm năng. Những câu trả lời này sau đó được sử dụng để nhắc mô hình tạo ra các câu hỏi ứng viên. Nếu phản hồi của mô hình đối với một câu hỏi, sử dụng chú thích làm bối cảnh phù hợp chặt chẽ với câu trả lời ban đầu, ví dụ được thêm vào tập dữ liệu của chúng tôi, tạo ra ~250k ví dụ 3D từ

⁴Thuật ngữ suy luận phân biệt, được thích ứng từ [105], đề cập đến khả năng phân biệt giữa các tập hợp đầu vào, trái ngược với suy luận kết hợp, việc tổng hợp thông tin từ các nguồn được căn chỉnh.

--- TRANG 8 ---
8 A. Panagopoulou et al.

H: Thực thể nào có mái nhà? Đ: thứ hai H: Cái nào đang di chuyển? Đ: phải

H: Thực thể nào được làm từ gốm sứ? Đ: trái

[Nhựa được gõ trong khi ai đó nói]

H: Thực thể nào có nhiều khả năng ở trong thành phố hơn? Đ: 1st [một người đàn ông nói và ong vo ve và chim hót]

Hình 5: DisCRn. Cho hai đầu vào phương thức khác biệt, chọn cái nào phù hợp với truy vấn.

Cap3D [64]⁵ và ~24k ví dụ âm thanh từ AudioCaps [43]. Chi tiết về quá trình tạo dữ liệu và phân phối được cung cấp trong phần bổ sung.

4.2 Suy luận Đa phương thức Phân biệt

X-InstructBLIP cung cấp một khả năng nổi lên đặc biệt: suy luận trên các phương thức khác nhau, mặc dù huấn luyện phương thức cá nhân. Điều này làm nổi bật tính linh hoạt và tiềm năng khả năng mở rộng của mô hình trên nhiều phương thức. Để nghiên cứu khả năng suy luận đa phương thức này, chúng tôi trình bày một tập dữ liệu thách thức Suy luận Đa phương thức Phân biệt (DisCRn). Như được hiển thị trong Hình 5, tác vụ yêu cầu mô hình phân biệt giữa các thuộc tính của hai thực thể trên các phương thức bằng cách chọn cái nào thỏa mãn một thuộc tính được truy vấn. Tác vụ này yêu cầu mô hình không chỉ phân biệt các đặc tính vốn có của các phương thức liên quan mà còn phải xem xét vị trí tương đối của chúng trong đầu vào. Sự áp đặt chiến lược này phục vụ để giảm sự phụ thuộc vào các heuristic khớp văn bản đơn giản, thiên vị thứ tự, hoặc các mối tương quan lừa dối tiềm năng giữa các phương thức.

Để tạo ra tập dữ liệu, chúng tôi nhắc google/flan-t5-xxl theo cách Chain-of-Thought [98] để tạo ra một tập hợp các thuộc tính cho mỗi thể hiện tập dữ liệu. Mỗi thể hiện sau đó được ghép nối với một thực thể ngẫu nhiên từ tập dữ liệu để tạo thành một bộ ba (câu hỏi, câu trả lời, giải thích) sử dụng ba ví dụ để tận dụng học trong bối cảnh [7]. Một bước quan trọng trong quá trình tạo này là kiểm tra tính nhất quán vòng tròn: một ví dụ chỉ được tích hợp vào tập dữ liệu cuối cùng khi dự đoán của mô hình về câu hỏi được tạo, cho các chú thích, thể hiện khoảng cách Levenshtein trên 0.9 so với câu trả lời ví dụ. Tập dữ liệu được tinh chỉnh này bao gồm 8.802 mẫu âm thanh-video có nguồn từ tập xác thực AudioCaps, và 29.072 thể hiện hình ảnh-point cloud từ một tập con được dành riêng 5k point cloud từ Cap3D [64]. Mỗi thể hiện trong tập dữ liệu được ghép nối với hai biểu diễn tương ứng với các chú thích: (âm thanh, video) từ AudioCaps và (point cloud, hình ảnh) từ Cap3D. Cho rằng sự sắp xếp của dữ liệu có thể được thay đổi, điều này cho phép duy trì một tập hợp câu trả lời cân bằng, không chỉ về mặt vị trí của các câu trả lời, mà còn về phương thức câu trả lời. Hiệu suất con người trên tác vụ đạt 90% cho thấy chất lượng cao của nó. Thêm chi tiết được tìm thấy trong tài liệu bổ sung.

⁵Một tập con 5k point cloud được giữ lại từ Cap3D để xây dựng DisCRn (Phần 4.2). Việc loại trừ này được duy trì cả trong chú thích và QA.

--- TRANG 9 ---
X-InstructBLIP 9

5 Thí nghiệm

Chúng tôi nghiên cứu hiệu quả của X-InstructBLIP như một giải pháp toàn diện để tích hợp đa phương thức vào các LLM đông lạnh được huấn luyện trước. Sau khi tóm tắt các chi tiết triển khai trong Phần 5.1, Phần 5.2 xác minh tính cạnh tranh của khung làm việc trong các tác vụ phương thức cá nhân-sang-văn bản, và khám phá khả năng suy luận đa phương thức nổi lên của nó ngay cả khi không có tối ưu hóa kết hợp.

5.1 Chi tiết Triển khai

X-InstructBLIP được xây dựng trên khung làm việc của thư viện LAVIS [48] trên nền tảng các mô hình Vicuna v1.1 7b và 13b [17]. Chúng tôi áp dụng EVA-CLIP-ViT-G/14 [24] làm bộ mã hóa cho hình ảnh và video, cho âm thanh BEATs iter3+ [13] và cho 3D ULIP-2 [PointBERT backbone] [78]. Trong thiết lập X-Instruct, mỗi Q-Former tối ưu hóa 188M tham số có thể huấn luyện và học K=32 token truy vấn với kích thước chiều ẩn 768 để chọn một mô hình tốt nhất cho mỗi phương thức. Các đầu vào thô trải qua tiền xử lý chuẩn hóa trước khi mã hóa. Tất cả Q-Former được khởi tạo trước với trọng số giai đoạn 1 BLIP-2 [19] ngoại trừ Q-Former video được khởi tạo từ lần lặp cuối cùng của Q-Former hình ảnh tương ứng. Chi tiết về tiền xử lý và siêu tham số huấn luyện cho mỗi phương thức được bao gồm trong phần bổ sung. Thiết lập phép chiếu kiểu X-LLaVA được khởi tạo đồng nhất và được điều chỉnh để khớp với số lượng tham số có thể huấn luyện trong X-Instruct.

Tất cả mô hình được tối ưu hóa trên 8 GPU A100 40GB sử dụng AdamW [62] với β1=0.9, β2=0.999, và suy giảm trọng số 0.05. Tốc độ học tăng tuyến tính trong 1.000 bước đầu tiên từ 10^-8 đến 10^-5, tiếp theo là suy giảm cosine đến tối thiểu 0. Siêu tham số đánh giá và mẫu nhất quán trên các tác vụ, được điều chỉnh tối thiểu cho từng phương thức như được chi tiết trong phần bổ sung.

5.2 Kết quả

Mục tiêu chính của chúng tôi là chứng minh khả năng thích ứng của khung làm việc của chúng tôi trên các phương thức khác nhau mà không dựa vào các giai đoạn huấn luyện trước quy mô lớn hoặc dữ liệu phương thức kết hợp. Tuy nhiên, để đảm bảo hiệu quả và khả năng so sánh của phương pháp của chúng tôi, chúng tôi so sánh hiệu suất của nó với các phương pháp khác sử dụng phép chiếu đến các LLM đông lạnh hoặc đông lạnh một phần, bất cứ khi nào có thể. Điều này phục vụ như một kiểm tra sự tỉnh táo, xác minh rằng phương pháp của chúng tôi vừa hiệu quả vừa cạnh tranh.

Hiểu biết Phương thức Cá nhân Chúng tôi đánh giá hiệu suất của khung làm việc trên một loạt các tác vụ từ một phương thức sang văn bản, minh họa tính linh hoạt và hiệu quả của nó trên tất cả bốn phương thức được khám phá. Bảng 1, 3, 4, và 2 tóm tắt hiệu suất ngoài miền của X-InstructBLIP trên 3D, âm thanh, hình ảnh, và video.

3D: Bảng 1 hiển thị kết quả về phân loại không có shot trên ModelNet40 [99] dưới hai thiết lập: phân loại trong từ vựng đóng sử dụng xếp hạng mất mát [52] và sinh sản mở trong đó mô hình được nhắc mô tả mô hình 3d và

--- TRANG 10 ---
10 A. Panagopoulou et al.

ULIP2 X-InstructBLIP LLaVA-style Projection máy bay ô tô ghế cửa tủ quần áo chậu hoa hộp thủy tinh guitar lò sưởi tủ đầu giường người piano cây radio máy hút mùi bồn rửa sofa bàn toilet bình hoa

(a) Biểu đồ TSne của các phép chiếu LLM cho 20 lớp Modelnet40 được lấy mẫu ngẫu nhiên.

hộp thư đèn piano mũ bảo hiểm máy in tai nghe guitar bàn Point Cloud đèn piano tai nghe Hình ảnh 0.13 0.21 0.18 0.12 0.14 0.16 0.13 0.19 0.12 0.16 0.20 0.09 0.15 0.13 0.11 0.19 0.12 0.22 0.17 0.17 0.15 0.27 0.15 0.18 0.10.2

(b) Bản đồ nhiệt của độ tương đồng cosine tương đối giữa đầu ra truy vấn hình ảnh và point cloud từ các mẫu trong Shapenet.

Hình 6: Phân tích về việc căn chỉnh các biểu diễn X-InstructBLIP.

tính đúng được xác thực nếu một lớp đơn từ 40 ứng viên có mặt trong việc sinh sản. Trong cả hai thiết lập phép chiếu, X-InstructBLIP vượt trội đáng kể so với đường cơ sở InstructBLIP, xử lý một kết xuất góc nhìn đơn của point cloud. Thú vị, thiết lập phép chiếu X-Instruct vượt trội không chỉ so với phép chiếu kiểu X-LLaVA mà còn so với PointLLM [32] học một phép chiếu tương tự nhưng - không giống như thiết lập này - sử dụng các đặc trưng RGB. Nó cũng vượt trội so với PointBindLLM [32] huấn luyện một adapter trên mã hóa ImageBind [27], và dựa vào không gian nhúng chung để tổng quát hóa đến point cloud, cho thấy tầm quan trọng của các bộ mã hóa phương thức cá nhân trong khung làm việc của chúng tôi. Điều này được tăng cường thêm bởi việc trực quan hóa TSNE [65] của các biểu diễn ULIP-2, X-Instruct và LLaVA-style Projection trong Hình 6a cho thấy rằng LLaVA-style Projection phá vỡ sự phân tách lớp dẫn đến hiệu suất thấp hơn 16.4 và 19.2 điểm trong độ chính xác phân loại và sinh sản mở so với X-Instruct Projection. Chúng tôi tiếp tục quan sát, trong Hình 6b, một hiệu ứng nhẹ của việc căn chỉnh tương đối giữa các lớp tương tự trên các phương thức vì độ tương đồng cosine của đầu ra truy vấn hình ảnh và point cloud của các lớp tương tự trong Shapenet cao hơn so với các lớp không tương tự.

Âm thanh: Bảng 3 cho thấy hiệu suất của X-InstructBLIP trong phân loại âm thanh, hỏi đáp, và chú thích trên ESC50 [76], ClothoAQA [57], và Clotho [22], tương ứng. Phân loại được đánh giá cả trong thiết lập đóng (cls) và sinh sản mở. Cả hai biến thể X-InstructBLIP đều vượt trội so với ImageBindLLM trong tất cả các tác vụ, có thể gợi ý rằng các bộ mã hóa riêng biệt và dữ liệu huấn luyện cụ thể cho âm thanh có lợi cho việc căn chỉnh âm thanh-văn bản. Đáng chú ý, X-LLaVA-style Projection vượt trội so với X-Instruct Projection trên Audio QA, trong khi kém hiệu suất trong tất cả các tác vụ khác. Điều này có thể do lượng dữ liệu Audio QA thấp làm cho các phép chiếu nhận thức hướng dẫn tạo ra một tập hợp nhỏ các phản hồi.

Hình ảnh: Trong khi không có biến đổi lớn nào về hiệu suất được mong đợi so với InstructBLIP [19], Bảng 4 trình bày kết quả về chú thích hình ảnh, hỏi đáp thị giác

--- TRANG 11 ---
X-InstructBLIP 11

đáp, MME [25], và MMVET [112] như một kiểm tra sự tỉnh táo. Trong khi X-InstructBLIP vượt trội so với InstructBLIP trên VizWiz [6], có sự sụt giảm nhẹ về hiệu suất tổng thể, có thể do thiếu tinh chỉnh giai đoạn 2 của BLIP2, và không gian mẫu mở rộng gây ra một sự đánh đổi giữa tổng quát hóa và hiệu suất như được hiển thị bởi sự mạnh mẽ của prompt tăng lên của X-InstructBLIP trong phần bổ sung.

Video Im lặng: Bảng 2 đánh giá X-InstructBLIP trên các tác vụ video ngoài miền. Chúng tôi so sánh hiệu suất với các đường cơ sở nổi bật dựa vào các LLM đông lạnh hoặc đông lạnh một phần và cho thấy hiệu suất tương đương hoặc cải thiện trên Video Question Answering (VQA). Tuy nhiên, do bản chất của việc nhận thức hướng dẫn

[THIS IS TABLE: Multiple tables showing performance comparisons across different models and tasks, including:
- Table 1: Zero-shot 3D classification on Modelnet40
- Table 2: Out-Domain Silent Video Results
- Table 3: Out-Domain Audio Quantitative Results  
- Table 4: Out-Domain Image Quantitative Results]

Kết quả Định lượng Phương thức Đơn. Số được gạch chân biểu thị đánh giá trong miền. In đậm biểu thị hiệu suất zero-shot hàng đầu. Màu xanh biểu thị tốt thứ hai zero-shot. Màu tím biểu thị đánh giá được thực hiện độc lập. Các mô hình được ký hiệu bằng 7b và 13b biểu thị kích thước LLM cơ bản. Các hàng tô màu xám tương ứng với các biến thể X-InstructBLIP, và màu vàng với mô hình tương đương kiểu LLaVA [59]. Điểm CIDEr [89] được báo cáo cho chú thích, kèm theo điểm SPIDEr [61] cho chú thích âm thanh và độ chính xác Top-1 cho QA và các tác vụ phân loại. †biểu thị một metric khớp chính xác nới lỏng trong đó sự thật cơ bản là một chuỗi con của dự đoán.

--- TRANG 12 ---
12 A. Panagopoulou et al.

thiết lập, X-InstructBLIP được điều chỉnh trên các tác vụ QA khác, do đó có lợi thế so với VideoLLaMA [113] và FrozenBiLM [107] mặc dù thiếu huấn luyện trước video (PT). Như chúng tôi cho thấy trong phần bổ sung, thành phần Video Q-Former của X-InstructBLIP, được khởi tạo với trọng số của Image Q-Former, đạt được sự hội tụ hiệu suất một cách đáng kể nhanh chóng, trong khoảng 1.000 lần lặp. Để có bối cảnh, VideoLLaMA được huấn luyện trước trên toàn bộ tập dữ liệu WebVideo 2 triệu video, ngoài việc huấn luyện trước hình ảnh BLIP-2. Mặt khác, FrozenBiLM trải qua hai epoch huấn luyện trên WebVideo10M lớn hơn.

Suy luận Kết hợp Đa phương thức: Mặc dù mỗi phép chiếu phương thức được huấn luyện riêng lẻ, X-InstructBLIP cho thấy suy luận phương thức kết hợp mạnh mẽ, đặc biệt dưới thiết lập X-Instruct Projection. Bảng 5 chứng minh khả năng của X-Instruct trong việc suy luận kết hợp trên video (V) và âm thanh (A). Đáng chú ý, X-Instruct Proj. (7b) có khả năng tăng cường đầu vào, hiển thị sự cải thiện về hiệu suất so với việc sử dụng một phương thức đơn khi mô hình được gợi ý với các phương thức khác nhau cả trong MusicAVQA [49] và VATEX [95]. Tuy nhiên, điều này không đúng với X-LLaVA-style Projection thể hiện hiệu suất bằng nhau hoặc thấp hơn dưới thiết lập đa phương thức như vậy.

Suy luận Phân biệt Đa phương thức Chúng tôi đánh giá X-InstructBLIP trong việc thực hiện suy luận phân biệt trên các phương thức khác nhau sử dụng bộ đánh giá DisCRn mới được giới thiệu của chúng tôi, được chi tiết trong Phần 4.2. Chúng tôi đóng khung vấn đề như một vấn đề sinh sản mở thực tế. LLM được đặt tiền tố với hướng dẫn: Bạn được cho hai đầu vào. Chọn chính xác một trong hai bằng cách tham chiếu đến vị trí tương đối của nó (đầu tiên hoặc thứ hai, trái hoặc phải) mà trả lời tốt nhất cho câu hỏi.

[THIS IS TABLE: Table 5 showing Emergent Joint (A)udio-(V)ideo Reasoning with columns for Music AVQA test and VATEX test]

[THIS IS TABLE: Table 6 showing DisCRn evaluation results]

Trong việc nhắc X-Instruct Proj. (7b), chúng tôi thấy rằng việc sử dụng một prompt chú thích Q-Former khác với prompt so sánh được cung cấp cho mô hình LLM tạo ra một biểu diễn tổng quát hơn có thể áp dụng hơn cho tác vụ so sánh, do đó chúng tôi sử dụng phương pháp này cho các kết quả trong Bảng 6. Điều này có thể do thiếu dữ liệu so sánh trong tinh chỉnh vì mỗi Q-Former phương thức được huấn luyện riêng biệt. Công việc tương lai có thể khám phá hiệu ứng của các prompt khác nhau được điều kiện hóa trên các tham số khác nhau trong thiết lập huấn luyện nhận thức hướng dẫn (ví dụ: dữ liệu, mẫu, huấn luyện kết hợp, và tối ưu hóa LLM một phần hoặc toàn bộ).

--- TRANG 13 ---
X-InstructBLIP 13

Đối với so sánh video-âm thanh, chúng tôi chọn hai khung hình cho mỗi phương thức để cho phép ảnh hưởng sinh sản cân bằng hơn.

Để đánh giá khả năng của mô hình, chúng tôi tích hợp một đường cơ sở chú thích mạnh mẽ bằng cách thay thế các đầu ra truy vấn bằng các chú thích tương ứng với các phương thức sử dụng mô hình Vicuna 7b. Đối với hình ảnh, 3D, và các phương thức video, chúng tôi kích thích các chú thích bằng cách nhắc InstructBLIP [19] Mô tả hình ảnh/video. Đối với 3D, một góc nhìn kết xuất được chọn ngẫu nhiên của point cloud được cung cấp cho InstructBLIP. Đối với video, chúng tôi theo [19] và lấy mẫu bốn khung hình và nối các biểu diễn đầu ra của chúng làm đầu vào cho mô hình. Đối với âm thanh, chúng tôi sử dụng WavCaps [100].

Trong khi khung làm việc X-InstructBLIP tạo ra các mô hình vượt trội so với đường cơ sở chú thích mạnh mẽ với một biên độ đáng kể, không có nhận xét kết luận nào về loại phép chiếu nào trong hai loại phù hợp hơn cho suy luận phân biệt đa phương thức6. X-LLaVA Proj. vượt trội so với X-Instruct Proj. trên Audio-Video, có thể do hiệu suất Audio QA mạnh hơn của nó cũng được báo cáo trong Bảng 3. Đối với image-3D, điều ngược lại là đúng, biểu thị kết quả trực giác rằng hiệu suất phương thức cá nhân đóng vai trò trong khả năng suy luận đa phương thức. Tuy nhiên, đáng chú ý rằng X-Instruct Proj. thể hiện khả năng chuyển đổi từ suy luận phân biệt sang kết hợp, bằng cách phân biệt hoặc kết hợp các đầu vào để tạo ra phản hồi. Như được thấy trong Bảng 5, điều này không đúng với các phép chiếu kiểu X-LLaVA, gợi ý rằng các biểu diễn nhận thức hướng dẫn có thể chuẩn bị LLM để phản hồi phù hợp hơn với tác vụ đang được đặt ra.

5.3 Ablation

Hiệu ứng Tiền tố: Chúng tôi khám phá hiệu ứng của việc đặt tiền tố đầu vào phương thức với một tiền tố cụ thể cho phương thức trong Bảng 7. Chúng tôi so sánh hiệu suất của X-Instruct Proj. (7b) với X-Instruct Proj. no-prefix được huấn luyện tương tự như X-Instruct Proj. (7b), với sự khác biệt là loại phương thức không được đặt trước các token đầu vào LLM của phương thức trước khi đưa vào LLM cho huấn luyện và suy luận. Trong cả hai tác vụ phương thức đơn âm thanh và 3D, việc loại bỏ tiền tố liên tục làm tổn hại hiệu suất. Sự cải thiện này có thể do thực tế là Q-Former được giải tỏa khỏi gánh nặng bổ sung để mã hóa loại phương thức và thay vào đó dành băng thông cho thông tin ngữ nghĩa. Bao gồm tiền tố cũng cho phép mô hình học kết hợp các phương thức tốt hơn như được hiển thị bởi hiệu suất cải thiện so với phương thức đơn cho MusicAVQA và VATEX. Ban đầu, người ta cho rằng ketunangan của mô hình trong việc phân biệt các token tương ứng với mỗi phương thức, thay vào đó coi chúng như một luồng liên tục, có thể là nguyên nhân của hành vi này. Tuy nhiên, kết quả từ tác vụ suy luận đa phương thức image-3D trong đó mô hình không có tiền tố vượt trội so với mô hình có tiền tố 10 điểm thách thức quan điểm này. Có vẻ như việc bao gồm các gợi ý có thể nhắc mô hình mã hóa thông tin cụ thể cho phương thức, có lợi trong các tình huống suy luận kết hợp. Việc mã hóa chuyên biệt này tuy nhiên không chuẩn bị mô hình

6Đáng chú ý rằng sử dụng một mẫu con nhỏ của dữ liệu, chúng tôi quan sát thấy rằng tác vụ nhạy cảm với prompt, chủ yếu trong thiết lập chỉ ngôn ngữ. Chúng tôi để lại cho công việc tương lai việc đánh giá có hệ thống khả năng của mô hình trên tác vụ dựa trên các prompt khác nhau và ví dụ trong bối cảnh.

--- TRANG 14 ---
14 A. Panagopoulou et al.

Phương thức Tác vụ X-Instruct Proj. X-Instruct Proj. no-prefix
3D Modelnet40 close 62.8 60.9
   Modelnet40 open 49.4 46.7
Âm thanh ESC50 close 75.9 67.5
   ESC50 open 38.2 36.0
   ClothoAQA 15.4 9.9
   Clotho v1/v2 29.4/26.7 26.9/24.5
Âm thanh+Video MusicAVQA (A/V/A+V/∆) 13.4/27.2/28.1/1.3 8.9/27.3/22.3/-5.0
   VATEX (A/V/A+V/∆) 6.7/59.3/60.9/1.7 6.8/59.5/58.3/-1.2
   DisCRn 34.0 31.4
Hình ảnh+3D DisCRn 48.1 57.7

Bảng 7: Ablation: Hiệu ứng Tiền tố

ESC50 close ESC50 open ClothoAQA Clotho v1 Clotho v2
X-Instruct Proj. (7b) 75.9 38.2 15.4 29.4 26.7
X-Instruct Proj. (7b) no-init 70.0 37.8 11.9 29.3 27.4

Bảng 8: Kết quả Định lượng Âm thanh Ngoài miền.

để nhận ra và xử lý các đặc tính thường liên quan đến các phương thức khác, cần thiết cho hiệu suất tăng cường trong các tác vụ tương phản. Lý luận cơ bản là mô hình ngôn ngữ, đã được điều chỉnh để tạo ra đầu ra liên quan đến phương thức, dẫn Q-Former chủ yếu nhận phản hồi về sinh sản cụ thể cho phương thức trong quá trình huấn luyện, cũng giải thích cho những cải thiện trong phương thức đơn.

Khởi tạo BLIP-2 Chúng tôi cũng khám phá hiệu quả của việc khởi tạo BLIP-2 bằng cách huấn luyện Q-Former âm thanh trong X-Instruct Proj. (7b) sử dụng phương pháp khởi tạo ngẫu nhiên được ký hiệu là X-Instruct Proj. (7b) no-init. Bảng 8 chứng minh lợi ích của tiền nhiệm này, cho thấy rằng có thể tích hợp các phương thức mới vào khung làm việc của chúng tôi mà không cần huấn luyện trước rộng rãi, vì từ các phương thức được xem xét, âm thanh ít có khả năng hưởng lợi từ huấn luyện trước hình ảnh-văn bản nhất. Sự cải thiện đáng kể nhất được quan sát trong hỏi đáp, cho thấy việc khởi tạo trọng số BLIP-2 có vẻ như tăng cường nhận thức hướng dẫn hơn là căn chỉnh âm thanh-ngôn ngữ trực tiếp, được xác nhận bởi khoảng cách trong hiệu suất phân loại từ vựng đóng.

6 Kết luận

Nghiên cứu này giới thiệu X-InstructBLIP, một khung làm việc có thể mở rộng để căn chỉnh độc lập biểu diễn của nhiều phương thức với LLM đông lạnh chứng minh kết quả cạnh tranh so với các phương pháp hàng đầu trên tất cả các phương thức được giải quyết. Khung làm việc thể hiện suy luận đa phương thức nổi lên, mặc dù huấn luyện phương thức riêng biệt. Để kiểm tra khả năng nổi lên này, một tác vụ suy luận phân biệt đa phương thức mới DisCRn được giới thiệu để cho thấy rằng khung làm việc tạo ra các mô hình có thể vượt trội so với các đường cơ sở chú thích mạnh mẽ trên tất cả bốn phương thức được kiểm tra. Mặc dù hiệu quả của phương pháp, tác vụ vẫn là một thách thức mở. Chúng tôi cũng tìm thấy sự phức tạp và các câu hỏi chưa được trả lời trong mỗi phương thức, mở đường cho các khám phá tương lai trên và trong các phương thức.

--- TRANG 15 ---
Tài liệu Bổ sung cho X-InstructBLIP: Một Khung Làm Việc để Căn Chỉnh Hình Ảnh, 3D, Âm Thanh, Video với LLMs và Khả Năng Suy Luận Đa Phương Thức Nổi Lên

Artemis Panagopoulou1⋆, Le Xue2,∗∗, Ning Yu2,∗∗, Junnan Li2, Dongxu Li2, Shafiq Joty2, Ran Xu2, Silvio Savarese2, Caiming Xiong2, và Juan Carlos Niebles2

1Đại học Pennsylvania
artemisp@seas.upenn.edu
2Salesforce AI Research

1 Tạo Dữ liệu

1.1 Tăng cường Dữ liệu Điều chỉnh Hướng dẫn

Đối với các phương thức âm thanh và 3D, phạm vi các tác vụ có sẵn cho việc điều chỉnh hướng dẫn tương đối hạn chế. Để giải quyết thách thức này, chúng tôi theo một mô hình phổ biến trong tài liệu [101] và trích xuất các cặp hỏi-đáp từ các tập dữ liệu chú thích, cụ thể từ các chú thích bao gồm 10 từ trở lên. Hình 1 vạch ra quy trình để tự động tạo dữ liệu hỏi đáp từ các tập dữ liệu chú thích. Mô hình google/flan-t5-xxl từ huggingface-transformers được sử dụng, và được nhắc tạo ra các câu trả lời ứng viên một từ dựa trên chú thích. Sau đó, mô hình được giao nhiệm vụ tạo ra một câu hỏi liên quan sử dụng câu trả lời và bối cảnh làm đầu vào. Phương pháp tính nhất quán vòng tròn [75] được sử dụng để chỉ giữ lại những cặp hỏi-đáp phù hợp với bối cảnh. Sự phù hợp này được xác minh bằng cách đảm bảo rằng độ tương đồng một phần Levenshtein giữa câu trả lời được dự đoán và ban đầu lớn hơn 0.90, được tính bằng gói Python Fuzzy Wuzzy. Sau đó, chúng tôi áp dụng hậu xử lý khớp chuỗi để lọc ra các thể hiện không tuân thủ định dạng được quy định. Kết quả, 250.070/1.157 ví dụ huấn luyện/xác thực phù hợp được rút ra từ 661.576/5.000 mẫu chú thích 3D ban đầu từ Cap3D [64], và 24.156/1.653 ví dụ huấn luyện/xác thực được rút ra từ 38.695/1.900 mẫu chú thích âm thanh ban đầu từ AudioCaps [43]. Hơn nữa, đối với dữ liệu 3D, điều quan trọng là đảm bảo rằng các cặp hỏi-đáp không ám chỉ đến màu sắc. Điều này do thực tế là bộ mã hóa 3D không nắm bắt được các đặc tính màu sắc. Để đạt được điều này, mô hình ngôn ngữ được hướng dẫn để diễn đạt lại các chú thích bằng cách bỏ qua bất kỳ tham chiếu màu sắc nào, được nhắc là: Viết lại câu {caption} bằng cách loại bỏ bất kỳ đề cập màu sắc nào, trước khi thực hiện ⋆Công việc được thực hiện trong thời gian thực tập tại Salesforce Research ⋆⋆Đóng góp cố vấn bằng nhau.

--- TRANG 16 ---
16 A. Panagopoulou et al.

Vòng 1 Đầu vào: Tạo một từ trả lời tiềm năng từ văn bản sau: {caption} Mô hình Ngôn ngữ Đầu ra: {answer} Vòng 2 Đầu vào: Tạo một câu hỏi cho câu trả lời sử dụng bối cảnh. Bối cảnh: {caption} Câu trả lời: {answer} Câu hỏi: Mô hình Ngôn ngữ Đầu ra: {question} Vòng 3 Đầu vào: Trả lời câu hỏi được cho bối cảnh. Bối cảnh: {caption} Câu hỏi: {question} Câu trả lời: Mô hình Ngôn ngữ Đầu ra: {prediction}

Hình 1: Nhắc Tính nhất quán Vòng tròn cho Tập dữ liệu QA trong 3D và Âm thanh.

kiểm tra tính nhất quán vòng tròn. Một đánh giá con người ngắn trên 50 mẫu cho mỗi phương thức cho thấy 90% dữ liệu âm thanh được tạo và 82% dữ liệu 3D là chính xác. Bảng 1 trình bày một mẫu ngẫu nhiên của dữ liệu được tạo và bảng 2 cung cấp tổng quan về thống kê phân phối của tập dữ liệu. Đáng chú ý là các trường hợp lỗi thường do các câu hỏi vô nghĩa, hơn là câu trả lời sai. Ví dụ, các cặp sau được đánh dấu là vô nghĩa: Máy may đang chạy ở tốc độ nào? tốc độ, Còi hơi nước làm gì? rít, Mô hình 3D của bức tường gạch với lỗ và khối lập phương xếp chồng, giống như? phần tử, và Chiếc mũ có gì? hoa văn.

1.2 Tạo Dữ liệu Suy luận Phân biệt Đa phương thức

Để đánh giá khả năng suy luận đa phương thức của X-InstructBLIP, chúng tôi đã thiết kế một tác vụ độc đáo tái sử dụng các tập dữ liệu chú thích hiện có, đặc biệt tập trung vào dữ liệu có thể biểu diễn trong nhiều phương thức. Chúng tôi chọn tập dữ liệu xác thực AudioCaps [43] và dành riêng một tập con 5k ví dụ từ Cap3D [64] làm tập dữ liệu xác thực của chúng tôi, đảm bảo rằng phép chiếu 3D không được tiếp xúc với tập con này trong giai đoạn huấn luyện trong cả thiết lập chú thích hoặc 3DQA.

Dữ liệu âm thanh từ AudioCaps có nguồn gốc từ video Youtube, cho phép chúng tôi tải xuống các tệp video tương ứng sử dụng ID Youtube của chúng. Đối với Cap3D, chúng tôi sử dụng các point cloud liên quan và chọn ngẫu nhiên một hình ảnh được kết xuất từ tám góc nhìn có sẵn.

Mô tả về quy trình tạo dữ liệu, cũng được nêu trong văn bản chính, được cung cấp trong Hình 2. Trong quá trình đánh giá, chúng tôi duy trì sự cân bằng, đảm bảo

--- TRANG 17 ---
X-InstructBLIP 17

Chú thích Câu hỏi Câu trả lời Âm thanh Một phụ nữ nói trong khi gõ bàn phím; Phụ nữ đang gõ gì? Bàn phím Một người đàn ông đang nói trong khi nhiều con chó đang sủa xung quanh họ; Con chó đang làm gì? Sủa Một người đàn ông nói và đám đông vỗ tay, anh ta tiếp tục nói; Đám đông làm gì sau khi người đàn ông nói? Vỗ tay Một máy bay bay ở xa trong khi một người đàn ông nói và kim loại kêu leng keng. Kim loại làm gì? Kêu leng keng 3D Một mô hình 3D của ghế gỗ và ghế đẩu với một xô xiềng trên đó Cái gì trên ghế đẩu? Xô Một mô hình 3D của hòn đá phủ rêu, giống như lá, bản đồ giấy, và đá Cái gì phủ trên hòn đá? Rêu Một quả bóng bay với dây buộc, có gấu teddy và mặt mèo trên đó Vật thể nào có dây buộc? Quả bóng bay Một mô hình 3D của các món ăn khác nhau, bao gồm một con hàu, một miếng trái cây, và các hình thức trứng khác nhau. Món ăn nào là loài giáp xác? hàu

Bảng 1: Ví dụ QA Được Tạo Tự động từ Dữ liệu Chú thích.

Tập dữ liệu AudioCapsQA Cap3DQA train val train val Kích thước 24.156 1.274 250.070 1.157 Câu hỏi Riêng biệt 10.010 810 67.001 953 Câu trả lời Riêng biệt 1.636 374 4.555 451 Độ dài Câu hỏi Trung bình (từ) 6.0 6.1 6.8 7.0 Kích thước Từ vựng 2.951 723 12.771 1.022

Bảng 2: Thống kê Tập dữ liệu QA Được Tạo

mỗi tùy chọn (A hoặc B) phục vụ như sự thật cơ bản 50% thời gian. Cho rằng vấn đề này được cấu trúc như một tác vụ sinh sản từ vựng mở, chúng tôi mở rộng không gian câu trả lời sự thật cơ bản để bao gồm các từ đồng nghĩa và biểu thức tương đương, như [{answer modality}, left, 1st, 1, first, input 1, entity 1, object 1, input A, entity A, object A] và [{answer modality}, right, 2nd, second, input 2, entity 2, object 2, input B, entity B, object B], tương ứng với việc liệu đầu vào thứ nhất hay thứ hai là sự thật cơ bản. Hiệu suất con người trên một mẫu con 100 ví dụ của tập dữ liệu là 90%. Bảng 3 cung cấp tổng quan về thống kê phân phối của tập dữ liệu.

Tập dữ liệu Audio-Video Video-3D Tổng kích thước 8.802 28.173 Số lượng Câu hỏi Riêng biệt 1.212 3.100 Độ dài Câu hỏi Trung bình 5.8 từ 6.6 từ Kích thước Từ vựng Câu hỏi 701 từ 1.272 từ

Bảng 3: Thống kê Tập dữ liệu Suy luận Đa phương thức Phân biệt DisCRn

--- TRANG 18 ---
18 A. Panagopoulou et al.

Vòng 1. Chain of Thought Đầu vào: Ba thuộc tính để mô tả một đối tượng với mô tả là gì: {caption} Danh sách thuộc tính: Mô hình Ngôn ngữ Đầu ra: {properties} Vòng 2. Ví dụ Few Shot Đầu vào: Cho thực thể A với chú thích {caption} và các thuộc tính tương ứng: {properties}, và thực thể B với chú thích {caption} với các thuộc tính {properties} bạn có thể tạo ra một tập hợp các cặp câu trả lời hướng dẫn để so sánh và tương phản các thực thể như sau: Ví dụ: Câu hỏi. {} Câu trả lời: {} Giải thích {} ... (x3) Tạo ra ba bộ ba Câu hỏi, Câu trả lời, Giải thích như vậy cho Thực thể A với chú thích {caption} và các thuộc tính {properties} và Thực thể B với chú thích {caption} và các thuộc tính {properties}. Mô hình Ngôn ngữ Đầu ra: {question} Vòng 3: Tính nhất quán Vòng tròn Đầu vào: Cho thực thể A với chú thích {caption} và các thuộc tính tương ứng: {properties}, và thực thể B với chú thích {caption} với các thuộc tính {properties} Trả lời câu hỏi {question}. Câu trả lời: Mô hình Ngôn ngữ Đầu ra: {prediction}

Hình 2: Khung Tạo Tập dữ liệu Suy luận Đa phương thức Phân biệt

2 Tinh chỉnh Video Q-Former So với Khởi tạo Hình ảnh

Để khám phá tác động của việc huấn luyện thêm Image Q-Former trên dữ liệu video, Bảng 4 trình bày kết quả đánh giá các tác vụ video sử dụng trọng số từ Image Q-Former. Rõ ràng rằng việc huấn luyện trên dữ liệu video tăng cường hiệu suất. Tuy nhiên, đáng chú ý là Video Q-Former đạt được sự hội tụ ở giai đoạn sớm hơn (15k và 5k lần lặp cho Vicuna7b và Vicuna13b, tương ứng). Điều này có thể là do Q-Former đã đạt được hiểu biết ngữ nghĩa trong giai đoạn căn chỉnh hình ảnh, chỉ cần huấn luyện bổ sung tối thiểu để nắm bắt các sắc thái của phép chiếu video tuần tự. Sự sụt giảm hiệu suất cao hơn trong chú thích MSVD [10] so với VATEX [95] có thể do sự tương đồng gần gũi hơn giữa MSVD và phân phối tập dữ liệu MSRVTT [103] held-in. Có sự sụt giảm hiệu suất thấp hơn đáng kể đối với các tác vụ Video QA, do bản chất bị ràng buộc hơn của tác vụ - huấn luyện trên video sẽ không tăng đáng kể hiệu suất vì câu trả lời thường bị ràng buộc trong một khung hình [8], và như vậy xử lý khung hình đó sẽ gần như tương đương với xử lý nó trong hình ảnh. Sự cải thiện có thể bắt nguồn từ việc xác định câu trả lời trên một chuỗi dài hơn của các token truy vấn.

--- TRANG 19 ---
X-InstructBLIP 19

3 Đánh giá Trong miền

Bảng 6 trình bày hiệu suất trong miền cho một mẫu các tập dữ liệu được thấy trong huấn luyện trên tất cả bốn phương thức. Quan trọng là phải làm rõ rằng khi chúng tôi đề cập đến 'trong miền', chúng tôi đang cụ thể đề cập đến các tập dữ liệu được lấy mẫu trong quá trình huấn luyện. Tuy nhiên, quan trọng là phải lưu ý rằng điều này không tạo thành tinh chỉnh rõ ràng, vì không có đảm bảo rằng Q-Former đã gặp toàn bộ tập dữ liệu trong quá trình huấn luyện của nó.

4 Tính mạnh mẽ của Prompt

Bảng 5 so sánh hiệu suất giữa InstructBLIP (7b) và X-Instruct Proj. (7b) trên NoCaps [1], sử dụng các prompt không gặp trong quá trình tối ưu hóa của cả hai mô hình. Trong khi X-InstructBLIP thể hiện một số biến đổi hiệu suất, nó duy trì hơn một nửa độ lệch chuẩn của InstructBLIP. Sự biến đổi này có thể được quy cho từ vựng mở rộng trong các mẫu của chúng tôi, cho phép Q-Former liên kết tốt hơn một hướng dẫn với một tác vụ cụ thể. Ví dụ, trong trường hợp prompt P2: "Cung cấp một bản tóm tắt về những gì đang xảy ra trong bức ảnh", InstructBLIP duy trì hiệu suất cao vì nó gần giống với prompt trong miền "Sử dụng một vài từ để minh họa những gì đang xảy ra trong bức ảnh". Lưu ý rằng sự sụt giảm hiệu suất trong InstructBLIP chủ yếu được quy cho việc mô hình ngôn ngữ dùng đến việc tạo ra các mô tả dài hơn khi đầu ra Q-Former không nắm bắt được tác vụ, dẫn đến ảo giác trong các giai đoạn sau của việc sinh sản.

5 Chi tiết Huấn luyện

Trước khi mã hóa, các đầu vào thô trải qua tiền xử lý chuẩn hóa: hình ảnh được thay đổi kích thước thành độ phân giải 224×224 với cắt ngẫu nhiên và chuẩn hóa; các tệp âm thanh trải qua chuyển đổi mono và tiền xử lý bộ lọc ngân hàng theo sau là chuẩn hóa như trong [13] trên hai khung hình 5 giây; video được lấy mẫu đồng nhất thành 5 khung hình chịu cùng tiền xử lý như hình ảnh, và các point cloud 3D được lấy mẫu đồng nhất và chuẩn hóa thành 8k điểm như trong [78,106]. Tất cả Q-Former phương thức được khởi tạo trước với trọng số giai đoạn 1 BLIP-2 [19] ngoại trừ video

MSVD VATEX MSVD QA test val test X-LLaVA Style Proj. (7b) 105.3 46.2 49.8 X-LLaVA Style Proj. (7b) [image] 16.4 10.7 23.2 X-Instruct Proj. (7b) 116.1 59.2 51.7 X-Instruct Proj. (7b) [image] 42.4(↓73.7) 28.1(↓30.1) 39.7(↓12.0) X-Instruct Proj. no-prefix(7b)[image] 62.0(↓56.7) 52.6(↓6.9) 38.8(↓11.7) X-Instruct Proj. (13b) 124.3 52.0 49.2 X-Instruct Proj. (13b) [image] 78.7(↓45.6) 53.5(↑1.5) 36.0(↓13.2)

Bảng 4: Tác động của Huấn luyện Image Q-Former trên Video. Các mô hình được gắn nhãn [image] sử dụng Image Q-Former cho việc căn chỉnh video.

--- TRANG 20 ---
20 A. Panagopoulou et al.

InstructBLIP (7b) X-Instruct Proj. (7b) P1 1.0 88.0 P2 121.9 109.7 P3 0.9 54.9 P4 5.4 112.7 P5 0.8 111.5 Trung bình 26.3 83.0 Độ lệch chuẩn 43.8 20.8

P1 Trong vài từ, mô tả các đặc điểm cơ bản của hình ảnh này. P2 Cung cấp một bản tóm tắt về những gì đang xảy ra trong bức ảnh. P3 Tôi muốn nghe cách diễn giải của bạn về hình ảnh này. Bạn nhìn thấy gì? P4 Cung cấp một bản chụp nhanh bằng lời về những gì đang xảy ra trong hình ảnh này. P5 Vui lòng trình bày các yếu tố và bối cảnh của hình ảnh này

Bảng 5: Tính mạnh mẽ đối với các prompt chưa thấy trên NoCaps (val) [1].

Q-Former được khởi tạo từ lần lặp cuối cùng của Image Q-Former tương ứng và được tối ưu hóa trong 15k/5k bước cho các mô hình Vicuna 7b và 13b tương ứng.

Bảng 7 biên soạn các siêu tham số huấn luyện được sử dụng cho mỗi phương thức và mô hình. Biến thể X-InstructProj. no-prefix được huấn luyện tương tự như X-Instruct Proj., với sự khác biệt đáng chú ý là loại phương thức không được đặt trước các đầu ra truy vấn của phương thức, cả trong huấn luyện và suy luận. Theo [19] nhận thấy rằng tỷ lệ lấy mẫu đóng vai trò quan trọng trong huấn luyện, chúng tôi thực hiện một số sửa đổi nhỏ trong tỷ lệ lấy mẫu mà chúng tôi cho thấy trong bảng 9 và 8 là hiệu quả trong việc cải thiện hiệu suất. Các quyết định được thảo luận thêm dưới đây. Đáng chú ý là do số lượng lớn các thí nghiệm bao gồm tất cả các phương thức, chúng tôi không cạn kiệt tất cả các khả năng, và có thể có các cấu hình huấn luyện tốt hơn. Chúng tôi để lại điều này cho công việc tương lai để khám phá.

Vì mỗi phương thức thể hiện các đặc tính độc đáo, chúng tôi đã tùy chỉnh phương pháp huấn luyện cho từng phương thức. Ví dụ, các phép chiếu 3D và Âm thanh được huấn luyện cho số lượng lần lặp tối đa được quy định trong Bảng 7.

Phép chiếu Hình ảnh Vicuna7b trải qua huấn luyện trong 735k lần lặp, sử dụng lấy mẫu dữ liệu được chuẩn hóa. Ngoài ra, thêm 40k lần lặp được thực hiện với tỷ lệ lấy mẫu của COCO Captions [9] được đặt thành 3.0 trong khi giữ các tỷ lệ khác nhất quán với lấy mẫu gốc. Điều chỉnh này tận dụng các chú thích sạch của COCO Captions, giảm thiểu nhiễu được giới thiệu bởi

Hình ảnh 3D Video Âm thanh OKVQA COCO Cap3D MSRVTT MSRVTT QA AudioCaps test val test val qa-val val test val test val test qa-val Finetuned SOTA 66.1 - 155.1 - - - 80.3 - 48.0 - 78.1 - [21] [47] [102] [102] [14] InstructBLIP (T5xl) 48.6 137.7 140.2 - - 44.1 44.0 25.0 22.3 - - - InstructBLIP (T5xxl) 47.8 139.1 140.8 - - 41.5 47.8 25.6 21.4 - - - InstructBLIP (7b) 57.3 141.0 142.3 - - 28.1 31.1 22.1 18.7 - - - InstructBLIP (13b) 56.3 139.1 141.0 - - 36.7 37.1 24.8 20.2 - - - X-LLaVA Style Proj. (7b) 28.5 126.0 118.1 126.7 39.9 55.5 53.1 41.0 41.4 44.3 46.1 53.2 X-Instruct Proj. (7b) 52.5 137.7 138.2 142.1 53.6 61.0 57.6 44.6 42.1 44.6 67.9 41.2 X-Instruct Proj. (13b) 51.9 128.2 128.7 148.8 54.9 57.7 52.2 36.4 36.1 54.2 53.7 37.4

Bảng 6: Hiệu suất Trong miền trên các phương thức.

--- TRANG 21 ---
X-InstructBLIP 21

các tập dữ liệu hình ảnh lớn hơn. Tuy nhiên, kỹ thuật upsampling này không được áp dụng cho Q-Former Hình ảnh Vicuna13b, vì nó có vẻ như làm giảm hiệu suất phân phối ngoài trong các tác vụ không phải chú thích như được hiển thị trong bảng 9. Có thể là do kích thước batch nhỏ hơn, Vicuna13b ít nhạy cảm hơn với dữ liệu nhiễu, vì nó thực sự thấy ít hơn trong số chúng. Trong cả hai trường hợp, checkpoint cuối cùng từ các lần lặp được quy định trong Bảng 7 được chọn, với hướng dẫn từ tập dữ liệu xác thực COCO Captions. Lưu ý rằng chúng tôi tối ưu hóa Q-Former Hình ảnh trong 10 lần nhiều lần lặp hơn InstructBLIP. Lý do là chúng tôi duy trì sự phù hợp với các Q-Former khác và không khởi tạo các lớp chú ý chéo từ huấn luyện trước BLIP-2 cũng như không cho phép huấn luyện giai đoạn 2. Tuy nhiên, chúng tôi cho thấy rằng với đủ lần lặp, các lớp chú ý chéo có thể được học tương đương mà không cần các hàm mất mát phụ trợ tương phản của BLIP-2 cũng như huấn luyện giai đoạn 2.

Phép chiếu video Vicuna7b được khởi tạo từ phép chiếu hình ảnh Vicuna7b tốt nhất và trải qua xác thực mỗi 5k lần lặp trên tập dữ liệu chú thích MSRVTT [103]. Quá trình lựa chọn liên quan đến việc chọn checkpoint đi trước bất kỳ sự sụt giảm hiệu suất nào trong các vòng xác thực tiếp theo ngay cả khi có checkpoint hoạt động tốt hơn sau đó trong huấn luyện, để tránh overfitting đối với các chú thích khung xương của MSRVTT. Bảng 8 định lượng cho thấy các quan sát của chúng tôi. Do việc khởi tạo Q-Former video với Q-Former hình ảnh được huấn luyện tốt, các chú thích nhiễu của WebVid2M làm giảm hiệu suất thay vì cải thiện nó. Tuy nhiên, điều này được sửa chữa với dữ liệu sạch hơn.

Tương tự, Q-Former video Vicuna13b được khởi tạo từ checkpoint tốt nhất của Q-Former Hình ảnh Vicuna13b và được xác thực mỗi 1k lần lặp. Trong khi chúng tôi để Q-Former video Vicuna7b và 13b huấn luyện trong 15k và 25k tương ứng, chúng tôi quan sát sự hội tụ sớm tại 15k và 5k lần lặp có thể do việc khởi tạo trước với Q-Former Hình ảnh. Trong quá trình huấn luyện, 5 khung hình được lấy mẫu cho Q-Former Video Vicuna7b, trong khi 4 khung hình được lấy mẫu cho Vicuna13b để giảm nhu cầu tính toán. Hình 3 cho thấy hiệu suất video hội tụ trong 1k lần lặp trên tập dữ liệu chú thích video ngoài miền.

Phương pháp huấn luyện tốt nhất cho mỗi mô hình được xác định thông qua kinh nghiệm, và vượt quá phạm vi của bài báo để phân tích nghiêm ngặt các lý do của sự khác biệt trong huấn luyện trên các phương thức. Chúng tôi để lại điều này cho công việc tương lai.

(7b/13b) Hình ảnh Âm thanh 3D Video∗ Lần lặp 775k/880k 65k/300k 65k/300k 15k/5k Kích thước Batch 64/16 64/16 128/32 32/8

Bảng 7: Siêu tham số huấn luyện. ∗Phép chiếu Video được khởi tạo từ Phép chiếu Hình ảnh. Các tham số cho mô hình 7b/13b tương ứng.

6 Siêu tham số Đánh giá

Trong quá trình đánh giá X-InstructBLIP, chúng tôi tuân thủ một tập hợp siêu tham số nhất quán, với các biến thể nhỏ để phù hợp với nhu cầu riêng biệt của mỗi

--- TRANG 22 ---
22 A. Panagopoulou et al.

0 2000 4000 6000 8000 10000 Lần lặp 40 60 80 100 120 CIDEr Image Q-Former No Upsampling X-InstructBLIP no prefix X-InstructBLIP (7b) X-InstructBLIP (13b)

Hình 3: Điểm CIDEr trên MSVD (ngoài miền) qua các lần lặp huấn luyện trên Video Q-Former được khởi tạo từ Image Q-Former. Hầu hết các cải thiện hiệu suất được đạt được chỉ trong 1000 lần lặp.

MSVD VATEX MSVD QA test val test X-Instruct Proj. (7b) 118.2 58.5 52.5 X-Instruct Proj. (7b) -upsample 73.3(↓44.9) 41.6(↓16.9) 49.1(↓3.4)

Bảng 8: Hiệu ứng của MSRVTT Upsampling (tại 10k lần lặp)

tác vụ. Danh sách toàn diện các cấu hình này được trình bày trong bảng 10. Trong mỗi thí nghiệm, chúng tôi sử dụng Beam Search cho việc sinh sản, đặt kích thước beam thành 5, penalty lặp lại và nhiệt độ bằng 1.5 và 1 tương ứng. Đối với các tác vụ liên quan đến suy luận tương phản trên các phương thức video-âm thanh, một biểu diễn cân bằng và hiệu quả tính toán được đạt được bằng cách truy vấn hai khung hình từ cả video và âm thanh. Penalty độ dài thường được cấu hình thành 1 cho các tác vụ chú thích dài, -1 cho các tác vụ Visual Question Answering (VQA) yêu cầu câu trả lời ngắn, và 0 cho các tác vụ chú thích ngắn. Các ràng buộc độ dài tối thiểu và tối đa được điều chỉnh dựa trên tác vụ: đối với chú thích, chúng tôi duy trì phạm vi từ 10 đến 80; đối với các tác vụ VQA câu trả lời ngắn, phạm vi được đặt từ 1 đến 10; đối với chú thích độ dài biến đổi, phạm vi từ 1 đến 80. Trong trường hợp đường cơ sở InstructBLIP cho các tập dữ liệu video, chúng tôi mượn thiết lập suy luận được khuyến nghị của việc lấy mẫu 4 khung hình cho các đường cơ sở chú thích của MSVD và VATEX với prompt Một video cho thấy và cùng siêu tham số sinh sản như X-InstructBLIP.

Zero-Shot Trong miền Flickr30k NoCaps VizWiz GQA OKVQA COCO test val-all test-dev balanced test-dev test val test X-Instruct Proj. (7b) 82.1 117.7 34.9 48.1 52.5 137.7 138.2 X-Instruct Proj. (7b)-coco 79.4(↓2.7) 116.5(↓1.2) 34.2(↓0.7) 48.2(↑0.1) 52.3(↓0.2) 133.5(↓4.2) 134.3(↓3.9) X-Instruct Proj. (13b) 74.7 114.5 36.0 49.2 51.9 128.2 128.7 X-Instruct Proj. (13b)+coco 83.8(↑9.1) 118.7(↑4.2) 32.0(↓4.0) 47.0(↓2.2) 44.6(↓7.3) 138.2(↑10.0) 139.0(↑10.3)

Bảng 9: Hiệu ứng của upsampling COCO.

--- TRANG 23 ---
X-InstructBLIP 23

Tập dữ liệu Split Prompt Len. Penalty Min Len. Max Len. Hình ảnh Flickr30k [88] test: 1.000 hình ảnh Mô tả ngắn 1. 10 80 NoCaps [1] val: 4.500 hình ảnh out-domain: 1.413 hình ảnh Mô tả ngắn 1. 10 80 COCO∗[9] train: 566.747 cặp hình ảnh-chú thích val: 5.000 hình ảnh test: 5.000 hình ảnh Mô tả ngắn. 1. 10 80 VizWiz [6] test-dev: 8.000 cặp hình ảnh-câu hỏi dựa trên hình ảnh đã cho phản hồi {question} -1. 1 10 OKVQA [68] test: 5.046 ví dụ dựa trên hình ảnh đã cho phản hồi {question} câu trả lời -1. 1 10 GQA [40] balanced test-dev: 12.578 cặp hình ảnh-câu hỏi dựa trên hình ảnh đã cho phản hồi {question} -1. 1 10 3D Modelnet40 [99] test: 2.468 point cloud Mô tả mô hình 3d. Một mô hình 3d của -1. 1 3 Modelnet40 † Mô tả mô hình 3d. 0. 10 80 Âm thanh Clotho [22] eval (v1): 1.045 âm thanh val(v2): 1.045 âm thanh Mô tả ngắn. 0. 10 80 ClothoAQA [57] test: 2.838 cặp âm thanh-câu hỏi {question} -1. 1 10 ESC50 [76] test: 2.000 âm thanh Mô tả âm thanh. Một âm thanh của 1. 10 80 AudioCaps∗[43] train: 38.695 cặp âm thanh-chú thích val: 380 âm thanh Mô tả ngắn 0. 1 80 Video MSVD [10] test: 670 hình ảnh1 Mô tả ngắn 1. 10 80 MSRVTT∗[103] train: 130.260 cặp video-chú thích val: 497 video test: 2.990 video Mô tả ngắn 1. 10 80 MSVD QA [101] test: 13.157 cặp video-câu hỏi dựa trên video đã cho phản hồi {question} -1. 1 10 A+V MusicAVQA [49] val: 3.698 ví dụ test: 7.402 cặp video-câu hỏi Câu hỏi: {question} Câu trả lời: -1. 1 10 VATEX [95] val: 3.000 video Mô tả ngắn 1. 10 80

Bảng 10: Siêu tham số được sử dụng trên mỗi tập dữ liệu đánh giá. Các tập dữ liệu được gạch chân là đánh giá trong miền. ∗các tập dữ liệu được sử dụng cho việc lựa chọn checkpoint tốt nhất. Văn bản màu xanh được cung cấp như đầu vào cho LLM nhưng không phải Q-Former.

7 Bộ Điều chỉnh Hướng dẫn

Bảng 11 trình bày danh sách toàn diện các tập dữ liệu được sử dụng trong quá trình điều chỉnh hướng dẫn cho X-InstructBLIP, kèm theo kích thước tập dữ liệu tương ứng. Các tập dữ liệu được gắn nhãn ∗∗ đã được tạo tự động thông qua quy trình tính nhất quán vòng tròn. Các tập dữ liệu được đánh dấu • biểu thị các trường hợp mất dữ liệu do tham nhũng tệp hoặc liên kết hết hạn.

8 Mẫu Prompt

X-InstructBLIP đã trải qua tinh chỉnh sử dụng một mảng đa dạng của các mẫu hướng dẫn, được điều chỉnh để bao gồm một loạt rộng các tác vụ và phương thức. Để tham khảo, các mẫu cụ thể tương ứng với mỗi phương thức có thể được tìm thấy trong các bảng sau: Bảng 12 cho hình ảnh, Bảng 13 cho âm thanh, Bảng 14 cho 3D,

--- TRANG 24 ---
24 A. Panagopoulou et al.

Tác vụ Tập dữ liệu Kích thước Huấn luyện Hình ảnh Chú thích CapFilt14M [50] 13.873.136 cặp hình ảnh-chú thích Conceptual Captions 12M [9] 6.029.862 cặp hình ảnh-chú thích• MS COCO Dataset [9] 566.747 cặp hình ảnh-chú thích SBU Captions [74] 859.739 cặp hình ảnh-chú thích Visual Genome Captions [46] 821.774 cặp hình ảnh-chú thích QA AOK VQA [79] 17.056 cặp câu hỏi-câu trả lời OK VQA [68] 9.009 cặp câu hỏi-câu trả lời OCR VQA [69] 1.002.146 cặp câu hỏi-câu trả lời Visual Genome QA [46] 1.440.069 cặp câu hỏi-câu trả lời VQAV2 [29] 658.104 cặp câu hỏi-câu trả lời Đối thoại LLaVA150k [59] 394.276 cặp hình ảnh-hướng dẫn Âm thanh Chú thích AudioCaps [43] 38.701 cặp âm thanh-chú thích• WAVCaps [100] 297.341 cặp âm thanh-chú thích• QA AudioCaps QA∗∗ 24.158 cặp câu hỏi-câu trả lời Phân loại AudioSet balanced train [26] 14.141 âm thanh được gắn nhãn• 3D Chú thích Cap3D [64] 651.576 cặp point cloud-chú thích QA Cap3D QA∗∗ 250.070 cặp câu hỏi-câu trả lời Video Chú thích MSRVTT [103] 130.260 cặp video-chú thích WebVid2M [4] 2M cặp video-chú thích QA MSRVTT QA [101] 149.075 cặp câu hỏi-câu trả lời

Bảng 11: Tập dữ liệu cho Điều chỉnh Hướng dẫn: Bảng này trình bày các tập dữ liệu được sử dụng cho điều chỉnh hướng dẫn, cùng với các loại tác vụ liên quan và kích thước. •Dữ liệu thiếu do liên kết hết hạn và tệp bị hỏng. ∗∗Các tập dữ liệu được đánh dấu bằng dấu sao kép được tạo tự động trong nghiên cứu này.

và Bảng 15 cho video. So với InstructBLIP [19], các mẫu chú thích đã tăng từ 13 lên 32, trong khi các mẫu hỏi đáp đã tăng từ 10 lên 21. Những cải tiến này đã được tích hợp một cách chiến lược để thúc đẩy khả năng thích ứng lớn hơn của mô hình với một loạt rộng các hướng dẫn người dùng.

9 Tuyên bố Đạo đức

Trong nghiên cứu này, chúng tôi trình bày một khung làm việc để căn chỉnh nhiều phương thức với một mô hình ngôn ngữ lớn đông lạnh (LLM). Phương pháp của chúng tôi nghiêm túc chỉ liên quan đến việc sử dụng các tập dữ liệu có sẵn công khai và miễn phí, đảm bảo chúng tôi không tham gia vào việc thu thập dữ liệu riêng tư. Tuy nhiên, điều quan trọng là phải thừa nhận rằng các tập dữ liệu có nguồn gốc công khai mang theo những thiên kiến ngầm [23,71,110]. Những thiên kiến này phản ánh sự bất bình đẳng lịch sử và xã hội, có thể ảnh hưởng đến đầu ra của mô hình. Khung làm việc của chúng tôi được xây dựng trên một LLM đông lạnh có sẵn trước. Trong khi phương pháp này hưởng lợi từ kiến thức rộng lớn được mã hóa trong LLM, điều quan trọng là phải nhận ra rằng các mô hình như vậy có thể vốn dĩ truyền bá những thiên kiến có mặt trong dữ liệu huấn luyện của chúng. Ngoài ra, có một rủi ro không thể bỏ qua là tạo ra thông tin sai lệch hoặc gây hiểu lầm. Mặc dù tồn tại các công cụ để đo lường độc tính của mô hình ngôn ngữ như Helm [55], các tập dữ liệu đánh giá của chúng bị hạn chế trong phương thức ngôn ngữ, và do đó không thể áp dụng để đo lường độc tính trên các phương thức là

--- TRANG 25 ---
X-InstructBLIP 25

Mẫu Hướng dẫn Hình ảnh QA "{question}" "Q: {question} A:" "Trả lời câu hỏi sau: {question}" "Câu hỏi: {question} Câu trả lời:" "Bạn sẽ trả lời {question} như thế nào?" "Câu trả lời cho câu hỏi {question} là gì?" "Trả lời câu hỏi dựa trên hình ảnh. Câu hỏi: {question} Câu trả lời:" "Hướng dẫn: Trả lời câu hỏi sau bằng cách tham chiếu đến hình ảnh đầu vào. Câu hỏi: {question} Câu trả lời:" "Cho bức ảnh, câu trả lời cho câu hỏi {question} là gì?" "Phản hồi của bạn đối với truy vấn {question} là gì?" "Vui lòng cung cấp câu trả lời cho {question}" "Phản hồi truy vấn {question}" "Dựa trên hình ảnh đã cho, phản hồi {question}" "Câu hỏi: {question} Phản hồi của bạn là gì?" "Xem xét truy vấn sau: {question}" "Bạn có thể giúp trả lời câu hỏi {question} không?" "Tham chiếu hình ảnh được cung cấp, bạn có thể trả lời câu hỏi {question} không?" "Về hình ảnh được hiển thị, vui lòng trả lời {question}" "Câu trả lời của bạn cho {question} trong bối cảnh hình ảnh được cung cấp là gì?" "Câu hỏi (tham chiếu hình ảnh để có bối cảnh): {question} Câu trả lời:" "Để phản hồi câu hỏi {question}, câu trả lời của bạn sẽ là gì?" Chú thích "Chú thích ngắn:" "Mô tả ngắn:" "Một bức ảnh của" "Một bức ảnh cho thấy" "Một hình ảnh của" "Một hình ảnh cho thấy" "Một ảnh của" "Một ảnh cho thấy" "Viết mô tả ngắn." "Viết mô tả cho hình ảnh." "Cung cấp mô tả về những gì được trình bày trong hình ảnh." "Mô tả ngắn gọn nội dung của hình ảnh." "Bạn có thể giải thích ngắn gọn những gì bạn nhìn thấy trong hình ảnh không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong hình ảnh không?" "Vui lòng cung cấp mô tả ngắn về hình ảnh." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về hình ảnh." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong bức ảnh." "Viết mô tả cho bức ảnh." "Cung cấp mô tả về những gì được trình bày trong bức ảnh." "Mô tả ngắn gọn nội dung của bức ảnh." "Bạn có thể giải thích ngắn gọn những gì bạn nhìn thấy trong bức ảnh không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong bức ảnh không?" "Vui lòng cung cấp mô tả ngắn về hình ảnh." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về hình ảnh." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong hình ảnh." "Viết mô tả cho hình ảnh." "Cung cấp mô tả về những gì được trình bày trong hình ảnh." "Mô tả ngắn gọn nội dung của hình ảnh." "Bạn có thể giải thích ngắn gọn những gì bạn nhìn thấy trong hình ảnh không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong hình ảnh không?" "Vui lòng cung cấp mô tả ngắn về hình ảnh." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về hình ảnh." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong hình ảnh."

Bảng 12: Mẫu điều chỉnh hướng dẫn cho các tác vụ hình ảnh

trọng tâm của công trình này. Chúng tôi để lại việc tạo ra các tập dữ liệu đa phương thức để đo lường độc tính và thiên kiến như một hướng nghiên cứu tương lai.

Người dùng khung làm việc của chúng tôi nên nhận thức về những hạn chế này và thận trọng, đặc biệt trong các ứng dụng mà tính chính xác và công bằng của đầu ra là quan trọng. Chúng tôi ủng hộ việc sử dụng có trách nhiệm khung làm việc của chúng tôi, đặc biệt trong các bối cảnh nhạy cảm. Người dùng nên đánh giá nghiêm túc và xác minh đầu ra của mô hình và xem xét tiềm năng củng cố thiên kiến hoặc lan truyền thông tin sai lệch.

Hơn nữa, chúng tôi cam kết minh bạch về khả năng và hạn chế của mô hình. Tất cả mã nguồn, dữ liệu và trọng số mô hình sẽ được phát hành để đảm bảo tính tái tạo và khuyến khích đánh giá bên ngoài và nghiên cứu tiếp theo.

10 Tuyên bố Tái tạo

Phù hợp với các nguyên tắc của khoa học mở và để thúc đẩy tính tái tạo, minh bạch, và nghiên cứu tiếp theo, chúng tôi hứa sẽ cung cấp quyền truy cập mã nguồn mở cho tất cả các tài nguyên liên quan đến nghiên cứu của chúng tôi, bao gồm: một codebase hoàn chỉnh, được tài liệu hóa, và công khai với tất cả các script, mô hình, tiền xử lý, và đánh giá

--- TRANG 26 ---
26 A. Panagopoulou et al.

Mẫu Hướng dẫn Âm thanh QA "{question}" "Câu hỏi: {question} Câu trả lời:" "Q: {question} A:" "Dựa trên âm thanh, {question}" "Trả lời câu hỏi sau dựa trên âm thanh: {question}" "Câu hỏi: {question} Cung cấp câu trả lời dựa trên âm thanh." "Bạn sẽ trả lời {question} dựa trên âm thanh như thế nào?" "Câu trả lời cho câu hỏi {question} sử dụng âm thanh làm tham chiếu là gì?" "Trả lời câu hỏi sử dụng âm thanh. Câu hỏi: {question} Câu trả lời:" "Hướng dẫn: Trả lời câu hỏi sau bằng cách tham chiếu âm thanh. Câu hỏi: {question} Câu trả lời:" "Cho âm thanh, câu trả lời cho câu hỏi {question} là gì?" "Phản hồi của bạn đối với truy vấn {question} xem xét âm thanh là gì?" "Vui lòng cung cấp câu trả lời cho {question} sử dụng âm thanh làm bối cảnh." "Phản hồi truy vấn {question} dựa trên nội dung âm thanh." "Dựa trên âm thanh được cung cấp, phản hồi {question}" "Câu hỏi: {question} Phản hồi của bạn sử dụng âm thanh để có bối cảnh là gì?" "Xem xét truy vấn sau và âm thanh: {question}" "Bạn có thể giúp trả lời câu hỏi {question} sử dụng âm thanh làm tham chiếu không?" "Tham chiếu âm thanh được cung cấp, bạn có thể trả lời câu hỏi {question} không?" "Về âm thanh được cung cấp, vui lòng trả lời {question}" "Câu trả lời của bạn cho {question} trong bối cảnh âm thanh được cung cấp là gì?" "Câu hỏi (tham chiếu âm thanh để có bối cảnh): {question} Câu trả lời:" "Để phản hồi câu hỏi {question}, câu trả lời của bạn dựa trên âm thanh sẽ là gì?" "Cho âm thanh, bạn sẽ phản hồi {question} như thế nào?" "Xem xét âm thanh, phản hồi của bạn đối với {question} là gì?" "Dựa trên âm thanh, bạn sẽ trả lời {question} như thế nào?" Phân loại "Phân loại âm thanh sau:" "Danh mục của đoạn âm thanh này là gì?" "Xác định nội dung của âm thanh sau:" "Cung cấp phân loại cho âm thanh." "Phân tích và phân loại âm thanh sau." "Mô tả danh mục của âm thanh đã cho." "Xác định loại của đoạn âm thanh này." "Bạn có thể phân loại những gì bạn nghe trong âm thanh không?" "Loại âm thanh này là gì?" "Bạn sẽ phân loại đoạn âm thanh này như thế nào?" "Vui lòng xác định danh mục của âm thanh sau:" "Âm thanh sau thuộc danh mục nào?" "Phân loại âm thanh trong đoạn âm thanh này." Chú thích "Chú thích ngắn:" "Mô tả ngắn:" "Một âm thanh của" "Một âm thanh cho thấy" "Viết mô tả ngắn." "Viết mô tả cho âm thanh." "Cung cấp mô tả về những gì được trình bày trong âm thanh." "Mô tả ngắn gọn nội dung của âm thanh." "Bạn có thể giải thích ngắn gọn những gì bạn nghe trong âm thanh không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong âm thanh không?" "Vui lòng cung cấp mô tả ngắn về âm thanh." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về âm thanh." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong âm thanh." "Mô tả ngắn gọn nội dung của âm thanh." "Vui lòng cung cấp tóm tắt ngắn về âm thanh." "Âm thanh chứa gì?" "Bạn có thể nghe thấy gì trong âm thanh?" "Âm thanh nào có mặt trong âm thanh?" "Tóm tắt âm thanh trong vài từ." "Viết tóm tắt ngắn về nội dung âm thanh." "Bạn có thể cung cấp giải thích ngắn gọn về nội dung của âm thanh không?" "Mô tả âm thanh đại diện cho gì." "Âm thanh mô tả gì?" "Trong vài từ, mô tả những gì bạn nghe trong âm thanh."

Bảng 13: Mẫu điều chỉnh hướng dẫn cho các tác vụ âm thanh

mã cần thiết để tái tạo các thí nghiệm. Chúng tôi sẽ tiếp tục phát hành các trọng số mô hình được huấn luyện trước cùng với các cấu hình đánh giá chính xác đã tạo ra kết quả được trích dẫn trong bài báo. Chúng tôi cho thấy cam kết của chúng tôi đối với tính tái tạo thông qua một phần bổ sung rộng rãi làm nổi bật chi tiết về huấn luyện và đánh giá. Hơn nữa, tất cả các thí nghiệm được hoàn thành với các seed ngẫu nhiên được chỉ định trước cũng sẽ được cung cấp trong các tệp cấu hình thí nghiệm. Cuối cùng, chúng tôi sẽ phát hành tất cả các tập dữ liệu được thu thập cho nghiên cứu này để tải xuống công khai, cũng như mã được sử dụng để tạo ra chúng. Ngoài việc cung cấp những tài nguyên này, chúng tôi cam kết duy trì chúng và cung cấp hỗ trợ cần thiết cho bất kỳ truy vấn hoặc làm rõ nào liên quan đến các tài nguyên được cung cấp, đóng góp vào một môi trường nghiên cứu hỗ trợ và bao dung.

--- TRANG 27 ---
X-InstructBLIP 27

Mẫu Hướng dẫn 3D QA "{question}" "Câu hỏi: {question} Câu trả lời:" "Q: {question} A:" "Dựa trên mô hình 3D, {question}" "Trả lời câu hỏi sau dựa trên mô hình 3D: {question}" "Câu hỏi: {question} Cung cấp câu trả lời dựa trên mô hình 3D." "Bạn sẽ trả lời {question} dựa trên mô hình 3D như thế nào?" "Câu trả lời cho câu hỏi {question} sử dụng mô hình 3D làm tham chiếu là gì?" "Trả lời câu hỏi sử dụng mô hình 3D. Câu hỏi: {question} Câu trả lời:" "Hướng dẫn: Trả lời câu hỏi sau bằng cách tham chiếu mô hình 3D. Câu hỏi: {question} Câu trả lời:" "Cho mô hình 3D, câu trả lời cho câu hỏi {question} là gì?" "Phản hồi của bạn đối với truy vấn {question} xem xét mô hình 3D là gì?" "Vui lòng cung cấp câu trả lời cho {question} sử dụng mô hình 3D làm bối cảnh." "Phản hồi truy vấn {question} dựa trên nội dung mô hình 3D." "Dựa trên mô hình 3D được cung cấp, phản hồi {question}" "Câu hỏi: {question} Phản hồi của bạn sử dụng mô hình 3D để có bối cảnh là gì?" "Xem xét truy vấn sau và mô hình 3D: {question}" "Bạn có thể giúp trả lời câu hỏi {question} sử dụng mô hình 3D làm tham chiếu không?" "Tham chiếu mô hình 3D được cung cấp, bạn có thể trả lời câu hỏi {question} không?" "Về mô hình 3D được cung cấp, vui lòng trả lời {question}" "Câu trả lời của bạn cho {question} trong bối cảnh mô hình 3D được cung cấp là gì?" "Câu hỏi (tham chiếu mô hình 3D để có bối cảnh): {question} Câu trả lời:" "Để phản hồi câu hỏi {question}, câu trả lời của bạn dựa trên mô hình 3D sẽ là gì?" "Cho mô hình 3D, bạn sẽ phản hồi {question} như thế nào?" "Xem xét mô hình 3D, phản hồi của bạn đối với {question} là gì?" "Dựa trên mô hình 3D, bạn sẽ trả lời {question} như thế nào?" Chú thích "Chú thích ngắn:" "Mô tả ngắn:" "Một mô hình 3D của" "Một mô hình 3D cho thấy" "Viết mô tả ngắn." "Viết mô tả cho mô hình 3D." "Cung cấp mô tả về những gì được trình bày trong mô hình 3D." "Mô tả ngắn gọn nội dung của mô hình 3D." "Bạn có thể giải thích ngắn gọn những gì bạn nhìn thấy trong mô hình 3D không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong mô hình 3D không?" "Vui lòng cung cấp mô tả ngắn về mô hình 3D." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về mô hình 3D." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong mô hình 3D." "Mô tả ngắn gọn nội dung của mô hình 3D." "Vui lòng cung cấp tóm tắt ngắn về mô hình 3D." "Mô hình 3D chứa gì?" "Bạn có thể xác định gì trong mô hình 3D?" "Cấu trúc nào có mặt trong mô hình 3D?" "Tóm tắt mô hình 3D trong vài từ." "Viết tóm tắt ngắn về nội dung mô hình 3D." "Bạn có thể cung cấp giải thích ngắn gọn về nội dung của mô hình 3D không?" "Mô tả mô hình 3D đại diện cho gì." "Mô hình 3D mô tả gì?" "Trong vài từ, mô tả những gì bạn nhìn thấy trong mô hình 3D."

Bảng 14: Mẫu điều chỉnh hướng dẫn cho các tác vụ 3D

Tài liệu tham khảo

1. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D., Parikh, D., Lee, S., Anderson, P.: nocaps: novel object captioning at scale. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 8948–8957 (2019)

2. Alamri, H., Hori, C., Marks, T.K., Batra, D., Parikh, D.: Audio visual scene-aware dialog (avsd) track for natural language generation in dstc7. In: DSTC7 at AAAI2019 Workshop. vol. 2 (2018)

3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems 35, 23716–23736 (2022)

4. Bain, M., Nagrani, A., Varol, G., Zisserman, A.: Frozen in time: A joint video and image encoder for end-to-end retrieval. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1728–1738 (2021)

5. Bansal, A., Zhang, Y., Chellappa, R.: Visual question answering on image sets. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16. pp. 51–67. Springer (2020)

6. Bigham, J.P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R.C., Miller, R., Tatarowicz, A., White, B., White, S., et al.: Vizwiz: nearly real-time answers to

--- TRANG 28 ---
28 A. Panagopoulou et al.

Mẫu Hướng dẫn Video QA "Cho video, {question}" "Q: {question} A:" "Trả lời câu hỏi sau dựa trên video: {question}" "Câu hỏi: {question} Câu trả lời:" "Bạn sẽ trả lời {question} sau khi xem video như thế nào?" "Câu trả lời cho câu hỏi {question} sau khi xem video là gì?" "Trả lời câu hỏi dựa trên video. Câu hỏi: {question} Câu trả lời:" "Hướng dẫn: Trả lời câu hỏi sau bằng cách tham chiếu video đầu vào. Câu hỏi: {question} Câu trả lời:" "Cho video, câu trả lời cho câu hỏi {question} là gì?" "Phản hồi của bạn đối với truy vấn {question} sau khi xem video là gì?" "Vui lòng cung cấp câu trả lời cho {question} sau khi xem video" "Phản hồi truy vấn {question} dựa trên video" "Dựa trên video đã cho, phản hồi {question}" "Câu hỏi: {question} Phản hồi của bạn sau khi xem video là gì?" "Xem xét truy vấn sau: {question}" "Bạn có thể giúp trả lời câu hỏi {question} không?" "Tham chiếu video được cung cấp, bạn có thể trả lời câu hỏi {question} không?" "Về video được hiển thị, vui lòng trả lời {question}" "Câu trả lời của bạn cho {question} trong bối cảnh video được cung cấp là gì?" "Câu hỏi (tham chiếu video để có bối cảnh): {question} Câu trả lời:" "Để phản hồi câu hỏi {question}, câu trả lời của bạn sau khi xem video sẽ là gì?" Chú thích "Chú thích ngắn cho video:" "Mô tả ngắn về video:" "Một video của" "Một video cho thấy" "Mô tả video ngắn gọn." "Viết mô tả cho video." "Cung cấp mô tả về những gì được trình bày trong video." "Mô tả ngắn gọn nội dung của video." "Bạn có thể giải thích ngắn gọn những gì bạn nhìn thấy trong video không?" "Bạn có thể sử dụng một vài từ để mô tả những gì bạn nhận thức trong video không?" "Vui lòng cung cấp mô tả ngắn về video." "Sử dụng ngôn ngữ, cung cấp một tài khoản ngắn về video." "Sử dụng một vài từ để minh họa những gì đang xảy ra trong video."

Bảng 15: Mẫu điều chỉnh hướng dẫn cho các tác vụ âm thanh

câu hỏi thị giác. In: Proceedings of the 23nd annual ACM symposium on User interface software and technology. pp. 333–342 (2010)

7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020)

8. Buch, S., Eyzaguirre, C., Gaidon, A., Wu, J., Fei-Fei, L., Niebles, J.C.: Revisiting the "Video" in Video-Language Understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022)

9. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3558–3568 (2021)

10. Chen, D., Dolan, W.B.: Collecting highly parallel data for paraphrase evaluation. In: Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. pp. 190–200 (2011)

11. Chen, F., Han, M., Zhao, H., Zhang, Q., Shi, J., Xu, S., Xu, B.: X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. arXiv preprint arXiv:2305.04160 (2023)

12. Chen, J., Guo, H., Yi, K., Li, B., Elhoseiny, M.: Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18030–18040 (2022)

13. Chen, S., Wu, Y., Wang, C., Liu, S., Tompkins, D., Chen, Z., Che, W., Yu, X., Wei, F.: BEATs: Audio pre-training with acoustic tokenizers. In: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) Proceedings of the 40th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 202, pp. 5178–5193. PMLR (23–29 Jul 2023), https://proceedings.mlr.press/v202/chen23ag.html

14. Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., Liu, J.: VAST: A vision-audio-subtitle-text omni-modality foundation model and dataset. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=scYa9DYUAy

15. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: Simclr: A simple framework for contrastive learning of visual representations. In: International Conference on Learning Representations. vol. 2 (2020)

16. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive learning of visual representations. In: International conference on machine learning. pp. 1597–1607. PMLR (2020)

17. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.E., et al.: Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023) (2023)

18. Cho, J., Lei, J., Tan, H., Bansal, M.: Unifying vision-and-language tasks via text generation. In: International Conference on Machine Learning. pp. 1931–1942. PMLR (2021)

19. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.: InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=vvoWPYqZJA

20. Deshmukh, S., Elizalde, B., Singh, R., Wang, H.: Pengi: An audio language model for audio tasks. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=gJLAfO4KUq

21. Driess, D., Xia, F., Sajjadi, M.S.M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., Florence, P.: PaLM-e: An embodied multimodal language model. In: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) Proceedings of the 40th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 202, pp. 8469–8488. PMLR (23–29 Jul 2023), https://proceedings.mlr.press/v202/driess23a.html

22. Drossos, K., Lipping, S., Virtanen, T.: Clotho: An audio captioning dataset. In: ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 736–740. IEEE (2020)

23. Fabbrizzi, S., Papadopoulos, S., Ntoutsi, E., Kompatsiaris, I.: A survey on bias in visual datasets. Computer Vision and Image Understanding 223, 103552 (2022)

24. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X., Cao, Y.: Eva: Exploring the limits of masked visual representation learning at scale. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19358–19369 (2023)

25. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394 (2023)

26. Gemmeke, J.F., Ellis, D.P., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio set: An ontology and human-labeled dataset for audio events. In: 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 776–780. IEEE (2017)

--- TRANG 29 ---
X-InstructBLIP 29

27. Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.V., Joulin, A., Misra, I.: Imagebind: One embedding space to bind them all. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 15180–15190 (June 2023)

28. Gong, Y., Luo, H., Liu, A.H., Karlinsky, L., Glass, J.R.: Listen, think, and understand. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=nBZBPXdJlC

29. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2017)

30. Guangyao li, Yixin Xu, D.H.: Multi-scale attention for audio question answering. Proc. INTERSPEECH (2023)

31. Gui, L., Wang, B., Huang, Q., Hauptmann, A.G., Bisk, Y., Gao, J.: Kat: A knowledge augmented transformer for vision-and-language. In: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 956–968 (2022)

32. Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., et al.: Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following. arXiv preprint arXiv:2309.00615 (2023)

33. Guzhov, A., Raue, F., Hees, J., Dengel, A.: Audioclip: Extending clip to image, text and audio. In: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 976–980. IEEE (2022)

34. Han, J., Gong, K., Zhang, Y., Wang, J., Zhang, K., Lin, D., Qiao, Y., Gao, P., Yue, X.: Onellm: One framework to align all modalities with language. arXiv preprint arXiv:2312.03700 (2023)

35. Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., et al.: Imagebind-llm: Multi-modality instruction tuning. arXiv preprint arXiv:2309.03905 (2023)

36. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised visual representation learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9729–9738 (2020)

37. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., Gan, C.: 3d-LLM: Injecting the 3d world into large language models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=YQA28p7qNz

38. Hu, E.J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. In: International Conference on Learning Representations (2021)

39. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., Liu, Q., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V., Som, S., Song, X., Wei, F.: Language is not all you need: Aligning perception with language models. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=UpN2wfrLec

40. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning and compositional question answering. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6700–6709 (2019)

41. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., Carreira, J.: Perceiver: General perception with iterative attention. In: International conference on machine learning. pp. 4651–4664. PMLR (2021)

--- TRANG 30 ---
30 A. Panagopoulou et al.

42. Jiang, C., Ye, W., Xu, H., Huang, S., Huang, F., Zhang, S.: Vision language pre-training by contrastive learning with cross-modal similarity regulation. In: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 14660–14679. Association for Computational Linguistics, Toronto, Canada (Jul 2023). https://doi.org/10.18653/v1/2023.acl-long.819, https://aclanthology.org/2023.acl-long.819

43. Kim, C.D., Kim, B., Lee, H., Kim, G.: Audiocaps: Generating captions for audios in the wild. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 119–132 (2019)

44. Kim, M., Sung-Bin, K., Oh, T.H.: Prefix tuning for automated audio captioning. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp. 1–5. IEEE (2023)

45. Koh, J.Y., Salakhutdinov, R., Fried, D.: Grounding language models to images for multimodal inputs and outputs. In: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) Proceedings of the 40th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 202, pp. 17283–17300. PMLR (23–29 Jul 2023), https://proceedings.mlr.press/v202/koh23a.html

46. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision 123, 32–73 (2017)

47. Li, C., Xu, H., Tian, J., Wang, W., Yan, M., Bi, B., Ye, J., Chen, H., Xu, G., Cao, Z., Zhang, J., Huang, S., Huang, F., Zhou, J., Si, L.: mPLUG: Effective and efficient vision-language learning by cross-modal skip-connections. In: Goldberg, Y., Kozareva, Z., Zhang, Y. (eds.) Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 7241–7259. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates (Dec 2022). https://doi.org/10.18653/v1/2022.emnlp-main.488, https://aclanthology.org/2022.emnlp-main.488

48. Li, D., Li, J., Le, H., Wang, G., Savarese, S., Hoi, S.C.: LAVIS: A one-stop library for language-vision intelligence. In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). pp. 31–41. Association for Computational Linguistics, Toronto, Canada (Jul 2023), https://aclanthology.org/2023.acl-demo.3

49. Li, G., Wei, Y., Tian, Y., Xu, C., Wen, J.R., Hu, D.: Learning to answer questions in dynamic audio-visual scenarios. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19108–19118 (2022)

50. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In: 40th International Conference on Machine Learning (2023)

51. Li, J., Li, D., Xiong, C., Hoi, S.: BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In: Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., Sabato, S. (eds.) Proceedings of the 39th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 162, pp. 12888–12900. PMLR (17–23 Jul 2022), https://proceedings.mlr.press/v162/li22n.html

--- TRANG 31 ---
X-InstructBLIP 31

52. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34, 9694–9705 (2021)

53. Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang, L., Hu, H., Dong, L., Wei, F., et al.: Oscar: Object-semantics aligned pre-training for vision-language tasks. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16. pp. 121–137. Springer (2020)

54. Li, Y., Li, W., Nie, L.: MMCoQA: Conversational question answering over text, tables, and images. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 4220–4231. Association for Computational Linguistics, Dublin, Ireland (May 2022). https://doi.org/10.18653/v1/2022.acl-long.290, https://aclanthology.org/2022.acl-long.290

55. Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C.A., Manning, C.D., Re, C., Acosta-Navas, D., Hudson, D.A., Zelikman, E., Durmus, E., Ladhak, F., Rong, F., Ren, H., Yao, H., WANG, J., Santhanam, K., Orr, L., Zheng, L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., Chatterji, N.S., Khattab, O., Henderson, P., Huang, Q., Chi, R.A., Xie, S.M., Santurkar, S., Ganguli, S., Hashimoto, T., Icard, T., Zhang, T., Chaudhary, V., Wang, W., Li, X., Mai, Y., Zhang, Y., Koreeda, Y.: Holistic evaluation of language models. Transactions on Machine Learning Research (2023), https://openreview.net/forum?id=iO4LZibEqW, featured Certification, Expert Certification

56. Lin, Y., Xie, Y., Chen, D., Xu, Y., Zhu, C., Yuan, L.: Revive: Regional visual representation matters in knowledge-based visual question answering. Advances in Neural Information Processing Systems 35, 10560–10571 (2022)

57. Lipping, S., Sudarsanam, P., Drossos, K., Virtanen, T.: Clotho-aqa: A crowdsourced dataset for audio question answering. In: 2022 30th European Signal Processing Conference (EUSIPCO). pp. 1140–1144. IEEE (2022)

58. Liu, H., Yan, W., Abbeel, P.: Language quantized autoencoders: Towards unsupervised text-image alignment. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=mlxRLIy7kc

59. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=w0H2xGHlkw

60. Liu*, P.J., Saleh*, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., Shazeer, N.: Generating wikipedia by summarizing long sequences. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=Hyg0vbWC-

61. Liu, S., Zhu, Z., Ye, N., Guadarrama, S., Murphy, K.: Improved image captioning via policy gradient optimization of spider. In: Proceedings of the IEEE international conference on computer vision. pp. 873–881 (2017)

62. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (2018)

63. Luo, R., Zhao, Z., Yang, M., junwei dong, Li, D., Wang, T., Qiu, M., Hu, L., zhongyu wei: Valley: Video assistant with large language model enhanced ability (2024), https://openreview.net/forum?id=bjyf5FyQ0a

64. Luo, T., Rockwell, C., Lee, H., Johnson, J.: Scalable 3d captioning with pretrained models. In: Proceedings of the NeurIPS 2023 (2023)

--- TRANG 32 ---
32 A. Panagopoulou et al.

65. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine learning research 9(11) (2008)

66. Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards detailed video understanding via large vision and language models (2023)

67. Mañas, O., Rodriguez Lopez, P., Ahmadi, S., Nematzadeh, A., Goyal, Y., Agrawal, A.: MAPL: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting. In: Vlachos, A., Augenstein, I. (eds.) Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. pp. 2523–2548. Association for Computational Linguistics, Dubrovnik, Croatia (May 2023). https://doi.org/10.18653/v1/2023.eacl-main.185, https://aclanthology.org/2023.eacl-main.185

68. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question answering benchmark requiring external knowledge. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2019)

69. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question answering by reading text in images. In: ICDAR (2019)

70. Moon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith, M., Jain, S., Yeh, C.F., Murugesan, P., Heidari, P., Liu, Y., et al.: Anymal: An efficient and scalable any-modality augmented language model. arXiv preprint arXiv:2309.16058 (2023)

71. Motoki, F., Neto, V.P., Rodrigues, V.: More human than human: Measuring chatgpt political bias. Public Choice pp. 1–21 (2023)

72. Nagrani, A., Seo, P.H., Seybold, B., Hauth, A., Manen, S., Sun, C., Schmid, C.: Learning audio-video modalities from image captions. In: European Conference on Computer Vision. pp. 407–426. Springer (2022)

73. Najdenkoska, I., Zhen, X., Worring, M.: Meta learning to bridge vision and language models for multimodal few-shot learning. In: The Eleventh International Conference on Learning Representations (2023), https://openreview.net/forum?id=3oWo92cQyxL

74. Ordonez, V., Kulkarni, G., Berg, T.: Im2text: Describing images using 1 million captioned photographs. In: Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., Weinberger, K. (eds.) Advances in Neural Information Processing Systems. vol. 24. Curran Associates, Inc. (2011), https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf

75. Paranjape, B., Lamm, M., Tenney, I.: Retrieval-guided counterfactual generation for qa. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1670–1686 (2022)

76. Piczak, K.J.: Esc: Dataset for environmental sound classification. In: Proceedings of the 23rd ACM international conference on Multimedia. pp. 1015–1018 (2015)

77. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021)

78. Salesforce: Ulip. https://github.com/salesforce/ULIP (2022), accessed: 2023-07-1

79. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A benchmark for visual question answering using world knowledge. In: European Conference on Computer Vision. pp. 146–162. Springer (2022)

80. Shao, Z., Yu, Z., Wang, M., Yu, J.: Prompting large language models with answer heuristics for knowledge-based visual question answering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 14974–14983 (2023)

--- TRANG 33 ---
X-InstructBLIP 33

81. Shu, F., Zhang, L., Jiang, H., Xie, C.: Audio-visual llm for video understanding. arXiv preprint arXiv:2312.06720 (2023)

82. Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., Cai, D.: PandaGPT: One model to instruction-follow them all. In: Hazarika, D., Tang, X.R., Jin, D. (eds.) Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants! pp. 11–23. Association for Computational Linguistics, Prague, Czech Republic (Sep 2023), https://aclanthology.org/2023.tllm-1.2

83. Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., Wang, X.: Emu: Generative pretraining in multimodality. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=mL8Q9OOamV

84. Tanaka, R., Nishida, K., Nishida, K., Hasegawa, T., Saito, I., Saito, K.: Slidevqa: A dataset for document visual question answering on multiple images. In: AAAI (2023)

85. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., MA, Z., Zhang, C.: SALMONN: Towards generic hearing abilities for large language models. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=14rn7HpKVk

86. Tsimpoukelli, M., Menick, J.L., Cabi, S., Eslami, S., Vinyals, O., Hill, F.: Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems 34, 200–212 (2021)

87. Uy, M.A., Pham, Q.H., Hua, B.S., Nguyen, T., Yeung, S.K.: Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 1588–1597 (2019)

88. Van Zwol, R.: Flickr: Who is looking? In: IEEE/WIC/ACM International Conference on Web Intelligence (WI'07). pp. 184–190. IEEE (2007)

89. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based image description evaluation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4566–4575 (2015)

90. Wang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z., Liu, C., Wang, L.: Git: A generative image-to-text transformer for vision and language. Transactions on Machine Learning Research (2022)

91. Wang, P., Wang, S., Lin, J., Bai, S., Zhou, X., Zhou, J., Wang, X., Zhou, C.: Onepeace: Exploring one general representation model toward unlimited modalities. arXiv preprint arXiv:2305.11172 (2023)

92. Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou, J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In: International Conference on Machine Learning. pp. 23318–23340. PMLR (2022)

93. Wang, T., Ge, Y., Zheng, F., Cheng, R., Shan, Y., Qie, X., Luo, P.: Accelerating vision-language pretraining with free language modeling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 23161–23170 (June 2023)

94. Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O.K., Singhal, S., Som, S., et al.: Image as a foreign language: Beit pretraining for vision and vision-language tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 19175–19186 (2023)

--- TRANG 34 ---
34 A. Panagopoulou et al.

95. Wang, X., Wu, J., Chen, J., Li, L., Wang, Y.F., Wang, W.Y.: Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (October 2019)

96. Wang, Z., Chen, C., Li, P., Liu, Y.: Filling the image information gap for vqa: Prompting large language models to proactively ask questions. In: Findings of the Association for Computational Linguistics: EMNLP 2023. pp. 2874–2890 (2023)

97. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M., Le, Q.V.: Finetuned language models are zero-shot learners. In: International Conference on Learning Representations (2022), https://openreview.net/forum?id=gEZrGCozdqR

98. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35, 24824–24837 (2022)

99. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3d shapenets: A deep representation for volumetric shapes. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1912–1920 (2015)

100. XinhaoMei: Wavcaps. https://github.com/XinhaoMei/WavCaps (2023), accessed: 2023-07-1

101. Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., Zhuang, Y.: Video question answering via gradually refined attention over appearance and motion. In: Proceedings of the 25th ACM international conference on Multimedia. pp. 1645–1653 (2017)

102. Xu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi, B., Qian, Q., Wang, W., Xu, G., Zhang, J., Huang, S., Huang, F., Zhou, J.: Mplug-2: A modularized multimodal foundation model across text, image and video. In: Proceedings of the 40th International Conference on Machine Learning. ICML'23, JMLR.org (2023)

103. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for bridging video and language. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 5288–5296 (2016)

104. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., Lin, D.: Pointllm: Empowering large language models to understand point clouds (2023)

105. Xu, W., Chen, K., Zhao, T.: Discriminative reasoning for document-level relation extraction. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1653–1663. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.findings-acl.144, https://aclanthology.org/2021.findings-acl.144

106. Xue, L., Gao, M., Xing, C., Martín-Martín, R., Wu, J., Xiong, C., Xu, R., Niebles, J.C., Savarese, S.: Ulip: Learning a unified representation of language, images, and point clouds for 3d understanding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1179–1189 (2023)

107. Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems 35, 124–141 (2022)

108. Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Unitab: Unifying text and box outputs for grounded vision-language modeling. In: European Conference on Computer Vision. pp. 521–539. Springer (2022)

109. Yang, Z., Gan, Z., Wang, J., Hu, X., Lu, Y., Liu, Z., Wang, L.: An empirical study of gpt-3 for few-shot knowledge-based vqa. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 3081–3089 (2022)

--- TRANG 35 ---
X-InstructBLIP 35

110. Yeh, K.C., Chi, J.A., Lian, D.C., Hsieh, S.K.: Evaluating interfaced llm bias. In: Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023). pp. 292–299 (2023)

111. Yu, L., Cheng, Y., Wang, Z., Kumar, V., Macherey, W., Huang, Y., Ross, D.A., Essa, I., Bisk, Y., Yang, M.H., Murphy, K.P., Hauptmann, A.G., Jiang, L.: SPAE: Semantic pyramid autoencoder for multimodal generation with frozen LLMs. In: Thirty-seventh Conference on Neural Information Processing Systems (2023), https://openreview.net/forum?id=CXPUg86A1D

112. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490 (2023)

113. Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual language model for video understanding. Empirical Methods in Natural Language Processing 2023, Demo Track (2023)

114. Zhang, R., Han, J., Liu, C., Zhou, A., Lu, P., Li, H., Gao, P., Qiao, Y.: LLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=d4UiXAHN2W

115. Zhao, Z., Guo, L., Yue, T., Chen, S., Shao, S., Zhu, X., Yuan, Z., Liu, J.: Chatbridge: Bridging modalities with large language model as a language catalyst. arXiv preprint arXiv:2305.16103 (2023)

116. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing vision-language understanding with advanced large language models. In: The Twelfth International Conference on Learning Representations (2024), https://openreview.net/forum?id=1tZbq88f27
