# 2310.06434.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.06434.pdf
# File size: 806343 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Whispering LLaMA: A Cross-Modal Generative Error Correction
Framework for Speech Recognition
Srijith Radhakrishnan1,2,5, Chao-Han Huck Yang1,3, Sumeer Ahmad Khan1,5, Rohit
Kumar1, Narsis A. Kiani4, David Gomez-Cabrero1, Jesper N. Tegner1,4
1King Abdullah University of Science and Technology2Manipal Institute of Technology
3Georgia Institute of Technology4Karolinska Institute
5SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence
srijithrkr@gmail.com; huckiyang@gatech.edu
Abstract
We introduce a new cross-modal fusion tech-
nique designed for generative error correc-
tion in automatic speech recognition (ASR).
Our methodology leverages both acoustic in-
formation and external linguistic representa-
tions to generate accurate speech transcrip-
tion contexts. This marks a step towards a
fresh paradigm in generative error correction
within the realm of n-best hypotheses. Unlike
the existing ranking-based rescoring methods,
our approach adeptly uses distinct initializa-
tion techniques and parameter-efficient algo-
rithms to boost ASR performance derived from
pre-trained speech and text models. Through
evaluation across diverse ASR datasets, we
assess our fusion technique, demonstrating
a 37.66% improvement in word error rate
(WER) relative performance compared to the
n-best Oracle. To encourage future research,
we have made our code and pre-trained mod-
els open source at: https://github.com/
Srijith-rkr/Whispering-LLaMA .
1 Introduction
End-to-end (E2E) trained speech models have
demonstrated state-of-the-art performance on Au-
tomatic speech recognition (ASR) tasks. Several
methods (Xia et al., 2017; Guo et al., 2019; Hu
et al., 2021b; Yang et al., 2021a; Salazar et al.,
2020) have widely adopted a two-pass rescoring
paradigm to leverage upon language models to fur-
ther enhance the capabilities of these models. In the
two-pass paradigm, the first pass ASR system ‚Äúgen-
erates‚Äù n-best hypotheses using an E2E acoustic
model, while the second pass ‚Äúre-ranks‚Äù these hy-
potheses by incorporating a language model (LM).
This two-pass reranking approach has been sev-
eral notable advantages over single-pass End-to-
End (E2E) ASR systems (Amodei et al., 2016;
Chan et al., 2016). Firstly, the subsequent large
language model often captures a more comprehen-
sive understanding (Stooke et al., 2023; Tur andDe Mori, 2011) of language structures beyond the
knowledge of transcribed audio present in the ASR
model‚Äôs pre-training data, thereby improving per-
formance on unseen words. Furthermore, adapting
the two-pass paradigm to accommodate domain
shifts (Li et al., 2023; Liu et al., 2021; Yu et al.,
2023) is much easier as only the language model
needs to be fine-tuned on the new dataset. This
alleviates the need for a spoken transcription cor-
pus, which can be particularly beneficial for under-
resourced or endangered spoken languages.
The recent emergence of conversational abilities
in large language models, such as ChatGPT (Ope-
nAI, 2023a) and GPT-4 (OpenAI, 2023b), has fur-
ther sparked interest in leveraging representational
power of large pre-trained models for more com-
plex tasks involving diverse data modalities (Yang
et al., 2021b; Chang et al., 2023). Moreover, this
new research direction also introduces a set of
unique challenges related to considering informa-
tion from other input modalities, such as acoustic
and visual conditions (Peng et al., 2023; Zhang
et al., 2023), in which could enrich using context
beyond text-only input.
Recognizing speech signals is a task that neces-
sitates both acoustic information (Hu et al., 2021a;
Hung et al., 2023) (e.g., speaking environments)
and linguistic information (Meng et al., 2023; Chen
et al., 2023b,c) (e.g., context and domains). Effi-
ciently amalgamating or integrating representation
learning from acoustic modeling into the language
modeling to bolster its performance represents a
notably intricate research domain that warrants fur-
ther exploration. In this paper, we present a token-
level fusion framework, merging two foundation
(large-scale pre-trained) models into a recognition
error correction paradigm, with the objective of
enhancing the performance of ASR systems.arXiv:2310.06434v2  [cs.CL]  16 Oct 2023

--- PAGE 2 ---
Learnable Matrix QLLaMA 
KLLaMA 
VLLaMA KWhisper 
VWhisper Padding 
Mdown MupŒªLŒªWAi
W  (AdapterWhisper)
Layer Output
Frozen Trainable Self-AttentionLLaMA 
SALSelf-AttentionWhisper 
SAWLanguage 
Embeddings Audio 
Features SiLU 
MùúÉ
üî•
Figure 1: Illustration of proposed generative ASR error correction with a trainable token ( ML) and fusion mechanism
inside a self-attention layer described in Section 3.2. A detailed model-wise illustration is discussed in Fig 2.
2 Related Work on ASR Post-processing
Transformer-based language models (Shin et al.,
2019; Salazar et al., 2020) approach the two-pass
paradigm by utilizing the summation of negative
log-likelihoods of individual tokens from the lan-
guage model to re-score the n-best output. Recent
works on deliberation method (Hu et al., 2020;
Prabhavalkar et al., 2018) and audio-attention
based rescoring (Futami et al., 2021; Gandhe and
Rastrow, 2020; Tanaka et al., 2021) in improv-
ing ASR-LM rescoring with the incorporation of
acoustic features. Recent works on decoder prompt-
ing (Yang et al., 2023a) and encoder-decoder based
error correction (Chen et al., 2023a; Ma et al., 2023)
have demonstrated benefits in using an external lan-
guage model for reducing the transcription error
rate. Meanwhile, how to inject or fuse represen-
tations from a large acoustic model into another
language model remains under investigation.
3 Method
We discuss the model architecture and the intuition
behind proposed feature combination in Section
3.1. The cross modal fusion mechanism and weight
initialization are explained in Section 3.2 and Sec-
tion 3.3, respectively.
3.1 Generative Error Correction for ASR
Our approach combines two pre-trained models,
Whisper (Radford et al., 2022) and LLaMA (Tou-
vron et al., 2023), to facilitate generative error cor-
rection (Yang et al., 2023a; Chen et al., 2023a).
Firstly, we employ Whisper, a multi-task encoder-
decoder-based transformer (Vaswani et al., 2017)
speech model trained on 680,000 hours of multi-
lingual data, to encode audio representations and
generate transcripts of n-best hypotheses. Sec-
ondly, we utilize LLaMA, a decoder-based largelanguage transformer model to generate error-
corrected transcripts by utilizing the n-best hy-
potheses via prompt (illustrated in Appendix, Fig 5)
and audio representations via our proposed frame-
work as input.
Whisper utilizes the encoder of a Transformer
model to derive features from audio input, which
are then fed into the decoder through multi-headed
cross-attention, enabling auto-regressive text token
prediction (Wang et al., 2023; Irie et al., 2022). The
encoded features provide information from audio
input via cross-attention, while the decoder‚Äôs self-
attention attends previous tokens using a key-value
caching mechanism.
We fuse the audio features and the Whisper lin-
ear layers that generate the key and value pairs
in the decoder‚Äôs cross-attention mechanism to the
LLaMa model to inject audio information. The in-
herent self-attention modules in LLaMA combined
with the added cross-attention module make it anal-
ogous to the Whisper decoder. An overview of the
proposed method is presented in Appendix, Fig. 2.
3.2 Cross-Modal Fusion Mechanism
We introduce our mechanism in Fig 1. To efficient
fine-tune large models, we incorporate two resid-
ual adapter (Houlsby et al., 2019; Radhakrishnan
et al., 2023; Chen et al., 2023d; Yang et al., 2023b)
modules ( Ai
LandAi
W) after the self-attention mod-
ules ( SAi
L) of the frozen LLaMA model at each
layer. The first variable Ai
Lrepresents the adapter
in layer iused to fine-tune the LLaMA model using
a scaled dot product attention mechanism. The sec-
ond variable Ai
Wrefers to another adapter in layer
iused to fuse Whisper features with the LLaMA
model by following an autoencoder mechanism.
In each Ai
L, we incorporate a learnable matrix
Mi
Œ∏‚ààRNŒ∏√óNL.NŒ∏denotes the dimensionality
of the adapter embeddings, while NLindicates

--- PAGE 3 ---
the dimensionality of LLaMA embeddings. The
language embedding feature extracted from the pre-
trained LLM is represented by Hi
Lfor each layer.
We repurpose the frozen LLaMA linear layers
Ki
llamaandLi
llamafrom the LLaMA self-attention
SAi
Lto transform Mi
Œ∏into key and value pairs,
thus reducing the number of trainable parameters.
We also reuse the query tensor from the frozen
LLaMA self-attention module SAi
Lto compute Ai
L,
as shown below; Srepresents the Softmax:
S 
Qi
llama(Hi
L)¬∑Ki
llama 
Mi
Œ∏T
‚àödk!
Vi
llama 
Mi
Œ∏
(1)
To integrate the audio representations and key-
value tensors from the Whisper decoder cross-
attention module into the LLaMA model, we in-
troduce two additional linear frozen transforma-
tions ( Ki
whisperandVi
whisper) at each layer of the
LLaMA model. These transformations are initial-
ized with the respective weights from the cross-
attention module of the Whisper decoder. By ap-
plying the audio representations to these additional
linear transformations, we generate the key-value
pairs that mirror the ones produced by Whisper.
We then utilize the second adapter module Ai
W, to
add trainable components to learn cross-modal rep-
resentation. We apply a learnable projection matrix
Mi
down‚ààRNW√óNW
rto down project the obtained
key and value pairs. Where NWdenote the size of
the Whisper encoded audio representations ( x). We
then apply the SiLU activation function (Elfwing
et al., 2018) followed by a learnable up-projection
Mi
up‚ààRNW
r√óNW, to compute trainable output:
Ai
W(x)‚ÜêSiLU 
x¬∑Mi
down
Mi
up. (2)
Using this setup, we transform the key-value pair
at each layer, merging the hidden representation
(Haudio) from the output of the Whisper frozen pre-
trained encoder with decoder from LLaMA:
ÀÜKi
whisper ‚ÜêAi
W(Ki
whisper (Haudio)); (3)
ÀÜVi
whisper ‚ÜêAi
W 
Vi
whisper (Haudio
).(4)
Once we obtain the corresponding Whisper key
and value pairs, we apply the padding mechanism
described in 3.3 to adjust the shape of ÀÜKi
whisperand
ÀÜVi
whisperto match Qi
llamato enable computation
of Multi Head Attention (MHA) and to preserve
the latent structure of the Whisper Key and ValueTable 1: Dataset sample statistics are provided with
alias names. The Science & Technology category of
GigaSpeech (Chen et al., 2021) is divided into two sub-
sets: GS SS(small) and GS SM(medium), to evaluate
performance differences with respect to data size.
Dataset Train Test
ATIS (Hemphill et al., 1990) 4978 893
GigaSpeech: Entertainment (GS E)4709 1000
People & Blogs (GS P) 6802 1000
Science & Technology (GS SS) 6908 1000
Science & Technology (GS SM) 10323 1000
embeddings. We merge it with the LLaMA model
by computing MHA with the query ( Qi
llama) from
the frozen LLaMA model as before to obtain its
adaptable self-attention head (SAi
W) as:
SÔ£´
Ô£¨Ô£≠Qi
llama(Hi
L)¬∑
ÀÜKi
whisperT
‚àödkÔ£∂
Ô£∑Ô£∏ÀÜVi
whisper (5)
Then, we utilize a gated fusion mechanism,
Whispering- LLaMA ( WL), to fuse all the mod-
ules together as shown below:
SAi
WL‚ÜêSAi
L+ŒªL¬∑Ai
L+ŒªW¬∑SAi
W,(6)
where ŒªLandŒªWare learnable scalars.
3.3 Weight Initialization
The latent dimensions of the Whisper and LLaMA
models are different, making it necessary to re-
shape the Whisper tensors to match the shape of
the LLaMA model while preserving the latent
structure and information inherent in the Whis-
per model. Tensors are shaped in the format of
[B, NH, T, HS ]which denotes the Batch size,
Number of heads, context length and Head Size,
respectively. The last two dimensions undergo
transformation during the attention mechanism.
Hence in order to preserve the Whisper latent
structure, We initialize a matrix of zeros of shape
‚ààRNHllama√óTwhisper √óHSllama and fill the principal
diagonal of the last two dimensions with ones. We
then place KiandVion the top left corner of the
padding template. We further initialize the projec-
tion matrices Mi
down, Mi
upon the second adapter
module Ai
Was identity matrices. The proposed
framework encounters significant losses and fails
to converge unless this initialization strategy is fol-
lowed to preserve Whisper‚Äôs latent representations.

--- PAGE 4 ---
Table 2: The experimental results are presented in terms of WER without text normalization. The performance of
our proposed framework is reported in rows 2‚àí4. Oracle refers to the candidate among the n-best hypothesis with
the lowest word error rate compared to the ground truth. Rows 5‚àí9represent different ablation experiments on the
best-performing model, WLM. The WERR is measured relative to the oracle performance as shown in B.2
#Method #Para. ATIS GSE GSP GSS GSM WER Avg(‚Üì)WERR ( ‚Üë)
1Oracle (1st-pass) - 13.76 28.22 22.84 23.93 19.5 21.64 -
2WL L 26.40M 2.04 21.76 19.21 20.55 11.6 15.03 30.52
3WL M 7.97M 1.77 21.61 16.20 18.02 9.82 13.48 37.66
4WL S 4.89M 1.89 22.24 17.23 19.157 10.185 14.144 34.62
5WL Mw/o masking 4.89M 3.94 27.56 18.10 21.71 12.79 20.04 22.25
6WL Mw/oHacoustics 4.89M 253.20 123.19 203.44 376.81 256.44 242.61 -1020.68
7WL Mw/o init. 4.89M 405.83 500.58 414.34 461.63 390.64 434.60 -1907.45
8WL Mw/o SA W 1.22M 1.66 24.99 18.734 20.73 10.86 15.39 28.83
9Big-scale Adapter 4.91M 1.45 23.65 16.59 19.93 10.62 14.45 33.21
4 Experimental Setup
4.1 Models
For our experiments, we utilize the LLaMA-7B
model architecture. As we instruct the language
model with the generated hypotheses (as illustrated
in Figure 4.3.1) to perform generative error cor-
rection, we initialize our model weights with Al-
paca (Taori et al., 2023), a model fine-tuned from
LLaMa-7B, utilizing 52,000 instruction-following
demonstrations to enable instruction following abil-
ities. To extract audio representations from input
audio clips, we employ Whisper-Large V2, a model
with 1.55B parameters trained on 620,000 hours
of audio data. Additionally, we employ Whisper-
Tiny, a model with 70M parameters, for generating
our transcripts, as described in the subsequent sec-
tion 4.2. We name our model Whispering LLaMA
(WL) and train three variants with our proposed
framework with NŒ∏= 10 andr= 8,16,32named
WL L(large), WL M(medium), WL S(small), re-
spectively. We design WL Lwith two separate AW
adapters modules for key and value, respectively.
WL MandWL Suse the same AWadapter in sec-
tion 3.2 to reduce trainable parameters.
4.2 Dataset
We curate our own transcripts by leveraging
two datasets: the Airline Travel Information
System (Hemphill et al., 1990) (ATIS) and Gi-
gaSpeech (Chen et al., 2021). ATIS consists of
audio recordings of individuals querying flight in-
formation. GigaSpeech, contains audio from audio-
books, podcasts and YouTube videos on diverse
topics. ATIS represents a semantically correct,
domain-specific dataset, while GigaSpeech repre-
sents a more noisy, real-world setting in our eval-uation. We select domain-specific subsets in Gi-
gaSpeech and focus on three specific categories:
Entertainment, People and Blogs, and Science and
Technology. To explore performance variations
with respect to number of data points, we further
divide the Science and Technology category into
two subsets. Table 1 provides detailed information
on the number of training points per dataset. We
chose Whisper-Tiny to generate the n-best hypoth-
esis baseline to establish a robust evaluation envi-
ronment that aligns more closely with real-world
settings dealing with sub-optimal hypotheses. By
employing Whisper-Tiny, we mimic a weak acous-
tic model with lower-quality hypotheses. Feeding
LMs with better-quality hypotheses from Whisper-
Large would make the generative error correction
task less challenging for LM adaptation and does
not explore the model‚Äôs performance under practi-
cal settings where our method is intended to be em-
ployed. However, we emphasize that our method
remains effective when starting with a Whisper-
Large hypothesis in Appendix E.
For each audio clip, we generate 200hypotheses
using a top-k value of 200and a randomly selected
temperature between the range of [0.7,0.8]. Sub-
sequently, we filter out redundant sentences and
select its top-15 with the highest log probability.
4.3 Training Pipeline
The input to our model consists of the encoded
audio representations extracted from the Whisper-
Large model, accompanied by the 15-best tran-
scripts generated by Whisper-Tiny. We employ
the prompt template used by the Alpaca model as
shown in Appendix Fig 5. We utilize the Adam
optimizer (Kingma and Ba, 2014) and experiment
with learning rates of 1√ó10‚àí2,1√ó10‚àí3, and

--- PAGE 5 ---
5√ó10‚àí4, selecting the optimal value. The model
is trained for 25 epochs, employing early stopping
to prevent overfitting. Training is conducted on two
Nvidia A100 GPUs to leverage efficient parallel
processing. An effective batch size of 32 is used,
and a weight decay of 1√ó10‚àí2is applied.
4.3.1 LLM Prompting Examples for ASR
We employ the Alpaca (Taori et al., 2023) prompt
template, as illustrated in Fig. 5 of the Appendix,
to generate the n-best hypotheses. This template
features an instructional segment designated by
theInstruction tag, which offers guidance to the
model. Essential contextual data required by the
model is housed under the Input tag. The prompt
concludes with the Response tag, directing the
model to enact the specified instruction within the
supplied input context. Rather than adopting the re-
cent advances of Task-Activating Prompting (Yang
et al., 2023a) (TAP), we opt to feed the LLM with
its task-specific data (e.g., speech recognition in
our instance). Our alternative approach facilitates
second-pass error correction, mitigating the latency
issues observed in the extensive context windows
of the TAP based generative ASR error correction.
4.4 Performance Studies
Results from our experiments have been reported
in Table 2. The WL Mmodel achieves the best per-
formance with a word-error-rate relative (WERR)
of37.66%, as defined in B.2. A comparison be-
tweenWL LandWL Mindicates that having sepa-
rate adapter modules for key and value pairs does
NOT result in performance improvements. Further
dataset-specific analyses are detailed in Appendix
B. The models exhibit better performance on the
Gigaspeech with more in-domain data.
4.5 Ablation Studies
We empirically discover that masking the prompt
except for the ground truth in the cross entropy
loss function significantly improves the perfor-
mance. We attribute this improvement to the
model‚Äôs enhanced capacity to grasp accurate se-
mantics, achieved by refraining from penalizing
the model for erroneous sentences found in the n-
best hypotheses. Row 5represents the performance
ofWL Mwithout masking. We further investigate
if the proposed framework is utilizing the audio
representations from Whisper by substituting them
with random tensors generated from a normal dis-
tribution as the input (Row 6). Additionally, weexplore the significance of our weight initialization
mechanism by replacing it with random initializa-
tion (Row 7). Both of these ablation studies vali-
date our intuition, demonstrating that the method
utilizes acoustic features effectively and highlight
the importance of the initialization mechanism in
preserving the latent structure of the acoustic em-
beddings. For further insights, please refer to Ap-
pendix D. We also remove the Whisper adapter
(SAW) module for an text feature only baseline
performance using adapters (Row 8). Since the dis-
parity between the number of trainable parameters
is high, we train another model with an increased
adapter context dimension of N‚Ä≤
Œ∏= 4NŒ∏(Row 9).
5 Conclusion
We propose a novel framework to leverage the ex-
ternal knowledge from LLM to improve the tran-
scription accuracy of ASR systems. Our frame-
work presents a parameter-efficient way to integrate
large foundational Speech and Language models
to achieve competitive WERR improvements. We
further conduct extensive ablation experiments to
validate our intuitions and open source our code
and pretrained-weights to the research community.
6 Limitation
Using large models such as LLaMA is intuitive,
as it provides a comprehensive comprehension of
language structure owing to its internet-scaled pre-
training. However, deploying these systems and
conducting research with them in real-world sce-
narios is challenging due to their computationally
intensive nature. In our approach, we aim to de-
sign our framework to be parameter-efficient by
re-using multiple model components with adapters
for model fusion. Nonetheless, incorporating au-
dio representations into the training pipeline ex-
tends the training duration by 394.76%. This un-
derscores the significance of alignment issues (Yen
et al., 2023). Furthermore, our proposed solution
demonstrates a need for a larger volume of data
to achieve optimal performance, despite having a
modest parameter count of only 7.97M to integrate
foundational models. During our experimentation,
we encountered issues related to over-fitting on
datasets. To mitigate this problem, we trained with
a reduced learning rate and monitored the Word Er-
ror Rate (WER) performance throughout the train-
ing process and selected the model checkpoint with
the best performance to implement early stopping.

--- PAGE 6 ---
References
Dario Amodei, Sundaram Ananthanarayanan, Rishita
Anubhai, Jingliang Bai, Eric Battenberg, Carl Case,
Jared Casper, Bryan Catanzaro, et al. 2016. Deep
speech 2: End-to-end speech recognition in english
and mandarin. In International conference on ma-
chine learning , pages 173‚Äì182. PMLR.
William Chan, Navdeep Jaitly, Quoc Le, and Oriol
Vinyals. 2016. Listen, attend and spell: A neural
network for large vocabulary conversational speech
recognition. In Proc. ICASSP , pages 4960‚Äì4964.
IEEE.
Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing
Kang, Wei-Cheng Tseng, Shang-Wen Li, and Hung-
yi Lee. 2023. Speechprompt v2: Prompt tun-
ing for speech classification tasks. arXiv preprint
arXiv:2303.00733 .
Chen Chen, Yuchen Hu, Chao-Han Huck Yang,
Sabato Macro Siniscalchi, Pin-Yu Chen, and
Eng Siong Chng. 2023a. Hyporadise: An open base-
line for generative speech recognition with large lan-
guage models. arXiv preprint arXiv:2309.15701 .
Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu
Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel
Povey, Jan Trmal, Junbo Zhang, et al. 2021. Gi-
gaspeech: An evolving, multi-domain asr corpus
with 10,000 hours of transcribed audio. In Proc.
Interspeech 2021 .
Zih-Ching Chen, Chin-Lun Fu, Chih-Ying Liu, Shang-
Wen Daniel Li, and Hung-yi Lee. 2023b. Exploring
efficient-tuning methods in self-supervised speech
models. In Proc. SLT , pages 1120‚Äì1127. IEEE.
Zih-Ching Chen, Yu-Shun Sung, and Hung-yi Lee.
2023c. Chapter: Exploiting convolutional neural
network adapters for self-supervised speech models.
InProc ICASSP Workshop , pages 1‚Äì5. IEEE.
Zih-Ching Chen, Chao-Han Huck Yang, Bo Li,
Yu Zhang, Nanxin Chen, et al. 2023d. How to es-
timate model transferability of pre-trained speech
models? In Proc. Interspeech 2023 .
Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018.
Sigmoid-weighted linear units for neural network
function approximation in reinforcement learning.
Neural Networks , 107:3‚Äì11.
Hayato Futami, Hirofumi Inaguma, Masato Mimura,
Shinsuke Sakai, and Tatsuya Kawahara. 2021. Asr
rescoring and confidence estimation with electra. In
Proc. ASRU , pages 380‚Äì387. IEEE.
Ankur Gandhe and Ariya Rastrow. 2020. Audio-
attention discriminative language model for asr
rescoring. In Proc. ICASSP , pages 7944‚Äì7948. IEEE.
Jinxi Guo, Tara N Sainath, and Ron J Weiss. 2019.
A spelling correction model for end-to-end speech
recognition. In Proc. ICASSP , pages 5651‚Äì5655.
IEEE.Charles T Hemphill, John J Godfrey, and George R
Doddington. 1990. The atis spoken language sys-
tems pilot corpus. In Speech and Natural Language:
Proceedings of a Workshop Held at Hidden Valley,
Pennsylvania, June 24-27, 1990 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790‚Äì2799. PMLR.
Hu Hu, Xuesong Yang, et al. 2021a. Redat: Accent-
invariant representation for end-to-end asr by domain
adversarial training with relabeling. In Proc. ICASSP ,
pages 6408‚Äì6412. IEEE.
Ke Hu, Ruoming Pang, Tara N Sainath, and Trevor
Strohman. 2021b. Transformer based deliberation
for two-pass speech recognition. In Proc. SLT , pages
68‚Äì74. IEEE.
Ke Hu, Tara N Sainath, Ruoming Pang, and Rohit Prab-
havalkar. 2020. Deliberation model based two-pass
end-to-end speech recognition. In Proc. ICASSP ,
pages 7799‚Äì7803. IEEE.
Yun-Ning Hung, Chao-Han Huck Yang, Pin-Yu Chen,
and Alexander Lerch. 2023. Low-resource music
genre classification with cross-modal neural model
reprogramming. In Proc. ICASSP , pages 1‚Äì5. IEEE.
Kazuki Irie, R√≥bert Csord√°s, and J√ºrgen Schmidhu-
ber. 2022. The dual form of neural networks re-
visited: Connecting test time predictions to training
patterns via spotlights of attention. In International
Conference on Machine Learning , pages 9639‚Äì9659.
PMLR.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Ke Li, Jay Mahadeokar, Jinxi Guo, Yangyang Shi, Gil
Keren, Ozlem Kalinli, Michael L Seltzer, and Duc Le.
2023. Improving fast-slow encoder based transducer
with streaming deliberation. In Proc. ICASSP , pages
1‚Äì5. IEEE.
Linda Liu, Yile Gu, Aditya Gourav, Ankur Gandhe,
Shashank Kalmane, Denis Filimonov, Ariya Ras-
trow, and Ivan Bulyko. 2021. Domain-aware neural
language models for speech recognition. In Proc.
ICASSP , pages 7373‚Äì7377. IEEE.
Rao Ma, Mark JF Gales, Kate Knill, and Mengjie Qian.
2023. N-best t5: Robust asr error correction using
multiple input hypotheses and constrained decoding
space. Proc. Interspeech .
Zhong Meng, Weiran Wang, Rohit Prabhavalkar, et al.
2023. Jeit: Joint end-to-end model and internal lan-
guage model training for speech recognition. In Proc.
ICASSP , pages 1‚Äì5. IEEE.

--- PAGE 7 ---
OpenAI. 2023a. Chatgpt. https://openai.com/
blog/chatgpt/ .
OpenAI. 2023b. Gpt-4 technical report.
Puyuan Peng, Brian Yan, Shinji Watanabe, and David
Harwath. 2023. Prompting the hidden talent of web-
scale speech models for zero-shot task generalization.
arXiv preprint arXiv:2305.11095 .
Rohit Prabhavalkar, Tara N Sainath, Yonghui Wu,
Patrick Nguyen, Zhifeng Chen, Chung-Cheng Chiu,
and Anjuli Kannan. 2018. Minimum word error rate
training for attention-based sequence-to-sequence
models. In Proc. ICASSP , pages 4839‚Äì4843. IEEE.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
man, Christine McLeavey, and Ilya Sutskever. 2022.
Robust speech recognition via large-scale weak su-
pervision. arXiv preprint arXiv:2212.04356 .
Srijith Radhakrishnan, Chao-Han Huck Yang, et al.
2023. A parameter-efficient learning approach to
arabic dialect identification with pre-trained general-
purpose speech model. In Proc. Interspeech 2023 ,
pages 1958‚Äì1962.
Julian Salazar, Davis Liang, Toan Q Nguyen, and Ka-
trin Kirchhoff. 2020. Masked language model scor-
ing. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
2699‚Äì2712.
Joonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.
Effective sentence scoring method using bert for
speech recognition. In Asian Conference on Machine
Learning , pages 1081‚Äì1093. PMLR.
Adam Stooke, Khe Chai Sim, Mason Chua, Tsendsuren
Munkhdalai, and Trevor Strohman. 2023. Internal
language model personalization of e2e automatic
speech recognition using random encoder features.
InProc. SLT , pages 213‚Äì220. IEEE.
Tomohiro Tanaka, Ryo Masumura, Mana Ihori, Akihiko
Takashima, Takafumi Moriya, Takanori Ashihara,
Shota Orihashi, and Naoki Makishima. 2021. Cross-
modal transformer-based neural correction models
for automatic speech recognition. In Proc. Inter-
speech 2021 . ISCA.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Gokhan Tur and Renato De Mori. 2011. Spoken lan-
guage understanding: Systems for extracting seman-
tic information from speech . John Wiley & Sons.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Siyin Wang, Chao-Han Huck Yang, Ji Wu, and Chao
Zhang. 2023. Can whisper perform speech-based in-
context learning. arXiv preprint arXiv:2309.07081 .
Shinji Watanabe, Takaaki Hori, et al. 2018. Espnet: End-
to-end speech processing toolkit. Proc. Interspeech
2018 .
Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,
Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation
networks: Sequence generation beyond one-pass de-
coding. Advances in neural information processing
systems , 30.
Chao-Han Huck Yang, Yile Gu, Yi-Chieh Liu, Shalini
Ghosh, Ivan Bulyko, and Andreas Stolcke. 2023a.
Generative speech recognition error correction with
large language models and task-activating prompting.
IEEE Proc. ASRU 2023 .
Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen,
Rohit Prabhavalkar, Tara N Sainath, and Trevor
Strohman. 2023b. From english to more languages:
Parameter-efficient model reprogramming for cross-
lingual speech recognition. In Proc. ICASSP , pages
1‚Äì5. IEEE.
Chao-Han Huck Yang, Linda Liu, Ankur Gandhe, Yile
Gu, Anirudh Raju, Denis Filimonov, and Ivan Bulyko.
2021a. Multi-task language modeling for improving
speech recognition of rare words. In Proc. ASRU ,
pages 1087‚Äì1093. IEEE.
Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen.
2021b. V oice2series: Reprogramming acoustic mod-
els for time series classification. In International
Conference on Machine Learning , pages 11808‚Äì
11819. PMLR.
Hao Yen, Pin-Jui Ku, Chao-Han Huck Yang, Hu Hu,
Sabato Marco Siniscalchi, Pin-Yu Chen, and Yu Tsao.
2023. Neural model reprogramming with similarity
based mapping for low-resource spoken command
classification. Proc. Interspeech 2023 .
Yu Yu, Chao-Han Huck Yang, Jari Kolehmainen, et al.
2023. Low-rank adaptation of large language model
rescoring for parameter-efficient speech recognition.
IEEE Proc. ASRU 2023 .
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,
George Karypis, and Alex Smola. 2023. Multi-
modal chain-of-thought reasoning in language mod-
els.arXiv preprint arXiv:2302.00923 .

--- PAGE 8 ---
A Appendix
In this Appendix, We investigate the performance
difference between datasets in Section B, provide
illustrations of the model-level architectural design
and prompt template in Section C and 4.3.1 respec-
tively. Provide more insight into the results from
ablation studies in Section D and Whisper Large
Hypothesis baseline in Section E.
B Dataset Analysis
We report the WER from our experiments before
and after text normalization on Table 3. We con-
vert the model prediction and the ground-truth to
lower-case and remove punctuation during text nor-
malization. The ATIS dataset is not impacted by
text normalization because the dataset does not con-
tain any punctuation. It only contains contractions
such as ‚ÄúI‚Äôd like‚Äù instead of ‚ÄúI would like‚Äù . ATIS
consists of audio recordings of individuals query-
ing automated airline travel inquiry systems for
flight information. We believe the lack of punctua-
tion and the consistent structure present within the
ATIS dataset enables improved WER performance
compared to GigaSpeech. The Gigaspeech dataset
contains punctuation and lacks consistency within
the dataset because it has diverse categories and
sources such as audiobooks, podcasts and YouTube
videos.
B.1 More Discussion on Ground Truth Match
Rate
During dataset generation, we remove the ground
truth if it is present among the Whisper generated n-
best hypothesis. This allows us to introduce a new
metric called Ground Truth Match Rate (GTMR).
GTMR calculates the percentage of predictions
generated by the model that exactly match the
ground-truth. This metric indicates the model‚Äôs to
learn the structure of the data source. The GTMR
of our experiments before and after text normaliza-
tion is reported in Table 5. The model is able to
learn the structure of the dataset better with more
data points as observed from the performance dif-
ference between ST-S and ST-M. It can also be
observed that the model is able to learn the sim-
pler structure of ATIS much better than other Gi-
gaSpecch datasets.B.2 WERR
Word error rate relative is calculated as
WERR (i)‚ÜêOracle (i)‚àíWER (i)
Oracel (i)√ó100 (7)
where Oracle (i)refers to the average oracle per-
formance in terms of WER and WER (i)refers to
the average performance of a particular method.
C Proposed Architecture Illustrations
We present a model-level overview of our proposed
method described in Section 3.2 in Fig 2. We add
two modules into each layer of LLaMa. The Whis-
per Cross-Attention module and Adapter module
which refer to AWandAL, respectively. We ini-
tialize the Whisper Cross-Attention module with
the Whisper decoder model, as illustrated. LLaMa
takes the encoded features generated by Whisper
encoder and the n-best hypothesis generated by the
Whisper in a prompt format as input to generate
the error-corrected response.
D Failure Case Studies of Generative
ASR with Whispering-LLaMa
Since the WER error rate in row 6 ( WL Mw/o
audio representations ) and row 7 ( WL Mw/o ini-
tialization) of table 2 is high and provides no insight
into model performance, we present the training
loss graphs of the best performing model ( WL M)
with and without audio representations in Figure 4.
The model is not able to converge below a certain
threshold without audio representations. Addition-
ally, we include the training loss graphs of WL M
with and without our initialization mechanism in
Figure 3. Without our initialization mechanism, the
latent structure of the Whisper encoder embedding
is not preserved, leading to an inability to converge.
E Whisper Large Decoding Baseline
We report the results of using the hypothesis
generated by Whisper Large to train our best-
performing model ( WL M) on GigaSpeech En-
tertainment ( GSE) and Science and Technology
(GSE) datasets on Table 4. By leveraging the
LLaMA model with the proposed generative er-
ror correction mechanism, we are able to match
the performance of the Whisper Large model with
1.5 billion parameters by using a Whisper -Tiny
model with just 70 million parameters. Using the
hypotheses generated by Whisper Large results in

--- PAGE 9 ---
Whisper 
Self-Attention Whisper 
MLP 
Whisper 
Self-Attention Whisper 
MLP ‚Ä¶
Whisper 
Self-Attention Whisper 
MLP 
Whisper 
Cross-Attention 
Whisper 
Self-Attention Whisper 
MLP 
Whisper 
Cross-Attention 
‚Ä¶
Input Audio Clip Whisper 
Encoder Whisper 
Decoder Audio 
Representations 
N-Best 
Hypotheses 
Hello, How are you? 
hello how how you 
Hello How were you 
Hell are you? LLaMA 
Self-Attention LLaMA 
MLP 
Fusion 
Adapter (A W)
LLaMA 
Self-Attention LLaMA 
MLP 
Fusion 
Adapter (A W)‚Ä¶LLaMA 
Decoder 
LLaMA 
Adapter (A L)
LLaMA 
Adapter (A L)Generative Speech 
Transcription 
Prompt 
Inputs Figure 2: Whispering-LLaMa model-overview of proposed adaptation pipelines described in Section 3.2
0 2500 5000 7500 10000 12500 15000 17500
Steps024681012Train lossTrain loss of WL-M (Row 3) vs WL-M without initialization (Row 7) on the Entertainment dataset
WL-M
WL-M without initialization
Figure 3: Train loss of WLM(Row 3) vs WLMwithout
initialization (Row 7) on the Entertainment dataset
0 2500 5000 7500 10000 12500 15000 17500
Steps0.000.250.500.751.001.251.501.752.00Train lossTrain loss of WL-M (Row 3) vs WL-M without audio features (Row 6) on the Entertainment dataset
WL-M
WL-M without audio features
Figure 4: Train loss of WLM(Row 3) vs WLMwithout
audio representations (Row 6) on the Entertainment
dataset
a higher WERR as expected. This finding confirms
the effectiveness of the proposed approach, partic-
ularly in the context of Whisper Large generated
N-best hypotheses.Table 3: The experimental results in terms of Word Er-
ror Rate (WER), Before and after text normalization.
We convert all text to lowercase and remove the follow-
ing punctuation [".", "-", "?", "‚Äô"]. Rows 1-3 represent
before text normalization and Rows 4-6 represent after
text normalization
#Method ATIS ET P&B ST-S ST-M Avg
1WL L 2.04 21.76 19.21 20.55 11.6 15.03
2WL M 1.77 21.61 16.20 18.02 9.82 13.48
3WL S 2.11 23.60 17.13 20.16 10.73 14.75
4WL L 2.04 14.71 13.36 14.23 7.44 10.35
5WL M 1.77 17.71 10.83 12.00 6.07 9.67
6WL S 1.89 15.21 11.49 13.03 6.41 9.61
F Reproducibility Resources
We have open-sourced the pre-trained model
weights and code, available at https://github.
com/Srijith-rkr/Whispering-LLaMA . Our fu-
ture plan includes integrating this baseline into
both Espnet (Watanabe et al., 2018) and HyPo-
radise (Yang et al., 2023a; Chen et al., 2023a) to
accommodate a broader range of use cases.
Acknowledgement
The authors thank Maxim Lvov and anonymous
reviewers for their feedback on the draft.

--- PAGE 10 ---
### Instruction: 
You are an ASR transcript selector. You have a few transcripts generated by an automatic  speech  
recognition model. Your task is to generate the most likely transcript from them. If the generated  
transcripts have grammatical or logical errors, you will modify them accordingly to produce the  
most accurate and coherent transcript. 
### Input: 
so that it can carry its momentum to the logic 
that he can carry his moment on tour with the logic 
that he can carry his momentum through with the logic 
that he can carry his momentum to with an logic 
that he can carry his momentum true within logic 
that he carries momentum through with logic 
that it can carry a moment and through with the logic 
that it can carry a moment of truth with the logic 
that it can carry a moment on tour with the logic 
that it can carry a moment on true with the logic 
that it can carry a momentum tool within logic 
that it can carry as a moment on tour with the logic 
that it can carry at moments and through with the logic 
that it can carry his moment on tour with a logic 
that it can carry his moment on tour with the logic 
### Response: 
Model Input 
that it can carry its momentum through with a logic. Model Output Figure 5: Illustration of the Alpaca prompt template used in our proposed framework
Table 4: Results of employing a Whisper Large generated hypothesise baseline in comparison to the proposed
Whisper Tiny hypothesis baseline
Method GSEGSSWER Avg‚ÜìWERR ‚Üë
Oracle with Whisper Tiny 28.22 23.93 26.08 -
WL Mwith Whisper Tiny 21.61 18.02 19.82 24.01
Oracle with Whisper Large 21.78 18.09 19.93 -
WL Mwith Whisper Large 15.59 12.77 14.18 28.86
Table 5: The experimental results in terms of Ground Truth Match Rate (GTMR), Before and after text normalization.
Rows 1-3 represent before text normalization, and Rows 5-7 represent after text normalization. Row 4 and 6 denote
the average GTMR across datasets and columns Avg denotes average GTMR across each method
#Method ATIS ET P&B ST-S ST-M Avg
1WL L 86.1 26.5 24.1 24.8 36.2 39.5
2WL M 88.3 26.3 31.4 28.8 41.0 43.17
3WL S 87.5 25.2 27.6 26.6 38.6.1 41.10
4 Avg 87.3 26.0 27.7 26.7 38.6 -
5WL L 86.1 46.9 46 45.5 56.4 56.2
6WL M 88.3 47.0 54.3 49.2 62.7 60.3
7WL S 87.5 44.3 49.5 46.3 60.0 57.5
8 Avg 87.3 46.0 49.9 47.0 59.7 -
