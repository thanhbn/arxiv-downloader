# 2304.03464.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2304.03464.pdf
# File size: 4577186 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Record Linkage with Multimodal Contrastive Learning
Abhishek Arora1, Xinmei Yang1, Shao-Yu Jheng1, Melissa Dell1,2∗
1Harvard University; Cambridge, MA, USA.
2National Bureau of Economic Research; Cambridge, MA, USA.
∗Corresponding author: melissadell@fas.harvard.edu.
Abstract
Many applications require linking individu-
als, firms, or locations across datasets. Most
widely used methods, especially in social
science, do not employ deep learning, with
record linkage commonly approached using
string matching techniques. Moreover, ex-
isting methods do not exploit the inherently
multimodal nature of documents. In histori-
cal record linkage applications, documents are
typically noisily transcribed by optical char-
acter recognition (OCR). Linkage with just
OCR’ed texts may fail due to noise, whereas
linkage with just image crops may also fail be-
cause vision models lack language understand-
ing ( e.g., of abbreviations or other different
ways of writing firm names). To leverage multi-
modal learning, this study develops CLIPPINGS
(Contrastively LInking Pooled Pre-trained Em-
beddings).CLIPPINGS aligns symmetric vision
and language bi-encoders, through contrastive
language-image pre-training on document im-
ages and their corresponding OCR’ed texts. It
then contrastively learns a metric space where
the pooled image-text embedding for a given
instance is close to embeddings in the same
class ( e.g., the same firm or location) and dis-
tant from embeddings of a different class. Data
are linked by treating linkage as a nearest neigh-
bor retrieval problem with the multimodal em-
beddings. CLIPPINGS outperforms widely used
string matching methods by a wide margin in
linking mid-20th century Japanese firms across
financial documents. A purely self-supervised
model - trained only by aligning the embed-
dings for the image crop of a firm name and its
corresponding OCR’ed text - also outperforms
popular string matching methods. Fascinat-
ingly, a multimodally pre-trained vision-only
encoder outperforms a unimodally pre-trained
vision-only encoder, illustrating the power of
multimodal pre-training even if only one modal-
ity is available for linking at inference time.1 Introduction
Linking information across sources is fundamen-
tal to many analyses. For example, researchers
and businesses frequently link individuals or firms
across censuses and company records, governments
de-duplicate benefit or voter rolls across locations,
and analysts seek to identify how information from
the same source spreads through media. In large
swathes of the relevant literatures, deep neural
methods have made few inroads. For example,
a recent comprehensive review of the computer sci-
ence, social science, and statistical record linkage
literatures in Science Advances (Binette and Steorts,
2022) concludes that other methods are preferred
over deep neural models for record linkage in struc-
tured data. This contrasts starkly with the literature
on disambiguating entities in unstructured texts,
where transformer language models overwhelm-
ingly predominate, e.g.Wu et al. (2019); De Cao
et al. (2020); Yamada et al. (2020). This distinc-
tion is explained by the premise that entity linkage
with unstructured texts can leverage the power of
transfer learning from pre-trained large language
models, whereas entity linkage in structured text
databases cannot (Binette and Steorts, 2022).
This study focuses on record linkage in his-
torical documents. We show that leveraging the
power of deep learning can significantly improve
the accuracy of linking firms across data sources, a
common record linkage application, using histor-
ical Japanese data as a test case. Figure 1 shows
Japanese firm level records on supply chains (Jinji
K¯ōshinjo, 1954) (left) that need to be linked with a
large firm level directory (Teikoku K ¯oshinjo, 1957)
(right) with rich information; the texts are written
with different orientations. Each firm in the dataset
is represented by its localized image crop and by its
OCR. The image crops are useful for record linkage
because the OCR contains noise; even small OCR
errors can destroy significant information when
1arXiv:2304.03464v3  [cs.CV]  21 Jun 2024

--- PAGE 2 ---
Figure 1: This figure shows representative Japanese
firm records, taken from a publication on trading part-
ners (left) and a firm index (right). The text orientation
of these publications differs. The numbering denotes
linked firms. Pink text denotes OCR errors.
matching entities with similar names. The text is
useful because there are visually distinctive ways
of writing the same firm’s name that are discernible
through language understanding. We focus on sup-
ply chains as they are fundamental to the transmis-
sion of economic shocks (Acemoglu et al., 2016,
2012), agglomeration (Ellison et al., 2010), and
economic development (Hirschman, 1958; Myrdal
and Sitohang, 1957; Rasmussen, 1956; Bartelme
and Gorodnichenko, 2015; Lane, 2022), but their
role in long-run economic development has been
difficult to study due to the challenges of accurately
linking large-scale historical records.
To leverage both the visual and textual infor-
mation illustrated in Figure 1, the study develops
CLIPPINGS (Contrastively LInking Pooled Pre-
trained Embedd ings).CLIPPINGS employs end-to-
end training of symmetric vision and language bi-
encoders to learn a metric space where the pooled
image-text representation for an instance is close to
representations in the same class and distant from
those in different classes.
To train CLIPPINGS , we implement self-
supervised contrastive language-image pre-training
on image crop-OCR pairs, starting with a pre-
trained Japanese CLIP encoder (Makoto Sheen,
2022). Language-image pre-training ensures that
image crops are aligned with their corresponding
texts, even in the presence of OCR noise.
CLIPPINGS can then be further tuned by using
a contrastive loss and paired data - e.g., pairing
records on the left and right of Figure 1 - to align
pooled image-text embeddings. These labeled data
were created by the study authors. To tune the
model, we use a loss function that aligns image-
image, text-text, and image-text embeddings of
paired data. Supervised contrastive loss (Khoslaet al., 2020) is a special case of this loss when the
problem is reduced to a single modality. Paired
training data include a modest number of human
gold standard annotations, in which a human de-
termined whether records referred to the same
firm. We also generated training data syntheti-
cally. Specifically, we rendered a list of common
Japanese words as images using different fonts, ap-
plied image augmentations, and fed the resulting
images to OCR. For the same word, this produced
varying views of the image, as well as different
views of the text due to varying OCR errors in-
duced by the image augmentations.
At inference time, entities are linked by retriev-
ing the nearest neighbor(s) of queries in the target
dataset (in our example, queries come from the
supplier lists and the target dataset is the firm di-
rectory with richer information about the firms).
The contrastive framework can flexibly incorpo-
rate blocking on database characteristics - common
in record linkage - by using a type-specific loss
(Leszczynski et al., 2022).
CLIPPINGS significantly outperforms string
matching methods - which predominate in the
record linkage literature - as well as unimodal deep
neural methods. When CLIPPINGS is trained on a
modest number of linked firms using supervised
contrastive learning, it achieves a 94.5% record
linkage accuracy, compared to a string matching
maximum accuracy of 73.1% achieved with cus-
tom OCR. When we forgo supervised contrastive
training - using only the self-supervised language-
image pre-training on the image-OCR pairs - this
also outperforms string matching, with an accu-
racy of 84.9%. When we use only the CLIPPINGS
vision encoder, accuracy is higher than when us-
ing a unimodally trained vision encoder, aligning
records that require some language understanding.
The multimodal pre-training allows the vision en-
coder to learn some language understanding. The
CLIPPINGS models and training data are publicly
released (CC-BY license).
The rest of this study is organized as follows:
Section 2 discusses the literature, and Section 3
describes the CLIPPINGS architecture, and Section
4 applies CLIPPINGS to the firm record linkage
problem. Finally, Section 5 outlines limitations,
and Section 6 discusses ethical considerations.
2

--- PAGE 3 ---
2 Literature
Contrastive learning :CLIPPINGS learns a metric
space for pooled image-text representations that
can be applied to linkage problems even when the
number of classes is extremely large, unknown ex
ante, or constantly changes as databases are up-
dated. It is reminiscent of a variety of unimodal
bi-encoder applications, such as semantic similarity
(Reimers and Gurevych, 2019), passage retrieval
(Karpukhin et al., 2020), entity disambiguation
(Wu et al., 2019) and co-reference resolution (Hsu
and Horwood, 2022) in unstructured text.
To create pooled image-text representations,
it is necessary to have an aligned space. Con-
trastive Language-Image Pre-training (CLIP) (Rad-
ford et al., 2021) contrastively trained aligned
text and image encoders using 400 million image-
caption pairs. CLIPPINGS begins with pre-trained
Japanese (Makoto Sheen, 2022) CLIP image and
text encoders. Japanese CLIP was trained with
the standard CLIP (Radford et al., 2021) loss but
used a BERT-based text encoder and the vision
transformer was initialized by weights from the
AugReg ViT-B/16 model (Steiner et al., 2021).
Record Linkage: A variety of disciplines - in-
cluding computer science, statistics, database man-
agement, economics, and political science - have
made extensive methodological contributions to
record linkage, alternatively referred to as entity
resolution, fuzzy matching, approximate dictionary
matching, and string matching.
Within this sprawling landscape, the literatures
on entity resolution in structured databases (Binette
and Steorts, 2022) versus natural language text ( e.g.
(Wu et al., 2019; De Cao et al., 2020; Yamada
et al., 2020)) remain largely divorced. In the liter-
ature on record linkage in structured data - which
emphasizes linking noisy text fields that contain
information such as individual names, firm names,
organizations, or locations - edit distance metrics
are commonly used, e.g.(Levenshtein et al., 1966;
Jaro, 1989; Winkler, 1990). Another widespread
approach computes the cosine similarity between
n-gram representations of strings, where n-grams
are defined as all substrings of size nin a string
(Okazaki and Tsujii, 2010).
A recent literature on industry applications, fo-
cused on matching across e-commerce datasets,
shows the promise of transformer large language
models (LLMs) for improving record linkage in
structured datasets (Li et al., 2020; Joshi et al.,2021; Brunner and Stockinger, 2020; Zhou et al.,
2022; Peeters and Bizer, 2023; Tang et al., 2022).
Yet these methods have not yet made widespread
inroads in social science applications, with rule-
based methods continuing to overwhelmingly pre-
dominate ( e.g., see reviews by Binette and Steorts
(2022); Abramitzky et al. (2021)). Because labeled
record linkage datasets are very small compared to
the massive corpora used for training transformer
models from scratch, a comprehensive 2022 review
of the record linkage literature in Science Advances
(Binette and Steorts, 2022) concludes that deep neu-
ral models are unlikely to be applicable to entity
resolution using structured data. Constructing train-
ing data for record linkage is indeed highly labor
intensive, but much of the knowledge needed to
improve record linkage is plausibly already encap-
sulated in pre-trained image and language encoders
such as CLIP (Radford et al., 2021), or can be
gleaned from the further self-supervised language-
image pre-training pursued in this study.
In their simplest form, approximate string match-
ing methods simply count the required number of
edits (insertions, deletions, substitutions) to trans-
form one string into another. In practice, not all
substitutions are equally probable, leading to ef-
forts to construct rule-based lists that adjust the
costs of substitutions. For example, the fuzzychi-
nese (znwang25, 2020) package uses strokes or rad-
icals as the fundamental unit for n-grams substring
representations of entities, where these strokes and
radicals are drawn from an external database (kfcd,
2015) covering a subset of the CJK script. Alterna-
tively, the masala merge package (Novosad, 2018)
adjusts Levenshtein distance (Levenshtein et al.,
1966) to impose smaller penalties for common al-
ternative spellings in Hindi. Soundex, first devel-
oped in 1918 - together with the updated 1970 New
York State Identification and Intelligence System
(NYSIIS) (Silbert, 1970) - account for the fact that
similar sounding substitutions are more likely since
census enumerators misspelled names according
to their sound. These remain a bedrock for record
linkage in historical U.S. census data (Abramitzky
et al., 2021).
Such rule-based methods may perform well in
the contexts to which they are tailored. However,
they can be brittle and are labor-intensive to extend
to new settings, due to the use of hand-crafted fea-
tures. This heavily skews linked datasets in social
science towards a few high resource settings for
which existing methods have been tailored, in turn
3

--- PAGE 4 ---
Figure 2: Model Architecture: This figure illustrates the CLIPPINGS model architecture.
skewing downstream knowledge. Even in high re-
source settings, low accuracy in some applications
can require extensive human intervention in the
matching process to achieve the desired accuracy
(Bailey et al., 2020), limiting the scale of problems.
There have also been some efforts to estimate
machine learning models for record linkage. For
example, (Ventura et al., 2015) use a random forest
classifier trained on labeled data to disambiguate
authors of U.S. patents, applying clustering to the
resulting dissimilarity scores to enforce transitivity.
Very recently, the LinkTransformer package for
using large language models to link structured data
has been released (Arora and Dell, Forthcoming).
This package cites the challenges of using highly
technical existing codebases as an impediment to
the use of deep learning for record linkage in social
science, and is designed to be user friendly for
social scientists who lack familiarity with deep
learning frameworks. The CLIPPINGS codebase is
similarly designed to be intuitive to social science
users.
3 Model Architecture
Figure 2 shows the CLIPPINGS architecture.
We begin with pre-trained, aligned image and
text embeddings from Japanese language CLIP
(Makoto Sheen, 2022), which was trained on
image-caption pairs using captions machine-
translated into Japanese. We continue self-
supervised pre-training of the Japanese CLIP vision
and text encoders on image crop-OCR pairs.
We then learn a metric space where the pooled
image-text representation for a given instance is
close to representations (embeddings) in the same
class ( e.g., the same firm) and distant from repre-
sentations in different classes. The model is initial-
ized with the encoders obtained through the self-
supervised pre-training described above. We use a
supervised contrastive loss (Khosla et al., 2020) on
the pooled representations:−X
i∈B1
|P(i)|X
k∈P(i)logexp 
τ(zi)T(zk)
P
j∈Bexp (τ(zi)T(zj))
where zi=f(xi) +g(ti)
2is the mean of the
image and text embeddings for instance i.Bde-
notes the batch and τis a temperature hyperparam-
eter. This loss incentivizes alignment of image-
image, text-text, image-text, and text-image rep-
resentations across positive instances. It has the
flavor of combining contrastive learning on text,
contrastive learning on images, and UniCL (Yang
et al., 2022), which has a bi-directional image-text
and text-image similarity objective. CLIPPINGS
does not model cross-modal attention, because in
record linkage applications where the inputs are an
image of a text and the corresponding OCR, cross-
modal attention is unlikely to lead to significantly
richer representations.
We utilize the AdamW optimizer for all model
training, combined with a Cosine Annealing with
Warm Restarts learning rate (LR) scheduler. The
maximum LR was set for each run, and the min-
imum LR was fixed at 0. The first restart was
scheduled after 10 steps, with a restart factor of
2, doubling the time to restart after each cycle.
Each epoch involved sampling mviews (image-
text pairs) of each label in the dataset and process-
ing them once.
The total training time was 34.6 hours on a sin-
gle A6000 GPU card: 24 hours for language-image
pretraining, 10 hours for supervised training using
synthetic data, and 40 minutes for training on the
hand-labeled data. Additional hyperparameters and
model training details are provided in the supple-
mental materials.
A single A6000 GPU card accommodates a
batch size Bof 153 image-text pairs. In compari-
son, the original CLIP model (Radford et al., 2021)
was trained with a batch size of 32,768 using 256
4

--- PAGE 5 ---
V100 GPUs. To ensure adequate in-batch negatives
with the smaller batch sizes suitable for various
downstream users, we employed offline hard nega-
tive mining. Details on batching and hard negative
mining are detailed in the supplementary materials.
At inference time, instances are linked using
nearest neighbor retrieval. Facebook Artificial In-
telligence Similarly Search (FAISS) (Johnson et al.,
2019), with IndexFlatIP , is used to calculate pair-
wise exact distances between the embeddings, in
order to retrieve the nearest neighbor of each query
(firm in the supplier lists) in the firm directories.1
Blocking - which incorporates type informa-
tion from structured databases into record link-
age - is important in many applications and has
been the subject of extensive research (see Steorts
et al. (2014); Papadakis et al. (2019) for reviews).
While not applicable to this study’s applications,
blocking is natural to incorporate into a contrastive
setup, through using a type-specific loss function
(Leszczynski et al., 2022). This will allow for
matches to be made even when there is some noise
in the type, a common scenario.
To examine how CLIPPINGS compares to an
analogous framework trained using a state-of-the-
art unimodal encoder, we train a symmetric DINO
vision transformer (Caron et al., 2021) using the
Japanese firm linked image crop data. Details on
self-supervised pre-training and hyperparameters
for the self-supervised and supervised training are
detailed in the supplementary materials.
4 Record Linkage
4.1 Data
This study’s first application is constructing histor-
ical Japanese supply chains. This entails match-
ing suppliers and customers recorded in firm level
records collected in 1956 for over 7,000 large
Japanese firms (Jinji K ¯ōshinjo, 1954) to a firm
level directory that provides additional rich in-
formation about nearly 70,000 firms (Teikoku
K¯oshinjo, 1957). The former are written horizon-
tally and the latter vertically, making a potentially
challenging case for visual matching. Firm name
crops were localized using a Mask R-CNN (He
et al., 2017) model custom-trained with Layout
Parser (Shen et al., 2021). To create labeled data
for training and evaluation, the customers and sup-
1Because FAISS range search is not GPU-supported, we
implement knearest neighbor search, conservatively setting k
to 900.pliers of randomly selected firms were hand merged
with the firm directory. Two annotators completed
this task and resolved all discrepancies by hand.
Many firms appear as customers and suppliers of
multiple firms, and the data were de-duplicated
such that each customer or supplier appears only
once, in order to avoid leakage. Sometimes a single
firm is linked to multiple entries in the firm direc-
tory, as firms can appear there more than once if
they were registered in multiple prefectures.
We used a 60-20-20 split to divide classes into
a training set (772 examples), validation set (257
examples), and test set (257 examples). The test
data links the customer-supplier list to all matching
firms in the directory (with multiple matches occur-
ring when a firm is registered in multiple prefec-
tures), whereas this costly labeling was not needed
for the training data, where each firm has a single
match. In the main text, we report results using
a dataset that drops customers and suppliers like
“the government” that do not appear in the firm di-
rectory. In the supplementary materials, we report
analyses including these instances, with similar
patterns emerging.
We also trained on 19,793 synthetically gener-
ated Japanese placenames, with an 80-20 train-val
split. Each are rendered using different fonts and
OCRed with two different OCR engines, that make
different errors (Carlson et al., 2023; Du et al.,
2022). Each epoch involved sampling 3 "views" of
each image crop-ocr pair.
Additionally, we conducted self-supervised
language-image pre-training of the Japanese CLIP
encoders, using 111,750 firm image crops and their
corresponding OCR, as well as the same 19,793
synthetically generated names and their OCR, with
an 80-20 test-val split.
As record linkage accuracy with string match-
ing is likely to relate to the quality of the OCR,
we apply string matching comparisons using two
different OCRs of the Japanese firm names. The
"noisy" dataset is created by using Google Cloud
Vision off-the-shelf, as off-the-shelf OCR usage is
the overwhelming norm (Google Cloud Vision does
not support fine-tuning) and is often noisy. The
"clean" dataset is created using a custom-trained
OCR that achieves a character error rate of 0.6%
(Carlson et al., 2023), a near best case scenario.
Nevertheless, in character-based languages entities
often have relatively few characters in their names,
meaning even a single OCR error can destroy sig-
nificant information.
5

--- PAGE 6 ---
Noisy Clean
OCR OCR
Panel A: String-Matching
Levenshtein distance 0.630 0.731
Stroke n-gram similarity 0.689 0.731
Panel B: Language-Image Self-Supervised Training
Visual Linking 0.769 0.769
Language Linking 0.740 0.790
Multimodal Linking 0.845 0.849
Panel C: Supervised Training on Linked Data
with Vision Pre-training
Visual Linking 0.878 0.878
Panel D: Supervised Training on Linked Data
with Language-Image Pre-training
Visual Linking 0.924 0.924
Language Linking 0.790 0.882
Multimodal Linking 0.937 0.945
Table 1: Baseline Matching Results: This table reports
accuracy on the test set using a variety of different meth-
ods for linking Japanese firms from supply chain records
to a large firm directory. Noisy OCR uses off-the-shelf
Google Cloud Vision for OCR, and Clean OCR uses an
accurate custom-trained OCR.
4.2 Results
CLIPPINGS achieves a record linkage accuracy of
94.5% (Table 1, Panel D), significantly outperform-
ing string matching metrics (Panel A) on both noisy
OCR (68.9% accuracy) and clean OCR (73.1% ac-
curacy). When using multimodal representations,
the accuracy of the OCR has only a very modest im-
pact on linking accuracy: 93.7% with noisy OCR
versus 94.5% with clean OCR.
Using the pooled representations also outper-
forms tuning only the pre-trained image or pre-
trained language encoders on paired entity data for
that modality, which achieve an accuracy of 92.4%
(vision matching) and 88.2% (language matching).
When there are OCR errors, the image crops are
useful as they avoid the destruction of informa-
tion through OCR. At the same time, language
understanding is helpful due to abbreviations and
multiple ways to write a firm name.
To this end, using a vision encoder that was mul-
timodally pre-trained on image crop-OCR’ed text
paired data achieves an accuracy of 92.4%, beat-
ing the unimodally trained vision encoder - which
achieves an accuracy of 87.8% - by a fairly wide
margin (Panel C). This suggests the utility of multi-
modal pre-training, which we will examine furtherin the error analysis.
Figure 3, Panel A shows representative errors
made by the supervised vision only, language only,
and multimodal encoders, that use language-image
pre-training (Panel D). In the vision-encoder er-
ror, the ground truth is Meiji Seika, a firm produc-
ing snacks and chocolates. Its prediction is Meiji
pharmaceutical company, as 菓and薬look sim-
ilar. This illustrates how the vision encoder can
sometimes confuse visually similar entities that
have very different meanings. In the language-
only encoder error, the ground truth is “Japanese
Processing and Manufacturing of Paper” （日本
加工製紙), and the prediction is “Japanese Paper
Processing” （日本紙加工), very similar in mean-
ing. The multimodal encoder predicts both of these
cases correctly. Finally, the multimodal error is a
company whose name is only two characters. The
ground truth is 丸永, and the prediction is 丸水,
with永and水very visually similar. These are
names without language context, so the language
encoder cannot distinguish them either.
Fascinatingly, the tuned vision only encoder with
language-image pre-training gets some matches
that require language understanding correct (accu-
racy 92.4%), that the tuned ViT model with vision-
only pre-training gets wrong (accuracy 87.8%),
suggesting some acquisition of language under-
standing from the multimodal pre-training.2Fig-
ure 3, Panel B provides several examples. In the
first example, レナウン 靴下（Renown Sock)
is matched by the ViT model to “レナウン 商
事(Renown Business), whereas the multimodally
pre-trained encoder matches it correctly to レナ
ウン靴下工業（Renown Sock Industry), de-
spite the extra two characters. In the second ex-
ample, the ground truth is 太陽鑛工(Sun Min-
eral Manufacturing), whereas the ViT prediction
is太陽紙工（Sun Paper Manufacturing). The
third example results from an error in our custom
trained layout detection model, that concatenates
two customer-suppliers. The detected firm is “ エヌ
ティエヌ 販賣會社[販賣 ]国鉄,” which translates
as “NTN Sales Company[Sales]The Government-
owned Railway”. The ViT simply predicts a com-
pany of similar length, whereas the encoder with
multimodal pre-training matches it to NTN Sales
2Tuning a Japanese S-BERT language bi-encoder does
even worse than the vision only encoder, but is less of an
apples-to-apples comparisons to the CLIPPINGS text-only
model due to the challenges of devising self-supervised
language-only pre-training recipes in this context.
6

--- PAGE 7 ---
Figure 3: Errors: This figure shows errors in record linkage made by different models.
Company.
The purely self-supervised multimodal model
also outperforms the string matching methods, with
an accuracy of 84.9% (Table 1, Panel B). n-gram
similarity at the stroke level with fuzzychinese (zn-
wang25, 2020) achieves an accuracy of 73.1% on
clean OCR, as coincidentally does Levenshtein dis-
tance. When only the vision encoder or language
encoder of the self-supervised multimodal model
is used at test time, the performance also exceeds
that of standard string matching techniques (76.9%
and 79.0% accuracy, respectively). This shows that
with only self-supervised training, neural methods
can still outperform widely used string matching
methods.
Table 2 examines the contribution of different
elements of the CLIPPINGS training recipe. Panel
A considers the performance of Japanese CLIP
(Makoto Sheen, 2022) off-the-shelf. Using the vi-
sion encoder alone, every instance is mispredicted.
The off-the-shelf text encoder, while better than
the vision encoder, is outperformed by traditional
string matching methods.
Panel B examines the performance of
CLIPPINGS when only supervised training isNoisy OCR Clean OCR
Panel A: Zero-shot Japanese-CLIP
Visual Linking 0.000 0.000
Language Linking 0.639 0.626
Multimodal Linking 0.491 0.433
Panel B: Only Supervised Training
Multimodal Linking 0.676 0.731
Table 2: Record Linkage Ablations: This table reports
accuracy for linking Japanese firms from supply chain
records to a large firm directory. Noisy OCR uses off-
the-shelf Google Cloud Vision for OCR, and Clean
OCR uses an accurate custom-trained OCR. Panel A
uses Japanese CLIP off-the-shelf, and Panel B uses only
supervised training, without further self-supervised pre-
training.
used, discarding the self-supervised training
on image-OCR pairs. Performance declines
significantly relative to when self-supervised
language-image pre-training is employed - with
accuracy similar to that of traditional string
matching methods - illustrating the importance of
first aligning the vision and text encoders for the
7

--- PAGE 8 ---
Figure 4: Input-Output Networks: This figure plots the average supply chain distance of Japanese firms to Mitsui,
Mitsubishi, and Sumitomo, the three most prominent Japanese firms.
document image-OCR domain.
Since CLIPPINGS leverages transfer learning
from language-image pre-training, it can be pre-
cisely tailored to different settings with small,
cheap-to-create training sets, or used as a purely
self-supervised solution. Moreover, hard negatives
in contrastive training encourage separation be-
tween different instance representations even if the
instances are very similar, making it highly suitable
for linking datasets with lots of confusables.
Deployment costs are important for extensibil-
ity, as record linkage problems often entail linking
millions of entities on a limited compute budget.
Experiments on the full dataset of 36,673 customer-
suppliers, using an 18 core Intel(R) i9-9980XE
CPU @ 3.00GHz and a single NVIDIA A6000
GPU, show that CLIPPING ’s deployment costs are
modest, making it a highly scalable solution. Mut-
limodal CLIPPINGS takes 6 minutes, 21 seconds
to run on the full data with a single GPU, all but
one second of which are embedding the crops and
OCR. This compares to 54 minutes to implement
our optimized CPU Levenshtein distance calcula-
tions (which is an order of magnitude faster than
R string matching, e.g.(van der Loo, 2014)) and
3 minutes and 7 seconds for stroke matching with
fuzzychinese (znwang25, 2020).
Figure 4 illustrates the supply chain networks
created by applying CLIPPINGS to the full dataset,
with the shading showing the average distance
in the supply chain networks to the three largest
Japanese conglomerates: Mitsui, Mitsubishi, and
Sumitomo. Node position and scaling are fixed
across the graphs, with scaling showing averagedegree centrality in the supply chain network. The
key takeaway is that using off-the-shelf OCR and
Levenshtein distance - a prevalent approach in the
literature - creates a visibly different network (left)
than the multimodal method (right), which is much
closer to the ground truth. For example, in the
bottom right corner of the graph created with edit
distance, there is a firm with lots of false links (and
hence a high degree centrality). A study of the
Japanese economy based on the noisier network
is likely to produce biased results (Chandrasekhar
and Lewis, 2011). This illustrates the utility for
downstream social science research of improving
record linkage through multimodal learning.
8

--- PAGE 9 ---
5 Limitations
The biggest limitation of CLIPPINGS is that it is cur-
rently tuned for a particular language and type of
document collection. Because strong performance
can be achieved even with purely self-supervised
training, supervised tuning does not require many
labels, and the compute requirements are not too
onerous, it would be relatively straightforward to
extend to a much more diverse, multilingual doc-
ument collection. We hope that by illustrating the
utility of this method, that it will encourage fur-
ther development of multimodal models for record
linkage.
6 Ethical Considerations
CLIPPINGS is ethically sound. Its methods are en-
tirely open source, and its training data are entirely
in the public domain. While CLIPPINGS is more
accurate than string matching, as well as unimodal
neural matching, some errors will often still arise
with multimodal matching. It is important for re-
searchers to assess whether these could affect their
conclusions.
9

--- PAGE 10 ---
Supplementary Materials
7 Methods
7.1 Japanese Multimodal Models
The Japanese multimodal models were initialized
with a Japanese CLIP checkpoint (Makoto Sheen,
2022). Japanese CLIP was trained with the stan-
dard CLIP (Radford et al., 2021) loss but used
a BERT-based text encoder and the vision trans-
former was initialized by weights from the AugReg
ViT-B/16 model (Steiner et al., 2021).
Synthetic Data
Both language-image pretraining and the super-
vised training of CLIPPINGS employed synthetic
data. To create synthetic data, we rendered a list of
common Japanese words as images using different
fonts (a type of augmentation), applied image aug-
mentations, and fed the resulting images to OCR.
For the same word, this produced varying views of
the word’s image due to different augmentations,
as well as different views of the word’s text due to
varying OCR errors induced by the augmentations.
We randomly sample one image-text pair per
label (word) and use this subset for language-image
pretraining. For training CLIPPINGS , we train on
the full synthetic dataset and then fine-tune on our
labelled data.
Other Training Details
Text crops are thin vertically or horizontally ori-
ented rectangles, with highly diverse aspect ra-
tios, whereas vision encoders are almost always
trained on square images. Center cropping would
discard important information and resizing often
badly morphs the image, given that the underlying
aspect ratios are far from a square. To preserve the
aspect ratio, we pad the rectangular crop with the
median value of the border pixel such that the text
region is always centered.
For language-image pretraining, the standard
CLIP loss was used to align the image and text
encoders (Radford et al., 2021). Supervised Con-
trastive loss (Khosla et al., 2020) was used for all
supervised training. We used the AdamW opti-
mizer for all model training along with a Cosine
Annealing with Warm Restarts learning rate (LR)
scheduler where the maximum LR was specified
for each run and the minimum LR was set to 0. 10
steps were chosen for the first restart with a restart
factor of 2 that doubles the time to restart after ev-
ery restart. An epoch involved sampling mviews(image-text pair) of each label in the dataset and
going through each of them once. It took a 24 hours
to perform language-image pretraining, 10 hours
to perform supervised training using synthetic data
and 40 minutes to train on the labelled data - a total
training time of 34.6 hours to train CLIPPINGS on
a single A6000 GPU card. Hyperparameters and
additional details about model training are listed in
Table S-1.
At inference time, we used IndexIPFlat from
FAISS (Johnson et al., 2019) to find the nearest
neighbor on L2-normalized embeddings.
Hard Negative Mining
CLIPPINGS was trained on a single A6000 GPU
card, which could fit a batch size Bof 153 image-
text pairs. This compares to a batch size of 32,768
used to train the original CLIP (Radford et al.,
2021) on 256 V100 GPUs. Offline hard negative
mining was used to achieve sufficient in-batch neg-
atives with the small batch sizes that can fit into
compute setups that are realistic for diverse down-
stream users.
Define the data as a triple (xn, tn, yn), where
xn∈ X is the image, tn∈ T is the text, and
yn∈ Y is an associated label (class). Let Dbe the
set of all triples. For supervised pertaining using
synthetic data, we randomly sampled one image-
text pair per label in Dto form D′⊂D. For each
image-text pair (xa, ta)inD′, we use the domain-
adapted CLIP model to find its knearest neighbor
pairs (xk, tk)(including itself). This gives us the
k−1nearest neighbours for each label yainD′⊂
D.
The anchor label yaand its k−1neighbors
form a "hard-negative" set. In a batch, we sample
m= 3 views of image-text pairs with replace-
ment. A batch-size divisible by k∗mcan fitB
k∗munique classes, each with their own mviews and
themviews of all kneighbors. We shuffle the
hard-negative sets and partition them into groups
ofB
k∗msuch that each group can constitute a
batch. Each constituent class has kneighbors and
mviews within the minibatch. For the next step -
fine-tuning with labeled data - we follow the same
approach but with the best model checkpoint from
synthetic pretraining.
7.2 Vision Transformer
We initialized the weights of the Vision Trans-
former from the DINO-pretrained checkpoint for
10

--- PAGE 11 ---
ViT/B16 (Caron et al., 2021). Hyperparameters
and other training details are listed in Table S-1.
As for CLIPPINGS , ViT training employed syn-
thetic data. The same pipeline as above was used
to generate synthetically noised images. For each
word in a list of Japanese words, the text was ren-
dered using different fonts and augmented to create
synthetically noised views.
Offline hard-negative mining was used to train
the ViT. The approach is similar to that described
above. We used a pretrained checkpoint to find k
nearest neighbors for each class. This was used
to create a batch containing mviews of that class,
along with mviews ofB
m−1other classes. When
using hard negatives, we substituted "k-1" of these
other classes with the nearest neighbor classes of
the anchor.
8 Additional Results
Table S-2 examines the inclusion of instances with-
out a match in the firm directory, for example, a
range of government agencies. While performance
declines somewhat relative to the results reported
in the main text, the relative comparisons between
models hold.
11

--- PAGE 12 ---
Model lr B w_decay temp im_wt m k epochs nm_thresh
Language-image Pretraining 5e-5 153 0.001 0.048 - - - 40 -
Supervised models
ViT (synthetic) 5.8e-5 256 0.0398 0.048 - 8 8 5 -
ViT (labelled) 2e-6 252 0.1 0.09 - 3 8 10 0.88
Sup. Lang.-only (synthetic) 5e-6 153 0.001 0.1 0 3 3 30 0.85, 0.85
Sup. Image-only (synthetic) 5e-6 153 0.001 0.1 1 3 3 30 0.76
Sup. Mean-pool (synthetic) 5e-6 153 0.001 0.1 0.5 3 3 30 0.81, 0.80
Sup. Lang-only (labelled) 5e-6 153 0.001 0.1 0 3 3 30 0.84,0.82
Sup. Image-only (labelled) 5e-6 153 0.001 0.1 1 3 3 30 0.79
Sup. Mean-pooling (labelled) 5e-6 153 0.001 0.1 0.5 3 3 30 0.82,0.82
Table S-1: Training Hyperparameters: lris the maximum learning rate, Bis the batch size, w_decay is the AdamW
weight decay, im_wtis the weight of the image embedding in the pooled embedding, mis the number of views
sampled in each epoch, kis the number of nearest neighbours in a hard-negative set, and epochs is the number
of epochs. nm_threshold is a tuple with two tuned similarity thresholds (for noisy and clean OCR respectively)
under which a retrieved neighbor is considered to not match with any of the target images.
12

--- PAGE 13 ---
Noisy OCR Clean OCR
Panel A: String-Matching
Levenshtein distance 0.605 0.625
Stroke n-gram similarity 0.625 0.650
Panel B: Language-Image Self-Supervised Training
Visual Linking 0.693 0.693
Language Linking 0.680 0.741
Multimodal Linking 0.770 0.770
Panel C: Supervised Training on Linked Data
with Vision Pre-training
Visual Linking 0.819 0.819
Panel D: Supervised Training on Linked Data
with Language-Image Pre-training
Visual Linking 0.829 0.829
Language Linking 0.757 0.825
Multimodal Linking 0.845 0.871
Table S-2: Including instances without a match: This table reports accuracy on the test set using a variety of different
methods for linking Japanese firms from supply chain records to a large firm directory. Noisy OCR uses off-the-shelf
Google Cloud Vision for OCR, and Clean OCR uses an accurate custom-trained OCR.
13

--- PAGE 14 ---
References
Ran Abramitzky, Leah Boustan, Katherine Eriksson, James Feigenbaum, and Santiago Pérez. 2021. Automated
linking of historical data. Journal of Economic Literature , 59(3):865–918.
Daron Acemoglu, Ufuk Akcigit, and William Kerr. 2016. Networks and the macroeconomy: An empirical
exploration. Nber macroeconomics annual , 30(1):273–335.
Daron Acemoglu, Vasco M Carvalho, Asuman Ozdaglar, and Alireza Tahbaz-Salehi. 2012. The network origins of
aggregate fluctuations. Econometrica , 80(5):1977–2016.
Abhishek Arora and Melissa Dell. Forthcoming. Linktransformer: A unified package for record linkage with
transformer language models. ACL Systems Demonstrations .
Martha J Bailey, Connor Cole, Morgan Henderson, and Catherine Massey. 2020. How well do automated linking
methods perform? lessons from us historical data. Journal of Economic Literature , 58(4):997–1044.
Dominick Bartelme and Yuriy Gorodnichenko. 2015. Linkages and economic development. Technical report,
National Bureau of Economic Research.
Olivier Binette and Rebecca C Steorts. 2022. (almost) all of entity resolution. Science Advances , 8(12):eabi8021.
Ursin Brunner and Kurt Stockinger. 2020. Entity matching with transformer architectures-a step forward in data
integration. In 23rd International Conference on Extending Database Technology, Copenhagen, 30 March-2
April 2020 , pages 463–473. OpenProceedings.
Jacob Carlson, Tom Bryan, and Melissa Dell. 2023. Efficient ocr for building a diverse digital history. arXiv e-prints
arxiv:2304.02737 .
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
2021. Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 .
Arun Chandrasekhar and Randall Lewis. 2011. Econometrics of sampled networks. Unpublished manuscript,
MIT.[422] .
Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv
preprint arXiv:2010.00904 .
Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li, Yuning Du, and Yu-Gang Jiang.
2022. Svtr: Scene text recognition with a single visual model. arXiv preprint arXiv:2205.00159 .
Glenn Ellison, Edward L Glaeser, and William R Kerr. 2010. What causes industry agglomeration? evidence from
coagglomeration patterns. American Economic Review , 100(3):1195–1213.
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE
international conference on computer vision , pages 2961–2969.
Albert O Hirschman. 1958. The strategy of economic development. Yale Univ. Press, New Haven, Conn.
Benjamin Hsu and Graham Horwood. 2022. Contrastive representation learning for cross-document coreference
resolution of events and entities. arXiv preprint arXiv:2205.11438 .
Matthew A Jaro. 1989. Advances in record-linkage methodology as applied to matching the 1985 census of tampa,
florida. Journal of the American Statistical Association , 84(406):414–420.
Jinji K ¯ōshinjo. 1954. Nihon shokuinrokj . Jinji K ¯ōshinjo.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity search with gpus. IEEE Transactions
on Big Data , 7(3):535–547.
Salil Rajeev Joshi, Arpan Somani, and Shourya Roy. 2021. Relink: Complete-link industrial record linkage
over hybrid feature spaces. In 2021 IEEE 37th International Conference on Data Engineering (ICDE) , pages
2625–2636. IEEE.
Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau
Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 .
kfcd. 2015. chaizi. https://github.com/kfcd/chaizi .
14

--- PAGE 15 ---
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu,
and Dilip Krishnan. 2020. Supervised contrastive learning. arXiv preprint arXiv:2004.11362 .
Nathan Lane. 2022. Manufacturing revolutions: Industrial policy and industrialization in south korea. Available at
SSRN 3890311 .
Megan Leszczynski, Daniel Y Fu, Mayee F Chen, and Christopher Ré. 2022. Tabi: Type-aware bi-encoders for
open-domain entity retrieval. arXiv preprint arXiv:2204.08173 .
Vladimir I Levenshtein et al. 1966. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet
physics doklady , volume 10, pages 707–710. Soviet Union.
Yuliang Li, Jinfeng Li, Yoshihiko Suhara, AnHai Doan, and Wang-Chiew Tan. 2020. Deep entity matching with
pre-trained language models. arXiv preprint arXiv:2004.00584 .
Kei Sawada Makoto Sheen, Tenu Cho. 2022. nihongo niokeru gengo gaz ¯o jizen gakush ¯u moderu no k ¯ochiku to
k¯okai. In The 25th Meeting on Image Recognition and Understanding .
Gunnar Myrdal and Paul Sitohang. 1957. Economic theory and under-developed regions. Regional Studies .
Paul Novosad. 2018. Masala merge. https://github.com/paulnov/masala-merge .
Naoaki Okazaki and Jun ’ichi Tsujii. 2010. Simple and efficient algorithm for approximate dictionary matching. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010) , pages 851–859.
George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, and Themis Palpanas. 2019. A survey of blocking and
filtering techniques for entity resolution. arXiv preprint arXiv:1905.06167 .
Ralph Peeters and Christian Bizer. 2023. Using chatgpt for entity matching. arXiv preprint arXiv:2305.03423 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural
language supervision. In International conference on machine learning , pages 8748–8763. PMLR.
Poul Nørregaard Rasmussen. 1956. Studies in inter-sectoral relations , volume 15. E. Harck.
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv
preprint arXiv:1908.10084 .
Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, and Weining Li. 2021.
Layoutparser: A unified toolkit for deep learning based document image analysis. International Conference on
Document Analysis and Recognition , 12821.
Jeffrey M Silbert. 1970. The world’s first computerized criminal-justice information-sharing system-the new york
state identification and intelligence system (nysiis). Criminology , 8:107.
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2021.
How to train your vit? data, augmentation, and regularization in vision transformers. CoRR , abs/2106.10270.
Rebecca C Steorts, Samuel L Ventura, Mauricio Sadinle, and Stephen E Fienberg. 2014. A comparison of blocking
methods for record linkage. In Privacy in Statistical Databases: UNESCO Chair in Data Privacy, International
Conference, PSD 2014, Ibiza, Spain, September 17-19, 2014. Proceedings , pages 253–268. Springer.
Jiawei Tang, Yifei Zuo, Lei Cao, and Samuel Madden. 2022. Generic entity resolution models. In NeurIPS 2022
First Table Representation Workshop .
Teikoku K ¯oshinjo. 1957. Teikoku Gink ¯o Kaisha Y ¯oroku . Teikoku K ¯oshinjo.
M.P.J. van der Loo. 2014. The stringdist package for approximate string matching. The R Journal , 6:111–122.
Samuel L Ventura, Rebecca Nugent, and Erica RH Fuchs. 2015. Seeing the non-stars:(some) sources of bias in past
disambiguation approaches and a new public tool leveraging labeled records. Research Policy , 44(9):1672–1701.
William E Winkler. 1990. String comparator metrics and enhanced decision rules in the fellegi-sunter model of
record linkage.
Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2019. Scalable zero-shot
entity linking with dense entity retrieval. arXiv preprint arXiv:1911.03814 .
15

--- PAGE 16 ---
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. Luke: Deep contextual-
ized entity representations with entity-aware self-attention. arXiv preprint arXiv:2010.01057 .
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. 2022. Unified
contrastive learning in image-text-label space. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 19163–19173.
Huchen Zhou, Wenfeng Huang, Mohan Li, and Yulin Lai. 2022. Relation-aware entity matching using sentence-bert.
Computers, Materials & Continua , 71(1).
znwang25. 2020. fuzzychinese. https://github.com/znwang25/fuzzychinese .
16
