AnyMAL: Một Mô hình Ngôn ngữ Tăng cường Đa phương thức Hiệu quả và Có thể Mở rộng

Seungwhan Moon∗Andrea Madotto∗Zhaojiang Lin∗Tushar Nagarajan∗
Matt Smith Shashank Jain Chun-Fu Yeh Prakash Murugesan
Peyman Heidari Yue Liu Kavya Srinet Babak Damavandi Anuj Kumar
FAIR, Meta & Meta Reality Labs

Tóm tắt

Chúng tôi giới thiệu Any-Modality Augmented Language Model (AnyMAL), một mô hình thống nhất có thể lý luận về các tín hiệu đầu vào đa phương thức khác nhau (tức là văn bản, hình ảnh, video, âm thanh, cảm biến chuyển động IMU), và tạo ra các phản hồi văn bản. AnyMAL kế thừa khả năng lý luận dựa trên văn bản mạnh mẽ của các LLM hiện đại nhất bao gồm LLaMA-2 (70B), và chuyển đổi các tín hiệu đặc trưng cho từng phương thức thành không gian văn bản chung thông qua một mô-đun căn chỉnh được huấn luyện trước. Để củng cố thêm khả năng của LLM đa phương thức, chúng tôi tinh chỉnh mô hình với một bộ hướng dẫn đa phương thức được thu thập thủ công để bao phủ các chủ đề và nhiệm vụ đa dạng vượt ra ngoài những câu hỏi đáp đơn giản. Chúng tôi tiến hành phân tích thực nghiệm toàn diện bao gồm cả đánh giá của con người và tự động, và chứng minh hiệu suất hiện đại nhất trên các nhiệm vụ đa phương thức khác nhau.

1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM), được biết đến với quy mô và độ phức tạp đáng kể, đã nâng cao đáng kể khả năng của máy móc trong việc hiểu và diễn đạt ngôn ngữ con người. Tiến bộ trong LLM cũng đã dẫn đến những tiến bộ đáng chú ý trong lĩnh vực thị giác-ngôn ngữ [1,2,3,4], thu hẹp khoảng cách giữa bộ mã hóa hình ảnh và LLM để kết hợp khả năng lý luận của chúng. Nghiên cứu LLM đa phương thức trước đây đã tập trung vào các mô hình kết hợp văn bản và một phương thức khác [3,5], chẳng hạn như mô hình văn bản và hình ảnh, hoặc đã tập trung vào các mô hình ngôn ngữ độc quyền không được mở nguồn [2, 4].

Để giải quyết những thách thức được đề cập trước đây, chúng tôi giới thiệu Any-Modality Augmented Language Model (AnyMAL) — một bộ sưu tập các bộ mã hóa đa phương thức được huấn luyện để biến đổi dữ liệu từ các phương thức khác nhau, bao gồm hình ảnh, video, âm thanh, và dữ liệu cảm biến chuyển động IMU, thành không gian nhúng văn bản của một LLM. Để đạt được điều này, chúng tôi mở rộng công việc của [1] đến (1) các LLM được điều chỉnh hướng dẫn có khả năng hơn (tức là LLaMA-2-70B-chat [6]), (2) các bộ mã hóa phương thức được huấn luyện trước lớn hơn, và (3) các lớp chiếu tiên tiến để xử lý độ dài đầu vào biến đổi. Các ví dụ đầu ra của mô hình được hiển thị trong Hình 1, và một minh họa về phương pháp tổng thể được hiển thị trong Hình 2.

Các đóng góp chính của công việc như sau:

• Chúng tôi trình bày một giải pháp hiệu quả và có thể mở rộng để xây dựng LLM Đa phương thức. Chúng tôi cung cấp các lớp chiếu được huấn luyện trước trên các tập dữ liệu lớn với đa phương thức đa dạng (ví dụ: 200M hình ảnh, 2.2M âm thanh, 500K chuỗi thời gian IMU, 28M video) tất cả được căn chỉnh với cùng một LLM (LLaMA-2-70B-chat), do đó cho phép việc nhắc nhở đa phương thức xen kẽ trong ngữ cảnh.

• Chúng tôi tinh chỉnh thêm mô hình với bộ hướng dẫn đa phương thức qua ba phương thức (hình ảnh, video, và âm thanh) bao phủ các nhiệm vụ đa dạng không bị ràng buộc vượt ra ngoài các lĩnh vực QA đơn giản. Tập dữ liệu có đặc trưng là dữ liệu hướng dẫn chất lượng cao được thu thập thủ công, mà chúng tôi cũng sử dụng như một điểm chuẩn cho các nhiệm vụ lý luận đa phương thức phức tạp.

• Mô hình tốt nhất của chúng tôi đạt hiệu suất zero-shot mạnh mẽ trong cả đánh giá tự động và của con người trên các nhiệm vụ và phương thức đa dạng, thiết lập SOTA mới với độ chính xác tương đối +7.0% cải thiện trên VQAv2, +8.4% CIDEr trên COCO image captioning zero-shot, và +14.5% CIDEr trên AudioCaps, khi so sánh với các mô hình có sẵn trong tài liệu.

2 Công việc Liên quan

Mô hình Ngôn ngữ Lớn (LLM): Đã có một làn sóng LLM với các kích thước mô hình khác nhau gần đây, thể hiện khả năng lý luận đáng chú ý. Trong khi dịch vụ thương mại nổi tiếng nhất là ChatGPT [4,7], các mô hình mã nguồn mở bao gồm FlanT5 [8], GPT-J [9], OPT [10], LLaMA [11], Vicuna [12], và gần đây nhất là LLaMA-2 [6].

Công việc của chúng tôi xây dựng dựa trên khả năng lý luận dựa trên văn bản mạnh mẽ của các LLM này, mở rộng những khả năng này cho đầu vào đa phương thức.

Mô hình Thị giác-Ngôn ngữ: Nhiều nghiên cứu đã giải quyết nhiệm vụ hướng dẫn một mô hình thống nhất tích hợp cả yếu tố thị giác và ngôn ngữ, tìm thấy các ứng dụng thực tế trong các lĩnh vực như chú thích hình ảnh [13] và các nhiệm vụ trả lời câu hỏi thị giác (VQA) [14,15,16]. Trong khi sự khan hiếm tương đối của các nguồn dữ liệu căn chỉnh các phương thức khác nhau thường được coi là nút thắt cổ chai trong việc mở rộng, các công việc gần đây đã chuyển hướng khai thác khả năng của các LLM được huấn luyện trước, khai thác kiến thức tích lũy từ các kho văn bản rộng lớn. Những công việc này bao gồm Flamingo [2], OpenFlamingo [17], Palm-E [18], BLIP-2 [3], InstructBLIP [19], LLaVA [20], IDEFICS [5], MiniGPT-4 [21] và nhiều hơn nữa [22,23,24,25,26,27,28], trong đó mỗi mô hình sử dụng các biến thể khác nhau của LLM cơ sở. Các mô hình này thường trải qua các giai đoạn tinh chỉnh, tái sử dụng một số tập dữ liệu thị giác-ngôn ngữ đặc trưng cho nhiệm vụ [20, 29].

Công việc của chúng tôi mở rộng các phương pháp trước đây bằng cách (1) cho phép đầu vào đa phương thức đa dạng vượt ra ngoài tín hiệu thị giác, (2) trình bày một quy trình tinh chỉnh với dữ liệu điều chỉnh hướng dẫn đa phương thức được thu thập thủ công của chúng tôi, và (3) mở rộng các tham số LLM lên 70B thông qua phương pháp huấn luyện trước hiệu quả.

3 Phương pháp

3.1 Huấn luyện trước

Căn chỉnh Phương thức: Chúng tôi đạt được khả năng hiểu đa phương thức bằng cách huấn luyện trước LLM với dữ liệu đa phương thức được ghép đôi (tín hiệu đặc trưng cho phương thức và mô tả văn bản) (Hình 2). Cụ thể, chúng tôi huấn luyện một bộ chuyển đổi nhẹ cho mỗi phương thức để chiếu các tín hiệu đầu vào vào không gian nhúng token văn bản của một LLM cụ thể. Theo cách này, không gian nhúng token văn bản của LLM trở thành không gian nhúng token chung, với các token đại diện cho văn bản hoặc các phương thức khác. Số lượng nhúng token được sử dụng để đại diện cho mỗi phương thức đầu vào được cố định mỗi bộ chuyển đổi, dao động từ 64 - 256 trong công việc này. Trong quá trình huấn luyện căn chỉnh, chúng tôi đóng băng các tham số mô hình của LLM cơ bản, điều này cho phép nó đạt sự hội tụ nhanh hơn so với huấn luyện end-to-end từ đầu, và kế thừa khả năng lý luận của LLM tại thời điểm suy luận. Ngoài ra, để tối đa hóa tính tương thích đặc trưng, cho mỗi phương thức chúng tôi sử dụng một bộ mã hóa g(·) đã được căn chỉnh với không gian nhúng văn bản, ví dụ CLIP [30,31] cho hình ảnh, CLAP [32] cho tín hiệu âm thanh, hoặc IMU2CLIP [33] cho tín hiệu IMU. Đối với mỗi cặp chú thích văn bản và phương thức (Xtext,Xmodality), chúng tôi căn chỉnh chúng bằng cách sử dụng các mục tiêu sau với một mô-đun chiếu (tức là Perceiver Resampler [2] cho bộ mã hóa thị giác, và các lớp tuyến tính cho các phương thức khác).

p(Xtext|Xmodality) = ∏(i=1 to L) pθ(X[i]text|Zmodality,Z[1:i-1]text) (1)
Zmodality = Projectionθ(hlatents, g(Xmodality)) (2)

Tập dữ liệu: Đối với căn chỉnh hình ảnh, chúng tôi sử dụng một tập con đã được làm sạch của tập dữ liệu LAION-2B, được lọc bằng phương pháp CAT và với bất kỳ khuôn mặt có thể phát hiện nào được làm mờ [34]. Đối với căn chỉnh âm thanh, chúng tôi sử dụng AudioSet [35] (2.1M mẫu), AudioCaps [36] (46K mẫu), và CLOTHO [37] (5K mẫu). Chúng tôi sử dụng tập dữ liệu Ego4D [38] cho căn chỉnh IMU và văn bản (528K).

Lượng tử hóa: Mở rộng huấn luyện trước đến các mô hình 70B tham số cho một tập dữ liệu lớn (200M+ trường hợp) đòi hỏi tài nguyên đáng kể, thường yêu cầu một wrapper FSDP [39] để phân chia mô hình trên nhiều GPU. Để mở rộng hiệu quả việc huấn luyện của chúng tôi, chúng tôi triển khai các chiến lược lượng tử hóa (4 bit và 8 bit) [40] trong cài đặt đa phương thức của chúng tôi, trong đó chúng tôi giữ thành phần LLM của mô hình đóng băng và chỉ có các bộ token hóa phương thức có thể huấn luyện. Phương pháp này làm giảm yêu cầu bộ nhớ theo bậc độ lớn. Do đó, chúng tôi có thể huấn luyện AnyMAL 70B trên một GPU VRAM 80GB duy nhất với kích thước batch là 4.

So với FSDP, chúng tôi quan sát thấy rằng phương pháp lượng tử hóa được đề xuất đạt được cùng thông lượng trong khi chỉ sử dụng một nửa tài nguyên GPU. Chúng tôi lưu ý rằng loss huấn luyện/xác thực liên tục cao hơn so với huấn luyện FSDP, nhưng tuy nhiên không ảnh hưởng đến chất lượng tạo sinh (tại thời điểm suy luận, chúng tôi sử dụng LLM gốc ở độ chính xác đầy đủ để tối đa hóa độ chính xác).

3.2 Tinh chỉnh với Tập dữ liệu Hướng dẫn Đa phương thức

Để cải thiện thêm khả năng tuân theo hướng dẫn của mô hình đối với các phương thức đầu vào đa dạng, chúng tôi thực hiện tinh chỉnh bổ sung với tập dữ liệu điều chỉnh hướng dẫn đa phương thức (MM-IT) của chúng tôi. Cụ thể, chúng tôi nối đầu vào dưới dạng [<instruction> <modality_tokens>], sao cho mục tiêu phản hồi được căn cứ trên cả hướng dẫn văn bản và đầu vào phương thức. Chúng tôi thực hiện ablation trên (1) huấn luyện các lớp chiếu mà không thay đổi các tham số LLM, hoặc (2) sử dụng Low-Rank Adaptation [41] để tinh chỉnh thêm các hành vi LM.

Chúng tôi sử dụng cả tập dữ liệu điều chỉnh hướng dẫn được thu thập thủ công và dữ liệu tổng hợp.

Chú thích Thủ công: Trong khi có các tập dữ liệu bên thứ ba có sẵn công khai trên các nhiệm vụ VQA khác nhau, chúng tôi quan sát thấy rằng nhiều dữ liệu này có sự đa dạng và chất lượng không đủ — đặc biệt là để căn chỉnh LLM hướng tới các nhiệm vụ tuân theo hướng dẫn đa phương thức đa dạng vượt ra ngoài các truy vấn QA đơn giản (ví dụ: "Tạo một bài thơ sử dụng hình ảnh này", "Trích xuất số điện thoại trên tờ rơi này"). Do đó, chúng tôi tập trung vào việc thu thập 60K ví dụ dữ liệu điều chỉnh hướng dẫn đa phương thức chất lượng cao cho nhiều phương thức, như được minh họa trong Bảng 1. Cụ thể, chúng tôi sử dụng các hình ảnh có sẵn công khai được cấp phép Creative Commons khác nhau, và bổ sung các hình ảnh này với các hướng dẫn và phản hồi được tạo thủ công. Các chú thích viên được yêu cầu cung cấp các cặp hướng dẫn và câu trả lời hoàn toàn đa phương thức, sao cho các truy vấn không thể được trả lời mà không hiểu ngữ cảnh đa phương thức đi kèm. Chúng tôi chỉ ra rằng kết quả của chúng tôi được cải thiện đáng kể khi sử dụng những ví dụ ít hơn nhưng cân bằng tốt và chất lượng cao hơn từ các nỗ lực chú thích dựa trên nhà cung cấp của chúng tôi.

Tăng cường Tổng hợp: Ngoài dữ liệu điều chỉnh hướng dẫn ground-truth chất lượng cao ở trên, chúng tôi bổ sung tập dữ liệu bằng cách sử dụng mô hình LLaMA-2 (70B) [6], theo các phương pháp tương tự được đề xuất bởi LLaVA [20]. Cụ thể, chúng tôi sử dụng một biểu diễn văn bản của hình ảnh (tức là nhiều chú thích, thông tin hộp giới hạn và đối tượng) để tạo các cặp câu hỏi-trả lời cho hình ảnh. Chúng tôi tạo 150K cặp hình ảnh-hướng dẫn-phản hồi trên các lĩnh vực và loại câu hỏi khác nhau.

Lưu ý rằng quy trình của chúng tôi chỉ sử dụng các mô hình mã nguồn mở – trái ngược với các công việc khác sử dụng các dịch vụ thương mại như ChatGPT hoặc GPT-4.

4 Thí nghiệm

4.1 Nhiệm vụ

Chúng tôi đánh giá hiệu suất của mô hình trên hai loại nhiệm vụ trong cài đặt zero-shot: (1) các nhiệm vụ chú thích cho các phương thức khác nhau, và (2) các nhiệm vụ lý luận đa phương thức và tuân theo hướng dẫn.

Nhiệm vụ Chú thích: Chúng tôi đánh giá khả năng chính của AnyMAL trong việc tạo chú thích cho các phương thức đầu vào, điều này phù hợp với mục tiêu huấn luyện trước. Mục đích chính của nhiệm vụ chú thích là hiểu mức độ căn chỉnh giữa văn bản và các phương thức khác sau huấn luyện trước. Vì các nhiệm vụ chú thích thường không yêu cầu các bước lý luận thứ cấp, chúng tôi mong đợi rằng trọng số LLM hoặc kích thước tham số có ảnh hưởng ít hơn đến nhiệm vụ.

Nhiệm vụ Lý luận Đa phương thức: Với mức độ căn chỉnh cao giữa các phương thức, chúng tôi đánh giá khả năng lý luận và tuân theo hướng dẫn của mô hình mà nó kế thừa từ LLM được điều chỉnh hướng dẫn cốt lõi, cũng như từ quá trình điều chỉnh hướng dẫn đa phương thức.

Chúng tôi tiến hành so sánh toàn diện với các mô hình baseline mạnh cho từng cặp phương thức tương ứng (thị giác-ngôn ngữ và âm thanh-ngôn ngữ) từ tài liệu mã nguồn mở.

Lưu ý: Vì các tập dữ liệu MM-IT bao gồm một số hình ảnh trong miền từ các điểm chuẩn công khai (ví dụ: COCO), chúng tôi báo cáo kết quả riêng biệt cho các mô hình được huấn luyện trước (không có điều chỉnh hướng dẫn thêm trong Phần 3.2) và các mô hình được điều chỉnh hướng dẫn – để biểu thị cài đặt zeroshot nghiêm ngặt. Tất cả các mô hình AnyMAL được điều chỉnh hướng dẫn đa phương thức đều được đánh dấu bằng "MM-IT" trong các phần sau.

4.2 Phân tích Định lượng

Tạo Chú thích Hình ảnh: Bảng 2 hiển thị hiệu suất chú thích hình ảnh zeroshot trên COCO [48] và một tập con của tập dữ liệu MM-IT được đánh dấu bằng nhiệm vụ "mô tả chi tiết" (MM-IT-Cap). Có thể thấy rằng các biến thể AnyMAL của chúng tôi vượt trội đáng kể so với các baseline trong cả hai tập dữ liệu. Đáng chú ý là không có khoảng cách đáng kể giữa hiệu suất của các biến thể AnyMAL-13B và AnyMAL-70B. Kết quả này chỉ ra rằng khả năng LLM cơ bản có tác động nhỏ hơn đến nhiệm vụ tạo chú thích hình ảnh (tương ứng với khả năng hiểu thị giác cốt lõi), nhưng phần lớn phụ thuộc vào quy mô dữ liệu và các phương pháp căn chỉnh. Chúng tôi quy hiệu suất kém hơn một chút của AnyMAL-70B trên COCO cho tính dài dòng chung của mô hình LLaMA-70B, điều này ảnh hưởng tiêu cực đến điểm số khi được đánh giá so với các chú thích COCO có xu hướng ngắn gọn và súc tích. Như mong đợi, đánh giá tự động trên MM-IT-Cap cho thấy điểm CIDEr thấp hơn tổng thể, do độ dài phản hồi dài hơn nhiều trong các mô tả chi tiết (Xem Bảng 1 để có ví dụ).

Đánh giá Con người về Nhiệm vụ Lý luận Đa phương thức: MM-IT có đặc trưng là các cặp hướng dẫn và câu trả lời ground-truth đa phương thức đa dạng. Chúng tôi đánh giá hiệu suất của các mô hình của chúng tôi (được huấn luyện trước và được điều chỉnh hướng dẫn) so với các mô hình thị giác-ngôn ngữ khác có sẵn công khai để chạy và sử dụng (tức là LLaVA [20], MiniGPT4 [21]). Vì các phản hồi mang tính chủ quan (ví dụ: viết sáng tạo – "Viết một bài thơ sử dụng hình ảnh này"), chúng tôi tin rằng đánh giá của con người cung cấp cái nhìn chính xác nhất về hiệu suất và khả năng của mô hình được đề xuất của chúng tôi.

Do đó, chúng tôi thu thập các so sánh theo cặp cho mỗi baseline so với 1K mẫu ground-truth (Hình 3), cũng như điểm thang Likert (0-2) cho từng tiêu chí sau. Tiêu chí để xếp hạng ưu tiên bao gồm độ chính xác phản hồi, độ chính xác nhận diện đối tượng, và tính toàn vẹn (xem quy tắc đầy đủ trong Phụ lục A). Độ chính xác phản hồi đo lường liệu phản hồi có chứa thông tin liên quan, chính xác về mặt sự thật và có thể xác minh (không có bất kỳ ảo tưởng nào) liên quan đến hình ảnh và hướng dẫn. Độ chính xác nhận diện đối tượng đo lường chặt chẽ liệu các đối tượng chính có được nhận dạng chính xác ở mức chi tiết – chủ yếu liên quan đến kiến thức thị giác của mô hình. Cuối cùng, chỉ số tính toàn vẹn đo lường liệu phản hồi có cho thấy bất kỳ ngôn ngữ có hại hoặc xúc phạm nào hoặc dấu hiệu của việc đối xử bất công hoặc định kiến liên quan đến các đặc điểm như chủng tộc, tuổi tác, giới tính, quốc tịch, v.v.

Hình 3 cho thấy AnyMAL đạt hiệu suất mạnh với khoảng cách hẹp hơn so với các mẫu ground-truth được chú thích thủ công (41.1% thắng), so với các baseline (LLaVA: 34.4% thắng, và MiniGPT4: 27.0% thắng). Đáng chú ý, mô hình được tinh chỉnh với bộ hướng dẫn đầy đủ thể hiện tỷ lệ thắng ưu tiên cao nhất, cho thấy mức độ cạnh tranh của khả năng hiểu thị giác và lý luận có thể so sánh với các phản hồi được chú thích bởi con người. Cũng đáng chú ý là BLIP-2 và InstructBLIP gặp khó khăn trên những truy vấn mở này (4.1% và 16.7% thắng ưu tiên, tương ứng), mặc dù hiệu suất mạnh của chúng trong các điểm chuẩn VQA công khai (Bảng 4).

Bảng 3 cho thấy sự phân tích chi tiết điểm số cho từng tiêu chí. Cụ thể, có thể thấy rằng mô hình được điều chỉnh hướng dẫn với cả bộ được thu thập thủ công và được tuyển chọn tổng hợp đạt điểm độ chính xác và liên quan phản hồi cao nhất (cải thiện tương đối 12.2% so với baseline mạnh nhất: LLaVA). Kết quả này nêu bật khả năng nâng cao của mô hình trong việc hiểu và trả lời chính xác các câu hỏi phù hợp với các hướng dẫn được cung cấp. Đáng đề cập là mô hình, sau khi điều chỉnh hướng dẫn, thể hiện sự suy giảm trong khả năng cung cấp nhận dạng và mô tả chi tiết về đối tượng. Chúng tôi quy kết quả này cho ý định của chúng tôi đằng sau việc thu thập tập dữ liệu AnyMAL nhằm thúc đẩy các phản hồi súc tích. Do đó, mô hình có xu hướng cung cấp các mô tả ngắn gọn hơn, thay vì trình bày chi tiết (điều này thường tăng nguy cơ không chính xác về mặt sự thật). Điểm tính toàn vẹn cao (99+%) được đạt bởi tất cả các baseline.

Điểm chuẩn VQA: Bảng 4 cho thấy hiệu suất zeroshot trên tập dữ liệu Hateful Meme [49], VQAv2 [14], TextVQA [50], ScienceQA [51] (tập con được ghép đôi hình ảnh), VizWiz [52], và OKVQA [53] so với các mô hình trong tài liệu báo cáo kết quả zeroshot trên điểm chuẩn tương ứng. Chúng tôi tập trung vào đánh giá zeroshot để ước tính tốt nhất hiệu suất của mô hình trên các truy vấn mở tại thời điểm suy luận.

Tổng thể, AnyMAL của chúng tôi thể hiện hiệu suất mạnh nhất so với các baseline trên nhiều nhiệm vụ. Đáng chú ý, các mô hình AnyMAL được huấn luyện trước cho thấy hiệu suất cạnh tranh ngay cả khi không có tinh chỉnh thêm trên MM-IT – chứng minh khả năng lý luận mạnh mẽ được duy trì trong giai đoạn huấn luyện trước cơ bản. So sánh hai bộ mã hóa thị giác được hướng dẫn bởi văn bản ViT-L [30] và ViT-G [31], ViT-G đạt điểm số cao hơn trong hầu hết các tập dữ liệu thị giác. Những kết quả này cho thấy rằng các bộ mã hóa lớn hơn được huấn luyện qua nhiều bước hơn mang lại biểu diễn tốt hơn. Tuy nhiên, tập dữ liệu TextVQA trình bày một trường hợp độc đáo trong đó độ phân giải của bộ mã hóa hình ảnh đóng vai trò quan trọng trong việc nhận dạng văn bản trong hình ảnh. Do đó, mô hình ViT-L, với độ phân giải 336x336, đạt điểm số cao hơn mô hình ViT-G, có độ phân giải 224x224. DinoV2 [54], được huấn luyện trong một phương pháp tự giám sát phân biệt, cho thấy hiệu suất tồi hơn so với các bộ mã hóa thị giác được hướng dẫn bởi văn bản, chứng minh tầm quan trọng của việc căn chỉnh không gian đặc trưng. Trong số các mô hình LLM cơ sở, mô hình 70B của chúng tôi chứng minh hiệu suất mạnh mẽ nhất, nhấn mạnh ảnh hưởng của khả năng lý luận đáng kể vốn có trong các LLM lớn hơn trên các nhiệm vụ liên quan đến lý luận thị giác.

Điểm chuẩn Video QA: Chúng tôi đánh giá mô hình của chúng tôi trên ba điểm chuẩn trả lời câu hỏi video thách thức trong Bảng 6: How2QA [55], STAR [56] và NextQA [57]. Mô hình của chúng tôi chứng minh kết quả cạnh tranh so với các baseline, và đạt hiệu suất hiện đại nhất trên điểm chuẩn STAR. Lưu ý rằng chúng tôi so sánh với các phương pháp xử lý clip video đầy đủ, chưa được cắt để tạo câu trả lời. Công việc trước đây đã cho thấy các cải thiện bổ sung với các chiến lược lựa chọn khung hình cẩn thận [58]. Phương pháp của chúng tôi tương thích với các chiến lược như vậy, tuy nhiên điều đó nằm ngoài phạm vi thí nghiệm của chúng tôi. Chúng tôi cũng báo cáo các biến thể mô hình được huấn luyện độc quyền trên video từ HowTo100M [59] kết hợp với văn bản từ bản ghi ASR, và sử dụng các bộ mã hóa video rõ ràng (Internvideo [46]) thay vì bộ mã hóa hình ảnh. Tuy nhiên, những mô hình này hoạt động tồi hơn do sự căn chỉnh yếu của ASR với các clip video và tính đa dạng thấp hơn trong nội dung (tức là video hướng dẫn).

Ablation về Siêu tham số: Hình 4 cho thấy các loss huấn luyện trên các biến thể của AnyMAL trong quá trình huấn luyện trước hình ảnh-văn bản. Do chi phí tính toán cao của việc huấn luyện các mô hình 70B, chúng tôi chỉ thực hiện ablation trên các mô hình 13B. Sau đó chúng tôi sử dụng bộ siêu tham số tối ưu từ những thí nghiệm này làm cấu hình mặc định để huấn luyện các mô hình 70B. Vì các tham số có thể huấn luyện giữ nguyên nhất quán cho cả mô hình 13B và 70B, chúng tôi dự đoán rằng hành vi trong quá trình huấn luyện mô hình sẽ không thay đổi. Các siêu tham số chính sau được xem xét: các lớp của Resampler (2 so với 6), số lượng nhúng token đa phương thức để đại diện cho đầu vào thị giác (64 so với 256 token), và kích thước batch của huấn luyện trước (2,048 so với 16,384). Tổng thể, chúng tôi quan sát thấy rằng việc tăng kích thước batch và số lượng token thị giác mang lại cải thiện tối thiểu. Ngược lại, việc tăng số lượng lớp resampling giảm loss đáng kể mà không tăng đáng kể ngân sách huấn luyện.

Ảnh hưởng của Việc Mở rộng Kích thước Tham số LLM (70B so với 13B): Mô hình 70B chứng minh loss huấn luyện giảm tổng thể khi so sánh với các phiên bản 13B. Hiệu suất loss này cũng phù hợp với kết quả nhiệm vụ downstream trong Bảng 2 và 4. Chúng tôi quy kết quả này cho khả năng lý luận vốn có và kiến thức được đồng hóa trong các mô hình 70B, điều này đẩy nhanh quá trình thu nhận và căn chỉnh khái niệm thị giác. Tổng thể, sự so sánh chứng minh tầm quan trọng của việc mở rộng các tham số LLM trong huấn luyện trước thị giác-ngôn ngữ, đây là một khía cạnh ít khi được đề cập trong tài liệu hiện có.

Tạo Chú thích Âm thanh: Bảng 5 cho thấy kết quả chú thích âm thanh trên tập dữ liệu điểm chuẩn AudioCaps [36]. AnyMAL vượt trội đáng kể so với các mô hình chú thích âm thanh hiện đại khác trong tài liệu (ví dụ: +10.9pp trong CIDEr, +5.8pp trong SPICE), cho thấy tính linh hoạt của phương pháp được đề xuất trên các phương thức khác nhau ngoài thị giác. Chúng tôi lưu ý rằng mô hình 70B của chúng tôi hiển thị hiệu suất mạnh mẽ đáng chú ý so với các biến thể 7B và 13B – cho thấy tầm quan trọng của mô-đun lý luận cho nhiệm vụ.

Tạo Mô tả Chuyển động IMU: Chúng tôi sử dụng tập dữ liệu Ego4D [38] để huấn luyện một mô hình AnyMAL-7B được căn chỉnh IMU, tận dụng dữ liệu cảm biến IMU được đồng bộ hóa và mô tả văn bản được cung cấp trong tập dữ liệu. Vì nhiệm vụ tạo mô tả văn bản từ tín hiệu chuyển động chưa từng có thể thực hiện hoặc được báo cáo trước đây, chúng tôi chỉ trình bày hiệu suất đạt được bởi mô hình của chúng tôi. Trên tập kiểm tra được giữ lại, chúng tôi đạt 52.5 CIDEr và 23.2 ROUGE-L so với các chú thích ground-truth, cho thấy tính khả thi của nhiệm vụ mới được đề xuất.

Kết hợp khả năng chú thích này với khả năng lý luận của LLM, trong Bảng 8 chúng tôi cho thấy các ví dụ về các ứng dụng mới mà AnyMAL có thể cho phép, ví dụ: suy luận trạng thái chuyển động của người dùng và kết hợp chúng như một phần của phản hồi (ví dụ: "Cách nào an toàn nhất để dừng lại?" → "Để dừng xe đạp an toàn, ..." mà không có bất kỳ gợi ý văn bản hoặc thị giác nào rằng người dùng đang đạp xe).

4.3 Phân tích Định tính

So sánh với các Mô hình Thị giác-Ngôn ngữ khác: Bảng 9 và 10 cho thấy đầu ra từ các mô hình thị giác-ngôn ngữ khác nhau [17,19,20,21] trên các cặp hình ảnh và gợi ý đa dạng, so với AnyMAL (LLaVA-70B). Có thể thấy rằng AnyMAL thể hiện khả năng hiểu thị giác mạnh mẽ (như nhận dạng đối tượng và trạng thái của chúng), cũng như khả năng tạo ngôn ngữ. Trong khi MiniGPT4 [21] và LLaVA [20] thật sự trình bày các phản hồi hợp lý và trôi chảy, độ chính xác của chúng không được đảm bảo nhất quán. Những ví dụ này làm nổi bật hiệu quả các lợi ích của phương pháp được đề xuất cho phép huấn luyện trước quy mô lớn bao phủ các khái niệm thị giác đa dạng, trong khi kế thừa khả năng lý luận mạnh mẽ xuất phát từ các LLM được điều chỉnh hướng dẫn.

Chúng tôi lưu ý rằng chúng tôi sử dụng các checkpoint mới nhất được cung cấp cho mỗi baseline để tạo phản hồi.

Phương thức Xen kẽ: Kiến trúc mô hình linh hoạt của AnyMAL cho phép các phương thức kết hợp làm ngữ cảnh điều kiện (ví dụ: hình ảnh + tín hiệu cảm biến chuyển động IMU), điều này cho phép lý luận đa phương thức toàn diện hơn. Chúng tôi chứng minh khả năng zeroshot của mô hình trong việc xử lý các phương thức xen kẽ như vậy trong Bảng 11 (ví dụ: soạn thảo tin nhắn với một hình ảnh đã cho (Cầu Golden Gate), với chuyển động phổ biến của người dùng (đạp xe) như một phần của ngữ cảnh).

Kết quả này minh họa cách tương tác mới và tự nhiên với một mô hình AI được AnyMAL làm cho có thể, trong đó người dùng có thể giả định sự hiểu biết chung về các nhận thức cảm giác kết hợp (ví dụ: gợi ý thị giác, thính giác, và chuyển động) khi soạn thảo truy vấn – tránh nhu cầu phải chỉ định ngữ cảnh đa phương thức.

5 An toàn

Tính Toàn vẹn Thời gian Suy luận: Để đảm bảo an toàn và tính toàn vẹn của mô hình AnyMAL, một số biện pháp được thực hiện trên các loại vi phạm tính toàn vẹn tiềm năng sau: (1) hình ảnh đầu vào, (2) gợi ý văn bản đầu vào, (3) đầu ra văn bản, và (4) sự kết hợp đa phương thức của hình ảnh đầu vào và đầu ra văn bản.

(1) Hình ảnh đầu vào: chúng tôi sử dụng một bộ phân loại hình ảnh được huấn luyện trước dựa trên RegNetY [60] để phát hiện bất kỳ nội dung nào vi phạm tiêu chuẩn tính toàn vẹn. Việc phát hiện này bao gồm tài liệu đồ họa, hình ảnh bạo lực, biểu tượng thù hận, trường hợp bắt nạt, quấy rối, v.v. Nếu một vi phạm như vậy được xác định trong hình ảnh, chúng tôi tiến hành từ chối toàn bộ truy vấn.

(2) Gợi ý văn bản đầu vào: chúng tôi sử dụng một bộ phân loại văn bản dựa trên RoBERTa [61] được huấn luyện để phát hiện các phát ngôn vi phạm tính toàn vẹn như bạo lực, quấy rối, lời nói thù hận, v.v. Khi một vi phạm được phát hiện trong gợi ý người dùng, chúng tôi tiến hành từ chối toàn bộ truy vấn.

(3) Văn bản đầu ra: chúng tôi sử dụng cùng bộ phân loại văn bản trong (b) để phát hiện bất kỳ vấn đề nào trong đầu ra được tạo. Đối với các trường hợp sử dụng streaming, chúng tôi chạy bộ phân loại cho mỗi câu để nhanh chóng xác định bất kỳ vi phạm nào.

(4) Liên kết đa phương thức trong hình ảnh đầu vào & văn bản đầu ra: trong trường hợp không phổ biến khi văn bản vô hại và hình ảnh vô hại (tự chúng có vẻ vô tội) có thể dẫn đến vấn đề khi chúng được liên kết, chúng tôi sử dụng một bộ phân loại đa phương thức để phát hiện các trường hợp như vậy.

An toàn Thời gian Huấn luyện: Các tập dữ liệu được sử dụng để huấn luyện trước (ví dụ: [34,62]) đã trải qua quá trình lọc để loại bỏ ngôn ngữ có hại hoặc hình ảnh làm tổn hại tính toàn vẹn, do đó làm giảm tiềm năng cho mô hình tạo ra nội dung vi phạm tiêu chuẩn tính toàn vẹn.

An toàn LLM: Vì huấn luyện trước AnyMAL của chúng tôi không thay đổi các tham số của LLM cơ sở, chúng tôi kế thừa các biện pháp phòng ngừa an toàn tương tự được triển khai cho việc tạo ngôn ngữ của nó. Ví dụ, LLaMA-2 (phiên bản chúng tôi báo cáo hầu hết kết quả) đặt các biện pháp bảo vệ như tinh chỉnh ví dụ tiêu cực, học tăng cường với phản hồi của con người (RLHF) [63, 64, 65].

6 Kết luận

AnyMAL được đề xuất của chúng tôi thể hiện một cách mới và tự nhiên để tương tác với một mô hình AI, ví dụ: đặt câu hỏi giả định sự hiểu biết chung về thế giới giữa người dùng và tác nhân, thông qua cùng một lăng kính và các nhận thức kết hợp (ví dụ: gợi ý thị giác, thính giác, và chuyển động). Cách huấn luyện AnyMAL có thể mở rộng được đề xuất làm cho có thể tận dụng khả năng lý luận mạnh mẽ của mô hình ngôn ngữ LLaMA-2 trong cài đặt đa phương thức.

Các đóng góp của chúng tôi như sau: (1) Chúng tôi trình bày một LLM Đa phương thức quy mô lớn (AnyMAL), được huấn luyện bằng cách sử dụng tài nguyên mã nguồn mở và các giải pháp có thể mở rộng cho nhiều phương thức. (2) Chúng tôi giới thiệu tập dữ liệu Điều chỉnh Hướng dẫn Đa phương thức (MM-IT), một bộ sưu tập đầu tiên về các chú thích thủ công chất lượng cao của dữ liệu hướng dẫn đa phương thức. (3) Phân tích thực nghiệm toàn diện của chúng tôi cho thấy cái nhìn sâu sắc về công thức hiệu quả và có thể mở rộng để xây dựng một mô hình lý luận đa phương thức, với các LLM và lựa chọn mô hình khác nhau.

7 Hạn chế

Chúng tôi thảo luận về những hạn chế hiện tại của công việc như sau.

Thứ nhất, phương pháp mô hình ngôn ngữ đa phương thức nhân quả được đề xuất vẫn gặp thách thức trong việc thiết lập sự căn cứ mạnh mẽ với phương thức đầu vào. Cụ thể, chúng tôi quan sát thấy rằng trong quá trình tạo sinh, mô hình đôi khi ưu tiên tập trung nhiều hơn vào văn bản được tạo thay vì hình ảnh đầu vào. Điều này dẫn đến việc tạo ra đầu ra kết hợp những thành kiến thu được từ mô hình ngôn ngữ cơ bản (LLM), có thể gây ra những không chính xác khi so sánh với ngữ cảnh hình ảnh. Chúng tôi mong đợi rằng các điều chỉnh kiến trúc bổ sung hoặc việc mở đóng băng các tham số LLM là cần thiết để giải quyết hạn chế này một cách hiệu quả (mặc dù chi phí tính toán cao hơn nhiều mà nó có thể đòi hỏi).

Thứ hai, trong khi chúng tôi tăng đáng kể kích thước của tập dữ liệu huấn luyện trước, việc hiểu các khái niệm và thực thể thị giác vẫn bị hạn chế bởi số lượng dữ liệu hình ảnh-văn bản được ghép đôi trong quá trình huấn luyện. Trong lĩnh vực mô hình ngôn ngữ chỉ văn bản, thường quan sát thấy các phương pháp kết hợp truy xuất kiến thức bên ngoài nâng cao đáng kể khả năng của mô hình để vượt qua các hạn chế kiến thức. Những phương pháp này cung cấp một phương tiện tiềm năng để giảm bớt những hạn chế được đề cập trước đó.

Cuối cùng, trong phạm vi công việc của chúng tôi, việc thích ứng đa phương thức của một LLM bị giới hạn bởi bốn phương thức: hình ảnh, video, âm thanh, và tín hiệu IMU. Trong khi chúng tôi tin rằng phương pháp được đề xuất có tiềm năng bao gồm bất kỳ phương thức nào khác, với điều kiện tồn tại một tập dữ liệu được ghép đôi, hiệu quả của nó đối với các phương thức như vậy vẫn cần được chứng minh.
