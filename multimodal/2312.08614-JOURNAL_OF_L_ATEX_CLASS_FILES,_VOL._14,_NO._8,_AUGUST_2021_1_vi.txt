# 2312.08614.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.08614.pdf
# Kích thước tệp: 6191624 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 1
Factorization Vision Transformer: Mô hình hóa Phụ thuộc Tầm xa với Chi phí Cửa sổ Cục bộ
Haolin Qin∗, Daquan Zhou∗, Tingfa Xu†, Ziyang Bian, Jianan Li†
Tóm tắt —Transformers có sức mạnh biểu diễn đáng kinh ngạc nhưng thường tiêu tốn tính toán đáng kể với độ phức tạp bậc hai theo độ phân giải hình ảnh. Swin transformer phổ biến giảm chi phí tính toán thông qua chiến lược cửa sổ cục bộ. Tuy nhiên, chiến lược này không thể tránh khỏi gây ra hai hạn chế: (1) self-attention dựa trên cửa sổ cục bộ cản trở khả năng mô hình hóa phụ thuộc toàn cục; (2) các nghiên cứu gần đây chỉ ra rằng các cửa sổ cục bộ làm giảm tính bền vững. Để vượt qua những thách thức này, chúng tôi theo đuổi một sự cân bằng tốt hơn giữa chi phí tính toán và hiệu suất. Theo đó, chúng tôi đề xuất một cơ chế self-attention phân tích mới (FaSA) vừa tận hưởng lợi thế của chi phí cửa sổ cục bộ vừa có khả năng mô hình hóa phụ thuộc tầm xa. Bằng cách phân tích ma trận attention thông thường thành các ma trận sub-attention thưa thớt, FaSA nắm bắt các phụ thuộc tầm xa trong khi tổng hợp thông tin đa cấp độ với chi phí tính toán tương đương self-attention dựa trên cửa sổ cục bộ. Tận dụng FaSA, chúng tôi giới thiệu factorization vision transformer (FaViT) với cấu trúc phân cấp. FaViT đạt được hiệu suất cao và tính bền vững, với độ phức tạp tính toán tuyến tính theo độ phân giải không gian của hình ảnh đầu vào. Các thí nghiệm mở rộng đã chứng minh hiệu suất vượt trội của FaViT trong phân loại và các tác vụ downstream. Hơn nữa, nó cũng thể hiện tính bền vững mạnh mẽ với dữ liệu bị hỏng và thiên lệch và do đó cho thấy lợi ích cho các ứng dụng thực tế. So với mô hình cơ sở Swin-T, FaViT-B2 của chúng tôi cải thiện đáng kể độ chính xác phân loại 1% và tính bền vững 7%, đồng thời giảm tham số mô hình 14%. Mã của chúng tôi sẽ sớm được công bố tại https://github.com/q2479036243/FaViT.
Từ khóa chỉ mục —Transformer, phân tích, phụ thuộc tầm xa, cửa sổ cục bộ, tính bền vững mô hình.

I. GIỚI THIỆU
KỂ TỪ thành công vĩ đại của Alexnet [1], những cải tiến mang tính cách mạng đã đạt được thông qua việc mở rộng quy mô mô hình lên quy mô lớn hơn với một số công thức đào tạo tiên tiến [2]. Với sự hỗ trợ của AutoML [3], các mạng nơ-ron tích chập (CNN) đã đạt được hiệu suất tiên tiến trên nhiều tác vụ thị giác máy tính [4], [5]. Mặt khác, các transformer phổ biến gần đây đã cho thấy hiệu suất vượt trội so với CNN thống trị trước đây [6], [7]. Sự khác biệt cơ bản giữa transformer và CNN nằm ở khả năng mô hình hóa phụ thuộc tầm xa của chúng. Các vision transformer thông thường chia hình ảnh đầu vào thành chuỗi các patch, được xử lý đồng thời, từ đó tạo ra sự gia tăng bậc hai về chi phí tính toán so với độ phân giải không gian đầu vào.

Haolin Qin, Tingfa Xu, Jianan Li và Ziyang Bian thuộc Viện Công nghệ Bắc Kinh, 100081 Bắc Kinh, Trung Quốc. E-mail: {3120225333, ciom xtf1, lijianan, 3220185049 }@bit.edu.cn.
Tingfa Xu cũng thuộc Trung tâm Đổi mới Chongqing Viện Công nghệ Bắc Kinh, 401135 Chongqing, Trung Quốc.
Daquan Zhou thuộc ByteDance TikTok, Singapore.
Ziyang Bian cũng thuộc Viện Nghiên cứu Điện tử Quang học Bắc Trung Quốc, 100015 Bắc Kinh, Trung Quốc.
∗Đóng góp bình đẳng.†Liên hệ với: Tingfa Xu và Jianan Li.

Hình 1. So sánh kết quả trực quan hóa attention span của ViT, Swin và FaViT của chúng tôi. (a) ViT có attention span toàn cục nhưng tính toán chuyên sâu. (b) Swin transformer hiệu quả nhưng kém hơn trong việc mô hình hóa phụ thuộc tầm xa. (c) FaViT đề xuất phân tích token và do đó thành công trong việc mô hình hóa phụ thuộc tầm xa với chi phí cửa sổ cục bộ.

Kết quả là, transformer tiêu tốn chi phí tính toán cao hơn đáng kể so với CNN, hạn chế tính khả thi của chúng trong các thiết bị hạn chế tài nguyên.
Để giải quyết vấn đề trên, một số phương pháp đã được đề xuất để giảm bớt gánh nặng tính toán. Ví dụ, Swin transformer [8] áp dụng chiến lược cửa sổ cục bộ trong đó token được chia thành nhiều cửa sổ, và việc tính toán self-attention được giới hạn trong những cửa sổ được định nghĩa trước này. Do đó, chi phí tính toán thay đổi thành bậc hai với kích thước cửa sổ, được cố ý đặt nhỏ hơn đáng kể so với độ phân giải không gian của hình ảnh đầu vào. So với vision transformer thông thường (ViT) [9], Swin transformer giảm chi phí đáng kể. Tuy nhiên, chiến lược cửa sổ cục bộ này không thể tránh khỏi làm giảm khả năng mô hình hóa phụ thuộc tầm xa. Như minh họa trong Hình 1 (b), Swin transformer chỉ nắm bắt mối quan hệ trong một vùng cục bộ, dẫn đến mất mát phụ thuộc tầm xa. Ngoài ra, thiệt hại của chiến lược này đối với tính bền vững mô hình đã được chứng minh bởi các nghiên cứu gần đây [10].

Sự cân bằng giữa chi phí tính toán và khả năng mô hình hóa phụ thuộc tầm xa do đó trở thành một thách thức cơ bản cần được khám phá. Trong bài báo này, chúng tôi đề xuất một cơ chế self-attention mới được gọi là factorization self-attention (FaSA). Hoạt động cốt lõi của FaSA nằm ở quá trình phân tích, được minh họa trong Hình 2. Chúng tôi phân tích ma trận attention thông thường thành nhiều ma trận sub-attention thưa thớt sao cho sự chồng chất của các ma trận sub-attention hiệu quả xấp xỉ ma trận attention đầy đủ ban đầu.

Về mặt thực tế, với hình ảnh đầu vào, chúng tôi lấy mỗi điểm riêng lẻ làm query. Để thu thập key, chúng tôi chia đều hình ảnh thành một chuỗi các cửa sổ cục bộ không chồng lấp. Tiếp theo, chúng tôi đồng đều lấy mẫu một số lượng điểm cố định từ mỗi cửa sổ thông qua lấy mẫu dilated và kết hợp các đặc trưng của các điểm được lấy mẫu tại cùng vị trí trong các cửa sổ khác nhau. Vì số lượng key bị hạn chế nghiêm ngặt và mỗi key kết hợp thông tin trải dài qua nhiều cửa sổ khác nhau trên toàn bộ hình ảnh, việc chú ý đến tập hợp key như vậy cho phép mô hình hóa phụ thuộc tầm xa với chi phí cửa sổ cục bộ. Xem xét rằng mỗi key thu được kết hợp thông tin đa điểm, có thể dẫn đến sự thiếu hụt chi tiết mịn, chúng tôi tiếp tục giới thiệu mixed-grained multi-head attention. Cụ thể, chúng tôi từng bước tăng kích thước cửa sổ cục bộ cho các head khác nhau và động điều chỉnh tỷ lệ dilation của việc lấy mẫu điểm để giữ số lượng key qua tất cả các head giống nhau. Kết quả là, các key thu được kết hợp đặc trưng từ ít vị trí hơn và có nhiều chi tiết mịn hơn mà không thêm chi phí tính toán bổ sung. Bằng cách tổng hợp các đặc trưng được chú ý từ nhiều head, có thể thu được thông tin tầm xa và đa cấp độ đồng thời.

Dựa trên FaSA đề xuất, chúng tôi trình bày các biến thể của transformer với các khả năng khác nhau được gọi là factorization vision transformer (FaViT). Như thể hiện trong Hình 1 (c), FaViT của chúng tôi tận hưởng hai lợi thế thiết yếu, xuất phát từ sự đổi mới của FaSA, vốn trước đây không thể đạt được trong các transformer hiện có. Thứ nhất, mỗi cửa sổ cục bộ tạo ra một số lượng token giống hệt nhau, đảm bảo chi phí tính toán cố định. Do đó, việc nắm bắt phụ thuộc tầm xa xảy ra mà không phát sinh chi phí phụ. Thứ hai, việc kết hợp các ma trận sub-attention trong FaViT thúc đẩy việc tổng hợp thông tin ở các mức độ chi tiết khác nhau.

Hình 2. Phân tích với nhiều tỷ lệ dilation. Chúng tôi phân tích ma trận attention thông thường thành nhiều ma trận sub-attention và thu được thông tin tầm xa và đa cấp độ với độ phức tạp tính toán tuyến tính theo độ phân giải hình ảnh.

Hình 3. So sánh về sự cân bằng giữa độ chính xác và tính bền vững. FaViT đề xuất đạt được hiệu suất tốt nhất trong cả độ chính xác phân loại và tính bền vững mô hình, với ít tham số hơn được biểu thị bằng kích thước vòng tròn.

Các thí nghiệm mở rộng xác nhận hiệu suất đặc biệt và tính bền vững vượt trội của FaViT đề xuất. Như mô tả trong Hình 3, FaViT liên tục vượt trội so với các mô hình cạnh tranh có kích thước tương tự về độ chính xác phân loại và tính bền vững. Đáng chú ý, so với mô hình cơ sở Swin transformer, FaViT-B2 vượt trội so với Swin-T trong tất cả các khía cạnh. Tính bền vững được cải thiện đáng kể 7%, và độ chính xác phân loại được cải thiện 1%, trong khi các tham số giảm đáng kể 14%. Hơn nữa, FaViT của chúng tôi cũng thể hiện hiệu suất ấn tượng trên nhiều tác vụ downstream như phát hiện đối tượng và phân đoạn ngữ nghĩa.

Tóm lại, công trình này có những đóng góp sau:
• Chúng tôi đề xuất một cơ chế factorization self-attention mới (FaSA), có khả năng mô hình hóa phụ thuộc tầm xa trong khi tổng hợp thông tin đa cấp độ với chi phí cửa sổ cục bộ.
• Dựa trên FaSA, chúng tôi trình bày một factorization vision transformer hiệu quả (FaViT), tìm được sự cân bằng giữa chi phí và hiệu suất và thể hiện tính bền vững tiên tiến.

II. CÔNG TRÌNH LIÊN QUAN
A. Vision Transformers
Transformer ban đầu được phát triển cho các tác vụ NLP [11], [12], nhưng hiện đã đạt được thành công đáng kể trên nhiều lĩnh vực [13]–[16]. Trong các tác vụ thị giác máy tính, ViT [9] là nỗ lực tiên phong để điều chỉnh transformer cho xử lý hình ảnh. Nó chia hình ảnh đầu vào thành một chuỗi token để mã hóa và xây dựng cấu trúc mạng không có convolution thông qua self-attention. DeiT [17] giới thiệu một loạt chiến lược đào tạo để làm cho ViT hoạt động trên bộ dữ liệu nhỏ hơn ImageNet-1K, thay thế JFT-300M quy mô lớn đã được sử dụng trước đây. Xây dựng

--- TRANG 2 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 2

--- TRANG 3 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 3
dựa trên nền tảng của ViT, các nỗ lực tiếp theo [18]–[20] cải tiến kiến trúc của nó để đạt được hiệu suất tăng cường. Ví dụ, Yu et al. [21] trừu tượng hóa kiến trúc MetaFormer tổng quát và khám phá nguồn gốc của tính cạnh tranh của transformer và các biến thể của chúng. Jiao et al. [22] sử dụng các trường tiếp nhận thưa thớt để giảm dư thừa toàn cục và cải thiện khả năng biểu hiện đặc trưng của transformer. Về bản chất, tác động chuyển đổi của transformer đã được mở rộng sang lĩnh vực thị giác máy tính và dần dần thay thế sự thống trị của CNN.

B. Biến thể Hiệu quả
Một làn sóng phương pháp gần đây đã xuất hiện để giảm bớt nhu cầu tính toán của vision transformer. Các phương pháp này có thể được phân loại thành hai chiến lược theo mối quan hệ giữa độ phức tạp tính toán và độ phân giải không gian của hình ảnh đầu vào: (1) self-attention dựa trên pyramid; và (2) self-attention dựa trên cửa sổ cục bộ. PVTv1 [23] phục vụ như một ví dụ đại diện của chiến lược đầu tiên. Nó giới thiệu spatial reduction và token fusion để giảm chi phí của multi-head attention. PVTv2 [24] được đề xuất tiếp theo cải thiện nó bằng cách giới thiệu overlapping patch embedding, depth-wise convolution [25], và linear Spatial Reduction Attention. Một shunted self-attention [26] được đưa ra để thống nhất việc trích xuất đặc trưng đa tỷ lệ thông qua tổng hợp token đa tỷ lệ. Điểm mấu chốt của chiến lược này là giảm chi phí thông qua nén không gian. Do đó, độ phức tạp tính toán của các mô hình self-attention dựa trên pyramid nói trên vẫn về cơ bản là bậc hai với độ phân giải hình ảnh. Vấn đề chi phí tính toán cao vẫn tồn tại khi xử lý hình ảnh độ phân giải cao. Đồng thời, thông tin của các mục tiêu nhỏ và kết cấu tinh tế sẽ bị áp đảo, phá hủy các đặc trưng mịn.

C. Local Self-attention
Self-attention dựa trên cửa sổ cục bộ chia hình ảnh thành các tập hợp riêng biệt và giới hạn các tính toán self-attention trong mỗi tập hợp. Do đó, độ phức tạp tính toán của các mô hình sử dụng cửa sổ cục bộ về mặt lý thuyết là tuyến tính với độ phân giải không gian của hình ảnh đầu vào. Trong số các mô hình này, Swin transformer [8] nổi bật là mô hình nổi bật nhất. Nó giới thiệu cấu trúc cửa sổ trượt phân cấp và sử dụng hoạt động Shift để trao đổi thông tin giữa các tập hợp. MOA [27] khai thác thông tin lân cận và toàn cục giữa tất cả các cửa sổ không cục bộ. SimViT [28] tích hợp cấu trúc không gian và kết nối liên cửa sổ của cửa sổ trượt vào visual transformer. Mặc dù chiến lược cửa sổ cục bộ giảm đáng kể chi phí tính toán, nhưng nhược điểm của nó không thể bỏ qua. Việc thiếu phụ thuộc tầm xa hạn chế khả năng biểu diễn, trong khi các cửa sổ cục bộ quá nhiều sẽ làm tổn hại đến tính bền vững của mô hình. Các công trình trước đây không thể khôi phục phụ thuộc tầm xa trong khi duy trì chi phí. Những vấn đề này thúc đẩy chúng tôi đề xuất FaViT để mô hình hóa phụ thuộc tầm xa với chi phí tính toán self-attention dựa trên cửa sổ cục bộ.

D. Tính bền vững Mô hình
So với CNN, transformer thường có tính bền vững mô hình mạnh hơn chống lại các tham nhũng và thiên lệch khác nhau, nhờ việc sử dụng cơ chế self-attention [29], [30]. Tuy nhiên, một số biến thể dựa trên transformer sử dụng các chiến lược nhằm tăng cường hiệu suất và giảm chi phí, điều này vô tình làm tổn hại đến tính bền vững của mô hình [10]. Do đó, chúng tôi đã tích hợp tính bền vững mô hình như một tiêu chí cơ bản trong khung đánh giá hiệu suất của chúng tôi. Đánh giá tính bền vững không còn dựa duy nhất vào các bộ dữ liệu hình ảnh sạch như ImageNet-1K [31]. Thay vào đó, việc đánh giá dựa trên ImageNetC biểu thị các hình ảnh bị hỏng có nguồn gốc từ bộ dữ liệu gốc theo cách tương tự như cách tiếp cận được đề xuất trong [32]. Ngoài ra, hiệu suất của mô hình chống lại nhiễu nhãn và mất cân bằng lớp cũng phục vụ như một chỉ báo về tính bền vững của nó. Quan điểm này được Li et al. [33] ủng hộ, người đã sử dụng hình ảnh có nhãn nhiễu trong thế giới thực, và Tian et al. [34] sử dụng bộ dữ liệu phân phối đuôi dài để phân tích tính bền vững mô hình thông qua độ chính xác phân loại. Lấy cảm hứng từ những đóng góp này, khung đánh giá toàn diện của chúng tôi đo lường tính bền vững của mô hình trong các tình huống tham nhũng hình ảnh, nhiễu nhãn và mất cân bằng lớp.

E. Phân tích Ma trận
Gần đây, việc xây dựng ma trận thưa thớt để giảm dư thừa mạng và tăng cường hiệu quả trích xuất đặc trưng đã nổi lên như một cách tiếp cận đầy hứa hẹn trên nhiều tác vụ khác nhau [35]–[37]. Cách tiếp cận này đã truyền cảm hứng cho việc xây dựng các chiến lược nhằm giảm chi phí tính toán của transformer. Một ví dụ đáng chú ý là VSA [38], giới thiệu một module hồi quy cửa sổ để dự đoán vùng attention nơi các token key và value được lấy mẫu. Loại phương pháp này nỗ lực tìm các token thưa thớt có thể thay thế hiệu quả tất cả các token. Bên cạnh đó, lấy mẫu dilated cũng là một chiến lược hiệu quả để đạt được biểu diễn thưa thớt [39]–[41]. Ví dụ, Jiao et al. [22] đã thiết kế DilateFormer, giới thiệu một tỷ lệ dilation bổ sung trong cửa sổ trượt của Swin transformer. Các phương pháp này tận dụng convolution dilated để trích xuất bản đồ đặc trưng thưa thớt, từ đó mở rộng trường tiếp nhận trong khi giảm chi phí tính toán. Tuy nhiên, lấy mẫu dilated sẽ mất tính liên tục thông tin, điều này bị bỏ qua bởi các phương pháp trên, dẫn đến mất mát các đặc trưng mịn. Để kết thúc việc này, bài báo này đề xuất FaSA để đạt được phân tích ma trận attention và thu được thông tin tầm xa và đa cấp độ đồng thời.

III. PHƯƠNG PHÁP
Trong phần này, chúng tôi trình bày một cái nhìn tổng quan toàn diện về factorization vision transformer (FaViT) đề xuất. Trong Phần III-A, chúng tôi xem xét các nguyên tắc cơ bản của transformer. Chúng tôi cung cấp một bảng so sánh ký hiệu chính tạo điều kiện cho việc hiểu rõ hơn về những khác biệt chính giữa FaViT và các mô hình hiện có. Phần III-B dành riêng cho việc làm rõ đổi mới cốt lõi của chúng tôi, factorization self-attention (FaSA). Cơ chế mới này trao quyền cho FaViT để mô hình hóa phụ thuộc tầm xa với chi phí cửa sổ cục bộ. Trong Phần III-C, chúng tôi mô tả khung tổng thể và các biến thể mô hình của FaViT. Phần III-D dành riêng cho việc phân tích tỉ mỉ về độ phức tạp tính toán của FaViT.

--- TRANG 4 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 4

Hình 4. Kiến trúc tổng thể của FaViT đề xuất. Chúng tôi chia các attention head thành nhiều nhóm và gán tỷ lệ dilation tăng dần. Mỗi nhóm được phân chia đều thành các cửa sổ cục bộ không chồng lấp và thu thập cùng một số lượng key thông qua lấy mẫu dilated để thu được thông tin tầm xa và đa cấp độ đồng thời.

A. Kiến thức cơ bản
Cho hình ảnh đầu vào I∈RH×W×3, trong đó H, W lần lượt đại diện cho chiều cao và chiều rộng của nó. Cơ chế self-attention thông thường (SA) được sử dụng trong ViT đầu tiên mã hóa nó thành bản đồ đặc trưng X∈RN×C bằng patch embedding, trong đó N = H × W đại diện cho độ phân giải không gian của hình ảnh đầu vào và C đại diện cho các chiều kênh của nó. Tiếp theo, SA sử dụng các embedding tuyến tính, được tham số hóa bởi các ma trận trọng số WK,WQ,WV∈RC×C, để nhúng tất cả các điểm thành key K=WKX∈RN×C, query Q=WQX∈RN×C, và value V=WVX∈RN×C, tương ứng. Sau đó, bản đồ đặc trưng attention được tạo ra như được chỉ ra bởi phương trình sau:

SA(X) =Softmax (QK⊤/√h)V, (1)

trong đó √h là một hệ số tỷ lệ. Kết quả là, độ phức tạp tính toán của ViT được cho bởi:

Ω(ViT) = 4NC2+ 2N2C, (2)

và độ phức tạp này tăng theo bậc hai cùng với sự gia tăng độ phân giải không gian của hình ảnh đầu vào. Cuối cùng, transformer chuyển đổi các đặc trưng được chú ý thông qua việc áp dụng một lớp feedforward, tạo ra các bản đồ đặc trưng cuối cùng được điều chỉnh cho các tác vụ thị giác cụ thể. Đáng chú ý, hầu hết các lớp feedforward dựa trên Multilayer Perceptron (MLP), thường bao gồm hai lớp tuyến tính và một lớp GELU.

Swin transformer áp dụng local window-based self-attention (WSA) để giảm chi phí tính toán. WSA chia bản đồ đặc trưng X thành các cửa sổ không chồng lấp và thực hiện tính toán self-attention trong mỗi cửa sổ một cách riêng biệt. Giả sử mỗi cửa sổ chứa M×M token, độ phức tạp tính toán của Swin transformer có thể được biểu diễn như:

Ω(Swin) = 4NC2+ 2M2NC. (3)

Đáng chú ý, độ phức tạp này thể hiện sự tăng trưởng tuyến tính đối với độ phân giải đầu vào cho M cố định.

Một so sánh giữa Phương trình 2 và Phương trình 3 nêu bật rằng SA có attention span toàn cục nhưng trở nên tính toán chuyên sâu cho độ phân giải đầu vào cao. Ngược lại, WSA đạt được hiệu quả cao hơn trong khi hy sinh khả năng mô hình hóa phụ thuộc tầm xa, do đó ảnh hưởng đến hiệu suất và tính bền vững. Để giải quyết những hạn chế này, Swin transformer giới thiệu hoạt động Shift để tạo điều kiện tương tác thông tin giữa các cửa sổ liền kề. Tuy nhiên, hoạt động Shift chỉ được sử dụng giữa các khối WSA, và việc mở rộng phạm vi mô hình hóa cần phải kết nối nhiều khối nối tiếp. Những hạn chế trên thúc đẩy chúng tôi khám phá một cơ chế self-attention mới có thể mô hình hóa phụ thuộc tầm xa hiệu quả trong khi tuân thủ các lợi thế tính toán của chiến lược cửa sổ cục bộ. Điều đáng chú ý là các ký hiệu chính được sử dụng xuyên suốt lời giải thích này được tóm tắt trong Bảng I, cung cấp tham chiếu cho các khái niệm cốt lõi được giới thiệu.

B. Factorization Self-attention
Hình 4 cung cấp một biểu diễn trực quan về cơ chế factorization self-attention (FaSA) đề xuất. Bản chất của FaSA nằm ở việc phân tích ma trận attention thông thường thành nhiều ma trận sub-attention thưa thớt. Phương pháp phân tích này thúc đẩy việc tích hợp các đặc trưng trên nhiều mức độ chi tiết khác nhau, đỉnh điểm là một phép xấp xỉ của khả năng biểu diễn đặc trưng được thể hiện bởi ma trận attention đầy đủ. Quan trọng là, việc xấp xỉ này được đạt được trong khi tuân thủ mối quan hệ tuyến tính giữa độ phức tạp tính toán và độ phân giải không gian của hình ảnh đầu vào.

Cụ thể, trước tiên chúng tôi chia đồng đều bản đồ đặc trưng đầu vào X∈RN×C thành nhiều nhóm. Mỗi nhóm độc lập trải qua một quá trình self-attention nhằm nắm bắt các đặc trưng tầm xa ở một mức độ chi tiết cụ thể. Về self-attention trong mỗi nhóm, chúng tôi lấy tất cả các điểm của bản đồ đặc trưng làm query token và thu thập key token trong ba bước:
1) Localization, chia đều toàn bộ bản đồ đặc trưng thành một chuỗi các cửa sổ cục bộ.
2) Dilated sampling, đồng đều lấy mẫu một số lượng điểm cố định trong mỗi cửa sổ cục bộ.
3) Cross-window fusion, kết hợp các đặc trưng của các điểm được lấy mẫu tại cùng vị trí trong các cửa sổ khác nhau.

Do đó, các key token được kết hợp kết quả bao gồm thông tin tầm xa từ nhiều cửa sổ trải dài trên toàn bộ bản đồ đặc trưng.

Để chống lại sự mất mát tiềm ẩn của các chi tiết mịn trong các key token được tạo ra do tính thưa thớt của ma trận, chúng tôi giới thiệu mixed-grained multi-head attention. Điều này bao gồm việc tăng dần kích thước cửa sổ cục bộ trên các nhóm head riêng biệt, cùng với sự gia tăng thích ứng trong tỷ lệ dilation tương ứng cho việc lấy mẫu điểm. Bước này đảm bảo số lượng điểm được lấy mẫu nhất quán bất chấp sự thay đổi kích thước cửa sổ, dẫn đến các key token với thông tin đa cấp độ. Tiếp theo, chúng tôi đi sâu vào nguyên tắc và triển khai của từng bước trong FaSA.

1) Nhóm Head: Chúng tôi bắt đầu quá trình bằng cách phân chia đồng đều bản đồ đặc trưng đầu vào X thành nhiều nhóm theo chiều kênh:

X={Xi∈RN×C′,|i= 1,···,G}, (4)

trong đó G là số nhóm head, Xi là bản đồ đặc trưng được gán cho nhóm thứ i, và C′= C/G. Chúng tôi lấy các đặc trưng được chia làm đầu vào cho các nhóm attention head riêng biệt, mỗi nhóm thực hiện factorization self-attention độc lập. Sự cô lập này tạo điều kiện cho việc nắm bắt thông tin tầm xa trên nhiều mức độ chi tiết khác nhau.

2) Thu thập Query: Việc thu thập query (Q), key (K) và value (V) được thực hiện riêng lẻ trong mỗi nhóm attention head. Đối với nhóm head thứ i, chúng tôi coi mỗi điểm của Xi như một query và thu được các đặc trưng query như:

Qi=WQ_i Xi∈RN×C′, (5)

trong đó WQ_i∈RC′×C′ biểu thị một embedding tuyến tính có thể học được qua một phép toán convolution 1×1. Ma trận query kết quả Qi tương ứng với các đặc trưng query trong nhóm head thứ i.

3) Thu thập Key: Việc thu thập key phần lớn quyết định attention span và chi phí tính toán của một cơ chế self-attention. Để đạt được sự cân bằng giữa việc mô hình hóa phụ thuộc tầm xa và duy trì chi phí cửa sổ cục bộ, chúng tôi thu thập key trong ba bước sau.

Bước 1: Local Windowing.
Chúng tôi bắt đầu bằng cách phân chia đồng đều Xi thành nhiều cửa sổ cục bộ không chồng lấp sử dụng chiến lược cửa sổ trượt. Để đơn giản, chúng tôi giả sử W = H, biểu thị rằng mỗi cửa sổ cục bộ có chiều dài và chiều rộng bằng nhau được ký hiệu là Si. Điều này có thể được biểu diễn toán học như:

Xi={Xj_i∈RSi×Si×C′,|j= 1,···,Mi}. (6)

Ở đây Mi= H/Si đại diện cho số lượng cửa sổ cục bộ trong nhóm attention head thứ i. Xj_i tương ứng với bản đồ đặc trưng của một cửa sổ cục bộ cụ thể. Đáng chú ý, chúng tôi dần dần tăng kích thước của mỗi cửa sổ cục bộ trên các nhóm head khác nhau, dẫn đến Mi giảm dần.

Bước 2: Dilated Sampling.
Tiếp theo, chúng tôi tiến hành lấy mẫu đồng đều một số lượng M×M điểm cố định trong mỗi cửa sổ cục bộ. Trên thực tế, chúng tôi đặt M là 7 theo mặc định. Đối với nhóm head thứ i, tập hợp điểm được lấy mẫu Pi được biểu diễn như:

Pi={Pj_i∈RM×M×C′,|j= 1,···,Mi}, (7)

trong đó Pj_i biểu thị các điểm được lấy mẫu cho cửa sổ cục bộ thứ j trong nhóm thứ i. Vì các nhóm head khác nhau có kích thước cửa sổ cục bộ riêng biệt, điều quan trọng là giữ số lượng điểm được lấy mẫu trên các cửa sổ cục bộ trong các nhóm head khác nhau giống nhau. Trên thực tế, chúng tôi sử dụng lấy mẫu dilated ngắn gọn nhưng hiệu quả với tỷ lệ dilation tăng dần trên các nhóm head để đạt được mục tiêu này. Tỷ lệ dilation tương ứng cho nhóm head thứ i Di được tính như:

Di= (Si−1)/(M−1). (8)

Do đó, các điểm được lấy mẫu được phân phối đồng đều trong mỗi cửa sổ cục bộ. Khi chỉ số nhóm head i tăng, khoảng cách giữa các điểm được lấy mẫu mở rộng, dẫn đến mức độ chi tiết thông tin thô hơn.

Bước 3: Cross-window Fusion.
Các nghiên cứu trước đây đã chứng minh rằng việc giới hạn self-attention độc quyền trong một cửa sổ cục bộ ảnh hưởng có hại đến khả năng mô hình hóa và tính bền vững, chủ yếu do

--- TRANG 5 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 5

BẢNG I
BẢNG SO SÁNH KÝ HIỆU CHÍNH.

Ký hiệu | Mô tả | Ký hiệu | Mô tả
I | Hình ảnh đầu vào | X | Đặc trưng nhúng
K | Token key | Q | Token query
V | Token value | X′ | Đặc trưng attention
F | Đặc trưng đầu ra | WK | Nhúng tuyến tính key
H | Chiều cao hình ảnh | W | Chiều rộng hình ảnh
N | Độ phân giải không gian hình ảnh | C | Chiều kênh hình ảnh
G | Số nhóm | M2 | Số cửa sổ
h | Hệ số tỷ lệ | Xi | Đặc trưng nhóm
Ki | Key nhóm | X′i | Đặc trưng attention nhóm
WKi | Nhúng key nhóm | Pi | Điểm lấy mẫu nhóm
C′ | Chiều kênh nhóm | Di | Tỷ lệ dilation nhóm
Si | Kích thước cửa sổ nhóm | Mi | Số cửa sổ nhóm
hi | Hệ số tỷ lệ nhóm | Fi | Đầu ra giai đoạn
Xji | Đặc trưng cửa sổ | Pji | Điểm lấy mẫu cửa sổ
Ω | Độ phức tạp tính toán | O | Tương quan dương

việc thiếu tương tác thông tin liên cửa sổ [8]. Để khắc phục hạn chế này đồng thời giảm số lượng key token, chúng tôi giới thiệu một chiến lược cross-window fusion sáng tạo. Cụ thể, trước tiên chúng tôi thực hiện nhúng đặc trưng cho mỗi điểm được lấy mẫu.

K′i=WKi Pi∈RMi×M2×C′, (9)
V′i=WVi Pi∈RMi×M2×C′, (10)

trong đó WKi,WVi∈RC′×C′ biểu thị các nhúng tuyến tính có thể học được cho nhóm thứ i được triển khai bởi hai convolution 1×1 riêng biệt. K′i và V′i biểu thị key và value trong nhóm thứ i. Tiếp theo, chúng tôi kết hợp các đặc trưng của các điểm được lấy mẫu nằm ở cùng vị trí trong các cửa sổ khác nhau để thu được các đặc trưng key và value cuối cùng:

Ki=σ(K′i)∈RM2×C′, (11)
Vi=σ(V′i)∈RM2×C′, (12)

trong đó σ(·) là một hàm tổng hợp đối xứng. Ở đây chúng tôi triển khai nó như một dạng đơn giản, tức là maximum pooling.

Trong quá trình trên, ma trận attention đầy đủ được phân tích thành nhiều ma trận sub-attention thưa thớt. Mỗi đặc trưng được kết hợp hưởng lợi từ thông tin tầm xa được làm phong phú, đạt được bằng cách kết hợp các đặc trưng của Mi điểm được phân phối đồng đều trên toàn bộ bản đồ đặc trưng. Hơn nữa, khi chỉ số nhóm head i tăng, giá trị của Mi giảm, từ đó cho phép đặc trưng được kết hợp thu thập thông tin từ ít vị trí hơn. Do đó, nó nắm bắt nhiều chi tiết mức độ chi tiết. Kết quả là, các ma trận sub-attention này có thể xấp xỉ hiệu quả hiệu suất biểu diễn đặc trưng của ma trận attention đầy đủ.

4) Mixed-grained Multi-head Attention: Với các query và key được thu thập cho mỗi nhóm head, chúng tôi thực hiện self-attention riêng lẻ:

X′i=Softmax (QiK⊤i/√hi)Vi∈RH×W×C′, (13)

trong đó √hi biểu thị một hệ số tỷ lệ. Tiếp theo, chúng tôi kết hợp các đặc trưng được chú ý từ tất cả các nhóm head để thu được đầu ra cuối cùng.

X′=δ(X′i,|i= 1,···,G)∈RH×W×C, (14)

trong đó δ(·) biểu thị phép nối theo chiều kênh đặc trưng. Do đó, FaSA có khả năng mô hình hóa phụ thuộc tầm xa trong khi tổng hợp thông tin đa cấp độ.

C. Kiến trúc Tổng thể
Xây dựng trên cơ chế FaSA được làm rõ ở trên, chúng tôi đề xuất factorization vision transformer (FaViT) được mô tả trong Hình 4. Cụ thể, FaViT đầu tiên đưa hình ảnh đầu vào I vào một lớp patch embedding để tạo ra bản đồ đặc trưng X. Thông thường, patch embedding được thực hiện sử dụng convolution để tạo thành các patch không chồng lấp. Tuy nhiên, lấy cảm hứng từ các công trình gần đây [26], [42], bài báo này sử dụng các lớp tích chập với kernel size 7×7 và stride 2 để tạo ra các patch chồng lấp. Tiếp theo, một lớp projection không chồng lấp với stride 2 được sử dụng để tạo ra bản đồ đặc trưng đầu vào X với kích thước H/4×W/4.

Theo các thiết kế trước [8], [23], chúng tôi áp dụng kiến trúc bốn giai đoạn để xử lý X và tạo ra các bản đồ đặc trưng phân cấp F. Các bản đồ đặc trưng ở mỗi giai đoạn được ký hiệu là F={Fi,|i= 1,2,3,4}, trong đó Fi∈RH/2^(i+1)×W/2^(i+1)×(C×2^(i-1)). Các giai đoạn này được kết nối với nhau bởi các lớp linear embedding, được triển khai sử dụng các lớp tích chập với stride 2. Các lớp này giảm kích thước của các bản đồ đặc trưng đi một nửa và tăng gấp đôi chiều của chúng so với giai đoạn trước, sau đó được đưa vào giai đoạn tiếp theo.

Đối với mỗi giai đoạn, trước tiên chúng tôi xếp chồng nhiều lớp FaSA. FaSA, như đã nêu chi tiết ở trên, sử dụng cửa sổ cục bộ và lấy mẫu dilated để thu được các ma trận sub-attention thưa thớt. Các ma trận sub-attention này tạo điều kiện cho việc tổng hợp thông tin ở nhiều mức độ chi tiết thông qua cross-window fusion. Do đó, chúng có khả năng biểu hiện đặc trưng tương tự

--- TRANG 6 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 6

BẢNG II
CÁC BIẾN THỂ KIẾN TRÚC CỦA FAVIT. PI,CI,HI,EI,BI,VÀ DI CHỈ KÍCH THƯỚC PATCH, CHIỀU ĐẶC TRƯNG, SỐ HEAD, TỶ LỆ MỞ RỘNG CỦA MLP, SỐ KHỐI, VÀ TẬP TỶ LỆ DILATION TƯƠNG ỨNG.

Kích thước Đầu ra | Tên Lớp | FaViT-B0 | FaViT-B1 | FaViT-B2 | FaViT-B3
Giai đoạn 1 H/4×W/4 | Linear Embedding | P1= 4; C1= 32 | P1= 4; C1= 64 | P1= 4; C1= 96
FaSA | H1= 1 E1= 8 B1= 2 D1= [1,8] | H1= 1 E1= 8 B1= 2 D1= [1,8] | H1= 1 E1= 8 B1= 2 D1= [1,8] | H1= 1 E1= 8 B1= 2 D1= [1,8]
Giai đoạn 2 H/8×W/8 | Linear Embedding | P2= 2; C2= 64 | P2= 2; C2= 128 | P2= 2; C2= 192
FaSA | H2= 2 E2= 6 B2= 2 D2= [1,4] | H2= 2 E2= 6 B2= 2 D2= [1,4] | H2= 2 E2= 6 B2= 3 D2= [1,4] | H2= 2 E2= 6 B2= 3 D2= [1,4]
Giai đoạn 3 H/16×W/16 | Linear Embedding | P3= 2; C3= 128 | P3= 2; C3= 256 | P3= 2; C3= 384
FaSA | H3= 4 E3= 4 B3= 6 D3= [1,2] | H3= 4 E3= 4 B3= 6 D3= [1,2] | H3= 4 E3= 4 B3= 18 D3= [1,2] | H3= 4 E3= 4 B3= 14 D3= [1,2]
Giai đoạn 4 H/32×W/32 | Linear Embedding | P4= 2; C4= 256 | P4= 2; C4= 512 | P4= 2; C4= 768
FaSA | H4= 8 E4= 4 B4= 2 D4= [1] | H4= 8 E4= 4 B4= 2 D4= [1] | H4= 8 E4= 4 B4= 3 D4= [1] | H4= 8 E4= 4 B4= 3 D4= [1]

ma trận attention đầy đủ trong khi đồng thời giảm chi phí tính toán. Bản đồ đặc trưng attention được tính toán X′ sau đó được đưa vào một lớp feedforward để tạo ra bản đồ đặc trưng đầu ra Fi của giai đoạn hiện tại. Lớp feedforward này được triển khai như một Multilayer Perceptron (MLP), theo cách tiếp cận của công trình trước [26].

Dựa trên cấu trúc mạng trên, chúng tôi trình bày bốn biến thể mô hình của FaViT với các kích thước mô hình khác nhau bằng cách áp dụng các cài đặt tham số khác nhau. Thông tin chi tiết hơn về kiến trúc mô hình có thể được tìm thấy trong bảng II.

D. Phân tích Độ phức tạp
FaViT của chúng tôi nhằm đạt được hiệu suất cao và tính bền vững với độ phức tạp tính toán tuyến tính đối với độ phân giải không gian của hình ảnh đầu vào. Vì self-attention được tính toán trong mỗi cửa sổ cục bộ, độ phức tạp tính toán của FaViT là bậc hai đối với kích thước cửa sổ và tuyến tính đối với độ phân giải hình ảnh. Độ phức tạp này có thể được biểu diễn như:

Ω(FaSA) = 4NC2+ 2M2NC, (15)

trong đó M là một giá trị cố định được đặt trước, do đó độ phức tạp tính toán của FaViT là O(N). Ngược lại, ViT có độ phức tạp O(N2), và các kiến trúc dựa trên pyramid có độ phức tạp O(N2/θ), trong đó θ là hệ số suy giảm. Độ phức tạp của FaViT về mặt lý thuyết thấp hơn đáng kể so với các phương pháp trên. FaViT nên được phân loại tự nhiên vào danh mục Swin transformer với hiệu suất và tính bền vững cao hơn. Kết quả là, FaSA có khả năng mô hình hóa phụ thuộc tầm xa với chi phí tính toán tương đương self-attention dựa trên cửa sổ cục bộ.

IV. THÍ NGHIỆM
Tính hiệu quả và khả năng tổng quát hóa của FaViT đề xuất đã được đánh giá trên nhiều thị giác máy tính. Các đánh giá này bao gồm các tác vụ phân loại hình ảnh, phát hiện đối tượng và phân đoạn ngữ nghĩa. Hơn nữa, chúng tôi đánh giá tính bền vững của FaViT thông qua thí nghiệm liên quan đến tham nhũng hình ảnh, nhiễu nhãn và các thách thức mất cân bằng lớp. Các nghiên cứu ablation được cung cấp để xác nhận các lựa chọn thiết kế của chúng tôi.

A. Phân loại Hình ảnh
Dữ liệu và thiết lập. Chúng tôi đánh giá các biến thể FaViT đề xuất trên bộ dữ liệu phân loại ImageNet-1K [68]. Để so sánh công bằng, chúng tôi tuân theo cùng chiến lược đào tạo với các nghiên cứu trước [24]. Chúng tôi sử dụng optimizer AdamW với 300 epoch bao gồm 10 epoch khởi động ban đầu và 10 epoch hạ nhiệt cuối cùng. Một bộ lập lịch tỷ lệ học cosine decay được áp dụng, giảm tỷ lệ học một hệ số 10 mỗi 30 epoch, bắt đầu từ tỷ lệ học cơ sở 0.001. Weight decay được đặt thành 0.05, và hình ảnh đầu vào được thay đổi kích thước thành 224×224. Các tăng cường dữ liệu và phương pháp regularization phù hợp với những phương pháp được sử dụng trong [24]. Chúng tôi sử dụng kích thước mini-batch 128 mẫu và tận dụng sức mạnh tính toán của 8 GPU NVIDIA 3090 cho đào tạo.

Kết quả chính. Bảng III chứng minh hiệu suất đáng kể của các biến thể FaViT đề xuất. Cụ thể, 

BẢNG III
KẾT QUẢ PHÂN LOẠI TRÊN IMAGE NET-1K. TẤT CẢ CÁC MÔ HÌNH ĐƯỢC ĐÀO TẠO TỪ ĐẦU SỬ DỤNG CÙNG CHIẾN LƯỢC ĐÀO TẠO.

Kích thước Mô hình | Phương pháp | #Param | FLOPs | Top-1 Acc (%)
Tiny Model | PVTv2-B0 [24] | 4 M | 0.6 G | 70.5
 | FaViT-B0 (Ours) | 3 M | 0.6 G | 71.5
Small Model | ResNet18 [43] | 12 M | 1.8 G | 69.8
 | PVTv1-T [23] | 13 M | 1.9 G | 75.1
 | PVTv2-B1 [24] | 14 M | 2.1 G | 78.7
 | Mobile-Former [44] | 14 M | 1.0 G | 79.3
 | FaViT-B1 (Ours) | 13 M | 2.4 G | 79.4
Medium Model | DeiT-S [17] | 22 M | 4.6 G | 79.9
 | Distilled DeiT-S [17] | 22 M | 4.6 G | 81.2
 | NesT-T [45] | 17 M | 5.8 G | 81.3
 | T2T-ViT-14 [46] | 22 M | 5.2 G | 81.5
 | ViL-S [47] | 25 M | 5.1 G | 82.0
 | CrossViT-15 [48] | 27 M | 5.8 G | 81.5
 | TNT-S [49] | 24 M | 5.2 G | 81.5
 | DW-ViT-T [50] | 30 M | 5.2 G | 82.0
 | DW-Conv-T [51] | 24 M | 3.8 G | 81.3
 | GG-T [52] | 28 M | 4.5 G | 82.0
 | CoAtNet-0 [53] | 25 M | 4.2 G | 81.6
 | VSA [38] | 29 M | 4.6 G | 82.3
 | ConvNeXt-T [54] | 29 M | 4.5 G | 82.1
 | MViTv2-T [55] | 24 M | 4.7 G | 82.3
 | ViTAEv2-S [56] | 20 M | 5.4 G | 82.2
 | CrossFormer [57] | 31 M | 4.9 G | 82.5
 | Swin-T [8] | 29 M | 4.5 G | 81.3
 | FaViT-B2 (Ours) | 24 M | 4.5 G | 82.3
Large Model | LV-ViT-M [18] | 56 M | 16.0 G | 84.1
 | SimViT-M [28] | 51 M | 10.9 G | 83.3
 | TNT-B [49] | 66 M | 14.1 G | 82.8
 | BoTNet-S1-59 [58] | 34 M | 7.3 G | 81.7
 | Focal-S [59] | 51 M | 9.4 G | 83.6
 | RSB-101 [60] | 45 M | 7.9 G | 81.3
 | Shuffle-B [61] | 88 M | 15.6 G | 84.0
 | Deep-ViT-L [62] | 55 M | 12.5 G | 83.1
 | PoolFormer-M36 [21] | 56 M | 8.8 G | 82.1
 | MetaFormer [21] | 57 M | 12.8 G | 84.5
 | PVTv2-B3 [24] | 45 M | 6.9 G | 83.2
 | MOA-S [27] | 39 M | 9.4 G | 83.5
 | ConvNeXt-S [54] | 50 M | 8.7 G | 83.1
 | MSG-S [63] | 56 M | 8.4 G | 83.4
 | MPViT-B [64] | 75 M | 16.4 G | 84.3
 | RepLKNet-31B [65] | 79 M | 15.3 G | 83.5
 | VAN-B4 [66] | 60 M | 12.2 G | 84.2
 | Next-ViT-L [67] | 58 M | 10.8 G | 83.6
 | DilateFormer [22] | 47 M | 10.0 G | 84.4
 | CrossFormer-L [57] | 92 M | 16.1 G | 84.0
 | Swin-S [8] | 50 M | 8.7 G | 83.0
 | FaViT-B3 (Ours) | 48 M | 8.5 G | 83.4

FaViT-B0 và FaViT-B1 nhẹ đạt được độ chính xác Top-1 phân loại lần lượt là 71.5% và 79.4%, vượt trội so với các mô hình tiên tiến trước đây với số lượng tham số tương tự. Hơn nữa, khi so sánh với Swin Transformer cơ sở [8], các biến thể FaViT liên tục thể hiện hiệu suất vượt trội với ít tham số và FLOP hơn. Một ví dụ nổi bật là FaViT-B2, đạt được độ chính xác ấn tượng

--- TRANG 7 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 7

là 82.3%, vượt trội so với Swin-T 1% trong khi giảm đáng kể tham số 14%. Độ chính xác và hiệu quả đặc biệt này có thể được gán cho khả năng duy nhất của FaViT trong việc mô hình hóa phụ thuộc tầm xa với chi phí cửa sổ cục bộ.

Trực quan hóa. FaViT đạt được độ chính xác phân loại cao do attention span của nó trên cả phạm vi ngắn và dài. Như thể hiện trong Hình 5, chúng tôi trực quan hóa attention span từ giai đoạn thứ nhất và thứ hai của các mô hình khác nhau cho một vùng token đã cho. Mô hình cơ sở Swin transformer [8] chỉ tập trung vào vùng lân cận nhỏ của token, dẫn đến giảm độ chính xác và tính bền vững. Ngược lại, ViT [9] có attention span tầm xa hơn nhờ cơ chế attention toàn cục được áp dụng, nhưng với chi phí tăng độ phức tạp tính toán bậc hai khi tăng độ phân giải không gian của hình ảnh đầu vào. FaViT đề xuất kết hợp tốt các điểm mạnh bổ sung của cả hai mô hình. Nó có attention span lớn tương tự như ViT trong khi duy trì chi phí tính toán tương đương với Swin Transformer dựa trên cửa sổ cục bộ. Cách tiếp cận cân bằng này đạt được sự cân bằng lý tưởng giữa khả năng mô hình hóa phụ thuộc tầm xa và chi phí tính toán.

Phân tích hiệu quả. Chúng tôi so sánh độ phức tạp tính toán của FaViT đề xuất với các nghiên cứu trước khác trong Hình 6 bằng cách báo cáo số lượng FLOP dưới các kích thước hình ảnh đầu vào khác nhau. Khi kích thước hình ảnh đầu vào nhỏ, ví dụ 224×224, tất cả các mô hình có số lượng FLOP tương đương. Tuy nhiên, khi kích thước hình ảnh đầu vào tăng, chúng tôi quan sát thấy sự khác biệt đáng kể. DeiT thông thường [17] chịu sự tăng mạnh về FLOP khi kích thước hình ảnh tăng, vì độ phức tạp tính toán của nó là bậc hai đối với kích thước hình ảnh. Hạn chế này hạn chế khả năng áp dụng của nó trong các tình huống thực tế. Mặc dù PVTV2 [24] đã cố gắng làm chậm tốc độ tăng FLOP, nó vẫn về cơ bản thể hiện sự tăng trưởng bậc hai về độ phức tạp tính toán. Ngược lại, FLOP của FaViT tăng tuyến tính theo kích thước hình ảnh do việc áp dụng chiến lược self-attention dựa trên cửa sổ cục bộ. Sự tăng trưởng tuyến tính này khiến FaViT trở thành lựa chọn có thể mở rộng và hiệu quả hơn để xử lý hình ảnh đầu vào lớn hơn.

Ưu thế tính toán của FaViT trở nên đặc biệt rõ ràng khi xử lý hình ảnh đầu vào lớn. Cụ thể, khi kích thước hình ảnh đạt 1000×1000 pixel, FaViT-B2 thể hiện lợi thế đáng kể về FLOP so với DeiT-S và PVTv2-B2, chỉ với 25% và 50% chi phí tính toán của chúng, tương ứng. So với Swin-T [8], FaViT-B2 đạt được cùng chi phí tính toán trong khi mang lại độ chính xác cao hơn. Hơn nữa, chúng tôi báo cáo throughput (imgs/s) ở kích thước hình ảnh cao cho PVTv2-B2 và FaViT-B2. FaViT đề xuất hiệu quả hơn và lợi thế trở nên rõ ràng hơn khi kích thước tăng.

Hình 5. Trực quan hóa attention span cho một vùng token đã cho (hộp đỏ) bởi các mô hình khác nhau. (a) Hình ảnh gốc với vùng được chọn được đánh dấu bằng hộp đỏ. (b) ViT có attention span toàn cục nhưng chi phí tính toán cao. (c) Swin hạn chế attention span đến một cửa sổ cục bộ. (d) FaViT đạt được attention span toàn cục với chi phí cửa sổ cục bộ.

Hình 6. Biểu đồ cho kích thước hình ảnh đầu vào so với FLOP. FLOP và Throughput của PVTv2 và FaViT ở độ phân giải cao được liệt kê ở góc trên bên trái.

B. Phát hiện Đối tượng
Dữ liệu và thiết lập. Chúng tôi đánh giá các biến thể FaViT đề xuất trong các tác vụ thách thức về phát hiện đối tượng và phân đoạn thực thể trên bộ dữ liệu COCO 2017 [69]. Để đảm bảo đánh giá toàn diện, chúng tôi sử dụng hai framework được thiết lập tốt, RetinaNet [70] và Mask R-CNN [71], và tích hợp các biến thể FaViT như backbone. Chúng tôi khởi tạo các mô hình với trọng số được pre-train trên bộ dữ liệu ImageNet-1K và fine-tune chúng sử dụng optimizer AdamW. Chúng tôi đặt tỷ lệ học ban đầu ở 0.0001 và áp dụng weight decay 0.05. Đào tạo của chúng tôi được thực hiện với batch size 16, tuân thủ lịch đào tạo 1× nhất quán, tương đương 12 epoch. Ngoài ra, chúng tôi áp dụng chiến lược thay đổi kích thước hình ảnh như các nghiên cứu trước [8], trong đó cạnh ngắn hơn được thay đổi kích thước thành 800 pixel trong khi đảm bảo rằng cạnh dài hơn vẫn dưới 1333 pixel. Phần còn lại của các hyperparameter nhất quán với Swin transformer [8] để đảm bảo so sánh công bằng.

Kết quả chính. Trong Bảng IV, chúng tôi trình bày kết quả hiệu suất của FaViT-B0 đến B3 trên các mức kích thước khác nhau. Ấn tượng là, tất cả các biến thể đều liên tục mang lại kết quả tiên tiến. So với Swin transformer cơ sở [8], FaViT thể hiện hiệu suất vượt trội trong cả tác vụ phát hiện đối tượng và phân đoạn thực thể. Cụ thể, FaViT-B2 thể hiện những cải thiện đáng kể, với các chỉ số như AP và APL chứng kiến mức tăng lần lượt là 2.9% và 3.3%. Đối với phân đoạn thực thể, FaViT-B2 cũng đạt được độ chính xác cao hơn và cải thiện đáng kể APm 1.9%.

--- TRANG 8 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 8

BẢNG IV
KẾT QUẢ PHÁT HIỆN ĐỐI TƯỢNG VÀ PHÂN ĐOẠN THỰC THỂ TRÊN COCO 2017. TẤT CẢ CÁC MÔ HÌNH ĐƯỢC PRE-TRAIN TRÊN IMAGE NET-1K VÀ FINE-TUNE VỚI LỊCH 1×. SỐ LƯỢNG THAM SỐ Ở ĐỘ PHÂN GIẢI ĐẦU VÀO 1280×800 ĐƯỢC BÁO CÁO (#PARAM).

Phương pháp | RetinaNet 1× | Mask R-CNN 1×
#Param | AP | AP50 | AP75 | APS | APM | APL | #Param | APb | APb50 | APb75 | APm | APm50 | APm75
PVTv2-B0 [24] | 13 M | 37.2 | 57.2 | 39.5 | 23.1 | 40.4 | 49.7 | 24 M | 38.2 | 60.5 | 40.7 | 36.2 | 57.8 | 38.6
FaViT-B0 (Ours) | 12 M | 37.4 | 57.2 | 39.8 | 22.9 | 40.6 | 49.5 | 23 M | 37.9 | 59.6 | 41.0 | 35.4 | 56.6 | 37.8
ResNet18 [43] | 21 M | 31.8 | 49.6 | 33.6 | 16.3 | 34.3 | 43.2 | 31 M | 34.0 | 54.0 | 36.7 | 31.2 | 51.0 | 32.7
PVTv2-B1 [24] | 24 M | 41.2 | 61.9 | 43.9 | 25.4 | 44.5 | 54.3 | 34 M | 41.8 | 64.3 | 45.9 | 38.8 | 61.2 | 41.6
FaViT-B1 (Ours) | 23 M | 41.4 | 61.8 | 44.0 | 25.7 | 44.9 | 55.0 | 33 M | 42.4 | 64.4 | 46.3 | 38.9 | 61.2 | 41.8
Twins-S [72] | 34 M | 43.0 | 64.2 | 46.3 | 28.0 | 46.4 | 57.5 | 44 M | 43.4 | 66.0 | 47.3 | 40.3 | 63.2 | 43.4
Swin-T [8] | 39 M | 41.5 | 62.1 | 44.2 | 25.1 | 44.9 | 55.5 | 48 M | 42.2 | 64.6 | 46.2 | 39.1 | 61.6 | 42.0
FaViT-B2 (Ours) | 35 M | 44.4 | 65.0 | 47.7 | 27.7 | 48.2 | 58.8 | 45 M | 45.4 | 67.1 | 49.4 | 41.0 | 64.0 | 44.1
PVTv1-M [23] | 54 M | 41.9 | 63.1 | 44.3 | 25.0 | 44.9 | 57.6 | 64 M | 42.0 | 64.4 | 45.6 | 39.0 | 61.6 | 42.1
Swin-S [8] | 60 M | 44.5 | 65.7 | 47.5 | 27.4 | 48.0 | 59.9 | 69 M | 44.8 | 66.6 | 48.9 | 40.9 | 63.4 | 44.2
FaViT-B3 (Ours) | 59 M | 46.0 | 66.7 | 49.1 | 28.4 | 50.3 | 62.0 | 68 M | 47.1 | 68.0 | 51.4 | 42.7 | 65.9 | 46.1

Trực quan hóa. Do phụ thuộc tầm xa, FaViT có lợi thế trong việc phát hiện các đối tượng có tỷ lệ khác nhau. Chúng tôi trực quan hóa heatmap attention của FaViT-B2 dưới thách thức đa đối tượng trong Hình 7. Để cung cấp phân tích so sánh, chúng tôi so sánh FaViT-B2 với ConvNeXt-T [54] và Swin-T, hai mô hình có kích thước tương tự. ConvNeXt-T dường như ít thành thạo hơn trong việc đồng thời tập trung vào nhiều đối tượng, có thể khiến nó bỏ qua các mục tiêu riêng lẻ. Heatmap attention của Swin transformer thể hiện cấu trúc giống lưới, tương đối lộn xộn. Ngược lại, phân phối attention của FaViT đồng đều hơn trên toàn bộ không gian đối tượng, cho phép nó đồng thời nắm bắt vị trí và đường viền của nhiều đối tượng với độ chính xác. So sánh heatmap này phục vụ như bằng chứng thuyết phục về sự vượt trội của FaViT.

Hình 7. Trực quan hóa attention map để phát hiện nhiều đối tượng. (a) Nhiều mục tiêu cần phát hiện trong hình ảnh gốc được đánh dấu bằng hộp đỏ. (b) ConvNeXt-T mất mục tiêu nhỏ khi phát hiện nhiều mục tiêu. (c) Swin-T bị ảnh hưởng nghiêm trọng bởi cửa sổ cục bộ. (d) FaViT-B2 có khả năng phát hiện đa mục tiêu xuất sắc.

BẢNG V
KẾT QUẢ PHÂN ĐOẠN TRÊN ADE20K.

Phương pháp | #Param | FLOPs | mIoU (%)
PVTv2-B0 [24] | 8 M | 25.0 G | 37.2
FaViT-B0 (Ours) | 7 M | 24.6 G | 37.2
ResNet18 [43] | 16 M | 32.2 G | 32.9
PVTv1-T [23] | 13 M | 31.9 G | 35.1
EFormer-L1 [75] | 16 M | 33.0 G | 38.9
FaViT-B1 (Ours) | 17 M | 33.9 G | 42.0
Twins-S [72] | 28 M | 37.5 G | 43.2
Swin-T [8] | 32 M | 46.0 G | 41.5
FaViT-B2 (Ours) | 29 M | 45.2 G | 45.0
CAT-B [76] | 55 M | 76.9 G | 44.9
Swin-S [8] | 53 M | 70.0 G | 45.2
FaViT-B3 (Ours) | 52 M | 66.7 G | 47.2

C. Phân đoạn Ngữ nghĩa
Dữ liệu và thiết lập. Để đánh giá hiệu suất của các biến thể FaViT đề xuất trong phân đoạn ngữ nghĩa, chúng tôi sử dụng bộ dữ liệu ADE20K [73] và tuân theo framework benchmark, Semantic FPN [74], để đảm bảo so sánh công bằng. Chiến lược đào tạo của chúng tôi phù hợp với các thực hành đã thiết lập [8], bao gồm 160K lần lặp, sử dụng optimizer AdamW, tỷ lệ học ban đầu 0.0002, weight decay 0.0001, và batch size 16.

Kết quả chính. Bảng V mô tả rằng các biến thể FaViT của chúng tôi với các kích thước mô hình khác nhau liên tục vượt trội so với các đối tác Swin transformer tương ứng [8] khi sử dụng Semantic FPN cho phân đoạn ngữ nghĩa. Ví dụ, với ít tham số và FLOP hơn, FaViT-B2/B3 cao hơn ít nhất 2% so với Swin-T/S. Đáng chú ý, FaViT thể hiện độ chính xác phân đoạn vượt trội trên tất cả các kích thước mô hình.

D. Phân tích Tính bền vững
Chúng tôi đánh giá tính bền vững của các biến thể FaViT đối với tham nhũng hình ảnh, nhiễu nhãn và mất cân bằng lớp.

--- TRANG 9 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 9

BẢNG VI
TÍNH BỀN VỮNG CHỐNG LẠI THAM NHŨNG HÌNH ẢNH TRÊN IMAGE NET-C.

Phương pháp | #Param | Retention | Blur (Acc %) | Noise (Acc %) | Digital (Acc %) | Weather (Acc %)
 | | | Motion | Fafoc | Glass | Gauss | Gauss | Impul | Shot | Speck | Contr | Satur | JEPG | Pixel | Bright | Snow | Fog | Frost
Mobile Setting (<15M)
MobileNetV2 [77] | 4 M | 49.2 | 33.4 | 29.6 | 21.3 | 32.9 | 24.4 | 21.5 | 23.7 | 32.9 | 57.6 | 49.6 | 38.0 | 62.5 | 28.4 | 45.2 | 37.6 | 28.3
EfficientNet-B0 [5] | 5 M | 54.7 | 36.4 | 26.8 | 26.9 | 39.3 | 39.8 | 38.1 | 47.1 | 39.9 | 65.2 | 58.2 | 52.1 | 69.0 | 37.3 | 55.1 | 44.6 | 37.4
PVTv2-B0 [24] | 3 M | 58.9 | 30.8 | 24.9 | 24.0 | 35.8 | 33.1 | 35.2 | 44.2 | 50.6 | 59.3 | 50.8 | 36.6 | 61.9 | 38.6 | 50.7 | 45.9 | 41.8
ResNet18 [43] | 11 M | 32.8 | 29.6 | 28.0 | 22.9 | 32.0 | 22.7 | 17.6 | 20.8 | 27.7 | 30.8 | 52.7 | 46.3 | 42.3 | 58.8 | 24.1 | 41.7 | 28.2
PVTv2-B1 [24] | 13 M | 65.4 | 45.7 | 41.3 | 30.5 | 43.9 | 48.1 | 46.2 | 46.6 | 55.0 | 57.6 | 68.6 | 59.9 | 50.2 | 71.0 | 49.8 | 56.8 | 53.0
FaViT-B0 (Ours) | 3 M | 59.2 | 38.1 | 31.6 | 24.8 | 37.4 | 38.3 | 35.6 | 39.9 | 45.2 | 47.9 | 60.8 | 51.6 | 38.9 | 63.2 | 38.5 | 44.6 | 42.5
FaViT-B1 (Ours) | 13 M | 68.1 | 48.2 | 43.2 | 30.7 | 45.6 | 53.8 | 52.4 | 52.6 | 58.7 | 59.6 | 70.1 | 61.7 | 53.5 | 72.1 | 50.9 | 57.1 | 54.7
GPU Setting (20M+)
ResNet50 [43] | 25 M | 62.5 | 42.1 | 40.1 | 27.2 | 42.2 | 42.2 | 36.8 | 41.0 | 50.3 | 51.7 | 69.2 | 59.3 | 51.2 | 71.6 | 38.5 | 53.9 | 42.3
ViT-S [9] | 25 M | 67.6 | 49.7 | 45.2 | 38.4 | 48.0 | 50.2 | 47.6 | 49.0 | 57.5 | 58.4 | 70.1 | 61.6 | 57.3 | 72.5 | 51.2 | 50.6 | 57.0
DeiT-S [17] | 22 M | 72.6 | 52.6 | 48.9 | 38.1 | 51.7 | 57.2 | 55.0 | 54.7 | 60.8 | 63.7 | 71.8 | 64.0 | 58.3 | 73.6 | 55.1 | 61.1 | 60.7
PVTv1-S [23] | 25 M | 66.9 | 54.3 | 48.4 | 34.7 | 46.4 | 51.7 | 51.7 | 50.0 | 55.8 | 57.6 | 69.4 | 60.7 | 53.7 | 49.5 | 62.3 | 55.2 | 53.1
PVTv2-B2 [24] | 25 M | 71.5 | 54.3 | 48.4 | 34.7 | 50.7 | 61.2 | 60.7 | 59.5 | 64.5 | 65.5 | 73.5 | 65.5 | 58.8 | 75.2 | 56.7 | 67.8 | 62.7
Swin-T [8] | 29 M | 66.8 | 49.5 | 45.0 | 31.7 | 47.6 | 54.7 | 51.6 | 52.6 | 58.4 | 62.1 | 71.4 | 62.2 | 54.4 | 73.4 | 60.0 | 64.7 | 60.2
FaViT-B2 (Ours) | 25 M | 73.4 | 55.9 | 49.8 | 34.8 | 51.7 | 62.6 | 62.1 | 61.2 | 65.3 | 64.9 | 73.3 | 66.1 | 64.8 | 75.0 | 55.3 | 62.9 | 59.5

Tính bền vững chống lại tham nhũng hình ảnh. Chúng tôi đánh giá tính bền vững của các biến thể FaViT đề xuất trên bộ dữ liệu ImageNetC [78], bao gồm các hình ảnh bị tham nhũng khác nhau với các hiệu ứng như mờ, nhiễu tự nhiên, nhiễu kỹ thuật số và điều kiện thời tiết nghiêm trọng. Để so sánh công bằng, tất cả các mô hình được pre-train trên ImageNet-1K mà không fine-tune thêm [10]. Kết quả trong Bảng VI chứng minh rằng FaViT thể hiện tính bền vững đáng kể, cả trong cài đặt Mobile và GPU, vượt trội so với các nghiên cứu trước dựa trên CNN và transformer. Tính bền vững tăng cường này có thể xuất phát từ khả năng biểu diễn vượt trội của nó. Để đánh giá tính bền vững mô hình tốt hơn, chúng tôi giới thiệu một chỉ số mới gọi là accuracy retention (Retention), được thiết kế để giảm thiểu ảnh hưởng của khả năng biểu diễn của mô hình trên bộ dữ liệu hình ảnh sạch. Chỉ số này định lượng tỷ lệ giữa độ chính xác của mô hình trên hình ảnh bị tham nhũng và độ chính xác trên hình ảnh sạch, cung cấp cái nhìn sâu sắc về mức độ mô hình bảo tồn độ chính xác khi được kiểm tra trên dữ liệu bị tham nhũng.

Ví dụ, xem xét FaViT-B1, đạt được độ chính xác 54.1% trên ImageNet-C và 79.4% trên ImageNet. Accuracy retention của nó là 68.1%, cho thấy rằng nó bảo tồn 68.1% độ chính xác khi được kiểm tra trên hình ảnh bị tham nhũng. So sánh với PVTv2-B1 [24], trong khi đạt được độ chính xác tương tự trên hình ảnh sạch, có accuracy retention là 65.4%. Điều này cho thấy FaViT liên tục vượt trội so với PVTv2 khi nói đến việc bảo tồn độ chính xác trên hình ảnh bị tham nhũng. Đáng chú ý, FaViT-B2 vượt trội đáng kể so với Swin-T [8] 6.6% trong accuracy retention và thể hiện hiệu suất vượt trội trên các loại tham nhũng hình ảnh khác nhau. Những kết quả này nhấn mạnh thành công của FaViT trong việc nắm bắt thông tin bối cảnh tầm xa, một yếu tố quan trọng trong việc tăng cường tính bền vững chống lại tham nhũng hình ảnh.

Tính bền vững chống lại nhiễu nhãn. Chúng tôi đánh giá tính bền vững của FaViT chống lại nhiễu nhãn thực tế sử dụng bộ dữ liệu Clothing1M [79] và WebVision [80]. Để đảm bảo so sánh công bằng,

BẢNG VII
TÍNH BỀN VỮNG CHỐNG LẠI NHIỄU NHÃN TRÊN CLOTHING 1M VÀ WEBVISION.

Phương pháp | #Param | Clothing1M | WebVision
 | | Test Acc (%) | Top-1 Acc (%) | Top-5 Acc (%)
PVTv1-S [23] | 25 M | 68.83 | 60.08 | 81.84
PVTv2-B2 [24] | 25 M | 69.89 | 65.28 | 85.72
Shunted-S [26] | 22 M | 70.04 | 67.44 | 86.24
Swin-T [8] | 29 M | 69.12 | 60.84 | 82.48
FaViT-B2 (Ours) | 25 M | 70.82 | 67.72 | 85.80

BẢNG VIII
TÍNH BỀN VỮNG CHỐNG LẠI MẤT CÂN BẰNG LỚP TRÊN I NATURALIST 2018.

Phương pháp | FLOPs | Top-1 Acc (%)
ResMLP-12 [81] | 3.0 G | 60.2
Inception-V3 [82] | 2.5 G | 60.2
LeViT-192 [83] | 0.7 G | 60.4
ResNet-50 [84] | 4.1 G | 64.1
FaViT-B1 (Ours) | 2.4 G | 64.2

chúng tôi áp dụng chiến lược đào tạo nhất quán như [33] trên tất cả các mô hình. Bảng VII trình bày kết quả, với FaViT-B2 liên tục đạt được độ chính xác cao nhất trên cả hai bộ dữ liệu. Đáng kể, FaViT-B2 đạt được độ chính xác top-1 là 67.72% trên WebVision, vượt trội so với Swin-T cơ sở [8] với biên độ đáng kể 6.9%. Những phát hiện này nhấn mạnh tính bền vững đáng kể của FaViT trong sự hiện diện của nhiễu nhãn thực tế.

Tính bền vững chống lại mất cân bằng lớp. Chúng tôi kiểm tra tính bền vững mô hình chống lại mất cân bằng lớp trên bộ dữ liệu iNaturalist đuôi dài [82]. Tất cả các mô hình được pre-train trên ImageNet-1K và fine-tune trong 100 epoch với tỷ lệ học ban đầu

--- TRANG 10 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 10

0.0001. Bảng VIII trình bày kết quả, với FaViT-B1 đạt được độ chính xác cao nhất so với các mô hình khác. Điều này nhấn mạnh tính bền vững xuất sắc của FaViT khi xử lý dữ liệu đuôi dài. Kết quả từ các thí nghiệm này chứng minh tính bền vững mạnh mẽ của FaViT chống lại tham nhũng và thiên lệch dữ liệu, thể hiện tiềm năng của nó cho các ứng dụng thực tế. Bảng VIII cũng minh họa rằng FaViT hoạt động cạnh tranh về FLOP so với các nghiên cứu tiên tiến.

E. Nghiên cứu Ablation
Chúng tôi thực hiện các nghiên cứu ablation trên CIFAR100 [85]. Tất cả các biến thể mô hình được đào tạo từ đầu trong 100 epoch với tỷ lệ học ban đầu 0.001. Phần còn lại của chiến lược đào tạo nhất quán với Phần IV-A.

Tính hiệu quả của FaSA. Chúng tôi đánh giá tính hiệu quả và khả năng tổng quát hóa của cơ chế factorization self-attention (FaSA) đề xuất bằng cách tích hợp nó vào các backbone dựa trên transformer khác. Cụ thể, chúng tôi đơn giản thay thế self-attention gốc trong Swin transformer [8] và PVTv2 [24] bằng FaSA trong khi giữ nguyên phần còn lại của kiến trúc mạng. Bảng IX chứng minh rằng FaSA liên tục cải thiện hiệu suất của các backbone khác nhau đồng thời giảm số lượng tham số và FLOP. Đặc biệt, nó cải thiện đáng kể độ chính xác của Swin-T và PVTv2-B1 lần lượt là 4.3% và 2.3%, trong khi giảm số lượng tham số của PVTv2-B1 18%. Kết quả cung cấp bằng chứng rõ ràng về sự vượt trội của FaSA so với các cơ chế self-attention phổ biến khác.

Tác động của tập tỷ lệ dilation. Trong FaSA, chúng tôi tập trung vào việc tổng hợp thông tin đa cấp độ được nắm bắt bởi các đặc trưng được nhóm. Để điều tra thêm tác động của việc tổng hợp đa cấp độ này, chúng tôi thiết kế hai mô hình bổ sung, FaSA-low và FaSA-high, mỗi mô hình chỉ sử dụng một mức thông tin cấp độ duy nhất dựa trên FaViT-B0. FaSA-low

BẢNG IX
ÁP DỤNG FASA VÀO CÁC FRAMEWORK KHÁC.

Phương pháp | #Param | FLOPs | Acc (%)
Swin-T [8] | 28 M | 4.4 G | 61.8
Swin-FaSA-T | 26 M | 4.0 G | 66.1
PVTv2-B0 [24] | 4 M | 0.6 G | 63.5
PVTv2-FaSA-B0 | 3 M | 0.6 G | 64.0
PVTv2-B1 [24] | 14 M | 2.1 G | 70.8
PVTv2-FaSA-B1 | 11 M | 2.1 G | 73.1

BẢNG X
GIẢM TỪ ĐA CẤPĐỘ XUỐNG ĐƠN CẤPĐỘ MỊN.

Phương pháp | #Param | FLOPs | Acc (%)
FaSA-low | 3 M | 0.6 G | 64.9
FaSA-high | 3 M | 0.6 G | 64.5
FaSA | 3 M | 0.6 G | 65.2

BẢNG XI
TÁC ĐỘNG CỦA CÁC ĐẶC TRƯNG TOÀN CỤC.

Phương pháp | Acc (%) | Param đối với kích thước hình ảnh
 | | 448 | 672 | 896 | 1120
FaViT-B2 | 75.7 | 18 G | 41 G | 72 G | 113 G
FaViT-B2 1/4 | 75.7 | 18 G | 43 G | 80 G | 135 G
FaViT-B2 1/8 | 75.8 | 18 G | 42 G | 76 G | 124 G
FaViT-B2 1/16 | 75.8 | 18 G | 41 G | 74 G | 118 G

BẢNG XII
SO SÁNH CÁC TOÁN TỬ KHÁC NHAU ĐƯỢC SỬ DỤNG CHO CROSS-WINDOW FUSION TRÊN CIFAR100.

Toán tử | #Param | FLOPs | Top-1 Acc (%)
Pointwise Convolution | 3.5 M | 0.6 G | 68.6
Linear Layer | 3.5 M | 0.6 G | 67.3
Average Pooling | 3.4 M | 0.6 G | 68.6
Maximum Pooling (FaViT-B0) | 3.4 M | 0.6 G | 68.9

đại diện cho một cấu hình trong đó tỷ lệ dilation cho mỗi nhóm được đặt thành 1, dẫn đến các query được trích xuất có thông tin cấp độ thấp nhất. Mặt khác, FaSA-high được cấu hình để có cửa sổ cục bộ tương tự kích thước với bản đồ đặc trưng, nhấn mạnh thông tin cấp độ cao hơn. Bảng X trình bày kết quả, cho thấy FaSA đề xuất liên tục vượt trội so với FaSA-low và FaSA-high, chỉ ra tính hiệu quả của việc tổng hợp thông tin đa cấp độ trong FaSA.

Cấu trúc tối ưu hóa với đặc trưng toàn cục. FaSA đề xuất giới thiệu tỷ lệ dilation để tăng kích thước cửa sổ cục bộ và mô hình phụ thuộc tầm xa nhưng không toàn cục. Tuy nhiên, chúng tôi lập luận rằng việc giới thiệu một lượng thích hợp các đặc trưng toàn cục có thể giúp cải thiện hiệu suất mô hình. Để điều tra điều này, chúng tôi chia một phần kênh và trích xuất đặc trưng toàn cục từ nó sử dụng phương pháp trong [24]. Bảng XI chứng minh rằng việc giới thiệu đặc trưng toàn cục cải thiện hiệu suất mô hình trong khi tăng chi phí tính toán. Tuy nhiên, sự cải thiện này đi kèm với chi phí tăng độ phức tạp tính toán. Để đạt được sự cân bằng giữa hiệu suất và chi phí, chúng tôi trích xuất đặc trưng toàn cục từ 1/8 kênh, trong khi FaSA xử lý phần còn lại. Cấu hình này đạt được sự cân bằng lý tưởng giữa cải thiện hiệu suất và chi phí tính toán có thể quản lý.

Toán tử của cross-window fusion. Trong bước cross-window fusion của FaSA, chúng tôi sử dụng một hàm tổng hợp đối xứng σ(·) để kết hợp các đặc trưng của các điểm được lấy mẫu tại cùng vị trí trong các cửa sổ khác nhau. Trong thực tế, toán tử fusion là average pooling. Để tìm toán tử tốt nhất, chúng tôi sử dụng FaViT-B0 làm benchmark để kiểm tra hiệu suất của các toán tử khác nhau, bao gồm pointwise convolution, linear layer, maximum pooling layer, và average pooling layer. Chúng tôi đánh giá độ chính xác phân loại trên CIFAR100, đào tạo 100 epoch từ đầu, và các chiến lược đào tạo khác nhất quán với Phần IV-A. Bảng XII cho thấy kết quả so sánh của các toán tử khác nhau, trong đó việc sử dụng các toán tử có thể học như pointwise convolution và linear layer tăng tham số, nhưng không có lợi thế hiệu suất. Hơn nữa, chúng tôi so sánh hai lớp pooling phổ biến và cuối cùng chọn maximum pooling với hiệu suất tốt hơn.

V. CÔNG VIỆC TƯƠNG LAI
FaViT đề xuất trong bài báo này thực sự tìm được sự cân bằng giữa chi phí và hiệu suất, nhưng vẫn có những lĩnh vực cần cải thiện. Thứ nhất, so với một số transformer hiện có [24], [26], FaViT đề xuất chỉ đổi mới cơ chế attention cốt lõi, mà không có lớp patch embedding và lớp feedforward được thiết kế đặc biệt. Những lớp này có thể không phù hợp tối ưu với FaSA, ảnh hưởng đến hiệu suất. Thứ hai, một số transformer hiện có [38], [86], [87] đã bắt đầu khám phá việc sử dụng convolution, pooling, hoặc multilayer perceptron để thay thế self-attention. So với các phương pháp này, FaViT không liên quan đến công việc liên quan, nhưng chúng có tính hướng dẫn và hấp dẫn. Do đó, công việc tương lai của chúng tôi sẽ tập trung vào tối ưu hóa mô hình, thiết kế cấu trúc mô hình với hiệu quả cao hơn và hiệu suất tốt hơn. Chúng tôi cũng sẽ khám phá thêm tính khả thi của các cách tiếp cận khác để triển khai phân tích ma trận attention. Hơn nữa, chúng tôi sẽ đánh giá hiệu suất của các phương pháp hiện có và FaViT đề xuất trên xử lý hình ảnh độ phân giải cao, làm cho transformer có giá trị hơn cho các ứng dụng thực tế.

VI. KẾT LUẬN
Bài báo này đề xuất một cơ chế factorization self-attention mới (FaSA) để khám phá sự cân bằng tối ưu giữa chi phí tính toán và khả năng mô hình hóa phụ thuộc tầm xa. Chúng tôi giới thiệu một phép toán phân tích để thu được thông tin tầm xa và đa cấp độ đồng thời. Với sự hỗ trợ của FaSA, phụ thuộc tầm xa sẽ được mô hình hóa với chi phí tính toán tương đương cửa sổ cục bộ. Các thí nghiệm mở rộng cho thấy mô hình đề xuất đạt được hiệu suất tiên tiến và tính bền vững vượt trội. Chúng tôi hy vọng công việc này sẽ cung cấp tham chiếu và cảm hứng cho nghiên cứu tương lai về visual transformer.

LỜI CẢM ƠN
Công việc này được hỗ trợ tài chính bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62101032), Chương trình Tài trợ Nhà khoa học trẻ Ưu tú của Hiệp hội Khoa học và Công nghệ Trung Quốc (Số YESS20220448), và Chương trình Tài trợ Nhà khoa học trẻ Ưu tú của Hiệp hội Khoa học và Công nghệ Bắc Kinh (Số BYESS2022167).

TÀI LIỆU THAM KHẢO
[1] A. Krizhevsky, I. Sutskever, và G. E. Hinton, "Imagenet classification with deep convolutional neural networks," Communications of the ACM, vol. 60, pp. 84 – 90, 2012.
[2] T. He, Z. Zhang, H. Zhang, Z. Zhang, J. Xie, và M. Li, "Bag of tricks for image classification with convolutional neural networks," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 558–567, 2019.
[3] B. Zoph và Q. V. Le, "Neural architecture search with reinforcement learning," ArXiv, vol. abs/1611.01578, 2017.
[4] R. Zhang, L. Jiao, D. Wang, F. Liu, X. Liu, và S. Yang, "A fast evolutionary knowledge transfer search for multiscale deep neural architecture," IEEE Transactions on Neural Networks and Learning Systems, 2023.
[5] M. Tan và Q. V. Le, "Efficientnet: Rethinking model scaling for convolutional neural networks," ArXiv, vol. abs/1905.11946, 2019.
[6] Y. Liu, Y. Zhang, Y. Wang, F. Hou, J. Yuan, J. Tian, Y. Zhang, Z. Shi, J. Fan, và Z. He, "A survey of visual transformers," IEEE Transactions on Neural Networks and Learning Systems, 2023.
[7] M. Kaselimi, A. Voulodimos, I. Daskalopoulos, N. Doulamis, và A. Doulamis, "A vision transformer model for convolution-free multilabel classification of satellite imagery in deforestation monitoring," IEEE Transactions on Neural Networks and Learning Systems, 2022.
[8] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, và B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," 2021 IEEE/CVF International Conference on Computer Vision, pp. 9992–10 002, 2021.
[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, và N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," ArXiv, vol. abs/2010.11929, 2021.
[10] D. Zhou, Z. Yu, E. Xie, C. Xiao, A. Anandkumar, J. Feng, và J. M. Álvarez, "Understanding the robustness in vision transformers," trong ICML, 2022.
[11] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. J. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language models are few-shot learners," ArXiv, vol. abs/2005.14165, 2020.
[12] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," ArXiv, vol. abs/1810.04805, 2019.
[13] W. Wang, K. Zhang, Y. Su, J. Wang, và Q. Wang, "Learning cross-attention discriminators via alternating time–space transformers for visual tracking," IEEE Transactions on Neural Networks and Learning Systems, 2023.
[14] H. Chen, G. Yang, và H. Zhang, "Hider: A hyperspectral image denoising transformer with spatial–spectral constraints for hybrid noise removal," IEEE Transactions on Neural Networks and Learning Systems, 2022.
[15] X. Zhao, M. Zhang, R. Tao, W. Li, W. Liao, L. Tian, và W. Philips, "Fractional fourier image transformer for multimodal remote sensing data classification," IEEE Transactions on Neural Networks and Learning Systems, 2022.
[16] Y. Cui, J. Cheng, L. Wang, và G. Wu, "Mixformer: End-to-end tracking with iterative mixed attention," 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13 598–13 608, 2022.
[17] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, và H. J'egou, "Training data-efficient image transformers & distillation through attention," trong ICML, 2021.
[18] Z. Jiang, Q. Hou, L. Yuan, D. Zhou, Y. Shi, X. Jin, A. Wang, và J. Feng, "All tokens matter: Token labeling for training better vision transformers," trong NeurIPS, 2021.
[19] M. Ding, B. Xiao, N. C. F. Codella, P. Luo, J. Wang, và L. Yuan, "Davit: Dual attention vision transformers," ArXiv, vol. abs/2204.03645, 2022.
[20] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. C. Bovik, và Y. Li, "Maxvit: Multi-axis vision transformer," ArXiv, vol. abs/2204.01697, 2022.
[21] W. Yu, M. Luo, P. Zhou, C. Si, Y. Zhou, X. Wang, J. Feng, và S. Yan, "Metaformer is actually what you need for vision," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 819–10 829.
[22] J. Jiao, Y.-M. Tang, K.-Y. Lin, Y. Gao, J. Ma, Y. Wang, và W.-S. Zheng, "Dilateformer: Multi-scale dilated transformer for visual recognition," IEEE Transactions on Multimedia, 2023.
[23] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, và L. Shao, "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions," 2021 IEEE/CVF International Conference on Computer Vision, pp. 548–558, 2021.
[24] ——, "Pvtv2: Improved baselines with pyramid vision transformer," ArXiv, vol. abs/2106.13797, 2022.
[25] F. Chollet, "Xception: Deep learning with depthwise separable convolutions," 2017 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1800–1807, 2017.
[26] S. Ren, D. Zhou, S. He, J. Feng, và X. Wang, "Shunted self-attention via multi-scale token aggregation," arXiv preprint arXiv:2111.15193, 2021.

--- TRANG 11 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 11

[27] K. Patel, A. M. Bur, F. Li, và G. Wang, "Aggregating global features into local vision transformer," ArXiv, vol. abs/2201.12903, 2022.
[28] G. Li, D. Xu, X. Cheng, L. Si, và C. Zheng, "Simvit: Exploring a simple vision transformer with sliding windows," 2022 IEEE International Conference on Multimedia and Expo, pp. 1–6, 2022.
[29] Y. Bai, J. Mei, A. L. Yuille, và C. Xie, "Are transformers more robust than cnns?" trong Neural Information Processing Systems, 2021.
[30] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Álvarez, và P. Luo, "Segformer: Simple and efficient design for semantic segmentation with transformers," trong Neural Information Processing Systems, 2021.
[31] S. Paul và P.-Y. Chen, "Vision transformers are robust learners," trong AAAI Conference on Artificial Intelligence, 2022.
[32] D. Hendrycks và T. G. Dietterich, "Benchmarking neural network robustness to common corruptions and perturbations," ArXiv, vol. abs/1903.12261, 2019.
[33] J. Li, R. Socher, và S. C. H. Hoi, "Dividemix: Learning with noisy labels as semi-supervised learning," ArXiv, vol. abs/2002.07394, 2020.
[34] C. Tian, W. Wang, X. Zhu, X. Wang, J. Dai, và Y. Qiao, "Vl-ltr: Learning class-wise visual-linguistic representation for long-tailed visual recognition," ArXiv, vol. abs/2111.13579, 2021.
[35] H. Che, J. Wang, và A. Cichocki, "Bicriteria sparse nonnegative matrix factorization via two-timescale duplex neurodynamic optimization," IEEE Transactions on Neural Networks and Learning Systems, 2021.
[36] X. Liu, J. Wang, Z. Li, Z. Shi, X. Fu, và L. Qiu, "Non-line-of-sight reconstruction with signal–object collaborative regularization," Light: Science & Applications, vol. 10, no. 1, p. 198, 2021.
[37] Y.-X. Ren, J. Wu, Q. T. Lai, H. M. Lai, D. M. Siu, W. Wu, K. K. Wong, và K. K. Tsia, "Parallelized volumetric fluorescence microscopy with a reconfigurable coded incoherent light-sheet array," Light: Science & Applications, vol. 9, no. 1, p. 8, 2020.
[38] Q. Zhang, Y. Xu, J. Zhang, và D. Tao, "Vsa: Learning varied-size window attention in vision transformers," trong European conference on computer vision. Springer, 2022, pp. 466–483.
[39] Z. Zhang, Y. Chen, D. Zhang, Y. Qian, và H. Wang, "Ctfnet: Long-sequence time-series forecasting based on convolution and time–frequency analysis," IEEE Transactions on Neural Networks and Learning Systems, 2023.
[40] X. Fu, W. Wang, Y. Huang, X. Ding, và J. Paisley, "Deep multiscale detail networks for multiband spectral image sharpening," IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 5, pp. 2090–2104, 2020.
[41] C. Liu, Y. Yao, D. Luo, Y. Zhou, và Q. Ye, "Self-supervised motion perception for spatiotemporal representation learning," IEEE Transactions on Neural Networks and Learning Systems, 2022.
[42] P. Wang, X. Wang, H. Luo, J. Zhou, Z. Zhou, F. Wang, H. Li, và R. Jin, "Scaled relu matters for training vision transformers," trong Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 3, 2022, pp. 2495–2503.
[43] K. He, X. Zhang, S. Ren, và J. Sun, "Deep residual learning for image recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.
[44] Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, và Z. Liu, "Mobile-former: Bridging mobilenet and transformer," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 5270–5279.
[45] Z. Zhang, H. Zhang, L. Zhao, T. Chen, và T. Pfister, "Aggregating nested transformers," arXiv preprint arXiv:2105.12723, vol. 2, no. 3, p. 5, 2021.
[46] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. H. Tay, J. Feng, và S. Yan, "Tokens-to-token vit: Training vision transformers from scratch on imagenet," 2021 IEEE/CVF International Conference on Computer Vision, pp. 538–547, 2021.
[47] P. Zhang, X. Dai, J. Yang, B. Xiao, L. Yuan, L. Zhang, và J. Gao, "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding," 2021 IEEE/CVF International Conference on Computer Vision, pp. 2978–2988, 2021.
[48] C.-F. Chen, Q. Fan, và R. Panda, "Crossvit: Cross-attention multi-scale vision transformer for image classification," 2021 IEEE/CVF International Conference on Computer Vision, pp. 347–356, 2021.
[49] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, và Y. Wang, "Transformer in transformer," trong NeurIPS, 2021.
[50] P. Ren, C. Li, G. Wang, Y. Xiao, và Q. D. X. L. X. Chang, "Beyond fixation: Dynamic window visual transformer," ArXiv, vol. abs/2203.12856, 2022.
[51] Q. Han, Z. Fan, Q. Dai, L. Sun, M.-M. Cheng, J. Liu, và J. Wang, "On the connection between local attention and dynamic depth-wise convolution," arXiv preprint arXiv:2106.04263, 2021.
[52] Q. Yu, Y. Xia, Y. Bai, Y. Lu, A. L. Yuille, và W. Shen, "Glance-and-gaze vision transformer," Advances in Neural Information Processing Systems, vol. 34, pp. 12 992–13 003, 2021.
[53] Z. Dai, H. Liu, Q. V. Le, và M. Tan, "Coatnet: Marrying convolution and attention for all data sizes," Advances in neural information processing systems, vol. 34, pp. 3965–3977, 2021.
[54] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, và S. Xie, "A convnet for the 2020s," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 976–11 986.
[55] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, và C. Feichtenhofer, "Mvitv2: Improved multiscale vision transformers for classification and detection," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 4804–4814.
[56] Q. Zhang, Y. Xu, J. Zhang, và D. Tao, "Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond," International Journal of Computer Vision, pp. 1–22, 2023.
[57] W. Wang, W. Chen, Q. Qiu, L. Chen, B. Wu, B. Lin, X. He, và W. Liu, "Crossformer++: A versatile vision transformer hinging on cross-scale attention," arXiv preprint arXiv:2303.06908, 2023.
[58] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, và A. Vaswani, "Bottleneck transformers for visual recognition," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 16 519–16 529.
[59] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, và J. Gao, "Focal attention for long-range interactions in vision transformers," trong NeurIPS, 2021.
[60] R. Wightman, H. Touvron, và H. J'egou, "Resnet strikes back: An improved training procedure in timm," arXiv preprint arXiv:2110.00476, 2021.
[61] Z. Huang, Y. Ben, G. Luo, P. Cheng, G. Yu, và B. Fu, "Shuffle transformer: Rethinking spatial shuffle for vision transformer," arXiv preprint arXiv:2106.03650, 2021.
[62] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Q. Hou, và J. Feng, "Deepvit: Towards deeper vision transformer," ArXiv, vol. abs/2103.11886, 2021.
[63] J. Fang, L. Xie, X. Wang, X. Zhang, W. Liu, và Q. Tian, "Msg-transformer: Exchanging local spatial information by manipulating messenger tokens," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 063–12 072.
[64] Y. Lee, J. Kim, J. Willette, và S. J. Hwang, "Mpvit: Multi-path vision transformer for dense prediction," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 7287–7296.
[65] X. Ding, X. Zhang, J. Han, và G. Ding, "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 963–11 975.
[66] M.-H. Guo, C.-Z. Lu, Z.-N. Liu, M.-M. Cheng, và S.-M. Hu, "Visual attention network," Computational Visual Media, pp. 1–20, 2023.
[67] J. Li, X. Xia, W. Li, H. Li, X. Wang, X. Xiao, R. Wang, M. Zheng, và X. Pan, "Next-vit: Next generation vision transformer for efficient deployment in realistic industrial scenarios," arXiv preprint arXiv:2207.05501, 2022.
[68] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, và L. Fei-Fei, "Imagenet large scale visual recognition challenge," International Journal of Computer Vision, vol. 115, pp. 211–252, 2015.
[69] T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, và C. L. Zitnick, "Microsoft coco: Common objects in context," trong ECCV, 2014.
[70] T.-Y. Lin, P. Goyal, R. B. Girshick, K. He, và P. Dollár, "Focal loss for dense object detection," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, pp. 318–327, 2020.
[71] K. He, G. Gkioxari, P. Dollár, và R. B. Girshick, "Mask r-cnn," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, pp. 386–397, 2020.
[72] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, và C. Shen, "Twins: Revisiting the design of spatial attention in vision transformers," trong NeurIPS, 2021.
[73] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, và A. Torralba, "Semantic understanding of scenes through the ade20k dataset," International Journal of Computer Vision, vol. 127, pp. 302–321, 2018.
[74] A. Kirillov, R. B. Girshick, K. He, và P. Dollár, "Panoptic feature pyramid networks," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6392–6401, 2019.

--- TRANG 12 ---
TẠP CHÍ CỦA CÁC TỆP CLASS L ATEX, TẬP 14, SỐ 8, THÁNG 8 2021 12

[75] Y. Li, G. Yuan, Y. Wen, E. Hu, G. Evangelidis, S. Tulyakov, Y. Wang, và J. Ren, "Efficientformer: Vision transformers at mobilenet speed," ArXiv, vol. abs/2206.01191, 2022.
[76] H. Lin, X. Cheng, X. Wu, F. Yang, D. Shen, Z. Wang, Q. Song, và W. Yuan, "Cat: Cross attention in vision transformer," 2022 IEEE International Conference on Multimedia and Expo, pp. 1–6, 2022.
[77] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, và L.-C. Chen, "Mobilenetv2: Inverted residuals and linear bottlenecks," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4510–4520, 2018.
[78] D. Hendrycks và T. G. Dietterich, "Benchmarking neural network robustness to common corruptions and perturbations," ArXiv, vol. abs/1903.12261, 2019.
[79] T. Xiao, T. Xia, Y. Yang, C. Huang, và X. Wang, "Learning from massive noisy labeled data for image classification," 2015 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2691–2699, 2015.
[80] W. Li, L. Wang, W. Li, E. Agustsson, và L. V. Gool, "Webvision database: Visual learning and understanding from web data," ArXiv, vol. abs/1708.02862, 2017.
[81] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, và H. J'egou, "Resmlp: Feedforward networks for image classification with data-efficient training," IEEE transactions on pattern analysis and machine intelligence, vol. PP, 2022.
[82] G. V. Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, và S. J. Belongie, "The inaturalist species classification and detection dataset," 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8769–8778, 2018.
[83] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. J'egou, và M. Douze, "Levit: a vision transformer in convnet's clothing for faster inference," 2021 IEEE/CVF International Conference on Computer Vision, pp. 12 239–12 249, 2021.
[84] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, và S. J. Belongie, "Class-balanced loss based on effective number of samples," 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9260–9269, 2019.
[85] A. Krizhevsky, G. Hinton et al., "Learning multiple layers of features from tiny images," 2009.
[86] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., "Mlp-mixer: An all-mlp architecture for vision," Advances in neural information processing systems, vol. 34, pp. 24 261–24 272, 2021.
[87] J. Guo, K. Han, H. Wu, Y. Tang, X. Chen, Y. Wang, và C. Xu, "Cmt: Convolutional neural networks meet vision transformers," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 12 175–12 185.
