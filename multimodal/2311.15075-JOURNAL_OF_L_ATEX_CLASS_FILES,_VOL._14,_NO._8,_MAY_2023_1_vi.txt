# 2311.15075.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2311.15075.pdf
# Kích thước tệp: 7421181 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 1
Mug-STAN: Thích ứng các Mô hình Tiền huấn luyện Hình ảnh-Ngôn ngữ
cho Hiểu Video Tổng quát
Ruyang Liu, Jingjia Huang, Wei Gao, Thomas H. Li, Ge Li
Tóm tắt—Các mô hình tiền huấn luyện hình ảnh-ngôn ngữ quy mô lớn, ví dụ
như CLIP, đã thể hiện khả năng đáng kể trong việc thu thập kiến thức đa phương tiện
tổng quát thông qua dữ liệu hình ảnh-văn bản quy mô web. Bất chấp hiệu suất ấn tượng
của các mô hình hình ảnh-ngôn ngữ trên nhiều tác vụ hình ảnh khác nhau, cách mở rộng
hiệu quả chúng trên hiểu video tổng quát vẫn là một lĩnh vực đang được khám phá liên tục.
Trong bài báo này, chúng tôi nghiên cứu việc chuyển đổi từ hình ảnh sang video từ góc độ
của mô hình và dữ liệu, tiết lộ hai trở ngại chính cản trở việc thích ứng các mô hình
hình ảnh-ngôn ngữ: mô hình hóa thời gian không khái quát và dữ liệu video-văn bản
bị lệch một phần. Để giải quyết những thách thức này, chúng tôi đề xuất Mạng Phụ trợ
Không gian-Thời gian với mô-đun căn chỉnh Hướng dẫn lẫn nhau (Mug-STAN) – một
khung đơn giản nhưng hiệu quả mở rộng mô hình hình ảnh-văn bản đến các tác vụ video
đa dạng và dữ liệu video-văn bản. Cụ thể, STAN áp dụng cấu trúc nhánh với các mô-đun
không gian-thời gian được phân tách để cho phép mô hình hóa thời gian khái quát được,
trong khi Mug ngăn chặn việc lệch bằng cách giới thiệu tập hợp đặc trưng theo token
của một phương tiện từ phương tiện khác. Kết quả thử nghiệm rộng rãi xác minh
Mug-STAN cải thiện đáng kể việc thích ứng các mô hình tiền huấn luyện ngôn ngữ-hình ảnh
như CLIP và CoCa ở cả giai đoạn hậu tiền huấn luyện và tinh chỉnh video-văn bản. Với
giải pháp của chúng tôi, kết quả zero-shot và tinh chỉnh tiên tiến nhất trên nhiều tập dữ liệu
hạ nguồn khác nhau, bao gồm MSR-VTT, DiDeMo, LSMDC, Kinetics-400,
Something-Something-2, HMDB-51, UCF-101 và AVA, đã được đạt được. Hơn nữa, bằng cách
tích hợp Mug-STAN được tiền huấn luyện với mô hình đối thoại đa phương tiện mới nổi, chúng ta
có thể thực hiện trò chuyện video zero-shot. Mã nguồn có sẵn tại
https://github.com/farewellthree/STAN
Từ khóa Index—Mô hình hình ảnh-ngôn ngữ, mô hình hóa thời gian,
lệch một phần, Mug-STAN, hiểu video tổng quát

I. GIỚI THIỆU
TRONG ba năm qua, cộng đồng thị giác máy tính đã
chứng kiến thành công đáng kể của các mô hình hình ảnh-ngôn ngữ
tiền huấn luyện quy mô web, như CLIP [1], CoCa [2], và
BEiTv3 [3]. Tuy nhiên, việc phát triển các mô hình video-ngôn ngữ
cơ bản là thách thức, do chi phí cao của tài nguyên tính toán
cần thiết cho tiền huấn luyện và tính có sẵn hạn chế của dữ liệu
về mặt quy mô, chất lượng và tính đa dạng. Thay vì tập trung vào
việc phát triển các mô hình video-ngôn ngữ tiền huấn luyện [4], [5],
một cách tiếp cận thay thế và đầy hứa hẹn là chuyển kiến thức
phong phú trong các mô hình hình ảnh-ngôn ngữ tiền huấn luyện
sang miền video, điều này đã thu hút sự chú ý ngày càng tăng
trong những năm gần đây [6]–[11].

Việc mở rộng các mô hình hình ảnh 2D tiền huấn luyện sang
lĩnh vực video đã được khám phá rộng rãi trong lĩnh vực

Ruyang Liu, Wei Gao, Thomas H. Li, và Ge Li thuộc Trường
Kỹ thuật Điện tử và Máy tính, Trường Đại học Sau đại học Thâm Quyến
Đại học Bắc Kinh. Ruyang Liu cũng thuộc Phòng thí nghiệm Bằng Thành. E-mail:
{ruyang@stu, gaowei262@, geli@ece., thomas@}pku.edu.cn
Jingjia Huang thuộc ByteDance Inc. E-mail: huangjingjia@bytedance.com
Tác giả liên hệ: Gao Wei.

học video [12], [13]. Thách thức chính nằm ở sự chênh lệch
về phương tiện giữa hình ảnh và video. Cụ thể, video vốn dĩ
chứa thông tin thời gian độc đáo, và dữ liệu video-văn bản
thường phức tạp hơn và nhiều nhiễu hơn so với dữ liệu hình ảnh-văn bản.
Do đó, cuộc điều tra của chúng tôi, dựa trên các phương pháp
mô hình hóa thời gian hiện có và nhiều tập dữ liệu video-ngôn ngữ
khác nhau, đã tiết lộ hai điểm thường bị bỏ qua. Như mô tả
trong Hình 1, chúng tôi đã phát hiện ra rằng các nỗ lực hiện tại
trong mô hình hóa thời gian chủ yếu bị giới hạn ở các tác vụ
video-ngôn ngữ [6], [7], [14], [15] hoặc các tác vụ đặc thù video [9]–[11],
dẫn đến hiệu quả giảm sút khi áp dụng cho một loại tác vụ video khác.
Trong khi đó, quan sát của chúng tôi cho thấy các mẫu huấn luyện
được ghép đôi video-văn bản thường gặp phải tình trạng lệch một phần
trong cả tập dữ liệu tiền huấn luyện và hạ nguồn.

Để có cái nhìn sâu sắc về vấn đề đầu tiên, chúng tôi tiếp tục
đi sâu vào cấu trúc của các mô-đun thời gian dựa trên CLIP hiện có.
Chúng tôi thấy rằng các nỗ lực hiện tại có thể được phân loại thô
thành các phương pháp dựa trên cấu trúc phía sau và các phương pháp
dựa trên cấu trúc trung gian như được thể hiện trong Hình 2. Các phương pháp
dựa trên cấu trúc phía sau [7], [14]–[17] áp dụng chiến lược mô hình hóa muộn,
sử dụng CLIP làm bộ trích xuất đặc trưng và áp dụng mô hình hóa thời gian
vào các embedding được trích xuất độc lập từ các khung hình khác nhau.
Được xây dựng trên các embedding có tính ngữ nghĩa cao, cấu trúc này,
trong khi có lợi cho việc bảo tồn các biểu diễn thị giác-ngôn ngữ
được căn chỉnh tốt, lại thiếu khả năng nắm bắt các mẫu thị giác
không gian-thời gian mức thấp giữa các khung hình, những điều
cần thiết cho hiểu video. Kết quả là, các phương pháp dựa trên
cấu trúc phía sau có xu hướng thể hiện các cải thiện hiệu suất
nhỏ, một xu hướng trở nên đặc biệt rõ ràng trong các tác vụ
nhận dạng hành động nơi các mẫu thị giác không gian-thời gian
mức thấp là quan trọng. Không giống như các phương pháp dựa trên
cấu trúc phía sau, các phương pháp dựa trên cấu trúc trung gian
[10], [11], [13] trang bị cho CLIP khả năng mô hình hóa thời gian
bằng cách tích hợp các mô-đun mô hình hóa thời gian giữa các lớp CLIP,
điều này thấy cải thiện đáng kể trong tác vụ nhận dạng video.
Tuy nhiên, chúng tôi đã quan sát thấy rằng việc tích hợp các mô-đun
bổ sung bên trong CLIP sẽ tác động đến kiến thức ngữ nghĩa
mức cao được tiền huấn luyện trong mô hình, dẫn đến tác động
tầm thường hoặc thậm chí tiêu cực đối với tác vụ truy xuất
văn bản-video. Những mẫu thống kê này rõ ràng hơn trong Hình 3,
nơi cả cấu trúc phía sau và cấu trúc trung gian đều xuất sắc
chỉ trong các tác vụ tương ứng của chúng.

Trái ngược với nghiên cứu rộng rãi về mô hình hóa thời gian,
một vấn đề quan trọng khác đã nhận được sự chú ý hạn chế:
các mẫu huấn luyện được ghép đôi video-văn bản thường thể hiện
sự lệch một phần. Lệch một phần đề cập đến tình huống mà
thông tin được căn chỉnh giữa một video và văn bản tương ứng
của nó chỉ được phân bố qua các khung hình và

--- TRANG 2 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 2
1.4 2.0 -0.5 0.2 3.8 
0.5 0.5 2.0 3.7 4.2 
-1012345Cải thiện trên CLIP Truy xuất (R@1)Nhận dạng (Top1 Acc)
CLIP4clip-seqTransCLIP2video-TDBSTadapterXCLIPSTAN(Của chúng tôi)

(a)Truy vấn: một đoạn trailer cho một bộ phim sắp tới với mọi người trên bãi biển
(b)38%23%39%trêngiữadưới42%23%35%19%39%42%MSRVTTDiDeMoWebvid

Hình 1. Hai vấn đề trong việc chuyển đổi hình ảnh sang video cho các mô hình thị giác-ngôn ngữ. (a) Khả năng khái quát: Chúng tôi minh họa các mô-đun thời gian dựa trên CLIP gặp khó khăn trong việc khái quát hóa qua các tác vụ video khác nhau. Chúng tôi trình bày hiệu suất của nhiều mô hình so với đường cơ sở, dựa trên CLIP với pooling trung bình. Các mô hình bao gồm các mô hình truy xuất văn bản-video CLIP4clip-seqTrans [7] và CLIP2video-TDB [14], cũng như các mô hình nhận dạng video STadapter [11] và XCLIP [10]. Đánh giá dựa trên Recall@1 cho MSRVTT [18] và Top-1 accuracy cho Kinetics-400 [19]. (b) Lệch Một phần: Ở trên, chúng tôi trình bày một mẫu huấn luyện bị lệch trong MSRVTT, nơi chỉ có "mọi người trên bãi biển" và khung hình thứ 1, 5 và 6 được căn chỉnh với nhau. Ở dưới, chúng tôi đánh giá định lượng mức độ lệch một phần trong các tập dữ liệu video-văn bản, bao gồm MSRVTT, DiDeMo [20], và WebVid2.5M [21]. Mức độ căn chỉnh dần dần xấu đi từ "trên" xuống "dưới".

videooutput⊕
videooutput
videooutput
Lớp mô hình hóa thời gianLuồng dữ liệuLớp CLIP

Hình 2. Các cấu trúc khác nhau của mô hình hóa thời gian: cấu trúc phía sau (trái), cấu trúc trung gian (giữa), và cấu trúc nhánh của chúng tôi (phải).

cụm từ cụ thể, trong khi các thành phần khác của video/văn bản
có nhiều nhiễu cản trở việc căn chỉnh thị giác-ngôn ngữ chính xác
và thích ứng hình ảnh sang video mạnh mẽ. Hình 1(b) cho thấy
một trường hợp lệch một phần, nơi chỉ có cụm từ "mọi người trên
bãi biển" và các khung hình được đánh dấu đỏ được căn chỉnh
về mặt ngữ nghĩa. Do sự phức tạp và dư thừa của nội dung video,
những trường hợp như vậy xảy ra thường xuyên hơn nhiều trong
dữ liệu video-văn bản so với dữ liệu hình ảnh-văn bản. Hơn nữa,
tình hình thậm chí còn nghiêm trọng hơn trong các tập dữ liệu
tiền huấn luyện video, được xây dựng bằng cách sử dụng video
hướng dẫn và lời tường thuật nhiều nhiễu [4], [22], [23]. Để đánh giá
định lượng sự lệch một phần có trong các tập dữ liệu video, chúng tôi
đã chọn và phân tích hai tập dữ liệu hạ nguồn (MSR-VTT [18] và
DiDeMo [20]) và một tập dữ liệu tiền huấn luyện (WebVid2.5m [21]).
Cụ thể, chúng tôi sử dụng CLIP-ViT-L/14 [1] để đo lường sự lệch,
sử dụng tương tự tích vô hướng theo sau bởi sigmoid để tính toán
mối tương quan giữa văn bản và mỗi khung hình. Một khung hình
được coi là căn chỉnh với văn bản nếu xác suất vượt quá 0.5.
Sau đó, chúng tôi phân loại mức độ căn chỉnh video-văn bản
thành ba cấp: (1) trên khi hơn 2/3 khung hình được căn chỉnh
với văn bản. (2) dưới khi ít hơn 1/3 khung hình được căn chỉnh
với văn bản. (3) giữa ở giữa hai cấp. Như được tiết lộ trong
Hình 1(b), trong tất cả ba tập dữ liệu, hơn một nửa các cặp
video-văn bản bị lệch một phần (giữa và dưới), ngay cả khi
những tập dữ liệu này được công nhận rộng rãi về chất lượng
cao của chúng trong các tác vụ video-văn bản.

Lệch một phần, cùng với mô hình hóa thời gian, đã nảy sinh
một thách thức tiếp theo: hậu tiền huấn luyện¹ các mô hình
hình ảnh-ngôn ngữ trên các tập dữ liệu video-ngôn ngữ quy mô lớn
cho thấy lợi ích rất hạn chế. Như mô tả trong Hình 3(b), chúng ta
có thể quan sát thấy rằng CLIP, sau khi được hậu tiền huấn luyện
trên WebVid10M hoặc HowTo100M, không vượt trội đáng kể so với
đường cơ sở không có hậu tiền huấn luyện.

Từ phân tích nêu trên, chúng tôi kết luận hai yếu tố chính
cho việc mở rộng các mô hình hình ảnh-ngôn ngữ tiền huấn luyện
sang miền video: (1) Mô hình hóa thời gian hiệu quả trong khi
tận dụng kiến thức ở các cấp độ biểu diễn khác nhau. (2) Ngăn chặn
sự lệch một phần trong quá trình huấn luyện trên dữ liệu video-văn bản.
Vì vậy, chúng tôi đề xuất Mạng Phụ trợ Không gian-Thời gian với
mô-đun căn chỉnh Hướng dẫn lẫn nhau (Mug-STAN) - một khung
plug-and-use thích ứng các mô hình hình ảnh-ngôn ngữ cho các tác vụ
video tổng quát, trong đó STAN giới thiệu mô hình hóa thời gian
hiệu quả và Mug giảm thiểu lệch một phần trong quá trình huấn luyện.
Trong Hình 2 và 3(a), có thể nhận thấy rằng cấu trúc mô hình hóa
thời gian trong STAN thể hiện hiệu suất mạnh mẽ trong cả tác vụ
truy xuất và tác vụ nhận dạng. Trong Hình 3(b), chúng ta có thể
thấy rằng STAN và Mug đóng góp đáng kể vào hiệu quả của
hậu tiền huấn luyện tương ứng, trong đó Mug xuất sắc đặc biệt
tốt trên tập dữ liệu HowTo100M nhiều nhiễu.

Cụ thể, thay vì cấu trúc phía sau hoặc trung gian, STAN
được đề xuất của chúng tôi giới thiệu một cấu trúc nhánh đặc biệt
nằm bên ngoài backbone thị giác, có nhiều cấp độ

¹Tiền huấn luyện thêm trên tập dữ liệu video-văn bản quy mô tương đối lớn dựa trên
các mô hình hình ảnh tiền huấn luyện cho các tác vụ video hạ nguồn được gọi là hậu
tiền huấn luyện. Tinh chỉnh có nghĩa là điều chỉnh trực tiếp để thích ứng các mô hình
hình ảnh-văn bản trên các tập dữ liệu video hạ nguồn.

--- TRANG 3 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 3
434547495153
CLIPSTANMug-STANR@1 trên DiDeMokhông có hậu tiền huấn luyệnHậu tiền huấn luyện WebVid10MHậu tiền huấn luyện HowTo100M

(b)-10123456
01234R@1 trên Truy xuấtTop 1 Acc trên Nhận dạngCLIP4clip-seqTransCLIP2video-TDBDRL-WtiTS2NetHunYuanSeparate-STStadapterXCLIPCủa chúng tôi

(a)Cấu trúc Phía sauCấu trúc Trung gian

Hình 3. (a) So sánh hiệu suất của các phương pháp khác nhau trên cả truy xuất văn bản-video và nhận dạng video. Đánh giá dựa trên Recall@1 cho MSRVTT [18] và Top-1 accuracy cho Kinetics-400 [19]. Các phương pháp được phân cụm thành cấu trúc phía sau, cấu trúc trung gian, và cấu trúc nhánh của chúng tôi. (b) So sánh hiệu suất của hậu tiền huấn luyện trên các mô hình khác nhau. Chúng tôi báo cáo kết quả tinh chỉnh của Recall@1 trên truy xuất văn bản-video DiDemo. Dựa trên CLIP, mô hình hóa thời gian hiệu quả (STAN) và ngăn chặn lệch một phần (Mug) tương ứng mang lại những cải thiện đáng chú ý.

đầu vào, như được thể hiện trong Hình 2. Cấu trúc mới này cho phép STAN
làm giàu các đặc trưng của khung hình video với ngữ cảnh không gian-thời gian,
tận dụng các cấp đầu ra khác nhau của mô hình hình ảnh-văn bản,
trong khi bảo tồn việc lan truyền thuận của mô hình nguồn. Do đó,
nó có thể sử dụng hiệu quả cả kiến thức cấp cao và cấp thấp
từ mô hình tiền huấn luyện đồng thời, làm cho nó có thể thích ứng
với nhiều tác vụ video hạ nguồn khác nhau. STAN bao gồm nhiều lớp
với thiết kế không gian-thời gian được tách biệt. Mỗi lớp thực hiện
mô hình hóa không gian-thời gian bằng cách xếp chồng luân phiên
hai mô-đun riêng biệt: một mô-đun trong khung hình và một mô-đun
giữa các khung hình. Cách tiếp cận này cho phép lớp này nâng cao
hiệu suất mô hình bằng cách tái sử dụng các tham số tiền huấn luyện
từ các mô hình hình ảnh-văn bản tiền huấn luyện để khởi tạo
các mô-đun không gian trong khung hình. Trong khi đó, Mug được
xây dựng bằng cách sử dụng cơ chế mô hình hóa tương tác theo token
không tham số với chi phí tính toán không đáng kể, có thể dễ dàng
được cắm vào các phương pháp tiên tiến hiện có. Với một cặp video-văn bản,
chúng ta có thể nhận được chuỗi đặc trưng theo khung hình và
chuỗi đặc trưng văn bản tương ứng. Để thực hiện căn chỉnh hướng dẫn
lẫn nhau, trước tiên chúng tôi thực hiện tương tác khung hình-token
để thu được embedding văn bản cụ thể cho khung hình cho mỗi khung hình
và embedding video cụ thể cho token cho mỗi token. Sau đó, đối với
mỗi phương tiện, chúng tôi đạt được embedding toàn cục cuối cùng của nó
thông qua hướng dẫn từ phương tiện khác. Cuối cùng, cặp biểu diễn
được hướng dẫn lẫn nhau được sử dụng trong học tương phản
trong quá trình hậu tiền huấn luyện hoặc tinh chỉnh. Bằng cách này,
chúng ta có thể nắm bắt và căn chỉnh các phần liên quan của video
và văn bản, giải phóng việc thích ứng các mô hình hình ảnh-văn bản
tiền huấn luyện khỏi vấn đề lệch một phần video-văn bản.

Thông qua các thí nghiệm rộng rãi, chúng tôi đã chứng minh
hiệu suất ấn tượng của Mug-STAN được đề xuất. Cụ thể, chúng tôi
đã triển khai Mug-STAN trên hai mô hình hình ảnh-ngôn ngữ
nổi tiếng, CLIP và CoCa. Hơn nữa, chúng tôi đã áp dụng một
góc nhìn mới về hậu huấn luyện bằng cách đánh giá mô hình của chúng tôi
trên các tập dữ liệu với các mức độ nhiễu khác nhau, như
WebVid10M và HowTo100M. Các kết quả toàn diện làm nổi bật
hiệu quả của Mug-STAN không chỉ trong việc tinh chỉnh mà còn
trong hậu tiền huấn luyện. Đáng chú ý, chúng tôi đạt được kết quả
tiên tiến trong cả cài đặt zero-shot và tinh chỉnh trên một
phạm vi đa dạng các tác vụ video, bao gồm truy xuất văn bản-video,
nhận dạng hành động video, và phát hiện video. Hơn nữa, với
sự phổ biến hiện tại của các hệ thống đối thoại đa phương tiện,
chúng tôi cũng đã cắm Mug-STAN được tiền huấn luyện vào LLaVa [24],
đạt được khả năng trò chuyện video zero-shot mà không cần
bất kỳ điều chỉnh hướng dẫn nào.

Các đóng góp chính của bài báo này là:
•Chúng tôi trình bày phân tích chuyên sâu về các yếu tố cản trở
việc thích ứng các mô hình hình ảnh-ngôn ngữ sang các miền video.
Bằng cách xem xét lại mô hình hóa thời gian trên CLIP trong
nghiên cứu hiện tại và kiểm tra cẩn thận các tập dữ liệu video-văn bản,
chúng tôi xác định mô hình hóa thời gian không khái quát và
dữ liệu video-văn bản bị lệch một phần là nguyên nhân chính
ảnh hưởng đến hiệu suất.

•Chúng tôi đề xuất Mạng Phụ trợ Không gian-Thời gian với
mô-đun căn chỉnh Hướng dẫn lẫn nhau (Mug-STAN) - một khung
đơn giản nhưng mạnh mẽ mở rộng các mô hình hình ảnh-văn bản
tiền huấn luyện đến các tác vụ video tổng quát. Trong Mug-STAN,
chúng tôi tận dụng cấu trúc nhánh mới của STAN cho mô hình hóa
thời gian hiệu quả, cho phép học thời gian kết hợp ngữ cảnh
không gian-thời gian ở nhiều cấp độ khác nhau. Ngoài ra, Mug
đóng vai trò quan trọng trong việc ngăn chặn nhiễu và khuyến khích
đóng góp của các phần được căn chỉnh tốt để đạt được căn chỉnh
video-ngôn ngữ mạnh mẽ.

•Chúng tôi tiến hành các thí nghiệm toàn diện trong nhiều
cài đặt khác nhau để đánh giá hiệu quả của Mug-STAN. Các
kết quả phong phú chứng minh rằng Mug-STAN đạt được kết quả
zero-shot và tinh chỉnh tiên tiến trên một phạm vi rộng
các tập dữ liệu và tác vụ video, cũng như khả năng đối thoại
video zero-shot.

II. CÔNG TRÌNH LIÊN QUAN

A. Tiền huấn luyện Hình ảnh-Ngôn ngữ

Tiền huấn luyện Hình ảnh-Ngôn ngữ đã thu hút sự chú ý ngày càng
tăng từ các nhà nghiên cứu trong cộng đồng thị giác máy tính
[23], [25]–[27]. Gần đây, tiền huấn luyện ngôn ngữ-hình ảnh
tương phản trên dữ liệu quy mô web [1]–[3], [28], [29] đã trải qua
thành công đáng kể, chủ yếu do hiệu suất xuất sắc của nó
khi áp dụng cho nhiều tác vụ hạ nguồn khác nhau. Một trong những
công trình nổi tiếng nhất là CLIP [1], đã thể hiện khả năng
đáng ngạc nhiên trong nhận dạng zero-shot và tổng quát hóa miền
[30], [31]. Kho kiến thức phong phú chứa trong các mô hình
hình ảnh-ngôn ngữ tiền huấn luyện này có một tương lai đầy hứa hẹn
cho việc thích ứng chúng vào các tác vụ video. May mắn thay, Mug-STAN
của chúng tôi có thể được triển khai trên các mô hình hình ảnh-ngôn ngữ
này theo cách plug-and-play, dẫn đến cải thiện hiệu suất đáng kể
trong nhiều tác vụ video khác nhau. Đáng chú ý là những tiến bộ
gần đây trong hiểu đa phương tiện phần lớn được thúc đẩy bởi
sự kết hợp của các mô hình thị giác dựa trên hình ảnh với LLM,
như Flamingo [32], BLIP-2 [33], và LLaVA [24], may mắn thay,
những mô hình đối thoại đa phương tiện này thường sử dụng
CLIP-L/14 làm bộ mã hóa thị giác. Do đó, Mug-STAN của chúng tôi
có thể được triển khai một cách liền mạch trên những mô hình này
để đạt được trò chuyện video zero-shot.

B. Tiền huấn luyện Video-Ngôn ngữ

Là một tập con của tiền huấn luyện thị giác-ngôn ngữ, tiền huấn luyện
video-ngôn ngữ cũng đã là chủ đề của nhiều khám phá

--- TRANG 4 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 4

trong những năm gần đây, như Violet [34], clipBert [35], Frozen
[21], BridgeFormer [36], và Clover [5]. Trong tiền huấn luyện video-ngôn ngữ,
các mô hình thường khởi tạo bộ mã hóa video và bộ mã hóa văn bản
với các trọng số được tiền huấn luyện riêng biệt [13], [37]–[39],
và sau đó sử dụng nhiều mục tiêu tiền huấn luyện để đạt được
căn chỉnh xuyên phương tiện và học đa phương tiện, như học
tương phản, mô hình hóa ngôn ngữ có mặt nạ, và khớp video-văn bản.
Tuy nhiên, các mô hình video-ngôn ngữ tiền huấn luyện gặp khó khăn
trong việc xử lý đồng thời mô hình hóa thời gian và căn chỉnh
phương tiện do những thách thức do khởi tạo không căn chỉnh
gây ra. Trái lại, các mô hình hình ảnh-văn bản tiền huấn luyện
vốn sở hữu kiến thức rộng lớn như một kết quả của sự đa dạng
và quy mô khổng lồ của dữ liệu hình ảnh-văn bản mà chúng được
huấn luyện. Kết quả là, khi được tinh chỉnh trên các tập dữ liệu
video-ngôn ngữ hạ nguồn, chúng tôi đã quan sát thấy những lợi thế
đáng kể của các mô hình hình ảnh-văn bản tiền huấn luyện so với
các mô hình video-ngôn ngữ tiền huấn luyện, ngay cả khi những
mô hình đầu tiên chưa được tiền huấn luyện trên các tập dữ liệu video.

Tương tự như nghiên cứu của chúng tôi, CLIP-ViP [6] là một
trong số ít các nghiên cứu đi sâu vào lĩnh vực hậu tiền huấn luyện
video. Tuy nhiên, CLIP-ViP dựa vào dữ liệu quy mô lớn và
chú thích từ một captioner bổ sung cho quá trình hậu tiền huấn luyện
của nó. Trái lại, công trình của chúng tôi chứng minh rằng với
một phương pháp phù hợp, hậu tiền huấn luyện có thể mang lại
kết quả vượt trội trên cả các tập dữ liệu nhỏ hơn (Webvid10M)
và các tập dữ liệu nhiều nhiễu (HowTo100M) mà không cần chú thích
theo khung hình bổ sung. Ngoài ra, một số nghiên cứu cũng đã
mạo hiểm vào lĩnh vực tiền huấn luyện dưới dữ liệu video-văn bản
nhiều nhiễu và lệch [40]–[42]. Miech et al. [40] và Han et al. [42]
đã giới thiệu hàm mất mát MIL-NCE và Mạng Căn chỉnh Thời gian
tương ứng cho tiền huấn luyện video-tường thuật nhiều nhiễu.
So với những công trình này, bài báo của chúng tôi khác biệt
ở ba khía cạnh: (1) Cài đặt. Các công trình trước đó chủ yếu
tập trung vào các tập dữ liệu chứa đầy các cặp video-văn bản
hoàn toàn lệch và chú thích ASR (ví dụ, Howto100M), trong khi
trọng tâm của chúng tôi nằm ở vấn đề lệch một phần, là vấn đề
tổng quát hơn và thậm chí có thể xảy ra trong các tập dữ liệu
chất lượng tương đối cao, như được mô tả trong Hình 2(b). (2) Phương pháp.
[42] sử dụng mạng hộp đen để học tương tự giữa video và văn bản,
trong khi chúng tôi đề xuất một mô-đun video-văn bản hướng dẫn
lẫn nhau không tham số để xác định và lọc bỏ các phần không
liên quan từ video và văn bản. (3) Kết quả. Trong các thí nghiệm,
chúng tôi truyền đạt kết quả tốt hơn nhiều so với những công trình
đó trong cùng cài đặt.

C. Các Mô hình Hình ảnh-Ngôn ngữ Tiền huấn luyện Cho Tác vụ Video

Trái ngược với hậu tiền huấn luyện thêm, phần lớn các nghiên cứu
hiện tại chủ yếu tập trung vào việc tinh chỉnh trực tiếp các mô hình
hình ảnh-văn bản cho các tác vụ video. Một hướng trực quan là
mô hình hóa thời gian [6], [7], [9]–[11], [14], [15], [24], [43], [44],
vì mô hình hình ảnh không thể nắm bắt thông tin thời gian. Trong
các tác vụ video-ngôn ngữ, như truy xuất văn bản-video, hầu hết
các mô hình thích ứng có xu hướng sử dụng cấu trúc dựa trên
phía sau để xử lý các khía cạnh thời gian, ví dụ, transformer
tuần tự trong [7], khối khác biệt thời gian trong [14], và mô-đun
lựa chọn token trong [15]. Bất chấp những tiến bộ đạt được bởi
các phương pháp này, mô hình hóa thời gian mà chúng cung cấp
bị hạn chế ở các embedding cấp cao và thiếu hiệu quả, như
được minh họa trong Hình 1(a). Trong các tác vụ chỉ video như
nhận dạng hành động, việc mở rộng chính thống của CLIP cho
mô hình hóa thời gian là sử dụng cấu trúc trung gian. Ví dụ,
Ni et al [10] đã phát triển cơ chế token tin nhắn để truyền
tin nhắn giữa các khung hình khác nhau. Pan et al [11] đã chèn
bộ adapter tích chập 3D bên trong transformer để kích hoạt
mô hình hóa thời gian. Bên cạnh mô hình hóa thời gian, cũng có
những nỗ lực khác tập trung vào việc thích ứng các mô hình
hình ảnh-ngôn ngữ cho các tác vụ video từ các góc độ khác nhau.
Ví dụ, [10], [45] đã khám phá mô hình hóa prompt, trong khi
[15], [16], [46], [47] đã cải thiện các cách tương tác xuyên phương tiện.
Tuy nhiên, hầu hết các phương pháp nêu trên có xu hướng hoạt động
kém hơn khi được chuyển sang tác vụ video khác, trong khi mô hình
của chúng tôi hoạt động tốt trên nhiều tác vụ video khác nhau.

III. PHƯƠNG PHÁP

Trong phần này, chúng tôi sẽ trình bày chi tiết về Mug-STAN
mạnh mẽ và linh hoạt được đề xuất của chúng tôi để thích ứng
các mô hình hình ảnh-ngôn ngữ cho các tác vụ video tổng quát.

A. Động lực

Các mô hình hình ảnh-ngôn ngữ quy mô lớn, như CLIP và CoCa,
trải qua tiền huấn luyện trên hàng trăm triệu đến hàng tỷ cặp
hình ảnh-văn bản, thường bao gồm hai bộ mã hóa làm thành phần
cơ bản. Mỗi bộ mã hóa chịu trách nhiệm mã hóa một phương tiện
để tạo điều kiện cho căn chỉnh xuyên phương tiện. Khi chúng ta
tiến lên qua các lớp của transformer thị giác [48], mô hình
dần học các mẫu thị giác ở các cấp độ trừu tượng khác nhau [49].
Cuối cùng, bộ mã hóa thị giác tạo ra các embedding thị giác
cấp cao được căn chỉnh về mặt ngữ nghĩa với các embedding
tương ứng trong phương tiện văn bản. Một cách chính thức, như
được minh họa trong Hình 4(trái), với một clip video có T khung hình
và một mô tả văn bản có K token, chúng tôi đưa chúng vào
một bộ mã hóa thị giác tiền huấn luyện hình ảnh-văn bản chuẩn
và bộ mã hóa văn bản, xử lý mỗi khung hình như một hình ảnh
riêng lẻ. Quá trình này tạo ra các biểu diễn video theo khung hình
được ký hiệu là V, và các biểu diễn văn bản theo token được
ký hiệu là C:

V={vi}T
i=1∈RT×D, C ={cj}K
j=1∈RK×D(1)

trong đó D là chiều đặc trưng. Lưu ý rằng vi có thể được thu thập
từ token CLS [1], [2] hoặc trung bình của tất cả các token patch [29]
của mỗi khung hình. Sau đó, các biểu diễn video theo khung hình
{vi}T
i=1 được tính trung bình làm embedding video toàn cục v
và embedding token CLS được chọn từ C làm biểu diễn văn bản
toàn cục c, trong đó v và c được sử dụng cho căn chỉnh xuyên phương tiện.
Tuy nhiên, trong quá trình trên, hai vấn đề quan trọng bị bỏ qua:
mô hình hóa thời gian và lệch một phần video-văn bản.

Thứ nhất, mỗi khung hình được mã hóa độc lập khi nó đi qua
bộ mã hóa thị giác, điều này bỏ qua các tương tác giữa các khung hình
và cản trở hiểu thời gian. Để giải quyết vấn đề này, nghiên cứu
hiện tại thường giới thiệu các mô-đun bổ sung như cấu trúc
phía sau hoặc trung gian cho bộ mã hóa thị giác để tích hợp
rõ ràng mô hình hóa thời gian cho các tác vụ video hạ nguồn
khác nhau. Đối với các tác vụ thống trị kiến thức ngữ nghĩa cấp cao,
tức là tác vụ video-ngôn ngữ, cấu trúc phía sau tận dụng đầy đủ
kiến thức căn chỉnh thị giác-ngôn ngữ tiền huấn luyện bằng cách
áp dụng mô hình hóa thời gian vào

--- TRANG 5 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 5

⊕Multi-Head Self-AttentionLayer NormLayer NormFeed ForwardNetworkLayer NormSpatial Self-AttentionCross-Frame Module
khởi tạo⊕Lớp CLIPIntra-Frame ModuleSTAN layerun trailer cho một bộ phim sắp tới ……TextVision
Mug 𝑐!!"#$𝑣%%"#&"𝑣̃𝑐
STAN𝑣!!"#$

Hình 4. (trái) Kiến trúc tổng thể của phương pháp được đề xuất của chúng tôi, bao gồm các bộ mã hóa văn bản và thị giác, mô-đun mô hình hóa thời gian (STAN), và mô-đun tương tác xuyên phương tiện (Mug). (giữa) Sơ đồ minh họa của lan truyền đặc trưng trong và giữa bộ mã hóa thị giác tiền huấn luyện và STAN. (phải) Chi tiết của cấu trúc bên trong của mô-đun không gian-thời gian STAN.

đầu ra bộ mã hóa thị giác {vi}T
i=1. Tuy nhiên, bản chất có tính ngữ nghĩa
cao của vi
T
i=1 làm cho việc nắm bắt các mẫu không gian-thời gian
mức thấp trở nên khó khăn, dẫn đến mô hình hóa thời gian
kém hiệu quả hơn. Đối với các tác vụ thống trị mẫu thị giác,
tức là tác vụ chỉ video, cấu trúc trung gian được tích hợp trong
bộ mã hóa thị giác tận dụng đầy đủ các mẫu thị giác mức thấp
tiền huấn luyện. Điều này trao quyền cho bộ mã hóa khả năng
học các mẫu không gian-thời gian từ video. Tuy nhiên, các
mô-đun cắm vào làm gián đoạn cấu trúc mô hình gốc và luồng
đặc trưng bên trong, dẫn đến việc không thể kế thừa khả năng
căn chỉnh thông tin ngữ nghĩa cấp cao từ các mô hình tiền huấn luyện.

Thứ hai, chiến lược đơn giản trong tương tác xuyên phương tiện
bỏ qua vấn đề lệch một phần phổ biến trong các cặp video-văn bản.
Sự lệch này dẫn đến việc thông tin được căn chỉnh được phân bố
một cách chọn lọc qua các khung hình và cụm từ cụ thể, trong khi
các yếu tố ngữ cảnh khác có thể thiếu mối liên quan với nhau.
Các phần không liên quan là một loại nhiễu đối với căn chỉnh
video-ngôn ngữ. Do đó, việc đơn giản biểu diễn video và văn bản
bằng biểu diễn trung bình hoặc embedding CLS sẽ giới thiệu
nhiễu cản trở việc học căn chỉnh xuyên phương tiện.

Để đáp ứng vấn đề các mô hình hiện tại không thể đồng thời
kế thừa kiến thức cấp cao và cấp thấp tiền huấn luyện, chúng tôi
giới thiệu Mạng Phụ trợ Không gian-Thời gian (STAN), một
cơ chế mô hình hóa thời gian mới cho các mô hình hình ảnh-ngôn ngữ
tiền huấn luyện. Như được thể hiện trong Hình 4(giữa), STAN
hoạt động như một cấu trúc nhánh bên cạnh bộ mã hóa thị giác
tiền huấn luyện. Với thiết kế tinh vi, STAN tận dụng các cấp độ
đặc trưng khác nhau trong khi vẫn giữ lại kiến thức tiền huấn luyện.
Hoạt động của STAN sẽ được chi tiết trong Phần III-B. Ngoài ra,
như được mô tả trong Hình 5, để giải quyết vấn đề lệch một phần,
chúng tôi giới thiệu một mô-đun tương tác xuyên phương tiện mới
gọi là căn chỉnh xuyên phương tiện Hướng dẫn lẫn nhau (Mug).
Mô-đun này lấy các biểu diễn video theo khung hình V và các
biểu diễn văn bản theo token C làm đầu vào. Với sự hướng dẫn
từ phương tiện khác, Mug lọc hiệu quả nội dung không liên quan
và bảo tồn thông tin được căn chỉnh trong mỗi phương tiện,
tạo ra biểu diễn video toàn cục mới ev và biểu diễn văn bản ec.
Chi tiết về Mug sẽ được cung cấp trong Phần III-C.

B. Mạng Phụ trợ Không gian-Thời gian

Một lần nữa, trong trường hợp của một video có T khung hình,
các khung hình được đưa vào backbone thị giác tiền huấn luyện,
tạo ra các đầu ra trung gian ở K + 1 cấp độ cuối cùng của
các lớp thị giác. Chúng tôi ký hiệu các đầu ra của lớp thị giác
thứ k được chọn là:

Vk={fk
i,l∈ RD|i∈[1, T], l∈[0, L]}, (2)

là một chuỗi embedding thị giác của video trong đó T, L
và D đại diện cho số khung hình, số patch mỗi khung hình
và chiều embedding tương ứng. Trong Vk, fk
i,0 đề cập đến
embedding của token [CLS] trong khung hình thứ i của video,
trong khi fk
i,l>0 đại diện cho embedding thị giác của patch
thứ l trong khung hình đó. Sau đó, chúng tôi lấy mỗi đầu ra
trung gian Vk và chuyển nó qua lớp tương ứng trong STAN
để mô hình hóa tương ứng không gian-thời gian giữa các khung hình
video. Cuối cùng, các đầu ra theo khung hình của lớp thị giác
tiền huấn luyện cuối cùng được kết hợp với đầu ra của STAN
để thu được biểu diễn video theo khung hình được ngữ cảnh hóa
với thông tin thời gian, được ký hiệu là {vi}T
i=1 trong Eq. 1.

STAN được cấu thành từ một chồng K lớp không gian-thời gian,
với đầu vào cho mỗi lớp được xây dựng dựa trên đầu ra của
một lớp thị giác tiền huấn luyện và lớp STAN cuối cùng. Đối với
lớp thứ k trong STAN, đầu vào của nó là một chuỗi embedding
của toàn bộ video được ký hiệu là:

V′k={f′k
0,0, f′k
1,1, .., f′k
1,L, .., f′k
T,1, .., f′k
T,L}, (3)

trong đó f′k
0,0 là embedding đại diện cho toàn bộ video trong khi
những cái khác ký hiệu embedding của các patch hình ảnh
trong các khung hình khác nhau. Đầu ra của lớp STAN cũng
là một chuỗi embedding duy trì cùng kích thước với đầu vào
của nó, được ký hiệu là:

ˆVk={ˆfk
0,0,ˆfk
1,1, ..,ˆfk
1,L, ..,ˆfk
T,1, ..,ˆfk
T,L}. (4)

Tại lớp STAN đầu tiên, để xây dựng đầu vào của nó từ đầu ra
của bất kỳ lớp thị giác tiền huấn luyện nào Vm, trước tiên chúng tôi
tính trung bình

--- TRANG 6 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 6

…
khung hình cụ thể t
softmaxsoftmaxtoken cụ thể vtrailormoviebeach...khung hình1khung hình2khung hình3…softmax
softmaxvideo được hướng dẫn bởi văn bản
văn bản được hướng dẫn bởi videoTương tác Token-Frame
…nhân element-wise nhân ma trận 
ma trận tương tựđặc trưng video theo khung hìnhđặc trưng văn bản theo tokencăn chỉnh
𝑐!!"#$𝑣!!"#$VisionSTAÑ𝑐#𝑣
……Text……

Hình 5. Tổng quan về Mug được đề xuất của chúng tôi. Dựa trên các đầu ra của bộ mã hóa video và văn bản, trước tiên chúng tôi triển khai tương tác token-frame lẫn nhau trên các đặc trưng video theo khung hình và các đặc trưng văn bản theo token. Sau đó, chúng tôi tính toán embedding video toàn cục và embedding văn bản thông qua hướng dẫn từ phương tiện khác. Cuối cùng, chúng tôi căn chỉnh embedding video được hướng dẫn bởi văn bản và embedding văn bản được hướng dẫn bởi video.

embedding của các token [CLS] trong mỗi khung hình làm một
embedding mới f′1
0,0=1
TP
i∈Tfm
i,0, và sau đó cập nhật các embedding
patch trong Vk với cả embedding vị trí không gian và thời gian như:

f′1
i,l= Dropout( fm
i,l+ Pos t(t) + Pos s(l)), (5)

trong đó l >0 và Post và Poss là các embedding có thể học được
cho các vị trí thời gian và không gian của mỗi patch. Đối với
các lớp khác trong STAN, đầu vào V′k được xây dựng dựa trên
đầu ra từ lớp STAN trước đó eVk−1 và đầu ra lớp thị giác
tiền huấn luyện Vm+k−1 như sau:

f′k
0,0=efk−1
0,0+ Wk
proj1
TX
i∈Tfm+k−1
i,0 , (6)

f′k
i,l=efk−1
i,l+ Wk
projfm+k−1
i,l, (7)

trong đó i∈[1, T], l∈[1, L], và Wk
proj∈RD×D là một
lớp chiếu. Khi so sánh với các phương pháp dựa trên cấu trúc
phía sau, STAN thực hiện mô hình hóa không gian-thời gian trên
các biểu diễn thị giác tiền huấn luyện đa cấp, cho phép nó nắm bắt
hiệu quả thông tin động lực thị giác trong video. Trong khi đó,
không giống như các phương pháp dựa trên cấu trúc trung gian
trước đây chèn các mô-đun vào bộ mã hóa thị giác tiền huấn luyện,
cấu trúc nhánh của STAN bảo vệ kiến thức tiền huấn luyện mà
không làm gián đoạn cấu trúc bộ mã hóa vốn có.

Với chuỗi embedding đầu vào của một video, lớp STAN học
thông tin không gian thời gian giữa các khung hình video. Như
được mô tả trong Hình 4(phải), nó thực hiện mô hình hóa thời gian
thông qua việc xếp chồng luân phiên của hai mô-đun độc lập
– mô-đun trong khung hình và mô-đun giữa khung hình. Nhờ
thiết kế tách biệt này, chúng tôi có thể tái sử dụng cấu trúc
của lớp bộ mã hóa thị giác tiền huấn luyện làm mô-đun không gian
trong khung hình của chúng tôi và khởi tạo nó với tham số tiền huấn luyện.
Cách tiếp cận này làm giảm đáng kể không gian tìm kiếm tối ưu hóa
và cải thiện hiệu suất của các tác vụ hạ nguồn. Giống như hầu hết
các mô hình hình ảnh-văn bản tiền huấn luyện như CLIP, mô-đun
trong khung hình cũng là một khối self-attention được thiết kế
cho mô hình hóa không gian. Để đơn giản hóa ký hiệu, chúng tôi
bỏ qua chỉ số trên của embedding và ký hiệu biểu diễn embedding
của khung hình thứ i là Xi∈R(L+1)×D. Ở đây, embedding của
token [CLS] trong video được sao chép và nối với các embedding
patch. Trong mỗi khung hình, mô-đun không gian cập nhật
các embedding bằng cách sử dụng self-attention:

ˆXi= softmax( XiWQ(XiWK)T/√
D)(XiWV) +Xi,(8)

trong đó WQ/WK/WV ký hiệu các chiếu tuyến tính cho query,
key và value trong lớp self-attention của mô-đun không gian.
Sau đó, các embedding [CLS] được sao chép trong mỗi khung hình
được tính trung bình để tạo thành embedding [CLS] của video.

Mô-đun cross-frame được dành cho mô hình hóa thời gian.
Để đơn giản hóa ký hiệu, chúng tôi bỏ qua chỉ số trên của
embedding và đại diện cho tập hợp các embedding patch thứ l
trong các khung hình khác nhau là Yl∈RT×D. Tại mỗi vị trí
không gian, các embedding patch được cập nhật bằng cách sử dụng
hàm Temp (), ký hiệu chiến lược truyền thông điệp qua các
chiều thời gian. Trong các thí nghiệm, chúng tôi sẽ cho thấy
rằng chiến lược này có thể được khởi tạo theo nhiều cách khác nhau
để tạo điều kiện cho việc trao đổi thông tin thời gian giữa các khung hình.
Ở đây, chúng tôi chi tiết việc khởi tạo temporal self-attention,
có lợi thế tự nhiên trong mô hình hóa chuỗi. Tại mỗi vị trí
không gian cụ thể, các embedding patch từ các khung hình khác nhau
có thể được cập nhật như:

ˆYl= W proj(softmax( YlWQ(YlWK)T/√
D)(YlWV) +Yl),
(9)

trong đó WQ/WK/WV ký hiệu các chiếu tuyến tính cho query,
key, và value trong lớp self-attention của mô-đun cross-frame,
và Wproj là chiếu tuyến tính thời gian bổ sung được khởi tạo
là zero. Bằng cách sử dụng attention thời gian, mỗi patch trong
video được ngữ cảnh hóa với thông tin thời gian từ cùng vị trí,
trong khi chiếu zero giúp duy trì tính ổn định huấn luyện trong
các giai đoạn đầu.

Ở giai đoạn cuối cùng, với đầu ra của lớp thị giác tiền huấn luyện
cuối cùng V−1 và đầu ra của lớp STAN cuối cùng ˆVK, chúng tôi
có thể đơn giản kết hợp chúng thông qua phép cộng để tạo thành
đầu ra cuối cùng của bộ mã hóa video:

V= W vproj(LN(V−1⊕ˆVK)), (10)

trong đó LN là layer normalization cuối cùng trong bộ mã hóa
thị giác tiền huấn luyện và Wvproj là trọng số tuyến tính chiếu
embedding thị giác vào không gian đặc trưng thị giác-văn bản
chung. Hơn nữa, ⊕ có nghĩa là token [CLS] toàn cục của STAN
được sao chép T

--- TRANG 7 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 7

lần và được cộng vào [CLS] của mỗi khung hình trong V−1, trong khi
các token patch được kết hợp thông qua phép cộng đơn giản. Cuối cùng,
giống như bộ mã hóa hình ảnh, chúng tôi chỉ có L + 1 token cho
việc mã hóa video. Đặc tính này làm giảm đáng kể gánh nặng tính toán
nếu chúng tôi cần đưa các token này vào các bộ mã hóa đa phương tiện
hoặc LLM, so với bộ mã hóa video không gian-thời gian chung [5], [6], [13].

C. Căn chỉnh Xuyên phương tiện Hướng dẫn Lẫn nhau

Trong phần trước, chúng tôi đã có được các embedding văn bản
theo token C và các embedding video theo khung hình V. Trong
phần này, chúng tôi sẽ đi sâu hơn vào cách lọc thông tin không
căn chỉnh bằng cách sử dụng Mug, như được mô tả trong Hình 5.

Mug trước tiên thiết lập tương ứng theo token-frame bằng cách
tính toán độ tương tự tích vô hướng giữa C và V. Với ma trận
tương tự, sau đó chúng tôi giới thiệu cách cung cấp hướng dẫn
lẫn nhau cho việc tập hợp đặc trưng từ góc độ của mỗi phương tiện
tương ứng.

Từ góc độ của phương tiện video, trước tiên chúng tôi lọc thông tin
liên quan nhất trong văn bản cho mỗi khung hình video. Điều này
được thực hiện bằng cách tính toán phân bố attention từ khung hình
đến token, gán điểm số cho mỗi token văn bản dựa trên mức độ
liên quan của nó với khung hình video hiện tại. Cụ thể, điểm attention
của khung hình video thứ i đối với token văn bản thứ j được cho bởi:

si,j=exp(τcj·vi)PK
j=1exp(τcj·vi), (11)

trong đó PK
j=1si,j= 1, · đại diện cho phép toán tích vô hướng và
τ kiểm soát độ nhọn của phân bố attention. Ví dụ, trong Hình 2(a),
khung hình video thứ 1 dự kiến sẽ có attention chủ đạo trên
các token tương ứng với hành động "mọi người trên bãi biển"
qua tất cả các token văn bản.

Sau đó, chúng tôi tập hợp các embedding văn bản dựa trên
phân bố attention và nhận được embedding văn bản cụ thể cho
khung hình cho mỗi khung hình:

ci=KX
j=1si,jcj,trong đó ci∈RD. (12)

Tập hợp các embedding văn bản cụ thể cho khung hình {ci}T
i=1
đại diện cho các embedding văn bản được cập nhật được chỉ định
cho mỗi khung hình, trong đó thông tin không liên quan trong
văn bản gốc không được căn chỉnh với khung hình bị ngăn chặn
và thông tin liên quan đến khung hình được tăng cường. Chúng tôi
sử dụng các embedding văn bản được cập nhật này để đánh giá
mức độ tương ứng của mỗi khung hình đối với văn bản. Việc
đánh giá này được thực hiện bằng cách sử dụng điểm số độ tương tự
tích vô hướng làm thước đo:

esi=exp(τci·vi)PT
n=1exp(τcn·vn), (13)

trong đó esi đại diện cho trọng số attention của mỗi khung hình
đối với văn bản. Thông qua esi, chúng tôi có thể tiếp tục tập hợp
embedding theo khung hình thành biểu diễn toàn cục cấp video
với sự hướng dẫn của văn bản. Một cách chính thức, chúng tôi
định nghĩa embedding video được hướng dẫn bởi văn bản toàn cục
này là:

ev=TX
i=1esivi,trong đó ev∈RD. (14)

Tương tự, từ góc độ của phương tiện văn bản, chúng tôi tuân theo
cùng một quy trình để nhận được embedding văn bản được cải thiện
dưới sự hướng dẫn của video. Cụ thể, trước tiên chúng tôi tính toán
phân bố attention từ token đến khung hình, gán điểm số cho mỗi
embedding khung hình dựa trên mức độ liên quan của nó với token
văn bản hiện tại:

s′
i,j=exp(τcj·vi)PT
i=1exp(τcj·vi), (15)

trong đó i và j chỉ ra chỉ số của token văn bản và khung hình. Sau đó,
chúng tôi nhận được embedding video cụ thể cho token cho mỗi token
văn bản để đánh giá mức độ tương ứng từ token đến video:

vj=TX
i=1s′
i,jvi,trong đó vi∈RD(16)

es′
j=exp(τcj·vj)PK
n=1exp(τcn·vn), (17)

trong đó es′
j đại diện cho trọng số attention của mỗi token văn bản
đối với video. Chúng tôi thu được embedding văn bản được hướng dẫn
bởi video toàn cục ec bằng cách tập hợp các embedding token văn bản
theo {es′
j}K
j=1:

ec=KX
j=1es′
jcj,trong đó ec∈RD, (18)

Trong Mug được đề xuất của chúng tôi, chúng tôi mặc định sử dụng
tương tác từ khung hình đến token. Tuy nhiên, phương pháp của chúng tôi
có thể dễ dàng thích ứng với nhiều mức độ chi tiết khác nhau của
tương tác video-văn bản, như tương tác video-đến-token, khung hình-đến-văn bản,
và token-đến-token. Sự linh hoạt này cho phép đánh đổi giữa tính toán
và mức độ chi tiết tương tác, phục vụ các yêu cầu khác nhau dựa trên
ứng dụng cụ thể. Chúng tôi sẽ khám phá thêm và thảo luận về điều này
trong các thí nghiệm của chúng tôi.

D. Huấn luyện

Hậu tiền huấn luyện & truy xuất văn bản-video. Cả hậu tiền huấn luyện
và các tác vụ truy xuất đều sử dụng các cặp video-văn bản làm nguồn
huấn luyện, dẫn đến cùng một pipeline huấn luyện. Cụ thể, với
embedding video được hướng dẫn bởi văn bản ev và embedding văn bản
được hướng dẫn bởi video ec, chúng tôi tính toán độ tương tự tích vô hướng
giữa hai embedding, phục vụ như thước đo tương tự cho video và văn bản
trong học tương phản trong một B-batch bằng:

Lt2v=−1
BBX
m=1logexp(τgcmn·gvnm)PB
n=1exp(τgcmn·gvnm),

Lv2t=−1
BBX
n=1logexp(τgvnm·gcmn)PB
m=1exp(τgvnm·gcmn),

Lco=Lt2v+Lv2t,(19)

trong đó gcmn và gvnm ký hiệu embedding văn bản/video được hướng dẫn
lẫn nhau của văn bản thứ m và video thứ n trong batch, và Lco ký hiệu
hàm mất mát tương phản cuối cùng. Đáng chú ý là các embedding video
và văn bản đã được chuẩn hóa trước khi tính toán Mug, do đó việc
chuẩn hóa không được bao gồm trong việc tính toán độ tương tự.

--- TRANG 8 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 8

BẢNG I
KẾT QUẢ ZERO-SHOT CỦA TRUY XUẤT VĂN BẢN-VIDEO VÀ NHẬN DẠNG VIDEO TRÊN SÁU TẬP DỮ LIỆU HẠ NGUỒN. CÁC MÔ HÌNH THỂ HIỆN SO SÁNH KHÔNG CÔNG BẰNG RÕ RÀNG ĐƯỢC GIẢM NHẸ, tức là BẢO GỒM PHƯƠNG TIỆN BỔ SUNG, CÁC MÔ HÌNH LỚN HƠN NHIỀU, HOẶC TIỀN HUẤN LUYỆN TỰ GIÁM SÁT.

PhươngPhápMSR-VTT DiDeMo LSMDC HMDB-51 UCF-101 Kinetics400
R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR Acc@1 Acc@1 Acc@1

Các mô hình không-CLIP
VideoCLIP [53] 10.4 22.2 30.0 - 16.6 46.9 - - - - - - - - -
Frozen [21] 18.7 39.5 51.6 10.0 21.1 46.0 56.2 7.0 9.3 22.0 30.1 51.0 27.5 45.4 -
ALPRO [54] 24.1 44.7 55.4 - 23.8 47.3 57.9 - - - - - - - -
VIOLET [34] 25.9 49.5 59.7 - 23.5 49.8 59.8 - - - - - - - -
BridgeFormer [36] 26.0 46.4 56.4 7.0 25.6 50.6 61.1 5.0 12.2 25.9 32.2 42.0 38.0 51.1 -
Clover [5] 26.4 49.5 60.0 6.0 29.5 55.2 66.3 4.0 17.4 29.2 38.2 24.0 - - -
OmniVL [55] 34.6 58.4 66.6 - 33.4 58.7 68.5 - - - - - - - -

CLIP-B/32
CLIP [1] 30.6 54.4 64.3 4.0 24.7 49.3 60.9 6.0 13.6 27.9 35.5 32.0 - - 42.1
CLIP-straight [56] 31.2 53.7 64.2 4.0 - - - - 11.3 22.7 29.2 56.5 - - -
CLIP4Clip [7] 32.0 57.0 66.9 4.0 - - - - 15.1 28.5 36.4 28.0 - - -
BridgeFormer [36] 33.2 58.0 68.6 4.0 - - - - 15.5 30.7 38.7 22.0 - - -
CLIP-ViP [6] 29.0 51.2 61.3 5.0 22.6 43.9 56.4 7.0 11.3 25.3 31.3 38.0 - - -
Mug-STAN-B/32 35.9 60.8 69.6 3.0 33.7 60.5 70.3 3.0 17.4 32.7 40.4 21.5 - - 48.1

CLIP-B/16
CLIP [1] 31.8 53.9 64.5 4.0 27.7 51.0 62.5 5.0 15.2 29.7 37.6 25.0 43.2 68.9 48.0
ActionCLIP [57] - - - - - - - - - - - - 40.8 58.3 -
CLIP-ViP [6] 31.7 53.8 63.2 4.0 24.6 50.7 59.7 5.0 12.5 26.1 33.3 39.0 41.2 48.9 37.6
X-CLIP [10] 31.7 53.8 63.2 4.0 24.6 50.7 59.7 5.0 12.5 26.1 33.3 39.0 44.6 72.0 -
Mug-STAN-B/16 38.7 64.0 74.0 2.0 36.2 62.3 71.1 3.0 18.0 33.3 41.4 19.0 50.9 70.3 55.7

CLIP-L/14
CLIP [1] 35.4 58.8 68.1 3.0 30.3 54.9 65.4 4 18.5 33.8 42.3 19.0 46.5 72.7 55.9
ImageBind∗[58] 36.8 61.8 70.0 - - - - - - - - - - - 50.0
InternVideo [59] 40.0 65.3 74.1 2.0 31.5 57.6 68.2 3.0 17.6 32.4 40.2 23.0 - - 64.2
Mug-STAN-L/14 41.7 65.7 75.8 2.0 39.6 64.3 72.6 2.0 20.7 38.8 46.2 14.0 52.1 76.9 65.0

Nhận dạng hành động video. Khác với các tác vụ video-ngôn ngữ,
các tác vụ nhận dạng hành động có nhãn văn bản cố định. Do đó,
chúng tôi đóng băng bộ mã hóa văn bản và chỉ huấn luyện bộ mã hóa
video trong quá trình tinh chỉnh. Bên cạnh đó, chúng tôi không sử dụng
bất kỳ mẫu prompt bổ sung nào như "một video về hành động { }" [1], [10]
để bao bọc các thẻ. Sau đó, chúng tôi tính toán hàm mất mát với ev và ec như sau:

Lcr=NX
n=1ynlogexp(τev·ecn)PN
i=1exp(τev·eci), (20)

trong đó N là số lượng lớp, yn là nhãn one-hot cho lớp n, cn là
giá trị của lớp n trong embedding văn bản toàn cục, và Lcr ký hiệu
hàm mất mát cross-entropy cuối cùng.

Phát hiện hành động video. Theo pipeline phát hiện hành động
trong Slowfast [50] và VideoMAE [51], chúng tôi thêm ROIAlign [52]
với MaxPooling để tạo ra các vùng quan tâm trong lớp cuối cùng,
theo sau bởi hàm mất mát cross-entropy với sigmoid cho dự đoán
đa nhãn.

IV. THÍ NGHIỆM

A. Tập dữ liệu

Chúng tôi đánh giá Mug-STAN của chúng tôi trên cả các tác vụ
video-ngôn ngữ, tức là truy xuất video-văn bản, và các tác vụ chỉ video,
tức là nhận dạng video và phát hiện video, điều này kiểm tra các phương pháp
của chúng tôi từ hai góc độ khác nhau. Đối với truy xuất video-văn bản,
chúng tôi sử dụng MSR-VTT [18], DiDemo [20] và LSMDC [60]; đối với
nhận dạng video, chúng tôi sử dụng Kinetics-400 [19] và Something-
Something-v2 [61]; đối với phát hiện video, chúng tôi áp dụng Atomic
Visual Action V2.2 [62]. Bên cạnh đó, chúng tôi tiến hành hậu tiền huấn luyện
video-văn bản trên các tập dữ liệu với các mức độ nhiễu khác nhau,
bao gồm WebVid10M [21] và HowTo100M [4].

Tập dữ liệu Video-Ngôn ngữ: MSR-VTT là benchmark được sử dụng
rộng rãi nhất cho truy xuất video-văn bản. Nó bao gồm 10.000 video
YouTube, mỗi video được liên kết với 20 chú thích. Chúng tôi báo cáo
kết quả của chúng tôi trên phân tách 1K-A [63], chứa 9000 video
cho huấn luyện và 1000 cho kiểm tra. DiDemo bao gồm 10.611 video
có nguồn gốc từ Flicker, đi kèm với 40.000 câu. Đáng chú ý, tập dữ liệu
này có thời lượng video dài hơn so với các tập dữ liệu truy xuất khác.
Theo các công trình trước đây [7], [14], chúng tôi nối tất cả các chú thích
của một video thành một truy vấn duy nhất. LSMDC là một benchmark
truy xuất video-văn bản quy mô lớn bao gồm 118.081 video có nguồn gốc
từ 202 bộ phim. Tập dữ liệu này cung cấp mức độ đa dạng cao hơn
về mặt khái niệm và thời lượng video so với các tập dữ liệu khác.

Tập dữ liệu Chỉ Video: Kinetics-400 (K-400) là benchmark nhận dạng
video phổ biến nhất. Bao gồm hơn 300.000 clip video, Kinetics-400
bao phủ 400 lớp hành động con người với trung bình 300 khung hình.
Something-Something-v2 (SSv2) là một benchmark nhận dạng hành động
video được thiết kế đặc biệt cho khả năng mô hình hóa thời gian.
Nó bao gồm 220.485 video, mỗi video được liên kết với 174 lớp hành động.
Trái lại, K-400 có xu hướng thiên về các danh mục hành động với
bối cảnh cảnh tĩnh, như được ghi chú trong [64]. Tuy nhiên, trong SSv2,
các lớp hành động ít bị ảnh hưởng bởi bối cảnh cảnh tĩnh và thay vào đó
tập trung nhiều hơn vào thông tin động trong video. Atomic Visual Action
(AVA) v2.2 được thiết kế cho phát hiện hành động không gian-thời gian.
Nó cung cấp chú thích dày đặc cho 80 hành động thị giác nguyên tử
qua 430 clip phim 15 phút, dẫn đến 1.62M nhãn hành động với nhiều
nhãn mỗi người xuất hiện thường xuyên.

Tập dữ liệu Tiền huấn luyện Video: WebVid10M là một tập dữ liệu
tiền huấn luyện video-văn bản quy mô lớn của các video ngắn với văn bản

--- TRANG 9 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 9

BẢNG II
KẾT QUẢ TINH CHỈNH CỦA TRUY XUẤT VĂN BẢN-VIDEO TRÊN MSRVTT, DIDEMO, VÀ LSMDC. CÁC MÔ HÌNH THỂ HIỆN SO SÁNH KHÔNG CÔNG BẰNG RÕ RÀNG ĐƯỢC GIẢM NHẸ. ĐỐI VỚI CÁC PHƯƠNG PHÁP DỰA TRÊN CLIP, * CÓ NGHĨA LÀ CÁC THỦ THUẬT BỔ SUNG (ví dụ, DSL [67] VÀ QB-NORM [68]) ĐƯỢC SỬ DỤNG TRONG QUÁ TRÌNH SUY LUẬN; VÀ † KÝ HIỆU HẬU TIỀN HUẤN LUYỆN CÁC MÔ HÌNH TRÊN CÁC TẬP DỮ LIỆU VIDEO-VĂN BẢN TRƯỚC KHI TINH CHỈNH.

PhươngPhápMSR-VTT DiDeMo LSMDC
R@1↑R@5↑R@10 ↑MdR↓R@1↑R@5↑R@10 ↑MdR↓R@1↑R@5↑R@10 ↑MdR↓

Các mô hình không-CLIP
CLIPBert [35] 22.0 46.8 59.9 6.0 20.4 48.0 60.8 6.0 - - - -
MMT [69] 26.6 57.1 69.6 4.0 - - - - 12.9 29.9 40.1 19.3
Frozen [21] 31.0 59.5 70.5 3.0 31.0 59.8 72.4 3.0 15.0 30.8 40.3 20.0
VIOLET [34] 34.5 63.0 73.7 - 32.6 62.8 74.7 - 16.1 36.6 41.2 -
HD-VILA [70] 35.6 65.3 78.0 3.0 28.8 57.4 69.1 4.0 17.4 34.1 44.1 15.0
All-in-one [71] 37.9 68.1 77.1 - 32.7 61.4 73.5 3.0 - - - -
BridgeFormer [36] 37.6 64.8 75.1 3.0 37.0 62.2 73.9 3.0 17.9 35.4 44.5 15.0
Clover [5] 40.5 68.8 79.4 2.0 50.1 76.7 85.6 1.0 24.8 44.0 54.5 8.0

CLIP-B/32
CLIP4Clip [7] 44.5 71.4 81.6 2.0 43.4 70.2 80.6 2.0 21.6 41.8 49.8 11.0
CenterCLIP [72] 44.2 71.6 82.1 2.0 - - - - 21.7 39.8 49.8 11.0
CAMoE* [67] 47.3 74.2 84.5 3.0 43.8 71.4 - - 25.9 46.1 53.7 -
CLIP2tv [43] 46.1 72.5 82.9 2.0 45.5 69.7 80.6 2.0 15.5 30.7 38.7 22.0
ts2net [15] 47.0 74.5 83.8 2.0 41.8 71.6 82.0 - 23.4 42.3 50.9 9.0
DRL [16] 47.4 74.6 83.8 2.0 47.9 73.8 82.7 2.0 24.9 45.7 55.3 7.0
CLIP-ViP† [6] 50.1 74.8 84.6 1.0 48.6 77.1 84.5 2.0 25.6 45.3 54.4 8.0
CLIP-ViP*† [6] 55.9 77.0 86.8 1.0 53.8 79.6 86.5 1.0 26.0 46.4 54.9 8.0
Mug-STAN-B/32 48.9 74.5 84.1 2.0 49.6 75.3 84.6 2.0 25.0 44.7 54.0 8.0
Mug-STAN-B/32† 50.9 74.6 84.1 1.0 52.4 78.1 85.8 1.0 25.8 45.9 54.6 8.0
Mug-STAN-B/32*† 55.9 77.3 88.0 1.0 57.2 79.9 87.3 1.0 26.9 46.0 55.4 7.0

CLIP-B/16
CenterCLIP [72] 48.4 73.8 82.0 2.0 - - - - 24.2 46.2 55.9 8.0
DRL* [16] 53.3 80.3 87.6 1.0 49.0 76.5 84.5 2.0 26.5 47.6 56.8 7.0
CLIP-ViP† [6] 54.2 77.2 84.8 1.0 50.5 78.4 84.5 1.0 29.4 50.6 59.0 5.0
CLIP-ViP*† [6] 57.7 80.5 88.2 1.0 55.3 82.0 89.3 1.0 30.7 51.4 60.6 5.0
Mug-STAN-B/16 51.9 77.8 85.3 1.0 53.1 78.9 86.8 1.0 28.0 49.7 59.4 6.0
Mug-STAN-B/16† 53.9 77.4 85.8 1.0 56.6 79.7 87.1 1.0 29.2 50.5 59.6 5.0
Mug-STAN-B/16*† 57.3 81.6 88.4 1.0 61.2 84.1 88.9 1.0 30.8 51.9 61.2 5.0

CLIP-L/14
InternVideo† [59] 55.2 - - - 57.9 - - - 34.0 - - -
Mug-STAN-L/14† 56.6 80.0 87.8 1.0 60.4 85.0 90.5 1.0 35.3 58.0 65.2 3.0
Mug-STAN-L/14*† 61.3 82.6 90.1 1.0 65.0 87.5 92.0 1.0 37.2 59.2 67.0 3.0

mô tả có nguồn gốc từ các trang web video stock. Với 10.7M cặp
video-chú thích và 52K tổng giờ video, các video đa dạng và
phong phú về nội dung, đã chứng minh kết quả tuyệt vời trong
cả các tác vụ video-ngôn ngữ hạ nguồn [65] và các tác vụ tạo
video [66]. HowTo100M là một tập dữ liệu quy mô lớn của các
video có lời tường thuật từ video Youtube. Nó có tổng cộng 136M
clip video với chú thích và 23k hoạt động. Không giống như
Webvid, hầu hết các chú thích trong HowTo100M được tạo ra
từ nhận dạng giọng nói tự động (ASR) hoặc phụ đề. Do đó, điều này
dẫn đến sự lệch nghiêm trọng hơn giữa video và văn bản.

B. Cài đặt Thí nghiệm

Cài đặt Mô hình. Trong hầu hết các thí nghiệm, chúng tôi áp dụng
CLIP làm mô hình hình ảnh-ngôn ngữ tiền huấn luyện cơ sở để
so sánh công bằng với các công trình trước đây. Đối với STAN,
số lượng lớp STAN được đặt là 4 cho tất cả các tập dữ liệu ngoại trừ
trên SSv2 khi nó được đặt là 6. Các lớp STAN và lớp CLIP có
mối tương ứng một-một từ trên xuống dưới. Đối với Mug, chúng tôi
sử dụng tương tác từ khung hình đến token theo mặc định. Scalar
nhiệt độ τ trong Mug được đặt thành cùng giá trị không thể học được
như logit scale trong CLIP vì Mug không thay đổi quy mô của
các đặc trưng CLIP trong quá trình biến đổi đặc trưng. Để đánh giá
thêm khả năng khái quát của Mug-STAN, chúng tôi cũng triển khai
Mug-STAN trên CoCa sử dụng cùng cấu hình như CLIP.

Hậu tiền huấn luyện. Trên cả hai tập dữ liệu, chúng tôi sử dụng
chiến lược lấy mẫu thưa [35] để lấy mẫu 12 khung hình với mỗi
khung hình được thay đổi kích thước thành 224*224 cho mỗi clip video,
và đối với văn bản, độ dài token được đặt thành 64. Chúng tôi sử dụng
tối ưu hóa AdamW [73] với weight decay 0.001, và đặt tốc độ học
ban đầu là 4e-6 và 4e-5 cho các lớp CLIP và lớp STAN với
lịch trình suy giảm cosine annealing. Chúng tôi huấn luyện mô hình
của chúng tôi chỉ sử dụng hàm mất mát tương phản được chuẩn hóa
và không bao gồm các mục tiêu khác như mô hình hóa ngôn ngữ có
mặt nạ hoặc khớp video-văn bản. Chúng tôi huấn luyện các mô hình
với kích thước batch là 1024 trong 3 epoch. Phải mất 1.6k giờ GPU
với 32 A100 GPU cho hậu tiền huấn luyện trên HowTo100M, trong khi
tiêu thụ là 0.8k giờ GPU trên WebVid10M. Để đánh giá hiệu quả
của hậu tiền huấn luyện, chúng tôi so sánh hiệu suất của các mô hình
hậu tiền huấn luyện thông qua cả cài đặt zero-shot và tinh chỉnh
trên các tác vụ hạ nguồn.

Tinh chỉnh. Đối với tất cả các tập dữ liệu, kích thước batch được
đặt thành 128, và chúng tôi áp dụng AdamW làm tối ưu hóa với
weight decay

--- TRANG 10 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 10

BẢNG III
KẾT QUẢ TINH CHỈNH CỦA NHẬN DẠNG VIDEO TRÊN KINETICS-400 VÀ SOMETHING-SOMETHING-2. CHÚNG TÔI TRÌNH BÀY CÁC PHƯƠNG PHÁP CÓ QUY MÔ TƯƠNG ĐƯƠNG ĐỂ SO SÁNH CÔNG BẰNG. CHÚNG TÔI BÁO CÁO FLOPS CỦA TẤT CẢ CÁC GÓCS NHÌN.

Phương pháp Khung hình Góc nhìn Kiểm tra GFLOPs K400 Acc@1 K400 Acc@5 SSv2 Acc@1 SSv2 Acc@5

Các mô hình không-CLIP
TimeSformer-L [13] 96 1×3 7140 80.7 94.7 62.4 -
Video-Swin-B [38] 32 10×5 14729 82.7 95.5 69.6 92.7
MViT [74] 32 3×1 1362 82.9 95.7 67.7 90.9
ViViT-L [37] 32 4×3 11940 83.5 94.3 65.9 89.9
MTV-B [75] 32 4×3 11160 82.4 95.2 68.5 90.4

CLIP-B/16
CLIP-B/16 [1] 8 4×3 - 81.1 94.8 44.0 76.8
Action-CLIP-B/16 [57] 32 10×3 16890 83.8 96.2 - -
A6 [45] 16 − - 76.9 93.5 - -
STadapter-CLIP-B/16 [11] 8 1×3 455 82.0 95.7 67.1 91.2
STadapter-CLIP-B/16 [11] 32 1×3 1821 82.7 96.2 69.5 92.6
X-CLIP-B/16 [10] 8 4×3 1740 83.8 96.7 63.1 89.0
X-CLIP-B/16 [10] 16 4×3 3444 84.7 96.8 - -
Mug-STAN-B/16 8 1×3 593 84.7 96.7 67.7 91.5
Mug-STAN-B/16 16 1×3 1187 85.1 96.9 69.5 92.8

BẢNG IV
KẾT QUẢ TINH CHỈNH CỦA PHÁT HIỆN VIDEO TRÊN AVA 2.2. CÁC MÔ HÌNH SỬ DỤNG TÁI TẠIO TỰ GIÁM SÁT ĐƯỢC GIẢM NHẸ. * CÓ NGHĨA LÀ TRIỂN KHAI CỦA CHÚNG TÔI.

Phương pháp Tiền huấn luyện Khung hình GFLOPs mAP
SlowFast [50] K400 32 138 23.8
MViTv1-B [74] K400 64 455 27.3
MViTv2-B [76] K400 32 255 28.1
MVD-B [77] K400 8 180 29.8
VideoMAE-B [51] K400 8 180 31.8
CLIP-B/16* [1] K400 8 180 24.9
XCLIP-B/16* [10] K400 8 185 27.6
Mug-STAN-B/16 K400 8 197 29.3

là 0.02. Đối với truy xuất video-văn bản, chúng tôi áp dụng số khung hình
là 12 và độ dài token là 32 cho MSRVTT, LSMDC. Trên Didemo
nơi các video có thời lượng dài hơn, số khung hình và số token
được đặt thành 64 và 64. Tốc độ học được khởi tạo thành 2e-6 và 2e-5
cho các tham số trong CLIP và STAN tương ứng. Đối với các tác vụ
chỉ video, chúng tôi lấy mẫu 8 khung hình theo mặc định. Tốc độ học
được khởi tạo thành 8e-6 và 8e-5 cho các lớp CLIP và STAN. Đối với
phát hiện hành động, chúng tôi tiếp tục tiền huấn luyện Mug-STAN trên
K400 theo công việc trước đây, và áp dụng khoảng cách khung hình là 300,
phù hợp với số khung hình mặc định của video Kinetics.

C. So sánh với Các Phương pháp Tiên tiến

Kết quả Zero-Shot. Kết quả zero-shot của hậu tiền huấn luyện
WebVid10M được đăng trong Bảng I. Chúng tôi đánh giá Mug-STAN
trên ba tập dữ liệu truy xuất văn bản-video và ba tập dữ liệu nhận dạng
video. Chúng tôi báo cáo kết quả của chúng tôi dưới các năng lực
mô hình khác nhau, bao gồm trên CLIP-B/32, CLIP-B/16, và CLIP-L/14.
Như rõ ràng từ bài trình bày, nhiều cách tiếp cận giới thiệu cấu trúc
mới vào CLIP có xu hướng làm tổn hại khả năng zero-shot của nó,
bất chấp việc đạt được kết quả tinh chỉnh được cải thiện, như
ActionCLIP, CLIP-ViP, và XCLIP. Trái lại, Mug-STAN thể hiện
những lợi thế zero-shot rõ ràng so với CLIP sau hậu tiền huấn luyện.
Lưu ý rằng so sánh của chúng tôi với CLIP được tiến hành một cách
công bằng, xem xét cải thiện nhỏ đạt được thông qua hậu tiền huấn luyện
CLIP được chi tiết trong Bảng I. Hơn nữa, so với các phương pháp
SOTA trước đây trong cài đặt zero-shot, cách tiếp cận của chúng tôi
thể hiện những lợi thế đáng kể trên tất cả các tập dữ liệu, ngay cả
khi các so sánh được tiến hành một cách không công bằng đối với chúng tôi.
Ví dụ, InternVideo [59] sử dụng bộ mã hóa thị giác kép, và các kỹ thuật
tự giám sát sinh tạo, và liên quan đến gấp 50 lần ngày GPU so với
cách tiếp cận của chúng tôi. Tuy nhiên, phương pháp của chúng tôi vượt
trội hơn InternVideo với lề đáng kể, đạt được cải thiện 1.7%, 8.1%,
3.1%, và 0.8% trên các tập dữ liệu MSRVTT, DiDeMo, LSMDC, và
Kinetics400 tương ứng. Các kết quả chứng minh hậu tiền huấn luyện
của chúng tôi trên Mug-STAN không làm tổn hại kiến thức phong phú
trong CLIP trong khi cung cấp khả năng zero-shot mạnh hơn cho
các tác vụ video.

Các Tác vụ Video-Ngôn ngữ. Chúng tôi báo cáo kết quả tinh chỉnh
của truy xuất văn bản-video trong Bảng II. Chúng tôi so sánh
Mug-STAN của chúng tôi với các SOTA hiện tại với nhiều cài đặt khác nhau,
bao gồm tinh chỉnh trực tiếp, tinh chỉnh sau hậu tiền huấn luyện
và sử dụng các thủ thuật bổ sung trong quá trình suy luận. Như được
thể hiện trong kết quả, khi tinh chỉnh trực tiếp cho các tác vụ truy xuất
video-văn bản, Mug-STAN mang lại lợi thế rõ ràng so với CLIP,
vượt trội hơn CLIP4clip 4.7% tại R@1 trung bình qua ba tập dữ liệu
với CLIP-B/32 làm backbone. So với phương pháp tiên tiến khác
DRL [16], cũng tận dụng tương tác theo token khung hình để tăng
hiệu suất, Mug-STAN vượt trội hơn nó 1.1% tại R@1 trung bình
qua ba tập dữ liệu. Khi nói đến hậu tiền huấn luyện, đáng chú ý
là chỉ có một số ít phương pháp [6], [7], [59] đã khám phá lĩnh vực
này, với CLIP-ViP [6] là đối thủ mạnh nhất. So với CLIP-ViP,
giới thiệu một captioner mạnh bên ngoài [78] để tăng cường tập dữ liệu
tiền huấn luyện với các chú thích bổ sung, phương pháp của chúng tôi
được giải phóng khỏi việc tăng cường dữ liệu phức tạp như vậy và
đạt được hiệu suất cạnh tranh hoặc thậm chí tốt hơn trên các tập dữ liệu
khác nhau. Hơn nữa, Mug-STAN có thể mang lại lợi ích hiệu suất
bằng hậu tiền huấn luyện trên các tập dữ liệu nhỏ hơn hoặc nhiều nhiễu hơn,
trong khi CLIP-ViP yêu cầu tập dữ liệu lớn hơn tức là HDVilla-100M [70].
Hơn nữa, so với các đối thủ lớn [59], bất chấp những bất lợi về
chi phí huấn luyện, phương pháp tiền huấn luyện, và quy mô mô hình,
MugSTAN vẫn vượt trội hơn Inter-

--- TRANG 11 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 11

BẢNG V
KẾT QUẢ PHÂN TÍCH CỦA CÁC THÀNH PHẦN KHÁC NHAU TRONG MÔ HÌNH CỦA CHÚNG TÔI TRÊN CÁC CÀI ĐẶT KHÁC NHAU. "FT" CÓ NGHĨA LÀ KẾT QUẢ TINH CHỈNH TRỰC TIẾP MÀ KHÔNG CÓ TIỀN HUẤN LUYỆN; "ZS" CÓ NGHĨA LÀ KẾT QUẢ ZERO-SHOT SAU TIỀN HUẤN LUYỆN. CHÚNG TÔI BÁO CÁO KẾT QUẢ CỦA RECALL@1.

Thành phần Kết quả
Cấu trúc nhánh Đa cấp ST tách biệt Hướng dẫn lẫn nhau FT-MSRVTT FT-DiDemo ZS-MSRVTT ZS-DiDemo
43.1 43.4 30.6 24.7
✓ ✓ ✓ 46.9 46.2 33.0 28.1
✓ 46.1 45.4 33.1 29.8
✓ 44.9 43.5 31.9 25.4
✓ ✓ 44.2 43.6 31.8 25.4
✓ ✓ 45.5 44.7 32.2 26.9
✓ ✓ ✓ ✓ 48.9 49.6 35.9 33.7

BẢNG VI
KẾT QUẢ PHÂN TÍCH VỀ HẬU TIỀN HUẤN LUYỆN. CHÚNG TÔI BÁO CÁO KẾT QUẢ TINH CHỈNH SAU HẬU TIỀN HUẤN LUYỆN VỚI CLIP-B/32 TRÊN CẢ MSR-VTT VÀ DIDEMO. CHÚNG TÔI TIẾN HÀNH TIỀN HUẤN LUYỆN VỚI CÁC PHƯƠNG PHÁP VÀ TẬP DỮ LIỆU TIỀN HUẤN LUYỆN KHÁC NHAU.

Mô hình Tập dữ liệu Tiền huấn luyện DiDemo MSR-VTT
R@1 R@5 R@10 MdR R@1 R@5 R@10 MdR
CLIP - 43.4 70.9 79.2 2 43.1 69.8 81.8 2
CLIP HowTo100M 43.0 70.2 80.3 2 43.4 69.7 82.3 2
CLIP WebVid10M 43.6 70.4 80.5 2 43.9 70.1 80.0 2
STAN+CLIP - 46.5 71.5 80.9 2 46.9 72.8 82.8 2
STAN+CLIP HowTo100M 47.0 72.1 81.6 2 47.1 72.3 82.5 2
STAN+CLIP WebVid10M 48.2 76.7 85.0 2 47.5 72.8 82.9 2
Mug+STAN+CLIP - 49.6 75.3 84.6 2 48.9 74.5 84.1 2
Mug+STAN+CLIP HowTo100M 51.6 77.3 85.2 1 50.0 75.1 83.6 2
Mug+STAN+CLIP WebVid10M 52.4 78.1 85.8 1 50.9 74.6 84.1 1

BẢNG VII
KẾT QUẢ TINH CHỈNH CỦA MUG-STAN TRÊN COCA [2] TRÊN TRUY XUẤT MSR-VTT VÀ DIDEMO. † KÝ HIỆU TINH CHỈNH SAU HẬU TIỀN HUẤN LUYỆN.

Mô hình R@1 R@5 R@10 MdR
MSR-VTT
CoCa (Cơ sở) 42.7 70.2 79.2 2.0
STAN-CoCa 44.7 72.9 81.9 2.0
Mug-STAN-CoCa 46.2 73.4 82.3 2.0
Mug-STAN-CoCa † 48.0 73.9 82.4 2.0
DiDemo
CoCa (Cơ sở) 39.6 67.9 77.0 2.0
STAN-CoCa 43.5 72.9 82.0 2.0
Mug-STAN-CoCa 46.7 73.9 82.0 2.0
Mug-STAN-CoCa † 48.8 73.8 83.7 1.5

nVideo qua ba tập dữ liệu.

Các Tác vụ Chỉ Video. Chúng tôi báo cáo kết quả tinh chỉnh của
nhận dạng video và phát hiện video trên Kinetics-400, Something-
Something-2, và AVA-v2.2 trong Bảng III và IV tương ứng. Trong
benchmark nhận dạng K400, các phương pháp dựa trên CLIP thể hiện
hiệu suất cạnh tranh với quy mô mô hình nhỏ hơn so với các phương pháp
tiền huấn luyện hình ảnh. Ví dụ, STAN dựa trên VIT-B/16 của chúng tôi
đạt được kết quả vượt trội so với các mô hình như ViViT [37] và
Video-swin [38], có hơn 10× GFLOPs so với phương pháp của chúng tôi.
Đối với benchmark SSv2 và AVA, chúng tôi quan sát thấy rằng, không có
mô hình hóa thời gian, mô hình CLIP trần [1] chỉ đạt được 44.0% top-1
accuracy và 25.9 mAP kém hiệu suất đáng kể so với các mô hình
tiền huấn luyện ImageNet-Kinetics, mặc dù nó sở hữu kiến thức tiền huấn luyện
thu được từ tập dữ liệu hình ảnh-văn bản lớn hơn nhiều. Kết quả cho thấy
rằng khoảng cách miền là đáng kể giữa SSv2/AVA và mô hình CLIP,
và khả năng mô hình hóa thời gian được mong muốn cho hai tập dữ liệu.
STAN mang lại cải thiện hiệu suất hơn 25.5% và 4.4% so với
đường cơ sở CLIP trên SSv2 và AVA, điều này chứng minh rằng
Mug-STAN trao quyền cho CLIP với khả năng mô hình hóa thời gian mạnh mẽ.
Đáng chú ý là, so với các tác vụ video-ngôn ngữ, tiền huấn luyện
video-văn bản tương phản không thể hiện lợi thế đáng kể so với
tiền huấn luyện hình ảnh trên các tác vụ chỉ video. Điều này đặc biệt
rõ ràng đối với các phương pháp tái tạo tự giám sát. Tuy nhiên,
Mug-STAN vẫn đạt được hiệu suất cạnh tranh ngay cả khi đối mặt
với thách thức này khi so sánh với các phương pháp tiền huấn luyện
đơn phương tiện. Hơn nữa, so với các phương pháp dựa trên CLIP khác,
Mug-STAN liên tục thể hiện lợi thế trên nhiều tập dữ liệu khác nhau.

D. Nghiên cứu Phân tích

Phân tích về các thành phần của Mug-STAN. Để đánh giá đóng góp
của các thành phần khác nhau trong phương pháp của chúng tôi, chúng tôi
tiến hành thí nghiệm phân tích trên cả cài đặt tinh chỉnh và cài đặt
zero-shot như được thể hiện trong Bảng V. Trước hết, trong ba dòng
đầu tiên là hiệu suất tổng thể của STAN và Mug, chúng tôi có thể
kết luận rằng STAN và Mug tương thích với nhau trong khi mỗi
cái trong số chúng đóng góp vào việc thích ứng các mô hình tiền huấn luyện
hình ảnh-ngôn ngữ, tức là Mug giải quyết vấn đề lệch một phần
trong dữ liệu video-văn bản và STAN tập trung vào mô hình hóa thời gian.
Hơn nữa, kết hợp Mug và STAN, hiệu suất được tăng thêm một
lề đáng kể, điều này chứng minh rằng khả năng mô hình hóa thời gian
và việc giải quyết lệch một phần có lợi lẫn nhau

--- TRANG 12 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 12

434853586368
434445464748
024681012Top1 Acc trên SSv2R@1 trên MSRVTTSố lượng lớp STANMSRVTTSSv24248546066
4344454647
Cơ sở1～45～89～12Top1 Acc trên SSv2R@1 trên MSRVTTVị trí lớp STANMSRVTTSSv24248546066
4344454647
Cơ sở321Top1 Acc trên SSv2R@1 trên MSRVTTKhoảng cách lớp STANMSRVTTSSv24249566370
4344454647
0123Top1 Acc trên SSv2R@1 trên MSRVTTSố lượng mạng STANMSRVTTSSv2

Hình 6. Kết quả phân tích về cài đặt tham số siêu của STAN. Chúng tôi báo cáo kết quả tinh chỉnh không có hậu tiền huấn luyện trên cả MSRVTT và SSv2. Chúng tôi nghiên cứu số lượng lớp STAN, vị trí tương đối của lớp STAN so với CLIP, khoảng cách của lớp STAN (tức là số lượng lớp CLIP giữa lớp STAN), và số lượng mạng STAN.

BẢNG VIII
KẾT QUẢ PHÂN TÍCH VỀ MÔ-ĐUN TƯƠNG TÁC, BAO GỒM MỨC ĐỘ CHI TIẾT TƯƠNG TÁC (GIỮA) VÀ CHIẾN LƯỢC TƯƠNG TÁC (DƯỚI). CHÚNG TÔI BÁO CÁO KẾT QUẢ TRÊN CẢ MSRVTT VÀ DIDEMO.

Mô hình MSR-VTT DiDemo
R@1 trung bình R@1 trung bình
STAN (Cơ sở) 46.9 67.5 46.5 66.3
Tương tác Khung hình-Văn bản 47.5 68.2 48.7 68.3
Tương tác Video-Token 46.8 68.0 47.2 67.1
Tương tác Khung hình-Token 48.9 69.2 49.6 69.8
Mug-STAN (max) 47.3 68.5 47.9 68.2
Mug-STAN (top3) 48.6 69.1 48.5 68.7
Mug-STAN (top5) 48.1 68.2 49.4 69.3
Mug-STAN (top7) 47.0 67.8 48.4 68.3
WTI-STAN [16] 47.5 68.8 47.2 67.9
Hunyuan-STAN [17] 47.5 68.8 47.5 67.4
Mug-STAN (softmax) 48.9 69.2 49.6 69.8

với nhau. Thứ hai, các dòng 4-7 thể hiện cấu trúc bên trong của STAN.
Cụ thể, khi chúng tôi loại bỏ cấu trúc nhánh hoặc học đặc trưng đa cấp,
hiệu suất của STAN trải qua sự suy giảm đáng kể trên tất cả bốn
benchmark. Điều này phục vụ như bằng chứng mạnh mẽ về sự ưu việt
của cấu trúc mô hình của chúng tôi so với cấu trúc phía sau. Ngoài ra,
việc áp dụng mô hình hóa thời gian ST chung trong STAN cũng mang lại
những cải thiện đáng chú ý, mặc dù không vượt qua cách tiếp cận
tách biệt, điều này nhấn mạnh tầm quan trọng của việc tái sử dụng
tham số từ mô hình tiền huấn luyện.

Phân tích về Hậu tiền huấn luyện. CLIP-ViP [6] chỉ ra hai yếu tố
có thể cản trở hậu tiền huấn luyện video để cải thiện thêm hiệu suất
trên các tác vụ video hạ nguồn: quy mô tập dữ liệu và khoảng cách miền.
Trong bài báo này, thông qua nghiên cứu phân tích về hậu tiền huấn luyện,
chúng tôi phát hiện ra rằng việc trao quyền cho mô hình tiền huấn luyện
với khả năng mô hình hóa thời gian và giải quyết vấn đề lệch một phần
cũng là quan trọng cho hậu tiền huấn luyện. Chúng tôi sử dụng HowTo100M
và WebVid10M làm tập dữ liệu tiền huấn luyện và huấn luyện các mô hình
khác nhau trên hai tập dữ liệu tương ứng.

Như được thể hiện trong Bảng VI, đối với đường cơ sở CLIP, sử dụng
chiến lược pooling trung bình đơn giản cho mô hình hóa giữa các khung hình,
nó có lợi thế tầm thường từ hậu tiền huấn luyện. Đối với các thí nghiệm
trên STAN, có chuyên môn về mô hình hóa thời gian, chúng tôi quan sát
thấy rằng hậu tiền huấn luyện trên WebVid10M mang lại lợi ích hiệu suất
hơn so với đường cơ sở CLIP. Khi nói đến Mug-STAN, lợi ích hiệu suất
của hậu tiền huấn luyện trên WebVid10M tăng thêm lên 2.8% trên DiDemo
và 2.0% trên MSR-VTT. Hơn nữa, ngay cả trên HowTo100M, bao gồm

BẢNG IX
KẾT QUẢ PHÂN TÍCH VỀ HIỆU QUẢ CỦA MUG TRONG VIỆC GIẢM THIỂU LỆCH MỘT PHẦN. TRÊN, CHÚNG TÔI BÁO CÁO ĐIỂM SỐ R@1 CHO DỮ LIỆU VỚI CÁC MỨC ĐỘ LỆCH KHÁC NHAU TRONG TẬP DỮ LIỆU MSRVTT VÀ DIDEMO. DƯỚI, CHÚNG TÔI SO SÁNH MUG-STAN VỚI CÁC PHƯƠNG PHÁP LOẠI BỎ NHIỄU VIDEO TIÊN TIẾN KHÁC.

cấp độ MSR-VTT DiDemo
% STAN +Mug % STAN +Mug
trên 38 53.7 54.0 42 54.3 54.8
giữa 23 47.2 49.0 23 43.3 46.9
dưới 39 40.1 43.0 35 39.2 45.1
tất cả 100 46.9 48.9 100 46.5 49.6

Mô hình R@1 trên HTM-Align R@1 trên YouCook2
MIL-NCE [40] 31.3 15.1
TAN [42] 49.4 20.1
Mug-STAN 51.6 29.7

các video hướng dẫn với lời tường thuật nhiễu và bị lệch một phần
cực kỳ nghiêm trọng, phương pháp của chúng tôi vẫn mang lại lợi ích
hiệu suất 2.0% và 1.1% trên DiDemo và MSR-VTT tương ứng. Kết quả
tiết lộ rằng khả năng mô hình hóa thời gian có lợi cho hậu tiền huấn luyện
trong khi việc giải quyết vấn đề lệch một phần có thể khuếch đại thêm
lợi ích hiệu suất một cách đáng kể.

Mug-STAN có thể hoạt động trên các mô hình hình ảnh-ngôn ngữ
tiền huấn luyện ngoài CLIP không? Để xác minh khả năng khái quát
của phương pháp của chúng tôi, chúng tôi tiếp tục triển khai Mug-STAN
dựa trên một mô hình hình ảnh-văn bản tiền huấn luyện nổi tiếng khác,
tức là CoCa [2]. Chúng tôi chỉ sử dụng bộ mã hóa thị giác và văn bản
của CoCa và tải các trọng số tiền huấn luyện được phát hành bởi
OpenCLIP, được tiền huấn luyện trên LAION2b [79]. Như được minh họa
trong Bảng VII, so với đường cơ sở CoCa, được tinh chỉnh trực tiếp
trên các tác vụ hạ nguồn với pooling trung bình làm chiến lược
mô hình hóa thời gian, cả STAN và Mug đều mang lại cải thiện hiệu suất
đáng kể, trong khi hậu tiền huấn luyện trên WebVid10M càng thúc đẩy
kết quả tinh chỉnh. Kết quả thực nghiệm chứng minh rằng Mug-STAN
có tiềm năng được di chuyển đến các mô hình hình ảnh-văn bản tiền huấn luyện
mới nổi khác nhau.

Cài đặt tham số siêu tốt nhất của STAN là gì? STAN hoạt động
như một nhánh mới được định vị cạnh backbone thị giác tiền huấn luyện,
lấy biểu diễn khung hình video ở các cấp độ khác nhau của các lớp
thị giác tiền huấn luyện làm đầu vào. Để nghiên cứu tác động của
cài đặt khác nhau của STAN, chúng tôi trình bày nghiên cứu phân tích
rộng rãi cho STAN-CLIP-B/32 trong Hình 6 trên cả các tác vụ
video-ngôn ngữ và các tác vụ chỉ video. Đầu tiên là số lượng lớp
STAN, như được thể hiện, đối với truy xuất MSRVTT, việc tăng cường
hiệu suất của STAN đạt đỉnh ở 4 lớp, sau đó hiệu suất bắt đầu
giảm với sự tăng thêm

--- TRANG 13 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 13

Truy vấn: một saladin một bát đang được quay phim trên một cái bàn
Truy vấn: một lớp học đang được giới thiệu về một thiết bị đọc kỹ thuật sốMug-STANXCLIP
XCLIP
Truy vấn: cướp biển hoạt hình trên một con tàu
XCLIPMug-STANMug-STAN

Hình 7. Kết quả định tính của truy xuất văn bản-video trên MSR-VTT. Với một truy vấn văn bản, chúng tôi trình bày video khớp chính xác được trả về bởi Mug-STAN ở hàng đầu tiên, và hiển thị kết quả sai của XCLIP ở hàng thứ hai. Từ được làm nổi bật bằng màu đỏ chỉ ra nội dung chính bị bỏ lỡ trong kết quả sai.

Truy vấn: ai đó gấp lên một xe đẩy
Truy vấn: một nhân vật trong ai đó đi bộ với đầu gối cong qua các phòng xi măng lớn hướng về một cánh cửa khi mọi người đang bị bắn
Truy vấn: video về các vận động viên thể dục dụng cụ tập luyện lănMug-STANCLIP4clip
Mug-STANCLIP4clip
Mug-STANCLIP4clip

Hình 8. Kết quả định tính của truy xuất văn bản-video trên MSR-VTT. Với một truy vấn văn bản, chúng tôi trình bày video khớp chính xác được trả về bởi Mug-STAN ở hàng đầu tiên, và hiển thị kết quả sai của CLIP4clip ở hàng thứ hai. Từ được làm nổi bật bằng màu đỏ chỉ ra nội dung chính bị bỏ lỡ trong kết quả sai.

của các lớp; Trên SSv2, việc cải thiện hiệu suất của STAN dường như
ổn định sau 6 lớp. Tổng thể, việc sử dụng STAN với 4 đến 6 lớp
được khuyến nghị như một lựa chọn phù hợp cho các tác vụ khác nhau,
xem xét sự cân bằng tối ưu giữa lợi ích hiệu suất và hiệu quả tính toán.
Thứ hai là vị trí của lớp STAN. Chúng tôi cố định số lượng lớp STAN
thành 4 và căn chỉnh các lớp STAN với các lớp CLIP 1-4, 5-8 và 9-12
tương ứng. Kết quả cho thấy rằng cấp độ trung bình đến cao của

Hình 9. Hình ảnh hóa mô-đun intra-frame của STAN trên MSR-VTT. Với
một truy vấn văn bản. Vùng màu đỏ nhận được nhiều attention hơn từ mô hình.
Chúng tôi hình ảnh hóa attention với VideoCAM.

khung hìnhmột chiếc xe ô tô trong một vụ tai nạn
văn bảnđiểm attention: 0.090.010.090.120.300.39hướng dẫnvideo
tokenđiểm attention: 0.000.000.050.000.000.00hướng dẫn[bắt đầu văn bản][token CLS][một][xe][trong][một][vụ][tai nạn]0.310.64

Hình 10. Kết quả định tính của điểm số softmax của câu hướng dẫn khung hình
trong Eq.13 và video hướng dẫn token trong Eq. 17.

biểu diễn CLIP tiền huấn luyện có ý nghĩa hơn cho các tác vụ hạ nguồn.
Sau đó, chúng tôi căn chỉnh lớp cuối cùng của CLIP và STAN, và
thay đổi khoảng cách của các lớp CLIP được chọn giữa các lớp STAN,
ví dụ, khoảng cách=2 có nghĩa là STAN nhận đầu ra của các lớp
thứ 6, 8, 10, và 12. Như được thể hiện trong Bảng, khoảng cách=1
là lựa chọn tốt nhất cho cả hai tập dữ liệu. Cuối cùng là số lượng
của toàn bộ mạng STAN. Chúng tôi thấy rằng việc giới thiệu thêm
các lớp STAN không tạo ra sự khác biệt trên MSRVTT nhưng có thể
mang lại cải thiện nhẹ cho ssv2, nhưng điều này không hiệu quả
về chi phí xem xét sự gia tăng của độ phức tạp tính toán.

Mug có phải là thiết kế tối ưu để căn chỉnh video và văn bản không?
Để hiểu thiết kế tối ưu cho mô-đun tương tác video-văn bản, chúng tôi
thực hiện phân tích phân tích chi tiết. Ban đầu, chúng tôi khám phá
mức độ chi tiết của tương tác trong Mug. Trong Bảng VIII (giữa),
Tương tác Khung hình-Văn bản chỉ ra việc thay thế embedding văn bản
được hướng dẫn bởi video trong Mug bằng embedding token [CLS]
thông thường, trong khi Tương tác Video-Token đại diện cho việc thay thế
embedding video được hướng dẫn bởi văn bản bằng embedding theo khung hình
trung bình thông thường. Kết quả chứng minh rằng embedding video được
hướng dẫn bởi văn bản quan trọng hơn embedding văn bản được hướng dẫn
bởi video, điều này tiết lộ rằng vấn đề lệch một phần nghiêm trọng
hơn trong phương tiện video. Sau đó, chúng tôi nghiên cứu các chiến lược
tương tác xuyên phương tiện khác nhau. Một mô-đun mô hình hóa tương tác
nổi tiếng là WTI trong DRL [16] và người theo sau hunyuan [17], học
điểm attention dựa trên đơn phương tiện để xác định điểm số token-frame
nào đại diện nhất cho tương ứng văn bản-video. Trái lại, Mug sử dụng
điểm số tương ứng token-frame để giới thiệu hướng dẫn xuyên phương tiện
lẫn nhau, trong đó các phần liên quan nhất giữa cặp video-văn bản,
có thể có điểm số cao hơn, sẽ được làm nổi bật. Bảng VIII (dưới)
cho thấy rằng Mug vượt trội hơn WTI và hunyuan về mặt hiệu suất.
Bên cạnh đó, việc tập hợp embedding xuyên phương tiện được hướng dẫn
lẫn nhau trong Mug giống như một quá trình lựa chọn khái niệm key mềm.
Để khám phá ý tưởng này, chúng tôi tiếp tục thay thế các phép toán
softmax trong Eq.11,13,15,17 bằng phép toán lựa chọn cứng top-k.
Tuy nhiên, chúng tôi thấy rằng giá trị "top-k" tối ưu thay đổi
qua các tập dữ liệu, và mô-đun với softmax

--- TRANG 14 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 14

Điều gì đang xảy ra khi camera được chỉ cố định về phía những người cưỡi ngựa và con chó?
Khi camera được chỉ cố định về phía những người cưỡi ngựa và con chó, nó đang ghi lại một cảnh trong đó một nhóm người đang cưỡi ngựa qua một khu vực có cây cối, và một con chó đang đi cùng họ. Những người cưỡi ngựa có thể đang tận hưởng một chuyến đi thư giãn qua khu rừng, và con chó có thể là một người bạn đồng hành hoặc một con chó làm việc, hỗ trợ những người cưỡi ngựa trong việc điều hướng địa hình hoặc cung cấp tình bạn. Hình ảnh ghi lại một khoảnh khắc yên bình và thanh thản trong thiên nhiên, thể hiện mối liên kết giữa con người và động vật khi họ chia sẻ trải nghiệm khám phá ngoài trời cùng nhau.

Điều gì đã xảy ra khi nữ vận động viên thể dục dụng cụ đang thực hiện động tác lộn nhào?
Trong video, nữ vận động viên thể dục dụng cụ đang thực hiện động tác lộn nhào, đây là một loại động tác nhào lộn. Khi cô ấy đang thực hiện động tác lộn nhào, cô ấy đã mất thăng bằng và rơi khỏi xà thăng bằng. Cú ngã này có thể là kết quả của nhiều yếu tố khác nhau, như kỹ thuật không chính xác, đánh giá sai chỗ hạ cánh, hoặc đơn giản là mất tập trung trong một khoảnh khắc.

Cậu bé nhỏ làm gì trên thanh xà?Cậu bé nhỏ đang thực hiện một thủ thuật trên thanh xà. Cậu bé đặt cả hai tay lên thanh xà và đung đưa chân lên không trung rồi đặt chúng lên thanh xà.

Trong video, một người đàn ông được thấy cầm vợt tennis và chuẩn bị giao bóng. Anh ta đang đứng trên sân tennis, và camera ghi lại chuỗi sự kiện khi anh ta di chuyển vào vị trí, vung vợt và đánh bóng. Quả bóng sau đó chuyển động, và người đàn ông tiếp tục với cú swing của mình, thể hiện kỹ thuật và kỹ năng của anh ta trong việc giao bóng tennis. Chuỗi này ghi lại tinh chất của một cú giao bóng tennis, thể hiện form và cách thực hiện của người chơi.

Bạn có thể mô tả chuỗi sự kiện trong quá trình giao bóng trong video không?
Trong video, một người đàn ông được thấy đánh một quả bóng tennis bằng vợt tennis. Sau đó anh ta đi để đánh bóng qua lại.

Khi camera được chỉ cố định về phía những người cưỡi ngựa và con chó, những người cưỡi ngựa được thấy cưỡi ngựa xuống một con sông. Con chó cũng đang cưỡi trên một con ngựa.

Cậu bé nhỏ làm động tác đứng tay trên thanh xà, và sau đó xuống.

Trong hình ảnh, một nữ vận động viên thể dục dụng cụ đang thực hiện động tác lộn nhào trên xà thăng bằng trong một cuộc thi thể dục dụng cụ. Khi cô ấy thực hiện động tác, cô ấy dường như đang ở giữa không trung, với một chân duỗi thẳng và chân kia co lại dưới thân người. Động tác lộn nhào là một yếu tố đầy thách thức trong thể dục dụng cụ đòi hỏi sức mạnh, sự linh hoạt và sự phối hợp.

Hình 11. Kết quả định tính của trò chuyện video. Chúng tôi trình bày kết quả
từ LLaVa (trên) và STAN-LLaVa (dưới).

liên tục vượt trội hơn những cái khác trên cả hai tập dữ liệu.

Mug có hiệu quả đối với lệch video-văn bản không? Trong Bảng
IV-D, chúng tôi điều tra hiệu quả của Mug trong việc giải quyết
lệch. Trước hết, như được mô tả trong Hình 2(b), các tập dữ liệu
video-văn bản thường thể hiện các mức độ lệch khác nhau. Trong
cận trên, chúng tôi tiếp tục đăng kết quả của Mug trên dữ liệu
với các mức độ lệch khác nhau. Đáng chú ý, phần lớn các cải thiện
của Mug được quan sát trong các trường hợp lệch trung bình đến
nghiêm trọng. Sự lệch càng rõ ràng trong dữ liệu video-văn bản,
lợi ích hiệu suất của Mug càng lớn so với STAN. Hơn nữa, chúng tôi
tiến hành so sánh với các phương pháp khác để giảm thiểu lệch,
như được chỉ ra trong cận dưới của bảng. Để đảm bảo so sánh
công bằng, theo [42], chúng tôi tiền huấn luyện Mug-STAN trên
HTM-370K và đánh giá hiệu suất zero-shot của nó trên các tập dữ liệu
như được trình bày trong [42]. Kết quả rõ ràng chứng minh rằng
Mug-STAN có lợi thế đáng kể so với các phương pháp tiên tiến khác
khi hoạt động trong cùng điều kiện thực nghiệm. Tóm lại, các
phát hiện thực nghiệm của chúng tôi qua nhiều chiều khác nhau
liên tục làm nổi bật hiệu quả của Mug trong việc giải quyết lệch
trong dữ liệu video-văn bản.

E. Kết quả Định tính

Trong các thí nghiệm, chúng tôi đã củng cố khả năng mô hình hóa
thời gian thành thạo của Mug-STAN, đồng thời khai thác những
lợi ích của kiến thức tiền huấn luyện. Mở rộng những phát hiện
định lượng này, bây giờ chúng tôi trình bày kết quả định tính
tiết lộ hiệu quả của Mug-STAN qua hai khía cạnh này.

Trước hết, chúng tôi trình bày kết quả truy xuất văn bản-video
của phương pháp dựa trên cấu trúc trung gian XCLIP [10] và
Mug-STAN của chúng tôi. Được minh họa trong Hình 7, những
trường hợp này có thể được giải quyết một cách dễ dàng nếu
một mô hình có thể căn chỉnh hiệu quả các khái niệm đối tượng
được nhấn mạnh trong truy vấn, như "salad", với các video
chứa nội dung thị giác tương ứng. Tuy nhiên, XCLIP tạo ra
kết quả không chính xác bằng cách trả về kết quả trong đó
các đối tượng quan trọng bị thiếu trong video. So sánh này
nhấn mạnh hạn chế của cấu trúc trung gian trong việc chuyển giao
hiệu quả kiến thức căn chỉnh thị giác-văn bản cấp cao, công việc
mà phương pháp của chúng tôi xuất sắc. Tiếp theo, chúng tôi cung cấp
kết quả so sánh truy xuất văn bản-video cho CLIP4clip [7] và
Mug-STAN trong Hình 8. Hình này chứng minh rằng CLIP4clip,
dựa trên cấu trúc phía sau, tạo ra kết quả không chính xác. Mặc dù
kết quả bao gồm các ngữ cảnh tĩnh chính xác như được mô tả
trong truy vấn (như "xe đẩy" và "vận động viên thể dục dụng cụ"),
chúng có thông tin động sai không phù hợp với các khái niệm
được nhấn mạnh trong truy vấn (như "gấp lên" và "lăn"). Những
kết quả này nhấn mạnh rằng cách tiếp cận của chúng tôi có thể
khai thác hiệu quả hơn thông tin không gian-thời gian để tăng cường
hiểu video. Sau đó, chúng tôi hình ảnh hóa attention của mô-đun
intra-frame của Mug-STAN bằng cách sử dụng VideoCAM, như được
mô tả trong Hình 9. Những hình ảnh hóa này chứng minh rằng
mô-đun STAN của chúng tôi liên tục hướng attention của nó về
phía nội dung quan trọng trong video, trải rộng qua các khoảnh khắc
khác nhau trong thời gian. Cuối cùng, để làm rõ thêm hiệu quả
của Mug, chúng tôi trình bày kết quả định tính của hướng dẫn
xuyên phương tiện. Trong Hình 10, chúng tôi trình bày cả điểm số
tương ứng văn bản-khung hình esi (trên) và điểm số tương ứng
video-token es′
j (dưới). Kết quả cho thấy rằng đối với hướng dẫn
văn bản-khung hình, hầu hết attention tập trung vào hai khung hình
cuối cùng nơi các xe đang lăn, chứa thông tin liên quan nhất
với văn bản. Đối với hướng dẫn video-token, attention được
hướng về phía các token "xe", "tai nạn", và token kết thúc
(token CLS). Nó tiết lộ rằng Mug làm nổi bật hiệu quả các
phần được căn chỉnh trong cặp video-văn bản cho căn chỉnh
xuyên phương tiện.

F. Trò chuyện Video

Lĩnh vực xử lý ngôn ngữ tự nhiên đã trải qua một sự biến đổi
đáng kể với việc giới thiệu

--- TRANG 15 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 15

các Mô hình Ngôn ngữ Lớn (LLM) tiền huấn luyện. Những thành tựu
của LLM cũng đã thúc đẩy sự phát triển của các hệ thống AI tích hợp
các mô hình thị giác với LLM, cho phép lý luận và hành động đa phương tiện
[24], [32], [33], [80]. Thông thường, các mô hình này xây dựng một
phép chiếu từ đầu ra của bộ mã hóa thị giác tiền huấn luyện (ví dụ, CLIP)
đến đầu vào của LLM. Sau đó, họ tham gia vào việc điều chỉnh hướng dẫn
thị giác, một quá trình tạo điều kiện cho các tương tác và cuộc trò chuyện
đa phương tiện. Lấy cảm hứng từ những chatbot thị giác-ngôn ngữ này,
một làn sóng phương pháp mới đã nổi lên liên quan đến trò chuyện video,
tích hợp backbone video với LLM và thực hiện điều chỉnh hướng dẫn video
[65]. Tuy nhiên, việc huấn luyện các chatbot video-ngôn ngữ gặp phải
những thách thức tương tự như tiền huấn luyện video-ngôn ngữ, cụ thể
là chi phí tính toán khổng lồ và nguồn huấn luyện hạn chế. May mắn thay,
Mug-STAN cung cấp một giải pháp tiềm năng cho những thách thức này.
Không giống như các chatbot video hiện tại, cách tiếp cận của chúng tôi
không liên quan đến việc điều chỉnh hướng dẫn tiêu tốn tài nguyên.
Thay vào đó, chúng tôi khai thác sức mạnh của kiến thức hình ảnh-ngôn ngữ
hiện có theo cách zero-shot. Cụ thể, trước tiên chúng tôi hậu tiền huấn luyện
Mug-STAN-CLIP trên các tập dữ liệu video-ngôn ngữ. Sau đó, chúng tôi
tích hợp các mạng nhánh tiền huấn luyện vào backbone thị giác của
các chatbot hình ảnh-ngôn ngữ. Với việc hầu hết các chatbot đa phương tiện
hiện tại thường sử dụng CLIP đông lạnh làm backbone thị giác, phương pháp
của chúng tôi có thể trao quyền một cách liền mạch cho các chatbot
hình ảnh-ngôn ngữ với khả năng hiểu và xử lý video. Cuối cùng,
token video cũng có thể được đưa vào LLM một cách liền mạch cho
trò chuyện video. Sự tích hợp này được tạo điều kiện bởi thực tế
rằng đầu ra của STAN khớp với số lượng token của bộ mã hóa hình ảnh.
Chúng tôi lấy LLaVa [24] làm chatbot hình ảnh-văn bản tiền huấn luyện
và trình bày kết quả định tính của STAN-LLaVa trong Hình 11. So với
LLaVa, phương pháp của chúng tôi trao quyền cho chatbot để tường thuật
chính xác các sự kiện trong chuỗi video và nhận dạng chính xác
các hành động mở rộng thời gian. Đáng chú ý, những kết quả này
được đạt được mà không cần sử dụng bất kỳ điều chỉnh hướng dẫn nào.
Điều này nhấn mạnh tiềm năng đáng kể của Mug-STAN trong việc thích ứng
các chatbot hình ảnh-ngôn ngữ tiền huấn luyện vào lĩnh vực video.

V. KẾT LUẬN

Trong bài báo này, trước tiên chúng tôi điều tra và xác định
điểm chính của việc thích ứng các mô hình hình ảnh-ngôn ngữ
tiền huấn luyện sang các miền video: xây dựng mô hình hóa thời gian
khái quát và ngăn chặn lệch một phần video-văn bản. Vì vậy,
chúng tôi đề xuất Mạng Phụ trợ Không gian-Thời gian với mô-đun
căn chỉnh Hướng dẫn lẫn nhau (Mug-STAN) mới, trong đó STAN
sử dụng cấu trúc nhánh đa cấp cho mô hình hóa thời gian hiệu quả
và Mug giới thiệu tập hợp đặc trưng được hướng dẫn xuyên phương tiện
lẫn nhau để giảm thiểu lệch. Cuối cùng, chúng tôi thực hiện các
thí nghiệm toàn diện để chứng minh sự ưu việt của Mug-STAN.
Kết quả thí nghiệm rộng rãi cho thấy rằng phương pháp thích ứng
của chúng tôi đạt được kết quả tiên tiến trên một phạm vi rộng
các tác vụ video.

TÀI LIỆU THAM KHẢO

[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., "Learning transferable
visual models from natural language supervision," in International
Conference on Machine Learning. PMLR, 2021, pp. 8748–8763.

[2] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu,
"Coca: Contrastive captioners are image-text foundation models," arXiv
preprint arXiv:2205.01917, 2022.

[3] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal,
O. K. Mohammed, S. Singhal, S. Som et al., "Image as a foreign
language: Beit pretraining for vision and vision-language tasks," in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 19 175–19 186.

[4] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and
J. Sivic, "Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips," in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 2630–2640.

[5] J. Huang, Y. Li, J. Feng, X. Wu, X. Sun, and R. Ji, "Clover: Towards a
unified video-language alignment and fusion model," in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2023, pp. 14 856–14 866.

[6] H. Xue, Y. Sun, B. Liu, J. Fu, R. Song, H. Li, and J. Luo, "Clip-vip:
Adapting pre-trained image-text model to video-language alignment,"
in The Eleventh International Conference on Learning Representations,
2022.

[7] H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li, "Clip4clip:
An empirical study of clip for end to end video clip retrieval and
captioning," Neurocomputing, vol. 508, pp. 293–304, 2022.

[8] R. Liu, J. Huang, G. Li, J. Feng, X. Wu, and T. H. Li, "Revisiting temporal modeling for clip-based image-to-video knowledge transferring,"
arXiv preprint arXiv:2301.11116, 2023.

[9] S. Buch, C. Eyzaguirre, A. Gaidon, J. Wu, L. Fei-Fei, and J. C.
Niebles, "Revisiting the" video" in video-language understanding," in
Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2022, pp. 2917–2927.

[10] B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang,
and H. Ling, "Expanding language-image pretrained models for general video recognition," in European Conference on Computer Vision.
Springer, 2022, pp. 1–18.

[11] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, "St-adapter: Parameterefficient image-to-video transfer learning," Advances in Neural Information Processing Systems, vol. 35, pp. 26 462–26 477, 2022.

[12] J. Carreira and A. Zisserman, "Quo vadis, action recognition? a new
model and the kinetics dataset," in proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2017, pp. 6299–6308.

[13] G. Bertasius, H. Wang, and L. Torresani, "Is space-time attention all
you need for video understanding?" in ICML, vol. 2, no. 3, 2021, p. 4.

[14] H. Fang, P. Xiong, L. Xu, and Y. Chen, "Clip2video: Mastering videotext retrieval via image clip," arXiv preprint arXiv:2106.11097, 2021.

[15] Y. Liu, P. Xiong, L. Xu, S. Cao, and Q. Jin, "Ts2-net: Token shift
and selection transformer for text-video retrieval," in Computer Vision–
ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27,
2022, Proceedings, Part XIV. Springer, 2022, pp. 319–335.

[16] Q. Wang, Y. Zhang, Y. Zheng, P. Pan, and X.-S. Hua, "Disentangled representation learning for text-video retrieval," arXiv preprint
arXiv:2203.07111, 2022.

[17] J. Jiang, S. Min, W. Kong, H. Wang, Z. Li, and W. Liu, "Tencent textvideo retrieval: Hierarchical cross-modal interactions with multi-level
representations," IEEE Access, 2022.

[18] J. Xu, T. Mei, T. Yao, and Y. Rui, "Msr-vtt: A large video description
dataset for bridging video and language," in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 5288–
5296.

[19] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., "The kinetics
human action video dataset," arXiv preprint arXiv:1705.06950, 2017.

[20] L. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and
B. Russell, "Localizing moments in video with natural language," in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 5803–5812.

[21] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, "Frozen in time: A
joint video and image encoder for end-to-end retrieval," in Proceedings
of the IEEE/CVF International Conference on Computer Vision, 2021,
pp. 1728–1738.

[22] H. Xue, T. Hang, Y. Zeng, Y. Sun, B. Liu, H. Yang, J. Fu, and B. Guo,
"Advancing high-resolution video-language representation with largescale video transcriptions," in International Conference on Computer
Vision and Pattern Recognition (CVPR), 2022.

[23] R. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and
Y. Choi, "Merlot: Multimodal neural script knowledge models," Advances in Neural Information Processing Systems, vol. 34, pp. 23 634–
23 651, 2021.

--- TRANG 16 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 16

[24] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," arXiv
preprint arXiv:2304.08485, 2023.

[25] Z. Huang, Z. Zeng, Y. Huang, B. Liu, D. Fu, and J. Fu, "Seeing out
of the box: End-to-end pre-training for vision-language representation
learning," in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2021, pp. 12 976–12 985.

[26] Z. Huang, Z. Zeng, B. Liu, D. Fu, and J. Fu, "Pixel-bert: Aligning
image pixels with text by deep multi-modal transformers," arXiv preprint
arXiv:2004.00849, 2020.

[27] H. Xue, Y. Huang, B. Liu, H. Peng, J. Fu, H. Li, and J. Luo,
"Probing inter-modality: Visual parsing with self-attention for visionand-language pre-training," Advances in Neural Information Processing
Systems, vol. 34, pp. 4514–4528, 2021.

[28] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H.
Sung, Z. Li, and T. Duerig, "Scaling up visual and vision-language
representation learning with noisy text supervision," in International
Conference on Machine Learning. PMLR, 2021, pp. 4904–4916.

[29] L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu,
X. Huang, B. Li, C. Li et al., "Florence: A new foundation model for
computer vision," arXiv preprint arXiv:2111.11432, 2021.

[30] K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Learning to prompt for visionlanguage models," International Journal of Computer Vision, vol. 130,
no. 9, pp. 2337–2348, 2022.

[31] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski,
"Styleclip: Text-driven manipulation of stylegan imagery," in Proceedings of the IEEE/CVF International Conference on Computer Vision,
2021, pp. 2085–2094.

[32] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,
A. Mensch, K. Millican, M. Reynolds et al., "Flamingo: a visual
language model for few-shot learning," Advances in Neural Information
Processing Systems, vol. 35, pp. 23 716–23 736, 2022.

[33] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language
models," arXiv preprint arXiv:2301.12597, 2023.

[34] T.-J. Fu, L. Li, Z. Gan, K. Lin, W. Y. Wang, L. Wang, and Z. Liu,
"Violet: End-to-end video-language transformers with masked visualtoken modeling," arXiv preprint arXiv:2111.12681, 2021.

[35] J. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu, "Less
is more: Clipbert for video-and-language learning via sparse sampling,"
in Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition, 2021, pp. 7331–7341.

[36] Y. Ge, Y. Ge, X. Liu, D. Li, Y. Shan, X. Qie, and P. Luo, "Bridging
video-text retrieval with multiple choice questions," in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 16 167–16 176.

[37] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luˇci´c, and C. Schmid,
"Vivit: A video vision transformer," in Proceedings of the IEEE/CVF
international conference on computer vision, 2021, pp. 6836–6846.

[38] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, "Video swin
transformer," in Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, 2022, pp. 3202–3211.

[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training
of deep bidirectional transformers for language understanding," arXiv
preprint arXiv:1810.04805, 2018.

[40] A. Miech, J.-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman, "End-to-end learning of visual representations from uncurated
instructional videos," in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2020, pp. 9879–9889.

[41] Z. Zeng, Y. Ge, X. Liu, B. Chen, P. Luo, S.-T. Xia, and Y. Ge,
"Learning transferable spatiotemporal representations from natural script
knowledge," in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 23 079–23 089.

[42] T. Han, W. Xie, and A. Zisserman, "Temporal alignment networks
for long-term video," in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022, pp. 2906–2916.

[43] Z. Gao, J. Liu, W. Sun, S. Chen, D. Chang, and L. Zhao, "Clip2tv:
Align, match and distill for video-text retrieval," arXiv preprint
arXiv:2111.05610, 2021.

[44] H. Zhang, A. Sun, W. Jing, and J. T. Zhou, "Temporal sentence
grounding in videos: A survey and future directions," IEEE Transactions
on Pattern Analysis and Machine Intelligence, 2023.

[45] C. Ju, T. Han, K. Zheng, Y. Zhang, and W. Xie, "Prompting visuallanguage models for efficient video understanding," in European Conference on Computer Vision. Springer, 2022, pp. 105–124.

[46] P. Hu, Z. Huang, D. Peng, X. Wang, and X. Peng, "Cross-modal retrieval
with partially mismatched pairs," IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2023.

[47] F. Liu, X. Wu, C. You, S. Ge, Y. Zou, and X. Sun, "Aligning source
visual and target language domains for unpaired video captioning," IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 44,
no. 12, pp. 9255–9268, 2021.

[48] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,
"An image is worth 16x16 words: Transformers for image recognition at
scale," in International Conference on Learning Representations, 2020.

[49] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J. Feng,
and S. Yan, "Tokens-to-token vit: Training vision transformers from
scratch on imagenet," in Proceedings of the IEEE/CVF International
Conference on Computer Vision, 2021, pp. 558–567.

[50] C. Feichtenhofer, H. Fan, J. Malik, and K. He, "Slowfast networks
for video recognition," in Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), October 2019.

[51] Z. Tong, Y. Song, J. Wang, and L. Wang, "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training,"
Advances in neural information processing systems, vol. 35, pp. 10 078–
10 093, 2022.

[52] K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, "Mask r-cnn," in
Proceedings of the IEEE international conference on computer vision,
2017, pp. 2961–2969.

[53] H. Xu, G. Ghosh, P.-Y. Huang, D. Okhonko, A. Aghajanyan, F. Metze,
L. Zettlemoyer, and C. Feichtenhofer, "Videoclip: Contrastive pretraining for zero-shot video-text understanding," in Proceedings of the
2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 6787–6800.

[54] D. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi, "Align and prompt:
Video-and-language pre-training with entity prompts," in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 4953–4963.

[55] J. Wang, D. Chen, Z. Wu, C. Luo, L. Zhou, Y. Zhao, Y. Xie, C. Liu, Y.-G.
Jiang, and L. Yuan, "Omnivl: One foundation model for image-language
and video-language tasks," Advances in neural information processing
systems, vol. 35, pp. 5696–5710, 2022.

[56] J. A. Portillo-Quintero, J. C. Ortiz-Bayliss, and H. Terashima-Mar´ın, "A
straightforward framework for video retrieval using clip," in Mexican
Conference on Pattern Recognition. Springer, 2021, pp. 3–12.

[57] M. Wang, J. Xing, and Y. Liu, "Actionclip: A new paradigm for video
action recognition," arXiv preprint arXiv:2109.08472, 2021.

[58] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,
and I. Misra, "Imagebind: One embedding space to bind them all,"
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2023, pp. 15 180–15 190.

[59] Y. Wang, K. Li, Y. Li, Y. He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y. Liu,
Z. Wang et al., "Internvideo: General video foundation models via generative and discriminative learning," arXiv preprint arXiv:2212.03191,
2022.

[60] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle,
A. Courville, and B. Schiele, "Movie description," International Journal
of Computer Vision, vol. 123, no. 1, pp. 94–120, 2017.

[61] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al.,
"The" something something" video database for learning and evaluating visual common sense," in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 5842–5850.

[62] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan, G. Toderici, S. Ricco, R. Sukthankar et al., "Ava: A
video dataset of spatio-temporally localized atomic visual actions," in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 6047–6056.

[63] Y. Yu, J. Kim, and G. Kim, "A joint sequence fusion model for video
question answering and retrieval," in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 471–487.

[64] L. Sevilla-Lara, S. Zha, Z. Yan, V. Goswami, M. Feiszli, and L. Torresani, "Only time can tell: Discovering temporal data for temporal
modeling," in Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (WACV), January 2021, pp. 535–544.

[65] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and
Y. Qiao, "Videochat: Chat-centric video understanding," arXiv preprint
arXiv:2305.06355, 2023.

[66] Z. Luo, D. Chen, Y. Zhang, Y. Huang, L. Wang, Y. Shen, D. Zhao,
J. Zhou, and T. Tan, "Videofusion: Decomposed diffusion models
for high-quality video generation," in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2023, pp.
10 209–10 218.

--- TRANG 17 ---
TẠP CHÍ CÁC TỆP LỚPAT EX, SỐ 14, SỐ 8, THÁNG 5 NĂM 2023 17

[67] X. Cheng, H. Lin, X. Wu, F. Yang, and D. Shen, "Improving video-text
retrieval by multi-stream corpus alignment and dual softmax loss," arXiv
preprint arXiv:2109.04290, 2021.

[68] S.-V. Bogolin, I. Croitoru, H. Jin, Y. Liu, and S. Albanie, "Cross
modal retrieval with querybank normalisation," in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 5194–5205.

[69] V. Gabeur, C. Sun, K. Alahari, and C. Schmid, "Multi-modal transformer
for video retrieval," in Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV
16. Springer, 2020, pp. 214–229.

[70] H. Xue, T. Hang, Y. Zeng, Y. Sun, B. Liu, H. Yang, J. Fu, and B. Guo,
"Advancing high-resolution video-language representation with largescale video transcriptions," in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2022, pp. 5036–5045.

[71] J. Wang, Y. Ge, R. Yan, Y. Ge, K. Q. Lin, S. Tsutsui, X. Lin, G. Cai,
J. Wu, Y. Shan et al., "All in one: Exploring unified video-language
pre-training," in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 2023, pp. 6598–6608.

[72] S. Zhao, L. Zhu, X. Wang, and Y. Yang, "Centerclip: Token clustering for
efficient text-video retrieval," in Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information
Retrieval, 2022, pp. 970–981.

[73] I. Loshchilov and F. Hutter, "Fixing weight decay regularization in
adam," 2018.

[74] H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer, "Multiscale vision transformers," in Proceedings of the IEEE/CVF
international conference on computer vision, 2021, pp. 6824–6835.

[75] S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid,
"Multiview transformers for video recognition," in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, 2022,
pp. 3333–3343.

[76] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and
C. Feichtenhofer, "Mvitv2: Improved multiscale vision transformers
for classification and detection," in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June
2022, pp. 4804–4814.

[77] R. Wang, D. Chen, Z. Wu, Y. Chen, X. Dai, M. Liu, L. Yuan, and Y.-G.
Jiang, "Masked video distillation: Rethinking masked feature modeling
for self-supervised video representation learning," in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2023, pp. 6312–6322.

[78] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou,
and H. Yang, "Ofa: Unifying architectures, tasks, and modalities through
a simple sequence-to-sequence learning framework," in International
Conference on Machine Learning. PMLR, 2022, pp. 23 318–23 340.

[79] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman,
M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., "Laion5b: An open large-scale dataset for training next generation image-text
models," Advances in Neural Information Processing Systems, vol. 35,
pp. 25 278–25 294, 2022.

[80] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4:
Enhancing vision-language understanding with advanced large language
models," arXiv preprint arXiv:2304.10592, 2023.
