# 2310.03734.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.03734.pdf
# Kích thước tệp: 8574103 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TẬN DỤNG DỮ LIỆU KHÔNG CÓ CẶP CHO CÁC MÔ HÌNH SINH TẠO THỊ GIÁC-NGÔN NGỮ
THÔNG QUA TÍNH NHẤT QUÁN CHU TRÌNH

Tianhong Li1∗Sangnie Bhardwaj2,3∗Yonglong Tian3Han Zhang4Jarred Barber3
Dina Katabi1Guillaume Lajoie2Huiwen Chang5Dilip Krishnan3
1MIT CSAIL2Mila3Google Research4Google DeepMind5OpenAI

TÓM TẮT
Các mô hình sinh tạo thị giác-ngôn ngữ hiện tại dựa vào các bộ dữ liệu rộng lớn của dữ liệu cặp hình ảnh-văn bản để đạt được hiệu suất và khả năng khái quát tối ưu. Tuy nhiên, việc thu thập tự động dữ liệu như vậy (ví dụ: thông qua thu thập web quy mô lớn) dẫn đến chất lượng thấp và tương quan hình ảnh-văn bản kém, trong khi chú thích bằng tay chính xác hơn nhưng đòi hỏi nỗ lực thủ công và chi phí đáng kể. Chúng tôi giới thiệu ITIT (InTegrating Image Text): một mô hình huấn luyện sáng tạo dựa trên khái niệm tính nhất quán chu trình cho phép huấn luyện thị giác-ngôn ngữ trên dữ liệu hình ảnh và văn bản không được ghép cặp. ITIT bao gồm một bộ mã hóa hình ảnh-văn bản chung với các bộ giải mã hình ảnh và văn bản riêng biệt cho phép sinh tạo hai chiều từ hình ảnh sang văn bản và từ văn bản sang hình ảnh trong một khung duy nhất. Trong quá trình huấn luyện, ITIT tận dụng một tập nhỏ dữ liệu cặp hình ảnh-văn bản để đảm bảo đầu ra của nó khớp hợp lý với đầu vào theo cả hai hướng. Đồng thời, mô hình cũng được huấn luyện trên các tập dữ liệu lớn hơn nhiều chỉ chứa hình ảnh hoặc văn bản. Điều này được thực hiện bằng cách thực thi tính nhất quán chu trình giữa các mẫu không cặp gốc và các bản tương ứng được tạo ra bởi chu trình. Ví dụ, nó tạo ra một chú thích cho một hình ảnh đầu vào cho trước và sau đó sử dụng chú thích đó để tạo ra một hình ảnh đầu ra, và thực thi sự tương tự giữa hình ảnh đầu vào và đầu ra. Các thí nghiệm của chúng tôi cho thấy ITIT với các tập dữ liệu không cặp thể hiện hành vi mở rộng tương tự như sử dụng dữ liệu cặp chất lượng cao. Chúng tôi trình bày hiệu suất sinh tạo hình ảnh và chú thích ngang bằng với các mô hình văn bản sang hình ảnh và hình ảnh sang văn bản tiên tiến với số lượng dữ liệu cặp hình ảnh-văn bản ít hơn hàng bậc độ lớn (chỉ 3M).

1 GIỚI THIỆU
Huấn luyện đa phương thức hình ảnh-văn bản đã thu hút sự chú ý đáng kể trong những năm gần đây. Các mô hình sinh tạo từ văn bản sang hình ảnh cho thấy khả năng ấn tượng trong việc tổng hợp các hình ảnh thực tế từ các lời nhắc văn bản (Rombach et al., 2022; Chang et al., 2023; Saharia et al., 2022; Yu et al., 2022b; Ramesh et al., 2021). Tương tự, các mô hình từ hình ảnh sang văn bản đã chứng minh khả năng hiểu hình ảnh nâng cao bằng cách cung cấp các mô tả chính xác về hình ảnh đầu vào (Chen et al., 2023; Wang et al., 2022a; Li et al., 2022; 2023a; Alayrac et al., 2022; Wang et al., 2022b). Huấn luyện những mô hình này đến hiệu suất đặc biệt đòi hỏi các tập dữ liệu bao gồm hàng trăm triệu đến hàng tỷ mẫu cặp hình ảnh-văn bản (Schuhmann et al., 2022). Việc thu thập các tập dữ liệu cặp rất lớn có chi phí đáng kể cũng như lo ngại về chất lượng thấp (Jia et al., 2021). Mặt khác, các tập dữ liệu hình ảnh hoặc văn bản đơn phương thức đa dạng và rộng lớn vẫn chưa được sử dụng trong huấn luyện thị giác-ngôn ngữ sinh tạo hiện tại (Raffel et al., 2020; Sun et al., 2017; Zhai et al., 2022). Điều này đặt ra một câu hỏi tự nhiên: liệu chúng ta có thể tận dụng dữ liệu hình ảnh và văn bản không cặp để tạo thuận lợi cho huấn luyện thị giác-ngôn ngữ sinh tạo không?

Vấn đề chính với việc sử dụng dữ liệu không cặp trong quá trình huấn luyện thị giác-ngôn ngữ là thiếu sự giám sát. Để khắc phục vấn đề này, chúng tôi giới thiệu ITIT, một mô hình huấn luyện mới sử dụng các hàm mất mát tính nhất quán chu trình giữa các hình ảnh/văn bản được tạo ra bởi chu trình và các đầu vào gốc tương ứng của chúng để cung cấp giám sát cho dữ liệu chỉ-hình ảnh và chỉ-văn bản (Hình 1). ITIT sử dụng một tập nhỏ dữ liệu cặp hình ảnh-văn bản để đạt được hiệu suất sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản hợp lý. Đồng thời, đối với dữ liệu hình ảnh (văn bản) không cặp, ITIT tạo ra các bản tương ứng văn bản (hình ảnh) tương ứng

∗Đồng tác giả đầu. Công việc này được thực hiện khi Tianhong Li thực tập tại và Huiwen Chang làm việc tại Google Research. Liên hệ với Tianhong Li <tianhong@mit.edu>.

--- TRANG 2 ---
T2I Văn bản không cặp Hình ảnh tổng hợp I2T một con hồng hạc trên hồ Văn bản tái tạo một con hồng hạc trên hồ

T2I Hình ảnh không cặp I2T Văn bản tổng hợp một con mèo trên cỏ xanh Hình ảnh tái tạo Mất mát tái tạo Mất mát tái tạo Chu trình Văn bản-Hình ảnh-Văn bản (T2I2T) Chu trình Hình ảnh-Văn bản-Hình ảnh (I2T2I)

xanh

Hình 1: Tổng quan về ITIT. Đối với dữ liệu không cặp, ITIT đầu tiên tạo ra bản tương ứng hình ảnh/văn bản, và sau đó sử dụng những bản tương ứng được tạo ra này để tái tạo văn bản hoặc hình ảnh gốc.

các bản tương ứng và sử dụng chúng làm đầu vào để tái tạo hình ảnh (văn bản) đầu vào: điều này tương ứng với một hàm mất mát chu trình đầy đủ. Chúng tôi xem xét hai loại chu trình đầy đủ: T2I2T (bắt đầu với một mẫu văn bản không cặp); và I2T2I (bắt đầu với một mẫu hình ảnh không cặp). Hai loại chu trình này cho phép chúng tôi tận dụng cả dữ liệu hình ảnh và văn bản không cặp để cung cấp các tín hiệu giám sát có thông tin cho huấn luyện.

Để cho phép huấn luyện chu trình, trước tiên chúng tôi thống nhất việc sinh tạo từ hình ảnh sang văn bản (I2T) và từ văn bản sang hình ảnh (T2I) trong cùng một khung, với một bộ mã hóa hình ảnh-văn bản hai chiều và các bộ giải mã hình ảnh và văn bản riêng biệt. Chúng tôi tokenize hình ảnh thành các token thị giác rời rạc (Van Den Oord et al., 2017) và kết hợp chúng với các embedding văn bản từ mô hình T5 được huấn luyện trước (Raffel et al., 2020) làm đầu vào cho bộ mã hóa hình ảnh-văn bản chung. Đối với sinh tạo I2T, chúng tôi sử dụng một bộ giải mã văn bản tự hồi quy (Wang et al., 2022a), trong khi đối với sinh tạo T2I, chúng tôi sử dụng một bộ giải mã hình ảnh song song không tự hồi quy (Chang et al., 2023), nhanh hơn một bậc độ lớn so với các bộ giải mã hình ảnh tự hồi quy như Yu et al. (2022b).

Một thách thức kỹ thuật của ITIT là các quy trình sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản tiên tiến thường bao gồm nhiều bước tiến của mô hình (Esser et al., 2021; Chang et al., 2023; Rombach et al., 2022; Wang et al., 2022a). Việc lan truyền ngược gradient qua tất cả các bước tiến này mang lại chi phí bộ nhớ và tính toán đáng kể. Để giải quyết vấn đề này, đối với chu trình T2I2T, chúng tôi đầu tiên tạo ra hình ảnh với việc giải mã song song. Sau đó chúng tôi lan truyền ngược gradient qua một bước của quá trình giải mã song song. Đối với chu trình I2T2I, chúng tôi đầu tiên tạo ra văn bản tự hồi quy với nhiều bước. Sau đó chúng tôi tiến bộ giải mã văn bản một lần với văn bản được tạo ra làm đầu vào, và lan truyền ngược gradient chỉ đến bước tiến này. Điều này giảm đáng kể chi phí tính toán của huấn luyện chu trình, làm cho nó khả thi để áp dụng trong các cài đặt mô hình lớn.

Chúng tôi đánh giá hiệu suất của ITIT trên các benchmark sinh tạo từ hình ảnh sang văn bản và từ văn bản sang hình ảnh tiêu chuẩn và chứng minh rằng, bằng cách tận dụng dữ liệu không cặp và tính nhất quán chu trình, ITIT đạt được mức hiệu suất tương tự như một baseline không chu trình. Tuy nhiên, ITIT sử dụng dữ liệu cặp ít hơn tới 2 bậc độ lớn. Hơn nữa, ITIT mở rộng tương tự với dữ liệu không cặp như baseline với số lượng dữ liệu cặp tương đương, trong khi mạnh mẽ hơn nhiều đối với chất lượng dữ liệu thấp. Chúng tôi cũng so sánh ITIT với các phương pháp tiên tiến và cho thấy rằng chúng tôi có thể đạt được hiệu suất tương đương trên các benchmark từ văn bản sang hình ảnh và từ hình ảnh sang văn bản phổ biến với dữ liệu cặp ít hơn đáng kể. Các đóng góp của chúng tôi được tóm tắt như sau:

• Chúng tôi giới thiệu một khung thống nhất việc sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản, và đề xuất ITIT, một kỹ thuật mới thực thi tính nhất quán giữa các hình ảnh/văn bản được tạo ra bởi chu trình và các bản gốc tương ứng của chúng. Cách tiếp cận này cho phép huấn luyện các mô hình từ hình ảnh sang văn bản và từ văn bản sang hình ảnh sử dụng dữ liệu hình ảnh và văn bản không cặp.

• Chúng tôi đánh giá toàn diện khung ITIT được đề xuất và phương pháp tính nhất quán chu trình hình ảnh-văn bản, và chứng minh rằng chúng cải thiện đáng kể hiệu suất mô hình.

--- TRANG 3 ---
• Chúng tôi cho thấy ITIT có thể đạt được hiệu suất ngang bằng với các phương pháp tiên tiến trên các benchmark từ văn bản sang hình ảnh và từ hình ảnh sang văn bản phổ biến với dữ liệu cặp ít hơn nhiều (~100x). Khi mở rộng dữ liệu huấn luyện để cải thiện hiệu quả mô hình, chúng tôi cho thấy rằng chúng tôi có thể chỉ thêm các ví dụ không cặp sử dụng khung của chúng tôi và đạt được hiệu suất tương tự như dữ liệu cặp được mở rộng, mà không có những nhược điểm của nỗ lực thủ công đáng kể và chất lượng ghép cặp kém.

2 TỔNG QUAN TÀI LIỆU

Sinh tạo từ Hình ảnh sang Văn bản. Nhiều công trình khám phá việc tự động tạo ra các mô tả văn bản từ hình ảnh đầu vào, hoặc huấn luyện mạng chỉ với hàm mất mát sinh tạo (Wang et al., 2022b; Alayrac et al., 2022; Chen et al., 2023; Li et al., 2022; 2023a), hoặc kết hợp nó với học tương phản (Yu et al., 2022a). GIT (Wang et al., 2022a) huấn luyện một mô hình bao gồm một bộ mã hóa hình ảnh và một bộ giải mã văn bản tự hồi quy sử dụng hàm mất mát mô hình hóa ngôn ngữ, bộ mã hóa hình ảnh được huấn luyện trước với hàm mất mát tương phản (Radford et al., 2021). Trong công trình của chúng tôi, chúng tôi áp dụng một khung tương tự như GIT cho khung Hình ảnh sang Văn bản (I2T) của chúng tôi, nhưng chúng tôi khởi tạo bộ mã hóa hình ảnh của chúng tôi từ đầu.

Sinh tạo từ Văn bản sang Hình ảnh. Các công trình gần đây tập trung vào hai mô hình chính: các mô hình dựa trên khuếch tán (Rombach et al. (2022); Dhariwal & Nichol (2021); Nichol et al. (2021); Saharia et al. (2022); Ramesh et al. (2022); Ruiz et al. (2023)); và các phương pháp dựa trên token. Các chiến lược dựa trên token biến đổi hình ảnh thô thành các token hình ảnh, và dự đoán các token này hoặc theo cách tự hồi quy (Esser et al., 2021; Ramesh et al., 2021; Gafni et al., 2022; Yu et al., 2021; Ding et al., 2021; Yu et al., 2022b) hoặc song song (Chang et al., 2022; Li et al., 2023b; Chang et al., 2023). Muse (Chang et al., 2023) chứng minh rằng các chiến lược dựa trên token với việc giải mã song song có thể nhanh hơn đáng kể so với các mô hình sinh tạo dựa trên khuếch tán hoặc tự hồi quy. Do lợi thế về tốc độ này tạo thuận lợi cho tổng hợp từ văn bản sang hình ảnh trong quá trình huấn luyện, chúng tôi áp dụng chiến lược này trong khung T2I của chúng tôi.

Thống nhất Sinh tạo Hình ảnh và Văn bản. COBIT (You et al. (2023)) đạt được điều này bằng cách sử dụng các bộ mã hóa đơn hình ảnh và văn bản riêng biệt, kết hợp với một bộ giải mã đa phương thức thống nhất. Ngoài ra, CM3 (Aghajanyan et al. (2022)) và CM3Leon (Yu et al. (2023)) tận dụng các mô hình sinh tạo được che causally được huấn luyện trên các tập dữ liệu tài liệu đa phương thức rộng lớn, và cho phép tổng hợp cả văn bản và hình ảnh. Tuy nhiên, tất cả các công trình này vẫn phụ thuộc nhiều vào các tập dữ liệu cặp hình ảnh-văn bản quy mô lớn.

Tận dụng Dữ liệu Không Cặp trong Huấn luyện Thị giác-Ngôn ngữ Sinh tạo. Các công trình đầu đã cố gắng sử dụng hình ảnh và văn bản không cặp để huấn luyện mô hình chú thích hình ảnh theo cách không giám sát (Feng et al., 2019). Tuy nhiên, hiệu suất tương đối kém. Các nỗ lực gần đây trong việc kết hợp dữ liệu không cặp vào huấn luyện thị giác-ngôn ngữ sinh tạo chủ yếu tập trung vào các bộ mã hóa hình ảnh và văn bản được huấn luyện trước (Esser et al., 2021; Roberts et al., 2019). Tuy nhiên, những ứng dụng này chỉ giới hạn trong việc huấn luyện trước và không bao gồm toàn bộ quy trình huấn luyện thị giác-ngôn ngữ sinh tạo, do đó chỉ cung cấp những cải thiện gia tăng. Trong một số trường hợp, các nhà nghiên cứu đã khám phá việc sử dụng dữ liệu chỉ-văn bản để cải thiện các bộ giải mã văn bản (Wang et al. (2022b)), sử dụng huấn luyện văn bản-sang-văn bản. Tuy nhiên, điều này chỉ cải thiện bộ giải mã văn bản chứ không phải bộ mã hóa hình ảnh, dẫn đến những cải thiện bị hạn chế một lần nữa.

Tính nhất quán chu trình. Khái niệm tính nhất quán chu trình đã được sử dụng trước đây để cung cấp điều chỉnh và/hoặc bù đắp cho việc thiếu dữ liệu được chú thích. Zach et al. (2010); Zhou et al. (2016); Godard et al. (2016); Zhu et al. (2017); Messikommer et al. (2022) khám phá nó cho các ứng dụng thị giác máy tính như học tương ứng dày đặc, phát hiện sự kiện, ước lượng độ sâu, và dịch hình ảnh-sang-hình ảnh. Liên quan nhất đến công trình của chúng tôi là Gorti & Ma (2018), sử dụng tính nhất quán chu trình văn bản-hình ảnh-văn bản để thực hiện dịch từ văn bản sang hình ảnh, nhưng hiệu suất kém. Hơn nữa, không công trình nào trước đây đã khám phá tiềm năng của tính nhất quán chu trình trong huấn luyện thị giác-ngôn ngữ sinh tạo sử dụng dữ liệu không cặp.

Cách tiếp cận mới của chúng tôi khác biệt với các mô hình thị giác-ngôn ngữ trước đây phụ thuộc nhiều vào một bộ dữ liệu lớn của dữ liệu cặp hình ảnh-văn bản, hoặc các phương pháp tinh chỉnh chỉ nhắm vào các bộ mã hóa/giải mã văn bản hoặc hình ảnh riêng biệt. Lần đầu tiên, phương pháp của chúng tôi tạo thuận lợi cho việc sử dụng dữ liệu hình ảnh và văn bản không cặp trong quá trình huấn luyện thị giác-ngôn ngữ sinh tạo. Sự đổi mới này giảm đáng kể sự phụ thuộc vào các mẫu cặp hình ảnh-văn bản trong quá trình huấn luyện, điều này trao quyền cho việc mở rộng huấn luyện thị giác-ngôn ngữ sinh tạo đến gần như vô hạn các tập dữ liệu chỉ-văn bản và chỉ-hình ảnh.

--- TRANG 4 ---
3 PHƯƠNG PHÁP

ITIT là khung đầu tiên cho phép huấn luyện thị giác-ngôn ngữ sinh tạo trên dữ liệu chỉ-hình ảnh và chỉ-văn bản không cặp. Nó sử dụng một kiến trúc đơn giản nhưng hiệu quả: một bộ mã hóa hình ảnh-văn bản thống nhất và hai bộ giải mã hình ảnh và văn bản riêng biệt. Thiết kế này cho phép liền mạch việc sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản trong cùng một khung, điều này mở đường cho các hàm mất mát chu trình văn bản-hình ảnh-văn bản (T2I2T) và hình ảnh-văn bản-hình ảnh (I2T2I). Dưới đây, chúng tôi mô tả chi tiết từng thành phần của kiến trúc ITIT và mô hình huấn luyện tính nhất quán chu trình.

3.1 KHUNG SINH TẠO HÌNH ẢNH-VĂN BẢN THỐNG NHẤT

Kiến trúc. Trước tiên chúng tôi thu được embedding văn bản T = [tl]L l=1 từ đầu ra của bộ mã hóa T5 (Roberts et al., 2019) trên văn bản thô. Tương tự, hình ảnh thô được truyền qua một VQ-tokenizer được huấn luyện trước (Esser et al., 2021) để xuất ra các token hình ảnh I = [ik]K k=1. L và K là độ dài chuỗi token cho văn bản và hình ảnh, tương ứng. Các token hình ảnh I sau đó được nhúng với một lớp nhúng và nối với các đặc trưng văn bản T5 T làm đầu vào cho bộ mã hóa hình ảnh-văn bản. Các bộ giải mã đặc thù cho phương thức sau đó hoạt động trên các đặc trưng hình ảnh-văn bản được mã hóa để tạo ra các token văn bản hoặc hình ảnh. Bộ giải mã văn bản là tự hồi quy (Wang et al., 2022a), trong khi bộ giải mã hình ảnh là song song (Chang et al., 2023). Cả bộ mã hóa và bộ giải mã đều dựa trên các lớp Transformer (Vaswani et al., 2017). Một mô tả chi tiết về kiến trúc mô hình được bao gồm trong Phụ lục B.

Huấn luyện Hình ảnh sang Văn bản (I2T). Như được hiển thị trong Hình 2, chúng tôi đưa vào các token hình ảnh được che cùng với embedding văn bản trống cho bộ mã hóa hình ảnh-văn bản. Việc che được sử dụng để tiết kiệm tính toán, tương tự như MAE (He et al., 2022). Sau đó chúng tôi sử dụng các đặc trưng được tạo ra bởi bộ mã hóa hình ảnh-văn bản, cũng như các token văn bản ground-truth được thêm vào đầu với token [BOS] (begin-of-sentence) làm đầu vào cho bộ giải mã văn bản của chúng tôi. Chúng tôi sử dụng hàm mất mát mô hình hóa ngôn ngữ (LM) tự hồi quy để huấn luyện bộ mã hóa và bộ giải mã:

LI2T = −E(I,T)∈D ∑L l=1 log p(tl|IM, t0, ···, tl−1), (1)

đây là hàm mất mát CE với label smoothing 0.1. Ở đây, t0 được đặt là token [BOS]. IM là (tập con của) các token không bị che trong I và p(ik|IM, T) là xác suất được dự đoán bởi mạng bộ mã hóa-giải mã (lớp 'logits'), D là phân phối của dữ liệu cặp hình ảnh-văn bản. Lưu ý rằng bộ giải mã văn bản sử dụng causal attention tương tự như GIT (Wang et al. (2022a)): mỗi token văn bản chỉ phụ thuộc vào các token văn bản trước đó và tất cả các đặc trưng hình ảnh.

Huấn luyện Văn bản sang Hình ảnh (T2I). Như được hiển thị trong Hình 2, panel phải, chúng tôi sử dụng mô hình hóa hình ảnh bị che để sinh tạo hình ảnh, trong đó mục tiêu huấn luyện là tái tạo các token hình ảnh bị che có điều kiện trên các token hình ảnh không bị che và các đặc trưng văn bản được ghép cặp. Chúng tôi ký hiệu mặt nạ nhị phân xác định token hình ảnh nào bị che bởi M = [mk]K k=1. Chúng tôi sử dụng hàm mất mát cross-entropy giữa các token hình ảnh ground-truth one-hot và đầu ra của bộ giải mã hình ảnh. Cụ thể,

LT2I = −E(I,T)∈D ∑∀k:mk=1 log p(ik|IM, T), (2)

Suy luận. Chúng tôi tuân theo GIT (Wang et al., 2022a) cho suy luận hình ảnh sang văn bản và Muse (Chang et al., 2023) cho suy luận văn bản sang hình ảnh. Thêm chi tiết được bao gồm trong Phụ lục B.

3.2 HUẤN LUYỆN VỚI TÍNH NHẤT QUÁN CHU TRÌNH

Mô hình huấn luyện tính nhất quán chu trình của chúng tôi cho phép huấn luyện với dữ liệu chỉ-hình ảnh và chỉ-văn bản. Ý tưởng chính là đầu tiên tổng hợp văn bản/hình ảnh tương ứng từ dữ liệu chỉ-hình ảnh hoặc chỉ-văn bản, và sau đó sử dụng dữ liệu được tổng hợp làm đầu vào để tái tạo hình ảnh/văn bản gốc. Điều này cho phép chúng tôi áp dụng giám sát tính nhất quán chu trình trên dữ liệu chỉ-hình ảnh và chỉ-văn bản.

Chu trình Văn bản-Hình ảnh-Văn bản (T2I2T). Đường ống huấn luyện T2I2T của chúng tôi được hiển thị trong Hình 3, panel trên. Tại mỗi lần lặp huấn luyện, chúng tôi đầu tiên tổng hợp các token hình ảnh cặp giả I′ cho văn bản đầu vào T = [tl]L l=1 sử dụng đường ống suy luận T2I của chúng tôi. Sau đó chúng tôi áp dụng mặt nạ ngẫu nhiên M cho I′, thực hiện tái tạo trên I′M với văn bản T sử dụng đường ống T2I, và thu được hình ảnh tổng hợp tái tạo Ĩ. Quy trình hai bước này cho phép chúng tôi tránh các yêu cầu bộ nhớ quá mức của việc lan truyền ngược gradient qua tất cả 24 bước của giải mã song song, trong khi vẫn huấn luyện module T2I. Cuối cùng, chúng tôi che ngẫu nhiên Ĩ và sử dụng ĨM để tạo ra văn bản sử dụng đường ống I2T. Mục tiêu của mô hình chu trình của chúng tôi là thực thi tính nhất quán giữa văn bản được tạo ra này và văn bản gốc. Do đó, hàm mất mát tính nhất quán chu trình T2I2T có thể được công thức hóa như sau:

LT2I2T = −ET∈Dtext ∑L l=1 log p(tl|ĨM, t0, ···, tl−1), (3)

Điều này rất tương tự như hàm mất mát I2T trong Phương trình (1), ngoại trừ việc Ĩ được tổng hợp từ T thay vì được rút ra từ phân phối kết hợp hình ảnh-văn bản.

Tính nhất quán Hình ảnh-Văn bản-Hình ảnh (I2T2I). Đường ống huấn luyện I2T2I của chúng tôi được hiển thị trong Hình 3, panel dưới. Tương tự như đường ống T2I2T, chúng tôi đầu tiên tổng hợp các token văn bản cặp giả T′ cho các token hình ảnh đầu vào I sử dụng đường ống suy luận I2T của chúng tôi. Sau đó chúng tôi sử dụng đường ống huấn luyện I2T để dự đoán t̃l từ t′0, ···, t′l−1 và IM. Như trước, điều này tránh các yêu cầu bộ nhớ quá mức của việc lan truyền ngược gradient qua việc giải mã tham lam tự hồi quy. Sau đó chúng tôi che I, và truyền nó qua đường ống T2I với T̃ được dự đoán để tái tạo các token hình ảnh bị che. Một lần nữa, hàm mất mát thực thi tính nhất quán giữa các token hình ảnh tái tạo và gốc sử dụng cross-entropy:

LI2T2I = −EI∈Dimage ∑∀k:mk=1 log p(ik|IM, T̃), (4)

Ước lượng Gradient. Một thách thức trong huấn luyện chu trình của chúng tôi là ĩk = arg max(p(ik|I′M, T) và t̃l = arg max p(tl|IM, t′0, ···, t′l−1), không thể vi phân được. Để giải quyết điều này, chúng tôi sử dụng ước lượng thẳng qua các logits được dự đoán để xấp xỉ gradient. Cụ thể, chúng tôi trực tiếp sao chép gradient trên dự đoán one-hot sang các logits được dự đoán sau softmax. Chúng tôi cho thấy trong phần 4.4 rằng điều này giúp cải thiện cả hiệu suất từ văn bản sang hình ảnh và từ hình ảnh sang văn bản.

--- TRANG 5 ---
   Module Huấn luyện I2T Bộ mã hóa Hình ảnh Văn bản Bộ giải mã Văn bản một con mèo trên cỏ xanh BOS Embedding văn bản trống Logits văn bản được tạo ra Các token hình ảnh bị che……   Module Huấn luyện T2I Bộ mã hóa Hình ảnh Văn bản Bộ giải mã Hình ảnh Embedding văn bản T5 Các token hình ảnh bị che Logits hình ảnh được tạo ra Các token văn bản Ground-truth

Hình 2: Đường ống huấn luyện I2T (trái) và T2I (phải) cho dữ liệu cặp hình ảnh và văn bản.

một con chó và T2I một con mèo trên cỏ xanh I2T Giải mã song song 24 bước Tạo cặp giả trực tuyến T2I2T

I2T2I T2I I2T Huấn luyện Tính nhất quán Chu trình

I2T một con mèo trên cỏ xanh T2I Cross-Entropy Cross-Entropy Dừng Grad Dừng Grad một con chó và

BOS một con mèo trên cỏ xanh BOS một con chó

argmax argmax & che

Giải mã Tham lam

Hình 3: Đường ống huấn luyện chu trình văn bản-hình ảnh-văn bản (trên) và hình ảnh-văn bản-hình ảnh (dưới) cho dữ liệu hình ảnh và văn bản không cặp. Chúng tôi sử dụng hình ảnh và văn bản được tạo ra giả để cho phép tính nhất quán chu trình. Các mặt nạ token hình ảnh M luôn được chọn ngẫu nhiên. Đường nét đứt biểu thị causal attention. Các token văn bản được thêm vào đầu với token [BOS] được sử dụng cho hàm mất mát mô hình hóa ngôn ngữ tự hồi quy.

--- TRANG 6 ---
~100x ít hơn cặp hình ảnh-văn bản ~100x ít hơn cặp hình ảnh-văn bản ~100x ít hơn cặp hình ảnh-văn bản

Hình 4: Hiệu suất của ITIT-H mở rộng như thế nào với dữ liệu Shutterstock cặp bổ sung. Baseline (T2I+I2T) được huấn luyện chỉ với các mẫu cặp. ITIT được huấn luyện với cùng số lượng mẫu cặp, cũng như 398M mẫu không cặp (toàn bộ tập dữ liệu Shutterstock) sử dụng hàm mất mát chu trình.

4 KẾT QUẢ

4.1 THIẾT LẬP THÍ NGHIỆM

Tập dữ liệu. Chúng tôi sử dụng ba tập dữ liệu trong các thí nghiệm của mình: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), và Shutterstock (Shutterstock, 2023). CC3M chứa 3,3 triệu cặp hình ảnh-văn bản chất lượng cao. WebLI (Web Language Image) chứa 111 triệu hình ảnh trong đó chất lượng ghép cặp hình ảnh-văn bản thấp hơn nhiều so với CC3M. Do đó, WebLI nhiều nhiễu hơn đáng kể và, như chúng tôi cho thấy, dẫn đến hiệu suất tồi tệ hơn cho I2T. Shutterstock chứa 398 triệu hình ảnh được gán nhãn bởi các chú thích viên con người, điều này phát sinh chi phí và nỗ lực đáng kể. Thêm chi tiết về tập dữ liệu được bao gồm trong Phụ lục C.

Chúng tôi sử dụng CC3M làm tập dữ liệu cặp của chúng tôi, 50% hình ảnh WebLI làm tập dữ liệu hình ảnh không cặp của chúng tôi, và 50% văn bản WebLI còn lại làm tập dữ liệu văn bản không cặp của chúng tôi cho hầu hết các thí nghiệm của chúng tôi (Phần 4.3 và Phần 4.4). Việc chia 50%-50% này đảm bảo rằng các cặp hình ảnh-văn bản tương ứng không có mặt trong các phần hình ảnh và văn bản không cặp của chúng tôi. Chúng tôi sử dụng tập dữ liệu Shutterstock trong Phần 4.2, nơi chúng tôi phân tích cách ITIT mở rộng so với số lượng khác nhau của các mẫu dữ liệu cặp và không cặp.

Huấn luyện. Chúng tôi đặt độ phân giải hình ảnh đầu vào là 256x256 để nhất quán với tài liệu trước đây. Sau khi truyền qua tokenizer VQGAN, độ dài chuỗi token hình ảnh là 16x16 (256 token). Văn bản thô (độ dài tối đa 64) được token hóa bằng token hóa SentencePiece (SentencePiece, 2023), và được nhúng sử dụng bộ mã hóa T5 được huấn luyện trước. Các embedding này sau đó được nối với các embedding token hình ảnh làm đầu vào cho bộ mã hóa hình ảnh-văn bản của chúng tôi.

Chúng tôi thí nghiệm với Transformer kích thước ViT-B, ViT-L, và ViT-H (Dosovitskiy et al. (2021)) cho bộ mã hóa hình ảnh-văn bản của chúng tôi. Chúng tôi kết hợp các hàm mất mát trong Phương trình 1 đến 4 với trọng số bằng nhau để huấn luyện. Đối với kết quả trong Phần 4.3, chúng tôi sử dụng Adafactor (Shazeer & Stern, 2018) để huấn luyện mô hình cho 1,5M bước với kích thước batch là 2048 (1024 cho các cặp hình ảnh-văn bản, 512 cho hình ảnh không cặp, và 512 cho văn bản không cặp). Chúng tôi sử dụng lịch trình tỷ lệ học cosine với 5K bước khởi động và tỷ lệ học tối đa 1×10−4. Đối với các thí nghiệm khác, chúng tôi sử dụng mô hình huấn luyện giống hệt nhau ngoại trừ việc chúng tôi huấn luyện các mô hình cho 500K bước. Thêm chi tiết được bao gồm trong Phụ lục B.

Đánh giá. Chúng tôi tuân theo benchmark MS-COCO thường được sử dụng và các giao thức đánh giá. Đối với chú thích hình ảnh, chúng tôi đánh giá cả hiệu suất zero-shot và fine-tuning của ITIT trên COCO Karpathy split (Karpathy & Fei-Fei, 2015) và báo cáo điểm CIDEr (Vedantam et al., 2015). Đối với sinh tạo từ văn bản sang hình ảnh, chúng tôi đánh giá ITIT trên 30K cặp hình ảnh-văn bản được chọn ngẫu nhiên từ tập huấn luyện COCO Captions và báo cáo điểm Frechet Inception Distance (FID) (Heusel et al., 2017). CIDEr là càng cao càng tốt, và FID là càng thấp càng tốt.

4.2 MỞ RỘNG VỚI DỮ LIỆU

Trong phần này, chúng tôi đánh giá toàn diện hiệu suất của ITIT với các lượng dữ liệu cặp và không cặp khác nhau trên tập dữ liệu Shutterstock (Shutterstock, 2023) bao gồm 398M cặp hình ảnh-văn bản.

--- TRANG 7 ---
0.0 (1.2) 2.8 (4.0) 396.8 (398.0)
Dữ liệu bổ sung (Tổng cộng) (triệu) 14 16 18 20 22 CIDEr
100% 30% 0.3% 100% 100% ShutterStock

0.0 (1.2) 2.8 (4.0) 396.8 (398.0)
Dữ liệu bổ sung (Tổng cộng) (triệu) 19.0 19.5 20.0 20.5 21.0 FID 100%
30%
0.3% 100%
100%

0 (3) 11 (14) 111 (114)
Dữ liệu bổ sung (Tổng cộng) (triệu) 24 26 28 30 32 34 36 CIDEr
100% 21% 2.6%
100% 100% CC3M + WebLi

0 (3) 11 (14) 111 (114)
Dữ liệu bổ sung (Tổng cộng) (triệu) 12.1 12.2 12.3 12.4 12.5 12.6 12.7 12.8 FID 100%
21%
2.6% 100%
100%

% Cặp
Tổng dữ liệu Chỉ dữ liệu cặp bổ sung Dữ liệu không cặp bổ sung với hàm mất mát chu trình

Hình 5: Hiệu suất của ITIT mở rộng như thế nào với tổng lượng dữ liệu được sử dụng (trục x). Baseline (T2I + I2T) màu xanh được huấn luyện hoàn toàn với lượng dữ liệu cặp tăng dần. ITIT (cam) được huấn luyện với lượng dữ liệu không cặp tăng dần sử dụng hàm mất mát chu trình, trong khi giữ tổng lượng dữ liệu bằng nhau cho cả hai đường cong. Ví dụ, điểm ngoài cùng bên phải với Shutterstock sử dụng 1,2M cặp hình ảnh-văn bản và 396,8M mẫu không cặp (một nửa là hình ảnh không cặp và một nửa là văn bản không cặp) cho ITIT với hàm mất mát chu trình, và 398M cặp hình ảnh-văn bản cho baseline. Trái: dữ liệu Shutterstock làm cả dữ liệu cặp và không cặp. Phải: CC3M làm dữ liệu cặp, và các phần khác nhau của WebLI làm dữ liệu cặp/không cặp bổ sung.

Hình 4 phân tích cách hiệu suất của ITIT mở rộng với dữ liệu cặp. Chúng tôi huấn luyện một baseline chỉ với dữ liệu cặp, với tổng của các hàm mất mát trong Phương trình (1) và Phương trình (2). ITIT được huấn luyện với cùng dữ liệu cặp như baseline, và toàn bộ tập 398M hình ảnh và văn bản có trong Shutterstock làm dữ liệu không cặp. Thêm dữ liệu cặp giúp cả hai cài đặt, nhưng huấn luyện với dữ liệu không cặp cải thiện đáng kể hiệu suất của ITIT so với baseline trên cả sinh tạo chú thích hình ảnh và từ văn bản sang hình ảnh. Đáng chú ý, chỉ với 4M dữ liệu cặp và 398M dữ liệu không cặp, ITIT đạt được hiệu suất tương tự như huấn luyện với 398M dữ liệu cặp. Lưu ý rằng ITIT không sử dụng bất kỳ mẫu nào không có trong baseline được huấn luyện với 398M dữ liệu cặp, vì tất cả các mẫu đều từ Shutterstock. Do đó ITIT có thể hoạt động tương tự như một baseline với 100x ít hơn các cặp hình ảnh-văn bản, giảm đáng kể nỗ lực và chi phí cho việc huấn luyện thị giác-ngôn ngữ sinh tạo.

Tiếp theo, chúng tôi đánh giá cách hiệu suất của ITIT mở rộng so với tổng lượng dữ liệu được sử dụng. Chúng tôi đầu tiên huấn luyện một mô hình với 1,2M dữ liệu cặp hình ảnh-văn bản Shutterstock. Sau đó chúng tôi đánh giá hiệu quả của việc huấn luyện các mô hình trên việc thêm lượng dữ liệu cặp bổ sung tăng dần so với thêm lượng dữ liệu không cặp tăng dần với hàm mất mát chu trình, giữ tổng lượng dữ liệu giống nhau cho cả hai. Như mong đợi, chúng tôi thấy trong Hình 5 rằng hiệu suất mở rộng với dữ liệu cặp bổ sung. Tuy nhiên, đáng ngạc nhiên, dữ liệu không cặp bổ sung thể hiện khả năng mở rộng tương tự như dữ liệu cặp. Trên thực tế, chúng tôi có thể đạt được 19,2 FID và 21,0 CIDEr chỉ với 1,2M cặp và 396,8M ví dụ không cặp, rất cạnh tranh với 19,0 FID và 22,2 CIDEr chỉ sử dụng 398M ví dụ cặp. Thí nghiệm này do đó chứng minh rằng khi mở rộng dữ liệu huấn luyện, các nhà thực hành có thể dựa vào chỉ thêm các ví dụ không cặp sử dụng phương pháp của chúng tôi và đạt được hiệu suất tương tự như dữ liệu cặp mà không cần nỗ lực thủ công bổ sung để thu thập nó.

Chúng tôi lặp lại thí nghiệm trên trong một cài đặt thực tế hơn, nơi tập dữ liệu cặp quy mô nhỏ có thể chứa các cặp hình ảnh-văn bản chất lượng cao nhưng một tập dữ liệu cặp quy mô lớn có chất lượng thấp hơn nhiều. Để làm điều này, chúng tôi sử dụng CC3M chất lượng cao làm tập dữ liệu cặp, và WebLI lớn hơn nhiều làm tập dữ liệu không cặp chất lượng thấp. Như trước, chúng tôi bắt đầu với một mô hình được huấn luyện trên 3M ví dụ cặp (từ CC3M), và thêm dữ liệu huấn luyện bổ sung từ WebLI dưới dạng cặp (xanh) hoặc không cặp (cam). Như được hiển thị trong Hình 5, cặp phải, việc thêm các cặp hình ảnh-văn bản chất lượng thấp làm tổn hại nghiêm trọng hiệu suất chú thích hình ảnh cho trường hợp hoàn toàn cặp. Tuy nhiên, chế độ ITIT không bị ảnh hưởng bởi chất lượng thấp này và mở rộng tương tự như trước. Điều này chứng minh rằng phương pháp của chúng tôi mạnh mẽ với chất lượng dữ liệu thấp trong các tập dữ liệu lớn, và thực sự có thể được sử dụng để đạt được hiệu suất tốt hơn đáng kể trong các cài đặt khi dữ liệu cặp có mặt nhưng chất lượng thấp.

4.3 SO SÁNH VỚI CÔNG TRÌNH TRƯỚC ĐÂY

Trong Bảng 1, chúng tôi so sánh ITIT với các mô hình từ hình ảnh sang văn bản và từ văn bản sang hình ảnh tiên tiến trên benchmark MS-COCO thường được sử dụng. Như được hiển thị, tất cả các phương pháp SOTA đều phụ thuộc nhiều vào việc huấn luyện trên một bộ dữ liệu lớn của dữ liệu cặp hình ảnh-văn bản. Tuy nhiên, ITIT được huấn luyện chỉ với 3M ví dụ cặp

--- TRANG 8 ---
Bảng 1: So sánh định lượng với các mô hình từ văn bản sang hình ảnh và từ hình ảnh sang văn bản tiên tiến trên MS-COCO. Hiệu suất chú thích hình ảnh được đánh giá trên COCO Karpathy split, và FID sinh tạo từ văn bản sang hình ảnh được đánh giá trên 30K hình ảnh COCO. †biểu thị việc tái hiện của chúng tôi. Chúng tôi đánh dấu màu xanh lá cây các mô hình khác sử dụng lượng dữ liệu cặp tương đương. Lưu ý rằng mô hình GIT (CLIP) sử dụng bộ mã hóa CLIP (Radford et al., 2021) được huấn luyện trước với 400M cặp hình ảnh-văn bản.

Phương pháp | #tham số | #dữ liệu cặp | #dữ liệu không cặp | FID↓ | CIDEr ↑(zs) | CIDEr ↑(ft)
T2I
StableDiffusion (Rombach et al., 2022) | 800M | 400M | - | 12.60 | - | -
GLIDE (Nichol et al., 2021) | 5B | 250M | - | 12.24 | - | -
Make-A-Scene (Gafni et al., 2022) | 4B | 35M | - | 11.84 | - | -
DALL-E 2 (Ramesh et al., 2022) | 3.5B | 650M | - | 10.39 | - | -
PARTI (Yu et al., 2022b) | 750M | 5000M | - | 10.71 | - | -
Muse-512 (Chang et al., 2023) | 3B | 860M | - | 7.88 | - | -
Muse†(Chang et al., 2023) | 750M | 3M | - | 23.7 | - | -
I2T
BLIP (Li et al., 2022) | 446M | 129M | - | - | - | 136.7
SimVLM base(Wang et al., 2022b) | - | 1100M | 365M T | - | 24.0 | 134.8
SimVLM huge(Wang et al., 2022b) | ∼1.4B | 1100M | 365M T | - | 32.2 | 143.3
GIT (CLIP) (Wang et al., 2022a) | 681M | 800M | - | - | - | 144.8
GITB(scratch) (Wang et al., 2022a) | 129M | 10M | - | - | - | 89.0
T2I+I2T
CoBIT-Base (You et al., 2023) | 626M | 5200M | - | 10.35 | 43.0 | 135.4
CoBIT-Large (You et al., 2023) | 1091M | 5200M | - | 9.37 | 44.8 | 139.5
CM3Leon (Yu et al., 2023) | 7B | 340M | - | 4.88 | 61.6 | -
ITIT-B | 221M | 3M | 55M I+55M T | 13.4 | 32.1 | 103.5
ITIT-L | 487M | 3M | 55M I+55M T | 12.0 | 35.1 | 116.4
ITIT-H | 868M | 3M | 55M I+55M T | 10.4 | 38.2 | 125.3

(CC3M), và thêm 55M ví dụ hình ảnh và văn bản không cặp mỗi loại (WebLI). Bất chấp điều này, nó đánh bại nhiều phương pháp khác được huấn luyện trên nhiều dữ liệu hơn cho sinh tạo từ văn bản sang hình ảnh (FID). Đối với I2T, nó đánh bại các phương pháp sử dụng lượng dữ liệu tương đương (được đánh dấu màu xanh lá cây), và đạt được hiệu suất cạnh tranh với các phương pháp SOTA khác. Chúng tôi thấy rằng dữ liệu huấn luyện trước (cả hỗn hợp và kích thước) cũng tạo ra sự khác biệt đối với điểm CIDEr. Ví dụ, GIT (Wang et al., 2022a) chỉ đạt được 89.0 hiệu suất CIDEr fine-tuning trên COCO captions khi được huấn luyện từ đầu với 10M cặp hình ảnh-văn bản, rất xa với hiệu suất được báo cáo của nó (144.8) khi được huấn luyện với 800M cặp hình ảnh-văn bản. Cách tiếp cận của chúng tôi trực giao với các cân nhắc hỗn hợp tập dữ liệu, và chúng tôi tin rằng việc mở rộng kích thước và đa dạng dữ liệu sẽ cải thiện thêm điểm FID và CIDEr. Chúng tôi để dành điều này cho công việc tương lai.

4.4 PHÂN TÍCH LOẠI BỎ

Trong Bảng 2, chúng tôi phân tích loại bỏ hiệu quả của bốn thành phần của ITIT: T2I, I2T, T2I2T, và I2T2I. Như được hiển thị trong các hàng 1-3, việc kết hợp huấn luyện T2I và I2T trong khung của chúng tôi đã cải thiện hiệu suất chú thích hình ảnh. Điều này có thể là do việc huấn luyện T2I làm giảm vấn đề overfitting của huấn luyện I2T, như được hiển thị trong GIT (Wang et al., 2022a).

Như trước (Hình 5), chúng tôi có thể thấy trong hàng 4 rằng việc kết hợp CC3M và WebLI cải thiện sinh tạo từ văn bản sang hình ảnh, nhưng làm tổn hại hiệu suất chú thích hình ảnh. Điều này là do chất lượng ghép cặp hình ảnh-văn bản thấp hơn của WebLI so với CC3M. Các hàng còn lại chứng minh rằng hàm mất mát chu trình làm giảm điều này bằng cách sử dụng WebLI làm dữ liệu không cặp và không phụ thuộc vào chất lượng ghép cặp hình ảnh-văn bản của nó. Do đó nó có thể khái quát hóa hơn đối với các tập dữ liệu hình ảnh-văn bản quy mô lớn.

Tiếp theo, các hàng 5-7 là các baseline ngây thơ để sử dụng dữ liệu hình ảnh hoặc văn bản không cặp trong quá trình huấn luyện thị giác-ngôn ngữ sinh tạo. Chúng tôi có thể đơn giản thực hiện huấn luyện tự hồi quy văn bản-sang-văn bản (T2T) mà không có điều kiện trên hình ảnh, đã được khám phá trong một số công trình trước đây (Wang et al. (2022b)). Tương tự, chúng tôi có thể thực hiện huấn luyện tái tạo hình ảnh-sang-hình ảnh (I2I) mà không có điều kiện trên văn bản. Các baseline như vậy cải thiện hiệu suất so với không sử dụng bất kỳ dữ liệu cặp nào (hàng 3).

Chúng tôi xem xét một phân tích loại bỏ nơi gradient của hàm mất mát tính nhất quán chu trình được lan truyền ngược đến bước argmax. Do đó, chỉ một nửa chu trình được huấn luyện. Trên thực tế, điều này tương đương với việc đầu tiên tổng hợp một bản tương ứng hình ảnh từ văn bản không cặp và sau đó sử dụng nó như một cặp hình ảnh-văn bản giả để huấn luyện mô hình I2T (tương tự cho T2I). Các hàng 8-10 cho thấy rằng hàm mất mát nửa chu trình đạt được hiệu suất tốt hơn nhiều so với các baseline không chu trình.

--- TRANG 9 ---
Bảng 2: So sánh định lượng giữa các biến thể khác nhau của ITIT trên MS-COCO. Tất cả thí nghiệm sử dụng ITIT B được huấn luyện với 500K bước. Chúng tôi lấy 50% dữ liệu WebLI và sử dụng hình ảnh làm dữ liệu hình ảnh không cặp của chúng tôi, và 50% dữ liệu WebLI còn lại và sử dụng văn bản làm dữ liệu văn bản không cặp của chúng tôi.

T2I | I2T | T2I2T | I2T2I | dữ liệu cặp | văn bản không cặp | hình ảnh không cặp | FID↓ | CIDEr ↑
Chỉ dữ liệu cặp
1 ✓ | ✗ | ✗ | ✗ | CC3M | ✗ | ✗ | 15.5 | N/A
2 ✗ | ✓ | ✗ | ✗ | CC3M | ✗ | ✗ | N/A | 19.0
3 ✓ | ✓ | ✗ | ✗ | CC3M | ✗ | ✗ | 15.7 | 23.5
4 ✓ | ✓ | ✗ | ✗ | CC3M+WebLI | ✗ | ✗ | 14.2 | 20.7
Dữ liệu cặp+không cặp, không chu trình
5 ✓ | ✓ | T2T | ✗ | CC3M | 50% WebLI | ✗ | 15.1 | 26.0
6 ✓ | ✓ | ✗ | I2I | CC3M | ✗ | 50% WebLI | 15.9 | 24.2
7 ✓ | ✓ | T2T | I2I | CC3M | 50% WebLI | 50% WebLI | 15.6 | 28.5
Dữ liệu cặp+không cặp, nửa chu trình
8 ✓ | ✓ | Nửa | ✗ | CC3M | 50% WebLI | ✗ | 14.8 | 27.6
9 ✓ | ✓ | ✗ | Nửa | CC3M | ✗ | 50% WebLI | 14.7 | 24.8
10 ✓ | ✓ | Nửa | Nửa | CC3M | 50% WebLI | 50% WebLI | 14.5 | 30.5
Dữ liệu cặp+không cặp, chu trình đầy đủ
11 ✓ | ✓ | Đầy đủ | ✗ | CC3M | 50% WebLI | ✗ | 14.6 | 28.4
12 ✓ | ✓ | ✗ | Đầy đủ | CC3M | ✗ | 50% WebLI | 14.6 | 26.3
13 ✓ | ✓ | Đầy đủ | Đầy đủ | CC3M | CC3M | CC3M | 15.4 | 24.4
14 ✓ | ✓ | Đầy đủ | Đầy đủ | CC3M | 50% WebLI | 50% WebLI | 14.3 | 31.1

Hình 6: Sinh tạo lặp lại văn bản sang hình ảnh sang văn bản và tiếp tục. Với ITIT, các kết quả được tạo ra nhất quán hơn so với kết quả từ một mô hình được huấn luyện mà không có hàm mất mát tính nhất quán chu trình.

Cuối cùng, các hàng 11-14 cho thấy hiệu suất của huấn luyện ITIT chu trình đầy đủ. Mặc dù T2I2T ưu tiên chú thích hình ảnh trong khi I2T2I ưu tiên sinh tạo từ văn bản sang hình ảnh, cả hai đều cho thấy cải thiện đáng kể trong sinh tạo từ văn bản sang hình ảnh và chú thích hình ảnh. Hơn nữa, hàng 14 chứng minh rằng hai hàm mất mát chu trình như vậy có thể được kết hợp để cải thiện hiệu suất hơn nữa. Ngoài ra, chúng tôi có thể thấy rằng hàm mất mát chu trình đầy đủ đánh bại các baseline nửa chu trình (hàng 8-10), chứng minh hiệu quả của bước ước lượng gradient.

Cuối cùng, chúng tôi thấy bằng cách so sánh hàng 3 và 13 rằng hàm mất mát tính nhất quán chu trình có thể cải thiện hiệu suất một chút ngay cả khi không có dữ liệu bổ sung nào. Chúng tôi tin rằng điều này là do nó buộc căn chỉnh hình ảnh-văn bản tốt hơn. Tuy nhiên, việc so sánh hàng 13 và 14 cho thấy rằng những cải thiện lớn trong cả sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản chủ yếu xuất phát từ việc sử dụng dữ liệu không cặp bổ sung.

4.5 KẾT QUẢ SINH TẠO CHU TRÌNH

Với một khung có thể thực hiện cả từ hình ảnh sang văn bản và từ văn bản sang hình ảnh, chúng tôi có thể dễ dàng thực hiện sinh tạo chu trình, như được hiển thị trong Hình 6. Với huấn luyện ITIT, việc sinh tạo chu trình thường giữ nguyên ngữ nghĩa giống như lời nhắc văn bản đầu vào. Mặt khác, mà không có huấn luyện tính nhất quán chu trình, việc sinh tạo chu trình bỏ lỡ ngữ nghĩa "xanh" sau chu trình đầu tiên. Điều này chứng minh rằng huấn luyện tính nhất quán chu trình của chúng tôi không chỉ cho phép tích hợp dữ liệu hình ảnh và văn bản không cặp vào huấn luyện thị giác-ngôn ngữ sinh tạo, mà còn cải thiện căn chỉnh hình ảnh-văn bản cho cả sinh tạo từ hình ảnh sang văn bản và từ văn bản sang hình ảnh. Chúng tôi bao gồm một số kết quả sinh tạo hình ảnh và văn bản trong Phụ lục A (Hình 1 đến 4).

--- TRANG 10 ---
5 THẢO LUẬN

Chúng tôi đề xuất ITIT, một sơ đồ huấn luyện mới lần đầu tiên kết hợp hình ảnh và văn bản không cặp vào huấn luyện thị giác-ngôn ngữ sinh tạo. Thông qua các phân tích loại bỏ mở rộng, chúng tôi chứng minh hiệu quả của cả chu trình T2I2T và chu trình I2T2I trong việc cải thiện hiệu suất sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản. Kết quả là, ITIT đạt được hiệu suất cạnh tranh với các mô hình sinh tạo thị giác-ngôn ngữ tiên tiến, nhưng chỉ với 3 triệu mẫu cặp hình ảnh-văn bản. Phương pháp của chúng tôi có thể được sử dụng ngay cả khi dữ liệu cặp hình ảnh-văn bản có mặt, và đặc biệt hữu ích khi chất lượng ghép cặp thấp. Các hướng tương lai bao gồm mở rộng ITIT đến dữ liệu hình ảnh và văn bản không cặp lớn hơn và kích thước mô hình, và sử dụng các tập dữ liệu đa dạng hơn.

--- TRANG 11 ---
TÀI LIỆU THAM KHẢO

Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022.

Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315–11325, 2022.

Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.

Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023.

Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021.

Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822–19835, 2021.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. on Learning Representations (ICLR), 2021.

Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873–12883, 2021.

Yang Feng, Lin Ma, Wei Liu, and Jiebo Luo. Unsupervised image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4125–4134, 2019.

Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Conference on Computer Vision, pp. 89–106. Springer, 2022.

Clément Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. CoRR, abs/1609.03677, 2016. URL http://arxiv.org/abs/1609.03677.

Satya Krishna Gorti and Jeremy Ma. Text-to-image-to-text translation using cycle consistent adversarial networks. arXiv preprint arXiv:1808.04538, 2018.

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16000–16009, June 2022.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.

Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 4904–4916. PMLR, 2021.

--- TRANG 12 ---
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128–3137, 2015.

Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888–12900. PMLR, 2022.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.

Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2142–2152, 2023b.

Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.

Nico Messikommer, Daniel Gehrig, Mathias Gehrig, and Davide Scaramuzza. Bridging the gap between events and frames through unsupervised domain adaptation. IEEE Robotics and Automation Letters, 7(2):3515–3522, apr 2022. doi: 10.1109/lra.2022.3145053. URL https://doi.org/10.1109%2Flra.2022.3145053.

Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 8821–8831. PMLR, 2021.

Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.

Adam Roberts, Colin Raffel, Katherine Lee, Michael Matena, Noam Shazeer, Peter J. Liu, Sharan Narang, Wei Li, and Yanqi Zhou. Exploring the limits of transfer learning with a unified text-to-text transformer. Technical report, Google, 2019.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.

Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22500–22510, 2023.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022.

Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278–25294, 2022.

SentencePiece. Unsupervised text tokenizer for neural network-based text generation. https://github.com/google/sentencepiece, 2023.

Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556–2565, 2018.

--- TRANG 13 ---
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.

Shutterstock. Shutterstock. https://www.shutterstock.com/, 2023.

Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pp. 843–852, 2017.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016.

Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4566–4575, 2015.

Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022a.

Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision, 2022b.

Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu. Cobit: A contrastive bi-directional image-text generation model, 2023.

Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021.

Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022a.

Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022b.

Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.

Christopher Zach, Manfred Klopschitz, and Marc Pollefeys. Disambiguating visual relations using loop constraints. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1426–1433, 2010. doi: 10.1109/CVPR.2010.5539801.

Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104–12113, 2022.

Tinghui Zhou, Philipp Krähenbühl, Mathieu Aubry, Qixing Huang, and Alexei A. Efros. Learning dense correspondence via 3d-guided cycle consistency. CoRR, abs/1604.05383, 2016. URL http://arxiv.org/abs/1604.05383.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks, 2017.

--- TRANG 14 ---
A KẾT QUẢ ĐỊNH TÍNH BỔ SUNG

Sinh tạo Hình ảnh sang Văn bản và Văn bản sang Hình ảnh. Trong Hình 7, Hình 8 và Hình 9, chúng tôi cho thấy hiệu suất của ITIT về sinh tạo từ văn bản sang hình ảnh và chú thích hình ảnh. Mô hình sử dụng ViT-H làm backbone và được huấn luyện với CC3M làm dữ liệu cặp và WebLI làm dữ liệu không cặp trong 1,5M bước. Như được hiển thị trong kết quả, mô hình của chúng tôi có thể tạo ra hình ảnh thực tế từ lời nhắc văn bản, và cũng có thể tạo ra chú thích chính xác cho hình ảnh đầu vào.

Sinh tạo Chu trình. Chúng tôi bao gồm thêm kết quả sinh tạo chu trình trong Hình 10. Với ITIT, các kết quả được tạo ra khá nhất quán với đầu vào gốc. Mà không có hàm mất mát tính nhất quán chu trình, văn bản/hình ảnh được tạo ra có thể dễ dàng bỏ lỡ một số thông tin quan trọng trong hình ảnh/văn bản đầu vào, khiến kết quả sinh tạo chu trình phân kỳ khỏi đầu vào gốc. Điều này chứng minh rằng hàm mất mát tính nhất quán chu trình được đề xuất buộc sinh tạo chu trình nhất quán hơn và cải thiện căn chỉnh đầu vào-đầu ra cho cả sinh tạo từ văn bản sang hình ảnh và từ hình ảnh sang văn bản.

B CHI TIẾT TRIỂN KHAI

Trong phần này, chúng tôi bao gồm chi tiết triển khai của chúng tôi, bao gồm siêu tham số, kiến trúc mô hình, và mô hình huấn luyện. Chúng tôi cũng sẽ phát hành mã của chúng tôi để tái tạo tốt hơn.

Tokenizer và Detokenizer Hình ảnh. Chúng tôi sử dụng bộ mã hóa VQGAN dựa trên CNN để mã hóa hình ảnh đầu vào 256x256 thành bản đồ đặc trưng 16x16. Quantizer sau đó lượng tử hóa mỗi pixel của đầu ra bộ mã hóa sử dụng một codebook với 8192 mục. Detokenizer hoạt động trên các token rời rạc 16x16 và tái tạo hình ảnh 256x256. Tokenizer và detokenizer VQGAN của chúng tôi được huấn luyện trên tập dữ liệu WebLI với kích thước batch 256.

Kiến trúc ViT. Sau tokenizer, độ dài chuỗi latent hình ảnh trở thành 256. Vì chúng tôi luôn sử dụng tỷ lệ che lớn hơn 50%, chúng tôi bỏ 128 token hình ảnh bị che từ đầu vào. Các token sau đó được nhúng với một lớp nhúng và nối với embedding văn bản từ T5-XXL với độ dài 64. Sau đó chúng tôi sử dụng Transformer bộ mã hóa hình ảnh-văn bản với kiến trúc ViT tiêu chuẩn Dosovitskiy et al. (2021), bao gồm một ngăn xếp các khối Transformer Vaswani et al. (2017), trong đó mỗi khối bao gồm một khối multi-head self-attention và một khối MLP.

Bộ giải mã văn bản tương tự như bộ được sử dụng trong GIT (Wang et al., 2022a), bao gồm 6 khối Transformer với causal attention mask. Attention mask đảm bảo mỗi token văn bản chỉ có thể chú ý đến các token văn bản trước đó và các token hình ảnh.

Bộ giải mã hình ảnh tương tự như bộ được sử dụng trong MAGE (Li et al., 2023b), bao gồm 8 khối Transformer với self-attention. Đầu vào cho bộ giải mã hình ảnh được đệm với các token hình ảnh bị che đã bỏ trước đó. Bằng cách này, chúng tôi tiết kiệm rất nhiều tính toán trong bộ mã hóa hình ảnh-văn bản.

Huấn luyện Thị giác-Ngôn ngữ. Vui lòng tham khảo Bảng 3 cho cài đặt huấn luyện thị giác-ngôn ngữ mặc định của chúng tôi. Với bước tổng hợp trực tuyến, ITIT yêu cầu ~2x thời gian huấn luyện so với huấn luyện I2T và T2I không chu trình tiêu chuẩn. Huấn luyện ViT-H của chúng tôi với 1,5M bước mất ~10,9 ngày trên 512 TPUv3.

Thang đo gradient cho hàm mất mát I2T: Tương tự như GIT (Wang et al. (2022a)), chúng tôi thấy rằng bộ mã hóa hình ảnh-văn bản nên nhận gradient nhỏ hơn so với bộ giải mã văn bản. Do đó, chúng tôi thu nhỏ gradient được lan truyền ngược từ bộ giải mã văn bản đến bộ mã hóa hình ảnh-văn bản bằng 0,1. Trong thực tế, điều này được triển khai đơn giản bằng z = 0,1z + stopgrad(0,9z) trong đó z biểu thị đầu ra của bộ mã hóa hình ảnh-văn bản.

Huấn luyện Chu trình. Trong quá trình huấn luyện chu trình, bộ mã hóa hình ảnh-văn bản trải qua forward pass hai lần. Chúng tôi thấy rằng nếu chúng tôi lan truyền ngược gradient về nó hai lần, việc huấn luyện trở nên không ổn định. Do đó, chúng tôi dừng gradient sau forward pass đầu tiên của bộ mã hóa hình ảnh-văn bản, điều này ổn định đáng kể việc huấn luyện chu trình.

Ước lượng Gradient. Chúng tôi sử dụng Gumbel softmax (Jang et al., 2016) với cường độ 1,0 cho ước lượng gradient trong chu trình T2I2T, và sử dụng straight-through softmax cho ước lượng gradient trong chu trình I2T2I. Chúng tôi thấy rằng việc sử dụng Gumbel softmax trong chu trình T2I2T cải thiện CIDEr zero-shot 0,3, trong khi việc sử dụng Gumbel softmax trong chu trình I2T2I không mang lại cải thiện.

Suy luận I2T. Để tạo ra chú thích cho một hình ảnh, chúng tôi đầu tiên trích xuất các đặc trưng thị giác từ các token hình ảnh (và các token văn bản trống) sử dụng bộ mã hóa hình ảnh-văn bản. Sau đó chúng tôi tạo ra các token một cách tự hồi quy, có điều kiện trên các đặc trưng thị giác và các token đã tạo ra trước đó. Theo GIT (Wang et al. (2022a)), chúng tôi sử dụng beam search để tạo ra các token với kích thước beam được đặt là 4.

Suy luận T2I. Để cho phép classifier-free guidance (Ho & Salimans, 2022) cho việc sinh tạo, khi thực hiện huấn luyện I2T với embedding văn bản trống làm đầu vào, chúng tôi cũng huấn luyện bộ giải mã hình ảnh để tái tạo các token bị thiếu. Trong những trường hợp như vậy, huấn luyện T2I trở thành huấn luyện hình ảnh-sang-hình ảnh (I2I), là tái tạo các token gốc từ các token không bị che.

Tương tự như Muse (Chang et al. (2023)), chúng tôi sử dụng giải mã song song với classifier-free guidance để tạo ra hình ảnh từ lời nhắc văn bản. Chúng tôi bắt đầu với các token hình ảnh hoàn toàn bị che, và nối chúng với các token lời nhắc văn bản. Tại mỗi lần lặp, bộ giải mã dự đoán một logit có điều kiện lc và một logit không điều kiện lu cho mỗi token bị che. Các logit cuối cùng lg được hình thành bằng cách di chuyển ra khỏi lu bằng thang đo hướng dẫn τ: lg = (1 + τ)lc − τlu. Sau đó chúng tôi lấy mẫu mỗi token sử dụng categorical sampling. Sau đó, điểm dự đoán tương ứng của mỗi token cộng với nhiễu được lấy mẫu từ phân phối Gumbel ngẫu nhiên nhân với nhiệt độ t được sử dụng làm điểm "tin cậy" cho biết niềm tin của mô hình về mỗi dự đoán token. Sau đó chúng tôi lấy mẫu các token top-k với xác suất dự đoán cao nhất, và thay thế các token bị che tương ứng với các token dự đoán được lấy mẫu này. Số lượng token bị che sẽ được thay thế trong mỗi lần lặp tuân theo hàm cosine (Chang et al., 2022). Chúng tôi sử dụng tổng cộng 24 bước để tạo ra một hình ảnh. Đối với mỗi mô hình, chúng tôi quét nhiệt độ t và thang đo classifier-free guidance τ cho FID tối ưu. Trong thực tế, chúng tôi thấy t = 32 và τ = 2,0 phục vụ như nhiệt độ và thang đo guidance gần tối ưu.

C TẬP DỮ LIỆU

Chúng tôi sử dụng ba tập dữ liệu trong bài báo chính của chúng tôi: CC3M, WebLI, và Shutterstock. CC3M chứa 3,3M cặp hình ảnh-văn bản. Các mô tả thô được thu thập từ thuộc tính alt-text HTML liên kết với hình ảnh web và sau đó được lọc để tạo thành tập dữ liệu cuối cùng. WebLI (Web Language Image) chứa 111 triệu hình ảnh từ web công khai với các nhãn alt-text liên kết với hình ảnh trong đó chất lượng ghép cặp hình ảnh-văn bản thấp hơn nhiều so với CC3M, vì không có việc lọc nào được áp dụng cho dữ liệu alt-text. Shutterstock chứa 398 triệu hình ảnh được gán nhãn bởi các chú thích viên con người, điều này phát sinh chi phí và nỗ lực đáng kể.

--- TRANG 15 ---
Bảng 3: Cài đặt Huấn luyện Trước.

config | giá trị
optimizer | Adafactor (Shazeer & Stern, 2018)
tỷ lệ học đỉnh | 1e-4
weight decay | 0.045
momentum optimizer | β1, β2 = 0.9, 0.96
kích thước batch T2I | 512
kích thước batch I2T/I2I | 512
kích thước batch T2I2T | 512
kích thước batch I2T2I | 512
lịch trình tỷ lệ học | cosine decay (Loshchilov & Hutter, 2016)
các bước khởi động | 5000
các bước huấn luyện | 1.5M
gradient clip | 3.0
label smoothing (Szegedy et al., 2016) | 0.1
dropout | 0.1
tỷ lệ che hình ảnh tối thiểu | 0.5
tỷ lệ che hình ảnh tối đa | 1.0 (T2I), 0.75 (I2T)
chế độ tỷ lệ che hình ảnh | 0.75
std tỷ lệ che hình ảnh | 0.25

--- TRANG 16 ---
Nửa chiếc bánh trắng với chocolate trên đỉnh

Một phòng tắm đầy đủ với giỏ giặt đồ bằng mây

Một con mèo trắng nhỏ trên một cái bát lớn

Một máy bay chở khách lớn bay qua không trung

Một xe buýt đỏ lớn bên cạnh một con phố thành phố

Một chiếc xe máy đỗ trên lối đi bằng cỏ

Một cảnh quan mùa thu với một ngôi nhà bên cạnh hồ

Một đĩa thức ăn với mì, cà rốt và bông cải xanh

Cây cối ngồi gần cửa sổ bên trong ngôi nhà

Hình 7: Kết quả sinh tạo từ văn bản sang hình ảnh của ITIT.

--- TRANG 17 ---
Baseline | ITIT
người và tôi trên bãi biển | người với ván lướt sóng trên bãi biển
người chuẩn bị thức ăn trong bếp | chuẩn bị thức ăn trong bếp
cận cảnh con bò | con bò trên trang trại
tv trong phòng khách | kệ tv trong phòng

Baseline | ITIT
một người vô gia cư ngủ trên ghế công viên | một người ngồi trên ghế
ảnh đen trắng của tháp đồng hồ | ảnh đen trắng của tháp đồng hồ
ảnh của em bé ăn dưa hấu | em bé dễ thương với bánh sinh nhật
lựa chọn thức ăn từ thực đơn | ảnh của bữa ăn

Baseline | ITIT
bàn làm việc trong phòng khách | bàn làm việc của tôi tại văn phòng
một đứa trẻ ngủ trên giường | người ngủ trên giường
người bán chuối trên đường phố | người bán hàng bán chuối tại chợ
phòng tắm: ý tưởng phòng tắm nhỏ với buồng tắm | ví dụ về thiết kế phòng tắm thời thượng

Baseline | ITIT
một chiếc máy bay trên bầu trời | một chiếc máy bay trên bầu trời
một chiếc pizza với mozzarella và cà chua cherry | một bức ảnh pizza
một người trượt ván thực hiện động tác trên ván trượt | một thanh niên trượt ván trong công viên
một người phụ nữ khiêu vũ trong trang phục truyền thống | một cô gái trong trang phục truyền thống

Hình 8: Hiệu suất sinh tạo từ hình ảnh sang văn bản (zero-shot trên COCO Captions). Baseline được huấn luyện chỉ trên CC3M. ITIT được huấn luyện trên CC3M làm dữ liệu cặp và WebLI làm dữ liệu không cặp. Chúng tôi làm mờ khuôn mặt của người trong hình ảnh.

--- TRANG 18 ---
một phòng khách chứa đầy đồ nội thất và một TV màn hình phẳng | một người mặc đồng phục bóng chày cầm gậy
một con mèo đứng dưới ô trong cỏ | một nhóm người thả diều trong ngày nhiều mây

một cặp ngựa vằn đứng cạnh nhau | một con mèo trắng ngồi trên bồn rửa
một nhóm người đi bộ dọc lối đi bên cạnh một vùng nước | một con hươu cao cổ đang đứng trong chuồng sở thú

một chiếc điện thoại di động ngồi trên một tờ báo | một người phụ nữ mặc váy xanh cầm vợt tennis
một phòng tắm với toilet, bồn rửa và cửa sổ | một vòi cứu hỏa trên phố thành phố vào ban đêm

một người đi bộ dọc phố cầm điện thoại di động | một chiếc xe máy đỏ đỗ bên lề đường
một người trượt ván trên ray | một tủ trưng bày chứa đầy các loại rau khác nhau

một nhóm gấu teddy ngồi trên bàn | một nhóm thuyền neo đậu trong nước
một người trượt ván lên một bên dốc | một chiếc đồng hồ trên cột phía trước tòa nhà

Hình 9: Kết quả sinh tạo từ hình ảnh sang văn bản của ITIT (fine-tuned trên COCO Captions).

--- TRANG 19 ---
ITIT
Không có hàm mất mát chu trình

một chiếc xe đạp hồng dựa vào cây | xe đạp hồng dựa vào cây | một chiếc xe đạp hồng dựa vào cây | xe đạp hồng dựa vào cây | xe đạp hồng dựa vào cây

một chiếc xe đạp hồng dựa vào cây | xe đạp hồng dựa vào cây | một chiếc xe đạp hồng dựa vào tường | xe đạp hồng | hình minh họa vector của xe đạp

ITIT
Không có hàm mất mát chu trình

miếng bánh chocolate trên đĩa màu vàng | lát bánh chocolate trên đĩa màu vàng | một lát bánh chocolate trên đĩa màu vàng | một lát bánh chocolate trên đĩa màu vàng | một lát bánh chocolate trên đĩa màu vàng

miếng bánh chocolate trên đĩa màu vàng | miếng bánh chocolate trên nền trắng | bánh chocolate cô lập trên nền trắng | bánh chocolate cô lập trên nền trắng | bánh trên nền trắng

ITIT
Không có hàm mất mát chu trình

một chiếc pizza và một tách cà phê trên bàn | pizza với một tách cà phê | pizza với một tách cà phê | cà phê và một lát pizza | cà phê và một lát pizza

một chiếc pizza và một tách cà phê trên bàn | pizza | pizza từ trên xuống - pizza cay | pizza từ trên xuống - pizza cay | miếng pizza trên nền trắng

Hình 10: Thêm kết quả sinh tạo chu trình có hoặc không có hàm mất mát tính nhất quán chu trình.
