# 2209.15176.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2209.15176.pdf
# Kích thước tệp: 598929 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Cơ chế Chú ý Thích ứng Thưa thớt và Đơn điệu cho
Nhận dạng Giọng nói Tự động dựa trên Transformer
Chendong Zhaoy?z, Jianzong Wangy, Wenqi Weiy, Xiaoyang Quy, Haoqian Wang?, Jing Xiaoy
yPing An Technology (Shenzhen) Co., Ltd., Shenzhen, China
?The Shenzhen International Graduate School, Tsinghua University, China
Tóm tắt —Mô hình kiến trúc Transformer, dựa trên cơ chế tự
chú ý và đa đầu chú ý, đã đạt được thành công đáng kể trong
Nhận dạng Giọng nói Tự động (ASR) đầu cuối offline. Tuy nhiên,
cơ chế tự chú ý và đa đầu chú ý không thể dễ dàng được áp dụng
cho ASR streaming hoặc online. Đối với cơ chế tự chú ý trong
Transformer ASR, cơ chế chú ý dựa trên hàm chuẩn hóa softmax
khiến việc làm nổi bật thông tin quan trọng trong giọng nói trở
nên không thể. Đối với cơ chế đa đầu chú ý trong Transformer
ASR, việc mô hình hóa việc căn chỉnh đơn điệu trong các đầu
khác nhau không dễ dàng. Để khắc phục hai giới hạn này, chúng
tôi tích hợp cơ chế chú ý thưa thớt và chú ý đơn điệu vào ASR
dựa trên Transformer. Cơ chế thưa thớt giới thiệu một sơ đồ
thưa thớt học được để cho phép mỗi cấu trúc tự chú ý phù hợp
tốt hơn với đầu tương ứng. Cơ chế chú ý đơn điệu triển khai
regularization để loại bỏ các đầu dư thừa cho cấu trúc chú ý
đa đầu. Các thí nghiệm cho thấy phương pháp của chúng tôi có
thể cải thiện hiệu quả cơ chế chú ý trên các benchmark được
sử dụng rộng rãi của nhận dạng giọng nói.
Từ khóa chỉ mục —Nhận dạng Giọng nói Tự động, Chú ý
Thưa thớt, Chú ý Đơn điệu, Tự Chú ý.

I. GIỚI THIỆU
Nhận dạng giọng nói tự động (ASR) đã được khám phá kỹ
lưỡng kể từ khi nó được nhận thức lần đầu tiên. ASR đầu cuối
đã chứng kiến những cải tiến lớn trong những năm gần đây.
Thành công to lớn của các mô hình đầu cuối cho ASR chủ yếu
nên được quy cho một số mô hình tiên tiến, chẳng hạn như
Phân loại Thời gian Kết nối (CTC) [1], Transducer Mạng Nơ-
ron Hồi quy (RNN-T) [2], và ASR dựa trên chú ý [3]–[5].
Kiến trúc Transformer đã cho thấy hiệu suất đầy hứa hẹn cho
ASR đầu cuối [6]–[9], kết hợp mô hình âm thanh, mô hình
ngôn ngữ, và cơ chế căn chỉnh âm thanh-văn bản. Khác với
mạng nơ-ron hồi quy, Transformer có thể thực hiện huấn luyện
song song rất tốt vì nó loại bỏ cấu trúc hồi quy có bản chất
tuần tự vốn có [10]–[13]. Hơn nữa, cơ chế chú ý đa đầu trong
Transformer có thể cải thiện đáng kể hiệu suất của mạng vì
nó có thể đạt được embedding động và nắm bắt các cấu trúc
nội tại mạnh mẽ [14]–[17]. Tuy nhiên, mặc dù Transformer
cho thấy sức mạnh biểu đạt cho các tác vụ Xử lý Ngôn ngữ
Tự nhiên (NLP), vẫn còn chỗ để khai thác nó trong các tác
vụ giọng nói.
Các phương pháp Transformer ASR [6], [7] khai thác cách
tiếp cận tự chú ý, trong đó mỗi lớp tự chú ý trong cả encoder
và decoder sử dụng cấu trúc đa đầu để trích xuất các biểu
diễn đầu vào khác nhau. Trong khi đã được nghiên cứu rằng
một số đầu chú ý là vô dụng trong các tác vụ khác, chúng tôi
trước đây đã quan sát thấy hiện tượng này trong Transformer
ASR cũng vậy, trong đó hàm chú ý tương tự như một ánh xạ
đồng nhất. Vấn đề này thúc đẩy chúng tôi tìm ra và giải quyết
các tính toán dư thừa trong cơ chế tự chú ý như vậy. Vì vậy
trong công việc này, chúng tôi đề xuất loại bỏ các thông tin
dư thừa mà không làm giảm hiệu suất tổng thể. Đối với mô
hình ASR dựa trên Transformer, một vấn đề cơ bản là cơ chế
chú ý dựa trên hàm chuẩn hóa softmax khiến việc làm nổi bật
thông tin quan trọng trong giọng nói trở nên không thể. Trong
cơ chế chú ý, softmax thường được sử dụng để tính toán trọng
số chú ý, và đầu ra của nó luôn dày đặc và không âm [18]–[20].
Chú ý dày đặc có nghĩa là tất cả các đầu vào luôn đóng góp
một chút vào quyết định, dẫn đến lãng phí chú ý. Bên cạnh
đó, phân phối điểm chú ý trở nên phẳng khi độ dài đầu vào
tăng. Do đó, khó tập trung vào thông tin quan trọng của giọng
nói, đặc biệt là đối với giọng nói dài. Xét rằng chú ý dày đặc
không thể làm nổi bật thông tin quan trọng trong giọng nói dài,
chúng tôi cố gắng sử dụng biến đổi thưa thớt để đạt được phân
phối chú ý thưa thớt. Lấy cảm hứng từ [21], chúng tôi lần đầu
tiên áp dụng α-entmax thích ứng vào tác vụ ASR. Cụ thể, chúng
tôi thay thế softmax trong mỗi cơ chế tự chú ý bằng α-entmax,
và duy trì khả năng học của α.
Đối với mô hình ASR dựa trên chú ý, một thách thức cơ bản
khác là việc căn chỉnh chính xác giữa giọng nói đầu vào và
văn bản đầu ra [22]. Tính đơn điệu có thể giữ cho việc biên
hóa việc căn chỉnh giữa đầu ra văn bản và đầu vào giọng nói
có thể xử lý được. Nhưng, việc căn chỉnh đa đầu đơn điệu
không thể làm cho tất cả các đầu đóng góp vào dự đoán cuối
cùng. Chúng tôi đề xuất một cơ chế chú ý đa đầu đơn điệu
thích ứng với ràng buộc regularization. Trong các thí nghiệm
của chúng tôi, chúng tôi tìm thấy một số kỹ thuật làm nổi bật
điểm chú ý của các phần quan trọng của giọng nói, mang lại
độ chính xác căn chỉnh tốt hơn. Trong thực tế, việc tìm kiếm
hàm tốt nhất tốn thời gian. Để giải quyết vấn đề này, chúng
tôi giới thiệu một cách tiếp cận dựa trên chú ý thích ứng tự
động. Góp phần vào việc căn chỉnh như vậy, mô hình được
huấn luyện tập trung và chính xác hơn về các chú ý đa dạng.
Bằng cách này, nó có thể thích ứng loại bỏ các đầu dư thừa
và để mỗi đầu học việc căn chỉnh một cách phù hợp.
Tóm lại, các đóng góp của chúng tôi như sau. Thứ nhất, chúng
tôi thống nhất cơ chế chú ý thưa thớt và chú ý đơn điệu vào
các tác vụ ASR. Thứ hai, chúng tôi đề xuất một phương pháp
thích ứng để có được một tham số duy nhất cho mỗi cơ chế
tự chú ý trong Transformer. Thứ ba, chúng tôi triển khai một
sơ đồ căn chỉnh đa đầu đơn điệu được điều chỉnh cho việc căn
chỉnh âm thanh-văn bản.

--- TRANG 2 ---
II. CÁC CÔNG TRÌNH LIÊN QUAN VÀ KIẾN THỨC CƠ SỞ
A. Chú ý Thưa thớt
Các công trình gần đây [23]–[25] dựa trên hàm chú ý đề xuất
sử dụng chuẩn hóa thưa thớt như sparsemax [19]. Sparsemax
có thể bỏ qua các mục có xác suất nhỏ và do đó có thể phân
phối khối lượng xác suất trên các đầu ra có xác suất cao hơn.
Nó cải thiện hiệu suất và khả năng giải thích ở một mức độ
nhất định [20]. Tuy nhiên, mỗi đầu của cơ chế chú ý đa đầu
được sử dụng trong Transformer được thiết kế để nắm bắt
các đặc tính từ một góc độ khác nhau. Ví dụ, trong khi các
đầu ở lớp đầu tiên của encoder tập trung vào trích xuất đặc
trưng, các đầu ở lớp cuối cùng của decoder tập trung vào
phân loại kết quả. Một cách trực quan, hàm chú ý của hai
tình huống nên khác nhau. Vì vậy, phương pháp thay thế tất
cả các cơ chế chú ý bằng sparsemax có thể không phù hợp
với Transformer [26]. Trong lĩnh vực NLP, có nhiều đột phá
trong Transformer liên quan đến độ thưa thớt. Từ góc độ
entropy Tsallis [27], [28] thống nhất tất cả các hàm biến đổi
thành dạng α-entmax. Khi α = 1, α-entmax là một hàm softmax.
Với bất kỳ α > 1, α-entmax là hàm sparsemax. Hơn nữa, để
làm cho α-entmax phù hợp với các đầu khác nhau cho các
tác vụ khác nhau, [21] giới thiệu một phương pháp thích ứng
để cho phép tham số α được cập nhật.

B. Chú ý Đơn điệu
Kỹ thuật chú ý đơn điệu [29] nhằm hạn chế việc chuyển tiếp
chú ý trong một đường ống từ quá khứ đến hiện tại. Nói chung,
chú ý đơn điệu được áp dụng để giảm độ phức tạp của việc
giải mã. Trong các tác vụ liên quan đến ngữ cảnh mạnh như
hiểu ngôn ngữ tự nhiên, một hàm chú ý đầy đủ thường được
áp dụng cho cả chuỗi đầu vào và đầu ra, vì thứ tự từ hoặc
từ loại ít quan trọng hơn. Nhưng nó có thể không hoạt động
tốt cho các tác vụ ASR vì dạng sóng giọng nói và chuỗi văn
bản mục tiêu được yêu cầu phải đơn điệu về mặt thời gian.
Để tăng cường khả năng căn chỉnh giữa dạng sóng và chuỗi
văn bản, [30] kết hợp chú ý đơn điệu cùng với mục tiêu phân
loại thời gian kết nối trong một sơ đồ học đa tác vụ bằng
cách sử dụng cùng một encoder. Công thức này trở thành
đường ống tối ưu hóa tiêu chuẩn cho hầu hết các mô hình
dựa trên chú ý [31]–[33]. Trong [34], một cách tiếp cận chú
ý đơn điệu theo chunk (MoChA) đã được giới thiệu cho chú
ý streaming. Nó đầu tiên chia đầu ra của encoder thành các
chunk có độ dài khá ngắn và sau đó áp dụng hàm chú ý mềm
trên các chunk nhỏ này. Hơn nữa, một cách tiếp cận chunk
có độ dài thích ứng đã được đề xuất trong [35]. Cụ thể, chú
ý nhìn lại vô hạn đơn điệu (MILK) [36] đã được áp dụng để
xem xét tổng thể các đặc trưng âm thanh trước đó để học một
lịch trình cân bằng độ trễ-chất lượng thích ứng cùng nhau.

C. ASR dựa trên Chú ý
Mặc dù các kiến trúc dựa trên chú ý [4], [5], [37] đã đạt được
thành công đáng kể trong các hệ thống ASR đầu cuối offline,
chúng không thể dễ dàng được áp dụng cho các hệ thống ASR
online hoặc streaming. Một số mô hình tối ưu hóa được đề
xuất để khắc phục giới hạn này. Chú ý đa đầu đơn điệu [38]
mở rộng tính đơn điệu cho đa đầu để trích xuất các biểu diễn
hữu ích và căn chỉnh phức tạp. Đối với chú ý đa đầu đơn điệu
[38], HeadDrop [39] được đề xuất để loại bỏ đầu dư thừa.
MoChA [34] thu hẹp khoảng cách giữa chú ý đơn điệu và
chú ý mềm. Khi nhận dạng giọng nói online ngày càng trở
nên quan trọng, nó cũng mang lại nhược điểm của các thiết
kế bổ sung trong mạng và các tiêu chí huấn luyện, và sự
suy giảm độ chính xác. Khi kích thước của Transformer lớn,
một cấu trúc nén [40] được đề xuất. Conformer [41] là một
Transformer được tăng cường bằng convolution cho nhận
dạng giọng nói. Trong chú ý được kích hoạt [42], một tác vụ
CTC được huấn luyện để học việc căn chỉnh kích hoạt việc
giải mã chú ý. Công trình trong [43] sử dụng chunk-hopping
cho ASR đầu cuối với điều khiển độ trễ.

D. Kiến trúc Mô hình Transformer ASR
Như được hiển thị trong Hình 1, kiến trúc mạng của chúng
tôi dựa trên một mô hình ASR dựa trên Transformer, dựa trên
một encoder và một decoder. Hãy ký hiệu chuỗi đầu vào là
x = {x1, ..., xT} với T là độ dài của chuỗi frame. Các trạng
thái encoder tương ứng được ký hiệu là h = {h1, ..., hS}.
Hãy ký hiệu chuỗi đầu ra là y = {y1, ..., yU} với U là độ dài
của chuỗi ký tự. Encoder đầu tiên xử lý đầu vào với một lớp
convolution down-sampling và xếp chồng với một số block
encoder. Encoder xử lý các đặc trưng giọng nói tuần tự, đó
là các năng lượng phổ log-mel 80 chiều. Lớp convolution
down-sampling sử dụng stride là 2, kernel 3×3, theo sau là
hàm kích hoạt ReLU. Convolution down-sampling có thể
trích xuất các encoding hữu ích, rút ngắn độ dài biểu diễn
âm thanh, và thúc đẩy căn chỉnh nhanh hơn trong việc giải
mã. Các module chính của Transformer chứa encoding vị trí,
tính toán tự chú ý đa đầu, linear và softmax, mạng feed-forward
theo điểm, kết nối dư, chuẩn hóa lớp. Ở đây, chúng tôi nhấn
mạnh khái niệm tự chú ý và chú ý đa đầu. Tự chú ý được
thiết kế để học các phụ thuộc nội bộ bằng cách tính toán biểu
diễn của một chuỗi đơn. Transformer sử dụng cơ chế tự chú
ý tích vô hướng có tỷ lệ được công thức hóa như sau:

headi = softmax(QiKiT/√(d/h))Vi (i = 1, 2, ..., h); (1)

trong đó Qi = XWqi, K = X1Wki, Vi = XWvi: (2)

Ở đây, h có nghĩa là số lượng đầu của chú ý đa đầu. Vì chiều
của X trong Transformer có cùng chiều với dmodel, các ma
trận chiếu là Wqi ∈ Rdmodel×dq, Wki ∈ Rdmodel×dk, Wvi ∈
Rdmodel×dv. Lưu ý rằng dq = dk = dv = dmodel/h. Đầu ra
của tự chú ý O được nối bằng cơ chế đa đầu, được công thức
hóa như sau:

O = concat(head1, head2, ..., headh)WO: (3)

WO ∈ Rhdv×dmodel và concat() có nghĩa là nối. Bên cạnh
đó, nó triển khai sơ đồ encoding vị trí sinusoidal, đạt được
việc tính toán chú ý để thích ứng tốt trên các độ dài giọng
nói khác nhau.

--- TRANG 3 ---
Hình 1. Sơ đồ kiến trúc mô hình của chúng tôi. Lưu ý rằng các kết nối
dư và chuẩn hóa lớp đã được bỏ qua.

III. PHƯƠNG PHÁP
Để chứng minh hiệu suất của transformer thưa thớt thích ứng
một cách có hệ thống, chúng tôi đã thiết kế nhiều cấu trúc
transformer dựa trên các hàm thưa thớt khác nhau, bao gồm
sparse transformer, 1.5-entmax transformer, và adaptive
α-entmax transformer. Các đổi mới của chúng tôi như sau:
thứ nhất, chúng tôi sử dụng các hàm thưa thớt khác nhau để
thay thế hàm softmax trong tự chú ý, và sử dụng phương pháp
thưa thớt thích ứng để mỗi tự chú ý có được mức độ thưa
thớt riêng. Thứ hai, chúng tôi đề xuất một cơ chế chú ý đa
đầu đơn điệu được điều chỉnh. Các đổi mới của chúng tôi
nằm trong hai thành phần: một cơ chế tự chú ý đa đầu thưa
thớt thích ứng được minh họa trong Mục III-A và một cơ chế
chú ý đa đầu đơn điệu thích ứng được minh họa trong Mục
III-B. Hình 1 cho thấy cách tích hợp hai thành phần cốt lõi
này vào baseline.

A. Tự Chú ý Đa đầu Thưa thớt Thích ứng
Như được hiển thị trong Hình 2, chúng tôi giới thiệu một
số hàm thưa thớt trong tự chú ý để ngăn chặn nhược điểm
của softmax. Ý tưởng đầu tiên là thay thế softmax bằng
sparsemax cho mỗi đầu độc lập của Transformer, có thể hoàn
toàn bỏ qua chú ý của xác suất thấp và chỉ phân phối khối
lượng xác suất trên các phần tử có sự quan tâm cao. Nó được
thực hiện bằng cách tính toán phép chiếu Euclidean của vector
đầu vào z lên simplex xác suất p:

Sparsemax(z) = argmin(p∈Δ) (1/2)||z - p||²₂; (4)

trong đó Δ = {p ∈ RJ | p ≥ 0, Σⱼpⱼ = 1} là simplex J-chiều.
Để khắc phục vấn đề tiềm ẩn của softmax, chúng tôi đề xuất
xử lý các giá trị α của mỗi đầu chú ý một cách khác nhau,
trong đó một số đầu có thể được huấn luyện để thông tin
thưa thớt hơn, và những đầu khác có thể trở nên tương tự như
dạng gốc của chúng. Chúng tôi cũng đề xuất xử lý các giá trị
α như các biến có thể học được, được tối ưu hóa thông qua
các tính toán lan truyền ngược cùng với các tham số mạng
khác. Trong [19], độ thưa thớt của sparsemax được chứng
minh và một giải pháp dạng đóng được đưa ra:

p = sparsemax(z) = [z - τ]₊; (5)

trong đó [·]₊ biểu thị hàm max(·, 0), và τ là nhân tử Lagrange
trong ngữ cảnh của ràng buộc Σⱼ(pⱼ) = 1. Ở đây τ cũng có
thể được hiểu là một ngưỡng mà dưới đó xác suất tương ứng
sẽ được đặt thành không. Tiếp theo, chúng tôi mở rộng khái
niệm chú ý thưa thớt trong kiến trúc đa đầu. Thay đổi cốt lõi
của kiến trúc này là chúng tôi sử dụng α-entmax [28] để giới
thiệu độ thưa thớt vào cơ chế chú ý của tự chú ý thưa thớt.
Đối với ba biến đổi đã đề cập (softmax, sparsemax, α-entmax),
mối quan hệ giữa chúng có thể được phân tích từ góc độ
entropy:

α-entmax(z) = argmax(p∈Δ) pᵀz + HᵀₐT(p): (6)

HᵀₐT(p) là định nghĩa họ α-entropy Tsallis với định nghĩa
sau:

HᵀₐT(p) = {
    (1/(α-1))Σᵢ(pⱼ - pⱼᵅ) nếu α ≠ 1
    -Σⱼpⱼlog pⱼ nếu α = 1
} (7)

ở đây α là một siêu tham số. Đối với các trường hợp α = 1
và α = 2, các α-entropy tương ứng là entropy Shannon và
Gini trong khi các α-entmax tương ứng bằng với softmax và
sparsemax tương ứng. Do đó, nó thiết lập kết nối giữa softmax
và sparsemax trong khi cũng chiếu sáng một mô hình trung
gian giữa hai biến đổi này. [28] chứng minh tính duy nhất
của α và đề xuất một phương pháp chính xác để tính toán
α-entmax khi α = 1.5 (còn gọi là 1.5-entmax). [44] đề xuất
một cách tiếp cận chia đôi lặp mở rộng có thể áp dụng cho
α-entmax. Hợp lý khi đặt α thành các giá trị khác nhau trong
Transformer để thích ứng tốt hơn với các đầu khác nhau ở
các lớp khác nhau.

Lấy cảm hứng từ [21], chúng tôi áp dụng phương pháp chú
ý α-entmax thích ứng trong cơ chế chú ý đa đầu cho các tác
vụ ASR. α-entmax là một bài toán tối ưu hóa lồi. Khi α được
đặt như một biến nhưng không phải là một siêu tham số, chúng
ta có p = α-entmax(z; α). Việc tính đạo hàm ∂α-entmax(z; α)/∂α
từ các điều kiện Lagrangian và tối ưu bằng cách lấy đạo hàm
theo z trong [19] không đơn giản. Do các framework mạng
nơ-ron sâu hiện tại không thể tự động tính đạo hàm cho bài
toán tối ưu hóa này. Do đó, chúng tôi giải quyết vấn đề này
từ góc độ giải pháp. Hiển nhiên đối với j ∉ S, pⱼ = 0 là một
hằng số do đó ∂pⱼ/∂α = 0. Để đơn giản hóa gradient cho
i ∈ S, chúng tôi sử dụng p và z để biểu thị các vector tương
ứng có chỉ số nằm trong support S. Cuối cùng, chúng tôi có
thể giải quyết thành phần của Jacobian.

B. Chú ý Đa đầu Đơn điệu Thích ứng
Đối với ASR đầu cuối, một thách thức cơ bản khác là việc
căn chỉnh giữa chuỗi đầu vào và chuỗi đầu ra. CTC [1] và
RNN-Transducer [2] là đơn điệu, do đó track căn chỉnh có
thể được biên hóa. Nhưng cách tiếp cận chú ý dẫn đến việc
căn chỉnh không đơn điệu, có thể làm giảm chất lượng nhận
dạng giọng nói. Cơ chế chú ý đơn điệu cứng đã được giới
thiệu sử dụng cho việc giải mã đồng bộ thời gian streaming
với kiến trúc chú ý ASR. Đối với cơ chế chú ý đơn điệu cứng,
quá trình được thể hiện như sau. Đối với bước thời gian giải
mã i, phương pháp chú ý bắt đầu kiểm tra các mục bắt đầu
từ bước thời gian đầu ra trước đó, được gọi là ti-1. Sau đó,
nó tính toán một vô hướng năng lượng ei,j dựa trên một hàm
năng lượng đơn điệu MonotonicEnergy(). Hàm này nhận
trạng thái đầu ra (i-1) và đặc trưng encoding hj làm đầu vào,
trong đó j = ti-1, ti-1+1, .... Vô hướng năng lượng được tính
toán như sau:

ei,j = MonotonicEnergy(si-1, hj): (8)

Chi tiết của hàm năng lượng đơn điệu có thể tham khảo [38].
Sau đó, các giá trị năng lượng ei,j được đưa vào hàm sigmoid
σ(·) để tạo ra xác suất lựa chọn pi,j được công thức hóa như:

pi,j = σ(ei,j): (9)

Sau đó, xác suất lựa chọn pi,j được sử dụng để tạo ra một
biến quyết định rời rạc zi,j, được công thức hóa như:

zi,j ~ Bernoulli(pi,j): (10)

Bất cứ khi nào pi,j được thỏa mãn, zi,j được kích thích (tức
là, giá trị tiến gần đến 1). Khi zi,j = 1, mô hình dừng lại và
đặt ti = j và ci = hti.

Trái ngược với một đầu chú ý đơn trong mô hình chuỗi-đến-
chuỗi hồi quy, cơ chế đa đầu có khả năng học ngữ cảnh với
các tốc độ đa dạng giữa chuỗi đầu vào và đầu ra. Bên cạnh
đó, đa đầu có thể học việc căn chỉnh phức tạp. Cụ thể, kết
quả của mỗi đầu có thể được coi là tương ứng với một số
frame cụ thể trong giọng nói, và các đầu khác nhau có liên
quan với nhau. Trong khi một số đầu có thể phát hiện ranh
giới, những đầu khác có thể nắm bắt các cấu trúc nội tại
mạnh mẽ của giọng nói.

Các cơ chế chú ý dày đặc có thể thất bại trong việc nắm bắt
hoặc sử dụng các cấu trúc nội tại mạnh mẽ có trong giọng
nói. Bằng cách tận dụng các phụ thuộc giữa các frame cụ thể
mà các đầu khác nhau tập trung vào, chúng ta có thể nhấn
mạnh thông tin quan trọng trong giọng nói và làm nổi bật
biểu diễn đặc trưng của các frame giọng nói này. Tuy nhiên,
không có gì đảm bảo rằng nhiều đầu đều đóng góp vào việc
học ngữ cảnh tổng thể. Bên cạnh đó, việc căn chỉnh đa đầu
đơn điệu có thể ngăn chặn việc giải mã trạng thái bị trì hoãn
do đa đầu cho việc phát hiện ranh giới. Mỗi đầu chú ý đơn
điệu phải học việc căn chỉnh một cách phù hợp. Cần thiết
phải loại bỏ một số đầu chú ý đơn điệu dư thừa. Do đó, để
tăng cường sự phù hợp giữa các đa đầu về việc phát hiện ranh
giới token, chúng tôi sử dụng regularization L1 để loại bỏ
các đầu dư thừa thông tin trong các lớp nông.

IV. THÍ NGHIỆM
Trong phần này, chúng tôi so sánh phương pháp của chúng
tôi với kiến trúc Transformer tiêu chuẩn của biến đổi softmax.
Ngoài ra, chúng tôi so sánh với hai biến thể phương pháp
khác để có một so sánh công bằng, khám phá các biến đổi
chuẩn hóa khác nhau:

1.5-entmax: Đặt giá trị α = 1.5 cố định cho tất cả các đa
đầu của sparse ent-max Transformer. Cách tiếp cận này được
coi là mới lạ, vì 1.5-entmax chỉ được giới thiệu cho các phương
pháp ASR hồi quy, nhưng chưa được khám phá trong Transformer
trước đây. Sự khác biệt chính là, gán sparse ent-max chỉ là
một module của phương pháp seq2seq, nhưng lại là một thành
phần tích hợp của tất cả các module Transformer.

α-entmax: Để được hưởng lợi từ một giá trị α thích ứng của
biến đổi sparse ent-max, một αti,j có thể học được được gán
cho mỗi đầu chú ý.

A. Tập dữ liệu và Thiết lập
Tập dữ liệu: Chúng tôi đánh giá hiệu suất mô hình một cách
có hệ thống trên AISHELL-1 [45] và WSJ. AISHELL-1 là
một corpus tiếng Trung, bao gồm 400 người nói và gần 170
giờ phát âm được ghi lại. Các phát âm của corpus được chia
thành tập dữ liệu huấn luyện, phát triển và kiểm tra, trong
đó tập huấn luyện bao gồm 120.098 giọng nói từ 340 người
nói; tập phát triển bao gồm 14.326 giọng nói từ 40 người
nói và tập kiểm tra bao gồm 7.176 phát âm từ 20 người nói.
Trong các thí nghiệm của AISHELL-1, chúng tôi không sử
dụng mô hình ngôn ngữ. Đối với tập dữ liệu WSJ, chúng tôi
sử dụng eval-92 để đánh giá và eval-93 để kiểm tra. Chúng
tôi sử dụng văn bản được phiên âm để huấn luyện mô hình
ngôn ngữ N-gram.

Các chỉ số: Tỷ lệ lỗi từ (WER) và tỷ lệ lỗi ký tự (CER) được
áp dụng làm các chỉ số đánh giá trong thí nghiệm. WER được
tính bằng phần trăm các từ có công thức văn bản được nhận
dạng không giống với nhãn thật. CER được tính dựa trên
khoảng cách Levenshtein giữa các ký tự được nhận dạng và
thật, sau đó chia cho độ dài chuỗi tuyệt đối. Cả hai chỉ số
đều tuân theo quy tắc giá trị càng thấp, hiệu suất càng tốt.

Chi tiết Huấn luyện: Như đã đề cập trước đó, dạng sóng
giọng nói đầu vào đầu tiên được xử lý thành các đặc trưng
sâu của 80 chiều filterbank, trong đó hop size là 10ms và
window size là 25ms. Ngoài ra, chúng tôi thực hiện phép
trừ sự khác biệt thời gian và người nói và chuẩn hóa bias.
Thiết lập kiến trúc bao gồm số block encoder Ne = 12, số
block decoder Nd = 6 và mạng feed-forward có dff = 1024.
Chúng tôi cũng giữ chiều đặc trưng dmodel = 512 và số đầu
chú ý h = 8 giống như baseline.

--- TRANG 5 ---
BẢNG I
KẾT QUẢ CỦA CÁC HÀM MAX KHÁC NHAU (CER (%) TRONG AISHELL, WER (%) TRONG WSJ), T ĐẠI DIỆN CHO NHIỆT ĐỘ SOFTMAX. CÁC SỐ TRONG BA HÀNG CUỐI CÙNG ĐẠI DIỆN CHO THƯA THỚT CHỈ ENCODER / CHỈ DECODER / CẢ HAI.

Phương pháp | Chú ý Thưa thớt | Chú ý Thưa thớt và Chú ý Đơn điệu
AISHELL-Dev | AISHELL-Test | WSJ-eval93 | AISHELL-Dev | AISHELL-Test | WSJ-eval93
softmax | 8.60 | 9.33 | 9.58 | 8.23 | 9.00 | 9.36
softmax(T=0.75) | 8.54 | 9.03 | 9.34 | 8.43 | 8.91 | 9.07
softmax(T=0.5) | 7.80 | 8.64 | 8.80 | 7.73 | 8.59 | 8.64
softmax(T=0.25) | 8.07 | 8.91 | 9.05 | 7.87 | 8.69 | 8.91
sparsemax | 8.91 / 8.99 / 8.98 | 9.56 / 9.53 / 9.55 | 9.69 / 9.64 / 9.62 | 8.64 / 8.58 / 8.64 | 9.13 / 9.29 / 9.06 | 9.45 / 9.38 / 9.34
1.5-entmax | 7.69 / 7.71 / 7.69 | 8.57 / 8.62 / 8.49 | 8.47 / 8.58 / 8.56 | 7.68 / 7.67 / 7.68 | 8.51 / 8.59 / 8.47 | 8.49 / 8.55 / 8.39
α-entmax (adaptive) | 7.67 / 7.71 / 7.65 | 8.54 / 8.60 / 8.53 | 8.35 / 8.47 / 8.38 | 7.71 / 7.69 / 7.64 | 8.51 / 8.52 / 8.45 | 8.38 / 8.49 / 8.37

BẢNG II
SO SÁNH WER (%) TRÊN WSJ EVAL 93

Phương pháp | WER
Transformer | 9.39
wav2letter [46] | 9.5
Jasper [47] | 9.9
Explicit Sparse Transformer [48] | 12.98
Factorized Attention Transformer [49] | 8.97
Combiner [50] | 9.32
Phương pháp đề xuất | 8.22

BẢNG III
SO SÁNH CER (%) TRÊN AISHELL-1

Phương pháp | Dev | Test
Transformer | 8.47 | 9.32
RNN-T | 10.13 | 11.82
LAS [5] | - | 10.56
SA-T [2] | 8.30 | 9.30
Sync-Transformer [51] | 7.91 | 8.91
Phương pháp đề xuất | 7.58 | 8.40

Chúng tôi sử dụng tối ưu hóa Adam với β₁ = 0.9, β₂ = 0.98
và số bước warmup là 25000. Dropout là 0.1. Để giải mã,
chúng tôi sử dụng tìm kiếm chùm tia với kích thước chùm
tia là 5. Chúng tôi chỉ thay đổi hàm softmax trong tự chú ý
trong encoder và thêm cơ chế chú ý đa đầu đơn điệu thích
ứng trong decoder. Để so sánh các phương pháp, chúng tôi
áp dụng RNN-Transducer. Mô hình là một mô hình bộ nhớ
ngắn hạn dài hai chiều 4 lớp với 320 đơn vị ẩn, mỗi hướng
chiếm một phần encoding, và một mô hình bộ nhớ ngắn hạn
dài đơn hướng 2 lớp với 512 đơn vị ẩn như module nhận
dạng. Kiến trúc này xem xét cả các đặc trưng âm thanh và
ngôn ngữ học, và chiếu dự đoán của hàm kích hoạt phi tuyến
lên softmax.

B. Kết quả Tổng thể
Kết quả được hiển thị trong Bảng 1. Khi chỉ thêm các biến
đổi thưa thớt vào encoder và decoder của Transformer, cả
hai đều có hiệu suất tốt hơn. Trong số đó, việc thêm vào cả
encoder và decoder đều đạt được kết quả tốt hơn. Chúng tôi
suy đoán rằng điều này là do biến đổi thưa thớt tăng cường
khả năng phân biệt của các đặc trưng âm thanh, và khả năng
phân biệt của thông tin văn bản ít quan trọng hơn so với việc
so sánh. Ngoài ra, mô hình thay thế hoàn toàn biến đổi thưa
thớt đạt được kết quả tốt nhất. Phương pháp thưa thớt được
cải thiện đáng kể so với phương pháp thay đổi nhiệt độ softmax.
Nhiệt độ softmax thấp có thể làm cho phân phối kết quả
softmax dốc hơn, từ đó làm nổi bật biểu hiện của một số
thông tin, nhưng đầu ra không âm của softmax vẫn không
thay đổi, vẫn dẫn đến lãng phí chú ý. Đáng chú ý là nhiệt
độ quá thấp sẽ khiến mô hình tập trung nhiều hơn vào một
phần thông tin nhất định và bỏ qua các thông tin quan trọng
khác, dẫn đến sự suy giảm kết quả.

Trong tự chú ý, việc thay thế hàm softmax bằng adaptive
α-entmax đều làm giảm WER, cho thấy rằng việc thêm độ
thưa thớt có lợi cho hiệu suất của mô hình. Tuy nhiên, sparsemax
gây ra sự giảm hiệu suất, cho thấy độ thưa thớt quá mức có
thể gây mất thông tin. Điều này chứng tỏ rằng độ thưa thớt
quá mức không thể làm nổi bật thông tin quan trọng trong
giọng nói, và chỉ một sparse thích ứng được thiết kế phù hợp
mới có thể làm nổi bật hiệu quả thông tin quan trọng. Chúng
tôi kết hợp adaptive α-entmax trong tự chú ý và báo cáo kết
quả tốt nhất trong Bảng II, III.

C. Phân tích Chú ý Thưa thớt Thích ứng
Trong phần này, chúng tôi giữ lại cơ chế chú ý đơn điệu và
nghiên cứu ảnh hưởng của các biến đổi thưa thớt khác nhau
trong tự chú ý thưa thớt. Trong Bảng 1, chúng tôi giữ module
tự chú ý ở trạng thái gốc nhất (softmax) và giới thiệu nó vào
block chú ý liên đầu để thí nghiệm. Hàng thứ ba đến thứ sáu
trong Bảng 1 đại diện cho các hàm max khác nhau trong
block chú ý liên đầu. Do số lượng đầu chú ý nhỏ, hàm max
quá thưa thớt (sparsemax) sẽ dẫn đến mất thông tin. Hàm
max với độ thưa thớt không đủ không đóng vai trò thưa thớt.
Đáng chú ý là hàm adaptive α-entmax có xu hướng α → 1.
Để điều tra sâu hơn phương pháp đề xuất trên các tập dữ liệu
thách thức hơn, chúng tôi cũng thực hiện các nghiên cứu
ablation trên tập dữ liệu LibriSpeech tiếng Anh trong Bảng
IV. Trong Hình 3, chúng tôi đưa ra một ví dụ với các biến
đổi thưa thớt khác nhau trong tự chú ý thưa thớt. So với
softmax, hàm sparsemax có thể tạo ra đầu ra thưa thớt cạnh
tranh, nhưng nó sẽ bỏ lỡ một số thông tin đầu ra. 1.5-entmax
có thể tạo ra kết quả đầu ra hoàn chỉnh hơn, do đó đạt được
hiệu suất tốt hơn. α-entmax cho phép mỗi module tự chú ý
sử dụng một α cụ thể theo các nhấn mạnh khác nhau, do đó
có được hiệu suất tốt nhất. Chúng ta có thể thấy rằng α-entmax
không chỉ tránh lãng phí điểm chú ý mà còn làm nổi bật hiệu
quả một số frame quan trọng trong giọng nói. Việc làm sâu
màu sắc có nghĩa là nó có điểm chú ý cao hơn.

BẢNG IV
SO SÁNH ĐỘ THƯA THỚT (WER %) TRÊN CORPUS LIBRISPEECH.

Phương pháp | dev-clean | dev | test-clean | test
softmax | 3.64 | 11.89 | 3.86 | 11.95
sparsemax | 3.88 | 12.01 | 4.00 | 12.19
1.5-entmax | 3.48 | 10.43 | 3.79 | 11.76
α-entmax | 3.36 | 10.29 | 3.70 | 11.64

--- TRANG 6 ---
Hình 3. Bản đồ chú ý của các biến đổi khác nhau trong tự chú ý thưa thớt.

Hình 4. (a) Sự thay đổi của α trong tự chú ý thưa thớt trong quá trình
huấn luyện, (b) Thống kê điểm cuối cùng của các cơ chế chú ý khác nhau.

Trong Hình 4(a), chúng tôi theo dõi sự thay đổi của tham số
α. Có thể thấy rằng ở đầu quá trình huấn luyện, tham số α
đã giảm xuống một mức độ nhất định, cho thấy rằng việc sử
dụng đầu ra dày đặc tương tự như softmax trong giai đoạn
đầu của huấn luyện là mong muốn. Đặc biệt đối với phần
encoder, càng gần với đầu vào đặc trưng, giá trị α càng gần
với 1, cho thấy rằng hàm softmax có thể tăng cường hiệu quả
khả năng phân biệt của đặc trưng. Sau một lượng huấn luyện
nhất định, ngoại trừ lớp đầu tiên của encoder, các phần khác
tiến gần hơn đến hướng thưa thớt, cho thấy rằng phương pháp
khởi tạo α không thể xác định mức độ thưa thớt trước, và
ở một mức độ nhất định, nó cho thấy phương pháp cố định
α như một tham số có thể học được phù hợp cho cơ chế chú
ý đa đầu. Trong Hình 3(b), chúng tôi trực quan hóa điểm
chú ý đầu ra cuối cùng của cơ chế chú ý liên đầu, và chúng
ta có thể thấy rằng softmax tạo ra nhiều chú ý điểm thấp do
thiếu độ thưa thớt. Cấu trúc mà chúng tôi đề xuất có thể tạo
ra nhiều điểm cao hơn để tránh lãng phí điểm chú ý.

D. Phân tích Chú ý Đơn điệu Thích ứng
Trong Bảng 1, so sánh với chỉ sử dụng cơ chế chú ý thưa
thớt, hiệu suất của các phương pháp sparsemax được cải
thiện trung bình -0.17% với sự giúp đỡ của cơ chế chú ý
đơn điệu. Hơn nữa, cơ chế chú ý đơn điệu cũng bổ sung cho
các phương pháp sparsemax và entmax. Kết quả kiểm tra
được tăng cường -0.13% CER trên AISHELL và -0.07% WER
trên WSJ, tương ứng. Kết quả cho thấy rằng cơ chế chú ý
đơn điệu có thể hợp tác với cơ chế chú ý thưa thớt để cải
thiện hiệu suất hơn nữa.

Chúng tôi tiếp tục thí nghiệm regularization L1 trên cơ chế
chú ý đơn điệu. Như được hiển thị trong Bảng 5, với sự giúp
đỡ của regularization, tất cả các phương pháp hàm max đều
có cải thiện cả trên AISHELL-1 và WSJ. Khoảng cách giữa
w/o reg. và phương pháp đề xuất có thể được giảm từ 9.29%
xuống 9.00% trên AISHELL-1 và 9.43% xuống 9.36% trên
WSJ cho phiên bản softmax gốc. Chúng tôi áp dụng sparse
thích ứng trong đó dẫn đến α → 1.0, việc căn chỉnh đa đầu
đơn điệu có thể ngăn chặn việc tạo token bị trì hoãn do các
đầu như vậy cho việc phát hiện ranh giới.

BẢNG V
NGHIÊN CỨU ABLATION VỀ REGULARIZATION L1 TRÊN CHÚ Ý ĐƠN ĐIỆU.

Phương pháp | AISHELL-Test | WSJ-eval93
 | reg. | w/o reg. | reg. | w/o reg.
softmax | 9.00 | 9.29 | 9.36 | 9.43
sparsemax | 9.13 | 9.32 | 9.45 | 9.47
α-entmax | 8.49 | 8.50 | 8.37 | 8.35

V. KẾT LUẬN
Bài báo này tích hợp cơ chế chú ý thưa thớt thích ứng và
cơ chế chú ý đơn điệu thích ứng vào mô hình ASR đầu cuối
dựa trên Transformer. Cơ chế chú ý thưa thớt thích ứng được
triển khai trong cơ chế tự chú ý đa đầu để làm nổi bật các
thông tin quan trọng trong giọng nói đầu vào bằng cách thay
thế hàm softmax bằng α-entmax. Bên cạnh đó, cơ chế chú ý
đa đầu đơn điệu thích ứng được triển khai bằng cách thêm
một thuật ngữ được điều chỉnh. Kết quả thí nghiệm chứng
minh rằng phương pháp của chúng tôi có thể tập trung chú
ý nhiều hơn vào thông tin quan trọng hơn và đạt được kết
quả tốt nhất. Cơ chế thích ứng chỉ dựa trên tối ưu hóa dựa
trên gradient, tìm kiếm độ thưa thớt cho mỗi đầu và tính
toán α-entmax.

VI. LỜI CẢM ƠN
Bài báo này được hỗ trợ bởi Chương trình Nghiên cứu và
Phát triển Trọng điểm của Tỉnh Quảng Đông theo tài trợ
số 2021B0101400003. Tác giả liên hệ là Jianzong Wang từ
Ping An Technology (Shenzhen) Co., Ltd (jzwang@188.com).

--- TRANG 7 ---
TÀI LIỆU THAM KHẢO
[1] A. Graves, S. Fernández, và F. Gomez, "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks," International Conference on Machine Learning, pp. 369–376, 2006.

[2] Z. Tian, J. Yi, J. Tao, Y. Bai, và Z. Wen, "Self-attention transducers for end-to-end speech recognition." Interspeech, 2019.

[3] S. Karita, N. E. Y. Soplin, S. Watanabe, M. Delcroix, A. Ogawa, và T. Nakatani, "Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration," trong Proc. Interspeech 2019, pp. 1408–1412.

[4] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, và Y. Bengio, "Attention-based models for speech recognition," Advances in neural information processing systems, vol. 28, pp. 577–585, 2015.

[5] W. Chan, N. Jaitly, Q. V. Le, và O. Vinyals, "Listen, attend and spell," 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.

[6] X. Song, Z. Wu, Y. Huang, C. Weng, D. Su, và H. Meng, "Non-autoregressive transformer asr with ctc-enhanced decoder input," trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5894–5898.

[7] M. Li, C. Zorilă, và R. Doddipatla, "Head-synchronous decoding for transformer-based streaming asr," trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5909–5913.

[8] C. Zhao, J. Wang, X. Qu, H. Wang, và J. Xiao, "r-g2p: Evaluating and enhancing robustness of grapheme to phoneme conversion by controlled noise introducing and contextual information incorporation," ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6197–6201, 2022.

[9] E. Tsunoo, Y. Kashiwagi, và S. Watanabe, "Streaming transformer asr with blockwise synchronous beam search," trong 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 22–29.

[10] Z. Hong, J. Wang, X. Qu, J. Liu, C. Zhao, và J. Xiao, "Federated learning with dynamic transformer for text to speech," trong Interspeech, 2021.

[11] C. Zhao, J. Wang, L. Li, X. Qu, và J. Xiao, "Adaptive few-shot learning algorithm for rare sound event detection," 2022 International Joint Conference on Neural Networks (IJCNN), 2022.

[12] N. Zhang, J. Wang, Z. Hong, C. Zhao, X. Qu, và J. Xiao, "Dt-sv: A transformer-based time-domain approach for speaker verification," 2022 International Joint Conference on Neural Networks (IJCNN), 2022.

[13] Z. Hong, J. Wang, W. Wei, J. Liu, X. Qu, B. Chen, Z. Wei, và J. Xiao, "When hearing the voice, who will come to your mind," 2021 International Joint Conference on Neural Networks (IJCNN), 2021.

[14] Z. Deng, Y. Cai, L. Chen, Z. Gong, Q. Bao, X. Yao, D. Fang, W. Yang, S. Zhang, và L. Ma, "Rformer: Transformer-based generative adversarial network for real fundus image restoration on a new clinical benchmark," IEEE Journal of Biomedical and Health Informatics, 2022.

[15] Y. Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y. Zhang, R. Timofte, và L. V. Gool, "Mask-guided spectral-wise transformer for efficient hyperspectral image reconstruction," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[16] Y. Cai, X. Hu, H. Wang, Y. Zhang, H. Pfister, và D. Wei, "Learning to generate realistic noisy images via pixel-level noise-aware adversarial training," trong Thirty-Fifth Conference on Neural Information Processing Systems, 2021.

[17] Y. Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y. Zhang, R. Timofte, và L. V. Gool, "Coarse-to-fine sparse transformer for hyperspectral image reconstruction," trong European Conference on Computer Vision (ECCV), 2022.

[18] R. Fan, W. Chu, P. Chang, và J. Xiao, "Cass-nat: Ctc alignment-based single step non-autoregressive transformer for speech recognition," trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5889–5893.

[19] A. Martins và R. Astudillo, "From softmax to sparsemax: A sparse model of attention and multi-label classification," trong International Conference on Machine Learning, 2016, pp. 1614–1623.

[20] V. Niculae và M. Blondel, "A regularized framework for sparse and structured neural attention," trong Advances in Neural Information Processing Systems, 2017, pp. 3338–3348.

[21] G. Correia, V. Niculae, và A. F. Martins, "adaptively sparse transformers," trong Conference on Empirical Methods in Natural Language Processing, 2019.

[22] X. Song, Z. Wu, Y. Huang, C. Weng, D. Su, và H. Meng, "Non-autoregressive transformer asr with ctc-enhanced decoder input," trong ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5894–5898.

[23] J. Xue, T. Zheng, và J. Han, "Structured sparse attention for end-to-end automatic speech," International Conference on Acoustics Speech and Signal Processing (ICASSP), 2020.

[24] V. Niculae, A. F. Martins, M. Blondel, và C. Cardie, "Sparsemap: Differentiable sparse structured inference," International Conference on Machine Learning, 2018.

[25] C. Malaviya, P. Ferreira, và A. F. Martins, "Sparse and constrained attention for neural machine translation," Annual Meeting of the Association for Computational Linguistics (ACL), 2018.

[26] D. Mareček và R. Rosa, "Extracting syntactic trees from transformer encoder self-attentions," trong Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018, pp. 347–349.

[27] A. Plastino và A. Plastino, "Stellar polytropes and tsallis' entropy," vol. 174, no. 5-6. Elsevier, 1993, pp. 384–386.

[28] B. Peters, V. Niculae, và A. F. Martins, "Sparse sequence-to-sequence models," trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 1504–1519.

[29] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, và D. Eck, "Online and linear-time attention by enforcing monotonic alignments," trong International Conference on Machine Learning. PMLR, 2017, pp. 2837–2846.

[30] S. Kim, T. Hori, và S. Watanabe, "Joint ctc-attention based end-to-end speech recognition using multi-task learning," trong 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 4835–4839.

[31] A. H. Liu, H.-y. Lee, và L.-s. Lee, "Adversarial training of end-to-end speech recognition using a criticizing language model," trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6176–6180.

[32] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y.-Y. Lee, J. Yeo, D. Kim, S. Jung và cộng sự, "Attention based on-device streaming speech recognition with large speech corpus," trong 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 956–963.

[33] T. Nakatani, "Improving transformer-based end-to-end speech recognition with connectionist temporal classification and language model integration," trong Proc. Interspeech 2019, 2019.

[34] C.-C. Chiu và C. Raffel, "Monotonic chunkwise attention," International Conference on Learning Representations (ICLR), 2017.

[35] R. Fan, P. Zhou, W. Chen, J. Jia, và G. Liu, "An online attention-based model for speech recognition," Proc. Interspeech 2019, pp. 4390–4394, 2019.

[36] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz, R. Pang, W. Li, và C. Raffel, "Monotonic infinite lookback attention for simultaneous machine translation," trong Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 1313–1323.

[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.

[38] X. Ma, J. Pino, J. Cross, L. Puzon, và J. Gu, "Monotonic multihead attention," International Conference on Learning Representations (ICLR), 2019.

[39] H. Inaguma, M. Mimura, và T. Kawahara, "Enhancing monotonic multihead attention for streaming asr," INTERSPEECH, 2020.

[40] S. Li, D. Raj, X. Lu, P. Shen, T. Kawahara, và H. Kawai, "Improving transformer-based speech recognition systems with compressed structure and speech attributes augmentation." trong INTERSPEECH, 2019, pp. 4400–4404.

[41] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu và cộng sự, "Conformer: Convolution-augmented transformer for speech recognition," INTERSPEECH, 2020.

[42] N. Moritz, T. Hori, và J. Le Roux, "Triggered attention for end-to-end speech recognition," trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5666–5670.

[43] L. Dong, F. Wang, và B. Xu, "Self-attention aligner: A latency-control end-to-end model for asr using self-attention network and chunk-hopping," trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5656–5660.

[44] M. Blondel, A. Martins, và V. Niculae, "Learning classifiers with fenchel-young losses: Generalized entropies, margins, and algorithms," trong The 22nd International Conference on Artificial Intelligence and Statistics, 2019, pp. 606–615.

[45] H. Bu, J. Du, X. Na, B. Wu, và H. Zheng, "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline," trong 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1–5.

[46] N. và cộng sự, "Fully convolutional speech recognition," CoRR, vol. abs/1812.06864, 2018.

[47] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen, H. Nguyen, và R. T. Gadde, "Jasper: An end-to-end convolutional neural acoustic model," trong Interspeech 2019.

[48] J. L. Guangxiang Zhao và X. Renz, "Explicit sparse transformer: Concentrated attention through explicit selection," arXiv preprint arXiv:1912.11637v1, 2019.

[49] R. Child, S. Gray, A. Radford, và I. Sutskever, "Generating long sequences with sparse transformers." Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2019.

[50] J. Wu, C. Xiong, và T. Schnabel, "Combiner: Inductively learning tree structured attention in transformer," trong International Conference on Learning Representations, 2020.

[51] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, và Z. Wen, "Synchronous transformer for end-to-end speech recognition," International Conference on Acoustics Speech and Signal Processing, 2020.

--- TRANG 8 ---
[43] L. Dong, F. Wang, và B. Xu, "Self-attention aligner: A latency-control end-to-end model for asr using self-attention network and chunk-hopping," trong ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5656–5660.

[44] M. Blondel, A. Martins, và V. Niculae, "Learning classifiers with fenchel-young losses: Generalized entropies, margins, and algorithms," trong The 22nd International Conference on Artificial Intelligence and Statistics, 2019, pp. 606–615.

[45] H. Bu, J. Du, X. Na, B. Wu, và H. Zheng, "Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline," trong 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1–5.

[46] N. và cộng sự, "Fully convolutional speech recognition," CoRR, vol. abs/1812.06864, 2018.

[47] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen, H. Nguyen, và R. T. Gadde, "Jasper: An end-to-end convolutional neural acoustic model," trong Interspeech 2019.

[48] J. L. Guangxiang Zhao và X. Renz, "Explicit sparse transformer: Concentrated attention through explicit selection," arXiv preprint arXiv:1912.11637v1, 2019.

[49] R. Child, S. Gray, A. Radford, và I. Sutskever, "Generating long sequences with sparse transformers." Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2019.

[50] J. Wu, C. Xiong, và T. Schnabel, "Combiner: Inductively learning tree structured attention in transformer," trong International Conference on Learning Representations, 2020.

[51] Z. Tian, J. Yi, Y. Bai, J. Tao, S. Zhang, và Z. Wen, "Synchronous transformer for end-to-end speech recognition," International Conference on Acoustics Speech and Signal Processing, 2020.
