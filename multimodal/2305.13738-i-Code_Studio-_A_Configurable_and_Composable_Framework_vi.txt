# 2305.13738.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2305.13738.pdf
# Kích thước tệp: 1362092 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
i-Code Studio: Một Framework Có Thể Cấu Hình và Tổ Hợp
cho AI Tích Hợp
Yuwei Fang∗, Mahmoud Khademi∗, Chenguang Zhu, Ziyi Yang, Reid Pryzant, Yichong Xu,
Yao Qian, Takuya Yoshioka, Lu Yuan, Michael Zeng và Xuedong Huang
Nhóm Nghiên cứu Dịch vụ Nhận thức Microsoft
{yuwfan, mkhademi, chezhu}@microsoft.com
Tóm tắt
Trí tuệ Nhân tạo Tổng quát (AGI) đòi hỏi
khả năng hiểu biết và tạo sinh toàn diện
cho nhiều tác vụ khác nhau trải rộng trên
các phương thức và chức năng khác nhau.
AI Tích hợp là một hướng quan trọng để
tiếp cận AGI, thông qua việc kết hợp nhiều
mô hình để giải quyết các tác vụ đa phương
thức phức tạp. Tuy nhiên, hiện thiếu một
nền tảng linh hoạt và có thể tổ hợp để tạo
thuận lợi cho việc kết hợp và phối hợp mô
hình hiệu quả và hiệu suất. Trong bài báo
này, chúng tôi đề xuất i-Code Studio, một
framework có thể cấu hình và tổ hợp cho
AI Tích hợp. i-Code Studio điều phối nhiều
mô hình được huấn luyện trước theo cách
không cần tinh chỉnh để thực hiện các tác
vụ đa phương thức phức tạp. Thay vì việc
tổ hợp mô hình đơn giản, i-Code Studio
cung cấp một môi trường tích hợp, linh hoạt
và có thể tổ hợp để các nhà phát triển có
thể nhanh chóng và dễ dàng tổ hợp các
dịch vụ và công nghệ tiên tiến phù hợp với
yêu cầu cụ thể của họ. i-Code Studio đạt
được kết quả ấn tượng trên nhiều tác vụ
đa phương thức zero-shot, như truy xuất
video-to-text, dịch speech-to-speech, và
trả lời câu hỏi thị giác. Chúng tôi cũng
trình bày cách nhanh chóng xây dựng một
tác nhân đa phương thức dựa trên i-Code
Studio có thể giao tiếp và cá nhân hóa cho
người dùng. Trang dự án với các demo và
mã nguồn tại https://i-code-studio.github.io/

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) như BERT (Devlin et al., 2018) và GPT-3 (Brown et al., 2020),
các mô hình thị giác-ngôn ngữ (VLM) như CLIP (Radford et al., 2021a) và DALL-E (Ramesh et al.,
2021), và các mô hình âm thanh-ngôn ngữ (ALM) như W2V-BERT (Chung et al., 2021) đã cho phép
nhiều khả năng khác nhau, từ phân loại hình ảnh zero-shot đến đọc hiểu, nhận dạng giọng nói tự động,
và tạo hình ảnh chân thực. Tuy nhiên, hiệu suất và khả năng của các mô hình được huấn luyện trước này
bị ảnh hưởng bởi dữ liệu mà chúng tiếp xúc, và dữ liệu này khác nhau giữa các lĩnh vực khác nhau; LLM
được huấn luyện trên các nguồn dữ liệu đa dạng, như trang web, tiểu thuyết, và kho văn bản Wikipedia,
trong khi VLM được huấn luyện trên cặp hình ảnh hoặc video và chú thích của chúng, và ALM được
huấn luyện trên dữ liệu âm thanh như giọng nói. Các lĩnh vực huấn luyện riêng biệt này khiến các mô
hình được huấn luyện trước có những khả năng khác nhau và đôi khi bổ sung cho nhau.

Hình 1: i-Code Studio là một kiến trúc có thể cấu hình và tổ hợp cho AI tích hợp cho phép các nhà phát triển nhanh chóng và dễ dàng điều phối các mô hình được huấn luyện trước tiên tiến khác nhau theo cách không cần tinh chỉnh.

Ví dụ, LLM phù hợp cho các tác vụ như đọc hiểu nhưng không thể diễn giải thông tin âm thanh và thị giác; VLM có thể tạo ra hình ảnh chân thực nhưng không thể giải quyết việc hiểu ngôn ngữ phức tạp. Mặt khác, con người thường có thể dễ dàng xử lý các tác vụ riêng biệt như trên với đầu vào và đầu ra đa phương thức. Do đó, để xây dựng Trí tuệ Nhân tạo Tổng quát (AGI), chúng ta cần phá bỏ rào cản giữa các phương thức và các tác vụ cụ thể.

--- TRANG 2 ---
Thay vì xây dựng một mô hình duy nhất để xử lý tất cả các tác vụ có thể, điều này không khả thi với công nghệ hiện tại, nhiều nghiên cứu gần đây đã xuất hiện tập trung vào việc kết hợp các mô hình lớn được huấn luyện trước để đạt được AI tích hợp, hoặc thông qua việc tinh chỉnh chúng cùng nhau trên các tác vụ mới (Yang et al., 2022; Hu và Singh, 2021; Wang et al., 2021b; Alayrac et al., 2022; Tewel et al., 2022), hoặc thông qua một phương thức chia sẻ, như ngôn ngữ, để nắm bắt các khả năng đa phương thức mới mà không cần tinh chỉnh (Tewel et al., 2022; Zeng et al., 2022; Wang et al., 2022; Li et al., 2022). Các vấn đề với những phương pháp này là 1) thường thiếu dữ liệu và tài nguyên tính toán cho việc tinh chỉnh chung, và 2) người ta không thể dễ dàng cấu hình và tổ hợp các mô hình lớn được huấn luyện trước khác nhau trong một framework linh hoạt để thích ứng với các nhu cầu khác nhau. Do đó, trong bài báo này, chúng tôi đề xuất i-Code Studio, một framework có thể cấu hình và tổ hợp cho AI tích hợp (Hình 1). i-Code Studio cho phép các nhà phát triển nhanh chóng và dễ dàng điều phối các mô hình được huấn luyện trước tiên tiến khác nhau theo cách không cần tinh chỉnh.

Các mô hình được huấn luyện trước này đến từ các phương thức khác nhau, và điểm mạnh của mỗi mô hình riêng lẻ được tích hợp để thực hiện các tác vụ đa phương thức phức tạp. Đối với mỗi tác vụ, một đồ thị có hướng không chu trình (DAG) được cấu hình để các mô hình liên quan hợp tác tạo ra đầu ra mong muốn. Dữ liệu đầu vào chảy qua mỗi nút trong DAG, cho phép hoàn thành các tác vụ đa phương thức phức tạp. Điều này làm cho i-Code Studio trở thành một framework tích hợp, linh hoạt và có thể tổ hợp. Ví dụ, đối với tác vụ trả lời câu hỏi thị giác, một DAG được cấu hình sử dụng hình ảnh đầu vào, câu hỏi đầu vào, mô hình nền tảng thị giác Florence (Yuan et al., 2021), một lời nhắc ngôn ngữ, ChatGPT, và một đầu ra, mỗi cái được đại diện bởi một nút. Thông tin thị giác từ hình ảnh đầu vào được đưa vào Florence. Nút Florence xử lý hình ảnh và xuất ra một tập hợp các danh mục/thẻ đối tượng được phát hiện và một chú thích. Các đầu ra này và câu hỏi đầu vào sau đó được đưa vào một nút tạo ra một lời nhắc ngôn ngữ có thông tin VLM. Cuối cùng, lời nhắc đa phương thức này được ChatGPT sử dụng để tạo ra câu trả lời cho câu hỏi đầu vào và gửi đến nút đầu ra.

Trong bài báo này, chúng tôi trình bày hiệu quả của i-Code Studio sử dụng các mô hình từ Azure Cognitive Services (ACS) và các dịch vụ OpenAI. Mô hình tích hợp kết quả đạt được hiệu suất tối tân (SOTA) hoặc tương đương với SOTA trên các tác vụ zero-shot như dịch speech-to-speech, truy xuất video-to-text, và trả lời câu hỏi thị giác. Chúng tôi cũng cho thấy cách nhanh chóng xây dựng một tác nhân đa phương thức để tương tác với người dùng. Tóm lại, các đóng góp chính của chúng tôi như sau:

(1) Chúng tôi đề xuất i-Code Studio, một framework tích hợp, có thể cấu hình và tổ hợp mới có thể được sử dụng để tổ hợp các mô hình được huấn luyện trước khác nhau.

(2) Chúng tôi cho thấy i-Code Studio có thể đạt được kết quả ấn tượng trên nhiều tác vụ đa phương thức zero-shot, ví dụ truy xuất video-to-text, dịch speech-to-speech, và trả lời câu hỏi thị giác.

(3) Chúng tôi sử dụng i-Code Studio để xây dựng một tác nhân đa phương thức có thể giao tiếp và cá nhân hóa cho người dùng bằng cách tận dụng các dịch vụ ACS và OpenAI.

2 Công trình liên quan

Gần đây, việc tổ hợp các mô hình lớn được huấn luyện trước đã được nghiên cứu rộng rãi. Cách phổ biến nhất để tổ hợp các mô hình này là tinh chỉnh chúng cùng nhau trên các tác vụ mới. Hu và Singh (2021) đề xuất UniT, một mô hình Transformer Thống nhất có khả năng học nhiều tác vụ trên nhiều lĩnh vực, bao gồm phát hiện đối tượng và lý luận đa phương thức. Mô hình này dựa trên kiến trúc transformer encoder-decoder, trong đó mỗi phương thức đầu vào được mã hóa bằng một encoder, và các decoder chia sẻ được sử dụng để đưa ra dự đoán cho mỗi tác vụ. Wang et al. (2021b) đề xuất một framework Pretraining Vision-Language, gọi là SimVLM được huấn luyện end-to-end với một mục tiêu mô hình hóa ngôn ngữ duy nhất. SimVLM giảm độ phức tạp của việc huấn luyện bằng cách sử dụng giám sát yếu ở quy mô lớn. Alayrac et al. (2022) đề xuất Flamingo, một tập hợp các VLM có thể kết nối các mô hình chỉ thị giác và chỉ ngôn ngữ được huấn luyện trước, xử lý các chuỗi dữ liệu thị giác và văn bản xen kẽ, và chấp nhận hình ảnh hoặc video làm đầu vào. Tuy nhiên, các phương pháp này có thể tốn kém về mặt tính toán. i-Code Studio khác với các phương pháp này vì nó không yêu cầu tinh chỉnh, điều này cho phép tổ hợp nhanh các mô hình được huấn luyện trước cho nhiều tác vụ và giảm thời gian và chi phí liên quan đến tinh chỉnh.

Khác với các công trình này, các mô hình có thể được tổ hợp thông qua một phương thức chia sẻ, như ngôn ngữ. Tewel et al. (2022) kết hợp một mô hình thị giác-ngữ nghĩa với một mô hình ngôn ngữ lớn, cho phép các mô hình tận dụng kiến thức có trong cả hai mô hình quy mô web cho tác vụ tạo chú thích hình ảnh. Liên quan hơn đến công trình của chúng tôi, Zeng et al. (2022) đề xuất Socratic Models, một framework mô-đun cho phép nhiều mô hình được huấn luyện trước trao đổi

--- TRANG 3 ---
thông tin với nhau, nắm bắt các khả năng đa phương thức mới mà không cần tinh chỉnh, và được tổ hợp mà không cần bất kỳ huấn luyện trước nào bằng cách sử dụng prompting có thông tin đa phương thức. i-Code Studio của chúng tôi là một framework tích hợp, linh hoạt và có thể tổ hợp hơn so với các công trình này, cho phép người dùng dễ dàng tổ hợp các mô hình và công nghệ tiên tiến được tùy chỉnh cho nhu cầu cụ thể của họ.

Khác biệt với các công trình đã đề cập, Li et al. (2022) đề xuất một phương pháp vòng lặp kín để kết hợp các mô hình được huấn luyện trước sao cho chúng hoạt động như các bộ tạo và bộ chấm điểm. Các bộ tạo tạo ra các đề xuất, trong khi các bộ chấm điểm cung cấp phản hồi để cải thiện kết quả được tạo. Loại tối ưu hóa đồng thuận lặp này cho phép các mô hình sửa chữa những sai lầm do các mô hình khác gây ra, dẫn đến cải thiện đáng kể trong các tác vụ downstream. (Huang et al., 2022) nghiên cứu việc ứng dụng LLM trong môi trường embodied để điều khiển robot. Họ kết hợp LLM với các nguồn phản hồi văn bản khác nhau và phát hiện rằng ngôn ngữ tự nhiên hoạt động như một phương tiện giao tiếp toàn cầu với mô hình. Hệ thống kết quả, được gọi là Inner Monologue, tích hợp các thành phần khác nhau như mô hình nhận thức, kỹ năng robot, và phản hồi của con người để thực hiện hiệu quả các lệnh của người dùng.

3 Framework i-Code Studio

Trong phần này, chúng tôi giới thiệu i-Code Studio, một framework có thể cấu hình và tổ hợp cho AI tích hợp. Với một tác vụ đa phương thức phức tạp, i-Code Studio cung cấp một framework chung để các nhà phát triển có thể nhanh chóng và dễ dàng tích hợp và tổ hợp nhiều mô hình lớn được huấn luyện trước và dịch vụ trên các phương thức khác nhau mà không cần bất kỳ huấn luyện hoặc tinh chỉnh nào để hoàn thành tác vụ. Hình 2 cho thấy các ví dụ về xây dựng giải pháp AI cho các tác vụ đa phương thức khác nhau sử dụng framework i-Code Studio.

Đối với mỗi tác vụ, framework có thể được biểu diễn thông qua một DAG, trong đó các nút không có cạnh đến là dữ liệu đầu vào thô như hình ảnh, văn bản, video và giọng nói, các nút không có cạnh đi là đầu ra của tác vụ đã cho, và phần còn lại của các nút là các mô hình/dịch vụ nền tảng hoặc chứa đầu ra mô hình trung gian từ các mô hình/dịch vụ khác. Đầu vào cho một nút đến từ đầu vào thô, và/hoặc đầu ra từ các nút trước đó. Dữ liệu đầu vào chảy qua mỗi nút trong DAG, cho phép hoàn thành các tác vụ đa phương thức phức tạp. Một cạnh đi từ nút mô hình/dịch vụ biểu thị một API được cung cấp bởi mô hình/dịch vụ. Đối với mỗi

Hình 2: i-Code Studio có thể được sử dụng để xây dựng giải pháp AI cho các tác vụ đa phương thức khác nhau. Đối với mỗi tác vụ, một DAG được cấu hình để các mô hình liên quan hợp tác tạo ra đầu ra mong muốn. Dữ liệu đầu vào chảy qua mỗi nút trong DAG, cho phép hoàn thành các tác vụ đa phương thức phức tạp. Các nút đầu vào được biểu thị bằng các vòng tròn màu xanh đôi, các nút mô hình/dịch vụ, ví dụ ChatGPT và Florence, bằng các vòng tròn màu đen, các nút đầu ra bằng các vòng tròn màu đen đôi, và phần còn lại bằng các vòng tròn màu đỏ gạch chấm. Xem văn bản để biết chi tiết về mỗi tác vụ đa phương thức.

tác vụ, các đầu vào vào DAG từ các nút đầu vào, và được xử lý bởi một hoặc nhiều mô hình hoặc dịch vụ mô hình. Trong quá trình này, các cạnh chuyển đổi định dạng đầu ra của một mô-đun, lọc dữ liệu, hoặc áp dụng một API như tóm tắt, dịch thuật, phát hiện đối tượng, chú thích hình ảnh, phiên âm, tổng hợp text-to-speech, v.v.

Đối với mỗi tác vụ, một DAG được cấu hình để

--- TRANG 4 ---
các mô hình liên quan hợp tác tạo ra đầu ra mong muốn. Các thành phần khác nhau của i-Code Studio hợp tác một cách liền mạch để tạo thành một giải pháp AI tích hợp duy nhất, và có thể được điều chỉnh để phù hợp với nhu cầu cụ thể của người dùng. Ví dụ, đối với tác vụ trả lời câu hỏi thị giác (VQA), đầu vào là một hình ảnh và một câu hỏi liên quan đến hình ảnh (xem Hình 2). Chúng ta có thể đầu tiên áp dụng các dịch vụ chú thích hình ảnh và phát hiện đối tượng cho hình ảnh đầu vào. Văn bản đầu ra, chứa thông tin thị giác, được hợp nhất với câu hỏi đầu vào làm lời nhắc cho ChatGPT, nó trả lời câu hỏi. Đối với dịch speech-to-speech, DAG được cấu hình với Nhận dạng Giọng nói (SR) → Dịch Máy (MT) → Text-To-Speech (TTS). DAG này phiên âm giọng nói nguồn, dịch bản phiên âm sang ngôn ngữ đích, và tạo giọng nói đích.

Để xây dựng i-Code Studio, chúng tôi sử dụng Azure Machine Learning Studio, một môi trường phát triển dựa trên đám mây, hợp tác, kéo và thả để xây dựng, kiểm tra và triển khai các mô hình machine learning. Chúng tôi đóng gói các mô hình và dịch vụ có sẵn từ Azure Cognitive Services (ACS) như các API độc lập và triển khai chúng như một dịch vụ web tích hợp để gọi thời gian thực. Theo cách này, nó cho phép các nhà phát triển linh hoạt kết hợp chúng để xây dựng ứng dụng riêng của họ. Chi tiết hơn về các mô hình nền tảng và dịch vụ có sẵn được trình bày trong Phụ lục A và B.

4 Đánh giá

Trong phần này, chúng tôi trình bày các thí nghiệm của chúng tôi trong ba tác vụ bao gồm các phương thức ngôn ngữ, giọng nói và thị giác: 1) truy xuất video-to-text; 2) trả lời câu hỏi thị giác và 3) dịch speech-to-speech.

4.1 Truy xuất Video-to-Text

Tác vụ truy xuất Video-to-Text là chọn văn bản có liên quan nhất từ một nhóm ứng viên cho trước video, thường liên quan đến tất cả các phương thức qua ngôn ngữ, thị giác và giọng nói. Do đó, nó có thể là một tác vụ lý tưởng để kiểm tra khả năng của i-Code Studio. Theo Zeng et al. (2022), pipeline được tổ chức thành các bước sau: (i) tính điểm tương tự s1 giữa đặc trưng thị giác trung bình của video và đặc trưng văn bản của chú thích thông qua dịch vụ ACS Vision (Yuan et al., 2021); (ii) gọi dịch vụ ACS Speech để phiên âm video thành văn bản; (iii) tóm tắt bản phiên âm với các dịch vụ Azure OpenAI sử dụng GPT-3 (Brown et al., 2020); (iv) tính điểm tương tự dựa trên văn bản s2 giữa bản tóm tắt được tạo và chú thích với mô hình ngôn ngữ được huấn luyện trước; (v) tính điểm liên quan cuối cùng s=s1×s2, kết hợp điểm dựa trên thị giác-văn bản và điểm dựa trên giọng nói-văn bản; (vi) chọn văn bản có điểm liên quan cao nhất làm câu trả lời.

[THIS IS TABLE: Bảng 1 shows video-to-text retrieval results on MSR-VTT dataset with different methods and their R@1, R@5, R@10 scores]

Bảng 1 cho thấy kết quả của chúng tôi trên MSR-VTT (Xu et al., 2016), đây là tập dữ liệu quy mô lớn phổ biến nhất cho truy xuất video-to-text và bao gồm 10.000 video clip từ 20 danh mục, và mỗi video clip được chú thích với 20 câu tiếng Anh bởi Amazon Mechanical Turks. Chúng tôi sử dụng các chỉ số recall tiêu chuẩn để đánh giá và so sánh phương pháp của chúng tôi với cả phương pháp tinh chỉnh và zero-shot. Chúng ta có thể thấy rằng trong setting zero-shot, i-Code Studio vượt trội hơn SOTA trước đó (SMs) 5.1 điểm trong R@1, do đó đạt được SOTA mới trong setting này. So với phương pháp tinh chỉnh, i-Code Studio đã thu hẹp đáng kể khoảng cách giữa phương pháp zero-shot và tinh chỉnh, cho thấy tiềm năng của phương pháp zero-shot.

4.2 Trả lời Câu hỏi Thị giác

i-Code Studio có thể được sử dụng để trả lời các câu hỏi thị giác (xem Hình 3). Cụ thể, Florence của Azure Cognitive Services (Yuan et al., 2021) được sử dụng để zero-shot phát hiện một tập hợp các danh mục đối tượng trong hình ảnh đầu vào, tạo ra một tập hợp các thẻ liên quan đến nó, và tạo ra một chú thích mô tả hình ảnh. Các mô tả này và câu hỏi đầu vào sau đó được sử dụng để tạo thành một lời nhắc ngôn ngữ có thông tin VLM, được đưa vào ChatGPT để dự đoán câu trả lời. Chúng tôi đánh giá hiệu suất của i-Code Studio trên tập dữ liệu FVQA (Wang et al., 2017) cho tác vụ trả lời câu hỏi thị giác. FVQA là một tập dữ liệu VQA chủ yếu chứa các câu hỏi yêu cầu kiến thức bên ngoài để trả lời, và cung cấp các bộ ba fact hỗ trợ cùng với các bộ ba hình ảnh-câu hỏi-câu trả lời. Theo (Wang et al., 2017), chúng tôi sử dụng 1.090 hình ảnh test, tương đương với 2.899 câu hỏi. Kết quả của chúng tôi được trình bày trong Bảng 2. i-Code Studio vượt trội đáng kể so với Fact-based VQA mà không có support facts từ tập dữ liệu,

--- TRANG 5 ---
có khả năng do sức mạnh của mô hình nền tảng thị giác Florence và khả năng của ChatGPT trong việc trả lời các câu hỏi yêu cầu kiến thức bên ngoài.

[THIS IS TABLE: Bảng 2 showing VQA results on FVQA dataset with methods and accuracy scores]
Phương pháp | Độ chính xác
Human | 77.99
Fact-based VQA (Wang et al., 2017) | 56.91
Fact-based VQA (Ensemble) (Wang et al., 2017) | 58.76
i-Code Studio | 60.59

Hình 3: VQA với i-Code Studio: một lời nhắc ngôn ngữ có thông tin VLM được tạo ra sử dụng đầu ra Florence và câu hỏi đầu vào. Văn bản gạch chân màu đỏ là chú thích, danh mục đối tượng, và thẻ được phát hiện bởi Florence. Lời nhắc sau đó được đưa vào ChatGPT để dự đoán câu trả lời.

4.3 Dịch Speech-to-Speech

Tác vụ dịch Speech-to-speech bao gồm việc phiên âm ngôn ngữ nói thành văn bản, dịch văn bản sang ngôn ngữ khác, và sau đó tạo giọng nói trong ngôn ngữ đích. Chúng tôi sử dụng tác vụ này để đánh giá khả năng đa ngôn ngữ và giọng nói của i-Code Studio. Cụ thể, chúng tôi đầu tiên tận dụng dịch vụ ACS Speech Recognition để phiên âm giọng nói đến, sau đó sử dụng dịch vụ ACS Language Machine Translation để dịch sang các ngôn ngữ đích, và cuối cùng gọi ACS Text-To-Speech để tổng hợp giọng nói trong các ngôn ngữ đích.

Chúng tôi đánh giá i-Code Studio trên tập dữ liệu CVSS (Jia et al., 2022), một corpus dịch speech-to-speech đa ngôn ngữ-to-English quy mô lớn.

[THIS IS TABLE: Bảng 3 showing speech-to-text evaluation results on CVSS dataset]
Mô hình | All | Hi-Res | Lo-Res
Li et al. (2021) (Scratch-BL) | - | 14.8 | -
Wang et al. (2021a) (A2A-L) | 7.5 | 24.0 | 3.7
Wang et al. (2021a) (A2E-M, arXiv) | - | 24.5 | -
Jia et al. (2022) | 11.0 | 29.4 | 6.7
Jia et al. (2022) (ASR pre-training) | 13.3 | 31.4 | 9.0
i-Code Studio | 35.8 | 39.7 | 34.8

Nó bao gồm các cặp dịch speech-to-speech song song cấp câu từ 21 ngôn ngữ sang tiếng Anh và được lấy từ corpus giọng nói Common Voice (Ardila et al., 2020) và corpus dịch speech-to-text CoVoST 2 (Wang et al., 2020). Giọng nói dịch trong CVSS được tổng hợp với hai mô hình TTS tối tân được huấn luyện trên corpus LibriTTS. Vì chất lượng tạo giọng nói được đo bằng con người trong điểm ý kiến trung bình (MOS) về các chỉ số tự nhiên và tương tự người nói, ở đây chúng tôi chỉ báo cáo kết quả văn bản dịch trong chỉ số BLEU sử dụng SacreBLEU với cấu hình mặc định của nó. Theo Jia et al. (2022), chúng tôi nhóm kết quả đánh giá trên các ngôn ngữ nguồn tài nguyên cao (tiếng Pháp, Đức, Catalan và Tây Ban Nha) và những ngôn ngữ tài nguyên thấp (tất cả những ngôn ngữ còn lại). Từ Bảng 3, chúng ta có thể thấy i-Code Studio vượt trội đáng kể so với các SOTA trước đó 22.5 điểm trung bình. Sự cải thiện của các ngôn ngữ tài nguyên cao vẫn có khoảng 8.3 điểm, chứng minh khả năng mạnh mẽ của framework i-Code Studio.

5 Ứng dụng: Tác nhân Đa phương thức

Là con người, chúng ta có một hệ thống cảm giác phức tạp cho phép chúng ta trải nghiệm thế giới xung quanh. Chúng ta sử dụng mắt để nhìn, tai để nghe, miệng để nói, và não để xử lý và diễn giải thông tin chúng ta nhận được. Lấy cảm hứng từ điều này, chúng tôi sử dụng i-Code Studio để xây dựng một tác nhân đa phương thức có thể giao tiếp và cá nhân hóa cho người dùng. Cụ thể, mắt của tác nhân sử dụng các dịch vụ Azure Vision để diễn giải tín hiệu hình ảnh thị giác và gửi tín hiệu đến não; tai và miệng sử dụng các dịch vụ Azure Speech để thu thập sóng âm và tạo ra âm thanh; não tận dụng các dịch vụ Azure OpenAI để tích hợp tất cả các tín hiệu cảm giác nhận được từ mắt, tai và sử dụng chúng để đưa ra quyết định. Hệ thống kết nối này

--- TRANG 6 ---
Hình 4: Tổng quan về tác nhân đa phương thức được xây dựng sử dụng i-Code Studio.

Hình 5: i-Code Studio có thể được sử dụng để xây dựng một trợ lý ảo đa phương thức. Trong cuộc trò chuyện, đầu vào của người dùng và ngữ cảnh lịch sử được thêm vào trước với các chú thích/thẻ từ Florence vision (hiển thị màu đỏ) và được đưa vào làm đầu vào cho GPT-3. Các hộp dưới cùng hiển thị cuộc trò chuyện cũng như hai ảnh chụp màn hình của video đầu vào từ camera.

của các cơ quan cảm giác và não là điều cho phép các tác nhân đa phương thức của chúng ta hiểu và tương tác với thế giới xung quanh. Tác nhân đa phương thức của chúng tôi là một trợ lý ảo với "mắt" (Florence), "tai" (ACS ASR), "não" (ví dụ ChatGPT và GPT-3) và miệng (ACS TTS). i-Code Studio tích hợp các tín hiệu giọng nói và thị giác từ người dùng bằng cách tổ hợp và cấu hình các dịch vụ từ ACS và OpenAI. Hình 5 cho thấy một ví dụ demo. Sử dụng prompting ngôn ngữ có thông tin VLM, i-Code Studio có thể cho phép đối thoại đa phương thức giữa người dùng và tác nhân. GUI gọi i-Code Studio một lần để đơn giản hóa chi phí phát triển trong khi mang lại trải nghiệm người dùng nhất quán.

6 Kết luận

i-Code Studio, là một framework có thể cấu hình và tổ hợp mới cho AI Tích hợp. Nó điều phối nhiều mô hình được huấn luyện trước để thực hiện các tác vụ đa phương thức phức tạp, mà không cần tinh chỉnh. Chúng tôi đã cho thấy i-Code Studio có thể đạt được kết quả ấn tượng trên ba tác vụ đa phương thức. Chúng tôi cũng đã trình bày cách xây dựng một tác nhân trợ lý ảo đa phương thức với i-Code Studio. Với nghiên cứu và phát triển thêm, i-Code Studio có thể được mở rộng để trở nên linh hoạt và mạnh mẽ hơn để tạo ra những ứng dụng phức tạp hơn nữa.

--- TRANG 7 ---
7 Video Screencast

Trong phần này, liên kết công khai đến một trong những demo ví dụ của chúng tôi cho tác nhân đa phương thức được cung cấp¹.

8 Hạn chế

i-Code Studio hiện tại dựa vào một số lượng hạn chế các mô hình và dịch vụ được huấn luyện trước. Mặc dù điều này đủ cho nhiều tác vụ đa phương thức, framework cần thêm các dịch vụ để hỗ trợ các tác vụ đa phương thức phức tạp hơn. Hơn nữa, để chứng minh khả năng của i-Code Studio, chúng tôi cần áp dụng framework cho các tác vụ đa phương thức phức tạp hơn như tóm tắt cuộc họp và tạo hình ảnh từ mô tả văn bản.

Tài liệu tham khảo

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198.

Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, và Gregor Weber. 2020. Common voice: A massively-multilingual speech corpus. Trong Proceedings of the Twelfth Language Resources and Evaluation Conference, trang 4218–4222, Marseille, France. European Language Resources Association.

Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.

Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, và Furu Wei. 2022. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):1505–1518.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,

¹https://drive.google.com/file/d/10lEZQ9LbQpR_kc8zsmenRXjCQejSO-G3/view?usp=share_link

Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, và Yonghui Wu. 2021. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. Trong 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), trang 244–250. IEEE.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Han Fang, Pengfei Xiong, Luhui Xu, và Yu Chen. 2021. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097.

Pengcheng He, Xiaodong Liu, Jianfeng Gao, và Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. Trong International Conference on Learning Representations.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.

Ronghang Hu và Amanpreet Singh. 2021. Unit: Multimodal multitask learning with a unified transformer. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision, trang 1439–1449.

Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. 2022. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, và Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. Trong International Conference on Machine Learning, trang 4904–4916. PMLR.

Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, và Heiga Zen. 2022. CVSS corpus and massively multilingual speech-to-speech translation. Trong Proceedings

--- TRANG 8 ---
of Language Resources and Evaluation Conference (LREC), trang 6691–6703.

Shuang Li, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, và Igor Mordatch. 2022. Composing ensembles of pre-trained models via iterative consensus. arXiv preprint arXiv:2210.11522.

Xian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, và Michael Auli. 2021. Multilingual speech translation from efficient finetuning of pretrained models. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), trang 827–838, Online. Association for Computational Linguistics.

Jian Liang, Chenfei Wu, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, và Nan Duan. 2022. NUWA-infinity: Autoregressive over autoregressive generation for infinite visual synthesis. Trong Advances in Neural Information Processing Systems.

Y. Liu, S. Albanie, A. Nagrani, và A. Zisserman. 2019. Use what you have: Video retrieval using representations from collaborative experts. Trong arXiv preprint arxiv:1907.13487.

Niluthpol Chowdhury Mithun, Juncheng Li, Florian Metze, và Amit K. Roy-Chowdhury. 2018. Learning joint embedding with multimodal cues for cross-modal video-text retrieval. Trong Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, ICMR '18, trang 19–27, New York, NY, USA. Association for Computing Machinery.

Jesús Andrés Portillo-Quintero, José Carlos Ortiz-Bayliss, và Hugo Terashima-Marín. 2021. A straightforward framework for video retrieval using clip. Trong Pattern Recognition: 13th Mexican Conference, MCPR 2021, Mexico City, Mexico, June 23–26, 2021, Proceedings, trang 3–12, Berlin, Heidelberg. Springer-Verlag.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021a. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, trang 8748–8763. PMLR.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021b. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, trang 8748–8763. PMLR.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, và Ilya Sutskever. 2021. Zero-shot text-to-image generation. Trong International Conference on Machine Learning, trang 8821–8831. PMLR.

Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487.

Yoad Tewel, Yoav Shalev, Idan Schwartz, và Lior Wolf. 2022. Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang 17918–17928.

Changhan Wang, Anne Wu, Jiatao Gu, và Juan Pino. 2021a. Covost 2 and massively multilingual speech translation. Trong Interspeech, trang 2247–2251.

Changhan Wang, Anne Wu, và Juan Pino. 2020. Covost 2 and massively multilingual speech-to-text translation. arXiv preprint arXiv:2007.10310.

Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111.

Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, và Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413–2427.

Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. 2022. Language models with image descriptors are strong few-shot video-language learners. arXiv preprint arXiv:2205.10747.

Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, và Yuan Cao. 2021b. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904.

Jun Xu, Tao Mei, Ting Yao, và Yong Rui. 2016. Msr-vtt: A large video description dataset for bridging video and language. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 5288–5296.

Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian, Mei Gao, Yi-Ling Chen, et al. 2022. i-code: An integrative and composable multimodal learning framework. arXiv preprint arXiv:2205.01818.

Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence:

--- TRANG 9 ---
A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.

Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

--- TRANG 10 ---
A Các Mô hình Nền tảng

Các mô hình nền tảng, lần đầu được giới thiệu bởi Bommasani et al. (2021), đề cập đến bất kỳ mô hình nào được huấn luyện trước trên dữ liệu rộng lớn ở quy mô lớn và có thể được thích ứng cho một loạt các tác vụ downstream. Như một mô hình chung của AI, các mô hình nền tảng đã cho thấy hiệu suất ấn tượng và khả năng tổng quát hóa trong các phương thức khác nhau (Brown et al., 2020; Radford et al., 2021b; Yuan et al., 2021).

Các Mô hình Ngôn ngữ Lớn Các mô hình ngôn ngữ lớn (LM), được huấn luyện trên các bộ sưu tập văn bản khổng lồ như BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), DeBERTa (He et al., 2021), đạt được hiệu suất tối tân trên nhiều benchmark xử lý ngôn ngữ tự nhiên. Các công trình gần đây hơn, như GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), PaLM (Chowdhery et al., 2022), Chinchilla (Hoffmann et al., 2022), đã cho thấy khả năng emergent đáng ngạc nhiên để tạo văn bản và có thể được "prompted" để thực hiện một loạt các tác vụ ngôn ngữ với zero hoặc vài ví dụ của tác vụ làm đầu vào. Trong framework i-Code Studio, chúng tôi bao gồm ba mô hình nền tảng dựa trên ngôn ngữ để hỗ trợ các tác vụ và ứng dụng đa dạng: Z-Code² cho các tác vụ đa ngôn ngữ như dịch máy, GPT-3 (Brown et al., 2020) và ChatGPT³ cho các tác vụ NLP chung như tóm tắt văn bản và trả lời câu hỏi.

Các Mô hình Ngôn ngữ Thị giác Các mô hình ngôn ngữ thị giác (Vision LM), được huấn luyện trên dữ liệu hình ảnh-văn bản và video quy mô web, như CLIP (Radford et al., 2021b), ALIGN (Jia et al., 2021), DALL-E (Ramesh et al., 2021), Imagen (Saharia et al., 2022) và Nuwa-infinity (Liang et al., 2022), chứng minh hiệu suất vượt trội trên các tác vụ thị giác máy tính khác nhau, như phân loại, truy xuất, phát hiện đối tượng, VQA, chú thích hình ảnh, truy xuất video và nhận dạng hành động. Trong Azure Cognitive Services, Project Florence⁴ được khởi xướng để thúc đẩy các công nghệ thị giác máy tính tối tân và phát triển framework thế hệ tiếp theo cho nhận dạng thị giác. Cụ thể, Florence (Yuan et al., 2021) được huấn luyện trên dữ liệu Web-scale nhiễu end-to-end với một mục tiêu thống nhất, cho phép mô hình đạt được hiệu suất tối tân trên một

²https://www.microsoft.com/en-us/research/project/project-zcode/
³https://chat.openai.com/
⁴https://www.microsoft.com/en-us/research/project/projectflorence/

loạt rộng các benchmark. Trong i-Code Studio, Florence được sử dụng như mô hình nền tảng thị giác.

Các Mô hình Ngôn ngữ Âm thanh Các mô hình ngôn ngữ âm thanh tận dụng các token/code âm thanh được rời rạc hóa để huấn luyện một mô hình bằng cách sử dụng một tác vụ mô hình hóa ngôn ngữ, như w2v-BERT (Chung et al., 2021), WavLM (Chen et al., 2022), và Vall-E (Wang et al., 2023), và mang lại cải thiện đáng kể cho các tác vụ xử lý giọng nói khác nhau như speech-to-text, text-to-speech, nhận dạng/phân loại người nói, tách giọng nói, v.v. Trong Azure Cognitive Speech Services, các mô hình giọng nói được huấn luyện bằng cách sử dụng hơn vài trăm nghìn giờ âm thanh giọng nói theo cách học có giám sát.

B Các Dịch vụ Machine Learning

Một dịch vụ machine learning thường được xây dựng trên đỉnh các mô hình nền tảng, cung cấp một bộ công cụ và dịch vụ trí tuệ nhân tạo (AI) và machine learning (ML) dựa trên đám mây toàn diện. Các công cụ này cung cấp cho các nhà phát triển các thuật toán và API được xây dựng sẵn, dễ sử dụng có thể được tích hợp vào một loạt rộng các ứng dụng. i-Code Studio áp dụng Azure Cognitive Services⁵, cung cấp nhiều mô hình và dịch vụ cho các phương thức khác nhau. Các nhà phát triển có thể dễ dàng tận dụng các dịch vụ Azure Cognitive để thêm các tính năng thông minh vào ứng dụng của họ, như phân tích cảm xúc, phát hiện đối tượng, nhận dạng giọng nói và text-to-speech, mà không cần phải xây dựng các mô hình AI từ đầu.

Chúng tôi bao gồm các dịch vụ sau cho mỗi phương thức trong một framework để kiến trúc của chúng tôi có thể linh hoạt cho phép các ứng dụng phức tạp khó tạo ra với phương pháp end-to-end và đồng thời cung cấp cho người dùng trải nghiệm nhất quán. i-Code Studio áp dụng thiết kế của prompt learning [cite] để nhanh chóng thích ứng kiến trúc với các tác vụ khác nhau thông qua prompting đa phương thức có thông tin với chỉ vài ví dụ được gán nhãn.

Ngôn ngữ Azure Cognitive Services for Language (ACS Language) là một dịch vụ dựa trên đám mây cung cấp các tính năng Xử lý Ngôn ngữ Tự nhiên (NLP) để hiểu và tạo sinh bằng cách sử dụng REST API và thư viện client. Sử dụng Z-Code làm backbone, các dịch vụ ngôn ngữ cung cấp các chức năng sau: hiểu ngôn ngữ tự nhiên, trả lời câu hỏi, tóm tắt văn bản

⁵https://azure.microsoft.com/en-us/products/cognitive-services/#overview

--- TRANG 11 ---
và dịch máy. Bên cạnh đó, chúng tôi cũng tích hợp Azure OpenAI Services sử dụng ChatGPT, GPT-3, Codex và Embeddings từ OpenAI làm backbone để cho phép các khả năng lý luận và hiểu biết mới để xây dựng các ứng dụng tiên tiến. Cụ thể, trong kiến trúc của chúng tôi, chúng tôi bao gồm ba API ngôn ngữ: (i) dịch máy: dịch văn bản từ ngôn ngữ này sang ngôn ngữ khác. Điều này có thể được sử dụng để thực hiện giao tiếp đa ngôn ngữ giữa con người và máy. (ii) ChatGPT: một mô hình ngôn ngữ đối thoại tương tác; (iii) GPT-3: có khả năng thực hiện một loạt rộng các tác vụ NLP như tạo văn bản, dịch thuật, tóm tắt và trả lời câu hỏi.⁶

Giọng nói Azure Cognitive Speech Service (ACS Speech) cung cấp khả năng giọng nói với tài nguyên Azure Speech. Nó có thể chính xác phiên âm giọng nói đa ngôn ngữ-to-text, tạo ra text-to-speech với giọng nói giống con người thật, dịch âm thanh nói, và nhận dạng chính xác người nói trong cuộc trò chuyện. Chúng tôi tích hợp hai API giọng nói trong kiến trúc của chúng tôi: (i) Speech-to-Text, để phiên âm giọng nói của bạn thành văn bản thời gian thực hoặc phiên âm các tệp âm thanh được ghi thành văn bản; (ii) Text-to-Speech, để chuyển đổi văn bản đầu vào thành giọng nói tổng hợp thời gian thực hoặc tạo các tệp âm thanh từ văn bản với giọng nói tự nhiên được xây dựng sẵn hoặc tùy chỉnh.

Thị giác Azure Cognitive Services for Vision (ACS Vision) là một tập hợp các dịch vụ được Microsoft Azure cung cấp cho phép các nhà phát triển thêm khả năng thị giác máy tính vào ứng dụng của họ. Nó cung cấp một loạt dịch vụ cho các tác vụ như phát hiện và nhận dạng đối tượng, phân tích hình ảnh, nhận dạng ký tự quang học (OCR), và nhận dạng khuôn mặt. Chúng tôi tích hợp hai API thị giác trong kiến trúc của chúng tôi: (i) phát hiện đối tượng: nhận dạng các đối tượng trong hình ảnh và định vị hộp bao quanh trong khung. (ii) chú thích hình ảnh: tạo ra mô tả về toàn bộ hình ảnh bằng ngôn ngữ con người có thể đọc được, sử dụng các câu hoàn chỉnh.

⁶Đối với GPT-3, chúng tôi sử dụng mô hình text-davinci-003 cho các tác vụ và ứng dụng downstream.
