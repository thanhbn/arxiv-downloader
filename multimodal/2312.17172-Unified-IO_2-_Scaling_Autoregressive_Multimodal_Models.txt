# 2312.17172.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2312.17172.pdf
# File size: 14617196 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Unified-IO 2: Scaling Autoregressive Multimodal Models
with Vision, Language, Audio, and Action
Jiasen Lu1*Christopher Clark1*Sangho Lee1*Zichen Zhang1*
Savya Khosla2Ryan Marten2Derek Hoiem2Aniruddha Kembhavi13
1Allen Institute for AI2University of Illinois Urbana-Champaign3University of Washington
{jiasenl, chrisc, sanghol, chralesz, anik }@allenai.org
unified-io-2.allenai.org
What food could you make with these ingredients ? Give me the recipe.One delicious recipe using these ingredients is chocolate pudding! Here's the recipe:Ingredients:- 1 cup all-purpose flour, - ½ cup sugar ….Instructions:1. In a large bowl whisk together the flour, sugar..2. In a separate bowl, mix together the eggs …
Generate an audio track for this band.Generate an audio track for this band.
Render a sunset
Remove the dockPaint this image like Van Gogh
Generate an image of a car with the model in the first image and the color in the second image.
Generate a depth imageGenerate a depth & surface normal mapGenerate a surface normal map
Which fruits are in this image ? List them in json format with the name of the fruit as the key and the color of the fruit as the value.{"banana" : "Yellow","apple" : "Red","grapes" : "Green"}
Segment grapes and one apple
Put the less kobar blicket into the dax.= blicket= daxis kobar than
Given the initial image and a sequence of actions, predict the next frames
GeneratedImages
Generate an image of an astronaut riding a horse in the forest. There is a river in front of them with water lilies. Generate an image of an elephant swimming underwater. aesthetic. Fantasy.
Add the missing details to the masked image (left) using the reference image (right).
Find the visible keypoints corresponding to the person located in the highlighted region.
Identify the locations of the instruments producing the given sound.(drum sounds)Image Editing
Image GenerationReference Image Generation
Multiview Image Completion
Depth & Surface Normal Keypoint EstimationVisual ParsingFree Form VQA
Visual based Audio GenerationVisual Audio Localization
Robotic ManipulationFuture Frame Predictionturn rightturn rightturn rightmove ahead
move aheadmove aheadmove aheadturn rightSegmentation&
Figure 1. U NIFIED -IO 2 is an instruction-following model with a huge breadth of abilities and supported modalities. It can generate images
(red box), including image editing, image generation, depth estimation, surface normal estimation, and future frame prediction etc. It can
also generate texts (blue box), including long-form answers to queries, keypoint estimation, visual audio localization, predicting actions
for robotic manipulation etc. It can generate audio (green box) from images or text. Click andfor the corresponding audio samples.
* Leading Authors, equal contribution. A description of each author’s
contribution is available in Appendix A. Corresponding to Jiasen Lu.
1arXiv:2312.17172v1  [cs.CV]  28 Dec 2023

--- PAGE 2 ---
Abstract
We present UNIFIED -IO 2 , the first autoregressive multi-
modal model that is capable of understanding and generat-
ing image, text, audio, and action. To unify different modal-
ities, we tokenize inputs and outputs – images, text, audio,
action, bounding boxes etc., into a shared semantic space
and then process them with a single encoder-decoder trans-
former model. Since training with such diverse modalities
is challenging, we propose various architectural improve-
ments to stabilize model training. We train our model from
scratch on a large multimodal pre-training corpus from di-
verse sources with a multimodal mixture of denoisers objec-
tive. To learn an expansive set of skills, such as following
multimodal instructions, we construct and finetune on an
ensemble of 120 datasets with prompts and augmentations.
With a single unified model, UNIFIED -IO 2 achieves state-
of-the-art performance on the GRIT benchmark and strong
results in more than 35 benchmarks, including image gener-
ation and understanding, natural language understanding,
video and audio understanding, and robotic manipulation.
We release all our models to the research community.
1. Introduction
As AI researchers, we seek to build intelligent agents that
can perceive their environment, communicate with others,
act in the world, and reason about their interactions. The
world is multimodal, so our agents must partake in rich
interactions that are multimodal in nature via vision, lan-
guage, sound, action etc. Psychologists have argued that
the redundancy of our sensory systems serves as supervi-
sory mechanisms to improve each other [48, 144, 167]. This
provides a natural motivation to create models with similar
learning capabilities, supporting many different modalities
that can supervise each other during training.
Building models that can parse and produce many
modalities is a complex undertaking. Training Large Lan-
guage Models (LLMs) with billions of parameters, despite
only supporting a single modality, is extremely challenging
across many fronts – from sourcing and processing massive
datasets, ensuring data quality and managing biases, design-
ing effective model architectures, maintaining stable train-
ing processes, and instruction tuning to enhance the model’s
ability to follow and understand user instructions. These
challenges are hugely amplified with the addition of each
new modality.
In light of these difficulties, a line of recent works
in building multimodal systems has leveraged pre-trained
LLMs, with some augmenting with new modality encoders
[5, 46, 119], some adding modality specific decoders [14,
96] and others leveraging the LLM’s capabilities to build
modular frameworks [64, 166, 173]. Another line of works
on training multimodal models from scratch has focused ongenerating text output [81, 143] with a few recent works
supporting the understanding and generation of two modal-
ities – text and images [123, 125]. Building generative mod-
els with a wider coverage of modalities, particularly when
training from scratch, remains an open challenge.
In this work, we present U NIFIED -IO 2, a large multi-
modal model (LMM) that can encode text, image, audio,
video, and interleaved sequences and produce text, action,
audio, image, and sparse or dense labels. It can output free-
form multimodal responses and handle tasks unseen during
training through instruction-following. U NIFIED -IO 2 con-
tains 7 billion parameters and is pre-trained from scratch on
an extensive variety of multimodal data – 1 billion image-
text pairs, 1 trillion text tokens, 180 million video clips,
130 million interleaved image & text, 3 million 3D assets,
and 1 million agent trajectories. We further instruction-tune
the model with a massive multimodal corpus by combining
more than 120 datasets covering 220 tasks across vision,
language, audio, and action.
Our pre-training and instruction tuning data, totaling
over 600 terabytes, presents significant challenges for train-
ing due to its diversity and volume. To effectively facilitate
self-supervised learning signals across multiple modalities,
we develop a novel multimodal mixture of denoiser objec-
tive that combines denoising and generation across modali-
ties. We also develop dynamic packing – an efficient imple-
mentation that provides a 4x increase in training throughput
to deal with highly variable sequences. To overcome the
stability and scalability issues in training, we propose to ap-
ply key architectural changes, including 2D rotary embed-
dings, QK normalization, and scaled cosine attention mech-
anisms on the perceiver resampler. For instruction tuning,
we ensure every task has a clear prompt, either using exist-
ing ones or crafting new ones. We also include open-ended
tasks and create synthetic tasks for less common modalities
to enhance task and instruction variety.
We evaluate U NIFIED -IO 2 on over 35 datasets across
the various modalities it supports. Our single model sets the
new state of the art on the GRIT [66] benchmark, which in-
cludes diverse tasks such as keypoint estimation and surface
normal estimation. On vision & language tasks, it matches
or outperforms the performance of many recently proposed
VLMs that leverage pre-trained LLMs. On image genera-
tion, it outperforms the closest competitor [174] that lever-
ages the pre-trained stable diffusion model [154], especially
in terms of faithfulness as per the metrics defined in [76]. It
also shows effectiveness in video, natural language, audio,
and embodied AI tasks, showcasing versatility despite its
broad capability range. Moreover, U NIFIED -IO 2 can fol-
low free-form instructions, including novel ones. Figure 1
offers a glimpse into how it handles various tasks. Further
examples, along with the code and models, are accessible
on our project website.
2

--- PAGE 3 ---
2. Related Work
Inspired by the success of language models as general-
purpose text processing systems [20, 122, 177], there has
been a recent wave of multimodal systems trying to achieve
similar general-purpose capabilities with additional modali-
ties. A common approach is to use a vision-encoder to build
features for input images and then an adapter to map those
features into embeddings that can be used as part of the in-
put to an LLM. The network is then trained on paired im-
age/language data to adapt the LLM to the visual features.
These models can already perform some tasks zero-shot or
with in-context examples [109, 132, 178], but generally a
second stage of visual instruction tuning follows using in-
structions, visual inputs, and target text triples to increase
zero-shot capabilities [25, 34, 118, 119, 205, 218, 225].
Building upon this design, many researchers have ex-
panded the breadth of tasks these models can support. This
includes creating models that can do OCR [12, 220], vi-
sual grounding [12, 26, 143, 189, 207, 212, 219], image-
text-retrieval [97], additional languages [112], embodied
AI tasks [17, 135, 140, 152] or leverage other expert sys-
tems [52]. Other efforts have added new input modali-
ties. This includes video inputs [110, 126], audio [80] or
both [216]. PandaGPT [170] and ImageBind-LLM [69] use
the universal encoder ImageBind [56] to encode many kinds
of input modalities, and ChatBridge [222] uses a similar
universal encoder based on language. While these efforts
are effective for understanding tasks, they do not allow com-
plex multimodal generation and often exclude modalities
long considered central to computer vision ( e.g., ImageBind
cannot support sparse annotation of images).
Fewer works have considered multimodal generation.
UNIFIED -IO [123], LaVIT [88], OFA [186], Emu [172]
and CM3Leon [210] train models to generate tokens that
a VQ-GAN [49, 179] can then decode into an image,
while GILL [96], Kosmos-G [141] and SEED [53] gener-
ate features that a diffusion model can use, and JAM [4]
fuses pre-trained language and image generation models.
UNIFIED -IO 2 also uses a VQ-GAN, but supports text, im-
age, and audio generation.
Overall, this shows a strong trend towards expanding the
number of supported tasks and modalities. U NIFIED -IO 2
pushes this trend to its limit, including the capabilities of
these prior works with few exceptions and the ability to gen-
erate outputs in more modalities. Recently, CoDi [174] also
achieved similar any-to-any generation capabilities by using
multiple independently trained diffusion models and align-
ing their embedding spaces. U NIFIED -IO 2 has stronger
language abilities and can perform well on many more
tasks.
A notable feature of U NIFIED -IO 2 is that the model
is trained from scratch instead of being initialized with a
pre-trained LLM. Prior works [114, 186, 188, 192] fol-lowing this approach are typically not designed to produce
complex generations like free-form text responses, images
or sounds, or follow text instructions. Compared to re-
cent general-purpose multimodals models [81, 143, 210],
UNIFIED -IO 2 has a significantly broader scope of tasks
and outputs. Training from scratch means that the method
can be reproduced without a costly preliminary stage of lan-
guage model pre-training and is a more natural fit for how
humans learn modalities simultaneously through their co-
occurrences, not one at a time.
3. Approach
In this section, we discuss the unified task representation
(3.1), the model architecture and techniques to stabilize
training (3.2), the multimodal training objective (3.3) and
the efficiency optimizations (3.4) used in U NIFIED -IO 2.
3.1. Unified Task Representation
UNIFIED -IO 2 processes all modalities with a single, uni-
fied encoder-decoder transformer [181]. This is achieved
by encoding various inputs and outputs – images, text, au-
dio, action, boxes etc., into sequences of tokens in a shared
representation space. Our encoding procedure follows the
design of U NIFIED -IO [123], with several modifications to
improve performance and new encoders and decoders for
additional modalities. Figure 2 shows an overview of the
model. Details about how modalities are encoded are given
below.
Text, Sparse Structures, and Action. Text inputs and out-
puts are tokenized using the byte-pair encoding [161] from
LLaMA [177], which we chose since it supports Unicode
symbols and preserves whitespace. Sparse structures such
as bounding boxes, keypoints, and camera poses are dis-
cretized and then encoded using 1000 special tokens added
to the vocabulary [27, 123]. Points are encoded with a se-
quence of two such tokens (one for xand one for y), boxes
are encoded with a sequence of four tokens (upper left and
lower right corners), and 3D cuboids are represented with
12 tokens that encode the projected center, virtual depth,
log-normalized box dimension, and continuous allocentric
rotation [16]. For embodied tasks, discrete robot actions
[17] are generated as text commands ( e.g., “move ahead” to
command the robot to move forward in navigation). Spe-
cial tokens are used to encode the robot’s state, such as its
position and rotation. Details are in Appendix B.1.
Images and Dense Structures. Images are encoded with a
pre-trained Vision Transformer (ViT) [84]. We concatenate
the patch features from the second and second-to-last layers
of the ViT to capture both low and high-level visual infor-
mation. These features are passed through a linear layer
to get embeddings that can be used as part of the input se-
quence for the transformer. To generate images, we use VQ-
GAN [49] to convert images into discrete tokens. These to-
3

--- PAGE 4 ---
Text
ImageImage HistoryAudioAudioHistoryViTEncoderViTEncoderASTEncoderASTEncoderEmb
Perceiver
PerceiverVQ-GAN DecoderVQ-GANDecoderspeechenvironmentalsoundimageSNSegmentationDepth  TextActionDetection
musicLinear
LinearDynamic PackingDynamic Unpacking512576128
64256512
1024
512BPEEncodeBPE Decode
Keypoint
Unified-IO 2Figure 2. U NIFIED -IO 2 architecture. Input text, images, audio, or image/audio history are encoded into sequences of embeddings which
are concatenated and used as input to an encoder-decoder transformer model. The transformer outputs discrete tokens that can be decoded
into text, an image, or an audio clip.
kens are added to the vocabulary and then used as the target
output sequence in order to generate an image. For better
image quality, we use a dense pre-trained VQ-GAN model
with8×8patch size that encodes a 256×256image into
1024 tokens with a codebook size of 16512.
Following [123], we represent per-pixel labels, which
include depth, surface normals, and binary segmentation
masks, as RGB images that can be generated or encoded
with our image generation and encoding abilities. For seg-
mentation, U NIFIED -IO 2 is trained to predict a binary
mask given a class and bounding box. An entire image can
be segmented by first doing detection, and then querying the
model for a segmentation mask for each detected bounding
box and class. See Appendix B.1 for details.
Audio. UNIFIED -IO 2 encodes up to 4.08 seconds of au-
dio into a spectrogram (See Appendix B.1 and Table 8).
The spectrogram is then encoded with a pre-trained Audio
Spectrogram Transformer (AST) [57], and the input embed-
dings are built by concatenating the second and second-to-
last layer features from the AST and applying a linear layer
just as with the image ViT. To generate audio, we use a ViT-
VQGAN [208] to convert the audio into discrete tokens.
Since there is no public codebase, we implement and train
our own ViT-VQGAN with 8×8patch size that encodes
a256×128spectrogram into 512 tokens with a codebook
size of 8196.
Image and Audio History. We allow up to four additional
images and audio segments to be given as input, which
we refer to as the image or audio history. These elements
are also encoded using the ViT or AST, but we then use a
perceiver resampler [5], see Table 8 for hyperparameters,
to further compress the features into a smaller number of
tokens (32 for images and 16 for audio). This approach
greatly reduces the sequence length and allows the model
to inspect an image or audio segment in a high level of de-tail while using elements in the history for context. This his-
tory is used to encode previous video frames, previous audio
segments, or reference images for tasks such as multi-view
image reconstruction or image-conditioned image editing.
Eight special tokens are added to the text vocabulary and
used to reference the individual elements in these histories
in the text input or output.
3.2. Architecture
UNIFIED -IO 2 uses a transformer encoder-decoder archi-
tecture. However, we observe that using a standard im-
plementation following U NIFIED -IO leads to increasingly
unstable training as we integrate additional modalities. As
shown in Figure 3 (a) and (b), training only on image gener-
ation (green curve) results in stable loss and gradient norm
convergence. Introducing a combination of image and text
tasks (orange curve) slightly increases the gradient norm
compared to a single modality, but remains stable. How-
ever, the subsequent inclusion of the video modality (blue
curve) leads to an unrestrained escalation in the gradient
norm. When an XXL version of this model is trained on
all modalities, as shown in Figure 3 (c) and (d), the loss
explodes after 350k steps, and the next token prediction ac-
curacy significantly drops at 400k steps. To address this,
we include various architectural changes that significantly
stabilize multimodal training.
2D Rotary Embedding. Instead of relative positional
embedding [147], we apply rotary positional embeddings
(RoPE) [169] at each transformer layer. For non-text
modalities, we extend RoPE to two-dimensional positions:
For any 2D indexes (i, j), we split each of the query and
key embeddings of the transformer attention heads in half
and apply separate rotary embeddings constructed by each
of the two coordinates to the halves, see Appendix B.2.
QK Normalization. We observe extremely large values in
4

--- PAGE 5 ---
05 0 0 01 0 0 0 01 5 0 0 02 0 0 0 0681 01 21 4L o s s05 0 0 01 0 0 0 01 5 0 0 02 0 0 0 0- 1
G r a d i e n t  N o r mi m a g e - t e x ti m a g ei m a g e - t e x t - v i d e o
1 001 01
1 02
1 0
1 0 0 k2 0 0 k3 0 0 k4 0 0 k1 21 41 61 82 02 2L o s s1 0 0 k2 0 0 k3 0 0 k4 0 0 k0 . 00 . 20 . 40 . 60 . 8A c c u r a c yT e x tI m a g eA u d i o
( a )( b )( d )( c )i m a g e - t e x ti m a g ei m a g e - t e x t - v i d e oFigure 3. Left: Training loss (a) and gradient norms (b) on different modality mixtures. Right : Training loss (c) and next token prediction
accuracy (d) of UIO-2 XXLon all modalities. Results were obtained before applying the proposed architectural improvements.
0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 1 . 4
S t e p1 e 61 1 . 51 2 . 01 2 . 51 3 . 01 3 . 51 4 . 0 T r a i n i n g  L o s s1 B
3 B
7 B
Figure 4. Training loss curves for the three models, which are
pretrained with dynamic packing and a batch size of 512.
the multi-head attention logits when including image and
audio modalities, which leads to attention weights becom-
ing either 0 or 1 and contributes to training instability. To
solve this, following [38], we apply LayerNorm [10] to the
queries and keys before the dot-product attention computa-
tion.
Scaled Cosine Attention. We use perceiver resampler [86]
to compress each image frame and audio segment into a
fixed number of tokens. We found that even with QK nor-
malization, the attention logits in the perceiver can grow to
extreme values. Therefore, we apply more strict normaliza-
tion in the perceiver by using scaled cosine attention [121],
which significantly stabilizes training.
To avoid numerical instabilities, we also enable float32
attention logits. Jointly updating the pre-trained ViT and
AST can also cause instabilities. Thus, we freeze the ViT
and AST during pretraining and finetune them at the end of
instruction tuning. Figure 4 shows that the pre-training loss
for our model is stable despite the heterogeneity of input
and output modalities.
3.3. Training Objective
A strong multimodal model has to be exposed to solving
diverse sets of problems during pre-training. UL2 [175]
proposed the Mixture of Denoisers (MoD), a unified per-
spective to train LLMs, which combines the span corruption
[147] and causal language modeling [19] objectives. Moti-
vated by this, we propose a generalized and unified perspec-
tive for multimodal pre-training.
Multimodal Mixture of Denoisers. MoD uses three
paradigms: [R] – standard span corruption, [S] – causal
12345s123425(a)123452525(b)12345s23425(c)maskedtargetcurrentleakedEncoder InputDecoder InputDecoder TargetFigure 5. Different training paradigms in masked image modeling:
(a) autoregressive, (b) mask auto-encoder, (c) autoregressive with
dynamic masking. Our proposed paradigms can maintain causal
generation while avoiding information leaks in the decoder.
language modeling, and [X] – extreme span corruption.
For text targets, we follow the UL2 paradigms. For im-
age and audio targets, we define two analogous paradigms:
[R] – masked denoising where we randomly mask x% of
the input image or audio patch features and task the model
to re-construct it and [S] – where we ask the model to gen-
erate the target modality conditioned only on other input
modalities. During training, we prefix the input text with a
modality token ( [Text] ,[Image] , or[Audio] ) and a
paradigm token ( [R],[S], or[X]) to indicate the task.
Autoregressive with Dynamic Masking. One problem
with image and audio masked denoising in an autoregres-
sive manner is an information leak on the decoder side; see
Figure 5 (a). The current decoder’s input token ( 3) is condi-
tioned on enocoder’s information ( 2,5) and all previous to-
kens ( s→2) to predict target ( 4). As a result, the predicted
token will be conditioned on 1even though it was masked
in the encoder since it appears in the decoder, which will
simplify the task and harm representation learning. Sim-
ply masking the token in the decoder, as shown in Figure 5
(b), avoids this information leakage but causes the genera-
tion and de-noising tasks to interfere with one another. For
example, we found that joint training with generation (50%
MAE and 50% causal modeling) significantly reduced im-
age generation performance. Our solution is to mask the
token in the decoder except when predicting that token, as
shown in Figure 5 (c), which does not interfere with causal
prediction whilst mostly eliminating data leakage. For im-
age and audio generation, we also use row, column, and
conv-shaped masked sparse attention [148] in the decoder.
5

--- PAGE 6 ---
AudioImageText
Video
AgentTrajectories
Synthetic
Image/TextInterleaved
Video
Image/TextPairs
AgentTrajectories
Synthetic
Image/TextInterleaved
Video
Image/TextPairs
Text
EmbodiedAI
NaturalLanguageAudioUnderstandingVideoSparseLabellingVideoUnderstandingImageDenseLabellingImageSparseLabellingImageUnderstandingAudioGenerationImageGeneration
EmbodiedQA
GoalGeneration
NextFrame/StatePrediction
ActionPrediction
LanguageModeling
TextInstructionFollowing
AudioCaptioning
AudioTagging
VideoSoundLocalization
VideoActionLocalization
VideoTracking
VideoInstructionFollowing
VideoQuestionAnswering
VideoCaptioning
VideoTagging
OpticalFlow
DepthEstimation
ReferringExpression_Segmentation
SurfaceNormalEstimation
LocalizedSegmentation
Semantic_Segmentation
KeypointDetection
TextDetection
3D
ReferringExpression
ObjectLocalization
ObjectDetection
ImagePair_QA
ImageInstructionFollowing
RegionCaptioning
RelationshipPrediction
ImageTagging
RegionClassification
ImageCaptioning
VQA
AudiofromVideo
AudiofromText
View_Synthesis
ImageInpainting
NextFrameGeneration
ImageEditing
ControllableImageEditing
ImagefromTextTextImageAudioFigure 6. Distribution of pre-training and instruction tuning data. Segments proportional to sampling rates. The inner section shows the
target modality, and the outer section shows the data type. Please refer to Figure 9 and Figure 11 in the Appendix for particular datasets.
Model model dims mlp dims encoder lyr decoder lyr heads Params
UIO-2 L 1024 2816 24 24 16 1.1B
UIO-2 XL 2048 5120 24 24 16 3.2B
UIO-2 XXL 3072 8192 24 24 24 6.8B
Table 1. Size variant of U NIFIED -IO 2.
3.4. Efficient Implementation
Training on heavily multimodal data results in highly vari-
able sequence lengths for the transformer’s inputs and out-
puts, both because modalities are often missing for individ-
ual examples and because the number of tokens used to en-
code particular modalities can vary from just a few tokens
(for a sentence) to 1024 tokens (for an output image). To
handle this efficiently, we use packing, a process where the
tokens of multiple examples are packed into a single se-
quence, and the attentions are masked to prevent the trans-
former from cross-attending between examples.
Typically, packing is done during pre-processing, but it is
challenging in our setup since our encoders and decoder do
not always support it. Instead, we do packing right before
and after the transformer encoder-decoder stage, which al-
lows the modality encoders/decoder to run on the unpacked
data. During training, we use a heuristic algorithm to re-
arrange data being streamed to the model so that long ex-
amples are matched with short examples they can be packed
with. Packing optimization was also explored in [100], but
not in the streaming setup. Dynamic packing leads to an
almost 4x increase in training throughput (Details in Ap-
pendix B.3).
3.5. Optimizer
We use Adafactor [164] as our optimizer with a linear
warm-up for the first 5,000 steps and a learning rate decayof1/√
k. We train with β1= 0.9andβ2= 1.0−k−0.8,
where kis the step number. We use global norm gradient
clipping with a threshold of 1.0 and find that this is crucial
to stabilized training. Table 1 gives the details of our differ-
ent models. For all models, we train 3.0M steps – 1.5M for
pre-training and 1.5M for instruction tuning, respectively.
More details in Appendix B.4.
4. Multimodal Data
One critical difference between U NIFIED -IO 2 and prior
work is that we train the model with a diverse set of mul-
timodal data from scratch. This requires curating high-
quality, open-source multimodal data for both pre-training
(4.1) and instruction tuning (4.2).
4.1. Pre-training Data
Our pre-training data comes from various sources and cov-
ers many modalities. We provide a high-level overview and
details in Appendix C.
NLP [33%]. We use the publicly available datasets that
were employed to train MPT-7B [176]. This dataset em-
phasizes English natural language text but also contains
code and markdown. It includes text from the RedPajama
dataset [32], C4 [68], Wikipedia, and stack overflow. We
follow the proportion suggested by [176] and remove multi-
lingual and scientific data.
Image & Text [40%]. Text and image paired data comes
from LAION-400M [159], CC3M [163], CC12M [23], and
RedCaps [42]. To help train the image-history modal-
ity, we also use the interleaved image/text data from
OBELICS [104]. We use the last image as the image in-
put and the remaining images as the image history. Special
tokens are used to mark where those images occur in the
text.
6

--- PAGE 7 ---
the side there's a brick base at the bottom complete with …
1. Select target modalities [Audio][R] the side there's a brick base at the bottom complete with …
2. Select input modalities 4. Generate input mask5. Pair with Prefix token
[Audio][R] (Text, Image, Image History)VideoDataModel Inputs1.2.3.3.……(Audio)3. Select objectiveMaskAudio Denoising [R]Model Targets
…Figure 7. Construction of training samples from video data for the model’s input and target. Given the video, we first extract the video
frames and the corresponding audio spectrograms and transcript. Then, the data pass through a random selection process to determine the
target modality, input modalities, training objective, input mask etc. The model’s final input and target are shown in the top right.
Video & Audio [25%]. Video provides strong self-
supervisory signals with high correlations between audio
and visual channels. We sample audio and video data from
various public datasets including YT-Temporal-1B [215],
ACA V100M [105], AudioSet [54], WebVid-10M [13], HD-
VILA-10M [200] and Ego4D [60].
3D & Embodiment [1%]. For self-supervised 3D and em-
bodiment pre-training, we use CroCo [194] for cross-view
generation and denoising; Objaverse [40] for view synthe-
sis; and random trajectories in ProcTHOR [39] and Habi-
tat [157] for the next action and frame predictions.
Augmentation [1%]. While there is a lot of unsupervised
data on the web for images, text, video, and audio, options
are much more limited for dense and sparse annotations.
We propose to solve this through large-scale data augmen-
tation. We consider two types of data augmentation: 1. Au-
tomatically generated segmentation data from SAM [94] to
train the model to segment an object given a point or bound-
ing box. 2. Synthetic patch-detection data which tasks
the model to list the bounding boxes of synthetically added
shapes in an image. We additionally train the model to out-
put the total number of patches in the image to pre-train its
counting abilities.
Training Sample Construction. During pre-training, most
of our data contains various modalities without a supervised
target. In these cases, we randomly pick one of the modal-
ities present to be the target output. Then, we either re-
move that modality from the example or replace it with a
corrupted version. Other modalities that might be present in
the example are randomly kept or masked to force the model
to make predictions using whatever information is left. Fig-
ure 7 shows an example when using a video that contains
a sequence of image frames, the corresponding audio, and
a text transcript. The pre-training sample is constructed by
following the procedure: 1. select the target modality; 2.
select which other input modalities to keep; 3. select the
objective; 4. generate the random input mask depending on
the task of denoising or generation; 5. add a prefix tokenindicating the task.
4.2. Instruction Tuning Data
Multimodal instruction tuning is the key process to equip
the model with diverse skills and capabilities across various
modalities and even adapt to new and unique instructions.
We construct the multimodal instruction tuning dataset by
combining a wide range of supervised datasets and tasks.
We ensure every task has a clear prompt, either using exist-
ing ones or writing new ones. We also include open-ended
tasks and create synthetic tasks for less common modali-
ties to enhance task and instruction variety. Our mixture
includes 220 tasks drawn from over 120 external datasets.
We provide a high-level overview and examples here and
leave details in Appendix D.
Natural Language [25.0%]. For natural language, we use
the mixture from FlanV2 [122] and various other instruc-
tion following datasets [33, 142]. In addition, we continue
pre-training on our unsupervised NLP mixture to help pre-
vent the model from forgetting information learned from
pre-training during the extensive instruction tuning stage.
Image Generation [17.6%]. For text-to-image generation,
we use the same image & text pairs we used during pre-
training. We also include data from [102, 103, 115] that
provide better caption quality. We additionally train the
model to generate images through view synthesis [40, 194],
image editing [18, 217], segmentation-based image genera-
tion [123] and inpainting [123].
Audio Generation [7.5%]. This includes text-to-audio
datasets with audio in the wild [47, 93, 131], music [2],
and human speech [85]. We also add pre-training data with
the task of predicting the next audio clip in a video. More
specifically, we divide the audio into segments and then
generate one of them given both the text and previous seg-
ments as input.
Image Understanding [17.8%]. We include various data
sources from visual question answering [6], image tagging
[41], region classification [102], and datasets with open-
7

--- PAGE 8 ---
What are the things I should be cautious about 
when I visit here?
When visiting this dock on the water surrounded 
by a forested area, there are several factors to 
consider:
1. Visibility: The visibility around the dock may be 
limited, so it is essential to take care while 
walking on the dock to avoid any sudden shifts in 
lighting conditions or obstructions on the shore.
2. Water levels: There might be flooding or 
excessive waves that could damage the wooden 
dock or affect the safety of the area. It is crucial to 
be mindful of the water levels to …Please provide a short description of the image.
An image of dock on the water with trees in the 
background.
Please provide a detailed description of the image 
and share your personal impressions of the scene.
The image features a wooden jetty over a beautiful 
lake with numerous green trees surrounding the 
area. The scene gives off a peaceful and serene 
atmosphere. On the far side of the scene, a 
mountain range can be seen, stretching in the 
distance. In the middle of the scene, …
Make the image haunted. Make the bridge red. Make it in midnight. Make it in sunset . Imagine you are da Vinci.
Detect the mountain. Detect the dock.
Segment the dock. Surface normals
Remove the bridge.Generate music about this scene . (original)
Generate music about this scene. (haunted)
Figure 8. Our single model can perform a multitude of multimodal tasks: captioning the image, following free-form instructions, image
editing, object detection, semantic segmentation, surface normal, and image-based audio generation, etc. Here, we show the outputs of our
model for a variety of prompts. Click andfor the corresponding audio samples.
ended chat-like responses [119, 220]. We also include
the multimodal instruction tuning datasets M3IT [112] and
MIMIC-IT [107].
Video Understanding [10.6%]. We include data sources
from video captioning [190, 199], video tagging [35, 111,
168], and video question answering [196, 198]. We also use
examples from M3IT [112] and MIMIC-IT [107] for video
instruction following.
Audio Understanding [10.6%]. We include data sources
from audio tagging [24, 54], and audio captioning [47, 93].
We also include data from video action classification [7]
with audio in the dataset.
Image Sparse Labelling [7.25%]. These tasks require out-
putting sparse coordinates based on an input image. We
mainly consider object detection [115], referring expression
[91], 3D detection [16], camera pose prediction [40], text
detection [183] and human keypoints [115].
Image Dense Labelling [4.06%]. We do several image la-
beling tasks, including surface normal estimation [78, 204],
depth estimation [138], and optical flow [21, 44]. We also
train our models on various segmentation tasks, including
semantic segmentation, localization segmentation, and re-
ferring expression segmentation.
Video Sparse Labelling [3.42%]. We do video detection
[151], single object tracking [50, 79] and video action lo-
calization [61].
Embodied AI [4.33%]. For VIMA-Bench [87], we use
the image input as the initial observation of the environ-
ment and the image history for the images or videos in
the prompt. We add large-scale manipulation datasets
[63, 127, 184] with continuous control in both simulated
and real-world environments. We also train on the Point-
Nav task from Habitat Gibson scenes.The distribution of the instruction tuning data is in Fig-
ure 6. Overall, our instruction tuning mixture is composed
of 60% prompting data, meaning supervised datasets com-
bined with prompts. To avoid catastrophic forgetting, 30%
of the data is carried over from pre-training. Addition-
ally, 6% is task augmentation data we build by constructing
novel tasks using existing data sources, which enhances ex-
isting tasks and increases task diversity. The remaining 4%
consists of free-form text to enable chat-like responses.
5. Experiments
In this section, we evaluate our pre-trained and instruction-
tuned models on a broad range of tasks that require parsing
and producing all modalities: images, video, audio, text,
and actions. We do not perform task-specific finetuning
in any experiments. Details about experimental setups, ad-
ditional result details, results on natural language tasks, and
additional studies for U NIFIED -IO 2’s instruction capabili-
ties are in Appendix E.
5.1. Pre-training Evaluation
We demonstrate the effectiveness of our pre-training by
evaluating U NIFIED -IO 2 on commonsense natural lan-
guage inference (HellaSwag [214]), text-to-image gener-
ation (TIFA [76]) and text-to-audio generation (Audio-
Caps [93]). We also assess spatial and temporal under-
standing on SEED-Bench [106], a benchmark for compre-
hensively evaluating perception and reasoning on image
and video modalities. Table 2 shows that U NIFIED -IO 2
achieves comparable or even better performance on both
generation and comprehension tasks compared to the
task-specific specialist [154] or the universal multimodal
model [9].
8

--- PAGE 9 ---
Method HellaSwag ↑TIFA↑SEED-S ↑SEED-T ↑AudioCaps ↓
LLaMA-7B [177] 76.1 - - - -
OpenLLaMa-3Bv2 [55] 52.1 - - - -
SD v1.5 [154] - 78.4 - - -
OpenFlamingo-7B [9] - - 34.5 33.1 -
UIO-2 L 38.3 70.2 37.2 32.2 3.08
UIO-2 XL 47.6 77.2 40.9 34.0 3.10
UIO-2 XXL 54.3 78.7 40.7 35.0 3.02
Table 2. Zero-shot performance on commonsense sentence com-
pletion (HellaSwag [214]), text-to-image generation (TIFA [76]),
spatial and temporal comprehension (Seed-Bench [106]), and text-
to-audio generation (AudioCaps [93]).
Method Cat. Loc. Vqa Ref. Seg. KP Norm. AllAblationUIO-2 L 70.1 66.1 67.6 66.6 53.8 56.8 44.5 60.8
UIO-2 XL 74.2 69.1 69.0 71.9 57.3 68.2 46.7 65.2
UIO-2 XXL 74.9 70.3 71.3 75.5 58.2 72.8 45.2 66.9TestGPV-2 [89] 55.1 53.6 63.2 52.1 - - - -
UIO XL[123] 60.8 67.1 74.5 78.9 56.5 67.7 44.3 64.3
UIO-2 XXL 75.2 70.2 71.1 75.5 58.8 73.2 44.7 67.0
Table 3. Results on the GRIT ablation and test sets [66].
Despite extensive multitasking, the results on HellaSwag
suggest that U NIFIED -IO 2 has language modeling capa-
bilities between typical 3B and 7B language models. This
may be due to that the model sees far fewer tokens com-
pared to language-based LLMs – approximately 250 billion
tokens in total. Qualitative results of pre-training are in Ap-
pendix E.1.
5.2. GRIT Results
We evaluate on the General Robust Image Task (GRIT)
Benchmark [66], which includes seven tasks: categoriza-
tion, localization, VQA, referring expression, instance seg-
mentation, keypoint, and surface normal estimation. Com-
pleting all 7 tasks requires understanding image, text, and
sparse inputs and generating text, sparse, and dense outputs.
Although this is a subset of the modalities U NIFIED -IO 2
supports, we evaluate on GRIT because it provides a stan-
dardized and comprehensive benchmark on this set of capa-
bilities. See Appendix E.3 for additional inference details
on GRIT.
Results are shown in Table 3. Overall, U NIFIED -IO 2
is state-of-the-art on GRIT, surpassing the previous best
model, U NIFIED -IO, by 2.7 points. On individual tasks,
we can observe gains in localization (3 points), catego-
rization (14 points), segmentation (2 points), and key-
point (5 points). On VQA, our GRIT evaluations show
UNIFIED -IO 2 is better on same-source (84.6 vs. 81.2)
questions, suggesting the gap is due to reduced performance
on the new-source questions that were constructed from Vi-
sual Genome; see Appendix E.3 for additional discussion.
Despite being slightly behind U NIFIED -IO, U NIFIED -IO 2MethodImage Audio Action
FID↓TIFA↑FAD↓IS↑KL↓Succ.↑
minDALL-E [37] - 79.4 - - - -
SD-1.5 [154] - 78.4 - - - -
AudioLDM-L [117] - - 1.96 8.13 1.59 -
AudioGen [101] - - 3.13 - 2.09 -
DiffSound [203] - - 7.75 4.01 2.52 -
VIMA [87] - - - - - 72.6
VIMA-IMG [87] - - - - - 42.5
CoDi [174] 11.26 71.6 1.80 8.77 1.40 -
Emu [172] 11.66 65.5 - - - -
UIO-2 L 16.68 74.3 2.82 5.37 1.93 50.2
UIO-2 XL 14.11 80.0 2.59 5.11 1.74 54.2
UIO-2 XXL 13.39 81.3 2.64 5.89 1.80 56.3
Table 4. Results on text-to-image generation (MS COCO [115]
and TIFA [76]), text-to-audio generation (AudioCaps [93]) and ac-
tion generation (VIMA-Bench [87]).
still obtains strong referring expression scores that compare
favorably to prior work on generalist multimodal models,
see Table 5. Surpassing U NIFIED -IO while also support-
ing much higher quality image and text generation, along
with many more tasks and modalities, illustrates the impres-
sive multi-tasking capabilities of our model. U NIFIED -IO 2
even maintains better overall performance with the 3-billion
parameter model (65.2 vs. 64.5), which is roughly equal in
size to U NIFIED -IO. Ablation results show average perfor-
mance, and all individual tasks improve with model size,
showing that U NIFIED -IO 2 benefits from scale.
5.3. Generation Results
Table 4 shows results on tasks that require generating
image, audio, and action outputs. We evaluate using
TIFA [76], which measures faithfulness to the prompt us-
ing VQA models and has been shown to correlate well
with human judgments, and FID [73] on MS COCO [115].
On TIFA, we find that U NIFIED -IO 2 scores close to
minDALL-E [37], and about 10 points ahead of other gen-
eralist models such as CoDi [174] and Emu [172]. We at-
tribute this strong image generation ability to extensive pre-
training and the use of a fine-grained VQ-GAN. We include
examples of our generation results from the TIFA bench-
mark in the Appendix E.5. U NIFIED -IO 2’s FID scores are
slightly higher than the compared models, although we note
that qualitatively the generated images are still very smooth
and detailed.
For text-to-audio generation, we evaluate on the Audio-
Caps [93] test set. AudioCaps consists of 10-second audio
clips, while our model can generate 4.08-second audio at
a time, so we cannot do a direct evaluation on this bench-
mark. Instead, we generate an audio segment based on the
text description and previous audio segments as additional
9

--- PAGE 10 ---
Method VQAv2OKVQA SQA SQAITally-QA RefCOCO RefCOCO+ RefCOCO-g COCO-Cap. POPE SEED MMB
InstructBLIP (8.2B) - - - 79.5 68.2†- - - 102.2 - 53.4 36
Shikra (7.2B) 77.4 47.2 - - - 87.0 81.6 82.3 117.5 84.7 - 58.8
Ferret (7.2B) - - - - - 87.5 80.8 83.9 - 85.8 - -
Qwen-VL (9.6B) 78.8 58.6 - 67.1∗- 89.4 83.1 85.6 131.9 - 38.2
mPLUG-Owl2 (8.2B) 79.4 57.7 - 68.7∗- - - - 137.3 86.2 57.8 64.5
LLaVa-1.5 (7.2B) 78.5 - - 66.8∗- - - - - 85.9 58.6 64.3
LLaVa-1.5 (13B) 80.0 - - 71.6∗72.4†- - - - 85.9 61.6 67.7
Single Task SoTA 86.0 [29] 66.8 [77] 90.9 [119] 90.7 [34] 82.4 [77] 92.64 [202] 88.77 [187] 89.22 [187] 149.1 [29] - - -
UIO-2 L(1.1B) 75.3 50.2 81.6 78.6 69.1 84.1 71.7 79.0♢128.2 77.8 51.1 62.1
UIO-2 XL(3.2B) 78.1 53.7 88.8 87.4 72.2 88.2 79.8 84.0♢130.3 87.2 60.2 68.1
UIO-2 XXL(6.8B) 79.4 55.5 88.7 86.2 75.9 90.7 83.1 86.6♢125.4 87.7 61.8 71.5
Table 5. Vision-language results on nine tasks [1, 28, 59, 91, 124, 129, 130, 136, 209] and three evaluation-only benchmarks [106, 113, 120].
Results marked with∗are zero-shot and†are evaluated with the open-source releases, and♢indicates that our RefCOCO-g results are on
the Google split rather than the UMD split.
input; see Appendix E.6 for more details. While this is not
a directly comparable setup to related work, it still gives
a reasonable quantitative measure of our audio generation
abilities. U NIFIED -IO 2 scores higher then specialist mod-
els except the recent latent diffusion model [117], which
shows it’s competitive audio generation ability.
For action, we evaluate using VIMA-Bench [87], a robot
manipulation benchmark containing 17 tasks with text-
image interleaved prompts. Since VIMA’s action space is
action primitives, U NIFIED -IO 2 directly predicts all ac-
tions at once given the initial observation and multimodal
prompt. We report the average success rate for 4-level eval-
uation protocol [87] and compare with the original casual
VIMA policy with object-centric inputs, as well as VIMA-
IMG, a Gato [152]-like policy with image inputs like ours.
5.4. Vision Language Results
We evaluate vision language performance and compare it
against other vision/language generalist models, i.e., mod-
els that are also designed to perform many tasks and can
follow instructions. Results on a collection of 12 vi-
sion/language benchmarks are shown in Table 5. SoTA re-
sults from specialist models are shown for reference.
UNIFIED -IO 2 achieves strong results on VQA, only
passed by the much larger 13B LLaVa model [118] on VQA
v2 [59], and ahead of all other generalist models on Sci-
enceQA [124] and TallyQA [1]. OK-VQA [130] is the ex-
ception. We hypothesize that because it requires external
knowledge, extensive language pre-training is important for
this task, and therefore our reduced performance is since
UNIFIED -IO 2 was not pre-trained as extensively on text as
the dedicated language models used by Qwen-VL [12] and
mPLUG-Owl2 [206].
On referring expression, U NIFIED -IO 2 is ahead of
Shikra [26] and Ferret [207] and matches the scores
achieved by Qwen-VL. On captioning, U NIFIED -IO 2 alsoVideo Audio
Method
Kinetics-400 [90]
V ATEXCaption [190]
MSR-VTT [199]
MSRVTT-QA [198]
MSVD-QA [198]
STAR [196]
SEED-T [106]
VGG-Sound [24]
AudioCaps [93]
Kinetics-Sounds [7]
MBT [137] - - - - - - - 52.3 - 85.0
CoDi [174] - - 74.4 - - - - - 78.9 -
ImageBind [69]∗50.0 - - - - - - 27.8 - -
BLIP-2 [109]∗- - - 9.2 18.3 - 36.7 - - -
InstructBLIP [34]∗- - - 22.1 41.8 - 38.3 - - -
Emu [172]∗∗- - - 24.1 39.8 - - - - -
Flamingo-9B [5]∗∗- 57.4 - 29.4 47.2 41.2 - -
Flamingo-80B [5] - 84.2 - 47.4 - - - - - -
UIO-2 L 68.5 37.1 44.0 39.6 48.2 51.0 37.5 37.8 45.7 86.1
UIO-2 XL 71.4 41.6 47.1 39.3 50.4 52.0 45.6 44.2 45.7 88.0
UIO-2 XXL 73.8 45.6 48.8 41.5 52.1 52.2 46.8 47.7 48.9 89.3
Table 6. Results on action classification, video captioning, VQA,
visual comprehension, audio classification, and audio captioning.
∗: zero-shot,∗∗: few-shot in-context learning.
achieves a strong CIDEr score [182] of 130.3, ahead of
Shikra and InstructBLIP [34] but behind Qwen-VL and
mPLUG-Owl2.
Finally, we evaluate using three recently proposed
evaluation-only benchmarks. MMB (MMBench [120])
tests multiple facets of vision language understanding with
multiple choice questions, while SEED-Bench additionally
tests video understanding. We show a detailed breakdown
of our score in the Appendix E.4. Regarding the over-
all score, U NIFIED -IO 2 has the strongest score of any
7B model on the SEED-Bench leaderboard1, and scores
the highest on MMB by 3.8 points. Notably, it excels
LLaVa-1.5 13B model in both benchmarks. U NIFIED -IO 2
1as of 11/17/23
10

--- PAGE 11 ---
AP3D AP3D@15 AP3D@25 AP3D@50
Cube-RCNN [16] 50.8 65.7 54.0 22.5
UIO-2 L 42.9 54.4 45.7 21.7
UIO-2 XL 43.3 54.4 46.8 21.8
UIO-2 XXL 42.4 54.0 45.6 20.9
Table 7. Single-object 3D detection results on Objectron [3].
also reaches 87.7 on the POPE object hallucination bench-
mark [113], showing that it is not very prone to object hal-
lucination.
Overall, U NIFIED -IO 2 can match or surpass other vi-
sion & language generalist models on these benchmarks
despite encompassing many more modalities and support-
ing high-quality image and audio generation. This shows
that its wide breadth of capabilities does not come at the
expense of vision/language performance.
5.5. Video, Audio and other Results
UNIFIED -IO 2 shows reasonable performance on au-
dio and video classification and captioning, as well as
video question answering, as shown in Table 6. No-
tably, U NIFIED -IO 2 outperforms BLIP-2 [109] and In-
structBLIP [34] on Seed-Bench Temporal [106] by 8.5
points. U NIFIED -IO 2 also achieves better performance
on Kinetics-Sounds [7] than MBT [137], which is trained
solely on that dataset.
We show the single-object 3D detection results in Ta-
ble 7. Our model shows decent results, similar to Cube-
RCNN [16], on the Objectron benchmark [3]. However, its
performance drops significantly in multi-object 3D detec-
tion tasks, like those on nuScenes [22] and Hypersim [153].
This could be because only 1.0% of our training data fo-
cuses on 3D detection. A potential solution might be to
combine 2D and 3D detection techniques.
In COCO object detection, excluding the ‘stuff’ cate-
gories, our model reached an average precision (AP) of
47.2, with AP50 at 57.7 and AP75 at 50.0. However, it
has difficulties with images containing many objects. Previ-
ous research, like Pix2Seq [27], suggests that autoregressive
models face similar challenges, which can be improved with
extensive data augmentation. Our model’s data augmenta-
tion on object detection is comparatively more limited.
Our model shows weak performance in depth estimation,
with an RMSE of 0.623 on NYUv2 depth dataset [138].
However, fine-tuning specifically for this task improved
the RMSE to 0.423. In our experiment, we simply nor-
malize the depth map with the max depth value in each
dataset. Due to the incompatibility of dense ground-truth
depth across different datasets [150], our model failed to
capture the exact scale in the current prompt, which could
potentially be solved by using better normalization and met-
ric evaluation.Appendix E shows qualitative visualizations of other
tasks, such as single object tracking, future state prediction
of robotic manipulation, and image-based 3D view synthe-
sis,etc.
6. Limitation
• Due to memory constraints, we use the base versions of
the ViT and AST models for image and audio features
throughout the project. Using a larger version of these im-
age and audio encoders could substantially improve per-
formance.
• While our image generation is more faithful compared to
SD-based methods, its quality doesn’t match that of the
stable diffusion model. Additionally, our audio genera-
tion is capped at approximately 4 seconds, which restricts
the practical application of the audio outputs.
• Limited computational resources constrained our explo-
ration of the model’s hyperparameters. It’s likely that
using a significantly larger batch size could enhance the
model’s performance.
• Our model is much less reliable for modalities like depth,
video or when requiring more niche abilities like 3D ob-
ject detection, etc. This is probably due to the limited
variety of tasks we have in these areas.
• Improving the quality of our data could enhance the
model’s performance. However, despite considerable ef-
forts, our human-written prompts still fall short in diver-
sity. We notice a notable decrease in the model’s perfor-
mance when dealing with new instruction tasks, as op-
posed to those it was trained on.
7. Conclusion
We introduced U NIFIED -IO 2, the first autoregressive mul-
timodal model that is capable of understanding and generat-
ing image, text, audio, and action. This model was trained
from scratch on a wide range of multimodal data and fur-
ther refined with instruction tuning on a massive multimodal
corpus. We developed various architectural changes to sta-
bilize the multimodal training and proposed a multimodal
mixture of denoiser objective to effectively utilize the multi-
modal signals. Our model achieves promising results across
a wide range of tasks. We show that going from LLMs to
LMMs enables new capabilities and opportunities. In the
future, we would like to extend U NIFIED -IO 2 from the
encoder-decoder model to a decoder-only model. Addition-
ally, we plan to expand the model’s size, enhance the data
quality, and refine the overall model design.
Acknowledgement We thank Klemen Kotar for helping gather Embodied
AI pre-training data, Jonathan Frankle from MosaicML for suggesting the
mixture of NLP pre-training data, Jack Hessel for interleaved image &
text dataset and Micheal Schmitz for helping support the compute infras-
tructure. We also thank Tanmay Gupta for helpful discussions, as well
as Hamish Ivison, and Ananya Harsh Jha for their insightful discussions
11

--- PAGE 12 ---
about model design. We additionally thank Oscar Michel, Yushi Hu and
Yanbei Chen for their help editing the paper, and Matt Deitke for help
setting up the webpage. Savya Khosla and Derek Hoiem were supported
in part by ONR award N00014-23-1-2383. This research was made
possible with cloud TPUs from Google’s TPU Research Cloud (TRC).
References
[1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tal-
lyQA: Answering Complex Counting Questions. In AAAI ,
2019. 10, 29
[2] Andrea Agostinelli, Timo I Denk, Zal ´an Borsos, Jesse En-
gel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,
Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al.
MusicLM: Generating Music From Text. arXiv preprint
arXiv:2301.11325 , 2023. 7, 29
[3] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski,
Jianing Wei, and Matthias Grundmann. Objectron: A Large
Scale Dataset of Object-Centric Videos in the Wild with
Pose Annotations. In CVPR , 2021. 11, 37, 38
[4] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan,
and Barlas Oguz. Jointly Training Large Autoregressive
Multimodal Models. arXiv preprint arXiv:2309.15564 ,
2023. 3
[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al.
Flamingo: a Visual Language Model for Few-Shot Learn-
ing. In NeurIPS , 2022. 2, 4, 10, 22, 23, 37
[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi
Parikh. VQA: Visual Question Answering. In ICCV , 2015.
7
[7] Relja Arandjelovic and Andrew Zisserman. Look, Listen
and Learn. In ICCV , 2017. 8, 10, 11, 30
[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
Program Synthesis with Large Language Models. arXiv
preprint arXiv:2108.07732 , 2021. 27
[9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,
Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan
Bitton, Samir Gadre, Shiori Sagawa, et al. Open-
Flamingo: An Open-Source Framework for Training Large
Autoregressive Vision-Language Models. arXiv preprint
arXiv:2308.01390 , 2023. 8, 9, 23
[10] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.
Layer Normalization. In NeurIPS Deep Learning Sympo-
sium , 2016. 5
[11] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Esti-
mating and Exploiting the Aleatoric Uncertainty in Surface
Normal Estimation. In ICCV , 2021. 33
[12] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-VL: A Versatile Vision-Language Model for
Understanding, Localization, Text Reading, and Beyond.
arXiv preprint arXiv:2308.12966 , 2023. 3, 10, 34[13] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in Time: A Joint Video and Image Encoder
for End-to-End Retrieval. In ICCV , 2021. 7, 26
[14] Zal ´an Borsos, Rapha ¨el Marinier, Damien Vincent, Eugene
Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Rob-
lek, Olivier Teboul, David Grangier, Marco Tagliasacchi,
et al. AudioLM: A Language Modeling Approach to Au-
dio Generation. IEEE/ACM Transactions on Audio, Speech,
and Language Processing , 31:2523–2533, 2023. 2
[15] James Bradbury, Roy Frostig, Peter Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. Jax: composable
transformations of python+numpy programs, 2018. 23
[16] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila
Ravi, Justin Johnson, and Georgia Gkioxari. Omni3D: A
Large Benchmark and Model for 3D Object Detection in
the Wild. In CVPR , 2023. 3, 8, 11, 21, 29
[17] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
Danny Driess, Avinava Dubey, Chelsea Finn, et al. RT-2:
Vision-Language-Action Models Transfer Web Knowledge
to Robotic Control. In CoRL , 2023. 3
[18] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structPix2Pix: Learning to Follow Image Editing Instruc-
tions. In CVPR , 2023. 7, 29
[19] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language Models are Few-Shot Learners. In NeurIPS ,
2020. 5, 37
[20] S ´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks
of Artificial General Intelligence: Early experiments with
GPT-4. arXiv preprint arXiv:2303.12712 , 2023. 3
[21] Daniel J Butler, Jonas Wulff, Garrett B Stanley, and
Michael J Black. A Naturalistic Open Source Movie for
Optical Flow Evaluation. In ECCV , 2012. 8, 30
[22] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuScenes: A mul-
timodal dataset for autonomous driving. In CVPR , 2020.
11
[23] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing Web-Scale Image-Text
Pre-Training To Recognize Long-Tail Visual Concepts. In
CVPR , 2021. 6, 26
[24] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew
Zisserman. VGGSound: A Large-Scale Audio-Visual
Dataset. In ICASSP , 2020. 8, 10, 30
[25] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
MiniGPT-v2: Large Language Model As a Unified In-
terface for Vision-Language Multi-task Learning. arXiv
preprint arXiv:2310.09478 , 2023. 3
12

--- PAGE 13 ---
[26] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing Multi-
modal LLM’s Referential Dialogue Magic. arXiv preprint
arXiv:2306.15195 , 2023. 3, 10, 34
[27] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A Language Modeling Framework
for Object Detection. In ICLR , 2022. 3, 11
[28] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna
Vedantam, Saurabh Gupta, Piotr Doll ´ar, and C. Lawrence
Zitnick. Microsoft COCO Captions: Data Collection and
Evaluation Server. arXiv preprint arXiv:1504.00325 , 2015.
10
[29] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,
Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Se-
bastian Goodman, Xiao Wang, Yi Tay, et al. PaLI-X:
On Scaling up a Multilingual Vision and Language Model.
arXiv preprint arXiv:2305.18565 , 2023. 10
[30] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom
Kwiatkowski, Michael Collins, and Kristina Toutanova.
BoolQ: Exploring the Surprising Difficulty of Natural
Yes/No Questions. In NAACL-HLT , 2019. 33
[31] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have Solved Question Answering? Try
ARC, the AI2 Reasoning Challenge. arXiv preprint
arXiv:1803.05457 , 2018. 33
[32] Together Computer. RedPajama: an Open Dataset
for Training Large Language Models. https : / /
github.com/togethercomputer/RedPajama-
Data , 2023. 6, 26
[33] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei
Zaharia, and Reynold Xin. Free Dolly: Introducing
the World’s First Truly Open Instruction-Tuned LLM.
https://www.databricks.com/blog/2023/
04 / 12 / dolly - first - open - commercially -
viable-instruction-tuned-llm , 2023. 7, 27
[34] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. InstructBLIP: Towards General-
purpose Vision-Language Models with Instruction Tuning.
InNeurIPS , 2023. 3, 10, 11, 34
[35] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide
Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and
Michael Wray. Rescaling Egocentric Vision: Collection,
Pipeline and Challenges for EPIC-KITCHENS-100. Inter-
national Journal of Computer Vision , 130:33–55, 2022. 8,
30
[36] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,
Deshraj Yadav, Jos ´e MF Moura, Devi Parikh, and Dhruv
Batra. Visual Dialog. In CVPR , 2017. 29
[37] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah,
Tanishq Abraham, Ph ´uc L ˆe Khac, Luke Melas, and Rito-
brata Ghosh. DALL·E Mini, 2021. 9, 34
[38] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr
Padlewski, Jonathan Heek, Justin Gilmer, Andreas PeterSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdul-
mohsin, et al. Scaling Vision Transformers to 22 Billion
Parameters. In ICML , 2023. 5
[39] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,
Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,
Aniruddha Kembhavi, and Roozbeh Mottaghi. ProcTHOR:
Large-Scale Embodied AI Using Procedural Generation. In
NeurIPS , 2022. 7, 25, 26
[40] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana
Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:
A Universe of Annotated 3D Objects. In CVPR , 2023. 7, 8,
25, 26, 29, 37, 38
[41] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and
Li Fei-Fei. ImageNet: A Large-Scale Hierarchical Image
Database. In CVPR , 2009. 7, 22
[42] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin John-
son. RedCaps: Web-curated image-text data created by the
people, for the people. In NeurIPS Datasets and Bench-
marks Track , 2021. 6, 26
[43] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. CogView: Mastering Text-to-Image
Generation via Transformers. In NeurIPS , 2021. 36
[44] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. FlowNet:
Learning Optical Flow with Convolutional Networks. In
ICCV , 2015. 8, 30
[45] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al. An Image is Worth 16x16 Words:
Transformers for Image Recognition at Scale. In ICLR ,
2021. 22
[46] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-
E: An Embodied Multimodal Language Model. In ICML ,
2023. 2, 32
[47] Konstantinos Drossos, Samuel Lipping, and Tuomas Virta-
nen. Clotho: An Audio Captioning Dataset. In ICASSP ,
2020. 7, 8, 29
[48] Gerald M Edelman. Neural Darwinism: Selection and reen-
trant signaling in higher brain function. Neuron , 10(2):115–
125, 1993. 2
[49] Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam-
ing Transformers for High-Resolution Image Synthesis. In
CVPR , 2021. 3, 22
[50] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge
Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu,
et al. LaSOT: A High-quality Large-scale Single Object
Tracking Benchmark. International Journal of Computer
Vision , 129:439–461, 2021. 8, 30, 37
[51] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An-
thony DiPofi, Charles Foster, Laurence Golding, Jeffrey
Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,
13

--- PAGE 14 ---
Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin
Wang, and Andy Zou. A framework for few-shot language
model evaluation, 2021. 33
[52] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-
jie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,
Xiangyu Yue, et al. LLaMA-Adapter V2: Parameter-
Efficient Visual Instruction Model. arXiv preprint
arXiv:2304.15010 , 2023. 3
[53] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying
Shan. Planting a SEED of Vision in Large Language Model.
arXiv preprint arXiv:2307.08041 , 2023. 3
[54] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,
Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj
Plakal, and Marvin Ritter. Audio Set: An Ontology and
Human-Labeled Dataset for Audio Events. In ICASSP ,
2017. 7, 8, 22, 26, 30
[55] Xinyang Geng and Hao Liu. OpenLLaMA: An Open
Reproduction of LLaMA. https://github.com/
openlm-research/open_llama , 2023. 9
[56] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
Misra. ImageBind: One Embedding Space To Bind Them
All. In CVPR , 2023. 3
[57] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio
Spectrogram Transformer. In Interspeech , 2021. 4, 22
[58] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz
Mueller-Freitag, et al. The” something something” video
database for learning and evaluating visual common sense.
InICCV , 2017. 30
[59] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv
Batra, and Devi Parikh. Making the V in VQA Matter: Ele-
vating the Role of Image Understanding in Visual Question
Answering. In CVPR , 2017. 10
[60] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jack-
son Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al.
Ego4D: Around the World in 3,000 Hours of Egocentric
Video. In CVPR , 2022. 7, 26
[61] Chunhui Gu, Chen Sun, David A Ross, Carl V ondrick, Car-
oline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,
George Toderici, Susanna Ricco, Rahul Sukthankar, et al.
A V A: A Video Dataset of Spatio-temporally Localized
Atomic Visual Actions. In CVPR , 2018. 8, 30
[62] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A
Dataset for Large V ocabulary Instance Segmentation. In
CVPR , 2019. 29
[63] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey
Levine, and Karol Hausman. Relay Policy Learning: Solv-
ing Long-Horizon Tasks via Imitation and Reinforcement
Learning. In CoRL , 2019. 8, 31
[64] Tanmay Gupta and Aniruddha Kembhavi. Visual Program-
ming: Compositional visual reasoning without training. In
CVPR , 2023. 2
[65] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and
Derek Hoiem. Towards General Purpose Vision Systems:An End-to-End Task-Agnostic Vision-Language Architec-
ture. In CVPR , 2022. 33
[66] Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and
Derek Hoiem. GRIT: General Robust Image Task Bench-
mark. arXiv preprint arXiv:2204.13653 , 2022. 2, 9, 22
[67] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
VizWiz Grand Challenge: Answering Visual Questions
from Blind People. In CVPR , 2018. 29
[68] Ivan Habernal, Omnia Zayed, and Iryna Gurevych.
C4Corpus: Multilingual Web-size Corpus with Free Li-
cense. In LREC , 2016. 6, 26
[69] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng
Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu
Guo, et al. ImageBind-LLM: Multi-modality Instruction
Tuning. arXiv preprint arXiv:2309.03905 , 2023. 3, 10
[70] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask R-CNN. In ICCV , 2017. 33
[71] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked Autoencoders Are Scal-
able Vision Learners. In CVPR , 2022. 24
[72] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Mea-
suring Massive Multitask Language Understanding. In
ICLR , 2021. 33
[73] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. GANs Trained by
a Two Time-Scale Update Rule Converge to a Local Nash
Equilibrium. In NeurIPS , 2017. 9, 36
[74] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion
Guidance. In NeurIPS Workshop on Deep Generative Mod-
els and Downstream Applications , 2021. 29, 34, 36
[75] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
Choi. The Curious Case of Neural Text Degeneration. In
ICLR , 2020. 29
[76] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari
Ostendorf, Ranjay Krishna, and Noah A. Smith. TIFA: Ac-
curate and Interpretable Text-to-Image Faithfulness Evalu-
ation with Question Answering. In ICCV , 2023. 2, 8, 9, 34,
35
[77] Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy
Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna,
and Ariel Fuxman. Visual Program Distillation: Distilling
Tools and Programmatic Reasoning into Vision-Language
Models. arXiv preprint arXiv:2312.03052 , 2023. 10
[78] Jingwei Huang, Yichao Zhou, Thomas Funkhouser, and
Leonidas J Guibas. FrameNet: Learning Local Canonical
Frames of 3D Surfaces from a Single RGB Image. In ICCV ,
2019. 8, 30
[79] Lianghua Huang, Xin Zhao, and Kaiqi Huang. GOT-10k:
A Large High-Diversity Benchmark for Generic Object
Tracking in the Wild. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence , 43(5):1562–1577, 2019. 8,
30
[80] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,
Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,
Jiawei Huang, Jinglin Liu, et al. AudioGPT: Understanding
14

--- PAGE 15 ---
and Generating Speech, Music, Sound, and Talking Head.
arXiv preprint arXiv:2304.12995 , 2023. 3
[81] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, et al. Language Is Not
All You Need: Aligning Perception with Language Models.
InNeurIPS , 2023. 2, 3
[82] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh,
Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Gir-
shick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al.
Visual Storytelling. In NAACL-HLT , 2016. 29
[83] Drew A Hudson and Christopher D Manning. GQA: A
New Dataset for Real-World Visual Reasoning and Com-
positional Question Answering. In CVPR , 2019. 29
[84] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
CLIP, 2021. 3
[85] Keith Ito and Linda Johnson. The LJ Speech
Dataset. https://keithito.com/LJ-Speech-
Dataset/ , 2017. 7, 22, 29
[86] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste
Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
Shelhamer, et al. Perceiver IO: A General Architecture for
Structured Inputs & Outputs. In ICLR , 2022. 5
[87] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang,
Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anand-
kumar, Yuke Zhu, and Linxi Fan. VIMA: General Robot
Manipulation with Multimodal Prompts. In ICML , 2023.
8, 9, 10, 21, 30, 37
[88] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan,
Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang
Lei, et al. Unified Language-Vision Pretraining in LLM
with Dynamic Discrete Visual Tokenization. arXiv preprint
arXiv:2309.04669 , 2023. 3
[89] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric
Kolve, Derek Hoiem, and Aniruddha Kembhavi. Webly
Supervised Concept Expansion for General Purpose Vision
Models. In ECCV , 2022. 9, 33
[90] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Vi-
ola, Tim Green, Trevor Back, Paul Natsev, et al. The
Kinetics Human Action Video Dataset. arXiv preprint
arXiv:1705.06950 , 2017. 10, 36
[91] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. ReferItGame: Referring to Objects in Pho-
tographs of Natural Scenes. In EMNLP , 2014. 8, 10, 29,
30
[92] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and
Matthew Sharifi. Fr ´echet Audio Distance: A Reference-
Free Metric for Evaluating Music Enhancement Algo-
rithms. In Interspeech , 2019. 36
[93] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. AudioCaps: Generating Captions for Audios
in The Wild. In NAACL-HLT , 2019. 7, 8, 9, 10, 29, 36[94] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi
Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar,
and Ross Girshick. Segment Anything. In ICCV , 2023. 7,
26, 33
[95] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
Chenghao Mou, Carlos Mu ˜noz Ferrandis, Yacine Jernite,
Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry
Bahdanau, Leandro von Werra, and Harm de Vries. The
Stack: 3 TB of permissively licensed source code. Trans-
actions on Machine Learning Research , 2023. 26
[96] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-
erating Images with Multimodal Language Models. In
NeurIPS , 2023. 2, 3
[97] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
Grounding Language Models to Images for Multimodal In-
puts and Outputs. In ICML , 2023. 3
[98] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-
GAN: Generative Adversarial Networks for Efficient and
High Fidelity Speech Synthesis. In NeurIPS , 2020. 22
[99] Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris
Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah
Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard
Nagyfi, et al. OpenAssistant Conversations – Democra-
tizing Large Language Model Alignment. In NeurIPS
Datasets and Benchmarks Track , 2023. 27
[100] Mario Michael Krell, Matej Kosec, Sergio P Perez, and
Andrew Fitzgibbon. Efficient Sequence Packing with-
out Cross-contamination: Accelerating Large Language
Models without Impacting Performance. arXiv preprint
arXiv:2107.02027 , 2021. 6
[101] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,
Alexandre D ´efossez, Jade Copet, Devi Parikh, Yaniv Taig-
man, and Yossi Adi. AudioGen: Textually Guided Audio
Generation. In ICLR , 2023. 9
[102] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein,
and Fei-Fei Li. Visual Genome: Connecting Language and
Vision Using Crowdsourced Dense Image Annotations. In-
ternational Journal of Computer Vision , 2017. 7, 27
[103] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-
fan Popov, Matteo Malloci, Alexander Kolesnikov, Tom
Duerig, and Vittorio Ferrari. The Open Images Dataset V4:
Unified image classification, object detection, and visual re-
lationship detection at scale. International Journal of Com-
puter Vision , 128(7):1956–1981, 2020. 7, 22, 27
[104] Hugo Laurenc ¸on, Lucile Saulnier, L ´eo Tronchon, Stas Bek-
man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
Siddharth Karamcheti, Alexander M Rush, Douwe Kiela,
et al. OBELICS: An Open Web-Scale Filtered Dataset of
Interleaved Image-Text Documents. In NeurIPS Datasets
and Benchmarks Track , 2023. 6, 24, 26
[105] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim,
Thomas Breuel, Gal Chechik, and Yale Song. ACA V100M:
Automatic Curation of Large-Scale Datasets for Audio-
15

--- PAGE 16 ---
Visual Video Representation Learning. In ICCV , 2021. 7,
22, 26
[106] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. SEED-Bench: Benchmarking
Multimodal LLMs with Generative Comprehension. arXiv
preprint arXiv:2307.16125 , 2023. 8, 9, 10, 11, 34
[107] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
MIMIC-IT: Multi-Modal In-Context Instruction Tuning.
arXiv preprint arXiv:2306.05425 , 2023. 8, 29
[108] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A Multi-Modal
Model with In-Context Instruction Tuning. arXiv preprint
arXiv:2305.03726 , 2023. 29, 34
[109] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping Language-Image Pre-training with
Frozen Image Encoders and Large Language Models. In
ICML , 2023. 3, 10, 11
[110] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
VideoChat: Chat-Centric Video Understanding. arXiv
preprint arXiv:2305.06355 , 2023. 3, 34
[111] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,
Limin Wang, and Yu Qiao. UniFormerV2: Unlocking the
Potential of Image ViTs for Video Understanding. In ICCV ,
2023. 8, 30, 37
[112] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,
Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu
Sun, et al. M3IT: A Large-Scale Dataset towards Multi-
Modal Multilingual Instruction Tuning. arXiv preprint
arXiv:2306.04387 , 2023. 3, 8, 29
[113] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji rong Wen. Evaluating Object Hallucination in
Large Vision-Language Models. In EMNLP , 2023. 10, 11,
34
[114] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Jeffrey Tsaw,
Yudong Liu, Shentong Mo, Dani Yogatama, Louis-Philippe
Morency, and Russ Salakhutdinov. High-Modality Mul-
timodal Transformer: Quantifying Modality & Interaction
Heterogeneity for High-Modality Representation Learning.
TMLR , 2023. 3
[115] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C Lawrence Zitnick. Microsoft COCO: Common Objects
in Context. In ECCV , 2014. 7, 8, 9, 27, 34
[116] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual Spa-
tial Reasoning. Transactions of the Association for Compu-
tational Linguistics , 11:635–651, 2023. 29
[117] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,
Danilo Mandic, Wenwu Wang, and Mark D Plumbley. Au-
dioLDM: Text-to-Audio Generation with Latent Diffusion
Models. In ICML , 2023. 9, 10, 36
[118] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved Baselines with Visual Instruction Tuning. arXiv
preprint arXiv:2310.03744 , 2023. 3, 10, 34
[119] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. Visual Instruction Tuning. In NeurIPS , 2023. 2, 3, 8,
10, 29[120] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,
Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. MM-
Bench: Is Your Multi-modal Model an All-around Player?
arXiv preprint arXiv:2307.06281 , 2023. 10
[121] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,
et al. Swin Transformer V2: Scaling Up Capacity and Res-
olution. In CVPR , 2022. 5
[122] Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Bar-
ret Zoph, Jason Wei, and Adam Roberts. The Flan Collec-
tion: Designing Data and Methods for Effective Instruction
Tuning. In ICML , 2023. 3, 7, 27
[123] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh
Mottaghi, and Aniruddha Kembhavi. Unified-IO: A Uni-
fied Model for Vision, Language, and Multi-Modal Tasks.
InICLR , 2023. 2, 3, 4, 7, 9, 22
[124] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to Explain: Multimodal Reasoning
via Thought Chains for Science Question Answering. In
NeurIPS , 2022. 10, 29
[125] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei
Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
Chameleon: Plug-and-Play Compositional Reasoning with
Large Language Models. In NeurIPS , 2023. 2
[126] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-
Hui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Val-
ley: Video Assistant with Large Language model Enhanced
abilitY . arXiv preprint arXiv:2306.07207 , 2023. 3
[127] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli
Ding, James Betker, Robert Baruch, Travis Armstrong, and
Pete Florence. Interactive Language: Talking to Robots in
Real Time. IEEE Robotics and Automation Letters , 2023.
8, 31
[128] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and
Fahad Shahbaz Khan. Video-ChatGPT: Towards Detailed
Video Understanding via Large Vision and Language Mod-
els.arXiv preprint arXiv:2306.05424 , 2023. 34
[129] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan Yuille, and Kevin Murphy. Generation and
Comprehension of Unambiguous Object Descriptions. In
CVPR , 2016. 10
[130] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. OK-VQA: A Visual Question Answer-
ing Benchmark Requiring External Knowledge. In CVPR ,
2019. 10, 29
[131] Irene Martin Morato and Annamaria Mesaros. Diversity
and Bias in Audio Captioning Datasets. In DCASE , 2021.
7, 29
[132] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie
Pavlick. Linearly Mapping from Image to Text Space. In
ICLR , 2023. 3
[133] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and
Anirban Chakraborty. OCR-VQA: Visual Question An-
swering by Reading Text in Images. In ICDAR , 2019. 29
16

--- PAGE 17 ---
[134] Utkarsh Mishra, Shangjie Xue, Yongxin Chen, and Danfei
Xu. Generative Skill Chaining: Long-Horizon Skill Plan-
ning with Diffusion Models. In CoRL , 2023. 32
[135] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang,
Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and
Ping Luo. EmbodiedGPT: Vision-Language Pre-Training
via Embodied Chain of Thought. In NeurIPS , 2023. 3
[136] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.
Modeling Context Between Objects for Referring Expres-
sion Understanding. In ECCV , 2016. 10, 29, 30
[137] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,
Cordelia Schmid, and Chen Sun. Attention Bottlenecks for
Multimodal Fusion. In NeurIPS , 2021. 10, 11
[138] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob
Fergus. Indoor Segmentation and Support Inference from
RGBD Images. In ECCV , 2012. 8, 11, 30
[139] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
DINOv2: Learning Robust Visual Features without Super-
vision. arXiv preprint arXiv:2304.07193 , 2023. 33
[140] Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bew-
ley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant
Rai, Anikait Singh, Anthony Brohan, et al. Open X-
Embodiment: Robotic Learning Datasets and RT-X Mod-
els. In CoRL Workshop TGR , 2023. 3
[141] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,
Wenhu Chen, and Furu Wei. Kosmos-G: Generating Im-
ages in Context with Multimodal Large Language Models.
arXiv preprint arXiv:2310.02992 , 2023. 3
[142] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley,
and Jianfeng Gao. Instruction Tuning with GPT-4. arXiv
preprint arXiv:2304.03277 , 2023. 7, 27
[143] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding
Multimodal Large Language Models to the World. arXiv
preprint arXiv:2306.14824 , 2023. 2, 3
[144] Jean Piaget, Margaret Cook, et al. The Origins of Intel-
ligence in Children . International Universities Press New
York, 1952. 2
[145] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang,
Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming
Xiong, Silvio Savarese, et al. UniControl: A Unified Diffu-
sion Model for Controllable Visual Generation In the Wild.
InNeurIPS , 2023. 29
[146] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing Transferable Visual Models From Natural Language
Supervision. In ICML , 2021. 33, 36
[147] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J. Liu. Exploring the Limits of Transfer Learning
with a Unified Text-to-Text Transformer. JMLR , 21(140):
1–67, 2020. 4, 5, 21
[148] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
Gray, Chelsea V oss, Alec Radford, Mark Chen, and IlyaSutskever. Zero-Shot Text-to-Image Generation. In ICML ,
2021. 5
[149] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Ab-
hishek Das. Habitat-Web: Learning Embodied Object-
Search Strategies from Human Demonstrations at Scale. In
CVPR , 2022. 25
[150] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards Robust Monocular
Depth Estimation: Mixing Datasets for Zero-Shot Cross-
Dataset Transfer. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 44(3):1623–1637, 2020. 11
[151] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin
Pan, and Vincent Vanhoucke. YouTube-BoundingBoxes: A
Large High-Precision Human-Annotated Data Set for Ob-
ject Detection in Video. In CVPR , 2017. 8, 30
[152] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez
Colmenarejo, Alexander Novikov, Gabriel Barth-Maron,
Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Sprin-
genberg, et al. A Generalist Agent. Transactions on Ma-
chine Learning Research , 2022. 3, 10, 37
[153] Mike Roberts, Jason Ramapuram, Anurag Ranjan, At-
ulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ
Webb, and Joshua M. Susskind. Hypersim: A Photorealis-
tic Synthetic Dataset for Holistic Indoor Scene Understand-
ing. In ICCV , 2021. 11
[154] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¨orn Ommer. High-Resolution Image
Synthesis With Latent Diffusion Models. In CVPR , 2022.
2, 8, 9, 34
[155] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved Techniques
for Training GANs. In NeurIPS , 2016. 36
[156] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,
Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar-
naud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask
Prompted Training Enables Zero-Shot Task Generalization.
InICLR , 2022. 27
[157] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A Plat-
form for Embodied AI Research. In ICCV , 2019. 7, 25, 26,
31
[158] Christoph Schuhmann. LAION-AESTHETICS. https:
//laion.ai/blog/laion-aesthetics/ , 2022.
26
[159] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-
400M: Open Dataset of CLIP-Filtered 400 Million Image-
Text Pairs. In NeurIPS Data-Centric AI Workshop , 2021.
6, 26
[160] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A
Benchmark for Visual Question Answering using World
Knowledge. In ECCV , 2022. 29
[161] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neu-
ral Machine Translation of Rare Words with Subword
Units. In ACL, 2016. 3
17

--- PAGE 18 ---
[162] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia,
Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine
Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J
Joshi, et al. RoboVQA: Multimodal Long-Horizon Reason-
ing for Robotics. arXiv preprint arXiv:2311.00899 , 2023.
32
[163] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual Captions: A Cleaned, Hypernymed,
Image Alt-text Dataset For Automatic Image Captioning.
InACL, 2018. 6, 26
[164] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive
Learning Rates with Sublinear Memory Cost. In ICML ,
2018. 6
[165] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,
Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards
VQA Models That Can Read. In CVPR , 2019. 29
[166] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit
Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse
Thomason, and Animesh Garg. ProgPrompt: Generating
Situated Robot Task Plans using Large Language Models.
ICRA , 2023. 2
[167] Linda Smith and Michael Gasser. The Development of Em-
bodied Cognition: Six Lessons from Babies. Artificial life ,
11(1-2):13–29, 2005. 2
[168] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.
UCF101: A Dataset of 101 Human Actions Classes From
Videos in The Wild. arXiv preprint arXiv:1212.0402 , 2012.
8, 30
[169] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo
Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer
with Rotary Position Embedding. Neurocomputing , 2023.
4, 23
[170] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and
Deng Cai. PandaGPT: One Model To Instruction-Follow
Them All. arXiv preprint arXiv:2305.16355 , 2023. 3
[171] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Hua-
jun Bai, and Yoav Artzi. A Corpus for Reasoning About
Natural Language Grounded in Photographs. In ACL, 2019.
29
[172] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative Pretraining in Mul-
timodality. arXiv preprint arXiv:2307.05222 , 2023. 3, 9,
10, 34
[173] D ´ıdac Sur ´ıs, Sachit Menon, and Carl V ondrick. ViperGPT:
Visual Inference via Python Execution for Reasoning. In
ICCV , 2023. 2
[174] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng,
and Mohit Bansal. Any-to-Any Generation via Composable
Diffusion. In NeurIPS , 2023. 2, 3, 9, 10, 34
[175] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia,
Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,
Tal Schuster, Steven Zheng, et al. UL2: Unifying Language
Learning Paradigms. In ICLR , 2023. 5
[176] MosaicML NLP Team. Introducing MPT-7B: A New Stan-
dard for Open-Source, Commercially Usable LLMs, 2023.
Accessed: 2023-05-05. 6, 24[177] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Bap-
tiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar,
et al. LLaMA: Open and Efficient Foundation Language
Models. arXiv preprint arXiv:2302.13971 , 2023. 3, 9, 23
[178] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali
Eslami, Oriol Vinyals, Felix Hill, and Zacharias Janssen.
Multimodal Few-Shot Learning with Frozen Language
Models. In NeurIPS , 2021. 3
[179] Aaron Van Den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural Discrete Representation Learning. In
NeurIPS , 2017. 3
[180] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona,
and Serge Belongie. The iNaturalist Species Classification
and Detection Dataset. In CVPR , 2018. 29
[181] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin. Attention Is All You Need. In
NeurIPS , 2017. 3
[182] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. CIDEr: Consensus-based Image Description Eval-
uation. In CVPR , 2015. 10, 37
[183] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas,
and Serge Belongie. COCO-Text: Dataset and Bench-
mark for Text Detection and Recognition in Natural Images.
arXiv preprint arXiv:1601.07140 , 2016. 8, 29
[184] Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim,
Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-
Estruch, Quan Vuong, Andre He, et al. BridgeData V2:
A Dataset for Robot Learning at Scale. In CoRL , 2023. 8,
31
[185] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal Segment
Networks for Action Recognition in Videos. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence , 41
(11):2740–2755, 2019. 30
[186] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai
Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,
and Hongxia Yang. OFA: Unifying Architectures, Tasks,
and Modalities Through a Simple Sequence-to-Sequence
Learning Framework. In ICML , 2022. 3, 33
[187] Peng Wang, Shijie Wang, Junyang Lin, Shuai Bai, Xi-
aohuan Zhou, Jingren Zhou, Xinggang Wang, and Chang
Zhou. ONE-PEACE: Exploring One General Representa-
tion Model Toward Unlimited Modalities. arXiv preprint
arXiv:2305.11172 , 2023. 10
[188] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, and Furu Wei.
Image as a Foreign Language: BEiT Pretraining for Vision
and Vision-Language Tasks. In CVPR , 2023. 3
[189] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou,
Yu Qiao, et al. VisionLLM: Large Language Model is
also an Open-Ended Decoder for Vision-Centric Tasks. In
NeurIPS , 2023. 3
18

--- PAGE 19 ---
[190] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang, and William Yang Wang. V ATEX: A Large-
Scale, High-Quality Multilingual Dataset for Video-and-
Language Research. In ICCV , 2019. 8, 10, 30
[191] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,
Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar,
Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,
David Stap, et al. Super-NaturalInstructions: Generaliza-
tion via Declarative Instructions on 1600+ NLP Tasks. In
EMNLP , 2022. 27
[192] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. SimVLM: Simple Visual Lan-
guage Model Pretraining with Weak Supervision. In ICLR ,
2022. 3
[193] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and
Quoc V Le. Finetuned Language Models are Zero-Shot
Learners. In ICLR , 2022. 27
[194] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Ro-
main Br ´egier, Yohann Cabon, Vaibhav Arora, Leonid Ants-
feld, Boris Chidlovskii, Gabriela Csurka, and J ´erˆome Re-
vaud. CroCo: Self-Supervised Pre-training for 3D Vision
Tasks by Cross-View Completion. In NeurIPS , 2022. 7, 25,
26, 29
[195] Peter Welinder, Steve Branson, Takeshi Mita, Catherine
Wah, Florian Schroff, Serge Belongie, and Pietro Perona.
Caltech-UCSD Birds 200. Technical Report CNS-TR-
2010-001, California Institute of Technology, 2010. 29
[196] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,
and Chuang Gan. STAR: A Benchmark for Situated Rea-
soning in Real-World Videos. In NeurIPS Datasets and
Benchmarks Track , 2021. 8, 10, 30
[197] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,
and Antonio Torralba. SUN Database: Large-scale Scene
Recognition from Abbey to Zoo. In CVPR , 2010. 29
[198] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,
Xiangnan He, and Yueting Zhuang. Video Question An-
swering via Gradually Refined Attention over Appearance
and Motion. In ACM MM , 2017. 8, 10, 30
[199] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT:
A Large Video Description Dataset for Bridging Video and
Language. In CVPR , 2016. 8, 10, 30
[200] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
vancing High-Resolution Video-Language Representation
with Large-Scale Video Transcriptions. In CVPR , 2022. 7,
26
[201] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin
Raffel. mT5: A Massively Multilingual Pre-trained Text-
to-Text Transformer. In NAACL-HLT , 2021. 26
[202] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan,
Ping Luo, and Huchuan Lu. Universal Instance Perception
as Object Discovery and Retrieval. In CVPR , 2023. 10
[203] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,
Chao Weng, Yuexian Zou, and Dong Yu. Diffsound:
Discrete Diffusion Model for Text-to-sound Generation.IEEE/ACM Transactions on Audio, Speech, and Language
Processing , 31:1720–1733, 2023. 9
[204] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan
Ren, Lei Zhou, Tian Fang, and Long Quan. BlendedMVS:
A Large-scale Dataset for Generalized Multi-view Stereo
Networks. In CVPR , 2020. 8, 30
[205] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mPLUG-Owl: Modularization Empow-
ers Large Language Models with Multimodality. arXiv
preprint arXiv:2304.14178 , 2023. 3
[206] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei
Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.
mPLUG-Owl2: Revolutionizing Multi-modal Large Lan-
guage Model with Modality Collaboration. arXiv preprint
arXiv:2311.04257 , 2023. 10, 34
[207] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du,
Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu
Chang, and Yinfei Yang. Ferret: Refer and Ground
Anything Anywhere at Any Granularity. arXiv preprint
arXiv:2310.07704 , 2023. 3, 10, 34
[208] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,
James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,
and Yonghui Wu. Vector-quantized Image Modeling with
Improved VQGAN. In ICLR , 2022. 4, 22
[209] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling Context in Referring Expres-
sions. In ECCV , 2016. 10, 29, 30
[210] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin
Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh
Tang, Brian Karrer, Shelly Sheynin, et al. Scaling Autore-
gressive Multi-Modal Models: Pretraining and Instruction
Tuning. arXiv preprint arXiv:2309.02591 , 2023. 3
[211] Amir R Zamir, Alexander Sax, William Shen, Leonidas J
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentangling Task Transfer Learning. In CVPR , 2018. 30
[212] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual Object Detection with
Multimodal Large Language Models. arXiv preprint
arXiv:2305.18279 , 2023. 3
[213] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
From Recognition to Cognition: Visual Commonsense
Reasoning. In CVPR , 2019. 29
[214] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
and Yejin Choi. HellaSwag: Can a Machine Really Finish
Your Sentence? In ACL, 2019. 8, 9, 33
[215] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yan-
peng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack
Hessel, Ali Farhadi, and Yejin Choi. MERLOT Reserve:
Neural Script Knowledge through Vision and Language and
Sound. In CVPR , 2022. 7, 22, 23, 26, 29
[216] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An
Instruction-tuned Audio-Visual Language Model for Video
Understanding. arXiv preprint arXiv:2306.02858 , 2023. 3
[217] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and
Yu Su. MagicBrush: A Manually Annotated Dataset for
Instruction-Guided Image Editing. In NeurIPS Datasets
and Benchmarks Track , 2023. 7, 29
19

--- PAGE 20 ---
[218] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu
Qiao. LLaMA-Adapter: Efficient Fine-tuning of Lan-
guage Models with Zero-init Attention. arXiv preprint
arXiv:2303.16199 , 2023. 3
[219] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
Shao, Wenwei Zhang, Kai Chen, and Ping Luo. GPT4RoI:
Instruction Tuning Large Language Model on Region-of-
Interest. arXiv preprint arXiv:2307.03601 , 2023. 3
[220] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,
Nedim Lipka, Diyi Yang, and Tong Sun. LLaV AR: En-
hanced Visual Instruction Tuning for Text-Rich Image Un-
derstanding. arXiv preprint arXiv:2306.17107 , 2023. 3,
8
[221] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing
Zhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu,
Guangze Li, et al. Towards Video Text Visual Question
Answering: Benchmark and Baseline. In NeurIPS Datasets
and Benchmarks Track , 2022. 30
[222] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen,
Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chat-
Bridge: Bridging Modalities with Large Language Model
as a Language Catalyst. arXiv preprint arXiv:2305.16103 ,
2023. 3
[223] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh
Phung. MoVQ: Modulating Quantized Vectors for High-
Fidelity Image Generation. In NeurIPS , 2022. 22
[224] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,
and Antonio Torralba. Places: A 10 Million Image
Database for Scene Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 40(6):1452–
1464, 2017. 29
[225] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny. MiniGPT-4: Enhancing Vision-
Language Understanding with Advanced Large Language
Models. arXiv preprint arXiv:2304.10592 , 2023. 3
[226] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-
GAN: Dynamic Memory Generative Adversarial Networks
for Text-to-Image Synthesis. In CVPR , 2019. 34
20

--- PAGE 21 ---
The appendix includes the following sections:
• Sec A - Contributions
• Sec B - Model Implementation Details
• Sec C - Pre-training Details
• Sec D - Tasks and Instruction Tuning
• Sec E - Experiment Details and Additional Results
A. Contributions
Jiasen Lu, Christopher Clark, Sangho Lee, and Zichen
Zhang collectively contributed to dataset construction,
prompt development, and conducted numerous exploratory
experiments for this project.
Jiasen Lu led and designed the main idea and scope of the
project. Developed the majority of the model pipeline –
image and audio tokenizer, main architecture, model stabi-
lization, and training objective. Led and designed the pre-
training and instruction tuning data pipelines. Conducted
experiments with various model and data hyperparameters,
oversaw the model training process, and wrote the paper.
Coordinate with the whole team.
Christopher Clark co-led and designed the infrastruc-
ture, instruction tuning, and evaluation. Developed the
dynamic packing system, modality processing pipeline,
and classifier-free guidance for image and audio inference.
Added the NLP and many V&L datasets, added many syn-
thetic tasks, and built prompts for instruction-tuning tasks.
Ran the evaluation in § 5.1 (NLP), 5.2, and 5.4 (detection,
depth) and wrote the paper.
Sangho Lee core contribution to the pre-training data
pipeline. Added all large-scale multimodal pretraining
datasets, and video and audio instruction tuning datasets.
Developed sample construction pipeline for pre-training.
Helped with the model implementation – position encod-
ing, perceiver resamplers, and model stabilization. Ran the
evaluation in § 5.1 (audio), 5.3 (audio and image FID), 5.4
(video and audio understanding) and wrote parts of the pa-
per.
Zichen Zhang core contribution to the instruction tuning
data pipeline. Added many V&L, embodiment, video, au-
dio, data augmentation, and all instruction tuning datasets.
Built prompts for instruction tuning. Investigated the model
architectures and training pipelines and stabilized the train-
ing. Ran the experiments in § 5.1 (image TIFA, Seed-
Bench), 5.3 (image TIFA, action), 5.4, wrote parts of the
paper, developed the model demo and project page.
Savya Khosla added 3D object detection, optical flow, and
multi-point tracking datasets, ran the evaluation of 3D de-
tection, and initiated the demo.
Ryan Marten added part of video and tracking datasets.
Derek Hoiem advised on the research direction.
Aniruddha Kembhavi advised on the research direction
and evaluation, helped manage compute resources and
wrote the paper.B. Model Implementation Details
In this section, we present the implementation details of our
model.
B.1. Detailed of Unified Task Representation
First, we provide details about how different modalities are
represented in our model.
Text representation. The Byte Pair Encoding (BPE) vo-
cabulary size is 32000. Similar to [147], we add 200 ad-
ditional special tokens to indicated masked spans when de-
noising. We further add 10 special tokens that can be used
to reference the image, audio, and history input in the text.
Two special tokens are to indicate the ⟨Image Input ⟩and
⟨Audio Input ⟩, and 8 special tokens represent individ-
ual elements in the image and audio history inputs, both of
which have a maximum of 4 frames. We use a maximum of
512 input and output tokens.
Sparse structures representation. We use an additional
1000 special tokens to represent all continuous values, such
as points, boxes, camera transformation, and 3D cuboids.
Points are represented with [y, x]coordinates and boxes
with[y1, x1, y2, x2]coordinates with values normalized by
the image size. Camera transformations are represented as
polar angle θ, azimuth angle ϕ, and distance r. 1000 special
tokens to represent discretized angle from −πtoπ. Fol-
lowing [16], 3D cuboids are represented with 12 parame-
ters including projected center [u, v], virtual depth z, log-
normalized box dimension [ ¯w,¯h,¯l], and continuous allocen-
tric rotation p.
•[u, v]represent the projected 3D center on the image
plane relative to the 2D RoI
•z∈R+is the object’s center depth in meters.
•[ ¯w,¯h,¯l]∈R+are the log-normalized physical box di-
mensions in meters.
•p∈R6is the continuous 6D allocentric rotation.
For 3D cuboid detection, we use prompts to indicate the tar-
get format, such as “ Locate all objects in 3D using projected
3D center, virtual depth, log-normalized box size, and rota-
tion in the image. ”
Action representation. For embodied navigation tasks,
the discrete action space is directly represented as texts,
e.g. “forward ”, “left ”, “right ”, “stop ”. For ob-
ject manipulation tasks, the action is represented differ-
ently based on the robots. Overall, the positional change
(e.g.(∆PosX ,∆PosY ,∆PosZ )), rotational change ( e.g.
(∆RotX ,∆RotY ,∆RotZ )), and gripper open or close are
discretized using the same 1000 special tokens, and we use
the text prompt to indicate the input and target format. For
tasks that require multi-step planning ( e.g. VIMA [87]), the
actions are represented as human-readable texts with the in-
dication of steps, skills used ( e.g. pick, place, or push), and
discretized positional and rotational parameters. Figure 11
provides a detailed illustration of the robot tasks.
21

--- PAGE 22 ---
Audio InputSample rate 16000 Hz
FFT hop length 256 samples
FFT window size 1024
Mel bins 128
Subsegment length 256 hops, ( ≈4.08 sec)
Mel Spectrogram size 128 mels ×256 hops
fmin 0
fmax 8000
AST patch size 16
token size 8×16
Pretrain sub-sample 64
Final size 64 or 128 tokensImage InputViT patch size 16
Pretraining size 384×384
Token size 24×24
Pretrain sub-sample 288
Final size 288 or 576 tokensTextSeq length 512
Final size 512 tokensImage HistoryViT patch size 16
Pretraining size 256×256
Token size 16×16
Pretrain sub-sample 128
Max num segments 4
Latent size 32
Final size 32, 64, 96, 128 tokensAudio HistoryAST patch size 16
Pretraining size 128×256
Token size 8×16
Pretrain sub-sample 64
Max num segments 4
Latent size 16
Final size 16, 32, 48, 64 tokens
Table 8. Input representations details
Images representation. Images are encoded with a pre-
trained ViT [45]. We use the ViT-B checkpoint trained on
LAION 2B dataset2. For image inputs, we use a maximum
length of 576 tokens ( i.e.24×24patch encoding from a
384×384image). We concatenate features from the sec-
ond and second-last layers of the ViT to capture both low
and high-level visual information. To generate the image,
we encode these images as discrete tokens [49]. Different
from U NIFIED -IO [123], which uses the VQ-GAN trained
on ImageNet [41] to convert 256×256resolution image into
16×16tokens, we use the VQ-GAN trained on the Open
Images dataset [103] with a compression ratio of 8 and a vo-
cabulary size of 163843. This converts 256×256resolution
image into 32×32tokens. We also compare the VQ-GAN
2https://github.com/mlfoundations/open_clip
3https://github.com/CompVis/taming-transformerstokenizer with the ViT-VQGAN [208] and MoVQ [223].
We empirically find VQ-GAN leads to best generation re-
sults.
Dense structures representation. To handle this modality,
we convert per-pixel labels into RGB images. For depth,
we construct a grayscale image by normalizing the depth
map. For surface normal estimation, we convert the x/y/z
orientations into r/g/b values. For segmentation, we train
UNIFIED -IO 2 to predict a single black-and-white mask for
a particular object specified by a class and a bounding box.
Instance segmentation (as done in GRIT [66]) can then be
performed by first performing localization for the target
class and then performing segmentation for each detected
box. U NIFIED -IO instead trains the model to produce an
image with a randomly selected color for each instance. We
found this makes post-processing difficult since output im-
ages sometimes do not exactly follow the color scheme, and
the model could struggle with images with many different
instances.
Audio representation. This modality encodes a 4.08-
second segment of audio. We take the waveform sampled
at 16000 Hz and convert it to a log-mel-scaled spectrogram.
We compute the spectrogram for an entire audio segment
(4.08 seconds) simultaneously. Each window involves 1024
samples and 256 samples ‘hops’ between windows. The re-
sulting spectrogram has a size of 128 mel bins with 256
windows. We chose these hyperparameters largely around
efficiency. We then encode this with a pre-trained AST [57]
with the patch size of 16×16, hence a total of 128 tokens.
To generate audio, we use ViT-VQGAN [208] to con-
vert the spectrograms into discrete tokens. Since the authors
of [208] did not release the source code or any pre-trained
models, we implement and train our own version of ViT-
VQGAN with 8×8patch size that encodes a 256×128
spectrogram into 512 tokens with a codebook size of 8196.
The model is trained with the audio on AudioSet [54],
ACA V100M [105], and YT-Temporal-1B [215] datasets.
After getting the log-mel-scaled spectrograms, we use HiFi-
GAN4[98] vocoder to decode the spectrograms back to
waveforms. We train the HiFi-GAN using the same param-
eters shown in Table 8. We trained the model on a mixture
of AudioSet and LJSpeech [85] to cover natural sound and
human voice.
History representation. Images and audio inputs in this
history are first encoded in the same way as image and au-
dio inputs. We then use a perceiver resampler [5] to further
compress the image and audio features and produce a fixed
number of visual outputs (32) and audio outputs (16) to re-
duce the total sequence length of the model. As shown in
Table 8, we consider a maximum of 4 images and audio
segments. In our experiments, we test with two different
variants of perceiver implementations: 1) a small group of
4https://github.com/jik876/hifi-gan
22

--- PAGE 23 ---
latent embeddings query each frame/segment individually
[5, 9], 2) a large group of latent embeddings query all his-
tory at once. While the second implementation can finely
represent the referenced image and audio, the first can pre-
serve better temporal information. Thus, our final imple-
mentation uses the first one.
B.2. 2D Rotary Embedding
We use a rotary position encoding to model the relative lo-
cation of input sequences [169]. We chose this primarily
because we did not want to use absolute (additive) position
embeddings, which would have to be added to the inputs
of each encoder, and also wanted to be consistent with the
LLaMA [177] position encoding.
The rotary encoding uses no parameters and instead uses
a kernel trick to allow the model to recover relative dis-
tances between key and query elements in a transformer’s
attention head. For text, we apply rotary encoding at each
layer of the network. For other modalities, we extend RoPE
to two-dimensional cases by splitting each of the query and
key embeddings of transformer attention heads in half and
apply separate rotary embeddings constructed by each of
the two coordinates to the halves.
We treat each token (image, audio, image history, and
audio history) as having a 2-dimensional position corre-
sponding to 1) h, w coordinates in the image or audio spec-
trogram, 2) (t, l)where tandlrepresent the indices of
frame and perceiver latent vector in the image or audio his-
tory, respectively. Different from [215], which uses a 4-
dimensional position to represent all the inputs, we use a
combination of learnable segment (modality) embeddings
and rotary encoding.
B.3. Dynamic Packing
Here, we describe the dynamic packing algorithm in more
detail. As is standard practice, when batching together in-
puts, we pad input tensors to a maximum length and use
attention masked to prevent the transformer from attending
to padding elements. This, however, is highly inefficient
in our multi-modal setting because many modalities are not
present in most examples, which results in a huge amount
of padding. For example, if one example in the batch has
an image output, every other example must be padded with
1024 target image tokens, even if their output is in a differ-
ent modality.
One solution is to arrange batches so that each batch
contains examples with similar numbers of tokens in each
modality. This is, however, complicated to do in practice
since (1) our data does not fit in RAM, so we cannot easily
sort and group data this way, especially if needing to match
tokens across five input and three output modalities and (2)
our coding framework, JAX [15], does not support variable
length tensors when constructing the execution graph whichL XL XXLTransformerParams 1.1B 3.2B 6.8B
V ocab size 33280
Image vocab size 16512
Audio vocab size 8320
Model dims 1024 2048 3072
MLP dims 2816 5120 8192
encoder layer 24
decoder layer 24
Heads 16 16 24
MLP activations silu, linear
Logits viaembedding True
Dropout 0Image ResamplerLatents size 32
Model dims 768 1024 1024
Heads 12 16 16
Head Dims 64
Number layer 2
MLP Dims 2048 4096 4096
MLP activations geluAudio ResamplerLatents size 16
Model dims 768 1024 1024
Heads 12 16 16
Head Dims 64
Number layer 2
MLP Dims 2048 4096 4096
MLP activations geluViTPatch size 16
Model dims 768
Heads 12
Head Dims 64
Number layer 11
MLP Dims 3072
MLP activations geluASTPatch size 16
Model dims 768
Heads 12
Head Dims 64
Number layer 11
MLP Dims 2048
MLP activations gelu
Table 9. Model Hyperparameters
makes handling variable lengths between batches extremely
difficult.
Instead, we use packing , a process where the tokens of
multiple examples are packed into a single sequence, and
the attentions are masked to prevent the transformer from
cross-attending between examples. Packing is often done
as a pre-processing step when handling text, but this does
not work in our setup since some parts of our network can-
not operate on packed data ( e.g., the VAE or image ViT).
Instead, we start with an unpacked batch of examples, run
23

--- PAGE 24 ---
these components first, and then dynamically pack the re-
sulting tokens in a backdrop-compatible way before running
the transformer. To run efficiently on TPUs we pack exam-
ples using matrix multiplication with carefully constructed
one-hot matrices.
To account for all modalities, the maximum sequence
length our transformer needs to take as input is 1152, and
the maximum target length is 2048. When packing, we can
generally pack two examples into an input sequence of 864
and a target sequence of 1280, which gives a roughly 4x
speed up due to reduced sequence length and the ability
to process two examples simultaneously. When streaming
data, packing cannot be done reliably. For example, if two
consecutive examples have an image output, they cannot be
packed since they will total over 1280 output tokens. To
handle this, we use a heuristic algorithm to re-arrange data
as it is being streamed. The algorithm keeps a small pool of
examples in memory. Given a new example, it pairs it with
the largest example in the pool it can be packed with and
outputs both as a pair. If no such example exists, it adds the
example to the pool. If the pool reaches a maximum size
of 10, the largest example is emitted and processed with-
out being packed with another example. We find this occurs
less than 0.1% of the time during training.
B.4. Full Model Details
In Table 9, we present the full hyperparameters of
our model. During pre-training, we train the UIO-2 L,
UIO-2 XL, and UIO-2 XXLwith a batch size of 512 due to
memory limit. We sub-sample 50% of the image, audio,
and history inputs patches. The total packing length is 864
for the encoder and 1280 for the decoder. During instruc-
tion tuning, we train all of our models with a batch size 256
due to computing constraints. We sub-sample 87.5% of the
image, audio, and history input patches. The total packing
length is 1024 for pretraining and 1280 for instruction tun-
ing. 8-way in-layer parallelism and 64-way data parallelism
were used to scale up to the 7B model training.
We train for 1.5 million steps with an effective batch size
of 512. This results in training on approximately 1 trillion
tokens. During pre-training, we keep at most 50% of the
image patches in the image history or image encoder, as is
common practice with MAE pre-training [71]. We use up
to four images/segments in image/audio history.
C. Pre-Training Details
In this section, we provide additional details about the data
UNIFIED -IO 2 is pre-trained on. The datasets we use for
pre-training are listed in Table 10. Unless otherwise speci-
fied, we use the pre-training objective described in Section
3.3, where one of the present modalities is randomly se-
lected as the target. We sample data to ensure all the output
modalities are well represented and to balance how oftenour various corpora are used based on their size. The distri-
bution is shown in Figure 9.
C.1. Data Sources
Text. Our data follows the mixture used by MPT-7B [176].
Image & Text. Image & text paired data comes from vari-
ous unsupervised corpora, shown in Table 10. For LAION
data, we only generate images from image/text pairs from
LAION aesthetic, which contains higher quality images,
while we generate text for image/text pairs from LAION
400M. We also only keep images from LAION if they are
marked as being unlikely to be NSFW in the LAION meta-
data. Web images is a dataset of images we download and
focuses on icons and stylized images.
Video. We gather a total of 180M short videos from various
sources. During training, we pick a random sequence of up
to five frames from the video. The first four will be encoded
with an image/audio history encoder, while the fifth frame
will be encoded with the image/audio encoder. The text
matching these frames is encoded with a text encoder along
with marker tokens to show where each frame occurred as
stated in B.1, or, if the dataset only includes a single caption
that is not aligned with individual frames, the entire caption
is encoded instead. The text, audio, or image modality can
be selected as the target modality. As usual, other modal-
ities are randomly masked, and the target modality is ran-
domly masked or injected with noise in the input. Note we
have sub-sampled data from many of these corpora to keep
the dataset size more manageable, and sometimes due to
broken video links.
Interleaved Image & Text. We primarily use
OBELICS [104], which contains paragraphs and im-
ages interleaved together. For each document, we randomly
select an image or a paragraph as the target and use up to
the previous four (if the target is an image) or five (if the
target is a paragraph) images as context. The last image is
encoded with the image encoder, and the remaining images
are encoded in the image history. The text matching those
images is concatenated and interjected with marker tokens
to indicate where the images in the image history or image
input occur. We either do de-noising, where a noisy version
of the target is included in the input, or generation, where
the target is not part of the input, although we always
include both the text and image input modalities.
In addition, we construct interleaved data by interleav-
ing multiple images and captions from several image/text
pair corpora. The images are encoded as the image input
and/or the image history, and matching text is constructed
by specifying the caption for one, or all, of these images us-
ing special tokens to mark which image each caption refers
to. For this task, we only target the text modality, and train
the model to either (1) de-noise the caption of a single im-
age, (2) generate a caption for a single image that is speci-
24

--- PAGE 25 ---
Audio
Image TextVideoAgent Trajectories
SyntheticImage/Text Interleaved
Video
Image/Text Pairs
Agent Trajectories
Synthetic
Image/Text Interleaved
Video
Image/Text PairsText
Ego4D
 ACAV
AudioSet
HD-VILA
YT-Temporal
Habitat Next Frame/State Prediction
ProcTHOR Next Frame/State Prediction
Segment Anything
Habitat Easy
Habitat Hard
Objaverse Multiview
 OBELICS
Ego4D
AudioSet
HD-VILA
WebVid
ACAV
YT-Temporal
CC3M
CC12M
RedCaps
Web Images
LAION Aesthetic v2.5
Habitat Next Frame/State Prediction
ProcTHOR Next Frame/State Prediction
RedCaps Patch Counting
LAION Aesthetic Patch Counting
LAION Aesthetic Patch Detect
RedCaps Caption Multiple
CC3M Caption Multiple
RedCaps Denoise One
CC3M Denoise One
CC12M Caption Multiple
RedCaps Caption One
Objaverse Camera Change
CC12M Denoise One
CC3M Caption One
CC12M Caption One
OBELICS
Ego4D
AudioSet
HD-VILA
WebVid
ACAV
YT-Temporal
CC3M
CC12M
RedCaps
Web Images
LAION 400M
RedPajama Book
Stack Markdown
Wikipedia
RedPajama Common Crawl
Stack
C4
MC4Figure 9. Pre-training data distribution, segments proportional to sampling rates. The inner section shows the target modality, the middle
section shows the type of data, and the third shows particular datasets.
fied in an input prompt using a marker token or (3) generate
a sequence of marker tokens and captions that describe each
input image. This task aims to ensure the model learns the
semantics of the images in the history and understands the
marker tokens.
Multi-View. We train on the cross-view completion task
from CroCo [194], where the model must complete a heav-
ily noised image using an image of the same scene, but from
a slightly different angle, as context. The noised input is en-
coded as an image and the second image is encoded through
the image history encoder. In addition, we generate data us-
ing Objaverse [40] objects by capturing multiple views of
the object in 3D, and either specify the camera coordinates
in the input text and train the model to generate a new image
matching new camera coordinates, or train the model to pre-
dict how the camera has moved between different images.We further augment the view synthesis task by providing
in-context examples. For example, by giving one or more
examples of the views and transformations in the image his-
tory, the model predicts the new view from the new camera
transformation specified by the prompt. Both tasks aim to
improve the model’s 3D understanding during pre-training.
Agent Trajectory. We use scripted shortest path trajec-
tories in ProcTHOR [39] and human-collected demonstra-
tions in Habitat [149, 157]. While the original datasets are
for object navigation with relatively long episode lengths,
we only subsample from the last few frames for image his-
tory and image input such that mostly the target object is
within the observation. The task is randomly selected from
1) generating the next visual observation frame as the target
image, 2) predicting the next positional observation coor-
dinates as the text target, and 3) predicting the next action
25

--- PAGE 26 ---
Size Rate Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio
Text 6.6b 33.0✓ - - - - - -✓ - - - -
MC4 [201] 5.0b 11.7 ✓ - - - - - - ✓ - - - -
C4 [68] 266m 10.6 ✓ - - - - - - ✓ - - - -
Stack [95] 147m 3.55 ✓ - - - - - - ✓ - - - -
RedPajama CC [32] 1.2b 3.55 ✓ - - - - - - ✓ - - - -
Wikipedia 6.8m 1.42 ✓ - - - - - - ✓ - - - -
RedPajama Book [32] 13m 1.06 ✓ - - - - - - ✓ - - - -
Stack-Markdown [95] 34m 1.06 ✓ - - - - - - ✓ - - - -
Image/Text 970m 31.3✓ - -✓ - - -✓ - -✓ -
LAION Aesthetics v2.5 [158] 491m 17.7 ✓ - - ✓ - - - - - - ✓ -
LAION-400M [159] 346m 8.95 ✓ - - ✓ - - - ✓ - - - -
CC12M [23] 11m 1.48 ✓ - - ✓ - - - ✓ - - ✓ -
RedCaps [42] 12m 1.39 ✓ - - ✓ - - - ✓ - - ✓ -
Web Images 107m 1.33 ✓ - - ✓ - - - ✓ - - ✓ -
CC3M [163] 3.0m 0.49 ✓ - - ✓ - - - ✓ - - ✓ -
Video 181m 25.0✓ - -✓✓ ✓ ✓ ✓ - -✓✓
YT-Temporal [215] 146m 13.7 ✓ - - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓
ACA V [105] 17m 3.98 ✓ - - ✓ - ✓ -✓ - - ✓ ✓
HD-VILA [200] 7.1m 2.75 ✓ - - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓
AudioSet [54] 1.7m 2.75 ✓ - - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓
WebVid [13] 9.2m 1.23 ✓ - - ✓ ✓ ✓ ✓ ✓ - - ✓ -
Ego4D [60] 0.7m 0.55 ✓ - - ✓ ✓ ✓ ✓ ✓ - - ✓ ✓
Interleaved Image/Text 157m 8.70✓ - -✓ - ✓ -✓ - -✓ -
OBELICS [104] 131m 8.00 ✓ - - ✓ - ✓ -✓ - - ✓ -
CC12M Interleaved 11m 0.35 ✓ - - ✓ - ✓ -✓ - - - -
CC3M Interleaved 3.0m 0.21 ✓ - - ✓ - ✓ -✓ - - - -
RedCaps Interleaved 12m 0.14 ✓ - - ✓ - ✓ -✓ - - - -
Multi-View 3.4m 0.67✓ - -✓ - ✓ - -✓ -✓ -
CroCo Habitat [157, 194] 2.6m 0.33 ✓ - - ✓ - ✓ - - - - ✓ -
Objaverse [40] 0.8m 0.33 ✓ - - ✓ - ✓ - - ✓ -✓ -
Agent Trajectories 1.3m 0.33✓ - -✓ - ✓ -✓ - -✓ -
ProcTHOR [39] 0.7m 0.17 ✓ - - ✓ - ✓ -✓ - - ✓ -
Habitat [157] 0.6m 0.17 ✓ - - ✓ - ✓ -✓ - - ✓ -
Synthetic 504m 1.00✓✓ -✓ - - - -✓ ✓ - -
Segment Anything [94] 1.1m 0.50 ✓ ✓ -✓ - - - - - ✓ - -
Laion Aesthetics Patches 491m 0.45 ✓ - - ✓ - - - - ✓ - - -
RedCaps Patches 12m 0.05 ✓ - - ✓ - - - - ✓ - - -
All 8.5b 100✓✓ -✓✓ ✓ ✓ ✓✓ ✓✓✓
Table 10. Datasets used for pre-training, rate shows the sampling percentage during pre-training and size shows the approximate number
of examples if iterating through the data once.
as the text target. 1) requires inferring from the image and
image history input and the last action specified in the text
input, 2) further requires the location information, and 3) is
based on the target object name and visual observations for
the next action prediction.
Synthetic. We add two synthetic tasks. First, we use the
automatically annotated data from Segment Anything [94].
We give the model either a set of points or a bounding box as
input and train it to generate a segmentation mask as output.
Second, we add artificial patches of various shapes and col-ors to images from other unsupervised datasets and train the
model to output their locations in order to train the model
to generate sparse coordinates as output. We additionally
train the model to output the total number of patches on the
image to pre-train its counting abilities.
D. Instruction Tuning Details
In this section, we provide additional details about the in-
struction tuning data and individual tasks U NIFIED -IO 2
26

--- PAGE 27 ---
Embodied AI
Natural LanguageAudio UnderstandingVideo Sparse LabellingVideo UnderstandingImage Dense LabellingImage Sparse LabellingImage UnderstandingAudio Generation
Image Generation
Embodied QA
Goal Generation
Next Frame/State Prediction
Action Prediction
Language Modeling
Text Instruction FollowingAudio CaptioningAudio Tagging
Video Sound LocalizationVideo Action LocalizationVideo Tracking
Video Instruction FollowingVideo Question AnsweringVideo CaptioningVideo Tagging
Optical FlowDepth EstimationReferring Expression_SegmentationSurface Normal EstimationLocalized SegmentationSemantic_SegmentationKeypoint DetectionText Detection3DReferring ExpressionObject LocalizationObject DetectionImage Pair _QA
Image Instruction FollowingRegion Captioning
Relationship PredictionImage TaggingRegion ClassificationImage CaptioningVQAAudio from Video
Audio from TextView_SynthesisImage Inpainting
Next Frame Generation
Image Editing
Controllable Image Editing
Image from Text
FrankaKitchen Caption
Habitat PointNav Affordance
BridgeData V2 Caption
VIMA detection
Language Table (sim) Caption
Language Table (real) Caption
FrankaKitchen
BridgeData V2
VIMA
Habitat Next Frame/State Prediction
ProcTHOR Next Frame/State Prediction
FrankaKicthen
VIMA
VIMA No Masking
VIMA w/BBox Prediction
BridgeData V2
Language Table (sim)
VIMA Pure Text
Habitat PointNav
Language Table (real)
Habitat PointNav w/State Prediction
VIMA w/BBox and Masking
VIMA w/BBox Input
RedPajama Book
Wikipedia
RedPajama Common Crawl
Stack
C4
MC4
MBPP Code
Flan2 Dialog
Flan2 CoT
Dolly
Code Aplaca
OASST
Flan2 SuperNLI
Alpaca
Flan2 T0
Flan2 FLan2021
LJ Speech
MACS
MusicCaps
Clotho
AudioCaps
MACS
Kinetic Sounds
AudioSet
VGG-Sound
Flickr SoundNet
AVA
GOT
LaSOT
YouTube-BB
M3IT MSRVTT QA
MIMIC-IT Navigation Dialogue
MIMIC-IT Navigation
MIMIC-IT Dense Captioning
MIMIC-IT Story Telling
MIMIC-IT TV Caption
M3IT MSVD-QA
M3IT SS
M3IT iVQA
Ego4D Talking to Me
Ego4D Looking at Me
MSVD
VTVQA
STAR
MSRVTT-QA
MSRVTT-Captioning
VATEX
WebWid
UCF101 Action Tagging
Action Anticipation EK100
Action Tagging EK100
SS V2
SS V2 Natural
Kinetics710
Fly Chairs
MPI Sintel
NYU Depth
Recoco Google
Recoco UNC
Refcoco UNC Plus
BlendedMV
BlendedMV Aug.
Framenet
Framenet Aug.
Taxonomy
Taxonomy Aug.
LVIS
COCO
Open Images
VizWiz Grounding
LVIS Aug.
COCO Aug.
Open Images Aug.
COCO Aug.
COCO Text Detection
COCO Text
COCO Text Multi-Localization
COCO Text Localization
Objaverse Camera Change
Omni3D
Refcoco Google
Refcoco UNC
Refcoco UNC Plus
 Recoco UNC Pair Aug.
Refcoco UNC Aug.
Recoco Google Pair Aug.
Refcoco Google Aug.
COCO
Visual Genome Class Subset
Open Images
Visual Genome
Visual Genome Multi-Localization
COCO
COCO Multi-Localization
Visual Genome Aug.
COCO Aug.
Open Images Multi-Localization
Open Images Aug.
MIMIC-IT Spot Difference Subtle
MIMIC-IT Spot Difference General
M3IT NLVR
M3IT Mocheg
LLaVA Instruction MC
LLaVAR Instruct MC
M3IT Visual Dialog
LLaVA Instruct
Visual Genome
VSR
VSR Binary
VG Relation from Expression
VG Relations from BBox
Visual Genome
Open Images
M3IT IQA
CalTech Birds 2011
M3IT COCO ITM
M3IT ImageNet
Sun397
iNaturalist
Imagenet
Places 365
M3IT COCO GOI
COCO Region Cls
Visual Genome Region Cls
Open Images Region Cls
M3IT Paragraph Captioning
VizWiz
SCICAP
Text Caps From History
TextCaps
M3IT TextCaps
CC3M
CC3M Aug.
COCO
COCO Aug.
Open Images
Open Images Aug.
RedCaps
RedCaps Aug.
M3IT ViQuAE
M3IT OKVQA
M3IT OCR VQA
M3IT Shapes
M3IT VisualMRC
M3IT A-OKVQA
M3IT CVR
OKVQA-v1
M3IT ST-VQA
M3IT Text VQA
M3IT CLEVR
M3IT GQA
M3IT VQA 2.0
OK-VQA-2
VizWiz
BlendMVS
TallyQA MC
Text VQA
GQA Sentence Answer
ScienceQA
ScienceQA Rationale
Framenet
Taxonmy
GQA
COCO Text
OCR-VQA
TallyQA
VQA 2.0
Visual Genome
VCR
YT-Temporal
LJ Speech
MACS
MusicCaps
Clotho
VGG-Sound
AudioCaps
ACAV
AudioSet
Objaverse
Objaverse Multiview
Habitat Easy
Habitat Hard
Visual Genome
COCO
Open Images
GOT
LaSOT
MagicBrush
pix2pix
COCO
LVIS Segmentation
Multigen Combination
Multigen
COCO
Open Images
CC3m
RedCap
LAION Aesthetic v2.5Figure 10. Sunburst chart of our instruction-tuning mixtures, sections are proportional to sampling rates.
supports. An overview of the instruction tuning data is
shown in Table 11. We show a visualization including indi-
vidual datasets in Figure 10. We sample broad categories of
tasks evenly and then generally sample individual datasets
in proportion to the square root of their size, although with
some minor hand-engineered adjustments to downweight
noisy datasets or upweight very rare tasks.
D.1. Natural Language
For natural language data we use the mixture from
FlanV2 [122], which in turn includes data from Muf-
fin [193], T0-SF [156], NIV2 [191], and CoT annotations,
as well data from Alpaca [142], Dolly [33], Open Assis-
tant [99], and MDPP [8]. In addition, we continue pre-
training on our unsupervised NLP mixture from our fine-
tuning stage to ensure the model does not forget informa-
tion learned from unsupervised data during the extensiveinstruction-tuning stage.
D.2. Image Generation
For text-to-image generation, we use the same image/text
pairs we used during pre-training, as well as localized
narratives from Open Images [103] and captions from
COCO [115] and Visual Genome (VG) [102]. Our prompts
for these tasks specify that the image might be noisy or ap-
proximate for unsupervised corpora ( e.g. “Generate an im-
age that roughly matches this text: {caption }”) and give
hints as to the style for supervised corpora ( e.g. “What
do you see in this image? Plainly describe the individ-
ual element you observe.” for localized narratives) to help
disambiguate the stylistic differences between the datasets.
We use simple prompts ( e.g. “Caption this image.”) for the
COCO captions.
We additionally train the model to generate images
27

--- PAGE 28 ---
Size Rate Datasets Text Sparse Dense Image Audio ImageH AudioH Text Sparse Dense Image Audio
Image Generation 506m 17.6 21 ✓✓ ✓✓ - ✓ ✓ ✓ - -✓ -
Image from Text 497m 10.6 5 ✓ - - - - - - - - - ✓ -
Controllable Image Editing 3.0m 2.92 4 ✓ -✓ ✓ - ✓ - - - - ✓ -
Image Editing 1.1m 1.66 3 ✓ - - ✓ - - - - - - ✓ -
Next Frame Generation 24k 0.96 2 ✓ ✓ - - - ✓ ✓ - - - ✓ -
Image Inpainting 1.0m 0.79 3 ✓ ✓ -✓ - - - - - - ✓ -
View Synthesis 4.2m 0.60 4 ✓ - - ✓ - ✓ -✓ - - ✓ -
Audio Generation 164m 7.50 9✓ - -✓✓ ✓ ✓ - - - -✓
Audio from Text 19m 5.62 8 ✓ - - - - - ✓ - - - - ✓
Audio from Video 145m 1.88 1 ✓ - - ✓ ✓ ✓ ✓ - - - - ✓
Image Understanding 53m 17.8 73 ✓✓ -✓ - ✓ -✓ - - - -
VQA 5.8m 6.23 31 ✓ - - ✓ - - - ✓ - - - -
Image Captioning 32m 4.25 14 ✓ - - ✓ - - - ✓ - - - -
Region Classification 6.1m 2.41 4 ✓ ✓ -✓ - - - ✓ - - - -
Image Tagging 3.8m 2.38 8 ✓ - - ✓ - - - ✓ - - - -
Relationship Prediction 0.8m 1.41 6 ✓ ✓ -✓ - - - ✓ - - - -
Region Captioning 3.5m 0.60 1 ✓ ✓ -✓ - - - ✓ - - - -
Image Instruction Following 0.4m 0.37 6 ✓ - - ✓ - - - ✓ - - - -
Image Pair QA 0.1m 0.17 3 ✓ - - ✓ - ✓ -✓ - - - -
Image Sparse Labelling 13m 7.25 26 ✓✓ -✓ - ✓ - -✓ -✓ -
Object Detection 5.3m 3.08 9 ✓ - - ✓ - - - - ✓ - - -
Object Localization 6.0m 1.31 3 ✓ - - ✓ - - - - ✓ - - -
Referring Expression 0.2m 1.08 7 ✓ - - ✓ - - - - ✓ - - -
3D 1.0m 1.00 2 ✓ - - ✓ - ✓ - - ✓ -✓ -
Text Detection 37k 0.41 3 ✓ - - ✓ - - - - ✓ - - -
Keypoint Detection 0.3m 0.38 2 ✓ ✓ -✓ - - - - ✓ - - -
Image Dense Labelling 6.9m 4.06 19 ✓✓ -✓ - ✓ - - -✓ - -
Semantic Segmentation 2.4m 1.23 4 ✓ - - ✓ - - - - - ✓ - -
Localized Segmentation 3.2m 1.17 3 ✓ ✓ -✓ - - - - - ✓ - -
Surface Normal Estimation 1.1m 1.03 6 ✓ - - ✓ - - - - - ✓ - -
Referring Expression Segmentation 0.1m 0.47 3 ✓ - - ✓ - - - - - ✓ - -
Depth Estimation 47k 0.11 1 ✓ - - ✓ - - - - - ✓ - -
Optical Flow 24k 0.06 2 ✓ - - ✓ - ✓ - - - ✓ - -
Video Understanding 13m 10.6 24 ✓ - -✓✓ ✓ ✓ ✓ - - - -
Video Captioning 9.1m 3.75 3 ✓ - - ✓ - ✓ ✓ ✓ - - - -
Video Tagging 1.1m 3.75 6 ✓ - - ✓ - ✓ ✓ ✓ - - - -
Video Question Answering 2.5m 2.84 9 ✓ - - ✓ ✓ ✓ ✓ ✓ - - - -
Video Instruction Following 0.2m 0.21 6 ✓ - - ✓ - ✓ ✓ ✓ - - - -
Video Sparse Labelling 0.4m 3.42 5✓✓ -✓ - ✓ ✓ -✓ - - -
Video Tracking 0.2m 2.50 3 ✓ ✓ -✓ - ✓ ✓ -✓ - - -
Video Action Localization 0.2m 0.61 1 ✓ - - ✓ - ✓ ✓ -✓ - - -
Video Sound Localization 2.5k 0.31 1 ✓ - - ✓ - ✓ ✓ -✓ - - -
Audio Understanding 2.2m 2.50 10 ✓ - -✓✓ ✓ -✓ - - - -
Audio Tagging 2.1m 1.25 5 ✓ - - ✓ ✓ ✓ -✓ - - - -
Audio Captioning 75k 1.25 5 ✓ - - - ✓ - - ✓ - - - -
Natural Language 11m 25.0 17 ✓ - - - - - -✓ - - - -
Text Instruction Following 11m 12.5 10 ✓ - - - - - - ✓ - - - -
Language Modeling - 12.5 7 ✓ - - - - - - ✓ - - - -
Embodied AI 7.2m 4.33 23 ✓ - -✓ - ✓ -✓✓ -✓ -
Action Prediction 4.3m 3.37 12 ✓ - - ✓ - ✓ -✓ - - - -
Next Frame/State Prediction 1.3m 0.33 2 ✓ - - ✓ - ✓ -✓ - - ✓ -
Goal Generation 0.7m 0.33 3 ✓ - - ✓ - ✓ - - - - ✓ -
Embodied QA 1.0m 0.30 6 ✓ - - ✓ - ✓ -✓ ✓ - - -
All Tasks 775m 100 227 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Table 11. Instruction tuning training mixture. Due to the number of datasets used, we group them by task and only show statistics for each
group. The rate shows the sampling rate, size shows the number of examples of iterating through the data once, and datasets show the
number of individual data sources used for the tasks.
28

--- PAGE 29 ---
through view synthesis [40, 194] as was done during pre-
training. We also integrate data for image editing [18, 217]
and image editing based on various dense control signals
such as depth maps, edges, segmentation, etc. Follow-
ing [145], and the segmentation-based image generation
from U NIFIED -IO using data from COCO and LVIS [62].
Finally, we train on inpainting by masking a region of an
input image that contains an object and training the model
to generate the complete image given the object name and
location. We derive data for this task from the object anno-
tation data in COCO, Open Images, and VG.
During inference, we use top-p sampling, also known as
nucleus sampling [75], for generating images with the tem-
perature t= 1.0andp= 0.95. We also enable classifier-
free guidance [74] by replacing the prompt with the un-
informative prompt “An image of a random picture.” 10%
of the time during training. That prompt is then used as the
classifier-free prompt with a guidance scale of α= 10 .0
during inference.
D.3. Audio Generation
Datasets for audio generation from text include Audio-
Caps [93], Clotho [47], MACS [131], MusicCaps [2], and
LJSpeech [85]. During training, we divided the audio into
4-second-long segments and then generated one segment of
the target audio, giving both the text and any previous seg-
ments as input. We also train on the next-frame prediction
task, which aims to generate the audio for the next frame in
a video from YT-Temporal-1B [215].
Our prompts for these tasks specify the characteristics
of target audio; e.g., “Generate the sound/music based on
the description: {caption }” for natural sound and music,
respectively, and “Speak: {passage }” for speech. We use
the same sampling method as the image generation, the top-
p sampling with the temperature t= 1.0andp= 0.95. We
do not use the classifier-free guidance because it can lead to
poor performance. When generating audio longer than 4.08
seconds during inference, we generate an initial segment
that is 4.08 seconds long and then extend it by generating
additional segments using previous audio segments as the
audio history input.
D.4. Image Understanding
These tasks require generating text in response to a query
about an image or a pair of images. We use the data from
M3IT [112] and MIMIC-IT [107, 108], as well as a variety
of other additional sources. For VQA, we add GQA [83],
TallyQA [1], OK-VQA [130], A-OKVQA [160], OCR-
based VQA datasets [133, 165], Visual Genome, Sci-
enceQA [124], VCR [213] and VizWiz [67]. For im-
age tagging we add Caltech Birds [195], iNaturalist [180],
Sun397 [197], and Places365 [224]. For region classifica-
tion, we add examples derived from object annotation fromOpen Images, VG, and COCO. We categorize datasets with
open-ended responses such as LLaVa [119], Visual Story-
telling [82], and Visual Dialog [36] as visual instruction
following, and we categorize NLVR [171] and the “spot the
differences” tasks from MIMIC-IT as image pair QA. For
image pair QA tasks, we encode the second image in the
image history modality.
We also add a grounded relationship prediction task us-
ing data from Visual Genome and VSR [116] as well as
image captioning using the same supervised sources we use
for image generation.
We again put stylistic hints in the prompts for these tasks.
For example, in VQA and captioning datasets, we specify
to return a short answer ( e.g. “Answer this question very
succinctly: {question }”), which we find is critical to allow
the model to produce longer, more natural responses when
asked user questions. Likewise, we roughly specify the kind
of class to output for image tagging, e.g., “”What is the sci-
entific name of this animal?” for the iNaturalist dataset.
D.5. Image Sparse Labelling
These tasks require outputting sparse coordinates based on
an input image. We use Open Images, Visual Genome, and
COCO for object detection and localization, which requires
detecting all objects belonging to a specific class and three
COCO referring expression datasets [91, 136, 209] for re-
ferring expressions.
In addition, we train on the OmniLabel [16] 3D detection
dataset by generating the projected 3D center, virtual depth,
log-normalized box size, and rotation of each 3D box, again
by normalizing these values between 0 and 1 and then en-
coding them using the location tokens. We also added the
camera pose prediction tasks using Objaverse objects that
were used during pre-training.
We include 3 text detection datasets from COCO-
Text [183], including finding the bounding box of an input
text string for multiple text strings or finding and listing all
text along with their bounding boxes in an image.
Lastly, we do keypoint detection using COCO pose data.
For keypoint detection, we input a bounding box around a
person in the image and train the model to return a list of
keypoints for that person. During inference, we first localize
all people in the image and then use each returned bounding
box as a keypoint query to find that person’s keypoints. Dur-
ing training, the model predicts “MISSING” for keypoints
that are not visible ( e.g. “right elbow: MISSING”). During
inference, we use a masking function over the model’s logit
to force it to guess a valid point for each keypoint since the
keypoint metric does not award points for correctly identi-
fying a keypoint as being not visible.
29

--- PAGE 30 ---
D.6. Image Dense Labelling
We do several image labeling tasks, including surface nor-
mal estimation on FramNet [78], BlendedMVS [204] and
Taskonomy [211], depth on NYU Depth [138], and optical
flow on Flying Chairs [44] and MPI Sintel [21].
We additionally train on several segmentation tasks: se-
mantic segmentation (segmenting a particular class), lo-
calization segmentation (segmenting an object in an input
bounding box), and referring expression segmentation (seg-
menting an object matching a referring expression). Data
comes from Open Images, COCO, LVIS, and referring ex-
pressions from the COCO refexp datasets [91, 136, 209]. To
do instance segmentation, as needed for GRIT, we first do
localization on the target class and then perform localized
segmentation on each returned bounding box.
During inference, we do temperature sampling with a
top-p of 0.95 as before, but without classifier-free guidance.
For segmentation, we find it beneficial to increase the value
of p to 0.97.
D.7. Video Understanding
These tasks require generating text in response to a query
about a video. For video captioning, we add V A-
TEX [190] and MSR-VTT [199]. For action classifica-
tion (video tagging), we add UCF101 [168], Kinetics-
710 [111], Something-Something v2 [58] and EPIC-
KITCHENS-100 [35]. We also use examples from EPIC-
KITCHENS-100 for action anticipation. For video question
answering, we add MSRVTT-QA [198], MSVD-QA [198],
STAR [196] and M4-ViteVQA [221]. Lastly, we use ex-
amples from M3IT and MIMIC-IT for the video instruction
following.
To cover the visual content of the entire video with a
small number of frames (5), we use the segment-based sam-
pling following [185]; we first divide the video into five
segments of equal duration and then randomly sample one
frame from each of the segments during training, and the
middle frame at inference. We use the first four frames as
the image history input and the final frame as the image in-
put for action classification and video captioning. We em-
pirically found that using the third frame as the image input
while using the other frames as the image history input per-
forms better for video question answering.
We use similar prompts to those for image understanding
tasks, e.g., “Write a short description of this video.”, “The
question {question }can be answered using the video. A
short answer is” and “What are they doing in this video?
Short answer:” in video captioning, video question answer-
ing, and video tagging, respectively, for ensuring a short
answer.D.8. Video Sparse Labelling
We do single object tracking and spatial-temporal action lo-
calization on video data. We train on YouTube-BB [151],
LaSOT [50] and GOT-10k [79] by inputting bounding boxes
around a target object in each of previous frames and hav-
ing the model return the next location as a bounding box
(“Anticipate the object’s next location from all previous
images and the location of the object in those frames:
{locations }.”). We also train the model on A V A [61] by
inputting a video snippet consisting of five frames and re-
quiring the model to detect all actions of humans appearing
in the middle (third) frame of the video snippet (“Given the
temporal context from the video, detect all of the humans
performing actions in the image.”). Note that we provide
the video snippet, not a single video frame, because some
of the actions require temporal context to answer ( e.g., stand
and sit) correctly. We use the final/middle frame of five con-
secutive frames in the video as the image input and the other
frames as the image history input for single object tracking
and action localization, respectively.
D.9. Audio Understanding
We train the model on audio tagging and audio captioning
tasks. For audio tagging, we add AudioSet [54], VGG-
Sound [24], and MACS. For audio captioning, we use the
same datasets as text-to-audio generation, that is, Audio-
Caps, Clotho, MACS, MusicCaps, and LJSpeech. For
audio-visual action classification, we train on Kinetics-
Sounds [7] and VGG-Sound.
We again use stylistic hints in the prompts for these tasks.
For example, we specify the characteristics of target audio
(e.g., “Describe the music.” and “Transcribe the audio to
text.” for MusicCaps and LJSpeech, respectively), enforce
a short answer ( e.g., “What is this in the audio? Short an-
swer:” and “Give a short description of this audio.”), and
specify the kind of class to output for audio tagging, e.g.,
“This audio depicts a scene of a” for MACS. We use the
same prompts as video tagging for audio-visual action clas-
sification.
We use the same sampling strategy as the video under-
standing; we sample five audio segments with uniform in-
tervals from the whole audio and use the middle/final audio
segment as the audio input while using the other segments
as the audio history input for audio classification and audio
captioning, respectively.
D.10. Embodied AI
While many robot manipulation tasks can be formulated by
multimodal prompts that interleave language and images
or video frames, we use VIMA-Bench [87] to evaluate the
robot manipulation skills. We use the image input as the
initial observation of the environment and the image history
for the images or videos in the prompt. The text inputs, or
30

--- PAGE 31 ---
I m a g e  H i s t o r y I m a g e  I n p u t[ T e x t ]  [ S ]  F o l l o w  th i s  m o t i o n  f o r  < i m a g e _ h i s t r o y _ 1 > :  < i m a g e _ h i s t o r y _ 2 >  < i m a g e _ h i s t o r y _ 3 >  < i m a g e _ h i s t o r y 4 > .   N o w  p r e d i c t  a c t i o n s  w i th  
th e  f o r m a t :  " s t e p :  s t a r t - a c t i o n  f r o m  (  x 1  y 1  r 1  )  a n d  e n d - a c t i o n  t o  (  x 2  y 2  r 2  ) "  b a s e d  o n  th e  a g e n t ' s  i n i t i a l  o b s e r v a t i o n  < i m a g e _ i n p u t > .T e x t  I n p u t
F o l l o w  th i s  m o t i o n  f o r            :                                                                                              .         .
1 :  p i c k  f r o m  (  < e x t r a _ i d _ 4 2 4 >  < e x t r a _ i d _ 6 6 5 >  < e x t r a _ i d _ 2 0 0 >  )  a n d  
p l a c e  t o  (  < e x t r a _ i d _ 6 7 4 >  < e x t r a _ i d _ 9 1 4 >  < e x t r a _ i d _ 2 0 0 >  )  
2 :  p i c k  f r o m  (  < e x t r a _ i d _ 6 6 8 >  < e x t r a _ i d _ 6 7 1 >  < e x t r a _ i d _ 2 0 0 >  )  a n d  p l a c e  t o  (  < e x t r a _ i d _ 6 6 8 >  < e x t r a _ i d _ 9 2 1 >  < e x t r a _ i d _ 1 1 9 9 >  )( 0 . 2 2 ,  0 . 4 6 ,  0 . 0 0 )
( 0 . 4 7 ,   0 . 7 1 ,   0 . 0 0 )( 0 . 4 6 ,  0 . 4 7 ,  0 . 0 0 )
( 0 . 4 6 ,  0 . 7 2 ,  0 . 9 9 )P r i m i t i v e :  
P i c k  &  P l a c e S t e p  1 : S t e p  2 :
T e x t  T a r g e tP o s  X ,   P o s  Y ,  R o t  Z P o s  X ,   P o s  Y ,  R o t  Z
[ T e x t ]  [ S ]  T h e  a g e n t  i s  a d v a n c i n g  t o w a r d s  (  < e x t r a _ i d _ 7 1 8 >  < e x t r a _ i d _ 6 5 3 >  ) ,  
a s s e s s  th e  n e x t  a c t i o n  w i th  th e  d i s c r e t e  a c t i o n  s p a c e  " f o r w a r d ,  l e f t ,  r i g h t ,  s t o p "  b a s e d  o n  a g e n t ' s  h i s t o r i e s  w i th  th e  f o r m a t  " o b s e r v a t i o n  (  x  z  y  )  a c t i o n " :

< i m a g e _ h i s t o r y _ 1 >  (  < e x t r a _ i d _ 6 8 6 >  < e x t r a _ i d _ 6 4 2 >  < e x t r a _ i d _ 2 1 1 >  )  f o r w a r d  
< i m a g e _ h i s t o r y _ 2 >  (  < e x t r a _ i d _ 6 8 6 >  < e x t r a _ i d _ 6 4 0 >  < e x t r a _ i d _ 2 1 1 >  )  f o r w a r d  < i m a g e _ h i s t o r y _ 3 >  (  < e x t r a _ i d _ 6 8 6 >  < e x t r a _ i d _ 6 3 7 >  < e x t r a _ i d _ 2 1 1 >  )  r i g h t  < i m a g e _ h i s t o r y _ 4 >  (  < e x t r a _ i d _ 6 8 6 >  < e x t r a _ i d _ 6 3 7 >  < e x t r a _ i d _ 1 1 8 2 >  )  f o r w a r d  < i m a g e _ i n p u t >  (  < e x t r a _ i d _ 6 8 6 >  < e x t r a _ i d _ 6 3 5 >  < e x t r a _ i d _ 1 1 8 2 >  )f o r w a r d
[ I m a g e ]  [ S ]  G i v e n  th e  i n i t i a l  o b s e r v a t i o n  < i m a g e _ i n p u t >  a n d  p r o m p t  
" O p e n  th e  d r a w e r . " ,  p r e d i c t  th e  g o a l  i m a g e .
[ T e x t ]  [ S ]  I m a g i n e  y o u  a r e  th e  r o b o t  i n  th e  s c e n e  w i th  th e  a c t i o n  s p a c e  ( x ,  y ,  z ,  r o l l ,  p i t c h ,  y a w ,  g r i p p e r ) .  B a s e d  o n  th e  p r o m p t  " M o v e d  th e  
s i l v e r  p o t  t o  th e  r i g h t  s i d e  o f  th e  t a b l e . "  a n d  c u r r e n t  o b s e r v a t i o n  < i m a g e _ i n p u t > ,  g u e s s  th e  f o l l o w i n g  a c t i o n .  H i s t o r y :
 
< i m a g e _ h i s t o r y _ 1 > :  (  < e x t r a _ i d _ 6 9 6 >  < e x t r a _ i d _ 7 0 5 >  < e x t r a _ i d _ 7 2 6 >  < e x t r a _ i d _ 7 6 7 >  < e x t r a _ i d _ 6 8 3 >  < e x t r a _ i d _ 7 1 1 >  < e x t r a _ i d _ 2 0 0 >  )  < i m a g e _ h i s t o r y _ 2 > :  (  < e x t r a _ i d _ 7 0 5 >  < e x t r a _ i d _ 6 7 0 >  < e x t r a _ i d _ 7 4 2 >  < e x t r a _ i d _ 7 5 3 >  < e x t r a _ i d _ 6 5 6 >  < e x t r a _ i d _ 6 7 4 >  < e x t r a _ i d _ 2 0 0 >  )  < i m a g e _ h i s t o r y _ 3 > :   (  < e x t r a _ i d _ 7 0 3 >  < e x t r a _ i d _ 6 3 2 >  < e x t r a _ i d _ 7 2 8 >  < e x t r a _ i d _ 7 6 5 >  < e x t r a _ i d _ 6 6 5 >  < e x t r a _ i d _ 6 5 6 >  < e x t r a _ i d _ 2 0 0 >  )  < i m a g e _ h i s t o r y _ 4 > :  (  < e x t r a _ i d _ 6 9 6 >  < e x t r a _ i d _ 6 2 2 >  < e x t r a _ i d _ 7 0 6 >  < e x t r a _ i d _ 7 5 0 >  < e x t r a _ i d _ 6 9 5 >  < e x t r a _ i d _ 6 4 5 >  < e x t r a _ i d _ 2 0 0 >  )
(  < e x t r a _ i d _ 6 8 7 >  < e x t r a _ i d _ 6 0 2 >  < e x t r a _ i d _ 6 9 5 >  < e x t r a _ i d _ 7 5 0 >  < e x t r a _ i d _ 6 9 1 >  < e x t r a _ i d _ 6 2 5 >  < e x t r a _ i d _ 2 0 0 >  )
[ T e x t ]  [ S ]  W h a t  i s  th e  r o b o t  d o i n g ? m o v e  th e  y e l l o w  h e a r t  t o w a r d s  u pI m a g e  H i s t o r yI m a g e  I n p u t
T e x t  T a r g e tT e x t  I n p u t
V i s u a l  O b s e r v a t i o n         P o s  X                    P o s  Z                    R o t  Y                A c t i o nP o i n t  G o a l :            P o s  X                    P o s  Z
T e x t  I n p u t
T e x t  T a r g e tV i s u a l  O b s e r v a t i o n      △  P o s  X             △  P o s  Y           △  P o s  Z              △  R o t  X               △  R o t  Y              △  R o t  Z             G r i p p e r
T e x t  I n p u t
T e x t  I n p u t
I m a g e  I n p u t I m a g e  T a r g e tT e x t  T a r g e tI m a g e  H i s t o r yFigure 11. Examples of input and target representations for embodied and robot tasks.
the language instructions, also include special tokens to ex-
plicitly express the interleaved multimodal prompt. The ac-
tion space consists of primitive actions of “pick and place”
for tasks with a suction cup as the end effector or “push”
for tasks with a spatula. Both primitive actions contain two
poses and one rotation ∈R3, specifying the start and target
states of the end effector.
With the action representation described in B.1, we
seamlessly add large-scale manipulation datasets Language
Table [127], BridgeData V2 [184], and FrankaKitchen [63]
with the continuous control in both simulated and real-
world environments. The model directly predicts the next
action as the text target based on the current observation
as image input, previous frames as image history, and lan-
guage instruction and previous actions as text inputs.
Due to the non-causality of the model and limited se-
quence length for the image history, we only added the
PointNav task from Habitat [157] Gibson scenes for the
navigation. The model is required to predict the next action,
with random augmentation for predicting the next position
and rotation state, based on the point goal (positions ∈R2),
visual observations, and previous actions and states, if any.
D.11. Task Augmentation
In addition to these sources, we derive several additional
tasks that use the same supervised annotations as other tasksbut require performing slightly different functions. We call
this task augmentation. The new tasks include prompts that
specify the desired output. These tasks serve to add diver-
sity to our instruction following data. We review the task
augmentation we construct below.
Segmentation. We build several augmentations of the seg-
mentation tasks, including (1) segmenting pixels belonging
to one of a set of 2-4 categories, possibly including cate-
gories that do not exist in the image, (2) segmenting pix-
els belonging to a class and are within an input bound-
ing box and (3) build a map of pixels that do not belong
to a set 1-4 classes. Prompts are designed for these that
state the requirement, e.g., “Show pixels that are part of
chair, paper and in <extra id289> < extra id871> < ex-
traid781> <extra id1156>”.
Detection and Referring Expression. For detection, local-
ization, and referring expressions, we also train the model to
output various properties of the output bounding boxes in-
stead of the boxes themselves. Properties include the width,
height, area, left/right/top/bottom half, center coordinates,
distance from the left/right/top/bottom edge of the image,
or the coordinates of different corners of the bounding box.
We also change the format of the output bounding box ( e.g.,
[x1, y1, w, h]instead of [y1, x1, y2, x2]format), and change
whether the model labels the boxes with the object category
or not.
31

--- PAGE 32 ---
Prompt Model Response
A video of a man (woman ) saying U NIFIED -IO 2 is a model that works with vision, language, audio, and action. ()
A video of a man playing guitar. 
 
Table 12. Audio generation samples from the pre-trained model.
Input imageRotate rightRotate rightMove forwardMove forwardRotate rightMove forward
Figure 12. Future frame prediction samples from the pre-trained model. Given the initial input image and action, the model can generate a
plausible frame that reflects the result of the action.
For detection, we train the model to detect any object
belonging to a set of 1-4 classes. For referring expressions,
we train the model to locate multiple referring expressions
from a single query. In this case, we sometimes train the
model to predict a property of both referenced boxes in-
stead of outputting the directly, for example, which box is
the smallest, which is the largest, the area of intersection, a
box containing both boxes, etc.
Relationship Prediction. We train the model to list all re-
lationships between a particular object in the image and any
other object. A bounding box and category specify the tar-
get object. Similarly, we train the model to predict all rela-
tionships between any instance of a particular class of ob-
jects and any other object in the image.
Captioning. For captioning, we train the model to generate
a caption that is longer or shorter than a given character or
word length or contains a particular word or set of words.
We also randomly require the caption to start with a partic-
ular prefix. Again, these requirements are specified in the
prompt, for example, “Generate a caption longer than five
words for this image. Start your output with the text ‘My
caption is:’”.
Surface Normal Estimation. For surface normal estima-
tion, we train the model to generate RGB images that en-
code the pixel orientation differently. This includes chang-
ing which RGB channels correspond to the x, y, and z ori-
entations and only including a subset of those orientations.
We also include tasks that require specifying the x, y, and
z orientation at a particular point specified in the prompt
using location tokens. Finally, we include tasks requiring
segmentation masks over pixels with particular orientations,
e.g., “Build a binary mask over surfaces with an upward ori-
entation”.
Figure 13. Image generation samples from the pre-trained model.
Prompts from left to right: 1) An image of an astronaut riding a
horse in the forest. There is a river in front of them with water
lilies, Fantasy, HD. 2) A image of a cat wearing sunglasses, HD.
3) A image of a black german shepherd wearing a red beret, HD.
4) An image of a stop sign in a Fantasy style with the text “1991”.
Embodied AI. We further augment the embodiment
datasets with the video QA and goal image generation tasks.
The QA augmentation aims for the robot’s planning and af-
fordance. For example, given a robot video trajectory, the
model is supposed to predict the plan (caption), or whether
a given action is reasonable from the language instruction.
Applying image editing in embodied space, we further let
the model generate the goal or subgoal images based on
the initial visual observation in the image input and the lan-
guage prompt in the text input. While recent works show
that embodiment QA with VLM [46, 162] and (sub-)goal
generation with diffusion model [134] are effective in the
decision-making downstream tasks, our model combines
the both augmentation strategies.
E. Experiment Details
E.1. Pre-training Visualization
In the main paper, we evaluate the effectiveness of our pre-
training by evaluating U NIFIED -IO 2 quantitively on a va-
riety of benchmarks. Here, we qualitatively show the vi-
32

--- PAGE 33 ---
Categorization Localization VQA Refexp Segmentation Keypoint Normal All
ablation test ablation test ablation test ablation test ablation test ablation test ablation test ablation test
0NLL-AngMF [11] - - - - - - - - - - - - 49.6 50.5 7.2 7.1
1Mask R-CNN [70] - - 44.7 45.1 - - - - 26.2 26.2 70.8 70.6 - - 20.2 20.3
2GPV-1 [65] 33.2 33.2 42.8 42.7 50.6 49.8 25.8 26.8 - - - - - - 21.8 21.8
3CLIP [146] 48.1 - - - - - - - - - - - - - 6.9 -
4OFA LARGE [186] 22.6 - - - 72.4 - 61.7 - - - - - - - 22.4 -
5GPV-2 [89] 54.7 55.1 53.6 53.6 63.5 63.2 51.5 52.1 - - - - - - 31.9 32.0
5DINO + SAM [94, 139] - - 66.0 66.0 - - - - 60.2 60.1 - - - - 18.0 18.0
6UNIFIED -IOSMALL 42.6 - 50.4 - 52.9 - 51.1 - 40.7 - 46.5 - 33.5 - 45.4 -
7UNIFIED -IOBASE 53.1 - 59.7 - 63.0 - 68.3 - 49.3 - 60.2 - 37.5 - 55.9 -
8UNIFIED -IOLARGE 57.0 - 64.2 - 67.4 - 74.1 - 54.0 - 67.6 - 40.2 - 60.7 -
9UNIFIED -IOXL 61.7 60.8 67.0 67.1 74.5 74.5 78.6 78.9 56.3 56.5 68.1 67.7 45.0 44.3 64.5 64.3
9UIO-2 L 70.1 - 66.1 - 67.6 - 66.6 - 53.8 - 56.8 - 44.5 - 60.8 -
10UIO-2 XL 74.2 - 69.1 - 69.0 - 71.9 - 57.3 - 68.2 - 46.7 - 65.2 -
11UIO-2 XXL 74.9 75.2 70.3 70.2 71.3 71.1 75.5 75.5 58.2 58.8 72.8 73.2 45.2 44.7 66.9 67.0
Table 13. GRIT results and additional baselines from the GRIT leaderboard.
HellaSwag MMLU Arc Easy Arc Cha. BoolQ
UIO-2 L 39.4 28.4 41.8 26.2 66.6
UIO-2 XL 49.9 29.7 49.5 31.3 72.8
UIO-2 XXL 52.7 30.4 55.3 33.5 77.3
Open LLaMA 3B 52.0 23.9 69.3 33.8 67.0
LLaMA 7B 57.1 42.6 76.4 43.5 77.7
LLaMA 7B Chat 57.7 47.6 74.4 44.0 80.7
Table 14. Results on NLP tasks.
sualizations from the pre-trained UIO-2 XXLmodel. Table
12 shows audio generation from text (top) and text + video
(bottom). We can see the pre-trained model learns text-to-
speech synthesis through video pre-training, and the model
can also synthesize music that matches the video input. Fig-
ure 12 shows the future frame prediction samples given the
initial input image and action sequence. Figure 13 shows
the image generation samples given prompts. The model
has a good understanding of different objects. However, it
struggles to generate the correct text from the given caption.
E.2. NLP Results
We present results on a set of NLP tasks to evaluate the
model’s language understanding abilities. We evaluate us-
ing the EleutherAI LM-Eval harness [51], tasks are evalu-
ated zero-shot using the default prompts without any adjust-
ments aside from adding the [Text] [S] prefix used for
all text generation tasks. We evaluate on HellaSwag [214]
and a selection of other question answering benchmarks:
MMLU [72], ARC [31], and BoolQ [30]. Results are shown
in Table 14. Baselines were evaluated in the same setting,
i.e., zero-shot, with the default prompts, and using LM-Eval. U NIFIED -IO 2 is generally ahead of Open LLaMA
3B but behind LLaMA.
E.3. GRIT Details
We present GRIT results in more detail in Table 13. No-
tably, U NIFIED -IO 2 is the first unified model to pass the
Masked R-CNN baseline for localization and goes a long
way toward closing the gap between SAM and unified mod-
els on segmentation.
For GRIT VQA, looking at the scores from GRIT on dif-
ferent VQA subsets, we find that U NIFIED -IO 2 does bet-
ter on the same-source subset (84.6 vs 58.5) but worse on
the new-source subset (57.7 vs 67.2). Same-source ques-
tions come from VQA 2.0, and new-source questions come
from VG, so the difference can be attributed to the kinds
of questions being asked. Qualitatively, it is hard to under-
stand why the scores differ on these subsets since the GRIT
ablation questions lack ground truth annotations. How-
ever, we notice the models often produce different answers
when faced with ambiguous questions ( e.g. “What color is
black on the horse”, “hair” for U NIFIED -IOvs. “mane” for
UNIFIED -IO 2), so one possibility is that U NIFIED -IO 2
does not match the VG answer style as well as U NIFIED -IO,
which would likely be due to differences in the kind of VQA
training data the models were trained on.
For GRIT localization, we find the model can struggle
with images with many instances of the target class, par-
ticularly when using beam search. We hypothesize that
this is because the probability mass can get split between
many similar location tokens, resulting in EOS becoming
the most probable token even if its probability is low. As a
solution, during inference, we only output EOS if the EOS
33

--- PAGE 34 ---
Splits Metrics UIO-2 XXL UIO-2 XLUIO-2 L[206] [207] [26]
RandomAccuracy ( ↑) 90.90 88.27 84.03 88.28 90.24 86.90
Precision ( ↑) 94.30 97.44 77.73 94.34 97.72 94.40
Recall ( ↑) 87.07 78.60 95.40 82.20 83.00 79.27
F1-Score ( ↑) 90.54 87.01 85.66 87.85 89.76 86.19
% Yes 46.17 40.33 61.37 44.91 43.78 43.26
PopularAccuracy ( ↑) 88.17 87.47 77.27 86.20 84.90 83.97
Precision ( ↑) 89.13 95.69 70.03 89.46 88.24 87.55
Recall ( ↑) 86.93 78.47 95.33 82.06 80.53 79.20
F1-Score ( ↑) 88.02 86.23 80.75 85.60 84.21 83.16
% Yes 48.77 41.00 68.07 45.86 45.63 45.23
AdversarialAccuracy ( ↑) 84.17 85.77 72.00 84.12 82.36 83.10
Precision ( ↑) 82.17 92.01 65.00 85.54 83.60 85.60
Recall ( ↑) 87.27 78.33 95.33 82.13 80.53 79.60
F1-Score ( ↑) 84.64 84.62 77.30 83.80 82.00 82.49
% Yes 53.10 42.57 73.30 48.00 48.18 46.50
Table 15. Object hallucination benchmark POPE results, in com-
parison with mPLUG-Owl2 [206], Ferret [207], and Shikra [26].
Img Video All
InstructBLIP [34] 58.8 38.1 53.4
VideoChat-7B [110] 39.0 33.9 37.6
Otter-7B [108] 42.9 30.6 39.7
Qwen-VL-7B [12] 62.3 39.1 56.3
Qwen-VL-chat-7B [12] 65.4 37.8 58.2
mPLUG-Owl2-7B [206] 64.1 39.8 57.8
LLaV A-1.5-7B [118] - - 58.6
LLaV A-1.5-13B [118] 68.2 42.7 61.6
UNIFIED -IO 2 L 56.0 37.5 51.1
UNIFIED -IO 2 XL 64.1 45.6 60.2
UNIFIED -IO 2 XXL 65.7 46.8 61.8
Table 16. Results on SEED-Bench [106]. Our XXL model out-
performs all 7B vision language models and is even slightly better
than the LLaV A-1.5 13B model.
token itself has a probability of over 0.5, which we find sig-
nificantly improves the performance on crowded images.
In rare cases, we observe this leads to the model generat-
ing bounding boxes for the same instance multiple times.
As a solution, we apply Non-maximum suppression with a
higher threshold of 0.8 to remove these duplicates. We ap-
ply this inference trick for localization and when doing the
initial localization step for the keypoint and segmentation
tasks.
E.4. Multimodal Benchmark Details
We now provide the breakdown results for the evaluation-
only multimodal benchmarks, POPE [113] and SEED-
Bench [106]. POPE is the object hallucination benchmark,
requiring ‘yes’ or ‘no’ answers. As shown in Table 15, our
largest model achieves the highest F1 score in all 3 dimen-
sions. Interestingly, smaller models favored ‘no’ responses,
possibly due to a bias from negative examples encountered
during the instruction tuning phase. SEED-Bench offers
19k multiple-choice questions with human annotations for
evaluating multimodal models across 12 dimensions, in-
cluding spatial (Image) and temporal (Video) understand-
GroundtruthUIO-2XXLWithout CLF13.39UIO-2XXLWith CLF33.77A pizza with pineapple andmeat topping on a metal pizza pan.
An elephant standing inthe dry grass at the edge of water.
The big grey bear is staring at something
A stack of pan cakes sitting ontop of a plate.
A picture of a vacant home's kitchen andliving room.
Figure 14. Samples generated by UIO-2 XXLfor the MS COCO
captions [115]. While the classifier-free guidance [74] signifi-
cantly boosts image quality and fidelity, it achieves poor FID.
ing. As shown in Table 16, our XXL model outperforms all
other 7B vision/video language models, and is even slightly
better than the LLaV A-1.5 13B model. Notably, our XL
(3B) model has already outperformed all other counterparts
in the temporal understanding split. While recent video lan-
guage models [108, 110, 128] have shown proficiency in
conventional video tasks like video tagging and caption-
ing, their performance in SEED-Bench’s temporal under-
standing is even worse than that of vision language mod-
els, which might be attributed to their limited instruction-
following capabilities.
E.5. Image Generation Details
Figure 15 shows generated images for the TIFA benchmark
captions [76] using several baselines as well as UIO-2 XXL.
We use the official implementation code (Emu [172] and
CoDi [174]) or the images shared in the official GitHub
repository of TIFA5(Stable Diffusion v1.5 [154] and
miniDALL-E [37]) for baselines. All the baselines except
miniDALL-E use the Stable Diffusion decoder trained on
large-scale, high-quality image datasets, generating images
of high fidelity. However, they often generate images that
do not fully follow the input captions while U NIFIED -IO 2
generates faithful images.
For text-to-image generation on MS COCO [115], we
follow the standard convention [226]; we evaluate on a sub-
5https://github.com/Yushi- Hu/tifa/tree/main/
human_annotations
34

--- PAGE 35 ---
Emu65.5CoDi71.6SD-1.578.4miniDALL-E79.4UIO-2XXL81.3
two laptops a mouse cordswires and a monitor
A red motorcycle parkedby paint chipped doors.
A boy wearing a green shirtposing with some fruit.
a photo of blue fire hydrant
a photo of bike and skateboard;skateboard is left to bike
a mountain with a cloudhanging over it
A Mesoamerican pyramidsurrounded by jungle.detailed charcoal sketch.
A photo of an Athenian vasewith a painting of toucans playing tennisin the style of Egyptian hieroglyphics
A Christmas tree withlights and teddy bear
Figure 15. Samples generated for the TIFA bnechmark captions [76]. Some of the images generated by baselines ( e.g., rows 1-2, 6, 9) have
high quality but do not fully follow the input text while U NIFIED -IO 2 generates faithful images.
35

--- PAGE 36 ---
Prompt Model Response
What is the sound of a duck quacking? 
A bell is ringing loudly and quickly. 
Speak out the text “Police protection was better and more effective;” for me. 
Speak: Printing, then, for our purpose, may be considered as the art of making books by means of movable types. 
Read “Many animals of even complex structure which live parasitically within others are wholly devoid of an alimentary cavity.” 
Generate the music based on the description “Slow tempo, bass-and-drums-led reggae song.” 
Based on the given description “Industrial techno sounds, repetitive, hypnotic rhythms”, produce a corresponding piece of music. 
Table 17. Audio generation examples. U NIFIED -IO 2 can generate not only environmental sound (rows 1-2), but also speech (rows 3-5)
and music (rows 6-7). Note that some of the outputs longer than 4.08 seconds have discontinuity in sound, or changes in tone, speed
or melody (rows 4-5, 7). Since our model can output 4.08-second audio at a time, we complete the audio clip by using any previously
generated clips as additional input. Click for audio samples.
set of 30K captions sampled from the validation set.6Fol-
lowing [43], we generate 8 images for each caption and se-
lect the best one using CLIP text-image similarity [146].
Despite classifier-free guidance [74] resulting in generated
images of qualitatively higher quality, the computed FID
score [73] is significantly worse compared to what would
have been achieved without employing it (33.77 vs 13.39);
see Figure 14.
E.6. Audio Generation Details
For text-to-audio generation, we evaluate on the Audio-
Caps [93] test set. Note that we cannot do an apples-to-
apples comparison with other methods because AudioCaps
consists of 10-second audio clips while our model can gen-
erate 4.08-second audio at a time. Instead, we evaluate the
dataset in the following setup: we first sample four 2-second
audio segments, convert them to log-mel-spectrograms with
zero-padding, and generate the following audio with the
prompt “Generate the following sound based on what you
heard and the description: {caption }”. We convert the
model output, that is, a log-mel-scaled spectrogram, into
a waveform using the pretrained HiFi-GAN, and compare
the ground-truth audio and generated audio using compu-
tational metrics including Fr ´echet Audio Distance [92], In-
ception Score [155] and Kullback–Leibler divergence. We
use the same evaluation code as AudioLDM7[117]. We
show the audio generation examples in Table 17 and audio-
visual qualitative examples in Table 18.
E.7. Video and Audio Understanding Details
We consider classification and question-answering tasks as
open-ended answer generation and use the Exact Match
(EM) to measure the performance. We also tried to formu-
late the classification task as multiple-choice answering and
6We use the evaluation code at https : / / github . com /
MinfengZhu/DM-GAN
7https://github.com/haoheliu/audioldm_evalInput Image Prompt Model Response
What is the sound of this instrument? 
What is the sound of this instrument? 
Generate music about this scene. 
Generate music about this scene. 
Locate the bounding boxes of the sound sources in the given image.
Identify the locations of the sound sources in the given image.
Identify the locations of the instruments producing the given sound.
Identify the locations of the instruments producing the given sound.
Table 18. Audio-visual qualitative examples showcasing the abil-
ity of U NIFIED -IO 2 to reason across modalities. U NIFIED -IO 2
can generate the sound of the instrument in the input image (rows
1-2), and generate the music that matches the mood of the input
image (rows 3-4). The last four examples (rows 5-8) show the re-
sults of visual sound localization. Note that U NIFIED -IO 2 can
accurately identify the instruments that make and do not make
sounds (rows 7-8). Click for audio samples.
generate answers by computing the logit for each dataset
label and selecting the one with the highest logit, but the
performance boost was quite marginal. Note that we do
not train our model directly on the Kinetics-400 [90]; we
instead train on Kinetics-710, a mixture of three different
datasets belonging to the Kinetics family, that is, Kinetics-
400, 600, and 700. Our model achieves top-1 accuracy 79.1
36

--- PAGE 37 ---
Guess the next location of the object from its prior locations: <image_history_1> <extra_id_619> <extra_id_936> <extra_id_714> <extra_id_1014> object.Utilizing the object's past locations: <image_history_1> <extra_id_667> <extra_id_932> <extra_id_687> <extra_id_943> object, can you guess its upcoming location?Figure 16. Single object tracking examples from LaSOT [50]. Given the first input image and the corresponding bounding box of the target
object, our model can track the object across video frames.
Describe the video.A man is trying to skip stones on a clear, blue lake.
What is the man doing in the video?The man in the video is dancing and posing in front of the camera, showing off his dance moves and body movements.
What is the robot doing?Move the orange object from the table to the toaster oven.
Figure 17. Video understanding qualitative examples.
(vs. instruction tuning only: 73.8) when further finetun-
ing on Kinetics-400 for 5 epochs, following [111]. For
Kinetics-Sounds, leveraging both audio and visual inputs
largely improves performance (audio-visual: 89.3 vs. video-
only: 87.4 vs. audio-only: 38.2). For captioning tasks, we
use CIDEr [182] as the evaluation metric. Figure 17 shows
the qualitative examples for video understanding tasks.
E.8. Embodiment Details
What is the final frame after completing the task: "Move the silver pot to the lower right of the burner"?
What does the goal image look like given the current robot RGB observation and prompt 'Pick the green rectangle block and put it on the table between the yellow and red block'?Task:Move the cloth from the left to the right side of the burner.Given the start state as image, generate the end state image.
InputPromptPredictionGT
Figure 18. Future state prediction examples on robotic manipula-
tion tasks. Given the input image and instructions, our model can
successfully generate the target state after the prompt instruction.
In VIMA-Bench [87], there are 4 levels of evaluation
protocols: L1 object placement, L2 novel combination, L3L1 L2 L3 L4 Avg.
VIMA [87] 81.5 81.5 78.7 48.6 72.6
VIMA-Gato [152] 57.0 53.9 45.6 13.5 42.5
VIMA-Flamingo [5] 47.4 46.0 40.7 12.1 36.6
VIMA-GPT [19] 46.9 46.9 42.2 12.1 37.0
UNIFIED -IO 2 L 66.9 63.8 57.5 12.6 50.2
UNIFIED -IO 2 XL 70.3 69.8 64.5 13.1 54.2
UNIFIED -IO 2 XXL 71.3 70.4 68.0 15.5 56.3
Table 19. Evaluations on VIMA-Bench [87]
novel object, and L4 novel task. Results and comparisons
are shown in Table 19. The inputs for the autoregressive
transformer model VIMA [87] are object tokens consist-
ing of cropped images and bounding boxes; image patch
tokens encoded by ViT for VIMA-Gato [152]; image patch
tokens encoded by ViT, further downsampled by a perceiver
module for VIMA-Flamingo [5]; and single image token
encoded by ViT for VIMA-GPT [19]. The output of those
baselines is all next-step action prediction. Since our model
has to predict all actions at once only with the initial ob-
servation, the task setting is then more challenging than the
casual policy learning baselines. Nevertheless, our mod-
els still outperform counterparts that input image or im-
age patches for all 4 levels and are only behind the object-
centric method [87]. In Figure 18, we show the future state
prediction examples on robotic manipulation tasks. Given
the input state image and natural language prompt, our
model can successfully synthesize the target image state.
E.9. Other Tasks
Figure 16 shows single object tracking examples from the
LaSOT [50] dataset. Note that U NIFIED -IO 2 does not
use specific class labels for tracking and tracks small mov-
ing objects such as a table tennis paddle well. Figure 19
presents qualitative examples of 3D object detection from
the Objectron dataset [3]. As outlined in our main paper,
UNIFIED -IO 2 exhibits suboptimal performance in bench-
marks for multi-object 3D detection. Additionally, Fig-
ure 20 illustrates examples of image-based 3D view syn-
thesis using the Objaverse dataset [40]. While the model
produces coherent results, it faces challenges in accurately
37

--- PAGE 38 ---
Figure 19. 3D object detection qualitative examples from Objec-
tron [3].
InputPromptPredictionGT
Relative camera transformation is in the format of “theta sin(phi) cos(phi) r”. What is the image after applying the transformation:( <extra_id_608> <extra_id_493> <extra_id_1154> <extra_id_661> )Complete a new image of the image following the implementation of the camera transformation theta sin(phi) cos(phi) r: ( <extra_id_616> <extra_id_1011> <extra_id_309> <extra_id_729> ).
[Complete a new image of the image following the implementation of the camera transformation theta sin(phi) cos(phi) r: ( <extra_id_789> <extra_id_464> <extra_id_258> <extra_id_450> )
Figure 20. Image-based 3D view synthesis examples from Obja-
verse [40].
representing relative camera transformations.
38
