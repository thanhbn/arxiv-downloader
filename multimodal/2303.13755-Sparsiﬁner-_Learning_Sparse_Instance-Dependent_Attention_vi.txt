# Sparsifiner: Học Attention Phụ Thuộc Instance Thưa Thớt
để Tăng Hiệu Quả Vision Transformers

Cong Wei1*Brendan Duke1,3*Ruowei Jiang3Parham Aarabi1,3Graham W. Taylor2,4Florian Shkurti1,4
1University of Toronto2University of Guelph3Modiface, Inc .4Vector Institute

Tóm tắt
Vision Transformers (ViT) đã cho thấy lợi thế cạnh tranh về hiệu suất so với mạng nơ-ron tích chập (CNNs) mặc dù chúng thường đi kèm với chi phí tính toán cao. Để giải quyết vấn đề này, các phương pháp trước đây khám phá các mẫu attention khác nhau bằng cách giới hạn một số lượng cố định các token gần nhau về mặt không gian để tăng tốc các phép toán multi-head self-attention (MHSA) của ViT. Tuy nhiên, các mẫu attention có cấu trúc như vậy giới hạn các kết nối token-to-token theo tính liên quan không gian của chúng, điều này bỏ qua các kết nối ngữ nghĩa đã học từ mask attention đầy đủ. Trong công trình này, chúng tôi đề xuất một phương pháp mới để học các mẫu attention phụ thuộc instance, bằng cách thiết kế một module dự đoán kết nối nhẹ để ước lượng điểm kết nối của từng cặp token. Một cách trực quan, hai token có điểm kết nối cao nếu các đặc trưng được coi là liên quan về mặt không gian hoặc ngữ nghĩa. Vì mỗi token chỉ chú ý đến một số lượng nhỏ các token khác, các mask kết nối nhị phân thường rất thưa thớt về bản chất và do đó cung cấp cơ hội tăng tốc mạng thông qua các phép tính thưa thớt. Được trang bị mẫu attention không có cấu trúc đã học, sparse attention ViT (Sparsifiner) tạo ra sự đánh đổi Pareto-optimal vượt trội giữa FLOPs và độ chính xác top-1 trên ImageNet so với token sparsity. Phương pháp của chúng tôi giảm 48%-69% FLOPs của MHSA trong khi độ giảm chính xác nằm trong 0.4%. Chúng tôi cũng cho thấy rằng việc kết hợp attention và token sparsity giảm FLOPs của ViT hơn 60%.

1. Giới thiệu
Vision Transformers (ViTs) [13] đã nổi lên như một mô hình thống trị cho các tác vụ thị giác cơ bản như phân loại hình ảnh [13], phát hiện đối tượng [3] và phân đoạn ngữ nghĩa [6, 7]. Tuy nhiên, việc mở rộng ViTs đến một số lượng lớn token là thách thức do độ phức tạp tính toán bậc hai của multi-head self-attention (MHSA) [35].

Điều này đặc biệt bất lợi cho các tác vụ thị giác quy mô lớn vì việc tính toán trên đầu vào có độ phân giải cao và chiều cao là mong muốn. Ví dụ, các phương thức đầu vào như khung hình video và đám mây điểm 3D có số lượng lớn token ngay cả đối với các trường hợp sử dụng cơ bản. Cần có các thuật toán mới để tiếp tục mở rộng ViTs đến các tác vụ thị giác lớn hơn, phức tạp hơn.

Các công trình trước đây chủ yếu đã áp dụng hai phương pháp để cải thiện hiệu quả tính toán của ViTs: token pruning và sử dụng các mẫu attention thưa thớt cố định trong MHSA. Các phương pháp token pruning [27] giảm số lượng token theo một tỷ lệ cố định gọi là tỷ lệ giữ lại, nhưng độ chính xác giảm nhanh khi pruning các lớp sớm trong mạng [15, 30, 31]. Ví dụ, việc đưa token pruning vào các lớp nông hơn của EViT [15] gây ra sự giảm đáng kể 3.16% độ chính xác top-1 trên ImageNet [12]. Vấn đề này là do hạn chế của việc pruning toàn bộ một token, điều này tương đương với việc pruning toàn bộ một hàng và cột của ma trận attention cùng một lúc. Một cách để giảm bớt điều này là pruning các kết nối riêng lẻ của ma trận attention thay vì toàn bộ token. Các phương pháp hiện có áp dụng phương pháp pruning kết nối ma trận attention này sử dụng các mẫu attention thưa thớt cố định [8]. Ví dụ, các mẫu attention cục bộ và strided cố định được sử dụng [8, 14], kết hợp với các kết nối được khởi tạo ngẫu nhiên [40]. Tuy nhiên, các mẫu attention cố định như vậy giới hạn khả năng của các kết nối self-attention đến một tập con cố định của token (Hình 1). Bản chất cố định của các mẫu attention này kém hiệu quả hơn so với giao tiếp trực tiếp giữa các token trong full self attention. Ví dụ, Swin transformer [21, 22] có trường tiếp nhận hạn chế ở các lớp nông hơn và cần nhiều lớp để mô hình hóa các phụ thuộc tầm xa. Và BigBird [40] cần kết hợp nhiều mẫu attention cố định để đạt được hiệu suất tốt. Thay vào đó, việc thiết kế các thuật toán attention thưa thớt mô phỏng bản chất phụ thuộc instance của full self attention [35] là mong muốn, từ đó nắm bắt phân phối biến đổi của thông tin ngữ nghĩa trong nội dung hình ảnh đầu vào.

Để giải quyết các thách thức nêu trên, chúng tôi đề xuất một phương pháp gọi là Sparsifiner học tính toán các mẫu kết nối thưa thớt trên attention vừa phụ thuộc instance vừa không có cấu trúc. Bản chất phụ thuộc instance của mẫu attention cho phép mỗi token sử dụng ngân sách attention hạn chế của các phần tử khác không hiệu quả hơn so với các mẫu attention thưa thớt cố định. Ví dụ, trong các head attention chú ý đến nội dung ngữ nghĩa thay vì vị trí [35, 36], các token chứa thông tin ngữ nghĩa tương tự nên được coi là có điểm kết nối cao bất chấp khoảng cách không gian của chúng. Tương tự, các token gần nhau với mối quan hệ ngữ nghĩa không liên quan nên có điểm kết nối thấp hơn bất chấp sự gần gũi về mặt không gian. Hơn nữa, Sparsifiner cải thiện tính linh hoạt của mẫu attention so với token pruning bằng cách pruning các kết nối riêng lẻ, thay vì toàn bộ hàng và cột của ma trận attention. Điều này cho phép Sparsifiner giảm FLOPs trong các lớp sớm của mạng mà không phải chịu sự suy giảm đáng kể về độ chính xác top-1 (§4). Bằng cách pruning các kết nối riêng lẻ phụ thuộc vào nội dung hình ảnh, Sparsifiner tổng quát hóa các phương pháp trước đó để sparsify MHSA trong ViTs, và khi làm như vậy tạo ra một sự đánh đổi thuận lợi giữa độ chính xác và FLOPs.

Các đóng góp của chúng tôi có thể được tóm tắt là:
• Chúng tôi đề xuất một thuật toán hiệu quả mới gọi là Sparsifiner để dự đoán các mẫu attention thưa thớt phụ thuộc instance sử dụng các mẫu kết nối low-rank. Nghiên cứu của chúng tôi về sparsity không có cấu trúc phụ thuộc instance theo hiểu biết tốt nhất của chúng tôi là mới lạ trong bối cảnh của ViTs.

• Chúng tôi cho thấy rằng attention sparsity không có cấu trúc đã học như vậy tạo ra sự đánh đổi Pareto-optimal vượt trội giữa FLOPs và độ chính xác top-1 trên ImageNet so với token sparsity. Hơn nữa, chúng tôi cho thấy rằng Sparsifiner bổ sung cho các phương pháp token sparsity, và hai phương pháp có thể được kết hợp để đạt được sự đánh đổi hiệu suất-độ chính xác vượt trội.

• Chúng tôi đề xuất một phương pháp dựa trên knowledge distillation để huấn luyện Sparsifiner từ ViTs đã được huấn luyện trước sử dụng một số lượng nhỏ epoch huấn luyện.

2. Công trình liên quan
Efficient Attention — Phát triển một cơ chế attention hiệu quả cho mã hóa hình ảnh độ phân giải cao là trọng tâm của công trình này. Các cơ chế attention hiệu quả đã được nghiên cứu rộng rãi trong các tác vụ NLP để mô hình hóa các chuỗi dài. Chúng có thể được phân loại như sau: Các phương pháp Low-rank như Linformer [37] sử dụng phép chiếu low-rank để tuyến tính hóa phép toán multi-head attention. Linformer [37] thay thế scaled dot product bằng linear attention xấp xỉ attention với ma trận low-rank. Kernelization, bao gồm Performer [9], Linear Transformers [18], và Random Feature Attention [24] sử dụng kernel để tránh tính toán ma trận attention một cách rõ ràng. Sparse attention với các mẫu attention cố định [8, 10, 16, 23, 25]. Loại kỹ thuật này sparsify ma trận attention bằng cách giới hạn trường nhìn đến các mẫu được định nghĩa trước như cửa sổ cục bộ và strided. Các phương pháp dựa trên similarity và clustering bao gồm Routing Transformer [29], Reformer [19], và Sinkhorn Transformer [33]. Các mô hình này đo lường mức độ liên quan của token bằng cách sắp xếp hoặc clustering và sau đó gán token vào bucket để attention trong bucket. Các cơ chế neural memory như Set Transformer [20], Compressive Transformer [26], và Longformer [1]. Chúng sử dụng các token toàn cục bổ sung thu thập thông tin tầm xa như bộ nhớ mô hình.

Vision Transformers — Tiến bộ gần đây đã chứng minh rằng các biến thể của Transformers [35] cũng có thể là các lựa chọn thay thế cạnh tranh cho CNNs và đạt được kết quả đầy hứa hẹn trên các tác vụ thị giác khác nhau. Ngoài phân loại hình ảnh, Transformers cũng đã được áp dụng cho các tác vụ thị giác khác nhau, bao gồm phát hiện đối tượng [4, 11, 44, 46], tạo hình ảnh [5, 23], và xử lý video [42, 45]. Vision Transformer (ViT) [13] chia hình ảnh thành các patch nhỏ và coi các patch như các token từ đầu vào. ViT cho thấy hiệu suất tốt hơn các mô hình kiểu CNN với dữ liệu huấn luyện rộng lớn đầy đủ. DeiT [34] kết hợp các kỹ thuật knowledge distillation vào huấn luyện ViT để chúng ta có thể huấn luyện một Transformer cạnh tranh chỉ sử dụng ImageNet-1k [12]. LV-ViT [17] cải thiện thêm hiệu suất của ViT bằng cách giới thiệu một mục tiêu huấn luyện mới được đặt tên là token labelling. Hầu hết các phương pháp này có độ phức tạp bậc hai của self-attention đối với kích thước hình ảnh đầu vào.

Efficient Vision Transformers — Có một xu hướng mô hình hóa các chuỗi dài của các patch hình ảnh ở độ phân giải cao hơn nhiều. Các công trình gần đây như Pyramid Vision Transformer (PVT) [38], Swin-Transformer [22], T2T-ViT [39], và Vision Longformer (ViL) [43] áp dụng các lớp transformer trên các thang độ phân giải khác nhau bằng cách xếp chồng một kim tự tháp ViTs để tạo thành kiến trúc đa thang. Để đạt được độ phức tạp tuyến tính, Swin-Transformer [22] sử dụng shifted local window attention. Vision Longformer [43] điều chỉnh mẫu attention cục bộ với các token bộ nhớ toàn cục từ Longformer [1]. TimeSformer [2] áp dụng nhiều attention, mỗi cái dọc theo một trục duy nhất của video đầu vào. Các phương pháp đó đều tận dụng các mẫu attention cố định, được định nghĩa trước để giảm chi phí bậc hai. Ngược lại, phương pháp của chúng tôi tạo ra các mẫu attention động thưa thớt dựa trên nội dung đầu vào. Một nhóm công trình khác giảm số lượng token bằng pruning [15, 27, 32], hoặc merging token [28, 30, 41]. Công trình gần đây, DynamicViT [27] và EViT [15] nghiên cứu token sparsification không có cấu trúc bằng cách dần loại bỏ token trong inference của ViTs [13]. Tuy nhiên, chi phí attention bậc hai vẫn còn ở các lớp sớm nơi các token đầu vào không thể được sparsify nhiều. Phương pháp của chúng tôi thay vào đó prune kết nối ở mọi lớp, cho phép tiết kiệm độ phức tạp ở các lớp sớm.

3. Phương pháp
Phương pháp được đề xuất của chúng tôi để học các mẫu attention thưa thớt, Sparsifiner, bao gồm một ViT [13] thông thường làm backbone với các module attention thưa thớt ở mỗi lớp. Module attention thưa thớt của chúng tôi bao gồm một bộ dự đoán mask kết nối và một module sparse multi-head self-attention (MHSA). Trong cả huấn luyện và inference, chúng tôi tạo ra một mask kết nối thưa thớt bằng cách hạn chế số lượng kết nối được dự đoán bởi bộ dự đoán mask theo kích thước ngân sách hyperparameter B. Sau đó, một module sparse MHSA được sử dụng để thực hiện attention thưa thớt dựa trên mask kết nối. Module sparse MHSA thực hiện một tính toán hiệu quả sử dụng tích element-wise thưa thớt giữa attention map đầy đủ và mask kết nối thưa thớt để tạo ra attention map tái tạo thưa thớt. Sau đó, một tích attention-value thưa thớt-dày đặc giữa attention map tái tạo thưa thớt và ma trận value tạo ra đầu ra của module sparse MHSA.

Để rõ ràng, trong phần sau chúng tôi mô tả MHSA chỉ cho một attention head duy nhất. Trong thực tế, chúng tôi áp dụng phương pháp được đề xuất cho mỗi attention head trong ViT. Chúng tôi nối các giá trị đầu ra kết quả từ tất cả attention head và đưa chúng vào một lớp tuyến tính để tạo ra đầu vào cho lớp transformer tiếp theo [35].

ViT Architecture và Naïve MHSA — Chúng tôi dựa phương pháp của mình trên kiến trúc mô hình ViT hiện có [13] và triển khai naïve của MHSA [35]. Một ViT đầu tiên tokenize một hình ảnh đầu vào I∈R^(h×w×3) thành một tập hợp n token X∈R^(n×d), mỗi cái có chiều d. Mỗi token bao gồm một patch embedding, được lấy thông qua phép chiếu tuyến tính của các patch hình ảnh không chồng lấp, và một mã hóa vị trí. Chuỗi token kết quả sau đó được đưa vào các module MHSA để tính toán ma trận attention A∈R^(n×n) như tích của ma trận query Q∈R^(n×d) = X^l W_Q và key K∈R^(n×d) = X^l W_K, trong đó các ma trận chiếu đã học W_Q∈R^(d×d) và W_K∈R^(d×d) tính toán query và key như các phép chiếu của đầu vào X^l∈R^(n×d) đến lớp l. Naïve MHSA sau đó tính toán ma trận attention A như softmax của tích ngoài của ma trận query và key như được thể hiện ở phần bên trái của Hình 2.

Connectivity Mask Predictor — Để kích hoạt các mẫu attention phụ thuộc instance và có ý nghĩa trong khi giới hạn số lượng kết nối, chúng tôi huấn luyện một bộ dự đoán mask kết nối và đạt được sparsity bằng cách thresholding. Cụ thể, chúng tôi đầu tiên tính toán xấp xỉ low-rank A_down∈R^(n×n_down) của ma trận attention A

A_down = softmax(Q(W_down K)^T / √d);     (1)

mà chúng tôi sparsify bằng thresholding:

Ã_down_ij = {A_down_ij nếu A_down_ij > τ
           {0 ngược lại.                    (2)

Trong tính toán attention low-rank (Eq. 1), chúng tôi đầu tiên down-project chiều token của ma trận key K đến một chiều thấp hơn n_down sử dụng ma trận chiếu đã học W_down∈R^(n_down×n). Sau đó, một xấp xỉ low-rank của ma trận attention được tính toán từ tích ngoài của ma trận query và key đã down-project. Lưu ý rằng trong sparsification attention low-rank (Eq. 2), với biểu diễn ma trận thưa thớt chúng ta không cần lưu trữ rõ ràng các số không.

Tiếp theo, bộ dự đoán mask kết nối (Eq. 3) thực hiện phép nhân ma trận thưa thớt của ma trận up-projection thưa thớt W_up∈R^(n_down×n) theo sau bởi binarization. Điều này tạo ra mask kết nối thưa thớt đã up-project:

M = 1_{Top-k(Ã_down W_up)}.               (3)

Ở đây, Ã_down W_up biểu thị phép nhân ma trận thưa thớt-thưa thớt, được tính toán một cách hiệu quả. Insight chính của chúng tôi là ma trận attention low-rank sau softmax (Eq. 1) nên thưa thớt một cách tự nhiên. Chúng tôi cho thấy một ví dụ trong Hình 7.

Chúng tôi áp dụng top-k trên ma trận attention thưa thớt đã up-project Ã_down W_up, đây là attention connectivity score map. k được đặt thành kích thước ngân sách B. Chúng tôi loại bỏ các giá trị không và binarize để tạo ra mask kết nối low-rank thưa thớt M∈R^(n×n). Chúng tôi chỉ ra binarization bằng hàm indicator 1[·] trong bộ dự đoán mask kết nối (Eq. 3).

Sparse MHSA — Trong Hình 2, chúng tôi so sánh phương pháp của mình với naïve MHSA [35] và Linformer [37] trong một ví dụ single head. Trong phương pháp của chúng tôi, được hướng dẫn bởi mask kết nối thưa thớt M, chúng tôi chỉ tính toán các phần tử khác không của ma trận attention full-rank thưa thớt Ã. Để đảm bảo hiệu quả tính toán, chúng tôi muốn có cả up-projection thưa thớt và ma trận attention low-rank thưa thớt. Điều này tương đương với việc tái tạo ma trận attention thưa thớt Ã như một kết hợp affine trên một tập hợp các vector cơ sở thưa thớt sử dụng vector hệ số thưa thớt:

Ã_ij = softmax(QK^T/√d)_ij nếu M_ij = 1.  (4)

Một cách khác để công thức hóa ma trận attention full-rank thưa thớt là như một tích element-wise thưa thớt của mask kết nối thưa thớt M với ma trận attention full-rank:

Ã = M ⊙_sparse A.                         (5)

Ở đây, ⊙_sparse là toán tử tích element-wise thưa thớt, bỏ qua các phép nhân với không. Do đó, việc tính toán ma trận attention full-rank thưa thớt Ã (Eq. 4) chỉ tốn nhiều FLOPs bằng số lượng phần tử khác không trong mask kết nối M. Cụ thể, việc tính toán ma trận attention full-rank thưa thớt tốn ít hơn O(n²d) yêu cầu bởi naïve MHSA.

Cuối cùng, Sparsifiner tính toán tích attention-value thưa thớt sử dụng ma trận attention full-rank thưa thớt Ã và ma trận value V:

X^(l+1) = ÃV.                            (6)

Bằng cách tính toán ma trận attention full-rank thưa thớt Ã (Eq. 4) được hướng dẫn bởi mask kết nối thưa thớt, và sau đó tính toán tích attention-value thưa thớt, chúng tôi loại bỏ độ phức tạp O(n²d) yêu cầu bởi phép toán naïve MHSA. Thay vào đó, phép toán sparse MHSA trong Sparsifiner thực hiện số lượng phép toán tỷ lệ với số lượng phần tử khác không trong mask kết nối M.

Objective functions — Việc huấn luyện Sparsifiner bao gồm huấn luyện các module dự đoán kết nối attention và fine-tuning backbone để làm cho nó thích ứng với attention thưa thớt. Chúng tôi áp dụng loss cross-entropy tiêu chuẩn:

L_cls = CrossEntropy(y_pred, y)           (7)

trong đó y_pred là phân phối lớp dự đoán và y là phân phối lớp ground-truth.

Để giảm thiểu ảnh hưởng đến hiệu suất của quá trình sparsification attention, chúng tôi sử dụng mô hình backbone đã được huấn luyện trước làm teacher trong khung knowledge distillation. Đầu tiên, chúng tôi làm cho các token ở lớp cuối gần với những token của mô hình teacher, trong đó x và x_teach là các token sau block cuối cùng của Sparsifiner và mô hình teacher, tương ứng.

L_distill^token = MSE(x, x_teach).        (8)

Thứ hai, chúng tôi giảm thiểu sự khác biệt của dự đoán Sparsifiner và mô hình teacher thông qua KL divergence:

L_distill^cls = KL(y_pred||y_teach).      (9)

Thứ ba, chúng tôi muốn connectivity score map được tạo bởi bộ dự đoán mask kết nối là một xấp xỉ low-rank tốt của teacher attention, có thể được xem như knowledge distillation của attention map. Chúng tôi giảm thiểu khoảng cách Euclidean giữa chúng:

L_distill^attn = MSE(Ã_down W_up, A_teach). (10)

Cuối cùng, để thực thi sparsity của ma trận up-projection, chúng tôi sử dụng regularization L2. Chúng tôi đã thử regularization L1 nhưng thấy rằng L2 cho hội tụ huấn luyện tốt hơn với sparsity đầy đủ trong thực tế.

L_spa = Σ_i (w_up^i)²                     (11)

Mục tiêu huấn luyện đầy đủ kết hợp tất cả các mục tiêu:

L = L_cls + λ_distill^token L_distill^token + λ_distill^cls L_distill^cls + λ_distill^attn L_distill^attn + λ_spa L_spa (12)

Trong đó chúng tôi đặt weight decay là 0.05 trong optimizer thay vì trực tiếp thêm λ_spa L_spa vào mục tiêu.

4. Thí nghiệm và Kết quả
Chi tiết triển khai — Chúng tôi huấn luyện tất cả các mô hình trên dataset ImageNet [12]. Theo mặc định, module dự đoán mask kết nối được kết hợp vào mỗi lớp của DeiT-S [34] và LV-ViT-S [17]. Trong tất cả các thí nghiệm của chúng tôi, chúng tôi đặt chiều giảm n_down thành 32 và τ thành 0.05 đảm bảo tỷ lệ sparsity 87% của hệ số cơ sở. Ngân sách attention B nằm trong khoảng (0, số lượng token]. Ngân sách B được xác định trực tiếp bởi tỷ lệ giữ attention trong (0,1] như trần của tỷ lệ giữ nhân với tổng số token.

Chúng tôi tuân theo hầu hết các kỹ thuật huấn luyện được sử dụng trong DeiT-S và LV-ViT-S. Chúng tôi sử dụng các mô hình ViT đã được huấn luyện trước để khởi tạo các mô hình backbone. Để cải thiện tốc độ hội tụ, chúng tôi đề xuất chiến lược huấn luyện hai giai đoạn. Trong giai đoạn đầu, chúng tôi đóng băng mô hình backbone và huấn luyện module dự đoán mask kết nối chỉ với loss attention distillation và regularization L2. Cụ thể, chúng tôi đặt λ_distill^token = 0.0, λ_distill^cls = 0.0, λ_distill^attn = 1.0 và chúng tôi cũng áp dụng ngưỡng 1e-2 trên cơ sở W_up để đảm bảo sparsity 90%. Chúng tôi thấy rằng thiết lập này giúp bộ dự đoán mask kết nối học W_up nhanh chóng và loss hội tụ trong vòng 5 epoch. Trong giai đoạn thứ hai, chúng tôi huấn luyện chung mô hình backbone và module dự đoán mask kết nối trong 40 epoch khác. chúng tôi đặt λ_distill^token = 0.5, λ_distill^cls = 0.5, λ_distill^attn = 0.0. Thêm chi tiết có thể được tìm thấy trong tài liệu bổ sung.

Sparse connectivities và attention visualization — Để nghiên cứu định tính về chất lượng của xấp xỉ attention thưa thớt của Sparsifiner, chúng tôi visualize mask kết nối và sparse reconstructed attention map của nó (Hình 3). Chúng tôi cho thấy hình ảnh đầu vào gốc và mask kết nối của query patch, trong đó các vùng tối đại diện cho các token không được chú ý bởi query patch token. Đối với mỗi attention head, Sparsifiner tạo ra mask kết nối tương ứng. Chúng tôi thấy rằng mask kết nối hoạt động như một cơ chế đề xuất vùng, cho phép các attention head khác nhau định vị các token thông tin khác nhau và thu thập thông tin ngữ nghĩa đa dạng. Hơn nữa, chúng tôi visualize sparse attention map được tạo ra hiệu quả sử dụng mask kết nối và so sánh nó với full attention map. Chúng tôi thấy rằng sparse attention map giữ lại tất cả các giá trị kết nối cao nhất, trong khi loại bỏ các giá trị kết nối thấp hơn. Do đó các visualization cho thấy rằng Sparsifiner giữ lại các mối quan hệ nổi bật nhất cho một token nhất định, trong khi loại bỏ các mối quan hệ nền nhiễu.

So sánh với token pruning — Chúng tôi huấn luyện và đánh giá Sparsifiner trên ImageNet và so sánh với các baseline token pruning tiên tiến (Bảng 1). Vì câu hỏi nghiên cứu của chúng tôi giải quyết vấn đề giảm độ phức tạp MHSA, chúng tôi báo cáo sự đánh đổi giữa độ chính xác top-1 trên ImageNet và tính toán theo MHSA FLOPs. Chúng tôi so sánh Sparsifiner với baseline bằng cách điều chỉnh hai hyperparameter: tỷ lệ giữ token và attention. Tỷ lệ giữ token là phần của token được giữ trong mạng tại các lớp được xác định trước nơi pruning xảy ra, mà chúng tôi đặt theo các baseline token pruning đã được thiết lập [15, 27]. Tỷ lệ giữ attention là phần của kết nối attention tại bất kỳ lớp MHSA nhất định nào, được xác định bởi bộ dự đoán mask kết nối (Eq. 3). Do đó, việc thay đổi tỷ lệ giữ attention giảm FLOPs mà không cần thiết phải loại bỏ token như trong token pruning. Nhưng cả hai kỹ thuật có thể được kết hợp để đạt được hiệu ứng bổ sung.

Để cung cấp nhiều so sánh khác nhau, chúng tôi thử nghiệm thêm token pruning và Sparsifiner vào hai mô hình ViT baseline phổ biến: DeiT [34] và LV-ViT [17]. Trên cả hai mô hình, Sparsifiner đạt được tiết kiệm tính toán đáng kể trong khi duy trì sự giảm tương đối khiêm tốn trong độ chính xác top-1. Ví dụ, LV-ViT-S [17] được huấn luyện với Sparsifiner với tỷ lệ giữ attention 0.25 giảm MHSA FLOPs 53.5% trong khi duy trì độ chính xác top-1 của mô hình LV-ViT-S baseline trên ImageNet. Khi được sử dụng kết hợp với token pruning, Sparsifiner đạt được sự giảm thậm chí vượt trội trong MHSA FLOPs trong khi duy trì độ chính xác top-1 tương đương với EViT, và độ chính xác top-1 vượt trội so với DynamicViT.

Thay đổi ngân sách attention MHSA — Chúng tôi thay đổi ngân sách attention của MHSA để nghiên cứu sự đánh đổi giữa MHSA FLOPs và độ chính xác top-1 cho Sparsifiner-S (Bảng 2). Các kết quả được đánh giá trên ImageNet cho thấy rằng Sparsifiner-S tạo ra Pareto frontier vượt trội so với các phương pháp trước đây (Hình 4). Cụ thể, các mô hình Sparsifiner-S với ngân sách attention từ 40 trở lên đạt được độ chính xác top-1 trong vòng 0.1% của mô hình DeiT-S [34] full-rank, trong khi sử dụng ít hơn 58.8% FLOPs trong MHSA. Hơn nữa, các mô hình Sparsifiner-S với ngân sách attention cao từ 79 trở lên đạt được độ chính xác top-1 vượt trội so với mô hình DeiT-S [34] full-rank, trong khi sử dụng ít FLOPs hơn trong MHSA. Điều này cho thấy rằng cơ chế tái tạo attention full-rank thưa thớt của Sparsifiner tạo ra hiệu ứng regularization hữu ích cải thiện khả năng tổng quát hóa của mô hình.

Tăng tốc ViT trên hình ảnh độ phân giải cao — Để cho thấy hiệu quả của phương pháp của chúng tôi trên kích thước đầu vào lớn hơn, chúng tôi áp dụng phương pháp của mình vào DeiT-T [34] với độ phân giải 384×384 (Bảng 3). Khi xử lý hình ảnh độ phân giải cao, do độ phức tạp bậc hai trong số lượng token, MHSA trở nên ngày càng đắt đỏ so với các phép toán feedforward. Chúng tôi giảm độ phức tạp MHSA của mô hình DeiT-T [34] với đầu vào 384×384 hơn 80% với sự giảm độ chính xác ít hơn 1%. Phương pháp của chúng tôi cho thấy tiềm năng tuyệt vời để tăng tốc ViT trên hình ảnh độ phân giải thậm chí cao hơn nơi số lượng token thống trị độ phức tạp mô hình.

Low-rank: connectivities hay attention? — Phương pháp của chúng tôi đặt ra một câu hỏi nghiên cứu: liệu tiện ích của ma trận attention low-rank dày đặc có đến từ việc sử dụng nó như một mask kết nối? Hay đủ để sử dụng ma trận attention low-rank dày đặc trực tiếp, bỏ qua nhu cầu tái tạo ma trận attention full-rank thưa thớt, tức là phương pháp Linformer [37]? Chúng tôi trả lời câu hỏi này bằng cách so sánh độ chính xác top-1 của hai phương pháp (Bảng 4). Trong thí nghiệm này, Sparsifiner-S và Linformer [37] được huấn luyện dưới thiết lập giống hệt nhau, chỉ khác ở phương pháp xấp xỉ attention. Sparsifiner-S sử dụng ma trận attention full-rank thưa thớt tái tạo, trong khi Linformer sử dụng ma trận attention low-rank dày đặc trực tiếp. Để cung cấp cho cả hai mô hình khả năng biểu diễn tương tự, chúng tôi đặt chiều low-rank của Linformer [37] bằng ngân sách attention thưa thớt của Sparsifiner-S. Điều này thực thi rằng tích attention-value của MHSA của cả hai mô hình có cùng độ phức tạp.

Việc sử dụng ma trận attention full-rank thưa thớt tạo ra cải thiện 2.1% điểm phần trăm tuyệt đối trong độ chính xác top-1 so với Linformer. Sự cải thiện này củng cố tính ưu việt của việc sử dụng tích query-key low-rank như một mask kết nối, thay vì sử dụng ma trận attention low-rank trực tiếp. Việc sử dụng ma trận attention low-rank để tính toán tích attention-value trực tiếp với value đã down-project loại bỏ đuôi dài của eigenspectrum của ma trận attention đầy đủ [37]. Ngược lại, việc sử dụng tích query-key low-rank như một mask kết nối giảm tính toán bằng một cơ chế khác. Bằng cách sử dụng mask kết nối low-rank để tạo ra ma trận attention full-rank thưa thớt, đuôi dài của eigenspectrum của ma trận attention đầy đủ được bảo tồn. Dựa trên sự cải thiện đáng kể trong độ chính xác top-1, chúng tôi kết luận rằng các eigenvalue đuôi dài này quan trọng cho chất lượng dự đoán mô hình trong ViTs.

Low- và full-rank attention visualization — Để làm sáng tỏ thêm sự khác biệt định tính giữa attention low- và full-rank trong ViTs, chúng tôi cũng trình bày masked attention heatmap và full attention heatmap của query patch (Hình 5). Chúng tôi cho thấy rằng mask kết nối có thể bảo tồn chính xác các token chính có liên quan cao đến query patch và loại bỏ những cái không liên quan. Kết quả là, masked attention heatmap bảo tồn cấu trúc và loại bỏ nhiễu so với full attention heatmap. Các kết quả visualization cũng xác nhận rằng Sparsifiner của chúng tôi có thể xấp xỉ hiệu quả full attention ViT.

Sparse low-rank basis và up-projection matrix visualization — Để chứng minh rằng mask kết nối có thể được tính toán bằng phép nhân ma trận thưa thớt-thưa thớt, chúng tôi visualize ma trận up-projection W_up của sáu lớp đầu tiên của Sparsifiner (Hình 6). Vì ma trận attention thưa thớt tái tạo là kết hợp của các weight của ma trận up-projection, chúng tôi gọi nó là cơ sở thưa thớt. Chúng tôi cho thấy rằng Sparsifiner tự nhiên học một cơ sở thưa thớt của các vùng cục bộ giống như Gaussian 2D. Đối với một token nhất định, các cơ sở thưa thớt tương ứng với các vị trí đối tượng có thông tin ngữ nghĩa và/hoặc không gian nổi bật sẽ kích hoạt. Vì sparse attention reconstruction (Eq. 5) là tích của ma trận attention low-rank thưa thớt với ma trận up-projection, chúng tôi cũng visualize ma trận attention low-rank sau softmax. Ở đây chúng tôi xem ma trận attention low-rank như hệ số thưa thớt của cơ sở thưa thớt (Hình 7). Định tính, hệ số thưa thớt cũng thể hiện mức độ sparsity cao, xác nhận thêm hiệu quả của sparse attention reconstruction thông qua phép nhân ma trận thưa thớt-thưa thớt.

5. Kết luận
Chúng tôi đã trình bày một phương pháp hiệu quả tính toán mới để học attention không có cấu trúc, phụ thuộc instance trong ViTs. Việc phát triển các cơ chế attention thưa thớt như Sparsifiner mở ra cánh cửa cho nghiên cứu thêm về việc tăng tốc ViTs thưa thớt sử dụng các phương pháp hệ thống phần mềm-phần cứng. Sparsifiner cho thấy triển vọng của attention thưa thớt để mở rộng ViTs đến các tác vụ thị giác lớn hơn và phức tạp hơn. Nhưng các phương pháp hệ thống phần mềm-phần cứng là cần thiết để nhận ra tiềm năng đầy đủ của nó. Chúng tôi hy vọng rằng công trình của chúng tôi truyền cảm hứng cho nghiên cứu thêm tại giao điểm của các thuật toán thưa thớt cho ViTs và các phương pháp hệ thống phần mềm-phần cứng để hỗ trợ các thuật toán thưa thớt đó.
