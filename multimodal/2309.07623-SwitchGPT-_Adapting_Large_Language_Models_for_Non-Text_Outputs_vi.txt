# 2309.07623.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.07623.pdf
# Kích thước tệp: 10159249 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SwitchGPT: Điều chỉnh Mô hình Ngôn ngữ Lớn cho Đầu ra Không phải Văn bản
Xinyu Wang1, Bohan Zhuang2, Qi Wu1*
1Đại học Adelaide, Úc
2Đại học Monash, Úc
Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLMs), chủ yếu được huấn luyện trên các tập dữ liệu dựa trên văn bản, thể hiện khả năng xuất sắc trong việc hiểu và thực hiện các hướng dẫn ngôn ngữ phức tạp thông qua đầu ra văn bản. Tuy nhiên, chúng gặp khó khăn khi được yêu cầu tạo ra đầu ra không phải văn bản. Đồng thời, các mô hình chuyển đổi phương thức, chẳng hạn như văn bản-sang-hình ảnh, mặc dù tạo ra hình ảnh chất lượng cao, nhưng lại thiếu việc tiền huấn luyện văn bản rộng rãi. Kết quả là, những mô hình này chỉ có thể xử lý các mô tả hình ảnh cụ thể thay vì hiểu các hướng dẫn phức tạp hơn. Để khắc phục khoảng cách này, chúng tôi đề xuất một cách tiếp cận mới, SwitchGPT, từ góc độ chuyển đổi phương thức nhằm phát triển một LLM dựa trên văn bản thành một mô hình đa phương thức. Chúng tôi đặc biệt sử dụng một tập dữ liệu tối thiểu để hướng dẫn LLMs nhận biết phương thức đầu ra dự định như được chỉ đạo bởi các hướng dẫn. Do đó, LLM được điều chỉnh có thể hiệu quả gọi các mô hình chuyển đổi phương thức khác nhau có sẵn từ kho mô hình để tạo ra các phản hồi không phải văn bản. Điều này tránh được sự cần thiết của việc tiền huấn luyện phức tạp thường đòi hỏi lượng lớn dữ liệu đa phương thức được ghép đôi, đồng thời kế thừa kiến thức rộng rãi của LLMs và khả năng của các mô hình tạo sinh chất lượng cao. Để đánh giá và so sánh LLM đa phương thức được điều chỉnh với các đối tác truyền thống, chúng tôi đã xây dựng một điểm chuẩn hướng dẫn đa phương thức yêu cầu đầu ra phương thức đa dạng. Kết quả thử nghiệm cho thấy rằng, với huấn luyện tối thiểu, LLMs có thể được điều chỉnh một cách thuận tiện để hiểu các yêu cầu phản hồi không phải văn bản, từ đó đạt được tính linh hoạt cao hơn trong các tình huống đa phương thức. Mã nguồn và dữ liệu sẽ được cung cấp tại https://github.com/xinke-wang/SwitchGPT.

Giới thiệu
Sự xuất hiện của các Mô hình Ngôn ngữ Lớn (LLMs) (Brown et al. 2020; Zhang et al. 2022; Touvron et al. 2023a; Chowdhery et al. 2022) có khả năng hiểu và thực hiện các tác vụ ngôn ngữ phức tạp là một tiến bộ đáng chú ý trong những năm gần đây. Sức mạnh của chúng trong việc hiểu, xử lý và tạo ra các phản hồi dựa trên văn bản đã mở đường cho các ứng dụng đột phá. Điều này bao gồm các lĩnh vực như hiểu ngôn ngữ tự nhiên (Wang et al. 2019), hệ thống trả lời câu hỏi tự động (Lin, Hilton, và Evans 2022), và trợ lý AI đối thoại (OpenAI 2023). Chủ yếu, các mô hình này được huấn luyện trên các tập dữ liệu dựa trên văn bản rộng rãi từ cộng đồng. Những tập dữ liệu như vậy bao gồm *Liên hệ: Prof. Qi Wu (qi.wu01@adelaide.edu.au)

Tháp nổi tiếng nơi Galileo Galilei tiến hành thí nghiệm rơi tự do là Tháp nghiêng Pisa, nằm ở Pisa, Ý. Thí nghiệm bao gồm việc thả các vật thể có khối lượng khác nhau từ tháp để chứng minh rằng tất cả chúng sẽ chạm đất cùng một lúc, bác bỏ niềm tin phổ biến vào thời đó rằng các vật thể nặng hơn rơi nhanh hơn các vật thể nhẹ hơn. Độ nghiêng độc đáo của tháp đã khiến nó trở thành một địa danh biểu tượng và điểm đến phổ biến cho khách du lịch từ khắp nơi trên thế giới.

Hãy cho tôi xem tháp nổi tiếng nơi Galileo tiến hành thí nghiệm rơi tự do.

Stable-Diffusion  ChatGPT3.5  SwitchGPT

ChatGPT
Con người Chuyên gia
Tập dữ liệu Phương thức
Hướng dẫn Mồi
Văn bản→Hình ảnh
Văn bản→Âm thanh
…
Tập dữ liệu Hướng dẫn

Hướng dẫn được Tạo: Bạn có thể tạo một hình ảnh đại diện cho khái niệm "du hành thời gian", sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?
Phản hồi: [Hình ảnh] Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.
Chú thích: Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.

Tạo Hướng dẫn Nhận biết Phương thức Được Căn chỉnh
Quy trình Điều chỉnh/Suy luận

Tập dữ liệu Hướng dẫn
Bộ Điều chỉnh
Chuyển đổi Phương thức
Mô hình Văn bản→Hình ảnh
Mô hình Văn bản→Âm thanh
…

Ví dụ Quy trình Tạo  Điều chỉnh Hướng dẫn  Suy luận
Đầu vào
Phản hồi Có Cấu trúc
Văn bản
Âm thanh
Hình ảnh
Mất mát
Chọn Tuyến Chuyển đổi
Đầu ra
LLM

Tôi dự định du lịch đến New York tuần tới. Tôi nghe nói có một tượng điêu khắc nổi tiếng là món quà từ Pháp. Nó có tên gì?

Tượng điêu khắc nổi tiếng mà bạn đang đề cập đến là Tượng Nữ thần Tự do.

Tuyệt vời! Bạn có thể cho tôi xem một bức ảnh của Tượng Nữ thần Tự do không?

Tuyệt đẹp! Bây giờ, xin hãy dùng giọng nói của bạn để giới thiệu cho tôi lịch sử của Tượng Nữ thần Tự do.

Tượng Nữ thần Tự do được thiết kế bởi nhà điêu khắc Pháp Fredric August Barthodi. Nó đứng trên đảo Tự do trong Cảng New York và đã trở thành biểu tượng của các giá trị Mỹ.

Nội dung Âm thanh

Của chúng tôi

Lời nhắc: Một phong cảnh siêu thực và tưởng tượng với những sinh vật và vật thể kỳ lạ.

Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về "wonderland" bên trong cuốn sách. Bạn có thể cho tôi thấy nó trông như thế nào không?

…Hướng dẫn Hugging-GPT
Lời nhắc: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Đầu vào: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Thất bại trong phân tích.

LLM-Grounded Diffusion
Bố cục Được Tạo
Sách Nền
Các vật thể cần bao gồm
Sách [140, 250, 211, 311]
Stable-Diffusion

Tất nhiên! "Alice in Wonderland" của Lewis Carroll là một câu chuyện kỳ quái và tưởng tượng diễn ra trong một thế giới đầy những yếu tố siêu thực và giàu trí tưởng tượng. Wonderland là một nơi mà logic và thực tế thường bị đảo ngược, và những sự kiện kỳ lạ là điều bình thường.

ChatGPT

Kế hoạch
Văn bản→Hình ảnh
Văn bản→Âm thanh
???
Đầu ra Mong đợi?

Vui lòng sử dụng hình ảnh để trả lời câu đố này (Gợi ý: đó là một loại động vật): Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì?

Một con Đà điểu
[85, 32, 342, 480]
Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì.
Một bức ảnh hươu cao cổ.

Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc.

Một địa danh biểu tượng ở Tây An
[12, 45, 488, 421]
Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc
Đội quân Đất nung ở Tây An, Trung Quốc.

Hình 1: Với một hướng dẫn mong đợi phản hồi không phải văn bản, các LLM dựa trên văn bản như ChatGPT (OpenAI 2023) bị hạn chế trong việc cung cấp phản hồi văn bản, trong khi các mô hình văn bản-sang-hình ảnh phổ biến như Stable Diffusion (Rombach et al. 2022) tạo ra hình ảnh dựa trên mô tả trực tiếp. Ngược lại, SwitchGPT được đề xuất của chúng tôi hiểu một cách toàn diện ý định cơ bản của hướng dẫn, tạo ra phản hồi phù hợp hơn một cách chính xác.

vô số chủ đề và ngôn ngữ, nắm bắt phạm vi rộng lớn của kiến thức nhân loại (Raffel et al. 2020; Radford et al. 2019). Tuy nhiên, do việc huấn luyện tập trung vào văn bản, các LLM truyền thống chủ yếu hoạt động với đầu vào và đầu ra văn bản, dẫn đến phản hồi không thỏa đáng khi được giao nhiệm vụ tạo ra đầu ra không phải văn bản (xem Hình 1). Mặc dù không được tiếp xúc trực tiếp với dữ liệu không phải văn bản như hình ảnh và âm thanh trong quá trình huấn luyện, nghiên cứu gần đây (Li et al. 2023a; Wang et al. 2023; Li et al. 2023c; Su et al. 2023; Zhu et al. 2023) đã chỉ ra rằng LLMs có tiềm năng sâu sắc để hiểu dữ liệu không phải văn bản như vậy. Khám phá này mở ra cánh cửa cho việc phát triển các LLM dựa trên văn bản thuần túy thành một mô hình đa phương thức. Ví dụ, Mini-GPT4 (Zhu et al. 2023) huấn luyện một lớp tuyến tính kết nối BLIP-2 (Li et al. 2023a) với Vicuna (Chiang et al. 2023), chứng minh khả năng mà LLMs có thể hiểu đầu vào hình ảnh. Tuy nhiên, các nghiên cứu về việc cho phép LLMs tạo ra đầu ra không phải văn bản vẫn còn tương đối hạn chế, hạn chế khả năng tương tác của LLM trong các tình huống đa phương thức.

Khác với các LLM truyền thống, được thiết kế chủ yếu cho tương tác đơn phương thức, cụ thể là giao tiếp văn bản-sang-văn bản, các mô hình chuyển đổi phương thức thành thạo trong việc xử lý dữ liệu qua các phương thức khác nhau. Ví dụ, chú thích hình ảnh

--- TRANG 2 ---
(Vinyals et al. 2015; Hossain et al. 2019) minh họa tuyến hình ảnh →văn bản, trong khi tạo hình ảnh có điều kiện văn bản (Goodfellow et al. 2014; Ramesh et al. 2021) minh họa việc chuyển đổi từ văn bản →hình ảnh. Các mô hình này báo hiệu một tiến bộ đáng kể trong việc tạo ra các mẫu thực tế qua các loại dữ liệu đa dạng. Tuy nhiên, việc huấn luyện chúng chủ yếu dựa trên dữ liệu được ghép đôi, như các cặp hình ảnh-văn bản. Khối lượng dữ liệu được ghép đôi có sẵn nhỏ hơn đáng kể so với dữ liệu phương thức đơn, với các so sánh thường trong phạm vi hàng trăm tỷ token (văn bản thuần túy) so với chỉ hàng trăm triệu cặp (hình ảnh-văn bản). Kết quả là, các mô hình chuyển đổi phương thức thường thiếu sự phong phú của kiến thức và độ sâu hiểu biết được thể hiện bởi LLMs. Hạn chế này cũng có nghĩa là chúng thường gặp khó khăn trong việc hiểu và thực hiện các hướng dẫn phức tạp hơn dựa trên kiến thức đã có và thường thức. (xem Hình 1).

Với những điểm mạnh và hạn chế nêu trên của cả LLMs và mô hình chuyển đổi phương thức, một đề xuất hấp dẫn xuất hiện: Liệu chúng ta có thể kết hợp kiến thức sâu sắc và khả năng hiểu biết của LLMs với các mô hình chuyển đổi phương thức không? Do đó, các mô hình tích hợp có thể diễn giải các hướng dẫn phức tạp hơn và cung cấp đầu ra ở nhiều phương thức khác nhau. Một ý tưởng tự nhiên nổi lên từ tình huống khó khăn này là đặt LLM như một điều phối viên để tổ chức và sử dụng các mô hình chuyển đổi phương thức. Một số công trình gần đây (Schick et al. 2023; Shen et al. 2023; Lian et al. 2023) đã chứng minh tiềm năng to lớn của việc tận dụng LLMs như một bộ điều khiển, thể hiện khả năng tổ chức vượt trội của chúng trong việc lập kế hoạch và thực hiện các hướng dẫn chi tiết với các công cụ bên ngoài. Ví dụ, HuggingGPT (Shen et al. 2023) đã thiết kế một quy trình làm việc mà ChatGPT được sử dụng như một bộ điều khiển để gọi các mô hình mã nguồn mở của HuggingFace, từ đó hoàn thành các tác vụ AI tinh vi. Tuy nhiên, nó gặp một vài khiếm khuyết. Không hiệu quả: HuggingGPT hoạt động như một mô hình trực tuyến, phụ thuộc nhiều vào API ChatGPT của OpenAI. Sự phụ thuộc này đòi hỏi việc gọi ChatGPT thường xuyên cho các chức năng như Lập kế hoạch Tác vụ và Lựa chọn Mô hình, làm tăng đáng kể cả chi phí vận hành và độ trễ. Không ổn định: HuggingGPT sử dụng ChatGPT như một hộp đen mà không có bất kỳ điều chỉnh nào, và đầu ra của LLM không được kiểm soát và không phải lúc nào cũng trả về kết quả mong muốn, dẫn đến các ngoại lệ trong quy trình làm việc. Một ví dụ khác là LLM-Grounded Diffusion (Lian et al. 2023), khai thác khả năng lập luận của LLMs để tạo ra bố cục cảnh dưới dạng hộp giới hạn dựa trên các hướng dẫn đầu vào. Kết quả là, nó cung cấp kiểm soát tỉ mỉ hơn về vị trí và bố cục của các vật thể với hình ảnh được tạo ra. Tuy nhiên, sự không linh hoạt của các phương pháp như vậy hạn chế chúng vào một con đường chuyển đổi phương thức đơn lẻ, và chúng thường gặp khó khăn khi diễn giải các yêu cầu gián tiếp. Những khiếm khuyết trên làm nổi bật các thách thức trong các nỗ lực tích hợp hiện tại và tiềm năng đầy đủ của việc kết hợp LLMs với các mô hình chuyển đổi phương thức.

Trong bài báo này, chúng tôi cố gắng giải quyết các thách thức trên và trình bày một LLM được điều chỉnh hướng dẫn để mở khóa khả năng tạo ra đầu ra không phải văn bản, với sự hỗ trợ của một số mô hình chuyển đổi phương thức. Những đóng góp chính của bài báo này gồm ba phần:

• Chúng tôi trình bày Điều chỉnh Hướng dẫn Căn chỉnh Phương thức giúp LLMs hiệu quả nhận biết phương thức đầu ra dự định như được quy định bởi các hướng dẫn. Do đó, LLMs được trao quyền triệu hồi các mô hình chuyển đổi phương thức phù hợp cho các phản hồi không phải văn bản trong khi vẫn giữ nguyên khả năng lập luận ban đầu của chúng. Với kỹ thuật này, bất kỳ LLM nào cũng có thể được điều chỉnh dễ dàng để tạo ra đầu ra không phải văn bản với huấn luyện tối thiểu.

• Chúng tôi giới thiệu một bộ đánh giá mới bao gồm hàng nghìn hướng dẫn nhắm vào đầu ra văn bản, hình ảnh và âm thanh, để đánh giá khả năng của LLMs trong việc xử lý các yêu cầu đầu ra đa phương thức, cho phép hiểu biết tốt hơn về các biến thể hiệu suất qua các LLM khác nhau trong các tình huống đa phương thức.

• Chúng tôi tiến hành các thí nghiệm toàn diện để xác thực cách tiếp cận của chúng tôi so với các LLM mã nguồn mở tiên tiến cũng như API ChatGPT của OpenAI trong các tình huống đa phương thức. Kết quả thí nghiệm cho thấy các mô hình của chúng tôi được điều chỉnh với các hướng dẫn căn chỉnh phương thức có thể giữ lại khả năng lập luận ban đầu trong khi tạo ra đầu ra không phải văn bản chính xác và phù hợp một cách nhất quán.

Công trình Liên quan
LLM như Bộ điều khiển. Cộng đồng AI đã chứng kiến một đợt tăng trưởng biến đổi trong những tiến bộ Xử lý Ngôn ngữ Tự nhiên trong những năm gần đây, phần lớn được thúc đẩy bởi sự xuất hiện của LLMs, như GPT (Brown et al. 2020), OPT (Zhang et al. 2022), PaLM (Chowdhery et al. 2022), Bloom (Scao et al. 2022), và LLaMa (Touvron et al. 2023a). Tiềm năng của LLMs mở rộng ra ngoài chức năng trực tiếp của việc tạo văn bản. Với cơ sở kiến thức rộng lớn và khả năng lập luận phức tạp, LLMs gần gũi với sự hiểu biết giống con người. Với những khả năng này, một hướng đi hứa hẹn đã xuất hiện, coi LLM như một bộ điều khiển. Thay vì sử dụng LLMs chỉ như các thực thể độc lập, chúng có thể phục vụ như một điều phối viên giữa các công cụ bên ngoài (Shen et al. 2023; Schick et al. 2023; Li et al. 2023b; Qin et al. 2023) hoặc thậm chí thao tác các hệ thống robot (Driess et al. 2023; Mai et al. 2023). Ví dụ, HuggingGPT (Shen et al. 2023) sử dụng ChatGPT như một bộ điều khiển để quản lý danh sách các mô hình mã nguồn mở trong Hub của HuggingFace để giải quyết các tác vụ AI. Tuy nhiên, các phương pháp này thường coi LLMs như một hộp đen, sử dụng các lời nhắc tùy chỉnh cho các quy tắc gọi cụ thể, dẫn đến đầu ra không ổn định đáng kể. Trái ngược với việc chỉ dựa vào các LLM được huấn luyện trước để tạo ra các lệnh điều khiển, phương pháp của chúng tôi tinh chỉnh LLM để tạo ra các phản hồi có cấu trúc, từ đó đảm bảo kết quả nhất quán và đáng tin cậy hơn.

Tinh chỉnh LLM. Cho rằng LLMs thường có hàng chục đến hàng trăm tỷ tham số và cần huấn luyện trên các tập dữ liệu rộng lớn, các rào cản cho việc huấn luyện và tinh chỉnh chúng được nâng lên đáng kể, cả về tài nguyên tính toán và thu thập dữ liệu. Gần đây, nhiều nỗ lực đã tìm hiểu các phương pháp tinh chỉnh LLM hiệu quả về tài nguyên hơn. Từ góc độ dữ liệu, điều chỉnh hướng dẫn (Wang et al. 2022; Liu et al. 2023; Taori et al. 2023) tận dụng các LLM quy mô lớn, hiệu suất cao như GPT-3.5 hoặc GPT-4 để tạo ra lượng lớn hướng dẫn và phản hồi,

--- TRANG 3 ---
Tháp nổi tiếng nơi Galileo Galilei tiến hành thí nghiệm rơi tự do là Tháp nghiêng Pisa, nằm ở Pisa, Ý. Thí nghiệm bao gồm việc thả các vật thể có khối lượng khác nhau từ tháp để chứng minh rằng tất cả chúng sẽ chạm đất cùng một lúc, bác bỏ niềm tin phổ biến vào thời đó rằng các vật thể nặng hơn rơi nhanh hơn các vật thể nhẹ hơn. Độ nghiêng độc đáo của tháp đã khiến nó trở thành một địa danh biểu tượng và điểm đến phổ biến cho khách du lịch từ khắp nơi trên thế giới.

Hãy cho tôi xem tháp nổi tiếng nơi Galileo tiến hành thí nghiệm rơi tự do.

Stable-Diffusion  ChatGPT3.5  Của chúng tôi

Của chúng tôi

Lời nhắc: Một phong cảnh siêu thực và tưởng tượng với những sinh vật và vật thể kỳ lạ.

Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về "wonderland" bên trong cuốn sách. Bạn có thể cho tôi thấy nó trông như thế nào không?

…Hướng dẫn  Hugging-GPT
Lời nhắc: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Đầu vào: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Thất bại trong phân tích.

LLM-Grounded Diffusion
Bố cục Được Tạo
Sách Nền
Các vật thể cần bao gồm
Sách [140, 250, 211, 311]
Stable-Diffusion

Tất nhiên! "Alice in Wonderland" của Lewis Carroll là một câu chuyện kỳ quái và tưởng tượng diễn ra trong một thế giới đầy những yếu tố siêu thực và giàu trí tưởng tượng. Wonderland là một nơi mà logic và thực tế thường bị đảo ngược, và những sự kiện kỳ lạ là điều bình thường.

ChatGPT

ChatGPT
Con người Chuyên gia
Tập dữ liệu Phương thức
Hướng dẫn Mồi
Văn bản→Hình ảnh
Văn bản→Âm thanh
…
Tập dữ liệu Hướng dẫn

Hướng dẫn được Tạo: Bạn có thể tạo một hình ảnh đại diện cho khái niệm "du hành thời gian", sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?
Phản hồi: [Hình ảnh] Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.
Chú thích: Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.

Tạo Hướng dẫn Nhận biết Phương thức Được Căn chỉnh
Quy trình Điều chỉnh/Suy luận

Tập dữ liệu Hướng dẫn
Bộ Điều chỉnh
Chuyển đổi Phương thức
Mô hình Văn bản→Hình ảnh
Mô hình Văn bản→Âm thanh
…

Ví dụ Quy trình Tạo  Điều chỉnh Hướng dẫn  Suy luận
Đầu vào
Phản hồi Có Cấu trúc
Văn bản
Âm thanh
Hình ảnh
Mất mát
Chọn Tuyến Chuyển đổi
Đầu ra
LLM

Tôi dự định du lịch đến New York tuần tới. Tôi nghe nói có một tượng điêu khắc nổi tiếng là món quà từ Pháp. Nó có tên gì?

Tượng điêu khắc nổi tiếng mà bạn đang đề cập đến là Tượng Nữ thần Tự do.

Tuyệt vời! Bạn có thể cho tôi xem một bức ảnh của Tượng Nữ thần Tự do không?

Tuyệt đẹp! Bây giờ, xin hãy dùng giọng nói của bạn để giới thiệu cho tôi lịch sử của Tượng Nữ thần Tự do.

Tượng Nữ thần Tự do được thiết kế bởi nhà điêu khắc Pháp Fredric August Barthodi. Nó đứng trên đảo Tự do trong Cảng New York và đã trở thành biểu tượng của các giá trị Mỹ.

Nội dung Âm thanh

Kế hoạch
Văn bản→Hình ảnh
Văn bản→Âm thanh
???
Đầu ra Mong đợi?

Hình 2: So sánh phản hồi với hướng dẫn của người dùng bởi các mô hình khác nhau. Trong khi các mô hình văn bản-sang-hình ảnh truyền thống như Stable Diffusion thường tạo ra hình ảnh dựa trên từ khóa bề mặt, chúng có thể bỏ lỡ ý định cơ bản của hướng dẫn. Hugging-GPT có thể tạo ra kết quả không ổn định, ví dụ, nó xuất ra ba phản hồi trong trường hợp này. LLM-Grounded Diffusion, mặc dù thành thạo trong việc kiểm soát bố cục, vẫn không nắm bắt được những sắc thái sâu sắc hơn đằng sau yêu cầu của người dùng, như được chứng minh bởi việc bao gồm không hợp lý một 'cuốn sách' trong hình ảnh của nó. Ngược lại, cách tiếp cận được đề xuất của chúng tôi không chỉ nắm bắt được bản chất thực sự của hướng dẫn mà còn căn chỉnh nó với đầu vào mong muốn cho mô hình chuyển đổi phương thức, dẫn đến một đại diện trực quan trung thực hơn. (Hình tốt nhất khi phóng to xem.)

sau đó được sử dụng để tinh chỉnh các LLM tương đối nhỏ hơn hoặc thậm chí chính những mô hình lớn đó. Từ góc độ tham số mô hình, Tinh chỉnh Hiệu quả Tham số (PEFT) khám phá việc sử dụng lượng tử hóa sau huấn luyện (Dettmers et al. 2022; Frantar et al. 2023) hoặc đóng băng tham số LLM để huấn luyện một bộ điều chỉnh (Hu et al. 2022; Dettmers et al. 2023), với mục tiêu giảm chi phí tính toán. Dựa trên những kỹ thuật này, huấn luyện LLMs chi phí thấp, có thể tùy chỉnh đã trở nên khả thi, ví dụ, Alpaca (Taori et al. 2023) và Vicuna (Chiang et al. 2023) cải thiện đáng kể hiệu suất của Llama (Touvron et al. 2023a) bằng cách tiến hành điều chỉnh hướng dẫn. LLaVA (Liu et al. 2023) trình bày điều chỉnh hướng dẫn trực quan, cho phép LLMs hiểu nội dung hình ảnh bằng cách điều chỉnh trên các hướng dẫn được ghép đôi ngôn ngữ-hình ảnh. Tuy nhiên, những nỗ lực này hoặc tập trung vào việc nâng cao hiệu suất lập luận của LLM hoặc vào khả năng hiểu hình ảnh →văn bản của nó. Ngược lại, cách tiếp cận của chúng tôi tìm hiểu sâu hơn về việc hiểu ý định đằng sau các hướng dẫn đầu vào, cho phép LLM quyết định phương thức đầu ra phù hợp nhất. Khả năng độc đáo này được đạt được bằng cách sử dụng điều chỉnh hướng dẫn căn chỉnh phương thức được đề xuất của chúng tôi, không chỉ giữ lại khả năng lập luận văn bản →văn bản của LLMs mà còn cho phép chúng tạo ra đầu ra không phải văn bản, bao gồm văn bản→hình ảnh và văn bản →âm thanh.

Mô hình Chuyển đổi Phương thức. Các ứng dụng đa phương thức đã là một chủ đề nghiên cứu lâu dài trong cộng đồng AI. Khả năng độc đáo của những mô hình này trong việc hiểu và chuyển đổi giữa các biểu diễn dữ liệu khác nhau tạo điều kiện cho việc chuyển đổi giữa các phương thức khác nhau, như chú thích hình ảnh (Vinyals et al. 2015; Hossain et al. 2019) (hình ảnh →văn bản), tạo hình ảnh có điều kiện văn bản (Goodfellow et al. 2014; Ramesh et al. 2021) (văn bản →hình ảnh), nhận dạng giọng nói (Ao et al. 2022) (âm thanh →văn bản), hoặc thậm chí chuyển đổi nhiều phương thức (văn bản/hình ảnh →văn bản/hình ảnh) như Uni-Diffuser (Bao et al. 2023). Chúng tôi tập trung cụ thể vào chuyển đổi văn bản →hình ảnh và văn bản →âm thanh trong bài báo này. Với một mô tả văn bản, một mô hình văn bản →hình ảnh nhằm mục đích tổng hợp một hình ảnh mô tả chính xác nội dung được miêu tả trong văn bản. Các phương pháp sớm chủ yếu dựa trên GAN (Xu et al. 2018; Goodfellow et al. 2014) và VQ-VAE (Ramesh et al. 2021), trong khi gần đây, do tính ổn định và chất lượng tạo sinh được cải thiện, các cách tiếp cận dựa trên khuếch tán (Rombach et al. 2022) đã trở nên ngày càng phổ biến. Đối với văn bản →âm thanh, mục tiêu là chuyển đổi một đoạn nội dung văn bản thành âm thanh tương ứng. Trong khi các mô hình này có thể chuyển đổi văn bản thành các phương thức khác, chúng gặp khó khăn trong việc hiểu các hướng dẫn phức tạp, khiến việc áp dụng chúng vào các tính năng tương tác tiên tiến, như trợ lý AI, trở nên khó khăn. Để giải quyết những thách thức này, phương pháp của chúng tôi kết hợp LLMs với các mô hình chuyển đổi phương thức, từ đó căn chỉnh khả năng lập luận tiên tiến vốn có trong LLMs với khả năng chuyển đổi của các mô hình đa phương thức.

SwitchGPT
Khái niệm Cơ bản
Tại sao đầu ra đa phương thức quan trọng? Trong khi văn bản vẫn là phương tiện giao tiếp chủ yếu của con người, hình ảnh và âm thanh thường đảm nhận vai trò không thể thiếu trong các ngữ cảnh khác nhau. Ví dụ, một bức ảnh chụp hoàng hôn vàng trên một bãi biển yên tĩnh có thể gợi lên những cảm xúc mà chỉ lời nói có thể thấy khó khăn để diễn tả. Tương tự, những người khiếm thị phụ thuộc nhiều vào các tín hiệu và mô tả âm thanh để hiểu môi trường xung quanh. Do đó, giá trị của các đầu ra không phải văn bản không thể được đánh giá thấp.

Tuy nhiên, vẫn còn ít nghiên cứu về việc điều chỉnh LLMs

--- TRANG 4 ---
Tháp nổi tiếng nơi Galileo Galilei tiến hành thí nghiệm rơi tự do là Tháp nghiêng Pisa, nằm ở Pisa, Ý. Thí nghiệm bao gồm việc thả các vật thể có khối lượng khác nhau từ tháp để chứng minh rằng tất cả chúng sẽ chạm đất cùng một lúc, bác bỏ niềm tin phổ biến vào thời đó rằng các vật thể nặng hơn rơi nhanh hơn các vật thể nhẹ hơn. Độ nghiêng độc đáo của tháp đã khiến nó trở thành một địa danh biểu tượng và điểm đến phổ biến cho khách du lịch từ khắp nơi trên thế giới.

Hãy cho tôi xem tháp nổi tiếng nơi Galileo tiến hành thí nghiệm rơi tự do.

Stable-Diffusion  ChatGPT3.5  Của chúng tôi

Của chúng tôi

Lời nhắc: Một phong cảnh siêu thực và tưởng tượng với những sinh vật và vật thể kỳ lạ.

Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về "wonderland" bên trong cuốn sách. Bạn có thể cho tôi thấy nó trông như thế nào không?

…Hướng dẫn  Hugging-GPT
Lời nhắc: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Đầu vào: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Thất bại trong phân tích.

LLM-Grounded Diffusion
Bố cục Được Tạo
Sách Nền
Các vật thể cần bao gồm
Sách [140, 250, 211, 311]
Stable-Diffusion

Tất nhiên! "Alice in Wonderland" của Lewis Carroll là một câu chuyện kỳ quái và tưởng tượng diễn ra trong một thế giới đầy những yếu tố siêu thực và giàu trí tưởng tượng. Wonderland là một nơi mà logic và thực tế thường bị đảo ngược, và những sự kiện kỳ lạ là điều bình thường.

ChatGPT

ChatGPT
Con người Chuyên gia
Tập dữ liệu Phương thức
Hướng dẫn Mồi
Văn bản→Hình ảnh
Văn bản→Âm thanh
…
Tập dữ liệu Hướng dẫn

Hướng dẫn được Tạo: Bạn có thể tạo một hình ảnh đại diện cho khái niệm "du hành thời gian", sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?
Phản hồi: [Hình ảnh] Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.
Chú thích: Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.

Tạo Hướng dẫn Nhận biết Phương thức Được Căn chỉnh
Quy trình Điều chỉnh/Suy luận

Tập dữ liệu Hướng dẫn
Bộ Điều chỉnh
Chuyển đổi Phương thức
Mô hình Văn bản→Hình ảnh
Mô hình Văn bản→Âm thanh
…

Ví dụ Quy trình Tạo  Điều chỉnh Hướng dẫn  Suy luận
Đầu vào
Phản hồi Có Cấu trúc
Văn bản
Âm thanh
Hình ảnh
Mất mát
Chọn Tuyến Chuyển đổi
Đầu ra
LLM

Tôi dự định du lịch đến New York tuần tới. Tôi nghe nói có một tượng điêu khắc nổi tiếng là món quà từ Pháp. Nó có tên gì?

Tượng điêu khắc nổi tiếng mà bạn đang đề cập đến là Tượng Nữ thần Tự do.

Tuyệt vời! Bạn có thể cho tôi xem một bức ảnh của Tượng Nữ thần Tự do không?

Tuyệt đẹp! Bây giờ, xin hãy dùng giọng nói của bạn để giới thiệu cho tôi lịch sử của Tượng Nữ thần Tự do.

Tượng Nữ thần Tự do được thiết kế bởi nhà điêu khắc Pháp Fredric August Barthodi. Nó đứng trên đảo Tự do trong Cảng New York và đã trở thành biểu tượng của các giá trị Mỹ.

Nội dung Âm thanh

Kế hoạch
Văn bản→Hình ảnh
Văn bản→Âm thanh
???
Đầu ra Mong đợi?

Vui lòng sử dụng hình ảnh để trả lời câu đố này (Gợi ý: đó là một loại động vật): Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì?

Một con Đà điểu
[85, 32, 342, 480]
Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì.
Một bức ảnh hươu cao cổ.

Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc.

Một địa danh biểu tượng ở Tây An
[12, 45, 488, 421]
Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc
Đội quân Đất nung ở Tây An, Trung Quốc.

Hình 3: Trái: Sơ đồ minh họa quy trình tạo hướng dẫn căn chỉnh phương thức. Bằng cách nhúng văn bản từ các tập dữ liệu phương thức vào lời nhắc, phương pháp của chúng tôi đảm bảo sự căn chỉnh giữa các hướng dẫn do ChatGPT tạo ra và các mô hình chuyển đổi phương thức. Phải: Quy trình điều chỉnh hướng dẫn và suy luận. Trong quá trình huấn luyện, các tham số của LLM được đóng băng, và một bộ điều chỉnh được huấn luyện trên tập dữ liệu hướng dẫn căn chỉnh phương thức được tạo ra. Ở giai đoạn suy luận, phản hồi có cấu trúc của LLM được phân tích và sử dụng để chọn tuyến chuyển đổi phương thức thích hợp để tạo ra đầu ra cuối cùng.

cho đầu ra không phải văn bản. Một lý do chính cho việc nghiên cứu hạn chế trong lĩnh vực này là chi phí cấm đoán liên quan đến việc tiền huấn luyện LLM đa phương thức. Một công trình tiên phong Emu (Sun et al. 2023) nhằm mục đích khắc phục khoảng cách này bằng cách giới thiệu một LLM được tiền huấn luyện về thị giác-ngôn ngữ. Emu được tiền huấn luyện trên các tập dữ liệu lớn, bao gồm hàng tỷ cặp hình ảnh-văn bản và hàng triệu cặp video-phụ đề, sử dụng 128 GPU A100. Điều này cho phép Emu không chỉ có khả năng chấp nhận đầu vào hình ảnh/văn bản mà còn tạo ra đầu ra văn bản và hình ảnh. Mặc dù Emu đã đạt được hiệu suất ấn tượng trong các tác vụ khác nhau, khả năng thích ứng của nó có chi phí cao. Ví dụ, việc giới thiệu một phương thức mới như âm thanh cần thu thập dữ liệu được ghép đôi tương ứng và huấn luyện lại hoàn toàn.

Với những phát triển thịnh vượng trong những năm gần đây, cộng đồng AI đã tích lũy vô số mô hình mã nguồn mở cho các tác vụ đa dạng. Việc tích hợp những mô hình này vào LLMs cung cấp một giải pháp khác để điều chỉnh chúng cho đầu ra không phải văn bản. Điều này không chỉ tiết kiệm tài nguyên tính toán và giảm lượng khí thải carbon mà còn cung cấp một quá trình chuyển đổi hiệu quả về chi phí và mượt mà sang các mô hình mới hơn mà không cần huấn luyện lại rộng rãi. Bài báo này thuộc danh mục này. Các phương pháp hiện tại như HuggingGPT (Shen et al. 2023) có thể gọi các mô hình bên ngoài để tạo ra phản hồi không phải văn bản vượt ra ngoài khả năng của các LLM truyền thống như ChatGPT. Tuy nhiên, kết quả của nó không ổn định. Ví dụ, Hình 2 mô tả rằng HuggingGPT có thể tạo ra một phản hồi âm thanh khi mong đợi một đầu ra hình ảnh. Hơn nữa, nó nhập toàn bộ hướng dẫn như một lời nhắc cho Stable Diffusion, dẫn đến kết quả không tối ưu. Các lý do dẫn đến những vấn đề trên có thể được tóm tắt như sau:

• Chỉ dựa vào thiết kế lời nhắc để sử dụng khả năng zero-shot của LLM để gọi các mô hình bên ngoài có thể tạo ra sự mơ hồ, dẫn đến đầu ra không ổn định.
• Sự không căn chỉnh giữa đầu ra LLMs và đầu vào mô hình bên ngoài thường dẫn đến hiệu suất kém.

Tạo Hướng dẫn Căn chỉnh Phương thức
Để giải quyết những vấn đề nêu trên, chúng tôi giới thiệu Điều chỉnh Hướng dẫn Căn chỉnh Phương thức (MaIT). Mục đích chính của MaIT là hai mặt. Đầu tiên, MaIT nhằm mục đích điều chỉnh LLM L một cách rẻ để hiểu và diễn giải phương thức đầu ra mong đợi t từ một hướng dẫn I cho trước. Chính thức, điều này có thể được viết như:

L(I) = (r)^{MaIT}→L'(I) = (r, t), (1)

trong đó L' là LLM được điều chỉnh sau khi điều chỉnh hướng dẫn, và r là phản hồi đầu ra. Ví dụ, xem xét một hướng dẫn đơn giản "Đáp án của phương trình sau 1+1=? là gì". Trong khi L có thể phản hồi '2', L' được mong đợi tạo ra ('2', 'text'), với 'text' là một cờ biểu thị phương thức đầu ra mong muốn. Cờ loại phương thức này, t, thông báo cho LLM về thời điểm gọi mô hình chuyển đổi phương thức nào. Ví dụ, nếu t là 'image', LLM biết sử dụng mô hình chuyển đổi văn bản→hình ảnh thay vì chỉ đưa ra phản hồi r. Tuy nhiên, nhận biết thời điểm và mô hình nào cần gọi không phải là yêu cầu duy nhất. Điều quan trọng không kém là hướng dẫn LLM về cách sử dụng mô hình chuyển đổi phương thức. Không có hướng dẫn này, LLM có thể hiểu sai hướng dẫn hoặc sử dụng một phản hồi không căn chỉnh như đầu vào khi giao tiếp với mô hình chuyển đổi phương thức. Điều này có thể được thấy trong ví dụ thất bại của HuggingGPT được mô tả trong Hình 2, dẫn đến kết quả kém lý tưởng. Vì vậy, một mục tiêu thứ hai của MaIT là đảm bảo đầu ra của LLM căn chỉnh một cách liền mạch với đầu vào của mô hình chuyển đổi phương thức. Điều này được xây dựng trên thực tế rằng đối với một tuyến chuyển đổi phương thức đơn lẻ, như văn bản →hình ảnh, các mô hình khác nhau thường chia sẻ gần như cùng một tập dữ liệu huấn luyện. Do đó, thay vì căn chỉnh LLM với một mô hình cụ thể, việc căn chỉnh nó với chính các mẫu huấn luyện hiệu quả hơn. Nói cách khác, mục tiêu là giảm thiểu khoảng cách phân bố

--- TRANG 5 ---
giữa đầu ra của LLM và các mô tả văn bản trong tập dữ liệu huấn luyện tác vụ chuyển đổi phương thức. Chúng ta có thể viết lại Phương trình 1 để đại diện cho mục đích này như sau:

L(I) = (r)^{MaIT}→ min Δ(D_{r'}, D_{text→t}) L'(I) = (r', t), (2)

trong đó D_{r'} và D_{text→t} tương ứng đại diện cho phân bố của đầu ra r' bởi L' được điều chỉnh, và phân bố mô tả văn bản của tập dữ liệu huấn luyện cho tác vụ text→t.

Để đạt được điều này, chúng tôi trực tiếp sử dụng dữ liệu huấn luyện từ các tác vụ chuyển đổi phương thức, như chú thích của hình ảnh, để xây dựng phản hồi.

Theo Phương trình 2, chúng ta có thể tạo ra các hướng dẫn căn chỉnh phương thức. Cụ thể, đối với loại phương thức t, chúng tôi xem xét ba phương thức phổ biến nhất trong bài báo này, tức là text, image, và speech. Theo cách tiếp cận của các nghiên cứu trước (Taori et al. 2023), chúng tôi khởi tạo quy trình bằng cách thiết kế các hướng dẫn mồi. Mỗi bản ghi gồm ba thành phần: hướng dẫn, phương thức đầu ra dự kiến, và phản hồi. Ví dụ, một bản ghi có thể trông như thế này: {"instruction": "Bạn phát âm tên thương hiệu thức ăn nhanh với logo cung vàng như thế nào?", "response": {"type": "speech", "McDonald's"}}. Đáng chú ý, với sự phong phú của các tập dữ liệu hướng dẫn text-to-text mã nguồn mở có sẵn, chúng tôi tập trung nỗ lực vào việc đưa ra các hướng dẫn mồi cụ thể cho các phương thức text→image và text→speech. Để tạo ra hướng dẫn với số lượng lớn hơn, chúng tôi sử dụng API ChatGPT của OpenAI. Điều này bao gồm việc tích hợp các mô tả từ dữ liệu huấn luyện tác vụ chuyển đổi phương thức vào lời nhắc ChatGPT để tạo ra các hướng dẫn căn chỉnh phương thức. Một ví dụ về quy trình này được minh họa trong Hình 3. Sử dụng chú thích hình ảnh như "Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước", chúng tôi nhắc ChatGPT tạo ra một hướng dẫn phù hợp yêu cầu tạo ra hình ảnh như vậy. Trong trường hợp này, ChatGPT đưa ra một hướng dẫn thú vị "Bạn có thể tạo ra một hình ảnh đại diện cho khái niệm 'du hành thời gian', sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?". Điều này thể hiện khả năng mạnh mẽ của ChatGPT trong việc tạo ra các hướng dẫn đa dạng. Ngoài ra, chú thích gốc, kết hợp với loại phương thức được chỉ định, được sử dụng để xây dựng phản hồi thực tế. Cụ thể, chúng tôi sử dụng LAION-aesthetic (Schuhmann et al. 2022) và LibriTTS (Zen et al. 2019) làm tập dữ liệu phương thức tham chiếu để lấy mẫu chú thích hình ảnh hoặc nội dung âm thanh. Theo cách này, chúng tôi đồng bộ hóa đầu ra của LLM với đầu vào của mô hình chuyển đổi phương thức. Quan trọng, vì chúng tôi chỉ sử dụng các mô tả văn bản của hình ảnh để tạo hướng dẫn, chỉ cần tiến hành điều chỉnh text-to-text trên LLM. Điều này tránh chi phí tính toán tiềm năng từ tinh chỉnh đa phương thức. Chúng tôi sẽ cung cấp thêm chi tiết về các hướng dẫn được tạo ra trong phụ lục.

Huấn luyện và Suy luận
Khi tập dữ liệu hướng dẫn căn chỉnh phương thức đã được tạo ra, chúng tôi tích hợp nó với các hướng dẫn chỉ có văn bản hiện tại. Điều này dẫn đến tập huấn luyện bao gồm ba tuyến hướng dẫn, tức là text→text, text→image, và text→speech. Phù hợp với nghiên cứu trước, chúng tôi duy trì số lượng hướng dẫn tương đương để huấn luyện, tổng cộng khoảng 52k, với mỗi tuyến chiếm khoảng một phần ba tổng số này. Trong khi tiến hành điều chỉnh hướng dẫn, điều cần thiết là bảo tồn khả năng lập luận và tạo sinh ban đầu của LLM. Do đó, đối với Phương trình 2, khi t là text, mong muốn rằng phân bố của phản hồi r và r' vẫn giống nhau nhất có thể:

min Δ(D_{r'}, D_r), t=text
min Δ(D_{r'}, D_{text→t}), t≠text (3)

Tính nhất quán này được đạt được bằng cách kết hợp các hướng dẫn text→text trong giai đoạn điều chỉnh hướng dẫn. Hơn nữa, các tham số của LLM được giữ không đổi. Chỉ có một bộ điều chỉnh Low-Rank (LoRA) (Hu et al. 2022) được tinh chỉnh, không chỉ bảo vệ các trọng số được huấn luyện trước mà còn giảm đáng kể chi phí tính toán.

Như được thể hiện trong Hình 2, một thách thức của phương pháp hiện tại như HuggingGPT (Shen et al. 2023) là định dạng đầu ra không thể dự đoán. Sự biến đổi như vậy có thể dẫn đến sự không nhất quán trong các giai đoạn phân tích sau, thường gây ra các ngoại lệ. Để giải quyết vấn đề này, chúng tôi mã hóa tất cả phản hồi thành định dạng JSON có cấu trúc, đảm bảo rằng LLM tạo ra đầu ra với cấu trúc nhất quán. Do việc tiền huấn luyện rộng rãi, LLM đã quen thuộc với các quy tắc định dạng JSON. Do đó, huấn luyện với một lượng dữ liệu khiêm tốn có thể hướng dẫn nó một cách hiệu quả để tạo ra các đầu ra được định dạng phù hợp. Như được thể hiện trong khối bên phải của Hình 3, dữ liệu hướng dẫn chỉ gồm văn bản, vì vậy LLM không cần tương tác với bất kỳ mô hình chuyển đổi phương thức nào trong giai đoạn huấn luyện. Khi chuyển sang giai đoạn suy luận, LLM xuất ra một phản hồi định dạng JSON có cấu trúc cho hướng dẫn đã cho. Điều này xác định thời điểm chọn mô hình chuyển đổi phương thức nào và cách sử dụng nó. Theo cách này, tất cả các mô hình chuyển đổi phương thức vẫn tĩnh mà không cần bất kỳ tinh chỉnh nào.

Thí nghiệm
Triển khai
Trong việc triển khai quy trình được đề xuất, chúng tôi đã sử dụng Llama-2 (Touvron et al. 2023b) làm LLM nền tảng. Để chuyển đổi phương thức, các mô hình StableDiffusion-v1-5 (Rombach et al. 2022) và SpeechT5 (Ao et al. 2022) đã được sử dụng. Các tham số của LLM được tải bằng Int8 (Dettmers et al. 2022) và sau đó được đóng băng. Với các hướng dẫn căn chỉnh phương thức được tạo ra bởi GPT-3.5-turbo, chúng tôi đã huấn luyện một bộ điều chỉnh LoRA (Hu et al. 2022) trên LLM bị đóng băng. Huấn luyện của chúng tôi được thực hiện trên bốn GPU Nvidia A100 (40GB) (tuy nhiên, huấn luyện trên một GPU duy nhất là có thể), sử dụng kích thước batch trên thiết bị là 4 và các bước tích lũy gradient là 8. Tối ưu hóa mô hình được thực hiện bằng AdamW (Loshchilov và Hutter 2019) trong 3 epoch, với tốc độ học nhất quán 3^{-4}. Toàn bộ quy trình huấn luyện đã được hoàn thành một cách hiệu quả trong khoảng 3 giờ.

Tập Xác thực và Chỉ số
Đánh giá hiệu suất LLM một cách khách quan là rất quan trọng để tương phản các phương pháp khác nhau. Các điểm chuẩn hiện tại, như The

--- TRANG 6 ---
Bảng 1: So sánh hiệu suất giữa SwitchGPT được đề xuất và các mô hình tiên tiến.

Phương pháp | Phương thức | Acc. (%) ↑ | Thị giác | Ngôn ngữ | Âm thanh
--- | --- | --- | --- | --- | ---
 | | | CLIP↑ | FID↓ | QA↑ | BLEU ↑
OPT-2.7B (Zhang et al. 2022) | | 68.4 | 16.6 | 131.3 | 0.53 | 0.07
OPT-6.7B (Zhang et al. 2022) | | 16.2 | 6.4 | 277.3 | 0.66 | 0.00
Llama-7B (Touvron et al. 2023a) | | 66.8 | 17.9 | 88.6 | 0.55 | 0.07
Llama2-7B (Touvron et al. 2023b) | | 76.2 | 18.7 | 86.1 | 0.69 | 0.08
Alpaca-7B (Taori et al. 2023) | | 72.3 | 17.2 | 94.7 | 0.58 | 0.07
Vicuna-7B (Chiang et al. 2023) | | 73.1 | 17.0 | 95.0 | 0.59 | 0.07
GPT-3.5-turbo (OpenAI 2023) | | 88.1 | 21.4 | 84.1 | 0.75 | 0.17
HuggingGPT (Shen et al. 2023) | | 81.7 | 19.9 | 85.2 | 0.62 | 0.10
SwitchGPT-7B (Của chúng tôi) | | 86.9 | 22.6 | 82.4 | 0.67 | 0.16

Bảng 2: So sánh hiệu suất giữa Stable Diffusion v1-5, LLM-grounded Diffusion, và phương pháp của chúng tôi cho các hướng dẫn text→image.

Phương pháp | CLIP | FID
--- | --- | ---
Stable Diffusion v1-5 | 15.2 | 140.3
LLM-grounded Diffusion | 18.9 | 85.9
SwitchGPT-7B (Của chúng tôi) | 22.6 | 82.4

Open LLM Leaderboard (Beeching et al. 2023), chủ yếu được thiết kế để đánh giá khả năng lập luận và tạo sinh text→text. Ngược lại, bài báo này tập trung vào việc cho phép LLM tạo ra đầu ra không phải văn bản. Với sự khác biệt này, các tiêu chuẩn đánh giá hiện tại không phù hợp để đánh giá toàn diện mô hình của chúng tôi. Để giải quyết khoảng cách này, chúng tôi giới thiệu một tập xác thực mới (sẽ được phát hành) được thiết kế đặc biệt để tạo điều kiện cho việc đánh giá các LLM đầu ra đa phương thức. Tập này chứa 2.400 hướng dẫn, mỗi hướng dẫn yêu cầu đầu ra trong một trong ba phương thức: text, image, hoặc speech. Đối với các hướng dẫn text→text, chúng tôi lấy mẫu câu hỏi từ các điểm chuẩn đã được thiết lập, bao gồm điểm chuẩn Truthful-QA (Lin, Hilton, và Evans 2022) và MMLU (Hendrycks et al. 2021), áp dụng một chỉ số lựa chọn đa dạng. Các hướng dẫn text→image được chia thành hai loại: 200 hướng dẫn phức tạp được con người tạo ra, và 600 hướng dẫn được chọn từ các đề xuất do ChatGPT-4 tạo ra sử dụng COCO-caption (Chen et al. 2015) làm tham chiếu. Hiệu suất của LLM trên tác vụ này được đánh giá bằng cả điểm CLIP (Radford et al. 2021) và FID (Heusel et al. 2017). Về các tác vụ text→speech, một chiến lược tương tự như của text→image được sử dụng để xây dựng các hướng dẫn, trong khi Libritts (Zen et al. 2019) được sử dụng làm tham chiếu. Vì mô hình text→speech dịch văn bản đầu vào một cách trực tiếp, chất lượng âm thanh không bị ảnh hưởng bởi đầu ra của LLM. Do đó, chúng tôi sử dụng điểm BLEU (Papineni et al. 2002) để đo lường sự tương đồng giữa đầu ra của LLM và nội dung tham chiếu. Ngoài ra, để xác định xem mô hình có thể nhận biết phương thức đầu ra mong muốn từ hướng dẫn hay không, chúng tôi đánh giá hiệu suất của nó bằng độ chính xác phân loại, và chỉ những dự đoán phù hợp với phương thức thực tế mới được đánh giá thêm về điểm vision, language, hoặc speech. Đáng chú ý, vì phương thức đầu ra mặc định của LLM là text, những đầu ra này sẽ không đóng góp vào độ chính xác phân loại. Chúng tôi giải thích thêm chi tiết về các chỉ số đánh giá trong phụ lục.

Kết quả Định lượng
Để đánh giá hiệu quả của SwitchGPT được đề xuất, chúng tôi so sánh chúng với một loạt các kỹ thuật tiên tiến. Bao gồm các LLM được huấn luyện trước như OPT (Zhang et al. 2022) và Llama (Touvron et al. 2023a,b); các LLM được điều chỉnh hướng dẫn như Alpaca (Taori et al. 2023) và Vicuna (Chiang et al. 2023); và API thương mại, GPT-3.5-turbo. Ngoài ra, chúng tôi xem xét các phương pháp sử dụng LLMs làm bộ điều khiển, như HuggingGPT (Shen et al. 2023) và LLM-grounded Diffusion (Lian et al. 2023). Cho rằng nhiều LLM dựa trên văn bản không được thiết kế vốn dĩ để tạo ra phản hồi không phải văn bản, một cách tiếp cận đánh giá few-shot đã được áp dụng. Cụ thể, bốn hướng dẫn và phản hồi tương ứng được đưa ra làm ví dụ, nhắc các phương pháp này tạo ra phản hồi cho đầu vào mới (vui lòng tham khảo phụ lục để biết thêm chi tiết). Đáng chú ý, tất cả các LLM chia sẻ các mô hình chuyển đổi phương thức giống hệt nhau để tạo ra phản hồi không phải văn bản.

Bảng 1 thể hiện hiệu suất của phương pháp được đề xuất so với các kỹ thuật hiện tại. Về độ chính xác phương thức, phương pháp của chúng tôi đạt được hiệu suất tương đương với GPT-3.5-turbo và vượt trội hơn tất cả các kỹ thuật khác. Điều này làm nổi bật khả năng hiểu phương thức mong đợi của hướng dẫn. Thú vị, mô hình OPT-6.7B tụt hậu đáng kể về độ chính xác khi so sánh với tất cả các phương pháp khác, thậm chí cả đối tác 2.7B của chính nó. Chúng tôi quan sát thấy nó chủ yếu cung cấp đầu ra văn bản cho hầu hết các hướng dẫn. Sự khác biệt này có thể là do độ nhạy cảm cao của nó với các lời nhắc và ví dụ cụ thể. Điểm CLIP chỉ ra sự phù hợp của các hình ảnh được tạo ra với các mô tả tương ứng. Vì tất cả các mô hình LLM sử dụng cùng một mô hình nền tảng text→image, điểm số có thể được sử dụng để đo lường chất lượng của lời nhắc được tạo ra bởi LLM. Điểm số phục vụ như một chỉ báo về chất lượng lời nhắc được tạo ra bởi LLM. Mô hình của chúng tôi liên tục xuất sắc trong chỉ số này, làm nổi bật hiệu quả của điều chỉnh hướng dẫn căn chỉnh phương thức. Về điểm ngôn ngữ, phương pháp của chúng tôi đạt được độ chính xác tương đương với Llama2-7B gốc. Điều này cho thấy khả năng lập luận cơ bản vẫn không bị ảnh hưởng. Ngoài ra, vì stable diffusion và LLM-grounded diffusion chỉ có thể tạo ra kết quả hình ảnh, chúng tôi so sánh hiệu suất của chúng trong Bảng 2.

--- TRANG 7 ---
Tháp nổi tiếng nơi Galileo Galilei tiến hành thí nghiệm rơi tự do là Tháp nghiêng Pisa, nằm ở Pisa, Ý. Thí nghiệm bao gồm việc thả các vật thể có khối lượng khác nhau từ tháp để chứng minh rằng tất cả chúng sẽ chạm đất cùng một lúc, bác bỏ niềm tin phổ biến vào thời đó rằng các vật thể nặng hơn rơi nhanh hơn các vật thể nhẹ hơn. Độ nghiêng độc đáo của tháp đã khiến nó trở thành một địa danh biểu tượng và điểm đến phổ biến cho khách du lịch từ khắp nơi trên thế giới.

Hãy cho tôi xem tháp nổi tiếng nơi Galileo tiến hành thí nghiệm rơi tự do.

Stable-Diffusion  ChatGPT3.5  Của chúng tôi

Của chúng tôi

Lời nhắc: Một phong cảnh siêu thực và tưởng tượng với những sinh vật và vật thể kỳ lạ.

Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về "wonderland" bên trong cuốn sách. Bạn có thể cho tôi thấy nó trông như thế nào không?

…Hướng dẫn  Hugging-GPT
Lời nhắc: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Đầu vào: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Thất bại trong phân tích.

LLM-Grounded Diffusion
Bố cục Được Tạo
Sách Nền
Các vật thể cần bao gồm
Sách [140, 250, 211, 311]
Stable-Diffusion

Tất nhiên! "Alice in Wonderland" của Lewis Carroll là một câu chuyện kỳ quái và tưởng tượng diễn ra trong một thế giới đầy những yếu tố siêu thực và giàu trí tưởng tượng. Wonderland là một nơi mà logic và thực tế thường bị đảo ngược, và những sự kiện kỳ lạ là điều bình thường.

ChatGPT

ChatGPT
Con người Chuyên gia
Tập dữ liệu Phương thức
Hướng dẫn Mồi
Văn bản→Hình ảnh
Văn bản→Âm thanh
…
Tập dữ liệu Hướng dẫn

Hướng dẫn được Tạo: Bạn có thể tạo một hình ảnh đại diện cho khái niệm "du hành thời gian", sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?
Phản hồi: [Hình ảnh] Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.
Chú thích: Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.

Tạo Hướng dẫn Nhận biết Phương thức Được Căn chỉnh
Quy trình Điều chỉnh/Suy luận

Tập dữ liệu Hướng dẫn
Bộ Điều chỉnh
Chuyển đổi Phương thức
Mô hình Văn bản→Hình ảnh
Mô hình Văn bản→Âm thanh
…

Ví dụ Quy trình Tạo  Điều chỉnh Hướng dẫn  Suy luận
Đầu vào
Phản hồi Có Cấu trúc
Văn bản
Âm thanh
Hình ảnh
Mất mát
Chọn Tuyến Chuyển đổi
Đầu ra
LLM

Tôi dự định du lịch đến New York tuần tới. Tôi nghe nói có một tượng điêu khắc nổi tiếng là món quà từ Pháp. Nó có tên gì?

Tượng điêu khắc nổi tiếng mà bạn đang đề cập đến là Tượng Nữ thần Tự do.

Tuyệt vời! Bạn có thể cho tôi xem một bức ảnh của Tượng Nữ thần Tự do không?

Tuyệt đẹp! Bây giờ, xin hãy dùng giọng nói của bạn để giới thiệu cho tôi lịch sử của Tượng Nữ thần Tự do.

Tượng Nữ thần Tự do được thiết kế bởi nhà điêu khắc Pháp Fredric August Barthodi. Nó đứng trên đảo Tự do trong Cảng New York và đã trở thành biểu tượng của các giá trị Mỹ.

Nội dung Âm thanh

Kế hoạch
Văn bản→Hình ảnh
Văn bản→Âm thanh
???
Đầu ra Mong đợi?

Vui lòng sử dụng hình ảnh để trả lời câu đố này (Gợi ý: đó là một loại động vật): Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì?

Một con Đà điểu
[85, 32, 342, 480]
Chiều cao của chân tôi, thường khoảng 6 feet, cao hơn hầu hết con người. Tôi là gì.
Một bức ảnh hươu cao cổ.

Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc.

Một địa danh biểu tượng ở Tây An
[12, 45, 488, 421]
Hãy cho tôi xem một địa danh biểu tượng ở Tây An, Trung Quốc
Đội quân Đất nung ở Tây An, Trung Quốc.

Hình 4: Kết quả định tính. Từ trái sang phải: hình ảnh được tạo ra bởi Stable Diffusion, LLM-grounded Diffusion, HuggingGPT, và SwitchGPT được đề xuất. Phía bên phải của mỗi hình ảnh thể hiện các tạo phẩm trung gian như bố cục và lời nhắc (Stable Diffusion sử dụng hướng dẫn làm lời nhắc trực tiếp) được tạo ra bởi các phương pháp tương ứng.

Kết quả Định tính
Trong Hình 4, chúng tôi cung cấp so sánh định tính của các phương pháp khác nhau dựa trên các hướng dẫn dành cho việc tạo ra đầu ra hình ảnh. Kết quả cho thấy rằng phương pháp được đề xuất của chúng tôi căn chỉnh tốt với các mô hình chuyển đổi phương thức, mang lại hiệu suất nâng cao. Các phương pháp khác, như HuggingGPT, có xu hướng sử dụng chính hướng dẫn như lời nhắc để đưa vào mô hình chuyển đổi phương thức, dẫn đến đầu ra không thỏa đáng. Trong khi đó, Hình 5 trình bày một demo (sẽ được phát hành) thể hiện một ví dụ cuộc hội thoại giữa mô hình của chúng tôi và người dùng về chủ đề 'Tượng Nữ thần Tự do'. Tương tác làm nổi bật khả năng của mô hình chúng tôi trong việc nhận biết những ý định cơ bản của các hướng dẫn, mặc dù những đầu vào đối thoại này khá khác biệt so với các hướng dẫn được sử dụng trong quá trình huấn luyện. Do đó, nó có khả năng tạo ra các phản hồi thích hợp bao gồm các phương thức khác nhau. Xem thêm kết quả trong phụ lục.

Kết luận
Trong bài báo này, chúng tôi đã trình bày điều chỉnh hướng dẫn căn chỉnh phương thức, một phương pháp được thiết kế để điều chỉnh các LLM dựa trên văn bản để tạo ra phản hồi không phải văn bản. Đây là nỗ lực đầu tiên căn chỉnh đầu ra của LLMs và đầu vào của các mô hình chuyển đổi phương thức từ góc độ dữ liệu. Một lợi thế chính là nó cho phép LLMs được huấn luyện mà không cần tiếp xúc trực tiếp với dữ liệu phương thức không phải văn bản. Thay vì đào sâu vào lĩnh vực phức tạp của việc tiền huấn luyện đa phương thức, chúng chỉ cần trải qua một quy trình điều chỉnh hướng dẫn tiêu chuẩn, đảm bảo hiệu quả tính toán. Để đánh giá khách quan phương pháp của chúng tôi, chúng tôi đã giới thiệu một tập xác thực gồm các hướng dẫn yêu cầu đầu ra văn bản, hình ảnh và âm thanh. Dựa trên tập dữ liệu này, chúng tôi đã

Tháp nổi tiếng nơi Galileo Galilei tiến hành thí nghiệm rơi tự do là Tháp nghiêng Pisa, nằm ở Pisa, Ý. Thí nghiệm bao gồm việc thả các vật thể có khối lượng khác nhau từ tháp để chứng minh rằng tất cả chúng sẽ chạm đất cùng một lúc, bác bỏ niềm tin phổ biến vào thời đó rằng các vật thể nặng hơn rơi nhanh hơn các vật thể nhẹ hơn. Độ nghiêng độc đáo của tháp đã khiến nó trở thành một địa danh biểu tượng và điểm đến phổ biến cho khách du lịch từ khắp nơi trên thế giới.

Hãy cho tôi xem tháp nổi tiếng nơi Galileo tiến hành thí nghiệm rơi tự do.

Stable-Diffusion  ChatGPT3.5  Của chúng tôi

Của chúng tôi

Văn bản-sang-Hình ảnh
Lời nhắc: Một phong cảnh siêu thực và tưởng tượng với những sinh vật và vật thể kỳ lạ.

Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về "wonderland" bên trong cuốn sách. Bạn có thể cho tôi thấy nó trông như thế nào không?

…Hướng dẫn  Hugging-GPT
Văn bản-sang-Hình ảnh
Lời nhắc: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Văn bản-sang-Âm thanh
Đầu vào: Tôi đang đọc Alice in Wonderland, và tôi rất tò mò về ……
Thất bại trong phân tích.

LLM-Grounded Diffusion
Bố cục Được Tạo
Sách Nền
Các vật thể cần bao gồm
Sách [140, 250, 211, 311]
Stable-Diffusion

Tất nhiên! "Alice in Wonderland" của Lewis Carroll là một câu chuyện kỳ quái và tưởng tượng diễn ra trong một thế giới đầy những yếu tố siêu thực và giàu trí tưởng tượng. Wonderland là một nơi mà logic và thực tế thường bị đảo ngược, và những sự kiện kỳ lạ là điều bình thường.

ChatGPT

ChatGPT
Con người Chuyên gia
Tập dữ liệu Phương thức
Hướng dẫn Mồi
Văn bản→Hình ảnh
Văn bản→Âm thanh
…
Tập dữ liệu Hướng dẫn

Hướng dẫn được Tạo: Bạn có thể tạo một hình ảnh đại diện cho khái niệm "du hành thời gian", sử dụng các vật thể hàng ngày như xe đạp và đồng hồ không?
Phản hồi: [Hình ảnh] Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.
Chú thích: Một chiếc xe đạp kim loại đen với đồng hồ bên trong bánh xe trước.

Tạo Hướng dẫn Nhận biết Phương thức Được Căn chỉnh
Quy trình Điều chỉnh/Suy luận

Tập dữ liệu Hướng dẫn
Bộ Điều chỉnh
Chuyển đổi Phương thức
Mô hình Văn bản→Hình ảnh
Mô hình Văn bản→Âm thanh
…

Ví dụ Quy trình Tạo  Điều chỉnh Hướng dẫn  Suy luận
Đầu vào
Phản hồi Có Cấu trúc
Văn bản
Âm thanh
Hình ảnh
Mất mát
Chọn Tuyến Chuyển đổi
Đầu ra
LLM

Tôi dự định du lịch đến New York tuần tới. Tôi nghe nói có một tượng điêu khắc nổi tiếng là món quà từ Pháp. Nó có tên gì?

Tượng điêu khắc nổi tiếng mà bạn đang đề cập đến là Tượng Nữ thần Tự do.

Tuyệt vời! Bạn có thể cho tôi xem một bức ảnh của Tượng Nữ thần Tự do không?

Tuyệt đẹp! Bây giờ, xin hãy dùng giọng nói của bạn để giới thiệu cho tôi lịch sử của Tượng Nữ thần Tự do.

Tượng Nữ thần Tự do được thiết kế bởi nhà điêu khắc Pháp Fredric August Barthodi. Nó đứng trên đảo Tự do trong Cảng New York và đã trở thành biểu tượng của các giá trị Mỹ.

Nội dung Âm thanh

Hình 5: SwitchGPT được đề xuất có thể tạo ra phản hồi không phải văn bản để thực hiện các hướng dẫn đã cho.

xem xét một số LLM tiên tiến trong thiết lập few-shot và đánh giá cách tiếp cận của chúng tôi so với chúng. Chúng tôi dự đoán rằng SwitchGPT được đề xuất của chúng tôi sẽ phục vụ như một điểm chuẩn cho các nghiên cứu tương lai về LLM đầu ra đa phương thức. Trong nghiên cứu tương lai,

--- TRANG 8 ---
một hướng đi hứa hẹn có thể là thiết kế các bộ điều chỉnh ở giai đoạn mã hóa giúp tích hợp đầu vào đa phương thức. Bằng cách kết nối liền mạch tính năng này với phương pháp được đề xuất trong nghiên cứu này, nó có thể mang lại một LLM Any-to-Any toàn diện có khả năng xử lý một loạt tác vụ và chức năng rộng rãi hơn.

Tài liệu tham khảo
Ao, J.; Wang, R.; Zhou, L.; Wang, C.; Ren, S.; Wu, Y.; Liu, S.; Ko, T.; Li, Q.; Zhang, Y.; et al. 2022. Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing. In Proc. Annu. Meet. Assoc. Comput. Linguist.

Bao, F.; Nie, S.; Xue, K.; Li, C.; Pu, S.; Wang, Y.; Yue, G.; Cao, Y.; Su, H.; and Zhu, J. 2023. One transformer fits all distributions in multi-modal diffusion at scale. arXiv preprint arXiv:2303.06555.

Beeching, E.; Fourrier, C.; Habib, N.; Han, S.; Lambert, N.; Rajani, N.; Sanseviero, O.; Tunstall, L.; and Wolf, T. 2023. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.

Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Proc. Advances in Neural Inf. Process. Syst., 33: 1877–1901.

Chen, X.; Fang, H.; Lin, T.-Y.; Vedantam, R.; Gupta, S.; Dollár, P.; and Zitnick, C. L. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.

Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org.

Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.

Conover, M.; Hayes, M.; Mathur, A.; Meng, X.; Xie, J.; Wan, J.; Shah, S.; Ghodsi, A.; Wendell, P.; Zaharia, M.; et al. 2023. Free dolly: Introducing the world's first truly open instruction-tuned llm.

Dettmers, T.; Lewis, M.; Belkada, Y.; and Zettlemoyer, L. 2022. Llm.int8(): 8-bit matrix multiplication for transformers at scale. Proc. Advances in Neural Inf. Process. Syst.

Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer, L. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314.

Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378.

Frantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh, D. 2023. Gptq: Accurate post-training quantization for generative pre-trained transformers. In Proc. Int. Conf. Learn. Representations.

Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. In Proc. Advances in Neural Inf. Process. Syst., volume 27.

Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring massive multitask language understanding. Proc. Int. Conf. Learn. Representations.

Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Proc. Advances in Neural Inf. Process. Syst., 30.

Hossain, M. Z.; Sohel, F.; Shiratuddin, M. F.; and Laga, H. 2019. A comprehensive survey of deep learning for image captioning. ACM Computing Surveys, 51(6): 1–36.

Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2022. Lora: Low-rank adaptation of large language models. In Proc. Int. Conf. Learn. Representations.

Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proc. Int. Conf. Mach. Learn.

Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li, Y. 2023b. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244.

Li, Y.; Hu, B.; Chen, X.; Ma, L.; and Zhang, M. 2023c. LM-Eye: An Interactive Perception Network for Large Language Models. arXiv preprint arXiv:2305.03701.

Lian, L.; Li, B.; Yala, A.; and Darrell, T. 2023. LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models. arXiv preprint arXiv:2305.13655.

Lin, S.; Hilton, J.; and Evans, O. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proc. Annu. Meet. Assoc. Comput. Linguist.

Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.

Loshchilov, I.; and Hutter, F. 2019. Decoupled weight decay regularization. In Proc. Int. Conf. Learn. Representations.

Mai, J.; Chen, J.; Li, B.; Qian, G.; Elhoseiny, M.; and Ghanem, B. 2023. LLM as A Robotic Brain: Unifying Egocentric Memory and Control. arXiv preprint arXiv:2304.09349.

OpenAI. 2023. ChatGPT. https://openai.com/blog/chatgpt.

Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. Annu. Meet. Assoc. Comput. Linguist., 311–318.

Qin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.; Cong, X.; Tang, X.; Qian, B.; et al. 2023. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. arXiv preprint arXiv:2307.16789.

Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;

--- TRANG 9 ---
et al. 2021. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn., 8748–8763. PMLR.

Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8): 9.

Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1): 5485–5551.

Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-image generation. In Proc. Int. Conf. Mach. Learn., 8821–8831. PMLR.

Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 10684–10695.

Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow, D.; Castagne, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.

Schick, T.; Dwivedi-Yu, J.; Dessì, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.

Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.; Wightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis, C.; Wortsman, M.; et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Proc. Advances in Neural Inf. Process. Syst., 35: 25278–25294.

Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang, Y. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.

Su, Y.; Lan, T.; Li, H.; Xu, J.; Wang, Y.; and Cai, D. 2023. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355.

Sun, Q.; Yu, Q.; Cui, Y.; Zhang, F.; Zhang, X.; Wang, Y.; Gao, H.; Liu, J.; Huang, T.; and Wang, X. 2023. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222.

Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.

Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Vinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015. Show and tell: A neural image caption generator. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 3156–3164.

Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. Int. Conf. Learn. Representations.

Wang, G.; Ge, Y.; Ding, X.; Kankanhalli, M.; and Shan, Y. 2023. What Makes for Good Visual Tokenizers for Large Language Models? arXiv preprint arXiv:2305.12223.

Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.; Khashabi, D.; and Hajishirzi, H. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560.

Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang, X.; and He, X. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 1316–1324.

Zen, H.; Dang, V.; Clark, R.; Zhang, Y.; Weiss, R. J.; Jia, Y.; Chen, Z.; and Wu, Y. 2019. Libritts: A corpus derived from librispeech for text-to-speech. In Proc. Interspeech.

Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.

Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.

--- TRANG 10 ---
Phụ lục

Tạo Hướng dẫn
Ở đây, chúng tôi trình bày thêm chi tiết về việc tạo ra các hướng dẫn căn chỉnh phương thức.

Hướng dẫn Mồi và Lời nhắc Tạo
Chúng tôi đã tự tạo ra 100 hướng dẫn mồi, trong đó 50 dành cho đầu ra hình ảnh và 50 cho đầu ra âm thanh. Listing 1 và Listing 2 thể hiện một số ví dụ về mồi hình ảnh và âm thanh tương ứng.

Listing 1: Ví dụ Hướng dẫn Mồi Hình ảnh
```
{
"instruction": "Hãy cho tôi xem một thiết kế web UI tốt.",
"output":
{
"type": "image",
"response": "Một bản phác thảo khái niệm thiết kế web UI chuyên nghiệp với bố cục sạch sẽ, thanh điều hướng trực quan, và phối màu nhất quán."
}
},
{
"instruction": "Thể hiện thành phố trong một trăm năm nữa.",
"output":
{
"type": "image",
"response": "Một cảnh quan thành phố tương lai với những tòa nhà chọc trời cao tầng và phương tiện bay."
}
},
{
"instruction": "Biến câu này thành hình ảnh: một bức ảnh phi hành gia cưỡi ngựa trên sao Hỏa.",
"output":
{
"type": "image",
"response": "Một bức ảnh phi hành gia cưỡi ngựa trên sao Hỏa"
}
}
```

Listing 2: Ví dụ Hướng dẫn Mồi Âm thanh
```
{
"instruction": "Tạo một đoạn âm thanh để giới thiệu Vũ Hán.",
"output":
{
"type": "audio",
"response": "Vũ Hán, nằm ở Trung Trung Quốc, nổi tiếng với các cơ sở giáo dục như Đại học Vũ Hán và các địa danh như Hoàng Hạc Lâu. Nổi tiếng với ẩm thực đường phố, thành phố thể hiện tinh thần của Trung Quốc hiện đại với lịch sử phong phú và văn hóa sôi động."
}
},
{
"instruction": "Bạn phát âm tên này như thế nào: John?",
"output":
{
"type": "audio",
"response": "John"
}
},
{
"instruction": "Thể hiện một câu thoại nổi tiếng từ phim 'The Godfather' bằng giọng nói.",
"output":
{
"type": "audio",
"response": "Tôi sẽ đưa ra cho anh ta một lời đề nghị mà anh ta không thể từ chối."
}
},
{
"instruction": "Đọc câu nói lắp này 'She sells sea shells by the seashore. The shells she sells are surely seashells. So if she sells shells on the seashore, I'm sure she sells seashore shells.'",
"output":
{
"type": "audio",
"response": "She sells sea shells by the seashore. The shells she sells are surely seashells. So if she sells shells on the seashore, I'm sure she sells seashore shells."
}
}
```

--- TRANG 11 ---
Sau khi xây dựng các hướng dẫn mồi, chúng tôi đã thiết kế các mẫu để nhắc ChatGPT tạo ra hướng dẫn hàng loạt.

Bạn được yêu cầu tạo ra hướng dẫn dựa trên các mô tả hình ảnh được cho. Đây là một số yêu cầu:
1. Mỗi hướng dẫn phải ngắn gọn và súc tích, lý tưởng nhất là 1 đến 2 câu bằng tiếng Anh.
2. Mỗi hướng dẫn phải dựa trên các mô tả hình ảnh. Tuy nhiên, cố gắng không sử dụng trực tiếp nội dung trong mô tả để hình thành hướng dẫn, và giảm việc sử dụng các từ đã xuất hiện trong mô tả.
3. Không tạo ra yêu cầu có thể dẫn đến câu trả lời gây nhầm lẫn. Ví dụ, không yêu cầu tạo ra một nhân vật mà không có thông tin bổ sung.
4. Ngôn ngữ của hướng dẫn phải đa dạng, cố gắng sử dụng các động từ khác nhau cho mỗi hướng dẫn để tối đa hóa sự đa dạng. Có thể là câu mệnh lệnh hoặc câu hỏi.

Đây là một số ví dụ:
// Mỗi lần chúng tôi ngẫu nhiên chèn 3 ví dụ từ các hướng dẫn mồi ở đây

[Một bức ảnh bơ]
Hướng dẫn: Hình dung câu trả lời cho câu đố này: Tôi là một loại trái cây xanh khi còn sống và đen khi chín. Tôi béo nhưng tôi không phải sản phẩm từ sữa. Tôi là gì?

[Một cảnh quan thành phố tương lai với những tòa nhà chọc trời cao tầng và phương tiện bay.]
Hướng dẫn: Thể hiện thành phố trong một trăm năm nữa.

[Sinh vật thần thoại với thân rồng, cánh phượng hoàng, và cả hai đều bao phủ trong lửa.]
Hướng dẫn: Tạo ra một biểu diễn trực quan của một sinh vật thần thoại kết hợp các yếu tố của rồng và phượng hoàng.

Sau đây là một số mô tả hình ảnh, Vui lòng tạo ra một hướng dẫn thích hợp cho mỗi cái:
// Các chú thích hình ảnh được lấy mẫu từ LAION-aesthetics

[Một máy bay chở khách lớn bay qua một số cây cọ]
[Một con chó nhìn một người đàn ông cắt một miếng thức ăn]
...

Mẫu được cung cấp ở trên minh họa đầu vào lời nhắc cho API GPT của OpenAI (OpenAI 2023), được thiết kế riêng để tạo ra hướng dẫn hình ảnh. Để tạo ra hướng dẫn âm thanh, chúng tôi chỉ cần điều chỉnh một số từ khóa, như thay đổi 'image' thành 'speech', và sử dụng tập huấn luyện text-to-speech làm tham chiếu. Mỗi lần API được sử dụng, ba ví dụ từ các hướng dẫn mồi được nhúng vào lời nhắc, và 60 chú thích được lấy mẫu từ tập dữ liệu tham chiếu. Do đó, API có thể cung cấp tới 60 hướng dẫn mỗi lần gọi, với chi phí <USD$ 0.01 mỗi lần.

generate create write
rewrite
give
explain
edit
provide
suggest describe make summarize classify construct find name design identify calculate list

list
sentence
story
title
list
story
sentence
poem
story
sentence
description
function
sentence
paragraph
statement
phrase
example
list
tip
reason
concept
difference
meaning
importance
sentence
text
paragraph
code
example
list
tip
definition
way
activity
strategy
title
process
difference
way
character
list
sentence
prediction
argument
article
text
point
story
sentence
item
animal
text
sentence
query
argument
expression
word
number
example
area
type
country
benefit
animal
logo
algorithm
game
interface
type
word
difference
theme
area
cost
sum
average
advantage
step
benefit
reason

(a) Hướng dẫn Chỉ có Văn bản (Taori et al. 2023)

generate
create
show
design provide recite compose produce write give describe explain suggest construct find draw name visualize list identify

image
clip
list
sentence
image
clip
representation
list
image
picture
sunset
photo
logo
image
poster
cover
clip
image
example
representation
quote
line
speech
poem
clip
speech
poem
tweet
image
clip
representation
story
sentence
description
function
example
clip
speech
list
process
taste
character
difference
concept
difference
meaning
purpose
way
strategy
idea
title
sentence
query
algorithm
timeline
word
number
area
synonym
landscape
portrait
cityscape
character
benefit
type
character
thing
cityscape
scene
cabin
sunset
benefit
advantage
example
element
type
strategy
country
way

(b) Hướng dẫn Căn chỉnh Phương thức (Của chúng tôi)

Hình 6: So sánh phân bố của một số động từ gốc hàng đầu và danh từ tương ứng giữa các hướng dẫn chỉ có văn bản được tạo ra bởi Alpaca (Taori et al. 2023) và các hướng dẫn căn chỉnh phương thức của chúng tôi.

Dựa trên phương pháp trên, chúng tôi đã tạo ra khoảng 40k hướng dẫn với đầu ra hình ảnh hoặc âm thanh mong đợi. Sau đó, chúng tôi loại bỏ những hướng dẫn bị biến dạng và không thể phân tích được. Ngoài ra, chúng tôi đã loại bỏ các hướng dẫn yêu cầu âm thanh không phải lời nói, như tiếng mèo kêu, tiếng sấm, hoặc âm nhạc, cũng như những hướng dẫn yêu cầu đọc câu bằng các ngôn ngữ khác, như tiếng Nhật hoặc tiếng Ả Rập.

Hình 6 so sánh phân bố của các động từ gốc và danh từ tương ứng giữa các hướng dẫn căn chỉnh phương thức của chúng tôi và tập dữ liệu hướng dẫn dựa trên văn bản được tạo ra bởi Alpaca (Taori et al. 2023). Như được mô tả, các hướng dẫn của chúng tôi chứa một số lượng đáng kể các chỉ thị không phải văn bản, như "Generate image", "Design logo", và "Recite quote", đã được Alpaca cố tình loại trừ. Chúng tôi kết hợp

--- TRANG 12 ---
Đánh giá

Suy luận Few-shot cho LLM dựa trên Văn bản
Trong bài báo chính, chúng tôi đã giải thích rằng với các lời nhắc phù hợp, các LLM dựa trên văn bản thuần túy được huấn luyện trước cũng có khả năng phân tích phương thức đầu ra dự định của các hướng dẫn đã cho. Do đó, chúng tôi đã thiết kế mẫu lời nhắc sau để đánh giá các LLM này.

Bạn là một trợ lý hữu ích. Với một hướng dẫn, nhiệm vụ của bạn là tạo ra một phản hồi định dạng JSON phù hợp. Đây là cách bạn có thể thực hiện:

1. Xác định phương thức đầu ra mong đợi của hướng dẫn đã cho. Các tùy chọn có sẵn là 'text', 'image', và 'audio'.
2. Đặt phương thức mong đợi vào trường 'type' của phản hồi.
3. Tiến hành thực hiện hướng dẫn dựa trên phương thức đã xác định:
   • Nếu phương thức mong muốn là 'text' hoặc 'speech', cung cấp phản hồi trực tiếp trong trường 'response'.
   • Nếu phương thức mong muốn là 'image', tạo ra một lời nhắc ngắn gọn và súc tích. Lời nhắc này sẽ được gửi đến một mô hình text-to-image, sau đó sẽ tạo ra hình ảnh thực hiện yêu cầu.

Hướng dẫn: "Hãy cho tôi xem phong cách kiến trúc của thời đại Baroque."
Phản hồi: '"type": "image", "response": "Cung điện phong cách Baroque lộng lẫy, trang trí với ánh sáng kịch tính và hình dạng phức tạp."'

Hướng dẫn: "Hãy cho tôi xem một khu vườn Zen Nhật Bản yên tĩnh."
Phản hồi: '"type": "image", "response": "Một khu vườn Zen Nhật Bản yên tĩnh với cát được gào, đá mịn, rêu, và một cây cầu gỗ bắc qua một ao nhỏ."'

Hướng dẫn: "Bạn nói 'ngày 1 tháng 10 năm 2000' trong tiếng Anh nói như thế nào?"
Phản hồi: '"type": "audio", "response": "October first, two thousand."'

Hướng dẫn: "Tạo một đoạn âm thanh với chủ đề Lễ Phục sinh."
Phản hồi: '"type": "audio", "response": "Lễ Phục sinh, một ngày lễ quý trọng, được tổ chức như một thời gian đổi mới và tái sinh. Nó mang lại niềm vui với các biểu tượng mùa xuân, như trứng, thỏ, và hoa."'

Hướng dẫn: "Ba màu cơ bản là gì?"
Phản hồi: '"type": "text", "response": "Ba màu cơ bản là đỏ, xanh lam, và vàng."'

Hướng dẫn: {instruction}
Phản hồi:

Trong giai đoạn suy luận, '{instruction}' sẽ được thay thế bằng đầu vào thực tế. Mặc dù phương pháp này có thể đo lường hiệu suất của các phương pháp hiện tại ở một mức độ nhất định, điều quan trọng cần lưu ý là các đầu ra có thể khác nhau đáng kể tùy thuộc vào phương pháp được sử dụng. Một nhược điểm rõ ràng của các cách tiếp cận như vậy là cần mã hóa các giải thích dài và ví dụ vào lời nhắc mỗi khi LLM được giao nhiệm vụ tạo ra phản hồi. Điều này có thể không hiệu quả; ví dụ, chính mẫu lời nhắc đã chứa hơn 400 token, dẫn đến sự dư thừa đáng kể. Để đảm bảo so sánh công bằng, tất cả các LLM dựa trên văn bản trải qua suy luận few-shot sử dụng mẫu giống hệt nhau.

Hướng dẫn Đánh giá
Không có bộ công cụ có sẵn để đánh giá các LLM đầu ra đa phương thức. Cho rằng tập huấn luyện gồm các hướng dẫn ngắn được tạo ra bởi GPT, việc sử dụng các hướng dẫn tương tự như tập xác thực để đánh giá hiệu suất mô hình là không phù hợp. Do đó, thay vì sử dụng các hướng dẫn ngắn, chúng tôi sử dụng các đầu vào dài hơn, giống cuộc hội thoại để xác thực. Cách tiếp cận này tránh sự chồng chéo tiềm năng với tập huấn luyện và đánh giá tốt hơn khả năng tổng quát hóa của mô hình. Chúng tôi đã thu thập thủ công các đoạn hội thoại làm đầu vào xác thực (200 cho mỗi phương thức, không bao gồm text). Những đoạn này, kết hợp với quy trình tạo hướng dẫn của chúng tôi và được lọc thủ công, tạo ra tổng cộng 800 hướng dẫn xác thực mỗi phương thức. Đối với phương thức text, với sự phong phú của các tập đánh giá hiện có, chúng tôi trực tiếp lấy mẫu 800 mục từ các tập dữ liệu như TruthfulQA (Lin, Hilton, và Evans 2022) và MMLU (Hendrycks et al. 2021). Listing 3 thể hiện một số ví dụ từ tập xác thực.

Listing 3: Ví dụ Hướng dẫn Xác thực
```
{
"instruction": "Tôi bị nhầm lẫn bởi quá nhiều loại lạc đà. Bạn có thể cho tôi xem một bức ảnh về các loại sinh vật này khác nhau không? Cám ơn bạn!",
"output": {
"type": "image",
"response": "Một bức ảnh so sánh llama, alpaca, vicuna, và Gunaco cạnh nhau.",
"image_id": "24531"
}
},
{
"instruction": "Bạn có thể cho tôi xem bức tranh Nhật Bản nổi tiếng bao gồm sóng và núi Phú Sĩ không?",
"output": {
"type": "image",
"response": "Sóng lớn ngoài khơi Kanagawa.",
"image_id": "42153"
}
}
```

Chỉ số
Độ chính xác Phân loại Phương thức. Chúng tôi sử dụng độ chính xác phân loại như chỉ số để đánh giá xem một phương pháp có thể diễn giải chính xác phương thức đầu ra mong muốn được chỉ định bởi các hướng dẫn đã cho hay không. Nó có thể được tính toán đơn giản như sau:

--- TRANG 13 ---
Tôi bị nhầm lẫn bởi quá nhiều loại lạc đà. Bạn có thể cho tôi xem một bức ảnh về các loại sinh vật này khác nhau không? Cám ơn bạn!

Một con llama, một con vicuna, một con lạc đà, một con alpaca, một con guanaco
Một bức ảnh về các loại lạc đà khác nhau.
Một bức ảnh về các loại lạc đà khác nhau, bao gồm lạc đà, llama, và alpaca.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi)

Tham khảo (GT)

Bạn có thể cho tôi xem bức tranh Nhật Bản nổi tiếng bao gồm sóng và núi Phú Sĩ không?

Một con sóng
[10, 250, 292, 212]
Núi Phú Sĩ
[320, 115, 182, 297]
Bạn có thể cho tôi xem bức tranh Nhật Bản nổi tiếng bao gồm sóng và núi Phú Sĩ không?
'Sóng lớn ngoài khơi Kanagawa' của Katsushika Hokusai

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Tôi nghe nói bạn đã đến bảo tàng Louvre ở Paris. Bạn chắc chắn đã thấy bức chân dung nổi tiếng nhất do Davinci vẽ! Hãy cho tôi xem một bức ảnh về điều này!

Trả về Lỗi 3 lần.
Một bức ảnh về bức tranh Mona Lisa của Leonardo da Vinci tại Bảo tàng Louvre ở Paris.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Một sinh vật nào đó từng thống trị Trái đất trong hai trăm triệu năm và sau đó tuyệt chủng trong sự kiện tuyệt chủng Cretaceous-Paleogene sáu mười triệu năm trước. Bạn có thể vẽ một bức tranh màu nước về sinh vật này cho tôi không?

Một sinh vật nào đó từng thống trị Trái đất trong hai trăm triệu năm và sau đó tuyệt chủng trong...
Một bức tranh màu nước về khủng long.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Tôi đang viết một bài báo khoa học phổ thông về vụ nổ siêu tân tinh. Vui lòng giúp tôi thiết kế một hình minh họa cho chủ đề này.

Tôi đang viết một bài báo khoa học phổ thông về vụ nổ siêu tân tinh.
Một hình minh họa về vụ nổ siêu tân tinh, với một ngôi sao ở trung tâm và một đám mây khí và bụi sáng, đang mở rộng.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Mona Lisa
[50, 60, 412, 392]
Một con khủng long
[25, 96, 462, 316]
Một vụ nổ siêu tân tinh
[50, 70, 412, 372]
Một ngôi sao
[218, 176, 75, 75]

Hình 7: Kết quả Định tính (Tốt nhất khi phóng to xem).

A = n_correct / n_total (4)

Trong phương trình này, n_correct đại diện cho số lượng phương thức được phân loại đúng, trong khi n_total biểu thị tổng số hướng dẫn thử nghiệm.

Ngôn ngữ. Để đánh giá ngôn ngữ, các tập dữ liệu hiện có giống định dạng lựa chọn đa dạng đã cung cấp một số câu trả lời đúng và sai cho mỗi hướng dẫn. Chúng tôi sử dụng điểm BLEU để so khớp các phản hồi được tạo ra bởi

--- TRANG 14 ---
Con gái tôi nói với tôi rằng cô ấy muốn vẽ một sinh vật thần thoại mà cô ấy thấy trong một bộ phim hoạt hình. Đó là một con vật giống ngựa đầy màu sắc với một chiếc sừng trên đầu và đôi cánh trên lưng. Bạn có thể cho tôi xem một ví dụ tranh vẽ không?

Một con vật giống ngựa đầy màu sắc với một chiếc sừng trên đầu và đôi cánh trên lưng.
Một bức tranh về sinh vật thần thoại, một con vật giống ngựa đầy màu sắc với một chiếc sừng trên đầu và đôi cánh trên lưng.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Một con vật giống ngựa đầy màu sắc với một chiếc sừng trên đầu và đôi cánh trên lưng.
[62, 137, 390, 250]

Tôi đang thiết kế một poster cho lễ hội bia. Bạn có thể cho tôi xem một ví dụ với hai cốc bia ở trung tâm poster không?

Một poster cho lễ hội bia với hai cốc bia ở trung tâm

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

Một cốc bia
[136, 206, 120, 180]
Một cốc bia
[256, 206, 120, 180]
Tôi đang thiết kế một poster cho lễ hội bia.
Hai cốc bia ở trung tâm poster

Chủ đề của cuộc thi này là "Không gian Tương lai trong Mắt Tôi". Tôi hy vọng bạn có thể cho tôi thấy thế giới trong mắt bạn thông qua các tác phẩm nghệ thuật.

Chủ đề của cuộc thi này là "Không gian Tương lai trong Mắt Tôi". Tôi hy vọng bạn có thể cho tôi thấy thế giới trong mắt bạn thông qua các tác phẩm nghệ thuật.
Tôi sẽ tạo ra một tác phẩm nghệ thuật theo chủ đề không gian tương lai thể hiện vẻ đẹp của vũ trụ qua mắt tôi.

Stable Diffusion | LLM-grounded Diffusion (ChatGPT-4) | HuggingGPT (GPT3.5) | SwitchGPT (của chúng tôi) | Tham khảo (GT)

[]
Không Tạo ra Hình ảnh
Không Tạo ra Hình ảnh

Hình 8: Trường hợp Thất bại.

LLMs với các câu trả lời tham chiếu. Sau đó, chúng tôi chọn phản hồi tương tự nhất để xác định độ chính xác cuối cùng.

Hình ảnh. Chúng tôi sử dụng hai chỉ số được chấp nhận rộng rãi để đánh giá chất lượng của hình ảnh được tạo ra dựa trên hướng dẫn của chúng: điểm CLIP và FID. Điểm CLIP đo lường sự liên quan giữa hình ảnh được tạo ra và các mô tả thực tế tương ứng. Ví dụ, Listing 3 trình bày một số bản ghi từ tập thử nghiệm. Trong mỗi bản ghi, có một mô tả văn bản về hình ảnh dự kiến trong trường 'response', mà chúng tôi sử dụng để tính toán điểm CLIP. Mặt khác, điểm FID đánh giá chất lượng tạo hình ảnh bằng cách tính toán khoảng cách inception Fréchet giữa hình ảnh được tạo ra và các đối tác tham chiếu của chúng. Hình ảnh tham chiếu được chỉ ra bởi trường 'image_id' trong các bản ghi.

Âm thanh. Tất cả các LLM sử dụng cùng một mô hình tạo âm thanh, dẫn đến chất lượng âm thanh gần như giống nhau. Do đó, mối quan tâm chính của chúng tôi là liệu nội dung có phù hợp với hướng dẫn hay không. Vì vậy, chúng tôi tiếp cận việc đánh giá tác vụ này như một tác vụ dịch/tạo sinh, sử dụng điểm BLEU để đo lường sự tương tự giữa phản hồi và sự thật cơ bản.

Kết quả Định tính
Trong bài báo chính, chúng tôi đã trình bày một số kết quả định tính. Ở đây, chúng tôi thể hiện thêm so sánh giữa các phương pháp của chúng tôi và các phương pháp hiện có.

Thêm so sánh
Hình 7 hiển thị các ví dụ bổ sung từ cả phương pháp hiện có và cách tiếp cận của chúng tôi. Một số quan sát thú vị như sau:

• Trong khi Stable Diffusion (Rombach et al. 2022) không được tối ưu hóa cụ thể để xử lý các đầu vào dài giống cuộc hội thoại, nó vẫn thành thạo trong việc nắm bắt các thuật ngữ chính trong những hướng dẫn như vậy. Do đó, nếu đầu ra dự kiến là hiển nhiên hoặc liên quan trực tiếp đến đầu vào, Stable Diffusion vẫn có thể tạo ra kết quả thỏa đáng. Ví dụ, trong hàng cuối cùng của Hình 7, thuật ngữ 'vụ nổ siêu tân tinh' có trong hướng dẫn đầu vào, khiến Stable Diffusion tạo ra một biểu diễn đáng khen ngợi về khái niệm này. Tương tự, trong hàng thứ hai của Hình 7, mặc dù hướng dẫn không đề cập rõ ràng đến tiêu đề của bức tranh — 'Sóng lớn ngoài khơi Kanagawa' — Stable Diffusion tạo ra một hình ảnh gần như hoàn hảo (mặc dù vị trí và kích thước của núi khác với hình ảnh tham chiếu) dựa trên đầu vào đã cho. Khả năng thành thạo này có thể là do Stable Diffusion đã tiếp xúc với những hình ảnh tương tự trong quá trình huấn luyện. Các cụm từ như 'sóng', 'tranh Nhật Bản', và 'núi Phú Sĩ' là những manh mối đủ để nó nhận ra và truy xuất từ kiến thức được lưu trữ. Về bản chất, đối với các khái niệm được công nhận rộng rãi, Stable Diffusion có thể tạo ra kết quả đáng khen ngợi với thông tin thưa thớt, tận dụng việc tiền huấn luyện rộng rãi. Tuy nhiên, các khái niệm ít được biết đến hoặc các yêu cầu đòi hỏi khả năng lập luận mạnh hơn có thể không luôn cho ra kết quả hứa hẹn.

• LLM-grounded Diffusion (Lian et al. 2023), tận dụng khả năng lập luận cực kỳ mạnh mẽ của ChatGPT-4, cho ra kết quả hứa hẹn. Tuy nhiên, khi được chuyển đổi sang ChatGPT-3.5, nó thất bại trong việc tạo ra bất kỳ kết quả nào cho phần lớn các đầu vào. Đáng chú ý, đối với các khái niệm được cấu thành từ nhiều mục phụ, LLM-grounded Diffusion tạo ra bố cục cho từng mục riêng lẻ. Cách tiếp cận này có thể vô tình phân đoạn một khái niệm gắn kết thành các mảnh rời rạc. Ví dụ, như được mô tả trong hàng thứ hai của Hình 7, LLM-grounded Diffusion đã tách bức tranh thành hai yếu tố riêng biệt: một con sóng và núi Phú Sĩ. Do đó, đầu ra cuối cùng xuất hiện như thể hai hình ảnh không liên quan được ghép lại với nhau, dẫn đến khoảng trống và nội dung không liên quan trong sự kết hợp của chúng.

• Trong trường hợp của HuggingGPT (Shen et al. 2023), với sự hỗ trợ cho các tác vụ đa bước, đôi khi nó tạo ra các đầu ra phương thức không cần thiết và không liên quan. Ví dụ, nó có thể thường xuyên cung cấp một đoạn âm thanh cùng với hình ảnh đầu ra. Ngoài ra, HuggingGPT đôi khi gặp khó khăn trong việc phân tích đúng đầu ra của chính nó, dẫn đến các lỗi bất ngờ. Như được mô tả trong hàng thứ ba của Hình 7, HuggingGPT đã thất bại ba lần trong việc thực hiện hướng dẫn được cung cấp. Một hạn chế khác là sự không căn chỉnh giữa đầu ra của nó và đầu vào của các mô hình text-to-image. Do đó, nó thường trả về một đoạn của hướng dẫn như một lời nhắc để tạo hình ảnh. Điều này dẫn đến việc đầu ra của nó tương tự đáng kể với stable diffusion gốc trong nhiều trường hợp.

Trường hợp Thất bại
Trong Hình 8, chúng tôi trình bày một số trường hợp thất bại của mô hình được đề xuất, SwitchGPT.

Trong hàng đầu tiên, tất cả các phương pháp đều thất bại trong việc nhận ra thực thể dự định "một con kỳ lân đầy màu sắc" từ các đầu vào. Thay vào đó, chúng mặc định sử dụng mô tả "một con vật giống ngựa đầy màu sắc" được cung cấp trong hướng dẫn đã cho, dẫn đến đầu ra sai lầm.

Hàng thứ hai minh họa rằng khi được đưa ra một hướng dẫn yêu cầu bố cục cụ thể, như "hai cốc bia ở trung tâm", phương pháp của chúng tôi chủ yếu dựa vào các mô hình chuyển đổi phương thức để diễn giải lời nhắc, mặc dù nó cung cấp một mô tả tương đối chính xác. Mặt khác, LLM-grounded Diffusion xuất sắc trong trường hợp này, được tối ưu hóa cho những tình huống như vậy. Nó trực tiếp tạo ra một bố cục với các hộp giới hạn cho hai cốc bia, cho ra kết quả tối ưu. Thú vị, HuggingGPT phân đoạn hướng dẫn thành hai phần. Nó sử dụng phần đầu tiên "Tôi đang thiết kế một poster cho lễ hội bia" cho tác vụ text→image và sau đó sử dụng hình ảnh được tạo ra, cùng với phần thứ hai "hai cốc bia ở trung tâm poster", như lời nhắc cho tác vụ image→image. Đầu ra cuối cùng thành công đặt "hai cốc bia ở trung tâm."

Trong hàng thứ ba, cả LLM-grounded Diffusion và SwitchGPT đều thất bại trong việc tạo ra hình ảnh, mặc dù vì những lý do khác nhau. Thất bại của phương pháp trước phát sinh vì ChatGPT-4 trả về một danh sách đối tượng trống cho hướng dẫn này, ngăn LLM-grounded Diffusion tạo ra bất kỳ hình ảnh nào. Ngược lại, mô hình của chúng tôi gặp khó khăn trong việc nhận biết phương thức dự định của hướng dẫn đã cho, dẫn đến việc nó tạo ra một phản hồi hoàn toàn bằng văn bản.
