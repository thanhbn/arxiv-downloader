# 2401.00849.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2401.00849.pdf
# File size: 6368500 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
COSMO :COntrastive Streamlined MultimOdal Model with
Interleaved Pre-Training
Alex Jinpeng Wang1Linjie Li2Kevin Qinghong Lin1Jianfeng Wang2
Kevin Lin2Zhengyuan Yang2Lijuan Wang2Mike Zheng Shou1
1Show Lab, National University of Singapore2Microsoft Azure AI
http://fingerrec.github.io/cosmo
Abstract
In the evolution of Vision-Language Pre-training, shift-
ing from short-text comprehension to encompassing ex-
tended textual contexts is pivotal. Recent autoregressive
vision-language models like [2, 14], leveraging the long-
context capability of Large Language Models, have ex-
celled in few-shot text generation tasks but face challenges
in alignment tasks. Addressing this gap, we introduce the
contrastive loss into text generation models, presenting the
COntrastive-Streamlined MultimOdal framework (CosMo),
strategically partitioning the language model into dedicated
unimodal text processing and adept multimodal data han-
dling components. CosMo, our unified framework, merges
unimodal and multimodal elements, enhancing model per-
formance for tasks involving textual and visual data while
notably reducing learnable parameters. However, these
models demand extensive long-text datasets, yet the avail-
ability of high-quality long-text video datasets remains lim-
ited. To bridge this gap, this work introduces Howto-
Interlink7M, an inaugural interleaved video-text dataset
featuring comprehensive captions, marking a significant
step forward. Demonstrating its impact, we illustrate how
Howto-Interlink7M enhances model performance in image-
text tasks. With 34% learnable parameters and utilizing
72% of the available data, our model demonstrates signif-
icant superiority over OpenFlamingo [3]. For instance, in
the 4-shot flickr captioning task, performance notably im-
proves from 57.2% to 65.1%. The contributions of CosMo
and Howto-Interlink7M are underscored by notable perfor-
mance gains across 14 diverse downstream datasets encom-
passing both image-text and video-text tasks.
1. Introduction
The emergence of Large Language Models (LLMs) [35, 47,
61] has significantly propelled the development of multi-
Figure 1. Advancements in Vision-Language Pre-training
(VLP) have transitioned towards accommodating long-form
text inputs . (a). Earlier studies emphasized short, paired
image/video text correlations, exemplified by works such as
CLIP [39] and GiT [51]. (b). Present research emphasizes
in-context learning strategies, showcased by approaches like
Flamingo [2] and Palm-E [14]. LLMs’ exceptional text-processing
enables effortless integration of lengthy documents, showcasing
robust few-shot learning sans extensive fine-tuning.
modal learning paradigms. A notable advantage lies in the
capacity of LLMs to effectively process extensively lengthy
textual inputs, with strong reasoning capabilities [7, 56].
This ability represents a significant stride forward in the do-
main of Natural Language Processing, underscoring the po-
tential of LLMs in addressing complex, multi-dimensional
data. The success of LLMs has spurred considerable inter-
ests and efforts in leveraging it for multi modalities.
In-context learning [6, 12] provides a possible pathway
for models to accept long text inputs in the realm of multi-
modal learning. Recent advancements in employing in-
context learning within multi-modal LLMs have catalyzed
the development of Unified Models with Emerging Capa-
bilities, exemplified by Flamingo [2] and PALM-E [14],
showcased in Figure 1. These unified frameworks offer the
remarkable ability to address numerous downstream tasks
without fine-tuning. This capability is partly attributed toarXiv:2401.00849v1  [cs.CV]  1 Jan 2024

--- PAGE 2 ---
their architectural design, which supports the utilization of
multiple image-text pairs as input and organizes the data
into an “interleaved” format. While these models have ex-
hibited remarkable success in tasks such as Visual Question
Answering (VQA) and Captioning, the architecture pro-
posed by Flamingo [2] is not optimally suited for classifica-
tion tasks as its inherent design focuses on text generation
rather than classification.
High-quality interleaved data is required to enable mod-
els with multimodal in-context learning. However, the ma-
jority of publicly available datasets, such as CC3M [44],
LAION400M [43], and DataComp1B [42], predominantly
consist of short image-text pairs. Recent efforts by CM3
dataset [1], MMC4 [63] and Obelics [25] have introduced
three publicly accessible interleaved datasets, based on web
documents. However, web documents can be noisy, as the
images on the same page might not be highly correlated (see
Figure 4). Compared to web documents, videos, naturally
encompass highly-correlated image sequences. In response,
initiatives like InternVid [53] and Video Chapters [58] have
proposed longer captions generated by language models or
user-annotated chapters. Despite these advancements, the
availability of interleaved video-text data, demonstrating re-
lationships across different clips, remains deficient.
To address these challenges, this paper introduces a
novel architecture capable of processing four distinct types
of inputs, including interleaved data, aiming to rectify
the limitations observed in Flamingo. Our approach in-
volves dividing the LLM into two segments: the first seg-
ment specializes as a text encoder, while the second part
is used for multimodal fusion. Additionally, we present
Howto-Interlink7M, a high-quality interleaved video-text
dataset derived from Howto100M [34], by leveraging GPT-
4 [35]. CosMo is evaluated across a total of 14 image-
text and video-text benchmarks, achieving superior per-
formance compared to Open-Flamingo [3], while utiliz-
ing fewer samples from the same public datasets. Our re-
sults further show that high-quality video-text data from
our Howto-Interlink7M further enhances the performance
of CosMo, even helps on image-text tasks.
Our key contributions include: (i). We introduce a novel
architecture CosMo for interleaved data pre-training, lever-
aging an additional contrastive loss. With only 34% learn-
able parameters, our method outperform [3] clearly. (ii).
We introduce Howto-Interlink7M, a noteworthy addition to
long text multi-modality datasets. (iii). We show that top-
tier interleaved video-text data boosts model performance
in various image-text and video-text tasks.
2. Related Work
Vision-Language Pretraining. The evolution of Lan-
guage Modeling has significantly impacted vision-language
pre-training methodologies. Traditional approaches suchas OSCAR [29], ViLT [23] and UNITER [10], built
upon BERT [11] language architectures, have demonstrated
prowess in downstream tasks without requiring extensive
fine-tuning. The focus, however, pivots towards harness-
ing larger language models [28, 48]. For instance, the
Flamingo [2] has escalated from a 1.4B to a staggering 70B
parameter language model, showcasing robust performance
across various downstream tasks. Notably, Flamingo’s
architecture is adept at handling interleaved image/video
text datasets through specialized input formats and cross-
attention mechanisms. Yet, its performance falls behind
contrastive models in classification tasks, a limitation com-
pared with other works like CLIP [39] and CoCa [59],
which thrive on contrastive learning paradigms.
To capitalize on these insights, we adopt Flamingo’s
input design and incorporate the contrastive loss within
middle-level LLM representations, to further enhance the
alignment between visual and text representations.
Interleaved Data for Multi-modality Learning. Ac-
quiring manually annotated vision-language datasets is
prohibitively expensive, leading to relatively small-scale
datasets (often smaller than 100k instances) such as
COCO [31] and Visual Genome [24]. Traditionally, vision-
language datasets are mainly composed of image-text pairs
from Internet, such as CC3M [44] and WIT in CLIP [39].
The text captions in these web-crawled datasets, are mostly
alt-text descriptions, which are mostly short in length, hence
less descriptive. An innovative shift was introduced by
Flamingo [2], pioneering the concept of long-form image-
text pairs. Recent advancements from Flamingo [2] and
CM3 [1] emphasized the significance of training on en-
tire multimodal webpages, presenting interleaved images
and text as a cohesive sequence. These interleaved datasets
inherently encompass multiple image and text pairs, con-
tributing to the evolving landscape of multimodal learning.
The MMC4 dataset [63] notably stands as the initial pub-
licly available work directly addressing multi-image/multi-
sentence interleaved data. However, MMC4 is missing a
crucial detail, the exact placements of images within the
document structure, which is addressed in obelics [25].
Existing research has predominantly concentrated on ex-
tracting highly correlated image-text data from noisy web
documents. In contrast, our work introduces a pioneer-
ing interleaved video-text dataset, marking a departure from
solely image-text modalities.
3. Howto-Interlink7M
Motivation. The effectiveness of video-language
pre-training often hinges on the availability of high-
quality annotated captions. Existing datasets such as
Howto100M [34] and YT-Temporal [60] predominantly
rely on YouTube videos with texts generated by Automatic

--- PAGE 3 ---
Figure 2. Comparing Conventional Video-Text Datasets with Our Howto-Interlink7M. Left: Conventional video-text datasets typically
contain brief ASR captions describing videos. Right: In Howto-Interlink7M, to include more details and improved video coherence, videos
are segmented into shots. Following this, the GPT-4 model [35] is employed to annotate each shot based on historical context anddetailed
annotations include ASR, captions, and dense captions. We highlight hard object labels as well as connectives between clips.
Datasets Videos Clips Docs Token Sim.
HowTo100M [34] 1.22M 136.6M - 30 0.22
Howto-Interlink7M 1M 7M 1M 55 0.32
Table 1. Data statistics of Howto-Interlink7M . The last two
columns are average token length and CLIP [39] image-text simi-
larity score for each clip, respectively.
Speech Recognition (ASR). However, these ASR-generated
annotations suffer from weak alignments with the video
content. As observed in Howto100M, only 51% of clips
feature objects or actions mentioned in their captions are
visibly depicted in the sampled clips [34]. This limitation
poses a significant challenge to the vision-language pre-
training community. To address this issue, we introduce
Howto-Interlink7M, a novel interleaved video-text dataset
aiming to provide high-quality annotations for improved
video-language understanding.
Generation Pipeline. To construct Howto-Interlink7M,
we start with the publicly available HowTo100M dataset,
but undertake a distinctive approach. Diverging from the
original dataset, we segment the original videos into shots
using a shot detector, specifically KTS [38]. For each shot,
we employ off-the-shelf caption models like BLIP2 [28]
to generate concise descriptions. Furthermore, we employ
GRIT [54] to produce dense region captions with bound-
ing box, enhancing the captions with rich descriptions and
position details. For simplicity, we name all these infor-
mation as detailed annotation . For first clip of video, we
leverage GPT-4 [35] model to generate comprehensive sum-
maries from detailed annotation only. Importantly, subse-
quent clips’ captions are conditioned on the context of pre-
ceding clips, maintaining narrative continuity. We provide
explicit instructions to GPT-4, emphasizing the preservation
of ASR information to retain the nouns and actions. This
approach strengthens the interconnection between individ-
ual clips, fostering a more coherent caption.Method Language model Vision Model #CE
CosMo-2B OPT-IML1.8B [21] Open-Clip ViT-L/14 [20] 6
CosMo-3.4B RedPajama-3B [47] Open-Clip ViT-L/14 [20] 4
CosMo-8.1B Mistral7B [22] Open-Clip ViT-L/14 [20] 4
Table 2. Architecture details of the CosMo . #CE is short for the
number of Cross Attention layers.
Analysis. In Figure 2, we showcase an example illustrat-
ing the comprehensive and detailed nature of the generated
paragraphs. Notably, our observations indicate the preser-
vation of specific details across different clips, including the
names of actors and mentioned objects like food. Moreover,
we conduct a comparative analysis between the original
HowTo100M [34] dataset and our Howto-Interlink7M, as
presented in Table 1. The generated captions exhibit greater
length and improved alignment with the video content, as
evidenced by the higher CLIP similarity scores.
4. CosMo
In this section, we introduce CosMo, short for COntrastive-
Streamlined MultimOdal framework, which strategically
partitioning a LLM into dedicated unimodal text processing
and adept multimodal data handling components. CosMo
adds an additional contrastive loss to the language model
loss in Flamingo [2, 3] baselines, supporting both clas-
sification and generation tasks. While ALBEF [27] and
CoCa [59] also integrate contrastive loss, our emphasis
lies in developing LLMs specialized in in-context learn-
ing and handling extensive interleaved data sequences, set-
ting our work apart from these methods. Additionally, we
streamline our model architecture, reducing the number of
learnable parameters for improved computational efficiency
while maintaining efficacy in multi-modality learning.
4.1. Overall Architecture
As shown in Figure 3, CosMo consists of two components:
a visual encoder and a pre-trained LLM. The visual encoder,
based on the Vision Transformer (ViT) [13] from Open-
CLIP [20], remains consistent across our experiments. For

--- PAGE 4 ---
Figure 3. An introduction to CosMo : This model handles both
image/video text pairs and inter-level image/video text pairs. The
Large Language Model is divided into two parts to compute con-
trastive loss and language modeling loss.
the language model, we primarily employ the OPT [61]
model, partitioned into two segments to accommodate di-
verse tasks while reducing overall parameters.
Input Representation as Document. CosMo accepts
four types of data: image-text, video-text, interleaved
image-text, and interleaved video-text, all processed
into a document-style text sequence format. This for-
mat encapsulates visual and textual information, struc-
tured as “ <s><Visual 1 >Text1 <EOC><Visual 2 >Text2
<EOC>”, with <s>marking the document start and
<EOC>denoting the end of each text caption. The <Vi-
sual>token represents the presence of an image or video.
To effectively handle longer documents while con-
strained with a fixed GPU memory budget, we implement a
random sampling strategy to gather inputs with a fixed num-
ber of tokens. We first determine the position of the <Visual
>token, and randomly select one image as the anchor. Sub-
sequently, we introduce a slight random shift to the image
position, and then sample 128 tokens subsequently. The
images or videos corresponding to the <Visual >tokens
serves as inputs to the visual encoder.
Uni-modality Feature Extraction. To mitigate catas-
trophic forgetting, inspired by recent works [2, 15], we
freeze both the LLMs and the vision encoder. Images or
videos are directly fed into the frozen vision encoder, while
documents are input into the first half layers of the LLM.
Lightweight Multi-modal Fusion. Then, we leverage vi-
sual tokens to condition the frozen language model block
through gated cross-attention layers, which share a similar
design with Flamingo [2] and CoCa [59]. This strategy
effectively integrates visual information for precise next-
token prediction. However, a key difference from previous
Figure 4. Instances of Low-Similarity Images : In datasets like
MMC4 [63], based on raw website content, there are often incon-
gruent images that don’t align with the accompanying text, leading
to training instability.
methods is that we introduce bottlenecks [18, 46] in input
and output feature channels, resulting in a substantial re-
duction in learnable parameters. Specifically, we compress
the feature channel dimension to one half at the beginning
and raise it again at last. Moreover, cross-attention layers
are strategically introduced at regular intervals, specifically
every 2 blocks for CosMo-2B, and 4 for CosMo-3.4B.
Loss: For document feature xand vision feature z, the
model predicts the next word yand the loss is computed as:
p(y|x) =−i=TX
i=1logp(y|x, z), (1)
where Tis the length of the input text. The final loss func-
tion is composed of both language modeling loss ( Ll) and
contrastive loss ( Lc):
L=i=NX
i=1λ1Ll+λ2Lc, (2)
where Nis the number of data types (3 by default).
To this end, our CosMo is trained to adeptly handle in-
terleaved text and visual sequences, naturally enabling its
application in in-context few-shot learning scenarios.
4.2. Enhancing Image-Text Alignment
The Vision Encoder generally stems from the CLIP [39]
model, meticulously trained with contrastive loss to pre-
serve rich and discriminative information. Preceiver resam-
pler [2] obtains spatio-temporal features from the Vision
Encoder, yielding a fixed number of visual tokens. But the
risk of missing discriminative details persists, potentially
leaving certain information unclear.
Contrarily, our approach extends this process by incor-
porating a learnable query to globally attend to all tokens,
including an additional learnable query for Text Fusion Lay-
ers. This modification allows us to attain a more compre-
hensive understanding of the entire token set. Subsequently,
the model employs a projection head to unify visual and text
embeddings into the same dimensionality. The training ob-
jective centers around optimizing the contrastive loss.

--- PAGE 5 ---
Data Type Dataset Sample
Image-TextCC3M [44] 0.75M
SBU [36] 0.3M
LAION400M [43] 30M
DataComp1B [42] 65.6M
Interlevel Image-Text MMC4 [63] 30M
Video-Text WebVid [4] 2.5M
Interlevel Video-Text Howto-Interlink7M 0.9M
Total - 130M
Table 3. Statistics of the Pre-training Dataset : We extract a sub-
set from the complete dataset using clustering and filtering.
4.3. Interleaved Data Preprocessing
Image-text Similarity Matrix. In the interleaved
MMC4 [63] dataset, each document (typically a website)
contains a text list and an image list extracted from the
document. Also, this dataset provides pairwise image-text
similarity computed using the CLIP [39] model. Due to the
absence of image positions within the document, during
pre-training, Open-Flamingo [3] select the matching index
utilizing Optimal Transport [49] from the similarity matrix.
Data Filtering. The MMC4 dataset comprises numerous
images with low similarity scores, which may include lo-
gos, failed image downloads, or images completely unre-
lated to the accompanying text, as shown in Figure 4. Train-
ing models directly on such data often results in gradient ex-
plosions due to the inherent irrelevance between images and
text. To mitigate this issue, MMC4 employs a simple filter-
ing using a threshold of similarity score 0.24, computed by
CLIP ViT-L/14. However, this approach discards a signifi-
cant number of samples, reducing the dataset’s diversity.
To overcome these limitations, we implement the follow-
ing strategies: (i.) Similarity Distribution : Unlike pre-
vious methods, we introduce a disturbance matrix with a
normal distribution having a standard deviation within the
range (−0.04,0.04)for the alignment score matrix. Fur-
thermore, we clamp the min and max value to -0.08 and
0.08, correspondingly. We then utilize optimal transport
for index matching. This approach allows images to match
with different texts within the same document. (ii.) Pre-
serving Document Context : For images with a similar-
ity score below 0.20, we leverage a pre-trained captioning
model to generate pseudo-captions to replace the matched
texts within the document. This operation significantly di-
versifies the sampled documents and enhances the training
stability. Refer to Section 6.1 for comprehensive analysis.
4.4. Training Details
Pre-training Data: Conventional datasets often suffer from
noise and duplication issues. Following the approach out-
lined in TLDR [50], we opt to sample a refined subset from
Figure 5. LAION400M [43] and similar large datasets
commonly suffer from redundancy. Clustering and uniform
distance-based sampling help alleviate this issue.
commonly used image-text pre-training datasets. Table 3
details the specific datasets sampled for our model train-
ing. Further analysis on our data sampling methodology is
shown in Section 5.1.
Training Configuration: All models are trained using
AdamW [32] optimizer with a cosine learning rate schedule,
initialized at 5e-4, and trained for 20 epochs. The warm-up
ratio is set to 0.03. Gradient accumulation steps align with
the data type, defaulting to 3 for our CosMo. We employ
DeepSpeed [40] fp16 precision for model training, executed
across 128 NVIDIA V100 GPUs.
5. Experiments
Our primary objective is to develop models capable of swift
adaptation to a wide array of diverse and challenging tasks.
To this end, we evaluate our model against numerous well-
established image-text [16, 17, 31, 33, 37, 45] and video-
text [9, 26, 30, 52, 57, 62] benchmarks, to assess the few-
shot in-context learning performance of CosMo.
5.1. Pre-training Data Selection
Our experimental setup draws inspiration from TLDR [50],
where we leverage a subset of the SBU [36], CC3M [44],
CC12M [8], LAION400M [43], and Data-comp 1B [42]
datasets. To obtain this subset, we begin by filtering out half
of the data with low similarity scores. Subsequently, we em-
ploy K-means clustering and uniformly sample data from
each cluster, resulting in a total of 100 million data points.
We compute the centroid of each cluster and uniform sam-
ple data according to the distance from the mcenter. An
illustrative example of this clustering process is shown in
Figure 5. For interleaved data, we utilize 30 million data
points from MMC4 [63] following a filtering process by re-
moving samples with too small images or no images. Also
the data filtering in Section 4.3.

--- PAGE 6 ---
Ablated setting Original Changed Parameter Iter. Captioning (CIDER) VQA Average
Value Value Time COCO FLICKR ok-vqa textvqa vizwiz vqav2
Baseline 683M/2.32G 4.9s 71.3 50.9 18.5 15.0 33.6 38.4 38.0
i. Contrastive
Losswo ContrastiveSingle GPU 683M/2.32G 5.1s 76.0 53.0 20.9 16.7 35.5 41.0 40.5
All GPUS 683M/2.32G 6.1s 73.1 52.0 20.4 14.0 26.4 39.4 39.7
ii. Training
DataAll Dataw/o Video-text 683M/2.32G 4.6s 68.8 51.6 16.2 14.1 30.7 35.3 36.1
w/o Interleaved 683M/2.32G 3.3s 42.1 26.0 9.5 6.1 9.4 27.2 20.1
w/o Image-text 683M/2.32G 4.5s 44.1 26.3 9.6 4.1 6.3 32.7 18.5
iii. Dataloader
SamplingRound-robinMin 683M/2.32G 4.9s 73.2 56.9 22.1 15.8 33.7 42.0 40.6
Max 683M/2.32G 4.9s 73.2 53.2 19.0 16.4 20.0 41.0 37.1
iv. Vision
EncoderViT-LViT-B 617M/1.98G 4.2s 64.7 45.9 19.3 13.5 21.4 39.0 34.0
ViT-G 700M/3.28G 7.5s 72.1 54.1 20.6 19.7 34.0 41.5 40.3
v. Layer Inter 12 462M/2.10G 4.5s 71.4 53.8 22.3 16.4 28.5 42.3 39.1
4 352M/1.99G 3.9s 70.4 51.3 20.8 15.8 30.7 41.8 38.5
vi. Compress
Ratio12 444M/2.08G 4.3s 71.5 54.9 22.1 14.7 31.7 41.1 39.3
4 325M/1.97G 4.2s 72.0 48.0 20.8 12.8 32.2 41.3 37.9
vii. Interleaved
Length64128 683M/2.32G 6.5s 73.1 53.5 22.8 15.7 33.4 40.8 39.9
192 683M/2.32G 9.3s 72.8 54.5 22.5 14.9 33.4 42.0 40.0
Table 4. Ablation Studies of CosMo on an 18M Subset . Our focus is primarily on presenting the 8-shot results for comparison, with the
baseline result positioned in the first row. We highlight the distinction between learnable and all parameters. For the captioning assessment,
we report CIDER, and ’Iter.’ abbreviates Iteration. Text in indigo denotes our default setting.
5.2. Ablation Study
In Table 4, we report our ablation results using our model
pre-trained on a 18M subset (12M Interleaved, 4M Image
and 2M Video-text). The average score is computed by av-
eraging the scores across all datasets.
Significance of Contrastive Loss: As seen in section
(i) of Table 4, the contrastive loss plays a critical role in
the success of our approach. We observe that using the
contrastive loss on both single and multiple GPUs consis-
tently improves model performance over the baseline. Yet,
our further investigations indicate that aggregating all con-
trastive losses across nodes does not provide commensurate
benefits. This can be attributed to the model’s strong fo-
cus on the contrastive loss, which can affect the learning
of language modeling loss. In addition, employing the con-
trastive loss on all GPUs introduces a significant lag in train-
ing speed (from 4.9s to 6.1s), which increases computation
expenses. As a result, by default we use the contrastive loss
over batch on each GPU.
All Data are important: As demonstrated in section (ii)
of Table 4, the selection of appropriate training data is im-
portant to the model performance. Notably, the removal
of the interleaved image-text dataset results in huge perfor-
mance degradation in terms of the average score (from 38.0
to 20.1), and omitting the conventional image-text pairs
similarly affect the performance. Additionally, the exclu-
sion of paired video-text datasets has a detrimental impact
on a majority of downstream tasks.
In light of these findings, we employ gradient accumula-
tion, and the number of accumulation steps is set to matchthe number of data types. This ensures that each iteration
encompasses all data types, allowing us to leverage the full
spectrum of data available for training. We also present var-
ious dataloader sampling strategies in row (iii) of Table 4.
Our findings indicate that the “minimum” strategy outper-
forms both “maximum” and “round-robin.” This under-
scores the importance to ensure balanced quantities across
each type of training data.
Visual Encoder Size: In section (iv) of Table 4, we evalu-
ate the effects of altering the size of the visual encoder. Our
observations reveals a general trend that larger visual en-
coders tend to produce slightly better results. However, this
improvement is offset by a corresponding increase in the
number of model parameters and computational demands.
Notably, the time required per iteration increases from 1.2
seconds to 2 seconds when using larger visual encoders.
Considering this trade-off between performance gains and
computational costs, we have chosen to adopt the larger vi-
sual encoder size as the default configuration.
Lightweighting the Model: In our pursuit of a more
lightweight and efficient network, we focus on reducing the
learnable parameters by minimizing the number of cross-
attention layers and compressing its associated parameters.
The results are presented in sections (v) and (vi). Sur-
prisingly, reducing the number of learnable parameters does
not generally lead to a performance decrease. It even re-
sults in enhanced performance, especially when employing
a layer interval of 2 and a compression ratio of 2, which we
incorporate into our final framework.

--- PAGE 7 ---
Method Samples Para. #Tokens ↓Shots Captioning (CIDER) VQA Classification Avg↑
COCO FLICKR ok-vqa textvqa vizwiz vqav2 hatefulmemes
Flamingo-3B [2] 2.1B 1.4B/3.2B 2560 73.0 60.6 41.2 30.1 28.9 49.2 53.7 48.1
4 85.0 72.0 43.3 32.7 34.0 53.2 53.6 53.4
32 99.0 71.2 45.9 30.6 45.5 57.1 56.3 57.9
Open-
Flamingo(3B) [3]180M 1B/3B 2560 74.9 52.3 28.2 24.2 23.7 44.6 51.2 42.7
4 77.3 57.2 30.3 27.0 27.0 45.8 50.6 45.0
32 93.0 61.1 31.0 28.3 39.8 47.0 50.2 50.0
CosMo-2B 130M 340M/1.9B 1280 79.9 51.3 28.3 21.9 24.7 46.7 51.2 43.4
4 91.7 61.2 27.3 23.4 30.0 45.5 50.6 47.1
32 95.4 64.7 26.8 25.6 42.3 43.9 50.9 49.9
CosMo-
2B+Howto-
Interlink7M131M 340M/1.9B 1280 81.3 52.8 25.5 27.7 27.4 45.3 50.5 44.4
4 97.9 65.1 28.7 25.8 30.3 47.8 51.8 49.6
32 100.1 63.8 25.8 25.9 42.1 44.6 49.8 50.3
Open-
Flamingo(4B) [3]180M 1.3B/4B 2560 76.7 53.6 30.7 21.8 18.8 45.1 52.3 42.7
4 81.8 60.7 35.1 25.9 26.6 49.0 51.5 47.2
32 95.1 56.9 26.4 14.1 23.1 43.0 52.2 44.4
CosMo-3.4B 130M 405M/2.9B 1280 78.7 55.4 32.5 23.4 20.9 46.2 50.6 44.0
4 91.4 63.4 35.3 21.0 28.4 48.8 52.7 48.7
32 101.4 67.4 30.4 20.8 35.4 47.8 51.3 50.6
Flamingo(9B) [2] 2.1B 1.8B/9.3B 2560 79.4 61.5 44.7 31.8 28.8 51.8 57.0 50.7
4 93.1 72.6 49.3 33.6 34.9 56.3 62.7 57.5
32 106.3 72.8 51.0 32.8 44.0 60.4 63.5 61.5
Open-
flamingo(9B) [3]180M 1.9B/9B 2560 74.9 52.3 28.2 24.2 23.7 44.6 51.6 42.8
4 77.3 57.2 30.3 27.0 27.0 45.8 54.0 45.5
32 93.0 61.1 31.0 28.3 39.8 47.0 50.2 50.1
CosMo-8.1B 180M 514M/8.1B 1280 77.5 58.2 32.7 23.5 22.5 47.2 57.1 45.5
4 85.5 63.4 33.7 26.8 30.5 47.7 58.9 50.0
32 103.5 67.7 34.0 28.9 41.3 49.2 62.0 55.2
Table 5. Comparison Across Various Scales . By employing only 128 tokens in pre-training, our computational costs are significantly
lower compared to related approaches. ’ILLength’ abbreviates Interlevel Token Length. It’s noteworthy that CosMo-3.4B and Open-
Flamingo (4B) utilize the same LLM (RedPajama-3B) and visual encoder (Clip ViT-L/14). Rows with color indicate our own method.
Interleaved Length Ablation: In section (vii) of Table 4,
we explore the impact of varying the interleaved sequence
length within the range of 64 to 192. In general, longer se-
quences lead to improved results. However, it’s worth not-
ing that longer input sequences also introduce higher com-
putational demands and can significantly slow down train-
ing due to increased GPU memory consumption.
In contrast to previous works [2, 3], which employ a se-
quence length of 256, we opt for a sequence length of 128
to expedite the training process, within a limited computa-
tion budget. It is important to note that our model has the
potential for further enhancement with the use of longer se-
quences if budget allows.
5.3. Few-shot Evaluation on Vision-Language Tasks
Results on Image-Text Tasks. Table 5 presents a com-
parative analysis with related works, demonstrating our
state-of-the-art performance. Our CosMo outperforms
Open-Flamingo [3] across nearly all benchmarks, and
this is achieved with a substantially reduced sample size
(130M vs. 180M) and fewer parameters. When using the
same RedPajama-3B [47] model as the language model,
our model also outperforms Open-Flamingo substantially.Furthermore, the incorporation of our proposed Howto-
Interlink7M dataset leads to even better results, underscor-
ing the strength of the high-quality data. It is worth not-
ing that, during pre-training, our model is trained with only
three frames, yet it delivers strong results even when evalu-
ated with 32 shots.
6. Training Details
Results on Video-Text Tasks. We test our model on
video captioning and video question-answering tasks.
When using an additional video dataset, we consistently ob-
serve enhanced model performance across various aspects.
However, it is important to note a significant limitation
in the current MSVD and MSRVTT datasets: their ground
truth quality is compromised due to being generated via
rule-based methods rather than human annotation. Conse-
quently, there are instances where our model generates more
detailed and accurate predictions, which may not align with
the ground truth. For example, while the ground truth is
”Carve”, our model predicts ”Carving sculpture”. To ad-
dress this challenge, we propose an alternative approach
for evaluating VQA performance. This involves evaluat-
ing text similarity using a pre-trained Language model from

--- PAGE 8 ---
Method Shots Captioning (CIDER) Open-ended VQA Average
TVC MSVD MSRVTT YouCook2 V ATEX MSRVTT MSVD TGIF
Flamingo-3B [2]0 - - - 55.8 40.1 11.0(-) 27.5(-) - N/A
4 - - - 64.6 50.0 14.9(-) 33.0(-) - N/A
32 - - - 76.7 59.2 25.6(-) 42.6(-) - N/A
CosMo-2B0 6.0 61.4 31.4 17.9 33.5 1.0(18.5) 4.8(32.5) 20.5(41.3) 22.0
4 6.7 64.2 34.2 21.5 37.8 2.3(21.4) 8.7(34.2) 23.2(45.2) 24.8
32 8.2 72.0 37.5 31.5 36.8 8.2(23.3) 11.5(33.4) 28.2(46.6) 32.3
CosMo-2B+Howto-
Interlink7M0 6.7 80.2 33.1 19.2 35.5 5.1(20.3) 15.2(34.3) 21.5(43.3) 27.1
4 8.3 82.7 42.3 28.4 39.9 8.4(22.5) 18.7 (35.2) 24.3(45.8) 31.6
32 12.2 85.3 53.5 33.5 42.3 9.1(24.3) 20.3(35.5) 30.4(49.3) 35.8
Table 6. Video-text Task Comparison . We assess these datasets using open-ended generation. Parentheses indicate evaluation via text
similarity matching using Language Models. Except for YouCook2, we only utilize 3 frames per video.
Figure 6. The clip image-to-text similarity score distribution of
MMC4. Left is all pairs and the right is matched pairs.
NLTK [5]. The results of our evaluations are presented in
the brackets of Table 6. Our findings indicate that, across
a majority of the evaluated downstream tasks, our model
consistently delivers robust performance.
6.1. Analysis on Interleaved Data
Visualization of Interleaved Similarity Scores. We vi-
sualize the distribution of image-text similarity scores. The
majority of similarity scores fall within the range of 0.2 to
0.4. By varying the threshold from 0.18 to 0.30 while main-
taining a dataset subset of 18 million, as consistent with our
ablation setting, we present the results in Table 7.
When the similarity threshold is below 0.22, frequent oc-
currences of gradient explosions hinder the successful train-
ing of our model. This issue predominantly arises from
the disruptive influence of noisy data, significantly impair-
ing training stability. Furthermore, large thresholds (e.g.,
0.28 and 0.30) yield suboptimal results on both COCO and
FLICKR30K, partially due to a substantial dropout of in-
terleaved samples. To address this, we experiment with re-
placing low-similarity captions with those generated from
a pre-trained CosMo, resulting in more stable training. We
have noted that while replacing noisy captions contributes
positively to performance enhancement, the correction of
accurate pairs notably results in a significant decrease.
Analysis on Interleaved Data Sampling. Our empirical
findings underscore that a larger number of shots signifi-
cantly improves outcomes in the experiments conducted onThresh 0.18 0.20 0.24 0.28 0.30
COCO CIDER N/A N/A 72.3 64.7 53.5
FLICKR CIDER N/A N/A 52.6 44.3 36.2
COCO CIDER 65.4 67.2 73.2 63.3 45.7
FLICKR CIDER 42.3 48.4 54.3 42.1 32.2
Table 7. The selection of similarity thresh score of interleaved
affects the result. N/A means the model fail into gradient ex-
plosion. The second line means we replace the noisy data with
generated caption from pretrained CosMo.
Subset 0 4 8 16 32
Random Selection 63.6 66.5 71.3 64.7 60.5
Minimum Frames 65.4 63.2 52.4 43.5 45.2
Maximum Frames 64.1 66.8 73.5 75.5 75.0
Table 8. Data quality highly affects the multiple-shots ability.
interleaved frames. The predominant origin of most sam-
ples lies within raw images. To address potential bias and
preserve the maximum number of frames, we implement a
strategic approach: replacing captions with pre-trained data
for images displaying exceptionally low similarity, account-
ing for approximately 5% of the dataset. This procedural
adjustment yields a marginal yet discernible enhancement
in the model’s overall performance.
Analysis of Interleaved Subsets: In this experiment, we
closely examine three unique subsets derived from the
MMC4 dataset:
• A randomly sampled 4 million subset.
• 4 million samples with the highest count of images.
• 4m samples that containing only a single frame.
To mitigate the potential issue of over-sampling docu-
ments with multiple frames, we limit the model training to
a single epoch. The detailed results, as presented in Table 8,
reveal a significant performance gap, particularly favoring
the subset enriched with the most frames. This notable dis-
parity suggests that the effectiveness in few-shot learning
largely stems from the integration of interleaved data com-

--- PAGE 9 ---
Method VTAB Retrieval Average over 38 Datasets
CosMo-2B 31.5 25.4 32.7
CosMo-3.4B 33.2 26.3 35.8
Table 9. Zero-shot performance for alignment tasks in [42].
bined with the strategic use of LLMs.
6.2. Zero-shot Alignment Tasks
In this experiment, we include zero-shot image classifica-
tion and retrieval task. We utilize the DataComp [42] eval-
uation pipeline to test our model’s capabilities. Specif-
ically, the model’s performance is evaluated across 38
datasetswithout any training. The result is shown in Table 9.
7. Conclusion and Limitations
In this work, we present a refined architecture aimed at in-
corporating contrastive loss into an existing autoregressive
multi-modality model, CosMo, tailored specifically for in-
context learning across multiple modalities. But in-context
learning require high-quality interleaved data and there still
no interleaved video-text dataset available. To address this
gap, we introduce Howto-Interlink7M, a pioneering inter-
leaved video-text dataset featuring comprehensive captions,
marking a significant stride in this field’s advancement.
Furthermore, the potential advantages of employing pre-
training models in more downstream tasks, particularly in
long text tasks, warrant further exploration. Our ongoing
endeavor involves the forthcoming release of our trained
models and the dataset, aiming to foster extensive research
in this domain.
References
[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir
Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Man-
dar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal
masked multimodal model of the internet. arXiv preprint
arXiv:2201.07520 , 2022. 2
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716–23736,
2022. 1, 2, 3, 4, 7, 8
[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-
source framework for training large autoregressive vision-
language models. arXiv preprint arXiv:2308.01390 , 2023.
1, 2, 3, 5, 7
[4] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 1728–1738,
2021. 5, 12[5] Steven Bird, Ewan Klein, and Edward Loper. Natural lan-
guage processing with Python: analyzing text with the natu-
ral language toolkit . ” O’Reilly Media, Inc.”, 2009. 8, 12
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877–1901, 2020. 1
[7] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scal-
ing transformer to 1m tokens and beyond with rmt. arXiv
preprint arXiv:2304.11062 , 2023. 1
[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3558–3568, 2021. 5
[9] David Chen and William B Dolan. Collecting highly paral-
lel data for paraphrase evaluation. In Proceedings of the 49th
annual meeting of the association for computational linguis-
tics: human language technologies , pages 190–200, 2011.
5
[10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:
Universal image-text representation learning. In European
conference on computer vision , pages 104–120. Springer,
2020. 2
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang
Sui. A survey for in-context learning. arXiv preprint
arXiv:2301.00234 , 2022. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 3
[14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-
e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 1
[15] Constantin Eichenberg, Sidney Black, Samuel Weinbach,
Letitia Parcalabescu, and Anette Frank. Magma–multimodal
augmentation of generative models through adapter-based
finetuning. arXiv preprint arXiv:2112.05253 , 2021. 4
[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 6904–6913, 2017. 5
[17] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi
Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
Vizwiz grand challenge: Answering visual questions from

--- PAGE 10 ---
blind people. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3608–3617,
2018. 5
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 4
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 13, 14
[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. If you use this software, please cite it as below.
3
[21] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor
Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu
Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling
language model instruction meta learning through the lens of
generalization. arXiv preprint arXiv:2212.12017 , 2022. 3,
13
[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
Chris Bamford, Devendra Singh Chaplot, Diego de las
Casas, Florian Bressand, Gianna Lengyel, Guillaume Lam-
ple, Lucile Saulnier, et al. Mistral 7b. arXiv preprint
arXiv:2310.06825 , 2023. 3, 12, 13
[23] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-
and-language transformer without convolution or region su-
pervision. In International Conference on Machine Learn-
ing, pages 5583–5594. PMLR, 2021. 2
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision ,
123:32–73, 2017. 2
[25] Hugo Laurenc ¸on, Lucile Saulnier, L ´eo Tronchon, Stas Bek-
man, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-
dharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.
Obelics: An open web-scale filtered dataset of interleaved
image-text documents. In Thirty-seventh Conference on
Neural Information Processing Systems Datasets and Bench-
marks Track , 2023. 2
[26] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr:
A large-scale dataset for video-subtitle moment retrieval. In
Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI
16, pages 447–463. Springer, 2020. 5
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
Align before fuse: Vision and language representation learn-
ing with momentum distillation. Advances in neural infor-
mation processing systems , 34:9694–9705, 2021. 3
[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 2, 3, 14
[29] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei
Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu
Wei, et al. Oscar: Object-semantics aligned pre-training
for vision-language tasks. In Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28,
2020, Proceedings, Part XXX 16 , pages 121–137. Springer,
2020. 2
[30] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault,
Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif:
A new dataset and benchmark on animated gif description.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 4641–4650, 2016. 5, 12
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740–755. Springer, 2014. 2, 5
[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 5
[33] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and
Roozbeh Mottaghi. Ok-vqa: A visual question answering
benchmark requiring external knowledge. In Proceedings
of the IEEE/cvf conference on computer vision and pattern
recognition , pages 3195–3204, 2019. 5
[34] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In Proceedings of the
IEEE/CVF international conference on computer vision ,
pages 2630–2640, 2019. 2, 3
[35] OpenAI. Gpt-4 technical report. 2023. 1, 2, 3
[36] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned pho-
tographs. Advances in neural information processing sys-
tems, 24, 2011. 5, 12, 14
[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2641–2649, 2015. 5
[38] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and
Cordelia Schmid. Category-specific video summarization.
InComputer Vision–ECCV 2014: 13th European Confer-
ence, Zurich, Switzerland, September 6-12, 2014, Proceed-
ings, Part VI 13 , pages 540–555. Springer, 2014. 3
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1, 2, 3, 4, 5
[40] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. Deepspeed: System optimizations enable train-

--- PAGE 11 ---
ing deep learning models with over 100 billion parame-
ters. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining , pages
3505–3506, 2020. 5
[41] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt
Schiele. A dataset for movie description. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 3202–3212, 2015. 12
[42] Alex Fang Samir Yitzhak Gadre, Gabriel Ilharco. Datacomp:
In search of the next generation of multimodal datasets.
arXiv preprint arXiv:2304.14108 , 2023. 2, 5, 9, 12, 14
[43] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
and Kaczmarczyk. Laion-400m: Open dataset of clip-
filtered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114 , 2021. 2, 5, 12, 14
[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556–2565, 2018. 2, 5, 12, 14
[45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
Rohrbach. Towards vqa models that can read. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 8317–8326, 2019. 5
[46] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1–9, 2015.
4
[47] Together.xyz. Releasing 3b and 7b redpajama incite family
of models including base, instruction-tuned and chat models.
https://www. together.xyz/blog/redpajama-models-v1 , 2023.
1, 3, 7, 13
[48] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. Advances in Neural
Information Processing Systems , 34:200–212, 2021. 2
[49] C ´edric Villani. Topics in optimal transportation . American
Mathematical Soc., 2021. 5
[50] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao
Zhang, Stan Weixian Lei, and Mike Zheng Shou. Too large;
data reduction for vision-language pre-training. ICCV , 2023.
5
[51] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. arXiv preprint arXiv:2205.14100 , 2022. 1, 14
[52] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang, and William Yang Wang. Vatex: A large-scale, high-
quality multilingual dataset for video-and-language research.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 4581–4591, 2019. 5
[53] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,
Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, ZiweiLiu, et al. Internvid: A large-scale video-text dataset for
multimodal understanding and generation. arXiv preprint
arXiv:2307.06942 , 2023. 2
[54] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,
Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A gen-
erative region-to-text transformer for object understanding.
arXiv preprint arXiv:2212.00280 , 2022. 3
[55] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Chris-
tian Szegedy. Memorizing transformers. arXiv preprint
arXiv:2203.08913 , 2022. 14
[56] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Chris-
tian Szegedy. Memorizing transformers. arXiv preprint
arXiv:2203.08913 , 2022. 1
[57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5288–5296, 2016. 5
[58] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and
Cordelia Schmid. Vidchapters-7m: Video chapters at scale.
arXiv preprint arXiv:2309.13952 , 2023. 2
[59] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 , 2022. 2, 3, 4
[60] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-
lot: Multimodal neural script knowledge models. Advances
in Neural Information Processing Systems , 34:23634–23651,
2021. 2
[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,
Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-
former language models. arXiv preprint arXiv:2205.01068 ,
2022. 1, 4
[62] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
automatic learning of procedures from web instructional
videos. In Proceedings of the AAAI Conference on Artificial
Intelligence , 2018. 5
[63] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak
Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig
Schmidt, William Yang Wang, and Yejin Choi. Multimodal
c4: An open, billion-scale corpus of images interleaved with
text. arXiv preprint arXiv:2304.06939 , 2023. 2, 4, 5, 12

--- PAGE 12 ---
Appendix
A. Scaling up the Language Model
A.1. Data and Comparison
Our endeavor in this experimental phase aimed at a signif-
icant upscaling of our language model, transitioning from
the previously employed 2B and 3.4B configurations to a
more expansive 8B setting.
To ensure a fair and consistent comparison across mod-
els, we augmented the data sample size within the Data-
Comp subset from 130M to 180M, as outlined in the Data-
Comp [42] dataset. The associated data statistics, presented
comprehensively in Table 10, showcase a substantial aug-
mentation, specifically involving a 60M increase in the sam-
ple count within the Datacomp [42] subset.
Our efforts weren’t solely confined to data augmenta-
tion; we also incorporated a larger Mistral-7B language
model [22] to achieve an overall parameter count of 8.1B.
The comparison between our method and Open-flamingo,
utilizing equivalent-scale pre-training data, consistently
demonstrates the superior performance of our approach
across diverse tasks.
B. Training Methodology Details
B.1. Hyperparameters
In this section, we delineate the crucial training specifics
necessary for replication. The experimentation encom-
passed three variations in model size. Notably, larger mod-
els necessitated smaller batch sizes owing to GPU memory
constraints. Our approach utilized deepspeed zero-stage 2
with fp16, while aligning gradient accumulation steps with
the data type count. Detailed results in Table 12.
B.2. Handling Exceptions
Given the utilization of at least 8 nodes (64 GPUs) in our
training setup, some challenges emerged during multi-node
training. We encountered and addressed various issues:
Skip Last Batch: Random sampling occasionally re-
sulted in unstable batches, leading to markedly high con-
trastive loss and language model instability.
Loss Upper Bound: To counter such occurrences, we
implemented a moment value mechanism to track con-
trastive loss and language modeling loss. If the loss of
batch surpassed the moment value by a certain threshold,
we scaled down the loss, fostering more stable training.
NAN Error Mitigation: Periodically, the model en-
countered NAN errors without the possibility of recovery.
To manage such instances, an automated job restart mecha-
nism utilizing NCCL was employed, ensuring continuity in
training despite potential failure points.Data Type Dataset Sample
Image-TextCC3M [44] 0.75M
SBU [36] 0.3M
LAION400M [43] 30M
DataComp1B [42] 116M
Interlevel Image-Text MMC4 [63] 30M
Video-Text WebVid [4] 2.5M
Interlevel Video-Text Howto-Interlink7M 0.9M
Total - 180M
Table 10. Statistics of the Pre-training Dataset for CosMo-
8.1B : Subsets are from the complete sets using clustering and fil-
tering.
Method TGIF-MC LSMDC-MC
CosMo-2B 47.2 45.3
CosMo-3.4B 50.3 46.6
CosMo-8.1B 53.4 50.2
Table 11. Results on Video Multiple-Choice task on TGIF [30]
action split and LSMDC [41] val set with three frames.
These strategies were pivotal in mitigating challenges in-
herent in multi-node training setups, ensuring a more stable
and reliable training process despite the complexities intro-
duced by distributed computing.
C. Video-based Multiple-choice Tasks
In this experiment, we introduce an additional set of video
tasks. Unlike open-ended video question answering, the
multiple-choice task involves selecting an answer from a
pool of candidates.
Precisely, our evaluation involves TGIF-MC [30] and
LSMDC-MC [41] as benchmark datasets. To assess these
multiple-choice tasks without any fine-tuning, we propose a
similarity-based matching approach. Initially, we generate
potential answers and then measure the maximum similarity
against all candidates. The most similar candidate, identi-
fied using a Language Model from NLTK [5], is selected.
If the index matches the correct index, we mark it as a cor-
rect answer. Our findings indicate that our model demon-
strates improved performance with larger model sizes in
these tasks.
D. Extended Results: Multi-shot Details
In these experiments, we present CosMo’s performance
across varying shot counts, ranging from 0 to 32 shots. For
these ablation studies, our training utilized 64 interleaved
tokens within an 18M subset. For the Validation accuracy,
we report the top1 and top5 accuracy.

--- PAGE 13 ---
CosMo-2B CosMo-3.4B CosMo-8.1B
ModelLanguage Model Backbone OPT-IML-1.8B [21] Redpajama-3B [47] Mistral-7B [22]
Vision Model Backbone openai/clip
-vit-large
-patch14openai/clip
-vit-large
-patch14laion/CLIP-ViT
-H-14-laion2B
-s32B-b79K
Cross-Layer Interval 4 4 4
TrainingSequence Length 128 128 128
Effective Batch Size 6144 3072 1536
Max Training Steps 200K 200K 500K
Weight Decay 0.1 0.1 0.1
Optimize r adamw(0.9, 0.999) adamw(0.9, 0.999) adamw(0.9, 0.999)
Gradient Clipping 1.0 1.0 1.0
Learning RateInitial Max 5e-5 5e-5 3e-5
Decay Schedule Cosine Cosine Cosine
Linear warmup Steps 500 500 500
Table 12. The hyperparameters used in pre-training for three distinct CosMovariations . The learning rate and batch size is smaller
for CosMo-8.1Bsine the GPU memory limitation is 32GB.
Method Val
Acc1Val
Acc5Shots Captioning (CIDER) VQA Classification Retrieval(t2v,v2t) Average
COCO FLICKR ok-
vqatextvqa vizwiz vqav2 hatefulmemes COCO FLICKR (wo Retrieval)
112,
682.52M
/2.32G,
4d2h54m62.229 78.740 60.5 41.1 14.6 17.9 9.1 22.3 48.3 22.8/21.2 50.4/40.5 29.3
4 67.3 49.5 21.6 16.2 20.5 42.0 48.1 - - 33.2
8 72.3 55.3 20.8 16.0 24.7 42.6 48.6 - - 37.6
16 67.8 49.3 15.5 12.4 28.7 32.2 52.5 - - 32.3
32 65.2 38.5 13.9 12.6 31.8 28.0 51.9 - - 31.6
121,
682.52M
/2.32G,
4d1h13m62.329 78.20 61.4 42.2 18.2 19.2 10.8 28.7 52.1 22.0/19.1 47.6/41.5 30.5
4 68.5 51.6 20.8 16.4 23.5 40.6 50.6 - - 36.3
8 73.7 54.3 21.9 16.3 30.3 39.9 54.7 - - 38.8
16 69.0 52.6 15.9 12.6 37.4 30.3 53.0 - - 36.3
32 66.4 47.3 13.8 13.2 39.8 26.3 49.5 - - 34.2
211,
682.52M
/2.32G,
4d1h13m61.01 77.30 66.4 45.7 17.9 19.2 9.5 36.2 49.0 22.2/20.8 48.8/43.8 32.8
4 71.0 52.4 21.5 16.3 22.5 42.0 50.3 - - 34.1
8 76.5 56.3 19.6 16.9 29.4 40.0 48.9 - - 38.2
16 70.6 51.3 15.9 13.0 35.5 33.0 50.1 - - 33.7
32 65.7 39.4 12.9 14.6 40.0 29.1 50.7 - - 32.6
Table 13. Ablation Study on Data Weights : Interleaved data emerges as a more pivotal contributor compared to other data types.
D.1. Data Weight Ablation
This experiment delves into the data weight distribution
among Image-text, Interleaved Image-text, and Video-Text.
Prior to computing all losses with gradient accumulation,
we applied corresponding data type weights. Table 13 illus-
trates that interleaved image-text pairs notably influence
downstream accuracy. Driven by this observations, the data
weight for interleaved data is 2 as default.
D.2. PEFT method vs. Cross Attention
Parameter-efficient fine-tuning methods like LORA [19] are
prevalent in LLM. The inquiry here is whether it is feasibleto introduce LORA for model fine-tuning.
To this end, we introduce the LORA into text decoder.
Specifically, we keep the original cross attention but bring
LORA additionaly. We show the result in Table 14. We find
the PEFT method bring negative gains for cross-attention
layers based method which already introduce a large num-
ber of trainable parameters.
D.3. Learning Rate Ablation
This experiment involved varying the learning rate from 3e-
4 to 3e-6, and the reported results in Table 15 reveal a cru-
cial finding. Extremely small learning rates significantly
impair downstream task performance. For instance, a drop

--- PAGE 14 ---
Method Val
Acc1Val
Acc5Shots Captioning (CIDER) VQA Classification Retrieval(t2v,v2t) Average
COCO FLICKR ok-
vqatextvqa vizwiz vqav2 hatefulmemes COCO FLICKR (wo Retrieval)
Cross-
attention
Layer61.402 77.3420 63.6 42.9 10.9 14.9 6.5 11.5 50.3 22.2/20.1 49.8/40.1 28.6
4 66.5 48.3 20.5 17.1 27.8 41.3 48.7 - - 38.6
8 71.3 52.6 20.9 15.0 33.6 42.3 49.5 - - 40.7
16 64.7 48.2 15.2 12.5 37.6 32.6 50.2 - - 37.3
32 60.5 33.6 13.8 11.5 40.3 26.9 49.7 - - 33.7
LORA [19] 61.33 77.940 69.6 49.0 15.0 17.9 8.4 30.1 51.7 22.3/19.8 49.4/45.2 31.5
4 68.7 52.0 18.2 16.5 19.3 40.0 51.4 - - 33.4
8 73.3 54.7 19.0 15.9 24.6 39.9 49.7 - - 36.8
16 71.9 52.0 16.7 12.4 31.8 33.6 47.3 - - 33.4
32 72.9 43.5 13.4 12.5 34.1 31.2 48.8 - - 33.3
Table 14. LORA Ablation Study : The introduction of LORA does not significantly enhance the cross-attention within the interleaved
framework.
Method Val
Acc1Val
Acc5Shots Captioning (CIDER) VQA Classification Retrieval(t2v,v2t) Average
COCO FLICKR ok-
vqatextvqa vizwiz vqav2 hatefulmemes COCO FLICKR (wo Retrieval)
3e-4
682.52M
/2.32G,
3d7h12m61.402 77.3420 63.6 42.9 10.9 14.9 6.5 11.5 50.3 22.2/20.1 49.8/40.1 28.6
4 66.5 48.3 20.5 17.1 27.8 41.3 48.7 - - 38.6
8 71.3 52.6 20.9 15.0 33.6 42.3 49.5 - - 40.7
16 64.7 48.2 15.2 12.5 37.6 32.6 50.2 - - 37.3
32 60.5 33.6 13.8 11.5 40.3 26.9 49.7 - - 33.7
3e-5,
682.52M
/2.32G,
3d7h12m55.78 72.960 37.8 21.6 21.8 12.3 8.4 42.2 48.0 1.6/1.8 2.7/4.1 24.0
4 42.4 22.0 16.0 7.1 30.0 37.8 49.2 - - 25.9
8 25.0 16.7 14.7 5.5 35.8 32.4 49.1 - - 21.9
16 12.4 9.9 1.9 3.0 40.8 24.8 51.5 - - 15.5
32 10.7 8.5 6.9 3.5 42.4 24.1 52.2 - - 16.0
3e-6,
682.52M
/2.32G,
3d12h29m49.70 69.700 3.6 2.5 20.6 5.9 6.7 37.9 48.7 3.5/3.4 6.7/8.3 12.9
4 4.6 3.5 14.2 4.4 30.0 34.7 45.3 - - 15.2
8 6.7 5.3 14.3 4.8 35.7 34.9 47.5 - - 17.1
16 8.0 6.4 13.0 4.7 40.8 32.7 49.1 - - 17.8
32 8.0 5.7 5.3 3.3 42.4 27.8 50.0 - - 15.4
Table 15. Learning rate ablation .
from 40.7 to 21.9 accuracy on 8 shots was observed.
As default, we use 5e-4 for CosMo-2Band CosMo-3.4B.
For CosMo-8.1B, we use learning rate 3e-4.
D.4. Learning Rate Schedule
Experimenting with four distinct learning rate sched-
ules—Cosine, Constant, Cosine-w-restart, and Inverse
Sqrt—unveiled substantial variations in final results, as
demonstrated in Table 16. Generally, the Cosine schedule
yields the best results across most cases, thus adopted as the
default scheduler due to its consistent performance.
E. Others
E.1. Data Selection Strategy for Image-Text
The CC3M [44], SBU [36], LAION400M [43], and Data-
Comp1B [42] datasets are widely accessible and commonly
used as pre-training data. We include these datasets to fa-
cilitate easy replication. However, our observations suggest
that while these image-text datasets are valuable, their im-pact on downstream performance is not as significant as that
of interlevel image-text datasets.
To illustrate, utilizing the entire 130M data solely from
DataComp [42] resulted in minimal changes in perfor-
mance. Furthermore, unlike previous methods like GiT [51]
and BLip2 [28], we opted to exclude COCO and Visual
Genome datasets during pre-training due to potential over-
laps with downstream data. This strategic exclusion was
implemented to minimize potential redundancy and maxi-
mize the distinctiveness of the learned representations.
E.2. Exploring Longer Sequences with Memory
Bank
Managing long text input data holds significant impor-
tance in NLP. A recent approach, the Memorizing Trans-
former [55], introduces a memory bank to store segment to-
kens from the entire document. This technique enables the
model to handle lengthy texts, surpassing even 10K tokens.
Following the principles of the Memorizing Trans-
former, our objective was to augment the interleaved data

--- PAGE 15 ---
Method Val
Acc1Val
Acc5Shots Captioning (CIDER) VQA Classification Retrieval(t2v,v2t) Average
COCO FLICKR ok-
vqatextvqa vizwiz vqav2 hatefulmemes COCO FLICKR (wo Retrieval)
Cosine
682.52M
/2.32G,
3d7h12m61.402 77.3420 63.6 42.9 10.9 14.9 6.5 11.5 50.3 22.2/20.1 49.8/40.1 28.6
4 66.5 48.3 20.5 17.1 27.8 41.3 48.7 - - 38.6
8 71.3 52.6 20.9 15.0 33.6 42.3 49.5 - - 40.7
16 64.7 48.2 15.2 12.5 37.6 32.6 50.2 - - 37.3
32 60.5 33.6 13.8 11.5 40.3 26.9 49.7 - - 33.7
Constant(1e-4),
682.52M
/2.32G,
2d21h40m57.00 73.950 35.5 30.3 22.2 14.6 9.3 42.9 46.4 3.4/3.8 8.2/9.0 26.0
4 52.3 52.3 19.9 10.0 23.8 41.2 50.2 - - 33.3
8 46.4 46.4 17.6 7.8 30.2 38.7 47.8 - - 31.1
16 38.6 38.6 9.1 5.7 32.6 35.2 52.9 - - 26.6
32 24.2 24.2 6.3 4.2 35.3 24.8 48.6 - - 19.8
Cosine-w-
restart,
682.52M
/2.32G,
3d12h29m57.31 74.5770 36.1 34.3 19.0 15.2 6.9 30.3 52.0 12.9/11.8 34.0/26.5 23.6
4 58.1 42.1 19.0 14.7 20.2 42.7 51.0 - - 32.8
8 51.7 38.9 11.8 10.7 26.8 34.2 52.8 - - 29.0
16 50.1 28.1 5.1 7.2 30.4 34.1 49.9 - - 25.8
32 21.5 4.5 2.7 3.7 36.2 25.3 50.5 - - 15.7
INVERSE
SQRT,
682.52M
/2.32G,
3d12h29m54.26 73.170 28.3 15.7 14.6 6.6 4.8 25.5 50.9 1.8/2.2 4.4/4.7 15.9
4 24.8 11.0 8.3 3.7 25.5 30.8 54.5 - - 17.3
8 15.7 11.0 5.1 2.7 28.4 26.0 48.5 - - 14.8
16 12.0 10.6 1.7 1.6 29.1 24.8 50.8 - - 13.5
32 12.6 8.6 1.0 1.7 23.9 23.3 46.4 - - 11.9
Table 16. Learning rate schedule ablation .
Method Val
Acc1Val
Acc5Shots Captioning (CIDER) VQA Classification Retrieval(t2v,v2t) Average
COCO FLICKR ok-
vqatextvqa vizwiz vqav2 hatefulmemes COCO FLICKR (wo Retrieval)
No Memory,
682.52M
/2.32G,
2d13h16m60.85 78.040 65.6 46.2 15.8 18.3 7.9 34.0 47.7 23.0/20.0 51.5/43.3 33.5
4 69.9 53.7 16.0 14.1 11.6 39.0 48.9 - - 36.3
8 74.4 56.2 18.5 15.3 16.4 42.0 44.6 - - 38.4
16 70.9 53.6 14.4 10.3 20.6 31.6 48.6 - - 35.8
32 70.0 45.5 11.9 12.1 27.6 29.2 47.9 - - 34.9
Retrieval Mem-
ory,
682.52M
/2.32G,
3d7h12m63.012 79.3190 66.3 45.8 17.0 16.3 12.2 25.3 49.3 26.4/24.3 55.5/46.4 33.2
4 68.3 54.2 15.3 13.2 14.4 35.0 40.8 - - 35.8
8 72.3 55.6 19.3 13.3 17.8 40.3 49.3 - - 36.5
16 73.4 52.4 10.3 13.4 18.2 35.3 50.3 - - 36.2
32 70.7 47.3 9.9 14.3 30.6 31.2 49.8 - - 36.2
Table 17. Memory bank ablation .
length. The results, detailed in Table 17, yielded intriguing
observations:
i.Improved Validation Results: Notably, the valida-
tion accuracy exhibited significant enhancement, e.g., from
63.012 to 60.85. ii.Increased Computational Cost: Intro-
ducing longer sequences incurred substantially higher com-
putational demands. iii. Limited Downstream Perfor-
mance Gain: Surprisingly, the performance gains on down-
stream tasks were relatively restricted and occasionally even
negative.
Consequently, based on these findings, the Memorizing
Transformer approach was not included in our final model
configuration. However, this doesn’t preclude the consid-
eration of alternative methods to bolster the model’s ability
to process longer texts. There’s potential to explore differ-
ent approaches aimed at enhancing the model’s handling of
extended sequences.F. Training Progression
In this section, we explore the training curves for three
scales of CosMo. Our findings reveal that larger Language
Large Models (LLMs) show a reduced Language Model
(LM) loss, particularly in processing interlevel image-text
data, which underscores their proficiency in word predic-
tion tasks.
However, a notable aspect was the slower convergence
in contrastive loss for CosMocompared to larger models
like CosMo-2Band CosMo-3.4B, attributed to its smaller
batch size. Additionally, hardware constraints were evident
during training CosMo-8.1Bon 32GB Tesla V100 GPUs,
where the maximum batch size of 1536 impacted the con-
vergence rate of contrastive loss.
These observations highlight the intricate balance be-
tween model size, batch size, and hardware limitations in
the efficient training of LLMs.

--- PAGE 16 ---
Figure 7. Generally, Larger Language Models exhibit lower Language Modeling (LM) loss . However, the convergence of contrastive
loss tends to be slower owing to the smaller batch size.
