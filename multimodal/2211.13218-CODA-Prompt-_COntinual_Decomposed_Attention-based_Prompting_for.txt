# 2211.13218.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2211.13218.pdf
# File size: 743410 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CODA-Prompt: COntinual Decomposed Attention-based Prompting for
Rehearsal-Free Continual Learning
James Seale Smith*1,2Leonid Karlinsky2,4Vyshnavi Gutta1Paola Cascante-Bonilla2,3
Donghyun Kim2,4Assaf Arbelle4Rameswar Panda2,4Rogerio Feris2,4Zsolt Kira1
1Georgia Institute of Technology2MIT-IBM Watson AI Lab3Rice University4IBM Research
Abstract
Computer vision models suffer from a phenomenon
known as catastrophic forgetting when learning novel con-
cepts from continuously shifting training data. Typical solu-
tions for this continual learning problem require extensive
rehearsal of previously seen data, which increases memory
costs and may violate data privacy. Recently, the emer-
gence of large-scale pre-trained vision transformer mod-
els has enabled prompting approaches as an alternative
to data-rehearsal. These approaches rely on a key-query
mechanism to generate prompts and have been found to
be highly resistant to catastrophic forgetting in the well-
established rehearsal-free continual learning setting. How-
ever, the key mechanism of these methods is not trained
end-to-end with the task sequence. Our experiments show
that this leads to a reduction in their plasticity, hence sac-
rificing new task accuracy, and inability to benefit from ex-
panded parameter capacity. We instead propose to learn a
set of prompt components which are assembled with input-
conditioned weights to produce input-conditioned prompts,
resulting in a novel attention-based end-to-end key-query
scheme. Our experiments show that we outperform the cur-
rent SOTA method DualPrompt on established benchmarks
by as much as 4.5% in average final accuracy. We also
outperform the state of art by as much as 4.4% accuracy
on a continual learning benchmark which contains both
class-incremental and domain-incremental task shifts, cor-
responding to many practical settings. Our code is avail-
able at https://github.com/GT-RIPL/CODA-Prompt
1. Introduction
For a computer vision model to succeed in the real world,
it must overcome brittle assumptions that the concepts it
will encounter after deployment will match those learned
a-priori during training. The real world is indeed dynamic
and contains continuously emerging objects and categories.
*Work done partially during internship at MIT-IBM Watson AI Lab.
ùö∫Query
valueprompt
key
component weightAttentionŒ±1
Œ±2
Œ±3
Œ±Mk1
k2
k3
kMv1
vMv2
v3
c1
cMc2
c3√ó
promptModel
Model
‚Ä¶
‚Ä¶‚Ä¶‚Ä¶Prior Work
Our Workgradient flow 
from model
Querylearned in separate 
optimization without 
model gradientsseparate 
optimizationFigure 1. Prior work [66] learns a pool of key-value pairs to select
learnable prompts which are inserted into several layers of a pre-
trained ViT (the prompting parameters are unique to each layer).
Our work introduces a decomposed prompt which consists of
learnable prompt components that assemble to produce attention-
conditioned prompts. Unlike prior work, ours is optimized in an
end-to-end fashion (denoted with thick, green lines).
Once deployed, models will encounter a range of differ-
ences from their training sets, requiring us to continuously
update them to avoid stale and decaying performance.
One way to update a model is to collect additional train-
ing data, combine this new training data with the old train-
ing data, and then retrain the model from scratch. While
this will guarantee high performance, it is not practical for
large-scale applications which may require lengthy training
times and aggressive replacement timelines, such as models
for self-driving cars or social media platforms. This could
incur high financial [26] and environmental [34] costs if the
replacement is frequent. We could instead update the model
by only training on the new training data, but this leads to
a phenomenon known as catastrophic forgetting [47] where
the model overwrites existing knowledge when learning the
new data, a problem known as continual learning .
1arXiv:2211.13218v2  [cs.CV]  30 Mar 2023

--- PAGE 2 ---
The most effective approaches proposed by the continual
learning community involve saving [51] or generating [56]
a subset of past training data and mixing it with future task
data, a strategy referred to as rehearsal . Yet many impor-
tant applications are unable to store this data because they
work with private user data that cannot be stored long term.
In this paper, we instead consider the highly-impactful
and well-established setting of rehearsal-free continual
learning [39, 57, 58, 66, 67]1, limiting our scope to strate-
gies for continual learning which do not store training
data. While rehearsal-free approaches have classically
under-performed rehearsal-based approaches in this chal-
lenging setting by wide-margins [58], recent prompt-based
approaches [66, 67], leveraging pre-trained Vision trans-
formers (ViT), have had tremendous success and can even
outperform state-of-the-art (SOTA) rehearsal-based meth-
ods. These prompting approaches boast a strong protection
against catastrophic forgetting by learning a small pool of
insert-able model embeddings (prompts) rather than mod-
ifying vision encoder parameters directly. One drawback
of these approaches, however, is that they cannot be opti-
mized in an end-to-end fashion as they use a key and query
to select a prompt index from the pool of prompts, and thus
rely on a second, localized optimization to learn the keys
because the model gradient cannot backpropagate through
the key/query index selection. Furthermore, these meth-
ods reduce forgetting by sacrificing new task accuracy (i.e.,
they lack sufficient plasticity ). We show that expanding the
prompt size does not increase the plasticity, motivating us
to grow learning capacity from a new perspective.
We replace the prompt pool with a decomposed prompt
that consists of a weighted sum of learnable prompt com-
ponents (Figure 1). This decomposition enables higher
prompting capacity by expanding in a new dimension (the
number of components). Furthermore, this inherently en-
courages knowledge re-use, as future task prompts will in-
clude contributions from prior task components. We also
introduce a novel attention-based component-weighting
scheme, which allows for our entire method to be optimized
in an end-to-end fashion unlike the existing works, increas-
ing our plasticity to better learn future tasks. We boast sig-
nificant performance gains on existing rehearsal-free con-
tinual learning benchmarks w.r.t. SOTA prompting base-
lines in addition to a challenging dual-shift benchmark con-
taining both incremental concept shifts and covariate do-
main shifts, highlighting our method‚Äôs impact and gener-
ality. Our method improves results even under equivalent
parameter counts, but importantly can scale performance
by increasing capacity, unlike prior methods which quickly
1We focus on continual learning over a single, expanding classification
head called class-incremental continual learning . This is different from the
multi-task continual learning setting, known as task-incremental continual
learning, where we learn separate classification heads for each task and the
task label is provided during inference. [25, 62]plateau. In summary, we make the following contributions:
1. We introduce a decomposed attention-based prompt-
ing for rehearsal-free continual learning, characterized
by an expanded learning capacity compared to exist-
ing continual prompting approaches. Importantly, our
approach can be optimized in an end-to-end fashion,
unlike the prior SOTA.
2. We establish a new SOTA for rehearsal-free continual
learning on the well-established ImageNet-R [21, 66]
and CIFAR-100 [33] benchmarks, beating the previous
SOTA method DualPrompt [66] by as much as 4.5%.
3. We evaluate on a challenging benchmark with dual
distribution shifts (semantic and covariate) using the
ImageNet-R dataset, and again outperform the state of
art, highlighting the real-world impact and generality
of our approach.
2. Background and Related Work
Continual Learning : Continual learning approaches can
be organized into a few broad categories which are all
useful depending on the problem setting and constraints.
One group of approaches expands a model‚Äôs architecture
as new tasks are encountered; these are highly effective for
applications where a model growing with tasks is practi-
cal [16,36,40,44,55]. Another approach is to regularize the
model with respect to past task knowledge while training
the new task. This can either be done by regularizing the
model in the weight space (i.e., penalize changes to model
parameters) [2, 15, 31, 59, 72] or the prediction space (i.e.,
penalize changes to model predictions) [1, 8, 23, 35, 39].
Regularizing knowledge in the prediction space is done us-
ingknowledge distillation [22] and it has been found to
perform better than model regularization-based methods for
continual learning when task labels are not given [37, 61].
Rehearsal with stored data [3‚Äì5, 7, 9, 10, 18, 19, 24, 29,
42, 51‚Äì53, 64, 68] or samples from a generative model [27,
28, 48, 56, 60] is highly effective when storing training data
or training/saving a generative model is possible. Unfortu-
nately, for many machine learning applications long-term
storage of training data will violate data privacy, as well as
incur a large memory cost. With respect to the generative
model, this training process is much more computationally
and memory intensive compared to a classification model
and additionally may violate data legality concerns because
using a generative model increases the chance of memo-
rizing potentially sensitive data [46]. This motivates us to
work on the important setting of rehearsal-free approaches
to mitigate catastrophic forgetting.
Rehearsal-Free Continual Learning : Recent works pro-
pose producing images for rehearsal using deep-model in-
version [12, 17, 57, 69]. While these methods perform well
2

--- PAGE 3 ---
compared to generative modeling approaches and simply
rehearse from a small number of stored images, model-
inversion is a slow process associated with high compu-
tational costs in the continual learning setting, and fur-
thermore these methods under-perform rehearsal-based ap-
proaches by significant margins [57]. Other works have
looked at rehearsal-free continual learning from an online
‚Äústreaming‚Äù learning perspective using a frozen, pre-trained
model [20, 41]. Because we allow our models to train to
convergence on task data (as is common for continual learn-
ing [68]), our setting is very different.
Continual Learning in Vision Transformers : Recent
work [6, 38, 70] has proven transformers to generalize well
to unseen domains. For example, one study varies the num-
ber of attention heads in Vision transformers and conclude
that ViTs offer more robustness to forgetting with respect to
CNN-based equivalents [45]. Another [71] shows that the
vanilla counterparts of ViTs are more prone to forgetting
when trained from scratch. Finally, DyTox [14] learns a
unified model with a parameter-isolation approach and dy-
namically expands the tokens processed by the last layer to
mitigate forgetting. For each task, they learn a new task-
specific token per head using task-attention based decoder
blocks. However, the above works either rely on exemplars
to defy forgetting or they need to train a new transformer
model from scratch, a costly operation, hence differing from
our objective of exemplar-free CL using pre-trained ViTs.
Prompting for Continual Learning : Prompt-based ap-
proaches for continual learning boast a strong protection
against catastrophic forgetting by learning a small num-
ber of insert-able model instructions (prompts) rather than
modifying encoder parameters directly. The current state-
of-the-art approaches for our setting, DualPrompt [66] and
L2P [67], create a pool of prompts to be selected for inser-
tion into the model, matching input data to prompts without
task id with a local clustering-like optimization. We build
upon the foundation of these methods, as discussed later in
this paper. We note that the recent S-Prompts [65] method
is similar to these approaches but designed for domain-
incremental learning (i.e., learning the same set of classes
under covariate distribution task shifts), which is different
than the class-incremental continual learning setting of our
paper (i.e., learn emerging object classes in new tasks).
3. Preliminaries
3.1. Continual Learning
In our continual learning setting, a model is sequentially
shown Ntasks corresponding to non-overlapping subsets
of semantic object classes. Each class appears in only a sin-
gle task, and the goal is to incrementally learn to classify
new object classes as they are introduced while retaining
performance on previously learned classes. To describe ourmodels, we denote Œ∏as a pre-trained vision encoder and œïn
as our classifier head with logits corresponding to task n
classes. In this paper, we deal with the class-incremental
continual learning setting rather than the task-incremental
continual learning setting. Class-incremental continual
learning is challenging because the learner must support
classification across all classes seen up to task n[25] (i.e.,
no task labels are provided to the learner during inference ).
Task-incremental continual learning is a simpler multi-task
setting where the task labels are given during both training
and inference.
We also include experiments that contain both class-
incremental and domain-incremental continual learning
at the same time . The difference between this setting and
the former is that we also inject covariate distribution
shifts (e.g., task 1 might include real images and clip-art,
whereas task 2 might include corrupted photos and artistic
paintings). The motivation is to make continual learning
more practical - as in the real world we might encounter
these dual types of domain shifts as well.
3.2. Prompting with Prefix-Tuning
In our work, we do not change the technical founda-
tions of prompting from the prior state-of-the-art Dual-
Prompt [66]. We instead focus on the selection and for-
mation of the prompt (including enabling an end-to-end op-
timization of all prompting components), and the resulting
prompt is used by the vision transformer encoder in an iden-
tical fashion to DualPrompt. This allows us to make a fair
comparison with respect to our novel contributions.
As done in DualPrompt, we pass prompts to several
multi-head self-attention (MSA) layers in a pre-trained ViT
transformer [13, 63] and use prefix-tuning over prompt-
tuning2, which prepends prompts to the keys and values of
an MSA layer rather than prepending prompts to the input
tokens. The layers in which prompting occurs is set as a hy-
perparameter, and the prompting parameters are unique be-
tween layers. We define a prompt parameter as p‚ààRLp√óD
where Lpis the prompt length (chosen as a hyperparame-
ter) and Dis the embedding dimension ( 768). Consider an
MSA layer with input h‚ààRL√óD, and query, key, and val-
ues given as hQ,hK,hV, respectively. In the ViT model,
hQ=hK=hV=h. The output of this layer is given as:
MSA( hQ,hK,hV) = Concat (h 1, . . . , hm)WO
where hi= Attention
hQWQ
i,hKWK
i,hVWV
i(1)
where WO,WQ
i,WK
i, and WV
iare projection matrices
andmis the number of heads. We split our prompt pinto
{pK,pV} ‚ààRLp
2√óDand prepend them to hKandhVwith
2We refer the reader to sections 4.2 and 5.4 of the the DualPrompt [66]
paper for a discussion on this design choice.
3

--- PAGE 4 ---
prefix-tuning (P-T) as:
fP‚àíT(p,h) = MSA( hQ,[pK;hK],[pV;hV]) (2)
The result is that we now only train a small number of pa-
rameter (the prompts) while leaving the rest of the trans-
former encoder unmodified. The critical question that now
remains is how to select and update prompts in a continuous
fashion? The next subsection will discuss how prior works
select and update prompts.
3.3. L2P and DualPrompt
L2P [67] and DualPrompt [66] select prompts from a
pool using an image-wise prompt query. Specifically, these
methods use a key-value pair based query strategy3to dy-
namically select instance-specific prompts from a pool of
candidate prompts. Each prompt pmis selected from a
pool of learnable keys km‚ààRDwhere Mis the size of
the prompt pool, based on cosine similarity to an input-
conditioned query. Queries are produced as: q(x)‚ààRD=
Œ∏(x)where Œ∏is the pretrained ViT encoder and xis the
input image4. A query q(x)is matched to a key kmby se-
lecting the key with the highest cosine similarity , denoted
as:Œ≥(q(x),km). The keys are learned with a separate op-
timization from the task classification loss by simply max-
imizing the cosine similarity between each matched q(x)
andkm, acting as a clustering-like approach. However, this
approach does not directly update the key with gradients
from the task loss, which is critical in our experiments.
4. Method
4.1. Prompt Formation
While prompting has been shown to perform excep-
tionally well in terms of mitigating catastrophic forgetting,
the existing state-of-the-art approach DualPrompt lacks the
ability to expand learning capacity within a given task.
Specifically, DualPrompt learns a single prompt for each
new task - regardless if the task is easy (such as learning a
handful of new object classes) or complex (such as learn-
ing a large number of new object classes). One might try to
increase the length of the prompt, but we show in our exper-
iments (Section 5.3) that increasing the prompt length has
saturated returns. Intuitively, we instead desire a method
where the learning capacity is correlated to the underlying
complexity of task data, rather than a single prompt. Addi-
tionally, we desire an approach that is end-to-end differen-
tiable, which we conjecture increases the ability to learn a
new task with higher accuracy.
3We note that the difference between L2P and DualPrompt w.r.t.
prompt querying is that the size of the prompt pool in L2P is chosen as a
hyperparameter, but in DualPrompt it is equal to the number of tasks and,
during training, DualPrompt selects the prompt using task-id (and selects
the closest key-query match during inference).
4The output is read from the class token.We grow our learning capacity by introducing a new
axis: a set of prompt components . Rather than pick and
choose prompts from a pool, we learn a set of prompt com-
ponents which, via a weighted summation, form a decom-
posed5prompt that is passed to the corresponding MSA
layer. This allows us to grow our prompting capacity to
arbitrary depth and capture the rich underlying complexity
of any task while maintaining a fixed prompt length. In
addition, prompting in new tasks will inherently reuse the
previously acquired knowledge of past tasks rather than ini-
tializing a new task prompt from scratch.
Specifically, we replace learnable prompt parameter p
with a weighted summation over the prompt components:
p=X
mŒ±mPm (3)
where P‚ààRLp√óD√óMis our set of prompt components, M
is the length of our set (i.e., the introduced axis of additional
capacity), and Œ±is a weighting vector that determines the
contribution of each component which will be discussed in
the next subsection.
A distinct difference between our method and
L2P/DualPrompt is that all of our learnable parame-
ters are optimized using the classification loss rather than
splitting into two separate loss optimizations. This allows
for better end-to-end learning in our method, and we argue
this is a key reason why our method performs better in the
benchmarks described in Section 5.
4.2. Prompt-Component Weighting
Rather than a pool of prompts, we have a set of prompt
components and desire to produce a weight vector Œ±given
a query Œ∏(x), rather than a prompt index. We compute the
cosine similarity between a query and keys to produce our
weighting vector as:
Œ±=Œ≥(q(x),K)
={Œ≥(q(x),K1), Œ≥(q(x),K2), . . . , Œ≥ (q(x),KM)}(4)
where K‚ààRD√óMcontains keys corresponding to our
prompt components. The intuition here is that the contribut-
ing magnitude of each prompt component Pmto the final
prompt pis determined by the similarity between the query
q(x)and corresponding key Km.
The prompt-query matching can be thought of as cluster-
ing in a high dimension space Dwhich is a well-known and
difficult problem [32]. To mitigate this drawback, we intro-
duce another component to our key-query matching: atten-
tion. Each Pmhas a corresponding attention vector Am
in addition to the key Km. This allows the the query to
focus on certain features from the high dimensional query
5Decomposed in the sense that our prompt is a weighted summation of
components.
4

--- PAGE 5 ---
Classifier
Cosine Similarity ‚äôquery attended
querycomponent
weightingpromptMatmulPre-Trained Transformer
Prompt Components
expansion
Prompt Keys
expansion
Attention 
expansion
optimizeQuery 
Function‚Ä¶ prompts inserted into layers
Multi -LayerEq. (3) Eq. (5) Eq. (5)image input
Figure 2. Our CODA-Prompt approach to rehearsal-free continual learning . An input (e.g., image) is passed through a query function
(we use the pretrained ViT encoder) and used for a novel attention-based prompt-selection scheme to produce prompts which are passed
to multiple layers of a large-scale, pre-trained transformer. Our method is parameterized by an expanding set of small prompt components,
with each prompt having a corresponding key and attention vector. Both the input and produced prompts are passed through the transformer
and sent to the task head. Only the prompting and task parameters are learned which is parameter efficient and no training data is stored for
replay which is memory-efficient and privacy preserving. Importantly, our prompt selection scheme can be optimized end-to-end, unlike
prior works which have a local similarity-based optimization for key-query matching.
q(x)output - for example, a prompt component used for
mammalian face features to classify dogs and cats would be
able to focus on relevant query features such as eye place-
ment and skin texture, and furthermore ignore features of
unrelated semantics, such as features useful for automo-
bile classification. We use a simple feature-selection at-
tention scheme with element-wise multiplication between
the query vector and attention vector to create an attended-
query which is then used for the key-similarity matching.
Specifically, our updated approach to producing our weight-
ing vector is:
Œ±=Œ≥(q(x)‚äôA,K)
={Œ≥(q(x)‚äôA1,K1), . . . , Œ≥ (q(x)‚äôAM,KM)}(5)
where A‚ààRD√óMcontains learnable parameters (atten-
tion vectors) corresponding to our prompt components and
‚äôis the element-wise product (also known as the Hadamard
product). Notice that our attention vectors are learnable fea-
ture weightings rather than input-conditioned modules - we
found that this fixed representation was less vulnerable to
forgetting by its simple design, similar to the our prompt
component keys.
4.3. Expansion & Orthogonality
Crucial to mitigating catastrophic forgetting is the avoid-
ance of overwriting knowledge acquired in prior tasks.
When we visit a new task we freeze our current compo-
nents and expand the set, only updating the new compo-
nents. This is visualized in the bottom of Figure 2 where
the existing parameters are locked and only the expanded
parameters are optimized. Specifically, in task twe learn
M
Ncomponents where Nis the number of tasks and Mis achosen hyperparameter, keeping the previous(t‚àí1)¬∑M
Ncom-
ponents frozen . This expansion is enabled by our attention-
based component-weighting scheme, in that expanding our
parameters do notalter the computation of weights Œ±cor-
responding previously learned components. For exam-
ple, expanding the capacity of a nonlinear model like an
MLP module, transformer attention head [13], or hyper-
network [64] would alter the weight outputs of previously
learned components.
While the prior heuristic is focused on reducing catas-
trophic forgetting (not destroying previously acquired
knowledge), we also want to reduce interference between
existing and new knowledge. To do this, we add an orthog-
onality constraint to P,K, andA. The intuition is that
orthogonal vectors have less interference with each other.
For example, we would want a key and prompt learned in
task 2 to not attract task 1 data, otherwise the encoder output
for task 1 data will change, resulting in misclassified data.
We use an orthogonal initialization and introduce a simple
orthogonality penalty loss to our method as:
Lortho(B) =||BB‚ä§‚àíI||2 (6)
where Brepresents any arbitrary matrix.
4.4. Full Optimization
Given task classification loss L, our full optimization is:
min
Pn,Kn,An,œïnL(fœï(fŒ∏,P,K,A(x)), y) +
Œª(Lortho (P) +Lortho (K) +Lortho (A))(7)
where Pn,Kn,Anrefer to the prompt components and
corresponding keys/attention vectors that are unfrozen and
5

--- PAGE 6 ---
trained during task n(see Section 4.3) and Œªis a hyperpa-
rameter balancing the orthogonality loss. Note that we do
not update the logits of past task classes, consistent with
L2P and DualPrompt. We formally refer to our method
asCOntinual Decomposed Attention-based Prompting, or
simply CODA-P .
5. Experiments
We benchmark our approach with several image datasets
in the class-incremental continual learning setting. We im-
plemented baselines which do not store training data for re-
hearsal: Learning without Forgetting (LwF) [39], Learning
to Prompt (L2P) [67], and DualPrompt [66]. Additionally,
we report the upper bound performance (i.e., trained offline)
and performance for a neural network trained only on classi-
fication loss using the new task training data (we refer to this
as FT), and include an improved version of FT which uses
the same classifier implementation6as L2P/DualPrompt
(referred to as FT++). We also compare to Experience Re-
play [11] to provide additional context to our results. We
implement our method and all baselines in PyTorch [49] us-
ing the ViT-B/16 backbone [13] pre-trained on ImageNet-
1K [54]. Our contribution includes faithful PyTorch im-
plementations of the popular prompting baselines L2P and
DualPrompt, which were released in JAX [66, 67]. Our im-
plementations of the competing methods actually achieve
slight boosts in the performance of DualPrompt in most
benchmarks, as well as significant performance boosts in
L2P due to the improved prompting type7(which we refer
to as L2P++).
DualPrompt uses length 5 prompts in layers 1-2 (referred
to as general prompts) and length 20 prompts in layers 3-5
(referred to as task-expert ) prompts. We insert prompts into
the same layers as DualPrompt (layers 1-5) for CODA-P
and use a prompt length of 8 and 100 prompt components,
which were chosen with a hyperparameter sweep on vali-
dation data to have the best trade-off between performance
and parameter efficiency. Because our approach introduces
more learnable parameters compared to DualPrompt, we
include a variant of out method CODA-P-S which uses a
smaller number of parameters equal to DualPrompt for ad-
ditional comparison. We show that our method outperforms
other methods even under this condition, while still retain-
ing the capability to scale if desired.
We report results on the test dataset, but emphasize that
all hyperparameters and design decisions (including for the
baselines) were made using validation data (20% of the
training data). Unlike DualPrompt, we run our benchmarks
for several different shuffles of the task class order and re-
6See Appendix B for additional details
7As discussed in Section 3.2, we use pre-fix tuning over prompt-tuning
for all implementations as is shown to work best for continual learn-
ing [66].port the mean and standard deviation of these runs. We do
this with a consistent seed (different for each trial) so that
results can be directly compared. This is crucial because the
results with different shuffling order can be different due to
differences in task difficulty and their appearance order. We
include additional implementation details and results in Ap-
pendix A and B .
Evaluation Metrics: We evaluate methods using (1) aver-
age final accuracy AN, or the final accuracy with respect to
all past classes averaged over Ntasks, and (2) average for-
getting [9, 42, 43] FN, or the drop in task performance av-
eraged over Ntasks. The reader is referred to Appendix B
for additional details about these metrics. We emphasize
thatANis the more important metric and encompasses both
method plasticity andforgetting, whereas FNsimply pro-
vides additional context.
5.1. CODA-P sets the SOTA on existing benchmarks
We first evaluate our method and state of art on sev-
eral well-established benchmarks. Table 1 provides results
for ImageNet-R [21, 66] which is composed of 200 object
classes with a wide collection of image styles, including
cartoon, graffiti, and hard examples from the original Im-
ageNet dataset [54]. This benchmark is attractive because
the distribution of training data has significant distance to
the pre-training data (ImageNet), thus providing a fair and
challenging problem setting. In addition to the original
10-task benchmark, we also provide results with a smaller
number of large tasks (5-task) and a larger number of small
tasks (20 task). We first notice that our reported perfor-
mance for DualPrompt is a few percentage points higher
compared to the original DualPrompt [66] paper. We re-
implemented the method from scratch in a different frame-
work (PyTorch [49]) and suspect the difference has to do
with our averaging over different class-order shuffling. We
note that our implementation of L2P (L2P++) performs sig-
nificantly better than originally reported because we use the
same form of prompting as DualPrompt (for fair compari-
son). Furthermore, our ‚ÄúDeep L2P ++‚Äù, which prompts at
the same 5 layers as DualPrompt (rather than only at layer
1), actually performs similarly to DualPrompt.
Importantly, we see that our method has strong gains
in average accuracy across all three task lengths, with
as much as 4.5% improvement in average accuracy
over DualPrompt . We remind the reader that CODA-P
is our proposed method with tuned prompt length and
component set size, whereas CODA-P-S is downsized (see
Appendix A for exact details) to exactly match the number
of learnable parameters as DualPrompt. We notice that
our method often has slightly higher average forgetting
compared to DualPrompt. Given that average accuracy is
the crucial metric that captures practical performance, we
are not worried by the slight uptick in forgetting. In fact,
6

--- PAGE 7 ---
Table 1. Results (%) on ImageNet-R . Results are included for 5 tasks (40 classes per task), 10 tasks (20 classes per task), and 20 tasks
(10 classes per task). ANgives the accuracy averaged over tasks and FNgives the average forgetting. We report results over 5 trials.
Tasks 5 10 20
Method AN(‚Üë) FN(‚Üì) AN(‚Üë) FN(‚Üì) AN(‚Üë) FN(‚Üì)
UB 77.13 - 77.13 - 77.13 -
ER (5000) 71.72¬±0.71 13 .70¬±0.26 64 .43¬±1.16 10 .30¬±0.05 52 .43¬±0.87 7 .70¬±0.13
FT 18.74¬±0.44 41 .49¬±0.52 10 .12¬±0.51 25 .69¬±0.23 4 .75¬±0.40 16 .34¬±0.19
FT++ 60.42¬±0.87 14 .66¬±0.24 48 .93¬±1.15 9 .81¬±0.31 35 .98¬±1.38 6 .63¬±0.11
LwF.MC 74.56¬±0.59 4 .98¬±0.37 66 .73¬±1.25 3 .52¬±0.39 54 .05¬±2.66 2 .86¬±0.26
L2P++ 70.83¬±0.58 3 .36¬±0.18 69 .29¬±0.73 2 .03¬±0.19 65 .89¬±1.30 1 .24¬±0.14
Deep L2P++ 73.93¬±0.37 2 .69¬±0.10 71 .66¬±0.64 1 .78¬±0.16 68 .42¬±1.20 1 .12¬±0.13
DualPrompt 73.05¬±0.50 2.64¬±0.17 71.32¬±0.62 1 .71¬±0.24 67 .87¬±1.39 1 .07¬±0.14
CODA-P-S 75.19¬±0.47 2 .65¬±0.15 73 .93¬±0.49 1 .60¬±0.20 70 .53¬±1.24 1 .00¬±0.15
CODA-P 76.51¬±0.38 2.99¬±0.19 75.45¬±0.56 1 .64¬±0.10 72 .37¬±1.19 0 .96¬±0.15
Table 2. Results (%) on CIFAR-100 and DomainNet .ANgives the accuracy averaged over tasks and FNgives the average forgetting.
We report results over 5 trials for CIFAR-100 and 3 trials for DomainNet (due to dataset size).
(a) 10-task CIFAR-100 (10 classes per task)
Method AN(‚Üë) FN(‚Üì)
Upper-Bound 89.30 -
ER (5000) 76.20¬±1.04 8 .50¬±0.37
FT 9.92¬±0.27 29 .21¬±0.18
FT++ 49.91¬±0.42 12 .30¬±0.23
LwF.MC 64.83¬±1.03 5 .27¬±0.39
L2P++ 82.50¬±1.10 1 .75¬±0.42
Deep L2P++ 84.30¬±1.03 1.53¬±0.40
DualPrompt 83.05¬±1.16 1 .72¬±0.40
CODA-P-S 84.59¬±0.87 1 .76¬±0.28
CODA-P 86.25¬±0.74 1.67¬±0.26(b) 5-task DomainNet (69 classes per task)
Method AN(‚Üë) FN(‚Üì)
Upper-Bound 79.65 -
ER (5000) 58.32¬±0.47 26 .25¬±0.24
FT 18.00¬±0.26 43 .55¬±0.27
FT++ 39.28¬±0.21 44 .39¬±0.31
LwF.MC 74.78¬±0.43 5.01¬±0.14
L2P++ 69.58¬±0.39 2 .25¬±0.08
Deep L2P++ 70.54¬±0.51 2 .05¬±0.07
DualPrompt 70.73¬±0.49 2.03¬±0.22
CODA-P-S 71.80¬±0.57 2 .54¬±0.10
CODA-P 73.24¬±0.59 3 .46¬±0.09
we argue it is very reasonable and reflects the strength of
our approach: our method has a larger capacity to learn
new tasks, and thus we can sacrifice marginally higher
forgetting. Crucially, we see that as the task sequence
length grows, the forgetting metric of our method versus
DualPrompt begin to converge to a similar value.
We provide results for experience replay to provide ad-
ditional context of our results. While the gap between re-
hearsal with a substantial coreset size (i.e., the buffer size
of replay data) and our approach is small for the short task
sequence, we see that our method strongly outperforms re-
hearsal in the longer task sequence.
Tables 2a and 2b provide results on additional8
benchmarks ten-task CIFAR-100 [33] and five-task Do-
mainNet [50]. While neither dataset is as impactful as
ImageNet-R in terms of challenge and distance from the
pre-trained distribution, these tables provide additional
8Note that the hyperparameters for both DualPrompt and CODA-P
were tuned on ImageNet-R, which was intentionally done to evaluate ro-
bustness of all methods.context to our method‚Äôs performance. These tables illus-
trate a similar story to the ImageNet-R benchmarks, with
improvements of +3.2% and +2.5%, respectively. We
do note that, surprisingly, LwF [39] slightly outperforms
prompting on this benchmark.
5.2. CODA-P sets the SOTA for a new dual-shift
benchmark
We also evaluate on a challenging dual-shift benchmark
using the ImageNet-R dataset. Our motivation is to show
robustness to two different types of continual distribution
shifts: semantic andcovariate. We accomplish this by ran-
domly selecting image types from the ImageNet-R dataset
to include in each task‚Äôs training data (while keeping the
evaluation data unmodified). The reader is refereed to Ap-
pendix C for more details. Results for this benchmark are
provided in Table 3. Compared to 5-task ImageNet-R where
our method improves SOTA by 4.5% average accuracy, we
see a 4.4% improvement on this 5-task benchmark with sim-
ilar forgetting margins. Our results indicate that CODA-P
7

--- PAGE 8 ---
Table 3. Results (%) on ImageNet-R with covariate domain
shifts . Results are included for 5 tasks (40 classes per task). We
simulate domain shifts by randomly removing 50% of the dataset‚Äôs
domains (e.g., clipart, paintings, and cartoon) for the training data
of each task (see SM for more details). ANgives the accuracy av-
eraged over tasks and FNgives the average forgetting. We report
results over 5 trials.
Method AN(‚Üë) FN(‚Üì)
Upper-Bound 77.13 2 .66
ER (5000) 67.39¬±0.37 11 .94¬±0.17
FT 17.93¬±0.27 37 .49¬±0.28
FT++ 54.51¬±0.68 14 .41¬±0.33
LwF.MC 64.02¬±1.55 7 .05¬±0.27
L2P++ 65.08¬±0.29 2 .79¬±0.32
Deep L2P++ 65.74¬±0.12 2 .48¬±0.30
DualPrompt 66.98¬±0.08 2.21¬±0.28
CODA-P-S 69.73¬±0.18 2 .35¬±0.19
CODA-P 71.35¬±0.08 2.56¬±0.26
Table 4. Ablation Results (%) on 10-task ImageNet-R .AN
gives the accuracy averaged over tasks and FNgives the average
forgetting. We report results over 5 trials.
Method AN(‚Üë) FN(‚Üì)
CODA-P 75.45¬±0.56 1 .64¬±0.10
Ablate Attention 74.52¬±0.65 1 .67¬±0.13
Ablate Freezing 74.60¬±0.64 2 .29¬±0.10
Ablate Orthogonality 70.66¬±0.60 1 .74¬±0.25
better generalizes to real-world type shifts.
5.3. Ablations and Additional Analysis
We take a closer look at our method with ablation stud-
ies and additional analysis. In Table 4 we show the effect of
removing a few key components of our method: attention
keys, freezing of past task components, and our orthogonal-
ity regularization. We show that removing attention slightly
reduces forgetting degrades performance on average accu-
racy (the more important metric). This makes sense be-
cause removing the attention keys makes our query process-
ing more aligned with the L2P/DualPrompt methods which
boast low forgetting but lack sufficient learning capacity.
We see larger drops when removing freezing and orthogo-
nality. This indicates that these aspects are crucial to our
approach. Our intuition is is that, without these, our prompt
formation is similar to a shallow MLP module which, un-
regularized, should suffer from high forgetting.
We also look at our ability to grow in model capacity
along our newly introduced prompt component dimension
using the 5-task ImageNet-R benchmark (with validation
data). We show in Figure 3 that, with a number of prompt
components equal to the number of prompts learned in
Figure 3. Analysis of average accuracy ANvs prompt com-
ponents (or pool size) for CODA-P, L2P, and DualPrompt. The
setting is 5 task ImageNet-R, and we report the mean over 3 trials.
Figure 4. Analysis of average accuracy ANvs prompt length
for CODA-P, L2P, and DualPrompt. The setting is 5 task
ImageNet-R, and we report the mean over 3 trials.
DualPrompt, we achieve higher performance. Importantly,
when we increase the number of components, we are able
to leverage the scaled parameters for significantly higher
performance, finding that our method is closer to upper
bound performance than it is to DualPrompt in average
accuracy. Because the prompt pool in L2P can be increased
to arbitrary size as well, we include results in this analysis
as well. However, we show that the L2P method peaks with
a pool size equal to twice the task sequence length, and
then drops in performance. This reflects that our prompt
components benefit from scale, whereas the existing prompt
pool actually suffer.
Finally, we show average accuracy versus prompt length
in Figure 4 using the same setting as above. The purpose
of this experiment is to emphasize that accuracy saturates
with prompt length , thus motivating the need to expand in
our introduced component dimension. Additional analysis
and hyperparameter sweeps is available in Appendix B.
6. Conclusion
We present COntinual decomposed attention-based
prompting (CODA-Prompt ) for rehearsal-free continual
learning . Our approach assembles learnable prompt compo-
nents which are then inserted into a pre-trained ViT encoder
for image classification. Importantly, CODA-Prompt is op-
timized in an end-to-end fashion (unlike prior SOTA meth-
8

--- PAGE 9 ---
ods which involve two, separate optimizations). Further-
more, CODA-Prompt can scale prompting capacity to ar-
bitrary sizes . We set a new SOTA on both well-established
benchmarks and a dual-distribution shift benchmark (con-
taining semantic and covariate shifts), highlighting the
potential real world impact and generality of our approach.
Acknowledgements
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 2239292.
References
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang,
Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax
for incremental learning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
844‚Äì853, October 2021. 2, 12
[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In ECCV , 2018.
2
[3] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. In Advances in Neural Information Processing Sys-
tems, pages 11849‚Äì11860, 2019. 2
[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. In Advances in Neural Information Processing Sys-
tems, pages 11816‚Äì11825, 2019. 2
[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,
and Jonghyun Choi. Rainbow memory: Continual learn-
ing with a memory of diverse samples. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8218‚Äì8227, 2021. 2
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877‚Äì1901, 2020. 3
[7] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for gen-
eral continual learning: a strong, simple baseline. Advances
in neural information processing systems , 33:15920‚Äì15930,
2020. 2
[8] Francisco M Castro, Manuel J Mar ¬¥ƒ±n-Jim ¬¥enez, Nicol ¬¥as Guil,
Cordelia Schmid, and Karteek Alahari. End-to-end incre-
mental learning. In Proceedings of the European Conference
on Computer Vision (ECCV) , pages 233‚Äì248, 2018. 2
[9] Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with a-
GEM. In International Conference on Learning Representa-
tions , 2019. 2, 6, 12
[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HSTorr, and Marc‚ÄôAurelio Ranzato. Continual learning with
tiny episodic memories. arXiv preprint arXiv:1902.10486 ,
2019. 2
[11] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marc‚ÄôAurelio Ranzato. On tiny episodic memo-
ries in continual learning. arXiv preprint arXiv:1902.10486 ,
2019. 6
[12] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Dual-
teacher class-incremental learning with data-free genera-
tive replay. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3543‚Äì
3552, 2021. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 3, 5, 6
[14] Arthur Douillard, Alexandre Ram ¬¥e, Guillaume Couairon,
and Matthieu Cord. Dytox: Transformers for continual
learning with dynamic token expansion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9285‚Äì9295, 2022. 3
[15] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and
Marcus Rohrbach. Uncertainty-guided continual learn-
ing with bayesian neural networks. arXiv preprint
arXiv:1906.02425 , 2019. 2
[16] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor
Darrell, and Marcus Rohrbach. Adversarial continual learn-
ing. arXiv preprint arXiv:2003.09553 , 2020. 2
[17] Qiankun Gao, Chen Zhao, Bernard Ghanem, and Jian
Zhang. R-dfcil: Relation-guided representation learning
for data-free class incremental learning. arXiv preprint
arXiv:2203.13104 , 2022. 2
[18] Alexander Gepperth and Cem Karaoguz. Incremental learn-
ing with self-organizing maps. 2017 12th International
Workshop on Self-Organizing Maps and Learning Vector
Quantization, Clustering and Data Visualization (WSOM) ,
pages 1‚Äì8, 2017. 2
[19] Tyler L Hayes, Nathan D Cahill, and Christopher Kanan.
Memory efficient experience replay for streaming learning.
In2019 International Conference on Robotics and Automa-
tion (ICRA) , pages 9769‚Äì9776. IEEE, 2019. 2
[20] Tyler L Hayes and Christopher Kanan. Lifelong machine
learning with deep streaming linear discriminant analysis.
arXiv preprint arXiv:1909.01520 , 2019. 3
[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-
vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,
and Justin Gilmer. The many faces of robustness: A critical
analysis of out-of-distribution generalization. ICCV , 2021.
2, 6, 12
[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 2
[23] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Lifelong learning via progressive distillation and
9

--- PAGE 10 ---
retrospection. In Proceedings of the European Conference
on Computer Vision (ECCV) , pages 437‚Äì452, 2018. 2
[24] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via re-
balancing. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 831‚Äì839, 2019.
2
[25] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and
Zsolt Kira. Re-evaluating continual learning scenarios: A
categorization and case for strong baselines. arXiv preprint
arXiv:1810.12488 , 2018. 2, 3
[26] Daniel Justus, John Brennan, Stephen Bonner, and An-
drew Stephen McGough. Predicting the computational cost
of deep learning models. In 2018 IEEE international confer-
ence on big data (Big Data) , pages 3873‚Äì3882. IEEE, 2018.
1
[27] Nitin Kamra, Umang Gupta, and Yan Liu. Deep generative
dual memory network for continual learning. arXiv preprint
arXiv:1710.10368 , 2017. 2
[28] Ronald Kemker and Christopher Kanan. Fearnet: Brain-
inspired model for incremental learning. International Con-
ference on Learning Representations (ICLR) , 2018. 2
[29] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler
Hayes, and Christopher Kanan. Measuring catastrophic for-
getting in neural networks. AAAI Conference on Artificial
Intelligence , 2018. 2
[30] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 12
[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 2017. 2
[32] Mario K ¬®oppen. The curse of dimensionality. In 5th online
world conference on soft computing in industrial applica-
tions (WSC5) , volume 1, pages 4‚Äì8, 2000. 4
[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Tech Report , 2009. 2, 7,
12
[34] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt,
and Thomas Dandres. Quantifying the carbon emissions of
machine learning. arXiv preprint arXiv:1910.09700 , 2019.
1
[35] Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee.
Overcoming catastrophic forgetting with unlabeled data in
the wild. In Proceedings of the IEEE International Confer-
ence on Computer Vision , pages 312‚Äì321, 2019. 2
[36] Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim.
A neural dirichlet process mixture model for task-free con-
tinual learning. arXiv preprint arXiv:2001.00689 , 2020. 2
[37] Timoth ¬¥ee Lesort, Hugo Caselles-Dupr ¬¥e, Michael Garcia-
Ortiz, Andrei Stoian, and David Filliat. Generative models
from the perspective of continual learning. In 2019 Interna-
tional Joint Conference on Neural Networks (IJCNN) , pages
1‚Äì8. IEEE, 2019. 2[38] Duo Li, Guimei Cao, Yunlu Xu, Zhanzhan Cheng, and Yi
Niu. Technical report for iccv 2021 challenge sslad-track3b:
Transformers are better continual learners. arXiv preprint
arXiv:2201.04924 , 2022. 3
[39] Zhizhong Li and Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelli-
gence , 40(12):2935‚Äì2947, 2017. 2, 6, 7
[40] Vincenzo Lomonaco and Davide Maltoni. Core50: a new
dataset and benchmark for continuous object recognition.
arXiv preprint arXiv:1705.03550 , 2017. 2
[41] Vincenzo Lomonaco, Davide Maltoni, and Lorenzo Pelle-
grini. Rehearsal-free continual learning over small non-iid
batches. In CVPR Workshops , pages 989‚Äì998, 2020. 3
[42] David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient
episodic memory for continual learning. In Proceedings
of the 31st International Conference on Neural Informa-
tion Processing Systems , NIPS‚Äô17, pages 6470‚Äì6479, USA,
2017. Curran Associates Inc. 2, 6, 12
[43] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyun-
woo Kim, and Scott Sanner. Online continual learning in
image classification: An empirical survey. Neurocomputing ,
469:28‚Äì51, 2022. 6, 12
[44] Davide Maltoni and Vincenzo Lomonaco. Continuous learn-
ing in single-incremental-task scenarios. Neural Networks ,
116:56‚Äì73, 2019. 2
[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Tim-
othy Nguyen, Razvan Pascanu, Dilan Gorur, and Mehrdad
Farajtabar. Architecture matters in continual learning. arXiv
preprint arXiv:2202.00275 , 2022. 3
[46] Vaishnavh Nagarajan, Colin Raffel, and Ian J Goodfellow.
Theoretical insights into memorization in gans. In Neural
Information Processing Systems Workshop , 2018. 2
[47] Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal
Hassner, Vijay Mahadevan, and Stefano Soatto. Toward
understanding catastrophic forgetting in continual learning.
arXiv preprint arXiv:1908.01091 , 2019. 1
[48] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jah-
nichen, and Moin Nabi. Learning to remember: A synaptic
plasticity driven framework for continual learning. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 11321‚Äì11329, 2019. 2
[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in neural information processing systems , 32, 2019.
6
[50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, and Bo Wang. Moment matching for multi-source
domain adaptation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1406‚Äì1415,
2019. 7, 12
[51] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H. Lampert. icarl: Incremental classi-
fier and representation learning. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition , CVPR‚Äô17, pages
5533‚Äì5542, 2017. 2
10

--- PAGE 11 ---
[52] Anthony Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science , 7(2):123‚Äì146, 1995.
2
[53] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. In Advances in Neural Information Processing Sys-
tems, pages 348‚Äì358, 2019. 2
[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115(3):211‚Äì252, 2015. 6
[55] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671 , 2016. 2
[56] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In I. Guyon,
U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 30 , pages 2990‚Äì2999. Curran
Associates, Inc., 2017. 2
[57] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen,
Hongxia Jin, and Zsolt Kira. Always be dreaming: A new ap-
proach for data-free class-incremental learning. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 9374‚Äì9384, October 2021. 2,
3
[58] James Seale Smith, Junjiao Tian, Yen-Chang Hsu, and Zsolt
Kira. A closer look at rehearsal-free continual learning.
arXiv preprint arXiv:2203.17269 , 2022. 2
[59] Michalis K Titsias, Jonathan Schwarz, Alexander G de G
Matthews, Razvan Pascanu, and Yee Whye Teh. Functional
regularisation for continual learning with gaussian processes.
InInternational Conference on Learning Representations ,
2019. 2
[60] Gido M van de Ven, Hava T Siegelmann, and Andreas S To-
lias. Brain-inspired replay for continual learning with arti-
ficial neural networks. Nature communications , 11(1):1‚Äì14,
2020. 2
[61] Gido M van de Ven and Andreas S Tolias. Generative replay
with feedback connections as a general strategy for continual
learning. arXiv preprint arXiv:1809.10635 , 2018. 2
[62] Gido M Van de Ven and Andreas S Tolias. Three scenar-
ios for continual learning. arXiv preprint arXiv:1904.07734 ,
2019. 2
[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[64] Johannes von Oswald, Christian Henning, Jo Àúao Sacramento,
and Benjamin F Grewe. Continual learning with hypernet-
works. arXiv preprint arXiv:1906.00695 , 2019. 2, 5
[65] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts
learning with pre-trained transformers: An occam‚Äôs razor for
domain incremental learning. In Conference on Neural In-
formation Processing Systems (NeurIPS) , 2022. 3[66] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-
cent Perot, Jennifer Dy, et al. Dualprompt: Complemen-
tary prompting for rehearsal-free continual learning. arXiv
preprint arXiv:2204.04799 , 2022. 1, 2, 3, 4, 6, 12
[67] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, and Tomas Pfister. Learning to prompt for continual
learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 139‚Äì149,
2022. 2, 3, 4, 6, 12
[68] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
374‚Äì382, 2019. 2, 3, 12
[69] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong
Li, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.
Dreaming to distill: Data-free knowledge transfer via deep-
inversion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 8715‚Äì
8724, 2020. 2
[70] Wenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking
zero-shot text classification: Datasets, evaluation and entail-
ment approach. arXiv preprint arXiv:1909.00161 , 2019. 3
[71] Pei Yu, Yinpeng Chen, Ying Jin, and Zicheng Liu. Improving
vision transformers for incremental learning. arXiv preprint
arXiv:2112.06103 , 2021. 3
[72] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In International
Conference on Machine Learning , 2017. 2
11

--- PAGE 12 ---
Appendix
A. Additional Implementation Details
For all methods, we use the Adam [30] optimizer with
Œ≤1=0.9 and Œ≤1=0.999, and a batch size of 128 im-
ages. We resize all images to 224x224 and normalize
them to [0,1]. We train CIFAR-100 and DomainNet for
20 epochs, and ImageNet-R for 50 epochs (chosen to en-
sure models converge fully for each task). As discussed in
the main text, we use the same prompting lengths and lo-
cations for L2P [67] and DualPrompt [66] as recommended
by the more recent DualPrompt paper. Specifically, for Du-
alprompt, we use a length 5 prompt in layers 1-2 (referred
to as general prompts) and length 20 prompts in layers 3-
5 (referred to as task-expert ). For L2P, we use a prompt
pool of size 20, total prompt length of size 20, and choose 5
prompts from the pool to use during inference.
As done in DualPrompt [66], we tuned all addi-
tional hyperparameters using 20% of the training data
as validation data. This resulted in using a learning
rate of 1e‚àí3for all prompting methods (as opposed
to5e‚àí3as reported in DualPrompt), and a learning
rate of 1e‚àí4for all methods which fully fine-tune the
model. We searched for learning rates in the values of
{1e‚àí6,5e‚àí6,1e‚àí5,5e‚àí5,1e‚àí4,5e‚àí4,1e‚àí3,5e‚àí3,1e‚àí2}.
We also found that cosine-decaying learning rate outper-
forms a constant learning rate (which was used in the
original DualPrompt implementation). We conjecture
that the reduced and decaying learning rate explain the
performance boost we obtained on the 10-task ImageNet-R
benchmark using our implementations.
For our method, we use a prompt length of 8 and 100
prompt components (and prompt at the same locations as
DualPrompt), which were chosen with a hyperparameter
sweep on validation data to have the best trade-off between
performance and parameter efficiency. We searched for
prompt lengths in the range of [4,40] and prompt compo-
nents in the range of [5,500]. We use Œª=0.1 to weight
the orthogonality regularization loss, chosen from sweep-
ing across decade values from 1e‚àí6up to 1e2. As shown
in Section 5.3 of our main text, we see that increasing the
prompt length has little effect on our method, whereas in-
creasing the prompt component size has strong returns all
the way up to 200 components.
Finally, when implementing classification loss for fine-
tuning, L2P, DualPrompt, and CODA-P, we re-use a tech-
nique from the official GitHub repo for the DualPrompt
and L2P papers [66, 67] and replace the predictions from
past-task logits with negative infinity when training a new
task. This results in a softmax prediction of ‚Äú0‚Äù for these
past task classes and prevents gradients from flowing to the
linear heads of past task classes. While not discussed in
these papers, this technique is crucial for performance ofthese methods, as we confirmed during reproduction. Es-
sentially, the linear layer is highly biased towards new tasks
in class-incremental learning in the absence of rehearsal, so
this technique prevents the linear head from learning a bias
towards new classes over past classes. We note that this bias
is a well-known issue [1, 68].
B. Additional Results
We report extended results, including standard devi-
ations and additional parameters trained, for all bench-
marks in Tables A (5-task ImageNet-R [21,66]), B (10-task
ImageNet-R), C (20-task Imagenet-R), D (10-task CIFAR-
100 [33]), E (5-task DomainNet [50]), and A (Dual-Shift
ImageNet-R).
We evaluate methods using (1) final average accuracy
AN, or the final model‚Äôs test accuracy averaged over all N
tasks, and (2) average forgetting [9, 42, 43] FN, or the drop
in task performance averaged over Ntasks. The reader is
referred to Appendix C of Wang et al. [66] for the formal
metric definitions. We emphasize that ANis the more im-
portant metric and encompasses both method plasticity and
forgetting, whereas FNprovides additional context subject
to the model‚Äôs plasticity (i.e., a lower FNvalue anda lower
ANvalue would indicate that the model‚Äôs lower forgetting
results from its reduced adaptivity to new tasks, which is an
undesirable trait).
For each result, we calculate the mean and standard devi-
ation over separate runs. Each run contains different shuf-
fles of the class order; specifically, we shuffle the classes
using a random seed that is set for each ‚Äútrial run‚Äù - and
form the tasks using this class shuffle. Importantly, the class
order and all randomized seeds, including model initializa-
tion, are consistent between different methods in the same
‚Äútrial run‚Äù.
We additionally report the number of parameters trained
(i.e., unlocked during training a task) as well as the total
number of parameters in the final model. These are re-
ported in % of the backbone model for easy comparison.
Importantly, we design CODA-P-S to have fewer param-
eters than DualPrompt in the 10-task ImageNet-R setting
(our main experiment setting). As we change the number
of tasks in ImageNet-R, the number of total parameters for
DualPrompt changes because the pool size is set as equal to
the number of total tasks by definition (unlike ours, which
is set as a hyper-parameter, allowing us to increase or de-
crease the number of trainable parameters to accommodate
the underlying complexity of the task.)
C. ImageNet-R Dual-Shift Benchmark
Our motivation for the challenging dual-shift ImageNet-
R [21, 66] benchmark is to show robustness to two dif-
ferent types of continual distribution shifts: semantic and
12

--- PAGE 13 ---
Table A. Results (%) on 5-task ImageNet-R (40 classes per task) .ANgives the accuracy averaged over tasks, FNgives the average
forgetting, and Nparam gives the % of trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean
and standard deviation over 5 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 77.13 - 100/100
ER (5000) 71.72¬±0.71 13 .70¬±0.26 100 /100
FT 18.74¬±0.44 41 .49¬±0.52 100 /100
FT++ 60.42¬±0.87 14 .66¬±0.24 100 /100
LwF.MC 74.56¬±0.59 4 .98¬±0.37 100 /100
L2P++ 70.83¬±0.58 3 .36¬±0.18 0 .7/100.7
Deep L2P++ 73.93¬±0.37 2 .69¬±0.10 9 .6/109.6
DualPrompt 73.05¬±0.50 2.64¬±0.17 0.5/100.5
CODA-P-S 75.19¬±0.47 2 .65¬±0.15 0 .7/100.7
CODA-P 76.51¬±0.38 2.99¬±0.19 4 .6/104.6
Table B. Results (%) on 10-task ImageNet-R (20 classes per task) .ANgives the accuracy averaged over tasks, FNgives the average
forgetting, and Nparam gives the % of trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean
and standard deviation over 5 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 77.13 - 100/100
ER (5000) 64.43¬±1.16 10 .30¬±0.05 100 /100
FT 10.12¬±0.51 25 .69¬±0.23 100 /100
FT++ 48.93¬±1.15 9 .81¬±0.31 100 /100
LwF.MC 66.73¬±1.25 3 .52¬±0.39 100 /100
L2P++ 69.29¬±0.73 2 .03¬±0.19 0 .7/100.7
Deep L2P++ 71.66¬±0.64 1 .78¬±0.16 9 .6/109.6
DualPrompt 71.32¬±0.62 1 .71¬±0.24 0 .8/100.8
CODA-P-S 73.93¬±0.49 1.60¬±0.20 0.7/100.7
CODA-P 75.45¬±0.56 1.64¬±0.10 4 .6/104.6
Table C. Results (%) on 20-task ImageNet-R (10 classes per task) .ANgives the accuracy averaged over tasks, FNgives the average
forgetting, and Nparam gives the % of trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean
and standard deviation over 5 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 77.13 - 100/100
ER (5000) 52.43¬±0.87 7 .70¬±0.13 100 /100
FT 4.75¬±0.40 16 .34¬±0.19 100 /100
FT++ 35.98¬±1.38 6 .63¬±0.11 100 /100
LwF.MC 54.05¬±2.66 2 .86¬±0.26 100 /100
L2P++ 65.89¬±1.30 1 .24¬±0.14 0 .7/100.7
Deep L2P++ 68.42¬±1.20 1 .12¬±0.13 9 .6/109.6
DualPrompt 67.87¬±1.39 1 .07¬±0.14 1 .3/101.3
CODA-P-S 70.53¬±1.24 1 .00¬±0.15 0 .7/100.7
CODA-P 72.37¬±1.19 0 .96¬±0.15 4.6/104.6
13

--- PAGE 14 ---
Table D. Results (%) on 10-task CIFAR-100 (10 classes per task) .ANgives the accuracy averaged over tasks, FNgives the average
forgetting, and Nparam gives the % of trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean
and standard deviation over 5 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 89.30 - 100/100
ER (5000) 76.20¬±1.04 8 .50¬±0.37 100 /100
FT 9.92¬±0.27 29 .21¬±0.18 100 /100
FT++ 49.91¬±0.42 12 .30¬±0.23 100 /100
LwF.MC 64.83¬±1.03 5 .27¬±0.39 100 /100
L2P++ 82.50¬±1.10 1 .75¬±0.42 0 .7/100.7
Deep L2P++ 84.30¬±1.03 1.53¬±0.40 9.5/109.5
DualPrompt 83.05¬±1.16 1 .72¬±0.40 0 .7/100.7
CODA-P-S 84.59¬±0.87 1 .76¬±0.28 0 .6/100.6
CODA-P 86.25¬±0.74 1.67¬±0.26 4 .6/104.6
Table E. Results (%) on 5-task DomainNet (69 classes per task) .ANgives the accuracy averaged over tasks, FNgives the average
forgetting, and Nparam gives the % of trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean
and standard deviation over 3 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 79.65 - 100/100
ER (5000) 58.32¬±0.47 26 .25¬±0.24 100 /100
FT 18.00¬±0.26 43 .55¬±0.27 100 /100
FT++ 39.28¬±0.21 44 .39¬±0.31 100 /100
LwF.MC 74.78¬±0.43 5.01¬±0.14 100 /100
L2P++ 69.58¬±0.39 2 .25¬±0.08 0 .9/100.9
Deep L2P++ 70.54¬±0.51 2 .05¬±0.07 9 .7/109.7
DualPrompt 70.73¬±0.49 2.03¬±0.22 0.6/100.6
CODA-P-S 71.80¬±0.57 2 .54¬±0.10 0 .6/100.6
CODA-P 73.24¬±0.59 3 .46¬±0.09 4 .8/104.8
Table F. Results (%) on ImageNet-R with covariate domain shifts . Results are included for 5 tasks (40 classes per task). We simulate
domain shifts by randomly removing 50% of the dataset‚Äôs domains (e.g., clipart, paintings, and cartoon) for the training data of each task
(see SM for more details). ANgives the accuracy averaged over tasks, FNgives the average forgetting, and Nparam gives the % of
trainable parameters and final parameters w.r.t. the base ViT pre-trained model. We report the mean and standard deviation over 5 trials.
Method AN(‚Üë) FN(‚Üì)Nparam (‚Üì)
Train/Final
Upper-Bound 77.13 - 100/100
ER (5000) 67.39¬±0.37 11 .94¬±0.17 100 /100
FT 17.93¬±0.27 37 .49¬±0.28 100 /100
FT++ 54.51¬±0.68 14 .41¬±0.33 100 /100
LwF.MC 64.02¬±1.55 7 .05¬±0.27 100 /100
L2P++ 65.08¬±0.29 2 .79¬±0.32 0 .7/100.7
Deep L2P++ 65.74¬±0.12 2 .48¬±0.30 9 .6/109.6
DualPrompt 66.98¬±0.08 2.21¬±0.28 0.8/100.8
CODA-P-S 69.73¬±0.18 2 .35¬±0.19 0 .7/100.7
CODA-P 71.35¬±0.08 2.56¬±0.26 4 .6/104.6
14

--- PAGE 15 ---
covariate. Specifically, there are 15 image types in the
ImageNet-R dataset: ‚Äòart‚Äô, ‚Äòcartoon‚Äô, ‚Äòdeviantart‚Äô, ‚Äòem-
broidery‚Äô, ‚Äògraffiti‚Äô, ‚Äògraphic‚Äô, ‚Äòmisc‚Äô, ‚Äòorigami‚Äô, ‚Äòpainting‚Äô,
‚Äòsculpture‚Äô, ‚Äòsketch‚Äô, ‚Äòsticker‚Äô, ‚Äòtattoo‚Äô, ‚Äòtoy‚Äô, ‚Äòvideogame‚Äô .
We divide the dataset into 5 tasks of 40 classes each, and
within each task we randomly remove 8 of the domain types
from the training data. The task becomes more challeng-
ing because we now have image type domain shifts injected
into the continual learning task sequence, in addition to the
already-present class-incremental shifts.
15
