# 2403.13447.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2403.13447.pdf
# File size: 1433146 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HyperLLaV A: Dynamic Visual and Language Expert Tuning for
Multimodal Large Language Models
Wenqiao Zhang1♠Tianwei Lin2♠Jiang Liu3♠Fangxun Shu4Haoyuan Li1
Lei Zhang4He Wanggui4Hao Zhou5Zheqi Lv1
Hao Jiang4♣Juncheng Li1♣Siliang Tang1Yueting Zhuang1♣
1Zhejiang University ,2ShanghaiTech University ,3Chongqing University ,4Alibaba Group ,5Harbin Institute of Technology
{wenqiaozhang, lihaoyuan, zl.leizhang, zheqilv, junchengli, siliang, yzhuang}@zju.edu.cn
linjiawei@shanghaitech.edu.cn,jiangliu@stu.cqu.edu.cn, {shufangxun.sfx, aoshu.jh}@alibaba-inc.com
Abstract
Recent advancements indicate that scal-
ing up Multimodal Large Language Models
(MLLMs) effectively enhances performance
on downstream multimodal tasks. The pre-
vailing MLLM paradigm, e.g., LLaV A, trans-
forms visual features into text-like tokens us-
ing a static vision-language mapper, thereby
enabling static LLMs to develop the capabil-
ity to comprehend visual information through
visual instruction tuning. Although promis-
ing, the static tuning strategy1that shares the
same parameters may constrain performance
across different downstream multimodal tasks.
In light of this, we introduce HyperLLaV A,
which involves adaptive tuning of the pro-
jector and LLM parameters, in conjunction
with a dynamic visual expert and language
expert, respectively. These experts are de-
rived from HyperNetworks, which generates
adaptive parameter shifts through visual and
language guidance, enabling dynamic projec-
tor and LLM modeling in two-stage training.
Our experiments demonstrate that our solu-
tion significantly surpasses LLaV A on existing
MLLM benchmarks, including MME, MM-
Bench, SEED-Bench, and LLaV A-Bench.2.
1 Introduction
The landscape of Large Language Models
(LLMs) (Devlin et al., 2018; Radford et al., 2018;
Ouyang et al., 2022) has undergone significant
evolution, highlighting their exceptional versatil-
ity in managing a wide variety of language-centric
applications. To extend the capabilities of LLMs
to a wider array of modal inputs, Multimodal
Large Language Models (MLLMs) have garnered
increasing attention (Radford et al., 2021; Li et al.,
2022; Huang et al., 2023; Achiam et al., 2023; Li
1The static tuning refers to the trained model with static
parameters.
2Our project is available on the link
https://github.com/DCDmllm/HyperLLaV Aet al., 2023c). MLLMs are crucial for the develop-
ment of flexible, general-purpose assistants, as ev-
eryday interactions encompass information from
various modalities ( e.g., videos, audio, 3D envi-
ronments, point clouds) in addition to text.
Contemporary MLLMs ( e.g., LLaV A (Liu et al.,
2023b,a)) typically adhere to a two-stage train-
ing protocol: (i) Vision-Language Alignment :
A static projector is trained by leveraging image-
text pairs to synchronize visual features with the
language model’s word embedding space. The
projector with static parameters connects the vi-
sion and language modalities by translating vi-
sual features into visual tokens, enabling the LLM
to understand the visual content. The qual-
ity of conveyed visual tokens directly influences
the MLLM’s performance (Zhou et al., 2023).
(ii)Multimodal Insturction Tuning. Following
vision-language alignment, multimodal instruc-
tion data are used to refine the LLM, enabling it to
respond to users’ varied requests involving visual
content. This step is crucial for augmenting the ca-
pabilities and controllability of MLLM to address
different downstream multimodal tasks.
Despite two-stages’ critical importance, the
projector’s structure and LLM tuning strategy
have been relatively underexplored, most of the
pieces of literature focus on scaling up the pre-
training data (Bai et al., 2023a; Dai et al., 2023),
instruction-following data (Li et al., 2023a; Zhang
et al., 2023b; Zhao et al., 2023), visual en-
coders (Bai et al., 2023a) or language models (Lu
et al., 2023) to facilitate vision-language under-
standing. What’s more, further quantitative anal-
yses show that the learned model with static
parameters may limit their potential for multi-
downstream tasks (Mahabadi et al., 2021; Zhang
et al., 2023a). Based on the aforementioned
insights, our investigation focuses on the two-
stage training process, transitioning from static
to dynamic tuning—that is, tuning both the pro-
1arXiv:2403.13447v1  [cs.AI]  20 Mar 2024

--- PAGE 2 ---
Visual 
Expert𝑬𝑬𝑽𝑽
Large Language Model
Vicuna v1.5
𝑭𝑭𝑳𝑳
Projector𝑭𝑭𝑷𝑷
Visual Encoder 𝑭𝑭𝑽𝑽𝑣𝑣1𝑣𝑣2 𝑣𝑣𝑇𝑇
Word 
Embedding𝑭𝑭𝑾𝑾
𝑡𝑡1𝑡𝑡2 𝑡𝑡𝑇𝑇…
Language 
Expert𝑬𝑬𝑳𝑳
𝑣𝑣3𝑣𝑣4𝑣𝑣2
Visual Instruction
𝑟𝑟1𝑟𝑟2𝑟𝑟3𝑟𝑟4𝑟𝑟5𝑟𝑟6 𝑟𝑟8𝑟𝑟9 𝑟𝑟𝑇𝑇 𝑟𝑟7…
Trainable Parameters Visual Tokens Textual Tokens Textual Response Tokens
 Fixed Parameters…
Expert Module
 HyperLLaVA Framework
 Comparison 
c
 b
 a
Visual 
Guidance
Language 
Guidance
Language 
Expert
Visual 
Expert
LLM
Projector
LLaVAHyperLLaVA
HyperNetworkFigure 1: (a) is the overview of LLaV A. (b) describes the simplified version of our HyperLLaV A. (c) shows that
compared to LLaV A, our method achieves superior performance across different MLLM benchmarks.
jector and LLM with dynamic parameters to
provide flexible design choices that bolster the
MLLM’s reasoning abilities across diverse multi-
modal tasks.
In this paper, we propose HyperLLaV A (Fig-
ure 1(b)), its dynamic characterization benefits
from a carefully designed expert module, which
is derived from HyperNetworks (Ha et al., 2017)
to generate the dynamic parameters based on in-
put information. Our bootstrapping philosophy is
to dynamically generate strongly correlated fea-
tures according to visual and language guidance,
thereby dynamically modeling the projector and
LLM layers, respectively. In detail, HyperLLaV A
is learned following the two steps: (i) In vision-
language alignment, we divide the projector into
static layers (original MLP in LLaV A (Liu et al.,
2023a)) and dynamic layers (visual expert), where
the parameters of static layers are fixed, while
the parameters of dynamic layers are dynamically
generated based on visual input. The visual expert
leverages the Hypernetwork to assist the static pro-
jector learn a visual-specific projector that adap-
tively models the visual features according to the
visual guidance. By doing so, the projector can de-
liver the adaptative visual tokens to the language
semantic space. (2) In the multimodal instruction
tuning stage, we equip the LLM with a language
expert, modeling dynamic parameters for LLM
blocks. We regard the intermediate output of LLM
as language guidance that guides the language ex-
pert to provide an improved instruction-specific
comprehension of the user’s request. By doing so,
the MLLM increases the flexibility by instead gen-
erating unique parameters for every input, allow-
ing the MLLM to make use of similarities between
samples across datasets and avoid potential inter-ference between samples within the same dataset.
Notably, the proposed language expert serves as
a parameter-efficient fine-tuning approach for the
MLLMs, yielding a comparable performance ac-
cording to the original LLaV A.
In summary, our contributions are three-fold as
follows:
• We study the under-explored dynamic tuning
strategy for MLLMs and introduce Hyper-
LLaV A, leveraging the visual and language-
guided dynamic tuning for projector and
LLM;
• The proposed visual and language expert
serves as a parameter-efficient method of
multitask fine-tuning;
• We conducted comprehensive and detailed
experiments across multiple MLLM bench-
marks. The rich experimental results prove
the effectiveness and universality of our pro-
posed method.
2 Related Work
Large Language Model. The proliferation of
Large Language Models (LLMs) has dramatically
reshaped the landscape of natural language pro-
cessing. Pioneering models such as encoder-
centric model BERT (Devlin et al., 2018) and
decoder-centric model GPT (Radford et al., 2018)
have led this charge, showcasing that enhance-
ments in model size and the expansiveness of
training datasets can result in unprecedented im-
provements in performance. Building on the
achievements of their predecessors, subsequent
models have brought forth substantial innova-
tions that further advance the prowess of LLMs.
2

--- PAGE 3 ---
PaLM (Chowdhery et al., 2023) highlighted the
benefits of increasing model parameters for en-
hanced language comprehension. Meanwhile, In-
structGPT (Ouyang et al., 2022) and ChatGPT
utilized fine-tuning and reinforcement learning
strategies to refine their performance in conversa-
tional interaction (Chen et al., 2023b). However,
the reliance on textual data as the sole source of
learning has been a limiting factor, as it constrains
the models’ ability to engage with the richly inter-
connected world of multimodal information.
Multimodal Large Language Model. In re-
cent years, the development of deep learning has
brought prosperity to the field of multimodal intel-
ligence (Baltrušaitis et al., 2018; Li et al., 2023d;
Zhang et al., 2022b, 2019b, 2022a). Multimodal
Large Language Models (MLLMs) leverage the
power of LLMs, mitigating extra computational
cost and enhancing the efficacy of multimodal
pre-training (Zhang et al., 2024), to bridge the
gap between textual and multimodal data( e.g., im-
ages, videos, and audios). A prominent attempt
is CLIP (Radford et al., 2021), demonstrating the
alignment of visual and textual modalities via con-
trastive learning across a broad dataset of image-
text pairs. (Li et al., 2022) and (Li et al., 2023e)
follow this trend, proposing BLIP and BLIP-2 im-
proved upon CLIP, and gain remarkable perfor-
mance in basic visual tasks. Flamingo (Alayrac
et al., 2022) led the way in merging vision and
language models by utilizing vast amounts of in-
tertwined image-text dataset, revealing unparal-
leled zero-shot capabilities in processing image-
text content within conversational contexts for the
first time. LLaV A (Liu et al., 2023b) distinc-
tively incorporates short captions annotated by
humans and bounding boxes into the GPT4 lan-
guage model. In the realm of audio process-
ing, there are also some brilliant works, such as
SpeechT5 (Ao et al., 2021), MMS (Pratap et al.,
2023), PandaGPT (Su et al., 2023), etc.
Hypernetworks. The original HyperNetwork (Ha
et al., 2017) is designed to reduce the num-
ber of parameters, i.e, a small neural network
generates parameters for another big neural net-
work, thereby obtaining the model compression
for different tasks. Subsequently, HyperNetwork
is developed to various domain tasks, includ-
ing few-shot learning (Brock et al., 2018), graph
modeling (Zhang et al., 2019a), domain adapta-
tion (Zhang et al., 2023a), device-cloud collabora-
tion (Lv et al., 2023b,a), etc.3 Methodology
This section describes the proposed MLLM
framework HyperLLaV A. We shall present each
module and its training strategy.
3.1 Problem Formulation
The primary goal is to effectively leverage the ca-
pabilities of both the pre-trained LLM and visual
model. The network architecture is illustrated in
Figure 2. Given an RGB image x∈RH×W×3,
where HandWare the origin resolution. The vi-
sion encoder processes input images to obtain a vi-
sual token sequence V= [v1, v2,···, vNv], where
Nvrepresents the sequence length of text tokens.
Subsequently, we concatenate the visual tokens
and text tokens T= [t1, t2,···, tNt], together and
feed them into a LLM Mllm, then generate the
language response R= [r1, r2,···, tNr], where
NtandNrindicate the length of text tokens and
textual response. In general, MLLM model M(·)
consists of two functions as below:
M(·)|{z}
MLLM:Mp((T |V); Θp)|{z }
Projector→ M l((R|V,T); Θl)| {z }
LLM,
(1)
where Mp(·; Θp)is the projector and Mt(·; Θl)
LLM tuning with multi-modal instructions with
parameters ΘpandΘl, respectively.
3.2 Preliminaries
LLaV A. LLaVA (Liu et al., 2023b) is trained fol-
lowing two steps: (i) First, a two-layer MLP is
employed as vision-language projector Mp(·)to
convert visual features into visual tokens V, which
have the same dimensionality as the word em-
bedding space in the language model. (ii) Then
LLaV A performs instruction-tuning with visual
tokens Vand language tokens Tfor the LLM
(Llama) Ml(·), generating response tokens Rby
optimizing its auto-regressive training objective.
HyperNetwork. Hypernetwork (Ha et al., 2016)
is a neural network that generates the weights for
another neural network. Specifically, HyperNet-
work treats the parameters of the multi-layer per-
ception (MLP) as a matrix K(n)∈RNin×Nout,
where NinandNoutrepresent the number of in-
put and output neurons of the nthlayer of MLP,
respectively. NinandNoutportray the structure of
the MLP layers together. The generation process
ofK(n)can be regarded as a matrix factorization:
K(n)=ξ(z(n); Θp),∀n= 1,···, Nl.(2)
3

--- PAGE 4 ---
In the training procedure ,z(n)andξ(·)are ran-
domly initialized. The gradients are backpropa-
gated to z(n)andξ(·), which can help to update
them. z(n)andξ(·)will be saved instead of K(n).
3.3 Vision-language Guided Expert Module
Original LLaV A’s projector and LLM are trained
with static parameters. We argue that the static
tuning paradigm may limit the flexible visual to-
ken delivery and appropriate expression for dif-
ferent downstream multi-modal tasks. Thus, we
propose to equip the original’s LLaV A projector
and LLM with a visual expert EVand a language
expert EL: (i) the visual expert adaptively fits the
projector’s output according to the specific visual
guidance ( e.g, visual features); (ii) the language
expert dynamically modeling the posterior blocks
of LLM through anterior LLM’s block output.
The expert module is derived from Hyperne-
torks, which is a neural network that generates its
parameters for another neural network. As Hyper-
Network dynamically generates a network condi-
tioned on the input embeddings, i.e., the “dynamic
characterization” can be modeled by HyperNet-
work. However, directly utilizing the HyperNet-
work may not satisfactorily model dynamic learn-
ing for two key reasons:
•Weak Correlation. The original Hyper-
Network learns the latent vector to generate
another model’s parameters. This lacks a
strong correlation between parameter gener-
ation and input guidance.
•Unstable Optimization. Using HyperNet-
work generate the parameters of the projector
or LLM block is large ( Dx×Nin×Nout),i.e.,
it is hard to optimize the such the numerous
parameters, the optimization process is intu-
itively unstable.
To this end, we carefully tailor the HyperNet-
work with the following adjustments:
Input Prior Guidance. We first propose to model
the dynamic layers by replacing the learned latent
vector zwith specific input. Specifically, given
the feature fx(i)extracted from backbone of sam-
plex(i), we first develop a layer-specific encoder
En(·)that encode the fx(i)ase(n). This vector
represents the nthlayer parameters.
e(n)=En(fx(i)),∀n= 1,···, Nl, (3)
where Nlis the number of the modeled layers.Then the HyperNetwork is used to convert the
embedding e(n)into parameters, i.e., we input e(n)
into the following two MLP layers to generate pa-
rameters of dynamic layers.
w(n)= (W1e(n)+B1)W2+B2,
K(n)=w(n)+b(n),(4)
where K(n)denotes the nthlayer parameters of
dynamic layers. Two MLP layers’s weight are de-
noted by W1andW2, respectively. b(n),B1and
B2are the biases.
HyperNetwork-aware Adapter. Adapters are
sub-networks with small parameters that are in-
serted after every attention and feed-forward layer
in a model (Houlsby et al., 2019). The original
adapter is a parameter-efficient learning approach
that learns downstream tasks by updating only a
small number of parameters. The adapters consist
of a pair of downsampling and upsampling lay-
ers, and a residual connection. We found that us-
ing downsampling and upsampling strategies, the
HyperNetwork-generated parameters can be sub-
stantially reduced.
Given the visual guidance xVand language
guidance xL, the vision-language guided expert
can be defined as:
EM(xM) =Wu
M(SwiGLU( Wd
M(xM)))
Wu
M, Wd
M=HM(xM),where M∈V, L(5)
where Mindicate the modality, Wu
M, Wd
Mre-
spectively denote the weights for upsampling and
downsampling. SwiGLU (Ramachandran et al.,
2017) is the activation function, Gaussian Error
Linear Unit. HMis the HyperNetwork.
3.4 Visual Expert-Assisted Projector
In this stage, our objective is to adapt the im-
age tokens to LLM, allowing the LLM to com-
prehend the instances in the images. As shown
in Figure 2, we divide the projector as static lay-
ers and dynamic layers. Following LLaV A1.5 (Liu
et al., 2023a), we employ an two-layer MLP as the
static layers. To empower the projector’s expres-
sion, we develop a visual expert that learning the
projector shift to model the dynamic text tokens.
Specifically, given the visual feature fVextracted
from visual encoder, the visual expert will adap-
tively convert fVto dynamic visual embeddings.
We show three alternatives for the dynamic vision-
language alignment, the visual tokens Vcan be
4

--- PAGE 5 ---
1st-layer MLP
𝑣𝑣1𝑣𝑣2 𝑣𝑣6𝑣𝑣5 𝑣𝑣3𝑣𝑣4
Word Embedding Layer
Visual Expert
 2nd-layer MLP
𝑣𝑣7𝑣𝑣𝑁𝑁𝑣𝑣…
𝑡𝑡1𝑡𝑡2 𝑡𝑡6𝑡𝑡5 𝑡𝑡3𝑡𝑡4 𝑡𝑡7𝑡𝑡𝑁𝑁𝑡𝑡…Input Image
Visual 
Guidance
Parameter
GenerationParameter
Generation
Upsample
Downsample
SwiGLU
HyperNetworkVisual Encoder
User:What is unusual about this image?
Visual Expert
Visual Expert -assisted Projector
 a
Language 
Expert
Parameter
Generation
Parameter
GenerationHyperNetwork
Upsample
Downsample
Q K V
SwiGLU
Self-Attention
RMS Norm
RMS Norm
RMS Norm
SwiGLU
Linear
 Softmax
Q K V
Feed-
Forward
SwiGLU
Self-Attention
RMS Norm
RMS Norm
RMS NormLanguage
Guidance
Language 
Expert
Parameter
Generation
Parameter
GenerationHyperNetwork
Upsample
Downsample
Q K V
SwiGLU
Self-Attention
RMS Norm
RMS Norm
RMS Norm
SwiGLU
Linear
 Softmax
Feed-
ForwardThe image shows a red pencil with a looped 
or coiled shape, which is unusual because pencils typically have a straight shaft. 𝑟𝑟1𝑟𝑟2𝑟𝑟3𝑟𝑟4𝑟𝑟5 𝑟𝑟𝑁𝑁𝑟𝑟…
Language Expert-integrated Tuning
 b
½ Layers Figure 2: Overview of proposed HyperLLaV A. (a) describes how the proposed visual expert assists the static
projector that dynamically converts the image features to adaptive visual tokens, yielding an augmented visual
expression for subsequent instruction tuning. (b) is the proposed language expert-integrated tuning, which uses the
output of the intermediate layer as language guidance to generate dynamic instruction-specific feature, increasing
the flexibility for processing different multimodal tasks.
calculated as:
V=

L2(L1(fV) +EV1(fV))| {z }
Use 1st Visual Expert
L2(L1(fV)) +EV2(L1(fV))| {z }
Use 2nd Visual Expert
L2(L1(fV) +EV1(fV)) +EV2(L1(fV))| {z }
Use 1st&2nd Visual Expert
(6)
where L1andL2denotes two MLPs, EV1andEV2
are the visual expert for first and second MLPs.
We give a details comparison in the Sec. experi-
ments.
Such visual experts learn the projector shift to
model the dynamic text tokens, and thus empower
the projector’s expression for downstream tasks.
3.5 Language Expert-integrated Tuning
In this stage, LLM is adjusted to become an
LVLM with multi-modal understanding. We use
more complex instructions, including tasks such
as image logical reasoning and text recognition,
which require the model to have a stronger multi-
modal understanding. Different Previous studies
have shown that features provided by the inter-mediate layer may suffice to preliminarily under-
stand the given input samples (Xin et al., 2020)and
can serve as guidance hints to improve train-
ing (Romero et al., 2014). Thus, generating guid-
ance in the intermediate LLM layer allows the
model to form a preliminary understanding of the
given instruction. Therefore, we regard the output
of the intermediate LLM layer as language guid-
ance that generates adaptive instruction-specific
features that enhance the generation accuracy. As
shown in Figure 2, given the language guidance
fL, the adapter’s parameters {Wu
L, Wd
L}are gen-
erated by HL(fL). By doing so, the instruction-
specific features can be calculated as below:
ˆxL=EL(xL) +xL+ FFN(SwiGLU( xl))(7)
where xLis the features generated from RMS nor-
malization and self-attention in LLM’s block.
4 Experiments
We verify HyperLLaV A’s effectiveness on multi-
ple datasets and then discuss HyperLLaV A’s prop-
erties with controlled studies.
5

--- PAGE 6 ---
Table 1: Comparison with SoTA methods on 12 benchmarks. Res, PT, IT indicate input image resolution, the number
of samples in the pretraining and instruction tuning stage, respectively. We color each row as the best and second best .
Improv. ↑indicates performance improvement compared with LLaV A.
Method LLM Res. PT ITVQA Datasets Benchmark Toolkits
VQAv2GQA VizWiz SQAIVQATPOPE MME MMB MMBCNSEED LLaV AWMM-Vet
BLIP-2 (Li et al., 2023e) Vicuna-13B 224 129M - 41.0 41 19.6 61 42.5 85.3 1293.8 - - 46.4 38.1 22.4
InstructBLIP (Dai et al., 2023) Vicuna-7B 224 129M 1.2M - 49.2 34.5 60.5 50.1 - - 36 23.7 53.4 60.9 26.2
InstructBLIP (Dai et al., 2023) Vicuna-13B 224 129M 1.2M - 49.5 33.4 63.1 50.7 78.9 1212.8 - - 58.2 - 25.6
Shikra (Chen et al., 2023a) Vicuna-13B 224 600K 5.5M 77.4 - - - - - 58.8 - - - - -
IDEFICS-9B (Laurençon et al., 2023) LLama-7B 224 353M 1M 50.9 38.4 35.5 - 25.9 - - 48.2 25.2 - - -
IDEFICS-80B (Laurençon et al., 2023) LLama-65B 224 353M 1M 60.0 45.2 36.0 - 30.9 - - 54.5 38.1 - - -
Qwen-VL (Bai et al., 2023b) Qwen-7B 448 1.4B 50M 78.8 59.3 35.2 67.1 63.8 - - 38.2 7.4 56.3 - -
Qwen-VL-Chat (Bai et al., 2023b) Qwen-7B 448 1.4B 50M 78.2 57.5 38.9 68.2 61.5 - 1487.5 60.6 56.7 58.2 - -
HyperLLaV A w/o EV(Ours) Vicuna-7B 336 558K 665K 79.0 62.5 50.3 70.4 58.1 85.9 1486.0 65.9 59.7 61.0 63.7 32.8
HyperLLaV A w/o EL(Ours) Vicuna-7B 336 558K 665K 78.8 61.9 52.1 70.7 57.5 85.6 1492.0 66.7 58.6 60.8 62.6 30.9
LLaV A-1.5 (Liu et al., 2023a) Vicuna-7B 336 558K 665K 78.5 62.0 50.0 66.8 58.2 85.9 1510.7 64.3 58.3 58.6 63.4 30.5
HyperLLaV A (Ours) Vicuna-7B 336 558K 665K 79.1 62.7 51.9 70.4 58.5 86.3 1481.2 65.9 60.6 61.4 64.0 31.0
Improv. ↑ - - - - 0.6 +0.7 +1.9 +3.6 +0.3 +0.4 - +1.6 +2.3 +2.8 +0.6 +0.5
Table 2: Three Alternatives for Dynamic Vision-language
Alignment. EV1andEV2denote visual expert for first and
second MLP layer.
MethodsVQA Datasets Benchmark Toolkits
GQA SQA-I VQA-T POPE MME
w/oEV 62.5 70.4 58.1 85.9 1486.0
EV2 62.0 69.8 58.0 86.4 1442.6
EV1&EV2 60.1 69.5 54.4 86.1 1449.8
EV1 62.7 70.4 58.5 86.3 1481.2
4.1 Dataset and Setting
Benchmark Datasets. We evaluate our pro-
posed HyperLLaV A on five VQA datasets: VQA-
v2 (Goyal et al., 2017); GQA (Hudson and Man-
ning, 2019); VizWiz (Gurari et al., 2018); SQAI:
ScienceQA-IMG (Lu et al., 2022); VQAT(Singh
et al., 2019): TextVQA and seven Benchmark
Toolkits: POPE (Li et al., 2023f); MME (Fu
et al., 2023); MMB: MMBench (Liu et al., 2023c);
MMBCN: MMBench-Chinese (Liu et al., 2023c);
SEED: SEED-Bench (Li et al., 2023b); LLaV AW:
LLaV A-Bench(In-the-Wild) (Liu et al., 2023b);
MM-Vet (Yu et al., 2023).
Implementation Details. The model was trained
on an 8-A100 machine in one day. The im-
plementation details refer to the Appendix. In
the training of the HyperLLaV A, we utilize the
ADAMW (Loshchilov and Hutter, 2017) opti-
mizer, adapting hyperparameters to cater to the
specific requirements of each phase. For the fea-
ture alignment stage, parameters are set as B=
32,Lr= 0.001, while for visual instruction
tuning stage, we adjust the parameters to B=
16,Lr= 0.00002 . The configuration for the
ADAMW optimizer incorporates the following
settings: β= (0.9,0.999) ,ε= 1×10−8, and
Wd= 0.0, ensuring a bespoke optimization strat-Table 3: Analysis of Language Expert Integration for Dif-
ferent LLM Layers.
MethodVQA Datasets Benchmark Toolkits
GQA SQA-I VQA-T POPE MME
w/oEL 61.9 70.7 57.5 85.6 1492.0
Anterior 16 Blocks 62.5 69.4 58.5 85.9 1481.4
All 32 Blocks 62.7 69.5 58.6 86.0 1460.3
Posterior 16 Blocks 62.7 70.4 58.5 86.3 1481.2
egy that effectively addresses the unique demands
of each training phase.
Besides, We train our model following the same
training process as LLaV A-1.5. The process in-
cludes two stages: (1) feature alignment stage:
use 558K subset of the LAION-CC-SBU dataset
to connect a frozen pretrained vision encoder to
a frozen LLM; (2) visual instruction tuning stage:
use 150K GPT-generated multimodal instruction-
following data, plus around 515K VQA data from
academic-oriented tasks, to teach the model to fol-
low multimodal instructions.
Comparison of Methods. For quantifying the ef-
ficacy of the proposed framework, we compare
HyperLLaV A with previous SOTA approaches.
We choose BLIP-2(Li et al., 2023e), Instruct-
BLIP(Dai et al., 2023) based on Vicuna-7B, In-
structBLIP(Dai et al., 2023) based on Vicuna-
13B, Shikra (Chen et al., 2023a), IDEFICS-
9B(Laurençon et al., 2023), IDEFICS-80B (Lau-
rençon et al., 2023), Qwen-VL (Bai et al., 2023b),
Qwen-VL-Chat (Bai et al., 2023b) and LLaV A-
1.5 (Liu et al., 2023a). More details of baselines
are in the Appendix.
4.2 Overall Performance
We benchmark HyperLLaV A on a wide range of
academic VQA benchmarks and recent bench-
6

--- PAGE 7 ---
Table 4: Zero-shot object hallucination evaluation results on POPE dataset. "Yes" indicates the proportion of positive
responses to the given question.
Method LLM ActivatedAdersarial Popular Random
Acc F1-Score Yes Acc F1-Score Yes Acc F1-Score Yes
mPLUG-Owl (Ye et al., 2023) LLaMA-7B 6.7B 82.4 81.6 45.2 85.5 84.3 42.1 86.3 85.3 42.3
MM-GPT (Gong et al., 2023) LLaMA-7B 6.7B 50.0 66.7 100.0 50.0 66.7 100.0 50.0 66.7 100.0
LLaV A-1.5 (Liu et al., 2023a) Vicuna-7B 7B 85.1 84.2 44.0 87.2 86.1 41.9 50.3 45.9 41.9
HyperLLaV A Vicuna-7B 7B 85.6 84.7 44.1 87.3 86.2 42.4 50.7 46.5 42.1
Table 5: Deep analysis of expert structure.
MethodVQA Datasets Benchmark Toolkits
GQA SQA-I VQA-T POPE MME
Adapter (Houlsby et al., 2019) 57.7 69.4 53.5 83.5 1371.8
Hypernetwork+MLP 60.2 68.8 52.9 84.6 1460.7
Hypernetwork+Adapter (Mahabadi et al., 2021) 62.1 69.9 57.0 85.4 1494.4
Ours 62.7 70.4 58.5 86.3 1481.2
marks specifically proposed for instruction-
following LMMs, totaling 12 benchmarks. Table 1
summarizes the quantitative results of our frame-
work and baselines on five VQA datasets and
fiveBenchmark Toolkits . We make the fol-
lowing observations: 1) In general, irrespective
of the different scenarios, compared to LLaV A,
HyperLLaV A achieves the best performance on
almost all the multimodal scenarios across both
datasets (Except for the MME benchmark), which
strongly demonstrates the generalizability of the
proposed HyperLLaV A. 2) HyperLLaV A (both 7B
and 13B) outperforms bigger MLLMs with bil-
lions of trainable parameters for cross-modal con-
nection ( e.g., 80B IDEFICS (Laurençon et al.,
2023)). This further indicates the effectiveness
of the proposed MLLM structure. 3) Compared
with the original LLaV A, we show that Hyper-
LLaV A achieves the best performance across 11
out of 12 benchmarks. Such results benefit from
the carefully designed lightweight visual and lan-
guage expert, which empowers the static projector
and LLM to facilitate different multimodal tasks.
4.3 Ablation Study
Effectiveness of Each Component. Table 1 also
illustrate the effectiveness of each component, i.e.,
visual expert EVand language expert EL. Compar-
ing HyperLLaV A and HyperLLaV A(- EV) (Row
11v.sRow 13), the EVcontributes 2.61% im-
provement on mean accuracy. Meanwhile, Row
11 indicates that it suffers from 0.94%, a notice-
able performance degradation without the EL. To
sum up, we can observe that the improvement of
using each module alone is distinguishable. Com-
bining all the components, our HyperLLaV A ex-
hibits steady improvement over the baselines.Table 6: Comparsion of parameter-efficient learning.
MethodVQA Datasets Benchmark Toolkits
GQA SQA-I VQA-T POPE MME
LoRa (Hu et al., 2021) 63.0 68.4 58.2 86.4 1496.9
Adapter (Houlsby et al., 2019) 42.6 61.2 41.0 81.2 874.4
Hypernetwork+Adapter (Mahabadi et al., 2021) 49.0 63.6 48.3 84.6 1140.0
Language Expert 62.5 70.4 58.1 85.9 1486.0
4.4 In-Depth Analysis
We validate the effectiveness of the proposed two
modules through the experiments on GQA, SQA-
I, VQA-T, POPE and MME benchmarks.
Three Alternatives for Vision-language Align-
ment. To build insights on the visual expert-
assisted projector in HyperLLaV A, we perform an
in-depth analysis of three alternatives for dynamic
vision-language alignment. Table 2 exhibits the
three results. According to our observation, us-
ing one visual expert to access the dynamic pro-
jection yields the best results. Besides, the other
two plans also obtained comparable results, indi-
cating the effectiveness of dynamic projection.
Analysis of Language Expert Integration for
Different Blocks. To deeply analyze the effective-
ness of language experts, we study the language
expert integration for different blocks in Table 3,
including anterior 16 blocks (before 1/2 LLM lay-
ers), all 32 blocks (all LLM layers) and poste-
rior 16 blocks (after 1/2 LMM layers). Generally
speaking, leveraging the language expert integra-
tion for the posterior 16 blocks obtained almost
the best performance. Besides, Row 2 and Row 3
utilize the initial language input as language guid-
ance, obtaining suboptimal results compared with
language expert integration for the posterior 16
blocks. Our intuition is that the language guid-
ance might not have gathered sufficient contextual
information for subsequent dynamic LLM layer
modeling.
Analysis on the Inserted Blocks for Language
Guidance. We investigate the impact of inserting
language guidance into different layers of LLMs.
We report the evaluation score of GQA and POPE
datasets in Figure 4. We observe that the per-
formance is low when we insert language guid-
7

--- PAGE 8 ---
(a) POPE Performance (b) GQA PerformanceFigure 3: Selected blocks for language guidance.
ance too early ( i.e., 4, 8) as the model might not
have gathered sufficient contextual information to
generate effective guidance. Meanwhile, inserting
language guidance too late ( i.e., 24, 28) degen-
erates the performance. We speculate this is due
to the generated guidance being too concentrated
and there not being enough layers to integrate the
language-aware details.
Analysis of Expert’s Structure. We sys-
tematically present the explicit benefits from the
carefully designed expert’s structure in Table 5.
The adapter-based structure surpasses MLP-based
structure across all datasets, mainly due to the gen-
erated MLP is no longer a lightweight network to
optimize, producing unstable performance. Com-
pared with HyperNetwork+Adapter (Row 3 vs
Row 4), our proposed vision-language guided ex-
pert structure obtained the best performance. The
results correspond with our assumption of the
original HyperNetworks, which lacks a strong cor-
relation between input and parameter generation.
Our method, allows the model to make use of sim-
ilarities between samples across datasets and avoid
potential interference between samples within the
same dataset.
Effect of Dimension of Expert Input and Down-
sampling. Figure 4 empirically provides an ap-
propriate dimension of input and downsampling,
i.e, 64 and 16, respectively, either increasing or
decreasing this value results in a performance de-
cay. According to our analysis, a bigger dimension
may result in an unstable HyperNetwork optimiza-
tion and a smaller value contains less language-
guided information for dynamic learning, and thus
yielding performance decay.
Parameter-efficient Fine-tuning. Our proposed
language expert also can serve as a parameter-
efficient fine-tuning function. The structure is
similar to the HyperNetwork+Adapter. However,
original hypernetwork-based approaches gener-
ally condition their parameters on a learned latent
(a) Effect of Expert’s Input Dimension (b) Effect of Expert’s Downsampling DimensionFigure 4: Performance with respect to the different
input and downsampling dimension in expert.
embedding, implying the model is the same for ev-
ery example, yield performance decay. Summing
up, the proposed language expert is an effective
and parameter-efficient way to share information
across multiple adapters to enable positive trans-
fer to low-resource and related tasks.
Object Hallucination Evaluation. We adopt the
evaluation pipeline of POPE (Li et al., 2023f),
a polling-based query method, to evaluate ob-
ject hallucination in HyperLLaV A. The results are
presented in Table 4, where HyperLLaV A ex-
hibits the best performance, indicating that Hyper-
LLaV A tends to generate objects consistent with
the given image. Additionally, we observe that the
“yes” ratio of HyperLLaV A remains relatively bal-
anced, indicating that our model is capable of pro-
viding accurate feedback based on the questions.
5 Conclusion
Building upon HyperLLaV A’s innovative dy-
namic tuning strategy, our work paves the way
for groundbreaking advancements in multimodal
learning systems. By adaptively tuning both
projector and LLM parameters, and integrating
dynamical visual and language experts, we not
only surpass the performance benchmarks set by
LLaV A but also introduce a parameter-efficient
methodology. This approach offers a new hori-
zon for enhancing multimodal task performances
through personalized, dynamic adjustments. Fu-
ture research could further explore the scalabil-
ity of dynamic tuning mechanisms, potentially un-
locking new avenues for understanding and inte-
grating multimodal information more seamlessly.
8

--- PAGE 9 ---
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical re-
port. arXiv preprint arXiv:2303.08774 .
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm
Reynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in Neural
Information Processing Systems , 35:23716–23736.
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang,
Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li,
Yu Zhang, et al. 2021. Speecht5: Unified-modal
encoder-decoder pre-training for spoken language
processing. arXiv preprint arXiv:2110.07205 .
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. 2023a. Qwen-vl: A frontier large
vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 .
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,
and Jingren Zhou. 2023b. Qwen-vl: A frontier large
vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 .
Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-
Philippe Morency. 2018. Multimodal machine
learning: A survey and taxonomy. IEEE transac-
tions on pattern analysis and machine intelligence ,
41(2):423–443.
Andrew Brock, Theodore Lim, James M. Ritchie, and
Nick Weston. 2018. SMASH: one-shot model archi-
tecture search through hypernetworks. In 6th Inter-
national Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May
3, 2018, Conference Track Proceedings . OpenRe-
view.net.
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. 2023a. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv
preprint arXiv:2306.15195 .
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-
ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. 2023b. Sharegpt4v: Improving large multi-
modal models with better captions. arXiv preprint
arXiv:2311.12793 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2023. Palm: Scaling lan-
guage modeling with pathways. Journal of Machine
Learning Research , 24(240):1–113.W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang,
B Li, P Fung, and S Hoi. 2023. Instructblip: to-
wards general-purpose vision-language models with
instruction tuning. arxiv. Preprint posted online on
June, 15:2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei
Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xi-
awu Zheng, Ke Li, Xing Sun, et al. 2023. Mme:
A comprehensive evaluation benchmark for mul-
timodal large language models. arXiv preprint
arXiv:2306.13394 .
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong
Wang, Miao Zheng, Qian Zhao, Kuikun Liu,
Wenwei Zhang, Ping Luo, and Kai Chen. 2023.
Multimodal-gpt: A vision and language model
for dialogue with humans. arXiv preprint
arXiv:2305.04790 .
Yash Goyal, Tejas Khot, Douglas Summers-Stay,
Dhruv Batra, and Devi Parikh. 2017. Making the
v in vqa matter: Elevating the role of image under-
standing in visual question answering. In Proceed-
ings of the IEEE conference on computer vision and
pattern recognition , pages 6904–6913.
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,
Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P
Bigham. 2018. Vizwiz grand challenge: Answering
visual questions from blind people. In Proceedings
of the IEEE conference on computer vision and pat-
tern recognition , pages 3608–3617.
David Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-
networks. arXiv preprint arXiv:1609.09106 .
David Ha, Andrew M. Dai, and Quoc V . Le. 2017.
Hypernetworks. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings . OpenReview.net.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efficient transfer learning for nlp.
InInternational Conference on Machine Learning ,
pages 2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei
Cui, Owais Khan Mohammed, Qiang Liu, et al.
2023. Language is not all you need: Aligning
perception with language models. arXiv preprint
arXiv:2302.14045 .
9

--- PAGE 10 ---
Drew A Hudson and Christopher D Manning. 2019.
Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In Proceed-
ings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 6700–6709.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon,
Stas Bekman, Amanpreet Singh, Anton Lozhkov,
Thomas Wang, Siddharth Karamcheti, Alexan-
der M. Rush, Douwe Kiela, Matthieu Cord, and Vic-
tor Sanh. 2023. Obelics: An open web-scale filtered
dataset of interleaved image-text documents.
Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei
Liu. 2023a. Mimic-it: Multi-modal in-context in-
struction tuning. arXiv preprint arXiv:2306.05425 .
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. 2023b. Seed-bench: Bench-
marking multimodal llms with generative compre-
hension. arXiv preprint arXiv:2307.16125 .
Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao,
Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat-Seng
Chua, Siliang Tang, and Yueting Zhuang. 2023c.
Fine-tuning multimodal llms to follow zero-shot
demonstrative instructions. In The Twelfth Interna-
tional Conference on Learning Representations .
Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao
Zhang, Yi Yang, Tat-Seng Chua, and Fei Wu.
2023d. Variational cross-graph reasoning and adap-
tive structured semantics learning for compositional
temporal grounding. IEEE Transactions on Pattern
Analysis and Machine Intelligence .
Junnan Li, Dongxu Li, Silvio Savarese, and Steven
Hoi. 2023e. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large
language models. arXiv preprint arXiv:2301.12597 .
Junnan Li, Dongxu Li, Caiming Xiong, and Steven
Hoi. 2022. Blip: Bootstrapping language-image
pre-training for unified vision-language understand-
ing and generation. In International Conference on
Machine Learning , pages 12888–12900. PMLR.
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang,
Wayne Xin Zhao, and Ji-Rong Wen. 2023f. Eval-
uating object hallucination in large vision-language
models. arXiv preprint arXiv:2305.10355 .
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Lee. 2023a. Improved baselines with visual instruc-
tion tuning. arXiv preprint arXiv:2310.03744 .
Haotian Liu, Chunyuan Li, Qingyang Wu, and
Yong Jae Lee. 2023b. Visual instruction tuning.
arXiv preprint arXiv:2304.08485 .
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, et al. 2023c. Mm-
bench: Is your multi-modal model an all-around
player? arXiv preprint arXiv:2307.06281 .Ilya Loshchilov and Frank Hutter. 2017. Decou-
pled weight decay regularization. arXiv preprint
arXiv:1711.05101 .
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-
Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. 2022. Learn to explain:
Multimodal reasoning via thought chains for science
question answering. Advances in Neural Informa-
tion Processing Systems , 35:2507–2521.
Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang,
Jianfeng Gao, and Yelong Shen. 2023. An empiri-
cal study of scaling instruct-tuned large multimodal
models. arXiv preprint arXiv:2309.09958 .
Zheqi Lv, Zhengyu Chen, Shengyu Zhang, Kun Kuang,
Wenqiao Zhang, Mengze Li, Beng Chin Ooi, and Fei
Wu. 2023a. Ideal: Toward high-efficiency device-
cloud collaborative and dynamic recommendation
system. arXiv preprint arXiv:2302.07335 .
Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun
Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen,
Tao Shen, Hongxia Yang, Beng Chin Ooi, and Fei
Wu. 2023b. Duet: A tuning-free device-cloud col-
laborative parameters generation framework for ef-
ficient device model generalization. In Proceedings
of the ACM Web Conference 2023 .
Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa
Dehghani, and James Henderson. 2021. Parameter-
efficient multi-task fine-tuning for transform-
ers via shared hypernetworks. arXiv preprint
arXiv:2106.04489 .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, et al. 2022. Training language models to fol-
low instructions with human feedback, 2022. URL
https://arxiv. org/abs/2203.02155 , 13.
Vineel Pratap, Andros Tjandra, Bowen Shi, Paden
Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky,
Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-
Zarandi, et al. 2023. Scaling speech tech-
nology to 1,000+ languages. arXiv preprint
arXiv:2305.13516 .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack
Clark, et al. 2021. Learning transferable visual mod-
els from natural language supervision. In Interna-
tional conference on machine learning , pages 8748–
8763. PMLR.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Prajit Ramachandran, Barret Zoph, and Quoc V Le.
2017. Searching for activation functions. arXiv
preprint arXiv:1710.05941 .
10

--- PAGE 11 ---
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2014. Fitnets: Hints for thin deep nets.
arXiv preprint arXiv:1412.6550 .
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards vqa models
that can read. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 8317–8326.
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan
Wang, and Deng Cai. 2023. Pandagpt: One
model to instruction-follow them all. arXiv preprint
arXiv:2305.16355 .
Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and
Jimmy Lin. 2020. Deebert: Dynamic early exit-
ing for accelerating bert inference. arXiv preprint
arXiv:2004.12993 .
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,
Ming Yan, Yiyang Zhou, Junyang Wang, An-
wen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.
mplug-owl: Modularization empowers large lan-
guage models with multimodality. arXiv preprint
arXiv:2304.14178 .
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng
Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and
Lijuan Wang. 2023. Mm-vet: Evaluating large mul-
timodal models for integrated capabilities. arXiv
preprint arXiv:2308.02490 .
Chris Zhang, Mengye Ren, and Raquel Urtasun. 2019a.
Graph hypernetworks for neural architecture search.
In7th International Conference on Learning Repre-
sentations, ICLR 2019, New Orleans, LA, USA, May
6-9, 2019 . OpenReview.net.
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong,
Dan Su, Chenhui Chu, and Dong Yu. 2024. Mm-
llms: Recent advances in multimodal large language
models. arXiv preprint arXiv:2401.13601 .
Wenqiao Zhang, Zheqi Lv, Hao Zhou, Jia-Wei Liu,
Juncheng Li, Mengze Li, Siliang Tang, and Yuet-
ing Zhuang. 2023a. Revisiting the domain shift and
sample uncertainty in multi-source active domain
transfer. arXiv preprint arXiv:2311.12905 .
Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu
Zhang, Qingpeng Cai, Juncheng Li, Sihui Luo, and
Yueting Zhuang. 2022a. Magic: Multimodal rela-
tional graph adversarial inference for diverse and un-
paired text-based image captioning. In Proceedings
of the AAAI Conference on Artificial Intelligence ,
volume 36, pages 3335–3343.
Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang
Pu, Fei Wu, and Yueting Zhuang. 2019b. Frame
augmented alternating attention network for video
question answering. IEEE Transactions on Multi-
media , 22(4):1032–1041.Wenqiao Zhang, Lei Zhu, James Hallinan, Shengyu
Zhang, Andrew Makmur, Qingpeng Cai, and
Beng Chin Ooi. 2022b. Boostmis: Boosting med-
ical image semi-supervised learning with adaptive
pseudo labeling and informative active annotation.
InProceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 20666–
20676.
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,
Nedim Lipka, Diyi Yang, and Tong Sun. 2023b. En-
hanced visual instruction tuning for text-rich image
understanding. In NeurIPS 2023 Workshop on In-
struction Tuning and Instruction Following .
Bo Zhao, Boya Wu, and Tiejun Huang. 2023. Svit:
Scaling up visual instruction tuning. arXiv preprint
arXiv:2307.04087 .
Qiang Zhou, Zhibin Wang, Wei Chu, Yinghui Xu,
Hao Li, and Yuan Qi. 2023. Infmllm: A unified
framework for visual-language tasks. arXiv preprint
arXiv:2311.06791 .
11
