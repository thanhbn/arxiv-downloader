# 2303.05668.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2303.05668.pdf
# File size: 697261 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
UNFUSED : UN SUPERVISED F INETUNING U SING SE LF SUPERVISED D ISTILLATION
Ashish Seth2?, Sreyan Ghosh1?, S. Umesh2, Dinesh Manocha1
1University of Maryland, College Park, USA
2Speech Lab, Department of Electrical Engineering, IIT Madras, Chennai, India
ABSTRACT
In this paper, we introduce UnFuSeD , a novel approach to
leverage self-supervised learning and reduce the need for
large amounts of labeled data for audio classiﬁcation. Un-
like prior works, which directly ﬁne-tune a self-supervised
pre-trained encoder on a target dataset, we use the encoder
to generate pseudo-labels for unsupervised ﬁne-tuning be-
fore the actual ﬁne-tuning step. We ﬁrst train an encoder
using a novel self-supervised learning algorithm (SSL) on
an unlabeled audio dataset. Then, we use that encoder to
generate pseudo-labels on our target task dataset via cluster-
ing the extracted representations. These pseudo-labels are
then used to guide self-distillation on a randomly initialized
model, which we call unsupervised ﬁne-tuning . Finally, the
resultant encoder is ﬁne-tuned on our target task dataset.
Through UnFuSeD, we propose the ﬁrst system that moves
away from generic SSL paradigms in literature, which pre-
train and ﬁne-tune the same encoder, and presents a novel
self-distillation-based system to leverage SSL pre-training
for low-resource audio classiﬁcation. In practice, UnFuSeD
achieves state-of-the-art results on the LAPE Benchmark,
signiﬁcantly outperforming all our baselines. Additionally,
UnFuSeD allows us to achieve this at a 40% reduction in
the number of parameters over the previous state-of-the-art
system. We make all our codes publicly available1.
Index Terms —audio, speech, self-supervision
1. INTRODUCTION
Self-Supervised Learning (SSL) has proven to be one of the
biggest success of the past decade enabling deep learning
models to learn useful representations under low-resource
labeled data settings. SSL has been adopted successfully
in speech [1], vision [2, 3], and text [4] outperforming all
prior-art trained with only labeled data supervision on several
benchmark datasets [5]. Though current SSL models pre-
trained using Masked Acoustic Modeling (MAM) have been
shown to generalize well over speech tasks like Automatic
Speech Recognition (ASR), Phoneme Recognition (PR), etc.,
they fail to perform well on non-speech tasks like acoustic
?These authors contributed equally to this work
1https://github.com/Sreyan88/LAPEscene classiﬁcation [6]. We list some possible reasons for this
phenomenon in Section 2. Thus, we emphasize the impor-
tance of learning general-purpose audio representations that
can generalize over both speech and non-speech tasks, which
is currently largely understudied in the literature compared to
SSL in speech using MAM.
In the recent past, researchers have proposed novel al-
gorithms for learning general-purpose audio representations
[7, 8, 9]. A common trait among all these systems is that they
directly ﬁne-tune the model post-SSL pre-training. However,
this direct ﬁne-tuning approach may result in sub-optimal
performance due to signiﬁcant discrepancy between the pre-
training and ﬁne-tuning domains [10]. For example, most
of these systems perform SSL pre-training on the AudioSet
[11] (every day sounds like the sound of a toothbrush) and
evaluate their learned representations on tasks like Speaker
Veriﬁcation [12] (human spoken utterances). Additionally,
under the linear evaluation setup, we argue that the down-
stream tasks cannot leverage the SSL representations to their
full extent due to their learning capacity being constrained to
an afﬁne transform.
Main Contributions: We present UnFuSeD, a new frame-
work to improve downstream audio classiﬁcation perfor-
mance in low-resource labeled data settings leveraging SSL.
Unlike all prior systems in literature, UnFuSeD does not
directly ﬁne-tune an SSL pre-trained model but uses it to
extract and cluster audio features to generate pseudo-labels
on a downstream task dataset which is then used to perform
un-supervised ﬁne-tuning . More precisely, we perform a step
ofself distillation , guided by the generated pseudo-labels, on
a randomly initialized convnet encoder, divided into student
and teacher encoders. Finally, post unsupervised ﬁne-tuning,
we perform supervised ﬁne-tuning and evaluate downstream
task performance on our model linear evaluation setup. Ad-
ditionally, to pre-train our encoder using SSL, we propose
a novel SSL algorithm by modifying over DECAR [13].
Fig.1 shows a clear pictorial representation of our complete
training process. We emphasize that UnFuSeD changes the
paradigm in which SSL is leveraged to tackle data scarcity
and improve downstream task performance. In practice, Un-
FuSeD achieves state-of-the-art (SOTA) performance on the
LAPE Benchmark [7] with an encoder with 40% fewer
parameters than the current SOTA model on LAPE.arXiv:2303.05668v2  [eess.AS]  18 May 2023

--- PAGE 2 ---
F AF AF A
Encoder
Prototype
AugmentationSSL Pre-training 
Block-1
Block-2
Block-3
Block-4
projector
 projector
 projector
Student  T eacher
ClassiﬁerClustering
Clustering
pseudolabels
Encoder  
( Student ) 
Linear Layer
Unsupervised Fine-tuningLinear Evaluation
Encoder
Mean Square Error
KL-Divergence
Cross Entropy
Frozen
Projector
Initialize with centroidFig. 1 .Illustration of UnFuSeD : UnFuSeD follows a 3 step training process from upstream SSL pre-training to downstream task-speciﬁc
ﬁne-tuning. 1SSL pre-training. We ﬁrst pre-train an convnet using un-labeled audio using DECAR-v2 (described in Section 3). 2
Unsupervised Fine-tuning. We now pass the downstream task-speciﬁc data through the upstream model pre-trained in the last stage and
extract and cluster these representations to generate pseudo-labels. These pseudo-labels are then used to perform unsupervised ﬁne-tuning on
a randomly initialized convnet. 3Linear Evaluation. Finally, a task-speciﬁc linear head is added to the convnet obtained from the previous
step, and we perform supervised ﬁne-tuning on the task-speciﬁc labeled dataset keeping the convnet frozen.
2. RELATED WORK
Self Supervised Learning in Speech and Audio. The past
decade has seen massive success in self-supervised learning
in vision (CV), speech (SLP), and text (NLP), pushing the
boundaries of low-resource representation learning for down-
stream classiﬁcation [1, 4]. The most common systems for
SSL with speech solve a Masked Acoustic Modelling (MAM)
task, either using contrastive learning [1], frame reconstruc-
tion [14], or pseudo-label prediction [15]. However, recent
research has shown that solving MAM makes the model rep-
resentations mimic human articulatory responses [16], thus
making it unsuitable for non-speech tasks. Thus, in the re-
cent past, researchers have proposed novel systems to learn
audio representations that can generalize over both speech
and non-speech tasks. Following SSL in speech, these sys-
tems either solve a contrastive learning-based instance dis-
crimination task [8], a clustering-based pseudo-label predic-
tion task [13], or a reconstruction task [9]. Knowledge dis-
tillation has shown great success in CV , and NLP with major
applications in model compression [17]. In a supervised set-ting, researchers have explored knowledge distillation for au-
tomatic speech recognition, speech emotion recognition, and
speaker veriﬁcation [18]. DistillHubert [19] was the ﬁrst work
on distilling SSL-based speech models and performs layer-
wise knowledge-distillation (KD) to compress a full HuBERT
[15]. On the other hand, when both encoder architectures are
the same, this is known as self distillation (SD) [20], and has
shown impressive results with the student often outperform-
ing the original teacher. However, to our knowledge, no ex-
isting work leverages SD for general-purpose self-supervised
audio representation learning, and we are the ﬁrst to explore
this through UnFuSeD.
3. METHODOLOGY
Fig.1 illustrates our proposed UnFuSeD learning algorithm.
Algorithm 1 provides a detailed algorithmic overview of the
same. In practice, UnFuSeD has three main steps, namely, (1)
Upstream SSL Pre-training, (2) Unsupervised Fine-tuning,
and (3) Downstream Supervised Fine-tuning. In the next
paragraphs, we describe each step in detail.

--- PAGE 3 ---
(1) Upstream SSL Pre-training. LetXprebe an unlabeled
dataset of size JwhereXpre=fx1;;xj;;xJg. In our
case, hereJ= 0.25 million, following the exact pre-training
setup proposed by the LAPE benchmark. Our primary aim is
to learn general-purpose audio representations from this unla-
beled audio dataset. To achieve this, we use a simple convnet-
based architecture [21, 22], popular in prior-art [7, 9] for a
fair comparison. For upstream SSL pre-training, we propose
DECAR-v2, an improved version of DECAR [13], based on
ﬁndings in [23]. DECAR-v2 has two main steps or phases:
(a) Assignment Phase and the (b) Training Phase.
(a) Assignment Phase: The primary purpose of this phase is
to obtain “pseudo-labels” qfor every unlabelled audio sam-
plex2Xpre. To achieve this, we ﬁrst store all the embed-
dingsg~ xobtained from our convnet projection head hproj in
memory for the entire Xpre. After this, we apply Spherical
K-means to cluster and get the “pseudo-labels” qfor everyx
as follows: minC2RdK1
NPN
n=1minq g~ x>Cqwhere Cis
the Centroid matrix. Both g~ xand columns of Carel2normal-
ized.Krepresents the number of clusters, and ~xxis an
augmented and sampled version of the original audio sample.
Additionally, for ConvNet training stability, we keep the pro-
totype head hprotparameters frozen throughout pre-training,
and at the end of every assignment phase, the parameters of
hprotare replaced by C.
(b) Training Phase: We train the network using supervi-
sion from the “pseudo-labels” qobtained from the assign-
ment phase. To do this, we ﬁrst obtain the prediction pusing
softmax (z)wherezis the output of the hprot. Post this step;
we minimize the multinomial logistic loss between pandq
with:`(p;q) = P
kq(k)logp(k). The “pseudo-labels” are
kept ﬁxed during the training phase and updated for the entire
Xonly once every epoch during the assignment phase. Simi-
lar to [23], the assignment phase and training phase take place
in isolation only at the ﬁrst epoch, after which we use the em-
beddings g~ xobtained from the previous epoch. These em-
beddings are stored in memory at every iteration of an epoch
right after the back-propagation step.
(2) Unsupervised Downstream Fine-tuning. After SSL pre-
training, we don’t ﬁne-tune the pre-trained convnet fpreon
the target task dataset directly but instead, use it for unsuper-
vised ﬁne-tuning on a randomly initialized convnet fsd. We
call this step unsupervised ﬁne-tuning as we use the target
task dataset but without using its actual labels. Let Dtarg=
fXtarg;Ytarggbe the target task labeled dataset of size I
whereYtarg are labels associated with audio samples Xtarg.
For unsupervised ﬁne-tuning, we ﬁrst generate Ypseduo by ex-
tracting and clustering representations obtained on passing
Xtarg throughfpre. DECAR-v2 generates clusterable em-
beddings, which helps in the Ypseudo generation. We then
useYpseduo to perform self-distillation on fsd. We ﬁrst di-
videfsd, which follows a similar architecture to fpre, into a
student (fs
sd) and teacher network ( ft
sd).fsdhas 4 individual
blocks, where the ﬁrst 3 make fs
sdand the last block makesAlgorithm 1: UnFuSeD
// SSL-pretraining
Data: datasetXpre; number of clusters K; epochE; batch
sizeN
forepoch = 1toEdo
Sample a mini batch Xn
prefromXpre
Perform augmentations on Xpreto get ~Xpre
Compute feature embedding obtained from encoder
fpre(~Xpre)and obtainz=hproj(fpre(~Xpre))
Initialize weights of hprotwith centroid matrix C
obtained byKmeans (z).
ComputeYpseudo forXpreusingC
Minimize the cross-entropy Lce(Yn
pseudo;^Yn)where
^Yn=softmax (hprot(zn))
Updatefpre,hproj using gradient descent
end
// Self-Distillation
Data: target datasetXtarg; number of classes t; epochE;
batch sizeN
forepoch = 1toEdo
Sample a mini batch Xn
targ fromXtarg
ComputeYpseudo usingKmeans (fpre(Xtarg))
whereK=t
Computezi=hi
proj(fi
sd(Xn
targ))andl=
hcl(fsd(Xn
targ))for each Block biwhere
i2f1;2;3g
Compute Cross-Entropy Li
ce(zi;Yn
pseudo ),
KL-divergence Li
kl(zi;l)and MSE
Li
mse(zi;fsd(Xtarg))loss for each Block biwhere
i2f1;2;3g(use Eq:1)
Combine all losses Lallwith appropriate parameters ,
as stated in Eq:1
Updatefsd,hi
proj for each Block biwhere
i2f1;2;3gandhclusing gradient descent
end
ft
sd. For more details on the architecture of fs
sd, we refer our
readers to [7, 21]. For self-distillation, we treat each block
bias a separate classiﬁer and add a linear transform hi
proj to
bito solve three losses parallelly, KL-divergence Lkl, Mean-
Square error Lmseand Cross Entorpy Lce.Lceensures that
the student blocks correctly classify the pseudo labels Ypseduo
and thus utilize the weak supervision knowledge hidden in
them.Lmseensures that knowledge of the deepest layers is
leveraged to improve feature extraction in shallow layers. Lkl
ensures that the classiﬁcation results of student classiﬁers are
similar to that of the teacher classiﬁer. Finally, to optimize
our network, we use a weighted average of Lkl,Lmse and
Lce, which we weigh by ,as shown:
Lall=Lce+3X
i=1Li
ce+ (1 )3X
i=1Li
kl+3X
i=1Li
mse (1)
Lce Lce(l;Ypseudo );Li
ce Li
ce(zi;Ypseudo );
Li
kl Li
kl(zi;l);Li
mse Li
mse(zi;fsd(Xtarg)))
wherezi=hi
proj(fi
sd(Xtarg));l=hcl(fsd(Xtarg))
(3) Supervised Downstream Fine-tuning Post unsupervised
downstream ﬁne-tuning, we do supervised downstream ﬁne-

--- PAGE 4 ---
Table 1 . Result comparison of various SSL methods with proposed method DECAR-v2 andUnFuSeD on the linear evaluation
setup with frozen encoder. The best results for each task are presented in bold. UnFuSeD outperforms all our baselines.
DT BYOL-A SimCLR DECAR-v1 DeLoRes-S MoCo DeLoRes-M DECAR-v2 UnFuSeD
Speech
SC-V1   77.3 82.3 86.1 93:6 94:0 91.6 94.4
SC-V2(12) 91.0 77.2 83.0 85.4 93:2 93:3 90.6 94.1
SC-V2(35) 92.2 66.0 73.6 80.0 89:3 89:7 87.2 90.1
LBS   89.0 91.0 90.0 95:5 95:7 92.5 97.0
VC 40.1 28.9 25.6 31.2 42:5 45:3 33.0 50.0
IC   59.8 63.2 60.7 65:1 65:2 65.2 66.0
VF 90.2 69.2 74.1 76.5 87:3 88:0 78.2 89.8
Non-Speech
NS 74.1 61.3 70.7 66.3 74:7 75:0 69.8 76.4
BSD   85.2 87.7 86.7 89:0 89:6 88.5 90.0
TUT   52.4 62.5 58.6 66:7 65:7 64.6 66.8
US8K 79.1 69.1 70.1 71.2 81:2 82:7 73.2 83.2
Average   66.9 71.2 72.1 79:8 80:4 75.8 81.6
tuning on the student model fsdusingDtarg. For a fair com-
parison with prior-art in this space, we don’t train all the lay-
ers of our model and instead just train a task-speciﬁc linear
head added to the encoder. This method of training is also
known as linear evaluation and proves to be an effective tech-
nique for evaluating learned audio representations.
4. EXPERIMENTAL SETUP
Datasets. In our experiments, we use the exact same up-
stream and downstream training setups proposed by LAPE
[7]. For SSL-based pre-training, we use a balanced subset of
10% of the complete AudioSet (0.2 million) and the FSD50K
[24]. For downstream tasks (DT), we evaluate our learned
representations on LibriSpeech (LBS) [25] and V oxCeleb
(VC) [26] for speaker identiﬁcation, Speech Commands (SC)
v1 and v2 [27] for keyword spotting, V oxForge (VF) [12]
for language identiﬁcation, IEMOCAP (IC) [28] for speech
emotion recognition, NSynth [29] for TUT Urban [6] and
US8K [30] for acoustic event classiﬁcation and ﬁnally Bird
Song Detection (BSD) [31].
Hyperparameter Tuning. For SSL Pre-training (DECAR-
v2), we ﬁnd the optimal values for the number of clusters as
512, learning rate as 0.005, batch size as 512, and number of
epochs as 100. Projector hproj performs a R2048!R512
non-linear transformation using multiple linear-layers. For
Unsupervised Fine-tuning, we use the learning rate as 0.007,
batch size as 512, number of epochs as 50, as 0.7, and
as 0.003.hcfperforms a R2048!Rtlinear transforma-
tion, wheretis number of classes in target dataset. Projectors
h1
proj,h2
proj andh3
proj perform R2048!Rt,R1024!Rt
andR512!Rtnon-linear transformations respectively. Fi-
nally, for Linear Evaluation, we use the learning rate as 0.001,
batch size as 32, and number of epochs as 50. All the hy-perparameter choices were made based on an extensive grid
search while considering the average performance across all
the downstream tasks.
5. RESULTS AND RESULT ANALYSIS
As clearly evident from Table 1, UnFuSeD outperforms all
other approaches in literature by a signiﬁcant margin. Re-
sults of BYOL-A were borrowed from their original papers.
SimCLR was proposed as the pre-training approach in COLA
[32] and was repeated on our convnet encoder using LAPE
upstream dataset settings. We hypothesize that the gap in re-
sults from the original paper may be due to using a powerful
encoder and 10more data from the AudioSet used in the pa-
per. Measuring the effect of change in encoders is beyond the
scope of this paper. Our proposed DECAR-v2 outperforms
the already proposed DECAR-v1 by a margin of 4.6% (av-
eraged across all tasks). Additionally, UnFuSeD outperforms
DECAR-v2 by a margin of 5.8% (averaged across all tasks).
Owing to space constraints, we provide results of UnFuSeD
with different SSL training frameworks on our GitHub. Ad-
ditionally, our ﬁnal convnet encoder fs
sdused for downstream
task evaluation has 40% fewer parameters than DeLoRes-
M [7] (current SOTA system on the LAPE Benchmark).
6. CONCLUSION
In this paper, we propose UnFuSeD, a novel methodology to
leverage SSL for low-resource audio classiﬁcation. In prac-
tice, UnFuSeD signiﬁcantly outperforms all other approaches
in literature on the LAPE audio evaluation benchmark. Addi-
tionally, we propose a new SSL algorithm called DECAR-v2
to learn general-purpose audio representations from unlabeled
data.

--- PAGE 5 ---
7. REFERENCES
[1] Baevski et al., “wav2vec 2.0: A framework for self-
supervised learning of speech representations,” NeurIPS
2020 , vol. 33, pp. 12449–12460.
[2] Grill et al., “Bootstrap your own latent-a new approach
to self-supervised learning,” NeurIPS 2020 , vol. 33, pp.
21271–21284.
[3] He et al., “Momentum contrast for unsupervised visual
representation learning,” in IEEE CVPR 2020 .
[4] Devlin et. al, “Bert: Pre-training of deep bidirec-
tional transformers for language understanding,” arXiv
preprint arXiv:1810.04805 , 2018.
[5] Yang et al, “Superb: Speech processing uni-
versal performance benchmark,” arXiv preprint
arXiv:2105.01051 , 2021.
[6] Annamaria Mesaros, Toni Heittola, and Tuomas Virta-
nen, “A multi-device dataset for urban acoustic scene
classiﬁcation,” 2018.
[7] Ghosh et al., “Decorrelating feature spaces for learning
general-purpose audio representations,” IEEE Journal
of Selected Topics in Signal Processing , pp. 1–13, 2022.
[8] Saeed et al., “Contrastive learning of general-purpose
audio representations,” in IEEE ICASSP 2021 , pp.
3875–3879.
[9] Niizumi et al, “Byol for audio: Self-supervised learn-
ing for general-purpose audio representation,” in IEEE
IJCNN 2021 , pp. 1–8.
[10] Lee et al, “Self-distillation for further pre-training of
transformers,” arXiv preprint arXiv:2210.02871 , 2022.
[11] Gemmeke et al, “Audio set: An ontology and human-
labeled dataset for audio events,” in IEEE ICASSP 2022 .
IEEE, 2017, pp. 776–780.
[12] V oxforge.org, “Free speech... recognition (linux, win-
dows and mac) - voxforge.org,” accessed 06/25/2014.
[13] Ghosh et al., “Deep clustering for general-purpose au-
dio representations,” arXiv preprint arXiv:2110.08895 ,
2021.
[14] Liu et al., “Mockingjay: Unsupervised speech represen-
tation learning with deep bidirectional transformer en-
coders,” in IEEE ICASSP 2020 , pp. 6419–6423.
[15] Hsu et al., “Hubert: Self-supervised speech represen-
tation learning by masked prediction of hidden units,”
IEEE/ACM TASLP , vol. 29, pp. 3451–3460, 2021.[16] Wu et al, “Speaker-independent acoustic-to-articulatory
speech inversion,” arXiv preprint arXiv:2302.06774 ,
2023.
[17] Jianping Gou, Baosheng Yu, Stephen J. Maybank, and
Dacheng Tao, “Knowledge distillation: A survey,” vol.
129, no. 6, pp. 1789–1819, jun 2021.
[18] Liu et al., “Self-knowledge distillation via feature en-
hancement for speaker veriﬁcation,” in IEEE ICASSP
2022 .
[19] Chang et al., “Distilhubert: Speech representation learn-
ing by layer-wise distillation of hidden-unit bert,” in
IEEE ICASSP 2022 .
[20] Pham at al., “Revisiting self-distillation,” arXiv preprint
arXiv:2206.08491 , 2022.
[21] Koizumi et al., “The ntt dcase2020 challenge task
6 system: Automated audio captioning with key-
words and sentence length estimation,” arXiv preprint
arXiv:2007.00225 , 2020.
[22] Takeuchi et al, “Effects of word-frequency based
pre-and post-processings for audio captioning,” arXiv
preprint arXiv:2009.11436 , 2020.
[23] Caron et al., “Unsupervised learning of visual features
by contrasting cluster assignments,” NeurIPS 2022 .
[24] Fonseca et al., “Fsd50k: an open dataset of human-
labeled sound events,” IEEE/ACM TASLP , vol. 30, pp.
829–852, 2021.
[25] Panayotov et al., “Librispeech: An asr corpus based on
public domain audio books,” in IEEE ICASSP 2015 , pp.
5206–5210.
[26] Nagrani at al., “V oxceleb: A large-scale speaker identi-
ﬁcation dataset,” ISCA Interspeech 2017 .
[27] Pete Warden, “Speech commands: A dataset for
limited-vocabulary speech recognition,” 2018.
[28] Busso et al., “Iemocap: Interactive emotional dyadic
motion capture database,” LREC 2008 , vol. 42, no. 4,
pp. 335–359.
[29] Engel et al., “Neural audio synthesis of musical notes
with wavenet autoencoders,” in ICML 2017 .
[30] Justin Salamon, Christopher Jacoby, and Juan Pablo
Bello, “A dataset and taxonomy for urban sound re-
search,” in ACM MM 2014 , 2014, p. 1041–1044.
[31] Stowell et al., “Automatic acoustic detection of birds
through deep learning: the ﬁrst bird audio detection
challenge,” Methods in Ecology and Evolution 2019 .
[32] Wang at al., “Towards learning universal audio repre-
sentations,” in IEEE ICASSP 2022 , pp. 4593–4597.
