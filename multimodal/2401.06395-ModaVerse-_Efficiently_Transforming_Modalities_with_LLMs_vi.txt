# ModaVerse: Chuyển đổi các phương thức hiệu quả với LLM

Xinyu Wang
Đại học Adelaide
Bohan Zhuang
Đại học Monash
Qi Wu
Đại học Adelaide

Hình 1. Minh họa so sánh các mô hình MLLM: (a) Huấn luyện trước đa phương thức, nơi các mô-đun mới như bộ mã hóa và giải mã hình ảnh được tích hợp trong khung LLM tiêu chuẩn. (b) Huấn luyện Bộ chuyển đổi, minh họa việc sử dụng các lớp chiếu để kết nối LLM với các mô-đun có sẵn. (c) LLM như một Agent, nổi bật ứng dụng chiến lược của prompts kết hợp với các công cụ bên ngoài. (d) Bộ chuyển đổi+Agent (của chúng tôi), chuyển đổi các phương thức với việc căn chỉnh Đầu vào/Đầu ra (I/O) dựa trên ngôn ngữ hiệu quả. E, D, và L đại diện cho Bộ mã hóa, Bộ giải mã, và Lớp tuyến tính tương ứng. T-to-x biểu thị mô hình sinh text-to-x, trong đó x có thể là Hình ảnh, Video, và Âm thanh.

## Tóm tắt

Con người sở hữu khả năng hiểu các phương thức đa dạng và chuyển đổi thông tin giữa chúng một cách liền mạch. Trong nghiên cứu này, chúng tôi giới thiệu ModaVerse, một Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) có khả năng hiểu và chuyển đổi nội dung qua các phương thức khác nhau bao gồm hình ảnh, video, và âm thanh. Các khung MLLM chủ đạo đã chủ yếu dựa vào việc căn chỉnh không gian tiềm ẩn của các đặc trưng văn bản và phi văn bản. Quá trình căn chỉnh này, đồng bộ hóa mô hình ngôn ngữ được huấn luyện trên dữ liệu văn bản với các bộ mã hóa và giải mã được huấn luyện trên dữ liệu đa phương thức, thường đòi hỏi huấn luyện mở rộng của nhiều lớp chiếu trong nhiều giai đoạn. Lấy cảm hứng từ các phương pháp LLM-như-agent, chúng tôi đề xuất một cơ chế căn chỉnh Đầu vào/Đầu ra (I/O) mới hoạt động trực tiếp ở cấp độ ngôn ngữ tự nhiên. Nó căn chỉnh đầu ra của LLM với đầu vào của các mô hình sinh, tránh những phức tạp liên quan đến căn chỉnh đặc trưng tiềm ẩn, và đơn giản hóa các giai đoạn huấn luyện đa tầng của các MLLM hiện có thành một quy trình duy nhất, hiệu quả. Bằng cách tiến hành thí nghiệm trên nhiều benchmark, chúng tôi chứng minh rằng phương pháp của chúng tôi đạt được hiệu suất tương đương với công nghệ tiên tiến nhất trong khi đạt được hiệu quả đáng kể trong việc sử dụng dữ liệu. Mã nguồn có sẵn tại https://github.com/xinke-wang/ModaVerse.

## 1. Giới thiệu

Trải dài từ các văn khắc cổ đại đến các bách khoa toàn thư trực tuyến đương đại, văn bản đã phục vụ như phương tiện tinh túy nhất để ghi chép sự mở rộng của kiến thức nhân loại. Việc tích lũy dữ liệu văn bản rộng lớn như vậy cung cấp một nền tảng phong phú cho việc huấn luyện Mô hình Ngôn ngữ Lớn (LLM) [4, 30, 31, 42, 43]. Thông qua việc huấn luyện mở rộng trên các corpus khổng lồ, LLM trải qua một quá trình biến đổi, một hiện tượng được nắm bắt bởi khái niệm nơi sự gia tăng về lượng dẫn đến sự thay đổi hành vi về chất [47], do đó nổi lên với các khả năng lý luận giống con người. Điều này cho phép chúng hiểu và phản hồi các hướng dẫn của con người với độ chính xác đáng chú ý. Năng lực như vậy mở rộng đáng kể phạm vi ứng dụng LLM qua các lĩnh vực khác nhau, như chatbot, copilot lập trình, và agent robot.

Tuy nhiên, sự ra đời của các hình thức giao tiếp phong phú hơn đòi hỏi một sự phát triển vượt ra ngoài những giới hạn truyền thống của văn bản. Trong thời đại mà "một hình ảnh đáng giá ngàn từ", khả năng diễn giải và tích hợp dữ liệu hình ảnh và âm thanh phức tạp là vô giá. Việc theo đuổi cho phép LLM xử lý và tạo ra thông tin vượt ra ngoài dữ liệu văn bản phản ánh sự tiến bộ tự nhiên của AI, khao khát mô phỏng toàn bộ chiều rộng của giao tiếp con người. Điều này đã thúc đẩy sự phát triển của LLM Đa phương thức (MLLM), được thiết kế để hiểu, chuyển đổi, và tạo ra nội dung qua các phương thức khác nhau, như hình ảnh, âm thanh, và video. Sự quan tâm ngày càng tăng này đã khuyến khích sự bùng nổ của nghiên cứu và đổi mới trong lĩnh vực [10, 16, 24, 39, 46, 48, 52, 54, 57]. Để giải quyết những hạn chế của LLM chỉ văn bản truyền thống, huấn luyện trước đa phương thức, huấn luyện bộ chuyển đổi, và LLM như một agent nổi lên như ba mô hình chính để trang bị cho LLM các khả năng đa phương thức. Hình 1 so sánh sơ đồ tổng quan của mô hình hiện có, đánh giá hiệu suất và hiệu quả của nó qua ba chiều. Cụ thể, Độ phức tạp Huấn luyện đề cập đến khối lượng dữ liệu huấn luyện, tài nguyên tính toán tiêu thụ, và số lượng giai đoạn huấn luyện liên quan. Tính nhất quán biểu thị mức độ mà đầu ra bị ảnh hưởng bởi các sửa đổi đối với đầu vào hoặc prompts. Tính linh hoạt liên quan đến khả năng cho nhiều cách diễn giải và tạo ra đầu ra dưới các điều kiện đa dạng.

Huấn luyện trước Đa phương thức (Hình 1 (a)) mở rộng khung LLM truyền thống để chứa đầu vào và đầu ra phi văn bản bằng cách tích hợp các bộ mã hóa và giải mã phương thức bổ sung vào khung hiện có. Thông qua các tác vụ huấn luyện trước được thiết kế tùy chỉnh, LLM học cách biểu diễn nhiều phương thức hiệu quả, đạt được tính nhất quán và linh hoạt vượt trội so với các mô hình hiện có. Tuy nhiên, việc thích ứng LLM dựa trên văn bản, đã được huấn luyện trước trên dữ liệu văn bản mở rộng, vào bối cảnh đa phương thức thường đòi hỏi tinh chỉnh đáng kể hoặc thậm chí huấn luyện lại hoàn toàn. Do đó, sự thích ứng này đòi hỏi tài nguyên tính toán đáng kể. Ví dụ, Emu [39] kết hợp Eva-CLIP [38], LLaMA [42], và Stable Diffusion [33] để phát triển mô hình đa phương thức nền tảng. Sự phát triển này bao gồm giai đoạn huấn luyện trước chuyên sâu trên tập dữ liệu quy mô lớn và việc sử dụng hàng trăm GPU.

Huấn luyện Bộ chuyển đổi (Hình 1 (b)) cung cấp một giải pháp thay thế tiết kiệm tính toán. Chiến lược này, được chứng minh bởi BLIP-2 [18] và MiniGPT-4 [57], thường được xây dựng dựa trên các LLM và bộ mã hóa/giải mã đa phương thức được thiết lập tốt. Nó bao gồm việc tích hợp các mô-đun đa phương thức này với LLM bằng cách huấn luyện một tập hợp các lớp chiếu trong khi giữ các tham số của LLM được đóng băng hoặc tinh chỉnh sử dụng các kỹ thuật hiệu quả tham số như LoRA [12]. Các lớp này dịch các biểu diễn phi văn bản, như đặc trưng hình ảnh, vào miền văn bản của LLM, do đó tránh huấn luyện mở rộng nhưng bảo tồn tính linh hoạt. Tuy nhiên, mặc dù giảm khối lượng dữ liệu huấn luyện và thời gian, các phương pháp này vẫn yêu cầu một quy trình huấn luyện phức tạp. Ví dụ, NExT-GPT [48] sử dụng quy trình huấn luyện ba bước nơi các lớp chiếu phía mã hóa/giải mã và bộ chuyển đổi LLM mỗi cái được huấn luyện trong các giai đoạn riêng biệt. Thiết lập phức tạp này làm tăng đáng kể độ phức tạp và dẫn đến sự dư thừa trong quá trình huấn luyện.

LLM như một Agent (Hình 1 (c)) chứng minh một khung không cần huấn luyện. Các phương pháp này sử dụng khả năng suy luận zero-shot của LLM, nhấn mạnh việc tạo prompt chiến lược và thiết kế quy trình làm việc. Cách tiếp cận này hướng dẫn việc diễn giải và tạo ra nội dung đa phương thức thông qua tương tác với các công cụ bên ngoài. Ví dụ, HuggingGPT [35] đã phát triển một quy trình bốn bước khuyến khích ChatGPT của OpenAI chọn và thực thi các mô hình từ kho mô hình của HuggingFace, do đó giải quyết nhiều tác vụ khác nhau. Tuy nhiên, điều quan trọng là nhận ra rằng các phương pháp này, thiếu huấn luyện có mục tiêu, thường dựa vào các mô hình x-to-text hoặc text-x, để xử lý đầu vào phi văn bản. Sự phụ thuộc này có thể dẫn đến tính linh hoạt hạn chế trong việc xử lý các loại dữ liệu đa dạng. Ngoài ra, sự phụ thuộc nặng nề vào thiết kế của prompt hệ thống và khả năng lý luận của LLM có thể dẫn đến kết quả không nhất quán.

Cho đến nay, mỗi mô hình trình bày một cách tiếp cận chuyên biệt để đạt được chức năng trong MLLM, mỗi cái có ưu điểm và hạn chế riêng. Xem xét các đánh đổi này, việc khám phá sự tích hợp các điểm mạnh của chúng thành một cách tiếp cận gắn kết là hấp dẫn. Cụ thể, bài báo này đề xuất Bộ chuyển đổi+Agent, một cách tiếp cận nhằm tìm ra sự cân bằng hài hòa giữa hiệu quả của các phương pháp LLM-như-agent và tính linh hoạt của các phương pháp huấn luyện bộ chuyển đổi.

Bộ chuyển đổi+Agent (Hình 1 (d)) nhằm kết hợp các lợi ích của huấn luyện bộ chuyển đổi với các phương pháp LLM-như-agent. Như được hiển thị trong hình, để duy trì tính linh hoạt của việc chấp nhận các kết hợp tùy ý của các phương thức đầu vào, chúng tôi huấn luyện một tập hợp các bộ chuyển đổi tuyến tính để ánh xạ các đặc trưng phi văn bản của đầu vào vào không gian văn bản của LLM. Cách tiếp cận này cho phép mô hình hiểu đầu vào đa phương thức trong khi bảo tồn hiệu quả huấn luyện bằng cách chỉ tinh chỉnh các bộ chuyển đổi. Đối với đầu ra, chúng tôi áp dụng thiết kế LLM-như-agent, sử dụng các mô hình text-to-x được thiết lập để tạo ra đầu ra phi văn bản. Chiến lược này tránh nhu cầu tinh chỉnh các lớp chiếu phía đầu ra bổ sung, do đó tăng cường hiệu quả. Thách thức chính trong khung Bộ chuyển đổi+Agent là căn chỉnh đầu ra của LLM với đầu vào của các mô hình text-to-x. Để giải quyết điều này, chúng tôi giới thiệu Căn chỉnh Đầu vào/Đầu ra (I/O). Trái ngược với các cách tiếp cận dựa trên bộ chuyển đổi trước đó tập trung vào căn chỉnh cấp độ đặc trưng giữa LLM và các mô hình sinh, chiến lược Căn chỉnh I/O của chúng tôi khuyến khích LLM tạo ra các meta-response được căn chỉnh ngôn ngữ. Các meta-response này chứa các hướng dẫn chi tiết để kích hoạt các mô hình sinh. Chúng tôi đạt được Căn chỉnh I/O này thông qua quá trình tinh chỉnh tuân theo hướng dẫn. Kết quả là, trong một giai đoạn tinh chỉnh duy nhất, LLM được trang bị để gọi các mô hình bên ngoài để tạo ra đầu ra phi văn bản, do đó bỏ qua căn chỉnh cấp độ đặc trưng phức tạp thường được yêu cầu trong mô hình huấn luyện bộ chuyển đổi.

Tóm lại, các đóng góp kỹ thuật của bài báo này là:

• Chúng tôi giới thiệu một mô hình huấn luyện Bộ chuyển đổi+Agent mới cho Mô hình Ngôn ngữ Lớn Đa phương thức tổng hợp các điểm mạnh của cả huấn luyện bộ chuyển đổi và phương pháp LLM-như-Agent. Sự tích hợp này hiệu quả gặt hái các lợi ích của hiệu quả huấn luyện và tính linh hoạt của mô hình.

• Để giải quyết các thách thức căn chỉnh vốn có trong phương pháp LLM-như-Agent, chúng tôi đề xuất một chiến lược Căn chỉnh I/O. Chiến lược này khác biệt so với căn chỉnh cấp độ đặc trưng thông thường và thay vào đó hoạt động ở cấp độ ngôn ngữ tự nhiên, cung cấp một giải pháp thay thế hiệu quả hơn.

• Sản phẩm cuối cùng của chúng tôi, ModaVerse, chứng minh hiệu suất tương đương với công nghệ tiên tiến hiện tại trên nhiều benchmark được sử dụng rộng rãi trong khi yêu cầu ít dữ liệu và tài nguyên huấn luyện hơn, do đó cung cấp một lựa chọn hiệu quả hơn mà không làm giảm hiệu quả.

## 2. Công trình liên quan

**MLLM Huấn luyện trước Đa phương thức.** Huấn luyện trước đa phương thức không phải là khái niệm mới. Các nỗ lực đầu tiên [20, 25, 36] đã khám phá các cách để mở rộng khả năng của các mô hình ngôn ngữ để hiểu nội dung hình ảnh. Các mô hình này đạt được hiệu suất đầy hứa hẹn trên các tác vụ ngôn ngữ-tầm nhìn cụ thể [3] nhưng thất bại trong việc tổng quát hóa đến các tình huống chung hơn. Tuy nhiên, các tiến bộ gần đây đã tiết lộ rằng các kiến trúc mô hình đơn giản hơn có thể mang lại kết quả ấn tượng khi được huấn luyện trước quy mô lớn mở rộng, nhờ vào tài nguyên tính toán tiên tiến và tập dữ liệu đa dạng. Ví dụ, PaLI [6] chứng minh điều này bằng cách tích hợp một vision transformer với language transformer và huấn luyện trên tập dữ liệu mở rộng gồm 10 tỷ cặp hình ảnh-văn bản. Tương tự, CM3Leon [54] sử dụng kiến trúc transformer chỉ giải mã đơn giản, được huấn luyện trên 340 triệu cặp hình ảnh-văn bản. Cách tiếp cận này đã cho phép tính linh hoạt đáng chú ý trong việc tạo ra và chỉnh sửa cả văn bản và hình ảnh, cho thấy hiệu suất mạnh mẽ trong chuyển đổi hình ảnh-to-văn bản và văn bản-to-hình ảnh. Ngoài ra, để cho phép một LLM có thể tạo ra nội dung phi văn bản, Emu [39] kết hợp mô hình stable diffusion với LLaMA làm bộ giải mã, được huấn luyện trên corpus đa dạng gồm 82 triệu cặp hình ảnh-văn bản và video-văn bản. Sự tích hợp này đánh dấu một bước tiến đáng kể trong lĩnh vực, thể hiện tính đa dạng ngày càng tăng của LLM trong bối cảnh đa phương thức.

**MLLM Huấn luyện Bộ chuyển đổi:** Tận dụng các tiến bộ gần đây trong kỹ thuật tinh chỉnh hiệu quả tham số [12] và các phương pháp hiệu quả dữ liệu [24], nhiều nghiên cứu đã khám phá tính khả thi của việc huấn luyện bộ chuyển đổi để căn chỉnh đặc trưng giữa LLM và các mô-đun phi văn bản khác nhau. Flamingo [1] đại diện cho nỗ lực tiên phong trong việc đóng băng các tham số của cả bộ mã hóa hình ảnh và LLM, huấn luyện một tập hợp các lớp attention chéo có cổng để tích hợp kiến thức hình ảnh vào LLM. Tuy nhiên, nó vẫn cần huấn luyện mở rộng trên tập dữ liệu khổng lồ. Một ví dụ đáng chú ý khác là BLIP-2 [17], giới thiệu Q-Former dựa trên BERT để dịch đặc trưng hình ảnh thành biểu diễn văn bản, do đó cho phép LLM hiểu nội dung hình ảnh. Sự đổi mới này đã truyền cảm hứng cho nghiên cứu tiếp theo [5, 37], tiết lộ rằng cấu trúc Q-Former có thể được đơn giản hóa thêm thành một lớp tuyến tính duy nhất, giảm đáng kể số lượng tham số có thể huấn luyện. Tuy nhiên, những tiến bộ này, mặc dù cho thấy tiềm năng đáng kể, đã chủ yếu được áp dụng cho các tác vụ hình ảnh-văn bản. Trong nỗ lực tạo ra một MLLM đa năng hơn có thể xử lý một mảng loại đầu vào rộng hơn, các công trình tiếp theo [10, 37] đã thay thế bộ mã hóa hai phương thức, CLIP [32], bằng bộ mã hóa sáu phương thức đa năng hơn, ImageBind [10]. Ngoài ra, các công trình khác [8, 16, 48, 55] cố gắng căn chỉnh phía đầu ra của LLM với các mô hình sinh, cho phép sử dụng các đặc trưng tiềm ẩn từ LLM để hướng dẫn các mô hình sinh trong việc tạo ra nội dung phi văn bản. Ví dụ, một nghiên cứu đồng thời, NExT-GPT [48], giới thiệu quy trình huấn luyện đa giai đoạn. Quy trình này bao gồm một loạt các bộ chuyển đổi căn chỉnh LLM với các bộ mã hóa và mô hình sinh ở cấp độ đặc trưng. Công trình của chúng tôi khác biệt so với NExT-GPT ở chỗ nó căn chỉnh các mô hình sinh ở cấp độ ngôn ngữ thay vì cấp độ đặc trưng, do đó giảm đáng kể độ phức tạp huấn luyện.

**MLLM LLM-như-agent:** Khả năng suy luận zero-shot đáng chú ý của LLM cho phép chúng sử dụng hiệu quả các công cụ bên ngoài [26, 34, 35, 40, 53]. Tiềm năng này tạo thuận lợi cho việc tạo ra các pipeline chuyên biệt và prompt liên quan, hướng dẫn LLM hiểu hoặc tạo ra nội dung đa phương thức. Ví dụ, HuggingGPT [29] đã phát triển một pipeline đa bước. Trong quá trình này, ChatGPT ban đầu diễn giải các hướng dẫn của con người và chọn các mô hình phù hợp từ kho mô hình để hoàn thành các tác vụ đã cho. Sau đó, các đầu ra từ các mô hình bên ngoài này được đưa trở lại ChatGPT để phân tích và tạo ra phản hồi cuối cùng. Một ví dụ đáng chú ý khác là MM-React [53], giới thiệu việc tích hợp các chuyên gia tầm nhìn, như OCR, chú thích hình ảnh, và các mô hình phát hiện đối tượng, để mở rộng khả năng của LLM trong việc xử lý nội dung hình ảnh. Đối với mỗi cặp hình ảnh đầu vào và hướng dẫn, LLM sử dụng chuyên gia tầm nhìn liên quan để trích xuất thông tin có liên quan từ hình ảnh, sau đó tạo ra các phản hồi liên quan.

Hình 2 trình bày sơ đồ tổng quan của các MLLM được đề xuất gần đây. Nó chứng minh đặc tính của huấn luyện bộ chuyển đổi, nơi tất cả các mô hình đều tích hợp các thành phần chiếu bổ sung, như một lớp tuyến tính hoặc Q-former, hoặc trước hoặc sau LLM. Các thành phần này được sử dụng để căn chỉnh biểu diễn văn bản và phi văn bản giữa LLM và bộ mã hóa/giải mã. Ngược lại, các phương pháp huấn luyện trước đa phương thức thường có kiến trúc đơn giản và súc tích hơn. Chúng hướng dẫn bản thân LLM học các đặc trưng đa phương thức, do đó tránh các cấu trúc chiếu giữa các mô-đun khác nhau. Hơn nữa, các phương pháp LLM-như-agent sử dụng kho mô hình bên ngoài để hỗ trợ xử lý hoặc tạo ra nội dung phi văn bản, mà không tích hợp các mô-đun có thể huấn luyện vào hệ thống. So sánh, mô hình Bộ chuyển đổi+Agent được đề xuất tuân theo cấu trúc bộ chuyển đổi ở phía mã hóa, nơi các lớp chiếu tuyến tính được huấn luyện để căn chỉnh các đặc trưng đầu vào với không gian văn bản của LLM. Ở phía giải mã, LLM được coi như một agent để gọi các mô hình bên ngoài để tạo ra nội dung phi văn bản.

## 3. ModaVerse

### 3.1. Tổng quan Pipeline

Hình 3 minh họa khung toàn diện của ModaVerse được đề xuất, chứa ba khối chức năng, bao gồm chiếu đầu vào, tạo meta-response, và tạo phản hồi cuối cùng.

**Chiếu Đầu vào:** Để thích ứng LLM dựa trên văn bản thành MLLM có khả năng diễn giải đầu vào phi văn bản, điều cần thiết là căn chỉnh các đặc trưng văn bản của LLM với các phương thức khác nhau trong giai đoạn đầu vào. Nghiên cứu gần đây [5, 37, 57] đã chứng minh tính khả thi của việc căn chỉnh các phương thức khác nhau này sử dụng một lớp tuyến tính duy nhất. Theo đó, chúng tôi sử dụng ImageBind [10] như một bộ mã hóa thống nhất, xử lý đầu vào từ các loại dữ liệu đa dạng, bao gồm hình ảnh, video, và âm thanh, chuyển đổi chúng thành một embedding cụ thể. Tiếp theo, đối với mỗi phương thức, chúng tôi học một tập hợp các lớp chiếu tuyến tính để ánh xạ các biểu diễn được mã hóa này vào không gian văn bản của LLM. Kết quả là, ModaVerse có được khả năng hiểu đầu vào đa phương thức.

**Tạo Meta Response:** Vì LLM nền tảng được huấn luyện trước chỉ trên dữ liệu chỉ văn bản, nó thiếu khả năng trực tiếp tạo ra đầu ra phi văn bản. Để giải quyết hạn chế này, chúng tôi coi LLM nền tảng như một agent, được thiết kế để chỉ tạo ra meta-response. Như được mô tả trong Hình 3, meta-response bao gồm thông tin được định dạng bao gồm các chi tiết gọi. Ví dụ, theo meta-response, hệ thống có thể kích hoạt mô hình text-to-image để tạo ra hình ảnh dựa trên prompt "Một bức ảnh của một con mèo". Thiết kế này tránh nhu cầu huấn luyện lớp chiếu phía đầu ra bổ sung để căn chỉnh không gian đặc trưng của LLM với các mô hình sinh, do đó đơn giản hóa quá trình huấn luyện.

**Tạo Phản hồi Cuối cùng:** Khối này tích hợp nhiều mô hình text-to-x có thể thay thế để tạo ra phản hồi cuối cùng, có thể bao gồm hình ảnh [33], video [27], và âm thanh [22]. Dựa trên các chi tiết gọi được phân tích từ meta-response, một hoặc nhiều mô hình sẽ được kích hoạt để tạo ra đầu ra phi văn bản.

Cho đến nay, mô hình Bộ chuyển đổi+Agent đã trở nên rõ ràng. Trong mô hình này, chiếu đầu vào được thiết kế với một tập hợp các bộ chuyển đổi tuyến tính ánh xạ các đặc trưng đa phương thức vào không gian văn bản của LLM. Bản thân LLM được coi như một agent, gọi các mô hình bên ngoài để tạo ra các phản hồi cuối cùng. Lợi ích của thiết kế như vậy là:

• Huấn luyện các bộ chuyển đổi trong giai đoạn đầu vào bảo tồn các chi tiết trong dữ liệu đầu vào, đồng thời giảm khối lượng huấn luyện so với huấn luyện trước đa phương thức đầy đủ.
• Coi LLM như một agent trong giai đoạn đầu ra không chỉ tách rời nó khỏi các mô hình sinh bên ngoài, cho phép phương pháp plug-and-play mà còn loại bỏ nhu cầu các lớp chiếu bổ sung. Điều này có nghĩa là việc chạy các mô hình sinh trong giai đoạn huấn luyện là không cần thiết, do đó giảm độ phức tạp huấn luyện.

### 3.2. Căn chỉnh I/O

Xem xét LLM dựa trên văn bản: I→O với các tập hợp đầu vào và đầu ra được định nghĩa là I=O={text}, mục tiêu của ModaVerse là khám phá một phép biến đổi hiệu quả mở rộng LLM thành mô hình đa phương thức có khả năng xử lý I′=O′={text, image, video, audio}, được mô tả như sau:

P: ImageBind(I′)→O1,
LLM′: O1→O2,
M: O2→O′                                                                                                                        (1)

Mỗi dòng của phương trình này tương ứng với một giai đoạn được mô tả trong Hình 3. Dòng đầu tiên biểu thị một chiếu P có thể huấn luyện từ đặc trưng đa phương thức - được trích xuất bởi ImageBind [10] từ đầu vào I′ - vào không gian văn bản của LLM, O1. Dòng thứ hai bao gồm việc tinh chỉnh bộ chuyển đổi LoRA [12] để khuyến khích LLM được thích ứng, được định nghĩa là LLM′, tạo ra meta-response O2 từ đặc trưng đầu vào O1. Cuối cùng, dòng thứ ba, nơi M đại diện cho kho mô hình text-to-x được thiết lập, đóng băng, sử dụng meta-response được phân tích để tạo ra đầu ra đa phương thức cuối cùng O′. Để đạt được các mục tiêu này, chúng tôi đề xuất Căn chỉnh I/O tuân theo hướng dẫn. Căn chỉnh này nhằm đồng thời khớp các căn chỉnh I′→O1 và O1→O2. Như vậy, các thành phần có thể huấn luyện, như được mô tả trong Hình 3, bao gồm ba lớp tuyến tính và bộ chuyển đổi LoRA. Cụ thể, Căn chỉnh I/O bao gồm hai thành phần chính: xây dựng hướng dẫn, và tinh chỉnh các bộ chuyển đổi tuyến tính và LoRA.

Để thực hiện Căn chỉnh I/O, hai vấn đề phải được giải quyết. Thứ nhất, vì biểu diễn chính xác của O1 không thể thu được trực tiếp, các phương pháp dựa trên bộ chuyển đổi hiện có thường huấn luyện ánh xạ trực tiếp từ I′→O2, sử dụng chú thích từ các tập dữ liệu ghép đôi làm O2 để học các chiếu, đóng khung quá trình như một tác vụ chú thích đa phương thức. Tuy nhiên, trong trường hợp của chúng tôi, O2 là meta response thay vì chỉ mô tả văn bản của I′. Nghĩa là, cho các hướng dẫn và đầu vào đa phương thức đi kèm, đầu ra mong đợi nên cụ thể xác định mô hình nào sử dụng và cách sử dụng nó. Ví dụ, cho hướng dẫn, 'Tạo ra hình ảnh cho một con vật dựa trên clip âm thanh tiếng kêu của nó được cung cấp', cùng với clip âm thanh đi kèm ghi lại tiếng kêu của mèo, thông tin gọi mong đợi nên như sau: '{model: "text-to-image", prompts: "a photo of a cat"}'. Do đó, đơn giản sử dụng các tập dữ liệu x-to-text để huấn luyện các lớp chiếu dưới tác vụ chú thích hình ảnh là không thể tạo thuận lợi cho các mục đích như vậy. Vấn đề thứ hai là sự không căn chỉnh cấp độ ngôn ngữ do các corpus huấn luyện khác nhau của LLM và mô hình sinh. Ví dụ, để mô tả hình ảnh cảnh quan, LLM có xu hướng tạo ra các đoạn văn mạch lạc, văn học, trong khi mô hình text-to-image thường thích các prompt mô tả súc tích, đi kèm với các thuộc tính như "4k" và "masterpiece". Do đó, Căn chỉnh I/O cũng nên đạt được O1→O2, đảm bảo căn chỉnh cấp độ ngôn ngữ giữa meta-response và các prompt đầu vào được yêu cầu bởi các mô hình sinh.

Để giải quyết các vấn đề được đề cập ở trên, Căn chỉnh I/O sử dụng phương pháp huấn luyện tuân theo hướng dẫn. Các bộ chuyển đổi được huấn luyện với đầu vào bao gồm cả hướng dẫn ngôn ngữ và các yếu tố đa phương thức đi kèm, với mục tiêu tạo ra meta-response chi tiết việc gọi tiếp theo. Chúng tôi sử dụng các tập dữ liệu huấn luyện từ các mô hình sinh để tạo ra các cặp hướng dẫn và sự thật cơ bản tương ứng của chúng (xem Mục 3.3 cho quy trình tạo hướng dẫn). Phương pháp này có lợi vì nhiều lý do: 1. Tinh chỉnh tuân theo hướng dẫn buộc LLM hiểu đầy đủ đầu vào đa phương thức, do đó hỗ trợ căn chỉnh các lớp chiếu đầu vào giữa đầu vào đa phương thức và LLM. 2. Các tập dữ liệu huấn luyện của mô hình sinh thường cung cấp cả dữ liệu phi văn bản, như hình ảnh, video, hoặc âm thanh, và mô tả văn bản tương ứng của chúng, do đó cung cấp nền tảng vững chắc của các mẫu dữ liệu được căn chỉnh. 3. Hầu hết các mô hình sinh mã nguồn mở được huấn luyện trên cùng các tập dữ liệu có sẵn công khai. Căn chỉnh meta-response với các mô tả văn bản từ các tập dữ liệu này có nghĩa là có thể chuyển đổi liền mạch giữa các mô hình sinh, do đó tạo thuận lợi cho phương pháp plug-and-play.

Dựa trên điều này, chúng tôi tạo ra các loại hướng dẫn khác nhau để hoàn thành các mục tiêu trên, như sau:

**Hướng dẫn Căn chỉnh Phía Đầu vào** tập trung vào việc căn chỉnh khả năng của LLM để hiểu đầu vào bao gồm các kết hợp của dữ liệu phương thức khác nhau, như text+image, image+video, hoặc image+audio+video. Ví dụ, khi được trình bày với kết hợp đầu vào hình ảnh, âm thanh, và video, hướng dẫn "Mô tả hình ảnh, âm thanh, và video đã cho" hướng dẫn MLLM mô tả tuần tự nội dung trong hình ảnh, tiếp theo là âm thanh, và sau đó là video.

**Hướng dẫn Căn chỉnh Phía Đầu ra** nhằm căn chỉnh khả năng của LLM để tạo ra meta-response bao gồm chi tiết gọi, như mô hình được chọn và prompt. Ví dụ, hướng dẫn "Tạo ra hình ảnh dựa trên âm thanh tiếng kêu động vật được cung cấp" dạy mô hình sử dụng mô hình text-to-image để tạo ra hình ảnh, có thể với prompt như "Một bức ảnh của một con mèo".

**Hướng dẫn Tăng cường Lý luận** được thiết kế để bảo tồn và tăng cường khả năng lý luận của LLM thông qua một loạt chủ đề đa dạng. Ví dụ, hướng dẫn như "Clip âm thanh này có thể đã được ghi ở đâu?" yêu cầu LLM đưa ra suy luận có lý dựa trên dữ liệu đầu vào, do đó tăng cường kỹ năng lý luận của nó.

### 3.3. Tạo Hướng dẫn và Huấn luyện

Trong phần này, chúng tôi giới thiệu cách tạo ra các cặp hướng dẫn gọi được sử dụng trong Căn chỉnh I/O.

```json
{"instruction": ["Generate an image of
an animal based on the provided
vocalization.", "cat_meowing.wav", ]
"invocation": [("text-to-image", "A
photo of a cat"), ]}
```

Như được chứng minh trong khối mã trên, cặp hướng dẫn-gọi bao gồm hai phần: hướng dẫn, đại diện cho đầu vào, và gọi, đại diện cho đầu ra mong đợi của LLM. Để thu được các dạng mẫu dữ liệu này, chúng tôi xây dựng chúng từ hai nguồn. Nguồn đầu tiên sử dụng các thành phần của công việc tinh chỉnh hướng dẫn hiện có, như LLaVA [24], VideoChat [19], và InstructBLIP [23]. Tuy nhiên, cần lưu ý rằng các hướng dẫn trong các công việc này chủ yếu rơi vào hai loại: Hướng dẫn Căn chỉnh Phía Đầu vào và Hướng dẫn Tăng cường Lý luận. Do đó, để tạo ra Hướng dẫn Căn chỉnh Phía Đầu ra, rất quan trọng cho sự thành công của ModaVerse, chúng tôi tạo ra các mẫu cụ thể để hỗ trợ API ChatGPT của OpenAI trong việc tạo ra hướng dẫn mới quy mô lớn. Cụ thể, mỗi truy vấn gửi đến API ChatGPT bao gồm ba thành phần: Ví dụ Hạt giống, Mô tả Ứng viên, và Tham chiếu Ngôn ngữ.

**Ví dụ Hạt giống** bao gồm một tập hợp các cặp hướng dẫn gọi tiêu chuẩn, được chọn ngẫu nhiên từ bộ sưu tập được chế tạo thủ công. Các ví dụ này phục vụ như hướng dẫn cho API ChatGPT, chứng minh cách tạo ra mẫu ở định dạng đã cho và cung cấp minh họa về tác vụ.

**Mô tả Ứng viên** bao gồm các mô tả văn bản được chọn ngẫu nhiên từ các tập dữ liệu ghép đôi, bao gồm mô tả hình ảnh, âm thanh, và video. Các mô tả này nhằm mô phỏng đầu vào thực, trong khi API ChatGPT được yêu cầu tạo ra hướng dẫn và chi tiết gọi phù hợp dựa trên các ứng viên và ví dụ hạt giống này.

**Tham chiếu Ngôn ngữ** bao gồm các mô tả văn bản được chọn ngẫu nhiên từ tập huấn luyện của mô hình sinh. Các mẫu này phục vụ như hướng dẫn cho API ChatGPT để học phong cách ngôn ngữ của các prompt được sử dụng trong mô hình sinh, giúp tạo ra chi tiết gọi được căn chỉnh ngôn ngữ.

Để huấn luyện, chúng tôi sử dụng Vicuna [58] làm LLM nền tảng, các phần có thể huấn luyện của ModaVerse được đề xuất (xem Hình 3) chỉ bao gồm ba lớp tuyến tính và bộ chuyển đổi LoRA của LLM. Cùng nhau, các thành phần này bao gồm khoảng 40M tham số có thể huấn luyện. Bảng 1 so sánh độ phức tạp huấn luyện của ModaVerse với một số MLLM được đề xuất gần đây. Nó cho thấy rằng phương pháp được đề xuất có độ phức tạp huấn luyện thấp hơn.

## 4. Thí nghiệm

### 4.1. Kết quả Định lượng

Để đánh giá ModaVerse được đề xuất, chúng tôi theo các công trình trước đây [41] để đánh giá khả năng hiểu của mô hình (x→text) và khả năng sinh của nó (text→x), trong đó x có thể là hình ảnh, âm thanh, và video. Bảng 2 và 3 minh họa hiệu suất text-to-image và image-to-text trên tập dữ liệu COCO caption [21]. Kết quả chứng minh rằng phương pháp của chúng tôi đạt được điểm FID 11.28 trong tác vụ tạo hình ảnh, tương đương với các phương pháp gần đây. Về khả năng hiểu hình ảnh, ModaVerse được đề xuất vượt trội hơn bất kỳ diffuser any-to-any CoDi [41] trong cả số liệu B@4 và METEOR, mặc dù nó thấp hơn một chút so với NExT-GPT [48] và OFA [45]. Hiệu suất text-to-audio và audio-to-text trên tập dữ liệu AudioCaps [14] được hiển thị trong Bảng 4 và 5. Phương pháp được đề xuất vượt trội hơn NExT-GPT và bị CoDi vượt qua một chút. Trong chú thích âm thanh, phương pháp của chúng tôi đạt hiệu suất tốt thứ hai, sánh ngang với các mô hình tiên tiến nhất. Bảng 6 và 7 thể hiện hiệu suất text-to-video và video-to-text trên MSR-VTT [50]. Phương pháp của chúng tôi đạt điểm FID 13.35 và điểm CLIPSIM 0.3014 trong tạo video, chứng minh sự ngang bằng với các phương pháp hàng đầu. Tương tự, phương pháp của chúng tôi cho thấy kết quả cạnh tranh trong các tác vụ video-to-text, như được chứng minh bởi điểm B@4 và METEOR của nó.

Mặc dù phương pháp của chúng tôi không vượt trội hơn tất cả các phương pháp tiên tiến nhất (bao gồm hai bài nộp arXiv đồng thời) trên sáu benchmark, điều quan trọng là lưu ý hiệu quả của ModaVerse được đề xuất. Thứ nhất, phương pháp của chúng tôi có khả năng chuyển đổi nhiều phương thức, trong khi một số phương pháp tiên tiến nhất, như SD [33] và OFA [45], được thiết kế cụ thể cho chuyển đổi tuyến đơn như text-to-image. Thứ hai, như Bảng 1 minh họa, ModaVerse hưởng lợi từ mô hình huấn luyện hiệu quả hơn. Nó yêu cầu ít dữ liệu hơn và ít tài nguyên tính toán hơn. Cụ thể, trái ngược với NExT-GPT cần ba giai đoạn để huấn luyện độc lập các lớp chiếu, phương pháp của chúng tôi đơn giản hóa quá trình này thành một giai đoạn duy nhất. Về dữ liệu huấn luyện, phương pháp của chúng tôi sử dụng ít hơn 2% khối lượng dữ liệu được yêu cầu bởi Emu [16] và BLIP-2 [17].

### 4.2. Kết quả Định tính

Vì các tập dữ liệu có sẵn công khai bị hạn chế đến một số kết hợp phương thức phổ biến, như image-to-text và text-to-video, hạn chế này có thể không nắm bắt toàn diện mức độ đầy đủ của khả năng của ModaVerse. Do đó, Hình 4 thể hiện các kết quả định tính khác nhau của ModaVerse qua các phương thức khác nhau. Ví dụ, các ví dụ (a), (c), (f), và (l) nhấn mạnh khả năng sinh có điều kiện của mô hình. Ngoài ra, các ví dụ (g), (h), và (i) chứng minh sự thành thạo của nó trong việc trả lời câu hỏi với đầu vào từ nhiều phương thức khác nhau. Hơn nữa, các ví dụ (d) và (e) chứng minh tiềm năng chuyển đổi phong cách.

### 4.3. Hạn chế và Trường hợp Thất bại

Trong việc khám phá khả năng của ModaVerse, Hình 5 bao gồm một số tình huống thách thức nơi hiệu suất của mô hình có thể được tăng cường thêm. Cụ thể, Ví dụ (a) minh họa hạn chế hiện tại của mô hình trong các tác vụ chỉnh sửa hình ảnh, nơi nó thất bại trong việc giữ lại nền và bố cục gốc của hình ảnh đầu vào. Thay vì sửa đổi hình ảnh hiện có, mô hình tạo ra một hình ảnh hoàn toàn mới. Hạn chế này nổi bật thách thức cụ thể trong phương pháp của chúng tôi, đặc biệt cho các tác vụ yêu cầu độ trung thực với độ phân giải và chi tiết của hình ảnh gốc. Tuy nhiên, điều này có thể được giải quyết bằng cách tích hợp mô hình chỉnh sửa bổ sung vào kho mô hình ở giai đoạn tạo phản hồi cuối cùng, một phát triển chúng tôi để lại cho công việc tương lai. Một trường hợp đáng chú ý khác là Ví dụ (b), nơi, trong trường hợp thiếu manh mối ngôn ngữ ở giai đoạn đầu vào, mô hình có xu hướng tạo ra đầu ra ngẫu nhiên, không liên quan. Vấn đề này phát sinh vì mô hình được huấn luyện tuân theo hướng dẫn dựa vào các hướng dẫn ngôn ngữ đã cho để lý luận ra phản hồi mong đợi. Không có manh mối như vậy, nó có thể gặp khó khăn trong việc tạo ra phản hồi phù hợp.

## 5. Kết luận

Trong bài báo này, chúng tôi đã trình bày ModaVerse, một MLLM có khả năng diễn giải và tạo ra dữ liệu trong các phương thức khác nhau. Mô hình này khác biệt so với các khung MLLM hiện có bằng cách áp dụng phương pháp tổng hợp kết hợp huấn luyện bộ chuyển đổi với phương pháp LLM-như-agent. Bằng cách sử dụng các bộ chuyển đổi, ModaVerse hiệu quả căn chỉnh LLM dựa trên văn bản với đầu vào đa phương thức thông qua một tập hợp các lớp chiếu tuyến tính. Điều này tăng cường khả năng diễn giải một mảng đa dạng các phương thức đầu vào. Ở phía đầu ra, thay vì huấn luyện các lớp chiếu bổ sung để căn chỉnh không gian đầu ra với các mô hình sinh, chúng tôi coi LLM như một agent. Agent này tạo ra meta-response chứa chi tiết gọi, sau đó được phân tích để kích hoạt các mô hình sinh để tạo ra phản hồi cuối cùng. Mô hình huấn luyện Bộ chuyển đổi+Agent tích hợp này không chỉ đơn giản hóa quá trình căn chỉnh đặc trưng đa giai đoạn phức tạp mà còn tăng cường đáng kể hiệu quả của quá trình huấn luyện, cung cấp giải pháp thay thế cho việc huấn luyện MLLM. Đối với công việc tương lai, chúng tôi nhằm giải quyết các hạn chế và điểm yếu hiện tại của khung, như bảo tồn thông tin bố cục gốc của đầu vào, do đó mở rộng khả năng ứng dụng của nó đến các tình huống yêu cầu thông tin gốc, như chỉnh sửa hình ảnh và video.
