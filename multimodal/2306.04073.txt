# 2306.04073.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2306.04073.pdf
# File size: 2619297 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for
Convolutional Neural Networks
Mohammed Nowaz Rabbani Chowdhury1Shuai Zhang1Meng Wang1Sijia Liu2 3Pin-Yu Chen4
Abstract
In deep learning, mixture-of-experts (MoE) ac-
tivates one or few experts (sub-networks) on a
per-sample or per-token basis, resulting in signif-
icant computation reduction. The recently pro-
posed patch-level routing in MoE (pMoE) divides
each input into npatches (or tokens) and sends l
patches ( l≪n) to each expert through prioritized
routing. pMoE has demonstrated great empirical
success in reducing training and inference costs
while maintaining test accuracy. However, the
theoretical explanation of pMoE and the general
MoE remains elusive. Focusing on a supervised
classification task using a mixture of two-layer
convolutional neural networks (CNNs), we show
for the first time that pMoE provably reduces the
required number of training samples to achieve
desirable generalization (referred to as the sample
complexity) by a factor in the polynomial order of
n/l, and outperforms its single-expert counterpart
of the same or even larger capacity. The advantage
results from the discriminative routing property,
which is justified in both theory and practice that
pMoE routers can filter label-irrelevant patches
and route similar class-discriminative patches to
the same expert. Our experimental results on
MNIST, CIFAR-10, and CelebA support our the-
oretical findings on pMoE’s generalization and
show that pMoE can avoid learning spurious cor-
relations.
1Department of Electrical, Computer, and Systems Engineer-
ing, Rensselaer Polytechnic Institute, NY , USA2Department of
Computer Science and Engineering, Michigan State University,
MI, USA3MIT-IBM Watson AI Lab, IBM Research, MA, USA
4IBM Research, Yorktown Heights, NY , USA. Correspondence to:
Mohammed Nowaz Rabbani Chowdhury <chowdm2@rpi.edu >,
Meng Wang <wangm7@rpi.edu >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).
Figure 1. An illustration of pMoE. The image is divided into 20
patches while the router selects 4of them for each expert.
1. Introduction
Deep learning has demonstrated exceptional empirical suc-
cess in many applications at the cost of high computational
and data requirements. To address this issue, mixture-of-
experts (MoE) only activates partial regions of a neural
network for each data point and significantly reduces the
computational complexity of deep learning without hurting
the performance in applications such as machine transla-
tion and natural image classification (Shazeer et al., 2017;
Yang et al., 2019). A conventional MoE model contains
multiple experts (subnetworks of the backbone architecture)
and one learnable router that routes each input sample to
a few but not all the experts (Ramachandran & Le, 2018).
Position-wise MoE has been introduced in language models
(Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al.,
2022), where the routing decisions are made on embeddings
of different positions of the input separately rather than rout-
ing the entire text-input. Riquelme et al. (2021) extended it
to vision models where the routing decisions are made on
image patches. Zhou et al. (2022) further extended where
the MoE layer has one router for each expert such that the
router selects partial patches for the corresponding expert
and discards the remaining patches. We termed this routing
1arXiv:2306.04073v1  [cs.LG]  7 Jun 2023

--- PAGE 2 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
mode as patch-level routing and the MoE layer as patch-
level MoE (pMoE) layer (see Figure 1 for an illustration of
a pMoE). Notably, pMoE achieves the same test accuracy
in vision tasks with 20% less training compute, and 50%
less inference compute compared to its single-expert (i.e.,
one expert which is receiving all the patches of an input)
counterpart of the same capacity (Riquelme et al., 2021).
Despite the empirical success of MoE, it remains elusive in
theory, why can MoE maintain test accuracy while signifi-
cantly reducing the amount of computation? To the best of
our knowledge, only one recent work by Chen et al. (2022)
shows theoretically that a conventional sample-wise MoE
achieves higher test accuracy than convolutional neural net-
works (CNN) in a special setup of a binary classification
task on data from linearly separable clusters. However, the
sample-wise analyses by Chen et al. (2022) do not extend
to patch-level MoE, which employ different routing strate-
gies than conventional MoE, and their data model might not
characterize some practical datasets. This paper addresses
the following question theoretically:
How much computational resource does pMoE save from
the single-expert counterpart while maintaining the same
generalization guarantee?
In this paper, we consider a supervised binary classifica-
tion task where each input sample consists of nequal-sized
patches including class-discriminative patterns that deter-
mine the labels and class-irrelevant patterns that do not
affect the labels. The neural network contains a pMoE
layer1and multiple experts, each of which is a two-layer
CNN2of the same architecture. The router sends l(l≪n)
patches to each expert. Although we consider a simplified
neural network model to facilitate the formal analysis of
pMoE, the insights are applicable to more general setups.
Our major results include:
1.To the best of our knowledge, this paper provides the
first theoretical generalization analysis of pMoE . Our
analysis reveals that pMoE with two-layer CNNs as ex-
perts can achieve the same generalization performance as
conventional CNN while reducing the sample complexity
(the required number of training samples to learn a proper
model) and model complexity. Specifically, we prove that
as long as lis larger than a certain threshold, pMoE reduces
the sample complexity and model complexity by a factor
1In practice, pMoEs are usually placed in the last layers of deep
models. Our analysis can be extended to this case as long as the
input to the pMoE layer satisfies our data model (see Section 4.2).
2We consider CNN as expert due to its wide applications, es-
pecially in vision tasks. Moreover, the pMoE in (Riquelme et al.,
2021; Zhou et al., 2022) uses two-layer Multi-Layer Perceptrons
(MLPs) as experts in vision transformer (ViT), which operates on
image patches. Hence, the MLPs in (Riquelme et al., 2021; Zhou
et al., 2022) are effectively non-overlapping CNNs.polynomial in n/l, indicating an improved generalization
with a smaller l.
2. Characterization of the desired property of the pMoE
router. We show that a desired pMoE router can dispatch
the same class-discriminative patterns to the same expert and
discard some class-irrelevant patterns. This discriminative
property allows the experts to learn the class-discriminative
patterns with reduced interference from irrelevant patterns,
which in turn reduces the sample complexity and model
complexity. We also prove theoretically that a separately
trained pMoE router has the desired property and empiri-
cally verify this property on practical pMoE routers.
3. Experimental demonstration of reduced sample com-
plexity by pMoE in deep CNN models. In addition to
verifying our theoretical findings on synthetic data prepared
from the MNIST dataset (LeCun et al., 2010), we demon-
strate the sample efficiency of pMoE in learning some bench-
mark vision datasets (e.g., CIFAR-10 (Krizhevsky, 2009)
and CelebA (Liu et al., 2015)) by replacing the last convo-
lutional layer of a ten-layer wide residual network (WRN)
(Zagoruyko & Komodakis, 2016) with a pMoE layer. These
experiments not only verify our theoretical findings but also
demonstrate the applicability of pMoE in reducing sam-
ple complexity in deep-CNN-based vision models, comple-
menting the existing empirical success of pMoE with vision
transformers.
2. Related Works
Mixture-of-Experts. MoE was first introduced in the 1990s
with dense sample-wise routing, i.e. each input sample
is routed to all the experts (Jacobs et al., 1991; Jordan &
Jacobs, 1994; Chen et al., 1999; Tresp, 2000; Rasmussen
& Ghahramani, 2001). Sparse sample-wise routing was
later introduced (Bengio et al., 2013; Eigen et al., 2013),
where each input sample activates few of the experts in an
MoE layer both for joint training (Ramachandran & Le,
2018; Yang et al., 2019) and separate training of the router
and experts (Collobert et al., 2001; 2003; Ahmed et al.,
2016; Gross et al., 2017). Position/patch-wise MoE (i.e.,
pMoE) recently demonstrated success in large language and
vision models (Shazeer et al., 2017; Lepikhin et al., 2020;
Riquelme et al., 2021; Fedus et al., 2022). To solve the issue
of load imbalance (Lewis et al., 2021), Zhou et al. (2022)
introduces the expert-choice routing in pMoE, where each
expert uses one router to select a fixed number of patches
from the input. This paper analyzes the sparse patch-level
MoE with expert-choice routing under both joint-training
and separate-training setups.
Optimization and generalization analyses of neural net-
works (NN). Due to the significant nonconvexity of deep
learning problem, the existing generalization analyses are
2

--- PAGE 3 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
limited to linearized or shallow neural networks. The
Neural-Tangent-Kernel (NTK) approach (Jacot et al., 2018;
Lee et al., 2019; Du et al., 2019; Allen-Zhu et al., 2019b;
Zou et al., 2020; Chizat et al., 2019; Ghorbani et al., 2021)
considers strong over-parameterization and approximates
the neural network by the first-order Taylor expansion. The
NTK results are independent of the input data, and perfor-
mance gaps in the representation power and generalization
ability exist between the practical NN and the NTK results
(Yehudai & Shamir, 2019; Ghorbani et al., 2019; 2020; Li
et al., 2020; Malach et al., 2021). Nonlinear neural networks
are analyzed recently through higher-order Taylor expan-
sions (Allen-Zhu et al., 2019a; Bai & Lee, 2019; Arora
et al., 2019; Ji & Telgarsky, 2019) or employing a model
estimation approach from Gaussian input data (Zhong et al.,
2017b;a; Zhang et al., 2020b;a; Fu et al., 2020; Li et al.,
2022b), but these results are limited to two-layer networks
with few papers on three-layer networks (Allen-Zhu et al.,
2019a; Allen-Zhu & Li, 2019; 2020a; Li et al., 2022a).
The above works consider arbitrary input data or Gaussian
input. To better characterize the practical generalization per-
formance, some recent works analyze structured data mod-
els using approaches such as feature mapping (Li & Liang,
2018), where some of the initial model weights are close to
data features, and feature learning (Daniely & Malach, 2020;
Shalev-Shwartz et al., 2020; Shi et al., 2021; Allen-Zhu &
Li, 2022; Li et al., 2023), where some weights gradually
learn features during training. Among them, Allen-Zhu
& Li (2020b); Brutzkus & Globerson (2021); Karp et al.
(2021) analyze CNN on learning structured data composed
of class-discriminative patterns that determine the labels and
other label-irrelevant patterns. This paper extends the data
models in Allen-Zhu & Li (2020b); Brutzkus & Globerson
(2021); Karp et al. (2021) to a more general setup, and our
analytical approach is a combination of feature learning in
routers and feature mapping in experts for pMoE.
3. Problem Formulation
This paper considers the supervised binary classification3
problem where given Ni.i.d. training samples {(xi, yi)}N
i=1
generated by an unknown distribution D, the objective is
to learn a neural network model that maps xtoyfor any
(x, y)sampled from D. Here, the input x∈Rndhasn
disjoint patches, i.e., x⊺= [x(1)⊺, x(2)⊺, ..., x(n)⊺], where
x(j)∈Rddenotes the j-th patch of x.y∈ {+1,−1}
denotes the corresponding label.
3Our results can be extended to multiclass classification prob-
lems. See Section M in the Appendix for details.3.1. Neural Network Models
We consider a pMoE architecture that includes kexperts and
the corresponding krouters. Each router selects lout of n
(l < n ) patches for each expert separately. Specifically, the
router for each expert s(s∈[k]) contains a trainable gating
kernel ws∈Rd. Given a sample x, the router computes
a routing value gj,s(x) =⟨ws, x(j)⟩for each patch j. Let
Js(x)denote the index set of top- lvalues of gj,samong all
the patches j∈[n]. Only patches with indices in Js(x)are
routed to the expert s, multiplied by a gating value Gj,s(x),
which are selected differently in different pMoE models.
Each expert is a two-layer CNN with the same architecture.
Letmdenote the total number of neurons in all the experts.
Then each expert contains (m/k)neurons. Let wr,s∈Rd
andar,s∈Rdenote the hidden layer and output layer
weights for neuron r(r∈[m/k])in expert s(s∈[k]),
respectively. The activation function is the rectified linear
unit (ReLU), where ReLU (z) =max(0, z).
Letθ={ar,s, wr,s, ws,∀s∈[k],∀r∈[m/k]}include all
the trainable weights. The pMoE model denoted as fM, is
defined as follows:
fM(θ, x) =kP
s=1m
kP
r=1ar,s
lP
j∈Js(ws,x)ReLU (⟨wr,s, x(j)⟩)Gj,s(ws, x)
(1)
An illustration of (1) is given in Figure 2.
Figure 2. An illustration of the pMoE model in (1) with k=
3, m= 6, n= 6, andl= 2.
The learning problem solves the following empirical risk
minimization problem with the logistic loss function,
min
θ:L(θ) =1
NNX
i=1log (1 + e−yifM(θ,xi))(2)
We consider two different training modes of pMoE,
Separate-training andJoint-training of the routers and the
experts. We also consider the conventional CNN architec-
ture for comparison.
3

--- PAGE 4 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
(I)Separate-training pMoE : Under the setup of the so-
called hard mixtures of experts (Collobert et al., 2003;
Ahmed et al., 2016; Gross et al., 2017), the router weights
wsare trained first and then fixed when training the weights
of the experts. In this case, the gating values are set as
Gj,s(ws, x)≡1,∀j, s, x (3)
We select k= 2in this case to simplify the analysis.
(II)Joint-training pMoE : The routers and the experts are
learned jointly, see, e.g., (Lepikhin et al., 2020; Riquelme
et al., 2021; Fedus et al., 2022). Here, the gating values are
softmax functions with
Gj,s(ws, x) =egj,s(x)/(X
i∈Js(x)gi,s(x)) (4)
(III) CNN single-expert counterpart : The conventional
two-layer CNN with mneurons, denoted as fC, satisfies,
fC(θ, x) =mX
r=1ar
1
nnX
j=1ReLU (⟨wr, x(j)⟩)
 (5)
Eq. (5) can be viewed as a special case of (1) when there is
only one expert ( k= 1), and all the patches are sent to the
expert ( l=n) with gating values Gj,s≡1.
Let˜θdenote the parameters of the learned model by solving
(1). The predicted label for a test sample xby the learned
model is sign(fM(˜θ, x)). The generalization accuracy, i.e.,
the fraction of correct predictions of all test samples equals
P
(x,y)∼D[yfM(θ, x)>0]. This paper studies both separate
and joint training of pMoE and compares their performance
with CNN, from the perspective of sample complexity to
achieve a desirable generalization accuracy.
3.2. Training Algorithms
In the following algorithms, we fix the output layer weights
ar,sandarat their initial values randomly sampled from the
standard Gaussian distribution N(0,1)and do not update
them during the training. This is a typical simplification
when analyzing NN, as used in (Li & Liang, 2018; Brutzkus
et al., 2018; Allen-Zhu et al., 2019a; Arora et al., 2019).
(I) Separate-training pMoE: The routers are separately
trained using Nrtraining samples ( Nr< N ), denoted by
{(xi, yi)}Nr
i=1without loss of generality. The gating kernels
w1andw2are obtained by solving the following minimiza-
tion problem:
min
w1,w2:lr(w1, w2) =−1
NrNrX
i=1yi⟨w1−w2,nX
j=1x(j)
i⟩(6)To solve (6), we implement the mini-batch SGD with batch
sizeBrforTr=Nr/Briterations, starting from the ran-
dom initialization as follows:
w(0)
s∼ N(0, σ2
rId×d),∀s∈[2] (7)
where, σr= Θ 
1
(n2log (poly(n))√
d)
.
After learning the routers, we train the hidden-layer weights
wr,sby solving (2) while fixing w1andw2. We implement
mini-batch SGD of batch size BforT=N/B iterations
starting from the initialization
w(0)
r,s∼ N(0,1
mId×d),∀s∈[2],∀r∈[m/2] (8)
(II) Joint-training pMoE: wsandwr,sin (1) are updated
simultaneously by mini-batch SGD of batch size BforT=
N/B iterations starting from the initialization in (7) and (8).
(III) CNN: wrin (5) are updated by mini-batch SGD of
batch size BforT=N/B iterations starting from the
initialization in (8).
4. Theoretical Results
4.1. Key Findings At-a-glance
Before defining the data model assumptions and rationale
in Section 4.2 and presenting the formal results in 4.3, we
first summarize our key findings. We assume that the data
patches are sampled from either class-discriminative pat-
terns that determine the labels or a possibly infinite number
ofclass-irrelevant patterns that have no impact on the label.
The parameter δ(defined in (9)) is inversely related to the
separation among patterns, i.e., δdecreases when (i) the
separation among class-discriminative patterns increases,
and/or (ii) the separation between class-discriminative and
class-irrelevant patterns increases. The key findings are as
follows.
(I). A properly trained patch-level router sends class-
discriminative patches of one class to the same expert
while dropping some class-irrelevant patches . We prove
that separate-training pMoE routes class-discriminative
patches of the class with label y= +1 (or the class with
label y=−1) to the expert 1 (or the expert 2) respec-
tively, and the class-irrelevant patterns that are sufficiently
away from class-discriminative patterns are not routed to
any expert (Lemma 4.1). This discriminative routing prop-
erty is also verified empirically for joint-training pMoE (see
section 5.1). Therefore, pMoE effectively reduces the inter-
ference by irrelevant patches when each expert learns the
class-discriminative patterns. Moreover, we show empiri-
cally that pMoE can remove class-irrelevant patches that are
spuriously correlated with class labels and thus can avoid
learning from spuriously correlated features of the data.
4

--- PAGE 5 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
(II). Both the sample complexity and the required num-
ber of hidden nodes of pMoE reduce by a polynomial
factor of n/lover CNN. We prove that as long as l, the num-
ber of patches per expert, is greater than a threshold (that
decreases as the separation between class-discriminative
and class-irrelevant patterns increases), the sample complex-
ity and the required number of neurons of learning pMoE
areΩ(l8)andΩ(l10)respectively. In contrast, the sample
and model complexities of the CNN are Ω(n8)andΩ(n10)
respectively, indicating improved generalization by pMoE.
(III). Larger separation among class-discriminative and
class-irrelevant patterns reduces the sample complexity
and model complexity of pMoE. Both the sample com-
plexity and the required number of neurons of pMoE is poly-
nomial in δ, which decreases when the separation among
patterns increases.
4.2. Data Model Assumptions and Rationale
The input xis comprised of one class-discriminative pat-
tern and n−1class-irrelevant patterns, and the label yis
determined by the class-discriminative pattern only.
Distributions of class-discriminative patterns : The unit
vectors o1ando2∈Rddenote the class-discriminative
patterns that determine the labels. The separation between
o1ando2is measured as δd:=⟨o1, o2⟩ ∈(−1,1).o1and
o2are equally distributed in the samples, and each sample
has exactly one of them. If xcontains o1(oro2), then yis
+1(or−1).
Distributions of class-irrelevant patterns. Class-
irrelevant patterns are unit vectors in Rdbelonging to pdis-
joint pattern sets S1, S2, ...., S p, and these patterns distribute
equally for both classes. δrmeasures the separation between
class-discriminative patterns and class-irrelevant patterns,
where |⟨oi, q⟩| ≤δr,∀i∈[2],∀q∈Sj,j= 1, ..., p . Each
Sjbelongs to a ball with a diameter of Θ(p
(1−δ2r)/dp2).
Note that NO separation among class-irrelevant patterns
themselves is required.
The rationale of our data model. The data distribution
Dcaptures the locality of the label-defining features in
image data. It is motivated by and extended from the data
distributions in recent theoretical frameworks (Yu et al.,
2019; Brutzkus & Globerson, 2021; Karp et al., 2021; Chen
et al., 2022). Specifically, Yu et al. (2019) and Brutzkus &
Globerson (2021) require orthogonal patterns, i.e., δrand
δdare both 0, and there are only a fixed number of non-
discriminative patterns. Karp et al. (2021) and Chen et al.
(2022) assume that δd=−1and a possibly infinite number
of patterns drawn from zero-mean Gaussian distribution.
In our model, δdtakes any value in (−1,1), and the class-
irrelevant patterns can be drawn from ppattern sets that
contain an infinite number of patterns that are not necessarilyGaussian or orthogonal.
Define
δ= 1/(1−max( δ2
d, δ2
r)) (9)
δdecreases if (1) o1ando2are more separated from each
other, and (2) Both o1ando2are more separated from any
setSi,i∈[p]. We also define an integer l∗(l∗≤n) that
measures the maximum number of class-irrelevant patterns
per sample that are sufficiently closer to o1thano2, and
vice versa . Specifically, a class-irrelevant pattern qis called
δ′-closer ( δ′>0) too1thano2, if⟨o1−o2, q⟩> δ′holds.
Similarly, qisδ′-closer to o2thano1if⟨o2−o1, q⟩> δ′.
Then, let l∗−1be the maximum number of class-irrelevant
patches that are either δ′-closer to o1thano2or vice versa
withδ′= Θ(1 −δd)in any xsampled from D.l∗depends
onDandδd. When Dis fixed, a smaller δdcorresponds to
a larger separation between o1ando2and leads to a small
l∗. In contrast to linearly separable data in (Yu et al., 2019;
Brutzkus et al., 2018; Chen et al., 2022), our data model is
NOT linearly separable as long as l∗= Ω(1) (see section K
in Appendix for the proof).
4.3. Main Theoretical Results
4.3.1. G ENERALIZATION GUARANTEE OF
SEPARATE -TRAINING P MOE
Lemma 4.1 shows that as long as the number of patches
per expert, l, is greater than l∗, then the separately learned
routers by solving (6) always send o1to expert 1 and o2
to expert 2. Based on this discriminative property of the
learned routers, Theorem 4.2 then quantifies the sample
complexity and network size of separate-training pMoE
to achieve a desired generalization error ϵ. Theorem 4.3
quantifies the sample and model complexities of CNN for
comparison.
Lemma 4.1 (Discriminative Property of Separately Trained
Routers) .For every l≥l∗, w.h.p. over the random
initialization defined in (7), after doing mini-batch SGD
with batch-size Br= Ω 
n2/(1−δd)2
and learning rate
ηr= Θ(1 /n), forTr= Ω (1 /(1−δd))iterations, the re-
turned w1andw2satisfy
arg
j∈[n](x(j)=o1)∈J1(w1, x),∀(x, y= +1) ∼ D
arg
j∈[n](x(j)=o2)∈J2(w2, x),∀(x, y=−1)∼ D
i.e., the learned routers always send o1to expert 1 and o2
to expert 2.
The main idea in proving Lemma 4.1 is to show that the
gradient in each iteration has a large component along the di-
rections of o1ando2. Then after enough iterations, the inner
product of w1ando1(similarly, w2ando2) is sufficiently
5

--- PAGE 6 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
large. The intuition of requiring l≥l∗is that because there
are at most l∗−1class-irrelevant patches sufficiently closer
too1thano2(or vice versa), then sending l≥l∗patches
to one expert will ensure that one of them is o1(oro2).
Note that the batch size Brand the number of iterations
Trdepend on δd, the separation between o1ando2, but are
independent of the separation between class-discriminative
and class-irrelevant patterns.
We then show that the separate-training pMoE reduces both
the sample complexity and the required model size (Theo-
rem 4.2) compared to the CNN (Theorem 4.3).
Theorem 4.2 (Generalization guarantee of separate-training
pMoE) .For every ϵ >0andl≥l∗, for every m≥MS=
Ω 
l10p12δ6
ϵ16
with at least NS= Ω(l8p12δ6/ϵ16)train-
ing samples, after performing minibatch SGD with the
batch size B= Ω 
l4p6δ3
ϵ8
and the learning rate η=
O 
1
(mpoly(l, p, δ, 1/ϵ,logm))
forT=O 
l4p6δ3
ϵ8
iterations, it holds w.h.p. that
P
(x,y)∼D
yfM(θ(T), x)>0
≥1−ϵ
Theorem 4.2 implies that to achieve generalization error ϵby
a separate-training pMoE, we need NS= Ω( l8p12δ6/ϵ16)
training samples and MS= Ω 
l10p12δ6
ϵ16
hidden
nodes. Therefore, both NSandMSincrease polynomially
with the number of patches lsent to each expert. Moreover,
bothNSandMSare polynomial in δdefined in (9), indicat-
ing an improved generalization performance with stronger
separation among patterns.
The proof of Theorem 4.2 is inspired by Li & Liang (2018),
which analyzes the generalization performance of fully-
connected neural networks (FCN) on structured data, but we
have new technical contributions in analyzing pMoE models.
In addition to analyzing the pMoE routers (Lemma 4.1),
which do not appear in the FCN analysis, our analyses also
significantly relax the separation requirement on the data,
compared with that by Li & Liang (2018). For example,
Li & Liang (2018) requires the separation between the two
classes, measured by the smallest ℓ2-norm distance of two
points in different classes, being Ω(n)to obtain a sample
complexity bound of poly( n) for the binary classification
task. In contrast, the separation between the two classes in
our data model is min{p
2(1−δd),2√1−δr}, much less
thanΩ(n)required by Li & Liang (2018).
Theorem 4.3 (Generalization guarantee of CNN) .For ev-
eryϵ > 0, for every m≥MC= Ω 
n10p12δ6
ϵ16
with at least NC= Ω( n8p12δ6/ϵ16)training sam-
ples, after performing minibatch SGD with the batch
size B= Ω 
n4p6δ3
ϵ8
and the learning rate
η=O 
1
(mpoly(n, p, δ, 1/ϵ,logm))
forT=
O 
n4p6δ3
ϵ8
iterations, it holds w.h.p. thatP
(x,y)∼D
yfC(θ(T), x)>0
≥1−ϵ
Theorem 4.3 implies that to achieve a generalization error ϵ
using CNN in (5), we need NC= Ω(n8p12δ6/ϵ16)training
samples and MC= Ω 
n10p12δ6
ϵ16
neurons.
Sample-complexity gap between single CNN and mix-
ture of CNNs. From Theorem 4.2 and Theorem 4.3, the
sample-complexity ratio of the CNN to the separate-training
pMoE is NC/NS= Θ 
(n/l)8
. Similarly, the required
number of neurons is reduced by a factor of MC/MS=
Θ 
(n/l)10
in separate-training pMoE4.
4.3.2. G ENERALIZATION GUARANTEE OF
JOINT -TRAINING P MOEWITH PROPER ROUTERS
Theorem 4.5 characterizes the generalization performance
of joint-training pMoE assuming the routers are properly
trained in the sense that after some SGD iterations, for
each class at least one of the kexperts receives all class-
discriminative patches of that class with the largest gating-
value (see Assumption 4.4).
Assumption 4.4. There exists an integer T′< T such that
for all t≥T′, it holds that:
There exists an expert s∈[k]s.t.∀(x, y= +1) ∼ D,
jo1∈Js(w(t)
s, x),andG(t)
jo1,s(x)≥G(t)
j,s(x)
and an expert s∈[k]s.t.∀(x, y=−1)∼ D,
jo2∈Js(w(t)
s, x),andG(t)
jo2,s(x)≥G(t)
j,s(x)
where jo1(jo2) denotes the index of the class-discriminative
pattern o1(o2),G(t)
j,s(x)is the gating output of patch j∈
Js(w(t)
s, x)of sample xfor expert sat the iteration t, and
w(t)
sis the gating kernel for expert sat iteration t.
Assumption 4.4 is required in proving Theorem 4.5 because
of the difficulty of tracking the dynamics of the routers in
joint-training pMoE. Assumption 4.4 is verified on empirical
experiments in Section 5.1, while its theoretical proof is left
for future work.
Theorem 4.5 (Generalization guarantee of joint-training
pMoE) .Suppose Assumption 4.4 hold. Then for every
ϵ > 0, for every m≥MJ= Ω 
k3n2l6p12δ6
ϵ16
with at least NJ= Ω( k4l6p12δ6/ϵ16)training sam-
ples, after performing minibatch SGD with the batch
size B= Ω 
k2l4p6δ3
ϵ8
and the learning rate
4The bounds for the sample complexity and model size in The-
orem 4.2 and Theorem 4.3 are sufficient but not necessary. Thus,
rigorously speaking, one can not compare sufficient conditions
only. In our analysis, however, the bounds for MoE and CNN are
derived with exactly the same technique with the only difference
to handle the routers. Therefore, it is fair to compare these two
bounds to show the advantage of pMoE.
6

--- PAGE 7 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Table 1. Computational complexity of pMoE and CNN.
COMPLEXITY TO
ACHIEVE ϵERROR
(COMPLX /ITER×T)PMOECNN
SEPARATE -TRAINING JOINT -TRAINING
O(Bml5d/ϵ8) O(Bmk2l3d/ϵ8) O(Bmn5d/ϵ8)
COMPLEXITY PER
ITERATION
(COMPLX /ITER)O(Bmld )ROUTER EXPERT
O(Bmnd ) O(Bknd )(FORWARD PASS )O(Bmld )
O(Bkl2d)(BACKWARD PASS )
ITERATION REQUIRED
TO CONVERGE WITH ϵ
ERROR (T)O(l4/ϵ8) O(k2l2/ϵ8) O(n4/ϵ8)
η=O 
1
(mpoly(l, p, δ, 1/ϵ,logm))
forT=
O 
k2l2p6δ3
ϵ8
iterations, it holds w.h.p. that
P
(x,y)∼D
yfM(θ(T), x)>0
≥1−ϵ
Theorem 4.5 indicates that, with proper routers, joint-
training pMoE needs NJ= Ω(k4l6p12δ6/ϵ16)training sam-
ples and MJ= Ω 
k3n2l6p12δ6
ϵ16
neurons to achieve
ϵgeneralization error. Compared with CNN in Theorem
4.3, joint-training pMoE reduces the sample complexity and
model size by a factor of Θ(n8/k4l6)andΘ(n10/k3l6),
respectively. With more experts (a larger k), it is easier to
satisfy Assumption 4.4 to learn proper routers but requires
larger sample and model complexities. When the number of
samples is fixed, the expression of NJalso indicates that ϵ
sales as k1/4l3/8, corresponding to an improved generaliza-
tion when kandldecrease.
We provide the end-to-end computational complexity com-
parison between the analyzed pMoE models and general
CNN model in Table 1 (see section N in Appendix for de-
tails). The results in Table 1 indicates that the computational
complexity in joint-training pMoE is reduced by a factor of
O(n5/k2l3)compared with CNN. Similarly, the reduction
of computational complexity of separate-training pMoE is
O(n5/l5).
5. Experimental Results
5.1. pMoE of Two-layer CNN
Dataset : We verify our theoretical findings about the model
in (1) on synthetic data prepared from MNIST (LeCun et al.,
2010) data set. Each sample contains n= 16 patches with
patch size d= 28×28. Each patch is drawn from the
MNIST dataset. See Figure 3 as an example. We treat
the digits “ 1” and “ 0” as the class-discriminative patterns
o1ando2, respectively. Each of the digits from “ 2” to “ 9”
represents a class-irrelevant pattern set.
Setup : We compare separate-training pMoE, joint-training
Figure 3. Sample
image of the
synthetic data
from MNIST.
Class label is
“1”.
 Figure 4. Generalization performance of pMoE
and CNN with a similar model size
Figure 5. Phase transition of sample complexity with lin separate-
training pMoE
pMoE, and CNN with similar model sizes. The separate-
training pMoE contains twoexperts with 20hidden nodes
in each expert. The joint-training pMoE has eight experts
with five hidden nodes per expert. The CNN has 40hidden
nodes. All are trained using SGD with η= 0.2until zero
training error. pMoE converges much faster than CNN,
which takes 150epochs. Before training the experts in the
separate-training pMoE, we train the router for 100epochs.
The models are evaluated on 1000 test samples.
Generalization performance : Figure 4 compares the test
accuracy of the three models, where l= 2 andl= 6 for
separate-training and joint-training pMoE, respectively. The
error bars show the mean plus/minus one standard deviation
7

--- PAGE 8 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Figure 6. Change of test accuracy in joint-training pMoE with k
for fixed sample sizes
Figure 7. Change of test accuracy in joint-training pMoE with lfor
fixed sample sizes
of five independent experiments. pMoE outperforms CNN
with the same number of training samples. pMoE only
requires 60% of the training samples needed by CNN to
achieve 95% test accuracy.
Figure 5 shows the sample complexity of separate-training
pMoE with respect to l. Each block represents 20 inde-
pendent trials. A white block indicates all success, and a
black block indicates all failure. The sample complexity
is polynomial in l, verifying Theorem 4.2. Figure 7 and 6
show the test accuracy of joint-training pMoE with a fixed
sample size when landkchange, respectively. When lis
greater than l∗, which is 6in Figure 7, the test accuracy
matches our predicted order. Similarly, the dependence on
kalso matches our prediction, when kis large enough to
make Assumption 4.4 hold.
Router performance : Figure 8 verifies the discrimina-
tive property of separately trained routers (Lemma 4.1)
by showing the percentage of testing data that have class-
discriminative patterns ( o1ando2) in top lpatches of the
separately trained router. With very few training samples
(such as 300), one can already learn a proper router that has
discriminative patterns in top- 4patches for 95% of data. Fig-
ure 9 verifies the discriminative property of jointly trained
routers (Assumption 4.4). With only 300training samples,
the jointly trained router dispatches o1with the largest gat-ing value to a particular expert for 95% of class-1 data and
similarly for o2in 92% of class-2 data.
Figure 8. Percentage of properly routed discriminative patterns by
a separately trained router.
Figure 9. Percentage of properly routed discriminative patterns by
a jointly trained router. l= 6.
5.2. pMoE of Wide Residual Networks (WRNs)
Neural network model : We employ the 10-layer WRN
(Zagoruyko & Komodakis, 2016) with a widening factor of
10 as the expert. We construct a patch-level MoE counterpart
of WRN, referred to as WRN-pMoE, by replacing the last
convolutional layer of WRN with an pMoE layer of an equal
number of trainable parameters (see Figure 18 in Appendix
for an illustration). WRN-pMoE is trained with the joint-
training method5. All the results are averaged over five
independent experiments.
Datasets : We consider both CelebA (Liu et al., 2015) and
CIFAR-10 datasets. The experiments on CIFAR-10 are
deferred to the Appendix (see section A). We down-sample
the images of CelebA to 64×64. The last convolutional
layer of WRN receives a ( 16×16×640) dimensional feature
map. The feature map is divided into 16patches with size
4×4×640in WRN-pMoE. k= 8andl= 2for the pMoE
layer.
5Code is available at https://github.com/
nowazrabbani/pMoE_CNN
8

--- PAGE 9 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Figure 10. Classification accuracy of WRN-
pMoE and WRN on “smiling” in CelebA
Figure 11. Classification accuracy of WRN-
pMoE and WRN on “smiling” when spuri-
ously correlated with “black hair” in CelebA
Figure 12. Classification accuracy of WRN-
pMoE and WRN on multiclass classification
in CelebA
Table 2. Comparison of training compute of WRN and WRN-pMoE.
NO.OF TRAINING SAMPLESCONVERGENCE TIME (SEC)TRAINING FLOP S(×1015)
WRN WRN- PMOE WRN WRN- PMOE
4000 260 156 6 3.5
8000 324 192 7.5 4.4
12000 468 280 11 6.4
16000 630 368 15 8.5
Performance Comparison : Figure 10 shows the test ac-
curacy of the binary classification problem on the attribute
“smiling.” WRN-pMoE requires less than one-fifth of the
training samples needed by WRN to achieve 86% accuracy.
Figure 11 shows the performance when the training data
contain spurious correlations with the hair color as a spu-
rious attribute. Specifically, 95% of the training images
with the attribute “smiling” also have the attribute “black
hair,” while 95% of the training images with the attribute
“not-smiling” have the attribute “blond hair.” The models
may learn the hair-color attribute rather than “smiling” due
to spurious correlation and, thus, the test accuracies are
lower in Figure 11 than those in Figure 10. Nevertheless,
WRN-pMoE outperforms WRN and reduces the sample
complexity to achieve the same accuracy.
Figure 12 shows the test accuracy of multiclass classification
(four classes with class attributes: “Not smiling, Eyeglass,”
“Smiling, Eyeglass,” “Smiling, No eyeglass,” and “Not smil-
ing, No eyeglass”) in CelebA. The results are consistent
with the binary classification results. Furthermore, Table 2
empirically verifies the computational efficiency of WRN-
pMoE over WRN on multiclass classification in CelebA6.
Even with same number of training samples, WRN-pMoE
6An NVIDIA RTX 4500 GPU was used to run
the experiments, training FLOPs are calculated as
Training FLOPs =Training time (second) ×Number of GPUs ×
peak FLOP/second ×GPU utilization rateis still more computationally efficient than WRN, because
WRN-pMoE requires fewer iterations to converge and has a
lower per-iteration cost.
6. Conclusion
MoE reduces computational costs significantly without hurt-
ing the generalization performance in various empirical
studies, but the theoretical explanation is mostly elusive.
This paper provides the first theoretical analysis of patch-
level MoE and proves its savings in sample complexity and
model size quantitatively compared with the single-expert
counterpart. Although centered on a classification task us-
ing a mixture of two-layer CNNs, our theoretical insights
are verified empirically on deep architectures and multiple
datasets. Future works include analyzing other MoE ar-
chitectures such as MoE in Vision Transformer (ViT) and
connecting MoE with other sparsification methods to further
reduce the computation.
Acknowledgements
This work was supported by AFOSR FA9550-20-1-0122,
NSF 1932196 and the Rensselaer-IBM AI Research Collabo-
ration (http://airc.rpi.edu), part of the IBM AI Horizons Net-
work (http://ibm.biz/AIHorizons). We thank Yihua Zhang
at Michigan State University for the help in experiments
with CelebA dataset. We thank all anonymous reviewers.
9

--- PAGE 10 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
References
Ahmed, K., Baig, M. H., and Torresani, L. Network of
experts for large-scale image categorization. In European
Conference on Computer Vision , pp. 516–532. Springer,
2016.
Allen-Zhu, Z. and Li, Y . What can resnet learn efficiently,
going beyond kernels? Advances in Neural Information
Processing Systems , 32, 2019.
Allen-Zhu, Z. and Li, Y . Backward feature correction: How
deep learning performs deep learning. arXiv preprint
arXiv:2001.04413 , 2020a.
Allen-Zhu, Z. and Li, Y . Towards understanding ensem-
ble, knowledge distillation and self-distillation in deep
learning. arXiv preprint arXiv:2012.09816 , 2020b.
Allen-Zhu, Z. and Li, Y . Feature purification: How adversar-
ial training performs robust deep learning. In 2021 IEEE
62nd Annual Symposium on Foundations of Computer
Science (FOCS) , pp. 977–988. IEEE, 2022.
Allen-Zhu, Z., Li, Y ., and Liang, Y . Learning and generaliza-
tion in overparameterized neural networks, going beyond
two layers. Advances in neural information processing
systems , 32, 2019a.
Allen-Zhu, Z., Li, Y ., and Song, Z. A convergence theory for
deep learning via over-parameterization. In International
Conference on Machine Learning , pp. 242–252. PMLR,
2019b.
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R. Fine-grained
analysis of optimization and generalization for overpa-
rameterized two-layer neural networks. In International
Conference on Machine Learning , pp. 322–332. PMLR,
2019.
Bai, Y . and Lee, J. D. Beyond linearization: On quadratic
and higher-order approximation of wide neural networks.
InInternational Conference on Learning Representations ,
2019.
Bengio, Y ., L ´eonard, N., and Courville, A. Estimating or
propagating gradients through stochastic neurons for con-
ditional computation. arXiv preprint arXiv:1308.3432 ,
2013.
Brutzkus, A. and Globerson, A. An optimization and gener-
alization analysis for max-pooling networks. In Uncer-
tainty in Artificial Intelligence , pp. 1650–1660. PMLR,
2021.
Brutzkus, A., Globerson, A., Malach, E., and Shalev-
Shwartz, S. SGD learns over-parameterized networks
that provably generalize on linearly separable data. In
International Conference on Learning Representations ,
2018.Chen, K., Xu, L., and Chi, H. Improved learning algorithms
for mixture of experts in multiclass classification. Neural
networks , 12(9):1229–1252, 1999.
Chen, Z., Deng, Y ., Wu, Y ., Gu, Q., and Li, Y . Towards
understanding mixture of experts in deep learning. arXiv
preprint arXiv:2208.02813 , 2022.
Chizat, L., Oyallon, E., and Bach, F. On lazy training in
differentiable programming. Advances in Neural Infor-
mation Processing Systems , 32, 2019.
Collobert, R., Bengio, S., and Bengio, Y . A parallel mixture
of SVMs for very large scale problems. Advances in
Neural Information Processing Systems , 14, 2001.
Collobert, R., Bengio, Y ., and Bengio, S. Scaling large learn-
ing problems with hard parallel mixtures. International
Journal of pattern recognition and artificial intelligence ,
17(03):349–365, 2003.
Daniely, A. and Malach, E. Learning parities with neural
networks. Advances in Neural Information Processing
Systems , 33:20356–20365, 2020.
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. Gradient
descent finds global minima of deep neural networks. In
International conference on machine learning , pp. 1675–
1685. PMLR, 2019.
Eigen, D., Ranzato, M., and Sutskever, I. Learning fac-
tored representations in a deep mixture of experts. arXiv
preprint arXiv:1312.4314 , 2013.
Fedus, W., Zoph, B., and Shazeer, N. Switch transformers:
Scaling to trillion parameter models with simple and ef-
ficient sparsity. Journal of Machine Learning Research ,
23(120):1–39, 2022.
Fu, H., Chi, Y ., and Liang, Y . Guaranteed recovery of one-
hidden-layer neural networks via cross entropy. IEEE
transactions on signal processing , 68:3225–3235, 2020.
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
Limitations of lazy training of two-layers neural network.
Advances in Neural Information Processing Systems , 32,
2019.
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
When do neural networks outperform kernel methods?
Advances in Neural Information Processing Systems , 33:
14820–14830, 2020.
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A.
Linearized two-layers neural networks in high dimension.
The Annals of Statistics , 49(2):1029–1054, 2021.
10

--- PAGE 11 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of
experts for large scale weakly supervised vision. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 6865–6873, 2017.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E.
Adaptive mixtures of local experts. Neural computation ,
3(1):79–87, 1991.
Jacot, A., Gabriel, F., and Hongler, C. Neural tangent ker-
nel: Convergence and generalization in neural networks.
Advances in neural information processing systems , 31,
2018.
Ji, Z. and Telgarsky, M. Polylogarithmic width suffices
for gradient descent to achieve arbitrarily small test error
with shallow relu networks. In International Conference
on Learning Representations , 2019.
Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of
experts and the em algorithm. Neural computation , 6(2):
181–214, 1994.
Karp, S., Winston, E., Li, Y ., and Singh, A. Local signal
adaptivity: Provable feature learning in neural networks
beyond kernels. Advances in Neural Information Process-
ing Systems , 34:24883–24897, 2021.
Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, Canadian Institute For
Advanced Research, 2009.
LeCun, Y ., Cortes, C., and Burges, C. MNIST handwritten
digit database. AT&T labs [online]. available http. yann.
lecun. com/exdb/mnist , 2010.
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y ., Novak, R., Sohl-
Dickstein, J., and Pennington, J. Wide neural networks of
any depth evolve as linear models under gradient descent.
Advances in neural information processing systems , 32,
2019.
Lepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,
Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling
giant models with conditional computation and automatic
sharding. In International Conference on Learning Rep-
resentations , 2020.
Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettle-
moyer, L. Base layers: Simplifying training of large,
sparse models. In International Conference on Machine
Learning , pp. 6265–6274. PMLR, 2021.
Li, H., Wang, M., Liu, S., Chen, P.-Y ., and Xiong, J. Gen-
eralization guarantee of training graph convolutional net-
works with graph topology sampling. In International
Conference on Machine Learning , pp. 13014–13051.
PMLR, 2022a.Li, H., Zhang, S., and Wang, M. Learning and generaliza-
tion of one-hidden-layer neural networks, going beyond
standard gaussian data. In 2022 56th Annual Conference
on Information Sciences and Systems (CISS) , pp. 37–42.
IEEE, 2022b.
Li, H., Wang, M., Liu, S., and Chen, P.-Y . A theoretical
understanding of shallow vision transformers: Learning,
generalization, and sample complexity. In The Eleventh
International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?
id=jClGv3Qjhb .
Li, Y . and Liang, Y . Learning overparameterized neural
networks via stochastic gradient descent on structured
data. Advances in neural information processing systems ,
31, 2018.
Li, Y ., Ma, T., and Zhang, H. R. Learning over-parametrized
two-layer neural networks beyond NTK. In Conference
on learning theory , pp. 2613–2682. PMLR, 2020.
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning
face attributes in the wild. In Proceedings of the IEEE
international conference on computer vision , pp. 3730–
3738, 2015.
Malach, E., Kamath, P., Abbe, E., and Srebro, N. Quan-
tifying the benefit of using differentiable learning over
tangent kernels. In International Conference on Machine
Learning , pp. 7379–7389. PMLR, 2021.
Ramachandran, P. and Le, Q. V . Diversity and depth in
per-example routing models. In International Conference
on Learning Representations , 2018.
Rasmussen, C. and Ghahramani, Z. Infinite mixtures of
gaussian process experts. Advances in neural information
processing systems , 14, 2001.
Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M.,
Jenatton, R., Susano Pinto, A., Keysers, D., and Houlsby,
N. Scaling vision with sparse mixture of experts. Ad-
vances in Neural Information Processing Systems , 34:
8583–8595, 2021.
Shalev-Shwartz, S. et al. Computational separation between
convolutional and fully-connected networks. In Interna-
tional Conference on Learning Representations , 2020.
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q. V ., Hinton, G. E., and Dean, J. Outrageously large neu-
ral networks: The sparsely-gated mixture-of-experts layer.
InInternational Conference on Learning Representations ,
2017.
Shi, Z., Wei, J., and Liang, Y . A theoretical analysis on
feature learning in neural networks: Emergence from
11

--- PAGE 12 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
inputs and advantage over fixed features. In International
Conference on Learning Representations , 2021.
Tresp, V . Mixtures of gaussian processes. In Leen, T.,
Dietterich, T., and Tresp, V . (eds.), Advances in Neural
Information Processing Systems , volume 13. MIT
Press, 2000. URL https://proceedings.
neurips.cc/paper/2000/file/
9fdb62f932adf55af2c0e09e55861964-Paper.
pdf.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in neural information
processing systems , 30, 2017.
Yang, B., Bender, G., Le, Q. V ., and Ngiam, J. Condconv:
Conditionally parameterized convolutions for efficient
inference. Advances in Neural Information Processing
Systems , 32, 2019.
Yehudai, G. and Shamir, O. On the power and limitations
of random features for understanding neural networks.
Advances in Neural Information Processing Systems , 32,
2019.
Yu, B., Zhang, J., and Zhu, Z. On the learning dynamics of
two-layer nonlinear convolutional neural networks. arXiv
preprint arXiv:1905.10157 , 2019.
Zagoruyko, S. and Komodakis, N. Wide residual networks.
arXiv preprint arXiv:1605.07146 , 2016.
Zhang, S., Wang, M., Liu, S., Chen, P.-Y ., and Xiong, J. Fast
learning of graph neural networks with guaranteed gener-
alizability: one-hidden-layer case. In International Con-
ference on Machine Learning , pp. 11268–11277. PMLR,
2020a.
Zhang, S., Wang, M., Xiong, J., Liu, S., and Chen, P.-Y .
Improved linear convergence of training CNNs with gen-
eralizability guarantees: A one-hidden-layer case. IEEE
Transactions on Neural Networks and Learning Systems ,
32(6):2622–2635, 2020b.
Zhong, K., Song, Z., and Dhillon, I. S. Learning non-
overlapping convolutional neural networks with multiple
kernels. arXiv preprint arXiv:1711.03440 , 2017a.
Zhong, K., Song, Z., Jain, P., Bartlett, P. L., and Dhillon,
I. S. Recovery guarantees for one-hidden-layer neural net-
works. In International conference on machine learning ,
pp. 4140–4149. PMLR, 2017b.
Zhou, Y ., Lei, T., Liu, H., Du, N., Huang, Y ., Zhao,
V . Y ., Dai, A. M., Chen, Z., Le, Q. V ., and Laudon, J.
Mixture-of-experts with expert choice routing. In Oh,
A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),Advances in Neural Information Processing Systems ,
2022. URL https://openreview.net/forum?
id=jdJo1HIVinI .
Zou, D., Cao, Y ., Zhou, D., and Gu, Q. Gradient descent op-
timizes over-parameterized deep relu networks. Machine
learning , 109(3):467–492, 2020.
12

--- PAGE 13 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
A. Experiments on CIFAR-10 Datasets
We also compare WRN and WRN-pMoE on CIFAR-10-based datasets. To better reflect local features, in addition to the
original CIFAR-10, we adopt techniques of Karp et al. (2021) to generate two datasets based on CIFAR-10:
1. CIFAR-10 with IMAGE NETnoise. Each CIFAR-10 image is down-sampled to size 16×16and placed at a random
location of a background image chosen from ImageNet Plants synset. Figure 13(c) shows an example image of this dataset.
2. CIFAR- VEHICLES .Each vehicle image of CIFAR-10 is down-sampled to size 16×16and placed in one quadrant of an
image randomly where the other quadrants are randomly filled with down-sampled animal images in CIFAR-10. See Figure
13(b) for a sample image.
The last convolutional layer of WRN receives a (8×8×640) dimensional feature map. In WRN-pMoE we divide this
feature map into 64patches with size (1×1×640). The MoE layer of WRN-pMoE contains k= 4experts with each
expert receiving l= 16 patches.
Figure 13. Example images from (a) CIFAR-10, (b)
CIFAR- VEHICLES , and (c) CIFAR-10, IMAGE NET
noise datasets
Figure 14. Ten-classification accuracy of WRN and WRN-pMoE on
CIFAR-10
Figure 15. Ten-classification accuracy of WRN and WRN-pMoE
on CIFAR-10, I MAGE NETnoise
Figure 16. Four-classification accuracy of WRN and WRN-pMoE
on CIFAR-V EHICLES
Figures 14, 15, and 16 compare the test accuracy of WRN and WRN-pMoE for the ten-classification problem on CIFAR10
and CIFAR-10 with IMAGE NETnoise, and the four-classification problem in CIFAR- VEHICLES , respectively. WRN-pMoE
outperforms WRN in all these datasets, indicating reduced sample complexity using the pMoE layer. The performance gap
is more significant in the other two datasets than the original CIFAR-10 dataset. That is because these constructed datasets
contain local features, and the pMoE layer has a clear advantage in learning local features effectively.
B. Preliminaries
The loss function for SGD at iteration twith minibatch Bt:
L(θ(t)) :=1
BX
(x,y)∈Btlog (1 + e−yfM(θ(t),x)) (10)
13

--- PAGE 14 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
For the router-training in separate-training pMoE, the loss function of SGD at iteration twith minibatch Br
t:
ℓr(w(t)
1, w(t)
2) :=−1
BrX
(x,y)∈Br
ty⟨w(t)
1−w(t)
2,nX
j=1x(j)⟩ (11)
Notations:
1.Generally ˜O(.)and˜Ω(.)hides factor log(poly(m, n, p, δ,1
ϵ)). At Lemma E.3 and D.4, ˜Ω(.)hides factor log(poly(n)).
2.Generally with high probability (abbreviated as w.h.p.) implies with probability 1−1
poly(m, n, p, δ,1
ϵ), where poly(.)
implies a sufficiently large polynomial. At Lemma E.2, E.3 and D.4 “w.h.p.” implies 1−1
poly(n).
3. We denote, σ=1√msuch that the expert initialization, w(0)
r,s∼ N(0, σ2Id×d),∀s∈[k],∀r∈[m/k].
The training algorithms for separate-training and joint-training pMoE are given in Algorithm 1 and Algorithm 2, respectively:
Algorithm 1 Two-phase SGD for separate-training pMoE
Input : Training data {(xi, yi)}N
i=1, learning rates ηrandη, number of iterations TrandT, batch-
sizes BrandB
Step-1 : Initialize w(0)
s, w(0)
r,s, ar,s,∀s∈ {1,2}, r∈[m/k]according to (7) and (8)
Step-2 : fort= 0,1, ..., T r−1do:
w(t+1)
s =w(t)
s−ηr∂ℓr(w(t)
1, w(t)
2)
∂w(t)
s,∀s∈ {1,2}
Step-3 : fort= 0,1, ..., T −1do:
w(t+1)
r,s =w(t)
r,s−η∂L(θ(t))
∂w(t)
r,s,∀r∈[m/k], s∈ {1,2}
Algorithm 2 SGD for joint-training pMoE
Input : Training data {(xi, yi)}N
i=1, learning rate η, number of iteration T, batch-size B
Step-1 : Initialize w(0)
s, w(0)
r,s, ar,s,∀s∈[k], r∈[m/k]according to (7) and (8)
Step-2 : fort= 0,1, ..., T −1do:
w(t+1)
s =w(t)
s−η∂L(θ(t))
∂w(t)
s,∀s∈[k]
w(t+1)
r,s =w(t)
r,s−η∂L(θ(t))
∂w(t)
r,s,∀r∈[m/k], s∈[k]
C. Proof Sketch
The proof of generalization guarantee for pMoE (i.e., Theorem 4.2 and 4.5) can be outlined as follows (the proof for single
CNN follows a simpler version of the outline provided below):
Step 1. (Feature learning in the router ) For separate-training pMoE, we first show that the batch-gradient of the router
loss (i.e., ℓr(w(t)
1, w(t)
2)) w.r.t. the gating kernels (i.e., w(t)
1andw(t)
2) has large component (of size1−δd
2−Ω
n√Br
) along
the class-discriminative pattern o1ando2respectively. Then, by selecting Br= Ω
n2
(1−δd)2
(which provides us Ω(1) loss
reduction per step) and training for Ω
1
1−δd
iterations, we can show that w1andw2is sufficiently aligned with o1ando2
14

--- PAGE 15 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
respectively to guarantee the selection of these class-discriminative patterns in TOP- lpatches when l≥l∗(see Lemma D.4
for exact statement).
Step 2. (Coupling the experts to pseudo experts ) When the experts of pMoE are sufficiently overparameterized, w.h.p.
the experts can be coupled to a smooth pseudo network7of experts as for every sample drawn from the distribution Dand
every τ >0, the activation pattern for 1−Ω τl
σ
(for separate-training pMoE) or 1−Ω τn
σ
(for joint-training pMoE)
fraction of hidden nodes in each expert does not change from the initialization for O(τ
η)iterations (see Lemma G.1 or H.1
for exact statement). This indicates that with τ=O σ
l
(for separate-training pMoE) or τ=O σ
n
(for joint-training
pMoE), η= Ω 1
ml
(for separate-training pMoE) or η= Ω 1
mn
(for joint-training pMoE) and σ=O
1√m
we can
couple Ω(1) fraction of hidden nodes of each expert to the corresponding pseudo experts for O(√m)iterations.
Step 3. (Large error implies large gradient ) We can now analyze the pseudo network of experts corresponding to the
separate-training pMoE to show that, at any iteration t, the magnitude of the expected gradient for any expert s∈ {1,2}of
the pseudo network is Ω
v(t)
s
l
where v(t)
scharacterizes the class-conditional expected error over samples with y= +1
andy=−1fors= 1ands= 2, respectively (see Lemma G.3 for exact statement). Similarly, for joint-training pMoE
we show that the magnitude of the expected gradient is Ω
v(t)
s
l
, but this time v(t)
scharacterizes the maximum of the
class-conditional expected-errors over the samples for which the expert “ s” receiving class-discriminative patterns from the
router (see Lemma H.3 for exact statement).
Step 4. (Convergence ) Now let us define v(t)=qP
s∈[k]v2s(t). For separate-training pMoE, by selecting the batch size
Bt= Ω(l4
(v(t))4)at iteration t,η= Ω((v(t))2
ml2)andτ=O(σ(v(t))2
l3), we can couple the empirical batch gradient of each
expert of the true network for that batch to the expected gradient of the corresponding expert of the pseudo network. Because
the pseudo network is smooth, we can show that SGD minimizes the expected loss of the true network by Ω(ηm(v(t))2
l2)at
each iteration for t=O(σ(v(t))2
ηl3)iterations (see Lemma G.4 for the exact statement). Similarly, for joint-training pMoE,
by selecting Bt= Ω(k2
(v(t))4)andη= Ω((v(t))2l3
mk2)we can show that SGD minimizes the expected loss of the true network
byΩ(ηm(v(t))2
l2)fort=O(σ(v(t))2l2
ηnk)(see Lemma H.4 for exact statement). As the loss of the true network is O(1)at
initialization, eventually the network will converge.
Step 5. (Generalization ) We show that to ensure at most ϵgeneralization error after any iteration t, we need
max{v(t)
1, v(t)
2}< ϵ2where v(t)
1andv(t)
2correspond to the class-conditional expected error of the class with y= +1
andy=−1, respectively. Now as we show that the router in the separate-training pMoE dispatch class-discriminative
patches of all the samples labeled as y= +1 to the expert indexed by s= 1and class-discriminative patches of all the
samples labeled as y=−1to the expert indexed by s= 2 from the beginning of expert-training, v(t)< ϵ2ensures
max{v(t)
1, v(t)
2}< ϵ2. On the other hand, for the joint-training pMoE, as we assume that the router ensures the dispatchment
of all the class-discriminative patches of a class to a particular expert before the convergence of the model and the gating
value of the patch is the largest among all the patches sent to that particular expert, v(t)<ϵ2
limplies max{v(t)
1, v(t)
2}< ϵ2.
Hence for separate-training pMoE, by setting v(t)≥ϵ2we show that with B= Ω( l4/ϵ8)andη= Ω(1 /mpoly(l,1/ϵ))
forT=O(l4/ϵ8)iterations, we can guarantee that the generalization error is less than ϵ(see Theorem F.3 for exact state-
ment). Similarly, for joint-training pMoE, by setting v(t)≥ϵ2
land setting B= Ω(k2l4/ϵ8)andη= Ω(1 /(mpoly(l,1/ϵ)
forT=O(k2l2/ϵ8)iterations, we can guarantee that the generalization error is less than ϵ(see Theorem F.5 for exact
statement).
D. Proof of the Lemma 4.1
Definition D.1. (δ′-closer class-irrelevant patterns) For any δ′>0, a class-irrelevant pattern qisδ′-closer to o1thano2, if
⟨o1, q⟩ − ⟨o2, q⟩> δ′for any δ′>0. Similarly, a class-irrelevant pattern qisδ′-closer to o2thano1if⟨o2, q⟩ − ⟨o1, q⟩> δ′.
Definition D.2. (Set of δ′-closer class-irrelevant patterns, Sc(δ′)) For any δ′>0, define the set of δ′-closer class-irrelevant
patterns, denoted as Sc(δ′)⊂Sp
i=1Sjsuch that: ∀q∈ Sc(δ′),|⟨o1−o2, q⟩|> δ′.
7The pseudo network is defined as the network which activation pattern does not change from the initialization i.e., the sign of the
pre-activation output of hidden nodes does not change from the sign at initialization; see (Li & Liang, 2018) for details.
15

--- PAGE 16 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Definition D.3. (Threshold, l∗) Define the threshold l∗such that:
∀(x, y)∼ D,{j∈[n] :x(j)̸=o1andx(j)∈Sc 1−δd
2
}≤l∗−1
Lemma D.4. (Full version of Lemma 4.1 ) For every l≥l∗, w.h.p. over the random initialization defined in (7), after
completing the Step-2 of Algorithm-1 with batch-size Br=˜Ω 
n2
(1−δd)2!
and learning rate ηr= Θ 1
n
forTr=
Ω 
1
1−δd!
iterations, the returned w(Tr)
1andw(Tr)
2satisfy
arg
j∈[n](x(j)=o1)∈J1(w(Tr)
1, x),∀(x, y= +1) ∼ D
arg
j∈[n](x(j)=o2)∈J2(w(Tr)
2, x),∀(x, y=−1)∼ D
Proof. The proof follows directly from the Definition D.3 and the Lemma E.3.
E. Lemmas Used to Prove the Lemma 4.1
We denote,
∇w(t)
sE[ℓr(w1, w2)] :=ED"
∂ℓr(w(t)
1, w(t)
2)
∂w(t)
s#
where w(t)
s∈ {w(t)
1, w(t)
2}for all t∈[Tr].
Lemma E.1. At any iteration t≤Trof the Step-2 of Algorithm 1,
∇w(t)
1E[l(f(x), y)] =−1
2(o1−o2), and∇w(t)
2E[l(f(x), y)] =−1
2(o2−o1)
Proof. As,ℓr(w(t)
1, w(t)
2) =−1
BrP
(x,y)∈Br
ty⟨w(t)
1−w(t)
2,Pn
j=1x(j)⟩,
∇w(t)
1E[lr(w1, w2)] =−EDh
yPn
j=1x(j)i
and∇w(t)
2E[lr(w1, w2)] =EDh
yPn
j=1x(j)i
Therefore,
∇w(t)
1E[lr(w1, w2)] =−1
2ED|y=+1
nX
j=1x(j)|y= +1
+1
2ED|y=−1
nX
j=1x(j)|y=−1

=−1
2ED|y=+1
X
j∈[n]/arg
jx(j)=o1x(j)|y= +1
+1
2ED|y=−1
X
j∈[n]/arg
jx(j)=o2x(j)|y=−1

−1
2(o1−o2)
=−1
2(o1−o2)
where the last equality comes from the fact that class-irrelevant patterns are distributed identically in both classes. Using
similar line of arguments we can show that, ∇w(t)
2E[l(f(x), y)] =−1
2(o2−o1).
Lemma E.2. With probability 1−1
poly(n)(i.e., w.h.p.) over the random initialization of the gating kernels defined in (7),w(0)
s≤1
n2;∀s∈ {1,2}
16

--- PAGE 17 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Proof. Let us denote the i-th element of the vector w(0)
sasw(0)
siwhere i∈[d].
Then according to the random initialization of w(0)
sand using a Gaussian tail-bound (i.e., for X∼ N(0, σ2) :Pr[|X| ≥
t]≤2e−t2/2σ2):Phw(0)
si≥1
n2√
di
≤1
poly(n).
Let us denote the event E:∀i∈[d],w(0)
si≤1
n2√
d. Therefore, P[E]≥1−1
poly(n).
Now, conditioned on the event E,w(0)
s≤1
n2.
Therefore, Phw(0)
s≤1
n2i
≤Phw(0)
s≥1
n2|Ei
P[E] = 1−1
poly(n)
Lemma E.3. W.h.p. over the random initialization of the gating-kernels defined in (7) and randomly selected batch of
batch-size Br=˜Ω 
n2
(1−δd)2!
at each iteration, after Tr= Ω 
1
1−δd!
iterations of Step-2 of Algorithm 1 with learning
rateηr= Θ 1
n
,∀(x, y)∼ D, j∈[n] :x(j)̸∈ Sc(1−δd
2),⟨w(Tr)
1, o1⟩>⟨w(Tr)
1, x(j)⟩and⟨w(Tr)
2, o2⟩>⟨w(Tr)
2, x(j)⟩.
Proof. Let, at t-th iteration of Step-2 of Algorithm 1, ˜∇(t)
ws=∂ℓr(w(t)
1, w(t)
2)
∂w(t)
sfor all s∈ {1,2}
Also let us denote, ∇w(t)
sEh
ℓr(w(t)
1, w(t)
2)i
=∇(t)
wsfor all s∈ {1,2}
Therefore, after Tr-th iteration of SGD and using Lemma E.1,
w(Tr)
1=w(0)
1−ηrTr−1X
t=0˜∇(t)
w1
=w(0)
1+ηrTr
2(o1−o2)−ηrTr−1X
t=0
˜∇(t)
w1− ∇(t)
w1
Similarly, w(Tr)
2=w(0)
2+ηrTr
2(o2−o1)−ηrTr−1P
t=0
˜∇(t)
w2− ∇(t)
w2
.
Now,||˜∇(t)
ws||=O(n). Hence, w.h.p. over a randomly sampled batch of size Br, using Hoeffding’s concentra-
tion,
||˜∇(t)
ws− ∇(t)
ws||=˜O 
n√Br!
;∀s∈ {1,2}.
Now,
⟨w(Tr)
1, o1⟩=⟨w(0)
1, o1⟩+ηrTr
2(1− ⟨o1, o2⟩)−ηrTr−1X
t=0⟨˜∇(t)
w1− ∇(t)
w1, o1⟩
≥ηrTr
2(1−δd)−ηrTr˜O 
n√Br!
−w(0)
1
On the other hand, ∀(x, y)∼ D,∀j∈[n] :x(j)̸∈ Sc 1−δd
2
,
⟨w(Tr)
1, x(j)⟩=⟨w(0)
1, x(j)⟩+ηrTr
2
⟨o1, x(j)⟩ − ⟨o2, x(j)⟩
−ηrTr−1X
t=0⟨˜∇(t)
w1− ∇ w1, x(j)⟩
≤ηrTr
4(1−δd) +ηrTr˜O 
n√Br!
+w(0)
1
From Lemma E.2, w.h.p. over the random initialization:w(0)
1≤1
n2.
17

--- PAGE 18 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Therefore, selecting Br=˜Ω 
n2
(1−δd)2!
andηr= Θ 1
n
, we need Tr= Ω 
1
1−δd!
iterations to achieve
⟨w(Tr)
1, o1⟩>⟨w(Tr)
1, x(j)⟩,∀j∈[n] :x(j)∈ Sc 1−δd
2
Similar line of arguments can be made to show with batch size Br=˜Ω 
n2
(1−δd)2!
and learning rate ηr= Θ 1
n
, after
Tr= Ω 
1
1−δd!
iterations, ⟨w(Tr)
2, o2⟩ ≥ ⟨w(Tr)
2, x(j)⟩,∀j∈[n] :x(j)∈ Sc 1−δd
2
.
F. Proofs of the Theorem 4.2, 4.3 and 4.5
Definition F.1. At any iteration tof the minibatch SGD,
1.Define the value function ,v(t)(θ(t), x, y) :=1
1 +eyfM(θ(t),x). It is easy to show that for any (x, y)∼ D ,0≤
v(t)(θ(t), x, y)≤1. The function captures the prediction error, i.e., a larger v(t)indicates a larger prediction error.
2.Define, the class-conditional expected value function ,v(t)
1:=ED|y=+1[v(t)(θ(t), x, y)|y= +1] andv(t)
2:=
ED|y=−1[v(t)(θ(t), x, y)|y=−1]. Here, v(t)
1captures the expected error for the class with label y= +1 andv(t)
2
captures the expected error for the class with label y=−1.
Definition F.2. At any iteration tof the minibatch SGD,
1. For any sample (x, y)∼ D, we define the reduction of loss at the t-th iteration of SGD as,
∆L(θ(t), θ(t+1), x, y) :=L(θ(t), x, y)− L(θ(t+1), x, y)
where, L(θ(t), x, y) := log(1 + e−yfM(θ(t),x))is the single-sample loss function .
2. Define the expected reduction of loss at the t-th iteration of SGD as,
∆L(θ(t), θ(t+1)) :=EDh
L(θ(t), x, y)− L(θ(t+1), x, y)i
Theorem F.3. (Full version of Theorem 4.2 ) For every ϵ >0andl≥l∗, for every m≥MS=˜Ω 
l10p12δ6
ϵ16
with at
leastNS=˜Ω(l8p12δ6/ϵ16)training samples, after performing minibatch SGD with the batch size B=˜Ω 
l4p6δ3
ϵ8
and
the learning rate η=˜O 
1
mpoly(l, p, δ, 1/ϵ,logm)
forT=˜O 
l4p6δ3
ϵ8
iterations, it holds w.h.p. that
P
(x,y)∼D
yfM(θ(T), x)>0
≥1−ϵ
Proof. First we will show that for any ϵ <1
2, ifP(x,y)∼D
yf(θ(t), x)>0
≤1−ϵ, then max{v(t)
1, v(t)
2} ≥ϵ2.
Now for any (x, y)∼ D andϵ <1
2, ifv(t)(θ(t), x, y)≤ϵ,yfM(θ(t), x, y)>0i.e., the prediction is correct.
Now if v(t)
1=ED|y=+1
v(t)(θ(t), x, y)y= +1
≤ϵ2, then using Markov’s inequality PD|y=+1
v(t)(θ(t), x, y)≤ϵ
≥
1−ϵwhich implies for any ϵ <1
2,PD|y=+1
yf(θ(t), x)>0
≥1−ϵ.
Similarly, if v(t)
2=ED|y=−1
v(t)(θ(t), x, y)y=−1
≤ϵ2, for any ϵ <1
2,PD|y=−1
yf(θ(t), x)>0
≥1−ϵ.
Therefore, for any ϵ <1
2, ifP(x,y)∼D
yf(θ(t), x)>0
≤1−ϵ, then max{v(t)
1, v(t)
2} ≥ϵ2.
18

--- PAGE 19 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now, if v(t):=rP
s∈{1,2}(v(t)
s)2≤ϵ2then max{v(t)
1, v(t)
2} ≤ ϵ2, which implies after a proper number of itera-
tions if v(t)≤ϵ2then P
(x,y)∼D
yfM(θ(T), x)>0
≥1−ϵ.
Let,v(t)≥ϵ2. Then by using Lemma G.4 for every l≥l∗, with η=˜O 
ϵ4
ml2p3δ3/2!
andB=˜Ω 
l4p6δ3
ϵ8!
,
at least for t=˜O 
σϵ4
ηl3p3δ3/2!
we have,
∆L(θ(t), θ(t+1)) =˜Ω 
ηmϵ4
l2p3δ3/2!
(12)
Now, as w(0)
r,s∼ N (0, σ2)withσ=1√m,D
w(0)
r,s, x(j)E
∼ N (0, σ2)∀j∈Js(w(0)
s, x)and∀(x, y)∼ D . Therefore,
w.h.p.fM(θ(0), x)=˜O(1)which implies L(θ(0), x, y) = ˜O(1). Now as L(θ(t), x, y)>0, (12) can happen at most
˜O
l2p3δ3/2
ηmϵ4
iterations. Now as ηm=˜O 
ϵ4
l2p3δ3/2!
, we need T=˜O 
l4p6δ3
ϵ8!
iterations to ensure that v(t)≤ϵ2.
On the other hand, to ensure (12) hold for Titerations, we need,
σϵ4
ηl3p3δ3/2=˜Ω 
l2p3δ3/2
ηmϵ4!
which implies we need m=˜Ω 
l10p12δ6
ϵ16!
.
Now, for any (x, y= +1) ∼ D and(x, y=−1)∼ D, let us denote the index of the class-discriminative patterns i.e., o1
ando2asjo1andjo2, respectively.
Definition F.4. At any iteration tofminibatch SGD of the joint-training pMoE (i.e., Step-2 of Algorithm 2),
1.For any (x, y= +1) ∼ D and the expert s∈[k], define the event that o1in Top- las,E(t)
1,s:jo1∈Js(w(t)
s, x).
Similarly, for any (x, y=−1)∼ D define the event that o2in Top- las,E(t)
2,s:jo2∈Js(w(t)
s, x).
2.For any expert s∈[k], define the probability of the event that o1in Top- las,p(t)
1,s:=PD|y=+1h
E(t)
1,sy= +1i
and
theprobability of the event that o2in Top- las,p(t)
2,s:=PD|y=−1h
E(t)
2,sy=−1i
3.For any expert s∈[k]define, v(t)
1,s:=ED|y=+1,E(t)
1,sh
p(t)
1,sG(t)
jo1,s(x)v(t)(θ(t), x, y)y= +1 ,E(t)
1,si
andv(t)
2,s:=
ED|y=−1,E(t)
2,sh
p(t)
2,sG(t)
jo2,s(x)v(t)(θ(t), x, y)y=−1,E(t)
2,si
where G(t)
jo1,s(x)andG(t)
jo2,s(x)denote the gating value
for the class-discriminative patterns o1ando2conditioned on E(t)
1,sandE(t)
2,s, respectively.
Theorem F.5. (Full version of the Theorem 4.5 ) Suppose Assumption 4.4 hold. Then for every ϵ >0, for every m≥MJ=
˜Ω 
k3n2l6p12δ6
ϵ16
with at least NJ=˜Ω(k4l6p12δ6/ϵ16)training samples, after performing minibatch SGD with the
batch size B=˜Ω 
k2l4p6δ3
ϵ8
and the learning rate η=˜O 
1
mpoly (l, p, δ, 1/ϵ,logm)
forT=˜O 
k2l2p6δ3
ϵ8
iterations, it holds w.h.p. that
P
(x,y)∼D
yfM(θ(T), x)>0
≥1−ϵ
Proof. From the argument of the proof of Theorem F.3, we know that for any ϵ <1
2, ifP(x,y)∼D
yf(θ(t), x)>0
≤1−ϵ,
thenmax{v(t)
1, v(t)
2} ≥ϵ2where v(t)
1:=ED|y=+1[v(t)(θ(t), x, y)|y= +1] andv(t)
2:=ED|y=−1[v(t)(θ(t), x, y)|y=−1]
19

--- PAGE 20 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now, we will consider the case when t≥T′where T′is defined in Assumption 4.4.
Now, if the expert s1∈[k]satisfies Assumption 4.4 for y= +1 , then p(t)
1,s1= 1 andG(t)
jo1,s1(x)≥1
lfor any
(x, y= +1) ∼ D. Therefore, v(t)
1,s1≥v(t)
1
l.
Similarly, if the expert s2∈[k]satisfies Assumption 4.4 for y=−1, then v(t)
2,s2≥v(t)
2
l.
Now for any expert s∈[k], let us define v(t)
s:= max {v(t)
1,s, v(t)
2,s}
Now, if v(t):=rP
s∈[k](v(t)
s)2≤ϵ2
l, then v(t)
s1≤ϵ2
landv(t)
s2≤ϵ2
l.
This implies, max{v(t)
1,s1, v(t)
2,s1} ≤ϵ2
landmax{v(t)
1,s2, v(t)
2,s2} ≤ϵ2
l.
Therefore, v(t)
1,s1≤ϵ2
landv(t)
2,s2≤ϵ2
lwhich implies v(t)
1≤ϵ2andv(t)
2≤ϵ2.
In that case, max{v(t)
1, v(t)
2} ≤ϵ2.
Therefore, by taking v(t)≥ϵ2
l, using the results of Lemma H.4 and following same procedure as in Theorem
F.3 we can complete the proof.
Theorem F.6. (Full version of the Theorem 4.3 ) For every ϵ >0, for every m≥MC=˜Ω 
n10p12δ6
ϵ16
with at least
NC=˜Ω(n8p12δ6/ϵ16)training samples, after performing minibatch SGD with the batch size B=˜Ω 
n4p6δ3
ϵ8
and the
learning rate η=˜O 
1
mpoly(n, p, δ, 1/ϵ,logm)
forT=˜O 
n4p6δ3
ϵ8
iterations, it holds w.h.p. that
P
(x,y)∼D
yfC(θ(T), x)>0
≥1−ϵ
Proof. From the argument of the proof of Theorem F.3, we know that for any ϵ <1
2, ifP(x,y)∼D
yf(θ(t), x)>0
≤1−ϵ,
then v(t):= max {v(t)
1, v(t)
2} ≥ ϵ2where v(t)
1 :=ED|y=+1[v(t)(θ(t), x, y)|y= +1] and v(t)
2 :=
ED|y=−1[v(t)(θ(t), x, y)|y=−1].
Therefore, taking v(t)≥ϵ2, using the results of Lemma I.3 and following similar procedure as in Theorem F.3
we can complete the proof.
G. Lemmas Used to Prove the Theorem 4.2
For any iteration tof the Step-3 of Algorithm 1, recall the loss function for a single-sample generated by the distribution D,
L(θ(t), x, y) := log(1 + e−yfM(θ(t),x)). The gradient of the loss for a single sample with respect to the hidden nodes of the
experts:
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(t)
r,s,x(j)⟩≥0
 (13)
We define the corresponding pseudo-gradient as:
∼
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
 (14)
20

--- PAGE 21 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Therefore, the expected pseudo-gradient:
∼
∂ˆL(θ(t))
∂w(t)
r,s=ED
∼
∂L(θ(t), x, y)
∂w(t)
r,s

=−ar,s
2
ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,Pjx⟩≥0
y=−1


=−ar,s
2P(t)
r,s
Here,
P(t)
r,s=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y=−1

Lemma G.1. W.h.p. over the random initialization of the hidden nodes of the experts defined in 8, for every (x, y)∼ D
and for every τ >0, for every t=˜O 
τ
η!
of the Step-3 of Algorithm 1, we have that for at least 
1−2eτl
σ!
fraction of
r∈[m/2]of the expert s∈ {1,2}:
∂L(θ(t), x, y)
∂w(t)
r,s=∼
∂L(θ(t), x, y)
∂w(t)
r,sand|⟨w(t)
r,s, x(j)⟩| ≥τ,∀j∈Js(w(t)
s, x)
Proof. Recall the gradient of the loss for single-sample (x, y)∼ D w.r.t. the hidden node r∈[m/2]of the expert s∈ {1,2}:
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(t)
r,s,x(j)⟩≥0

and the corresponding pseudo-gradient:
∼
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)x(j)1⟨w(0)
r,s,x(j)⟩≥0

Now, ar,s∼ N (0,1). Hence, using the concentration bound of Gaussian random variable (i.e., for X∼ N (0, σ2) :
Pr[|X| ≤t]≥1−2e−t2/2σ2) and as ˜O(.)hides factor log 
poly(m, n, p, δ,1
ϵ)
we get:
Ph
|ar,s|=˜O(1)i
≥1−1
poly(m, n, p, δ,1
ϵ)(i.e., w.h.p.)
Now as v(t)(θ(t), x, y)≤1and||x(j)||= 1 , w.h.p.∂L(θ(t),x,y)
∂w(t)
r,s=˜O(1)so as the mini-batch gradient,
∂L(θ(t))
∂w(t)
r,s=˜O(1).
21

--- PAGE 22 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now, from the update rule of the Step-3 of Algorithm 1, w(t)
r,s−w(t+1)
r,s =η∂L(θ(t))
∂w(t)
r,s
Therefore, using the property of Telescoping series, w(0)
r,s−w(t)
r,s=ηtX
i=1∂L(θ(i−1))
∂w(t)
r,s
Therefore,w(t)
r,s−w(0)
r,s= Υηtwhere we denote ˜O(1)byΥ
Now, for every τ >0,consider the set Hs:=n
r∈[m/2] :∀j∈Js(w(t)
s, x),|⟨w(0)
r,s, x(j)⟩| ≥2τo
Now, for every t≤τ
Υη,|⟨w(t)
r,s−w(0)
r,s, x(j)⟩| ≤τ
Which implies for every r∈ H s,t≤τ
Υηandj∈Js(w(t)
s, x),|⟨w(t)
r,s, x(j)⟩| ≥τ
Therefore, for every r∈ H s,t=˜O(τ
η)andj∈Js(w(t)
s, x),1⟨w(t)
r,s,x(j)⟩≥0= 1⟨w(0)
r,s,x(j)⟩≥0and hence,
∂L(θ(t), x, y)
∂w(t)
r,s=∼
∂L(θ(t), x, y)
∂w(t)
r,s
Now, we will find the lower bound of |Hs|:
As,w(0)
r,s∼ N(0, σ2Id×d),∀j∈Js(w(t)
s, x),⟨w(0)
r,s, x(j)⟩ ∼ N (0, σ2)
Hence, P[|⟨w(0)
r,s, x(j)⟩| ≤2τ]≤2eτ
σ
Now as |Js(w(t)
s, x)|=l,P[∀j∈Js(w(t)
s, x),|⟨w(0)
r,s, x(j)⟩| ≥2τ]≥1−2eτl
σ
Therefore, |Hs| ≥ 
1−2eτl
σ!
m
2
Using the following two lemmas we show that when v(t)
1is large, the expected pseudo-gradient of the loss function w.r.t.
the hidden nodes of the expert 1 is large. Similar thing happens for expert 2 when v(t)
2is large. We prove the first of
these two lemmas for a fixed set {v(t)(θ(t), x, y) : (x, y)∼ D} which does not depend on the random initialization of the
hidden nodes of the experts (i.e., on {w(0)
r,s}). In the second of these two lemmas we remove the dependency on fixed
set by means of a sampling trick introduced in (Li & Liang, 2018) to take a union bound over an epsilon-net on the set
{v(t)(θ(t), x, y) : (x, y)∼ D} .
Lemma G.2. For any possible fixed set {v(t)(θ(t), x, y) : (x, y)∼ D} (that does not depend on w(0)
r,s) such that v(t)
s=v(t)
1
fors= 1andv(t)
s=v(t)
2fors= 2we have for every l≥l∗:
P"
||P(t)
r,s||=∼
Ω 
v(t)
s
lp√
δ!#
= Ω 
1
p√
δ!
Proof. WLOG, let’s assume s= 1. Now,
P(t)
r,1=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)x(j)1⟨w(0)
r,1,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)x(j)1⟨w(0)
r,1,x(j)⟩≥0
y=−1

22

--- PAGE 23 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Then,
h(w(0)
r,1) :=⟨Pr,1, w(0)
r,1⟩
=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
⟨w(0)
r,1, x(j)⟩
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
⟨w(0)
r,1, x(j)⟩
y=−1

Now, let us decompose w(0)
r,1=αo1+β, where β⊥o1
Then,
h(w(0)
r,1) =ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩
y=−1

=ϕ(α)−l(α)
Where,
ϕ(α) :=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩
y= +1

and,
l(α) :=ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈J1(w(t)
1,x)ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩
y=−1

Note that, ϕ(α)andl(α)both are convex functions.
Now for l≥l∗, using Lemma D.4, we can express ϕ(α)as follows:
ϕ(α) =v(t)
1
lReLU (α)
+ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈J1(x)/arg
j∈J1(x)x(j)=o1ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩
y= +1

Now, for any class-irrelevant pattern set Siwhere i∈[p], let us define q∗
i∈Sisuch that q∗
i=ESi[q]
||ESi[q]||. Also, let us define
the set, H:={q∗
i:i∈[p]} ∪ {o2}
Now let us define the event Eτ: (i)|α| ≤τ; (ii)∀q′∈ H:|⟨β, q′⟩| ≥4τ
23

--- PAGE 24 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now, as α∼ N(0, σ2), for every q′∈ H,⟨β, q′⟩ ∼ N 
0, 
1− ⟨o1, q′⟩2
σ2
Now, 1− ⟨o1, q′⟩2≥1
δ. Hence, P[∃q′∈ H:|⟨β, q′⟩| ≤4τ]≤4eτp√
δ
σ
Therefore, P[∀q′∈ H:|⟨β, q′⟩| ≥4τ]≥1−4eτp√
δ
σ.
Picking, τ≤σ
8ep√
δgives,P[∀q′∈ H:|⟨β, q′⟩| ≥4τ]≥1
2.
On the other hand, P[|α| ≤τ] = Ω 
τ
σ!
. Therefore, P[Eτ] = Ω 
τ
σ!
Now,∀i∈[p]s.t.q∈Si,E[|⟨w(0)
r,1, q−q∗
i⟩|]≤EN(0,σ2Id×d)[||w(0)
r,1||]ESi[||q−q∗
i||]≤τ, where the last inequality comes
from the bound of the diameter of the pattern sets and the fact that for any X∼ N(0, σ2Id×d),E[||X||]≤4σ√
d.
Therefore, using Markov’s inequality ∀i∈[p]s.t.q∈Si,P[|⟨w(0)
r,1, q−q∗
i⟩| ≤2τ]≥1
2
Now,
∀i∈[p],s.t.q∈Si,ReLU (α⟨o1, q⟩+⟨β, q⟩) =ReLU
α⟨o1, q∗
i⟩+⟨β, q∗
i⟩+⟨w(0)
r,1, q−q∗
i⟩
Now, conditioned on the event Eτ, for a fixed βandαis the only random variable,
∀i∈[p]s.t.q∈Si,ReLU (α⟨o1, q⟩+⟨β, q⟩) = ( α⟨o1, q⟩+⟨β, q⟩) 1⟨β,q∗
i⟩≥0which is a linear function of α∈[−τ, τ]
with probability at least1
2and, ReLU (α⟨o1, o2⟩+⟨β, o2⟩) = (α⟨o1, o2⟩+⟨β, o2⟩) 1⟨β,o2⟩≥0which is a linear function of
α∈[−τ, τ]with probability 1.
Now, let us define {∂l(α)}and{∂ϕ(α)}as the set of sub-gradient at the point αforl(α)andϕ(α)respectively
such that ∂maxl(α) =max{∂l(α)},∂maxϕ(α) =max{∂ϕ(α)},∂minl(α) =min{∂l(α)}and∂minϕ(α) =min{∂ϕ(α)}.
Then, using the above argument, conditioned on the event Eτ,∂maxl(τ)−∂minl(−τ) = 0 .
On the other hand, ∂maxϕ(τ/2)−∂minϕ(−τ/2) =v(t)
1
l.
Now using Lemma J.1, conditioned on the event Eτ,P
α∼U(−τ,τ)"
|ϕ(α)−l(α)| ≥v(t)
1τ
512l#
≥1
64.
Now, for τ≤σ
8ep√
δ, conditioned on Eτ, the density p(α)∈"
1
eτ,e
τ#
, which implies that,
P"
h(w(0)
r,1)≥v(t)
1τ
128l#
≥P"
h(w(0)
r,1)≥v(t)
1τ
128lEτ#
P[Eτ] = Ω 
τ
σ!
(15)
Now, as v(t)
1does not depends on w(0)
r,1,⟨P(t)
r,1, w(0)
r,1⟩ ∼ N (0, σ2||P(t)
r,1||2).
Now, using a concentration bound of Gaussian RV (i.e., P[X≥σx]≤e−x2/2),
P[⟨P(t)
r,1, w(0)
r,1⟩ ≥(σ||P(t)
r,1||)10c]≤e−50c2;herec >10. (16)
Now, taking c= 100s
logp√
δ
σin (16) we get,
P[⟨P(t)
r,1, w(0)
r,1⟩=˜Ω(σ||P(t)
r,1||)] =o(1) (17)
24

--- PAGE 25 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
On the other hand, picking τ= Θ(σ
p√
δ)and plugging in at (15) gives,
P"
⟨P(t)
r,1, w(0)
r,1⟩= Ω 
σv(t)
1
lp√
δ!#
= Ω1
p√
δ
(18)
Comparing (17) and (18) we get, P"
||P(t)
r,1||=˜Ω 
v(t)
1
lp√
δ!#
= Ω 
1
p√
δ!
Lemma G.3. Letv(t)
s=v(t)
1fors= 1andv(t)
s=v(t)
2fors= 2. Then, for every v(t)
s>0, form=˜Ω 
l2p3δ3/2
(v(t)
s)2!
, for
every possible set {v(t)(θ(t), x, y) : (x, y)∼ D} (that depends on w(0)
r,s), there exist at least Ω
1
p√
δ
fraction of r∈[m/2]
of the expert s∈ {1,2}such that for every l≥l∗,
∼
∂ˆL(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Proof. Let us pick Ssamples to form S={(xi, yi)}S
i=1withS/2many samples from y= +1 andS/2many samples from
y=−1. Let us denote the subset of samples with y= +1 asS1and the subset of samples with y=−1asS2. Therefore,
|S1|=|S2|=S/2. Let us denote the corresponding value function of i-th sample of Sasv(t)(θ(t), xi, yi). Since, each
v(t)(θ(t), xi, yi)∈[0,1]using Hoeffding’s inequality we know that w.h.p. :
v(t)
s−1
S/2X
(xi,yi)∈Ssv(t)(θ(t), xi, yi)=˜O 
1√
S!
This implies that, as long as S=˜Ω 
1
(v(t)
s)2!
, we will have that,
1
S/2X
(xi,yi)∈Ssv(t)(θ(t), xi, yi)∈"
1
2v(t)
s,3
2v(t)
s#
Now, the average pseudo-gradient over the set S,
1
SX
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=1
SX
(xi,yi)∈S−yar,sv(t)(θ(t), xi, yi)
1
lX
j∈Js(w(t)
s,xi)x(j)
i1⟨w(0)
r,s,x(j)
i⟩≥0

=−ar,s
2P(t)
r,s(S)
where,
P(t)
r,s(S) =1
S/2X
(xi,yi)∈S1v(t)(θ(t), xi, yi)
1
lX
j∈Js(w(t)
s,xi)x(j)
i1⟨w(0)
r,s,x(j)
i⟩≥0

−1
S/2X
(xi,yi)∈S2v(t)(θ(t), xi, yi)
1
lX
j∈Js(w(t)
s,xi)x(j)
i1⟨w(0)
r,s,x(j)
i⟩≥0

25

--- PAGE 26 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now as ar,s∼ N(0,1),P
1
SP
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=ar,s
2P(t)
r,s(S)≥1
2P(t)
r,s(S)
≥1
e
Now for a fixed set {v(t)(θ(t), xi, yi) : (xi, yi)∈S}as long as S=˜Ω 
1
v2s(t)!
, for every l≥l∗using Lemma
G.2,
P"
||P(t)
r,s(S)||=˜Ω 
v(t)
s
lp√
δ!#
= Ω 
1
p√
δ!
Hence, for a fixed set {v(t)(θ(t), xi, yi) : (xi, yi)∈S}, the probability that there are less than O
1
p√
δ
fraction of
r∈[m/2]such that1
SP
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,sis˜Ω 
v(t)
s
lp√
δ!
is no more than pfixwhere, pfix≤exp
−Ω
m
p√
δ
.
Moreover, for every ¯ε > 0, for two different {v(t)(θ(t), xi, yi) : (xi, yi)∈S},{v′(t)(θ(t), xi, yi) : (xi, yi)∈S}
such that ∀(xi, yi)∈S,|v(t)(θ(t), xi, yi)−v′(t)(θ(t), xi, yi)| ≤¯ε, since w.h.p. |ar,s|=˜O(1),
1
SX
(xi,yi)∈S−yar,s(v(t)(θ(t), xi, yi)−v′(t)(θ(t), xi, yi))
1
lX
j∈Js(w(t)
s,xi)x(j)
i1⟨w(0)
r,s,x(j)
i⟩≥0

=˜O(¯ε)
which implies that we can take ¯ε-net with ¯ε=˜Θ 
v(t)
s
lp√
δ!
.
Thus, the probability that there exists {v(t)(θ(t), xi, yi) : ( xi, yi)∈S}such that there are no more
than O
1
p√
δ
fraction of r∈[m/2]with1
SP
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
is no more than,
p≤pfix 
v(t)
s
¯ε!S
≤exp 
−Ω
m
p√
δ
+Slog 
v(t)
s
¯ε!!
.
Hence, for m=∼
Ω
Sp√
δ
withS=˜Ω 
1
v2s(t)!
, w.h.p. for every possible choice of {v(t)(θ(t), xi, yi) : (xi, yi)∈S},
there are at least Ω
1
p√
δ
fraction of r∈[m/2]such that,
1
SX
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now, we consider the difference between the sample gradient and the expected gradient. Since,∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=∼
O(1),
by using the Hoeffding’s inequality, we know that for every r∈[m/2]:
1
SX
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s−∼
∂ˆL(θ(t))
∂w(t)
r,s=˜O 
1√
S!
26

--- PAGE 27 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
This implies that as long as S=˜Ω
 
lp√
δ
v(t)
s!2
and hence for m=˜Ω 
l2p3δ3/2
(v(t)
s)2!
, such r∈[m/2]also have:
∼
∂ˆL(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Lemma G.4. Let us define v(t):=rP
s∈{1,2}(v(t)
s)2where v(t)
s=v(t)
1fors= 1andv(t)
s=v(t)
2fors= 2;γ:= Ω
1
p√
δ
.
Then, by selecting learning rate η=˜O 
γ3(v(t))2
ml2!
and batch size B=˜Ω 
l4
γ6(v(t))4!
, at each iteration tof the Step-3
of Algorithm 1 such that t=˜O 
σγ3(v(t))2
ηl3!
, w.h.p. we can ensure that for every l≥l∗,
∆L(θ(t), θ(t+1))≥ηmγ3
l2˜Ω
(v(t))2
Proof. For every l≥l∗, from Lemma G.3, for at least γfraction of r∈[m/2]of expert s:
∼
∂ˆL(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now w.h.p.,˜∂L(θ(t), x, y)
∂w(t)
r,s=∼
O(1). Therefore, w.h.p. over a randomly sampled batch from Dat iteration tdenoted as
Btof size B:
1
BX
(x,y)∈Bt∼
∂L(θ(t), x, y)
∂w(t)
r,s−∼
∂ˆL(θ(t))
∂w(t)
r,s=˜O 
1√
B!
This implies, by selecting batch-size of B= Ω 
l2p2δ
(v(t)
s)2!
, for these γfraction of r∈[m/2]of expert swe can ensure that:
1
BX
(xi,yi)∈Bt∼
∂L(θ(t), x, y)
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now using Lemma G.1, for a fixed (x, y)∈ Bt, by selecting τ=σγ
4elBwe have 
1−γ
2B!
fraction of r∈[m/2]of the
expert s:
∂L(θ(t), x, y)
∂w(t)
r,s=∼
∂L(θ(t), x, y)
∂w(t)
r,s
Therefore, at least (1−γ/2)fraction of r∈[m/2]of the expert s:
∂L(θ(t), x, y)
∂w(t)
r,s=∼
∂L(θ(t), x, y)
∂w(t)
r,s∀(x, y)∈ Bt
27

--- PAGE 28 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Recall our definition of loss-function for SGD at iteration twith mini-batch Bt,L(θ(t)) =
1
BP
(x,y)∈Btlog (1 + e−yfM(θ(t),x)) =1
BP
(x,y)∈BtL(θ(t), x, y)and the corresponding batch-gradient at itera-
tiont,∂L(θ(t))
∂w(t)
r,s=1
BP
(x,y)∈Bt∂L(θ(t), x, y)
∂w(t)
r,s. Therefore, there are at least γ/2fraction of r∈[m/2]of the expert
s:
∂L(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now for any (x′, y′)∼ D, according to Lemma G.1, w.h.p. there are at least 1−2eτl
σfraction of r∈[m/2]of the expert
ssuch that ∀j∈Js(w(t)
s, x′),|⟨w(t)
r,s, x′(j)⟩| ≥ τ. Let us denote the set of these r’s ofsasSr,s. Therefore, on the set
S
s∈{1,2}Sr,s, the loss function L(θ(t), x′, y′)is∼
O(1)-smooth and∼
O(1)-Lipschitz smooth.
On the other hand, the update rule of SGD at the iteration tis,θ(t+1)=θ(t)−η∂L(θ(t))
∂w(t)
r,s
Therefore, using Lemma J.2,
∆L(θ(t), θ(t+1), x′, y′) :=L(θ(t), x′, y′)− L(θ(t+1), x′, y′)
≥ηX
r∈S
s∈[2]Sr,s*
∂L(θ(t))
∂w(t)
r,s,∂L(θ(t), x′, y′)
∂w(t)
r,s+
−X
s∈[2],r∈[m/2]\ ∪
s∈[2]Sr,s∼
O(η)−∼
O 
η2m2
≥ηX
r∈[m/2],s∈[2]*
∂L(θ(t))
∂w(t)
r,s,∂L(θ(t), x′, y′)
∂w(t)
r,s+
−∼
O 
ητlm
σ!
−∼
O 
η2m2
Let us denote the event,
E0:
∆L(θ(t), θ(t+1), x′, y′)
≥ηX
r∈[m/2],s∈[2]*
∂L(θ(t))
∂w(t)
r,s,∂L(θ(t), x′, y′)
∂w(t)
r,s+
−∼
O 
ητlm
σ!
−∼
O 
η2m2
Then,P[E0]≥1−1
poly(m, n, p, δ,1
ϵ)(i.e., w.h.p.) and hence P[¬E0]≤1
poly(m, n, p, δ,1
ϵ)
Also, let us define the event,
E1:
L(θ(t), x′, y′)=˜O(m),∂L(θ(t), x′, y′)
∂w(t)
r,s=˜O(1)and∂L(θ(t))
∂w(t)
r,s=˜O(1)
Then,P[E1]≥1−1
poly(m, n, p, δ,1
ϵ)and hence P[¬E1]≤1
poly(m, n, p, δ,1
ϵ)
Now, the expected gradient at iteration t,∂ˆL(θ(t))
∂w(t)
r,s:=ED"
∂L(θ(t), x′, y′)
∂w(t)
r,s#
28

--- PAGE 29 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Therefore condition on E1,
∂ˆL(θ(t))
∂w(t)
r,s=ED"
∂L(θ(t), x′, y′)
∂w(t)
r,sE1#
=ED"
∂L(θ(t), x′, y′)
∂w(t)
r,sE0,E1#
P
E0E1
+ED"
∂L(θ(t), x′, y′)
∂w(t)
r,s¬E0,E1#
P
¬E0E1
Which implies,
∂ˆL(θ(t))
∂w(t)
r,s−ED"
∂L(θ(t), x′, y′)
∂w(t)
r,sE0,E1#≤˜O(1)
poly(m, n, p, δ,1
ϵ)
Again, condition on E1,
∆L(θ(t), θ(t+1)) :=EDh
L(θ(t), x′, y′)− L(θ(t+1), x′, y′)E1i
=EDh
L(θ(t), x′, y′)− L(θ(t+1), x′, y′)E0,E1i
P
E0E1
+EDh
L(θ(t), x′, y′)− L(θ(t+1), x′, y′)¬E0,E1i
P
¬E0E1
≥ηX
r∈[m/2],s∈[2]*
∂L(θ(t))
∂w(t)
r,s,ED"
∂L(θ(t), x′, y′)
∂w(t)
r,sE0,E1#+
−∼
O 
ητlm
σ!
−∼
O 
η2m2
−˜O(m)
poly(m, n, p, δ,1
ϵ)
≥ηX
r∈[m/2],s∈[2]*
∂L(θ(t))
∂w(t)
r,s,∂ˆL(θ(t))
∂w(t)
r,s+
−∼
O 
ητlm
σ!
−∼
O 
η2m2
−˜O(m)
poly(m, n, p, δ,1
ϵ)
−˜O(ηm)
poly(m, n, p, δ,1
ϵ)
≥ηX
r∈[m/2],s∈[2]*
∂L(θ(t))
∂w(t)
r,s,∂ˆL(θ(t))
∂w(t)
r,s+
−∼
O 
ητlm
σ!
−∼
O 
η2m2
Now, w.h.p.
∂L(θ(t))
∂w(t)
r,s−∂ˆL(θ(t))
∂w(t)
r,s=˜O 
1√
B!
Therefore,
*
∂L(θ(t))
∂w(t)
r,s,∂ˆL(θ(t))
∂w(t)
r,s+
≥∂L(θ(t))
∂w(t)
r,s2
−˜O 
1√
B!
Therefore,
∆L(θ(t), θ(t+1))≥ηX
r∈[m],s∈[2]∂L(θ(t))
∂w(t)
r,s2
−˜O 
ητlm
σ!
−˜O 
η2m2
−η˜O 
m√
B!
≥ηmγ3
l2˜Ω
X
s∈[2](v(t)
s)2
−˜O 
ητlm
σ!
−˜O 
η2m2
−η˜O 
m√
B!
≥ηmγ3
l2∼
Ω
(v(t))2
−˜O 
ητlm
σ!
−˜O 
η2m2
−η˜O 
m√
B!
29

--- PAGE 30 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Now selecting, η=˜O 
γ3(v(t))2
ml2!
,B=˜Ω 
l4
γ6(v(t))4!
,τ=˜O 
σγ3(v(t))2
l3!
and hence for
t=˜O 
σγ3(v(t))2
ηl3!
, we get,
∆L(θ(t), θ(t+1))≥ηmγ3
l2˜Ω
(v(t))2
H. Lemmas Used to Prove the Theorem 4.5
In joint-training pMoE i.e., for any iteration tof the Step-2 of Algorithm 2, the gradient of the loss for single-sample with
respect to the hidden nodes of the experts:
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(t)
r,s,x(j)⟩≥0
 (19)
and the corresponding pseudo-gradient :
∼
∂L(θ(t), x, y)
∂w(t)
r,s=−yar,sv(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
 (20)
and the expected pseudo-gradient:
∼
∂ˆL(θ(t))
∂w(t)
r,s=ED
∼
∂L(θ(t), x, y)
∂w(t)
r,s

=−ar,s
2
ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,Pjx⟩≥0
y=−1


=−ar,s
2P(t)
r,s
with,
P(t)
r,s=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y=−1

Lemma H.1. W.h.p. over the random initialization of the hidden nodes of the experts defined in (8), for every (x, y)∼ D
and for every τ >0, for every t=˜O 
τl
η!
of the Step-2 of Algorithm 2, we have that for at least 
1−2eτn
σ!
fraction of
r∈[m/k]of the expert s∈[k]:
∂L(θ(t), x, y)
∂w(t)
r,s=∼
∂L(θ(t), x, y)
∂w(t)
r,sand|⟨w(t)
r,s, x(j)⟩| ≥τ,∀j∈[n]
30

--- PAGE 31 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Proof. Using similar argument as in Lemma G.1 and asP
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x) = 1 w.h.p.∂L(θ(t),x,y)
∂w(t)
r,s=˜O 1
l
so
as the mini-batch gradient,∂L(θ(t))
∂w(t)
r,s=˜O 1
l
.
Therefore,w(t)
r,s−w(0)
r,s=˜O ηt
l
.
Now, for every τ > 0,considering the set Hs:=n
r∈[m/k] :∀j∈[n],|⟨w(0)
r,s, x(j)⟩| ≥2τo
and following the
same procedure as in Lemma G.1 we can complete the proof.
Lemma H.2. For the expert s∈[k]and any possible fixed set {v(t)(θ(t), x, y)Gj,s(w(t)
s, x) : (x, y)∼ D, j∈Js(w(t)
s, x)}
(that does not depend on w(0)
r,s) such that v(t)
s=v(t)
1,s= max {v(t)
1,s, v(t)
2,s}, we have:
P"
||P(t)
r,s||=∼
Ω 
v(t)
s
lp√
δ!#
= Ω 
1
p√
δ!
Proof. We know that,
P(t)
r,s=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)x(j)1⟨w(0)
r,s,x(j)⟩≥0
y=−1

Therefore,
h(w(0)
r,s) :=⟨Pr,s, w(0)
r,s⟩
=ED|y=+1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)ReLU
⟨w(0)
r,s, x(j)⟩
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(w(t)
s,x)Gj,s(w(t)
s, x)ReLU
⟨w(0)
r,s, x(j)⟩
y=−1

Now, decomposing w(0)
r,s=αo1+βwithβ⊥o1we get,
h(w(0)
r,s) =v(t)
1,s
lReLU (α)
+ED|y=+1,E(t)
1,s
p(t)
1,sv(t)(θ(t), x, y)
1
lX
j∈Js(x)/jo1Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


+ED|y=+1,¬E(t)
1,s
(1−p(t)
1,s)v(t)(θ(t), x, y)
1
lX
j∈Js(x)Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


−ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(x)Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


=ϕ(α)−l(α)
31

--- PAGE 32 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
where,
ϕ(α) :=v(t)
1,s
lReLU (α)
+ED|y=+1,E(t)
1,s
p(t)
1,sv(t)(θ(t), x, y)
1
lX
j∈Js(x)/jo1Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


+ED|y=+1,¬E(t)
1,s
(1−p(t)
1,s)v(t)(θ(t), x, y)
1
lX
j∈Js(x)Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


and
l(α) :=ED|y=−1
v(t)(θ(t), x, y)
1
lX
j∈Js(x)Gj,sReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


Now as ϕ(α)andl(α)both are convex functions, using the same procedure as in Lemma G.1 we can complete the proof.
Lemma H.3. Letv(t)
s= max {v(t)
1,s, v(t)
2,s}. Then, for every v(t)
s>0, form=˜Ω 
klp3δ3/2
(v(t)
s)2!
, for every possible set
{v(t)(θ(t), x, y)Gj,s(w(t)
s, x) : (x, y)∼ D, j∈Js(w(t)
s, x)}(that depends on w(0)
r,s), there exist at least Ω
1
p√
δ
fraction
ofr∈[m/k]of the expert s∈[k]such that,
∼
∂ˆL(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Proof. Let us pick Ssamples to form S={(xi, yi)}S
i=1withS/2many samples from y= +1 such that1
2p(t)
1,sSmany
samples of them satisfy the event E(t)
1,sandS/2many samples from y=−1such that1
2p(t)
2,sSmany samples of them satisfy
the event E(t)
2,s. We denote the subset of Ssatisfying the event E(t)
1,sbyS1and the subset of Ssatisfying the event E(t)
2,sbyS2.
Therefore, |S1|=1
2p(t)
1,sSand|S2|=1
2p(t)
2,sS. Now, w.h.p. :
v(t)
1,s−2
p(t)
1,sSX
(xi,yi)∈S1p(t)
1,sG(t)
jo1,s(xi)v(t)(θ(t), xi, yi)=˜O
1q
p(t)
1,sS
and
v(t)
2,s−2
p(t)
2,sSX
(xi,yi)∈S2p(t)
2,sG(t)
jo2,s(xi)v(t)(θ(t), xi, yi)=˜O
1q
p(t)
2,sS

This implies that, as long as S=˜Ω 
1
(v(t)
s)2!
, we will have that,
max

2
p(t)
1,sSX
(xi,yi)∈S1p(t)
1,sG(t)
jo1,s(xi)v(t)(θ(t), xi, yi),
2
p(t)
2,sSX
(xi,yi)∈S2p(t)
2,sG(t)
jo2,s(xi)v(t)(θ(t), xi, yi)

∈"
1
2v(t)
s,3
2v(t)
s#
Now using the same procedure as in Lemma G.3 and using Lemma H.2 we can show that, for a fixed set
{v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi) : (xi, yi)∈S, j∈Js(w(t)
s, xi)}as long as S=˜Ω 
1
(v(t)
s)2!
, the probability that
32

--- PAGE 33 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
there are less than O
1
p√
δ
fraction of r∈[m/k]such that1
SP
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,sis˜Ω 
v(t)
s
lp√
δ!
is no more than
pfixwhere, pfix≤exp
−Ω
m
kp√
δ
.
Now, for every ¯ε > 0, for two different {v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi) : ( xi, yi)∈S, j∈Js(w(t)
s, xi)},
{v′(t)(θ(t), xi, yi)G′
j,s(w(t)
s, xi) : ( xi, yi)∈S, j∈Js(w(t)
s, xi)}such that ∀(xi, yi)∈S, j∈Js(w(t)
s, xi),
|v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi)−v′(t)(θ(t), xi, yi)G′
j,s(w(t)
s, xi)| ≤¯ε, w.h.p.,
1
SX
(xi,yi)∈S−yar,s
lX
j∈Js(w(t)
s,xi)
v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi)
−v′(t)(θ(t), xi, yi)G′
j,s(w(t)
s, xi)
x(j)
i1⟨w(0)
r,s,x(j)
i⟩≥0=˜O(¯ε)
Therefore taking ¯ε-net with ¯ε= ˜Θ 
v(t)
s
lp√
δ!
we can show that the probability that there exists
{v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi) : ( xi, yi)∈S, j∈Js(w(t)
s, xi)}such that there are no more than O
1
p√
δ
frac-
tion of r∈[m/k]with1
SP
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
is no more than, p≤pfix 
v(t)
s
¯ε!Sl
≤
exp 
−Ω
m
kp√
δ
+Sllog 
v(t)
s
¯ε!!
.
Hence, for m=∼
Ω
kSlp√
δ
withS=˜Ω 
1
(v(t)
s)2!
, w.h.p. for every possible choice of {v(t)(θ(t), xi, yi)Gj,s(w(t)
s, xi) :
(xi, yi)∈S, j∈Js(w(t)
s, xi)}, there are at least Ω
1
p√
δ
fraction of r∈[m/k]such that,
1
SX
(xi,yi)∈S∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now as∼
∂L(θ(t), xi, yi)
∂w(t)
r,s=∼
O(1/l), using the same procedure as in Lemma G.3 we can complete the proof which gives
usm=˜Ω 
klp3δ3/2
(v(t)
s)2!
.
Lemma H.4. Let us define v(t):=rP
s∈[k](v(t)
s)2where v(t)
s= max {v(t)
1,s, v(t)
2,s}for all s∈[k];γ:= Ω
1
p√
δ
. Then, by
selecting learning rate η=˜O 
γ3(v(t))2l3
mk2!
and batch size B=˜Ω 
k2
γ6(v(t))4!
, at each iteration tof the Step-2 of
Algorithm 2 such that t=˜O 
σγ3(v(t))2l2
ηnk!
, w.h.p. we can ensure that,
∆L(θ(t), θ(t+1))≥ηmγ3
l2˜Ω
(v(t))2
Proof. As w.h.p.˜∂L(θ(t), x, y)
∂w(t)
r,s=∼
O(1/l), for a randomly sampled batch Btof size B, by selecting τ=σγ
4enBin
Lemma H.1 and using the same procedure as in Lemma G.4, we can show that for at least γ/2fraction of r∈[m/k]of
33

--- PAGE 34 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
expert s∈[k]:
∂L(θ(t))
∂w(t)
r,s=˜Ω 
v(t)
s
lp√
δ!
Now, for any (x′, y′)∼ D , from Lemma H.1 we know that for at least 1−2eτn
σfraction of r∈[m/k]of any expert
s∈[k], the loss function is ˜O(1/l)-Lipschitz smooth and also ˜O(1/l)-smooth.
Therefore, using same procedure as in Lemma G.4 we can complete the proof.
I. Lemmas Used to Prove the Theorem 4.3
For the single CNN model, as all the patches of an input (x, y)∼ D are sent to the model (i.e., there is no router), the
gradient of the single sample loss function w.r.t. hidden node r∈[m],
∂L(θ(t), x, y)
∂w(t)
r=−yarv(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(t)
r,x(j)⟩≥0
 (21)
the corresponding pseudo-gradient ,
∂L(θ(t), x, y)
∂w(t)
r=−yarv(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,x(j)⟩≥0

and the expected pseudo-gradient,
∼
∂ˆL(θ(t))
∂w(t)
r=ED
∼
∂L(θ(t), x, y)
∂w(t)
r

=−ar
2
ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,Pjx⟩≥0
y=−1


=−ar
2P(t)
r
where,
P(t)
r=ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n])x(j)1⟨w(0)
r,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,x(j)⟩≥0
y=−1

Lemma I.1. W.h.p. over the random initialization, for every (x, y)∼ D and for every τ > 0, for every iteration
t=˜O 
τ
η!
of the minibatch SGD, we have that for at least 
1−2eτn
σ!
fraction of r∈[m]:
∂L(θ(t), x, y)
∂w(t)
r=∼
∂L(θ(t), x, y)
∂w(t)
rand|⟨w(t)
r, x(j)⟩| ≥τ,∀j∈[n]
34

--- PAGE 35 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Proof. Using similar argument as in Lemma G.1 we can show that w.h.p.,∂L(θ(t),x,y)
∂w(t)
r=˜O(1)so as the mini-batch
gradient,∂L(θ(t))
∂w(t)
r=˜O(1).
Therefore,w(t)
r−w(0)
r=˜O(1).
Now, for every τ > 0,considering the set H:=n
r∈[m] :∀j∈[n],|⟨w(0)
r, x(j)⟩| ≥2τo
and following the
same procedure as in Lemma G.1 we can complete the proof.
Recall, v(t)
1:=ED|y=+1[v(t)(θ(t), x, y)|y= +1] andv(t)
2:=ED|y=−1[v(t)(θ(t), x, y)|y=−1].
Lemma I.2. For any possible fixed set {v(t)(θ(t), x, y) : ( x, y)∼ D} (that does not depend on w(0)
r) such that
v(t)=v(t)
1= max {v(t)
1, v(t)
2}, we have:
P"
||P(t)
r||=∼
Ω 
v(t)
np√
δ!#
= Ω 
1
p√
δ!
Proof. We know that,
P(t)
r=ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,x(j)⟩≥0
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]x(j)1⟨w(0)
r,x(j)⟩≥0
y=−1

Therefore,
h(w(0)
r) :=⟨Pr, w(0)
r⟩
=ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n]ReLU
⟨w(0)
r, x(j)⟩
y= +1

−ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]ReLU
⟨w(0)
r, x(j)⟩
y=−1

Now, decomposing w(0)
r=αo1+βwithβ⊥o1we get,
h(w(0)
r) =v(t)
nReLU (α)
+ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n]/jo1ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


−ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


=ϕ(α)−l(α)
35

--- PAGE 36 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
where,
ϕ(α) :=v(t)
nReLU (α)
+ED|y=+1
v(t)(θ(t), x, y)
1
nX
j∈[n]/jo1ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


and
l(α) :=ED|y=−1
v(t)(θ(t), x, y)
1
nX
j∈[n]ReLU
α⟨o1, x(j)⟩+⟨β, x(j)⟩


Now as ϕ(α)andl(α)both are convex functions, using the same procedure as in Lemma G.1 we can complete the proof.
Lemma I.3. Letv(t)= max {v(t)
1, v(t)
2}. Then, for every v(t)>0, form=˜Ω 
n2p3δ3/2
(v(t))2!
, for every possible set
{v(t)(θ(t), x, y)(w(t)
s, x) : (x, y)∼ D} (that depends on w(0)
r), there exist at least Ω
1
p√
δ
fraction of r∈[m]such that,
∼
∂ˆL(θ(t))
∂w(t)
r=˜Ω 
v(t)
np√
δ!
Proof. Similar as in the proof of Lemma G.3, by picking Ssamples from the distribution Dto form the set S={(xi, yi)}S
i=1
such that S/2many samples from y= +1 (denoting the sub-set by S+1) and S/2many samples from y=−1(denoting
the sub-set by S−1), we can show that w.h.p.,
v(t)
1−1
S/2X
(xi,yi)∈S+1v(t)(θ(t), xi, yi)=˜O 
1√
S!
and
v(t)
2−1
S/2X
(xi,yi)∈S−1v(t)(θ(t), xi, yi)=˜O 
1√
S!
This implies that, as long as S=˜Ω 
1
(v(t))2!
we have,
max

1
S/2X
(xi,yi)∈S+1v(t)(θ(t), xi, yi),1
S/2X
(xi,yi)∈S−1v(t)(θ(t), xi, yi)

∈"
1
2v(t),3
2v(t)#
Now using Lemma I.2 and following similar procedure as in Lemma G.3 we can complete the proof.
Lemma I.4. Withv(t)= max {v(t)
1, v(t)
2}andγ= Ω
1
p√
δ
, by selecting learning rate η=˜O 
γ3(v(t))2
mn2!
and batch-size
B=˜Ω 
n4
γ6(v(t))4!
, fort=˜O 
σγ3(v(t))2
ηn3!
iterations of SGD, w.h.p. we can ensure that,
∆L(θ(t), θ(t+1))≥ηmγ3
n2˜Ω
(v(t))2
Proof. Using Lemma I.1 and I.3 and following similar technique as in Lemma G.4, the proof can be completed.
36

--- PAGE 37 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
J. Auxiliary Lemmas
Lemma J.1. (Li & Liang, 2018) Let ψ:R→Randζ:R→Rare convex functions. Let {∂ψ(x)}and{∂ζ(x)}
are the sets of sub-gradient of ψandζatxrespectively such that ∂maxψ(x) =max{∂ψ(x)},∂maxζ(x) =max{∂ζ(x)},
∂minψ(x) =min{∂ψ(x)}and∂minζ(x) =min{∂ζ(x)}. Then for any τ≥0such that γ= (∂maxψ(τ/2)−∂minψ(−τ/2)−
(∂maxζ(τ/2)−∂minζ(−τ/2),
Pα∼U(−τ,τ)
|ψ(α)−ζ(α)| ≥τγ
512
≥1
64
Lemma J.2. (Li & Liang, 2018) Let for any i∈[m], the function hi:Rd→RisL-Lipschitz smooth and there exists
r∈[m]such that for all i∈[m−r]the function hiis also L-smooth. Furthermore, let us assume that the function
g:R→Ris both L-Lipschitz smooth and L-smooth. Let define f(w) :=gP
i∈[m]hi(wi)
where w∈Rdmsuch that
wi∈Rd. Then for every ξ∈Rdmsuch that ξi∈Rdwith||ξi|| ≤ρ, we have:
g
X
i∈[m]hi(wi+ξi)
−g
X
i∈[m]hi(wi)
≤X
i∈[m−r]*
∂f(w)
∂wi, ξi+
+L3m2ρ2+L2rρ
K. Proof of the Non-linear Separability of the Data-model
Lemma K.1. As long as l∗= Ω(1) , the distribution DisNOT linearly separable.
Proof. We will prove the Lemma by contradiction.
Now, if the distribution, Dis linearly separable, then there exists a hyperplane h=
h(1)T, h(2)T, ..., h(n)T
with
||h||= 1(here, h(j)represents the j-th patch of the hyperplane for j∈[n]) such that,
∀(x1, y= +1) ∼ D and(x2, y=−1)∼ D, xT
1h−xT
2h≥0 (22)
Now, as the class-discriminative patterns o1ando2can occur at any position j∈[n],||h(j)||2= Θ(1
n);∀j∈[n].
Now,∀j∈[n], we can decompose h(j)ash(j)=ajo1+bjo2.
Then,|aj|=|bj|= Θ 
1p
n(1−δd)!
,∀j∈[n]as||o1||=||o2||= 1.
Now,
xT
1h−xT
2h=⟨o1, h(jo1)⟩ − ⟨o2, h(jo2)⟩+X
j∈[n]/jo1⟨x(j)
1, h(j)⟩ −X
j∈[n]/jo2⟨x(j)
2, h(j)⟩
Now,
⟨o1, h(jo1)⟩ − ⟨o2, h(jo2)⟩= (ajo1−bjo2)−(ajo2−bjo1)δd
≤ |ajo1−bjo2| − |ajo1−bjo2|δd [WLOG, let assume δd<0]
=O r
1−δd
n!
Now,
X
j∈[n]/jo2⟨x(j)
2, h(j)⟩ −X
j∈[n]/jo1⟨x(j)
1, h(j)⟩=X
j∈[n]/jo2⟨x(j)
2, ajo1+bjo2⟩ −X
j∈[n]/jo1⟨x(j)
1, ajo1+bjo2⟩
=O 
l∗r
(1−δd)
n!
Therefore, for l∗= Ω(1) there is contradiction with (22).
37

--- PAGE 38 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
L. WRN and WRN-pMoE Architectures Implemented in the Experiments
Figure 17. The WRN architecture
implemented to learn CelebA
dataset
Figure 18. The WRN-pMoE architecture implemented to learn CelebA dataset
M. Extension to Multi-class Classification
Let us consider c-class classification problem where c >2. Then, we have (x, y)∼ D cwhere y∈ {1,2, ..., c}for the
multi-class distribution Dc.
The multi-class data model:
Now, according to the data model presented in section 4.2, we have {o1, o2, ..., o c}as class-discriminative pattern set.
∀j, j′∈[c]such that j̸=j′, we define δdj,j′:=⟨oj, oj′⟩. We further define δd:= max {δdj,j′}. Then,
δ=1
(1−max{δ2
dj,j′, δ2r}j,j′∈[c],j̸=j′)
38

--- PAGE 39 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
The multi-class pMoE model:
The pMoE model for multi-class case is given by,
∀i∈[c], fMi(θ, x) =kX
s=1m
kX
r=1ar,s,i
lX
j∈Js(ws,x)ReLU (⟨wr,s, x(j)⟩)Gj,s(ws, x) (23)
An illustration of (23) is given in Figure 19.
Figure 19. An illustration of the pMoE model in (23) with c= 4, k= 4, m= 8, n= 6andl= 2.
For mult-class case, we replace the logistic loss function by the softmax loss function (also known as cross-entropy loss).
For the training dataset {xj, yj}N
j=1, we minimize the following empirical risk minimization problem:
min
θ:L(θ) =1
NNX
j=1logPc
i=1exp (fMi(θ, xj))
exp (fMyj(θ, xj))(24)
M.1. The Multi-class Separate-training pMoE
Number of experts: For the multi-class separate-training pMoE, we take k=c, i.e. number of experts is equal to the
number of classes.
Training algorithm:
Input : Training data {(xi, yi)}N
i=1, learning rates ηrandη, number of iterations TrandT, batch-
sizes BrandB
Step-1 : Initialize w(0)
s, w(0)
r,s, ar,s,∀s∈ {1,2}, r∈[m/k]according to (7) and (8)
Step-2: (Pair-wise router training) We train the router, i.e. the gating-kernels w1, w2, ..., w cusing pair-wise training
describe below:
1.At first, we separate the training set of Nrsamples into cdisjoint subsets {Nr,1, Nr,2, ..., N r,c}according to the
class-labels.
2.Now, we prepare c/2pairs of training sets {(Nr,1, Nr,2),(Nr,3, Nr,4), ...,(Nr,c−1, Nr,c)}(here WLOG we assume
thatcis even).
3.Under each pair (Nr,i, Nr,i+1), we re-define the label as y= +1 andy=−1for the class iandi+ 1respectively and
train the gating-kernels wiandwi+1by minimizing (6) for Triterations
4.After the end of pair-wise training for all the pairs {(Nr,1, Nr,2),(Nr,3, Nr,4), ...,(Nr,c−1, Nr,c)}, we receive
w(Tr)
1, w(Tr)
2, ..., w(Tr)
cas the learned gating-kernels.
39

--- PAGE 40 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
Step-3: (Expert training)
Using the learned gating-kernels w(Tr)
1, w(Tr)
2, ..., w(Tr)
cin Step-2 and using the same procedure as in Step-3 of Algorithm 1
we train the experts.
The multi-class counterpart of the Lemma 4.1:
Now, using the same proof techniques as for Lemma 4.1 (i.e. following same procedures as in section D and E) we can show
that, we need Nr= Ω(c2n2
(1−δd)2)training samples to ensure,
argj∈[n](x(j)=oi)∈Ji(w(Tr)
i, x) ∀(x, y=i)∼ D cand∀i∈[c]
The multi-class counterpart of the Theorem 4.2:
We redefine the value-function for each class i∈[c]as,
v(t)
i,a(θ(t), x, y =a) :=

P
j̸=aefMj(θ(t),x)
cP
j=1efMj(θ(t),x);if,i=a
−efMi(θ(t),x)
cP
j=1efMj(θ(t),x);otherwise(25)
Now using similar techniques as in the proof of Theorem 4.2 (i.e. following same procedure as in the proof of Theorem
F.3 and section G) we can show that for every ϵ >0, we need number of hidden nodes m≥MS= Ω 
l10p12δ6c11
ϵ16
,
batch-size B= Ω 
l4p6δ3c6
ϵ8
forT=O 
l4p6δ3c6
ϵ8
iterations (i.e. NS= Ω(l8p12δ6c12/ϵ16)) to ensure,
P
(x,y)∼Dch
∀j∈[c], j̸=y, fMy(θ(T), x)> fMj(θ(T), x)i
≥1−ϵ
M.2. The Multi-class Joint-training pMoE
Training algorithm: Same as the Algorithm 2 except that for multi-class case the loss function is softmax instead of logistic
loss.
The multi-class counterpart of the Theorem 4.5:
Using the value-function define in (25) and as long as the Assumption 4.4 satisfied for all the classes i∈[c], following the
similar techniques as in the proof of Theorem 4.5 (i.e. following same procedure as in the proof of Theorem F.5 and section
H), we can show that for every ϵ >0, we need number of hidden nodes m≥MJ= Ω 
k3n2l6p12δ6c8
ϵ16
, batch-size
B= Ω 
k2l4p6δ3c4
ϵ8
forT=O 
k2l2p6δ3c4
ϵ8
iterations (i.e. NJ= Ω(k4l6p12δ6c8/ϵ16)) to ensure,
P
(x,y)∼Dch
∀j∈[c], j̸=y, fMy(θ(T), x)> fMj(θ(T), x)i
≥1−ϵ
N. Details of the Results in Table 1
Complexity in forward pass. The computational complexity of a non-overlapping convolution operation by a filter of
dimension don an input sample of npatches (of same dimension as the filter) is O(nd)(Vaswani et al., 2017). Therefore,
the complexity of forward pass of a batch of size Bthrough a convolution layer of mneurons is O(Bmnd ). Similarly,
the forward pass complexity of a the batch through the experts (of same total number of neurons as in the convolution
layer) of a pMoE layer is O(Bmld ). The operations in a pMoE router includes convolution (with complexity O(nd)),
softmax operation (with complexity O(1)) and TOP- loperation (with complexity O(nl)when l≪n). Therefore, the
overall forward pass complexity of a pMoE router with kexpert is O(Bknd ).
Complexity in backward pass. The gradient of neurons in convolution layer for an input sample is given in (21), which
implies that the complexity of the gradient calculation is O(nd)(addition of nvectors of dimension d) and hence the
40

--- PAGE 41 ---
Patch-level Routing in MoE is Provably Sample-efficient for CNN
backward pass complexity of CNN is O(Bmnd ). Similarly, the backward pass complexity of pMoE experts is O(Bmld ).
Now the gradient of gating kernels in pMoE router is given in (26), which implies that the complexity of the gradient
calculation is O(l2d)(addition of l2vectors of dimension d) and hence the backward pass complexity of pMoE router is
O(Bkl2d).
∂L(θ(t), x, y)
∂w(t)
s=−yv(t)(θ(t), x, y)
X
r∈[m]ar,s
1
lX
j∈Js(x)ReLU (⟨w(t)
r,s, x(j)⟩)Gj,s(X
i∈Js(x)/j(x(j)−x(i))Gi,s)


(26)
Complexity to achieve ϵgeneralization error. From Theorem 4.5, to achieve ϵgeneralization error we need O(k2l2/ϵ8)iter-
ations of training in pMoE, which implies that the computational complexity to achieve ϵerror in pMoE is O(Bmk2l3d/ϵ8).
Similarly, using the results from Theorem 4.3, the corresponding complexity in CNN is O(Bmn5d/ϵ8).
41
