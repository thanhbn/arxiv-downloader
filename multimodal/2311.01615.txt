# 2311.01615.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.01615.pdf
# File size: 917105 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FLAP: FAST LANGUAGE-AUDIO PRE-TRAINING
Ching-Feng Yeh, Po-Yao Huang, Vasu Sharma, Shang-Wen Li and Gargi Gosh
{cfyeh,berniehuang,vasusharma,shangwel,gghosh }@meta.com
FAIR, Meta
ABSTRACT
We propose Fast Language-Audio Pre-training (FLAP), a
self-supervised approach that efficiently and effectively learns
aligned audio and language representations through masking,
contrastive learning and reconstruction. For efficiency, FLAP
randomly drops audio spectrogram tokens, focusing solely on
the remaining ones for self-supervision. Through inter-modal
contrastive learning, FLAP learns to align paired audio and
text representations in a shared latent space. Notably, FLAP
leverages multiple augmented views via masking for inter-
modal contrast and learns to reconstruct the masked portion
of audio tokens. Moreover, FLAP leverages large language
models (LLMs) to augment the text inputs, contributing to
improved performance. These approaches lead to more robust
and informative audio-text representations, enabling FLAP
to achieve state-of-the-art (SoTA) performance on audio-text
retrieval tasks on AudioCaps (achieving 53.0% R@1) and
Clotho (achieving 25.5% R@1).
Index Terms —Contrastive learning, audio-text retrieval
1. INTRODUCTION
Representation learning [1] has garnered significant mo-
mentum on creating information-rich embeddings for down-
stream tasks. Recently, self-supervised representation learn-
ing (SSL) [2, 3] has emerged as a prominent research area
in hope of reducing human annotations. Traditionally, SSL
approaches have been developed under the single-modality
setup for image [4, 5], text [6, 7], or audio/speech [8, 9, 10]
independently. However, there is a growing interest in repre-
sentation learning across multiple modalities [11, 12, 13, 14],
which brings both challenges and exciting new possibili-
ties. One breakthrough is Contrastive Language-Image Pre-
training (CLIP) [11] which projects text and image embed-
dings into a shared latent space, enabling applications like
cross-modality retrieval and automatic captioning. More
recently, Contrastive Language-Audio Pre-training (CLAP)
[15, 16] learns representations for both text and audio and
delivered strong performance on audio-text retrieval tasks.
The key ingredients in CLIP and CLAP are their SSL ob-
jectives and model architectures. On objective, both CLIP
and CLAP utilize contrastive learning, which aims to mini-
mize the distance between embeddings in different modalitiesof the same instance, while differentiating the embeddings
from different instances [17, 18, 19]. On model architec-
ture, both CLIP and CLAP adopted Transformer-like models
[20], which have proven to be effective. Previous studies sug-
gest this transformer +contrastive learning combination pro-
duces high-quality embeddings for both uni-modal [4, 18, 21]
and multi-modal [14, 22, 23] tasks. One major limitation of
Transformer-like models is their quadratic complexity with
respect to sequence lengths, which becomes a computational
bottleneck and restricts overall efficiency.
To improve computational efficiency, techniques with
Masked AutoEncoders (MAE) such as image MAE [5],
VideoMAE [24, 25] and AudioMAE [8] were recently pro-
posed and achieved significant efficiency wins with minor
performance trade-off. Recently, Fast Language-Image Pre-
training (FLIP) [26] applied similar techniques to image-text
SSL. Recognizing that audio signals in nature are continuous
and with variable in lengths, we explored the masking strate-
gies for self-supervised language-audio representation learn-
ing. We term our model Fast Language-Audio Pre-training
(FLAP). FLAP endeavors to establish aligned audio and lan-
guage representations by incorporating masking, contrastive
learning and reconstruction techniques. For language-audio
datasets, very often the audio signals contain much richer
information than the text counterparts. For example, an audio
segment of dog barking may reveal additional information
such as volume and frequency, which are often missing in
the text. Also, text descriptions can vary in writing styles
and generate inconsistent embeddings for the same seman-
tics. Given such imbalanced information richness between
audio and text, we utilize large language models (LLMs)
[27, 28, 29] to enrich and unify the writing style for texts in
the language-audio task.
Previous works [16, 30, 31, 32] on language-audio pre-
training received wide research interests. Recently, large-
scale CLAP (LS-CLAP) [16] demonstrated strong results on
audio-text retrieval on AudioCaps andClotho benchmarks. In
this study, we further improve the LS-CLAP results by 1) us-
ing Masked Audio-Video Learners (MA ViL) as pre-trained
audio encoder 2) efficient masking for efficiency and robust-
ness 3) adding audio reconstruction for better embedding 4)
utilizing LLMs for text augmentation. We observed signifi-
cant performance boosts from FLAP, which outperformed the
recently proposed state-of-the-art systems [16].arXiv:2311.01615v1  [cs.SD]  2 Nov 2023

--- PAGE 2 ---
Fig. 1 . The architecture of FLAP, including audio/text encoders, efficient masking and audio reconstruction.
2. CONTRASTIVE LEARNING
The fundamental framework of contrastive learning involves
selecting an “anchor” data sample, a data point from the same
distribution referred to as the “positive” sample, and a data
point from a different distribution known as the “negative”
sample. Contrastive learning aims to reduce the distance be-
tween the anchor and positive samples, which are part of the
same distribution, in the latent space. Simultaneously, it seeks
to maximize the distance between the anchor and the negative
samples. For learning aligned audio and text representations,
the “positive” examples refers to the representations of paired
audio and text samples (i.e., an audio and its corresponding
captions), while the negative examples are all the combina-
tions of the unpaired audios and captions sampled in a batch.
In this work we employ the InfoNCE [33] loss for inter-modal
contrastive learning over audio and text pairs sampled from
a dataset (a,t)∈ D . Let aandtrespectively denote the
instance-level audio and text representations. The InfoNCE
lossLc(a,t)is defined as:
Lc(a,t) =−1
BBX
i=1logexp( S(ai,ti)/τ)PB
j=1exp( S(ai,tj)/τ)), (1)
where S (ai,tj) =aT
itj
∥ai∥∥tj∥is the cosine similarity between
ai,tjandτis the softmax temperature. In Eq 1, the loss func-
tion encourages the distance between the embeddings from
audio and text from the same sample to be minimized and
to be maximized from different samples, therefore achieving
the desired ”contrasting” effects. It is worth noting that the
performance of contrastive learning depends highly on the
number of samples ( B) being contrasted against each other
within the same batch. Larger batch sizes ( B) offers more
inter-sample connections to stabilize the aggregated gradients
for updating model parameters, with increased need of com-
putation and memory consumption.
3. FLAP: EFFICIENT MASKING
Inspired by the recent success of FLIP[26] which attempts to
employ the masking technique for learning image-text rep-resentations, we propose Fast Language-Audio Pre-training
(FLAP) for learning self-supervised audio-language represen-
tations by employing masking for both contrastive learning
and reconstruction. As depicted in Fig. 1, FLAP consists of
an audio encoder, a text encoder, and audio decoder. For
FLAP’s audio encoder, we adopt the audio backbone from
MA ViL [22], the SoTA audio model pre-trained on the audio
and video clips of AudioSet [34]. MA ViL is a self-supervised
framework for learning audio-video representations that com-
prises two stages. In the first stage, MA ViL simultaneously
learns to reconstruct spectrogram and pixels, leveraging the
complementary information from both modalities. In the sec-
ond stage, MA ViL performs self-distillation where a student
model predicts the contextualized features generated by the
first-stage teacher model.
FLAP performs instance-wise inter-modal contrastive
learning using Eq. 1 over the non-masked (visible) portion
of audio spectrogram tokens. The masking strategy in FLAP
significantly enhances the computation efficiency and pro-
motes more robust representation as masking can also be
viewed as a data augmentation approach over the audio to-
kens. Specifically, given a input tensor of shape (B, N, D ),
where Bis the batch size, Nis the sequence length, Dis
the embedding dimension, masking reduces the shape to
(B, N′, D), where N′is smaller than N. This enables sig-
nificant computation reduction for Transformer-like models
as the model complexity grows quadratically with sequence
length (i.e. O(N2)).
We investigated two masking strategies, namely 1-D and
2-D masking, as illustrated in Fig. 2. Before masking, the in-
put (in the form of mel-spectrogram) is transformed into patch
embeddings. For 1-D masking, the input tensor of shape
(B, N, D )is first augmented with positional embeddings and
then randomly sampled on the T-axis to become (B, N′, D).
The random sampling is performed on a shuffled and per-
frame basis to the desired length N′. 1-D masking is simple
and effective in boosting robustness by random frame drop-
ping and reducing computation along with sequence lengths
N. On the other hand, 2-D masking aims to build a more
structured sampling strategy on top of 1-D masking. Instead
of directly sampling on the N-axis, 2-D masking first splits

--- PAGE 3 ---
Fig. 2 . Frame dropping by 1-D and 2-D Masking.
theN-axis into Mgroups, each having K=N/M consec-
utive frames. Next, both the Mgroups and the Kframes in
each group are sampled individually in the same fashion as in
1-D masking and reduced to M′andK′respectively. Finally,
bothM′andK′are merged back together and becomes the
newN′=M′∗K′. 2-D masking essentially splits the overall
sequence ( N) into numerous ( M) fine-grained segments ( K),
therefore enables more structured sampling through both ho-
mogeneous sampling and dropping in each fine-grained seg-
ments. Both 2-D and 1-D maskings can achieve similar ef-
ficiency improvement with different masking ratios. For ex-
ample, a 75% masking ratio on Nleads to 25% (= 100% -
75%) computation cost for 1-D masking, while 50% on M
and 50% on Kfor 2-D masking also leads to 25% (= 50%
* 50%). The masked tensors are then directly sent to the au-
dio encoder for computing the output embeddings for each
frame and then averaged across the N-axis for per-instance
embeddings. These masking strategies are particularly use-
ful for contrastive learning tasks as the per-example outputs
are more robust to frame dropping. In addition, reduced se-
quence length by masking also enables larger batch sizes to
fit in GPUs, which benefits contrastive learning as more pairs
are involved in the loss function for a single batch. Further-
more, the masking strategy can be view as a type of audio
augmentation (e.g. SpecAug [35]) that promotes robustness
of the learned representations. The masking is applied during
the training stage and is disabled during evaluation.
4. AUDIO RECONSTRUCTION
To bolster the robustness of the learned audio embeddings,
we further propose an additional objective that promotes the
incorporation of audio information into the embeddings. This
can be achieved by tasking the model with reconstructing
the original audio spectrogram tokens using the per-sample
embeddings. As depicted in Fig. 1, before being aggre-
gated across sequence length to produce the per-sample
audio embeddings, the per-frame embeddings (of shape
(B, T′, D)) is sent to an audio decoder for reconstructingthe mel-spectrogram. Empirically, we observe that recon-
structing only the spectrogram but notthe text tokens yields
better performance. We employ vanilla Transformer blocks
as the audio f−1
a(.)decoders. The encoder’s outputs ( ammare
firstly projected and padded with trainable [MASK] tokens.
After restoring the original order (time-frequency for audio
and space-time for video tokens), we add the decoders’ (fixed
2-D sinusoidal) positional embeddings and input the restored
sequences into the decoders. At the top of the decoders, we
incorporate linear heads to reconstruct the raw inputs. Specif-
ically, the decoder outputs for spectrogram reconstruction
are denoted as ˆ a=f−1
a(gav(fa(a′))). For notation clarity,
we omit the [MASK] tokens and linear projection head. Let
ˆai,araw
i∈RHa
raw;i= 1. . . n denote the audio decoder’s
output and the ground truth reference of the i-th masked
spectrogram patch. In masked audio reconstruction, FLAP is
self-supervised by minimizing the mean squared error (MSE)
lossLraw
rdefined as:
Lraw
r=1
nnX
i=1(ˆai−araw
i)2(2)
The MSE loss from reconstruction is then weighted and
added to the final loss along with the contrastive loss. With
reconstruction, the model is encouraged to preserve con-
densed information into per-sample embeddings, as these
embeddings not only have to be close to their text domain
counterparts, but also useful in producing original inputs. It
is worth noting that reconstruction does come with a trade-off
on efficiency and batch size, as the audio decoder requires
non-trivial computation and memory usage.
5. ENRICHED AUGMENTATION BY LLM
Learning audio-text representations faces an additional chal-
lenge stemming from the scarcity of audio-text pairs in exist-
ing audio-text corpora. Collecting human annotations for au-
dio is both expensive and non-scalable. To address this issue,
we present a novel approach that harnesses the power of large
language models (LLMs) and audio event detection models
(AEDs) to augment the limited number of text descriptions
available for audio. Table 1 shows examples of the original
text descriptions from training data and the class list to cap-
tion transformation. From the original text descriptions, it is
clear that both the richness of information is behind the corre-
sponding audio signals and the writing styles are inconsistent
across samples. To reinterpret and enrich the same semantic
for natural language, we leverage the power of LLMs [27, 36,
29] to enhance the descriptiveness of the audio captions on
audio-text datasets such as AudioCaps andClotho , which only
contains weak and limited descriptive captions. We first em-
ploy off-the-shelf AED model (i.e., MA ViL [22]) to detect the
audio events within a sample. And then we exploits a LLM
(i.e., Vicuna [36]) along with engineered prompts to combine

--- PAGE 4 ---
Original LLM-only AED+LLM
Wind blows as waves crash
against a shoreline.The wind gusts while the waves
crash against the shorelineThe waves crash against the shoreline, with the sound of wind blowing and creating
wind noise. The ocean is in motion, with the wind blowing strongly
A man is speaking. A person is delivering a speech. A man’s speech can be heard rustling the leaves in the wind.
A loud siren whizzes past. A piercing siren blares by. An ambulance (siren) with a loud siren sound whizzes pasts by.
An engine revving. A car’s engine revving. An accelerating car engine revving with a vroom sound.
Water sound. The sound of water. An intermittent sound of water flowing from a tap or faucet.
The sound of a boat. A boat is making sound. A water vehicle, specifically a motorboat or speedboat, is moving at a rapid pace
with winds blowing.
Table 1 . Comparison between original captions and augmented captions generated by LLM and AED.
the classification outputs and the original caption to generate
richer captions for samples in AudioCaps andClotho . Vicuna
is an open-source instruction-following model fine-tuned on
the Llama-7b model [37]. From the examples in Table 1, uti-
lizing this model generates more grammatical captions that
remain faithful to the audio events.
To enrich text captions with LLM and detected audio
events, we used the following prompt: “Describe a situation
with AED results sounds and combine it with the origi-
nalcaption together.” A limitation of the Vicuna model is
its tendency to add unnecessary details or ignore relevant la-
bels when generating captions. By adding AED outputs and
original captions into the prompt, we leveraged the in-context
learning ability of Vicuna to enrich captions. During training,
the same set of audio signals with text descriptions replaced
with generated captions are augmented to the datasets.
6. EXPERIMENTS
6.1. Datasets and Setup
Across all experiments, similar to LS-CLAP [16], we use Au-
dioCaps, Clotho, and 5 other datasets (Freesound, Epidemic
Sound, BBC Sound Effects, Free To Use Sounds, Sonniss
Game effects) for training, while AudioCaps [38] and Clotho
[39] are used for evaluation. It is worth noting that compared
to LS-CLAP, we drop AudioStock due to its unavailability
and therefore the size of the dataset for training is smaller
than LS-CLAP. The evaluation sets are identical for fair com-
parisons. We built experiments on top of the LS-CLAP [16]
toolkit and adopted fvcore [40] for efficiency analysis. Cross-
modality retrieval between audio and text is used for evalu-
ation of the quality of the embeddings. For text-audio (T-A)
retrieval, given the text as query, the audio recordings in the
evaluation set are ranked based on the cosine similarities be-
tween text and audio embeddings. The same procedure ap-
plies to audio-text (A-T) retrieval. Recalls at top 1, 5 and 10
(R@1, R@5 and R@10) are reported as metrics for both tasks
onAudioCaps andClotho datasets.
For experiments without feature fusion, depending on the
audio length, we either randomly chunk 10 seconds from
longer audios or pad to 10 seconds for shorter ones to form
input data of uniform lengths. For feature extraction, 25ms
window size and 10ms window shift were used to extractmel-spectrogram features with 128 mel-bins. For experi-
ments with feature fusion enabled, we followed the same
procedure as LS-CLAP [16], where audios are either padded
or strided to create global and local versions followed by
2-D convolutions for merging. For SpecAug [35], up to
192 audio frames (e.g. 1.92 seconds) and up to 48 mel-
bins are randomly replaced with zeros for each sample. For
text embedding generation, the texts paired with audio data
are tokenized with a capped length of 77. RoBERTa [7] is
used as text encoder for all experiments to be consistent with
LS-CLAP [16].
The Adam [41] optimizer with β1= 0.99,β2= 0.9was
used during model training. The learning rate starts with a
warm-up stage, peaks at 10−4and was decayed on a cosine
schedule until the target number of epochs (45) is reached.
Since both masking or reconstruction affects GPU mem-
ory usage which translates to largest batch size allowed per
GPU, we report results with similar batch sizes to the base-
line (2304) and also results with larger batch sizes enabled
by efficient masking but using the equivalent computational
resources (i.e. the same number of GPUs).
6.2. Results on Efficient Masking and Reconstruction
To evaluate the performance of efficient masking and recon-
struction, the experimental results are summarized in Table
2, in which all results are without feature fusion. The re-
sults from LS-CLAP [16] are listed in row 1 serving as the
baseline. In row 2, the audio encoder is replaced with the re-
cent MA ViL [22] model with audio-and-video self-supervised
pre-training that achieves state-of-the-art performance on au-
dio classification tasks. Note that we simply train MA ViL
with the contrastive loss Eq. 1 on audio-text datasets without
masking or reconstruction applied. The results validate that
stronger audio-modal yields improved audio-text representa-
tions in audio-text retrieval tasks. In rows 3 and 5, 1-D and
2-D masking are applied with masking ratio selected from
additional ablation studies, 0.4 for 1-D and 0.2/0.2 for 2-D
respectively. For 2-D masking, we split the sequence into 64
(e.g. N= 64) groups of 8 (e.g. K= 8) frames from patch
embeddings of length 256. From the comparison, we ob-
served similar sequence length reduction from 1-D (1 - 0.4
= 60%) and 2-D ((1 - 0.2) ×(1 - 0.2) = 64%). But 2-D
masking delivers better improvement due to more structured

--- PAGE 5 ---
Model Global Batch Size MaskingAudioCaps Eval. Clotho Eval.
T-A Retrieval A-T Retrieval T-A Retrieval A-T Retrieval
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
(1) LS-CLAP[16] 2304 – 32.7 68.0 81.2 43.9 77.7 87.6 15.6 38.6 52.3 23.7 48.9 59.9
(2) FLAP 2304 (36 x 64 GPUs) – 34.8 70.0 82.7 49.0 79.5 88.7 16.3 41.4 53.9 23.0 49.2 61.4
(3) FLAP 2304 (36 x 64 GPUs) 1-D: 0.4 36.0 70.5 83.0 49.0 78.9 89.2 16.8 40.7 53.4 23.9 48.9 61.2
(4) FLAP (+recon) 2304 (36 x 64 GPUs) 1-D: 0.4 36.7 71.2 83.3 47.2 81.9 90.0 15.6 39.5 51.9 21.7 50.6 61.3
(5) FLAP 2304 (36 x 64 GPUs) 2-D: 0.2/0.2 37.5 73.5 84.6 49.6 82.3 89.4 17.2 41.1 52.8 23.7 48.7 62.3
(6) FLAP (+recon) 2304 (36 x 64 GPUs) 2-D: 0.2/0.2 37.2 73.0 84.9 50.3 81.4 90.0 17.0 41.2 53.5 22.4 49.0 62.7
(7) FLAP 4608 (72 x 64 GPUs) 2-D: 0.2/0.2 38.3 73.6 85.1 50.6 83.1 91.2 16.7 41.5 54.2 23.0 48.6 62.9
Table 2 . Experimental results on masking type, masking ratio and audio reconstruction (without feature fusion).
masking strategy. Both 1-D and 2-D masking reduce mem-
ory usage and preserve room for additional operations. On
top of masking, audio reconstruction is applied with 4 layers
of Transformer decoding layers with 4 heads and 512 em-
bedding dimensions for each layer. The results with audio
reconstruction are listed in rows 4 and 6. The reconstruction
objective encourages FLAP to capture more abstract concepts
from audio context to represent and predict raw audio spec-
trograms, without replying on additional class labels. This
results in stronger audio-text retrieval performance on Audio-
Caps . Alternatively, the memory saving from masking can be
also utilized to process more samples in a single batch instead
of audio reconstruction. Doubling the batch size produces the
results in row 7. Compared with rows 5 and 6, increasing
the batch size improves the robustness of the contrastive ob-
jective. In Eq.1, the positive pairs are encouraged to contrast
against a larger collection of negative samples in the denom-
inator, resulting in more well-aligned audio-text latent space
where semantically correlated audio-text pairs are closer to
each other and uncorrelated ones are distant. For contrastive
learning, sufficiently large batch size is crucial to the model
performance. It is worth noting that the number of GPUs are
kept the same across the comparisons and larger batch size is
achieved through efficient masking, which not only improves
the robustness of the model but also reduces computation and
memory footprints.
6.3. Efficiency Analysis of 1-D/2-D Masking
Masking provides benefits including bringing down the se-
quence length for efficiency and improvement on model ro-
bustness. However, similar to many efficiency-focused ap-
proaches, the typical efficiency/performance tradeoff also ap-
plies here. To analyze the correlation between masking ra-
tios and the impact on model performance, models with dif-
ferent masking strategies and incremental masking ratios are
trained and compared in operational curves in Fig. 3, with
AudioCaps results on top and Clotho results at bottom. In the
operational curves, the computation complexity (in terms of
GFLOPs) serves as the horizontal axis while the top 1 recall in
retrieval (in terms of R@1) serves as the vertical axis. We also
annotate each data point with (masking ratio, R@1) for eas-
ier numerical comparison. The GFLOPs are calculated using
Fig. 3 . Text-Audio R@1 vs. GFLOPs on AudioCaps and
Clotho with Different Ratios for 1-D and 2-D Masking.
the fvcore [40] tool for the audio encoder only with a batch
of 8 samples of 10-second lengths. The batch size was kept
the same for all masking ratios for fair comparison. The base-
line results with no masking are also included at the rightmost
positions in Fig. 3.
For each dataset, 1-D and 2-D masking are compared with
incremental masking ratios. The masking ratios are on a per-
dimension basis, meaning for the same masking ratio, 2-D
masking presents more aggressive frame dropping. For exam-
ple, when masking ratio is 0.3, 1-D masking preserves 70% of
the sequence while 2-D masking only preserves 49% (= 0.7
* 0.7). From the curves in Fig. 3, efficient masking started
improving model robustness until too many frames in the se-
quence are dropped (around 0.5). This is expected as infor-
mation loss is increased along with the masking ratio. In addi-
tion, similar to the observation in Table 2, 2-D masking pro-
vides better recalls around similar GFLOPs therefore offers
better trade-off than 1-D masking for being more structured.
Taking masking ration = 0.2 for example, 2-D masking ap-
proximately saves 25% of the computation and delivers better
recalls than result without masking. This shows that the ef-
ficient masking is effective in both improving efficiency and
model robustness.

--- PAGE 6 ---
ModelFeature
FusionBatch
SizeAudioCaps Eval. Clotho Eval.
T-A Retrieval A-T Retrieval T-A Retrieval A-T Retrieval
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
(1) LS-CLAP[16]
No2304 32.7 68.0 81.2 43.9 77.7 87.6 15.6 38.6 52.3 23.7 48.9 59.9
(2) FLAP 2304 37.5 73.5 84.6 49.6 82.3 89.4 17.2 41.1 52.8 23.7 48.7 62.3
(3) FLAP (+recon) 2304 37.2 73.0 84.9 50.3 81.4 90.0 17.0 41.2 53.5 22.4 49.0 62.7
(4) FLAP 4608 38.3 73.6 85.1 50.6 83.1 91.2 16.7 41.5 54.2 23.0 48.6 62.9
(5) FLAP (+LLM-aug) 4608 40.4 74.7 85.0 51.5 82.5 92.5 17.4 41.3 53.7 21.6 51.2 63.1
(6) LS-CLAP[16]
Yes2304 36.2 70.3 82.5 45.0 76.7 88.0 17.2 42.9 55.4 24.2 51.1 66.9
(7) FLAP 2304 38.6 74.2 85.6 49.6 83.8 91.1 17.3 43.1 55.7 24.4 53.2 66.4
(8) FLAP (+recon) 2304 40.1 74.8 86.0 50.8 81.9 91.0 17.8 44.0 56.3 24.6 53.0 66.7
(9) FLAP 4608 39.9 75.4 86.6 50.6 81.7 91.9 17.5 43.4 56.0 24.4 52.1 67.1
(10) FLAP (+LLM-aug) 4608 41.5 75.5 86.0 53.0 84.1 92.6 20.3 46.5 58.8 25.5 53.4 67.9
Table 3 . Experimental Results on Feature Fusion and Text Augmentation with Large Language Models (LLM).*NOTE: FLAP
uses the same dataset as LS-CLAP [16], excluding AudioStock due to its unavailability.
6.4. Results on Feature Fusion and LLM Augmentation
The setup without feature fusion in LS-CLAP adds padding
for shorter audio signals and applies random cropping for
longer ones to generate inputs to the model of uniform lengths
of 10 seconds. It works well for feeding long audio signals to
the audio encoder without increasing the computational com-
plexity. However, random cropping also implies information
loss. Therefore, feature fusion [16] was introduced to further
enhance the final retrieval performance and achieved signifi-
cant improvements. To evaluate FLAP on the same setup, we
adopted the same feature fusion and the corresponding results
are listed in Table 3. In Table 3, results without feature fusion
are listed in rows 1 to 5 and results with feature fusion are in
rows 6 to 10. Rows 1 to 5 share same setups in Table 2, where
row 1 is the same CLAP baseline, row 2 is the 2-D masked
MA ViL with ratio 0.2, row 3 incorporates reconstruction loss
on top of row 2, row 4 doubles the batch size compared with
row 2 and row 5 augments LLM-generated text descriptions
on top of row 4. Rows 6 to 10 repeats the same setups as rows
1 to 5 except inputs with feature fusion were used.
Compared with rows 1 to 5, rows 6 to 10 are effectively
improved with feature fusion, as feature fusion combines
global and cropped segments as inputs to the model. This
benefits more for long audio signals, as observed from the
larger improvements on Clotho , which contains more audio
segments longer than 10 seconds. Comparing rows 7-10 to
row 6, FLAP delivers similar performance improvement for
feature fusion setups similarly to rows 2-5. This demonstrates
that FLAP is highly versatile and adds complementary gains
on top of the already competitive feature fusion results. In
rows 5 and 10, LLM augmentation mentioned in section 5
is also applied on top of the best models to demonstrate the
impact from enriched and more consistent text descriptions.
Compared with rows 4 and 9, results with augmentation from
LLM-generated text descriptions show either similar or better
performance. Particularly, the results on Clotho with feature
fusion showed larger improvement. Since the enriched text
description tends to be longer as observed from examples inTable 1, feature fusion setups potentially benefit more for bet-
ter audio-text match and alignment. Rows 5 and 10 also serve
as the best results with and without feature fusion for the pro-
posed FLAP framework. Compared with CLAP, combining
efficient masking which leads to increased batch sizes along
with enriched text description by LLMs yields significant
improvements across both text-audio and audio-text retrieval
tasks on both datasets. For top 1 recall (R@1), FLAP in row
5 without feature fusion performs better on majority of tasks
than the previous best results with feature fusion in row 6
(36.2 to 40.4 for text-audio and 45.0 to 51.5 for audio-text on
AudioCaps , 17.2 to 17.4 for text-audio on Clotho, with excep-
tion on 24.2 to 21.6 for audio-text on Clotho ). On the same
feature fusion setup, FLAP in row 10 further outperforms
the previous best results in row 6 on all tasks (36.2 to 41.5
for text-audio and 45.0 to 53.0 for audio-text on AudioCaps ,
17.2 to 20.3 for text-audio and 24.2 to 25.5 for audio-text on
Clotho ). To the best of our knowledge, these results also serve
as the current best performances on audio-text and text-audio
retrieval tasks for AudioCaps andClotho .
7. CONCLUSION
In this paper, we introduce Fast Language-Audio Pre-training
(FLAP) where contrastive learning meet masking. FLAP
leads to better audio understanding, task performance, and
enables efficient and effective learning on sequence modali-
ties such as audio and video. In addition, audio reconstruction
and enriched text description augmentation by large language
models (LLMs) are also investigated. Efficient masking re-
duces both computation and memory footprint for training
samples, therefore enables larger batch sizes for contrastive
learning. Text augmentation from LLMs further enriches the
text descriptions for audio signals and produces more consis-
tent writing styles. Combining both, FLAP delivers strong
performance on audio-text retrieval tasks with evaluation
onAudioCaps andClotho benchmarks. The techniques in
FLAP are versatile and applicable to representation learning
in sequence modalities such as text, audio and video.

--- PAGE 7 ---
8. REFERENCES
[1] Yoshua Bengio, Aaron C. Courville, and Pascal Vincent,
“Unsupervised feature learning and deep learning: A re-
view and new perspectives,” CoRR , vol. abs/1206.5538,
2012.
[2] Linus Ericsson, Henry Gouk, Chen Change Loy, and
Timothy M. Hospedales, “Self-supervised representa-
tion learning: Introduction, advances and challenges,”
CoRR , vol. abs/2110.09327, 2021.
[3] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin
Riedmiller, and Thomas Brox, “Discriminative unsu-
pervised feature learning with convolutional neural net-
works,” in Advances in Neural Information Process-
ing Systems , Z. Ghahramani, M. Welling, C. Cortes,
N. Lawrence, and K.Q. Weinberger, Eds. 2014, vol. 27,
Curran Associates, Inc.
[4] Xinlei Chen, Saining Xie, and Kaiming He, “An empir-
ical study of training self-supervised vision transform-
ers,” in ICCV , 2021.
[5] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-
otr Doll ´ar, and Ross Girshick, “Masked autoencoders
are scalable vision learners,” in CVPR , 2022.
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova, “BERT: pre-training of deep bidi-
rectional transformers for language understanding,” in
Proc. NAACL-HLT , 2019.
[7] Yinhan Liu, Myle Ott, and Naman Goyal et al.,
“RoBERTa: A robustly optimized BERT pretraining ap-
proach,” CoRR , vol. abs/1907.11692, 2019.
[8] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski,
Michael Auli, Wojciech Galuba, Florian Metze, and
Christoph Feichtenhofer, “Masked autoencoders that
listen,” in Advances in Neural Information Processing
Systems , 2022.
[9] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-
man Mohamed, “HuBERT: self-supervised speech rep-
resentation learning by masked prediction of hidden
units,” IEEE/ACM Transactions on Audio, Speech, and
Language Processing , vol. 29, pp. 3451–3460, 2021.
[10] Ashish Jaiswal, Ashwin Ramesh Babu, Moham-
mad Zaki Zadeh, Debapriya Banerjee, and Fillia Make-
don, “A survey on contrastive self-supervised learning,”
Technologies , vol. 9, no. 1, pp. 2, 2020.
[11] Alec Radford, Jong Wook Kim, and Chris Hallacy et al.,
“Learning transferable visual models from natural lan-
guage supervision,” CoRR , vol. abs/2103.00020, 2021.[12] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer,
and Christoph Feichtenhofer, “VideoCLIP: Contrastive
pre-training for zero-shot video-text understanding,” in
Proceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , 2021, pp. 6787–
6800.
[13] Po-Yao Huang, Mandela Patrick, Junjie Hu, Gra-
ham Neubig, Florian Metze, and Alexander Haupt-
mann, “Multilingual multimodal pre-training for zero-
shot cross-lingual transfer of vision-language models,”
inProceedings of the 2021 Conference of the North
American Chapter of the Association for Computational
Linguistics: Human Language Technologies , 2021, pp.
2443–2459.
[14] Mandela Patrick, Po-Yao Huang, Ishan Misra, Florian
Metze, Andrea Vedaldi, Yuki M. Asano, and Jo ˜ao F.
Henriques, “Space-time crop & attend: Improving
cross-modal video representation learning,” in 2021
IEEE/CVF International Conference on Computer Vi-
sion, ICCV 2021, Montreal, QC, Canada, October 10-
17, 2021 . 2021, pp. 10540–10552, IEEE.
[15] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-
mail, and Huaming Wang, “CLAP: learning audio con-
cepts from natural language supervision,” in ICASSP
2023-2023 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP) . IEEE,
2023, pp. 1–5.
[16] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-
lor Berg-Kirkpatrick, and Shlomo Dubnov, “Large-scale
contrastive language-audio pretraining with feature fu-
sion and keyword-to-caption augmentation,” in IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing, ICASSP , 2023.
[17] R. Hadsell, S. Chopra, and Y . LeCun, “Dimensionality
reduction by learning an invariant mapping,” in 2006
IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR’06) , 2006, vol. 2, pp.
1735–1742.
[18] Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoffrey E. Hinton, “A simple framework for con-
trastive learning of visual representations,” CoRR , vol.
abs/2002.05709, 2020.
[19] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross Girshick, “Momentum contrast for unsupervised
visual representation learning,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern
recognition , 2020, pp. 9729–9738.
[20] Ashish Vaswani, Noam Shazeer, and Niki Parmar et al.,
“Attention is all you need,” in Proc. NeurIPS , 2017.

--- PAGE 8 ---
[21] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli, “wav2vec 2.0: A framework for self-
supervised learning of speech representations,” in Ad-
vances in Neural Information Processing Systems , 2020.
[22] Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali,
Haoqi Fan, Yanghao Li, Shang-Wen Li, Gargi Ghosh,
Jitendra Malik, and Christoph Feichtenhofer, “MA ViL:
masked audio-video learners,” in Advances in Neural
Information Processing Systems , 2023.
[23] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and
Juan Pablo Bello, “Wav2CLIP: learning robust audio
representations from CLIP,” in Proc. ICASSP , 2022.
[24] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang,
“VideoMAE: masked autoencoders are data-efficient
learners for self-supervised video pre-training,” Ad-
vances in neural information processing systems , vol.
35, pp. 10078–10093, 2022.
[25] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and
Kaiming He, “Masked autoencoders as spatiotemporal
learners,” in NeurIPS , 2022.
[26] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Fe-
ichtenhofer, and Kaiming He, “Scaling language-
image pre-training via masking,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 2023, pp. 23390–23400.
[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,
Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal
Azhar, et al., “LLaMA: open and efficient foundation
language models,” arXiv preprint arXiv:2302.13971 ,
2023.
[28] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Sto-
ica, and Eric P. Xing, “Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality,” https:
//lmsys.org/blog/2023-03-30-vicuna/ ,
2023.
[29] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto, “Stanford alpaca: An
instruction-following llama model,” https://
github.com/tatsu-lab/stanford_alpaca ,
2023.
[30] Soham Deshmukh, Benjamin Elizalde, and Huaming
Wang, “Audio retrieval with wavtext5k and clap train-
ing,” arXiv preprint arXiv:2209.14275 , 2022.[31] A.S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata,
and S. Albanie, “Audio retrieval with natural language
queries: A benchmark study,” in IEEE Transactions on
Multimedia , 2022.
[32] Xinhao Mei, Xubo Liu, Jianyuan Sun, Mark D.
Plumbley, and Wenwu Wang, “On metric learning
for audio-text cross-modal retrieval,” arXiv preprint
arXiv:2203.15537 , 2022.
[33] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-
resentation learning with contrastive predictive coding,”
arXiv preprint arXiv:1807.03748 , 2018.
[34] Jort F. Gemmeke, Daniel P. W. Ellis, and Dylan Freed-
man et al., “AudioSet: an ontology and human-labeled
dataset for audio events,” in Proc. ICASSP , 2017.
[35] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng
Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le,
“SpecAugment: a simple data augmentation method
for automatic speech recognition,” arXiv preprint
arXiv:1904.08779 , 2019.
[36] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica, “Judging LLM-
as-a-judge with MT-bench and chatbot arena,” CoRR ,
vol. abs/2306.05685, 2023.
[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix,
Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample, “LLaMA: open and
efficient foundation language models,” arXiv preprint
arXiv:2302.13971 , 2023.
[38] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,
and Gunhee Kim, “AudioCaps: generating captions for
audios in the wild,” in Proc. NAACL-HLT , 2019.
[39] Konstantinos Drossos, Samuel Lipping, and Tuomas
Virtanen, “Clotho: an audio captioning dataset,” in
Proc. ICASSP , 2020.
[40] Meta AI, “fvcore: Collection of common code
that’s shared among different research projects in fair
computer vision team.,” https://github.com/
facebookresearch/fvcore .
[41] Diederik P Kingma and Jimmy Ba, “Adam: A method
for stochastic optimization,” in Proc. ICLR , 2014.
