# 2304.13357.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2304.13357.pdf
# Kích thước tệp: 4377872 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
1
Băm Đa Phương Thức Học Suốt Đời Sâu
Liming Xu, Hanqi Li, Bochuan Zheng, Weisheng Li, Jiancheng Lv
Tóm tắt —Các phương pháp băm đã đạt được tiến bộ đáng kể trong các tác vụ truy xuất đa phương thức với tốc độ truy vấn nhanh và chi phí lưu trữ thấp. Trong số đó, băm dựa trên học sâu đạt được hiệu suất tốt hơn trên dữ liệu quy mô lớn do khả năng trích xuất và biểu diễn tuyệt vời cho các đặc trưng không tuyến tính không đồng nhất. Tuy nhiên, vẫn còn hai thách thức chính là quên thảm khốc khi dữ liệu với các danh mục mới xuất hiện liên tục, và tốn thời gian cho việc truy xuất băm không liên tục để huấn luyện lại để cập nhật. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp băm đa phương thức học suốt đời sâu mới để đạt được truy xuất băm suốt đời thay vì huấn luyện lại hàm băm lặp đi lặp lại khi dữ liệu mới đến. Cụ thể, chúng tôi thiết kế chiến lược học suốt đời để cập nhật hàm băm bằng cách trực tiếp huấn luyện dữ liệu tăng cường thay vì huấn luyện lại hàm băm mới sử dụng tất cả dữ liệu tích lũy, điều này giảm đáng kể thời gian huấn luyện. Sau đó, chúng tôi đề xuất tổn thất băm suốt đời để cho phép các mã băm gốc tham gia vào học suốt đời nhưng vẫn không thay đổi, và tiếp tục bảo tồn tính tương tự và không tương tự giữa các mã băm gốc và tăng cường để duy trì hiệu suất. Ngoài ra, xem xét tính không đồng nhất phân phối khi dữ liệu mới đến liên tục, chúng tôi giới thiệu tương tự ngữ nghĩa đa nhãn để giám sát học băm, và đã được chứng minh rằng tương tự này cải thiện hiệu suất với phân tích chi tiết. Kết quả thực nghiệm trên các tập dữ liệu chuẩn cho thấy phương pháp đề xuất đạt được hiệu suất so sánh với các phương pháp băm đa phương thức tiên tiến gần đây, và nó mang lại sự gia tăng trung bình đáng kể hơn 20% trong độ chính xác truy xuất và giảm gần 80% thời gian huấn luyện khi dữ liệu mới đến liên tục.
Từ khóa chỉ mục —Băm suốt đời, truy xuất đa phương thức, tính không đồng nhất phân phối, quên thảm khốc, tương tự ngữ nghĩa đa nhãn.

I. GIỚI THIỆU
DỮ liệu đa phương tiện hàng ngày được liên kết với mối tương quan ngữ nghĩa tiềm năng cho thấy các đặc tính đa nguồn không đồng nhất và đa phương thức. Dựa trên mối tương quan này, các phương pháp truy xuất đa phương thức [1] nhằm đạt được khớp cặp chéo giữa các phương thức khác nhau. Băm đa phương thức [2], một nhánh quan trọng trong truy xuất đa phương thức, ánh xạ dữ liệu gốc chiều cao vào các mã băm nhị phân chiều thấp trong không gian Hamming trong khi duy trì tính tương tự trong không gian gốc bằng cách buộc hai thể hiện với khoảng cách Hamming nhỏ hơn (lớn hơn) phải tương tự hơn (ít hơn). Sau đó, nhiều phương pháp băm đa phương thức đã được đề xuất và nó có thể được chia thành các phương pháp dựa trên thủ công và học sâu [3].

Liming Xu là tác giả liên hệ. Liming Xu, Hanqi Li và Bochuan Zheng thuộc Khoa Khoa học Máy tính, Đại học Sư phạm Tây Nam Trung Quốc, Nam Chung 637009, Trung Quốc. Liming Xu và Jiancheng Lv thuộc Cao đẳng Khoa học Máy tính, Đại học Tứ Xuyên, Thành Đô 610065, Trung Quốc. Weisheng Li thuộc Cao đẳng Khoa học Máy tính và Công nghệ, Đại học Bưu chính và Viễn thông Trùng Khánh, Trùng Khánh 400065, Trung Quốc. E-mail: xulimmail@gmail.com; lihq@gmail.com; zhengbc@cwnu.edu.cn; liws@cqupt.edu.cn; lvjiancheng@scu.edu.cn.

Mặc dù các phương pháp băm đa phương thức sâu hiện tại đã đạt được hiệu suất thỏa đáng, vẫn còn hai thách thức chính: (1) quên thảm khốc khi dữ liệu tăng cường với các danh mục mới được thêm vào và (2) tính không liên tục của truy xuất băm. Cụ thể, khi dữ liệu tăng cường với danh mục mới đến liên tục, nó sẽ gây ra quên thảm khốc [4], tức là hiệu suất trên hàm băm đã học giảm đáng kể theo thời gian khi dữ liệu tăng cường với các danh mục mới được thêm vào. Từ kết quả tiếp theo, có thể ước tính rằng khi thêm dữ liệu hai danh mục, các phương pháp sâu so sánh phải chịu sự giảm độ chính xác hơn 20%. Sau đó, hầu như tất cả băm đa phương thức sâu cần phải huấn luyện lại hàm băm sử dụng tất cả dữ liệu để tạo ra mã băm mới khi dữ liệu mới đến, điều này có thể tốn thời gian và không hiệu quả. Kết quả tiếp theo cho thấy những phương pháp này yêu cầu nhiều thời gian huấn luyện hơn để huấn luyện lại dữ liệu tích lũy, và sẽ còn tệ hơn trên tập dữ liệu lớn hơn. Bên cạnh đó, việc xem xét tính không đồng nhất phân phối khi dữ liệu mới đến là không đủ. Hầu hết băm đa phương thức hiện tại sử dụng nhãn đơn mà xem xét hai thể hiện là tương tự nếu chúng chia sẻ ít nhất một nhãn để xây dựng tương tự ngữ nghĩa, điều này không thể bảo tồn tương tự ngữ nghĩa trong không gian gốc và không gian Hamming một cách hiệu quả.

Xem xét các vấn đề trên, chúng tôi đề xuất một phương pháp Băm Đa Phương Thức Học Suốt Đời Sâu mới (DLCH) bằng cách kết hợp học cơ bản và học suốt đời. Trong đó, học băm suốt đời được thiết kế để trực tiếp học và cập nhật hàm băm của dữ liệu tăng cường với các danh mục mới. Sau đó, tương tự ngữ nghĩa đa nhãn và chiến lược tự giám sát được đề xuất để có được mã băm chất lượng cao. Các đóng góp chính của bài báo này có thể được tóm tắt như sau:

Chúng tôi đề xuất một phương pháp băm đa phương thức học suốt đời sâu mới để khắc phục quên thảm khốc khi dữ liệu tăng cường với các danh mục mới được thêm vào. Các mã băm gốc giữ không thay đổi trong quá trình học mã băm tăng cường, hiệu suất mô hình trên dữ liệu gốc sẽ không giảm sau khi huấn luyện trên dữ liệu tăng cường.

Chúng tôi thiết kế tổn thất băm suốt đời để trực tiếp học mã băm tăng cường thay vì huấn luyện lại hàm băm đã học, và tối ưu hóa mã băm tăng cường từng bit, điều này tránh chi phí thời gian lớn gây ra bởi việc huấn luyện lại tất cả dữ liệu tích lũy.

Chúng tôi định nghĩa tương tự ngữ nghĩa đa nhãn với thông tin đa nhãn để mô tả mối tương quan ngữ nghĩa chính xác hơn và tạo ra mã băm gốc chất lượng cao. Phân tích chi tiết được cung cấp để cho thấy nó cải thiện hiệu suất đặc biệt trong băm suốt đời.

Các thực nghiệm rộng rãi trên ba tập dữ liệu đa phương thức chuẩn chứng minh rằng DLCH đề xuất đạt được hiệu suất truy xuất cạnh tranh và có được giảm thời gian đáng kể so với các phương pháp tiên tiến.

--- TRANG 2 ---
2
Phần còn lại của bài báo này được tổ chức như sau: Phần II xem xét công việc liên quan về truy xuất băm đa phương thức. Phần III mô tả phương pháp băm đa phương thức học suốt đời sâu đề xuất và tối ưu hóa. Phần IV trình bày kết quả và phân tích thực nghiệm. Cuối cùng, Phần V kết luận công việc của chúng tôi.

II. CÔNG VIỆC LIÊN QUAN
Nói chung, các phương pháp băm đa phương thức hiện tại có thể được phân loại thô thành các phương pháp thủ công và dựa trên học sâu theo trích xuất đặc trưng [5], [6]. Phương pháp trước học hàm băm bằng đặc trưng thủ công và kiến trúc nông, trong khi phương pháp sau nhúng mạng sâu và điều chỉnh huấn luyện theo cách đầu cuối. Chúng tôi chủ yếu tập trung vào băm đa phương thức dựa trên học sâu, và tóm tắt các vấn đề liên quan theo quan điểm tính liên tục như sau.

A. Băm Đa Phương Thức Không Liên Tục
Theo quan điểm tính liên tục, hầu hết các phương pháp băm đa phương thức hiện tại là không liên tục, có nghĩa là tất cả dữ liệu huấn luyện cần được cung cấp từ đầu và tải một lần, và nó sẽ được huấn luyện lại khi dữ liệu tăng cường được thêm vào. Như được báo cáo trong các công việc trước [1], [2], băm đa phương thức dựa trên học sâu chủ yếu chứa mạng nơ-ron sâu (DNN)- và mạng đối kháng tạo sinh (GAN)-dựa trên băm đa phương thức theo các mô hình sâu khác nhau.

Hơn nữa, theo việc có liên quan thông tin giám sát hay không, các phương pháp dựa trên DNN có thể được chia thành không giám sát và có giám sát. Cụ thể, không giám sát học mã băm bằng cách khám phá mối tương quan của dữ liệu không đồng nhất với đồ thị tương quan hoặc không gian ngữ nghĩa tiềm ẩn. DCSH [7] áp dụng phân cụm phổ và ánh xạ neo-đến-neo cho đồ thị tương tự tốt hơn. Để bắc cầu khoảng cách phương thức, JDSH [8] khai thác ma trận tương tự phương thức chung, trong khi DUCMH [9] dựa vào căn chỉnh dữ liệu và cặp dữ liệu hình ảnh-văn bản. DGCPN [10] khám phá mối quan hệ ngữ nghĩa nội tại với sự gắn kết hàng xóm đồ thị để tránh không gian Hamming truy xuất dưới tối ưu. Với việc giới thiệu sơ đồ chưng cất kiến thức, KDCMH [11] huấn luyện một phương pháp không giám sát như mô hình giáo viên được sử dụng để cung cấp thông tin chưng cất để hướng dẫn phương pháp có giám sát. CMIMH [12] cố gắng tìm ra sự cân bằng giữa việc giảm khoảng cách phương thức và mất thông tin riêng tư phương thức bằng cách tối đa hóa thông tin tương hỗ. UCHM [13] tập trung vào tương tác hình ảnh-văn bản để tạo ra ma trận tương tự tương tác phương thức-được kích hoạt ưu việt cho tập huấn luyện. Mặt khác, các phương pháp có giám sát sử dụng đầy đủ thông tin ngữ nghĩa trong thông tin giám sát (ví dụ: thẻ hoặc thông tin tương tự theo cặp) và thường đạt được độ chính xác tốt hơn. Để nổi bật thông tin hữu ích và ngăn chặn thông tin dư thừa, TEACH [14] và MMACH [15] thêm cơ chế chú ý vào quá trình học đặc trưng, và phương pháp sau sử dụng thông tin đa nhãn để cải thiện độ chính xác thêm nữa. HSSAH [16] thay thế ma trận tương tự nhị phân bằng tương tự ngữ nghĩa cấp cao bất đối xứng để duy trì thông tin ngữ nghĩa phong phú hơn. Đối với các cặp dữ liệu với tương tự khác nhau, DCH-SCR [17] điều chỉnh trọng số của các thể hiện tích cực để thay đổi độ mạnh tối ưu hóa.

Ngoài các phương pháp dựa trên DNN được mô tả ở trên, GANs [18] cũng thường được sử dụng cho truy xuất băm đa phương thức. MLCAH [19] đề xuất cơ chế căn chỉnh ngữ nghĩa toàn cục và địa phương để mã hóa thông tin tương quan đa cấp vào mã băm. DADH [20] sử dụng ràng buộc bộ ba cosine có trọng số để học mức độ liên quan tương tự dựa trên xếp hạng của các điểm dữ liệu. MGAH [21] phù hợp với cấu trúc đa tạp cơ bản sử dụng mạng đối kháng tạo sinh đa đường dẫn. CPAH [22] sử dụng mô-đun tinh chế nhất quán để thực hiện việc tách biệt các biểu diễn chung phương thức và riêng tư phương thức không tương thích. DAGNN [23] trích xuất các vectơ đặc trưng chung đại diện hơn với sự hỗ trợ của mạng đối kháng tạo sinh kép và mạng nơ-ron đồ thị đa bước nhảy. SAAH [24] phụ thuộc vào các mô-đun bộ mã hóa tự động đối kháng liên phương thức và nội phương thức để tạo ra biểu diễn đặc trưng đồng nhất. DFAH [25] khắc phục khoảng cách ngữ nghĩa và dịch chuyển phân phối thông qua học đối kháng giữa Bộ phân biệt Phương thức và Bộ trích xuất Đặc trưng Cụ thể Phương thức. CMGCAH [26] giới thiệu mạng trích xuất đặc trưng dựa trên transformer để tận dụng thêm thông tin vị trí.

Mặc dù tiến bộ lớn được thực hiện bởi các phương pháp băm đa phương thức sâu được đề cập, toàn bộ mô hình và hàm băm sẽ được huấn luyện lại một khi dữ liệu thay đổi do tính không liên tục và bản chất của quên thảm khốc, điều này gây ra nhiều thời gian huấn luyện và chi phí tài nguyên.

B. Băm Đa Phương Thức Liên Tục
Xem xét rằng các mô hình không liên tục trên không phù hợp với tập dữ liệu quy mô lớn do chi phí tài nguyên lớn khi huấn luyện dữ liệu luồng, một số đã nhúng xử lý học trực tuyến vào học băm. Để đạt được mục tiêu này, OCMFH [27] sử dụng phân tích ma trận tập thể để học mã băm cho dữ liệu luồng và cập nhật mã băm của dữ liệu cũ theo những thay đổi động của mô hình băm mà không cần truy cập dữ liệu cũ. Tương tự, OCMH [28] chuyển đổi cập nhật không hiệu quả của mã băm thành cập nhật hiệu quả của ma trận mã hóa tiềm ẩn chia sẻ và ma trận chuyển đổi động. Bằng cách xem xét thông tin nhãn, OLSH [29] ánh xạ nhãn rời rạc vào không gian khái niệm ngữ nghĩa tiềm ẩn liên tục, trên đó các thước đo tương tự giữa các điểm dữ liệu được thực hiện. Sau đó, LEMON [30] thiết kế khung nhúng nhãn để tạo ra mã băm phân biệt, điều này sử dụng đầy đủ thông tin ngữ nghĩa. OLCH [31] giới thiệu chiến lược học biểu diễn ngữ nghĩa trực tuyến để bảo tồn tính tương tự giữa dữ liệu mới và dữ liệu cũ trong không gian Hamming. Bên cạnh đó, để tránh lỗi lượng tử hóa, DOCH [32] tối ưu hóa rời rạc các ràng buộc nhị phân và mang lại mã băm chất lượng cao đồng nhất. OMGH [33] sử dụng nhúng đa tạp dựa trên neo để biểu diễn thưa thớt dữ liệu cũ và hướng dẫn thích ứng học băm.

Mặc dù các phương pháp băm này cố gắng giới thiệu quá trình học trực tuyến để cải thiện hiệu quả tính toán và giảm chi phí tài nguyên, chúng tập trung vào tối ưu hóa và không loại bỏ hạn chế rằng mã băm cũ sẽ bị thay đổi. Sau đó, có thể thấy rõ ràng rằng các phương pháp băm đa phương thức trực tuyến hiện tại là kiến trúc nông, và hiện tại không có phương pháp băm đa phương thức học suốt đời sâu nào. Như đã biết, mạng nơ-ron sâu có khả năng

--- TRANG 3 ---
3
nắm bắt các đặc trưng không đồng nhất không tuyến tính. Thứ ba, các băm đa phương thức trực tuyến này đã xem xét việc tiết kiệm chi phí tính toán khi dữ liệu tăng cường được thêm liên tục, nhưng chúng bỏ qua sự thay đổi phân phối sẽ gây ra quên thảm khốc nghiêm trọng và không đạt được hiệu suất cao khi các danh mục mới xuất hiện. Ngoài ra, hiệu suất truy xuất có thể bị hạn chế bởi việc sử dụng đánh giá tương tự nhãn đơn đơn giản.

Được thúc đẩy bởi những điểm yếu của các phương pháp hiện tại, chúng tôi đề xuất băm đa phương thức học suốt đời sâu để tránh quên thảm khốc và giảm thời gian huấn luyện. Đầu tiên, chúng tôi thiết kế chiến lược học suốt đời để trực tiếp học mã băm của dữ liệu tăng cường trên cơ sở mã băm cũ không thay đổi thay vì huấn luyện lại hàm băm sử dụng tất cả dữ liệu tích lũy. Sau đó, chúng tôi kết hợp tổn thất băm suốt đời để bảo tồn tính tương tự và không tương tự giữa các mã băm gốc và tăng cường, điều này được kỳ vọng để khắc phục quên thảm khốc và giảm hiệu suất. Thứ ba, chúng tôi giới thiệu tương tự ngữ nghĩa đa nhãn để giám sát học băm để sử dụng thêm thông tin nhãn để thích ứng với tính không đồng nhất phân phối và cải thiện độ chính xác. Ba thành phần này tạo thành DLCH được đề xuất.

III. PHƯƠNG PHÁP ĐỀ XUẤT
Trong phần này, các chi tiết của DLCH được trình bày. Hình 1 minh họa tổng quan khung, bao gồm phương thức hình ảnh và văn bản để mô tả thuận tiện. Chúng tôi phải lưu ý rằng phương pháp đề xuất của chúng tôi có thể dễ dàng được mở rộng cho nhiều phương thức hơn (ví dụ: hình ảnh, văn bản, âm thanh và đồ họa).

A. Ký Hiệu và Định Nghĩa Bài Toán
Cần thiết phải trình bày ký hiệu và định nghĩa bài toán trước. Trong bài báo này, các chữ cái viết hoa in đậm, như W, biểu thị ma trận; các chữ cái viết thường in nghiêng, như w, biểu thị vectơ. Wi biểu thị hàng thứ i của ma trận W, Wj biểu thị cột thứ j của ma trận W, và Wij biểu thị phần tử của hàng thứ i và cột thứ j của W. Chuyển vị của W được ký hiệu là WT và tr(W) là vết của W. kWk2F=tr(WTW) là chuẩn Frobenius. sign() là hàm dấu được định nghĩa như:

sign(x) = {
1; x≥0
-1; x< 0                                                              (1)

Giả sử có m thể hiện có nhãn O={X,Y,L} trong cơ sở dữ liệu, trong đó X={xi}m i=1∈Rm×dx, Y={yi}m i=1∈Rm×dy và L={li}m i=1∈{0,1}m×c biểu thị hình ảnh, văn bản và nhãn, tương ứng. dx và dy biểu thị chiều của đặc trưng hình ảnh và văn bản gốc, tương ứng. c là tổng số lớp gốc. Tức là, một thể hiện oi được liên kết với nhãn lớp li có cả đặc trưng hình ảnh xi và đặc trưng văn bản yi. Nếu xi hoặc yi thuộc về lớp thứ j, thì lij = 1, ngược lại, lij = 0.

Đối với các m thể hiện này, mục tiêu của chúng tôi là học mạng đa nhãn h(li;θl), hàm băm hình ảnh f(xi;θx) và hàm băm văn bản g(yi;θy) để tạo ra biểu diễn băm tương ứng H={Hi|Hi=h(li;θl)}∈Rk×m, F={Fi|Fi=f(xi;θx)}∈Rk×m và G={Gi|Gi=g(yi;θy)}∈Rk×m, trong đó k biểu thị độ dài của mã băm. Tiếp theo, chúng ta

BẢNG I: Ký Hiệu và Ý Nghĩa.
Ký hiệu | Ý nghĩa
X(Y) | Đặc trưng hình ảnh (văn bản) cho dữ liệu gốc.
X'(Y') | Đặc trưng hình ảnh (văn bản) cho dữ liệu tăng cường.
L(L') | Nhãn cho dữ liệu gốc (tăng cường).
F/(G/H) | Biểu diễn băm của phương thức hình ảnh/văn bản/nhãn.
BX(BY) | Mã băm gốc cho hình ảnh (văn bản).
BX'(BY') | Mã băm tăng cường cho hình ảnh (văn bản).
AX(AY) | Mã băm hình ảnh (văn bản) của dữ liệu huấn luyện.
S | Ma trận tương tự ngữ nghĩa đa nhãn.
Q | Ma trận tương tự biểu diễn băm.
m | Số thể hiện hiện tại với các lớp gốc.
n | Số thể hiện với các lớp mới.
a | Số thể hiện huấn luyện.
c(c') | Số lớp gốc (tăng cường).
k | Số bit mã băm.

có thể sử dụng biểu diễn băm trên để có được mã băm gốc cuối cùng BX∈{-1,+1}k×m và BY∈{-1,+1}k×m.

Tương tự, khi n dữ liệu có nhãn tăng cường O'={X',Y',L'} đến, mỗi thể hiện cũng có phương thức hình ảnh X'={xi}m+n i=m+1, phương thức văn bản Y'={yi}m+n i=m+1 và nhãn L'={li}m+n i=m+1∈{0,1}n×c', trong đó c' là số lớp mới. Đối với n thể hiện mới này, chúng tôi mong muốn cập nhật hàm băm f(xi;θx) và g(yi;θy), và có được mã băm nhị phân tương ứng BX'∈{-1,+1}k×n và BY'∈{-1,+1}k×n với mã băm gốc không thay đổi.

Để đạt được mục tiêu, chúng tôi đầu tiên lấy mẫu tập dữ liệu a từ tập dữ liệu gốc O và tập dữ liệu tăng cường O' và đặt tên là tập huấn luyện Oa={Xa,Ya,La}, trong đó Xa∈Ra×dx⊆{X,X'}, Ya∈Ra×dy⊆{Y,Y'}, La={li}a i=1∈{0,1}a×(c+c'), và a=a1+a2. a1 và a2 biểu thị số dữ liệu huấn luyện được lấy mẫu từ tập dữ liệu gốc và tăng cường, tương ứng.

Các ký hiệu chính trong bài báo này được tóm tắt trong Bảng II.

B. Học Mã Băm Gốc
1) Tổn Thất Băm Gốc: Trong giai đoạn học gốc, mục tiêu của chúng tôi là huấn luyện LabelNet để giám sát việc huấn luyện và huấn luyện ImgNet và TxtNet để xuất ra biểu diễn băm bảo tồn tính tương tự trong không gian gốc. Do đó, đối với biểu diễn băm F cho trước của ImgNet và biểu diễn băm H của LabelNet, tương tự biểu diễn băm được định nghĩa như:

Qxl ij = (Fi/||Fi||2)(Hj/||Hj||2)T                                    (2)

Để thống nhất phạm vi của tương tự ngữ nghĩa đa nhãn và tương tự biểu diễn băm, biến đổi ReLu được áp dụng cho Qxl ij thuộc [-1,1]. Sau đó, tương tự biểu diễn băm có thể được viết lại như:

Qxl ij = max(0, Qxl ij)                                              (3)

Sau đó chúng tôi định nghĩa tổn thất bảo tồn tương tự liên phương thức đo lường khoảng cách giữa tương tự ngữ nghĩa đa nhãn và tương tự biểu diễn băm của hai thể hiện từ các phương thức khác nhau như:

Jinter = Jxl inter + Jyl inter
= ||Sxl - Qxl||2F + ||Syl - Qyl||2F                                 (4)

--- TRANG 4 ---
4
Hình 1: DLCH bao gồm giai đoạn học băm gốc và học suốt đời. Giai đoạn học băm gốc (hộp nét đứt màu xanh) trong đó LabelNet hoạt động như người giám sát để hướng dẫn học hàm băm học hàm băm như các công việc trước đây thực hiện. Giai đoạn học suốt đời (hộp nét đứt màu cam) bao gồm ba bước: (1) lấy mẫu dữ liệu huấn luyện; (2) xây dựng tổn thất băm suốt đời, và (3) cập nhật mã băm tăng cường.

trong đó Jxl inter (Jyl inter) là tổn thất bảo tồn tương tự cho phương thức hình ảnh (văn bản) và đa nhãn. Sxl và Qxl là tương tự ngữ nghĩa đa nhãn và tương tự biểu diễn băm cho đặc trưng hình ảnh x và nhãn l, tương ứng. Syl và Qyl là tương tự ngữ nghĩa đa nhãn và tương tự biểu diễn băm cho đặc trưng hình ảnh y và nhãn l, tương ứng. Biểu diễn băm tối ưu F và G có thể được thu được bằng cách tối thiểu hóa Eq.(4).

Tương ứng, để đo lường khoảng cách giữa tương tự ngữ nghĩa đa nhãn và tương tự biểu diễn băm của hai thể hiện từ cùng một phương thức, chúng tôi định nghĩa tổn thất bảo tồn tương tự nội phương thức như:

Jintra = Jll intra + Jxx intra + Jyy intra
= ||Sll - Qll||2F + ||Sxx - Qxx||2F + ||Syy - Qyy||2F                (5)

trong đó Jll intra, Jxx intra và Jyy intra là tổn thất bảo tồn tương tự nội phương thức cho đa nhãn, phương thức hình ảnh và văn bản, tương ứng. Sll, Sxx và Syy là tương tự ngữ nghĩa đa nhãn cho ba phương thức này. Qll, Qxx và Qyy là tương tự biểu diễn băm tương ứng. Bằng cách tối thiểu hóa Eq.(5), tính tương tự gốc giữa các thể hiện từ cùng phương thức có thể được bảo tồn trong không gian Hamming nhị phân.

Để cho phép đầu ra của hàm băm hướng tới -1 hoặc +1 để có được mã băm nhị phân phân biệt, chúng tôi thêm tổn thất lượng tử hóa làm giảm chênh lệch giữa mã băm nhị phân rời rạc và biểu diễn băm liên tục như:

Jquan = ||F - BX||2F + ||G - BY||2F
+ α/2(||H - BX||2F + ||H - BY||2F)                                   (6)

Tiếp theo, chúng tôi áp dụng tối ưu hóa từng bit để tạo ra mã băm tối ưu theo các công việc trước đây. Kết hợp Eq.(4)-(6), chúng tôi có được hàm mục tiêu tổn thất cuối cùng sau:

min J
BX,BY,θx,θy,θl = Jinter + βJintra + γJquan
s.t.: BX ∈ {-1,+1}k×m
BY ∈ {-1,+1}k×m;                                                     (7)

trong đó β và γ là các siêu tham số điều khiển tỷ lệ trọng số của các tổn thất.

--- TRANG 5 ---
5
2) Tối Ưu Hóa: Chúng tôi tối ưu hóa các tham số θx, θy và θl, và học mã băm gốc BX và BY với chiến lược xen kẽ. Các bước chi tiết được cung cấp dưới đây.

Bước 1: Học θl, với BX, BY, θx và θy cố định
Khi huấn luyện LabelNet, chúng tôi viết lại Eq.(7) như sau:

min θl J = ||Sxl - Qxl||2F + ||Syl - Qyl||2F + ||Sll - Qll||2F
+ α/2(||H - BX||2F + ||H - BY||2F)                                   (8)

Với BX, BY, θx và θy cố định, chúng tôi sử dụng thuật toán giảm gradient ngẫu nhiên (SGD) và lan truyền ngược (BP) để học tham số của mạng đa nhãn. Gradient của tham số được tính toán bởi

∂J/∂Hj = 2(Qxl - Sxl) ∂Qxl ij/∂Hj + 2(Qyl - Syl) ∂Qyl ij/∂Hj
+ 2(Qll - Sll) ∂Qll ij/∂Hj + α(Hj - BXj) + α(Hj - BYj)             (9)

trong đó

∂Qxl ij/∂Hj = {
(Fi/||Fi||2||Hj||2)(∂HTj/∂Hj); nếu Qxl ij ≥ 0
0; nếu Qxl ij < 0                                                     (10)

∂Qyl ij/∂Hj = {
(Gi/||Gi||2||Hj||2)(∂HTj/∂Hj); nếu Qyl ij ≥ 0
0; nếu Qyl ij < 0                                                     (11)

∂Qll ij/∂Hj = {
(Hi/||Hi||2||Hj||2)(∂HTj/∂Hj); nếu Qll ij ≥ 0
0; nếu Qll ij < 0                                                     (12)

Sau đó, chúng ta có thể tính toán ∂J/∂θl với ∂J/∂Hj bằng quy tắc chuỗi và cập nhật tham số θl với thuật toán BP.

Bước 2: Học θx, với BX, BY, θy và θl cố định
Khi huấn luyện ImgNet, chúng tôi viết lại Eq.(7) như:

min θx J = ||Sxl - Qxl||2F + ||Sxx - Qxx||2F + γ||F - BX||2F         (13)

Tương tự, với BX, BY, θy và θl cố định, chúng tôi cũng sử dụng thuật toán SGD và BP để học các tham số θx của hàm băm hình ảnh f(). Đầu tiên, gradient được tính toán bởi:

∂J/∂Fi = 2(Qxl - Sxl) ∂Qxl ij/∂Fi
+ 2(Qxx - Sxx) ∂Qxx ij/∂Fi + 2γ(Fi - BXi)                           (14)

trong đó

∂Qxl ij/∂Fi = {
(1/||Fi||2)(∂Fi/∂Fi)(Hj/||Hj||2)T; nếu Qxl ij ≥ 0
0; nếu Qxl ij < 0                                                     (15)

∂Qxx ij/∂Fi = {
(1/||Fi||2)(∂Fi/∂Fi)(Fj/||Fj||2)T; nếu Qxx ij ≥ 0
0; nếu Qxx ij < 0                                                     (16)

Sau đó, chúng ta có thể tính toán ∂J/∂θx với ∂J/∂Fi bằng quy tắc chuỗi và cập nhật tham số θx với thuật toán BP.

Bước 3: Học θy, với BX, BY, θx và θl cố định
Tương tự, khi huấn luyện TxtNet, chúng tôi viết lại Eq.(7) như sau:

min θy J = ||Syl - Qyl||2F + ||Syy - Qyy||2F + γ||G - BY||2F         (17)

Sau đó với BX, BY, θx và θl cố định, chúng tôi tính toán gradient:

∂J/∂Gi = 2(Qyl - Syl) ∂Qyl ij/∂Gi
+ 2(Qyy - Syy) ∂Qyy ij/∂Gi + 2γ(Gi - BYi)                          (18)

trong đó

∂Qyl ij/∂Gi = {
(1/||Gi||2)(∂Gi/∂Gi)(Hj/||Hj||2)T; nếu Qyl ij ≥ 0
0; nếu Qyl ij < 0                                                     (19)

∂Qyy ij/∂Gi = {
(1/||Gi||2)(∂Gi/∂Gi)(Gj/||Gj||2)T; nếu Qyy ij ≥ 0
0; nếu Qyy ij < 0                                                     (20)

Tiếp theo, chúng ta có thể tính toán ∂J/∂θy với ∂J/∂Gi bằng quy tắc chuỗi và cập nhật tham số θy với thuật toán BP.

Bước 4: Học BX, với BY, θx, θy và θl cố định
Sau khi cập nhật hàm băm hình ảnh gốc, chúng tôi học mã băm gốc BX và viết lại Eq.(7) như:

min BX J = min BX (γ||F - BX||2F + α/2||H - BX||2F)
= min BX tr((2γF + αH)T(BX))
s.t.: BX ∈ {-1,+1}k×m                                               (21)

Rõ ràng, khi dấu của BX khác với (2γF + αH), Eq.21 sẽ đạt giá trị tối thiểu. Tức là, BX và (2γF + αH) giữ cùng dấu và được công thức hóa như:

BX = sign((2γF + αH)) = sign(2γF + αH)                              (22)

Bước 5: Học BY, với BX, θx, θy và θl cố định
Tương tự, sau khi cập nhật hàm băm văn bản gốc, chúng ta có:

BY = sign((2γG + αH)) = sign(2γG + αH)                              (23)

Kết hợp Eq.(8)-(23) và Bước 1-5 trong giai đoạn học gốc, quá trình học băm gốc có thể được tóm tắt trong Thuật toán 1.

C. Học Mã Băm Suốt Đời
1) Tổn Thất Băm Suốt Đời: Để tránh giảm hiệu suất khi huấn luyện trên dữ liệu tăng cường và giữ cho hàm băm đã học có thể sử dụng liên tục, chúng tôi thiết kế tổn thất băm suốt đời chứa tổn thất bảo tồn tương tự gốc và tổn thất bảo tồn tương tự tăng cường. Để đạt được mục tiêu này, chúng tôi đầu tiên định nghĩa tổn thất bảo tồn tương tự gốc như Eq.(24) để giữ tính tương tự giữa dữ liệu huấn luyện và dữ liệu gốc.

Jold = ||(BX)TAX - kSto||2F + ||(BY)TAY - kSto||2F                   (24)

trong đó Sto với kích thước a×m là tương tự ngữ nghĩa đa nhãn giữa dữ liệu huấn luyện và dữ liệu gốc. Tương tự, để giữ tính tương tự giữa dữ liệu huấn luyện và dữ liệu tăng cường, chúng tôi định nghĩa tổn thất bảo tồn tương tự tăng cường như:

Jnew = ||(BX')TAX - kSti||2F + ||(BY')TAY - kSti||2F;                (25)

--- TRANG 6 ---
6
Thuật toán 1 Thuật toán học cho giai đoạn gốc.
Đầu vào:
m dữ liệu gốc O={X,Y,L}; Độ dài mã k; Kích thước mini-batch Nl=Nx=Ny=128;
Đầu ra:
Mã băm gốc BX và BY; Mạng h(), f() và g() với các tham số θl, θx và θy, tương ứng;
1: Khởi tạo các tham số θl, θx và θy, ma trận mã băm BX và BY ngẫu nhiên; số lần lặp Tl=⌈m/Nl⌉, Tx=⌈m/Nx⌉, Ty=⌈m/Ny⌉.
2: Tính toán tương tự ngữ nghĩa đa nhãn S={Sxl,Syl,Sll,Sxx,Syy} theo Eq.41, tương ứng.
3: repeat
4: for iter = 1 to Tl do
5: Lấy mẫu ngẫu nhiên Nl thể hiện từ L để xây dựng một mini-batch.
6: Đối với mỗi thể hiện được lấy mẫu li trong mini-batch, tính toán H=h(li;θl) bằng lan truyền thuận.
7: Tính toán đạo hàm theo Eq.(9).
8: Cập nhật tham số θl bằng cách sử dụng lan truyền ngược.
9: end for
10: for iter = 1 to Tx do
11: Lấy mẫu ngẫu nhiên Nx thể hiện từ X để xây dựng một mini-batch.
12: Đối với mỗi thể hiện được lấy mẫu xi trong mini-batch, tính toán F=f(xi;θx) bằng lan truyền thuận.
13: Tính toán đạo hàm theo Eq.(14).
14: Cập nhật tham số θx bằng cách sử dụng lan truyền ngược.
15: end for
16: for iter = 1 to Ty do
17: Lấy mẫu ngẫu nhiên Ny thể hiện từ Y để xây dựng một mini-batch.
18: Đối với mỗi thể hiện được lấy mẫu yi trong mini-batch, tính toán G=g(yi;θy) bằng lan truyền thuận.
19: Tính toán đạo hàm theo Eq.(18).
20: Cập nhật tham số θy bằng cách sử dụng lan truyền ngược.
21: end for
22: Cập nhật BX theo Eq.(22).
23: Cập nhật BY theo Eq.(23).
24: until một số lần lặp cố định

trong đó Sti với kích thước a×n là tương tự ngữ nghĩa đa nhãn giữa dữ liệu huấn luyện và dữ liệu tăng cường. Để xấp xỉ mã băm rời rạc, chúng tôi công thức hóa tổn thất lượng tử hóa như:

Jquan = ||BX' - F'||2F + ||BY' - G'||2F                             (26)

trong đó BX' ∈ {-1,+1}k×a2 và BY' ∈ {-1,+1}k×a2 là các mã băm nhị phân đã học của a2 dữ liệu tăng cường được lấy mẫu cho tập huấn luyện. F'=f(xa;θx) ∈ Rk×a2 và G'=g(ya;θy) ∈ Rk×a2 là đầu ra của dữ liệu huấn luyện được lấy mẫu từ tập tăng cường thông qua các hàm băm được cập nhật.

Xem xét rằng càng lớn entropy thông tin, thì lượng thông tin nhận được sau khi loại bỏ sự không chắc chắn càng lớn, chúng tôi giới thiệu tổn thất cân bằng cân bằng số lượng -1 và +1 cho mỗi bit để tối đa hóa entropy của các bit băm như:

Jbalance = ||F'1||2F + ||G'1||2F                                     (27)

Bằng cách kết hợp Eq.(27), tổng tích lũy của mỗi hàng của F' và G' càng gần 0 càng tốt, điều này đạt được số lượng cân bằng của -1 và +1 trên mỗi bit. Kết hợp tổn thất bảo tồn tương tự gốc, tổn thất bảo tồn tương tự tăng cường, tổn thất lượng tử hóa và tổn thất cân bằng, hàm mục tiêu cuối cùng của giai đoạn học băm suốt đời có thể được định nghĩa như:

min BX',BY',θx,θy J = λJold + μJnew + νJquan + ηJbalance
s.t.: BX' ∈ {-1,+1}k×n
BY' ∈ {-1,+1}k×n                                                     (28)

trong đó λ, μ, ν và η là các siêu tham số điều khiển trọng số của mỗi phần.

2) Tối Ưu Hóa: Chúng tôi cũng sử dụng chiến lược học xen kẽ để cập nhật hàm băm f() và g(), và các tham số tương ứng θx và θy, để tìm BX' và BY'.

Bước 1: Học θx, với BX', BY', θy cố định
Tương tự như tối ưu hóa của giai đoạn học gốc, với BX', BY' và θy cố định, thuật toán SGD và BP được sử dụng để cập nhật tham số θx của hàm băm hình ảnh f(). Lưu ý rằng hàm liên tục tanh() được sử dụng để phù hợp với AX, và Eq.(28) có thể được viết lại như:

min θx J = ||(BX)TAX - kSto||2F + ||(BX')TAX - kSti||2F
+ ν||BX' - F'||2F + η||F'1||2F                                       (29)

Đối với mỗi đặc trưng hình ảnh xi trong tập huấn luyện, chúng tôi tính toán gradient như:

∂J/∂F'i = 2((BXi)TAXi - kBXi) + 2((BX'i)TAXi - kBX'i)
+ 2ν(F'i - BX'i) + 2ηF'1                                            (30)

Sau đó, chúng ta có thể tính toán ∂J/∂θx với ∂J/∂F'i bằng quy tắc chuỗi và cập nhật tham số θx với thuật toán BP.

Bước 2: Học θy, với BX', BY', θx cố định
Chúng tôi cũng sử dụng tanh() để phù hợp với AY để cập nhật tham số θy của hàm băm văn bản g() với BX', BY', θx cố định. Sau đó, chúng tôi viết lại Eq.(28) như:

min θy J = ||(BY)TAY - kSto||2F + ||(BY')TAY - kSti||2F
+ ν||BY' - G'||2F + η||G'1||2F                                       (31)

Đối với mỗi đặc trưng văn bản yj trong tập huấn luyện, chúng tôi tính toán gradient:

∂J/∂G'j = 2((BYj)TAYj - kBYj) + 2((BY'j)TAYj - kBY'j)
+ 2ν(G'j - BY'j) + 2ηG'1                                            (32)

Sau đó, chúng ta có thể tính toán ∂J/∂θy với ∂J/∂G'i bằng quy tắc chuỗi và cập nhật tham số θy với thuật toán BP.

Bước 3: Học BX', với BY', θx, θy cố định
Khi BY', θx, θy được cố định, chúng tôi ký hiệu BX' ∈ {-1,+1}k×a2 trong Eq.(26) như BX'a2. Vậy chúng ta có:

min BX' J = ||(BX')TAX - kSti||2F + ν||BX'a2 - F'||2F
= ||(BX')TAX||2F - 2k tr(BX'Sti(AX)T) + k2||Sti||2F
+ (ν||BX'a2||2F - 2ν tr(BX'a2F'T) + ν||F'||2F)                      (33)

Mục tiêu là cập nhật BX', và Eq.(33) có thể được viết lại như:

min BX' J = ||(BX')TAX||2F - 2k tr(BX'Sti(AX)T)
- 2ν tr(BX'a2F'T)                                                    (34)

Để đơn giản hóa thêm Eq.(34), BX'a2 ∈ {-1,+1}k×a2 trong số hạng thứ ba có thể được chuyển đổi thành BX' ∈ {-1,+1}k×n. Do đó, chúng tôi định nghĩa một ma trận k×n ~F' như một phần mở rộng của F', tức là, các bit tương ứng trong ~F' giữ lại mã băm trong F' và đặt các bit khác thành 0. Trong trường hợp này, Eq.(34) có thể được viết lại như:

min BX' J = ||(BX')TAX||2F - 2k tr(BX'Sti(AX)T)
- 2ν tr(BX'~F'T)
= ||(BX')TAX||2F - 2 tr((BX')T(kAX(Sti)T + ν~F'))
= ||(BX')TAX||2F + tr((BX')TPX)                                      (35)

trong đó PX = -2kAX(Sti)T - 2ν~F'.

Để đảm bảo độ chính xác truy xuất, chúng tôi không áp dụng chiến lược thư giãn, mà sử dụng thuật toán giảm tọa độ chu kỳ rời rạc (DCC) để tối ưu hóa Eq.(35) và cập nhật BX' từng bit. Để làm điều này, chúng tôi ký hiệu BX'r là hàng thứ r của BX' và BX'−r là ma trận của BX' loại trừ BX'r. Đối với AX và PX, AXr là hàng thứ r của AX, AX−r là ma trận của AXr loại trừ AXr, và PXr là hàng thứ r của PX. Do đó, Eq.(35) được chuyển đổi như sau:

min BX'r J = ||(BX')TAX||2F + tr((BX')TPX)
= tr(BX'r(2(BX'−r)TAX−r(AXr)T + (PXr)T))                            (36)

Dễ dàng thấy rằng Eq.(36) đạt tối thiểu khi mỗi bit trong BX'r có dấu ngược với bit tương ứng trong 2(BX'−r)TAX−r(AXr)T + (PXr)T. Do đó, nghiệm tối ưu của Eq.(36) là

BX'r = -sign(2(BX'−r)TAX−r(AXr)T + (PXr)T)                          (37)

Chúng ta có thể tính toán hàng thứ r của BX' với công thức trên, và sau đó cập nhật tất cả hàng của BX' bằng cách thay thế BX'r.

Bước 4: Học BY', với BX', θx, θy cố định
Tương tự, khi BX', θx và θy được cố định, chúng tôi đơn giản hóa Eq.(28) như sau:

min BY' J = ||(BY')TAY||2F + tr((BY')TPY);                           (38)

trong đó PY = -2kAY(Sti)T - 2ν~G'. ~G' là ma trận mở rộng k×n của G', tức là, các bit tương ứng trong ~G' giữ lại mã băm trong G' và các bit khác là 0. Gọi BY'r là hàng thứ r của BY', BY'−r là ma trận của BY' loại trừ BY'r, AYr là hàng thứ r của AY, AY−r là ma trận của AY loại trừ AYr, PYr là hàng thứ r của PY. Do đó, Eq.(38) được chuyển đổi thành công thức sau:

min BY'r J = ||(BY')TAY||2F + tr((BY')TPY)
= tr(BY'r(2(BY'−r)TAY−r(AYr)T + (PYr)T))                            (39)

Dễ dàng thấy rằng BY'r có dấu ngược với bit tương ứng trong 2(BY'−r)TAY−r(AYr)T + (PYr)T. Do đó, chúng ta có thể có được nghiệm tối ưu của Eq.(39), đó là

BY'r = -sign(2(BY'−r)TAY−r(AYr)T + (PYr)T)                          (40)

Tiếp theo, hàng thứ r của BX' có thể được tính toán với công thức trên, và tất cả hàng của BX' sẽ được cập nhật bằng cách thay thế BX'r.

Kết hợp Eq.(24)-(40) và Bước 1-4 trong giai đoạn học suốt đời, quá trình học băm suốt đời có thể được tóm tắt trong Thuật toán 2.

--- TRANG 7 ---
7
Thuật toán 2 Thuật toán học cho giai đoạn suốt đời.
Đầu vào:
n dữ liệu tăng cường O'={X',Y',L'}; Mã băm gốc BX và BY; Số dữ liệu huấn luyện a; Độ dài mã k; Kích thước mini-batch Nx=Ny=128.
Đầu ra:
Mã băm tăng cường BX' và BY'; Các tham số được cập nhật θx và θy.
1: Lấy mẫu a dữ liệu từ tập gốc và tập tăng cường ngẫu nhiên làm tập huấn luyện Oa={Xa,Ya,La}.
2: Khởi tạo các tham số θx và θy, ma trận mã băm BX' và BY' ngẫu nhiên; số lần lặp Tx=⌈a/Nx⌉, Ty=⌈a/Ny⌉.
3: Tính toán tương tự ngữ nghĩa đa nhãn S={Sto,Sti} theo Eq.(41).
4: repeat
5: for iter = 1 to Tx do
6: Lấy mẫu ngẫu nhiên Nx thể hiện từ Xa để xây dựng một mini-batch.
7: Đối với mỗi thể hiện được lấy mẫu xi trong mini-batch, tính toán F'=f(xi;θx) bằng lan truyền thuận.
8: Tính toán đạo hàm theo Eq.(30).
9: Cập nhật tham số θx bằng cách sử dụng lan truyền ngược.
10: end for
11: for iter = 1 to Ty do
12: Lấy mẫu ngẫu nhiên Ny thể hiện từ Ya để xây dựng một mini-batch.
13: Đối với mỗi thể hiện được lấy mẫu yi trong mini-batch, tính toán G'=g(yi;θy) bằng lan truyền thuận.
14: Tính toán đạo hàm theo Eq.(32).
15: Cập nhật tham số θy bằng cách sử dụng lan truyền ngược.
16: end for
17: Cập nhật BX' theo Eq.(37).
18: Cập nhật BY' theo Eq.(40).
19: until một số lần lặp cố định

D. Tương Tự Ngữ Nghĩa Đa Nhãn
Như đã đề cập trước đây, các phương pháp băm đa phương thức trực tuyến hiện tại sử dụng tương tự nhãn đơn để đánh giá mối tương quan của hai thể hiện, tức là chúng được coi là tương tự miễn là hai thể hiện có một nhãn chung. Ví dụ, khi thể hiện x1 chia sẻ một nhãn chung với thể hiện x2, và chia sẻ ba nhãn giống nhau với thể hiện x3, khái niệm nhãn đơn cho rằng tương tự ngữ nghĩa của x1 và x2, x1 và x3 đều là 1. Rõ ràng rằng thể hiện x1 và x3 tương tự hơn vì chúng có nhiều nhãn chung hơn, điều này minh họa hạn chế của tương tự nhãn đơn trong việc mô tả mối quan hệ tương tự. Do đó, để sử dụng đầy đủ thông tin nhãn, chúng tôi áp dụng tương tự ngữ nghĩa đa nhãn thay vì tương tự nhãn đơn thô, được định nghĩa như sau:

So1o2ij = (Lo1i/||Lo1i||2)(Lo2j/||Lo2j||2)T                         (41)

trong đó Lo1i biểu thị nhãn của thể hiện thứ i trong phương thức o1, Lo2j biểu thị nhãn của thể hiện thứ j trong phương thức o2, và o1,o2 ∈ {x,y,l}. Phạm vi của So1o2ij là [0,1]. Nếu các thể hiện oi và oj có So1o2ij lớn hơn, có nghĩa là chúng tương tự hơn về mặt ngữ nghĩa, ngược lại ít tương tự hơn.

Đối với các thể hiện oi và oj và các mã băm tương ứng bi và bj, chúng tôi sử dụng khoảng cách Hamming disH(bi,bj) = 1/2(K - ⟨bi,bj⟩) để đo lường tính tương tự giữa hai mã băm, trong đó ⟨bi,bj⟩ biểu thị tích vô hướng của bi và bj.

Khi sử dụng tương tự ngữ nghĩa nhãn đơn S'ij = li(lj)T, hàm khả năng có thể được viết như:

P(S'ij|bi,bj) = {
σ(⟨bi,bj⟩); S'ij = 1
1-σ(⟨bi,bj⟩); S'ij = 0
= S'ijσ(⟨bi,bj⟩) + (1-S'ij)(1-σ(⟨bi,bj⟩));                        (42)

trong đó σ(⟨bi,bj⟩) = 1/(1+e^(-⟨bi,bj⟩)). Cho mã băm bi và

--- TRANG 8 ---
8
bj, xác suất của S' là:

L1 = P(S'ij|bi,bj) = ∏(i,j=1 to n) e^((S'ij-1)⟨bi,bj⟩)/(1+e^⟨bi,bj⟩)   (43)

Để thuận tiện cho tính toán, lấy logarit của Eq.(43):

log L1 = log(P(S'ij|bi,bj))
= ∑(i,j=1 to n)[(li(lj)T)⟨bi,bj⟩ - log(1+e^⟨bi,bj⟩)]                    (44)

Tương tự, đối với tương tự đa nhãn Sij = (li/||li||2)(lj/||lj||2)T, hàm khả năng là:

P(Sij|bi,bj) = Sij^σ(⟨bi,bj⟩)                                              (45)

Cho mã băm bi và bj, xác suất của S là:

L2 = P(Sij|bi,bj) = ∏(i,j=1 to n) Sij^1/(1+e^⟨bi,bj⟩)                    (46)

Lấy logarit của công thức trên:

log L2 = log(P(Sij|bi,bj))
= ∑(i,j=1 to n)[log(li/||li||2)(lj/||lj||2)T + ⟨bi,bj⟩ - log(1+e^⟨bi,bj⟩)]  (47)

Kết hợp Eq.(44) và Eq.(47), chúng ta có thể thấy cả L1 và L2 đều tạo ra cùng kết quả khi thể hiện truy vấn oi và thể hiện được truy xuất oj chỉ có một nhãn chung. Khi oi và oj có nhiều hơn một nhãn chung, điều này phổ biến hơn đối với các tập dữ liệu đa phương thức hiện tại, tối ưu hóa L2 mang lại tích vô hướng lớn hơn và khoảng cách Hamming nhỏ hơn.

Cho bán kính Hamming cố định, thể hiện truy vấn oi, thể hiện được truy xuất oj và các mã băm tương ứng bi và bj, xác suất có điều kiện của mức độ liên quan rel(oi,oj) có thể được định nghĩa bởi phân phối Bernoulli như:

P(rel(oi,oj)|bi,bj) = {
σ(disH(bi,bj)); rel(oij) = 1
1-σ(disH(bi,bj)); rel(oij) = 0                                             (48)

Từ Eq.(48) chúng ta có thể thấy rằng khoảng cách Hamming nhỏ hơn disH(bi,bj) làm cho xác suất có điều kiện P(rel(oi,oj) = 1|bi,bj) lớn hơn, điều này có nghĩa là oi và oj nên được đánh giá là có liên quan. Ngược lại, xác suất có điều kiện lớn hơn P(rel(oi,oj) = 0|bi,bj) cho thấy rằng nó nên được đánh giá là không liên quan. Do đó, đối với các thể hiện oi và oj, có thể biết rằng khoảng cách Hamming khi tối ưu hóa L2 nhỏ hơn khi tối ưu hóa L1, tức là xác suất oi và oj được đánh giá là có liên quan khi tối ưu hóa L2 lớn hơn xác suất khi tối ưu hóa L1.

Tra cứu băm, một giao thức truy xuất được sử dụng rộng rãi trong truy xuất dựa trên băm [10], coi thể hiện được truy xuất có khoảng cách Hamming đến truy vấn nhỏ hơn bán kính Hamming là một mẫu tích cực. Khi đo lường độ chính xác của giao thức tra cứu băm, chúng tôi hy vọng rằng một phương pháp băm tốt có thể truy xuất càng nhiều mẫu tích cực càng tốt, tức là khi thể hiện truy vấn oi và thể hiện được truy xuất oj thực sự có liên quan, xác suất được đánh giá là có liên quan nên càng lớn càng tốt, được ký hiệu như:

Precision ∝ P(rel(oi,oj) = 1|bi,bj)                                        (49)

Như được hiển thị trong Eq.(49), tương tự ngữ nghĩa đa nhãn cho phép khoảng cách Hamming nhỏ hơn giữa các thể hiện truy vấn có liên quan oi và các thể hiện được truy xuất oj, dẫn đến độ chính xác cao hơn. Do đó, chúng ta có thể kết luận rằng tương tự ngữ nghĩa đa nhãn được đề xuất có thể cải thiện độ chính xác truy xuất một cách hiệu quả, điều này sẽ được chứng minh với các kết quả thực nghiệm tiếp theo.

E. Phân Tích Độ Phức Tạp
Độ phức tạp tính toán của giai đoạn học gốc chủ yếu được tạo ra bởi việc tối ưu hóa các tham số mạng nơ-ron và học mã băm gốc. Cụ thể, độ phức tạp của việc tối ưu hóa các tham số θl, θx và θy của LabelNet, ImgNet và TxtNet có thể được tính toán bởi Eq.(9), Eq.(14) và Eq.(18), tương ứng, như O(m²k). Độ phức tạp của việc học mã băm gốc được tìm thấy bởi Eq.(22) và Eq.(23), như O(m×k×m×k) = O(m²k²). Do k≪m, độ phức tạp tính toán của học gốc là O(m²).

Độ phức tạp tính toán của giai đoạn học suốt đời chủ yếu được tạo ra bởi việc cập nhật các tham số của hàm băm f(), g() và học mã băm tăng cường. Cụ thể, độ phức tạp của việc cập nhật tham số θx và θy có thể được tính toán bởi Eq.(30) và Eq.(32), như O((m+n)ak), trong khi độ phức tạp của việc cập nhật mã băm tăng cường được tính toán bởi Eq.(37) và Eq.(40), như O(n×k×a×k) = O(ank²). Vì a và k nhỏ hơn nhiều so với m và n, độ phức tạp tính toán của việc cập nhật các tham số là O(m+n), và của việc học mã băm tăng cường là O(n).

IV. THỰC NGHIỆM
Để xác minh hiệu quả của DLCH, các thực nghiệm rộng rãi được thực hiện trên ba tập dữ liệu chuẩn được sử dụng rộng rãi. Chúng tôi đầu tiên so sánh DLCH của chúng tôi với một số thuật toán băm đa phương thức sâu bao gồm các phương pháp không liên tục và trực tuyến về hiệu suất truy xuất, và phân tích kết quả. Sau đó, thảo luận về DLCH và các biến thể của nó để phân tích chức năng của thành phần.

Việc triển khai dựa trên PyTorch có thể có sẵn tại https://github.com.

A. Tập Dữ Liệu
MIRFlickr25K là một tập dữ liệu đa nhãn chứa khoảng 25000 cặp hình ảnh-văn bản được liên kết với 24 nhãn lớp và được thu thập từ Flickr. Chú thích văn bản cho mỗi điểm được biểu diễn như vectơ túi từ 1386 chiều. Trong thực nghiệm của chúng tôi, chúng tôi chọn các cặp hình ảnh-văn bản với ít nhất 20 chú thích văn bản, tạo ra 20,015 cặp văn bản hình ảnh.

NUS-WIDE là một tập dữ liệu đa nhãn khác được sử dụng phổ biến bao gồm hơn 269,000 cặp hình ảnh-văn bản với 81 nhãn lớp. Chú thích văn bản cho mỗi điểm dữ liệu được biểu diễn như vectơ túi từ 1000 chiều. Trong thực nghiệm của chúng tôi, chúng tôi chọn các cặp thuộc về 21 nhãn được sử dụng thường xuyên nhất, trong đó mỗi nhãn chứa ít nhất 5,000 dữ liệu, dẫn đến hơn 195,000 cặp hình ảnh-văn bản.

Wiki là một tập dữ liệu nhãn đơn chứa 2,866 cặp hình ảnh-văn bản với 10 nhãn lớp. Chú thích văn bản cho mỗi điểm dữ liệu được biểu diễn như một vectơ SIFT 128 chiều. Trong thực nghiệm của chúng tôi, chúng tôi chọn tất cả các cặp hình ảnh-văn bản.

--- TRANG 9 ---
9
BẢNG II: Cài đặt chi tiết của các tập dữ liệu thực nghiệm
Tập | MIRFlickr | NUS-WIDE | Wiki
Tổng | 20015 | 195834 | 2866
Truy vấn | 2000 | 2100 | 693
Truy xuất | 18015 | 193734 | 2173
23/1 | 20/1 | 9/1
Lớp của tập gốc | 22/2 | 19/2 | 8/2
VS | 21/3 | 18/3 | 7/3
Lớp của tập tăng cường | 20/4 | 17/4 | 6/4
12/12 | 10/11 | 4/6

Đối với tất cả các tập dữ liệu, 10% cặp từ mỗi lớp sẽ được chọn để tạo thành tập kiểm tra và phần còn lại là tập cơ sở dữ liệu. Trong tập cơ sở dữ liệu, 90% cặp sẽ được sử dụng làm tập huấn luyện và các cặp còn lại được sử dụng cho tập xác thực. Sau đó chúng tôi thay đổi kích thước tất cả các hình ảnh thành kích thước 256×256. Để mô phỏng rằng dữ liệu tăng cường với các danh mục xuất hiện liên tục, chúng tôi chia các tập dữ liệu chuẩn thành hai phần, tức là tập gốc và tập tăng cường, theo nhãn lớp. Đối với Wiki, cài đặt chia '9/1' biểu thị rằng chín lớp được chọn làm tập gốc, và lớp còn lại được sử dụng cho tập tăng cường. Đối với MIRFlickr, '23/1' có nghĩa là tất cả dữ liệu trong tập gốc chứa tối đa 23 nhãn lớp, và dữ liệu lớp còn lại được coi là tập tăng cường. Cài đặt tương tự cho NUS-WIDE, và các cài đặt khác cũng tương tự. Thống kê và cài đặt chia của ba tập dữ liệu chuẩn được hiển thị trong BẢNG II.

B. Chi Tiết Triển Khai và Đánh Giá
Tất cả các thực nghiệm được thực hiện trên Intel(R) Xeon(R) Gold6148CPU với 20 lõi và tám GPU Tesla V100-SXM2 với việc sử dụng cài đặt tham số trong các bài báo gốc để đảm bảo tính công bằng và khách quan. Kích thước batch được đặt thành 64, và epoch được cố định ở 2,000. Được truyền cảm hứng bởi [39], [40], tốc độ học ban đầu trong giai đoạn học băm gốc và tăng cường lần lượt là 0.001 và 0.000001. Tất cả các siêu tham số sẽ được đặt tự động trong phạm vi [10^-2; 10^5] theo kết quả xác thực chéo. Một phân tích chi tiết và so sánh độ nhạy tham số sẽ được cung cấp.

Để đánh giá khách quan hiệu suất của DLCH, chúng tôi áp dụng hai đánh giá được sử dụng rộng rãi, bao gồm Độ Chính Xác Trung Bình (MAP) và Đường Cong Độ Chính Xác-Nhớ Lại với khoảng cách Hamming 2. Sau đó, chúng tôi so sánh phương pháp của chúng tôi với chín phương pháp băm tiên tiến, bao gồm SSAH [34], DBRC [35], RDCMH [36], SADCH [37], MESDCH [38], OCMFH [27], OLSH [29], LEMON [30] và DOCH [32]. Năm phương pháp đầu tiên là các phương pháp băm đa phương thức sâu không liên tục và phần còn lại là các phương pháp băm đa phương thức trực tuyến. Một số đã cung cấp mã nguồn một cách tử tế, chúng tôi tham khảo cài đặt tham số trong các bài báo gốc.

C. Kết Quả Truy Xuất Băm
Các so sánh MAP của DLCH với các bit băm khác nhau trên ba tập dữ liệu chuẩn được báo cáo trong Bảng V, từ đó chúng ta có thể thấy rằng DLCH đạt được hiệu suất tốt hơn so với các baseline khác trong hầu hết các trường hợp. Trong Bảng III, DLCH-i biểu thị số lớp tăng cường.

So sánh với baseline không liên tục tối ưu (tức là MESDCH và SADCH) khi sử dụng hình ảnh để truy xuất văn bản (tức là I→T), của chúng tôi đạt được mức tăng trung bình 3.1%, 12.8% và 10.4% trên MIRFlickr25K, NUS-WIDE và Wiki, tương ứng. Khi sử dụng văn bản để truy xuất hình ảnh (tức là T→I), của chúng tôi cũng đạt được mức tăng trung bình 4.5%, 6.8%, và 6.7% trên MIRFlickr25K, NUS-WIDE và Wiki, tương ứng. Sau đó, so sánh với các phương pháp trực tuyến, của chúng tôi cũng tăng hiệu suất đáng kể, đặc biệt là trên tác vụ I→T. Cụ thể, của chúng tôi đạt được mức tăng trung bình 8.8%, 16.2% so với DOCH có hiệu suất tốt nhất trên MIRFlickr25K và NUS-WIDE. Ngoài ra, khi đánh giá trên Wiki, của chúng tôi đạt được mức tăng đáng kể trung bình 21.6% và 20.2% trên tác vụ I→T và T→I, tương ứng.

Ngoài đánh giá MAP, chúng tôi cũng minh họa các đường cong độ chính xác-nhớ lại với độ dài mã băm khác nhau của các phương pháp so sánh trên các tập dữ liệu chuẩn trong Hình 2, từ đó có thể quan sát thấy rằng của chúng tôi đạt được điểm số tốt nhất trên mỗi độ dài, điều này phù hợp với các quan sát trên điểm số MAP. Lưu ý rằng DLCH không đạt được điểm số tốt nhất trên mỗi tập dữ liệu, và nó phải chịu sự giảm nhỏ trên NUS-WIDE so với DOCH khi sử dụng văn bản để truy xuất hình ảnh. Có thể phân tích rằng không có dữ liệu tăng cường với danh mục mới xuất hiện, điều này làm cho các so sánh trong Bảng III và Hình 2 tốt hơn so với chúng sẽ như thế nào nếu các danh mục mới đã xuất hiện. Kết hợp tất cả kết quả trong Bảng III và Hình 2, chúng ta có thể kết luận rằng phương pháp đề xuất của chúng tôi vượt trội hơn hầu hết các phương pháp băm đa phương thức tiên tiến gần đây, bao gồm băm không liên tục và trực tuyến. Sau đó, càng lớn các lớp tăng cường, hiệu suất càng cao.

D. Phân Tích Quên Thảm Khốc
Như đã đề cập trước đây, hầu như tất cả các phương pháp băm đa phương thức hiện tại đều gặp phải quên thảm khốc khi thêm dữ liệu tăng cường với các danh mục mới và sử dụng hàm băm đã được huấn luyện để truy xuất dữ liệu tăng cường. Chúng tôi bây giờ cung cấp một số kết quả về đánh giá quên thảm khốc như được hiển thị trong Hình 3.

Từ Hình 3, có thể thấy rõ ràng rằng tất cả các phương pháp băm đa phương thức không liên tục đều gặp phải sự suy giảm hiệu suất nghiêm trọng do quên thảm khốc. Hơn nữa, khi có nhiều danh mục mới hơn, hiệu suất giảm nghiêm trọng. Sau đó, chúng ta phải lưu ý rằng quá trình học trực tuyến làm giảm hiện tượng này ở một mức độ nào đó. Tuy nhiên, nó vẫn phải chịu sự giảm không thể chấp nhận được. Lấy DOCH làm ví dụ được hiển thị trong Hình 5, đối với mỗi danh mục bổ sung của dữ liệu, hiệu suất sẽ giảm tương ứng khoảng 3.2% và 3.7% trên tác vụ I→T và T→I, tương ứng. So sánh với các phương pháp này, DLCH của chúng tôi về cơ bản duy trì hiệu suất và đôi khi thậm chí cải thiện khi số lượng danh mục mới tăng lên, điều này được hưởng lợi từ chiến lược học băm suốt đời được đề xuất.

E. Phân Tích Chi Phí Thời Gian
Ngoài độ chính xác truy xuất và quên thảm khốc, thời gian huấn luyện và chi phí tài nguyên cũng được quan tâm. Như đã phân tích trước đây, độ phức tạp của việc học mã băm tăng cường trong phương pháp của chúng tôi phụ thuộc tuyến tính vào kích thước

--- TRANG 10 ---
10
BẢNG III: Điểm Số MAP của Tác Vụ Truy Xuất Đa Phương Thức trên Các Tập Dữ Liệu Chuẩn với Độ Dài Khác Nhau của Mã Băm. In đậm và gạch chân chỉ ra hiệu suất tốt nhất và thứ hai. Tất cả các phương pháp sâu đều dựa trên đặc trưng VGG19.

[Bảng dữ liệu MAP scores với các phương pháp khác nhau và độ dài bit khác nhau cho các tập dữ liệu MIRFlickr25K, NUS-WIDE và Wiki]

Hình 2: Đường Cong Độ Chính Xác-Nhớ Lại được Đánh Giá trên các tập dữ liệu MIRFilckr, NUS-WIDE và Wiki.

Hình 3: Kết quả đánh giá quên thảm khốc.

của dữ liệu tăng cường n. Do đó, chúng tôi tiến hành thêm các thực nghiệm để chứng minh điều này, và kết quả được hiển thị trong Bảng IV.

Như được hiển thị trong Bảng IV, DLCH của chúng tôi có lợi thế tuyệt đối so với các phương pháp không liên tục, và lợi thế này rõ ràng hơn trên tập dữ liệu quy mô lớn hơn. Khi dữ liệu tăng cường được thêm vào cơ sở dữ liệu, các phương pháp baseline này cần sử dụng tất cả dữ liệu tích lũy để huấn luyện lại, điều này đòi hỏi nhiều thời gian huấn luyện hơn theo cách lặp lại. Ngược lại, DLCH chỉ sử dụng dữ liệu tăng cường để cập nhật hàm băm, điều này giảm đáng kể thời gian huấn luyện và chi phí tài nguyên.

Cũng có thể quan sát thấy rằng của chúng tôi cần nhiều thời gian huấn luyện hơn so với các phương pháp trực tuyến khác, điều này là do

--- TRANG 11 ---
11
BẢNG IV: So Sánh Thời Gian Huấn Luyện (tính bằng Phút) của Các Baseline trên Các Tập Dữ Liệu Chuẩn với Một Lớp Tăng Cường và 32 bit.

Fashion Học | Phương pháp | MIRFlicker | NUS-WIDE | WIKI
Băm Không liên tục | DCMH [39] | 227.8 | 17170.1 | 20.6
 | SSAH | 431.2 | 22000.4 | 25.8
 | DBRC | 358.8 | 20681.0 | 35.4
 | AADCMH [40] | 447.8 | 30488.6 | 45.2
 | AGCN [41] | 289.3 | 21085.9 | 34.6
 | MESDCH | 337.8 | 20201.1 | 25.4
Băm Trực tuyến | OCMFH | 32.6 | 310.1 | 2.3
 | OLSH | 33.9 | 297.6 | 2.1
 | LEMON | 46.3 | 360.1 | 2.3
 | DOCH | 41.2 | 357.2 | 2.5
 | OMGH | 41.6 | 393.1 | 2.6
Băm Suốt Đời | DLCH | 76.1 | 1016.2 | 6.7

BẢNG V: Tham Số Tối Ưu của Các Tác Vụ Khác Nhau trên Các Tập Dữ Liệu Chuẩn.

Tham số | MIRFlicker | NUS-WIDE | WIKI
 | T→I | I→T | T→I | I→T | T→I | I→T
β | 10 | 10 | 10000 | 10000 | 10000 | 10000
γ | 10 | 100 | 12 | 10 | 7 | 9
α | 1 | 1 | 1 | 1 | 0.6 | 0.8
λ | 10 | 10 | 200 | 200 | 1000 | 200
μ | 10 | 100 | 50 | 50 | 1 | 1

DLCH của chúng tôi là phương pháp dựa trên học sâu. Kết hợp với các kết quả thực nghiệm trên, chúng ta có thể thấy rằng DLCH có hiệu suất thời gian xuất sắc, điều này đáng kể hơn trên các tập dữ liệu quy mô lớn. Xem xét độ chính xác truy xuất cao và sự suy giảm hiệu suất thấp, DLCH được đề xuất có tính cạnh tranh mạnh so với các phương pháp băm đa phương thức này.

F. Độ Nhạy với Tham Số
Có năm siêu tham số trong DLCH, trong đó β, γ và α được thiết kế cho giai đoạn học gốc và λ và μ được sử dụng trong giai đoạn học suốt đời. Chúng tôi bây giờ đánh giá ảnh hưởng của các siêu tham số trong việc kiểm soát tỷ lệ trọng số giữa các tổn thất. Chúng tôi đầu tiên đặt độ dài mã băm k = 64. Sau đó, chúng tôi tính toán các giá trị MAP bằng cách điều chỉnh các tham số giữa 10^-2 và 10^5 với bước nhân 10. Hình 4 hiển thị kết quả thực nghiệm của các siêu tham số β, γ, α, λ và μ trên MIRFlickr.

Như được minh họa trong Hình 4, λ và μ khác nhau có tác động rõ ràng hơn đến giá trị MAP, trong khi nó thay đổi nhẹ nhàng hơn với β. Trong quá trình học băm tăng cường, γ và α không có tác động đặc biệt lớn đến các giá trị MAP. Theo cách tương tự, các giá trị tối ưu tốt nhất cho ba tập dữ liệu chuẩn được tóm tắt trong BẢNG V.

G. Thực Nghiệm Cắt Bỏ
Để đạt được truy xuất băm suốt đời, một số tổn thất được định nghĩa. Chúng tôi bây giờ điều tra các biến thể của DLCH để phân tích thêm hiệu quả của mỗi tổn thất. Trong quá trình khám phá, chúng tôi so sánh các biến thể này trên tập dữ liệu MIRFlickr với việc đặt số lớp tăng cường là 1.

Đầu tiên, chúng tôi thiết kế ba biến thể, DLCH-intra, DLCH-inter và DLCH-quant, để đảm bảo hàm tổn thất băm gốc. DLCH-intra, DLCH-inter và DLCH-quant là biến thể DLCH loại bỏ tương tự nội phương thức, liên phương thức

BẢNG VI: So Sánh MAP trong Băm Gốc.

Phương pháp | I→T |  |  | T→I |  | 
 | 16bits | 32bits | 64bits | 16bits | 32bits | 64bits
DLCH | 0.848 | 0.865 | 0.879 | 0.815 | 0.827 | 0.850
DLCH-intra | 0.823 | 0.841 | 0.837 | 0.743 | 0.717 | 0.735
DLCH-inter | 0.808 | 0.828 | 0.836 | 0.739 | 0.730 | 0.732
DLCH-quant | 0.810 | 0.835 | 0.860 | 0.791 | 0.819 | 0.839
DLCH-O | 0.841 | 0.851 | 0.872 | 0.811 | 0.822 | 0.838
DLCH-I | 0.836 | 0.855 | 0.877 | 0.806 | 0.819 | 0.836
DLCH-Q | 0.833 | 0.852 | 0.877 | 0.803 | 0.810 | 0.837
DLCH-B | 0.835 | 0.851 | 0.879 | 0.811 | 0.820 | 0.832

BẢNG VII: So Sánh MAP của Tương Tự Ngữ Nghĩa Nhãn.

Phương pháp | I→T |  |  | T→I |  | 
 | 16bits | 32bits | 64bits | 16bits | 32bits | 64bits
DLCH-multi | 0.848 | 0.865 | 0.879 | 0.815 | 0.827 | 0.850
DLCH-single | 0.823 | 0.841 | 0.837 | 0.743 | 0.717 | 0.735

bảo tồn và tổn thất lượng tử hóa, tương ứng. Sau đó, để khám phá hiệu quả của băm suốt đời, bốn biến thể bổ sung được thiết kế. DLCH-O, DLCH-L, DLCH-Q và DLCH-B là các biến thể không sử dụng tổn thất bảo tồn tương tự gốc, tổn thất băm suốt đời, tổn thất lượng tử hóa và tổn thất cân bằng bit, tương ứng. Các so sánh được hiển thị trong Bảng VI.

Trong quá trình học băm gốc, có thể thấy từ Bảng VI rằng DLCH đạt được mức tăng trung bình 4.1% và 3.7% trên tất cả các bit so với DLCH-intra không xem xét tính tương tự giữa các phương thức nội tại trong hai tác vụ này. DLCH đạt được mức tăng trung bình 7.3% và 8.1% so với DLCH-inter bỏ qua tương tự liên phương thức. Sau đó, DLCH-quant sử dụng thư giãn liên tục để giải quyết tối ưu hóa rời rạc và kết quả, điều này tự nhiên phải chịu một số sự giảm so với DLCH. Trong quá trình học băm tăng cường, do việc giới thiệu tương tự nội phương thức và liên phương thức bảo tồn, và các ràng buộc lượng tử hóa, DLCH cũng đạt được lợi thế rõ ràng so với DLCH-O, DLCH-L, DLCH-Q và DLCH-B. Tổn thất băm suốt đời được thiết kế để giữ cho hàm băm đã được huấn luyện có thể sử dụng liên tục bằng cách bảo tồn tính tương tự giữa dữ liệu gốc và tăng cường. Từ Bảng VI, chúng ta có thể thấy rằng DLCH đạt được mức tăng cao hơn, tức là 6.7% và 7.5% so với DLCH-O, có nghĩa là nó thực sự bảo tồn hiệu suất và tránh quên thảm khốc.

Hơn nữa, để xác minh tính hợp lệ của tương tự ngữ nghĩa đa nhãn, chúng tôi so sánh các biến thể sử dụng tương tự ngữ nghĩa đa nhãn và nhãn đơn, và các so sánh được hiển thị trong Bảng VII. Từ Bảng VII, có thể thấy rằng DLCH với tương tự ngữ nghĩa đa nhãn cải thiện độ chính xác truy xuất 20% so với tương tự ngữ nghĩa nhãn đơn, điều này phù hợp với phân tích trong Phần III-D.

V. KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một băm đa phương thức học suốt đời sâu mới, giải quyết hiệu quả vấn đề được mang lại bởi các danh mục mới xuất hiện trong cơ sở dữ liệu. Phương pháp của chúng tôi được chia thành hai giai đoạn: học gốc và học suốt đời. Chúng tôi sử dụng tương tự ngữ nghĩa đa nhãn cho giai đoạn đầu tiên để giám sát việc học hàm băm và có được mã băm gốc chất lượng cao. Đồng thời, chúng tôi thiết kế tổn thất băm suốt đời cho giai đoạn thứ hai, trong đó mã băm tăng cường được học trực tiếp trong khi mã băm gốc vẫn không thay đổi. Các thực nghiệm rộng rãi trên ba điểm chuẩn thực tế nổi tiếng cho thấy rằng phương pháp đề xuất có hiệu suất tốt hơn so với các phương pháp baseline khác với việc giảm thời gian huấn luyện đáng kể và là một phương pháp truy xuất băm đa phương thức hiệu quả.

LỜI CẢM ƠN
Các tác giả muốn cảm ơn các nhà đánh giá ẩn danh vì sự giúp đỡ của họ. Công việc này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số hiệu 62176217, 62206224), Quỹ Khoa học Tự nhiên tỉnh Tứ Xuyên (Số hiệu 2022NSFSC0866), Quỹ Nhóm Đổi mới Đại học Sư phạm Tây Nam Trung Quốc (Số hiệu KCXTD2022-3), và Dự án Đổi mới Nghiên cứu Tiến sĩ (Số hiệu 21E025).

TÀI LIỆU THAM KHẢO
[1] P. Kaur, H. Pannu và A. Malhi, "Phân tích so sánh về truy xuất thông tin đa phương thức: Một đánh giá," trong Comput. Sci. Rev., tập 39, tr. 100336, 2021.
[2] W. Cao, W. Feng, Q. Lin, G. Cao và Z. He, "Một Đánh Giá về Các Phương Pháp Băm cho Truy Xuất Đa Phương Thức," trong IEEE Access, tập 8, tr. 15377-15391, 2020.
[3] G. Menghani, "Học Sâu Hiệu Quả: Một Khảo Sát về Việc Làm cho Các Mô Hình Học Sâu Nhỏ Hơn, Nhanh Hơn và Tốt Hơn," trong ACM Comput. Surv., tập 55, số 12, tr. 1-37, 2023.
[4] M. Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh và T. Tuytelaars, "Một Khảo Sát Học Liên Tục: Thách Thức Quên Lãng trong Các Tác Vụ Phân Loại," trong IEEE Trans. Pattern Anal. Mach. Intell., tập 44, tr. 3366-3385, 2021.

[Tiếp tục với các tài liệu tham khảo khác...]

--- TRANG 12 ---
12
Hình 4: Phân tích độ nhạy trên Tập dữ liệu MIRFlickr trong phạm vi [10^-2; 10^5].

mã băm được học trực tiếp trong khi mã băm gốc vẫn không thay đổi. Các thực nghiệm rộng rãi trên ba điểm chuẩn thực tế nổi tiếng cho thấy rằng phương pháp đề xuất có hiệu suất tốt hơn so với các phương pháp baseline khác với việc giảm thời gian huấn luyện đáng kể và là một phương pháp truy xuất băm đa phương thức hiệu quả.

LỜI CẢM ƠN
Các tác giả muốn cảm ơn các nhà đánh giá ẩn danh vì sự giúp đỡ của họ. Công việc này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số hiệu 62176217, 62206224), Quỹ Khoa học Tự nhiên tỉnh Tứ Xuyên (Số hiệu 2022NSFSC0866), Quỹ Nhóm Đổi mới Đại học Sư phạm Tây Nam Trung Quốc (Số hiệu KCXTD2022-3), và Dự án Đổi mới Nghiên cứu Tiến sĩ (Số hiệu 21E025).

TÀI LIỆU THAM KHẢO
[1] P. Kaur, H. Pannu và A. Malhi, "Phân tích so sánh về truy xuất thông tin đa phương thức: Một đánh giá," trong Comput. Sci. Rev., tập 39, tr. 100336, 2021.
[2] W. Cao, W. Feng, Q. Lin, G. Cao và Z. He, "Một Đánh Giá về Các Phương Pháp Băm cho Truy Xuất Đa Phương Thức," trong IEEE Access, tập 8, tr. 15377-15391, 2020.

[Tiếp tục với các tài liệu tham khảo từ [3] đến [41]...]

--- TRANG 13 ---
13
[21] J. Zhang và Y. Peng, "Băm Đối Kháng Tạo Sinh Đa Đường Dẫn cho Truy Xuất Đa Phương Thức Không Giám Sát," trong IEEE Trans. Multimedia, tập 22, tr. 174-187, 2020.

[Tiếp tục với các tài liệu tham khảo còn lại...]
