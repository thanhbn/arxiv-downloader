# 2204.09636.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2204.09636.pdf
# Kích thước tệp: 3728209 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hỗn hợp Chuyên gia Dư
Lemeng Wu1*, Mengchen Liu2, Yinpeng Chen2, Dongdong Chen2, Xiyang Dai2, Lu Yuan2
1Đại học Texas tại Austin 2Microsoft
lmwu@cs.utexas.edu, {mengcliu,yiche,dochen,xidai,luyuan}@microsoft.com

Tóm tắt
Hỗn hợp Chuyên gia (MoE) có thể mở rộng quy mô vision transformer một cách hiệu quả. Tuy nhiên, nó đòi hỏi tài nguyên tính toán cấm cao để huấn luyện một MoE transformer lớn. Trong bài báo này, chúng tôi đề xuất Hỗn hợp Chuyên gia Dư (RMoE), một quy trình huấn luyện hiệu quả cho MoE vision transformer trên các nhiệm vụ downstream, chẳng hạn như phân đoạn và phát hiện. RMoE đạt được kết quả tương đương với huấn luyện MoE giới hạn trên, trong khi chỉ tăng thêm chi phí huấn luyện nhỏ so với quy trình huấn luyện non-MoE giới hạn dưới. Hiệu quả này được hỗ trợ bởi quan sát chính của chúng tôi: trọng số của một MoE transformer có thể được phân tích thành một lõi độc lập với đầu vào và một phần dư phụ thuộc vào đầu vào. So với lõi trọng số, phần dư trọng số có thể được huấn luyện hiệu quả với ít tài nguyên tính toán hơn nhiều, ví dụ như tinh chỉnh trên dữ liệu downstream. Chúng tôi chỉ ra rằng, so với quy trình huấn luyện MoE hiện tại, chúng tôi có được kết quả tương đương trong khi tiết kiệm hơn 30% chi phí huấn luyện. Khi so sánh với các transformer non-MoE tiên tiến, như Swin-T / CvT-13 / Swin-L, chúng tôi có được +1.1 / 0.9 / 1.0 mIoU trên phân đoạn ADE20K và +1.4 / 1.6 / 0.6 AP trên nhiệm vụ phát hiện đối tượng MS-COCO với ít hơn 3% chi phí huấn luyện bổ sung.

1 Giới thiệu
Vision transformer gần đây đã đạt được nhiều đột phá, với khả năng mạnh mẽ để nắm bắt một lượng lớn dữ liệu. Khả năng như vậy cho phép chúng ta tiền huấn luyện một vision transformer trên một tập dữ liệu upstream quy mô lớn. Mô hình được tiền huấn luyện giúp dễ dàng tinh chỉnh trên các nhiệm vụ downstream thách thức, với tốc độ hội tụ nhanh và hiệu suất xuất sắc.

Để tăng cường hơn nữa khả năng mô hình của vision transformer, một số nghiên cứu mở rộng quy mô vision transformer lên hàng tỷ tham số có thể nắm bắt hiệu quả dữ liệu upstream khổng lồ [3, 62]. Những mô hình này đạt được hiệu suất tiên tiến trên các nhiệm vụ downstream khác nhau, chẳng hạn như phân đoạn ngữ nghĩa và phát hiện đối tượng. Tuy nhiên, việc mở rộng quy mô trực tiếp một vision transformer bằng cách tăng độ rộng và độ sâu mô hình sẽ làm tăng đáng kể chi phí huấn luyện. Điều này khiến việc huấn luyện cho các vision transformer lớn trở nên không thể chi trả đối với hầu hết các nhà nghiên cứu và thực hành viên về thị giác [62]. Ngoài ra, nhiều nhiệm vụ downstream thị giác máy tính sử dụng hình ảnh độ phân giải cao làm đầu vào. Do đó, việc mở rộng quy mô trực tiếp cũng bị giới hạn bởi bộ nhớ GPU.

Để giải quyết các vấn đề trong việc mở rộng quy mô mô hình, có các nỗ lực nghiên cứu gần đây [18, 35, 41, 44] giới thiệu tính toán có điều kiện và độ thưa trong transformer. Là một trong những kỹ thuật đại diện nhất, Hỗn hợp Chuyên gia (MoE) mở rộng quy mô một transformer với các chuyên gia được tính toán có điều kiện. Trong một MoE transformer,

*Công việc được thực hiện trong thời gian thực tập tại Microsoft.

--- TRANG 2 ---
Hình 1: RMoE: cân bằng cải thiện hiệu suất và chi phí huấn luyện bổ sung giữa quy trình huấn luyện MoE giới hạn trên và non-MoE giới hạn dưới. RMoE-D và RMoE-I là hai biến thể của RMoE.

mỗi điểm dữ liệu chỉ được xử lý bởi một số lượng chuyên gia nhất định. Bằng cách làm cho các thành phần của mô hình được tính toán có điều kiện, chi phí huấn luyện của MoE transformer ít hơn nhiều so với transformer non-MoE ở cùng quy mô.

Trong nghiên cứu này, chúng tôi đề xuất Hỗn hợp Chuyên gia Dư (RMoE), một quy trình huấn luyện hiệu quả cho MoE vision transformer lớn. Như thể hiện trong Hình 1, RMoE đạt được kết quả tương đương với quy trình huấn luyện MoE giới hạn trên, trong khi chỉ tăng thêm chi phí huấn luyện nhỏ so với quy trình huấn luyện non-MoE giới hạn dưới. Hiệu quả này được hỗ trợ bởi quan sát của chúng tôi từ việc phân tích trọng số của các chuyên gia trong một MoE transformer đã được huấn luyện. Chúng tôi thấy rằng trọng số có thể được phân tích thành một lõi độc lập với đầu vào và một phần dư phụ thuộc vào đầu vào. Mặc dù kích thước của phần dư trọng số lớn hơn nhiều so với lõi trọng số, phần dư có thể được huấn luyện hiệu quả với ít tài nguyên tính toán hơn nhiều, ví dụ như tinh chỉnh trên dữ liệu downstream. Sử dụng quan sát này, chúng tôi phát triển quy trình huấn luyện RMoE. Như thể hiện trong Hình 2 (b) và (c), trong huấn luyện RMoE, chúng ta có thể tiền huấn luyện một non-MoE transformer trên dữ liệu upstream và mở rộng quy mô mô hình trong quá trình tinh chỉnh downstream hoặc tinh chỉnh trung gian. Bằng cách bỏ qua việc huấn luyện mô hình được mở rộng quy mô trên nhiệm vụ upstream nặng nề, chúng ta chỉ tăng thêm chi phí huấn luyện nhỏ trong khi tận hưởng sự cải thiện hiệu suất được mang lại từ khả năng mô hình lớn. Hơn nữa, vì nhiều checkpoint non-MoE transformer được tiền huấn luyện có sẵn công khai, các thực hành viên có thể tận dụng những transformer được huấn luyện tốt này làm khởi tạo. Do đó, phương pháp của chúng tôi mở ra khả năng tùy chỉnh một transformer quy mô lớn cho các nhiệm vụ khác nhau mà không bị giới hạn bởi lượng lớn tài nguyên tính toán. Chúng tôi chứng minh hiệu quả của RMoE trên phát hiện đối tượng và phân đoạn. Bằng cách áp dụng RMoE cho các backbone thị giác khác nhau, Swin-T, CvT-13 và Swin-L, chúng tôi có được +1.1 / 0.9 / 1.0 mIoU trên ADE20K và +1.4 / 1.6 / 0.6 AP trên phát hiện đối tượng với ít hơn 3% chi phí bổ sung.

2 Nghiên cứu liên quan

2.1 Vision Transformer
Mạng nơ-ron tích chập (CNN), như những cấu trúc phổ biến và mạnh mẽ trong lĩnh vực thị giác máy tính [24, 26, 27, 28, 32, 42, 45, 46, 47], đã đạt được thành công lớn trong thập kỷ qua. Gần đây, vision transformer cho thấy kết quả hứa hẹn trong các nhiệm vụ thị giác khác nhau và thu hút sự quan tâm nghiên cứu sâu sắc. ViT [15], là công trình đột phá đầu tiên, mô hình hóa mỗi hình ảnh như một tập hợp các token, trong đó mỗi token là một patch hình ảnh. Nó đạt được kết quả phân loại hình ảnh cạnh tranh so với CNN truyền thống. Được kích hoạt bởi ViT, một loạt các nghiên cứu tiếp theo [8, 9, 14, 17, 21, 25, 30, 38, 49, 50, 52, 54, 57, 59, 60] liên tục làm mới các kỷ lục phân loại hình ảnh và đẩy tiềm năng của vision transformer trong thị giác máy tính lên một tầm cao mới. Trong

--- TRANG 3 ---
Hình 2: Quy trình huấn luyện cho (a) non-MoE, (b)(c) RMoE và (d) MoE. Ở đây chúng tôi sử dụng 3 chuyên gia làm ví dụ để minh họa cách RMoE hoạt động trong các giai đoạn tinh chỉnh trung gian và downstream. So với huấn luyện MoE, chúng ta kế thừa non-MoE transformer sau tiền huấn luyện upstream để tiết kiệm chi phí. Trong hình này, chúng tôi đơn giản hóa thiết kế khối transformer tổng quát bằng cách chỉ hiển thị các mô-đun self-attention (SA) và multilayer perceptron (MLP). Các thao tác Norm và add không được hiển thị ở đây.

các nghiên cứu trên, nhiều nghiên cứu áp dụng kiến trúc phân cấp và tiền huấn luyện dữ liệu upstream quy mô lớn. Do đó, chúng mở khóa khả năng tinh chỉnh cho các nhiệm vụ thị giác downstream khác nhau như phân đoạn và phát hiện. Trong nghiên cứu của chúng tôi, chúng tôi tận dụng kiến trúc và các checkpoint có sẵn của những vision transformer mạnh mẽ này và khám phá khả năng mở rộng quy mô chúng thành một mô hình mạnh mẽ hơn với chi phí thấp. Chúng tôi chứng minh rằng RMoE không bị giới hạn bởi vision transformer cụ thể nào.

2.2 Hỗn hợp Chuyên gia
Hỗn hợp Chuyên gia (MoE) có lịch sử lâu dài qua những thập kỷ [6, 29, 31, 61]. Nhiều kiến trúc chuyên gia khác nhau được đề xuất [10, 12, 43, 48, 51]. Trong khi đó, thay vì thiết kế các chuyên gia trong kiến trúc, có những ý tưởng nghiên cứu tương tự để tạo ra một tập hợp đầu ra và chọn đầu ra đúng, chẳng hạn như học đa lựa chọn [20, 33, 34].

Gần đây, việc mở rộng quy mô transformer sử dụng MoE được chứng minh là hiệu quả để đạt được hiệu suất tiên tiến trên nhiều nhiệm vụ khác nhau [40, 41]. Tuy nhiên, vẫn còn đắt đỏ để huấn luyện một mạng nơ-ron với hàng tỷ tham số. So với các mô hình non-MoE, một mạng nơ-ron MoE chứa một tập hợp các chuyên gia, ví dụ như multilayer perceptron (MLP), và một router để chọn tập con chuyên gia nào được sử dụng cho mỗi điểm dữ liệu đầu vào. Nó tăng khả năng mạng bằng tính toán có điều kiện như vậy trong khi duy trì huấn luyện tương đối hiệu quả. MoE đã được sử dụng rộng rãi trong Xử lý Ngôn ngữ Tự nhiên [18, 22, 35, 44] và thị giác máy tính [1, 16, 19, 41, 53, 58]. Trong nghiên cứu của chúng tôi, chúng tôi cũng nỗ lực mở rộng quy mô khả năng mô hình của vision transformer. So với các nghiên cứu trước về MoE vision transformer, chúng tôi phát triển một quy trình huấn luyện hiệu quả hơn, giảm tổng chi phí huấn luyện để có được một MoE vision transformer lớn. Hiệu quả như vậy đạt được thông qua việc phân tích trọng số của các chuyên gia. Ngoài ra, chúng tôi là những người đầu tiên giải quyết vấn đề áp dụng MoE trên các nhiệm vụ downstream với hình ảnh độ phân giải cao như phân đoạn và phát hiện. Những loại nhiệm vụ này đặt ra thách thức kỹ thuật về bộ nhớ thiết bị trong khi yêu cầu một backbone mạnh mẽ để có hiệu suất tốt, điều này tự nhiên phù hợp với công việc của chúng tôi.

--- TRANG 4 ---
3 Kiến thức nền

Trong phần này, chúng tôi giới thiệu ý tưởng cơ bản của MoE và cách sử dụng nó cho transformer để mở rộng quy mô mô hình.

Lớp MoE. Các lớp MoE là thành phần quan trọng trong một mô hình MoE. Bằng cách cho phép tính toán có điều kiện phụ thuộc vào đầu vào, các chuyên gia khác nhau trong một lớp MoE được gán để xử lý các phần khác nhau của không gian đầu vào [29]. Như một trong những thiết lập phổ biến nhất [44], một lớp MoE chứa hai thành phần: (1) n chuyên gia Ei(x) : R^Din → R^Dout, i = 1...n để xử lý các đầu vào khác nhau và (2) một hàm cổng G(x) : R^Din → R^n để định tuyến các đầu vào khác nhau đến các chuyên gia khác nhau. Cho một đầu vào x ∈ R^Din, một lớp MoE n-chuyên gia tính toán đầu ra có điều kiện y ∈ R^Dout như tổng có trọng số của hàm cổng G(x) và đầu ra chuyên gia [Ei(x)]:

y = Σ(i=1 to n) G(x)i Ei(x).                    (1)

G và Ei thường được mô hình hóa bằng mạng nơ-ron. Trong nghiên cứu của chúng tôi, chúng tôi theo các thiết kế trước [41, 44] để đặt G như một cổng tuyến tính với hàm softmax. Để khuyến khích độ thưa trong một lớp MoE, thường nó chỉ hạn chế k chuyên gia trong đó k < n tham gia vào tính toán cho mỗi đầu vào. Vậy nên một toán tử TopK được sử dụng trong hàm cổng để buộc chỉ k chuyên gia được sử dụng trong khi những chuyên gia khác bị bỏ qua cho một đầu vào. Do đó chúng ta có thể viết G(x) như

G(x) = TopK(softmax(gx)),                    (2)

trong đó g là các tham số của phép toán tuyến tính trong một cổng.

Mất mát Cân bằng Tải. Trong thực tế, một vấn đề chính để huấn luyện một mô hình MoE là một số chuyên gia được định tuyến với ít điểm dữ liệu hơn nhiều so với các chuyên gia khác. Điều này sẽ gây ra huấn luyện không đầy đủ cho các chuyên gia được định tuyến với ít điểm dữ liệu hơn. Ngoài ra, vì các chuyên gia thường được xử lý song song, nó cũng sẽ làm hại hiệu quả huấn luyện do hiệu ứng bucket. Để tránh vấn đề này, chúng ta thường thêm một mất mát cân bằng tải [44] trên hàm cổng vào tổng mất mát huấn luyện. Cho một batch đầu vào X, một mất mát cân bằng tải được sử dụng rộng rãi L được định nghĩa như

L(X) = std(Imp(X)) / mean(Imp(X))²,                    (3)

trong đó Imp(X) := Σ(x∈X) G(x). Trong thực tế, chúng ta có thể thêm mất mát cân bằng tải này vào tổng mất mát huấn luyện với trọng số cân bằng wbalance.

MoE Transformer. Để sử dụng MoE cho transformer để xây dựng MoE transformer, một cách tiếp cận được sử dụng rộng rãi là thay thế một số lớp MLP trong non-MoE transformer bằng các lớp MoE [41]. Cụ thể, trong một lớp MoE, các chuyên gia chia sẻ cùng cấu trúc với MLP gốc. Hàm cổng nhận đầu ra từ lớp attention trước đó và định tuyến các biểu diễn cho token đến các chuyên gia khác nhau. Các đầu ra thưa của chuyên gia được kết hợp qua Phương trình 1.

4 Hỗn hợp Chuyên gia Dư

4.1 Động lực

Nghiên cứu của chúng tôi được thúc đẩy bởi việc phân tích cách trọng số của các chuyên gia trong một MoE vision transformer phát triển trong quá trình huấn luyện. Cụ thể, chúng tôi đã huấn luyện một MoE Swin-T 8-chuyên gia [38] trên ImageNet22K [13] trong 90 epoch, trong đó chúng tôi áp dụng MoE cho mỗi khối Swin transformer khác theo nghiên cứu trước [41].

--- TRANG 5 ---
Cho mô hình đã được huấn luyện, chúng tôi tập trung phân tích trọng số của các chuyên gia trong lớp MoE cuối cùng vì: (1) các quyết định định tuyến sâu hơn tương quan với các lớp hình ảnh và thể hiện thông tin ngữ nghĩa phong phú nhất [41]; và (2) các lớp cuối cùng ảnh hưởng nhiều nhất đến hiệu suất phân loại. Trọng số của các chuyên gia tại các epoch khác nhau được trực quan hóa bằng cách chiếu lên mặt phẳng 2D. Để tránh biến dạng phi tuyến trong quá trình chiếu, chúng tôi đã sử dụng phân tích thành phần chính và trực quan hóa 2 thành phần chính hàng đầu như các giá trị trên trục X và Y trong biểu đồ phân tán. Như thể hiện trong Hình 3 (a), chúng tôi quan sát thấy rằng trọng số của các chuyên gia khác nhau được nhóm theo epoch huấn luyện của chúng. Quan trọng hơn, phương sai của mỗi cụm trọng số nhỏ hơn nhiều so với phương sai của các tâm cụm trong quá trình huấn luyện.

Những quan sát này khuyến khích chúng tôi thiết kế một quy trình huấn luyện hiệu quả hơn so với huấn luyện MoE hiện tại. Cụ thể, như thể hiện trong Hình 3 (b), chúng ta có thể phân tích huấn luyện MoE thành hai giai đoạn. Đầu tiên chúng ta huấn luyện các tâm của cụm chuyên gia (đường màu đỏ liền). Thứ hai, chúng ta huấn luyện trọng số dư của các chuyên gia (đường màu xanh đứt nét). So sánh hai giai đoạn này, sự thay đổi trọng số trong giai đoạn thứ hai nhỏ hơn nhiều so với giai đoạn thứ nhất. Do đó, chi phí huấn luyện cần thiết cho giai đoạn thứ hai nhỏ hơn nhiều và tổng chi phí huấn luyện chủ yếu được xác định bởi giai đoạn thứ nhất. Trong giai đoạn thứ nhất, để huấn luyện các tâm cụm trọng số, chúng ta có thể sử dụng một non-MoE transformer thay vì mô hình MoE vì tâm chuyên gia độc lập với đầu vào. Do đó, giai đoạn huấn luyện thứ nhất bản thân nó đã hiệu quả vì chi phí huấn luyện cho một MoE vision transformer cao hơn 1.5× so với các mô hình non-MoE [41]. Trong thực tế, đối với các nhiệm vụ downstream, chúng ta thường có thể bỏ qua giai đoạn huấn luyện thứ nhất và chỉ thực hiện huấn luyện dư bằng cách sử dụng trực tiếp các checkpoint non-MoE vision transformer có sẵn. Do đó, chúng ta có thể huấn luyện một MoE vision transformer lớn một cách hiệu quả.

Hình 3: Trực quan hóa sự phát triển trọng số chuyên gia trong quá trình huấn luyện MoE. Ở đây chúng tôi hiển thị trực quan hóa các chuyên gia trong lớp cuối cùng của một MoE Swin-T 8-chuyên gia. Chúng tôi chiếu trọng số lên mặt phẳng 2D sử dụng phân tích thành phần chính. Mỗi điểm là trọng số của một chuyên gia tại một epoch huấn luyện. Trục X và Y là thành phần chính thứ nhất và thứ hai tương ứng: (a) trong quá trình huấn luyện, trọng số chuyên gia tại các epoch khác nhau được nhóm lại và phương sai của mỗi cụm trọng số (các điểm cùng màu) nhỏ hơn nhiều so với phương sai của các tâm cụm (đường màu xanh liền) trong quá trình huấn luyện. Được thúc đẩy bởi mô hình này, RMoE của chúng tôi trong (b) đầu tiên học các tâm của trọng số chuyên gia (đường màu đỏ liền) và sau đó học trọng số dư của chúng (đường màu xanh đứt nét).

4.2 Công thức

Nói chung, tính toán có điều kiện của một mô hình MoE f có thể được công thức hóa như y = f(x; θ(x)), trong đó x là một điểm dữ liệu đầu vào, y là đầu ra và θ là toàn bộ tập hợp các tham số mô hình. Sử dụng công thức này, việc huấn luyện một mô hình MoE nhằm mục đích tối thiểu hóa mất mát thực nghiệm L:

min_θ Σ(x∈X) L(f(x; θ(x))).                    (4)

trong đó X là tập huấn luyện.

Được thúc đẩy bởi các quan sát trong Phần 4.1, chúng tôi phân tích tính toán có điều kiện θ(x) như một θ₀ độc lập với đầu vào và một dư θᵣ(x) phụ thuộc vào đầu vào:

θ(x) = θ₀ + θᵣ(x).                    (5)

Trong phân tích này, kích thước của trọng số dư θᵣ(x) lớn hơn nhiều so với lõi trọng số θ₀ vì các chuyên gia trong một lớp MoE chia sẻ cùng lõi trọng số và chúng ta thường có 8-32 chuyên gia trong một MoE vision transformer [41]. Ngược lại, trọng số dư có thể được huấn luyện hiệu quả với ít tài nguyên tính toán hơn nhiều, ví dụ như ít dữ liệu huấn luyện và ít epoch huấn luyện hơn.

Sử dụng phân tích này, chúng tôi đề xuất quy trình huấn luyện Hỗn hợp Chuyên gia Dư (RMoE), công thức hóa việc huấn luyện MoE vision transformer như một tối ưu hóa hai cấp với dư θᵣ như biến cấp trên và lõi θ₀ như biến cấp dưới:

min_θᵣ Σ_X L(f(x; θ₀ + θᵣ(x)))
s.t. θ₀ = argmin_θ Σ_X L(f(x; θ)).

4.3 Thiết kế của chúng tôi

Để tận dụng đầy đủ lợi ích của quy trình huấn luyện RMoE, có một số cân nhắc thiết kế thực tế cần thảo luận:

Thiết kế Quy trình Huấn luyện. Quy trình huấn luyện của các mô hình non-MoE và MoE tương tự và có thể được chia thành nhiều giai đoạn. Như thể hiện trong Hình 2 (a) và (d), trong giai đoạn đầu tiên, vision transformer được tiền huấn luyện trên tập dữ liệu upstream quy mô lớn.

Sau đó, có một giai đoạn tinh chỉnh trung gian tùy chọn. Nó để bắc cầu khoảng cách nhiệm vụ hoặc khoảng cách độ phân giải giữa upstream và downstream, ví dụ như BeiT [2].

Cuối cùng, mô hình được tinh chỉnh trên tập dữ liệu downstream, chẳng hạn như phân đoạn ngữ nghĩa hoặc phát hiện đối tượng. So với quy trình huấn luyện non-MoE và MoE, RMoE bắt đầu với huấn luyện non-MoE transformer trên nhiệm vụ upstream và tinh chỉnh hiệu quả non-MoE transformer thành MoE transformer. Như thể hiện trong Hình 2 (b) và (c), trong huấn luyện RMoE, chúng ta có thể tinh chỉnh trung gian mô hình MoE trên tập dữ liệu upstream trong vài epoch hoặc tinh chỉnh trực tiếp mô hình MoE trên tập dữ liệu downstream.

Chúng tôi ký hiệu hai quy trình khác nhau này là RMoE-I (Trung gian) và RMoE-D (Downstream).

Bảo toàn hiệu suất giữa non-MoE và MoE transformer. Trong RMoE, chúng ta cần kế thừa lõi trọng số đã học từ tiền huấn luyện non-MoE upstream để thực hiện học dư.

Để làm điều này, chúng tôi khởi tạo trọng số của mỗi chuyên gia trong một lớp MoE như trọng số đã học trong lớp MLP tương ứng từ tiền huấn luyện non-MoE. Ngoài ra, chúng tôi thêm một nhiễu nhỏ vào mỗi chuyên gia để thoát khỏi cực tiểu cục bộ.

--- TRANG 6 ---
Trong thực tế, chúng tôi thấy rằng việc kế thừa trực tiếp trọng số chuyên gia sẽ gây ra suy giảm hiệu suất. Cụ thể, như trong Phương trình 1, đầu ra của một lớp MoE là tổng có trọng số của hàm cổng G(x) và đầu ra chuyên gia [Ei(x)]: MoE(x) = Σ(i=1 to n) G(x)i Ei(x). Trong RMoE, vì tất cả chuyên gia kế thừa trọng số từ tiền huấn luyện non-MoE và G(x) được hậu xử lý bởi một thao tác Top-K để đảm bảo độ thưa, chúng ta có:

MoE(x) ≈ (Σ(i=1 to n) G(x)i) MLP(x) < MLP(x).    (6)

Do đó, đầu ra của các lớp MoE được thu nhỏ so với các lớp MLP gốc và gây ra suy giảm hiệu suất.

Để giải quyết vấn đề này, chúng tôi đề xuất căn chỉnh đầu ra của các lớp MoE và MLP tương ứng trong khi duy trì dòng gradient trong một lớp MoE:

y = Σ(i=1 to n) StopGrad((1 - G(x)i) Ei(x)) + G(x)i Ei(x),    (7)

trong đó thao tác StopGrad là để dừng gradient của số hạng đã cho. Bằng cách này, khi lan truyền ngược gradient, các chuyên gia vẫn có thể nhận được cập nhật gradient bình thường chỉ cho top-K chuyên gia. Trong RMoE, chúng tôi chỉ áp dụng điều này cho tinh chỉnh trung gian vì: (1) giai đoạn tinh chỉnh trung gian trong RMoE thường ngắn với tốc độ học nhỏ và khó có thể khôi phục hoàn toàn suy giảm hiệu suất; và (2) tinh chỉnh downstream với đầu giải mã mới được thực hiện với tốc độ học lớn và sự sụt giảm hiệu suất sẽ khôi phục nhanh chóng.

Lựa chọn Lớp cho các lớp MoE. Như được giới thiệu trong Phần 3, các lớp MoE là sự khác biệt chính giữa MoE và non-MoE transformer. Do đó, để tinh chỉnh một non-MoE transformer được tiền huấn luyện thành MoE transformer, chúng ta cần thay thế một số lớp MLP bằng các lớp MoE [18, 41]. Để tránh overfitting tiềm năng, chúng tôi chỉ chọn các lớp quan trọng nhất đóng góp nhiều nhất cho việc huấn luyện mạng.

Được lấy cảm hứng từ Firefly Splitting [55], trong RMoE, chúng tôi chọn lớp có thể giảm tối đa hàm mất mát theo cách từng lớp. Đầu tiên, chúng tôi over-grow non-MoE transformer bằng cách thay thế tất cả các lớp MLP bằng các lớp MoE. Sau khi over-growing, chúng tôi tính toán sự giảm mất mát. Cụ thể, cho f đại diện cho một vision transformer với L-lớp. Do đó, Lt(fnon-MoE) là mất mát huấn luyện cho non-MoE vision transformer được tiền huấn luyện và Lt(fRMoE; θr) đại diện cho mất mát sau khi đưa các lớp MoE vào mô hình non-MoE trong huấn luyện RMoE, với trọng số dư θr.

Như chúng tôi đã đề cập trước đó, chúng tôi thêm một nhiễu nhỏ trên trọng số của MLP để khởi tạo trọng số chuyên gia, nó tương đương với khởi tạo θr ≈ ε, trong đó ε là một giá trị đủ nhỏ chỉ làm nhiễu mạng và đầu ra trong một phạm vi nhỏ. Do đó, sự giảm mất mát có thể được phân tách qua xấp xỉ Taylor như:

Lt(fRMoE; θr) = Lt(fnon-MoE) + Σ(l=1 to L) ∂/∂θr^l sl + O(θr²).    (8)

sl ≈ θr^l ∇θr Lt(fRMoE; θr).

Tiếp theo, chúng tôi tìm N lớp quan trọng nhất để giảm mất mát. Chúng tôi đạt được điều này bằng cách đầu tiên tối ưu hóa θr ban đầu trong vài bước gradient descent và vì trọng số dư θr được khởi tạo với một yếu tố đủ nhỏ:

--- TRANG 7 ---
θ̂r = argmin_θr {Σ(l=1 to L) ∂/∂θr^l sl} s.t. ||θr||₀ ≤ N,    (9)

trong đó ||θr||₀ := Σ(l=1 to L) I(θr^l ≠ 0). Để chọn các lớp tốt nhất để giảm tối đa hàm mất mát, giải pháp tối ưu là chọn lớp có N độ lớn gradient lớn nhất |sl|. Trong thực tế, chúng tôi muốn có được một mô hình được mở rộng quy mô tổng quát một lần cho tất cả các nhiệm vụ downstream, vậy nên chúng tôi tính toán độ lớn gradient trên tất cả các nhiệm vụ downstream mục tiêu và chọn N lớp có tổng điểm cao nhất trong tất cả các nhiệm vụ.

5 Thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu quả và hiệu suất của RMoE.

Chúng tôi đầu tiên cung cấp một so sánh toàn diện về các quy trình huấn luyện khác nhau, tức là non-MoE, RMoE và MoE. Sau đó, để hiển thị khả năng của RMoE trong việc huấn luyện vision transformer lớn, chúng tôi sử dụng RMoE để huấn luyện các mô hình sử dụng backbone Swin-L và BeiT-L [2]. Cuối cùng, chúng tôi thực hiện một loạt nghiên cứu ablation để so sánh các lựa chọn thiết kế khác nhau của RMoE.

Bảng 1: So sánh toàn diện giữa các quy trình huấn luyện khác nhau bao gồm non-MoE, MoE, RMoE-I và RMoE-D trên nhiệm vụ phân đoạn ADE20K và phát hiện đối tượng MS-COCO. RMoE-I (1k) đại diện cho tinh chỉnh trung gian mô hình trên ImageNet 1k thay vì ImageNet 22k. Non-MoE đại diện cho việc sử dụng transformer gốc làm backbone. GPU Days đo tổng thời gian huấn luyện trong một GPU Nvidia Tesla V100-32GB. 'Scratch' đại diện cho quy trình được thực hiện đầy đủ từ tiền huấn luyện upstream đến tinh chỉnh downstream. 'Pretrained' có nghĩa là tải từ checkpoint upstream non-MoE được tiền huấn luyện sẵn có, thường có sẵn trong cộng đồng thị giác máy tính. Lưu ý rằng Swin-T và CvT-13 có kích thước batch khác nhau trong quá trình huấn luyện upstream, vì vậy GPU Days không đo mối quan hệ tốc độ giữa hai backbone khác nhau.

--- TRANG 8 ---
5.1 So sánh các Quy trình Huấn luyện khác nhau

Chúng tôi đầu tiên so sánh các quy trình huấn luyện khác nhau trong Hình 2, tức là non-MoE, MoE, RMoE-I và RMoE-D để chứng minh hiệu quả của RMoE. Trong mỗi quy trình huấn luyện, chúng tôi sử dụng phân loại hình ảnh trên ImageNet22k [13] như tiền huấn luyện upstream. Chúng tôi áp dụng hai nhiệm vụ downstream đại diện, tức là phân đoạn ngữ nghĩa và phát hiện đối tượng. Hiệu suất mô hình trên phân đoạn ngữ nghĩa và phát hiện đối tượng được đánh giá trên ADE20K [63] và MS-COCO [37] tương ứng. Đối với backbone, chúng tôi chọn hai vision transformer đại diện, Swin-T và CvT-13. Đối với đầu giải mã, chúng tôi sử dụng UperNet [56] và RetinaNet [36] cho các nhiệm vụ phân đoạn và phát hiện.

Cài đặt Huấn luyện. Đối với tiền huấn luyện upstream, chúng tôi tuân theo chiến lược huấn luyện trong các bài báo gốc [38, 54]. Cụ thể, tất cả các nhiệm vụ upstream được huấn luyện trong 90 epoch với độ phân giải đầu vào 224×224. Chúng tôi đặt kích thước batch là 1024 cho Swin-T và 2048 cho CvT-13 theo cài đặt gốc và tối ưu hóa chúng sử dụng bộ tối ưu AdamW [39] với tốc độ học ban đầu 0.001 cho Swin-T và 0.01 cho CvT-13. Một scheduler suy giảm học cosine được sử dụng cùng với việc huấn luyện. Weight decay là 0.05.

Đối với giai đoạn tinh chỉnh trung gian của RMoE-I, chúng tôi giảm epoch huấn luyện mô hình non-MoE còn 5 và tinh chỉnh mô hình MoE trên ImageNet22K thêm 5 epoch để căn chỉnh tổng số epoch với các cài đặt khác. Tốc độ học ban đầu được đặt là 0.0001 và suy giảm tốc độ học cosine được sử dụng. Các cài đặt khác giống như huấn luyện upstream. Chúng tôi cũng áp dụng tinh chỉnh trung gian trên ImageNet 1k trong 30 epoch và độ phân giải 384×384 cho một thiết lập khả thi hơn. Chúng tôi đánh dấu nó là RMoE-I (1k) trong Bảng 1. Các cài đặt trong giai đoạn tinh chỉnh downstream giống với những cài đặt trong Swin Transformer cho cả Swin-T và CvT-13 trên nhiệm vụ phân đoạn ngữ nghĩa ngoại trừ các bước huấn luyện dài hơn 160k → 200k để hội tụ đầy đủ cho RMoE. Chúng tôi áp dụng cùng chiến lược huấn luyện cho tất cả các mô hình trên ADE20K. Đối với phát hiện đối tượng trên MS-COCO, chúng tôi huấn luyện tất cả các mô hình sử dụng bộ tối ưu AdamW với tốc độ học 0.0001 và lịch trình 1× thông thường mà không có augmentation đa tỷ lệ.

Cài đặt MoE. Đối với tất cả các mô hình MoE, chúng tôi sử dụng 8 chuyên gia MLP với cổng switch [18], có nghĩa là mỗi token chỉ được định tuyến đến một chuyên gia trong một lớp MoE. Đối với huấn luyện MoE, chúng tôi tuân theo các cài đặt của quy trình huấn luyện MoE tiên tiến [41].

Đặc biệt, đối với CvT-13, chúng tôi thêm các lớp MoE sau giai đoạn 2 để ổn định việc huấn luyện.

Đối với huấn luyện RMoE, chúng tôi thêm một nhiễu ngẫu nhiên ε = 0.01 trên mỗi chuyên gia sau khi khởi tạo từ mô hình non-MoE để khuyến khích đa dạng.

Chúng tôi tính toán điểm grow dựa trên gradient và chọn 3 lớp có điểm cao nhất để áp dụng MoE. Các lựa chọn lớp chi tiết có thể được tìm thấy trong Bảng 2. Đối với cả huấn luyện MoE và RMoE, chúng tôi áp dụng mất mát cân bằng tải trong Phần 3 với trọng số wbalance = 0.01 trong quá trình huấn luyện upstream và tinh chỉnh trung gian, trọng số wbalance = 0.0001 được thêm vào trong quá trình tinh chỉnh downstream.

Kết quả Thí nghiệm. Như chúng ta có thể thấy trong Bảng 1, huấn luyện MoE gốc có được hiệu suất tốt nhất trên tất cả các nhiệm vụ và mô hình nhưng với chi phí huấn luyện cao nhất. Hiệu suất của RMoE-I tương đương với MoE với việc tiết kiệm khoảng 30% chi phí huấn luyện (huấn luyện từ đầu). RMoE-D hoạt động hơi kém hơn RMoE-I, nhưng cả hai đều vượt trội hơn huấn luyện non-MoE với sự gia tăng chi phí huấn luyện nhỏ. Trên backbone Swin-T và CvT-13, so với baseline non-MoE, RMoE-I có được +1.1 / 0.9 mIoU. Nó cũng có được +1.4 / 1.6 AP trên nhiệm vụ phát hiện. RMoE-D, trực tiếp sử dụng các chuyên gia trên các nhiệm vụ downstream, vẫn có thể có được +0.7 / 0.5 mIoU và +1.0 / 1.3 AP. Trong thực tế, chúng ta có thể tận dụng các checkpoint được chia sẻ của Swin-T và CVT-13 để giảm thêm chi phí huấn luyện. Trong trường hợp đó, chúng ta có thể sử dụng RMoE để mở rộng quy mô vision transformer cho

--- TRANG 9 ---
các nhiệm vụ downstream với ít hơn 10% chi phí huấn luyện của huấn luyện MoE.

Bảng 2: Lựa chọn lớp trong huấn luyện RMoE cho các nhiệm vụ downstream. Ví dụ, trong Swin-T, chúng tôi chọn lớp thứ 2 trong giai đoạn 2,4 và lớp thứ 6 trong giai đoạn 3 làm lớp MoE. Lưu ý rằng CvT chỉ có 3 giai đoạn, và BeiT không có giai đoạn. Vì vậy để căn chỉnh, chúng tôi biểu thị CvT13 sử dụng Giai đoạn 1,2,3 và BeiT-L chỉ sử dụng Giai đoạn 1.

Phân tích Khoảng cách giữa RMoE-I và RMoE-D. Mất mát cân bằng được tìm thấy là quan trọng để giải thích sự khác biệt hiệu suất giữa RMoE-I và RMoE-D. Trong giai đoạn tinh chỉnh của quy trình huấn luyện MoE, mất mát cân bằng tải thường không được thêm vào. Ví dụ, trong V-MoE, họ tuyên bố rằng router được huấn luyện tốt có thể cân bằng mỗi chuyên gia tốt mà không cần mất mát cân bằng. Tuy nhiên, trong RMoE-D, chúng tôi thấy rằng đầu giải mã mới được giới thiệu và tốc độ học ban đầu lớn trong các nhiệm vụ downstream có thể phá vỡ sự cân bằng tải giữa các chuyên gia. Để chứng minh điều này, chúng tôi so sánh các quá trình huấn luyện của MoE, RMoE-D và RMoE-I có/không có mất mát cân bằng trong giai đoạn tinh chỉnh downstream. Như thể hiện trong Hình 4, chúng tôi thấy rằng không có mất mát cân bằng, tất cả các phương pháp, bao gồm MoE, đều dẫn đến cân bằng tải kém. Khi chúng tôi thêm mất mát cân bằng với trọng số nhỏ wbalance = 0.0001, cân bằng tải trong RMoE-D vẫn kém. RMoE-I, được hưởng lợi từ tinh chỉnh trung gian, có thể giảm mất mát cân bằng một cách mượt mà. Một trọng số lớn hơn wbalance = 0.1 có thể làm cho RMoE-D cân bằng nhưng nó cũng sẽ làm hại hiệu suất.

Chuyên môn hóa Chuyên gia. Để phân tích cách các chuyên gia phân phối các hình ảnh khác nhau, chúng tôi trực quan hóa có bao nhiêu hình ảnh của một lớp ImageNet cho trước sử dụng mỗi chuyên gia trong Hình 5. Cụ thể, mỗi hàng là một lớp ImageNet và mỗi cột là một chuyên gia. Màu sắc trong một ô biểu thị trọng số định tuyến trung bình (G(x) lớn nhất trong Phương trình 1) cho tất cả các token trong một lớp cụ thể cho một chuyên gia. Do đó, màu tối hơn trong một ô có nghĩa là chuyên gia chuyên môn hóa để xử lý các hình ảnh trong lớp đó. Chúng tôi so sánh cả huấn luyện RMoE và MoE sử dụng cài đặt tinh chỉnh trung gian RMoE-I (1k) và chọn lớp MoE cuối cùng để phân tích. Chúng tôi thấy rằng các chuyên gia trong cả huấn luyện RMoE và MoE đều chuyên môn hóa qua các lớp trong ImageNet. Nó phù hợp với quan sát của chúng tôi trong Phần 4.1 rằng, mặc dù RMoE sử dụng ít tài nguyên tính toán hơn nhiều để huấn luyện các chuyên gia, chuyên môn hóa chuyên gia được đạt được bởi cả huấn luyện RMoE và MoE.

5.2 Sử dụng RMoE để Huấn luyện Transformer Lớn

Chúng tôi tiếp tục chứng minh khả năng của RMoE để mở rộng quy mô hơn nữa các vision transformer lớn, bao gồm Swin-L và BeiT-L, vì chúng đạt được kết quả tiên tiến trên các nhiệm vụ downstream.

Cụ thể, chúng tôi áp dụng RMoE cho Swin-L với UperNet [56] và Maskformer [7], BeiT-L với UperNet cho phân đoạn ADE20K và Swin-L với HTC++ [4, 38] trên phát hiện đối tượng MS-COCO.

Thiết lập Thí nghiệm. Các siêu tham số huấn luyện giống với những cài đặt trong giai đoạn tinh chỉnh của Swin-L, Maskformer và BeiT-L. Đối với đầu giải mã HTC++, chúng tôi giảm lịch trình từ 6× xuống 3× vì mô hình được mở rộng quy mô có tốc độ hội tụ tốt hơn. Đối với tinh chỉnh trung gian, trọng số của các chuyên gia trực tiếp

--- TRANG 10 ---
Hình 4: Mất mát cân bằng trên các nhiệm vụ phân đoạn ADE20K với các trọng số mất mát khác nhau. Weight=0 biểu thị loại bỏ mất mát cân bằng. Cả RMoE-I và MoE đều có thể có được sự cân bằng tốt dưới một trọng số nhỏ, nhưng RMoE-D chỉ có thể cân bằng khi một trọng số mất mát cân bằng lớn được sử dụng.

Hình 5: Trực quan hóa chuyên môn hóa chuyên gia của huấn luyện RMoE và MoE. Một màu tối hơn trong một ô cho thấy rằng chuyên gia chuyên môn hóa hơn để xử lý các hình ảnh trong lớp đó của ImageNet. Chúng tôi sắp xếp lại các lớp ImageNet riêng biệt cho RMoE và MoE để có được trực quan hóa tốt hơn.

kế thừa từ checkpoint được tiền huấn luyện chính thức¹ ². Các cài đặt còn lại giữ nguyên như trong Phần 5.1. Đối với Swin-L, chúng tôi chọn 6 lớp và đối với BeiT-L chúng tôi chọn 8 lớp làm lớp MoE sử dụng điểm grow dựa trên gradient. Các lựa chọn lớp chi tiết được hiển thị trong Bảng 2. Các cài đặt khác liên quan đến RMoE giống như Phần 5.1.

Kết quả Thí nghiệm. Như chúng ta có thể thấy từ Bảng 3, RMoE-I có thể mở rộng quy mô thêm các backbone lên khả năng mô hình lớn hơn, với cải thiện hiệu suất khoảng 1.0 trên phân đoạn Swin-L. Chúng tôi cũng cải thiện kết quả phân đoạn BeiT-L bằng 0.4. RMoE-D cũng có thể có được cải thiện +0.7 so với mô hình non-MoE với chi phí gần như miễn phí trên mô hình Swin-L với Head khác nhau. Đối với nhiệm vụ phát hiện đối tượng, kết quả được hiển thị trong Bảng 4 cho thấy rằng RMoE-I và RMoE-D có được +0.6 / 0.4 AP cao hơn so với baseline mô hình non-MoE chỉ với một nửa epoch tinh chỉnh.

¹https://github.com/microsoft/Swin-Transformer
²https://github.com/microsoft/unilm/tree/master/beit

--- TRANG 11 ---
Bảng 3: Kết quả ADE-20K trên các backbone lớn và đầu giải mã khác nhau. †Kết quả của chúng tôi trên MaskFormer và BeiT-L UperNet không có TTA thấp hơn một chút so với kết quả được báo cáo trong các bài báo gốc. Tuy nhiên, kết quả TTA giữ nguyên.

Bảng 4: Kết quả Non-MoE và RMoE của phát hiện đối tượng MS-COCO trên Swin-L và HTC++.

5.3 Tốc độ Suy luận và FLOPs

Một tính chất hấp dẫn cho MoE transformer là nó mở rộng khả năng mô hình (số lượng tham số) bằng cách chỉ giới thiệu thời gian suy luận và FLOPs bổ sung tối thiểu so với non-MoE transformer. RMoE cũng có tính chất này. Như thể hiện trong Bảng 5, chúng tôi phân tích thời gian suy luận và FLOPs của Swin-T và Swin-L trên các nhiệm vụ phân đoạn ADE20K sử dụng đầu giải mã UperNet. Chúng tôi cho thấy rằng RMoE có cải thiện hiệu suất rõ ràng nhưng chỉ giới thiệu thời gian suy luận bổ sung tối thiểu, điều này quan trọng trong việc triển khai mô hình học máy.

5.4 Nghiên cứu Ablation

Chúng tôi thực hiện một loạt nghiên cứu ablation cho RMoE để so sánh các lựa chọn thiết kế khác nhau. Các nghiên cứu ablation được thực hiện trên Swin-T sử dụng UperNet trên phân đoạn ADE20K và RetinaNet trên phát hiện đối tượng MS-COCO. Chúng tôi sử dụng RMoE-I làm baseline do sự cân bằng tốt hơn giữa hiệu suất và chi phí huấn luyện.

Lựa chọn lớp MoE. Là một trong những yếu tố chính, việc lựa chọn lớp MoE trong transformer ảnh hưởng đến hiệu suất cuối cùng và kích thước mô hình. Chúng tôi báo cáo các lựa chọn lớp khác nhau trong Bảng 6. Trong những cài đặt đó, Last-2 và Every-2 được giới thiệu trong [41]. Last-2 đặt MoE trong 2 khối chẵn cuối cùng và Every-2 đặt MoE trong mỗi lớp khác. Chúng tôi cũng giới thiệu Every-Last, mở rộng lớp trong lớp cuối cùng của mỗi giai đoạn trong Swin-T Transformer với trực giác rằng lớp gần đầu giải mã hơn quan trọng hơn. Bảng 6 hiển thị kết quả cho các chiến lược khác nhau. Chúng tôi thấy rằng việc lựa chọn dựa trên điểm gradient của chúng tôi hoạt động tốt hơn Last-2 và Every-2 trong khi giữ nguyên hiệu suất so với Every-Last. Đối với các nhiệm vụ downstream,

--- TRANG 12 ---
Bảng 5: So sánh mIoU với thời gian suy luận và FLOPs giữa non-MoE, RMoE-I trên ADE20K với đầu giải mã UperNet. Chúng tôi kiểm tra tốc độ suy luận trên một RTX3090.

Bảng 6: Nghiên cứu ablation về các lựa chọn lớp MoE khác nhau.

Bảng 7: Nghiên cứu ablation về cổng top-k khác nhau và số lượng chuyên gia n.

tính năng của lớp cuối cùng của mỗi giai đoạn sẽ được đưa vào đầu giải mã, vậy nên Last-2 không còn là lựa chọn tốt. Nó cũng giải thích hiệu suất tốt của Every-Last. Bên cạnh đó, chúng tôi tối ưu hóa thêm Every-last bằng cách giảm một lớp, cho thấy ưu điểm của phương pháp dựa trên điểm. Trên các backbone lớn, Every-Last chỉ có thể sử dụng bốn lớp MoE, có thể không đủ. Đối với Every-2, hiệu suất giảm nhẹ so với của chúng tôi. Chúng tôi nghĩ rằng có thể là do Every-2 cần ba lớp MoE bổ sung trong khi một số lớp trung gian không thể đóng góp nhiều cho hiệu suất.

Số lượng Chuyên gia và Cổng Top-K. Là các siêu tham số phổ biến, số lượng chuyên gia n và k của thao tác top-k trong một cổng thường được xem xét trong thí nghiệm MoE. Trong Bảng 7 chúng tôi so sánh cài đặt của chúng tôi với các cài đặt khác, tức là n = 4, 16 và k = 2. Chúng ta thấy rằng k = 2 cải thiện hiệu suất trên cả hai nhiệm vụ downstream trong khi tăng thời gian huấn luyện 10% và nhiều FLOPs hơn một chút. Với ít chuyên gia hơn, n = 4 giảm hiệu suất được mang lại bởi việc mở rộng quy mô với RMoE-I khoảng 0.3 trên ADE20K và MS-COCO. n = 16 không cải thiện hiệu suất, có thể khác với các kết luận trong tài liệu MoE khác. Chúng tôi suy đoán rằng điều này chủ yếu do mất mát cân bằng tải được sử dụng [41]. Nó có thể làm hại hiệu suất của mỗi chuyên gia trong giai đoạn tinh chỉnh. Một quá trình tinh chỉnh chuyên dụng hơn với mất mát cân bằng được cải thiện cần được thiết kế khi số lượng chuyên gia tăng, ví dụ như 128 trong [18] hoặc 32 trong [41].

Stop Gradient. Chúng tôi áp dụng kỹ thuật stop gradient trong Phương trình 7 cho tinh chỉnh trung gian RMoE-I để tránh sụt giảm hiệu suất. Như thể hiện trong Bảng 8, hiệu suất giảm nhiều mà không có stop gradient trong tinh chỉnh trung gian.

Quy mô Khởi tạo Nhiễu. Để khuyến khích chuyên môn hóa cho mỗi chuyên gia sau khi áp dụng RMoE, chúng tôi áp dụng một mức độ nhiễu nhất định trên mỗi trọng số được sao chép. Bảng 9 hiển thị cách mức độ nhiễu khác nhau ảnh hưởng đến kết quả cuối cùng.

6 Tốc độ Suy luận và FLOPs

Một tính chất hấp dẫn cho MoE transformer là nó mở rộng khả năng mô hình (số lượng tham số) bằng cách chỉ giới thiệu thời gian suy luận và FLOPs bổ sung tối thiểu so với non-MoE transformer. RMoE cũng có tính chất này. Như thể hiện trong Bảng 5, chúng tôi phân tích thời gian suy luận và FLOPs của Swin-

--- TRANG 13 ---
Bảng 8: Nghiên cứu ablation về kỹ thuật stop gradient.

Bảng 9: Mức độ nhiễu khác nhau so với hiệu suất ADE20k.

T và Swin-L trên các nhiệm vụ phân đoạn ADE20K sử dụng đầu giải mã UperNet. Chúng tôi cho thấy rằng RMoE có cải thiện hiệu suất rõ ràng nhưng chỉ giới thiệu thời gian suy luận bổ sung tối thiểu, điều này quan trọng trong việc triển khai mô hình học máy.

7 Cài đặt Khả thi cho RMoE

Từ các nghiên cứu ablation, chúng tôi kết luận một baseline hiệu quả nhưng đơn giản cho các thực hành viên, tức là chiến lược Every-Last để chọn các lớp MoE, cổng top-1, và 8 chuyên gia. Theo Bảng 5 trong bài báo chính, so với RMoE-I có hiệu suất tốt nhất với firefly splitting, cấu hình này có được cùng hiệu suất với chỉ 1M tham số bổ sung trên backbone.

8 Kết luận

Trong bài báo này, chúng tôi đề xuất một quy trình huấn luyện hiệu quả cho MoE vision transformer, RMoE. So với huấn luyện MoE truyền thống gặp phải chi phí tính toán cao, chúng tôi huấn luyện các phần dư khác nhau cho mỗi chuyên gia bằng cách khởi tạo trọng số từ mô hình non-MoE được tiền huấn luyện. Điều này được thúc đẩy bởi việc phân tích quỹ đạo huấn luyện của trọng số, trong đó chúng tôi thấy rằng trọng số chuyên gia có thể được phân tích như một lõi trọng số và một phần dư. Chúng tôi có các nghiên cứu toàn diện trên các transformer khác nhau, bao gồm Swin Transformer, CvT và BeiT, trên các nhiệm vụ downstream khác nhau, bao gồm phân đoạn ADE20K và phát hiện đối tượng COCO. Chúng tôi cho thấy rằng RMoE của chúng tôi có thể cải thiện hiệu suất của các vision transformer non-MoE thông thường với chi phí bổ sung nhỏ.

--- TRANG 14 ---
Tài liệu tham khảo

[1] Abbas, A., Andreopoulos, Y.: Biased mixtures of experts: Enabling computer vision inference under data transfer limitations. IEEE Transactions on Image Processing 29, 7656–7667 (2020)

[2] Bao, H., Dong, L., Wei, F.: Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254 (2021)

[3] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020)

[4] Chen, K., Pang, J., Wang, J., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Shi, J., Ouyang, W., et al.: Hybrid task cascade for instance segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 4974–4983 (2019)

[5] Chen, K., Wang, J., Pang, J., Cao, Y., Xiong, Y., Li, X., Sun, S., Feng, W., Liu, Z., Xu, J., Zhang, Z., Cheng, D., Zhu, C., Cheng, T., Zhao, Q., Li, B., Lu, X., Zhu, R., Wu, Y., Dai, J., Wang, J., Shi, J., Ouyang, W., Loy, C.C., Lin, D.: MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155 (2019)

[6] Chen, K., Xu, L., Chi, H.: Improved learning algorithms for mixture of experts in multiclass classification. Neural networks 12(9), 1229–1252 (1999)

[7] Cheng, B., Schwing, A.G., Kirillov, A.: Per-pixel classification is not all you need for semantic segmentation. arXiv (2021)

[8] Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., Shen, C.: Twins: Revisiting spatial attention design in vision transformers. arXiv preprint arXiv:2104.13840 (2021)

[9] Chu, X., Zhang, B., Tian, Z., Wei, X., Xia, H.: Do we really need explicit position encodings for vision transformers? arXiv e-prints pp. arXiv–2102 (2021)

[10] Collobert, R., Bengio, S., Bengio, Y.: A parallel mixture of svms for very large scale problems. Advances in Neural Information Processing Systems 14(2001)

[11] Contributors, M.: Mmsegmentation, an open source semantic segmentation toolbox. https://github.com/open-mmlab/mmsegmentation (2020)

[12] Deisenroth, M., Ng, J.W.: Distributed gaussian processes. In: International Conference on Machine Learning. pp. 1481–1490. PMLR (2015)

[13] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255. Ieee (2009)

[14] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., Guo, B.: Cswin transformer: A general vision transformer backbone with cross-shaped windows. arXiv preprint arXiv:2107.00652 (2021)

--- TRANG 15 ---
[15] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)

[16] Eigen, D., Ranzato, M., Sutskever, I.: Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314 (2013)

[17] El-Nouby, A., Neverova, N., Laptev, I., Jégou, H.: Training vision transformers for image retrieval. arXiv preprint arXiv:2102.05644 (2021)

[18] Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 (2021)

[19] Gross, S., Ranzato, M., Szlam, A.: Hard mixtures of experts for large scale weakly supervised vision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6865–6873 (2017)

[20] Guzman-Rivera, A., Batra, D., Kohli, P.: Multiple choice learning: Learning to produce multiple structured outputs. Advances in neural information processing systems 25(2012)

[21] Han, K., Xiao, A., Wu, E., Guo, J., Xu, C., Wang, Y.: Transformer in transformer. arXiv preprint arXiv:2103.00112 (2021)

[22] Hansen, J.V.: Combining predictors: comparison of five meta machine learning methods. Information Sciences 119(1-2), 91–105 (1999)

[23] He, J., Qiu, J., Zeng, A., Yang, Z., Zhai, J., Tang, J.: Fastmoe: A fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262 (2021)

[24] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778 (2016)

[25] He, S., Luo, H., Wang, P., Wang, F., Li, H., Jiang, W.: Transreid: Transformer-based object re-identification. arXiv preprint arXiv:2102.04378 (2021)

[26] Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)

[27] Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 7132–7141 (2018)

[28] Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4700–4708 (2017)

[29] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local experts. Neural computation 3(1), 79–87 (1991)

[30] Jiang, Z., Hou, Q., Yuan, L., Zhou, D., Jin, X., Wang, A., Feng, J.: Token labeling: Training a 85.5% top-1 accuracy vision transformer with 56m parameters on imagenet. arXiv preprint arXiv:2104.10858 (2021)

--- TRANG 16 ---
[31] Jordan, M.I., Jacobs, R.A.: Hierarchical mixtures of experts and the em algorithm. Neural computation 6(2), 181–214 (1994)

[32] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems 25, 1097–1105 (2012)

[33] Lee, K., Hwang, C., Park, K., Shin, J.: Confident multiple choice learning. In: International Conference on Machine Learning. pp. 2014–2023. PMLR (2017)

[34] Lee, S., Purushwalkam Shiva Prakash, S., Cogswell, M., Ranjan, V., Crandall, D., Batra, D.: Stochastic multiple choice learning for training diverse deep ensembles. Advances in Neural Information Processing Systems 29(2016)

[35] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., Chen, Z.: Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 (2020)

[36] Lin, T.Y., Goyal, P., Girshick, R., He, K., Dollár, P.: Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. pp. 2980–2988 (2017)

[37] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision. pp. 740–755. Springer (2014)

[38] Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021)

[39] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019)

[40] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019)

[41] Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Pinto, A.S., Keysers, D., Houlsby, N.: Scaling vision with sparse mixture of experts. arXiv preprint arXiv:2106.05974 (2021)

[42] Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4510–4520 (2018)

[43] Shahbaba, B., Neal, R.: Nonlinear models using dirichlet process mixtures. Journal of Machine Learning Research 10(8) (2009)

[44] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean, J.: Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 (2017)

[45] Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)

[46] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 1–9 (2015)

--- TRANG 17 ---
[47] Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International Conference on Machine Learning. pp. 6105–6114. PMLR (2019)

[48] Theis, L., Bethge, M.: Generative image modeling using spatial lstms. Advances in neural information processing systems 28(2015)

[49] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877 (2020)

[50] Touvron, H., Cord, M., Sablayrolles, A., Synnaeve, G., Jégou, H.: Going deeper with image transformers. arXiv preprint arXiv:2103.17239 (2021)

[51] Tresp, V.: Mixtures of gaussian processes. Advances in neural information processing systems 13 (2000)

[52] Wang, W., Xie, E., Li, X., Fan, D.P., Song, K., Liang, D., Lu, T., Luo, P., Shao, L.: Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122 (2021)

[53] Wang, X., Yu, F., Dunlap, L., Ma, Y.A., Wang, R., Mirhoseini, A., Darrell, T., Gonzalez, J.E.: Deep mixture of experts via shallow embedding. In: Uncertainty in Artificial Intelligence. pp. 552–562. PMLR (2020)

[54] Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.: Cvt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808 (2021)

[55] Wu, L., Liu, B., Stone, P., Liu, Q.: Firefly neural architecture descent: a general approach for growing neural networks. arXiv preprint arXiv:2102.08574 (2021)

[56] Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Unified perceptual parsing for scene understanding. In: Proceedings of the European Conference on Computer Vision (ECCV). pp. 418–434 (2018)

[57] Xu, W., Xu, Y., Chang, T., Tu, Z.: Co-scale conv-attentional image transformers. arXiv preprint arXiv:2104.06399 (2021)

[58] Yang, B., Bender, G., Le, Q.V., Ngiam, J.: Condconv: Conditionally parameterized convolutions for efficient inference. arXiv preprint arXiv:1904.04971 (2019)

[59] Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., Wu, W.: Incorporating convolution designs into visual transformers. arXiv preprint arXiv:2103.11816 (2021)

[60] Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan, S.: Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986 (2021)

[61] Yuksel, S.E., Wilson, J.N., Gader, P.D.: Twenty years of mixture of experts. IEEE transactions on neural networks and learning systems 23(8), 1177–1193 (2012)

[62] Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L.: Scaling vision transformers. arXiv preprint arXiv:2106.04560 (2021)

[63] Zhou, B., Zhao, H., Puig, X., Xiao, T., Fidler, S., Barriuso, A., Torralba, A.: Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision 127(3), 302–321 (2019)

--- TRANG 18 ---
Thuật toán 1 Chèn RMoE vào vision transformer cho nhiệm vụ downstream
1: Đầu vào: một vision transformer L-lớp được huấn luyện tốt f, các hàm mất mát trên m nhiệm vụ downstream đã cho Lt(f); t = 1...m, số lớp tối đa để mở rộng Lmax, số chuyên gia để mở rộng n. Giai đoạn để mở rộng (Trung gian, Downstream).
2: Bắt đầu trước giai đoạn đã cho.
3: Với t = 1...m, tính toán điểm cho mỗi lớp sử dụng Phương trình 8 và tổng chúng lại cùng các nhiệm vụ làm điểm cuối cùng sl = Σ(i=1 to m) |sl|i trong đó |sl|i là điểm lớp l cho nhiệm vụ i.
4: Chọn điểm Lmax hàng đầu, áp dụng RMoE để mở rộng lớp MLP thành RMoE n-chuyên gia tại lớp tương ứng với điểm cao nhất.
5: Tinh chỉnh mô hình RMoE được mở rộng trên các nhiệm vụ downstream.

A Chi tiết cho quy trình Firefly Splitting

Cho một non-MoE vision transformer đã được huấn luyện, chúng tôi sử dụng cách tiếp cận Firefly để chọn lớp MLP nào được thay thế bằng các lớp MoE.

Cụ thể, nó nhằm mục đích tối thiểu hóa sự giảm mất mát giữa mô hình non-MoE fMLP và mô hình RMoE fRMoE;θr,g:

max_{θr,g} {L(fMLP) - L(fRMoE;θr,g)},    (10)

trong đó θr là trọng số dư và g là các tham số cổng. Bài toán tối ưu hóa được giải quyết bằng gradient descent. Để tăng tốc tối ưu hóa, θr được khởi tạo như θr ≈ ε, trong đó ε = 0.001 trong các thí nghiệm của chúng tôi. Khởi tạo như vậy kết hợp với kỹ thuật stop gradient đảm bảo hiệu suất sẽ không giảm quá nhiều ban đầu, ví dụ như L(fMLP) = L(fRMoE;θr,g) khi ε = 0.

Trong thực tế, để tiết kiệm chi phí tính toán và tránh overfitting, chúng tôi không thêm các lớp MoE trong tất cả các khối transformer, mà chọn một tập con các lớp {l} với trọng số dư {θr^l} ⊆ θr, góp phần nhiều nhất vào Phương trình 10.

Để làm điều này, chúng tôi theo Firefly và sử dụng tối ưu hóa hai bước để tìm một giải pháp thưa cho Phương trình 10:

Bước một. Tạo ra một mô hình MoE over-grown bằng cách thay thế tất cả các lớp MLP bằng các lớp MoE và tối ưu hóa θr, g sử dụng gradient descent trong một vài bước.

Bước hai. Cố định g, tái tối ưu hóa θr bằng cách chọn các ứng cử viên tốt nhất θr^l. Để đạt được điều này, chúng tôi sử dụng khai triển Taylor và chọn tập ứng cử viên lớn nhất theo Phương trình 8 và Phương trình 9. Chúng tôi tóm tắt thuật toán grow dựa trên gradient RMoE tổng thể trong Thuật toán 1.

B Trực quan hóa Chuyên gia

Để hiểu rõ hơn những gì các chuyên gia học trong huấn luyện RMoE, chúng tôi trình bày trực quan hóa định tuyến patch, đại diện cho cách mỗi patch trong chuỗi token đầu vào được định tuyến bởi cổng. Trong một mô hình MoE, mỗi chuyên gia đang cố gắng học các hàm khác nhau để xử lý các loại token đầu vào khác nhau cho các hình ảnh đầu vào. Do đó, chúng ta có thể sử dụng trực quan hóa này để so sánh định tính mô hình được huấn luyện với RMoE và MoE.

Để trực quan hóa định tuyến patch, chúng tôi ghi lại kết quả định tuyến patch cho mỗi chuyên gia và reshape chuỗi token patch đầu vào thành hình dạng giống hình ảnh của giai đoạn transformer hiện tại. Bằng cách này, chúng ta có thể trực quan hóa trực tiếp điều kiện định tuyến patch theo kiểu hình ảnh như thể hiện trong Hình 6. Trong hình, ô vuông trắng

--- TRANG 19 ---
đại diện cho patch được định tuyến đến chuyên gia này. Ô vuông đen có nghĩa là patch không được định tuyến đến chuyên gia này. Chúng tôi chọn 4 chuyên gia hàng đầu có số lượng patch được định tuyến nhiều nhất để cung cấp trực quan hóa rõ ràng hơn.

Chúng tôi sử dụng Swin-T làm backbone và trực quan hóa định tuyến token trong lớp cuối cùng của giai đoạn 3. Chúng tôi chọn lớp này vì giai đoạn 3 có độ phân giải phù hợp để trực quan hóa, và feature map của nó chứa thông tin ngữ nghĩa nhiều hơn so với các giai đoạn nông. Đối với các nhiệm vụ phân đoạn và phát hiện, chúng tôi sử dụng cùng cài đặt trong Phần 5. Đối với RMoE, chúng tôi sử dụng các mô hình RMoE-D được huấn luyện trên các nhiệm vụ downstream.

Từ Hình 6, chúng ta thấy rằng RMoE và MoE chia sẻ các mô hình tương tự trong định tuyến patch, ví dụ như foreground và background được định tuyến vào các chuyên gia khác nhau. Những điều này cho thấy các chuyên gia của RMoE chia sẻ các chức năng tương tự như các chuyên gia MoE.

C Chi tiết Triển khai bổ sung

Đối với huấn luyện downstream, chúng tôi xây dựng codebase của mình trên MMSegmentation [11] và MMDetection [5]. Đối với triển khai MoE, chúng tôi sử dụng FastMoE [23] làm code cơ bản.

--- TRANG 20 ---
Phân đoạn
Phát hiện RMoE MoE RMoE MoE RMoE MoE RMoE MoE
nền chính nền chính tiền cảnh chính nền khác tiền cảnh khác

tiền cảnh chính tiền cảnh khác nền chính

tiền cảnh chính nền chính nền khác khác

tiền cảnh khác nền chính nền khác tiền cảnh chính tiền cảnh khác

Hình 6: Trực quan hóa định tuyến patch cho lớp cuối cùng của Swin-T giai đoạn 3 trong các nhiệm vụ phân đoạn và phát hiện. Chúng tôi định nghĩa main/other chủ yếu dựa trên số lượng patch.

--- TRANG 21 ---
