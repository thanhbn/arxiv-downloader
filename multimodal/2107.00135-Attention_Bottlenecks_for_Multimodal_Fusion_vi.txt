# Attention Bottlenecks cho Multimodal Fusion
Arsha Nagrani Shan Yang Anurag Arnab Aren Jansen
Cordelia Schmid Chen Sun
{anagrani, shanyang, aarnab, arenjansen, cordelias, chensun}@google.com
Google Research
Tóm tắt
Con người cảm nhận thế giới bằng cách xử lý và hợp nhất đồng thời các đầu vào đa chiều từ nhiều phương thức như thị giác và âm thanh. Ngược lại, các mô hình nhận thức máy thường chỉ dành riêng cho một phương thức và được tối ưu hóa cho các bộ đánh giá đơn phương thức, do đó việc hợp nhất giai đoạn muộn của các biểu diễn cuối cùng hoặc dự đoán từ mỗi phương thức ('late-fusion') vẫn là một mô hình thống trị cho phân loại video đa phương thức. Thay vào đó, chúng tôi giới thiệu một kiến trúc mới dựa trên transformer sử dụng 'fusion bottlenecks' để hợp nhất phương thức ở nhiều tầng. So với self-attention theo cặp truyền thống, mô hình của chúng tôi buộc thông tin giữa các phương thức khác nhau phải đi qua một số lượng nhỏ các bottleneck latent, yêu cầu mô hình phải thu thập và cô đọng thông tin có liên quan trong mỗi phương thức và chia sẻ những gì cần thiết. Chúng tôi thấy rằng chiến lược này cải thiện hiệu suất hợp nhất, đồng thời giảm chi phí tính toán. Chúng tôi tiến hành các nghiên cứu ablation kỹ lưỡng và đạt được kết quả tốt nhất trên nhiều bộ đánh giá phân loại âm thanh-hình ảnh bao gồm Audioset, Epic-Kitchens và VGGSound. Tất cả mã nguồn và mô hình sẽ được phát hành.

1 Giới thiệu
Các cảm giác đa phương thức đồng thời là một yếu tố quan trọng cho việc học nhận thức của con người [57]. Tuy nhiên, đối với các hệ thống học tập nhân tạo, việc thiết kế một mô hình thống nhất để hợp nhất phương thức là thách thức do một số yếu tố: (i) sự khác biệt về động lực học tập giữa các phương thức [63], (ii) các cấu trúc tô-pô nhiễu khác nhau, với một số luồng phương thức chứa nhiều thông tin hơn cho nhiệm vụ cụ thể, cũng như (iii) các biểu diễn đầu vào chuyên biệt. Sự khác biệt về biểu diễn đầu vào giữa âm thanh và hình ảnh đặc biệt rõ ràng - nhiều phương pháp phân loại âm thanh tiên tiến dựa vào phân tích Fourier ngắn hạn để tạo ra log-mel spectrograms, thường sử dụng chúng làm đầu vào cho các kiến trúc CNN được thiết kế cho hình ảnh [29,55]. Các biểu diễn thời gian-tần số này có phân phối khác với hình ảnh - nhiều đối tượng âm thanh có thể có năng lượng ở cùng một tần số, và tính bất biến dịch chuyển của CNN có thể không còn là thuộc tính mong muốn (trong khi một đối tượng âm thanh có thể được dịch chuyển theo thời gian, việc dịch chuyển tần số có thể thay đổi hoàn toàn ý nghĩa). Ngược lại, luồng hình ảnh trong video là ba chiều (hai không gian và một thời gian), và trong khi các vùng không gian khác nhau của hình ảnh tương ứng với các đối tượng khác nhau, có thách thức độc đáo về tính dư thừa cao qua nhiều khung hình. Do đó, các biểu diễn đầu vào và do đó các kiến trúc mạng neural và bộ đánh giá có xu hướng khác nhau hoàn toàn cho các phương thức khác nhau. Để đơn giản, mô hình thống trị cho hợp nhất đa phương thức do đó thường bao gồm một sơ đồ ad-hoc liên quan đến tích hợp các mạng âm thanh và hình ảnh riêng biệt thông qua các biểu diễn đầu ra hoặc điểm số tức 'late-fusion' [25, 49].

Trong công việc này, chúng tôi trình bày một mô hình mới dựa trên transformer để hợp nhất âm thanh-hình ảnh trong video. Mặc dù ban đầu được đề xuất cho các nhiệm vụ NLP, gần đây có sự quan tâm đến transformers [61] như các mô hình nhận thức phổ quát [32], do khả năng mô hình hóa các tương quan dày đặc giữa các token, đồng thời tạo ra ít giả định về đầu vào của chúng (và vì các đầu vào nhận thức liên tục có thể được token hóa). Bằng cách chia các tín hiệu liên tục dày đặc thành các patch và rasterize chúng thành các token 1D, transformers đã được chỉ ra có hiệu suất cạnh tranh cho phân loại hình ảnh (ViT [18]) và video (ViViT [6]), và gần đây hơn, phân loại âm thanh (AST [26]). Vì các mô hình này có thể xử lý các chuỗi có độ dài biến thiên một cách tinh tế, một mở rộng tự nhiên đầu tiên sẽ là đưa vào một chuỗi các patch hình ảnh và âm thanh vào transformer, với những thay đổi tối thiểu đối với kiến trúc. Mô hình 'early fusion' này cho phép luồng attention tự do giữa các vùng không gian và thời gian khác nhau trong hình ảnh, cũng như qua tần số và thời gian trong spectrogram âm thanh. Mặc dù hấp dẫn về mặt lý thuyết, chúng tôi đưa ra giả thuyết rằng full pairwise attention ở tất cả các tầng của mô hình là không cần thiết vì đầu vào âm thanh và hình ảnh chứa thông tin dày đặc, chi tiết, phần lớn là dư thừa. Điều này đặc biệt đúng với video, như được thể hiện bởi hiệu suất của các phiên bản 'factorised' của [6]. Mô hình như vậy cũng sẽ không mở rộng tốt cho video dài hơn do độ phức tạp bậc hai của pairwise attention với độ dài chuỗi token. Để giảm thiểu điều này, chúng tôi đề xuất hai phương pháp để hạn chế luồng attention trong mô hình của chúng tôi. Phương pháp đầu tiên theo một mô hình phổ biến trong học đa phương thức, đó là hạn chế luồng cross-modal đến các tầng sau của mạng, cho phép các tầng đầu chuyên biệt học và trích xuất các mẫu đơn phương thức. Từ đây điều này được gọi là 'mid fusion' (Hình 1, giữa trái), trong đó tầng mà tại đó các tương tác cross-modal được giới thiệu được gọi là 'fusion layer'. Hai phiên bản cực đoan của điều này là 'early fusion' (tất cả các tầng đều cross-modal) và 'late fusion' (tất cả đều đơn phương thức) mà chúng tôi so sánh như các baseline. Ý tưởng thứ hai của chúng tôi (và đóng góp chính), là hạn chế luồng cross-modal attention giữa các token trong một tầng. Chúng tôi làm điều này bằng cách cho phép luồng attention tự do trong một phương thức, nhưng buộc mô hình của chúng tôi phải thu thập và 'cô đọng' thông tin từ mỗi phương thức trước khi chia sẻ nó với phương thức khác. Ý tưởng cốt lõi là giới thiệu một tập hợp nhỏ các fusion unit tiềm ẩn tạo thành một 'attention bottleneck', mà thông qua đó các tương tác cross-modal trong một tầng phải đi qua.

Chúng tôi chứng minh rằng phiên bản 'bottlenecked' này, mà chúng tôi đặt tên là Multimodal Bottleneck Transformer (MBT), vượt qua hoặc khớp với phiên bản không bị hạn chế của nó, nhưng với chi phí tính toán thấp hơn.

Cụ thể, chúng tôi có các đóng góp sau: (i) Chúng tôi đề xuất một kiến trúc mới (MBT) cho hợp nhất âm thanh-hình ảnh. Mô hình của chúng tôi hạn chế luồng thông tin cross-modal giữa các latent unit thông qua các 'bottleneck' hợp nhất chặt chẽ, buộc mô hình phải thu thập và 'cô đọng' các đầu vào có liên quan nhất trong mỗi phương thức (và do đó chỉ chia sẻ những gì cần thiết với phương thức khác). Điều này tránh chi phí mở rộng bậc hai của full pairwise attention và dẫn đến tăng hiệu suất với ít tính toán hơn; (ii) Chúng tôi áp dụng MBT vào các patch hình ảnh và spectrogram (Hình 2), và khám phá một số ablation liên quan đến fusion layer, việc lấy mẫu đầu vào và kích thước dữ liệu; và cuối cùng (iii) Chúng tôi đặt ra state-of-the-art mới cho phân loại video trên một số bộ đánh giá âm thanh-hình ảnh phổ biến, bao gồm AudioSet [24], Epic-Kitchens100 [14] và VGGSound [12]. Trên bộ dữ liệu Audioset, chúng tôi vượt qua state of the art hiện tại 5.9 mAP (cải thiện tương đối 12.7%).

2 Công trình liên quan
Học âm thanh-hình ảnh: Học đa phương thức âm thanh-hình ảnh có lịch sử phong phú, cả trước và trong thời đại deep learning [53]. Với dữ liệu hạn chế và tài nguyên tính toán có sẵn, các công trình đầu tập trung vào giai đoạn đầu tương đối đơn giản (ví dụ xếp chồng các đặc trưng được thiết kế thủ công) và giai đoạn cuối (ví dụ hợp nhất điểm số) [13]. Deep learning đã cho phép các chiến lược tinh vi hơn trong đó các latent dành riêng cho phương thức hoặc chung được học ngầm để trung gian cho việc hợp nhất. Kết quả đã cho phép những tiến bộ lớn trong một loạt các nhiệm vụ âm thanh-hình ảnh có giám sát downstream [48,38,19]. Trong setting có giám sát, nhiều mạng convolutional dành riêng cho phương thức có thể được huấn luyện chung, mà các activation trung gian của chúng sau đó được kết hợp bằng phép cộng [36] hoặc thông qua 'lateral connections' [64]. Trong setting không có giám sát, học âm thanh-hình ảnh thường được sử dụng để học các biểu diễn đơn phương thức tốt, với một nhiệm vụ pretraining phổ biến là đồng bộ hóa tín hiệu từ các phương thức khác nhau thông qua loss contrastive [4, 5, 7, 49, 33, 2, 3], tuy nhiên mỗi phương thức thường được mã hóa riêng biệt dưới thiết lập này.

Transformers đa phương thức: Phép toán self attention của transformers cung cấp một cơ chế tự nhiên để kết nối các tín hiệu đa phương thức. Transformers đa phương thức đã được áp dụng cho nhiều nhiệm vụ khác nhau bao gồm tăng cường âm thanh [19,60], nhận dạng giọng nói [27], phân đoạn hình ảnh [66,60], sinh chuỗi cross-modal [43,41,56], truy xuất hình ảnh và video [28,23,8], điều hướng hình ảnh [51] và phân loại/chú thích hình ảnh/video [46,59,58,40,31]. Đối với nhiều công trình, các đầu vào cho transformers là các biểu diễn đầu ra của CNN đơn phương thức [39,23] - không giống như những công trình này, chúng tôi sử dụng các khối transformer xuyên suốt, chỉ sử dụng một lớp convolutional duy nhất để rasterize các patch 2D. Các token từ các phương thức khác nhau thường được kết hợp trực tiếp làm đầu vào cho transformers [42], ví dụ, mô hình Perceiver được phát hành gần đây [32] giới thiệu một cơ chế attention lặp lại nhận các tín hiệu đa phương thức thô được nối tiếp làm đầu vào, điều này tương ứng với baseline 'early fusion' của chúng tôi. Ngược lại, chúng tôi xem xét cẩn thận tác động của các chiến lược hợp nhất phương thức khác nhau, bao gồm giới hạn luồng cross-modal attention đến các tầng sau của mô hình, và 'dẫn' các kết nối cross-modal thông qua bottleneck trong Multimodal Bottleneck Transformer (MBT) được đề xuất.

3 Multimodal fusion transformers
Trong phần này, chúng tôi mô tả Multimodal Bottleneck Transformer (MBT) được đề xuất. Chúng tôi bắt đầu bằng việc tóm tắt Vision Transformer (ViT) [18] và Audio Spectrogram Transformer (AST) [26] được đề xuất gần đây, được phát triển cho phân loại hình ảnh và âm thanh tương ứng, trong Phần 3.1. Sau đó chúng tôi mô tả mở rộng của chúng tôi cho trường hợp hợp nhất âm thanh-hình ảnh. Chúng tôi thảo luận về ba chiến lược hợp nhất token khác nhau (Phần 3.2), và cuối cùng thảo luận về fusion pathway trong toàn bộ mô hình (Phần 3.3), liên quan đến việc hạn chế hợp nhất đa phương thức đến các tầng nhất định của mô hình.

3.1 Kiến trúc ViT và AST
Vision Transformer (ViT) [18] (và một mở rộng gần đây cho âm thanh - Audio Spectrogram Transformer (AST) [26]) thích ứng kiến trúc Transformer [61], ban đầu được thiết kế cho xử lý ngôn ngữ tự nhiên, để xử lý đầu vào 2D với những thay đổi tối thiểu. Insight chính là trích xuất N patch không chồng lấp từ hình ảnh RGB (hoặc spectrogram âm thanh), xi∈R^{h×w}, và chuyển đổi chúng thành một chuỗi token 1D zi∈R^d, như sau:

z=g(x;E;z_{cls}) = [z_{cls};Ex_1;Ex_2;...;Ex_N] + p. (1)

Ở đây, E là một ánh xạ chiếu tuyến tính mỗi token vào R^d, z_{cls} là một token đặc biệt được thêm vào trước chuỗi này để biểu diễn của nó ở tầng cuối có thể được chuyển đến một classifier cho các nhiệm vụ phân loại [17], và p∈R^{(N+1)×d} là một embedding vị trí học được thêm vào các token để giữ lại thông tin vị trí (vì tất cả các phép toán self-attention tiếp theo đều bất biến hoán vị).

Các token sau đó được chuyển qua một encoder bao gồm một chuỗi L tầng transformer. Mỗi tầng transformer bao gồm Multi-Headed Self-Attention (MSA), Layer Normalisation (LN) và các khối Multilayer Perceptron (MLP) được áp dụng sử dụng residual connections. Chúng tôi ký hiệu một tầng transformer, z^{l+1} = Transformer(z^l) là

y^l = MSA(LN(z^l)) + z^l (2)
z^{l+1} = MLP(LN(y^l)) + y^l. (3)

Ở đây, phép toán MSA [61] tính toán dot-product attention [61] trong đó queries, keys và values đều là các phép chiếu tuyến tính của cùng một tensor, MSA(X) = Attention(W_Q X; W_K X; W_V X).

Chúng tôi tiếp tục định nghĩa Multi-Headed Cross Attention (MCA) giữa hai tensor, X và Y, trong đó X tạo thành query và Y tạo thành keys và values được sử dụng để tính trọng số lại cho query là MCA(X;Y) = Attention(W_Q X; W_K Y; W_V Y). Điều này sẽ được sử dụng trong trường hợp đa phương thức của chúng tôi, như được mô tả tiếp theo.

3.2 Multimodal transformer
Bây giờ chúng tôi mô tả mở rộng của chúng tôi cho trường hợp đa phương thức. Chúng tôi bắt đầu bằng việc thảo luận về ba chiến lược hợp nhất token khác nhau.

3.2.1 Hợp nhất thông qua vanilla self-attention
Chúng tôi bắt đầu bằng việc mô tả một mô hình hợp nhất 'vanilla', chỉ đơn giản bao gồm transformer thông thường được áp dụng cho đầu vào đa phương thức. Phương pháp token hóa video của chúng tôi rất đơn giản - cho một clip video có độ dài t giây, chúng tôi lấy mẫu đồng đều F khung RGB và chuyển đổi dạng sóng âm thanh thành một spectrogram duy nhất. Sau đó chúng tôi embed mỗi khung và spectrogram độc lập theo mã hóa được đề xuất trong ViT [18], và nối tiếp tất cả các token lại thành một chuỗi duy nhất.

Chính thức, nếu chúng tôi đã trích xuất tổng cộng N_v patch RGB từ tất cả F khung được lấy mẫu, x_{rgb}∈R^{N_v×d}, và N_a patch spectrogram, x_{spec}∈R^{N_a×d}, chuỗi token của chúng tôi là

z = [z_{rgb}||z_{spec}] trong đó z_{rgb} = g(x_{rgb}; E_{rgb}; z_{cls-rgb}) và z_{spec} = g(x_{spec}; E_{spec}; z_{cls-spec}). (4)

Ở đây, [z_{rgb}||z_{spec}] biểu thị việc nối tiếp các token cho mỗi phương thức. Chúng tôi sử dụng các phép chiếu khác nhau E_{rgb} và E_{spec} cho các patch RGB và spectrogram tương ứng, và thêm vào trước một token phân loại riêng biệt cho mỗi phương thức.

Encoder đa phương thức của chúng tôi sau đó áp dụng một chuỗi các tầng transformer theo cách tương tự như trên. Attention được cho phép chảy tự do qua mạng, tức là mỗi token RGB có thể chú ý đến tất cả các token RGB và spectrogram khác như sau: z^{l+1} = Transformer(z^l; θ) với các tham số mô hình θ. Ở đây Transformer chỉ một tầng transformer tiêu chuẩn với các khối vanilla self-attention.

3.2.2 Hợp nhất với tham số dành riêng cho phương thức
Chúng tôi có thể tổng quát hóa mô hình này bằng cách cho phép mỗi phương thức có các tham số riêng θ_{rgb} và θ_{spec}, nhưng vẫn trao đổi thông tin thông qua cơ chế attention. Với mục đích này, chúng tôi định nghĩa một tầng Cross-Transformer:

z^{l+1}_{rgb} = Cross-Transformer(z^l_{rgb}; z^l; θ_{rgb}) (5)
z^{l+1}_{spec} = Cross-Transformer(z^l_{spec}; z^l; θ_{spec}),

trong đó Cross-Transformer sử dụng phép toán cross-attention tổng quát nhận hai tập hợp đầu vào z_1 và z_2 không nhất thiết phải chồng lấp. Tầng này tuân theo tầng transformer ban đầu với sự khác biệt là Công thức 2 trở thành

y^l = MCA(LN(z^l_1); LN(z^l_2)) + z^l_1. (6)

Cuối cùng, lưu ý rằng chúng tôi đã định nghĩa rõ ràng các tham số, θ_{rgb} và θ_{spec} của các tầng cross-transformer trong Công thức 5 vì chúng khác nhau cho mỗi phương thức. Tuy nhiên, khi θ_{rgb} và θ_{spec} bằng nhau, (θ_{rgb} = θ_{spec} = θ), phép tính được định nghĩa trong Công thức 5 tương đương với Phần 3.2.1.

3.2.3 Hợp nhất thông qua attention bottlenecks
Để kiểm soát độ phức tạp bậc hai của pairwise attention, chúng tôi tiếp theo giới thiệu một tập hợp nhỏ B fusion bottleneck token z_{fsn} = [z^1_{fsn}; z^2_{fsn}; ...; z^B_{fsn}] vào chuỗi đầu vào của chúng tôi (xem Hình 2). Chuỗi đầu vào bây giờ là

z = [z_{rgb}||z_{fsn}||z_{spec}]. (7)

Sau đó chúng tôi hạn chế tất cả luồng cross-modal attention trong mô hình của chúng tôi phải thông qua các bottleneck token này. Chính thức hơn cho tầng l, chúng tôi tính toán các biểu diễn token như sau:

[z^{l+1}_i||ẑ^{l+1}_{fsn_i}] = Transformer([z^l_i||z^l_{fsn}]; θ_i) (8)
z^{l+1}_{fsn} = Avg_i(ẑ^{l+1}_{fsn_i}) (9)

Ở đây i chỉ mục mỗi phương thức, trong trường hợp này là RGB và Spec, và z_{rgb} và z_{spec} chỉ có thể trao đổi thông tin thông qua bottleneck z_{fsn} trong một tầng transformer. Chúng tôi đầu tiên tạo ra các token bottleneck fusion tạm thời dành riêng cho phương thức ẑ_{fsn_i}, được cập nhật riêng biệt và đồng thời với thông tin âm thanh và hình ảnh (Công thức 8). Các token fusion cuối cùng từ mỗi bản cập nhật cross-modal sau đó được trung bình trong Công thức 9. Chúng tôi cũng thử nghiệm với các bản cập nhật bất đối xứng cho các token bottleneck (xem phụ lục) và thấy hiệu suất ổn định với lựa chọn này. Chúng tôi giữ số lượng token bottleneck trong mạng nhỏ hơn nhiều so với tổng số latent unit trên mỗi phương thức (B ≪ N_v và B ≪ N_a). Bởi vì tất cả luồng cross-modal attention phải đi qua các unit này, các 'fusion' bottleneck chặt chẽ này buộc mô hình phải cô đọng thông tin từ mỗi phương thức và chia sẻ những gì cần thiết. Như chúng tôi chỉ ra trong các thí nghiệm, điều này làm tăng hoặc duy trì hiệu suất cho hợp nhất đa phương thức, đồng thời giảm độ phức tạp tính toán. Chúng tôi cũng lưu ý rằng công thức của chúng tôi là tổng quát cho loại và số lượng phương thức.

3.3 Hợp nhất ở đâu: early, mid và late
Các chiến lược trên thảo luận về hợp nhất trong một tầng, và trong hầu hết các kiến trúc transformer (như ViT), mỗi tầng bao gồm một tập hợp các phép toán giống hệt nhau. Tuy nhiên, một mô hình phổ biến trong học đa phương thức là hạn chế các tầng đầu của mạng tập trung vào xử lý đơn phương thức, và chỉ giới thiệu các kết nối cross-modal ở các tầng sau. Điều này có tính trực quan về mặt khái niệm nếu chúng ta tin rằng các tầng thấp hơn liên quan đến xử lý các đặc trưng mức thấp, trong khi các tầng cao hơn tập trung vào học các khái niệm ngữ nghĩa - các đặc trưng hình ảnh mức thấp như cạnh và góc trong hình ảnh có thể không có chữ ký âm thanh cụ thể, và do đó có thể không hưởng lợi từ early fusion với âm thanh [64].

Điều này có thể được thực hiện với mô hình của chúng tôi như sau: Chúng tôi ban đầu thực hiện vanilla self-attention giữa các token từ một phương thức duy nhất cho L_f tầng. Sau đó, chúng tôi nối tiếp tất cả các token tiềm ẩn lại với nhau, z^{L_f} = [z^{L_f}_{rgb}||z^{L_f}_{spec}] và chuyển chúng qua L - L_f tầng còn lại trong đó các token được hợp nhất theo Phần 3.2. Ở đây, L_f = 0 tương ứng với mô hình 'early-fusion', L_f = L là mô hình 'late-fusion', và 0 < L_f < L là mô hình 'mid-fusion'. Chính thức hơn, điều này có thể được ký hiệu là

z^{l+1}_{rgb} = Transformer(z^l_{rgb}; θ_{rgb}); z^{l+1}_{spec} = Transformer(z^l_{spec}; θ_{spec}) nếu l < L_f
z^l = [z^l_{rgb}||z^l_{spec}]; z^{l+1} = Multimodal-Transformer(z^l; θ_{spec}; θ_{rgb}) ngược lại

trong đó Multimodal-Transformer() có thể chỉ một trong 3 chiến lược hợp nhất được mô tả trong Phần 3.2.

3.4 Phân loại
Đối với tất cả các biến thể mô hình được mô tả ở trên, chúng tôi chuyển các biểu diễn đầu ra của các token CLS z^L_{cls-rgb} và z^L_{cls-spec} đến cùng một linear classifier và trung bình các logit trước softmax.

4 Thí nghiệm
Chúng tôi áp dụng MBT vào nhiệm vụ phân loại video. Trong phần này, chúng tôi đầu tiên mô tả các bộ dữ liệu được sử dụng để huấn luyện và kiểm tra hợp nhất đa phương thức và các giao thức đánh giá tương ứng của chúng (Phần 4.1), sau đó thảo luận về chi tiết thực hiện (Phần 4.2). Chúng tôi sau đó ablate các lựa chọn thiết kế chính trong mô hình của chúng tôi (Phần 4.3), trước khi cuối cùng so sánh mô hình của chúng tôi với state of the art (Phần 4.4).

4.1 Bộ dữ liệu và giao thức đánh giá
Chúng tôi thử nghiệm với ba bộ dữ liệu phân loại video - AudioSet [24], Epic-Kitchens-100 [14] và VGGSound [12], được mô tả chi tiết hơn bên dưới. Kết quả trên hai bộ dữ liệu bổ sung Moments in Time [47] và Kinetics [35] được cung cấp trong phụ lục.

AudioSet [24] bao gồm gần 2 triệu clip video 10 giây từ YouTube, được chú thích với 527 lớp. Giống như các bộ dữ liệu YouTube khác, đây là một bộ dữ liệu động (chúng tôi chỉ sử dụng các clip vẫn có sẵn trực tuyến). Điều này cung cấp cho chúng tôi 20,361 clip cho tập huấn luyện cân bằng (từ đây được gọi là mini-AudioSet hoặc miniAS) và 18,589 clip cho tập kiểm tra. Tập kiểm tra này hoàn toàn giống với các công trình gần đây mà chúng tôi so sánh, bao gồm Perceiver [32]. Thay vì sử dụng tập huấn luyện không cân bằng 2M, chúng tôi huấn luyện trên một tập con cân bằng hơn (một chút) bao gồm 500K mẫu (AS-500K). Chi tiết được cung cấp trong phụ lục. Vì mỗi mẫu có nhiều nhãn, chúng tôi huấn luyện với binary cross-entropy (BCE) loss và báo cáo mean average precision (mAP) trên tất cả các lớp, theo thực hành tiêu chuẩn.

Epic-Kitchens 100 [14] bao gồm các video egocentric ghi lại các hoạt động nhà bếp hàng ngày. Bộ dữ liệu bao gồm 90,000 clip có độ dài biến thiên trải dài 100 giờ. Chúng tôi báo cáo kết quả cho nhận dạng hành động theo giao thức tiêu chuẩn [14] - mỗi nhãn hành động là một kết hợp của động từ và danh từ, và chúng tôi dự đoán cả hai sử dụng một mạng duy nhất với hai 'head', cả hai đều được huấn luyện với cross-entropy loss. Cặp động từ và hành động có điểm cao nhất được dự đoán bởi mạng được sử dụng, và Top-1 action accuracy là metric chính. Các hành động chủ yếu ngắn hạn (độ dài trung bình là 2.6s với độ dài tối thiểu 0.25s).

VGGSound [12] chứa gần 200K clip video có độ dài 10s, được chú thích với 309 lớp âm thanh bao gồm các hành động của con người, đối tượng phát âm thanh và tương tác giữa con người và đối tượng. Không giống như AudioSet, nguồn âm thanh cho mỗi clip 'hiện diện trực quan' trong video. Điều này được đảm bảo trong quá trình tạo bộ dữ liệu thông qua việc sử dụng các classifier hình ảnh. Sau khi lọc các clip không còn có sẵn trên YouTube, chúng tôi có 172,427 clip huấn luyện và 14,448 clip kiểm tra. Chúng tôi huấn luyện với standard cross-entropy loss cho phân loại và báo cáo Top-1 và Top-5 classification accuracy.

4.2 Chi tiết thực hiện
Kiến trúc backbone của chúng tôi tuân theo ViT [18] một cách giống hệt, cụ thể chúng tôi sử dụng ViT-Base (ViT-B, L = 12, N_H = 12, d = 3072) được khởi tạo từ ImageNet-21K [16], tuy nhiên chúng tôi lưu ý rằng phương pháp của chúng tôi là agnostic đối với transformer backbone. Trừ khi được chỉ định khác, chúng tôi sử dụng B = 4 bottleneck token cho tất cả các thí nghiệm với bottleneck fusion. Các token bottleneck được khởi tạo sử dụng Gaussian với trung bình 0 và độ lệch chuẩn 0.02, tương tự như positional embedding trong mã ViT công khai [18]. Chúng tôi lấy mẫu ngẫu nhiên các clip t giây để huấn luyện. Các khung RGB cho tất cả các bộ dữ liệu được trích xuất ở 25 fps. Đối với AudioSet và VGGSound, chúng tôi lấy mẫu 8 khung RGB qua cửa sổ lấy mẫu có độ dài t với stride đồng đều có độ dài (t×25)/8. Chúng tôi trích xuất các patch 16×16 từ mỗi khung có kích thước 224×224, cho chúng tôi tổng cộng 8×14×14 = 1568 patch trên mỗi video. Đối với Epic-Kitchens (vì các đoạn ngắn hơn), chúng tôi lấy mẫu 32 khung với stride 1. Âm thanh cho tất cả các bộ dữ liệu được lấy mẫu ở 16kHz và chuyển đổi thành kênh mono. Tương tự như [26], chúng tôi trích xuất log mel spectrogram với chiều tần số 128 được tính toán sử dụng cửa sổ Hamming 25ms với hop length 10ms. Điều này cho chúng tôi một đầu vào có kích thước 128×100t cho t giây âm thanh. Các patch spectrogram được trích xuất với kích thước 16×16, cho chúng tôi 5×8 = 40 patch cho 8 giây âm thanh.

Đối với hình ảnh, chúng tôi áp dụng các data augmentation tiêu chuẩn được sử dụng trong [6] (random crop, flip, colour jitter), và đối với spectrogram, chúng tôi sử dụng SpecAugment [50] với max time mask length 192 khung và max frequency mask length 48 bin theo AST [26]. Chúng tôi đặt base learning rate là 0.5 và huấn luyện trong 50 epoch, sử dụng Mixup [67] với α = 0.3 và stochastic depth regularisation [30] với xác suất p = 0.3. Tất cả các mô hình (qua các bộ dữ liệu) được huấn luyện với batch size 64, synchronous SGD với momentum 0.9, và cosine learning rate schedule với warmup 2.5 epoch trên các bộ tăng tốc TPU sử dụng thư viện Scenic [15].

Suy luận: Theo thực hành tiêu chuẩn, chúng tôi lấy mẫu đồng đều nhiều temporal crop từ clip và trung bình logit trên mỗi view để có được kết quả cuối cùng. Số lượng test crop được đặt là 4.

4.3 Phân tích ablation
Trong phần này, chúng tôi điều tra tác động của các lựa chọn kiến trúc khác nhau trong MBT. Trừ khi được chỉ định khác, chúng tôi sử dụng mini-AudioSet split để huấn luyện và báo cáo kết quả trên AudioSet eval split. Thêm ablation về kích thước backbone và khởi tạo pretraining có thể được tìm thấy trong phụ lục.

4.3.1 Chiến lược hợp nhất
Chúng tôi thực hiện tất cả ba chiến lược hợp nhất được mô tả trong Phần 3.2:
(i) Vanilla self-attention - Pairwise attention không bị hạn chế giữa tất cả các latent unit trong một tầng;
(ii) Vanilla cross-attention với trọng số riêng biệt: Giống như trên, nhưng bây giờ chúng tôi có trọng số riêng biệt cho mỗi phương thức. Các latent unit được cập nhật thông qua pairwise attention với tất cả các latent unit khác từ cả hai phương thức; và cuối cùng
(iii) Bottleneck fusion: Ở đây tất cả cross-modal attention phải đi qua bottleneck fusion latent.

Lưu ý rằng ba chiến lược hợp nhất này chỉ mô tả luồng attention giữa các token trong một tầng. Đối với chiến lược (ii) và (iii), chúng tôi cũng tiến hành các thí nghiệm chỉ ra tác động của việc hạn chế cross-modal attention đến các tầng sau một fusion layer cố định L_f. Chúng tôi điều tra các mô hình với các fusion layer khác nhau, L_f = 0, 2, 4, 6, 8, 10, 12, và trình bày kết quả trong Hình 3.

Chia sẻ trọng số cho cả hai phương thức: Chúng tôi đầu tiên điều tra tác động của việc chia sẻ trọng số encoder cho cả hai phương thức (chiến lược (i) vs (ii)). Kết quả có thể được tìm thấy trong Hình 7 trong phụ lục. Khi các phương thức được hợp nhất ở các tầng sớm hơn, việc sử dụng encoder riêng biệt cải thiện hiệu suất. Đối với các mô hình với fusion layer sau này, hiệu suất tương tự cho cả hai mô hình. Do đó chúng tôi sử dụng trọng số phương thức riêng biệt cho các thí nghiệm tiếp theo.

Fusion layer: Chúng tôi sau đó điều tra tác động của việc thay đổi fusion layer L_f, cho hai chiến lược sau: (ii) Vanilla Cross-Attention và (iii) Bottleneck Fusion. Chúng tôi tiến hành thí nghiệm với L_f = 0, 2, 4, 6, 8, 10, 12. Chúng tôi cố định input span t là 4s và số lượng bottleneck token B là 4. Chúng tôi tiến hành 3 lần chạy cho mỗi thí nghiệm và báo cáo trung bình và độ lệch chuẩn. Như có thể thấy từ Hình 3 (trái), 'mid fusion' vượt trội hơn cả early (L_f = 0) và late fusion (L_f = 12), với hiệu suất tối ưu đạt được bằng cách sử dụng fusion layer L_f = 10 cho vanilla cross-attention và L_f = 8 cho bottleneck attention. Điều này cho thấy rằng mô hình hưởng lợi từ việc hạn chế các kết nối cross-modal đến các tầng sau, cho phép các tầng sớm hơn chuyên biệt học các đặc trưng đơn phương thức, tuy nhiên vẫn hưởng lợi từ nhiều tầng luồng thông tin cross-modal. Trong phụ lục D, chúng tôi xác nhận rằng mid fusion vượt trội hơn late fusion qua một số bộ dữ liệu khác nhau.

Attention bottleneck: Trong Hình 3, chúng tôi cũng xem xét hiệu ứng của bottleneck attention vs vanilla cross-attention cho hợp nhất đa phương thức. Chúng tôi thấy rằng đối với tất cả giá trị của L_f, việc hạn chế luồng đến bottleneck cải thiện hoặc duy trì hiệu suất, với các cải thiện nổi bật hơn ở các giá trị L_f thấp hơn. Ở L_f = 10, cả hai đều hoạt động tương tự, lưu ý rằng ở giai đoạn này chúng tôi chỉ có 3 fusion layer trong mô hình.

Mô hình hoạt động tốt nhất của chúng tôi sử dụng attention bottleneck với L_f = 8, và chúng tôi cố định điều này cho tất cả các thí nghiệm tiếp theo. Chúng tôi cũng so sánh lượng tính toán, được đo bằng GFLOP, cho cả hai chiến lược hợp nhất (Hình 3, phải). Sử dụng một số lượng nhỏ bottleneck token (trong thí nghiệm của chúng tôi B = 4) thêm vào tính toán không đáng kể so với mô hình late fusion, với tính toán vẫn phần lớn không đổi với fusion layer L_f thay đổi. Điều này trái ngược với vanilla cross-fusion, có chi phí tính toán đáng kể cho mỗi tầng nó được áp dụng. Chúng tôi lưu ý rằng đối với early fusion (L_f = 0), bottleneck fusion vượt trội hơn vanilla cross-attention hơn 2 mAP, với ít hơn một nửa chi phí tính toán.

Số lượng bottleneck token B: Chúng tôi thử nghiệm với B = 4, 36, 64, 256 và 1024, và thấy rằng hiệu suất tương đối nhất quán (tất cả trong vòng 0.5 mAP). Do đó chúng tôi cố định số lượng token là B = 4 cho tất cả các thí nghiệm. Thật thú vị khi với một số lượng nhỏ như vậy các kết nối cross-modal thông qua chỉ 4 hidden unit (B = 4) ở mỗi tầng cross-modal, chúng tôi nhận được tăng hiệu suất lớn so với late fusion (Hình 3), làm nổi bật tầm quan trọng của việc cho phép thông tin cross-modal chảy ở nhiều tầng của mô hình.

4.3.2 Lấy mẫu đầu vào và kích thước bộ dữ liệu
Trong phần này, chúng tôi điều tra tác động của các chiến lược lấy mẫu phương thức khác nhau. Chúng tôi cũng so sánh với các baseline đơn phương thức - các baseline chỉ hình ảnh và chỉ âm thanh bao gồm một mô hình transformer vanilla được áp dụng cho chỉ các patch RGB hoặc spectrogram tương ứng.

Kích thước cửa sổ lấy mẫu t: Một lợi thế của mô hình dựa trên transformer của chúng tôi là chúng tôi có thể dễ dàng nhập các chuỗi token có độ dài biến thiên. Chúng tôi thử nghiệm với việc thay đổi cửa sổ lấy mẫu t với các giá trị sau t = 2, 4, 6 và 8 giây (lưu ý rằng tất cả video trong AudioSet đều là 10s), và hiển thị kết quả trong Hình 4. Khi suy luận, chúng tôi lấy mẫu đồng đều nhiều cửa sổ bao phủ toàn bộ video. Trong khi số lượng patch spectrogram N_a thay đổi với t, chúng tôi giữ số lượng patch RGB N_v cố định bằng cách thay đổi stride của các khung (để tránh hết bộ nhớ). Kết quả của chúng tôi chỉ ra rằng hiệu suất của cả mô hình âm thanh và hợp nhất âm thanh-hình ảnh tăng với input span, tuy nhiên hiệu suất của mô hình chỉ hình ảnh giảm nhẹ (chúng tôi đưa ra giả thuyết rằng điều này là do stride cố định tăng lên, có nghĩa là ít khung hình được lấy mẫu ngẫu nhiên hơn trong quá trình huấn luyện). Chúng tôi cố định t = 8s trong tất cả các thí nghiệm tiếp theo.

Lấy mẫu đồng bộ vs không đồng bộ: Cho rằng các sự kiện âm thanh và hình ảnh có thể không phải lúc nào cũng được căn chỉnh hoàn hảo trong video [36], chúng tôi cũng điều tra việc lấy mẫu không đồng bộ của các phương thức khác nhau. Ở đây các cửa sổ đầu vào được lấy mẫu độc lập từ toàn bộ clip video cho mỗi phương thức. Kết quả được cung cấp trong Hình 8 trong phụ lục. Chúng tôi thấy hiệu suất phần lớn ổn định với cả hai trường hợp, và vì vậy để đơn giản, chúng tôi sử dụng lấy mẫu đồng bộ cho tất cả các thí nghiệm tiếp theo.

Modality MixUp: Trong khi áp dụng regularization Mixup [67] cho huấn luyện, chúng tôi lưu ý rằng có hai cách khác nhau để áp dụng nó cho đầu vào đa phương thức - phương pháp tiêu chuẩn là lấy mẫu một tập hợp trọng số mixup từ phân phối Beta sử dụng tham số α, và sử dụng nó để tạo ra tất cả các cặp phương thức-nhãn ảo [67]. Chúng tôi cũng khám phá một phiên bản sửa đổi mà chúng tôi gọi là modality mixup, lấy mẫu trọng số độc lập cho mỗi phương thức. Modality mixup áp đặt augmentation mạnh hơn so với mixup tiêu chuẩn, dẫn đến cải thiện nhẹ (42.6 mAP thành 43.9 mAP) trên AudioSet.

Tác động của kích thước bộ dữ liệu: Chúng tôi hiển thị tác động của việc thay đổi số lượng mẫu huấn luyện trong Hình 5, và thấy sự tăng đơn điệu với kích thước bộ dữ liệu (tăng mạnh hơn cho chỉ âm thanh so với chỉ hình ảnh).

4.4 Kết quả
So sánh với hiệu suất đơn phương thức: Chúng tôi so sánh MBT với các baseline chỉ hình ảnh và chỉ âm thanh trên AudioSet (Bảng 1), Epic-Kitchens (Bảng 2) và VGGSound (Bảng 3). Lưu ý chúng tôi sử dụng các tham số tốt nhất thu được thông qua các ablation ở trên, tức là bottleneck fusion với t = 8, B = 4, F_l = 8 và modality mixup. Đối với tất cả các bộ dữ liệu, hợp nhất đa phương thức vượt trội hơn baseline đơn phương thức có hiệu suất cao hơn, chứng minh giá trị của thông tin bổ sung. Tầm quan trọng tương đối của các phương thức cho các nhãn phân loại khác nhau (chỉ âm thanh có hiệu suất tương đối cao hơn cho AudioSet và thấp hơn cho Epic-Kitchens, trong khi cả baseline âm thanh và hình ảnh đều mạnh như nhau cho VGGSound). Điều này (không ngạc nhiên) phần lớn là một hàm của quy trình chú thích bộ dữ liệu và đặt VGGSound là một bộ dữ liệu phù hợp độc đáo cho hợp nhất. Chúng tôi cũng chỉ ra rằng hợp nhất âm thanh-hình ảnh cung cấp tăng hiệu suất nhẹ cho các bộ dữ liệu truyền thống chỉ video như Kinetics và Moments in Time (chi tiết được cung cấp trong Phụ lục C). Chúng tôi cũng xem xét hiệu suất theo lớp trên bộ dữ liệu Audioset (Hình 9 và 10 trong Phụ lục), và thấy rằng đối với 60 lớp hàng đầu (được xếp hạng theo hiệu suất tổng thể), hợp nhất âm thanh-hình ảnh cải thiện hiệu suất so với chỉ âm thanh hoặc chỉ hình ảnh cho hầu hết (57 trong 60) lớp, ngoại trừ 'bagpiping', 'emergency vehicle' và 'didgeridoo' có chữ ký âm thanh mạnh. Đối với các lớp như 'bicycle' và 'shuffling cards' trong đó tín hiệu âm thanh yếu hơn, hợp nhất cải thiện so với baseline chỉ âm thanh hơn 60% AP tuyệt đối.

So sánh với state of the art: Chúng tôi so sánh MBT với các phương pháp hợp nhất trước đây trên AudioSet trong Bảng 1. Chúng tôi vượt trội hơn tất cả các công trình trước đây về hợp nhất (mặc dù chúng tôi chỉ huấn luyện trên một phần tư tập huấn luyện - 500K mẫu), bao gồm Perceiver được giới thiệu gần đây [32] sử dụng early fusion theo sau bởi nhiều tầng self attention, và Attn Audio-Visual [21] sử dụng self-attention fusion trên đầu các CNN phương thức riêng lẻ. Chúng tôi so sánh với các phương pháp phân loại video trước đây trên Epic-Kitchens trong Bảng 2, và lưu ý rằng mô hình của chúng tôi vượt trội hơn tất cả các công trình trước đây chỉ sử dụng hình ảnh, cũng như TBN [36] sử dụng ba phương thức - RGB, âm thanh và optical flow. Cho rằng VGGSound là một bộ dữ liệu tương đối mới, chúng tôi so sánh với hai công trình chỉ âm thanh hiện có (Bảng 3), và đặt benchmark âm thanh-hình ảnh đầu tiên (mà chúng tôi biết) trên bộ dữ liệu này.

Trực quan hóa attention map: Cuối cùng, chúng tôi tính toán các map attention từ các token CLS đầu ra đến không gian đầu vào hình ảnh RGB sử dụng Attention Rollout [1]. Kết quả trên hình ảnh kiểm tra cho cả mô hình vanilla fusion và MBT được huấn luyện trên Audioset-mini (fusion layer L_f = 8) được hiển thị trong Hình 6. Chúng tôi hiển thị các attention map được tổng hợp trên tất cả các khung trong clip video. Chúng tôi lưu ý rằng đầu tiên, mô hình tập trung vào các vùng có ý nghĩa ngữ nghĩa trong video cho phân loại âm thanh, đặc biệt là các vùng có chuyển động tạo ra hoặc sửa đổi âm thanh, tức là miệng của con người tạo ra âm thanh, đầu ngón tay trên piano, tay và nhạc cụ. Điều này không giống như các kỹ thuật định vị nguồn âm thanh tiên tiến được huấn luyện với hình ảnh [11], có xu hướng làm nổi bật toàn bộ đối tượng. Chúng tôi tiếp tục lưu ý rằng các attention map cho MBT được định vị hơn đến các vùng này, chỉ ra rằng các bottleneck chặt chẽ thực sự buộc mô hình chỉ tập trung vào các patch hình ảnh thực sự có liên quan cho nhiệm vụ phân loại âm thanh và hưởng lợi từ early fusion với âm thanh.

5 Kết luận
Chúng tôi đề xuất một kiến trúc transformer mới (MBT) cho hợp nhất âm thanh-hình ảnh, và khám phá một số chiến lược hợp nhất khác nhau sử dụng cross-attention giữa các latent token. Chúng tôi đề xuất một chiến lược mới để hạn chế cross-modal attention thông qua một tập hợp nhỏ các 'bottleneck' hợp nhất, và chứng minh rằng điều này cải thiện hiệu suất so với vanilla cross-attention với chi phí tính toán thấp hơn, đạt được kết quả state of the art trên một số benchmark. Công việc tương lai sẽ liên quan đến mở rộng MBT sang các phương thức khác như văn bản và optical flow.

Hạn chế: Fusion layer là một siêu tham số và có thể cần được điều chỉnh cụ thể cho các nhiệm vụ và bộ dữ liệu khác nhau. Chúng tôi cũng chỉ khám phá hợp nhất có giám sát hoàn toàn, và công việc tương lai sẽ giải quyết các mở rộng cho khung self-supervised learning.

Tác động rộng hơn: Các chiến lược hợp nhất đa phương thức quan trọng cho machine learning, vì hợp nhất thông tin bổ sung từ các phương thức khác nhau có thể tăng độ bền khi áp dụng cho các ứng dụng thế giới thực. Chúng tôi cũng lưu ý rằng transformer nói chung là nặng về tính toán, có thể có tác động môi trường bất lợi. Chúng tôi đề xuất một phương pháp hợp nhất token thông qua bottleneck giúp giảm độ phức tạp tính toán khi áp dụng transformer cho hợp nhất đa phương thức. Cuối cùng, chúng tôi quan sát thấy rằng các bộ dữ liệu huấn luyện chứa các bias có thể làm cho các mô hình được huấn luyện trên chúng không phù hợp cho các ứng dụng nhất định. Do đó có thể mọi người sử dụng các mô hình phân loại (có chủ ý hoặc không) để đưa ra các quyết định tác động đến các nhóm khác nhau trong xã hội một cách khác nhau, và điều quan trọng là phải ghi nhớ điều này khi triển khai, phân tích và xây dựng dựa trên các mô hình này.

Lời cảm ơn: Chúng tôi muốn cảm ơn Joao Carreira cho các thảo luận hữu ích về Perceiver [32].
