# 2305.14839.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2305.14839.pdf
# Kích thước tệp: 3048731 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
PaCE: Tiền huấn luyện Đối thoại Đa phương thức Thống nhất với
Các Chuyên gia Tiến bộ và Tổng hợp
Yunshui Li1,2∗†Binyuan Hui3∗Zhichao Yin1,4Min Yang1‡Fei Huang3Yongbin Li3‡
1Viện Công nghệ Tiên tiến Thâm Quyến, Viện Hàn lâm Khoa học Trung Quốc
2Đại học Viện Hàn lâm Khoa học Trung Quốc
3Học viện DAMO, Tập đoàn Alibaba
4Đại học Khoa học và Công nghệ Trung Quốc
{ys.li, min.yang}@siat.ac.cn, {binyuan.hby, shuide.lyb}@alibaba-inc.com
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/pace
Tóm tắt
Nhận thức thông tin đa phương thức và thực hiện đối thoại với con người là mục tiêu dài hạn của trí tuệ nhân tạo. Tiền huấn luyện thường được coi là một phương pháp hiệu quả cho đối thoại đa phương thức. Tuy nhiên, do tính sẵn có hạn chế của dữ liệu đối thoại đa phương thức, vẫn còn rất ít nghiên cứu về tiền huấn luyện đối thoại đa phương thức. Một thử thách hấp dẫn khác nổi lên từ bản chất bao quát của đối thoại đa phương thức, bao gồm nhiều phương thức và nhiệm vụ khác nhau. Hơn nữa, các hình thức nhiệm vụ mới có thể xuất hiện tại những thời điểm không thể dự đoán trong tương lai. Do đó, các mô hình đối thoại đa phương thức được thiết kế cần phải có đủ tính linh hoạt để thích ứng với các kịch bản như vậy. Bài báo này đề xuất PaCE, một khung tiền huấn luyện đối thoại đa phương thức thống nhất, có cấu trúc, tổng hợp. Nó sử dụng sự kết hợp của một số chuyên gia cơ bản để thích ứng với nhiều nhiệm vụ liên quan đến đối thoại và có thể được tiền huấn luyện bằng cách sử dụng dữ liệu đối thoại hạn chế và dữ liệu đa phương thức không phải đối thoại rộng rãi. Hơn nữa, chúng tôi đề xuất một phương pháp huấn luyện tiến bộ trong đó các chuyên gia cũ từ quá khứ có thể hỗ trợ các chuyên gia mới, tạo điều kiện cho việc mở rộng khả năng của họ. Kết quả thực nghiệm cho thấy PaCE đạt được kết quả tốt nhất hiện tại trên tám điểm chuẩn đối thoại đa phương thức.

1 Giới thiệu
Cho phép giao tiếp liền mạch giữa con người và máy móc là một mục tiêu lâu dài của nghiên cứu trí tuệ nhân tạo. Sự xuất hiện gần đây của chatGPT1 đã tăng niềm tin vào khả năng đạt được mục tiêu này. Ngoài việc sử dụng ngôn ngữ văn bản như một giao diện duy nhất giữa con người và máy móc, việc nhận thức và sử dụng thông tin đa phương thức, đặc biệt là thông tin hình ảnh, đã trở thành một khả năng quan trọng được gọi là đối thoại đa phương thức (Shuster et al., 2020; Sun et al., 2021).

Để tạo điều kiện cho nghiên cứu về đối thoại đa phương thức, nhiều nhiệm vụ và bộ dữ liệu cụ thể đã xuất hiện trong cộng đồng (Das et al., 2017; Shuster et al., 2018; Feng et al., 2022; Long et al., 2023). Tuy nhiên, tổng lượng dữ liệu vẫn còn hạn chế. Hơn nữa, đối thoại đa phương thức đưa ra thách thức lớn hơn so với track đối thoại chỉ có văn bản truyền thống (Hui et al., 2021; He et al., 2022; Si et al., 2022), vì nó liên quan đến việc tích hợp các phương thức khác nhau và các kịch bản nhiệm vụ phức tạp hơn. Như được thể hiện trong Hình 1, các nhiệm vụ trung tâm của đối thoại đa phương thức bao gồm phân loại ý định đa phương thức (Zang et al., 2021), truy xuất đối thoại đa phương thức (Das et al., 2017; Zang et al., 2021), theo dõi trạng thái đối thoại đa phương thức (Liao et al., 2021), và tạo phản hồi đa phương thức (Kottur et al., 2021). Mặc dù tiền huấn luyện đã trở thành sự đồng thuận cho học đa nhiệm vụ trong học máy (Devlin et al., 2018; Radford et al., 2019, 2021), nghiên cứu về tiền huấn luyện mô hình cho đối thoại đa phương thức là một lĩnh vực chưa được khám phá đầy đủ.

[Chào bạn, tôi đến từ Los Angeles và quan tâm đến việc thăm một bảo tàng lịch sử để tìm hiểu thêm về văn hóa peranakan. Bạn có khuyến nghị gì không?]

[Xin chào, nếu bạn du lịch đến châu Á, Cung điện Hoàng gia rất đáng để xem, bạn có thể thấy nhiều hiện vật cổ ở đó.]

[Wow, điều đó nghe có vẻ thú vị với tôi. Bạn có thể kể cho tôi thêm về một số câu chuyện liên quan đến Cung điện Hoàng gia không?]

[Cung điện Hoàng gia nằm trong Tử Cấm Thành, là cung điện hoàng gia của các triều đại Minh và Thanh ở Trung Quốc. Nó được gọi là "Tử Cấm Thành" vì người bình thường không được phép vào mà không có sự cho phép.]

[Nơi này trong bức ảnh có đáng để đến không? Nó trông bí ẩn, bạn có thể giới thiệu cho tôi một chút về nó không?]

[Chắc chắn! Đền Thiên Đàn cũng là một địa điểm có ý nghĩa văn hóa, nổi tiếng với việc sử dụng lịch sử như một địa điểm cho các lễ tế hoàng gia trong Trung Quốc cổ đại.]

[Tuyệt vời! Bạn có thể hỗ trợ tôi đặt chuyến bay đến Bắc Kinh vào sáng thứ Ba lúc 11 giờ sáng không?]

[Chắc chắn, chuyến bay sớm nhất từ Los Angeles đến Bắc Kinh là vào sáng thứ Ba lúc 11 giờ sáng. Được rồi, việc đặt chỗ đã thành công.]

[Phân loại Ý định Đa phương thức] [Truy xuất Đối thoại Đa phương thức (T2I)] [Truy xuất Đối thoại Đa phương thức (I2T)] [Theo dõi Trạng thái Đối thoại Đa phương thức] [Tạo Phản hồi Đa phương thức]

Hình 1: Một ví dụ về đối thoại đa phương thức, bao gồm nhiều nhiệm vụ, bao gồm phân loại ý định đa phương thức, theo dõi trạng thái đa phương thức, truy xuất đối thoại đa phương thức và tạo phản hồi.

--- TRANG 2 ---

phân loại (Zang et al., 2021), truy xuất đối thoại đa phương thức (Das et al., 2017; Zang et al., 2021), theo dõi trạng thái đối thoại đa phương thức (Liao et al., 2021), và tạo phản hồi đa phương thức (Kottur et al., 2021). Mặc dù tiền huấn luyện đã trở thành sự đồng thuận cho học đa nhiệm vụ trong học máy (Devlin et al., 2018; Radford et al., 2019, 2021), nghiên cứu về tiền huấn luyện mô hình cho đối thoại đa phương thức là một lĩnh vực chưa được khám phá đầy đủ.

Trong bài báo này, chúng tôi tập trung vào việc xây dựng các mô hình tiền huấn luyện của đối thoại đa phương thức. Một thách thức chính là thống nhất các phương thức và hình thức nhiệm vụ khác nhau, và tận dụng tối đa dữ liệu đối thoại đa phương thức và không phải đối thoại hiện có. Một xu hướng phổ biến gần đây trong các nhiệm vụ văn bản là xây dựng các mô hình nền tảng tiền huấn luyện thống nhất bằng học đa nhiệm vụ, ví dụ như T5 (Raffel et al., 2020). Tuy nhiên, nó cố gắng trộn lẫn tất cả các nhiệm vụ được học từ đầu do đó khó kiểm soát quá trình học, điều này hoàn toàn là một hộp đen. Mặc dù kiến trúc Hỗn hợp Chuyên gia (MoE) (Fedus et al., 2021; Du et al., 2022) cố gắng chọn các chuyên gia độc lập cho mỗi mẫu đầu vào thông qua định tuyến cấp token, nó thiếu ngữ nghĩa cụ thể, tức là hoàn toàn không biết các chuyên gia chịu trách nhiệm về điều gì. Chúng tôi hy vọng tìm một cách mới để xử lý nhiều nhiệm vụ đối thoại đa phương thức đồng thời và kết hợp các kỹ năng cụ thể hiện có để học các nhiệm vụ mới hiệu quả hơn.

Để đạt được mục tiêu này, chúng tôi đề xuất PaCE, một khung tiền huấn luyện đối thoại đa phương thức thống nhất với Các Chuyên gia Tiến bộ và Tổng hợp. Đầu tiên, chúng tôi phân tách đối thoại đa phương thức phức tạp thành các khả năng con cơ bản có thể được học với dữ liệu cụ thể. Khác với MoE truyền thống, mỗi chuyên gia trong PaCE được thiết kế riêng cho một khả năng con cơ bản cụ thể của đối thoại đa phương thức, bao gồm CAPTION, CONTEXT, IMAGE, GROUNDING và GENERATION. Thứ hai, chúng tôi đề xuất một chiến lược tiền huấn luyện tiến bộ để phát triển mô hình bằng cách kiểm soát sự kết hợp của các chuyên gia trong các giai đoạn tiền huấn luyện khác nhau. Cụ thể, trong giai đoạn I, chúng tôi đầu tiên huấn luyện trên dữ liệu đa phương thức không phải đối thoại để có được các chuyên gia CAPTION, IMAGE và GROUNDING. Trong giai đoạn II, chúng tôi huấn luyện chuyên gia CONTEXT, được hướng dẫn bởi chuyên gia CAPTION trên dữ liệu đối thoại đa phương thức để học các phụ thuộc trong ngữ cảnh. Hơn nữa, một chuyên gia GENERATION đối thoại được tạo ra bằng cách thêm một nhiệm vụ tạo phản hồi dựa trên các chuyên gia đã học trước đó. Thứ ba, để tiền huấn luyện PaCE, chúng tôi thu thập một kho dữ liệu đối thoại đa phương thức với 1,4 triệu đối thoại và một kho dữ liệu đa phương thức không phải đối thoại với 4 triệu mẫu. Một khi việc tiền huấn luyện PaCE kết thúc, chúng tôi có thể linh hoạt chọn các chuyên gia khả năng khác nhau để giải quyết một nhiệm vụ downstream cụ thể.

Như được minh họa trong Hình 2, PaCE đạt được hiệu suất tốt nhất hiện tại trên một loạt rộng các điểm chuẩn đối thoại đa phương thức bao gồm bốn nhiệm vụ downstream đa dạng, tức là phân loại ý định đa phương thức, truy xuất đối thoại đa phương thức, theo dõi trạng thái đa phương thức, và tạo phản hồi đa phương thức. Điều này cho thấy PaCE không chỉ có kiến trúc mô hình linh hoạt mà còn thể hiện các phương pháp huấn luyện thích ứng, dẫn đến hiệu suất đáng chú ý.

[Hình 2: PaCE đạt được hiệu suất tốt nhất hiện tại trên một loạt rộng các nhiệm vụ đối thoại so với các mô hình tùy chỉnh hoặc mô hình nền tảng khác.]

2 Công trình liên quan

Mô hình Thị giác-Ngôn ngữ Tiền huấn luyện Mô hình tiền huấn luyện, với những thành công trong xử lý ngôn ngữ tự nhiên (Devlin et al., 2018; Radford et al., 2019), đã làm nổi ra một cuộc cách mạng trong Học Đa phương thức. ViLBERT (Lu et al., 2019) là công trình đầu tiên thích ứng kiến trúc giống BERT cho mô hình hóa thị giác-ngôn ngữ, cho phép học biểu diễn chung của hình ảnh và văn bản. ViLT (Kim et al., 2021) xây dựng mô-đun thị giác theo cách tương tự như mô-đun văn bản với một Transformer thống nhất (Vaswani et al., 2017), loại bỏ nhu cầu trích xuất đặc trưng hình ảnh tốn tài nguyên và tăng tốc đáng kể mô hình. CLIP (Radford et al., 2021) sử dụng học tương phản để căn chỉnh trực tiếp hình ảnh với văn bản ngôn ngữ tự nhiên, loại bỏ các ràng buộc của các danh mục hình ảnh được định nghĩa trước. ALIGN (Jia et al., 2021)

--- TRANG 3 ---

và Florence (Yuan et al., 2021) mở rộng thêm ý tưởng này trên các cặp hình ảnh-văn bản ồn ào hơn nhưng lớn hơn. Các mô hình này đã chứng minh khả năng học biểu diễn hình ảnh và văn bản mạnh mẽ cho các nhiệm vụ căn chỉnh xuyên phương thức. Ngoài ra, một số mô hình (Cho et al., 2021; Wang et al., 2021, 2022; Yu et al., 2022; Alayrac et al., 2022) đã sử dụng các mô hình tự hồi quy để mô hình hóa mối liên kết giữa hình ảnh và văn bản, sử dụng một phương pháp tạo thống nhất để xây dựng nhiệm vụ theo cách end-to-end. Mặc dù các mô hình thị giác-ngôn ngữ tiền huấn luyện đã cho thấy kết quả đầy hứa hẹn, chúng chủ yếu tập trung vào văn bản chú thích vốn khác biệt nội tại với các cuộc trò chuyện của con người (Kulhánek et al., 2021). Theo hiểu biết của chúng tôi, mô hình PaCE được đề xuất là mô hình tiền huấn luyện đối thoại đa phương thức đầu tiên.

Mô hình hóa Đối thoại Đa phương thức Nhiều công trình tiên tiến đã được đề xuất cùng với sự phát triển của các bộ dữ liệu đối thoại đa phương thức (Das et al., 2017; Mostafazadeh et al., 2017; Shuster et al., 2018; Zang et al., 2021; Zheng et al., 2021; Kottur et al., 2021; Liao et al., 2021; Feng et al., 2022). Một số công trình mô hình hóa đối thoại (Qi et al., 2020; Lee et al., 2021) đã được tiến hành để cải thiện hiệu suất của các agent đối thoại trong đối thoại dựa trên hình ảnh. Zang et al. (2021) đề xuất một mô hình mã hóa kép sử dụng nhãn đối tượng để mã hóa đặc trưng hình ảnh nhằm thực hiện nhiệm vụ truy xuất hình ảnh dựa trên đối thoại. Sau đó, các nhà nghiên cứu (Yang et al., 2021; Chen et al., 2021) đã khám phá việc làm phong phú biểu đạt văn bản của các phản hồi đối thoại được tạo thông qua các cảnh thị giác liên kết. Đối với các nhiệm vụ phản hồi văn bản, Zheng et al. (2021) đề xuất một mô hình tạo đối thoại đa phương thức dựa trên kiến trúc Seq2Seq, được chứng minh là vượt trội hơn mô hình Seq2Seq văn bản. Lee et al. (2022) đề xuất một mô hình mã hóa-giải mã đa phương thức chung để kết hợp các đầu vào thị giác. Tuy nhiên, các mô hình trên đã chứng minh thành công trong các nhiệm vụ con cụ thể với một bộ dữ liệu cụ thể, không thể đáp ứng yêu cầu của một loạt rộng các nhiệm vụ đối thoại đa phương thức. Để giải quyết thách thức này, chúng tôi đề xuất một mô hình tiền huấn luyện đối thoại đa phương thức thống nhất dựa trên chiến lược chia để trị, có thể kết hợp các chuyên gia khác nhau để hoàn thành một loạt nhiệm vụ.

3 Xây dựng Dữ liệu Tiền huấn luyện

Trong bài báo này, chúng tôi thu thập cả dữ liệu đa phương thức không phải đối thoại và dữ liệu đối thoại đa phương thức cho việc tiền huấn luyện PaCE. Thống kê tổng thể của các kho dữ liệu tiền huấn luyện mà chúng tôi thu thập được thể hiện trong Bảng 1.

[Bảng 1: Thống kê của các kho dữ liệu tiền huấn luyện mà chúng tôi thu thập.]

Dữ liệu Đa phương thức Không phải Đối thoại (MultiNonDialog) Tương tự như công trình trước đó (Kim et al., 2021), chúng tôi đầu tiên thu thập bốn bộ dữ liệu đa phương thức không phải đối thoại để học biểu diễn hình ảnh và văn bản, bao gồm MSCOCO (Lin et al., 2014), VG (Krishna et al., 2017), SBU (Ordonez et al., 2011) và GCC (Sharma et al., 2018). Trong MultiNonDialog, mỗi hình ảnh được đi kèm với một hoặc nhiều chú thích có độ dài thường được giới hạn ở 20 token. Vì GCC và SBU chỉ cung cấp URL hình ảnh, chúng tôi thu thập hình ảnh qua các URL đã cho vẫn còn khả dụng.

Dữ liệu Đối thoại Đa phương thức (MultiDialog) Chúng tôi thu thập sáu kho dữ liệu cuộc trò chuyện đa phương thức hiện có từ nhật ký trò chuyện diễn đàn trực tuyến (Das et al., 2017; Shuster et al., 2018; Zang et al., 2021; Feng et al., 2022) đến các cuộc trò chuyện dịch vụ khách hàng (Liao et al., 2021; Kottur et al., 2021) và xây dựng một kho dữ liệu đối thoại đa phương thức quy mô lớn. Để đảm bảo rằng mỗi cuộc trò chuyện có ít nhất một hình ảnh tương ứng, chúng tôi loại bỏ các cuộc trò chuyện chỉ có văn bản từ các bộ dữ liệu gốc. Ngoài ra, để đáp ứng yêu cầu của việc tiền huấn luyện Giai đoạn II, chúng tôi sử dụng mô hình BLIP (Li et al., 2022b) được triển khai bởi Li et al. (2022a) để tạo chú thích văn bản phù hợp cho mỗi hình ảnh. Các chú thích được giới hạn ở 20 token.

4 Phương pháp Tiền huấn luyện

Cho một tập hợp n mẫu đối thoại đa phương thức D={(Ui, Ri)}^n_{i=1}, trong đó Ui và Ri lần lượt đại diện cho ngữ cảnh và phản hồi đối thoại. So với đối thoại văn bản truyền thống, cả Ui={u^m_k}^K_{k=1} và Ri={r^m_q}^Q_{q=1} có thể kết hợp nhiều loại thông tin khác nhau bao gồm văn bản và hình ảnh thị giác, trong đó K và Q là số lượng phần tử, và m∈{t,v} biểu thị phương thức của Ui (hoặc Ri). Ký hiệu t chỉ ra các phát ngôn văn bản, trong khi v chỉ ra hình ảnh thị giác.

Chúng tôi thiết kế một chiến lược tiền huấn luyện chia để trị cho đối thoại đa phương thức. Cụ thể, chúng tôi phân tách đối thoại đa phương thức phức tạp thành năm khả năng con cơ bản và thiết kế năm chuyên gia tương ứng (tức là, các chuyên gia CAPTION, CONTEXT, IMAGE, GROUNDING, và GENERATION). Sau đó, chúng tôi đề xuất một chiến lược huấn luyện tiến bộ để phát triển mô hình bằng cách kiểm soát sự kết hợp của các chuyên gia trong các giai đoạn tiền huấn luyện khác nhau. Tiếp theo, chúng tôi mô tả chi tiết mô-đun học biểu diễn đầu vào, chiến lược tiền huấn luyện chia để trị, các mục tiêu tiền huấn luyện, và quá trình tinh chỉnh.

4.1 Học Biểu diễn Đầu vào

Mô hình được đề xuất được thiết kế để xử lý dữ liệu đầu vào từ hai phương thức: biểu diễn thị giác và biểu diễn văn bản.

Biểu diễn Thị giác Ngữ cảnh và phản hồi đối thoại có thể là dữ liệu thị giác hoặc văn bản. Chúng tôi sử dụng Vision Transformer (Dosovitskiy et al., 2020) để học biểu diễn thị giác của hình ảnh. Chính thức, chúng tôi xử lý hình ảnh thị giác v∈R^{H×W×C} bằng cách chia nó thành N=HW/P^2 vá v_p∈R^{N×(P^2C)}, trong đó C là số kênh, (H, W) là độ phân giải của hình ảnh đầu vào, và P là độ phân giải vá. Điều này cho phép mô hình trích xuất các đặc trưng có ý nghĩa từ hình ảnh bằng cách xem xét nó như một tập hợp các vùng nhỏ, thay vì một mảng lớn pixel duy nhất. Các vá hình ảnh sau đó được làm phẳng thành vector và được xử lý bởi một phép chiếu tuyến tính sử dụng ma trận trọng số W_V∈R^{(P^2·C)×E} và một embedding vị trí W^{pos}_V∈R^{(N+1)×E}, dẫn đến embedding vá v̄∈R^{N×E}, trong đó E là chiều của embedding. Embedding vị trí được sử dụng để thêm thông tin bổ sung về vị trí của vá trong hình ảnh. Cuối cùng, chúng tôi có được biểu diễn thị giác H^v_0 sau khi cộng embedding vá và embedding vị trí.

Biểu diễn Văn bản Văn bản đầu vào t∈R^{L×|O|} được nhúng vào một biểu diễn dày đặc t̄∈R^{L×E} bằng cách sử dụng ma trận embedding từ W_T∈R^{|O|×E} và ma trận embedding vị trí W^{pos}_T∈R^{(L+1)×E}, trong đó |O| là kích thước từ vựng, L là độ dài văn bản, và E là chiều của embedding. Đáng chú ý rằng chúng tôi thường nối ngữ cảnh với phát ngôn hiện tại để tạo thành đầu vào văn bản cuối cùng. Biểu diễn văn bản có thể được biểu thị là H^t_0.

4.2 Chiến lược Tiền huấn luyện Chia để Trị

Chúng tôi thiết kế một chiến lược tiền huấn luyện mới theo cách chia để trị. Cụ thể, chúng tôi đầu tiên chia đối thoại đa phương thức phức tạp thành một số vấn đề con, có thể được học theo cách dễ dàng hơn. Các giải pháp cho các vấn đề con sau đó được kết hợp để đưa ra giải pháp cho các nhiệm vụ đối thoại đa phương thức downstream khác nhau.

Kiến trúc Đa chuyên gia PaCE áp dụng một phần mở rộng của Transformer chuẩn, học nhiều chuyên gia ngữ nghĩa thay vì một mạng feed-forward (FFN) duy nhất như trong Transformer gốc (Bao et al., 2021). Cụ thể, các chuyên gia chia sẻ thông tin từ cả hai phương thức văn bản và thị giác thông qua cơ chế attention đa đầu (MSA), trong khi mỗi chuyên gia FFN_{expert} có các tham số riêng độc đáo để học một biểu diễn ngữ nghĩa khác biệt. Chính thức, thông tin độc đáo, được thu được bằng cách chuyển đổi chuyên gia trong mỗi khối, có thể được công thức hóa như sau:

H'_l = MSA(LN(H_{l-1})) + H_{l-1}
H^{expert_k}_l = FFN_{expert_k}(LN(H'_l)) + H'_l (1)

trong đó H_{l-1} (l∈[1,L]) đại diện cho biểu diễn đầu ra của lớp l-1 và L là số khối Transformer. H^{expert_k}_l là biểu diễn của chuyên gia thứ k. Biểu diễn đầu vào có thể được công thức hóa là H_0 = [H^v_0, H^t_0]. Ở đây, MSA và LN lần lượt là attention tự đa đầu chuẩn và chuẩn hóa lớp.

Chuyên gia Phương thức và Khả năng Như được minh họa trong Hình 3, chúng tôi chia nhiệm vụ đối thoại đa phương thức phức tạp thành năm vấn đề con dễ dàng hơn bao gồm mô hình hóa CAPTION, mô hình hóa CONTEXT, mô hình hóa IMAGE, GROUNDING, và GENERATION. Chúng tôi thiết kế một chuyên gia ngữ nghĩa để giải quyết mỗi vấn đề con. Năm chuyên gia này có thể được chia thành hai danh mục: chuyên gia phương thức (chuyên gia CAPTION và IMAGE) và chuyên gia khả năng (chuyên gia GROUNDING, CONTEXT MODELING và GENERATION) được thiết kế riêng cho đối thoại đa phương thức. Cuối cùng, chúng tôi kích hoạt các chuyên gia phương thức và khả năng theo cách phân cấp, với các lớp dưới (L-F) chỉ kích hoạt các chuyên gia phương thức và các lớp trên F kích hoạt các chuyên gia khả năng, trong đó F là một siêu tham số được định nghĩa trước.

Kết hợp Chuyên gia cho Các Nhiệm vụ Khác nhau Chúng tôi đề xuất một chiến lược tiền huấn luyện cascade tiến bộ giải quyết các nhiệm vụ đối thoại đa phương thức khác nhau bằng cách kết hợp thích ứng các giải pháp cho các vấn đề con. Chúng tôi sẽ giới thiệu chi tiết về tiền huấn luyện cascade tiến bộ trong Phần 4.3.

4.3 Mục tiêu Tiền huấn luyện

Quá trình tiền huấn luyện cascade tiến bộ của chúng tôi bao gồm ba giai đoạn, mỗi giai đoạn với một mục tiêu tiền huấn luyện được thiết kế riêng.

Giai đoạn I: Khớp Hình ảnh-Văn bản Trong giai đoạn I, tương tự như ViLT (Kim et al., 2021), chúng tôi sử dụng dữ liệu đa phương thức không phải đối thoại D_n để học căn chỉnh liên phương thức cơ bản, và giai đoạn này chỉ bao gồm ba chuyên gia, bao gồm chuyên gia CAPTION, chuyên gia IMAGE và chuyên gia GROUNDING. Như được mô tả trong Hình 3(a), sau embedding từ và vá, văn bản và hình ảnh được xử lý riêng biệt thành biểu diễn văn bản và hình ảnh bởi các chuyên gia CAPTION và IMAGE chuyên biệt. Các biểu diễn này sau đó được hợp nhất và đưa vào chuyên gia GROUNDING, tạo ra một biểu diễn thống nhất của hình ảnh và văn bản. Sau đó chúng tôi sử dụng biểu diễn của token '[CLS]' từ đầu ra chuyên gia làm đầu vào cho một mạng phân loại nhị phân để dự đoán sự căn chỉnh giữa văn bản và hình ảnh hiện tại. Hàm mất mát của khớp hình ảnh-văn bản được định nghĩa là:

L_{itm} = E_{(V,T)∼D_n} CE(y_{itm}, p_{itm}(V,T)) (2)

Ngoài L_{itm}, chúng tôi cũng sử dụng mất mát MLM L_{mlm} trong giai đoạn này để hiểu phương thức văn bản độc đáo. Cụ thể, theo phương pháp của BERT, chúng tôi chọn ngẫu nhiên các token trong chuỗi văn bản và thay thế chúng bằng token [MASK]. Mô hình được huấn luyện để dự đoán các token bị che này bằng cách sử dụng ngữ cảnh của các token không bị che còn lại và các manh mối thị giác. Chúng tôi áp dụng xác suất che 15%. Các vector đầu ra cuối cùng của các token bị che sau đó được đưa vào một bộ phân loại trên toàn bộ từ vựng văn bản, với mất mát huấn luyện là mất mát cross-entropy.

L_{mlm} = E_{(V,T̂)∼{D_n∪D_d}} CE(y_{mask}, p_{mask}(V,T̂)) (3)

trong đó T̂ là văn bản bị che, V là hình ảnh gốc và p_{mask}(V,T̂) biểu thị xác suất dự đoán của mô hình cho token bị che T̂. D_n và D_d lần lượt đại diện cho dữ liệu đa phương thức không phải đối thoại và đối thoại.

Mất mát chung trong giai đoạn I có thể được công thức hóa là:
L^I_{stage} = L_{itm} + L_{mlm} (4)

Giai đoạn II: Khớp Hình ảnh-Ngữ cảnh Trong giai đoạn II, chúng tôi sử dụng dữ liệu đối thoại đa phương thức D_d để tiền huấn luyện PaCE, nhằm mô hình hóa ngữ cảnh đối thoại cho các nhiệm vụ đối thoại đa phương thức. Ở giai đoạn này, chuyên gia CAPTION sẽ được kích hoạt ngoài ba chuyên gia từ giai đoạn đầu tiên. Cụ thể, trong giai đoạn thứ hai, ngữ cảnh đối thoại C được đưa vào chuyên gia CONTEXT, hình ảnh V được đưa vào chuyên gia IMAGE, và các chú thích hình ảnh tương ứng T được đưa vào chuyên gia CAPTION. Hàm mất mát của khớp hình ảnh-ngữ cảnh được định nghĩa là:

L_{icm} = E_{(V,T,C)∼D_d} CE(y_{icm}, p_{icm}(V,T,C)) (5)

Ngoài ra, chúng tôi sử dụng chuyên gia CAPTION đã học trong Giai đoạn I như một giáo viên để tạo điều kiện cho việc học của chuyên gia CONTEXT.

L_{tca} = ||H^t_{L-F} - H^c_{L-F}||^2_2, (6)

trong đó H^t_{L-F} và H^c_{L-F} lần lượt là đầu ra của lớp thứ {L-F} của chuyên gia CAPTION và chuyên gia CONTEXT.

Bên cạnh đó, chúng tôi cũng sử dụng mất mát MLM trong giai đoạn II như được định nghĩa trong giai đoạn I, và mất mát chung L^{II}_{stage} trong giai đoạn II có thể được công thức hóa là:

L^{II}_{stage} = L_{icm} + L_{tca} + L_{mlm} (7)

Giai đoạn III: Mô hình hóa Tạo Giai đoạn thứ ba nhằm cho phép mô hình tạo phản hồi. Chuyên gia GENERATION được kích hoạt, và đầu vào cho chuyên gia này bao gồm chuyên gia CONTEXT và chuyên gia IMAGE. Hàm mất mát trong giai đoạn III được định nghĩa như sau:

L^{III}_{stage} = -∑^N_{n=1} log p_{rgm}(C_n|V, C_{<n}) (8)

Ở đây, chúng tôi mô hình hóa khả năng tạo bằng tự hồi quy, tức là sử dụng lịch sử đối thoại quá khứ C_{<n} và hình ảnh liên quan V để dự đoán lượt hiện tại C_n của đối thoại.

4.4 Tinh chỉnh trên Các Nhiệm vụ Downstream

Một khi việc tiền huấn luyện PaCE kết thúc, chúng tôi thực hiện tinh chỉnh trên các nhiệm vụ downstream cụ thể. Nhờ vào phương pháp tiền huấn luyện chia để trị của chúng tôi, chúng tôi có thể linh hoạt chọn các chuyên gia khả năng khác nhau để giải quyết một nhiệm vụ downstream cụ thể. Cụ thể, đối với các nhiệm vụ hiểu, bao gồm dự đoán ý định, truy xuất đối thoại, và theo dõi trạng thái đối thoại, chúng tôi kích hoạt chuyên gia CONTEXT, chuyên gia IMAGE, và chuyên gia GROUNDING. Đối với nhiệm vụ tạo, tức là theo dõi trạng thái đối thoại, và tạo phản hồi, chúng tôi kích hoạt chuyên gia CONTEXT, chuyên gia IMAGE, và chuyên gia GENERATION.

5 Thí nghiệm

5.1 Bộ dữ liệu Downstream

Để đánh giá toàn diện PaCE của chúng tôi, chúng tôi tiến hành các thí nghiệm rộng rãi trên bảy bộ dữ liệu thuộc bốn nhiệm vụ downstream.

Dự đoán Ý định Đa phương thức Đối với dự đoán ý định đa phương thức, PhotoChat (Zang et al., 2021) và MMDialog (Feng et al., 2022) được chọn làm bộ dữ liệu chuẩn. Nhiệm vụ này nhằm xác định ý định cụ thể của người dùng trong ngữ cảnh đa phương thức. Cụ thể hơn, nó dự đoán xác suất chia sẻ ảnh trong lượt trò chuyện sắp tới.

Truy xuất Đối thoại Đa phương thức Đối với truy xuất văn bản sang hình ảnh, chúng tôi chọn PhotoChat (Zang et al., 2021) làm bộ dữ liệu chuẩn. Nó bao gồm 12k đối thoại, mỗi đối thoại đi kèm với một bức ảnh người dùng được trao đổi trong cuộc trò chuyện. Mục tiêu của nhiệm vụ này là chọn bức ảnh phù hợp nhất cho ngữ cảnh đối thoại. Đối với truy xuất hình ảnh sang văn bản, chúng tôi chọn Image-Chat (Shuster et al., 2018) để đánh giá mô hình của chúng tôi, bao gồm 202k đối thoại trên 202k hình ảnh.

Theo dõi Trạng thái Đối thoại Đa phương thức Các bộ dữ liệu MMConv (Liao et al., 2021) và SIMMC2.0 (Kottur et al., 2021) cung cấp một cơ sở tốt để thực hiện theo dõi trạng thái đối thoại đa phương thức. Bộ dữ liệu MMConv chứa 5.1k đối thoại được thu thập bằng cách cho phép các cuộc trò chuyện đa phương thức giữa các cặp nhập vai người-với-người dưới các kịch bản du lịch thực tế. Ngược lại, kho dữ liệu SIMMC2.0 bao gồm 11.000 đối thoại hướng nhiệm vụ trong lĩnh vực mua sắm được dựa trên các ngữ cảnh nhập vai và có tính ảnh thực.

Tạo Phản hồi Đa phương thức Tạo phản hồi thích hợp để hoàn thành nhiệm vụ thỏa đáng là mục tiêu cuối cùng của các agent đối thoại hướng nhiệm vụ. Trong nhiệm vụ này, chúng tôi chọn MMConv (Liao et al., 2021) và SIMMC2.0 (Kottur et al., 2021) làm bộ dữ liệu chuẩn.

5.2 Thiết lập Thí nghiệm

Chúng tôi sử dụng tokenizer bert-base-uncased để tokenize đầu vào văn bản. Chúng tôi học các tham số liên quan đến embedding văn bản từ đầu, thay vì tinh chỉnh chúng từ BERT tiền huấn luyện. Đối với tất cả các thí nghiệm, chúng tôi sử dụng optimizer AdamW (Loshchilov and Hutter, 2017) với tỷ lệ học cơ bản 10^{-4} và weight decay 10^{-2}. Tỷ lệ học được khởi động trong 10% tổng số bước huấn luyện và giảm tuyến tính về không cho phần còn lại của việc huấn luyện. Chúng tôi đặt tổng số lớp Transformer L là 12, với số lớp F cho Transformer trên được đặt là 3. Chúng tôi khởi tạo trọng số Transformer với ViT tiền huấn luyện (Dosovitskiy et al., 2020). Trong quá trình tiền huấn luyện, chúng tôi sử dụng 200K bước, 25K bước, và 10K bước, tương ứng cho ba giai đoạn trên 8 GPU NVIDIA A100 với batch size 4.096.

5.3 Phương pháp Đánh giá

Đối với dự đoán ý định, chúng tôi áp dụng điểm F1 làm thước đo đánh giá để đo lường hiệu quả của mô hình, tương tự như công trình trước đó (Zang et al., 2021). Đối với truy xuất đối thoại đa phương thức, chúng tôi sử dụng các thước đo đánh giá dựa trên xếp hạng như recall tại k bao gồm R@1, R@5 và R@10 phù hợp với các nghiên cứu trước đó (Zang et al., 2021; Shuster et al., 2018). Các thước đo này đo lường liệu các đầu ra văn bản hoặc thị giác đúng có được xếp hạng trong top k∈{1,5,10} vị trí trong số n phần tử ứng viên hay không. Đối với theo dõi trạng thái đối thoại đa phương thức, chúng tôi báo cáo điểm Categorical, Non-categorical và overall làm thước đo đánh giá theo (Liao et al., 2021). Để đo lường chất lượng tạo phản hồi, chúng tôi sử dụng BLEU (Papineni et al., 2002) làm thước đo đánh giá cho SIMMC2.0. Đối với MMConv, chúng tôi báo cáo điểm kết hợp (Comb.), được tính thông qua (Inform + Success) × 0.5 + BLEU như một thước đo đánh giá tổng thể như trong (Mehri et al., 2019).

5.4 So sánh Định lượng

Như được thể hiện trong Hình 2 và Bảng 2, PaCE chứng minh hiệu suất tốt nhất hiện tại trên một loạt rộng các nhiệm vụ đối thoại đa phương thức. Cụ thể, chúng tôi đã đạt được sự cải thiện đáng kể trên bộ dữ liệu PhotoChat và MMConv, với cải thiện 4.8 điểm trong truy xuất đối thoại đa phương thức và 21.2 điểm trong theo dõi trạng thái đối thoại đa phương thức, tương ứng. Đáng chú ý là PaCE có tổng số tham số là 338 triệu. Ngoài ra, vì một số chuyên gia có thể nhàn rỗi trong quá trình thực hiện các nhiệm vụ downstream cụ thể, kích thước tham số sẽ giảm thêm cho các nhiệm vụ downstream cụ thể. Dưới đây, chúng tôi cung cấp phân tích chi tiết về kết quả cho từng bộ dữ liệu nhiệm vụ con.

[Bảng 2: Kết quả thí nghiệm trên các điểm chuẩn đối thoại đa phương thức khác nhau. Chúng tôi so sánh PaCE với các mô hình tốt nhất hiện tại trước đó, bao gồm T5-3B (Raffel et al., 2020), Divter (Feng et al., 2022), SCAN (Lee et al., 2018), TransResNet (Shuster et al., 2018), BART-large (Lewis et al., 2019) và SimpleTOD (Hosseini-Asl et al., 2020).]

Dự đoán Ý định Đa phương thức Đối với bộ dữ liệu PhotoChat, chúng tôi báo cáo hiệu suất của các baseline mạnh như trong (Zang et al., 2021), bao gồm ALBERT-base (Lan et al., 2019), BERT (Devlin et al., 2018), T5-base, và T5-3B (Raffel et al., 2020). Đối với bộ dữ liệu MMDialog, chúng tôi áp dụng DE++, Divter (Feng et al., 2022), và ViLT (Kim et al., 2021) làm các mô hình baseline. Như được thể hiện trong Bảng 3, mặc dù một số mô hình như T5-3B lớn hơn nhiều so với chúng tôi, mô hình của chúng tôi vẫn đạt được hiệu suất tốt nhất trên tất cả các thước đo đánh giá.

[Bảng 3: Kết quả dự đoán ý định đa phương thức trên PhotoChat và MMDialog.]

Truy xuất Đối thoại Đa phương thức Đối với PhotoChat, chúng tôi so sánh PaCE với các baseline mạnh được báo cáo trong (Zang et al., 2021), bao gồm BM25 (Robertson et al., 2009), DE* (Zang et al., 2021), VSE++ (Faghri et al., 2017) và SCAN (Lee et al., 2018). Chúng tôi cũng thích ứng VLMo (Bao et al., 2021) và ViLT (Kim et al., 2021) để thực hiện truy xuất đối thoại đa phương thức. Kết quả trên PhotoChat được báo cáo trong Bảng 4, PaCE đạt được hiệu suất tốt hơn đáng kể so với các baseline hoạt động tốt nhất. Đối với Image-Chat, chúng tôi so sánh PaCE với TransResNet152 (Liao et al., 2021), VLMo và ViLT, và báo cáo kết quả baseline như trong Bảng 5. PaCE đạt được kết quả tốt nhất cho truy xuất đối thoại hình ảnh sang văn bản với cải thiện 3.0 về mặt Sum.

[Bảng 4: Truy xuất đối thoại đa phương thức trên PhotoChat.]

[Bảng 5: Truy xuất đối thoại đa phương thức trên Image-Chat.]

Theo dõi Trạng thái Đối thoại Đa phương thức Đối với bộ dữ liệu MMConv, chúng tôi so sánh PaCE với DS-DST (Zhang et al., 2019); đối với bộ dữ liệu SIMMC2.0, chúng tôi so sánh PaCE với GPT-2 (Radford et al., 2019), MTN (Le et al., 2019), BART-large và BART-base (Lewis et al., 2019). Kết quả trên MMConv và SIMMC2.0 được báo cáo lần lượt trong Bảng 6 và Bảng 7. PaCE có thể đạt được kết quả tốt nhất trên hầu hết các thước đo đánh giá. Đáng chú ý, chúng tôi quan sát thấy PaCE đạt được kết quả cạnh tranh ở quy mô tham số nhỏ hơn so với SOTA trước đó trong SIMMC2.0 slot F1.

[Bảng 6: Hiệu suất theo dõi trạng thái đối thoại đa phương thức trên MMConv.]

[Bảng 7: Theo dõi trạng thái đối thoại đa phương thức trên SIMMC2.0. Các thước đo đánh giá Slot F1 và Act. F1 được sử dụng để đánh giá nhiệm vụ theo dõi trạng thái đối thoại, trong khi BLEU được áp dụng để đánh giá tạo phản hồi.]

Tạo Phản hồi Đa phương thức Đối với nhiệm vụ tạo phản hồi, chúng tôi tiến hành thí nghiệm trên bộ dữ liệu SIMMC2.0 và MMConv. Đối với MMConv, chúng tôi áp dụng baseline mạnh SimpleTOD (Hosseini-Asl et al., 2020) được triển khai bởi (Liao et al., 2021). Chúng tôi tóm tắt kết quả thí nghiệm của SIMMC2.0 và MMConv trong Bảng 7 và Bảng 8, xác minh hiệu quả của mô hình trong cả nhiệm vụ phân biệt và tạo.

[Bảng 8: Hiệu suất tạo phản hồi đa phương thức trên MMConv.]

5.5 Nghiên cứu Ablation

Hiệu quả của Mục tiêu Tiền huấn luyện Để đánh giá hiệu quả của mỗi giai đoạn tiền huấn luyện, chúng tôi tiến hành nghiên cứu ablation bằng cách loại bỏ tiền huấn luyện Giai đoạn I (PaCE w/o L^I_{stage}), loại bỏ tiền huấn luyện Giai đoạn II (PaCE w/o L^{II}_{stage}), loại bỏ tiền huấn luyện Giai đoạn III (PaCE w/o L^{III}_{stage}), và loại bỏ cả Giai đoạn II và Giai đoạn III (PaCE only L^I_{stage}). Để so sánh công bằng, thiết lập thí nghiệm của nghiên cứu ablation phù hợp với các thí nghiệm chính, sử dụng cùng các siêu tham số và chiến lược tinh chỉnh downstream. Kết quả kiểm tra ablation trên PhotoChat và Image-Chat được cung cấp trong Bảng 9. Chúng tôi có thể quan sát thấy khớp hình ảnh-văn bản (Giai đoạn I) và khớp hình ảnh-ngữ cảnh (Giai đoạn II) đóng vai trò quan trọng nhất trong PaCE. Điều này nằm trong mong đợi của chúng tôi vì Giai đoạn I và Giai đoạn II là cơ sở của mô hình hóa tạo sau này (Giai đoạn III). Không ngạc nhiên khi kết hợp cả ba giai đoạn đạt được hiệu suất tốt nhất trên các bộ dữ liệu thí nghiệm. Chúng tôi cũng điều tra tác động của L_{tca} bằng cách loại bỏ nó khỏi tiền huấn luyện Giai đoạn II (được ký hiệu là PaCE w/o L_{tca}). Chúng tôi có thể quan sát thấy L_{tca} có tác động đáng kể đến hiệu suất của PaCE trong tiền huấn luyện Giai đoạn II.

Hiệu quả của Dữ liệu Tiền huấn luyện Ngoài ra, chúng tôi cũng tiến hành nghiên cứu ablation để xác minh tác động của các dữ liệu tiền huấn luyện khác nhau trên bộ dữ liệu PhotoChat và Image-Chat. Chúng tôi định nghĩa các mô hình chỉ sử dụng MultiNonDialog và MultiDialog để tiền huấn luyện lần lượt là PaCE only MultiNonDialog và PaCE only MultiDialog. Kết quả kiểm tra ablation trên PhotoChat và Image-Chat được cung cấp trong Bảng 10. Chúng tôi có thể quan sát thấy cả kho dữ liệu tiền huấn luyện MultiNonDialog và MultiDialog đều đóng góp cải thiện hiệu suất lớn cho PaCE. Điều này nằm trong mong đợi của chúng tôi vì dữ liệu MultiNonDialog giúp mô hình học được biểu diễn hình ảnh-văn bản ấn tượng và sự căn chỉnh của chúng, trong khi dữ liệu MultiDialog khuyến khích PaCE nắm bắt thông tin ngữ cảnh đối thoại.

[Bảng 9: Kết quả kiểm tra ablation trên nhiệm vụ truy xuất đối thoại đa phương thức bằng cách sử dụng các mục tiêu tiền huấn luyện khác nhau.]

[Bảng 10: Kết quả kiểm tra ablation trên nhiệm vụ truy xuất đối thoại đa phương thức bằng cách sử dụng các dữ liệu tiền huấn luyện khác nhau.]

5.6 Nghiên cứu Trường hợp

Để đánh giá PaCE một cách định tính, chúng tôi chọn hai cuộc trò chuyện mẫu từ bộ kiểm tra PhotoChat và Image-Chat, và minh họa các phản hồi được truy xuất bởi PaCE trong Hình 4 và Hình 5. Mô hình PaCE của chúng tôi có thể truy xuất các ứng viên có liên quan cao đến kịch bản trò chuyện. Đối với nhiệm vụ truy xuất văn bản sang hình ảnh (T2I), vì các hình ảnh ứng viên có thể khá tương tự, việc truy xuất chính xác hình ảnh đúng từ các ứng viên là thách thức. Mặc dù PaCE có thể không thu được hình ảnh đúng, chúng tôi vẫn có thể thu được các hình ảnh ứng viên liên quan.

[Hình 4: Hai trường hợp trên bộ kiểm tra PhotoChat. Đối với mỗi truy vấn đối thoại, chúng tôi hiển thị 5 hình ảnh được xếp hạng hàng đầu từ trái sang phải.]

[Hình 5: Hai trường hợp trên bộ kiểm tra Image-Chat. Đối với mỗi truy vấn đối thoại, chúng tôi hiển thị 5 phản hồi được xếp hạng hàng đầu từ trên xuống dưới.]

6 Kết luận

Trong bài báo này, chúng tôi đề xuất PaCE, một khung tiền huấn luyện đối thoại đa phương thức thống nhất, có cấu trúc, tổng hợp, áp dụng chiến lược chia để trị. Chúng tôi đầu tiên chia nhỏ nhiệm vụ tạo đối thoại đa phương thức phức tạp thành một số khả năng con, có thể được học theo cách dễ dàng hơn. Sau đó, các giải pháp cho các khả năng con được kết hợp để thu được một giải pháp hiệu quả và hiệu quả cho mỗi nhiệm vụ đối thoại đa phương thức downstream. Kết quả thí nghiệm trên tám bộ dữ liệu chuẩn chứng minh rằng PaCE đạt được hiệu suất tốt nhất hiện tại mới.

Thảo luận

PaCE áp dụng cấu trúc mô hình linh hoạt phân tách các cuộc đối thoại đa phương thức phức tạp thành các khả năng con cơ bản. Kết quả là, nó có thể được huấn luyện một cách tiến bộ trên các dữ liệu khác nhau và thể hiện khả năng mở rộng xuất sắc, làm cho nó áp dụng được cho các nhiệm vụ mới. Một lợi thế bổ sung là nó phù hợp với nhiều nỗ lực khác nhau để nâng cao hiệu suất về mặt khả năng diễn giải. Tuy nhiên, chúng tôi tin rằng vẫn còn nhiều khía cạnh của PACE đáng được khám phá. Đầu tiên là việc khám phá kết hợp các phương thức bổ sung và điều tra liệu lớp attention tự có thể xử lý hiệu quả một loạt rộng hơn các phương thức cho một biểu diễn thống nhất. Một khía cạnh khác đáng khám phá là phát triển một phương pháp hiệu quả hơn để thích ứng các mô hình đa phương thức với các ứng dụng downstream đa dạng, loại bỏ sự cần thiết phải tinh chỉnh tất cả các tham số của mô hình. Hơn nữa, cho những khác biệt đáng kể trong các mạng mô hình được sử dụng cho tạo văn bản và tạo hình ảnh trong nghiên cứu đương đại, việc khám phá tích hợp tạo đa phương thức vào một khung thống nhất là một nỗ lực đáng giá.

Hạn chế

Để phân tích tốt hơn các hạn chế của PaCE, chúng tôi tiến hành phân tích các lỗi được tạo ra bởi PaCE trên các bộ kiểm tra PhotoChat và SIMMC2.0. Chúng tôi tiết lộ một số lý do cho các lỗi, có thể được chia thành các danh mục sau. Đầu tiên, vì có nhiều hình ảnh tương tự trong các bộ dữ liệu, PaCE không thể phân biệt một số hình ảnh vàng từ các ứng viên tương tự. Điều này có thể là do chúng tôi không thiết kế một mô-đun lý luận tinh vi tường minh để nắm bắt các chi tiết của hình ảnh và văn bản. Ví dụ, đối với ngữ cảnh đề cập "Tôi và bố tôi đều có máy ảnh", mô hình của chúng tôi có thể nắm bắt thực thể "máy ảnh", nhưng không thể suy luận thực tế rằng nên có hai máy ảnh. Một giải pháp khả thi là giới thiệu một chiến lược lý luận và hiểu sâu để trao quyền cho mô hình với khả năng lý luận xuất sắc. Thứ hai, do thiếu hiểu biết cấu trúc tinh vi về hình ảnh, các câu được tạo ra bởi PaCE gặp khó khăn trong việc xác định vị trí tương đối của các thực thể. Ví dụ, PaCE có thể gặp khó khăn trong việc nhận ra thực tế rằng phía bên phải của một chiếc áo vàng là quần đen. Vấn đề này đặc biệt nghiêm trọng trong SIMMC vì có nhiều thực thể trong hình ảnh và mô tả không gian của các thực thể trong phản hồi. Một ý tưởng khả thi là trích xuất vị trí tương đối của các đối tượng được đề cập trong cuộc trò chuyện như dữ liệu phụ trợ để hướng dẫn việc tạo của mô hình.

Lời cảm ơn

Min Yang được hỗ trợ một phần bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (2022YFF0902100), Chương trình Đổi mới Khoa học và Công nghệ Thâm Quyến (KQTD20190929172835662), Quỹ Nghiên cứu Cơ bản Thâm Quyến (JCYJ20210324115614039 và JCYJ20200109113441941), và NSFC (số 92270122). Công trình này được hỗ trợ bởi Tập đoàn Alibaba thông qua Chương trình Nghiên cứu Đổi mới Alibaba.

Tài liệu tham khảo

[Danh sách tài liệu tham khảo tiếng Anh được giữ nguyên như trong bản gốc]

--- TRANG 11 ---

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 12 ---

[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 13 ---

[Tiếp tục và kết thúc danh sách tài liệu tham khảo...]
