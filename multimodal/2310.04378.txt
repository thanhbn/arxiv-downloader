# 2310.04378.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.04378.pdf
# File size: 15939992 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint
LATENT CONSISTENCY MODELS :
SYNTHESIZING HIGH-RESOLUTION IMAGES
WITH FEW-STEP INFERENCE
Simian Luo∗Yiqin Tan∗Longbo Huang†Jian Li†Hang Zhao†
Institute for Interdisciplinary Information Sciences, Tsinghua University
{luosm22, tyq22 }@mails.tsinghua.edu.cn
{longbohuang, lijian83, hangzhao }@tsinghua.edu.cn
ABSTRACT
Latent Diffusion models (LDMs) have achieved remarkable results in synthesiz-
ing high-resolution images. However, the iterative sampling process is compu-
tationally intensive and leads to slow generation. Inspired by Consistency Mod-
els (Song et al., 2023), we propose Latent Consistency Models ( LCMs ), enabling
swift inference with minimal steps on any pre-trained LDMs, including Stable
Diffusion (Rombach et al., 2022). Viewing the guided reverse diffusion process
as solving an augmented probability flow ODE (PF-ODE), LCMs are designed
to directly predict the solution of such ODE in latent space, mitigating the need
for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently
distilled from pre-trained classifier-free guided diffusion models, a high-quality
768×768 2∼4-step LCM takes only 32 A100 GPU hours for training. Further-
more, we introduce Latent Consistency Fine-tuning (LCF), a novel method that
is tailored for fine-tuning LCMs on customized image datasets. Evaluation on
the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-
art text-to-image generation performance with few-step inference. Project Page:
https://latent-consistency-models.github.io/
1 I NTRODUCTION
Diffusion models have emerged as powerful generative models that have gained significant attention
and achieved remarkable results in various domains (Ho et al., 2020; Song et al., 2020a; Nichol &
Dhariwal, 2021; Ramesh et al., 2022; Song & Ermon, 2019; Song et al., 2021). In particular, latent
diffusion models (LDMs) (e.g., Stable Diffusion (Rombach et al., 2022)) have demonstrated excep-
tional performance, especially in high-resolution text-to-image synthesis tasks. LDMs can generate
high-quality images conditioned on textual descriptions by utilizing an iterative reverse sampling
process that performs gradual denoising of samples. However, diffusion models suffer from a no-
table drawback: the iterative reverse sampling process leads to slow generation speed, limiting their
real-time applicability. To overcome this drawback, researchers have proposed several methods to
improve the sampling speed, which involves accelerating the denoising process by enhancing ODE
solvers (Ho et al., 2020; Lu et al., 2022a;b), which can generate images within 10 ∼20 sampling
steps. Another approach is to distill a pre-trained diffusion model into models that enable few-step
inference Salimans & Ho (2022); Meng et al. (2023). In particular, Meng et al. (2023) proposed a
two-stage distillation approach to improving the sampling efficiency of classifier-free guided mod-
els. Recently, Song et al. (2023) proposed consistency models as a promising alternative aimed at
speeding up the generation process. By learning consistency mappings that maintain point consis-
tency on ODE-trajectory, these models allow for single-step generation, eliminating the need for
computation-intensive iterations. However, Song et al. (2023) is constrained to pixel space image
generation tasks, making it unsuitable for synthesizing high-resolution images. Moreover, the appli-
cations to the conditional diffusion model and the incorporation of classifier-free guidance have not
been explored, rendering their methods unsuitable for text-to-image generation synthesis.
∗Equal Contribution†Corresponding Authors
1arXiv:2310.04378v1  [cs.CV]  6 Oct 2023

--- PAGE 2 ---
Preprint
4-Steps Inference
1-Step Inference
2-Steps Inference
Figure 1: Images generated by Latent Consistency Models (LCMs) with CFG scale ω= 8.0. LCMs can be
distilled from any pre-trained Stable Diffusion (SD) in only 4,000 training steps ( ∼32 A100 GPU Hours ) for
generating high quality 768 ×768 resolution images in 2 ∼4 steps or even one step, significantly accelerating
text-to-image generation. We employ LCM to distill the Dreamer-V7 version of SD in just 4,000 training
iterations.
In this paper, we introduce Latent Consistency Models (LCMs) for fast, high-resolution image
generation. Mirroring LDMs, we employ consistency models in the image latent space of a pre-
trained auto-encoder from Stable Diffusion (Rombach et al., 2022). We propose a one-stage guided
distillation method to efficiently convert a pre-trained guided diffusion model into a latent consis-
tency model by solving an augmented PF-ODE. Additionally, we propose Latent Consistency Fine-
tuning, which allows fine-tuning a pre-trained LCM to support few-step inference on customized
image datasets. Our main contributions are summarized as follows:
• We propose Latent Consistency Models (LCMs) for fast, high-resolution image generation. LCMs
employ consistency models in the image latent space, enabling fast few-step or even one-step
high-fidelity sampling on pre-trained latent diffusion models (e.g., Stable Diffusion (SD)).
• We provide a simple and efficient one-stage guided consistency distillation method to distill SD
for few-step (2 ∼4) or even 1-step sampling. We propose the S KIPPING -STEPtechnique to further
2

--- PAGE 3 ---
Preprint
accelerate the convergence. For 2- and 4-step inference, our method costs only 32 A100 GPU
hours for training and achieves state-of-the-art performance on the LAION-5B-Aesthetics dataset.
• We introduce a new fine-tuning method for LCMs, named Latent Consistency Fine-tuning, en-
abling efficient adaptation of a pre-trained LCM to customized datasets while preserving the abil-
ity of fast inference.
2 R ELATED WORK
Diffusion Models have achieved great success in image generation (Ho et al., 2020; Song et al.,
2020a; Nichol & Dhariwal, 2021; Ramesh et al., 2022; Rombach et al., 2022; Song & Ermon, 2019).
They are trained to denoise the noise-corrupted data to estimate the score of data distribution. During
inference, samples are drawn by running the reverse diffusion process to gradually denoise the data
point. Compared to V AEs (Kingma & Welling, 2013; Sohn et al., 2015) and GANs (Goodfellow
et al., 2020), diffusion models enjoy the benefit of training stability and better likelihood estimation.
Accelerating DMs. However, diffusion models are bottlenecked by their slow generation speed.
Various approaches have been proposed, including training-free methods such as ODE solvers
(Song et al., 2020a; Lu et al., 2022a;b), adaptive step size solvers (Jolicoeur-Martineau et al., 2021),
predictor-corrector methods (Song et al., 2020b). Training-based approaches include optimized dis-
cretization (Watson et al., 2021), truncated diffusion (Lyu et al., 2022; Zheng et al., 2022), neural
operator (Zheng et al., 2023) and distillation (Salimans & Ho, 2022; Meng et al., 2023). More re-
cently, new generative models for faster sampling have also been proposed (Liu et al., 2022; 2023).
Latent Diffusion Models (LDMs) (Rombach et al., 2022) excel in synthesizing high-resolution text-
to-images. For example, Stable Diffusion (SD) performs forward and reverse diffusion processes in
the data latent space, resulting in more efficient computation.
Consistency Models (CMs) (Song et al., 2023) have shown great potential as a new type of genera-
tive model for faster sampling while preserving generation quality. CMs adopt consistency mapping
to directly map any point in ODE trajectory to its origin, enabling fast one-step generation. CMs
can be trained by distilling pre-trained diffusion models or as standalone generative models. Details
of CMs are elaborated in the following section.
3 P RELIMINARIES
In this section, we briefly review diffusion and consistency models and define relevant notations.
Diffusion Models: Diffusion models, or score-based generative models Ho et al. (2020); Song et al.
(2020a) is a family of generative models that progressively inject Gaussian noises into the data, and
then generate samples from noise via a reverse denoising process. In particular, diffusion models
define a forward process transitioning the origin data distribution pdata(x)to marginal distribution
qt(xt), via transition kernel: q0t(xt|x0) =N 
xt|α(t)x0, σ2(t)I, where α(t), σ(t)specify the
noise schedule. In continuous time perspective, the forward process can be described by a stochastic
differential equation (SDE) Song et al. (2020b); Lu et al. (2022a); Karras et al. (2022) for t∈[0, T]:
dxt=f(t)xtdt+g(t)dwt,x0∼pdata(x0),where wtis the standard Brownian motion, and
f(t) =d logα(t)
dt, g2(t) =dσ2(t)
dt−2d logα(t)
dtσ2(t). (1)
By considering the reverse time SDE (see Appendix A for more details), one can show that the
marginal distribution qt(x)satisfies the following ordinary differential equation, called the Proba-
bility Flow ODE (PF-ODE) (Song et al., 2020b; Lu et al., 2022a):
dxt
dt=f(t)xt−1
2g2(t)∇xlogqt(xt),xT∼qT(xT). (2)
In diffusion models, we train the noise prediction model ϵθ(xt, t)to fit−∇logqt(xt)(called the
score function ). Approximating the score function by the noise prediction model in 21, one can
obtain the following empirical PF-ODE for sampling:
dxt
dt=f(t)xt+g2(t)
2σtϵθ(xt, t),xT∼ N 
0,˜σ2I
. (3)
For class-conditioned diffusion models, Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) is
an effective technique to significantly improve the quality of generated samples and has been widely
3

--- PAGE 4 ---
Preprint
used in several large-scale diffusion models including GLIDE Nichol et al. (2021), Stable Diffusion
(Rombach et al., 2022), DALL·E 2 (Ramesh et al., 2022) and Imagen (Saharia et al., 2022). Given
a CFG scale ω, the original noise prediction is replaced by a linear combination of conditional and
unconditional noise prediction, i.e., ˜ϵθ(zt, ω,c, t) = (1 + ω)ϵθ(zt,c, t)−ωϵθ(z,∅, t).
Consistency Models: The Consistency Model (CM) (Song et al., 2023) is a new family of generative
models that enables one-step or few-step generation. The core idea of the CM is to learn the function
that maps any points on a trajectory of the PF-ODE to that trajectory’s origin (i.e., the solution of
the PF-ODE). More formally, the consistency function is defined as f: (xt, t)7−→xϵ,where ϵ
is a fixed small positive number. One important observation is that the consistency function should
satisfy the self-consistency property :
f(xt, t) =f(xt′, t′),∀t, t′∈[ϵ, T]. (4)
The key idea in (Song et al., 2023) for learning a consistency model fθis to learn a consistency
function from data by effectively enforcing the self-consistency property in Eq. 4. To ensure that
fθ(x, ϵ) =x, the consistency model fθis parameterized as:
fθ(x, t) =cskip(t)x+cout(t)Fθ(x, t), (5)
where cskip(t)andcout(t)are differentiable functions with cskip(ϵ) = 1 andcout(ϵ) = 0 , andFθ(x, t)is a
deep neural network. A CM can be either distilled from a pre-trained diffusion model or trained from
scratch. The former is known as Consistency Distillation . To enforce the self-consistency property,
we maintain a target model θ−, updated with exponential moving average (EMA) of the parameter
θwe intend to learn, i.e., θ−←µθ−+ (1−µ)θ, and define the consistency loss as follows:
L(θ,θ−; Φ) = Ex,th
d
fθ(xtn+1, tn+1),fθ−(ˆxϕ
tn, tn)i
, (6)
where d(·,·)is a chosen metric function for measuring the distance between two samples, e.g., the
squared ℓ2distance d(x,y) =||x−y||2
2.ˆxϕ
tnis a one-step estimation of xtnfromxtn+1as:
ˆxϕ
tn←xtn+1+ (tn−tn+1)Φ(xtn+1, tn+1;ϕ). (7)
where Φdenotes the one-step ODE solver applied to PF-ODE in Eq. 24. (Song et al., 2023) used
Euler (Song et al., 2020b) or Heun solver (Karras et al., 2022) as the numerical ODE solver. More
details and the pseudo-code for consistency distillation (Algorithm 2) are provided in Appendix A.
4 L ATENT CONSISTENCY MODELS
Consistency Models (CMs) (Song et al., 2023) only focused on image generation tasks on Ima-
geNet 64 ×64 (Deng et al., 2009) and LSUN 256 ×256 (Yu et al., 2015). The potential of CMs
to generate higher-resolution text-to-image tasks remains unexplored. In this paper, we introduce
Latent Consistency Models (LCMs) in Sec 4.1 to tackle these more challenging tasks, unleashing
the potential of CMs. Similar to LDMs, our LCMs adopt a consistency model in the image latent
space. We choose the powerful Stable Diffusion (SD) as the underlying diffusion model to distill
from. We aim to achieve few-step (2 ∼4) and even one-step inference on SD without compromising
image quality. The classifier-free guidance (CFG) (Ho & Salimans, 2022) is an effective technique
to further improve sample quality and is widely used in SD. However, its application in CMs re-
mains unexplored. We propose a simple one-stage guided distillation method in Sec 4.2 that solves
anaugmented PF-ODE , integrating CFG into LCM effectively. We propose S KIPPING -STEPtech-
nique to accelerate the convergence of LCMs in Sec. 4.3. Finally, we propose Latent Consistency
Fine-tuning to finetune a pre-trained LCM for few-step inference on a customized dataset in Sec 4.4.
4.1 C ONSISTENCY DISTILLATION IN THE LATENT SPACE
Utilizing image latent space in large-scale diffusion models like Stable Diffusion (SD) (Rombach
et al., 2022) has effectively enhanced image generation quality and reduced computational load. In
SD, an autoencoder ( E,D) is first trained to compress high-dim image data into low-dim latent vector
z=E(x), which is then decoded to reconstruct the image as ˆx=D(z). Training diffusion models in
the latent space greatly reduces the computation costs compared to pixel-based models and speeds up
the inference process; LDMs make it possible to generate high-resolution images on laptop GPUs.
For LCMs, we leverage the advantage of the latent space for consistency distillation, contrasting
with the pixel space used in CMs (Song et al., 2023). This approach, termed Latent Consistency
Distillation (LCD) is applied to pre-trained SD, allowing the synthesis of high-resolution (e.g.,
4

--- PAGE 5 ---
Preprint
768×768) images in 1 ∼4 steps. We focus on conditional generation. Recall that the PF-ODE of the
reverse diffusion process (Song et al., 2020b; Lu et al., 2022a) is
dzt
dt=f(t)zt+g2(t)
2σtϵθ(zt,c, t),zT∼ N 
0,˜σ2I
, (8)
where ztare image latents, ϵθ(zt,c, t)is the noise prediction model, and cis the given condition
(e.g text). Samples can be drawn by solving the PF-ODE from Tto0. To perform LCD , we
introduce the consistency function fθ: (zt,c, t)7→z0to directly predict the solution of PF-ODE
(Eq. 8) for t= 0. We parameterize fθby the noise prediction model ˆϵθ, as follows:
fθ(z,c, t) =cskip(t)z+cout(t)z−σtˆϵθ(z,c, t)
αt
,(ϵ-Prediction ) (9)
where cskip(0) = 1 , cout(0) = 0 andˆϵθ(z,c, t)is a noise prediction model that initializes with the
same parameters as the teacher diffusion model. Notably, fθcan be parameterized in various ways,
depending on the teacher diffusion model parameterizations of predictions (e.g., x,ϵ(Ho et al.,
2020), v(Salimans & Ho, 2022)). We discuss other possible parameterizations in Appendix D.
We assume that an efficient ODE solver Ψ(zt, t, s,c)is available for approximating the integration
of the right-hand side of Eq equation 8 from time ttos. In practice, we can use DDIM (Song
et al., 2020a), DPM-Solver (Lu et al., 2022a) or DPM-Solver++ (Lu et al., 2022b) as Ψ(·,·,·,·).
Note that we only use these solvers in training/distillation, not in inference. We will discuss these
solvers further when we introduce the SKIPPING -STEP technique in Sec. 4.3. LCM aims to predict
the solution of the PF-ODE by minimizing the consistency distillation loss (Song et al., 2023):
LCD
θ,θ−; Ψ
=Ez,c,nh
d
fθ(ztn+1,c, tn+1),fθ−(ˆzΨ
tn,c, tn)i
. (10)
Here, ˆzΨ
tnis an estimation of the evolution of the PF-ODE from tn+1→tnusing ODE solver Ψ:
ˆzΨ
tn−ztn+1=Ztn
tn+1
f(t)zt+g2(t)
2σtϵθ(zt,c, t)
dt≈Ψ(ztn+1, tn+1, tn,c), (11)
where the solver Ψ(·,·,·,·)is used to approximate the integration from tn+1→tn.
4.2 O NE-STAGE GUIDED DISTILLATION BY SOLVING AUGMENTED PF-ODE
Classifier-free guidance (CFG) (Ho & Salimans, 2022) is crucial for synthesizing high-quality text-
aligned images in SD, typically needing a CFG scale ωover6. Thus, integrating CFG into a distilla-
tion method becomes indispensable. Previous method Guided-Distill (Meng et al., 2023) introduces
a two-stage distillation to support few-step sampling from a guided diffusion model. However, it is
computationally intensive (e.g. at least 45A100 GPUs Days for 2-step inference, estimated in (Liu
et al., 2023)). An LCM demands merely 32A100 GPUs Hours training for 2-step inference, as de-
picted in Figure 1. Furthermore, the two-stage guided distillation might result in accumulated error,
leading to suboptimal performance. In contrast, LCMs adopt efficient one-stage guided distillation
by solving an augmented PF-ODE. Recall the CFG used in reverse diffusion process:
˜ϵθ(zt, ω,c, t) := (1 + ω)ϵθ(zt,c, t)−ωϵθ(zt,∅, t), (12)
where the original noise prediction is replaced by the linear combination of conditional and uncon-
ditional noise and ωis called the guidance scale . To sample from the guided reverse process, we
need to solve the following augmented PF-ODE : (i.e., augmented with the terms related to ω)
dzt
dt=f(t)zt+g2(t)
2σt˜ϵθ(zt, ω,c, t),zT∼ N 
0,˜σ2I
. (13)
To efficiently perform one-stage guided distillation, we introduce an augmented consistency func-
tionfθ: (zt, ω,c, t)7→z0to directly predict the solution of augmented PF-ODE (Eq. 13) for
t= 0. We parameterize the fθin the same way as in Eq. 9, except that ˆϵθ(z,c, t)is replaced by
ˆϵθ(z, ω,c, t), which is a noise prediction model initializing with the same parameters as the teacher
diffusion model, but also contains additional trainable parameters for conditioning on ω. The consis-
tency loss is the same as Eq. 10 except that we use augmented consistency function fθ(zt, ω,c, t).
LCD
θ,θ−; Ψ
=Ez,c,ω,nh
d
fθ(ztn+1, ω,c, tn+1),fθ−(ˆzΨ,ω
tn, ω,c, tn)i
(14)
5

--- PAGE 6 ---
Preprint
In Eq 14, ωandnare uniformly sampled from interval [ωmin, ωmax]and{1, . . . , N −1}respectively.
ˆzΨ,ω
tnis estimated using the new noise model ˜ϵθ(zt, ω,c, t), as follows:
ˆzΨ,ω
tn−ztn+1=Ztn
tn+1
f(t)zt+g2(t)
2σt˜ϵθ(zt, ω,c, t)
dt
= (1 + ω)Ztn
tn+1
f(t)zt+g2(t)
2σtϵθ(zt,c, t)
dt−ωZtn
tn+1
f(t)zt+g2(t)
2σtϵθ(zt,∅, t)
dt
≈(1 +ω)Ψ(ztn+1, tn+1, tn,c)−ωΨ(ztn+1, tn+1, tn,∅).
(15)
Again, we can use DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a) or DPM-Solver++
(Lu et al., 2022b) as the PF-ODE solver Ψ(·,·,·,·).
4.3 A CCELERATING DISTILLATION WITH SKIPPING TIMESTEPS
Discrete diffusion models (Ho et al., 2020; Song & Ermon, 2019) typically train noise prediction
models with a long time-step schedule {ti}i(also called discretization schedule or time schedule)
to achieve high quality generation results. For instance, Stable Diffusion (SD) has a time schedule
of length 1,000. However, directly applying Latent Consistency Distillation ( LCD ) to SD with such
an extended schedule can be problematic. The model needs to sample across all 1,000 time steps,
and the consistency loss attempts to aligns the prediction of LCM model fθ(ztn+1,c, tn+1)with the
prediction fθ(ztn,c, tn)at the subsequent step along the same trajectory. Since tn−tn+1is tiny, ztn
andztn+1(and thus fθ(ztn+1,c, tn+1)andfθ(ztn,c, tn)) are already close to each other, incurring
small consistency loss and hence leading to slow convergence. To address this issues, we introduce
the SKIPPING -STEP method to considerably shorten the length of time schedule (from thousands to
dozens) to achieve fast convergence while preserving generation quality.
Consistency Models (CMs) (Song et al., 2023) use the EDM (Karras et al., 2022) continuous time
schedule, and the Euler, or Heun Solver as the numerical continuous PF-ODE solver. For LCMs,
in order to adapt to the discrete-time schedule in Stable Diffusion, we utilize DDIM (Song et al.,
2020a), DPM-Solver (Lu et al., 2022a), or DPM-Solver++ (Lu et al., 2022b) as the ODE solver. (Lu
et al., 2022a) shows that these advanced solvers can solve the PF-ODE efficiently in Eq. 8. Now, we
introduce the S KIPPING -STEPmethod in Latent Consistency Distillation (LCD). Instead of ensuring
consistency between adjacent time steps tn+1→tn, LCMs aim to ensure consistency between the
current time step and k-step away, tn+k→tn. Note that setting k=1 reduces to the original schedule
in (Song et al., 2023), leading to slow convergence, and very large kmay incur large approximation
errors of the ODE solvers. In our main experiments, we set k=20, drastically reducing the length
of time schedule from thousands to dozens. Results in Sec. 5.2 show the effect of various kvalues
and reveal that the SKIPPING -STEP method is crucial in accelerating the LCD process. Specifically,
consistency distillation loss in Eq. 14 is modified to ensure consistency from tn+ktotn:
LCD
θ,θ−; Ψ
=Ez,c,ω,nh
d
fθ(ztn+k, ω,c, tn+k),fθ−(ˆzΨ,ω
tn, ω,c, tn)i
, (16)
withˆzΨ,ω
tnbeing an estimate of ztnusing numerical augmented PF-ODE solver Ψ:
ˆzΨ,ω
tn←−ztn+k+ (1 + ω)Ψ(ztn+k, tn+k, tn,c)−ωΨ(ztn+k, tn+k, tn,∅). (17)
The above derivation is similar to Eq. 15. For LCM, we use three possible ODE solvers here:
DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a), DPM-Solver++ (Lu et al., 2022b), and
we compare their performance in Sec 5.2. In fact, DDIM (Song et al., 2020a) is the first-order
discretization approximation of the DPM-Solver (Proven in (Lu et al., 2022a)). Here we provide the
detailed formula of the DDIM PF-ODE solver ΨDDIM from tn+ktotn. The formulas of the other
two solver ΨDPM-Solver ,ΨDPM-Solver++ are provided in Appendix E.
ΨDDIM(ztn+k, tn+k, tn,c) =αtn
αtn+kztn+k−σtnσtn+k·αtn
αtn+k·σtn−1
ˆϵθ(ztn+k,c, tn+k)
| {z }
DDIM Estimated ztn−ztn+k(18)
We present the pseudo-code for LCD with CFG and SKIPPING -STEP techniques in Algorithm 1
The modifications from the original Consistency Distillation (CD) algorithm in Song et al. (2023)
are highlighted in blue. Also, the LCM sampling algorithm 3 is provided in Appendix B.
6

--- PAGE 7 ---
Preprint
MODEL (512×512) R ESOFID↓ CLIP S CORE↑
1 STEP2 STEPS 4 STEPS 8 STEPS 1 STEPS 2 STEPS 4 STEPS 8 STEPS
DDIM (Song et al., 2020a) 183.29 81.05 22.38 13.83 6.03 14.13 25.89 29.29
DPM (Lu et al., 2022a) 185.78 72.81 18.53 12.24 6.35 15.10 26.64 29.54
DPM++ (Lu et al., 2022b) 185.78 72.81 18.43 12.20 6.35 15.10 26.64 29.55
Guided-Distill (Meng et al., 2023) 108.21 33.25 15.12 13.89 12.08 22.71 27.25 28.17
LCM (Ours) 35.36 13.31 11.10 11.84 24.14 27.83 28.69 28.84
Table 1: Quantitative results with ω= 8 at 512×512 resolution. LCM significantly surpasses baselines in
the 1-4 step region on LAION-Aesthetic-6+ dataset. For LCM, DDIM-Solver is used with a skipping step of
k= 20 .
MODEL (768×768) R ESOFID↓ CLIP S CORE↑
1 STEP2 STEPS 4 STEPS 8 STEPS 1 STEPS 2 STEPS 4 STEPS 8 STEPS
DDIM (Song et al., 2020a) 186.83 77.26 24.28 15.66 6.93 16.32 26.48 29.49
DPM (Lu et al., 2022a) 188.92 67.14 20.11 14.08 7.40 17.11 27.25 29.80
DPM++ (Lu et al., 2022b) 188.91 67.14 20.08 14.11 7.41 17.11 27.26 29.84
Guided-Distill (Meng et al., 2023) 120.28 30.70 16.70 14.12 12.88 24.88 28.45 29.16
LCM (Ours) 34.22 16.32 13.53 14.97 25.32 27.92 28.60 28.49
Table 2: Quantitative results with ω= 8at 768×768 resolution. LCM significantly surpasses the baselines in
the 1-4 step region on LAION-Aesthetic-6.5+ dataset. For LCM, DDIM-Solver is used with a skipping step of
k= 20 .
Algorithm 1 Latent Consistency Distillation (LCD)
Input: dataset D, initial model parameter θ, learning rate η, ODE solver Ψ(·,·,·,·), distance metric d(·,·),
EMA rate µ, noise schedule α(t), σ(t), guidance scale [wmin, wmax], skipping interval k, and encoder E(·)
Encoding training data into latent space: Dz={(z,c)|z=E(x),(x,c)∈ D}
θ−←θ
repeat
Sample (z,c)∼ D z,n∼ U[1, N−k]andω∼[ωmin, ωmax]
Sample ztn+k∼ N(α(tn+k)z;σ2(tn+k)I)
ˆzΨ,ω
tn←ztn+k+ (1 + ω)Ψ(ztn+k, tn+k, tn,c)−ωΨ(ztn+k, tn+k, tn,∅)
L(θ,θ−; Ψ)←d(fθ(ztn+k, ω,c, tn+k),fθ−(ˆzΨ,ω
tn, ω,c, tn))
θ←θ−η∇θL(θ,θ−)
θ−←stopgrad (µθ−+ (1−µ)θ)
until convergence
4.4 L ATENT CONSISTENCY FINE-TUNING FOR CUSTOMIZED DATASET
Foundation generative models like Stable Diffusion excel in diverse text-to-image generation tasks
but often require fine-tuning on customized datasets to meet the requirements of downstream tasks.
We propose Latent Consistency Fine-tuning (LCF), a fine-tuning method for pretrained LCM. In-
spired by Consistency Training (CT) (Song et al., 2023), LCF enables efficient few-step inference
on customized datasets without relying on a teacher diffusion model trained on such data. This
approach presents a viable alternative to traditional fine-tuning methods for diffusion models. The
pseudo-code for LCF is provided in Algorithm 4, with a more detailed illustration in Appendix C.
5 E XPERIMENT
In this section, we employ latency consistency distillation to train LCM on two subsets of LAION-
5B. In Sec 5.1, we first evaluate the performance of LCM on text-to-image generation tasks. In
Sec 5.2, we provide a detailed ablation study to test the effectiveness of using different solvers,
skipping step schedules and guidance scales. Lastly, in Sec 5.3, we present the experimental results
of latent consistency finetuning on a pretrained LCM on customized image datasets.
5.1 T EXT-TO-IMAGE GENERATION
Datasets We use two subsets of LAION-5B (Schuhmann et al., 2022): LAION-Aesthetics-6+ (12M)
and LAION-Aesthetics-6.5+ (650K) for text-to-image generation. Our experiments consider reso-
lutions of 512 ×512 and 768 ×768. For 512 resolution, we use LAION-Aesthetics-6+, which com-
prises 12M text-image pairs with predicted aesthetics scores higher than 6. For 768 resolution, we
use LAION-Aesthetics-6.5+, with 650K text-image pairs with aesthetics score higher than 6.5.
Model Configuration For 512 resolution, we use the pre-trained Stable Diffusion-V2.1-Base (Rom-
bach et al., 2022) as the teacher model, which was originally trained on resolution 512 ×512 with ϵ-
Prediction (Ho et al., 2020). For 768 resolution, we use the widely used pre-trained Stable Diffusion-
7

--- PAGE 8 ---
Preprint
LCM 4-Steps (Ours)
Guided Distill 4-Steps
LCM 2-Steps (Ours)
Guided Distill 2-Steps
DPM-Solver++ 2-Steps
DPM-Solver++ 4-Steps
Figure 2: Text-to-Image generation results on LAION-Aesthetic-6.5+ with 2-, 4-step inference. Images gen-
erated by LCM exhibit superior detail and quality, outperforming other baselines by a large margin.
Figure 3: Ablation study on different ODE solvers and skipping step k. Appropriate skipping step kcan
significantly accelerate convergence and lead to better FID within the same number of training steps.
V2.1, originally trained on resolution 768 ×768 with v-Prediction (Salimans & Ho, 2022). We
train LCM with 100K iterations and we use a batch size of 72 for (512×512) setting, and 16 for
(768×768) setting, the same learning rate 8e-6 and EMA rate µ= 0.999943 as used in (Song et al.,
2023). For augmented PF-ODE solver Ψand skipping step kin Eq. 17, we use DDIM-Solver (Song
et al., 2020a) with skipping step k= 20 . We set the guidance scale range [wmin, wmax] = [2 ,14],
consistent with (Meng et al., 2023). More training details are provided in the Appendix F.
Baselines & Evaluation We use DDIM (Song et al., 2020a), DPM (Lu et al., 2022a), DPM++ (Lu
et al., 2022b) and Guided-Distill (Meng et al., 2023) as baselines. The first three are training-free
samplers requiring more peak memory per step with classifier-free guidance. Guided-Distill requires
two stages of guided distillation. Since Guided-Distill is not open-sourced, we strictly followed the
training procedure outlined in the paper to reproduce the results. Due to the limited resource (Meng
et al. (2023) used a large batch size of 512, requiring at least 32 A100 GPUs), we reduce the batch
size to 72, the same as ours, and trained for the same 100K iterations. Reproduction details are
provided in Appendix G. We admit that longer training and more computational resources can lead
to better results as reported in (Meng et al., 2023). However, LCM achieves faster convergence and
superior results under the same computation cost. For evaluation, We generate 30K images from
10K text prompts in the test set (3 images per prompt), and adopt FID and CLIP scores to evaluate
the diversity and quality of the generated images. We use ViT-g/14 for evaluating CLIP scores.
Results. The quantitative results in Tables 1 and 2 show that LCM notably outperforms baseline
methods at 512and768resolutions, especially in the low step regime (1 ∼4), highlighting its effi-
cency and superior performance. Unlike DDIM, DPM, DPM++, which require more peak memory
per sampling step with CFG, LCM requires only one forward pass per sampling step, saving both
time and memory. Moreover, in contrast to the two-stage distillation procedure employed in Guided-
Distill, LCM only needs one-stage guided distillation, which is much simpler and more practical.
Thequalitative results in Figure 2 further show the superiority of LCM with 2- and 4-step inference.
5.2 A BLATION STUDY
ODE Solvers & Skipping-Step Schedule. We compare various solvers Ψ(DDIM (Song et al.,
2020a), DPM (Lu et al., 2022a), DPM++ (Lu et al., 2022b)) for solving the augmented PF-ODE
specified in Eq 17, and explore different skipping step schedules with different k. The results
are depicted in Figure 3. We observe that: 1) Using S KIPPING -STEP techniques (see Sec 4.3),
LCM achieves fast convergence within 2,000 iterations in the 4-step inference setting. Specifically,
the DDIM solver converges slowly at skipping step k= 1, while setting k= 5,10,20leads to
8

--- PAGE 9 ---
Preprint
Figure 4: Ablation study on different classifier-free guidance scales ω. Larger ωleads to better sample quality
(CLIP Scores). The performance gaps across 2, 4, and 8 steps are minimal, showing the efficacy of LCM.
𝛚= 2.0𝛚= 4.0𝛚= 8.0𝛚= 12.0𝛚= 2.0𝛚= 4.0𝛚= 8.0𝛚= 12.0
Figure 5: 4-step LCMs using different CFG scales ω. LCMs utilize one-stage guided distillation to directly
incorporate CFG scales ω. Larger ωenhances image quality.
much faster convergence, underscoring the effectiveness of the Skipping-Step method. 2) DPM and
DPM++ solvers perform better at a larger skipping step ( k= 50 ) compared to the DDIM solver
which suffers from increased ODE approximation error with larger k. This phenomenon is also dis-
cussed in (Lu et al., 2022a). 3) Very small kvalues (1 or 5) result in slow convergence and very large
ones (e.g., 50 for DDIM) may lead to inferior results. Hence, we choose k= 20 , which provides
competitive performance for all three solvers, for our main experiment in Sec 5.1.
The Effect of Guidance Scale ω.We examine the effect of using different CFG scales ωin LCM.
Typically, ωbalances sample quality and diversity. A larger ωgenerally tends to improve sample
quality (indicated by CLIP), but may compromise diversity (measured by FID). Beyond a certain
threshold, an increased ωyields better CLIP scores at the expense of FID. Figure 4 presents the
results for various ωacross different inference steps. Our findings include: 1) Using large ωen-
hances sample quality (CLIP Scores) but results in relatively inferior FID. 2) The performance gaps
across 2, 4, and 8 inference steps are negligible, highlighting LCM’s efficacy in 2 ∼8 step regions.
However, a noticeable gap exists in one-step inference, indicating rooms for further improvements.
We present visualizations for different ωin Figure 5. One can see clearly that a larger ωenhances
sample quality, verifying the effectiveness of our one-stage guided distillation method.
5.3 D OWNSTREAM CONSISTENCY FINE-TUNING RESULTS
We perform Latent Consistency Fine-tuning (LCF) on two customized image datasets, Pokemon
dataset (Pinkney, 2022) and Simpsons dataset (Norod78, 2022), to demonstrate the efficiency of
LCF. Each dataset, comprised of hundreds of customized text-image pairs, is split such that 90%
is used for fine-tuning and the rest 10% for testing. For LCF, we utilize pretrained LCM that was
originally trained at the resolution of 768 ×768 used in Table 2. For these two datasets, we fine-tune
the pre-trained LCM for 30K iterations with a learning rate 8e-6. We present qualitative results of
adopting LCF on two customized image datasets in Figure 6. The finetuned LCM is capable of
generating images with customized styles in few steps, showing the effectiveness of our method.
6 C ONCLUSION
We present Latent Consistency Models (LCMs), and a highly efficient one-stage guided distillation
method that enables few-step or even one-step inference on pre-trained LDMs. Furthermore, we
present latent consistency fine-tuning (LCF), to enable few-step inference of LCMs on customized
image datasets. Extensive experiments on the LAION-5B-Aesthetics dataset demonstrate the supe-
9

--- PAGE 10 ---
Preprint
Origin LCM1K Finetuning10K Finetuning30K Finetuning
Origin LCM1K Finetuning10K Finetuning30K Finetuning
Figure 6: 4-step LCMs using Latent Consistency Fine-tuning (LCF) on two customized datasets: Pokemon
Dataset (left), Simpsons Dataset (right). Through LCF, LCM produces images with customized styles.
rior performance and efficiency of LCMs. Future work include extending our method to more image
generation tasks such as text-guided image editing, inpainting and super-resolution.
REFERENCES
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition ,
pp. 248–255. Ieee, 2009.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the
ACM , 63(11):139–144, 2020.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 , 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems , 33:6840–6851, 2020.
Aapo Hyv ¨arinen and Peter Dayan. Estimation of non-normalized statistical models by score match-
ing.Journal of Machine Learning Research , 6(4), 2005.
Alexia Jolicoeur-Martineau, Ke Li, R ´emi Pich ´e-Taillefer, Tal Kachman, and Ioannis Mitliagkas.
Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080 ,
2021.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. Advances in Neural Information Processing Systems , 35:26565–26577,
2022.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265 ,
2019.
Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transfer data with rectified flow. arXiv preprint arXiv:2209.03003 , 2022.
Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough
for high-quality diffusion-based text-to-image generation. arXiv preprint arXiv:2309.06380 ,
2023.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A
fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint
arXiv:2206.00927 , 2022a.
10

--- PAGE 11 ---
Preprint
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast
solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 ,
2022b.
Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models
via early stop of the diffusion process. arXiv preprint arXiv:2205.12524 , 2022.
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and
Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 14297–14306, 2023.
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,
Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with
text-guided diffusion models. arXiv preprint arXiv:2112.10741 , 2021.
Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.
InInternational Conference on Machine Learning , pp. 8162–8171. PMLR, 2021.
Norod78. Simpsons blip captions. https://huggingface.co/datasets/Norod78/
simpsons-blip-captions , 2022.
Justin N. M. Pinkney. Pokemon blip captions. https://huggingface.co/datasets/
lambdalabs/pokemon-blip-captions/ , 2022.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 , 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pp. 10684–10695, 2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic
text-to-image diffusion models with deep language understanding. Advances in Neural Informa-
tion Processing Systems , 35:36479–36494, 2022.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512 , 2022.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi
Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b:
An open large-scale dataset for training next generation image-text models. arXiv preprint
arXiv:2210.08402 , 2022.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep
conditional generative models. Advances in neural information processing systems , 28, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502 , 2020a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
Advances in Neural Information Processing Systems , 32, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint
arXiv:2011.13456 , 2020b.
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-
based diffusion models. Advances in Neural Information Processing Systems , 34:1415–1428,
2021.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint
arXiv:2303.01469 , 2023.
11

--- PAGE 12 ---
Preprint
Daniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to efficiently sam-
ple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802 , 2021.
Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365 , 2015.
Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.
arXiv preprint arXiv:2302.05543 , 2023.
Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast
sampling of diffusion models via operator learning. In International Conference on Machine
Learning , pp. 42390–42402. PMLR, 2023.
Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion proba-
bilistic models. stat, 1050:7, 2022.
A M ORE DETAILS ON DIFFUSION AND CONSISTENCY MODELS
A.1 D IFFUSION MODELS
Consider the forward process, described by the following SDE for t∈[0, T]:
dxt=f(t)xtdt+g(t)dwt,x0∼pdata(x0), (19)
where wtdenotes the standard Brownian motion. Leveraging the classic result of Anderson (1982),
Song et al. (2020b) show that the reverse process of the above forward process is also a diffusion
process, specified by the following reverse-time SDE:
dxt=
f(t)xt−g2(t)∇xlogqt(xt)
dt+g(t)dwt,xT∼qT(xT), (20)
where wtis a standard reverse-time Brownian motion. One can leverage the reverse SDE for data
sampling from Tto0, starting with qT(xT), which follows a Gaussian distribution approximately.
However, directly sampling from the reverse SDE requires a large number of discretization steps
and is typically very slow. To accelerate the sampling process, prior work (e.g., (Song et al., 2020b;
Lu et al., 2022a) leveraged the relation between the above SDE and ODE and designed ODE solvers
for sampling. In particular, it is known that for SDE (Eq.20), the following ordinary differential
equation (ODE), called the Probability Flow ODE (PF-ODE), has the same marginal distribution
qt(x)(Song et al., 2020b; Lu et al., 2022a):
dxt
dt=f(t)xt−1
2g2(t)∇xlogqt(xt),xT∼qT(xT) (21)
The term −∇logqt(xt)in Eq. 21 is typically called the score function ofqt(xt). In diffusion
models, we train the noise prediction model ϵθ(xt, t)to fit the scaled score function, via minimizing
the following score matching objective:
L(θ) =Et∈[0,T],xt∼qt
w(t)||ϵθ(xt, t) +σ(t)∇logqt(xt))||2
=Et∈[0,T],x0∼q0,ϵ
w(t)||ϵθ(xt, t)−ϵ||2
(22)
where w(t)is the weight function, ϵ∼N(0, I)andxt=α(t)x0+σ(t)ϵ. By substituting the score
function with the noise prediction model in Eq. 21, we obtain the following ODE, which can be used
for sampling:
dxt
dt=f(t)xt+g2(t)
2σtϵθ(xt, t),xT∼ N 
0,˜σ2I
. (23)
A.2 M ORE DETAILS ON CONSISTENCY MODELS IN (SONG ET AL ., 2023)
In this subsection, we provide more details on the consistency models and consistency distillation
algorithm in (Song et al., 2023). The pre-trained diffusion model used in (Song et al., 2023) adopts
the continuous noise schedule from EDM (Karras et al., 2022), therefore the PF-ODE in Eq. 23 can
be simplified as:
dxt
dt=−t∇logqt(xt)≈ −tsϕ(xt, t), (24)
12

--- PAGE 13 ---
Preprint
where the sϕ(xt, t)≈ ∇ logqt(xt)is a score prediction model trained via score matching
(Hyv ¨arinen & Dayan, 2005; Song & Ermon, 2019). Note that different noise schedules result in
different PF-ODE and the PF-ODE in Eq. 24 corresponds to the EDM noise schedule (Karras et al.,
2022). We denote the one-step ODE solver applied to PF-ODE in Eq. 24 as Φ(xt, t;ϕ). One can
either use Euler (Song et al., 2020b) or Heun solver (Karras et al., 2022) as the numerical ODE
solver. Then, we use the ODE solver to estimate the evolution of a sample xtnfromxtn+1as:
ˆxϕ
tn←xtn+1+ (tn−tn+1)Φ(xtn+1, tn+1;ϕ). (25)
(Song et al., 2020b) used the same time schedule as in (Karras et al., 2022): ti= (ϵ1/ρ+i−1
N−1(T1/ρ−
ϵ1/ρ))ρ, and ρ= 7. To enforce the self-consistency property in Eq. 4, we maintain a target model
θ−, which is updated with exponential moving average (EMA) of the parameter θwe intend to
learn, i.e., θ−←µθ−+ (1−µ)θ, and define the consistency loss as follows:
L(θ,θ−; Φ) = Ex,th
d
fθ(xtn+1, tn+1),fθ−(ˆxϕ
tn, tn)i
, (26)
where d(·,·)is a chosen metric function for measuring the distance between two samples, e.g., the
squared ℓ2distance d(x,y) =||x−y||2
2. The pseudo-code for consistency distillation in Song et al.
(2023). is presented in Algorithm 2. In their original paper, an Euler solver was used as the ODE
solver for the continuous-time setting.
Algorithm 2 Consistency Distillation (CD) (Song et al., 2023)
Input: dataset D, initial model parameter θ, learning rate η, ODE solver Φ(·,·,·), distance metric
d(·,·), and EMA rate µ
θ−←θ
repeat
Sample x∼ D andn∼ U[1, N−1]
Sample xtn+1∼ N(x;t2
n+1I)
ˆxϕ
tn←xtn+1+ (tn−tn+1)Φ(xtn+1, tn+1, ϕ)
L(θ,θ−; Φ)←d(fθ(xtn+1, tn+1),fθ−(ˆxϕ
tn, tn))
θ←θ−η∇θL(θ,θ−; Φ)
θ−←stopgrad (µθ−+ (1−µ)θ)
until convergence
B M ULTISTEP LATENT CONSISTENCY SAMPLING
Now, we present the multi-step sampling algorithm for latent consistency model. The sampling
algorithm for LCM is very similar to the one in consistency models (Song et al., 2023) except the
incorporation of classifier-free guidance in LCM. Unlike multi-step sampling in diffusion models,
in which we predict zt−1fromzt, the latent consistency models directly predicts the origin z0of
augmented PF-ODE trajectory (the solution of the augmented of PF-ODE), given guidance scale
ω. This generates samples in a single step. The sample quality can be improved by alternating
the denoising and noise injection steps. In particular, in the n-th iteration, we first perform noise-
injecting forward process to the previous predicted sample zasˆzτn∼ N(α(τn)z;σ2(τn)I), where
τnis a decreasing sequence of time steps. This corresponds to going back to point ˆzτnon the PF-
ODE trajectory. Then, we perform the next z0prediction again using the trained latent consistency
function. In our experiments, one can see the second iteration can already refine the generation
quality significantly, and high quality images can be generated in just 2-4 steps. We provide the
pseudo-code in Algorithm 3.
13

--- PAGE 14 ---
Preprint
Algorithm 3 Multistep Latent Consistency Sampling
Input: Latent Consistency Model fθ(·,·,·,·), Sequence of timesteps τ1> τ2>···> τN−1,
Text condition c, Classifier-Free Guidance Scale ω, Noise schedule α(t), σ(t), Decoder D(·)
Sample initial noise ˆzT∼ N(0;I)
z←fθ(ˆzT, ω,c, T)
forn= 1toN−1do
ˆzτn∼ N(α(τn)z;σ2(τn)I)
z←fθ(ˆzτn, ω,c, τn)
end for
x←D(z)
Output: x
C A LGORITHM DETAILS OF LATENT CONSISTENCY FINE-TUNING
In this section, we provide further details of Latent Consistency Fine-tuning (LCF). The pseudo-code
of LCF is provided in Algorithm 4. During the Latent Consistency Fine-tuning (LCF) process, we
randomly select two time steps tnandtn+kthat are ktime steps apart and apply the same Gaussian
noiseϵto obtain the noised data ztn,ztn+kas follows:
ztn+k=α(tn+k)z+σ(tn+k)ϵ,ztn=α(tn)z+σ(tn)ϵ.
Then, we can directly calculate the consistency loss for these two time steps to enforce self-
consistency property in Eq.4. Notably, this method can also utilize the skipping-step technique
to speedup the convergence. Furthermore, we note that latent consistency fine-tuning is independent
of the pre-trained teacher model, facilitating direct fine-tuning of a pre-trained latent consistency
model without reliance on the teacher diffusion model.
Algorithm 4 Latent Consistency Fine-tuning (LCF)
Input: customized dataset D(s), pre-trained LCM parameter θ, learning rate η, distance metric
d(·,·), EMA rate µ, noise schedule α(t), σ(t), guidance scale [wmin, wmax], skipping interval k,
and encoder E(·)
Encode training data into the latent space: D(s)
z={(z,c)|z=E(x),(x,c)∈ D(s)}
θ−←θ
repeat
Sample (z,c)∼ D(s)
z,n∼ U[1, N−k]andw∼[wmin, wmax]
Sample ϵ∼ N (0,I)
ztn+k←α(tn+k)z+σ(tn+k)ϵ,ztn←α(tn)z+σ(tn)ϵ
L(θ,θ−)←d(fθ(ztn+k, tn+k,c, w),fθ−(ztn, tn,c, w))
θ←θ−η∇θL(θ,θ−)
θ−←stopgrad (µθ−+ (1−µ)θ)
until convergence
D D IFFERENT WAYS TO PARAMETERIZE THE CONSISTENCY FUNCTION
As previously discussed in Eq 9, we can parameterize our consistency model function fθ(z,c, t)
in different ways, depending on the way the teacher diffusion model is parameterized. For
ϵ-Prediction (Song et al., 2020a), we use the following parameterization:
fθ(z,c, t) =cskip(t)z+cout(t)ˆz0(ϵ-Prediction ) (27)
where
ˆz0=zt−σ(t)ˆϵθ(z,c, t)
α(t)
. (28)
Recalling that zt=α(t)z0+σ(t)ϵ,ˆz0can be seen as a prediction of z0at time t.
Next, we provide the parameterization of (x-Prediction )(Ho et al., 2020; Salimans & Ho, 2022)
with the following form:
fθ(z,c, t) =cskip(t)z+cout(t)xθ(zt,c, t),(x-Prediction ) (29)
14

--- PAGE 15 ---
Preprint
where xθ(zt,c, t)corresponds to the teacher diffusion model with x-prediction.
Finally, for v-prediction (Salimans & Ho, 2022), the consistency function is parameterized as
fθ(z,c, t) =cskip(t)z+cout(t) (αtzt−σtvθ(zt,c, t)),(v-Prediction ) (30)
where vθ(zt,c, t)corresponds to the teacher diffusion model with v-prediction.
As mentioned in Sec 5.1, we use the ϵ-Parameterization in Eq. 27 to train LCM at 512 ×512
resolution using the teacher diffusion model, Stable-Diffusion-V2.1-Base (originally trained
withϵ-Prediction at 512 resolution). For resolution 768 ×768, we train the LCM using the
v-Parameterization in Eq. 30, adopting the teacher diffusion model, Stable-Diffusion-V2.1 (orig-
inally trained with v-Prediction at 768 resolution).
E F ORMULAS OF OTHER ODE S OLVERS
As discussed in Sec 4.3, we use the DDIM (Song et al., 2020a), DPM-Solver (Lu et al., 2022a)
and DPM-Solver++ (Lu et al., 2022b) as the PF-ODE solvers. Proven in (Lu et al., 2022a), the
DDIM-Solver is actually the first-order discretization approximation of the DPM-Solver.
ForDDIM (Song et al., 2020a) , the detailed formula of DDIM PF-ODE solver ΨDDIM from tn+k
totnis provided as follows.
ΨDDIM(ztn+k, tn+k, tn,c) =ˆztn−ztn+k
=αtn
αtn+kztn+k−σtnσtn+k·αtn
αtn+k·σtn−1
ˆϵθ(ztn+k,c, tn+k)
| {z }
DDIM Estimated ztn−ztn+k
(31)
ForDPM-Solver (Lu et al., 2022a), we only consider the case for order = 2, and the detailed
formula of PF-ODE solver ΨDPM-Solver is provided as follows. First we define some notations. We
denote λtn= log(αtn
σtn), which is the Log-SNR, h0
tn=λtn−λtn+k, h1
tn=λtn−λtn+k/2, and
rtn=h1
tn/h0
tn.
ΨDPM-Solver (ztn+k, tn+k, tn,c)
=αtn
αtn+kztn+k−σtn(eh0
tn−1)ˆϵθ(ztn+k,c, tn+k)
−σtn
2rtn(eh0
tn−1)
ˆϵθ(zΨ
tn+k/2,c, tn+k/2)−ˆϵθ(ztn+k,c, tn+k)
−ztn+k,(32)
where ˆϵis the noise prediction model, and zΨ
tn+k/2is the middle point between n+kandn, given
by the following formula:
zΨ
tn+k/2=αtn+k/2
αtn+kztn+k−σtn+k/2(eh1
tn−1)ˆϵθ(ztn+k,c, tn+k) (33)
ForDPM-Solver++ (Lu et al., 2022b), we consider the case for order = 2, DPM-Solver++ re-
places the original noise prediction to data prediction (Lu et al., 2022b), with the detailed formula
ofΨDPM-Solver++ provided as follows.
ΨDPM-Solver++ (ztn+k, tn+k, tn,c)
=σtn
σtn+kztn+k−αtn(e−h0
tn−1)ˆxθ(ztn+k,c, tn+k)
−αtn
2rtn(e−h0
tn−1)
ˆxθ(zΨ
tn+k/2,c, tn+k/2)−ˆxθ(ztn+k,c, tn+k)
−ztn+k,(34)
where ˆxis the data prediction model (Lu et al., 2022a) and zΨ
tn+k/2is the middle point between
n+kandn, given by the following formula:
zΨ
tn+k/2=σtn+k/2
σtn+kztn+k−αtn+k/2(e−h1
tn−1)ˆxθ(ztn+k,c, tn+k) (35)
15

--- PAGE 16 ---
Preprint
F T RAINING DETAILS OF LATENT CONSISTENCY DISTILLATION
As mentioned in Section 5.1, we conduct our experiments in two resolution settings 512 ×512 and
768×768. For the former setting, we use the LAION-Aesthetics-6+ (Schuhmann et al., 2022) 12M
dataset, consisting of 12M text-image pairs with predicted aesthetics scores higher than 6. For the
latter setting, we use the LAIOIN-Aesthetic-6.5+ (Schuhmann et al., 2022), which comprise 650K
text-image pairs with predicted aesthetics scores higher than 6.5.
For 512 ×512 resolution, we train the LCM with the teacher diffusion model Stable-Diffusion-V2.1-
Base (SD-V2.1-Base) (Rombach et al., 2022), which is originally trained on 512 ×512 resolution
images using the ϵ-Prediction (Ho et al., 2020). We train LCM (512 ×512) with 100K iterations
on 8 A100 GPUs, using a batch size of 72, the same learning rate 8e-6 , EMA rate µ= 0.999943
and Rectified Adam optimizer (Liu et al., 2019) used in (Song et al., 2023). We select the DDIM-
Solver (Song et al., 2020a) and skipping step k= 20 in Eq. 17. We set the guidance scale range
[ωmin, ωmax] = [2 ,14], which is consistent with the setting in Guided-Distill (Meng et al., 2023).
During training, we initialize the consistency function fθ(ztn, ω,c, tn)with the same parameters as
the teacher diffusion model (SD-V2.1-Base). To encode the CFG scale ωinto the LCM, we apply-
ing Fourier embedding to ω, integrating it into the origin LCM backbone by adding the projected
ω-embedding into the original embedding, as done in (Meng et al., 2023). We use a zero param-
eter initialization method mentioned in (Zhang & Agrawala, 2023) on projected ω-embedding for
better training stability. For training LCM (512 ×512), we use a augmented consistency function
parameterized in ϵ-prediction as discussed in Appendix. D.
For 768 ×768 resolution, we train the LCM with the teacher diffusion model Stable-Diffusion-V2.1
(SD-V2.1) (Rombach et al., 2022), which is originally trained on 768 ×768 resolution images using
thev-Prediction (Salimans & Ho, 2022). We train LCM (768 ×768) with 100K iterations on 8 A100
GPUs using a batch size of 16, while the other hyper-parameters remain the same as in 512 ×512
resolution setting.
G R EPRODUCTION DETAILS OF GUIDED -DISTILL
Guided-Distill (Meng et al., 2023) serves as a significant baseline for guided distillation but is not
open-sourced. We adhered strictly to the training procedure described in the paper, reproducing
the method for accurate comparisons. For 512 ×512 resolution setting, Guided-Distill (Meng et al.,
2023) used a large batch size of 512, which requires at least 32 A100 GPUs for training. Due to
limited resource, we reduced the batch size to 72 (512 resolution), while set the batchsize to 16 for
768 resolution, the same as ours, and trained for 100K iterations, also the same as in LCM.
Specifically, Guided Distill involves two stages of distillation. For the first stage, it use a student
model to fit the outputs of the pre-trained guided diffusion model using classifier-free guidance
scales ω. The loss function is as follows:
Ew∼pw,t∼U[0,1],x∼pdata(x)[ω(λt)||ˆxη1(zt, w)−ˆxw
θ(zt)||2
2], (36)
where ˆxθ(zt) = (1 + w)ˆxc,θ(zt)−wˆxθ(zt),zt∼q(zt|x)andpw(w) =U[wmin, wmax].
In our implementation, we follow the same training procedure in (Meng et al., 2023) except the
difference of computation resources. For first stage distillation , we train the student model with
25,000 gradient updates (batch size 72), roughly the same computation costs in (Meng et al., 2023)
(3,000 gradient updates, batch size 512), and we reduce the original learning rate 1e−4to5e−5
for smaller batch size. For second stage distillation , we progressively train the student model using
the same schedule as in Guided-Distill (Meng et al., 2023) except for batch size difference. We
train the student model with 2500 gradient updates except when the sampling step equals to 1,2, or
4, where we train for 20000 gradient updates, the same schedule used in (Meng et al., 2023). We
trained until the total number of gradient iterations for the entire stage reached 100K the same as
LCM training. The generation results of Guided Distill are shown in Figure 2. We can also see that
the performances in Table 1 and Table 2 are similar, further verifying the correctness of our Guided-
Distill implementation. Nevertheless, we acknowledge that longer training and more computational
resources can lead to better results as reported in (Meng et al., 2023). However, LCM achieves faster
convergence and superior results under the same computation cost (same batch size, same number
of iterations), demonstrating its practicability and superiority.
16

--- PAGE 17 ---
Preprint
4-Steps Inference
Figure 7: More generated images results with LCM 4-steps inference (768 ×768 Resolution). We
employ LCM to distill the Dreamer-V7 version of SD in just 4,000 training iterations.
H M ORE FEW -STEP INFERENCE RESULTS
We present more images (768 ×768) generation results with LCM using 4 and 2-steps inference in
Figure 7 and Figure 8. It is evident that LCM is capable of synthesizing high-resolution images with
just 2, 4 steps of inference. Moreover, LCM can be derived from any pre-trained Stable Diffusion
(SD) (Rombach et al., 2022) in merely 4,000 training steps, equivalent to around 32 A100 GPU
Hours, showcasing the effectiveness and superiority of LCM.
17

--- PAGE 18 ---
Preprint
2-Steps Inference
Figure 8: More generated images results with LCM 2-steps inference (768 ×768 Resolution). We
employ LCM to distill the Dreamer-V7 version of SD in just 4,000 training iterations.
18
