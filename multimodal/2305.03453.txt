# 2305.03453.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2305.03453.pdf
# File size: 1811548 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large
Language Model Signals for Science Question Answering
Lei Wang1, Yi Hu2, Jiabang He2, Xing Xu2, Ning Liu3, Hui Liu4, Heng Tao Shen2
1Singapore Management University2University of Electronic Science and Technology of China
3Beijing Forestry University4Beijing Rongda Technology Co., Ltd.
lei.wang.2019@phdcs.smu.edu.sg, yihu0118@gmail.com, JiaBangH@outlook.com
xing.xu@uestc.edu.cn, liuning0928@bjfu.edu.cn, ryuki122382@gmail.com, shenhengtao@hotmail.com
Abstract
Large Language Models (LLMs) have recently demonstrated
exceptional performance in various Natural Language Pro-
cessing (NLP) tasks. They have also shown the ability to
perform chain-of-thought (CoT) reasoning to solve complex
problems. Recent studies have explored CoT reasoning in
complex multimodal scenarios, such as the science question
answering task, by fine-tuning multimodal models with high-
quality human-annotated CoT rationales. However, collecting
high-quality COT rationales is usually time-consuming and
costly. Besides, the annotated rationales are hardly accurate
due to the external essential information missed. To address
these issues, we propose a novel method termed T-SciQ that
aims at teaching science question answering with LLM signals.
The T-SciQ approach generates high-quality CoT rationales as
teaching signals and is advanced to train much smaller models
to perform CoT reasoning in complex modalities. Addition-
ally, we introduce a novel data mixing strategy to produce
more effective teaching data samples for simple and complex
science question answer problems. Extensive experimental
results show that our T-SciQ method achieves a new state-
of-the-art performance on the ScienceQA benchmark, with
an accuracy of 96.18%. Moreover, our approach outperforms
the most powerful fine-tuned baseline by 4.5%. The code is
publicly available at https://github.com/T-SciQ/T-SciQ.
Introduction
Scientific problem solving has recently been employed to
evaluate the multi-hop reasoning capability and interpretabil-
ity of AI systems (Kembhavi et al. 2017; Sampat, Yang, and
Baral 2020; Dalvi et al. 2021). However, these datasets (Kem-
bhavi et al. 2017; Jansen et al. 2018) suffer from limited scale.
To address this issue, Lu et al. (2022a) introduces a large-
scale science question-answering dataset across broad topics
and skills called ScienceQA. This dataset consists of 21,208
multimodal data examples associated with questions, context,
images, options, lectures, and explanations. An example is
shown in Figure 1, illustrating that a model must comprehend
multimodal inputs and incorporate external knowledge to
answer scientific questions.
Recently, Large Language Models (LLMs) have shown
exceptional performance in various Natural Language Pro-
cessing (NLP) tasks (Brown et al. 2020; Thoppilan et al.
Copyright ©2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
ProblemSample of ScienceQA
Vision
Lecture: Maps have four cardinal directions, or main directions. Those 
directions are north, south, east, and west. A compass rose is a set of 
arrows that point to the cardinal directions. A compass rose usually 
shows only the first letter of each cardinal direction. The north arrow 
points to the North Pole. On most maps, north is at the top of the map.
Solution: To find the answer, look at the compass rose. Look at which 
way the north arrow is pointing. West Virginia is farthest north.Question: Which of these states is 
farthest north?
Context: N/A
Skill: Read a map: cardinal directions
Options: (A) West Virginia (B) 
Louisiana (C) Arizona (D) Oklahoma
Answer: The answer is (A)
Human -Annotated CoT Signal 
Generated CoT Signal 
Lecture: N/A
Solution: West Virginia is the farthest north of the four states listed. 
West Virginia is located in the Appalachian region of the United States, 
which is in the northeastern part of the country. Louisiana, Arizona, and 
Oklahoma are all located in the southern and southwestern parts of the 
United States. West Virginia is the northernmost of the four states, 
making it the farthest north.Figure 1: The input of a ScienceQA data example includes a
question, context, image, skill, and options. Annotations in-
clude the ground truth answer and CoT rationale (lecture and
solution). Compared to annotated CoT, LLM-generated CoT
includes greater amounts of essential external knowledge.
2022). Specifically, they have demonstrated the chain-of-
thought (CoT) ability to solve complex reasoning problems
by using a few demonstration examples without additional
training (Wei et al. 2022a; Kojima et al. 2022; Zhang et al.
2022). However, the existing research on CoT reasoning is
mainly limited to the language modality (Wang et al. 2022a;
Zhou et al. 2022; Lu et al. 2022b; Fu et al. 2022), with little
attention paid to multimodal scenarios, such as science ques-
tion answering. To address this issue, a common approach
is to use caption models to translate visual information into
the language modality and prompt LLMs to perform CoT
reasoning (Lu et al. 2022a). However, the use of caption gen-
eration models in scientific problems may result in significant
information loss when meeting highly complex images. To
overcome this issue, Zhang et al. (2023b) proposed a frame-
work called Multimodal-CoT that models both language andarXiv:2305.03453v4  [cs.CL]  18 Dec 2023

--- PAGE 2 ---
visual modalities into a two-stage fine-tuning process, which
separates rationale generation and answer inference.
The Multimodal-CoT method has a significant disadvan-
tage because it relies on the human-annotated CoT rationale
to fine-tune the model. While incorporating human-annotated
CoT signals is helpful for training models to facilitate CoT
reasoning ability, it has two fundamental limitations. First, the
human annotation of CoT reasoning is time-consuming (Nye
et al. 2021; Cobbe et al. 2021), particularly for complex tasks
like ScienceQA, which necessitates extensive expert knowl-
edge to create a reasoning process for the answer. Second, as
shown in Figure 1, the annotated rationale may lack essential
external information to derive the final answer due to the
limited expertise of human annotators.
To address these issues, we propose a novel approach
named T-SciQ to solve the ScienceQA task. The proposed
T-SciQ framework consists of three stages: generating teach-
ing data, mixing teaching data, and fine-tuning. For teaching
data generation, we use a simple zero-shot instruction and
a hint of the correct answer to generate a CoT rationale for
a QA data example to obtain a QA-CoT sample. Although
the model taught by QA-CoT samples excels at tackling
simple problems, it still struggles with highly complicated
problems. To overcome this challenge, we follow the zero-
shot plan-and-solve prompting (Wang et al. 2023) to generate
plan-based CoT (PCoT) rationales, which decompose com-
plex problems into simpler subproblems to solve, to obtain
QA-PCoT teaching samples. To combine the strengths of
both teaching signals, we create a new teaching dataset called
T-SciQ by mixing QA-CoT and QA-PCoT datasets. Specif-
ically, we use the validation set to determine whether the
PCoT teaching signal or CoT teaching signal is more ap-
propriate for each data example in a given skill. Then, we
fine-tune the student model with teaching data. We follow
the Multimodal-CoT (Zhang et al. 2023b) to build our stu-
dent model, which consists of two-stage: rationale generation
teaching and answer inference teaching. During inference,
the model trained in the first stage generates rationales for
the test data. The generated rationales are subsequently used
in the second stage to infer answers.
Experiment results on the ScienceQA benchmark show
that our method surpasses the previous state-of-the-art by
a large margin. Specifically, the student model taught by
T-SciQ teaching samples outperforms the most powerful
fine-tuned baseline by 4.5%, the strongest instruction-tuning
based multimodal baseline by 5.26%, the best GPT-4 based
few-shot baseline by 9.64%, and human performance by
7.78%. To demonstrate the versatility of our teaching ap-
proach, we additionally conduct experiments to compare
Reason-Teacher (Ho, Schmid, and Yun 2022) on six reason-
ing tasks. Our main contributions are summarized as follows:
1) We propose a novel framework for generating high-quality
CoT rationale and training student models to perform CoT
reasoning for the ScienceQA task; 2) We introduce a data
mixing strategy to produce effective teaching data samples
for simple and complex problems; 3) Our method achieves a
new state-of-the-art performance on the ScienceQA bench-
mark, surpassing all previous models by a large margin.Related Work
Chain-of-Thought Prompting. Recently, to solve complex
reasoning tasks, Wei et al. (2022b) propose CoT prompting
by prompting large language models to generate interme-
diate reasoning processes before reaching the final answer.
Subsequently, a lot of work has been proposed to further
improve CoT prompting from different aspects, including
improving the quality of demonstrations (Rubin, Herzig, and
Berant 2021; Zhang et al. 2022; Fu et al. 2022; Lu et al.
2022b; He et al. 2023) and improving the quality of reason-
ing chains (Zhou et al. 2022; Khot et al. 2022; Chen et al.
2022; Wang et al. 2022b,a; Li et al. 2022b; Tian et al. 2023).
Zero-shot CoT (Kojima et al. 2022) elicited reasoning step by
appending a prompt like “ Let’s think step by step ” to the test
question. Iterative Prompting (Wang, Deng, and Sun 2022)
dynamically synthesizes prompts conditioned on the current
step’s contexts. PoT Prompting (Chen et al. 2022) writes a
program as a rationale and invoked the reasoning ability of
LLMs by executing the generated program. Chameleon (Lu
et al. 2023) proposed a plug-and-play compositional reason-
ing framework to utilize multiple modules to obtain high
quality prompting. Our work mainly focuses on mixing dif-
ferent teaching CoT rationales for different problems.
LLMs as Teachers. In recent studies, CoT reasoning is
elicited in small models using fine-tuned language models.
Magister et al. (2022) benefit smaller models through CoT
distillation. Huang et al. (2022) show that LLMs can en-
hance reasoning using self-generated solutions from unla-
beled data. Ho, Schmid, and Yun (2022) propose Fine-tune-
CoT to leverage the capabilities of LLMs to generate rea-
soning samples and teach smaller models via fine-tuning.
Distilling step-by-step (Hsieh et al. 2023) improves small
model performance using LLM rationales with less data.
Multimodal-CoT (Zhang et al. 2023b) uses two-stage fine-
tuning with annotated CoT rationales and visual features to
achieve state-of-the-art results on the ScienceQA benchmark.
Our work exploits generating two types of teaching data from
LLMs and mixing teaching data. We discover that this simple
method highly improves student performance in complex
multi-modality tasks, which has not yet been recognized in
previous studies on fine-tuning with CoT reasoning (Hsieh
et al. 2023; Ho, Schmid, and Yun 2022; Huang et al. 2022;
Magister et al. 2022; Fu et al. 2023; Hu et al. 2023).
Our T-SciQ Approach
Overview
This section presents the proposed fine-tuning strategy T-
SciQ, which utilizes a LLM named SciTeacher to generate
teaching data and improve the performance of a smaller stu-
dent model (SciStudent) by generated teaching data. The
proposed T-SciQ strategy comprises three components: gener-
ating teaching data, mixing teaching data, and fine-tuning, as
depicted in Figure 2. To generate the teaching data, we lever-
age SciTeacher to produce CoT rationales to obtain Question-
Answer-CoT (QA-CoT) samples, and planning-based CoT ra-
tionale (PCoT) to obtain Question-Answer-PCoT (QA-PCoT)
samples. To combine the strengths of both datasets, we create
a new teaching dataset called T-SciQ by mixing QA-CoT

--- PAGE 3 ---
Figure 2: Key steps of our T-SciQ approach. T-SciQ consists of three stages: (i) generating teaching data; (ii) mixing teaching
data; and (iii) fine-tuning.
and QA-PCoT datasets. Specifically, we use the validation
set to determine whether the PCoT teaching signal or CoT
teaching signal is more appropriate for each data example in a
given skill. We then use T-SciQ teaching samples to fine-tune
the smaller student models. In the following, we provide a
detailed description of these three components.
Generating Teaching Data
We produce two types of data samples for teaching: QA-CoT
sample with a generated CoT rationale and QA-PCoT sample
equipped with a generated PCoT rationale.
QA-CoT Sample Generation. Although using human-
annotated CoT signals is valuable for training models to
elicit CoT reasoning ability, it has two inherent limitations:
time-consuming and lack of external essential information
due to human annotators’ restricted expertise.
To address these issues, we introduce a zero-shot prompt-
ing to generate high-quality CoT rationales from LLMs. We
achieve this by converting the input training data example X
into a prompt, utilizing a straightforward template that reads
as follows: “Question: [Xq]. Context: [Xc]. Options: [Xo].
Correct Answer: [A].[Instruct ]”. Here, the [Xq]slot is
for the input question, the [Xc]slot is for the input context,
the[Xo]slot contains the possible options, the [A]slot is
for the correct answer that can work as a hint to guide LLMs
to generate a more reliable rationale, and the [Instruct ]
slot contains instructions, i.e., “ Please give me a detailed
explanation. ”, to guide LLMs to perform the task. Note that
the context may not be included for some data examples,
in which case the context slot is replaced with “N/A”. Sub-
sequently, we feed the filled prompt to LLMs to output a
reasoning process for a given training data example to obtain
QA-CoT data DQA-CoT .
QA-PCoT Sample Generation. Although using QA-CoT
samples can address issues of human-annotated CoT, ad-
dressing highly complex problems remains a challenge. To
overcome this challenge and obtain appropriate teaching CoT
rationale, we introduce a 3-step zero-shot prompting to de-
compose complex problems into simpler subproblems.Step 1: Lecture Generation. The lecture template used to
generate a lecture for a particular skill is formulated as fol-
lows: “Skill: [S]. QA pairs: [Xq, A]...[Instruct ].” In
this prompt, [Instruct ]is as follows: “ based on the prob-
lems above, please give a general lecture on the [S]type
of question in one sentence. ”. Note that many QA examples
need the same skill to be solved.
Step 2: Plan Generation. The template used to generate
a plan for a specific skill based on the generated lecture
is formulated as follows: “Skill: [S]. Lecture: [L]. QA
pairs: [Xq, A]...[Instruct ].”. In this prompt, [Instruct ]
is written as follows: “ Based on the lecture above and these
problems, let’s understand these problems and devise a gen-
eral and brief plan step by step to solve these problems (begin
with 1, 2, 3...) ”.
Step 3: Rationale Generation. The lecture and plan gener-
ated by the first two prompts are used to generate a plan-based
CoT rationale for each training example. The rationale gener-
ation template is formulated as follows: “Skill: [S]. Lecture:
[L]. Plan: [P]. QA pair: [Xq, A].[Instruct ].”. In this
prompt, [Instruct ]is written as follows: “Based on the
lecture, the plan and the problem, please carry out the plan
and solve the problem step by step (begin with 1, 2, 3...)”.
Examples of this three-step prompting can be found in the
supplementary material.
Mixing Teaching Data
The QA-PCoT dataset is effective for teaching problem-
solving skills for complex problems, while simpler prob-
lems don’t require decomposition. In contrast, the QA-CoT
dataset is suitable for teaching problem-solving skills for sim-
ple problems. To combine the strengths of both datasets, we
create a new teaching dataset called T-SciQ by mixing QA-
CoT and QA-PCoT datasets. We introduce a new approach
that uses the validation set to determine whether the PCoT
teaching signal or CoT teaching signal is more appropriate
for a data example in a given skill.
Given a ScienceQA problem Piwith the language input
Xi,laand the visual input Xi,v, our objective is to let an an-
swer generation model Fs
ahelp identify the optimal teaching
signal Ti,kfrom the possible choices Ti, i.e., CoT teaching

--- PAGE 4 ---
signal Ti,cotor PCoT teaching signal Ti,pcot, thereby maxi-
mizing the answer accuracy of the validation set. The an-
swer generation module Fs
ais similar to the one described in
Multimodal-CoT (Zhang et al. 2023b). The generated answer
ˆAiis produced by Fs
a(Xi,la, Xi,v, Ti,k), and the number of
errors is obtained by comparing the generated answer ˆAiand
the label Ai. If the number of errors for validation samples
with PCoT in a skill is lower than that of validation samples
with CoT in a skill, we select PCoT rationale as the teaching
rationale for all training data examples in this skill. Otherwise,
we select CoT rationale. The obtained teaching samples are
then used to fine-tune the student model. To train the answer
generation module, we utilize a subset of training data exam-
ples, each of which is associated with the human-annotated
teaching signal from the original ScienceQA dataset.
Fine-Tuning
Our teaching follows the Multimodal-CoT (Zhang et al.
2023b) two-stage fine-tuning framework: rationale gener-
ation teaching and answer inference teaching.
Rationale Generation Teaching. In this stage, the ratio-
nale generation model Fr(Pi)is trained to predict the teach-
ing signal Tifor a given problem Pi, where Tieither be CoT
rationale or PCoT rationale. The input of Fr(Pi)consists of
X1
i,laandXi,v, where X1
i,larepresents the language input and
Xi,vrepresents the visual input. Formally, the probability of
generating rationale Tican be formulated as follows:
p(Ti∣Xi,la1, Xi,v)=NTi
∏
j=1pθr(Ti,j∣X1
i,la, Xi,v, Ti,<j),(1)
where θrrepresents learnable parameters of the rationale
generation model FrandNTiis the length of Ti.
Answer Inference Teaching. In the second stage, we con-
struct the language input X2
i,laby appending the teaching
rationale Tito the original language input X1
i,la. The new
input X′
iis then fed to the answer inference model to infer
the final answer Ai=Fa(X′
i), where X′
i={X2
i,la, Xi,v}.
Formally, the probability of generating answer Aican be
formulated as follows:
p(Ai∣X2
i,la, Xi,v)=NAi
∏
j=1pθa(Ai∣X2
i,la, Xi,v, Ai,<j),(2)
where θarepresents learnable parameters in the answer infer-
ence teaching stage.
Model Architecture We utilize the Multimodal-
CoT (Zhang et al. 2023b) model architecture as our default,
which employs a Transformer model (Vaswani et al. 2017)
for encoding language and a vision Transformer for encoding
visual information. The gated fusion mechanism, proposed in
(Li et al. 2022a), is used to effectively integrate the language
and vision representations. Finally, a Transformer decoder
is used to generate the target output. Note that rationale
generation and answer inference share the same model but
differ in the input and output.Experiment
Experimental Setup
Dataset. We evaluate our proposed method on the Sci-
enceQA (Lu et al. 2022a) dataset, a latest multimodal
multiple-choice science question dataset comprising 21,208
examples. ScienceQA encompasses a wide range of topics
across three distinct subjects: natural science, social science,
and language science. The dataset comprises 26 topics, 127
categories, and 379 skills that are relevant to these three sub-
jects. We employ the official split provided by ScienceQA,
which divides the dataset into training, validation, and test
sets with a ratio of 3:1:1, i.e., 12,726,4,241, and 4,241
examples, respectively. The dataset includes annotated rea-
soning chains for each data example. In this work, we extract
our training signals from large language models instead of
using human annotated signals.
Baselines. We provide a comparison of our proposed method
with extensive baseline methods. Specifically, we have sev-
eral early VQA models, including MCAN (Yu et al. 2019),
Top-Down (Anderson et al. 2018), BAN(Kim, Jun, and Zhang
2018), DFAF (Gao et al. 2019). These VQA baselines use
the question, context, and answer choices as textual input
and the image as the visual input. They predict a score dis-
tribution over the answer candidates using a linear classifier.
In addition, we include pre-trained text-to-text and multi-
modal models such as ViLT (Kim, Son, and Kim 2021),
Patch-TRM (Lu et al. 2021), and VisualBERT (Li et al.
2019), UnifiedQA (Khashabi et al. 2020), MM-COT (Zhang
et al. 2023b). These methods use pre-trained models as
backbone models and incorporate additional modules to
handle multimodal signals if necessary. We also include
recent LLM-based multimodal fine-tuned baselines such
asLLaMa-Adapter (Zhang et al. 2023a) and LLaVA (Liu
et al. 2023). They use a strong open-access LLM such as
LLaMa (Touvron et al. 2023) as the base model and incor-
porate a vision module to model visual information. We
also include widely-used in-context learning baselines: the
chain of thought (CoT) prompting (Wei et al. 2022a), where
each in-context demonstration example comprises the input
question and output annotated reasoning process. We com-
pare to the CoT baselines over different API-based OpenAI
LLMs (OpenAI 2022, 2023), such as GPT-3.5 ( GPT-3.5
w/ COT ), ChatGPT ( ChatGPT w/ COT ), GPT-4 ( GPT-4 w/
COT), and Chameleon (Lu et al. 2023). Additionally, we also
compare to the standard few-shot prompting approach using
GPT-3.5 ( GPT-3.5 ).
Evaluation Metrics. As ScienceQA is a benchmark for
multiple-choice question answering, the accuracy of the an-
swer is evaluated by comparing the ground truth option with
the final prediction generated by the evaluated model.
Implementation Details. By default, we utilize the GPT-
3.5 of text-davinci-003 version as the teacher model for our
approach unless otherwise specified. To validate the gen-
eralizability of our method, we experiment with three dis-
tinct student models, namely UnifiedQA Basew/ CoT (Lu
et al. 2022a), Mutimodal-CoT Base(Lu et al. 2022a), and
Mutimodal-CoT Large (Zhang et al. 2023b). These models are
chosen due to their strong performances achieved by fine-

--- PAGE 5 ---
Table 1: Main results (%) on the test set of ScienceQA. There are totally 8 classes of questions, namely natural science ( NAT ),
social science ( SOC ), language science ( LAN ), text context ( TXT ), image context ( IMG ), no context ( NO), grades 1-6 ( G1-6 ),
and grades 7-12 ( G7-12 ). The best results are boldfaced. The improvements are shown in blue.
Model Size NAT SOC LAN TXT IMG NO G1-6 G7-12 Avg
Human -90.23 84.97 87.48 89.60 87.50 88.10 91.59 82.42 88.40
MCAN (Yu et al. 2019) 95M 56.08 46.23 58.09 59.43 51.17 55.40 51.65 59.72 54.54
Top-Down (Anderson et al. 2018) 70M 59.50 54.33 61.82 62.90 54.88 59.79 57.27 62.16 59.02
BAN (Kim, Jun, and Zhang 2018) 112M 60.88 46.57 66.64 62.61 52.60 65.51 56.83 63.94 59.37
DFAF (Gao et al. 2019) 74M 64.03 48.82 63.55 65.88 54.49 64.11 57.12 67.17 60.72
ViLT (Kim, Son, and Kim 2021) 113M 60.48 63.89 60.27 63.20 61.38 57.00 60.72 61.90 61.14
Patch-TRM (Lu et al. 2021) 90M 65.19 46.79 65.55 66.96 55.28 64.95 58.04 67.50 61.42
VisualBERT (Li et al. 2019) 111M 59.33 69.18 61.18 62.71 62.17 58.54 62.96 59.92 61.87
UnifiedQA Base(Khashabi et al. 2020) 223M 68.16 69.18 74.91 63.78 61.38 77.84 72.98 65.00 70.12
LLaMa-Adapter (Zhang et al. 2023a) >7B 84.37 88.30 84.36 83.72 80.32 86.90 85.83 84.05 85.19
LLaV A (Liu et al. 2023) >7B 90.36 95.95 88.00 89.49 88.00 90.66 90.93 90.90 90.92
GPT-3.5 (Chen et al. 2020) >175B 74.64 69.74 76.00 74.44 67.28 77.42 76.80 68.89 73.97
GPT-3.5 w/ CoT (Lu et al. 2022a) >175B 75.44 70.87 78.09 74.68 67.43 79.93 78.23 69.68 75.17
ChatGPT w/ CoT (Lu et al. 2023) >175B 78.82 70.98 83.18 77.37 67.92 86.13 80.72 74.03 78.31
GPT-4 w/ CoT (Lu et al. 2023) >175B 84.06 73.45 87.36 81.87 70.75 90.73 84.69 79.10 82.69
Chameleon (Lu et al. 2023) >175B 89.83 74.13 89.82 88.27 77.64 92.13 88.03 83.72 86.54
UnifiedQA-CoT Base(Lu et al. 2022a) 223M 71.00 76.04 78.91 66.42 66.53 81.81 77.06 68.82 74.11
UnifiedQA-T-SciQ Base(Ours) 223M 76.56 88.99 80.45 72.90 73.84 83.47 81.09 75.19 79.41
Improvement -+5.56 +12.95 +1.54 +6.48 +7.31 +1.66 +4.03 +6.37 +5.30
Mutimodal-CoT Base(Zhang et al. 2023b) 223M 87.52 77.17 85.82 87.88 82.90 86.83 84.65 85.37 84.91
Mutimodal-T-SciQ Base(Ours) 223M 91.52 91.45 92.45 91.94 90.33 92.26 92.11 91.10 91.75
Improvement -+4.00 +14.28 +6.63 +4.06 +7.43 +5.43 +7.46 +5.73 +6.84
Mutimodal-CoT Large (Zhang et al. 2023b) 738M 95.91 82.00 90.82 95.26 88.80 92.89 92.44 90.31 91.68
Mutimodal-T-SciQ Large (Ours) 738M 96.89 95.16 95.55 96.53 94.70 96.79 96.44 95.72 96.18
Improvement -+0.98 +13.16 +4.73 +1.27 +5.90 +3.90 +4.00 +5.41 +4.50
Table 2: Ablation study of the impact of different signals
provided by LLMs across all topics.
Model Avg
Multimodal-T-SciQ Base(Mixing) 91.75
Multimodal-T-SciQ Baseonly w/ QA-CoT 85.99
Multimodal-T-SciQ Baseonly w/ QA-PCoT 88.56
Mutimodal-CoT Base 84.91
Multimodal-T-SciQ Large (Mixing) 96.18
Multimodal-T-SciQ Large only w/ QA-CoT 93.44
Multimodal-T-SciQ Large only w/ QA-PCoT 94.11
Mutimodal-CoT Large 91.68
tuning with annotated reasoning signals. To ensure fairness
of comparison and effectiveness of our proposed method, we
only replace the training signals generated by our approach
with annotated signals while maintaining the same settings
as the original paper. These student models are 200 ×smaller
than their teacher models.
Main Results
T-SciQ v.s.Baselines. Table 1 details the performance ac-
curacy of baselines and student models trained using the
proposed T-SciQ signals. Mutimodal-T-SciQ Large , which is
the model architecture of Mutimodal-CoT Large fine-tuned
with mixed teacher signals, attains an accuracy of 96.18%
and consistently outperforms all state-of-the-art methods
by a large margin for all topics across all subjects. Specifi-cally, Mutimodal-T-SciQ Large outperforms the most powerful
fine-tuning baseline, Mutimodal-CoT Large , which is trained
by annotated chain-of-thought signals, by 4.5% (91.68%
→96.18%), the strongest instruction-tuning based multi-
modal baseline, LLaVa, by 5.26% (90.92% →96.18%), the
best GPT-4 based few-shot baseline, Chameleon, by 9.64%
(86.54% →96.18%), and human performance by 7.78%
(88.40% →96.18%). This significant improvement of our
proposed method suggests that higher-quality teaching sig-
nals of planning and reasoning provided by LLMs elicit better
planning and chain-of-thought reasoning ability in student
models smaller than 1B.
T-SciQ with Different Base Student Models. Instead of
only using the model architecture of Mutimodal-CoT Large
as the base student model, we evaluate different base stu-
dent models fine-tuned with mixed teaching signals: the
variant UnifiedQA-T-SciQ Baseand Mutimodal-T-SciQ Base.
The relative performance ranking between the base student
model with annotated CoT signals and the one with mixing
teacher signals remains unchanged. Specifically, UnifiedQA-
T-SciQ Base outperforms UnifiedQA Base w/ CoT by 5.3%
(74.11% →79.41%), and Mutimodal-T-SciQ Base outper-
forms Mutimodal-CoT Baseby 6.84% (84.91% →91.75%).
T-SciQ still achieves the best performance with different
base student models. These encouraging results indicate the
generalizability of the proposed teaching signals.

--- PAGE 6 ---
Table 3: Accuracy (%) of Mutimodal-T-SciQ Baseusing dif-
ferent visual features.
MethodT-SciQ
QA-CoT QA-PCoT T-SciQ
Language Only 84.44 85.38 87.24
w/ CLIP 86.18 87.41 90.90
w/ DETR 85.99 88.56 91.75
w/ ResNet 86.06 87.69 91.44
/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013 /uni0000001b/uni00000013 /uni00000014/uni00000013/uni00000013
/uni00000033/uni00000055/uni00000052/uni00000053/uni00000052/uni00000055/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044/uni00000003/uni0000000b/uni00000008/uni0000000c/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni0000001c/uni00000018/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni0000005a/uni00000012/uni00000003/uni00000030/uni0000004c/uni0000005b/uni00000057/uni00000058/uni00000055/uni00000048
/uni0000005a/uni00000012/uni00000003/uni00000034/uni00000024/uni00000010/uni00000026/uni00000052/uni00000037
/uni0000005a/uni00000012/uni00000003/uni00000034/uni00000024/uni00000010/uni00000033/uni00000026/uni00000052/uni00000037
(a)
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000001b/uni00000013/uni0000001b/uni00000018/uni0000001c/uni00000013/uni0000001c/uni00000018/uni00000014/uni00000013/uni00000013/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000037/uni00000010/uni00000036/uni00000046/uni0000004c/uni00000034/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c (b)
Figure 3: Further analysis on (a) the effect of Mutimodal-T-
SciQ Basetrained with different proportion of generated data
and (b) accuracy curve of the baseline Mutimodal-CoT Base
and our Mutimodal-T-SciQ Baseacross epochs.
Further Analysis
Effect of Different Signals of T-SciQ. Our approach in-
corporates two distinct components for teaching signals: QA-
CoT and QA-PCoT. We early show that combining these
two signals (i.e., Mutimodal-T-SciQ) yields significantly bet-
ter results than using only human-annotated CoT signals
(i.e., Mutimodal-CoT) when teaching student models. In this
section, we aim to evaluate the impact of each teaching sig-
nals by testing the performance of Mutimodal-T-SciQ Base
and Mutimodal-T-SciQ Large when either QA-CoT or QA-
PCoT signal is removed. As demonstrated in Table 2, we can
observe a significant decrease in answering accuracy when
either teaching signal was removed. These findings indicate
the effectiveness of both proposed teaching signals. This is
because 1) student models taught by QA-CoT signals can
incorporate a more extensive range of knowledge from the
open world rather than solely relying on the knowledge of
annotators and 2) student models taught by QA-PCoT sig-
nals can decompose complex problems into several simpler
sub-problems.
Impact of visual Features. The choice of visual features
can significantly affect the performance of models on Sci-
enceQA. Thus, we conduct an evaluation of three widely-
used visual features, which are CLIP (Radford et al. 2021),
DETR (Carion et al. 2020), and ResNet (He et al. 2016). Both
CLIP and DETR can provide patch-level features, and DETR
is designed for object detection. As for ResNet features, we
use ResNet-50 to derive visual features. Table 3 shows the
results of comparing these three visual features. Our find-
ings suggest that incorporating visual features yields superior
performance than relying on language-only baselines. No-
tably, DETR consistently outperforms the other two features
in most cases, and hence, we adopt it as the default visual
feature in our main experiments.
/uni00000013/uni00000013/uni00000015 /uni00000013/uni00000013/uni00000016 /uni00000046/uni0000004b/uni00000044/uni00000057
/uni00000034/uni00000024/uni00000010/uni00000033/uni00000026/uni00000052/uni00000037/uni00000013/uni00000013/uni00000015 /uni00000013/uni00000013/uni00000016 /uni00000046/uni0000004b/uni00000044/uni00000057/uni00000034/uni00000024/uni00000010/uni00000026/uni00000052/uni00000037/uni0000001b/uni0000001c/uni00000011/uni00000015/uni0000001c /uni0000001c/uni00000014/uni00000011/uni0000001a/uni00000018 /uni0000001c/uni00000014/uni00000011/uni00000015/uni00000018
/uni0000001b/uni0000001a/uni00000011/uni00000019/uni00000017 /uni0000001c/uni00000014/uni00000011/uni0000001a/uni00000018 /uni0000001c/uni00000013/uni00000011/uni00000014/uni00000015
/uni0000001b/uni0000001b/uni00000011/uni00000013/uni00000013 /uni0000001b/uni0000001c/uni00000011/uni00000017/uni00000019 /uni0000001b/uni0000001a/uni00000011/uni0000001b/uni00000016(a)
/uni00000024 /uni00000025 /uni00000026 /uni00000027 /uni00000028 /uni00000029
/uni00000036/uni0000004e/uni0000004c/uni0000004f/uni0000004f/uni00000056/uni00000013/uni00000014/uni00000013/uni00000015/uni00000013/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000019/uni00000013/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000028/uni00000055/uni00000055/uni00000052/uni00000055/uni00000056/uni00000025/uni00000044/uni00000056/uni00000048/uni0000004f/uni0000004c/uni00000051/uni00000048
/uni00000037/uni00000010/uni00000036/uni00000046/uni0000004c/uni00000034/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c (b)
Figure 4: Further analysis on (a) accuracy (%) of Mutimodal-
T-SciQ Basewith teaching signals provided by different base
LLMs and (b) error analysis of prediction for specific skills.
Proportion of Generated Data in Training Data. To fur-
ther compare the T-SciQ signals produced by LLMs and the
annotated CoT signals, we experiment with manipulating
the proportion of these two signals within the training data.
We vary the proportion of T-SciQ signals from 0% to 100%.
As demonstrated in Figure 3a, the increasing proportion of
training data with T-SciQ signals increases performance.
Performance Change with Epoch. Figure 3b shows the
performance trends of the baseline Mutimodal-CoT Baseand
our proposed Mutimodal-T-SciQ Baseacross different training
epochs. Notably, our method consistently outperforms the
baseline across all epochs. We adopt a two-stage training
approach similar to the baseline Mutimodal-CoT Base, where
we first train the explanation generation module and then
train the answer prediction. Hence, like the baseline, our
method exhibits relatively higher accuracy at the initial train-
ing stages.
Effect of Teaching Signals Provided by Different Base
LLMs. We use the GPT-3.5 model by default, specifically
the text-davinci-003 version, to generate teaching signals in
the main experiment. However, other powerful LLMs can
also provide useful signals, such as the earlier version of
GPT-3.5, text-davinci-002, and the recently popular ChatGPT
model. This study explores the effectiveness of a mixture of
QA-CoT signals from text-davinci-002, text-davinci-003, or
ChatGPT, and QA-PCoT signals from the above API-based
models. We conduct this experiment using the Multimodal-T-
SciQ Base. Figure 4a shows the comparison of the performance
of nine different mixture strategies. Our results show that
even the worst strategy, which involves a mixture of QA-
CoT signal from text-davinci-003 and QA-PCoT signal from
text-davince-002, outperforms annotated CoT signal by a
significant margin. It indicates that regardless of the mixture
strategy used, LLMs can provide signals with more useful
knowledge from the open world.
Error Analysis. To better understand the model’s behavior
trained using our proposed T-SciQ signals, we analyze six
selected skills shown in Figure 4b. It shows the error analysis
of prediction for six specific skills (A-F), i.e., “ Using guide
words ”, “Comparing properties of objects ”, “Reading a map:
cardinal directions ”, “Identifying oceans and continents ”,

--- PAGE 7 ---
Rationale: 1. ... 2. ... 3. ... Louisiana is to the west of the other states, 
Ohio is to the north, Maine is to the east, and Tennessee is to the 
south. Therefore, Maine is farthest east. 4. Select the correct answer 
from the given options. The correct answer is (C) Maine.
Answer: The answer is (C)VisionProblem
Question: Which of these states is 
farthest east?
Context: N/A
Options: (A) Louisiana (B) Ohio (C) 
Maine (D) Tennessee
Rationale: ... To find the answer, look at the compass rose. Look at 
which way the east arrow is pointing. Ohio is farthest east.
Answer: The answer is (B)
MM -CoT
T-SciQ
(a)
Rationale: 1. Read the question carefully … 2. Determine if the word 
given is between the two guide words 3. If the word is between the 
two guide words … 4. If the question asks for a word between the 
two guide words … so the answer is "stockade".
Answer: The answer is (B)VisionProblem
Question: Which word would you 
find on a dictionary page with the 
following guide words? shot -suit
Context: N/A
Options: (A) service (B) stockade
Rationale: ... Since serviceade is between the guide words shot -suit, 
it would be found on that page.
Answer: The answer is (A)MM -CoT
T-SciQ
N/A (b)
Figure 5: Examples of MM-CoT (baseline) and the model trained with T-SciQ (ours) signals for generating rationales and
predicting answers. To solve these examples, commonsense knowledge such as geographic knowledge (a) and multi-step
reasoning (b) are required.
Table 4: Accuracy (%) on other six reasoning datasets.
MethodAqua Date Shuffled Coin Common Strategy
Unders. Objects Flip SenseQA QA
Reason-Teacher 24.02 60.36 64.44 98.67 56.76 55.02
T-SciQ 74.80 89.29 70.28 98.67 70.76 76.74
“How is temperature related to thermal energy? ”, and “ Iden-
tifying the Thirteen Colonies ”, respectively. We can observe
that training with T-SciQ signals can significantly reduce the
number of errors. Examples of skills such as “ Identifying
oceans and continents ” require multi-step complex reasoning
that T-SciQ teaching signals can teach. On the other hand, ex-
amples of skills such as “ Reading a map: cardinal directions ”
require common sense and factual knowledge from the open
world, which T-SciQ signals can also provide.
Case Study. The case study compares T-SciQ and
Multimodal-CoT on the ScienceQA benchmark (Figure 5).
Figure 5a shows cases needing geographic knowledge.
Human-annotated CoT may lack open-world information,
while T-SciQ includes it. Figure 5b shows a multi-step rea-
soning case without image input. Multimodal-CoT errors
while our model decomposes and answers correctly. These
highlight that T-SciQ is well-suited to handle problems that
require open knowledge and decomposition.
Comparison on Other NLP Reasoning Datasets.
To verify the versatility of our teaching approach, we addi-
tionally assess our approach on six reasoning tasks, follow-
ing Reason-Teacher (Ho, Schmid, and Yun 2022): arithmetic
(Aqua (Ling et al. 2017)), symbolic (Coin Flip (Wei et al.
2022b)), commonsense (CommonSenseQA (Talmor et al.
2018), StrategyQA (Geva et al. 2021)) reasoning, and logic
(Date Understanding, Tracking Shuffled Objects) (Geva et al.
2021). In Table 4, we compare T-SciQ to diverse reasoning
teaching signals introduced by Reason-Teacher. The resultsshow that our T-SciQ surpasses Reason-Teacher by a large
margin in 5 out of 6 datasets. It performs equally well in
the remaining dataset, Coin Flip. These results indicate that
higher-quality teaching signals of planning and reasoning can
lead to a remarkable improvement in small student models
across different scenarios.
Conclusion
This paper introduces a new approach named T-SciQ that
utilizes large language models’ chain-of-thought (CoT) rea-
soning capabilities to teach small multimodal models for
complex science question answering tasks. Our zero-shot
prompting method generates QA-CoT samples as teaching
data. We also present a 3-step zero-shot prompting approach
using plan-based CoT for highly complex problems. Fur-
thermore, our data mixture strategy combines CoT and plan-
based CoT to create a new T-SciQ teaching dataset. Empiri-
cal evaluation on ScienceQA shows significant improvement
over previous state-of-the-art baselines. Our method over-
comes the limitations of human-annotated CoT, providing
a promising approach for complex science question answer-
ing. Future work includes exploring extensive LLMs and
parameter-efficient fine-tuning with LLM teachers.
References
Anderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;
Gould, S.; and Zhang, L. 2018. Bottom-up and top-down
attention for image captioning and visual question answering.
InProceedings of the IEEE conference on computer vision
and pattern recognition , 6077–6086.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners.

--- PAGE 8 ---
Advances in neural information processing systems , 33: 1877–
1901.
Carion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,
A.; and Zagoruyko, S. 2020. End-to-End Object Detection
with Transformers. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part I , 213–229.
Chen, T.; Kornblith, S.; Swersky, K.; Norouzi, M.; and Hin-
ton, G. E. 2020. Big self-supervised models are strong semi-
supervised learners. Advances in neural information process-
ing systems , 33: 22243–22255.
Chen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022. Pro-
gram of thoughts prompting: Disentangling computation
from reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .
Cobbe, K.; Kosaraju, V .; Bavarian, M.; Chen, M.; Jun, H.;
Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.;
et al. 2021. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168 .
Dalvi, B.; Jansen, P.; Tafjord, O.; Xie, Z.; Smith, H.; Pi-
patanangkura, L.; and Clark, P. 2021. Explaining answers
with entailment trees. Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP) .
Fu, Y .; Peng, H.; Ou, L.; Sabharwal, A.; and Khot, T. 2023.
Specializing Smaller Language Models towards Multi-Step
Reasoning. arXiv preprint arXiv:2301.12726 .
Fu, Y .; Peng, H.; Sabharwal, A.; Clark, P.; and Khot, T. 2022.
Complexity-based prompting for multi-step reasoning. arXiv
preprint arXiv:2210.00720 .
Gao, P.; Jiang, Z.; You, H.; Lu, P.; Hoi, S. C.; Wang, X.; and
Li, H. 2019. Dynamic fusion with intra-and inter-modality
attention flow for visual question answering. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , 6639–6648.
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Be-
rant, J. 2021. Did aristotle use a laptop? a question answering
benchmark with implicit reasoning strategies. Transactions
of the Association for Computational Linguistics , 9: 346–361.
He, J.; Wang, L.; Hu, Y .; Liu, N.; Liu, H.; Xu, X.; and Shen,
H. T. 2023. ICL-D3IE: In-context learning with diverse
demonstrations updating for document information extrac-
tion. arXiv preprint arXiv:2303.05063 .
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016,
Las Vegas, NV , USA, June 27-30, 2016 , 770–778. IEEE Com-
puter Society.
Ho, N.; Schmid, L.; and Yun, S.-Y . 2022. Large Lan-
guage Models Are Reasoning Teachers. arXiv preprint
arXiv:2212.10071 .
Hsieh, C.-Y .; Li, C.-L.; Yeh, C.-K.; Nakhost, H.; Fujii, Y .;
Ratner, A.; Krishna, R.; Lee, C.-Y .; and Pfister, T. 2023. Dis-
tilling step-by-step! outperforming larger language models
with less training data and smaller model sizes. arXiv preprint
arXiv:2305.02301 .Hu, Z.; Lan, Y .; Wang, L.; Xu, W.; Lim, E.-P.; Lee, R. K.-W.;
Bing, L.; and Poria, S. 2023. LLM-Adapters: An Adapter
Family for Parameter-Efficient Fine-Tuning of Large Lan-
guage Models. arXiv preprint arXiv:2304.01933 .
Huang, J.; Gu, S. S.; Hou, L.; Wu, Y .; Wang, X.; Yu, H.; and
Han, J. 2022. Large language models can self-improve. arXiv
preprint arXiv:2210.11610 .
Jansen, P. A.; Wainwright, E.; Marmorstein, S.; and Morrison,
C. T. 2018. Worldtree: A corpus of explanation graphs for
elementary science questions supporting multi-hop inference.
arXiv preprint arXiv:1802.03052 .
Kembhavi, A.; Seo, M.; Schwenk, D.; Choi, J.; Farhadi, A.;
and Hajishirzi, H. 2017. Are you smarter than a sixth grader?
textbook question answering for multimodal machine com-
prehension. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 4999–5007.
Khashabi, D.; Min, S.; Khot, T.; Sabharwal, A.; Tafjord,
O.; Clark, P.; and Hajishirzi, H. 2020. Unifiedqa: Crossing
format boundaries with a single qa system. arXiv preprint
arXiv:2005.00700 .
Khot, T.; Trivedi, H.; Finlayson, M.; Fu, Y .; Richardson, K.;
Clark, P.; and Sabharwal, A. 2022. Decomposed prompt-
ing: A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406 .
Kim, J.-H.; Jun, J.; and Zhang, B.-T. 2018. Bilinear atten-
tion networks. Advances in neural information processing
systems , 31.
Kim, W.; Son, B.; and Kim, I. 2021. Vilt: Vision-and-
language transformer without convolution or region super-
vision. In International Conference on Machine Learning ,
5583–5594. PMLR.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa, Y .
2022. Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916 .
Li, B.; Lv, C.; Zhou, Z.; Zhou, T.; Xiao, T.; Ma, A.; and
Zhu, J. 2022a. On Vision Features in Multimodal Machine
Translation. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1:
Long Papers) , 6327–6337.
Li, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-
W. 2019. Visualbert: A simple and performant baseline for
vision and language. arXiv preprint arXiv:1908.03557 .
Li, Y .; Lin, Z.; Zhang, S.; Fu, Q.; Chen, B.; Lou, J.-G.; and
Chen, W. 2022b. On the advance of making language models
better reasoners. arXiv preprint arXiv:2206.02336 .
Ling, W.; Yogatama, D.; Dyer, C.; and Blunsom, P. 2017.
Program induction by rationale generation: Learning to
solve and explain algebraic word problems. arXiv preprint
arXiv:1705.04146 .
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual instruction
tuning. arXiv preprint arXiv:2304.08485 .
Lu, P.; Mishra, S.; Xia, T.; Qiu, L.; Chang, K.-W.; Zhu, S.-C.;
Tafjord, O.; Clark, P.; and Kalyan, A. 2022a. Learn to explain:
Multimodal reasoning via thought chains for science ques-
tion answering. Advances in Neural Information Processing
Systems , 35: 2507–2521.

--- PAGE 9 ---
Lu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,
Y . N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-and-
Play Compositional Reasoning with Large Language Models.
arXiv preprint arXiv:2304.09842 .
Lu, P.; Qiu, L.; Chang, K.-W.; Wu, Y . N.; Zhu, S.-C.; Rajpuro-
hit, T.; Clark, P.; and Kalyan, A. 2022b. Dynamic prompt
learning via policy gradient for semi-structured mathematical
reasoning. arXiv preprint arXiv:2209.14610 .
Lu, P.; Qiu, L.; Chen, J.; Xia, T.; Zhao, Y .; Zhang, W.; Yu,
Z.; Liang, X.; and Zhu, S.-C. 2021. Iconqa: A new bench-
mark for abstract diagram understanding and visual language
reasoning. arXiv preprint arXiv:2110.13214 .
Magister, L. C.; Mallinson, J.; Adamek, J.; Malmi, E.; and
Severyn, A. 2022. Teaching small language models to reason.
arXiv preprint arXiv:2212.08410 .
Nye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski, H.;
Austin, J.; Bieber, D.; Dohan, D.; Lewkowycz, A.; Bosma,
M.; Luan, D.; et al. 2021. Show your work: Scratchpads
for intermediate computation with language models. arXiv
preprint arXiv:2112.00114 .
OpenAI. 2022. Introducing chatgpt. https://openai.com/blog/
chatgpt.
OpenAI. 2023. GPT-4 Technical Report. CoRR ,
abs/2303.08774.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from nat-
ural language supervision. In International Conference on
Machine Learning , 8748–8763. PMLR.
Rubin, O.; Herzig, J.; and Berant, J. 2021. Learning to
retrieve prompts for in-context learning. arXiv preprint
arXiv:2112.08633 .
Sampat, S. K.; Yang, Y .; and Baral, C. 2020. Visuo-Lingustic
Question Answering (VLQA) Challenge. In Proceedings
of the 2020 Conference on Empirical Methods in Natural
Language Processing: Findings (EMNLP) , 4606–4616.
Talmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2018. Com-
monsenseqa: A question answering challenge targeting com-
monsense knowledge. arXiv preprint arXiv:1811.00937 .
Thoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kul-
shreshtha, A.; Cheng, H.-T.; Jin, A.; Bos, T.; Baker, L.; Du,
Y .; et al. 2022. Lamda: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Tian, Q.; Zhu, H.; Wang, L.; Li, Y .; and Lan, Y . 2023. R3
Prompting: Review, Rephrase and Resolve for Chain-of-
Thought Reasoning in Large Language Models under Noisy
Context. arXiv preprint arXiv:2310.16535 .
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023. Llama: Open and efficient founda-
tion language models. arXiv preprint arXiv:2302.13971 .
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-
tention is All you Need. In Advances in Neural Information
Processing Systems 30 , 5998–6008.Wang, B.; Deng, X.; and Sun, H. 2022. Iteratively prompt pre-
trained language models for chain of thought. In Proceedings
of the 2022 Conference on Empirical Methods in Natural
Language Processing , 2714–2730.
Wang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.; and
Lim, E.-P. 2023. Plan-and-solve prompting: Improving zero-
shot chain-of-thought reasoning by large language models.
arXiv preprint arXiv:2305.04091 .
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and
Zhou, D. 2022a. Rationale-augmented ensembles in language
models. arXiv preprint arXiv:2207.00747 .
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and Zhou,
D. 2022b. Self-consistency improves chain of thought reason-
ing in language models. arXiv preprint arXiv:2203.11171 .
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;
Le, Q.; and Zhou, D. 2022a. Chain of Thought Prompting
Elicits Reasoning in Large Language Models. ArXiv preprint ,
abs/2201.11903.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi, E.;
Le, Q.; and Zhou, D. 2022b. Chain of thought prompting
elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 .
Yu, Z.; Yu, J.; Cui, Y .; Tao, D.; and Tian, Q. 2019. Deep
modular co-attention networks for visual question answering.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , 6281–6290.
Zhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li,
H.; Gao, P.; and Qiao, Y . 2023a. Llama-adapter: Efficient
fine-tuning of language models with zero-init attention. arXiv
preprint arXiv:2303.16199 .
Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Automatic
chain of thought prompting in large language models. arXiv
preprint arXiv:2210.03493 .
Zhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and
Smola, A. 2023b. Multimodal chain-of-thought reasoning in
language models. arXiv preprint arXiv:2302.00923 .
Zhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,
X.; Schuurmans, D.; Bousquet, O.; Le, Q.; and Chi, E. 2022.
Least-to-most prompting enables complex reasoning in large
language models. arXiv preprint arXiv:2205.10625 .

--- PAGE 10 ---
More Detailed Analysis
In addition to exploring the effects of different visual features, we also tested the performance of different backbones. Table 5
shows the results of UnifiedQA and FLAN-T5 on our generated datasets. When using various backbone networks, the model
trained by QA-CoT data is the worst among the three generated data types. However, it is also better than the manually annotated
data, which indicates that the manually annotated data has certain limitations, such as redundant information, single style, etc.
Furthermore, when using the Mixture dataset of QA-CoT and QA-PCoT, the fact that all four backbones achieved the best
performances illustrates the effectiveness and generality of our strategy.
Table 5: Accuracy (%) of using different backbones.
Method Size Annotated CoTT-SciQ
QA-CoT QA-PCoT Mixture
UnifiedQA Base 223M 84.91 85.99 88.56 91.75
UnifiedQA Large 738M 91.68 93.44 93.54 96.18
FLAN-T5 Base 248M 85.85 86.87 89.04 92.33
FLAN-T5 Large 783M 93.02 93.54 95.68 96.49
More Cases Analysis
To investigate the impact of different teaching signals, we conducted an evaluation of model predictions trained on various
types of data, including both generated teaching data and manually annotated data. In particular, we compared the test examples
produced by the model trained with manually annotated data (MM-CoT) and the QA-CoT teaching data (T-SciQ (QA-CoT)) in
Figure 6. We also displayed prediction cases of models trained on manually annotated data (MM-CoT) and QA-PCoT teaching
data (T-SciQ (QA-PCoT)) in Figure 7. Furthermore, we compared the prediction cases of models trained using our proposed
teaching data, QA-CoT and QA-PCoT, as depicted in Figure 8 and Figure 9. From the figures, it is evident that the model trained
with QA-PCoT data performs better in solving multi-step reasoning problems, while the one trained with QA-CoT is more
proficient at solving straightforward problems.
Data Generation Process of PCoT
In this section, we several show cases of how to generate PCoT teaching signals. To obtain appropriate planning-based chain-of-
thought rationale, we introduce a 3-step zero-shot prompting approach that enables Language Models to decompose complex
problems into simpler subproblems that are easier to solve.
The lecture template used to generate a lecture for a particular skill is formulated as follows: “Skill: [S]. QA pairs: [Xq, A]...
[Instruct ].” In this prompt, [Instruct ]is as follows: “ based on the problems above, please give a general lecture on the [S]
type of question in one sentence. ”. The lecture generated for this skill will be used in the second step of prompting, facilitating
the LLM’s generation of a plan for this skill. Examples of lecture generation are shown in Table 7.
The template used to generate a plan for a specific skill based on a lecture is formulated as follows: “Skill: [S]. Lecture: [L].
QA pairs: [Xq, A]...[Instruct ].”. In this prompt, [Instruct ]is written as follows: “ Based on the lecture above and these
problems, let’s understand these problems and devise a general and brief plan step by step to solve these problems (begin with 1,
2, 3...) ”. The plan generated using this template will be utilized in the third stage of prompting to assist the LLM in constructing
a plan-based chain-of-thought rationale for each training example. Examples are shown in Table 8.
The lecture and plan generated by the first two prompts are used to generate a plan-based chain-of-thought rationale for each
training example. The rationale generation template is formulated as follows: “Skill: [S]. Lecture: [L]. Plan: [P]. QA pairs:
[Xq, A]...[Instruct ].”. In this prompt, [Instruct ]is written as follows: “Based on the lecture, the plan and the problem,
please carry out the plan and solve the problem step by step (begin with 1, 2, 3...)”. This prompting method provides a strategy
for problem-solving by leveraging the expertise of the lecture and plan, and utilizing a structured reasoning framework to solve
each problem. Examples are shown in Table 9.

--- PAGE 11 ---
Vision
MM -CoT T-SciQ (QA-CoT)Problem
Question: Which of these states is farthest east?
Context: N/A
Options: (A) Nebraska (B) Louisiana (C) Idaho (D) Arizona
Answer: The answer is (B)
Rationale: Maps have four cardinal directions, or main directions. 
Those directions are north, south, east, and west. A compass rose is 
a set of arrows that point to the cardinal directions. A compass rose 
usually shows only the first letter of each cardinal direction. The 
north arrow points to the North Pole. On most maps, north is at the 
top of the map. To find the answer, look at the compass rose. Look at 
which way the east arrow is pointing. Idaho is farthest east..
Answer: The answer is (C)Rationale: Louisiana is the farthest east of the four states listed. 
Nebraska is located in the Midwestern United States, Idaho is 
located in the Northwestern United States, and Arizona is located in 
the Southwestern United States. Louisiana is located in the 
Southeastern United States, making it the farthest east of the four 
states.
Answer: The answer is (B)
(a) There is some redundancy in manually annotating data.
Vision
MM -CoT T-SciQ (QA-CoT)Problem
N/AQuestion: When was Benjamin Franklin born?
Context: N/A
Options: (A) 1856 (B) 1656 (C) 1776 (D) 1706
Answer: The answer is (D)
Rationale: Benjamin Franklin was the thefirst of Independence. 
1776.., 17he was have been born in 17. Declaration was56 gave still 
short. was notve been bornly years old. 1776.., 17 17 year was 1776.
Answer: The answer is (C)Rationale: Benjamin Franklin was born on January 17, 1706 in 
Boston, Massachusetts. He was one of the Founding Fathers of the 
United States and a leading author, printer, political theorist, 
politician, freemason, postmaster, scientist, inventor, civic activist, 
statesman, and diplomat. He was a major figure in the American 
Enlightenment and the history of the United States. He is best 
known for his discoveries and theories regarding electricity, as well 
as his famous experiments with a kite and a key. He also wrote the 
famous Poor Richard's Almanack and was a major contributor to the 
Declaration of Independence and the Constitution of the United 
States. Therefore, the correct answer is (D) 1706.
Answer: The answer is (D)
(b) The QA-CoT data can be used to solve problems with commonsense knowledge.
Figure 6: Examples of MM-CoT (baseline) and T-SciQ (QA-CoT) (ours) for generating rationales and predicting answers.

--- PAGE 12 ---
VisionQuestion: Which word would you find on a dictionary page with the following guide words? shot -suit
Context: N/A
Options: (A) service (B) stockade
Answer: The answer is (B)
Rationale: Guide words appear on each page of a dictionary. They 
tell you the first word and last word on the page. The other words on 
the page come between the guide words in alphabetical order. To 
put words in alphabetical order, put them in order by their first 
letters. If the first letters are the same, look at the second letters. If 
the second letters are the same, look at the third letters, and so on. 
If one word is shorter, and there are no more letters to compare, 
then the shorter word comes first in alphabetical order. For example, 
be comes before bed. Put the words in alphabetical order. Since 
serviceade is between the guide words shot -suit, it would be found 
on that page.
Answer: The answer is (A)Rationale: 1. Read the question carefully and identify the guide 
words given. The guide words given are "shot" and "suit". 2. 
Determine if the word given is between the two guide words. The 
word given is "stockade". 3. If the word is between the two guide 
words, answer "yes". If the word is not between the two guide 
words, answer "no". The word "stockade" is between the two guide 
words, so the answer is "yes". 4. If the question asks for a word 
between the two guide words, look for a word that fits the context 
of the question. The context of the question is not given, so the 
answer is "stockade".
Answer: The answer is (B)MM -CoT T-SciQ (QA-PCoT )Problem
N/A
(a) T-SciQ (QA-PCoT) can solve problems by executing the plan in multiple steps.
Vision
MM -CoT T-SciQ (QA-PCoT )Problem
Question: Based on the map, which of the following areas did the Mongol Empire control?
Context: The Mongol Empire controlled most of Asia and some parts of Eastern Europe from around 1210 
to 1375. Look at the map of the Mongol Empire. Then answer the question below.
Options: (A) South Asia (B) Southeast Asia (C) the Middle East
Answer: The answer is (C)
Rationale: Look at atthe map. Mon indicate the map show that 
areas of the area. of areas on that area the by the Mongol Empire. 
The Mon shows that the Mongol Empire controlled most of ofAsia 
Asia, Southeast Asia, and Middle East, and Africaberia . ThenThe
Mongol Empire controlled not control all Asia or any Asia.
Answer: The answer is (B)Rationale: 1. Read the lecture and understand the context of the 
question. 2. Look at the map of the Mongol Empire. 3. Identify the 
areas that the Mongol Empire controlled. 4. Compare the areas 
identified in the map with the options given in the question. 5. 
Select the option that matches the areas identified in the map. 
Answer: The Mongol Empire controlled most of Asia and some parts 
of Eastern Europe from around 1210 to 1375. Looking at the map of 
the Mongol Empire, it can be seen that the Mongol Empire 
controlled the Middle East, which is option (C). Therefore, the 
correct answer is (C) the Middle East.
Answer: The answer is (C)
(b) The QA-PCoT data can be used to solve problems with commonsense knowledge.
Figure 7: Examples of MM-CoT (baseline) and T-SciQ (QA-PCoT) (ours) for generating rationales and predicting answers.

--- PAGE 13 ---
Vision
T-SciQ (CoT) T-SciQ (QA-PCoT )Problem
N/AQuestion: Which of the following contains a vague pronoun reference?
Context: N/A
Options: (A) Abby was surprised when the article said that at least one -third of American adults get less 
than seven hours of sleep each night. (B) Abby was surprised when it said that at least one -third of 
American adults get less than seven hours of sleep each night.
Answer: The answer is (B)
Rationale: Option (A) contains a vague pronoun reference because 
the pronoun "it" is not clearly defined. It is not clear who "it" is 
referring to, this sentence. Option (A) does not contain a vague 
pronoun reference because the pro clearly defined.
Answer: The answer is (A)Rationale: Vague pronoun references occur when a pronoun is used 
to refer to a noun without providing enough context to determine 
which noun the pronoun is referring to. 1. Read the question and 
the context carefully. 2. Identify the pronoun in the sentence. The 
pronoun in the sentence is "it". 3. Determine the noun that the 
pronoun is referring to. The pronoun "it" is referring to the article. 4. 
Compare the two options and decide which one contains a vague 
pronoun reference. Option B contains a vague pronoun reference 
because it pronoun "it" is not clear enough to determine which 
noun the is referring to. Option A does not contain a vague pronoun 
reference because the pronoun “the article" is specific enough to 
determine which noun it is referring to. Therefore, the correct 
answer is Option B
Answer: The answer is (B)
(a) T-SciQ (QA-PCoT) can solve problems by executing the plan in multiple steps.
Vision
T-SciQ (CoT) T-SciQ (QA-PCoT )Problem
Question: Which property do these three objects have in common?
Context: Select the best answer.
Options: (A) shiny (B) slippery (C) opaque
Answer: The answer is (C)
Rationale: The three objects have the property of being shiny in 
common. Opaque objects are those that do not allow light to pass 
through them. Examples means that they light hits an opaque object, 
it is not able to pass through it. can reflectivereflected back of it. 
This of opaque objects include glass, metal, and, and plastic. All
Answer: The answer is (A)Rationale: Comparing properties of objects involves selecting the 
best or better answer from a given set of options that have a 
common property. 1. Read the question and context carefully. 2. 
Identify the common property among the given options. 3. The 
common property among the given options is that they are all 
physical properties. objects. 4. Select the best answer that has the 
common property. 5. The best answer that has the common 
property is (C) opaque. 6. Check your answer to make sure it is 
correct. 7. Yes, (C) opaque is the correct answer.
Answer: The answer is (C)
(b) T-SciQ (QA-PCoT) can solve problems by executing the plan in multiple steps.
Figure 8: Examples of T-SciQ (QA-CoT) (ours) and T-SciQ (QA-PCoT) (ours) for generating rationales and predicting answers.

--- PAGE 14 ---
Vision
T-SciQ (QA-PCoT ) T-SciQ (QA-CoT)Problem
Question: What is the probability that a muskmelon plant produced by this cross will be homozygous 
recessive for the fruit taste gene?
Context: In a group of muskmelon plants, some individuals have sour fruit and others have sweet fruit. In 
this group, the gene for the fruit taste trait has two alleles. The allele for sweet fruit (f) is recessive to the 
allele for sour fruit (F). This Punnett square shows a cross between two muskmelon plants.
Options: (A) 2/4(B) 1/4 (C) 0/4 (D) 3/4 (E) 4/4
Answer: The answer is (C)
Rationale: Punnett squares can be used to calculate the probability 
of different offspring types based on the alleles of the parents. 1. 
Read the context of the problem carefully and identify the alleles of 
the parents. The alleles of the parents are F (sour fruit) and f (sweet 
fruit). 2. Draw a Punnett square to represent the cross between the 
two parents. F fFFF Fff Ff ff3. Count the number of offspring with 
each genotype and calculate the probability of each genotype. FF: 0 
offspring, probability 0/4 Ff: 2 offspring, probability 2/4 ff: 2 
offspring, probability 2/4 4. Compare the probabilities of the 
different genotypes to determine the probability of the desired 
genotype. The desired genotype is homozygous recessive for the 
fruit taste gene (ff). The probability of this genotype is 2/4."
Answer: The answer is (A)Rationale: Since the allele for sweet fruit (f) is recessive, the only 
way for a muskmelon plant to be homozygous recessive for the fruit 
taste gene is if both alleles are f. However, this cross will not 
produce any muskmelon plants with two f alleles, so the probability 
that a muskmelon plant produced by this cross will be homozygous 
recessive for the fruit taste gene is 0/4, or 0%.
Answer: The answer is (C)(a) T-SciQ (QA-PCoT) may generate error rationale when visual features are complicated.
Vision
T-SciQ (QA-PCoT ) T-SciQ (QA-CoT)Problem
Question: Which is this organism's common name?
Context: This organism is a goldfish. It is also called Carassius auratus.
Options: (A) Carassius auratus (B) goldfish
Answer: The answer is (B)
Rationale: In this type of question, the goal is to identify the 
common name of an organism given its scientific name, or vice versa. 
1. Read the question and context carefully. 2. Identify the type of 
question. This is a question that requires us to identify the common 
name of an organism given its scientific name. 3. Identify the 
organism's common name or scientific name. The organism's 
scientific name is Carassius auratus. 4. Select the correct answer 
from the given options. The correct answer is (A) goldfish.
Answer: The answer is (A)Rationale: The correct answer is (B) goldfish. This scientific name of  
goldfish is Carassius auratus, but its common name is gold goldfish. 
Common scientific name is used to identify a species, while the 
common name is used to refer to organism in everyday language.
Answer: The answer is (B)
(b) T-SciQ (QA-CoT) is better than T-SciQ (QA-PCoT) on some simple problems.
Figure 9: Examples of T-SciQ (QA-CoT) (ours) and T-SciQ (QA-PCoT) (ours) for generating rationales and predicting answers.

--- PAGE 15 ---
Table 6: Data generation process of CoT prompting.
Input: Which of these states is farthest north?
Context: N/A
Options: (A) West Virginia (B) Louisiana (C) Arizona (D) Oklahoma
Correct Answer: (A) West Virginia
Please give a detailed explanation.
Solution: West Virginia isthefarthest north ofthefour states listed. West Virginia islocated intheAppalachian region oftheUnited States,
which isinthenorth easternpartofthecoun try.Louisiana, Arizona, andOklahoma arealllocated inthesouth ernandsouth westernparts ofthe
United States. West Virginia isthenorth ernmost ofthefour states, makingitthefarthest north.
Question: Identify the question that Tom and Justin’s experiment can best answer.
Context: The passage below describes an experiment. ... They compared the distances the balls traveled when launched from a 45 °angle to the
distances the balls traveled when launched from a 30° angle. Figure: a catapult for launching ping pong balls.
Options: (A) Do ping pong balls stop rolling along the ground sooner after being launched from a 30 °angle or a 45 °angle? (B) Do ping pong
balls travel farther when launched from a 30° angle compared to a 45° angle?
Correct Answer: (B) Do ping pong balls travel farther when launched from a 30° angle compared to a 45° angle?
Please give a detailed explanation.
Solution: ThequestionthatTom andJustin’s experiment canbest answer is(B)Doping pong balls travel fartherwhen launched from a
30°angle compared toa45°angle? This isbecause theexperiment isdesigned tocompare thedistances theballs traveled when launched from
a45°angle tothedistances theballs traveled when launched from a30°angle. Bymeasuringthedistances between thecatapultandtheplace
where theballhittheground, Tom andJustin candetermine which angleproduces thegreat estdistance.
Question: What does the verbal irony in this text suggest? According to Mr. Herrera’s kids, his snoring is as quiet as a jackhammer.
Context: N/A
Options: (A) The snoring is loud. (B) The snoring occurs in bursts.
Correct Answer: (A) The snoring is loud.
Please give a detailed explanation.
Solution: Verbalirony iswhen some onesays some thing thatistheoppositeofwhat they mean. Inthistext, Mr.Herrera’s kids aresayingthat
hissnoringisasquiet asajackhammer, which implies thatitisactuallyvery loud. This suggests thatthesnoringisloud, which isoption(A).
Question: Which of the following could Wendy’s test show?
Context: People can use the engineering-design process to develop solutions to problems. One step in the process is testing if a potential
solution meets the requirements of the design. ... She read that giving the bacteria more nutrients could affect the amount of insulin they
produced. So, Wendy gave extra nutrients to some of the bacteria. Then, she measured how much insulin those bacteria produced compared to
bacteria that did not get extra nutrients. Figure: studying bacteria in a laboratory.
Options: (A) whether producing more insulin would help the bacteria grow faster (B) whether different types of bacteria would need different
nutrients to produce insulin (C) whether she added enough nutrients to help the bacteria produce 20% more insulin
Correct Answer: (C) whether she added enough nutrients to help the bacteria produce 20% more insulin
Please give a detailed explanation.
Solution: Thepassage describes how Wendy used theengineering-design process totestasolutiontoaproblem. Shewanted toincrease the
amount ofinsulin thatthebacteriaproduced by20%, soshegave extranutrientstosome ofthebacteriaandthen measured how much insulin
those bacteriaproduced compared tobacteriathatdidnotgetextranutrients. This testingwould show whether sheadded enough nutrientsto
help thebacteriaproduce 20% more insulin. OptionAisincorrectbecause thepassage does notmentionanything about thebacteriagrow ing
faster. OptionBisincorrectbecause thepassage does notmentionanything about differenttypes ofbacterianeed ingdifferentnutrients.
Question: Does this passage describe the weather or the climate?
Context: Figure: Chicago. Chicago is known as The Windy City. But on average, the wind there only blows at about 10 miles per hour. Hint:
Weather is what the atmosphere is like at a certain place and time. Climate is the pattern of weather in a certain place.
Options: (A) weather (B) climate
Correct Answer: (B) climate
Please give a detailed explanation.
Solution: Thepassage describes theaveragewind speed inChicago, which isapatternofweather inthecity. This patternofweather ispartof
theclimate ofChicago, sothecorrectanswer is(B)climate. Climate istheaverageweather conditions inacertainarea over along periodof
time. Itisdetermined byfactorssuch astemperature, precipitation, humidity,andwind speed. Weather, ontheother hand, istheatmospheric
conditions atacertainplace andtime. Itisdetermined bythesame factorsasclimate, butitismore localized andcanchange quickly.

--- PAGE 16 ---
Table 7: The process of generating lectures.
Here are some problems about ”Read a map: cardinal directions”
Question: Which of these states is farthest north?
Context: N/A
Options: (A) West Virginia (B) Louisiana (C) Arizona (D) Oklahoma
Question: Which of these states is farthest north?
Context: N/A
Options: (A) Alabama (B) Idaho (C) South Carolina (D) Oklahoma
Question: Which of these states is farthest north?
Context: N/A
Options: (A) Maine (B) South Carolina (C) Kansas (D) Delaware
Question: Which of these states is farthest east?
Context: N/A
Options: (A) Montana (B) New Mexico (C) Arkansas (D) Nebraska
Question: Which of these states is farthest north?
Context: N/A
Options: (A) West Virginia (B) Mississippi (C) Nebraska (D) Oklahoma
Based on the problems above, please give a general lecture on the ”Read a map: cardinal directions” type of question in one sentence.
Lecture: Read ingamap toidentifycardinaldirections involves determiningwhich directionagiven locationisinrelationtoother locations on
themap.
Here are some problems about ”Interpret figures of speech”
Question: What does the verbal irony in this text suggest?According to Mr. Herrera’s kids, his snoring is as quiet as a jackhammer.
Context: N/A
Options: (A) The snoring is loud. (B) The snoring occurs in bursts.
Question: What does the metaphor in this text suggest? All religions, arts, and sciences are branches of the same tree. ˘2014Albert Einstein
Context: N/A
Options: (A) All religions, arts, and sciences are distant from one another. (B) All religions, arts, and sciences are related.
Question: What does the verbal irony in this text suggest? This is the best day of my life, Mr. Hogan mumbled after his car broke down on the
way to an important job interview.
Context: N/A
Options: (A) Mr. Hogan was having a bad day. (B) Mr. Hogan liked fixing cars.
Question: What does the allusion in this text suggest?Tyler seems to have the Midas touch. Without any special experience or training, he
launched a thriving business and then established a well-respected charity.
Context: N/A
Options: (A) Tyler is successful at all that he does. (B) Tyler has a hands-on approach to his work.
Question: What does the idiom in this text suggest? Speak of the devil! Whitney declared when Charlie strolled into the room.
Context: N/A
Options: (A) Whitney had just been speaking about Charlie. (B) Whitney didn’t trust Charlie.
Based on the problems above, please give a general lecture on the ”Interpret figures of speech” type of question in one sentence.
Lecture: Interpretfigures ofspeech questions require thereader toidentifythefigurativelanguage used inatextanddetermine themean ingor
connotationitconveys.
Here are some problems about ”Animal adaptations: beaks, mouths, and necks”
Question: Which animal’s mouth is also adapted for bottom feeding?
Context: Sturgeons eat invertebrates, plants, and small fish. They are bottom feeders. Bottom feeders find their food at the bottom of rivers,
lakes, and the ocean. The ’s mouth is located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. Figure:
sturgeon.
Options: (A) discus (B) armored catfish
Question: Which bird’s beak is also adapted to tear through meat?
Context: Red-tailed hawks eat fish, mammals, and other birds. The shape of the ’s beak is adapted to tear through meat. Figure: red-tailed
hawk.
Options: (A) sand martin (B) Cape vulture
Question: Which animal’s mouth is also adapted for bottom feeding?
Context: Armored catfish eat plants and small
invertebrates. They are bottom feeders. Bottom feeders find their food at the bottom of rivers, lakes, and the ocean. The catfish’s mouth is
located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. Figure: armored catfish.
Options: (A) clown triggerfish (B) sturgeon
Question: Which bird’s beak is also adapted to get nectar out of long flowers?
Context: Green violetears live in the forests of Central and South America. The shape of the ’s beak is adapted to get nectar out of long flowers.
Figure: green violetear.
Options: (A) ground hornbill (B) violet sabrewing
Question: ...
Based on the problems above, please give a general lecture on the ”Animal adaptations: beaks, mouths, and necks” type of question in one
sentence.
Lecture: Animaladap tations such asbeaks, mouths, andnecks arespecialized features thatenable animals tofeed ondifferenttypes offood,
such asmeat, insects, nuts, andplant matter.

--- PAGE 17 ---
Table 8: The process of generating plans.
Here are some problems about ”Read a map: cardinal directions”
The lecture about ”Read a map: cardinal directions” is ”Reading a map to identify cardinal directions involves determining which direction a
given location is in relation to other locations on the map.”
Question: Which of these states is farthest north?
Context: N/A
Options: (A) West Virginia (B) Louisiana (C) Arizona (D) Oklahoma
Question: Which of these states is farthest north?
Context: N/A
Options: (A) Alabama (B) Idaho (C) South Carolina (D) Oklahoma
Question: Which of these states is farthest north?
Context: N/A
Options: (A) Maine (B) South Carolina (C) Kansas (D) Delaware
Question: ...
Based on the lecture above and these problems, let’s understand these problems and devise a general and brief plan step by step to solve these
problems (begin with 1, 2, 3...).
Plan: 1.Read thequestioncarefully andidentifythecontext. 2.Identifythecardinaldirections (north, south, east, andwest) inrelationtothe
given locations onthemap. 3.Com pare thelocations anddetermine which oneisfarthest inthegiven direction. 4.Selectthecorrectanswer
from thegiven options.
Here are some problems about ”Interpret figures of speech”
The lecture about ”Interpret figures of speech” is ”Interpret figures of speech questions require the reader to identify the figurative language
used in a text and determine the meaning or connotation it conveys.”
Question: What does the verbal irony in this text suggest?According to Mr. Herrera’s kids, his snoring is as quiet as a jackhammer.
Context: N/A
Options: (A) The snoring is loud. (B) The snoring occurs in bursts.
Question: What does the metaphor in this text suggest? All religions, arts, and sciences are branches of the same tree. ˘2014Albert Einstein
Context: N/A
Options: (A) All religions, arts, and sciences are distant from one another. (B) All religions, arts, and sciences are related.
Question: What does the verbal irony in this text suggest? This is the best day of my life, Mr. Hogan mumbled after his car broke down on the
way to an important job interview.
Context: N/A
Options: (A) Mr. Hogan was having a bad day. (B) Mr. Hogan liked fixing cars.
Question: ...
Based on the lecture above and these problems, let’s understand these problems and devise a general and brief plan step by step to solve these
problems (begin with 1, 2, 3...).
Plan: 1.Read thetextcarefully andidentifythefigureofspeech used. 2.Analyzethecontextofthetexttodetermine themean ingorconnotation
ofthefigureofspeech. 3.Choose theoptionthatbestmatches themean ingorconnotationofthefigureofspeech.
Here are some problems about ”Animal adaptations: beaks, mouths, and necks”
The lecture about ”Animal adaptations: beaks, mouths, and necks” is ”Animal adaptations such as beaks, mouths, and necks are specialized
features that enable animals to feed on different types of food, such as meat, insects, nuts, and plant matter.”
Question: Which animal’s mouth is also adapted for bottom feeding?
Context: Sturgeons eat invertebrates, plants, and small fish. They are bottom feeders. Bottom feeders find their food at the bottom of rivers,
lakes, and the ocean. The ’s mouth is located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. Figure:
sturgeon.
Options: (A) discus (B) armored catfish
Question: Which bird’s beak is also adapted to tear through meat?
Context: Red-tailed hawks eat fish, mammals, and other birds. The shape of the ’s beak is adapted to tear through meat. Figure: red-tailed
hawk.
Options: (A) sand martin (B) Cape vulture
Question: Which animal’s mouth is also adapted for bottom feeding?
Context: Armored catfish eat plants and small
invertebrates. They are bottom feeders. Bottom feeders find their food at the bottom of rivers, lakes, and the ocean. The catfish’s mouth is
located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. Figure: armored catfish.
Options: (A) clown triggerfish (B) sturgeon
Question: ...
Based on the lecture above and these problems, let’s understand these problems and devise a general and brief plan step by step to solve these
problems (begin with 1, 2, 3...).
Plan: 1.Read thelectureandunderstand thecontextofthequestions. 2.Read thequestionandidentifythetype ofadap tationbeingasked
about. 3.Look atthefigureprovided andidentifytheanimalbeingdiscussed. 4.Research theanimaltofindoutwhat type ofadap tationithas.
5.Com pare theoptions provided andselecttheonethatbestmatches theadap tation.

--- PAGE 18 ---
Table 9: Data generation of PCoT prompting.
Here are is a problem about ”Read a map: cardinal directions”
The lecture about ”Read a map: cardinal directions” is ”Reading a map to identify cardinal directions involves determining which direction a
given location is in relation to other locations on the map.”
The plan to solve ”Read a map: cardinal directions” problem is ”1. Read the question carefully and identify the context. 2. Identify the cardinal
directions (north, south, east, and west) in relation to the given locations on the map. 3. Compare the locations and determine which one is
farthest in the given direction. 4. Select the correct answer from the given options.”
Question: Which of these states is farthest north?
Context: N/A
Options: (A) West Virginia (B) Louisiana (C) Arizona (D) Oklahoma
Correct Answer: (A) West Virginia
Based on the lecture, the plan and the problem, please carry out the plan and solve the problem step by step (begin with 1, 2, 3...).
Solution: 1.Read thequestioncarefully andidentifythecontext. Thecontextofthequestionistodetermine which ofthegiven states isfarthest
north. 2.Identifythecardinaldirections (north, south, east, andwest) inrelationtothegiven locations onthemap. North isthedirectionthatis
farthest away from theequa torandclosesttotheNorth Pole. South isthedirectionthatisfarthest away from theNorth Pole andclosesttothe
equator.East isthedirectionthatisfarthest away from thePrime Merid ianandclosesttotheInternational Date Line. West isthedirectionthat
isfarthest away from theInternational Date Line andclosesttothePrime Merid ian.3.Com pare thelocations anddetermine which oneis
farthest inthegiven direction. West Virginia isfarthest north, followed byLouisiana, Arizona, andOklahoma. 4.Selectthecorrectanswer
from thegiven options. Thecorrectanswer is(A)West Virginia.
Here is a problem about ”Interpret figures of speech”
The lecture about ”Interpret figures of speech” is ”Interpret figures of speech questions require the reader to identify the figurative language
used in a text and determine the meaning or connotation it conveys.”
The plan to solve ”Interpret figures of speech” problem is ”1. Read the text carefully and identify the figure of speech used. 2. Analyze the
context of the text to determine the meaning or connotation of the figure of speech. 3. Choose the option that best matches the meaning or
connotation of the figure of speech.”
Question: What does the verbal irony in this text suggest?According to Mr. Herrera’s kids, his snoring is as quiet as a jackhammer.
Context: N/A
Options: (A) The snoring is loud. (B) The snoring occurs in bursts.
Correct Answer: (A) The snoring is loud.
Based on the lecture, the plan and the problem, please carry out the plan and solve the problem step by step (begin with 1, 2, 3...).
Solution: 1.Read thetextcarefully andidentifythefigureofspeech used. Thefigureofspeech used inthetextisverbalirony. 2.Analyze the
contextofthetexttodetermine themean ingorconnotationofthefigureofspeech. Thecontextofthetextsuggests thattheverbalirony is
used toconveytheidea thatMr.Herrera’s snoringisactuallyvery loud, despite what hiskids say.3.Choose theoptionthatbestmatches the
mean ingorconnotationofthefigureofspeech. Theoptionthatbestmatches themean ingorconnotationofthefigureofspeech is(A)The
snoringisloud.
Here is a problem about ”Animal adaptations: beaks, mouths, and necks”
The lecture about ”Animal adaptations: beaks, mouths, and necks” is ”Animal adaptations such as beaks, mouths, and necks are specialized
features that enable animals to feed on different types of food, such as meat, insects, nuts, and plant matter.”
The plan to solve ”Animal adaptations: beaks, mouths, and necks” problem is ”1. Read the lecture and understand the context of the questions.
2. Read the question and identify the type of adaptation being asked about. 3. Look at the figure provided and identify the animal being
discussed. 4. Research the animal to find out what type of adaptation it has. 5. Compare the options provided and select the one that best
matches the adaptation.”
Question: Which animal’s mouth is also adapted for bottom feeding?
Context: Sturgeons eat invertebrates, plants, and small fish. They are bottom feeders. Bottom feeders find their food at the bottom of rivers,
lakes, and the ocean. The ’s mouth is located on the underside of its head and points downward. Its mouth is adapted for bottom feeding. Figure:
sturgeon.
Options: (A) discus (B) armored catfish
Correct Answer: (B) armored catfish
Based on the lecture, the plan and the problem, please carry out the plan and solve the problem step by step (begin with 1, 2, 3...).
Solution: 1.Read thelectureandunderstand thecontextofthequestions. Thelecturestates thatanimaladaptations such asbeaks, mouths, and
necks arespecialized features thatenable animals tofeed ondifferenttypes offood. 2.Read thequestionandidentifythetype ofadap tation
beingasked about. Thequestionisaskingabout anadaptationrelated tobottomfeeding.3.Look atthefigureprovided andidentifytheanimal
beingdiscussed. Thefigureprovided isasturgeon. 4.Research theanimaltofindoutwhat type ofadap tationithas.Research ingthesturgeon
reveals thatitsmouth islocated ontheundersideofitshead andpoints down ward. This adaptationisspecif ically adapted forbottomfeeding.5.
Com pare theoptions provided andselecttheonethatbestmatches theadaptation. Theoptions provided are(A)discusand(B)armored catfish.
Thearmored catfishhasamouth thatisadapted forbottomfeeding,makingitthecorrectanswer. There fore, thecorrectanswer is(B)armored
catfish.
