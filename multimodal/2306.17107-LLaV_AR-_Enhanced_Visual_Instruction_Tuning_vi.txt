# LLaVAR: Điều chỉnh Hướng dẫn Thị giác Nâng cao cho Hiểu biết Hình ảnh Giàu văn bản

Yanzhe Zhang1∗, Ruiyi Zhang2, Jiuxiang Gu2, Yufan Zhou2, Nedim Lipka2,
Diyi Yang3, Tong Sun2
1Georgia Tech, 2Adobe Research, 3Stanford University

## Tóm tắt

Điều chỉnh hướng dẫn nâng cao khả năng của Mô hình Ngôn ngữ Lớn (LLMs) để tương tác với con người. Hơn nữa, các tập dữ liệu theo hướng dẫn gần đây bao gồm hình ảnh như đầu vào thị giác, thu thập phản hồi cho các hướng dẫn dựa trên hình ảnh. Tuy nhiên, các mô hình được điều chỉnh hướng dẫn thị giác hiện tại không thể hiểu rõ chi tiết văn bản trong hình ảnh. Công trình này nâng cao quy trình điều chỉnh hướng dẫn thị giác hiện tại với hình ảnh giàu văn bản (ví dụ: áp phích phim, bìa sách, v.v.). Cụ thể, chúng tôi đầu tiên sử dụng các công cụ OCR có sẵn công khai để thu thập kết quả trên 422K hình ảnh giàu văn bản từ tập dữ liệu LAION. Hơn nữa, chúng tôi nhắc GPT-4 chỉ văn bản với văn bản được nhận dạng và chú thích hình ảnh để tạo ra 16K cuộc hội thoại, mỗi cuộc hội thoại chứa các cặp câu hỏi-trả lời cho hình ảnh giàu văn bản. Bằng cách kết hợp dữ liệu đã thu thập của chúng tôi với dữ liệu theo hướng dẫn đa phương thức trước đó, mô hình của chúng tôi, LLaVAR, cải thiện đáng kể khả năng của mô hình LLaVA trên các tập dữ liệu VQA dựa trên văn bản (cải thiện độ chính xác lên đến 20%). Đánh giá theo hướng dẫn dựa trên GPT-4 cũng chứng minh sự cải thiện của mô hình chúng tôi trên cả hình ảnh tự nhiên và hình ảnh giàu văn bản. Thông qua phân tích định tính, LLaVAR cho thấy kỹ năng tương tác đầy hứa hẹn (ví dụ: suy luận, viết và mở rộng) với con người dựa trên nội dung trực tuyến thực tế mới nhất kết hợp văn bản và hình ảnh. Chúng tôi công khai mã/dữ liệu/mô hình của mình.

## 1. Giới thiệu

Điều chỉnh hướng dẫn [1,2] cải thiện khả năng tổng quát hóa cho các tác vụ chưa được nhìn thấy bằng cách hình thành các tác vụ khác nhau thành hướng dẫn. Khả năng trả lời câu hỏi mở như vậy thúc đẩy sự bùng nổ chatbot gần đây kể từ ChatGPT. Gần đây, các mô hình được điều chỉnh hướng dẫn thị giác [3-5] tiếp tục tăng cường các tác nhân hội thoại với các bộ mã hóa thị giác như CLIP-ViT [6,7], cho phép tương tác người-tác nhân dựa trên hình ảnh. Tuy nhiên, có thể do sự thống trị của hình ảnh tự nhiên trong dữ liệu huấn luyện (ví dụ: Conceptual Captions [8] và COCO [9]), chúng gặp khó khăn trong việc hiểu văn bản trong hình ảnh [10]. Tuy nhiên, hiểu biết văn bản là một phần không thể thiếu của nhận thức thị giác trong cuộc sống hàng ngày.

May mắn thay, các công cụ như Nhận dạng Ký tự Quang học (OCR, 11) cho phép chúng ta nhận dạng văn bản trong hình ảnh. Một cách đơn giản để sử dụng điều này là thêm văn bản được nhận dạng vào đầu vào của các mô hình được điều chỉnh hướng dẫn thị giác [12]. Tuy nhiên, cách tiếp cận này làm tăng đáng kể tính toán (độ dài ngữ cảnh dài hơn), và có thể không tận dụng đầy đủ khả năng mã hóa của các bộ mã hóa thị giác. Để đạt được mục tiêu này, chúng tôi đề xuất nâng cao mô hình được điều chỉnh hướng dẫn thị giác đầu cuối bằng cách thu thập dữ liệu theo hướng dẫn yêu cầu hiểu văn bản trong hình ảnh.

Cụ thể, chúng tôi đầu tiên thu thập 422K dữ liệu theo hướng dẫn nhiễu bằng cách sử dụng hình ảnh giàu văn bản bằng cách kết hợp các hướng dẫn được viết thủ công (ví dụ: "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp.") và kết quả OCR. Dữ liệu căn chỉnh nhiễu quy mô lớn như vậy cải thiện hiệu quả việc căn chỉnh đặc trưng giữa các đặc trưng thị giác và bộ giải mã ngôn ngữ. Hơn nữa, chúng tôi nhắc GPT-4 chỉ văn bản [13] với kết quả OCR và chú thích hình ảnh để tạo ra 16K cuộc hội thoại, trong đó mỗi cuộc hội thoại có thể là nhiều lượt cặp câu hỏi & trả lời, như các ví dụ theo hướng dẫn chất lượng cao. Quá trình này yêu cầu GPT-4 loại bỏ nhiễu kết quả OCR và phát triển các câu hỏi cụ thể để tạo ra các hướng dẫn phức tạp dựa trên đầu vào (Hình 1).

Để đánh giá hiệu quả của dữ liệu đã thu thập, chúng tôi sử dụng các ví dụ nhiễu và chất lượng cao để tăng cường các giai đoạn tiền huấn luyện và tinh chỉnh của LLaVA [3] tương ứng. Chúng tôi đặt tên mô hình của mình là LLaVAR, biểu thị LLaVA (Trợ lý Ngôn ngữ và Tầm nhìn Lớn) có thể Đọc. So với LLaVA gốc, chúng tôi cũng tiến hành thí nghiệm mở rộng độ phân giải đầu vào từ 224² đến 336² để mã hóa tốt hơn các chi tiết văn bản nhỏ. Thực nghiệm, chúng tôi báo cáo kết quả trên bốn tập dữ liệu VQA dựa trên văn bản theo giao thức đánh giá từ Liu et al. [10]. Hơn nữa, chúng tôi áp dụng đánh giá theo hướng dẫn dựa trên GPT-4 trên 30 hình ảnh tự nhiên từ COCO [9,3] và 50 hình ảnh giàu văn bản từ LAION [14]. Chúng tôi cũng cung cấp phân tích định tính (ví dụ: trên áp phích, ảnh chụp màn hình trang web và tweet) để kiểm tra các kỹ năng theo hướng dẫn phức tạp hơn.

Tóm lại, đóng góp của chúng tôi như sau:
• Chúng tôi thu thập 422K dữ liệu theo hướng dẫn nhiễu và 16K dữ liệu theo hướng dẫn chất lượng cao. Cả hai đều được chứng minh là hiệu quả trong việc tăng cường điều chỉnh hướng dẫn thị giác.
• Mô hình của chúng tôi, LLaVAR, nâng cao đáng kể khả năng hiểu văn bản trong hình ảnh trong khi cải thiện nhẹ hiệu suất của mô hình trên hình ảnh tự nhiên.
• Khả năng nâng cao cho phép mô hình của chúng tôi cung cấp tương tác đầu cuối dựa trên các hình thức khác nhau của nội dung trực tuyến kết hợp văn bản và hình ảnh.
• Chúng tôi mở nguồn dữ liệu huấn luyện và đánh giá cùng với các checkpoint mô hình.

## 2. Công trình liên quan

**Điều chỉnh Hướng dẫn** Tuân theo hướng dẫn ngôn ngữ tự nhiên là khả năng chủ chốt để một tác nhân tương tác với người dùng thực tế. Điều chỉnh hướng dẫn bắt đầu từ việc thu thập phản hồi được con người ưa thích cho các hướng dẫn do con người viết [1] hoặc xây dựng huấn luyện đa tác vụ theo cách theo hướng dẫn đa tác vụ [2,15]. Tuy nhiên, các mô hình được điều chỉnh hướng dẫn lớn, có khả năng thường là mã nguồn đóng và chỉ phục vụ như API thương mại. Gần đây, Alpaca [16,17], Vicuna [18], và Baize [19] bắt đầu xu hướng tạo ra dữ liệu theo hướng dẫn chất lượng cao dựa trên LLMs như GPT-3.5 / ChatGPT / GPT-4 và tinh chỉnh mô hình LLaMA mã nguồn mở [20]. Tuy nhiên, đánh giá khả năng tuân theo hướng dẫn vẫn là một thách thức. Trong khi GPT-4 đã chứng minh khả năng đánh giá vượt trội [21], vẫn có một số lo ngại, chẳng hạn như thiên vị đối với độ dài phản hồi [19] và thiếu tính mạnh mẽ đối với thứ tự của các ví dụ [22]. Theo Chiang et al. [18], Liu et al. [3], Dubois et al. [23], chúng tôi sử dụng đánh giá theo hướng dẫn dựa trên GPT-4 trong công trình này.

**Điều chỉnh Hướng dẫn Đa phương thức** Gần đây, điều chỉnh hướng dẫn đã được mở rộng đến thiết lập đa phương thức, bao gồm hình ảnh, video [24,25], và âm thanh [26,27]. Đối với điều chỉnh hướng dẫn dựa trên hình ảnh, MiniGPT-4 [28] sử dụng ChatGPT để tuyển chọn và cải thiện chú thích chi tiết cho dữ liệu theo hướng dẫn chất lượng cao. LLaVA [3] tạo ra dữ liệu theo hướng dẫn đa phương thức bằng cách nhắc GPT-4 chỉ văn bản với chú thích và hộp giới hạn của đối tượng. LLaMA-Adapter [29,12] sử dụng dữ liệu COCO để căn chỉnh đặc trưng văn bản-hình ảnh và chỉ sử dụng dữ liệu văn bản để điều chỉnh hướng dẫn. mPLUG-owl [30] kết hợp hơn 1000M cặp hình ảnh-văn bản để tiền huấn luyện và hỗn hợp 400K dữ liệu theo hướng dẫn chỉ văn bản/đa phương thức để tinh chỉnh. Tuy nhiên, theo Liu et al. [10], hầu hết các mô hình này gặp khó khăn trong việc hoàn thành các tác vụ yêu cầu khả năng OCR. InstructBLIP [31] chuyển đổi 13 tác vụ ngôn ngữ-tầm nhìn (bao gồm OCR-VQA [32]) thành định dạng theo hướng dẫn để điều chỉnh hướng dẫn. Cream [33] áp dụng học đa tác vụ bao gồm dự đoán văn bản bị che trong hình ảnh. Một khảo sát toàn diện hơn có thể được tìm thấy trong Li et al. [34]. Trong công trình này, chúng tôi chọn LLaVA làm baseline, đây là mô hình hiệu quả nhất về dữ liệu và mạnh mẽ nhất, và chứng minh hiệu quả của quy trình đề xuất của chúng tôi.

## 3. Thu thập Dữ liệu

Bắt đầu từ tập dữ liệu LAION-5B [14], mục tiêu của chúng tôi chỉ là giữ lại những hình ảnh giàu văn bản. Xem xét rằng tài liệu thường chứa nhiều văn bản, chúng tôi đầu tiên thu được một tập dữ liệu phân loại nhị phân bằng cách kết hợp hình ảnh tự nhiên và dữ liệu tài liệu. Sau đó, chúng tôi huấn luyện một bộ phân loại hình ảnh sử dụng backbone DiT [35]-base, được tinh chỉnh trên tập dữ liệu RVL-CDIP [36]. Hy vọng rằng, bộ phân loại như vậy có thể dự đoán liệu một hình ảnh có chứa văn bản hay không. Chúng tôi đầu tiên xây dựng một tập con bằng cách chọn những hình ảnh có xác suất dự đoán lớn hơn 0.8 đồng thời thỏa mãn p(watermark) < 0.8 và p(unsafe) < 0.5. Tập con thu được có nhiễu do hạn chế của bộ phân loại. Để làm sạch thêm dữ liệu và kết hợp đánh giá của con người, chúng tôi lấy mẫu ngẫu nhiên 50K hình ảnh và gom chúng thành 100 cụm dựa trên các đặc trưng thị giác CLIP-ViT-B/32. Sau khi kiểm tra kết quả phân cụm, chúng tôi cẩn thận chọn 14 cụm (xem Hình 10 trong Phụ lục để biết ví dụ) chứa các hình ảnh giàu văn bản đa dạng từ áp phích, bìa, quảng cáo, infographic, tài liệu giáo dục, và logo. Mô hình cụm sau đó được sử dụng làm bộ lọc để thu thập hình ảnh để xây dựng các ví dụ theo hướng dẫn của chúng tôi. Làm tham chiếu, chúng tôi cung cấp phân loại dựa trên CLIP [7] (xem Phụ lục A để biết chi tiết) để minh họa phân phối hình ảnh cho cả hai loại dữ liệu chúng tôi thu thập trong Hình 2. Chúng tôi tóm tắt dữ liệu đã thu thập của chúng tôi và dữ liệu của LLaVA trong Bảng 1.

**Dữ liệu Theo hướng dẫn Nhiễu** Sử dụng mô hình phân cụm làm bộ lọc, chúng tôi thu thập 422K hình ảnh đã loại bỏ trùng lặp thuộc về 14 cụm được ưa thích. Để cân bằng các ví dụ từ các danh mục khác nhau, chúng tôi giữ tối đa 52K ví dụ cho một cụm. Chúng tôi chạy tất cả hình ảnh qua PaddleOCR. Lưu ý rằng chạy OCR ở độ phân giải gốc (ví dụ: 1024²) có thể nhận dạng các phông chữ nhỏ không thể nhìn thấy bởi các bộ mã hóa thị giác như CLIP ViT (6,7, độ phân giải lên đến 336²). Để đảm bảo nhận dạng các phông chữ có thể nhìn thấy trong khi duy trì độ chính xác OCR, chúng tôi thực hiện OCR trên hình ảnh sau khi giảm mẫu (cạnh ngắn được thay đổi kích thước xuống 384 pixel nếu dài hơn) để trích xuất văn bản. Sau đó, dựa trên mối quan hệ hình học giữa các từ được nhận dạng, chúng tôi gộp chúng thành các đoạn văn trước khi nối chúng lại. Vì một mô hình theo hướng dẫn mạnh mẽ nên phản ứng tương tự với các hướng dẫn có nghĩa tương tự, chúng tôi viết lại "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp." thành mười hướng dẫn riêng biệt (Bảng 6 trong Phụ lục). Sau đó chúng tôi tạo một cuộc hội thoại một lượt cho một hình ảnh nhất định bằng cách (i) lấy mẫu ngẫu nhiên một hướng dẫn đầu vào và (ii) sử dụng văn bản được nhận dạng làm phản hồi đầu ra mong muốn. Dữ liệu theo hướng dẫn như vậy có nhiễu vì hiệu suất tương đối hạn chế của các công cụ OCR trên các phông chữ đa dạng và nền màu sắc.

**Dữ liệu Theo hướng dẫn Dựa trên GPT-4** So với dữ liệu theo hướng dẫn chất lượng cao, chủ yếu có hai vấn đề đối với dữ liệu nhiễu được thu thập ở trên. (i) Phản hồi nên chứa các câu có tổ chức thay vì kết quả OCR thô với từ bị thiếu và lỗi ngữ pháp. (ii) Hướng dẫn nên đa dạng, phù hợp và cụ thể với hình ảnh nhất định thay vì đơn điệu yêu cầu tất cả văn bản hiển thị. Để giải quyết những vấn đề này, chúng tôi theo Liu et al. [3] để tạo ra dữ liệu theo hướng dẫn bằng cách nhắc GPT-4 chỉ văn bản [13] với kết quả OCR và chú thích.

Thật khó khăn để nhắc GPT-4 với các kết quả OCR phân mảnh trong vài từ để tạo ra các hướng dẫn không tầm thường. Để đạt được mục tiêu này, chúng tôi cẩn thận chọn 4 trong số 14 cụm đã đề cập trước đó (cụm thứ 3, 4, 6 và 9 trong Hình 10) để thu thập hình ảnh với đủ câu hiển thị và mạch lạc. Như được hiển thị trong Hình 2, việc lọc như vậy làm tăng đáng kể tỷ lệ phần trăm của bìa sách và hình ảnh trích dẫn. Chúng tôi chọn ngẫu nhiên 4K ví dụ từ mỗi cụm (không trùng lặp với hình ảnh được sử dụng cho dữ liệu theo hướng dẫn nhiễu), tạo ra tổng cộng 16K hình ảnh. Theo công trình trước [16,17,3], chúng tôi cung cấp trực quan hóa các cặp động từ-danh từ cho các hướng dẫn được tạo bởi GPT-4 trong Phụ lục Hình 11. Đối với những hướng dẫn không có cặp động từ-danh từ, chúng tôi chứng minh tần suất của các đối tượng được hỏi trong Phụ lục Hình 12.

Hơn nữa, dựa trên thông điệp hệ thống và hai ví dụ few-shot trong ngữ cảnh (được hiển thị trong Phụ lục B), chúng tôi yêu cầu GPT-4 tạo ra dữ liệu hội thoại dựa trên kết quả OCR và chú thích hình ảnh (Hình 1). Các câu hỏi được tạo ra được sử dụng làm hướng dẫn đầu vào, và câu trả lời được sử dụng làm phản hồi đầu ra. Cụ thể, đối với một hình ảnh nhất định, chúng tôi đầu tiên cung cấp hai kết quả OCR từ EasyOCR và PaddleOCR, có thể bổ sung cho nhau. Để minh họa các yếu tố thị giác khác ngoài văn bản trong hình ảnh, chúng tôi cũng cung cấp kết quả của việc tạo chú thích hình ảnh BLIP-2 [37]. Để ngăn chú thích tập trung vào văn bản, chúng tôi sử dụng hộp giới hạn OCR để che văn bản và sau đó sử dụng inpainting [38] để điền lại vùng che trước khi sử dụng tạo chú thích. Lưu ý rằng các mô hình tạo chú thích có thể gặp phải ảo giác [39]. Chúng tôi đề cập đến sự không đáng tin cậy này trong thông điệp hệ thống của chúng tôi và yêu cầu GPT-4 chỉ tạo ra các câu hỏi có câu trả lời chắc chắn. Chúng tôi để việc tạo ra các chú thích chi tiết hơn [40, 41] cho công việc tương lai.

## 4. Kiến trúc Mô hình và Huấn luyện

**Kiến trúc** Trong hầu hết nghiên cứu của chúng tôi, chúng tôi sử dụng cùng kiến trúc mô hình như LLaVA. Đối với bộ mã hóa thị giác V, chúng tôi sử dụng CLIP-ViT-L/14 cho độ phân giải 224² và CLIP-ViT-L/14-336 cho độ phân giải 336². Các đặc trưng lưới trước lớp transformer cuối cùng sau đó được chuyển đổi vào không gian nhúng từ của bộ giải mã ngôn ngữ thông qua ma trận chiếu có thể huấn luyện W. Chúng tôi sử dụng Vicuna-13B [18], một mô hình ngôn ngữ được điều chỉnh hướng dẫn dựa trên LLaMA [20], làm bộ giải mã ngôn ngữ D ngoại trừ nghiên cứu ablation trong Bảng 4.

Trong Phần 5.1 và Phụ lục H, chúng tôi mở rộng kiến trúc hiện tại bằng cách thêm một bộ mã hóa thị giác độ phân giải cao (high-res) bổ sung. Bộ mã hóa high-res như vậy xuất ra hàng nghìn đặc trưng patch, có nghĩa là các đặc trưng được chuyển đổi và token hướng dẫn không thể vừa với độ dài ngữ cảnh của bộ giải mã ngôn ngữ. Để đạt được mục tiêu này, chúng tôi đề xuất thêm các mô-đun cross-attention vào bộ giải mã, chú ý đến các cặp key-value được chuyển đổi từ các đặc trưng patch high-res.

**Huấn luyện** Chúng tôi tuân theo thiết kế huấn luyện hai giai đoạn của LLaVA (Hình 3). Mục tiêu huấn luyện của cả hai giai đoạn đều giống nhau: tạo ra phản hồi đầu ra (<res>) cho các hướng dẫn đầu vào (<ins>). Các token hình ảnh được chuyển đổi (<img>) được thêm trước hoặc sau hướng dẫn đầu vào đầu tiên. (i) Trong giai đoạn tiền huấn luyện đầu tiên, chỉ ma trận chiếu W được huấn luyện để căn chỉnh đặc trưng. Vì bộ giải mã D bị đóng băng, huấn luyện chịu đựng được dữ liệu nhiễu. Trong giai đoạn tiền huấn luyện, chúng tôi kết hợp 595K dữ liệu tiền huấn luyện từ LLaVA với 422K dữ liệu theo hướng dẫn nhiễu của chúng tôi. (ii) Cả ma trận chiếu W và bộ giải mã ngôn ngữ D đều được huấn luyện trong giai đoạn tinh chỉnh, nơi chúng tôi gộp 16K dữ liệu theo hướng dẫn của chúng tôi vào 158K dữ liệu theo hướng dẫn từ LLaVA làm tập huấn luyện. Lưu ý rằng bộ mã hóa thị giác bị đóng băng trong suốt thời gian huấn luyện, điều này có thể hạn chế hiệu suất nhận dạng văn bản, vì CLIP được huấn luyện để căn chỉnh văn bản-hình ảnh tổng quát. Các lựa chọn tốt hơn của bộ mã hóa thị giác [42] hoặc tinh chỉnh CLIP-ViT [30] có thể mang lại lợi ích thêm cho khả năng hiểu thị giác, điều mà chúng tôi để dành cho công việc tương lai.

## 5. Thí nghiệm

Chúng tôi sử dụng cùng siêu tham số huấn luyện như LLaVA, ngoại trừ (i) Chúng tôi đặt độ dài chuỗi tối đa là 1024 trong quá trình tiền huấn luyện. (ii) Chúng tôi đầu tiên pad bất kỳ hình ảnh nào thành hình vuông trước khi thay đổi kích thước nó đến kích thước đầu vào mong muốn, ngăn một số nội dung hình ảnh bị cắt trong quá trình tiền xử lý. Đối với cả hai độ phân giải (224², 336²), chúng tôi tái tạo LLaVA gốc để so sánh công bằng. Mô hình GPT-4 được sử dụng trong công trình này đề cập đến phiên bản gpt-4-0314, trong khi chi phí để thu thập dữ liệu tinh chỉnh là khoảng $300. Nhiệt độ được sử dụng để lấy mẫu GPT-4 được đặt là 1.0 để tạo dữ liệu huấn luyện, 0.7 để tạo dữ liệu đánh giá, và 0.2 để đánh giá dựa trên GPT-4. Tất cả thí nghiệm được chạy trên GPU NVIDIA A100 80GB. Trong quá trình đánh giá, nhiệt độ được sử dụng để lấy mẫu từ mô hình của chúng tôi được đặt ở 0.9 cho VQA dựa trên văn bản, 0.7 cho đánh giá theo hướng dẫn dựa trên GPT-4, và 0.2 cho các minh họa định tính khác.

### 5.1 Phân tích Định lượng

**VQA dựa trên Văn bản** Theo giao thức đánh giá trong Liu et al. [10], chúng tôi đánh giá hiệu suất của LLaVAR trên bốn tập dữ liệu VQA dựa trên văn bản: ST-VQA [45], OCR-VQA [32], TextVQA [46], và DocVQA [47], đại diện cho các lĩnh vực khác nhau (xem Phụ lục C để biết thêm chi tiết và Phụ lục E để biết thêm tập dữ liệu). Chúng tôi trình bày kết quả của các mô hình baseline và mô hình của chúng tôi trong Bảng 2. Lưu ý rằng InstructBLIP bao gồm OCR-VQA trong tập huấn luyện của nó, khiến nó không thể so sánh được với cài đặt của chúng tôi. Trong cả hai cài đặt độ phân giải và tất cả bốn tập dữ liệu, LLaVAR cải thiện đáng kể baseline LLaVA, chứng minh rằng dữ liệu đã thu thập của chúng tôi có thể mang lại sự cải thiện mạnh mẽ. Hơn nữa, sự cải thiện đáng kể hơn ở độ phân giải 336² so với 224², cho thấy dữ liệu đã thu thập có thể mang lại sự cải thiện lớn hơn ở độ phân giải thậm chí cao hơn. Mô hình tốt nhất của chúng tôi, LLaVAR dựa trên 336², hoạt động tốt nhất trong 3 trên 4 tập dữ liệu được đánh giá. Lưu ý rằng đây không phải là so sánh công bằng. Một số yếu tố chính bao gồm các bộ giải mã ngôn ngữ khác nhau, độ phân giải, và độ lớn của dữ liệu huấn luyện văn bản-hình ảnh. Chúng tôi cung cấp thêm thảo luận về so sánh với mPLUG-Owl và kết quả của việc tinh chỉnh mPLUG-Owl bằng dữ liệu của chúng tôi trong Phụ lục F.

**Nghiên cứu Ablation về dữ liệu tiền huấn luyện/tinh chỉnh** Chúng tôi báo cáo kết quả trong Bảng 3 và Hình 4. (i) Dựa trên các biến thể (2) và (3), chúng tôi thấy rằng dữ liệu đã thu thập có thể có lợi cho giai đoạn tiền huấn luyện (R_pretraining) và giai đoạn tinh chỉnh (R_finetuning) một cách riêng lẻ trong khi bổ sung cho nhau trong hầu hết các trường hợp. Quan trọng hơn, việc nâng cao riêng giai đoạn tiền huấn luyện đạt được hiệu suất tổng thể tốt thứ hai, cho thấy tiềm năng tăng cường hiểu biết chi tiết văn bản mà không phụ thuộc vào dữ liệu chất lượng cao được tạo bởi GPT-4. (ii) Sử dụng hình ảnh tiền huấn luyện, chúng tôi thu được C_pretraining bằng cách thay thế các hướng dẫn tiền huấn luyện bằng câu hỏi & chú thích, cùng mẫu như LLaVA. Vì biến thể (4) không tốt bằng (2), chúng tôi có thể kết luận rằng OCR có lợi hơn chú thích. (iii) Chúng tôi tiếp tục xác thực giá trị của dữ liệu được tạo bởi GPT-4 bằng cách tạo dữ liệu tinh chỉnh nhiễu (N_finetuning), tương tự như dữ liệu tiền huấn luyện. Biến thể (5) đạt được độ chính xác tương đương với biến thể (3). Tuy nhiên, như được hiển thị trong Hình 4, dữ liệu tinh chỉnh nhiễu như vậy làm tổn hại khả năng theo hướng dẫn: (5) phản hồi với tất cả văn bản được nhận dạng trong khi bỏ qua các câu hỏi.

**Nghiên cứu Ablation về bộ mã hóa/độ phân giải hình ảnh** Trong khi giữ dữ liệu tinh chỉnh giống nhau, chúng tôi báo cáo kết quả định lượng của việc thêm bộ mã hóa thị giác bổ sung và thay đổi dữ liệu tiền huấn luyện trong Bảng 4. (i) Lấy Pix2Struct-base làm ví dụ, chúng tôi thấy rằng việc thêm bộ mã hóa thị giác high-res bổ sung với cross-attention thực sự cải thiện hiệu suất ((g) so với (a)), đặc biệt đạt được hiệu suất zero-shot tốt nhất trên DocVQA (15.3% độ chính xác). Mức độ cải thiện hiệu suất trên các tập dữ liệu khác tương đối hạn chế, có thể do bộ mã hóa bổ sung chúng tôi sử dụng được tiền huấn luyện trên các trang web thay vì hình ảnh tự nhiên. Mặt khác, hiệu suất của (e) và (f) vẫn kém, không nhân đôi số lượng ví dụ high-res trong R_pretraining. Với số lượng tham số lớn hơn được khởi tạo trong mô-đun cross-attention, chúng có thể bị underfitting khi được huấn luyện trên cùng dữ liệu như ma trận chiếu W (ví dụ: (e) so với (b)), tương tự như phát hiện trong Zeng et al. [48]. (ii) Xem xét (c) so với (a) và (d) so với (b), trong khi hình ảnh được thay đổi kích thước cùng kích thước sau tiền xử lý, kết quả OCR high-res hóa ra không nhất thiết tốt hơn phiên bản độ phân giải thấp, cho thấy khả năng của bộ mã hóa thị giác gần như bão hòa trong (a) và (b). Để biết thêm chi tiết và kết quả về bộ mã hóa high-res bổ sung, vui lòng tham khảo Phụ lục H.

**Đánh giá theo hướng dẫn dựa trên GPT-4** Chúng tôi cũng báo cáo kết quả đánh giá GPT-4 về các câu hỏi theo hướng dẫn trong Bảng 5. (i) Hình ảnh Tự nhiên: 90 câu hỏi dựa trên 30 hình ảnh xác thực COCO từ Liu et al. [3], bao gồm ba khía cạnh: hội thoại, mô tả chi tiết, và suy luận phức tạp. Điều này nhằm kiểm tra liệu dữ liệu đã thu thập của chúng tôi sẽ làm tổn hại, duy trì, hay cải thiện hiệu suất của mô hình trên hình ảnh tự nhiên. Trước hết, sử dụng độ phân giải cao hơn mang lại sự cải thiện (+2.9) trong hiệu suất mô tả chi tiết, điều này trực quan. Hơn nữa, LLaVAR đạt được sự cân bằng tốt hơn và tăng hiệu suất của cả ba khía cạnh (+1.6 trung bình). Thêm chi tiết trong Phụ lục J. (ii) Hình ảnh Giàu văn bản: Tương tự như thu thập dữ liệu tinh chỉnh, chúng tôi tận dụng 50 hình ảnh giàu văn bản từ LAION để thu thập các câu hỏi theo hướng dẫn dựa trên kết quả OCR và chú thích do con người chú thích. Sau đó chúng tôi thu thập phản hồi từ mô hình đã huấn luyện của chúng tôi và sử dụng GPT-4 để tính điểm tương đối so với phản hồi GPT-4. Chúng tôi thêm điều này như một chiều bổ sung "Read" vào Bảng 5, nơi mô hình của chúng tôi chứng minh sự cải thiện đáng kể (+3.8). Phụ lục cung cấp một ví dụ trong Bảng 17.

### 5.2 Phân tích Định tính

Chúng tôi sử dụng một áp phích phim gần đây để chứng minh sự khác biệt giữa LLaVA và LLaVAR khi tương tác với con người dựa trên hình ảnh giàu văn bản. LLaVA, không tăng cường hiểu biết văn bản trong hình ảnh, gặp phải ảo giác khi trả lời những câu hỏi này. Một số phim được đề cập, như "A Man Called Ove" và "The Ugly Truth", là những bộ phim thực tế, cho thấy bộ giải mã ngôn ngữ đang ảo giác kiến thức nội tại của nó, trong khi bộ mã hóa thị giác không thể mã hóa thông tin hữu ích. Thay vào đó, LLaVAR có thể trả lời chính xác nhiều câu hỏi được cung cấp với thông tin trung thực, rõ ràng được căn cứ trên hình ảnh. Tuy nhiên, một số hạn chế vẫn còn, chẳng hạn như lỗi chính tả "ottol" (Chúng tôi cung cấp thêm thống kê liên quan đến những lỗi chính tả như vậy trong Phụ lục I). Ngoài ra, câu hỏi cuối cùng yêu cầu thông tin không thể quan sát được từ áp phích đã cho, nơi phản hồi dự kiến nên thể hiện sự không chắc chắn như vậy thay vì đưa ra câu trả lời cụ thể. Tuy nhiên, cả hai mô hình đều không trả lời đúng câu hỏi.

### 5.3 Nghiên cứu Trường hợp: Kích thước Phông chữ Có thể Nhận dạng

Chúng tôi đầu tiên thu thập 825 ví dụ từ OCR-VQA, có câu trả lời được trình bày trực tiếp trong hình ảnh và có thể được phát hiện bởi công cụ OCR. Bằng cách thay đổi tỷ lệ hình ảnh, chúng tôi kiểm tra hiệu suất của mô hình trong việc trả lời những câu hỏi này trong khi chiều cao dọc của câu trả lời dao động từ 3 pixel đến 19 pixel. Chúng tôi báo cáo kết quả trong Hình 6. (i) Đối với mô hình baseline LLaVA, nó gặp khó khăn trong việc cung cấp câu trả lời chính xác trong tất cả các tình huống, cho cả phiên bản dựa trên 224² và 336². (ii) Mô hình LLaVAR của chúng tôi đạt được kết quả tốt hơn đáng kể trong tất cả các tỷ lệ. Chúng tôi quan sát một ngưỡng cho văn bản có thể nhận dạng cho cả phiên bản dựa trên 224² và 336² khi độ chính xác giảm mạnh khi chiều cao nhỏ hơn 7 pixel. Thú vị hơn, phiên bản dựa trên 224² đạt được hiệu suất tốt hơn trên văn bản nhỏ với chiều cao 3 pixel trong khi phiên bản dựa trên 336² đạt được hiệu suất tốt hơn trên văn bản lớn với hơn 7 pixel chiều cao. Chúng tôi cho rằng giai đoạn huấn luyện bổ sung của CLIP 336² làm cho nó tốt hơn ở tỷ lệ lớn hơn nhưng tệ hơn ở tỷ lệ nhỏ hơn.

### 5.4 Khả năng Theo hướng dẫn Được chuyển giao

Theo thống kê tập dữ liệu (Bảng 1) và trực quan hóa (Hình 11), dữ liệu theo hướng dẫn đã thu thập của chúng tôi không đa dạng và đáng kể như LLaVA. Điều này có thể được quy cho thông tin tương đối hạn chế được cung cấp cho GPT-4 so với năm chú thích khác nhau do con người viết được sử dụng trong LLaVA. Nội dung của hình ảnh giàu văn bản cũng ít đa dạng hơn so với hình ảnh tự nhiên. Trong khi sử dụng các ví dụ trong ngữ cảnh phức tạp hơn chắc chắn có thể kích thích tạo ra các ví dụ theo hướng dẫn phức tạp hơn, nó cũng có thể nhân lên chi phí. Trong Phụ lục Hình 9, chúng tôi chứng minh khả năng theo hướng dẫn được chuyển giao của LLaVA, có thể từ cả dữ liệu LLaVA và backbone Vicuna. Trong khi dữ liệu bổ sung chúng tôi thêm chủ yếu tập trung vào hiểu các văn bản hiển thị trong hình ảnh, LLaVAR quản lý để xây dựng kỹ năng suy luận, viết và mở rộng của mình dựa trên khả năng nhận dạng văn bản theo cách đầu cuối. Điều này cho phép người dùng tương tác với các nội dung trực tuyến khác nhau dựa trên ảnh chụp màn hình đơn giản.

## 6. Kết luận

Trong công trình này, chúng tôi nâng cao các mô hình được điều chỉnh hướng dẫn thị giác về khả năng đọc văn bản trong hình ảnh. Sử dụng hình ảnh giàu văn bản từ tập dữ liệu LAION, chúng tôi thu thập 422K ví dụ theo hướng dẫn nhiễu chỉ sử dụng kết quả OCR và 16K dữ liệu theo hướng dẫn chất lượng cao dựa trên GPT-4 chỉ văn bản. Hai tập dữ liệu này được tận dụng để tăng cường giai đoạn tiền huấn luyện và giai đoạn tinh chỉnh của LLaVA tương ứng. Mô hình của chúng tôi, LLaVAR, chứng minh hiệu suất vượt trội trong việc hiểu văn bản trong hình ảnh và tuân theo hướng dẫn của con người trên cả benchmark trước đó và nội dung trực tuyến thực tế. Hơn nữa, phân tích của chúng tôi cho thấy cùng dữ liệu tăng cường hiệu quả hơn với độ phân giải cao hơn. Ngoài ra, sử dụng các ví dụ theo hướng dẫn nhiễu để tăng cường tiền huấn luyện về cơ bản tăng hiệu suất mô hình mà không cần nhắc GPT-4. Đối với công việc tương lai, chúng tôi khuyến khích khám phá (i) tiêu chí lựa chọn hình ảnh tốt hơn hoặc chiến lược cân bằng lại miền [49] và (ii) các cách hiệu quả hơn về dữ liệu và tính toán để tăng cường các mô hình theo hướng dẫn với khả năng đa phương thức, đặc biệt trong tình huống high-res.
