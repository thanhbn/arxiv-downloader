<<<<<<< Updated upstream
# LLaVAR: Điều chỉnh Hướng dẫn Thị giác Nâng cao cho Hiểu biết Hình ảnh Giàu văn bản

Yanzhe Zhang1∗, Ruiyi Zhang2, Jiuxiang Gu2, Yufan Zhou2, Nedim Lipka2,
Diyi Yang3, Tong Sun2
1Georgia Tech, 2Adobe Research, 3Stanford University

## Tóm tắt

Điều chỉnh hướng dẫn nâng cao khả năng của Mô hình Ngôn ngữ Lớn (LLMs) để tương tác với con người. Hơn nữa, các tập dữ liệu theo hướng dẫn gần đây bao gồm hình ảnh như đầu vào thị giác, thu thập phản hồi cho các hướng dẫn dựa trên hình ảnh. Tuy nhiên, các mô hình được điều chỉnh hướng dẫn thị giác hiện tại không thể hiểu rõ chi tiết văn bản trong hình ảnh. Công trình này nâng cao quy trình điều chỉnh hướng dẫn thị giác hiện tại với hình ảnh giàu văn bản (ví dụ: áp phích phim, bìa sách, v.v.). Cụ thể, chúng tôi đầu tiên sử dụng các công cụ OCR có sẵn công khai để thu thập kết quả trên 422K hình ảnh giàu văn bản từ tập dữ liệu LAION. Hơn nữa, chúng tôi nhắc GPT-4 chỉ văn bản với văn bản được nhận dạng và chú thích hình ảnh để tạo ra 16K cuộc hội thoại, mỗi cuộc hội thoại chứa các cặp câu hỏi-trả lời cho hình ảnh giàu văn bản. Bằng cách kết hợp dữ liệu đã thu thập của chúng tôi với dữ liệu theo hướng dẫn đa phương thức trước đó, mô hình của chúng tôi, LLaVAR, cải thiện đáng kể khả năng của mô hình LLaVA trên các tập dữ liệu VQA dựa trên văn bản (cải thiện độ chính xác lên đến 20%). Đánh giá theo hướng dẫn dựa trên GPT-4 cũng chứng minh sự cải thiện của mô hình chúng tôi trên cả hình ảnh tự nhiên và hình ảnh giàu văn bản. Thông qua phân tích định tính, LLaVAR cho thấy kỹ năng tương tác đầy hứa hẹn (ví dụ: suy luận, viết và mở rộng) với con người dựa trên nội dung trực tuyến thực tế mới nhất kết hợp văn bản và hình ảnh. Chúng tôi công khai mã/dữ liệu/mô hình của mình.

## 1. Giới thiệu

Điều chỉnh hướng dẫn [1,2] cải thiện khả năng tổng quát hóa cho các tác vụ chưa được nhìn thấy bằng cách hình thành các tác vụ khác nhau thành hướng dẫn. Khả năng trả lời câu hỏi mở như vậy thúc đẩy sự bùng nổ chatbot gần đây kể từ ChatGPT. Gần đây, các mô hình được điều chỉnh hướng dẫn thị giác [3-5] tiếp tục tăng cường các tác nhân hội thoại với các bộ mã hóa thị giác như CLIP-ViT [6,7], cho phép tương tác người-tác nhân dựa trên hình ảnh. Tuy nhiên, có thể do sự thống trị của hình ảnh tự nhiên trong dữ liệu huấn luyện (ví dụ: Conceptual Captions [8] và COCO [9]), chúng gặp khó khăn trong việc hiểu văn bản trong hình ảnh [10]. Tuy nhiên, hiểu biết văn bản là một phần không thể thiếu của nhận thức thị giác trong cuộc sống hàng ngày.

May mắn thay, các công cụ như Nhận dạng Ký tự Quang học (OCR, 11) cho phép chúng ta nhận dạng văn bản trong hình ảnh. Một cách đơn giản để sử dụng điều này là thêm văn bản được nhận dạng vào đầu vào của các mô hình được điều chỉnh hướng dẫn thị giác [12]. Tuy nhiên, cách tiếp cận này làm tăng đáng kể tính toán (độ dài ngữ cảnh dài hơn), và có thể không tận dụng đầy đủ khả năng mã hóa của các bộ mã hóa thị giác. Để đạt được mục tiêu này, chúng tôi đề xuất nâng cao mô hình được điều chỉnh hướng dẫn thị giác đầu cuối bằng cách thu thập dữ liệu theo hướng dẫn yêu cầu hiểu văn bản trong hình ảnh.

Cụ thể, chúng tôi đầu tiên thu thập 422K dữ liệu theo hướng dẫn nhiễu bằng cách sử dụng hình ảnh giàu văn bản bằng cách kết hợp các hướng dẫn được viết thủ công (ví dụ: "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp.") và kết quả OCR. Dữ liệu căn chỉnh nhiễu quy mô lớn như vậy cải thiện hiệu quả việc căn chỉnh đặc trưng giữa các đặc trưng thị giác và bộ giải mã ngôn ngữ. Hơn nữa, chúng tôi nhắc GPT-4 chỉ văn bản [13] với kết quả OCR và chú thích hình ảnh để tạo ra 16K cuộc hội thoại, trong đó mỗi cuộc hội thoại có thể là nhiều lượt cặp câu hỏi & trả lời, như các ví dụ theo hướng dẫn chất lượng cao. Quá trình này yêu cầu GPT-4 loại bỏ nhiễu kết quả OCR và phát triển các câu hỏi cụ thể để tạo ra các hướng dẫn phức tạp dựa trên đầu vào (Hình 1).

Để đánh giá hiệu quả của dữ liệu đã thu thập, chúng tôi sử dụng các ví dụ nhiễu và chất lượng cao để tăng cường các giai đoạn tiền huấn luyện và tinh chỉnh của LLaVA [3] tương ứng. Chúng tôi đặt tên mô hình của mình là LLaVAR, biểu thị LLaVA (Trợ lý Ngôn ngữ và Tầm nhìn Lớn) có thể Đọc. So với LLaVA gốc, chúng tôi cũng tiến hành thí nghiệm mở rộng độ phân giải đầu vào từ 224² đến 336² để mã hóa tốt hơn các chi tiết văn bản nhỏ. Thực nghiệm, chúng tôi báo cáo kết quả trên bốn tập dữ liệu VQA dựa trên văn bản theo giao thức đánh giá từ Liu et al. [10]. Hơn nữa, chúng tôi áp dụng đánh giá theo hướng dẫn dựa trên GPT-4 trên 30 hình ảnh tự nhiên từ COCO [9,3] và 50 hình ảnh giàu văn bản từ LAION [14]. Chúng tôi cũng cung cấp phân tích định tính (ví dụ: trên áp phích, ảnh chụp màn hình trang web và tweet) để kiểm tra các kỹ năng theo hướng dẫn phức tạp hơn.

Tóm lại, đóng góp của chúng tôi như sau:
• Chúng tôi thu thập 422K dữ liệu theo hướng dẫn nhiễu và 16K dữ liệu theo hướng dẫn chất lượng cao. Cả hai đều được chứng minh là hiệu quả trong việc tăng cường điều chỉnh hướng dẫn thị giác.
• Mô hình của chúng tôi, LLaVAR, nâng cao đáng kể khả năng hiểu văn bản trong hình ảnh trong khi cải thiện nhẹ hiệu suất của mô hình trên hình ảnh tự nhiên.
• Khả năng nâng cao cho phép mô hình của chúng tôi cung cấp tương tác đầu cuối dựa trên các hình thức khác nhau của nội dung trực tuyến kết hợp văn bản và hình ảnh.
• Chúng tôi mở nguồn dữ liệu huấn luyện và đánh giá cùng với các checkpoint mô hình.

## 2. Công trình liên quan

**Điều chỉnh Hướng dẫn** Tuân theo hướng dẫn ngôn ngữ tự nhiên là khả năng chủ chốt để một tác nhân tương tác với người dùng thực tế. Điều chỉnh hướng dẫn bắt đầu từ việc thu thập phản hồi được con người ưa thích cho các hướng dẫn do con người viết [1] hoặc xây dựng huấn luyện đa tác vụ theo cách theo hướng dẫn đa tác vụ [2,15]. Tuy nhiên, các mô hình được điều chỉnh hướng dẫn lớn, có khả năng thường là mã nguồn đóng và chỉ phục vụ như API thương mại. Gần đây, Alpaca [16,17], Vicuna [18], và Baize [19] bắt đầu xu hướng tạo ra dữ liệu theo hướng dẫn chất lượng cao dựa trên LLMs như GPT-3.5 / ChatGPT / GPT-4 và tinh chỉnh mô hình LLaMA mã nguồn mở [20]. Tuy nhiên, đánh giá khả năng tuân theo hướng dẫn vẫn là một thách thức. Trong khi GPT-4 đã chứng minh khả năng đánh giá vượt trội [21], vẫn có một số lo ngại, chẳng hạn như thiên vị đối với độ dài phản hồi [19] và thiếu tính mạnh mẽ đối với thứ tự của các ví dụ [22]. Theo Chiang et al. [18], Liu et al. [3], Dubois et al. [23], chúng tôi sử dụng đánh giá theo hướng dẫn dựa trên GPT-4 trong công trình này.

**Điều chỉnh Hướng dẫn Đa phương thức** Gần đây, điều chỉnh hướng dẫn đã được mở rộng đến thiết lập đa phương thức, bao gồm hình ảnh, video [24,25], và âm thanh [26,27]. Đối với điều chỉnh hướng dẫn dựa trên hình ảnh, MiniGPT-4 [28] sử dụng ChatGPT để tuyển chọn và cải thiện chú thích chi tiết cho dữ liệu theo hướng dẫn chất lượng cao. LLaVA [3] tạo ra dữ liệu theo hướng dẫn đa phương thức bằng cách nhắc GPT-4 chỉ văn bản với chú thích và hộp giới hạn của đối tượng. LLaMA-Adapter [29,12] sử dụng dữ liệu COCO để căn chỉnh đặc trưng văn bản-hình ảnh và chỉ sử dụng dữ liệu văn bản để điều chỉnh hướng dẫn. mPLUG-owl [30] kết hợp hơn 1000M cặp hình ảnh-văn bản để tiền huấn luyện và hỗn hợp 400K dữ liệu theo hướng dẫn chỉ văn bản/đa phương thức để tinh chỉnh. Tuy nhiên, theo Liu et al. [10], hầu hết các mô hình này gặp khó khăn trong việc hoàn thành các tác vụ yêu cầu khả năng OCR. InstructBLIP [31] chuyển đổi 13 tác vụ ngôn ngữ-tầm nhìn (bao gồm OCR-VQA [32]) thành định dạng theo hướng dẫn để điều chỉnh hướng dẫn. Cream [33] áp dụng học đa tác vụ bao gồm dự đoán văn bản bị che trong hình ảnh. Một khảo sát toàn diện hơn có thể được tìm thấy trong Li et al. [34]. Trong công trình này, chúng tôi chọn LLaVA làm baseline, đây là mô hình hiệu quả nhất về dữ liệu và mạnh mẽ nhất, và chứng minh hiệu quả của quy trình đề xuất của chúng tôi.

## 3. Thu thập Dữ liệu

Bắt đầu từ tập dữ liệu LAION-5B [14], mục tiêu của chúng tôi chỉ là giữ lại những hình ảnh giàu văn bản. Xem xét rằng tài liệu thường chứa nhiều văn bản, chúng tôi đầu tiên thu được một tập dữ liệu phân loại nhị phân bằng cách kết hợp hình ảnh tự nhiên và dữ liệu tài liệu. Sau đó, chúng tôi huấn luyện một bộ phân loại hình ảnh sử dụng backbone DiT [35]-base, được tinh chỉnh trên tập dữ liệu RVL-CDIP [36]. Hy vọng rằng, bộ phân loại như vậy có thể dự đoán liệu một hình ảnh có chứa văn bản hay không. Chúng tôi đầu tiên xây dựng một tập con bằng cách chọn những hình ảnh có xác suất dự đoán lớn hơn 0.8 đồng thời thỏa mãn p(watermark) < 0.8 và p(unsafe) < 0.5. Tập con thu được có nhiễu do hạn chế của bộ phân loại. Để làm sạch thêm dữ liệu và kết hợp đánh giá của con người, chúng tôi lấy mẫu ngẫu nhiên 50K hình ảnh và gom chúng thành 100 cụm dựa trên các đặc trưng thị giác CLIP-ViT-B/32. Sau khi kiểm tra kết quả phân cụm, chúng tôi cẩn thận chọn 14 cụm (xem Hình 10 trong Phụ lục để biết ví dụ) chứa các hình ảnh giàu văn bản đa dạng từ áp phích, bìa, quảng cáo, infographic, tài liệu giáo dục, và logo. Mô hình cụm sau đó được sử dụng làm bộ lọc để thu thập hình ảnh để xây dựng các ví dụ theo hướng dẫn của chúng tôi. Làm tham chiếu, chúng tôi cung cấp phân loại dựa trên CLIP [7] (xem Phụ lục A để biết chi tiết) để minh họa phân phối hình ảnh cho cả hai loại dữ liệu chúng tôi thu thập trong Hình 2. Chúng tôi tóm tắt dữ liệu đã thu thập của chúng tôi và dữ liệu của LLaVA trong Bảng 1.

**Dữ liệu Theo hướng dẫn Nhiễu** Sử dụng mô hình phân cụm làm bộ lọc, chúng tôi thu thập 422K hình ảnh đã loại bỏ trùng lặp thuộc về 14 cụm được ưa thích. Để cân bằng các ví dụ từ các danh mục khác nhau, chúng tôi giữ tối đa 52K ví dụ cho một cụm. Chúng tôi chạy tất cả hình ảnh qua PaddleOCR. Lưu ý rằng chạy OCR ở độ phân giải gốc (ví dụ: 1024²) có thể nhận dạng các phông chữ nhỏ không thể nhìn thấy bởi các bộ mã hóa thị giác như CLIP ViT (6,7, độ phân giải lên đến 336²). Để đảm bảo nhận dạng các phông chữ có thể nhìn thấy trong khi duy trì độ chính xác OCR, chúng tôi thực hiện OCR trên hình ảnh sau khi giảm mẫu (cạnh ngắn được thay đổi kích thước xuống 384 pixel nếu dài hơn) để trích xuất văn bản. Sau đó, dựa trên mối quan hệ hình học giữa các từ được nhận dạng, chúng tôi gộp chúng thành các đoạn văn trước khi nối chúng lại. Vì một mô hình theo hướng dẫn mạnh mẽ nên phản ứng tương tự với các hướng dẫn có nghĩa tương tự, chúng tôi viết lại "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp." thành mười hướng dẫn riêng biệt (Bảng 6 trong Phụ lục). Sau đó chúng tôi tạo một cuộc hội thoại một lượt cho một hình ảnh nhất định bằng cách (i) lấy mẫu ngẫu nhiên một hướng dẫn đầu vào và (ii) sử dụng văn bản được nhận dạng làm phản hồi đầu ra mong muốn. Dữ liệu theo hướng dẫn như vậy có nhiễu vì hiệu suất tương đối hạn chế của các công cụ OCR trên các phông chữ đa dạng và nền màu sắc.

**Dữ liệu Theo hướng dẫn Dựa trên GPT-4** So với dữ liệu theo hướng dẫn chất lượng cao, chủ yếu có hai vấn đề đối với dữ liệu nhiễu được thu thập ở trên. (i) Phản hồi nên chứa các câu có tổ chức thay vì kết quả OCR thô với từ bị thiếu và lỗi ngữ pháp. (ii) Hướng dẫn nên đa dạng, phù hợp và cụ thể với hình ảnh nhất định thay vì đơn điệu yêu cầu tất cả văn bản hiển thị. Để giải quyết những vấn đề này, chúng tôi theo Liu et al. [3] để tạo ra dữ liệu theo hướng dẫn bằng cách nhắc GPT-4 chỉ văn bản [13] với kết quả OCR và chú thích.

Thật khó khăn để nhắc GPT-4 với các kết quả OCR phân mảnh trong vài từ để tạo ra các hướng dẫn không tầm thường. Để đạt được mục tiêu này, chúng tôi cẩn thận chọn 4 trong số 14 cụm đã đề cập trước đó (cụm thứ 3, 4, 6 và 9 trong Hình 10) để thu thập hình ảnh với đủ câu hiển thị và mạch lạc. Như được hiển thị trong Hình 2, việc lọc như vậy làm tăng đáng kể tỷ lệ phần trăm của bìa sách và hình ảnh trích dẫn. Chúng tôi chọn ngẫu nhiên 4K ví dụ từ mỗi cụm (không trùng lặp với hình ảnh được sử dụng cho dữ liệu theo hướng dẫn nhiễu), tạo ra tổng cộng 16K hình ảnh. Theo công trình trước [16,17,3], chúng tôi cung cấp trực quan hóa các cặp động từ-danh từ cho các hướng dẫn được tạo bởi GPT-4 trong Phụ lục Hình 11. Đối với những hướng dẫn không có cặp động từ-danh từ, chúng tôi chứng minh tần suất của các đối tượng được hỏi trong Phụ lục Hình 12.

Hơn nữa, dựa trên thông điệp hệ thống và hai ví dụ few-shot trong ngữ cảnh (được hiển thị trong Phụ lục B), chúng tôi yêu cầu GPT-4 tạo ra dữ liệu hội thoại dựa trên kết quả OCR và chú thích hình ảnh (Hình 1). Các câu hỏi được tạo ra được sử dụng làm hướng dẫn đầu vào, và câu trả lời được sử dụng làm phản hồi đầu ra. Cụ thể, đối với một hình ảnh nhất định, chúng tôi đầu tiên cung cấp hai kết quả OCR từ EasyOCR và PaddleOCR, có thể bổ sung cho nhau. Để minh họa các yếu tố thị giác khác ngoài văn bản trong hình ảnh, chúng tôi cũng cung cấp kết quả của việc tạo chú thích hình ảnh BLIP-2 [37]. Để ngăn chú thích tập trung vào văn bản, chúng tôi sử dụng hộp giới hạn OCR để che văn bản và sau đó sử dụng inpainting [38] để điền lại vùng che trước khi sử dụng tạo chú thích. Lưu ý rằng các mô hình tạo chú thích có thể gặp phải ảo giác [39]. Chúng tôi đề cập đến sự không đáng tin cậy này trong thông điệp hệ thống của chúng tôi và yêu cầu GPT-4 chỉ tạo ra các câu hỏi có câu trả lời chắc chắn. Chúng tôi để việc tạo ra các chú thích chi tiết hơn [40, 41] cho công việc tương lai.

## 4. Kiến trúc Mô hình và Huấn luyện

**Kiến trúc** Trong hầu hết nghiên cứu của chúng tôi, chúng tôi sử dụng cùng kiến trúc mô hình như LLaVA. Đối với bộ mã hóa thị giác V, chúng tôi sử dụng CLIP-ViT-L/14 cho độ phân giải 224² và CLIP-ViT-L/14-336 cho độ phân giải 336². Các đặc trưng lưới trước lớp transformer cuối cùng sau đó được chuyển đổi vào không gian nhúng từ của bộ giải mã ngôn ngữ thông qua ma trận chiếu có thể huấn luyện W. Chúng tôi sử dụng Vicuna-13B [18], một mô hình ngôn ngữ được điều chỉnh hướng dẫn dựa trên LLaMA [20], làm bộ giải mã ngôn ngữ D ngoại trừ nghiên cứu ablation trong Bảng 4.

Trong Phần 5.1 và Phụ lục H, chúng tôi mở rộng kiến trúc hiện tại bằng cách thêm một bộ mã hóa thị giác độ phân giải cao (high-res) bổ sung. Bộ mã hóa high-res như vậy xuất ra hàng nghìn đặc trưng patch, có nghĩa là các đặc trưng được chuyển đổi và token hướng dẫn không thể vừa với độ dài ngữ cảnh của bộ giải mã ngôn ngữ. Để đạt được mục tiêu này, chúng tôi đề xuất thêm các mô-đun cross-attention vào bộ giải mã, chú ý đến các cặp key-value được chuyển đổi từ các đặc trưng patch high-res.

**Huấn luyện** Chúng tôi tuân theo thiết kế huấn luyện hai giai đoạn của LLaVA (Hình 3). Mục tiêu huấn luyện của cả hai giai đoạn đều giống nhau: tạo ra phản hồi đầu ra (<res>) cho các hướng dẫn đầu vào (<ins>). Các token hình ảnh được chuyển đổi (<img>) được thêm trước hoặc sau hướng dẫn đầu vào đầu tiên. (i) Trong giai đoạn tiền huấn luyện đầu tiên, chỉ ma trận chiếu W được huấn luyện để căn chỉnh đặc trưng. Vì bộ giải mã D bị đóng băng, huấn luyện chịu đựng được dữ liệu nhiễu. Trong giai đoạn tiền huấn luyện, chúng tôi kết hợp 595K dữ liệu tiền huấn luyện từ LLaVA với 422K dữ liệu theo hướng dẫn nhiễu của chúng tôi. (ii) Cả ma trận chiếu W và bộ giải mã ngôn ngữ D đều được huấn luyện trong giai đoạn tinh chỉnh, nơi chúng tôi gộp 16K dữ liệu theo hướng dẫn của chúng tôi vào 158K dữ liệu theo hướng dẫn từ LLaVA làm tập huấn luyện. Lưu ý rằng bộ mã hóa thị giác bị đóng băng trong suốt thời gian huấn luyện, điều này có thể hạn chế hiệu suất nhận dạng văn bản, vì CLIP được huấn luyện để căn chỉnh văn bản-hình ảnh tổng quát. Các lựa chọn tốt hơn của bộ mã hóa thị giác [42] hoặc tinh chỉnh CLIP-ViT [30] có thể mang lại lợi ích thêm cho khả năng hiểu thị giác, điều mà chúng tôi để dành cho công việc tương lai.

## 5. Thí nghiệm

Chúng tôi sử dụng cùng siêu tham số huấn luyện như LLaVA, ngoại trừ (i) Chúng tôi đặt độ dài chuỗi tối đa là 1024 trong quá trình tiền huấn luyện. (ii) Chúng tôi đầu tiên pad bất kỳ hình ảnh nào thành hình vuông trước khi thay đổi kích thước nó đến kích thước đầu vào mong muốn, ngăn một số nội dung hình ảnh bị cắt trong quá trình tiền xử lý. Đối với cả hai độ phân giải (224², 336²), chúng tôi tái tạo LLaVA gốc để so sánh công bằng. Mô hình GPT-4 được sử dụng trong công trình này đề cập đến phiên bản gpt-4-0314, trong khi chi phí để thu thập dữ liệu tinh chỉnh là khoảng $300. Nhiệt độ được sử dụng để lấy mẫu GPT-4 được đặt là 1.0 để tạo dữ liệu huấn luyện, 0.7 để tạo dữ liệu đánh giá, và 0.2 để đánh giá dựa trên GPT-4. Tất cả thí nghiệm được chạy trên GPU NVIDIA A100 80GB. Trong quá trình đánh giá, nhiệt độ được sử dụng để lấy mẫu từ mô hình của chúng tôi được đặt ở 0.9 cho VQA dựa trên văn bản, 0.7 cho đánh giá theo hướng dẫn dựa trên GPT-4, và 0.2 cho các minh họa định tính khác.

### 5.1 Phân tích Định lượng

**VQA dựa trên Văn bản** Theo giao thức đánh giá trong Liu et al. [10], chúng tôi đánh giá hiệu suất của LLaVAR trên bốn tập dữ liệu VQA dựa trên văn bản: ST-VQA [45], OCR-VQA [32], TextVQA [46], và DocVQA [47], đại diện cho các lĩnh vực khác nhau (xem Phụ lục C để biết thêm chi tiết và Phụ lục E để biết thêm tập dữ liệu). Chúng tôi trình bày kết quả của các mô hình baseline và mô hình của chúng tôi trong Bảng 2. Lưu ý rằng InstructBLIP bao gồm OCR-VQA trong tập huấn luyện của nó, khiến nó không thể so sánh được với cài đặt của chúng tôi. Trong cả hai cài đặt độ phân giải và tất cả bốn tập dữ liệu, LLaVAR cải thiện đáng kể baseline LLaVA, chứng minh rằng dữ liệu đã thu thập của chúng tôi có thể mang lại sự cải thiện mạnh mẽ. Hơn nữa, sự cải thiện đáng kể hơn ở độ phân giải 336² so với 224², cho thấy dữ liệu đã thu thập có thể mang lại sự cải thiện lớn hơn ở độ phân giải thậm chí cao hơn. Mô hình tốt nhất của chúng tôi, LLaVAR dựa trên 336², hoạt động tốt nhất trong 3 trên 4 tập dữ liệu được đánh giá. Lưu ý rằng đây không phải là so sánh công bằng. Một số yếu tố chính bao gồm các bộ giải mã ngôn ngữ khác nhau, độ phân giải, và độ lớn của dữ liệu huấn luyện văn bản-hình ảnh. Chúng tôi cung cấp thêm thảo luận về so sánh với mPLUG-Owl và kết quả của việc tinh chỉnh mPLUG-Owl bằng dữ liệu của chúng tôi trong Phụ lục F.

**Nghiên cứu Ablation về dữ liệu tiền huấn luyện/tinh chỉnh** Chúng tôi báo cáo kết quả trong Bảng 3 và Hình 4. (i) Dựa trên các biến thể (2) và (3), chúng tôi thấy rằng dữ liệu đã thu thập có thể có lợi cho giai đoạn tiền huấn luyện (R_pretraining) và giai đoạn tinh chỉnh (R_finetuning) một cách riêng lẻ trong khi bổ sung cho nhau trong hầu hết các trường hợp. Quan trọng hơn, việc nâng cao riêng giai đoạn tiền huấn luyện đạt được hiệu suất tổng thể tốt thứ hai, cho thấy tiềm năng tăng cường hiểu biết chi tiết văn bản mà không phụ thuộc vào dữ liệu chất lượng cao được tạo bởi GPT-4. (ii) Sử dụng hình ảnh tiền huấn luyện, chúng tôi thu được C_pretraining bằng cách thay thế các hướng dẫn tiền huấn luyện bằng câu hỏi & chú thích, cùng mẫu như LLaVA. Vì biến thể (4) không tốt bằng (2), chúng tôi có thể kết luận rằng OCR có lợi hơn chú thích. (iii) Chúng tôi tiếp tục xác thực giá trị của dữ liệu được tạo bởi GPT-4 bằng cách tạo dữ liệu tinh chỉnh nhiễu (N_finetuning), tương tự như dữ liệu tiền huấn luyện. Biến thể (5) đạt được độ chính xác tương đương với biến thể (3). Tuy nhiên, như được hiển thị trong Hình 4, dữ liệu tinh chỉnh nhiễu như vậy làm tổn hại khả năng theo hướng dẫn: (5) phản hồi với tất cả văn bản được nhận dạng trong khi bỏ qua các câu hỏi.

**Nghiên cứu Ablation về bộ mã hóa/độ phân giải hình ảnh** Trong khi giữ dữ liệu tinh chỉnh giống nhau, chúng tôi báo cáo kết quả định lượng của việc thêm bộ mã hóa thị giác bổ sung và thay đổi dữ liệu tiền huấn luyện trong Bảng 4. (i) Lấy Pix2Struct-base làm ví dụ, chúng tôi thấy rằng việc thêm bộ mã hóa thị giác high-res bổ sung với cross-attention thực sự cải thiện hiệu suất ((g) so với (a)), đặc biệt đạt được hiệu suất zero-shot tốt nhất trên DocVQA (15.3% độ chính xác). Mức độ cải thiện hiệu suất trên các tập dữ liệu khác tương đối hạn chế, có thể do bộ mã hóa bổ sung chúng tôi sử dụng được tiền huấn luyện trên các trang web thay vì hình ảnh tự nhiên. Mặt khác, hiệu suất của (e) và (f) vẫn kém, không nhân đôi số lượng ví dụ high-res trong R_pretraining. Với số lượng tham số lớn hơn được khởi tạo trong mô-đun cross-attention, chúng có thể bị underfitting khi được huấn luyện trên cùng dữ liệu như ma trận chiếu W (ví dụ: (e) so với (b)), tương tự như phát hiện trong Zeng et al. [48]. (ii) Xem xét (c) so với (a) và (d) so với (b), trong khi hình ảnh được thay đổi kích thước cùng kích thước sau tiền xử lý, kết quả OCR high-res hóa ra không nhất thiết tốt hơn phiên bản độ phân giải thấp, cho thấy khả năng của bộ mã hóa thị giác gần như bão hòa trong (a) và (b). Để biết thêm chi tiết và kết quả về bộ mã hóa high-res bổ sung, vui lòng tham khảo Phụ lục H.

**Đánh giá theo hướng dẫn dựa trên GPT-4** Chúng tôi cũng báo cáo kết quả đánh giá GPT-4 về các câu hỏi theo hướng dẫn trong Bảng 5. (i) Hình ảnh Tự nhiên: 90 câu hỏi dựa trên 30 hình ảnh xác thực COCO từ Liu et al. [3], bao gồm ba khía cạnh: hội thoại, mô tả chi tiết, và suy luận phức tạp. Điều này nhằm kiểm tra liệu dữ liệu đã thu thập của chúng tôi sẽ làm tổn hại, duy trì, hay cải thiện hiệu suất của mô hình trên hình ảnh tự nhiên. Trước hết, sử dụng độ phân giải cao hơn mang lại sự cải thiện (+2.9) trong hiệu suất mô tả chi tiết, điều này trực quan. Hơn nữa, LLaVAR đạt được sự cân bằng tốt hơn và tăng hiệu suất của cả ba khía cạnh (+1.6 trung bình). Thêm chi tiết trong Phụ lục J. (ii) Hình ảnh Giàu văn bản: Tương tự như thu thập dữ liệu tinh chỉnh, chúng tôi tận dụng 50 hình ảnh giàu văn bản từ LAION để thu thập các câu hỏi theo hướng dẫn dựa trên kết quả OCR và chú thích do con người chú thích. Sau đó chúng tôi thu thập phản hồi từ mô hình đã huấn luyện của chúng tôi và sử dụng GPT-4 để tính điểm tương đối so với phản hồi GPT-4. Chúng tôi thêm điều này như một chiều bổ sung "Read" vào Bảng 5, nơi mô hình của chúng tôi chứng minh sự cải thiện đáng kể (+3.8). Phụ lục cung cấp một ví dụ trong Bảng 17.

### 5.2 Phân tích Định tính

Chúng tôi sử dụng một áp phích phim gần đây để chứng minh sự khác biệt giữa LLaVA và LLaVAR khi tương tác với con người dựa trên hình ảnh giàu văn bản. LLaVA, không tăng cường hiểu biết văn bản trong hình ảnh, gặp phải ảo giác khi trả lời những câu hỏi này. Một số phim được đề cập, như "A Man Called Ove" và "The Ugly Truth", là những bộ phim thực tế, cho thấy bộ giải mã ngôn ngữ đang ảo giác kiến thức nội tại của nó, trong khi bộ mã hóa thị giác không thể mã hóa thông tin hữu ích. Thay vào đó, LLaVAR có thể trả lời chính xác nhiều câu hỏi được cung cấp với thông tin trung thực, rõ ràng được căn cứ trên hình ảnh. Tuy nhiên, một số hạn chế vẫn còn, chẳng hạn như lỗi chính tả "ottol" (Chúng tôi cung cấp thêm thống kê liên quan đến những lỗi chính tả như vậy trong Phụ lục I). Ngoài ra, câu hỏi cuối cùng yêu cầu thông tin không thể quan sát được từ áp phích đã cho, nơi phản hồi dự kiến nên thể hiện sự không chắc chắn như vậy thay vì đưa ra câu trả lời cụ thể. Tuy nhiên, cả hai mô hình đều không trả lời đúng câu hỏi.

### 5.3 Nghiên cứu Trường hợp: Kích thước Phông chữ Có thể Nhận dạng

Chúng tôi đầu tiên thu thập 825 ví dụ từ OCR-VQA, có câu trả lời được trình bày trực tiếp trong hình ảnh và có thể được phát hiện bởi công cụ OCR. Bằng cách thay đổi tỷ lệ hình ảnh, chúng tôi kiểm tra hiệu suất của mô hình trong việc trả lời những câu hỏi này trong khi chiều cao dọc của câu trả lời dao động từ 3 pixel đến 19 pixel. Chúng tôi báo cáo kết quả trong Hình 6. (i) Đối với mô hình baseline LLaVA, nó gặp khó khăn trong việc cung cấp câu trả lời chính xác trong tất cả các tình huống, cho cả phiên bản dựa trên 224² và 336². (ii) Mô hình LLaVAR của chúng tôi đạt được kết quả tốt hơn đáng kể trong tất cả các tỷ lệ. Chúng tôi quan sát một ngưỡng cho văn bản có thể nhận dạng cho cả phiên bản dựa trên 224² và 336² khi độ chính xác giảm mạnh khi chiều cao nhỏ hơn 7 pixel. Thú vị hơn, phiên bản dựa trên 224² đạt được hiệu suất tốt hơn trên văn bản nhỏ với chiều cao 3 pixel trong khi phiên bản dựa trên 336² đạt được hiệu suất tốt hơn trên văn bản lớn với hơn 7 pixel chiều cao. Chúng tôi cho rằng giai đoạn huấn luyện bổ sung của CLIP 336² làm cho nó tốt hơn ở tỷ lệ lớn hơn nhưng tệ hơn ở tỷ lệ nhỏ hơn.

### 5.4 Khả năng Theo hướng dẫn Được chuyển giao

Theo thống kê tập dữ liệu (Bảng 1) và trực quan hóa (Hình 11), dữ liệu theo hướng dẫn đã thu thập của chúng tôi không đa dạng và đáng kể như LLaVA. Điều này có thể được quy cho thông tin tương đối hạn chế được cung cấp cho GPT-4 so với năm chú thích khác nhau do con người viết được sử dụng trong LLaVA. Nội dung của hình ảnh giàu văn bản cũng ít đa dạng hơn so với hình ảnh tự nhiên. Trong khi sử dụng các ví dụ trong ngữ cảnh phức tạp hơn chắc chắn có thể kích thích tạo ra các ví dụ theo hướng dẫn phức tạp hơn, nó cũng có thể nhân lên chi phí. Trong Phụ lục Hình 9, chúng tôi chứng minh khả năng theo hướng dẫn được chuyển giao của LLaVA, có thể từ cả dữ liệu LLaVA và backbone Vicuna. Trong khi dữ liệu bổ sung chúng tôi thêm chủ yếu tập trung vào hiểu các văn bản hiển thị trong hình ảnh, LLaVAR quản lý để xây dựng kỹ năng suy luận, viết và mở rộng của mình dựa trên khả năng nhận dạng văn bản theo cách đầu cuối. Điều này cho phép người dùng tương tác với các nội dung trực tuyến khác nhau dựa trên ảnh chụp màn hình đơn giản.

## 6. Kết luận

Trong công trình này, chúng tôi nâng cao các mô hình được điều chỉnh hướng dẫn thị giác về khả năng đọc văn bản trong hình ảnh. Sử dụng hình ảnh giàu văn bản từ tập dữ liệu LAION, chúng tôi thu thập 422K ví dụ theo hướng dẫn nhiễu chỉ sử dụng kết quả OCR và 16K dữ liệu theo hướng dẫn chất lượng cao dựa trên GPT-4 chỉ văn bản. Hai tập dữ liệu này được tận dụng để tăng cường giai đoạn tiền huấn luyện và giai đoạn tinh chỉnh của LLaVA tương ứng. Mô hình của chúng tôi, LLaVAR, chứng minh hiệu suất vượt trội trong việc hiểu văn bản trong hình ảnh và tuân theo hướng dẫn của con người trên cả benchmark trước đó và nội dung trực tuyến thực tế. Hơn nữa, phân tích của chúng tôi cho thấy cùng dữ liệu tăng cường hiệu quả hơn với độ phân giải cao hơn. Ngoài ra, sử dụng các ví dụ theo hướng dẫn nhiễu để tăng cường tiền huấn luyện về cơ bản tăng hiệu suất mô hình mà không cần nhắc GPT-4. Đối với công việc tương lai, chúng tôi khuyến khích khám phá (i) tiêu chí lựa chọn hình ảnh tốt hơn hoặc chiến lược cân bằng lại miền [49] và (ii) các cách hiệu quả hơn về dữ liệu và tính toán để tăng cường các mô hình theo hướng dẫn với khả năng đa phương thức, đặc biệt trong tình huống high-res.
=======
LLaVAR: Điều chỉnh Hướng dẫn Thị giác Nâng cao
cho Hiểu Ảnh Giàu Văn bản
Yanzhe Zhang1∗, Ruiyi Zhang2, Jiuxiang Gu2, Yufan Zhou2, Nedim Lipka2,
Diyi Yang3, Tong Sun2
1Georgia Tech,2Adobe Research,3Stanford University

Tóm tắt
Điều chỉnh hướng dẫn nâng cao khả năng của Mô hình Ngôn ngữ Lớn (LLM) để
tương tác với con người. Hơn nữa, các bộ dữ liệu hướng dẫn theo dõi gần đây bao gồm
hình ảnh làm đầu vào thị giác, thu thập phản hồi cho các hướng dẫn dựa trên hình ảnh. Tuy nhiên,
các mô hình điều chỉnh hướng dẫn thị giác hiện tại không thể hiểu tốt chi tiết văn bản trong
hình ảnh. Công trình này nâng cao quy trình điều chỉnh hướng dẫn thị giác hiện tại với
hình ảnh giàu văn bản (ví dụ: áp phích phim, bìa sách, v.v.). Cụ thể, chúng tôi đã sử dụng
các công cụ OCR có sẵn công khai để thu thập kết quả trên 422K hình ảnh giàu văn bản từ
bộ dữ liệu LAION. Hơn nữa, chúng tôi nhắc nhở GPT-4 chỉ văn bản với văn bản đã nhận dạng và
chú thích hình ảnh để tạo ra 16K cuộc hội thoại, mỗi cuộc chứa cặp câu hỏi-trả lời
cho hình ảnh giàu văn bản. Bằng cách kết hợp dữ liệu thu thập của chúng tôi với dữ liệu
hướng dẫn theo dõi đa phương thức trước đó, mô hình của chúng tôi, LLaVAR, cải thiện đáng kể
khả năng của mô hình LLaVA trên các bộ dữ liệu VQA dựa trên văn bản (cải thiện độ chính xác
lên đến 20%). Đánh giá hướng dẫn theo dõi dựa trên GPT-4 cũng chứng minh
sự cải thiện của mô hình chúng tôi trên cả hình ảnh tự nhiên và hình ảnh giàu văn bản.
Thông qua phân tích định tính, LLaVAR cho thấy kỹ năng tương tác hứa hẹn (ví dụ:
lý luận, viết và mô tả chi tiết) với con người dựa trên nội dung trực tuyến thế giới thực mới nhất
kết hợp văn bản và hình ảnh. Chúng tôi công khai mã/dữ liệu/mô hình
của mình.

1 Giới thiệu
Điều chỉnh hướng dẫn [1,2] cải thiện khả năng tổng quát cho các tác vụ chưa gặp bằng cách diễn đạt các tác vụ khác nhau thành
hướng dẫn. Khả năng trả lời câu hỏi mở như vậy thúc đẩy sự bùng nổ chatbot gần đây kể từ
ChatGPT. Gần đây, các mô hình điều chỉnh hướng dẫn thị giác [3-5] tiếp tục tăng cường các đại lý hội thoại
với bộ mã hóa thị giác như CLIP-ViT [6,7], cho phép tương tác người-đại lý dựa trên hình ảnh.
Tuy nhiên, có thể do sự thống trị của hình ảnh tự nhiên trong dữ liệu huấn luyện (ví dụ: Conceptual Captions
[8] và COCO [9]), chúng gặp khó khăn với việc hiểu văn bản trong hình ảnh [10]. Tuy nhiên, hiểu văn bản là một phần không thể thiếu của nhận thức thị giác trong cuộc sống hàng ngày.

May mắn thay, các công cụ như Nhận dạng Ký tự Quang học (OCR, 11) cho phép chúng ta nhận dạng văn bản trong
hình ảnh. Một cách ngây thơ để sử dụng điều này là thêm văn bản đã nhận dạng vào đầu vào của các mô hình điều chỉnh hướng dẫn thị giác [12]. Tuy nhiên, cách tiếp cận như vậy tăng đáng kể tính toán (độ dài ngữ cảnh dài hơn),
và có thể không tận dụng đầy đủ khả năng mã hóa của bộ mã hóa thị giác. Vì vậy, chúng tôi đề xuất
nâng cao mô hình điều chỉnh hướng dẫn thị giác đầu cuối bằng cách thu thập dữ liệu hướng dẫn theo dõi
yêu cầu hiểu văn bản trong hình ảnh.

Cụ thể, chúng tôi đầu tiên thu thập 422K dữ liệu hướng dẫn theo dõi nhiễu bằng cách sử dụng hình ảnh giàu văn bản bằng cách kết hợp
các hướng dẫn viết tay (ví dụ: "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp.") và
kết quả OCR. Dữ liệu căn chỉnh nhiễu quy mô lớn như vậy cải thiện hiệu quả căn chỉnh tính năng giữa
tính năng thị giác và bộ giải mã ngôn ngữ. Hơn nữa, chúng tôi nhắc nhở GPT-4 chỉ văn bản [13] với
kết quả OCR và chú thích hình ảnh để tạo ra 16K cuộc hội thoại, trong đó mỗi cuộc hội thoại có thể là
nhiều lượt cặp câu hỏi & trả lời, như các ví dụ hướng dẫn theo dõi chất lượng cao. Quá trình này
yêu cầu GPT-4 loại bỏ nhiễu khỏi kết quả OCR và phát triển các câu hỏi cụ thể để tạo ra các hướng dẫn phức tạp
dựa trên đầu vào (Hình 1).

Để đánh giá hiệu quả của dữ liệu thu thập, chúng tôi sử dụng các ví dụ nhiễu và chất lượng cao để tăng cường
các giai đoạn tiền huấn luyện và tinh chỉnh của LLaVA [3] tương ứng. Chúng tôi đặt tên mô hình của mình là LLaVAR,
biểu thị LLaVA (Trợ lý Ngôn ngữ và Thị giác Lớn) có thể Đọc. So với
LLaVA gốc, chúng tôi cũng tiến hành thử nghiệm tăng độ phân giải đầu vào từ 224² đến 336²
để mã hóa tốt hơn các chi tiết văn bản nhỏ. Về mặt thực nghiệm, chúng tôi báo cáo kết quả trên bốn bộ dữ liệu VQA dựa trên văn bản
theo giao thức đánh giá từ Liu et al. [10]. Hơn nữa, chúng tôi áp dụng đánh giá hướng dẫn theo dõi dựa trên GPT-4
cho 30 hình ảnh tự nhiên từ COCO [9,3] và 50 hình ảnh giàu văn bản từ
LAION [14]. Chúng tôi cũng cung cấp phân tích định tính (ví dụ: trên áp phích, ảnh chụp màn hình trang web và tweet)
để kiểm tra các kỹ năng hướng dẫn theo dõi phức tạp hơn.

Tóm lại, đóng góp của chúng tôi như sau:
• Chúng tôi thu thập 422K dữ liệu hướng dẫn theo dõi nhiễu và 16K dữ liệu hướng dẫn theo dõi chất lượng cao. Cả hai đều được chứng minh là hiệu quả trong việc tăng cường điều chỉnh hướng dẫn thị giác.
• Mô hình của chúng tôi, LLaVAR, nâng cao đáng kể khả năng hiểu văn bản trong hình ảnh trong khi cải thiện nhẹ hiệu suất của mô hình trên hình ảnh tự nhiên.
• Khả năng nâng cao cho phép mô hình của chúng tôi cung cấp tương tác đầu cuối dựa trên
các hình thức nội dung trực tuyến khác nhau kết hợp văn bản và hình ảnh.
• Chúng tôi mở nguồn dữ liệu huấn luyện và đánh giá cùng với các checkpoint mô hình.

2 Công trình Liên quan
Điều chỉnh Hướng dẫn Việc tuân theo hướng dẫn ngôn ngữ tự nhiên là khả năng chính để một đại lý
tương tác với người dùng thế giới thực. Điều chỉnh hướng dẫn bắt đầu từ việc thu thập phản hồi ưa thích của con người cho
các hướng dẫn do con người viết [1] hoặc công thức hóa huấn luyện đa tác vụ theo cách hướng dẫn theo dõi đa tác vụ [2,15]. Tuy nhiên, các mô hình điều chỉnh hướng dẫn lớn, có khả năng thường đóng nguồn và
chỉ phục vụ như API thương mại. Gần đây, Alpaca [16,17], Vicuna [18] và Baize [19] bắt đầu xu hướng
tạo ra dữ liệu hướng dẫn theo dõi chất lượng cao dựa trên LLM như GPT-3.5 / ChatGPT /
GPT-4 và tinh chỉnh mô hình LLaMA nguồn mở [20]. Tuy nhiên, việc đánh giá khả năng tuân theo
hướng dẫn vẫn là một thách thức. Trong khi GPT-4 đã chứng minh khả năng đánh giá vượt trội [21],
vẫn có một số mối quan tâm, như thiên vị đối với độ dài phản hồi [19] và thiếu tính mạnh mẽ
đối với thứ tự của các ví dụ [22]. Theo Chiang et al. [18], Liu et al. [3], Dubois et al. [23], chúng tôi sử dụng
đánh giá hướng dẫn theo dõi dựa trên GPT-4 trong công trình này.

Điều chỉnh Hướng dẫn Đa phương thức Gần đây, điều chỉnh hướng dẫn đã được mở rộng sang bối cảnh đa
phương thức, bao gồm hình ảnh, video [24,25] và âm thanh [26,27]. Đối với điều chỉnh hướng dẫn dựa trên hình ảnh,
MiniGPT-4 [28] sử dụng ChatGPT để tuyển chọn và cải thiện chú thích chi tiết cho dữ liệu hướng dẫn theo dõi chất lượng cao. LLaVA [3] tạo ra dữ liệu hướng dẫn theo dõi đa phương thức bằng cách nhắc nhở
GPT-4 chỉ văn bản với chú thích và hộp giới hạn của đối tượng. LLaMA-Adapter [29,12] sử dụng
dữ liệu COCO để căn chỉnh tính năng văn bản-hình ảnh và sử dụng dữ liệu chỉ văn bản cho điều chỉnh hướng dẫn.
mPLUG-owl [30] kết hợp hơn 1000M cặp hình ảnh-văn bản cho tiền huấn luyện và hỗn hợp 400K
dữ liệu hướng dẫn theo dõi chỉ văn bản/đa phương thức cho tinh chỉnh. Tuy nhiên, theo Liu et al.
[10], hầu hết các mô hình này gặp khó khăn trong việc hoàn thành các tác vụ yêu cầu khả năng OCR. InstructBLIP
[31] chuyển đổi 13 tác vụ thị giác-ngôn ngữ (bao gồm OCR-VQA [32]) thành định dạng hướng dẫn theo dõi
cho điều chỉnh hướng dẫn. Cream [33] áp dụng học đa tác vụ bao gồm dự đoán văn bản bị che khuất
trong hình ảnh. Một khảo sát toàn diện hơn có thể được tìm thấy trong Li et al. [34]. Trong công trình này, chúng tôi chọn
LLaVA làm đường cơ sở, là mô hình hiệu quả dữ liệu và mạnh mẽ nhất, và chứng minh
hiệu quả của quy trình đề xuất của chúng tôi.

3 Thu thập Dữ liệu
Bắt đầu từ bộ dữ liệu LAION-5B [14], mục tiêu của chúng tôi chỉ là giữ lại những hình ảnh giàu văn bản.
Xem xét rằng tài liệu thường chứa nhiều văn bản, chúng tôi đầu tiên có được bộ dữ liệu phân loại nhị phân
bằng cách kết hợp hình ảnh tự nhiên và dữ liệu tài liệu. Sau đó, chúng tôi huấn luyện một bộ phân loại hình ảnh
sử dụng backbone DiT [35]-base, được tinh chỉnh trên bộ dữ liệu RVL-CDIP
[36]. Hy vọng rằng, bộ phân loại như vậy có thể dự đoán liệu một hình ảnh có chứa văn bản hay không. Chúng tôi đầu tiên
xây dựng một tập con bằng cách chọn hình ảnh với xác suất dự đoán lớn hơn 0,8 trong khi cũng thỏa mãn
p(watermark) <0,8 và p(unsafe) <0,5. Tập con dẫn xuất có nhiễu do
hạn chế của bộ phân loại. Để làm sạch thêm dữ liệu và kết hợp đánh giá của con người,
chúng tôi lấy mẫu ngẫu nhiên 50K hình ảnh
và nhóm chúng thành 100 cụm
dựa trên các tính năng thị giác CLIP-ViT-B/32. Sau khi kiểm tra kết quả phân cụm, chúng tôi cẩn thận chọn 14 cụm
(xem Hình 10 trong Phụ lục để xem ví dụ) chứa các hình ảnh giàu văn bản đa dạng từ áp phích, bìa, quảng cáo, infographic, tài liệu giáo dục và logo. Mô hình cụm sau đó được sử dụng làm bộ lọc để thu thập
hình ảnh để xây dựng các ví dụ hướng dẫn theo dõi của chúng tôi. Để tham khảo,
chúng tôi cung cấp phân loại dựa trên CLIP [7] (xem Phụ lục A để biết chi tiết.)
để minh họa phân bố hình ảnh
cho cả hai loại dữ liệu chúng tôi thu thập
trong Hình 2. Chúng tôi tóm tắt dữ liệu thu thập của mình và dữ liệu của LLaVA trong Bảng 1.

Dữ liệu Hướng dẫn theo dõi Nhiễu Sử dụng mô hình phân cụm làm bộ lọc, chúng tôi thu thập 422K hình ảnh
đã loại bỏ trùng lặp thuộc về 14 cụm ưa thích. Để cân bằng các ví dụ từ các danh mục khác nhau, chúng tôi giữ tối đa 52K ví dụ cho một cụm. Chúng tôi chạy tất cả hình ảnh qua PaddleOCR.

Lưu ý rằng việc chạy OCR ở độ phân giải gốc (ví dụ: 1024²) có thể nhận dạng các phông chữ nhỏ mà
bộ mã hóa thị giác như CLIP ViT (6,7, độ phân giải lên đến 336²) không thể nhìn thấy. Để đảm bảo nhận dạng
các phông chữ có thể nhìn thấy trong khi duy trì độ chính xác OCR, chúng tôi thực hiện OCR trên hình ảnh sau khi giảm mẫu (cạnh ngắn được thay đổi kích thước xuống 384 pixel nếu dài hơn thế.) để trích xuất văn bản. Sau đó, dựa trên
mối quan hệ hình học giữa các từ được nhận dạng, chúng tôi hợp nhất chúng thành các đoạn văn trước khi
nối chúng lại. Vì một mô hình hướng dẫn theo dõi mạnh mẽ nên phản ứng tương tự với các hướng dẫn
có ý nghĩa tương tự, chúng tôi viết lại "Xác định bất kỳ văn bản nào hiển thị trong hình ảnh được cung cấp." thành mười hướng dẫn khác biệt (Bảng 6 trong Phụ lục). Sau đó, chúng tôi tạo ra một cuộc hội thoại một lượt cho một hình ảnh nhất định bằng cách (i) lấy mẫu ngẫu nhiên một hướng dẫn đầu vào và (ii) sử dụng văn bản đã nhận dạng làm phản hồi đầu ra mong muốn. Dữ liệu hướng dẫn theo dõi như vậy có nhiễu vì hiệu suất tương đối hạn chế của
các công cụ OCR trên các phông chữ đa dạng và nền màu sắc.

Dữ liệu Hướng dẫn theo dõi dựa trên GPT-4 So với dữ liệu hướng dẫn theo dõi chất lượng cao,
có chủ yếu hai vấn đề đối với dữ liệu nhiễu thu thập ở trên. (i) Phản hồi nên chứa các câu có tổ chức thay vì kết quả OCR thô với từ thiếu và lỗi ngữ pháp. (ii) Hướng dẫn nên đa dạng, phù hợp và cụ thể với hình ảnh nhất định thay vì đơn điệu yêu cầu tất cả văn bản hiển thị. Để giải quyết những vấn đề này, chúng tôi theo Liu et al. [3] để tạo ra dữ liệu hướng dẫn theo dõi bằng cách
nhắc nhở GPT-4 chỉ văn bản [13] với kết quả OCR và chú thích.

Thật khó khăn để nhắc nhở GPT-4 với kết quả OCR phân mảnh trong một vài từ để tạo ra các hướng dẫn không tầm thường. Vì vậy, chúng tôi cẩn thận chọn 4 trong số 14 cụm đã đề cập trước đó (cụm thứ 3, 4,
6 và 9 trong Hình 10) để thu thập hình ảnh với đủ câu có thể nhìn thấy và mạch lạc. Như
được hiển thị trong Hình 2, việc lọc như vậy làm tăng đáng kể tỷ lệ bìa sách và hình ảnh trích dẫn. Chúng tôi chọn ngẫu nhiên 4K ví dụ từ mỗi cụm (không trùng lặp với hình ảnh được sử dụng cho dữ liệu hướng dẫn theo dõi nhiễu), tạo ra tổng cộng 16K hình ảnh. Theo công trình trước đó [16,17,3], chúng tôi
cung cấp trực quan hóa các cặp động từ-danh từ cho các hướng dẫn được tạo bởi GPT-4 trong Phụ lục Hình
11. Đối với những hướng dẫn không có cặp động từ-danh từ, chúng tôi chứng minh tần suất của các đối tượng được hỏi trong Phụ lục Hình 12.

Hơn nữa, dựa trên thông điệp hệ thống và hai ví dụ few-shot trong ngữ cảnh (được hiển thị trong Phụ lục
B), chúng tôi yêu cầu GPT-4 tạo ra dữ liệu hội thoại dựa trên kết quả OCR và chú thích hình ảnh (Hình
1). Các câu hỏi được tạo ra được sử dụng làm hướng dẫn đầu vào, và câu trả lời được sử dụng làm phản hồi đầu ra.
Cụ thể, đối với một hình ảnh nhất định, chúng tôi đầu tiên cung cấp hai kết quả OCR từ EasyOCR và PaddleOCR,
có thể bổ sung cho nhau. Để minh họa các yếu tố thị giác khác ngoài văn bản trong hình ảnh, chúng tôi
cũng cung cấp kết quả của chú thích hình ảnh BLIP-2 [37]. Để ngăn chú thích tập trung vào
văn bản, chúng tôi sử dụng hộp giới hạn OCR để che văn bản và sau đó sử dụng inpainting [38] để điền lại mặt nạ
trước khi sử dụng chú thích tạo sinh. Lưu ý rằng các mô hình chú thích có thể gặp phải ảo giác [39].
Chúng tôi đề cập đến sự không đáng tin cậy này trong thông điệp hệ thống của chúng tôi và yêu cầu GPT-4 chỉ tạo ra các câu hỏi với
câu trả lời chắc chắn. Chúng tôi để việc tạo ra chú thích chi tiết hơn [40, 41] cho công việc tương lai.

4 Kiến trúc Mô hình và Huấn luyện
Kiến trúc Trong hầu hết nghiên cứu của chúng tôi, chúng tôi sử dụng cùng kiến trúc mô hình như LLaVA. Đối với bộ mã hóa thị giác V, chúng tôi sử dụng CLIP-ViT-L/14 cho độ phân giải 224² và CLIP-ViT-L/14-336 cho độ phân giải 336². Các tính năng lưới trước lớp transformer cuối cùng sau đó được chuyển đổi thành
không gian embedding từ của bộ giải mã ngôn ngữ thông qua ma trận chiếu có thể huấn luyện W. Chúng tôi sử dụng Vicuna-13B [18], một
mô hình ngôn ngữ điều chỉnh hướng dẫn dựa trên LLaMA [20], làm bộ giải mã ngôn ngữ D ngoại trừ nghiên cứu loại bỏ trong Bảng 4.

Trong Phần 5.1 và Phụ lục H, chúng tôi mở rộng kiến trúc hiện tại bằng cách thêm một bộ mã hóa thị giác độ phân giải cao (high-res) bổ sung. Bộ mã hóa high-res như vậy đưa ra hàng nghìn tính năng patch, có nghĩa là
các tính năng đã chuyển đổi và token hướng dẫn không thể vừa trong độ dài ngữ cảnh của bộ giải mã ngôn ngữ. Vì vậy, chúng tôi đề xuất thêm các mô-đun cross-attention vào bộ giải mã, tập trung vào các cặp key-value được chuyển đổi từ các tính năng patch high-res.

Huấn luyện Chúng tôi tuân theo thiết kế huấn luyện hai giai đoạn của LLaVA (Hình 3). Mục tiêu huấn luyện của
cả hai giai đoạn đều giống nhau: tạo ra phản hồi đầu ra (<res>) cho các hướng dẫn đầu vào (<ins>). Các
token hình ảnh đã chuyển đổi (<img>) được thêm trước hoặc sau hướng dẫn đầu vào đầu tiên. (i) Trong
giai đoạn tiền huấn luyện đầu tiên, chỉ ma trận chiếu W được huấn luyện để căn chỉnh tính năng. Vì
bộ giải mã D bị đóng băng, huấn luyện chấp nhận dữ liệu nhiễu. Trong giai đoạn tiền huấn luyện, chúng tôi kết hợp
595K dữ liệu tiền huấn luyện từ LLaVA với 422K dữ liệu hướng dẫn theo dõi nhiễu của chúng tôi. (ii) Cả ma trận chiếu W và bộ giải mã ngôn ngữ D đều được huấn luyện trong giai đoạn tinh chỉnh, nơi chúng tôi
hợp nhất 16K dữ liệu hướng dẫn theo dõi của chúng tôi vào 158K dữ liệu hướng dẫn theo dõi từ LLaVA làm
tập huấn luyện. Lưu ý rằng bộ mã hóa thị giác bị đóng băng trong suốt thời gian huấn luyện, điều này có thể
hạn chế hiệu suất nhận dạng văn bản, vì CLIP được huấn luyện để căn chỉnh văn bản-hình ảnh đa mục đích.
Lựa chọn tốt hơn của bộ mã hóa thị giác [42] hoặc tinh chỉnh CLIP-ViT [30] có thể hưởng lợi thêm cho khả năng hiểu thị giác, điều mà chúng tôi để dành cho công việc tương lai.

5 Thử nghiệm
Chúng tôi sử dụng cùng siêu tham số huấn luyện như LLaVA, ngoại trừ (i) Chúng tôi đặt độ dài chuỗi tối đa
thành 1024 trong tiền huấn luyện. (ii) Chúng tôi đầu tiên pad bất kỳ hình ảnh nhất định nào thành hình vuông trước khi thay đổi kích thước
nó thành kích thước đầu vào mong muốn, ngăn một số nội dung hình ảnh bị cắt trong quá trình tiền xử lý. Đối với
cả hai độ phân giải (224², 336²), chúng tôi tái tạo LLaVA gốc để so sánh công bằng. Mô hình GPT-4
được sử dụng trong công trình này đề cập đến phiên bản gpt-4-0314, trong khi chi phí để thu thập dữ liệu tinh chỉnh là
khoảng $300. Nhiệt độ được sử dụng để lấy mẫu GPT-4 được đặt thành 1.0 cho việc tạo dữ liệu huấn luyện,
0.7 cho việc tạo dữ liệu đánh giá và 0.2 cho đánh giá dựa trên GPT-4. Tất cả thử nghiệm
được chạy trên GPU NVIDIA A100 80GB. Trong quá trình đánh giá, nhiệt độ được sử dụng để lấy mẫu từ
mô hình của chúng tôi được đặt ở 0.9 cho VQA dựa trên văn bản, 0.7 cho đánh giá hướng dẫn theo dõi dựa trên GPT-4,
và 0.2 cho các minh họa định tính khác.

5.1 Phân tích Định lượng
VQA dựa trên Văn bản Theo giao thức đánh giá trong Liu et al. [10], chúng tôi đánh giá hiệu suất
của LLaVAR trên bốn bộ dữ liệu VQA dựa trên văn bản: ST-VQA [45], OCR-VQA [32], TextVQA [46] và
DocVQA [47], đại diện cho các miền khác nhau (xem Phụ lục C để biết thêm chi tiết và Phụ lục E cho
các bộ dữ liệu khác). Chúng tôi trình bày kết quả của các mô hình đường cơ sở và mô hình của chúng tôi trong Bảng 2. Lưu ý rằng
InstructBLIP bao gồm OCR-VQA trong tập huấn luyện của nó, làm cho nó không thể so sánh với cài đặt của chúng tôi. Trong
cả hai cài đặt độ phân giải và tất cả bốn bộ dữ liệu, LLaVAR cải thiện đáng kể đường cơ sở LLaVA,
chứng minh rằng dữ liệu thu thập của chúng tôi có thể mang lại cải thiện mạnh mẽ. Hơn nữa, sự cải thiện đáng kể hơn ở độ phân giải 336² so với 224², cho thấy dữ liệu thu thập
có thể mang lại cải thiện lớn hơn ở độ phân giải cao hơn. Mô hình tốt nhất của chúng tôi, LLaVAR dựa trên 336², hoạt động tốt nhất trong 3 trên 4 bộ dữ liệu được đánh giá. Lưu ý rằng đây không phải là so sánh công bằng. Một số yếu tố chính bao gồm các bộ giải mã ngôn ngữ khác nhau, độ phân giải và độ lớn của dữ liệu huấn luyện văn bản-hình ảnh. Chúng tôi cung cấp thêm thảo luận về so sánh với mPLUG-Owl và kết quả tinh chỉnh mPLUG-Owl sử dụng dữ liệu của chúng tôi trong Phụ lục F.

Nghiên cứu Loại bỏ về dữ liệu tiền huấn luyện/tinh chỉnh Chúng tôi báo cáo kết quả trong Bảng 3 và Hình 4.
(i) Dựa trên các biến thể (2) và (3), chúng tôi thấy rằng dữ liệu thu thập có thể có lợi cho giai đoạn tiền huấn luyện
(Rpretraining) và giai đoạn tinh chỉnh (Rfinetuning) riêng biệt trong khi bổ sung cho nhau trong
hầu hết các trường hợp. Quan trọng hơn, việc tăng cường giai đoạn tiền huấn luyện một mình đạt được hiệu suất tổng thể tốt thứ hai, cho thấy tiềm năng tăng cường khả năng hiểu chi tiết văn bản mà không phụ thuộc vào dữ liệu chất lượng cao được tạo bởi GPT-4. (ii) Sử dụng hình ảnh tiền huấn luyện, chúng tôi có được Cpretraining bằng cách thay thế
các hướng dẫn tiền huấn luyện bằng câu hỏi & chú thích, cùng mẫu như LLaVA. Vì biến thể (4) không tốt bằng (2), chúng tôi có thể kết luận rằng OCR có lợi hơn chú thích. (iii) Chúng tôi tiếp tục xác thực giá trị của dữ liệu được tạo bởi GPT-4 bằng cách tạo ra dữ liệu tinh chỉnh nhiễu (Nfinetuning), tương tự như dữ liệu tiền huấn luyện. Biến thể (5) đạt được độ chính xác tương đương với biến thể (3). Tuy nhiên, như được hiển thị trong
Hình 4, dữ liệu tinh chỉnh nhiễu như vậy làm tổn hại khả năng hướng dẫn theo dõi: (5) phản hồi với tất cả
văn bản đã nhận dạng trong khi bỏ qua các câu hỏi.

Nghiên cứu Loại bỏ về bộ mã hóa và độ phân giải hình ảnh Trong khi giữ nguyên dữ liệu tinh chỉnh, chúng tôi báo cáo
kết quả định lượng của việc thêm bộ mã hóa thị giác bổ sung và thay đổi dữ liệu tiền huấn luyện trong Bảng
4. (i) Lấy Pix2Struct-base làm ví dụ, chúng tôi thấy rằng việc thêm bộ mã hóa thị giác high-res bổ sung
với cross-attention thực sự cải thiện hiệu suất ((g) so với (a)), đặc biệt đạt được hiệu suất zero-shot tốt nhất trên DocVQA (15.3% độ chính xác). Tăng hiệu suất trên các bộ dữ liệu khác
tương đối hạn chế, có thể do bộ mã hóa bổ sung chúng tôi sử dụng được tiền huấn luyện trên các trang web thay vì hình ảnh tự nhiên. Mặt khác, hiệu suất của (e) và (f) vẫn kém, mà không tăng gấp đôi số lượng ví dụ high-res trong Rpretraining. Với số lượng tham số lớn hơn được khởi tạo trong
mô-đun cross-attention, chúng có thể bị underfitting khi huấn luyện trên cùng dữ liệu như ma trận chiếu W
(ví dụ: (e) so với (b)), tương tự như phát hiện trong Zeng et al. [48]. (ii) Xem xét (c) so với (a) và
(d) so với (b), trong khi hình ảnh được thay đổi kích thước thành cùng kích thước sau tiền xử lý, kết quả OCR high-res
hóa ra không nhất thiết tốt hơn phiên bản độ phân giải thấp, cho thấy khả năng của
bộ mã hóa thị giác gần như bão hòa trong (a) và (b). Để biết thêm chi tiết và kết quả về bộ mã hóa high-res bổ sung, vui lòng tham khảo Phụ lục H.

Đánh giá hướng dẫn theo dõi dựa trên GPT-4 Chúng tôi cũng báo cáo kết quả đánh giá GPT-4 về
các câu hỏi hướng dẫn theo dõi trong Bảng 5. (i) Hình ảnh Tự nhiên: 90 câu hỏi dựa trên 30 hình ảnh xác thực COCO
từ Liu et al. [3], bao gồm ba khía cạnh: hội thoại, mô tả chi tiết và
lý luận phức tạp. Điều này nhằm kiểm tra liệu dữ liệu thu thập của chúng tôi sẽ làm tổn hại, duy trì hay cải thiện
hiệu suất của mô hình trên hình ảnh tự nhiên. Trước hết, việc sử dụng độ phân giải cao hơn mang lại cải thiện
(+2.9) trong hiệu suất mô tả chi tiết, điều này trực quan. Hơn nữa, LLaVAR đạt được
sự cân bằng tốt hơn và tăng hiệu suất của cả ba khía cạnh (+1.6 trung bình). Thêm chi tiết
trong Phụ lục J. (ii) Hình ảnh Giàu Văn bản: Tương tự như thu thập dữ liệu tinh chỉnh, chúng tôi tận dụng
50 hình ảnh giàu văn bản từ LAION để thu thập các câu hỏi hướng dẫn theo dõi dựa trên kết quả OCR
và chú thích do con người chú thích. Sau đó chúng tôi thu thập phản hồi từ mô hình đã huấn luyện và sử dụng GPT-4
để tính điểm tương đối so với phản hồi GPT-4. Chúng tôi thêm điều này như một chiều bổ sung "Read" vào
Bảng 5, nơi mô hình của chúng tôi chứng minh sự cải thiện đáng kể (+3.8). Phụ lục cung cấp một
ví dụ trong Bảng 17.

5.2 Phân tích Định tính
Chúng tôi sử dụng một áp phích phim gần đây để chứng minh sự khác biệt giữa LLaVA và LLaVAR khi
tương tác với con người dựa trên hình ảnh giàu văn bản. LLaVA, mà không tăng cường khả năng hiểu văn bản
trong hình ảnh, gặp phải ảo giác khi trả lời những câu hỏi này. Một số phim được đề cập,
như "A Man Called Ove" và "The Ugly Truth", là những phim thực, cho thấy rằng bộ giải mã ngôn ngữ đang ảo giác kiến thức nội bộ của nó, trong khi bộ mã hóa thị giác không thể mã hóa thông tin hữu ích. Ngược lại, LLaVAR có thể trả lời chính xác nhiều câu hỏi được cung cấp với thông tin trung thực, rõ ràng được dựa trên hình ảnh. Tuy nhiên, một số hạn chế vẫn còn, như lỗi chính tả "ottol" (Chúng tôi cung cấp thêm thống kê liên quan đến những lỗi chính tả như vậy trong Phụ lục I).
Ngoài ra, câu hỏi cuối cùng yêu cầu thông tin không thể quan sát được từ áp phích nhất định, nơi một phản hồi mong đợi nên thể hiện sự không chắc chắn như vậy thay vì đưa ra câu trả lời cụ thể. Tuy nhiên,
không mô hình nào trả lời chính xác câu hỏi.

5.3 Nghiên cứu Trường hợp: Kích thước Phông chữ Có thể Nhận dạng
Chúng tôi đầu tiên thu thập 825 ví dụ từ OCR-VQA, có câu trả lời được trình bày trực tiếp trong hình ảnh
và có thể phát hiện bằng công cụ OCR. Bằng cách thay đổi tỷ lệ hình ảnh, chúng tôi kiểm tra hiệu suất của mô hình trong
việc trả lời những câu hỏi này trong khi chiều cao dọc của câu trả lời dao động từ 3 pixel đến 19 pixel. Chúng tôi
báo cáo kết quả trong Hình 6. (i) Đối với mô hình đường cơ sở LLaVA, nó gặp khó khăn trong việc cung cấp câu trả lời chính xác
trong tất cả các tình huống, cho cả phiên bản dựa trên 224² và 336². (ii) Mô hình LLaVAR của chúng tôi đạt được
kết quả tốt hơn đáng kể trong tất cả các tỷ lệ. Chúng tôi quan sát thấy một ngưỡng cho văn bản có thể nhận dạng cho cả phiên bản dựa trên 224² và 336² khi độ chính xác giảm mạnh khi chiều cao nhỏ hơn
7 pixel. Thú vị hơn, phiên bản dựa trên 224² đạt được hiệu suất tốt hơn trên văn bản nhỏ với chiều cao 3 pixel trong khi phiên bản 336² đạt được hiệu suất tốt hơn trên văn bản lớn với hơn
7 pixel chiều cao. Chúng tôi cho rằng giai đoạn huấn luyện bổ sung của CLIP 336² làm cho nó tốt hơn ở tỷ lệ lớn hơn
nhưng tệ hơn ở tỷ lệ nhỏ hơn.

5.4 Khả năng Hướng dẫn theo dõi Được Chuyển giao
Theo thống kê bộ dữ liệu (Bảng 1) và trực quan hóa (Hình 11), dữ liệu hướng dẫn theo dõi thu thập của chúng tôi
không đa dạng và đáng kể như LLaVA. Điều này có thể được quy cho thông tin tương đối hạn chế được cung cấp cho GPT-4 so với năm chú thích khác nhau do con người viết được sử dụng trong LLaVA.
Nội dung của hình ảnh giàu văn bản cũng ít đa dạng hơn so với hình ảnh tự nhiên. Trong khi việc sử dụng các ví dụ trong ngữ cảnh phức tạp hơn chắc chắn có thể kích thích tạo ra các ví dụ hướng dẫn theo dõi phức tạp hơn, nó cũng có thể nhân lên chi phí. Trong Phụ lục Hình 9, chúng tôi chứng minh khả năng hướng dẫn theo dõi được chuyển giao của LLaVA, có khả năng từ cả dữ liệu LLaVA và
backbone Vicuna. Trong khi dữ liệu bổ sung chúng tôi thêm chủ yếu tập trung vào việc hiểu văn bản hiển thị
trong hình ảnh, LLaVAR quản lý để xây dựng kỹ năng lý luận, viết và mô tả chi tiết của mình dựa trên
khả năng nhận dạng văn bản theo cách đầu cuối. Điều này cho phép người dùng tương tác với
nội dung trực tuyến khác nhau dựa trên ảnh chụp màn hình đơn giản.

6 Kết luận
Trong công trình này, chúng tôi nâng cao các mô hình điều chỉnh hướng dẫn thị giác về khả năng đọc văn bản
trong hình ảnh. Sử dụng hình ảnh giàu văn bản từ bộ dữ liệu LAION, chúng tôi thu thập 422K ví dụ hướng dẫn theo dõi nhiễu
chỉ sử dụng kết quả OCR và 16K dữ liệu hướng dẫn theo dõi chất lượng cao dựa trên
GPT-4 chỉ văn bản. Hai bộ dữ liệu này được tận dụng để tăng cường giai đoạn tiền huấn luyện và
giai đoạn tinh chỉnh của LLaVA tương ứng. Mô hình của chúng tôi, LLaVAR, chứng minh hiệu suất vượt trội
trong việc hiểu văn bản trong hình ảnh và tuân theo hướng dẫn của con người trên cả benchmark trước đó
và nội dung trực tuyến thế giới thực. Hơn nữa, phân tích của chúng tôi cho thấy rằng cùng dữ liệu tăng cường hiệu quả hơn với độ phân giải cao hơn. Ngoài ra, việc sử dụng các ví dụ hướng dẫn theo dõi nhiễu để tăng cường
tiền huấn luyện về cơ bản tăng cường hiệu suất mô hình mà không cần nhắc nhở GPT-4. Đối với công việc tương lai, chúng tôi
khuyến khích khám phá (i) tiêu chí lựa chọn hình ảnh tốt hơn hoặc chiến lược cân bằng lại miền [49] và
(ii) các cách hiệu quả dữ liệu và hiệu quả tính toán hơn để tăng cường các mô hình hướng dẫn theo dõi với
khả năng đa phương thức, đặc biệt trong tình huống high-res.
>>>>>>> Stashed changes
