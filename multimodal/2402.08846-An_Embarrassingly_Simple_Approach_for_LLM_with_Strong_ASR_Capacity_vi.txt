# 2402.08846.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2402.08846.pdf
# Kích thước tệp: 1000101 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Một Phương Pháp Đơn Giản Đáng Xấu Hổ cho LLM với Khả Năng ASR Mạnh
Ziyang Ma♠, Guanrou Yang♠, Yifan Yang♠,
Zhifu Gao♡, Jiaming Wang♡, Zhihao Du♡, Fan Yu♡, Qian Chen♡, Siqi Zheng♡,
Shiliang Zhang♡, Xie Chen♠†,
♠Phòng thí nghiệm Trọng điểm MoE về Trí tuệ Nhân tạo, Viện AI,
Phòng thí nghiệm X-LANCE, Đại học Giao thông Thượng Hải, Thượng Hải, Trung Quốc
♡Alibaba Group, Trung Quốc
Tóm tắt
Trong bài báo này, chúng tôi tập trung vào việc giải quyết một trong những nhiệm vụ quan trọng nhất trong lĩnh vực xử lý tiếng nói, tức là nhận dạng giọng nói tự động (ASR), với bộ mã hóa tiếng nói nền tảng và mô hình ngôn ngữ lớn (LLM). Các nghiên cứu gần đây có thiết kế phức tạp như nén đầu ra theo thời gian cho bộ mã hóa tiếng nói, giải quyết vấn đề căn chỉnh phương thức cho bộ chiếu, và sử dụng tinh chỉnh hiệu quả tham số cho LLM. Chúng tôi phát hiện rằng các thiết kế tinh tế là không cần thiết, trong khi một thành phần đơn giản đáng xấu hổ của bộ mã hóa tiếng nói có sẵn, LLM, và chỉ bộ chiếu tuyến tính có thể huấn luyện là đủ khả năng cho nhiệm vụ ASR. Cụ thể hơn, chúng tôi đánh giá và khám phá các kết hợp khác nhau của LLM và bộ mã hóa tiếng nói, dẫn đến hệ thống ASR dựa trên LLM tối ưu, mà chúng tôi gọi là SLAM-ASR1. SLAM-ASR được đề xuất cung cấp một thiết lập sạch sẽ và ít thiết kế cụ thể theo nhiệm vụ, trong đó chỉ bộ chiếu tuyến tính được huấn luyện. Theo hiểu biết của chúng tôi, SLAM-ASR đạt được hiệu suất tốt nhất trên điểm chuẩn Librispeech trong số các mô hình ASR dựa trên LLM và thậm chí vượt trội hơn mô hình âm thanh-phổ quát dựa trên LLM mới nhất được huấn luyện trên dữ liệu cặp khổng lồ. Cuối cùng, chúng tôi khám phá sự xuất hiện khả năng của ASR dựa trên LLM trong quá trình căn chỉnh phương thức. Chúng tôi hy vọng rằng nghiên cứu của chúng tôi có thể tạo điều kiện thuận lợi cho việc nghiên cứu về mở rộng LLM với khả năng đa phương thức và làm sáng tỏ cộng đồng ASR dựa trên LLM.

1 Giới thiệu
Nhận dạng giọng nói tự động (ASR) đứng như một nền tảng trong lĩnh vực công nghệ tiếng nói thông minh, cho phép máy móc hiểu và phiên âm tiếng nói của con người. Tầm quan trọng của ASR trong việc cải thiện tương tác giữa con người và máy tính cũng như khả năng tiếp cận làm cho nó trở thành một lĩnh vực nghiên cứu và ứng dụng quan trọng trong lĩnh vực xử lý tiếng nói.

†Tác giả liên hệ
1SLAM-ASR là một dự án con của SLAM-LLM, trong đó SLAM viết tắt của Speech, Language, Audio and Music. Công việc đang tiến hành và sẽ mở mã nguồn sớm.

Sự phát triển của công nghệ ASR đã được đánh dấu bởi việc áp dụng các mô hình khác nhau, mỗi mô hình đại diện cho một bước tiến về độ chính xác, hiệu quả và khả năng áp dụng (Li, 2022). Trong số này, các phương pháp có giám sát bao gồm phân loại thời gian kết nối (CTC) (Graves et al., 2006), mã hóa-giải mã dựa trên chú ý (AED) (Chan et al., 2016), bộ chuyển mạng nơ-ron hồi quy (RNN-T) (Graves et al., 2013) và các biến thể của chúng đã đóng vai trò quan trọng. Ngoài ra, việc sử dụng các phương pháp tự giám sát để tiền huấn luyện theo sau bởi các phương pháp có giám sát để tinh chỉnh cũng đã được chứng minh là hiệu quả (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022; Ma et al., 2023; Yang et al., 2023). Tuy nhiên, mỗi mô hình đều đi kèm với các thử thách và hạn chế riêng, chẳng hạn như nhu cầu về dữ liệu được gán nhãn rộng rãi, khó khăn trong việc nắm bắt các phụ thuộc ngữ cảnh tầm xa trong tiếng nói, và chi phí huấn luyện khổng lồ.

Trong bối cảnh phát triển này, sự xuất hiện của các mô hình ngôn ngữ lớn (LLM) đã giới thiệu một mô hình đột phá: khung mô hình ngôn ngữ lớn đa phương thức (MLLM) (Liu et al., 2023; Li et al., 2023a; Gao et al., 2024), dựa trên kiến trúc chỉ giải mã. Cách tiếp cận sáng tạo này khác biệt so với ASR truyền thống bằng cách sử dụng khả năng sinh khổng lồ của LLM, được tiền huấn luyện trên các tập dữ liệu khổng lồ bao gồm các ngữ cảnh ngôn ngữ đa dạng, dẫn đến ASR dựa trên LLM. Sự phát triển của mô hình ASR từ các mô hình ASR dựa trên NN trước đây sang các mô hình ASR dựa trên LLM, nhấn mạnh sự khác biệt về thiết kế loss và tiêu chí, kiến thức ngôn ngữ trước, và quy mô mô hình. Mô hình này khai thác kiến thức ngôn ngữ có sẵn, cho phép hiểu biết toàn diện hơn về ngôn ngữ, mà đến lượt nó, được dịch thành những cải tiến đáng kể trong nhiệm vụ nhận dạng giọng nói.

Kiến trúc của ASR dựa trên LLM có thể được khái niệm hóa như bao gồm ba thành phần chính: một bộ mã hóa tiếng nói, một bộ chiếu, và một LLM. Các nghiên cứu gần đây trong ASR dựa trên LLM thường mạo hiểm vào các thiết kế phức tạp, chẳng hạn như nén đầu ra theo thời gian từ bộ mã hóa tiếng nói (Wu et al., 2023; Fathullah et al., 2023), giải quyết vấn đề căn chỉnh phương thức với bộ chiếu (Tang et al., 2024; Yu et al., 2024), và tinh chỉnh LLM một phần hoặc toàn bộ (Wu et al., 2023; Li et al., 2023b; Tang et al., 2024; Wang et al., 2023). Mặc dù có những nỗ lực này, kết quả không phải lúc nào cũng đáp ứng kỳ vọng, cho thấy sự không khớp tiềm ẩn giữa độ phức tạp của thiết kế và hiệu quả của các nhiệm vụ nhận dạng giọng nói thực tế. Quan sát này đã dẫn đến một nhận thức quan trọng trong nghiên cứu của chúng tôi: bản chất của một hệ thống ASR dựa trên LLM hiệu quả nằm ở sự kết hợp của một bộ mã hóa tiếng nói mạnh mẽ và một LLM phù hợp, và sau đó, đáng chú ý nhất, một bộ chiếu tuyến tính có thể huấn luyện duy nhất là đủ để căn chỉnh giữa các phương thức.

Các phát hiện của chúng tôi thách thức quan niệm phổ biến rằng sự phức tạp tương đương với sự vượt trội trong thiết kế hệ thống ASR dựa trên LLM.

Trong nghiên cứu này, chúng tôi đầu tiên đánh giá hiệu suất nhiệm vụ nhận dạng giọng nói tự động với các kết hợp khác nhau của các bộ mã hóa tiếng nói nổi tiếng và các mô hình ngôn ngữ lớn mới nhất được phát hành. Các thí nghiệm cho thấy rằng LLM với tinh chỉnh có giám sát (SFT, hay còn gọi là mô hình chat) hoạt động tốt hơn so với LLM tiền huấn luyện thô cho nhiệm vụ ASR, trong khi các bộ mã hóa tiếng nói được tinh chỉnh với dữ liệu hạn chế từ các mô hình tự giám sát vượt trội hơn các bộ mã hóa ASR nền tảng có giám sát. Dựa trên những hiểu biết này, chúng tôi đề xuất SLAM-ASR, trong đó chỉ một bộ chiếu tuyến tính được huấn luyện để thực hiện nhiệm vụ ASR. SLAM-ASR chỉ yêu cầu 4 GPU trong 4 giờ huấn luyện để đạt được hiệu suất tối tiến trên tập dữ liệu Librispeech (Panayotov et al., 2015), so với các mô hình ASR dựa trên LLM khác và một loạt các mô hình ASR dựa trên NN có hiệu suất tốt nhất trước đây. Bên cạnh đó, nghiên cứu của chúng tôi bắt đầu khám phá sâu sắc về khả năng của các mô hình ASR dựa trên LLM. Thú vị thay, chúng tôi quan sát thấy hiện tượng xuất hiện khả năng trong quá trình huấn luyện ASR dựa trên LLM. Điểm chuẩn và khám phá thực nghiệm cho thấy cách chúng tôi thu hoạch kết quả thú vị từng bước với một thiết lập sạch sẽ và ít thiết kế cụ thể theo nhiệm vụ.

2 Nhận Dạng Giọng Nói Gặp Mô Hình Ngôn Ngữ Lớn

2.1 ASR Dựa Trên NN Trước Đây
Các hệ thống ASR dựa trên NN trước đây được thiết kế để căn chỉnh tín hiệu tiếng nói với chuỗi nhãn một cách chính xác. Như thể hiện trong bảng 1, các mô hình khác nhau

Bảng 1: Mô hình ASR với các mô hình đại diện. QF có nghĩa là các biến thể của Q-Former (Li et al., 2023a). Cả QF và Linear đều là các mô-đun chiếu được sử dụng để căn chỉnh bộ mã hóa tiếng nói và LLM.

Mô hình | Loss | Có thể học
---|---|---
**ASR Dựa Trên NN Trước Đây** | |
Quartznet (Kriman et al., 2020) | CTC | Tất cả
Whisper (Radford et al., 2023) | AED | Tất cả
Branchformer (Peng et al., 2022) | CTC + AED | Tất cả
Conformer (Gulati et al., 2020) | RNN-T | Tất cả
Zipformer (Yao et al., 2024) | Pruned RNN-T | Tất cả
Paraformer (Gao et al., 2022) | CIF | Tất cả
**ASR Dựa Trên LLM** | |
LauraGPT (Wang et al., 2023) | Decoder-Only, Cross Entropy | Tất cả
SpeechGPT (Zhang et al., 2023) | | LLM
Li et al.'s (2023b) | | Encoder, LLM Adapter
SpeechLLaMA (Wu et al., 2023) | | Encoder, LLM LoRA
Qwen-Audio (Chu et al., 2023) | | Encoder, Linear
SALMONN (Tang et al., 2024) | | QF, LLM LoRA
Fathullah et al.'s (2023) | | Linear, LLM LoRA
Yu et al.'s (2024) | | QF
SLAM-ASR | | Linear

được thực hiện với một loạt các mô hình đại diện. Quartznet (Kriman et al., 2020) tận dụng CTC (Graves et al., 2006), công nghệ E2E đầu tiên được áp dụng rộng rãi trong ASR, nhưng vẫn đối mặt với những hạn chế về hiệu suất do giả định độc lập khung hình của nó. Whisper (Radford et al., 2023) sử dụng dữ liệu cặp tiếng nói-văn bản khổng lồ để huấn luyện kiến trúc mã hóa-giải mã dựa trên chú ý (Chan et al., 2016) (AED, hay còn gọi là LAS trong ASR), trao quyền cho mô hình khả năng nhận dạng và dịch tiếng nói bằng nhiều ngôn ngữ. Branchformer (Peng et al., 2022) sử dụng kiến trúc lai kết hợp CTC và AED (Chan et al., 2016), việc tích hợp cơ chế chú ý giải quyết hạn chế này bằng cách giới thiệu mô hình hóa ngôn ngữ ngầm qua các khung tiếng nói. Conformer (Gulati et al., 2020) sử dụng bộ chuyển mạng nơ-ron (Graves et al., 2013), loại bỏ trực tiếp giả định độc lập khung hình bằng cách kết hợp một bộ giải mã nhãn và một mạng kết hợp, dẫn đến hiệu suất vượt trội. Zipformer (Yao et al., 2024) áp dụng Pruned RNN-T (Kuang et al., 2022), là một biến thể tiết kiệm bộ nhớ của loss transducer, sử dụng các đường dẫn được cắt tỉa với xác suất hậu nghiệm nhỏ. Paraformer (Gao et al., 2022) sử dụng Continuous Integrate-and-Fire (CIF) (Dong and Xu, 2020), cung cấp cơ chế căn chỉnh mềm và đơn điệu, ước tính số lượng token và tạo ra các biến ẩn.

2.2 ASR Dựa Trên LLM Hiện Tại
Các mô hình ASR dựa trên LLM áp dụng kiến trúc chỉ giải mã dựa trên LLM được tiền huấn luyện như một mô hình mới. LauraGPT (Wang et al., 2023) kết nối một bộ mã hóa Conformer được sửa đổi (Gulati et al., 2020) với Qwen-2B (Bai et al., 2023) để huấn luyện từ đầu đến cuối cho nhiều nhiệm vụ tiếng nói và âm thanh, với tinh chỉnh tham số đầy đủ được thực hiện. SpeechGPT (Zhang et al., 2023) rời rạc hóa các token tiếng nói với HuBERT (Hsu et al., 2021) và tinh chỉnh LLaMA-13B (Touvron et al., 2023a) với nhiều giai đoạn. Mặc dù cả hai mô hình đều tốn kém về mặt tính toán, hiệu suất của chúng vẫn bị hạn chế. (Li et al., 2023b) và (Wu et al., 2023) đề xuất sử dụng Gated-XATT-FFN được chèn (Alayrac et al., 2022) hoặc LoRA nhánh bên (Hu et al., 2022) để tinh chỉnh LLM một phần cho việc thực hiện nhiệm vụ ASR, cùng với một bộ mã hóa tiếng nói có thể huấn luyện. Qwen-Audio (Chu et al., 2023) là một mô hình âm thanh-phổ quát, sử dụng dữ liệu cặp khổng lồ để tinh chỉnh bộ mã hóa được khởi tạo từ mô hình Whisper-large (Radford et al., 2023), được tối ưu hóa bằng cách sử dụng loss của đầu ra Qwen-7B đông lạnh (Bai et al., 2023) cho lan truyền ngược. Tất cả các mô hình này đều yêu cầu tinh chỉnh bộ mã hóa. SALMONN (Tang et al., 2024) sử dụng Whisper-large (Radford et al., 2023) và BEATs (Chen et al., 2023) để mã hóa tiếng nói và âm thanh, tương ứng, cùng với Q-Former cấp cửa sổ (win-QF), có thể thực hiện nhiều nhiệm vụ âm thanh. (Fathullah et al., 2023) kết nối Conformer với LLaMA-7B để thực hiện thành công ASR đơn ngữ và đa ngữ. Các mô hình này yêu cầu sử dụng LoRA để hiệu quả. Công trình gần gũi nhất là (Yu et al., 2024), đạt được kết quả tốt trên ASR chỉ sử dụng Q-Former cấp đoạn (sef-QF) tương tự như win-QF làm bộ chiếu. Chiến lược huấn luyện nối ngẫu nhiên được thiết kế để giảm thiểu vấn đề tự nhiên của Whisper (Radford et al., 2023) yêu cầu đầu vào tiếng nói 30 giây.

2.3 Phương Pháp Đề Xuất
Như thể hiện trong Hình 1, một khung làm việc đơn giản đáng xấu hổ được đề xuất để huấn luyện mô hình SLAM-ASR. Đối với mỗi mẫu, với tiếng nói XS, bản phiên âm tương ứng XT, và lời nhắc XP, chúng tôi đầu tiên chuyển đổi tiếng nói thành các đặc trưng tiếng nói thông qua bộ mã hóa tiếng nói, có thể được viết như:

HS = Encoder(XS), (1)

trong đó HS = [hS1, ···, hST] có T khung trong chiều thời gian. Do tính thưa thớt của biểu diễn tiếng nói, chuỗi đặc trưng tiếng nói HS vẫn rất dài để LLM xử lý2, chúng tôi giảm mẫu tiếng nói với một bộ giảm mẫu. Cụ thể hơn, chúng tôi nối k khung liên tiếp trong chiều đặc trưng để thực hiện giảm mẫu k lần, dẫn đến ZS = [zS1, ···, zSN], trong đó

zSi = hSk*i ⊕ hSk*i+1 ⊕ ··· ⊕ hSk*i+k-1, (2)

và

N = T//k. (3)

Tiếp theo, một bộ chiếu được áp dụng để biến đổi các đặc trưng tiếng nói ZS thành ES với cùng chiều như nhúng đầu vào LLM. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng một lớp ẩn duy nhất theo sau bởi một kích hoạt ReLU và một lớp hồi quy làm bộ chiếu, được ký hiệu là:

ES = Linear(ReLU(Linear(ZS))). (4)

Cuối cùng, chúng tôi đưa nhúng tiếng nói ES, nhúng phiên âm ET, và nhúng lời nhắc EP vào mẫu để tạo thành đầu vào cuối cùng E của LLM, được ký hiệu là:

ET = Tokenizer(XT), (5)
EP = Tokenizer(XP), (6)

E = {
Template(ES, EP, ET) nếu huấn luyện,
Template(ES, EP) nếu suy luận,
(7)

trong đó mẫu được chi tiết trong Phần 3.3 và Phần 3.4.

--- TRANG 4 ---
3 Thiết Lập Thí Nghiệm
Quy trình thí nghiệm của chúng tôi tuân theo nguyên tắc KISS (Keep It Simple, Stupid!) để điều tra các yếu tố quan trọng nhất cho ASR dựa trên LLM.

3.1 Mô Hình và Mô-đun

3.1.1 Bộ Mã Hóa Tiếng Nói
Hai loại bộ mã hóa tiếng nói được điều tra trong bài báo này, đó là các bộ mã hóa tiếng nói có giám sát được huấn luyện trên dữ liệu cặp tiếng nói-văn bản khổng lồ và các bộ mã hóa tiếng nói tự giám sát được huấn luyện trên dữ liệu tiếng nói không nhãn quy mô lớn. Đối với các mô hình nền tảng có giám sát, chúng tôi chủ yếu khảo sát họ mô hình Whisper nổi tiếng (Radford et al., 2023)3 từ tiny đến large, bao gồm whisper-tiny, whisper-base, whisper-small, whisper-medium và whisper-large-v2. Chúng tôi loại bỏ bộ giải mã của mỗi mô hình Whisper và chỉ sử dụng bộ mã hóa như một bộ trích xuất đặc trưng. Chúng tôi cũng điều tra Qwen-Audio Encoder4, bộ mã hóa được tinh chỉnh từ checkpoint whisper-large-v2 trên dữ liệu tiếng nói, âm thanh và nhạc quy mô lớn, được phát hành cùng với mô hình Qwen-Audio (Chu et al., 2023). Đối với các mô hình tự giám sát, chúng tôi điều tra HuBERT5 và WavLM6 ở các quy mô khác nhau, hoặc tiền huấn luyện thô hoặc được tinh chỉnh thêm. Đối với các mô hình kích thước base, cả HuBERT (Hsu et al., 2021) và WavLM (Chen et al., 2022) đều thực hiện tiền huấn luyện tự giám sát trên tập dữ liệu LibriSpeech (Panayotov et al., 2015) với 960 giờ. Đối với các mô hình kích thước large, HuBERT được huấn luyện trên tập dữ liệu LibriLight (Kahn et al., 2020) với 60.000 giờ, trong khi WavLM được huấn luyện trên dữ liệu lớn hơn nhiều 94.000 giờ bao gồm LibriLight (Kahn et al., 2020), VoxPopuli (Wang et al., 2021), và GigaSpeech (Chen et al., 2021). Hơn nữa, HuBERT cung cấp các mô hình tiền huấn luyện kích thước X-Large, là bộ mã hóa tiếng nói tự giám sát lớn nhất có sẵn công khai. Tất cả các mô hình được đề cập trong phần này đều được lấy từ các kho lưu trữ chính thức của họ. Tham khảo Phần 4.3 để biết chi tiết về tham số và kích thước ẩn của mỗi mô hình cụ thể.

3.1.2 LLM
Hai loại mô hình ngôn ngữ lớn được điều tra trong bài báo này, đó là LLM tiền huấn luyện thô

3https://github.com/openai/whisper
4https://github.com/QwenLM/Qwen-Audio
5https://github.com/facebookresearch/fairseq/tree/main/examples/hubert
6https://github.com/microsoft/unilm/tree/master/unilm

không có tinh chỉnh có giám sát và LLM chat với SFT (cùng với RLHF nếu được thực hiện). Đối với các LLM tiền huấn luyện, chúng tôi thử TinyLLaMA (Zhang et al., 2024)7 cỡ 1B và LLaMA-2 (Touvron et al., 2023b)8 cỡ 7B. Đối với các LLM chat, TinyLLaMA-Chat9 cỡ 1B, Phi-210 cỡ 2B, LLaMA-2-Chat11 và Vicuna (Chiang et al., 2023)12 cỡ 7B được xem xét. Tham khảo Phần 4.2 để biết chi tiết về tham số và kích thước ẩn của mỗi LLM cụ thể.

3.1.3 Bộ Chiếu
Bộ chiếu có thể được xem như một bộ điều hợp cho các phương thức khác để thực hiện căn chỉnh với LLM. Trong tất cả các thí nghiệm của chúng tôi, đầu ra của bộ mã hóa tiếng nói là 50Hz, và tỷ lệ giảm mẫu k = 5, dẫn đến các đặc trưng tiếng nói đầu vào ES của mô hình lớn là 10Hz. Chiều lớp ẩn được đặt là 2048, trong khi chiều của đầu ra bộ mã hóa tiếng nói HS và chiều đầu vào LLM thay đổi tùy thuộc vào mô hình được sử dụng, tương ứng.

3.2 Tập Dữ Liệu
Để đánh giá khả năng của các mô hình ASR dựa trên LLM, chúng tôi sử dụng điểm chuẩn được sử dụng rộng rãi nhất cho nhiệm vụ ASR, điểm chuẩn Librispeech chuẩn với 960 giờ dữ liệu huấn luyện mà không có bất kỳ tăng cường dữ liệu hoặc ghép nối nào. Chúng tôi sử dụng tập con dev-other làm tập xác thực và test-clean/test-other làm tập kiểm tra, mỗi tập chứa 10 giờ tiếng nói.

3.3 Chi Tiết Huấn Luyện
Trong quá trình huấn luyện, dữ liệu được tổ chức theo định dạng sau: "USER: <S> <P> ASSISTANT: <T>", trong đó <S> đại diện cho nhúng tiếng nói, <P> đại diện cho lời nhắc, và <T> đại diện cho văn bản phiên âm tương ứng. Chúng tôi chỉ tính toán loss trên <T>, như là thực hành thông thường. Đối với chiến lược tối ưu hóa, chúng tôi sử dụng AdamW (Loshchilov and Hutter, 2019) với tốc độ học tối đa 1×10-4 mà không có giảm trọng số. Đối với bộ lập lịch tốc độ học, chúng tôi thực hiện khởi động ở 1.000 bước đầu tiên

7https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4
8https://huggingface.co/meta-llama/Llama-2-7b-hf
9https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
10https://huggingface.co/microsoft/phi-2
11https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
12https://huggingface.co/lmsys/vicuna-7b-v1.5

và sau đó giữ tốc độ học tối đa cho việc huấn luyện suốt thời gian. Số bước huấn luyện tối đa được đặt là 100.000, nhưng chúng tôi sẽ dừng sớm nếu loss trên tập xác thực không giảm. Đối với nhúng âm thanh được cung cấp bởi họ mô hình Whisper, chúng tôi phát hiện rằng việc không đệm sẽ ảnh hưởng đến hiệu suất. Kết quả là, chúng tôi đệm tiếng nói đến 30 giây cho tất cả các mô hình Whisper và kích thước batch được đặt là 4. Đối với các mô hình khác, độ dài của âm thanh đầu vào vẫn phù hợp với độ dài ban đầu trong chiều thời gian, và batch được đặt là 6, cải thiện đáng kể hiệu quả huấn luyện và suy luận, so với các mô hình Whisper.

3.4 Chi Tiết Suy Luận
Trong quá trình suy luận, dữ liệu được tổ chức theo định dạng sau: "USER: <S> <P> ASSISTANT:", trong đó các mô hình ngôn ngữ lớn trả lời một cách tự hồi quy. Thông thường, LLM sử dụng các thuật toán lấy mẫu để tạo ra các đầu ra văn bản đa dạng. Vì nhận dạng giọng nói là một nhiệm vụ chuỗi-sang-chuỗi với đầu ra xác định, chúng tôi sử dụng tìm kiếm chùm với beam = 4 để đưa ra giả thuyết tương ứng với tiếng nói.

4 Khám Phá
Trong phần này, chúng tôi đầu tiên đưa ra một điểm chuẩn cơ bản về các kết hợp của các LLM và bộ mã hóa tiếng nói khác nhau và phát hiện rằng các mô hình chat hoạt động tốt hơn so với LLM tiền huấn luyện thô trên nhiệm vụ ASR. Tiếp theo, chúng tôi đánh giá các mô hình chat khác nhau và phát hiện Vicuna là một LLM phù hợp và HuBERT được tinh chỉnh là một bộ mã hóa tiếng nói mạnh mẽ để thực hiện nhiệm vụ ASR. Cuối cùng, chúng tôi đề xuất SLAM-ASR, và so sánh SLAM-ASR với các mô hình ASR dựa trên NN tối tiến trước đây và các mô hình ASR dựa trên LLM có hiệu suất tốt nhất mới nhất.

4.1 Điểm Chuẩn Cơ Bản
Để bắt đầu, chúng tôi đánh giá các mô hình Whisper với kích thước khác nhau trên LLM tiền huấn luyện và LLM tinh chỉnh có giám sát. Chúng tôi chọn TinyLLaMA cỡ 1B và LLaMA-2 cỡ 7B để đưa ra đánh giá sơ bộ. Như thể hiện trong Bảng 2, hiệu suất của nhiệm vụ ASR được cải thiện khi kích thước tham số của bộ mã hóa tiếng nói tăng lên, nhưng sự cải thiện có lợi ích biên giảm dần cho họ mô hình Whisper. Đối với việc lựa chọn LLM, các mô hình chat hoạt động tốt hơn so với các mô hình tiền huấn luyện, bất kể kích thước. Một giải thích có thể là các mô hình chat coi nhúng tiếng nói như một dạng "ngôn ngữ" và thực hiện một nhiệm vụ dịch máy, được kích hoạt trong quá trình SFT.

4.2 Khám Phá trong LLM
Tiếp theo, chúng tôi cố định bộ mã hóa tiếng nói là Whisper-large và sau đó khám phá một mô hình ngôn ngữ lớn tốt hơn. Như thể hiện trong Bảng 3, mô hình chat Phi-2 với 2,78B tham số có tỷ lệ lỗi từ tương đương với LLaMA-2 với 6,74B tham số trên test-other. Vicuna là một LLM chat mã nguồn mở được tinh chỉnh trên dữ liệu hội thoại do người dùng chia sẻ được thu thập từ ShareGPT13, sử dụng LLaMA như một LLM tiền huấn luyện. Mô hình ASR dựa trên LLM cho thấy kết quả tốt hơn khi Vicuna được sử dụng làm LLM so với LLaMA-2 và LLaMA-2-Chat. Tất cả các kết quả thí nghiệm trên xác nhận khả năng của các mô hình chat trên hệ thống ASR dựa trên LLM.

4.3 Khám Phá về Bộ Mã Hóa Tiếng Nói
Hơn nữa, chúng tôi cố định Vicuna làm LLM và đánh giá hiệu suất của các bộ mã hóa tiếng nói khác nhau. Đối với các bộ mã hóa tiếng nói có giám sát, hiệu suất được cải thiện dần dần khi kích thước tham số của bộ mã hóa tiếng nói tăng lên, điều này phù hợp với xu hướng trên các mô hình dòng LLaMA. Khi Qwen-Audio Encoder được sử dụng làm bộ mã hóa tiếng nói, hiệu suất ASR được cải thiện thêm so với Whisper-large, điều này cho thấy rằng bộ mã hóa được tinh chỉnh trên LLM khác (tức là Qwen-7B) với lan truyền gradient ngược, có thể được chuyển giao sang LLM khác (tức là Vicuna-7B), và duy trì một mức độ hiệu suất nhất định.

Đối với các bộ mã hóa tiếng nói học tự giám sát, HuBERT Base và WavLM Base có khoảng 95M tham số, với 768 chiều kích thước ẩn. Trong cấu hình này, hiệu suất ASR tương tự so với Whisper-small cùng quy mô, nơi học tự giám sát không đóng vai trò gì. Khi mở rộng các bộ mã hóa tiếng nói tự giám sát lên 0,3B, WavLM Large vượt trội hơn tất cả các bộ mã hóa tiếng nói có giám sát được liệt kê, bao gồm Whisper-medium với 0,3B tham số và Whisper-large với 0,6B tham số, trong khi sự cải thiện từ HuBERT Base sang HuBERT Large không rõ ràng. Tuy nhiên, nếu bộ mã hóa HuBERT Large được tinh chỉnh trước trên dữ liệu huấn luyện Librispeech 960 giờ, và được sử dụng làm bộ mã hóa tiếng nói để huấn luyện bộ chiếu trong mô hình ASR dựa trên LLM của chúng tôi, mô hình đạt được WER 2,30% trên test-clean và 4,53% trên test-other, vượt quá hiệu suất với WavLM Large làm bộ mã hóa tiếng nói. Hơn nữa, chúng tôi sử dụng HuBERT X-Large làm bộ mã hóa tiếng nói, mở rộng bộ mã hóa tiếng nói lên 1B tham số. Với HuBERT X-Large được tinh chỉnh Librispeech-960, mô hình ASR dựa trên LLM của chúng tôi có tỷ lệ lỗi từ 1,94% trên test-clean và 3,81% trên test-other, đạt được giảm WER tương đối 24,8% và 41,1% so với mô hình với Whisper-large làm bộ mã hóa tiếng nói, tương ứng. Ngoài ra, lấy cảm hứng từ Fuyu (Bavishi et al., 2024), chúng tôi cũng thử loại bỏ bộ mã hóa tiếng nói và trực tiếp đưa các đặc trưng FBank 80 chiều vào bộ chiếu, điều này thua xa việc sử dụng các bộ mã hóa tiếng nói được huấn luyện tốt, như thể hiện trong hàng đầu tiên của Bảng 4. Kết quả thí nghiệm cho thấy hiệu quả của việc sử dụng các bộ mã hóa tiếng nói tự giám sát và mở rộng kích thước của các bộ mã hóa tiếng nói.

Bảng 6: So sánh với các mô hình dựa trên NN trước đây. Mô hình chuyên biệt có nghĩa là các mô hình được huấn luyện trên Librispeech-960, và LM in-domain có nghĩa là các mô hình ngôn ngữ được huấn luyện trên tập dữ liệu mô hình ngôn ngữ LibriSpeech cùng với các phiên âm LibriSpeech-960. Mô hình phổ quát có nghĩa là các mô hình tổng quát được huấn luyện trên dữ liệu cặp khổng lồ.

Mô hình | WER(%) ↓
---|---
 | test-clean | test-other
**Mô hình chuyên biệt** | |
ContextNet-large (Han et al., 2020) | 2.1 | 4.6
+ in-domain LM | 1.9 | 4.1
Conformer-large (Gulati et al., 2020) | 2.1 | 4.3
+ in-domain LM | 1.9 | 3.9
Branchformer-large (Peng et al., 2022) | 2.4 | 5.5
+ in-domain LM | 2.1 | 4.5
Zipformer-large (Yao et al., 2024) | 2.0 | 4.4
+ in-domain LM | 1.9 | 3.9
**Mô hình phổ quát** | |
Whisper-large-v2 (Radford et al., 2023) | 2.7 | 5.2
OWSM-v3.1 (Peng et al., 2024) | 2.4 | 5.0
**Của chúng tôi** | |
SLAM-ASR | 1.9 | 3.8

4.4 SLAM-ASR
Ở đây chúng tôi giới thiệu SLAM-ASR, một mô hình ASR dựa trên LLM với HuBERT X-Large làm bộ mã hóa tiếng nói và Vicuna-7B làm LLM, với chỉ bộ chiếu tuyến tính có thể huấn luyện, được triển khai dựa trên khung SLAM-LLM. Như thể hiện trong Bảng 5, chúng tôi trưng bày các mô hình ASR dựa trên LLM khác nhau từ nghiên cứu đồng thời, hoặc cụ thể cho ASR hoặc âm thanh-phổ quát. Một nghiên cứu đương đại (Yu et al., 2024) sử dụng Whisper-large làm bộ mã hóa tiếng nói và Vicuna-13B làm LLM. Q-Former cấp đoạn (seg-QF) được sử dụng làm bộ chiếu để giải quyết tính tương thích giữa chuỗi tiếng nói và LLM. So với phương pháp của họ, SLAM-ASR của chúng tôi mang lại giảm WER tương đối 17,4/26,9% trên các tập con test-clean/other được huấn luyện với cùng 960 giờ dữ liệu Librispeech. Khi mô hình của họ được huấn luyện trên lượng tiếng nói lớn hơn trên 4.000 giờ, SLAM-ASR được đề xuất vẫn hoạt động tốt hơn. Chúng tôi cũng so sánh SLAM-ASR với các mô hình âm thanh-phổ quát dựa trên LLM mới nhất, SALMONN (Tang et al., 2024) và Qwen-Audio (Chu et al., 2023), cung cấp kết quả trên điểm chuẩn Librispeech. So với các MLLM dựa trên âm thanh này, SLAM-ASR vẫn đạt được hiệu suất tốt hơn mặc dù có sự chênh lệch lớn trong dữ liệu huấn luyện.

Chúng tôi cũng so sánh SLAM-ASR với các mô hình dựa trên NN tối tiến trước đây. Đối với các mô hình chuyên biệt được huấn luyện trên Librispeech-960, chúng tôi so sánh SLAM-ASR với ContextNet (Han et al., 2020), Conformer (Gulati et al., 2020), Branchformer (Peng et al., 2022), và Zipformer (Yao et al., 2024). Tất cả các mô hình đều có kích thước large, và các kết quả từ các bài báo của họ được trình bày. Các mô hình ASR này sử dụng kỹ thuật hệ thống phức tạp, bao gồm SpecAugment và nhiễu loạn tốc độ cho tăng cường dữ liệu, và kỹ thuật trung bình di động theo cấp số nhân cho trung bình mô hình. Để cải thiện hiệu suất hơn nữa, các mô hình ngôn ngữ in-domain được huấn luyện trên tập dữ liệu mô hình ngôn ngữ LibriSpeech cùng với các phiên âm LibriSpeech-960 được thêm vào để kết hợp hoặc chấm điểm lại. SLAM-ASR đạt được hiệu suất ASR tương đương (test-clean) hoặc tốt hơn (test-other) so với các mô hình có hiệu suất tốt nhất mà không sử dụng kỹ thuật hệ thống phức tạp. So với các mô hình tổng quát được huấn luyện trên dữ liệu khổng lồ, SLAM-ASR vượt trội hơn Whisper-large-v2 (Radford et al., 2023) trong công nghiệp, và OWSM-v3.1 (Peng et al., 2024) trong cộng đồng học thuật. Các kết quả thí nghiệm chứng minh tính vượt trội của SLAM-ASR và tiềm năng lớn của ASR dựa trên LLM.

5 Xuất Hiện Khả Năng
Chúng tôi quan sát rằng có sự xuất hiện khả năng cho ASR dựa trên LLM trong quá trình huấn luyện trong vòng 1 epoch (khoảng 12k bước). Cụ thể, độ chính xác của dự đoán token tiếp theo tăng nhanh chóng ở đầu quá trình huấn luyện, sau đó bắt đầu tăng chậm, và sau đó "tăng vọt" tại một thời điểm nào đó, như thể "khả năng đột nhiên được học".

Hình 2 cho thấy độ chính xác huấn luyện của dự đoán token tiếp theo với các bước huấn luyện, trong đó LLM được giữ là Vicuna-7B và các bộ mã hóa tiếng nói thay đổi. Có thể thấy từ hình rằng các bộ mã hóa tiếng nói với hiệu suất tốt hơn, trong trường hợp này, Whisper Large và WavLM Large, sẽ xuất hiện sớm hơn. Một giải thích có thể là nhiệm vụ của chúng tôi về bản chất là căn chỉnh biểu diễn tiếng nói với LLM, trong khi một bộ mã hóa tiếng nói mạnh mẽ có thể cung cấp các biểu diễn dễ dàng hơn cho bộ chiếu căn chỉnh với LLM.

Hình 3: Độ chính xác huấn luyện của dự đoán token tiếp theo với các bước huấn luyện. Bộ mã hóa tiếng nói được cố định với Whisper-large-v2 và các đường cong màu khác nhau đại diện cho các LLM khác nhau.

Chúng tôi giữ bộ mã hóa tiếng nói là Whisper Large, thay đổi các LLM khác nhau, và vẽ độ chính xác huấn luyện, như thể hiện trong Hình 3. Các thí nghiệm cho thấy rằng các mô hình ASR dựa trên LLM với LLM nhỏ hơn như TinyLLaMA-Chat và Phi-2 xuất hiện sớm hơn, tuy nhiên, chúng không hiệu quả bằng các LLM lớn hơn như LLaMA-2-7B-Chat và Vicuna-7B. Điều này cho thấy rằng các mô hình ngôn ngữ lớn hơn khó căn chỉnh với các đặc trưng tiếng nói hơn so với các mô hình nhỏ hơn.

Chúng tôi cũng khám phá việc đông lạnh bộ mã hóa tiếng nói có ảnh hưởng đến sự xuất hiện khả năng hay không. Chúng tôi lấy TinyLLaMA-1.1B-Chat làm LLM và đông lạnh hoặc tinh chỉnh bộ mã hóa tiếng nói, tương ứng. Như thể hiện trong Hình 4, cả hai mô hình đều nhanh chóng tăng lên khoảng 40% độ chính xác huấn luyện trong quá trình huấn luyện đầu. Khi bộ mã hóa tiếng nói được đông lạnh, mô hình hoàn thành căn chỉnh đa phương thức trong 1k bước, trong khi nút thời gian đến 25K bước khi bộ mã hóa tiếng nói có thể huấn luyện, muộn hơn nhiều. Bảng 7 so sánh WER của các hệ thống ASR dựa trên LLM với việc đông lạnh và tinh chỉnh bộ mã hóa tiếng nói, trong đó cái trước hoạt động tốt hơn nhiều. Điều này cho thấy rằng 1k giờ tiếng nói vẫn chưa đủ để huấn luyện bộ mã hóa tiếng nói dựa trên LLM cụ thể theo nhiệm vụ, thay vào đó, việc đông lạnh bộ mã hóa tiếng nói và chú ý đến căn chỉnh phương thức là lựa chọn tốt hơn.

Hình 4: Độ chính xác huấn luyện của dự đoán token tiếp theo với các bước huấn luyện. Bộ mã hóa tiếng nói được cố định với Whisper-large-v2 và LLM được cố định với TinyLLaMA-1.1B-Chat. Các đường cong màu khác nhau đại diện cho việc đông lạnh hoặc tinh chỉnh bộ mã hóa tiếng nói.

Bảng 7: Kết quả WER của việc đông lạnh hoặc tinh chỉnh bộ mã hóa tiếng nói được thể hiện trong Hình 4 trên các tập con test-clean và test-other của Librispeech.

Đông lạnh bộ mã hóa tiếng nói | WER(%) ↓
---|---
 | test-clean | test-other
✓ | 4.33 | 8.62
✗ | 12.79 | 22.83

6 Kết Luận
Trong bài báo này, chúng tôi khám phá một cách có hệ thống các hệ thống ASR dựa trên LLM với một khung làm việc sạch sẽ, trong đó chỉ bộ chiếu tuyến tính có thể huấn luyện được sử dụng để căn chỉnh bộ mã hóa tiếng nói và LLM. Nghiên cứu chỉ ra rằng các LLM trải qua tinh chỉnh có giám sát, thể hiện hiệu suất và độ bền vững được cải thiện. Hơn nữa, các bộ mã hóa tiếng nói được tinh chỉnh từ các mô hình tự giám sát thể hiện khả năng vượt trội. Mô hình SLAM-ASR được đề xuất và vượt trội hơn các mô hình ASR dựa trên LLM khác và các mô hình ASR dựa trên NN trước đây trên điểm chuẩn Librispeech. Các thí nghiệm khám phá cho thấy có sự xuất hiện khả năng trong các hệ thống ASR dựa trên LLM. Chúng tôi mong muốn nghiên cứu của chúng tôi sẽ phục vụ như một bước tiến trong việc khám phá ASR dựa trên LLM, cung cấp hỗ trợ và hiểu biết sâu sắc cho cộng đồng rộng lớn hơn.

--- TRANG 9 ---
Lời Cảm Ơn
Chúng tôi cảm ơn Changli Tang và Wenyi Yu vì các thảo luận và phản hồi hữu ích của họ.

Tài Liệu Tham Khảo
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. In Proc. NeurIPS.

Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proc. NeurIPS.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.

Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. 2024. Fuyu-8B: A multimodal architecture for AI agents. https://www.adept.ai/blog/fuyu-8b.

William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In Proc. ICASSP.

Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. 2021. Gigaspeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio. In Proc. Interspeech.

Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. WavLM: Large-scale self-supervised pre-training for full stack speech processing. In Proc. JSTSP.

Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, and Furu Wei. 2023. BEATs: Audio pre-training with acoustic tokenizers. In Proc. ICML.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, et al. 2023. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. https://vicuna.lmsys.org.

Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919.

Linhao Dong and Bo Xu. 2020. CIF: Continuous integrate-and-fire for end-to-end speech recognition. In Proc. ICASSP.

Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. 2023. Prompting large language models with speech recognition abilities. arXiv preprint arXiv:2307.11795.

Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. 2024. LLaMA-Adapter v2: Parameter-efficient visual instruction model. In Proc. ICLR.

Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. 2022. Paraformer: Fast and accurate parallel Transformer for non-autoregressive end-to-end speech recognition. In Proc. Interspeech.

Alex Graves, Santiago Fernández, Faustino J. Gomez, and Jürgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proc. ICML.

Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proc. ICASSP.

Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. 2020. Conformer: Convolution-augmented Transformer for speech recognition. In Proc. Interspeech.

Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, and Yonghui Wu. 2020. Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv preprint arXiv:2005.03191.

Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. In Proc. TASLP.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In Proc. ICLR.

Jacob Kahn, Morgane Rivière, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazaré, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. 2020. Libri-light: A benchmark for asr with limited or no supervision. In Proc. ICASSP.

Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang. 2020. Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions. In Proc. ICASSP.

Fangjun Kuang, Liyong Guo, Wei Kang, Long Lin, Mingshuang Luo, Zengwei Yao, and Daniel Povey. 2022. Pruned RNN-T for fast, memory-efficient ASR training. In Proc. Interspeech.

Jinyu Li. 2022. Recent advances in end-to-end automatic speech recognition. In Proc. APSIPA.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proc. ICML.

Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu. 2023b. Prompting large language models for zero-shot domain adaptation in speech recognition. In Proc. ASRU.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Proc. NeurIPS.

Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proc. ICLR.

Ziyang Ma, Zhisheng Zheng, Changli Tang, Yujin Wang, and Xie Chen. 2023. MT4SSL: Boosting self-supervised speech representation learning by integrating multiple targets. In Proc. Interspeech.

Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In Proc. of ICASSP.

Yifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe. 2022. Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding. In Proc. ICML.

Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, et al. 2024. Owsm v3. 1: Better and faster open Whisper-style speech models based on E-Branchformer. arXiv preprint arXiv:2401.16658.

Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In Proc. ICML.

Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2024. SALMONN: Towards generic hearing abilities for large language models. In Proc. ICLR.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, et al. 2023a. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, et al. 2023b. LLaMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.

Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proc. ACL.

Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, et al. 2023. LauraGPT: Listen, attend, understand, and regenerate audio with GPT. arXiv preprint arXiv:2310.04673.

Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023. On decoder-only architecture for speech-to-text and large language model integration. In Proc. ASRU.

Guanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, Zhikang Niu, and Xie Chen. 2023. FastHuBERT: an efficient training framework for self-supervised speech representation learning. In Proc. ASRU.

Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, and Daniel Povey. 2024. Zipformer: A faster and better encoder for automatic speech recognition. In Proc. ICLR.

Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2024. Connecting speech encoder and large language model for ASR. In Proc. ICASSP.

Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. 2023. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. In Proc. EMNLP.

Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLLaMA: An open-source small language model. arXiv preprint arXiv:2401.02385.

--- TRANG 11 ---
A Phụ Lục: Khám Phá Thêm
A.1 Độ Phức Tạp Văn Bản
Bảng 8: Độ phức tạp văn bản cấp từ (PPL) và tỷ lệ lỗi từ (WER) của các LLM khác nhau trên các tập con test-clean và test-other của Librispeech. Trong số các mô hình được liệt kê, mô hình ASR dựa trên LLM với Vicuna có tỷ lệ lỗi từ tốt nhất, trong khi LLaMA có hiệu suất tồi tệ nhất.

LLM | PPL (WER(%)) ↓
---|---
 | test-clean | test-other
LLaMA-2 | 53.74 (3.01) | 58.78 (7.15)
LLaMA-2-Chat | 77.60 (2.72) | 85.74 (6.79)
Vicuna | 76.44 (2.58) | 84.95 (6.47)

Độ phức tạp văn bản cấp từ (PPL) của các LLM khác nhau được đo lường để điều tra xem hiệu suất tốt hơn của Vicuna có liên quan đến sự thống nhất miền hay không, thay vì tinh chỉnh có giám sát. Như thể hiện trong Bảng 8, chúng tôi đo độ phức tạp trên các tập con test-clean và test-other. Một cách đáng ngạc nhiên, LLaMA-2 không có SFT đạt được độ phức tạp thấp nhất với một biên độ lớn so với các mô hình chat, trong khi có hiệu suất tồi tệ nhất về tỷ lệ lỗi từ. Điều này chứng minh rằng kết quả tốt hơn của các mô hình chat không phải do sự thống nhất miền với các phiên âm.

A.2 Kỹ Thuật Lời Nhắc
Bảng 9: Ví dụ về lời nhắc trong ASR dựa trên LLM.

Loại | Ví dụ
---|---
lời nhắc ngắn | Phiên âm tiếng nói thành văn bản.
lời nhắc dài | Phiên âm tiếng nói thành văn bản. Xuất ra phiên âm trực tiếp mà không có nội dung dư thừa. Đảm bảo rằng đầu ra không bị trùng lặp.

Chúng tôi cũng điều tra hiệu suất của các lời nhắc khác nhau trong ASR dựa trên LLM, và các ví dụ lời nhắc được thể hiện trong Bảng 9. Như thể hiện trong Bảng 10, khi chúng tôi sử dụng lời nhắc ngắn, mô hình đạt được kết quả tốt hơn so với mô hình sử dụng lời nhắc dài trong mô tả phức tạp. Tuy nhiên, khi chúng tôi không sử dụng bất kỳ lời nhắc nào (tức là, lời nhắc ngắn hơn chỉ với thẻ "ASSISTANT" còn lại), hiệu suất của mô hình giảm. Điều này cho thấy rằng mặc dù mô hình ASR dựa trên LLM là một MLLM cụ thể theo nhiệm vụ, việc thiết lập lời nhắc vẫn quan trọng. Một giải thích có thể là lời nhắc cho phép mô hình tối ưu hóa trong không gian con cụ thể theo nhiệm vụ thông qua học in-context, trong khi các lời nhắc quá phức tạp sẽ tăng độ khó học và dẫn đến giải pháp không tối ưu. Để điều tra giả định này, chúng tôi thiết lập một định dạng lời nhắc phức tạp hơn. Chúng tôi sử dụng cùng lời nhắc seed cho nhiệm vụ ASR trong SpeechGPT (Zhang et al., 2023) để tạo ra 10 lời nhắc để tạo thành một thư viện lời nhắc. Ở cả giai đoạn huấn luyện và kiểm tra, một lời nhắc ngẫu nhiên được rút ra từ thư viện lời nhắc. Như thể hiện trong hàng cuối cùng của Bảng 10, có sự giảm lớn trong hiệu suất mô hình, phù hợp với giả định của chúng tôi.

Bảng 10: Hiệu suất với các thiết kế lời nhắc khác nhau trong ASR dựa trên LLM trên các tập con test-clean và test-other của Librispeech.

Lời nhắc | WER(%) ↓
---|---
 | test-clean | test-other
không có lời nhắc | 3.19 | 6.97
lời nhắc ngắn | 2.58 | 6.47
lời nhắc dài | 2.88 | 6.79
lời nhắc được chọn ngẫu nhiên | 5.90 | 10.02
