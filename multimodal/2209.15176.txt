# 2209.15176.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2209.15176.pdf
# File size: 598929 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Adaptive Sparse and Monotonic Attention for
Transformer-based Automatic Speech Recognition
Chendong Zhaoy?z, Jianzong Wangy, Wenqi Weiy, Xiaoyang Quy, Haoqian Wang?, Jing Xiaoy
yPing An Technology (Shenzhen) Co., Ltd., Shenzhen, China
?The Shenzhen International Graduate School, Tsinghua University, China
Abstract ‚ÄîThe Transformer architecture model, based on self-
attention and multi-head attention, has achieved remarkable suc-
cess in ofÔ¨Çine end-to-end Automatic Speech Recognition (ASR).
However, self-attention and multi-head attention cannot be easily
applied for streaming or online ASR. For self-attention in Trans-
former ASR, the softmax normalization function-based attention
mechanism makes it impossible to highlight important speech
information. For multi-head attention in Transformer ASR, it
is not easy to model monotonic alignments in different heads.
To overcome these two limits, we integrate sparse attention and
monotonic attention into Transformer-based ASR. The sparse
mechanism introduces a learned sparsity scheme to enable each
self-attention structure to Ô¨Åt the corresponding head better. The
monotonic attention deploys regularization to prune redundant
heads for the multi-head attention structure. The experiments
show that our method can effectively improve the attention
mechanism on widely used benchmarks of speech recognition.
Index Terms ‚ÄîAutomatic Speech Recognition, Sparse Atten-
tion, Monotonic Attention, Self-Attention.
I. I NTRODUCTION
Automatic speech recognition (ASR) has been well explored
since it was Ô¨Årst perceived. End-to-end ASR has seen large
improvements in recent years. The great success of end-to-
end paradigms for ASR should largely be attributed to several
advanced models, such as Connectionist Temporal ClassiÔ¨Åca-
tion (CTC) [1], Recurrent Neural Network Transducer (RNN-
T) [2], and attention-based ASR [3]‚Äì[5]. The Transformer
architecture has shown promising performance for end-to-end
ASR [6]‚Äì[9], which combines an acoustic model, a language
model, and an acoustic-to-text alignment mechanism. Unlike
recurrent neural networks, Transformer can perform parallel
training very well as it abandons the recurrent structure that
has an inherently sequential nature [10]‚Äì[13]. Moreover, multi-
head attention in the Transformer can signiÔ¨Åcantly improve
the network‚Äôs performance because it can achieve dynamic
embedding and capture the strong intrinsic structures [14]‚Äì
[17]. However, although the Transformer shows expressive
power for Natural Language Processing (NLP) tasks, there
still remains room for exploiting it in speech tasks.
Transformer ASR methods [6], [7] exploit the self-attention
approach, where each self-attention layer in both encoder and
decoder utilizes the multi-head structure to extract different
input representations. While it has been studied that some of
Corresponding Author: Jianzong Wang, jzwang@188.com
zWork done during an internship at Ping An Technology.the attention heads are useless in other tasks, we previously
observed this phenomenon in Transformer ASR as well, where
the attention function is similar to an identity mapping. This
issue motivates us to Ô¨Ånd out and address the redundant
calculations in such self-attention. So in this work, we pro-
pose to remove the redundant informations without degrading
the overall performance. For Transformer-based ASR model,
one fundamental problem is that the softmax normalization
function-based attention mechanism makes it impossible to
highlight important speech information. In the attention mech-
anism, softmax is often used to calculate the attention weight,
and its output is always dense, and non-negative [18]‚Äì[20].
Dense attention means that all inputs always contribute a little
bit to the decision, which leads to a waste of attention. Besides,
the attention score distribution becomes Ô¨Çat as the input length
increases. Therefore, it is difÔ¨Åcult to focus on the speech‚Äôs
critical information, especially for a long speech. Considering
that dense attention cannot highlight important information
in a long speech, we try to use sparse transformation to
achieve sparse attention distribution. Inspired by [21], we Ô¨Årst
apply the adaptive -entmax to the ASR task. SpeciÔ¨Åcally, we
replace the softmax in each self-attention with -entmax, and
maintain the learnability of .
For attention-based ASR model, another fundamental chal-
lenge is the accurate alignment between the input speeches
and output texts [22]. Monotonicity can keep the marginal-
ization of the alignment between text output and speech
input tractable. But, the monotonic multi-head alignment can
not make all heads contribute to the Ô¨Ånal predictions. We
proposed an adaptive monotonic multi-head attention with a
regularization constraint. In our experiments, we found some
techniques which highlight attention scores of important parts
of the speech, offering better alignment accuracy. In practice,
searching for the best function is time-consuming. To tackle
this problem, we introduce an approach based on automatically
adaptive attention. Contributive to such alignment, the trained
model is more focusing and accurate of multiple attentions. In
this way, it can adaptively eliminate the redundant heads and
let every head learn alignments properly.
Overall, our contributions are as follows. Firstly, we unify
the sparse attention and monotonic attention into ASR tasks.
Secondly, we propose an adaptive method to obtain a unique
parameter for each self-attention in Transformer. Third,
we deployed a regularized monotonic multi-head alignment
scheme for acoustic-to-text alignment.arXiv:2209.15176v1  [cs.CL]  30 Sep 2022

--- PAGE 2 ---
II. R ELATED WORKS AND PRELIMINARY
A. Sparse Attention
Recent works [23]‚Äì[25] based on the attention function
propose to use sparse normalizing such as sparsemax [19].
The sparsemax can omit items with small probability and
thus can distribute the probability mass on outputs with higher
probabilities. It improves the performance and interpretability
to a certain extent [20]. However, each head of the multi-
head attention mechanism used in Transformer is designed for
capturing characteristics from a different point of view. For
example, while the heads at the Ô¨Årst layer of encoder focus on
feature extraction, the heads in the last layer of decoder focus
on classiÔ¨Åcation of results. Intuitively the attention function
of the two scenarios should be different. So the method
of replacing all attention mechanisms with sparsemax may
not suit for Transformer [26]. In the Ô¨Åeld of NLP, there
are many breakthroughs in Transformer regarding sparsity.
From the perspective of Tsallis entropy [27], [28] unify all
transformation functions into the form of -entmax. When 
= 1,-entmax is a softmax function. For any > 1,-entmax
is the sparsemax function. Moreover, in order to make the -
entmax Ô¨Åt different heads for different tasks, [21] introduces
an adaptive method to enable the parameter to be updated.
B. Monotonic Attention
Monotonic attention technique [29] aims to restrict the tran-
sition of attention in a past-to-now pipeline. Generally speak-
ing, monotonic attention is adopted to reduce the complexity
of decoding. In strong context-related tasks as natural language
understanding, a fully attention function is commonly applied
to both input and output sequences, because the word-order
or the part-of-speech is less important. But it may not work
well for ASR tasks because the speech waveforms and target
text sequence are required to be temporally monotonic. For
enhancing the alignment ability between the waveform and
text sequence, [30] incorporate the monotonic attention to-
gether with the connectionist temporal classiÔ¨Åcation objective
in a multi-task learning diagram by using the same encoder
together. This recipe becomes the standard optimizing pipeline
for most attention-based models [31]‚Äì[33]. In [34], a mono-
tonic chunkwise attention (MoChA) approach was introduced
for a streaming attention. It Ô¨Årst splitted the encoder output
latents into rather short-length chunks and then applied the
soft attention function on these small chunks. Furthermore, an
adaptive-length chunk approach was proposed in [35]. Specif-
ically, monotonic inÔ¨Ånite lookback (MILK) attention [36] was
applied to consider the overall acoustic features preceding to
learn an adaptive latency-quality trade-off schedule jointly.
C. Attention-based ASR
Although attention-based architectures [4], [5], [37] have
achieved remarkable success in ofÔ¨Çine end-to-end ASR sys-
tems, they cannot easily be applied for the online or streaming
ASR systems. Several optimization paradigms are proposed
to overcome this limit. Monotonic multi-head attention [38]extends monotonicity to the multi-head to extract useful rep-
resentations and complex alignment. For Monotonic multi-
head attention [38], HeadDrop [39] is proposed to eliminate
the redundant head. MoChA [34] bridges the gap between
monotonic attention and soft attention. As the online speech
recognizing becoming more and more important, it also brings
a drawback of additional designings in the network and
training criterias, and precision degradations. As the Trans-
former‚Äôs size is large, a compressed structure [40] is proposed.
Conformer [41] is a convolution-augmented Transformer for
speech recognition. In triggered attention [42], a CTC task
is trained to learn the alignment that triggers the attention
decoding. The work in [43] use chunk-hopping for a latency-
control end-to-end ASR.
D. Transformer ASR Model Architecture
As shown in Figure 1, our network architecture is based on
a Transformer-based ASR model, based on an encoder and a
decoder. Let us denote the input sequence as x=fx1;:::;xTg
with T being the length of the frame sequence. The corre-
sponding encoder states are denoted as h=fh1;:::;hSg. Let
the output sequence as y=fy1;:::;yUgwithUbeing the
length of the character sequence. The encoder Ô¨Årst processes
the input with a down-sampling convolution layer and stacks
with several encoder blocks. The encoder processes sequential
speech features, which are 80-dimensional log-mel spectral
energies. The down-sampling convolution layer uses a stride
of 2, kernels of 33, followed by the ReLU activation
function. The down-sampling convolution is able to extract
useful encodings, shorten acoustic representations‚Äô length, and
promote faster alignments in the decoding. Main modules
of Transformer contain position encoding, multi-head self-
attention calculation, linear, and softmax, point-wise feed-
forward network, residual connections, layer normalization.
Here, we emphasize the concept of self-attention and multi-
head attention. Self-attention is designed to learn the internal
dependencies by computing the representation of a single
sequence. The Transformer employs the scaled dot-product
self-attention formulated as:
headi=softmax (QiKT
ip
d=h)Vi(i= 1;2;:::;h ); (1)
where Qi=XWq
i;K=X1Wk
i;Vi=XWv
i: (2)
Here,hmeans the number of heads of multi-head attention. As
the dimension of Xin Transformer has the same dimension
asdmodel , the project matrices are Wq
i2Rdmodeldq,Wk
i2
Rdmodeldk,Wv
i2Rdmodeldv. Note that dq=dk=dv=
dmodel=h. The output of self-attention Ois concatenated by
multi-head mechanism, which is formulated as follows
O=concat (head 1;head 2;:::;headh)WO: (3)
WO2Rhdvdmodel andconcat ()means concatenation.
Besides, it deploys the sinusoidal positional encoding scheme,
which achieves the attention calculation to adapt well on
different speech length.

--- PAGE 3 ---
Fig. 1. The diagram of our model architecture. Note that the residual
connections and layer normalization are omitted.
III. A PPROACH
In order to systematically prove the performance of adaptive
sparse transformer, we designed multiple transformer struc-
tures based on different sparse functions, including sparse
transformer, 1.5-entmax transformer, and adaptive -entmax
transformer. Our innovations are as follows: Ô¨Årstly, we use
different sparse functions to replace the softmax function in
self-attention, and use adaptive sparse methods to each self-
attention gets its own degree of sparsity. Secondly, we propose
a regularized monotonic multi-head attention. Our innovations
lie in two components: an adaptive sparse multi-head self-
attention illustrated in Sec. III-A and an adaptive monotonic
multi-head attention illustrated in Sec. III-B. Figure 1 shows
how to integrate these two core components into the baseline.
A. Adaptive Sparse Multi-head Self-Attention
As shown in Figure 2, we introduce several sparse functions
in self-attention to prevent the disadvantage of softmax. The
Ô¨Årst idea is to substitute softmax with sparsemax for every
independent head of the Transformer, which can totally omit
the attention of low probability and distribute probability mass
only on high-interest elements. It is achieved by computing the
Euclidean projection of the input vector zonto the probability
simplex p:
Sparsemax (z) =argmin
p2J1
2jjz pjj2
2; (4)
where J=p2RJjp0;JP
jpj= 1 is J-dimension sim-
plex. To overcome the potential issue of softmax, we propose
to treatvalues of each attention head differently, where some
heads may train to be informative sparser, and others maybecome similar its original form. We also propose to treat the 
values as learnable variables, optimized via back-propagation
calculations along with other network parameters. In [19], the
sparsity of sparsemax is proved and a closed-form solution is
given:
p=sparsemax (z) = [ z ]+; (5)
where []+denotes function max(;0), andis the Lagrange
multiplier in the context of the constraintP
j(pj) = 1 . Here
can also be interpreted as a threshold under which the
corresponding probability will be set to zero. Next, we extend
the concept of sparse attention in the multi-head architecture.
The core change of this architecture is we use -entmax [28]
to introduce sparsity into attention mechanism of sparse self-
attention. For the three mentioned transformations (softmax,
sparsemax, -entmax), the relationship among them can be
analyzed from the perspective of entropy:
-entmax (z) =argmax
p2JpTz+HT
(p): (6)
HT
(p)is the Tsallis -entropy family deÔ¨Ånition with the
following deÔ¨Ånition:
HT
(p) =8
<
:1
( 1)P
i(pj p
j)6= 1
 P
jpjlogpj= 1(7)
here is a hyperparameter. For the cases = 1 and= 2,
the corresponding -entropies are Shannon and Gini entropy
while the corresponding -entmaxes are equal to softmax and
sparsemax respectively. Thus it establishes the connection be-
tween softmax and sparsemax while also throwing lights on a
medium model between these two transformations. [28] proves
the uniqueness of and proposed an accurate method for
calculating-entmax when = 1.5 (also called 1.5-entmax).
[44] proposed an extended iterative bisection approach that is
applicable for -entmax. It is rational to set the to different
values in Transformer for better adaptation to different heads
in different layers.
Inspired by [21], we apply an adaptive -entmax attention
method in the multi-head attention mechanism for ASR tasks.
-entmax is a convex optimization problem. When is
set as a variable but not a hyperparameter, we have p=
 entmax (z;). It is not trivial to derivate@ entmax (z;)
@
from the Lagrangian and optimality conditions by taking the
derivative with respect to zin [19]. Due to current deep
neural network frameworks can not take the derivatives for
this optimization problem automatically. Thus, we tackle this
problem from the perspective of the solution. It is trivial that
forj =2S,p
j= 0 which is a constant thus@p
j
@= 0. To
simplify the gradient for i2S, we usepandzto denote
the corresponding vectors whose indices are in the support S.
Finally we can solve the component of the Jacobian.
B. Adaptive Monotonic Multi-head Attention
For end-to-end ASR, another fundamental challenge is the
alignment between the input sequence and output sequence.
CTC [1] and RNN-Transducer [2] are monotonic, so that

--- PAGE 4 ---
Fig. 2. Illustrations of the proposed adaptive sparse attention.
the alignment track is able to be marginal. But the attention
approach results in non-monotonic alignment, which may
diminish the quality of speech recognition. The hard mono-
tonic attention mechanism was introduced used for streaming
time-sync decoding with attention architecture ASR. For the
hard monotonic attention mechanism, the process is shown
as follows. For the decoding time-step i, the attention method
begins inspecting entries starting at the preceding output time-
step, referred to as ti 1. It then computes an energy scalar ei;j
based on a monotonic energy function MonotonicEnergy ().
This function refers the (i 1)thoutput state and encoding
featureshjas inputs, where j=ti 1;ti 1+1;:::. The energy
scalar is computed as follows:
ei;j=MonotonicEnergy (si 1;hj): (8)
The detail of monotonic energy function can refer to [38].
Then the energy values ei;jare passed into a sigmoid function
()to produce selection probability pi;jformulated as
pi;j=(ei;j): (9)
Then, the selection probability pi;jis used to produce a
discrete decision variable zi;j, which is formulated as
zi;jBernoulli (pi;j): (10)
Wheneverpi;is satisÔ¨Åed, the zi;jis stimulated (i.e., value
approaching 1). Once zi;j= 1, the model stops and set ti=j
andci=hti.
In contrast to a single attention head in the recurrent
sequence-to-sequence model, multi-head mechanism is capa-
ble of learning contexts with diverse paces between input and
output sequence. Besides, the multi-head can learn complex
alignment. In speciÔ¨Åc, the results of each head can be regarded
as corresponding to some speciÔ¨Åc frames in the speech, and
different heads are related to each other. While some heads
can detect boundaries, other heads can capture strong intrinsic
speech structures.
Dense attention mechanisms can fail to capture or utilize the
strong intrinsic structures present in speech. By taking advan-
tage of the dependencies between speciÔ¨Åc frames that different
heads focus on, we can emphasize the key information in the
speech and highlight the characteristic representation of thesespeech frames. However, there is no gurantee that multiple
heads are all contributive to the overall context learning.
Besides, the monotonic multi-head alignment can prevent
delayed state decoding caused by multi-heads for boundary de-
tection. Every monotonic attention head must learn alignments
properly. It is necessary to prune some redundant monotonic
attention heads. Thus, to enhance accordance among multi-
heads on token boundary discovery, we use L1 regularization
to eliminate information-redundant heads in shallow layers.
IV. E XPERIMENT
In this section, we compare our method with the standard
Transformer architecture of the softmax transform. Also, we
compare with other two method variants for a fair compara-
sion, which explore different normalizing transformations:
1.5-entmax : Set Ô¨Åxed value = 1.5 for all multi-heads of
a sparse ent-max Transformer. This approach is considered
to be novel, since 1.5-entmax had solely been introduced
for recurrent ASR methods, but not explored in Transformer
before. The major difference is, assigning sparse ent-max is
only one module of seq2seq method, but being an integral
component of all the Transformer modules.
-entmax : To be beneÔ¨Åt from an adaptive value of the sparse
ent-max transformation, a learnable t
i;jis assigned for every
attention head.
A. Dataset and Setup
Datasets : We systematically evaluated model performance
on AISHELL-1 [45] and WSJ. AISHELL-1 is a Chinese
corpus, which consists of 400 speakers and almost 170 hours
of recorded utterances. The corpus utterances are splitted to
training, develop and test datasets, where the training set
consists of 120,098 speeches of 340 speakers; the develop
set consists of 14,326 speeches from 40 speakers and test
set consists of 7,176 utterances from 20 speakers. In the
experiments of AISHELL-1, we do not use language models.
For the WSJ dataset, we utilize eval-92 for evaluation and
eval-93 for test. We use transcribed text to train a N-gram
language model.
Metrics : Word error rate (WER) and character error rate
(CER) were applied as the evaluation metrics in the exper-
iments. WER is calculated by the portion of words whose
recognized text-formulations were not same with the ground-
truth labels. CER is calculated based on the Levenshtein dis-
tance between the recognized and the ground-truth characters,
then divided by the absolute sequence length. Both metrics
follows that the lower of values, the better the performance.
Training Details : As mentioned before, the input speech
waveform is Ô¨Årstly processed to deep features of 80-
dimensional Ô¨Ålterbanks, where the hop size is 10ms and the
window size is 25ms. Also, we conduct on temporal and
speaker differences subtraction and bias normalization. The ar-
chitecture setting includes the encoder block number Ne= 12 ,
the decoder block number Nd= 6 and the feed-forward
network of dff= 1024 . We also keep feature dimension
dmodel = 512 and attention heads h= 8same as the baseline.

--- PAGE 5 ---
TABLE I
RESULTS OF DIFFERENT MAX FUNCTIONS (CER (%) INAISHELL, WER (%) INWSJ), T REPRESENTS SOFTMAX TEMPERATURE . NUMBERS IN THE
LAST THREE ROWS REPRESENT SPARSING ONLY ENCODER /ONLY DECODER /BOTH .
MethodSparse Attention Sparse Attention and Monotonic Attention
AISHELL-Dev AISHELL-Test WSJ-eval93 AISHELL-Dev AISHELL-Test WSJ-eval93
softmax 8.60 9.33 9.58 8.23 9.00 9.36
softmax(T=0.75) 8.54 9.03 9.34 8.43 8.91 9.07
softmax(T=0.5) 7.80 8.64 8.80 7.73 8.59 8.64
softmax(T=0.25) 8.07 8.91 9.05 7.87 8.69 8.91
sparsemax 8.91 / 8.99 / 8.98 9.56 / 9.53 / 9.55 9.69 / 9.64 / 9.62 8.64 / 8.58 / 8.64 9.13 / 9.29 / 9.06 9.45 / 9.38 / 9.34
1.5-entmax 7.69 / 7.71 / 7.69 8.57 / 8.62 / 8.49 8.47 / 8.58 / 8.56 7.68 / 7.67 / 7.68 8.51 / 8.59 / 8.47 8.49 / 8.55 / 8.39
-entmax (adaptive) 7.67 / 7.71 / 7.65 8.54 / 8.60 / 8.53 8.35 / 8.47 / 8.38 7.71 / 7.69 / 7.64 8.51 / 8.52 / 8.45 8.38 / 8.49 / 8.37
TABLE II
WER (%) COMPARISONS ON WSJ EVAL 93
Method WER
Transformer 9.39
wav2letter [46] 9.5
Jasper [47] 9.9
Explicit Sparse Transformer [48] 12.98
Factorized Attention Transformer [49] 8.97
Combiner [50] 9.32
Proposed method 8.22
TABLE III
CER (%) COMPARISONS ON ON AISHELL-1
Method Dev Test
Transformer 8.47 9.32
RNN-T 10.13 11.82
LAS [5] - 10.56
SA-T [2] 8.30 9.30
Sync-Transformer [51] 7.91 8.91
Proposed method 7.58 8.40
We used the Adam optimizer with 1= 0.9,2=0.98 and
the warmup steps is 25000. The dropout is 0.1. For decoding,
we use beam search with a beam size of 5. We only changed
the softmax function in the self-attention in the encoder and
add adaptive monotonic multi-head attention in the decoder.
For comparison methods, we adopt a RNN-Transducer. The
model is a 4-layer bidirectional long short-term memory model
of 320 hidden units, each direction account for encoding, and
a 2-layer uni-directional long short-term memory model of
512 hidden units as recognization module. This architecture
considers both the acoustic and linguistic features, and projects
the prediction of non-linear activation function to softmax.
B. Overall Results
The results are shown in Table 1. When only sparse trans-
forms are added to the Transformer encoder and decoder,
both have better performance. Among them, adding to both
the encoder and decoder have achieved better results. We
speculate that this is because the sparse transform enhances the
discrimination of the acoustic features, and the discrimination
of the text information is less important than the comparison.
In addition, the model that completely replaces the sparse
transform achieves the best results. The method of sparsityTABLE IV
SPARSITY COMPARISONS (WER %) ONLIBRISPEECH CORPUS .
Method dev-clean dev test-clean test
softmax 3.64 11.89 3.86 11.95
sparsemax 3.88 12.01 4.00 12.19
1.5-entmax 3.48 10.43 3.79 11.76
-entmax 3.36 10.29 3.70 11.64
is signiÔ¨Åcantly improved compared to the method of changing
the softmax temperature. Low softmax temperatures can make
the distribution of softmax results steeper, thereby highlighting
the expression of some information, but the non-negative
output of softmax is still not changed, which still leads to
waste of attention. It is worth noting that a too low temperature
will cause the model to focus more on a certain part of the
information and ignore other important information, leading
to a decline in the results.
In self-attention, replacing the softmax function with adap-
tive-entmax all reduces WER, which shows that adding
sparseness is beneÔ¨Åcial to the performance of the model.
However, sparsemax caused a decrease in performance, which
shows that excessive sparseness can cause loss of information.
This demonstrates that excessive sparseness cannot highlight
the key information in the speech, and only a properly de-
signed adaptive sparse can effectively highlight the key infor-
mation. We combine the adaptive -entmax in self-attention
and report the best result in Table II, III.
C. The Analysis of Adaptively Sparse Attention
In this section, we retain the monotonic attention and study
the effect of different sparse transformations in the sparse self-
attention. In Table 1, we keep the self-attention module in its
most original state (softmax) and introduce it into the cross-
head attention block for experiments. The third to sixth rows
in Table 1 represent different max functions in the cross-head
attention block. Due to the small number of attention heads,
too sparse max function (sparsemax) will lead to the loss of
information. The max function with insufÔ¨Åcient sparsity does
not play a sparsity role. It is worth mentioning that the adaptive
-entmax function tends to =1. To further investigate the
proposed method on more challenging data sets, we also
conduct ablation studies on English LibriSpeech dataset in
Table IV. In Figure 3, we pose an example with different

--- PAGE 6 ---
Fig. 3. Attention map of different transformations in sparse self-attention.
Fig. 4. (a) The change of in sparse self-attention during training, (b) The
statistics of the Ô¨Ånal score of the various attention mechanisms.
sparse transformations in sparse self-attention. Compared with
softmax, the sparsemax function can produce competitive
sparse output, but it will miss some output information.
1.5-entmax can produce more complete output results, thus
achieving better performance. -entmax enables each self-
attention module to use a speciÔ¨Åc according to different
emphases, thus obtaining the best performance. We can see
that-entmax does not only avoid the waste of attention
scores, but also effectively highlights some important frames
in speech. The deepening of the color means that it has a
higher attention score.
In Figure 4(a), we tracked the change of the parameter.
It can be seen that at the beginning of the training, the 
parameter has decreased to a certain degree, which shows that
it is desirable to use a dense output similar to softmax in the
early stage of training. Especially for the encoder part, the
closer to the feature input, the closer the value of is to 1,
which shows that the softmax function can effectively enhance
the distinguishability of features. After a certain amount of
training, except for the Ô¨Årst layer of the encoder, the other
parts move closer to the sparse direction, which shows that
the method of initializing cannot determine the degree of
sparseness in advance, and to some extent, it shows the method
of Ô¨Åxingas a learnable parameter is suitable for multi-TABLE V
ABLATION STUDIES ABOUT THE L1REGULARIZATION ON MONOTONIC
ATTENTION .
MethodAISHELL-Test WSJ-eval93
reg. w/o reg. reg. w/o reg.
softmax 9.00 9.29 9.36 9.43
sparsemax 9.13 9.32 9.45 9.47
-entmax 8.49 8.50 8.37 8.35
head attention. In Figure 3(b), we visualized the Ô¨Ånal output
attention score of the cross-head attention, and we can see that
softmax generates a lot of low score attention due to lack of
sparsity. The structure we proposed can produce more high
scores to avoid the waste of attention scores.
D. The Analysis of Adaptively monotonic Attention
In Table 1, comparing with the sparse attention alone, the
preformance of sparsemax methods is improved by averaging
-0.17% with the help of monotonic attention. Furthermore, the
monotonic attention is also complementary to the sparsemax
and entmax methods. Test results are enhanced by -0.13%
CER on AISHELL and -0.07% WER on WSJ, respectively.
Results show that the monotonic attention can cooperate with
the sparse attention to improve the performance further.
We further experiment the L1 regularization on the mono-
tonic attention. As shown in Table 5, with the help of the
regularization, all max function methods gain improvements
both on AISHELL-1 and WSJ. The gap between w/o reg.
and the proposed can be reduced from 9.29% to 9.00% on
AISHELL-1 and 9.43% to 9.36% on WSJ for original softmax
version. We applied adaptive sparse where leading to 1.0,
the monotonic multi-head alignment can prevent delayed token
generation caused by such heads for boundary detection.
V. C ONCLUSION
This paper integrates an adaptive sparse attention mecha-
nism and an adaptive monotonic attention mechanism into
a Transformer-based end-to-end ASR model. The adaptive
sparse attention mechanism is implemented in multi-head
self-attention to highlight important informations in the input
speech by replacing the softmax function with -entmax.
Besides, the adaptive monotonic multi-head attention is im-
plemented by adding a regularized term. The experimental
results prove that our method can focus more attention on the
more important information and achieve the best results. The
adaptive mechanism bases only on gradient-based optimiza-
tion, searching for per-head sparsity and computing -entmax.
VI. A CKNOWLEDGMENT
This paper is supported by the Key Research and De-
velopment Program of Guangdong Province under grant
No.2021B0101400003. Corresponding author is Jianzong
Wang from Ping An Technology (Shenzhen) Co., Ltd
(jzwang@188.com).

--- PAGE 7 ---
REFERENCES
[1] A. Graves, S. Fern ¬¥andez, and F. Gomez, ‚ÄúConnectionist temporal clas-
siÔ¨Åcation: Labelling unsegmented sequence data with recurrent neural
networks,‚Äù International Conference on Machine Learning , pp. 369‚Äì376,
2006.
[2] Z. Tian, J. Yi, J. Tao, Y . Bai, and Z. Wen, ‚ÄúSelf-attention transducers
for end-to-end speech recognition.‚Äù Interspeech , 2019.
[3] S. Karita, N. E. Y . Soplin, S. Watanabe, M. Delcroix, A. Ogawa,
and T. Nakatani, ‚ÄúImproving Transformer-Based End-to-End Speech
Recognition with Connectionist Temporal ClassiÔ¨Åcation and Language
Model Integration,‚Äù in Proc. Interspeech 2019 , pp. 1408‚Äì1412.
[4] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y . Bengio,
‚ÄúAttention-based models for speech recognition,‚Äù Advances in neural
information processing systems , vol. 28, pp. 577‚Äì585, 2015.
[5] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, ‚ÄúListen, attend and spell,‚Äù
2016 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , 2015.
[6] X. Song, Z. Wu, Y . Huang, C. Weng, D. Su, and H. Meng, ‚ÄúNon-
autoregressive transformer asr with ctc-enhanced decoder input,‚Äù in
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2021, pp. 5894‚Äì5898.
[7] M. Li, C. Zoril Àòa, and R. Doddipatla, ‚ÄúHead-synchronous decoding
for transformer-based streaming asr,‚Äù in ICASSP 2021-2021 IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2021, pp. 5909‚Äì5913.
[8] C. Zhao, J. Wang, X. Qu, H. Wang, and J. Xiao, ‚Äúr-g2p: Evaluating and
enhancing robustness of grapheme to phoneme conversion by controlled
noise introducing and contextual information incorporation,‚Äù ICASSP
2022 - 2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pp. 6197‚Äì6201, 2022.
[9] E. Tsunoo, Y . Kashiwagi, and S. Watanabe, ‚ÄúStreaming transformer
asr with blockwise synchronous beam search,‚Äù in 2021 IEEE Spoken
Language Technology Workshop (SLT) . IEEE, 2021, pp. 22‚Äì29.
[10] Z. Hong, J. Wang, X. Qu, J. Liu, C. Zhao, and J. Xiao, ‚ÄúFederated
learning with dynamic transformer for text to speech,‚Äù in Interspeech ,
2021.
[11] C. Zhao, J. Wang, L. Li, X. Qu, and J. Xiao, ‚ÄúAdaptive few-shot
learning algorithm for rare sound event detection,‚Äù 2022 International
Joint Conference on Neural Networks (IJCNN) , 2022.
[12] N. Zhang, J. Wang, Z. Hong, C. Zhao, X. Qu, and J. Xiao, ‚ÄúDt-sv: A
transformer-based time-domain approach for speaker veriÔ¨Åcation,‚Äù 2022
International Joint Conference on Neural Networks (IJCNN) , 2022.
[13] Z. Hong, J. Wang, W. Wei, J. Liu, X. Qu, B. Chen, Z. Wei, and
J. Xiao, ‚ÄúWhen hearing the voice, who will come to your mind,‚Äù 2021
International Joint Conference on Neural Networks (IJCNN) , 2021.
[14] Z. Deng, Y . Cai, L. Chen, Z. Gong, Q. Bao, X. Yao, D. Fang,
W. Yang, S. Zhang, and L. Ma, ‚ÄúRformer: Transformer-based generative
adversarial network for real fundus image restoration on a new clinical
benchmark,‚Äù IEEE Journal of Biomedical and Health Informatics , 2022.
[15] Y . Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y . Zhang, R. Timofte, and L. V .
Gool, ‚ÄúMask-guided spectral-wise transformer for efÔ¨Åcient hyperspectral
image reconstruction,‚Äù in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) , 2022.
[16] Y . Cai, X. Hu, H. Wang, Y . Zhang, H. PÔ¨Åster, and D. Wei, ‚ÄúLearning
to generate realistic noisy images via pixel-level noise-aware adversarial
training,‚Äù in Thirty-Fifth Conference on Neural Information Processing
Systems , 2021.
[17] Y . Cai, J. Lin, X. Hu, H. Wang, X. Yuan, Y . Zhang, R. Timofte, and
L. V . Gool, ‚ÄúCoarse-to-Ô¨Åne sparse transformer for hyperspectral image
reconstruction,‚Äù in European Conference on Computer Vision (ECCV) ,
2022.
[18] R. Fan, W. Chu, P. Chang, and J. Xiao, ‚ÄúCass-nat: Ctc alignment-based
single step non-autoregressive transformer for speech recognition,‚Äù in
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2021, pp. 5889‚Äì5893.
[19] A. Martins and R. Astudillo, ‚ÄúFrom softmax to sparsemax: A sparse
model of attention and multi-label classiÔ¨Åcation,‚Äù in International Con-
ference on Machine Learning , 2016, pp. 1614‚Äì1623.
[20] V . Niculae and M. Blondel, ‚ÄúA regularized framework for sparse
and structured neural attention,‚Äù in Advances in Neural Information
Processing Systems , 2017, pp. 3338‚Äì3348.[21] G. Correia, V . Niculae, and A. F. Martins, ‚Äúadaptively sparse trans-
former,‚Äù in Conference on Empirical Methods in Natural Language
Processing , 2019.
[22] X. Song, Z. Wu, Y . Huang, C. Weng, D. Su, and H. Meng, ‚ÄúNon-
autoregressive transformer asr with ctc-enhanced decoder input,‚Äù in
ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP) . IEEE, 2021, pp. 5894‚Äì5898.
[23] J. Xue, T. Zheng, and J. Han, ‚ÄúStructured sparse attention for end-to-end
automatic speech,‚Äù International Conference on Acoustics Speech and
Signal Processing (ICASSP) , 2020.
[24] V . Niculae, A. F. Martins, M. Blondel, and C. Cardie, ‚ÄúSparsemap:
Differentiable sparse structured inference,‚Äù International Conference on
Machine Learning , 2018.
[25] C. Malaviya, P. Ferreira, and A. F. Martins, ‚ÄúSparse and constrained
attention for neural machine translation,‚Äù Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL). , 2018.
[26] D. Mare Àácek and R. Rosa, ‚ÄúExtracting syntactic trees from transformer
encoder self-attentions,‚Äù in Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP ,
2018, pp. 347‚Äì349.
[27] A. Plastino and A. Plastino, ‚ÄúStellar polytropes and tsallis‚Äô entropy,‚Äù vol.
174, no. 5-6. Elsevier, 1993, pp. 384‚Äì386.
[28] B. Peters, V . Niculae, and A. F. Martins, ‚ÄúSparse sequence-to-sequence
models,‚Äù in Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics , 2019, pp. 1504‚Äì1519.
[29] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, ‚ÄúOnline and
linear-time attention by enforcing monotonic alignments,‚Äù in Interna-
tional Conference on Machine Learning . PMLR, 2017, pp. 2837‚Äì2846.
[30] S. Kim, T. Hori, and S. Watanabe, ‚ÄúJoint ctc-attention based end-
to-end speech recognition using multi-task learning,‚Äù in 2017 IEEE
international conference on acoustics, speech and signal processing
(ICASSP) . IEEE, 2017, pp. 4835‚Äì4839.
[31] A. H. Liu, H.-y. Lee, and L.-s. Lee, ‚ÄúAdversarial training of end-to-end
speech recognition using a criticizing language model,‚Äù in ICASSP 2019-
2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) . IEEE, 2019, pp. 6176‚Äì6180.
[32] K. Kim, K. Lee, D. Gowda, J. Park, S. Kim, S. Jin, Y .-Y . Lee, J. Yeo,
D. Kim, S. Jung et al. , ‚ÄúAttention based on-device streaming speech
recognition with large speech corpus,‚Äù in 2019 IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU) . IEEE, 2019, pp.
956‚Äì963.
[33] T. Nakatani, ‚ÄúImproving transformer-based end-to-end speech recog-
nition with connectionist temporal classiÔ¨Åcation and language model
integration,‚Äù in Proc. Interspeech 2019 , 2019.
[34] C.-C. Chiu and C. Raffel, ‚ÄúMonotonic chunkwise attention,‚Äù Interna-
tional Conference on Learning Representations (ICLR) , 2017.
[35] R. Fan, P. Zhou, W. Chen, J. Jia, and G. Liu, ‚ÄúAn online attention-based
model for speech recognition,‚Äù Proc. Interspeech 2019 , pp. 4390‚Äì4394,
2019.
[36] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz, R. Pang,
W. Li, and C. Raffel, ‚ÄúMonotonic inÔ¨Ånite lookback attention for simulta-
neous machine translation,‚Äù in Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics , 2019, pp. 1313‚Äì1323.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in
neural information processing systems , vol. 30, 2017.
[38] X. Ma, J. Pino, J. Cross, L. Puzon, and J. Gu, ‚ÄúMonotonic multihead at-
tention,‚Äù International Conference on Learning Representations (ICLR) ,
2019.
[39] H. Inaguma, M. Mimura, and T. Kawahara, ‚ÄúEnhancing monotonic
multihead attention for streaming asr,‚Äù INTERSPEECH , 2020.
[40] S. Li, D. Raj, X. Lu, P. Shen, T. Kawahara, and H. Kawai, ‚ÄúImproving
transformer-based speech recognition systems with compressed structure
and speech attributes augmentation.‚Äù in INTERSPEECH , 2019, pp.
4400‚Äì4404.
[41] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y . Wu et al. , ‚ÄúConformer: Convolution-augmented
transformer for speech recognition,‚Äù INTERSPEECH , 2020.
[42] N. Moritz, T. Hori, and J. Le Roux, ‚ÄúTriggered attention for end-
to-end speech recognition,‚Äù in ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP) .
IEEE, 2019, pp. 5666‚Äì5670.
[43] L. Dong, F. Wang, and B. Xu, ‚ÄúSelf-attention aligner: A latency-
control end-to-end model for asr using self-attention network and chunk-

--- PAGE 8 ---
hopping,‚Äù in ICASSP 2019-2019 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2019, pp.
5656‚Äì5660.
[44] M. Blondel, A. Martins, and V . Niculae, ‚ÄúLearning classiÔ¨Åers with
fenchel-young losses: Generalized entropies, margins, and algorithms,‚Äù
inThe 22nd International Conference on ArtiÔ¨Åcial Intelligence and
Statistics , 2019, pp. 606‚Äì615.
[45] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, ‚ÄúAishell-1: An open-
source mandarin speech corpus and a speech recognition baseline,‚Äù
in2017 20th Conference of the Oriental Chapter of the International
Coordinating Committee on Speech Databases and Speech I/O Systems
and Assessment (O-COCOSDA) . IEEE, 2017, pp. 1‚Äì5.
[46] N. et al, ‚ÄúFully convolutional speech recognition,‚Äù CoRR, vol.
abs/1812.06864, , 2018.
[47] J. Li, V . Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Cohen,
H. Nguyen, and R. T. Gadde, ‚ÄúJasper: An end-to-end convolutional
neural acoustic model,‚Äù in Interspeech 2019 .
[48] J. L. Guangxiang Zhao and X. Renz, ‚ÄúExplicit sparse transformer:
Concentrated attention through explicit selection,‚Äù arXiv preprint
arXiv:1912.11637v1 , 2019.
[49] R. Child, S. Gray, A. Radford, and I. Sutskever, ‚ÄúGenerating long
sequences with sparse transformers.‚Äù Annual Conference of the North
American Chapter of the Association for Computational Linguistics ,
2019.
[50] J. Wu, C. Xiong, and T. Schnabel, ‚ÄúCombiner: Inductively learning
tree structured attention in transformer,‚Äù in International Conference on
Learning Representations , 2020.
[51] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, ‚ÄúSynchronous trans-
former for end-to-end speech recognition,‚Äù International Conference on
Acoustics Speech and Signal Processing , 2020.
