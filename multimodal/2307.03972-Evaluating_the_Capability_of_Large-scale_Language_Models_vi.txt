# Đánh giá khả năng của các mô hình ngôn ngữ quy mô lớn trong tác vụ sửa lỗi ngữ pháp tiếng Trung Quốc

Fanyi Qu Chenming Tang Yunfang Wu∗
Phòng thí nghiệm trọng điểm quốc gia về xử lý thông tin đa phương tiện, Đại học Bắc Kinh
Phòng thí nghiệm trọng điểm BOE về ngôn ngữ học tính toán, Đại học Bắc Kinh
Khoa Khoa học máy tính, Đại học Bắc Kinh
{fanyiqu@, tangchenming@stu, wuyf@}pku.edu.cn

## Tóm tắt
Các mô hình ngôn ngữ quy mô lớn (LLMs) đã thể hiện khả năng đáng chú ý trong nhiều tác vụ xử lý ngôn ngữ tự nhiên (NLP) và thu hút nhiều sự chú ý gần đây. Tuy nhiên, một số nghiên cứu chỉ ra rằng các mô hình ngôn ngữ lớn không đạt được kết quả đầy hứa hẹn vượt qua các mô hình tiên tiến nhất trong tác vụ sửa lỗi ngữ pháp tiếng Anh (GEC). Trong báo cáo này, chúng tôi nhằm khám phá cách LLMs hoạt động trong các tác vụ GEC tiếng Trung (CGEC) và cung cấp hướng dẫn cho công việc tương lai. Chúng tôi tiến hành thí nghiệm với 12 LLMs có quy mô mô hình khác nhau trên 4 bộ dữ liệu CGEC tiếng Trung. Kết quả thí nghiệm của chúng tôi cho thấy hiệu suất của LLMs trên các chỉ số đánh giá tự động (ví dụ: điểm F0.5) kém hơn so với các mô hình tiên tiến nhất trước đây (SOTA) do vấn đề sửa chữa quá mức. Hơn nữa, chúng tôi cũng phát hiện những biến đổi đáng chú ý trong hiệu suất của LLMs khi được đánh giá trên các phân phối dữ liệu khác nhau và sự ưu tiên của các mô hình đa năng so với các đối tác lý luận của chúng. Các phát hiện của chúng tôi chứng minh rằng cần có thêm nghiên cứu để ứng dụng LLMs trong tác vụ CGEC.

## 1 Giới thiệu
Dựa trên InstructGPT (Ouyang et al., 2022), ChatGPT đã chứng minh khả năng mạnh mẽ trong việc hiểu các hướng dẫn phức tạp và tạo ra các phản hồi hợp lý trong nhiều tác vụ NLP. Theo quỹ đạo kỹ thuật của ChatGPT, một số lượng đáng kể các LLMs chất lượng cao đã xuất hiện gần đây trong cả học thuật và công nghiệp, như LLaMA (Touvron et al., 2023), ChatGLM (Du et al., 2022) và PaLM (Anil et al., 2023). Các nghiên cứu trước đây phát hiện rằng những LLMs này đã đạt được hiệu suất tuyệt vời trên một loạt rộng các tác vụ NLP, bao gồm dịch máy (Jiao et al., 2023), nhận dạng thực thể có tên (Li et al., 2023), tóm tắt văn bản (Yang et al., 2023), v.v.

Một số nghiên cứu đã tiến hành điều tra toàn diện về hiệu suất của LLMs trong lĩnh vực GEC tiếng Anh, mang lại một số phát hiện thú vị (Fang et al., 2023; Wu et al., 2023). LLMs không thể vượt trội hơn các mô hình SOTA về các chỉ số đánh giá tự động. Điều này chủ yếu là do LLMs có xu hướng thực hiện các sửa đổi không cần thiết để làm cho các câu đầu vào trở nên lưu loát hơn, có thể dẫn đến vấn đề sửa chữa quá mức, và trong một số trường hợp, thậm chí thay đổi ngữ nghĩa gốc của các câu đầu vào.

Trong báo cáo này, chúng tôi nhằm khám phá hiệu suất của LLMs trong tác vụ GEC tiếng Trung (CGEC). Chúng tôi đã tiến hành thí nghiệm trên nhiều LLMs khác nhau để điều tra ảnh hưởng của kích thước mô hình đến kết quả GEC. Ngoài ra, chúng tôi đã thử nghiệm các bộ dữ liệu kiểm tra khác nhau từ nhiều nguồn dữ liệu khác nhau để khám phá tác động của phân phối dữ liệu đến kết quả. Kết quả thí nghiệm cho thấy các LLMs cập nhật vẫn tụt hậu so với các mô hình CGEC SOTA. Chúng tôi cũng thấy rằng hiệu suất mô hình phụ thuộc vào phân phối dữ liệu và các mô hình ngôn ngữ đa năng vượt trội hơn các mô hình lý luận mạnh mẽ ngay cả với chi phí thấp hơn.

## 2 Thiết lập thí nghiệm

### 2.1 Bộ dữ liệu
Chúng tôi tiến hành thí nghiệm trên bốn bộ dữ liệu CGEC để cung cấp một minh chứng toàn diện về khả năng của LLMs. Thống kê chi tiết của các bộ dữ liệu này được hiển thị trong Bảng 1.

#### 2.1.1 Dữ liệu GEC từ người học tiếng Trung
Chúng tôi áp dụng bộ kiểm tra của NLPCC-2018 (Zhao et al., 2018) và bộ xác thực của MuCGEC (Zhang et al., 2022) để đánh giá. Hai bộ dữ liệu này thu thập các lỗi ngữ pháp do người nước ngoài mắc phải trong quá trình học tiếng Trung.

#### 2.1.2 Dữ liệu GEC từ kỳ thi của người bản xứ tiếng Trung
Chúng tôi áp dụng bộ xác thực của FCGEC (Xu et al., 2022) và bộ xác thực của NaCGEC (Ma et al., 2022) để đánh giá. Hai bộ dữ liệu này được thu thập từ các kỳ thi ngôn ngữ của người bản xứ tiếng Trung.

### 2.2 Mô hình
Chúng tôi tiến hành thí nghiệm trên 12 LLMs với các quy mô mô hình khác nhau:

• Nhóm mô hình ChatGPT của OpenAI: chúng tôi đánh giá hiệu suất của gpt-3.5-turbo, gpt-4o-mini và o3-mini với API của OpenAI. Hai mô hình đầu là mô hình đa năng trong khi mô hình thứ ba là mô hình lý luận mạnh mẽ.

• Doubao của ByteDance: chúng tôi đánh giá doubao-1.5-pro với API của Volcengine.

• Nhóm mô hình DeepSeek (DeepSeek-AI, 2024, 2025): chúng tôi đánh giá deepseek-v3 và deepseek-r1 với API của Volcengine. Mô hình trước là mô hình đa năng trong khi mô hình sau là mô hình lý luận mạnh mẽ.

• Nhóm mô hình ChatGLM (Du et al., 2022; Zeng et al., 2024): chúng tôi đánh giá chatglm-6b trên 4 GPU NVIDIA 3080Ti và glm3-130b với API của Volcengine.

• Nhóm mô hình LLaMA (Touvron et al., 2023): chúng tôi đánh giá llama-7B trên 4 GPU NVIDIA 3080Ti và llama-3.1-8b-instruct trên một GPU NVIDIA A40.

• Nhóm mô hình Qwen (Yang et al., 2024): chúng tôi đánh giá qwen-2.5-7b-instruct trên một GPU NVIDIA A40 và qwq-32b trên một GPU NVIDIA A100. Mô hình trước là mô hình ngôn ngữ đa năng và mô hình sau là mô hình lý luận.

### 2.3 Chỉ số đánh giá
Chúng tôi đánh giá hiệu suất của các mô hình với Precision (P), Recall (R) và F0.5 từ cấp độ từ và cấp độ ký tự tương ứng.

Chúng tôi áp dụng cài đặt chính thức của MaxMatch (M2) (Dahlmeier và Ng, 2012) scorer để tính điểm F0.5 cấp độ từ và chọn PKUNLP làm công cụ phân đoạn từ. Chúng tôi áp dụng ChERRANT để tính toán chỉ số cấp độ ký tự.

### 2.4 Prompt
Xem xét sự khác biệt trong hiệu suất của các mô hình ngôn ngữ lớn, chúng tôi đã thiết kế các prompt khác nhau cho chúng. Những prompt này gần giống nhau về mặt ngữ nghĩa, nhưng có một số khác biệt trong chi tiết. Các prompt được hiển thị trong Hình 1.

## 3 Kết quả thí nghiệm
Kết quả thí nghiệm cấp độ ký tự và cấp độ từ được hiển thị trong Bảng 2 và 3 tương ứng.

Đầu tiên, các nguồn dữ liệu khác nhau dẫn đến kết quả đánh giá khác biệt. Hầu hết LLMs thể hiện hiệu suất vượt trội đáng kể trên dữ liệu người học tiếng Trung (NLPCC và MuCGEC), trái ngược với dữ liệu kỳ thi của người bản xứ tiếng Trung (FCGEC và NaCGEC). Theo quan sát của chúng tôi, các lỗi ngữ pháp do người học tiếng Trung mắc phải chủ yếu liên quan đến việc sử dụng sai các từ hoặc cụm từ tương tự, thay vì cấu trúc câu không chính xác. Ngược lại, dữ liệu GEC từ kỳ thi của người bản xứ tiếng Trung duy trì mức độ đều đặn cao hơn và bao gồm nhiều lỗi cấu trúc phức tạp hơn. Đáng chú ý là tồn tại khoảng cách giữa dữ liệu GEC từ kỳ thi tiếng Trung và thói quen nói hàng ngày của người bản xứ tiếng Trung. Đặc biệt, Doubao, QwQ và các mô hình DeepSeek thể hiện hiệu suất cạnh tranh trên dữ liệu kỳ thi của người bản xứ tiếng Trung. Điều này có thể do rò rỉ dữ liệu hoặc nội dung GEC chuyên biệt trong dữ liệu huấn luyện.

Thứ hai, các mô hình mới hơn hoạt động tốt hơn các mô hình trước đó và các mô hình lớn hơn tốt hơn các mô hình nhỏ hơn. Ví dụ, LLaMA-3.1 mới hơn vượt trội đáng kể so với LLaMA cũ hơn trên tất cả các bộ dữ liệu, trong khi Doubao và DeepSeek lớn hơn vượt trội hơn tất cả các mô hình tương đối nhỏ hơn khác.

Thứ ba, các mô hình đa năng tốt hơn các mô hình lý luận trong CGEC. Các mô hình lý luận cập nhật o3-mini và deepseek-r1, mặc dù tốn nhiều tiền và thời gian hơn, không vượt trội hơn các đối tác tương đối cũ hơn gpt-4o-mini và deepseek-v3. Mô hình qwq-32b mới hơn và lớn hơn, mặc dù hoạt động tốt hơn qwen-2.5-7b-instruct, phải trả giá bằng kích thước mô hình lớn hơn 4 lần và thời gian suy luận dài hơn nhiều (64 giờ trên một GPU NVIDIA A100 đơn trong khi qwen-2.5-7b-instruct chỉ mất 1 giờ trên một GPU NVIDIA A40 đơn). Điều này cho thấy quá trình suy nghĩ sâu dài của các mô hình lý luận không thể đóng góp vào hiệu suất CGEC của chúng. Trong các ứng dụng thực tế, khuyến nghị áp dụng các mô hình ngôn ngữ đa năng cho tác vụ CGEC.

Thứ tư, vẫn tồn tại khoảng cách lớn giữa các mô hình SOTA và LLMs trên các chỉ số đánh giá tự động. Công việc trước đây (Jiao et al., 2023) đã phát hiện vấn đề sửa chữa quá mức đối với LLMs, điều này cũng đã được chú ý trong thí nghiệm của chúng tôi.

Hơn nữa, khó giải thích tại sao các chỉ số đánh giá cấp độ ký tự lại thấp hơn đáng kể so với các chỉ số đánh giá cấp độ từ, điều này không được chú ý trong công việc trước đây.

## 4 Kết luận
Trong báo cáo này, chúng tôi khám phá hiệu suất của nhiều LLMs khác nhau trong tác vụ CGEC. Kết quả thí nghiệm cho thấy vẫn còn khoảng cách giữa hiệu suất của LLMs và các mô hình SOTA hiện tại. Hơn nữa, hiệu suất của các LLMs khác nhau phụ thuộc vào phân phối dữ liệu kiểm tra và các mô hình đa năng tốt hơn trong CGEC so với các mô hình lý luận với chi phí thấp hơn. Công việc tương lai có thể tập trung vào việc giải quyết vấn đề sửa chữa quá mức của LLMs và khám phá tiềm năng chưa được khai thác của LLMs trong lĩnh vực các tác vụ GEC.
