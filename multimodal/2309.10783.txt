# 2309.10783.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2309.10783.pdf
# File size: 1167925 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Language as the Medium: Multimodal Video Classification through text only
Laura Hanu Anita L. Ver ˝o
Unitary
{laura,anita,james }@unitary.aiJames Thewlis
Abstract
Despite an exciting new wave of multimodal machine
learning models, current approaches still struggle to inter-
pret the complex contextual relationships between the dif-
ferent modalities present in videos. Going beyond exist-
ing methods that emphasize simple activities or objects, we
propose a new model-agnostic approach for generating de-
tailed textual descriptions that captures multimodal video
information. Our method leverages the extensive knowl-
edge learnt by large language models, such as GPT-3.5
or Llama2, to reason about textual descriptions of the vi-
sual and aural modalities, obtained from BLIP-2, Whis-
per and ImageBind. Without needing additional finetun-
ing of video-text models or datasets, we demonstrate that
available LLMs have the ability to use these multimodal
textual descriptions as proxies for “sight” or “hearing”
and perform zero-shot multimodal classification of videos
in-context. Our evaluations on popular action recogni-
tion benchmarks, such as UCF-101 or Kinetics, show these
context-rich descriptions can be successfully used in video
understanding tasks. This method points towards a promis-
ing new research direction in multimodal classification,
demonstrating how an interplay between textual, visual and
auditory machine learning models can enable more holistic
video understanding.
1. Introduction
Imagine it is the year 2008 and you have just watched
the latest episode of Breaking Bad - a highly multimodal
experience featuring moving pictures, speech and sound ef-
fects. Suddenly you receive a text message on your mobile
phone – it is your colleague, who is urgently requesting a
description of the episode so that they may participate in
water cooler discussions without arousing suspicion.
You must now convey to your colleague, using only text
messages, a description of the episode that will stand up to
scrutiny. Although reducing the vast amount of pixels and
audio samples you have just consumed down to a few words
seems like a daunting task, you recognize that by combiningsuccinct descriptions of key images, speech and sounds with
your colleague’s inherent ability to fill in gaps using contex-
tual reasoning you will be able to provide a comprehensive
recount of the episode without the need for the direct expe-
rience. In this work, we explore to what extent Large Lan-
guage Models (LLMs) are able to perform a similar task,
namely classifying the action in videos when receiving only
textual clues about the video contents from other models.
The last few years have seen remarkable progress in
large language models for text, which have shown unprece-
dented capabilities and performance on downstream tasks
[7, 17, 14, 18]. This has led to methods trying to bridge
the gap between vision and language. Contrastive meth-
ods such as CLIP train joint vision language representa-
tions [15]. Perceiver IO [12] offers a generic scheme to
encode arbitrary modalities. Kosmos [11] is a large multi-
modal model trained from scratch on web-scale image and
text data. GPT-4 [14] accepts image input, but this feature
is not currently publicly available. Numerous works adapt
pretrained LLMs in order to understand information from
different modalities. Flamingo [5] injects representations
of images and short videos into the language model back-
bone using Gated X-attn layers. BLIP-2 [13] introduces a
Q-Former which provides “soft visual prompts” to condi-
tion an LLM on visual information. Mini-GPT4 [19] lever-
ages the Q-Former to provide a soft prompt to a Llama-
based model. In contrast to these techniques, we demon-
strate that using only text as the medium can convey multi-
modal information to downstream LLMs. This has several
key advantages. Firstly, this approach ensures a straightfor-
ward “plug and play” interface for chaining models without
extra adaptation. This is particularly relevant with the rise
of API-based language models that prohibit modifications.
Secondly, inter-model communication becomes transparent
and interpretable in natural language. Crucially, this method
simplifies tasks like multimodal video classification into
two phases: a “perception” phase using unimodal or multi-
modal models as surrogates for various senses, followed by
a “reasoning” phase where a foundation model consolidates
diverse inputs to create a comprehensive video narrative.
More recent methods such as LENS [6] or Video Chat-arXiv:2309.10783v1  [cs.CV]  19 Sep 2023

--- PAGE 2 ---
  Language  Model
Perception moduleReasoning moduleHere are 5 action categories from the list that seem most likely to match this video: 1.Dribbling basketball   - The person is bouncing and picking up orange balls, likely basketballs. 2.Shooting basketball    - Shooting a basketball aligns with the person holding balls and the mention of basketball in the transcript. 3.Playing basketball    - The overall context points to playing basketball as an action. 4.Catching or throwing basketball   - The person holds balls and simulating catching or throwing a basketball would match. 5.Bouncing basketball   - Bouncing a basketball is directly mentioned in the transcript and would match the video description.
Explainable outputPrompt: Think step by step what the most likely category is given this video information and these categories {LABELS}. Answer with the five most likely categories:
Speech Recognition“Alright, alright, go here.”Visual  Captioning    “a boy is holding two orange balls” 
Audio Tags“Basketball bounce”    “a boy is holding two orange balls”   “a boy is holding two orange balls” 
Figure 1. Our method combines a “perception” module, which uses visual and auditory models to get multimodal textual descriptors as
sensory proxies for “sight” and “hearing”, and a “reasoning” module that processes these textual inputs to form a coherent narrative and
identify the likeliest content in the video, completed by justifications.
Captioner [8] explore similar textual interactions between
models. While LENS only explores the ability of LLMs
to reason over visual question answering tasks given visual
clues about images, Video ChatCaptioner proposes chain-
ing together BLIP-2 and ChatGPT in order to have conver-
sations about images. Our method goes beyond just ques-
tion answering tasks, demonstrating that both visual and au-
ditory clues can be used by LLMs for video classification.
In summary, our contributions are: 1) We introduce a
new multimodal classification approach consisting of two
phases: a “perception” phase where models act as sen-
sory proxies and a “reasoning” phase that consolidates mul-
timodal textual inputs into a coherent narrative. 2) We
demonstrate the efficacy of text as the primary medium of
interpreting multimodal data. 3) For the first time, we show-
case that textual representations of visual and auditory cues
alone can effectively classify actions within videos.
2. Method
Perception models To extract visual captions from video
frames, we use the BLIP-2 [13] model. We process only 5
equidistant frames per video to ensure a diverse sampling
of the video content. We use Whisper [16] to obtain au-
dio transcripts, specifically the Faster Whisper version [1]
which has been optimised for fast inference. We use a tem-
perature of 0, a beam size of 5 and the V AD filter to exclude
the parts of the video that don’t have any speech. In order to
generate audio tags, we leverage ImageBind [9] to get au-
dio embeddings and compute the similarity with the textual
embeddings of the AudioSet labels. We then only select the
labels that have a similarity over a certain threshold, whichcan be obtained by qualitatively checking a few examples.
Reasoning models For our reasoning module, we test
out 3 different state-of-the-art large language models. We
use the GPT completion API, specifically the GPT3.5-turbo
version [2]. Additionally, we use the newly launched func-
tion calling feature which allows the user to specify a json
schema for the output. The second LLM we evaluate is
Claude-instant-1 [3], which has reported similar perfor-
mance and capabilities to GPT3.5-turbo. For Llama2 we
use the Llama-2-13b-chat variant [18], which has 13
Billion parameters and is specialised for conversation. We
use a temperature of 0 or near 0 for all reasoning models to
ensure more consistent outputs that are able to better adhere
to the instructions given. The prompts we use for classi-
fication follow this simple template with slight variations
among the different LLMs to accomodate specific prompt
guidelines: Given this {multimodal clues }and these action
recognition labels: {labels }Please return the 5 labels that
apply the most to the video in a json format, from the most
likely to the least likely.
Structured Output LLMs usually generate free-flowing
natural language outputs, however, for the task of classifica-
tion we want the model to provide us with 5 ranked guesses
from a set of pre-defined class names. To accomplish this
with GPT we use the function calling API, providing the
model with a JSON Schema of the function to call, where
the schema contains an enum of the possible class names.
For Claude, we provide the class names in the prompt
and ask for the results to be returned as JSON, which, in

--- PAGE 3 ---
UCF-101 K400*
Model 1-Acc. 5-Acc. 1-Acc. 5-Acc.
BLIP2+Claude-1(caps) 63.01 85.35 38.90 54.20
BLIP2+Claude-1(caps, speech) 67.06 86.13 41.20 57.00
BLIP2+Claude-1(caps, speech, audio) 67.13 86.15 41.20 57.35
Table 1. Comparing different levels of context on the UCF101 test
set and a subset of Kinetics400 using Claude-instant-1.
the majority of cases, results in a JSON object with a “la-
bels” key containing a list of most likely labels, or an object
whose keys are the classes and values are the rank.
For Llama2, we provide class names in the System
prompt, and observe that predictions are usually included as
a numbered list in the output, hence we simply parse lines
beginning with a number. To compare with ground truth,
we normalise to remove spaces and convert to lowercase.
For all models, occasionally the output cannot be parsed
(such as hallucinated class names or extra characters), and
in this case we consider the prediction to be incorrect.
3. Evaluation Datasets
UCF101 The UCF-101 test set comprises of 13,320 short
video clips from YouTube spanning 101 action categories,
providing a diverse set of everyday human actions, ranging
from playing instruments to sports activities.
Kinetics400 The Kinetics400 test set contains 10s
Youtube video clips and 400 human action classes. In order
to circumvent API costs, since the test set contains 38,685
video clips, we construct a smaller representative subset of
2000 videos, sampling 5 videos per category.
4. Experiments
First, we run experiments to see the role of each modality
in classifying videos on the UCF-101 test and the 2k sub-
set of Kinetics400. As Table 1 shows, the language model is
able to benefit from additional audio information. In Table 2
we compare how well the 3 different large language models
used are able to interpret the visual and auditory information
given. We find that both GPT3.5-turbo and Claude-instant-1
outperform Llama2, with Claude-instant-1 obtaining on av-
erage the highest accuracy. In Figure 2 we test the effect of
including more or less frame captions. Interestingly, while
both GPT3.5 and Claude-1 benefit from ”seeing” more cap-
tions, LLama2’s performance is negatively affected. We hy-
pothesise this is due to the model becoming overwhelmed
with the redundant information, making it more likely to
pick a word from the captions rather than the label list.
5. Discussion and Future Work
Using separate models to translate vision and speech into
text might result in the inability to model interactions be-
60.5964.1565.74 66.3667.7277.8280.5984.35 84.9586.73
54.65
49.547.72 47.33 46.4463.17
55.8457.0355.6453.27
#framesTop-k Accuracy
20406080100
1 2 3 4 5Claude1 top-1 Claude1 top-5 Llama2 top-1 Llama2 top-5
GPT3.5 top-1 GPT3.5 top-5Figure 2. Comparing the ability of language models to filter con-
text from captions corresponding to a varying number of frames.
While Claude1 and GPT3.5 are able to leverage the extra informa-
tion, it seems that the extra information overwhelms Llama2-13B.
Model Top-1 acc. Top-3 acc. Top-5 acc.
BLIP2(FlanT5-XXL)+Llama2-13B 49.56 56.70 58.51
BLIP2(FlanT5-XXL)+GPT3.5 66.37 79.27 82.04
BLIP2(FlanT5-XXL)+Claude-1 63.01 81.49 85.35
Table 2. Comparing varying LLMs on the UCF101 test set.
tween modalities, resulting in an incomplete context. Im-
age analysis on a frame by frame basis also lacks proper
temporal modeling of persistent identities and relationships
over time. Generative models are prone to hallucinations
and unreliable outputs, therefore relying on them for com-
plex tasks that require consistent outputs might prove chal-
lenging in practice. Moreover, by only providing the text
of the class names to the LLM, we are relying on the class
names being sufficiently descriptive. For example, UCF-
101 contains classes related to very specific musical instru-
ments such as PlayingDhol orPlayingTabla , which
would require the LLM to know that these are percussion
instruments and would also require the captioning model to
be able to give enough granular detail to distinguish these
instruments. As Figure 3 shows, the models struggle with
actions which are harder to determine from frame informa-
tion alone, such as headmassage or more obscure ob-
jects, such as nunchuks oryoyo . Failure modes usually
include the inability of the visual captioning model to recog-
nise the level of specificity needed to differentiate between
similar actions. Finally, even though our method is not quite
competitive with the state of the art zero-shot performance
of 91.5 Top-1 Accuracy on UCF-101 and 76.8 on Kinet-
ics400 [4], it is more generalizable to video understanding
scenarios that require complex contextual reasoning.
Future work includes leveraging additional context, such
as video comments [10] or employing a chat-based ap-
proach where the “reasoning” module can ask the “percep-
tion” module for clarification to get more information.

--- PAGE 4 ---
drumming
playingviolinplayingpianoplayingflutefencingrowingsurfing
skateboardingplayingcellobasketballskiingbowling
balancebeampunchtyping...
baseballpitch
rockclimbingindoortaichi
fieldhockeypenaltymoppingfloorbilliards
soccerpenaltyicedancing
cuttinginkitchenwalkingwithdogdivingarchery
hammering
volleyballspiking...
jumpingjack
boxingspeedbagjavelinthrowplayingdhol
basketballdunkcleanandjerkheadmassagelunges
handstandpushupsnunchucks
hammerthrowpommelhorseplayingdafshotputyoyo
Label0.00.20.40.60.81.0T op-1 AccuracyLlama2
GPT-3.5
Claude-1Figure 3. Best and worst performing UCF-101 classes among the 3 language models used: Claude-instant-1, GPT3.5-turbo, Llama2-13B.
6. Conclusion
In this work, we have introduced a new framework for
multimodal video classification that leverages text as the
primary medium for combining signals across modalities.
We demonstrate for the first time that chaining together per-
ception models for vision, speech and audio with large lan-
guage models can enable zero-shot video classification us-
ing only textual representations of multimodal signals. Our
work highlights the potential of using natural language as a
flexible interface for integrating signals across modalities.
References
[1]https://github.com/guillaumekln/
faster-whisper . 2
[2]https://platform.openai.com/docs/guides/
gpt/chat-completions-api . 2
[3]https://www.anthropic.com/index/
introducing-claude . 2
[4] H. Akbari, D. Kondratyuk, Y . Cui, R. Hornung, H. Wang,
and H. Adam. Alternating gradient descent and mixture-of-
experts for integrated multimodal perception. arXiv preprint
arXiv:2305.06324 , 2023. 3
[5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Has-
son, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al.
Flamingo: a visual language model for few-shot learning.
NeurIPS , 2022. 1
[6] W. Berrios, G. Mittal, T. Thrush, D. Kiela, and A. Singh.
Towards language models that can see: Computer vision
through the lens of natural language. arXiv:2306.16410 ,
2023. 1
[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P.
Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,
et al. Language models are few-shot learners. NeurIPS ,
2020. 1
[8] J. Chen, D. Zhu, K. Haydarov, X. Li, and M. Elhoseiny.
Video chatcaptioner: Towards the enriched spatiotemporal
descriptions. arXiv:2304.04227 , 2023. 2[9] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V . Alwala, A.
Joulin, and I. Misra. Imagebind: One embedding space to
bind them all. In CVPR , 2023. 2
[10] L. Hanu, J. Thewlis, Y . M. Asano, and C. Rupprecht. Vtc:
Improving video-text retrieval with user comments. In
ECCV , 2022. 3
[11] S. Huang, L. Dong, W. Wang, Y . Hao, S. Singhal, S. Ma, T.
Lv, L. Cui, O. K. Mohammed, Q. Liu, et al. Language is
not all you need: Aligning perception with language models.
arXiv:2302.14045 , 2023. 1
[12] A. Jaegle, S. Borgeaud, J.-B. Alayrac, C. Doersch, C.
Ionescu, D. Ding, S. Koppula, D. Zoran, A. Brock, E. Shel-
hamer, et al. Perceiver IO: A general architecture for struc-
tured inputs & outputs. arXiv:2107.14795 , 2021. 1
[13] J. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: Bootstrapping
language-image pre-training with frozen image encoders and
large language models. arXiv:2301.12597 , 2023. 1, 2
[14] OpenAI. GPT-4 technical report, 2023. 1
[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S.
Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.
Learning transferable visual models from natural language
supervision. In ICML , 2021. 1
[16] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey,
and I. Sutskever. Robust speech recognition via large-scale
weak supervision. In ICML , 2023. 2
[17] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,
R. Castagn ´e, A. S. Luccioni, F. Yvon, M. Gall ´e, et al. Bloom:
A 176b-parameter open-access multilingual language model.
arXiv:2211.05100 , 2022. 1
[18] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,
et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv:2307.09288 , 2023. 1, 2
[19] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-
4: Enhancing vision-language understanding with advanced
large language models. arXiv:2304.10592 , 2023. 1
