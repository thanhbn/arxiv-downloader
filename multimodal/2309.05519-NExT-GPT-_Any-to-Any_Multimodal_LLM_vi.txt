# 2309.05519.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2309.05519.pdf
# Kích thước file: 9056510 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
NExT-GPT: Mô hình Ngôn ngữ Lớn Đa phương thức Bất kỳ-sang-Bất kỳ
Shengqiong Wu1Hao Fei1Leigang Qu1Wei Ji1Tat-Seng Chua1
Tóm tắt
Trong khi gần đây các Mô hình Ngôn ngữ Lớn Đa phương thức (MM-LLMs) đã có những bước tiến thú vị, chúng phần lớn bị giới hạn bởi khả năng hiểu đa phương thức chỉ ở phía đầu vào, mà không có khả năng tạo ra nội dung theo nhiều phương thức. Khi con người luôn nhận thức thế giới và giao tiếp với mọi người thông qua các phương thức khác nhau, việc phát triển các MM-LLMs bất kỳ-sang-bất kỳ có khả năng chấp nhận và truyền đạt nội dung trong bất kỳ phương thức nào trở nên thiết yếu cho AI cấp độ con người. Để lấp đầy khoảng trống này, chúng tôi trình bày một hệ thống MM-LLM bất kỳ-sang-bất kỳ tổng quát từ đầu đến cuối, NExT-GPT. Chúng tôi kết nối một LLM với các bộ điều hợp đa phương thức và các bộ giải mã khuếch tán khác nhau, cho phép NExT-GPT nhận thức đầu vào và tạo ra đầu ra trong các kết hợp tùy ý của văn bản, hình ảnh, video và âm thanh. Bằng cách tận dụng các bộ mã hóa và giải mã hiệu suất cao đã được huấn luyện tốt hiện có, NExT-GPT chỉ được điều chỉnh với một lượng nhỏ tham số (1%) của các lớp chiếu nhất định, điều này không chỉ có lợi cho việc huấn luyện chi phí thấp mà còn tạo điều kiện thuận lợi cho việc mở rộng sang nhiều phương thức tiềm năng hơn. Hơn nữa, chúng tôi giới thiệu một phương pháp điều chỉnh hướng dẫn chuyển đổi phương thức (MosIT) và tuyển chọn thủ công một bộ dữ liệu chất lượng cao cho MosIT, dựa trên đó NExT-GPT được trao quyền với khả năng hiểu ngữ nghĩa đa phương thức phức tạp và tạo ra nội dung. Nhìn chung, nghiên cứu của chúng tôi cho thấy khả năng hứa hẹn của việc xây dựng một tác nhân AI thống nhất có khả năng mô hình hóa các phương thức phổ quát, mở đường cho nghiên cứu AI giống con người hơn trong cộng đồng. Trang web dự án: https://next-gpt.github.io/

1. Giới thiệu
Gần đây, chủ đề về Nội dung Được Tạo ra bởi Trí tuệ Nhân tạo (AIGC) đã chứng kiến những tiến bộ chưa từng có với các công nghệ nhất định, như ChatGPT cho việc tạo văn bản (OpenAI, 2022a) và các mô hình khuếch tán cho việc tạo hình ảnh (Fan et al., 2022). Trong số này, sự nổi lên của các Mô hình Ngôn ngữ Lớn (LLMs) đã đặc biệt đáng chú ý, ví dụ như Flan-T5 (Chung et al., 2022), Vicuna (Chiang et al., 2023), LLaMA (Touvron et al., 2023) và Alpaca (Taori et al., 2023), thể hiện khả năng lý luận ngôn ngữ và ra quyết định ở mức độ con người đáng kinh ngạc của chúng, soi sáng con đường của Trí tuệ Nhân tạo Tổng quát (AGI).

Thế giới của chúng ta vốn dĩ là đa phương thức, và con người nhận thức thế giới với các cơ quan cảm giác khác nhau cho thông tin phương thức đa dạng, như ngôn ngữ, hình ảnh, video và âm thanh, những thứ thường bổ sung và tạo ra sự hiệp đồng với nhau. Với trực giác như vậy, các LLM chỉ dựa trên văn bản gần đây đã được trang bị khả năng hiểu và nhận thức phương thức khác như hình ảnh, video, âm thanh, v.v.

Một phương pháp đáng chú ý bao gồm việc sử dụng các bộ điều hợp để căn chỉnh các bộ mã hóa đã được huấn luyện trước trong các phương thức khác với các LLM văn bản. Nỗ lực này đã dẫn đến sự phát triển nhanh chóng của các LLM đa phương thức (MM-LLMs), như BLIP-2 (Li et al., 2023c), Flamingo (Alayrac et al., 2022), MiniGPT-4 (Zhu et al., 2023), Video-LLaMA (Zhang et al., 2023c), LLaVA (Liu et al., 2023b), PandaGPT (Su et al., 2023), và SpeechGPT (Zhang et al., 2023b). Tuy nhiên, hầu hết các nỗ lực này chú ý đến việc hiểu nội dung đa phương thức ở phía đầu vào. Gần đây, ít công trình hơn đã xem xét việc tạo ra đa phương thức, như Emu (Sun et al., 2023), DREAM LLM (Dong et al., 2023), GILL (Koh et al., 2023), SEED (Ge et al., 2023). Đáng chú ý, các mô hình này bị giới hạn trong việc tạo ra văn bản và hình ảnh xen kẽ. Chúng tôi nhấn mạnh rằng nhận thức và giao tiếp tự nhiên của con người một cách không thể thiếu đòi hỏi sự chuyển đổi liền mạch giữa bất kỳ phương thức thông tin nào. Điều này khiến cho việc khám phá các MM-LLMs bất kỳ-sang-bất kỳ trở nên quan trọng, tức là khả năng chấp nhận đầu vào trong bất kỳ phương thức nào và truyền đạt phản hồi trong bất kỳ phương thức thích hợp nào.

Một số nỗ lực đã được thực hiện để bắt chước việc chuyển đổi phương thức bất kỳ-sang-bất kỳ giống con người. Gần đây, CoDi (Tang et al., 2023) đã có những bước tiến trong việc thực hiện khả năng xử lý và tạo ra đồng thời các kết hợp tùy ý của các phương thức; tuy nhiên, nó thiếu sức mạnh lý luận và ra quyết định của LLMs làm cốt lõi, và cũng bị giới hạn trong việc tạo ra nội dung ghép đôi đơn giản. Mặt khác, một số nỗ lực, ví dụ như Visual-ChatGPT (Wu et al., 2023) và HuggingGPT (Shen et al., 2023), đã tìm cách kết hợp LLMs với các công cụ bên ngoài để đạt được khả năng hiểu và tạo ra đa phương thức 'bất kỳ-sang-bất kỳ' gần đúng. Thật không may, các hệ thống này gặp phải những thách thức nghiêm trọng do kiến trúc pipeline hoàn chỉnh của chúng. Đầu tiên, việc truyền thông tin giữa các mô-đun khác nhau hoàn toàn dựa trên các văn bản rời rạc được tạo ra bởi LLM, nơi quá trình nối tiếp không tránh khỏi gây ra tiếng ồn và lan truyền lỗi. Quan trọng hơn, toàn bộ hệ thống tận dụng các công cụ đã được huấn luyện trước hiện có chỉ để suy luận. Do thiếu việc huấn luyện từ đầu đến cuối tổng thể, khả năng hiểu nội dung và tạo ra đa phương thức có thể rất hạn chế, đặc biệt trong việc diễn giải các hướng dẫn của người dùng phức tạp và ngầm ẩn. Tóm lại, có một nhu cầu cấp thiết để xây dựng một MM-LLM từ đầu đến cuối với các phương thức tùy ý.

Theo đuổi mục tiêu này, chúng tôi trình bày NExT-GPT, một MM-LLM bất kỳ-sang-bất kỳ được thiết kế để xử lý liền mạch đầu vào và đầu ra trong bất kỳ kết hợp nào của bốn phương thức: văn bản, hình ảnh, video và âm thanh. Như được mô tả trong Hình 1, NExT-GPT bao gồm ba tầng. Đầu tiên, chúng tôi tận dụng các bộ mã hóa đã được thiết lập để mã hóa đầu vào trong các phương thức khác nhau, nơi những biểu diễn này được chiếu thành các biểu diễn giống ngôn ngữ có thể hiểu được bởi LLM thông qua một lớp chiếu. Thứ hai, chúng tôi khai thác một LLM mã nguồn mở hiện có làm cốt lõi để xử lý thông tin đầu vào cho việc hiểu và lý luận ngữ nghĩa. LLM không chỉ tạo ra trực tiếp các token văn bản mà còn tạo ra các token 'tín hiệu phương thức' độc đáo phục vụ như hướng dẫn để chỉ đạo các lớp giải mã về việc liệu có và nội dung phương thức nào cần xuất ra tương ứng. Thứ ba, sau khi chiếu, các tín hiệu đa phương thức được tạo ra với các hướng dẫn cụ thể được định tuyến đến các bộ mã hóa khác nhau và cuối cùng tạo ra nội dung trong các phương thức tương ứng.

Vì NExT-GPT bao gồm việc mã hóa và tạo ra các phương thức khác nhau, việc huấn luyện hệ thống từ đầu sẽ đòi hỏi chi phí đáng kể. Thay vào đó, chúng tôi tận dụng các bộ mã hóa và giải mã hiệu suất cao đã được huấn luyện trước hiện có, như CLIP (Radford et al., 2021), ImageBind (Girdhar et al., 2023) và các mô hình khuếch tán tiềm ẩn tiên tiến (Rombach et al., 2022; Ruiz et al., 2022; Cerspense, 2023; An et al., 2023; Liu et al., 2023a; Huang et al., 2023a). Bằng cách tải các tham số có sẵn, chúng tôi không chỉ tránh được việc huấn luyện khởi động lạnh mà còn tạo điều kiện cho sự phát triển tiềm năng của nhiều phương thức hơn. Để căn chỉnh tính năng qua ba tầng, chúng tôi chỉ xem xét việc điều chỉnh tinh cục bộ các lớp chiếu đầu vào và đầu ra, với việc căn chỉnh lấy LLM làm trung tâm ở phía mã hóa và căn chỉnh tuân theo hướng dẫn ở phía giải mã, nơi chi phí tính toán tối thiểu đảm bảo hiệu quả cao hơn. Hơn nữa, để trao quyền cho MM-LLM bất kỳ-sang-bất kỳ của chúng tôi với khả năng ở mức độ con người trong việc tạo ra và lý luận đa phương thức phức tạp, chúng tôi giới thiệu một phương pháp điều chỉnh hướng dẫn chuyển đổi phương thức, để trang bị cho hệ thống khả năng hiểu ngữ nghĩa đa phương thức tinh vi và tạo ra nội dung. Để chống lại sự vắng mặt của dữ liệu điều chỉnh hướng dẫn đa phương thức như vậy trong cộng đồng, chúng tôi thu thập và chú thích thủ công một bộ dữ liệu MosIT bao gồm 5.000 mẫu chất lượng cao. Bằng cách sử dụng kỹ thuật LoRA (Hu et al., 2022), chúng tôi điều chỉnh tinh toàn bộ hệ thống NExT-GPT trên dữ liệu điều chỉnh hướng dẫn, cập nhật cả các lớp chiếu đầu vào và đầu ra và một số tham số LLM nhất định.

Nhìn chung, công trình này thể hiện khả năng hứa hẹn của việc phát triển một tác nhân MM-LLM giống con người hơn có khả năng mô hình hóa các phương thức phổ quát. Các đóng góp của nghiên cứu này bao gồm:

• Chúng tôi, lần đầu tiên, trình bày một MM-LLM bất kỳ-sang-bất kỳ tổng quát từ đầu đến cuối, có tên NExT-GPT, có khả năng hiểu và lý luận ngữ nghĩa và tạo ra các kết hợp đầu vào và đầu ra tự do của văn bản, hình ảnh, video và âm thanh.

• Chúng tôi giới thiệu các kỹ thuật học căn chỉnh nhẹ, căn chỉnh lấy LLM làm trung tâm ở phía mã hóa, và căn chỉnh tuân theo hướng dẫn ở phía giải mã, chỉ yêu cầu điều chỉnh tham số tối thiểu một cách hiệu quả (chỉ 1% tham số) trong khi duy trì căn chỉnh ngữ nghĩa hiệu quả cao.

• Chúng tôi chú thích một bộ dữ liệu điều chỉnh hướng dẫn chuyển đổi phương thức chất lượng cao bao gồm các hướng dẫn phức tạp qua các kết hợp phương thức khác nhau của văn bản, hình ảnh, video và âm thanh, hỗ trợ MM-LLM với khả năng hiểu và lý luận nội dung đa phương thức giống con người.

2. Công trình Liên quan
Hiểu và Tạo ra Đa phương thức Thế giới của chúng ta tràn ngập thông tin đa phương thức, trong đó chúng ta liên tục tham gia vào nhiệm vụ phức tạp của việc hiểu và tạo ra nội dung đa phương thức. Cộng đồng AI tương ứng xuất hiện các hình thức đa dạng của các nhiệm vụ học đa phương thức (Zeng et al., 2023; Dessì et al., 2023; Yang et al., 2021; Ding et al., 2021; Liu et al., 2023a; Dorkenwald et al., 2021). Hơn nữa, để tạo ra nội dung chất lượng cao, một loạt các phương pháp hiệu suất mạnh đã được đề xuất, như Transformer (Vaswani et al., 2017; Zhang et al., 2022; Ding et al., 2021; Ge et al., 2022), GANs (Liu et al., 2020; Brock et al., 2019; Xu et al., 2018; Zhu et al., 2019), VAEs (Vahdat & Kautz, 2020; Razavi et al., 2019), các mô hình Flow (Shibata et al., 2022; Bashiri et al., 2021) và các mô hình khuếch tán tiên tiến hiện tại (Hoogeboom et al., 2021; Qu et al., 2023b; Mou et al., 2023; Feng et al., 2022; Rombach et al., 2022). Đặc biệt, các phương pháp dựa trên khuếch tán gần đây đã mang lại hiệu suất đáng kể trong rất nhiều nhiệm vụ tạo ra đa phương thức, như DALL-E (Ramesh et al., 2021), Stable Diffusion (Rombach et al., 2022). Trong khi tất cả các nỗ lực trước đây của học đa phương thức đều bị giới hạn trong việc hiểu các đầu vào đa phương thức, CoDi (Tang et al., 2023) gần đây đã trình bày sự phát triển đột phá. Tận dụng sức mạnh của các mô hình khuếch tán, CoDi có khả năng tạo ra bất kỳ kết hợp nào của các phương thức đầu ra, bao gồm ngôn ngữ, hình ảnh, video hoặc âm thanh, từ bất kỳ kết hợp nào của các phương thức đầu vào song song. Thật đáng tiếc, CoDi vẫn thiếu khả năng lý luận sâu sắc giống con người về nội dung đầu vào, vì nó chỉ có thể thực hiện việc cấp dữ liệu và tạo ra đa phương thức song song mà không có bất kỳ khả năng lý luận và đánh dấu quyết định nào.

Các Mô hình Ngôn ngữ Lớn Đa phương thức LLMs đã tạo ra tác động và cách mạng sâu sắc đối với toàn bộ cộng đồng AI và hơn thế nữa (OpenAI, 2022a;b), nơi một loạt các LLM mã nguồn mở đã thúc đẩy mạnh mẽ sự tiến bộ và đóng góp cho cộng đồng (Chiang et al., 2023; Touvron et al., 2023; Zhu et al., 2023; Zhang et al., 2023a). Dựa trên những LLM này, những nỗ lực đáng kể đã được thực hiện để mở rộng chúng để xử lý các đầu vào và nhiệm vụ đa phương thức, dẫn đến sự phát triển của MM-LLMs. Một mặt, hầu hết các nhà nghiên cứu xây dựng các MM-LLM cơ bản bằng cách căn chỉnh các bộ mã hóa đã được huấn luyện tốt của các phương thức khác nhau với không gian đặc trưng văn bản của LLMs để nhận thức các đầu vào phương thức khác (Huang et al., 2023c; Zhu et al., 2023; Su et al., 2022; Koh et al., 2023). Ví dụ, Flamingo (Alayrac et al., 2022) sử dụng một lớp cross-attention để kết nối một bộ mã hóa hình ảnh đông lạnh với LLMs. BLIP-2 (Li et al., 2023c) sử dụng một Q-Former để dịch các truy vấn hình ảnh đầu vào sang LLMs. Cũng có nhiều thực hành tương tự khác để xây dựng MM-LLMs có khả năng hiểu video (ví dụ, Video-Chat (Li et al., 2023d) và Video-LLaMA (Zhang et al., 2023c)), âm thanh (ví dụ, SpeechGPT (Zhang et al., 2023b)), v.v. Đặc biệt, PandaGPT (Su et al., 2023) đạt được sự hiểu biết toàn diện về sáu phương thức khác nhau đồng thời bằng cách tích hợp bộ mã hóa đa phương thức, tức là ImageBind (Girdhar et al., 2023).

Tuy nhiên, những MM-LLM này đều bị giới hạn trong việc chỉ nhận thức dữ liệu đa phương thức, mà không có khả năng tạo ra nội dung trong các phương thức tùy ý. Để cho phép LLMs với cả đầu vào và đầu ra đa phương thức, một số nỗ lực khám phá việc sử dụng LLMs như những người ra quyết định, và sử dụng các bộ mã hóa và giải mã đa phương thức có sẵn như công cụ để thực hiện đầu vào và đầu ra đa phương thức, như Visual-ChatGPT (Wu et al., 2023), HuggingGPT (Shen et al., 2023), và AudioGPT (Huang et al., 2023b). Như đã đề cập trước đó, việc truyền tin nhắn giữa các mô-đun bằng văn bản thuần túy (tức là hướng dẫn văn bản LLM) dưới sơ đồ pipeline rời rạc sẽ không tránh khỏi gây ra tiếng ồn. Ngoài ra, việc thiếu điều chỉnh toàn diện trên toàn bộ hệ thống làm hạn chế đáng kể hiệu quả của việc hiểu ngữ nghĩa. Công trình của chúng tôi lấy những lợi ích tương hỗ của cả hai loại trên, tức là học một MM-LLM bất kỳ-sang-bất kỳ theo cách từ đầu đến cuối.

3. Kiến trúc Tổng thể
Hình 1 trình bày tổng quan sơ đồ của khung NExT-GPT, bao gồm ba giai đoạn chính: mã hóa, hiểu và lý luận LLM, và giải mã.

Giai đoạn Mã hóa Đa phương thức Đầu tiên, chúng tôi tận dụng các mô hình đã được thiết lập tốt hiện có để mã hóa đầu vào của các phương thức khác nhau. Có một tập hợp các lựa chọn thay thế của các bộ mã hóa cho các phương thức khác nhau, ví dụ như CLIP (Radford et al., 2021), HuBERT (Hsu et al., 2021). Ở đây chúng tôi tận dụng ImageBind (Girdhar et al., 2023), đây là một bộ mã hóa hiệu suất cao thống nhất qua sáu phương thức. Với ImageBind, chúng tôi được tránh khỏi việc quản lý nhiều bộ mã hóa phương thức không đồng nhất. Sau đó, thông qua một lớp chiếu, các biểu diễn đầu vào khác nhau được ánh xạ thành các biểu diễn giống ngôn ngữ có thể hiểu được bởi LLM.

Giai đoạn Hiểu và Lý luận LLM Một LLM được sử dụng làm tác nhân cốt lõi của NExT-GPT. Về mặt kỹ thuật, chúng tôi sử dụng Vicuna (7B-v0) (Chiang et al., 2023), đây là LLM dựa trên văn bản mã nguồn mở được sử dụng rộng rãi trong các MM-LLM hiện có (Su et al., 2023; Zhang et al., 2023c). LLM nhận đầu vào là các biểu diễn từ các phương thức khác nhau và thực hiện hiểu ngữ nghĩa và lý luận trên các đầu vào. Nó xuất ra: 1) các phản hồi văn bản trực tiếp, và 2) các token tín hiệu của mỗi phương thức phục vụ như hướng dẫn để chỉ đạo các lớp giải mã về việc có tạo ra nội dung đa phương thức hay không và nội dung gì cần tạo ra nếu có.

Giai đoạn Tạo ra Đa phương thức Nhận các tín hiệu đa phương thức với các hướng dẫn cụ thể từ LLM (nếu có), các lớp chiếu đầu ra dựa trên Transformer ánh xạ các biểu diễn token tín hiệu thành những biểu diễn có thể hiểu được bởi các bộ giải mã đa phương thức sau đó. Về mặt kỹ thuật, chúng tôi sử dụng các mô hình khuếch tán có điều kiện tiềm ẩn hiện tại có sẵn của các thế hệ phương thức khác nhau, tức là Stable Diffusion (SD-v1.5) để tổng hợp hình ảnh (Rombach et al., 2022), Zeroscope (v2-576w) để tổng hợp video (Cerspense, 2023), và AudioLDM (l-full) để tổng hợp âm thanh (Liu et al., 2023a). Sau một lớp chiếu, các biểu diễn tín hiệu được cấp vào các mô hình khuếch tán có điều kiện để tạo ra nội dung. Trong Bảng 1, chúng tôi tóm tắt các cấu hình hệ thống tổng thể. Đáng chú ý là trong toàn bộ hệ thống, chỉ các lớp chiếu đầu vào và đầu ra với tham số quy mô nhỏ hơn (so với khung khả năng lớn tổng thể) mới được yêu cầu cập nhật trong quá trình học sau đó, với tất cả các bộ mã hóa và giải mã khác đều đông lạnh. Điều này tương đương với 155M(=28+33+31+31+32) / [155M + 12.275B(=1.2+7+1.3+1.8+0.975)], hoặc chỉ 1% tham số cần được cập nhật. Đây cũng là một trong những ưu điểm chính của MM-LLM của chúng tôi.

4. Học Căn chỉnh Đa phương thức Nhẹ
Để thu hẹp khoảng cách giữa không gian đặc trưng của các phương thức khác nhau, và đảm bảo hiểu ngữ nghĩa trôi chảy của các đầu vào khác nhau, việc thực hiện học căn chỉnh cho NExT-GPT là cần thiết. Vì chúng tôi thiết kế hệ thống kết hợp lỏng lẻo với chủ yếu ba tầng, chúng tôi chỉ cần cập nhật hai lớp chiếu ở phía mã hóa và phía giải mã.

4.1. Căn chỉnh Đa phương thức Lấy LLM làm Trung tâm Phía Mã hóa
Hầu hết các MM-LLM hiện có áp dụng các bộ mã hóa đa phương thức có kiến trúc Transformer và tạo ra các đặc trưng lưới mức patch (ví dụ, cho hình ảnh, âm thanh hoặc video). Chúng chuyển đổi các đặc trưng đa phương thức để có thể hiểu được bởi LLM cốt lõi bằng cách chiếu chúng vào không gian đặc trưng văn bản một cách trực tiếp thông qua các lớp tuyến tính. Tuy nhiên, chúng tôi lưu ý rằng các đơn vị đặc trưng dựa trên patch có thể không phù hợp nhất với ngữ nghĩa token văn bản phức tạp, vì trực giác các token ngôn ngữ luôn đóng gói các khái niệm riêng biệt. Điều này có thể dẫn đến nhận thức thông tin không tối ưu (Zhong et al., 2022) trong MM-LLMs. Do đó, được truyền cảm hứng bởi (Xu et al., 2022), chúng tôi thiết kế một loại token khái niệm có thể học để tổng hợp phân cấp các đặc trưng mức lưới thành các token khái niệm ngữ nghĩa thông qua một cơ chế nhóm, và sau đó biểu diễn khái niệm được cấp vào LLM.

Để hoàn thành việc căn chỉnh, chúng tôi áp dụng nhiệm vụ tạo ra 'X-sang-văn bản' được huấn luyện trên dữ liệu cặp 'X-chú thích' ('X' đại diện cho hình ảnh, âm thanh hoặc video) từ các kho và điểm chuẩn hiện có, tức là, cho biểu diễn của một 'X', để nhắc LLM đông lạnh tạo ra mô tả văn bản tương ứng. Cụ thể, chúng tôi sử dụng ba loại dữ liệu cặp 'X-chú thích', bao gồm 1) bộ dữ liệu cặp 'Video-chú thích': Webvid-2M (Bain et al., 2021), một bộ dữ liệu quy mô lớn về các video ngắn với mô tả văn bản có nguồn gốc từ các trang web cổ phiếu phim, 2) bộ dữ liệu cặp 'Hình ảnh-chú thích': CC3M (Sharma et al., 2018), chứa hơn 3 triệu hình ảnh kèm theo các mô tả ngôn ngữ tự nhiên đa dạng, và 3) bộ dữ liệu cặp 'Âm thanh-chú thích': AudioCaps (Kim et al., 2019), một bộ dữ liệu mở rộng gồm khoảng 46k clip âm thanh được ghép với các mô tả văn bản do con người viết được thu thập thông qua crowdsourcing. Hình 2(a) minh họa quá trình học.

4.2. Căn chỉnh Tuân theo Hướng dẫn Phía Giải mã
Ở phía giải mã, chúng tôi đã tích hợp các mô hình khuếch tán có điều kiện đã được huấn luyện trước từ các nguồn bên ngoài. Mục đích chính của chúng tôi là căn chỉnh các mô hình khuếch tán với các hướng dẫn đầu ra của LLM. Tuy nhiên, việc thực hiện một quá trình căn chỉnh quy mô đầy đủ giữa mỗi mô hình khuếch tán và LLM sẽ đòi hỏi gánh nặng tính toán đáng kể. Thay vào đó, chúng tôi khám phá một phương pháp hiệu quả hơn, căn chỉnh tuân theo hướng dẫn phía giải mã, như được mô tả trong Hình 2(b).

Cụ thể, thay vì xuất ra các hướng dẫn văn bản trực tiếp, chúng tôi thiết kế ba loại token đặc biệt (Koh et al., 2023), tức là '[IMGi]' (i= 0,···,4) như token tín hiệu hình ảnh; '[AUDi]' (i= 0,···,8) như token tín hiệu âm thanh; và '[VIDi]' (i= 0,···,24) như token tín hiệu video; những token này mang ngầm các hướng dẫn phong phú và linh hoạt cho mô hình khuếch tán phía sau. Chúng tôi muốn cho phép LLM học được nội dung gì cần tạo ra, tức là các token văn bản, và các token tín hiệu phương thức. Nếu LLM xác định một nội dung phương thức nhất định cần được tạo ra, một loại token đặc biệt sẽ được xuất ra chỉ báo sự kích hoạt của phương thức đó; ngược lại, không có đầu ra token đặc biệt có nghĩa là vô hiệu hóa phương thức đó.

Chúng tôi nhận thấy rằng các mô hình khuếch tán tạo ra nội dung chỉ dựa vào các biểu diễn định hướng văn bản, tức là từ các bộ mã hóa văn bản khuếch tán. Tuy nhiên, việc điều kiện hóa tập trung vào văn bản này khác biệt đáng kể so với các token tín hiệu phương thức trong LLM của chúng tôi. Điều này dẫn đến một khoảng cách ngăn cản các mô hình khuếch tán diễn giải chính xác các hướng dẫn từ LLM. Do đó, một mặt, chúng tôi xem xét việc lấy các biểu diễn token tín hiệu phương thức của LLM (sau mỗi lớp dự án dựa trên Transformer) như một đầu vào có điều kiện trong quá trình khử nhiễu để hướng dẫn mô hình khuếch tán tạo ra hình ảnh, video hoặc âm thanh phù hợp. Mặt khác, chúng tôi cũng đề xuất giảm thiểu khoảng cách giữa các biểu diễn token tín hiệu được chiếu và các biểu diễn văn bản có điều kiện của các mô hình khuếch tán để đẩy nhanh việc học căn chỉnh. Lưu ý rằng tất cả các xương sống khuếch tán (tức là U-Net) đều đông lạnh, điều này cũng đảm bảo huấn luyện cực kỳ nhẹ.

Trong giai đoạn huấn luyện căn chỉnh, chúng tôi lấy các chú thích từ CC3M, WebVid và AudioCaps làm đầu vào và nối chúng với các token tín hiệu làm đầu ra. Hàm mất mát bao gồm ba thành phần chính: 1) khả năng log âm của việc tạo ra token tín hiệu, và 2) mất mát căn chỉnh chú thích: khoảng cách l2 giữa các trạng thái ẩn của token tín hiệu được tạo ra bởi LLM và các biểu diễn văn bản có điều kiện được lấy từ bộ mã hóa văn bản trong các mô hình khuếch tán, và 3) mất mát khử nhiễu tiềm ẩn có điều kiện (Rombach et al., 2022).

5. Điều chỉnh Hướng dẫn Chuyển đổi Phương thức
5.1. Điều chỉnh Hướng dẫn
Mặc dù căn chỉnh cả phía mã hóa và giải mã với LLM, vẫn còn một khoảng cách hướng tới mục tiêu cho phép toàn bộ hệ thống tuân thủ một cách trung thành và hiểu các hướng dẫn của người dùng và tạo ra các đầu ra đa phương thức mong muốn. Để giải quyết điều này, việc điều chỉnh hướng dẫn (IT) thêm (Yin et al., 2023; Su et al., 2023; Liu et al., 2023b) được coi là cần thiết để nâng cao khả năng và khả năng kiểm soát của LLM.

IT bao gồm việc huấn luyện bổ sung các MM-LLM tổng thể sử dụng các cặp '(ĐẦU VÀO, ĐẦU RA)', nơi 'ĐẦU VÀO' đại diện cho hướng dẫn của người dùng, và 'ĐẦU RA' biểu thị đầu ra mô hình mong muốn tuân thủ hướng dẫn đã cho. Về mặt kỹ thuật, chúng tôi tận dụng LoRA (Hu et al., 2022) để cho phép một tập hợp nhỏ các tham số trong NExT-GPT được cập nhật đồng thời với hai lớp chiếu trong giai đoạn IT. Như được minh họa trong Hình 3, khi một mẫu đối t화 IT được cấp vào hệ thống, LLM tái tạo và tạo ra nội dung văn bản của đầu vào (và biểu diễn nội dung đa phương thức với các token tín hiệu đa phương thức). Việc tối ưu hóa được áp dụng dựa trên các chú thích vàng và đầu ra của LLM. Ngoài việc điều chỉnh LLM, chúng tôi cũng điều chỉnh tinh phía giải mã của NExT-GPT. Chúng tôi căn chỉnh biểu diễn token tín hiệu phương thức được mã hóa bởi chiếu đầu ra với biểu diễn chú thích đa phương thức vàng được mã hóa bởi bộ mã hóa điều kiện khuếch tán. Do đó, quá trình điều chỉnh toàn diện đưa gần hơn đến mục tiêu tương tác trung thành và hiệu quả với người dùng.

5.2. Bộ dữ liệu Hướng dẫn
Đối với IT của NExT-GPT, chúng tôi trước tiên xem xét việc tận dụng các bộ dữ liệu 'Văn bản' → 'Văn bản+X' đã được thiết lập tốt, nơi 'X' có thể là hình ảnh, video, âm thanh hoặc khác, ví dụ như LLaVA-150K (Liu et al., 2023b), và VideoChat (Li et al., 2023d). Tuy nhiên, các bộ dữ liệu IT này bị giới hạn trong việc xuất ra các phản hồi văn bản từ LLMs. Trong kịch bản bất kỳ-sang-bất kỳ của chúng tôi, mục tiêu không chỉ bao gồm việc tạo ra văn bản, mà còn cả nội dung đa phương thức, tức là 'Văn bản+X'. Do đó, chúng tôi xây dựng bộ dữ liệu 'Văn bản' → 'Văn bản+X', tức là dữ liệu văn bản-sang-đa phương thức (gọi là T2M). Dựa trên khối lượng phong phú của các cặp 'X-chú thích' từ các kho và điểm chuẩn hiện có (Sharma et al., 2018; Lin et al., 2014; Bain et al., 2021; Kim et al., 2019), với một số mẫu, chúng tôi sử dụng GPT-4 để tạo ra các hướng dẫn văn bản đa dạng để bao bọc các chú thích, và dẫn đến bộ dữ liệu.

Bộ dữ liệu MosIT Việc tạo ra các hướng dẫn chất lượng cao bao gồm toàn diện các hành vi mục tiêu mong muốn là không tầm thường. Chúng tôi nhận thấy rằng các bộ dữ liệu IT trên không đáp ứng các yêu cầu cho kịch bản MM-LLM bất kỳ-sang-bất kỳ của chúng tôi. Thứ nhất, trong quá trình tương tác người-máy, người dùng và LLM liên quan đến các phương thức đa dạng và thay đổi động trong đầu vào và đầu ra của họ. Ngoài ra, chúng tôi cho phép các cuộc trò chuyện nhiều lượt trong quá trình, và do đó việc xử lý và hiểu các ý định phức tạp của người dùng là cần thiết. Tuy nhiên, hai loại bộ dữ liệu trên thiếu các phương thức biến đổi, và cũng tương đối ngắn trong các đối t화, không mô phỏng đầy đủ các kịch bản thế giới thực.

Để tạo điều kiện cho sự phát triển của MM-LLM bất kỳ-sang-bất kỳ, chúng tôi đề xuất một phương pháp Điều chỉnh Hướng dẫn Chuyển đổi Phương thức (MosIT) mới. MosIT không chỉ hỗ trợ hiểu và lý luận đa phương thức phức tạp mà còn cho phép tạo ra nội dung đa phương thức tinh vi. Kết hợp với MosIT, chúng tôi xây dựng thủ công và tỉ mỉ một bộ dữ liệu chất lượng cao. Bộ dữ liệu MosIT bao gồm một phạm vi rộng các đầu vào và đầu ra đa phương thức, cung cấp sự phức tạp và biến đổi cần thiết để tạo điều kiện cho việc huấn luyện các MM-LLM có thể xử lý các tương tác người dùng đa dạng và truyền đạt các phản hồi mong muốn một cách chính xác. Cụ thể, chúng tôi thiết kế một số ví dụ đối thua mẫu giữa vai trò 'Con người' và vai trò 'Máy móc', dựa trên đó chúng tôi nhắc GPT-4 tạo ra nhiều cuộc trò chuyện hơn trong các kịch bản khác nhau với hơn 100 chủ đề hoặc từ khóa. Các tương tác được yêu cầu đa dạng, ví dụ, bao gồm cả các yêu cầu trực tiếp và ngầm ẩn của 'Con người', và thực hiện nhận thức, lý luận, đề xuất và lập kế hoạch, v.v., bởi 'Máy móc'. Và nội dung tương tác nên được kết nối logic và vốn có ngữ nghĩa và phức tạp, với các chi tiết lý luận sâu sắc trong mỗi phản hồi của 'Máy móc'. Mỗi cuộc trò chuyện nên bao gồm 3-7 lượt (tức là các cặp QA), nơi các tương tác 'Con người'-'Máy móc' nên liên quan đến nhiều phương thức ở phía đầu vào hoặc đầu ra, và chuyển đổi các phương thức xen kẽ. Bất cứ khi nào có nội dung đa phương thức (ví dụ, hình ảnh, âm thanh và video) trong các cuộc trò chuyện, chúng tôi tìm kiếm nội dung phù hợp nhất từ các nguồn bên ngoài, bao gồm các hệ thống truy xuất, ví dụ như Youtube, và thậm chí các công cụ AIGC, ví dụ như Stable-XL (Podell et al., 2023), và Midjourney. Sau khi kiểm tra con người và lọc các trường hợp không phù hợp, chúng tôi thu được tổng cộng 5K đối thoại chất lượng cao. Trong Bảng 6 của Phụ lục §C.4, chúng tôi so sánh các thống kê của các bộ dữ liệu IT đa phương thức hiện có với dữ liệu MosIT của chúng tôi trong các thống kê chi tiết.

6. Thí nghiệm
Trong các thí nghiệm, chúng tôi nhằm mục đích định lượng hiệu suất của NExT-GPT trên một loạt các nhiệm vụ phía sau yêu cầu nhận thức và tạo ra bất kỳ phương thức nào. Các cài đặt và chi tiết thực hiện khác có thể được tìm thấy trong Phụ lục §C. Cũng do giới hạn không gian, chúng tôi trình bày một số lượng tốt các kết quả thí nghiệm và phân tích khác trong Phụ lục §D.

6.1. Kết quả Chính
Nhận thức Đa phương thức Trước tiên, chúng tôi đánh giá khả năng hiểu ngữ nghĩa của NExT-GPT đối với hình ảnh, video hoặc âm thanh, qua nhiều điểm chuẩn. Kết quả được hiển thị trong Bảng 2 và 3. Đáng chú ý, NExT-GPT thể hiện hiệu suất đặc biệt trong việc hiểu hình ảnh, thể hiện những cải thiện đáng kể so với mức cơ sở trong các nhiệm vụ như chú thích hình ảnh và hỏi đáp hình ảnh. Hơn nữa, khi được đánh giá trên các bộ dữ liệu điểm chuẩn chỉ đánh giá như MMBench (MMB) và SEED-Bench (SEED), NExT-GPT liên tục đạt được hiệu suất so sánh. Ngoài ra, mô hình xuất sắc trong việc hiểu video và âm thanh. So với Codi, NExT-GPT đạt được kết quả tăng cường được quy cho khả năng tạo ra văn bản trực tiếp từ LLM, tận dụng chuyên môn vốn có của LLM.

Tạo ra Đa phương thức Sau đó chúng tôi kiểm tra chất lượng tổng hợp của hình ảnh, video hoặc âm thanh có điều kiện trên văn bản. Bảng 4 trình bày các so sánh giữa chúng tôi và một số hệ thống tiên tiến. Nhìn chung, NExT-GPT thể hiện hiệu suất vượt trội trong việc tạo ra hình ảnh, âm thanh và video có điều kiện trên văn bản. So với các mô hình lấy LLM làm trung tâm, tức là GILL, Emu và UIO-2XXL, chúng tôi nổi bật bằng cách hỗ trợ một phạm vi phương thức đa dạng hơn. Hơn nữa, trong việc tạo ra các phương thức riêng lẻ, NExT-GPT duy trì hiệu suất tối ưu, ngay cả trong các kịch bản zero-shot. Đáng chú ý, so với các mô hình không lấy LLM làm trung tâm, chúng tôi vẫn thể hiện sự cải thiện rõ ràng về chất lượng tạo ra.

6.2. Phân tích Sâu
Tác động của Số lượng Token Tín hiệu Trong Hình 6, chúng tôi hiển thị kết quả của NExT-GPT sử dụng số lượng token tín hiệu cụ thể phương thức đề xuất khác nhau. Kết quả thí nghiệm tiết lộ rằng số lượng token cần thiết thay đổi cho mỗi phương thức. Đáng chú ý, video, do tạo ra nội dung phức tạp hơn, đòi hỏi số lượng token tín hiệu cao nhất. Hai phương thức khác, hình ảnh và âm thanh, đạt được sự tạo ra thỏa mãn chỉ với 4 và 8 token tín hiệu, tương ứng. Tuy nhiên, việc lựa chọn số lượng token tín hiệu phụ thuộc vào các yếu tố như kích thước dữ liệu huấn luyện và việc lựa chọn mô hình khuếch tán. Ví dụ, với dữ liệu rộng hơn và mô hình khuếch tán mạnh mẽ, việc tăng số lượng token tín hiệu có thể dẫn đến kết quả cải thiện.

Tác động của Cơ chế Nhóm Để minh họa thêm hiệu quả của việc sử dụng cơ chế nhóm để căn chỉnh các đặc trưng thị giác với LLM, chúng tôi thực hiện thí nghiệm với các thiết kế kiến trúc chiếu khác nhau. Những thiết kế này bao gồm 'với Lớp Tuyến tính' loại bỏ mô-đun nhóm và trực tiếp ánh xạ đầu ra của Imagebind vào không gian nhúng ngôn ngữ thông qua một lớp tuyến tính duy nhất, và 'với Q-former + Lớp Tuyến tính' tích hợp Q-former thay vì cơ chế nhóm. Tất cả các biến thể trải qua huấn luyện theo cùng quy trình như thiết kế gốc. Kết quả của hai bộ dữ liệu QA hình ảnh, hai bộ dữ liệu QA video và một bộ dữ liệu chú thích âm thanh được trình bày trong Bảng 4. Các phát hiện thí nghiệm chỉ ra sự giảm đáng kể trong khả năng nhận thức của mô hình qua ba phương thức khi sử dụng phương pháp tuyến tính đơn giản. Ngoài ra, việc tích hợp Q-former mang lại sự cải thiện khiêm tốn về khả năng nhận thức. Sự tăng cường này có thể được quy cho khả năng của Q-former trong việc thực hiện nhóm đặc trưng thị giác nhẹ, căn chỉnh hiệu quả với ngữ nghĩa token văn bản phức tạp, do đó nâng cao khả năng nhận thức. Tuy nhiên, cơ chế nhóm của NExT-GPT thể hiện hiệu suất tối ưu.

Đánh giá về Pipeline vs MM-LLMs Từ đầu đến cuối Để đánh giá xem hệ thống có thực sự hoặc mức độ hiểu đầu vào và tạo ra nội dung đầu ra (văn bản phản hồi + hình ảnh), chúng tôi thực hiện đánh giá con người. Để xây dựng dữ liệu kiểm tra, chúng tôi trước tiên tận dụng GPT-4 để tổng hợp 100 hướng dẫn phức tạp (ví dụ, liên quan đến các cảnh phức tạp và giàu ngữ nghĩa) đòi hỏi khả năng lý luận ngầm ẩn để tạo ra nội dung hình ảnh. Sau đó, các hướng dẫn đã tổng hợp được cấp vào các mô hình để tạo ra nội dung văn bản phản hồi + hình ảnh. Sau đó, năm tình nguyện viên không thiên vị đánh giá các kết quả được tạo ra dưới ba khía cạnh, 1) Tuân theo hướng dẫn, xác định, trong số bốn mô hình, mô hình nào tạo ra văn bản+hình ảnh phản hồi chính xác với các hướng dẫn đầu vào, 2) Hợp lý, xác định mô hình nào tạo ra hình ảnh tuân thủ các hướng dẫn đầu vào, 3) Chất lượng, đánh giá mô hình nào tạo ra hình ảnh có chất lượng cao nhất. Hình 5 minh họa hiệu suất vượt trội trong việc tuân theo các hướng dẫn phức tạp và tạo ra hình ảnh chất lượng cao, so với hai hệ thống hiện có và NExT-GPT-caption, trực tiếp tạo ra chú thích văn bản cho các mô hình khuếch tán phía sau.

Phân tích Định tính Để trực tiếp thể hiện hiệu quả và tiềm năng của NExT-GPT trong việc phát triển các tác nhân đối thoại giống con người, chúng tôi cung cấp thêm các ví dụ định tính minh họa sinh động khả năng của hệ thống để hiểu và lý luận nội dung qua các phương thức khác nhau trong bất kỳ kết hợp nào, như được hiển thị trong Hình 4. Từ ví dụ (A), chúng tôi có thể lưu ý rằng NExT-GPT có thể hiểu phần bất thường của video đầu vào, và tổng hợp âm thanh nhẹ nhàng và các cảnh bất thường tương tự, tức là một con mèo đi trên ván trượt. Ngoài ra, ngoài việc phản hồi các truy vấn rõ ràng thúc đẩy tổng hợp mô hình trong các phương thức cụ thể, NExT-GPT thể hiện thành thạo trong việc suy luận ý định ngầm ẩn của người dùng. Trong ví dụ (B), khi người dùng truyền đạt một tâm trạng tiêu cực, NExT-GPT phản hồi đồng cảm và tự chủ và quyết định trình bày một video chó con vui vẻ để nâng cao tinh thần của người dùng. Tương tự, khi chuẩn bị một bài thuyết trình cho lớp lịch sử, NExT-GPT thể hiện sự linh hoạt trong việc tạo ra các mẹo và hình ảnh hóa phù hợp. Vui lòng tham khảo Phụ lục §D.4 để biết thêm các minh chứng với các hướng dẫn ngầm ẩn và rõ ràng.

7. Kết luận
Trong công trình này, chúng tôi đã trình bày một Mô hình Ngôn ngữ Lớn đa phương thức (MM-LLM) bất kỳ-sang-bất kỳ tổng quát từ đầu đến cuối. Bằng cách kết nối một LLM với các bộ điều hợp đa phương thức và các bộ giải mã khuếch tán khác nhau, NExT-GPT có khả năng nhận thức đầu vào và tạo ra đầu ra trong bất kỳ kết hợp nào của văn bản, hình ảnh, video và âm thanh. Khai thác các bộ mã hóa và giải mã hiệu suất cao đã được huấn luyện tốt hiện có, việc huấn luyện NExT-GPT chỉ đòi hỏi một số lượng nhỏ tham số (1%) của các lớp chiếu nhất định, điều này không chỉ có lợi cho chi phí thấp mà còn tạo điều kiện thuận lợi cho việc mở rộng thuận tiện của nhiều phương thức tiềm năng hơn trong tương lai. Để cho phép NExT-GPT của chúng tôi với khả năng hiểu ngữ nghĩa đa phương thức phức tạp và tạo ra nội dung, chúng tôi cũng đã giới thiệu một phương pháp điều chỉnh hướng dẫn chuyển đổi phương thức (MosIT), và tuyển chọn thủ công một bộ dữ liệu chất lượng cao cho MosIT. Nhìn chung, nghiên cứu của chúng tôi thể hiện tiềm năng của các MM-LLM bất kỳ-sang-bất kỳ trong việc thu hẹp khoảng cách giữa các phương thức khác nhau và mở đường cho các hệ thống AI giống con người hơn trong tương lai.

Lời cảm ơn
Công trình này được hỗ trợ bởi CCF-Baidu Open Fund và NExT Research Center.

Tuyên bố Tác động
Bài báo này nhằm mục đích phát triển một tác nhân AI cấp độ con người, một MM-LLM bất kỳ-sang-bất kỳ tổng quát từ đầu đến cuối. NExT-GPT, bị hạn chế bởi số lượng dữ liệu điều chỉnh tinh và chất lượng của các mô hình cơ sở, có thể tạo ra nội dung chất lượng thấp hoặc ảo giác có thể có hại. Người dùng được khuyến cáo diễn giải kết quả một cách cẩn thận và tuân thủ các quy tắc cấp phép, với việc sử dụng thương mại bị cấm. Chúng tôi ưu tiên quyền riêng tư dữ liệu bằng cách tuân theo các điều khoản nền tảng truyền thông xã hội và có được sự đồng ý của người dùng khi cần thiết, đảm bảo tất cả thông tin cá nhân được ẩn danh hoặc che giấu. Ngoài ra, chúng tôi cảnh giác trong việc giảm thiểu thiên kiến trong việc thu thập bộ dữ liệu, phấn đấu cho một bộ dữ liệu đại diện và công bằng không thiên vị hoặc phân biệt đối xử với bất kỳ nhóm hoặc quan điểm cụ thể nào.
