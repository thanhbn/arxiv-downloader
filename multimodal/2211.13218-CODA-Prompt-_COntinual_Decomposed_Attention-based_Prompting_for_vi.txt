# 2211.13218.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2211.13218.pdf
# Kích thước tệp: 743410 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CODA-Prompt: Gợi ý Chú ý Phân tách Liên tục dựa trên COntinual Decomposed Attention cho
Học tập Liên tục không cần Luyện tập lại
James Seale Smith*1,2Leonid Karlinsky2,4Vyshnavi Gutta1Paola Cascante-Bonilla2,3
Donghyun Kim2,4Assaf Arbelle4Rameswar Panda2,4Rogerio Feris2,4Zsolt Kira1
1Viện Công nghệ Georgia2MIT-IBM Watson AI Lab3Đại học Rice4IBM Research
Tóm tắt
Các mô hình thị giác máy tính gặp phải hiện tượng
được gọi là quên thảm khốc khi học các khái niệm mới
từ dữ liệu huấn luyện liên tục thay đổi. Các giải pháp
thông thường cho vấn đề học tập liên tục này đòi hỏi
việc luyện tập lại rộng rãi các dữ liệu đã thấy trước đó,
điều này làm tăng chi phí bộ nhớ và có thể vi phạm
quyền riêng tư dữ liệu. Gần đây, sự xuất hiện của các
mô hình vision transformer tiền huấn luyện quy mô
lớn đã cho phép các phương pháp gợi ý như một giải
pháp thay thế cho việc luyện tập lại dữ liệu. Các phương
pháp này dựa trên cơ chế khóa-truy vấn để tạo ra gợi
ý và được phát hiện là có khả năng chống lại quên thảm
khốc cao trong cài đặt học tập liên tục không cần luyện
tập lại đã được thiết lập tốt. Tuy nhiên, cơ chế khóa của
các phương pháp này không được huấn luyện từ đầu
đến cuối với chuỗi tác vụ. Các thí nghiệm của chúng tôi
cho thấy điều này dẫn đến việc giảm tính dẻo dai của
chúng, do đó hy sinh độ chính xác tác vụ mới, và không
thể hưởng lợi từ khả năng tham số mở rộng. Thay vào
đó, chúng tôi đề xuất học một tập hợp các thành phần
gợi ý được lắp ráp với trọng số phụ thuộc đầu vào để
tạo ra gợi ý phụ thuộc đầu vào, tạo ra một sơ đồ khóa-
truy vấn từ đầu đến cuối dựa trên chú ý mới. Các thí
nghiệm của chúng tôi cho thấy chúng tôi vượt trội hơn
phương pháp SOTA hiện tại DualPrompt trên các tiêu
chuẩn đã thiết lập lên đến 4,5% trong độ chính xác cuối
cùng trung bình. Chúng tôi cũng vượt trội hơn tiên tiến
lên đến 4,4% độ chính xác trên tiêu chuẩn học tập liên
tục có chứa cả sự dịch chuyển tác vụ tăng dần lớp và
tăng dần miền, tương ứng với nhiều cài đặt thực tế.
Mã của chúng tôi có sẵn tại https://github.com/GT-RIPL/CODA-Prompt

1. Giới thiệu
Để một mô hình thị giác máy tính thành công trong thế giới thực,
nó phải vượt qua các giả định mong manh rằng các khái niệm mà
nó sẽ gặp phải sau khi triển khai sẽ khớp với những gì đã học
trước đó trong quá trình huấn luyện. Thế giới thực thực sự là
động và chứa các đối tượng và danh mục liên tục xuất hiện.
*Công việc được thực hiện một phần trong thời gian thực tập tại MIT-IBM Watson AI Lab.

Một khi được triển khai, các mô hình sẽ gặp phải một loạt các
khác biệt từ bộ dữ liệu huấn luyện của chúng, đòi hỏi chúng ta
phải liên tục cập nhật chúng để tránh hiệu suất cũ và suy giảm.
Một cách để cập nhật mô hình là thu thập dữ liệu huấn luyện
bổ sung, kết hợp dữ liệu huấn luyện mới này với dữ liệu huấn
luyện cũ, sau đó huấn luyện lại mô hình từ đầu. Mặc dù điều
này sẽ đảm bảo hiệu suất cao, nhưng nó không thực tế cho các
ứng dụng quy mô lớn có thể yêu cầu thời gian huấn luyện dài
và lịch trình thay thế tích cực, chẳng hạn như mô hình cho xe
tự lái hoặc nền tảng mạng xã hội. Điều này có thể phát sinh
chi phí tài chính [26] và môi trường [34] cao nếu việc thay thế
diễn ra thường xuyên. Thay vào đó, chúng ta có thể cập nhật
mô hình bằng cách chỉ huấn luyện trên dữ liệu huấn luyện mới,
nhưng điều này dẫn đến một hiện tượng được gọi là quên thảm
khốc [47] khi mô hình ghi đè kiến thức hiện có khi học dữ liệu
mới, một vấn đề được gọi là học tập liên tục.

--- TRANG 2 ---
Các phương pháp hiệu quả nhất được đề xuất bởi cộng đồng
học tập liên tục bao gồm việc lưu [51] hoặc tạo ra [56] một
tập con dữ liệu huấn luyện trong quá khứ và trộn nó với dữ
liệu tác vụ tương lai, một chiến lược được gọi là luyện tập lại.
Tuy nhiên, nhiều ứng dụng quan trọng không thể lưu trữ dữ
liệu này vì chúng làm việc với dữ liệu người dùng riêng tư
không thể được lưu trữ trong thời gian dài.
Trong bài báo này, thay vào đó chúng tôi xem xét cài đặt có
tác động cao và được thiết lập tốt của học tập liên tục không
cần luyện tập lại [39, 57, 58, 66, 67]1, giới hạn phạm vi của
chúng tôi vào các chiến lược học tập liên tục không lưu trữ dữ
liệu huấn luyện. Mặc dù các phương pháp không cần luyện tập
lại theo truyền thống đã kém hiệu quả hơn các phương pháp
dựa trên luyện tập lại trong cài đặt thách thức này với biên
rộng [58], các phương pháp dựa trên gợi ý gần đây [66, 67],
tận dụng Vision transformer (ViT) tiền huấn luyện, đã có
thành công to lớn và thậm chí có thể vượt trội hơn các phương
pháp dựa trên luyện tập lại tiên tiến (SOTA). Các phương pháp
gợi ý này tự hào có khả năng bảo vệ mạnh mẽ chống lại quên
thảm khốc bằng cách học một pool nhỏ các embedding mô hình
có thể chèn vào (gợi ý) thay vì sửa đổi trực tiếp các tham số
bộ mã hóa thị giác. Tuy nhiên, một nhược điểm của các phương
pháp này là chúng không thể được tối ưu hóa theo kiểu từ đầu
đến cuối vì chúng sử dụng khóa và truy vấn để chọn một chỉ
số gợi ý từ pool gợi ý, và do đó dựa vào tối ưu hóa cục bộ
thứ hai để học các khóa vì gradient mô hình không thể lan
truyền ngược qua việc chọn chỉ số khóa/truy vấn. Hơn nữa,
các phương pháp này giảm quên bằng cách hy sinh độ chính
xác tác vụ mới (tức là chúng thiếu tính dẻo dai đủ). Chúng
tôi cho thấy việc mở rộng kích thước gợi ý không tăng tính
dẻo dai, thúc đẩy chúng tôi tăng khả năng học từ một góc
nhìn mới.
Chúng tôi thay thế pool gợi ý bằng một gợi ý phân tách bao
gồm tổng có trọng số của các thành phần gợi ý có thể học
(Hình 1). Việc phân tách này cho phép khả năng gợi ý cao
hơn bằng cách mở rộng trong một chiều mới (số lượng thành
phần). Hơn nữa, điều này vốn dĩ khuyến khích việc tái sử dụng
kiến thức, vì các gợi ý tác vụ tương lai sẽ bao gồm đóng góp
từ các thành phần tác vụ trước đó. Chúng tôi cũng giới thiệu
một sơ đồ cân bằng trọng số thành phần dựa trên chú ý mới,
cho phép toàn bộ phương pháp của chúng tôi được tối ưu hóa
theo kiểu từ đầu đến cuối không giống như các công trình hiện
có, tăng tính dẻo dai của chúng tôi để học tốt hơn các tác vụ
tương lai. Chúng tôi tự hào về những cải thiện hiệu suất đáng
kể trên các tiêu chuẩn học tập liên tục không cần luyện tập lại
hiện có so với các đường cơ sở gợi ý SOTA bên cạnh một tiêu
chuẩn dịch chuyển kép thách thức chứa cả sự dịch chuyển khái
niệm tăng dần và sự dịch chuyển miền hiệp phương sai, làm
nổi bật tác động và tính tổng quát của phương pháp chúng tôi.
Phương pháp của chúng tôi cải thiện kết quả ngay cả dưới số
lượng tham số tương đương, nhưng quan trọng là có thể mở
rộng hiệu suất bằng cách tăng khả năng, không giống như các
phương pháp trước đó nhanh chóng
1Chúng tôi tập trung vào học tập liên tục trên một đầu phân loại duy nhất,
mở rộng được gọi là học tập liên tục tăng dần lớp. Điều này khác với cài đặt
học tập liên tục đa tác vụ, được gọi là học tập liên tục tăng dần tác vụ, nơi
chúng ta học các đầu phân loại riêng biệt cho mỗi tác vụ và nhãn tác vụ được
cung cấp trong quá trình suy luận. [25, 62]

đạt đến giới hạn. Tóm lại, chúng tôi có những đóng góp sau:
1. Chúng tôi giới thiệu gợi ý dựa trên chú ý phân tách cho học
   tập liên tục không cần luyện tập lại, đặc trưng bởi khả năng
   học mở rộng so với các phương pháp gợi ý liên tục hiện có.
   Quan trọng là, phương pháp của chúng tôi có thể được tối
   ưu hóa theo kiểu từ đầu đến cuối, không giống như SOTA
   trước đó.
2. Chúng tôi thiết lập một SOTA mới cho học tập liên tục không
   cần luyện tập lại trên các tiêu chuẩn được thiết lập tốt
   ImageNet-R [21, 66] và CIFAR-100 [33], đánh bại phương
   pháp SOTA trước đó DualPrompt [66] lên đến 4,5%.
3. Chúng tôi đánh giá trên một tiêu chuẩn thách thức với sự
   dịch chuyển phân phối kép (ngữ nghĩa và hiệp phương sai)
   sử dụng bộ dữ liệu ImageNet-R, và một lần nữa vượt trội
   hơn tiên tiến, làm nổi bật tác động thế giới thực và tính
   tổng quát của phương pháp chúng tôi.

2. Bối cảnh và Công trình Liên quan
Học tập Liên tục: Các phương pháp học tập liên tục có thể
được tổ chức thành một vài danh mục rộng, tất cả đều hữu
ích tùy thuộc vào cài đặt vấn đề và các ràng buộc. Một nhóm
phương pháp mở rộng kiến trúc của mô hình khi gặp phải các
tác vụ mới; chúng rất hiệu quả cho các ứng dụng mà một mô
hình phát triển cùng với các tác vụ là thực tế [16,36,40,44,55].
Một phương pháp khác là điều chỉnh mô hình đối với kiến
thức tác vụ trong quá khứ khi huấn luyện tác vụ mới. Điều
này có thể được thực hiện bằng cách điều chỉnh mô hình trong
không gian trọng số (tức là, phạt các thay đổi đối với tham số
mô hình) [2, 15, 31, 59, 72] hoặc không gian dự đoán (tức là,
phạt các thay đổi đối với dự đoán mô hình) [1, 8, 23, 35, 39].
Điều chỉnh kiến thức trong không gian dự đoán được thực hiện
bằng cách sử dụng chưng cất kiến thức [22] và nó được tìm
thấy hoạt động tốt hơn các phương pháp dựa trên điều chỉnh
mô hình cho học tập liên tục khi nhãn tác vụ không được
cung cấp [37, 61].
Luyện tập lại với dữ liệu được lưu trữ [3–5, 7, 9, 10, 18, 19,
24, 29, 42, 51–53, 64, 68] hoặc mẫu từ mô hình sinh [27, 28,
48, 56, 60] rất hiệu quả khi việc lưu trữ dữ liệu huấn luyện
hoặc huấn luyện/lưu mô hình sinh là khả thi. Thật không may,
đối với nhiều ứng dụng học máy, việc lưu trữ dữ liệu huấn
luyện lâu dài sẽ vi phạm quyền riêng tư dữ liệu, cũng như
phát sinh chi phí bộ nhớ lớn. Đối với mô hình sinh, quá trình
huấn luyện này tốn kém về mặt tính toán và bộ nhớ hơn nhiều
so với mô hình phân loại và có thể vi phạm các mối quan tâm
về tính hợp pháp của dữ liệu vì việc sử dụng mô hình sinh
tăng khả năng ghi nhớ dữ liệu có khả năng nhạy cảm [46].
Điều này thúc đẩy chúng tôi làm việc trên cài đặt quan trọng
của các phương pháp không cần luyện tập lại để giảm thiểu
quên thảm khốc.
Học tập Liên tục không cần Luyện tập lại: Các công trình
gần đây đề xuất tạo ra hình ảnh để luyện tập lại bằng cách
sử dụng đảo ngược mô hình sâu [12, 17, 57, 69]. Mặc dù các
phương pháp này hoạt động tốt

--- TRANG 3 ---
so với các phương pháp mô hình sinh và đơn giản luyện tập
lại từ một số lượng nhỏ hình ảnh được lưu trữ, đảo ngược mô
hình là một quá trình chậm liên quan đến chi phí tính toán cao
trong cài đặt học tập liên tục, và hơn nữa các phương pháp
này kém hiệu quả hơn các phương pháp dựa trên luyện tập
lại với biên đáng kể [57]. Các công trình khác đã xem xét học
tập liên tục không cần luyện tập lại từ góc độ học tập "dòng
chảy" trực tuyến sử dụng mô hình tiền huấn luyện cố định [20,
41]. Vì chúng tôi cho phép các mô hình của chúng tôi huấn
luyện đến hội tụ trên dữ liệu tác vụ (như thường thấy trong
học tập liên tục [68]), cài đặt của chúng tôi rất khác biệt.
Học tập Liên tục trong Vision Transformer: Các công trình
gần đây [6, 38, 70] đã chứng minh transformer tổng quát tốt
cho các miền chưa thấy. Ví dụ, một nghiên cứu thay đổi số
lượng đầu chú ý trong Vision transformer và kết luận rằng
ViT cung cấp khả năng chống lại quên mạnh mẽ hơn so với
các tương đương dựa trên CNN [45]. Một nghiên cứu khác
[71] cho thấy rằng các đối tác vanilla của ViT dễ bị quên hơn
khi được huấn luyện từ đầu. Cuối cùng, DyTox [14] học một
mô hình thống nhất với phương pháp cách li tham số và mở
rộng động các token được xử lý bởi lớp cuối cùng để giảm
thiểu quên. Đối với mỗi tác vụ, họ học một token đặc trưng
cho tác vụ mới cho mỗi đầu sử dụng các khối giải mã dựa
trên chú ý tác vụ. Tuy nhiên, các công trình trên hoặc dựa
vào exemplar để chống lại quên hoặc cần huấn luyện một mô
hình transformer mới từ đầu, một hoạt động tốn kém, do đó
khác với mục tiêu CL không exemplar của chúng tôi sử dụng
ViT tiền huấn luyện.
Gợi ý cho Học tập Liên tục: Các phương pháp dựa trên gợi ý
cho học tập liên tục tự hào có khả năng bảo vệ mạnh mẽ chống
lại quên thảm khốc bằng cách học một số lượng nhỏ các hướng
dẫn mô hình có thể chèn vào (gợi ý) thay vì sửa đổi trực tiếp
các tham số bộ mã hóa. Các phương pháp tiên tiến hiện tại
cho cài đặt của chúng tôi, DualPrompt [66] và L2P [67], tạo
ra một pool gợi ý để được chọn để chèn vào mô hình, khớp
dữ liệu đầu vào với gợi ý không có id tác vụ với tối ưu hóa
giống như phân cụm cục bộ. Chúng tôi xây dựng trên nền
tảng của các phương pháp này, như được thảo luận sau trong
bài báo này. Chúng tôi lưu ý rằng phương pháp S-Prompts
[65] gần đây tương tự như các phương pháp này nhưng được
thiết kế cho học tập tăng dần miền (tức là, học cùng một tập
các lớp dưới sự dịch chuyển tác vụ phân phối hiệp phương
sai), khác với cài đặt học tập liên tục tăng dần lớp của bài
báo chúng tôi (tức là, học các lớp đối tượng mới nổi trong
các tác vụ mới).

3. Kiến thức Cơ bản
3.1. Học tập Liên tục
Trong cài đặt học tập liên tục của chúng tôi, một mô hình
được hiển thị tuần tự N tác vụ tương ứng với các tập con
không trùng lắp của các lớp đối tượng ngữ nghĩa. Mỗi lớp
xuất hiện trong chỉ một tác vụ duy nhất, và mục tiêu là học
tăng dần để phân loại các lớp đối tượng mới khi chúng được
giới thiệu trong khi duy trì hiệu suất trên các lớp đã học trước
đó. Để mô tả các

mô hình của chúng tôi, chúng tôi ký hiệu θ như một bộ mã hóa
thị giác tiền huấn luyện và ϕn như đầu phân loại của chúng
tôi với logit tương ứng với các lớp tác vụ n. Trong bài báo
này, chúng tôi giải quyết cài đặt học tập liên tục tăng dần lớp
thay vì cài đặt học tập liên tục tăng dần tác vụ. Học tập liên
tục tăng dần lớp là thách thức vì người học phải hỗ trợ phân
loại trên tất cả các lớp đã thấy cho đến tác vụ n [25] (tức là,
không có nhãn tác vụ nào được cung cấp cho người học trong
quá trình suy luận). Học tập liên tục tăng dần tác vụ là một
cài đặt đa tác vụ đơn giản hơn trong đó các nhãn tác vụ được
cung cấp trong cả quá trình huấn luyện và suy luận.
Chúng tôi cũng bao gồm các thí nghiệm chứa cả học tập liên
tục tăng dần lớp và tăng dần miền cùng một lúc. Sự khác biệt
giữa cài đặt này và cài đặt trước là chúng tôi cũng tiêm các
sự dịch chuyển phân phối hiệp phương sai (ví dụ, tác vụ 1
có thể bao gồm hình ảnh thực và clip-art, trong khi tác vụ 2
có thể bao gồm ảnh bị hỏng và tranh nghệ thuật). Động lực
là làm cho học tập liên tục thực tế hơn - vì trong thế giới thực
chúng ta có thể gặp phải các loại dịch chuyển miền kép này
cũng như.

3.2. Gợi ý với Tinh chỉnh Tiền tố
Trong công trình của chúng tôi, chúng tôi không thay đổi
nền tảng kỹ thuật của gợi ý từ DualPrompt [66] tiên tiến
trước đó. Thay vào đó, chúng tôi tập trung vào việc lựa chọn
và hình thành gợi ý (bao gồm việc cho phép tối ưu hóa từ
đầu đến cuối của tất cả các thành phần gợi ý), và gợi ý kết
quả được sử dụng bởi bộ mã hóa vision transformer theo
cách giống hệt như DualPrompt. Điều này cho phép chúng
tôi so sánh công bằng đối với những đóng góp mới của chúng
tôi.
Như đã thực hiện trong DualPrompt, chúng tôi truyền gợi ý
đến một số lớp chú ý tự đa đầu (MSA) trong transformer
ViT [13, 63] tiền huấn luyện và sử dụng tinh chỉnh tiền tố
thay vì tinh chỉnh gợi ý2, điều này thêm gợi ý vào đầu các
khóa và giá trị của lớp MSA thay vì thêm gợi ý vào đầu
các token đầu vào. Các lớp mà gợi ý xảy ra được đặt như
một siêu tham số, và các tham số gợi ý là duy nhất giữa các
lớp. Chúng tôi định nghĩa một tham số gợi ý như p∈RLp×D
trong đó Lp là độ dài gợi ý (được chọn như một siêu tham
số) và D là chiều embedding (768). Xem xét một lớp MSA
với đầu vào h∈RL×D, và truy vấn, khóa, và giá trị được
cho như hQ,hK,hV, tương ứng. Trong mô hình ViT,
hQ=hK=hV=h. Đầu ra của lớp này được cho như:
MSA(hQ,hK,hV) = Concat(h1, . . . , hm)WO
trong đó hi = Attention(hQWQ_i,hKWK_i,hVWV_i) (1)
trong đó WO,WQ_i,WK_i, và WV_i là các ma trận phép
chiếu và m là số lượng đầu. Chúng tôi chia gợi ý p của chúng
tôi thành {pK,pV} ∈RLp/2×D và thêm chúng vào đầu hK
và hV với
2Chúng tôi giới thiệu độc giả đến các phần 4.2 và 5.4 của bài báo
DualPrompt [66] để thảo luận về lựa chọn thiết kế này.

tinh chỉnh tiền tố (P-T) như:
fP−T(p,h) = MSA(hQ,[pK;hK],[pV;hV]) (2)
Kết quả là bây giờ chúng tôi chỉ huấn luyện một số lượng
nhỏ tham số (các gợi ý) trong khi để phần còn lại của bộ
mã hóa transformer không thay đổi. Câu hỏi quan trọng hiện
tại là làm thế nào để chọn và cập nhật gợi ý một cách liên
tục? Phần tiếp theo sẽ thảo luận về cách các công trình trước
đây chọn và cập nhật gợi ý.

3.3. L2P và DualPrompt
L2P [67] và DualPrompt [66] chọn gợi ý từ một pool sử
dụng truy vấn gợi ý theo hình ảnh. Cụ thể, các phương pháp
này sử dụng chiến lược truy vấn dựa trên cặp khóa-giá trị3
để chọn động gợi ý cụ thể cho thể hiện từ một pool các gợi
ý ứng viên. Mỗi gợi ý pm được chọn từ một pool các khóa
có thể học km∈RD trong đó M là kích thước của pool gợi
ý, dựa trên độ tương đồng cosine với truy vấn phụ thuộc đầu
vào. Các truy vấn được tạo ra như: q(x)∈RD= θ(x) trong
đó θ là bộ mã hóa ViT tiền huấn luyện và x là hình ảnh đầu
vào4. Một truy vấn q(x) được khớp với một khóa km bằng
cách chọn khóa có độ tương đồng cosine cao nhất, được ký
hiệu như: γ(q(x),km). Các khóa được học với tối ưu hóa riêng
biệt từ mất mát phân loại tác vụ bằng cách đơn giản tối đa
hóa độ tương đồng cosine giữa mỗi q(x) được khớp và km,
hoạt động như một phương pháp giống phân cụm. Tuy nhiên,
phương pháp này không trực tiếp cập nhật khóa với gradient
từ mất mát tác vụ, điều này quan trọng trong các thí nghiệm
của chúng tôi.

4. Phương pháp
4.1. Hình thành Gợi ý
Mặc dù gợi ý đã được chứng minh hoạt động cực kỳ tốt trong
việc giảm thiểu quên thảm khốc, phương pháp tiên tiến hiện
tại DualPrompt thiếu khả năng mở rộng khả năng học trong
một tác vụ nhất định. Cụ thể, DualPrompt học một gợi ý
duy nhất cho mỗi tác vụ mới - bất kể tác vụ đó dễ (như học
một số ít lớp đối tượng mới) hay phức tạp (như học một số
lượng lớn lớp đối tượng mới). Người ta có thể thử tăng độ
dài của gợi ý, nhưng chúng tôi cho thấy trong các thí nghiệm
của chúng tôi (Phần 5.3) rằng việc tăng độ dài gợi ý có lợi
ích bão hòa. Một cách trực quan, thay vào đó chúng tôi mong
muốn một phương pháp trong đó khả năng học tương quan
với độ phức tạp cơ bản của dữ liệu tác vụ, thay vì một gợi
ý duy nhất. Ngoài ra, chúng tôi mong muốn một phương pháp
có thể vi phân từ đầu đến cuối, điều mà chúng tôi phỏng
đoán làm tăng khả năng học một tác vụ mới với độ chính
xác cao hơn.
3Chúng tôi lưu ý rằng sự khác biệt giữa L2P và DualPrompt đối với
truy vấn gợi ý là kích thước của pool gợi ý trong L2P được chọn như một
siêu tham số, nhưng trong DualPrompt nó bằng số lượng tác vụ và, trong
quá trình huấn luyện, DualPrompt chọn gợi ý sử dụng task-id (và chọn
khớp khóa-truy vấn gần nhất trong quá trình suy luận).
4Đầu ra được đọc từ token lớp.

Chúng tôi tăng khả năng học của mình bằng cách giới thiệu
một trục mới: một tập hợp các thành phần gợi ý. Thay vì chọn
lựa gợi ý từ một pool, chúng tôi học một tập hợp các thành
phần gợi ý mà, thông qua tổng có trọng số, tạo thành một
gợi ý phân tách5 được truyền đến lớp MSA tương ứng. Điều
này cho phép chúng tôi tăng khả năng gợi ý đến độ sâu tùy
ý và nắm bắt độ phức tạp cơ bản phong phú của bất kỳ tác
vụ nào trong khi duy trì độ dài gợi ý cố định. Ngoài ra, việc
gợi ý trong các tác vụ mới sẽ vốn dĩ tái sử dụng kiến thức
đã thu được trước đó của các tác vụ trong quá khứ thay vì
khởi tạo gợi ý tác vụ mới từ đầu.
Cụ thể, chúng tôi thay thế tham số gợi ý có thể học p bằng
tổng có trọng số trên các thành phần gợi ý:
p=Σ_m αmPm (3)
trong đó P∈RLp×D×M là tập hợp các thành phần gợi ý của
chúng tôi, M là độ dài của tập hợp chúng tôi (tức là, trục
bổ sung của khả năng mở rộng được giới thiệu), và α là
vector trọng số xác định đóng góp của mỗi thành phần sẽ
được thảo luận trong phần tiếp theo.
Một sự khác biệt rõ ràng giữa phương pháp của chúng tôi và
L2P/DualPrompt là tất cả các tham số có thể học của chúng
tôi được tối ưu hóa sử dụng mất mát phân loại thay vì chia
thành hai tối ưu hóa mất mát riêng biệt. Điều này cho phép
học từ đầu đến cuối tốt hơn trong phương pháp của chúng
tôi, và chúng tôi cho rằng đây là lý do chính tại sao phương
pháp của chúng tôi hoạt động tốt hơn trong các tiêu chuẩn
được mô tả trong Phần 5.

4.2. Cân bằng Trọng số Thành phần-Gợi ý
Thay vì một pool gợi ý, chúng tôi có một tập hợp các thành
phần gợi ý và mong muốn tạo ra một vector trọng số α cho
một truy vấn θ(x), thay vì một chỉ số gợi ý. Chúng tôi tính
toán độ tương đồng cosine giữa một truy vấn và các khóa
để tạo ra vector trọng số của chúng tôi như:
α=γ(q(x),K)
={γ(q(x),K1), γ(q(x),K2), . . . , γ(q(x),KM)} (4)
trong đó K∈RD×M chứa các khóa tương ứng với các thành
phần gợi ý của chúng tôi. Trực giác ở đây là cường độ đóng
góp của mỗi thành phần gợi ý Pm vào gợi ý cuối cùng p
được xác định bởi độ tương đồng giữa truy vấn q(x) và
khóa tương ứng Km.
Việc khớp gợi ý-truy vấn có thể được nghĩ như phân cụm
trong không gian chiều cao D, đây là một vấn đề khó và
nổi tiếng [32]. Để giảm thiểu nhược điểm này, chúng tôi
giới thiệu một thành phần khác vào việc khớp khóa-truy vấn
của chúng tôi: chú ý. Mỗi Pm có một vector chú ý tương
ứng Am ngoài khóa Km. Điều này cho phép truy vấn tập
trung vào các tính năng nhất định từ truy vấn chiều cao
5Phân tách theo nghĩa là gợi ý của chúng tôi là tổng có trọng số của
các thành phần.

q(x) đầu ra - ví dụ, một thành phần gợi ý được sử dụng cho
các tính năng mặt động vật có vú để phân loại chó và mèo
sẽ có thể tập trung vào các tính năng truy vấn liên quan như
vị trí mắt và kết cấu da, và hơn nữa bỏ qua các tính năng
của ngữ nghĩa không liên quan, như các tính năng hữu ích
cho phân loại ô tô. Chúng tôi sử dụng một sơ đồ chú ý lựa
chọn tính năng đơn giản với phép nhân theo phần tử giữa
vector truy vấn và vector chú ý để tạo ra truy vấn được chú
ý sau đó được sử dụng cho việc khớp độ tương đồng khóa.
Cụ thể, phương pháp cập nhật của chúng tôi để tạo ra vector
trọng số của chúng tôi là:
α=γ(q(x)⊙A,K)
={γ(q(x)⊙A1,K1), . . . , γ(q(x)⊙AM,KM)} (5)
trong đó A∈RD×M chứa các tham số có thể học (vector chú
ý) tương ứng với các thành phần gợi ý của chúng tôi và ⊙
là phép nhân theo phần tử (còn được gọi là phép nhân
Hadamard). Lưu ý rằng các vector chú ý của chúng tôi là
trọng số tính năng có thể học thay vì các module phụ thuộc
đầu vào - chúng tôi thấy rằng biểu diễn cố định này ít dễ
bị quên do thiết kế đơn giản của nó, tương tự như các khóa
thành phần gợi ý của chúng tôi.

4.3. Mở rộng & Trực giao
Quan trọng để giảm thiểu quên thảm khốc là việc tránh ghi
đè kiến thức đã thu được trong các tác vụ trước đó. Khi
chúng tôi đến một tác vụ mới, chúng tôi đóng băng các
thành phần hiện tại và mở rộng tập hợp, chỉ cập nhật các
thành phần mới. Điều này được hiển thị ở phía dưới Hình
2 nơi các tham số hiện có được khóa và chỉ các tham số mở
rộng được tối ưu hóa. Cụ thể, trong tác vụ t chúng tôi học
M/N thành phần trong đó N là số lượng tác vụ và M là
một siêu tham số được chọn, giữ các thành phần (t−1)·M/N
trước đó đóng băng. Việc mở rộng này được kích hoạt bởi
sơ đồ cân bằng trọng số thành phần dựa trên chú ý của
chúng tôi, trong đó việc mở rộng tham số của chúng tôi
không làm thay đổi việc tính toán trọng số α tương ứng
với các thành phần đã học trước đó. Ví dụ, việc mở rộng
khả năng của một mô hình phi tuyến như module MLP, đầu
chú ý transformer [13], hoặc hypernetwork [64] sẽ thay đổi
đầu ra trọng số của các thành phần đã học trước đó.
Mặc dù heuristic trước đó tập trung vào việc giảm quên thảm
khốc (không phá hủy kiến thức đã thu được trước đó), chúng
tôi cũng muốn giảm sự can thiệp giữa kiến thức hiện có và
mới. Để làm điều này, chúng tôi thêm một ràng buộc trực
giao vào P, K, và A. Trực giác là các vector trực giao có
ít sự can thiệp với nhau hơn. Ví dụ, chúng tôi muốn một
khóa và gợi ý đã học trong tác vụ 2 không thu hút dữ liệu
tác vụ 1, nếu không đầu ra bộ mã hóa cho dữ liệu tác vụ 1
sẽ thay đổi, dẫn đến dữ liệu bị phân loại sai. Chúng tôi sử
dụng khởi tạo trực giao và giới thiệu một mất mát phạt trực
giao đơn giản vào phương pháp của chúng tôi như:
Lortho(B) =||BB⊤−I||2 (6)
trong đó B biểu diễn bất kỳ ma trận tùy ý nào.

4.4. Tối ưu hóa Đầy đủ
Cho mất mát phân loại tác vụ L, tối ưu hóa đầy đủ của
chúng tôi là:
min_Pn,Kn,An,ϕn L(fϕ(fθ,P,K,A(x)), y) +
λ(Lortho(P) +Lortho(K) +Lortho(A)) (7)
trong đó Pn,Kn,An đề cập đến các thành phần gợi ý và
các khóa/vector chú ý tương ứng được mở khóa và

--- TRANG 6 ---
được huấn luyện trong tác vụ n (xem Phần 4.3) và λ là một
siêu tham số cân bằng mất mát trực giao. Lưu ý rằng chúng
tôi không cập nhật logit của các lớp tác vụ trong quá khứ,
phù hợp với L2P và DualPrompt. Chúng tôi chính thức gọi
phương pháp của chúng tôi là COntinual Decomposed 
Attention-based Prompting, hoặc đơn giản là CODA-P.

5. Thí nghiệm
Chúng tôi đo chuẩn phương pháp của chúng tôi với một số
bộ dữ liệu hình ảnh trong cài đặt học tập liên tục tăng dần
lớp. Chúng tôi triển khai các đường cơ sở không lưu trữ dữ
liệu huấn luyện để luyện tập lại: Learning without Forgetting
(LwF) [39], Learning to Prompt (L2P) [67], và DualPrompt
[66]. Ngoài ra, chúng tôi báo cáo hiệu suất giới hạn trên
(tức là, được huấn luyện ngoại tuyến) và hiệu suất cho một
mạng nơ-ron được huấn luyện chỉ trên mất mát phân loại
sử dụng dữ liệu huấn luyện tác vụ mới (chúng tôi gọi đây
là FT), và bao gồm một phiên bản cải tiến của FT sử dụng
cùng triển khai phân loại6 như L2P/DualPrompt (được gọi
là FT++). Chúng tôi cũng so sánh với Experience Replay
[11] để cung cấp ngữ cảnh bổ sung cho kết quả của chúng
tôi. Chúng tôi triển khai phương pháp và tất cả các đường
cơ sở trong PyTorch [49] sử dụng xương sống ViT-B/16
[13] tiền huấn luyện trên ImageNet-1K [54]. Đóng góp của
chúng tôi bao gồm các triển khai PyTorch trung thực của
các đường cơ sở gợi ý phổ biến L2P và DualPrompt, được
phát hành trong JAX [66, 67]. Các triển khai của chúng tôi
về các phương pháp cạnh tranh thực sự đạt được những cải
thiện nhẹ trong hiệu suất của DualPrompt ở hầu hết các
tiêu chuẩn, cũng như cải thiện hiệu suất đáng kể trong L2P
do loại gợi ý được cải thiện7 (mà chúng tôi gọi là L2P++).
DualPrompt sử dụng gợi ý độ dài 5 ở các lớp 1-2 (được
gọi là gợi ý chung) và gợi ý độ dài 20 ở các lớp 3-5 (được
gọi là gợi ý chuyên gia tác vụ). Chúng tôi chèn gợi ý vào
các lớp giống như DualPrompt (lớp 1-5) cho CODA-P và
sử dụng độ dài gợi ý 8 và 100 thành phần gợi ý, được chọn
với quét siêu tham số trên dữ liệu xác thực để có sự cân
bằng tốt nhất giữa hiệu suất và hiệu quả tham số. Vì phương
pháp của chúng tôi giới thiệu nhiều tham số có thể học hơn
so với DualPrompt, chúng tôi bao gồm một biến thể của
phương pháp CODA-P-S sử dụng số lượng tham số nhỏ
hơn bằng DualPrompt để so sánh bổ sung. Chúng tôi cho
thấy rằng phương pháp của chúng tôi vượt trội hơn các
phương pháp khác ngay cả trong điều kiện này, trong khi
vẫn giữ khả năng mở rộng nếu muốn.
Chúng tôi báo cáo kết quả trên bộ dữ liệu thử nghiệm, nhưng
nhấn mạnh rằng tất cả các siêu tham số và quyết định thiết
kế (bao gồm cho các đường cơ sở) được thực hiện sử dụng
dữ liệu xác thực (20% của dữ liệu huấn luyện). Không giống
như DualPrompt, chúng tôi chạy các tiêu chuẩn của mình
cho một số lần xáo trộn khác nhau của thứ tự lớp tác vụ và
báo cáo
6Xem Phụ lục B để biết chi tiết bổ sung
7Như đã thảo luận trong Phần 3.2, chúng tôi sử dụng tinh chỉnh tiền tố
thay vì tinh chỉnh gợi ý cho tất cả các triển khai vì được chỉ ra hoạt động
tốt nhất cho học tập liên tục [66].

trung bình và độ lệch chuẩn của các lần chạy này. Chúng tôi
làm điều này với seed nhất quán (khác nhau cho mỗi thử
nghiệm) để kết quả có thể được so sánh trực tiếp. Điều này
rất quan trọng vì kết quả với thứ tự xáo trộn khác nhau có
thể khác nhau do sự khác biệt về độ khó tác vụ và thứ tự
xuất hiện của chúng. Chúng tôi bao gồm chi tiết triển khai
bổ sung và kết quả trong Phụ lục A và B.
Chỉ số Đánh giá: Chúng tôi đánh giá các phương pháp sử
dụng (1) độ chính xác cuối cùng trung bình AN, hoặc độ
chính xác cuối cùng đối với tất cả các lớp trong quá khứ
được tính trung bình trên N tác vụ, và (2) quên trung bình
[9, 42, 43] FN, hoặc sự sụt giảm hiệu suất tác vụ được tính
trung bình trên N tác vụ. Độc giả được giới thiệu đến Phụ
lục B để biết chi tiết bổ sung về các chỉ số này. Chúng tôi
nhấn mạnh rằng AN là chỉ số quan trọng hơn và bao gồm
cả tính dẻo dai và quên của phương pháp, trong khi FN
chỉ đơn giản cung cấp ngữ cảnh bổ sung.

5.1. CODA-P thiết lập SOTA trên các tiêu chuẩn hiện có
Đầu tiên chúng tôi đánh giá phương pháp và tiên tiến của
chúng tôi trên một số tiêu chuẩn được thiết lập tốt. Bảng 1
cung cấp kết quả cho ImageNet-R [21, 66] bao gồm 200 lớp
đối tượng với một bộ sưu tập rộng các phong cách hình ảnh,
bao gồm phim hoạt hình, graffiti, và các ví dụ khó từ bộ
dữ liệu ImageNet gốc [54]. Tiêu chuẩn này hấp dẫn vì phân
phối dữ liệu huấn luyện có khoảng cách đáng kể so với dữ
liệu tiền huấn luyện (ImageNet), do đó cung cấp một cài
đặt vấn đề công bằng và thách thức. Ngoài tiêu chuẩn 10
tác vụ gốc, chúng tôi cũng cung cấp kết quả với số lượng
tác vụ lớn nhỏ hơn (5 tác vụ) và số lượng tác vụ nhỏ lớn
hơn (20 tác vụ). Đầu tiên chúng tôi nhận thấy rằng hiệu
suất báo cáo của chúng tôi cho DualPrompt cao hơn vài
phần trăm so với bài báo DualPrompt [66] gốc. Chúng tôi
triển khai lại phương pháp từ đầu trong một framework
khác (PyTorch [49]) và nghi ngờ sự khác biệt có liên quan
đến việc tính trung bình của chúng tôi trên các lần xáo trộn
thứ tự lớp khác nhau. Chúng tôi lưu ý rằng triển khai L2P
(L2P++) của chúng tôi hoạt động tốt hơn đáng kể so với
báo cáo gốc vì chúng tôi sử dụng cùng hình thức gợi ý như
DualPrompt (để so sánh công bằng). Hơn nữa, "Deep L2P
++" của chúng tôi, gợi ý ở cùng 5 lớp như DualPrompt
(thay vì chỉ ở lớp 1), thực sự hoạt động tương tự như
DualPrompt.
Quan trọng là, chúng tôi thấy rằng phương pháp của chúng
tôi có những cải thiện mạnh mẽ về độ chính xác trung bình
trên tất cả ba độ dài tác vụ, với cải thiện lên đến 4,5% về
độ chính xác trung bình so với DualPrompt. Chúng tôi
nhắc nhở độc giả rằng CODA-P là phương pháp đề xuất
của chúng tôi với độ dài gợi ý được điều chỉnh và kích
thước tập hợp thành phần, trong khi CODA-P-S được thu
nhỏ (xem Phụ lục A để biết chi tiết chính xác) để khớp
chính xác số lượng tham số có thể học như DualPrompt.
Chúng tôi nhận thấy rằng phương pháp của chúng tôi thường
có quên trung bình cao hơn một chút so với DualPrompt.
Cho rằng độ chính xác trung bình là chỉ số quan trọng nắm
bắt hiệu suất thực tế, chúng tôi không lo lắng về sự tăng
nhẹ trong quên. Thực tế,

--- TRANG 7 ---
Bảng 1. Kết quả (%) trên ImageNet-R. Kết quả được bao gồm cho 5 tác vụ (40 lớp mỗi tác vụ), 10 tác vụ (20 lớp mỗi tác vụ), và 20 tác vụ (10 lớp mỗi tác vụ). AN cho độ chính xác được tính trung bình trên các tác vụ và FN cho quên trung bình. Chúng tôi báo cáo kết quả trên 5 thử nghiệm.

[Bảng với các kết quả số liệu cho các phương pháp khác nhau]

chúng tôi cho rằng điều này rất hợp lý và phản ánh sức mạnh
của phương pháp chúng tôi: phương pháp của chúng tôi có
khả năng lớn hơn để học các tác vụ mới, và do đó chúng tôi
có thể hy sinh quên cao hơn một chút. Điều quan trọng là,
chúng tôi thấy rằng khi độ dài chuỗi tác vụ tăng lên, chỉ số
quên của phương pháp chúng tôi so với DualPrompt bắt đầu
hội tụ về một giá trị tương tự.
Chúng tôi cung cấp kết quả cho experience replay để cung
cấp ngữ cảnh bổ sung cho kết quả của chúng tôi. Mặc dù
khoảng cách giữa luyện tập lại với kích thước coreset đáng
kể (tức là, kích thước bộ đệm của dữ liệu replay) và phương
pháp của chúng tôi là nhỏ đối với chuỗi tác vụ ngắn, chúng
tôi thấy rằng phương pháp của chúng tôi vượt trội mạnh mẽ
hơn luyện tập lại trong chuỗi tác vụ dài hơn.
Bảng 2a và 2b cung cấp kết quả trên các tiêu chuẩn bổ sung8
mười tác vụ CIFAR-100 [33] và năm tác vụ DomainNet [50].
Mặc dù không bộ dữ liệu nào có tác động lớn như ImageNet-R
về mặt thách thức và khoảng cách từ phân phối tiền huấn
luyện, các bảng này cung cấp ngữ cảnh bổ sung cho hiệu
suất phương pháp của chúng tôi. Các bảng này minh họa
một câu chuyện tương tự như các tiêu chuẩn ImageNet-R,
với các cải thiện +3,2% và +2,5%, tương ứng. Chúng tôi
lưu ý rằng, đáng ngạc nhiên, LwF [39] vượt trội hơn một
chút so với gợi ý trên tiêu chuẩn này.

5.2. CODA-P thiết lập SOTA cho tiêu chuẩn dịch chuyển kép mới
Chúng tôi cũng đánh giá trên một tiêu chuẩn dịch chuyển kép
thách thức sử dụng bộ dữ liệu ImageNet-R. Động lực của
chúng tôi là để cho thấy độ bền vững đối với hai loại dịch
chuyển phân phối liên tục khác nhau: ngữ nghĩa và hiệp
phương sai. Chúng tôi thực hiện điều này bằng cách chọn
ngẫu nhiên các loại hình ảnh từ bộ dữ liệu ImageNet-R để
bao gồm trong dữ liệu huấn luyện của mỗi tác vụ (trong khi
giữ dữ liệu đánh giá không thay đổi). Độc giả được giới
thiệu đến Phụ lục C để biết thêm chi tiết. Kết quả cho tiêu
chuẩn này được cung cấp trong Bảng 3. So với 5 tác vụ
ImageNet-R nơi phương pháp của chúng tôi cải thiện SOTA
4,5% độ chính xác trung bình, chúng tôi thấy cải thiện 4,4%
trên tiêu chuẩn 5 tác vụ này với biên quên tương tự. Kết quả
của chúng tôi chỉ ra rằng CODA-P

8Lưu ý rằng các siêu tham số cho cả DualPrompt và CODA-P đều được
điều chỉnh trên ImageNet-R, điều này được thực hiện có chủ ý để đánh giá
độ bền vững của tất cả các phương pháp.

tổng quát hóa tốt hơn cho các loại dịch chuyển kiểu thế giới thực.

5.3. Ablation và Phân tích Bổ sung
Chúng tôi xem xét kỹ hơn phương pháp của chúng tôi với
các nghiên cứu ablation và phân tích bổ sung. Trong Bảng 4
chúng tôi cho thấy tác động của việc loại bỏ một vài thành
phần chính của phương pháp chúng tôi: khóa chú ý, đóng
băng các thành phần tác vụ trong quá khứ, và điều chỉnh
trực giao của chúng tôi. Chúng tôi cho thấy rằng việc loại
bỏ chú ý làm giảm nhẹ quên làm giảm hiệu suất về độ chính
xác trung bình (chỉ số quan trọng hơn). Điều này hợp lý
vì việc loại bỏ các khóa chú ý làm cho quá trình xử lý truy
vấn của chúng tôi phù hợp hơn với các phương pháp
L2P/DualPrompt tự hào có quên thấp nhưng thiếu khả năng
học đủ. Chúng tôi thấy sự sụt giảm lớn hơn khi loại bỏ đóng
băng và trực giao. Điều này chỉ ra rằng các khía cạnh này
rất quan trọng đối với phương pháp của chúng tôi. Trực giác
của chúng tôi là nếu không có chúng, việc hình thành gợi ý
của chúng tôi tương tự như một module MLP nông mà,
không được điều chỉnh, sẽ gặp phải quên cao.
Chúng tôi cũng xem xét khả năng phát triển của chúng tôi
trong khả năng mô hình dọc theo chiều thành phần gợi ý
mới được giới thiệu của chúng tôi sử dụng tiêu chuẩn 5 tác
vụ ImageNet-R (với dữ liệu xác thực). Chúng tôi cho thấy
trong Hình 3 rằng, với số lượng thành phần gợi ý bằng
số lượng gợi ý được học trong

[Hình 3. Phân tích độ chính xác trung bình AN so với các thành phần gợi ý]

DualPrompt, chúng tôi đạt được hiệu suất cao hơn. Quan
trọng là, khi chúng tôi tăng số lượng thành phần, chúng tôi
có thể tận dụng các tham số được mở rộng để có hiệu suất
cao hơn đáng kể, tìm thấy rằng phương pháp của chúng tôi
gần với hiệu suất giới hạn trên hơn là với DualPrompt về
độ chính xác trung bình. Vì pool gợi ý trong L2P cũng có
thể được tăng lên kích thước tùy ý, chúng tôi bao gồm kết
quả trong phân tích này. Tuy nhiên, chúng tôi cho thấy rằng
phương pháp L2P đạt đỉnh với kích thước pool bằng hai
lần độ dài chuỗi tác vụ, sau đó giảm hiệu suất. Điều này
phản ánh rằng các thành phần gợi ý của chúng tôi hưởng
lợi từ quy mô, trong khi pool gợi ý hiện có thực sự gặp
khó khăn.
Cuối cùng, chúng tôi cho thấy độ chính xác trung bình so
với độ dài gợi ý trong Hình 4 sử dụng cùng cài đặt như
trên. Mục đích của thí nghiệm này là nhấn mạnh rằng độ
chính xác bão hòa với độ dài gợi ý, do đó thúc đẩy nhu cầu
mở rộng trong chiều thành phần được giới thiệu của chúng
tôi. Phân tích bổ sung và quét siêu tham số có sẵn trong
Phụ lục B.

6. Kết luận
Chúng tôi trình bày COntinual decomposed attention-based
prompting (CODA-Prompt) cho học tập liên tục không cần
luyện tập lại. Phương pháp của chúng tôi lắp ráp các thành
phần gợi ý có thể học sau đó được chèn vào bộ mã hóa ViT
tiền huấn luyện để phân loại hình ảnh. Quan trọng là,
CODA-Prompt được tối ưu hóa theo kiểu từ đầu đến cuối
(không giống như các phương pháp SOTA trước đây có
hai tối ưu hóa riêng biệt). Hơn nữa, CODA-Prompt có thể
mở rộng khả năng gợi ý đến kích thước tùy ý. Chúng tôi
thiết lập một SOTA mới trên cả các tiêu chuẩn được thiết
lập tốt và một tiêu chuẩn dịch chuyển phân phối kép (chứa
các dịch chuyển ngữ nghĩa và hiệp phương sai), làm nổi
bật tác động thế giới thực tiềm năng và tính tổng quát của
phương pháp chúng tôi.

Lời cảm ơn
Tài liệu này dựa trên công trình được hỗ trợ bởi Quỹ Khoa
học Quốc gia dưới Grant số 2239292.

Tài liệu tham khảo
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang,
Hyojun Kim, và Taesup Moon. Ss-il: Separated softmax
for incremental learning. Trong Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV), trang
844–853, Tháng 10 2021. 2, 12
[2] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, và Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. Trong ECCV, 2018.
2
[3] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin, và Lucas PageCaccia. Online continual learning with maximal interfered
retrieval. Trong Advances in Neural Information Processing Systems, trang 11849–11860, 2019. 2
[4] Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Gradient based sample selection for online continual
learning. Trong Advances in Neural Information Processing
Systems, trang 11816–11825, 2019. 2
[5] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,
và Jonghyun Choi. Rainbow memory: Continual learning with a memory of diverse samples. Trong Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, trang 8218–8227, 2021. 2
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 3
[7] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, và Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. Advances
in neural information processing systems, 33:15920–15930,
2020. 2
[8] Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil,
Cordelia Schmid, và Karteek Alahari. End-to-end incremental learning. Trong Proceedings of the European Conference
on Computer Vision (ECCV), trang 233–248, 2018. 2
[9] Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach,
và Mohamed Elhoseiny. Efficient lifelong learning with agem. Trong International Conference on Learning Representations, 2019. 2, 6, 12
[10] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HSTorr, và Marc'Aurelio Ranzato. Continual learning with
tiny episodic memories. arXiv preprint arXiv:1902.10486,
2019. 2

--- TRANG 8 ---
[11] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, và Marc'Aurelio Ranzato. On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486,
2019. 6
[12] Yoojin Choi, Mostafa El-Khamy, và Jungwon Lee. Dualteacher class-incremental learning with data-free generative replay. Trong Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, trang 3543–
3552, 2021. 2
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 3, 5, 6
[14] Arthur Douillard, Alexandre Ramé, Guillaume Couairon,
và Matthieu Cord. Dytox: Transformers for continual
learning with dynamic token expansion. Trong Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, trang 9285–9295, 2022. 3
[15] Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, và
Marcus Rohrbach. Uncertainty-guided continual learning with bayesian neural networks. arXiv preprint
arXiv:1906.02425, 2019. 2
[16] Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor
Darrell, và Marcus Rohrbach. Adversarial continual learning. arXiv preprint arXiv:2003.09553, 2020. 2
[17] Qiankun Gao, Chen Zhao, Bernard Ghanem, và Jian
Zhang. R-dfcil: Relation-guided representation learning
for data-free class incremental learning. arXiv preprint
arXiv:2203.13104, 2022. 2
[18] Alexander Gepperth và Cem Karaoguz. Incremental learning with self-organizing maps. 2017 12th International
Workshop on Self-Organizing Maps and Learning Vector
Quantization, Clustering and Data Visualization (WSOM),
trang 1–8, 2017. 2
[19] Tyler L Hayes, Nathan D Cahill, và Christopher Kanan.
Memory efficient experience replay for streaming learning.
Trong 2019 International Conference on Robotics and Automation (ICRA), trang 9769–9776. IEEE, 2019. 2
[20] Tyler L Hayes và Christopher Kanan. Lifelong machine
learning with deep streaming linear discriminant analysis.
arXiv preprint arXiv:1909.01520, 2019. 3
[21] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,
Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,
và Justin Gilmer. The many faces of robustness: A critical
analysis of out-of-distribution generalization. ICCV, 2021.
2, 6, 12
[22] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015. 2
[23] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, và
Dahua Lin. Lifelong learning via progressive distillation and
retrospection. Trong Proceedings of the European Conference
on Computer Vision (ECCV), trang 437–452, 2018. 2

--- TRANG 9 ---
[24] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, và
Dahua Lin. Learning a unified classifier incrementally via rebalancing. Trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, trang 831–839, 2019.
2
[25] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, và
Zsolt Kira. Re-evaluating continual learning scenarios: A
categorization and case for strong baselines. arXiv preprint
arXiv:1810.12488, 2018. 2, 3
[26] Daniel Justus, John Brennan, Stephen Bonner, và Andrew Stephen McGough. Predicting the computational cost
of deep learning models. Trong 2018 IEEE international conference on big data (Big Data), trang 3873–3882. IEEE, 2018.
1
[27] Nitin Kamra, Umang Gupta, và Yan Liu. Deep generative
dual memory network for continual learning. arXiv preprint
arXiv:1710.10368, 2017. 2
[28] Ronald Kemker và Christopher Kanan. Fearnet: Braininspired model for incremental learning. International Conference on Learning Representations (ICLR), 2018. 2
[29] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler
Hayes, và Christopher Kanan. Measuring catastrophic forgetting in neural networks. AAAI Conference on Artificial
Intelligence, 2018. 2
[30] Diederik P Kingma và Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 12
[31] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 2017. 2
[32] Mario Köppen. The curse of dimensionality. Trong 5th online
world conference on soft computing in industrial applications (WSC5), tập 1, trang 4–8, 2000. 4
[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. Tech Report, 2009. 2, 7,
12
[34] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt,
và Thomas Dandres. Quantifying the carbon emissions of
machine learning. arXiv preprint arXiv:1910.09700, 2019.
1
[35] Kibok Lee, Kimin Lee, Jinwoo Shin, và Honglak Lee.
Overcoming catastrophic forgetting with unlabeled data in
the wild. Trong Proceedings of the IEEE International Conference on Computer Vision, trang 312–321, 2019. 2
[36] Soochan Lee, Junsoo Ha, Dongsu Zhang, và Gunhee Kim.
A neural dirichlet process mixture model for task-free continual learning. arXiv preprint arXiv:2001.00689, 2020. 2
[37] Timothée Lesort, Hugo Caselles-Dupré, Michael GarciaOrtiz, Andrei Stoian, và David Filliat. Generative models
from the perspective of continual learning. Trong 2019 International Joint Conference on Neural Networks (IJCNN), trang
1–8. IEEE, 2019. 2
[38] Duo Li, Guimei Cao, Yunlu Xu, Zhanzhan Cheng, và Yi
Niu. Technical report for iccv 2021 challenge sslad-track3b:
Transformers are better continual learners. arXiv preprint
arXiv:2201.04924, 2022. 3
[39] Zhizhong Li và Derek Hoiem. Learning without forgetting.
IEEE transactions on pattern analysis and machine intelligence, 40(12):2935–2947, 2017. 2, 6, 7
[40] Vincenzo Lomonaco và Davide Maltoni. Core50: a new
dataset and benchmark for continuous object recognition.
arXiv preprint arXiv:1705.03550, 2017. 2
[41] Vincenzo Lomonaco, Davide Maltoni, và Lorenzo Pellegrini. Rehearsal-free continual learning over small non-iid
batches. Trong CVPR Workshops, trang 989–998, 2020. 3
[42] David Lopez-Paz và Marc'Aurelio Ranzato. Gradient
episodic memory for continual learning. Trong Proceedings
of the 31st International Conference on Neural Information Processing Systems, NIPS'17, trang 6470–6479, USA,
2017. Curran Associates Inc. 2, 6, 12
[43] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, và Scott Sanner. Online continual learning in
image classification: An empirical survey. Neurocomputing,
469:28–51, 2022. 6, 12
[44] Davide Maltoni và Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios. Neural Networks,
116:56–73, 2019. 2
[45] Seyed Iman Mirzadeh, Arslan Chaudhry, Dong Yin, Timothy Nguyen, Razvan Pascanu, Dilan Gorur, và Mehrdad
Farajtabar. Architecture matters in continual learning. arXiv
preprint arXiv:2202.00275, 2022. 3
[46] Vaishnavh Nagarajan, Colin Raffel, và Ian J Goodfellow.
Theoretical insights into memorization in gans. Trong Neural
Information Processing Systems Workshop, 2018. 2
[47] Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal
Hassner, Vijay Mahadevan, và Stefano Soatto. Toward
understanding catastrophic forgetting in continual learning.
arXiv preprint arXiv:1908.01091, 2019. 1
[48] Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, và Moin Nabi. Learning to remember: A synaptic
plasticity driven framework for continual learning. Trong Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, trang 11321–11329, 2019. 2
[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.
6
[50] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, và Bo Wang. Moment matching for multi-source
domain adaptation. Trong Proceedings of the IEEE/CVF international conference on computer vision, trang 1406–1415,
2019. 7, 12
[51] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, và Christoph H. Lampert. icarl: Incremental classifier and representation learning. Trong 2017 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR'17, trang
5533–5542, 2017. 2
[52] Anthony Robins. Catastrophic forgetting, rehearsal and
pseudorehearsal. Connection Science, 7(2):123–146, 1995.
2
[53] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, và Gregory Wayne. Experience replay for continual
learning. Trong Advances in Neural Information Processing
Systems, trang 348–358, 2019. 2
[54] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision, 115(3):211–252, 2015. 6
[55] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, và Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671, 2016. 2
[56] Hanul Shin, Jung Kwon Lee, Jaehong Kim, và Jiwon Kim.
Continual learning with deep generative replay. Trong I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, và R. Garnett, editors, Advances in Neural Information Processing Systems 30, trang 2990–2999. Curran
Associates, Inc., 2017. 2
[57] James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen,
Hongxia Jin, và Zsolt Kira. Always be dreaming: A new approach for data-free class-incremental learning. Trong Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), trang 9374–9384, Tháng 10 2021. 2,
3
[58] James Seale Smith, Junjiao Tian, Yen-Chang Hsu, và Zsolt
Kira. A closer look at rehearsal-free continual learning.
arXiv preprint arXiv:2203.17269, 2022. 2
[59] Michalis K Titsias, Jonathan Schwarz, Alexander G de G
Matthews, Razvan Pascanu, và Yee Whye Teh. Functional
regularisation for continual learning with gaussian processes.
Trong International Conference on Learning Representations,
2019. 2
[60] Gido M van de Ven, Hava T Siegelmann, và Andreas S Tolias. Brain-inspired replay for continual learning with artificial neural networks. Nature communications, 11(1):1–14,
2020. 2
[61] Gido M van de Ven và Andreas S Tolias. Generative replay
with feedback connections as a general strategy for continual
learning. arXiv preprint arXiv:1809.10635, 2018. 2
[62] Gido M Van de Ven và Andreas S Tolias. Three scenarios for continual learning. arXiv preprint arXiv:1904.07734,
2019. 2
[63] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 3
[64] Johannes von Oswald, Christian Henning, João Sacramento,
và Benjamin F Grewe. Continual learning with hypernetworks. arXiv preprint arXiv:1906.00695, 2019. 2, 5
[65] Yabin Wang, Zhiwu Huang, và Xiaopeng Hong. S-prompts
learning with pre-trained transformers: An occam's razor for
domain incremental learning. Trong Conference on Neural Information Processing Systems (NeurIPS), 2022. 3
[66] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,
Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. arXiv
preprint arXiv:2204.04799, 2022. 1, 2, 3, 4, 6, 12
[67] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,
Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer
Dy, và Tomas Pfister. Learning to prompt for continual
learning. Trong Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, trang 139–149,
2022. 2, 3, 4, 6, 12
[68] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, và Yun Fu. Large scale incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, trang
374–382, 2019. 2, 3, 12
[69] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong
Li, Arun Mallya, Derek Hoiem, Niraj K Jha, và Jan Kautz.
Dreaming to distill: Data-free knowledge transfer via deepinversion. Trong Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, trang 8715–
8724, 2020. 2
[70] Wenpeng Yin, Jamaal Hay, và Dan Roth. Benchmarking
zero-shot text classification: Datasets, evaluation and entailment approach. arXiv preprint arXiv:1909.00161, 2019. 3
[71] Pei Yu, Yinpeng Chen, Ying Jin, và Zicheng Liu. Improving
vision transformers for incremental learning. arXiv preprint
arXiv:2112.06103, 2021. 3
[72] Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence. Trong International
Conference on Machine Learning, 2017. 2

--- TRANG 10 ---
[phần này tiếp tục danh sách tài liệu tham khảo từ trang trước]

--- TRANG 11 ---
Phụ lục
A. Chi tiết Triển khai Bổ sung
Đối với tất cả các phương pháp, chúng tôi sử dụng bộ tối ưu hóa Adam [30] với
β1=0.9 và β1=0.999, và kích thước batch là 128 hình ảnh. Chúng tôi thay đổi kích thước tất cả hình ảnh thành 224x224 và chuẩn hóa chúng thành [0,1]. Chúng tôi huấn luyện CIFAR-100 và DomainNet trong 20 epoch, và ImageNet-R trong 50 epoch (được chọn để đảm bảo các mô hình hội tụ hoàn toàn cho mỗi tác vụ). Như đã thảo luận trong văn bản chính, chúng tôi sử dụng cùng độ dài và vị trí gợi ý cho L2P [67] và DualPrompt [66] như khuyến nghị bởi bài báo DualPrompt gần đây hơn. Cụ thể, đối với Dualprompt, chúng tôi sử dụng gợi ý độ dài 5 ở các lớp 1-2 (được gọi là gợi ý chung) và gợi ý độ dài 20 ở các lớp 3-5 (được gọi là chuyên gia tác vụ). Đối với L2P, chúng tôi sử dụng pool gợi ý kích thước 20, tổng độ dài gợi ý kích thước 20, và chọn 5 gợi ý từ pool để sử dụng trong quá trình suy luận.

Như đã thực hiện trong DualPrompt [66], chúng tôi điều chỉnh tất cả các siêu tham số bổ sung sử dụng 20% dữ liệu huấn luyện làm dữ liệu xác thực. Điều này dẫn đến việc sử dụng tốc độ học 1e−3 cho tất cả các phương pháp gợi ý (thay vì 5e−3 như báo cáo trong DualPrompt), và tốc độ học 1e−4 cho tất cả các phương pháp tinh chỉnh mô hình hoàn toàn. Chúng tôi tìm kiếm tốc độ học trong các giá trị {1e−6,5e−6,1e−5,5e−5,1e−4,5e−4,1e−3,5e−3,1e−2}.

Chúng tôi cũng thấy rằng tốc độ học suy giảm cosine vượt trội hơn tốc độ học không đổi (được sử dụng trong triển khai DualPrompt gốc). Chúng tôi phỏng đoán rằng tốc độ học giảm và suy giảm giải thích cho cải thiện hiệu suất chúng tôi đạt được trên tiêu chuẩn 10 tác vụ ImageNet-R sử dụng các triển khai của chúng tôi.

Đối với phương pháp của chúng tôi, chúng tôi sử dụng độ dài gợi ý 8 và 100 thành phần gợi ý (và gợi ý ở cùng vị trí như DualPrompt), được chọn với quét siêu tham số trên dữ liệu xác thực để có sự cân bằng tốt nhất giữa hiệu suất và hiệu quả tham số. Chúng tôi tìm kiếm độ dài gợi ý trong khoảng [4,40] và thành phần gợi ý trong khoảng [5,500]. Chúng tôi sử dụng λ=0.1 để cân bằng mất mát điều chỉnh trực giao, được chọn từ quét qua các giá trị thập phân từ 1e−6 đến 1e2. Như được hiển thị trong Phần 5.3 của văn bản chính, chúng tôi thấy rằng việc tăng độ dài gợi ý có ít ảnh hưởng đến phương pháp của chúng tôi, trong khi việc tăng kích thước thành phần gợi ý có lợi ích mạnh mẽ lên đến 200 thành phần.

Cuối cùng, khi triển khai mất mát phân loại cho fine-tuning, L2P, DualPrompt, và CODA-P, chúng tôi sử dụng lại một kỹ thuật từ repo GitHub chính thức cho các bài báo DualPrompt và L2P [66, 67] và thay thế các dự đoán từ logit tác vụ trong quá khứ bằng âm vô cùng khi huấn luyện một tác vụ mới. Điều này dẫn đến dự đoán softmax "0" cho các lớp tác vụ trong quá khứ này và ngăn gradient chảy đến các đầu tuyến tính của các lớp tác vụ trong quá khứ. Mặc dù không được thảo luận trong các bài báo này, kỹ thuật này rất quan trọng cho hiệu suất của các phương pháp này, như chúng tôi đã xác nhận trong quá trình tái tạo. Về cơ bản, lớp tuyến tính bị thiên vị cao đối với các tác vụ mới trong học tập liên tục tăng dần lớp khi không có luyện tập lại, vì vậy kỹ thuật này ngăn đầu tuyến tính học thiên vị đối với các lớp mới so với các lớp trong quá khứ. Chúng tôi lưu ý rằng thiên vị này là một vấn đề nổi tiếng [1, 68].

B. Kết quả Bổ sung
Chúng tôi báo cáo kết quả mở rộng, bao gồm độ lệch chuẩn và tham số bổ sung được huấn luyện, cho tất cả các tiêu chuẩn trong các Bảng A (5 tác vụ ImageNet-R [21,66]), B (10 tác vụ ImageNet-R), C (20 tác vụ Imagenet-R), D (10 tác vụ CIFAR-100 [33]), E (5 tác vụ DomainNet [50]), và A (Dual-Shift ImageNet-R).

Chúng tôi đánh giá các phương pháp sử dụng (1) độ chính xác trung bình cuối cùng AN, hoặc độ chính xác thử nghiệm cuối cùng của mô hình được tính trung bình trên tất cả N tác vụ, và (2) quên trung bình [9, 42, 43] FN, hoặc sự sụt giảm hiệu suất tác vụ được tính trung bình trên N tác vụ. Độc giả được giới thiệu đến Phụ lục C của Wang et al. [66] để biết định nghĩa chính thức của các chỉ số. Chúng tôi nhấn mạnh rằng AN là chỉ số quan trọng hơn và bao gồm cả tính dẻo dai và quên của phương pháp, trong khi FN cung cấp ngữ cảnh bổ sung tùy thuộc vào tính dẻo dai của mô hình (tức là, một giá trị FN thấp hơn và một giá trị AN thấp hơn sẽ chỉ ra rằng quên thấp hơn của mô hình là kết quả từ khả năng thích ứng giảm đối với các tác vụ mới, đây là một đặc điểm không mong muốn).

Đối với mỗi kết quả, chúng tôi tính toán trung bình và độ lệch chuẩn trên các lần chạy riêng biệt. Mỗi lần chạy chứa các lần xáo trộn khác nhau của thứ tự lớp; cụ thể, chúng tôi xáo trộn các lớp sử dụng seed ngẫu nhiên được đặt cho mỗi "lần chạy thử nghiệm" - và tạo thành các tác vụ sử dụng lần xáo trộn lớp này. Quan trọng là, thứ tự lớp và tất cả các seed ngẫu nhiên, bao gồm khởi tạo mô hình, đều nhất quán giữa các phương pháp khác nhau trong cùng một "lần chạy thử nghiệm".

Chúng tôi báo cáo thêm số lượng tham số được huấn luyện (tức là, được mở khóa trong quá trình huấn luyện một tác vụ) cũng như tổng số tham số trong mô hình cuối cùng. Những điều này được báo cáo theo % của mô hình xương sống để so sánh dễ dàng. Quan trọng là, chúng tôi thiết kế CODA-P-S để có ít tham số hơn DualPrompt trong cài đặt 10 tác vụ ImageNet-R (cài đặt thí nghiệm chính của chúng tôi). Khi chúng tôi thay đổi số lượng tác vụ trong ImageNet-R, số lượng tham số tổng cho DualPrompt thay đổi vì kích thước pool được đặt bằng số lượng tác vụ tổng theo định nghĩa (không giống như chúng tôi, được đặt như một siêu tham số, cho phép chúng tôi tăng hoặc giảm số lượng tham số có thể huấn luyện để phù hợp với độ phức tạp cơ bản của tác vụ.)

C. Tiêu chuẩn ImageNet-R Dịch chuyển Kép
Động lực của chúng tôi cho tiêu chuẩn ImageNet-R [21, 66] dịch chuyển kép thách thức là để cho thấy độ bền vững đối với hai loại dịch chuyển phân phối liên tục khác nhau: ngữ nghĩa và

--- TRANG 12 ---
hiệp phương sai. Cụ thể, có 15 loại hình ảnh trong bộ dữ liệu ImageNet-R: 'art', 'cartoon', 'deviantart', 'embroidery', 'graffiti', 'graphic', 'misc', 'origami', 'painting', 'sculpture', 'sketch', 'sticker', 'tattoo', 'toy', 'videogame'. Chúng tôi chia bộ dữ liệu thành 5 tác vụ với 40 lớp mỗi tác vụ, và trong mỗi tác vụ chúng tôi ngẫu nhiên loại bỏ 8 trong số các loại miền từ dữ liệu huấn luyện. Tác vụ trở nên thách thức hơn vì bây giờ chúng tôi có các dịch chuyển miền loại hình ảnh được tiêm vào chuỗi tác vụ học tập liên tục, ngoài các dịch chuyển tăng dần lớp đã có sẵn.

--- TRANG 13 ---
[Chứa các bảng kết quả chi tiết với số liệu thống kê đầy đủ cho các thí nghiệm khác nhau]

--- TRANG 14 ---
[Tiếp tục các bảng kết quả chi tiết]

--- TRANG 15 ---
[Kết thúc các bảng kết quả và mô tả chi tiết về tiêu chuẩn dịch chuyển kép]
