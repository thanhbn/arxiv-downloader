Tài liệu tham khảo

Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, và Stefan Lee. 2019. nocaps: chú thích đối tượng mới ở quy mô lớn. Trong Hội nghị Quốc tế IEEE/CVF về Thị giác Máy tính 2019, ICCV 2019, Seoul, Hàn Quốc, 27 tháng 10 - 2 tháng 11, 2019.

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: một mô hình ngôn ngữ thị giác cho học tập ít mẫu. Advances in Neural Information Processing Systems, 35.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Các mô hình ngôn ngữ là những người học ít mẫu. Trong Advances in Neural Information Processing Systems 33: Hội nghị Thường niên về Hệ thống Xử lý Thông tin Thần kinh 2020, NeurIPS 2020, 6-12 tháng 12, 2020, ảo.

[Tiếp tục với các tài liệu tham khảo khác...]

--- TRANG 10 ---

[Tiếp tục dịch nội dung còn lại của tài liệu...]

A Minh chứng
Bốn đặc điểm cấp cao của bộ dữ liệu TEXTBIND trang bị cho MIM với nhiều khả năng đa dạng. Chúng tôi minh chứng những khả năng đó với các trường hợp sử dụng cụ thể của người dùng.

Tạo hình ảnh Một sự đổi mới cốt lõi của TEXTBIND là nó cho phép mô hình tạo ra hình ảnh dựa trên ngữ cảnh trò chuyện mà không cần chỉ dẫn rõ ràng từ người dùng. Đặc điểm này cực kỳ hữu ích cho các tình huống thế giới mở, bởi vì trong nhiều trường hợp mọi người có thể chỉ có một ý định ngầm và không có suy nghĩ rõ ràng về những gì hình ảnh nên là. Chúng tôi quan sát thấy rằng mô hình của chúng tôi có thể giải thích các khái niệm và ý tưởng cho người dùng với hình ảnh sinh động (Hình 5a), tạo ra hình ảnh với cảm xúc đúng (Hình 5b), và chỉnh sửa hình ảnh dựa trên toàn bộ ngữ cảnh (Hình 5c và 5d). Hơn nữa, như được hiển thị trong Hình 4, chúng tôi khám phá ra rằng mô hình của chúng tôi thành thạo trong việc tạo ra các câu chuyện dài có văn bản và hình ảnh đan xen trong khi duy trì tính nhất quán đặc biệt.

So sánh hình ảnh Một đặc điểm thú vị khác của TEXTBIND là nó có thể so sánh hoặc liên kết thông tin trong nhiều hình ảnh. Ví dụ, mô hình của chúng tôi có thể giải thích đúng các phần khác nhau và chung trong nhiều hình ảnh trong Hình 6.

Hiểu biết hình ảnh nội tại & ngoại tại
Mô hình được đào tạo trên TEXTBIND có thể hiểu nội dung trong hình ảnh một cách chính xác trong một cuộc trò chuyện nhiều lượt. Trong cả ba hình phụ của Hình 7, mô hình theo dõi chính xác các hướng dẫn của con người và giải thích chi tiết của hình ảnh cho người dùng. Hơn nữa, TEXTBIND cũng cho phép mô hình khám phá ý nghĩa của một hình ảnh vượt ra ngoài các ký hiệu trong nó. Ví dụ, mô hình cũng giải thích ảnh hưởng của album của Bob Dylan trong Hình 7b và tác động của iPhone trong Hình 7c.

B Lời nhắc của TEXTBIND
Lời nhắc được hiển thị trong Hình 8.

C Ví dụ về các cuộc trò chuyện được xây dựng
Các ví dụ được đưa ra trong Hình 9.

D So sánh với các bộ dữ liệu trước đó
Chúng tôi tiếp tục điều tra sự đa dạng từ vựng của dữ liệu được tạo ra, bao gồm cả hướng dẫn và phản hồi. Sự đa dạng của văn bản trong một bộ dữ liệu được định nghĩa là ∑(n=2 đến 4)(#N-gram duy nhất/#Tổng số n-gram), phù hợp với các công trình trước đó (Su et al., 2022). Như được hiển thị trong Bảng 2, bộ dữ liệu của chúng tôi đạt điểm số đa dạng tốt hơn so với hầu hết các bộ dữ liệu thị giác-ngôn ngữ hiện có, cho thấy rằng ngôn ngữ được sử dụng trong bộ dữ liệu của chúng tôi có nhiều thông tin hơn.

E Hướng dẫn chú thích con người
Hướng dẫn toàn diện cho đánh giá con người được hiển thị trong Bảng 8.

F Chi tiết triển khai (Dữ liệu)
Chúng tôi xây dựng bộ dữ liệu TEXTBIND của mình dựa trên bộ dữ liệu CONCEPTUAL CAPTIONS 3M (CC3M) (Sharma et al., 2018; Changpinyo et al., 2021), chỉ cung cấp các cặp hình ảnh-chú thích. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng mô hình clip-vit-base-patch16 được phát hành bởi OpenAI (Radford et al., 2021) để lọc ra các cặp hình ảnh-chú thích có điểm khớp thấp hơn 30. Chúng tôi sử dụng thuật toán phân cụm k-means được triển khai bởi bộ công cụ FAISS (Johnson et al., 2019) để phân loại bộ dữ liệu CC3M đã được làm sạch thành 4096 cụm. Các đặc trưng được sử dụng cho phân cụm k-means là các biểu diễn ẩn của hình ảnh được mã hóa bởi mô hình clip-vit-base-patch16. Ngoài ra, các cụm có ít hơn 32 hình ảnh được coi là ngoại lai và sẽ không được xem xét. Số lượng hình ảnh trong mỗi cuộc trò chuyện được lấy mẫu từ {2,3,4}. Chúng tôi truy cập mô hình GPT-4 thông qua OpenAI API, và đặt các siêu tham số top_p và temperature thành 1.0.

G Các cuộc trò chuyện được xây dựng với nhãn "Kém"
Trong Bảng 9, chúng tôi xác định ba loại lỗi điển hình có trong bộ dữ liệu được xây dựng. Mặc dù đặt ngưỡng cao để lọc ra các cặp hình ảnh-chú thích không khớp, một số trường hợp không khớp không thể được phát hiện bởi mô hình CLIP (Radford et al., 2021). Một vài cuộc trò chuyện gặp phải sự không nhất quán và ảo giác có thể được quy cho mô hình GPT-4. Tổng thể, trong khi một số lượng nhỏ cuộc trò chuyện bị ảnh hưởng bởi các lỗi khó phát hiện bằng quy tắc, hầu hết các cuộc trò chuyện được tạo ra thể hiện chất lượng cao. Chúng tôi trình bày một số trường hợp được gắn nhãn "Kém". Chúng tôi có thể thấy rằng hầu hết những trường hợp "Kém" này chỉ có các vấn đề nhỏ và không rõ ràng.

H Chi tiết triển khai (Mô hình)
Các thí nghiệm của chúng tôi dựa trên Huggingface Transformers (Wolf et al., 2020) và DeepSpeed (Rasley et al., 2020). Chúng tôi sử dụng các chú thích tổng hợp được lọc do BLIP (Li et al., 2022) cung cấp, bao gồm Conceptual Captions (Changpinyo et al., 2021; Sharma et al., 2018) và SBU (Ordonez et al., 2011), tổng cộng 12M cặp hình ảnh-chú thích. Chúng tôi sử dụng cùng bộ mã hóa thị giác và Q-Former như được sử dụng trong BLIP-2 (Li et al., 2023b) và sử dụng trọng số của chúng để khởi tạo. LLama2-Chat (Touvron et al., 2023) được sử dụng làm mô hình ngôn ngữ xương sống. Đối với mô hình tạo hình ảnh, chúng tôi sử dụng Stable Diffusion XL (Podell et al., 2023). Cấu hình đào tạo được hiển thị trong Bảng 10. Chúng tôi sử dụng 8 GPU NVIDIA A100 (40G) cho tất cả các thí nghiệm.

I Hướng dẫn đánh giá con người
Đối với đánh giá chất lượng tổng thể, chúng tôi áp dụng các tiêu chí sau đây.

• Điểm 4: Phản hồi xuất sắc.
• Điểm 3: Phản hồi chấp nhận được nhưng có thể không quá nhiều thông tin và thú vị.
• Điểm 2: Phản hồi có vấn đề nhỏ, như ảo giác nhẹ khi mô tả hình ảnh trong ngữ cảnh.
• Điểm 1: Phản hồi không hợp lệ và có nhược điểm đáng kể, ví dụ như không liên quan đến ngữ cảnh.

Đối với các chú thích chi tiết, nhận dạng ý định đánh giá liệu phản hồi có thực hiện ý định của người dùng hay không, hiểu biết ngữ cảnh xem xét liệu phản hồi có hiểu đúng thông tin trong văn bản và hình ảnh hay không, và tính thông tin của phản hồi đo lường tính hữu ích của phản hồi. Đối với mỗi khía cạnh, một chú thích viên con người sẽ gán một điểm trong {1,2,3,4}. Bốn điểm từ 1 đến 4 lần lượt chỉ ra "lỗi lớn", "lỗi nhỏ", "chấp nhận được", và "hoàn hảo".

J Định dạng dữ liệu của đầu vào mô hình
Định dạng dữ liệu của đầu vào mô hình được đưa ra trong Hình 12.

K Sử dụng TEXTBIND
TEXTBIND hoàn toàn mã nguồn mở và có thể được sử dụng cho mục đích nghiên cứu học thuật và thương mại.
