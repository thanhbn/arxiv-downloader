# 2401.00849.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2401.00849.pdf
# Kích thước tệp: 6368500 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
COSMO :Mô hình MultimOdal Contrastive Streamlined với
Huấn luyện trước Interleaved
Alex Jinpeng Wang1Linjie Li2Kevin Qinghong Lin1Jianfeng Wang2
Kevin Lin2Zhengyuan Yang2Lijuan Wang2Mike Zheng Shou1
1Show Lab, Đại học Quốc gia Singapore2Microsoft Azure AI
http://fingerrec.github.io/cosmo
Tóm tắt
Trong sự tiến hóa của Huấn luyện trước Thị giác-Ngôn ngữ, việc chuyển đổi từ hiểu ngắn gọn văn bản sang bao gồm các bối cảnh văn bản mở rộng là then chốt. Các mô hình thị giác-ngôn ngữ tự hồi quy gần đây như [2, 14], tận dụng khả năng bối cảnh dài của Các Mô hình Ngôn ngữ Lớn, đã xuất sắc trong các nhiệm vụ tạo văn bản few-shot nhưng gặp thách thức trong các nhiệm vụ căn chỉnh. Để giải quyết khoảng trống này, chúng tôi giới thiệu mất mát contrastive vào các mô hình tạo văn bản, trình bày khung COntrastive-Streamlined MultimOdal (CosMo), phân chia chiến lược mô hình ngôn ngữ thành các thành phần xử lý văn bản unimodal chuyên dụng và xử lý dữ liệu đa phương thức thành thạo. CosMo, khung thống nhất của chúng tôi, kết hợp các yếu tố unimodal và multimodal, nâng cao hiệu suất mô hình cho các nhiệm vụ liên quan đến dữ liệu văn bản và thị giác trong khi đáng chú ý là giảm các tham số có thể học được. Tuy nhiên, các mô hình này đòi hỏi các bộ dữ liệu văn bản dài mở rộng, nhưng sự sẵn có của các bộ dữ liệu video-văn bản dài chất lượng cao vẫn còn hạn chế. Để bắc cầu khoảng trống này, công trình này giới thiệu Howto-Interlink7M, một bộ dữ liệu video-văn bản interleaved đầu tiên có các chú thích toàn diện, đánh dấu một bước tiến đáng kể. Chứng minh tác động của nó, chúng tôi minh họa cách Howto-Interlink7M nâng cao hiệu suất mô hình trong các nhiệm vụ hình ảnh-văn bản. Với 34% tham số có thể học được và sử dụng 72% dữ liệu có sẵn, mô hình của chúng tôi thể hiện sự vượt trội đáng kể so với OpenFlamingo [3]. Ví dụ, trong nhiệm vụ captioning flickr 4-shot, hiệu suất cải thiện đáng kể từ 57.2% lên 65.1%. Các đóng góp của CosMo và Howto-Interlink7M được nhấn mạnh bởi những cải thiện hiệu suất đáng chú ý trên 14 bộ dữ liệu downstream đa dạng bao gồm cả nhiệm vụ hình ảnh-văn bản và video-văn bản.

1. Giới thiệu
Sự xuất hiện của Các Mô hình Ngôn ngữ Lớn (LLMs) [35, 47, 61] đã thúc đẩy đáng kể sự phát triển của các mô hình học đa phương thức. Một lợi thế đáng chú ý nằm ở khả năng của LLMs xử lý hiệu quả các đầu vào văn bản cực kỳ dài, với khả năng lý luận mạnh mẽ [7, 56]. Khả năng này đại diện cho một bước tiến đáng kể trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên, nhấn mạnh tiềm năng của LLMs trong việc giải quyết dữ liệu phức tạp, đa chiều. Thành công của LLMs đã khơi dậy sự quan tâm và nỗ lực đáng kể trong việc tận dụng nó cho đa phương thức.

Học trong bối cảnh [6, 12] cung cấp một con đường khả thi cho các mô hình chấp nhận các đầu vào văn bản dài trong lĩnh vực học đa phương thức. Những tiến bộ gần đây trong việc sử dụng học trong bối cảnh trong các LLMs đa phương thức đã thúc đẩy sự phát triển của các Mô hình Thống nhất với Khả năng Nổi lên, được minh họa bởi Flamingo [2] và PALM-E [14], được trưng bày trong Hình 1. Các khung thống nhất này cung cấp khả năng đáng chú ý để giải quyết nhiều nhiệm vụ downstream mà không cần fine-tuning. Khả năng này một phần được quy cho thiết kế kiến trúc của chúng, hỗ trợ việc sử dụng nhiều cặp hình ảnh-văn bản làm đầu vào và tổ chức dữ liệu thành định dạng "interleaved". Trong khi các mô hình này đã thể hiện thành công đáng chú ý trong các nhiệm vụ như Trả lời Câu hỏi Thị giác (VQA) và Captioning, kiến trúc được đề xuất bởi Flamingo [2] không được tối ưu hóa cho các nhiệm vụ phân loại vì thiết kế cố hữu của nó tập trung vào việc tạo văn bản thay vì phân loại.

Dữ liệu interleaved chất lượng cao được yêu cầu để cho phép các mô hình có khả năng học đa phương thức trong bối cảnh. Tuy nhiên, phần lớn các bộ dữ liệu có sẵn công khai, như CC3M [44], LAION400M [43], và DataComp1B [42], chủ yếu bao gồm các cặp hình ảnh-văn bản ngắn. Những nỗ lực gần đây của bộ dữ liệu CM3 [1], MMC4 [63] và Obelics [25] đã giới thiệu ba bộ dữ liệu interleaved có thể truy cập công khai, dựa trên các tài liệu web. Tuy nhiên, các tài liệu web có thể nhiễu, vì các hình ảnh trên cùng một trang có thể không có mối tương quan cao (xem Hình 4). So với các tài liệu web, video tự nhiên bao gồm các chuỗi hình ảnh có tương quan cao. Để đáp ứng, các sáng kiến như InternVid [53] và Video Chapters [58] đã đề xuất các chú thích dài hơn được tạo bởi các mô hình ngôn ngữ hoặc các chương được chú thích bởi người dùng. Mặc dù có những tiến bộ này, sự sẵn có của dữ liệu video-văn bản interleaved, thể hiện mối quan hệ giữa các clip khác nhau, vẫn còn thiếu.

Để giải quyết những thách thức này, bài báo này giới thiệu một kiến trúc mới có khả năng xử lý bốn loại đầu vào khác nhau, bao gồm dữ liệu interleaved, nhằm khắc phục các hạn chế được quan sát trong Flamingo. Cách tiếp cận của chúng tôi bao gồm việc chia LLM thành hai phần: phần đầu tiên chuyên biệt như một bộ mã hóa văn bản, trong khi phần thứ hai được sử dụng để hợp nhất đa phương thức. Ngoài ra, chúng tôi trình bày Howto-Interlink7M, một bộ dữ liệu video-văn bản interleaved chất lượng cao được lấy từ Howto100M [34], bằng cách tận dụng GPT-4 [35]. CosMo được đánh giá trên tổng cộng 14 benchmark hình ảnh-văn bản và video-văn bản, đạt được hiệu suất vượt trội so với Open-Flamingo [3], trong khi sử dụng ít mẫu hơn từ cùng các bộ dữ liệu công khai. Kết quả của chúng tôi cũng cho thấy dữ liệu video-văn bản chất lượng cao từ Howto-Interlink7M của chúng tôi nâng cao thêm hiệu suất của CosMo, thậm chí còn giúp đỡ trong các nhiệm vụ hình ảnh-văn bản.

Các đóng góp chính của chúng tôi bao gồm: (i). Chúng tôi giới thiệu một kiến trúc mới CosMo cho huấn luyện trước dữ liệu interleaved, tận dụng một mất mát contrastive bổ sung. Chỉ với 34% tham số có thể học được, phương pháp của chúng tôi vượt trội rõ ràng so với [3]. (ii). Chúng tôi giới thiệu Howto-Interlink7M, một bổ sung đáng chú ý cho các bộ dữ liệu đa phương thức văn bản dài. (iii). Chúng tôi cho thấy rằng dữ liệu video-văn bản interleaved hàng đầu làm tăng hiệu suất mô hình trong các nhiệm vụ hình ảnh-văn bản và video-văn bản khác nhau.

2. Công trình liên quan
Huấn luyện trước Thị giác-Ngôn ngữ. Sự tiến hóa của Mô hình hóa Ngôn ngữ đã tác động đáng kể đến các phương pháp huấn luyện trước thị giác-ngôn ngữ. Các cách tiếp cận truyền thống như OSCAR [29], ViLT [23] và UNITER [10], được xây dựng trên các kiến trúc ngôn ngữ BERT [11], đã thể hiện sự thành thạo trong các nhiệm vụ downstream mà không cần fine-tuning mở rộng. Tuy nhiên, trọng tâm chuyển sang khai thác các mô hình ngôn ngữ lớn hơn [28, 48]. Ví dụ, Flamingo [2] đã tăng từ 1.4B lên 70B tham số mô hình ngôn ngữ đáng kinh ngạc, thể hiện hiệu suất mạnh mẽ trên các nhiệm vụ downstream khác nhau. Đáng chú ý, kiến trúc của Flamingo thành thạo trong việc xử lý các bộ dữ liệu hình ảnh/video văn bản interleaved thông qua các định dạng đầu vào chuyên biệt và cơ chế cross-attention. Tuy nhiên, hiệu suất của nó thua kém các mô hình contrastive trong các nhiệm vụ phân loại, một hạn chế so với các công trình khác như CLIP [39] và CoCa [59], thành công với các mô hình học contrastive.

Để tận dụng những hiểu biết này, chúng tôi áp dụng thiết kế đầu vào của Flamingo và kết hợp mất mát contrastive trong các biểu diễn LLM cấp trung, để nâng cao thêm sự căn chỉnh giữa các biểu diễn thị giác và văn bản.

Dữ liệu Interleaved cho Học Đa phương thức. Việc có được các bộ dữ liệu thị giác-ngôn ngữ được chú thích thủ công là cực kỳ đắt đỏ, dẫn đến các bộ dữ liệu quy mô tương đối nhỏ (thường nhỏ hơn 100k trường hợp) như COCO [31] và Visual Genome [24]. Theo truyền thống, các bộ dữ liệu thị giác-ngôn ngữ chủ yếu bao gồm các cặp hình ảnh-văn bản từ Internet, như CC3M [44] và WIT trong CLIP [39]. Các chú thích văn bản trong các bộ dữ liệu được thu thập từ web này, chủ yếu là các mô tả alt-text, thường ngắn về độ dài, do đó ít mô tả hơn. Một sự thay đổi đổi mới được giới thiệu bởi Flamingo [2], tiên phong trong khái niệm các cặp hình ảnh-văn bản dài. Những tiến bộ gần đây từ Flamingo [2] và CM3 [1] nhấn mạnh tầm quan trọng của việc huấn luyện trên toàn bộ các trang web đa phương thức, trình bày các hình ảnh và văn bản interleaved như một chuỗi gắn kết. Các bộ dữ liệu interleaved này vốn bao gồm nhiều cặp hình ảnh và văn bản, đóng góp vào cảnh quan đang phát triển của học đa phương thức.

Bộ dữ liệu MMC4 [63] đáng chú ý là công trình có sẵn công khai đầu tiên trực tiếp giải quyết dữ liệu interleaved đa hình ảnh/đa câu. Tuy nhiên, MMC4 thiếu một chi tiết quan trọng, vị trí chính xác của hình ảnh trong cấu trúc tài liệu, được giải quyết trong obelics [25].

Nghiên cứu hiện tại đã tập trung chủ yếu vào việc trích xuất dữ liệu hình ảnh-văn bản có tương quan cao từ các tài liệu web nhiễu. Ngược lại, công trình của chúng tôi giới thiệu một bộ dữ liệu video-văn bản interleaved tiên phong, đánh dấu sự khởi đầu từ các phương thức chỉ hình ảnh-văn bản.

3. Howto-Interlink7M
Động lực. Hiệu quả của huấn luyện trước video-ngôn ngữ thường phụ thuộc vào sự sẵn có của các chú thích được chú thích chất lượng cao. Các bộ dữ liệu hiện có như Howto100M [34] và YT-Temporal [60] chủ yếu dựa vào các video YouTube với văn bản được tạo bởi Nhận dạng Giọng nói Tự động (ASR). Tuy nhiên, các chú thích được tạo bởi ASR này gặp vấn đề về sự căn chỉnh yếu với nội dung video. Như quan sát trong Howto100M, chỉ 51% clip có các đối tượng hoặc hành động được đề cập trong chú thích của chúng được mô tả một cách rõ ràng trong các clip được lấy mẫu [34]. Hạn chế này đặt ra một thách thức đáng kể cho cộng đồng huấn luyện trước thị giác-ngôn ngữ. Để giải quyết vấn đề này, chúng tôi giới thiệu Howto-Interlink7M, một bộ dữ liệu video-văn bản interleaved mới nhằm cung cấp các chú thích chất lượng cao để cải thiện hiểu biết video-ngôn ngữ.

Quy trình Tạo. Để xây dựng Howto-Interlink7M, chúng tôi bắt đầu với bộ dữ liệu HowTo100M có sẵn công khai, nhưng thực hiện một cách tiếp cận đặc biệt. Khác với bộ dữ liệu gốc, chúng tôi phân đoạn các video gốc thành các cảnh bằng cách sử dụng một detector cảnh, cụ thể là KTS [38]. Đối với mỗi cảnh, chúng tôi sử dụng các mô hình captioning có sẵn như BLIP2 [28] để tạo các mô tả ngắn gọn. Hơn nữa, chúng tôi sử dụng GRIT [54] để tạo các chú thích vùng dày đặc với hộp bao, nâng cao các chú thích với các mô tả phong phú và chi tiết vị trí. Để đơn giản, chúng tôi đặt tên tất cả thông tin này là chú thích chi tiết. Đối với clip đầu tiên của video, chúng tôi tận dụng mô hình GPT-4 [35] để tạo các tóm tắt toàn diện chỉ từ chú thích chi tiết. Quan trọng là, các chú thích của các clip tiếp theo được điều kiện hóa trên bối cảnh của các clip trước đó, duy trì tính liên tục của câu chuyện. Chúng tôi cung cấp các hướng dẫn rõ ràng cho GPT-4, nhấn mạnh việc bảo tồn thông tin ASR để giữ lại các danh từ và hành động. Cách tiếp cận này tăng cường mối liên kết giữa các clip riêng lẻ, thúc đẩy một chú thích gắn kết hơn.

Phân tích. Trong Hình 2, chúng tôi trưng bày một ví dụ minh họa bản chất toàn diện và chi tiết của các đoạn văn được tạo. Đáng chú ý, các quan sát của chúng tôi chỉ ra sự bảo tồn các chi tiết cụ thể trên các clip khác nhau, bao gồm tên của các diễn viên và các đối tượng được đề cập như thực phẩm. Hơn nữa, chúng tôi thực hiện một phân tích so sánh giữa bộ dữ liệu HowTo100M [34] gốc và Howto-Interlink7M của chúng tôi, như được trình bày trong Bảng 1. Các chú thích được tạo thể hiện độ dài lớn hơn và căn chỉnh tốt hơn với nội dung video, như được chứng minh bởi điểm tương đồng CLIP cao hơn.

4. CosMo
Trong phần này, chúng tôi giới thiệu CosMo, viết tắt của khung COntrastive-Streamlined MultimOdal, phân chia chiến lược một LLM thành các thành phần xử lý văn bản unimodal chuyên dụng và xử lý dữ liệu đa phương thức thành thạo. CosMo thêm một mất mát contrastive bổ sung vào mất mát mô hình ngôn ngữ trong các baseline Flamingo [2, 3], hỗ trợ cả nhiệm vụ phân loại và tạo. Trong khi ALBEF [27] và CoCa [59] cũng tích hợp mất mát contrastive, trọng tâm của chúng tôi nằm ở việc phát triển các LLMs chuyên biệt trong học trong bối cảnh và xử lý các chuỗi dữ liệu interleaved mở rộng, tách biệt công trình của chúng tôi khỏi các phương pháp này. Ngoài ra, chúng tôi hợp lý hóa kiến trúc mô hình của mình, giảm số lượng tham số có thể học được để cải thiện hiệu quả tính toán trong khi duy trì hiệu lực trong học đa phương thức.

4.1. Kiến trúc Tổng thể
Như được thể hiện trong Hình 3, CosMo bao gồm hai thành phần: một bộ mã hóa thị giác và một LLM được huấn luyện trước. Bộ mã hóa thị giác, dựa trên Vision Transformer (ViT) [13] từ Open-CLIP [20], vẫn nhất quán trong các thí nghiệm của chúng tôi. Đối với mô hình ngôn ngữ, chúng tôi chủ yếu sử dụng mô hình OPT [61], được phân chia thành hai phần để phù hợp với các nhiệm vụ đa dạng trong khi giảm tham số tổng thể.

Biểu diễn Đầu vào như Tài liệu. CosMo chấp nhận bốn loại dữ liệu: hình ảnh-văn bản, video-văn bản, hình ảnh-văn bản interleaved, và video-văn bản interleaved, tất cả được xử lý thành định dạng chuỗi văn bản kiểu tài liệu. Định dạng này bao gồm thông tin thị giác và văn bản, được cấu trúc như " <s><Visual 1 >Text1 <EOC><Visual 2 >Text2 <EOC>", với <s> đánh dấu bắt đầu tài liệu và <EOC> biểu thị kết thúc mỗi chú thích văn bản. Token <Visual> đại diện cho sự hiện diện của một hình ảnh hoặc video.

Để xử lý hiệu quả các tài liệu dài hơn trong khi bị hạn chế với ngân sách bộ nhớ GPU cố định, chúng tôi triển khai một chiến lược lấy mẫu ngẫu nhiên để thu thập các đầu vào với số lượng token cố định. Chúng tôi đầu tiên xác định vị trí của token <Visual>, và chọn ngẫu nhiên một hình ảnh làm điểm neo. Tiếp theo, chúng tôi giới thiệu một sự dịch chuyển ngẫu nhiên nhỏ đến vị trí hình ảnh, và sau đó lấy mẫu 128 token tiếp theo. Các hình ảnh hoặc video tương ứng với các token <Visual> phục vụ như đầu vào cho bộ mã hóa thị giác.

Trích xuất Đặc trưng Uni-modality. Để giảm thiểu quên thảm khốc, lấy cảm hứng từ các công trình gần đây [2, 15], chúng tôi đóng băng cả LLMs và bộ mã hóa thị giác. Hình ảnh hoặc video được đưa trực tiếp vào bộ mã hóa thị giác đóng băng, trong khi các tài liệu được đưa vào nửa đầu các lớp của LLM.

Hợp nhất Đa phương thức Nhẹ. Sau đó, chúng tôi tận dụng các token thị giác để điều kiện hóa khối mô hình ngôn ngữ đóng băng thông qua các lớp cross-attention có cổng, có thiết kế tương tự với Flamingo [2] và CoCa [59]. Chiến lược này tích hợp hiệu quả thông tin thị giác để dự đoán token tiếp theo chính xác. Tuy nhiên, một điểm khác biệt chính từ các phương pháp trước đó là chúng tôi giới thiệu các bottleneck [18, 46] trong các kênh đặc trưng đầu vào và đầu ra, dẫn đến việc giảm đáng kể các tham số có thể học được. Cụ thể, chúng tôi nén chiều kênh đặc trưng xuống một nửa ở đầu và tăng lại ở cuối. Hơn nữa, các lớp cross-attention được giới thiệu một cách chiến lược ở các khoảng thời gian đều đặn, cụ thể là mỗi 2 khối cho CosMo-2B, và 4 cho CosMo-3.4B.

Mất mát: Đối với đặc trưng tài liệu x và đặc trưng thị giác z, mô hình dự đoán từ tiếp theo y và mất mát được tính như:
p(y|x) = −∑[i=1 to T] log p(y|x, z), (1)
trong đó T là độ dài của văn bản đầu vào. Hàm mất mát cuối cùng bao gồm cả mất mát mô hình hóa ngôn ngữ (Ll) và mất mát contrastive (Lc):
L = ∑[i=1 to N] λ1Ll + λ2Lc, (2)
trong đó N là số lượng loại dữ liệu (mặc định là 3).

Đến đây, CosMo của chúng tôi được huấn luyện để xử lý thành thạo các chuỗi văn bản và thị giác interleaved, tự nhiên cho phép ứng dụng của nó trong các tình huống học few-shot trong bối cảnh.

4.2. Nâng cao Căn chỉnh Hình ảnh-Văn bản
Bộ Mã hóa Thị giác thường xuất phát từ mô hình CLIP [39], được huấn luyện tỉ mỉ với mất mát contrastive để bảo tồn thông tin phong phú và phân biệt. Preceiver resampler [2] có được các đặc trưng không gian-thời gian từ Bộ Mã hóa Thị giác, tạo ra một số lượng token thị giác cố định. Nhưng nguy cơ bỏ lỡ các chi tiết phân biệt vẫn tồn tại, có thể để lại một số thông tin không rõ ràng.

Ngược lại, cách tiếp cận của chúng tôi mở rộng quá trình này bằng cách kết hợp một truy vấn có thể học được để chú ý toàn cục đến tất cả các token, bao gồm một truy vấn có thể học được bổ sung cho Các Lớp Hợp nhất Văn bản. Sự sửa đổi này cho phép chúng tôi đạt được một hiểu biết toàn diện hơn về toàn bộ tập token. Tiếp theo, mô hình sử dụng một đầu chiếu để thống nhất các embeddings thị giác và văn bản vào cùng một chiều. Mục tiêu huấn luyện tập trung vào tối ưu hóa mất mát contrastive.

4.3. Tiền xử lý Dữ liệu Interleaved
Ma trận Tương đồng Hình ảnh-văn bản. Trong bộ dữ liệu MMC4 [63] interleaved, mỗi tài liệu (thường là một trang web) chứa một danh sách văn bản và một danh sách hình ảnh được trích xuất từ tài liệu. Ngoài ra, bộ dữ liệu này cung cấp tương đồng hình ảnh-văn bản theo cặp được tính bằng mô hình CLIP [39]. Do thiếu vị trí hình ảnh trong tài liệu, trong quá trình huấn luyện trước, Open-Flamingo [3] chọn chỉ số phù hợp sử dụng Optimal Transport [49] từ ma trận tương đồng.

Lọc Dữ liệu. Bộ dữ liệu MMC4 bao gồm nhiều hình ảnh với điểm tương đồng thấp, có thể bao gồm logo, tải xuống hình ảnh thất bại, hoặc hình ảnh hoàn toàn không liên quan đến văn bản đi kèm, như được thể hiện trong Hình 4. Huấn luyện mô hình trực tiếp trên dữ liệu như vậy thường dẫn đến bùng nổ gradient do sự không liên quan cố hữu giữa hình ảnh và văn bản. Để giảm thiểu vấn đề này, MMC4 sử dụng một lọc đơn giản bằng cách sử dụng ngưỡng điểm tương đồng 0.24, được tính bởi CLIP ViT-L/14. Tuy nhiên, cách tiếp cận này loại bỏ một số lượng đáng kể mẫu, giảm tính đa dạng của bộ dữ liệu.

Để vượt qua những hạn chế này, chúng tôi triển khai các chiến lược sau: (i.) Phân bố Tương đồng: Không giống như các phương pháp trước đó, chúng tôi giới thiệu một ma trận nhiễu loạn với phân bố chuẩn có độ lệch chuẩn trong khoảng (-0.04, 0.04) cho ma trận điểm căn chỉnh. Hơn nữa, chúng tôi kẹp giá trị min và max lần lượt thành -0.08 và 0.08. Sau đó chúng tôi sử dụng optimal transport để khớp chỉ số. Cách tiếp cận này cho phép hình ảnh khớp với các văn bản khác nhau trong cùng một tài liệu. (ii.) Bảo tồn Bối cảnh Tài liệu: Đối với các hình ảnh có điểm tương đồng dưới 0.20, chúng tôi tận dụng một mô hình captioning được huấn luyện trước để tạo pseudo-captions thay thế các văn bản được khớp trong tài liệu. Thao tác này đa dạng hóa đáng kể các tài liệu được lấy mẫu và nâng cao tính ổn định huấn luyện. Tham khảo Phần 6.1 để phân tích toàn diện.

4.4. Chi tiết Huấn luyện
Dữ liệu Huấn luyện trước: Các bộ dữ liệu thông thường thường gặp vấn đề về nhiễu và trùng lặp. Theo cách tiếp cận được nêu trong TLDR [50], chúng tôi chọn lấy mẫu một tập con tinh chỉnh từ các bộ dữ liệu huấn luyện trước hình ảnh-văn bản thường được sử dụng. Bảng 3 chi tiết các bộ dữ liệu cụ thể được lấy mẫu cho huấn luyện mô hình của chúng tôi. Phân tích thêm về phương pháp lấy mẫu dữ liệu của chúng tôi được thể hiện trong Phần 5.1.

Cấu hình Huấn luyện: Tất cả các mô hình được huấn luyện bằng bộ tối ưu hóa AdamW [32] với lịch trình tỷ lệ học cosine, được khởi tạo ở 5e-4, và huấn luyện trong 20 epoch. Tỷ lệ khởi động được đặt thành 0.03. Các bước tích lũy gradient căn chỉnh với loại dữ liệu, mặc định là 3 cho CosMo của chúng tôi. Chúng tôi sử dụng độ chính xác fp16 của DeepSpeed [40] để huấn luyện mô hình, được thực hiện trên 128 GPU NVIDIA V100.

5. Thí nghiệm
Mục tiêu chính của chúng tôi là phát triển các mô hình có khả năng thích ứng nhanh chóng với nhiều nhiệm vụ đa dạng và thách thức. Để đạt mục tiêu này, chúng tôi đánh giá mô hình của mình so với nhiều benchmark hình ảnh-văn bản [16, 17, 31, 33, 37, 45] và video-văn bản [9, 26, 30, 52, 57, 62] được thiết lập tốt, để đánh giá hiệu suất học trong bối cảnh few-shot của CosMo.

5.1. Lựa chọn Dữ liệu Huấn luyện trước
Thiết lập thí nghiệm của chúng tôi lấy cảm hứng từ TLDR [50], nơi chúng tôi tận dụng một tập con của các bộ dữ liệu SBU [36], CC3M [44], CC12M [8], LAION400M [43], và Data-comp 1B [42]. Để có được tập con này, chúng tôi bắt đầu bằng cách lọc bỏ một nửa dữ liệu với điểm tương đồng thấp. Tiếp theo, chúng tôi sử dụng phân cụm K-means và lấy mẫu đồng đều dữ liệu từ mỗi cụm, dẫn đến tổng cộng 100 triệu điểm dữ liệu. Chúng tôi tính trọng tâm của mỗi cụm và lấy mẫu đồng đều dữ liệu theo khoảng cách từ trung tâm. Một ví dụ minh họa của quá trình phân cụm này được thể hiện trong Hình 5. Đối với dữ liệu interleaved, chúng tôi sử dụng 30 triệu điểm dữ liệu từ MMC4 [63] sau một quá trình lọc bằng cách loại bỏ các mẫu có hình ảnh quá nhỏ hoặc không có hình ảnh. Cũng như lọc dữ liệu trong Phần 4.3.

5.2. Nghiên cứu Loại bỏ
Trong Bảng 4, chúng tôi báo cáo kết quả loại bỏ của mình bằng cách sử dụng mô hình được huấn luyện trước trên tập con 18M (12M Interleaved, 4M Hình ảnh và 2M Video-văn bản). Điểm trung bình được tính bằng cách lấy trung bình các điểm trên tất cả các bộ dữ liệu.

Tầm quan trọng của Mất mát Contrastive: Như thấy trong phần (i) của Bảng 4, mất mát contrastive đóng vai trò quan trọng trong thành công của cách tiếp cận của chúng tôi. Chúng tôi quan sát thấy việc sử dụng mất mát contrastive trên cả GPU đơn và đa GPU đều cải thiện hiệu suất mô hình một cách nhất quán so với baseline. Tuy nhiên, các điều tra thêm của chúng tôi chỉ ra rằng việc tập hợp tất cả các mất mát contrastive trên các node không mang lại lợi ích tương xứng. Điều này có thể được quy cho sự tập trung mạnh mẽ của mô hình vào mất mát contrastive, có thể ảnh hưởng đến việc học mất mát mô hình hóa ngôn ngữ. Ngoài ra, việc sử dụng mất mát contrastive trên tất cả các GPU giới thiệu độ trễ đáng kể trong tốc độ huấn luyện (từ 4.9s lên 6.1s), làm tăng chi phí tính toán. Kết quả là, mặc định chúng tôi sử dụng mất mát contrastive trên batch trên mỗi GPU.

Tất cả Dữ liệu đều quan trọng: Như được thể hiện trong phần (ii) của Bảng 4, việc lựa chọn dữ liệu huấn luyện phù hợp là quan trọng đối với hiệu suất mô hình. Đáng chú ý, việc loại bỏ bộ dữ liệu hình ảnh-văn bản interleaved dẫn đến sự suy giảm hiệu suất lớn về điểm trung bình (từ 38.0 xuống 20.1), và việc bỏ qua các cặp hình ảnh-văn bản thông thường cũng ảnh hưởng tương tự đến hiệu suất. Ngoài ra, việc loại trừ các bộ dữ liệu video-văn bản được ghép cặp có tác động bất lợi đến phần lớn các nhiệm vụ downstream.

Dựa trên những phát hiện này, chúng tôi sử dụng tích lũy gradient, và số bước tích lũy được đặt để phù hợp với số lượng loại dữ liệu. Điều này đảm bảo rằng mỗi lần lặp bao gồm tất cả các loại dữ liệu, cho phép chúng tôi tận dụng toàn bộ phổ dữ liệu có sẵn để huấn luyện. Chúng tôi cũng trình bày các chiến lược lấy mẫu dataloader khác nhau ở hàng (iii) của Bảng 4. Các phát hiện của chúng tôi chỉ ra rằng chiến lược "tối thiểu" vượt trội hơn cả "tối đa" và "round-robin." Điều này nhấn mạnh tầm quan trọng của việc đảm bảo số lượng cân bằng giữa từng loại dữ liệu huấn luyện.

Kích thước Bộ mã hóa Thị giác: Trong phần (iv) của Bảng 4, chúng tôi đánh giá tác động của việc thay đổi kích thước bộ mã hóa thị giác. Các quan sát của chúng tôi cho thấy một xu hướng chung là các bộ mã hóa thị giác lớn hơn có xu hướng tạo ra kết quả tốt hơn một chút. Tuy nhiên, sự cải thiện này được bù đắp bởi sự gia tăng tương ứng về số lượng tham số mô hình và yêu cầu tính toán. Đáng chú ý, thời gian cần thiết cho mỗi lần lặp tăng từ 1.2 giây lên 2 giây khi sử dụng các bộ mã hóa thị giác lớn hơn. Xem xét sự cân bằng này giữa lợi ích hiệu suất và chi phí tính toán, chúng tôi đã chọn áp dụng kích thước bộ mã hóa thị giác lớn hơn làm cấu hình mặc định.

Làm nhẹ Mô hình: Trong việc theo đuổi một mạng nhẹ và hiệu quả hơn, chúng tôi tập trung vào việc giảm các tham số có thể học được bằng cách giảm thiểu số lượng lớp cross-attention và nén các tham số liên quan của nó. Kết quả được trình bày trong các phần (v) và (vi). Đáng ngạc nhiên, việc giảm số lượng tham số có thể học được nhìn chung không dẫn đến sự giảm hiệu suất. Nó thậm chí còn dẫn đến hiệu suất tăng cường, đặc biệt khi sử dụng khoảng cách lớp là 2 và tỷ lệ nén là 2, mà chúng tôi đưa vào khung cuối cùng của mình.

Loại bỏ Độ dài Interleaved: Trong phần (vii) của Bảng 4, chúng tôi khám phá tác động của việc thay đổi độ dài chuỗi interleaved trong khoảng từ 64 đến 192. Nói chung, các chuỗi dài hơn dẫn đến kết quả tốt hơn. Tuy nhiên, đáng chú ý là các chuỗi đầu vào dài hơn cũng giới thiệu yêu cầu tính toán cao hơn và có thể làm chậm đáng kể quá trình huấn luyện do tiêu thụ bộ nhớ GPU tăng.

Trái ngược với các công trình trước đó [2, 3], sử dụng độ dài chuỗi là 256, chúng tôi chọn độ dài chuỗi là 128 để tăng tốc quá trình huấn luyện, trong ngân sách tính toán hạn chế. Điều quan trọng cần lưu ý là mô hình của chúng tôi có tiềm năng nâng cao thêm với việc sử dụng các chuỗi dài hơn nếu ngân sách cho phép.

5.3. Đánh giá Few-shot trên Các Nhiệm vụ Thị giác-Ngôn ngữ
Kết quả trên Các Nhiệm vụ Hình ảnh-Văn bản. Bảng 5 trình bày một phân tích so sánh với các công trình liên quan, thể hiện hiệu suất tiên tiến của chúng tôi. CosMo của chúng tôi vượt trội hơn Open-Flamingo [3] trên gần như tất cả các benchmark, và điều này đạt được với kích thước mẫu giảm đáng kể (130M so với 180M) và ít tham số hơn. Khi sử dụng cùng mô hình RedPajama-3B [47] làm mô hình ngôn ngữ, mô hình của chúng tôi cũng vượt trội hơn Open-Flamingo đáng kể. Hơn nữa, việc kết hợp bộ dữ liệu Howto-Interlink7M được đề xuất của chúng tôi dẫn đến kết quả còn tốt hơn, nhấn mạnh sức mạnh của dữ liệu chất lượng cao. Đáng chú ý là, trong quá trình huấn luyện trước, mô hình của chúng tôi được huấn luyện chỉ với ba khung hình, nhưng vẫn mang lại kết quả mạnh mẽ ngay cả khi được đánh giá với 32 shot.

6. Chi tiết Huấn luyện
Kết quả trên Các Nhiệm vụ Video-Văn bản. Chúng tôi kiểm tra mô hình của mình trên các nhiệm vụ captioning video và trả lời câu hỏi video. Khi sử dụng một bộ dữ liệu video bổ sung, chúng tôi liên tục quan sát hiệu suất mô hình tăng cường trên các khía cạnh khác nhau. Tuy nhiên, điều quan trọng cần lưu ý là một hạn chế đáng kể trong các bộ dữ liệu MSVD và MSRVTT hiện tại: chất lượng ground truth của chúng bị tổn hại do được tạo qua các phương pháp dựa trên quy tắc thay vì chú thích của con người. Do đó, có những trường hợp mô hình của chúng tôi tạo ra các dự đoán chi tiết và chính xác hơn, có thể không phù hợp với ground truth. Ví dụ, trong khi ground truth là "Carve", mô hình của chúng tôi dự đoán "Carving sculpture". Để giải quyết thách thức này, chúng tôi đề xuất một cách tiếp cận thay thế để đánh giá hiệu suất VQA. Điều này bao gồm việc đánh giá độ tương tự văn bản bằng cách sử dụng một mô hình Ngôn ngữ được huấn luyện trước từ NLTK [5]. Kết quả đánh giá của chúng tôi được trình bày trong ngoặc đơn của Bảng 6. Các phát hiện của chúng tôi chỉ ra rằng, trên phần lớn các nhiệm vụ downstream được đánh giá, mô hình của chúng tôi liên tục mang lại hiệu suất mạnh mẽ.

6.1. Phân tích về Dữ liệu Interleaved
Trực quan hóa Điểm Tương đồng Interleaved. Chúng tôi trực quan hóa phân bố của các điểm tương đồng hình ảnh-văn bản. Phần lớn các điểm tương đồng nằm trong khoảng từ 0.2 đến 0.4. Bằng cách thay đổi ngưỡng từ 0.18 đến 0.30 trong khi duy trì một tập con bộ dữ liệu 18 triệu, phù hợp với thiết lập loại bỏ của chúng tôi, chúng tôi trình bày kết quả trong Bảng 7.

Khi ngưỡng tương đồng dưới 0.22, việc xảy ra thường xuyên các vụ nổ gradient cản trở việc huấn luyện thành công mô hình của chúng tôi. Vấn đề này chủ yếu phát sinh từ ảnh hưởng phá hoại của dữ liệu nhiễu, làm suy giảm đáng kể tính ổn định huấn luyện. Hơn nữa, các ngưỡng lớn (ví dụ, 0.28 và 0.30) mang lại kết quả không tối ưu trên cả COCO và FLICKR30K, một phần do việc loại bỏ đáng kể các mẫu interleaved. Để giải quyết điều này, chúng tôi thử nghiệm với việc thay thế các chú thích có độ tương tự thấp bằng những chú thích được tạo từ một CosMo được huấn luyện trước, dẫn đến huấn luyện ổn định hơn. Chúng tôi đã lưu ý rằng trong khi việc thay thế các chú thích nhiễu đóng góp tích cực vào việc nâng cao hiệu suất, việc sửa chữa các cặp chính xác đáng chú ý dẫn đến sự giảm đáng kể.

Phân tích về Lấy mẫu Dữ liệu Interleaved. Các phát hiện thực nghiệm của chúng tôi nhấn mạnh rằng một số lượng shot lớn hơn cải thiện đáng kể kết quả trong các thí nghiệm được thực hiện trên các khung hình interleaved. Nguồn gốc chủ yếu của hầu hết các mẫu nằm trong các hình ảnh thô. Để giải quyết sự thiên vị tiềm năng và bảo tồn số lượng khung hình tối đa, chúng tôi thực hiện một cách tiếp cận chiến lược: thay thế chú thích bằng dữ liệu được huấn luyện trước cho các hình ảnh thể hiện độ tương tự cực kỳ thấp, chiếm khoảng 5% bộ dữ liệu. Việc điều chỉnh thủ tục này mang lại một sự nâng cao krên biên nhưng có thể nhận biết trong hiệu suất tổng thể của mô hình.

Phân tích các Tập con Interleaved: Trong thí nghiệm này, chúng tôi xem xét chặt chẽ ba tập con duy nhất được lấy từ bộ dữ liệu MMC4:
• Một tập con 4 triệu được lấy mẫu ngẫu nhiên.
• 4 triệu mẫu có số lượng hình ảnh cao nhất.
• 4 triệu mẫu chỉ chứa một khung hình duy nhất.

Để giảm thiểu vấn đề tiềm năng của việc lấy mẫu quá mức các tài liệu với nhiều khung hình, chúng tôi giới hạn việc huấn luyện mô hình thành một epoch duy nhất. Kết quả chi tiết, như được trình bày trong Bảng 8, cho thấy một khoảng cách hiệu suất đáng kể, đặc biệt ủng hộ tập con được làm giàu với nhiều khung hình nhất. Sự chênh lệch đáng chú ý này cho thấy rằng hiệu quả trong học few-shot phần lớn xuất phát từ sự tích hợp của dữ liệu interleaved kết hợp với việc sử dụng chiến lược các LLMs.

6.2. Các Nhiệm vụ Căn chỉnh Zero-shot
Trong thí nghiệm này, chúng tôi bao gồm nhiệm vụ phân loại hình ảnh và truy xuất zero-shot. Chúng tôi sử dụng pipeline đánh giá DataComp [42] để kiểm tra khả năng của mô hình. Cụ thể, hiệu suất của mô hình được đánh giá trên 38 bộ dữ liệu mà không cần bất kỳ huấn luyện nào. Kết quả được thể hiện trong Bảng 9.

7. Kết luận và Hạn chế
Trong công trình này, chúng tôi trình bày một kiến trúc tinh chỉnh nhằm kết hợp mất mát contrastive vào một mô hình đa phương thức tự hồi quy hiện có, CosMo, được thiết kế riêng cho việc học trong bối cảnh trên nhiều phương thức. Nhưng học trong bối cảnh yêu cầu dữ liệu interleaved chất lượng cao và vẫn chưa có bộ dữ liệu video-văn bản interleaved nào có sẵn. Để giải quyết khoảng trống này, chúng tôi giới thiệu Howto-Interlink7M, một bộ dữ liệu video-văn bản interleaved tiên phong có các chú thích toàn diện, đánh dấu một bước tiến đáng kể trong việc tiến bộ của lĩnh vực này.

Hơn nữa, các lợi thế tiềm năng của việc sử dụng các mô hình huấn luyện trước trong nhiều nhiệm vụ downstream hơn, đặc biệt là trong các nhiệm vụ văn bản dài, đáng được khám phá thêm. Nỗ lực đang diễn ra của chúng tôi bao gồm việc phát hành sắp tới các mô hình được huấn luyện và bộ dữ liệu của chúng tôi, nhằm thúc đẩy nghiên cứu rộng rãi trong lĩnh vực này.

Tài liệu tham khảo
[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv preprint arXiv:2201.07520, 2022. 2
[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–23736, 2022. 1, 2, 3, 4, 7, 8
[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 1, 2, 3, 5, 7
[4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728–1738, 2021. 5, 12
[5] Steven Bird, Ewan Klein, and Edward Loper. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc.", 2009. 8, 12
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 1
[7] Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062, 2023. 1
[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558–3568, 2021. 5
[9] David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 190–200, 2011. 5
[10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104–120. Springer, 2020. 2
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2
[12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022. 1
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3
[14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1
[15] Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. Magma–multimodal augmentation of generative models through adapter-based finetuning. arXiv preprint arXiv:2112.05253, 2021. 4
[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904–6913, 2017. 5
[17] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3608–3617, 2018. 5
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 4
[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 13, 14
[20] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 3
[21] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 3, 13
[22] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 3, 12, 13
[23] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583–5594. PMLR, 2021. 2
[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:32–73, 2017. 2
[25] Hugo Laurencon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 2
[26] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. Tvr: A large-scale dataset for video-subtitle moment retrieval. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16, pages 447–463. Springer, 2020. 5
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694–9705, 2021. 3
[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 2, 3, 14
[29] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX 16, pages 121–137. Springer, 2020. 2
[30] Yuncheng Li, Yale Song, Liangliang Cao, Joel Tetreault, Larry Goldberg, Alejandro Jaimes, and Jiebo Luo. Tgif: A new dataset and benchmark on animated gif description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4641–4650, 2016. 5, 12
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 2, 5
[32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5
[33] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195–3204, 2019. 5
[34] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2630–2640, 2019. 2, 3
[35] OpenAI. Gpt-4 technical report. 2023. 1, 2, 3
[36] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems, 24, 2011. 5, 12, 14
[37] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641–2649, 2015. 5
[38] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-specific video summarization. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 540–555. Springer, 2014. 3
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 1, 2, 3, 4, 5
[40] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020. 5
[41] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for movie description. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3202–3212, 2015. 12
[42] Alex Fang Samir Yitzhak Gadre, Gabriel Ilharco. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. 2, 5, 9, 12, 14
[43] Christoph Schuhmann, Richard Vencu, Romain Beaumont, and Kaczmarczyk. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 2, 5, 12, 14
[44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556–2565, 2018. 2, 5, 12, 14
[45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317–8326, 2019. 5
[46] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. 4
[47] Together.xyz. Releasing 3b and 7b redpajama incite family of models including base, instruction-tuned and chat models. https://www.together.xyz/blog/redpajama-models-v1, 2023. 1, 3, 7, 13
[48] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200–212, 2021. 2
[49] Cédric Villani. Topics in optimal transportation. American Mathematical Soc., 2021. 5
[50] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang, Stan Weixian Lei, and Mike Zheng Shou. Too large; data reduction for vision-language pre-training. ICCV, 2023. 5
[51] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 1, 14
[52] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4581–4591, 2019. 5
[53] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al. Internvid: A large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 2
[54] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv preprint arXiv:2212.00280, 2022. 3
[55] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. 14
[56] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. 1
[57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288–5296, 2016. 5
[58] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vidchapters-7m: Video chapters at scale. arXiv preprint arXiv:2309.13952, 2023. 2
[59] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 2, 3, 4
[60] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34:23634–23651, 2021. 2
[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 1, 4
[62] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 5
[63] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023. 2, 4, 5, 12

--- TRANG 12 ---
Phụ lục
A. Mở rộng quy mô Mô hình Ngôn ngữ
A.1. Dữ liệu và So sánh
Nỗ lực của chúng tôi trong giai đoạn thí nghiệm này nhằm mục đích mở rộng quy mô đáng kể mô hình ngôn ngữ của chúng tôi, chuyển từ các cấu hình 2B và 3.4B được sử dụng trước đó sang một thiết lập 8B mở rộng hơn.

Để đảm bảo một so sánh công bằng và nhất quán giữa các mô hình, chúng tôi tăng cường kích thước mẫu dữ liệu trong tập con DataComp từ 130M lên 180M, như được nêu trong bộ dữ liệu DataComp [42]. Các thống kê dữ liệu liên quan, được trình bày toàn diện trong Bảng 10, thể hiện một sự tăng cường đáng kể, cụ thể là tăng 60M trong số lượng mẫu trong tập con Datacomp [42].

Các nỗ lực của chúng tôi không chỉ giới hạn ở việc tăng cường dữ liệu; chúng tôi cũng kết hợp một mô hình ngôn ngữ Mistral-7B [22] lớn hơn để đạt được tổng số tham số 8.1B. Việc so sánh giữa phương pháp của chúng tôi và Open-flamingo, sử dụng dữ liệu huấn luyện trước quy mô tương đương, liên tục thể hiện hiệu suất vượt trội của cách tiếp cận của chúng tôi trên các nhiệm vụ đa dạng.

B. Chi tiết Phương pháp Huấn luyện
B.1. Siêu tham số
Trong phần này, chúng tôi mô tả các chi tiết huấn luyện quan trọng cần thiết cho việc sao chép. Thí nghiệm bao gồm ba biến thể về kích thước mô hình. Đáng chú ý, các mô hình lớn hơn cần kích thước batch nhỏ hơn do hạn chế bộ nhớ GPU. Cách tiếp cận của chúng tôi sử dụng deepspeed zero-stage 2 với fp16, trong khi căn chỉnh các bước tích lũy gradient với số lượng loại dữ liệu. Kết quả chi tiết trong Bảng 12.

B.2. Xử lý Ngoại lệ
Với việc sử dụng ít nhất 8 node (64 GPU) trong thiết lập huấn luyện của chúng tôi, một số thách thức nảy sinh trong quá trình huấn luyện đa node. Chúng tôi đã gặp và giải quyết các vấn đề khác nhau:

Bỏ qua Batch Cuối cùng: Lấy mẫu ngẫu nhiên thỉnh thoảng dẫn đến các batch không ổn định, dẫn đến mất mát contrastive cao đáng kể và bất ổn định mô hình ngôn ngữ.

Cận trên Mất mát: Để chống lại những trường hợp như vậy, chúng tôi triển khai một cơ chế giá trị moment để theo dõi mất mát contrastive và mất mát mô hình hóa ngôn ngữ. Nếu mất mát của batch vượt quá giá trị moment một ngưỡng nhất định, chúng tôi thu nhỏ mất mát, thúc đẩy huấn luyện ổn định hơn.

Giảm thiểu Lỗi NAN: Định kỳ, mô hình gặp lỗi NAN mà không có khả năng phục hồi. Để quản lý những trường hợp như vậy, một cơ chế khởi động lại công việc tự động sử dụng NCCL được sử dụng, đảm bảo tính liên tục trong huấn luyện mặc dù có những điểm thất bại tiềm năng.

C. Nhiệm vụ Trắc nghiệm Video
Trong thí nghiệm này, chúng tôi giới thiệu một tập hợp nhiệm vụ video bổ sung. Không giống như trả lời câu hỏi video mở, nhiệm vụ trắc nghiệm liên quan đến việc lựa chọn một câu trả lời từ một nhóm các ứng viên.

Chính xác, đánh giá của chúng tôi bao gồm TGIF-MC [30] và LSMDC-MC [41] như các bộ dữ liệu benchmark. Để đánh giá các nhiệm vụ trắc nghiệm này mà không cần fine-tuning, chúng tôi đề xuất một cách tiếp cận khớp dựa trên độ tương tự. Ban đầu, chúng tôi tạo ra các câu trả lời tiềm năng và sau đó đo độ tương tự tối đa với tất cả các ứng viên. Ứng viên tương tự nhất, được xác định bằng cách sử dụng một Mô hình Ngôn ngữ từ NLTK [5], được lựa chọn. Nếu chỉ số khớp với chỉ số chính xác, chúng tôi đánh dấu nó là một câu trả lời đúng. Các phát hiện của chúng tôi chỉ ra rằng mô hình của chúng tôi thể hiện hiệu suất tốt hơn với kích thước mô hình lớn hơn trong các nhiệm vụ này.

D. Kết quả Mở rộng: Chi tiết Multi-shot
Trong những thí nghiệm này, chúng tôi trình bày hiệu suất của CosMo trên các số lượng shot khác nhau, từ 0 đến 32 shot. Đối với các nghiên cứu loại bỏ này, huấn luyện của chúng tôi sử dụng 64 token interleaved trong một tập con 18M. Đối với độ chính xác Xác thực, chúng tôi báo cáo độ chính xác top1 và top5.

D.1. Loại bỏ Trọng số Dữ liệu
Thí nghiệm này đi sâu vào phân bố trọng số dữ liệu giữa Hình ảnh-văn bản, Hình ảnh-văn bản Interleaved, và Video-Văn bản. Trước khi tính tất cả các mất mát với tích lũy gradient, chúng tôi áp dụng các trọng số loại dữ liệu tương ứng. Bảng 13 minh họa rằng các cặp hình ảnh-văn bản interleaved ảnh hưởng đáng kể đến độ chính xác downstream. Được thúc đẩy bởi những quan sát này, trọng số dữ liệu cho dữ liệu interleaved là 2 theo mặc định.

D.2. Phương pháp PEFT so với Cross Attention
Các phương pháp fine-tuning hiệu quả tham số như LORA [19] phổ biến trong LLM. Câu hỏi ở đây là liệu có khả thi để giới thiệu LORA cho fine-tuning mô hình hay không.

Để thực hiện điều này, chúng tôi giới thiệu LORA vào bộ giải mã văn bản. Cụ thể, chúng tôi giữ cross attention gốc nhưng mang thêm LORA. Chúng tôi thể hiện kết quả trong Bảng 14. Chúng tôi thấy rằng phương pháp PEFT mang lại lợi ích tiêu cực cho các phương pháp dựa trên lớp cross-attention đã giới thiệu một số lượng lớn tham số có thể huấn luyện.

D.3. Loại bỏ Tỷ lệ Học
Thí nghiệm này liên quan đến việc thay đổi tỷ lệ học từ 3e-4 đến 3e-6, và các kết quả được báo cáo trong Bảng 15 cho thấy một phát hiện quan trọng. Các tỷ lệ học cực kỳ nhỏ làm suy giảm đáng kể hiệu suất nhiệm vụ downstream. Ví dụ, một sự giảm từ 40.7 xuống 21.9 độ chính xác trên 8 shot được quan sát.

Theo mặc định, chúng tôi sử dụng 5e-4 cho CosMo-2B và CosMo-3.4B. Đối với CosMo-8.1B, chúng tôi sử dụng tỷ lệ học 3e-4.

D.4. Lịch trình Tỷ lệ Học
Thí nghiệm với bốn lịch trình tỷ lệ học khác nhau—Cosine, Constant, Cosine-w-restart, và Inverse Sqrt—cho thấy những biến động đáng kể trong kết quả cuối cùng, như được thể hiện trong Bảng 16. Nói chung, lịch trình Cosine mang lại kết quả tốt nhất trong hầu hết các trường hợp, do đó được áp dụng làm bộ lập lịch mặc định do hiệu suất nhất quán của nó.

E. Khác
E.1. Chiến lược Lựa chọn Dữ liệu cho Hình ảnh-Văn bản
Các bộ dữ liệu CC3M [44], SBU [36], LAION400M [43], và DataComp1B [42] có sẵn rộng rãi và thường được sử dụng làm dữ liệu huấn luyện trước. Chúng tôi bao gồm các bộ dữ liệu này để tạo điều kiện sao chép dễ dàng. Tuy nhiên, các quan sát của chúng tôi cho thấy rằng trong khi các bộ dữ liệu hình ảnh-văn bản này có giá trị, tác động của chúng đến hiệu suất downstream không đáng kể bằng các bộ dữ liệu hình ảnh-văn bản interlevel.

Để minh họa, việc sử dụng toàn bộ 130M dữ liệu chỉ từ DataComp [42] dẫn đến những thay đổi tối thiểu trong hiệu suất. Hơn nữa, không giống như các phương pháp trước đó như GiT [51] và BLip2 [28], chúng tôi chọn loại trừ các bộ dữ liệu COCO và Visual Genome trong quá trình huấn luyện trước do khả năng chồng chéo với dữ liệu downstream. Việc loại trừ chiến lược này được thực hiện để giảm thiểu sự dư thừa tiềm năng và tối đa hóa tính đặc biệt của các biểu diễn được học.

E.2. Khám phá Chuỗi Dài hơn với Ngân hàng Bộ nhớ
Quản lý dữ liệu đầu vào văn bản dài có tầm quan trọng đáng kể trong NLP. Một cách tiếp cận gần đây, Memorizing Transformer [55], giới thiệu một ngân hàng bộ nhớ để lưu trữ các token phân đoạn từ toàn bộ tài liệu. Kỹ thuật này cho phép mô hình xử lý các văn bản dài, vượt qua ngay cả 10K token.

Theo các nguyên tắc của Memorizing Transformer, mục tiêu của chúng tôi là tăng cường độ dài dữ liệu interleaved. Kết quả, được chi tiết trong Bảng 17, mang lại những quan sát thú vị:

i. Kết quả Xác thực Tốt hơn: Đáng chú ý, độ chính xác xác thực thể hiện sự nâng cao đáng kể, ví dụ, từ 63.012 lên 60.85. ii. Chi phí Tính toán Tăng: Việc giới thiệu các chuỗi dài hơn phát sinh yêu cầu tính toán cao hơn đáng kể. iii. Lợi ích Hiệu suất Downstream Hạn chế: Đáng ngạc nhiên, các lợi ích hiệu suất trên các nhiệm vụ downstream tương đối hạn chế và đôi khi thậm chí còn tiêu cực.

Do đó, dựa trên những phát hiện này, cách tiếp cận Memorizing Transformer không được bao gồm trong cấu hình mô hình cuối cùng của chúng tôi. Tuy nhiên, điều này không loại trừ việc xem xét các phương pháp thay thế để tăng cường khả năng xử lý văn bản dài hơn của mô hình. Có tiềm năng khám phá các cách tiếp cận khác nhau nhằm nâng cao khả năng xử lý các chuỗi mở rộng của mô hình.

F. Tiến trình Huấn luyện
Trong phần này, chúng tôi khám phá các đường cong huấn luyện cho ba quy mô của CosMo. Các phát hiện của chúng tôi cho thấy rằng các Mô hình Ngôn ngữ Lớn (LLMs) lớn hơn thể hiện mất mát Mô hình Ngôn ngữ (LM) giảm, đặc biệt là trong việc xử lý dữ liệu hình ảnh-văn bản interlevel, nhấn mạnh sự thành thạo của chúng trong các nhiệm vụ dự đoán từ.

Tuy nhiên, một khía cạnh đáng chú ý là sự hội tụ chậm hơn trong mất mát contrastive cho CosMo so với các mô hình lớn hơn như CosMo-2B và CosMo-3.4B, được quy cho kích thước batch nhỏ hơn của nó. Ngoài ra, các hạn chế phần cứng rõ ràng trong quá trình huấn luyện CosMo-8.1B trên GPU Tesla V100 32GB, nơi kích thước batch tối đa là 1536 đã tác động đến tốc độ hội tụ của mất mát contrastive.

Những quan sát này làm nổi bật sự cân bằng phức tạp giữa kích thước mô hình, kích thước batch, và các hạn chế phần cứng trong việc huấn luyện hiệu quả các LLMs.

--- TRANG 16 ---
Hình 7. Nói chung, các Mô hình Ngôn ngữ Lớn hơn thể hiện mất mát Mô hình hóa Ngôn ngữ (LM) thấp hơn. Tuy nhiên, sự hội tụ của mất mát contrastive có xu hướng chậm hơn do kích thước batch nhỏ hơn.
