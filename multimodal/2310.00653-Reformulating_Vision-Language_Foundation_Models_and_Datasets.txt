# 2310.00653.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.00653.pdf
# File size: 1930303 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Reformulating Vision-Language Foundation Models and Datasets
Towards Universal Multimodal Assistants
Tianyu Yu*1Jinyi Hu*1Yuan Yao1†Haoye Zhang1Yue Zhao2
Chongyi Wang3Shan Wang3Yinxu Pan4Jiao Xue3Dahai Li3
Zhiyuan Liu1†Hai-Tao Zheng1†Maosong Sun1†
1Tsinghua University2Beijing University of Posts and Telecommunications
3Zhihu Inc.4ModelBest Inc.
yiranytianyu@gmail.com
Abstract
Recent Multimodal Large Language Models (MLLMs) ex-
hibit impressive abilities to perceive images and follow open-
ended instructions. The capabilities of MLLMs depend on
two crucial factors: the model architecture to facilitate the
feature alignment of visual modules and large language mod-
els; the multimodal instruction tuning datasets for human in-
struction following. (i) For the model architecture , most ex-
isting models introduce an external bridge module to connect
vision encoders with language models, which needs an ad-
ditional feature-alignment pre-training. In this work, we dis-
cover that compact pre-trained vision language models can
inherently serve as “out-of-the-box” bridges between vision
and language. Based on this, we propose Muffin framework,
which directly employs pre-trained vision-language models
to act as providers of visual signals. (ii) For the multimodal
instruction tuning datasets , existing methods omit the com-
plementary relationship between different datasets and sim-
ply mix datasets from different tasks. Instead, we propose
UniMM-Chat dataset which explores the complementarities
of datasets to generate 1.1M high-quality and diverse mul-
timodal instructions. We merge information describing the
same image from diverse datasets and transforms it into more
knowledge-intensive conversation data. Experimental results
demonstrate the effectiveness of the Muffin framework and
UniMM-Chat dataset. Muffin achieves state-of-the-art per-
formance on a wide range of vision-language tasks, signif-
icantly surpassing state-of-the-art models like LLaV A and
InstructBLIP. Our model and dataset are all accessible at
https://github.com/thunlp/muffin.
1 Introduction
Building a general model capable of tackling diverse tasks
across multiple modalities has remained a longstanding goal
within the realm of Artificial Intelligence. Recently, pow-
erful Multimodal Large Language Models (MLLMs) have
emerged as one of the most promising ways to achieve this
goal, such as MiniGPT-4 (Zhu et al. 2023), LLaV A (Liu
et al. 2023), and InstructBLIP (Dai et al. 2023). These mod-
els empower large language models (LLMs) with impressive
multimodal instruction-following capabilities by equipping
LLMs with vision encoders to perceive visual content.
*These authors contributed equally.
†Corresponding authors.
OKVQA
AOKVQA
GQA
VQAv2
Visual Question
AnsweringVisual ChatConversationComplex
ReasoningDetail
Description
61.865.569.2
61.064.067.0
61.865.569.2
64.5
69.0
73.564.9
67.8
70.768.0
74.0
80.060.8
68.5
76.279.085.091.055.563.070.5
Muffin (This Work)
LLaVA
InstructBLIP
Ziya-Visual
mPLUG-owlFigure 1: Muffin achieves state-of-the-art performances on
various tasks compared with strong MLLMs. Visual Ques-
tion Answering: the average score over four visual question
answering datasets. Visual Chat: the average score over the
conversation task, the complex reasoning task and the detail
description generation task.
Despite existing capabilities, several crucial factors of de-
veloping MLLM are still under-explored. In this work, we
focus on two key challenges of building MLLMs: (i) effec-
tiveness of model architectures to achieve feature alignment;
(ii) construction of multimodal instruction tuning dataset.
For the model architecture, existing MLLMs can roughly
be summarized as two streams: (1) A linear projector is op-
timized to align the frozen visual encoder with the frozen
LLM, such as LLaV A (Liu et al. 2023) and PaLM-E (Driess
et al. 2023); (2) A visual feature re-sampler (Alayrac et al.
2022; Li et al. 2023; Dai et al. 2023) is optimized to com-
press the output of the visual encoder into a fixed-length fea-
ture sequence and align the these features with LLMs. How-
ever, merely using a linear projector restrains the model’s
capacity to learn new knowledge and the feature sequence
length is quadratically related to the resolution of the input
image, leading to a significant computational burden. On the
other side, introducing a visual feature re-sampler requires a
resource-consuming additional training process to primarilyarXiv:2310.00653v1  [cs.CV]  1 Oct 2023

--- PAGE 2 ---
achieve the alignment of modalities before connecting the
visual encoder with LLMs (Li et al. 2023).
To address the aforementioned limitations, we propose
Muffin1, an efficient architecture to build powerful MLLMs.
Intuitively, we notice that compact pre-trained vision-
language models (VLMs), such as ALBEF (Li et al. 2021),
CoCa (Yu et al. 2022), and BEiT-3 (Wang et al. 2023a),
have already exhibited remarkable performance in vision-
language (V-L) tasks through pre-training on extensive mul-
timodal datasets. As a result, these VLMs inherently achieve
the alignment of modalities and are potentially competent
as “out-of-the-box” bridge modules to empower LLMs with
visual capabilities. Based on this intuition, Muffin directly
leverages pre-trained VLMs and learns a set of query vec-
tors in the embedding space of VLMs to perceive the visual
representation for LLMs. In this way, we are able to directly
optimize the visual module to connect with LLMs without
losing capacity or undergoing the additional alignment pro-
cess. Experimental results show that Muffin can achieve the
state-of-the-art performance among existing MLLMs.
In terms of the construction of multimodal instruction tun-
ing datasets, most recent works (Gong et al. 2023; Dai et al.
2023) simply formulate the downstream vision-language
dataset into a unified format, while the short and limited for-
mat of responses in these datasets will harm the generative
abilities of LLMs. Another line of works (Zhu et al. 2023;
Liu et al. 2023) converts isolated datasets into conversation
corpora based on ChatGPT or GPT-4 (OpenAI 2023). How-
ever, they neglect the compementarities of different datasets
which is crucial to form a comprehensive view of the im-
age content and consequently lead to knowledge scarcity in
generated data.
To overcome such shortcomings, we design a simple and
effective approach to reformulate multiple datasets into chat
corpora with a flexible format in responses. Therefore, de-
spite the lack of information in one annotation, multiple
annotations for the same image can be complementarily
merged to form a more comprehensive description of the
image. Specifically, we use images from COCO (Lin et al.
2014) to construct the dataset. Based on the combined anno-
tations, we require ChatGPT to generate high-quality chat
corpora that are accurate and knowledge-intensive. Follow-
ing this process, we construct UniMM-Chat, a high-quality
multimodal instruction tuning dataset containing over 1.1M
instructions. We conduct a series of experiments to demon-
strate the effectiveness of our data construction pipeline and
the resultant UniMM-Chat dataset. Besides, we construct
the UniMM-Bench benchmark to evaluate MLLMs’ abili-
ties in reasoning and world knowledge. Specifically we col-
lect questions from existing VL benchmarks and leverage
GPT-4 to score the model output.
In general, we summarize our contribution as follows:
• We propose a novel architecture, Muffin, which reformu-
lates pre-trained VLMs as bridges between vision mod-
ules and LLMs. Muffin achieves state-of-the-art perfor-
mance among existing baselines on a wide range of tasks.
1Multimodal f oundation models are f ound to be “out-of-the-
box” multimodal in terfaces for LLMs.ModelVisual
EncoderBridge
ModuleLLM
BLIP-2 ViT Q-Former Flan-T5
MiniGPT-4 ViT Q-Former Vicuna-13B
VisualGLM ViT Q-Former ChatGLM
Ziya-Visual ViT Q-Former Ziya-LLaMA-13B
mPLUG-owl ViT Q-Former LLaMA-7B
InstructBLIP ViT Q-Former Vicuna-13B
LLA V A ViT Linear Vicuna-13B
Muffin BEiT-3 Vicuna-13B
Table 1: Summary of the structure of existing MLLMs.
• We construct a knowledge-intensive multimodal in-
struction tuning dataset, UniMM-Chat, which is con-
structed by requiring ChatGPT to generate dialogues
given merged information from different datasets.
• We construct the UniMM-Bench benchmark to evaluate
the overall capability of MLLMs involving diverse tasks
and evaluate Muffin and other MLLM models on it.
• We open-source Muffin, UniMM-Chat, and UniMM-
Bench to the community.
2 Related Work
Vision Language Models The research of pretrained VLMs
has been a hot topic for years. These models are pre-trained
on a large scale of image-text pairs to achieve the alignment
between visual and text modalities. Some work focuses on
improving the training objectives, such as contrastive loss
(Radford et al. 2021; Jia et al. 2021), masked data model-
ing (Wang et al. 2023a), and image-text matching (Li et al.
2021). Some work devotes to optimizing the model architec-
ture, such as UNITER (Chen et al. 2020) and VinVL (Zhang
et al. 2021), and the recent unified transformer architecture
VLMo (Bao et al. 2022). Based on these techniques, sev-
eral large-scale VLMs are proposed, such as Florence (Yuan
et al. 2021) and BEiT-3 (Wang et al. 2023a). These models
exhibit good performance on VL tasks while lacking capa-
bilities in following human instructions.
Multimodal Large Language Models MLLMs aim to
bridge a visual module with pre-trained LLMs for multi-
modal interaction. The pioneering work, BLIP-2 (Li et al.
2023), introduces the Q-Former architecture, a shallow
transformer to align the visual feature from the frozen vi-
sual encoder with LLMs. Subsequent works largely adopt
the Q-Former architecture, such as MiniGPT-4 (Zhu et al.
2023), VisualGLM (Du et al. 2022), Ziya-Visual (Wang
et al. 2022). LLaV A (Liu et al. 2023) employs a linear layer
to map the visual feature from the frozen vision encoder
into the embedding space of pre-trained LLM (Chiang et al.
2023). mPLUG-Owl (Ye et al. 2023) leverages a modified
Q-Former module to align the vision encoder CLIP with
LLM using both text-only and multimodal instruction tun-
ing datasets. InstructBLIP (Dai et al. 2023) improves the Q-
Former, obtaining the instruction-aware visual features by
inputting the instruction into the Q-Former as well. Table 1
summarizes the detailed structure of these models.

--- PAGE 3 ---
Multimodal Instruction Tuning Datasets To equip
MLLMs with potent instruction-following capabilities, sev-
eral multimodal instruction tuning datasets are proposed.
MiniGPT-4 (Zhu et al. 2023) proposes using ChatGPT to
rewrite the image description and collect nearly 3.5K in-
struction instances. InstructBLIP (Dai et al. 2023) formu-
late 26 publicly available datasets of different tasks with
handcrafted templates for each dataset. LLaV A (Liu et al.
2023) proposes to leverage GPT-4 to write instructions for
three different categories, including detail description, com-
plex reasoning, and conversation, given annotations of im-
ages from the COCO dataset (Lin et al. 2014). Though the
generated data of LLaV A is more diverse compared with
MiniGPT-4, the instructions are still scarce in knowledge
since annotations from only one dataset can hardly give a
comprehensive understanding of images.
3 Muffin Framework
3.1 Architecture
The architecture of our proposed Muffin is shown in Fig-
ure 2. Instead of training a separate module to connect the
vision encoder and LLMs, Muffin directly utilizes a pre-
trained VLM model, denoted as G, to summarize the visual
representation for LLMs, denoted as F. Commonly, VLMs
consist of a visual channel and a text channel, which are
deeply fused with each other to achieve modality alignment.
By extensive pre-training in large-scale V-L datasets, VLMs
inherently excel in serving as the “out-of-the-box” bridge for
LLM. In this work, we leverage BEiT-3 (Wang et al. 2023a)
as VLM backbone, which is pre-trained with masked data
modeling and achieves good performance on many vision
and vision-language tasks.
To leverage VLM for visual features extraction, Muf-
fin introduces a sequence of trainable query vectors in
the text embedding space of VLM, denoted as Q=
[q1,q2,···,qn],qn∈Rd, where nis the number of train-
able query vectors, dis the hidden size of VLM G. These
query vectors Qand the image Xvare input into the text
and vision channels, respectively. Within each block of
the Transformer, to deeply fuse two modalities, the hidden
states from each channel will perform both self-attention and
cross-attention with each other.
After deep fusion between the trainable query vectors Q
and the image, the final output in the last layer correspond-
ing to the query vectors’ position effectively captures a vi-
sual feature of the input image. This progression can be suc-
cinctly expressed as an end-to-end formulation:
Zv=G(Xv,Qϑ). (1)
Subsequently, we apply a fully connected projection layer
to transform the perceived visual feature Zvinto the embed-
ding space of the pre-trained LLM Hv=Wξ·Zv, which
will serve as the prefix context for the LLM and concate-
nate with the text embedding of Xtas the final input to be
forwarded to the LLM.
3.2 Pre-training
Following the most existing MLLMs (Liu et al. 2023; Dai
et al. 2023), we first conduct pre-training on an extensive
Projection
ImagePatchesQueriesResponse
Text Instruction. . .Pre-trained LLM
Textual ChannelVisual ChannelVLM. . .. . .
Fuse. . .
Figure 2: Architecture of our proposed Muffin. A sequence
of trainable query vectors and the image patches are input
into the textual and visual channels of VLM, respectively.
By deeply fusing in the VLM blocks, the output from the
textual channel serves as the summarized image feature.
number of image-text pairs to align the VLM Gand the
LLM F. Since the text data used in this stage share rel-
atively simple formats, we freeze the parameters of LLM
during pre-training to retain the powerful knowledge and
complex reasoning ability of LLM. For a pair of images and
text(Xv, Xt), we randomly select an instruction Xinsfrom
a pre-defined set, as used in Liu et al. (2023). This instruc-
tion, such as “Describe the image briefly.”, serves to pre-
fix the caption and narrow the gap in training data format
between the current stage and following instruction tuning.
The training objective is to maximize the probability of tar-
get response given the instruction Xinsand input image Xv,
shown as:
L=kX
i=1logp(xi|Hv, Xins, Xt,<i) (2)
3.3 Multimodal Instruction Tuning
While naive multimodal pre-trained models exhibit the ca-
pacity to comprehend the content of input images and gen-
erate concise captions, they often lack the ability to execute
intricate tasks based on human instructions. As a result, we
proceed to undertake further multimodal instruction tuning.
Unlike the previous stage, we make the LLM trainable
during the instruction tuning process to harness the full po-
tential of high-quality instructional data. We structure each
data instance in the form of a conversational snippet follow-
ing Vicuna (Chiang et al. 2023) and train the model to de-
code tokens of the answer spans. We utilize the same train-
ing objective, as represented by Equation (2), used in the
pre-training stage.
4 UniMM-Chat
To construct high-quality and diverse instruction tuning
datasets with comprehensive image descriptions, we pro-

--- PAGE 4 ---
ChatGPT
Human: Can you tell me what season it is in this picture? AI: It appears to be springin this picture. The presence of cherry blossoms in bloomsuggests that it's a season characterized by growth, vitality, and renewal. Human: Can you tell me what reminds you of spring in the picture? AI: In the left part of the picture, there are cherry blossom trees, which are traditionally associated with the spring season. The cherry blossom trees are characterized by their pink flowers,which add a touch of vibrancy and liveliness to the overall scenery. Their presence suggests the image was taken during spring or when the cherry blossom bloom season is in full swing in the particular area. GeneratedConversationRound5Round10···VisualQuestionAnsweringVQAv2Visual DialogOKVQAA-OKVQAQ: What season is it in this picture? A: springQ: based on the leaves, what season do you think it is A: springbecause the trees are in bloomQ: What season is this? A: springQ: What type of trees are on the left?A: cherry blossomsRationales:The trees have cherry blossoms actively blooming on them.The flowers are pink which is characteristic of the blooms of this type of tree.CaptionCOCO CaptionA scene of a park with a plane in the sky in the background.A plane lanes in the distance, behind cherry blossomsA large passenger jet flying over a park filled with cherry trees.Figure 3: Demonstration of the framework designed for constructing the UniMM-Chat dataset. UniMM-Chat combines various
VL datasets to generate knowledge-intensive dialogues. Text spans highlighted in colors indicate different knowledge from
origin annotations which are required to answer the questions.
pose the UniMM-Chat dataset, which consists of 1.1M di-
verse instructions. We incorporate complementary annota-
tions from different VL datasets and employ ChatGPT to
generate multi-turn dialogues corresponding to each image.
As shown in Figure 3, the incorporated annotations furnish
a richer image context and effectively empower ChatGPT to
generate more knowledge-intensive conversational datasets.
4.1 Dataset Construction
Five commonly utilized VL datasets, as outlined in Table 2,
serve as seeds to craft multimodal instructions. As images in
these five datasets are drawn from COCO (Lin et al. 2014),
we first aggregate the annotations for each image from the
seed VL datasets. For VQAv2 (Goyal et al. 2017), OKVQA
(Marino et al. 2019), and Visual Dialog (Das et al. 2017), we
use the annotation of both questions and their corresponding
answers. For AOKVQA (Schwenk et al. 2022), we use the
question-answer pair and the annotated rationales. Five cap-
tions from COCO (Lin et al. 2014) are directly employed as
fundamental descriptions for each image.
Next, these annotations are meticulously structured into
a refined format, incorporating some additional human-
written few-shot learning instances. These elements are col-
lectively presented as prompts, prompting ChatGPT to gen-
erate multi-turn dialogues centered on the respective images.
We refer readers to the Appendix for the prompts we used
during data construction.
4.2 Dataset Statistics
In total, we collect 117,238 dialogues, with an average of
9.89 turns per dialogue. Each dialogue is associated with
one distinct image. To quantify the dataset’s diversity, we
follow (Wang et al. 2023b) and parse the question types and
their direct nouns or verb with Berkeley Neural Parser tool
(Stern, Andreas, and Klein 2017). We plot the seven most
is therepeoplecolormanimagepersonwhatcolortypekindtimedoescan youseedescribetellestimateconfirmhowmanyoldisimagephotodoesmanbigwouldmuchisdoeswhyFigure 4: Instruction distribution in UniMM-Chat.
common question types and their top direct noun objects or
verbs, which account for 44% instructions in UniMM-Chat
in Figure 4. This plot underscores the considerable breadth
of intents and formats in UniMM-Chat.
4.3 UniMM-Bench
We propose UniMM-Bench, a question-answering bench-
mark designed for MLLMs to evaluates the abilities involv-
ing reasoning and world knowledge. As traditional exact-
matched accuracy is not suitable for evaluating MLLMs,
which often respond a complete sentence to answer ques-
tions, we leverage GPT-4 to score the generated answer.
Considering the evaluation cost, we sample one hundred
samples from the test set of OKVQA (Marino et al. 2019),
AOKVQA (Schwenk et al. 2022), GQA (Hudson and Man-

--- PAGE 5 ---
VL dataset #Images #Annotation
VQAv2 (Goyal et al. 2017) 123,287 658,111
OKVQA (Marino et al. 2019) 14,031 14,055
AOKVQA (Schwenk et al. 2022) 17,662 18,201
Visual Dialogue (Das et al. 2017) 123,287 1,232,870
COCO Caption (Lin et al. 2014) 123,287 616,767
Table 2: Statistics of source vision-language datasets to con-
struct UniMM-Chat.
ning 2019) and VQAv2 (Goyal et al. 2017), respectively,
whose annotations has undergone meticulous inspection.
This benchmark evaluates the capabilities of MLLMs in
reasoning, commonsense, and world knowledge (Schwenk
et al. 2022).
5 Experiments
5.1 Experimental Settings
Evaluation Details . We evaluate MLLMs on our proposed
UniMM-Bench and LLaV A test set(Liu et al. 2023). The
LLaV A test set consists of 90 questions from three cate-
gories spanning conversation, complex reasoning, and de-
tail description. UniMM-Bench mainly evaluates the model
abilities in reasoning and world knowledge, while the
LLaV A test set evaluates the model performance on mul-
timodal conversational. We leverage GPT-4 to score the
model output based on the ground truth answers. We em-
pirically verified the scores of GPT-4 are well aligned with
human judgment. We refer readers to the Appendix for com-
plete prompts.
Training Details . The pre-training of Muffin is performed
with 180M image-text pairs collected from Visual Genome
(Krishna et al. 2017), COCO (Lin et al. 2014), CC3M
(Sharma et al. 2018), CC12M (Changpinyo et al. 2021)
and LAION-COCO (Christoph Schuhmann 2022) and lasts
for 100K steps with batch size of 2048 and learning rate
of 1e-4. For instruction tuning, we use both the LLaV A-
Instruct-150K and UniMM-Chat instruction tuning dataset.
The training lasts for 3200 steps with batch size of 512 and
learning rate of 2e-5. We adopt the resolution of 448 during
pre-training and 672 during the instruction tuning stage.
Baselines . We compare our method with a series of existing
strong baselines:
•MiniGPT-4 : MiniGPT-4 (Zhu et al. 2023) is one of the
earliest open-source trials of MLLMs, which is fine-
tuned on over 3.5K simple instructions that require model
to generate image descriptions.
•VisualGLM : VisualGLM (Du et al. 2022) is a bilingual
multimodal assistant model built upon ChatGLM-6B and
the vision encoder of BLIP2 which devises complex fea-
ture alignment training process.
•Ziya-Visual : (Wang et al. 2022) is a bilingual multi-
modal assistant model based on Ziya-LLaMA-13B and
pre-trained visual encoder of BLIP-2.
•mPLUG-owl : mPLUG-owl (Ye et al. 2023) is a mul-
timodal assistant model based on CLIP ViT-L/14 andLLaMA-7B, which use both text-only and multimodal
instruction tuning datasets.
•InstructBLIP : InstructBLIP (Dai et al. 2023) constructs
a multimodal instruction tuning dataset based on 26 pub-
lic datasets by apply pre-defined templates to directly for-
mulate these datasets into a unified format. They devise
a novel instruction-aware Q-Former and train the model
on the proposed dataset.
•LLaV A : LLaV A (Liu et al. 2023) constructs 150K multi-
modal instructions based on the COCO dataset. It simply
leverages a linear projector to connect the vision encoder
and LLM.
5.2 Main Results
Table 3 presents the performance of Muffin and baselines
on UniMM-Bench and LLaV A test set. On both of these
two benchmarks, Muffin achieves the state-of-the-art perfor-
mance and significantly surpasses all baselines.
Based on these experimental results, we have the follow-
ing observations:
• Compared with LLaV A, Muffin achieves an impressive
5.1-point advancement on average. Also, even when em-
ploying the same LLaV A-Instruct-150K dataset for in-
struction tuning, akin to LLaV A’s training, Muffin still
achieves better results with 1.2-point, demonstrating the
effectiveness of the Muffin framework.
• Compared with InstructBLIP, Muffin exhibits substantial
performance enhancements over InstructBLIP. Specifi-
cally, despite directly training on OKVQA, AOKVQA,
and VQAv2, InstructBLIP achieves lower performances
on these datasets compared with Muffin, especially on
OKVQA and AOKVQA, which contains limited annota-
tions. This indicates simply combining training samples
of different datasets is sub-optimal for the model to learn
a wide range of knowledge. On the LLaV A test set, we
hypothesize the limited format of responses in the train-
ing data harms the generation ability and consequently
results in InstructBLIP significantly lag behind Muffin.
• Excluding UniMM-Chat from the training set leads to a
substantial performance drop across all visual question-
answering tasks. This emphasizes the pivotal role of
UniMM-Chat in equipping MLLMs with the skills to ef-
fectively address a variety of task types.
These results collectively demonstrate the effectiveness of
the Muffin framework and highlight the crucial role of
UniMM-Chat. Enriched by UniMM-Chat, Muffin exhibits
strong reasoning capabilities and abundant knowledge.
5.3 Human Evaluation
For a more comprehensive analysis, we conduct a pair-wise
human evaluation of different models on a diverse range
of instructions. Specifically, we randomly sample eighty
samples from UniMM-Bench and twenty samples from the
LLaV A test set. We recruit six well-educated annotators and
present them with the answer pairs generated by Muffin and
other three baselines, LLaV A, InstructBLIP, and MiniGPT-
4. We assure the model names are hidden during the whole

--- PAGE 6 ---
ModelUniMM-Bench LLaV A Test SetALL
OKVQA AOKVQA GQA VQAv2 A VG Con CR DD A VG
VisualGLM (Du et al. 2022) 56.2 56.6 59.2 63.9 59.0 65.8 80.6 64.5 70.3 60.6
MiniGPT-4 (Zhu et al. 2023) 59.2 56.0 66.0 58.9 60.0 65.3 75.6 66.3 69.1 61.8
mPLUG-owl (Ye et al. 2023) 63.9 63.3 63.3 64.1 63.6 69.0 84.1 59.0 70.8 65.1
Ziya-Visual (Wang et al. 2022) 67.3 65.6 68.2 67.0 67.0 82.3 90.2 71.2 81.3 69.9
LLaV A (Liu et al. 2023) 68.8 64.2 68.1 67.7 67.2 83.0 96.5 75.0 84.9 70.7
InstructBLIP (Dai et al. 2023) 67.0 64.9 67.6 75.5 68.8 82.2 90.2 68.4 80.7 71.1
Muffin (w/o UniMM-Chat) 69.1 68.4 70.1 66.8 68.6 82.2 96.0 77.5 85.3 71.9
Muffin 72.8 69.8 72.9 77.9 73.4 83.5 96.5 77.2 85.7 75.8
Table 3: Performance of our proposed Muffin and baselines on UniMM-Bench and LLaV A Test Set. A VG: average of scores.
Con: conversation category. CR: complex reasoning category. DD: detail description category. We report the average perfor-
mance of three trials for each model to improve the stability.
LVA
IBP
MG4
All0.00.20.40.60.81.0Win Ratio
LVA
IBP
MG4
HelpfulnessWin Tie LossLVA
IBP
MG4
Correctness
LVA
IBP
MG4
Consistency
Figure 5: Human evaluation winning ratio of Muffin com-
pared with different baseline models. LV A: LLaV A. IBP: In-
structBLIP. MG4: MiniGPT-4.
evaluation process. Annotators are required to decide which
answer is better in each pair, based on three criteria: Helpful-
ness, Correctness, and question-answer Consistency. More
details are introduced in the Appendix. The evaluation re-
sults are shown in Figure 5. Muffin outperforms all other
models on all metrics. The evident advantage of Helpfulness
and Correctness originates from training on the knowledge-
intensive dialogues from the UniMM-Chat dataset. Specif-
ically, we find the MiniGPT-4 (Zhu et al. 2023), which is
trained with only a few thousand simple instruction samples,
usually responds that is not related to the question and con-
sequently obtains the lowest consistency win ratio. We refer
readers to the Appendix for the detail statistics of human
evaluation results comparing Muffin with baseline models.
5.4 Ablation Results
Reformulating VL datasets . To verify the effectiveness
of the dataset construction framework, we build a variant
of UniMM-Chat without merging annotations across dif-Instruction Data UniMM-Bench LLaV A Test Set
Origin Datasets 72.5 36.6
UniMM-Chat-sep 70.7 65.6
UniMM-Chat 72.6 78.6
Table 4: Muffin trained with different instruction tuning
dataset settings. UniMM-Chat-sep is a variant of UniMM-
Chat that is constructed without merging annotations from
different VL datasets.
Model UniMM-Bench LLaV A Test Set
Muffin 73.4 85.7
- pre-training resolution 70.1 85.5
- instruction tuning resolution 72.1 85.3
- tune LLM 64.0 68.4
Table 5: Ablation results for different training settings.
ferent datasets, named UniMM-Chat-sep. More details of
UniMM-Chat-sep are presented in Appendix. We train mul-
tiple models using different data configurations and present
the result in Table 4. On the LLaV A test set, using original
datasets fails to establish a chat model due to the short text
in these datasets. Incorporating UniMM-Chat-sep, while re-
sulting in improved performance compared to using the orig-
inal datasets, still yields suboptimal results owing to the lim-
ited information available during construction. On UniMM-
Bench, directly using the original datasets corresponds to in-
domain fine-tuning, serving as the performance upper bound
for the constructed dataset. The experimental results show
that using UniMM-Chat does not lead to the in-domain per-
formance drop, while using UniMM-Chat-sep will undergo
a performance decline. These results emphasize the neces-
sity of our dataset construction framework.
Training Settings. We also analyze the effect of some train-
ing settings and present the results in Table 5. As for the
image resolution, when decreasing the input image reso-
lution during the pre-training (224) and instruction tuning

--- PAGE 7 ---
The advertisement on the bus is for what type of medium?
The advertisement on the bus is for a movie, specifically the movie "Planet 51." The advertisement is visible on the side of the bus and features an image from the movie. This medium allows for the promotion of the movie to a large audience, as buses tend to travel through various areas and are seen by multiple people.
Muffin (This work)
LLaVAThe advertisement on the bus is for a video game.
InstructBLIPFilm or movie. Yes, the advertisement on the bus is for a film or movie.What airline are the planes from?
The planes in the image are from Korean Air. This can be inferred from the color scheme and markings on the planes, which are characteristic of the Korean Air fleet.
Muffin (This work)
LLaVAThe planes in the image belong to China Airlines. They are parked at the terminal on the runway of the airport.It is not specified which airline the planes belong to.
InstructBLIP
How can you determine the time from this image? Explain it in detail.
In the image, a sundial is used to determine the time. The sundial is placed on a table and consists of a cardboard or paper plate with a stick or pin sticking out of it. The stick or pin, also known as a gnomon, is designed to cast a shadow when exposed to sunlight.The shadow's position on the paper plate or cardboard corresponds to the hour of the day. By reading the shadow's position, one can estimate the local apparent solar time, which can be converted to standard time if necessary. Sundials are simple, cost-effective, and eco-friendly tools for telling time outdoors, especially during daylight saving time when accurate timekeeping can be challenging for some devices.In the image, there is a small clock, which is placed on top of a white cardboard with a black marker. The clock is visible through the round hole of the cardboard. By examining the position of the clock's hands, one can determine the time. Moreover, the presence of the clockon the cardboard indicates that it might be an art project or a creative display of time-telling, so it could have a symbolic or decorative significance rather than being a conventional clock setup.
Muffin (This work)
LLaVAIn the image, there is a sundialon top of a cardboard box. The sundial consists of a stick with a small circle at the end, which is pointing towards the sky. There are several rocks placed around the sundial, and one rock is positioned in the center of the sundial's circle. This setup suggests that the sundial is designed to determine the time based on the position of the sun in the sky. By observing the position of the shadow cast by the sundial's stick, you can determine the time of day. As the sun moves across the sky during the day, the shadow will move accordingly, providing an accurate reading of the time. The presence of rocks around the sundial may indicate that the device is intended to be used in an outdoor setting, such as agarden or park.
InstructBLIP
The couple is wearing traditional Indian attire. The bride is likely wearing a lehenga choli, a popular Indian bridal dress, while the groom is dressed in a sherwani, a formal suit-like garment. These outfits are often adorned with intricate embroidery, beads, and other decorative elements, reflecting the rich cultural heritageofIndia.
Muffin (This work)
The man and woman in the image are wearing traditional Indian attire, which signifies that they might be a part of a culturalevent or celebration, such as a wedding or a religious ceremony. The woman is wearing a veil, and the man has a turban, both of which are common elements in Indian weddings and religious ceremonies.
LLaVAtraditional indianwedding attire
InstructBLIPWhat are they wearing?
What is this vegetable?
The vegetable in the image is an onion. Onions are a common ingredient in many dishes, adding flavor and texture to various recipes.
Muffin (This work)
InstructBLIP
If the onion is being cooked in the pan, its color might change due to the cooking process. Onions typically turn brown as they are cooked, which can occur due to the Maillard reaction -a chemical reaction between amino acids and reducing sugars that results in the formation of a variety of aroma and flavor compounds, giving the onion a richer, sweeter taste.
Muffin (This work)The color of the onion will not change significantly in the next 30 minutes, as it is already sliced and cooked in the pan.
InstructBLIP
How might the color of the onion change in the following 30 minutes?
oniononsFigure 6: Examples generated by Muffin and other baselines.

--- PAGE 8 ---
stage (448), the performance has a distinct drop on UniMM-
Bench, since detailed image information necessary to solve
complex multimodal tasks is hard to be retrained with low
resolution. Besides, we also observed freezing the LLM dur-
ing the instruction tuning process can limit the model’s abil-
ities on all evaluated datasets.
5.5 Qualitative Results
Benefiting from the deep fusion within the VLM and knowl-
edge intensive instruction data from UniMM-Chat, Muf-
fin can effectively activate the knowledge embedded in the
LLM and generate more helpful response to open-ended
questions. Figure 6 shows some examples of responses from
Muffin, LLaV A, and InstructBLIP for demonstration. In the
first example, our model accurately identifies the country of
origin of the airplane based on visual details, while both
LLaV A and InstructBLIP fail to provide the correct an-
swer, highlighting our model’s superiority in comprehend-
ing and identifying image details. In the second example,
our model combines textual cues from the image with its
inherent knowledge to generate a more accurate response.
Moreover, in the fourth example, Muffin correctly identifies
the exact name of the traditional Indian attire shown in the
image. Except having the ability to answer questions with
a broad range of knowledge, Muffin can also generate more
helpful responses. In the last example show in 6, though both
pointing the vegetable is onion, Muffin gives more detail and
helpful response.
6 Conclusion
In this paper, we present Muffin, an innovative framework
to directly utilize pre-trained VLMs to bridge visual sig-
nals and LLMs. Also, we develop a new paradigm to build
a multimodal instruction tuning dataset by merging annota-
tions from different datasets describing the same images. In
this way, we construct a high-quality and diverse multimodal
instruction tuning dataset, UniMM-Chat. We perform com-
prehensive experiments to demonstrate the effectiveness of
Muffin and UniMM-Chat, which shows that Muffin achieves
state-of-the-art performance on a wide range of tasks. In the
future, we will apply Muffin framework and UniMM-Chat
dataset to more combinations of VLMs and LLMs.
References
Alayrac, J.-B.; Donahue, J.; Luc, P.; Miech, A.; Barr, I.; Has-
son, Y .; Lenc, K.; Mensch, A.; Millican, K.; Reynolds, M.;
et al. 2022. Flamingo: a Visual Language Model for Few-
Shot Learning. Proceedings of NeurIPS .
Bao, H.; Wang, W.; Dong, L.; Liu, Q.; Mohammed, O. K.;
Aggarwal, K.; Som, S.; Piao, S.; and Wei, F. 2022. VLMo:
Unified Vision-Language Pre-Training with Mixture-of-
Modality-Experts. In Proceedings of NeurIPS .
Changpinyo, S.; Sharma, P.; Ding, N.; and Soricut, R.
2021. Conceptual 12M: Pushing Web-Scale Image-Text
Pre-Training To Recognize Long-Tail Visual Concepts. In
CVPR .Chen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan,
Z.; Cheng, Y .; and Liu, J. 2020. Uniter: Universal Image-
Text Representation Learning. In Proceedings of ECCV .
Springer.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Christoph Schuhmann, R. V . T. C. R. B., Andreas K ¨opf.
2022. LAION-COCO: 600m Synthetic Captions from
LAION2b-en.
Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;
Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards
General-purpose Vision-Language Models with Instruction
Tuning.
Das, A.; Kottur, S.; Gupta, K.; Singh, A.; Yadav, D.; Moura,
J. M.; Parikh, D.; and Batra, D. 2017. Visual Dialog. In
Proceedings of CVPR .
Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery,
A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.;
et al. 2023. PaLM-E: An embodied multimodal language
model. arXiv preprint arXiv:2303.03378 .
Du, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
Tang, J. 2022. GLM: General Language Model Pretraining
with Autoregressive Blank Infilling. In Proceedings of ACL .
Gong, T.; Lyu, C.; Zhang, S.; Wang, Y .; Zheng, M.; Zhao,
Q.; Liu, K.; Zhang, W.; Luo, P.; and Chen, K. 2023.
MultiModal-GPT: A Vision and Language Model for Dia-
logue with Humans. arXiv preprint arXiv:2305.04790 .
Goyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and
Parikh, D. 2017. Making the V in VQA Matter: Elevating
the Role of Image Understanding in Visual Question An-
swering. In Proceedings of CVPR .
Hudson, D. A.; and Manning, C. D. 2019. GQA: A New
Dataset for Real-World Visual Reasoning and Composi-
tional Question Answering. In Proceedings of CVPR .
Jia, C.; Yang, Y .; Xia, Y .; Chen, Y .-T.; Parekh, Z.; Pham, H.;
Le, Q.; Sung, Y .-H.; Li, Z.; and Duerig, T. 2021. Scaling up
Visual and Vision-Language Representation Learning with
Noisy Text Supervision. In Proceedings of ICML .
Krishna, R.; Zhu, Y .; Groth, O.; Johnson, J.; Hata, K.;
Kravitz, J.; Chen, S.; Kalantidis, Y .; Li, L.-J.; Shamma,
D. A.; et al. 2017. Visual Genome: Connecting Language
and Vision Using Crowdsourced Dense Image Annotations.
IJCV .
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. BLIP-2: Boot-
strapping Language-Image Pre-training with Frozen Image
Encoders and Large Language Models. arXiv preprint
arXiv:2301.12597 .
Li, J.; Selvaraju, R.; Gotmare, A.; Joty, S.; Xiong, C.; and
Hoi, S. C. H. 2021. Align Before Fuse: Vision and Language
Representation Learning with Momentum Distillation. In
Proceedings of NeurIPS .
Lin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft
COCO: Common Objects in Context. In Proceedings of
ECCV .

--- PAGE 9 ---
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual instruc-
tion tuning. arXiv preprint arXiv:2304.08485 .
Marino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R.
2019. OK-VQA: A Visual Question Answering Benchmark
Requiring External Knowledge. In Proceedings of CVPR .
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from natural
language supervision. In Proceedings of ICML .
Schwenk, D.; Khandelwal, A.; Clark, C.; Marino, K.; and
Mottaghi, R. 2022. A-OKVQA: A Benchmark for Visual
Question Answering using World Knowledge. In Proceed-
ings of ECCV .
Sharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.
Conceptual Captions: A Cleaned, Hypernymed, Image Alt-
Text Dataset for Automatic Image Captioning. In Proceed-
ings of ACL .
Stern, M.; Andreas, J.; and Klein, D. 2017. A Minimal Span-
Based Neural Constituency Parser. In Proceedings of ACL .
Wang, J.; Zhang, Y .; Zhang, L.; Yang, P.; Gao, X.; Wu, Z.;
Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y .; Li, X.; Wu,
Y .; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.;
Wang, H.; Wu, X.; Zeng, Z.; Chen, C.; Gan, R.; and Zhang,
J. 2022. Fengshenbang 1.0: Being the Foundation of Chi-
nese Cognitive Intelligence. In Proceedings of CoRR .
Wang, W.; Bao, H.; Dong, L.; Bjorck, J.; Peng, Z.; Liu, Q.;
Aggarwal, K.; Mohammed, O. K.; Singhal, S.; Som, S.; et al.
2023a. Image as a Foreign Language: BEiT Pretraining
for Vision and Vision-Language Tasks. In Proceedings of
CVPR .
Wang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi, D.; and Hajishirzi, H. 2023b. Self-Instruct: Align-
ing Language Models with Self-Generated Instructions. In
Proceedings of ACL .
Ye, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;
Hu, A.; Shi, P.; Shi, Y .; Jiang, C.; Li, C.; Xu, Y .; Chen, H.;
Tian, J.; Qi, Q.; Zhang, J.; and Huang, F. 2023. mPLUG-
Owl: Modularization Empowers Large Language Models
with Multimodality. arXiv:2304.14178.
Yu, J.; Wang, Z.; Vasudevan, V .; Yeung, L.; Seyedhos-
seini, M.; and Wu, Y . 2022. CoCa: Contrastive Caption-
ers are Image-Text Foundation Models. arXiv preprint
arXiv:2205.01917 .
Yuan, L.; Chen, D.; Chen, Y .-L.; Codella, N.; Dai, X.; Gao,
J.; Hu, H.; Huang, X.; Li, B.; Li, C.; et al. 2021. Florence: A
New Foundation Model for Computer Vision. arXiv preprint
arXiv:2111.11432 .
Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.;
Choi, Y .; and Gao, J. 2021. VinVL: Revisiting Visual Rep-
resentations in Vision-Language Models. In Proceedings of
CVPR .
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.
2023. MiniGPT-4: Enhancing Vision-Language Under-
standing with Advanced Large Language Models. arXiv
preprint arXiv:2304.10592 .
0 20 40 60 80
Length02468Percent %Answer Length
Question LengthFigure 7: Length distribution of the generated questions and
answers in UniMM-Chat.
A Prompts
In this section, we list details of all prompts we use in this
work for reproducibility, including the prompts to construct
UniMM-Chat, the prompt to evaluate for UniMM-Bench
and the prompt used to pre-train Muffin.
A.1 UniMM-Chat Construction Prompts
We show the full prompt we used to require ChatGPT
to generate high quality knowledge-intensive dialogues for
UniMM-Chat in Table 7. We present the raw prompt and
how we organize few human annotated demonstrations to-
gether with the raw prompt. We also list the prompt to
amass origin annotations from different VL datasets in Ta-
ble 6, which generate the input to be used in Table 7. We
also present the question and answer length distribution of
UniMM-Chat in Figure 7 for reference.
[Image statements]
{VQAv2 qas}
{OKVQA qas}
{AOKVQA qas}
{VisualDialog qas}
[Image information]
{AOKVQA raionales }
[ Image description ]
{COCO captions }
[Conversation]
Table 6: The template to amass annotations from different
VL datasets.
A.2 UniMM-Bench Evaluation Prompt
We list the full prompt we used to evaluate the performance
of models on UniMM-Bench in Table 8. To enable GPT-
4 generate more accurate scores, we put both the ground
truth answer and other related annotations into the prompt.
Specifically, we list all human answers and corresponding

--- PAGE 10 ---
Prompt:
messages = [ “role”: “system”, “content”: f“You are an AI visual assistant, and you are seeing a single image. What you see are
provided with [Image statements], as well as the [Image information] and [Image description] in several sentences, describing the
same image you are looking at. You should pretend not seeing [Image statements], [Image information], [Image description], etc.
Instead, ask and answer all questions as you are only seeing the image.
Design a 10 rounds conversation between you and a person asking about this photo. The answers should be in a tone that a visual
AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers.
Include questions asking about the visual content of the image, including the object types, object actions, object locations, relative
positions between objects, etc. Only include questions that have definite answers:
(1) one can see the content in the image that the question asks about and can answer confidently.
(2) one can determine confidently from the image that it is not in the image.
Do not ask any question that cannot be answered confidently. Do not add unprovided details to answer the questions.
Also include complex questions that are relevant to the content in the image, for example, asking about background knowledge of
the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details.
You should do your best to fully cover the image content in the conversation. Try to ask questions using special interrogative
sentences.
Provide detailed answers when answering questions if necessary. For example, give detailed examples or reasoning steps to make
the content more convincing and well-organized. You can include multiple paragraphs if necessary. Double-check to ensure your
answer is correct and consistent with the image and the previous conversation. Stay within the scope of the image, refraining
from introducing any information not present in the visual content. Again, do not add unsupported details to answer the questions.
Answer all questions as you are only seeing the image.” ]
for sample in fewshot samples:
messages.append( {‘role’:‘user’, ‘content’:sample[‘input’] })
messages.append( {‘role’:‘assistant’, ‘content’:sample[‘output’] })
messages.append( {‘role’:‘user’, ‘content’:query })
Table 7: The detail prompt used to guide ChatGPT to generate conversations.
confidences from VQAv2 (Goyal et al. 2017) and rationales
from AOKVQA (Schwenk et al. 2022) into the prompt.
A.3 Pre-training Prompts
We adopt the pre-training prompts used in (Liu et al. 2023)
for the training of Muffin. The full list of prompts to generate
caption are listed in Table 9.
B UniMM-Chat-sep
We construct another version of UniMM-Chat for exper-
imental usage, namely UniMM-Chat-sep, which is con-
structed without merging annotations from different VL
datasets. Specifically, we reuse the same prompt (see Table
7) of UniMM-Chat to construct dialogues for each datasets
except COCO Caption by removing annotations from other
datasets. As for COCO Caption, since caption text is already
flexible in format, we simply adopt the instructions listed in
Table 9 following our pre-training setting. As shown in in
Table 4, we find the combination of different VL datasets can
yield better performance on both visual question answering
and visual chat tasks. We argue this is because simply merg-
ing different datasets during training is not enough to fully
explore the complementary nature of different datasets and
thus give sub-optimal results.C UniMM-Bench
In order to evaluate the overall capability of MLLMs involv-
ing diverse tasks, we construct the UniMM-Bench bench-
mark. We randomly add 100 samples from each of the val-
idation sets of OKVQA (Marino et al. 2019), GQA (Hud-
son and Manning 2019), AOKVQA (Schwenk et al. 2022)
into UniMM-Bench. As for VQAv2 (Goyal et al. 2017), we
first randomly select two sample for each type of question
to keep the diversity of our benchmark, which results in 130
samples. Then we randomly choose 100 samples out of these
130 samples. Finally we combined all these 400 samples to
form the UniMM-Bench benchmark.
To prevent the evaluation results being affected by data
leak, we explicitly removed images in UniMM-Bench out
of the image set we used to construct UniMM-Chat.
We list some examples from UniMM-bench and the re-
sults generated by Muffin and other baselines in Figure 8.
These examples show that UniMM-bench can evaluate the
model abilities in both reasoning and world knowledge.
D Human Evaluation
We list the detail statistics of human evaluation results in
Table 10 for reference.

--- PAGE 11 ---
How many green plants are on the right side of the room?
There are two green plants visible on the right side of the room. They appear to be placed on a surface near the wall, adding a touch of natural elements to the space.
Muffin (This work)
LLaVAThere are threegreen plants on the right side of the room.1
InstructBLIP
What material are the white cups?
The white cups in the image are made of paper. The use of paper cups suggests that this is a casual event where disposable tableware is used to make cleanup quick and easy.
Muffin (This work)
LLaVAThe white cups are made of paper.Plastic.
InstructBLIP
Is the box behind a skateboard?
No,there is no skateboard present in the image. The picture depicts a row of bicycles locked to a pole on a city street.
Muffin (This work)
LLaVAYes, the box is behind a skateboard. It is attached to a pole on the street.No, there is no boxbehind the skateboard.
InstructBLIP
What type of fuel does this vehicle use?
The vehicle in the image appears to be a dump truck, and it is likely that it uses diesel fuel to operate. Diesel fuel is commonly used in larger heavy-duty vehicles such as dump trucks because it provides more power and torque than gasoline.The vehicle in the image is a dump truck, which is a large, heavy-duty vehicle designed for transporting loose materials, such as sand, gravel, or dirt, for construction or other purposes. Dump trucks typically feature an open-box bed that allows the material to be unloaded easily through tipping the bed up. In the image, the dump truck is driving near cones, which could suggest a job site or an area that requires some form of construction, maintenance, or repair work.
Muffin (This work)
LLaVADiesel
InstructBLIP
Who invented this vehicle?
The vehicle in this image is a plane, and planes were invented by the Wright brothers. Orville and Wilbur Wright were credited with inventing and flying the first successful airplane in 1903.The vehicle in the image is an older single-engine propeller airplane, invented and developed by a team of experts at UW Madison WAIV Lab. It is designed for general aviation purposes and is parked on the airport tarmac, possibly during an airshow or a special event. The airplane has a distinctive white and red striped body, and it is displayed alongside other aircraft, showcasing the history and evolution of aviation technology. The presence of the cones suggests that the aircraft is on display for visitors to explore and admire. So, in a way, the university and the lab are proud to present their contribution to aviation history with this innovative aircraft model.
Muffin (This work)
LLaVA
orvillewright
InstructBLIP
Figure 8: Examples of questions from UniMM-Bench and results generated by Muffin and other models.

--- PAGE 12 ---
[Question]
{question }
[Assistant Response]
{response }
[End of Assistant Response]
[System]
We would like to request your feedback to evaluate the
performance of an AI assistant in the response to an user
question displayed above. The AI assistant is asked to
look the image and answer the question. You need to give
an overall score to the assistant’s response to the question
on a scale of 1 to 5, where a higher score indicates bet-
ter overall performance. Please first output a single line
containing only one value indicating the score for the as-
sistant.
In the subsequent line, please provide a comprehensive
explanation of your evaluation.
We will give you some additional information about the
image and question for reference in the following (such as
the expected answer, human answers and hints given by
annotators). Note that the assistant can only see the image
content and question text, all other reference information
are used to help you better understand the question and
content of the image only. The major criteria is the cor-
rectness of the answer, you don’t have to care about the
conciseness or structure or other irrelevant factors of the
answer.
[Expected Answer]
{ground truth answer }
[Human Answers]
{human answers and rationales from origin datasets }
Table 8: GPT-4 evaluation prompt used to evaluate UniMM-
Bench.
Caption-generation Instruction
Describe the image concisely.
Provide a brief description of the given image.
Offer a succinct explanation of the picture presented.
Summarize the visual content of the image.
Give a short and clear explanation of the subsequent image.
Share a concise interpretation of the image provided.
Present a compact description of the photo’s key features.
Relay a brief, clear account of the picture shown.
Render a clear and concise summary of the photo.
Write a terse but informative summary of the picture.
Create a compact narrative representing the image presented.
Table 9: Instructions used to generate captions during the
multimodal pre-training stage.Muffin vs. Helpfulness Correctness Consistency
Mini-GPT4 112/ 26 76/ 24 62/ 6
LLaV A 76/ 47 61/ 26 28/ 22
Instruct BLIP 82/ 31 52/ 32 28/ 18
Table 10: Number of samples marked as win/loss on differ-
ent metrics comparing Muffin and baseline models.
