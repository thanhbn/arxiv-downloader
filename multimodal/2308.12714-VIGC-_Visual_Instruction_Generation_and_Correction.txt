# 2308.12714.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2308.12714.pdf
# File size: 11998639 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
VIGC: Visual Instruction Generation and Correction
Bin Wang‚àó1, Fan Wu‚àó1, Xiao Han‚àó1, Jiahui Peng‚àó1, Huaping Zhong‚àó2,
Pan Zhang1, Xiaoyi Dong1,3, Weijia Li4, Wei Li1, Jiaqi Wang1, Conghui He‚Ä†1
1Shanghai AI Laboratory,
2SenseTime Research,3The Chinese University of Hong Kong,4Sun Yat-sen University
{wangbin,wufan,hanxiao,pengjiahui,zhangpan,dongxiaoyi,liwei,wangjiaqi,heconghui }@pjlab.org.cn
zhonghuaping@sensetime.com, liweij29@mail.sysu.edu.cn
Abstract
The integration of visual encoders and large language mod-
els (LLMs) has driven recent progress in multimodal large
language models (MLLMs). However, the scarcity of high-
quality instruction-tuning data for vision-language tasks re-
mains a challenge. The current leading paradigm, such as
LLaV A, relies on language-only GPT-4 to generate data,
which requires pre-annotated image captions and detection
bounding boxes, suffering from understanding image details.
A practical solution to this problem would be to utilize the
available multimodal large language models to generate in-
struction data for vision-language tasks. However, it‚Äôs worth
noting that the currently accessible MLLMs are not as pow-
erful as their LLM counterparts, as they tend to produce inad-
equate responses and generate false information. As a solu-
tion for addressing the current issue, this paper proposes the
Visual Instruction Generation and Correction (VIGC) frame-
work that enables multimodal large language models to gen-
erate instruction-tuning data and progressively enhance its
quality on-the-fly. Specifically, Visual Instruction Generation
(VIG) guides the vision-language model to generate diverse
instruction-tuning data. To ensure generation quality, Visual
Instruction Correction (VIC) adopts an iterative update mech-
anism to correct any inaccuracies in data produced by VIG,
effectively reducing the risk of hallucination. Leveraging the
diverse, high-quality data generated by VIGC, we finetune
mainstream models and validate data quality based on various
evaluations. Experimental results demonstrate that VIGC not
only compensates for the shortcomings of language-only data
generation methods, but also effectively enhances the bench-
mark performance. The models, datasets, and code are avail-
able at https://opendatalab.github.io/VIGC.
Introduction
Over the past year, there have been significant advancements
in language models, particularly with the emergence of in-
struction tuning within Large Language Models (LLMs).
This technology enables models to perform complex tasks
in a zero-shot manner (OpenAI 2023a,b). The fusion of vi-
sual encoders with these LLMs (Touvron et al. 2023; Chi-
ang et al. 2023) has led to substantial strides in the field of
multimodal LLMs, resulting in the creation of frameworks
such as BLIP-2 (Li et al. 2023b), MiniGPT-4 (Zhu et al.
2023b), LLaV A (Liu et al. 2023b), InstructBLIP (Dai et al.
*Equal contribution. ‚Ä†Corresponding author.
BBox & CategoryA sleek silver motorcycle parked in a parking lot next to a brick wall. 
CaptionQuestion:  Can you elaborate on the elements of the picture provided? Answer: The image features a sleek silver motorcycle parked in a parking lot next to a brick wall. The motorcycle is positioned prominently in the scene, taking up most of the space in the parking lot. The close-up view of ‚Ä¶
Initial
Dataset
GPT
-
4
Prompt
QuestionQuestion: What color is the motorcycle in the image?
SFT
Data
VIGC
Instruction
SFT
Data
RGB
Images(a)Text-onlyGPT-4
(b)TheProposedVIGC[x1,y1,x2,y2,motorcycle]
Figure 1: Comparison between the language-only GPT-4 ap-
proach and the proposed method, highlighting two key limi-
tations of the former: (1) The necessity for extensive human
annotation, and (2) The inability to process visual data, re-
sulting in a loss of detailed information.
2023) and InternLM-XComposer (Zhang et al. 2023). These
frameworks have propelled the rapid evolution of image-
text multimodal tasks, exhibiting impressive capabilities in
image-text dialogue.
Traditional multimodal models follow a two-stage train-
ing process. The initial stage involves training the model
with image-text pairs to enhance feature alignment between
the two modalities. The subsequent stage utilizes high-
quality multimodal instruction tuning data to augment the
model‚Äôs ability to follow instructions, thereby improving its
response to user inquiries. However, compared to a large
amount of available multimodal pre-training data (Schuh-
mann et al. 2022; Sharma et al. 2018; Changpinyo et al.
2021; He et al. 2023), acquiring high-quality instruction
tuning data is relatively more challenging. Current high-
quality multimodal fine-tuning data (Liu et al. 2023b; Li
et al. 2023a) is primarily generated based on language-only
GPT-4 (OpenAI 2023b) as illustrated in Figure 1-(a). This
approach necessitates costly manual pre-annotation and re-
stricts the design of questions and generated responses to
existing annotated information. Consequently, if the ques-
tion posed is not within this annotated information, GPT-4arXiv:2308.12714v3  [cs.CV]  4 Feb 2024

--- PAGE 2 ---
is unable to respond. This method also loses the detailed in-
formation in the image for answerable questions.
To address this issue, researchers have started to consider
generating data with Vision-Language Models (VLMs) (Zhu
et al. 2023a; You et al. 2023; Zhang et al. 2023) as VLMs
have seen a vast amount of image-text pairs during the pre-
training phase and inherently contain a wealth of knowl-
edge. Currently, the accessible MLLMs are less powerful
than their LLM counterparts. They often produce inadequate
responses and generate false information, e.g., hallucination.
However, existing approaches attempt to generate data using
VLMs without considering how to ensure the quality of the
generated data or validate it experimentally.
In contrast to the aforementioned methods, we propose
Visual Instruction Generation and Correction, a new method
for high-quality instruction data generation. This method,
based on existing visual-language models, guides the model
to generate diverse visual-language question-answer pairs
on new images through the fine-tuning of initial instruction
data. The ability to generate diverse data is derived from
the fact that both the visual encoder and the large language
model have been fine-tuned on extensive datasets, encom-
passing rich image understanding and logical language ca-
pabilities. However, we found that data generated directly
from provided instructions suffer from severe hallucination
issues, which is a common problem plaguing large multi-
modal models (Peng et al. 2023b; Liu et al. 2023a; Zhao
et al. 2023; Huang et al. 2023). Fortunately, our visual in-
struction correction module can significantly reduce model
hallucination phenomena through iterative updates. The pri-
mary contributions of this study include:
‚Ä¢ We introduce Visual Instruction Generation and Correc-
tion (VIGC), a framework capable of autonomously gen-
erating high-quality image-text instruction fine-tuning
datasets. The VIGC framework consists of two sub-
modules: Visual Instruct Generation (VIG) and Visual
Instruct Correction (VIC). Specifically, the VIG gener-
ates initial visual question-answer pairs, while VIC mit-
igates model hallucination and obtains high-quality data
through an Iterative Q-Former (IQF) update strategy.
‚Ä¢ We release a series of datasets1(He et al. 2022) gen-
erated using VIGC, including 36,781 VIGC-LLaV A-
COCO and approximately 1.8 million VIGC-LLaV A-
Objects365, for research on large multimodal models. To
the best of our knowledge, this is the first-ever multi-
modal instruction fine-tuning dataset autonomously gen-
erated by a MLLM.
‚Ä¢ We have conducted extensive experiments on the gen-
erated data. When trained in conjunction with the
VIGC-generated data, the performance of the LLaV A-7B
model significantly improved, even surpassing that of the
LLaV A-13B model. Furthermore, on mainstream multi-
modal evaluation datasets such as MMBench, OKVQA,
and A-OKVQA, models trained with the VIGC data uni-
formly demonstrated performance enhancements.
1https://opendatalab.com/OpenDataLab/VIGC-InstDataRelated Work
Instruction-following LLMs
The domain of Natural Language Processing (NLP) has
been significantly shaped by the advent and evolution of
large language models (LLMs), including but not limited
to GPT-3 (Brown et al. 2020), PaLM (Chowdhery et al.
2022), T5 (Raffel et al. 2020), and OPT (Zhang et al. 2022).
These models, equipped with extensive training data and so-
phisticated optimization techniques, have demonstrated re-
markable performance across various tasks. However, a no-
table challenge persists in their ability to effectively fol-
low instructions, often leading to suboptimal results in di-
verse real-world applications. Efforts to address this issue
have led to the introduction of various instruction fine-tuning
datasets. Enhanced models, such as InstructGPT (Ouyang
et al. 2022), ChatGPT (OpenAI 2023a), FLAN-T5 (Chung
et al. 2022), FLAN-PaLM (Chung et al. 2022), and OPT-
IML (Iyer et al. 2022), have been developed to improve
upon zero-shot and few-shot learning capabilities, primar-
ily by learning to map instructions to the corresponding ex-
pected outputs. Despite these advancements, the generation
of instruction datasets frequently relies on pre-existing NLP
tasks, which curtails their generalizability. To augment the
quality and diversity of instructions, Wang et al. (Wang et al.
2022) introduce SELF-INSTRUCT, a methodology that em-
ploys generated instruction data to enhance the performance
of LLMs. While these methods have made significant strides
in augmenting the instruction-following capabilities of lan-
guage models, they exhibit a standard limitation in that they
cannot be directly generalized to multimodal data.
Multi-modal Instruction Tunning
Compared to creating language instruction fine-tuning
datasets, constructing multimodal instruction fine-tuning
datasets requires a thorough understanding of image content
and the development of the corresponding text. MiniGPT-4
utilizes a feature-aligned model to interpret the CC dataset
(Sharma et al. 2018; Changpinyo et al. 2021), employs Chat-
GPT for initial filtering, and ultimately curates 3,500 image-
text pairs for model refinement. However, this methodol-
ogy encounters restrictions in terms of instruction diversity
and volume. In contrast, LLaV A proposes an innovative ap-
proach based on a language-only GPT-4 (OpenAI 2023b) to
generate multimodal instruction data from information that
includes caption descriptions and target data. While this ap-
proach generates high-quality data, it demands manual an-
notation of each caption description, target information, and
question, which inherently limits scalability. To extend data
across a more comprehensive array of tasks, InstructBLIP
pioneers an Instruction template construction methodology,
converting 26 unique datasets into instruction fine-tuning
data and achieving impressive results across several tasks.
Concurrently, MIMIC (Li et al. 2023a) assembles larger-
scale instruction fine-tuning datasets.
Nevertheless, all these datasets require human interven-
tion in the form of annotations, and their diversity is in-
herently limited by the existing data. By contrast, our study
aims to propose a self-guided, model-driven instruction fine-

--- PAGE 3 ---
VIG-Instruction:Generate a question based on the content of the given image and then answer it.
Q:What are the colors of the bus in the image?A:The bus in the image is white and red.
Conversation Detail Description Complex Reasoning ViT-GQ-FormerFCVicuna
InitialInstructionDataset
VIGC-Model
Question,AnswerTrainingStage
weight sharingConversation InstructionInstructionQuery
QuestionQuestionQueryVIGInstruction: Generate a question to describe the image content in detail and then answer it.Question:  Can you elaborate on the elements of the picture provided? Answer: The image features a sleek silver motorcycle parked in a parking lot next to a brick wall. The motorcycle is positioned prominently in the scene, taking up most of the space in the parking lot. The close-up view of the motorcycle emphasizes its sleek and modern design, showcasing the attention to detail and craftsmanship that went into its creation. AsubsAsubsAsubs
Q
QA1A2QInst
A1A2AsubsQA1
Inst
Inst
Inst
VICVICVICVIGùêºùëõùë†ùë°ùëüùë¢ùëêùë°ùëñùëúùëõ‚ÜíùëÑ+ùê¥ùêºùëõùë†ùë°ùëüùë¢ùëêùë°ùëñùëúùëõ+ùëÑ‚Üíùê¥!+ùê¥!ùêºùëõùë†ùë°ùëüùë¢ùëêùë°ùëñùëúùëõ+ùëÑ+ùê¥!‚Üíùê¥"+ùê¥"‚Ä¶‚Ä¶VIGC-Model
InferenceStage
ViT-GQ-FormerFCVicuna
Answer
‚Ä¶VICFigure 2: The proposed Visual Instruction Generation and Correction (VIGC) framework. The left panel illustrates the VIGC
training process: Instruction fine-tuning data is fed into the VIG and VIC sub-modules. VIG aims to generate image-related
question-answer pairs, while VIC refines the VIG-produced answers for precision. The right panel depicts the inference phase,
where VIGC takes an arbitrary image as input, generates initial answers, and then refines them to construct high-quality data.
tuning data generation method, which is capable of creating
high-quality fine-tuning data suitable for any novel image.
Visual Question Generation
Visual Question Generation (VQG) aims to generate rele-
vant questions based on provided images, which poses con-
siderable challenges due to the need for diversity, natural-
ness, and engagement. Mostafazadeh et al. (Mostafazadeh
et al. 2016) propose the task of Visual Question Genera-
tion (VQG) and attempt to establish a foundational VQG
framework, employing both retrieval-based and generative
methodologies. iQAN (Li et al. 2018) later proposed a uni-
fied, reversible network addressing both VQA and VQG
tasks, enabling both answer retrieval and question gener-
ation from images. Guiding models like Guiding Visual
Question Generation (Vedd et al. 2021) have also con-
tributed significantly to the field.
This paper proposes the Visual Instruction Generation and
Correction, a model that generates image-related content,
similar to VQG. Unlike the existing work, our method in-
troduces an additional layer of complexity by developing di-
verse questions and providing appropriate answers based on
different requirement categories. Leveraging the vast knowl-
edge of large language models, our model‚Äôs output outper-
forms traditional VQG tasks, which are usually limited by
their training sample size.
Methods
This paper concentrates on leveraging the power of exist-
ing vision-language models to generate multimodal instruc-
tion following data autonomously. The proposed approach
facilitates the creation of robust and diverse fine-tuning
datasets, eliminating the requirement for intensive manual
intervention. However, utilizing existing multimodal mod-
els to achieve this objective presents substantial challenges.To mitigate these, we introduce a self-instructing frame-
work named VIGC. Guided by existing fine-tuning data,
this framework can generate higher quality and more diverse
new data, as depicted in Figure 2.
Initial Instruction Construction
In contrast to language instructions, which can be effort-
lessly generated by standalone language models (Peng et al.
2023a; Wang et al. 2022), the construction of visual-text
multimodal instructions requires a detailed understanding of
visual content, as well as the ability to pose relevant ques-
tions and provide correct answers based on the actual con-
tent of the images. Nevertheless, existing multimodal mod-
els are deficient in their capacity to directly generate visual-
language instruction data. To overcome this limitation, we
exploit readily available instruction fine-tuning data and for-
mulate additional instruction templates, thereby facilitating
the automatic generation of instruction data.
Our proposed method is universally applicable to gen-
erating various types of image-text multimodal instruction
fine-tuning data. To elucidate our approach, we exemplify
it using the generation of LLaV A-style data instructions.
Specifically, we construct instruction templates encompass-
ing dialogue, detailed description, and complex reasoning,
following the categorization of instruction fine-tuning data
types as delineated in LLaV A. Figure 3 presents instances
of these three types of instruction templates, which are es-
sentially uncomplicated, principally requesting, ‚Äúgenerate
T-type question-answer pairs predicated on the image con-
tent. ‚Äù Theoretically, if a model can comply with these in-
struction descriptions following training, it should be profi-
cient in generating question-answer pairs.
With the instruction templates and existing visual
instruction-tuning data (i.e., Question-Answer pairs in
LLaV A), we construct a comprehensive VIG instruction-
tuning dataset as follows:

--- PAGE 4 ---
Conversation:Generate a question based on the content of the given image and then answer it.VIG Instruction Sample
VIC Instruction Sample‚Ä¶‚Ä¶
DetailDescription:Generate a question to describe the image content in detail and then answer it.
ComplexReasoning:Based on the given image, generate an in-depth reasoning question and then answer it.
Question: {Question}    Answer:
Figure 3: Template examples corresponding to instruction
tuning in VIG and VIC submodules.
TV IG = (Xi, It, Qt
i, At
i)Nt(1)
where i‚àà {1,2, ..., N t},Ntdenotes the instruction type,
such conversation, detailed description, etc. Xirepresents
an RGB image, Iirepresents an instruction corresponding
to a specific type t,Qt
iis a question related to the image
Xiunder the context of instruction It, andAt
iis the answer
to the question Qt
i. Our objective is to leverage this dataset
for the training of models that, given a specific instruction
It, can generate corresponding question-answer pairs for a
given image, following the designated instruction type. Fig-
ure 2 provides illustrations of the initial instruction dataset.
Distinguished from the VIG, the VIC instruction em-
ploys an image and a query as input for its fine-tuning pro-
cess, with the objective of generating precise responses. The
dataset for the VIC instruction is presented below:
TV IC = (Xi, Qt
i, At
i)Nt(2)
Visual Instruction Generation
In alignment with current popular multimodal models such
as MiniGPT-4 (Zhu et al. 2023b) and InstructBLIP (Dai
et al. 2023), the architecture of the proposed VIGC can be
dissected into four primary components: the visual encoder
(ViT) (Fang et al. 2023), the large language model (Vicuna)
(Chiang et al. 2023), the Q-Former (Li et al. 2023b) for vi-
sual feature extraction, and the Fully-Connected (FC) pro-
jection for reconciling visual-language features. Function-
ally, the model can be further segmented into two distinc-
tive sub-modules: the Visual Instruction Generation (VIG)
module and the Visual Instruction Correction (VIC) module.
It is imperative to underscore that these two sub-modules
share network parameters, the primary differentiator being
the data type employed for training.
The principal objective of the VIG module is to au-
tonomously produce relevant visual question-answer pairs
that correspond to a specific instructional command for any
given image. Figure 2 illustrates the process that the VIGmodule follows in the training phase. In the training phase,
the VIG module stochastically selects an image, which is
subsequently processed via a visual encoder. It generates a
set of fixed visual feature embeddings. The Q-Former mod-
ule, purposefully designed to be aware of instructional infor-
mation, further refines these visual features. At this stage,
the model employs learnable visual queries that perform
self-attention operations in conjunction with the instruction.
This operation is followed by a cross-attention phase with
visual embeddings. This mechanism impels the visual fea-
tures to concentrate on the instructional information, thereby
augmenting their relevance and precision within the context
of the assigned task. Following the cross-attention phase, the
refined features are channeled through an FC mapping layer,
a crucial step that aligns visual features with their linguis-
tic counterparts, thereby ensuring a seamless integration of
visual and language features. Subsequently, the instruction-
aligned features are ingested by the language model. This
process guides the model to generate the predicted results.
Specifically, the objective in this context is to generate vi-
sual questions and answers that are intrinsically linked to the
content of image Xi, the nature of which is determined by
the instruction. We utilize the original auto-regressive loss
function inherent to the large language model. This method-
ology guides the model in generating sentences that align
with the question-answer pairs provided in the training set.
Visual Instruction Correction
In the exploration conducted for this study, we discovered
that existing multimodal models (Liu et al. 2023b), (Dai
et al. 2023), much like language models (Radford et al.
2018, 2019; Brown et al. 2020; OpenAI 2023b,a), often ex-
hibit hallucination issues. This hallucination phenomenon is
also present in the data generated by the VIG, especially
in instances of extensive descriptions. We attribute this to
the tendency of multimodal models to progressively rely on
the current answer text during the answer generation phase,
thereby gradually neglecting the image information and con-
sequently leading to the description of targets not present
in the image. To eliminate the hallucination phenomenon in
generated data and ensure that downstream tasks based on
this data are not contaminated, we specifically introduce an
instruction correction module to update the answers and re-
duce the occurrence of hallucinations.
To effectively utilize the VIC, specific actions need to
be undertaken during both the model training and inference
stages:
During the training phase: The goal of the VIG phase is to
generate corresponding visual question-answer pairs given
an instruction. Conversely, the objective of the VIC training
phase is to supply the model with a Question, thereby direct-
ing the model to focus on extracting features pertinent to the
input question/text during the Q-Former feature extraction
process. These features lay the groundwork for subsequent
answers.
During the inference phase: After training the model us-
ing the aforementioned VIC method, it can take the ques-
tions from the question-answer pairs generated by the VIG
as input and regenerate answers. Since the model places

--- PAGE 5 ---
greater emphasis on the question when formulating re-
sponses, the generated results are typically more accurate.
Furthermore, we iterate this Q-Former feature updating pro-
cess, termed as Iterativate-Q-Former (IQF), as illustrated in
the VIGC inference phase in Figure 2. Before deploying the
VIC module, we initially generate the initial question (Q)
and answer (A) using the VIG. In the first iteration, we use
the Instruction and Question as inputs to output answers A1
and¬ØA1, where A1represents the first sentence of the answer
and ¬ØA1signifies all content following the first sentence. In
the second iteration, we input the Instruction, Question, and
the answer A1from the previous step to predict A2, and this
process continues iteratively until a termination symbol is
encountered. The efficacy of this iterative approach is pri-
marily due to the continual updating of visual features with
the most recent textual information, making subsequent re-
sults more accurate. However, it should be noted that while
this method is highly beneficial for providing detailed de-
scriptions of image content, its effectiveness for dialogue
tasks and inference tasks is relatively limited. This is be-
cause dialogue tasks usually consist of single sentences, and
the subsequent content in inference tasks does not heavily
depend on image information.
Experiments
Datasets
Training Data . We trained the VIGC network using two
types of visual-language instruction fine-tuning data. The
first type, represented by the LLaV A dataset (Liu et al.
2023b), is manually curated and combined with language-
only GPT-4 (OpenAI 2023b) for multimodal models. It in-
cludes 150K training samples, subdivided into simple dia-
logue (57,669 samples), detailed description (23,240 sam-
ples), and complex reasoning vision-language data (76,803
samples). This dataset spans various facets of multimodal
dialogue, including category recognition, counting, action
recognition, and scene recognition. The detailed descrip-
tions demand careful image observation and comprehensive
detailing, while the complex reasoning tasks require deep
inference and external knowledge integration. The second
type of data is multimodal instruction fine-tuning data de-
rived from publicly available image-text datasets. Specifi-
cally, we used OKVQA (Marino et al. 2019) and A-OKVQA
(Schwenk et al. 2022) datasets, as utilized in InstructBLIP
(Dai et al. 2023), for VIGC training. These datasets, neces-
sitating extensive external knowledge, are ideal for assessing
the VIGC‚Äôs capabilities.
Inference Data . Following the VIGC network training, we
generated fine-tuning data for multimodal instruction using
image datasets. We employed two distinct datasets, COCO
(Lin et al. 2014) and Objects365 (Shao et al. 2019), to eval-
uate VIGC‚Äôs effectiveness in handling data within the same
or different image domains. The COCO dataset serves as
the foundation for the construction of the LLaV A, OKVQA,
and A-OKVQA datasets. It‚Äôs crucial to emphasize that dur-
ing the data generation phase, we intentionally omitted any
images that were previously included in the test set to ensure
the fairness and effectiveness of the evaluation.Implementation Details
During the training phase of VIGC, we utilize the MiniGPT-
4 (Zhu et al. 2023b) first-stage pre-trained model as the
source of initial parameters. This ensures that the ini-
tial model does not incorporate additional instruction fine-
tuning data for training, thereby preserving the fairness of
downstream task validation. This model encompasses the
ViT-G/14 from EV A-CLIP (Fang et al. 2023), the Q-Former
(Li et al. 2023b), and a linear projection layer. The lan-
guage models employed are Vicuna7B and Vicuna13B (Chi-
ang et al. 2023). It is noteworthy that, as illustrated in Figure
1, our Q-Former is designed to receive either Instruction or
Question text simultaneously, which is crucial for the itera-
tive correction in VIC. Therefore, we utilize the Q-Former
from BLIP2-FlanT5 XXL(Li et al. 2023b) as the initial pa-
rameters for the Q-Former. We designate this network model
as MiniGPT-4+. During the training process, only the pa-
rameters of the Q-Former and the linear projection layer
are subjected to fine-tuning, while the parameters of the
language and visual models remain constant. The training
is conducted throughout 10epochs, with the model‚Äôs per-
formance being validated after each epoch. The model that
demonstrates the best performance is subsequently selected
for data generation.
In terms of batch sizes, we utilize 64 for both 7B and 13B
models. The entire training process, executed on 8 A100
(80GB) GPUs, completes in approximately 10 hours.
LLaVA Data and Evaluation
Dataset Analysis . In the pursuit of generating a more di-
verse set of LLaV A-like data, the VIGC model is trained
using a combination of LLaV A-150K data and three types
of instruction templates. During the inference phase, we uti-
lized images from the COCO 2017 training set, intentionally
excluding those already included in the LLaV A dataset. This
resulted in the selection of a total of 36,781 initial images,
which served as the foundation for instruction data genera-
tion; we refer to this data as coco-extra , which serves as the
default supplementary data used for model training during
evaluation.
Based on the aforementioned data, the VIG network gen-
erates diverse initial questions and answers. Subsequently,
the VIC network refines the outputs by taking the questions
and the existing answers as inputs through the Iterative Q-
Former (IQF) operation, thus generating higher-quality re-
sponses. Figure 4 illustrates the three categories of data gen-
erated via the VIGC process:
‚Ä¢ Conversation: The questions are typically specific, elicit-
ing concise and clear responses.
‚Ä¢ Detail Description: The questions are relatively fixed and
revolve around describing the image‚Äôs content. This re-
quires the model to clearly observe all targets within the
image. It is observed that the detailed descriptions gen-
erated directly from Visual Instruction Generation (VIG)
are fraught with numerous illusions. However, after the
application of Visual Instruction Correction (VIC), these
illusory phenomena have significantly diminished.

--- PAGE 6 ---
Answer (VIG): The cat is sitting or standing in front of a television, watching a bird on the screen.
Answer (VIC): The cat is sitting or standing in front of a large television screen, watching a bird fly on the screen.Answer (VIG): The image features a cat sitting in front of a large television screen, attentively watching a show featuring two birds on the screen. The television is positioned at the center of the scene, capturing the cat's full attention. In addition to the television, there are several books scattered around the room. Some of them are placed near the television, while others are located closer to the edges of the scene. A small bowl can also be spotted in the room, likely used for food or other items.Answer (VIC): The image features a cat sitting in front of a large television screen that displays a scene of birds flying, capturing the cat‚Äòs attention. The cat is sitting on a desk or counter in front of the television, and it appears to be engrossed in watching the birds on the screen. The cat‚Äôs posture and facial expression suggest that it is intrigued by the moving images on the television. The cat‚Äòs presence in front of the television is a common sight in many households, as cats are known to be fascinated by moving images, especially those that involve other animals. In this case, the birds flying on the screen have likely caught‚Ä¶Answer (VIG): The cat might be attracted to the TV screen due to the presence of a bird on the screen. Cats are naturally curious creatures, and they are often drawn to moving objects, especially those that resemble prey. In this case, the bird on the TV screen is likely captivating the cat‚Äòs attention, making it an interesting and engaging visual stimulus for the cat. Additionally, cats may also be attracted to the colorful and dynamic nature of‚Ä¶Answer (VIC): The cat might be attracted to the TV screen because it is displaying an image of a bird, which is a natural prey for cats. Cats have a strong instinct to hunt and chase moving objects, especially those that resemble potential prey. In this case, the bird on the TV screen triggers the cat's hunting instinct, making it curious and drawn to the screen. Additionally, cats are often fascinated by bright and colorful‚Ä¶Question:Can you elaborate on the elements of the picture provided?Question:  What is the cat doing in the image?   
Question: Why might the cat be attracted to the TV screen?  
ConversationDetail Description
Complex ReasoningFigure 4: Generated Instructions based on the proposed VIGC.
Model Metrics
Conv Detail Complex All
LLaV A-7B (Baseline) 75.1 75.4 92.3 81.0
add 36K Conv (VIG) 80.9 76.1 92.6 83.3
add 36K Conv (VIC) 83.9 76.9 90.9 84.0
add 36K Detail (VIG) 80.2 72.7 90.9 81.4
add 36K Detail (VIC) 83.3 80.6 93.1 85.8
add 36K Complex (VIG) 81.4 75.6 90.5 82.6
add 36K Complex (VIC) 80.2 76.2 93.2 83.3
replace 10K Conv 78.2 76.5 91.6 82.1
replace 10K Detail 75.8 79.8 91.2 82.2
replace 10K Complex 77.5 77.8 92.8 82.8
replace Conv/Detail/Complex 78.3 76.6 92.4 82.5
Table 1: Comparative Evaluation of VIGC Data Addition vs.
Replacement in Model Training on the LLaV A Evaluation
‚Ä¢ Complex Reasoning: The posed questions necessitate the
integration of external knowledge and the application of
sophisticated logical reasoning skills.
Overall, the quality of the visual-language question-
answer pairs autonomously generated by the model has ex-
ceeded our initial expectations. We posit that this rich new
knowledge inherently resides within the language model it-
self, and we have merely employed multimodal instruction
fine-tuning to distill this knowledge onto new multimodal
data.
Dataset Evaluation . Based on the generated data, we con-
ducted detailed ablation experiments on LLaV A-7B to ver-ify the performance improvement of the model after training
with the generated data. The evaluation method used here is
the quantitative evaluation proposed by LLaV A, where GPT-
4 assesses the quality of the model‚Äôs responses to given eval-
uation questions, which can be understood as relative scores
compared to GPT-4. LLaV A provides 30 test images, each
containing three types of questions, for 90 questions.
Table 1 presents the results of augmenting the original
LLaV A-150K dataset with three types of generated data,
followed by fine-tuning the LLaV A first-stage model with
instructions. Including instruction data directly generated
from VIG during the training phase has proven to be ben-
eficial. We observed a marginal improvement when adding
detailed description data generated by VIG, which can be
attributed to the severe illusions present in this data. In con-
trast, the incorporation of conversation data and complex
reasoning data has led to appreciable performance gains.
Further refining the data using VIC and then training the
model with the augmented conversation data, detailed de-
scription data, and complex reasoning data resulted in addi-
tional improvements. The performance metrics have reached
84.0%,85.8%, and 83.3%, respectively. These results under-
score the critical role of VIC in eliminating hallucinations,
thereby enhancing the model‚Äôs overall performance. Simul-
taneously, to validate the superiority of the VIGC-generated
dataset over the LLaV A dataset, we conducted an experi-
ment where we randomly replaced 10,000 instances from
each type of data, as well as a complete replacement of
all three types of data. The experimental results indicated
that, under the condition of constant data volume, the per-
formance of the model trained on a mixture of the LLaV A

--- PAGE 7 ---
MMBench LLaV A
Method LR AR RR FP-S FP-C CP Overall Conv Detail Complex All
MiniGPT-4+ 10.0 31.3 7.83 18.9 13.1 43.0 24.4 83.5 77.8 92.4 84.7
MiniGPT-4+ w/ coco 11.7 27.8 19.1 27.9 11.0 44.3 27.5(‚Üë3.1) 84.1 84.1 92.7 87.0(‚Üë2.3)
Table 2: Performance of MiniGPT-4+ Models on MMBench and LLaV A-Eval Datasets. MMBench Metric include Logic Rea-
soning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Fine-grained Perception at Instance-level (FP-S), Fine-
grained Perception at Cross-instance (FP-C), and Coarse Perception (CP).
Model Conv Detail Complex All
LLaV A-7B 75.1 75 .4 92 .3 81 .0
LLaV A-7B w/ coco 83.3 80 .6 93 .1 85.8(‚Üë4.8)
LLaV A-7B w/ objects365 86.8 77 .6 90 .9 85 .2
LLaV A-13B* 82.7 76 .6 94 .8 84 .8
LLaV A-13B w/ coco 88.9 77 .4 93 .5 86.8(‚Üë2.0)
Table 3: Relative scores for different settings w.r.t. GPT-
4 (language-only) on LLaV A-Eval Dataset. The results for
LLaV A-13B are reproduced from (Liu et al. 2023b).
Model OKVQA A-OKVQA
PaLM-E (Driess et al. 2023) 66.1 -
PromptCap (Hu et al. 2022) 60.4 56.3
MiniGPT-4+ w/o VIGC 59.1 58.3
MiniGPT-4+ w/ VIGC 59.8(‚Üë0.7) 58.9(‚Üë0.6)
InstructBLIP w/o VIGC 63.1 62.5
InstructBLIP w/ VIGC 63.8 (‚Üë0.7) 64.1 (‚Üë1.6)
Table 4: Results of finetuning MiniGPT-4+ and Instruct-
BLIP on OKVQA and A-OKVQA dataset.
dataset and the VIGC dataset surpasses that of the model
trained solely on the LLaV A dataset.
Table 3 presents experiments conducted on different
datasets and models of varying sizes, substantiating that the
use of generated data from different domains, such as Ob-
jects365 and COCO, can still lead to remarkable perfor-
mance improvements. This offers a novel solution for en-
hancing the performance of cross-domain tasks. We also
conducted experiments on LLaV A-13B, proving that perfor-
mance enhancement can be achieved on larger models as
well.
We also evaluated the performance of the VIGC model on
MMBench, LLaV A (as shown in Table 2) and further fine-
tuned the VIGC model based on 36K COCO data generated
by VIGC. We discovered that following this self-iterative
training process, the model performance improved on both
MMBench and LLaV A. This promising capability of self-
enhancement through iterative training is a subject we plan
to continue exploring in our future research.
OK-VQA Dataset and Evaluation
To further assess the quality of the data generated by
the VIGC model, we conducted training and evaluation
on the OKVQA dataset, which requires external knowl-edge. Specifically, we trained the VIGC network using
the OKVQA dataset and corresponding instruction tem-
plates. Subsequently, we generated additional instruction
fine-tuning data based on VIGC on COCO. Ultimately, we
fine-tuned InstructBLIP based on OKVQA and the gener-
ated data. We found that despite InstructBLIP already utiliz-
ing a substantial amount of data in the instruction fine-tuning
phase, the use of additional generated data for downstream
task fine-tuning still enhanced the model‚Äôs performance on
specific datasets. We performed the same experimental vali-
dation on A-OKVQA.
The experimental results are presented in Table 4. It can
be seen that the performance of the InstructBLIP model,
when fine-tuned with the addition of generated data, outper-
forms the model only fine-tuned with original data. There
were improvements of 0.7% and 1.6% on OKVQA and A-
OKVQA, respectively, achieving state-of-the-art results for
models of this scale on both datasets. BUsing the MiniGPT-
4+ pre-training model, we arrived at similar conclusions.
This demonstrates that generated data can effectively en-
hance downstream fine-tuning performance, a finding that
holds significant value for domains where data acquisition
is challenging.
Conclusion
In this paper, we introduced the Visual Instruction Genera-
tion and Correction framework, a novel self-instruct method
for autonomously generating high-quality vision-language
instruction data. Leveraging the VIGC framework, we have
generated diverse multimodal instruction-following data on
the COCO and Objects365 datasets. The quality of this data
has been validated through various evaluations. The VIGC-
based approach provides a convenient means to acquire
more high-quality instruction tuning data. While using the
Visual Instruct Correction has significantly reduced model
hallucination, some instances persist. We intend to delve
deeper into the exploration of solutions aimed at eliminat-
ing multimodal hallucinations in the future. Moreover, we
are considering the potential of forming a closed-loop sys-
tem by integrating the VIGC‚Äôs autonomous data generation
with multimodal model training. This system would enhance
model performance through data improvement and, recipro-
cally, elevate data quality through model enhancement.
Acknowledgments
This project was supported by National Key R&D Program
of China (No. 2022ZD0160101) and Shanghai Artificial In-
telligence Laboratory.

--- PAGE 8 ---
References
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. In
NeurIPS . 2, 4
Changpinyo, S.; Sharma, P.; Ding, N.; and Soricut, R. 2021.
Conceptual 12m: Pushing web-scale image-text pre-training
to recognize long-tail visual concepts. In CVPR . 1, 2
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.
2023. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality. 1, 4, 5
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;
Gehrmann, S.; et al. 2022. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 . 2
Chung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-
dus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.
2022. Scaling instruction-finetuned language models. arXiv
preprint arXiv:2210.11416 . 2
Dai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;
Li, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards
General-purpose Vision-Language Models with Instruction
Tuning. arXiv preprint arXiv:2305.06500 . 1, 4, 5
Driess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.;
Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al.
2023. Palm-e: An embodied multimodal language model.
arXiv preprint arXiv:2303.03378 . 7
Fang, Y .; Wang, W.; Xie, B.; Sun, Q.; Wu, L.; Wang, X.;
Huang, T.; Wang, X.; and Cao, Y . 2023. Eva: Exploring the
limits of masked visual representation learning at scale. In
CVPR . 4, 5
He, C.; Jin, Z.; Xu, C.; Qiu, J.; Wang, B.; Li, W.; Yan, H.;
Wang, J.; and Lin, D. 2023. Wanjuan: A comprehensive
multimodal dataset for advancing english and chinese large
models. arXiv preprint arXiv:2308.10755 . 1
He, C.; Li, W.; Jin, Z.; Wang, B.; Xu, C.; and Lin, D. 2022.
OpenDataLab: Empowering General Artificial Intelligence
with Open Datasets. https://opendatalab.com. 2
Hu, Y .; Hua, H.; Yang, Z.; Shi, W.; Smith, N. A.; and Luo,
J. 2022. Promptcap: Prompt-guided task-aware image cap-
tioning. arXiv preprint arXiv:2211.09699 . 7
Huang, Q.; Dong, X.; Zhang, P.; Wang, B.; He, C.; Wang,
J.; Lin, D.; Zhang, W.; and Yu, N. 2023. OPERA: Alleviat-
ing Hallucination in Multi-Modal Large Language Models
via Over-Trust Penalty and Retrospection-Allocation. arXiv
preprint arXiv:2311.17911 . 2
Iyer, S.; Lin, X. V .; Pasunuru, R.; Mihaylov, T.; Simig, D.;
Yu, P.; Shuster, K.; Wang, T.; Liu, Q.; Koura, P. S.; et al.
2022. Opt-iml: Scaling language model instruction meta
learning through the lens of generalization. arXiv preprint
arXiv:2212.12017 . 2
Jain, S. M. 2022. Hugging face. In Introduction to Trans-
formers for NLP: With the Hugging Face Library and Mod-
els to Solve Problems , 51‚Äì67. Springer. 12Li, B.; Zhang, Y .; Chen, L.; Wang, J.; Pu, F.; Yang, J.; Li,
C.; and Liu, Z. 2023a. Mimic-it: Multi-modal in-context
instruction tuning. arXiv preprint arXiv:2306.05425 . 1, 2
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:
Bootstrapping language-image pre-training with frozen im-
age encoders and large language models. arXiv preprint
arXiv:2301.12597 . 1, 4, 5
Li, Y .; Duan, N.; Zhou, B.; Chu, X.; Ouyang, W.; Wang, X.;
and Zhou, M. 2018. Visual question generation as dual task
of visual question answering. In CVPR . 3
Lin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Doll ¬¥ar, P.; and Zitnick, C. L. 2014. Microsoft
coco: Common objects in context. In ECCV . Springer. 5
Liu, F.; Lin, K.; Li, L.; Wang, J.; Yacoob, Y .; and Wang,
L. 2023a. Aligning Large Multi-Modal Model with Robust
Instruction Tuning. arXiv preprint arXiv:2306.14565 . 2
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023b. Visual in-
struction tuning. arXiv preprint arXiv:2304.08485 . 1, 4, 5,
7
Marino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R.
2019. Ok-vqa: A visual question answering benchmark re-
quiring external knowledge. In CVPR . 5
Mostafazadeh, N.; Misra, I.; Devlin, J.; Mitchell, M.; He, X.;
and Vanderwende, L. 2016. Generating natural questions
about an image. In ACL. 3
OpenAI. 2023a. ChatGPT. https://openai.com/blog/chatgpt.
1, 2, 4
OpenAI. 2023b. GPT-4 Technical Report.
arXiv:2303.08774. 1, 2, 4, 5
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. In NeurIPS . 2
Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J.
2023a. Instruction tuning with gpt-4. arXiv preprint
arXiv:2304.03277 . 3
Peng, Z.; Wang, W.; Dong, L.; Hao, Y .; Huang, S.; Ma,
S.; and Wei, F. 2023b. Kosmos-2: Grounding Multimodal
Large Language Models to the World. arXiv preprint
arXiv:2306.14824 . 2
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;
et al. 2018. Improving language understanding by gener-
ative pre-training. 4
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;
Sutskever, I.; et al. 2019. Language models are unsupervised
multitask learners. OpenAI blog , 1(8): 9. 4
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Explor-
ing the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. 2
Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.;
Wightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis,
C.; Wortsman, M.; et al. 2022. Laion-5b: An open large-
scale dataset for training next generation image-text models.
InNeurIPS . 1

--- PAGE 9 ---
Schwenk, D.; Khandelwal, A.; Clark, C.; Marino, K.; and
Mottaghi, R. 2022. A-okvqa: A benchmark for visual ques-
tion answering using world knowledge. In ECCV . 5, 12
Shao, S.; Li, Z.; Zhang, T.; Peng, C.; Yu, G.; Zhang, X.; Li,
J.; and Sun, J. 2019. Objects365: A large-scale, high-quality
dataset for object detection. In ICCV . 5
Sharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018.
Conceptual captions: A cleaned, hypernymed, image alt-text
dataset for automatic image captioning. In ACL. 1, 2
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023. Llama: Open and efficient founda-
tion language models. arXiv preprint arXiv:2302.13971 . 1
Vedd, N.; Wang, Z.; Rei, M.; Miao, Y .; and Specia, L. 2021.
Guiding visual question generation. In NAACL . 3
Wang, Y .; Mishra, S.; Alipoormolabashi, P.; Kordi, Y .;
Mirzaei, A.; Arunkumar, A.; Ashok, A.; Dhanasekaran,
A. S.; Naik, A.; Stap, D.; et al. 2022. Super-
naturalinstructions: Generalization via declarative instruc-
tions on 1600+ nlp tasks. In EMNLP . 2, 3
You, H.; Sun, R.; Wang, Z.; Chen, L.; Wang, G.; Ayyubi,
H. A.; Chang, K.-W.; and Chang, S.-F. 2023. IdealGPT: It-
eratively Decomposing Vision and Language Reasoning via
Large Language Models. arXiv preprint arXiv:2305.14985 .
2
Zhang, P.; Wang, X. D. B.; Cao, Y .; Xu, C.; Ouyang, L.;
Zhao, Z.; Ding, S.; Zhang, S.; Duan, H.; Yan, H.; et al. 2023.
Internlm-xcomposer: A vision-language large model for ad-
vanced text-image comprehension and composition. arXiv
preprint arXiv:2309.15112 . 1, 2
Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V .; et al. 2022.
Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068 . 2
Zhao, Z.; Wang, B.; Ouyang, L.; Dong, X.; Wang, J.; and
He, C. 2023. Beyond Hallucinations: Enhancing LVLMs
through Hallucination-Aware Direct Preference Optimiza-
tion. arXiv preprint arXiv:2311.16839 . 2
Zhu, D.; Chen, J.; Haydarov, K.; Shen, X.; Zhang, W.; and
Elhoseiny, M. 2023a. Chatgpt asks, blip-2 answers: Au-
tomatic questioning towards enriched visual descriptions.
arXiv preprint arXiv:2303.06594 . 2
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.
2023b. Minigpt-4: Enhancing vision-language understand-
ing with advanced large language models. arXiv preprint
arXiv:2304.10592 . 1, 4, 5

--- PAGE 10 ---
Supplementary Material
A. Instruction Templates
The instruction templates for VIGC, including conversation, detailed description, complex reasoning, and OKVQA types, are
presented in Tables 5, 6, 7, and 8 respectively.
No. Instruction
1 Generate a question based on the content of the given image and then answer it.
2 Given the image, generate a question along with the answer.
3 From the image provided, craft a question and answer it.
4 Come up with a question related to the content of the image and provide the answer.
5 Brainstorm a query associated to the image and provide the response.
6 Construct a question based on the information presented in the image and answer it.
7 Ask yourself a question about the content of the image and respond to it.
8 Establish a query related to the content of the image and give the answer.
9 Ask a question derived from the image and then answer it.
10 Create a question about the image and answer it.
Table 5: The list of instructions for conversation.
No. Instruction
1 Generate a question to describe the image content in detail and then answer it.
2 Considering the picture, come up with a question to describe the image content in detail along with
the answer.
3 Describe the image content with a question and give the response.
4 Come up with a creative question to express the image content and then provide the answer.
5 Draft a query to address the image content and give the reply.
6 Create a question to reveal the image content and give the resolution.
7 Given the photo, state a question that reveals the details of the image and then answer it.
8 Ask a question about what is depicted in the image and then answer it.
9 Make up a query to explain the photo in more detail and answer it.
10 Compose a question describing the subject of the image, followed by the answer.
Table 6: The list of instructions for detailed description.
No. Instruction
1 Based on the given image, generate an in-depth reasoning question and then answer it.
2 Given the image, generate an in-depth reasoning question and answer.
3 Taking the image into account, generate an reasoning question along with the answer.
4 Can you come up with a reasoning question based on the image and then provide the answer?
5 After looking at the image, devise a reasoning question and provide the answer to it.
6 Contemplate the image and create a reasoning question with the answer provided.
7 Analyze the image and provide a reasoning question as well as the answer.
8 Compose a reasoning question using the image with its answer.
9 Evaluate the image and create a comprehensive reasoning question and its answer.
10 Analyze the image and craft an effective reasoning question and its response.
Table 7: The list of instructions for complex reasoning.

--- PAGE 11 ---
No. Instruction
1 Based on the content of the given image, generate a question that requires common sense to answer
and then briefly answer it.
2 Construct a question that draws upon common sense to answer, using the content presented in the
given image, and then briefly answer it.
3 Explain the content of the image in a question and then provide a short answer using knowledge
types such as commonsense and facts.
4 Generate a query that requires reasoning on the information depicted in the image, utilizing a variety
of knowledge types like commonsense, and then offer a concise answer.
5 Develop a query to demonstrate the knowledge types such as commonsense and facts related to the
given image and then provide a brief answer.
6 Based on knowledge types such as commonsense and facts, come up with a query related to the
given image and then briefly answer it.
7 Come up with a question related to the content shown in the image that requires reasoning using a
variety of knowledge types such as commonsense and then succinctly answer it.
8 Brainstorm a question about the content of the given image that requires reasoning with a variety of
knowledge types such as common sense and then state the answer briefly.
9 Construct a query that requires logic based on the contents of the given image and involves a variety
of knowledge types such as commonsense, and then deliver a brief response.
10 Invent an inquiry derived from the pictured material that calls for the use of different knowledge
types like commonsense and subsequently summarize the solution with brevity.
Table 8: The list of instructions for outside knowledge-based visual question answering.
Q: Explain the visual content of the image in great detail. A: The image features ‚Ä¶In addition to the bus and bicycle, there is a fire hydrant visible on the right side of the image, adding to the urban setting. Q: Can you describe the main features of this image for me? A: The image features a gray cat sitting comfortably on a laptop computer‚Äòs keyboard. ‚Ä¶The desk is adorned with various items, including a computer mouse placed towards the right side of the laptopQ: Can you describe the main features of this image for me? A: The image captures a bustlingcityscene with a tall clock tower standing prominently in the background. The tower is surrounded by palm trees, adding a touch of beauty to the cityscape. There are several peoplescattered throughout the area, some walking and others on bicycles. In total, there are at least 12 people visible in the image, with some closer to the clock tower and others further away. In addition to the people and bicycles, there are two handbagsvisible in the scene, likely belonging to some of the pedestrians or cyclists. 
abc
Figure 5: Visualization of hallucination phenomena in VIGC failure cases: (a) model limitations, (b) training data bias, and (c)
Information decay in sequence generation.
Model H. Count 1st 50% 2nd 50% H. Word
VIG 66 0 66 93
VIC 10 1 9 12
Table 9: Hallucination statistics for VIG and VIC models. H. Count: Hallucination Count, 1st 50%: Hallucinations in the first
50% of sentences, 2nd 50%: Hallucinations in the second 50% of sentences, H. Words: Hallucinated Words
B. Hallucination analysis for the VIGC model.
The ‚Äúhallucination‚Äù phenomenon in multimodal models refers to the creation of non-existent objects or details in the output.
We have identified three primary causes:
‚Ä¢Model limitations : Even the most advanced visual models today cannot capture all details in images and are prone to
misidentifications (see Fig. 5-a).

--- PAGE 12 ---
‚Ä¢Training data bias : During model training, certain types of objects or scenes are more prevalent than others. Additionally,
there is co-occurrence in the data, meaning that certain targets are more likely to appear in the same description/response
(see Fig. 5-b).
‚Ä¢Information decay in sequence generation : In detailed description tasks, the model‚Äôs output may neglect the initial image
information over time, relying more on self-generated language context and creating ‚Äùhallucinations‚Äù(see Fig. 5-c and Tab. 9-
2nd 50%).
Our VIG sub-module is designed to function as a self-questioning and answering model, distinguishing it from models that
merely answer questions. It addresses the hallucination issue, particularly the third point, through iterative updates of the input
image and text. This strategy helps prevent information decay and the fitting of common language patterns. As evidenced
in Table 9, tests conducted on 100 images from the evaluation set and synthetic data demonstrate a significant reduction in
hallucination following VIC correction, decreasing from 66% in VIG to 10% in VIC.
C. Data Statistical Analysis
Based on VIGC, we generate fine-tuning data for dialogue, detailed description, and complex reasoning tasks using both COCO
and Objects365 images. Each image and its corresponding question-answer pair are treated as an instance. The sample data, the
average length of questions and answers, and the diversity of questions are presented in Table 10.
Question Diversity. Figure 6 visualizes the frequency of questions beginning with different words, illustrating the diversity
of questions. Many dialogue questions begin with phrases like ‚Äùwhat is‚Äù or ‚Äùwhat color‚Äù, whereas complex reasoning questions
often start with phrases such as ‚Äùwhat might be‚Äù or ‚Äùwhat can be‚Äù. This is because complex reasoning questions typically place
more emphasis on the reasons behind phenomena.
In Table 10, we quantify diversity using the average cosine distance between questions in the dataset. Unlike A-OKVQA
(Schwenk et al. 2022), we remove punctuation from the questions before using the sentence transformer (Jain 2022), and then
calculate the average cosine distance among all pairs in the dataset.
Dataset unique instance Avg. length Mean Q distance
(Q/A)
Conversation (LLaV A) 256 .9k 10.2/47.4 0.845
Conversation (COCO) 36.8k 8.4/10.4 0.721
Conversation (Objects365) 1.7M 8.5/13.0 0.676
Detail Description (LLaV A) 23k 8.0/104.9 -
Detail Description (COCO) 36.6k 9.8/157.3 -
Detail Description (Objects365) 1.7M 10.0/176.6 -
Complex Reasoning (LLaV A) 76.6k 12.8/117.8 0.845
Complex Reasoning (COCO) 36.8k 11.9/121.8 0.830
Complex Reasoning (Objects365) 1.7M 11.6/116.3 0.810
Table 10: Statistical analysis of LLaV A data generated by VIGC

--- PAGE 13 ---
Objects365 Conv (VIGC) Objects365 Complex (VIGC)COCO Conv (VIGC)
 COCO Complex (VIGC)LLaV A Complex (VIGC) LLaV A Conv (VIGC)
Figure 6: Distribution of question words in generated conversation and complex reasoning data on LLaV A, COCO, and Ob-
jects365.
D. Data Visualization

--- PAGE 14 ---
Answer (VIG): 
There are twopeople and twohorses visible in the 
image.
Answer (VIC): 
There are twopeople and several horses visible in the image.
Answer (VIG): 
The image depicts a group of people and horses walking through a shallow river. There are 
several horses in the scene, with some of them being black and white, giving the impression of 
a black and white photo. A man is leading one of the horses, guiding it through the water.
In total, there are eight people and five horses visible in the image. The people are spread out 
across the scene, with some standing closer to the left side , others in the middle, and a few 
more on the right side. The horses are positioned in various locations within the river, with one 
horse near the left edge , two horses in the middle, and another horse further to the right. 
Overall, the scene conveys a sense of camaraderie as the people and horses navigate the body 
of water together.Answer (VIC): 
The image depicts a group of people and horses walking through a shallow river. There are 
several people and horses in the scene, with some of them holding ropes to guide the horses 
through the water. The black and white color scheme of the image adds a sense of 
timelessness or nostalgia to the scene. The people and horses appear to be engaged in a 
leisurely activity, enjoying the water and each other's company.
Answer (VIG): 
The purpose of this gathering of people and horses in the river could be for a variety of reasons. 
One possibility is that they are participating in a horse -riding or equestrian event , such as a 
trail ride or a riding competition , where they need to cross a body of water to complete the 
course. Another possibility is that they are engaging in a leisure activity , such as a picnic or a 
family outing, where they have decided to take their horses for a walk in the river to enjoy the 
scenery and spend quality time together. Lastly, it could also be a training session for the 
horses, where they are being taught how to swim or navigate shallow waters, which can be 
beneficial for their overall well -being and safety.Answer (VIC): 
The purpose of this gathering of people and horses in the river could be for a variety of 
reasons. It could be a recreational activity , such as a horse -riding excursion or a leisurely 
walk in the countryside. Alternatively, it could be a part of a training or rehabilitation 
process for the horses, where they are being led through the water to improve their strength, 
coordination, and overall well -being. Additionally, it could be a social event where the 
people and horses are spending time together and enjoying each other's company.Question: Can you describe the main features of this image for me?Question:  How many people and horses are visible in the image?
Question: What might be the purpose of this gathering of people and horses in the river?
Conversation
Detail Description
Complex Reasoning
Answer (VIG): 
The building in the image appears to be in a state of disrepair, 
as it has been partially torn down.
Answer (VIC): 
The building in the image is in a state of disrepair, as evidenced by its damaged roof.
Answer (VIG): 
The image depicts a city street with several traffic lights scattered throughout the scene. A 
man is standing on the rooftop of a building , possibly surveying the area or working on 
repairs. Nearby, there is another person walking on the sidewalk .
Additionally, there are two cars parked on the street , one closer to the left side and the other 
towards the right side of the image . The overall atmosphere suggests a bustling urban 
environment with people going about their daily routines .Answer (VIC): 
The image depicts a city street with several traffic lights scattered throughout the scene. In 
the foreground, there is a man standing on the roof of a building , which appears to be in a 
state of disrepair or undergoing renovation. The presence of the man on the roof suggests 
that he might be working on the building or inspecting its condition. In the background of 
the image, there are several other buildings , some of which appear to be in better condition 
than the building with the man on the roof. The overall atmosphere of the scene is one of 
urban decay or renovation , as evidenced by the dilapidated building and the man's presence 
on the roof.
Furthermore, the traffic lights in the image serve as a reminder of the city's infrastructure 
and its role in regulating traffic flow and ensuring the safety of pedestrians and vehicles. The 
presence of these traffic lights suggests that the area is a bustling urban environment with a 
mix of residential, commercial, and possibly industrial properties.
In conclusion, the image depicts a city street with several traffic lights, a dilapidated 
building undergoing renovation, and a man working on the roof. The overall atmosphere of 
the scene is one of urban decay or renovation , with the traffic lights serving as a reminder of 
the city's infrastructure and its role in regulating traffic flow and ensuring the safety of 
pedestrians and vehicles.
Answer (VIG): 
The man's presence on the rooftop could be due to various reasons. It is possible that he is a 
construction worker , repairing or maintaining the building's roof . Another possibility is that 
he is an urban explorer or a thrill -seeker who has climbed up to the rooftop for a panoramic 
view of the surroundings . Alternatively, the man might be a resident of the building who has 
access to the rooftop for personal reasons, such as enjoying the view or using the space for 
leisure activities. Regardless of the reason, his presence on the rooftop highlights the potential 
risks associated with unauthorized access to rooftops and the importance of following safety 
guidelines and regulations.Answer (VIC): 
The man's presence on the rooftop could be due to various reasons. It is possible that he is a 
maintenance worker , repairing or inspecting the building's roof. Alternatively, he might be a 
construction worker involved in the renovation or demolition of the building. In either case, 
his presence on the rooftop highlights the importance of proper safety measures and 
adherence to regulations when working at elevated heights.Question: Analyze the image in a comprehensive and detailed manner.Question: What is the condition of the building in the image?
Question: What might be the reason for the man's presence on the rooftop?
Conversation
Detail Description
Complex ReasoningFigure 7: Generated instruction examples based on the proposed VIGC.

--- PAGE 15 ---
Q: why is the stop sign so 
close to the street sign?
VIG -A: safety
VIC-A: for visibilit y
Q:what color is the frisbee?
VIG -A: blue
VIC-A: white
Q: what company makes the 
mouse?
VIG -A: logitech
VIC-A: appleQ: how many people are 
eating this sandwich?
VIG -A: two
VIC-A: one
A-OKVQA (VIGC)
Q: what sport is this?
VIG -A: little league
VIC-A: baseballQ: can you guess the speed 
of this train?
VIG -A: 100mph
VIC-A: 300mphQ: how many layers is this 
cake?
VIG -A: 4
VIC-A: 3Q: does the person riding the 
bike have a helmet on?
VIG -A: no
VIC-A: yesOKVQA (VIGC)Figure 8: OKVQA and A-OKVQA QA-pairs generated by VIG and VIC

--- PAGE 16 ---
Knowledge: Visual
Q: what type of material is 
used to make the vases ?
A: crochet
Knowledge: Commonsense
Q: what's the name of the flag 
on the tail of the plane on the 
far right ?
A:swiss
Knowledge: Visual
Q: what video game is this 
character from ?
A:sonic the hedgehog
Knowledge: Visual
Q: what kind of food is this ?
A:asianKnowledge: Commonsense
Q: in what country is this clock 
tower located?
A:EnglandKnowledge: Knowledge base
Q: who invented this type of 
vehicle?
A: wright brothersKnowledge: Commonsense
Q: why is the window open?
A: ventilation
Knowledge: Physical
Q: why is there a shadow on 
the wall?
A: sunlightA-OKVQA (VIGC) A-OKVQA (VIGC)
Knowledge: Brands, 
Companies and Products
Q: which brand of laptop are 
they using? 
A:toshibaKnowledge: Objects, Material 
and Clothing
Q: what material is the roof 
made of?
A: bambooKnowledge: Weather and 
Climate
Q: what season is it?
A: autumnKnowledge: People and 
Everyday Life
Q: how often do you use this 
room? 
A: daily
Knowledge: Vehicles and 
Transportation
Q: which type of vehicle is 
allowed to park in this area?
A: electric
Knowledge: Plants and 
Animals
Q: the animal in this picture is 
known for its what feature?
A: long neck
Knowledge: Cooking and 
Food
Q: are these foods healthy or 
unhealthy?
A: unhealthy
Knowledge: Geography, 
History, Language and Culture
Q: what language is the 
symbol on the laptop?
A: ChineseOKVQA (VIGC) OKVQA (VIGC)
Figure 9: OKVQA and A-OKVQA QA-pairs generated by VIGC
