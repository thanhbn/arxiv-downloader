# 2310.11441.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.11441.pdf
# Kích thước tệp: 39379332 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Prompting Set-of-Mark Mở Ra
Khả Năng Grounding Thị Giác Phi Thường trong GPT-4V
Jianwei Yang1∗‡, Hao Zhang2∗, Feng Li2∗, Xueyan Zou3∗, Chunyuan Li1, Jianfeng Gao1
1Microsoft Research, Redmond2HKUST3University of Wisconsin-Madison
∗Cộng Tác Viên Chính‡Trưởng Dự Án
Sân chơi: https://som-gpt4v.github.io/
Hình 1: So sánh các kỹ thuật prompting GPT-4V: (trái) prompting tiêu chuẩn và (phải) Set-of-Mark Prompting được đề xuất. Chỉ cần phủ lên id trên các vùng hình ảnh là có thể mở ra khả năng grounding thị giác và sửa chữa câu trả lời cho GPT-4V. Lưu ý rằng không có dấu hiệu nào bị rò rỉ ra đầu vào văn bản của người dùng.
Tóm tắt
Chúng tôi trình bày Set-of-Mark (SoM), một phương pháp prompting thị giác mới, để mở ra khả năng grounding thị giác của các mô hình đa phương thức lớn (LMM), như GPT-4V. Như được minh họa trong Hình 1 (phải), chúng tôi sử dụng các mô hình phân đoạn tương tác có sẵn, như SEEM/SAM, để phân chia một hình ảnh thành các vùng ở các mức độ chi tiết khác nhau, và phủ các vùng này với một tập hợp các dấu hiệu ví dụ, chữ số, mặt nạ, hộp. Sử dụng hình ảnh được đánh dấu làm đầu vào, GPT-4V có thể trả lời các câu hỏi yêu cầu grounding thị giác. Chúng tôi thực hiện một nghiên cứu thực nghiệm toàn diện để xác nhận hiệu quả của SoM trên một loạt các tác vụ thị giác và đa phương thức tinh tế. Ví dụ, các thí nghiệm của chúng tôi cho thấy rằng GPT-4V với SoM vượt trội so với mô hình hiểu và phân đoạn biểu thức tham chiếu được fine-tune hoàn toàn tiên tiến nhất trên RefCOCOg trong thiết lập zero-shot. Mã cho SoM prompting được công khai tại đây: https://github.com/microsoft/SoM .arXiv:2310.11441v2  [cs.CV]  6 Nov 2023

--- TRANG 2 ---
1 Giới thiệu
Trong vài năm qua, chúng ta đã chứng kiến sự tiến bộ đáng kể trong các mô hình ngôn ngữ lớn (LLM) [2,3,10,43,60,35]. Đặc biệt, Generative Pre-trained Transformers hay GPT [2,35] đã dẫn đến một số bước đột phá trong ngành công nghiệp và học thuật. Kể từ khi phát hành GPT-4, các mô hình đa phương thức lớn (LMM) đã thu hút sự quan tâm ngày càng tăng trong cộng đồng nghiên cứu. Một số nghiên cứu cố gắng xây dựng GPT-4 đa phương thức dựa trên các mô hình mã nguồn mở [28,63,57,12,47,50,20]. Gần đây, GPT-4V(ision) được phát hành và ngay lập tức thu hút sự chú ý từ cộng đồng vì khả năng nhận thức và lý luận đa phương thức xuất sắc. Tính ưu việt và tổng quát của nó được thể hiện trong [49]. Mặc dù có khả năng hiểu ngôn ngữ-thị giác mạnh mẽ, chưa từng có, khả năng grounding thị giác tinh tế của GPT-4V tương đối yếu, hoặc chưa được mở ra. Ví dụ, GPT-4V khó có thể tạo ra một chuỗi tọa độ chính xác cho một con chó trong hình ảnh1, hoặc một vài đèn giao thông [54].

Nghiên cứu của chúng tôi được thúc đẩy bởi việc giải quyết các trường hợp thất bại của GPT-4V trên các tác vụ thị giác tinh tế, như tham chiếu đối tượng. Về nguyên tắc, các tác vụ này yêu cầu mô hình phải có hiểu biết ngữ nghĩa phong phú và hiểu biết không gian chính xác về nội dung thị giác. Tuy nhiên, việc khuyến khích GPT-4V tạo ra một chuỗi token chứa mô tả văn bản và tọa độ số không chỉ làm tổn hại tính trôi chảy tự nhiên trong LLM mà còn bỏ qua khả năng hiểu không gian trong các mô hình thị giác được sử dụng bởi LMM, như được chỉ ra bởi [7]. Do đó, trong nghiên cứu này, chúng tôi tập trung vào việc cải thiện đầu vào thị giác bằng cách đề xuất một phương pháp prompting thị giác mới để mở ra khả năng grounding thị giác của LMM.

Prompting thị giác đã được khám phá cho các tác vụ thị giác và đa phương thức khác nhau [9,19,65,45,41]. Các nghiên cứu này có thể được phân loại thành hai loại. Một là mã hóa các prompt thị giác như điểm, hộp và nét vẽ thành các đặc trưng tiềm ẩn, sau đó được sử dụng để prompt các mô hình thị giác [65,19]. Loại khác là phủ các dấu hiệu thị giác trực tiếp lên hình ảnh đầu vào. Các dấu hiệu phủ có thể là một vòng tròn đỏ [41], một vùng được tô sáng [48], hoặc một vài vòng tròn có mũi tên [49]. Mặc dù các nghiên cứu này chứng minh tiềm năng của prompting thị giác ở mức pixel, chúng bị hạn chế trong việc tham chiếu thị giác đến một hoặc vài đối tượng. Hơn nữa, tất cả các dấu hiệu này không dễ dàng "nói được" bởi LLM, do đó cản trở khả năng grounding cho GPT-4V.

Trong bài báo này, chúng tôi trình bày một cơ chế prompting mới được gọi là Set-of-Mark (SoM) prompting, tức là chỉ cần thêm một tập hợp các dấu hiệu thị giác trên đầu các vùng hình ảnh. Đầu tiên, chúng tôi phân chia một hình ảnh đầu vào thành một tập hợp các vùng có ý nghĩa ngữ nghĩa. Sau đó, chúng tôi phủ mỗi vùng với một dấu hiệu ở nhiều định dạng khác nhau như số, chữ cái, mặt nạ, hoặc hộp. Chúng tôi thực hiện một nghiên cứu thực nghiệm để xác nhận liệu GPT-4V có thể ground tốt hơn các vùng với những dấu hiệu thị giác có thể diễn giải và "nói được" này. Các thí nghiệm của chúng tôi cho thấy rằng SoM có thể mở ra một cách mạnh mẽ khả năng grounding của GPT-4V. Như được thể hiện trong Hình 1, so với prompting tiêu chuẩn, SoM giúp GPT-4V ground chính xác các câu trả lời của mình trong các vùng tương ứng, ví dụ, laptop bên phải là "9" và đèn là "12". Tương tự, nó cũng có thể liên kết chiếc ghế bên trái với "7". Lưu ý rằng không có thông tin nào về các dấu hiệu bị rò rỉ vào cuộc trò chuyện với GPT-4V, điều này cho thấy mô hình không chỉ có thể hiểu ngữ nghĩa của các dấu hiệu thị giác mà còn biết cách và khi nào liên kết nội dung thị giác với các dấu hiệu. Theo hiểu biết của chúng tôi, đây là nghiên cứu đầu tiên chứng minh rằng khả năng grounding thị giác mới nổi của GPT-4V có thể được mở ra bởi prompting thị giác, SoM. Để tóm tắt, các đóng góp chính của chúng tôi là:

• Chúng tôi đề xuất một kỹ thuật prompting thị giác đơn giản nhưng hiệu quả, được gọi là Set-of-Mark (SoM) prompting cho GPT-4V, và cho thấy một cách thực nghiệm rằng SoM hiệu quả mở ra khả năng grounding thị giác phi thường của GPT-4V.

• Chúng tôi đã phát triển một bộ benchmark đánh giá mới để kiểm tra khả năng grounding của GPT-4V và các LMM khác. Đối với mỗi hình ảnh trong tập dữ liệu, chúng tôi sử dụng các mô hình phân đoạn có sẵn để phân đoạn một hình ảnh thành các vùng, và phủ mỗi vùng với các dấu hiệu thị giác, như số.

• Chúng tôi đã tiến hành phân tích định lượng và định tính để xác nhận hiệu quả của SoM trên một loạt rộng các tác vụ thị giác. Các thí nghiệm của chúng tôi cho thấy rằng SoM cải thiện đáng kể hiệu suất của GPT-4V trên các tác vụ thị giác phức tạp yêu cầu grounding. Ví dụ, GPT-4V với SoM vượt trội so với mô hình phân đoạn tham chiếu được fine-tune hoàn toàn tiên tiến nhất trên RefCOCOg trong thiết lập zero-shot.

1https://blog.roboflow.com/gpt-4-vision/
2

--- TRANG 3 ---
Hình 2: So sánh GPT-4V tiêu chuẩn và sự kết hợp của nó với Set-of-Mark (SoM) Prompting. Nó cho thấy rõ ràng rằng phương pháp prompting được đề xuất của chúng tôi giúp GPT-4V nhìn chính xác hơn và cuối cùng đưa ra câu trả lời đúng. Chúng tôi làm nổi bật sự khác biệt giữa phương pháp của chúng tôi và phương pháp tiêu chuẩn. (Hình ảnh được tạo bởi Dalle-3 và tốt hơn khi xem màu.)

2 Set-of-Mark Prompting
Phần này giới thiệu Set-of-Mark Prompting và giải thích cách nó có thể được áp dụng để prompt các LMM, đặc biệt là GPT-4V.

2.1 Định nghĩa Vấn đề
Thông thường, LMM F nhận một hình ảnh I∈ RH×W×3 và một truy vấn văn bản có độ dài li, Ti = [ti1, ..., tili] làm đầu vào, và tạo ra một chuỗi đầu ra văn bản có độ dài lo, To = [to1, ..., tolo], được công thức hóa như sau:

To = F(I, Ti) (1)

Với tính đa năng của các LLM hiện tại được sử dụng trong LMM, văn bản đầu vào và đầu ra có thể bao gồm các loại ký tự khác nhau (ví dụ, cả chữ cái và số), và cũng đa ngôn ngữ. Một số lượng lớn các nghiên cứu đã cố gắng cải thiện kỹ thuật prompt trên văn bản đầu vào Ti để trang bị thêm khả năng lý luận trong LLM [46, 52].

Trái ngược với kỹ thuật prompt cho LLM, mục tiêu của nghiên cứu này là phát triển một phương pháp prompting mới cho hình ảnh đầu vào để mở ra khả năng grounding thị giác của LMM. Nói cách khác, chúng tôi cố gắng trang bị cho LMM khả năng nhìn từng vị trí một. Điều này đòi hỏi hai đặc tính thiết yếu cho chiến lược prompting:

• Phương pháp này phải có khả năng phân chia một hình ảnh thành một tập hợp các vùng có ý nghĩa ngữ nghĩa để phù hợp với đầu ra văn bản, một khả năng được gọi là grounding.

• Thông tin phụ trợ được đưa vào hình ảnh đầu vào phải có thể diễn giải và nói được bởi LMM, để nó có thể được mô tả trong đầu ra văn bản của mình.

Với suy nghĩ này, chúng tôi phát triển Set-of-Mark Prompting, một phương pháp prompting đơn giản là phủ một số dấu hiệu lên các vùng có ý nghĩa trong hình ảnh. Thao tác này tăng cường hình ảnh đầu vào I thành một hình ảnh được đánh dấu Im trong khi giữ nguyên các đầu vào khác cho LMM, như được thể hiện trong Hình 2.

Về mặt toán học, Phương trình (1) trở thành

To = F(SoM(I)|{z}Im, Ti). (2)

Mặc dù việc áp dụng SoM cho tất cả LMM là đơn giản, chúng tôi thấy rằng không phải tất cả LMM đều có khả năng "nói ra" về các dấu hiệu. Thực tế, chúng tôi thấy rằng chỉ có GPT-4V, khi được trang bị SoM, mới thể hiện khả năng grounding mới nổi và vượt trội đáng kể so với các LMM khác. Dưới đây, chúng tôi sẽ giải thích cách chúng tôi phân chia hình ảnh thành các vùng và đánh dấu các vùng hình ảnh trong SoM.

2.2 Phân chia Hình ảnh
Với một hình ảnh đầu vào, chúng tôi cần trích xuất các vùng có ý nghĩa và phù hợp về mặt ngữ nghĩa. Lý tưởng nhất, việc trích xuất nên tự động hoặc bán tự động để tránh gánh nặng thêm cho người dùng. Để đạt được điều này, chúng tôi sử dụng một bộ công cụ phân đoạn hình ảnh. Để hỗ trợ các trường hợp sử dụng khác nhau, các công cụ phân đoạn cần có các đặc tính sau:

3

--- TRANG 4 ---
Hình 3: Chúng tôi tổng hợp các mô hình phân đoạn khác nhau bao gồm SEEM [65], Semantic-SAM [21] và SAM [19] làm công cụ phân chia hình ảnh được sử dụng trong nghiên cứu của chúng tôi. Người dùng có thể chọn mức độ chi tiết nào của mặt nạ để tạo ra, và chế độ nào để sử dụng giữa tự động (trên) và tương tác (dưới). Giá trị alpha blending cao hơn (0.4) được sử dụng để trực quan hóa tốt hơn.

• Hiệu suất mạnh mẽ: việc phân chia vùng phải đủ chính xác để truyền đạt thông tin bố cục không gian tinh tế cho GPT-4V. Theo nghĩa này, chúng tôi chọn một trong những mô hình phân đoạn hình ảnh tiên tiến nhất MaskDINO [24].

• Từ vựng mở: các mô hình phân đoạn phải có từ vựng mở để chúng có thể nhận dạng các đối tượng ngoài từ vựng được định nghĩa trước. Do đó, chúng tôi chọn các mô hình tiên tiến như SEEM [65].

• Độ chi tiết phong phú: Cuối cùng, vùng quan tâm có thể không chỉ là một đối tượng đầy đủ mà còn là một phần của nó. Do đó, chúng tôi cũng sử dụng SAM [19] và Semantic-SAM [21].

Chúng tôi đã thu thập một bộ mô hình phân đoạn hình ảnh cung cấp một hộp công cụ toàn diện cho người dùng phân chia hình ảnh tùy ý. Hầu hết các mô hình đều tương tác và có thể được promote để người dùng có thể tùy chỉnh các prompt SoM một cách tương tác.

Dựa trên bộ công cụ phân chia hình ảnh của chúng tôi, chúng tôi chia một hình ảnh đầu vào I có kích thước H×W thành K vùng R = [r1, ..., rK] ∈ {0,1}K×H×W, được biểu diễn bởi K mặt nạ nhị phân. Chúng tôi hiển thị một số ví dụ SoM trong Hình 3 được tạo bằng hộp công cụ của chúng tôi.

2.3 Tạo Set-of-Mark
Khi chúng tôi có được phân chia hình ảnh M, chúng tôi cần tạo cho mỗi vùng một dấu hiệu hữu ích cho việc grounding bởi GPT-4V. Chúng tôi xem xét hai yếu tố, loại và vị trí của dấu hiệu.

Loại Dấu hiệu. Như chúng tôi đã thảo luận trước đó, loại dấu hiệu phụ thuộc vào việc chúng có thể được diễn giải bởi GPT-4V hay không. Trong nghiên cứu này, chúng tôi xem xét chữ số vì nó không chỉ đủ nhỏ gọn để không chiếm nhiều không gian hình ảnh mà còn có thể nhận dạng được bởi GPT-4V (sử dụng khả năng OCR của nó). Bên cạnh đó, chúng tôi cũng xem xét các hộp và ranh giới mặt nạ như các dấu hiệu phụ trợ. Ngoài ra, chúng tôi lưu ý rằng các loại dấu hiệu phải phụ thuộc vào hình ảnh để tránh bất kỳ xung đột nào với nội dung hình ảnh gốc. Ví dụ, với một hình ảnh số học đầy số, các dấu hiệu số nên được tránh, trong khi các ký tự chữ cái không nên được sử dụng cho ảnh chụp màn hình của một tài liệu. Chúng tôi để lại việc tự động xác định loại dấu hiệu nào cần sử dụng và cách kết hợp chúng cho nghiên cứu trong tương lai. Hình 4 hiển thị các loại dấu hiệu khác nhau và đầu ra được diễn giải từ GPT-4V.

Vị trí Dấu hiệu. Với một hình ảnh có kích thước H×W, chúng tôi cần tìm các vị trí dấu hiệu tốt cho tất cả K vùng. Một cách đơn giản là tính toán tọa độ trung tâm ck = (xck, yck) của mặt nạ thứ k, và phủ các dấu hiệu ở đó. Tuy nhiên, điều này không tránh khỏi việc tạo ra một số chồng chéo hoặc xung đột có thể gây nhầm lẫn cho GPT-4V, đặc biệt đối với hình ảnh có đối tượng dày đặc, ví dụ, hai đối tượng có trung tâm xung quanh các vị trí tương tự hoặc các vùng lõm, như được thể hiện trong Hình 5.

Để giảm thiểu vấn đề này, chúng tôi đề xuất một thuật toán phân bổ dấu hiệu như được minh họa trong thuật toán ở phía bên phải của Hình 5. Với tập hợp các mặt nạ M, đầu tiên chúng tôi tính toán diện tích cho tất cả và sắp xếp

4

--- TRANG 5 ---
Hình 4: Các loại dấu hiệu khác nhau có thể được sử dụng trong Set-of-Mark của chúng tôi.

// Tìm trung tâm cho một vùng
def Find_Center(r)
D=DT(r)// Chạy distance transform
c= arg max(D)// Tìm vị trí cực đại
return c

// Hàm chính
def Mark_Allocation(R):
R̂=Sorted(R)// Sắp xếp các vùng theo thứ tự
tăng dần diện tích
for k in range(K):do
rk=R̂[k] & ¬R̂[:k−1].sum(0)// Loại trừ
k−1 vùng
C[k]=Find_Center(rk)
end
return C

Hình 5: Trái: một số xung đột do đặt tất cả dấu hiệu ở trung tâm. Phải: thuật toán phân bổ dấu hiệu được đề xuất của chúng tôi để giải quyết các xung đột.

chúng theo thứ tự tăng dần (dòng 6). Chiến lược này đảm bảo rằng các vùng nhỏ hơn được xem xét trước các vùng lớn. Để tránh thêm các chồng chéo tiềm ẩn, đối với mặt nạ thứ k, chúng tôi loại trừ vùng được bao phủ bởi bất kỳ k−1 mặt nạ nào (dòng 8). Mặt nạ kết quả sau đó được đưa vào thuật toán distance transform, giúp tìm vị trí bên trong mặt nạ nơi khoảng cách tối thiểu đến tất cả các điểm ranh giới là tối đa. Tuy nhiên, trong thực tế, một vùng có thể quá nhỏ đến mức dấu hiệu có thể che (gần như) toàn bộ vùng. Trong trường hợp này, chúng tôi di chuyển các dấu hiệu ra khỏi vùng một chút. Chúng tôi thấy rằng GPT-4V vẫn có thể xây dựng một liên kết tốt giữa các dấu hiệu và vùng.

Khi chúng tôi xác định loại dấu hiệu và vị trí cho tất cả các vùng, chúng tôi phủ dấu hiệu mk lên vùng rk tại vị trí ck. Chúng tôi đảm bảo rằng mỗi dấu hiệu là duy nhất để tập hợp đầy đủ M={m1, ...mk} có thể phân biệt và nói được bởi LLM.

2.4 Interleaved Prompt
Cho đến nay, chúng tôi đã có được một hình ảnh mới Im với các dấu hiệu được phủ lên. Ngoài ra, chúng tôi có một tập hợp K cặp vùng-dấu hiệu {⟨r1, m1⟩, ..., ⟨rK, mK⟩}. Với các gợi ý bổ sung trong Im, chúng tôi có thể sử dụng prompt văn bản đơn giản hoặc prompt xen kẽ để prompt LMM:

• Plain Text Prompt. Chúng tôi có thể sử dụng prompt văn bản đơn giản như thường lệ mà không có bất kỳ tham chiếu rõ ràng nào đến các dấu hiệu/vùng trong hình ảnh. Như chúng tôi đã thể hiện trong Hình 1, ngay cả khi không có bất kỳ prompt văn bản đặc biệt nào, GPT-4V có thể tự động ground mình trong các vùng và dấu hiệu tương ứng. Nó có thể được sử dụng trong một loạt các tình huống khi người dùng không có các vùng quan tâm cụ thể.

• Interleaved Text Prompt. Chúng tôi có thể sử dụng prompt xen kẽ bằng cách tiêm các dấu hiệu vào văn bản trực tiếp. Vì các dấu hiệu có thể diễn giải được cho LMM, chúng tôi có thể pha trộn chúng một cách liền mạch vào prompt văn bản gốc để tạo ra một tham chiếu tượng trưng.

Các ví dụ về việc áp dụng hai loại prompt văn bản này dựa trên SoM được trình bày trong Hình 6. Lưu ý rằng đối với mỗi câu hỏi, chúng tôi sử dụng một cửa sổ chat mới để tránh rò rỉ ngữ cảnh trong cuộc trò chuyện. Trong các tình huống thực tế, hai chiến lược prompt văn bản trên có thể được kết hợp và sử dụng trong các cuộc trò chuyện nhiều vòng với GPT-4V để làm phong phú đáng kể tương tác người-AI. Hơn nữa, người dùng cũng có thể chọn tự vẽ các dấu hiệu hoặc chỉnh sửa các dấu hiệu được tạo bằng hộp công cụ.

5

--- TRANG 6 ---
Prompt Đơn giản và Xen kẽ Hỗ trợ SoM

Người dùng: Có gì trong hình ảnh?
GPT-4V: Trong hình ảnh, tôi quan sát thấy các mục sau:
Bánh mì hoặc Bánh (1): Một món nướng hình chữ nhật, có thể là bánh mì hoặc bánh ngọt.
Hỗn hợp Trái cây Kem (2): Một bát chứa hỗn hợp kem với nhiều loại quả mọng và có thể là các loại trái cây khác. Đây có thể là salad trái cây với nước sốt kem, sữa chua với trái cây, hoặc một loại tráng miệng nào đó.
Trái cây Thái lát (3): Một đĩa với những lát trái cây khác nhau, bao gồm cái trông giống dứa và có thể là lát dưa hấu hoặc loại khác.
Mặt bàn (4): Một phần của bàn hoặc mặt bàn bếp có màu xanh lam. (tiếp tục) Ngoài ra, còn có nhiều yếu tố trang trí, đĩa và bát khác nhau.

Người dùng: Có gì trong 3?
GPT-4V: Trong hình ảnh, mục được gắn nhãn "3" có vẻ là một đĩa trái cây thái lát. Trông giống như có dứa và có thể là loại dưa khác, như dưa hấu hoặc bưởi hồng.

Người dùng: Tôi có thể cho trái cây ở 3 vào 2 không?
GPT-4V: Có, dựa trên hình ảnh, trái cây trong 3 có vẻ là trái cây tươi thái lát, và chúng có thể được trộn hoặc đổ vào hỗn hợp trái cây kem ở 2 để tăng hương vị và kết cấu. Đây là một thực hành phổ biến trong món tráng miệng để kết hợp trái cây tươi với hỗn hợp kem.

Người dùng: Cái nào có thể nướng được, 1 hay 2?
GPT-4V: Dựa trên hình ảnh, mục 1 (trông giống như bánh mì hoặc bánh ngọt) là thứ có thể nướng được. Mục 2 là hỗn hợp trái cây kem và không thích hợp để nướng.

Người dùng: Thường mất bao lâu để nướng một lát bánh mì ở 1?
GPT-4V: Thông thường, để nướng một lát bánh mì như cái được gắn nhãn "1" trong hình ảnh, sẽ mất khoảng 2 đến 5 phút trong máy nướng bánh mì ở mức trung bình.

Hình 6: Chúng tôi có thể sử dụng hai loại prompt văn bản cho LMM: Trái hiển thị prompt văn bản đơn giản, không bao gồm bất kỳ manh mối nào về các dấu hiệu; Phải là prompt văn bản xen kẽ, sử dụng các dấu hiệu để ground các vùng hình ảnh.

3 SoM Prompting cho Thị giác
Chúng tôi nhấn mạnh rằng ưu điểm độc đáo của việc sử dụng SoM để prompt GPT-4V là nó có thể tạo ra đầu ra vượt quá văn bản. Vì mỗi dấu hiệu được liên kết độc quyền với một vùng hình ảnh được đại diện bởi một mặt nạ, chúng tôi có thể truy ngược lại các mặt nạ cho bất kỳ dấu hiệu nào được đề cập trong đầu ra văn bản. Xem xét ví dụ trong Hình 7, phản hồi GPT-4V ở phía bên trái chứa tên và chi tiết của bốn vùng. Nó có thể tạo ra ánh xạ một-một giữa các dấu hiệu và mô tả văn bản, tức là, mk↔textk. Với rk↔mk, chúng tôi có thể liên kết thêm mô tả văn bản với các mặt nạ cho tất cả các vùng. Cuối cùng, chúng tôi có thể kết nối các bộ ba ⟨rk, mk, textk⟩ cho tất cả các vùng, tức là, rk↔mk↔textk. Khả năng tạo ra văn bản và mặt nạ được ghép nối cho phép GPT-4V được prompt bởi SoM tạo ra văn bản được ground thị giác hợp lý và quan trọng hơn là hỗ trợ nhiều tác vụ thị giác tinh tế, vốn là thách thức đối với mô hình GPT-4V thông thường.

Các Tác vụ Thị giác. Chúng tôi kiểm tra định lượng hiệu suất của GPT-4V được prompt bởi SoM. Với kỹ thuật prompt đơn giản, nó có thể được sử dụng ngay trong, nhưng không giới hạn ở, một loạt rộng các tác vụ thị giác như:

• Phân đoạn Hình ảnh Từ vựng Mở: Chúng tôi yêu cầu GPT-4V nói đầy đủ các loại cho tất cả các vùng được đánh dấu và các loại được chọn từ một nhóm được xác định trước.

• Phân đoạn Tham chiếu: Với một biểu thức tham chiếu, nhiệm vụ của GPT-4V là chọn vùng phù hợp nhất từ các ứng cử viên được tạo bởi hộp công cụ phân chia hình ảnh của chúng tôi.

6

--- TRANG 7 ---
Phân đoạn Từ vựng Mở
Người dùng: Tôi đã gắn nhãn một ID số sáng ở trung tâm cho mỗi đối tượng thị giác trong hình ảnh. Vui lòng liệt kê tên của chúng. Bạn phải trả lời bằng cách chọn từ các tên sau: [Person, ..., curtain] (bỏ qua 80 loại trong COCO)
GPT-4V: 1. Person 2. Person 3. Person 4. Surfboard 5. Handbag

Phân đoạn Tham chiếu
Người dùng: Tôi đã gắn nhãn một ID số sáng ở trung tâm cho mỗi đối tượng thị giác trong hình ảnh. Vui lòng cho tôi biết ID cho: Laptop phía sau chai bia; Laptop đang bật.
GPT-4V: ID cho các mục bạn đã đề cập: Laptop phía sau chai bia: 6 Laptop đang bật: 2

Phân đoạn Đối tượng Video
Người dùng: 2 hình ảnh này từ cùng một video, trong đó hình ảnh đầu tiên là khung hình đầu tiên và hình ảnh thứ hai là khung hình sau. Trong hình ảnh đầu tiên, có 2 đối tượng được gắn nhãn 1,2. Bạn có thể theo dõi 2 đối tượng này trong hình ảnh thứ hai không?
GPT-4V: 1. Đối tượng được gắn nhãn 1 (một võ sĩ quyền anh mang mũ bảo hiểm đỏ) tương tự nhất với đối tượng được gắn nhãn 2 (một võ sĩ quyền anh khác mang mũ bảo hiểm đỏ). 2. Đối tượng được gắn nhãn 2 (một võ sĩ quyền anh mang đồ xanh) tương tự nhất với đối tượng được gắn nhãn 1 (một võ sĩ quyền anh khác mang đồ xanh).

Phân đoạn Grounded
Người dùng: Tôi đã gắn nhãn một ID số sáng ở trung tâm cho mỗi đối tượng thị giác trong hình ảnh. Với hình ảnh hiển thị một người đàn ông đeo kính cầm một tờ giấy, hãy tìm các vùng tương ứng cho một người đàn ông đeo kính, một tờ giấy.
GPT-4V: "Người đàn ông đeo kính" tương ứng với vùng được gắn nhãn "2". "Tờ giấy" tương ứng với vùng được gắn nhãn "5".

Hình 7: Ví dụ về một số tác vụ thị giác chúng tôi đánh giá trên GPT-4V.

• Grounding Cụm từ: Hơi khác so với phân đoạn tham chiếu, grounding cụm từ sử dụng một câu hoàn chỉnh bao gồm nhiều cụm danh từ. Chúng tôi yêu cầu GPT-4V phân bổ các vùng tương ứng cho tất cả các cụm từ được gắn nhãn.

• Phân đoạn Đối tượng Video: Nó nhận hai hình ảnh làm đầu vào. Hình ảnh đầu tiên là hình ảnh truy vấn chứa một vài đối tượng quan tâm để xác định trong hình ảnh thứ hai. Với việc GPT-4V hỗ trợ nhiều hình ảnh làm đầu vào, phương pháp prompting của chúng tôi cũng có thể được áp dụng để ground các đối tượng thị giác qua các khung hình trong video.

Chúng tôi thể hiện cách chúng tôi prompt GPT-4V bằng SoM cho các tác vụ thị giác trên trong Hình 7. Đáng chú ý là SoM có thể được áp dụng cho các tác vụ rộng lớn hơn, như mô tả vùng, định vị keypoint, và phân đoạn phần, v.v. Do quyền truy cập hạn chế (quota) đến GPT-4V, chúng tôi tập trung vào các tác vụ đã đề cập

7

--- TRANG 8 ---
Tác vụ Thị giác | Tập dữ liệu | #Hình ảnh | #Thực thể | Dấu hiệu | Thước đo
Phân đoạn Chung | COCO [26] | 100 | 567 | Số & Mặt nạ | Precision
Phân đoạn OV | ADE20K [62] | 100 | 488 | Số & Mặt nạ | Precision
Grounding Cụm từ | Flickr30K [38] | 100 | 274 | Số & Hộp | Recall@1
Phân đoạn Biểu thức Tham chiếu | RefCOCO [18] | 100 | 177 | Số & Mặt nạ | mIoU
Hiểu Biểu thức Tham chiếu | RefCOCO [18] | 100 | 177 | Số & Mặt nạ | ACC@0.5
Phân đoạn Đối tượng Video | DAVIS [37] | 71 | 157 | Số & Mặt nạ | J&F

Bảng 1: Benchmark được sử dụng để đánh giá khả năng grounding thị giác của GPT-4V. Lưu ý rằng chúng tôi chọn một tập con nhỏ hình ảnh hoặc video từ các tập dữ liệu tương ứng.

trước và để lại việc khám phá các tác vụ khác cho nghiên cứu trong tương lai. Chúng tôi trình bày nghiên cứu thực nghiệm chi tiết trong phần tiếp theo.

4 Thí nghiệm

4.1 Thiết lập Thí nghiệm

Triển khai. Chúng tôi không cần huấn luyện bất kỳ mô hình nào cho phương pháp của chúng tôi. Tuy nhiên, do quota hạn chế và không có API GPT-4V, chúng tôi phải gửi toàn bộ hình ảnh tăng cường SoM đến giao diện ChatGPT. Các tác giả trong nghiên cứu này đã sử dụng chiến lược chia-và-trị để chạy thí nghiệm và đánh giá. Đối với mỗi thực thể, chúng tôi sử dụng một cửa sổ chat mới để không có rò rỉ ngữ cảnh trong quá trình đánh giá. Theo nghĩa này, tất cả các kết quả được báo cáo cho phương pháp của chúng tôi đều là zero-shot.

Trừ khi được đề cập khác, chúng tôi luôn sử dụng số làm dấu hiệu để báo cáo kết quả chính. Việc sử dụng các loại dấu hiệu khác được thể hiện trong nghiên cứu ablation và định tính của chúng tôi.

Benchmark. Với quota hạn chế của GPT-4V, hiện tại không thể đánh giá đầy đủ tập validation trong mỗi tác vụ thị giác riêng lẻ như chúng tôi đã liệt kê ở trên. Do đó, chúng tôi chọn một tập con nhỏ dữ liệu validation từ mỗi tập dữ liệu cho nghiên cứu của chúng tôi. Đối với mỗi hình ảnh trong tập dữ liệu, chúng tôi phủ một tập hợp dấu hiệu trên các vùng được trích xuất bằng hộp công cụ phân chia hình ảnh của chúng tôi. Tùy thuộc vào tác vụ cụ thể, chúng tôi khai thác các công cụ phân chia khác nhau để đề xuất vùng. Trong Bảng 1, chúng tôi liệt kê thiết lập cho mỗi tác vụ.

So sánh. Chúng tôi so sánh phương pháp của chúng tôi với:

• Baseline GPT-4V dự đoán tọa độ. Chúng tôi sử dụng đây làm mô hình baseline. Theo mặc định, GPT-4V không thể dự đoán hộp cũng không thể tạo mặt nạ. Theo thực hành trước đây, chúng tôi prompt mô hình để dự đoán tọa độ hộp. So sánh với baseline GPT-4V mặc định giúp kiểm tra lợi ích của Set-of-Mark Prompting được đề xuất của chúng tôi.

• Các mô hình chuyên gia tiên tiến nhất. Đối với mỗi tác vụ thị giác, một số phương pháp đã được đề xuất. Chúng tôi chọn các phương pháp tiên tiến nhất và đại diện để so sánh. Cụ thể, MaskDINO [24] cho phân đoạn chung trên COCO, OpenSeeD [55] cho phân đoạn zeroshot trên ADE20K, Grounding DINO [31] và GLIPv2 [56] cho grounding cụm từ trên Flick30K, Grounding DINO và PolyFormer [29] cho hiểu biểu thức tham chiếu, PolyFormer và SEEM [65] cho phân đoạn biểu thức tham chiếu và SegGPT [45] cho phân đoạn đối tượng video. Chúng tôi cố gắng làm rõ khoảng cách giữa mô hình thị giác tổng quát mạnh nhất GPT-4V và các mô hình chuyên gia được thiết kế tinh vi và huấn luyện với dữ liệu cụ thể cho tác vụ.

• LMM mã nguồn mở. Chúng tôi đánh giá định lượng một trong những LMM mã nguồn mở tiên tiến nhất, LLaVa-1.5 [27], và so sánh định tính với MiniGPT-v2 [5] trong nghiên cứu của chúng tôi. Lưu ý rằng cả hai mô hình đều được huấn luyện với một lượng dữ liệu tốt từ các tác vụ thị giác mục tiêu. Chúng tôi là người đầu tiên so sánh LMM đóng và mở nguồn trên benchmark thị giác.

4.2 Kết quả Định lượng

Chúng tôi trình bày kết quả thí nghiệm trong Bảng 2.

Phân đoạn Hình ảnh Trên các tác vụ phân đoạn hình ảnh, chúng tôi đánh giá precision phân loại của GPT-4V + SoM và so sánh với mô hình phân đoạn mạnh MaskDINO trên tập dữ liệu phân đoạn Panoptic COCO và OpenSeeD trên tập dữ liệu phân đoạn Panoptic ADE20K. Đối với MaskDINO

8

--- TRANG 9 ---
Mô hình | LMM | Zero-shot | OV Segmentation | RefCOCOg | Phrase Grounding | VOS
 | | | COCO | ADE20K | REC | RES | Flickr30K | DAVIS2017
MaskDINO [23] | ✗ | ✗ | 80.7 | - | n/a | n/a | n/a | n/a
OpenSeeD [55] | ✗ | ✓ | - | 23.4 | n/a | n/a | n/a | n/a
GLIPv2 [56] | ✗ | ✗ | - | - | - | - | 87.7∗ | n/a
GDINO [31] | ✗ | ✗ | n/a | n/a | 86.1∗ | n/a | 90.5 | n/a
X-Decoder [64] | ✗ | ✗ | - | - | - | 64.6∗ | n/a | 62.8
PolyFormer [29] | ✗ | ✗ | n/a | n/a | 85.8∗ | 67.2 | n/a | n/a
SegGPT [45] | ✗ | ✓ | n/a | n/a | n/a | n/a | n/a | 75.6
SEEM [65] | ✗ | ✗ | - | - | - | 65.7∗ | - | 62.8
RedCircle [41] | ✓ | ✓ | n/a | n/a | 59.4∗ | n/a | n/a | n/a
FGVP [48] | ✓ | ✓ | n/a | n/a | 63.3∗ | n/a | n/a | n/a
Shikra [6] | ✓ | ✗ | n/a | n/a | 82.6∗ | n/a | 77.4 | n/a
LLaVA-1.5 [27] | ✓ | ✗ | n/a | n/a | 63.3 | n/a | n/a | n/a
MiniGPT-v2 [5] | ✓ | ✗ | n/a | n/a | 84.4∗ | n/a | n/a | n/a
Ferret [54] | ✓ | ✗ | n/a | n/a | 85.8* | n/a | 81.1 | n/a
GPT-4V [36] | ✓ | ✓ | n/a | n/a | 25.7 | n/a | n/a | n/a
GPT-4V [36] + SoM (Ours) | ✓ | ✓ | 75.7 | 63.4 | 86.4 | 75.6 | 89.2 | 78.8

Bảng 2: Kết quả định lượng chính. ∗ biểu thị rằng số được báo cáo trong các bài báo gốc được đánh giá trên toàn bộ tập dữ liệu validation. Các số khác được đánh giá trên tập dữ liệu validation được lấy mẫu phụ của chúng tôi bị hạn chế bởi giao diện GPT-4V.

Loại Dấu hiệu | Flickr30K (R@1)
Số & Mặt nạ | 84.4
Số & Mặt nạ & Hộp | 89.2

Bảng 3: Hiệu suất trên Flickr30K với các loại dấu hiệu khác nhau trên một tập con dữ liệu của chúng tôi.

Loại Mặt nạ | Refcocog (mIoU)
MaskDINO | 75.6
GT mask | 90.1

Bảng 4: Hiệu suất trên Flickr30K với các loại mặt nạ khác nhau.

và OpenSeeD, chúng tôi cung cấp cho chúng hộp GT và đánh giá lớp đầu ra. Đối với GPT-4V + SoM, chúng tôi phủ mặt nạ GT với alpha=0.4 và 0.2 cho COCO và OpenSeeD tương ứng, và thêm số ID trên mỗi mặt nạ. Chúng tôi cung cấp cho GPT-4V từ vựng của các tập dữ liệu và yêu cầu nó chọn một nhãn lớp cho mỗi vùng. Kết quả cho thấy hiệu suất zero-shot của GPT-4V + SoM gần với hiệu suất của MaskDINO được fine-tune và cao hơn nhiều so với hiệu suất zero-shot của OpenSeeD trên ADE20K. Hiệu suất tương tự trên COCO và ADE20K cho GPT-4V cho thấy khả năng tổng quát hóa mạnh mẽ của nó đến nhiều miền thị giác và ngữ nghĩa.

Tham chiếu Đối với các tác vụ tham chiếu, chúng tôi đánh giá RES và REC trên RefCOCOg. Chúng tôi sử dụng MaskDINO để đề xuất mặt nạ và phủ mặt nạ và số trên hình ảnh. Chúng tôi sử dụng mIoU làm thước đo đánh giá và so sánh với PolyFormer [29] và SEEM [65] chuyên gia tiên tiến nhất. Tương ứng, GPT-4V + SoM vượt trội so với PolyFormer với một khoảng cách lớn. Lưu ý rằng hiệu suất của PolyFormer được đánh giá trên tập dữ liệu của chúng tôi để so sánh công bằng. Đối với REC, chúng tôi chuyển đổi mặt nạ thành hộp cho phương pháp của chúng tôi. Khi prompt GPT-4V để trực tiếp xuất tọa độ, chúng tôi đạt được hiệu suất kém đáng kể (25.7), điều này xác nhận giả thuyết trước đó của chúng tôi. Khi được tăng cường với SoM, GPT-4V vượt qua cả các chuyên gia như Grounding DINO và Polyformer và LMM mã nguồn mở gần đây bao gồm Shikra, LLaVA-1.5, MiniGPT-v2, và Ferret.

Grounding Cụm từ Đối với grounding cụm từ trên Flickr30K, chúng tôi sử dụng Grounding DINO để tạo đề xuất hộp cho mỗi hình ảnh. Chúng tôi sử dụng ngưỡng 0.27 để lọc các hộp dư thừa, sau đó chúng tôi sử dụng SAM để dự đoán một mặt nạ cho mỗi hộp. Chúng tôi vẽ hộp, mặt nạ và số trên hình ảnh. Chúng tôi cung cấp cho GPT-4V một chú thích và các cụm danh từ của chú thích cho mỗi hình ảnh và để GPT-4V ground mỗi cụm danh từ đến một vùng. Hiệu suất zero-shot của chúng tôi có thể so sánh với các mô hình SOTA GLIPv2 và Grounding DINO.

Phân đoạn Đối tượng Video Chúng tôi đánh giá DAVIS2017 [39] cho tác vụ phân đoạn video. Để có được các đề xuất mặt nạ của mỗi khung hình video, chúng tôi sử dụng MaskDINO [24]. Các mặt nạ được dự đoán sau đó được phủ lên các khung hình tương ứng với nhãn số theo SoM của chúng tôi. Chúng tôi theo thiết lập semi-supervised để sử dụng mặt nạ khung hình đầu tiên làm tham chiếu và phân đoạn tất cả các khung hình khác của cùng một video. Vì GPT-4V có thể nhận nhiều hình ảnh, chúng tôi prompt GPT-4V với khung hình đầu tiên và khung hình hiện tại để thực hiện phân đoạn bằng cách so sánh các đối tượng tương tự qua hình ảnh. Như được thể hiện trong

9

--- TRANG 10 ---
Hình 8: (a, b, c) Ví dụ về một số chú thích sai cho phân đoạn tham chiếu (RefCOCOg) và grounding (Flikr30k). (e, f) Ví dụ về các nhãn chú thích gây nhầm lẫn.

cột cuối của Bảng 2, GPT-4V đạt được hiệu suất theo dõi tốt nhất (78.8 J&F) so với các mô hình thị giác tổng quát khác.

4.3 Nghiên cứu Ablation

Loại dấu hiệu. Chúng tôi nghiên cứu cách lựa chọn loại dấu hiệu ảnh hưởng đến hiệu suất cuối cùng cho tác vụ grounding cụm từ trên Flickr30k. Chúng tôi so sánh hai loại dấu hiệu. Loại thứ nhất là số và mặt nạ và loại thứ hai là số, mặt nạ và hộp. Như được thể hiện trong Bảng 3, phương án thêm hộp có thể cải thiện hiệu suất đáng kể.

Vị trí dấu hiệu vàng. Các vùng được trích xuất từ hộp công cụ của chúng tôi có thể thường có một số lỗi. Trong nghiên cứu này, chúng tôi điều tra cách GPT-4V hoạt động khi chúng tôi tạo các dấu hiệu với chú thích ground-truth. Cụ thể, chúng tôi thay thế mặt nạ phân đoạn được dự đoán bằng mặt nạ ground-truth trong tập validation RefCOCOg của chúng tôi. Điều này có nghĩa là GPT-4V chỉ cần chọn một từ các vùng cho các cụm từ được chú thích. Như mong đợi, hiệu suất cho phân đoạn tham chiếu có thể được cải thiện thêm, đặc biệt là khi mô hình phân đoạn có một số vùng bị thiếu. Kết quả được thể hiện trong Bảng 4, cho thấy việc sử dụng mặt nạ ground-truth trong SoM của chúng tôi cải thiện hiệu suất RefCOCOg thêm 14.5mIoU. Chúng tôi cũng quan sát thấy rằng hầu hết các trường hợp thất bại khi sử dụng mặt nạ GT không phải là vấn đề của GPT-4V, mà chính bản thân chú thích là mơ hồ hoặc không chính xác.

4.4 Quan sát Định tính

Khi chạy qua nghiên cứu này, chúng tôi quan sát thấy một số phát hiện định tính thú vị.

Các chú thích "vàng" không phải lúc nào cũng vàng. Khi đánh giá SoM, chúng tôi thấy rằng một vài chú thích của con người trong tập dữ liệu là không chính xác. Một số ví dụ được thể hiện trong Hình 8 (a,b,c). Ví dụ, người dùng chú thích nhiều người trong khi biểu thức tham chiếu là "chân của một người ngồi trên ghế sofa" hoặc chỉ có một chiếc xe tải được chú thích để tham chiếu "cuộc đua xe tải". Ngược lại, GPT-4V với SoM đưa ra câu trả lời đúng. Điều này ngụ ý rằng các nhãn "vàng" trong tập dữ liệu thị giác có thể có một số nhiễu trong khi GPT-4V có thể là một thẩm phán tốt để giúp tìm ra các chú thích mơ hồ để làm sạch thêm.

Các dấu hiệu trung tâm không phải lúc nào cũng tốt nhất. Theo mặc định, thuật toán phân bổ dấu hiệu của chúng tôi tìm trung tâm cho mỗi vùng. Tuy nhiên, chúng tôi quan sát thấy rằng việc đặt dấu hiệu ở các vị trí trung tâm không nhất thiết mang lại hiệu suất tốt nhất. Ví dụ trong Hình 8 (e), có hai dấu hiệu, một cho người và một cho ba lô. Khi được hỏi cái nào là người, GPT-4V đưa ra dấu hiệu cho ba lô nằm ở phần trên của cơ thể con người. Tình huống tương tự cũng xảy ra trong hình ảnh thứ hai nơi có nền trải khắp toàn bộ hình ảnh. Một ví dụ được thể hiện trong 8 (f), nơi trung tâm của bãi cỏ (được gắn nhãn 2) nằm trên người (được gắn nhãn 1). Các kết quả này cho thấy rằng trọng tâm của GPT-4V để hiểu nội dung thị giác không nhất thiết ở trung tâm. Việc phát triển một thuật toán phân bổ dấu hiệu tốt hơn nên được xem xét.

Lựa chọn động các loại dấu hiệu. Trong các tình huống thực tế, chúng tôi thấy việc xác định động loại dấu hiệu nào cần sử dụng là quan trọng. Ví dụ, một hình ảnh các bài toán số học thường chứa nhiều số. Trong trường hợp này, việc phủ các dấu hiệu số lên sẽ gây nhầm lẫn cho GPT-4V. Tương tự, đối với ảnh chụp màn hình của một tài liệu, có thể không hợp lý khi phủ thêm chữ cái lên hình ảnh. Kết quả là, để tận dụng đầy đủ khả năng thị giác của GPT-4V, người dùng có thể cần thiết kế cẩn thận các prompt SoM trước khi gửi hình ảnh.

10

--- TRANG 11 ---
5 Nghiên cứu Liên quan
Chúng tôi thảo luận về nghiên cứu liên quan từ góc độ prompting.

LLM và Prompting. Chúng ta đã chứng kiến tiến bộ đáng kể trong các mô hình ngôn ngữ lớn (LLM) [2, 3,10,43,60,35]. Đặc biệt, Generative Pre-trained Transformers, thường được gọi là GPT [2, 35], đã dẫn đến một bước đột phá trong lĩnh vực xử lý ngôn ngữ tự nhiên. Mặc dù kích thước của LLM tăng lên đáng kể, việc khai thác khả năng lý luận vẫn yêu cầu các truy vấn được thiết kế tinh vi hơn, tức là prompting. Trong quá khứ, một số nghiên cứu đã cố gắng thực hiện kỹ thuật prompt để trang bị thêm khả năng cho LLM. Học ngữ cảnh là một cách chính để dạy LLM tuân theo các hướng dẫn cụ thể như được thể hiện bởi một vài ví dụ [2,15]. Tiếp theo, một số kỹ thuật khác như chain-of-thought và tree-of-thought [46,52] được đề xuất để tăng cường các tác vụ lý luận số học, thường thức và tượng trưng. Tương tự với các nghiên cứu này, Set-of-Mark Prompting của chúng tôi có thể được coi là một cách prompting mô hình để nhìn các vùng hình ảnh từng vị trí một. Tuy nhiên, nó cũng khác ở chỗ không cần ví dụ ngữ cảnh trong phương pháp của chúng tôi.

Prompting Thị giác và Đa phương thức. Trong các nghiên cứu sớm về phân đoạn hình ảnh tương tác [34,8,9], prompting không gian được sử dụng để mô hình có thể nhận nhiều đầu vào của người dùng (ví dụ, nhấp chuột) để dần dần tinh chỉnh mặt nạ. Gần đây, nghiên cứu tiên phong SAM [19] và các biến thể của nó [42,11] đã đề xuất một mô hình thống nhất để hỗ trợ các loại chế độ prompting khác nhau bao gồm điểm, hộp và văn bản. Trong SEEM [65], các tác giả đã đề xuất một visual sampler mới để mở rộng prompting thị giác qua hình ảnh. Tương tự, PerSAM [58] và SAM-PT [40] fine-tune SAM để hỗ trợ phân đoạn cá nhân hóa và theo dõi điểm. Một hướng nghiên cứu khác trực tiếp tạo ra các prompt trên hình ảnh đầu vào. Trong [1], image inpainting được sử dụng như prompting để dạy mô hình dự đoán đầu ra dày đặc với hình ảnh ví dụ và đầu ra tương ứng, cả hai đều trong không gian pixel. Painter [44] và SegGPT [45] chia sẻ tinh thần tương tự trong khi sử dụng masked image modeling để giải mã pixel trong không gian liên tục. Prompting thị giác cũng có thể được áp dụng cho các mô hình đa phương thức. Colorful prompting tuning (CPT) là một trong những nghiên cứu đầu tiên phủ các vùng hình ảnh với các màu khác nhau và sử dụng mô hình ngôn ngữ có mặt nạ để điền vào chỗ trống [53]. RedCircle [41] vẽ một vòng tròn đỏ trên hình ảnh để tập trung mô hình thị giác vào vùng được bao quanh. Trong [48], các tác giả tăng cường các prompt bằng cách phân đoạn và làm nổi bật một đối tượng mục tiêu trong hình ảnh. Cả hai phương pháp sau đó yêu cầu mô hình CLIP đo lường độ tương tự giữa hình ảnh được prompt và một số khái niệm văn bản.

LMM và Prompting. Trong thời gian ngắn, các mô hình đa phương thức lớn (LMM) đã nổi lên như một hướng nghiên cứu mới trong cộng đồng. Mục tiêu là xây dựng một hệ thống đa phương thức tổng quát hoạt động như LLM có thể nhận thức và lý luận với đầu vào đa phương thức [28,63,57,12,59,47,50,6,20,5,54]. Các nghiên cứu sớm như MiniGPT-4 [5] và LLaVa [28] đã đề xuất một cách đơn giản nhưng hiệu quả để kết nối thị giác và LLM theo sau bởi instruction tuning. Sau đó, một chiến lược huấn luyện tương tự được áp dụng cho các tác vụ video [33,4] và các tác vụ thị giác tinh tế [6,59,20,5,54]. Vui lòng tham khảo [20] để xem xét toàn diện hơn về LMM. Cho đến nay, prompting LMM ít được khám phá trong học thuật, một phần vì hầu hết các mô hình mã nguồn mở gần đây có khả năng hạn chế và do đó không có khả năng như vậy [46]. Gần đây nhất, GPT-4V [36] đã được phát hành theo sau bởi một nghiên cứu định tính toàn diện về nó [49]. Các tác giả trong [49] đã sử dụng một chiến lược prompting tương tự như trong RedCircle [41] để prompt GPT-4V. Tuy nhiên, nó chỉ thể hiện một số khả năng tham chiếu của GPT-4V với các vòng tròn hoặc mũi tên được vẽ tay, chứ chưa kể đến một nghiên cứu định lượng toàn diện.

6 Thảo luận
Bí ẩn trong GPT-4V. Vẫn còn bí ẩn tại sao Set-of-Mark Prompting được đề xuất có thể hoạt động rất tốt với GPT-4V. Vì tò mò, chúng tôi cũng chạy một số ví dụ trên các LMM mã nguồn mở khác như LLaVa-1.5 và MiniGPT-v2. Tuy nhiên, cả hai mô hình đều khó có thể diễn giải các dấu hiệu và ground mình trên những dấu hiệu đó. Chúng tôi đưa ra giả thuyết về một vài lý do cho khả năng grounding thị giác phi thường được thể hiện trong GPT-4V. Thứ nhất, quy mô quan trọng. Chúng tôi tin rằng quy mô mô hình và dữ liệu huấn luyện được sử dụng trong GPT-4V lớn hơn vài bậc so với các LMM mã nguồn mở đã đề cập. Thứ hai, chiến lược curation dữ liệu có lẽ là một bí quyết khác cho GPT-4V. GPT-4V có thể tự động liên kết các vùng hình ảnh và dấu hiệu mà không cần bất kỳ prompt rõ ràng nào trong văn bản. Loại dữ liệu như vậy có thể đến từ các hình ảnh văn học, biểu đồ, v.v., thường được gắn nhãn hoặc đánh dấu rõ ràng [1]. Chúng tôi nghi ngờ rằng GPT-4V cụ thể sử dụng dữ liệu thị giác tinh tế như được đề cập trong nghiên cứu này. Lưu ý rằng số lượng cực kỳ hạn chế, và việc chú thích thêm loại dữ liệu này rất tốn kém, chưa kể đến việc khó kiểm soát chất lượng. Cuối cùng, chúng tôi lưu ý rằng tất cả các nghi ngờ trên không dựa trên sự thật mà trên một số nghiên cứu thực nghiệm như chúng tôi đã thực hiện ở trên.

Kết nối prompting thị giác và LLM. Mặc dù những điều chưa biết đằng sau GPT-4V. Nghiên cứu của chúng tôi thực sự thực hiện một trong những bước đầu tiên để kết nối prompting thị giác và prompting LLM. Trong quá khứ, nhiều nghiên cứu đã nghiên cứu cách làm cho các mô hình thị giác có thể prompt được hơn, điều này tách biệt khỏi việc prompt văn bản cho LLM. Rào cản chủ yếu là do bottleneck ngôn ngữ vì chúng ta khó có thể thể hiện chính xác các prompt thị giác bằng ngôn ngữ. Ví dụ, hình dạng/vị trí/màu sắc của một nét vẽ được vẽ ngẫu nhiên trên hình ảnh khó có thể được mô tả bằng lời, trừ khi chúng ta có thể mã hóa các prompt thị giác và fine-tune toàn bộ mô hình [54]. Tuy nhiên, với dữ liệu huấn luyện tinh tế hạn chế và các mô hình thị giác và ngôn ngữ lớn mã nguồn mở kém hơn. Chúng tôi vẫn thấy một khoảng cách rõ ràng. Ở giai đoạn hiện tại, Set-of-Mark Prompting được đề xuất của chúng tôi chứng minh một cách đơn giản nhưng chỉ khả thi để kế thừa tất cả khả năng hiện có của LMM mạnh nhất trong khi mở ra khả năng grounding được khao khát mạnh mẽ của nó. Chúng tôi hy vọng nghiên cứu này là nghiên cứu đầu tiên kết nối liền mạch prompting thị giác và ngôn ngữ, có thể giúp mở đường hướng tới các LMM có khả năng hơn.

Mở rộng dữ liệu thông qua Set-of-Mark Prompting với GPT-4V. Trong quá khứ, toàn bộ cộng đồng đã cố gắng xây dựng các hệ thống thị giác tinh tế, từ vựng mở, trải dài từ detection [17,61,25, 56,51,31] đến segmentation [16,64,14], và mở rộng thêm đến 3D [32,13,30]. Mặc dù tiến bộ to lớn, chúng ta vẫn đang gặp khó khăn với cách ánh xạ kiến thức ngữ nghĩa phong phú từ CLIP đến các miền tinh tế. Điều này chủ yếu do số lượng cực kỳ hạn chế các chú thích ngữ nghĩa tinh tế, vẫn là một thách thức đáng kể đối với các LMM hiện tại. Như chúng ta đã thấy, tính khả thi của việc mở rộng dữ liệu với không gian tinh tế đã được chứng minh trong SAM [19] và Semantic-SAM [22], nhưng cách chú thích thêm các vùng này với nhãn ngữ nghĩa vẫn là một vấn đề mở. Dưới ánh sáng của nghiên cứu trong công trình này, chúng tôi hình dung tiềm năng của việc sử dụng GPT-4V cộng với Set-of-Mark Prompting của chúng tôi để mở rộng quy mô dữ liệu đa phương thức một cách chuyên sâu có cả mô tả không gian tinh tế và ngôn ngữ chi tiết.

7 Kết luận
Chúng tôi đã trình bày Set-of-Mark Prompting, một cơ chế prompting thị giác đơn giản nhưng hiệu quả cho LMM, đặc biệt là GPT-4V. Chúng tôi cho thấy rằng chỉ cần phủ một số dấu hiệu tượng trưng trên một tập hợp các vùng của hình ảnh đầu vào có thể mở ra khả năng grounding thị giác của GPT-4V. Chúng tôi trình bày một nghiên cứu thực nghiệm toàn diện trên một loạt rộng các tác vụ thị giác tinh tế để chứng minh rằng GPT-4V được prompt bởi SoM vượt trội so với các mô hình chuyên gia được fine-tune hoàn toàn và các LMM mã nguồn mở khác. Hơn nữa, kết quả định tính của chúng tôi cho thấy GPT-4V với SoM sở hữu khả năng nhận thức, nhận thức và lý luận đa phương thức tinh tế phi thường trên toàn bộ. Chúng tôi hy vọng rằng SoM sẽ truyền cảm hứng cho các nghiên cứu trong tương lai về prompting đa phương thức cho LMM và mở đường hướng tới AGI đa phương thức.

Lời cảm ơn
Chúng tôi cảm ơn Fangrui Zhu vì những thảo luận sâu sắc. Chúng tôi cảm ơn Xuan Li và Biyi Fang từ Microsoft Office Team vì việc brainstorming sớm.

Tài liệu tham khảo
[1] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005–25017, 2022.

12

--- TRANG 13 ---
[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.

[3] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.

[4] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, and Limin Wang. Videollm: Modeling video sequence with large language models, 2023.

[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning. github, 2023.

[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic, 2023.

[7] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey Hinton. Pix2seq: A language modeling framework for object detection, 2022.

[8] Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion for interactive segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7345–7354, 2021.

[9] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and Hengshuang Zhao. Focalclick: Towards practical interactive image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300–1309, 2022.

[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

[11] Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Dajiang Zhu, Wei Liu, et al. Samaug: Point prompt augmentation for segment anything model. arXiv preprint arXiv:2307.01187, 2023.

[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.

[13] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7010–7019, 2023.

[14] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with maskclip. arXiv preprint arXiv:2208.08984, 2022.

[15] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.

[16] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, pages 540–557. Springer, 2022.

[17] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.

[18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787–798, 2014.

[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023.

13

--- TRANG 14 ---
[20] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023.

[21] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity, 2023.

[22] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023.

[23] Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, and Heung-Yeung Shum. Mask dino: Towards a unified transformer-based framework for object detection and segmentation, 2022.

[24] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M Ni, and Heung-Yeung Shum. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3041–3050, 2023.

[25] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965–10975, 2022.

[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015.

[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.

[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.

[29] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and R. Manmatha. Polyformer: Referring image segmentation as sequential polygon generation, 2023.

[30] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik, Christian Theobalt, Eric Xing, and Shijian Lu. 3d open-vocabulary segmentation with foundation models. arXiv preprint arXiv:2305.14093, 2023.

[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2023.

[32] Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang. Open-vocabulary 3d detection via image-level class and debiased cross-modal contrastive learning. arXiv preprint arXiv:2207.01987, 2022.

[33] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration, 2023.

[34] Kevin McGuinness and Noel E O'connor. A comparative evaluation of interactive segmentation algorithms. Pattern Recognition, 43(2):434–444, 2010.

[35] OpenAI. Gpt-4 technical report, 2023.

[36] OpenAI. Gpt-4v(ision) system card, 2023.

[37] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724–732, 2016.

[38] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016.

14

--- TRANG 15 ---
[39] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.

[40] Frano Rajič, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment anything meets point tracking. arXiv preprint arXiv:2307.01197, 2023.

[41] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlms, 2023.

[42] Lv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv preprint arXiv:2304.04709, 2023.

[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[44] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6830–6839, 2023.

[45] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context, 2023.

[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.

[48] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual prompting, 2023.

[49] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023.

[50] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.

[51] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. Advances in Neural Information Processing Systems, 35:9125–9138, 2022.

[52] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

[53] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models, 2022.

[54] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity, 2023.

[55] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection, 2023.

[56] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding, 2022.

[57] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.

[58] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li. Personalize segment anything model with one shot. arXiv preprint arXiv:2305.03048, 2023.

15

--- TRANG 16 ---
[59] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest, 2023.

[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

[61] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793–16803, 2022.

[62] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.

[63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.

[64] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized decoding for pixel, image, and language, 2022.

[65] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.

16

--- TRANG 17 ---
A Trường hợp Sử dụng

Kết hợp GPT-4V với SoM prompting được đề xuất, chúng tôi chạy nhiều trường hợp sử dụng thực tế để kiểm tra tính tổng quát và ưu điểm của nó so với GPT-4V gốc.

Lý luận Grounded và Tham chiếu Chéo Hình ảnh

Người dùng: Bạn có thể chỉ cho tôi điều gì bất thường nhất trong hình ảnh này không?

GPT-4V:
Chắc chắn! Điều bất thường nhất trong hình ảnh này là một người đàn ông đang ủi quần áo trên bàn ủi được đặt trên đỉnh một chiếc taxi màu vàng đang di chuyển trên đường phố thành phố. Không phải cảnh thường thấy khi nhìn thấy ai đó ủi quần áo trên đỉnh một chiếc taxi, đặc biệt là khi nó đang chạy trên đường!

Người dùng: Bạn có thể chỉ cho tôi điều gì bất thường nhất trong hình ảnh này không?

GPT-4V (SoM):
Điều bất thường nhất trong hình ảnh này là người đàn ông (các yếu tố 8, 17, 19) đang đứng trên bàn ủi (yếu tố 12) được đặt trên đỉnh một chiếc taxi màu vàng đang di chuyển (các yếu tố 2, 6, 11). Anh ta đang ủi quần áo giữa đường phố thành phố, đây là hành vi rất bất thường và không an toàn. Tình huống này nổi bật như khía cạnh kỳ lạ nhất của cảnh.

Người dùng: Các đối tượng chung trong hai hình ảnh là gì?

GPT-4V (SoM): Các đối tượng chung trong hai hình ảnh là:
• Một người đàn ông (Trong hình ảnh đầu tiên, các yếu tố 8, 17, 19; Trong hình ảnh thứ hai, yếu tố a)
• Một bàn ủi (Trong hình ảnh đầu tiên, yếu tố 12; Trong hình ảnh thứ hai, yếu tố b)
• Một bàn ủi (Không được gắn nhãn rõ ràng trong hình ảnh đầu tiên nhưng người đàn ông đang sử dụng nó; Trong hình ảnh thứ hai, yếu tố f)
• Quần áo/áo sơ mi đang được ủi (Không được gắn nhãn rõ ràng trong hình ảnh đầu tiên nhưng người đàn ông đang ủi nó; Trong hình ảnh thứ hai, yếu tố c)

Hình 9: So với GPT-4V không có SoM, việc thêm dấu hiệu cho phép GPT-4V ground lý luận trên nội dung chi tiết của hình ảnh (Trái). Tham chiếu đối tượng chéo hình ảnh rõ ràng được quan sát ở phía bên phải.

17

--- TRANG 18 ---
Giải quyết Vấn đề

Người dùng: Vui lòng giải quyết điều này
GPT-4V:
Các ô vuông có đèn giao thông trong hình ảnh CAPTCHA được cung cấp là:
• Ô vuông đầu tiên ở hàng đầu tiên.
• Ô vuông thứ hai ở hàng đầu tiên.
• Ô vuông đầu tiên ở hàng thứ hai.
• Ô vuông thứ hai ở hàng thứ hai.
Vui lòng chọn những ô vuông này để giải quyết CAPTCHA. Hãy nhớ rằng, khi bạn gặp CAPTCHA trong các tình huống thực tế, điều quan trọng là tự mình giải quyết chúng để đảm bảo tính bảo mật và tính xác thực của các hành động trực tuyến của bạn.

GPT-4V (SoM):
Trong hình ảnh này, có vẻ như các ô vuông được đánh số 7, 11, 12 chắc chắn chứa các phần của đèn giao thông.

Hình 10: Nghiên cứu trường hợp về giải quyết CAPTCHA. GPT-4V đưa ra câu trả lời sai với số lượng ô vuông sai trong khi tìm ra các ô vuông đúng với các dấu hiệu tương ứng sau khi SoM prompting.

18

--- TRANG 19 ---
Chia sẻ Kiến thức

Người dùng: Có gì trong món ăn này?
GPT-4V:
Món ăn có vẻ bao gồm:
• Một miếng phi lê cá nướng hoặc chiên chảo, trông có thể là loại cá trắng, có thể là cá vược, cá tuyết hoặc cá bơn.
• Măng tây.
• Khoai tây, có thể nướng hoặc luộc.
• Một lát chanh để vắt lên món ăn.
• Trang trí bằng lá mùi tây hoặc ngò tươi.
• Có thể có nước sốt nhẹ hoặc gia vị rắc lên cá và rau, nhưng khó xác định thành phần từ hình ảnh.
Trông như một bữa ăn ngon và tốt cho sức khỏe!

GPT-4V (SoM):
Đây là mô tả về các mục trong món ăn:
1. Phi lê Cá: Thành phần chính của món ăn có vẻ là một miếng phi lê cá, có thể nướng hoặc chiên chảo.
2. Lát Chanh: Một món ăn kèm thường gặp với món cá để tăng hương vị khi vắt lên trên.
3. Mùi tây hoặc Ngò: Loại thảo mộc xanh này được dùng làm đồ trang trí và có thể thêm hương vị tươi cho món ăn.
4. Măng tây: Những cọng xanh này thường được hấp, xào hoặc nướng và phục vụ như món ăn phụ.
5. Nước sốt hoặc Nước dùng: Có nước sốt hoặc nước dùng màu vàng trên đĩa.
6. Khoai tây: Trông như khoai tây nướng hoặc luộc.
7. Đồ trang trí: Có thể là hành lá hoặc hẹ rắc trên đĩa để thêm màu sắc và hương vị.
8. Nước sốt hoặc Dầu rưới: Có một lớp rưới tối màu xung quanh khoai tây, có thể là một loại nước sốt hoặc có thể là giấm balsamic.
Món ăn có vẻ là một bữa ăn cân bằng tốt với protein (cá), rau (măng tây và khoai tây), và nhiều chất tăng hương vị khác nhau (nước sốt, chanh và thảo mộc).

Hình 11: Nghiên cứu trường hợp về hình ảnh món ăn cho GPT-4V. GPT-4V không tạo ra câu trả lời được ground với hình ảnh gốc. Dựa trên SoM prompting, GPT-4V không chỉ nói ra các thành phần mà còn tương ứng chúng với các vùng.

19

--- TRANG 20 ---
Gợi ý Cá nhân hóa

Người dùng: Tôi đang ăn kiêng, những thứ hàng đầu tôi nên tránh ăn là gì?
GPT-4V:
Nếu bạn đang ăn kiêng, điều đó sẽ phụ thuộc vào loại chế độ ăn kiêng cụ thể bạn đang thực hiện. Tuy nhiên, dựa trên hình ảnh bạn cung cấp, đây là một số mục phổ biến mà nhiều chế độ ăn kiêng có thể hạn chế hoặc tránh:
• Hamburger: Đặc biệt nếu làm từ thịt nhiều chất béo hoặc phục vụ với phô mai và mayonnaise.
• Khoai tây chiên: Thực phẩm chiên ngập dầu thường có nhiều calo và chất béo không lành mạnh.
• Khoai tây chiên giòn: Có thể có nhiều muối và chất béo, đặc biệt nếu chúng được sản xuất thương mại.
• Hành tây chiên: Một món chiên ngập dầu khác.
• Bánh mì với bánh mì trắng: Nhiều chế độ ăn kiêng khuyến nghị ngũ cốc nguyên hạt thay vì ngũ cốc tinh chế.
• Thịt chế biến: Như thịt ham trong bánh mì, vì chúng có thể có nhiều natri và chất bảo quản.
• Nước sốt: Một số có thể có nhiều đường, natri, hoặc chất béo không lành mạnh.
• Nước ngọt hoặc đồ uống có đường: Nhiều đường và calo rỗng.
Luôn tham khảo ý kiến chuyên gia dinh dưỡng hoặc bác sĩ dinh dưỡng để nhận hướng dẫn phù hợp với nhu cầu cụ thể và mục tiêu sức khỏe của bạn. Họ có thể đưa ra khuyến nghị dựa trên chế độ ăn kiêng và mục tiêu sức khỏe cá nhân của bạn.

GPT-4V (SoM):
Dựa trên các cân nhắc chế độ ăn kiêng thông thường, đây là những mục hàng đầu từ hình ảnh được cung cấp mà bạn có thể nên tránh hoặc hạn chế:
1. Hành tây chiên (1 và 11) - Thực phẩm chiên như thế này thường có nhiều chất béo không lành mạnh và calo.
2. Khoai tây chiên (8) - Một món chiên khác có thể có nhiều calo và chứa chất béo không lành mạnh.
3. Hamburger (3 và 5) - Tùy thuộc vào thành phần của chúng, chúng có thể có nhiều calo, chất béo không lành mạnh, và có thể chứa thịt chế biến.
4. Thịt chế biến (2 và 15) - Chúng có thể có nhiều natri, chất béo không lành mạnh, và các chất phụ gia khác.
5. Khoai tây chiên giòn (16) - Thường có nhiều chất béo không lành mạnh và có thể có nhiều calo.
Mặc dù thỉnh thoảng thưởng thức là được, tốt nhất là nên tiêu thụ những mục này một cách vừa phải nếu bạn đang cố gắng duy trì chế độ ăn lành mạnh.

Hình 12: GPT-4V được prompt bởi SoM đưa ra các gợi ý rất chính xác trong khi cái gốc thất bại, thậm chí với thực phẩm ảo tưởng, ví dụ nước ngọt.

20

--- TRANG 21 ---
Hướng dẫn Sử dụng Công cụ

Người dùng: Cách sử dụng bộ điều khiển này?
GPT-4V (SoM):
Đây có vẻ là bộ điều khiển Xbox One hoặc Xbox Series X|S. Dưới đây là hướng dẫn cơ bản về cách sử dụng nó:
• Thumbstick (2 & 3): Chúng chủ yếu được sử dụng để di chuyển và ngắm bắn.
– Thumbstick Trái (2): Điều khiển di chuyển của nhân vật.
– Thumbstick Phải (3): Điều khiển camera hoặc thước ngắm.
• Directional Pad (D-Pad) (1): Được sử dụng cho các chức năng khác nhau như điều hướng menu, chọn vũ khí, hoặc các hành động khác trong game.
• Các nút Mặt (4, 5, 6, & 7):
– A (4): Thường là nút hành động chính hoặc nút "chấp nhận".
– B (7): Thường là nút "quay lại" hoặc "hủy".
– X (5): Thay đổi theo game, thường dùng cho tương tác.
– Y (6): Giống như X, chức năng thay đổi nhưng thường dùng cho hành động phụ hoặc tương tác.
• Bumper và Trigger (8 & 9):
– Bumper Trái và Phải (8): Dùng cho hành động nhanh hoặc chuyển đổi giữa các mục.
– Trigger Trái và Phải: Dùng cho hành động chính như bắn hoặc tăng tốc trong game đua xe.
• Nút Menu và View: Nằm ở giữa, dùng để mở menu trong game hoặc các overlay khác.
• Nút Xbox: Nhấn để đi đến trang chủ Xbox hoặc bật/tắt bộ điều khiển. Nhấn giữ mở menu để tắt bộ điều khiển hoặc console.
• Nút Ghép nối: Nằm ở cạnh trên, dùng để ghép nối bộ điều khiển.
• Jack âm thanh 3.5mm: Ở phía dưới, cắm tai nghe hoặc headset.
• Cổng USB: Ở cạnh trên, dùng để sạc hoặc chơi có dây.
Tham khảo hướng dẫn game hoặc cài đặt điều khiển trong game để biết chức năng chính xác cho từng game.

Hình 13: Tương tự, GPT4-V với SoM có thể giúp cung cấp hướng dẫn sử dụng công cụ kỹ lưỡng, dạy người dùng chức năng của từng nút trên bộ điều khiển. Lưu ý rằng hình ảnh này không được gắn nhãn đầy đủ, trong khi GPT-4V cũng có thể cung cấp thông tin về các nút không được gắn nhãn.

21

--- TRANG 22 ---
Lập kế hoạch Game 2D

Người dùng: Làm thế nào tôi có thể lấy được những đồng xu ở trên?
GPT-4V (SoM):
Trông như bạn đang chơi một game platformer giống Super Mario Bros. Đây là chiến lược tổng quát dựa trên bố cục:
• Nền tảng Cao: Các nền tảng được gắn nhãn '4' và '7' là chìa khóa. Bạn cần lên được các nền tảng này trước để đạt độ cao lớn hơn. Tùy thuộc vào cơ chế game, bạn có thể cần tìm đà nhảy hoặc sử dụng power-up tăng cường nhảy nếu có.
• Gạch và Khối: Những viên gạch được gắn nhãn '9' có thể chứa một số power-up hoặc có thể phá được. Trong platformer cổ điển như Super Mario, đánh những khối này từ bên dưới có thể tiết lộ power-up hoặc thậm chí tạo ra nền tảng. Điều tương tự cũng áp dụng cho khối được gắn nhãn '1'.
• Nhảy Dọc: Khi đã lên được nền tảng cao '4', hãy thực hiện nhảy dọc để đạt đến những đồng xu được gắn nhãn '11'. Đảm bảo rằng bạn tính thời gian nhảy chính xác và sử dụng bất kỳ tăng cường tốc độ hoặc nhảy nào, nếu có.
• Động lực: Một số game cũng cho phép đạt được độ cao nhảy thêm bằng cách tích lũy động lực. Nếu game này hoạt động tương tự, hãy thử chạy từ xa và nhảy vào phút cuối để lấy được đồng xu.
• Tìm Power-up Ẩn: Một số game ẩn power-up trong level có thể hỗ trợ người chơi đạt đến những nơi khó. Chúng có thể được ẩn trong các viên gạch không đánh dấu hoặc ở những nơi khác. Nếu bạn có khả năng như nhảy đôi, hãy sử dụng chúng.
• Cơ chế Game: Mỗi game có thể có cơ chế độc đáo. Nếu có bất kỳ điều khiển hoặc cơ chế nào khác bạn chưa thử, hãy xem xét thử nghiệm với chúng. Có thể có cách để nảy, nhảy đôi, hoặc sử dụng kẻ thù để tăng độ cao nhảy.
Nếu bạn đã thử tất cả chiến lược tổng quát và vẫn không thể đạt đến đồng xu, có thể hữu ích khi tra cứu chiến lược cụ thể hoặc walkthrough cho game cụ thể bạn đang chơi. Chúc may mắn!

Hình 14: GPT-4V với SoM đưa ra gợi ý hợp lý về cách đạt được mục tiêu trong tình huống gaming.

22

--- TRANG 23 ---
Điều hướng Mô phỏng

Người dùng: Tôi hơi đói, tôi có thể tìm thức ăn ở đâu?
GPT-4V (SoM):
Dựa trên hình ảnh được cung cấp, có vẻ như bạn đang xem sơ đồ mặt bằng của một căn hộ. Để tìm thức ăn, bạn nên đi đến khu vực bếp được gắn nhãn số "10". Đó là nơi bạn có khả năng tìm thấy thức ăn hoặc thiết bị nhà bếp để chuẩn bị bữa ăn nhất. Nếu bạn đang ở trong căn hộ này, hãy đi đến khu vực đó! Nếu không, hãy xem xét kiểm tra nhà bếp của riêng bạn hoặc một nhà hàng gần đó. Chúc ngon miệng!

Người dùng: tuyệt, tôi nên ngồi đâu để ăn nếu tôi tìm thấy thức ăn từ 10?
GPT-4V (SoM):
Dựa trên sơ đồ mặt bằng, nơi phù hợp nhất để ngồi và ăn sẽ là khu vực ăn uống được gắn nhãn số "14". Nó có vẻ có bàn ăn và ghế, tạo ra một chỗ thoải mái để thưởng thức bữa ăn.

Người dùng: ok, tôi bị bẩn tay, tôi nên làm gì?
GPT-4V (SoM):
Dựa trên sơ đồ mặt bằng, bạn nên đi đến khu vực phòng tắm để rửa tay. Bạn có thể đi đến không gian được gắn nhãn số "13" có vẻ là một phòng tắm. Nó có khả năng có bồn rửa và xà phòng để bạn có thể rửa tay. Luôn nhớ rửa tay kỹ, đặc biệt là trước khi ăn!

Người dùng: bây giờ tôi cần ngủ.
GPT-4V (SoM):
Để ngủ, bạn nên đi đến khu vực được gắn nhãn số "19". Nó có vẻ là một phòng ngủ với giường, sẽ là nơi phù hợp nhất để nghỉ ngơi. Chúc ngủ ngon!

Người dùng: TV ở đâu, tôi muốn bật nó.
GPT-4V (SoM):
TV nằm ở khu vực được gắn nhãn số "6". Bạn có thể đi đến đó để bật nó và thưởng thức giải trí.

Người dùng: Đủ để xem TV rồi, tôi muốn tắm.
GPT-4V (SoM):
Để tắm, bạn nên đi đến không gian được gắn nhãn số "17". Nó có vẻ là khu vực tắm trong phòng tắm. Đảm bảo bạn đã chuẩn bị đồ vệ sinh và khăn tắm, và thưởng thức việc tắm!

Người dùng: được rồi, cần giặt một số quần áo, đặt chúng ở đâu?
GPT-4V (SoM):
Để giặt quần áo, bạn nên đi đến khu vực được gắn nhãn số "8". Nó có vẻ là một vị trí được chỉ định cho máy giặt hoặc khu vực giặt ủi. Bạn có thể đặt quần áo của mình ở đó và bắt đầu quá trình giặt.

Hình 15: GPT-4V với SoM có tiềm năng giúp điều hướng thể hiện vì nó có thể nhanh chóng giúp tìm các vùng/vị trí có chức năng cụ thể.

23
