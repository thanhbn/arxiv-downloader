# 2312.17172.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2312.17172.pdf
# Kích thước tệp: 14617196 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Unified-IO 2: Mở rộng Mô hình Đa phương thức Tự hồi quy
với Thị giác, Ngôn ngữ, Âm thanh, và Hành động
Jiasen Lu1*Christopher Clark1*Sangho Lee1*Zichen Zhang1*
Savya Khosla2Ryan Marten2Derek Hoiem2Aniruddha Kembhavi13
1Allen Institute for AI2University of Illinois Urbana-Champaign3University of Washington
{jiasenl, chrisc, sanghol, chralesz, anik }@allenai.org
unified-io-2.allenai.org

Bạn có thể làm món gì với những nguyên liệu này? Đưa cho tôi công thức.Một công thức ngon sử dụng những nguyên liệu này là pudding chocolate! Đây là công thức:Nguyên liệu:- 1 cốc bột mì đa dụng, - ½ cốc đường....Hướng dẫn:1. Trong một bát lớn, đánh bông bột mì, đường..2. Trong một bát riêng, trộn đều trứng...

Tạo một track âm thanh cho ban nhạc này.Tạo một track âm thanh cho ban nhạc này.

Vẽ một cảnh hoàng hôn
Loại bỏ bến cảngVẽ bức tranh này theo phong cách Van Gogh

Tạo một hình ảnh chiếc ô tô với mẫu trong hình đầu tiên và màu sắc trong hình thứ hai.

Tạo một hình ảnh độ sâuTạo bản đồ độ sâu & pháp tuyến bề mặtTạo bản đồ pháp tuyến bề mặt

Những trái cây nào có trong hình này? Liệt kê chúng ở định dạng json với tên trái cây làm khóa và màu sắc trái cây làm giá trị.{"banana" : "Yellow","apple" : "Red","grapes" : "Green"}

Phân đoạn nho và một quả táo

Đặt kobar blicket ít hơn vào dax.= blicket= daxis kobar than

Dựa trên hình ảnh ban đầu và một chuỗi hành động, dự đoán các khung hình tiếp theo
Hình ảnh được tạo

Tạo một hình ảnh phi hành gia cưỡi ngựa trong rừng. Có một dòng sông phía trước họ với hoa súng. Tạo một hình ảnh con voi bơi dưới nước. thẩm mỹ. Tưởng tượng.

Thêm các chi tiết bị thiếu vào hình ảnh bị che (trái) bằng cách sử dụng hình ảnh tham chiếu (phải).

Tìm các điểm khóa có thể nhìn thấy tương ứng với người nằm trong vùng được đánh dấu.

Xác định vị trí của các nhạc cụ tạo ra âm thanh đã cho.(âm thanh trống)Chỉnh sửa hình ảnh
Tạo hình ảnh
Tạo hình ảnh tham chiếu
Hoàn thành hình ảnh đa góc nhìn
Độ sâu & Pháp tuyến bề mặt Ước tính điểm khóaPhân tích thị giácVQA tự do
Tạo âm thanh dựa trên thị giácĐịnh vị âm thanh thị giác
Thao tác robotDự đoán khung hình tương laiquay phảiquay phảiquay phảidi thẳng
di thẳngdi thẳngdi thẳngquay phảiPhân đoạn &

Hình 1. U NIFIED -IO 2 là một mô hình tuân theo hướng dẫn với khả năng và các phương thức được hỗ trợ rộng lớn. Nó có thể tạo ra hình ảnh (hộp đỏ), bao gồm chỉnh sửa hình ảnh, tạo hình ảnh, ước tính độ sâu, ước tính pháp tuyến bề mặt, và dự đoán khung hình tương lai v.v. Nó cũng có thể tạo ra văn bản (hộp xanh), bao gồm câu trả lời dạng dài cho các truy vấn, ước tính điểm khóa, định vị âm thanh thị giác, dự đoán hành động cho thao tác robot v.v. Nó có thể tạo ra âm thanh (hộp xanh lá) từ hình ảnh hoặc văn bản. Nhấp và để nghe các mẫu âm thanh tương ứng.

* Tác giả chính, đóng góp ngang nhau. Mô tả về đóng góp của mỗi tác giả có sẵn trong Phụ lục A. Liên hệ với Jiasen Lu.
1arXiv:2312.17172v1 [cs.CV] 28 Dec 2023

--- TRANG 2 ---
Tóm tắt
Chúng tôi trình bày UNIFIED -IO 2, mô hình đa phương thức tự hồi quy đầu tiên có khả năng hiểu và tạo ra hình ảnh, văn bản, âm thanh, và hành động. Để thống nhất các phương thức khác nhau, chúng tôi token hóa đầu vào và đầu ra - hình ảnh, văn bản, âm thanh, hành động, hộp giới hạn v.v., vào một không gian ngữ nghĩa chung và sau đó xử lý chúng bằng một mô hình transformer encoder-decoder duy nhất. Vì việc huấn luyện với các phương thức đa dạng như vậy là thách thức, chúng tôi đề xuất nhiều cải tiến kiến trúc để ổn định việc huấn luyện mô hình. Chúng tôi huấn luyện mô hình từ đầu trên một corpus tiền huấn luyện đa phương thức lớn từ các nguồn đa dạng với mục tiêu hỗn hợp các bộ khử nhiễu đa phương thức. Để học một tập hợp kỹ năng mở rộng, chẳng hạn như tuân theo hướng dẫn đa phương thức, chúng tôi xây dựng và tinh chỉnh trên một tập hợp 120 bộ dữ liệu với lời nhắc và tăng cường. Với một mô hình thống nhất duy nhất, UNIFIED -IO 2 đạt được hiệu suất tiên tiến trên benchmark GRIT và kết quả mạnh mẽ trên hơn 35 benchmark, bao gồm tạo và hiểu hình ảnh, hiểu ngôn ngữ tự nhiên, hiểu video và âm thanh, và thao tác robot. Chúng tôi phát hành tất cả các mô hình cho cộng đồng nghiên cứu.

1. Giới thiệu
Là các nhà nghiên cứu AI, chúng tôi tìm cách xây dựng các tác nhân thông minh có thể cảm nhận môi trường của chúng, giao tiếp với người khác, hành động trong thế giới, và suy luận về các tương tác của chúng. Thế giới là đa phương thức, vì vậy các tác nhân của chúng tôi phải tham gia vào các tương tác phong phú có bản chất đa phương thức qua thị giác, ngôn ngữ, âm thanh, hành động v.v. Các nhà tâm lý học đã lập luận rằng sự dư thừa của các hệ thống cảm giác của chúng ta phục vụ như các cơ chế giám sát để cải thiện lẫn nhau [48, 144, 167]. Điều này cung cấp một động lực tự nhiên để tạo ra các mô hình với khả năng học tập tương tự, hỗ trợ nhiều phương thức khác nhau có thể giám sát lẫn nhau trong quá trình huấn luyện.

Xây dựng các mô hình có thể phân tích và tạo ra nhiều phương thức là một công việc phức tạp. Huấn luyện các Mô hình Ngôn ngữ Lớn (LLM) với hàng tỷ tham số, mặc dù chỉ hỗ trợ một phương thức duy nhất, đã cực kỳ thách thức trên nhiều mặt trận - từ thu thập và xử lý các bộ dữ liệu khổng lồ, đảm bảo chất lượng dữ liệu và quản lý thiên vị, thiết kế các kiến trúc mô hình hiệu quả, duy trì các quy trình huấn luyện ổn định, và tinh chỉnh hướng dẫn để tăng cường khả năng của mô hình trong việc tuân theo và hiểu các hướng dẫn của người dùng. Những thách thức này được khuếch đại rất lớn với việc bổ sung mỗi phương thức mới.

Trước những khó khăn này, một dòng công việc gần đây trong việc xây dựng các hệ thống đa phương thức đã tận dụng các LLM đã được tiền huấn luyện, với một số tăng cường bằng các bộ mã hóa phương thức mới [5, 46, 119], một số thêm các bộ giải mã đặc thù cho phương thức [14, 96] và những người khác tận dụng khả năng của LLM để xây dựng các khung mô-đun [64, 166, 173]. Một dòng công việc khác về huấn luyện các mô hình đa phương thức từ đầu đã tập trung vào tạo ra đầu ra văn bản [81, 143] với một số công việc gần đây hỗ trợ việc hiểu và tạo ra hai phương thức - văn bản và hình ảnh [123, 125]. Xây dựng các mô hình tạo sinh với phạm vi bao phủ rộng hơn về các phương thức, đặc biệt khi huấn luyện từ đầu, vẫn là một thách thức mở.

Trong công việc này, chúng tôi trình bày U NIFIED -IO 2, một mô hình đa phương thức lớn (LMM) có thể mã hóa văn bản, hình ảnh, âm thanh, video, và các chuỗi xen kẽ và tạo ra văn bản, hành động, âm thanh, hình ảnh, và nhãn thưa thớt hoặc dày đặc. Nó có thể đưa ra các phản hồi đa phương thức tự do và xử lý các nhiệm vụ chưa thấy trong quá trình huấn luyện thông qua việc tuân theo hướng dẫn. U NIFIED -IO 2 chứa 7 tỷ tham số và được tiền huấn luyện từ đầu trên một loạt dữ liệu đa phương thức rộng lớn - 1 tỷ cặp hình ảnh-văn bản, 1 nghìn tỷ token văn bản, 180 triệu clip video, 130 triệu hình ảnh & văn bản xen kẽ, 3 triệu tài sản 3D, và 1 triệu quỹ đạo tác nhân. Chúng tôi tiếp tục tinh chỉnh hướng dẫn cho mô hình với một corpus đa phương thức khổng lồ bằng cách kết hợp hơn 120 bộ dữ liệu bao gồm 220 nhiệm vụ trên thị giác, ngôn ngữ, âm thanh, và hành động.

Dữ liệu tiền huấn luyện và tinh chỉnh hướng dẫn của chúng tôi, tổng cộng hơn 600 terabyte, đặt ra những thách thức đáng kể cho việc huấn luyện do tính đa dạng và khối lượng của nó. Để hiệu quả tạo điều kiện cho các tín hiệu học tự giám sát trên nhiều phương thức, chúng tôi phát triển một mục tiêu hỗn hợp các bộ khử nhiễu đa phương thức mới kết hợp khử nhiễu và tạo sinh trên các phương thức. Chúng tôi cũng phát triển đóng gói động - một triển khai hiệu quả cung cấp sự gia tăng 4 lần về thông lượng huấn luyện để đối phó với các chuỗi có độ biến thiên cao. Để vượt qua các vấn đề về ổn định và khả năng mở rộng trong huấn luyện, chúng tôi đề xuất áp dụng các thay đổi kiến trúc chính, bao gồm embedding xoay 2D, chuẩn hóa QK, và các cơ chế attention cosine có tỷ lệ trên bộ tái lấy mẫu perceiver. Đối với tinh chỉnh hướng dẫn, chúng tôi đảm bảo mỗi nhiệm vụ có một lời nhắc rõ ràng, hoặc sử dụng những cái hiện có hoặc tạo ra những cái mới. Chúng tôi cũng bao gồm các nhiệm vụ mở và tạo ra các nhiệm vụ tổng hợp cho các phương thức ít phổ biến hơn để tăng cường sự đa dạng của nhiệm vụ và hướng dẫn.

Chúng tôi đánh giá U NIFIED -IO 2 trên hơn 35 bộ dữ liệu trên các phương thức khác nhau mà nó hỗ trợ. Mô hình đơn lẻ của chúng tôi thiết lập tiêu chuẩn mới trên benchmark GRIT [66], bao gồm các nhiệm vụ đa dạng như ước tính điểm khóa và ước tính pháp tuyến bề mặt. Trên các nhiệm vụ thị giác & ngôn ngữ, nó ngang bằng hoặc vượt trội so với hiệu suất của nhiều VLM được đề xuất gần đây tận dụng các LLM đã được tiền huấn luyện. Trên tạo hình ảnh, nó vượt trội so với đối thủ cạnh tranh gần nhất [174] tận dụng mô hình stable diffusion đã được tiền huấn luyện [154], đặc biệt về mặt độ trung thực theo các chỉ số được định nghĩa trong [76]. Nó cũng cho thấy hiệu quả trong các nhiệm vụ video, ngôn ngữ tự nhiên, âm thanh, và AI nhúng, thể hiện tính linh hoạt mặc dù phạm vi khả năng rộng lớn. Hơn nữa, U NIFIED -IO 2 có thể tuân theo các hướng dẫn tự do, bao gồm cả những hướng dẫn mới. Hình 1 cung cấp cái nhìn tổng quan về cách nó xử lý các nhiệm vụ khác nhau. Các ví dụ thêm, cùng với mã nguồn và mô hình, có thể truy cập trên trang web dự án của chúng tôi.

--- TRANG 3 ---
2. Công việc liên quan
Được truyền cảm hứng bởi sự thành công của các mô hình ngôn ngữ như các hệ thống xử lý văn bản đa mục đích [20, 122, 177], gần đây đã có một làn sóng các hệ thống đa phương thức cố gắng đạt được khả năng đa mục đích tương tự với các phương thức bổ sung. Một phương pháp phổ biến là sử dụng bộ mã hóa thị giác để xây dựng các tính năng cho hình ảnh đầu vào và sau đó một bộ chuyển đổi để ánh xạ những tính năng đó thành các embedding có thể được sử dụng như một phần của đầu vào cho LLM. Mạng sau đó được huấn luyện trên dữ liệu hình ảnh/ngôn ngữ ghép đôi để thích ứng LLM với các tính năng thị giác. Những mô hình này đã có thể thực hiện một số nhiệm vụ zero-shot hoặc với các ví dụ trong ngữ cảnh [109, 132, 178], nhưng thường là một giai đoạn thứ hai của tinh chỉnh hướng dẫn thị giác theo sau sử dụng các bộ ba hướng dẫn, đầu vào thị giác, và văn bản mục tiêu để tăng khả năng zero-shot [25, 34, 118, 119, 205, 218, 225].

Xây dựng trên thiết kế này, nhiều nhà nghiên cứu đã mở rộng phạm vi các nhiệm vụ mà những mô hình này có thể hỗ trợ. Điều này bao gồm việc tạo ra các mô hình có thể làm OCR [12, 220], nối đất thị giác [12, 26, 143, 189, 207, 212, 219], truy xuất hình ảnh-văn bản [97], ngôn ngữ bổ sung [112], nhiệm vụ AI nhúng [17, 135, 140, 152] hoặc tận dụng các hệ thống chuyên gia khác [52]. Các nỗ lực khác đã thêm các phương thức đầu vào mới. Điều này bao gồm đầu vào video [110, 126], âm thanh [80] hoặc cả hai [216]. PandaGPT [170] và ImageBind-LLM [69] sử dụng bộ mã hóa phổ quát ImageBind [56] để mã hóa nhiều loại phương thức đầu vào, và ChatBridge [222] sử dụng bộ mã hóa phổ quát tương tự dựa trên ngôn ngữ. Trong khi những nỗ lực này hiệu quả cho các nhiệm vụ hiểu, chúng không cho phép tạo sinh đa phương thức phức tạp và thường loại trừ các phương thức từ lâu được coi là trung tâm của thị giác máy tính (ví dụ, ImageBind không thể hỗ trợ chú thích thưa thớt của hình ảnh).

Ít công việc hơn đã xem xét tạo sinh đa phương thức. UNIFIED -IO [123], LaVIT [88], OFA [186], Emu [172] và CM3Leon [210] huấn luyện các mô hình để tạo ra các token mà VQ-GAN [49, 179] có thể giải mã thành hình ảnh, trong khi GILL [96], Kosmos-G [141] và SEED [53] tạo ra các tính năng mà mô hình khuếch tán có thể sử dụng, và JAM [4] kết hợp các mô hình tạo sinh ngôn ngữ và hình ảnh đã được tiền huấn luyện. UNIFIED -IO 2 cũng sử dụng VQ-GAN, nhưng hỗ trợ tạo sinh văn bản, hình ảnh, và âm thanh.

Nhìn chung, điều này cho thấy một xu hướng mạnh mẽ hướng tới việc mở rộng số lượng nhiệm vụ và phương thức được hỗ trợ. U NIFIED -IO 2 đẩy xu hướng này đến giới hạn, bao gồm các khả năng của những công việc trước đây này với ít ngoại lệ và khả năng tạo ra đầu ra trong nhiều phương thức hơn. Gần đây, CoDi [174] cũng đạt được khả năng tạo sinh any-to-any tương tự bằng cách sử dụng nhiều mô hình khuếch tán được huấn luyện độc lập và căn chỉnh các không gian embedding của chúng. U NIFIED -IO 2 có khả năng ngôn ngữ mạnh hơn và có thể thực hiện tốt trên nhiều nhiệm vụ hơn.

Một đặc điểm đáng chú ý của U NIFIED -IO 2 là mô hình được huấn luyện từ đầu thay vì được khởi tạo bằng LLM đã được tiền huấn luyện. Các công việc trước đây [114, 186, 188, 192] theo phương pháp này thường không được thiết kế để tạo ra các sinh phẩm phức tạp như phản hồi văn bản tự do, hình ảnh hoặc âm thanh, hoặc tuân theo hướng dẫn văn bản. So với các mô hình đa phương thức đa mục đích gần đây [81, 143, 210], UNIFIED -IO 2 có phạm vi nhiệm vụ và đầu ra rộng hơn đáng kể. Huấn luyện từ đầu có nghĩa là phương pháp có thể được tái tạo mà không cần giai đoạn sơ bộ tốn kém của việc tiền huấn luyện mô hình ngôn ngữ và là một sự phù hợp tự nhiên hơn cho cách con người học các phương thức đồng thời thông qua sự cùng xuất hiện của chúng, không phải từng cái một.

3. Phương pháp
Trong phần này, chúng tôi thảo luận về biểu diễn nhiệm vụ thống nhất (3.1), kiến trúc mô hình và các kỹ thuật để ổn định huấn luyện (3.2), mục tiêu huấn luyện đa phương thức (3.3) và các tối ưu hóa hiệu quả (3.4) được sử dụng trong U NIFIED -IO 2.

3.1. Biểu diễn nhiệm vụ thống nhất
UNIFIED -IO 2 xử lý tất cả các phương thức bằng một transformer encoder-decoder thống nhất, duy nhất [181]. Điều này được đạt được bằng cách mã hóa các đầu vào và đầu ra khác nhau - hình ảnh, văn bản, âm thanh, hành động, hộp v.v., thành các chuỗi token trong một không gian biểu diễn chung. Thủ tục mã hóa của chúng tôi tuân theo thiết kế của U NIFIED -IO [123], với một số sửa đổi để cải thiện hiệu suất và các bộ mã hóa và giải mã mới cho các phương thức bổ sung. Hình 2 cho thấy tổng quan về mô hình. Chi tiết về cách các phương thức được mã hóa được đưa ra dưới đây.

Văn bản, Cấu trúc thưa thớt, và Hành động. Đầu vào và đầu ra văn bản được token hóa sử dụng mã hóa byte-pair [161] từ LLaMA [177], mà chúng tôi chọn vì nó hỗ trợ các ký hiệu Unicode và bảo toàn khoảng trắng. Các cấu trúc thưa thớt như hộp giới hạn, điểm khóa, và tư thế camera được rời rạc hóa và sau đó mã hóa sử dụng 1000 token đặc biệt được thêm vào từ vựng [27, 123]. Các điểm được mã hóa với một chuỗi hai token như vậy (một cho x và một cho y), hộp được mã hóa với một chuỗi bốn token (góc trên trái và dưới phải), và các hình hộp 3D được biểu diễn bằng 12 token mã hóa tâm được chiếu, độ sâu ảo, kích thước hộp chuẩn hóa log, và xoay allocentric liên tục [16]. Đối với các nhiệm vụ nhúng, các hành động robot rời rạc [17] được tạo ra như các lệnh văn bản (ví dụ, "move ahead" để ra lệnh cho robot di chuyển tiến trong điều hướng). Các token đặc biệt được sử dụng để mã hóa trạng thái của robot, chẳng hạn như vị trí và xoay của nó. Chi tiết có trong Phụ lục B.1.

Hình ảnh và Cấu trúc dày đặc. Hình ảnh được mã hóa bằng Vision Transformer (ViT) đã được tiền huấn luyện [84]. Chúng tôi kết hợp các tính năng patch từ lớp thứ hai và thứ hai từ cuối của ViT để nắm bắt cả thông tin thị giác cấp thấp và cấp cao. Những tính năng này được truyền qua một lớp tuyến tính để có được các embedding có thể được sử dụng như một phần của chuỗi đầu vào cho transformer. Để tạo ra hình ảnh, chúng tôi sử dụng VQ-GAN [49] để chuyển đổi hình ảnh thành các token rời rạc. Những to-

--- TRANG 4 ---
Văn bản
Hình ảnhLịch sử hình ảnhÂm thanhLịch sử âm thanhViTEncoderViTEncoderASTEncoderASTEncoderEmb
Perceiver
PerceiverVQ-GAN DecoderVQ-GANDecoderspeechenvironmentalsoundimageSNSegmentationDepth  TextActionDetection
musicLinear
LinearDynamic PackingDynamic Unpacking512576128
64256512
1024
512BPEEncodeBPE Decode
Keypoint
Unified-IO 2

Hình 2. Kiến trúc U NIFIED -IO 2. Văn bản, hình ảnh, âm thanh đầu vào, hoặc lịch sử hình ảnh/âm thanh được mã hóa thành các chuỗi embedding được nối và sử dụng như đầu vào cho mô hình transformer encoder-decoder. Transformer đưa ra các token rời rạc có thể được giải mã thành văn bản, hình ảnh, hoặc clip âm thanh.

ken được thêm vào từ vựng và sau đó được sử dụng như chuỗi đầu ra mục tiêu để tạo ra hình ảnh. Để có chất lượng hình ảnh tốt hơn, chúng tôi sử dụng mô hình VQ-GAN dày đặc đã được tiền huấn luyện với kích thước patch 8×8 mã hóa hình ảnh 256×256 thành 1024 token với kích thước codebook là 16512.

Theo [123], chúng tôi biểu diễn các nhãn từng pixel, bao gồm độ sâu, pháp tuyến bề mặt, và mặt nạ phân đoạn nhị phân, như các hình ảnh RGB có thể được tạo ra hoặc mã hóa bằng khả năng tạo sinh và mã hóa hình ảnh của chúng tôi. Đối với phân đoạn, U NIFIED -IO 2 được huấn luyện để dự đoán mặt nạ nhị phân cho một lớp và hộp giới hạn. Toàn bộ hình ảnh có thể được phân đoạn bằng cách đầu tiên thực hiện phát hiện, và sau đó truy vấn mô hình cho mặt nạ phân đoạn cho mỗi hộp giới hạn và lớp được phát hiện. Xem Phụ lục B.1 để biết chi tiết.

Âm thanh. UNIFIED -IO 2 mã hóa lên đến 4.08 giây âm thanh thành một spectrogram (Xem Phụ lục B.1 và Bảng 8). Spectrogram sau đó được mã hóa bằng Audio Spectrogram Transformer (AST) đã được tiền huấn luyện [57], và các embedding đầu vào được xây dựng bằng cách nối các tính năng lớp thứ hai và thứ hai từ cuối từ AST và áp dụng một lớp tuyến tính giống như với ViT hình ảnh. Để tạo ra âm thanh, chúng tôi sử dụng ViT-VQGAN [208] để chuyển đổi âm thanh thành các token rời rạc. Vì không có codebase công khai, chúng tôi triển khai và huấn luyện ViT-VQGAN của riêng mình với kích thước patch 8×8 mã hóa spectrogram 256×128 thành 512 token với kích thước codebook là 8196.

Lịch sử hình ảnh và âm thanh. Chúng tôi cho phép lên đến bốn hình ảnh và đoạn âm thanh bổ sung được cung cấp như đầu vào, mà chúng tôi gọi là lịch sử hình ảnh hoặc âm thanh. Những yếu tố này cũng được mã hóa sử dụng ViT hoặc AST, nhưng chúng tôi sau đó sử dụng một bộ tái lấy mẫu perceiver [5], xem Bảng 8 cho các siêu tham số, để nén thêm các tính năng thành một số lượng token nhỏ hơn (32 cho hình ảnh và 16 cho âm thanh). Phương pháp này giảm đáng kể độ dài chuỗi và cho phép mô hình kiểm tra hình ảnh hoặc đoạn âm thanh ở mức chi tiết cao trong khi sử dụng các yếu tố trong lịch sử để làm ngữ cảnh. Lịch sử này được sử dụng để mã hóa các khung hình video trước đó, các đoạn âm thanh trước đó, hoặc hình ảnh tham chiếu cho các nhiệm vụ như tái tạo hình ảnh đa góc nhìn hoặc chỉnh sửa hình ảnh có điều kiện. Tám token đặc biệt được thêm vào từ vựng văn bản và được sử dụng để tham chiếu các yếu tố riêng lẻ trong những lịch sử này trong đầu vào hoặc đầu ra văn bản.

3.2. Kiến trúc
UNIFIED -IO 2 sử dụng kiến trúc transformer encoder-decoder. Tuy nhiên, chúng tôi quan sát thấy rằng việc sử dụng triển khai tiêu chuẩn theo U NIFIED -IO dẫn đến việc huấn luyện ngày càng không ổn định khi chúng tôi tích hợp thêm các phương thức. Như được hiển thị trong Hình 3 (a) và (b), chỉ huấn luyện trên tạo sinh hình ảnh (đường cong xanh lá) dẫn đến sự hội tụ ổn định của loss và gradient norm. Việc giới thiệu sự kết hợp của các nhiệm vụ hình ảnh và văn bản (đường cong cam) tăng nhẹ gradient norm so với một phương thức duy nhất, nhưng vẫn ổn định. Tuy nhiên, việc bao gồm tiếp theo của phương thức video (đường cong xanh) dẫn đến sự leo thang không kiểm soát của gradient norm. Khi một phiên bản XXL của mô hình này được huấn luyện trên tất cả các phương thức, như được hiển thị trong Hình 3 (c) và (d), loss bùng nổ sau 350k bước, và độ chính xác dự đoán token tiếp theo giảm đáng kể ở 400k bước. Để giải quyết điều này, chúng tôi bao gồm các thay đổi kiến trúc khác nhau làm ổn định đáng kể việc huấn luyện đa phương thức.

Embedding xoay 2D. Thay vì embedding vị trí tương đối [147], chúng tôi áp dụng embedding vị trí xoay (RoPE) [169] tại mỗi lớp transformer. Đối với các phương thức không phải văn bản, chúng tôi mở rộng RoPE đến các vị trí hai chiều: Đối với bất kỳ chỉ số 2D nào (i, j), chúng tôi chia mỗi embedding query và key của các đầu attention transformer thành một nửa và áp dụng các embedding xoay riêng biệt được xây dựng bởi mỗi trong hai tọa độ vào các nửa, xem Phụ lục B.2.

Chuẩn hóa QK. Chúng tôi quan sát thấy các giá trị cực lớn trong

--- TRANG 5 ---
05 0 0 01 0 0 0 01 5 0 0 02 0 0 0 0681 01 21 4L o s s05 0 0 01 0 0 0 01 5 0 0 02 0 0 0 0- 1
G r a d i e n t  N o r mi m a g e - t e x ti m a g ei m a g e - t e x t - v i d e o
1 001 01
1 02
1 0
1 0 0 k2 0 0 k3 0 0 k4 0 0 k1 21 41 61 82 02 2L o s s1 0 0 k2 0 0 k3 0 0 k4 0 0 k0 . 00 . 20 . 40 . 60 . 8A c c u r a c yT e x tI m a g eA u d i o
( a )( b )( d )( c )i m a g e - t e x ti m a g ei m a g e - t e x t - v i d e o

Hình 3. Trái: Loss huấn luyện (a) và gradient norm (b) trên các hỗn hợp phương thức khác nhau. Phải: Loss huấn luyện (c) và độ chính xác dự đoán token tiếp theo (d) của UIO-2 XXL trên tất cả các phương thức. Kết quả được thu được trước khi áp dụng các cải tiến kiến trúc được đề xuất.

0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 0 1 . 2 1 . 4
S t e p1 e 61 1 . 51 2 . 01 2 . 51 3 . 01 3 . 51 4 . 0 T r a i n i n g  L o s s1 B
3 B
7 B

Hình 4. Đường cong loss huấn luyện cho ba mô hình, được tiền huấn luyện với đóng gói động và kích thước batch 512.

các logit attention đa đầu khi bao gồm các phương thức hình ảnh và âm thanh, dẫn đến trọng số attention trở thành 0 hoặc 1 và góp phần vào sự không ổn định huấn luyện. Để giải quyết điều này, theo [38], chúng tôi áp dụng LayerNorm [10] cho các query và key trước tính toán attention dot-product.

Attention cosine có tỷ lệ. Chúng tôi sử dụng bộ tái lấy mẫu perceiver [86] để nén mỗi khung hình ảnh và đoạn âm thanh thành một số lượng token cố định. Chúng tôi thấy rằng ngay cả với chuẩn hóa QK, các logit attention trong perceiver có thể phát triển đến các giá trị cực đoan. Do đó, chúng tôi áp dụng chuẩn hóa nghiêm ngặt hơn trong perceiver bằng cách sử dụng attention cosine có tỷ lệ [121], làm ổn định đáng kể việc huấn luyện.

Để tránh sự không ổn định số, chúng tôi cũng kích hoạt logit attention float32. Cập nhật chung ViT và AST đã được tiền huấn luyện cũng có thể gây ra sự không ổn định. Do đó, chúng tôi đóng băng ViT và AST trong quá trình tiền huấn luyện và tinh chỉnh chúng ở cuối việc tinh chỉnh hướng dẫn. Hình 4 cho thấy rằng loss tiền huấn luyện cho mô hình của chúng tôi ổn định mặc dù tính không đồng nhất của các phương thức đầu vào và đầu ra.

3.3. Mục tiêu huấn luyện
Một mô hình đa phương thức mạnh mẽ phải được tiếp xúc với việc giải quyết các tập hợp vấn đề đa dạng trong quá trình tiền huấn luyện. UL2 [175] đã đề xuất Mixture of Denoisers (MoD), một quan điểm thống nhất để huấn luyện LLM, kết hợp các mục tiêu span corruption [147] và causal language modeling [19]. Được động lực bởi điều này, chúng tôi đề xuất một quan điểm tổng quát và thống nhất cho việc tiền huấn luyện đa phương thức.

Mixture of Denoisers đa phương thức. MoD sử dụng ba mô hình: [R] - span corruption tiêu chuẩn, [S] - causal language modeling, và [X] - span corruption cực đoan. Đối với mục tiêu văn bản, chúng tôi tuân theo các mô hình UL2. Đối với mục tiêu hình ảnh và âm thanh, chúng tôi định nghĩa hai mô hình tương tự: [R] - khử nhiễu có mặt nạ nơi chúng tôi ngẫu nhiên che x% các tính năng patch hình ảnh hoặc âm thanh đầu vào và giao cho mô hình nhiệm vụ tái tạo nó và [S] - nơi chúng tôi yêu cầu mô hình tạo ra phương thức mục tiêu chỉ dựa trên các phương thức đầu vào khác. Trong quá trình huấn luyện, chúng tôi thêm tiền tố vào văn bản đầu vào với một token phương thức ([Text], [Image], hoặc [Audio]) và một token mô hình ([R], [S], hoặc [X]) để chỉ ra nhiệm vụ.

Tự hồi quy với che động. Một vấn đề với khử nhiễu hình ảnh và âm thanh có mặt nạ theo cách tự hồi quy là rò rỉ thông tin ở phía decoder; xem Hình 5 (a). Token đầu vào hiện tại của decoder (3) được điều kiện bởi thông tin của encoder (2,5) và tất cả các token trước đó (s→2) để dự đoán mục tiêu (4). Kết quả là, token được dự đoán sẽ được điều kiện bởi 1 mặc dù nó đã được che trong encoder vì nó xuất hiện trong decoder, điều này sẽ đơn giản hóa nhiệm vụ và làm hại việc học biểu diễn. Đơn giản là che token trong decoder, như được hiển thị trong Hình 5 (b), tránh được rò rỉ thông tin này nhưng gây ra sự can thiệp giữa các nhiệm vụ tạo sinh và khử nhiễu. Ví dụ, chúng tôi thấy rằng huấn luyện chung với tạo sinh (50% MAE và 50% causal modeling) giảm đáng kể hiệu suất tạo sinh hình ảnh. Giải pháp của chúng tôi là che token trong decoder ngoại trừ khi dự đoán token đó, như được hiển thị trong Hình 5 (c), không can thiệp vào dự đoán nhân quả trong khi chủ yếu loại bỏ rò rỉ dữ liệu. Đối với tạo sinh hình ảnh và âm thanh, chúng tôi cũng sử dụng attention thưa thớt có mặt nạ hình dạng hàng, cột, và conv [148] trong decoder.

12345s123425(a)123452525(b)12345s23425(c)maskedtargetcurrentleakedEncoder InputDecoder InputDecoder Target

Hình 5. Các mô hình huấn luyện khác nhau trong mô hình hóa hình ảnh có mặt nạ: (a) tự hồi quy, (b) mask auto-encoder, (c) tự hồi quy với che động. Các mô hình được đề xuất của chúng tôi có thể duy trì tạo sinh nhân quả trong khi tránh rò rỉ thông tin trong decoder.

--- TRANG 6 ---
Âm thanh
Hình ảnh
Văn bản
Video
Quỹ đạo tác nhân
Tổng hợp
Hình ảnh/văn bản xen kẽ
Video
Cặp hình ảnh/văn bản
Quỹ đạo tác nhân
Tổng hợp
Hình ảnh/văn bản xen kẽ
Video
Cặp hình ảnh/văn bản
Văn bản
AI nhúng
Ngôn ngữ tự nhiên
Hiểu âm thanh
Ghi nhãn thưa thớt video
Hiểu video
Ghi nhãn dày đặc hình ảnh
Ghi nhãn thưa thớt hình ảnh
Hiểu hình ảnh
Tạo sinh âm thanh
Tạo sinh hình ảnh
QA nhúng
Tạo mục tiêu
Dự đoán khung hình/trạng thái tiếp theo
Dự đoán hành động
Mô hình hóa ngôn ngữ
Tuân theo hướng dẫn văn bản
Phụ đề âm thanh
Gắn thẻ âm thanh
Định vị âm thanh video
Định vị hành động video
Theo dõi video
Tuân theo hướng dẫn video
Hỏi đáp video
Phụ đề video
Gắn thẻ video
Optical Flow
Ước tính độ sâu
Phân đoạn biểu thức tham chiếu
Ước tính pháp tuyến bề mặt
Phân đoạn cục bộ
Phân đoạn ngữ nghĩa
Phát hiện điểm khóa
Phát hiện văn bản
3D
Biểu thức tham chiếu
Định vị đối tượng
Phát hiện đối tượng
QA cặp hình ảnh
Tuân theo hướng dẫn hình ảnh
Phụ đề vùng
Dự đoán mối quan hệ
Gắn thẻ hình ảnh
Phân loại vùng
Phụ đề hình ảnh
VQA
Âm thanh từ video
Âm thanh từ văn bản
Tổng hợp góc nhìn
Inpainting hình ảnh
Tạo khung hình tiếp theo
Chỉnh sửa hình ảnh
Chỉnh sửa hình ảnh có điều khiển
Hình ảnh từ văn bản
Văn bản
Hình ảnh
Âm thanh

Hình 6. Phân phối dữ liệu tiền huấn luyện và tinh chỉnh hướng dẫn. Các phân đoạn tỷ lệ với tỷ lệ lấy mẫu. Phần trong cho thấy phương thức mục tiêu, và phần ngoài cho thấy loại dữ liệu. Vui lòng tham khảo Hình 9 và Hình 11 trong Phụ lục cho các bộ dữ liệu cụ thể.

Mô hình | kích thước mô hình | kích thước mlp | lớp encoder | lớp decoder | đầu | Tham số
UIO-2 L | 1024 | 2816 | 24 | 24 | 16 | 1.1B
UIO-2 XL | 2048 | 5120 | 24 | 24 | 16 | 3.2B
UIO-2 XXL | 3072 | 8192 | 24 | 24 | 24 | 6.8B

Bảng 1. Biến thể kích thước của U NIFIED -IO 2.

3.4. Triển khai hiệu quả
Huấn luyện trên dữ liệu đa phương thức nặng dẫn đến độ dài chuỗi rất biến thiên cho đầu vào và đầu ra của transformer, cả vì các phương thức thường thiếu đối với các ví dụ riêng lẻ và vì số lượng token được sử dụng để mã hóa các phương thức cụ thể có thể thay đổi từ chỉ một vài token (cho một câu) đến 1024 token (cho một hình ảnh đầu ra). Để xử lý điều này một cách hiệu quả, chúng tôi sử dụng đóng gói, một quá trình trong đó các token của nhiều ví dụ được đóng gói thành một chuỗi duy nhất, và các attention được che để ngăn transformer cross-attend giữa các ví dụ.

Thông thường, đóng gói được thực hiện trong quá trình tiền xử lý, nhưng nó thách thức trong thiết lập của chúng tôi vì các encoder và decoder của chúng tôi không phải lúc nào cũng hỗ trợ nó. Thay vào đó, chúng tôi thực hiện đóng gói ngay trước và sau giai đoạn transformer encoder-decoder, cho phép các encoder/decoder phương thức chạy trên dữ liệu chưa đóng gói. Trong quá trình huấn luyện, chúng tôi sử dụng một thuật toán heuristic để sắp xếp lại dữ liệu đang được stream đến mô hình sao cho các ví dụ dài được ghép với các ví dụ ngắn mà chúng có thể được đóng gói cùng. Tối ưu hóa đóng gói cũng được khám phá trong [100], nhưng không trong thiết lập streaming. Đóng gói động dẫn đến sự gia tăng gần 4 lần trong thông lượng huấn luyện (Chi tiết trong Phụ lục B.3).

3.5. Optimizer
Chúng tôi sử dụng Adafactor [164] làm optimizer với linear warm-up cho 5,000 bước đầu tiên và decay tỷ lệ học của 1/√k. Chúng tôi huấn luyện với β1 = 0.9 và β2 = 1.0 − k^(-0.8), trong đó k là số bước. Chúng tôi sử dụng gradient clipping norm toàn cục với ngưỡng 1.0 và thấy rằng điều này rất quan trọng để ổn định huấn luyện. Bảng 1 đưa ra chi tiết về các mô hình khác nhau của chúng tôi. Đối với tất cả các mô hình, chúng tôi huấn luyện 3.0M bước - 1.5M cho tiền huấn luyện và 1.5M cho tinh chỉnh hướng dẫn, tương ứng. Chi tiết thêm trong Phụ lục B.4.

4. Dữ liệu đa phương thức
Một sự khác biệt quan trọng giữa U NIFIED -IO 2 và công việc trước đây là chúng tôi huấn luyện mô hình với một tập hợp dữ liệu đa phương thức đa dạng từ đầu. Điều này yêu cầu tuyển chọn dữ liệu đa phương thức chất lượng cao, mã nguồn mở cho cả tiền huấn luyện (4.1) và tinh chỉnh hướng dẫn (4.2).

4.1. Dữ liệu tiền huấn luyện
Dữ liệu tiền huấn luyện của chúng tôi đến từ các nguồn khác nhau và bao gồm nhiều phương thức. Chúng tôi cung cấp tổng quan cấp cao và chi tiết trong Phụ lục C.

NLP [33%]. Chúng tôi sử dụng các bộ dữ liệu có sẵn công khai được sử dụng để huấn luyện MPT-7B [176]. Bộ dữ liệu này nhấn mạnh văn bản tiếng Anh tự nhiên nhưng cũng chứa mã nguồn và markdown. Nó bao gồm văn bản từ bộ dữ liệu RedPajama [32], C4 [68], Wikipedia, và stack overflow. Chúng tôi tuân theo tỷ lệ được đề xuất bởi [176] và loại bỏ dữ liệu đa ngôn ngữ và khoa học.

Hình ảnh & Văn bản [40%]. Dữ liệu ghép đôi văn bản và hình ảnh đến từ LAION-400M [159], CC3M [163], CC12M [23], và RedCaps [42]. Để giúp huấn luyện phương thức lịch sử hình ảnh, chúng tôi cũng sử dụng dữ liệu hình ảnh/văn bản xen kẽ từ OBELICS [104]. Chúng tôi sử dụng hình ảnh cuối cùng làm đầu vào hình ảnh và các hình ảnh còn lại làm lịch sử hình ảnh. Các token đặc biệt được sử dụng để đánh dấu nơi những hình ảnh đó xuất hiện trong văn bản.

--- TRANG 7 ---
bên cạnh có một cơ sở gạch ở phía dưới hoàn chỉnh với …

1. Chọn phương thức mục tiêu [Audio][R] bên cạnh có một cơ sở gạch ở phía dưới hoàn chỉnh với …

2. Chọn phương thức đầu vào 4. Tạo mặt nạ đầu vào 5. Ghép với token tiền tố

[Audio][R] (Text, Image, Image History) Video Data Model Inputs 1. 2. 3. 3. …… (Audio) 3. Chọn mục tiêu Mask Audio Denoising [R] Model Targets

Hình 7. Xây dựng mẫu huấn luyện từ dữ liệu video cho đầu vào và mục tiêu của mô hình. Cho video, trước tiên chúng tôi trích xuất các khung hình video và spectrogram âm thanh và bản ghi tương ứng. Sau đó, dữ liệu trải qua quá trình lựa chọn ngẫu nhiên để xác định phương thức mục tiêu, phương thức đầu vào, mục tiêu huấn luyện, mặt nạ đầu vào v.v. Đầu vào và mục tiêu cuối cùng của mô hình được hiển thị ở góc phải trên.

Video & Âm thanh [25%]. Video cung cấp tín hiệu tự giám sát mạnh mẽ với tương quan cao giữa các kênh âm thanh và thị giác. Chúng tôi lấy mẫu dữ liệu âm thanh và video từ nhiều bộ dữ liệu công khai bao gồm YT-Temporal-1B [215], ACA V100M [105], AudioSet [54], WebVid-10M [13], HD-VILA-10M [200] và Ego4D [60].

3D & Nhúng [1%]. Để tiền huấn luyện 3D và nhúng tự giám sát, chúng tôi sử dụng CroCo [194] để tạo sinh và khử nhiễu cross-view; Objaverse [40] để tổng hợp góc nhìn; và các quỹ đạo ngẫu nhiên trong ProcTHOR [39] và Habitat [157] để dự đoán hành động và khung hình tiếp theo.

Tăng cường [1%]. Trong khi có rất nhiều dữ liệu không giám sát trên web cho hình ảnh, văn bản, video, và âm thanh, các tùy chọn bị hạn chế nhiều hơn cho các chú thích dày đặc và thưa thớt. Chúng tôi đề xuất giải quyết điều này thông qua tăng cường dữ liệu quy mô lớn. Chúng tôi xem xét hai loại tăng cường dữ liệu: 1. Dữ liệu phân đoạn được tạo tự động từ SAM [94] để huấn luyện mô hình phân đoạn một đối tượng cho một điểm hoặc hộp giới hạn. 2. Dữ liệu phát hiện patch tổng hợp giao cho mô hình nhiệm vụ liệt kê các hộp giới hạn của các hình dạng được thêm tổng hợp trong hình ảnh. Chúng tôi bổ sung huấn luyện mô hình để đưa ra tổng số patch trong hình ảnh để tiền huấn luyện khả năng đếm của nó.

Xây dựng mẫu huấn luyện. Trong quá trình tiền huấn luyện, hầu hết dữ liệu của chúng tôi chứa các phương thức khác nhau mà không có mục tiêu giám sát. Trong những trường hợp này, chúng tôi ngẫu nhiên chọn một trong các phương thức có mặt làm đầu ra mục tiêu. Sau đó, chúng tôi loại bỏ phương thức đó khỏi ví dụ hoặc thay thế nó bằng phiên bản bị hỏng. Các phương thức khác có thể có mặt trong ví dụ được giữ lại hoặc che ngẫu nhiên để buộc mô hình đưa ra dự đoán sử dụng bất kỳ thông tin nào còn lại. Hình 7 cho thấy một ví dụ khi sử dụng video chứa một chuỗi khung hình ảnh, âm thanh tương ứng, và bản ghi văn bản. Mẫu tiền huấn luyện được xây dựng bằng cách tuân theo quy trình: 1. chọn phương thức mục tiêu; 2. chọn phương thức đầu vào khác nào để giữ lại; 3. chọn mục tiêu; 4. tạo mặt nạ đầu vào ngẫu nhiên tùy thuộc vào nhiệm vụ khử nhiễu hoặc tạo sinh; 5. thêm token tiền tố chỉ ra nhiệm vụ.

4.2. Dữ liệu tinh chỉnh hướng dẫn
Tinh chỉnh hướng dẫn đa phương thức là quá trình then chốt để trang bị cho mô hình các kỹ năng và khả năng đa dạng trên các phương thức khác nhau và thậm chí thích ứng với các hướng dẫn mới và độc đáo. Chúng tôi xây dựng bộ dữ liệu tinh chỉnh hướng dẫn đa phương thức bằng cách kết hợp một loạt bộ dữ liệu và nhiệm vụ giám sát. Chúng tôi đảm bảo mỗi nhiệm vụ có một lời nhắc rõ ràng, hoặc sử dụng những cái hiện có hoặc viết mới. Chúng tôi cũng bao gồm các nhiệm vụ mở và tạo ra các nhiệm vụ tổng hợp cho các phương thức ít phổ biến hơn để tăng cường sự đa dạng nhiệm vụ và hướng dẫn. Hỗn hợp của chúng tôi bao gồm 220 nhiệm vụ được rút ra từ hơn 120 bộ dữ liệu bên ngoài. Chúng tôi cung cấp tổng quan cấp cao và ví dụ ở đây và để lại chi tiết trong Phụ lục D.

Ngôn ngữ tự nhiên [25.0%]. Đối với ngôn ngữ tự nhiên, chúng tôi sử dụng hỗn hợp từ FlanV2 [122] và nhiều bộ dữ liệu tuân theo hướng dẫn khác [33, 142]. Ngoài ra, chúng tôi tiếp tục tiền huấn luyện trên hỗn hợp NLP không giám sát của chúng tôi để giúp ngăn mô hình quên thông tin học được từ tiền huấn luyện trong giai đoạn tinh chỉnh hướng dẫn mở rộng.

Tạo sinh hình ảnh [17.6%]. Đối với tạo sinh hình ảnh từ văn bản, chúng tôi sử dụng cùng các cặp hình ảnh & văn bản mà chúng tôi đã sử dụng trong tiền huấn luyện. Chúng tôi cũng bao gồm dữ liệu từ [102, 103, 115] cung cấp chất lượng phụ đề tốt hơn. Chúng tôi bổ sung huấn luyện mô hình để tạo ra hình ảnh thông qua tổng hợp góc nhìn [40, 194], chỉnh sửa hình ảnh [18, 217], tạo sinh hình ảnh dựa trên phân đoạn [123] và inpainting [123].

Tạo sinh âm thanh [7.5%]. Điều này bao gồm các bộ dữ liệu văn bản-âm thanh với âm thanh trong tự nhiên [47, 93, 131], âm nhạc [2], và giọng nói của con người [85]. Chúng tôi cũng thêm dữ liệu tiền huấn luyện với nhiệm vụ dự đoán clip âm thanh tiếp theo trong video. Cụ thể hơn, chúng tôi chia âm thanh thành các phân đoạn và sau đó tạo ra một trong số chúng với cả văn bản và các phân đoạn trước đó làm đầu vào.

Hiểu hình ảnh [17.8%]. Chúng tôi bao gồm nhiều nguồn dữ liệu khác nhau từ visual question answering [6], gắn thẻ hình ảnh [41], phân loại vùng [102], và bộ dữ liệu với các phản hồi mở

--- TRANG 8 ---
Những điều tôi nên cẩn thận khi
tôi đến thăm nơi này là gì?

Khi thăm bến tàu này trên mặt nước được bao quanh 
bởi khu vực có rừng cây, có một số yếu tố cần 
xem xét:

1. Tầm nhìn: Tầm nhìn xung quanh bến tàu có thể 
bị hạn chế, vì vậy điều quan trọng là phải cẩn thận 
khi đi bộ trên bến tàu để tránh bất kỳ sự thay đổi 
đột ngột nào về điều kiện ánh sáng hoặc vật cản 
trên bờ.

2. Mực nước: Có thể có lũ lụt hoặc 
sóng lớn có thể làm hỏng bến tàu gỗ 
hoặc ảnh hưởng đến sự an toàn của khu vực. 
Điều quan trọng là phải chú ý đến mực nước để ...

Vui lòng cung cấp một mô tả ngắn gọn về hình ảnh.

Một hình ảnh bến tàu trên mặt nước với cây cối 
ở phía sau.

Vui lòng cung cấp một mô tả chi tiết về hình ảnh 
và chia sẻ ấn tượng cá nhân của bạn về cảnh này.

Hình ảnh có một cầu tàu bằng gỗ trên một hồ 
đẹp với nhiều cây xanh bao quanh 
khu vực. Cảnh tượng mang lại bầu không khí 
yên bình và thanh thản. Ở phía xa của cảnh, 
có thể thấy một dãy núi trải dài 
ở khoảng cách. Ở giữa cảnh, ...

Làm cho hình ảnh ma quái. Làm cho cây cầu màu đỏ. Làm cho nó vào lúc nửa đêm. Làm cho nó vào lúc hoàng hôn. Tưởng tượng bạn là da Vinci.

Phát hiện núi. Phát hiện bến tàu.

Phân đoạn bến tàu. Pháp tuyến bề mặt

Loại bỏ cây cầu. Tạo nhạc về cảnh này. (gốc)

Tạo nhạc về cảnh này. (ma quái)

Hình 8. Mô hình duy nhất của chúng tôi có thể thực hiện vô số nhiệm vụ đa phương thức: phụ đề hình ảnh, tuân theo hướng dẫn tự do, chỉnh sửa hình ảnh, phát hiện đối tượng, phân đoạn ngữ nghĩa, pháp tuyến bề mặt, và tạo sinh âm thanh dựa trên hình ảnh, v.v. Ở đây, chúng tôi hiển thị đầu ra của mô hình cho nhiều lời nhắc khác nhau. Nhấp và để nghe các mẫu âm thanh tương ứng.

ended [119, 220]. Chúng tôi cũng bao gồm các bộ dữ liệu tinh chỉnh hướng dẫn đa phương thức M3IT [112] và MIMIC-IT [107].

Hiểu video [10.6%]. Chúng tôi bao gồm các nguồn dữ liệu từ phụ đề video [190, 199], gắn thẻ video [35, 111, 168], và hỏi đáp video [196, 198]. Chúng tôi cũng sử dụng các ví dụ từ M3IT [112] và MIMIC-IT [107] để tuân theo hướng dẫn video.

Hiểu âm thanh [10.6%]. Chúng tôi bao gồm các nguồn dữ liệu từ gắn thẻ âm thanh [24, 54], và phụ đề âm thanh [47, 93]. Chúng tôi cũng bao gồm dữ liệu từ phân loại hành động video [7] với âm thanh trong bộ dữ liệu.

Ghi nhãn thưa thớt hình ảnh [7.25%]. Những nhiệm vụ này yêu cầu đưa ra tọa độ thưa thớt dựa trên hình ảnh đầu vào. Chúng tôi chủ yếu xem xét phát hiện đối tượng [115], biểu thức tham chiếu [91], phát hiện 3D [16], dự đoán tư thế camera [40], phát hiện văn bản [183] và điểm khóa con người [115].

Ghi nhãn dày đặc hình ảnh [4.06%]. Chúng tôi thực hiện một số nhiệm vụ ghi nhãn hình ảnh, bao gồm ước tính pháp tuyến bề mặt [78, 204], ước tính độ sâu [138], và optical flow [21, 44]. Chúng tôi cũng huấn luyện mô hình của mình trên nhiều nhiệm vụ phân đoạn khác nhau, bao gồm phân đoạn ngữ nghĩa, phân đoạn định vị, và phân đoạn biểu thức tham chiếu.

Ghi nhãn thưa thớt video [3.42%]. Chúng tôi thực hiện phát hiện video [151], theo dõi đối tượng đơn [50, 79] và định vị hành động video [61].

AI nhúng [4.33%]. Đối với VIMA-Bench [87], chúng tôi sử dụng đầu vào hình ảnh làm quan sát ban đầu của môi trường và lịch sử hình ảnh cho các hình ảnh hoặc video trong lời nhắc. Chúng tôi thêm các bộ dữ liệu thao tác quy mô lớn [63, 127, 184] với điều khiển liên tục trong cả môi trường mô phỏng và thực tế. Chúng tôi cũng huấn luyện trên nhiệm vụ PointNav từ các cảnh Habitat Gibson.

Phân phối của dữ liệu tinh chỉnh hướng dẫn có trong Hình 6. Nhìn chung, hỗn hợp tinh chỉnh hướng dẫn của chúng tôi bao gồm 60% dữ liệu prompting, có nghĩa là các bộ dữ liệu giám sát kết hợp với lời nhắc. Để tránh quên thảm khốc, 30% dữ liệu được chuyển từ tiền huấn luyện. Ngoài ra, 6% là dữ liệu tăng cường nhiệm vụ chúng tôi xây dựng bằng cách tạo ra các nhiệm vụ mới sử dụng các nguồn dữ liệu hiện có, tăng cường các nhiệm vụ hiện có và tăng sự đa dạng nhiệm vụ. 4% còn lại bao gồm văn bản tự do để cho phép các phản hồi giống như trò chuyện.

5. Thí nghiệm
Trong phần này, chúng tôi đánh giá các mô hình đã được tiền huấn luyện và tinh chỉnh hướng dẫn trên một loạt nhiệm vụ đòi hỏi phân tích và tạo ra tất cả các phương thức: hình ảnh, video, âm thanh, văn bản, và hành động. Chúng tôi không thực hiện tinh chỉnh đặc thù cho nhiệm vụ trong bất kỳ thí nghiệm nào. Chi tiết về thiết lập thí nghiệm, chi tiết kết quả bổ sung, kết quả trên các nhiệm vụ ngôn ngữ tự nhiên, và các nghiên cứu bổ sung cho khả năng hướng dẫn của U NIFIED -IO 2 có trong Phụ lục E.

5.1. Đánh giá tiền huấn luyện
Chúng tôi chứng minh tính hiệu quả của việc tiền huấn luyện bằng cách đánh giá U NIFIED -IO 2 trên suy luận ngôn ngữ tự nhiên thông thường (HellaSwag [214]), tạo sinh hình ảnh từ văn bản (TIFA [76]) và tạo sinh âm thanh từ văn bản (AudioCaps [93]). Chúng tôi cũng đánh giá hiểu biết không gian và thời gian trên SEED-Bench [106], một benchmark để đánh giá toàn diện nhận thức và suy luận trên các phương thức hình ảnh và video. Bảng 2 cho thấy rằng U NIFIED -IO 2 đạt được hiệu suất có thể so sánh hoặc thậm chí tốt hơn trên cả các nhiệm vụ tạo sinh và hiểu so với các chuyên gia đặc thù cho nhiệm vụ [154] hoặc mô hình đa phương thức phổ quát [9].

--- TRANG 9 ---
Phương pháp | HellaSwag ↑ | TIFA ↑ | SEED-S ↑ | SEED-T ↑ | AudioCaps ↓
LLaMA-7B [177] | 76.1 | - | - | - | -
OpenLLaMa-3Bv2 [55] | 52.1 | - | - | - | -
SD v1.5 [154] | - | 78.4 | - | - | -
OpenFlamingo-7B [9] | - | - | 34.5 | 33.1 | -
UIO-2 L | 38.3 | 70.2 | 37.2 | 32.2 | 3.08
UIO-2 XL | 47.6 | 77.2 | 40.9 | 34.0 | 3.10
UIO-2 XXL | 54.3 | 78.7 | 40.7 | 35.0 | 3.02

Bảng 2. Hiệu suất zero-shot trên hoàn thiện câu thông thường (HellaSwag [214]), tạo sinh hình ảnh từ văn bản (TIFA [76]), hiểu không gian và thời gian (Seed-Bench [106]), và tạo sinh âm thanh từ văn bản (AudioCaps [93]).

Phương pháp | Cat. | Loc. | Vqa | Ref. | Seg. | KP | Norm. | All
Ablation
UIO-2 L | 70.1 | 66.1 | 67.6 | 66.6 | 53.8 | 56.8 | 44.5 | 60.8
UIO-2 XL | 74.2 | 69.1 | 69.0 | 71.9 | 57.3 | 68.2 | 46.7 | 65.2
UIO-2 XXL | 74.9 | 70.3 | 71.3 | 75.5 | 58.2 | 72.8 | 45.2 | 66.9
Test
GPV-2 [89] | 55.1 | 53.6 | 63.2 | 52.1 | - | - | - | -
UIO XL[123] | 60.8 | 67.1 | 74.5 | 78.9 | 56.5 | 67.7 | 44.3 | 64.3
UIO-2 XXL | 75.2 | 70.2 | 71.1 | 75.5 | 58.8 | 73.2 | 44.7 | 67.0

Bảng 3. Kết quả trên tập ablation và test của GRIT [66].

Mặc dù đa nhiệm vụ mở rộng, kết quả trên HellaSwag cho thấy rằng U NIFIED -IO 2 có khả năng mô hình hóa ngôn ngữ giữa các mô hình ngôn ngữ điển hình 3B và 7B. Điều này có thể do mô hình thấy ít token hơn rất nhiều so với các LLM dựa trên ngôn ngữ - khoảng 250 tỷ token tổng cộng. Kết quả định tính của tiền huấn luyện có trong Phụ lục E.1.

5.2. Kết quả GRIT
Chúng tôi đánh giá trên General Robust Image Task (GRIT) Benchmark [66], bao gồm bảy nhiệm vụ: phân loại, định vị, VQA, biểu thức tham chiếu, phân đoạn thực thể, điểm khóa, và ước tính pháp tuyến bề mặt. Hoàn thành tất cả 7 nhiệm vụ yêu cầu hiểu hình ảnh, văn bản, và đầu vào thưa thớt và tạo ra văn bản, thưa thớt, và đầu ra dày đặc. Mặc dù đây là một tập con của các phương thức mà U NIFIED -IO 2 hỗ trợ, chúng tôi đánh giá trên GRIT vì nó cung cấp một benchmark tiêu chuẩn hóa và toàn diện về tập hợp khả năng này. Xem Phụ lục E.3 cho chi tiết suy luận bổ sung trên GRIT.

Kết quả được hiển thị trong Bảng 3. Nhìn chung, U NIFIED -IO 2 là tiên tiến trên GRIT, vượt qua mô hình tốt nhất trước đây, U NIFIED -IO, 2.7 điểm. Trên các nhiệm vụ riêng lẻ, chúng tôi có thể quan sát được các lợi ích trong định vị (3 điểm), phân loại (14 điểm), phân đoạn (2 điểm), và điểm khóa (5 điểm). Trên VQA, các đánh giá GRIT của chúng tôi cho thấy UNIFIED -IO 2 tốt hơn trên các câu hỏi cùng nguồn (84.6 vs. 81.2), cho thấy khoảng cách là do hiệu suất giảm trên các câu hỏi nguồn mới được xây dựng từ Visual Genome; xem Phụ lục E.3 để thảo luận thêm. Mặc dù hơi thua U NIFIED -IO, U NIFIED -IO 2

vẫn có được điểm số biểu thức tham chiếu mạnh mẽ so sánh thuận lợi với công việc trước đây về các mô hình đa phương thức tổng quát, xem Bảng 5. Vượt qua U NIFIED -IO trong khi cũng hỗ trợ tạo sinh hình ảnh và văn bản chất lượng cao hơn nhiều, cùng với nhiều nhiệm vụ và phương thức hơn, minh họa khả năng đa nhiệm vụ ấn tượng của mô hình chúng tôi. U NIFIED -IO 2 thậm chí duy trì hiệu suất tổng thể tốt hơn với mô hình 3 tỷ tham số (65.2 vs. 64.5), gần bằng kích thước với U NIFIED -IO. Kết quả ablation cho thấy hiệu suất trung bình, và tất cả các nhiệm vụ riêng lẻ đều cải thiện theo kích thước mô hình, cho thấy U NIFIED -IO 2 hưởng lợi từ quy mô.

5.3. Kết quả tạo sinh
Bảng 4 cho thấy kết quả trên các nhiệm vụ yêu cầu tạo ra đầu ra hình ảnh, âm thanh, và hành động. Chúng tôi đánh giá sử dụng TIFA [76], đo lường độ trung thực với lời nhắc sử dụng các mô hình VQA và đã được chứng minh là tương quan tốt với đánh giá của con người, và FID [73] trên MS COCO [115]. Trên TIFA, chúng tôi thấy rằng U NIFIED -IO 2 ghi điểm gần với minDALL-E [37], và khoảng 10 điểm trước các mô hình tổng quát khác như CoDi [174] và Emu [172]. Chúng tôi cho rằng khả năng tạo sinh hình ảnh mạnh mẽ này là do việc tiền huấn luyện mở rộng và việc sử dụng VQ-GAN chi tiết. Chúng tôi bao gồm các ví dụ về kết quả tạo sinh của chúng tôi từ benchmark TIFA trong Phụ lục E.5. Điểm số FID của U NIFIED -IO 2 cao hơn một chút so với các mô hình được so sánh, mặc dù chúng tôi lưu ý rằng về mặt định tính các hình ảnh được tạo ra vẫn rất mượt mà và chi tiết.

Phương pháp | Hình ảnh | Âm thanh | Hành động
 | FID ↓ | TIFA ↑ | FAD ↓ | IS ↑ | KL ↓ | Succ. ↑
minDALL-E [37] | - | 79.4 | - | - | - | -
SD-1.5 [154] | - | 78.4 | - | - | - | -
AudioLDM-L [117] | - | - | 1.96 | 8.13 | 1.59 | -
AudioGen [101] | - | - | 3.13 | - | 2.09 | -
DiffSound [203] | - | - | 7.75 | 4.01 | 2.52 | -
VIMA [87] | - | - | - | - | - | 72.6
VIMA-IMG [87] | - | - | - | - | - | 42.5
CoDi [174] | 11.26 | 71.6 | 1.80 | 8.77 | 1.40 | -
Emu [172] | 11.66 | 65.5 | - | - | - | -
UIO-2 L | 16.68 | 74.3 | 2.82 | 5.37 | 1.93 | 50.2
UIO-2 XL | 14.11 | 80.0 | 2.59 | 5.11 | 1.74 | 54.2
UIO-2 XXL | 13.39 | 81.3 | 2.64 | 5.89 | 1.80 | 56.3

Bảng 4. Kết quả trên tạo sinh hình ảnh từ văn bản (MS COCO [115] và TIFA [76]), tạo sinh âm thanh từ văn bản (AudioCaps [93]) và tạo sinh hành động (VIMA-Bench [87]).

Đối với tạo sinh âm thanh từ văn bản, chúng tôi đánh giá trên tập test AudioCaps [93]. AudioCaps bao gồm các clip âm thanh 10 giây, trong khi mô hình của chúng tôi có thể tạo ra âm thanh 4.08 giây tại một thời điểm, vì vậy chúng tôi không thể thực hiện đánh giá trực tiếp trên benchmark này. Thay vào đó, chúng tôi tạo ra một đoạn âm thanh dựa trên mô tả văn bản và các đoạn âm thanh trước đó làm đầu vào bổ sung

--- TRANG 10 ---
Phương pháp | VQAv2 | OKVQA | SQA | SQAI | Tally-QA | RefCOCO | RefCOCO+ | RefCOCO-g | COCO-Cap. | POPE | SEED | MMB

InstructBLIP (8.2B) | - | - | - | 79.5 | 68.2† | - | - | - | 102.2 | - | 53.4 | 36
Shikra (7.2B) | 77.4 | 47.2 | - | - | - | 87.0 | 81.6 | 82.3 | 117.5 | 84.7 | - | 58.8
Ferret (7.2B) | - | - | - | - | - | 87.5 | 80.8 | 83.9 | - | 85.8 | - | -
Qwen-VL (9.6B) | 78.8 | 58.6 | - | 67.1∗ | - | 89.4 | 83.1 | 85.6 | 131.9 | - | 38.2 | -
mPLUG-Owl2 (8.2B) | 79.4 | 57.7 | - | 68.7∗ | - | - | - | - | 137.3 | 86.2 | 57.8 | 64.5
LLaVa-1.5 (7.2B) | 78.5 | - | - | 66.8∗ | - | - | - | - | - | 85.9 | 58.6 | 64.3
LLaVa-1.5 (13B) | 80.0 | - | - | 71.6∗ | 72.4† | - | - | - | - | 85.9 | 61.6 | 67.7
Single Task SoTA | 86.0 [29] | 66.8 [77] | 90.9 [119] | 90.7 [34] | 82.4 [77] | 92.64 [202] | 88.77 [187] | 89.22 [187] | 149.1 [29] | - | - | -
UIO-2 L(1.1B) | 75.3 | 50.2 | 81.6 | 78.6 | 69.1 | 84.1 | 71.7 | 79.0♢ | 128.2 | 77.8 | 51.1 | 62.1
UIO-2 XL(3.2B) | 78.1 | 53.7 | 88.8 | 87.4 | 72.2 | 88.2 | 79.8 | 84.0♢ | 130.3 | 87.2 | 60.2 | 68.1
UIO-2 XXL(6.8B) | 79.4 | 55.5 | 88.7 | 86.2 | 75.9 | 90.7 | 83.1 | 86.6♢ | 125.4 | 87.7 | 61.8 | 71.5

Bảng 5. Kết quả thị giác-ngôn ngữ trên chín nhiệm vụ [1, 28, 59, 91, 124, 129, 130, 136, 209] và ba benchmark chỉ đánh giá [106, 113, 120]. Kết quả đánh dấu với ∗ là zero-shot và † được đánh giá với các bản phát hành mã nguồn mở, và ♢ chỉ ra rằng kết quả RefCOCO-g của chúng tôi là trên phân tách Google thay vì phân tách UMD.

[; xem Phụ lục E.6 để biết thêm chi tiết. Trong khi đây không phải là thiết lập có thể so sánh trực tiếp với công việc liên quan, nó vẫn cung cấp một thước đo định lượng hợp lý về khả năng tạo sinh âm thanh của chúng tôi. U NIFIED -IO 2 ghi điểm cao hơn các mô hình chuyên biệt ngoại trừ mô hình khuếch tán tiềm ẩn gần đây [117], cho thấy khả năng tạo sinh âm thanh cạnh tranh.

Đối với hành động, chúng tôi đánh giá sử dụng VIMA-Bench [87], một benchmark thao tác robot chứa 17 nhiệm vụ với lời nhắc văn bản-hình ảnh xen kẽ. Vì không gian hành động của VIMA là các primitive hành động, U NIFIED -IO 2 trực tiếp dự đoán tất cả các hành động cùng một lúc cho quan sát ban đầu và lời nhắc đa phương thức. Chúng tôi báo cáo tỷ lệ thành công trung bình cho giao thức đánh giá 4 cấp [87] và so sánh với chính sách VIMA nhân quả ban đầu với đầu vào tập trung vào đối tượng, cũng như VIMA-IMG, một chính sách giống Gato [152] với đầu vào hình ảnh như của chúng tôi.

5.4. Kết quả thị giác ngôn ngữ
Chúng tôi đánh giá hiệu suất thị giác ngôn ngữ và so sánh với các mô hình tổng quát thị giác/ngôn ngữ khác, tức là các mô hình cũng được thiết kế để thực hiện nhiều nhiệm vụ và có thể tuân theo hướng dẫn. Kết quả trên một tập hợp 12 benchmark thị giác/ngôn ngữ được hiển thị trong Bảng 5. Kết quả SoTA từ các mô hình chuyên biệt được hiển thị để tham khảo.

UNIFIED -IO 2 đạt được kết quả mạnh mẽ trên VQA, chỉ bị vượt qua bởi mô hình LLaVa lớn hơn nhiều 13B [118] trên VQA v2 [59], và đi trước tất cả các mô hình tổng quát khác trên ScienceQA [124] và TallyQA [1]. OK-VQA [130] là ngoại lệ. Chúng tôi đưa ra giả thuyết rằng vì nó yêu cầu kiến thức bên ngoài, việc tiền huấn luyện ngôn ngữ mở rộng là quan trọng cho nhiệm vụ này, và do đó hiệu suất giảm của chúng tôi là do UNIFIED -IO 2 không được tiền huấn luyện mở rộng trên văn bản như các mô hình ngôn ngữ chuyên dụng được sử dụng bởi Qwen-VL [12] và mPLUG-Owl2 [206].

Trên biểu thức tham chiếu, U NIFIED -IO 2 đi trước Shikra [26] và Ferret [207] và ngang bằng với điểm số đạt được bởi Qwen-VL. Trên phụ đề, U NIFIED -IO 2 cũng đạt được điểm số CIDEr mạnh mẽ [182] là 130.3, đi trước InstructBLIP [34] và Shikra nhưng sau Qwen-VL và mPLUG-Owl2.

Cuối cùng, chúng tôi đánh giá sử dụng ba benchmark chỉ đánh giá được đề xuất gần đây. MMB (MMBench [120]) kiểm tra nhiều khía cạnh của hiểu thị giác ngôn ngữ với các câu hỏi trắc nghiệm, trong khi SEED-Bench bổ sung kiểm tra hiểu video. Chúng tôi hiển thị phân tích chi tiết về điểm số trong Phụ lục E.4. Về điểm số tổng thể, U NIFIED -IO 2 có điểm số mạnh nhất của bất kỳ mô hình 7B nào trên bảng xếp hạng SEED-Bench¹, và ghi điểm cao nhất trên MMB 3.8 điểm. Đáng chú ý, nó vượt trội so với mô hình LLaVa-1.5 13B trên cả hai benchmark. U NIFIED -IO 2

¹tính đến ngày 11/17/23

cũng đạt 87.7 trên benchmark ảo giác đối tượng POPE [113], cho thấy rằng nó không rất dễ bị ảo giác đối tượng.

Nhìn chung, U NIFIED -IO 2 có thể ngang bằng hoặc vượt qua các mô hình tổng quát thị giác & ngôn ngữ khác trên những benchmark này mặc dù bao gồm nhiều phương thức hơn nhiều và hỗ trợ tạo sinh hình ảnh và âm thanh chất lượng cao. Điều này cho thấy rằng phạm vi khả năng rộng lớn của nó không phải với giá của hiệu suất thị giác/ngôn ngữ.

5.5. Kết quả video, âm thanh và khác
UNIFIED -IO 2 cho thấy hiệu suất hợp lý trên phân loại và phụ đề âm thanh và video, cũng như hỏi đáp video, như được hiển thị trong Bảng 6. Đáng chú ý, U NIFIED -IO 2 vượt trội so với BLIP-2 [109] và InstructBLIP [34] trên Seed-Bench Temporal [106] 8.5 điểm. U NIFIED -IO 2 cũng đạt hiệu suất tốt hơn trên Kinetics-Sounds [7] so với MBT [137], được huấn luyện chỉ trên bộ dữ liệu đó.

Phương pháp | Video | Âm thanh
 | Kinetics-400 [90] | V ATEX Caption [190] | MSR-VTT [199] | MSRVTT-QA [198] | MSVD-QA [198] | STAR [196] | SEED-T [106] | VGG-Sound [24] | AudioCaps [93] | Kinetics-Sounds [7]

MBT [137] | - | - | - | - | - | - | - | 52.3 | - | 85.0
CoDi [174] | - | - | 74.4 | - | - | - | - | - | 78.9 | -
ImageBind [69]∗ | 50.0 | - | - | - | - | - | - | 27.8 | - | -
BLIP-2 [109]∗ | - | - | - | 9.2 | 18.3 | - | 36.7 | - | - | -
InstructBLIP [34]∗ | - | - | - | 22.1 | 41.8 | - | 38.3 | - | - | -
Emu [172]∗∗ | - | - | - | 24.1 | 39.8 | - | - | - | - | -
Flamingo-9B [5]∗∗ | - | 57.4 | - | 29.4 | 47.2 | 41.2 | - | - | - | -
Flamingo-80B [5] | - | 84.2 | - | 47.4 | - | - | - | - | - | -
UIO-2 L | 68.5 | 37.1 | 44.0 | 39.6 | 48.2 | 51.0 | 37.5 | 37.8 | 45.7 | 86.1
UIO-2 XL | 71.4 | 41.6 | 47.1 | 39.3 | 50.4 | 52.0 | 45.6 | 44.2 | 45.7 | 88.0
UIO-2 XXL | 73.8 | 45.6 | 48.8 | 41.5 | 52.1 | 52.2 | 46.8 | 47.7 | 48.9 | 89.3

Bảng 6. Kết quả trên phân loại hành động, phụ đề video, VQA, hiểu thị giác, phân loại âm thanh, và phụ đề âm thanh. ∗: zero-shot, ∗∗: few-shot in-context learning.

Chúng tôi hiển thị kết quả phát hiện 3D đối tượng đơn trong Bảng 7. Mô hình của chúng tôi cho thấy kết quả khá tốt, tương tự như Cube-RCNN [16], trên benchmark Objectron [3]. Tuy nhiên, hiệu suất của nó giảm đáng kể trong các nhiệm vụ phát hiện 3D đa đối tượng, như những nhiệm vụ trên nuScenes [22] và Hypersim [153]. Điều này có thể là do chỉ 1.0% dữ liệu huấn luyện của chúng tôi tập trung vào phát hiện 3D. Một giải pháp tiềm năng có thể là kết hợp các kỹ thuật phát hiện 2D và 3D.

 | AP3D | AP3D@15 | AP3D@25 | AP3D@50
Cube-RCNN [16] | 50.8 | 65.7 | 54.0 | 22.5
UIO-2 L | 42.9 | 54.4 | 45.7 | 21.7
UIO-2 XL | 43.3 | 54.4 | 46.8 | 21.8
UIO-2 XXL | 42.4 | 54.0 | 45.6 | 20.9

Bảng 7. Kết quả phát hiện 3D đối tượng đơn trên Objectron [3].

Trong phát hiện đối tượng COCO, loại trừ các danh mục 'stuff', mô hình của chúng tôi đạt độ chính xác trung bình (AP) 47.2, với AP50 ở 57.7 và AP75 ở 50.0. Tuy nhiên, nó gặp khó khăn với các hình ảnh chứa nhiều đối tượng. Nghiên cứu trước đây, như Pix2Seq [27], cho thấy rằng các mô hình tự hồi quy đối mặt với những thách thức tương tự, có thể được cải thiện bằng tăng cường dữ liệu mở rộng. Tăng cường dữ liệu của mô hình chúng tôi trên phát hiện đối tượng tương đối hạn chế hơn.

Mô hình của chúng tôi cho thấy hiệu suất yếu trong ước tính độ sâu, với RMSE 0.623 trên bộ dữ liệu độ sâu NYUv2 [138]. Tuy nhiên, tinh chỉnh đặc biệt cho nhiệm vụ này đã cải thiện RMSE lên 0.423. Trong thí nghiệm của chúng tôi, chúng tôi đơn giản chuẩn hóa bản đồ độ sâu với giá trị độ sâu tối đa trong mỗi bộ dữ liệu. Do sự không tương thích của độ sâu ground-truth dày đặc trên các bộ dữ liệu khác nhau [150], mô hình của chúng tôi không thể nắm bắt tỷ lệ chính xác trong lời nhắc hiện tại, có thể được giải quyết bằng cách sử dụng chuẩn hóa và đánh giá metric tốt hơn.

Phụ lục E cho thấy trực quan hóa định tính của các nhiệm vụ khác, chẳng hạn như theo dõi đối tượng đơn, dự đoán trạng thái tương lai của thao tác robot, và tổng hợp góc nhìn 3D dựa trên hình ảnh, v.v.

6. Hạn chế
• Do hạn chế bộ nhớ, chúng tôi sử dụng các phiên bản cơ sở của các mô hình ViT và AST cho các tính năng hình ảnh và âm thanh trong suốt dự án. Sử dụng phiên bản lớn hơn của những bộ mã hóa hình ảnh và âm thanh này có thể cải thiện đáng kể hiệu suất.

• Trong khi tạo sinh hình ảnh của chúng tôi trung thực hơn so với các phương pháp dựa trên SD, chất lượng của nó không ngang bằng với mô hình stable diffusion. Ngoài ra, tạo sinh âm thanh của chúng tôi bị giới hạn ở khoảng 4 giây, hạn chế ứng dụng thực tế của đầu ra âm thanh.

• Tài nguyên tính toán hạn chế đã hạn chế việc khám phá các siêu tham số của mô hình. Có khả năng sử dụng kích thước batch lớn hơn đáng kể có thể tăng cường hiệu suất của mô hình.

• Mô hình của chúng tôi kém đáng tin cậy hơn nhiều cho các phương thức như độ sâu, video hoặc khi yêu cầu khả năng chuyên biệt hơn như phát hiện đối tượng 3D, v.v. Điều này có thể do sự đa dạng hạn chế của các nhiệm vụ chúng tôi có trong những lĩnh vực này.

• Cải thiện chất lượng dữ liệu của chúng tôi có thể tăng cường hiệu suất của mô hình. Tuy nhiên, mặc dù có những nỗ lực đáng kể, các lời nhắc do con người viết của chúng tôi vẫn thiếu đa dạng. Chúng tôi nhận thấy sự giảm đáng kể trong hiệu suất của mô hình khi xử lý các nhiệm vụ hướng dẫn mới, trái ngược với những nhiệm vụ mà nó được huấn luyện.

7. Kết luận
Chúng tôi đã giới thiệu U NIFIED -IO 2, mô hình đa phương thức tự hồi quy đầu tiên có khả năng hiểu và tạo ra hình ảnh, văn bản, âm thanh, và hành động. Mô hình này được huấn luyện từ đầu trên một loạt dữ liệu đa phương thức rộng lớn và được tinh chỉnh thêm với tinh chỉnh hướng dẫn trên một corpus đa phương thức khổng lồ. Chúng tôi đã phát triển nhiều thay đổi kiến trúc để ổn định việc huấn luyện đa phương thức và đề xuất một mục tiêu mixture of denoiser đa phương thức để hiệu quả tận dụng các tín hiệu đa phương thức. Mô hình của chúng tôi đạt được kết quả hứa hẹn trên một loạt nhiệm vụ rộng lớn. Chúng tôi cho thấy rằng việc chuyển từ LLM sang LMM cho phép các khả năng và cơ hội mới. Trong tương lai, chúng tôi muốn mở rộng U NIFIED -IO 2 từ mô hình encoder-decoder sang mô hình chỉ decoder. Ngoài ra, chúng tôi dự định mở rộng kích thước của mô hình, tăng cường chất lượng dữ liệu, và tinh chỉnh thiết kế mô hình tổng thể.

Lời cảm ơn Chúng tôi cảm ơn Klemen Kotar đã giúp thu thập dữ liệu tiền huấn luyện AI nhúng, Jonathan Frankle từ MosaicML đã đề xuất hỗn hợp dữ liệu tiền huấn luyện NLP, Jack Hessel về bộ dữ liệu hình ảnh & văn bản xen kẽ và Micheal Schmitz đã giúp hỗ trợ cơ sở hạ tầng tính toán. Chúng tôi cũng cảm ơn Tanmay Gupta về các cuộc thảo luận hữu ích, cũng như Hamish Ivison, và Ananya Harsh Jha về các cuộc thảo luận sâu sắc của họ

về thiết kế mô hình. Chúng tôi bổ sung cảm ơn Oscar Michel, Yushi Hu và Yanbei Chen về sự giúp đỡ chỉnh sửa bài báo, và Matt Deitke về sự giúp đỡ thiết lập trang web. Savya Khosla và Derek Hoiem được hỗ trợ một phần bởi giải thưởng ONR N00014-23-1-2383. Nghiên cứu này được thực hiện với cloud TPU từ Google's TPU Research Cloud (TRC).

Tài liệu tham khảo
[1] Manoj Acharya, Kushal Kafle, và Christopher Kanan. TallyQA: Answering Complex Counting Questions. Trong AAAI, 2019. 10, 29
[2] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. MusicLM: Generating Music From Text. arXiv preprint arXiv:2301.11325, 2023. 7, 29
[3] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, và Matthias Grundmann. Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations. Trong CVPR, 2021. 11, 37, 38
[4] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, và Barlas Oguz. Jointly Training Large Autoregressive Multimodal Models. arXiv preprint arXiv:2309.15564, 2023. 3
[5] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a Visual Language Model for Few-Shot Learning. Trong NeurIPS, 2022. 2, 4, 10, 22, 23, 37
[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, và Devi Parikh. VQA: Visual Question Answering. Trong ICCV, 2015. 7
[7] Relja Arandjelovic và Andrew Zisserman. Look, Listen and Learn. Trong ICCV, 2017. 8, 10, 11, 30
[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, và Charles Sutton. Program Synthesis with Large Language Models. arXiv preprint arXiv:2108.07732, 2021. 27
[9] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. arXiv preprint arXiv:2308.01390, 2023. 8, 9, 23
[10] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer Normalization. Trong NeurIPS Deep Learning Symposium, 2016. 5
[11] Gwangbin Bae, Ignas Budvytis, và Roberto Cipolla. Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation. Trong ICCV, 2021. 33
[12] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren Zhou. Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966, 2023. 3, 10, 34

[Tiếp tục với danh sách tài liệu tham khảo...]
