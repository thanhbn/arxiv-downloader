# 2304.08953.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2304.08953.pdf
# File size: 530361 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
From Words to Music: A Study of Subword Tokenization Techniques in Symbolic
Music Generation
Adarsh Kumar1and Pedro Sarmento2
1Indian Institute of Technology Kharagpur, India
2Queen Mary University of London, UK
adarshkumar712.ak@gmail.com, p.p.sarmento@qmul.ac.uk
Abstract
Subword tokenization has been widely successful
in text-based natural language processing (NLP)
tasks with Transformer-based models. As Trans-
former models become increasingly popular in
symbolic music-related studies, it is imperative
to investigate the efﬁcacy of subword tokeniza-
tion in the symbolic music domain. In this pa-
per, we explore subword tokenization techniques,
such as byte-pair encoding (BPE), in symbolic mu-
sic generation and its impact on the overall struc-
ture of generated songs. Our experiments are based
on three types of MIDI datasets: single track-
melody only, multi-track with a single instrument,
and multi-track and multi-instrument. We apply
subword tokenization on post-musical tokenization
schemes and ﬁnd that it enables the generation of
longer songs at the same time and improves the
overall structure of the generated music in terms of
objective metrics like structure indicator (SI), Pitch
Class Entropy, etc. We also compare two subword
tokenization methods, BPE and Unigram, and ob-
serve that both methods lead to consistent improve-
ments. Our study suggests that subword tokeniza-
tion is a promising technique for symbolic music
generation and may have broader implications for
music composition, particularly in cases involving
complex data such as multi-track songs.
1 Introduction
Subword tokenization is a widely used technique for text rep-
resentation in natural language processing (NLP). Such tok-
enization techniques based on the creation of subword tokens,
like byte pair enconding (BPE) [Sennrich et al. , 2016 ], Uni-
gram [Kudo, 2018 ]and WordPiece [Wuet al. , 2016 ], have be-
come ubiquitous in various NLP tasks. Attributed to their ef-
ﬁciency in modeling longer patterns, rather than simply char-
acters, these subword tokenization techniques became ex-
tremely successful with Transformer models like BERT [De-
vlinet al. , 2018 ]and GPT [Radford et al. , 2019 ], achieving
state-of-the-results in multiple text-based NLP applications.
Works like [Park et al. , 2020 ]and[Gall´e, 2019 ]have further
Figure 1: Example of similar frequently co-occurring structures
within the text (at character level) and musical representations
(MIDI-like, at event level).
shown the universality of its application across languages, not
just in English.
Inspired by the success of Transformer models in text-
NLP,1recent years have witnessed a shift in research work to-
wards leveraging Transformers [Vaswani et al. , 2017 ]in the
domain of symbolic music generation [Huang et al. , 2018;
Muhamed et al. , 2021; Guo et al. , 2022 ]. This shift can
be ascribed to the resemblance of symbolic music post-
musical-tokenization2to that of text tokens. With Trans-
formers’ extraordinary capability to model longer sequences,
we are able to generate coherent, adequate pieces of mu-
sic end-to-end [Hsiao et al. , 2021; Huang and Yang, 2020;
Neves et al. , 2022 ].
Despite all the success of the predecessors in improving
the state of music generation, these models are often ac-
cused of failing to entirely capture the repetitive structure
and overall musical development of songs [Daiet al. , 2022;
1For the sake of clarity and distinction, we will henceforth refer
to tasks related to the text in NLP as text-NLP.
2Musical Tokenization refers to tokenization of symbolic formats
like MIDI or GuitarPro with Music Tokenization such as REMI or
MIDI-like.arXiv:2304.08953v2  [cs.SD]  25 Apr 2023

--- PAGE 2 ---
Wu and Yang, 2020; Wu and Yang, 2022 ]. This becomes
more apparent as the structure of the music becomes complex
such as in the case of polyphonic music or multi-track music.
A reasonable explanation for this could be the signiﬁcantly
longer sequence of symbolic music tokens, which limits the
segment of a song visible to the Transformer model, hinder-
ing its understanding of the overall musical structure. As an
analogy, this would be equivalent to representing a piece of
text as a sequence of individual characters.
One possible solution to this problem is at the token level.
The idea is to group individual events into subgroups, similar
to subwords in text-NLP. There have been works like [Hsiao
et al. , 2021 ]and[Liuet al. , 2022 ], which tried to group mu-
sical events, exploiting the properties of musical structure for
certain MIDI-based datasets. However, most of these works
are dependent on the musical structure of certain datasets in-
volved, and hence cannot be extrapolated to other formats
easily. So, to the best of our knowledge, no work has been
done so far to assess the use of subword tokenization tech-
niques like BPE and Unigram that utilize the co-occurrence-
based structure of musical events, independent of the musical
structure of the training dataset itself.
We are therefore motivated to study whether the use of sub-
word tokenization can improve the overall musical structure
of generated songs, while at the same time being independent
of the dataset or format of symbolic music involved. In this
work, we speciﬁcally investigate the usefulness of subword
tokenization techniques like BPE [Sennrich et al. , 2016 ]and
Unigram [Kudo, 2018 ]in modeling the task of symbolic mu-
sic generation. Through our experiments, we try to answer
the following two primary questions:
• Q1: Can we use subword tokenization techniques to im-
prove the overall musical structure and musical quality
of the generated examples?
• Q2: How do these ﬁndings generalize between two dif-
ferent subword tokenization techniques, namely BPE
and Unigram?
In an effort to answer the above questions, our main contri-
butions through this paper are as follows:
• Creation and implementation of an evaluation environ-
ment to objectively assess the usefulness of subword to-
kenization techniques, namely BPE and Unigram, to im-
prove overall musical structure (in terms of quantitative
metrics like Pitch Class Entropy, Structureness Indica-
tors etc.), independent of data-speciﬁc factors like music
ﬁle formats (e.g. MIDI or GuitarPro);
• Establishing the efﬁciency of subword tokenization
across datasets, data formats and musical tokenization
techniques, with our study involving melody-only, poly-
phonic and multi-track datasets;
• Demonstrating the usefulness of subword tokens to-
wards facilitating longer pieces of generated music
within the same inference time.2 Background and Related Work
2.1 Subword Tokenization
Tokenization has become a fundamental process in NLP
which involves breaking the macroscopic units of text such as
‘words’ or ‘sentences’ into smaller units called tokens. Since
representing the much longer textual data as individual char-
acters is highly ineffective, the concept of ‘subword tokeniza-
tion’ was introduced [Mielke et al. , 2021 ]. It involves break-
ing the word or sentence into sub-words (a subsequence of
characters with length 1), which are then used to repre-
sent the data. Over the years, several subword tokenization
techniques have been introduced such as BPE [Gage, 1994;
Sennrich et al. , 2016 ], Unigram [Kudo, 2018 ], WordPiece
[Wuet al. , 2016 ]and SentencePiece [Kudo and Richardson,
2018 ]. Most of these techniques involve the selection of sub-
words based on their frequency within the training data.
Byte-Pair-Encoding (BPE) [Sennrich et al. , 2016 ]is a data
compression technique [Gage, 1994 ]which involves the re-
placement of the most frequent pair of bytes by a single, un-
used byte. In text-NLP, BPE is applied to subword units,
rather than bytes, by ﬁnding the most frequent character n-
grams in a text corpus and merging them into a single token.
This allows the model to learn a more ﬁne-grained representa-
tion of the language, especially for rare or out-of-vocabulary
words. The result is a variable-length subword vocabulary,
which balances the ability to capture complex language struc-
ture with reducing the risk of overﬁtting.
In contrast to BPE, Unigram [Kudo, 2018 ]is another sub-
word tokenization technique, which starts from a large vocab-
ulary and gradually trims the vocabulary towards a smaller
one. It involves a probabilistic Unigram language model,
which decides whether to keep a subword or not based on
its likelihood and loss function. Both these models have been
extensively used in various NLP applications such as [Wang
et al. , 2020 ],[Liuet al. , 2019 ]and[Conneau and Lample,
2019 ]. Furthermore, papers like [Gall´e, 2019 ]and[Park et
al., 2020 ]have shown that these subword tokenization tech-
niques can be applied effectively across languages. This mo-
tivates us to explore whether these results can be extended to
symbolic music as a language, and what impact it could have
on the overall musical structure of generated songs.
2.2 Symbolic Music Generation
Symbolic Music Generation involves representation of mu-
sic data from formats like MIDI or GuitarPro with symbols
or sequences of events which are then used to train gen-
erative models. This has been an extensively researched
area, where researchers are trying to come up with algo-
rithms and models that can generate music at par with hu-
man performance [Hiller, 2019 ]. Recently, the domain of
symbolic music generation has witnessed steady improve-
ments, mostly driven by advances in deep learning architec-
tures. Overall, approaches towards symbolic music genera-
tion with deep learning can be aggregated according to the
architecture used, namely Variational Autoencoder (V AEs)
models [Tan and Herremans, 2020 ], Generative Adversarial
Networks (GANs) [Dong and Yang, 2018 ], and models that
stem from natural language processing (NLP) ﬁeld, such as

--- PAGE 3 ---
Figure 2: Symbolic music processing pipeline with subword tokenization. Please note that, here, musical symbols refer to mapping of musical
events to unicode symbols or characters, which are then processed with subword tokenizer.
Recurrent Neural Networks (RNNs) [Meade et al. , 2019 ],
Long Short-Term Memory (LSTMs) [Sturm et al. , 2016 ],
or Transformers [Vaswani et al. , 2017 ]. As stated before,
the Transformer architecture [Vaswani et al. , 2017 ]is suit-
able for generating longer sequences when compared to pre-
vious approaches used by RNNs. Transformer-like sequence-
to-sequence models are able to learn the dependencies and
patterns among elements of a given sequence. The work
by[Huang et al. , 2019 ], the Music Transformer, pioneered
the application of the self-attention mechanism to generate
longer sequences of symbolic piano music. Other examples
include works like Musenet [Payne, 2019 ], in which a large-
scale Transformer model, GPT-2, was used to generate sym-
bolic multi-instrument music from different musical genres,
the Pop Music Transformer [Huang and Yang, 2020 ], which
uses Transformer-XL [Daiet al. , 2019a ]as a backbone ar-
chitecture and is able to generate pop piano symbolic mu-
sic with a better rhythmic structure, the Compound Word
Transformer [Hsiao et al. , 2021 ], that presents novel and
more efﬁcient ways of tokenizing symbolic music for train-
ing purposes, and GTR-CTRL [Sarmento et al. , 2023 ]ex-
plores genre and instrument conditioning using special con-
trol tokens with multi-track music dataset DadaGP, thereby
controlling overall musical structure of generated songs.
3 Methodology
In order to evaluate the usefulness of subword tokenization
and to answer the questions mentioned in Section 1, we de-
signed an empirical study where we objectively assess the
performance of models with and without subword tokeniza-
tion methods, while keeping all the other factors identical.
For clariﬁcation, we here refer to the model without subword
tokens as the ‘base model’, while the other models are named
according to the respective subword tokenization techniqueused while modeling (e.g. ‘BPE model’ as the model that
used BPE for subword tokenization).
3.1 Datasets and Music Tokenization Schemes
As mentioned earlier, we used three datasets for our experi-
ments, namely the Folk Songs dataset [Sturm et al. , 2016 ],
the MAESTRO dataset [Hawthorne et al. , 2019 ], and the
DadaGP dataset [Sarmento et al. , 2021 ]. Furthermore, for
the music tokenization procedure, we utilized REMI [Huang
and Yang, 2020 ], Midi-like [Oore et al. , 2018 ]and DadaGP
tokenization [Sarmento et al. , 2021 ]respectively, primar-
ily to demonstrate the compatibility of subword tokeniza-
tion with existent different music tokenization approaches.
Given that training the models such as Music Transformer
[Huang et al. , 2018 ]and Transformer-XL [Daiet al. , 2019b;
Huang and Yang, 2020 ]on such large datasets is very re-
source intensive, we restricted our study to subsets of each
dataset, using 1000 samples from the Folk Songs dataset, 400
samples from the MAESTRO dataset and 2000 songs from
the Rock genre in DadaGP. However, since similar settings
were kept for both the models with and without subword to-
kenization, our restriction does not impact any conclusions
from our results.
3.2 Data Processing
While there are several subword tokenization methods avail-
able, we focused our experiments on two of the most com-
mon ones, BPE and Unigram, primarily because of their ease
of applicability to any dataset. To leverage subword tokeniza-
tion with symbolic music, we ﬁrst converted songs from sym-
bolic format (MIDI or GuitarPro) to musical events using mu-
sic tokenization schemes such as REMI [Huang and Yang,
2020 ], following which we create a mapping, from musical
events to unicode symbols. Thus, we obtained a relatively
easy-to-process corpus of symbols, in which we assume each

--- PAGE 4 ---
Music Tokenization Original BPE Unigram
REMI 227 300 300
MIDI-Like 331 1000 1000
DadaGP 2104 5000 5000
Table 1: V ocabulary size of original tokens (using respective musical
tokenization), post-processing with BPE and Unigram.
song is a single entity similar to SentencePiece [Kudo and
Richardson, 2018 ]. We then trained subword tokenization
methods on the respective datasets to create a larger vocab-
ulary of subword tokens. Furthermore, we used this vocabu-
lary to process musical tokens of all songs in a given dataset.
A statistical summary of the original vocabulary size of mu-
sical tokens and subword tokens used in these experiments is
given in Table 1.
3.3 Experimental Conﬁguration
As stated before, in order to assess the independence of re-
sults in terms of improvements while using subword tok-
enization methods against factors like type of dataset, mu-
sic tokenization procedure, and model, we experimented with
three different combinations:
1. Folk Songs (monophonic, single instrument, MIDI for-
mat) + REMI tokenization + Music Transformer;
2. MAESTRO (polyphonic, single instrument, MIDI for-
mat) + MIDI-Like tokenization + Music Transformer;
3. DadaGP (polyphonic, multi-instrument, GuitarPro for-
mat) + DadaGP tokenization + Transformer-XL;
The key idea here is to experiment with subword tokeniza-
tion schemes in different settings and conﬁgurations and to
see if the results in terms of improvement hold true irrespec-
tive of the rest of the factors. Furthermore, the choice of
monophonic, polyphonic, and multi-track music, allows us
to evaluate the usefulness of subword tokenization for music
generation tasks with different levels of complexities.
4 Evaluation Metrics
To objectively evaluate the results of applying subword tok-
enization in music generation, we subdivided dedicated met-
rics into categories: musical quality and structure and efﬁ-
ciency in representation.
4.1 Musical Quality and Structure
Structureness Indicator ( SI):Proposed in [Wu and Yang,
2020 ],the structureness indicator (SI) is designed to capture
the structureness of music, induced by repetitions of musi-
cal content. It is based on the ﬁtness-scapeplot algorithm
[M¨uller et al. , 2011 ]and the self-similarity matrix (SSM)
[Foote, 1999 ]computed from ﬁtness , where the degree of rep-
etition is derived from SSM for a given segment ( i,j). Simi-
lar to [Wu and Yang, 2020 ], in our experiments, we used SI8
3,
SI15
8andSI15, in which SIu
l,landurepresent the lower and
upper bound of intervals of consideration (in seconds). We re-
spectively refer to these as SI-short ,SI-medium , and SI-long ,
as they are used to examine the short, medium, and long-termstructureness of generated songs. Here it is important to note
that a higher SI-xvalue doesn’t necessarily mean ‘better’ mu-
sic, since the generated samples may then be too repetitive.
Instead, a more valid assumption is that it is better the closer
it is to the real music (i.e. testing corpus), which will be the
basis of our evaluation.
Pitch Class Entropy ( H):Also described in [Wu and Yang,
2020 ], gives an insight about different pitches, and thereby
tonality used in a song. Here, the main idea is to calculate
entropy from a normalized 12-dimensional pitch class his-
togram (corresponding to 12 pitch classes C, C#, D, ... B)
and analyze how close these values are to the real values.
Groove Pattern Similarity ( GS):Another metric deﬁned
in[Wu and Yang, 2020 ], Groove Pattern Similarity helps in
measuring the rhythmic consistency within a song. It cal-
culates the pairwise similarity of each bar’s groove vector g
(indicating the position in bar with at least a note onset) as
1 HammingDist (ga; gb), across all pairs gaandgb. Simi-
lar to the other two metrics mentioned previously, in this case
also, the closer the values of groove similarity from the gen-
erated songs are to the real songs, the better.
4.2 Efﬁciency in Representation
Average Number of Tokens per Song: Within this metric
we measure the average number of tokens per song present
post-data processing, which are then fed into a Transformer
model for training. A more efﬁcient representation will be the
one, which has smaller average number of tokens per song.
Average Number of Tokens per Song for the Same Infer-
ence Time: In this metric, for a given dataset, we generate an
equal number of tokens (i.e. same inference)for each of the
three models we experimented with. After a post-conversion
process back to the original musical tokens, we compare the
average number of tokens generated in terms of the original
tokens. This metric helps us assess how efﬁcient the represen-
tation is in terms of generating longer music within the same
inference time. Hence, for this particular metric, the larger
the value, the better representation of data will be.
4.3 Other Metrics
NLL Loss: Negative log-likelihood is a common metric, of-
ten used to measure how well a model ﬁts the training dataset
[Huang et al. , 2018; Peracha, 2020; Hsiao et al. , 2021 ]. While
a relatively closer and smaller value of NLL represents a well-
ﬁtted model, in our case we observe that it is not a good met-
ric to compare performances across models, since the model
with subword tokenization has a higher number of parame-
ters. Furthermore, a lower NLL loss doesn’t necessarily mean
better generation quality. We have still added this to give an
idea of how well the BPE or the Unigram model ﬁts relative
to the base model.
5 Results
5.1 Experimental Settings
The experiments were conducted using HuggingFace [Wolf
et al. , 2020 ]and PyTorch implementations of Music Trans-

--- PAGE 5 ---
former3and Transformer-XL4. For musical tokenization, we
used the MidiTok [Fradet et al. , 2021 ]library, which we fur-
ther processed for subword tokenization using Huggingface’s
tokenizers5. For the ﬁrst two parts of our experiment (i.e.
Folk Songs and MAESTRO songs dataset involving the Mu-
sic Transformer model), we used a 3-layer Transformer archi-
tecture, with an embedding dimension 256, which we trained
on Google Colab Free Tier with a P100 16GB GPU machine.
For the last part (i.e. DadaGP with Transformer-XL), we used
the same architecture as in [Huang and Yang, 2020 ], training
the model on a 24GB Quadro RTX 6000 GPU. For the evalu-
ation, we used implementation of the metrics described in the
previous section in MusPy [Dong et al. , 2020 ]and MusDr6.
Finally, we evaluated the model performance by generating
20 songs for each model conﬁguration. Samples from the
generation can be accessed here.
5.2 Objective Evaluation
Results from the three distinct experiments can be seen in
tables 2, 3 and 4.
Metrics Real Original BPE Unigram
SI-short 0.4637 0.2707 0.3376 0.2712
SI-medium 0.4959 0.2759 0.3379 0.2719
SI-long 0.4543 0.2583 0.3340 0.2451
H 2.6011 2.6924 2.6754 2.6842
GS 0.9987 0.9984 0.9986 0.9985
Table 2: Results with the Folk Songs dataset.
Metrics Real Original BPE Unigram
SI-short 0.3228 0.5119 0.3880 0.3483
SI-medium 0.3066 0.4663 0.3334 0.2828
SI-long 0.2343 0.4173 0.3031 0.2205
H 3.0555 2.5152 2.8705 2.9297
GS 0.9917 0.9971 0.9942 0.9936
Table 3: Results with the MAESTRO songs dataset.
Metrics Real Base BPE Unigram
SI-short 0.5069 0.5110 0.5125 0.4894
SI-medium 0.5270 0.4219 0.4892 0.4539
SI-long 0.4972 0.2943 0.4397 0.3924
H 2.5842 2.0529 2.3084 2.4333
GS 0.9991 0.9994 0.9993 0.9992
Table 4: Results with the DadaGP dataset.
As can be observed from the tables, the use of subword
tokenization methods outperforms the base models by a sig-
3https://github.com/jason9693/MusicTransformer-pytorch
4https://github.com/YatingMusic/compound-word-transformer
5urlhttps://huggingface.co/docs/tokenizers/index
6https://github.com/slSeanWU/MusDrDataset Base BPE Unigram
FOLK 796 359 436
MAESTRO 12925 8831 8618
DadaGP 5332 2875 2954
Table 5: Average number of tokens per song in each representation.
niﬁcant margin in both cases (i.e. BPE and Unigram), in al-
most all the conﬁgurations, irrespective of the model, dataset,
data format, or music tokenization scheme used. The values
of SI-short, SI-medium, and SI-long, which closely resemble
real song data, indicate an overall improvement in the mu-
sical structure of songs using subword tokenization. Addi-
tionally, longer repetitive structures exhibit more signiﬁcant
improvements compared to shorter ones. The results sug-
gest that subword tokenization techniques have the potential
to model the musical structure better and can leverage the la-
tent co-occurring structure within musical tokens to improve
the quality of generated music.
That being said, the relatively smaller improvements in the
case of the Folk Songs dataset, suggest a correlation between
the complexity of the dataset being modeled and the perfor-
mance change, with more scope for improvements in the case
of datasets with higher complexities such as MAESTRO and
DadaGP. Adding to this, negligible improvements in terms
of SI for Unigram could be possible because the subword
tokens generated with Unigram are harder to learn by the
model, thereby collapsing to simpler tokens (which is a case
similar to base model, which also works on simpler tokens).
This again could be possibly attributed to the relative sim-
plicity in terms of structure and lesser scope of co-occurrence
when the dataset is melody-only. However, when we move
to more complicated datasets, we have musical co-occurring
structures like Chords, which improve the feasibility of using
Subword Tokenization Techniques.
Dataset Base BPE Unigram
FOLK 500 1307 994
MAESTRO 1000 1570 1534
DadaGP 1000 1437 1828
Table 6: Average number of tokens generated for same inference
time in a dataset (i.e. for same time taken to generate xbase model
(without subword tokenization) tokens, corresponding yandzto-
kens generated (in terms of original musical tokens) for BPE and
Unigram models respectively.)
Furthermore, the ﬁgures from Tables 5 and 6 demonstrate
the improved efﬁciency of representation with the use of sub-
word tokenization. This adds up to the advantage of using
subword tokenization with the base model, as it allows to
model of longer sequences within the same inference time
from a model. These results become particularly important in
the case of complex symbolic music datasets such as DadaGP.
This dataset, being complex in representation, requires that
much longer sequences are generated even for a short song
segment. Using subword tokenization with such datasets can

--- PAGE 6 ---
Dataset Model Train Time Base BPE Unigram
FOLK MT20mins 0.08 0.13 0.11
MAESTRO MT3hrs 2.58 3.16 3.62
DadaGP Tr-XL3days 0.10 0.11 0.11
Table 7: Ngeative Log-Likelihood Loss for the models (MT !Mu-
sic Transformer, Tr-XL !Transformer XL). As design choices,
here we decided the train time based on the complexity of the dataset
and the model in use, having kept the same time for all three experi-
ments.
allow shortening the sequences, thereby allowing longer mu-
sic to be inferred.
A comparison between the performance of models with
BPE or Unigram tokenization throughout the results suggests
that the improvements hold true in general for subword to-
kenization, leveraging the frequent cooccurrence of musical
events within songs. Though there are some localized differ-
ences on what method is more efﬁcient in terms of structural
modeling or musical quality, with one working better than the
other in certain cases or datasets, in all, these perform better
than the base model ‘without’ subword tokenization. This
answers our second question of how the results of our study
generalize for two different subword tokenization methodolo-
gies. Furthermore, this generalization is independent of the
constraints of a given musical structure of a particular dataset.
Another interesting aspect of our results is to observe
the improvement of results with the experiments involving
Transformer-XL i.e. beyond ﬁxed length modeling. While
the main purpose of using this model, as proposed in [Huang
and Yang, 2020 ]for music generation, is to model musical
structure beyond a ﬁxed length of the input, it is intriguing to
see the improvements with subword tokenization even in this
case, particularly with longer repetitive structureness. This
suggests while the dedicated architecture of Transformer-
XL is capable of modeling shorter musical structure bet-
ter, there is still a loss of information as the model propa-
gates through the windows of Transformer-XL input, lead-
ing to lesser long-term repetitiveness. However, on reducing
this length with subword tokenization, this loss can be re-
duced, thereby allowing improved representation of the mu-
sical structure, closer to the real data, we are trying to model.
Lastly, it is important to note the NLL loss performance of
the models we trained from Table 7. While the closer values
of loss functions in the case of the base, BPE and Unigram
suggest the model is able to model the dataset almost equiva-
lently in all the cases, differences in the objective evaluation
of generated musical quality, indicate that not all the infor-
mation is captured within NLL loss. Furthermore, it supports
our initial assumption (in section 4.3)that NLL is not an ap-
propriate metric to act upon conclusively over the model per-
formances. However, it can still provide an overview of how
well the model is ﬁtting the dataset, which in the case of us-
age of subword tokenization is almost the same as without
it.6 Discussion
In order to provide some qualitative insights from the gener-
ated content, we here present an individual subjective analy-
sis of some of the results. Despite good results in terms of
overall structureness presented in Table 3, we noticed that on
some occasions the model opts by resorting to rests (silence)
for a few measures. This can obviously be a desirable out-
come sometimes, but as is observable in Figure 3, the rests
from measure 24 to 28 seem to detract from the previous ﬂow
in terms of the musical idea behind it.
Figure 3: MuseScore screenshot of measures 17 to 34 of sample 19
from the MAESTRO BPE experiment.
Furthermore, despite the frequentist approach in subword
tokenization procedures, often giving emphasis to combina-
tions of words/subwords that are more common in a given
corpus, for the particular case of guitar-focused symbolic mu-
sic generation with the DadaGP dataset, it is interesting to
observe that tokens concerning guitar expressivity techniques
are preserved, despite its diminished frequency when com-
pared to note tokens.
Figure 4: GuitarPro screenshot of the ﬁrst ﬁve measures from sam-
ple 18 from the DadaGP BPE experiment. Only the distorted guitar
is visible.
As we can see from Figure 4, guitar-speciﬁc expressivity
techniques such as hammer-ons and pull-offs, bends, slides,
and vibrato, are adequately used.

--- PAGE 7 ---
Figure 5: GuitarPro screenshot of the ﬁrst nine measures from sam-
ple 10 from the DadaGP Unigram experiment. Only the distorted
guitar is visible.
In order to visually support the arguments made in Section
5.2 towards the improvement in terms of ‘structureness’ of the
generated examples from the subword tokenization models,
in Figure 5 we can observe that the model is able to call-
back to the motifs played on the ﬁrst two measures (i.e. same
pattern repeating in measures seven and eight). From measure
5, it is also interesting to observe that the model was able to
refer to the same pattern that was introduced in the ﬁrst two
measures but also incorporated a few connecting notes in its
ﬁrst beats.
Figure 6: Fitness scape plots for two generated samples from each
of Base, BPE and Unigram along with real songs corresponding to
the MAESTRO dataset. The x-axis represents the segment center
(in sec) and y-axis represents the segment length (in sec) for any
repetitive structure in music
A similar observation on improvements in terms of struc-
tural representation can be made if we analyze the ﬁtness
scape plots for the generated songs against real songs. A sam-
ple of such observation is shown in Figure 6 for MAESTRO
dataset samples. Contours in yellowish/brownish regions inscape plots represent the repetitive structure, with the lower
side implying short-term and the higher side implying long-
term repetitive structure. A better model is one that generates
songs having a scape-plot similar to that of real songs since
then the musical structure of the training dataset is captured
in a more accurate way. As we can observe from the scape
plots given in Figure 6, there is a much less long-term repet-
itive structure in the real data. Both BPE and Unigram have
their scape plots more similar to real songs, with lesser yel-
lowish regions on the upper side of plots, than that of the base
model. This suggests that the BPE and Unigram models cap-
ture the overall musical structure of real songs better than the
base model, with Unigram being the best among the three.
This observation is in coherence with Table 3, suggesting im-
provements with the use of subword tokenization methods.
7 Conclusion and Future Work
In this paper, we conducted an empirical study to assess the
usefulness of subword tokenization methods in symbolic mu-
sic generation. We objectively studied the change in perfor-
mance of music generation models with the use of subword
tokenization techniques such as BPE and Unigram while an-
swering the two fundamental questions posed in Section 1.
Our study not only shows that BPE and Unigram, as data
compression techniques, are not only able to represent the
data in a more efﬁcient manner but also improve the overall
structure of music generated with model-inculcating subword
tokenization. Furthermore, this result (and the trends in im-
provement) stands irrespective of the model, dataset, or mu-
sical tokenization used. Overall, from our study, we can con-
clude that the inclusion of subword tokenization in Symbolic
Music generation has a potential impact on the performance
of the model and structure of generated music, while allowing
longer music to be generated at the same time.
From this point onwards, future work can be multifaceted.
One direction could be to explore the impact of vocabu-
lary size on the model performance, i.e. how the vocal-
performance trade-off. Similar to text-NLP, where we usu-
ally have a vocabulary size in the range of 50k, we can in-
crease the vocab size here as well to see how performance
varies with changes in vocabulary size. Another interest-
ing direction could be to explore whether the knowledge of
Music theory can be leveraged in coordination with BPE or
Unigram-type techniques, to develop a hybrid version of sub-
word tokenization, involving concurrency (such as in Musical
BPE) musical events with adjacency of tokens. In all, there
is a large scope of future work that can be done, beyond this
study.
Ethical Statement
The training of large language models is an intensive com-
putational process that requires vast amounts of energy, re-
sulting in a signiﬁcant carbon footprint. Cloud providers are
increasingly offering services for training and hosting these
models, but not all of them have committed to being carbon
neutral, meaning they may contribute to greenhouse gas emis-
sions. This is an important consideration when selecting a

--- PAGE 8 ---
cloud provider for training models, as it has both environ-
mental and ethical implications.
To minimize the impact of training large language models,
one solution is to release pre-trained models. This approach
enables others to use these models without having to undergo
the energy-intensive process of training them from scratch.
By releasing pre-trained models, we aim to reduce the car-
bon footprint associated with training and make it easier for
others to use these models in a more sustainable manner. Ad-
ditionally, releasing pre-trained models may also encourage
collaboration and innovation in the ﬁeld, further advancing
the development of AI technology.
Acknowledgement
We would like to express our sincere gratitude to Dr. Yi-
Hsuan Yang, for his supervision and guidance throughout this
research project. Dr. Yang’s expert advice and feedback were
invaluable in shaping the direction and outcome of this study.
His constant support and motivation were crucial in keeping
us focused and inspired. We would also like to thank Dr.
Sourav Mukhopadhyay, IIT Kharagpur for giving us an op-
portunity to work on this project as Adarsh’s Master’s Thesis
Project.
References
[Conneau and Lample, 2019 ]Alexis Conneau and Guil-
laume Lample. Cross-lingual language model pretraining.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-
Buc, E. Fox, and R. Garnett, editors, Advances in Neural
Information Processing Systems , volume 32. Curran Asso-
ciates, Inc., 2019.
[Daiet al. , 2019a ]Zihang Dai, Zhilin Yang, Yiming Yang,
Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-XL: Attentive Language Models Be-
yond a Fixed-Length Context. In Proc. of the 57th Annual
Meeting of the Association for Computational Linguistics ,
2019.
[Daiet al. , 2019b ]Zihang Dai, Zhilin Yang, Yiming Yang,
Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond
a ﬁxed-length context, 2019.
[Daiet al. , 2022 ]Shuqi Dai, Huiran Yu, and Roger B. Dan-
nenberg. What is missing in deep music generation? a
study of repetition and structure in popular music, 2022.
[Devlin et al. , 2018 ]Jacob Devlin, Ming-Wei Chang, Ken-
ton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understand-
ing, 2018.
[Dong and Yang, 2018 ]Hao-Wen Dong and Yi-Hsuan Yang.
Convolutional Generative Adversarial Networks with Bi-
nary Neurons for Polyphonic Music Generation. In Proc.
of the 19th Int. Soc. for Music Information Retrieval Conf.
(ISMIR) , 2018.
[Dong et al. , 2020 ]Hao-Wen Dong, Ke Chen, Julian J.
McAuley, and Taylor Berg-Kirkpatrick. Muspy: A toolkitfor symbolic music generation. CoRR , abs/2008.01951,
2020.
[Foote, 1999 ]Jonathan Foote. Visualizing music and audio
using self-similarity. In Proceedings of the Seventh ACM
International Conference on Multimedia (Part 1) , MUL-
TIMEDIA ’99, page 77–80, New York, NY , USA, 1999.
Association for Computing Machinery.
[Fradet et al. , 2021 ]Nathan Fradet, Jean-Pierre Briot, Fa-
bien Chhel, Amal El Fallah Seghrouchni, and Nicolas
Gutowski. Miditok: A python package for midi ﬁle to-
kenization. In Extended Abstracts for the Late-Breaking
Demo Session of the 22nd International Society for Music
Information Retrieval Conference , 2021.
[Gage, 1994 ]Philip Gage. A new algorithm for data com-
pression. The C Users Journal archive , 12:23–38, 1994.
[Gall´e, 2019 ]Matthias Gall ´e. Investigating the effectiveness
of BPE: The power of shorter sequences. In Proceedings
of the 2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-
IJCNLP) , pages 1375–1381, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics.
[Guo et al. , 2022 ]Rui Guo, Ivor Simpson, Chris Kiefer,
Thor Magnusson, and Dorien Herremans. Musiac: An ex-
tensible generative framework for music inﬁlling applica-
tions with multi-level control, 2022.
[Hawthorne et al. , 2019 ]Curtis Hawthorne, Andriy Stasyuk,
Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang,
Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas
Eck. Enabling factorized piano music modeling and gen-
eration with the MAESTRO dataset. In International Con-
ference on Learning Representations , 2019.
[Hiller, 2019 ]Lejaren Hiller. IV . Music Composed With
Computers—A Historical Survey , pages 42–96. Cornell
University Press, Ithaca, NY , 2019.
[Hsiao et al. , 2021 ]Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng
Yeh, and Yi-Hsuan Yang. Compound Word Transformer:
Learning to Compose Full-Song Music Over Dynamic Di-
rected Hypergraphs. In Proc. of the AAAI Conf. on Artiﬁ-
cial Intelligence , 2021.
[Huang and Yang, 2020 ]Yu-Siang Huang and Yi-Hsuan
Yang. Pop Music Transformer: Beat-based Modeling and
Generation of Expressive Pop Piano Compositions. In
Proc. of the 28th ACM Int. Conf. on Multimedia , 2020.
[Huang et al. , 2018 ]Cheng-Zhi Anna Huang, Ashish
Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis
Hawthorne, Andrew M. Dai, Matthew D. Hoffman,
and Douglas Eck. An improved relative self-attention
mechanism for transformer with application to music
generation. CoRR , abs/1809.04281, 2018.
[Huang et al. , 2019 ]Cheng-Zhi Anna Huang, Ashish
Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon,
Curtis Hawthorne, Andrew M. Dai, Matthew D. Hoffman,
Monica Dinculescu, and Douglas Eck. Music Trans-
former: Generating Music with Long-term Structure. In

--- PAGE 9 ---
Proc. of the 7th Int. Conf. on Learning Representations ,
2019.
[Kudo and Richardson, 2018 ]Taku Kudo and John Richard-
son. Sentencepiece: A simple and language independent
subword tokenizer and detokenizer for neural text process-
ing. CoRR , abs/1808.06226, 2018.
[Kudo, 2018 ]Taku Kudo. Subword regularization: Improv-
ing neural network translation models with multiple sub-
word candidates. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 66–75, Melbourne, Australia,
July 2018. Association for Computational Linguistics.
[Liuet al. , 2019 ]Yinhan Liu, Myle Ott, Naman Goyal,
Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
A robustly optimized BERT pretraining approach. CoRR ,
abs/1907.11692, 2019.
[Liuet al. , 2022 ]Jiafeng Liu, Yuanliang Dong, Zehua
Cheng, Xinran Zhang, Xiaobing Li, Feng Yu, and
Maosong Sun. Symphony generation with permutation in-
variant language model. 2022.
[Meade et al. , 2019 ]Nicholas Meade, Nicholas Barreyre,
Scott C Lowe, and Sageev Oore. Exploring Conditioning
for Generative Music Systems with Human-Interpretable
Controls. 2019.
[Mielke et al. , 2021 ]Sabrina J. Mielke, Zaid Alyafeai, Eliz-
abeth Salesky, Colin Raffel, Manan Dey, Matthias Gall ´e,
Arun Raja, Chenglei Si, Wilson Y . Lee, Beno ˆıt Sagot, and
Samson Tan. Between words and characters: A brief his-
tory of open-vocabulary modeling and tokenization in nlp,
2021.
[Muhamed et al. , 2021 ]Aashiq Muhamed, Liang Li,
Xingjian Shi, Suri Yaddanapudi, Wayne Chi, Dylan Jack-
son, Rahul Suresh, Zachary C. Lipton, and Alexander J.
Smola. Symbolic music generation with transformer-gans.
In35th AAAI Conference on Artiﬁcial Intelligence, AAAI
2021 , 2021.
[M¨uller et al. , 2011 ]Meinard M ¨uller, Peter Grosche, and
Nanzhu Jiang. A segment-based ﬁtness measure for cap-
turing repetitive structures of music recordings. pages
615–620, 01 2011.
[Neves et al. , 2022 ]Pedro Neves, Jose Fornari, and Jo ˜ao
Florindo. Generating music with sentiment using
transformer-gans, 2022.
[Oore et al. , 2018 ]Sageev Oore, Ian Simon, Sander Diele-
man, Douglas Eck, and Karen Simonyan. This time with
feeling: Learning expressive musical performance. Neural
Computing and Applications , 2018.
[Park et al. , 2020 ]Kyubyong Park, Joohong Lee, Seongbo
Jang, and Dawoon Jung. An empirical study of tokeniza-
tion strategies for various Korean NLP tasks. In Pro-
ceedings of the 1st Conference of the Asia-Paciﬁc Chap-
ter of the Association for Computational Linguistics and
the 10th International Joint Conference on Natural Lan-
guage Processing , pages 133–142, Suzhou, China, De-
cember 2020. Association for Computational Linguistics.[Payne, 2019 ]Christine Payne. Musenet, 2019.
[Peracha, 2020 ]Omar A Peracha. Improving polyphonic
music models with feature-rich encoding. 2020.
[Radford et al. , 2019 ]Alec Radford, Jeff Wu, Rewon Child,
David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
[Sarmento et al. , 2021 ]Pedro Sarmento, Adarsh Kumar,
CJ Carr, Zack Zukowski, Mathieu Barthet, and Yi-Hsuan
Yang. DadaGP: a Dataset of Tokenized GuitarPro Songs
for Sequence Models. In Proc. of the 22nd Int. Soc. for
Music Information Retrieval Conf. , 2021.
[Sarmento et al. , 2023 ]Pedro Sarmento, Adarsh Kumar, Yu-
Hua Chen, CJ Carr, Zack Zukowski, and Mathieu Barthet.
Gtr-ctrl: Instrument and genre conditioning for guitar-
focused music generation with transformers, 2023.
[Sennrich et al. , 2016 ]Rico Sennrich, Barry Haddow, and
Alexandra Birch. Neural machine translation of rare words
with subword units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1715–1725, Berlin, Ger-
many, August 2016. Association for Computational Lin-
guistics.
[Sturm et al. , 2016 ]Bob L. Sturm, Jo ˜ao Felipe Santos, Oded
Ben-Tal, and Iryna Korshunova. Music transcription mod-
elling and composition using deep learning. In Proc. on
the 1st Conf. on Computer Simulation of Musical Creativ-
ity, 2016.
[Tan and Herremans, 2020 ]Hao Hao Tan and Dorien Her-
remans. Music FaderNets: Controllable Music Genera-
tion Based On High-Level Features via Low-Level Feature
Modelling. In Proc. of the 21st Int. Soc. for Music Infor-
mation Retrieval Conf. , 2020.
[Vaswani et al. , 2017 ]Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention Is All You
Need. In Proc. of the 31st Conf. on Neural Information
Processing Systems , 2017.
[Wang et al. , 2020 ]Ziyu Wang, Dingsu Wang, Yixiao
Zhang, and Gus Xia. Learning Interpretable Represen-
tation for Controllable Polyphonic Music Generation. In
Proc. of the 21st Int. Soc. for Music Information Retrieval
Conf. , Montr ´eal, Canada, 2020.
[Wolf et al. , 2020 ]Thomas Wolf, Lysandre Debut, Victor
Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, R ´emi Louf, Morgan
Funtowicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen
Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Huggingface’s
transformers: State-of-the-art natural language processing,
2020.
[Wu and Yang, 2020 ]Shih-Lun Wu and Yi-Hsuan Yang. The
jazz transformer on the front line: Exploring the shortcom-
ings of ai-composed music through quantitative measures.
CoRR , abs/2008.01307, 2020.

--- PAGE 10 ---
[Wu and Yang, 2022 ]Shih-Lun Wu and Yi-Hsuan Yang.
Compose & embellish: Well-structured piano perfor-
mance generation via a two-stage approach, 2022.
[Wuet al. , 2016 ]Yonghui Wu, Mike Schuster, Zhifeng
Chen, Quoc V . Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson,
Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo
Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason
Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg
Corrado, Macduff Hughes, and Jeffrey Dean. Google’s
neural machine translation system: Bridging the gap be-
tween human and machine translation, 2016.
