# Bộ dữ liệu Xúc giác, Thị giác và Ngôn ngữ cho Việc Căn chỉnh Đa phương thức

# Tóm tắt

Xúc giác là một phương thức cảm biến quan trọng đối với con người, nhưng nó chưa được tích hợp vào một mô hình ngôn ngữ sinh tự động đa phương thức. Điều này một phần là do khó khăn trong việc thu thập các nhãn ngôn ngữ tự nhiên cho dữ liệu xúc giác và sự phức tạp của việc căn chỉnh các bài đọc xúc giác với cả quan sát thị giác và mô tả ngôn ngữ. Như một bước hướng tới việc thu hẹp khoảng cách đó, công trình này giới thiệu một bộ dữ liệu mới gồm 44K cặp thị giác-xúc giác trong thực tế, với các nhãn ngôn ngữ tiếng Anh được chú thích bởi con người (10%) và các nhãn giả từ GPT-4V (90%). Chúng tôi sử dụng bộ dữ liệu này để đào tạo một bộ mã hóa xúc giác được căn chỉnh với thị giác-ngôn ngữ cho phân loại từ vựng mở và một mô hình xúc giác-thị giác-ngôn ngữ (TVL) để tạo văn bản bằng cách sử dụng bộ mã hóa đã đào tạo. Kết quả cho thấy rằng bằng cách tích hợp xúc giác, mô hình TVL cải thiện (+29% độ chính xác phân loại) việc căn chỉnh xúc giác-thị giác-ngôn ngữ so với các mô hình hiện có được đào tạo trên bất kỳ cặp phương thức nào trong số đó. Mặc dù chỉ có một phần nhỏ của bộ dữ liệu được gán nhãn bởi con người, mô hình TVL cho thấy sự hiểu biết về thị giác-xúc giác được cải thiện so với GPT-4V (+12%) và các mô hình thị giác-ngôn ngữ mã nguồn mở (+32%) trên một tiêu chuẩn hiểu biết xúc giác-thị giác mới. Mã nguồn và dữ liệu: https://tactile-vlm.github.io.

# 1. Giới thiệu

Hầu hết mọi nhận thức sinh học đều có bản chất đa phương thức (Bertelson & De Gelder, 2004; Turk, 2014; Bruck et al., 2022), cho phép các tác nhân suy luận và đưa ra quyết định dựa trên nhiều luồng thông tin. Nghiên cứu gần đây trong học biểu diễn đa phương thức nhân tạo đã khám phá việc liên kết các phương thức như thị giác, ngôn ngữ, âm thanh, nhiệt độ và hành động robot (Radford et al., 2021; Girdhar et al., 2023; Guzhov et al., 2021; Brohan et al., 2023; Radosavovic et al., 2023). Tuy nhiên, phương thức xúc giác vẫn còn ít được khám phá trong hiểu biết đa phương thức. Xúc giác cho phép con người phân biệt kết cấu bề mặt, vật liệu đối tượng, kích thước và lực tiếp xúc (Johansson & Flanagan, 2009; Dahiya et al., 2009; Klatzky & Lederman, 2003). Nhận thức xúc giác cũng đã được chứng minh là hữu ích trong các ứng dụng robot, đặc biệt là cho các nhiệm vụ thao tác giàu tiếp xúc (Lambeta et al., 2020; Dahiya et al., 2009; Calandra et al., 2018; Yuan et al., 2017; Dave et al., 2024; Qi et al., 2023).

Nhiều công trình cũng khám phá mối liên kết thị giác-xúc giác, xây dựng các bộ tạo đa phương thức và tận dụng việc đào tạo trước đa phương thức cho thuộc tính vật liệu, kết cấu bề mặt và phân loại vải trên một tập từ vựng đóng (Yang et al., 2022; Dave et al., 2024; Li & Adelson, 2013; Ojala et al., 2002; Kampouris et al., 2016; Yuan et al., 2018; Kerr et al., 2023). Tuy nhiên, nhận thức xúc giác của con người nắm bắt nhiều hơn là các mối liên kết xúc giác-thị giác; phương thức xúc giác nắm bắt thông tin ngữ nghĩa đa dạng và cho thấy sự tích hợp sâu sắc với ngôn ngữ (Schmidt et al., 2019; Speed et al., 2021; Miller et al., 2018; ajbarnett, 2023). Một rào cản chính đối với việc tích hợp xúc giác và ngôn ngữ là sự khan hiếm dữ liệu đa dạng. Trong khi công trình gần đây đã thu thập cả các bộ dữ liệu gồm các quan sát xúc giác và thị giác được ghép cặp và các bộ dữ liệu được gán nhãn bởi con người cho phân loại kết cấu hoặc vật liệu dựa trên xúc giác, chúng tôi không biết bất kỳ bộ dữ liệu xúc giác nào có chứa nhãn ngôn ngữ từ vựng mở. Do đó, chúng tôi phát triển một thiết bị tùy chỉnh để thu thập dữ liệu xúc giác-thị giác đồng bộ "trong thực tế", bên ngoài môi trường phòng thí nghiệm được kiểm soát.

Thiết lập này cho phép chúng tôi nắm bắt các quan sát thị giác cận cảnh và các bài đọc xúc giác trong khi ấn và trượt trên các bề mặt tiền cảnh và vật thể khác nhau với nền đa dạng. Một thách thức khác là việc gán nhãn bởi con người có thể tốn kém và các mô tả ngôn ngữ về trải nghiệm xúc giác mang tính chủ quan và khác nhau giữa các cá nhân. Để giải quyết những thách thức này, chúng tôi lấy cảm hứng từ các công trình trước đây về đào tạo mô hình ngôn ngữ lớn (LLM) và mô hình ngôn ngữ thị giác (VLM) (Taori et al., 2023; Wang et al., 2022b; Liu et al., 2023b; Chen et al., 2023b), những mô hình chứng minh hiểu biết ngôn ngữ thị giác bằng cách đào tạo trên dữ liệu được tổng hợp bởi chính chúng hoặc các LLM hiện có. Chúng tôi tạo ra các mô tả xúc giác từ quan sát thị giác bằng cách sử dụng một LLM có sẵn (GPT-4V (OpenAI et al., 2023)) và giả thuyết rằng nó có thể phục vụ như một người chú thích hiệu quả để giảm thiểu sự khan hiếm dữ liệu xúc giác-ngôn ngữ được gán nhãn.

Trong công trình này, chúng tôi trình bày bộ dữ liệu Xúc giác-Thị giác-Ngôn ngữ (TVL), một bộ dữ liệu mới gồm 44K quan sát thị giác-xúc giác được ghép cặp, trong đó 10% dữ liệu được chú thích bởi con người trong khi phần còn lại được gán nhãn bởi GPT-4V. Thay vì ràng buộc tất cả các phương thức với thị giác (Girdhar et al., 2023), chúng tôi đào tạo một bộ mã hóa xúc giác trên bộ dữ liệu này bằng cách thực hiện học tương phản theo cặp giữa cả ba phương thức. Chúng tôi tận dụng các bộ mã hóa thị giác và ngôn ngữ hiện có từ OpenCLIP (Ilharco et al., 2021) để đào tạo một bộ mã hóa xúc giác được căn chỉnh với cả phương thức văn bản và thị giác. Chúng tôi đánh giá sự căn chỉnh bằng cách sử dụng khả năng của bộ mã hóa cho phân loại xúc giác-thị giác và xúc giác-ngôn ngữ.

Tận dụng bộ dữ liệu và bộ mã hóa xúc giác đã đào tạo, chúng tôi tiếp tục tinh chỉnh LLaMA2 7B (Touvron et al., 2023) để tạo ra các mô tả văn bản về hình ảnh xúc giác dựa trên quan sát thị giác và xúc giác (Hình 1). Để đánh giá mô hình này, chúng tôi đề xuất một Tiêu chuẩn Xúc giác-Thị giác-Ngôn ngữ trong đó chúng tôi truy vấn các mô hình đa phương thức để tạo ra các mô tả xúc giác và sử dụng một LLM để đánh giá sự nhất quán của chúng với các chú thích của con người làm cơ sở.

Mô hình xúc giác-thị giác-ngôn ngữ được đề xuất, được đào tạo chỉ trên một lượng nhỏ dữ liệu được gán nhãn bởi con người, cho thấy sự cải thiện có ý nghĩa thống kê về hiệu suất trên Tiêu chuẩn TVL khi so sánh với các VLM mã nguồn mở (cải thiện +32%) và GPT-4V (cải thiện +12%), mô hình tạo nhãn.

Bài báo này đóng góp những điểm sau:

1. TVL, một bộ dữ liệu mới chứa 44K quan sát xúc giác-thị giác được ghép cặp được chú thích bằng các mô tả xúc giác được tạo ra bởi con người hoặc VLM, giải quyết tình trạng thiếu hụt dữ liệu xúc giác được chú thích ngôn ngữ;

2. Một Bộ mã hóa Xúc giác Được Căn chỉnh với Thị giác và Ngôn ngữ được đào tạo trên bộ dữ liệu TVL thông qua học tương phản theo cặp giữa cả ba phương thức và một Mô hình Xúc giác-Thị giác-Ngôn ngữ, một mô hình đa phương thức có khả năng tạo ra các mô tả xúc giác từ cả đầu vào thị giác và xúc giác;

3. Các thí nghiệm trên Tiêu chuẩn TVL cho thấy rằng sự kết hợp giữa các chú thích của con người và nhãn giả VLM cải thiện hiệu suất mô hình trong hiểu biết xúc giác-thị giác-ngôn ngữ, vượt qua các VLM hiện có ít nhất 12%.

# 2. Công trình liên quan

## 2.1. Học Bộ mã hóa Đa phương thức

Đào tạo trước các bộ mã hóa đa phương thức là một bước cần thiết hướng tới học đa nhiệm vụ, vì nó có thể cấu trúc tự nhiên không gian tiềm ẩn để thực hiện suy luận đa phương thức zero-shot. CLIP (Radford et al., 2021; Ilharco et al., 2021) là một trong những cái đầu tiên sử dụng dữ liệu quy mô internet để thực hiện đào tạo trước tương phản nhằm học một không gian nhúng chung giữa thị giác và văn bản. Guzhov et al. (2021) và Zhang et al. (2021); Guo et al. (2023) mở rộng CLIP để bao gồm âm thanh và điểm đám mây. ImageBind (Girdhar et al., 2023) đào tạo tương phản các bộ mã hóa cho sáu phương thức chỉ sử dụng dữ liệu được ghép cặp với hình ảnh. Nhiều công trình cũng khám phá việc che dấu như một chiến lược thay thế cho đào tạo trước đa phương thức (Bachmann et al., 2022; Li et al., 2023b; Geng et al., 2022). Trong công trình này, chúng tôi căn chỉnh phương thức xúc giác với không gian tiềm ẩn CLIP để nắm bắt mối quan hệ của nó với quan sát hình ảnh và mô tả ngôn ngữ tự nhiên về tính xúc giác của con người.

## 2.2. Nhận thức Xúc giác

Việc tích hợp cảm giác xúc giác với thị giác, được lấy cảm hứng từ việc sử dụng đồng thời thị giác và xúc giác trong nhận thức của con người (Bresciani et al., 2006; Ittyerah & Marks, 2007; Jones et al., 2005; Camponogara & Volcic, 2021; Stone & Gonzalez, 2015), là một lĩnh vực nghiên cứu tích cực trong cả robot học và AI có thể hiện (Goldberg & Bajcsy, 1984; Pacchierotti et al., 2017). Công trình trong lĩnh vực này được hỗ trợ bởi các cảm biến xúc giác dựa trên thị giác chi phí thấp (Chorley et al., 2009; Yamaguchi & Atkeson, 2016; Yuan et al., 2017; Lambeta et al., 2020; Sferrazza & D'Andrea, 2019; Shimonomura, 2019). Một số công trình gần đây phát hiện ra rằng tận dụng sự kết hợp của thị giác và xúc giác giúp ích cho ước tính lực và tư thế cảm biến (Suresh et al., 2022), tạo và dự đoán hình ảnh đa phương thức (Higuera et al., 2023; Zhong et al., 2022; Yang et al., 2022; Li et al., 2019), thao tác khéo léo (Calandra et al., 2018; Fu et al., 2023; Zhang & Demiris, 2023; Chen et al., 2022; Qi et al., 2023; Kerr et al., 2023), và đã tạo ra các bộ dữ liệu bao gồm dữ liệu xúc giác, thị giác và âm thanh (Gao et al., 2021; 2022).

Nhiều công trình nghiên cứu việc sử dụng cảm biến xúc giác để phân loại kết cấu bề mặt, vật liệu đối tượng và quần áo. Li & Adelson (2013) phân loại 40 thuộc tính vật liệu từ quan sát xúc giác bằng cách sử dụng một phương pháp phân loại kết cấu không dựa trên học (Ojala et al., 2002); các công trình tiếp theo sử dụng các phương pháp dựa trên học cho phân loại trang phục (Kampouris et al., 2016; Yuan et al., 2018). Bằng cách thu thập dữ liệu "trong thực tế", Yang et al. (2022) đã mở rộng sự đa dạng quan sát xúc giác và đào tạo một bộ phân loại vật liệu. Tất cả các công trình này sử dụng các chú thích của con người từ vựng đóng cho toàn bộ bộ dữ liệu, trong khi chúng tôi sử dụng một mô hình ngôn ngữ thị giác để gán nhãn một bộ dữ liệu được thu thập "trong thực tế" và kiểm tra trên các nhiệm vụ từ vựng mở.

Đồng thời với công trình này, Yang et al. (2024) ràng buộc xúc giác với phương thức thị giác, thực hiện phân loại từ vựng mở trên các phương thức xúc giác, thị giác và ngôn ngữ, và căn chỉnh các đầu vào xúc giác với các mô hình ngôn ngữ để tạo văn bản mà không cần tinh chỉnh ImageBind-LLM (Han et al., 2023).

## 2.3. Căn chỉnh Đa phương thức trong LLM

Các bộ mã hóa đa phương thức được đào tạo trước, khi được căn chỉnh với các mô hình ngôn ngữ, cho phép các mô hình ngôn ngữ suy luận với các phương thức không phải văn bản. Dựa trên khả năng của Mô hình Ngôn ngữ Lớn (LLM), Unified-IO 2 (Lu et al., 2023), Generalist Agent (Reed et al., 2022), Robot Transformer 2 (Brohan et al., 2023), và PaLM-E (Driess et al., 2023) tinh chỉnh end-to-end các mô hình ngôn ngữ với dữ liệu internet và thị giác từ nhiều miền. Công trình gần đây cố gắng làm cho việc căn chỉnh nhanh hơn và hiệu quả hơn về tham số (Zhu et al., 2023; Moon et al., 2023; Dai et al., 2023; Lin et al., 2023; Chen et al., 2023a; Cai et al., 2023a; Bai et al., 2023; Hu et al., 2022). Tương tự như cách các mô hình ngôn ngữ mã nguồn mở đào tạo trên dữ liệu được tạo ra bởi GPT (Taori et al., 2023), nhiều mô hình ngôn ngữ thị giác (Liu et al., 2023b;a; Zhang et al., 2023; Gao et al., 2023; Chen et al., 2023b) tinh chỉnh mô hình trên dữ liệu tuân theo hướng dẫn ngôn ngữ-hình ảnh được tạo ra bởi GPT-4 (OpenAI et al., 2023) và cho thấy khả năng suy luận thị giác tổng quát. ImageBind-LLM (Han et al., 2023) và PandaGPT (Su et al., 2023) giới thiệu khả năng suy luận đa phương thức bằng cách sử dụng các bộ mã hóa ImageBind. Công trình gần đây hơn căn chỉnh các LLM được đào tạo trước, bộ mã hóa và bộ giải mã để tinh chỉnh một mô hình có thể hiểu và tạo ra dữ liệu đa phương thức (Wu et al., 2023; Tang et al., 2023; Sun et al., 2023). Tương tự như Imagebind-LLM, công trình này căn chỉnh bộ mã hóa đa phương thức với LLaMA-2 được đào tạo trước (Touvron et al., 2023).

## 2.4. Đào tạo từ Nhãn giả

Hiệu quả của học có giám sát thường bị giới hạn bởi tính có sẵn của dữ liệu được gán nhãn. Các mô hình giáo viên được đào tạo trên một tập nhỏ dữ liệu được gán nhãn có thể cung cấp một nguồn giám sát không tốn kém dưới dạng nhãn giả. Sau đó, một mô hình học sinh học từ các nhãn giả được tạo ra bởi mô hình giáo viên trên một khối lượng lớn dữ liệu không được gán nhãn (Sohn et al., 2020; Lee et al., 2013; Wang et al., 2022a; Rosenberg et al., 2005; McLachlan, 1975). Trong khi các công trình trước đây tận dụng việc đào tạo các mô hình giáo viên trên các bộ dữ liệu được gán nhãn, các công trình gần đây trong cả văn học thị giác và ngôn ngữ đều tận dụng các mô hình được đào tạo trước quy mô lớn. CutLER (Wang et al., 2023) sử dụng các đặc trưng DINO (Caron et al., 2021) để tạo ra các hộp giới hạn, cho phép đào tạo không giám sát các mô hình phát hiện và phân đoạn đối tượng. InstructPix2Pix và InstructNeRF2NeRF (Brooks et al., 2023; Haque et al., 2023) sử dụng GPT (Brown et al., 2020) và Stable Diffusion (Rombach et al., 2022) để tạo ra một bộ dữ liệu các ví dụ chỉnh sửa hình ảnh và sau đó đào tạo một mô hình khuếch tán dựa trên những ví dụ này. Các LLM và VLM gần đây (Wang et al., 2022b; Taori et al., 2023; Liu et al., 2023b;a) được đào tạo bằng cách sử dụng các nhãn giả được tạo ra bởi các mô hình GPT (Brown et al., 2020; OpenAI et al., 2023). Tuy nhiên, trong những công trình này, các mô hình giáo viên và học sinh chia sẻ cùng các phương thức đầu vào và đầu ra. Tương tự như khung được đề xuất bởi Burnel et al. (2023), chúng tôi sử dụng một mô hình đa phương thức chỉ có thị giác để tạo ra các nhãn văn bản từ dữ liệu thị giác, sau đó để khớp với dữ liệu xúc giác nhằm đào tạo bộ mã hóa xúc giác được căn chỉnh với ngôn ngữ và mô hình TVL. Giáo viên mà chúng tôi sử dụng (GPT-4V) tổng quát hơn một mô hình chuyên biệt được đào tạo chỉ trên nhiệm vụ của học sinh.

# 3. Bộ dữ liệu TVL

Bộ dữ liệu TVL (các ví dụ trong Hình 3) chứa các quan sát xúc giác và thị giác được ghép cặp được gán nhãn với các cảm giác xúc giác bằng ngôn ngữ tự nhiên. Ở đây chúng tôi mô tả phần cứng và các quy trình được sử dụng để thu thập, làm sạch và gán nhãn dữ liệu.

## 3.1. Thu thập Dữ liệu

TVL sử dụng dữ liệu thị giác từ webcam Logitech BRIO và dữ liệu xúc giác từ DIGIT, một cảm biến xúc giác chi phí thấp, nhỏ gọn và mã nguồn mở cung cấp các quan sát xúc giác độ phân giải cao dưới dạng hình ảnh RGB của một bề mặt có thể biến dạng bên trong (Lambeta et al., 2020). Bộ dữ liệu thị giác-xúc giác thô kết hợp hai tập con riêng biệt: 1) bộ dữ liệu Self-Supervised Visuo-Tactile Pretraining (SSVTP) (Kerr et al., 2023) và 2) bộ dữ liệu Human Collected Tactile (HCT). Bộ dữ liệu SSVTP (4,587 cặp hình ảnh-xúc giác) được thu thập bởi một robot UR5, robot này trước tiên chụp các hình ảnh từ trên xuống từ phía trên một bề mặt làm việc mà trên đó một tập hợp các đối tượng được sắp xếp trước, sau đó nhấn cảm biến DIGIT vào vị trí tương ứng trong không gian làm việc. Tuy nhiên, bộ dữ liệu SSVTP gặp phải hai hạn chế: 1) việc thu thập trong môi trường phòng thí nghiệm hạn chế sự đa dạng của các đối tượng, và 2) việc thu thập không đồng bộ dữ liệu xúc giác và thị giác có thể dẫn đến sự không khớp, đặc biệt nếu đối tượng vô tình bị di chuyển bởi robot trong quá trình thu thập dữ liệu. Để giải quyết những vấn đề này, HCT nhấn mạnh việc thu thập đồng bộ dữ liệu xúc giác và thị giác để đảm bảo sự căn chỉnh trong thông tin cảm biến được thu thập.

HCT bao gồm các ví dụ dữ liệu xúc giác-thị giác trong thực tế được thu thập bởi 5 người trong tổng cộng 20 giờ sử dụng thiết bị thu thập dữ liệu cầm tay được in 3D được giới thiệu trong Hình 2. Thiết bị ghi lại cả quan sát thị giác và xúc giác ở tốc độ 30 Hz. Các khung dữ liệu được thu thập trong "quỹ đạo" của các lần chạm: mỗi quỹ đạo bao gồm con người tiếp cận, tiếp xúc, trượt và rút lui khỏi một đối tượng với cảm biến xúc giác. Chúng tôi phân loại các cặp xúc giác-thị giác là đang hoặc không tiếp xúc với bề mặt. Dữ liệu thị giác được thu thập ở góc xiên sao cho cảm biến xúc giác và điểm tiếp xúc luôn nằm trong trường nhìn của camera để duy trì tính đồng bộ thị giác-xúc giác. Để cải thiện sự đa dạng trong bộ dữ liệu này, các nhà thu thập dữ liệu được hướng dẫn tìm kiếm các ví dụ xúc giác thực tế thú vị và mới lạ, chẳng hạn như kết cấu và cạnh. Một tập kiểm tra nhỏ được giữ lại (1% các cặp) từ HCT được chú thích bằng tay, trong khi phần còn lại được gán nhãn giả bởi GPT-4V, như mô tả trong Phần 3.3.

## 3.2. Làm sạch Hình ảnh Xúc giác Ứng viên

Chúng tôi phân loại dữ liệu được thu thập thành các khung tiếp xúc và không tiếp xúc bằng cách sử dụng bộ mã hóa xúc giác được đào tạo trước từ SSVTP (Kerr et al., 2023). Đối với mỗi quỹ đạo chạm, dưới giả định rằng các khung đầu tiên và cuối cùng là không tiếp xúc, chúng tôi tính toán trung bình của những khung này để tạo ra một hình ảnh nền tham chiếu. Hình ảnh này sau đó được nhúng bởi bộ mã hóa xúc giác được đào tạo trước để thu được biểu diễn tiềm ẩn. Để xác định liệu một khung trong quỹ đạo chạm có đang tiếp xúc hay không, chúng tôi tính toán độ tương tự cosin giữa biểu diễn tiềm ẩn xúc giác của nó và của khung nền được ước tính. Chúng tôi coi một khung xúc giác là đang tiếp xúc khi độ tương tự cosin giảm xuống dưới 0,6 (Kerr et al., 2023). Dữ liệu được thu thập chứa 43,741 cặp khung tiếp xúc và 169,292 cặp khung không tiếp xúc.

## 3.3. Gán nhãn Ngôn ngữ

**Gán nhãn bởi Con người** Vì bộ dữ liệu SSVTP cho thấy sự căn chỉnh thị giác-xúc giác mạnh mẽ, chúng tôi sử dụng nó làm cơ sở để căn chỉnh xúc giác và ngôn ngữ; chúng tôi chú thích thủ công bộ dữ liệu với các mô tả ngôn ngữ tự nhiên về các cảm giác xúc giác được nắm bắt bởi mỗi điểm dữ liệu. Chúng tôi cung cấp cho các người chú thích một danh sách từ vựng xúc giác gồm 400 từ (ajbarnett, 2023) để tạo ra các mô tả ngôn ngữ về thuộc tính vật liệu và cảm giác xúc giác của các cặp trong bộ dữ liệu SSVTP. Những người chú thích này được hướng dẫn chọn tối đa năm tính từ áp dụng được mô tả chính xác nhất các mẫu xúc giác được hiển thị trong mỗi cặp thị giác-xúc giác.

**Tạo Nhãn giả với GPT-4V** Chúng tôi thực hiện gán nhãn giả trên phần của bộ dữ liệu HCT đang tiếp xúc, sử dụng GPT-4V để tạo ra các nhãn ngôn ngữ mô tả cảm giác xúc giác. Chúng tôi thấy rằng việc cung cấp cả hình ảnh đầy đủ và một phiên bản được cắt xung quanh điểm tiếp xúc khuyến khích GPT-4V tạo ra các nhãn văn bản được căn chỉnh với những người của con người, vì các hình ảnh đầy đủ có thể chứa nhiều yếu tố gây nhiễu và các đối tượng không tiếp xúc (xem các trường hợp thành công và thất bại trong Hình 3). Lời nhắc cụ thể được cung cấp cho GPT-4V để tạo nhãn giả được báo cáo trong Phụ lục C.4.

Đôi khi, GPT-4V thất bại hoặc từ chối tạo ra các nhãn xúc giác cho các hình ảnh bị mờ chuyển động hoặc ánh sáng yếu. Trong những trường hợp như vậy, chúng tôi trước tiên cố gắng tạo ra các nhãn cho các hình ảnh khác trong cùng quỹ đạo, sau đó điền các nhãn bị thiếu bằng cách lấy mẫu ngẫu nhiên từ tập hợp các từ được áp dụng cho các hình ảnh tiếp xúc khác trong cùng quỹ đạo. Nếu không có hình ảnh nào trong quỹ đạo có thể được gán nhãn thành công, quỹ đạo đó sẽ bị loại khỏi phần đào tạo của bộ dữ liệu. Sau quá trình này, chúng tôi còn lại 39,154 hình ảnh được gán nhãn giả.

## 3.4. Thống kê Bộ dữ liệu

Thành phần SSVTP chứa 4,587 cặp hình ảnh-xúc giác độc lập. Thành phần HCT bao gồm 39,154 cặp khung hình ảnh-xúc giác tiếp xúc mới được thu thập và 169,292 cặp dữ liệu không tiếp xúc. Bộ dữ liệu trước chứa một quỹ đạo chạm duy nhất cho mỗi điểm dữ liệu, trong khi bộ sau được thu thập dưới dạng 1,486 quỹ đạo liên tục duy nhất, mỗi quỹ đạo bao gồm một hoặc nhiều sự kiện tiếp xúc với một đối tượng quan tâm. Trên cả phần được gán nhãn bởi con người và GPT-4V của bộ dữ liệu, các người chú thích sử dụng 254 tính từ xúc giác duy nhất. Chúng tôi thực hiện chia 99%-1% train-test trên cả hai thành phần bộ dữ liệu, với các người chú thích con người gán nhãn thủ công cho tập kiểm tra (402 cặp hình ảnh-xúc giác) cho cả hai bộ dữ liệu. Trung bình, GPT-4V sử dụng 4,25 tính từ để mô tả cảm giác xúc giác trên HCT, trong khi các người chú thích con người trung bình 2,70 tính từ. Một phân tích chi tiết hơn về các mô tả được hiển thị trong Phụ lục C.3.

# 4. Mô hình Xúc giác-Thị giác-Ngôn ngữ

Chúng tôi trước tiên xem xét lại công thức của ImageBind và ImageBind-LLM. Sau đó chúng tôi mô tả phương pháp tương phản theo cặp của chúng tôi để đào tạo bộ mã hóa xúc giác, và cuối cùng thảo luận về công thức đào tạo của Mô hình TVL được căn chỉnh.

## 4.1. Sơ bộ

ImageBind (Girdhar et al., 2023) là một mô hình đa phương thức học một biểu diễn nhúng chung trên sáu phương thức khác nhau: hình ảnh, văn bản, âm thanh, độ sâu, nhiệt và dữ liệu IMU. Nó sử dụng các cặp dữ liệu bao gồm thị giác và một trong các phương thức khác, để tất cả được "ràng buộc" với thị giác. Các bộ mã hóa thị giác và ngôn ngữ được khởi tạo từ OpenCLIP (Ilharco et al., 2021) và được giữ đóng băng, trong khi các bộ mã hóa cho các phương thức khác được khởi tạo ngẫu nhiên. Mỗi bộ mã hóa sử dụng một mạng adapter nhỏ, có thể đào tạo ở cuối để chiếu các đầu vào lên một không gian tiềm ẩn có cùng kích thước. Các bộ mã hóa được đào tạo chung thông qua học tương phản trên các biểu diễn nhúng tiềm ẩn được chuẩn hóa bằng cách sử dụng mất mát InfoNCE.

LLaMA-Adapter (Zhang et al., 2023) và ImageBind-LLM (Han et al., 2023) cung cấp các phương pháp tinh chỉnh hướng dẫn hiệu quả cho VLM, tận dụng các mô hình đa phương thức được đào tạo trước để mã hóa các phương thức mới. Hiệu quả của những phương pháp này đến từ (1) việc tính trung bình các quan sát đa phương thức trong một token duy nhất và (2) một cổng được khởi tạo bằng không một cách thích ứng hợp nhất token đa phương thức với mô hình ngôn ngữ. LLaMA-Adapter trước tiên đào tạo trước cổng được khởi tạo bằng không và bộ chiếu từ bộ mã hóa đến mô hình ngôn ngữ, sau đó tinh chỉnh mô hình ngôn ngữ với LoRA (Hu et al., 2022).

## 4.2. Bộ mã hóa Xúc giác

Trái ngược với ImageBind, vốn độc lập ràng buộc tất cả các phương thức với thị giác, chúng tôi ràng buộc mỗi cặp phương thức để cung cấp giám sát mạnh mẽ cho phương thức xúc giác. Chúng tôi tính toán mất mát tương phản giữa các cặp thị giác-ngôn ngữ, xúc giác-ngôn ngữ và xúc giác-thị giác cho mỗi lô dữ liệu. Chúng tôi khởi tạo ngẫu nhiên bộ mã hóa xúc giác như một Vision Transformer (ViT) (Dosovitskiy et al., 2020) và kiểm tra trên ba kích thước mô hình: ViT-Tiny (5,7M tham số), ViT-Small (22M), và ViT-Base (86M). Chúng tôi nhận thấy rằng việc áp dụng trực tiếp công thức đào tạo ImageBind dẫn đến việc overfitting bộ dữ liệu đào tạo tương đối nhỏ gồm 44K cặp dữ liệu tiếp xúc. Trái ngược với các công trình trước đây (Kerr et al., 2023; Yang et al., 2022; Dave et al., 2024), chúng tôi thấy rằng tận dụng dữ liệu trong đó cảm biến xúc giác không tiếp xúc với bề mặt (hình ảnh nền) có thể giảm thiểu vấn đề overfitting này và tăng cường học biểu diễn xúc giác bằng cách cải thiện sự đa dạng dữ liệu thị giác (xem Hình 6 trong phụ lục). Do đó, chúng tôi đảm bảo rằng đối với một phần γ= 10% của dữ liệu đào tạo, cảm biến không tiếp xúc, và chúng tôi gán cho những ví dụ này một nhãn văn bản là "background". Ngoài ra, chúng tôi loại bỏ các bộ chiếu khỏi các bộ mã hóa thị giác và ngôn ngữ, để bộ mã hóa xúc giác chiếu trực tiếp vào không gian tiềm ẩn chung của CLIP gốc. Cuối cùng, để tăng sự đa dạng của các nhãn ngôn ngữ, chúng tôi xáo trộn ngẫu nhiên và chọn một tập con các từ trong mô tả xúc giác cho mỗi hình ảnh. Cùng nhau, những phương pháp này giúp giảm thiểu overfitting (tham khảo Phụ lục B.1).

## 4.3. Căn chỉnh với Mô hình Ngôn ngữ

Chúng tôi tuân theo đào tạo hai giai đoạn được đề xuất trong ImageBind-LLM (Han et al., 2023), thay thế các bộ mã hóa ImageBind bằng các bộ mã hóa TVL. Chúng tôi đào tạo trước trên cả tập con 595K LLaVA Visual Instruct CC3M (Liu et al., 2023b) và bộ dữ liệu TVL. Đối với tập con CC3M, chúng tôi cung cấp một hình ảnh xúc giác trống cho phương thức xúc giác. Trong quá trình tinh chỉnh, chúng tôi sử dụng sự kết hợp của TVL, Alpaca (Taori et al., 2023) và LLaVA Visual Instruct 150K (Liu et al., 2023b). Theo kinh nghiệm, chúng tôi thấy rằng việc đào tạo chỉ trên bộ dữ liệu của chúng tôi là không đủ để vượt qua việc tinh chỉnh an toàn của LLaMA2 (Touvron et al., 2023), dẫn đến việc mô hình từ chối trả lời các câu hỏi về cảm giác xúc giác. Chi tiết về các lời nhắc cho TVL để tinh chỉnh hướng dẫn có trong Phụ lục C.2.

# 5. Thí nghiệm

Chúng tôi đánh giá định lượng khả năng đa phương thức của mô hình TVL trong hai bối cảnh thí nghiệm: một nhiệm vụ phân loại đa phương thức và một nhiệm vụ mô tả ngữ nghĩa-xúc giác.

## 5.1. Đánh giá & Chỉ số

**Phân loại Xúc giác Từ vựng Mở** Chúng tôi biến tập kiểm tra TVL được gán nhãn bởi con người thành một bài toán phân loại 402 lớp và đánh giá hiệu suất của bộ mã hóa xúc giác bằng cách đo độ chính xác top-1 và top-5 cho cả phân loại xúc giác-thị giác và xúc giác-ngôn ngữ. Vì nhiều quan sát xúc giác có thể được mô tả bằng nhiều cách tương tự về mặt ngữ nghĩa (ví dụ: rigid đồng nghĩa với stiff) và biểu diễn nhúng ngôn ngữ CLIP không bất biến với hoán vị (ví dụ: "soft, smooth" và "smooth, soft" có các biểu diễn nhúng khác nhau), chúng tôi đề xuất một phương pháp thay thế để tính toán các nhãn cơ sở cho phân loại xúc giác-ngôn ngữ.

Chúng tôi trước tiên nhắc GPT-4 tạo ra một tập hợp 5 (độ dài trung bình của các nhãn giả xúc giác) từ đồng nghĩa cho mỗi từ trong tập hợp các bộ mô tả được sử dụng bởi các người chú thích con người của bộ dữ liệu SSVTP, dẫn đến 799 tính từ riêng biệt mô tả cảm giác xúc giác. Chúng tôi thu được biểu diễn nhúng ngôn ngữ CLIP cho những tính từ này và tính toán độ tương tự cosin của mỗi bộ mô tả gốc với mỗi từ đồng nghĩa được tạo ra của nó. Chúng tôi coi giá trị tối thiểu ϕ của những độ tương tự cosin này là một ngưỡng cho từ vựng tương tự về mặt ngữ nghĩa. Đối với mỗi hình ảnh xúc giác, chúng tôi định nghĩa tập hợp các nhãn ngôn ngữ đúng là tất cả các nhãn trong tập kiểm tra có độ tương tự cosin với nhãn ngôn ngữ gốc của hình ảnh vượt quá ϕ. Sử dụng những nhãn này, chúng tôi tính toán độ chính xác top-1 và top-5. Theo kinh nghiệm, chúng tôi thấy ϕ= 0,636. Chúng tôi cũng báo cáo độ chính xác top-1 và top-5 sử dụng phân vị thứ 25, 50 và 75 của độ tương tự cosin làm ngưỡng trong Bảng 6.

**Tiêu chuẩn TVL** Chúng tôi đánh giá khả năng của các LLM để tạo ra các mô tả xúc giác trên tập kiểm tra TVL. Cho một hình ảnh đầu vào thị giác, một hình ảnh thị giác được cắt tập trung vào cảm biến xúc giác, và một hình ảnh xúc giác tương ứng, chúng tôi yêu cầu mô hình mô tả cảm giác xúc giác của đối tượng đang được đề cập với một tập hợp không quá 5 tính từ.

Để thu được một so sánh số lượng, chúng tôi nhắc GPT-4 chỉ có văn bản để chấm điểm sự tương tự của phản hồi của mô hình so với các nhãn ngữ nghĩa cơ sở được chú thích bởi con người trên thang điểm từ 1 đến 10 (trong đó điểm số cao hơn cho thấy việc tuân theo hướng dẫn tốt hơn và khớp mô tả gần hơn), cũng như giải thích điểm số được đưa ra, tương tự như các công trình trước đây (Liu et al., 2023b; Chiang et al., 2023). Một mẫu đầu ra của mô hình được cung cấp trong Hình 5, và các lời nhắc được sử dụng để tạo ra và đánh giá được báo cáo trong Phụ lục C.4. Chúng tôi so sánh với các VLM mã nguồn mở hiện có (Liu et al., 2023a; Cai et al., 2023b; Li et al., 2023a; Dai et al., 2023) và GPT-4V. Như một baseline bổ sung, chúng tôi sử dụng bộ mã hóa xúc giác và hình ảnh SSVTP (Kerr et al., 2023) để tinh chỉnh mô hình ngôn ngữ; chúng tôi gọi mô hình kết quả là SSVTP-LLaMA.

## 5.2. Kết quả

**Phân loại** Chúng tôi tóm tắt kết quả nhiệm vụ phân loại xúc giác trong Bảng 1. Vì chúng tôi sử dụng OpenCLIP để mã hóa quan sát hình ảnh và ngôn ngữ, bộ mã hóa TVL chia sẻ điểm số độ chính xác thị giác-ngôn ngữ với OpenCLIP. Chúng tôi so sánh độ chính xác xúc giác-thị giác của bộ mã hóa của chúng tôi với Kerr et al. (2023); vì họ đào tạo trên một bộ dữ liệu nhỏ được thu thập trong thiết lập phòng thí nghiệm, mô hình của họ hoạt động tốt trên bộ dữ liệu SSVTP, nhưng không khái quát hóa tốt cho bộ dữ liệu "trong thực tế" mới. Vì bộ mã hóa xúc giác được căn chỉnh với mô tả ngôn ngữ về tính xúc giác, nó cho thấy sự căn chỉnh xúc giác-văn bản tốt hơn so với sự căn chỉnh thị giác-văn bản của OpenCLIP.

**Tiêu chuẩn TVL** Chúng tôi trình bày thống kê tóm tắt cho kết quả tạo ngữ nghĩa-xúc giác trong Bảng 2. Chúng tôi thấy rằng các VLM mã nguồn mở hoạt động kém hơn GPT-4V trên tiêu chuẩn đề xuất, có khả năng do sự đa dạng hạn chế và thiếu tập trung vào tính xúc giác của con người trong dữ liệu thị giác mà chúng đã được đào tạo. Mặt khác, tất cả các phiên bản của TVL-LLaMA đều vượt trội hơn GPT-4V, cho thấy rằng các mô hình được đào tạo có thể khái quát hóa vượt qua phần nhỏ các nhãn của con người được cung cấp như một phần của bộ dữ liệu. Cả hai phát hiện này đều có ý nghĩa thống kê ở mức α= 0,05. Kết quả cũng cho thấy rằng sự căn chỉnh xúc giác-ngôn ngữ là cần thiết, như được chứng minh bằng điểm số thấp hơn của SSVTP-LLaMA, mô hình chỉ sử dụng các phương thức xúc giác và thị giác trong quá trình đào tạo trước.

Nhìn chung, các thí nghiệm của chúng tôi cho thấy rằng: 1) bộ mã hóa xúc giác TVL được đào tạo trên bộ dữ liệu TVL được căn chỉnh với không gian tiềm ẩn ngôn ngữ và ghi điểm cao hơn (+29%) trong nhiệm vụ phân loại so với các bộ mã hóa được đào tạo trước thị giác-xúc giác và các bộ mã hóa thị giác-ngôn ngữ chung (OpenCLIP); và 2) các mô hình TVL-LLaMA được đào tạo để tạo ra các mô tả ngôn ngữ xúc giác từ quan sát thị giác và xúc giác khớp gần hơn với các mô tả của con người trên Tiêu chuẩn TVL mới (ít nhất +12%) so với các VLM hiện có.

## 5.3. Phân tích loại bỏ

Phần này trình bày sáu phân tích loại bỏ và độ nhạy được hiển thị trong Bảng 3 xem xét tác động của kích thước mô hình và bộ dữ liệu đề xuất đối với hiệu suất phân loại đa phương thức của bộ mã hóa. Thêm các phân tích loại bỏ được bao gồm trong phụ lục.

**Kích thước Mô hình (Bảng 3a)** Hiệu suất khác nhau đáng kể giữa các kích thước bộ mã hóa khác nhau. ViT-Base có độ chính xác validation cao nhất nhưng chậm hơn trên tập kiểm tra do sự chuyển dịch phân phối: các nhãn đào tạo từ GPT-4V ít chi tiết và chính xác hơn so với dữ liệu kiểm tra được chú thích bởi con người. Tuy nhiên, trong phân loại xúc giác-thị giác trên dữ liệu đồng bộ, ViT-Base vượt trội hơn cả hai mô hình nhỏ hơn.

**Tắt Mất mát Xúc giác-Văn bản (Bảng 3b)** tương tự như thiết lập trong ImageBind (Girdhar et al., 2023), trong đó dữ liệu trong cả ba phương thức được xem xét nhưng mất mát xúc giác-văn bản bị bỏ qua. Kết quả cho thấy rằng việc sử dụng ngôn ngữ để giám sát bộ mã hóa xúc giác căn chỉnh tốt hơn hai phương thức đó.

**Dữ liệu (Bảng 3c-f)** Chúng tôi thực hiện bốn phân tích độ nhạy về các thành phần khác nhau của bộ dữ liệu để đào tạo. Chúng tôi thấy rằng tận dụng dữ liệu từ cả ba phương thức cải thiện sự căn chỉnh xúc giác-ngôn ngữ. Trong khi việc thêm dữ liệu không tiếp xúc ngăn chặn mô hình overfitting với tập đào tạo, hiệu suất tập kiểm tra của nó có thể so sánh với việc chỉ có dữ liệu tiếp xúc. Chúng tôi cũng thử nghiệm với prompting được sử dụng trong đào tạo CLIP vanilla (Radford et al., 2021), mang lại cải thiện biên độ trong độ chính xác. Cuối cùng, chúng tôi đào tạo riêng mô hình trên SSVTP và HCT, và chúng tôi thấy rằng bộ dữ liệu được gán nhãn giả có thể cung cấp hiệu suất tương đương với việc đào tạo trên toàn bộ bộ dữ liệu, điều này cho thấy rằng bộ mã hóa xúc giác của TVL có thể tận dụng hiệu quả việc học tự giám sát để giảm sự phụ thuộc vào các bộ dữ liệu lớn, được gán nhãn đầy đủ trong khi vẫn duy trì hiệu suất nhiệm vụ.

# 6. Thảo luận và Kết luận

Nghiên cứu được trình bày có một số hạn chế. Trong khi nghiên cứu làm nổi bật việc sử dụng VLM để gán nhãn dữ liệu xúc giác, bản chất riêng biệt của xúc giác so với nhận thức thị giác cho thấy một giới hạn đối với độ chính xác của các nhãn xúc giác được rút ra chỉ từ thị giác. Do phần cứng thu thập dữ liệu, camera có thể không có tầm nhìn không bị cản trở về bề mặt hoặc đối tượng mà cảm biến xúc giác tiếp xúc, điều này có thể làm tăng khó khăn trong việc căn chỉnh xúc giác với thị giác và giảm chất lượng các nhãn giả được tạo ra từ hình ảnh. Chúng tôi hy vọng rằng nghiên cứu trong tương lai có thể tiếp tục tăng quy mô của các bộ dữ liệu xúc giác-thị giác-ngôn ngữ để cải thiện sự căn chỉnh đa phương thức.

Tóm lại, để căn chỉnh các phương thức xúc giác và ngôn ngữ, công trình này giới thiệu TVL, một bộ dữ liệu có đặc trưng là xúc giác, thị giác và các mô tả ngữ nghĩa-xúc giác. Sử dụng bộ dữ liệu, chúng tôi đào tạo một bộ mã hóa xúc giác được căn chỉnh với cả thị giác và ngôn ngữ tự nhiên. Chúng tôi chứng minh rằng bằng cách sử dụng bộ mã hóa xúc giác đã đào tạo, TVL-LLaMA có thể tạo ra các mô tả xúc giác bằng ngôn ngữ tự nhiên căn chỉnh gần hơn với các mô tả của con người so với những mô tả được tạo ra bởi các VLM hiện có.

# 7. Tuyên bố Tác động

Dữ liệu có mặt trong bài báo này được ẩn danh hóa. Công trình này có thể có lợi cho các mô hình sinh lớn trong tương lai cũng xem xét xúc giác như một phương thức cảm biến và có thể hữu ích cho các nhà nghiên cứu nghiên cứu các phương pháp học dựa trên nhãn giả. Đồng thời, mô hình được giới thiệu sẽ góp phần đạt được sự số hóa tốt hơn của xúc giác và việc sử dụng xúc giác trong robot học. Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều lợi ích xã hội tiềm năng từ công trình của chúng tôi, không có lợi ích nào mà chúng tôi cảm thấy cần được nhấn mạnh cụ thể ở đây.

# 8. Lời cảm ơn

Nghiên cứu này được hỗ trợ như một Dự án Chung Nghiên cứu Mở BAIR với Meta. Nghiên cứu này được thực hiện tại AUTOLAB tại UC Berkeley liên kết với Phòng thí nghiệm Nghiên cứu AI Berkeley (BAIR), và Sáng kiến CITRIS "People and Robots" (CPAR). Trong vai trò học thuật của họ tại UC Berkeley, Letian Fu, Gaurav Datta, Huang Huang, William Chung-Ho Panitch, Jaimyn Drake, và Ken Goldberg được hỗ trợ một phần bởi các khoản tài trợ từ Meta, Google, Autodesk, Siemens, Toyota Research Institute, Bosch, và bởi các khoản tài trợ thiết bị từ PhotoNeo, Nvidia, và Intuitive Surgical. Roberto Calandra được tài trợ bởi Quỹ Nghiên cứu Đức (DFG, Deutsche Forschungsgemeinschaft) như một phần của Chiến lược Xuất sắc của Đức – EXC 2050/1 – Project ID 390696704 – Cluster of Excellence "Centre for Tactile Internet with Human-in-the-Loop" (CeTI) của Technische Universität Dresden, và bởi Bundesministerium für Bildung und Forschung (BMBF) và German Academic Exchange Service (DAAD) trong dự án 57616814 (SECAI, School of Embedded and Composite AI). Chúng tôi cảm ơn Justin Kerr, Chung Min Kim, Ryan Hoque, và Xudong Wang cho các cuộc thảo luận hữu ích và phản hồi của họ.
