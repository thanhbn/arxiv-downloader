# 2311.04901.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.04901.pdf
# File size: 6127735 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint
GENOME: G ENERATIV E N EURO -SYMB OLICVISUAL
REASONING BY GROWING AND REUSING MODUL ES
Zhenfang Chen∗
MIT-IBM Watson AI LabRui Sun∗
Columbia UniversityWenjun Liu∗
Tsinghua University
Yining Hong
University of California, Los AngelesChuang Gan
MIT-IBM Watson AI Lab and UMass Amherst
ABSTRACT
Recent works have shown that Large Language Models (LLMs) could empower
traditional neuro-symbolic models via programming capabilities to translate lan-
guage into module descriptions, thus achieving strong visual reasoning results
while maintaining the model’s transparency and efficiency. However, these mod-
els usually exhaustively generate the entire code snippet given each new instance
of a task, which is extremely ineffective. On the contrary, human beings grad-
ually acquire knowledge that can be reused and grow into more profound skills
for fast generalization to new tasks since we are an infant. Inspired by this, we
propose generative neuro-symbolic visual reasoning by growing and reusing mod-
ules. Specifically, our model consists of three unique stages, module initialization,
module generation, and module execution. First, given a vision-language task, we
adopt LLMs to examine whether we could reuse and grow over established mod-
ules to handle this new task. If not, we initialize a new module needed by the task
and specify the inputs and outputs of this new module. After that, the new module
is created by querying LLMs to generate corresponding code snippets that match
the requirements. In order to get a better sense of the new module’s ability, we
treat few-shot training examples as test cases to see if our new module could pass
these cases. If yes, the new module is added to the module library for future reuse.
Finally, we evaluate the performance of our model on the testing set by executing
the parsed programs with the newly made visual modules to get the results. We
find the proposed model possesses several advantages. First, it performs compet-
itively on standard tasks like visual question answering and referring expression
comprehension; Second, the modules learned from one task can be seamlessly
transferred to new tasks; Last but not least, it is able to adapt to new visual rea-
soning tasks by observing a few training examples and reusing modules1.
1 I NTRODUCTION
Neuro-symbolic visual reasoning models (Andreas et al., 2016b; Mao et al., 2019b) refer to the al-
gorithm family that combines deep neural networks (lec, 1998; Hochreiter & Schmidhuber, 1997)
for learning correlations among the training data and symbolic methods (Yi et al., 2018; Andreas
et al., 2016a) to perform explicit and transparent multi-step reasoning. In contrast to pure neu-
ral network-based models (Hudson & Manning, 2018; Li et al., 2023), neuro-symbolic approaches
achieve strong performance in visual reasoning tasks, simultaneously offering superior model trans-
parency and data efficiency.
Nevertheless, such models suffer from several inherent limitations. Firstly, their language parsers (Yi
et al., 2018; Andreas et al., 2016b), employed for the conversion from natural language into sym-
bolic programs, typically demand extensive domain-specific language-program pairs to train on,
and struggle to generalize effectively to unconstrained natural language instructions. Additionally,
these models necessitate a custom design for every module, rendering the process labor-intensive
and lacking scalability.
∗indicates equal contributions
1Project page: https://vis-www.cs.umass.edu/genome
1arXiv:2311.04901v1  [cs.CV]  8 Nov 2023

--- PAGE 2 ---
Preprint
def execute_command (image):
image_patch = ImagePatch (image)
couch_patches = image_patch.find (”couch")
color1 = couch_patch.simple_query (f”What’s
the couch’s color?")
sky_patches = image_patch.find (”sky")
color2 = couch_patch.simple_query (f”What’s
the sky’s color?")
if color1==color2:
return “yes”
else return “No”BOX0= LOC (image= IMAGE,object ='couch’)
IMAGE0= CROP (image= IMAGE,box =BOX0)
BOX1= LOC (image= IMAGE,object ='sky’) 
IMAGE1= CROP (image= IMAGE,box =BOX1)
ANSWER0= VQA (image=IMAGE0,question=
'What color is the couch?’)
ANSWER1= VQA (image=IMAGE1,question=
'What color is the sky’)
ANSWER2= EV AL (expr= f"'yes ' if {ANSWER0}
=={ANSWER1} else 'no’”)Q: Are the couch and 
the sky the same 
color ?
A: No.
Q: Does the microwave have 
a different color than the 
toaster?LLMFixed  Module 
Library
(A) VisProg (B) ViperGPT
(C) Generative Module LearningGenerate
modules
(D) Applications
A: No.
Instruction : 
replace the apple 
with the same 
color as the grape
with a peach
Module 1:
LOC(…)
Module 2:
VQA(…)
Module  3:
CROP(…)
…
Existing  Methods
Our MethodNeuro -symbolic Visual Reasoning
Module 1:
LOC(…)
…
New Module:
Class COMPARE_ATTRIBUTE(  ):
""" Input: 
image: an image
box1: a box list
attribute:
……
Output:
True/False 
""" Update
Figure 1: The motivation of the GENOME. Compared with VisProg and ViperGPT which exhaus-
tively generate a code snippet for each input case, our GENOME is able to generate new modules and
reuse old modules to handle the query. The module generated by GENOME can be used to handle
other instances of the task for better performance. Second, the generated module can be transferred
to different tasks like image editing. Finally, it can learn to handle new tasks like Raven (Burke,
1985; Zhang et al., 2019) by learning modules from only a few training samples. The edited region
and the correct answer for the Raven task are labeled with red boxes for better visualization.
Recent advancements in large language models (LLMs) (Brown et al., 2020; Ouyang et al., 2022)
have ushered in a new era with its remarkable performances across various applications, including
chatbots (Shuster et al., 2022), virtual assistants (Dong et al., 2023), and programming assistants
(Chen et al., 2021a). Riding this unprecedented wave, researchers reformulate the old wisdom by
incorporating LLMs into neuro-symbolic reasoning, bypassing the inflexibility and ineffectiveness
of domain-specific language-to-program parsers. Specifically, VisProg (Gupta & Kembhavi, 2022)
pre-defines a set of visual modules, and uses LLMs to transform language instructions into symbolic
programs consisting of the pre-defined visual modules. Taking a step forward, ViperGPT (Sur ´ıs
et al., 2023) releases the burden on manually-defined visual modules by introducing a code generator
that could produce a code snippet based on each input instance of a new task.
Promising as these LLM-based neuro-symbolic models can be, they inevitably bear several weak-
nesses compared to the learning and reasoning processes of human beings. First, both VisProg and
ViperGPT exhaustively produce one code snippet for each new instance of a task, which is extremely
ineffective. This is in stark contrast with the human learning process: from an early age, we organ-
ically accumulate knowledge from particular experiences. Such knowledge acquired from specific
cases could be reused and reconfigured, enabling us to quickly adapt to new tasks and new demands
(Harlow, 1949; Mitchell et al., 1986; Lake et al., 2016; Ellis et al., 2023). The knowledge blocks
grow progressively over time, gradually into a library with extraordinary richness and flexibility
for fast generalization to any unseen task - the knowledge library that these models fall short of.
Second, both models do not verify and examine the codes they generate. It seems that when the
models generate a bad code snippet that cannot solve the input case, they just “let it go” without
taking another stab for larger chance towards success. And of course, when they encounter similar
cases again, they keep “stepping on the same rake”. Human beings, on the other hand, would verify
and examine the acquired knowledge by proposing a set of test scenarios before storing them in the
library (Brul ´e & Blount, 1989). It’s crucial that a neuro-symbolic reasoning model is equipped with
the same abilities to verify the codes it produces, stores them in a library if satisfactory, and makes
another attempt when the codes fail.
To this end, we introduce a novel Generative Neuro-symbolic Visual Reasoning Model (GENOME),
proficient in assimilating new neural modules from a limited set of training examples. This model
2

--- PAGE 3 ---
Preprint
excels in handling standard visual reasoning tasks such as visual question answering. Addition-
ally, it demonstrates outstanding module transfer capabilities for tasks like image editing, and ex-
hibits an exceptional ability to generalize to new reasoning tasks with limited training examples.
As illustrated in Figure 2, GENOME comprises three stages: 1) module initialization, 2) mod-
ule generation, and 3) module execution. From the initial training set examples, an LLM discerns
the necessity for a new module to tackle the task and, if required, produces the respective input
and output. In the second stage, LLMs implement and refine the new module, ensuring seamless
integration with existing modules and resulting in accurate responses to the training queries. Dur-
ing testing, the LLM first converts language instructions into executable high-level programs like
COMPARE ATTRIBUTE(IMAGE,BOX0,BOX1,ATTR) for comparing attributes of different bound-
ing boxes. The program will be run with the new module sets, producing the desired outputs.
We assessed the performance of GENOME across six visual reasoning tasks, spanning from vi-
sual question answering (Hudson & Manning, 2019) to Raven’s Progressive Matrices (Zhang et al.,
2019). The experimental findings reveal that GENOME delivers competitive results on standard
benchmarks, ensuring both transparency and interoperability. Notably, modules honed on these
standard tasks can be adeptly adapted to diverse domains, including image editing and knowl-
edge tagging (Gupta & Kembhavi, 2022). Additionally, with minimal training examples, GENOME
demonstrates the capability to manage new visual reasoning tasks (Burke, 1985; Jiang et al., 2023)
by repurposing modules. Our code and data will be publicly available.
2 R ELATED WORK
Visual Reasoning. Our work aims to handle visual reasoning tasks, which require a model to draw
new inferences based on the acquired visual cues in images or videos (Hudson & Manning, 2019;
Kazemzadeh et al., 2014; Goyal et al., 2017; Zhang et al., 2019; Jiang et al., 2023). Typical tasks
for visual reasoning include visible question answering (Goyal et al., 2017; Hudson & Manning,
2019), visual grounding (Kazemzadeh et al., 2014; Yu et al., 2016; Chen et al., 2020) and Raven’s
Progressive Matrices (Burke, 1985; Zhang et al., 2019). Various models (Hudson & Manning, 2018;
Yu et al., 2018; Zhang et al., 2021; Ding et al., 2023) have been developed to handle these tasks but
most of them are ad-hoc and carefully designed for a specific task, leaving it an open research ques-
tion on how to build a general model that can handle different kinds of visual reasoning problems
by only showing a few examples.
Neuro-symbolic Visual Reasoning. Our work is also closely related to neuro-symbolic visual rea-
soning models (Andreas et al., 2016b; Mao et al., 2019a; Chen et al., 2021c; 2022), where the
models decompose the query of the visual reasoning tasks into a series of reasoning steps and repre-
sent each reasoning step with a neural module (i.e., a code snippet for achieving specific functions
like localizing objects and recognizing object categories). While these models have better model
interoperability and data efficiency than previous connectionist models (Hudson & Manning, 2018;
Anderson et al., 2018), they often show their limitations in representing natural language instruc-
tions in the wild with the limited pre-defined reasoning steps (Yang et al., 2020; Chen et al., 2021b).
Moreover, they need to manually define and implement each neural module one by one, making it
hard to scale up and handle multiple tasks within a single model.
Foundation Models for Reasoning. Recently, large language models (LLMs) (Brown et al., 2020;
Ouyang et al., 2022) have been widely used in language understanding (Hendrycks et al., 2020) and
reasoning (Cobbe et al., 2021; Amini et al., 2019). Schick et al. (2023) develop the toolformer to
show that LLMs can use external tools to better handle language tasks. Cai et al. (2023) shows that
LLMs can make simple tools for natural language tasks by writing code snippets. LLMs have also
been used in vision-language tasks. Most of these works (Li et al., 2023; Alayrac et al., 2022) con-
nect LLMs with additional vision encoders and fine-tune them with massive vision-language pairs.
As evaluated by Xu et al. (2023b), while these models show great performance on in-domain tasks,
they also perform poorly on tasks out of the training domains. They are also extremely computation-
expensive. For example, it takes 15 days to use 1536 TPUv4 for training Flamingo (Alayrac et al.,
2022).
Visual Programming by LLMs. Another line of research has been combining vision models (Li*
et al., 2022; Kirillov et al., 2023; Radford et al., 2021) with LLMs in an off-shelf manner. Early
models (Yang et al., 2022; Chen et al., 2023b) transformed images into captions and append the cap-
3

--- PAGE 4 ---
Preprint
Test cases
Module signaturesClass COMPARE_ATTRIBUTE(): """ Input:   ……       Output:   ……  """  Module signaturesClass COMPARE_ATTRIBUTE(): """ Input:   ……       Output:   …… """  
Operation stepsBOX0=LOC(IMAGE,object='couch’)BOX1=LOC(IMAGE,object='sky’) ANSWER0=COMPARE_ATTRIBUTE(IMAGE,BOX0,BOX1)…….BOX0=LOC(IMAGE,object='couch’)BOX1=LOC(IMAGE,object='sky’) ANSWER0=COMPARE_ATTRIBUTE(IMAGE,BOX0,BOX1)…….Q1: Does the dusbin have the same material as the fridge?Q2. …
Testing examples
Class COMPARE_ATTRIBUTE(): def __init__(self):  … def execution(self, image, box..  … 
Module 1:  LOC(IMAGE,OBJECT)Module 2:  VQA(IMAGE,QUESTION)…New Moule : COMPARE_ATTRIBUTE(IMAGE, BOX1, …)Scalable Module LibraryScalable Module LibraryGenerate modulesEvaluated on test casesPass the test cases?YesBOX0=LOC(IMAGE,object=‘dusbin’)BOX1=LOC(IMAGE,object=‘fridge’) ANSWER0=COMPARE_ATTRIBUTE(IMAGE,BOX0,BOX1)…….Code ExecutionAnswer: No Can existing modules handle the case?What module should we make?
Test image
TrainimageLLMLLMNo
(1). Module Initialization(2). Module Generation(3). Module ExecutionV3
Q1: Is the desk made of different material from the storage box? A1: No.Q2: What’s the material of the chair? A2: Plastic….
Trainingexamples
NoDebug
Figure 2: The framework of our GENOME, which contains three stages, module initialization,
module generation, and module execution. In stage 1, we feed the questions and the signature of
the existing modules to the LLM and ask it to identify whether we can handle the query within the
operations of the existing modules. If not, we ask the LLM to generate the signature of the new
module (i.e. the input and output) and predict the reasoning steps to handle the query task. In stage
2, we feed the module signature and the testing cases to the LLM and ask the LLM to implement the
module and test its pass rate on the training examples. We only accept the modules that successfully
handle the query. In stage 3, we first use the LLM to parse the query into symbolic operations and
then execute these operations on the test images with the help of the scalable module library. We
take VQA as an example and such a framework can also be expanded to other tasks like referring
expression comprehension and Raven.
tions into the LLMs’ prompt to handle vision-language tasks. While the method is simple, they also
perform inferior and lack transparency. Recently, VisPROG (Gupta & Kembhavi, 2022) uses LLMs
to transform language instructions into pre-defined modular operations for step-by-step reasoning.
However, it still requires manually implementing each module one by one. Later, ViperGPT (Sur ´ıs
et al., 2023) shows that the LLMs can be used to write a code snippet for each query instance in-
dependently to handle vision tasks. However, the code it writes has not been examined and tested
by any training examples and there is no guarantee about the performance and code safety. Instead,
we propose GENOME that ask LLMs to create new neural models (i.e. general code snippets to
achieve specific functions) and handle the given tasks through only a few training examples. Such
newly generated modules can cooperate with each other and be reused for other tasks for better
performance.
3 M ETHOD
3.1 O VERALL
In this section, we present a novel framework named as Generative Neuro-symbolic Visual Rea-
soning Model (GENOME) for the acquisition of neural modules and solutions of visual reasoning
tasks with only a limited set of training examples. GENOME comprises several pre-defined opera-
tors that serve as the initial building blocks. Each neural operator corresponds to a neural module
and is implemented using a Python code snippet, thereby enabling specific functions such as object
localization within an image. Nevertheless, it is not possible to pre-define all the necessary neural
modules prior to addressing the visual reasoning tasks. Consequently, there arises a need to generate
new modules based on a limited number of visual reasoning task examples.
Figure 2 illustrates that GENOME consists of three distinct stages: 1) module initialization, 2) mod-
ule generation, and 3) module execution. In the module initialization stage, when provided with
a small set of training examples from the visual reasoning dataset, the primary objective is to de-
4

--- PAGE 5 ---
Preprint
termine whether the existing neural modules are sufficient to address the query examples. If the
existing modules are inadequate for the query instance, GENOME will identify the requirements
for creating new modules, including defining the modules’ input and output specifications. Dur-
ing the module generation stage, GENOME leverages the LLM to implement the neural module
based on the provided training examples and the specified input and output format (i.e., function
signature) and add the module to the module library only when it passes the test cases. Once the
new module is successfully implemented, the module execution orchestrates the transformation of
input queries into a sequence of neural operations. Subsequently, these operations are applied to
the neural modules to obtain the correct output. All these three stages are powered by robust code
generation capabilities and the in-context learning technique. Prompts for each stage can be found
at Figure 15-17
3.2 M ODEL DETAILS
Utilizing a limited number of examples from the training set of visual reasoning tasks, we employ
the GENOME framework, comprising three distinctive stages: module initialization, module gener-
ation, and module execution.
Module Initialization. The initial phase within our GENOME framework is module initializa-
tion, dedicated to determining the set of new modules required to address the visual reasoning task.
As depicted in Figure 2-1, we employ an LLM to assess the feasibility of handling these training
instances using existing neural modules. Should this not be achievable, we task the LLM with
specifying the necessary new modules (e.g. COMPARE ATTRIBUTE in Figure 2) for an accurate
response to the query. The outcome of this stage comprises function signatures detailing the input
and output formats for these new modules. Furthermore, it facilitates the transformation of the input
query into a sequence of reasoning steps, which function as test cases to validate the correctness of
the generated program within module generation. The prompt for the LLM in this stage is shown in
Figure 15.
Module Generation. The second phase of our GENOME framework is module generation, which
focuses on implementing the correct new modules proposed during the module initialization stage.
Specifically, after receiving the signature of a new module, we incorporate corresponding test cases
into the prompt and employ learning-in-context techniques to generate multiple program candidates.
These program candidates are subsequently executed using the provided training examples. If a
program encounters errors during execution, we incorporate the error information into the LLM’s
prompt and instruct it to rectify these issues. We only accept program candidates that achieve a pass
rate surpassing a predefined threshold ( η). This procedure bears resemblance to the code transla-
tion of LLMs discussed in (Chen et al., 2023a), but we extend it to accommodate more intricate
multi-modal input types and instructions from natural language and raw images. The inclusion of
module generation in the context of visual reasoning tasks offers two principal advantages. Firstly,
it upholds the transparency and interpretability of neuro-symbolic models while preserving compet-
itive performance. Secondly, it exhibits generative capabilities and scalability as our GENOME can
autonomously generate new modules tailored to specific tasks.
Module Execution. Given the integration of newly-generated modules with existing neural mod-
ules tailored for visual reasoning, the GENOME framework initiates query parsing from the testing
dataset, transforming them into executable operations through in-context learning. An illustrative
prompt for this stage is depicted in Figure 17. Notably, although various visual reasoning tasks may
possess distinct inputs and outputs, they can re-purpose these intermediary modules designed for
other tasks to enhance overall performance. This feature represents a unique capability for code
generation at the module level, an aspect hitherto unexplored by prior methods(Sur ´ıs et al., 2023;
Gupta & Kembhavi, 2022).
4 E XPERIMENTS
In this section, we present a comprehensive series of experiments to evaluate the performance of our
models. Initially, we demonstrate our models’ effectiveness in learning neural modules on two estab-
lished benchmarks: GQA (Hudson & Manning, 2019), focusing on compositional visual question
5

--- PAGE 6 ---
Preprint
answering, and RefCOCO (Kazemzadeh et al., 2014), which assesses referring expression com-
prehension. Subsequently, we illustrate how the modules acquired from these two datasets can be
successfully applied to novel tasks such as image editing and knowledge tagging. Moreover, we
highlight the adaptability of our framework to address novel visual reasoning tasks (Raven (Zhang
et al., 2019) and MEWL (Jiang et al., 2023)), even with limited training examples. Before delving
into these experiments, we provide an overview of the experimental settings.
Experimental Details. The success of our GENOME relies on a set of pre-defined modules and
APIs as the starting point. We utilize handcrafted modules from VisProg (Gupta & Kembhavi,
2022) as our initial components. Additionally, we incorporate several new APIs from ViperGPT to
enhance module creation. We also include some new APIs from ViperGPT (Sur ´ıs et al., 2023) for
making new modules. In Section 4.4, we also include results parsed by the open-source LLM from
WLM (Xu et al., 2023a) to investigate the influence of different LLM models. A comprehensive list
of the pretrained modules employed in our approach can be found in Section A.1 of the Appendix.
We extract training examples to acquire new modules. More precisely, we extracted 300 examples
from GQA, 100 from RefCOCO, 10 from Raven, and 10 from MEWL.
Datasets and Evaluation Metric. We show experiments of our GENOME on standard vision-
language benchmarks, GQA (Hudson & Manning, 2019) and RefCOCO Kazemzadeh et al. (2014).
GQA is a popular compositional visual reasoning dataset with synthesis multi-hop questions, mak-
ing it suitable for multi-step reasoning. RefCOCO is a typical visual grounding dataset, evaluating
models’ ability to localize objects and understand fine-grained spatial and semantic relationships.
Following ViperGPT, we evaluate GQA on test-dev split and RefCOCO on the testA split. Then, we
show GENOME’s abilities on other the transferred tasks, image editing, and knowledge tagging and
compare it with VisProg. Since the image editing and knowledge tagging datasets from VisProg are
not publicly available, we built two new datasets for evaluation. The new editing dataset contains
50 images and instruction pairs. The new knowledge tagging dataset contains 50 images with 50
referring expressions. We provide more details about the dataset in Appendix A.3. The datasets
will be released for research purposes. Finally, we show that GENOME can learn to handle new
visual reasoning tasks like Raven (Zhang et al., 2019) and MEWL (Jiang et al., 2023) by observing
a few training examples and module learning. Raven is a task for relational and analogical visual
reasoning of image sets and has been widely used for non-verbal intelligence tests. MEWL is a
recent benchmark proposed to assess how machines learn word meaning in grounded visual scenes.
Examples of these tasks can be found at Figure 5 and Figure 6.
4.1 C OMPARISON WITH BASELINES ON VISUAL REASONING .
We conducted analysis between our model and several baseline models us-
ing the GQA and RefCOCO datasets. Due to the deprecation of the origi-
nal professional Codex API ( code-davinci-002 ), we replaced it with the
currently available API ( gpt-3.5-turbo-instruct ) and conducted experi-
ments with both our model and the baseline models to ensure a fair compari-
son. We did not carry out experiments with GPT-4 due to the prohibitive cost.
Methods GQA RefCOCO
BLIP-2 44.7 -
KOSMOS-2 - 57.4
ViperGPT-CodeX 48.1 72.0
VisPROG-Instruct 45.4 -
ViperGPT-Instruct 38.2 62.4
Ours-Instruct 45.6 69.2
Table 1: Evaluation on standard visual rea-
soning benchmarks, GQA and RefCOCO.The results, as presented in Table 1, demonstrate that
our model achieves competitive performance in both
visual question answering and referring expression
comprehension, thus confirming its effectiveness. Fur-
thermore, we provide an illustrative module from our
model in Figure 11. This newly created module has the
capability to utilize various available APIs to select at-
tributes from the images. The step-by-step reasoning
process of our model is detailed in Figure 3, offering
greater transparency compared to end-to-end models.
4.2 GENOME FOR TRANSFER LEARNING
In this section, we demonstrate our model’s robust ca-
pabilities in transfer learning. We augment the modular library by incorporating modules created
6

--- PAGE 7 ---
Preprint
classCHOOSE_ATTRIBUTE():
defpredict(self, img,boxes,obj,attr1,attr2):
iflen(boxes) > 0:
box = boxes[0]
box = self.expand_box (box, img.size)
out_img = img.crop(box)
else:
out_img = img
prompt1 = f'Tellme the attributes when the {obj} is {attr1} in one sentence.'
prompt2 = f'Tellme the attributes when the {obj} is {attr2} in one sentence.'
obj_desc1 = API.gpt3(prompt1, 'gpt3_general')
obj_desc2 = API.gpt3(prompt2, 'gpt3_general')
result1 = API.clip (out_img,obj_desc1 )
result2 = API.clip (out_img,obj_desc2 )
ifresult1 > result2:
result= attr1
else:
result= attr2
returnresult
defexecute(self, img,boxes,obj,attr1,attr2):
result= self.predict (img,boxes,obj,attr1,attr2)
returnresultBOX0=LOC(image= IMAGE,object =‘shirt’) 
ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,
obj=‘shirt’,attr1=‘long sleeved’, attr2 
=‘short sleeved’)
FINAL_RESULT=RESULT(var=ANSWER0)
Question: Is the shirt short sleeved or long sleeved?
obj_desc1 : The shirt has long sleeves, a collar, and buttons.
obj_desc2 : The shirt is short sleeved with a round neckline, 
short sleeves, and a straight hem.
result1 : 0.9; result2 : 0.2
result : short sleeved
IMAGE:
Generated Program:
result: short sleevedimg: out_img :GQA
RefCOCO
BOXLIST0=LOC(image=IMAGE,object=‘kid’)
BOXLIST1=FILTER_PROPERTY(image= IMAGE,box_list = 
BOXLIST0,object=‘ kid’,attribute =‘blue’)
BOX2=SORT_SPATIAL(image= IMAGE,box_list =BOXLIST1,
location=‘ front’,index =1)
FINAL_RESULT=RESULT(var=BOX2)Generated Program:
classSORT_SPATIAL():
defparse_depth (self,img,box_list ):        
box_depth_list = [] # compute depths for front or background
depth_map = API.depth (img)        
forbox inbox_list :            
x1, y1, x2, y2 = box            
depth_map = np.array (depth_map )            
avg_depth = np.median (depth_map [x1:x2, y1:y2])            
box_depth_list.append ((box, avg_depth ))        
returnbox_depth_list
defpredict( self,img,box_list,location,index ):        
if"front" inlocation or"behind" inlocation:            
box_depth_list = self.parse_depth (img, box_list )            
box_list_sorted = sorted( box_depth_list , key=lambda x: x[1])            
out_box_list = [box_i[0] forbox_iinbox_list_sorted ]            
if"behind" in location:                
out_box_list.reverse ()        
else:            
if"left" inlocation:                
box_list = sorted( box_list , key=lambda x: x[0])                    
.
.
returnout_box_list
defexecute( self,img,box_list,location,index ):        
returnself.predict (img,box_list,location,index )IMAGE:img: box_depth_list :
depth: 0.13,
0.03
box_list_sorted :
depth: 0.13 0.03,out_box_list :
,
result:
Question: Closest kid in blue.
Figure 3: Qualitative examples of GENOME’s on GQA and RefCOCO. The query images, language
instructions, and the parsed programs are shown on the left. The corresponding new modules and
the value of important variables are shown on the right.
Tag the second actor who played James Bond from the leftTag the Grammy-winning musician in the middleTag the second gas planet from the left in our solar systemTag the social media platform logo in the same color as FacebookSelect the superhero with the same color as the Hulk and create a color popReplace the bottle of the same material as the blue one with a red bottleCreate a color pop of the third person from the rightHide the second man from the left with 8)
Figure 4: Qualitative examples of GENOME’s on the image editing and knowledge tagging tasks.
The language instructions of the tasks are shown the original images while the modified results of
the images are shown below the original images. The emphasis of the instructions is highlighted
with red colors, which requires our new modules to handle. While the key regions of the output
images are bounded with green colors.
from GQA and RefCOCO, employing in-context examples to guide the Language Model (LLM) in
generating step-by-step instructions for task execution. Qualitative results for this task are depicted
in Figure 4. As illustrated in Figure 4, our model excels in generating semantically accurate images
using the newly added module, whereas the baseline VisProg struggles to capture the required re-
lationships with its fixed, pre-defined module library. To provide a more comprehensive evaluation
of image editing, we enlist annotators to manually assess the correctness of the generated images.
7

--- PAGE 8 ---
Preprint
MethodsImage Editing Tagging Localization
Accuracy Precision Recall F1 Precision Recall F1
VisProg 16.7 18.4 21.7 19.9 32.8 35.3 34.0
GENOME 55.3 67.1 52.3 58.8 76.9 57.9 66.0
Table 2: Evaluation of GENOME on Transfer Learning with image editing and knowledge tagging
tasks. Our GENOME shows much better performance on all criteria, showing the effectiveness of
the transferred modules. A qualitative comparison can be seen Figure 7 in the Appendix.
Methods Center L-R U-D
ResNet+DRT 58.1 65.8 67.1
ALANS-V 98.4 97.3 96.4
GENOME 80.1 67.6 69.1
Human 95.5 86.4 81.8
Table 3: Evaluation of GENOME on
Raven (Zhang et al., 2019). Compared with
methods trained with massive in-domain data,
our model performs competitively.Methods shape color material
Aloe 34.2 33.2 31.0
Flamingo-1.1B 49.3 35.3 48.5
GENOME 43.7 45.3 41.0
Human 92.4 87.2 72.7
Table 4: Evaluation of GENOME on
MEWL (Jiang et al., 2023). Compared with
approaches trained on extensive in-domain data,
our model shows competitive performance.
The models’ performance is compared in Table 2, where our model outperforms the baseline. In
the context of knowledge tagging, we task annotators with marking image regions referenced by ex-
pressions and employ the same metrics as RefCOCO for evaluating the accuracy of bounding boxes
and employ the BERT score to assess the correctness of labeled names. Our model demonstrates
superior performance in both image editing and knowledge tagging. We show a typical example in
Figure 7 of the Appendix to show how our GENOME make use of new modules to perform better
knowledge tagging result the baseline.
4.3 GENOME ONFEW-SHOT TASK LEARNING .
As a general module learning framework, our model is not only able to learn new modules to handle
existing tasks but also can learn to handle new visual reasoning tasks from a few training examples.
We evaluate such abilities on new tasks, Raven (Zhang et al., 2019) and MEWL (Jiang et al., 2023).
Specifically, we first prompt the LLM to learn pattern recognition modules for visual understanding
and then ask the LLM to generate a solver module to handle the task. The instances of our model
prediction are shown in Figure 5 and Figure 6. Note that visual reasoning from Raven is widely
used in intelligent testing for humans, which shows our model’s strong capabilities and potential.
We report the performance of our model and the baselines in Table 3 and Table 4. Our model is
significantly better than previous fully-supervised methods like ResNet+DRT (Zhang et al., 2019)
and Aloe (Ding et al., 2021), showing its effectiveness. Note that all these models ResNet+DST,
ALANS-V (Zhang et al., 2022), Aloe (Ding et al., 2021) and Flamingo (Alayrac et al., 2022) are
models fully-finetuned on in-domain data, while our GENOME is a general few-shot framework to
learn modules for problem-solving. Moreover, we can observe the new compositionality and module
re-usage from Figure 8 of the Appendix. Although the SOLVER module was originally learned from
center-type problems, it can be naturally transferred to other types like left-right and up-down.
4.4 A BLATIONS
Methods RefCOCO
GENOME w/o ML 62.3
GENOME-WLM 64.4
GENOME (10) 49.4
GENOME (50) 67.0
GENOME (100) 67.1
Table 5: Ablation study of GENOME on
RefCOCO.To gauge the efficacy of our model, we conducted
a series of ablation studies addressing the following
key inquiries: Q1How effective is module learn-
ing? Q2What impact does the quantity of train-
ing examples have on model performance? Q3How
crucial is the LLM’s capability for optimal perfor-
mance? In our experiments, GENOME w/o ML repre-
sents a configuration without any new module learn-
8

--- PAGE 9 ---
Preprint
12563478AnswerSetProblemMatrixCOLOR=DETECT_COLOR(image=IMAGE)SHAPE=DETECT_SHAPE(image=IMAGE)SIZE=DETECT_SIZE(image=IMAGE)RAVEN
MEWL
acnoacnoatlecatlecercanercancyangraycyanCOLOR=DETECT_COLOR(image=IMAGE)ANSWER=SOLVER(image=IMAGE,color=COLOR,word=WORD)FINAL_RESULT=RESULT(var=ANSWER)ANSWER=SOLVER(image=IMAGE,color=COLOR,shape=SHAPE,size=SIZE)FINAL_RESULT=RESULT(var=ANSWER)IMAGE:color: 252shape: ‘square’size: 32color: 84shape: ‘triangle’size: 56color: 140shape: ‘hexagon’size: 72color: 140shape: ‘triangle’size: 40
Problem MatrixAnswer Set......
color_type: arithmeticshape_type: constantsize_type: progressionANSWER: 3
IMAGE:WORD:
isageatlecercanpreauacnoContextQuery
New WordsOptions……
cyanacnograyatlecgreenercancyan
cyanacnoANSWER: acno
Figure 5: A qualitative example from the Raven dataset (Zhang et al., 2019) is provided. This task
involves a set of images with varying visual attributes, such as colors, shapes, and locations. Models
are tasked with identifying the image that best matches the missing item in the Problem Matrix.
GENOME exhibits the capability to compose modules (i.e. DETECTSHAPE and SOLVER ) for
detecting these attribute rules and constructing a solver module to address the task. The correct
answer is indicated by a green box.
12563478AnswerSetProblemMatrixCOLOR=DETECT_COLOR(image=IMAGE)SHAPE=DETECT_SHAPE(image=IMAGE)SIZE=DETECT_SIZE(image=IMAGE)RAVEN
MEWL
acnoacnoatlecatlecercanercancyangraycyanCOLOR=DETECT_COLOR(image=IMAGE)ANSWER=SOLVER(image=IMAGE,color=COLOR,word=WORD)FINAL_RESULT=RESULT(var=ANSWER)ANSWER=SOLVER(image=IMAGE,color=COLOR,shape=SHAPE,size=SIZE)FINAL_RESULT=RESULT(var=ANSWER)IMAGE:color: 252shape: ‘square’size: 32color: 84shape: ‘triangle’size: 56color: 140shape: ‘hexagon’size: 72color: 140shape: ‘triangle’size: 40
Problem MatrixAnswer Set......
color_type: arithmeticshape_type: constantsize_type: progressionANSWER: 3
IMAGE:WORD:
isageatlecercanpreauacnoContextQuery
New WordsOptions……
cyanacnograyatlecgreenercancyan
cyanacnoANSWER: acno
Figure 6: A qualitative illustration from the MEWL dataset (Jiang et al., 2023) is presented. This
task entails a set of images featuring diverse visual attributes, such as material and shapes, and it
necessitates models to determine the word that corresponds to the query image. GENOME demon-
strates the capability to generate modules for identifying these attribute rules and composing a solver
module to address the task. The correct answer is indicated by a green box.
ing but relies heavily on ViperGPT and VisProg-defined modules, directing the LLM to pin-
point a region matching the referring expression. On the other hand, GENOME-WLM replaces
thegpt-3.5-turbo-instruct API withWizardCoder-Python-34B-V1.0 from Wiz-
ardLM (Xu et al., 2023a). The designations GENOME (10)/ (50) / (100) indicate models trained with
10, 50, and 100 examples, respectively. For resource constraints, we limited our experimentation to
800 RefCOCO samples.
Table 5 presents the outcomes, leading to these insights: module learning, given sufficient test in-
stances, can bolster task performance ( Q1addressed). A paucity of training examples, such as 10 for
RefCOCO, might induce overfitting, but this diminishes with increased training data (50 examples),
improving overall performance ( Q2addressed). Finally, model performance appears intrinsically
tied to the LLM’s capacity, with superior LLMs delivering enhanced results ( Q3addressed).
5 C ONCLUSION
In this study, we introduce GENOME, which is designed to tackle visual reasoning tasks when
confronted with limited training data. This approach combines language models to parse natural
language into executable operations and create specialized visual modules tailored to the given task.
Our model exhibits competitive performance on conventional tasks, effortless transfer of acquired
9

--- PAGE 10 ---
Preprint
modules to novel tasks, and the capability to adapt to new tasks even with limited training data. Our
GENOME also proposes numerous avenues for future research. Firstly, it still necessitates task-
specific prompts for each distinct reasoning task, and it would be intriguing to explore the use of a
universal prompt for all tasks. Secondly, the framework can be extended to encompass a broader
range of multi-modal reasoning tasks, incorporating diverse inputs such as audio, video, and tactile
information.
REFERENCES
Gradient-based learning applied to document recognition. Proceedings of the IEEE , 86(11):2278–
2324, 1998.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. arXiv , 2022.
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. Mathqa: Towards interpretable math word problem solving with operation-based for-
malisms. arXiv preprint arXiv:1905.13319 , 2019.
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and
Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answer-
ing. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
6077–6086, 2018.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural
networks for question answering. arXiv preprint arXiv:1601.01705 , 2016a.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
CVPR , pp. 39–48, 2016b.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
James F. Brul ´e and Alexander Blount. Knowledge acquisition. 1989. URL https://api.
semanticscholar.org/CorpusID:18663796 .
Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Boerschinger, and Tal Schuster.
Tomayto, tomahto. beyond token-level answer equivalence for question answering evaluation,
2022.
Henry R Burke. Raven’s progressive matrices (1938): More on norms, reliability, and validity.
Journal of Clinical Psychology , 41(2):231–235, 1985.
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as
tool makers. arXiv , 2023.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
language models trained on code. arXiv preprint arXiv:2107.03374 , 2021a.
Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network
for compositional visual reasoning. Proceedings of WACV , 2021b.
Xinyun Chen, Maxwell Lin, Nathanael Sch ¨arli, and Denny Zhou. Teaching large language models
to self-debug. arXiv preprint arXiv:2304.05128 , 2023a.
Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K Wong, and Qi Wu. Cops-ref: A new dataset and
task on compositional referring expression comprehension. In CVPR , 2020.
Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B Tenenbaum, and
Chuang Gan. Grounding physical concepts of objects and events through dynamic visual reason-
ing. In ICLR , 2021c.
10

--- PAGE 11 ---
Preprint
Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B Tenenbaum, and
Chuang Gan. Comphy: Compositional physical reasoning of objects and events from videos.
arXiv , 2022.
Zhenfang Chen, Qinhong Zhou, Yikang Shen, Yining Hong, Hao Zhang, and Chuang Gan. See,
think, confirm: Interactive prompting between vision and language models for knowledge-based
visual reasoning. arXiv , 2023b.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
David Ding, Felix Hill, Adam Santoro, Malcolm Reynolds, and Matt Botvinick. Attention over
learned object embeddings enables complex visual reasoning. Advances in neural information
processing systems , 34:9112–9124, 2021.
Mingyu Ding, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B Tenenbaum, and
Chuang Gan. Embodied concept learner: Self-supervised learning of concepts and mapping
through instruction following. In CoRL , 2023.
Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. Towards next-
generation intelligent assistants leveraging llm techniques. In Proceedings of the 29th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 5792–5793, 2023.
Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke
Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: growing generalizable,
interpretable knowledge with wake–sleep bayesian program learning. Philosophical Transactions
of the Royal Society A , 381(2251):20220050, 2023.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2017.
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning
without training. ArXiv , abs/2211.11559, 2022.
Harry Frederick Harlow. The formation of learning sets. Psychological review , 56 1:51–65, 1949.
URLhttps://api.semanticscholar.org/CorpusID:22804426 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300 , 2020.
Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):
1735–1780, 1997.
Drew A Hudson and Christopher D Manning. Compositional attention networks for machine rea-
soning. arXiv preprint arXiv:1803.03067 , 2018.
Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning
and compositional question answering. In CVPR , 2019.
Guangyuan Jiang, Manjie Xu, Shiji Xin, Wei Liang, Yujia Peng, Chi Zhang, and Yixin Zhu. Mewl:
Few-shot multimodal word learning with referential uncertainty. In ICML , 2023.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to
objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP) , pp. 787–798, 2014.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and Ross Girshick.
Segment anything. arXiv:2304.02643 , 2023.
11

--- PAGE 12 ---
Preprint
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people, 2016.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models. arXiv , 2023.
Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong,
Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao.
Grounded language-image pre-training. In CVPR , 2022.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The Neuro-
Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervi-
sion. In ICLR , 2019a.
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neuro-
symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In
ICLR , 2019b.
Tom Michael Mitchell, Richard M. Keller, and Smadar T. Kedar-Cabelli. Explanation-based gen-
eralization: A unifying view. Machine Learning , 1:47–80, 1986. URL https://api.
semanticscholar.org/CorpusID:117264 .
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision. In ICML , 2021.
Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction.
ICCV , 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-
ence on computer vision and pattern recognition , pp. 10684–10695, 2022.
Timo Schick, Jane Dwivedi-Yu, Roberto Dess `ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to
use tools. arXiv , 2023.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational agent that
continually learns to responsibly engage. arXiv , 2022.
D´ıdac Sur ´ıs, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution for
reasoning. arXiv , 2023.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.
arXiv preprint arXiv:2304.12244 , 2023a.
Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan
Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large
vision-language models. arXiv , 2023b.
Jianwei Yang, Jiayuan Mao, Jiajun Wu, Devi Parikh, David D Cox, Joshua B Tenenbaum, and
Chuang Gan. Object-centric diagnosis of visual reasoning. arXiv preprint arXiv:2012.11587 ,
2020.
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang.
An empirical study of gpt-3 for few-shot knowledge-based vqa. In AAAI , 2022.
12

--- PAGE 13 ---
Preprint
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. Neural-
symbolic vqa: Disentangling reasoning from vision and language understanding. In NeurIPS ,
2018.
Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context
in referring expressions. In Computer Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pp. 69–85. Springer, 2016.
Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mat-
tnet: Modular attention network for referring expression comprehension. In Proceedings of the
IEEE conference on computer vision and pattern recognition , pp. 1307–1315, 2018.
Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts
with visual concepts. arXiv preprint arXiv:2111.08276 , 2021.
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational
and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pp. 5317–5327, 2019.
Chi Zhang, Baoxiong Jia, Song-Chun Zhu, and Yixin Zhu. Abstract spatial-temporal reasoning
via probabilistic abduction and execution. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , 2021.
Chi Zhang, Sirui Xie, Baoxiong Jia, Ying Nian Wu, Song-Chun Zhu, and Yixin Zhu. Learning alge-
braic representation for systematic generalization in abstract reasoning. In European Conference
on Computer Vision , pp. 692–709. Springer, 2022.
13

--- PAGE 14 ---
Preprint
A A PPENDIX
In this section, we substantiate our claims in the paper by providing additional implementation
details (Section A.1), exemplar prompts for each stage (Section A.2), details on dataset collection
(Section A.3), qualitative examples of new learned modules (Section A.4).
A.1 I MPLEMENATION DETAILS
Pre-defined Modules and API models. The success of our model still requires a set of pre-
defined APIs. Following the modules in VisProg and ViperGPT, we adopt the following APIs.
We adopt GLIP (Li* et al., 2022) for object localization. We adopt gpt-3.5-turbo-instruct from
OpenAI and WizardCoder-Python-34B-V1.0 from WizardLM for code generation. We use BLIP (Li
et al., 2023) for answering simple questions about images. We use CLIP (Radford et al., 2021) and
X-VLM (Zeng et al., 2021) for image-text classification. We use MiDaS (Ranftl et al., 2021) for
estimating depths in images. We use stable diffusion (Rombach et al., 2022) for modifying image
patches. Based on these APIs, we construct a set of pre-defined modules following VisProg. These
pre-defined modules will be used to cooperate with the new learned modules for visual reasoning.
Descriptions Modules
Image UnderstandingLocfor object location, FaceDet for face detection, Select and
Filter Property for image-text classification. Filter Spatial for
selecting image regions;
Image ManipulationReplace for image editing, colorPop for changing images colors,
BgBlur for blurring background, Tagfor annotating box regions
andEmoji for face tagging. Crop and its variants for cropping
patches from the images.
OthersList for retrieving factual knowledge, Count for counting object
numbers, Eval ,Result ,BOX2MASK andMASK2BOX for
formatting outputs.
Table 6: Pre-defined Modules used in GENOME.
A.2 P ROMPTS FOR EACH STAGE .
The ability of Our GENOME is from in-context learning of LLMs (Brown et al., 2020), when the
prompts are keys to tell what the LLM should generate. We show the exemplar prompts of our
models to learn the VQA task in Figure 15-17.
A.3 D ETAILS AND EXAMPLES OF THE NEWDATASETS .
To evaluate Knowledge Tagging, 50 tagging instructions are annotated on 50 internet images in-
cluding personalities and a variety of objects such as logos, flowers, buildings, fruits and sports,
among others. For each instruction, we manually annotated the ground truth bounding box and the
associated tag. For image editing assessment, we collected 50 editing instructions on 50 images
including personalities and various objects like foods, furniture, animals, utensils etc. 25 images are
from the COCO dataset and the other 25 images are from the internet. For the image editing tasks,
we ask three annotators to estimate whether the editing is correct or not. For the knowledge tagging
task, we consider the localization is correct if the detected region has an IoU higher 0.5 with the
ground-truth annotation. For text tagging, we compare the prediction with the annotated text with
BERT matching (BEM) (Bulian et al., 2022). If the matching score is higher than 0.5, we consider
it a successful matching. More examples of the two datasets can be found at Figure 9 and Figure 10.
A.4 Q UALITATIVE EXAMPLES .
In this subsection, we show the qualitative examples of the learned modules and qualitative cases of
how they handle different tasks. We show an example of GENOME performs better than VisProg
14

--- PAGE 15 ---
Preprint
OBJ0=LOC(image=IMAGE,object='fruit or vegetable’)LIST0=LIST(query='fruits and vegetables',max=20)OBJ1=CLASSIFY(image=IMAGE,object=OBJ0,categories=LIST0)OBJ2=CLASSIFY(image=IMAGE,object=OBJ0,categories='grape’)OBJ3=REDUCE_MASK(mask_list1=OBJ1,mask_list2=OBJ2)OBJ4=META_COMPARE(function_name='COMPARE_COLOR',image=IMAGE,obj_list=OBJ3,obj_cmp=OBJ2,name1='fruit or vegetable’,name2 ='grape',attribute='same’)IMAGE0=TAG(image=IMAGE,object=OBJ4)FINAL_RESULT=RESULT(var=IMAGE0)Generated Program:
OBJ0=LOC(image=IMAGE,object='grape’)LIST0=LIST(query='common fruits and vegetables of the same color as the grape',max=20)OBJ1=CLASSIFY(image=IMAGE,object=OBJ0,categories=LIST0)IMAGE0=TAG(image=IMAGE,object=OBJ1)FINAL_RESULT=RESULT(var=IMAGE0)Generated Program:VisProgOursKnowledge TaggingOBJ2:COMPARE_COLOROBJ4:FINAL_RESULT:OBJ3:,,OBJ3,Question: Tag common fruits and vegetables of the same color as the grape.IMAGE:
. . .
OBJ1:FINAL_RESULT:
Figure 7: A typical example of how our GENOME outperforms VisProg on knowledge tagging. In
the top, our model is able to make use of the COMPARE COLOR module learned from GQA to
localize the correct region while VisProg fail to generate the correct program with its fixed module
library.
in Figure 7. At the top of Figure 7, our model effectively utilizes the COMPARE COLOR module
acquired from GQA to pinpoint the correct region, whereas VisProg fails to generate the correct
program due to its rigid module library. Figure 8 highlights emerging forms of compositionality
and module re-usage. Notably, although the SOLVER module was originally trained on center-type
problems within the Raven dataset, it demonstrates inherent adaptability to other problem types,
including left-right and up-down orientations.
New Learned Modules. We show the examplar new learned modules from the GQA and Ref-
COCO in Figure 11-14. As shown in Figure 11, the new learned module ( CHOOSE ATTRIBUTE)
is able to use the LLM to retrieve relevant knowledge first and then adopt the image-text classifier
to match the attributes. In Figure 13-14, we see that the new module SORT SPATIAL is able to
localize objects with spatial index.
15

--- PAGE 16 ---
Preprint
Generated Program:COLOR=DETECT_COLOR(image=IMAGE)SHAPE=DETECT_SHAPE(image=IMAGE)SIZE=DETECT_SIZE(image=IMAGE)ANSWER=SOLVER(image=IMAGE,color=COLOR,shape=SHAPE,size=SIZE)FINAL_RESULT=RESULT(var=ANSWER)AnswerSetProblemMatrixIMAGE:
AnswerSetProblemMatrixIMAGE:
AnswerSetProblemMatrixIMAGE:
CenterLeft-RightUp-Down
Generated Program:BOX0=LOC(image=IMAGE,object=‘LEFT’)IMAGE0=CROP(image=IMAGE,box=BOX0)BOX1=LOC(image=IMAGE,object=‘RIGHT’)IMAGE1=CROP(image=IMAGE,box=BOX1)COLOR=DETECT_COLOR(image0=IMAGE0,image1=IMAGE1)SHAPE=DETECT_SHAPE(image0=IMAGE0,image1=IMAGE1)SIZE=DETECT_SIZE(image0=IMAGE0,image1=IMAGE1)ANSWER=SOLVER(image0=IMAGE0,image1=IMAGE1,color=COLOR,shape=SHAPE,size=SIZE)FINAL_RESULT=RESULT(var=ANSWER)Generated Program:BOX0=LOC(image=IMAGE,object=‘TOP’)IMAGE0=CROP(image=IMAGE,box=BOX0)BOX1=LOC(image=IMAGE,object=‘BOTTOM’)IMAGE1=CROP(image=IMAGE,box=BOX1)COLOR=DETECT_COLOR(image0=IMAGE0,image1=IMAGE1)SHAPE=DETECT_SHAPE(image0=IMAGE0,image1=IMAGE1)SIZE=DETECT_SIZE(image0=IMAGE0,image1=IMAGE1)ANSWER=SOLVER(image0=IMAGE0,image1=IMAGE1,color=COLOR,shape=SHAPE,size=SIZE)FINAL_RESULT=RESULT(var=ANSWER)
Figure 8: New compositionality and module re-usage in the Raven dataset. While the SOLVER
module was initially trained on center-type problems in the Raven dataset, it exhibits a natural
transferability to other types, such as left-right and up-down problems.
Create a color pop of the first boat from the frontCreate a color pop of the first child from the right
Hide Tim Robbins with ;) and Morgan Freeman with 8)Replace the second pizza from the top with a hamburgerReplace the spoon of the same material as the spatula with a knife
Select the lamp with the same color as the one at the bottom and create a color pop
Figure 9: More examples of the new image edit dataset. The dataset asks models to edit images’
fine-grained and regional details according to diverse language instructions.
16

--- PAGE 17 ---
Preprint
Tag the famous painting of the 
Louvre in the leftTag the famous landmark of Europe 
in the bottom rightTag the second famous film 
director from the leftTag the common bird of 
the same color as the ibis
Tag the second dog from 
the rightTag the second Nobel Laureate 
in Physics from the left
Figure 10: More examples of the new knowledge tagging dataset. The dataset requires models to
localize the target region and tag the region with the desired information.
17

--- PAGE 18 ---
Preprint
1class CHOOSE_ATTRIBUTE ():
2 """
3 Input:
4 image: an image object
5 box: a list of bounding boxes
6 object: a string
7 attribute1: a string
8 attribute2: a string
9Output:
10 result: a string
11 Examples:
12 Question: Is the coat thick or thin?
13 BOX0=LOC(image=IMAGE,object='coat')
14 ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,object='coat',
15 attribute1='thick',attribute2='thin')
16 FINAL_RESULT=RESULT(var=ANSWER0)
17"""
18step_name = 'CHOOSE_ATTRIBUTE'
19
20 def__init__(self):
21 print(f'Registering {self.step_name }step')
22
23 defexpand_box(self,box,img_size,factor=1.5):
24 W,H = img_size
25 x1,y1,x2,y2 = box
26 dw = int(factor *(x2-x1)/2)
27 dh = int(factor *(y2-y1)/2)
28 cx = int((x1 + x2) / 2)
29 cy = int((y1 + y2) / 2)
30 x1 = max(0,cx - dw)
31 x2 = min(cx + dw,W)
32 y1 = max(0,cy - dh)
33 y2 = min(cy + dh,H)
34 return[x1,y1,x2,y2]
35
36 defpredict(self,img,boxes,obj,attr1,attr2):
37 iflen(boxes) > 0:
38 box = boxes[0]
39 box = self.expand_box(box, img.size)
40 out_img = img.crop(box)
41 else:
42 out_img = img
43 prompt1 = f'Tell me the attributes when the {obj}is{attr1 }in
44 one sentence.'
45 prompt2 = f'Tell me the attributes when the {obj}is{attr2 }in
46 one sentence.'
47 obj_desc1 = API.gpt3(prompt1, 'gpt3_general')
48 obj_desc2 = API.gpt3(prompt2, 'gpt3_general')
49 result1 = API.clip(out_img,obj_desc1)
50 result2 = API.clip(out_img,obj_desc2)
51 ifresult1 > result2:
52 result = attr1
53 else:
54 result = attr2
55 returnresult
56
57 defexecute(self,img,boxes,obj,attr1,attr2):
58 result = self.predict(img,boxes,obj,attr1,attr2)
59 returnresult
Figure 11: Exemplar generated module from the GQA dataset. This automatically constructed
module can make use of different APIs to compare attributes of an image region.
18

--- PAGE 19 ---
Preprint
1class COMPARE_COLOR ():
2"""
3Input:
4 image: an image object
5 box1: a list of bounding boxes
6 box2: a list of bounding boxes
7 object1: a string
8 object2: a string
9 compare_type: a string
10Output:
11 result: a string
12 """
13 defexpand_box(self,box,img_size,factor=1.5):
14 W,H = img_size
15 x1,y1,x2,y2 = box
16 dw = int(factor *(x2-x1)/2)
17 dh = int(factor *(y2-y1)/2)
18 cx = int((x1 + x2) / 2)
19 cy = int((y1 + y2) / 2)
20 x1 = max(0,cx - dw)
21 x2 = min(cx + dw,W)
22 y1 = max(0,cy - dh)
23 y2 = min(cy + dh,H)
24 return[x1,y1,x2,y2]
25 defpredict(self,img,boxes1,boxes2,obj1,obj2,compare_type):
26 iflen(boxes1) > 0:
27 box1 = boxes1[0]
28 box1 = self.expand_box(box1,img.size)
29 out_img1 = img.crop(box1)
30 else:
31 out_img1 = img
32 iflen(boxes2) > 0:
33 box2 = boxes2[0]
34 box2 = self.expand_box(box2,img.size)
35 out_img2 = img.crop(box2)
36 else:
37 out_img2 = img
38 color1 = API.vqa(out_img1, f'What color is the {obj1 }?')
39 color2 = API.vqa(out_img2, f'What color is the {obj2 }?')
40 prompt = f'Can the {color1 }be regarded as the same color as'
41 f'{color2 }? You should just reply yes or no without any other
42 words.'
43 temp = API.gpt3(prompt, 'gpt3_general')
44 if'same' == compare_type:
45 if'yes' intemp.lower():
46 result = 'yes'
47 elif'no' intemp.lower():
48 result = 'no'
49 elif'different' == compare_type:
50 if'yes' intemp.lower():
51 result = 'no'
52 elif'no' intemp.lower():
53 result = 'yes'
54 else:
55 if'yes' intemp.lower():
56 result = 'yes'
57 elif'no' intemp.lower():
58 result = 'no'
59 returnresult
60 defexecute(self,img,boxes1,boxes2,obj1,obj2,compare_type):
61 result = self.predict(img,boxes1,boxes2,obj1,obj2,compare_type)
62 returnresult
Figure 12: Exemplar generated module from the GQA dataset.
19

--- PAGE 20 ---
Preprint
1class SORT_SPATIAL ():
2"""
3Select objects from the image that match the spatial location.
4Objects are represented by the bounding boxes.
5Returns the bounding boxes that satisfie the condition.
6Input:
7 image: raw PIL image
8 box_list: a list of unormalized bounding boxes
9 location: the location can only be left, middle, right, top,
10 bottom, front and behind
11 index: a number for the rank the object
12Output:
13 box: a bounding box
14Examples:
15 Question: second sandwich from the right on the bottom
16 BOXLIST0=LOC(image=IMAGE,object='sandwich')
17 BOXLIST1=SORT_SPATIAL(image=IMAGE,box_list=BOXLIST0,location=
18 'right',index=2)
19 BOXLIST2=SORT_SPATIAL(image=IMAGE,box_list=BOXLIST1,location=
20 'bottom',index=1)
21 FINAL_RESULT=RESULT(var=BOXLIST2)
22"""
23step_name = 'SORT_SPATIAL'
24 defpredict(self,img,box_list,location,index):
25 ifindex < 0 orindex > len(box_list):
26 return[]
27 ifindex == 0:
28 return[box_list[0]]
29 if"front" inlocation or"behind" inlocation:
30 box_depth_list = self.parse_depth(img, box_list)
31 box_list_sorted = sorted(box_depth_list, key= lambdax: x[1])
32 out_box_list = [box_i[0] forbox_i inbox_list_sorted]
33 if"behind" inlocation:
34 out_box_list.reverse()
35 else:
36 if"left" inlocation:
37 box_list = sorted(box_list, key= lambdax: x[0])
38 elif"right" inlocation:
39 box_list = sorted(box_list, key= lambdax: x[2], reverse
40 =True)
41 elif"top" inlocation:
42 box_list = sorted(box_list, key= lambdax: x[1])
43 elif"bottom" inlocation:
44 box_list = sorted(box_list, key= lambdax: x[3], reverse
45 =True)
46 else:
47 return[]
48 ifindex > len(box_list):
49 return[]
50 out_box_list = [box_list[index-1]]
51 returnout_box_list
52 defcheck_location(self,img,box,location):
53 w, h = img.size
54 x1, y1, x2, y2 = box
55 cx = (x1 + x2) / 2
56 cy = (y1 + y2) / 2
57 if'left' inlocation:
58 ifcx > w / 2:
59 return False
Figure 13: Exemplar generated module from the RefCOCO dataset. The rest part of the code is in
Figure 14.
20

--- PAGE 21 ---
Preprint
1 elif'right' inlocation:
2 ifcx < w / 2:
3 return False
4 if'top' inlocation:
5 ifcy > h / 2:
6 return False
7 elif'bottom' inlocation:
8 ifcy < h / 2:
9 return False
10 return True
11
12 defparse_depth(self,img,box_list):
13 box_depth_list = []
14 # compute depths for front or background
15 depth_map = API.depth(img)
16 forbox inbox_list:
17 x1, y1, x2, y2 = box
18 depth_map = np.array(depth_map)
19 avg_depth = np.median(depth_map[x1:x2, y1:y2])
20 box_depth_list.append((box, avg_depth))
21 returnbox_depth_list
22
23 defexecute(self,img,box_list,location,index):
24 returnself.predict(img,box_list,location,index)
Figure 14: Exemplar generated module from the RefCOCO dataset. The former part of the code is
in Figure 13. This generated module is able to localize objects based on their location in images and
the depth of images.
21

--- PAGE 22 ---
Preprint
1Pre-defined Modules:
2class LOC ():
3"""
4Generate boxes of the object on the image.
5Input:
6 image: an image object
7 object: an object string
8Output:
9 box: a list of bounding boxes
10 Examples:
11 BOX0=LOC(image=IMAGE,object='camel')
12 """
13class COUNT ():
14"""
15Count the number of boxes in the list.
16Input:
17 box: a list of bounding boxes
18Output:
19 number: number of boxes
20Examples:
21 ANSWER0=COUNT(box=BOX1)
22"""
23Suppose you are a program expert. Given a set of pre-defined modules,
24could you identify whether it ispossible to write a program to get the
25answer to the question ?Ifnot, what new modules do we need ?
26Note that you can only use the below pre-defined modules:
27LOC, COUNT, CROP .......
28
29Question: Is the purse to the left orto the right of the person ?
30Yes. The program is:
31BOX0=LOC(image=IMAGE,object='person')
32IMAGE0=CROP_LEFTOF(image=IMAGE,box=BOX0)
33BOX1=LOC(image=IMAGE0,object='purse')
34ANSWER0=COUNT(box=BOX1)
35ANSWER1=EVAL(expr=f"'left' if {ANSWER0 }> 0 else 'right'")
36FINAL_RESULT=RESULT(var=ANSWER1)
37
38Question: Which object islarger, the sphere orthe blue cube ?
39No. We need to make a new module "COMPARE_SIZE" first. Here isthe header
40of the class:
41class COMPARE_SIZE ():
42"""
43Compare the size of two objects in the image.
44One object is identified by the first bounding box of box0
45Another object is identified by the first bounding box of box1
46Input:
47 image: an image object
48 box0: a list of bounding boxes
49 box1: a list of bounding boxes
50Output:
51 flag: return True if first object is larger else False
52Examples:
53 Question: Which object is larger, the sphere or the blue cube?
54 BOX0=LOC(image=IMAGE,object='sphere')
55 BOX1=LOC(image=IMAGE,object='blue cube')
56 FLAG0=COMPARE_SIZE(image=IMAGE,box0=BOX0,box1=BOX1)
57 ANSWER2=EVAL(expr=f"'sphere' if {FLAG0} else 'blue cube'")
58 FINAL_RESULT=RESULT(var=ANSWER)
59"""
60.......
61Question: __INSERT_NEW_QUESTION__
Figure 15: Prompt of module initialization (Stage 1) to propose the module candidates.
22

--- PAGE 23 ---
Preprint
1Pre-defined APIs:
2class API ():
3 defloc(cls, image: Image.Image, obj_name: str) -> List[List[int]]:
4 """
5 Returns boxes of the specific object on the image.
6 Input:
7 image: an image object
8 obj_name: a text string
9 Output:
10 boxes: a list of bounding boxes
11 Example:
12 [[261, 160, 525, 299]] = API.loc(image, "camel")
13 """
14 returnboxes
15 defgpt3(cls, prompt: str) -> str:
16 """
17 Returns response from gpt3
18 Input:
19 prompt: prompt to gpt3
20 Output:
21 response: gpt3 response
22 Example:
23 "joe biden" = API.gpt3("Who is the US President?")
24 """
25 returnresponse
26.......
27
28Given the above APIs, could you write a new module, Loc() ?
29class Loc ():
30"""
31Generate boxes of the object on the image.
32Input:
33 img: an image object
34 obj_name: an object string
35Output:
36 selected_boxes: a list of bounding boxes
37Examples:
38 BOX1=LOC(image=IMAGE0,object='food')
39"""
40Response:
41class Loc ():
42step_name = 'LOC'
43 defexecute(self,img,obj_name):
44 ifobj_name=='TOP':
45 bboxes = [self.top_box(img)]
46 elifobj_name=='BOTTOM':
47 bboxes = [self.bottom_box(img)]
48 elifobj_name=='LEFT':
49 bboxes = [self.left_box(img)]
50 elifobj_name=='RIGHT':
51 bboxes = [self.right_box(img)]
52 else:
53 bboxes = self.predict(img,obj_name)
54 returnbboxes
55
56 defpredict(self,img,obj_name):
57 returnAPI.loc(img,obj_name)
58.......
59Given the above APIs, could you write a new module, __MODULE_NAME__ ?
60__MODULE_HEAD__
Figure 16: Prompt of module generation (Stage 2) to make a module based on the module’s input
and output.
23

--- PAGE 24 ---
Preprint
1Think step by step to answer the question.
2
3You can only use modules below:
4LOC
5COUNT
6EVAL
7RESULT
8VERIFY_ATTRIBUTE
9VERIFY_COLOR
10VERIFY_MATERIAL
11.......
12
13Question: Is the vehicle inthe top of the image ?
14Program:
15BOX0=LOC(image=IMAGE,object='TOP')
16IMAGE0=CROP(image=IMAGE,box=BOX0)
17BOX1=LOC(image=IMAGE0,object='vehicle')
18ANSWER0=COUNT(box=BOX1)
19ANSWER1=EVAL(expr=f"'yes' if {ANSWER0 }> 0 else 'no'")
20FINAL_RESULT=RESULT(var=ANSWER1)
21
22Question: Who iscarrying the umbrella ?
23Program:
24BOX0=LOC(image=IMAGE,object='umbrella')
25IMAGE0=CROP(image=IMAGE,box=BOX0)
26ANSWER0=VQA(image=IMAGE0,question='Who is carrying the umbrella?')
27FINAL_RESULT=RESULT(var=ANSWER0)
28
29Question: Do the towel andthe box have a different colors ?
30Program:
31BOX0=LOC(image=IMAGE,object='towel')
32BOX1=LOC(image=IMAGE,object='box')
33ANSWER0=COMPARE_ATTRIBUTE(image=IMAGE,box1=BOX0,box2=BOX1,object1='towel'
34,object2='box',attribute='color',question=QUESTION)
35FINAL_RESULT=RESULT(var=ANSWER0)
36
37Question: Is the knife made of ceramic ?
38Program:
39BOX0=LOC(image=IMAGE,object='knife')
40ANSWER0=VERIFY_MATERIAL(image=IMAGE,box=BOX0,material='ceramic',object=
41'knife',question=QUESTION)
42ANSWER1=EVAL(expr=f"'yes' if {ANSWER0 }else 'no'")
43FINAL_RESULT=RESULT(var=ANSWER1)
44
45Question: Is the coat thick orthin?
46Program:
47BOX0=LOC(image=IMAGE,object='coat')
48ANSWER0=CHOOSE_ATTRIBUTE(image=IMAGE,box=BOX0,object='coat',attribute1=
49'thick',attribute2='thin')
50FINAL_RESULT=RESULT(var=ANSWER0)
51.......
52
53Question: __INSERT_NEW_QUESTION__
54Program:
Figure 17: Prompt of module execution (Stage 3) to parse programs for a new test case.
24
