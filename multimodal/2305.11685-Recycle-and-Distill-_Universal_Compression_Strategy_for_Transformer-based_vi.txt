# 2305.11685.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2305.11685.pdf
# Kích thước file: 370516 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Tái chế và Chưng cất: Chiến lược Nén Toàn cầu cho Mô hình SSL Giọng nói dựa trên Transformer với Tái sử dụng Bản đồ Chú ý và Chưng cất Che giấu
Kangwook Jang1∗, Sungnyun Kim2∗, Se-Young Yun2, Hoirin Kim1
1Trường Kỹ thuật Điện, KAIST
2Trường Cao học AI, KAIST
{dnrrkdwkd12, ksn4397, yunseyoung, hoirkim }@kaist.ac.kr
Tóm tắt
Các mô hình học tự giám sát (SSL) giọng nói dựa trên Transformer, như HuBERT, cho thấy hiệu suất đáng ngạc nhiên trong nhiều tác vụ xử lý giọng nói khác nhau. Tuy nhiên, số lượng tham số khổng lồ trong các mô hình SSL giọng nói đòi hỏi việc nén thành mô hình compact hơn để sử dụng rộng rãi hơn trong học thuật hoặc các công ty nhỏ. Trong nghiên cứu này, chúng tôi đề xuất tái sử dụng bản đồ chú ý qua các lớp Transformer, nhằm loại bỏ các tham số key và query trong khi vẫn giữ nguyên số lượng lớp. Hơn nữa, chúng tôi đề xuất một chiến lược chưng cất che giấu mới để cải thiện chất lượng biểu diễn giọng nói của mô hình học sinh. Chúng tôi mở rộng hàm mất mát chưng cất để sử dụng cả khung giọng nói bị che giấu và không bị che giấu nhằm tận dụng đầy đủ biểu diễn chất lượng cao của mô hình giáo viên. Chiến lược nén toàn cầu của chúng tôi tạo ra mô hình học sinh đạt tỷ lệ lỗi âm vị (PER) 7.72% và tỷ lệ lỗi từ (WER) 9.96% trên benchmark SUPERB.
Từ khóa chỉ mục: học tự giám sát giọng nói, nén mô hình, tái sử dụng bản đồ chú ý, chưng cất che giấu

1. Giới thiệu
Các mô hình SSL giọng nói dựa trên Transformer [1, 2, 3] đã được nghiên cứu tích cực trong lĩnh vực xử lý giọng nói [4] khi SSL nổi lên như một phương pháp học biểu diễn thành công trong những năm gần đây [5, 6, 7, 8]. Đặc biệt đối với wav2vec 2.0 [9], HuBERT [10], và wavLM [11], tất cả đều được kế thừa từ BERT [12], cho thấy hiệu suất đáng ngạc nhiên trong nhận dạng giọng nói tự động (ASR), có thể so sánh với các phương pháp học có giám sát [13, 14]. Vì tính linh hoạt của SSL giọng nói cũng trở nên quan trọng, các mô hình trên đã được khám phá thêm trong nhiều ứng dụng khác nhau bao gồm xác minh người nói tự động (ASV) [15] hoặc nhận dạng cảm xúc (ER) [16].

Tuy nhiên, các mô hình này có số lượng tham số khổng lồ và được huấn luyện trong thời gian rất dài, điều này gây khó khăn cho các nhóm hạn chế tài nguyên trong việc huấn luyện mô hình riêng của họ. Ví dụ, wav2vec 2.0 LARGE với 317M tham số cần được huấn luyện trước trong hơn 290 ngày trên một GPU V100 đơn lẻ [9] trên tập dữ liệu LibriSpeech [17]. Điều này đòi hỏi chúng ta phải xây dựng một mô hình nén cho phép huấn luyện hiệu quả tham số hơn nhiều và chi phí tính toán thấp hơn.

Chưng cất tri thức (KD) [18] là một kỹ thuật nén mô hình phổ biến trong đó một mô hình học sinh nhỏ hơn được huấn luyện bằng cách chưng cất tri thức từ mô hình giáo viên. Các nỗ lực trước đây trong việc chưng cất các mô hình SSL giọng nói quy mô lớn đã được thực hiện với việc giảm số lượng lớp Transformer hoặc thu hẹp chiều rộng của chúng. DistilHuBERT [19] được chưng cất theo cách dự đoán đầu ra đa lớp của HuBERT, với hầu hết các lớp Transformer bị loại bỏ. FitHuBERT [20], thay vì loại bỏ các lớp, đề xuất cắt giảm chiều rộng của chú ý và mạng nơ-ron truyền thẳng (FFN) trong mỗi lớp Transformer. LightHuBERT [21] tạo ra một supernet có thể cắt tỉa thông qua chưng cất và thực hiện tìm kiếm kiến trúc để tạo ra một học sinh nhỏ.

Mặc dù hiệu quả của các phương pháp trước đây trong việc giảm thiểu sự sụt giảm hiệu suất do nén, chúng vẫn đối mặt với một số vấn đề. (1) Các học sinh rộng và nông [19, 22] vẫn thể hiện sự suy thoái trên các tác vụ hạ nguồn liên quan đến nội dung. (2) Chưng cất lớp-đến-lớp (L2L) được chứng minh là hiệu quả [20, 22], tuy nhiên, nó phản trực giác về mặt nén vì cần tất cả tham số của mọi lớp. (3) Cắt tỉa bằng tìm kiếm kiến trúc [21] chuẩn bị một supernet có kích thước giáo viên bổ sung sử dụng 32 GPU, điều này không phải end-to-end (E2E) và không thể dễ dàng được huấn luyện bởi các nhóm hạn chế tài nguyên.

Chúng tôi đề xuất tái sử dụng bản đồ chú ý qua các lớp Transformer của học sinh, được lấy cảm hứng từ các nghiên cứu trước đây [23, 24] đã khẳng định sự tương đồng giữa các bản đồ chú ý. Tái sử dụng bản đồ chú ý cho phép chúng ta loại bỏ các tham số key và query trong một số lớp Transformer nhất định, làm cho việc giữ lại tất cả tham số lớp cho chưng cất L2L trở nên không cần thiết. Hơn nữa, chúng ta có thể tái đầu tư các tham số đã tiết kiệm vào các phần khác của Transformer.

Chúng tôi cũng đề xuất che giấu với chưng cất L2L để có chất lượng biểu diễn giọng nói tốt hơn của mô hình học sinh. Che giấu khung giọng nói là một kỹ thuật được sử dụng rộng rãi trong các mô hình SSL giọng nói [9, 10], được huấn luyện bằng cách dự đoán biểu diễn bị che giấu. Kỹ thuật này đã được áp dụng đơn giản để chưng cất HuBERT [21], nhưng không theo cách L2L. Sơ đồ chưng cất che giấu mới của chúng tôi nhằm tận dụng đầy đủ biểu diễn của giáo viên bằng cách mở rộng hàm mất mát chưng cất cho cả khung giọng nói bị che giấu và không bị che giấu. Chúng tôi nhấn mạnh rằng sơ đồ của chúng tôi là theo phong cách E2E và nâng cao chất lượng tổng quát của biểu diễn giọng nói, đặc biệt trong các tác vụ liên quan đến nội dung và ngữ nghĩa.

Kết hợp hai phương pháp được mô tả của chúng tôi (Hình 1), chúng tôi tái đầu tư các tham số đã tiết kiệm từ tái sử dụng bản đồ chú ý vào FFN, và tạo ra mô hình hàng đầu của chúng tôi, ARMHuBERT (Attention map Reused Mask HuBERT). Như được đánh giá trên benchmark SUPERB [25], ARMHuBERT đạt điểm tổng thể [11] là 78.1, chưng cất E2E tốt nhất hiện tại. Nó cũng đạt 7.72% PER trong nhận dạng âm vị (PR), và 9.96% WER trong ASR.

2. Kiến thức nền tảng
2.1. Mô hình SSL Giọng nói dựa trên Transformer
Các mô hình SSL chiếm ưu thế gần đây trong lĩnh vực giọng nói là wav2vec 2.0 [9], HuBERT [10], và wavLM [11], trong đó ba cấu trúc mô hình này giống hệt nhau ngoại trừ ở mức độ chi tiết. Cụ thể, chúng chia sẻ 12 hoặc 24 lớp Transformer [26] và CNN 1D 7 lớp. Các sơ đồ huấn luyện trước của chúng dựa trên dự đoán bị che giấu, ước tính các từ mã bằng biểu diễn đầu ra của các khung bị che giấu. Mặc dù có tính ưu việt và khả năng mở rộng arXiv:2305.11685v2 [eess.AS] 26 Oct 2023

--- TRANG 2 ---
Lớp 12 KD
CHE GIẤU
CHE GIẤU
Lớp 1 KD
CHE GIẤUGiáo viênHọc sinh
(𝑄!,𝐾!)→𝑨𝟏𝑉!𝑨𝟏→𝑨𝟐𝑉$Lớp Transformer 12
Lớp Transformer 1Lớp Transformer 2⋮Chiếu tuyến tính 1Chiếu tuyến tính 2⋮⋮
Bản đồ chú ý được tái sử dụng⋮
CHE GIẤUmất mát bị che giấu ℒ!,ℓ$%mất mát không bị che giấu ℒ&,ℓ$%✕✕
✕Hình 1: Chiến lược nén của chúng tôi bao gồm tái sử dụng bản đồ chú ý của lớp trước đó và mở rộng quá trình chưng cất cho các biểu diễn bị che giấu (mũi tên đỏ) và không bị che giấu (mũi tên xanh). Các khung đầu vào bị che giấu giống hệt nhau cho cả giáo viên và học sinh.

của các mô hình SSL giọng nói, số lượng tham số lớn và chi phí tính toán của chúng khiến việc huấn luyện các mô hình này trở nên khó khăn. Do đó chúng tôi thực hiện nén mô hình trên HuBERT và wavLM, hai mô hình SSL chiếm ưu thế trong giọng nói, để chứng minh hiệu quả của chiến lược nén của chúng tôi.

2.2. Benchmark SUPERB
Sự khởi đầu của các mô hình SSL giọng nói tập trung vào các tác vụ hạ nguồn liên quan đến nội dung như ASR hoặc PR [27, 28], tuy nhiên, tính linh hoạt của chúng đối với các tác vụ khác đã được công nhận là quan trọng gần đây [11]. Trong bối cảnh này, benchmark SUPERB [25] đã được đề xuất để đánh giá khả năng tổng quát hóa của các mô hình SSL giọng nói, bao phủ các khía cạnh về nội dung, người nói, ngữ nghĩa, và cận ngôn ngữ học. Chúng tôi đánh giá biểu diễn của chúng tôi so với benchmark SUPERB để xác minh khả năng tổng quát hóa của mô hình học sinh.

Các tác vụ hạ nguồn SUPERB bao gồm PR, ASR, nhận dạng từ khóa (KS), phát hiện thuật ngữ được nói theo truy vấn ví dụ (QbE), nhận dạng người nói (SID), ASV, phân tách người nói (SD), phân loại ý định (IC), điền khe (SF), và ER.

3. Phương pháp luận
3.1. Tái sử dụng Bản đồ Chú ý
Tái sử dụng bản đồ chú ý là một kỹ thuật để thay thế bản đồ chú ý của lớp hiện tại bằng bản đồ của lớp trước đó, đã được đề cập trong một số lĩnh vực [23, 29]. Các nghiên cứu trước đây [23, 24] đã chỉ ra sự tương đồng của các bản đồ chú ý qua các đầu và lớp trong các mô hình Transformer được huấn luyện trước, như BERT [12] và ViT [30]. Chúng tôi tận dụng thuộc tính này bằng cách tái sử dụng các bản đồ chú ý để nén mô hình học sinh. Thay vào đó, chúng ta có thể phân bổ lại lượng tham số được tiết kiệm từ tái sử dụng bản đồ chú ý, mà không tăng tổng số tham số.

Trong mô-đun tự chú ý đa đầu (MHSA) của Transformer [26], đầu vào x∈Rn×d với độ dài chuỗi n được biến đổi thành H truy vấn, khóa và giá trị độc lập bởi các ma trận biến đổi Wh,k, Wh,q∈Rd×dk, và Wh,v∈Rd×dv, tương ứng, cho mỗi đầu h. Ở đây, dk, dv, và d là chiều rộng của khóa, giá trị, và mô hình, tương ứng.

Kh=Wh,kx, Kh∈Rn×dk,
Qh=Wh,qx, Qh∈Rn×dk,
Vh=Wh,vx, Vh∈Rn×dv(1)

Sau đó, khóa và truy vấn được nhân theo trục chiều rộng để thu được bản đồ chú ý tích vô hướng có tỷ lệ, Ah∈Rn×n. Các tổ hợp tuyến tính của bản đồ chú ý và giá trị cho mỗi đầu được nối với nhau, sau đó được chiếu về chiều rộng gốc.

Ah=softmax(QhK⊤h/√dk), (2)
MHSA(x) = [A1V1, ..., AHVH]Wo, Wo∈RHdv×d(3)

Tái sử dụng bản đồ chú ý là thay thế Ah bằng bản đồ của lớp trước đó. Ví dụ, nếu chúng ta tái sử dụng bản đồ chú ý thứ k trước đó trên lớp hiện tại ℓ, mô-đun ReuseMHSA là

ReuseMHSA(x) = [Aℓ-k1Vℓ1, ..., Aℓ-kHVℓH]Wℓo. (4)

Theo đó, việc tính toán Kh và Qh có thể được bỏ qua, giảm số lượng phép nhân và phép cộng bởi (2nd2 + n2d). Giả sử d/H = dv = dk, tính toán bị bỏ qua chiếm một nửa tính toán gốc cho MHSA, là (4nd2 + 2n2d). Kết quả là, cần ít tham số và phép nhân-tích lũy (MAC) hơn khi sử dụng nhiều mô-đun ReuseMHSA hơn (xem Mục 5.1).

3.2. Chưng cất Che giấu
Tái sử dụng bản đồ chú ý đã giảm số lượng tham số, tuy nhiên, nó có thể ảnh hưởng đến chất lượng biểu diễn của mô hình học sinh. Để cải thiện việc học biểu diễn của học sinh, chúng tôi đưa ra một sơ đồ chưng cất che giấu mới tận dụng tri thức biểu diễn của giáo viên theo cách tinh vi hơn.

Che giấu khung giọng nói bao gồm việc học biểu diễn thông qua dự đoán bị che giấu, trong đó mô hình học cách biểu diễn các khung bị che giấu một cách chính xác dựa trên các khung không bị che giấu khác. LightHuBERT [21], được lấy cảm hứng từ data2vec [8], đã đầu tiên áp dụng chiến lược che giấu để chưng cất HuBERT. Trong phương pháp này, mô hình giáo viên hướng dẫn biểu diễn của các khung bị che giấu.

Gọi μ(x) là đầu vào bị che giấu, và ft và fs là giáo viên và

--- TRANG 3 ---
Bảng 1: Kết quả đánh giá trên benchmark SUPERB. Các chỉ số bao gồm kích thước tham số tính bằng triệu, PER%, WER% (không có mô hình ngôn ngữ), độ chính xác (Acc%), giá trị trọng số thuật ngữ tối đa (MTWV), tỷ lệ lỗi bằng nhau (EER%), tỷ lệ lỗi phân tách (DER%), điểm F1 (F1%), và tỷ lệ lỗi khái niệm (CER%). "Overall" biểu thị điểm trung bình của tất cả các tác vụ được đề xuất trong [11]. LightHuBERT [21] hoạt động bằng huấn luyện hai giai đoạn, trong đó supernet có kích thước HuBERT cần được huấn luyện trước, do đó không được so sánh với các mô hình chưng cất E2E. ARMHuBERT-S và ARMwavLM-S với chưng cất 960h được huấn luyện trong 100 epoch.

[Bảng dữ liệu hiệu suất chi tiết - giữ nguyên cấu trúc bảng gốc]

mô hình học sinh. Sau đó, hàm mất mát bị che giấu trở thành

L(x) = 1/|M| ∑(i∈M) ||ft_i(x) - fs_i(μ(x))||²(5)

trong đó fi là khung thứ i của biểu diễn giọng nói, và M là tập hợp các khung bị che giấu.

Ngoài mất mát phần bị che giấu (công thức 5), chúng tôi đề xuất sử dụng mất mát không bị che giấu vì mô hình giáo viên có thể cung cấp biểu diễn chất lượng cao ngay cả trên các khung không bị che giấu. Tuy nhiên, nếu quá trình che giấu loại bỏ các khung thiết yếu, việc chưng cất dạng nguyên vẹn của ft(x) có thể rò rỉ tri thức thiết yếu đó mà lẽ ra đã bị loại bỏ. Điều này tạo ra dự đoán thiên vị của học sinh, vì nó học thông tin không thể suy luận từ đầu vào bị che giấu.

Để ngăn chặn điều này, chúng tôi làm cho mô hình giáo viên nhận cùng đầu vào bị che giấu như học sinh khi chưng cất phần không bị che giấu. Do đó, toàn bộ hàm mất mát chưng cất trở thành

L(x) = ∑ℓ αℓ [Lm,ℓ(x) + Lu,ℓ(x)]
= ∑ℓ αℓ [1/|M| ∑(i∈M) ||ft_i,ℓ(x) - fs_i,ℓ(μ(x))||²](6)
+ ∑ℓ αℓ [1/(n-|M|) ∑(i∉M) ||ft_i,ℓ(μ(x)) - fs_i,ℓ(μ(x))||²]

trong đó αℓ là hệ số theo lớp. Lm,ℓ và Lu,ℓ biểu thị mất mát bị che giấu và mất mát không bị che giấu của lớp thứ ℓ, tương ứng.

Tóm lại, chiến lược chưng cất che giấu mới của chúng tôi hướng dẫn việc thu thập tri thức của học sinh một cách thích hợp, bằng cách chưng cất không chỉ biểu diễn bị che giấu của dữ liệu không bị che giấu mà còn biểu diễn không bị che giấu của dữ liệu bị che giấu (xem Hình 1). Trong Mục 5.2, chúng tôi điều tra sức mạnh của chiến lược che giấu so với các loại mất mát khác.

4. Kết quả
4.1. Chi tiết Triển khai
Chúng tôi chưng cất hai mô hình SSL giọng nói dựa trên Transformer chiếm ưu thế, HuBERT BASE [10] và wavLM BASE [11], được huấn luyện trước trên tập dữ liệu LibriSpeech 960 giờ [17]. Mô hình học sinh của chúng tôi bao gồm 12 lớp Transformer như các giáo viên, trong khi thiết kế chi tiết chủ yếu theo FitHuBERT [20]: chiều rộng của chú ý và FFN được giảm và chiếu tuyến tính được áp dụng tại mỗi lớp. Các hệ số theo lớp αℓ được đặt là 0.1 ngoại trừ lớp cuối cùng, nơi nó được đặt là 1. Trừ khi được chỉ định, tập dữ liệu LibriSpeech [17] được chưng cất trong 200 epoch với kích thước batch hiệu quả là 72 bao gồm tích lũy gradient.

Mẫu tái sử dụng Chúng tôi sử dụng mẫu tái sử dụng xen kẽ cho các bản đồ chú ý, theo đó bản đồ chú ý của lớp Transformer có số chẵn được lặp lại bởi lớp có số lẻ trước đó. Chúng tôi ký hiệu mẫu này là 2by6, thiết lập mặc định của chúng tôi. Chúng tôi kiểm tra các mẫu tái sử dụng khác trong Mục 5.1 về hiệu suất, số lượng tham số, và MAC.

Mô tả mô hình Để xác minh chiến lược chưng cất che giấu của chúng tôi, trước tiên chúng tôi xây dựng mô hình học sinh, MaskHuBERT, chỉ sử dụng chưng cất che giấu. MaskHuBERT có chiều rộng (chú ý, FFN) là (480, 640). Sau đó, mẫu tái sử dụng 2by6 được áp dụng cho MaskHuBERT, dẫn đến giảm 10.3% tham số. Chúng tôi mở rộng mô hình này thành hai tùy chọn: ARMHuBERT và ARMHuBERT-S. ARMHuBERT là phiên bản tái đầu tư của MaskHuBERT, trong đó các tham số được tiết kiệm từ tái sử dụng bản đồ chú ý được phân bổ lại cho FFN, dẫn đến tăng chiều rộng (480, 864). ARMHuBERT-S là phiên bản giảm để khớp tham số với các nghiên cứu trước đây, có chiều rộng (432, 816). Để thiết lập tính toàn cầu của chiến lược, chúng tôi giới thiệu ARMwavLM-S có cấu trúc giống hệt ARMHuBERT-S, với sự thay đổi duy nhất trong giáo viên từ HuBERT sang wavLM.

--- TRANG 4 ---
Bảng 2: So sánh hiệu suất của các mẫu tái sử dụng khác nhau. Kích thước tham số (M) và MAC (G) được đo thêm. Chiều rộng (chú ý, FFN) cho mỗi mô hình là (432, 816), trong khi hậu tố "-up" biểu thị nhiều tham số hơn được gán cho FFN để khớp với 2by6. Che giấu không được áp dụng ở đây.

[Bảng dữ liệu so sánh các mẫu - giữ nguyên cấu trúc]

4.2. Kết quả Benchmark SUPERB
Trong Bảng 1, chúng tôi đánh giá các mô hình học sinh trên benchmark SUPERB [25]. Chúng tôi tuân theo các công thức tinh chỉnh mặc định, bao gồm bộ lập lịch tỷ lệ học, với tỷ lệ học được chia tỷ lệ 10× trong tác vụ SID. MaskHuBERT vượt trội hơn 12-L HALF-L2L, phương pháp chưng cất E2E tốt nhất trước đây, với ít tham số được sử dụng hơn. Quan sát của chúng tôi cho thấy việc kết hợp chiến lược che giấu vào chưng cất L2L [20, 22] dẫn đến nâng cao chất lượng biểu diễn của học sinh. Đặc biệt, MaskHuBERT cải thiện đáng kể hiệu suất trong các tác vụ liên quan đến nội dung và ngữ nghĩa.

ARMHuBERT đạt điểm tổng thể tốt hơn là 78.1 với ít tham số hơn MaskHuBERT. Mặc dù loại bỏ một số tham số chú ý nhất định, việc tăng chiều rộng FFN góp phần vào chất lượng biểu diễn giọng nói tốt hơn, đạt 7.72% PER và 9.96% WER. Chúng tôi phát hiện rằng ARMHuBERT cho thấy cải thiện đầy hứa hẹn khi so sánh với MaskHuBERT trong các tác vụ SF và SID, thể hiện mức hiệu suất tương tự trong các tác vụ khác. Cuối cùng, số lượng tham số và MAC trong ARMHuBERT đã giảm xuống 28% và 30% của mô hình giáo viên, HuBERT BASE [10], tương ứng.

Trong nhóm tham số nhỏ hơn, ARMHuBERT-S, phiên bản giảm tham số, vượt trội hơn DistilHuBERT và FitHuBERT với biên độ lớn. Cụ thể, ARMHuBERT-S cũng cho thấy kết quả xuất sắc trong các tác vụ liên quan đến nội dung và ngữ nghĩa, có nghĩa là tính nhất quán của các biểu diễn được tạo ra bởi MaskHuBERT và ARMHuBERT-S. Ngoài ra, kết quả ARMwavLM-S vượt trội hơn ARMHuBERT-S ngụ ý tính toàn cầu của chiến lược: không có bất kỳ sửa đổi nào về cấu trúc mô hình học sinh, việc thay thế bằng mô hình giáo viên vượt trội tạo ra học sinh tốt hơn. Kết quả chưng cất LibriSpeech [17] 100h cũng nhất quán với kết quả được chứng minh trước đây.

5. Thảo luận
Trong phần này, chúng tôi khám phá bản đồ chú ý của lớp nào nên được tái sử dụng trong các lớp khác và cách triển khai chưng cất che giấu. Trừ khi được chỉ định, chúng tôi đã thực hiện chưng cất trên 100 giờ LibriSpeech [17] và đánh giá trên các tác vụ ASR, ASV, và SF của benchmark SUPERB [25].

5.1. Nơi Tái sử dụng
Bảng 2 tóm tắt hiệu suất tùy thuộc vào các mẫu tái sử dụng bản đồ chú ý khác nhau, và nói chung, mẫu 2by6 hoạt động tốt nhất. Các mẫu tái sử dụng khác đã làm giảm khả năng biểu diễn của Transformer do tái sử dụng quá thường xuyên. Việc gán nhiều tham số hơn cho FFN (-up) vẫn có giới hạn về mức tăng hiệu suất. So với không áp dụng mẫu tái sử dụng nào,

Bảng 3: Nghiên cứu loại bỏ về chiến lược che giấu của chúng tôi.
[Bảng dữ liệu nghiên cứu loại bỏ - giữ nguyên cấu trúc]

Bảng 4: So sánh hiệu suất với các tỷ lệ che giấu khác nhau. "sch" biểu thị lập lịch tuyến tính của tỷ lệ từ 0.4 đến 0.8.
[Bảng dữ liệu so sánh tỷ lệ che giấu - giữ nguyên cấu trúc]

việc giảm hiệu suất của 2by6 là nhỏ, nhưng nó có lợi thế trong việc giảm 9.13% và 8.16% tham số và MAC, tương ứng. Chúng tôi lưu ý rằng số lượng MAC trong một mô-đun MHSA tái sử dụng đơn lẻ (công thức 4) được giảm một nửa, từ 13.2G xuống 6.6G.

5.2. Cách Che giấu
Chiến lược che giấu Bảng 3 cho thấy hiệu quả của chiến lược che giấu của chúng tôi. Đầu tiên chúng tôi loại bỏ hàm mất mát trên các khung không bị che giấu (Lu,ℓ), làm cho nó tương đương với phiên bản L2L của hàm mất mát chưng cất LightHuBERT [21]. Phương pháp này làm hỏng nghiêm trọng hiệu suất, đặc biệt trong các tác vụ ASR và ASV. Tiếp theo, chúng tôi sửa đổi hàm mất mát không bị che giấu để chưng cất từ đầu vào không bị che giấu, tức là chỉ ft(x) được chưng cất cho học sinh. Điều này cũng dẫn đến hiệu suất suy giảm trong hầu hết các tác vụ, cho thấy rằng mất mát không bị che giấu với đầu vào bị che giấu của chúng tôi hướng dẫn việc thu thập tri thức một cách thích hợp mà không áp đặt dự đoán thiên vị.

Tỷ lệ che giấu Giá trị cao của tỷ lệ che giấu có thể dẫn đến mô hình học sinh tạo ra biểu diễn tốt, vì nó có ít thông tin hơn để suy luận [10, 31]. Tuy nhiên, nó cũng có thể làm cho quá trình học trở nên khó khăn hơn. Trong Bảng 4, chúng tôi kiểm tra tỷ lệ che giấu tối ưu cho mỗi tập huấn luyện. Đối với LibriSpeech 960h [17], cả tỷ lệ 0.4 và 0.8 đều tạo ra kết quả xuất sắc. Mặt khác, đối với tập dữ liệu 100h, tỷ lệ 0.4 tạo ra kết quả tốt nhất tổng thể. Điều này ngụ ý rằng tỷ lệ che giấu thấp hơn được ưa thích trong thiết lập chưng cất tài nguyên thấp. Theo đó, trong các thí nghiệm chính của chúng tôi, chúng tôi đã sử dụng tỷ lệ 0.8 và 0.4 cho chưng cất 960h và 100h, tương ứng.

6. Kết luận và Nghiên cứu Tương lai
Tóm lại, chúng tôi đã đề xuất chiến lược nén toàn cầu bao gồm tái sử dụng bản đồ chú ý và chưng cất che giấu mới. Mô hình tái đầu tư tham số của chúng tôi, ARMHuBERT, đạt hiệu suất tuyệt vời trong các tác vụ liên quan đến nội dung và ngữ nghĩa. Chiến lược của chúng tôi có thể được áp dụng cho bất kỳ mô hình SSL giọng nói dựa trên Transformer nào, và góp phần nâng cao chất lượng tổng quát của biểu diễn giọng nói. Nghiên cứu tương lai có thể tập trung vào việc cải thiện thêm mô hình của chúng tôi trên các tác vụ liên quan đến người nói.

7. Lời cảm ơn
Nghiên cứu được hỗ trợ bởi Dự án R&D Công nghệ Y tế Hàn Quốc thông qua Viện Phát triển Công nghiệp Y tế Hàn Quốc được tài trợ bởi Bộ Y tế và Phúc lợi, Cộng hòa Hàn Quốc (HR18C0016).

--- TRANG 5 ---
8. Tài liệu tham khảo
[1] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, và H.-y. Lee, "Mockingjay: Học biểu diễn giọng nói không giám sát với bộ mã hóa transformer hai chiều sâu," trong IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, tr. 6419–6423.

[2] P.-H. Chi, P.-H. Chung, T.-H. Wu, C.-C. Hsieh, Y.-H. Chen, S.-W. Li, và H.-y. Lee, "Audio albert: Một bert nhẹ cho học tự giám sát biểu diễn âm thanh," trong Spoken Language Technology Workshop (SLT). IEEE, 2021, tr. 344–350.

[3] A. T. Liu, S.-W. Li, và H.-y. Lee, "Tera: Học tự giám sát biểu diễn bộ mã hóa transformer cho giọng nói," IEEE/ACM Transactions on Audio, Speech, and Language Processing, tập 29, tr. 2351–2366, 2021.

[4] S. Liu, A. Mallol-Ragolta, E. Parada-Cabaleiro, K. Qian, X. Jing, A. Kathan, B. Hu, và B. W. Schuller, "Học tự giám sát âm thanh: Một khảo sát," Patterns, tập 3, số 12, tr. 100616, 2022.

[5] T. Mikolov, K. Chen, G. Corrado, và J. Dean, "Ước lượng hiệu quả các biểu diễn từ trong không gian vector," arXiv preprint arXiv:1301.3781, 2013.

[6] M. Caron, P. Bojanowski, A. Joulin, và M. Douze, "Phân cụm sâu cho học không giám sát các đặc trưng thị giác," trong Proceedings of the European conference on computer vision (ECCV), 2018, tr. 132–149.

[7] T. Chen, S. Kornblith, M. Norouzi, và G. Hinton, "Một khung đơn giản cho học tương phản biểu diễn thị giác," trong International conference on machine learning. PMLR, 2020, tr. 1597–1607.

[8] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, và M. Auli, "Data2vec: Một khung tổng quát cho học tự giám sát trong giọng nói, thị giác và ngôn ngữ," trong International Conference on Machine Learning. PMLR, 2022, tr. 1298–1312.

[9] A. Baevski, Y. Zhou, A. Mohamed, và M. Auli, "wav2vec 2.0: Một khung cho học tự giám sát biểu diễn giọng nói," Advances in Neural Information Processing Systems, tập 33, tr. 12 449–12 460, 2020.

[10] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, và A. Mohamed, "Hubert: Học biểu diễn giọng nói tự giám sát bằng dự đoán có che giấu các đơn vị ẩn," IEEE/ACM Transactions on Audio, Speech, and Language Processing, tập 29, tr. 3451–3460, 2021.

[11] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao và cộng sự, "Wavlm: Huấn luyện trước tự giám sát quy mô lớn cho xử lý giọng nói đầy đủ," IEEE Journal of Selected Topics in Signal Processing, tập 16, số 6, tr. 1505–1518, 2022.

[12] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Huấn luyện trước các transformer hai chiều sâu cho hiểu ngôn ngữ," trong Proceedings of NAACL-HLT, 2019, tr. 4171–4186.

[13] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu và cộng sự, "Conformer: Transformer tăng cường tích chập cho nhận dạng giọng nói," trong Proc. Interspeech, 2020.

[14] S. Kim, A. Gholami, A. E. Shaw, N. Lee, K. Mangalam, J. Malik, M. W. Mahoney, và K. Keutzer, "Squeezeformer: Một transformer hiệu quả cho nhận dạng giọng nói tự động," trong Advances in Neural Information Processing Systems, 2022.

[15] Y. Wang, A. Boumadane, và A. Heba, "Một benchmark wav2vec 2.0/hubert được tinh chỉnh cho nhận dạng cảm xúc giọng nói, xác minh người nói và hiểu ngôn ngữ nói," arXiv preprint arXiv:2111.02735, 2021.

[16] L. Pepino, P. Riera, và L. Ferrer, "Nhận dạng cảm xúc từ giọng nói sử dụng nhúng wav2vec 2.0," trong Proc. Interspeech, 2021, tr. 3400–3404.

[17] V. Panayotov, G. Chen, D. Povey, và S. Khudanpur, "Librispeech: một tập dữ liệu asr dựa trên sách âm thanh miền công cộng," trong 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, tr. 5206–5210.

[18] G. Hinton, O. Vinyals, J. Dean và cộng sự, "Chưng cất tri thức trong mạng nơ-ron," arXiv preprint arXiv:1503.02531, tập 2, số 7, 2015.

[19] H.-J. Chang, S.-w. Yang, và H.-y. Lee, "Distilhubert: Học biểu diễn giọng nói bằng chưng cất theo lớp của hidden-unit bert," trong IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, tr. 7087–7091.

[20] Y. Lee, K. Jang, J. Goo, Y. Jung, và H. Kim, "Fithubert: Đi mỏng hơn và sâu hơn cho chưng cất tri thức học tự giám sát giọng nói," trong Proc. Interspeech, 2022.

[21] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, và H. Li, "Lighthubert: Học biểu diễn giọng nói nhẹ và có thể cấu hình với hidden-unit bert một-lần-cho-tất-cả," trong Proc. Interspeech, 2022.

[22] T. Ashihara, T. Moriya, K. Matsuura, và T. Tanaka, "Sâu so với rộng: Một phân tích các kiến trúc học sinh cho chưng cất tri thức bất khả tri tác vụ của các mô hình giọng nói tự giám sát," trong Proc. Interspeech, 2022.

[23] T. Xiao, Y. Li, J. Zhu, Z. Yu, và T. Liu, "Chia sẻ trọng số chú ý cho transformer nhanh," trong Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI), 2019, tr. 5292–5298.

[24] S. Bhojanapalli, A. Chakrabarti, A. Veit, M. Lukasik, H. Jain, F. Liu, Y.-W. Chang, và S. Kumar, "Tận dụng dư thừa trong chú ý với transformer tái sử dụng," arXiv preprint arXiv:2110.06821, 2021.

[25] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, và H. yi Lee, "SUPERB: Benchmark Hiệu suất Toàn cầu Xử lý Giọng nói," trong Proc. Interspeech 2021, 2021, tr. 1194–1198.

[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Chú ý là tất cả những gì bạn cần," Advances in neural information processing systems, tập 30, 2017.

[27] A. v. d. Oord, Y. Li, và O. Vinyals, "Học biểu diễn với mã hóa dự đoán tương phản," arXiv preprint arXiv:1807.03748, 2018.

[28] S. Schneider, A. Baevski, R. Collobert, và M. Auli, "wav2vec: Huấn luyện trước không giám sát cho nhận dạng giọng nói," trong Proc. Interspeech, 2019.

[29] K. Shim, J. Choi, và W. Sung, "Hiểu vai trò của tự chú ý cho nhận dạng giọng nói hiệu quả," trong International Conference on Learning Representations, 2022.

[30] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly và cộng sự, "Một hình ảnh có giá trị 16x16 từ: Transformer cho nhận dạng hình ảnh ở quy mô," trong International Conference on Learning Representations, 2021.

[31] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, và R. Girshick, "Bộ mã hóa tự động có che giấu là những người học thị giác có thể mở rộng," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, tr. 16 000–16 009.
