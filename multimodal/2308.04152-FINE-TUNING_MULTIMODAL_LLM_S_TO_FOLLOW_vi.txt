# 2308.04152.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.04152.pdf
# Kích thước tệp: 9694110 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
TINH CHỈNH CÁC MLLM ĐA PHƯƠNG THỨC ĐỂ TUÂN THEO
CÁC HƯỚNG DẪN MINH HỌA ZERO-SHOT
Juncheng Li1, 2∗Kaihang Pan1∗Zhiqi Ge1∗Minghe Gao1∗Wei Ji2Wenqiao Zhang1†
Tat-Seng Chua2Siliang Tang1†Hanwang Zhang3Yueting Zhuang1†
1Đại học Chiết Giang,2Đại học Quốc gia Singapore,3Đại học Công nghệ Nanyang

TÓM TẮT
Những tiến bộ gần đây trong các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) đã
sử dụng các Bộ tạo Gợi ý Hình ảnh (VPGs) để chuyển đổi các đặc trưng hình ảnh thành
các token mà LLMs có thể nhận diện. Điều này được thực hiện bằng cách huấn luyện các VPGs
trên hàng triệu cặp hình ảnh-chú thích, trong đó các token được tạo bởi VPG của hình ảnh
được đưa vào một LLM đông lạnh để tạo ra các chú thích tương ứng. Tuy nhiên, mục tiêu
huấn luyện dựa trên chú thích hình ảnh này vốn dĩ làm thiên lệch VPG chỉ tập trung
vào nội dung hình ảnh chính đủ để tạo chú thích, thường bỏ qua các chi tiết hình ảnh khác.
Hạn chế này dẫn đến hiệu suất kém của MLLMs trong việc hiểu các hướng dẫn minh họa
bao gồm nhiều hướng dẫn đa phương thức, xen kẽ và chồng chéo nhau để minh họa bối cảnh
cần thiết để hoàn thành một nhiệm vụ. Để giải quyết vấn đề này, chúng tôi giới thiệu một
mô-đun Hoàn thiện Bộ tạo Gợi ý Hình ảnh (VPG-C) chung và nhẹ, có thể suy luận
và hoàn thiện các chi tiết bị thiếu cần thiết để hiểu các hướng dẫn minh họa. Hơn nữa,
chúng tôi đề xuất một chiến lược huấn luyện phân biệt tổng hợp để tinh chỉnh VPG-C,
loại bỏ nhu cầu về các hướng dẫn minh họa có giám sát. Đối với đánh giá, chúng tôi xây dựng
DEMON, một bộ đánh giá toàn diện cho việc hiểu hướng dẫn minh họa. Được huấn luyện
tổng hợp với chiến lược đề xuất, VPG-C đạt được hiệu suất zero-shot mạnh mẽ hơn đáng kể
trên tất cả các nhiệm vụ của DEMON. Đánh giá thêm trên các bộ đánh giá MME và OwlEval
cũng chứng minh sự vượt trội của VPG-C. Mã nguồn và các mô hình có sẵn tại
https://github.com/DCDmllm/Cheetah.

1 GIỚI THIỆU
Những tiến bộ gần đây trong các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) (Li et al., 2023c; Liu et al., 2023;
Zhu et al., 2023a) đã thể hiện khả năng hứa hẹn trong việc xử lý các hướng dẫn hình ảnh đơn,
chẳng hạn như tạo ra các mô tả hình ảnh chi tiết và trả lời câu hỏi về hình ảnh. Tuy nhiên,
chúng còn thiếu sót trong các hướng dẫn minh họa bao gồm nhiều hướng dẫn đa phương thức
xen kẽ và chồng chéo nhau để minh họa bối cảnh cần thiết để hoàn thành một nhiệm vụ. Ví dụ, hướng dẫn
trong Hình 1 chứa bối cảnh hình ảnh và văn bản xen kẽ, yêu cầu mô hình xác định
tính xác thực của sữa trong hình ảnh thứ hai dựa trên hình ảnh chính thức được cung cấp trong hình đầu tiên.

Một MLLM ít nhất phải có hai khả năng sau để hiểu các hướng dẫn minh họa
một cách hiệu quả:
1) Không chỉ chủ đề chính: Ngoài việc tập trung vào nội dung hình ảnh chính, nó phải có thể
phân biệt tỉ mỉ các chi tiết trong các minh họa. Những chi tiết này, bổ sung cho
nội dung chính, đóng vai trò quan trọng trong việc kết nối ngữ nghĩa các hướng dẫn. Một trường hợp điển hình là
Hình 1, trong đó việc phân biệt chính xác dựa vào việc nhận diện chi tiết logo trên hộp sữa.
2) Chi tiết nhận biết suy luận: Làm thế nào để quyết định chi tiết nào bổ sung cho suy luận? Chúng
tôi mong đợi rằng một MLLM có thể "suy nghĩ hai lần", nghĩa là, đưa ra suy luận sơ bộ sử dụng
nội dung chính, nó sẽ biết những nội dung bổ sung nào cần thiết như chi tiết bổ sung. Ví dụ,
trong Hình 1, sau suy luận sơ bộ, mô hình nên tập trung lại vào các chi tiết như logo và

∗Đóng góp ngang nhau.†Tác giả liên hệ.
1arXiv:2308.04152v4 [cs.CV] 25 Tháng 5 2024

--- TRANG 2 ---
Câu trả lời của chúng tôi: Không, biểu tượng con bò trong hộp hình kim cương màu đỏ trên bao bì sữa đã mua khác biệt rõ rệt so với sữa thật. So với sữa thật được hiển thị trong                             , hãy kiểm tra thiết kế bao bì của sữa tôi đã mua được hiển thị trong                         . Sữa đã mua có thật không?
Hướng dẫn minh họa
Giai đoạn 2: bản đồ chú ý sau suy luận sơ bộ
Giai đoạn 1: bản đồ chú ý ban đầu
Câu trả lời của InstructBLIP: Có, sữa đã mua là thật.
Bản đồ chú ý được hình dung của VPG của InstructBLIP.
Hình 1: Một ví dụ về InstructBLIP (Dai et al., 2023) và MLLM của chúng tôi được tăng cường bởi VPG-C.

tên thương hiệu trên hộp sữa, từ đó phân biệt tính xác thực của nó. Tuy nhiên, để tuân theo các hướng dẫn minh họa zero-shot, khả năng "nhận biết suy luận" này phải được học mà không cần
các hướng dẫn minh họa có giám sát.

Thật không may, chúng tôi thấy rằng lý do tại sao các MLLMs hiện tại không hiệu quả trong các hướng dẫn minh họa là do thiếu các khả năng trên. Cụ thể hơn, vấn đề cốt lõi nằm ở
Bộ tạo Gợi ý Hình ảnh (VPG) trong MLLMs. VPG, chẳng hạn như Q-former (Li et al., 2023c) và Re-
sampler (Alayrac et al., 2022), dịch các đặc trưng hình ảnh thành các token có thể nhận diện bởi LLMs, và
việc dịch được huấn luyện trên hàng triệu cặp hình ảnh-chú thích bằng cách đưa các token được tạo bởi VPG
của hình ảnh vào một LLM đông lạnh để tạo ra các chú thích tương ứng. Tuy nhiên, chiến lược huấn luyện chú thích hình ảnh này chắc chắn đưa ra thiên lệch quy nạp rằng VPG chỉ tập trung vào
nội dung hình ảnh chính vừa đủ cho nhiệm vụ chú thích, nhưng có xu hướng bỏ qua các chi tiết hình ảnh khác. Ví dụ trong Hình 1, bản đồ chú ý trung bình của InstructBLIP (Dai et al., 2023) (Hình
1) cho thấy sự tập trung chủ đạo vào nội dung chính, bỏ qua chi tiết logo, tuy nhiên đây lại là
chìa khóa để trả lời câu hỏi.

Bộ mã hóa hình ảnh Bộ tạo gợi ý hình ảnh···×	𝐿2×	𝐿2⨁···

···Chi tiết hình ảnh còn lại Lớp LLM Hướng dẫn Lớp LLM

Đầu ra LLM

Hình 2: Tổng quan về VPG-C. Để giải quyết vấn đề này, chúng tôi đề xuất một mô-đun Hoàn thiện Bộ tạo Gợi ý Hình ảnh nhẹ (VPG-C), có thể suy luận và hoàn thiện các chi tiết bị thiếu cần thiết để hiểu các hướng dẫn minh họa (Phần 2.1). Như được thể hiện trong Hình 2, 1) VPG-C trước tiên rút ra hướng dẫn cụ thể cho lệnh bằng cách chặn đầu ra LLM trung gian của nội dung chính được trích xuất bởi VPG thông thường, và sau đó 2) hướng dẫn VPG khôi phục các chi tiết hình ảnh còn lại bị thiếu. Cuối cùng, 3) những chi tiết còn lại này sau đó được tích hợp lại một cách liền mạch vào lớp LLM trung gian thông qua kết nối bỏ qua. Cùng với đầu ra trung gian ban đầu, VPG-C được kỳ vọng sẽ cung cấp sự hiểu biết cải thiện về các hướng dẫn minh họa. Tuy nhiên, VPG-C chưa sẵn sàng để tuân theo các hướng dẫn minh họa zero-shot vì bước "Hướng dẫn" yêu cầu tinh chỉnh để chuyên môn hóa trong việc khôi phục chi tiết bị thiếu. Do đó, chúng tôi đề xuất một chiến lược huấn luyện phân biệt tổng hợp để tinh chỉnh VPG-C, mà không cần thu thập dữ liệu đắt đỏ của các cặp "chi tiết-chú thích" (Phần 2.2).

Để đánh giá VPG-C và chẩn đoán các MLLMs hiện tại, chúng tôi xây dựng DEMON, một bộ đánh giá toàn diện cho việc hiểu hướng dẫn minh họa, bao gồm 31 nhiệm vụ đa dạng trên 7 danh mục, như được thể hiện trong Hình 4 (Phần 3). Đánh giá hệ thống trên DEMON xác nhận hạn chế của các MLLMs hiện tại trong các hướng dẫn minh họa. Không có dữ liệu hướng dẫn minh họa bổ sung, mô-đun VPG-C nhẹ có thể được tinh chỉnh hiệu quả bởi chiến lược huấn luyện tổng hợp trong vài giờ với một GPU A100 duy nhất. Mặc dù hiệu quả về tính toán và dữ liệu, VPG-C vượt trội đáng kể so với các MLLMs hiện tại trên bộ đánh giá DEMON. Đánh giá zero-shot trên các bộ đánh giá hướng dẫn đa phương thức khác (Fu et al., 2023; Ye et al., 2023) cũng cho thấy cải thiện đáng kể bởi VPG-C.

2 PHƯƠNG PHÁP
2.1 HOÀN THIỆN BỘ TẠO GỢI Ý HÌNH ẢNH

Như được minh họa trong Hình 2, VPG-C được xây dựng dựa trên LLM đông lạnh (Vicuna-7B (Chiang et al., 2023))
và bộ mã hóa thị giác (EV A-CLIP (Fang et al., 2023)). Chúng tôi áp dụng Q-Former được sử dụng rộng rãi từ BLIP-
2

--- TRANG 3 ---
2 (Li et al., 2023c) làm bộ tạo gợi ý hình ảnh của chúng tôi. VPG-C trước tiên sử dụng đầu ra trung gian của
LLM để suy luận hướng dẫn cụ thể cho lệnh. Điều này sau đó hỗ trợ VPG trong việc chú ý đến các chi tiết hình ảnh bị thiếu từ hình ảnh. Bằng cách hợp nhất những chi tiết còn lại này trở lại thông qua kết nối bỏ qua, VPG-C
đạt được sự nắm bắt toàn diện về hướng dẫn minh họa.

Với một hướng dẫn minh họa, trước tiên chúng tôi áp dụng Q-former để tạo gợi ý hình ảnh chung
cho mỗi hình ảnh trong hướng dẫn. Q-former lấy một số lượng cố định K vector truy vấn để tương tác với các đặc trưng hình ảnh qua một số lớp chú ý chéo, và các biểu diễn truy vấn đầu ra được sử dụng làm gợi ý hình ảnh, được chèn vào vị trí của hình ảnh tương ứng trong hướng dẫn. Chúng tôi ký hiệu hướng dẫn đầu vào cho bộ giải mã ngôn ngữ là H0=
{h0₁,h0₂, ...,v0₁₁, ...,v0₁K, ...,h0ᵢ, ...,v0ⱼ₁, ...,v0ⱼK, ...,h0N}, trong đó h0ᵢ đại diện cho token văn bản thứ i
và V0ⱼ={v0ⱼ₁, ...,v0ⱼK} đại diện cho K gợi ý hình ảnh cho hình ảnh xen kẽ thứ j. Lấy
hướng dẫn làm đầu vào cho bộ giải mã ngôn ngữ L-lớp, sau đó chúng tôi trích xuất biểu diễn ẩn của
token đầu vào cuối cùng hL/2N tại lớp thứ L/2, có thể nhận thức đầy đủ toàn bộ bối cảnh đa phương thức
trong L/2 lớp đầu tiên và chứa ngữ nghĩa nhận biết hướng dẫn toàn diện. Tiếp theo, chúng tôi suy luận
hướng dẫn cụ thể cho lệnh g từ hL/2N thông qua một lớp chiếu tuyến tính: g=Linear (hL/2N).

Sau khi có được hướng dẫn cụ thể cho lệnh từ bộ giải mã ngôn ngữ, chúng tôi kết hợp nó với một
tập hợp các truy vấn có thể học mới: g+Q, trong đó Q ∈RK×d và g được thêm vào mỗi truy vấn của Q. Sau đó,
chúng tôi tái sử dụng cùng Q-former với các truy vấn được tạo có điều kiện ở trên để chú ý đến chi tiết hình ảnh còn lại, do đó thu được các gợi ý hình ảnh Vⱼ={vⱼ₁, ...,vⱼK} cho mỗi hình ảnh j, chứa
các chi tiết bổ sung bị bỏ lỡ bởi các gợi ý hình ảnh ban đầu. Cuối cùng, Vⱼ được chuyển đổi
được tích hợp lại với các biểu diễn trung gian ban đầu tương ứng của VL/2ⱼ, thông qua
kết nối bỏ qua: ṼL/2ⱼ=VL/2ⱼ+Linear (Vⱼ), được lấy làm đầu vào cho lớp thứ (L/2+ 1).

Huấn luyện hiệu quả. Mô-đun VPG-C của chúng tôi có hiệu quả tham số vì Q-former được đông lạnh và chỉ một
tập hợp các embedding truy vấn và hai lớp chiếu tuyến tính cần được tinh chỉnh, chỉ chiếm
0.09% (∼6.3M) của toàn bộ mô hình. Để ổn định quá trình huấn luyện (Zhang & Agrawala, 2023),
chúng tôi khởi tạo các lớp chiếu tuyến tính với số không. Do đó, ở giai đoạn huấn luyện sớm, đầu vào cho
lớp thứ (L/2+ 1) có thể được chuyển đổi thành: ṼL/2ⱼ=VL/2ⱼ, điều này sẽ không gây ảnh hưởng gì đến LLMs.

Phân tích về việc chèn VPG-C vào lớp trung gian (L/2): 1) Tạo hướng dẫn. Các nghiên cứu trước đây đã chỉ ra rằng các đặc trưng được cung cấp bởi lớp trung gian có thể đủ để hiểu sơ bộ các mẫu đầu vào đã cho (Xin et al., 2020) và có thể phục vụ như gợi ý hướng dẫn để cải thiện huấn luyện (Romero et al., 2014). Do đó, việc tạo hướng dẫn trong lớp trung gian cho phép mô hình hình thành sự hiểu biết sơ bộ về hướng dẫn đã cho. Tạo hướng dẫn quá sớm có thể có vấn đề, vì mô hình có thể chưa thu thập đủ thông tin bối cảnh để tạo ra hướng dẫn hiệu quả. Ngược lại, tạo hướng dẫn quá muộn có thể dẫn đến việc chú ý của mô hình bị thu hẹp vào những gì nó coi là câu trả lời cuối cùng, cản trở khả năng hướng dẫn Q-former trích xuất các chi tiết liên quan từ hình ảnh. Do đó, việc đặt bước tạo hướng dẫn trong lớp trung gian tạo ra sự cân bằng. 2) Tích hợp lại chi tiết. Việc tích hợp lại chi tiết hình ảnh còn lại ở lớp trung gian bảo tồn kiến thức trước và cho phép các lớp tiếp theo tích hợp thông tin mới một cách hiệu quả. Tích hợp lại chi tiết còn lại quá sớm trong pipeline có thể ghi đè bối cảnh quan trọng, trong khi tích hợp lại quá muộn có thể hạn chế tác động đến suy luận của mô hình. Do đó, lớp trung gian cung cấp vị trí chiến lược cho việc tích hợp lại chi tiết còn lại, cho phép mô hình suy luận hiệu quả và đến được câu trả lời đúng bằng cách tận dụng các chi tiết hình ảnh còn lại được bổ sung. Chúng tôi cung cấp thêm phân tích định lượng trong Phần 4.4.

2.2 CHIẾN LƯỢC HUẤN LUYỆN PHÂN BIỆT TỔNG HỢP

Chiến lược huấn luyện được đề xuất chẩn đoán các khu vực ban đầu bị Q-former bỏ qua theo các bản đồ chú ý chéo của nó giữa các truy vấn và các đặc trưng hình ảnh, và tạo ra một hình ảnh tổng hợp bằng cách thực hiện một số loại chỉnh sửa trên các khu vực bị bỏ qua. Sau đó, một nhiệm vụ phân biệt giữa hình ảnh được hình thành như mô tả sự khác biệt tinh tế giữa hình ảnh gốc và hình ảnh tổng hợp.
Xem xét các chỉnh sửa được thực hiện trong các khu vực chủ yếu bị bỏ qua, VPG-C được thử thách để khôi phục
các chi tiết bị thiếu để mô tả sự khác biệt. Một tổng quan được minh họa trong Hình 3.

Xác định mục tiêu chỉnh sửa. Q-former lấy các truy vấn để tương tác với các đặc trưng hình ảnh đông lạnh thông qua một số lớp chú ý chéo và sử dụng các biểu diễn truy vấn đầu ra làm
3

--- TRANG 4 ---
Q-Former Bản đồ chú ý Hình ảnh đầu vào Bản đồ phân đoạn Mục tiêu chỉnh sửa các khu vực bị bỏ qua

" đỏ…"

" đá…"" ngựa…"Tạo mô tả chỉnh sửa phù hợp

Phân đoạn mọi thứ Blended Diffusion

Xóa đối tượng

Thêm đối tượng Hoán đổi đối tượng" loại bỏ…"

Chỉnh sửa hình ảnh tổng hợp được hướng dẫn bởi chú ý chéo Sửa đổi đối tượng

Huấn luyện phân biệt giữa hình ảnh Mô tả sự khác biệt giữa các hình ảnh. Hướng dẫn:

12 Hình ảnh gốc Hình ảnh đã chỉnh sửa Phản hồi: Trong hình đầu tiên, chiếc xe trên đường có màu vàng, trong khi ở hình thứ hai, nó có màu đỏ. MLLM Hình 3: Minh họa pipeline của chiến lược huấn luyện phân biệt tổng hợp cho VPG-C.

gợi ý hình ảnh. Do đó, các bản đồ chú ý chéo giữa các truy vấn và các đặc trưng hình ảnh phản ánh sự quan tâm
của các truy vấn. Chúng tôi tính trung bình các bản đồ chú ý chéo trên tất cả các lớp và tất cả các truy vấn để thu được bản đồ chú ý chéo toàn cục A, trong đó giá trị Aij chỉ ra mức độ quan trọng của đặc trưng hình ảnh tương ứng bởi các truy vấn Q-former ban đầu không phụ thuộc nhiệm vụ. Sau đó, chúng tôi sử dụng mô hình nền tảng thị giác tiên tiến (Kirillov et al., 2023) để thu được tất cả các đối tượng với mặt nạ phân đoạn
trong hình ảnh. Sau đó, mức độ quan trọng của mỗi đối tượng Φ(oi) được tính toán dựa trên bản đồ chú ý chéo A với RoIAlign (He et al., 2017), trong đó chúng tôi tính trung bình các giá trị của A trong mặt nạ
mi bằng nội suy. Φ(oi) phản ánh mức độ các đặc trưng hình ảnh của đối tượng oi được trích xuất bởi
Q-former. Do đó, chúng tôi chọn các đối tượng bị bỏ qua nhiều nhất dựa trên giá trị Φ(oi).

Tạo mô tả chỉnh sửa. Chúng tôi định nghĩa bốn loại chỉnh sửa: sửa đổi đối tượng, hoán đổi
đối tượng, xóa đối tượng và thêm đối tượng. Với đối tượng được chọn, chúng tôi hướng dẫn ChatGPT (OpenAI, 2023a) tạo ra mô tả chỉnh sửa phù hợp hài hòa với bối cảnh, trong đó
ChatGPT được gợi ý với chú thích hình ảnh tương ứng và thông tin đối tượng chi tiết (tức là,
nhãn, vị trí). Đối với việc thêm đối tượng, chúng tôi chỉ chọn các đối tượng NỀN để thêm đối tượng.

Tạo hình ảnh tổng hợp. Sau khi có được mô tả chỉnh sửa, chúng tôi tạo ra hình ảnh tổng hợp
sử dụng một mô hình khuếch tán ẩn văn bản-thành-hình ảnh (tức là, Blended Diffusion (Avrahami et al., 2022)).
Blended Diffusion thực hiện chỉnh sửa cục bộ trên hình ảnh theo mặt nạ đối tượng mục tiêu và
mô tả chỉnh sửa, do đó tạo ra hình ảnh tổng hợp. Để đảm bảo chất lượng, chúng tôi lọc các hình ảnh đã chỉnh sửa
sử dụng độ tương tự CLIP (Radford et al., 2021b).

Huấn luyện phân biệt giữa hình ảnh. Với cặp hình ảnh gốc và hình ảnh tổng hợp, cùng với
hướng dẫn nhiệm vụ ("Mô tả sự khác biệt giữa các hình ảnh"), nhiệm vụ huấn luyện phân biệt giữa hình ảnh được định nghĩa là tạo ra các câu để mô tả sự khác biệt tinh tế giữa
các hình ảnh. Chúng tôi chuyển đổi mô tả chỉnh sửa để có được các câu đúng.

3 BỘ ĐÁNH GIÁ DEMON

Định dạng dữ liệu. Tất cả các thể hiện nhiệm vụ được chuyển đổi thành dạng hướng dẫn-phản hồi thống nhất cho đánh giá zero-shot. Chính thức, mỗi thể hiện trong DEMON bao gồm các thành phần sau:
•Hướng dẫn_nhiệm vụ: cung cấp định nghĩa ngôn ngữ tự nhiên hoàn chỉnh của một nhiệm vụ đã cho,
bao gồm định dạng đầu vào/đầu ra và mục tiêu nhiệm vụ.
•Thể hiện_nhiệm vụ: là một thể hiện cụ thể của một nhiệm vụ đã cho bao gồm bối cảnh tuần tự hình ảnh-văn bản minh họa (ví dụ, sách giáo khoa phong phú hình ảnh, câu hỏi cụ thể về bối cảnh).
•Phản hồi: đại diện cho đầu ra mục tiêu bằng ngôn ngữ tự nhiên cho một hướng dẫn nhiệm vụ và thể hiện nhiệm vụ đã cho. Đối với các nhiệm vụ phân loại, chúng tôi chuyển đổi các nhãn lớp thành tùy chọn trong hướng dẫn và
yêu cầu mô hình xuất ra chỉ số tùy chọn bằng ngôn ngữ tự nhiên làm phản hồi.

Không có nhấn mạnh cụ thể nào, chúng tôi sử dụng thuật ngữ "hướng dẫn" để chỉ sự kết hợp
của Hướng dẫn_nhiệm vụ và Thể hiện_nhiệm vụ. Đối với mỗi nhiệm vụ, chúng tôi thiết kế thủ công 10
mẫu Hướng dẫn_nhiệm vụ bằng ngôn ngữ tự nhiên để tăng tính đa dạng.

Thu thập và phân loại nhiệm vụ. Để đánh giá toàn diện khả năng tuân theo hướng dẫn minh họa, chúng tôi thu thập rộng rãi nhiều bộ dữ liệu đa phương thức đa dạng từ các lĩnh vực và tình huống khác nhau. Như được minh họa trong Hình 4, DEMON có ba đặc tính quan trọng: 1) Bối cảnh thị giác-ngôn ngữ minh họa, tất cả các hướng dẫn chứa các chuỗi hình ảnh và văn bản có liên quan lẫn nhau, chẳng hạn như storyboard với kịch bản, và sách giáo khoa với sơ đồ. 2) Các hình thức đa dạng của hướng dẫn phức tạp

4

--- TRANG 5 ---
Danh mục: Kể chuyện hình ảnh Mọi người ở mọi lứa tuổi đều ra ngoài để xem xét các sản phẩm thủ công để bán. Phản hồi: Một số thợ thủ công thậm chí còn mặc trang phục độc đáo như một phần của màn trình diễn bán hàng.

4
5

1 Giáo xứ địa phương tổ chức triển lãm thủ công hàng năm.

2 Rất nhiều người ra ngoài và dựng bàn để bán đồ thủ công của họ.

3 Một số sản phẩm thủ công này rất độc đáo và cần rất nhiều tài năng để làm. Hướng dẫn: Với những câu chuyện được ghép nối với những hình ảnh đầu tiên, bạn có thể hoàn thành câu chuyện dựa trên hình ảnh cuối cùng không? Mọi người ở mọi lứa tuổi đều ra ngoài để xem xét các sản phẩm thủ công để bán. Danh mục: Suy luận quan hệ hình ảnh

Có ba người trong ảnh bên trái và không có ai trong ảnh bên phải. Có thể quan sát được những thay đổi nào giữa hai bức ảnh này?

1
2

Danh mục: Hoàn thiện đa phương thức

1
2
3 Hướng dẫn: Với một chuỗi các bảng comic được cung cấp, bạn có thể xác định bảng tiếp theo phù hợp nhất từ các lựa chọn có sẵn không?

ABC

D Phản hồi: Bảng C. Danh mục: Hỏi đáp hình ảnh phong phú văn bản Hướng dẫn: Vui lòng phân tích dữ liệu trong các slide này và trả lời câu hỏi được đề xuất tương ứng.

1
2

Câu hỏi: Có bao nhiều hồ sơ được xem mỗi ngày trên trang web xã hội có hơn 350 triệu thành viên và nơi người dùng trung bình có 240 kết nối? Phản hồi: Hơn 25 triệu. Danh mục: Đối thoại đa phương thức Là một trợ lý thông minh, mục tiêu cuối cùng của bạn là đặt một hộp xà phòng lên quầy. Hãy bắt đầu bằng cách quan sát cẩn thận môi trường xung quanh và đưa ra quyết định từng bước.

1 Quay trái và đi đến bồn rửa bên phải.

3
2

Tiến một bước sang trái. Mở tủ dưới bên trái và lấy hộp xà phòng gần nhất. Danh mục: Suy luận đa hình ảnh Hướng dẫn: Hướng dẫn & Phản hồi:

Phản hồi:

Tiếp theo nên làm gì? Bước tiếp theo là gì?

OK. Tiếp theo là gì?

…

1
3

Ba bức ảnh quần áo này có giống nhau về phong cách không?

Có. Chúng có phong cách tương tự. Hướng dẫn:

2

Danh mục: Hỏi đáp dựa trên kiến thức Mọi người ở mọi lứa tuổi đều ra ngoài để xem xét các sản phẩm thủ công để bán. Phản hồi: A. giảm carbon dioxide, tăng pH, hoặc giảm nhiệt độ. Biểu đồ cho thấy đường cong phân ly oxy cho hemoglobin. Dựa trên biểu đồ, điều gì có thể gây ra việc đường cong dịch chuyển sang trái, như được thể hiện bởi đường vẽ chấm? Hướng dẫn: Được cung cấp một chuỗi sơ đồ từ sách giáo khoa, trách nhiệm của bạn là trả lời đúng câu hỏi sau.

1
3

A. giảm carbon dioxide, tăng pH, hoặc giảm nhiệt độ B. tăng carbon dioxide, tăng pH, hoặc giảm nhiệt độ C. giảm carbon dioxide, giảm pH, hoặc giảm nhiệt độ D. giảm carbon dioxide, tăng pH, hoặc tăng nhiệt độ

2 Phản hồi: Hình 4: Minh họa và phân loại nhiệm vụ của bộ đánh giá DEMON được đề xuất.

hướng dẫn, các hướng dẫn từ thiết kế bảng cho comic, đến khám phá sự khác biệt
giữa hình ảnh giám sát, và đến các nhiệm vụ đối thoại nhúng. 3) Phạm vi rộng lớn của các tình huống tuân theo hướng dẫn, bộ đánh giá bao gồm nhiều tình huống thực tế, bao gồm phim hoạt hình, hình ảnh công nghiệp, ghi âm lái xe, công thức nấu ăn, v.v.

Giao thức đánh giá. Nhờ định dạng nhiệm vụ thống nhất của DEMON, tất cả các nhiệm vụ có thể được đánh giá theo cách zero-shot. Đối với các nhiệm vụ tạo sinh mở, chúng tôi áp dụng ROUGE-L để đánh giá. Đối với các
nhiệm vụ yêu cầu mô hình xuất ra chỉ số tùy chọn, chúng tôi lấy Độ chính xác làm thước đo đánh giá.
Mặc dù các tùy chọn được định dạng tốt được cung cấp, chúng tôi quan sát thực nghiệm rằng nhiều MLLMs gặp khó khăn trong việc tuân theo nghiêm ngặt hướng dẫn để xuất ra chỉ số tùy chọn mà tạo ra văn bản tự do. Do đó, khi
mô hình không xuất ra chính xác các tùy chọn yêu cầu, chúng tôi khớp đầu ra của chúng với một trong các tùy chọn đã cho dựa trên khoảng cách TF-IDF, mà chúng tôi thấy mạnh mẽ hơn các phương pháp dựa trên mô hình (OpenAI,
2023a; Reimers & Gurevych, 2019). Vì chúng tôi khám phá một số lượng lớn nhiệm vụ, chúng tôi lấy tối đa
500 thể hiện mỗi nhiệm vụ để đánh giá hiệu quả và loại trừ một số bộ dữ liệu khó có được và tuân theo các hạn chế bản quyền nghiêm ngặt (được gọi là DEMON-Core). Trong khi đó,
chúng tôi báo cáo phiên bản đầy đủ của bộ đánh giá để tạo điều kiện cho nghiên cứu tương lai về tinh chỉnh hướng dẫn đa phương thức quy mô lớn (được gọi là DEMON-Full). Trừ khi được nêu cụ thể, chúng tôi sử dụng DEMON để chỉ
DEMON-Core trong phần sau.

Bảng 1: Thống kê chi tiết của bộ đánh giá DEMON.

Nhiệm vụ Tình huống Hình ảnh Hướng dẫn Trung bình Hình ảnh / Hướng dẫn Trung bình Từ / Hướng dẫn
DEMON-Core 29 19 62.81K 18.18K 3.46 92.69
DEMON-Full 31 20 1.77M 477.72K 3.70 97.58

Phân tích bộ đánh giá. Bảng 1 chi tiết thống kê. Bộ đánh giá DEMON bao gồm 31 nhiệm vụ của 7 danh
mục trên 20 tình huống. Tổng cộng, DEMON-Full bao gồm 477.72K cặp hướng dẫn-phản hồi,
5

--- TRANG 6 ---
Bảng 2: Kết quả trung bình của đánh giá zero-shot trên mỗi danh mục nhiệm vụ của Bộ đánh giá DEMON.

Đối thoại đa phương thức Kể chuyện hình ảnh Suy luận quan hệ hình ảnh Hoàn thiện đa phương thức Hỏi đáp dựa trên kiến thức Hỏi đáp hình ảnh phong phú văn bản Suy luận đa hình ảnh

BLIP-2 (Li et al., 2023c) 26.12 21.31 10.67 17.94 39.23 33.53 39.65
InstructBLIP (Dai et al., 2023) 33.58 24.41 11.49 21.20 47.40 44.40 48.55
LLaMA-Adapter V2 (Gao et al., 2023) 14.22 17.57 13.51 18.00 44.80 32.00 44.03
LLaV A (Liu et al., 2023) 7.79 10.70 8.27 15.85 36.20 28.33 41.53
MiniGPT-4 (Zhu et al., 2023a) 13.69 17.07 7.95 16.60 30.27 26.40 43.50
mPLUG-Owl (Ye et al., 2023) 12.67 19.33 5.40 16.25 33.27 32.47 42.50
OpenFlamingo (Awadalla et al., 2023) 16.88 24.22 13.85 21.65 32.00 30.60 41.63
Otter (Li et al., 2023a) 15.37 15.57 11.39 16.00 41.67 27.73 43.85
VPG-C 37.50 25.20 25.90 22.15 48.60 44.93 50.28

phục vụ như một bộ đánh giá quy mô lớn cho việc tuân theo hướng dẫn minh họa. Trung bình, mỗi hướng dẫn chứa 3.70 hình ảnh và 97.58 từ. Vui lòng tham khảo Phụ lục B để biết thêm chi tiết.

4 THỰC NGHIỆM
4.1 ĐÁNH GIÁ ZERO-SHOT TRÊN BỘ ĐÁNH GIÁ DEMON

So sánh với các MLLMs tiên tiến. Trong phần này, chúng tôi tiến hành đánh giá toàn diện VPG-C của chúng tôi và các MLLMs tiên tiến gần đây trên bộ đánh giá DEMON được đề xuất. Đối với tất cả các phương pháp, chúng tôi chọn các phiên bản có kích thước tham số dưới 10B. Vui lòng tham khảo Phụ lục D, F để biết chi tiết. Kết quả trung bình của mỗi danh mục nhiệm vụ được tóm tắt trong Bảng 2, cho thấy những điều sau.

•VPG-C liên tục vượt trội so với các mô hình hiện tại với biên độ lớn trên tất cả các danh mục, điều này chứng minh khả năng tổng quát mạnh mẽ hơn để tuân theo các hướng dẫn minh họa phức tạp như vậy.
• Mặc dù các công trình trước đây chủ yếu tinh chỉnh trên dữ liệu hướng dẫn đa phương thức lớn, VPG-C vẫn đạt được hiệu suất cao nhất sử dụng dữ liệu huấn luyện tổng hợp với chi phí tính toán thấp hơn nhiều.
Điều này xác nhận hiệu quả của mô-đun VPG-C được đề xuất và chiến lược huấn luyện tổng hợp của nó.
• So với các công trình trước đây tinh chỉnh bộ giải mã ngôn ngữ quy mô lớn hoặc bộ mã hóa hình ảnh (tức là, LLaV A, mPLUG-Owl), mô hình của chúng tôi chỉ tinh chỉnh mô-đun VPG-C nhẹ với
6.3M tham số và đạt được cải thiện hiệu suất đáng kể.
•VPG-C thể hiện ưu thế đáng kể trong một số nhiệm vụ thách thức. Chẳng hạn, VPG-C vượt trội
so với các phương pháp SOTA 3.92% trong đối thoại đa phương thức, yêu cầu mô hình liên kết hiệu quả các hình ảnh xen kẽ được đề cập trong các lượt đối thoại khác nhau.

Phát hiện sáng tạo. Đánh giá rộng rãi trên bộ đánh giá DEMON tiết lộ một số phát hiện chính.
•Hiệu suất kém trên các hướng dẫn minh họa. Mặc dù một số mô hình (ví dụ, OpenFlamingo,
Otter, mPLUG-owl) đã được huấn luyện trên dữ liệu thị giác-ngôn ngữ xen kẽ, chẳng hạn như mmc4 (Zhu
et al., 2023b), chúng vẫn gặp khó khăn để thực hiện tốt trên các hướng dẫn minh họa. Chúng tôi cho rằng
mặc dù mmc4 chứa các chuỗi hình ảnh xen kẽ làm bối cảnh, các hình ảnh được thu thập từ web
thường có liên quan yếu. Ngược lại, các hình ảnh và văn bản trong các hướng dẫn minh họa có liên quan chặt chẽ, yêu cầu mô hình kết hợp sâu chúng để hiểu ý định nhiệm vụ.
•Khả năng tuân theo hướng dẫn hạn chế. Mặc dù các mô hình thị giác-ngôn ngữ hiện tại tận dụng
LLMs hiện đại, đã chứng minh khả năng ấn tượng trong việc tuân theo hướng dẫn ngôn ngữ,
năng lực này dường như giảm sút khi xử lý các hướng dẫn đa phương thức phức tạp. Chẳng hạn,
khi được giao nhiệm vụ chọn câu trả lời đúng từ danh sách lựa chọn dựa trên bối cảnh hình ảnh và
văn bản, chúng tôi quan sát một số mô hình có xu hướng mô tả nội dung hình ảnh hơn là giải quyết các câu hỏi được đặt ra. Điều này được coi là một thiếu sót trong quá trình huấn luyện căn chỉnh hình ảnh-văn bản, mà chúng tôi cho là nguyên nhân của sự khác biệt.
•Thất bại trong việc xử lý câu hỏi lựa chọn hình ảnh. Khi xử lý các nhiệm vụ hoàn thiện đa phương thức, tất cả
mô hình đều bị hạn chế trong việc xử lý các hướng dẫn liên quan đến hình ảnh làm tùy chọn. Chúng tôi hy vọng công trình tương lai sẽ
sử dụng bộ đánh giá mới để đạt tiến bộ trên loại hướng dẫn minh họa này.

4.2 ĐÁNH GIÁ ZERO-SHOT TRÊN BỘ ĐÁNH GIÁ MME

Chúng tôi đánh giá VPG-C của chúng tôi trên bộ đánh giá MME được đề xuất đồng thời (Fu et al., 2023) để
minh họa thêm khả năng tổng quát mạnh mẽ của nó để tuân theo một loạt hướng dẫn hình ảnh đơn đa dạng. Bộ đánh giá MME
đo lường cả khả năng nhận thức và nhận thức trên tổng cộng 14 nhiệm vụ con. Chúng tôi báo cáo
6

--- TRANG 7 ---
Bảng 3: Đánh giá zero-shot khả năng nhận thức và nhận thức trên bộ đánh giá MME.

BLIP-2 InstructBLIP LA-V2 LLaV A MiniGPT-4 mPLUG-Owl Otter VPG-C
Nhận thức 1293.84 1212.82 972.67 502.82 866.57 967.34 1292.26 1299.24
Nhận thức 290.00 291.79 248.93 214.64 292.14 276.07 306.43 321.07

kết quả trung bình của các nhiệm vụ nhận thức và nhiệm vụ nhận thức trong Bảng 3, tương ứng. Mặc dù chúng tôi
không sử dụng dữ liệu hướng dẫn đa phương thức lớn để tinh chỉnh VPG-C, VPG-C vẫn đạt được hiệu suất vượt trội, so với các mô hình được tinh chỉnh hướng dẫn có giám sát. Điều này cho thấy phương pháp của chúng tôi
hiệu quả vượt qua hạn chế vốn có của VPGs và các chi tiết còn lại được hoàn thiện cũng
cần thiết cho các hướng dẫn hình ảnh đơn. Vui lòng tham khảo Phụ lục E để biết kết quả chi tiết.

4.3 ĐÁNH GIÁ CON NGƯỜI VỀ TẠO SINH NGÔN NGỮ MỤC ĐÍCH CHUNG

OpenFlamingo InstructBLIP MiniGPT-4 mPLUG-Owl VPG-C 1.69 2.53 3.24 3.62 4.19

Hình 5: Đánh giá con người. Chúng tôi tiến hành thêm đánh giá con người trên bộ đánh giá OwlEval (Ye et al., 2023), chứa 82 câu hỏi mở bao gồm tạo quảng cáo và thơ, hiểu sơ đồ và lưu đồ, và giảng dạy, v.v. Cụ thể, chúng tôi tuyển dụng 8 người có học vấn cao để xếp hạng các phản hồi được xáo trộn ngẫu nhiên từ VPG-C, MiniGPT-4, mPLUG-Owl, OpenFlamingo và InstructBLIP. Điểm số từ 1 đến 5 (5 có nghĩa là tốt nhất) và được phép bằng nhau cho các thể hiện so sánh được. Như được thể hiện trong Hình 5, VPG-C cũng chứng minh khả năng tạo sinh ngôn ngữ mở tốt hơn trong các trường hợp thực tế đa dạng.

4.4 PHÂN TÍCH SÂU

Hiệu quả của từng thành phần. Chúng tôi nghiên cứu hiệu quả của mỗi thành phần trong
Bảng 4. Chúng tôi bắt đầu với mô hình backbone sử dụng Q-former để tạo gợi ý hình ảnh.
1) Thay vì áp dụng VPG-C để nắm bắt các chi tiết bị thiếu, trước tiên chúng tôi thử một phương pháp đơn giản dựa trên heuristic trực tiếp trích xuất các đặc trưng hình ảnh ít được chú ý theo các bản đồ chú ý chéo của Q-former và tích hợp lại chúng vào lớp trung gian của LLM như chúng tôi. Chúng tôi tinh chỉnh một lớp tuyến tính trước khi tích hợp lại với 0.5 triệu cặp hình ảnh-chú thích. Kết quả của Hàng 2 cho thấy rằng heuristic đơn giản như vậy có thể mang lại một số cải thiện. Điều này xác nhận tầm quan trọng của việc trích xuất lại
các đặc trưng hình ảnh bị thiếu từ hình ảnh để hiểu các hướng dẫn minh họa. 2) Sau đó, chúng tôi
thay thế nó bằng VPG-C và huấn luyện nó chỉ sử dụng các cặp hình ảnh-chú thích mà không có huấn luyện tổng hợp. Kết quả của Hàng 3 chứng minh rằng VPG-C có thể hoàn thiện chính xác hơn các chi tiết bị thiếu cần thiết bằng cách tận dụng kết quả suy luận trung gian của LLM. 3) Tuy nhiên, chỉ sử dụng dữ liệu hình ảnh-chú thích thông thường không thể khai thác hoàn toàn sức mạnh của VPG-C. So sánh Hàng 3 và Hàng 4, chúng tôi quan sát một cải thiện đáng kể cho tất cả các nhiệm vụ, cho thấy rằng chiến lược huấn luyện phân biệt tổng hợp được đề xuất có thể một cách có hệ thống trao quyền cho VPG-C trích xuất các chi tiết hình ảnh bị thiếu.

VPG-C có thể hướng dẫn VPGs tốt hơn. Vì InstructBLIP có thể thực hiện trích xuất đặc trưng hình ảnh có điều kiện, chúng tôi triển khai một phiên bản biến thể nối đáp án được tạo ban đầu với hướng dẫn làm điều kiện để trích xuất lại đặc trưng. Đáp án được tạo ban đầu phục vụ như một heuristic bổ sung từ LLM để hướng dẫn trích xuất đặc trưng. Sau đó, các gợi ý hình ảnh được trích xuất mới được sử dụng để tạo lại đáp án. Để so sánh công bằng, chúng tôi cung cấp một phiên bản zero-shot (Hàng 5) và một phiên bản được tinh chỉnh (Hàng 6) sử dụng huấn luyện tổng hợp như chúng tôi. Như được thể hiện trong Bảng 4, việc trực tiếp sử dụng dữ liệu tổng hợp và đáp án được suy luận làm điều kiện heuristic không mang lại cải thiện đáng chú ý. Ngược lại, VPG-C có thể hướng dẫn VPG tốt hơn để hoàn thiện các chi tiết hình ảnh bị thiếu bằng cách chặn các biểu diễn trung gian của LLM. Hơn nữa, VPG-C hiệu quả tính toán hơn vì nó chỉ yêu cầu một lần chuyển tiếp đầy đủ của LLM, trong khi các biến thể InstructBLIP yêu cầu hai lần.

VPG-C hoạt động tốt trên các backbone ngôn ngữ khác nhau. Bảng 4 cũng xác nhận rằng phương pháp của chúng tôi có thể
hợp tác tốt với các backbone ngôn ngữ của các họ khác nhau (LLaMA2) và kích thước (Vicuna-13B).

VPG-C có thể được triển khai với VPG rất đơn giản. Như một phương pháp chung, VPG-C có thể được
triển khai với các VPGs khác nhau. Ngoài Q-former được sử dụng rộng rãi bao gồm nhiều
khối Transformer, chúng tôi tiến hành thăm dò hiệu quả của VPG-C với VPG đơn giản hơn, tức là, Chiếu Tuyến tính, như được sử dụng trong LLaV A (vui lòng tham khảo Phụ lục C để biết chi tiết triển khai). Bảng 4
Hàng 7 cho thấy kết quả hứa hẹn. VPG-C cũng có thể tăng cường đáng kể hiệu suất của một VPG tuyến tính đơn giản
7

--- TRANG 8 ---
Bảng 4: Kết quả ablation trên Bộ đánh giá DEMON.

Đối thoại đa phương thức Kể chuyện hình ảnh Suy luận quan hệ hình ảnh Hoàn thiện đa phương thức Hỏi đáp dựa trên kiến thức Hỏi đáp hình ảnh phong phú văn bản Suy luận đa hình ảnh

1 Backbone 25.65 21.72 9.33 17.06 37.21 32.42 41.30
2 +Chi tiết Heuristic 28.13 22.76 12.69 18.81 38.75 34.14 43.26
3 +VPG-C 31.76 23.62 19.12 20.09 42.53 39.68 46.71
4 +Huấn luyện tổng hợp 37.50 25.20 25.90 22.15 48.60 44.93 50.28

InstructBLIP 33.58 24.41 11.48 21.20 47.40 44.40 48.55
5 +Điều kiện đáp án 32.10 23.76 11.02 21.86 47.94 42.08 49.01
6 +Huấn luyện tổng hợp 31.76 24.32 12.78 19.87 46.58 42.36 49.82

LLaV A 7.79 10.70 8.27 15.85 36.20 28.33 41.53
7 VPG tuyến tính 16.43 19.48 14.75 18.54 41.32 36.87 46.02

8 VPG-C-LLaMA2-7B 42.70 24.76 25.50 22.95 51.00 44.93 48.68
9 VPG-C-Vicuna-13B 38.14 26.59 27.15 27.15 52.93 49.33 53.65

VPG, xác minh tính chuyển đổi của VPG-C. Thật hứa hẹn khi điều chỉnh VPG-C chung của chúng tôi và
chiến lược huấn luyện tổng hợp tài nguyên thấp tương ứng cho các VPGs khác nhau trong tương lai.

4 8 12 16 20 24 28
Lớp chèn của VPG-C 15 20 25 30 35 40 45 50

Độ chính xác

ROUGE-L

Hình 6: Hiệu suất trên DEMON với
các lớp chèn khác nhau. Phân tích về lớp được chèn của VPG-C. Chúng tôi nghiên cứu tác động của việc chèn VPG-C vào các lớp khác nhau của LLMs. Chúng tôi báo cáo độ chính xác trung bình cho các nhiệm vụ đa lựa chọn và ROUGE-L trung bình cho các nhiệm vụ tạo sinh mở trong Hình 6. Chúng tôi quan sát rằng hiệu suất thấp khi chúng tôi chèn VPG-C quá sớm (tức là, 4, 8) vì mô hình có thể chưa thu thập đủ thông tin bối cảnh để tạo ra hướng dẫn hiệu quả. Trong khi đó, việc chèn VPG-C quá muộn (tức là, 24, 28) làm giảm hiệu suất. Chúng tôi suy đoán điều này là do
hướng dẫn được tạo quá tập trung và không có đủ lớp để tích hợp các chi tiết còn lại.

Bảng 5: Phân tích hiệu quả của
huấn luyện tổng hợp.

Độ chính xác ROUGE-L
16K 38.93 25.67
32K 39.62 27.38
48K 40.45 28.81
64K 41.49 29.53
80K 41.62 29.73
96K 40.12 28.31

Huấn luyện tổng hợp có hiệu quả dữ liệu. Vì chiến lược huấn luyện tổng hợp được đề xuất của chúng tôi có thể xây dựng các nhiệm vụ phân biệt thách thức một cách có mục tiêu, tăng cường khả năng của VPG-C hoàn thiện các chi tiết bị thiếu, nó tránh được nhu cầu về một lượng lớn dữ liệu hướng dẫn minh họa có giám sát. Chúng tôi tiến hành nghiên cứu tác động của số lượng dữ liệu huấn luyện tổng hợp khác nhau. Như được minh họa trong Bảng 5, hiệu suất tiếp tục tăng khi số lượng dữ liệu được tăng từ 16K lên 64K. Vượt quá này, việc tăng số lượng dữ liệu từ 64K lên 80K chỉ mang lại cải thiện nhỏ. Việc tăng thêm dữ liệu cuối cùng kích hoạt một sự sụt giảm hiệu suất vì dữ liệu quá mức dẫn đến overfitting mô hình với nhiệm vụ huấn luyện tổng hợp.

/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni00000030/uni00000052/uni00000047/uni00000044/uni0000004f
/uni00000027/uni0000004c/uni00000044/uni0000004f/uni00000052/uni0000004a/uni00000058/uni00000048/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f
/uni00000036/uni00000057/uni00000052/uni00000055/uni0000005c
/uni00000037/uni00000048/uni0000004f/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000039/uni0000004c/uni00000056/uni00000058/uni00000044/uni0000004f
/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051
/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni00000030/uni00000052/uni00000047/uni00000044/uni0000004f
/uni00000026/uni0000004f/uni00000052/uni0000005d/uni00000048/uni0000002e/uni00000051/uni00000052/uni0000005a/uni0000004f/uni00000048/uni00000047/uni0000004a/uni00000048
/uni0000002a/uni00000055/uni00000052/uni00000058/uni00000051/uni00000047/uni00000048/uni00000047
/uni00000034/uni00000024/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000035/uni0000004c/uni00000046/uni0000004b
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000056/uni00000034/uni00000024/uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048
/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000039/uni00000033/uni0000002a/uni00000010/uni00000026
/uni00000030/uni0000004c/uni00000051/uni0000004c/uni00000010/uni0000002a/uni00000033/uni00000037/uni00000017
/uni00000032/uni00000053/uni00000048/uni00000051
/uni00000029/uni0000004f/uni00000044/uni00000050/uni0000004c/uni00000051/uni0000004a/uni00000052
/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni00000025/uni0000004f/uni0000004c/uni00000053
/uni00000013/uni00000018/uni00000014/uni00000013/uni00000014/uni00000018/uni00000015/uni00000013

Hình 7: Phân tích về độ nhạy cảm thứ tự hình ảnh. Độ nhạy cảm thứ tự hình ảnh. Thứ tự của các hình ảnh xen kẽ trong các hướng dẫn minh họa là
quan trọng đối với ngữ nghĩa tổng hợp của hướng dẫn. Một cách trực quan, việc thay đổi thứ tự hình ảnh trong một hướng dẫn minh họa có thể
thay đổi đáng kể ngữ nghĩa của nó. Do đó,
các biến đổi trong hiệu suất mô hình có thể tiết lộ
độ nhạy cảm của mô hình với ngữ nghĩa hướng dẫn.
Một mô hình lý tưởng nên nắm bắt sắc sảo các thay đổi
trong ngữ nghĩa hướng dẫn. Do đó, chúng tôi hình dung các biến đổi hiệu suất của các mô hình bằng cách xáo trộn ngẫu nhiên thứ tự của các hình ảnh xen kẽ trong các hướng dẫn minh họa. Theo
Hình 7, chúng tôi ngạc nhiên thấy rằng các mô hình SOTA ít nhạy cảm với thứ tự hình ảnh.
Ngược lại, VPG-C có thể nắm bắt sắc sảo các thay đổi ngữ nghĩa gây ra bởi thứ tự hình ảnh bị xáo trộn.
Đặc biệt, hiệu suất của chúng tôi thay đổi đáng kể trong đối thoại đa phương thức, vì thứ tự hình ảnh
trong các nhiệm vụ này liên quan chặt chẽ với nội dung đối thoại.

Ví dụ định tính. Như được minh họa trong Hình 8, VPG-C chứng minh khả năng mạnh mẽ để thực hiện
suy luận trên các hướng dẫn minh họa phức tạp. Chẳng hạn, trong (a), VPG-C có thể nắm bắt sắc sảo
các kết nối giữa các hình ảnh và từ đó suy luận lý do gây ra hiện tượng bất thường này. Trong (b, c), VPG-C thể hiện khả năng hiểu các đối tượng phi lý thông qua
cuộc trò chuyện đa phương thức với con người. Trong (d, e), VPG-C có thể suy luận một cách hợp lý các mối quan hệ giữa
8

--- TRANG 9 ---
Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh. Hình thứ ba tiết lộ một số vòng tròn rám nắng kỳ lạ trên chân của một người đàn ông, hoàn toàn thẳng hàng với các lỗ trong đôi dép được thể hiện trong hình thứ hai. Nó có thể liên quan đến các hoạt động ngoài trời lâu dài của anh ta vì hình đầu tiên cho thấy mặt trời rất gắt. Do đó, vết cháy nắng thú vị xảy ra khi chân của người đàn ông tiếp xúc với ánh nắng mặt trời kéo dài. Do dép một phần chặn tia UV, da cách xa các lỗ vẫn không bị rám nắng trong khi da dưới các lỗ bị rám nắng. Tuy nhiên, ánh nắng mặt trời vẫn chiếu đến da giữa các lỗ, tạo ra các vòng tròn rám nắng.

(a)

Bộ hình ảnh này là một quảng cáo sáng tạo cho việc nhận nuôi động vật, khuyến khích mọi người lấy một tờ ghi chú với thông tin liên lạc cũng phục vụ như thanh lồng cho các động vật phía sau nó. Khi mọi người lấy các ghi chú, động vật có nhiều khả năng được nhận nuôi và được giải thoát khỏi lồng của chúng. Bốn bức ảnh này muốn truyền đạt điều gì cho chúng ta?

(e)

Đây là gì trong ảnh? Đây là hình ảnh của một con cá. Con cá có màu đen và bạc, với miệng và mắt nhỏ. Nó đang nằm nghiêng, với đuôi và vây có thể nhìn thấy. Con cá dài khoảng 5,51 inch và rộng 12,20 inch.

(c)

Bây giờ có phải vẫn là con cá mà bạn nghĩ không? Không, tôi không nghĩ bây giờ nó là cá, có nhiều bút màu và một hộp giống cá trong hình này, những cây bút này nên được đóng gói trong hộp bút chì, vậy đây nên là một hộp bút chì hình cá màu đen trắng.

Hộp bút chì này có thể được sử dụng để làm gì khác ngoài đựng bút? Nó cũng có thể được sử dụng để đựng nhãn dán, tẩy, hoặc đồ văn phòng khác, nhưng hình dạng con cá của nó quá lừa đảo!

Những thay đổi nào đã xảy ra với bức tường này trong các bức ảnh theo thời gian, và bộ ảnh này hy vọng cho chúng ta thấy điều gì thông qua những thay đổi này? Bộ ảnh này chủ yếu cho thấy sự thay đổi của một bức tường. Ban đầu, bức tường là một bức tường kim loại chỉ có một dòng chữ, có độ bóng bạc và rất sạch. Trong hình thứ ba, một số vết gỉ xuất hiện trên tường. Một người đàn ông dừng lại và nhìn chằm chằm vào bức tường. Trong hình thứ ba, có một số xe và một bức tường bị gỉ với đường viền của một con dao. Hình cuối cùng đã là một bức tường bị gỉ và đường viền của một con dao không bị gỉ. Nó chứng minh chất lượng ổn định của con dao bằng cách cho thấy rằng nó sẽ không bị gỉ sau một thời gian dài. (d)

Kết nối giữa hai bức ảnh này là gì? Hình đầu tiên là một chiếc áo phông màu xám với chữ Muggle được in trên đó, trong khi hình thứ hai là logo của Google. Kết nối giữa hai hình này là phông chữ và bảng màu của chúng rất giống nhau. Điều gì khiến bộ ảnh này trông thú vị? Bởi vì từ 'muggle' sử dụng cùng phông chữ và bảng màu như logo của Google, và cách phát âm của chúng rất giống nhau. Sự hài hước của nó nằm ở việc sử dụng một gốc đồng âm và làm cho người mặc chiếc áo này trông giống như một muggle.

(f) Đây là gì và cách sử dụng nó? Đây là hình ảnh của một ống kính máy ảnh. Đó là một loại ống kính máy ảnh được thiết kế để chụp hình ảnh và video. Nó thường được sử dụng kết hợp với thân máy ảnh để tạo thành một hệ thống máy ảnh. Ống kính được thiết kế để tập trung ánh sáng lên phim hoặc cảm biến kỹ thuật số, chụp hình ảnh.

(b) Bạn có chắc đây là hình ảnh của một ống kính máy ảnh không? Tôi xin lỗi. Hình ảnh cho thấy một cái cốc bạc lớn có tay cầm, có thiết kế giống ống kính. Cốc được làm bằng thép không gỉ và cũng có một cái thìa nằm bên trong cốc. Ống kính này có thể chụp ảnh không? Không, nó chỉ được thiết kế để trông giống một ống kính máy ảnh, nhưng nó không phải là ống kính máy ảnh thật.

Hình 8: Ví dụ định tính được tạo bởi mô hình VPG-C-Vicuna-7B của chúng tôi.

các hình ảnh và hiểu những hàm ý ẩn dụ mà chúng muốn truyền đạt. Trong Phụ lục G, chúng tôi
cung cấp thêm các ví dụ thực tế cũng như so sánh với các MLLMs khác, nơi chúng tôi thấy rằng
các mô hình baseline không liên kết đúng nhiều hình ảnh và hiểu bối cảnh minh họa.

5 CÔNG TRÌNH LIÊN QUAN
MLLMs (Yin et al., 2023) nhằm phục vụ như một trợ lý đa năng để thực hiện các nhiệm vụ thị giác-ngôn ngữ khác nhau bằng cách tạo văn bản tự do. Flamingo (Alayrac et al., 2022) và BLIP-2 (Li et al., 2023c)
kết nối LLMs với các bộ mã hóa hình ảnh được tiền huấn luyện mạnh mẽ và chứng minh khả năng zero-shot mạnh mẽ
bằng cách căn chỉnh các đặc trưng hình ảnh với LLMs. Các công trình tiếp theo của LLaV A (Liu et al., 2023), MiniGPT-
4 (Zhu et al., 2023a), InstructBLIP (Dai et al., 2023), Hallucidoctor (Yu et al., 2024), mPLUG-
Owl (Ye et al., 2023) đề xuất tinh chỉnh MLLMs với dữ liệu tinh chỉnh hướng dẫn đa phương thức. Để
đánh giá hiệu quả tiến bộ gần đây trong MLLMs, các công trình đồng thời của LVLM-eHub (Xu et al.,
2023) và MME Benchmark (Fu et al., 2023) được đề xuất, trong khi chúng chủ yếu tập trung vào các hướng dẫn
chỉ liên quan đến một hình ảnh duy nhất với tính đa dạng hướng dẫn hạn chế. Trong bài báo này, chúng tôi đề xuất
bộ đánh giá tuân theo hướng dẫn minh họa đầu tiên, bao gồm các nhiệm vụ đa dạng của các tình huống đa dạng.
Hơn nữa, chúng tôi đề xuất một mô-đun VPG-C nhẹ và chung để giải quyết hạn chế vốn có
của các VPGs hiện tại. VPG-C của chúng tôi được tinh chỉnh hiệu quả bởi chiến lược huấn luyện phân biệt tổng hợp của chúng tôi,
điều này chứng minh tiềm năng mạnh mẽ của các mô hình khuếch tán văn bản-thành-hình ảnh (He et al., 2022; Lin et al.,
2023; Prabhu et al., 2023; Bansal & Grover, 2023; Yu et al., 2023b) để tạo điều kiện hiểu biết thị giác-ngôn ngữ (Radford et al., 2021b; Jia et al., 2021; Li et al., 2022b).

6 KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất VPG-C, một phương pháp chung và hiệu quả tham số suy luận và
hoàn thiện các chi tiết hình ảnh bị thiếu cho MLLMs để hiểu các hướng dẫn minh họa với bối cảnh đa phương thức xen kẽ. Trong khi đó, chúng tôi trình bày một chiến lược huấn luyện phân biệt tổng hợp để
tinh chỉnh VPG-C, loại bỏ nhu cầu về dữ liệu hướng dẫn minh họa có giám sát. Để thúc đẩy
nghiên cứu về hiểu hướng dẫn minh họa, chúng tôi xây dựng DEMON, một bộ đánh giá toàn diện
cho các mô hình ngôn ngữ lớn đa phương thức, bao gồm 31 nhiệm vụ với bối cảnh minh họa thị giác-ngôn ngữ phức tạp, bao gồm một loạt các tình huống. Thông qua huấn luyện tổng hợp, VPG-C
thể hiện hiệu suất zero-shot đáng chú ý trên bộ đánh giá DEMON. Hiệu suất vượt trội của nó trên
các bộ đánh giá đã được thiết lập khác như MME và OwlEval tiếp tục nhấn mạnh hiệu quả của nó.

Lời cảm ơn. Công trình này được hỗ trợ bởi NSFC (Số 62272411), Dự án Nghiên cứu và Phát triển Chính
ở Tỉnh Chiết Giang (Số 2024C01106), Dự án Nghiên cứu và Phát triển Chính Quốc gia của Trung Quốc (2018AAA0101900), Chương trình Nghiên cứu Đặc biệt Tencent WeChat Rhino-Bird (Tencent WXG-FR-2023-10), và Tài trợ nghiên cứu từ Tập đoàn FinV olution.
9

--- TRANG 10 ---
TÀI LIỆU THAM KHẢO

Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–
23736, 2022.

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-
nick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international
conference on computer vision, pp. 2425–2433, 2015.

Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of
natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 18208–18218, 2022.

Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani
Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel
Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. URL https:
//doi.org/10.5281/zenodo.7733589.

Haoping Bai, Shancong Mou, Tatiana Likhomanenko, Ramazan Gokberk Cinbis, Oncel Tuzel, Ping
Huang, Jiulong Shan, Jianjun Shi, and Meng Cao. Vision datasets: A benchmark for vision-based
industrial inspection. arXiv preprint arXiv:2306.07890, 2023.

Ankan Bansal, Yuting Zhang, and Rama Chellappa. Visual question answering on image sets. In
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXI 16, pp. 51–67. Springer, 2020.

Hritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via generated
datasets. arXiv preprint arXiv:2302.02503, 2023.

Nilavra Bhattacharya, Qing Li, and Danna Gurari. Why does a visual question have different an-
swers? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.
4271–4280, 2019.

Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 11621–11631, 2020.

Yingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk.
Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 16495–16504, 2022.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90% chatgpt quality, 2023. URL https://vicuna.lmsys.org.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-
els. arXiv preprint arXiv:2210.11416, 2022.

Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language
models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023.

Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong
Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
19358–19369, 2023.

Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge Belongie. Neural naturalist:
generating fine-grained image comparisons. arXiv preprint arXiv:1909.04101, 2019.
10

--- TRANG 11 ---
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei
Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multi-
modal large language models. arXiv preprint arXiv:2306.13394, 2023.

Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu,
Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model.
arXiv preprint arXiv:2304.15010, 2023.

Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine
this! scripts to compositions to videos. In Proceedings of the European conference on computer
vision (ECCV), pp. 598–613, 2018.

Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao,
and Larry S Davis. Automatic spatially-aware fashion concept discovery. In Proceedings of the
IEEE international conference on computer vision, pp. 1463–1471, 2017.

Darryl Hannan, Akshay Jain, and Mohit Bansal. Manymodalqa: Modality disambiguation and qa
over diverse inputs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
pp. 7879–7886, 2020.

Kaiming He, Georgia Gkioxari, Piotr Doll ́ar, and Ross Girshick. Mask r-cnn. In Proceedings of the
IEEE international conference on computer vision, pp. 2961–2969, 2017.

Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and Xiao-
juan Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint
arXiv:2210.07574, 2022.

Mehrdad Hosseinzadeh and Yang Wang. Image change captioning by learning from an auxiliary
task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 2725–2734, 2021.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021.

Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob
Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In
Proceedings of the 2016 conference of the North American chapter of the association for compu-
tational linguistics: Human language technologies, pp. 1233–1239, 2016.

Phillip Isola, Joseph J Lim, and Edward H Adelson. Discovering states and transformations in image
collections. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 1383–1391, 2015.

Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daume,
and Larry S Davis. The amazing mysteries of the gutter: Drawing inferences between panels in
comic book narratives. In Proceedings of the IEEE Conference on Computer Vision and Pattern
recognition, pp. 7186–7195, 2017.

Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of
similar images. arXiv preprint arXiv:1808.10584, 2018.

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan
Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning
with noisy text supervision. In International conference on machine learning, pp. 4904–4916.
PMLR, 2021.

Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh
Hajishirzi. Are you smarter than a sixth grader? textbook question answering for multimodal
machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern
recognition, pp. 4999–5007, 2017.
11

--- TRANG 12 ---
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv
preprint arXiv:2304.02643, 2023.

Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan
Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint
arXiv:2306.05425, 2023a.

Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven CH Hoi. Lavis: A
library for language-vision intelligence. arXiv preprint arXiv:2209.09019, 2022a.

Juncheng Li, Xin Wang, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, and William Yang
Wang. Unsupervised reinforcement learning of transferable meta-skills for embodied navigation.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
12123–12132, 2020.

Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian,
and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in
neural information processing systems, 35:7290–7303, 2022b.

Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi Yang, Yueting Zhuang,
and Xin Eric Wang. Compositional temporal grounding with structured variational cross-graph
correspondence learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 3032–3041, 2022c.

Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi Tian,
Tat-Seng Chua, and Yueting Zhuang. Gradient-regulated meta-prompt learning for generalizable
vision-language models. 2023b.

Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597, 2023c.

Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou Zhao, Jiaxu Miao, Wenqiao Zhang,
Wenming Tan, Jin Wang, Peng Wang, et al. End-to-end modeling via information tree for one-
shot natural language spatial video grounding. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 8707–8717, 2022d.

Mengze Li, Haoyu Zhang, Juncheng Li, Zhou Zhao, Wenqiao Zhang, Shengyu Zhang, Shiliang
Pu, Yueting Zhuang, and Fei Wu. Unsupervised domain adaptation for video object grounding
with cascaded debiasing learning. In Proceedings of the 31st ACM International Conference on
Multimedia, pp. 3807–3816, 2023d.

Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carl-
son, and Jianfeng Gao. Storygan: A sequential conditional gan for story visualization. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6329–6338,
2019.

Yongqi Li, Wenjie Li, and Liqiang Nie. Mmcoqa: Conversational question answering over text,
tables, and images. In Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pp. 4220–4231, 2022e.

Shaobo Lin, Kun Wang, Xingyu Zeng, and Rui Zhao. Explore the power of synthetic data on
few-shot object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 638–647, 2023.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485, 2023.

Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-
image transformers for story continuation. In Computer Vision–ECCV 2022: 17th European Con-
ference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII, pp. 70–87. Springer,
2022.
12

--- TRANG 13 ---
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document
images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision,
pp. 2200–2209, 2021.

Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual
question answering by reading text in images. In 2019 international conference on document
analysis and recognition (ICDAR), pp. 947–952. IEEE, 2019.

OpenAI. Chatgpt: A language model for conversational ai. Technical report, OpenAI, 2023a. URL
https://www.openai.com/research/chatgpt.

OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023b.

Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman. Lance: Stress-
testing visual models by generating language-guided counterfactual images. arXiv preprint
arXiv:2305.19164, 2023.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pp.
8748–8763. PMLR, 2021a.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pp.
8748–8763. PMLR, 2021b.

Hareesh Ravi, Kushal Kafle, Scott Cohen, Jonathan Brandt, and Mubbasir Kapadia. Aesop: Ab-
stract encoding of stories, objects, and pictures. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pp. 2052–2063, 2021.

Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 3982–3992, 2019.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.

Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
2556–2565, 2018.

Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,
Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions
for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 10740–10749, 2020.

Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for
reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491,
2018.

Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco,
Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over
text, tables and images. arXiv preprint arXiv:2104.06039, 2021.

Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal. Expressing visual relation-
ships via language. arXiv preprint arXiv:1906.07689, 2019.

Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito.
Slidevqa: A dataset for document visual question answering on multiple images. arXiv preprint
arXiv:2301.04883, 2023.
13

--- TRANG 14 ---
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ́ee
Lacroix, Baptiste Rozi ̀ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv:2302.13971, 2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXiv:2307.09288, 2023b.

Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson
env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 9068–9079, 2018.

Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for
accelerating bert inference. arXiv preprint arXiv:2004.12993, 2020.

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048–2057. PMLR, 2015.

Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan
Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large
vision-language models. arXiv preprint arXiv:2306.09265, 2023.

Semih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. Recipeqa: A challenge
dataset for multimodal comprehension of cooking recipes. arXiv preprint arXiv:1809.00812,
2018.

Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen
Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models
with multimodality. arXiv preprint arXiv:2304.14178, 2023.

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on
multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.

Qifan Yu, Juncheng Li, Yu Wu, Siliang Tang, Wei Ji, and Yueting Zhuang. Visually-prompted
language model for fine-grained scene graph generation in an open world. In Proceedings of the
IEEE/CVF International Conference on Computer Vision, pp. 21560–21571, 2023a.

Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, and Yueting Zhuang. Interactive data synthesis
for systematic vision adaptation via llms-aigcs collaboration. arXiv preprint arXiv:2305.12799,
2023b.

Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian,
and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.

Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.
arXiv preprint arXiv:2302.05543, 2023.

Wenqiao Zhang, Siliang Tang, Yanpeng Cao, Shiliang Pu, Fei Wu, and Yueting Zhuang. Frame
augmented alternating attention network for video question answering. IEEE Transactions on
Multimedia, 22(4):1032–1041, 2019.
14

--- TRANG 15 ---
Wenqiao Zhang, Haochen Shi, Jiannan Guo, Shengyu Zhang, Qingpeng Cai, Juncheng Li, Sihui
Luo, and Yueting Zhuang. Magic: Multimodal relational graph adversarial inference for diverse
and unpaired text-based image captioning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 36, pp. 3335–3343, 2022.

Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592, 2023a.

Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Young-
jae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-
scale corpus of images interleaved with text. arXiv preprint arXiv:2304.06939, 2023b.
15

--- TRANG 16 ---
A TỔNG QUAN
Trong phụ lục này, chúng tôi trình bày:
• Thông tin chi tiết về bộ đánh giá DEMON được đề xuất (Phần B).
• Chi tiết triển khai VPG-C của chúng tôi (Phần C).
• Chi tiết triển khai các MLLMs hiện tại trên bộ đánh giá DEMON (Phần D).
• Hiệu suất zero-shot chi tiết trên bộ đánh giá MME (Phần E).
• Hiệu suất zero-shot chi tiết trên bộ đánh giá DEMON (Phần F).
• So sánh định tính với các MLLMs hiện tại (Phần G).

B CHI TIẾT BỘ ĐÁNH GIÁ

Nhiệm vụ Tình huống Bộ dữ liệu Thước đo
Đối thoại đa phương thức
Đối thoại nhúng đàm thoại Nhúng ALFRED (Shridhar et al., 2020) ROUGE-L
Đối thoại đa phương thức Đàm thoại MMCoQA (Li et al., 2022e) ROUGE-L

Kể chuyện hình ảnh
Hoàn thành câu chuyện hoạt hình Phim hoạt hình AESOP (Ravi et al., 2021) ROUGE-L
Hoàn thành câu chuyện hoạt hình Phim hoạt hình PororoSV (Li et al., 2019) ROUGE-L
Hoàn thành câu chuyện hoạt hình Phim hoạt hình FlintstonesSV (Gupta et al., 2018) ROUGE-L
Kể chuyện ảnh tuần tự Album VIST (Huang et al., 2016) ROUGE-L
Kể chuyện ảnh tuần tự Phim hoạt hình DiDeMoSV (Maharana et al., 2022) ROUGE-L

Suy luận quan hệ hình ảnh
Chú thích thay đổi hình ảnh Giám sát Spot-the-Diff (Jhamtani & Berg-Kirkpatrick, 2018) ROUGE-L
Chú thích thay đổi hình ảnh Tổng hợp CLEVR-Change (Hosseinzadeh & Wang, 2021) ROUGE-L
Biểu đạt mối quan hệ hình ảnh Tổng quát IEdit (Tan et al., 2019) ROUGE-L
Biểu đạt sự khác biệt tinh tế Tinh tế Birds-to-Words (Forbes et al., 2019) ROUGE-L

Hoàn thiện đa phương thức
Nhận diện đối thoại comic Phim hoạt hình COMICS-Dialogue (Iyyer et al., 2017) Độ chính xác
Nhận diện bảng comic Phim hoạt hình COMICS-Panel (Iyyer et al., 2017) Độ chính xác
Hoàn thành công thức nấu ăn Công thức RecipeQA-TextCloze (Yagcioglu et al., 2018) Độ chính xác
Hoàn thiện bước hình ảnh Công thức RecipeQA-VisualCloze (Yagcioglu et al., 2018) Độ chính xác

Hỏi đáp dựa trên kiến thức
Hỏi đáp trang web Trang web WebQA (Chang et al., 2022) Độ chính xác
Hỏi đáp sách giáo khoa Sách giáo khoa TQA (Kembhavi et al., 2017) Độ chính xác
Hỏi đáp đa phương thức phức tạp Wikipedia MMQA (Talmor et al., 2021) Độ chính xác
Hỏi đáp đa phương thức phức tạp* Wikipedia MANYMODALQA (Hannan et al., 2020) Độ chính xác

Hỏi đáp hình ảnh phong phú văn bản
Hỏi đáp slide Slide SlideVQA (Tanaka et al., 2023) Độ chính xác
Hỏi đáp OCR Bìa sách OCR-VQA (Mishra et al., 2019) Độ chính xác
Hỏi đáp tài liệu Hình ảnh tài liệu DocVQA (Mathew et al., 2021) Độ chính xác

Suy luận đa hình ảnh
Hỏi đáp tập hình ảnh* Nhìn từ góc độ thứ nhất trong nhà Gibson (Bansal et al., 2020; Xia et al., 2018) Độ chính xác
Hỏi đáp tập hình ảnh Ghi âm lái xe nuScenes (Bansal et al., 2020; Caesar et al., 2020) Độ chính xác
Kiểm tra công nghiệp Công nghiệp VISION (Bai et al., 2023) Độ chính xác
Hỏi đáp thời trang Thời trang Fashion200K (Han et al., 2017) Độ chính xác
Tính nhất quán thuộc tính Tổng quát MIT-States-PropertyCoherence (Isola et al., 2015) Độ chính xác
Tính nhất quán chuyển đổi trạng thái Tổng quát MIT-States-StateCoherence (Isola et al., 2015) Độ chính xác
Khớp bước hình ảnh Công thức RecipeQA-ImageCoherence (Yagcioglu et al., 2018) Độ chính xác
Suy luận hình ảnh đa ngôn ngữ Tổng quát NLVR2 (Suhr et al., 2018) Độ chính xác
Phân tích sự mơ hồ Ảnh di động VizWiz (Bhattacharya et al., 2019) Độ chính xác

Bảng 6: Tóm tắt các nhiệm vụ tuân theo hướng dẫn minh họa trong bộ đánh giá DEMON. * chỉ ra các nhiệm vụ không được bao gồm trong DEMON-Core.

C CHI TIẾT TRIỂN KHAI

Mô hình. Chúng tôi chọn ViT-G/14 từ EV A-CLIP (Fang et al., 2023) làm bộ mã hóa hình ảnh và Q-former được tiền huấn luyện từ BLIP-2 không có tinh chỉnh hướng dẫn làm bộ tạo gợi ý hình ảnh không phụ thuộc nhiệm vụ. Đối với mô hình ngôn ngữ lớn, chúng tôi triển khai ba phiên bản: LLaMA2-7B (Touvron et al.,
16

--- TRANG 17 ---
Q-Former···Truy vấn có thể học Truy vấn đông lạnh Bước 1: Suy luận với Gợi ý hình ảnh không phụ thuộc nhiệm vụ (VP) Bước 2: Hoàn thiện Bộ tạo gợi ý hình ảnh (VPG-C)......Hướng dẫn thị giác-ngôn ngữ xen kẽ Lớp LLM 1 Lớp LLM L/2 Q-Former++ Q-Former được tái sử dụng lấy cùng đầu vào từ bộ mã hóa hình ảnh*

*Bước 3: Suy luận với VP cụ thể cho hướng dẫn Bộ mã hóa hình ảnh Lớp LLM L/2 + 1 Lớp LLM L···Tuyến tính
Tuyến tính Gợi ý hình ảnh cụ thể cho hướng dẫn Phản hồi ngôn ngữ ①Tạo hướng dẫn ②Trích xuất chi tiết còn lại ③tích hợp lại 	ℎ!" 	ℎ#" 	ℎ$" 	ℎ%"............	𝑣!!" 	𝑣!&"...	𝑣'!" 	𝑣'&"...	ℎtoken văn bản 	𝑣gợi ý hình ảnh 	ℎ!(/#		ℎ#(/#			ℎ$(/#			ℎ%(/#............	𝑣'!(/#	𝑣'&(/#...	𝑣'!(/#	𝑣'&(/#...	ℎ!(/#	ℎ#(/#..................	ℎ%(/#			ℎ$(/#		𝑣$'!(/#𝑣$'&(/#𝑣$!&(/#𝑣$!!(/#𝑣̅!!𝑣̅!&...𝑣̅'!𝑣̅'&......

Hình 9: Khung chi tiết của MLLM của chúng tôi được tăng cường với VPG-C.

2023b), Vicuna-7B (Chiang et al., 2023), Vicuna-13B, với 32, 32, 48 lớp Transformer, tương ứng. Chúng tôi rút ra các điều kiện cụ thể cho hướng dẫn từ lớp thứ 16 / 24 và tiêm lại kiến thức hình ảnh có điều kiện vào lớp thứ 17 / 25. Hơn nữa, chúng tôi cung cấp khung chi tiết của MLLM được tăng cường với VPG-C trong Hình 9.

Lựa chọn Q-former. Gần đây, InstructBLIP (Dai et al., 2023) đề xuất lấy hướng dẫn làm đầu vào bổ sung cho Q-former và tinh chỉnh Q-former để trích xuất các đặc trưng hình ảnh theo hướng dẫn sử dụng 16M dữ liệu tinh chỉnh hướng dẫn đa phương thức. Mặc dù đạt được hiệu suất xuất sắc trên các nhiệm vụ trong miền, một nghiên cứu gần đây (Xu et al., 2023) chỉ ra rằng việc tinh chỉnh trên dữ liệu trong miền lớn làm suy yếu nghiêm trọng khả năng tổng quát của nó trên các tình huống thế giới mở. Thay vì dựa trực tiếp vào Q-former để đạt được trích xuất đặc trưng cụ thể cho nhiệm vụ bằng tinh chỉnh hướng dẫn lớn, chúng tôi nhằm sử dụng khả năng suy luận tinh vi của LLMs để hướng dẫn Q-former chú ý có điều kiện đến các chi tiết hình ảnh còn lại. Do đó, chúng tôi sử dụng Q-former không có tinh chỉnh dữ liệu hướng dẫn từ BLIP-2 (Li et al., 2023c), trích xuất nội dung hình ảnh chính không phụ thuộc nhiệm vụ lần đầu tiên.

Huấn luyện. Chúng tôi triển khai VPG-C trong thư viện LA VIS (Li et al., 2022a). Chúng tôi giữ backbone hình ảnh, bộ tạo gợi ý hình ảnh và mô hình ngôn ngữ đông lạnh, và tinh chỉnh mô-đun VPG-C sử dụng chiến lược huấn luyện được đề xuất. Vì các mô hình BLIP-2 không bao gồm Q-former được tiền huấn luyện phù hợp với Vicuna và LLaMA2, chúng tôi tái sử dụng Q-former phù hợp với FlanT5-XXL và tinh chỉnh lớp chiếu tuyến tính cuối cùng với 5 triệu cặp hình ảnh-văn bản để căn chỉnh nó với Vicuna/LLaMA2. Tất cả các tham số có thể tinh chỉnh của mô-đun VPG-C của chúng tôi là một tập hợp các embedding truy vấn và hai lớp chiếu tuyến tính, chỉ chiếm 0.09% (∼6.3M) của toàn bộ mô hình. Đối với huấn luyện tổng hợp, chúng tôi chọn khoảng 30k hình ảnh từ CC3M (Sharma et al., 2018) chứa các đối tượng bị bỏ qua đáng kể và thực hiện các loại chỉnh sửa khác nhau trên chúng. Tổng cộng, chúng tôi tạo ra khoảng 64k hình ảnh tổng hợp với các thay đổi phù hợp. Để ổn định huấn luyện và tránh overfitting, chúng tôi sử dụng 500k cặp hình ảnh-chú thích từ CC3M để huấn luyện chung mô-đun VPG-C. Chúng tôi tinh chỉnh mô-đun VPG-C trong 18k bước sử dụng kích thước batch 24 cho huấn luyện tổng hợp và 64 cho chú thích hình ảnh, mất khoảng 7 giờ để hoàn thành với một GPU A100 duy nhất. Ngoài ra, chúng tôi áp dụng bộ tối ưu AdamW với β= (0.9,0.999), và đặt tốc độ học và weight decay lần lượt là 0.00002 và 0.05. Chúng tôi khởi động huấn luyện với 2k bước khởi động, tiếp theo là cơ chế giảm tốc độ học với lịch trình cosine.

Triển khai VPG-C với VPG tuyến tính. Như một phương pháp chung, VPG-C có thể được triển khai với các VPGs khác nhau. Ngoài Q-former được sử dụng rộng rãi bao gồm nhiều khối Transformer, chúng tôi tiến hành thăm dò hiệu quả của VPG-C với VPG đơn giản hơn, tức là, Chiếu tuyến tính, như được sử dụng trong LLaV A (Liu et al., 2023). LLaV A huấn luyện một lớp tuyến tính đơn giản làm VPG để kết nối các đặc trưng hình ảnh vào không gian embedding từ. Để triển khai VPG-C với VPG tuyến tính, trước tiên chúng tôi chiếu tuyến tính hướng dẫn được tạo g và sau đó lấy nó làm bộ lọc để thực hiện tích Hadamard theo từng phần tử với các đặc trưng hình ảnh XIᵢ từ bộ mã hóa hình ảnh:

V= (W1g1ᵀ)⊙(W2XIᵢ) (1)

trong đó W1 và W2 là các ma trận chiếu tuyến tính, 1ᵀ là transpose của vector toàn một, và ⊙ đại diện cho tích Hadamard. Đầu ra V đại diện cho các chi tiết hình ảnh bị thiếu được trích xuất mới theo hướng dẫn được suy luận. Và V được tích hợp lại vào LLM theo cùng cách.
17

--- TRANG 18 ---
D CHI TIẾT MÔ HÌNH TRONG BỘ ĐÁNH GIÁ DEMON

Những tiến bộ gần đây trong LLMs (OpenAI, 2023a;b) đã báo hiệu những thành tựu đáng kể trên các lĩnh vực khác nhau. Được truyền cảm hứng từ thành công này, nhiều MLLMs (Li et al., 2023c; Liu et al., 2023; Zhu et al., 2023a; Alayrac et al., 2022; Ye et al., 2023; Gao et al., 2023; Li et al., 2023a) đã được đề xuất để thúc đẩy suy luận thị giác-ngôn ngữ tổng quát (Xu et al., 2015; Li et al., 2023b; 2020; Yu et al., 2023a; Li et al., 2022d; Zhang et al., 2022; Li et al., 2022c; 2023d; Zhang et al., 2019; Antol et al., 2015). Trong các thực nghiệm của chúng tôi, chúng tôi đã tiến hành so sánh với một số MLLMs gần đây và đại diện nhất như sau.

•LLaV A (Liu et al., 2023) thiết lập kết nối giữa bộ mã hóa hình ảnh ViT-L/14 từ CLIP (Radford et al., 2021a) và bộ giải mã ngôn ngữ LLaMA (Touvron et al., 2023a), sử dụng một lớp kết nối đầy đủ (FC) nhẹ. Ban đầu, hệ thống huấn luyện lớp FC này sử dụng 595K cặp hình ảnh-văn bản, trong khi giữ cả bộ mã hóa hình ảnh và LLM tĩnh. Sau đó, LLaV A tinh chỉnh cả lớp FC và LLM sử dụng bộ dữ liệu bao gồm 158K cặp hướng dẫn thị giác-ngôn ngữ. Phiên bản được thử nghiệm là "LLaV A-7B-v0".

•LLaMA-Adapter V2 (Gao et al., 2023) là một mô hình hiệu quả tham số trong lĩnh vực hướng dẫn hình ảnh. Mặc dù duy trì bộ mã hóa hình ảnh (ViT-L/14) và LLM trong trạng thái tĩnh, LA-V2 phân phối khả năng tuân theo hướng dẫn của toàn bộ hệ thống LLaMA thông qua bias-tuning. Phương pháp này cho phép tinh chỉnh các tham số scale, bias, norm và prompt trên các bộ dữ liệu đa dạng. Chúng bao gồm 200M dữ liệu chú thích hình ảnh, 158K dữ liệu tuân theo hướng dẫn hình ảnh, và thêm 52K dữ liệu tuân theo hướng dẫn ngôn ngữ, được tập hợp bởi GPT-4 (OpenAI, 2023b). Phiên bản được thử nghiệm là "LLaV A-7B".

•MiniGPT-4 (Zhu et al., 2023a) bắc cầu khoảng cách giữa bộ mã hóa hình ảnh và bộ mã hóa văn bản sử dụng một lớp kết nối đầy đủ (FC). Ban đầu, mô hình này huấn luyện lớp FC trên bộ dữ liệu bao gồm 5M cặp hình ảnh-văn bản trước khi tinh chỉnh nó trên 3.5K dữ liệu hướng dẫn thị giác-ngôn ngữ. Mặc dù đơn giản, MiniGPT-4 yêu cầu tải một bộ mã hóa hình ảnh được tiền huấn luyện từ BLIP2, cũng như một LLM Vicuna (Chiang et al., 2023). Phiên bản được thử nghiệm là "minigpt4-aligned-with-vicuna7b".

•BLIP2 (Li et al., 2023c) sử dụng chiến lược hai giai đoạn để bắc cầu liền mạch khoảng cách phương thức, sử dụng Q-Former lean được tiền huấn luyện trên 129 triệu cặp hình ảnh-văn bản. Giai đoạn đầu tiên khởi động quá trình học biểu diễn thị giác-ngôn ngữ, tận dụng bộ mã hóa hình ảnh đông lạnh, ViT-g/14 từ EV A-CLIP (Fang et al., 2023). Tiếp theo, giai đoạn thứ hai khai thác LLM đông lạnh, FlanT5 (Chung et al., 2022), để khởi tạo học tạo sinh thị giác-thành-ngôn ngữ. Chiến lược sáng tạo này hiệu quả tạo điều kiện cho việc tạo hình ảnh-thành-văn bản có hướng dẫn zero-shot. Phiên bản được thử nghiệm là "blip2-pretrain-flant5xl".

•mPLUG-Owl (Ye et al., 2023) giới thiệu một visual abstractor, cơ bản gần với Perceiver Resampler trong Flamingo (Alayrac et al., 2022), làm cầu nối giữa bộ mã hóa hình ảnh được tiền huấn luyện ViT-L/14 và LLM (LLaMA (Touvron et al., 2023a)). Mô hình này áp dụng quy trình tinh chỉnh hai giai đoạn. Trong giai đoạn đầu, cả bộ mã hóa hình ảnh và visual abstractor trải qua tinh chỉnh toàn diện sử dụng bộ dữ liệu 204M cặp hình ảnh-văn bản. Tiếp theo, trong giai đoạn thứ hai, mPLUG-Owl áp dụng bộ dữ liệu 158K LLaV A-Instruct để tinh chỉnh LLM được tiền huấn luyện theo cách hiệu quả tham số thông qua việc sử dụng LoRA (Hu et al., 2021). Phiên bản được thử nghiệm là "mplug-owl-llama-7b".

•Otter (Li et al., 2023a) là một mô hình đa phương thức áp dụng tinh chỉnh hướng dẫn trong bối cảnh dựa trên OpenFlamingo (Alayrac et al., 2022). Mô hình này tích hợp một bộ mã hóa ngôn ngữ LLaMA-7B (Touvron et al., 2023a) và một CLIP ViT-L/14. Trong khi các bộ mã hóa hình ảnh và văn bản vẫn tĩnh, Otter tinh chỉnh thêm 1.3 tỷ tham số. Những tham số này được rút ra từ các mô-đun thích ứng và được huấn luyện sử dụng 158K dữ liệu tuân theo hướng dẫn. Phiên bản được thử nghiệm là "OTTER-Image-LLaMA7B-LA-InContext".

•InstructBLIP (Dai et al., 2023) bắt nguồn từ một mô hình BLIP-2 được tiền huấn luyện, bao gồm một bộ mã hóa hình ảnh ViT-g/14, một Vicuna, và một Q-Former để làm cầu nối giữa hai thành phần này. Trong quá trình tinh chỉnh hướng dẫn thị giác-ngôn ngữ, chỉ Q-Former trải qua tinh chỉnh, với quá trình huấn luyện tận dụng dữ liệu từ 13 bộ dữ liệu trả lời câu hỏi hình ảnh khác nhau. Phiên bản được thử nghiệm là "blip2-vicuna-instruct-7b".

•OpenFlamingo (Alayrac et al., 2022; Awadalla et al., 2023) đại diện cho một trong những nỗ lực tiên phong để kết hợp Học mô hình ngôn ngữ (LLMs) vào lĩnh vực tiền huấn luyện thị giác-ngôn ngữ. Để tối ưu hóa điều kiện của nó trên các đặc trưng hình ảnh, Flamingo một cách chiến lược tích hợp một số khối dense chú ý chéo có cổng giữa các lớp của bộ mã hóa ngôn ngữ được tiền huấn luyện. OpenFlamingo cung cấp một phiên bản mã nguồn mở của mô hình tiên tiến này. Phiên bản được thử nghiệm là "llama-7b".

Bộ đánh giá DEMON chủ yếu có các hướng dẫn thị giác-ngôn ngữ xen kẽ, phân biệt nó với các bộ dữ liệu hình ảnh đơn truyền thống. Trong khi phương pháp sáng tạo của chúng tôi, VPG-C, cùng với OpenFlamingo và MiniGPT-4, vốn dĩ chứa các chuỗi hình ảnh-văn bản xen kẽ, các mô hình khác như BLIP-2, InstructBlip, LLaV A, mPLUG-Owl, Otter, và LLaMA-Adapter V2 thì không. Đối với những mô hình này, chúng tôi sử dụng chiến lược nối các embedding của tất cả hình ảnh. Phương pháp này có thể được tương tự như coi hình ảnh như các khung hình trong video. Để duy trì bối cảnh vị trí của mỗi hình ảnh trong hướng dẫn hình ảnh-văn bản xen kẽ, chúng tôi chỉ ra rõ ràng vị trí của mỗi hình ảnh trong bối cảnh.

E HIỆU SUẤT ZERO-SHOT CHI TIẾT TRÊN BỘ ĐÁNH GIÁ MME

Trong phần này, chúng tôi báo cáo hiệu suất chi tiết trên 14 nhiệm vụ con của bộ đánh giá MME trong Bảng 7.

Bảng 7: Hiệu suất zero-shot chi tiết trên bộ đánh giá MME.

BLIP-2 InstructBLIP LA-V2 LLaV A MiniGPT-4 mPLUG-Owl Otter VPG-C
Tồn tại 160.00 185.00 120.00 50.00 115.00 120.00 195.00 180.00
Đếm 135.00 143.33 50.00 50.00 123.33 88.33 50.00 96.67
Vị trí 73.33 66.67 48.33 50.00 81.67 50.00 86.67 80.00
Màu sắc 148.33 153.33 75.00 55.00 110.00 55.00 113.33 116.67
Áp phích 141.84 123.81 99.66 50.00 55.78 136.05 138.78 147.28
Người nổi tiếng 105.59 101.18 86.18 48.82 65.29 100.29 172.65 164.12
Cảnh 145.25 153.00 148.50 50.00 95.75 135.50 158.75 156.00
Địa danh 138.00 79.75 150.25 50.00 69.00 159.25 137.25 145.00
Tác phẩm nghệ thuật 136.50 134.25 69.75 49.00 55.75 96.25 129.00 113.50
OCR 110.00 72.50 125.00 50.00 95.00 65.00 72.50 100.00
Nhận thức 1293.84 1212.82 972.67 502.82 866.57 967.34 1292.26 1299.24
Thường thức 110.00 129.29 81.43 57.14 72.14 78.57 106.43 98.57
Số học 40.00 40.00 62.50 50.00 55.00 60.00 72.50 77.50
Dịch văn bản 65.00 65.00 50.00 57.50 55.00 80.00 57.50 57.50
Suy luận mã 75.00 57.50 55.00 50.00 110.00 57.50 70.00 87.50
Nhận thức 290.00 291.79 248.93 214.64 292.14 276.07 306.43 321.07

F HIỆU SUẤT ZERO-SHOT CHI TIẾT TRÊN BỘ ĐÁNH GIÁ DEMON

Bảng 8: Đánh giá zero-shot trên đối thoại đa phương thức.

Đối thoại nhúng đàm thoại Đối thoại đa phương thức
BLIP-2 16.75 35.49
InstructBLIP 18.07 49.09
LLaMA-Adapter V2 19.04 9.40
LLaV A 10.19 5.39
MiniGPT-4 16.82 10.57
mPLUG-Owl 11.07 14.27
OpenFlamingo 24.27 9.49
Otter 16.06 14.68
VPG-C-LLaMA2-7B 48.31 37.04
VPG-C-Vicuna-7B 41.02 33.99
VPG-C-Vicuna-13B 42.25 34.02
19

--- TRANG 19 ---
Bảng 9: Đánh giá zero-shot về kể chuyện hình ảnh.

Hoàn thành câu chuyện hoạt hình-AESOP Hoàn thành câu chuyện hoạt hình-PororoSV Hoàn thành câu chuyện hoạt hình-FlintstonesSV Kể chuyện ảnh tuần tự-VIST Kể chuyện ảnh tuần tự-DiDeMoSV

BLIP-2 21.64 26.24 29.61 13.16 24.2
InstructBLIP 18.80 28.20 33.32 16.92 24.80
LLaMA-Adapter V2 18.01 20.15 24.22 10.89 14.57
LLaV A 13.56 11.44 12.77 8.00 7.71
MiniGPT-4 12.23 16.00 26.48 14.82 15.81
mPLUG-Owl 18.28 20.49 32.12 10.82 14.94
OpenFlamingo 23.32 32.35 37.79 15.14 12.50
Otter 13.94 17.52 22.21 9.96 14.23
VPG-C-LLaMA2-7B 19.98 28.67 38.14 16.95 20.05
VPG-C-Vicuna-7B 19.93 28.36 39.19 17.34 21.27
VPG-C-Vicuna-13B 20.53 29.81 41.32 19.04 22.26

Bảng 10: Đánh giá zero-shot về suy luận quan hệ hình ảnh.

Chú thích thay đổi hình ảnh -Spot-the-Diff Chú thích thay đổi hình ảnh -CLEVR-Change Biểu đạt mối quan hệ hình ảnh Biểu đạt sự khác biệt tinh tế

BLIP-2 17.48 3.21 12.37 9.62
InstructBLIP 19.71 4.61 10.70 10.92
LLaMA-Adapter V2 16.72 15.52 7.88 13.92
LLaV A 8.50 8.76 6.72 9.11
MiniGPT-4 7.50 7.49 7.84 8.97
mPLUG-Owl 6.06 1.46 6.22 7.86
OpenFlamingo 13.01 11.90 12.57 17.90
Otter 12.69 11.63 8.85 12.38
VPG-C-LLaMA2-7B 21.02 42.05 14.10 24.81
VPG-C-Vicuna-7B 20.01 41.60 16.35 25.64
VPG-C-Vicuna-13B 21.56 40.67 20.27 26.08

Bảng 11: Đánh giá zero-shot về hoàn thiện đa phương thức.

Nhận diện đối thoại comic Nhận diện bảng comic¹ Hoàn thành công thức nấu ăn Hoàn thiện bước hình ảnh¹

BLIP-2 39.70 0.00 30.46 1.60
InstructBLIP 40.60 0.00 27.40 16.80
LLaMA-Adapter V2 24.40 0.40 38.20 9.00
LLaV A 30.60 0.00 32.80 0.00
MiniGPT-4 33.00 1.00 31.60 0.80
mPLUG-Owl 36.60 0.00 27.60 0.80
OpenFlamingo 38.40 1.20 29.00 18.00
Otter 29.00 0.00 35.00 0.00
VPG-C-LLaMA2-7B 36.80 1.80 51.80 1.40
VPG-C-Vicuna-7B 39.20 3.60 30.40 15.40
VPG-C-Vicuna-13B 42.20 8.20 39.80 18.40

¹Đối với các nhiệm vụ có hình ảnh làm tùy chọn, chỉ những phản hồi bắt đầu bằng câu trả lời đúng mới được đánh giá là đúng.

Bảng 12: Đánh giá zero-shot về hỏi đáp dựa trên kiến thức.

Hỏi đáp trang web Hỏi đáp sách giáo khoa Hỏi đáp đa phương thức phức tạp

BLIP-2 47.60 29.73 40.36
InstructBLIP 45.20 30.20 66.80
LLaMA-Adapter V2 44.60 46.00 43.80
LLaV A 39.40 39.60 29.60
MiniGPT-4 27.40 28.60 34.80
mPLUG-Owl 34.20 30.00 35.60
OpenFlamingo 37.80 32.40 25.80
Otter 45.00 39.00 41.00
VPG-C-LLaMA2-7B 49.40 42.40 61.20
VPG-C-Vicuna-7B 50.00 33.40 62.40
VPG-C-Vicuna-13B 50.60 43.40 64.80
20

--- TRANG 20 ---
Bảng 13: Đánh giá zero-shot về hỏi đáp hình ảnh phong phú văn bản.

Hỏi đáp slide Hỏi đáp OCR Hỏi đáp tài liệu

BLIP-2 43.80 10.40 46.40
InstructBLIP 42.00 44.20 47.00
LLaMA-Adapter V2 43.00 3.40 49.60
LLaV A 38.80 2.60 43.60
MiniGPT-4 35.20 7.20 36.80
mPLUG-Owl 35.60 22.60 39.20
OpenFlamingo 35.60 3.80 52.40
Otter 38.40 2.20 42.60
VPG-C-LLaMA2-7B 45.80 39.60 49.40
VPG-C-Vicuna-7B 46.80 39.40 48.60
VPG-C-Vicuna-13B 48.80 46.60 52.60

Bảng 14: Đánh giá zero-shot về suy luận đa hình ảnh.

Hỏi đáp tập hình ảnh Kiểm tra công nghiệp Hỏi đáp thời trang Tính nhất quán thuộc tính Tính nhất quán chuyển đổi trạng thái Khớp bước hình ảnh¹ Suy luận hình ảnh đa ngôn ngữ Phân tích sự mơ hồ

BLIP-2 34.60 42.80 43.20 59.00 38.20 0.20 53.40 45.80
instructblip7b 65.00 50.60 44.40 59.20 59.40 11.60 55.20 43.00
LLaMA-Adapter V2 41.60 55.00 45.60 48.80 63.00 0.00 54.80 43.40
LLaV A 29.60 53.00 45.20 50.40 59.20 0.80 50.80 43.20
MiniGPT-4 30.40 59.80 49.20 52.00 57.80 0.20 50.60 48.00
mPLUG-Owl 29.20 54.20 45.80 50.00 60.60 0.00 55.00 45.20
OpenFlamingo 25.80 52.20 44.20 59.60 51.40 2.20 53.60 44.00
Otter 44.80 69.80 47.00 51.40 46.40 0.00 49.00 42.40
VPG-C-LLaMA2-7B 62.60 61.40 46.00 56.60 57.80 0.00 53.80 51.20
VPG-C-Vicuna-7B 67.20 48.80 50.00 60.80 60.00 0.20 57.80 57.40
VPG-C-Vicuna-13B 73.40 54.00 51.00 63.20 63.40 2.60 60.20 61.40

¹Đối với các nhiệm vụ có hình ảnh làm tùy chọn, chỉ những phản hồi bắt đầu bằng câu trả lời đúng mới được đánh giá là đúng.

G SO SÁNH ĐỊNH TÍNH

Trong phần này, chúng tôi so sánh mô hình của chúng tôi với các MLLMs hiện tại trên một số hướng dẫn minh họa phức tạp.
21

--- TRANG 21 ---
Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh. Hình thứ ba tiết lộ một số vòng tròn rám nắng kỳ lạ trên chân của một người đàn ông, hoàn toàn thẳng hàng với các lỗ trong đôi dép được thể hiện trong hình thứ hai. Nó có thể liên quan đến các hoạt động ngoài trời lâu dài của anh ta vì hình đầu tiên cho thấy mặt trời rất gắt. Do đó, vết cháy nắng thú vị xảy ra khi chân của người đàn ông tiếp xúc với ánh nắng mặt trời kéo dài. Do dép một phần chặn tia UV, da cách xa các lỗ vẫn không bị rám nắng trong khi da dưới các lỗ bị rám nắng. Tuy nhiên, ánh nắng mặt trời vẫn chiếu đến da giữa các lỗ, tạo ra các vòng tròn rám nắng.

Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh. Mặt trời đang chiếu sáng lên chân của người đó.

Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh. 1. Hình ảnh bầu trời xanh và những đám mây. 2. Hình ảnh bầu trời xanh và những đám mây. 3. Hình ảnh bầu trời xanh và những đám mây. 4. Hình ảnh bầu trời xanh và những đám mây. 5. Hình ảnh bầu trời xanh và những đám mây. 6. Hình ảnh bầu trời xanh và những đám mây. 7. Hình ảnh bầu trời xanh và những đám mây. 8. Hình ảnh bầu trời xanh và những đám mây. 9. Hình ảnh bầu trời xanh và những đám mây. 10. Hình ảnh bầu trời xanh và

Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh. Vui lòng giải thích điểm thú vị dựa trên ba bức ảnh.

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4 Hình 10: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.
22

--- TRANG 22 ---
Bộ hình ảnh này là một quảng cáo sáng tạo cho việc nhận nuôi động vật, khuyến khích mọi người lấy một tờ ghi chú với thông tin liên lạc cũng phục vụ như thanh lồng cho các động vật phía sau nó. Khi mọi người lấy các ghi chú, động vật có nhiều khả năng được nhận nuôi và được giải thoát khỏi lồng của chúng. Bốn bức ảnh này muốn truyền đạt điều gì cho chúng ta?

Đừng ăn thịt chó. Bốn bức ảnh này muốn truyền đạt điều gì cho chúng ta?

Thông điệp là gì? Mục đích của những bức ảnh này là gì? Thông điệp là gì? Mục đích của những bức ảnh này là gì? Thông điệp là gì? Mục đích của những bức ảnh này là gì? Thông điệp là gì? Mục đích của những bức ảnh này là gì? Thông điệp là gì Bốn bức ảnh này muốn truyền đạt điều gì cho chúng ta?

Áp phích là về một con chó đã bị tấn công bởi một kẻ săn mồi. Nó đang nằm trên mặt đất với đôi mắt nhắm và tai cụp xuống. Áp phích được đặt trên một bức tường gạch với thông điệp nói "xin hãy đưa một con về nhà. Bốn bức ảnh này muốn truyền đạt điều gì cho chúng ta?

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4 Hình 11: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.

Những thay đổi nào đã xảy ra với bức tường này trong các bức ảnh theo thời gian, và bộ ảnh này hy vọng cho chúng ta thấy điều gì thông qua những thay đổi này? Bộ ảnh này chủ yếu cho thấy sự thay đổi của một bức tường. Ban đầu, bức tường là một bức tường kim loại chỉ có một dòng chữ, có độ bóng bạc và rất sạch. Trong hình thứ ba, một số vết gỉ xuất hiện trên tường. Một người đàn ông dừng lại và nhìn chằm chằm vào bức tường. Trong hình thứ ba, có một số xe và một bức tường bị gỉ với đường viền của một con dao. Hình cuối cùng đã là một bức tường bị gỉ và đường viền của một con dao không bị gỉ. Nó chứng minh chất lượng ổn định của con dao bằng cách cho thấy rằng nó sẽ không bị gỉ sau một thời gian dài.

Những thay đổi nào đã xảy ra với bức tường này trong các bức ảnh theo thời gian, và bộ ảnh này hy vọng cho chúng ta thấy điều gì thông qua những thay đổi này? time lapse

Những thay đổi nào đã xảy ra với bức tường này trong các bức ảnh theo thời gian, và bộ ảnh này hy vọng cho chúng ta thấy điều gì thông qua những thay đổi này? Bức tường là một tấm vải trắng, và nghệ sĩ là người vẽ lên nó. Nghệ sĩ là người vẽ lên tường, và tường là người vẽ nghệ sĩ. Tường là người vẽ nghệ sĩ, và nghệ sĩ là người vẽ lên tường. Tường là người vẽ nghệ sĩ, và nghệ sĩ là người vẽ lên tường. Tường là người vẽ nghệ sĩ, và nghệ sĩ là người vẽ lên tường. Tường là người vẽ nghệ sĩ, và nghệ sĩ là người vẽ lên

Những thay đổi nào đã xảy ra với bức tường này trong các bức ảnh theo thời gian, và bộ ảnh này hy vọng cho chúng ta thấy điều gì thông qua những thay đổi này? </Img>

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4

Hình 12: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.
23

--- TRANG 23 ---
Kết nối giữa hai bức ảnh này là gì? Hình đầu tiên là một chiếc áo phông màu xám với chữ Muggle được in trên đó, trong khi hình thứ hai là logo của Google. Kết nối giữa hai hình này là phông chữ và bảng màu của chúng rất giống nhau. Điều gì khiến bộ ảnh này trông thú vị? Bởi vì từ 'muggle' sử dụng cùng phông chữ và bảng màu như logo của Google, và cách phát âm của chúng rất giống nhau. Sự hài hước của nó nằm ở việc sử dụng một gốc đồng âm và làm cho người mặc chiếc áo này trông giống như một muggle.

Kết nối giữa hai bức ảnh này là gì? google Điều gì khiến bộ ảnh này trông thú vị? Điều này có thể được coi là một trò chơi chữ thông minh hoặc một tham chiếu tinh tế đến loạt phim Harry Potter nổi tiếng, nơi thuật ngữ "Muggle" được sử dụng để mô tả những người không có phép thuật trong thế giới phù thủy.

Kết nối giữa hai bức ảnh này là gì? Google, Muggle, Harry Potter, J.K. Rowling, Wizarding World, Wizard, Witch, Wizarding, Wizarding World of Harry Potter, Universal Orlando Resort, Universal Studios Florida, Universal Orlando, Điều gì khiến bộ ảnh này trông thú vị? Hình đầu tiên là một chiếc áo phông màu xám với chữ Muggle được in trên đó, trong khi hình thứ hai là logo của Google. Kết nối giữa hai hình này là phông chữ và bảng màu của chúng rất giống nhau.

Kết nối giữa hai bức ảnh này là gì? Hình ảnh cho thấy từ "google" được viết bằng các chữ cái màu đỏ, xanh và xanh lá cây trên nền trắng. Đây là một ví dụ về cách bảng màu có thể được thay đổi theo nhiều cách khác nhau. Điều gì khiến bộ ảnh này trông thú vị? </img></s>

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4 Hình 13: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.
24

--- TRANG 24 ---
Đây là gì trong ảnh? Đây là hình ảnh của một con cá. Con cá có màu đen và bạc, với miệng và mắt nhỏ. Nó đang nằm nghiêng, với đuôi và vây có thể nhìn thấy. Con cá dài khoảng 5,51 inch và rộng 12,20 inch.

Bây giờ có phải vẫn là con cá mà bạn nghĩ không? Không, tôi không nghĩ bây giờ nó là cá, có nhiều bút màu và một hộp giống cá trong hình này, những cây bút này nên được đóng gói trong hộp bút chì, vậy đây nên là một hộp bút chì hình cá màu đen trắng.

Hộp bút chì này có thể được sử dụng để làm gì khác ngoài đựng bút? Nó cũng có thể được sử dụng để đựng nhãn dán, tẩy, hoặc đồ văn phòng khác, nhưng hình dạng con cá của nó quá lừa đảo!

Đây là gì trong ảnh? fish

Bây giờ có phải vẫn là con cá mà bạn nghĩ không? Có, nó vẫn là con cá giống như trước.

Hộp bút chì này có thể được sử dụng để làm gì khác ngoài đựng bút? Đây là một số cách sử dụng có thể:\n\n1. Phụ kiện bàn làm việc: Hộp bút chì có thể phục vụ như một phụ kiện trang trí bàn làm việc, thêm một chút vui nhộn và cá tính cho không gian làm việc của bạn.

Đây là gì trong ảnh? 1. 13 cm 2. 13 cm 3. 13 cm 4. 13 cm 5. 13 cm 6. 13 cm 7. 13 cm 8. 13 cm 9. 13 cm 10. 13 cm 11.

Bây giờ có phải vẫn là con cá mà bạn nghĩ không? Đây là hình ảnh của một con cá. Con cá có màu đen và bạc, với miệng và mắt nhỏ. Nó đang nằm nghiêng, với đuôi và vây có thể nhìn thấy. Con cá dài khoảng 5,51 inch và rộng 12,20 inch.

Hộp bút chì này có thể được sử dụng để làm gì khác ngoài đựng bút? Nó có thể được sử dụng để đựng những thứ khác, như bút chì, thước kẻ, máy chuốt bút chì, cao su, tẩy bút chì, máy chuốt bút chì, máy chuốt bút chì, máy chuốt bút chì,

Đây là gì trong ảnh? "Đây là hình ảnh của một con cá. Nó đang nằm nghiêng với đuôi cong lên và miệng há ra. Hình ảnh có màu đen trắng và có một vảy ở bên cạnh con cá.

Bây giờ có phải vẫn là con cá mà bạn nghĩ không? Không, nó không phải là cá. Đó là một con cá da trơn đen lớn được đặt trên một đống bút màu và bút chì.

Hộp bút chì này có thể được sử dụng để làm gì khác ngoài đựng bút? Con cá có màu đen và bạc, với miệng và mắt nhỏ. Nó đang nằm nghiêng, với đuôi và vây có thể nhìn thấy. Con cá dài khoảng 5,51 inch và rộng 12,20 inch.

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4 Hình 14: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.
25

--- TRANG 25 ---
Đây là gì và cách sử dụng nó? Đây là hình ảnh của một ống kính máy ảnh. Đó là một loại ống kính máy ảnh được thiết kế để chụp hình ảnh và video. Nó thường được sử dụng kết hợp với thân máy ảnh để tạo thành một hệ thống máy ảnh. Ống kính được thiết kế để tập trung ánh sáng lên phim hoặc cảm biến kỹ thuật số, chụp hình ảnh. Bạn có chắc đây là hình ảnh của một ống kính máy ảnh không? Tôi xin lỗi. Hình ảnh cho thấy một cái cốc bạc lớn có tay cầm, có thiết kế giống ống kính. Cốc được làm bằng thép không gỉ và cũng có một cái thìa nằm bên trong cốc. Ống kính này có thể chụp ảnh không? Không, nó chỉ được thiết kế để trông giống một ống kính máy ảnh, nhưng nó không phải là ống kính máy ảnh thật.

Đây là gì và cách sử dụng nó? Camera

Bạn có chắc đây là hình ảnh của một ống kính máy ảnh không? Có, tôi chắc chắn đây là hình ảnh của một ống kính máy ảnh Ống kính này có thể chụp ảnh không? Có, một ống kính máy ảnh có thể chụp ảnh.

Đây là gì và cách sử dụng nó? EOS 1000D là máy ảnh DSLR cấp độ khởi đầu 10.1 megapixel được Canon công bố vào ngày 26 tháng 6 năm 2008. Nó là người kế thừa của EOS 450D và tiền nhiệm của EOS 1100D. EOS 1000D là máy ảnh DSLR đầu tiên sử dụng ống kính EF-S 18-55mm f/3.5-5.6 IS mới. Bạn có chắc đây là hình ảnh của một ống kính máy ảnh không? Nó trông giống như một cái cốc cà phê hơn. Tôi không chắc bạn đang cố gắng nói gì. Tôi không chắc bạn đang cố gắng nói gì. Tôi không chắc bạn đang cố gắng nói gì. Ống kính này có thể chụp ảnh không? Có, nó có thể. Đó là một ống kính máy ảnh. Nó được thiết kế để chụp hình ảnh và video.

Đây là gì và cách sử dụng nó? Đây là hình ảnh của một ống kính máy ảnh. Đó là một loại ống kính máy ảnh được thiết kế để chụp hình ảnh và video. Nó thường được sử dụng kết hợp với thân máy ảnh để tạo thành một hệ thống máy ảnh. Ống kính được thiết kế để tập trung ánh sáng lên phim hoặc cảm biến kỹ thuật số, chụp hình ảnh. Bạn có chắc đây là hình ảnh của một ống kính máy ảnh không? Tôi xin lỗi. Hình ảnh cho thấy một cái cốc bạc lớn có tay cầm, có thiết kế giống ống kính. Cốc được làm bằng thép không gỉ và cũng có một cái thìa nằm bên trong cốc. Ống kính này có thể chụp ảnh không? Không, nó chỉ được thiết kế để trông giống một ống kính máy ảnh, nhưng nó không phải là ống kính máy ảnh thật.

VPG-C InstructBLIP

OpenFlamingo MiniGPT-4 Hình 15: So sánh định tính giữa VPG-C của chúng tôi và các MLLMs hiện tại.
26
