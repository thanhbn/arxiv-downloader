# 2308.12714.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2308.12714.pdf
# Kích thước tệp: 11998639 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
VIGC: Tạo Lệnh Hướng Dẫn Trực Quan và Hiệu Chỉnh
Bin Wang∗1, Fan Wu∗1, Xiao Han∗1, Jiahui Peng∗1, Huaping Zhong∗2,
Pan Zhang1, Xiaoyi Dong1,3, Weijia Li4, Wei Li1, Jiaqi Wang1, Conghui He†1
1Shanghai AI Laboratory,
2SenseTime Research,3The Chinese University of Hong Kong,4Sun Yat-sen University
{wangbin,wufan,hanxiao,pengjiahui,zhangpan,dongxiaoyi,liwei,wangjiaqi,heconghui }@pjlab.org.cn
zhonghuaping@sensetime.com, liweij29@mail.sysu.edu.cn

Tóm tắt
Việc tích hợp bộ mã hóa hình ảnh và các mô hình ngôn ngữ lớn (LLM) đã thúc đẩy tiến bộ gần đây trong các mô hình ngôn ngữ lớn đa phương thức (MLLM). Tuy nhiên, sự khan hiếm dữ liệu điều chỉnh lệnh hướng dẫn chất lượng cao cho các tác vụ thị giác-ngôn ngữ vẫn là một thách thức. Phương pháp tiếp cận hàng đầu hiện tại, như LLaV A, dựa vào GPT-4 chỉ sử dụng ngôn ngữ để tạo dữ liệu, đòi hỏi phải có chú thích hình ảnh và hộp giới hạn phát hiện được ghi chú trước, gặp khó khăn trong việc hiểu chi tiết hình ảnh. Một giải pháp thực tiễn cho vấn đề này sẽ là sử dụng các mô hình ngôn ngữ lớn đa phương thức có sẵn để tạo dữ liệu lệnh hướng dẫn cho các tác vụ thị giác-ngôn ngữ. Tuy nhiên, cần lưu ý rằng các MLLM hiện có thể truy cập không mạnh mẽ như các đối tác LLM của chúng, vì chúng có xu hướng tạo ra các phản hồi không đầy đủ và tạo ra thông tin sai lệch. Như một giải pháp để giải quyết vấn đề hiện tại, bài báo này đề xuất khung Tạo Lệnh Hướng Dẫn Trực Quan và Hiệu Chỉnh (VIGC) cho phép các mô hình ngôn ngữ lớn đa phương thức tạo dữ liệu điều chỉnh lệnh hướng dẫn và liên tục nâng cao chất lượng của nó trong thời gian thực. Cụ thể, Tạo Lệnh Hướng Dẫn Trực Quan (VIG) hướng dẫn mô hình thị giác-ngôn ngữ tạo ra dữ liệu điều chỉnh lệnh hướng dẫn đa dạng. Để đảm bảo chất lượng tạo, Hiệu Chỉnh Lệnh Hướng Dẫn Trực Quan (VIC) áp dụng cơ chế cập nhật lặp lại để sửa chữa bất kỳ sai sót nào trong dữ liệu được tạo bởi VIG, hiệu quả giảm nguy cơ ảo giác. Tận dụng dữ liệu đa dạng, chất lượng cao được tạo bởi VIGC, chúng tôi tinh chỉnh các mô hình chính và xác thực chất lượng dữ liệu dựa trên các đánh giá khác nhau. Kết quả thử nghiệm cho thấy VIGC không chỉ bù đắp cho những thiếu sót của các phương pháp tạo dữ liệu chỉ sử dụng ngôn ngữ, mà còn nâng cao hiệu quả hiệu suất so với các tiêu chuẩn đánh giá. Các mô hình, bộ dữ liệu và mã nguồn có sẵn tại https://opendatalab.github.io/VIGC.

Giới thiệu
Trong năm qua, đã có những tiến bộ đáng kể trong các mô hình ngôn ngữ, đặc biệt là với sự xuất hiện của điều chỉnh lệnh hướng dẫn trong các Mô hình Ngôn ngữ Lớn (LLM). Công nghệ này cho phép các mô hình thực hiện các tác vụ phức tạp một cách zero-shot (OpenAI 2023a,b). Việc kết hợp các bộ mã hóa hình ảnh với các LLM này (Touvron et al. 2023; Chiang et al. 2023) đã dẫn đến những bước tiến đáng kể trong lĩnh vực MLLM đa phương thức, tạo ra các khung như BLIP-2 (Li et al. 2023b), MiniGPT-4 (Zhu et al. 2023b), LLaV A (Liu et al. 2023b), InstructBLIP (Dai et al. 2023) và InternLM-XComposer (Zhang et al. 2023). Các khung này đã thúc đẩy sự phát triển nhanh chóng của các tác vụ đa phương thức hình ảnh-văn bản, thể hiện khả năng ấn tượng trong đối thoại hình ảnh-văn bản.

Các mô hình đa phương thức truyền thống tuân theo một quy trình đào tạo hai giai đoạn. Giai đoạn đầu bao gồm việc đào tạo mô hình với các cặp hình ảnh-văn bản để tăng cường sự liên kết đặc trưng giữa hai phương thức. Giai đoạn tiếp theo sử dụng dữ liệu điều chỉnh lệnh hướng dẫn đa phương thức chất lượng cao để tăng cường khả năng tuân thủ lệnh hướng dẫn của mô hình, từ đó cải thiện phản hồi của nó đối với các câu hỏi của người dùng. Tuy nhiên, so với lượng lớn dữ liệu tiền đào tạo đa phương thức có sẵn (Schuhmann et al. 2022; Sharma et al. 2018; Changpinyo et al. 2021; He et al. 2023), việc thu thập dữ liệu điều chỉnh lệnh hướng dẫn chất lượng cao tương đối khó khăn hơn. Dữ liệu tinh chỉnh đa phương thức chất lượng cao hiện tại (Liu et al. 2023b; Li et al. 2023a) chủ yếu được tạo ra dựa trên GPT-4 chỉ sử dụng ngôn ngữ (OpenAI 2023b) như được minh họa trong Hình 1-(a). Phương pháp này đòi hỏi phải có chú thích thủ công tốn kém và hạn chế việc thiết kế câu hỏi cũng như các phản hồi được tạo ra chỉ dựa trên thông tin đã được chú thích. Do đó, nếu câu hỏi được đặt ra không nằm trong thông tin được chú thích này, GPT-4 không thể phản hồi. Phương pháp này cũng mất đi thông tin chi tiết trong hình ảnh đối với những câu hỏi có thể trả lời.

Để giải quyết vấn đề này, các nhà nghiên cứu đã bắt đầu xem xét việc tạo dữ liệu với các Mô hình Thị giác-Ngôn ngữ (VLM) (Zhu et al. 2023a; You et al. 2023; Zhang et al. 2023) vì VLM đã tiếp xúc với một lượng lớn các cặp hình ảnh-văn bản trong giai đoạn tiền đào tạo và vốn có chứa một kho kiến thức phong phú. Hiện tại, các MLLM có thể truy cập ít mạnh mẽ hơn so với các đối tác LLM của chúng. Chúng thường tạo ra các phản hồi không đầy đủ và tạo ra thông tin sai lệch, ví dụ như ảo giác. Tuy nhiên, các phương pháp tiếp cận hiện tại cố gắng tạo dữ liệu bằng VLM mà không xem xét cách đảm bảo chất lượng của dữ liệu được tạo ra hoặc xác thực nó một cách thử nghiệm.

Ngược lại với các phương pháp nêu trên, chúng tôi đề xuất Tạo Lệnh Hướng Dẫn Trực Quan và Hiệu Chỉnh, một phương pháp mới để tạo dữ liệu lệnh hướng dẫn chất lượng cao. Phương pháp này, dựa trên các mô hình thị giác-ngôn ngữ hiện có, hướng dẫn mô hình tạo ra các cặp câu hỏi-trả lời thị giác-ngôn ngữ đa dạng trên các hình ảnh mới thông qua việc tinh chỉnh dữ liệu lệnh hướng dẫn ban đầu. Khả năng tạo dữ liệu đa dạng xuất phát từ thực tế là cả bộ mã hóa hình ảnh và mô hình ngôn ngữ lớn đều đã được tinh chỉnh trên các bộ dữ liệu rộng lớn, bao gồm khả năng hiểu hình ảnh phong phú và khả năng ngôn ngữ logic. Tuy nhiên, chúng tôi nhận thấy rằng dữ liệu được tạo trực tiếp từ các lệnh hướng dẫn được cung cấp gặp phải các vấn đề ảo giác nghiêm trọng, đây là một vấn đề phổ biến gây khó khăn cho các mô hình đa phương thức lớn (Peng et al. 2023b; Liu et al. 2023a; Zhao et al. 2023; Huang et al. 2023). May mắn thay, mô-đun hiệu chỉnh lệnh hướng dẫn trực quan của chúng tôi có thể giảm đáng kể các hiện tượng ảo giác của mô hình thông qua các cập nhật lặp lại. Những đóng góp chính của nghiên cứu này bao gồm:

• Chúng tôi giới thiệu Tạo Lệnh Hướng Dẫn Trực Quan và Hiệu Chỉnh (VIGC), một khung có khả năng tự động tạo ra các bộ dữ liệu tinh chỉnh lệnh hướng dẫn hình ảnh-văn bản chất lượng cao. Khung VIGC bao gồm hai mô-đun con: Tạo Lệnh Hướng Dẫn Trực Quan (VIG) và Hiệu Chỉnh Lệnh Hướng Dẫn Trực Quan (VIC). Cụ thể, VIG tạo ra các cặp câu hỏi-trả lời trực quan ban đầu, trong khi VIC giảm thiểu ảo giác của mô hình và có được dữ liệu chất lượng cao thông qua chiến lược cập nhật Q-Former Lặp lại (IQF).

• Chúng tôi phát hành một loạt bộ dữ liệu1(He et al. 2022) được tạo ra bằng VIGC, bao gồm 36.781 VIGC-LLaV A-COCO và khoảng 1,8 triệu VIGC-LLaV A-Objects365, cho nghiên cứu về các mô hình đa phương thức lớn. Theo hiểu biết của chúng tôi, đây là bộ dữ liệu tinh chỉnh lệnh hướng dẫn đa phương thức đầu tiên được tạo ra tự động bởi một MLLM.

• Chúng tôi đã tiến hành các thử nghiệm mở rộng trên dữ liệu được tạo ra. Khi được đào tạo kết hợp với dữ liệu được tạo bởi VIGC, hiệu suất của mô hình LLaV A-7B đã được cải thiện đáng kể, thậm chí vượt qua hiệu suất của mô hình LLaV A-13B. Hơn nữa, trên các bộ dữ liệu đánh giá đa phương thức chính như MMBench, OKVQA và A-OKVQA, các mô hình được đào tạo với dữ liệu VIGC đồng loạt thể hiện sự nâng cao hiệu suất.

1https://opendatalab.com/OpenDataLab/VIGC-InstData

Công trình Liên quan
LLM tuân thủ lệnh hướng dẫn
Lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP) đã được định hình đáng kể bởi sự xuất hiện và phát triển của các mô hình ngôn ngữ lớn (LLM), bao gồm nhưng không giới hạn ở GPT-3 (Brown et al. 2020), PaLM (Chowdhery et al. 2022), T5 (Raffel et al. 2020) và OPT (Zhang et al. 2022). Các mô hình này, được trang bị dữ liệu đào tạo rộng lớn và các kỹ thuật tối ưu hóa tinh vi, đã thể hiện hiệu suất đáng chú ý trên các tác vụ khác nhau. Tuy nhiên, một thách thức đáng chú ý vẫn tồn tại trong khả năng tuân thủ lệnh hướng dẫn hiệu quả của chúng, thường dẫn đến kết quả không tối ưu trong các ứng dụng thực tế đa dạng. Những nỗ lực để giải quyết vấn đề này đã dẫn đến việc giới thiệu các bộ dữ liệu tinh chỉnh lệnh hướng dẫn khác nhau. Các mô hình nâng cao, như InstructGPT (Ouyang et al. 2022), ChatGPT (OpenAI 2023a), FLAN-T5 (Chung et al. 2022), FLAN-PaLM (Chung et al. 2022) và OPT-IML (Iyer et al. 2022), đã được phát triển để cải thiện khả năng học zero-shot và few-shot, chủ yếu bằng cách học cách ánh xạ các lệnh hướng dẫn đến các đầu ra mong đợi tương ứng. Mặc dù có những tiến bộ này, việc tạo ra các bộ dữ liệu lệnh hướng dẫn thường dựa vào các tác vụ NLP có sẵn, điều này hạn chế khả năng tổng quát hóa của chúng. Để tăng cường chất lượng và tính đa dạng của các lệnh hướng dẫn, Wang et al. (Wang et al. 2022) giới thiệu SELF-INSTRUCT, một phương pháp sử dụng dữ liệu lệnh hướng dẫn được tạo ra để nâng cao hiệu suất của LLM. Trong khi các phương pháp này đã có những bước tiến đáng kể trong việc tăng cường khả năng tuân thủ lệnh hướng dẫn của các mô hình ngôn ngữ, chúng thể hiện một hạn chế tiêu chuẩn là chúng không thể được tổng quát hóa trực tiếp cho dữ liệu đa phương thức.

Điều chỉnh Lệnh hướng dẫn Đa phương thức
So với việc tạo các bộ dữ liệu tinh chỉnh lệnh hướng dẫn ngôn ngữ, việc xây dựng các bộ dữ liệu tinh chỉnh lệnh hướng dẫn đa phương thức đòi hỏi sự hiểu biết kỹ lưỡng về nội dung hình ảnh và phát triển văn bản tương ứng. MiniGPT-4 sử dụng một mô hình liên kết đặc trưng để diễn giải bộ dữ liệu CC (Sharma et al. 2018; Changpinyo et al. 2021), sử dụng ChatGPT cho việc lọc ban đầu và cuối cùng tuyển chọn 3.500 cặp hình ảnh-văn bản cho việc tinh chỉnh mô hình. Tuy nhiên, phương pháp này gặp hạn chế về tính đa dạng và khối lượng của lệnh hướng dẫn. Ngược lại, LLaV A đề xuất một phương pháp tiếp cận sáng tạo dựa trên GPT-4 chỉ sử dụng ngôn ngữ (OpenAI 2023b) để tạo dữ liệu lệnh hướng dẫn đa phương thức từ thông tin bao gồm mô tả chú thích và dữ liệu mục tiêu. Mặc dù phương pháp này tạo ra dữ liệu chất lượng cao, nhưng nó đòi hỏi chú thích thủ công cho mỗi mô tả chú thích, thông tin mục tiêu và câu hỏi, điều này vốn dĩ hạn chế khả năng mở rộng. Để mở rộng dữ liệu trên một loạt các tác vụ toàn diện hơn, InstructBLIP tiên phong trong phương pháp xây dựng mẫu Lệnh hướng dẫn, chuyển đổi 26 bộ dữ liệu duy nhất thành dữ liệu tinh chỉnh lệnh hướng dẫn và đạt được kết quả ấn tượng trên một số tác vụ. Đồng thời, MIMIC (Li et al. 2023a) tập hợp các bộ dữ liệu tinh chỉnh lệnh hướng dẫn quy mô lớn hơn.

Tuy nhiên, tất cả các bộ dữ liệu này đều yêu cầu sự can thiệp của con người dưới dạng chú thích, và tính đa dạng của chúng vốn dĩ bị hạn chế bởi dữ liệu hiện có. Ngược lại, nghiên cứu của chúng tôi nhằm đề xuất một phương pháp tạo dữ liệu tinh chỉnh lệnh hướng dẫn tự hướng dẫn, được điều khiển bởi mô hình, có khả năng tạo dữ liệu tinh chỉnh chất lượng cao phù hợp với bất kỳ hình ảnh mới nào.

Tạo Câu hỏi Trực quan
Tạo Câu hỏi Trực quan (VQG) nhằm tạo ra các câu hỏi liên quan dựa trên các hình ảnh được cung cấp, điều này đặt ra những thách thức đáng kể do nhu cầu về tính đa dạng, tự nhiên và thu hút. Mostafazadeh et al. (Mostafazadeh et al. 2016) đề xuất nhiệm vụ Tạo Câu hỏi Trực quan (VQG) và cố gắng thiết lập một khung VQG nền tảng, sử dụng cả phương pháp dựa trên truy xuất và phương pháp tạo sinh. iQAN (Li et al. 2018) sau đó đề xuất một mạng thống nhất, có thể đảo ngược để giải quyết cả tác vụ VQA và VQG, cho phép cả truy xuất câu trả lời và tạo câu hỏi từ hình ảnh. Các mô hình hướng dẫn như Hướng dẫn Tạo Câu hỏi Trực quan (Vedd et al. 2021) cũng đã đóng góp đáng kể cho lĩnh vực này.

Bài báo này đề xuất Tạo Lệnh Hướng Dẫn Trực Quan và Hiệu Chỉnh, một mô hình tạo ra nội dung liên quan đến hình ảnh, tương tự như VQG. Khác với công trình hiện có, phương pháp của chúng tôi giới thiệu một lớp phức tạp bổ sung bằng cách phát triển các câu hỏi đa dạng và cung cấp câu trả lời phù hợp dựa trên các danh mục yêu cầu khác nhau. Tận dụng kiến thức rộng lớn của các mô hình ngôn ngữ lớn, đầu ra của mô hình của chúng tôi vượt trội so với các tác vụ VQG truyền thống, thường bị hạn chế bởi kích thước mẫu đào tạo của chúng.

Phương pháp
Bài báo này tập trung vào việc tận dụng sức mạnh của các mô hình thị giác-ngôn ngữ hiện có để tạo ra dữ liệu tuân thủ lệnh hướng dẫn đa phương thức một cách tự động. Phương pháp tiếp cận được đề xuất tạo điều kiện cho việc tạo ra các bộ dữ liệu tinh chỉnh mạnh mẽ và đa dạng, loại bỏ yêu cầu can thiệp thủ công chuyên sâu. Tuy nhiên, việc sử dụng các mô hình đa phương thức hiện có để đạt được mục tiêu này đặt ra những thách thức đáng kể. Để giảm thiểu những thách thức này, chúng tôi giới thiệu một khung tự hướng dẫn có tên VIGC. Được hướng dẫn bởi dữ liệu tinh chỉnh hiện có, khung này có thể tạo ra dữ liệu mới chất lượng cao hơn và đa dạng hơn, như được mô tả trong Hình 2.

Xây dựng Lệnh hướng dẫn Ban đầu
Ngược lại với các lệnh hướng dẫn ngôn ngữ, có thể được tạo ra một cách dễ dàng bởi các mô hình ngôn ngữ độc lập (Peng et al. 2023a; Wang et al. 2022), việc xây dựng các lệnh hướng dẫn đa phương thức thị giác-văn bản đòi hỏi sự hiểu biết chi tiết về nội dung trực quan, cũng như khả năng đặt ra các câu hỏi liên quan và cung cấp các câu trả lời chính xác dựa trên nội dung thực tế của hình ảnh. Tuy nhiên, các mô hình đa phương thức hiện có thiếu khả năng tạo ra dữ liệu lệnh hướng dẫn thị giác-ngôn ngữ trực tiếp. Để vượt qua hạn chế này, chúng tôi khai thác dữ liệu tinh chỉnh lệnh hướng dẫn có sẵn và xây dựng các mẫu lệnh hướng dẫn bổ sung, từ đó tạo điều kiện cho việc tạo ra dữ liệu lệnh hướng dẫn tự động.

Phương pháp được đề xuất của chúng tôi có thể áp dụng phổ biến để tạo ra các loại dữ liệu tinh chỉnh lệnh hướng dẫn đa phương thức hình ảnh-văn bản khác nhau. Để làm rõ phương pháp của chúng tôi, chúng tôi ví dụ hóa nó bằng cách sử dụng việc tạo ra các lệnh hướng dẫn dữ liệu kiểu LLaV A. Cụ thể, chúng tôi xây dựng các mẫu lệnh hướng dẫn bao gồm đối thoại, mô tả chi tiết và lý luận phức tạp, tuân theo việc phân loại các loại dữ liệu tinh chỉnh lệnh hướng dẫn như được mô tả trong LLaV A. Hình 3 trình bày các ví dụ về ba loại mẫu lệnh hướng dẫn này, về cơ bản là không phức tạp, chủ yếu yêu cầu, "tạo ra các cặp câu hỏi-trả lời loại T dựa trên nội dung hình ảnh." Về mặt lý thuyết, nếu một mô hình có thể tuân thủ những mô tả lệnh hướng dẫn này sau khi đào tạo, nó sẽ thành thạo trong việc tạo ra các cặp câu hỏi-trả lời.

Với các mẫu lệnh hướng dẫn và dữ liệu điều chỉnh lệnh hướng dẫn trực quan hiện có (tức là các cặp Câu hỏi-Trả lời trong LLaV A), chúng tôi xây dựng một bộ dữ liệu điều chỉnh lệnh hướng dẫn VIG toàn diện như sau:

--- TRANG 4 ---
Hội thoại: Tạo một câu hỏi dựa trên nội dung của hình ảnh đã cho và sau đó trả lời nó.
Mẫu Lệnh hướng dẫn VIG
Mẫu Lệnh hướng dẫn VIC
……
Mô tả Chi tiết: Tạo một câu hỏi để mô tả nội dung hình ảnh một cách chi tiết và sau đó trả lời nó.
Lý luận Phức tạp: Dựa trên hình ảnh đã cho, tạo ra một câu hỏi lý luận sâu sắc và sau đó trả lời nó.
Câu hỏi: {Câu hỏi}    Trả lời:

Hình 3: Ví dụ về mẫu tương ứng với việc điều chỉnh lệnh hướng dẫn trong các mô-đun con VIG và VIC.

TV IG = (Xi, It, Qt
i, At
i)Nt(1)
trong đó i∈ {1,2, ..., N t},Nt biểu thị loại lệnh hướng dẫn, như hội thoại, mô tả chi tiết, v.v. Xi đại diện cho một hình ảnh RGB, Ii đại diện cho một lệnh hướng dẫn tương ứng với một loại cụ thể t, Qt
i là một câu hỏi liên quan đến hình ảnh Xi trong bối cảnh của lệnh hướng dẫn It, và At
i là câu trả lời cho câu hỏi Qt
i. Mục tiêu của chúng tôi là tận dụng bộ dữ liệu này để đào tạo các mô hình mà, khi được đưa ra một lệnh hướng dẫn cụ thể It, có thể tạo ra các cặp câu hỏi-trả lời tương ứng cho một hình ảnh đã cho, tuân theo loại lệnh hướng dẫn được chỉ định. Hình 2 cung cấp các minh họa về bộ dữ liệu lệnh hướng dẫn ban đầu.

Khác với VIG, lệnh hướng dẫn VIC sử dụng một hình ảnh và một truy vấn làm đầu vào cho quá trình tinh chỉnh của nó, với mục tiêu tạo ra các phản hồi chính xác. Bộ dữ liệu cho lệnh hướng dẫn VIC được trình bày dưới đây:

TV IC = (Xi, Qt
i, At
i)Nt(2)

Tạo Lệnh hướng dẫn Trực quan
Phù hợp với các mô hình đa phương thức phổ biến hiện tại như MiniGPT-4 (Zhu et al. 2023b) và InstructBLIP (Dai et al. 2023), kiến trúc của VIGC được đề xuất có thể được phân tách thành bốn thành phần chính: bộ mã hóa hình ảnh (ViT) (Fang et al. 2023), mô hình ngôn ngữ lớn (Vicuna) (Chiang et al. 2023), Q-Former (Li et al. 2023b) để trích xuất đặc trưng hình ảnh, và phép chiếu Kết nối Đầy đủ (FC) để điều hòa các đặc trưng thị giác-ngôn ngữ. Về mặt chức năng, mô hình có thể được phân đoạn thêm thành hai mô-đun con khác biệt: mô-đun Tạo Lệnh hướng dẫn Trực quan (VIG) và mô-đun Hiệu chỉnh Lệnh hướng dẫn Trực quan (VIC). Điều quan trọng cần nhấn mạnh là hai mô-đun con này chia sẻ các tham số mạng, điểm khác biệt chính là loại dữ liệu được sử dụng để đào tạo.

Mục tiêu chính của mô-đun VIG là tự động tạo ra các cặp câu hỏi-trả lời trực quan liên quan tương ứng với một lệnh hướng dẫn cụ thể cho bất kỳ hình ảnh nào được cho. Hình 2 minh họa quy trình mà mô-đun VIG tuân theo trong giai đoạn đào tạo. Trong giai đoạn đào tạo, mô-đun VIG chọn ngẫu nhiên một hình ảnh, sau đó được xử lý qua bộ mã hóa hình ảnh. Nó tạo ra một tập hợp các nhúng đặc trưng hình ảnh cố định. Mô-đun Q-Former, được thiết kế có mục đích để nhận biết thông tin lệnh hướng dẫn, tinh chỉnh thêm các đặc trưng hình ảnh này. Ở giai đoạn này, mô hình sử dụng các truy vấn hình ảnh có thể học được thực hiện các hoạt động tự chú ý kết hợp với lệnh hướng dẫn. Hoạt động này được theo sau bởi một giai đoạn chú ý chéo với các nhúng hình ảnh. Cơ chế này thúc đẩy các đặc trưng hình ảnh tập trung vào thông tin lệnh hướng dẫn, từ đó tăng cường tính liên quan và độ chính xác của chúng trong bối cảnh của nhiệm vụ được giao. Sau giai đoạn chú ý chéo, các đặc trưng được tinh chỉnh được truyền qua một lớp ánh xạ FC, một bước quan trọng để liên kết các đặc trưng hình ảnh với các đối tác ngôn ngữ của chúng, từ đó đảm bảo sự tích hợp liền mạch của các đặc trưng hình ảnh và ngôn ngữ. Sau đó, các đặc trưng được liên kết với lệnh hướng dẫn được mô hình ngôn ngữ tiếp thu. Quá trình này hướng dẫn mô hình tạo ra các kết quả dự đoán. Cụ thể, mục tiêu trong bối cảnh này là tạo ra các câu hỏi và câu trả lời trực quan có liên kết nội tại với nội dung của hình ảnh Xi, bản chất của nó được xác định bởi lệnh hướng dẫn. Chúng tôi sử dụng hàm mất mát hồi quy tự động gốc vốn có của mô hình ngôn ngữ lớn. Phương pháp này hướng dẫn mô hình tạo ra các câu phù hợp với các cặp câu hỏi-trả lời được cung cấp trong tập đào tạo.

Hiệu chỉnh Lệnh hướng dẫn Trực quan
Trong quá trình khám phá được thực hiện cho nghiên cứu này, chúng tôi phát hiện ra rằng các mô hình đa phương thức hiện có (Liu et al. 2023b), (Dai et al. 2023), giống như các mô hình ngôn ngữ (Radford et al. 2018, 2019; Brown et al. 2020; OpenAI 2023b,a), thường thể hiện các vấn đề ảo giác. Hiện tượng ảo giác này cũng có mặt trong dữ liệu được tạo ra bởi VIG, đặc biệt là trong các trường hợp mô tả mở rộng. Chúng tôi quy kết điều này cho xu hướng của các mô hình đa phương thức dần dần dựa vào văn bản câu trả lời hiện tại trong giai đoạn tạo câu trả lời, từ đó dần dần bỏ qua thông tin hình ảnh và do đó dẫn đến việc mô tả các mục tiêu không có mặt trong hình ảnh. Để loại bỏ hiện tượng ảo giác trong dữ liệu được tạo ra và đảm bảo rằng các nhiệm vụ hạ lưu dựa trên dữ liệu này không bị ô nhiễm, chúng tôi đặc biệt giới thiệu một mô-đun hiệu chỉnh lệnh hướng dẫn để cập nhật các câu trả lời và giảm sự xuất hiện của các ảo giác.

Để sử dụng VIC hiệu quả, các hành động cụ thể cần được thực hiện trong cả giai đoạn đào tạo và suy luận của mô hình:

Trong giai đoạn đào tạo: Mục tiêu của giai đoạn VIG là tạo ra các cặp câu hỏi-trả lời trực quan tương ứng khi được đưa ra một lệnh hướng dẫn. Ngược lại, mục tiêu của giai đoạn đào tạo VIC là cung cấp cho mô hình một Câu hỏi, từ đó hướng dẫn mô hình tập trung vào việc trích xuất các đặc trưng liên quan đến câu hỏi/văn bản đầu vào trong quá trình trích xuất đặc trưng Q-Former. Những đặc trưng này đặt nền tảng cho các câu trả lời tiếp theo.

Trong giai đoạn suy luận: Sau khi đào tạo mô hình bằng phương pháp VIC nêu trên, nó có thể lấy các câu hỏi từ các cặp câu hỏi-trả lời được tạo ra bởi VIG làm đầu vào và tái tạo các câu trả lời. Vì mô hình chú trọng nhiều hơn đến câu hỏi khi xây dựng các phản hồi, các kết quả được tạo ra thường chính xác hơn. Hơn nữa, chúng tôi lặp lại quá trình cập nhật đặc trưng Q-Former này, được gọi là Q-Former Lặp lại (IQF), như được minh họa trong giai đoạn suy luận VIGC trong Hình 2. Trước khi triển khai mô-đun VIC, chúng tôi ban đầu tạo ra câu hỏi ban đầu (Q) và câu trả lời (A) bằng VIG. Trong lần lặp đầu tiên, chúng tôi sử dụng Lệnh hướng dẫn và Câu hỏi làm đầu vào để xuất ra các câu trả lời A1 và ¯A1, trong đó A1 đại diện cho câu đầu tiên của câu trả lời và ¯A1 biểu thị tất cả nội dung sau câu đầu tiên. Trong lần lặp thứ hai, chúng tôi nhập Lệnh hướng dẫn, Câu hỏi và câu trả lời A1 từ bước trước để dự đoán A2, và quá trình này tiếp tục lặp lại cho đến khi gặp ký hiệu kết thúc. Hiệu quả của phương pháp lặp này chủ yếu do việc cập nhật liên tục các đặc trưng hình ảnh với thông tin văn bản mới nhất, làm cho các kết quả tiếp theo chính xác hơn. Tuy nhiên, cần lưu ý rằng mặc dù phương pháp này rất có lợi cho việc cung cấp các mô tả chi tiết về nội dung hình ảnh, hiệu quả của nó đối với các nhiệm vụ đối thoại và nhiệm vụ suy luận tương đối hạn chế. Điều này là do các nhiệm vụ đối thoại thường bao gồm các câu đơn, và nội dung tiếp theo trong các nhiệm vụ suy luận không phụ thuộc nhiều vào thông tin hình ảnh.

Thí nghiệm
Bộ dữ liệu
Dữ liệu Đào tạo. Chúng tôi đã đào tạo mạng VIGC bằng hai loại dữ liệu tinh chỉnh lệnh hướng dẫn thị giác-ngôn ngữ. Loại đầu tiên, được đại diện bởi bộ dữ liệu LLaV A (Liu et al. 2023b), được tuyển chọn thủ công và kết hợp với GPT-4 chỉ sử dụng ngôn ngữ (OpenAI 2023b) cho các mô hình đa phương thức. Nó bao gồm 150K mẫu đào tạo, được phân chia thành đối thoại đơn giản (57.669 mẫu), mô tả chi tiết (23.240 mẫu) và dữ liệu thị giác-ngôn ngữ lý luận phức tạp (76.803 mẫu). Bộ dữ liệu này bao gồm nhiều khía cạnh khác nhau của đối thoại đa phương thức, bao gồm nhận dạng danh mục, đếm, nhận dạng hành động và nhận dạng cảnh. Các mô tả chi tiết đòi hỏi quan sát hình ảnh cẩn thận và chi tiết toàn diện, trong khi các nhiệm vụ lý luận phức tạp yêu cầu suy luận sâu và tích hợp kiến thức bên ngoài. Loại dữ liệu thứ hai là dữ liệu tinh chỉnh lệnh hướng dẫn đa phương thức được lấy từ các bộ dữ liệu hình ảnh-văn bản có sẵn công khai. Cụ thể, chúng tôi đã sử dụng các bộ dữ liệu OKVQA (Marino et al. 2019) và A-OKVQA (Schwenk et al. 2022), như được sử dụng trong InstructBLIP (Dai et al. 2023), để đào tạo VIGC. Những bộ dữ liệu này, đòi hỏi kiến thức bên ngoài rộng lớn, là lý tưởng để đánh giá khả năng của VIGC.

Dữ liệu Suy luận. Sau khi đào tạo mạng VIGC, chúng tôi đã tạo ra dữ liệu tinh chỉnh cho lệnh hướng dẫn đa phương thức bằng cách sử dụng các bộ dữ liệu hình ảnh. Chúng tôi sử dụng hai bộ dữ liệu khác biệt, COCO (Lin et al. 2014) và Objects365 (Shao et al. 2019), để đánh giá hiệu quả của VIGC trong việc xử lý dữ liệu trong cùng một lĩnh vực hình ảnh hoặc lĩnh vực hình ảnh khác nhau. Bộ dữ liệu COCO phục vụ như nền tảng để xây dựng các bộ dữ liệu LLaV A, OKVQA và A-OKVQA. Điều quan trọng cần nhấn mạnh là trong giai đoạn tạo dữ liệu, chúng tôi cố ý bỏ qua bất kỳ hình ảnh nào đã được bao gồm trước đó trong tập thử nghiệm để đảm bảo tính công bằng và hiệu quả của đánh giá.

Chi tiết Triển khai
Trong giai đoạn đào tạo VIGC, chúng tôi sử dụng mô hình tiền đào tạo giai đoạn đầu tiên của MiniGPT-4 (Zhu et al. 2023b) làm nguồn tham số ban đầu. Điều này đảm bảo rằng mô hình ban đầu không kết hợp dữ liệu tinh chỉnh lệnh hướng dẫn bổ sung để đào tạo, từ đó bảo tồn tính công bằng của xác thực nhiệm vụ hạ lưu. Mô hình này bao gồm ViT-G/14 từ EV A-CLIP (Fang et al. 2023), Q-Former (Li et al. 2023b), và một lớp chiếu tuyến tính. Các mô hình ngôn ngữ được sử dụng là Vicuna7B và Vicuna13B (Chiang et al. 2023). Đáng chú ý là, như được minh họa trong Hình 1, Q-Former của chúng tôi được thiết kế để nhận văn bản Lệnh hướng dẫn hoặc Câu hỏi đồng thời, điều này rất quan trọng cho việc hiệu chỉnh lặp lại trong VIC. Do đó, chúng tôi sử dụng Q-Former từ BLIP2-FlanT5 XXL(Li et al. 2023b) làm tham số ban đầu cho Q-Former. Chúng tôi đặt tên cho mô hình mạng này là MiniGPT-4+. Trong quá trình đào tạo, chỉ có các tham số của Q-Former và lớp chiếu tuyến tính được tinh chỉnh, trong khi các tham số của mô hình ngôn ngữ và hình ảnh vẫn không đổi. Việc đào tạo được thực hiện trong 10 epoch, với hiệu suất của mô hình được xác thực sau mỗi epoch. Mô hình thể hiện hiệu suất tốt nhất sau đó được chọn để tạo dữ liệu.

Về kích thước batch, chúng tôi sử dụng 64 cho cả mô hình 7B và 13B. Toàn bộ quá trình đào tạo, được thực hiện trên 8 GPU A100 (80GB), hoàn thành trong khoảng 10 giờ.

Dữ liệu và Đánh giá LLaVA
Phân tích Bộ dữ liệu. Trong việc tạo ra một tập hợp dữ liệu giống LLaV A đa dạng hơn, mô hình VIGC được đào tạo bằng cách sử dụng kết hợp dữ liệu LLaV A-150K và ba loại mẫu lệnh hướng dẫn. Trong giai đoạn suy luận, chúng tôi sử dụng hình ảnh từ tập đào tạo COCO 2017, cố ý loại trừ những hình ảnh đã được bao gồm trong bộ dữ liệu LLaV A. Điều này dẫn đến việc lựa chọn tổng cộng 36.781 hình ảnh ban đầu, phục vụ như nền tảng cho việc tạo dữ liệu lệnh hướng dẫn; chúng tôi gọi dữ liệu này là coco-extra, phục vụ như dữ liệu bổ sung mặc định được sử dụng cho việc đào tạo mô hình trong quá trình đánh giá.

Dựa trên dữ liệu nêu trên, mạng VIG tạo ra các câu hỏi và câu trả lời ban đầu đa dạng. Sau đó, mạng VIC tinh chỉnh các đầu ra bằng cách lấy các câu hỏi và các câu trả lời hiện có làm đầu vào thông qua hoạt động Q-Former Lặp lại (IQF), từ đó tạo ra các phản hồi chất lượng cao hơn. Hình 4 minh họa ba danh mục dữ liệu được tạo ra thông qua quá trình VIGC:

• Hội thoại: Các câu hỏi thường cụ thể, dẫn đến các phản hồi ngắn gọn và rõ ràng.
• Mô tả Chi tiết: Các câu hỏi tương đối cố định và xoay quanh việc mô tả nội dung của hình ảnh. Điều này đòi hỏi mô hình quan sát rõ ràng tất cả các mục tiêu trong hình ảnh. Quan sát thấy rằng các mô tả chi tiết được tạo ra trực tiếp từ Tạo Lệnh hướng dẫn Trực quan (VIG) chứa đầy nhiều ảo tưởng. Tuy nhiên, sau khi áp dụng Hiệu chỉnh Lệnh hướng dẫn Trực quan (VIC), những hiện tượng ảo tưởng này đã giảm đáng kể.

--- TRANG 6 ---
Trả lời (VIG): Con mèo đang ngồi hoặc đứng trước một chiếc tivi, quan sát một con chim trên màn hình.
Trả lời (VIC): Con mèo đang ngồi hoặc đứng trước một màn hình tivi lớn, quan sát một con chim bay trên màn hình.

Trả lời (VIG): Hình ảnh có một con mèo ngồi trước một màn hình tivi lớn, chăm chú xem một chương trình có hai con chim trên màn hình. Chiếc tivi được đặt ở trung tâm của cảnh, thu hút toàn bộ sự chú ý của con mèo. Ngoài chiếc tivi, còn có một số quyển sách rải rác khắp phòng. Một số được đặt gần tivi, trong khi những cuốn khác nằm gần các cạnh của cảnh. Một chiếc bát nhỏ cũng có thể được nhìn thấy trong phòng, có thể được sử dụng để đựng thức ăn hoặc các vật dụng khác.

Trả lời (VIC): Hình ảnh có một con mèo ngồi trước một màn hình tivi lớn hiển thị cảnh các con chim bay, thu hút sự chú ý của con mèo. Con mèo đang ngồi trên một chiếc bàn hoặc quầy trước tivi, và có vẻ như nó đang mải mê xem những con chim trên màn hình. Tư thế và biểu cảm khuôn mặt của con mèo cho thấy rằng nó bị hấp dẫn bởi những hình ảnh chuyển động trên tivi. Sự có mặt của con mèo trước tivi là một cảnh tượng phổ biến trong nhiều gia đình, vì mèo được biết đến là bị cuốn hút bởi những hình ảnh chuyển động, đặc biệt là những hình ảnh liên quan đến các loài động vật khác. Trong trường hợp này, những con chim bay trên màn hình có thể đã thu hút...

Trả lời (VIG): Con mèo có thể bị thu hút bởi màn hình TV do sự hiện diện của một con chim trên màn hình. Mèo là những sinh vật tò mò tự nhiên, và chúng thường bị thu hút bởi các vật thể chuyển động, đặc biệt là những vật thể giống con mồi. Trong trường hợp này, con chim trên màn hình TV có thể đang làm mê hoặc sự chú ý của con mèo, khiến nó trở thành một kích thích thị giác thú vị và hấp dẫn đối với con mèo. Ngoài ra, mèo cũng có thể bị thu hút bởi tính chất đầy màu sắc và năng động của...

Trả lời (VIC): Con mèo có thể bị thu hút bởi màn hình TV vì nó đang hiển thị hình ảnh của một con chim, là con mồi tự nhiên của mèo. Mèo có bản năng săn mồi và rượt đuổi các vật thể chuyển động mạnh mẽ, đặc biệt là những vật thể giống con mồi tiềm năng. Trong trường hợp này, con chim trên màn hình TV kích hoạt bản năng săn mồi của con mèo, khiến nó tò mò và bị thu hút đến màn hình. Ngoài ra, mèo thường bị cuốn hút bởi những hình ảnh sáng và đầy màu sắc...

Câu hỏi: Bạn có thể mô tả chi tiết các yếu tố của bức tranh được cung cấp không?
Câu hỏi: Con mèo đang làm gì trong hình ảnh?
Câu hỏi: Tại sao con mèo có thể bị thu hút bởi màn hình TV?

Hội thoại
Mô tả Chi tiết
Lý luận Phức tạp

Hình 4: Các Lệnh hướng dẫn được tạo ra dựa trên VIGC được đề xuất.

[Bảng 1 chứa các số liệu đánh giá so sánh - giữ nguyên cấu trúc bảng]

• Lý luận Phức tạp: Các câu hỏi được đặt ra đòi hỏi sự tích hợp kiến thức bên ngoài và việc áp dụng các kỹ năng lý luận logic tinh vi.

Nhìn chung, chất lượng của các cặp câu hỏi-trả lời thị giác-ngôn ngữ được tạo ra tự động bởi mô hình đã vượt quá những kỳ vọng ban đầu của chúng tôi. Chúng tôi khẳng định rằng kiến thức phong phú mới này vốn tồn tại trong chính mô hình ngôn ngữ, và chúng tôi chỉ đơn giản là sử dụng việc tinh chỉnh lệnh hướng dẫn đa phương thức để chưng cất kiến thức này vào dữ liệu đa phương thức mới.

Đánh giá Bộ dữ liệu. Dựa trên dữ liệu được tạo ra, chúng tôi đã tiến hành các thí nghiệm cắt bỏ chi tiết trên LLaV A-7B để xác minh sự cải thiện hiệu suất của mô hình sau khi đào tạo với dữ liệu được tạo ra. Phương pháp đánh giá được sử dụng ở đây là đánh giá định lượng do LLaV A đề xuất, trong đó GPT-4 đánh giá chất lượng phản hồi của mô hình đối với các câu hỏi đánh giá được cho, có thể được hiểu là điểm số tương đối so với GPT-4. LLaV A cung cấp 30 hình ảnh thử nghiệm, mỗi hình chứa ba loại câu hỏi, tổng cộng 90 câu hỏi.

Bảng 1 trình bày kết quả của việc bổ sung ba loại dữ liệu được tạo ra vào bộ dữ liệu LLaV A-150K gốc, sau đó tinh chỉnh mô hình giai đoạn đầu tiên của LLaV A với các lệnh hướng dẫn. Việc bao gồm dữ liệu lệnh hướng dẫn được tạo ra trực tiếp từ VIG trong giai đoạn đào tạo đã được chứng minh là có lợi. Chúng tôi quan sát thấy một sự cải thiện nhỏ khi thêm dữ liệu mô tả chi tiết được tạo ra bởi VIG, điều này có thể được quy cho những ảo tưởng nghiêm trọng có mặt trong dữ liệu này. Ngược lại, việc kết hợp dữ liệu hội thoại và dữ liệu lý luận phức tạp đã dẫn đến những cải thiện hiệu suất đáng kể.

Tinh chỉnh thêm dữ liệu bằng VIC và sau đó đào tạo mô hình với dữ liệu hội thoại được tăng cường, dữ liệu mô tả chi tiết và dữ liệu lý luận phức tạp đã mang lại những cải thiện bổ sung. Các số liệu hiệu suất đã đạt đến 84.0%, 85.8% và 83.3% tương ứng. Những kết quả này nhấn mạnh vai trò quan trọng của VIC trong việc loại bỏ ảo giác, từ đó nâng cao hiệu suất tổng thể của mô hình. Đồng thời, để xác thực tính ưu việt của bộ dữ liệu được tạo ra bởi VIGC so với bộ dữ liệu LLaV A, chúng tôi đã tiến hành một thí nghiệm trong đó chúng tôi thay thế ngẫu nhiên 10.000 trường hợp từ mỗi loại dữ liệu, cũng như việc thay thế hoàn toàn tất cả ba loại dữ liệu. Kết quả thí nghiệm cho thấy rằng, trong điều kiện khối lượng dữ liệu không đổi, hiệu suất của mô hình được đào tạo trên hỗn hợp bộ dữ liệu LLaV A và bộ dữ liệu VIGC vượt trội so với mô hình được đào tạo chỉ trên bộ dữ liệu LLaV A.

--- TRANG 7 ---
[Bảng 2 và 3 chứa kết quả hiệu suất - giữ nguyên cấu trúc bảng]

Bảng 3 trình bày các thí nghiệm được thực hiện trên các bộ dữ liệu khác nhau và các mô hình có kích thước khác nhau, chứng minh rằng việc sử dụng dữ liệu được tạo ra từ các lĩnh vực khác nhau, như Objects365 và COCO, vẫn có thể dẫn đến những cải thiện hiệu suất đáng kể. Điều này cung cấp một giải pháp mới để nâng cao hiệu suất của các nhiệm vụ đa lĩnh vực. Chúng tôi cũng đã tiến hành thí nghiệm trên LLaV A-13B, chứng minh rằng sự nâng cao hiệu suất có thể đạt được trên các mô hình lớn hơn.

Chúng tôi cũng đã đánh giá hiệu suất của mô hình VIGC trên MMBench, LLaV A (như được trình bày trong Bảng 2) và tinh chỉnh thêm mô hình VIGC dựa trên 36K dữ liệu COCO được tạo ra bởi VIGC. Chúng tôi phát hiện ra rằng sau quá trình đào tạo tự lặp lại này, hiệu suất mô hình đã được cải thiện trên cả MMBench và LLaV A. Khả năng tự nâng cao đầy hứa hẹn này thông qua đào tạo lặp lại là một chủ đề mà chúng tôi dự định tiếp tục khám phá trong nghiên cứu tương lai.

Bộ dữ liệu và Đánh giá OK-VQA
Để đánh giá thêm chất lượng của dữ liệu được tạo ra bởi mô hình VIGC, chúng tôi đã tiến hành đào tạo và đánh giá trên bộ dữ liệu OKVQA, đòi hỏi kiến thức bên ngoài. Cụ thể, chúng tôi đã đào tạo mạng VIGC bằng cách sử dụng bộ dữ liệu OKVQA và các mẫu lệnh hướng dẫn tương ứng. Sau đó, chúng tôi tạo ra dữ liệu tinh chỉnh lệnh hướng dẫn bổ sung dựa trên VIGC trên COCO. Cuối cùng, chúng tôi tinh chỉnh InstructBLIP dựa trên OKVQA và dữ liệu được tạo ra. Chúng tôi nhận thấy rằng mặc dù InstructBLIP đã sử dụng một lượng lớn dữ liệu trong giai đoạn tinh chỉnh lệnh hướng dẫn, việc sử dụng dữ liệu được tạo ra bổ sung để tinh chỉnh nhiệm vụ hạ lưu vẫn nâng cao hiệu suất của mô hình trên các bộ dữ liệu cụ thể. Chúng tôi đã thực hiện cùng một xác thực thí nghiệm trên A-OKVQA.

Kết quả thí nghiệm được trình bày trong Bảng 4. Có thể thấy rằng hiệu suất của mô hình InstructBLIP, khi được tinh chỉnh với việc bổ sung dữ liệu được tạo ra, vượt trội so với mô hình chỉ được tinh chỉnh với dữ liệu gốc. Có sự cải thiện 0.7% và 1.6% trên OKVQA và A-OKVQA tương ứng, đạt được kết quả tiên tiến nhất cho các mô hình ở quy mô này trên cả hai bộ dữ liệu. Sử dụng mô hình tiền đào tạo MiniGPT-4+, chúng tôi đã đến kết luận tương tự. Điều này cho thấy rằng dữ liệu được tạo ra có thể nâng cao hiệu quả hiệu suất tinh chỉnh hạ lưu, một phát hiện có giá trị đáng kể đối với các lĩnh vực mà việc thu thập dữ liệu là thách thức.

Kết luận
Trong bài báo này, chúng tôi đã giới thiệu khung Tạo Lệnh hướng dẫn Trực quan và Hiệu chỉnh, một phương pháp tự hướng dẫn mới để tự động tạo ra dữ liệu lệnh hướng dẫn thị giác-ngôn ngữ chất lượng cao. Tận dụng khung VIGC, chúng tôi đã tạo ra dữ liệu tuân thủ lệnh hướng dẫn đa phương thức đa dạng trên các bộ dữ liệu COCO và Objects365. Chất lượng của dữ liệu này đã được xác thực thông qua các đánh giá khác nhau. Phương pháp tiếp cận dựa trên VIGC cung cấp một phương tiện thuận tiện để thu thập thêm dữ liệu điều chỉnh lệnh hướng dẫn chất lượng cao. Mặc dù việc sử dụng Hiệu chỉnh Lệnh hướng dẫn Trực quan đã giảm đáng kể ảo giác của mô hình, một số trường hợp vẫn tồn tại. Chúng tôi dự định nghiên cứu sâu hơn việc khám phá các giải pháp nhằm loại bỏ ảo giác đa phương thức trong tương lai. Hơn nữa, chúng tôi đang xem xét tiềm năng của việc tạo thành một hệ thống vòng lặp khép kín bằng cách tích hợp việc tạo dữ liệu tự động của VIGC với đào tạo mô hình đa phương thức. Hệ thống này sẽ nâng cao hiệu suất mô hình thông qua cải thiện dữ liệu và, một cách tương hỗ, nâng cao chất lượng dữ liệu thông qua cải thiện mô hình.

Lời cảm ơn
Dự án này được hỗ trợ bởi Chương trình R&D Trọng điểm Quốc gia của Trung Quốc (Số 2022ZD0160101) và Phòng thí nghiệm Trí tuệ Nhân tạo Thượng Hải.

--- TRANG 8 ---
[Phần tài liệu tham khảo - giữ nguyên danh sách tài liệu tham khảo]

--- TRANG 9 ---
[Tiếp tục phần tài liệu tham khảo]

--- TRANG 10 ---
Tài liệu Bổ sung

A. Mẫu Lệnh hướng dẫn
Các mẫu lệnh hướng dẫn cho VIGC, bao gồm hội thoại, mô tả chi tiết, lý luận phức tạp và các loại OKVQA, được trình bày trong Bảng 5, 6, 7 và 8 tương ứng.

[Bảng 5-8 chứa các mẫu lệnh hướng dẫn - giữ nguyên cấu trúc bảng]

--- TRANG 11 ---
[Tiếp tục các bảng mẫu lệnh hướng dẫn]

[Ví dụ về câu hỏi trả lời và hình ảnh minh họa]

Hình 5: Trực quan hóa hiện tượng ảo giác trong các trường hợp thất bại của VIGC: (a) hạn chế của mô hình, (b) thiên vị dữ liệu đào tạo, và (c) Suy giảm thông tin trong việc tạo chuỗi.

[Bảng 9 chứa thống kê ảo giác - giữ nguyên cấu trúc bảng]

B. Phân tích ảo giác cho mô hình VIGC.
Hiện tượng "ảo giác" trong các mô hình đa phương thức đề cập đến việc tạo ra các vật thể hoặc chi tiết không tồn tại trong đầu ra. Chúng tôi đã xác định ba nguyên nhân chính:

• Hạn chế của mô hình: Ngay cả những mô hình thị giác tiên tiến nhất ngày nay cũng không thể nắm bắt tất cả các chi tiết trong hình ảnh và dễ bị nhận dạng sai (xem Hình 5-a).

--- TRANG 12 ---
• Thiên vị dữ liệu đào tạo: Trong quá trình đào tạo mô hình, một số loại vật thể hoặc cảnh phổ biến hơn những loại khác. Ngoài ra, có sự đồng xuất hiện trong dữ liệu, có nghĩa là một số mục tiêu có khả năng xuất hiện trong cùng một mô tả/phản hồi cao hơn (xem Hình 5-b).

• Suy giảm thông tin trong việc tạo chuỗi: Trong các nhiệm vụ mô tả chi tiết, đầu ra của mô hình có thể bỏ qua thông tin hình ảnh ban đầu theo thời gian, dựa nhiều hơn vào bối cảnh ngôn ngữ tự tạo ra và tạo ra "ảo giác" (xem Hình 5-c và Tab. 9-2nd 50%).

Mô-đun con VIG của chúng tôi được thiết kế để hoạt động như một mô hình tự hỏi và tự trả lời, phân biệt nó với các mô hình chỉ trả lời câu hỏi. Nó giải quyết vấn đề ảo giác, đặc biệt là điểm thứ ba, thông qua các cập nhật lặp lại của hình ảnh đầu vào và văn bản. Chiến lược này giúp ngăn chặn suy giảm thông tin và việc khớp các mẫu ngôn ngữ phổ biến. Như được chứng minh trong Bảng 9, các thử nghiệm được thực hiện trên 100 hình ảnh từ tập đánh giá và dữ liệu tổng hợp cho thấy sự giảm đáng kể ảo giác sau khi hiệu chỉnh VIC, giảm từ 66% trong VIG xuống 10% trong VIC.

C. Phân tích Thống kê Dữ liệu
Dựa trên VIGC, chúng tôi tạo ra dữ liệu tinh chỉnh cho các nhiệm vụ đối thoại, mô tả chi tiết và lý luận phức tạp bằng cách sử dụng cả hình ảnh COCO và Objects365. Mỗi hình ảnh và cặp câu hỏi-trả lời tương ứng của nó được coi là một trường hợp. Dữ liệu mẫu, độ dài trung bình của câu hỏi và câu trả lời, và tính đa dạng của câu hỏi được trình bày trong Bảng 10.

Đa dạng Câu hỏi. Hình 6 trực quan hóa tần suất của các câu hỏi bắt đầu với các từ khác nhau, minh họa tính đa dạng của câu hỏi. Nhiều câu hỏi đối thoại bắt đầu với các cụm từ như "what is" hoặc "what color", trong khi các câu hỏi lý luận phức tạp thường bắt đầu với các cụm từ như "what might be" hoặc "what can be". Điều này là do các câu hỏi lý luận phức tạp thường chú trọng nhiều hơn đến lý do đằng sau các hiện tượng.

Trong Bảng 10, chúng tôi định lượng tính đa dạng bằng cách sử dụng khoảng cách cosine trung bình giữa các câu hỏi trong bộ dữ liệu. Khác với A-OKVQA (Schwenk et al. 2022), chúng tôi loại bỏ dấu câu khỏi các câu hỏi trước khi sử dụng bộ chuyển đổi câu (Jain 2022), và sau đó tính toán khoảng cách cosine trung bình giữa tất cả các cặp trong bộ dữ liệu.

[Bảng 10 chứa phân tích thống kê - giữ nguyên cấu trúc bảng]

--- TRANG 13 ---
[Hình 6 hiển thị phân phối từ câu hỏi]

D. Trực quan hóa Dữ liệu

--- TRANG 14 ---
[Các ví dụ trực quan hóa dữ liệu được tạo ra]

--- TRANG 15 ---
[Tiếp tục các ví dụ trực quan hóa]

--- TRANG 16 ---
[Các ví dụ cuối cùng về dữ liệu được tạo ra]
