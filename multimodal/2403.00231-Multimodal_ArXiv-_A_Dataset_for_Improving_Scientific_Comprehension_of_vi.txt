# 2403.00231.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2403.00231.pdf
# Kích thước tệp: 3480668 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Multimodal ArXiv: Bộ Dữ liệu Cải thiện Khả năng Hiểu Khoa học của 
Mô hình Thị giác-Ngôn ngữ Lớn
Lei Li*†, Yuqi Wang*†, Runxin Xu‡, Peiyi Wang‡
Xiachong Feng†, Lingpeng Kong†, Qi Liu†
†Đại học Hồng Kông
‡Đại học Bắc Kinh
{nlp.lilei, runxinxu, wangpeiyi9979, xiachongfeng1996}@gmail.com
wangyuqi@connect.hku.hk {lpk, liuqi}@cs.hku.hk

Tóm tắt
Các mô hình thị giác-ngôn ngữ lớn (LVLM) xuất sắc trong nhiều tác vụ đa dạng liên quan đến hình ảnh cụ thể từ các cảnh tự nhiên. Tuy nhiên, khả năng diễn giải các hình ảnh trừu tượng, như hình dạng hình học và biểu đồ khoa học, vẫn còn hạn chế do thiếu hụt các bộ dữ liệu huấn luyện trong các lĩnh vực khoa học. Để lấp đầy khoảng trống này, chúng tôi giới thiệu Multimodal ArXiv, bao gồm ArXivCap và ArXivQA, để nâng cao khả năng hiểu khoa học của LVLM. ArXivCap là bộ dữ liệu hình ảnh-chú thích gồm 6.4M hình ảnh và 3.9M chú thích, được lấy từ 572K bài báo ArXiv trải rộng trên nhiều lĩnh vực khoa học khác nhau. Dựa trên ArXivCap, chúng tôi giới thiệu ArXivQA, một bộ dữ liệu hỏi-đáp được tạo ra bằng cách nhắc GPT-4V dựa trên các hình ảnh khoa học. ArXivQA cải thiện đáng kể khả năng lý luận toán học của các LVLM mã nguồn mở, đạt được 10.4% tăng độ chính xác tuyệt đối trên một benchmark lý luận toán học đa phương thức. Hơn nữa, sử dụng ArXivCap, chúng tôi thiết kế bốn tác vụ thị giác sang văn bản để đánh giá LVLM. Kết quả đánh giá với các LVLM tiên tiến nhấn mạnh sự khó khăn của chúng với ngữ nghĩa tinh tế của các hình ảnh học thuật, trong khi việc huấn luyện chuyên biệt theo lĩnh vực mang lại những cải thiện hiệu suất đáng kể. Phân tích lỗi của chúng tôi phát hiện việc diễn giải sai bối cảnh thị giác, lỗi nhận dạng và việc tạo ra các chú thích quá đơn giản bởi các LVLM hiện tại, làm sáng tỏ các cải thiện trong tương lai.¹

1 Giới thiệu
Các mô hình thị giác-ngôn ngữ lớn (LVLM), tích hợp các mô hình ngôn ngữ lớn (LLM) (Brown et al., 2020a; Touvron et al., 2023) với các bộ mã hóa thị giác được huấn luyện trước thông qua huấn luyện căn chỉnh đa phương thức (Madureira, 2021; Liu et al., 2023b; Li et al., 2023d), đã thể hiện khả năng nhận thức và nhận thức đáng chú ý trong xử lý

*Đóng góp ngang nhau.
¹Bộ dữ liệu và mô hình được phát hành tại trang dự án của chúng tôi:
https://mm-arxiv.github.io . hình ảnh cụ thể từ các cảnh hàng ngày (OpenAI, 2023; Fu et al., 2023; Yang et al., 2023a; Reka, 2024). Tuy nhiên, các nghiên cứu gần đây đã cho thấy rằng các LVLM mã nguồn mở gặp khó khăn trong việc hiểu các hình ảnh trừu tượng, như hình dạng hình học trong lý luận toán học đa phương thức (Lu et al., 2023; Zhang et al., 2024b) và các biểu đồ khoa học (Yue et al., 2023). Sự thiếu hụt các bộ dữ liệu huấn luyện trong các lĩnh vực khoa học liên quan đến lý luận phức tạp với các hình ảnh trừu tượng là nguyên nhân chính.

Để giải quyết vấn đề này, chúng tôi xây dựng Multimodal ArXiv bằng cách sử dụng các tài nguyên phong phú trong các bản thảo được lưu trữ trên arXiv để cải thiện khả năng hiểu tài liệu khoa học trong LVLM. Đầu tiên chúng tôi tuyển chọn ArXivCap, một bộ dữ liệu hình ảnh-chú thích khoa học đa dạng. Trái ngược với các bộ dữ liệu hình ảnh khoa học trước đây, bao gồm các hình ảnh tổng hợp (Chen et al., 2020) hoặc bị hạn chế trong các tình huống chú thích đơn giản trong lĩnh vực khoa học máy tính (Hsu et al., 2021), bộ dữ liệu của chúng tôi được tạo thành từ các hình ảnh được trích xuất từ các bài báo học thuật trên nhiều lĩnh vực. ArXivCap có 6.4M hình ảnh và 3.9M chú thích từ 572K bài báo. Chúng tôi cũng giữ lại cấu trúc hình phụ và tiêu đề của các bài báo gốc, do đó hỗ trợ các tác vụ đánh giá đa dạng. Chúng tôi tiếp tục hướng dẫn GPT-4V tạo ra 100K cặp hỏi-đáp trắc nghiệm (QA) cho các hình ảnh trong ArXivCap. Bộ dữ liệu ArXivQA kết quả có thể tự nhiên phục vụ như một tài nguyên then chốt để cải thiện khả năng lý luận khoa học của LVLM.

Chúng tôi xác nhận hiệu quả của bộ dữ liệu Multimodal ArXiv từ hai khía cạnh: khả năng lý luận được đo bằng độ chính xác QA và hiệu suất tạo sinh thông qua các tác vụ thị giác sang văn bản mới. Các thí nghiệm của chúng tôi chứng minh rằng ArXivQA mang lại sự cải thiện độ chính xác tuyệt đối đáng kể 10.4% cho Qwen-VL-Chat (Bai et al., 2023b), trên MathVista (Lu et al., 2023), một benchmark đầy thử thách cho lý luận toán học đa phương thức. Ngoài ra, phân tích chi tiết khám phá mối quan hệ giữa các lĩnh vực bài báo và hiệu suất tác vụ chi tiết.arXiv:2403.00231v3 [cs.CV] 2 Jun 2024

--- TRANG 2 ---
Hơn nữa, sử dụng ArXivCap, chúng tôi định nghĩa bốn tác vụ tạo sinh với độ phức tạp khác nhau để đánh giá khả năng hiểu biểu đồ khoa học của LVLM: (1) chú thích một hình ảnh học thuật đơn lẻ, (2) tạo tóm tắt tổng thể cho nhiều hình phụ, (3) chú thích hình ảnh trong bối cảnh với các cặp hình ảnh-chú thích trước đó, và (4) tạo tiêu đề bài báo từ các cặp hình ảnh-chú thích. Chúng tôi kiểm tra nhiều LVLM khác nhau, bao gồm các mô hình mã nguồn mở cũng như các mô hình độc quyền bao gồm GPT-4V (OpenAI, 2023) và Gemini 1.0 Pro Vision (Gemini Team, 2023). Kết quả đánh giá cho thấy rằng mặc dù các LVLM hiện tại vẫn đối mặt với thử thách trong việc tạo ra các chú thích trung thực cho các hình ảnh khoa học, việc huấn luyện trong lĩnh vực trên bộ dữ liệu của chúng tôi mang lại những cải thiện hiệu suất đáng kể trên cả bốn tác vụ. Phân tích lỗi thủ công nhấn mạnh rằng LVLM vẫn gặp phải việc diễn giải sai bối cảnh thị giác, lỗi nhận dạng và chú thích quá đơn giản, mở đường cho các nghiên cứu tương lai.

2 Nghiên cứu Liên quan
Những tiến bộ gần đây trong LVLM đã chứng kiến sự tiến bộ đáng chú ý trong kiến trúc mô hình, mô hình huấn luyện và tạo bộ dữ liệu (Zhang et al., 2024a).

Kiến trúc Mô hình LVLM thường bao gồm ba mô-đun cốt lõi: (i) một bộ mã hóa thị giác để trích xuất đặc trưng hình ảnh, (ii) một mô-đun căn chỉnh phương thức để tích hợp các đặc trưng thị giác vào không gian nhúng mô hình ngôn ngữ, và (iii) một xương sống LLM để giải mã bối cảnh đa phương thức. CLIP (Radford et al., 2021) được sử dụng rộng rãi để mã hóa hình ảnh, trong khi LLaMA (Touvron et al., 2023) và Vicuna (Chiang et al., 2023) phục vụ như những lựa chọn phổ biến cho LLM. Mô-đun căn chỉnh dao động từ các phép chiếu tuyến tính đơn giản (Liu et al., 2023b; Zhu et al., 2023) đến các kiến trúc phức tạp hơn như các lớp chú ý chéo có cổng được chứng thực bởi Flamingo và IDEFICS (Alayrac et al., 2022; Awadalla et al., 2023). Các cải tiến như mô-đun Q-Former trong BLIP2 (Li et al., 2023b) và tích hợp hướng dẫn trong InstructBLIP (Dai et al., 2023) tiếp tục nâng cao khả năng căn chỉnh. Ngoài ra, Fuyu-8B (Bavishi et al., 2023) giới thiệu một khung mới ánh xạ trực tiếp các pixel hình ảnh thô vào không gian nhúng LLM.

Mô hình Huấn luyện Về các công thức huấn luyện, PaLI-X (Chen et al., 2023b) điều tra các hiệu ứng mở rộng của cả bộ mã hóa thị giác và mô hình ngôn ngữ, làm nổi bật lợi thế của việc mở rộng cả hai thành phần. Qwen-VL (Bai et al., 2023b) tăng độ phân giải hình ảnh đầu vào và khám phá các chiến lược giải phóng mô-đun khác nhau. Các phương pháp căn chỉnh như huấn luyện RLHF (Ouyang et al., 2022), ví dụ LLaVA-RLHF (Sun et al., 2023), và tối ưu hóa sở thích thông qua phản hồi AI (Li et al., 2023c) chứng minh hiệu quả trong việc căn chỉnh LVLM với sở thích của con người.

Tuyển chọn Bộ dữ liệu Chất lượng bộ dữ liệu ảnh hưởng đáng kể đến hiệu suất LVLM. Huấn luyện căn chỉnh phương thức thường sử dụng các cặp hình ảnh-chú thích quy mô web như Laion-400M (Schuhmann et al., 2021), với các nghiên cứu gần đây ưa thích các chú thích được làm sạch (Chen et al., 2023a; Yu et al., 2023). Tinh chỉnh hướng dẫn (IFT) giúp LVLM phản hồi theo truy vấn của người dùng, kích hoạt việc khám phá các bộ dữ liệu IFT chất lượng cao. Các nỗ lực bao gồm các bộ sưu tập hướng dẫn đa phương thức như MultiInstruct (Xu et al., 2023) và M3IT (Li et al., 2023d), các bộ dữ liệu kiểu đối thoại như LLaVA (Liu et al., 2023b) và các bộ dữ liệu chuyên biệt theo lĩnh vực cho y tế (Li et al., 2023a) và hình ảnh giàu văn bản (Zhang et al., 2023). Trong lĩnh vực khoa học, FigCAP (Chen et al., 2019) và FigureQA (Kahou et al., 2017) được tạo ra dựa trên các hình ảnh tổng hợp. DVQA (Kafle et al., 2018) tạo ra các câu hỏi dựa trên heuristic chỉ cho biểu đồ cột. SciCap (Hsu et al., 2021), SciCap+ (Yang et al., 2023b), và M-Paper (Hu et al., 2023) thu thập các cặp hình ảnh-chú thích từ các lĩnh vực cụ thể như khoa học máy tính. So với các bộ dữ liệu này, ArXivCap của chúng tôi được lấy từ các lĩnh vực khoa học đa dạng với quy mô lớn hơn nhiều, cho phép các cải thiện và đánh giá toàn diện hơn. Bên cạnh đó, chúng tôi sử dụng GPT-4V để tạo ArXivQA với các câu hỏi thử thách, thể hiện hiệu quả của nó trong việc thúc đẩy khả năng lý luận toán học của LVLM.

3 Multimodal ArXiv
Phần này trình bày quy trình xây dựng chi tiết của bộ dữ liệu Multimodal ArXiv, bao gồm hai tập: ArXivCap (§3.1) và ArXivQA (§3.2).

3.1 ArXivCap
Quy trình Xây dựng Chúng tôi phác thảo quy trình tạo ArXivCap dưới đây và Hình 1 đưa ra tổng quan.

Lọc Bài báo theo Loại Xuất bản: ArXivCap được trích xuất từ ArXiv (Clement et al., 2019), được cấp phép CC-0 để sửa đổi và phân phối. Các tệp thô của các bài báo được đăng trên

--- TRANG 3 ---
1. Tệp Nguồn Bài báo ArXiv 2. Lọc Bài báo 3. Trích xuất Cặp Hình-Chú thích
4. Làm sạch Dựa trên Quy tắc
5. Nhắc GPT-4 Vision ArXivCap ArXivQA

Hình 1: Tổng quan quy trình tuyển chọn bộ dữ liệu của chúng tôi. Bắt đầu từ các tệp nguồn bài báo ArXiv, chúng tôi đảm bảo chất lượng bài báo bằng cách chọn các bài báo theo hồ sơ xuất bản. Các cặp hình ảnh và chú thích được trích xuất và sau đó được làm sạch theo các quy tắc được thiết kế thủ công. ArXivQA được tạo ra bằng cách nhắc GPT-4V với một mẫu được tuyển chọn.

Hình:     Chú thích: Chú thích của hình.
Hình phụ 1:     Chú thích phụ 1: Chú thích phụ của hình phụ....Hình phụ N:     Chú thích phụ N: Chú thích phụ của hình phụ.
Chú thích Chính: Chú thích cho các hình phụ.

Hình 2: Minh họa hai loại cặp hình-chú thích. (Trái) Cặp Hình đơn lẻ. (Phải) Cặp chú thích Nhiều-Hình có nhiều hình phụ với các chú thích phụ tương ứng và một chú thích chính tổng thể.

ArXiv các tệp tar trước tháng 6 năm 2023 được tải xuống. Để đảm bảo chất lượng bộ dữ liệu của chúng tôi, chúng tôi sử dụng một quy trình lựa chọn nghiêm ngặt để lọc các bài báo có thể chất lượng thấp có thể ảnh hưởng đến chất lượng cặp hình-chú thích. Đầu tiên, chúng tôi truy xuất thông tin meta cho các bài báo từ Semantic Scholar (Kinney et al., 2023), chứa hồ sơ xuất bản cho mỗi bài báo. Các bài báo có loại xuất bản JournalArticle, Conference, hoặc Review được giữ lại vì chúng tôi cho rằng quy trình đánh giá đồng nghiệp có thể đảm bảo chất lượng tổng thể của cặp hình-chú thích là thỏa đáng. Chúng tôi tiếp tục loại trừ các bài báo có tiêu đề vượt quá 100 từ hoặc tóm tắt dài hơn 300 từ, phù hợp với các yêu cầu nộp bài thông thường.

Trích xuất Cặp Hình-Chú thích: Hình ảnh và chú thích được trích xuất từ các tệp LaTeX gốc bằng cách khớp cú pháp. Chúng tôi tiếp tục sử dụng một công cụ mạnh mẽ ImageMagisk (ImageMagick Studio LLC) để chuyển đổi hình ảnh sang định dạng JPEG để xử lý dễ dàng. Các hình ảnh và chú thích được trích xuất được lưu trữ trong một cấu trúc chunk được thiết kế, bao gồm hoặc một cặp hình-chú thích đơn lẻ hoặc nhiều hình với các chú thích phụ tương ứng và một chú thích chính cho mô tả tổng thể. Định dạng này phù hợp hơn với bố cục của các bài báo học thuật, và Hình 2 minh họa cấu trúc chunk.

Làm sạch Chú thích và Lọc Hình ảnh: Sau khi kiểm tra thủ công bộ dữ liệu được thu thập ban đầu, chúng tôi thiết kế một số phép biến đổi để làm sạch chú

Trường Số lượng Độ dài Trung bình Phân vị Độ dài
Tiêu đề 572K 10.4 (8, 10, 12)
Tóm tắt 572K 167.6 (126, 165, 207)
Chú thích Chính 3.9M 47.6 (15, 35, 65)
Chú thích phụ 1.0M 4.8 (2, 3, 5)
Chú thích Chunk 3.9M 48.8 (16, 36, 67)
Hình ảnh 6.4M N / A N / A

Bảng 1: Thống kê số từ cho tiêu đề, tóm tắt, chú thích và số lượng hình ảnh. Chú thích chunk đề cập đến sự kết hợp của các chú thích phụ và chú thích chính cho trường hợp nhiều hình.

thích và lọc hình ảnh.

Làm sạch Chú thích: (i) Các chunk có chú thích ngắn hơn 5 từ bị loại bỏ; (ii) Đối với các chú thích có biểu thức LaTeX như công thức toán học và tham chiếu, chúng tôi áp dụng pylatexenc² để biến đổi LaTeX thành văn bản với các công thức toán học được giữ lại, trích dẫn thành một ký hiệu đặc biệt <cit.>, tham chiếu thành <ref>. Một minh họa về làm sạch chú thích có thể được tìm thấy trong Phụ lục A.1.

Lọc Hình ảnh: Chúng tôi loại bỏ các hình ảnh được coi là có vấn đề theo các quy tắc sau: (i) Hình ảnh có tỷ lệ khung hình lớn hơn 100; (ii) Hình ảnh có cạnh ngắn nhất ngắn hơn 224 pixel; và (iii) Hình ảnh có số pixel lớn hơn ngưỡng bom giải nén.

Sau các quy trình này, 100 cặp được lấy mẫu để thực hiện kiểm tra thủ công bổ sung, nơi chúng tôi thấy tất cả các cặp này đều chứa hình ảnh rõ ràng và mô tả chú thích chính xác. Chúng tôi cung cấp các cặp hình-chú thích được trực quan hóa trong Phụ lục A.2.

Thống kê của ArXivCap Bảng 1 liệt kê các thống kê bộ dữ liệu. ArXivCap bao gồm 572K bài báo, chứa 6.4M hình ảnh chất lượng cao tổng cộng với 193M từ. Một minh họa đám mây từ của chú thích có thể được tìm thấy trong Phụ lục A.3. Hình 3 thể hiện phân phối lĩnh vực bài báo được trích xuất từ ArXiv, nơi chúng tôi thấy rằng ArXivCap của chúng tôi bao gồm 32 lĩnh vực, như khoa học máy tính, toán học,

²https://github.com/phfaist/pylatexenc

--- TRANG 4 ---
Bộ dữ liệu Số lượng Hình ảnh Số lượng Bài báo Danh mục Hình ảnh Lĩnh vực Dữ liệu Thực
FigCAP (Chen et al., 2020) 219K N / A Biểu đồ Cột, Đường và Tròn N / A ✗
SciCap (Yang et al., 2023b) 2.1M 295K Danh mục Mở Khoa học Máy tính và Học máy ✓
M-Paper (Hu et al., 2023) 350K 48K Danh mục Mở Chủ yếu "Học sâu" ✓
ArXivCap (Của chúng tôi) 6.4M 572K Danh mục Mở Lĩnh vực Mở ✓
FigureQA (Kahou et al., 2017) 140K N / A Biểu đồ Cột, Đường và Tròn N / A ✗
DVQA (Kafle et al., 2018) 300K N / A Biểu đồ Cột N / A ✗
ArXivQA (Của chúng tôi) 32K 16.6K Danh mục Mở Lĩnh vực Mở ✓

Bảng 2: So sánh với các bộ dữ liệu hình ảnh khoa học trước đây. ArXivCap của chúng tôi là bộ dữ liệu chú thích lớn nhất và ArXivQA của chúng tôi là bộ dữ liệu QA duy nhất bao gồm một loạt các lĩnh vực từ các bài báo thực.

Hình 3: Phân phối lĩnh vực bài báo của ArXivCap. Xem Bảng 10 trong Phụ lục A để biết tên đầy đủ của mỗi lĩnh vực.

vật lý và kinh tế. Như thể hiện trong Bảng 2, so với các bộ dữ liệu hình ảnh khoa học trước đây, ArXivCap của chúng tôi là bộ dữ liệu hình-chú thích lớn nhất được thu thập từ các bài báo thực và bao gồm một loạt các lĩnh vực khoa học, phục vụ như một tài nguyên quý giá để cải thiện và đánh giá LVLM.

3.2 ArXivQA

Vì ArXivCap của chúng tôi chứa các hình ảnh đa dạng từ các lĩnh vực khoa học, chúng tôi cho rằng việc học trả lời các câu hỏi về những hình ảnh này có thể thúc đẩy khả năng lý luận khoa học. Theo thực hành thành công của LLaVA (Liu et al., 2023b), chúng tôi áp dụng GPT-4V để tạo ra các bộ dữ liệu tinh chỉnh hướng dẫn để tạo ra các cặp QA dựa trên các hình ảnh được trích xuất từ các bài báo khoa học. Cụ thể, chúng tôi thiết kế một mẫu nhắc để truy vấn GPT-4V tạo ra các cặp QA dựa trên 35K hình ảnh được lấy mẫu ngẫu nhiên từ ArXivCap của chúng tôi. Bảng 11 trong Phụ lục A.4 cung cấp mẫu chúng tôi sử dụng cho nhắc. Các cặp được tạo ra được phân tích theo yêu cầu định dạng và chúng tôi loại bỏ các mẫu không có lựa chọn

Mô hình Độ chính xác
InstructBLIP-Vicuna7B 7.0%
LLaVA-1.5-7B 44.2%
LLaVA-1.5-13B 46.8%
OpenFlamingo-9B 9.9%
IDEFICS-Instruct-9B 34.5%
Qwen-VL-Chat 46.6%
Con người (Tập con 100 mẫu) 80.0%
Con người (tập con CS) 88.2%

Bảng 3: Kết quả đánh giá trên 1,000 mẫu ArXivQA được lấy mẫu.

và lý luận. Có 100K cặp QA sau khi lọc các mẫu không hợp lệ. Bộ dữ liệu bao gồm các câu hỏi với số từ trung bình là 16.98 cho văn bản câu hỏi. Trung bình có 4.20 lựa chọn mỗi câu hỏi và độ dài trung bình của văn bản cho một lựa chọn đơn lẻ là 7.59 từ. Phụ lục A.2 cung cấp các mẫu từ bộ dữ liệu ArXivQA.

Như một nghiên cứu sơ bộ, chúng tôi lấy mẫu 1,000 mẫu từ ArXivQA và nhắc các LVLM mã nguồn mở dự đoán câu trả lời cho các câu hỏi và lựa chọn. Một nhắc đơn giản được thiết kế để sử dụng GPT-4 để trích xuất nhãn câu trả lời từ các thế hệ mô hình. Đối với hiệu suất của con người, chúng tôi yêu cầu bốn tác giả thực hiện dự đoán trên một tập con 100 mẫu (trong đó 17 mẫu từ lĩnh vực CS). Mỗi người trong số họ được yêu cầu trả lời 50 mẫu và điểm độ chính xác được thu được bằng cách tính trung bình hai người chú thích. Như thể hiện trong Bảng 3, hầu hết các mô hình gặp khó khăn trong việc thực hiện thỏa đáng trên bộ dữ liệu ArXivQA, tụt hậu xa so với hiệu suất của con người. Điều này xác minh tiền đề của chúng tôi rằng các LVLM mã nguồn mở hiện tại thất bại trong việc hiểu các hình ảnh khoa học. Chúng tôi cũng nhận thấy rằng việc đơn giản tăng quy mô mô hình từ 7B (LLaVa-1.5-7B) lên 13B (LLaVa-1.5-13B) không mang lại sự cải thiện đáng kể, điều này cho thấy rằng khả năng lý luận toán học đa phương thức không thể được thu được đơn giản từ phía LLM.

--- TRANG 5 ---
Mô hình Figure QA Geometry Problem Solving Math Word Problem Textbook QA Visual QA ALL
IDEFICS-Instruct-9B†41.4 22.0 18.2 34.6 44.6 33.7
InstructBLIP-Vicuna13B†41.4 19.9 45.5 45.8 57.6 39.3
LLaVa-v1.5-13B†44.0 26.7 40.9 45.8 44.6 39.3
Qwen-VL-Chat-7B 48.3 19.1 22.7 46.7 57.6 40.0
Qwen-VL-Chat-7B ArXivCap 39.7 19.8 27.2 39.7 52.1 36.2
Qwen-VL-Chat-7B ArXivQA 44.8 34.0 27.3 70.0 64.1 50.2
Qwen-VL-Chat-7B ArXivCap + ArXivQA 44.0 37.6 27.3 68.2 63.0 50.4
Bard†38.8 51.1 27.3 64.5 51.1 50.0
GPT-4V†52.6 51.8 54.5 83.2 66.3 61.9

Bảng 4: Đánh giá trên bộ dữ liệu MathVista. ArXivCap và ArXivQA cùng nhau nâng cao hiệu suất tổng thể của Qwen-VL-Chat, vượt qua mô hình thương mại Bard.†biểu thị kết quả dựa trên các dự đoán gốc từ Lu et al. (2023). Kết quả tốt nhất được tô đậm, trong khi điểm số tốt thứ hai được đánh dấu gạch chân.

4 Thí nghiệm
Chúng tôi tiến hành thí nghiệm để (i) xác nhận hiệu quả của ArXivQA trong việc thúc đẩy lý luận khoa học đa phương thức cho LVLM mã nguồn mở (§4.1) và (ii) đánh giá khả năng hiểu các hình ảnh khoa học của LVLM với ArXivCap (§4.2).

4.1 Thúc đẩy LVLM với ArXivQA
4.1.1 Cài đặt Thí nghiệm
Chúng tôi áp dụng Qwen-VL-Chat-7B (Bai et al., 2023b) làm xương sống do hỗ trợ các định dạng đầu vào hình ảnh-văn bản xen kẽ và hình ảnh độ phân giải cao. Chúng tôi tinh chỉnh nó trên ArXivCap (Qwen-VL-Chat-7B ArXivCap), ArXivQA (Qwen-VL-Chat-7B ArXivQA) và kết hợp của hai bộ dữ liệu này (Qwen-VL-Chat-7B ArXivCap + ArXivQA) trong ba epoch với tốc độ học 1e-5 theo bài báo gốc. Chúng tôi kết hợp câu trả lời và lý luận trong ArXivQA để tạo thành đầu ra mục tiêu trong quá trình huấn luyện. Các mô hình được đánh giá trên MathVista (Lu et al., 2023), một benchmark yêu cầu hiểu thị giác chi tiết, sâu sắc và lý luận tổng hợp. MathVista chứa 6,141 ví dụ, bao gồm năm tác vụ đa phương thức Figure QA, Geometry Problem Solving, Math word problem, Text Book QA, và Visual QA. Chúng tôi chọn 478 câu hỏi trắc nghiệm trong phần testmini để tránh sự không nhất quán trong phân tích câu trả lời. Chúng tôi tính điểm độ chính xác và áp dụng các tệp dự đoán được cung cấp để tính hiệu suất cơ sở.

4.1.2 Kết quả
Như thể hiện trong Bảng 4, tinh chỉnh trên Multimodal ArXiv của chúng tôi, đặc biệt trên bộ dữ liệu ArXivQA, liên tục thúc đẩy hiệu suất, giúp Qwen-VL-Chat mã nguồn mở đạt được hiệu suất lý luận MathVista tổng thể tương đương. Do phạm vi bao phủ rộng của các hình ảnh khoa học, sự cải thiện hiệu suất chủ yếu đến từ việc cải thiện đáng kể các tác vụ Geometry Problem Solving, Textbook QA, và Visual QA. Ví dụ, sau khi tinh chỉnh trên bộ dữ liệu ArXivQA, độ chính xác tăng từ 19.1% lên 34.0% và từ 46.7% lên 70.0% lần lượt trên các tác vụ Geometry Problem Solving và Textbook QA. Sự cải thiện trên Math Word Problem là nhỏ, nơi chúng tôi nghĩ việc tăng cường dữ liệu chuyên biệt theo lĩnh vực có thể được khám phá thêm với một bộ dữ liệu lọc được tuyển chọn trên bộ dữ liệu của chúng tôi (Gao et al., 2023). Ngược lại, độ chính xác của Figure QA giảm nhẹ so với mô hình xương sống gốc, điều mà chúng tôi cho là do hầu hết các biểu đồ trong đánh giá Figure QA được lấy mẫu từ các bộ dữ liệu tổng hợp như DVQA (Kafle et al., 2018), thể hiện khoảng cách lớn giữa các hình ảnh bài báo thế giới thực.

4.1.3 Phân tích
Chúng tôi điều tra cách các lĩnh vực chủ đề khác nhau ảnh hưởng đến khả năng lý luận toán học bằng cách sử dụng các cặp câu hỏi và câu trả lời (QA). Chúng tôi tập trung vào sáu lĩnh vực với hơn 5K mẫu mỗi lĩnh vực. Từ mỗi lĩnh vực, chúng tôi chọn ngẫu nhiên một tập con 5K mẫu để đảm bảo công bằng trong so sánh. Sau đó chúng tôi tinh chỉnh mô hình cơ sở Qwen-VL-Chat bằng cách sử dụng các cặp QA từ mỗi lĩnh vực và quan sát cách nó ảnh hưởng đến độ chính xác của mô hình so với trạng thái ban đầu. Hình 4 thể hiện các thay đổi độ chính xác tương đối (tức là, Độ chính xác sau Tinh chỉnh / Độ chính xác Gốc −1) sau khi huấn luyện mô hình trên các cặp QA từ mỗi lĩnh vực. Các phát hiện của chúng tôi tiết lộ một số điểm chính: (i) Các cặp QA từ lĩnh vực Khoa học Máy tính (CS) rất hiệu quả trong việc cải thiện khả năng lý luận toán học, đạt được sự cải thiện tương đối đáng chú ý 27.09%. Chúng tôi cho rằng điều này là do bản chất tổng hợp của lĩnh vực CS. (ii) Lĩnh vực có lợi nhất thay đổi tùy thuộc vào tác vụ cụ thể. Ví dụ, các cặp QA từ lĩnh vực vật lý thiên văn nâng cao việc giải quyết vấn đề hình học, trong khi những cặp từ Condensed Matter cải thiện hiệu suất trong các bài toán từ toán học. (iii) Hầu hết các lĩnh vực làm tổn hại tác vụ Figure QA. Điều này cho thấy rằng Figure QA tổng hợp có thể không phải là benchmark tốt nhất để đánh giá khả năng lý luận thực tế. Những phát hiện này nhấn mạnh hiệu quả của các cặp QA được tạo ra và cung cấp những hiểu biết có giá trị cho nghiên cứu tương lai, như điều chỉnh trọng số cụ thể cho tác vụ trong bộ dữ liệu một cách phù hợp.

4.2 Đánh giá LVLM trên ArXivCap
4.2.1 Các Tác vụ Được Đánh giá
Bốn tác vụ thị giác sang văn bản để đánh giá khả năng hiểu các hình ảnh khoa học của LVLM.

Chú thích Hình Đơn lẻ Tương tự như thiết lập chú thích hình ảnh truyền thống (Lin et al., 2014), chú thích hình đơn lẻ yêu cầu mô hình tạo ra một chú thích cho hình đã cho. Các chú thích được tạo ra bởi mô hình được kỳ vọng bao bọc các chi tiết tinh tế trong những hình này, bao gồm số và công thức toán học, đưa ra một thử thách độc đáo cho các mô hình để xác định và diễn đạt những yếu tố này một cách chính xác. Chính thức, cho một cặp hình ảnh-chú thích (I, C), LVLM M được yêu cầu tạo ra chú thích cho một nhắc hướng dẫn Ps để gợi ý mục tiêu của chú thích khoa học:
Ĉ=M(I, Ps),
trong đó Ĉ sẽ được đánh giá theo ground-truth C.

Chú thích Nhiều-Hình Chúng tôi giới thiệu một thử thách phức tạp hơn liên quan đến việc áp dụng lý luận trên nhiều hình ảnh. Tác vụ này, được gọi là Chú thích Nhiều-Hình, đòi hỏi mô hình tạo ra một chú thích tóm tắt toàn diện cho các hình phụ. Như được minh họa trong Hình 2, mô hình được giao nhiệm vụ tạo ra một chú thích bao quát cho hai hoặc nhiều hình phụ, tận dụng các manh mối thị giác để rút ra so sánh và công thức hóa các chú thích tóm tắt. Chính thức, cho một danh sách các hình L= (I1, . . . , In), mô hình được yêu cầu tạo ra chú thích chính ground-truth C bằng cách xem xét tất cả ngữ nghĩa trong các hình với một nhắc tác vụ Pm:
Ĉ=M(L, Pm) =M(I1, . . . , In, Pm).

Chú thích Theo Bối cảnh Lấy cảm hứng từ khả năng học trong bối cảnh đang phát triển của LLM (Brown et al., 2020b; Dong et al., 2022), chúng tôi giới thiệu một tác vụ chú thích theo bối cảnh để kiểm tra khả năng học trong bối cảnh của LVLM. Trong tác vụ này, mô hình được trình bày với một tập các cặp hình-chú thích, và mục tiêu của nó là tạo ra một chú thích cho một hình ảnh đã cho dựa trên các minh họa được cung cấp. Cho một cặp hình ảnh-chú thích tuần tự S={(Ii, Ci)}n_i=1 bao gồm n cặp hình ảnh Ii và Ci tương ứng, tác vụ chú thích hình ảnh theo bối cảnh có thể được chính thức hóa như sau:
Ĉn=M(I1, C1, . . . , In−1, Cn−1, In, Pc).
Mô hình được cho là tận dụng lịch sử bối cảnh để nâng cao độ chính xác và sự mạch lạc của chú thích được tạo ra.

Tạo Tiêu đề Tác vụ này yêu cầu hiểu tinh tế về các hình và chú thích để chắt lọc các quan sát thiết yếu thành một tóm tắt cấp cao về các kết quả được trình bày cho LVLM. Cụ thể, thay vì tạo ra các chú thích cho các hình, tác vụ này yêu cầu mô hình kết nối các hình khác nhau và các chú thích tương ứng để suy ra tiêu đề bài báo. Để S={(Ii, Ci)}m_i=1 là một chuỗi của m cặp hình-chú thích trong bài báo được trích xuất. Lưu ý rằng Ii có thể là một hình đơn lẻ hoặc một nhiều-hình, và chúng tôi tái sử dụng Ii để đơn giản hóa ở đây. Việc tạo tiêu đề yêu cầu M tạo ra tiêu đề cho bài báo cho một nhắc tác vụ Pt:
T̂=M(I1, C1, . . . , Im, Cm, Pt).
Dự đoán T̂ được đánh giá bằng cách so sánh với tiêu đề gốc T.

4.2.2 Cài đặt Thí nghiệm
Bộ dữ liệu Chúng tôi chia ArXivCap thành các tập huấn luyện và kiểm tra với tỷ lệ 9:1 để đánh giá. Tập kiểm tra bao gồm: 161.3K mẫu cho chú thích hình đơn lẻ, 12.8K mẫu cho chú thích nhiều-hình, 57.2K mẫu cho chú thích theo bối cảnh, và 57.2K mẫu cho tạo tiêu đề.

Các Mô hình Được Đánh giá Chúng tôi chọn nhiều LVLM khác nhau bao gồm các kiến trúc khác nhau. (1) LVLM được thiết kế để xử lý một hình ảnh đơn lẻ, BLIP2-OPT-6.7B (Li et al., 2023b), InstructBLIP-Vicuna7B (Dai et al., 2023), LLaVA-1.5-7B/13B (Liu et al., 2023a). Do hạn chế về khả năng, chúng tôi chỉ đánh giá những mô hình này trên tác vụ chú thích hình ảnh đơn lẻ; (2) LVLM có khả năng xử lý đầu vào văn bản-hình ảnh xen kẽ, như OpenFlamingo-9B (Alayrac et al., 2022; Awadalla

--- TRANG 6 ---
Math CS Physics Astrophysics Condensed Matter Statistics
Lĩnh vực Arxiv -20 0 20 40 60 80 100 Thay đổi Độ chính xác Tương đối (%) ALL
Geometry Problem Solving Math Word Problem
Visual QA Figure QA
Textbook QA

Hình 4: Các thay đổi độ chính xác tương đối được mang lại bởi việc huấn luyện trên các mẫu ArXivQA lĩnh vực khác nhau.

et al., 2023), IDEFICS-Instruct-9B (Laurençon et al., 2023), Qwen-VL-Chat-7B (Bai et al., 2023b). Những mô hình này được đánh giá trên tất cả các tác vụ chúng tôi đề xuất; (3) Các mô hình độc quyền như Gemini 1.0 Pro Vision và GPT-4V. Do quy mô lớn của tập kiểm tra của chúng tôi, chúng tôi lấy mẫu ngẫu nhiên một tập con gồm 500 trường hợp để đánh giá hai mô hình này nhằm giảm chi phí, với các điểm số tương ứng được tô màu xám. Chi tiết về các mô hình được đánh giá và các nhắc tác vụ được sử dụng được cung cấp trong Phụ lục B.

Cài đặt Huấn luyện Để điều tra liệu huấn luyện trong lĩnh vực có thể nâng cao khả năng của mô hình, chúng tôi huấn luyện Qwen-VL-Chat-7B trên ArXivCap sử dụng cùng cài đặt như trong §4.1.1. Để phù hợp với giới hạn độ dài đầu vào, chúng tôi đặt số lượng hình tối đa mỗi mẫu là bốn. Quá trình huấn luyện mất 70 giờ với 8 NVIDIA A100.

Thước đo BLEU-2 (Papineni et al., 2002), ROUGE-L (Lin, 2004) và BERT-Score (Zhang et al., 2020) được áp dụng làm thước đo đánh giá tự động. Chúng tôi cũng khám phá việc sử dụng GPT-4 để hỗ trợ đánh giá chú thích. Các phát hiện của chúng tôi trong Phụ lục B.3 cho thấy rằng điểm ROUGE-L và BLEU-2 có tương quan cao với các chú thích của GPT-4. Chúng tôi chủ yếu sử dụng ba thước đo này do sự tiện lợi của chúng. Một phân tích lỗi thủ công được tiến hành để bổ sung cho các thước đo tự động (§4.3).

4.2.3 Kết quả
Kết quả Chú thích Hình Đơn lẻ Kết quả đánh giá cho tác vụ chú thích hình đơn lẻ được trình bày trong Bảng 5. Mặc dù đạt được hiệu suất gần hoàn hảo trên các tác vụ chú thích hình ảnh thông thường như MSCOCO (Lin et al., 2014), các LVLM mã nguồn mở, như các mô hình LLaVA, đối mặt với thử thách khi được áp dụng cho các hình ảnh học thuật. Đối với các mô hình đóng, GPT-4V thực hiện tương đương với

Mô hình BLEU-2 ROUGE-L BERT-S
BLIP-2-OPT-6.7B 2.1 7.1 81.1
InstructBLIP-Vicuna7B 3.7 10.1 83.3
LLaVA-v1.5-7B 2.3 10.6 83.0
LLaVA-v1.5-13B 2.6 10.7 83.3
OpenFlamingo-9B 5.7 9.9 82.4
IDEFICS-Instruct-9B 2.5 9.1 83.5
Qwen-VL-Chat-7B 4.4 11.1 81.8
Qwen-VL-Chat-7B ArXivCap 8.9 15.8 83.3
Gemini 1.0 Pro Vision 5.6 14.5 82.2
GPT-4V 5.5 14.2 83.3

Bảng 5: Kết quả đánh giá chú thích hình đơn lẻ. Kết quả màu xám được thu được từ một tập con 500 mẫu. Mặc dù hầu hết LVLM gặp khó khăn trong việc tạo ra các chú thích chất lượng cao của các hình ảnh khoa học, việc huấn luyện với ArXivCap làm tăng đáng kể hiệu suất.

Mô hình BLEU-2 ROUGE-L BERT-S
Qwen-VL-Chat-7B 4.4 11.1 81.8
+ Tiêu đề 5.7 13.1 81.6
+ Tiêu đề và Tóm tắt 6.0 12.7 81.4
Qwen-VL-Chat-7B ArXivCap 8.9 15.8 83.3
+ Tiêu đề 12.9 18.6 83.8
+ Tiêu đề và Tóm tắt 12.7 18.5 83.8

Bảng 6: Kết quả đánh giá chú thích hình đơn lẻ với thông tin meta bài báo.

Gemini 1.0 Pro Vision. Hơn nữa, việc huấn luyện liên tục trên bộ dữ liệu của chúng tôi mang lại sự thúc đẩy hiệu suất đáng kể cho tác vụ này. Ví dụ, tinh chỉnh dẫn đến sự tăng đáng chú ý trong điểm BLEU-2 từ 4.4 lên 8.9, cho thấy một hướng đầy hứa hẹn để nâng cao khả năng hiểu hình ảnh học thuật thông qua huấn luyện chuyên biệt theo lĩnh vực. Chúng tôi cũng điều tra liệu việc cung cấp thông tin bối cảnh bổ sung, như tiêu đề bài báo và tóm tắt, có thể giúp các mô hình tạo ra chú thích hình tốt hơn. Như thể hiện trong Bảng 6, việc thêm tiêu đề có lợi được chứng minh bởi các điểm số được thúc đẩy, trong khi việc cung cấp tóm tắt mang lại lợi ích không đáng kể.

Kết quả Chú thích Nhiều-Hình Như thể hiện trong khối đầu tiên của Bảng 7, tương tự như chú thích hình đơn lẻ, chú thích nhiều hình ảnh đặt ra thử thách cho các LVLM mã nguồn mở hiện tại. Ví dụ, Qwen-VL-Chat chỉ đạt được 3.0 BLEU-2 và 7.2 điểm ROUGE-L trên tác vụ này, thấp hơn đáng kể so với hiệu suất của nó trong chú thích hình đơn lẻ. Ngược lại, GPT-4V liên tục thể hiện thành thạo trong cả hai tác vụ, cho thấy khả năng cân bằng để nắm bắt ngữ nghĩa trên nhiều hình ảnh. Đáng chú ý, việc huấn luyện trên bộ dữ liệu ArXivCap của chúng tôi mang lại những cải thiện rõ rệt hơn

--- TRANG 7 ---
Mô hình Chú thích Nhiều-Hình Chú thích Theo Bối cảnh Tạo Tiêu đề
BLEU-2 ROUGE-L BERT-S BLEU-2 ROUGE-L BERT-S BLEU-2 ROUGE-L BERT-S
OpenFlamingo-9B 3.7 11.3 81.9 20.0 20.5 83.7 2.7 17.7 82.7
IDEFICS-Instruct-9B 3.6 10.8 82.8 20.7 22.6 85.7 3.5 18.4 85.8
Qwen-VL-Chat-7B 3.0 7.2 79.7 17.0 22.1 85.0 2.6 15.8 85.1
Qwen-VL-Chat-7B ArXivCap 10.6 18.0 83.6 16.1 21.2 84.8 6.7 23.5 86.8
Gemini 1.0 Pro Vision 6.1 16.2 83.1 10.2 20.2 84.5 5.7 21.8 85.9
GPT-4V 5.7 14.7 83.0 9.6 20.1 84.7 4.0 20.2 86.0

Bảng 7: Kết quả đánh giá ba tác vụ mới được định nghĩa. Kết quả tốt nhất được tô đậm.

Mô hình BLEU-2 ( ∆↓) ROUGE-L ( ∆↓)
Qwen-VL-Chat-7B 17.0 22.1
+ bối cảnh ngẫu nhiên 5.7 (66.5%) 13.0 (38.1%)
+ thay đổi thứ tự 12.0 (29.4%) 15.1 (31.7%)
Qwen-VL-Chat-7B ArXivCap 16.1 21.2
+ bối cảnh ngẫu nhiên 7.5 (53.4%) 14.3 (32.5%)
+ thay đổi thứ tự 14.1 (12.4%) 19.5 (8.0%)

Bảng 8: Hiệu suất chú thích theo bối cảnh bị ảnh hưởng bởi thứ tự. Sau khi tinh chỉnh trên ArXivCap, mô hình mạnh mẽ hơn đối với thứ tự của các chú thích lịch sử.

cho tác vụ này, đỉnh điểm là Qwen-VL-Chat thậm chí vượt qua hiệu suất của mô hình GPT-4V. Sự nâng cao này nhấn mạnh vai trò then chốt của bộ dữ liệu của chúng tôi trong việc tạo điều kiện cho LVLM nâng cao khả năng lý luận trên nhiều hình ảnh, dẫn đến việc tóm tắt các hình ảnh khoa học hiệu quả hơn.

Kết quả Chú thích Theo Bối cảnh Trong khối giữa của Bảng 7, chúng tôi thấy rằng IDEFICS-Instruct-9B đạt được hiệu suất tốt nhất trên tác vụ này. Thành tích này phần lớn được cho là do thành thạo đáng chú ý của nó trong việc tận dụng các manh mối bối cảnh, xuất phát từ việc huấn luyện trước rộng rãi của nó liên quan đến các cặp hình ảnh-văn bản xen kẽ (Laurençon et al., 2023). Thú vị thay, tinh chỉnh trên ArXivCap dẫn đến sự suy giảm hiệu suất nhỏ trên tất cả các thước đo, với GPT-4V cũng đạt được điểm số thấp nhất. Hiện tượng này có thể được cho là do xu hướng của các chú thích tuần tự thể hiện các mẫu tương tự, do đó ưa thích các mô hình hiệu quả tận dụng các manh mối bối cảnh. Chúng tôi thực hiện hai đánh giá thử thách hơn bằng cách (i) cung cấp các cặp bối cảnh từ bài báo khác và (ii) thay đổi ngẫu nhiên thứ tự của các cặp hình-chú thích trong bối cảnh. Như thể hiện trong Bảng 8, hiệu suất với bối cảnh ngẫu nhiên suy giảm đáng kể, xác nhận giả thuyết trước đó của chúng tôi. Thay vào đó, mô hình được tinh chỉnh thể hiện kết quả chú thích mạnh mẽ hơn dưới những cài đặt này, được chứng minh bởi sự giảm nhẹ 8% trên ROUGE-L so với 31% của mô hình gốc với thứ tự bối cảnh bị xáo trộn.

16.00%
42.00%
19.00% 23.00%
Loại
Chấp nhận được
Diễn giải Sai Bối cảnh Lỗi Nhận dạng
Đơn giản Hóa Quá mức

Hình 5: Phân tích thủ công của các chú thích được tạo ra.

Kết quả Tạo Tiêu đề Kết quả được trình bày trong khối cuối của Bảng 7. Đáng chú ý, tác vụ tạo tiêu đề đặt ra một thử thách đáng gờm, rõ ràng trong điểm BLEU-2 tổng thể thấp hơn đáng kể so với các tác vụ chú thích. Điều này cho thấy khó khăn nội tại trong việc tạo ra các dự đoán chính xác cho tiêu đề bài báo. Một bức tranh tương phản xuất hiện khi xem xét các thước đo ROUGE-L và BERT-Score, hoặc gần như phù hợp hoặc vượt qua hiệu suất trên các tác vụ chú thích. Điều này nhấn mạnh thành thạo của mô hình trong việc tạo ra kết quả liên quan đến ngữ nghĩa cho các hình được trình bày. Phù hợp với hai tác vụ trước đó, tinh chỉnh mô hình trên bộ dữ liệu của chúng tôi mang lại những nâng cao đáng kể cho tác vụ tạo tiêu đề. Điểm BLEU-2 tăng ấn tượng từ 2.6 lên 6.7, trong khi điểm ROUGE-L thấy sự tăng đáng khen ngợi từ 15.8 lên 23.5. Những phát hiện này làm nổi bật thử thách của việc tạo tiêu đề cho LVLM hiện tại và hiệu quả của bộ dữ liệu của chúng tôi trong việc cải thiện khả năng tạo tiêu đề chính xác của mô hình.

4.3 Phân tích
Đánh giá Thủ công các Chú thích Được tạo ra Chúng tôi tiến hành kiểm tra thủ công cho kết quả chú thích hình đơn lẻ. Để đảm bảo đánh giá có thông tin hơn, chúng tôi tập trung vào một bài báo từ lĩnh vực CS, tận dụng kiến thức lĩnh vực của chúng tôi để đánh giá chất lượng chú thích tốt hơn. Chất lượng của các chú thích được tạo ra

--- TRANG 8 ---
Câu hỏi: Khả năng nào có tỷ lệ phần trăm (%) cao nhất? A. Rec B. OCR C. Know D. Gen
Ground-Truth: A. Rec
Qwen-VL-Chat-7B: Tỷ lệ cao nhất là trong (b) Math.
Qwen-VL-Chat-7B Tinh chỉnh với ArXivQA: Câu trả lời là A. Biểu đồ cột cho thấy rằng khả năng 'Rec' (Recognition) có cột cao nhất, cho thấy nó có tỷ lệ phần trăm (%) cao nhất trong số các khả năng được liệt kê.

Câu hỏi: Đạo hàm của y tại x=6 là ____ so với tại x=8. Lựa chọn: (A). lớn hơn (B). bằng nhau (C). nhỏ hơn.
Ground-Truth: A. lớn hơn
Qwen-VL-Chat-7B: Đạo hàm của y tại x=6 là 6, tại x=8 là 8. Do đó, câu trả lời là (B) Bằng nhau.
Qwen-VL-Chat-7B Tinh chỉnh với ArXivQA: Đạo hàm của y theo x được cho bởi độ dốc của đường thẳng. Tại x=6, độ dốc dốc hơn so với tại x=8, cho thấy rằng giá trị của đạo hàm lớn hơn. Vậy câu trả lời là A.

Hình 6: ArXivQA cho phép mô hình không chỉ trả lời các câu hỏi liên quan đến hình ảnh khoa học trong các bài báo (trái) mà còn cải thiện khả năng hiểu toán học (phải). Mô hình không chỉ chọn các lựa chọn đúng mà còn đưa ra lý luận hợp lý.

được đánh giá bằng cách kiểm tra kỹ hình, chú thích ground-truth, tiêu đề bài báo và tóm tắt. Chúng tôi phân loại chú thích thành các loại chất lượng sau theo kiểm tra sơ bộ của chúng tôi: (1) Chấp nhận được, nơi các chú thích bao quát chính xác bản chất của hình ảnh khoa học, phù hợp với thông tin dự định của ground-truth; (2) Đơn giản Hóa Quá mức, các trường hợp nơi mô hình đơn giản hóa quá mức nội dung, cung cấp tổng quan rộng trong khi bỏ qua các chi tiết cụ thể và sắc thái có trong ground truth; (3) Lỗi Nhận dạng, nơi mô hình nhận dạng và mô tả không chính xác các yếu tố thị giác và văn bản chính trong hình ảnh khoa học, như màu sắc, giá trị số hoặc bối cảnh văn bản; và (4) Diễn giải Sai Bối cảnh, nơi mô hình diễn giải sai bối cảnh cụ thể của hình ảnh khoa học, dẫn đến các chú thích có liên quan theo nghĩa chung nhưng không chính xác cho hình đã cho. Các chú thích được tạo ra được trực quan hóa của các loại khác nhau được thể hiện trong Hình 14 của Phụ lục C.1. Kết quả của 100 chú thích được kiểm tra thủ công được mô tả trong Hình 5, tiết lộ rằng chỉ 16% chú thích được coi là chấp nhận được khi so sánh với những chú thích được viết bởi con người. Trong số các chú thích không thỏa đáng, diễn giải sai bối cảnh nổi lên như vấn đề chủ yếu, cho thấy nhu cầu tích hợp thêm thông tin bối cảnh như được đề xuất trong Bảng 6. Đơn giản hóa quá mức là một mối quan tâm khác, với các chú thích chung chung được xác định. Ngoài ra, 23% và 19% chú thích được kiểm tra lần lượt gặp phải vấn đề đơn giản hóa quá mức và lỗi nhận dạng trong các số/văn bản được báo cáo trong chú thích. Vấn đề trước được cho là do chú thích đơn giản xuất hiện thường xuyên cao trong bộ dữ liệu huấn luyện và vấn đề sau có thể được giải quyết thông qua tích hợp tiềm năng với kết quả OCR. Đánh giá thủ công của chúng tôi cho thấy các nỗ lực tương lai có thể được hưởng lợi từ việc tích hợp các manh mối bối cảnh bổ sung, như siêu dữ liệu bài báo, cải thiện khả năng nhận thức cơ bản của mô hình và sử dụng thông tin bên ngoài.

Nghiên cứu Trường hợp của MathVista Chúng tôi tiến hành các nghiên cứu trường hợp để làm sáng tỏ các hiệu ứng tinh chỉnh được tạo điều kiện bởi bộ dữ liệu ArXivQA của chúng tôi. Trong phần bên trái của Hình 6, ArXivQA giúp mô hình trả lời chính xác một câu hỏi liên quan đến biểu đồ cột được trình bày. Phần bên phải trong Hình 6 chứng minh rằng ArXivQA có thể nâng cao khả năng lý luận đại số. Ở đây, một câu hỏi liên quan đến đạo hàm của một hàm được trả lời đúng, kèm theo một lý luận sáng suốt. Hình 15 trong Phụ lục C.2 làm nổi bật một bài toán hình học thử thách nơi cả hai mô hình tạo ra đầu ra ảo giác. Những trường hợp minh họa này cùng nhau khẳng định hiệu quả của bộ dữ liệu của chúng tôi.

5 Kết luận
Công trình của chúng tôi giới thiệu Multimodal ArXiv, bao gồm ArXivCap và ArXivQA, nhằm mục đích thúc đẩy khả năng hiểu khoa học của LVLM. Thí nghiệm cho thấy rằng tinh chỉnh trên ArXivQA nâng cao đáng kể khả năng lý luận toán học của LVLM. Hơn nữa, các đánh giá toàn diện của chúng tôi trên bốn tác vụ thị giác sang văn bản trên ArXivCap nhấn mạnh những thử thách trong việc hiểu các hình ảnh khoa học cho LVLM, đồng thời làm nổi bật những cải thiện đáng kể đạt được bởi huấn luyện trong lĩnh vực. Phân tích lỗi của chúng tôi cung cấp những hiểu biết có giá trị cho sự phát triển liên tục của LVLM.

--- TRANG 9 ---
Hạn chế
Nghiên cứu của chúng tôi có một số hạn chế đáng chú ý. Thứ nhất, khám phá của chúng tôi có thể không bao gồm toàn bộ phổ LVLM do sự phát triển nhanh chóng của các kiến trúc và phương pháp huấn luyện như tinh chỉnh hiệu quả tham số (Hu et al., 2022; Ma et al., 2024). Tuy nhiên, chúng tôi tin rằng bộ dữ liệu của chúng tôi vẫn có thể hiệu quả cho các LVLM khác và các phát hiện có thể tổng quát hóa. Chúng tôi cho thấy rằng bộ dữ liệu ArXivQA của chúng tôi cũng có thể thúc đẩy các mô hình dòng LLaVA trên các benchmark hiểu khoa học trong Phụ lục D. Thứ hai, bộ dữ liệu Multimodal ArXiv của chúng tôi lấy nguồn từ các bài báo ArXiv do khả năng tiếp cận và giấy phép mã nguồn mở của chúng. Cách tiếp cận này có thể bỏ qua sự đa dạng của các ngành và phương thức dữ liệu có trong tài liệu khoa học rộng lớn hơn. Nghiên cứu tương lai có thể tích hợp một loạt các bộ dữ liệu và lĩnh vực rộng hơn để làm phong phú phạm vi bao phủ kiến thức khoa học, và khám phá các phương pháp lựa chọn dữ liệu động để cải thiện hiệu suất và hiệu quả mẫu (Li et al., 2021; Chen et al., 2024).

Lời cảm ơn
Chúng tôi muốn cảm ơn tất cả các nhà đánh giá ẩn danh vì những nhận xét và đề xuất sâu sắc của họ, điều này đã giúp chúng tôi cải thiện chất lượng và sự rõ ràng của công trình này. Chúng tôi đặc biệt biết ơn Shuhuai Ren vì phản hồi có giá trị của anh ấy trong việc chuẩn bị bản thảo. Nghiên cứu này được hỗ trợ một phần bởi chương trình nghiên cứu chung của Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (NSFC) và Hội đồng Tài trợ Nghiên cứu (RGC) dưới số tài trợ N HKU714/21.

Tài liệu tham khảo
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, và Karen Simonyan. 2022. Flamingo: một mô hình ngôn ngữ thị giác cho học few-shot. ArXiv preprint, abs/2204.14198.

Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, và Ludwig Schmidt. 2023. Openflamingo: Một khung mã nguồn mở để huấn luyện các mô hình ngôn ngữ thị giác tự động lớn. ArXiv preprint, abs/2308.01390.

Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, và Tianhang Zhu. 2023a. Báo cáo kỹ thuật Qwen. ArXiv preprint, abs/2309.16609.

Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, và Jingren Zhou. 2023b. Qwen-vl: Một mô hình thị giác-ngôn ngữ lớn tiên phong với khả năng đa năng. ArXiv preprint, abs/2308.12966.

Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, và Sağnak Taşırlar. 2023. Giới thiệu các mô hình đa phương thức của chúng tôi.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020a. Các mô hình ngôn ngữ là những người học few-shot. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020b. Các mô hình ngôn ngữ là những người học few-shot. Trong Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, và Ryan Rossi. 2020. Chú thích hình ảnh với bản đồ quan hệ để lý luận. Trong Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1537-1545.

Charles Chen, Ruiyi Zhang, Eunyee Koh, Sungchul Kim, Scott Cohen, Tong Yu, Ryan Rossi, và Razvan Bunescu. 2019. Chú thích hình ảnh với lý luận và huấn luyện cấp độ chuỗi. ArXiv preprint, abs/1906.02850.

Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, và Dahua Lin. 2023a. Sharegpt4v: Cải thiện các mô hình đa phương thức lớn với chú thích tốt hơn. ArXiv preprint, abs/2311.12793.

Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, và Heng Huang. 2024. Mô hình thị giác-ngôn ngữ của bạn chính là một bộ lọc mạnh: Hướng tới tinh chỉnh hướng dẫn chất lượng cao với lựa chọn dữ liệu. ArXiv preprint, abs/2402.12501.

Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel M. Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim M. Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, và Radu Soricut. 2023b. Pali-x: Về việc mở rộng một mô hình thị giác và ngôn ngữ đa ngôn ngữ. ArXiv preprint, abs/2305.18565.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: Một chatbot mã nguồn mở gây ấn tượng với gpt-4 với chất lượng 90%* chatgpt.

Colin B Clement, Matthew Bierbaum, Kevin P O'Keeffe, và Alexander A Alemi. 2019. Về việc sử dụng arxiv như một bộ dữ liệu. ArXiv preprint, abs/1905.00075.

Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, và Steven Hoi. 2023. Instructblip: Hướng tới các mô hình thị giác-ngôn ngữ đa năng với tinh chỉnh hướng dẫn. ArXiv preprint, abs/2305.06500.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, và Zhifang Sui. 2022. Một khảo sát về học trong bối cảnh.

Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. 2023. Mme: Một benchmark đánh giá toàn diện cho các mô hình ngôn ngữ lớn đa phương thức. ArXiv preprint, abs/2306.13394.

--- TRANG 10 ---
Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, và Lingpeng Kong. 2023. G-llava: Giải quyết bài toán hình học với mô hình ngôn ngữ lớn đa phương thức.

Gemini Team. 2023. Gemini: một gia đình các mô hình đa phương thức có khả năng cao. ArXiv preprint, abs/2312.11805.

Google. 2023. Bard.

Ting-Yao Hsu, C Lee Giles, và Ting-Hao Huang. 2021. SciCap: Tạo chú thích cho các hình ảnh khoa học. Trong Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3258-3264.

Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, và Fei Huang. 2023. mplug-paperowl: Phân tích sơ đồ khoa học với mô hình ngôn ngữ lớn đa phương thức.

Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, và Weizhu Chen. 2022. Lora: Thích ứng thứ hạng thấp của các mô hình ngôn ngữ lớn. Trong The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.

ImageMagick Studio LLC. Imagemagick.

Kushal Kafle, Brian L. Price, Scott Cohen, và Christopher Kanan. 2018. DVQA: hiểu trực quan hóa dữ liệu thông qua trả lời câu hỏi. Trong 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 5648-5656.

Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, và Yoshua Bengio. 2017. Figureqa: Một bộ dữ liệu hình ảnh có chú thích cho lý luận thị giác. ArXiv preprint, abs/1710.07300.

Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D. Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen, và Daniel S. Weld. 2023. Nền tảng dữ liệu mở Semantic Scholar.

Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, và Victor Sanh. 2023.

--- TRANG 11 ---
Obelics: Một bộ dữ liệu được lọc quy mô web mở của các tài liệu hình ảnh-văn bản xen kẽ.

Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, và Jianfeng Gao. 2023a. Llava-med: Huấn luyện một trợ lý ngôn ngữ và thị giác lớn cho y sinh học trong một ngày. ArXiv preprint, abs/2306.00890.

Junnan Li, Dongxu Li, Silvio Savarese, và Steven Hoi. 2023b. Blip-2: Khởi động huấn luyện ngôn ngữ-hình ảnh với các bộ mã hóa hình ảnh đông lạnh và các mô hình ngôn ngữ lớn. ArXiv preprint, abs/2301.12597.

Lei Li, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, và Xu Sun. 2021. Chưng cất kiến thức động cho các mô hình ngôn ngữ được huấn luyện trước. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 379-389.

Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, và Lingpeng Kong. 2023c. Silkie: Chưng cất sở thích cho các mô hình ngôn ngữ thị giác lớn. ArXiv preprint, abs/2312.10665.

Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, và Qi Liu. 2023d. M3IT: Một bộ dữ liệu quy mô lớn hướng tới tinh chỉnh hướng dẫn đa phương thức đa ngôn ngữ. ArXiv preprint, abs/2306.04387.

Chin-Yew Lin. 2004. ROUGE: Một gói cho đánh giá tự động các tóm tắt. Trong Text Summarization Branches Out, pages 74-81.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, và C Lawrence Zitnick. 2014. Microsoft coco: Các đối tượng thông thường trong bối cảnh. Trong Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer.

Haotian Liu, Chunyuan Li, Yuheng Li, và Yong Jae Lee. 2023a. Các đường cơ sở được cải thiện với tinh chỉnh hướng dẫn thị giác.

Haotian Liu, Chunyuan Li, Qingyang Wu, và Yong Jae Lee. 2023b. Tinh chỉnh hướng dẫn thị giác. ArXiv preprint, abs/2304.08485.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, và Jianfeng Gao. 2023. Mathvista: Đánh giá lý luận toán trong bối cảnh thị giác với gpt-4v, bard và các mô hình đa phương thức lớn khác. ArXiv preprint, abs/2310.02255.

Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, và Ashwin Kalyan. 2022. Học để giải thích: Lý luận đa phương thức thông qua chuỗi suy nghĩ cho trả lời câu hỏi khoa học. Trong The 36th Conference on Neural Information Processing Systems (NeurIPS).

Xinyu Ma, Xu Chu, Zhibang Yang, Yang Lin, Xin Gao, và Junfeng Zhao. 2024. Tinh chỉnh hiệu quả tham số gần trực giao thông qua phép quay Givens. ArXiv preprint, abs/2404.04316.

Brielen Madureira. 2021. Flamingo và nhím trong sân croquet: Dạy đánh giá các hệ thống NLP cho sinh viên đại học. Trong Proceedings of the Fifth Workshop on Teaching NLP, pages 87-91.

OpenAI. 2023. Thẻ hệ thống Gpt-4v(ision).

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Huấn luyện các mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi của con người. Advances in Neural Information Processing Systems, 35:27730-27744.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: một phương pháp đánh giá tự động dịch máy. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, và Ilya Sutskever. 2021. Học các mô hình thị giác có thể chuyển giao từ giám sát ngôn ngữ tự nhiên. Trong Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763.

Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Mở khóa hiểu đa phương thức trên hàng triệu token bối cảnh. ArXiv preprint, abs/2403.05530.

Team Reka. 2024. Reka Core, Flash, và Edge: Một dòng các mô hình ngôn ngữ đa phương thức mạnh mẽ.

Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, và Aran Komatsuzaki. 2021. Laion-400m: Bộ dữ liệu mở của 400 triệu cặp hình ảnh-văn bản được lọc clip. ArXiv preprint, abs/2111.02114.

Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, và Trevor Darrell. 2023. Căn chỉnh các mô hình đa phương thức lớn với rlhf được tăng cường thực tế. ArXiv preprint, abs/2309.14525.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Các mô hình ngôn ngữ nền tảng mở và hiệu quả. ArXiv preprint, abs/2302.13971.

--- TRANG 12 ---
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, và Zhifang Sui. 2023. Các mô hình ngôn ngữ lớn không phải là những người đánh giá công bằng. ArXiv preprint, abs/2305.17926.

Zhiyang Xu, Ying Shen, và Lifu Huang. 2023. Multiinstruct: Cải thiện học zero-shot đa phương thức thông qua tinh chỉnh hướng dẫn. Trong Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 11445-11465.

Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, và Lijuan Wang. 2023a. Bình minh của lmms: Khám phá sơ bộ với gpt-4v(ision). ArXiv preprint, abs/2309.17421.

Zhishen Yang, Raj Dabre, Hideki Tanaka, và Naoaki Okazaki. 2023b. Scicap+: Một bộ dữ liệu tăng cường kiến thức để nghiên cứu các thử thách của chú thích hình ảnh khoa học. ArXiv preprint, abs/2306.03491.

Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Xinlong Wang, và Jingjing Liu. 2023. Capsfusion: Suy nghĩ lại dữ liệu hình ảnh-văn bản ở quy mô lớn. ArXiv preprint, abs/2310.20550.

Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, và Lijuan Wang. 2024. Mm-vet: Đánh giá các mô hình đa phương thức lớn cho khả năng tích hợp. Trong International conference on machine learning. PMLR.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, và Wenhu Chen. 2023. Mmmu: Một benchmark hiểu và lý luận đa phương thức đa ngành khổng lồ cho agi chuyên gia. ArXiv preprint, abs/2311.16502.

Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, và Dong Yu. 2024a. Mm-llms: Những tiến bộ gần đây trong các mô hình ngôn ngữ lớn đa phương thức. arXiv preprint arXiv:2401.13601.

Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. 2024b. Mathverse: Mô hình đa phương thức llm của bạn có thực sự nhìn thấy các sơ đồ trong bài toán toán thị giác không? ArXiv preprint, abs/2403.14624.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, và Yoav Artzi. 2020. Bertscore: Đánh giá tạo văn bản với BERT. Trong 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.

Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, và Tong Sun. 2023. Llavar: Tinh chỉnh hướng dẫn thị giác tăng cường cho hiểu hình ảnh giàu văn bản.

Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, và Mohamed Elhoseiny. 2023. Minigpt-4: Nâng cao hiểu thị giác-ngôn ngữ với các mô hình ngôn ngữ lớn tiên tiến. ArXiv preprint, abs/2304.10592.

A Chi tiết của Multimodal ArXiv
A.1 Làm sạch Chú thích
Chúng tôi áp dụng một công cụ Python để làm sạch chú thích gốc và Bảng 9 minh họa chú thích trước và sau khi làm sạch.

Trước khi Làm sạch Sau khi Làm sạch
Một hình ảnh H alpha Kính thiên văn Hale năm 1995 của Guitar Nebula (bộ lọc 20 angstrom tại 6564 angstrom). Cổ sao chổi kết nối với một bong bóng hình cầu được thể hiện rõ ràng. Ghi công: \cite{cha02}. Một hình ảnh H alpha Kính thiên văn Hale năm 1995 của Guitar Nebula (bộ lọc 20 angstrom tại 6564 angstrom). Cổ sao chổi kết nối với một bong bóng hình cầu được thể hiện rõ ràng. Ghi công: <cit.>.
Như Hình \ref{z0} ngoại trừ tại z∼6 (z= 4.37 trong mô hình EdS). Như Hình <ref> ngoại trừ tại z∼6 (z= 4.37 trong mô hình EdS).

Bảng 9: Chú thích trước và sau khi làm sạch bằng pylatexenc.

A.2 Minh họa Các trường hợp của Multimodal ArXiv
Chúng tôi cung cấp các trường hợp minh họa từ bộ dữ liệu của chúng tôi để hiểu rõ hơn. Hình 7 thể hiện một cặp hình-chú thích đơn lẻ điển hình, và Hình 8 cho thấy trường hợp chú thích nhiều hình. Hình 10, 11, 12 và 13 minh họa các trường hợp từ bộ dữ liệu ArXivQA của chúng tôi, bao gồm các loại hình khác nhau và chứa các câu hỏi đa dạng.

A.3 Đám mây Từ Chú thích
Chúng tôi trực quan hóa đám mây từ của các chú thích trong bộ dữ liệu ArXivCap của chúng tôi trong Hình 9. Có thể thấy rằng các chú thích có từ vựng đa dạng để mô tả các hình khác nhau trong các bài báo học thuật.

A.4 Mẫu Nhắc ArXivQA
Nhắc được sử dụng để truy vấn GPT-4V được cung cấp trong Bảng 11.

--- TRANG 13 ---
Hình:     
Chú thích: Ví dụ về biểu diễn đồ họa của một CRS. CRS bao gồm năm chất hóa học !={$,&,',(,)} và bốn phản ứng $+&→','+&→(,&+(→) và '+(→), được xúc tác bởi (,$,) và (, tương ứng. Tập thức ăn được cho bởi -={$,&}.

Các hình và chú thích là từ bài báo arxiv:1908.04642.

Hình 7: Một cặp chú thích hình đơn lẻ trong bộ dữ liệu ArXivCap của chúng tôi.

Lĩnh vực Tên Đầy đủ
dg-ga Hình học Vi phân
acc-phys Vật lý Gia tốc
solv-int Hệ thống Giải được Chính xác và Tích phân được
q-alg Đại số Lượng tử và Tô pô
atom-ph Vật lý Nguyên tử, Phân tử và Quang học
alg-geom Hình học Đại số
comp-gas Tế bào Tự động và Khí Mạng lưới
supr-con Siêu dẫn
chem-ph Vật lý Hóa học
mtrl-th Lý thuyết Vật liệu
adap-org Hệ thống Thích ứng, Tiếng ồn và Tự tổ chức
patt-sol Hình thành Mẫu và Soliton
chao-dyn Động lực học Hỗn loạn
cmp-lg Tính toán và Ngôn ngữ
econ Kinh tế
hep-lat Vật lý Năng lượng Cao - Mạng lưới
nucl-ex Thí nghiệm Hạt nhân
q-fin Tài chính Định lượng
math-ph Vật lý Toán học
nucl-th Lý thuyết Hạt nhân
gr-qc Tương đối Rộng và Vũ trụ học Lượng tử
hep-ex Vật lý Năng lượng Cao - Thí nghiệm
hep-th Vật lý Năng lượng Cao - Lý thuyết
nlin Khoa học Phi tuyến
hep-ph Vật lý Năng lượng Cao - Hiện tượng học
q-bio Sinh học Định lượng
quant-ph Vật lý Lượng tử
eess Kỹ thuật Điện và Hệ thống
stat Thống kê
astro-ph Vật lý Thiên văn
physics Vật lý
cond-mat Vật chất Ngưng tụ
math Toán học
cs Khoa học Máy tính

Bảng 10: Tên của mỗi lĩnh vực.

A.5 Phân tích Chất lượng của ArXivQA
Để đánh giá chất lượng của ArXivQA, chúng tôi đánh giá thủ công nó từ sáu khía cạnh khác nhau. Chúng tôi phát triển

Tạo Cặp Câu hỏi-Trả lời Trắc nghiệm cho Hình ảnh Khoa học

Hướng dẫn
Mục tiêu của nhiệm vụ này là tạo ra các câu hỏi trắc nghiệm có thể trả lời được dựa trên các hình ảnh từ các bài báo khoa học, để cải thiện khả năng của một mô hình ngôn ngữ thị giác lớn.

Các câu hỏi nên mang tính thử thách và yêu cầu lý luận cấp đại học. Loại câu hỏi nên đa dạng. Câu hỏi nên có thể trả lời được dựa trên hình ảnh. Câu trả lời nên là một trong các lựa chọn trả lời. Các lựa chọn trả lời nên hợp lý và mang tính thử thách.

Định dạng
Dưới đây là một ví dụ về định dạng của đầu vào và đầu ra cho nhiệm vụ.

Đầu vào
Hình ảnh: [Đầu vào hình ảnh trong nhiệm vụ]

Đầu ra
Câu hỏi: [Câu hỏi]
Lựa chọn Trả lời: [Các lựa chọn trả lời, một danh sách bullet.]
Lựa chọn Đúng: [Lựa chọn trả lời đúng, ví dụ A]
Lý luận: [Lý luận cho câu trả lời đúng, giải thích tại sao câu trả lời đúng]

Bảng 11: Nhắc được sử dụng cho GPT-4V để tạo ra các cặp QA dựa trên hình ảnh khoa học.

một hướng dẫn kiểm tra chất lượng cho các người chú thích, như thể hiện trong Bảng 12, giải quyết các khía cạnh khác nhau của các cặp QA. Chúng tôi lấy mẫu 100 ví dụ và yêu cầu bốn tác giả tiến hành phân tích chất lượng. Bốn tác giả được chia thành hai nhóm, với mỗi nhóm được giao nhiệm vụ đánh giá 50 ví dụ trên sáu khía cạnh phụ, theo giao thức chấm điểm.

Kết quả đánh giá được trình bày trong Bảng 13. Chúng tôi thấy rằng hầu hết các mẫu có hình ảnh rõ ràng, chất lượng cao với hình ảnh rõ ràng và chất lượng cao, với mô tả câu hỏi và lựa chọn không mơ hồ. Tuy nhiên, một phần nhỏ các câu hỏi được tạo ra có thể không thể trả lời được do nhận dạng sai các yếu tố trong hình ảnh, như được phản ánh bởi điểm căn chỉnh thực tế thấp hơn. Ngoài ra, chúng tôi coi các mẫu có tổng điểm 5 hoặc cao hơn từ cả hai người chú thích là có chất lượng đủ. Dưới tiêu chí nghiêm ngặt này, 79 trong số 100 mẫu đáp ứng ngưỡng, chứng minh rằng chất lượng của bộ dữ liệu nói chung là thỏa đáng.

B Chi tiết Đánh giá
B.1 Chi tiết về Các Mô hình Được Đánh giá
BLIP2 (Li et al., 2023b), giới thiệu một Q-Former nhẹ được thiết kế để bắc cầu khoảng cách phương thức và tận dụng các LLM đông lạnh. Tận dụng LLM, BLIP-2 có thể tiến hành tạo hình ảnh sang văn bản zero-shot bằng cách sử dụng các nhắc ngôn ngữ tự nhiên. Chúng tôi chọn phiên bản BLIP2-OPT-6.7B để đánh giá.³

InstructBLIP (Dai et al., 2023) sử dụng một mô-đun trích xuất đặc trưng thị giác nhận biết hướng dẫn dựa trên BLIP2 (Li et al., 2023b) và được huấn luyện với các bộ dữ liệu tinh chỉnh hướng dẫn đa phương thức thống nhất.

³https://huggingface.co/Salesforce/blip2-opt-6.7b

--- TRANG 14 ---
Hình phụ 1: 
Chú thích phụ 1: Hiệu suất của phương án phân bổ kênh phức tạp thấp được đề xuất dựa trên lý thuyết khớp.

Chú thích Chính: Các nút LoRa hoạt động được phân phối ngẫu nhiên trong một vòng tròn có bán kính 10 km, tần số trung tâm là 868 MHz, số kênh có sẵn là 3 với băng thông 125 kHz cho mỗi kênh, công suất truyền của mỗi người dùng LoRa từ 0 dBm đến 20 dBm.

Hình phụ 2: 
Chú thích phụ 2: Hiệu suất của phương án phân bổ công suất tối ưu được đề xuất cho các người dùng được phân bổ cho cùng một kênh.

Các hình và chú thích là từ bài báo arxiv:1908.04642. Hình và chú thích là từ bài báo arxiv:1810.10761.

Hình 8: Một cặp chú thích nhiều hình trong bộ dữ liệu ArXivCap của chúng tôi.

1. Đảm bảo Tính Toàn vẹn Thực tế và Trình bày Rõ ràng:
- Căn chỉnh Thực tế: Đảm bảo các câu hỏi và lựa chọn được dựa trên các phản ánh chính xác của dữ liệu biểu đồ.
- Rõ ràng Thị giác: Duy trì các biểu đồ độ phân giải cao để đảm bảo rằng tất cả các chi tiết liên quan đều có thể nhận biết được.
- Thông tin Văn bản Không mơ hồ: Sử dụng ngôn ngữ chính xác và không mơ hồ để công thức hóa các câu hỏi và câu trả lời, do đó giảm thiểu các diễn giải tiềm năng.

2. Đảm bảo Tính Liên quan và Tính Toàn diện Tích hợp:
- Tính Liên quan của Câu hỏi và Lựa chọn: Biểu đồ phải phù hợp với câu hỏi của chúng, và tất cả các lựa chọn nên có thể áp dụng và liên quan đến dữ liệu đã cho.
- Tích hợp Toàn diện: Đảm bảo việc cung cấp thông tin toàn diện cần thiết cho việc diễn giải biểu đồ và giải quyết câu hỏi, đảm bảo sự kết hợp mạch lạc của dữ liệu văn bản và thị giác.

3. Thúc đẩy Công bằng và Tránh Thiên vị:
- Nội dung Công bằng: Phấn đấu cho sự không thiên vị trong bộ dữ liệu để ngăn chặn thiên vị và đảm bảo đại diện công bằng của các nhóm và quan điểm đa dạng.

Giao thức Chấm điểm:
Mỗi tiêu chí sẽ được đánh giá nghiêm ngặt cho mỗi mục bộ dữ liệu. Đánh giá sẽ được tiến hành trên thang đo định tính với ba cấp độ khác biệt: Cao, Trung bình và Thấp.
Những cấp độ này sẽ biểu thị mức độ tuân thủ tiêu chí tương ứng:
- 1: Cao: Mục bộ dữ liệu thể hiện sự tuân thủ mẫu mực với tiêu chí đánh giá, chứng minh sự căn chỉnh mạnh mẽ và toàn diện với tiêu chuẩn được chỉ định.
- 0.5: Trung bình: Mục bộ dữ liệu đáp ứng tiêu chí đánh giá ở mức độ vừa phải, cho thấy sự phù hợp thỏa đáng nhưng không tối ưu với tiêu chuẩn.
- 0 Thấp: Mục bộ dữ liệu không đạt được tiêu chí đánh giá, báo hiệu nhu cầu cải thiện đáng kể để đáp ứng tiêu chuẩn.

Bảng 12: Hướng dẫn người chú thích về kiểm tra chất lượng thủ công ArXivQA.

Hình 9: Trực quan hóa đám mây từ của các chú thích trong bộ dữ liệu ArXivCap của chúng tôi.

Khía cạnh Điểm Trung bình
Căn chỉnh Thực tế 0.6975
Rõ ràng Thị giác 0.9925
Thông tin Văn bản Không mơ hồ 0.9825
Tính Liên quan của Câu hỏi và Lựa chọn 0.9375
Tích hợp Toàn diện 0.905
Nội dung Công bằng 1.0
Tổng Điểm 5.515

Bảng 13: Phân tích chất lượng thủ công của ArXivQA. Điểm trung bình cho mỗi khía cạnh được trình bày.

Chúng tôi chọn InstructBLIP-Vicuna-7B để đánh giá.⁴

LLaVA (Liu et al., 2023b), áp dụng các mô hình Vicuna làm xương sống LLM và được huấn luyện trên bộ dữ liệu tinh chỉnh hướng dẫn được tạo ra bởi ChatGPT/GPT-4.

⁴https://huggingface.co/Salesforce/instructblip-vicuna-7b

--- TRANG 15 ---
Câu hỏi: Tham số nào đang được đo trong cả hai thí nghiệm được mô tả trong các hình? Lựa chọn: A. Cường độ của xung laser B. Độ trễ thời gian tính bằng femtosecond (fs) C. Hiệu suất chuẩn hóa vi phân (Y_ND) D. Biên độ của thiết lập thí nghiệm
Nhãn: C
Lý luận: Trục Y trong cả hai hình được ghi nhãn là Y_ND, cho thấy hiệu suất chuẩn hóa vi phân là tham số được đo.

Hình 10: Một trường hợp từ bộ dữ liệu ArXivQA của chúng tôi.

Câu hỏi: Có thể suy ra điều gì về chuyển động của hạt trong hình c? Lựa chọn: A. Hạt di chuyển trong đường thẳng với vận tốc không đổi. B. Hạt thể hiện chuyển động tròn với vận tốc thay đổi. C. Các thành phần vận tốc của hạt độc lập với nhau. D. Hạt trải qua gia tốc đều trong đường tròn.
Nhãn: B
Lý luận: Trong hình c, quỹ đạo tạo thành một vòng khép kín với hướng chuyển động được chỉ ra, gợi ý chuyển động tròn. Khoảng cách khác nhau giữa các vòng liên tiếp ngụ ý rằng vận tốc đang thay đổi.

Hình 11: Một trường hợp từ bộ dữ liệu ArXivQA của chúng tôi.

Câu hỏi: Điều nào sau đây mô tả tốt nhất 'liên kết mục tiêu' trong bối cảnh của hình này? Lựa chọn: A. Một liên kết kết nối các nút có giá trị cao nhất B. Liên kết đang được kiểm tra để xây dựng đặc trưng C. Một liên kết được định nghĩa trước hướng dẫn việc xây dựng các liên kết khác D. Liên kết xuất hiện thường xuyên nhất trong mạng
Nhãn: B
Lý luận: 'Liên kết mục tiêu' được hiển thị với 'giá trị liên kết' p, tách biệt khỏi 'giá trị nút' L, gợi ý rằng đó là liên kết cụ thể đang được phân tích cho các đặc trưng của nó.

Hình 12: Một trường hợp từ bộ dữ liệu ArXivQA của chúng tôi.

Câu hỏi: Các bước nào trong quy trình trực tiếp liên quan đến việc chuẩn bị dữ liệu cho phân tích biểu hiện vi phân? Lựa chọn: A. Bước 1-2 B. Bước 7-12 C. Bước 13-14 D. Bước 15
Nhãn: C
Lý luận: Các bước trực tiếp liên quan đến việc chuẩn bị dữ liệu cho phân tích biểu hiện vi phân bao gồm "Đếm đặc trưng" để có được bảng đếm và các quy trình cấu trúc và chuẩn hóa dữ liệu tiếp theo được liệt kê dưới "Bước 14". Đây là những bước trực tiếp đi trước các phương pháp phân tích biểu hiện vi phân thực tế như "So sánh vi phân 2 nhóm" và "So sánh vi phân dựa trên GLM".

Hình 13: Một trường hợp từ bộ dữ liệu ArXivQA của chúng tôi.

ing dataset. LLaVA-v1.5 (Liu et al., 2023a) cải thiện trên các mô hình LLaVA bằng cách sử dụng các bộ dữ liệu tác vụ được tuyển chọn và một mô-đun căn chỉnh phương thức được cải thiện. Chúng tôi đánh giá cả LLaVA-v1.5-7B⁵ và

⁵https://huggingface.co/liuhaotian/llava-v1.5-7b LLaVA-v1.5-13B.⁶

Flamingo (Alayrac et al., 2022) tiên phong trong việc phát triển LVLM bằng cách giới thiệu một lớp cổng chéo cho LLM để tạo ra văn bản dựa trên thị giác. Bộ dữ liệu huấn luyện bao gồm dữ liệu thị giác và văn bản xen kẽ từ các trang web, cho phép nó tạo ra văn bản tự do như đầu ra. Chúng tôi chọn việc triển khai mã nguồn mở OpenFlamingo-9B (Awadalla et al., 2023) để đánh giá.⁷

IDEFICS là một triển khai mã nguồn mở khác của Flamingo (Alayrac et al., 2022). Được huấn luyện trên các cặp căn chỉnh hình ảnh-văn bản có sẵn công khai và các bộ dữ liệu tinh chỉnh hướng dẫn, nó thể hiện kết quả tương đương với mô hình nguồn đóng gốc trên các benchmark hình ảnh-văn bản khác nhau. Chúng tôi chọn IDEFICS-Instruct-9B để đánh giá.⁸

Qwen-VL-Chat (Bai et al., 2023b) là một LVLM song ngữ hỗ trợ cả tiếng Anh và tiếng Trung được xây dựng trên Qwen LLM (Bai et al., 2023a). Trong giai đoạn huấn luyện, Qwen-VL-Chat áp dụng chiến lược đóng gói để tạo ra nhiều hình ảnh làm đầu vào, cải thiện khả năng hiểu bối cảnh thị giác của nó. Chúng tôi chọn Qwen-VL-Chat-7B để đánh giá.⁹

GPT-4V (OpenAI, 2023), các mô hình thị giác-ngôn ngữ độc quyền được phát triển bởi OpenAI, được cho là mạnh mẽ trên nhiều tác vụ đa phương thức khác nhau (Yang et al., 2023a). Phiên bản API chúng tôi truy vấn là gpt-4-vision-preview.

Bard (Google, 2023), một LVLM thương mại được phát triển bởi Google. Chúng tôi sử dụng API không chính thức¹⁰ truy vấn mô hình với các nhắc tác vụ của chúng tôi, được truy cập vào 2023-11-17.

Gemini 1.0 Pro Vision (Reid et al., 2024), một LVLM được nâng cấp bởi Google. Chúng tôi sử dụng API chính thức truy vấn mô hình với các nhắc tác vụ của chúng tôi, được truy cập vào 2024-05-20.

⁶https://huggingface.co/liuhaotian/llava-v1.5-13b
⁷https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b
⁸https://huggingface.co/HuggingFaceM4/idefics-9b-instruct
⁹https://github.com/QwenLM/Qwen-VL
¹⁰https://github.com/dsdanielpark/Bard-API

--- TRANG 16 ---
B.2 Nhắc Tác vụ
Chúng tôi đánh giá tất cả các mô hình với cùng nhắc tác vụ trong các thí nghiệm của chúng tôi, và các nhắc cho bốn tác vụ của chúng tôi được liệt kê dưới đây:

Chú thích Hình Đơn lẻ: Tạo một chú thích cho hình được cung cấp.

Chú thích Nhiều-Hình: Tạo một chú thích cho các hình được cung cấp.

Chú thích Theo Bối cảnh: Chúng tôi tái sử dụng các nhắc trong các tác vụ chú thích trước đó tùy thuộc vào loại hình hiện tại.

Tạo Tiêu đề: Theo các hình và chú thích, tạo một tiêu đề cho bài báo này. Tiêu đề:

B.3 Đánh giá GPT-4 về Chú thích
Ngoài BLEU-2, ROUGE-L và BERT-S, chúng tôi cũng sử dụng GPT-4 để đánh giá một mẫu 500 chú thích được tạo ra. Cụ thể, chúng tôi sử dụng GPT-4 để đánh giá các tác vụ chú thích hình đơn lẻ theo FairEval (Wang et al., 2023). Mẫu để nhắc GPT-4 đánh giá các chú thích được tạo ra được trình bày trong Bảng 14. GPT-4 được yêu cầu thực hiện phân tích và sau đó tạo ra một điểm chất lượng, sau đó được ánh xạ vào thang từ 1 đến 5. Kết quả được trình bày trong Bảng 15. Chúng tôi quan sát rằng thước đo ROUGE-L thể hiện tương quan cao nhất với Điểm GPT-4 (Pearson r = 0.91), tiếp theo là BLEU-2 (Pearson r = 0.64). BERT-S thay vào đó thể hiện tương quan vừa phải (Pearson r = 0.39). Điểm GPT-4 thấp đồng đều trên tất cả các mô hình cho thấy rằng chúng gặp khó khăn trong việc tạo ra các chú thích thỏa đáng, điều này phù hợp với các phát hiện trong bài báo chính của chúng tôi. Đáng chú ý, việc huấn luyện trên ArXivCap dẫn đến cải thiện đáng kể 12% trong điểm GPT-4 so với mô hình Qwen-VL-Chat gốc, dẫn đến kết quả thuận lợi nhất trong đánh giá này.

C Phân tích Lỗi
C.1 Minh họa Loại Chú thích
Chúng tôi minh họa các chú thích của bốn loại trong bài báo chính của chúng tôi trong Hình 14. Chú thích Chấp nhận được cung cấp mô tả toàn diện về hình được trình bày. Chú thích đơn giản hóa quá mức quá ngắn so với chú thích được viết bởi con người gốc. Hơn nữa, như thể hiện trong khối thứ ba trong Hình 14, Diễn giải Sai Bối cảnh đề cập đến các chú thích có nội dung không được đề cập trong hình, như bộ dữ liệu được tô màu đỏ. Lỗi Nhận dạng biểu thị mô hình đã nhận dạng sai số hoặc văn bản trong hình, như tên mô hình bị nhận dạng sai trong khối cuối của Hình 14.

C.2 Mẫu Thất bại của MathVista
Hình 15 cho thấy một bài toán lý luận toán học hình học thử thách nơi cả hai mô hình thất bại trong việc tạo ra câu trả lời đúng. Echoing kết quả định lượng trong bài báo chính của chúng tôi, chúng tôi tin rằng tương lai

--- TRANG 17 ---
Hướng dẫn Chú thích:
Với tư cách là một người chú thích, vai trò của bạn là phục vụ như một thẩm phán không thiên vị và khách quan trong việc đánh giá độ chính xác của các chú thích được tạo ra bởi một Mô hình Thị giác-Ngôn ngữ Lớn (LVLM) cho các hình ảnh khoa học. Những hình ảnh này được trích xuất từ các bài báo học thuật, và để hỗ trợ đánh giá của bạn, chúng tôi sẽ cung cấp cho bạn tiêu đề và tóm tắt của bài báo để có bối cảnh cần thiết.

Bạn sẽ được trình bày với chú thích gốc-được gọi là 'ground truth'-và chú thích được tạo ra bởi LVLM, được gọi là 'prediction'. Bạn có thể tính đến bối cảnh được cung cấp bởi tiêu đề và tóm tắt của bài báo để có kiến thức nền, so sánh nó một cách phê phán với cả hai chú thích.

Trong đánh giá của bạn, vui lòng chú ý đến sự căn chỉnh thực tế, bao gồm nhưng không giới hạn ở các khía cạnh sau:
- Dữ liệu số và thống kê: Xác minh độ chính xác và sự tương ứng của chúng với dữ liệu được trình bày trong hình.
- Ký hiệu: Kiểm tra việc biểu diễn và sử dụng đúng trong bối cảnh của chủ đề khoa học.
- Nội dung thực tế: Đảm bảo tất cả các sự kiện đều nhất quán với những điều được nêu trong chú thích ground truth và nội dung của bài báo.

<title>tiêu đề</title>
<abstract>tóm tắt</abstract>
<ground truth>gt</ground truth>
<prediction>pred</prediction>

So sánh prediction với ground truth, cung cấp phân tích ngắn gọn, và gán một điểm sử dụng một trong các nhãn chất lượng sau: <Perfect>, <Good>, <Fair>, <Poor>, <Incorrect>.

Dưới đây chúng tôi mô tả tiêu chí chi tiết cho điểm: <Perfect>: Prediction gần như giống hệt ground truth, chỉ có những khác biệt nhỏ, không quan trọng không thay đổi ý nghĩa. Tất cả dữ liệu số, ký hiệu và nội dung thực tế đều chính xác và nhất quán.

<Good>: Prediction phần lớn tương tự như ground truth nhưng có một số khác biệt đáng chú ý có thể thay đổi nhẹ ý nghĩa. Tuy nhiên, thông tin cốt lõi vẫn đúng, và dữ liệu số, ký hiệu và nội dung thực tế phần lớn chính xác và nhất quán với nội dung hình.

<Fair>: Prediction nắm bắt được ý tưởng cơ bản của ground truth nhưng có những khác biệt đáng kể thay đổi ý nghĩa theo cách không thể bỏ qua. Có thể có một số điều không chính xác hoặc không nhất quán trong dữ liệu số, ký hiệu hoặc nội dung thực tế khi so sánh với nội dung hình.

<Poor>: Prediction liên quan đến ground truth nhưng có lỗi nghiêm trọng hoặc thiếu sót thay đổi đáng kể ý nghĩa. Dữ liệu số, ký hiệu hoặc nội dung thực tế có thể phần lớn không chính xác hoặc không nhất quán với nội dung hình.

<Incorrect>: Prediction hoàn toàn khác biệt hoặc không liên quan đến ground truth, không có sự tương đồng nào giữa hai bên. Dữ liệu số, ký hiệu và nội dung thực tế hoàn toàn không chính xác hoặc không nhất quán với nội dung hình.

Đưa ra phân tích ngắn gọn trong vòng 100 từ và sau đó xuất ra một nhãn chất lượng được bao quanh bởi "<>".

Bảng 14: Mẫu nhắc được thiết kế cho GPT-4 để đánh giá các chú thích được tạo ra dựa trên tiêu đề bài báo, tóm tắt và ground truth.

Chấp nhận được Đơn giản Hóa Quá mức Diễn giải Sai Bối cảnh

Được Viết bởi Con người: Kiến trúc mô hình khuếch tán đám mây điểm của chúng tôi. Hình ảnh được đưa qua một mô hình CLIP đông lạnh, được huấn luyện trước, và lưới đầu ra được đưa như các token vào transformer. Cả timestep $t$ và đầu vào nhiễu $x_t$ cũng được đưa vào như các token. Các token đầu ra tương ứng với $x_t$ được sử dụng để dự đoán $\epsilon$ và $\Sigma$.

Được Tạo ra: Kiến trúc của mô hình được đề xuất.

Được Viết bởi Con người: (A) Mối quan hệ giữa hiệu suất của mạng nơ-ron sâu và kích thước dữ liệu chúng tôi sử dụng. Chúng tôi cố định hàm kích hoạt là RELU và số lớp ẩn là 3, với 200 nút trong mỗi lớp. (B) So sánh hiệu suất của mạng nơ-ron sâu sử dụng các hàm kích hoạt khác nhau.

Được Tạo ra: Hiệu suất của mô hình với dữ liệu và hàm kích hoạt khác nhau. A: Hiệu suất của mô hình với tỷ lệ khác nhau của dữ liệu. B: Hiệu suất của mô hình với hàm kích hoạt khác nhau.

Được Viết bởi Con người: Hiệu suất trên tác vụ Decompensation cho các giá trị khác nhau của $\alpha$ trong $\NCLY$. Kết quả được tính trung bình trên 5 lần chạy và được huấn luyện với nhãn Decompensation. Bằng màu xanh, chúng tôi hiển thị độ chính xác tương phản. Đối với $\alpha > 0.95$ nó giảm vì tác vụ pre-text trở nên quá khó giải thích hiệu suất thấp của SCL.

Được Tạo ra: AUPRC và độ chính xác tương phản của mô hình end-to-end và mô hình có trọng số $\alpha$ trên tập kiểm tra của bộ dữ liệu 20 Newsgroups. Độ chính xác tương phản là độ chính xác trung bình của các dự đoán tương phản. Các thanh lỗi hiển thị độ lệch chuẩn.

Được Viết bởi Con người: Độ chính xác kiểm tra của việc triển khai ResNet và ResNet-B với lớp IC trên các bộ dữ liệu CIFAR10/100 theo các epoch huấn luyện.

Được Tạo ra: So sánh các đường cong lỗi kiểm tra của ResNet-110, ResNet-164 và ResNet-164 trên CIFAR10 và CIFAR100.

Lỗi Nhận dạng

Hình 14: Minh họa chú thích chấp nhận được và ba loại lỗi của các chú thích được tạo ra.

Mô hình BLEU-2 ROUGE-L BERT-S Điểm GPT-4
BLIP-2-OPT-6.7B 1.5 6.6 81.3 1.18
InstructBLIP-Vicuna7B 3.5 10.3 83.6 1.48
LLaVA-1.5-7B 2.3 10.4 83.3 1.80
LLaVA-1.5-13B 2.7 11.0 83.6 1.69
OpenFlamingo-9B 5.8 10.3 82.7 1.52
IDEFICS-Instruct-9B 2.1 9.3 83.8 1.55
Qwen-VL-Chat 4.7 11.1 82.0 1.81
Qwen-VL-Chat tinh chỉnh với ArXivCap 8.6 15.3 83.2 2.03

Bảng 15: Kết quả của 500 chú thích hình đơn lẻ được tạo ra bởi các mô hình khác nhau.

các nghiên cứu có thể tích hợp corpus tập trung hơn để nâng cao khả năng lý luận hình học và toán học của LVLM.

D Kết quả với Xương sống LLaVA
Chúng tôi điều tra liệu ArXivQA cũng có thể nâng cao các LVLM khác, như các mô hình LLaVA (Liu et al., 2023b). Để duy trì hiệu suất mô hình, chúng tôi kết hợp bộ dữ liệu ArXivQA của chúng tôi với bộ dữ liệu tinh chỉnh hướng dẫn LLaVA SFT 665K. LLaVA-v1.5-7B được áp dụng làm xương sống và mô hình được huấn luyện theo công thức gốc. Kết quả trên các benchmark khác nhau được liệt kê trong Bảng 16. Chúng tôi thấy rằng không chỉ hiệu suất lý luận khoa học được cải thiện trên các tác vụ lý luận đa phương thức (MathVista (Lu et al., 2023), MMMU (Yue et al.,

--- TRANG 18 ---
Mô hình MathVista MMMU(val) ScienceQA(IMG only) MM-Vet
LLaVA-v1.5-7B 26.6 35.3 66.8 30.5
SFT Gốc +ArXivQA 28.2 36.0 68.3 32.4

Bảng 16: Sau khi tinh chỉnh với sự kết hợp của ArXivQA và dữ liệu SFT gốc, mô hình LLaVA cho thấy khả năng lý luận toán học được thúc đẩy trên các benchmark.

Câu hỏi: Như thể hiện trong hình, AB là đường kính của ⊙O, CD là dây cung của ⊙O, ∠ADC = 26.0, thì độ của ∠CAB là (). Lựa chọn: A. 26° B. 74° C. 64° D. 54°
Ground-Truth: D. 54°
Qwen-VL-Chat: Chúng ta biết rằng một dây cung của đường tròn cắt đường tròn tại hai điểm, và số đo của góc được tạo bởi dây cung và bán kính bằng phần của chu vi đường tròn mà dây cung đại diện. Trong hình này, ∠ADC=26.0°, và ∠ACB= ∠CAD, nên ∠ACB + ∠AD=90°. Vì ∠ACB = ∠AD, chúng ta có ∠ACB + ∠ACB=90°, nên ∠ACB=45°. Sau đó ∠CAB = 90°-∠ACB = 90°-45°= 45°. Do đó, độ của ∠CAB là 45°. Vậy, câu trả lời là (A).
Qwen-VL-Chat-Tinh chỉnh: Câu trả lời là B. ∆ABC là tam giác 74°-64°, và ∆BCD là tam giác 54°-26°. Cộng hai góc này cho chúng ta 74°+ 54°= 128°, điều này không thể vì tổng các góc trong tam giác phải bằng 180°. Do đó, ∠CAB phải là 74°, đây là góc duy nhất phù hợp với các điều kiện đã cho.

Hình 15: Một trường hợp thất bại trên tác vụ giải quyết vấn đề hình học.

2023), và ScienceQA (Lu et al., 2022)), mà khả năng tổng thể trên MM-Vet (Yu et al., 2024) cũng được thúc đẩy. Cùng với kết quả của chúng tôi sử dụng Qwen-VL-Chat, những phát hiện này cho thấy rằng bộ dữ liệu ArXivQA của chúng tôi có thể nâng cao các xương sống mô hình khác nhau và có lợi trên nhiều benchmark khác nhau.
