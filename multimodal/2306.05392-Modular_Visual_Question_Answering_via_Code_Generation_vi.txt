<<<<<<< Updated upstream
# Trả lời câu hỏi trực quan theo mô-đun thông qua sinh mã

Sanjay Subramanian1 Medhini Narasimhan1 Kushal Khangaonkar1
Kevin Yang1 Arsha Nagrani2 Cordelia Schmid2 Andy Zeng2
Trevor Darrell1 Dan Klein1
1UC Berkeley 2Google Research

## Tóm tắt

Chúng tôi trình bày một khung xử lý công thức hóa trả lời câu hỏi trực quan như sinh mã theo mô-đun. Trái ngược với các nghiên cứu trước đây về các phương pháp tiếp cận theo mô-đun với VQA, phương pháp của chúng tôi không yêu cầu đào tạo bổ sung và dựa vào các mô hình ngôn ngữ được đào tạo trước (LM), các mô hình trực quan được đào tạo trước trên các cặp hình ảnh-chú thích, và 50 ví dụ VQA được sử dụng để học theo ngữ cảnh. Các chương trình Python được sinh ra gọi và kết hợp các đầu ra của các mô hình trực quan bằng cách sử dụng logic số học và điều kiện. Phương pháp của chúng tôi cải thiện độ chính xác trên tập dữ liệu COVR ít nhất 3% và trên tập dữ liệu GQA khoảng 2% so với đường cơ sở few-shot không sử dụng sinh mã.

## 1 Giới thiệu

Phạm vi lý luận cần thiết cho trả lời câu hỏi trực quan (VQA) là rất rộng lớn và đòi hỏi tổng hợp nhiều kỹ năng - từ việc gắn kết ngôn ngữ với pixel và lý luận không gian đến lý luận dựa trên thường thức và kiến thức. Xem xét câu hỏi "Xe ngựa có ở bên phải con ngựa không?". Để trả lời nhất quán các câu hỏi như vậy một cách chính xác, một hệ thống phải nhận ra rằng câu hỏi này là sự kết hợp của hai câu hỏi con: "Có con ngựa không?" và "Xe ngựa có ở bên phải con ngựa không?"

Việc mở rộng mô hình tinh chỉnh điển hình cho tất cả các kết hợp có thể của các kỹ năng lý luận là cực kỳ tốn kém về chi phí chú thích và khiến việc thêm kỹ năng vào một hệ thống đã được đào tạo trở nên khó khăn.

Mặt khác, các phương pháp tiếp cận theo mô-đun - từ các phương pháp cổ điển đến các mạng mô-đun thần kinh có thể vi phân (NMN) - cung cấp một con đường tiềm năng để tận dụng và mở rộng tính chất tổ hợp của lý luận trực quan như một phương tiện để tổng quát hóa: tức là sử dụng vô hạn các phương tiện hữu hạn. Tuy nhiên, các mô-đun của NMN vẫn phải được đào tạo cùng nhau trên một tập dữ liệu lớn, và cũng bị hạn chế ở chỗ chúng (i) yêu cầu một bộ phân tích cú pháp, phải được sửa đổi nếu các mô-đun được thêm vào hoặc loại bỏ khỏi hệ thống, và (ii) yêu cầu đào tạo lại nếu một mô-đun được thay thế.

Trong nghiên cứu này, chúng tôi điều tra một lớp phương pháp tiếp cận VQA theo mô-đun thay thế, trong đó dựa trên sự xuất hiện gần đây của các mô hình ngôn ngữ có khả năng cao ngay từ đầu (LM) và các mô hình ngôn ngữ trực quan (VLM), chúng tôi phát triển các hệ thống công thức hóa VQA như một vấn đề tổng hợp chương trình. Cụ thể, phương pháp CodeVQA của chúng tôi, được minh họa trong Hình 1, sử dụng các LM viết mã để nhận câu hỏi làm đầu vào và đưa ra mã để (i) điều phối một chuỗi các API nguyên thủy trực quan bao quanh các VLM để thăm dò hình ảnh để tìm các thông tin trực quan cụ thể (ví dụ: chú thích, vị trí pixel của các thực thể, hoặc điểm tương tự hình ảnh-văn bản), và (ii) lý luận về thông tin đó với biểu thức đầy đủ của mã Python (ví dụ: số học, cấu trúc logic, vòng lặp phản hồi, v.v.) để đưa ra câu trả lời. Từ góc độ thực tế, tính mô-đun của CodeVQA kết hợp với khả năng prompting few-shot của LM cho phép nó thích ứng với phạm vi rộng các phân phối nhãn VQA mong muốn mà không cần đào tạo mô hình bổ sung, và hưởng lợi từ việc thay thế các mô-đun riêng lẻ bằng các phiên bản cải tiến khi chúng có sẵn.

Chúng tôi đánh giá CodeVQA trong thiết lập VQA few-shot, đã thấy rất nhiều nghiên cứu gần đây. Phương pháp của chúng tôi vượt trội hơn các phương pháp trước đây ít nhất 3% trên tập dữ liệu COVR, yêu cầu lý luận trên nhiều hình ảnh, và khoảng 2% trên tập dữ liệu GQA. Kết quả của chúng tôi cho thấy rằng lợi ích của tính mô-đun với các mô hình sẵn có gần đây có thể được thực hiện trong VQA mà không cần đào tạo mô hình bổ sung.

## 2 Nghiên cứu liên quan

Một số phương pháp tiếp cận gần đây cho các nhiệm vụ lý luận bao gồm một LM viết chương trình và một trình thông dịch cho các chương trình này. Liang et al. (2022) áp dụng phương pháp này cho robotics. Cheng et al. (2023) giới thiệu một khung để lý luận kết hợp trên bảng, văn bản và hình ảnh, trong đó các hình ảnh được biểu diễn bằng chú thích hình ảnh. Subramanian et al. (2022) đã sử dụng một bộ phân tích cú pháp và các quy tắc được mã hóa cứng thay vì LM để tổng hợp các đầu ra từ CLIP cho việc hiểu biểu thức tham chiếu zero-shot; phát hiện của họ rằng CLIP không hữu ích cho các từ khóa không gian thúc đẩy phương pháp sinh mã của chúng tôi cho lý luận không gian.

Đồng thời với công trình của chúng tôi, các bài báo khác đã giới thiệu các khung tương tự cho VQA multi-hop. Những bài báo này nhầm lẫn lợi ích của tổng hợp chương trình với lợi ích của LM, các ví dụ theo ngữ cảnh, và các mô hình thị giác được sử dụng như nguyên thủy. Ngược lại, chúng tôi phân tích hiệu quả của tổng hợp chương trình bằng cách so sánh CodeVQA với một đường cơ sở few-shot dựa trên LM mạnh sử dụng cùng phương pháp chọn ví dụ theo ngữ cảnh. Hơn nữa, trong khi các khung này dựa vào các mô hình VQA có giám sát hoặc phát hiện đối tượng, chúng tôi cho thấy rằng chúng tôi có thể đạt được hiệu suất tương đương (trên tập dữ liệu GQA) chỉ sử dụng LM và các mô hình được đào tạo trước trên các cặp hình ảnh-văn bản.

## 3 VQA Few-shot thông qua sinh mã

Trong trả lời câu hỏi trực quan (VQA), các đầu vào của hệ thống là một hình ảnh và một câu hỏi và đầu ra là một câu trả lời dạng văn bản. Chúng tôi xem xét thiết lập VQA few-shot trong đó hệ thống chỉ có quyền truy cập vào một số lượng nhỏ (50) các thực thể VQA được chú thích bởi con người.

**Tổng quan.** Hình 1 minh họa phương pháp của chúng tôi. Cho một hình ảnh và câu hỏi tương ứng, CodeVQA đầu tiên sinh ra một chương trình Python chỉ sử dụng câu hỏi. Sau đó nó thực thi chương trình này, sử dụng hình ảnh khi cần thiết, để dự đoán câu trả lời. Chúng tôi đầu tiên định nghĩa tập hợp các nguyên thủy mã mà hệ thống của chúng tôi sử dụng (§ 3.1). Sau đó chúng tôi mô tả cách chúng tôi sinh ra một chương trình kết hợp các nguyên thủy này dựa trên câu hỏi (§ 3.2). Cuối cùng, chúng tôi liệt kê các mô hình được đào tạo trước mà chúng tôi sử dụng (§ 3.3).

### 3.1 Nguyên thủy mã

Các nguyên thủy định nghĩa các hoạt động cơ bản trên hình ảnh hoặc trên văn bản thường hữu ích cho VQA. Trong CodeVQA, chúng tôi sử dụng ba nguyên thủy, được định nghĩa bên dưới. Mỗi nguyên thủy này được thực hiện bằng cách sử dụng các mô hình khớp hình ảnh-văn bản (ITM), tương phản hình ảnh-văn bản (ITC), và chú thích hình ảnh, mỗi mô hình có thể được đào tạo chỉ với các cặp hình ảnh-chú thích. Sự khác biệt giữa ITM và ITC là ITC tính toán các nhúng hình ảnh và văn bản riêng biệt và lấy tích vô hướng, trong khi ITM thực hiện hợp nhất sớm trên các đặc trưng hình ảnh và văn bản và do đó tốn kém tính toán hơn. Chúng tôi lưu ý rằng khung của chúng tôi không bị ràng buộc với lựa chọn nguyên thủy này và có thể hỗ trợ các nguyên thủy khác, phức tạp hơn có thể tận dụng các khía cạnh khác của ngôn ngữ lập trình và thư viện bên thứ ba.

**query(image, question)** Hàm này trả lời một câu hỏi về hình ảnh đã cho. Việc triển khai hàm này của chúng tôi dựa trên PnP-VQA và PICa và được thực hiện với các bước sau: (1) sử dụng mô hình ITM, tính toán GradCAM giữa câu hỏi và hình ảnh (trung bình trên các token câu hỏi), (2) lấy mẫu K = 20 bản vá hình ảnh dựa trên điểm GradCAM của chúng, (3) sinh chú thích từ các bản vá được lấy mẫu sử dụng mô hình chú thích, (4) Lặp lại các bước (2) và (3) cho đến khi Cunique chú thích được sinh ra, và (5) dự đoán câu trả lời bằng cách prompting LM với câu hỏi, chú thích, và các ví dụ theo ngữ cảnh. Các ví dụ theo ngữ cảnh trong bước (5) được chọn như mô tả trong § 3.2. Khi tập dữ liệu liên quan đến lý luận trên nhiều hình ảnh, mỗi ví dụ theo ngữ cảnh có chú thích cho tất cả hình ảnh.

**get_pos(image, text)** Hàm này tính toán GradCAM giữa các token văn bản đã cho và hình ảnh sử dụng mô hình ITM và trả về cặp (x, y) tối đa hóa giá trị GradCAM. Lưu ý rằng việc ứng dụng GradCAM này khác với việc trong query vì chúng tôi không lấy trung bình trên tất cả các token câu hỏi. Xem Phụ lục B để biết thêm thông tin về cách chúng tôi tính toán bản đồ GradCAM.

**find_matching_image(images, text)** Trong thiết lập có nhiều hình ảnh được liên kết với mỗi câu hỏi, có những câu hỏi tham chiếu cụ thể đến một hình ảnh (ví dụ "Người phụ nữ đang cầm gì?"). Hàm này có thể được sử dụng để chọn hình ảnh có liên quan nhất từ tập hợp. Nó được thực hiện bằng cách chấm điểm mỗi hình ảnh với văn bản sử dụng mô hình ITC và chọn hình ảnh có điểm cao nhất.

### 3.2 Sinh mã

Trong giai đoạn đầu của CodeVQA, chúng tôi sinh ra một chương trình Python dựa trên câu hỏi. Sử dụng Python thay vì ngôn ngữ dành riêng cho miền có lợi thế vì (1) nó hỗ trợ số học cũng như luồng điều khiển bao gồm vòng lặp và câu lệnh if - tất cả đều được sử dụng trong các chương trình của chúng tôi - và (2) các LM lớn cho sinh mã (ví dụ Codex) đã được đào tạo trên một lượng lớn mã Python.

Chúng tôi xây dựng một prompt bao gồm một hướng dẫn, các hằng số định nghĩa kích thước của hình ảnh, và các câu lệnh import và tài liệu API (như một nhận xét mã) chỉ định các hàm có sẵn. Ngoài prompt, đầu vào cho LM cũng bao gồm các chương trình được chú thích bởi chuyên gia cho một số ví dụ theo ngữ cảnh.

Vì tất cả các chương trình được chú thích không thể vừa vào một đầu vào duy nhất cho mô hình, chúng tôi phải chọn chương trình nào để sử dụng như các ví dụ theo ngữ cảnh cho mỗi câu hỏi kiểm tra. Theo Wang et al. (2022), chúng tôi sử dụng nhúng câu để truy xuất các câu hỏi tương tự nhất cho mỗi câu hỏi kiểm tra.

### 3.3 Các mô hình thành phần

Phương pháp của chúng tôi dựa vào bốn mô hình được đào tạo trước: một mô hình sinh mã, một mô hình ITM, một mô hình ITC, một mô hình IC, và một LM trả lời câu hỏi để trả lời câu hỏi dựa trên chú thích. Chúng tôi sử dụng mô hình code-davinci-002 thông qua OpenAI API cho cả việc sinh chương trình và trả lời câu hỏi. Chúng tôi sử dụng các mô hình BLIP được tinh chỉnh cho ITM, ITC, và chú thích.

## 4 Thí nghiệm

### 4.1 Chi tiết triển khai

Xem Phụ lục C để biết chi tiết triển khai.

### 4.2 Tập dữ liệu

Tập dữ liệu GQA chứa các câu hỏi multi-hop được sinh ra từ các đồ thị cảnh được chú thích bởi con người của các hình ảnh riêng lẻ trong Visual Genome. Tập dữ liệu COVR chứa các câu hỏi multi-hop về tập hợp hình ảnh trong các tập dữ liệu Visual Genome và imSitu. Những câu hỏi này được sinh ra tổng hợp từ các mẫu và sau đó được diễn giải lại bởi con người. Trừ khi được chỉ định khác, chúng tôi trình bày kết quả trên các câu hỏi được diễn giải lại. Tập dữ liệu NLVR2 chứa các câu lệnh về các cặp hình ảnh, và nhiệm vụ là xác định xem mỗi câu lệnh là đúng hay sai (chúng tôi diễn giải lại các câu lệnh thành câu hỏi trước khi đưa vào các phương pháp mà chúng tôi đánh giá). Phụ lục H có thêm chi tiết về các tập dữ liệu. Đối với mỗi tập dữ liệu trong ba tập dữ liệu, chúng tôi đã viết chương trình cho 50 câu hỏi được lấy mẫu ngẫu nhiên từ tập huấn luyện tương ứng. Trừ khi được nêu khác, chúng tôi đặt 12 ví dụ theo ngữ cảnh trong một prompt cho tập dữ liệu hình ảnh đơn và 6 ví dụ theo ngữ cảnh trong một prompt cho tập dữ liệu nhiều hình ảnh (vì bao gồm chú thích cho nhiều hình ảnh làm tăng kích thước ngữ cảnh cần thiết cho mỗi ví dụ). Chúng tôi báo cáo độ chính xác khớp chính xác của các câu trả lời viết thường.

### 4.3 Đường cơ sở

Đường cơ sở chính của chúng tôi là một sự thích ứng của PnP-VQA với thiết lập few-shot. Chúng tôi gọi nó là "Few-shot PnP-VQA." Đường cơ sở này tương đương với việc chạy quy trình truy vấn năm bước được mô tả trong § 3.1 cho mọi câu hỏi. Chúng tôi cũng so sánh với các phương pháp zero-shot và few-shot từ công trình trước đây và đồng thời. Phụ lục D chứa thêm chi tiết về những phương pháp đó.

### 4.4 Kết quả

Bảng 1 cho thấy kết quả trên ba tập dữ liệu. CodeVQA có độ chính xác cao nhất trong số các kỹ thuật few-shot. So với Few-shot PnP-VQA, nó có hiệu suất tốt hơn đáng kể trên COVR, điều này có lý vì trong tập dữ liệu này, phương pháp đường cơ sở phải kết hợp thông tin qua các chú thích hình ảnh cho nhiều hình ảnh khi được đưa ra một prompt duy nhất. Mặt khác, phương pháp của chúng tôi lặp qua các hình ảnh và truy vấn một hình ảnh duy nhất mỗi lần hoặc chọn hình ảnh có liên quan nhất đến câu hỏi. Thực sự, Bảng 3 cho thấy rằng CodeVQA có lợi thế lớn nhất trên các thực thể liên quan đến 4 hoặc 5 hình ảnh. So với công trình đồng thời cũng sử dụng tổng hợp chương trình, CodeVQA thường thực hiện tốt hơn, có thể do sự khác biệt phương pháp luận như truy xuất ví dụ theo ngữ cảnh của chúng tôi hoặc do chi tiết triển khai.

Hình 2 cho thấy so sánh định tính của CodeVQA và đường cơ sở Few-shot PnP-VQA trên tập dữ liệu COVR. CodeVQA trả lời câu hỏi chính xác bằng cách trả lời một câu hỏi đơn giản hơn cho mỗi hình ảnh và so sánh các câu trả lời, trong khi Few-shot PnP-VQA trả lời không chính xác mặc dù tạo ra chú thích với thông tin cần thiết.

### 4.5 Ablation

Bảng 2 so sánh truy xuất ví dụ theo ngữ cảnh dựa trên nhúng với truy xuất ngẫu nhiên. Sự cải thiện của CodeVQA so với Few-shot PnP-VQA lớn hơn khi các ví dụ theo ngữ cảnh được truy xuất bằng nhúng. Truy xuất dựa trên nhúng cung cấp một cách thức có hệ thống để thu thập các ví dụ theo ngữ cảnh có liên quan thay vì tuyển chọn một tập hợp ví dụ duy nhất.

Trong Phụ lục F, chúng tôi bao gồm các ablation cho LM trả lời câu hỏi và cho số lượng shot trong prompt cũng như kết quả trên các tập xác thực. Bảng 4 cho thấy rằng CodeVQA cải thiện so với Few-shot PnP-VQA khi sử dụng code-davinci-002 hoặc text-davinci-003 làm LM trả lời câu hỏi. Bảng 5 cho thấy độ chính xác khá ổn định khi số lượng ví dụ theo ngữ cảnh thay đổi.

### 4.6 Phân tích

Hình 3 phân tích độ chính xác theo loại câu hỏi. Sự cải thiện lớn nhất của CodeVQA (khoảng 30%) là trong tập con gồm các câu hỏi về vị trí đối tượng trái/phải hoặc trên/dưới. Cũng có sự cải thiện trong các câu hỏi "và" và "hoặc". Sự cải thiện này có thể liên quan đến phát hiện gần đây rằng LM hưởng lợi từ việc chuyển đổi multi-hop thành các câu hỏi single-hop.

Chúng tôi đã phân tích các nguồn lỗi trong CodeVQA trên 100 ví dụ trong tập xác thực COVR mà CodeVQA trả lời không chính xác: chú thích không liên quan (31%), lỗi trong find_matching_image (12%), lỗi sinh chương trình (14%), lỗi trả lời câu hỏi (25%), câu trả lời dự đoán có thể được coi là chính xác (14%), ground-truth không rõ ràng/không chính xác (16%), và lỗi số (1%). Lưu ý rằng các danh mục này không loại trừ lẫn nhau, và 13 trong số 100 ví dụ được đánh dấu với nhiều danh mục. Do đó, nhiều lỗi hơn là do việc thực thi các mô-đun hơn là sinh chương trình.

## 5 Kết luận

Trong bài báo này, chúng tôi đã giới thiệu một khung cho VQA few-shot theo mô-đun. Phương pháp của chúng tôi prompts một LM để sinh ra một chương trình Python gọi các mô-đun trực quan được đào tạo trước và kết hợp các đầu ra của những mô-đun này để dự đoán câu trả lời. Không giống như các kỹ thuật VQA theo mô-đun trước đây, khung này không yêu cầu (tái) đào tạo các mô-đun hoặc một bộ phân tích cú pháp. Ngoài ra, việc có được các đầu ra mô-đun có thể diễn giải từ các phương pháp mô-đun trước đây là không dễ dàng, trong khi trong phương pháp của chúng tôi, các mô-đun được đóng băng và do đó có thể diễn giải. CodeVQA cũng có thể được xem như mở rộng các hệ thống pipeline cho biểu thức đầy đủ của mã. Phương pháp của chúng tôi thể hiện những lợi ích thực nghiệm, thúc đẩy nghiên cứu tương lai về VQA few-shot theo mô-đun.

## 6 Hạn chế

Trong khi các kết quả ban đầu là đầy hứa hẹn, độ chính xác của phương pháp chúng tôi vẫn thấp hơn độ chính xác VQA của con người và các mô hình được tinh chỉnh trên các tập dữ liệu VQA, điều này cho thấy rằng vẫn có thể có tiến bộ đáng kể phải được thực hiện trước khi các phương pháp VQA few-shot với tổng hợp mã hữu ích cho các ứng dụng thực tế trong thế giới thực. Ngoài ra, cần thêm nghiên cứu về việc mở rộng khung cho các nguyên thủy bổ sung, vì kết quả trong Phụ lục G cho thấy rằng việc làm như vậy không luôn dẫn đến cải thiện so với phương pháp đường cơ sở. Một hạn chế khác của phương pháp chúng tôi là nó dựa vào các LM lớn có khả năng, có thể bị hạn chế trong sử dụng do yêu cầu tính toán hoặc chi phí (ví dụ thông qua các API có sẵn). Chúng tôi cũng tập trung trong công trình này vào việc đánh giá khả năng VQA với tiếng Anh là ngôn ngữ chính - nghiên cứu tương lai có thể mở rộng điều này sang các ngôn ngữ khác thông qua các LM đa ngôn ngữ.
=======
# 2306.05392.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2306.05392.pdf
# Kích thước tệp: 2056458 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Trả lời Câu hỏi Thị giác Mô-đun thông qua Sinh mã
Sanjay Subramanian1Medhini Narasimhan1Kushal Khangaonkar1
Kevin Yang1Arsha Nagrani2Cordelia Schmid2Andy Zeng2
Trevor Darrell1Dan Klein1
1UC Berkeley2Google Research
{sanjayss,medhini,kushaltk,yangk,trevordarrell,klein}@berkeley.edu,
{anagrani,cordelias,andyzeng}@google.com

Tóm tắt
Chúng tôi trình bày một khung công việc xây dựng
trả lời câu hỏi thị giác như sinh mã mô-đun.
Khác với các nghiên cứu trước về các phương pháp
mô-đun cho VQA, phương pháp của chúng tôi
không yêu cầu huấn luyện bổ sung và dựa vào
các mô hình ngôn ngữ được huấn luyện trước (LM),
các mô hình thị giác được huấn luyện trước trên các
cặp ảnh-chú thích, và năm mươi ví dụ VQA được
sử dụng cho học trong ngữ cảnh. Các chương trình
Python được sinh ra gọi và kết hợp các đầu ra của
các mô hình thị giác sử dụng logic số học và điều kiện.
Phương pháp của chúng tôi cải thiện độ chính xác trên
tập dữ liệu COVR ít nhất 3% và trên tập dữ liệu GQA
khoảng 2% so với đường cơ sở few-shot không sử dụng
sinh mã.

1 Giới thiệu
Phạm vi lý luận cần thiết cho trả lời câu hỏi thị giác
(VQA) là rất lớn, và đòi hỏi sự tổng hợp nhiều kỹ năng
– từ gắn kết ngôn ngữ với pixel (Goyal et al., 2017;
Radford et al., 2021; Zhai et al., 2022) và lý luận không
gian (Hudson và Manning, 2019) đến lý luận thông thường
và dựa trên kiến thức (Marino et al., 2019). Xem xét câu
hỏi "Xe ngựa có ở bên phải con ngựa không?". Để trả lời
nhất quán các câu hỏi như vậy một cách chính xác, một hệ
thống phải nhận ra rằng câu hỏi là sự kết hợp của hai câu
hỏi con: "Có con ngựa không?" và "Xe ngựa có ở bên phải
con ngựa không?" Mở rộng mô hình tinh chỉnh điển hình
cho tất cả các kết hợp có thể của các kỹ năng lý luận là cực
kỳ tốn kém về chi phí chú thích và khiến việc thêm kỹ năng
vào một hệ thống đã được huấn luyện trở nên khó khăn.

Mặt khác, các phương pháp mô-đun – từ các phương pháp
cổ điển (Krishnamurthy và Kollar, 2013), đến mạng mô-đun
thần kinh khả vi (NMN) (Andreas et al., 2016; Hu et al.,
2017; Saqur và Narasimhan, 2020)) – cung cấp một tuyến
đường tiềm năng để tận dụng và mở rộng cho bản chất tổ
hợp của lý luận thị giác như một phương tiện để tổng quát
hóa: tức là, sử dụng vô hạn các phương tiện hữu hạn. Tuy
nhiên, các mô-đun của một NMN vẫn phải được huấn luyện
cùng nhau trên một tập dữ liệu lớn, và cũng bị hạn chế ở
chỗ chúng (i) yêu cầu một bộ phân tích cú pháp, phải được
sửa đổi nếu các mô-đun được thêm hoặc loại bỏ khỏi hệ
thống, và (ii) yêu cầu huấn luyện lại nếu một mô-đun được
thay thế.

Trong nghiên cứu này, chúng tôi điều tra một lớp phương
pháp VQA mô-đun thay thế, theo đó dựa trên sự ra đời gần
đây của các mô hình ngôn ngữ có khả năng cao ngay từ đầu
(LM) (Chen et al., 2021; Ouyang et al., 2022) và các mô
hình ngôn ngữ thị giác (VLM) (Li et al., 2022), chúng tôi
phát triển các hệ thống xây dựng VQA như một vấn đề tổng
hợp chương trình. Cụ thể, phương pháp CodeVQA của chúng
tôi, được minh họa trong Hình 1, sử dụng các LM viết mã
để nhận câu hỏi làm đầu vào, và xuất mã để (i) điều phối
một loạt các API thị giác nguyên thủy bao bọc xung quanh
VLM để thăm dò ảnh cho các phần thông tin thị giác cụ thể
(ví dụ, chú thích, vị trí pixel của các thực thể, hoặc điểm
tương đồng ảnh-văn bản), và (ii) lý luận về thông tin đó với
biểu thức đầy đủ của mã Python (ví dụ, số học, cấu trúc
logic, vòng lặp phản hồi, v.v.) để đến một câu trả lời. Từ
góc độ thực tế, tính mô-đun của CodeVQA kết hợp với khả
năng gợi ý few-shot của LM cho phép nó thích ứng với một
phạm vi rộng các phân phối nhãn VQA mong muốn mà không
cần huấn luyện mô hình bổ sung, và được hưởng lợi từ việc
thay thế các mô-đun riêng lẻ bằng các phiên bản cải thiện
khi chúng có sẵn.

Chúng tôi đánh giá CodeVQA trong thiết lập VQA few-shot,
mà đã thấy rất nhiều nghiên cứu gần đây (Alayrac et al.,
2022; Jin et al., 2021; Yang et al., 2021; Tiong et al., 2022).
Phương pháp của chúng tôi vượt trội hơn các phương pháp
trước đó ít nhất 3% trên tập dữ liệu COVR (Bogin et al.,
2021), yêu cầu lý luận trên nhiều ảnh, và khoảng 2% trên
tập dữ liệu GQA (Hudson và Manning, 2019). Kết quả của
chúng tôi cho thấy rằng những lợi ích của tính mô-đun với
các mô hình sẵn có gần đây có thể được thực hiện trong VQA
mà không cần huấn luyện mô hình bổ sung.1

1Mã của chúng tôi và các chương trình có chú thích có sẵn tại https:
//github.com/sanjayss34/codevqa .arXiv:2306.05392v1 [cs.CL] 8 Jun 2023

--- TRANG 2 ---
Sinh mã
horse_exists = query(img, "Is there a horse?")
answer = "no"
if horse_exists == "yes":
    carriage_pos_x, carriage_pos_y = get_pos(img, "carriage")
    horse_pos_x, horse_pos_y = get_pos(img, "horse")
    if carriage_pos_x > horse_pos_x:
        answer = "yes"

'Câu hỏi: Xe ngựa có ở bên phải con ngựa không?
Trả lời: Không

Codex (Gợi ý Few-Shot)

Ví dụ trong ngữ cảnh
# Ảnh 1: Tấm thảm ở phía nào của bức tranh?
img = open_image("Image1.jpg")
rug_pos_x, rug_pos_y = get_pos(img, "rug")
if rug_pos_x < (LEFT+RIGHT)/2:
    answer = "left"
else:
    answer = "right"
...

query(img, "Is there a horse?")

Thực thi Mã

get_pos(img, "carriage")
get_pos(img, "horse")
carriage_pos_x < horse_pos_x

trả về 5, 11    trả về 12, 11    trả về "yes"

Chú thích:
1. 'một con ngựa cảnh sát được kéo bởi một cảnh sát cứu hỏa trong một toa xe',
2. 'người đàn ông cưỡi xe ngựa kéo ngựa bên cạnh một sĩ quan', ...

Hình 1: Tổng quan CodeVQA. CodeVQA đầu tiên gợi ý Codex với các ví dụ trong ngữ cảnh chia nhỏ một câu hỏi cho trước thành mã Python. Chỉ sử dụng câu hỏi, Codex sinh ra một chương trình có thể thực thi kết hợp các mô-đun thị giác được định nghĩa trước sử dụng logic điều kiện, số học, v.v. Mô-đun thị giác, query trả lời một câu hỏi bằng cách tạo chú thích cho ảnh và sử dụng một LM để trả lời dựa trên các chú thích. get_pos lấy vị trí của đối tượng. Ở đây, CodeVQA xác định chính xác câu hỏi như sự kết hợp của một truy vấn và một so sánh không gian và đến câu trả lời đúng.

2 Nghiên cứu Liên quan
Một số phương pháp gần đây cho các tác vụ lý luận bao gồm một LM viết chương trình và một trình thông dịch cho các chương trình này. Liang et al. (2022) áp dụng phương pháp này cho robot học. Cheng et al. (2023) giới thiệu một khung cho lý luận chung trên bảng, văn bản và ảnh, trong đó các ảnh được biểu diễn bằng chú thích ảnh. Subramanian et al. (2022) sử dụng một bộ phân tích cú pháp và các quy tắc được mã hóa cứng thay vì một LM để tổng hợp đầu ra từ CLIP (Radford et al., 2021) cho hiểu biểu thức tham chiếu zero-shot; phát hiện của họ rằng CLIP không hữu ích cho từ khóa không gian thúc đẩy phương pháp sinh mã của chúng tôi cho lý luận không gian.

Song song với nghiên cứu của chúng tôi, các bài báo khác đã giới thiệu các khung tương tự cho VQA đa bước (Gupta và Kembhavi, 2023; Surís et al., 2023). Các bài báo này gộp chung lợi ích của tổng hợp chương trình với lợi ích của LM, ví dụ trong ngữ cảnh, và các mô hình thị giác được sử dụng như nguyên thủy. Ngược lại, chúng tôi phân tích hiệu ứng của tổng hợp chương trình bằng cách so sánh CodeVQA với một đường cơ sở mạnh dựa trên LM few-shot sử dụng cùng phương pháp chọn ví dụ trong ngữ cảnh. Hơn nữa, trong khi các khung này dựa vào các mô hình VQA được giám sát hoặc phát hiện đối tượng, chúng tôi cho thấy rằng chúng tôi có thể đạt được hiệu suất tương đương (trên tập dữ liệu GQA) chỉ sử dụng LM và các mô hình được huấn luyện trước trên các cặp ảnh-văn bản.

3 VQA Few-shot thông qua Sinh mã
Trong trả lời câu hỏi thị giác (VQA), đầu vào của hệ thống là một ảnh và một câu hỏi và đầu ra là một câu trả lời văn bản. Chúng tôi xem xét thiết lập VQA few-shot trong đó hệ thống chỉ có quyền truy cập vào một số lượng nhỏ (50) các thể hiện VQA được chú thích bởi con người.

Tổng quan. Hình 1 minh họa phương pháp của chúng tôi. Cho một ảnh và một câu hỏi tương ứng, CodeVQA đầu tiên sinh ra một chương trình Python chỉ sử dụng câu hỏi. Sau đó nó thực thi chương trình này, sử dụng ảnh khi cần thiết, để dự đoán câu trả lời. Chúng tôi đầu tiên định nghĩa tập hợp các nguyên thủy mã mà hệ thống của chúng tôi sử dụng (§ 3.1). Sau đó chúng tôi mô tả cách chúng tôi sinh ra một chương trình kết hợp các nguyên thủy này dựa trên câu hỏi (§ 3.2). Cuối cùng, chúng tôi liệt kê các mô hình được huấn luyện trước mà chúng tôi sử dụng (§ 3.3).

3.1 Nguyên thủy Mã
Nguyên thủy định nghĩa các thao tác cơ bản trên ảnh hoặc trên văn bản thường hữu ích cho VQA. Trong CodeVQA, chúng tôi sử dụng ba nguyên thủy, được định nghĩa dưới đây. Mỗi nguyên thủy này được triển khai sử dụng khớp ảnh-văn bản (ITM), tương phản ảnh-văn bản (ITC), và các mô hình tạo chú thích ảnh, mỗi cái có thể được huấn luyện chỉ với các cặp ảnh-chú thích. Sự khác biệt giữa ITM và ITC là ITC tính toán các embedding ảnh và văn bản riêng biệt và lấy tích vô hướng, trong khi ITM thực hiện fusion sớm trên các đặc trưng ảnh và văn bản và do đó tốn kém tính toán hơn. Chúng tôi lưu ý rằng khung của chúng tôi không gắn liền với lựa chọn nguyên thủy này và có thể hỗ trợ các nguyên thủy khác, phức tạp hơn có thể dựa vào các khía cạnh khác của ngôn ngữ lập trình và thư viện bên thứ ba.

query(image, question) Hàm này trả lời một câu hỏi về ảnh cho trước. Triển khai của chúng tôi cho hàm này dựa trên PnP-VQA (Tiong et al., 2022) và PICa (Yang et al., 2021) và được triển khai với các bước sau: (1) sử dụng mô hình ITM, tính toán GradCAM (Sel-

--- TRANG 3 ---
varaju et al., 2016) giữa câu hỏi và ảnh (trung bình trên các token câu hỏi), (2) lấy mẫu K = 20 miếng ảnh dựa trên điểm GradCAM của chúng, (3) tạo chú thích từ các miếng được lấy mẫu sử dụng mô hình tạo chú thích, (4) Lặp lại bước (2) và (3) cho đến khi Cunique chú thích được tạo ra, và (5) dự đoán câu trả lời bằng cách gợi ý một LM với câu hỏi, chú thích, và ví dụ trong ngữ cảnh. Các ví dụ trong ngữ cảnh ở bước (5) được chọn như mô tả trong § 3.2. Khi tập dữ liệu bao gồm lý luận trên nhiều ảnh, mỗi ví dụ trong ngữ cảnh có chú thích cho tất cả các ảnh.

get_pos(image, text) Hàm này tính toán GradCAM giữa các token văn bản cho trước và ảnh sử dụng mô hình ITM và trả về cặp (x, y) tối đa hóa giá trị GradCAM. Lưu ý rằng ứng dụng GradCAM này khác với ứng dụng trong query vì chúng tôi không tính trung bình trên tất cả các token câu hỏi. Xem Phụ lục B để biết thêm thông tin về cách chúng tôi tính toán bản đồ GradCAM.

find_matching_image(images, text) Trong thiết lập có nhiều ảnh được liên kết với mỗi câu hỏi, có các câu hỏi tham chiếu cụ thể đến một ảnh (ví dụ "Người phụ nữ đang cầm gì?"). Hàm này có thể được sử dụng để chọn ảnh có liên quan nhất từ tập hợp. Nó được triển khai bằng cách tính điểm cho mỗi ảnh với văn bản sử dụng mô hình ITC và chọn ảnh có điểm cao nhất.

3.2 Sinh mã
Trong giai đoạn đầu của CodeVQA, chúng tôi sinh ra một chương trình Python dựa trên câu hỏi. Sử dụng Python thay vì ngôn ngữ cụ thể miền có lợi thế vì (1) nó hỗ trợ số học cũng như điều khiển luồng bao gồm vòng lặp và câu lệnh if (Liang et al., 2022)–tất cả đều được chúng tôi sử dụng trong các chương trình của mình–và (2) các LM lớn cho sinh mã (ví dụ Codex (Chen et al., 2021)) đã được huấn luyện trên một lượng lớn mã Python.

Chúng tôi xây dựng một gợi ý bao gồm một hướng dẫn, các hằng số định nghĩa kích thước của ảnh, và các câu lệnh import và tài liệu API (như một chú thích mã) chỉ định các hàm có sẵn. Ngoài gợi ý, đầu vào cho LM cũng bao gồm các chương trình được chú thích bởi chuyên gia cho một số ví dụ trong ngữ cảnh. Một ví dụ trong ngữ cảnh cho gợi ý few-shot trên tập dữ liệu COVR được hiển thị dưới đây (câu hỏi màu xám, chương trình được làm nổi bật):

# Tập ảnh 1: Có bao nhiêu ảnh chứa chính xác 2 đôi giày hồng??
images = open_images("ImageSet1.jpg")
count = 0
for image in images:
    two_pink_shoes = query(image, "Are there exactly 2 pink shoes?")
    if two_pink_shoes == "yes":
        count += 1
answer = count

Để xem ví dụ về phần còn lại của gợi ý cho LM, xem Phụ lục A. Khi thực thi chương trình được sinh ra dẫn đến lỗi runtime, chúng tôi trả về gọi query trên ảnh và câu hỏi gốc (bao gồm chú thích cho tất cả các ảnh nếu thể hiện bao gồm nhiều ảnh).

Vì tất cả các chương trình được chú thích không thể vừa trong một đầu vào duy nhất cho mô hình, chúng tôi phải chọn chương trình nào để sử dụng làm ví dụ trong ngữ cảnh cho mỗi câu hỏi test. Theo Wang et al. (2022), chúng tôi sử dụng embedding câu2 để lấy các câu hỏi tương tự nhất cho mỗi câu hỏi test.

3.3 Các mô hình thành phần
Phương pháp của chúng tôi dựa vào bốn mô hình được huấn luyện trước: một mô hình sinh mã, một mô hình ITM, một mô hình ITC, một mô hình IC, và một LM trả lời câu hỏi để trả lời câu hỏi dựa trên chú thích. Chúng tôi sử dụng mô hình code-davinci-002 (Chen et al., 2021) thông qua OpenAI API cho cả sinh chương trình và trả lời câu hỏi. Chúng tôi sử dụng các mô hình BLIP (Li et al., 2022) được tinh chỉnh cho ITM, ITC, và tạo chú thích.

4 Thí nghiệm
4.1 Chi tiết Triển khai
Xem Phụ lục C cho chi tiết triển khai.

4.2 Tập dữ liệu
Tập dữ liệu GQA (Hudson và Manning, 2019) chứa các câu hỏi đa bước được sinh ra từ đồ thị cảnh được chú thích bởi con người của các ảnh riêng lẻ trong Visual Genome (Krishna et al., 2016). Tập dữ liệu COVR (Bogin et al., 2021) chứa các câu hỏi đa bước về tập hợp ảnh trong Visual Genome và imSitu (Yatskar et al., 2016). Các câu hỏi này được sinh ra tổng hợp từ các mẫu và sau đó được diễn giải lại bởi con người. Trừ khi có chỉ định khác, chúng tôi trình bày kết quả trên các câu hỏi được diễn giải lại. Tập dữ liệu NLVR2 (Suhr

2https://huggingface.co/sentence-transformers/all-mpnet-base-v2

--- TRANG 4 ---
[Tôi tiếp tục dịch phần còn lại của tài liệu...]

Mô hình                    GQA    COVR   NLVR2
                          Acc.   Acc.   Acc.
Tinh chỉnh
VisualBERT                –      57.9   67.0
VinVL-Base               65.1    –      83.1
Zero-shot
FewVLM                   29.3    –      –
PnP-VQA                  42.3    –      –
BLIP-v2*                 44.7    –      –
Few-shot
FewVLM                   35.7    –      –
VisProg*                 50.5†   –      62.4
ViperGPT*                48.2    –      –
Few-shot PnP-VQA         46.6   45.8   63.4
CodeVQA (của chúng tôi)   49.0   50.7   64.0

Bảng 1: Kết quả trên tập dữ liệu GQA (testdev), COVR (test), và NLVR2 (test-public) từ CodeVQA, Few-shot PnP-VQA, nghiên cứu trước VisualBERT (Li et al., 2019), VinVL-Base (Zhang et al., 2021), FewVLM (Jin et al., 2021), PnP-VQA (Tiong et al., 2022), và nghiên cứu đồng thời (được đánh dấu với *) BLIP-v2 (Li et al., 2023), VisProg (Gupta và Kembhavi, 2023), và ViperGPT (Surís et al., 2023). Phương pháp của chúng tôi vượt trội hơn tất cả các phương pháp few-shot từ nghiên cứu trước. Điểm few-shot cao nhất cho mỗi tập dữ liệu đầy đủ được in đậm. †chỉ ra một đánh giá trên mẫu phân tầng của tập test, có thể không so sánh trực tiếp được với kết quả trên tập test đầy đủ.

et al., 2019) chứa các câu phát biểu về các cặp ảnh, và nhiệm vụ là xác định xem mỗi câu phát biểu là đúng hay sai (chúng tôi diễn giải lại các câu phát biểu thành câu hỏi trước khi đưa vào các phương pháp chúng tôi đánh giá). Phụ lục H có thêm chi tiết về các tập dữ liệu. Đối với mỗi trong ba tập dữ liệu, chúng tôi viết chương trình cho 50 câu hỏi được lấy mẫu ngẫu nhiên từ tập huấn luyện tương ứng. Trừ khi có phát biểu khác, chúng tôi đặt 12 ví dụ trong ngữ cảnh trong một gợi ý cho tập dữ liệu ảnh đơn và 6 ví dụ trong ngữ cảnh trong một gợi ý cho tập dữ liệu đa ảnh (vì bao gồm chú thích cho nhiều ảnh tăng kích thước ngữ cảnh cần thiết cho mỗi ví dụ). Chúng tôi báo cáo độ chính xác khớp chính xác của các câu trả lời viết thường.

4.3 Đường cơ sở
Đường cơ sở chính của chúng tôi là một sự thích ứng của PnP-VQA (Tiong et al., 2022) cho thiết lập few-shot. Chúng tôi gọi nó là "Few-shot PnP-VQA." Đường cơ sở này tương đương với việc chạy quy trình query năm bước được mô tả trong § 3.1 cho mọi câu hỏi. Chúng tôi cũng so sánh với các phương pháp zero-shot và few-shot từ nghiên cứu trước và đồng thời. Phụ lục D chứa thêm chi tiết về các phương pháp đó.

4.4 Kết quả
Bảng 1 hiển thị kết quả trên ba tập dữ liệu. CodeVQA có độ chính xác cao nhất trong các kỹ thuật few-shot. So với Few-shot PnP-VQA, nó có hiệu suất tốt hơn đáng kể trên COVR, điều này hợp lý vì trong tập dữ liệu này, phương pháp đường cơ sở phải kết hợp thông tin qua chú thích ảnh cho nhiều ảnh khi được đưa một gợi ý duy nhất. Mặt khác, phương pháp của chúng tôi lặp qua các ảnh và truy vấn một ảnh duy nhất mỗi lần hoặc chọn ảnh có liên quan nhất đến câu hỏi. Thực sự, Bảng 3 cho thấy CodeVQA có lợi thế lớn nhất trên các thể hiện bao gồm 4 hoặc 5 ảnh. So với nghiên cứu đồng thời cũng sử dụng tổng hợp chương trình, CodeVQA thường có hiệu suất tốt hơn, có thể do sự khác biệt về phương pháp như lấy ví dụ trong ngữ cảnh của chúng tôi hoặc do chi tiết triển khai.

Hình 2 hiển thị so sánh định tính của CodeVQA và đường cơ sở Few-shot PnP-VQA trên tập dữ liệu COVR. CodeVQA trả lời câu hỏi chính xác bằng cách trả lời một câu hỏi đơn giản hơn cho mỗi ảnh và so sánh các câu trả lời, trong khi Few-shot PnP-VQA trả lời sai mặc dù tạo ra chú thích với thông tin cần thiết.

4.5 Phân tích loại bỏ
Bảng 2 so sánh lấy ví dụ trong ngữ cảnh dựa trên embedding với lấy ngẫu nhiên. Cải thiện của CodeVQA so với Few-shot PnP-VQA lớn hơn khi các ví dụ trong ngữ cảnh được lấy bằng embedding. Lấy dựa trên embedding cung cấp một cách có hệ thống để thu thập các ví dụ trong ngữ cảnh có liên quan thay vì tuyển chọn một tập hợp ví dụ duy nhất như trong Gupta và Kembhavi (2023).

Trong Phụ lục F, chúng tôi bao gồm các phân tích loại bỏ cho LM trả lời câu hỏi và cho số lượng shot trong gợi ý cũng như kết quả trên tập validation. Bảng 4 cho thấy CodeVQA cải thiện so với Few-shot PnP-VQA khi sử dụng code-davinci-002 hoặc text-davinci-003 làm LM trả lời câu hỏi. Bảng 5 cho thấy độ chính xác gần như không đổi khi số lượng ví dụ trong ngữ cảnh thay đổi.

4.6 Phân tích
Hình 3 chia nhỏ độ chính xác theo loại câu hỏi. Cải thiện lớn nhất của CodeVQA (khoảng 30%) là trong tập con gồm các câu hỏi về vị trí đối tượng trái/phải hoặc trên/dưới. Cũng có cải thiện trong các câu hỏi "and" và "or". Cải thiện này

--- TRANG 5 ---
Ngữ cảnh:
Ảnh 1: Một đoàn tàu đậu tại một nhà ga lớn màu vàng. Một đoàn tàu đang kéo vào nhà ga tại một sân ga. Một đoàn tàu màu vàng đậu tại bến tàu.
Ảnh 2: Một đầu máy di chuyển xuống con đường tuyết. Tàu tàu hộp đỏ trên tàu tàu tàu lên tàu hàng tàu đỏ tàu tàu đỏ. Một đoàn tàu đi qua thành phố tuyết bên cạnh đèn đỏ.
===
Hỏi: Có đúng là đoàn tàu bên cạnh sân ga và đoàn tàu gần tuyết có cùng màu không?
Trả lời:

Câu hỏi: Có đúng là đoàn tàu bên cạnh sân ga và đoàn tàu gần tuyết có cùng màu không?
Câu trả lời Thực tế: không

images = open_images("ImageSet7.jpg")
platform_image = find_matching_image(images, "train next to a platform")
snow_image = find_matching_image(images, "train near the snow")
platform_train_color = query(platform_image, "What color is the train?")
snow_train_color = query(snow_image, "What color is the train?")
if platform_train_color == snow_train_color:
    answer = "yes"
else:
    answer = "no"

CodeVQA                     Few-shot PnP-VQA
Trả lời: không             Trả lời: có

Hình 2: So sánh Định tính. CodeVQA trả lời chính xác câu hỏi này từ COVR bằng cách chia nó thành các câu hỏi đơn giản hơn, trả lời từng câu riêng biệt, và so sánh các câu trả lời. Few-shot PnP-VQA trả lời sai, mặc dù chú thích chứa thông tin cần thiết. (Lưu ý rằng CodeVQA cũng tạo chú thích, không được hiển thị ở đây.)

Phương pháp Lấy          Few-shot PnP-VQA    CodeVQA
text-davinci-003
Ngẫu nhiên                48.15               49.9
Embedding                 49.4                52.5
code-davinci-002
Ngẫu nhiên                49.5                50.7
Embedding                 52.1                55.3

Bảng 2: So sánh Kỹ thuật Lấy Ví dụ trên 2000 ví dụ validation GQA. Tên mô hình GPT in nghiêng biểu thị mô hình được sử dụng làm LM trả lời câu hỏi.

có thể liên quan đến phát hiện gần đây rằng LM được hưởng lợi từ việc chuyển đổi đa bước thành câu hỏi đơn bước (Press et al., 2022).3

Số lượng ảnh
1    2     3     4     5
# Thể hiện      12   915   828   696   4440
Few-shot PnP-VQA 91.7 51.5  48.3  47.0  46.9
CodeVQA          75.0 53.3  48.7  53.2  53.4

Bảng 3: Độ chính xác theo số lượng ảnh mỗi thể hiện trên tập validation COVR.

Chúng tôi phân tích nguồn lỗi trong CodeVQA trên 100 ví dụ trong tập validation COVR mà CodeVQA trả lời sai: chú thích không liên quan (31%), lỗi trong find_matching_image (12%), lỗi sinh chương trình (14%), lỗi trả lời câu hỏi (25%), câu trả lời dự đoán có thể được coi là đúng (14%), thực tế không rõ ràng/sai (16%), và lỗi số (1%). Lưu ý rằng các danh mục này không loại trừ lẫn nhau, và 13 trong 100 ví dụ được đánh dấu với nhiều danh mục. Do đó, nhiều lỗi hơn do thực thi các mô-đun hơn là sinh chương trình.

3Độ chính xác trên loại câu hỏi này cũng có thể được cải thiện bằng cách cải thiện LM. Ví dụ, sử dụng text-davinci-003 làm LM cho QA thu hẹp khoảng cách giữa Few-shot PnP-VQA và CodeVQA trên câu hỏi "and" trong GQA.

Hình 3: Độ chính xác theo loại câu hỏi trong tập test GQA. CodeVQA (xanh) vượt trội hơn Few-shot PnP-VQA (cam) trên các câu hỏi không gian, and, or. "Không gian" đề cập đến các câu hỏi tập trung vào quan hệ trái/phải hoặc trên/dưới hoặc vị trí đối tượng.

5 Kết luận
Trong bài báo này, chúng tôi đã giới thiệu một khung cho VQA few-shot mô-đun. Phương pháp của chúng tôi gợi ý một LM để sinh ra một chương trình Python gọi các mô-đun thị giác được huấn luyện trước và kết hợp các đầu ra của các mô-đun này để dự đoán câu trả lời. Không giống như các kỹ thuật VQA mô-đun trước đây, khung này không yêu cầu (tái-)huấn luyện các mô-đun hoặc một bộ phân tích cú pháp. Ngoài ra, việc có được đầu ra mô-đun có thể diễn giải từ các phương pháp mô-đun trước đây là không tầm thường (Subramanian et al., 2020), trong khi trong phương pháp của chúng tôi các mô-đun được đóng băng và do đó có thể diễn giải. CodeVQA cũng có thể được xem như mở rộng các hệ thống pipeline (Zeng et al., 2022) sang biểu thức đầy đủ của mã. Phương pháp của chúng tôi thể hiện những cải thiện thực nghiệm, thúc đẩy nghiên cứu tương lai về VQA few-shot mô-đun.

6 Hạn chế
Mặc dù kết quả ban đầu đầy hứa hẹn, độ chính xác của phương pháp chúng tôi vẫn thấp hơn độ chính xác VQA của con người và các mô hình được tinh chỉnh trên tập dữ liệu VQA, điều này gợi ý rằng vẫn có thể có tiến bộ đáng kể phải được thực hiện trước khi các phương pháp VQA few-shot với tổng hợp mã hữu ích cho các ứng dụng thế giới thực thực tế. Ngoài ra, cần thêm nghiên cứu về việc mở rộng khung cho các nguyên thủy bổ sung, vì kết quả trong Phụ lục G cho thấy việc làm như vậy không phải lúc nào cũng dẫn đến cải thiện so với phương pháp đường cơ sở. Một hạn chế khác của phương pháp chúng tôi là nó dựa vào các LM lớn có khả năng, có thể bị hạn chế sử dụng do yêu cầu tính toán hoặc chi phí (ví dụ qua các API có sẵn). Chúng tôi cũng tập trung trong nghiên cứu này vào việc đánh giá khả năng VQA với tiếng Anh làm ngôn ngữ chính – nghiên cứu tương lai có thể mở rộng điều này sang các ngôn ngữ khác thông qua LM đa ngôn ngữ.

7 Lời cảm ơn
Chúng tôi cảm ơn các thành viên của nhóm Berkeley NLP (đặc biệt là Eric Wallace), Grace Luo, và các reviewer ẩn danh vì phản hồi về các bản thảo trước của bài báo này. Chúng tôi biết ơn Ben Bogin và Shivanshu Gupta vì sự hỗ trợ trong việc đánh giá CodeVQA và Few-shot PnP-VQA trên tập test COVR riêng tư. SS, MN, và TD được hỗ trợ một phần bởi DoD, bao gồm học bổng NDSEG (cho SS) và các chương trình LwLL, PTG, và/hoặc SemaFor của DARPA, bởi NSF, và/hoặc bởi chương trình liên minh công nghiệp Berkeley Artificial Intelligence Research (BAIR).

Tài liệu tham khảo
[Phần tài liệu tham khảo tiếp tục với các trích dẫn bằng tiếng Anh như trong bản gốc]

--- TRANG 6 ---
[Tiếp tục với phần còn lại của tài liệu...]
>>>>>>> Stashed changes
