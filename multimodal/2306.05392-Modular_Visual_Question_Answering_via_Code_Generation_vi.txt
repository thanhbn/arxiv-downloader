# Trả lời câu hỏi trực quan theo mô-đun thông qua sinh mã

Sanjay Subramanian1 Medhini Narasimhan1 Kushal Khangaonkar1
Kevin Yang1 Arsha Nagrani2 Cordelia Schmid2 Andy Zeng2
Trevor Darrell1 Dan Klein1
1UC Berkeley 2Google Research

## Tóm tắt

Chúng tôi trình bày một khung xử lý công thức hóa trả lời câu hỏi trực quan như sinh mã theo mô-đun. Trái ngược với các nghiên cứu trước đây về các phương pháp tiếp cận theo mô-đun với VQA, phương pháp của chúng tôi không yêu cầu đào tạo bổ sung và dựa vào các mô hình ngôn ngữ được đào tạo trước (LM), các mô hình trực quan được đào tạo trước trên các cặp hình ảnh-chú thích, và 50 ví dụ VQA được sử dụng để học theo ngữ cảnh. Các chương trình Python được sinh ra gọi và kết hợp các đầu ra của các mô hình trực quan bằng cách sử dụng logic số học và điều kiện. Phương pháp của chúng tôi cải thiện độ chính xác trên tập dữ liệu COVR ít nhất 3% và trên tập dữ liệu GQA khoảng 2% so với đường cơ sở few-shot không sử dụng sinh mã.

## 1 Giới thiệu

Phạm vi lý luận cần thiết cho trả lời câu hỏi trực quan (VQA) là rất rộng lớn và đòi hỏi tổng hợp nhiều kỹ năng - từ việc gắn kết ngôn ngữ với pixel và lý luận không gian đến lý luận dựa trên thường thức và kiến thức. Xem xét câu hỏi "Xe ngựa có ở bên phải con ngựa không?". Để trả lời nhất quán các câu hỏi như vậy một cách chính xác, một hệ thống phải nhận ra rằng câu hỏi này là sự kết hợp của hai câu hỏi con: "Có con ngựa không?" và "Xe ngựa có ở bên phải con ngựa không?"

Việc mở rộng mô hình tinh chỉnh điển hình cho tất cả các kết hợp có thể của các kỹ năng lý luận là cực kỳ tốn kém về chi phí chú thích và khiến việc thêm kỹ năng vào một hệ thống đã được đào tạo trở nên khó khăn.

Mặt khác, các phương pháp tiếp cận theo mô-đun - từ các phương pháp cổ điển đến các mạng mô-đun thần kinh có thể vi phân (NMN) - cung cấp một con đường tiềm năng để tận dụng và mở rộng tính chất tổ hợp của lý luận trực quan như một phương tiện để tổng quát hóa: tức là sử dụng vô hạn các phương tiện hữu hạn. Tuy nhiên, các mô-đun của NMN vẫn phải được đào tạo cùng nhau trên một tập dữ liệu lớn, và cũng bị hạn chế ở chỗ chúng (i) yêu cầu một bộ phân tích cú pháp, phải được sửa đổi nếu các mô-đun được thêm vào hoặc loại bỏ khỏi hệ thống, và (ii) yêu cầu đào tạo lại nếu một mô-đun được thay thế.

Trong nghiên cứu này, chúng tôi điều tra một lớp phương pháp tiếp cận VQA theo mô-đun thay thế, trong đó dựa trên sự xuất hiện gần đây của các mô hình ngôn ngữ có khả năng cao ngay từ đầu (LM) và các mô hình ngôn ngữ trực quan (VLM), chúng tôi phát triển các hệ thống công thức hóa VQA như một vấn đề tổng hợp chương trình. Cụ thể, phương pháp CodeVQA của chúng tôi, được minh họa trong Hình 1, sử dụng các LM viết mã để nhận câu hỏi làm đầu vào và đưa ra mã để (i) điều phối một chuỗi các API nguyên thủy trực quan bao quanh các VLM để thăm dò hình ảnh để tìm các thông tin trực quan cụ thể (ví dụ: chú thích, vị trí pixel của các thực thể, hoặc điểm tương tự hình ảnh-văn bản), và (ii) lý luận về thông tin đó với biểu thức đầy đủ của mã Python (ví dụ: số học, cấu trúc logic, vòng lặp phản hồi, v.v.) để đưa ra câu trả lời. Từ góc độ thực tế, tính mô-đun của CodeVQA kết hợp với khả năng prompting few-shot của LM cho phép nó thích ứng với phạm vi rộng các phân phối nhãn VQA mong muốn mà không cần đào tạo mô hình bổ sung, và hưởng lợi từ việc thay thế các mô-đun riêng lẻ bằng các phiên bản cải tiến khi chúng có sẵn.

Chúng tôi đánh giá CodeVQA trong thiết lập VQA few-shot, đã thấy rất nhiều nghiên cứu gần đây. Phương pháp của chúng tôi vượt trội hơn các phương pháp trước đây ít nhất 3% trên tập dữ liệu COVR, yêu cầu lý luận trên nhiều hình ảnh, và khoảng 2% trên tập dữ liệu GQA. Kết quả của chúng tôi cho thấy rằng lợi ích của tính mô-đun với các mô hình sẵn có gần đây có thể được thực hiện trong VQA mà không cần đào tạo mô hình bổ sung.

## 2 Nghiên cứu liên quan

Một số phương pháp tiếp cận gần đây cho các nhiệm vụ lý luận bao gồm một LM viết chương trình và một trình thông dịch cho các chương trình này. Liang et al. (2022) áp dụng phương pháp này cho robotics. Cheng et al. (2023) giới thiệu một khung để lý luận kết hợp trên bảng, văn bản và hình ảnh, trong đó các hình ảnh được biểu diễn bằng chú thích hình ảnh. Subramanian et al. (2022) đã sử dụng một bộ phân tích cú pháp và các quy tắc được mã hóa cứng thay vì LM để tổng hợp các đầu ra từ CLIP cho việc hiểu biểu thức tham chiếu zero-shot; phát hiện của họ rằng CLIP không hữu ích cho các từ khóa không gian thúc đẩy phương pháp sinh mã của chúng tôi cho lý luận không gian.

Đồng thời với công trình của chúng tôi, các bài báo khác đã giới thiệu các khung tương tự cho VQA multi-hop. Những bài báo này nhầm lẫn lợi ích của tổng hợp chương trình với lợi ích của LM, các ví dụ theo ngữ cảnh, và các mô hình thị giác được sử dụng như nguyên thủy. Ngược lại, chúng tôi phân tích hiệu quả của tổng hợp chương trình bằng cách so sánh CodeVQA với một đường cơ sở few-shot dựa trên LM mạnh sử dụng cùng phương pháp chọn ví dụ theo ngữ cảnh. Hơn nữa, trong khi các khung này dựa vào các mô hình VQA có giám sát hoặc phát hiện đối tượng, chúng tôi cho thấy rằng chúng tôi có thể đạt được hiệu suất tương đương (trên tập dữ liệu GQA) chỉ sử dụng LM và các mô hình được đào tạo trước trên các cặp hình ảnh-văn bản.

## 3 VQA Few-shot thông qua sinh mã

Trong trả lời câu hỏi trực quan (VQA), các đầu vào của hệ thống là một hình ảnh và một câu hỏi và đầu ra là một câu trả lời dạng văn bản. Chúng tôi xem xét thiết lập VQA few-shot trong đó hệ thống chỉ có quyền truy cập vào một số lượng nhỏ (50) các thực thể VQA được chú thích bởi con người.

**Tổng quan.** Hình 1 minh họa phương pháp của chúng tôi. Cho một hình ảnh và câu hỏi tương ứng, CodeVQA đầu tiên sinh ra một chương trình Python chỉ sử dụng câu hỏi. Sau đó nó thực thi chương trình này, sử dụng hình ảnh khi cần thiết, để dự đoán câu trả lời. Chúng tôi đầu tiên định nghĩa tập hợp các nguyên thủy mã mà hệ thống của chúng tôi sử dụng (§ 3.1). Sau đó chúng tôi mô tả cách chúng tôi sinh ra một chương trình kết hợp các nguyên thủy này dựa trên câu hỏi (§ 3.2). Cuối cùng, chúng tôi liệt kê các mô hình được đào tạo trước mà chúng tôi sử dụng (§ 3.3).

### 3.1 Nguyên thủy mã

Các nguyên thủy định nghĩa các hoạt động cơ bản trên hình ảnh hoặc trên văn bản thường hữu ích cho VQA. Trong CodeVQA, chúng tôi sử dụng ba nguyên thủy, được định nghĩa bên dưới. Mỗi nguyên thủy này được thực hiện bằng cách sử dụng các mô hình khớp hình ảnh-văn bản (ITM), tương phản hình ảnh-văn bản (ITC), và chú thích hình ảnh, mỗi mô hình có thể được đào tạo chỉ với các cặp hình ảnh-chú thích. Sự khác biệt giữa ITM và ITC là ITC tính toán các nhúng hình ảnh và văn bản riêng biệt và lấy tích vô hướng, trong khi ITM thực hiện hợp nhất sớm trên các đặc trưng hình ảnh và văn bản và do đó tốn kém tính toán hơn. Chúng tôi lưu ý rằng khung của chúng tôi không bị ràng buộc với lựa chọn nguyên thủy này và có thể hỗ trợ các nguyên thủy khác, phức tạp hơn có thể tận dụng các khía cạnh khác của ngôn ngữ lập trình và thư viện bên thứ ba.

**query(image, question)** Hàm này trả lời một câu hỏi về hình ảnh đã cho. Việc triển khai hàm này của chúng tôi dựa trên PnP-VQA và PICa và được thực hiện với các bước sau: (1) sử dụng mô hình ITM, tính toán GradCAM giữa câu hỏi và hình ảnh (trung bình trên các token câu hỏi), (2) lấy mẫu K = 20 bản vá hình ảnh dựa trên điểm GradCAM của chúng, (3) sinh chú thích từ các bản vá được lấy mẫu sử dụng mô hình chú thích, (4) Lặp lại các bước (2) và (3) cho đến khi Cunique chú thích được sinh ra, và (5) dự đoán câu trả lời bằng cách prompting LM với câu hỏi, chú thích, và các ví dụ theo ngữ cảnh. Các ví dụ theo ngữ cảnh trong bước (5) được chọn như mô tả trong § 3.2. Khi tập dữ liệu liên quan đến lý luận trên nhiều hình ảnh, mỗi ví dụ theo ngữ cảnh có chú thích cho tất cả hình ảnh.

**get_pos(image, text)** Hàm này tính toán GradCAM giữa các token văn bản đã cho và hình ảnh sử dụng mô hình ITM và trả về cặp (x, y) tối đa hóa giá trị GradCAM. Lưu ý rằng việc ứng dụng GradCAM này khác với việc trong query vì chúng tôi không lấy trung bình trên tất cả các token câu hỏi. Xem Phụ lục B để biết thêm thông tin về cách chúng tôi tính toán bản đồ GradCAM.

**find_matching_image(images, text)** Trong thiết lập có nhiều hình ảnh được liên kết với mỗi câu hỏi, có những câu hỏi tham chiếu cụ thể đến một hình ảnh (ví dụ "Người phụ nữ đang cầm gì?"). Hàm này có thể được sử dụng để chọn hình ảnh có liên quan nhất từ tập hợp. Nó được thực hiện bằng cách chấm điểm mỗi hình ảnh với văn bản sử dụng mô hình ITC và chọn hình ảnh có điểm cao nhất.

### 3.2 Sinh mã

Trong giai đoạn đầu của CodeVQA, chúng tôi sinh ra một chương trình Python dựa trên câu hỏi. Sử dụng Python thay vì ngôn ngữ dành riêng cho miền có lợi thế vì (1) nó hỗ trợ số học cũng như luồng điều khiển bao gồm vòng lặp và câu lệnh if - tất cả đều được sử dụng trong các chương trình của chúng tôi - và (2) các LM lớn cho sinh mã (ví dụ Codex) đã được đào tạo trên một lượng lớn mã Python.

Chúng tôi xây dựng một prompt bao gồm một hướng dẫn, các hằng số định nghĩa kích thước của hình ảnh, và các câu lệnh import và tài liệu API (như một nhận xét mã) chỉ định các hàm có sẵn. Ngoài prompt, đầu vào cho LM cũng bao gồm các chương trình được chú thích bởi chuyên gia cho một số ví dụ theo ngữ cảnh.

Vì tất cả các chương trình được chú thích không thể vừa vào một đầu vào duy nhất cho mô hình, chúng tôi phải chọn chương trình nào để sử dụng như các ví dụ theo ngữ cảnh cho mỗi câu hỏi kiểm tra. Theo Wang et al. (2022), chúng tôi sử dụng nhúng câu để truy xuất các câu hỏi tương tự nhất cho mỗi câu hỏi kiểm tra.

### 3.3 Các mô hình thành phần

Phương pháp của chúng tôi dựa vào bốn mô hình được đào tạo trước: một mô hình sinh mã, một mô hình ITM, một mô hình ITC, một mô hình IC, và một LM trả lời câu hỏi để trả lời câu hỏi dựa trên chú thích. Chúng tôi sử dụng mô hình code-davinci-002 thông qua OpenAI API cho cả việc sinh chương trình và trả lời câu hỏi. Chúng tôi sử dụng các mô hình BLIP được tinh chỉnh cho ITM, ITC, và chú thích.

## 4 Thí nghiệm

### 4.1 Chi tiết triển khai

Xem Phụ lục C để biết chi tiết triển khai.

### 4.2 Tập dữ liệu

Tập dữ liệu GQA chứa các câu hỏi multi-hop được sinh ra từ các đồ thị cảnh được chú thích bởi con người của các hình ảnh riêng lẻ trong Visual Genome. Tập dữ liệu COVR chứa các câu hỏi multi-hop về tập hợp hình ảnh trong các tập dữ liệu Visual Genome và imSitu. Những câu hỏi này được sinh ra tổng hợp từ các mẫu và sau đó được diễn giải lại bởi con người. Trừ khi được chỉ định khác, chúng tôi trình bày kết quả trên các câu hỏi được diễn giải lại. Tập dữ liệu NLVR2 chứa các câu lệnh về các cặp hình ảnh, và nhiệm vụ là xác định xem mỗi câu lệnh là đúng hay sai (chúng tôi diễn giải lại các câu lệnh thành câu hỏi trước khi đưa vào các phương pháp mà chúng tôi đánh giá). Phụ lục H có thêm chi tiết về các tập dữ liệu. Đối với mỗi tập dữ liệu trong ba tập dữ liệu, chúng tôi đã viết chương trình cho 50 câu hỏi được lấy mẫu ngẫu nhiên từ tập huấn luyện tương ứng. Trừ khi được nêu khác, chúng tôi đặt 12 ví dụ theo ngữ cảnh trong một prompt cho tập dữ liệu hình ảnh đơn và 6 ví dụ theo ngữ cảnh trong một prompt cho tập dữ liệu nhiều hình ảnh (vì bao gồm chú thích cho nhiều hình ảnh làm tăng kích thước ngữ cảnh cần thiết cho mỗi ví dụ). Chúng tôi báo cáo độ chính xác khớp chính xác của các câu trả lời viết thường.

### 4.3 Đường cơ sở

Đường cơ sở chính của chúng tôi là một sự thích ứng của PnP-VQA với thiết lập few-shot. Chúng tôi gọi nó là "Few-shot PnP-VQA." Đường cơ sở này tương đương với việc chạy quy trình truy vấn năm bước được mô tả trong § 3.1 cho mọi câu hỏi. Chúng tôi cũng so sánh với các phương pháp zero-shot và few-shot từ công trình trước đây và đồng thời. Phụ lục D chứa thêm chi tiết về những phương pháp đó.

### 4.4 Kết quả

Bảng 1 cho thấy kết quả trên ba tập dữ liệu. CodeVQA có độ chính xác cao nhất trong số các kỹ thuật few-shot. So với Few-shot PnP-VQA, nó có hiệu suất tốt hơn đáng kể trên COVR, điều này có lý vì trong tập dữ liệu này, phương pháp đường cơ sở phải kết hợp thông tin qua các chú thích hình ảnh cho nhiều hình ảnh khi được đưa ra một prompt duy nhất. Mặt khác, phương pháp của chúng tôi lặp qua các hình ảnh và truy vấn một hình ảnh duy nhất mỗi lần hoặc chọn hình ảnh có liên quan nhất đến câu hỏi. Thực sự, Bảng 3 cho thấy rằng CodeVQA có lợi thế lớn nhất trên các thực thể liên quan đến 4 hoặc 5 hình ảnh. So với công trình đồng thời cũng sử dụng tổng hợp chương trình, CodeVQA thường thực hiện tốt hơn, có thể do sự khác biệt phương pháp luận như truy xuất ví dụ theo ngữ cảnh của chúng tôi hoặc do chi tiết triển khai.

Hình 2 cho thấy so sánh định tính của CodeVQA và đường cơ sở Few-shot PnP-VQA trên tập dữ liệu COVR. CodeVQA trả lời câu hỏi chính xác bằng cách trả lời một câu hỏi đơn giản hơn cho mỗi hình ảnh và so sánh các câu trả lời, trong khi Few-shot PnP-VQA trả lời không chính xác mặc dù tạo ra chú thích với thông tin cần thiết.

### 4.5 Ablation

Bảng 2 so sánh truy xuất ví dụ theo ngữ cảnh dựa trên nhúng với truy xuất ngẫu nhiên. Sự cải thiện của CodeVQA so với Few-shot PnP-VQA lớn hơn khi các ví dụ theo ngữ cảnh được truy xuất bằng nhúng. Truy xuất dựa trên nhúng cung cấp một cách thức có hệ thống để thu thập các ví dụ theo ngữ cảnh có liên quan thay vì tuyển chọn một tập hợp ví dụ duy nhất.

Trong Phụ lục F, chúng tôi bao gồm các ablation cho LM trả lời câu hỏi và cho số lượng shot trong prompt cũng như kết quả trên các tập xác thực. Bảng 4 cho thấy rằng CodeVQA cải thiện so với Few-shot PnP-VQA khi sử dụng code-davinci-002 hoặc text-davinci-003 làm LM trả lời câu hỏi. Bảng 5 cho thấy độ chính xác khá ổn định khi số lượng ví dụ theo ngữ cảnh thay đổi.

### 4.6 Phân tích

Hình 3 phân tích độ chính xác theo loại câu hỏi. Sự cải thiện lớn nhất của CodeVQA (khoảng 30%) là trong tập con gồm các câu hỏi về vị trí đối tượng trái/phải hoặc trên/dưới. Cũng có sự cải thiện trong các câu hỏi "và" và "hoặc". Sự cải thiện này có thể liên quan đến phát hiện gần đây rằng LM hưởng lợi từ việc chuyển đổi multi-hop thành các câu hỏi single-hop.

Chúng tôi đã phân tích các nguồn lỗi trong CodeVQA trên 100 ví dụ trong tập xác thực COVR mà CodeVQA trả lời không chính xác: chú thích không liên quan (31%), lỗi trong find_matching_image (12%), lỗi sinh chương trình (14%), lỗi trả lời câu hỏi (25%), câu trả lời dự đoán có thể được coi là chính xác (14%), ground-truth không rõ ràng/không chính xác (16%), và lỗi số (1%). Lưu ý rằng các danh mục này không loại trừ lẫn nhau, và 13 trong số 100 ví dụ được đánh dấu với nhiều danh mục. Do đó, nhiều lỗi hơn là do việc thực thi các mô-đun hơn là sinh chương trình.

## 5 Kết luận

Trong bài báo này, chúng tôi đã giới thiệu một khung cho VQA few-shot theo mô-đun. Phương pháp của chúng tôi prompts một LM để sinh ra một chương trình Python gọi các mô-đun trực quan được đào tạo trước và kết hợp các đầu ra của những mô-đun này để dự đoán câu trả lời. Không giống như các kỹ thuật VQA theo mô-đun trước đây, khung này không yêu cầu (tái) đào tạo các mô-đun hoặc một bộ phân tích cú pháp. Ngoài ra, việc có được các đầu ra mô-đun có thể diễn giải từ các phương pháp mô-đun trước đây là không dễ dàng, trong khi trong phương pháp của chúng tôi, các mô-đun được đóng băng và do đó có thể diễn giải. CodeVQA cũng có thể được xem như mở rộng các hệ thống pipeline cho biểu thức đầy đủ của mã. Phương pháp của chúng tôi thể hiện những lợi ích thực nghiệm, thúc đẩy nghiên cứu tương lai về VQA few-shot theo mô-đun.

## 6 Hạn chế

Trong khi các kết quả ban đầu là đầy hứa hẹn, độ chính xác của phương pháp chúng tôi vẫn thấp hơn độ chính xác VQA của con người và các mô hình được tinh chỉnh trên các tập dữ liệu VQA, điều này cho thấy rằng vẫn có thể có tiến bộ đáng kể phải được thực hiện trước khi các phương pháp VQA few-shot với tổng hợp mã hữu ích cho các ứng dụng thực tế trong thế giới thực. Ngoài ra, cần thêm nghiên cứu về việc mở rộng khung cho các nguyên thủy bổ sung, vì kết quả trong Phụ lục G cho thấy rằng việc làm như vậy không luôn dẫn đến cải thiện so với phương pháp đường cơ sở. Một hạn chế khác của phương pháp chúng tôi là nó dựa vào các LM lớn có khả năng, có thể bị hạn chế trong sử dụng do yêu cầu tính toán hoặc chi phí (ví dụ thông qua các API có sẵn). Chúng tôi cũng tập trung trong công trình này vào việc đánh giá khả năng VQA với tiếng Anh là ngôn ngữ chính - nghiên cứu tương lai có thể mở rộng điều này sang các ngôn ngữ khác thông qua các LM đa ngôn ngữ.
