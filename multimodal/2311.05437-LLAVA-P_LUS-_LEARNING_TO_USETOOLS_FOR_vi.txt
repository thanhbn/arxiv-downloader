# LLAVA-PLUS: HỌC SỬ DỤNG CÔNG CỤ ĐỂ TẠO RA CÁC TÁC NHÂN ĐA PHƯƠNG THỨC

Shilong Liu♠∗, Hao Cheng♣, Haotian Liu♢∗, Hao Zhang♡∗, Feng Li♡∗,
Tianhe Ren, Xueyan Zou♢∗, Jianwei Yang♣, Hang Su♠, Jun Zhu♠,
Lei Zhang, Jianfeng Gao♣, Chunyuan Li♣†

♠Khoa Khoa học Máy tính & Công nghệ, Viện AI, BNRist, Đại học Thanh Hoa
♣Microsoft Research, Redmond
♢Đại học Wisconsin-Madison ♡HKUST IDEA Research
∗Công việc thực hiện trong thời gian thực tập tại Microsoft †Người dẫn dự án

https://llava-vl.github.io/llava-plus/

TÓM TẮT

Bài báo này giới thiệu LLaVA-Plus (Trợ lý Ngôn ngữ và Thị giác Lớn có thể Cắm và Học Sử dụng Kỹ năng), một trợ lý đa phương thức đa năng được huấn luyện bằng phương pháp đầu cuối đến đầu cuối nhằm mở rộng có hệ thống khả năng của các mô hình đa phương thức lớn (LMM). LLaVA-Plus duy trì một kho kỹ năng chứa nhiều mô hình tiền huấn luyện thị giác và thị giác-ngôn ngữ (công cụ), và có khả năng kích hoạt các công cụ liên quan, khi nhận được đầu vào đa phương thức từ người dùng, để kết hợp kết quả thực thi của chúng một cách linh hoạt nhằm hoàn thành nhiều nhiệm vụ thực tế. Để có được khả năng sử dụng công cụ, LLaVA-Plus được huấn luyện trên dữ liệu tuân theo hướng dẫn đa phương thức mà chúng tôi đã tuyển chọn. Dữ liệu huấn luyện bao gồm nhiều ví dụ sử dụng công cụ về hiểu biết thị giác, tạo sinh, truy xuất kiến thức bên ngoài và các kết hợp của chúng. Kết quả thực nghiệm cho thấy LLaVA-Plus vượt trội so với LLaVA trong các khả năng hiện có và thể hiện nhiều khả năng mới. So với các LLM được tăng cường công cụ, LLaVA-Plus có đặc điểm riêng là truy vấn hình ảnh được định vị trực tiếp và tham gia tích cực trong suốt toàn bộ phiên tương tác giữa con người và AI, cải thiện đáng kể hiệu suất sử dụng công cụ và cho phép các tình huống mới.

1 GIỚI THIỆU

Một khát vọng lâu dài trong trí tuệ nhân tạo là phát triển các trợ lý đa năng có thể tuân theo hiệu quả các hướng dẫn (đa phương thức) của người dùng để hoàn thành nhiều nhiệm vụ thực tế (Askell et al., 2021; Li et al., 2023c). Gần đây, cộng đồng đã chứng kiến sự quan tâm ngày càng tăng trong việc phát triển các mô hình nền tảng với khả năng hiểu biết và tạo sinh đa phương thức mới nổi trong các nhiệm vụ thế giới mở (Gan et al., 2022; Li et al., 2022). Mặc dù các công thức sử dụng Mô hình Ngôn ngữ Lớn (LLM) như ChatGPT (OpenAI, 2023a) để phát triển trợ lý đa năng cho các nhiệm vụ ngôn ngữ tự nhiên đã được chứng minh hiệu quả, các công thức xây dựng trợ lý đa phương thức, đa năng cho các nhiệm vụ thị giác máy tính và thị giác-ngôn ngữ vẫn còn cần được khám phá.

Các nỗ lực đang diễn ra để phát triển các tác nhân đa phương thức có thể được phân loại rộng rãi thành hai lớp (Li et al., 2023c): (i) Huấn luyện đầu cuối đến đầu cuối với LLM, trong đó dữ liệu hình ảnh-văn bản và dữ liệu tuân theo hướng dẫn đa phương thức được thu thập để huấn luyện liên tục LLM nhằm có được khả năng xử lý thông tin thị giác, dẫn đến một chuỗi Mô hình Đa phương thức Lớn (LMM). Hiệu suất hiểu biết và lý luận thị giác ấn tượng đã được chứng minh bởi cả các mô hình độc quyền như Flamingo (Alayrac et al., 2022) và GPT-4 đa phương thức (OpenAI, 2023c), và các mô hình mã nguồn mở như LLaVA (Liu et al., 2023a) và MiniGPT-4 (Zhu et al., 2023). Mặc dù các phương pháp huấn luyện đầu cuối đến đầu cuối này hiệu quả trong việc giúp LMM có được các khả năng mới nổi (như học trong ngữ cảnh), vẫn còn thách thức trong việc phát triển một kiến trúc thống nhất có thể tích hợp liền mạch nhiều kỹ năng, như phân đoạn và tạo sinh hình ảnh, những kỹ năng quan trọng cho các ứng dụng đa phương thức thực tế. (ii) Liên kết công cụ với LLM, trong đó các lời nhắc được tạo ra một cách tỉ mỉ để cho phép LLM (ví dụ, thông qua LangChain (2022)) gọi các công cụ khác nhau (ví dụ, mô hình thị giác tiền huấn luyện) để thực hiện các nhiệm vụ (phụ) mong muốn, mà không cần huấn luyện mô hình bổ sung. Một số công trình nổi bật bao gồm VisProg (Gupta & Kembhavi, 2022), ViperGPT (Surís et al., 2023), Visual ChatGPT (Wu et al., 2023), X-GPT (Zou et al., 2023a), và MM-REACT (Yang et al., 2023b). Điểm mạnh của các phương pháp này là khả năng thực hiện nhiều nhiệm vụ thị giác thông qua việc sử dụng các công cụ (mới), có thể được tích hợp vào tác nhân AI với chi phí phát triển rất thấp. Tuy nhiên, việc nhắc lệnh không đủ linh hoạt cũng không đủ mạnh mẽ để cho phép các tác nhân đa phương thức luôn chọn và kích hoạt chính xác các công cụ phù hợp (từ một bộ công cụ lớn và đa dạng) và kết hợp kết quả của chúng để tạo ra câu trả lời cuối cùng một cách linh hoạt cho các nhiệm vụ đa phương thức thực tế.

Trong bài báo này, chúng tôi giới thiệu LLaVA-Plus (Trợ lý Ngôn ngữ và Thị giác Lớn có thể Cắm và Học Sử dụng Kỹ năng), một trợ lý đa phương thức đa năng học cách sử dụng công cụ bằng phương pháp huấn luyện đầu cuối đến đầu cuối nhằm mở rộng có hệ thống khả năng của LMM thông qua điều chỉnh hướng dẫn thị giác. Theo hiểu biết tốt nhất của chúng tôi, đây là nỗ lực đầu tiên được báo cáo để kết hợp điểm mạnh của các phương pháp huấn luyện đầu cuối đến đầu cuối và liên kết công cụ được đề cập ở trên. LLaVA-Plus được trang bị một kho kỹ năng chứa nhiều công cụ thị giác và thị giác-ngôn ngữ. Thiết kế này là hiện thân của sơ đồ "Xã hội Tâm trí" (Minsky, 1988), trong đó mỗi công cụ ban đầu được thiết kế cho một kỹ năng cụ thể và bản thân nó chỉ hữu ích cho các tình huống cụ thể, nhưng sự kết hợp của các công cụ này dẫn đến các khả năng mới nổi thể hiện dấu hiệu của trí thông minh cao hơn. Ví dụ, LLaVA-Plus có thể xây dựng một quy trình làm việc mới một cách linh hoạt, khi nhận được đầu vào đa phương thức từ người dùng, lựa chọn và kích hoạt các công cụ liên quan từ kho kỹ năng, và kết hợp kết quả thực thi của chúng để hoàn thành nhiều nhiệm vụ thực tế chưa từng thấy trong quá trình huấn luyện mô hình.

LLaVA-Plus có thể được cải thiện liên tục bằng cách tích hợp các kỹ năng hoặc công cụ mới thông qua điều chỉnh hướng dẫn. Xem xét một công cụ đa phương thức mới đã được phát triển cho một tình huống hoặc kỹ năng cụ thể. Chúng tôi thu thập các hướng dẫn người dùng có liên quan yêu cầu công cụ này và kết quả thực thi (hoặc theo sau) của chúng để tạo thành dữ liệu tuân theo hướng dẫn cho việc điều chỉnh. Sau khi điều chỉnh hướng dẫn, LLaVA-Plus mở rộng khả năng của mình khi học cách sử dụng công cụ mới này để xử lý các nhiệm vụ mà trước đây không thể xử lý. LLaVA-Plus cũng khác biệt với các công trình hiện có về việc dạy LLM sử dụng công cụ (ví dụ, Yang et al., 2023a; Patil et al., 2023), trong đó các tín hiệu thị giác chỉ được sử dụng khi các công cụ đa phương thức được kích hoạt. Ngược lại, LLaVA-Plus sử dụng các tín hiệu thị giác thô trong suốt toàn bộ phiên tương tác giữa con người và AI để cải thiện khả năng lập kế hoạch và lý luận của LMM.

Tóm lại, bài báo của chúng tôi đóng góp những điểm sau:

• Dữ liệu sử dụng công cụ tuân theo hướng dẫn đa phương thức mới. Chúng tôi trình bày một quy trình mới để tuyển chọn dữ liệu tuân theo hướng dẫn thị giác-ngôn ngữ, dành riêng cho việc sử dụng công cụ trong các phiên tương tác giữa con người và AI, tận dụng ChatGPT và GPT-4 làm công cụ gán nhãn.

• Trợ lý đa phương thức lớn mới. Chúng tôi đã phát triển LLaVA-Plus, một trợ lý đa phương thức đa năng mở rộng LLaVA (Liu et al., 2023a) bằng cách tích hợp một tập hợp lớn và đa dạng các công cụ bên ngoài có thể được lựa chọn, kết hợp và kích hoạt một cách linh hoạt để thực hiện các nhiệm vụ. Như được hiển thị trong Hình 1, LLaVA-Plus mở rộng đáng kể khả năng của LMM. Nghiên cứu thực nghiệm của chúng tôi xác nhận hiệu quả của LLaVA-Plus với kết quả cải thiện nhất quán trên nhiều bộ đánh giá, và đặc biệt là SoTA mới trên VisiT-Bench với một tập hợp đa dạng các nhiệm vụ thực tế.

• Mã nguồn mở. Chúng tôi sẽ phát hành các tài sản sau cho công chúng: dữ liệu hướng dẫn đa phương thức được tạo ra, cơ sở mã, các điểm kiểm tra LLaVA-Plus, và một demo trò chuyện trực quan.

2 HỌC SỬ DỤNG CÔNG CỤ VỚI ĐIỀU CHỈNH HƯỚNG DẪN THỊ GIÁC

Được truyền cảm hứng bởi hiệu suất ấn tượng của GPT-4 đa phương thức và các LMM mã nguồn mở như LLaVA/MiniGPT-4, cộng đồng đã chứng kiến sự gia tăng trong việc phát triển LMM và dữ liệu tuân theo hướng dẫn đa phương thức, theo mô hình điều chỉnh hướng dẫn (ví dụ, Liu et al., 2023a; Peng et al., 2023a). Trong bài báo này, chúng tôi sử dụng LLaVA làm ví dụ chạy. Nhưng lưu ý rằng công thức được đề xuất có thể dễ dàng áp dụng cho các LMM khác. Bắt đầu với truy vấn hình ảnh đầu vào từ người dùng Iq, các LMM hiện có như LLaVA thường chấp nhận đầu vào hướng dẫn ngôn ngữ tự nhiên Xq từ người dùng, và xuất ra phản hồi ngôn ngữ tự nhiên Xanswer. Do đó, chúng ta có thể sử dụng một sơ đồ thống nhất để biểu diễn dữ liệu tuân theo hướng dẫn đa phương thức như:
