# 2304.13357.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2304.13357.pdf
# File size: 4377872 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Deep Lifelong Cross-modal Hashing
Liming Xu, Hanqi Li, Bochuan Zheng, Weisheng Li, Jiancheng Lv
Abstract ‚ÄîHashing methods have made signiÔ¨Åcant progress in
cross-modal retrieval tasks with fast query speed and low storage
cost. Among them, deep learning-based hashing achieves better
performance on large-scale data due to its excellent extraction
and representation ability for nonlinear heterogeneous features.
However, there are still two main challenges in catastrophic
forgetting when data with new categories arrive continuously, and
time-consuming for non-continuous hashing retrieval to retrain
for updating. To this end, we, in this paper, propose a novel deep
lifelong cross-modal hashing to achieve lifelong hashing retrieval
instead of re-training hash function repeatedly when new data
arrive. SpeciÔ¨Åcally, we design lifelong learning strategy to update
hash functions by directly training the incremental data instead
of retraining new hash functions using all the accumulated
data, which signiÔ¨Åcantly reduce training time. Then, we propose
lifelong hashing loss to enable original hash codes participate
in lifelong learning but remain invariant, and further preserve
the similarity and dis-similarity among original and incremental
hash codes to maintain performance. Additionally, considering
distribution heterogeneity when new data arriving continuously,
we introduce multi-label semantic similarity to supervise hash
learning, and it has been proven that the similarity improves
performance with detailed analysis. Experimental results on
benchmark datasets show that the proposed methods achieves
comparative performance comparing with recent state-of-the-art
cross-modal hashing methods, and it yields substantial average
increments over 20% in retrieval accuracy and almost reduces
over 80% training time when new data arrives continuously.
Index Terms ‚ÄîLifelong hashing, cross-modal retrieval, distri-
bution heterogeneity, catastrophic forgetting, multi-label seman-
tic similarity.
I. I NTRODUCTION
DAILY multimedia data associated with potential semantic
correlation shows multi-source heterogeneous and multi-
modal characteristics. Based on the correlation, cross-modal
retrieval methods [1] aim to achieve cross-matching between
different modalities. Cross-modal hashing [2], an important
branch in cross-modal retrieval, maps high-dimensional orig-
inal data into low-dimensional compact binary hash codes in
Hamming space while maintaining similarity in original space
by enforcing two instances with smaller (larger) Hamming
distance be more (less) similar. Then, many cross-modal
hashing methods have been proposed and it can generally
divided into hand-craft and deep learning [3] based methods.
Due to the data-driven characteristics and excellent feature
extraction ability, we mainly focus on the deep learning based
cross-modal hashing methods.
Liming Xu is the corresponding author. Liming Xu, Hanqi Li and
Bochuan Zheng are with the School of Computer Science, China West
Normal University, Nanchong 637009, China. Liming Xu and Jiancheng Lv
are with the College of Computer Science, Sichuan University, Chengdu
610065, China. Weisheng Li is with the College of Computer Science
and Technology, Chongqing University of Posts and Telecommunication,
Chongqing 400065, China. E-mail: xulimmail@gmail.com; lihq@gmail.com;
zhengbc@cwnu.edu.cn; liws@cqupt.edu.cn; lvjiancheng@scu.edu.cn.Although existing deep cross-modal hashing methods have
achieved satisfactory performance, there are still two main
challenges: (1) catastrophic forgetting when incremental data
with new categories is added and (2) non-continuity of hashing
retrieval. SpeciÔ¨Åcally, when incremental data with new cate-
gory arriving continuously, it will yield catastrophic forget-
ting [4], i.e., the performance on learned hashing function
signiÔ¨Åcantly degrades over time as incremental data with
new categories is added. From subsequent results, it can be
estimated that when adding two categories data, the compared
deep methods incur over 20% accuracy drop. Then, almost all
deep cross-modal hashing need to retrain hash functions using
all the data to generate new hash codes when new data arrives,
which can be time-consuming and inefÔ¨Åcient. The subsequent
results show that these methods require more training time
to retrain the accumulated data, and it will be even worse
on larger dataset. Besides, the consideration of distribution
heterogeneity when new data arriving is insufÔ¨Åcient. Most
current cross-modal hashing use single-label which considers
two instances as similar if they share at least one label to
construct semantic similarity, which cannot preserve semantic
similarity in original space and Hamming space effectively.
Taking the above issues into consideration, we propose a
novel D eep L ifelong C ross-modal H ashing method(DLCH) by
incorporating the base learning and lifelong learning. Within it,
lifelong hashing learning is design to directly learn and update
hashing function of incremental data with new categories.
Then, multi-label semantic similarity and self-supervised strat-
egy are proposed to obtain high-quality hash codes. The main
contributions of this paper can be summarized as follows:
We propose a novel deep lifelong cross-modal hashing
method to overcome catastrophic forgetting when incre-
mental data with new categories is added. The original
hash codes keep unchanged during incremental hash
codes learning, the model performance on original data
will not degrade after training on the incremental data.
We design lifelong hashing loss to directly learn incre-
mental hash codes instead of retraining the learned hash-
ing functions, and optimize incremental hash codes bit
by bit, which avoids huge time cost caused by retraining
all the accumulated data.
We deÔ¨Åne multi-label semantic similarity with multi-
label information to describe semantic correlation more
accurately and generate high-quality original hash codes.
The detailed analysis is provided to show that it improves
performance especially in the lifelong hashing.
Extensive experiments on three benchmark cross-modal
datasets demonstrate that the proposed DLCH gains com-
petitive retrieval performance and obtains signiÔ¨Åcant time
reduction comparing with the state-of-the-art methods.arXiv:2304.13357v1  [cs.CV]  26 Apr 2023

--- PAGE 2 ---
2
The rest of this paper is organized as follows: Section II
reviews related work on cross-modal hashing retrieval. Section
III describes the proposed deep lifelong cross-modal hashing
method and optimization. Section IV presents the experimental
results and analysis. Finally, Section V concludes our work.
II. R ELATED WORK
Generally, the existing cross-modal hashing methods can be
roughly categorized into hand-crafted and deep learning-based
methods according to features extraction [5], [6]. The former
learns hash functions by hand-crafted features and shallow
architecture, while the latter embeds deep network and Ô¨Åne-
tunes training in an end-to-end way. We mainly focus on the
deep learning-based cross-modal hashing, and summarizes the
related issues in the view of continuity as follows.
A. Non-continuous Cross-modal Hashing
In the view of continuity, most of the existing cross-
modal hashing methods are non-continuous, which means
all training data need be provided at the beginning and
loaded at once, and it will be re-trained when incremen-
tal data is added. As reported in previous works [1], [2],
the deep learning-based cross-modal hashing mainly con-
tains deep neural networks(DNN)- and Generative adversarial
networks(GAN)-based cross-modal hashing according to dif-
ferent deep models.
Furthermore, according to whether supervised information
is involved, DNN-based methods can be divided into the unsu-
pervised and the supervised. SpeciÔ¨Åcally, unsupervised learns
hash codes by exploring the correlation of heterogeneous data
with correlation graphs or latent semantic space. DCSH [7]
adopts spectral clustering and anchor-to-anchor mapping for
better similarity graphs. To bridge modality gap, JDSH [8]
exploits a joint-modal similarity matrix, while DUCMH [9] re-
lies on data alignment and image-text data pairs. DGCPN [10]
explores intrinsic semantic relationships with graph-neighbor
coherence to avoid suboptimal retrieval Hamming space. With
introducing knowledge distillation scheme, KDCMH [11]
trains an unsupervised method as the teacher model used to
provide distillation information to guide supervised method.
CMIMH [12] tries to Ô¨Ånd a balance between reducing the
modality gap and losing modality-private information by maxi-
mizing mutual information. UCHM [13] focuses on image-text
interaction to generate a superior modality-interaction-enabled
similarity matrix for training set. On the other hand, supervised
methods make full use of semantic information in supervised
information(e.g. tags or pair-wise similarity information) and
generally achieve better accuracy. To highlight useful infor-
mation and suppress redundant information, TEACH [14] and
MMACH [15] add attention mechanism to feature learning
process, and the latter utilizes multi-label information to
further improve accuracy additionally. HSSAH [16] replaces
binary similarity matrix by asymmetric high-level semantic
similarity to maintain richer semantic information. For data
pairs with different similarities, DCH-SCR [17] Ô¨Åne-tunes
weights of positive instances to vary optimization strengths.In addition to the DNN-based methods described above,
GANs [18] are also often used for cross-modal hashing
retrieval. MLCAH [19] proposes a global and local semantic
alignment mechanism to encode multi-level correlation in-
formation into hash codes. DADH [20] employs a weighted
cosine triplet constraint to learn the ranking based similarity
relevance of data points. MGAH [21] Ô¨Åts the underlying man-
ifold structure using a multi-pathway generative adversarial
network. CPAH [22] utilizes consistency reÔ¨Åned module to
implement the separation of incompatible modality-common
and modality-private representations. DAGNN [23] extracts
more representative common feature vectors with the aid of
the dual generative adversarial networks and the multi-hop
graph neural networks. SAAH [24] depends on inter-modal
and intra-modal adversarial autoencoder modules to generate
uniform feature representation. DFAH [25] overcomes seman-
tic gap and distribution shift via adversarial learning between
Modality Discriminator and Modality-SpeciÔ¨Åc Feature Extrac-
tor. CMGCAH [26] introduces a transformer-based feature
extraction network for further leveraging position information.
Despite the much progress made by the mentioned deep
cross-modal hashing methods, the whole model and hash
function will be re-trained once data changes due to its non-
continuity and the nature of catastrophic forgetting, which
causes much training time and resource cost.
B. Continuous Cross-modal Hashing
Considering that the above non-continuous models are not
suitable for large-scale data sets due to large resource cost
when training streaming data, some have embedded online
learning processing into hash learning. To this end, OCMFH
[27] uses collective matrix factorization to learn hash codes for
streaming data and updates hash codes of old data according to
dynamic changes of hash model without accessing to old data.
Similarly, OCMH [28] transforms inefÔ¨Åcient update of hash
codes into efÔ¨Åcient update of shared latent encoding matrix
and dynamic transition matrix. By taking the label information
into account, OLSH [29] maps discrete labels to continuous
latent semantic concept space, on which similarity measures
between data points are performed. Then, LEMON [30] de-
signs label embedding framework to generate discriminative
hash codes, which makes full use of semantic information.
OLCH [31] introduces online semantic representation learning
strategy to preserve the similarity between new data and old
data in Hamming space. Besides, in order to avoid quantization
error, DOCH [32] optimizes discretely binary constraints and
yields uniform high-quality hash codes. OMGH [33] utilizes
anchor-based manifold embedding to sparsely represent old
data and adaptively guide hash learning.
Although these hashing methods attempts to introduce the
online learning process to improve computational efÔ¨Åcient
and reduce resource cost, they focus on optimization and do
not eliminate the limitation that the old hash codes will be
changed. Then, it can be clearly found that the existing online
cross-modal hashing methods are shallow architectures, and
there is current no deep lifelong cross-modal hashing method.
It‚Äôs well known that the deep neural network is capable to

--- PAGE 3 ---
3
capture the non-linear heterogeneous features. Thirdly, these
online cross-modal hashing has considered to save computa-
tional cost when incremental data is added continuously, but
they ignore the distribution change which will cause server
catastrophic forgetting and fail to reach high performance
when new categories appear. Additionally, the retrieved per-
formance can be limited by using the simple single-label
similarity evaluation.
Motivated by the weaknesses of existing methods, we pro-
pose deep lifelong cross-modal hashing to avoid catastrophic
forgetting and reduce training time. Firstly, we design lifelong
learning strategy to directly learn hash codes of incremental
data on the basis of unchanging old hash codes instead of
retraining hash functions using all the accumulated data. Then,
we incorporate lifelong hashing loss preserve the similarity
and dis-similarity among the original and incremental hash
codes, which is excepted to overcome catastrophic forget-
ting and performance drops. Thirdly, we introduce multi-
label semantic similarity to supervise hash learning to further
utilize label information to adapt distribution heterogeneity
and improve accuracy. These three composition constitute the
proposed DLCH.
III. T HE PROPOSED METHOD
In this section, the details of DLCH are presented. Fig.1
illustrates the framework overview, which includes image and
text modality for convenient description. We have to note
that our proposed method can easily be extended to more
modalities (e.g., image, text, audio and graphics).
A. Notations and Problem DeÔ¨Ånition
It is necessary to present notations and problem deÔ¨Ånition
Ô¨Årstly. In this paper, bold uppercase letters, like W, denote
matrices; italic lowercase letters, like w, denote vectors. Wi
denotes the i-th row of matrix W,Wjdenotes the j-th
column of matrix W, andWijdenotes the element of the i-th
row andj-th column of W. The transpose of Wis denoted
asWTandtr(W)is the trace of W.kWk2
F=tr(WTW)
is the Frobenius-norm. sign ()is sign function deÔ¨Åned as:
sign(x) =
1; x0
 1; x< 0(1)
Suppose there are mlabeled instances O=fX;Y;Lgin
the database, where X=fxigm
i=12Rmdx,Y=fyigm
i=12
RmdyandL=fligm
i=12f0;1gmcdenote image, text and
label, respectively. dxanddydenote the dimension of original
image and text features, respectively. cis the total number of
original classes. That is, an instance oiassociated with class
labellihas both image feature xiand text feature yi. Ifxior
yibelongs to the j-th class, then lij= 1, otherwise, lij= 0.
For theseminstances, our goal is to learn multi-label
networkh(li;l), image hash function f(xi;x)and text hash
functiong(yi;y)to generate the corresponding hash repre-
sentation H=fHijHi=h(li;l)g2Rkm,F=fFijFi=
f(xi;x)g2RkmandG=fGijGi=g(yi;y)g2Rkm,
where kdenotes the length of hash codes. Subsequently, weTABLE I: Notations and the Meanings.
Notation Meaning
X(Y) The image (text) features for original data.
X0(Y0) The image (text) features for incremental data.
L(L0) The labels for original (incremental) data.
F/(G/H) The hash representation of image/text/label modality.
BX(BY) The original hash codes for images (texts).
BX0(BY0) The incremental hash codes for images (texts).
AX(AY) The image (text) hash codes of training data.
S The multi-label semantic similarity matrix.
Q The hash representation similarity matrix.
m Number of existing instances with original classes.
n Number of instances with new classes.
a Number of training instances.
c(c0) Number of original (incremental) classes.
k Number of hash codes bits.
can utilize above hash representation to obtain Ô¨Ånal original
hash codes BX2f  1;+1gkmandBY2f  1;+1gkm.
Analogously, when nincremental labeled data O0=
fX0;Y0;L0garriving, each instance has also image modality
X0=fxigm+n
i=m+1, text modality Y0=fyigm+n
i=m+1and label
L0=fligm+n
i=m+12f0;1gnc0, wherec0is the number of new
classes. For these new ninstances, we expect to update hash
functionf(xi;x)andg(yi;y), and gain corresponding binary
hash codes BX02f  1;+1gknandBY02f  1;+1gkn
with original hash codes invariant.
To reach the goal, we Ô¨Årstly sample dataset afrom original
dataset Oand incremental dataset O0and name as training
setOa=fXa;Ya;Lag, where Xa2Radx fX;X0g,
Ya2RadyfY;Y0g,La=fliga
i=12f0;1ga(c+c0), and
a=a1+a2.a1anda2denote the number of training data
sampled from original and incremental dataset, respectively.
The main notations in this paper are summarized in Table II.
B. Original Hash Codes Learning
1) Original Hashing Loss: In original learning phase, our
goal is to train LabelNet to supervise the training and to
train ImgNet and TxtNet to output hash representation which
preserves the similarity in original space. Thus, for given
hash representation Fof ImgNet and hash representation H
of LabelNet, hash representation similarity is deÔ¨Åned as:
Qxl
ij=Fi
kFik2(Hj
kHjk2)T(2)
In order to unify the range of multi-label semantic similarity
and hash representation similarity, ReLu transformation is
applied to Qxl
ijwhich belongs to [ 1;1]. Then, the hash
representation similarity can be rewritten as:
Qxl
ij=max(0;Qxl
ij) (3)
Then we deÔ¨Åne inter-modality similarity preserving loss
which measures the gap between multi-label semantic simi-
larity and hash representation similarity of two instances from
different modalities as:
Jinter =Jxl
inter +Jyl
inter
=Sxl Qxl2
F+Syl Qyl2
F(4)

--- PAGE 4 ---
4
Fig. 1: DLCH consists of original and lifelong hash learning phase. Original hash learning phase (blue dashed box) where LabelNet acts
as supervisor to guide hash function learning learns hash function as previous works conduct. Lifelong learning phase (orange dashed box)
encompasses three steps: (1) sample training data; (2) construct lifelong hashing loss, and (3) update incremental hash codes.
whereJxl
inter (Jyl
inter ) is the similarity preserving loss for
image (text) modality and multi-label. SxlandQxlare multi-
label semantic similarity and hash representation similarity for
image feature xand label l, respectively. SylandQylare multi-
label semantic similarity and hash representation similarity for
image feature yand label l, respectively. The optimal hash
representation FandGcan be obtained by minimizing Eq.(4).
Correspondingly, to measure the gap between multi-label
semantic similarity and hash representation similarity of two
instances from the same modality, we deÔ¨Åne intra-modality
similarity preserving loss as:
Jintra =Jll
intra +Jxx
intra +Jyy
intra
=Sll Qll2
F+kSxx Qxxk2
F+kSyy Qyyk2
F(5)
whereJll
intra ,Jxx
intra andJyy
intra are the intra-modality simi-
larity preserving loss for multi-label, image and text modal-
ity, respectively. Sll,SxxandSyyare multi-label semantic
similarity for these three modalities. Qll,QxxandQyyare
corresponding hash representation similarity. By minimizingEq.(5), the original similarity between instances from same
modality can be preserved in binary Hamming space.
In order to enable the output of hash function tends to -
1 or +1 to obtain distinctive binary hash codes, we further
add quantization loss which gaps difference between discrete
binary hash codes and continuous hash representation as:
Jquan =F BX2
F+G BY2
F
+1
2(H BX2
F+H BY2
F)(6)
Subsequently, we adopt bit-by-bit optimization to yield
optimal hash codes following the previous works. Combin-
ing Eq.(4)-(6), we obtain the following Ô¨Ånal objective loss
function:
minJ
BX;BY;x;y;l=Jinter +Jintra +Jquan
s:t:BX2f  1;+1gkm
BY2f  1;+1gkm;(7)
whereandare hyper-parameters which control the weight
ratio of losses.

--- PAGE 5 ---
5
2) Optimization: We optimize parameters x,yandl,
and learn original hash codes BXandBYwith an alternating
strategy. The detailed steps are provided below.
Step 1: Learnl, with BX,BY,xandyÔ¨Åxed
When training LabelNet, we rewrite Eq.(7) as follows:
min
lJ=Sxl Qxl2
F+Syl Qyl2
F+Sll Qll2
F
+
2(H BX2
F+H BY2
F)
(8)
WithBX,BY,xandyÔ¨Åxed, we use stochastic gradient
descent (SGD) and back-propagation (BP) algorithm to learn
the parameter of multi-label network. The parameter gradient
is calculated by
@J
@Hj= 2(Qxl Sxl)@Qxl
ij
@Hj+ 2(Qyl Syl)@Qyl
ij
@Hj
+2(Qll Sll)@Qll
ij
@Hj+(Hj BX
j) +(Hj BY
j)
(9)
where
@Qxl
ij
@Hj=(
Fi
kFik2kHjk2(@HT
j
@Hj);ifQxl
ij0
0 ;ifQxl
ij<0(10)
@Qyl
ij
@Hj=(
Gi
kGik2kHjk2(@HT
j
@Hj);ifQyl
ij0
0 ;ifQyl
ij<0(11)
@Qll
ij
@Hj=(
Hi
kHik2kHjk2(@HT
j
@Hj);ifQll
ij0
0 ;ifQll
ij<0(12)
Then, we can calculate@J
@lwith@J
@Hjby chain rule and
update the parameter lwith BP algorithm.
Step 2: Learnx, with BX,BY,yandlÔ¨Åxed
When training ImgNet, we rewrite Eq.(7) as:
min
xJ=Sxl Qxl2
F+kSxx Qxxk2
F+F BX2
F
(13)
Analogously, with BX,BY,yandlÔ¨Åxed, we also use
SGD and BP algorithm to learn the parameters xof image
hash function f(). Firstly, the gradient is calculated by:
@J
@Fi= 2(Qxl Sxl)@Qxl
ij
@Fi
+2(Qxx Sxx)@Qxx
ij
@Fi+ 2(Fi BX
i)(14)
where
@Qxl
ij
@Fi=(
1
kFik2(@Fi
@Fi)(Hj
kHjk2)T;ifQxl
ij0
0 ;ifQxl
ij<0(15)
@Qxx
ij
@Fi=(
1
kFik2(@Fi
@Fi)(Fj
kFjk2)T;ifQxx
ij0
0 ;ifQxx
ij<0(16)
Then, we can calculate@J
@xwith@J
@Fiby chain rule and
update the parameter xwith BP algorithm.
Step 3 : Learny, with BX,BY,xandlÔ¨Åxed
Similarly, when training TxtNet, we rewrite Eq.(7) as fol-
lows:
min
yJ=Syl Qyl2
F+kSyy Qyyk2
F+G BY2
F
(17)Then with BX,BY,xandlÔ¨Åxed, we compute the
gradient:
@J
@Gi= 2(Qyl Syl)@Qyl
ij
@Gi
+2(Qyy Syy)@Qyy
ij
@Gi+ 2(Gi BY
i)(18)
where
@Qyl
ij
@Gi=(
1
kGik2(@Gi
@Gi)(Hj
kHjk2)T;ifQyl
ij0
0 ;ifQyl
ij<0(19)
@Qyy
ij
@Gi=(
1
kGik2(@Gi
@Gi)(Gj
kGjk2)T;ifQyy
ij0
0 ;ifQyy
ij<0(20)
Subsequently, we can calculate@J
@ywith@J
@Giby chain rule
and update the parameter ywith BP algorithm.
Step 4: Learn BX, with BY,x,yandlÔ¨Åxed
After updating original image hash function, we learn the
original hash codes BXand rewrite Eq.(7) as:
min
BXJ= min
BX(F BX2
F+1
2H BX2
F)
= min
BXtr
( 2F H)T(BX)
s:t:BX2f  1;+1gkm(21)
Obviously, when the sign of BXis different from
 (2F+H), Eq.21 will reach the minimum value. Namely,
BXand(2F+H)keep the same sign and formulates as:
BX= sign( (2F+H)) =sign(2F+H)(22)
Step 5: Learn BY, with BX,x,yandlÔ¨Åxed
Similarly, after updating original text hash function, we
have:
BY= sign( (2G+H)) =sign(2G+H)(23)
Combing Eq.(8)-(23) and the Step 1-5 in original learning
phase, the original hashing learning process can be summa-
rized in Algorithm 1.
C. Lifelong Hash Codes Learning
1) Lifelong Hashing Loss: To avoid performance degrada-
tion when training on incremental data and keep the learned
hash function continuously usable, we design lifelong hashing
loss which contains original similarity preserving loss and
incremental similarity preserving loss. To this end, we Ô¨Årstly
deÔ¨Åne original similarity preserving loss as Eq.(24) to keep
the similarity between training and original data.
Jold=(BX)TAX kSto2
F+(BY)TAY kSto2
F(24)
where Stowith the size of mais the multi-label semantic
similarity between training data and original data. Similarly,
to keep the similarity between training and incremental data,
we deÔ¨Åne incremental similarity preserving loss as:
Jnew=(BX0)TAX kSti2
F+(BY0)TAY kSti2
F;
(25)

--- PAGE 6 ---
6
Algorithm 1 The learning algorithm for original phase.
Input:
moriginal data O=fX;Y;Lg; Code length k; Mini-batch size Nl=Nx=
Ny= 128 ;
Output:
Original hash codes BXandBY; Networkh(),f()andg()with the parameters
l,xandy, respectively;
1:Initialize the parameters l,xandy, hash code matrix BXandBYrandomly;
iteration number Tl=dm=N le,Tx=dm=N xe,Ty=dm=N ye.
2:Calculate multi-label semantic similarity S=fSxl;Syl;Sll;Sxx;Syygaccord-
ing to Eq.41, respectively.
3:repeat
4: foriter= 1 toTldo
5: Randomly sample Nlinstances from Lto construct a mini-batch.
6: For each sampled instance liin the mini-batch, calculate H=h(li;l)by
forward propagation.
7: Calculate the derivative according to Eq.(9).
8: Update the parameter lby using back propagation.
9: end for
10: foriter= 1 toTxdo
11: Randomly sample Nxinstances from Xto construct a mini-batch.
12: For each sampled instance xiin the mini-batch, calculate F=f(xi;x)
by forward propagation.
13: Calculate the derivative according to Eq.(14).
14: Update the parameter xby using back propagation.
15: end for
16: foriter= 1 toTydo
17: Randomly sample Nyinstances from Yto construct a mini-batch.
18: For each sampled instance yiin the mini-batch, calculate G=g(yi;y)
by forward propagation.
19: Calculate the derivative according to Eq.(18).
20: Update the parameter yby using back propagation.
21: end for
22: UpdateBXaccording to Eq.(22).
23: UpdateBYaccording to Eq.(23).
24: until a Ô¨Åxed number of iterations
where Stiwith the size of nais multi-label semantic sim-
ilarity between training and incremental data. To approximate
discrete hash codes, we formulate quantization loss as:
Jquan =BX0 F0k2
F+BY0 G0k2
F (26)
where BX02f  1;+1gka2andBY02f  1;+1gka2are
the learned binary hash codes of a2incremental data sampled
to training set. F0=f(xa;x)2Rka2andG0=g(ya;y)2
Rka2are outputs of training data sample from incremental
set through the updated hash functions.
Considering that the larger is the information entropy,
the larger is information amount obtained after eliminating
uncertainty, we introduce balance loss which balances the
number of -1 and +1 for each bit to maximize the entropy
of hash bits as:
Jbalance =kF01k2
F+kG01k2
F(27)
By incorporating Eq.(27), the accumulation of each row
ofF0andG0is as close to 0 as possible, which achieves
balanced number of -1 and +1 on each bit. Combining original
similarity preserving loss, incremental similarity preserving
loss, quantization loss and balance loss, the Ô¨Ånal objective
function of lifelong hash learning phase can be deÔ¨Åned as:
min
BX0;BY0;x;yJ=Jold+Jnew+Jquan+Jbalance
s:t:BX02f  1;+1gkn
BY02f  1;+1gkn
(28)
whereandare hyper-parameters that control the weight
of each part.2) Optimization: We also utilize the alternating learning
strategy to update the hash function f()andg(), and their
respective parameters xandy, so as to Ô¨Ånd BX0andBY0.
Step 1: Learnx, with BX0,BY0,yÔ¨Åxed
Similar to the optimization of the original learning phase,
withBX0,BY0andyÔ¨Åxed, SGD and BP algorithm are
utilized to update the parameter xof image hash function
f(). Note that continuous function tanh()is used to Ô¨Åt AX,
and Eq.(28) can be rewritten as:
min
xJ=(BX)TAX kStok2
F+(BX0)TAX kSti2
F
+BX0 F02
F+kF01k2
F
(29)
For each image feature xiin training set, we compute the
gradient as:
@J
@F0
i= 2((BX
i)TAX
iBX
i) + 2(( BX0
i)TAX
iBX0
i)
+2(F0
i BX0
i) + 2F01(30)
Then, we can compute@J
@xwith@J
@F0
iby chain rule and
update the parameter xwith BP algorithm.
Step 2: Learny, with BX0,BY0,xÔ¨Åxed
We also use tanh()to Ô¨Åt AYto update parameter yof
text hash function g()withBX0,BY0,xÔ¨Åxed. Then, we
rewrite Eq.(28) as:
min
yJ=(BY)TAY kStok2
F+(BY0)TAY kSti2
F
+BY0 G02
F+kG01k2
F
(31)
For each text feature yjin training set, we compute the
gradient:
@J
@G0
j= 2((BY
j)TAY
jBY
j) + 2(( BY0
j)TAY
jBY0
j)
+2(G0
j BY0
j) + 2G01(32)
Then, we can compute@J
@ywith@J
@G0
iby chain rule and
update the parameter ywith BP algorithm.
Step 3: Learn BX0, with BY0,y,yÔ¨Åxed
When BY0,y,yare Ô¨Åxed, we denote BX02
f 1;+1gka2in Eq.(26) as BX0
a2. So we have:
min
BX0J=(BX0)TAX kSti2
F+BX0
a2 F02
F
=(BX0)TAX2
F 2ktr(BX0Sti(AX)T) +k2Sti2
F
+(BX0
a22
F 2tr(BX0
a2F0T) +kF0k2
F)
(33)
The goal is to update BX0, and Eq.(33) can be rewritten as:
min
BX0J=(BX0)TAX2
F 2ktr(BX0Sti(AX)T)
 2tr(BX0
a2F0T)(34)
To further simplify Eq.(34), BX0
a22f  1;+1gka2in the
third term can be transformed to BX02f  1;+1gkn. Thus,
we deÔ¨Åne a knmatrix ~F0as an extension of F0, i.e., the

--- PAGE 7 ---
7
corresponding bits in ~F0retain hash codes in F0and set the
other bits to 0. In this case, Eq.(34) can be rewritten as:
min
BX0J=(BX0)TAX2
F 2ktr(BX0Sti(AX)T)
 2tr(BX0~F0T)
=(BX0)TAX2
F 2tr((BX0)T(kAX(Sti)T+~F0))
=(BX0)TAX2
F+tr((BX0)TPX)
(35)
where PX= 2kAX(Sti)T 2~F0.
To ensure retrieval accuracy, we do not adopt the relaxation
strategy, but use discrete cyclic coordinate descent (DCC)
algorithm to optimize Eq.(35) and update BX0bit by bit. For
this, we denote BX0
ras the r-th row of BX0andBX0
 ras the
matrix of BX0excluding BX0
r. ForAXandPX,AX
ris the
r-th row of AX,AX
 ris the matrix of AX
rexcluding AX
r,
andPX
ris ther-th row of PX. Hence, Eq.(35) is transformed
as follows:
min
BX0
rJ=(BX0)TAX2
F+tr((BX0)TPX)
=tr(BX0
r(2(BX0
 r)TAX
 r(AX
r)T+ (PX
r)T))(36)
It is easy to Ô¨Ånd that Eq.(36) achieves the minimum when
each bit in BX0
rhas opposite sign to the corresponding bit
in
2(BX0
 r)TAX
 r(AX
r)T+ (PX
r)T
. Therefore, the optimal
solution of Eq.(36) is
BX0
r= sign(2(BX0
 r)TAX
 r(AX
r)T+ (PX
r)T) (37)
We can compute the r-th row of BX0with the above
formula, and then update all row of BX0by replacing BX0
r.
Step 4: Learn BY0, with BX0,y,yÔ¨Åxed
Similarly, when BX0,yandyare Ô¨Åxed, we simplify
Eq.(28) as follows:
min
BY0J=(BY0)TAY2
F+tr((BY0)TPY); (38)
where PY= 2kAY(Sti)T 2~G0.~G0is aknextension
matrix of G0, i.e., the corresponding bits in ~G0retain hash
codes in G0and the other bits are 0. Let BY0
rbe the r-th row
ofBY0,BY0
 rbe the matrix of BY0excluding BY0
r,AY
rbe
ther-th row of AY,AY
 rbe the matrix of AYexcluding AY
r,
PY
rbe the r-th row of PY. Thus, Eq.(38) is converted to the
following formula:
min
BY0
rJ=(BY0)TAY2
F+tr((BY0)TPY)
=tr(BY0
r(2(BY0
 r)TAY
 r(AY
r)T+ (PY
r)T))(39)
It is easy to Ô¨Ånd that BY0
rhas opposite sign to the cor-
responding bit in
2(BY0
 r)TAY
 r(AY
r)T+ (PY
r)T
. Hence,
we can obtain the optimal solution of Eq.(39), that is
BY0
r= sign(2(BY0
 r)TAY
 r(AY
r)T+ (PY
r)T) (40)
Subsequently, the r-th row of BX0can be calculated with
the above formula, and all row of BX0will be updated by
replacing BX0
r.
Combing Eq.(24)-(40) and the Step 1-4 in lifelong learning
phase, the lifelong hashing learning process can be summa-
rized in Algorithm 2.Algorithm 2 The learning algorithm for lifelong phase.
Input:
nincremental data O0=fX0;Y0;L0g; Original hash codes BXandBY;
Number of training data a; Code length k; Mini-batch size Nx=Ny= 128 .
Output:
Incremental hash codes BX0andBY0; Updated parameters xandy.
1:Sampleadata from original set and incremental set randomly as training set Oa=
fXa;Ya;Lag.
2:Initialize parameters xandy, hash code matrix BX0andBY0randomly;
iteration number Tx=da=Nxe,Ty=da=Nye.
3:Calculate multi-label semantic similarity S=fSto;Stigaccording to Eq.(41).
4:repeat
5: foriter= 1 toTxdo
6: Randomly sample Nxinstances from Xato construct a mini-batch.
7: For each sampled instance xiin the mini-batch, calculate F0=f(xi;x)
by forward propagation.
8: Calculate the derivative according to Eq.(30).
9: Update the parameter xby using back propagation.
10: end for
11: foriter= 1 toTydo
12: Randomly sample Nyinstances from Yato construct a mini-batch.
13: For each sampled instance yiin the mini-batch, calculate G0=g(yi;y)
by forward propagation.
14: Calculate the derivative according to Eq.(32).
15: Update the parameter yby using back propagation.
16: end for
17: UpdateBX0according to Eq.(37).
18: UpdateBY0according to Eq.(40).
19: until a Ô¨Åxed number of iterations
D. Multi-label Semantic Similarity
As mentioned before, the existing online cross-modal hash-
ing methods utilize single-label similarity to evaluate correla-
tion of two instances, i.e., they are considered as similar as
long as two instances have one common label. For example,
when instance x1shares one common label with instance x2,
and shares three same labels with instance x3, the single-label
concept holds that the semantic similarity of x1andx2,x1and
x3are both 1. It is obvious that instance x1andx3are more
similar as they have more common labels, which illustrates
the limitation of single-label similarity in describing similarity
relationships. Thus, to make full use of label information, we
adopt multi-label semantic similarity instead of coarse-grained
single-label similarity, which is deÔ¨Åned as follows:
So1o2
ij=Lo1
i
kLo1
ik2(Lo2
jLo2
j
2)T(41)
where Lo1
idenotes the label of i-th instance in o1modality ,
Lo2
jdenotes the label of j-th instance in o2modality, and
o1;22 fx;y;lg. The range of So1o2
ij is[0;1]. If instances
oiandojhave a larger So1o2
ij, it means that they are more
semantically similar, otherwise less similar.
For instances oiandojand their corresponding hash codes
biandbj, we utilize Hamming distance disH(bi;bj) =
1
2(K <bi;bj>)to measure the similarity between two hash
codes, where <bi;bj>denotes the inner product of biandbj.
When using single-label semantic similarity S0
ij=li(lj)T,
the likelihood function can be written as:
P(S0
ijjbi;bj) =(<bi;bj>);S0
ij= 1
1 (<bi;bj>);S0
ij= 0
=S0
ij(<bi;bj>) + (1 S0
ij)(1 (<bi;bj>));(42)
where(<bi;bj>) =1
1+e <bi;bj>. Given hash codes biand

--- PAGE 8 ---
8
bj, the probability of S0is:
L1=P(S0
ijjbi;bj) =nY
i;j=1e(S0
ij 1)<bi;bj>
1 +e <bi;bj>(43)
To facilitate calculation, take the logarithm of Eq.(43):
logL1= log(P(S0
ijjbi;bj))
=nP
i;j=1[(li(lj)T)<bi;bj> log(1 +e<bi;bj>)](44)
Similarly, for multi-label similarity Sij=li
klik2(lj
kljk2)T,
the likelihood function is:
P(Sijjbi;bj) =Sij(<bi;bj>) (45)
Given hash codes biandbj, the probability of Sis:
L2=P(Sijjbi;bj) =nY
i;j=1Sij
1 +e <bi;bj>(46)
Take the logarithm of the above formula:
logL2= log(P(Sijjbi;bj))
=nP
i;j=1[logli
klik2(lj
kljk2)T+<bi;bj> log(1 +e<bi;bj>)]
(47)
Combining Eq.(44) and Eq.(47), we can see both L1andL2
generate the same results when query instance oiand retrieved
instanceojhave only one common label. When oiandoj
have more than one common label which is more common
for existing cross-modal datasets, optimizing L2yields larger
inner product and smaller Hamming distance.
Given Ô¨Åxed Hamming radius, query instance oi, retrieved
instanceojand their corresponding hash codes biandbj, the
conditional probability of relevance rel(oi;oj)can be deÔ¨Åned
by the Bernoulli distribution as:
P(rel(oi;oj)jbi;bj) =(disH(bi;bj));rel(oij) = 1
1 (disH(bi;bj));rel(oij) = 0
(48)
From Eq.(48) we can see that smaller Hamming dis-
tancedisH(bi;bj)makes larger conditional probability
P(rel(oi;oj) = 1jbi;bj), which signiÔ¨Åes that oiandojshould
be judged as relevant. Otherwise, larger conditional probability
P(rel(oi;oj) = 0jbi;bj)indicates that it should be judged as
irrelevant. Therefore, for instances oiandoj, it is known that
the Hamming distance when optimizing L2is smaller than it
when optimizing L1, namely, the probability that oiandoj
are judged to be relevant when optimizing L2is larger than
the probability when optimizing L1.
Hash lookup, a widely used retrieval protocol in hashing-
based retrieval [10], considers the retrieved instance whose
Hamming distance to the query is less than Hamming radius
as a positive sample. When measuring the precision of hash
lookup protocol, we hope that a good hashing method can
retrieve as many positive samples as possible, i.e., when
the query instance oiand the retrieved instance ojare true
relevant, the probability of being judged as relevant should be
as large as possible, denoted as:
Precision (oi;oj)/P(rel(oi;oj) = 1jbi;bj) (49)As shown in Eq.(49), multi-label semantic similarity enables
smaller Hamming distance between relevant query instances
oiand retrieved instances oj, which results in higher precision.
Thus, we can conclude that the proposed multi-label semantic
similarity can effectively improve retrieved precision, which
will be proved with the subsequent experimental results.
E. Complexity Analysis
The computational complexity of original learning phase
is mainly generated by optimizing neural network parameters
and learning original hash codes. SpeciÔ¨Åcally, the complexity
of optimizing parameters l,xandyof LabelNet, ImgNet
and TxtNet can be calculated by Eq.(9), Eq.(14) and Eq.(18),
respectively, as O(m2k). The complexity of learning original
hash codes is found by Eq.(22) and Eq.(23), as O(mkmk) =
O(m2k2). Due tokm, the computational complexity of
original learning is O(m2).
The computational complexity of lifelong learning phase
is mainly generated by updating parameters of hash function
f(),g()and learning incremental hash codes. SpeciÔ¨Åcally,
the complexity of updating parameter xandycan be
calculated by Eq.(30) and Eq.(32), as O((m+n)ak), while
the complexity of updating the incremental hash codes is
computed by Eq.(37) and Eq.(40), as O(nkak) =O(ank2).
Sinceaandkare much smaller than mandn, the compu-
tational complexity of updating the parameters is O(m+n),
and that of learning incremental hash codes is O(n).
IV. E XPERIMENT
To verify the effectiveness of DLCH, extensive experiments
are carried out on three widely-used benchmark datasets. We
Ô¨Årstly compare our DLCH with several deep cross-modal hash-
ing algorithms including non-continuous and online methods
in retrieval performance, and analyze the results. Then, discuss
DLCH and its variants to analyze function of composition.
The implementation based on PyTorch can be available at
https://github.com.
A. Datasets
MIRFlickr25K is a multi-label dataset which contains
about 25000 image-text pairs associated with 24 class labels
and are collected from Flickr. The text annotation for each
point is represented as 1386-dimensional bag-of-words vector.
In our experiment, we select the image-text pairs with at least
20 text annotations, which yields 20,015 image text pairs.
NUS-WIDE is another common-used multi-label dataset
which includes over 269,000 image-text pairs with 81 class
labels. The text annotation for each data point is represented as
1000-dimensional bag-of-words vector. In our experiment, we
choose the pairs which belong to the 21 most frequently used
labels, where each label contains at least 5,000 data, resulting
in over 195,000 image-text pairs.
Wiki is a single-label dataset which contains 2,866 image-
text pairs with 10 class labels. The text annotation for each
data point is represented as a 128-dimensional SIFT vector. In
our experiment, we choose all the image-text pairs.

--- PAGE 9 ---
9
TABLE II: Detailed settings of experimental datasets
Set MIRFlickr NUS-WIDE Wiki
Total 20015 195834 2866
Query 2000 2100 693
Retrieval 18015 193734 2173
23/1 20/1 9/1
Class of original set 22/2 19/2 8/2
VS 21/3 18/3 7/3
Class of incremental set 20/4 17/4 6/4
12/12 10/11 4/6
For all datasets, 10% pairs from each class will be chose to
form testing set and the rest are database set. Within database
set, 90% pairs from will be used as training set and the
remaining pairs are used for validation set. We then resize
all of the images to be the size of 256 256. To simulate
that incremental data with categories appears continuously,
we divide the benchmark datasets into two parts, i.e., original
set and incremental set, according to class labels. For WiKi,
the split setting ‚Äô9/1‚Äô denotes that nine classes are selected as
original set, and the remain one is used for incremental set.
For MIRFlickr, ‚Äô23/1‚Äô means that all the data in original set
contain at most 23 class labels, and the rest one class data is
treated as incremental set. Similar setting for NUS-WIDE, and
the same are the other settings. The statistics and split setting
of three benchmark datasets are shown in TABLE II.
B. Implementation Details and Evaluations
All experiments were conducted on Intel(R) Xeon(R)
Gold6148CPU with 20 cores and eight Tesla V100-SXM2
GPUs with using the parameter setting in original papers to
ensure impartiality and objectivity. The batchsize is set to 64,
and the epoch is Ô¨Åxed to 2,000. Inspired by [39], [40], initial
learning rate in original and incremental hashing learning stage
are 0.001 and 0.000001, respectively. All the hyper-parameters
will be set automatically in the range of [10 2;105]according
cross-validation results. A detailed analysis and comparison of
parameter sensitivity will be provided.
In order to objectively evaluate the performance of DLCH,
we adopt two widely-used evaluations, including Mean Aver-
age Precision (MAP) and Precision-Recall Curves Hamming
distance 2. Then, we compare our method with nine state-
of-the-art hashing methods, including SSAH [34], DBRC
[35], RDCMH [36], SADCH [37], MESDCH [38], OCMFH
[27], OLSH [29], LEMON [30]and DOCH [32]. The Ô¨Årst
Ô¨Åve methods are non-continuous deep cross-modal hashing
methods and the rest are online cross-modal hashing methods.
Some have kindly provided the source codes, we refer to
parameters settings in original papers.
C. Results of hash retrieval
The MAP comparisons of DLCH with different hash bits on
three benchmark datasets are reported in Table V , from which
we can see that DLCH achieves better performance compared
to other baselines in most cases. In Table III, DLCH- idenotes
that the number of incremental classes.
Comparing with the optimal non-continuous baseline (i.e.
MESDCH and SADCH) when using image to retrieve text(i.e., I!T), ours achieves average increments of 3.1%, 12.8%
and 10.4% on MIRFlickr25K, NUS-WIDE and Wiki, re-
spectively. When using text to retrieve image (i.e., T !I),
ours also achieves average increments of 4.5%, 6.8%, and
6.7% on MIRFlickr25K, NUS-WIDE and Wiki, respectively.
Then, comparing with the online methods, ours also boosts
the performance signiÔ¨Åcantly, especially on I !T task. To
be speciÔ¨Åc, ours reaches average increments of 8.8%, 16.2%
comparing DOCH which performs the best on MIRFlickr25K
and NUS-WIDE. Additionally, when evaluating on Wiki, ours
achieve signiÔ¨Åcantly gains average increments of 21.6% and
20.2% on I!T and T!I task, respectively.
Apart from MAP evaluations, we also illustrate precision-
recall curves with different hash code lengths of comparative
methods on benchmark datasets in Fig.2, from which it can
be observed that ours achieve the best score on each length,
which is consistent with observations on the MAP scores. Note
that DLCH does not achieve best score on each dataset, and
it incurs minor drops on NUS-WIDE comparing with DOCH
when using text to retrieve image. It can be analyzed that
no incremental data with new category appears, which makes
comparisons in Table III and Fig.2 better than they would
have been if the new categories had appeared. Combing all
the results in Table III and Fig.2, we can conclude that our
proposed method outperforms than most of recent state-of-the-
art cross-modal hashing methods, including non-continuous
and online hashing. Then, the larger is the incremental classes,
the higher is the performance.
D. Catastrophic Forgetting Analysis
As mentioned before, almost all the existing cross-modal
hashing method is suffering from catastrophic forgetting when
when adding the incremental data with new categories and
using trained hash functions to retrieve the incremental data.
We now provide some results on catastrophic forgetting eval-
uations as shown in Fig.3.
From Fig.3, it can be clearly seen that all the non-continuous
cross-modal hashing methods suffer from serve performance
degradation due to the catastrophic forgetting. Moreover, when
there are more new categories, the performance degrades
seriously. Then, we have to note that the online learning
process alleviates the phenomenon to some extent. However, it
still incurs unacceptable drops. Taking DOCH as an example
shown in Fig.5, for each additional category of data, the
performance will respectively decrease about by 3.2% and
3.7% on I!T and T!I task, respectively. Comparing with
these methods, our DLCH basically maintains the performance
and sometimes even improves as the number of new categories
increases, which beneÔ¨Åts from the proposed lifelong hashing
learning strategy.
E. Time cost analysis
Apart from retrieval accuracy and catastrophic forgetting,
training time and resource cost are also concerned. As an-
alyzed before, the complexity of incremental hash codes
learning in our method is linearly dependent on the size of

--- PAGE 10 ---
10
TABLE III: MAP Scores of Cross-modal Retrieval Task on Benchmark Datasets with Different Lengths of Hash Codes. The bold and
underlined indicate the best and the second performance. All deep methods are based on VGG19 features.
Tasks Fashions MethodsMIRFlickr25K NUS-WIDE Wiki
16bits 32bits 48bits 64bits 16bits 32bits 48bits 64bits 16bits 32bits 48bits 64bits
SSAH [34] 0.780 0.791 0.793 0.795 0.621 0.630 0.623 0.615 0.253 0.306 0.317 0.327
DBRC [35] 0.587 0.590 0.590 0.590 0.394 0.409 0.413 0.417 0.253 0.265 0.267 0.269
Non-continuous RDCMH [36] 0.772 0.774 0.777 0.779 0.623 0.624 0.626 0.627 0.294 0.297 0.299 0.300
Hashing SADCH [37] 0.759 0.784 0.805 0.826 0.649 0.706 0.730 0.753 0.360 0.402 0.406 0.410
MESDCH [38] 0.813 0.830 0.834 0.837 0.653 0.670 0.673 0.675 0.379 0.371 0.376 0.381
OCMFH [27] 0.635 0.632 0.632 0.631 0.412 0.423 0.419 0.414 0.177 0.186 0.187 0.188
I!T Online OLSH [29] 0.671 0.677 0.679 0.680 0.595 0.605 0.606 0.606 0.243 0.252 0.256 0.259
Hashing LEMON [30] 0.742 0.748 0.750 0.752 0.656 0.681 0.676 0.671 0.367 0.382 0.399 0.416
DOCH [32] 0.762 0.766 0.774 0.781 0.647 0.654 0.658 0.662 0.411 0.422 0.423 0.424
DLCH-1 0.848 0.865 0.872 0.879 0.819 0.831 0.835 0.838 0.526 0.537 0.530 0.523
Lifelong DLCH-2 0.834 0.859 0.870 0.880 0.821 0.834 0.837 0.840 0.472 0.507 0.494 0.480
Hashing DLCH-3 0.834 0.845 0.861 0.877 0.808 0.835 0.836 0.836 0.502 0.493 0.483 0.472
DLCH-4 0.839 0.857 0.865 0.873 0.816 0.844 0.846 0.847 0.462 0.465 0.470 0.475
SSAH 0.791 0.800 0.791 0.782 0.623 0.627 0.625 0.622 0.228 0.283 0.297 0.311
DBRC 0.588 0.596 0.596 0.596 0.425 0.429 0.434 0.438 0.544 0.538 0.543 0.548
Non-continuous RDCMH 0.749 0.752 0.756 0.765 0.605 0.605 0.609 0.613 0.293 0.296 0.299 0.301
Hashing SADCH 0.773 0.795 0.805 0.814 0.680 0.717 0.730 0.743 0.624 0.632 0.632 0.632
MESDCH 0.802 0.812 0.815 0.818 0.651 0.672 0.676 0.679 0.589 0.602 0.605 0.608
OCMFH 0.711 0.722 0.736 0.749 0.423 0.417 0.428 0.439 0.410 0.455 0.460 0.464
T!I Online OLSH 0.709 0.720 0.729 0.738 0.705 0.713 0.717 0.720 0.507 0.544 0.564 0.583
Hashing LEMON 0.816 0.829 0.832 0.834 0.778 0.787 0.792 0.796 0.641 0.680 0.681 0.681
DOCH 0.817 0.832 0.841 0.850 0.779 0.795 0.803 0.810 0.647 0.653 0.662 0.671
DLCH-1 0.815 0.827 0.839 0.850 0.761 0.782 0.790 0.797 0.702 0.729 0.728 0.727
Lifelong DLCH-2 0.820 0.839 0.835 0.831 0.778 0.790 0.795 0.799 0.651 0.689 0.705 0.721
Hashing DLCH-3 0.814 0.820 0.827 0.833 0.753 0.781 0.787 0.793 0.649 0.690 0.692 0.693
DLCH-4 0.813 0.820 0.821 0.822 0.767 0.787 0.790 0.792 0.632 0.698 0.694 0.690
Fig. 2: Precision-Recall Curves Evaluated on MIRFilckr, NUS-WIDE and Wiki datasts.
Fig. 3: Results on catastrophic forgetting evaluations.
incremental data n. Thus, we further conduct experiments to
demonstrate it, and the results are shown in Table IV .
As shown in Table IV , our DLCH possesses absolute
advantage comparing with non-continuous methods, and the
advantage is more obvious on larger scale dataset. When in-
cremental data is added to the database, these baseline methodsneed to use all the accumulated data for retraining, which
requires more training time in repeat way. On the contrary,
DLCH only utilizes incremental data to update hash functions,
which signiÔ¨Åcantly reduces training time and resource cost.
It can be also observed that ours need more training time
comparing with other online methods, which results from that

--- PAGE 11 ---
11
TABLE IV: Training Time Comparisons (in Minutes) of Baselines
on Benchmark Datasets with One Incremental Class and 32 bits.
Learning Fashion Methods MIRFlicker NUS-WIDE WIKI
Non-continuous
HashingDCMH [39] 227.8 17170.1 20.6
SSAH 431.2 22000.4 25.8
DBRC 358.8 20681.0 35.4
AADCMH [40] 447.8 30488.6 45.2
AGCN [41] 289.3 21085.9 34.6
MESDCH 337.8 20201.1 25.4
Online HashingOCMFH 32.6 310.1 2.3
OLSH 33.9 297.6 2.1
LEMON 46.3 360.1 2.3
DOCH 41.2 357.2 2.5
OMGH 41.6 393.1 2.6
Lifelong Hashing DLCH 76.1 1016.2 6.7
TABLE V: Optimum Parameter of Different Tasks on Benchmark
Datasets.
ParametersMIRFlicker NUS-WIDE WIKI
T!II!T T!I I!T T!I I!T
 10 10 10000 10000 10000 10000
 10 100 12 10 7 9
 1 1 1 1 0.6 0.8
 10 10 200 200 1000 200
 10 100 50 50 1 1
our DLCH is deep learning-based methods. Combining with
the above experimental results, we can Ô¨Ånd that DLCH has
excellent time performance, which is more signiÔ¨Åcant on large-
scale datasets. Considering the high retrieval accuracy and low
performance performance degradation, the proposed DLCH
possesses strong competitiveness comparing with these cross-
modal hashing methods.
F . Sensitivity to parameters
There are Ô¨Åve hyper-parameters in DLCH, where ,and
are designed for original learning phase and andare
used in lifelong learning phase. We now evaluate the inÔ¨Çuence
of hyper-parameters in controlling weight ratio among losses.
We Ô¨Årstly set the hash code length k= 64 . Then, we calculate
MAP values by adjusting the parameters between 10 2and
105with a multiplication step of 10. Fig.4 shows experimental
results of hyper-parameters ,,,andon MIRFlickr.
As illustrated in Fig.4, different andhave more obvious
impact on the MAP value, while it changes more gently with
. During incremental hashing learning, anddo not have
particularly great impact on MAP values. According to the
similar way, the best optimum values for three benchmark
datasets are summarized in TABLE V .
G. Ablation experiment
In order to achieve lifelong hashing retrieval, several losses
are deÔ¨Åned. We now investigate the variants of DLCH to fur-
ther analyze effectiveness of each loss. During the exploration,
we compare these variants on MIRFlickr dataset with setting
the number of incremental class be 1.
Firstly, we design three variants, DLCH-intra, DLCH-inter
and DLCH-quant, to make sure original hashing loss function.
DLCH-intra, DLCH-inter and DLCH-quant are the DLCH
variant which remove intra-modality, inter-modality similarityTABLE VI: MAP Comparison in Original Hashing.
MethodsI!T T!I
16bits 32bits 64bits 16bits 32bits 64bits
DLCH 0.848 0.865 0.879 0.815 0.827 0.850
DLCH-intra 0.823 0.841 0.837 0.743 0.717 0.735
DLCH-inter 0.808 0.828 0.836 0.739 0.730 0.732
DLCH-quant 0.810 0.835 0.860 0.791 0.819 0.839
DLCH-O 0.841 0.851 0.872 0.811 0.822 0.838
DLCH-I 0.836 0.855 0.877 0.806 0.819 0.836
DLCH-Q 0.833 0.852 0.877 0.803 0.810 0.837
DLCH-B 0.835 0.851 0.879 0.811 0.820 0.832
TABLE VII: MAP Comparison of Label Semantic Similarity.
MethodsI!T T!I
16bits 32bits 64bits 16bits 32bits 64bits
DLCH-multi 0.848 0.865 0.879 0.815 0.827 0.850
DLCH-single 0.823 0.841 0.837 0.743 0.717 0.735
preserving and quantization loss, respectively. Then, to explore
the effectiveness of lifelong hashing, four additional variants
are designed. DLCH-O, DLCH-L, DLCH-Q and DLCH-B are
the variants without using original similarity preserving loss,
lifelong hashing loss, quantization loss and bit balance loss,
respectively. The comparisons are shown Table VI.
During the original hashing learning, it can be seen from
Table VI that DLCH achieves average increments of 4.1% and
3.7% on all bits over DLCH-intra which does not consider
the similarity among intra modalities these two tasks. DLCH
gains average increments of 7.3% and 8.1% over DLCH-
inter which ignores inter-similarity. Then, DLCH-quant uses
continuous relaxation to address discrete optimization and
results, which naturally incurs some drops comparing with
DLCH. During the incremental hashing learning, due to the
introduction of intra-modality and inter-similarity similarity
preserving, and quantization constraints, DLCH also gains
clear advantages over DLCH-O, DLCH-L, DLCH-Q and
DLCH-B. Lifelong hashing loss is designed to keep the trained
hash function continuously usable by preserving the similarity
between original and incremental data. From Table VI, we
can see that DLCH achieves higher increments, i.e., 6.7% and
7.5% comparing with DLCH-O, which means that it indeed
preserves performance and avoids catastrophic forgetting.
Furthermore, in order to verify the validity of multi-label
semantic similarity, we compare the variants which use multi-
and single-label semantic similarity, and the comparisons are
shown in Table VII. From Table VII, it can be seen that DLCH
with multi-label semantic similarity improves retrieval accu-
racy by 20% comparing with single-label semantic similarity,
which is consistent with the analysis in Section III-D.
V. C ONCLUSION
In this paper, we propose a novel deep lifelong cross-modal
hashing, which effectively solves the problem brought by
new categories appearing in database. Our method is divided
into two phases: original learning and lifelong learning. We
employ a multi-label semantic similarity for the Ô¨Årst phase
to supervise the learning of hash functions and obtain high-
quality original hash codes. Meanwhile, we design a lifelong
hashing loss for the second phase, where incremental hash

--- PAGE 12 ---
12
Fig. 4: Sensitivity analysis on MIRFlickr Dataset in the range of [10 2;105].
codes are directly learned while original hash code remains
unchanged. Extensive experiments on three well-known real-
world benchmarks show that the proposed method has better
performance than other baseline methods with reducing train-
ing time signiÔ¨Åcantly and is an effective cross-modal hashing
retrieval method.
ACKNOWLEDGMENT
The authors would like to thank the anonymous review-
ers for their help. This work was supported by the Na-
tional Natural Science Foundation of China (Grant No.
62176217, 62206224), the Natural Science Foundation of
Sichuan Province (Grant No. 2022NSFSC0866), the Innova-
tion Team Funds of China West Normal University (Grant
No. KCXTD2022-3), and the Doctoral Research Innovation
Project (Grant No. 21E025).
REFERENCES
[1] P. Kaur, H. Pannu and A. Malhi, ‚ÄúComparative analysis on cross-modal
information retrieval: A review,‚Äù in Comput. Sci. Rev. , vol. 39, pp.
100336, 2021.
[2] W. Cao, W. Feng, Q. Lin, G. Cao and Z. He, ‚ÄúA Review of Hashing
Methods for Multimodal Retrieval,‚Äù in IEEE Access , vol. 8, pp. 15377-
15391, 2020.
[3] G. Menghani, ‚ÄúEfÔ¨Åcient Deep Learning: A Survey on Making Deep
Learning Models Smaller, Faster, and Better,‚Äù in ACM Comput. Surv. ,
vol. 55, no. 12, pp. 1-37, 2023.
[4] M. Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh and T. Tuytelaars, ‚ÄúA Continual Learning Survey: Defying
Forgetting in ClassiÔ¨Åcation Tasks,‚Äù in IEEE Trans. Pattern Anal. Mach.
Intell. , vol. 44, pp. 3366-3385, 2021.
[5] X. Luo, C. Chen, H. Zhong, H. Zhang, M. Deng, J. Huang and X. Hua,
‚ÄúA Survey on Deep Hashing Methods,‚Äù ACM Trans. Knowl. Discov.
Data. , vol. 17, no. 1, pp. 1-50, 2023.
[6] Y . Peng, H. Xin, and Y . Zhao, ‚ÄúAn Overview of Cross-Media Retrieval:
Concepts, Methodologies, Benchmarks and Challenges,‚Äù IEEE Trans.
Cir. Syst. Video Techn. , vol. 28, no. 9, pp. 2372-2385, 2019.[7] T. Hoang, T. Do, T. Nguyen and N. Cheung, ‚ÄúUnsupervised Deep Cross-
modality Spectral Hashing,‚Äù IEEE Trans. Image Process. , vol. 29, pp.
8391-8406, 2020.
[8] S. Liu, S. Qian, Y . Guan, J. Zhan and L. Ying, ‚ÄúJoint-modal Distribution-
based Similarity Hashing for Large-scale Unsupervised Deep Cross-
modal Retrieval,‚Äù in Proc. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr. ,
2020, pp. 1618-1625.
[9] J. Yu, H. Zhou, Y . Zhan and D. Tao, ‚ÄúDeep Graph-neighbor Coherence
Preserving Network for Unsupervised Cross-modal Hashing,‚Äù Proc.
AAAI Conf. Artif. Intell. , 2021, pp. 4626-4634.
[10] Y . Wang, B. Xue, Q. Cheng, Y . Chen and L. Zhang, ‚ÄúDeep UniÔ¨Åed
Cross-Modality Hashing by Pairwise Data Alignment,‚Äù Proc. Int. Joint
Conf. Artif. Intell. , 2021, pp. 1129-1135.
[11] M. Li and H. Wang, ‚ÄúUnsupervised Deep Cross-Modal Hashing by
Knowledge Distillation for Large-scale Cross-modal Retrieval,‚Äù in Proc.
Int. Conf. Multimedia Retr. , 2021, pp. 183-191.
[12] T. Hoang, T. Do, T. Nguyen and N. Cheung, ‚ÄúMultimodal Mutual
Information Maximization: A Novel Approach for Unsupervised Deep
Cross-Modal Hashing,‚Äù in IEEE Trans. Neural Networks Learn. Syst. ,
pp. 1-14, 2022.
[13] R. Tu, J. Jiang, Q. Lin, C. Cai, S. Tian, H. Wang and W. Liu, ‚ÄúUnsuper-
vised Cross-modal Hashing with Modality-interaction,‚Äù in IEEE Trans.
Circuits Syst. Video Technol. , 2023.
[14] H. Yao, Y . Zhan, Z. Chen, X. Luo and X. Xu, ‚ÄúTEACH: Attention-Aware
Deep Cross-Modal Hashing,‚Äù in Proc. Int. Conf. Multimedia Retr. , 2021,
pp. 376-384.
[15] X. Zou, S. Wu, N. Zhang and E. Bakker, ‚ÄúMulti-label modality enhanced
attention based self-supervised deep cross-modal hashing,‚Äù in Knowl.
Based Syst. , vol. 239, pp. 107927, 2022.
[16] F. Yang, Y . Liu, X. Ding, F. Ma and J. Cao, ‚ÄúAsymmetric crossCmodal
hashing with highClevel semantic similarity,‚Äù in Pattern Recognit. , vol.
130, pp. 108823, 2022.
[17] X. Liu, H. Zeng, Y . Shi, J. Zhu, C. Hsia and K. Ma, ‚ÄúDeep Cross-
modal Hashing Based on Semantic Consistent Ranking,‚Äù in IEEE Trans.
Multimedia , pp. 1-12, 2023.
[18] Z. Wang, Q. She and T. Ward, ‚ÄúGenerative Adversarial Networks in
Computer Vision: A Survey and Taxonomy,‚Äù in ACM Comput. Surv. ,
vol. 54, no. 2, pp. 1-38, 2021.
[19] X. Ma, T. Zhang, and C. Xu, ‚ÄúMulti-Level Correlation Adversarial
Hashing for Cross-Modal Retrieval,‚Äù IEEE Trans. Multimedia , vol. 20,
no. 4, pp. 1224-1237, 2020.
[20] Xi Zhang and Hanjiang Lai and Jiashi Feng, ‚ÄúDeep Adversarial Discrete
Hashing for Cross-Modal Retrieval,‚Äù in Proc. ACM SIGMM Int. Conf.
Multimedia Retr. , 2020, pp. 525-531.

--- PAGE 13 ---
13
[21] J. Zhang and Y . Peng, ‚ÄúMulti-Pathway Generative Adversarial Hashing
for Unsupervised Cross-Modal Retrieval,‚Äù in IEEE Trans. Multimedia ,
vol. 22, pp. 174-187, 2020.
[22] D. Xie, C. Deng, C. Li, X. Liu and D. Tao, ‚ÄúMulti-Task Consistency-
Preserving Adversarial Hashing for Cross-Modal Retrieval,‚Äù in IEEE
Trans. Image Process. , vol. 29, pp. 3626-3637, 2020.
[23] S. Qian, D. Xue, H. Zhang, Q. Fang and C. Xu, ‚ÄúDual Adversarial Graph
Neural Networks for Multi-label Cross-modal Retrieval,‚Äù in Proc. AAAI
Conf. Artif. Intell. , 2021, pp. 2440-2448.
[24] M. Li, Q. Li, Y . Ma and D. Yang, ‚ÄúSemantic-guided autoencoder
adversarial hashing for large-scale cross-modal retrieval,‚Äù in Complex
Intell. Syst. , vol. 8, pp. 1603-1617, 2022.
[25] J. Li, E. Yu, J. Ma, X. Chang, H. Zhang and J. Sun, ‚ÄúDiscrete Fusion
Adversarial Hashing for cross-modal retrieval,‚Äù in Knowledge-Based
Syst., vol. 253, pp. 109503, 2022.
[26] W. Ou, J. Deng, L. Zhang, J. Gou and Q. Zhou, ‚ÄúCross-Modal Gener-
ation and Pair Correlation Alignment Hashing,‚Äù in IEEE Trans. Intell.
Transp. Syst. , vol. 24, pp. 3018-3026, 2023.
[27] D. Wang and Q. Wang, Y . An, X. Gao and Y . Tian, ‚ÄúOnline Collective
Matrix Factorization Hashing for Large-Scale Cross-Media Retrieval,‚Äù in
Proc. Int. ACM SIGIR Conf. Res. Deve. Inf. Retr. , 2020, pp. 1409-1418.
[28] L. Xie, J. Shen and L. Zhu, ‚ÄúOnline Cross-Modal Hashing for Web
Image Retrieval,‚Äù in Proc. AAAI Conf. Artif. Intell. , 2016, pp. 294-300.
[29] T. Yao, G. Wang, L. Yan, X. Kong, Q. Su, C. Zhang and Q. Tian, ‚ÄúOnline
latent semantic hashing for cross-media retrieval,‚Äù in Pattern Recognit. ,
vol. 89, pp. 1-11, 2019.
[30] Y . Wang, X. Luo and X. Xu, ‚ÄúLabel Embedding Online Hashing for
Cross-Modal Retrieval,‚Äù Proc. ACM Int. Conf. Multimedia , 2020, pp.
871-879.
[31] J. Yi, X. Liu, Y . Cheung, X. Xu, W. Fan and Y . He, ‚ÄúEfÔ¨Åcient Online
Label Consistent Hashing for Large-Scale Cross-Modal Retrieval,‚Äù in
Proc. IEEE Int. Conf. Multimedia Expo , 2021, pp. 1-6.
[32] Y . Zhan, Y . Wang, Y . Sun, X. Wu, X. Luo and X. Xu, ‚ÄúDiscrete online
cross-modal hashing,‚Äù Pattern Recognit. , vol. 122, pp. 108262, 2022.
[33] X. Liu, J. Yi, Y . Cheung, X. Xu and Z. Cui, ‚ÄúOMGH: Online Manifold-
Guided Hashing for Flexible Cross-modal Retrieval,‚Äù in IEEE Trans.
Multimedia , 2022.
[34] C.Li, C. Deng, N. Li, W. Liu, X. Gao, and D. Tao, ‚ÄúSelf-supervised
adversarial hashing networks for cross-modal retrieval, ‚Äù Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. , 2018, pp. 4242-4251.
[35] D. Hu, F. Nie, and X. Li, ‚ÄúDeep Binary Reconstruction for Cross-Modal
Hashing,‚Äù in IEEE Trans. Multimedia , vol. 21, no. 4, pp. 973-985, 2019.
[36] X. Liu, G. Yu, C. Domeniconi, J. Wang, Y . Ren and M. Guo, ‚ÄúRanking-
based Deep Cross-modal Hashing,‚Äù in Proc. AAAI Conf. Artif. Intell. ,
2019, pp. 4400-4407.
[37] Y . Wang, X. Shen, Z. Tang, T. Zhang and J. Lv, ‚ÄùSemi-Paired Asym-
metric Deep Cross-Modal Hashing Learning,‚Äù in IEEE Access , vol. 8,
pp. 113814-113825, 2020.
[38] X. Zou, S. Wu, M. B. Erwin and X. Wang, ‚ÄúMulti-label enhancement
based self-supervised deep cross-modal hashing,‚Äù in Neurocomputing ,
vol. 467, pp. 138-162, 2022.
[39] Q. Jiang and W. Li, ‚ÄúDeep Cross-Modal Hashing,‚Äù in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit. , 2017, pp. 3232-3240.
[40]
[41] X. Dong, L. Liu, L. Zhu, L. Nie and H. Zhang, ‚ÄùAdversarial Graph
Convolutional Network for Cross-Modal Retrieval,‚Äù in IEEE Trans.
Circuits Syst. Video Technol. , vol. 32, pp. 1634-1645, 2021.
