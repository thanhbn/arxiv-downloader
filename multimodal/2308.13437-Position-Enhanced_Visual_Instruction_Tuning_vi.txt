# Tăng cường hướng dẫn thị giác dựa trên vị trí
# cho các mô hình ngôn ngữ lớn đa phương thức

Chi Chen1, Ruoyu Qin1, Fuwen Luo1, Xiaoyue Mi3, Peng Li2, Maosong Sun1, Yang Liu1,2
1Khoa Khoa học Máy tính & Công nghệ, Viện AI, Đại học Thanh Hoa
2Viện Nghiên cứu Công nghiệp AI (AIR), Đại học Thanh Hoa
3Viện Công nghệ Tính toán, Viện Hàn lâm Khoa học Trung Quốc

Tóm tắt
Gần đây, các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) cho phép các Mô hình Ngôn ngữ Lớn (LLMs) diễn giải hình ảnh thông qua điều chỉnh hướng dẫn thị giác đã đạt được thành công đáng kể. Tuy nhiên, các phương pháp điều chỉnh hướng dẫn thị giác hiện tại chỉ sử dụng dữ liệu hướng dẫn hình ảnh-ngôn ngữ để căn chỉnh các phương thức ngôn ngữ và hình ảnh, thiếu sự căn chỉnh đa phương thức tinh tế hơn. Trong bài báo này, chúng tôi đề xuất Điều chỉnh Hướng dẫn Thị giác Tăng cường Vị trí (PVIT), mở rộng chức năng của MLLMs bằng cách tích hợp một bộ mã hóa thị giác cấp vùng bổ sung. Việc tích hợp này thúc đẩy sự hiểu biết chi tiết hơn về hình ảnh cho MLLM. Ngoài ra, để đạt được sự căn chỉnh tinh tế hiệu quả giữa các mô-đun thị giác và LLM, chúng tôi thiết kế nhiều chiến lược tạo dữ liệu để xây dựng tập dữ liệu hướng dẫn hình ảnh-vùng-ngôn ngữ. Cuối cùng, chúng tôi trình bày cả thí nghiệm định lượng và phân tích định tính chứng minh tính ưu việt của mô hình đề xuất. Mã và dữ liệu sẽ được phát hành tại https://github.com/PVIT-official/PVIT.

1 Giới thiệu
Gần đây, các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs) đã có tiến bộ đáng kể trong việc cho phép các Mô hình Ngôn ngữ Lớn (LLMs) hiện tại hiểu hình ảnh. Nguyên lý cơ bản của các phương pháp này là tích hợp khả năng của các mô hình thị giác hoặc đa phương thức hiện có vào LLM. Các MLLMs hiện tại có thể được phân loại thành hai lớp dựa trên cách chúng đạt được điều này. Lớp đầu tiên trực tiếp tận dụng khả năng zero-shot và few-shot của các mô hình ngôn ngữ, cho phép LLM gọi các mô hình đa phương thức bên ngoài bằng cách thiết kế các lời nhắc cụ thể. Loại thứ hai căn chỉnh các đặc trưng thị giác với không gian biểu diễn của mô hình ngôn ngữ thông qua điều chỉnh hướng dẫn thị giác, đạt được tích hợp mô hình end-to-end. Các MLLMs end-to-end này có nhiều khả năng đa phương thức hơn loại đầu tiên và do đó đang nhận được ngày càng nhiều sự chú ý.

Bất chấp thành công của chúng, các MLLMs end-to-end này chỉ căn chỉnh bộ mã hóa thị giác cấp hình ảnh đã được huấn luyện trước với LLM bằng cách sử dụng dữ liệu hướng dẫn hình ảnh-ngôn ngữ. Không có dữ liệu hướng dẫn đa phương thức tinh tế, khả năng của các mô hình để hiểu hình ảnh chi tiết vẫn còn hạn chế. Ví dụ, trong trường hợp minh họa ở Hình 1, việc phân biệt các vật thể cụ thể trong cảnh phức tạp là thách thức đối với các MLLMs hiện tại. Ngoài ra, định dạng của dữ liệu hướng dẫn thị giác hiện tại cũng hạn chế khả năng của mô hình tuân theo các hướng dẫn phức tạp hơn, chẳng hạn như những hướng dẫn chứa thông tin không gian (ví dụ, "Vật thể này trong [VÙNG] là gì?" trong Hình 1). Những loại hướng dẫn này có tiềm năng giảm độ phức tạp của tương tác với mô hình và tăng cường độ chính xác của các hướng dẫn được cung cấp. Do đó, để tiếp tục tăng cường các MLLMs hiện có, việc khám phá dữ liệu hướng dẫn đa phương thức tinh tế là rất quan trọng, đặc biệt là các hướng dẫn liên quan đến thông tin không gian, để cho phép mô hình đạt được sự hiểu biết chi tiết hơn về hình ảnh và tạo điều kiện cho các tương tác linh hoạt hơn.

Điều này đặt ra hai thách thức. Một mặt, không có nhiều dữ liệu đa phương thức căn chỉnh tinh tế so với các cặp hình ảnh-văn bản, chưa kể đến dữ liệu hướng dẫn tương ứng để điều chỉnh tinh MLLM. Mặt khác, cách sử dụng những dữ liệu này để mở rộng và tăng cường hiệu quả khả năng của MLLM là một câu hỏi mở. Một số công trình đồng thời của chúng tôi đã thực hiện những nỗ lực sơ bộ bằng cách điều chỉnh tinh các MLLMs hiện có để hỗ trợ các hướng dẫn đa phương thức với tọa độ không gian. Cụ thể, Chen et al. và Zhao et al. trực tiếp kết hợp tọa độ không gian ở dạng số trong ngôn ngữ tự nhiên vào dữ liệu hướng dẫn và điều chỉnh tinh MLLM để hiểu chúng. Zhang et al. trước tiên trích xuất các đặc trưng không gian từ bộ mã hóa thị giác dựa trên các vùng đầu vào và tích hợp chúng vào các hướng dẫn ngôn ngữ tự nhiên như đầu vào cho LLM. Tuy nhiên, cần lưu ý rằng bộ mã hóa thị giác được sử dụng trong các MLLMs hiện có, chẳng hạn như CLIP, được huấn luyện trước bằng giám sát cấp hình ảnh và vốn có khả năng hiểu hình ảnh tinh tế hạn chế. Do đó, việc điều chỉnh tinh trực tiếp trên cơ sở này có thể không tối ưu và có thể xung đột với các khả năng đã có của MLLM. Với sự có sẵn của các mô hình Huấn luyện Trước Thị giác và Ngôn ngữ (VLP) căn chỉnh cấp vùng, một khả năng thú vị nảy sinh: Chúng ta có thể tăng cường thêm MLLMs bằng cách tích hợp khả năng của các bộ mã hóa thị giác cấp vùng không?

Trong bài báo này, chúng tôi đề xuất Điều chỉnh Hướng dẫn Thị giác Tăng cường Vị trí (PVIT) mở rộng MLLM bằng cách kết hợp một bộ mã hóa thị giác cấp vùng bổ sung để tạo điều kiện hỗ trợ cho các đầu vào dựa trên vùng, như được thể hiện trong Hình 1. Cụ thể, chúng tôi sử dụng bộ mã hóa thị giác từ RegionCLIP và sử dụng nó để trích xuất các đặc trưng cấp vùng bằng cách lấy hình ảnh và vùng làm đầu vào. Là một nguồn thông tin bổ sung, việc kết hợp các đặc trưng cấp vùng theo cách này có tác động tối thiểu đến MLLM gốc. Hơn nữa, vì các đặc trưng được cung cấp bởi RegionCLIP đã được căn chỉnh với ngôn ngữ ở mức tinh tế, chi phí căn chỉnh nó với MLLM sẽ tương đối nhỏ. Được lấy cảm hứng từ Liu et al., chúng tôi thiết kế một chiến lược huấn luyện hai giai đoạn cho PVIT, trước tiên huấn luyện trước một phép chiếu tuyến tính để căn chỉnh các đặc trưng vùng với phần nhúng từ LLM, sau đó điều chỉnh tinh end-to-end để tuân theo các hướng dẫn tinh tế phức tạp.

Như đã đề cập trước đó, dữ liệu hướng dẫn đa phương thức tinh tế rất hiếm, điều này ảnh hưởng đến nghiên cứu liên quan từ cả khía cạnh huấn luyện và đánh giá. Để đạt được điều này, chúng tôi đề xuất một sơ đồ tạo dữ liệu hướng dẫn cấp vùng, thiết kế các phương pháp khác nhau dựa trên các nguồn dữ liệu khác nhau để phù hợp với nhu cầu tạo dữ liệu hướng dẫn cấp vùng. Ngoài ra, chúng tôi trình bày một tập dữ liệu đánh giá mới, FineEval, được thiết kế đặc biệt để đánh giá khả năng của MLLMs tuân theo các hướng dẫn đòi hỏi chi tiết không gian tinh tế. Chúng tôi hy vọng dữ liệu được trình bày sẽ giúp ích cho nghiên cứu tương lai trong lĩnh vực này.

Để tóm tắt, đóng góp của chúng tôi có ba phần:
• Chúng tôi giới thiệu điều chỉnh hướng dẫn thị giác tăng cường vị trí (PVIT), một phương pháp mở rộng khả năng hiểu biết và tương tác tinh tế cho MLLM.
• Chúng tôi đề xuất một sơ đồ xây dựng dữ liệu hướng dẫn cấp vùng, cũng như một tập dữ liệu đánh giá để tạo điều kiện cho việc huấn luyện và đánh giá PVIT.
• Chúng tôi thực hiện các thí nghiệm rộng rãi và chứng minh tính hiệu quả của phương pháp đề xuất.

2 Công trình liên quan
2.1 Mô hình Ngôn ngữ Lớn Đa phương thức
Để tận dụng khả năng zero-shot và lý luận mạnh mẽ của LLMs, ngày càng nhiều công trình đã chuyển sang xây dựng các mô hình thị giác và ngôn ngữ dựa trên LLMs, được gọi là Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs). Cụ thể, những MLLMs này có thể được phân loại thành hai lớp. Một số công trình trực tiếp sử dụng khả năng học zero-shot và few-shot của LLM có sẵn để diễn giải ý định của người dùng và gọi các mô hình đa phương thức bên ngoài tương ứng. Mặc dù những mô hình này cho phép các khả năng đa phương thức linh hoạt, hiệu suất của chúng phụ thuộc vào khả năng của LLM và bản thân mô hình bên ngoài, do đó bị hạn chế. Một loạt công trình khác đạt được tích hợp mô hình end-to-end bằng cách căn chỉnh các đặc trưng đầu ra của bộ mã hóa thị giác với không gian đặc trưng của mô hình ngôn ngữ và sử dụng chúng trực tiếp như đầu vào cho mô hình ngôn ngữ. Bất chấp thành công của chúng, những MLLMs end-to-end này chỉ căn chỉnh bộ mã hóa thị giác cấp hình ảnh đã được huấn luyện trước với LLM. Ngược lại, chúng tôi tập trung vào việc tích hợp khả năng của bộ mã hóa thị giác cấp vùng thông qua điều chỉnh hướng dẫn thị giác tăng cường vị trí.

2.2 Hiểu biết Cấp vùng cho MLLMs
Trong Huấn luyện Trước Thị giác và Ngôn ngữ (VLP), để tăng cường sự hiểu biết tinh tế của mô hình về hình ảnh, thường là thực hành kết hợp giám sát cấp vùng rộng rãi trong quá trình huấn luyện trước. Về MLLMs, một số công trình gần đây đã thực hiện những nỗ lực sơ bộ bằng cách điều chỉnh tinh MLLM để hỗ trợ các hướng dẫn có liên quan đến vùng. Cụ thể, GPT4RoI tập hợp các đặc trưng cấp vùng từ bộ mã hóa thị giác cấp hình ảnh của MLLM, và tạo thành một đầu vào lai của các đặc trưng cấp hình ảnh, đặc trưng cấp vùng và hướng dẫn ngôn ngữ cho LLM. Shikra và ChatSpot trực tiếp kết hợp tọa độ không gian vào dữ liệu hướng dẫn và điều chỉnh tinh MLLM để hiểu chúng. Tuy nhiên, vì những mô hình này được xây dựng dựa trên bộ mã hóa thị giác cấp hình ảnh, việc điều chỉnh tinh trực tiếp để có được sự hiểu biết cấp vùng có thể không tối ưu và có thể xung đột với các khả năng hiện có. Trong bài báo này, chúng tôi mở rộng MLLM bằng cách tích hợp nó với một bộ mã hóa thị giác cấp vùng bổ sung để khai thác khả năng hiểu hình ảnh tinh tế của nó.

2.3 Dữ liệu Hướng dẫn Đa phương thức
Công trình hiện tại thu thập dữ liệu hướng dẫn đa phương thức theo hai cách. Hầu hết các công trình sử dụng các tập dữ liệu có chú thích có sẵn để xây dựng tập dữ liệu ở định dạng hướng dẫn. Thông thường, các mô tả nhiệm vụ được tạo ra thông qua thiết kế thủ công hoặc tạo tự động bởi LLMs cho mỗi tập dữ liệu để phục vụ như hướng dẫn. Sau đó chúng được kết hợp với đầu vào và đầu ra nhiệm vụ gốc để tạo một tập dữ liệu hướng dẫn. Trong khi các tập dữ liệu chuẩn hiện có cung cấp một nguồn dữ liệu đáng kể, chúng thường không đáp ứng được các yêu cầu của con người trong các ứng dụng thực tế. Do đó, một số công trình sử dụng pipeline self-instruct để thu thập dữ liệu hướng dẫn đa dạng bằng cách nhắc LLM với các ví dụ mẫu để tạo thêm ví dụ hướng dẫn. Ví dụ, LLaV A sử dụng các mô tả văn bản của hình ảnh bao gồm chú thích và hộp giới hạn để nhắc GPT-4 tạo ra các ví dụ hướng dẫn đa phương thức chất lượng cao và đa dạng. Chúng tôi lấy cảm hứng từ những công trình này và giới thiệu một phương pháp tạo dữ liệu được thiết kế đặc biệt để xây dựng dữ liệu hướng dẫn cấp vùng.

3 Phương pháp
3.1 Thiết kế Mô hình
Kiến trúc mô hình của chúng tôi, được mô tả trong Hình 2, bao gồm ba thành phần chính: một bộ mã hóa thị giác, một bộ mã hóa vùng và một mô hình ngôn ngữ lớn (LLM). Mô hình xử lý một hình ảnh đầu vào cùng với các hướng dẫn chứa các vùng nhúng và tạo ra các phản hồi tương ứng.

Lấy Hình 2 làm ví dụ, hướng dẫn có thể được thể hiện là "Mô tả mối quan hệ giữa và". Trong hướng dẫn này, "" và "" là các token đặc biệt phục vụ như các placeholder chỉ ra vị trí chèn cho các đặc trưng tương ứng. Đối với phần văn bản của hướng dẫn, chúng tôi trực tiếp lấy các phần nhúng từ XT của chúng.

Đối với phần vùng của hướng dẫn, mỗi vùng rk được biểu diễn như [x1, y1, x2, y2], với (x1, y1) và (x2, y2) đại diện cho tọa độ tương đối của góc trên-trái và góc dưới-phải. Chúng tôi sử dụng RegionCLIP như bộ mã hóa vùng để trích xuất các đặc trưng vùng bằng cách sử dụng RoI pooling với hình ảnh I và rk như đầu vào. Sau đó chúng tôi áp dụng một lớp chiếu tuyến tính để ánh xạ các đặc trưng vùng vào không gian biểu diễn của LLM. Chúng tôi ký hiệu tập hợp của tất cả các đặc trưng vùng cuối cùng là XR.

Chúng tôi sử dụng CLIP ViT-L/14 như bộ mã hóa hình ảnh để xử lý hình ảnh I và tạo ra các đặc trưng hình ảnh XI. LLM sau đó kết hợp các đặc trưng XI, XT và XR như đầu vào từ hình ảnh, hướng dẫn và vùng tương ứng, và tạo ra một phản hồi Y.

3.2 Huấn luyện
Được lấy cảm hứng từ Liu et al., mô hình của chúng tôi được huấn luyện theo cách hai giai đoạn. Trong giai đoạn đầu tiên, chúng tôi khởi tạo mô hình với LLaV A đã được huấn luyện trước, và đóng băng các tham số của bộ mã hóa hình ảnh, bộ mã hóa vùng và LLM. Chúng tôi chỉ huấn luyện lớp chiếu tuyến tính có trách nhiệm chuyển đổi các đặc trưng vùng. Mục đích của giai đoạn huấn luyện này là căn chỉnh các đặc trưng vùng với không gian nhúng của MLLM, mà không ảnh hưởng đến bản thân MLLM. Để đạt được điều này, chúng tôi thu thập một tập dữ liệu căn chỉnh cấp vùng quy mô lớn, với mỗi ví dụ bao gồm một hình ảnh, một hộp giới hạn và một mô tả văn bản ngắn gọn về đối tượng trong hộp giới hạn. Trong quá trình huấn luyện, mô hình nhận hình ảnh và hộp giới hạn như đầu vào và sau đó dự đoán văn bản tương ứng.

Sau giai đoạn huấn luyện đầu tiên, mô hình đã có khả năng hiểu các đặc trưng vùng và tận dụng khả năng hiểu cấp vùng của bộ mã hóa vùng. Để tiếp tục đạt được khả năng mạnh mẽ trong việc tuân theo các hướng dẫn chứa vùng, chúng tôi áp dụng giai đoạn huấn luyện thứ hai với dữ liệu hướng dẫn cấp vùng. Trong giai đoạn huấn luyện này, chúng tôi chỉ giữ đóng băng các tham số của bộ mã hóa hình ảnh và bộ mã hóa vùng, và điều chỉnh tinh phần còn lại của mô hình để thích ứng với các hướng dẫn cấp vùng. Chi tiết về việc xây dựng dữ liệu hướng dẫn cấp vùng sẽ được cung cấp trong phần sau.

3.3 Xây dựng Dữ liệu Hướng dẫn Cấp vùng
Như được thể hiện trong Hình 3, sơ đồ xây dựng dữ liệu của chúng tôi bao gồm ba chiến lược: (1) Chuyển đổi Tập dữ liệu, chuyển đổi các tập dữ liệu Trả lời Câu hỏi Thị giác (VQA) hiện có được giao với hộp giới hạn thành dạng hướng dẫn cấp vùng; (2) Tạo Dữ liệu Hướng dẫn Cụ thể cho Nhiệm vụ, tận dụng ChatGPT để tạo dữ liệu hướng dẫn cấp vùng cho một tập hợp các nhiệm vụ đa phương thức được định trước; và (3) Tạo Dữ liệu Hướng dẫn Tổng quát, làm phong phú hình ảnh với các mô tả chi tiết và chú thích grounding được tạo tự động, bổ sung bởi các ví dụ trong ngữ cảnh đa dạng, để tạo ra dữ liệu hướng dẫn cấp vùng tổng quát hơn. Như được chỉ ra ở dưới cùng của Hình 3, tính đa dạng của dữ liệu hướng dẫn cấp vùng được tạo tăng từ chiến lược đầu tiên đến chiến lược thứ ba, trong khi số lượng giảm do các ràng buộc tính toán và kinh tế. Nhìn chung, ba chiến lược hoạt động cộng tác, làm cho chúng tôi có khả năng thu được một lượng lớn dữ liệu hướng dẫn cấp vùng chất lượng cao và đa dạng.

3.3.1 Chuyển đổi Tập dữ liệu
Trong chiến lược này, chúng tôi chuyển đổi các tập dữ liệu VQA hiện có thành định dạng hướng dẫn cấp vùng bằng cách sử dụng các mẫu cụ thể cho tập dữ liệu. Chúng tôi sử dụng hai tập dữ liệu VQA cho việc chuyển đổi này, bao gồm GQA và VCR. Nó tạo ra 146k dữ liệu hướng dẫn cấp vùng một lượt. Các mẫu được sử dụng cho chuyển đổi có thể được tìm thấy trong tài liệu bổ sung.

3.3.2 Tạo Dữ liệu Hướng dẫn Cụ thể cho Nhiệm vụ
Mặc dù chúng tôi có thể thu được một lượng lớn dữ liệu thông qua chuyển đổi tập dữ liệu với chi phí thấp, tính đa dạng của dữ liệu này vẫn còn hạn chế. Để giải quyết vấn đề này, chúng tôi đề xuất tạo dữ liệu hướng dẫn cấp vùng bằng ChatGPT cho một tập hợp các nhiệm vụ đa phương thức được định trước. Cụ thể, chúng tôi chọn năm nhiệm vụ đại diện, bao gồm nhận dạng đối tượng nhỏ, phân biệt đối tượng cùng loại, lý luận dựa trên mối quan hệ đối tượng, lý luận dựa trên thuộc tính đối tượng và nhận dạng ký tự quang học (OCR). Chúng tôi thiết kế một lời nhắc cụ thể cho từng nhiệm vụ, bao gồm ba thành phần: (1) một thông điệp hệ thống phác thảo nhiệm vụ và yêu cầu định dạng dữ liệu, (2) các ví dụ trong ngữ cảnh của nhiệm vụ cụ thể, và (3) các mô tả văn bản của hình ảnh mà các hướng dẫn cấp vùng mới đang được tạo. Các lời nhắc toàn diện được hiển thị trong tài liệu bổ sung. Bằng cách điều chỉnh thông điệp hệ thống và các ví dụ trong ngữ cảnh tương ứng, chúng tôi có thể thu được cả dữ liệu một lượt và nhiều lượt. Tổng cộng, chúng tôi đạt được 20k dữ liệu một lượt và 66k dữ liệu nhiều lượt.

Để có được các mô tả văn bản của hình ảnh, chúng tôi dựa vào các chú thích chi tiết có sẵn trong các tập dữ liệu hiện có. Cụ thể, chúng tôi sử dụng các tập dữ liệu MS COCO, Visual Genome và COCO-Text. Và các chú thích bao gồm chú thích, thuộc tính đối tượng, hộp giới hạn, v.v.

3.3.3 Tạo Dữ liệu Hướng dẫn Tổng quát
Để tiếp tục cải thiện cả tính đa dạng và chất lượng của dữ liệu được tạo, chúng tôi mở rộng quy trình để tạo dữ liệu hướng dẫn tổng quát hơn. Đường viền của quy trình tạo dữ liệu nâng cao này được mô tả trong Hình 3.

Đầu tiên, chúng tôi nhận thấy rằng ChatGPT tạo ra kết quả tốt hơn khi được cung cấp các mô tả văn bản thông tin hơn về hình ảnh. Do đó, chúng tôi áp dụng phương pháp của LLaV A để khai thác ChatGPT cho việc tạo mô tả hình ảnh chi tiết. Những mô tả này thường dài hơn chú thích, phong phú hơn về thông tin và dễ hiểu hơn đối với ChatGPT so với các chú thích đơn giản hơn, chẳng hạn như thuộc tính đối tượng.

Thứ hai, chúng tôi sử dụng một mô hình grounding thị giác có sẵn để ground các đối tượng trong các mô tả chi tiết đến các vị trí tương ứng của chúng trong hình ảnh, tức là để xác định các hộp giới hạn của các đối tượng. Các hộp giới hạn quá nhỏ sẽ bị loại bỏ.

Thứ ba, vì các ví dụ trong ngữ cảnh có nguồn gốc từ các tập dữ liệu hiện có có xu hướng chỉ bao gồm một phổ hẹp các chủ đề, chúng tôi đã soạn thảo một số ví dụ trong ngữ cảnh thông qua các phiên não. Do đó, những ví dụ trong ngữ cảnh mới được tạo này khác biệt rõ rệt so với những ví dụ có nguồn gốc từ các tập dữ liệu có sẵn.

Cuối cùng, đối với mỗi hình ảnh, chúng tôi kết hợp chú thích, mô tả chi tiết và chú thích grounding để phục vụ như mô tả văn bản của nó, và chọn ngẫu nhiên ba thể hiện từ tập hợp các ví dụ trong ngữ cảnh mới được tạo. Sau đó chúng tôi đưa cả mô tả văn bản và các ví dụ trong ngữ cảnh được chọn vào ChatGPT để tạo dữ liệu hướng dẫn cấp vùng. Cấu trúc của lời nhắc tương tự như lời nhắc được sử dụng trong chiến lược tạo dữ liệu hướng dẫn cụ thể cho nhiệm vụ, nhưng chỉ sử dụng một lời nhắc. Thông qua chiến lược nâng cao này, chúng tôi đã thành công có được tổng cộng 22k mục dữ liệu chất lượng cao, đầy đủ với các loại câu hỏi đa dạng và phản hồi lý luận phức tạp.

4 Thí nghiệm
4.1 Baseline
Chúng tôi so sánh mô hình của chúng tôi với ba baseline mạnh:
• LLaV A, một MLLM được huấn luyện trên dữ liệu hướng dẫn đa phương thức cấp hình ảnh.
• Shikra, một MLLM được huấn luyện trên dữ liệu hội thoại tham khảo được tổng hợp bằng cách chuyển đổi các tập dữ liệu hiện có thành dạng hội thoại với GPT-4.
• GPT4RoI, một MLLM được huấn luyện trên dữ liệu hướng dẫn đa phương thức có nguồn gốc từ các tập dữ liệu hiện có bằng cách sử dụng mẫu và tập dữ liệu từ LLaV A được nâng cao với các hộp giới hạn được phát hiện tự động.

4.2 Chi tiết Thực hiện
Chúng tôi xây dựng mô hình của chúng tôi dựa trên khung LLaV A-7B. Đối với bộ mã hóa vùng, chúng tôi sử dụng một biến thể của mô hình RegionCLIP, sử dụng ResNet50x4 như backbone thị giác và được huấn luyện trước trên tập dữ liệu Conceptual Captions. Trong giai đoạn huấn luyện đầu tiên, chúng tôi sử dụng kích thước batch 128 và tốc độ học 2×10−3 cho một epoch. Một lịch trình tốc độ học cosine annealing được áp dụng, với tỷ lệ warmup 0.03. Trong giai đoạn huấn luyện thứ hai, chúng tôi giảm tốc độ học xuống 2×10−5 và huấn luyện trong ba epoch. Độ dài chuỗi tối đa của LLM được đặt là 2048. Tất cả huấn luyện được thực hiện trên tám GPU A100, mỗi GPU có 40GB bộ nhớ.

4.3 Đánh giá Khách quan
Trong phần này, chúng tôi kiểm tra định lượng khả năng nhận dạng đối tượng và lý luận đa phương thức của các mô hình.

4.3.1 Nhận dạng Đối tượng
Để đánh giá, chúng tôi sử dụng tập validation của MS COCO. Khi được trình bày một hình ảnh và một hộp giới hạn, một mô hình được yêu cầu xác định loại của đối tượng trong hộp giới hạn. Và độ chính xác được tận dụng như thước đo đánh giá. Đối với LLaV A, không xử lý đầu vào cụ thể cho vùng, chúng tôi thích ứng bằng cách sử dụng hình ảnh được cắt phù hợp với các hộp giới hạn được chỉ định.

Như được thể hiện trong Bảng 1, cả PVIT được đề xuất và baseline GPT4RoI đều vượt trội hơn LLaV A và Shikra đáng kể. Chúng tôi tin rằng hiệu suất vượt trội này là do thực tế là cả PVIT và GPT4RoI đều kết hợp các đặc trưng cấp vùng. Trong số các mô hình được so sánh, LLaV A tụt lại đáng kể. Kết quả này phù hợp với kỳ vọng của chúng tôi. Việc sử dụng hình ảnh được cắt tạo ra sự chuyển đổi trong phân bố dữ liệu, làm cho nó không tối ưu. Do đó, điều chỉnh hướng dẫn đa phương thức tinh tế là một hướng đi hứa hẹn để theo đuổi.

4.3.2 Lý luận Đa phương thức
Chúng tôi sử dụng tập validation của GQA để đánh giá. GQA là một tập dữ liệu trả lời câu hỏi thị giác (QA) được thiết kế đặc biệt để đánh giá lý luận thị giác và khả năng QA compositional. Đối với LLaV A, các đầu vào là câu hỏi và toàn bộ hình ảnh. Đối với ba MLLMs khác, thông tin về hộp giới hạn đi kèm với câu hỏi được cung cấp trong tập dữ liệu GQA cũng được bao gồm. Và độ chính xác được sử dụng như thước đo đánh giá.

Kết quả được trình bày trong Bảng 1. PVIT được đề xuất đạt hiệu suất cao nhất, thể hiện hiệu quả của nó. Tuy nhiên, trái ngược với hiệu suất của nó trên tập dữ liệu COCO, GPT4RoI không vượt trội hơn Shikra. Chúng tôi cho rằng kết quả này phát sinh vì các câu hỏi trong tập dữ liệu GQA có thể được trả lời mà không cần tham chiếu đến các hộp giới hạn. Do đó, GPT4RoI không có được lợi thế từ các đặc trưng cấp vùng, nhấn mạnh tính ưu việt của phương pháp của chúng tôi. LLaV A cũng tụt lại phía sau. Chúng tôi suy đoán rằng điều này có thể được quy cho kích thước hạn chế và tính đa dạng của tập dữ liệu hướng dẫn mà LLaV A được huấn luyện.

4.4 Đánh giá của Con người
Tương tự như LLMs, việc tự động đánh giá khả năng tuân theo hướng dẫn của MLLMs đặt ra một thách thức đáng kể. Do đó, chúng tôi chuyển sang các đánh giá của con người. Chúng tôi trình bày một tập dữ liệu đánh giá mới, FineEval, được thiết kế đặc biệt để đánh giá khả năng của MLLMs tuân theo các hướng dẫn đòi hỏi chi tiết không gian tinh tế. FineEval bao gồm 130 câu hỏi được tạo thủ công dựa trên 50 hình ảnh. Những câu hỏi này thăm dò khả năng của các mô hình thông qua bốn góc độ độc đáo: nhận dạng đối tượng, mô tả thuộc tính, lý luận và khác. Đáng chú ý, các câu hỏi trong FineEval nhấn mạnh thông tin không gian chi tiết, liên quan đến các đối tượng tương đối nhỏ khác nhau và giải quyết các mối quan hệ phức tạp giữa các đối tượng. Hai ví dụ và thống kê của FineEval được thể hiện trong Hình 4.

4.4.1 Kết quả Định lượng
Được lấy cảm hứng từ Ouyang et al., chúng tôi sử dụng so sánh từng cặp để đánh giá hiệu suất mô hình trên FineEval. Đối với bất kỳ hai mô hình nào, các đánh giá viên con người xếp hạng phản hồi của chúng, và tỷ lệ thắng trên toàn bộ tập dữ liệu sau đó được tính toán như thước đo đánh giá. Để giảm thiểu bias, chúng tôi ngẫu nhiên hóa thứ tự trình bày câu trả lời và tuyển dụng năm đánh giá viên cho việc đánh giá, có nghĩa là mỗi phản hồi sẽ nhận được năm kết quả xếp hạng cá nhân.

Kết quả cho PVIT được đề xuất so với LLaV A, Shikra và GPT4RoI được mô tả trong Hình 5. Từ những kết quả này, rõ ràng là PVIT luôn luôn vượt trội hơn ba baseline, và thường với một khoảng cách đáng kể. Ngoại lệ duy nhất là trong lĩnh vực nhận dạng đối tượng, nơi PVIT tụt lại phía sau Shikra một chút. Đào sâu hơn vào các kết quả, chúng tôi thấy rằng điều này là do khả năng đếm đối tượng của PVIT yếu hơn so với Shikra. Chúng tôi lý thuyết rằng điều này có thể được khắc phục bằng cách tích hợp thêm dữ liệu hướng dẫn cụ thể cho việc đếm vào tập huấn luyện của chúng tôi và để lại điều này như công việc tương lai.

4.4.2 Kết quả Định tính
Để cung cấp sự hiểu biết toàn diện về khả năng của mô hình, chúng tôi trình bày một số trường hợp trong Hình 6. Đầu tiên, điều quan trọng là nhấn mạnh rằng nhiều câu hỏi trong những trường hợp này sẽ thách thức việc đóng khung rõ ràng mà không có sự hỗ trợ của các hộp giới hạn. Ví dụ, trong các trường hợp 1, 2, 3 và 6, nhiều đối tượng thuộc về cùng một loại, làm cho việc chỉ định đối tượng mục tiêu chỉ bằng ngôn ngữ trở nên khó khăn mà không vô tình tiết lộ câu trả lời. Quan sát này phù hợp với lý do của chúng tôi để khám phá khả năng tuân theo hướng dẫn đa phương thức tinh tế của MLLMs. Đào sâu vào những trường hợp này, chúng tôi muốn nhấn mạnh bốn khả năng sau:

(1) Nhận dạng Đối tượng: Mô hình của chúng tôi xuất sắc trong việc xác định các đối tượng được phân định bởi các hộp giới hạn. Đầu tiên, như dự kiến, nó nhận dạng hiệu quả các đối tượng lớn hơn, điều này phù hợp với các kết quả được trình bày trong Bảng 1. Thứ hai, mô hình thể hiện năng lực trong việc nhận dạng các đối tượng nhỏ hơn, như được thể hiện bởi việc xác định chính xác "[REGION-1]" trong trường hợp 1 như một màn hình và "[REGION-2]" trong trường hợp 5 như một con cá. Hơn nữa, nó phân biệt giữa các hộp giới hạn khác nhau trong cùng một hình ảnh. Ví dụ, trong trường hợp 1, nó nhận biết chính xác rằng các hộp giới hạn tương ứng với các đối tượng khác nhau.

(2) Mô tả Thuộc tính: Ngoài việc nhận dạng đối tượng thuần túy, mô hình của chúng tôi mô tả hiệu quả các thuộc tính của chúng. Trong khi nó có thể chi tiết các thuộc tính có mặt trực quan — chẳng hạn như màu sắc và vị trí, ngay cả đối với các đối tượng nhỏ hơn — nó cũng có thể trình bày các đặc điểm vốn có của đối tượng nhưng không thể nhìn thấy trong hình ảnh. Ví dụ, phần lớn mô tả được tạo cho con sứa trong trường hợp 5 được ngoại suy từ kiến thức bên ngoài, không phải từ bản thân hình ảnh. Điều này cho thấy rằng việc sử dụng MLLMs cho các nhiệm vụ thị giác thuần túy thông thường có thể mang lại tiềm năng đáng kể, do MLLMs có thể cung cấp kiến thức rộng lớn không được bao gồm trong hình ảnh.

(3) Lý luận: Mô hình của chúng tôi thể hiện kỹ năng lý luận dựa trên hình ảnh và các hướng dẫn được cung cấp. Ví dụ, trong trường hợp 2, nó xác định cả người bơi và bàn tay của cô ấy, suy luận logic rằng bàn tay thuộc về người bơi. Trong trường hợp 3, nó nhận biết sự biến đổi màu sắc giữa các con cá và tận dụng kiến thức về độ tương phản thị giác để giải thích tại sao con cá đỏ nổi bật.

(4) Tạo Văn bản: Mặc dù được điều chỉnh tinh trên một tập dữ liệu đa phương thức, mô hình của chúng tôi duy trì khả năng tạo văn bản mạnh mẽ. Như được thể hiện trong các trường hợp, phần lớn phản hồi của nó đều mạch lạc và đúng ngữ pháp, với các trường hợp 1 và 4 phục vụ như các ví dụ đại diện. Tuy nhiên, điều quan trọng là nhấn mạnh rằng những trường hợp này không thể đánh giá đầy đủ khả năng tạo văn bản của phương pháp của chúng tôi. Đánh giá toàn diện được dành cho công việc tương lai.

4.5 Nghiên cứu Ablation
4.5.1 Ảnh hưởng của Biểu diễn Vùng
Để đánh giá hiệu quả của các đặc trưng cấp vùng thu được bằng cách sử dụng bộ mã hóa vùng, chúng tôi thử thay thế chúng bằng tọa độ văn bản. Cụ thể, mỗi vùng rk được biểu diễn như [x1, y1, x2, y2], với (x1, y1) và (x2, y2) đại diện cho tọa độ tương đối của góc trên-trái và góc dưới-phải. Tất cả tọa độ được chuẩn hóa về phạm vi [0,1] và được làm tròn đến ba chữ số thập phân. Những tọa độ này được kết hợp trực tiếp vào hướng dẫn như văn bản, ví dụ, "Mô tả đối tượng trong [0.121, 0.212, 0.301, 0.413]". Chúng tôi huấn luyện mô hình này theo cách hai giai đoạn tương tự. Trong giai đoạn đầu tiên, chỉ có các phần nhúng từ của LLM có thể huấn luyện được. Trong giai đoạn thứ hai, tất cả các tham số mô hình có thể huấn luyện được ngoại trừ những tham số trong bộ mã hóa hình ảnh. Lưu ý rằng không có bộ mã hóa vùng trong mô hình này.

Như rõ ràng từ các kết quả trong Bảng 2, việc sử dụng các đặc trưng cấp vùng cải thiện đáng kể hiệu suất. Điều này nhấn mạnh giá trị của việc kết hợp bộ mã hóa vùng.

4.5.2 Tác động của Mô tả Văn bản cho Hình ảnh
Cho tầm quan trọng quyết định của mô tả văn bản cho hình ảnh trong quá trình tạo dữ liệu, chúng tôi khám phá bốn loại mô tả văn bản độc đáo trong quá trình tạo dữ liệu hướng dẫn tổng quát: (1) mô tả chi tiết và chú thích grounding tự động; (2) chú thích và chú thích đối tượng thủ công; (3) chú thích, mô tả chi tiết và chú thích grounding tự động; (4) chú thích, chú thích đối tượng thủ công và mô tả chi tiết. Chúng tôi đánh giá thủ công dữ liệu được tạo từ 30 hình ảnh giống nhau sử dụng những mô tả văn bản này từ ba góc độ bao gồm tính đúng đắn của định dạng, tính đa dạng và sáng tạo của câu hỏi, và tính đúng đắn của câu trả lời. Các kết quả trong Bảng 3 làm nổi bật rằng loại thứ ba tạo ra dữ liệu tốt nhất và được tận dụng trong công việc của chúng tôi.

5 Kết luận
Để mở rộng khả năng tuân theo hướng dẫn thị giác tinh tế của các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLMs), chúng tôi giới thiệu một phương pháp mới: điều chỉnh hướng dẫn thị giác tăng cường vị trí (PVIT). Kỹ thuật này tăng cường MLLMs bằng cách sử dụng một bộ mã hóa vùng hiện có. Chúng tôi cũng trình bày một sơ đồ xây dựng dữ liệu hướng dẫn cấp vùng mới và một tập dữ liệu đánh giá thách thức được viết bởi con người, FineEval. Các thí nghiệm rộng rãi của chúng tôi nhấn mạnh hiệu quả của phương pháp của chúng tôi.

Việc đánh giá khả năng tuân theo hướng dẫn tinh tế của MLLMs tiếp tục là một thách thức. Trong tương lai, chúng tôi dự định mở rộng phạm vi của FineEval và kết hợp một phạm vi câu hỏi rộng hơn, với trọng tâm vào các cuộc hội thoại nhiều lượt. Trong khi dữ liệu hướng dẫn cấp vùng được tạo đã chứng minh có giá trị, vẫn còn các con đường để nâng cao thêm. Ví dụ, việc theo đuổi tạo ra dữ liệu phong cách hội thoại tự nhiên hơn vẫn là một nỗ lực xứng đáng.

Lời cảm ơn
Chúng tôi cảm ơn Siyu Wang, Qiusi Zou và Zhaolu Kang vì sự tham gia của họ trong công việc này.
