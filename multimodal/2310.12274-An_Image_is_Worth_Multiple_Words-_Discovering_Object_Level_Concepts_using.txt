# 2310.12274.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.12274.pdf
# File size: 47735819 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using
Multi-Concept Prompt Learning
Chen Jin1Ryutaro Tanno2Amrutha Saseendran1Tom Diethe1Philip Teare1
"a photo of   
* (teddybear)  "
"a photo of  
& (skateboar d)"Cones / CDCrop-based 
Break-A-SceneMask-based 
learnable pr ompts ar e represented as  
colour ed pseudo wor ds
"a photo of br own  
* (teddybear)  on a r olling  
& (skateboar d) at times squar e"
& ** (teddybear)   
panda* (teddybear)   
cat
rolling & (skateboard)   
surfing boardrolling & (skateboard)   
flying blanketMCPL RecomposingEditing concepts 
aligned cr oss-attentionMask-Fr ee learning and editing multiple new concepts with MCPL (ours) 
"a photo of  
* (teddybear)  and a  
& (skateboar d) at times squar e"
Figure 1. Language driven multi-concepts learning and applications. Custom Diffusion (CD) and Cones learn concepts from crops of
objects, while Break-A-Scene uses masks. In contrast, our method learns object-level concepts using image-sentence pairs, aligning the
cross-attention of each learnable prompt with a semantically meaningful region, and enabling mask-free local editing. The project page,
code, and data are available at https://astrazeneca.github.io/mcpl.github.io.
Abstract
Textual Inversion, a prompt learning method,
learns a singular text embedding for a new “word”
to represent image style and appearance, allowing
it to be integrated into natural language sentences
to generate novel synthesised images. However,
identifying multiple unknown object-level con-
cepts within one scene remains a complex chal-
lenge. While recent methods have resorted to
cropping or masking individual images to learn
multiple concepts, these techniques require image
annotations which can be scarce or unavailable.
To address this challenge, we introduce Multi-
Concept Prompt Learning (MCPL) , where multi-
ple unknown “words” are simultaneously learned
from a single sentence-image pair, without any
imagery annotations. To enhance the accuracy
of word-concept correlation and refine attention
1Centre for AI, DS&AI, AstraZeneca, UK2Google DeepMind,
UK. Correspondence to: Chen Jin <chen.jin@astrazeneca.com >,
Philip Teare <philip.teare@astrazeneca.com >.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).mask boundaries, we propose three regularisa-
tion techniques: Attention Masking ,Prompts Con-
trastive Loss , and Bind Adjective . Extensive quan-
titative comparisons with both real-world cate-
gories and biomedical images demonstrate that
our method can learn new semantically disentan-
gled concepts. Our approach emphasises learning
solely from textual embeddings, using less than
10% of the storage space compared to others.
1. Introduction
Language-driven vision concept discovery is a human-
machine interaction process in which the human describes
an image, leaving out multiple unfamiliar concepts. The
machine then learns to link each new concept with a corre-
sponding learnable prompt (pseudo words in Figure 1) from
the sentence-image pair. Such capacity would accelerate the
scientific knowledge discovery process either from exper-
imental observations or mining existing textbooks. It also
facilitates hypothesis generation through local image editing
without concrete knowledge of the new vision concept.
Recent research ((Gal et al., 2022; Ruiz et al., 2022)) shows
that the appearance and style of an image can be encapsu-
lated as a cohesive concept via a learned prompt (“word”)
1arXiv:2310.12274v2  [cs.CV]  25 May 2024

--- PAGE 2 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
optimised in the frozen embedding space of a pre-trained
text-to-image diffusion model. To bring the learning down
to multiple objects in a scene, Custom Diffusion (Kumari
et al., 2023) and Cones (Liu et al., 2023), make use of the
crops of objects, while Break-A-Scene (Avrahami et al.,
2023) uses masks, as shown in Figure 1. These approaches
optimise the integration performance of multiple learned
concepts via fine-tuning and storing the diffusion model
weights. Yet, these approaches are less suitable for discov-
ering unknown semantic concepts from historical data at
scale when: 1) annotations are absent (e.g. medical log); 2)
concepts are unknown (e.g. discovering new biomarker); 3)
minimizing storage per concept is crucial (see Table 1).
Table 1. Competing methods. Our method is the first to suggest a
solution for discovering new visual concepts using text descriptions
(ideally assisted by one adjective per concept). We focus on token
learning for minimal storage cost (every 1 ∼4 token in the table).
Other methods like BAS perform fine-tuning and storing diffusion
model weights for optimal integration performance.
Method Multi- Single Auxiliary Token Storage
concept image input only cost
Textual Inversion ✗ ✗ - ✓ <0.1MB
Dreambooth ✗ ✗ - ✗ 3.3GB
Custom Diffusion ✓ ✗ Crop ✗ 72MB
Cones ✓ ✓ Crop ✗ 1∼10MB
Break-A-Scene ✓ ✓ Mask ✗ 4.9GB
MCPL (Ours) ✓ ✓ Text ✓ <0.1MB
In this work, we explore learning object-level concepts us-
ing only natural language descriptions and only updating
and storing the textual embedding (token). We start with
a motivational study that confirms, without updating DM
parameters, while applying masking or cropping yields dis-
tinct embeddings, object-level learning and editing relying
solely on linguistic descriptions remains challenging. Moti-
vated by this finding, we introduce Multi-Concept Prompt
Learning (MCPL) Figure 3 (Top) for mask-free text-guided
learning of multiple prompts from one scene .
However, without further assumptions on the embedding
relationships, jointly learning multiple prompts is problem-
atic. The model may disregard the semantic associations
and instead prioritise optimising multiple embedding vec-
tors for optimal image-level reconstruction. To enhance the
accuracy of prompt-object level correlation, we propose the
following regularisation techniques: 1) To ensure a concen-
trated correlation between each prompt-concept pair, we
propose Attention Masking (AttnMask) , restricting prompt
learning to relevant regions defined by a cross-attention-
guided mask. 2) Recognising that multiple objects within a
scene are semantically distinct, we introduce Prompts Con-
trastive Loss (PromptCL) to facilitate the disentanglement
of prompt embeddings associated with multiple concepts.
3) To further enable accurate control of each learned em-
bedding, we bind each learnable prompt with a related de-scriptive adjective word, referred to as Bind adj. , that we
empirically observe has a strong regional correlation. The
middle and bottom row of Figure 3 illustrates the proposed
regularisation techniques. Our evaluation shows that our
framework improves precision in learning object-level con-
cepts and facilitates explainable hypothesis generation via
local editing. This is achieved without requiring explicit
image annotations, as exemplified in Figure 1 (ours).
In this work we implement our proposed method based on
Textual Inversion by (Gal et al., 2022), which only learns
and stores textual embeddings, and is complementary to
the more expensive generation-focused approaches that up-
date DM parameters (see Table 1). To our knowledge, our
technique is the first to learn multiple object-level concepts
without using a crop or mask. To evaluate this new task, we
generate and collect in-distribution natural images and out-
of-distribution biomedical images, each featuring 2 to 5 con-
cepts along with object-level masks. This results in a dataset
comprising 25 concepts and 1,000 sentence-image pairs. We
run around 3500 GPU hours experiments to compare with
competitive methods, with each run taking approximately
one hour. We assess concept disentanglement using t-SNE
and evaluate object embedding similarity against masked
ground truth in pre-trained BERT, CLIP, DINOv1, and DI-
NOv2 embedding spaces. Our results show that our method
can identify multiple concepts in an image and supports the
discovery of new concepts using only text descriptions.
2. Related Works
Language-driven vision concept discovery. In many sci-
entific fields, discovery often begins with visual observation
and then progresses by exploring the existing knowledge
base to pinpoint unfamiliar object-level concepts. These
concepts are subsequently defined using new terms, facil-
itating the development of hypotheses (Schickore, 2022).
The emergence of artificial intelligence, particularly large
pre-trained Vision-Language Models (VLM), has laid the
groundwork for automating this discovery process (Wang
et al., 2023). Language-driven local editing in VLMs shows
promise for helping scientists to generate hypotheses and
create designs (Hertz et al., 2022; Tumanyan et al., 2023;
Patashnik et al., 2023). However, a key challenge remains:
relying solely on linguistic descriptions, current methods
may not always map words to their corresponding object-
level concepts precisely.
Prompt learning for Diffusion Model. In text-guided
image synthesis, prompt learning links the appearance and
style of an unseen image to a learnable prompt, enabling
transfer to new images. This is achieved either by learning
and storing textual embeddings, as in Textual Inversion (Gal
et al., 2022), or by optimising the entire diffusion model
to reconstruct a given example image, as demonstrated in
DreamBooth (Ruiz et al., 2022).
2

--- PAGE 3 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Multiple concept learning and composing. Recent ad-
vancements focus on efficiently composing multiple con-
cepts learned separately from single object images or crops
(Custom Diffusion (Kumari et al., 2023), Cones (Liu et al.,
2023), SVDiff (Han et al., 2023), Perfusion (Tewel et al.,
2023). ELITE (Wei et al., 2023) and Break-A-Scene (Avra-
hami et al., 2023) adopt masks for improved object-level
concept learning, with Break-A-Scene specifically aiming
for multi-concept learning and integrating within single im-
ages, aligning closely with our objectives. Our approach
differs from Break-A-Scene in two key aspects: 1) we aim
toeliminate the need for labour-intensive image annota-
tions , and 2) we explore the limits of multi-concept learning
without updating or storing the DM parameters . Inspira-
tion Tree (IT) (Vinker et al., 2023) shares a similar spirit
as our work. MCPL excels in instruction following dis-
covery, adept at identifying concepts distinct in both visual
and linguistic aspects, whereas IT specializes in unveiling
abstract and subtle concepts independently of direct human
guidance.
3. Methods
In this section, we outline the preliminaries in Section 3.1
and present a motivational study in Section 3.2. These tests
validate the presence of object-level embeddings in the pre-
trained textual embedding space, highlighting the challenges
in learning multiple concepts without image annotations. In-
spired by these results, we introduce the Multi-Concept
Prompt Learning (MCPL) in Section 3.3. To address the
multi-object optimisation challenge in tandem with a single
image-level reconstruction goal, we propose several regular-
isation techniques in Section 3.4.
3.1. Preliminaries
Text-guided diffusion models are probabilistic generative
models that approximate the data distribution (specifically,
images in our work) by progressively denoising Gaussian
random noise, conditioned on text embeddings. Specifically,
we are interested in a denoising network ϵθbeing pre-trained
such that, given an initial Gaussian random noise map ϵ∼
N(0,I), conditioned to text embeddings v, generates an
image ˜x=ϵθ(ϵ, v)closely resembling a given example
image x. Here, v=cϕ(p), where cϕis a pre-trained text
encoder with parameters ϕandpis the text. During training,
ϕandθare jointly optimised to denoise a noised image
embedding ztto minimise the loss:
LDM=LDM(x,˜x) :=Ez,v,ϵ,t∥ϵ−ϵθ(zt, v)∥2.(1)
Here, zt:=αtz+σtϵis the noised version of the initial
image embedding zat time t∼Uniform (1, T),αt, σtare
noise scheduler terms, and z=E(x), where Eis the en-
coder of a pretrained autoencoder D(E(x))≈x, followingLatent Diffusion Models (LDMs) (Rombach et al., 2022) for
computational efficiency. During inference, the pre-trained
model iteratively eliminates noise from a new random noise
map to generate a new image.
Textual Inversion (Gal et al., 2022) is aimed at identifying
the text embedding v∗for a new prompt p∗with pre-trained
{ϵθ, cϕ}. Given a few (3-5) example images representing a
specific subject or concept, the method optimises v∗in the
frozen latent space of text encoder cϕ. The objective is to
generate images via the denoising network ϵθthat closely
resembles the example images (after decoding) when con-
ditioned on v∗. The optimisation is guided by the diffusion
model loss defined in equation 1, updating only v∗while
keeping cϕandϵθfrozen. During training, the generation is
conditioned on prompts combining randomly selected text
templates y(e.g., “A photo of”, “A sketch of” from CLIP
(Radford et al., 2021)) with the new prompt p∗, resulting in
phrases like “A photo of p∗” and “A sketch of p∗”.
Cross-attention layers play a pivotal role in directing the
text-guided diffusion process. Within the denoising network,
ϵθ, at each time step tthe textual embedding, v=cϕ(p), in-
teracts with the image embedding, zt, via the cross-attention
layer. Here, Q=fQ(zt),K=fK(v), and V=fV(v)are
acquired using learned linear layers fQ, fK, fV. As (Hertz
et al., 2022) highlighted, the per-prompt cross-attention
maps, M=Softmax (QKT/√
d), correlate to the similar-
ity between QandK. Therefore the average of the cross-
attention maps over all time steps reflects the crucial regions
corresponding to each prompt word, as depicted in Figure 3.
In this study, the per-prompt attention map is a key metric
for evaluating the prompt-concept correlation. Our results
will show that without adequate constraints, the attention
maps for newly learned prompts often lack consistent disen-
tanglement and precise prompt-concept correlation.
3.2. Motivational study
To understand the possibility of learning multiple concepts
within a frozen textual embedding space, we explored
whether Textual Inversion can discern semantically distinct
concepts from both masked and cropped images, each high-
lighting a single concept. Figure 2 (two examples on the
left) gives a highlight of our result, with a full version in Ap-
pendix A.9, confirms that: 1) multiple unique embeddings
can be derived from a single multi-concept image, albeit
with human intervention, and 2) despite having well-learned
individual concepts, synthesising them into a unified multi-
concept scene remains challenging. To address these issues,
we introduce the Multi-Concept Prompt Learning (MCPL)
framework. MCPL modifies Textual Inversion to enable
simultaneous learning of multiple prompts within the same
string.
3

--- PAGE 4 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
T.I.: separately learn + compose
"a photo of green *" "a photo of orange  @"
merge embeddings 
 "a photo of green * and orange  @"MCPL-one: jointly learn multi-concepts MCPL-diverse: learn per image multi-concepts
"a photo of green * {} orange  @"
on under in front of on under in front of
"a photo of green * {} orange  @"on under in front of
"a photo of green * {} orange  @"
on under in front of
"a photo of green * {} orange  @""a photo of  
* (watch face + watch band) "learn generate
"a photo of @ (watch face ) "
"a photo of ! (watch band) "
Figure 2. Motivational study and preliminary MCPL results. We use Textual Inversion (T.I.) to learn concepts from both masked
(left-first) or cropped (left-second) images; MCPL-one , learning both concepts jointly from the full image with a single string; and
MCPL-diverse accounting for per-image specific relationships.
3.3. Multi-Concept Prompt Learning (MCPL)
For example image(s) x, MCPL learn a list of embeddings
V= [v∗, . . . , v&]corresponds to multiple new prompts
P= [p∗, . . . , p&]within the descriptive sentence as shown
in Figure 3. The optimisation is guided by the image-level
LDM(equation 1), but now updating Vwhile keeping cϕ
andϵθfrozen. The MCPL algorithm is outlined in Algo-
rithm 1. Here each sentence Pcan be sub-grouped into a
set of noun words of target concepts [n∗, . . . , n&], a set of
adjective words [a∗, . . . , a&]describing each noun and the
rest preposition texts [t∗, . . . , t&]in the sentence (exclude
random neutral texts e.g. “a photo of”). Each sub-group
can be designated as either learnable or fixed, depending on
the training strategies that we will define in the subsequent
section. For evaluation, we request a human or a machine,
such as GPT-4, to describe each image using one adjective
and one noun for each target concept.
Algorithm 1 MCPL (generic form)
1:Input: example image(s) x, pre-trained {cθ, ϵθ}.
2:Output: a list of embeddings V= [v∗, . . . , v&]corresponds
to multiple new prompts P= [p∗, . . . , p&].
3: initialise [v∗, . . . , v&] = [cθ(p∗), . . . , c θ(p&)]
4:# optimising {v∗, . . . , v&}with LDM
5:forstep = 1toSdo
6: Encode example image(s) z=E(x)and randomly sample neutral texts y
to make string [y, p∗, . . . , p&]
7: Compute V†= [vy, v∗, . . . , v&] = [cθ(py), cθ(p∗), . . . , c θ(p&)]
8: fort=Tdown to 1do
9: V:= arg minVEz,V,ϵ,t∥ϵ−ϵθ(zt,V†)∥2
10: end for
11:end for
12:Return (P,V)
Training strategies and preliminary results. Recognis-
ing the complexity of learning multiple embeddings with
a single image-generation goal, we propose three training
strategies: 1) MCPL-all , a naive approach that learns em-
beddings for all prompts in the string (including adjectives,
prepositions and nouns. etc.); 2) MCPL-one , which simpli-
fies the objective by learning single prompt (nouns) per con-
cept; 3) MCPL-diverse , where different strings are learnedper image to observe variances among examples. The for-
mal definitions of each MCPL training strategy are given
in Appendix A.14. Preliminary evaluations of MCPL-one
andMCPL-diverse methods on the “ball” and “box” multi-
concept task are shown in Figure 2. Our findings indicate
thatMCPL-one enhance the joint learning of multiple con-
cepts within the same scene over separate learning. Mean-
while, MCPL-diverse goes further by facilitating the learn-
ing of intricate relationships between multiple concepts.
Limitations of plain MCPL. We aim to discover new
visual concepts using only linguistic descriptions and then
enable accurate local editing. It requires accurate object-
level prompt-concept correlation, to evaluate, we visualise
the average cross-attention maps for each prompt. As de-
picted in Figure 4 (top), plain MCPL inadequately capture
this correlation, especially for the target concept. These
results suggest that naively extending image-level prompt
learning techniques (Gal et al., 2022) to object-level multi-
concept learning poses optimisation challenges , notwith-
standing the problem reformulation efforts discussed in
Section 3.3. Specifically, optimising multiple object-level
prompts based on a single image-level objective proves to
be non-trivial. Given the image generation loss equation 1,
prompt embeddings may converge to trivial solutions that
prioritize image-level reconstruction at the expense of se-
mantic prompt-object correlations, thereby contradicting
our objectives. In the next section, we introduce multiple
regularisation terms to overcome this challenge.
  "a        photo       of       green        *          on      orange       @" Mask Image
MCPL
MCPL
+AttnMask
+PromptCL
+Bind adj.
Figure 4. Enhancing object-level prompt-concept correlation in
MCPL using the proposed regularisations: AttnMask ,PromptCL
andBind adj. . We compare MCPL-one applying all regularisation
terms against the MCPL-one , using a “Ball and Box” example. We
use the average cross-attention maps and the AttnMask to assess
the accuracy of correlation. Full ablation results in Appendix A.10
4

--- PAGE 5 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
  "a          brown          *             on            a          r olling          &            at            times       squar e"At one diffusion step
Attention Masking (AttnMask): average cr oss-attention maps (           ) acr oss all time steps
"a br own * (teddybear)  on
a rolling & (skateboar d) at
times squar e"frozen  
text encoder  frozen text-to-image
denoising networktext
embeddings
Prompts Contrastive Loss (Pr omptCL)text embeddings
K
Qpixel embeddings
denoising U-net cr oss-
attentioncr oss-
attention
Figure 3. Method overview. MCPL takes a sentence (top-left) and a sample image x0(top-right) as input, feeding them into a pre-trained
text-guided diffusion model comprising a text encoder cϕand a denoising network ϵθ. The string’s multiple prompts are encoded into a
sequence of embeddings which guide the network to generate images ˜x0close to the target one x0. MCPL focuses on learning multiple
learnable prompts (coloured texts), updating only the embeddings v∗andv&of the learnable prompts while keeping cϕandϵθfrozen. We
introduce Prompts Contrastive Loss (PromptCL) to help separate multiple concepts within learnable embeddings. We also apply Attention
Masking (AttnMask) , using masks based on the average cross-attention of prompts, to refine prompt learning on images. Optionally we
associate each learnable prompt with an adjective (e.g., “brown”) to improve control over each learned concept, referred to as Bind adj.
3.4. Regularising the multi-concept prompts learning
Encouraging focused prompt-concept correlation with
Attention Masking ( AttnMask ).Previous results show
plain MCPL may learn prompts focused on irrelevant areas.
To correct this, we apply masks to both generated and target
images over all the denoising steps (Figure 3, middle-right).
These masks, derived from the average cross-attention of
selected learnable prompts (Figure 3, bottom-row), con-
strain the image generation loss (equation 1) to focus on
pertinent areas, thereby improving prompt-concept correla-
tion. To calculate the mask, we compute for each selected
learnable prompt p∈ P the average attention map over
all time steps Mp= 1/TPT
t=1Mp
t. We then apply a
threshold to produce binary maps for each learnable prompt,
where B(Mp) :={1ifMp> k,0otherwise }andk= 0.5
throughout all our experiments. For multiple prompt learn-
ing objectives, the final mask Mis a union of multiple
binary masks of all learnable prompts M=S
p∈PB(Mp).
We compute the Hadamard product of Mwithxand˜xto de-
rive our masked loss LAttnMask
DM as equation 2. Our AttnMask
is inspired by (Hertz et al., 2022), but a reverse of the same
idea, where the AttnMask is applied over the pixel-level loss
equation 1 to constrain the prompt learning to only related
regions.
LAttnMask
DM =LDM(M ⊙ x,M ⊙ ˜x), (2)Encouraging semantically disentangled multi-concepts
with Prompts Contrastive Loss ( PromptCL ).AttnMask
focuses the learning of multiple prompts on the joint area of
target objects, eliminating the influence of irrelevant regions
like the background. However, it doesn’t inherently promote
separation between the embeddings of different target con-
cepts. Leveraging the mutual exclusivity of multiple objects
in an image, we introduce a contrastive loss in the latent
space where embeddings are optimised. Specifically, we
employ an InfoNCE loss (Oord et al., 2018), a standard in
contrastive and representation learning, to encourage disen-
tanglement between groups of embeddings corresponding
to distinct learnable concepts (Figure 3, middle-left).
Concretely, at each learning step as described in Algorithm
1, a mini-batch Bminor augmented (e.g. with random flip)
example images are sampled, with Nlearnable prompts for
each image, yields a set of BN embeddings, {vn
b}B
b=1,N
n=1.
Then, the similarity between every pair viandvjof the BN
samples is computed using cosine similarity:
sim(vi, vj) =vT
i.vj/||vi||||vj||. (3)
Given our goal is to differentiate the embeddings corre-
sponding to each prompt, we consider the embeddings of
the same concept as positive samples while the others as
negative. Next, the contrastive loss lη
i,j∈Bfor a positive pair
vη
iandvη
jof each concept η∈N(two augmented views of
5

--- PAGE 6 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
the example image) is shown in the equation 4, where τis a
temperature parameter following (Chen et al., 2020). The
contrastive loss is computed for BN views of each of the N
learnable concepts. The total contrastive loss LPromptCL is
shown in equation 5.
lη
i,j∈B=−log 
exp(sim(vη
i, vη
j))/τ
PN
η=1PB
j=1,j̸=iexp(sim(vη
i, vη
j)/τ)!
(4)
LPromptCL =1
N1
BNX
η=1BX
i=1lη
i,j∈B (5)
Enhance prompt-concept correlation by binding learn-
able prompt with the adjective word ( Bind adj. ).An ad-
ditional observation from the misaligned results in Figure 4
(top) reveals that adjective words often correlate strongly
with specific regions. This suggests that the pre-trained
model is already adept at recognising descriptive concepts
like colour or the term ”fluffy” (see full results in Figure 36).
To leverage this innate understanding, we propose to option-
ally associate one adjective word for each learnable prompt
as one positive group during the contrastive loss calcula-
tion. In particular, consider Madjective words associated
withNlearnable prompts. Then the positive pair vη
iand
vη
jof each concept is sampled from η∈MB instead of B.
Therefore the contrastive loss is now computed for BNM
views of each of the Nlearnable concepts. The resulting
total contrastive loss Ladj
PromptCL is detailed in equation 6.
We scale Ladj
PromptCL with a scaling term γand add with
LAttnMask
DM (equation 2), for them to have comparable magni-
tudes, resulting our final loss in equation 7.
Ladj
PromptCL =1
N1
MBNX
η=1MBX
i=1lη
i,j∈B (6)
L=LAttnMask
DM +γLadj
PromptCL , (7)
Assessing regularisation terms with cross-attention.
We assess our proposed regularisation terms on improving
the accuracy of semantic correlations between prompts and
concepts. We visualise the cross-attention and segmentation
masks, as shown in Figure 4. Our visual results suggest that
incorporating all of the proposed regularisation terms en-
hances concept disentanglement, whereas applying them in
isolation yields suboptimal outcomes (refer to full ablation
results in Appendix A.10). Moreover, the results demon-
strate that MCPL-one is a more effective learning strategy
than MCPL-all , highlighting the importance of excluding
irrelevant prompts to maintain a focused learning objective.4. Experiments
4.1. Experiment and Implementation Details
Multi-concept dataset. We generate in-distribution nat-
ural images and collect out-of-distribution biomedical im-
ages, each featuring 2 to 5 concepts along with object-level
masks. This results in a dataset comprising 25 concepts and
1,000 sentence-image pairs. For natural images, we generate
multi-concept images using prior local editing (Patashnik
et al., 2023) and multi-concept composing method (Avra-
hami et al., 2023), both support generating or predicting ob-
ject masks. We generate each image using simple prompts,
comprising one adjective and one noun for every relevant
concept. For biomedical images, we request a human or a
machine, such as GPT-4, to similarly describe each image
using one adjective and one noun for each pertinent concept.
For more details, a full list of prompts used and examples,
please read Appendix A.16. We release the dataset here.
Competing Methods. We compare three baseline meth-
ods: 1) Textual Inversion (TI-m) applied to each masked
object serving as our best estimate for the unknown disen-
tangled “ground truth” object-embedding. 2) Break-A-Scene
(BAS) , the state-of-the-art (SoTA) mask-based multi-concept
learning method, serves as a performance upper bound,
though it’s not directly comparable. 3) MCPL-all as our
naive adaptation of the Textual Inversion method to achieve
the multi-concepts learning goal. For our method, we com-
pare two training strategies of our method: MCPL-all and
MCPL-one. For each, we examine three variations to scruti-
nise the impact of the regularisation terms discussed in Sec-
tion 3.4. All MCPL learnings are performed on unmasked
images without updating DM parameters. To evaluate the
robustness and concept disentanglement capability of each
learning method, we repeatedly learn each multi-concept
pair around 10 times, randomly sampling four images each
time. We generated a total of 560 masked objects for each
MCPL variant and 320 for the BAS baseline.
However, it’s important to note that preparing the BAS as the
object-level embedding upper bound is costly , as it requires
an additional pre-trained segmentation model, and occa-
sionally human-in-the-loop, to obtain masks during both
the learning and evaluation phases (see Appendix A.17 for
details). In contrast, our method utilises its own AttnMask
togenerate masked concepts during image generation with
no extra cost .
Implementation details. We use the same prompts col-
lected during the data preparation, substituting nouns as
learnable prompts, which are merged with CLIP prompts
from Section 3.1. This process creates phrases such as “A
photo of brown * on a rolling @ at times square” . Please
find full implementation details in Appendix A.15.
6

--- PAGE 7 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
4.2. Quantitative Evaluations
Investigate the concepts disentanglement with t-SNE.
We aim to learn semantically distinct classes hence our ex-
perimental design, therefore we anticipate a clustering effect
where objects of the same class naturally cluster together.
To assess clustering, we begin by calculating and visualis-
ing the t-SNE projection of the learned features (Van der
Maaten & Hinton, 2008). The results, depicted in Figure 5,
encompass both natural and biomedical datasets. They illus-
trate that our MCPL-one combined with all regularisation
terms can effectively distinguish all learned concepts com-
pared to all baselines. The learned embeddings from both
the mask-based “ground truth” and BAS are less distinct
than ours, due to their absence of a disentanglement goal
like MCPL’s PromptCL loss.
In-distribution natural images Out-of-distribution biomedical imagesMask-based SoT A:  
Break-A-SceneMask-based SoT A:  
Break-A-Scene'Ground T ruth' :  
Textural Inversion (T .I.) + mask
Ours : MCPL-one  
+ AttnMask + PromptCL  + Bind adj.MCPL-all   
(modify T .I. to learn all prompts)
MCPL-all   
(modify T .I. to learn all prompts)Ours : MCPL-one  
+ AttnMask + PromptCL  + Bind adj.
'Ground T ruth' :  
Textural Inversion (T .I.) + mask
Figure 5. The t-SNE projection of the learned embeddings . Our
method can effectively distinguish all learned concepts (about 10
embeddings each concept) compared to Textual Inversion (MCPL-
all), the SoTA mask-based learning method, Break-A-Scene, and
the masked “ground truth” (see full results in Appendix A.7).
Embedding similarity relative to the “ground truth”.
To assess the preservation of per-concept semantic and tex-
tual details, we calculate both prompt and image fidelity.
This evaluation follows prior research by (Gal et al., 2022)
and (Ruiz et al., 2022), but differently, we perform the calcu-
lations at the object level. Prompt fidelity is determined by
measuring the average pairwise cosine similarity between
the embeddings learned from the estimated “ground truth”
and the generated masked images, in the pre-trained embed-
ding space of BERT (Devlin et al., 2018). Image fidelity
refers to the average pairwise cosine similarity between
masked ”ground truth” images and generated masked ob-
jects within four pre-trained embedding spaces of CLIP
(Radford et al., 2021), DINOv1 (Caron et al., 2021) and
DINOv2 (Oquab et al., 2023), all based on the ViT-S.
The results in Figure 6 show our method combined with all
the proposed regularisation terms can improve both prompt
and image fidelity consistently. Our fully regularised version
(MCPL-one+CL+Mask ) achieved competitive performance
compared to the SoTA mask-based method (BAS) on thenatural dataset. In the OOD medical dataset, BAS outper-
formed our method significantly in the DINOv1 embedding
space, although the performance was comparable in other
spaces. The discrepancy stems from our method’s attention
masks yielding less accurate object masks compared to BAS
whose masks are obtained through a specialised human-
in-the-loop segmentation protocol , as detailed in Appendix
A.17. This difference is depicted in Figures 43, 44 and 7.
BERT DINOv1 CLIP DINOv20.10.20.30.40.50.60.7cosine similarities - BERT, DINOv1Object-level fidelity (natural images).
MCPL-all
MCPL-one
MCPL-all+CL
MCPL-all+CL+Mask
MCPL-one+CL
MCPL-one+CL+Mask
BAS (mask-based SoTA)
0.60.70.80.91.0
cosine similarities - CLIP, DINOv2
BERT DINOv1 CLIP DINOv20.10.20.30.40.50.6cosine similarities - BERT, DINOv1Object-level fidelity (medical images).
MCPL-all
MCPL-one
MCPL-all+CL
MCPL-all+CL+Mask
MCPL-one+CL
MCPL-one+CL+Mask
BAS (mask-based SoTA)
0.600.650.700.750.800.850.900.951.00
cosine similarities - CLIP, DINOv2
Figure 6. Embedding similarity in learned object-level concepts
compared to masked “ground truth” (two concepts per image).
We compare Textual Inversion (MCPL-all) and the SoTA mask-
based learning method, BAS, against our regularised versions. The
analysis is conducted in both pre-trained text (BERT) and image
encoder spaces (CLIP, DINOv1, and DINOv2), with each bar
representing an average of 40k pairwise cosine similarities.
Learning more than two concepts. To validate our
method’s robustness, we expanded the evaluation to learning
tasks with more than two concepts per image, specifically
natural images containing 3, 4, and 5 concepts. We group the
learned embeddings by the number of concepts per image to
evaluate their impact on learning efficiency. The results in
Figure 8 reveal that: 1) learning efficiency diminishes with
the increase in the number of concepts per image, a trend
also evident in the mask-based BAS approach; 2) although
our fully regularised version continues to outperform under-
regularised versions, the performance gap to BAS widens,
highlighting the heightened challenge of mask-free multi-
concept learning in more complex scenes. We also collect
real image datasets for the same evaluation and obtain con-
sistent conclusions, with full details in Appendix A.2.
Object-level evaluations. As our main goal is to accu-
rately learn object-level embeddings, we perform an analy-
7

--- PAGE 8 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
SoT A: 
MCPL-all
Ours : 
MCPL-one  
+AttnMask  
+PromptCL  
+Bind adj.
masked @ mask masked  * attention - @  attention -  * 
 masked @ mask masked  * attention - @  attention -  * 
 masked @ mask masked  * attention - @  attention -  * 
SoT A: 
MCPL-all
Ours : 
MCPL-one  
+AttnMask  
+PromptCL  
+Bind adj.masked @ mask masked  * attention - @  attention -  * masked @ mask masked  * attention - @  attention -  * masked @ mask masked  * attention - @  attention -  * "a gr een @
(cactus)  and a
red * (ball)  in
the desert""a br own @
(basket)  with
yellow *
(bananas) ""a fluffy @
(hamster) 
eating  red *
(watermelon)
on the beach"
"round @
(cavity)
encir cled by
circle *
(myocardium) ""brighter  @
(tumour)
encir cled by
grey *
(edema) ""round  @
(transition)
encir cled by
circle *
(peripheral) "image
Figure 7. Visualisation of generated concepts with the MCPL-all (top) and our fully regularised method (bottom). Masks are
derived from cross-attentions. Full ablation results are presented in the Appendix A.10
2 3 4 5
Number of concepts per image0.300.350.400.450.500.550.600.65DINOv1 cosine similarities
Object-level fidelity versus number of concepts per image
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS
Figure 8. Evaluate the learning as the number of concepts per
image increases. Here each data point represents an average of
20∼40kpairwise cosine similarities measured by DINOv1.
sis of embedding similarity at the object level as highlighted
in Figure 9. Notably, our method sometimes surpasses the
mask-based method, BAS, at the object level. This un-
derscores the potential of our mask-free, language-driven
approach to learning multiple concepts from a single image.
User studies. Lastly, we collect 41 anonymized user stud-
ies to evaluate prompt-following during image editing. Each
study encompassed three types of tasks: text alignment,
prompt following, and semantic correspondence, totalling
30 quizzes. The user studies further justify our method, with
full results in Appendix A.1.
tableclothbeachbananasjacket
watermelonpotspoon basketroad
skateboardtaxi
hamsterteddybearmugcheese0.10.20.30.40.50.60.70.8cosine similarities - DINOv1Object-level fidelity (natural mix 3,4,5 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+MaskBAS (mask-based SoTA)
Figure 9. Object-level embedding similarities with DINOv1
(mix of 3 to 5 concepts per image). Each bar representing an
average of 160k pairwise cosine similarities. A comprehensive
object-level comparison is available in the Appendix (Section A.8).4.3. Qualitative Evaluation
Visualise concepts disentanglement and learning. To
evaluate disentanglement and prompt-to-concept correla-
tion, we visualise attention and attention masks for learnable
prompts. Figures 7 and 10 display results for both natural
and medical images. The visual outcomes align with earlier
quantitative findings, affirming the effectiveness of our pro-
posed MCPL method and regularisation terms. In Figure 12
our method demonstrates capability in learning concepts
having similar colour, with cross-attention derived masks
outperforming pre-trained segmentation models in terms of
semantic accuracy.
Generated
imageMCPL-all MCPL-one+R2 MCPL-one+R3 MCPL-one MCPL-all+R2 MCPL-all+R3
"a @ {basket}  with yellow ~ {bananas}  beside a white ! {mug} "
"a brown @ {bear}  on a rolling * {skateboard } wearing a blue ~ {jacket}  beside a red & {taxi}  over grey ! {road}  at times square"
Figure 10. Visualisation of generated concepts with the “SoTA”
and our method (3 or 5 concepts). Masks are derived from cross-
attentions. Full ablation results are presented in the Appendix A.10
Image editing over disentangled concepts. Hypothesis
generation in scientific fields can be accelerated by inte-
grating new concepts into existing observations, a process
that benefits from local image editing. We demonstrate our
method enables mask-free object-level learning, editing and
quantification (Figure 11 top-row), with flexibility to handle
per-image specified string to learn the different concepts
within each image . Furthermore, our method can also learn
unknown concepts from challenging out-of-distribution im-
ages (Figure 11 bottom rows), opening an avenue of knowl-
edge mining from pairs of textbook figures and captions . It
is worth noting that, compared to BAS, our method does not
rely on a separate segmentation model and mask to achieve
local editing. Our method optimises the disentanglement
8

--- PAGE 9 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Generated image Cross-attention mask
 Refer ence image(s) Per-image differ ent
multiple concepts Generating or editing each disentangled concept OOD concept
discovery
unicorn ~ top *
`
@ (cat)  tiger @ (cat)  lion black !  brown & black !  unicorn ~ "a fluf fy @ (cat) with a per-image-prompt  is lying on a beach chair"
"a photo of ! with round *  and thin &  on the side circled by yellow lines "
 thin & !
 round *
 yellow lines
Figure 11. MCPL learning and editing capabilities. Top-row: discovering per-image different concepts with per-image specified string
with input image from P2P (Hertz et al., 2022). Bottom-row: learning to disentangle multiple unseen concepts from cardiac MRI images
with input images from LGE-CMR (Karim et al., 2016). More examples in Figure 28.
Maskformer  
(S-L-coco)Maskformer  
(S-L-ade)
Segment  
AnythingInput  
image
“a patterned @(butterfly) resting on an orange *(flower)”
Figure 12. Learning concepts with a similar appearance.
MCPL’s fully regularised version (right) demonstrates compet-
itive cross-attention masks against segmentation models. Full
stress tests are presented in the Appendix A.4.
of multiple concepts, leading to accurate word-concept cor-
relation in the cross-attention hence supporting mask-free
local editing method (e.g. P2P (Hertz et al., 2022)) directly.
Learning abstract concepts. In this Appendix A.3., we
compare MCPL with Inspiration Tree (IT) (Vinker et al.,
2023), showcases MCPL’s performance in both concepts
exploration Figure 20 and combination Figure 22.
4.4. Ablation studies.
We also conduct a set of ablation studies to assess various
components and capabilities of our method, with details in
the Appendix. They are: 1) The MCPL-diverse training
strategy has demonstrated potential in learning tasks with
varying concepts per image. Therefore, we performed fur-
ther experiments to assess its effectiveness, with findings
detailed in Section A.11 confirming its efficacy. 2) Ourlanguage-driven approach benefits from the proposed adjec-
tive binding mechanism. To better understand its role, we
conducted an ablation study detailed in Section A.12, which
confirmed its significance. 3) For a comprehensive evalua-
tion, we visually compare our tuning-free method, which
is not specifically designed for composing complex scenes
when prompt interactions change, with SoTA composing-
focused methods in complex scene tasks. This comparison
is detailed in Section A.13 with promising results.
5. Limitations and Conclusions
MCPL enhances prompt-region semantic correlation
through natural language instructions but may encounter
difficulties in scenarios such as: 1) When concepts are
linguistically indistinct, for example, multiple identical or
highly similar instances that natural language struggles to
differentiate. 2) In highly complex scenes containing many
concepts with limited example images available, a challenge
recognized by (Liu et al., 2023) and (Avrahami et al., 2023).
In conclusion, we introduced MCPL to tackle the novel
challenge of mask-free learning of multiple concepts using
images and natural language descriptions. This approach is
expected to assist in the discovery of new concepts through
natural language-driven human-machine interaction, poten-
tially advancing task hypothesis generation and local image
editing without requiring explicit knowledge of the new
vision concept.
9

--- PAGE 10 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Kopp-
Schneider, A., Landman, B. A., Litjens, G., Menze, B.,
Ronneberger, O., Summers, R. M., et al. The medical
segmentation decathlon. Nature communications , 13(1):
4128, 2022.
Avrahami, O., Aberman, K., Fried, O., Cohen-Or, D.,
and Lischinski, D. Break-a-scene: Extracting mul-
tiple concepts from a single image. arXiv preprint
arXiv:2305.16311 , 2023.
Caron, M., Touvron, H., Misra, I., J ´egou, H., Mairal, J.,
Bojanowski, P., and Joulin, A. Emerging properties in
self-supervised vision transformers, 2021.
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
simple framework for contrastive learning of visual rep-
resentations. In International conference on machine
learning , pp. 1597–1607. PMLR, 2020.
Cheng, B., Schwing, A., and Kirillov, A. Per-pixel clas-
sification is not all you need for semantic segmentation.
Advances in Neural Information Processing Systems , 34:
17864–17875, 2021.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 ,
2018.
Gal, R., Alaluf, Y ., Atzmon, Y ., Patashnik, O., Bermano,
A. H., Chechik, G., and Cohen-Or, D. An image is worth
one word: Personalizing text-to-image generation using
textual inversion, 2022. URL https://arxiv.org/
abs/2208.01618 .
Han, L., Li, Y ., Zhang, H., Milanfar, P., Metaxas, D., and
Yang, F. Svdiff: Compact parameter space for diffusion
fine-tuning. arXiv preprint arXiv:2303.11305 , 2023.
Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch,
Y ., and Cohen-Or, D. Prompt-to-prompt image editing
with cross attention control. 2022.
Johnson, A. E., Pollard, T. J., Berkowitz, S. J., Greenbaum,
N. R., Lungren, M. P., Deng, C.-y., Mark, R. G., and
Horng, S. Mimic-cxr, a de-identified publicly available
database of chest radiographs with free-text reports. Sci-
entific data , 6(1):317, 2019.Karim, R., Bhagirath, P., Claus, P., Housden, R. J., Chen,
Z., Karimaghaloo, Z., Sohn, H.-M., Rodr ´ıguez, L. L.,
Vera, S., Alb `a, X., et al. Evaluation of state-of-the-art
segmentation algorithms for left ventricle infarct from
late gadolinium enhancement mr images. Medical image
analysis , 30:95–107, 2016.
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,
Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C.,
Lo, W.-Y ., et al. Segment anything. arXiv preprint
arXiv:2304.02643 , 2023.
Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and
Zhu, J.-Y . Multi-concept customization of text-to-image
diffusion. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pp. 1931–
1941, 2023.
Lalande, A., Chen, Z., Decourselle, T., Qayyum, A., Pom-
mier, T., Lorgis, L., de La Rosa, E., Cochet, A., Cottin,
Y ., Ginhac, D., et al. Emidec: a database usable for
the automatic evaluation of myocardial infarction from
delayed-enhancement cardiac mri. Data , 5(4):89, 2020.
Lin, T.-Y ., Maire, M., Belongie, S., Hays, J., Perona, P., Ra-
manan, D., Doll ´ar, P., and Zitnick, C. L. Microsoft coco:
Common objects in context. In Computer Vision–ECCV
2014: 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part V 13 , pp. 740–
755. Springer, 2014.
Liu, Z., Feng, R., Zhu, K., Zhang, Y ., Zheng, K., Liu, Y .,
Zhao, D., Zhou, J., and Cao, Y . Cones: Concept neurons
in diffusion models for customized generation. arXiv
preprint arXiv:2303.05125 , 2023.
Ma, J. and Wang, B. Segment anything in medical images.
arXiv preprint arXiv:2304.12306 , 2023.
Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J.,
Farahani, K., Kirby, J., Burren, Y ., Porz, N., Slotboom,
J., Wiest, R., et al. The multimodal brain tumor image
segmentation benchmark (brats). IEEE transactions on
medical imaging , 34(10):1993–2024, 2014.
Oord, A. v. d., Li, Y ., and Vinyals, O. Representation learn-
ing with contrastive predictive coding. arXiv preprint
arXiv:1807.03748 , 2018.
Oquab, M., Darcet, T., Moutakanni, T., V o, H., Szafraniec,
M., Khalidov, V ., Fernandez, P., Haziza, D., Massa, F., El-
Nouby, A., et al. Dinov2: Learning robust visual features
without supervision. arXiv preprint arXiv:2304.07193 ,
2023.
Patashnik, O., Garibi, D., Azuri, I., Averbuch-Elor, H., and
Cohen-Or, D. Localizing object-level shape variations
with text-to-image diffusion models, 2023.
10

--- PAGE 11 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J., Krueger, G., and Sutskever, I. Learning transferable
visual models from natural language supervision, 2021.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pp.
10684–10695, 2022.
Ruiz, N., Li, Y ., Jampani, V ., Pritch, Y ., Rubinstein, M.,
and Aberman, K. Dreambooth: Fine tuning text-to-image
diffusion models for subject-driven generation. 2022.
Schickore, J. Scientific Discovery. In Zalta, E. N. and Nodel-
man, U. (eds.), The Stanford Encyclopedia of Philosophy .
Metaphysics Research Lab, Stanford University, Winter
2022 edition, 2022.
Tewel, Y ., Gal, R., Chechik, G., and Atzmon, Y . Key-locked
rank one editing for text-to-image personalization. In
ACM SIGGRAPH 2023 Conference Proceedings , pp. 1–
11, 2023.
Tumanyan, N., Geyer, M., Bagon, S., and Dekel, T. Plug-
and-play diffusion features for text-driven image-to-
image translation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pp.
1921–1930, 2023.
Van der Maaten, L. and Hinton, G. Visualizing data using
t-sne. Journal of machine learning research , 9(11), 2008.
Vinker, Y ., V oynov, A., Cohen-Or, D., and Shamir, A. Con-
cept decomposition for visual exploration and inspiration.
ACM Transactions on Graphics (TOG) , 42(6):1–13, 2023.
Wang, H., Fu, T., Du, Y ., Gao, W., Huang, K., Liu, Z.,
Chandak, P., Liu, S., Van Katwyk, P., Deac, A., et al.
Scientific discovery in the age of artificial intelligence.
Nature , 620(7972):47–60, 2023.
Wei, Y ., Zhang, Y ., Ji, Z., Bai, J., Zhang, L., and Zuo, W.
Elite: Encoding visual concepts into textual embeddings
for customized text-to-image generation. arXiv preprint
arXiv:2302.13848 , 2023.
Wu, Z., Lischinski, D., and Shechtman, E. Stylespace analy-
sis: Disentangled controls for stylegan image generation,
2020.
Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and
Torralba, A. Scene parsing through ade20k dataset. In
Proceedings of the IEEE conference on computer vision
and pattern recognition , pp. 633–641, 2017.
11

--- PAGE 12 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A. Appendix
A.1. User study for instruction following with prompt guided image editing.
•We conduct anonymized user studies to evaluate text alignment , following Cones (Liu et al., 2023), comparing edited
images to baselines. Participants are instructed to select the image that most accurately represents the given text, with the
sentence’s learnable pseudo word replaced by specific editing terms, see the left example in Figure 13.
•To assess the prompt following , we present the user the original image, followed by the edited versions with prompt
instructions (e.g. replacing ‘cat’ with ‘panda’). We ask the users to select “the image that best follows the instructions
while minimizing changes to other parts”, see the middle example in Figure 13.
•Semantic correspondence is evaluated by displaying the target prompt alongside both the masked ground truth and its
edited counterpart, obscured by corresponding prompt cross-attention. Users are asked to identify the masked image that
most closely resembles the ’ground truth’. This evaluation captures the accuracy of prompt-region correlation and the
quality of generated objects, aligning with our research goals.
Method MCPL-all MCPL-one MCPL-all MCPL-all MCPL-one MCPL-one
+PromptCL +PromptCL +PromptCL +PromptCL
+AttnMask +AttnMask
Text Alignment (%) 7.18 3.33 7.95 25.64 15.38 40.51
Prompt Following (%) 3.08 5.64 2.31 11.54 21.28 56.15
Semantic Correspondence (natural images) (%) 3.59 7.18 3.08 5.13 26.67 54.36
Semantic Correspondence (medical images) (%) 6.15 5.13 6.67 3.08 13.33 65.64
Table 2. User Study on Language-Driven Image Editing. We collected 41 user studies, each study consists of three types of tasks
(text alignment, prompt following and semantic correspondence), each 10 and a total of 30 quizzes. For the semantic, 5 quizzes are
from natural images and 5 from medical images. The value represents the percentage of users who think the image generated by the
corresponding method is the best, following Cones (Liu et al., 2023). All images are generated and edited with models learnt in our
quantitative evaluation as presented in Section 4.2.
• Table 2 demonstrates that our fully regularized version consistently outperforms all baseline methods across all metrics.
•The table further indicates that our method excels in tasks with tighter regional constraints, showing improved performance
moving from broader image-level evaluations (text alignment) to more localized assessments (semantic correspondence).
This underscores the effectiveness of our proposed method and regularization approach in enhancing text-semantic
accuracy.
• The visual comparisons in Figure 14 reinforce these quantitative findings.
Figure 13. Example of anonymized user studies of text alignment (left), prompt following (middle) and semantic correspondence (right).
12

--- PAGE 13 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Visual examples demonstrate the replacement of ’bananas’ with ’cat,’
’hedgehog,’ ’teddy,’ and ’snake,’ sequentially from left to right.
Visual examples demonstrate the replacement of ’hamster’ with ’fox,’
’lamma,’ ’koala,’ and ’penguin,’ sequentially from left to right.
Figure 14. Visualisation of text-guided image editing. We compare all baseline methods across each row (from top to bottom): 1) MCPL-
all, 2) MCPL-one, 3) MCPL-all+ PromptCL +Bind adj. , 4) MCPL-all+ AttnMask +PromptCL +Bind adj. , 5) MCPL-one+ PromptCL +Bind
adj., 6) MCPL-one+ AttnMask +PromptCL +Bind adj. .The results corroborated the quantitative findings from the user study : the fully
regularized MCPL enhances the accuracy of prompt instruction editing by improving prompt-region correlation. Notably, there were
instances of failed edits, for example, the model’s inability to substitute ‘hamster’ with ‘llama’ in the third column of the right figure. This
failure stems from the model’s unfamiliarity with the word ‘llama’, not from an incorrect correlation of regions.
13

--- PAGE 14 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.2. Evaluating real image dataset
Real image collection and segmentation
•To evaluate our method with real images, we sourced a collection from Unsplash, following the precedent set by previous
works like Custom Diffusion (Kumari et al., 2023) and Cones (Liu et al., 2023). We utilized both automated tools
(MaskFormer (Cheng et al., 2021) and Segment Anything (Ma & Wang, 2023)) and manual segmentation for cases where
automated methods were inadequate (refer to Figure 16 for examples of challenging segmentation), resulting in a dataset
of 137 unmasked and masked images across 50 classes.
•This dataset of real images, combined with our previously compiled natural and real medical multi-concept datasets,
totals 1447 images, encompassing both masked objects and concepts. We plan to release this comprehensive dataset here,
addressing the need for more extensive evaluation datasets in multi-concept learning.
Figure 15. Visual examples of real images containing various number of objects.
Segment Anything
 Maskformer (Swin-Large-ade)
 Maskformer (Swin-Large-coco)
 Input image
Figure 16. Challenge segmentation examples. To streamline segmentation tasks, we utilize the MaskFormer model (Cheng et al., 2021)
trained on the COCO (Lin et al., 2014) and ADE20K (Zhou et al., 2017) datasets, along with the Segment Anything (SAM) model (Ma &
Wang, 2023). We observe that MaskFormer occasionally under-segments, whereas SAM is prone to over-segmentation. Consequently,
manual adjustments are necessary to ensure the datasets are accurately prepared for evaluation.
Quantitative and qualitative evaluation. We executed the complete evaluation process, as done with the generated
natural image dataset, and observed consistent outcomes, where our method notably enhances text-semantic correlation.
Nevertheless, due to constrained time and resources, all learning was conducted using a single image. This limitation
likely impacts learning efficacy, as previously identified in research (Gal et al., 2022; Vinker et al., 2023).
•We conducted learning exercises with MCPL and various baseline methods, calculating the embedding similarity of
learned object-level concepts against the masked ”ground truth” as indicated by DINOv1 (refer to the left figure in
Figure 17 for an overview and Figure 33 for detailed object-level outcomes).
•Additionally, we categorized the dataset based on the number of concepts per image to assess learning performance as
the number of concepts per image grows. This analysis, depicted in the right figure in Figure 17, revealed a consistent
challenge: as the number of concepts increases, the difficulty mirrors that observed in previous imagery annotation-based
learning methods such as Cones (Liu et al., 2023) and Break-a-Scene (Avrahami et al., 2023).
14

--- PAGE 15 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
DINOv10.3000.3250.3500.3750.4000.4250.4500.4750.500cosine similarities - DINOv1Object-level fidelity (mix of 2~5 concepts).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CL
MCPL-one+CL+Mask
Embedding similarity in learned object-level concepts compared to
masked “ground truth” (real image)
2 3 4 5
Number of concepts per image0.300.350.400.450.500.550.600.65DINOv1 cosine similarities
Object-level fidelity versus number of concepts per image
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+MaskEvaluate the learning as the number of concepts per image increases
(real image).
Figure 17. Quantitative results on real images 2-5 concepts per image and a total of 50 concepts, data point represents the pairwise cosine
similarities measured by DINOv1. Also see the full object-level result in Figure 33
•Visual outcomes presented in Figure 18 and Figure 19 1) validate our quantitative findings our fully regularized version
consistently achieving higher accuracy in semantic correlation compared to the baseline methods and 2) highlight the
increasing challenge as scenes become more complex.
•It’s important to recognise that the observed performance decline is mainly attributed to 1) all learnings are performed with
a single image due to constrained time and resources and 2) our method is implemented on the relatively less powerful
LDM (Rombach et al., 2022) model, operating at a resolution of 256×256to enhance computational efficiency.
“a green cactus/ token1 settled in a white pot/ token2 with red flower/ token3 on it”
“a black ink/ token1 a brown paper/ token2 and a stack photographs/ token3”
“a green cactus/ token1 a pink pot/ token2 glass perfume/ token3 and white box/ token4”
“a red mug/ token1 green plants/ token2 blank notebook/ token3 and a black pen/ token4”
Figure 18. Visual results on real images. From left to right: 1) real input image, 2) generated image, 3) MCPL-all, 4) MCPL-one,
5) MCPL-all+ PromptCL +Bind adj. , 6) MCPL-all+ AttnMask +PromptCL +Bind adj. , 7) MCPL-one+ PromptCL +Bind adj. , 8) MCPL-
one+ AttnMask +PromptCL +Bind adj. .The final two columns showcase our optimised versions, consistently achieving higher accuracy
in semantic correlation compared to the baseline methods.
15

--- PAGE 16 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
“a black ink/ token1 a brown paper/ token2 and a stack photographs/ token3”
“a red mug/ token1 green plants/ token2 blank notebook/ token3 and a black pen/ token4”
Figure 19. Visualisation with mask and attention. We compare all baseline methods across each row (from top to bottom): 1) MCPL-all,
2) MCPL-one, 3) MCPL-all+ PromptCL +Bind adj. , 4) MCPL-all+ AttnMask +PromptCL +Bind adj. , 5) MCPL-one+ PromptCL +Bind adj. ,
6) MCPL-one+ AttnMask +PromptCL +Bind adj. .
16

--- PAGE 17 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.3. Comparing concepts discovery with Inspiration Tree
Inspiration Tree (IT) (Vinker et al., 2023) shares a similar spirit as our work in discovering multiple concepts from images,
diverging from the language-guided discovery characteristic of MCPL to allow for unstructured concept exploration. MCPL
excels in instruction following discovery, adept at identifying concepts distinct in both visual and linguistic aspects, whereas
IT specializes in unveiling abstract and subtle concepts independently of direct human guidance. We view MCPL and IT
as complementary, enhancing different aspects of concept identification across diverse fields like arts, science, and
healthcare. In this section, we compare MCPL with IT in both concepts exploration Figure 20 and combination Figure 22.
MCPL (full regularisation)
MCPL learn original image with "a wood v2  decorated with colourful v1 "Inspiration Tree
"a wood v2  decorated with colourful v1 " "a v2 decorated with  v1"
"colourful v1 "
"v1"
"wood v2 "
"v2"
MCPL learn v2 with "a wood v5  of pretend v6 "
"wood v5 " "v5"
 "pretend v6 " "v6"
Figure 20. Comparing MCPL with Inspiration Tree (Vinker et al., 2023) in exploring concepts from image . We observe: 1) Although
MCPL was not specifically designed for abstract concept learning, it demonstrates decent performance in comparison to IT, even
in its deeper layers. 2) Adjectives assume a different role from their function in previous tasks focused on semantic differentiation.
Typically, an adjective defines an abstract ’style,’ like ’colourful’. Yet, in certain instances, the adjective can overpower the underlying
concept of interest, as seen in comparisons like ‘wood v5’ versus ‘wood v2’.
17

--- PAGE 18 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
MCPL (full regularisation)Inspiration Tree
"a wood v5  decorated with colourful v1 "
"a v5 decorated with  v1""a pretend v6  decorated with colourful v1 "
"a v6 decorated with  v1"
Figure 21. Comparing MCPL with Inspiration Tree (Vinker et al., 2023) in combining separately learnt concepts into one image . We
observe: 1) MCPL showcases notable effectiveness in comparison to IT, despite its initial focus on semantic regions. It adeptly discerns
underlying abstract styles and integrates them into new scenes. 2) Adjectives exhibit varied influences: terms like ‘colourful’ and ‘pretend’
effectively direct ‘v1’ and ‘v6’ towards accurately learning and reproducing the intended concept, even after removing ‘colourful’ in the
combination process. Conversely, ‘wood’ dominates the intended concept, misguiding ‘v5’.
Composing concepts learnt from different images
"a brown v1 (basket)  with
yellow v2 (banana) ""a black v3 (cat)  with white
v4 (cat) "
"a black v3 (cat)  and yellow v2 (banana) " "a v3 (cat)  and v2 (banana) "Input images
Figure 22. Combination of objects from different images. We observe: 1) MCPL demonstrates a good ability to combine concepts without
updating the diffusion model parameters but only update text embedding . 2) Yet, the performance of this combination approach has
limitations. As MCPL is aimed at identifying semantic regions within image-text pairs , for more sophisticated image editing, it is
advisable to use MCPL as a language-driven mask proposer . The identified masked objects can then serve as inputs for mask-based
techniques like Break-a-Scene (Avrahami et al., 2023), which updates the diffusion model parameters for enhanced recomposition.
18

--- PAGE 19 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.4. Stress test with challenge images
•To fully explore the boundary of our method, we conducted a set of challenging experiments including learning concepts
with similar colour (Figure 23), same category (Figure 24), similar appearance (Figure 25, Figure 26) and learning a large
number of concepts from a single image (Figure 27).
Maskformer  
(S-L-coco)Maskformer  
(S-L-ade)
Segment  
AnythingInput  
image
“a patterned butterfly/ token1 resting on an orange flower/ token2”
Figure 23. Successful stress test in learning concepts with a similar colour . On the left, the input image and masks predicted by
pre-trained segmentation models are displayed, highlighting the difficulty in distinguishing textures between two concepts . On the
right, our results are presented, with the last two columns demonstrating our optimized approach using various learning strategies, which
consistently outperforms baseline methods including segmentation models in terms of semantic accuracy. From left to right we have: 1)
MCPL generated image, 2) MCPL-all, 3) MCPL-one, 4) MCPL-all+ PromptCL +Bind adj. , 5) MCPL-all+ AttnMask +PromptCL +Bind adj. ,
6) MCPL-one+ PromptCL +Bind adj. , 7) MCPL-one+ AttnMask +PromptCL +Bind adj. .
Input image MCPL AttnMask masked 'cat' (black) masked 'cat' (white) attention 'cat' (black) attention 'cat' (white)
“a running black cat/ token1 and a sitting white cat/ token2”
Figure 24. Successful stress test in learning concepts with the same category — two cats. From left to right, we present the input
image, the MCPL-generated image with full regularisation, the semantic masks, masked concepts, and per-prompt cross-attention. We
demonstrate MCPL’s capability to identify the same category when the semantic appearances are distinct in both visual and
linguistic aspects .
Input image
 MCPL AttnMask masked dog masked cat attention dog attention cat
“a smiling dog/ token1 and an elegant cat/ token2”
Figure 25. Mixed stress test in learning concepts with a similar appearance (i.e., shared adjectives) — a dog and a cat with a similar
colour. From left to right, we present the input image and our fully regularised result. In this scenario, MCPL demonstrates varied
performance, distinguishing the animal’s face from its body instead of by category. This issue stems from the indistinct appearance,
particularly linguistically. Since adjectives critically guide the model’s focus, learning falters when the adjective does not effectively
differentiate regions .
19

--- PAGE 20 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Input image MCPL AttnMask masked 'bird' (bold) masked 'bird' (curious) attention bird' (bold) attention 'bird' (curious)
“a bold bird/ token1 on top of a curious bird/ token2”
Figure 26. Failed stress test in learning concepts with visually similar concepts — a photo of two identical birds. From left to right, we
present the input image and our fully regularised result. In an extreme case where two birds are identical, MCPL fails to differentiate
between them. MCPL aims to accurately learn multiple concepts following language instructions. However, when concepts are
linguistically indistinguishable, learning falters. In these instances, we recommend adopting learning methods with imagery annotation
such as Cones (Liu et al., 2023), Breas-a-Scene (Avrahami et al., 2023) or less reliant on human input, like the Inspiration Tree (Vinker
et al., 2023), as discussed in Section A.3.
Input image MCPL
Figure 27. Failed stress test in learning concepts with a large number of concepts from a single image . MCPL struggles to learn
a large number of concepts from a single image, a limitation also noted in previous image annotation-based multi-concept learning
methods like Cones (Liu et al., 2023) and Break-a-Scene (Avrahami et al., 2023). This challenge is exacerbated by a few key factors: 1)
Limited Example Images : As highlighted by (Gal et al., 2022) and (Vinker et al., 2023), learning is more efficient with 4 to 6 example
images showcasing the same concepts. Yet, sourcing multiple images containing a broad array of concepts for learning is difficult. 2)
Weak Backbone Model : Our methodology employs the LDM (Rombach et al., 2022) model, chosen for its computational efficiency
despite being less powerful and operating at a resolution of 256×256. This limitation hampers the model’s ability to reconstruct
complex scenes (see the second example) leading to inaccurately associated concepts. Alternative Solution : Despite these challenges, we
propose combining the MCPL-diverse strategy with the random crop technique introduced by Break-a-Scene (Avrahami et al., 2023).
MCPL-diverse is a strategy to learn per-image specified concepts, while random cropping helps in subsampling concepts from a larger
image leading to increased examples. This combination enhances the ability to learn numerous concepts from a single image, as shown in
our six objects example in Figure 42, where MCPL achieves results comparable to the mask-based BAS method (Avrahami et al., 2023).
A.5. Limitations
MCPL enhances prompt-region correlation through natural language instructions but may encounter difficulties in scenarios
such as:
•When concepts are linguistically indistinct, for example, multiple identical or highly similar concepts that natural language
struggles to differentiate.
•In highly complex scenes containing many concepts with limited example images available, a challenge recognized by
(Liu et al., 2023) and (Avrahami et al., 2023).
20

--- PAGE 21 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.6. More editing examples
"a photo of brown * (basket) with yellow & (banana) "Generated image Cross-attention mask
 Refer ence image(s)  Multiple concepts
from single image
brown *   
stainless pot*   
pottery
&   
pineappleyellow &   
green grapesGenerating or editing each disentangled concept OOD concept
discovery 
"a photo of white ! (chest X-ray)  and black @ (lung)  which have smoky *
(consolidation) "
"white !" "smoky * " remove  "white !" remove  "smoky *"
Figure 28. MCPL learning and editing capabilities. Top-row: learning and editing multiple concepts with a single string with input
image from our generated 2-concept dataset (A.16). Bottom-row: learning to disentangle multiple unseen concepts from chest X-ray
images with input image from MIMIC-CXR dataset (Johnson et al., 2019).
A.7. Full t-SNE results
To assess disentanglement, we calculate and visualise the t-SNE projections. We approximate the “ground truth” using
features learned through Textual Inversion on per-concept masked images. For benchmarking, we compare our method with
the SoTA mask-based multi-concept learning method, Break-A-Scene (BAS) , which acts as a performance benchmark but
isn’t directly comparable. Additionally, we assess variants integrating our proposed regularization techniques to align with
the learning objectives.
MCPL-all (learn all pr ompts in a string)
MCPL-one + AttnMask  
+ PromptCL  + Bind adj.Estimated 'gr ound truth' : Textural
Inversion (T .I.) + maskMCPL-one + AttnMask MCPL-one (learn one noun pr ompt per
concept)
Image level pr ompt learning
with  Textural Inversion (T .I.)MCPL-all + AttnMask  
+ PromptCL  + Bind adj.MCPL-all + AttnMask
Figure 29. The t-SNE visualisations of learned prompt-concept features (comparing all variants) on the in-distribution natural (top)
dataset. The results confirmed ourMCPL-one combined with all regularisation terms can effectively distinguish all learned concepts
compared to all baselines.
21

--- PAGE 22 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
3
MCPL-all + AttnMask + PromptCL  +
Bind adj.MCPL-all + AttnMask MCPL-all (learn all pr ompts in a string)
MCPL-one + AttnMask + PromptCL  +
Bind adj.Estimated 'gr ound truth' : Textural
Inversion (T .I.) + maskMCPL-one (learn one noun pr ompt per
concepImage level pr ompt learning
with  Textural Inversion (T .I.)
MCPL-one + AttnMask
Figure 30. The t-SNE visualisations of learned prompt-concept features (comparing all variants) on the out-distribution medical (bottom)
dataset. The results confirmed ourMCPL-one combined with all regularisation terms can effectively distinguish all learned concepts
compared to all baselines.
A.8. All object-level embedding similarity of the learned concept relative to the estimated “ground truth”.
To assess how well our method preserves object-level semantic and textual details, we evaluate both prompt and image
fidelity. Our experiments, depicted in Figures 31 and 32, involve learning from dual-concept natural and medical images, and
from 3 to 5 natural images. Results consistently demonstrate that our method adding all proposed regularisation terms
accurately learns object-level embeddings , in comparison with masked ground truth in pre-trained embedding spaces such
as BERT, CLIP, DINOv1, and DINOv2. Notably, our method sometimes surpasses the state-of-the-art mask-based technique,
Break-A-Scene, at the object level. This underscores the effectiveness and potential of our mask-free, language-driven
approach to learning multiple concepts from a single image.
22

--- PAGE 23 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
bananasbasketchairdog
skateboard teddybearballcactushamster
watermelon0.10.20.30.40.5cosine similarities - BERTObject-level fidelity (natural images - BERT).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
cavity
myocardiumtumour edematransition peripheral0.10.20.30.40.5cosine similarities - BERTObject-level fidelity (medical images - BERT).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
bananasbasketchairdog
skateboard teddybearballcactushamster
watermelon0.600.650.700.750.800.850.900.951.00cosine similarities - CLIPObject-level fidelity (natural images - CLIP).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
cavity
myocardiumtumour edematransition peripheral0.600.650.700.750.800.850.900.951.00cosine similarities - CLIPObject-level fidelity (medical images - CLIP).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
bananasbasketchairdog
skateboard teddybearballcactushamster
watermelon0.00.20.40.60.81.0cosine similarities - DINOv1Object-level fidelity (natural images - DINOv1).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
cavity
myocardiumtumour edematransition peripheral0.10.20.30.40.50.60.70.8cosine similarities - DINOv1Object-level fidelity (medical images - DINOv1).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
bananasbasketchairdog
skateboard teddybearballcactushamster
watermelon0.920.940.960.981.001.021.041.06cosine similarities - DINOv2Object-level fidelity (natural images - DINOv2).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
cavity
myocardiumtumour edematransition peripheral0.900.920.940.960.981.001.021.04cosine similarities - DINOv2Object-level fidelity (medical images - DINOv2).
MCPL-all
MCPL-one
MCPL-all+CLMCPL-all+CL+Mask
MCPL-one+CLMCPL-one+CL+Mask
BAS (mask-based SoTA)
Figure 31. Two-concepts natural (left column) and medical (right column) per-object embedding similarity between the learned concept
relative to the masked “ground truth”. Each plot computes either the textural or image embeddings in one of the four embedding spaces
(BERT, CLIP, DINOv1 and DINOv2). Each bar represents 4000 (natural images) or 6000 (medical images) pairwise cosine similarities.
We compare our base version adding variations of our proposed regularisation terms. We also compare against the state-of-the-art (SoTA)
mask-based learning method, Break-A-Scene (BAS) (Avrahami et al., 2023).
23

--- PAGE 24 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
bananas
watermelonpot
basket
skateboardtaxi
hamsterteddybearmug0.10.20.30.40.50.60.70.80.9cosine similarities - DINOv1Object-level fidelity (natural 3 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+MaskBAS (mask-based SoTA)
bananasjacket
watermelonpotspoon basket
skateboardtaxi
hamsterteddybearmugcheese0.10.20.30.40.50.60.70.80.9cosine similarities - DINOv1Object-level fidelity (natural 4 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+MaskBAS (mask-based SoTA)
tableclothbeachbananasjacket
watermelonpotspoon basketroad
skateboardtaxi
hamsterteddybearmugcheese0.10.20.30.40.50.60.70.8cosine similarities - DINOv1Object-level fidelity (natural 5 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+MaskBAS (mask-based SoTA)
Figure 32. Natural images (each containing 3,4 or 5 objects) per-object embedding similarity between the learned concept relative to
the masked “ground truth”. Each plot computes image embeddings in the embedding spaces of DINOv1. We compare our base version
adding variations of our proposed regularisation terms. Notably, our method sometimes surpasses the SoTA mask-based technique,
Break-A-Scene (Avrahami et al., 2023), at the object level such as bananas and teddybear.
24

--- PAGE 25 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
magazineflower tableink
photographscactus paperpot dog0.00.20.40.60.81.0cosine similarities - DINOv1Object-level fidelity (real 3 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+Mask
pouchboxplantsmug boots cactuspen
perfume notebooksweaternovelpot0.00.20.40.60.81.0cosine similarities - DINOv1Object-level fidelity (real 4 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+Mask
shoes chestclothestiechair
notebookcamera bucketbanknotepainting0.00.20.40.60.81.0cosine similarities - DINOv1Object-level fidelity (real 5 concepts images).
MCPL-all
MCPL-oneMCPL-all+CL
MCPL-all+CL+MaskMCPL-one+CL
MCPL-one+CL+Mask
Figure 33. Real images (each containing 3,4 or 5 objects) per-object embedding similarity between the learned concept relative to the
masked “ground truth”. Each plot computes image embeddings in the embedding spaces of DINOv1. We compare our base version
adding variations of our proposed regularisation terms. We observe our method consistently improving the accuracy of prompt-region
correlation at the object level.
25

--- PAGE 26 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.9. Full motivational experiment results
Do multiple distinct embeddings arise from the same image? To understand the possibility of learning multiple concepts
within a frozen textual embedding space, we explored whether Textual Inversion can discern semantically distinct concepts
from processed images, each highlighting a single concept. Following (Wu et al., 2020), we used images with manual masks
to isolate concepts, as seen in Figure 34. We applied Textual Inversion to these images to learn embeddings for the unmasked
or masked images. Our findings indicate that when focusing on isolated concepts, Textual Inversion can successfully learn
distinct embeddings, as validated by the generated representations of each concept.
"a photo of * (watch face + watch band) "
Learning multi-  
concepts images 
Generating multi-
concepts  images Learning single concept
from masked image 
"a photo of ! (watch band) "
Generating single
concept  images Learning single concept
from masked images Generating single
concept  images 
"a photo of @ (watch face ) "
Figure 34. Motivational study with watch images. We learn embeddings using Textual Inversion on both unmasked multi-concept
images (“watch face” and “watch band”) and masked single-concept images (“watch face” or “watch band”).
Is separate learning of concepts sufficient for multi-object image generation? While separate learning with carefully
sampled or masked images in a multi-object scene deviates from our objective, it is valuable to evaluate its effectiveness.
Specifically, we use Textual Inversion to separately learn concepts like “ball” and “box” from carefully cropped images, as
shown in Figure 35. We then attempt to compose images using strings that combine these concepts, such as “a photo of a
green ball on orange box.” Our results indicate that the accurate composition of multi-object images remains challenging,
even when individual concepts are well-learned.
T.I.: separately learn each concept + compose
"a photo of green *""a photo of orange  @"
merge embeddings of " green *" and " orange  @"
"a photo of green * and orange  @"MCPL-one: jointly learn multi-concepts MCPL-diverse: learn per image multi-concepts
"a photo of  
green *  
under   
orange  @""a photo of  
green *  
on  
orange  @""a photo of  
green *  
front  
orange  @"
"a photo of green * on orange  @"
"a photo of green * under orange  @"
"a photo of green * in front of orange  @"
"a photo of green * on orange  @"
"a photo of green * under orange  @"
"a photo of green * in front of orange  @"
"a photo of green * on orange  @"
"a photo of green * under orange  @"
"a photo of green * in front of orange  @"
Figure 35. Learning and Composing “ball” and “box” . We learned the concepts of “ball” and “box” using different methods (top row)
and composed them into unified scenes (bottom row). We compare three learning methods: Textual Inversion (Gal et al., 2022), which
learns each concept separately from isolated images (left); MCPL-one , which jointly learns both concepts from uncropped examples using
a single prompt string (middle); and MCPL-diverse , which advances this by learning both concepts with per-image specific relationships
(right).
A.10. Full ablation results of assessing regularisation terms with cross-attention
We present in this section the full results of assessing our proposed regularisation terms in Section 3.4. The results presented
in Figure 36 indicate that plain MCPL may not accurately capture semantic correlations between prompts and objects. While
adding incorporating the proposed regularisation terms enhances concept disentanglement. We assess the efficacy of these
terms in disentangling learned concepts by visualising attention and segmentation masks, as shown in Figure 36. Figure 37
and Figure 38 present the same visualisation of the scaled quantitative experiments involving 2 to 5 concepts.
26

--- PAGE 27 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
   "a        photo        of       green        *           on      orange       @" Mask Image
MCPL-one
MCPL-one
+AttnMask
MCPL-one
+PromptCL
MCPL-one
+PromptCL
+Bind adj .
MCPL-one
+AttnMask
+PromptCL
+Bind adj.
   "a         fluffy        @       eating       red         *           on          the      beach" Mask Image
MCPL-all
MCPL-one
+PromptCL
+Bind adj .
MCPL-one
+AttnMask
+PromptCL
+Bind adj.MCPL-one
MCPL-all
+PromptCL
+Bind adj .
MCPL-all
+AttnMask
+PromptCL
+Bind adj.
Figure 36. Enhancing object-level prompt-concept correlation in MCPL using proposed AttnMask ,PromptCL andBind adj. regularisation
techniques. We use average cross-attention maps to quantify the correlation of each prompt with its corresponding object-level concept.
Additionally, we construct attention-based masks from multiple selected prompts for the concepts of interest. The visual results confirm
thatincorporating all of the proposed regularisation terms enhances concept disentanglement, whereas applying them in isolation
yields suboptimal outcomes.
Ball Cactus Bananas Basket Hamster Watermelon
Cavity Myocardium Tumour Edema Peripheral Transition
Figure 37. Visualisation of concepts (two concepts). We compare all baseline methods across each row (from top to bottom): 1) MCPL-all,
2) MCPL-one, 3) MCPL-all+ PromptCL +Bind adj. , 4) MCPL-all+ AttnMask +PromptCL +Bind adj. , 5) MCPL-one+ PromptCL +Bind adj. ,
6) MCPL-one+ AttnMask +PromptCL +Bind adj. .The results on both the natural and medical images confirmed our conclusion — the
inclusion of all proposed regularisation terms (toward the bottom row) consistently demonstrated their effectiveness in enhancing the
accuracy of prompt-concept correlation.
27

--- PAGE 28 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Figure 38. Visualisation of concepts (3 to 5 concepts). We compare all baseline methods across each row: 1) MCPL-all, 2) MCPL-one,
3) MCPL-all+ PromptCL +Bind adj. , 4) MCPL-all+ AttnMask +PromptCL +Bind adj. , 5) MCPL-one+ PromptCL +Bind adj. , 6) MCPL-
one+ AttnMask +PromptCL +Bind adj. . Our findings indicate an increased challenge in learning multiple concepts as the number of objects
in an image rises, particularly with a mask-free, language-driven approach like ours. Despite this, we consistently observe improved
accuracy in prompt-concept correlation when incorporating all proposed regularization terms (toward the bottom row) .
28

--- PAGE 29 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.11. Ablation study comparing MCPL-diverse versus MCPL-one in learning per-image different concept tasks
TheMCPL-diverse training strategy has demonstrated potential in learning tasks with varying concepts per image. Therefore,
we performed further experiments as detailed in Figure 39 confirming its efficacy.
Learning with per image differ ent pr ompts  
"a fluffy @ (cat)  with a per-image-prompt  is lying on a beach chair"Input MCPL-diverse MCPL-one
black ! brown & top * unicorn ~
Learning with per image differ ent pr ompts  
"a photo of green * per-image-pr ompt  orange  @"Input MCPL-diverse MCPL-one
on fr ont under
Figure 39. Visual comparison of MCPL-diverse versus MCPL-one in learning per-image different concept tasks: Left - cat with different
hat example. Right - ball and box relationships example. As MCPL-diverse are specially designed for such tasks, it outperforms
MCPL-one, which fails to capture per image different relationships.
A.12. Ablation study on the effect of Adjective words.
Our language-driven approach benefits from the proposed adjective binding mechanism. To better understand its role, we
performed experiments removing adjective words, as shown in Figure 40, which confirmed its significance.
MCPL-diverse (with adjective)
"a photo of  
green *  
under   
orange  @""a photo of  
green *  
on  
orange  @""a photo of  
green *  
front  
orange  @"
"a photo of green * on orange  @"
"a photo of green * under orange  @"
"a photo of green * in front of orange  @"MCPL-diverse (without adjective)
"a photo of  
*  
under   
@""a photo of  
*  
on  
@""a photo of  
*  
front  
@"
"a photo of * on @"
"a photo of * under @"
"a photo of * in front of @"
Figure 40. Visual comparison of MCPL-diverse with adjective word versus without adjective word. Adjective words are crucial in
linking each prompt to the correct region; without them, the model may struggle for regional guidance and we observe reduced
performance.
29

--- PAGE 30 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.13. Visual comparison with BAS in composing complex scenes.
In tasks learning more than two concepts from a single image, we compare MCPL with Break-A-Scene (BAS). Unlike BAS,
MCPL neither uses segmentation masks as input nor updates model parameters . To level the playing field, we adopted
BAS’s ’union sampling’ training strategy, which randomly selects subsets of multi-concepts in each iteration. We manually
prepared a set of cropped images of each individual concept and randomly selected subsets to combine. This approach,
termed ’random crop, ’ serves as our equivalent training strategy, see Figure 41 for an illustration. Given that each cropped
image has a different number of concepts, we utilised our MCPL-diverse , designed to learn varying concepts per image. In
Figure 41 and Figure 42 we showcase examples of such tasks against a set of competitive baselines.
"a photo of colorful [ v1] sitting on
a rock in the Grand Canyon"Textural Inversion-m Dreambooth-m Break-A-Scene
"a photo of colorful [ v1]
sitting on [ v2] with green
[v3] next to a river"Masked Image-T ext
iNPUT  Image-T ext Input
Our mask-fr ee multi-concept learning
Text input for random crop  
 
"colorful [ v1]" 
"black [ v2] with gr een [ v3]" 
 
 
"colorful [ v1] sitting on
black [ v2] with gr een [ v3]"Mask-based multi-concept learning
MCPL-diverse +
random cr op Custom Diffusion-m ELITE
Figure 41. A qualitative comparison between our method ( MCPL-diverse ) and mask-based approaches: BAS, Textual Inversion (Gal et al.,
2022) (masked), DreamBooth (Ruiz et al., 2022) (masked), Custom Diffusion (Kumari et al., 2023) (masked) and ELITE (Wei et al.,
2023). We stress our focus is not on optimising Diffusion Model (DM) parameters for enhanced composing performance , unlike other
compared methods. Our approach delivers commendable results by learning from textual tokens (less than 0.1 MB) instead of
updating the Diffusion Model of 4.9 GB as done in BAS, and it does so without depending on visual annotations such as masks.
For tasks prioritising composition, our method can serve as an initial mask proposal to identify pertinent tokens, subsequently integrating
finetuning-based techniques for refinement. Images modified from BAS (Avrahami et al., 2023).
Mask-fr ee learning (ours MCPL-diverse)
 text input:"a blue [ v5] grey [v6]
shiny [ v3] brown [ v4] orbital
[v1] marble [ v2]"Masked learning (Br eak-A-Scene)  
"a photo of shiny [ v1] in
the desert""a photo of orbital [ v1]
and marble [ v2] on the
grass""a photo of orbital [ v1]
and blue [ v5] and grey
[ v6] in the forest"
"a photo of blue [ v1]
at the beach""a photo of red [ v2]
at the beach""a photo of blue [ v1]
red [ v2] at the beach"text input: "blue [ v1] with r ed
[v2] and r ound [ v3]"Inputs  
Figure 42. A qualitative comparison between BAS and our method (MCPL-one and MCPL-diverse). Top, learning six concepts from a
single image and then composing a subset, is particularly challenging, which BAS has acknowledged. Similar to BAS, our method also
faces challenges with a high number of concepts, but it shows promising and competitive results. Bottom learns three concepts
from a single image. In this example, BAS performed better. Our MCPL-diverse, which neither uses mask inputs nor updates model
parameters, showed decent results and was closer to BAS. Images modified from BAS (Avrahami et al., 2023).
30

--- PAGE 31 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
A.14. MCPL Algorithm and Variations
We provide the formal definitions of all MCPL training
strategies: MCPL (generic form) in Algorithm 1, MCPL-all
in Algorithm 2, MCPL-one in Algorithm 3 and MCPL-
diverse in Algorithm 4.
Algorithm 2 MCPL-all
1:Input: example image(s) x, pre-trained {cθ, ϵθ}.
2:Output: a list of embeddings V= [v∗, . . . , v&]corresponds
to multiple new prompts P= [p∗, . . . , p&],
3:which includes [v∗
n, . . . , v&
n]corresponds to noun words of
target concepts [n∗, . . . , n&],[v∗
a, . . . , v&
a]corresponds to as-
sociated adjective words [a∗, . . . , a&]and[v1
t, . . . , v&
t]corre-
sponds to the rest texts in the string T= [t∗, . . . , t&](exclude
random neutral texts).
4: initialise [v∗, . . . , v&] = [cθ(p∗), . . . , c θ(p&)]
5:# optimising {v∗, . . . , v&}with LDM
6:forstep = 1toSdo
7: Encode example image(s) z=E(x)and randomly sample neutral texts y
to make string [y, p∗, . . . , p&]
8: Compute V†= [vy, v∗, . . . , v&] = [cθ(py), cθ(p∗), . . . , c θ(p&)]
9: fort=Tdown to 1do
10: V:= arg minVEz,V,ϵ,t∥ϵ−ϵθ(zt,V†)∥2
11: end for
12:end for
13:Return (P,V)
Algorithm 3 MCPL-one
1:Input: example image(s) x, pre-trained {cθ, ϵθ}.
2:Output: a list of embeddings including V= [v∗, . . . , v&]
corresponds to multiple new prompts P= [p∗, . . . , p&],
3:which includes [v∗
n, . . . , v&
n]corresponds to noun words of
target concepts [n∗, . . . , n&].
4: initialise [v∗, . . . , v&] = [cθ(p∗), . . . , c θ(p&)]
5:# optimising {v∗, . . . , v&}with LDM
6:forstep = 1toSdo
7: Encode example image(s) z=E(x)and randomly sample neutral texts y
to make string [y, p∗, . . . , p&]
8: Compute V†= [vy, v∗, . . . , v&] = [cθ(py), cθ(p∗), . . . , c θ(p&)]
9: fort=Tdown to 1do
10: V:= arg minVEz,V,ϵ,t∥ϵ−ϵθ(zt,V†)∥2
11: end for
12:end for
13:Return (P,V)
A.15. Implementation details.
We use the same prompts collected during the data prepa-
ration, substituting nouns as learnable prompts, which are
merged with CLIP prompts from Section 3.1. This process
creates phrases such as “A photo of brown * on a rolling
@ at times square” . Unless otherwise noted, we retain the
original hyper-parameter choices of LDM (Rombach et al.,
2022). Our experiments were conducted using a single
V100 GPU with a batch size of 4. The base learning rate
was set to 0.005. Following LDM, we further scale the base
learning rate by the number of GPUs and the batch size, forAlgorithm 4 MCPL-diverse (generic form)
1:Input: a set of Dexample images Xd, d∈D, each described
by prompts Pd= [p∗
d, . . . , p&
d], pre-trained {cθ, ϵθ}.
2:Output: a list of embeddings V= [v∗, . . . , v&]corresponds
to multiple new prompts P= [p∗, . . . , p&],
3:which includes [v∗
d, . . . , v&
d]corresponds to multiple new
prompts [p∗
d, . . . , p&
d]of each Xd, d∈D.
4: initialise [v∗, . . . , v&] = [cθ(p∗), . . . , c θ(p&)]
5:# optimising {v∗, . . . , v&}with LDM
6:forstep = 1toSdo
7: Encode example image(s) z=E(x)and randomly sample neutral texts y
to make string [y, p∗, . . . , p&]
8: Compute V†= [vy, v∗, . . . , v&] = [cθ(py), cθ(p∗), . . . , c θ(p&)]
9: fort=Tdown to 1do
10: V:= arg minVEz,V,ϵ,t∥ϵ−ϵθ(zt,V†)∥2
11: end for
12:end for
13:Return (P,V)
an effective rate of 0.02. On calculating LPromptCL , we ap-
ply the temperature and scaling term (τ, γ)of(0.2,0.0005)
when AttnMask is not applied, and (0.3,0.00075) when At-
tnMask is applied. All results were produced using 6100
optimisation steps. We find that these parameters work well
for most cases. The experiments were executed on a single
V100 GPU, with each run taking approximately one hour,
resulting in a total computational cost of around 3500 GPU
hours (or 150 days on a single GPU). We employed various
metrics to evaluate the method.
A.16. Dataset preparation.
For the in-distribution natural images dataset, we first gen-
erate variations of two-concept images using local text-
driven editing, as proposed by (Patashnik et al., 2023). This
minimizes the influence of irrelevant elements like back-
ground. This approach also produces per-text local masks
based on attention maps, assisting us in getting our best ap-
proximation for the “ground truth” of disentangled embed-
dings. We generate five sets of natural images containing
10 object-level concepts. We generate each image using
simple prompts, comprising one adjective and one noun for
every relevant concept. For three to five concept images,
we use break-a-scene (Avrahami et al., 2023) to generate
the more complex composed images. We generate nine
sets containing 9 more object-level concepts. We then use
separate pre-trained segmentation models—MaskFormer
(Cheng et al., 2021) to create masked objects, refer to Ap-
pendix A.17 for details of this process.
For the out-of-distribution bio-medical image dataset, we
assemble three sets of radiological images featuring 6 or-
gan/lesion concepts. These images are sourced from three
public MRI segmentation datasets: heart myocardial in-
farction (Lalande et al., 2020), prostate segmentation (An-
tonelli et al., 2022), and Brain Tumor Segmentation (BraTS)
31

--- PAGE 32 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
(Menze et al., 2014). Each dataset includes per-concept
masks. For biomedical images, we request a human or a
machine, such as GPT-4, to similarly describe each image
using one adjective and one noun for each pertinent concept.
For both natural and biomedical datasets, we collected 40
images for each concept. Figure 43 and Figure 44 gives
some examples of the prepared datasets.
Two-concepts (natural images)
•“a brown {bear/ tokens1 }on a rolling {skateboard/ tokens2 }
at times square”
•“a fluffy {hamster/ tokens1 }eating red {watermelon/ to-
kens2}on the beach”
• “a green {cactus/ tokens1 }and a red {ball/ tokens2 }in the
desert”
•“a brown {basket/ tokens1 }with yellow {bananas/ to-
kens2}”
•“a white {chair tokens1 }with a black {dog/ tokens2 }on it”
Two-concepts (medical images)
•“a{scan/ tokens1 }with brighter {cavity/ tokens2 }encircled
by grey {myocardium/ tokens3 }”
•“a{scan/ tokens1 }with round {transition/ tokens2 }encir-
cled by circle {peripheral/ tokens3 }”
•“a{scan/ tokens1 }with round {tumour/ tokens2 }encircled
by circle {edema/ tokens3 }”
Three-concepts (natural images)
•“a brown {bear/ tokens1 }on a rolling {skateboard/
tokens2 }beside a red {taxi/ tokens3 }at times square”
•“a green {pot/ tokens1 }and a {hamster/ tokens1 }
eating red {watermelon/ tokens2 }on the beach”
•“a{basket/ tokens1 }with yellow {bananas/ tokens2 }
beside a white {mug/ tokens3 }”
Four-concepts (natural images)
•“a brown {bear/ tokens1 }on a rolling {skateboard/
tokens2 }wearing a blue {jacket/ tokens3 }beside a
red{taxi/ tokens4 }at times square”
•“a green {pot/ tokens1 }and a {hamster/ tokens1 }
eating red {watermelon/ tokens2 }beside a yellow
{cheese/ tokens4 }on the beach”
•“a{basket/ tokens1 }with yellow {bananas/ tokens2 }
beside a white {mug/ tokens3 }and silver {spoon/
tokens4 }”Five-concepts (natural images)
•“a brown {bear/ tokens1 }on a rolling {skateboard/
tokens2 }wearing a blue {jacket/ tokens3 }beside a
red{taxi/ tokens4 }over grey {road/ tokens5 }at times
square”
•“a green {pot/ tokens1 }and a {hamster/ tokens1 }
eating red {watermelon/ tokens2 }beside a yellow
{cheese/ tokens4 }on the sandy {beach/ tokens5 }”
•“a{basket/ tokens1 }with yellow {bananas/ tokens2 }
beside a white {mug/ tokens3 }and silver {spoon/
tokens4 }over blue {tablecloth/ tokens5 }”
A.17. Break-A-Scene experiments setup
Break-A-Scene (BAS) (Avrahami et al., 2023) learns mul-
tiple concepts from images paired with object-level masks.
It augments input images with masks to highlight target
concepts and updates both textual embeddings and model
weights accordingly. BAS introduces ’union sampling’,
a training strategy that randomly selects subsets of multi-
concepts in each iteration to enhance the combination of
multiple concepts in generated images, see Figure 41 for an
illustration. During inference, BAS employs a pre-trained
segmentation model to obtain masked objects, facilitating
localised editing.
To fit BAS (Avrahami et al., 2023) into our evaluation pro-
tocol, we first learned object-level concepts and then gener-
ated masked objects for evaluation, including the following
steps:
1.BAS Learning: For each concept pair, we randomly
selected 20 images with ground truth segmentations
from our dataset for BAS learning, resulting in 20 BAS
embeddings per concept.
2.BAS Generation: We then generated 20 images for
each concept pair, producing a total of 100 BAS-
generated natural images and 60 medical images.
3.Segmentation: For masked object production with
BAS, we used different pre-trained segmentation mod-
els. MaskFormer (Cheng et al., 2021) was effective for
natural images, but segmenting medical images posed
challenges due to their out-of-distribution characteris-
tics.
4.Quantitative Evaluation: With the obtained masked
objects (20 per concept), we applied the embedding
similarity evaluation protocol from Section 4.2 to as-
sess the preservation of semantic and textual details
per concept in four embedding spaces.
32

--- PAGE 33 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
hamster (top) - watermelon (bottom) cavity (top) - myocardium (bottom)
bananas (top) - basket (bottom) transition (top) - peripheral (bottom)
cactus (top) - ball (bottom) tumour (top) - edema (bottom)
Figure 43. Evaluation dataset (two concepts). We prepared five sets of in-distribution natural images and three sets of out-of-distribution
biomedical images, each containing two concepts resulting in a total of 16 concepts.
Figure 44. Evaluation dataset (three to five concepts). We generate nine sets containing 9 more object-level concepts.
For segmenting medical images, given the diversity of
classes in our dataset, we utilised MedSAM (Ma & Wang,
2023), a state-of-the-art foundation model adapted from
SAM (Kirillov et al., 2023) for the medical domain. Med-
SAM requires a bounding box for input, making it a multi-
step, human-in-the-loop process. We initially assessed seg-
mentation quality from several (up to five) bounding box
proposals, as exemplified in Figure 45. MedSAM, despite
having a bounding box, cannot fully automate segmentation
for all classes.
Thus, we employed an additional post-processing step to
discern the segmentation of both classes by calculating the
difference between the two segmentations.
Figure 45. This demonstration shows how MedSAM is used to
segment medical images generated by BAS. On the left, MedSAM
segmentation with a large bounding box prompt can identify the
combined area of the cavity-myocardium classes, but it does not
distinguish between the two. On the right, using a smaller bound-
ing box prompt, MedSAM successfully segments the central cavity
class. We calculate the difference to get the segmentation of the
missing myocardium class (outer ring-like pattern).
33

--- PAGE 34 ---
An Image is Worth Multiple Words: Discovering Object Level Concepts using Multi-Concept Prompt Learning
Figure 46. Visualisation of the Break-A-Scene results of generated
and masked natural images.
Figure 47. Visualisation of the Break-A-Scene results of generated
and masked medical images.
34
