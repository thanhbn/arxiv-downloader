# Zorro: Transformer đa phương thức có mặt nạ

Adrià Recasens1†Jason Lin1João Carreira1Drew Jaegle1Luyu Wang1Jean-baptiste Alayrac1
Pauline Luc1Antoine Miech1Lucas Smaira1Ross Hemsley1Andrew Zisserman1,2
1DeepMind2VGG, Dept. of Engineering Science, University of Oxford

## Tóm tắt

Các mô hình dựa trên attention rất hấp dẫn cho xử lý đa phương thức vì đầu vào từ nhiều phương thức có thể được nối lại và cung cấp cho một mạng backbone duy nhất – do đó chỉ cần rất ít kỹ thuật fusion. Tuy nhiên, các biểu diễn thu được hoàn toàn bị vướng víu trong toàn bộ mạng, điều này không phải lúc nào cũng được mong muốn: trong quá trình học, học tập tự giám sát âm thanh-hình ảnh contrastive yêu cầu các đặc trưng âm thanh và hình ảnh độc lập để hoạt động, nếu không thì việc học sẽ sụp đổ; trong suy luận, việc đánh giá các mô hình âm thanh-hình ảnh nên có thể thực hiện trên các benchmark chỉ có âm thanh hoặc chỉ có video. Trong bài báo này, chúng tôi giới thiệu Zorro, một kỹ thuật sử dụng mặt nạ để kiểm soát cách đầu vào từ mỗi phương thức được định tuyến bên trong Transformers, giữ một số phần của biểu diễn thuần khiết theo phương thức. Chúng tôi áp dụng kỹ thuật này cho ba kiến trúc transformer phổ biến (ViT, Swin và HiP) và cho thấy với pre-training contrastive, Zorro đạt được kết quả tối ưu trên hầu hết các benchmark liên quan cho các tác vụ đa phương thức (AudioSet và VGGSound). Hơn nữa, các mô hình thu được có thể thực hiện suy luận đơn phương thức trên cả benchmark video và âm thanh như Kinetics-400 hoặc ESC-50.

## 1. Giới thiệu

Nhận thức của chúng ta về thế giới vốn dĩ là đa phương thức: con người và các động vật khác dễ dàng tích hợp nhiều phương thức để xây dựng cái nhìn của họ về thế giới [5,23]. Mặc dù tích hợp đa phương thức có thể giúp xây dựng một góc nhìn phong phú hơn về thực tế [10,44], con người có thể dễ dàng xử lý thông tin và thực hiện các tác vụ ngay cả khi chỉ có một phương thức duy nhất (ví dụ: âm thanh, thị giác, hoặc xúc giác) [11,32,45]. Tuy nhiên, tính linh hoạt này khó tìm thấy trong các mô hình tính toán nhận thức. Các kiến trúc cho nhận thức đa phương thức thường được chia thành early fusion, mid-fusion và late-fusion, nhưng hầu hết chúng cần tất cả các phương thức có mặt để hoạt động. Với tính linh hoạt của con người làm cảm hứng, trong bài báo này chúng tôi giới thiệu Zorro, một kiến trúc Transformer đa phương thức có thể hoạt động trong cả môi trường đơn phương thức và đa phương thức. Tính chất này cải thiện hiệu suất tổng thể của mô hình đồng thời mở ra cánh cửa cho pre-training tự giám sát có sẵn.

Đổi mới kiến trúc chính của chúng tôi trong Zorro là tạo ra các luồng biểu diễn đơn phương thức và đa phương thức (fusion) riêng biệt trong một backbone Transformer tiêu chuẩn duy nhất. Chúng tôi đạt được điều này mà không cần thiết kế kiến trúc, mà thay vào đó bằng cách áp dụng các mặt nạ phù hợp trong tất cả các phép toán attention, dẫn đến một số đầu ra chỉ nắm bắt các phương thức riêng lẻ và một số đầu ra nắm bắt thông tin đa phương thức. Điều này mang lại lợi ích trực tiếp là mô hình có thể được áp dụng khi một tập con các phương thức bị thiếu, ví dụ: một mô hình được huấn luyện trên âm thanh và video có thể được đánh giá chỉ trên âm thanh.

Trong khi hầu hết sự nhấn mạnh của các phát triển mới trong không gian có giám sát được đặt vào kiến trúc, các đầu ra đơn phương thức có thể được khai thác thêm bằng cách giới thiệu các sơ đồ huấn luyện tự giám sát bổ sung. Trái ngược với các mô hình đa phương thức dựa trên attention gần đây [27,36] mà làm vướng víu cả hai phương thức trong toàn bộ mạng, Zorro hỗ trợ huấn luyện contrastive tự giám sát trong một mạng duy nhất mà không có sự sụp đổ biểu diễn, nhờ vào các đầu ra đơn phương thức của nó (xem Hình 1). Trong công việc này, chúng tôi khám phá khả năng này bằng cách pre-train mô hình của chúng tôi với một loss contrastive âm thanh-hình ảnh [3]. Khác với công việc trước đây, chúng tôi có thể thực hiện pre-training này mà không cần thiết các backbone riêng biệt cho mỗi phương thức.

Bài báo này trình bày bốn đóng góp: (a) chúng tôi giới thiệu Zorro, một tập hợp mới các kiến trúc đa phương thức dựa trên Transformer cho phép cả huấn luyện có giám sát và tự giám sát và, một khi được huấn luyện, có thể được sử dụng cho đầu vào đa phương thức hoặc đơn phương thức; (b) chúng tôi giới thiệu ba kiến trúc dựa trên Zorro sử dụng các mô hình tối ưu như ViT, SWIN và HiP; (c) chúng tôi chỉ ra rằng Zorro có thể được pre-train trên một tập dữ liệu âm thanh-hình ảnh quy mô lớn theo cách tự giám sát, và cũng có thể được pre-train trên các tập dữ liệu đơn phương thức; và (d) chúng tôi benchmark các mô hình thu được trên AudioSet, VGGSounds, Kinetics-400 và ESC-50. Mô hình đạt được hiệu suất tối ưu khi so sánh với các kỹ thuật học tự giám sát trước đây trên hầu hết các benchmark liên quan, đồng thời cũng đạt được hiệu suất tương đương với công việc trước đây cho huấn luyện có giám sát với nhãn.

## 2. Công việc liên quan

**Nhận thức đa phương thức**: Nhận thức đa phương thức là thách thức vì dữ liệu từ các phương thức khác nhau có thể có các cấu trúc liên kết, tần số thời gian và tầm quan trọng tương đối khác nhau phụ thuộc vào từng tác vụ [9]. Với sự xuất hiện của mạng nơ-ron tích chập, nhiều công trình đã fusion các kích hoạt từ các tensor trung gian [7,14,19,21,47,52,53], nhưng điều này đòi hỏi kỹ thuật đáng kể, vì các phương thức khác nhau có dạng lưới đặc trưng khác nhau và có nhiều cách khác nhau để kết hợp chúng.

**Học tự giám sát âm thanh-hình ảnh**: Các phương pháp khác nhau đã được sử dụng để sử dụng độ tương tự giữa các phương thức như một tín hiệu tự giám sát [4,6,7,30,35,37,39,43]. Hầu hết các phương pháp dựa trên các backbone đơn phương thức tạo ra các biểu diễn được sử dụng trong loss tự giám sát [3,4,39,41]. Các kỹ thuật này xử lý các phương thức khác nhau với các tập trọng số khác nhau và hạn chế khả năng suy luận qua các phương thức. Ít phổ biến hơn là các phương pháp học các mô hình tự giám sát với nhiều phương thức cùng một lúc. Một công việc gần đây theo hướng này là [46], học các biểu diễn sử dụng âm thanh, video và văn bản. Tuy nhiên, để tránh sự sụp đổ của loss tự giám sát, họ cung cấp các phương thức từng cặp một, tăng số lượng forward pass cần thiết. Thay vào đó, masking Zorro có thể tạo ra các đầu ra đơn phương thức mà không cần chạy mô hình nhiều lần.

**Kiến trúc Transformer**: Lấy cảm hứng từ ViT [18], các công việc tiếp theo đã đề xuất xử lý đơn phương thức cho video [8] và âm thanh [24] sử dụng mã hóa dựa trên patch. Các phương pháp dựa trên Transformer cũng đã được đề xuất để giải quyết phân loại âm thanh-hình ảnh. Gần nhất với phương pháp của chúng tôi là MBT [36], xây dựng một kiến trúc đa phương thức từ các Transformer đơn phương thức cho video [8,18] và âm thanh [24]. MBT kết hợp các phương thức bằng cách tạo ra một attention bottleneck hạn chế giao tiếp giữa các head âm thanh và hình ảnh. Phương pháp của chúng tôi cũng điều chỉnh giao tiếp giữa các phương thức, nhưng bằng cách masking các kết nối tiềm ẩn, chúng tôi có thể thu được các head đặc biệt theo phương thức trong khi ở MBT biểu diễn hoàn toàn là đa phương thức. Một công việc liên quan khác là VATT [2], một kiến trúc dựa trên Transformer để mô hình hóa video, âm thanh và văn bản với một backbone duy nhất. Khác với công việc của chúng tôi, trong VATT mỗi phương thức được xử lý độc lập bởi transformer. Cuối cùng, kiến trúc Perceiver [27] mở rộng đến một số lượng lớn đầu vào bằng cách cross-attend đến một tập các truy vấn tiềm ẩn. Trong công việc này, chúng tôi sử dụng Hierarchical Perceiver tiếp theo [13] chia đầu vào và đầu ra thành các nhóm để cải thiện hiệu quả mô hình.

**Masking attention trong Transformers**: Kiến trúc transformer gốc [50] sử dụng attention-masking cho mô hình hóa ngôn ngữ. Sau thành công của các kiến trúc dựa trên hình ảnh, các phương án khác đã được đề xuất để sử dụng attention masking để giảm bớt các yêu cầu tính toán của kiến trúc. Swin [34] đề xuất sử dụng các cửa sổ cục bộ, hạn chế các lớp self-attention chỉ đến các pixel lân cận. Hơn nữa, mask2former [16], cũng hạn chế cross-attention đến các vùng cục bộ, cho phép sử dụng transformers cho đầu ra có chiều cao (ví dụ: phân đoạn).

## 3. Zorro: Transformer đa phương thức có mặt nạ

Trong bài báo này, chúng tôi giới thiệu Zorro, một kiến trúc đa phương thức cho phép cả huấn luyện có giám sát và tự giám sát. Trong phần này, chúng tôi giải thích cách Zorro thực hiện điều này bằng cách sử dụng masking nhận biết phương thức và bằng cách tái sử dụng các thành phần transformer gốc để cho phép học contrastive giữa các phương thức. Đổi mới chính của kiến trúc là giới thiệu các phân bổ tiềm ẩn riêng biệt cho các phương thức khác nhau, dẫn đến một biểu diễn cuối cùng một phần là đơn phương thức (một phần của biểu diễn chỉ thấy một phương thức duy nhất) và một phần là đa phương thức (một phần của biểu diễn có thể attend đến tất cả các phương thức). Đầu tiên, chúng tôi sẽ mô tả Zorro được áp dụng cho kiến trúc ViT. Thứ hai, chúng tôi mở rộng Zorro cho hai kiến trúc transformer tối ưu khác, Swin và HiP. Cuối cùng, chúng tôi kết thúc phần này bằng cách mô tả cách sử dụng Zorro cho học contrastive tự giám sát.

### 3.1. Kiến trúc

**Tổng quan Zorro-ViT**: Hình 2 mô tả kiến trúc Zorro, bao gồm ba khối chính. Đầu tiên, Zorro xử lý dữ liệu dưới dạng các patch (tương tự như ViT [18]). Trong giai đoạn này, dữ liệu từ mỗi phương thức đầu tiên được chuyển đổi thành một mảng 2D các biểu diễn. Điều này có thể được thực hiện bằng cách (i) chia tensor đầu vào thành các nhóm tuần tự (điểm hoặc patch) và áp dụng một phép chiếu tuyến tính, hoặc (ii) áp dụng xử lý đặc biệt theo miền như tích chập 1D/2D/3D và làm phẳng. Chúng tôi sử dụng tích chập 2D để trích xuất các patch 16×16 và chiếu chúng đến chiều đầu vào D. Tiếp theo, các embedding vị trí được thêm vào các vector đã chiếu để mô hình có thể định vị và phân biệt từng patch đã nhúng. Các vector fusion đa phương thức đã học sau đó được giới thiệu. Thứ hai, các token kết quả được nối để tạo thành một tập duy nhất và sau đó được xử lý bởi L lớp của một Transformer [50] với masking Zorro. Cuối cùng, để tạo ra đầu ra cuối cùng, chúng tôi học một tập các truy vấn cross-attend đến đầu ra của lớp self-attention cuối cùng tương tự như PerceiverIO [26]. Chúng tôi sử dụng phép toán cross-attention tiêu chuẩn [26], và tạo ra 4 đầu ra khác nhau: một đầu ra âm thanh, một đầu ra video, một đầu ra fusion (chỉ thấy phần đa phương thức của biểu diễn) và một đầu ra toàn cục thấy toàn bộ biểu diễn. Ba bước này được mô tả chi tiết hơn tiếp theo.

**Tiền xử lý đầu vào**: Gọi x = (xv; xa) là một mẫu video bao gồm các khung hình xv ∈ R^(Nf×H×W×3) và spectrogram âm thanh xa ∈ R^(T×Ns) trong đó Nf là số khung hình, Ns là chiều của spectrogram, H là chiều cao của khung hình, W là chiều rộng của khung hình và T là số bước thời gian trong spectrogram. Để giảm kích thước đầu vào, chúng tôi sử dụng tích chập 2D fpatch để thu được u = (uv; ua) = (f^pre_v(xv); f^pre_a(xa)). Các mảng (uv; ua) sau đó được làm phẳng và mã hóa vị trí tuyệt đối đã học được thêm vào. Cuối cùng, chúng tôi học một tập gồm nfusion vector tiềm ẩn được nối với các token đầu vào âm thanh và video.

**Masked attention**: Đóng góp chính của bài báo này là chia biểu diễn Transformer thành các nhóm chuyên biệt. Sử dụng masked attention, chúng tôi buộc một phần của biểu diễn chỉ attend đến chính nó, trong khi các phần khác có thể attend đến toàn bộ biểu diễn. Mục tiêu chính của phương pháp này là chia biểu diễn thành ba phần: một phần chỉ tập trung vào các token video, một phần tập trung vào các token âm thanh, và các vector còn lại có thể attend đến toàn bộ biểu diễn.

Chúng tôi mask hai phần của mô hình: self-attention [50] và decoding cross-attention [26]. Cả hai phần đều bao gồm cùng một phép toán cơ bản nhận các key k, value v và query q để tạo ra đầu ra cuối cùng o. Để làm điều này, chúng tôi giới thiệu một tensor nhị phân masking m chỉ định các vector nào được kết nối với nhau. Các mục của ma trận masking là mij = 1 nếu thông tin có thể chảy từ tiềm ẩn i đến tiềm ẩn j. Bằng cách đặt mij = 0, chúng tôi cho mô hình biết rằng kết nối này nên được bỏ qua. Mặt nạ này được áp dụng cho phép toán đầu ra attention tiêu chuẩn oi = Σj aij vj trở thành oi = Σj âij vj trong đó:

âij = (mij exp(qi^T kj / √D)) / (Σ{j'|mij'=1} exp(qi^T kj' / √D))  (1)

Trái ngược với MBT [36], biểu diễn đặc biệt theo phương thức của chúng tôi không có quyền truy cập vào biểu diễn toàn cục, điều này ngăn chặn luồng thông tin giữa các phương thức. Cụ thể, chúng tôi đặt mij = 1 nếu j là một phần của biểu diễn fusion, nếu không chúng tôi chỉ đặt mij = 1 nếu i và j là các vector của cùng một phương thức. Bằng cách làm này, chúng tôi ngăn chặn rõ ràng thông tin từ luồng fusion rò rỉ vào biểu diễn đơn phương thức. Đây là chìa khóa để bảo tồn các luồng thuần khiết tương ứng với các phương thức đơn lẻ.

**Không gian đầu ra**: Trong kiến trúc ViT, một token CLS có thể học được được sử dụng để tạo ra vector embedding đầu ra. Thay vào đó, lấy cảm hứng từ PerceiverIO [26], chúng tôi học một tập các vector decoding được sử dụng để truy vấn đầu ra từ Transformer để tạo ra đầu ra cuối cùng. Mỗi vector decoding cross attend đến một tập con các token để tạo ra vector đầu ra cuối cùng. Chiến lược decoding này có thể được sử dụng để tạo ra nhiều đầu ra tùy ý, mở ra khả năng cho các tác vụ dày đặc như phân đoạn hoặc ước lượng luồng.

Vì chúng tôi dựa vào việc có biểu diễn Transformer được chia thành các nhóm chuyên biệt, chúng tôi cần áp dụng masking của Zorro cho output cross attention. Cụ thể, chúng tôi thấy có lợi khi định nghĩa bốn đầu ra cho mô hình của chúng tôi. Đầu ra đặc biệt âm thanh oA, chỉ chứa thông tin đến từ đầu vào âm thanh. Đầu ra đặc biệt video ov, chỉ bao gồm thông tin từ phương thức video. Đầu ra đặc biệt fusion oF, được tính bằng cách chỉ attend đến luồng fusion. Và cuối cùng, một đầu ra toàn cục oG, attend đến tất cả các đầu ra trong mô hình. Mặc dù oG và oF có chứa thông tin tương tự, chúng tôi thấy vẫn hữu ích khi giữ hai head khác nhau.

### 3.2. Mở rộng Zorro cho các kiến trúc khác

Trong phần này, chúng tôi đề xuất các biến thể của Zorro cho hai kiến trúc dựa trên attention tối ưu, Swin và HiP. Khác với việc triển khai ViT, khi xây dựng Zorro-Swin và Zorro-HiP, chúng tôi sử dụng khối xây dựng kiến trúc cụ thể cho mỗi phương thức và luồng fusion trong khi chúng tôi kết hợp các phương thức với một phép toán cross-attention. Điều này là cần thiết vì masking ViT không thể áp dụng trực tiếp cho Swin và HiP, nhưng ý tưởng tổng thể vẫn giữ nguyên.

**Zorro-Swin**: Swin [34] là một kiến trúc transformer lấy cảm hứng từ ViT đã cho thấy hiệu quả và hiệu suất được cải thiện. Đổi mới chính so với kiến trúc ViT gốc là áp dụng các phép toán self-attention trên các token gần đó thay vì tất cả các token trong hình ảnh đầu vào. Điều này giảm yêu cầu tính toán trong khi cho phép mô hình thực hiện suy luận từ dưới lên. Để xây dựng Zorro-Swin, sửa đổi chính của chúng tôi đối với kiến trúc gốc là xử lý các phương thức riêng lẻ bằng các transformer Swin. Ở cuối mỗi khối Swin, chúng tôi cập nhật biểu diễn fusion bằng cách cross-attend đến cả biểu diễn đơn phương thức và đa phương thức. Để xử lý biểu diễn fusion, chúng tôi sử dụng cùng self-attention như trong Zorro-ViT. Với thiết kế này, chúng tôi tự do sử dụng các kiến trúc khác nhau để xử lý từng phương thức. Chúng tôi sử dụng Swin 2D gốc [34] để xử lý các spectrogram âm thanh trong khi chúng tôi điều chỉnh kiến trúc Swin cho video. Tương tự như Zorro-ViT, không có thông tin đa phương thức nào chảy vào các luồng đơn phương thức. Mô tả chi tiết về Zorro-Swin có thể được tìm thấy trong Phần A trong Phụ lục.

**Zorro-HiP**: Perceiver phân cấp [13] mở rộng các mô hình Perceiver được giới thiệu trước đây [26,27], bằng cách chia đầu vào thành các nhóm và chỉ hoạt động trong các nhóm đó. Thông qua kiến trúc phân cấp, các nhóm đó fusion với nhau để tổng hợp thông tin và suy luận toàn cục về đầu vào. Trong việc triển khai HiP của chúng tôi, thay vì sử dụng trực tiếp các pixel và tín hiệu âm thanh làm đầu vào, chúng tôi tạo các patch tương tự như việc triển khai ViT/Swin. Để tạo Zorro-HiP, chúng tôi sử dụng các khối xây dựng HiP cho mỗi phương thức. Cụ thể, các khối đó nhóm các đầu vào thành các tập nhỏ hơn, cross-attend sử dụng các đặc trưng đã học và cuối cùng áp dụng các lớp self-attention cho các đầu ra của phép toán cross attention (xem [13] để biết thêm chi tiết). Để cập nhật biểu diễn fusion, chúng tôi học một tập các truy vấn cross attend đến cả biểu diễn đơn phương thức và đa phương thức cho mỗi lớp. Thêm chi tiết có thể được tìm thấy trong Phần A trong Phụ lục.

### 3.3. Học contrastive với Zorro

Các phương pháp contrastive âm thanh-hình ảnh học các biểu diễn bằng cách căn chỉnh âm thanh và video vào một không gian embedding chung. Trái ngược với các phương pháp đơn phương thức, thay vì tạo ra nhiều view của dữ liệu, chúng sử dụng các phương thức khác nhau làm view. Một yêu cầu quan trọng là hai backbone không được chia sẻ thông tin. Nếu thông tin được chia sẻ giữa các phương thức, việc huấn luyện tự giám sát có thể dễ dàng sụp đổ hoặc hội tụ đến một giải pháp tầm thường.

Các mô hình cho nhận thức đa phương thức thường tạo ra một đầu ra duy nhất cho nhiều đầu vào. Điều này đủ cho các ứng dụng có giám sát, nhưng ngăn chặn việc sử dụng các kỹ thuật contrastive âm thanh-hình ảnh này. Chúng tôi thiết kế Zorro để xử lý các đầu ra đơn phương thức và đa phương thức, với ý định cho phép sử dụng các loss contrastive tự giám sát.

**Noise Contrastive Estimation**: Để huấn luyện với loss noise-contrastive estimation tiêu chuẩn, chúng tôi làm theo việc triển khai loss âm thanh-hình ảnh từ [3]. Cho đầu ra âm thanh oa và đầu ra video ov, chúng tôi áp dụng một phép chiếu tuyến tính cuối cùng (khác nhau cho mỗi phương thức) ga và gv để thu được các vector embedding cuối cùng: za = ga(oa) và zv = gv(ov). Chúng tôi tính độ tương tự giữa za và zv bằng cách lấy tích vô hướng chuẩn hóa và chia cho một tham số nhiệt độ, sim(za; zv) = exp(ẑa · ẑv / τ). Cuối cùng chúng tôi áp dụng loss NCE:

LNCE(za; zv) = Σi log(sim(zi_a; zi_v) / Σj,k sim(zka; zjv))  (2)

Phương trình 2 mô tả loss cho huấn luyện contrastive âm thanh-hình ảnh. Tuy nhiên, kỹ thuật này không huấn luyện bất kỳ tham số nào đặc biệt cho biểu diễn hoặc đầu ra fusion (ví dụ: cross-attention fusion hoặc trọng số fusion nếu mô hình có trọng số riêng biệt cho mỗi phương thức). Để tự giám sát đầu ra của luồng fusion, chúng tôi thêm một loss contrastive fusion-visual và fusion-audio. Chúng tôi định nghĩa một loss tự giám sát tương phản cả hai biểu diễn đơn phương thức (âm thanh và video) riêng biệt với đa phương thức (fusion). Với những thay đổi đó, loss mới là:

LNCE = LNCE(za; zv) + LNCE(za; zf) + LNCE(zI; zf)  (3)

## 4. Thí nghiệm

Trong phần này, chúng tôi đánh giá kiến trúc Zorro trên nhiều cài đặt. Chúng tôi đầu tiên trình bày chi tiết về các quy trình huấn luyện và đánh giá, cũng như các tập dữ liệu chính mà chúng tôi sử dụng. Chúng tôi đánh giá phương pháp so với các mô hình tối ưu trên ba benchmark âm thanh-hình ảnh tiêu chuẩn (AudioSet [22], VGGSound [15] và Kinetics-400 [14]), một benchmark thị giác (Kinetics-400 [14]) và một benchmark âm thanh (ESC-50 [40]). Cuối cùng, chúng tôi ablate các quyết định thiết kế chính đã thúc đẩy nghiên cứu của chúng tôi và giới thiệu tính linh hoạt của Zorro. Cụ thể, chúng tôi so sánh các kiến trúc khác nhau, nghiên cứu hiệu ứng của các phương thức bị thiếu, pre-train Zorro với dữ liệu đơn phương thức và khám phá các chiến lược attention-masking thay thế.

### 4.1. Chi tiết thí nghiệm

Để giới thiệu khả năng suy luận của Zorro qua các phương thức khác nhau, chúng tôi pre-train nó bằng tự giám sát cũng như với giám sát tiêu chuẩn sử dụng nhãn lớp. Trong phần này, chúng tôi cung cấp các chi tiết quan trọng nhất của quy trình huấn luyện. Chi tiết bổ sung về đầu vào, kiến trúc và huấn luyện có thể được tìm thấy trong Phần A và B trong Phụ lục.

**Tập dữ liệu pre-training**: Chúng tôi sử dụng bốn tập dữ liệu để pre-training: AudioSet [22], YouTube-8M, ACAV-100M [33] và ImageNet-21k [42]. AudioSet bao gồm 1.9M video chứa 527 lớp âm thanh được chú thích. Vì tập dữ liệu rất mất cân bằng, [36] đã đề xuất một biến thể nhỏ hơn cân bằng hơn của tập huấn luyện với 500k ví dụ. Đối với các thí nghiệm ablation và huấn luyện từ đầu, chúng tôi sử dụng phiên bản 1.9M trong khi để fine-tuning chúng tôi cũng sử dụng AudioSet-500k để so sánh công bằng với tối ưu. YouTube-8M [1] bao gồm 8M video với các khung hình âm thanh và hình ảnh, được chú thích theo cách multi-label với 3862 lớp khác nhau. Video đại diện cho nhiều hoạt động, tạo ra một phân phối dữ liệu rất tự nhiên. ACAV-100M bao gồm 100M video với các khung hình âm thanh và hình ảnh mà không có nhãn liên quan, đã được tuyển chọn để chứa một tương quan âm thanh-hình ảnh mạnh mẽ. Chúng tôi sử dụng 59M trong số những video đó để học tự giám sát. ImageNet-21k bao gồm 13M hình ảnh được chú thích trên 21k lớp, và thường được sử dụng để pre-training quy mô lớn các mô hình transformer thị giác [18].

**Benchmark đánh giá âm thanh-hình ảnh**: Để đánh giá khả năng của Zorro học và chuyển giao các biểu diễn đa phương thức, chúng tôi đánh giá trên các benchmark âm thanh-hình ảnh tiêu chuẩn. Cụ thể, chúng tôi đánh giá Zorro trong AudioSet, VGGSound [15] và Kinetics-400 [28]. VGGSound bao gồm 163,603 mẫu huấn luyện và 13579 mẫu kiểm tra được rút ra từ các video YouTube 10 giây trải rộng 309 lớp đơn nhãn, loại trừ lẫn nhau. Nó tập trung vào đánh giá âm thanh thực tế với tương ứng âm thanh-hình ảnh nơi âm thanh rõ ràng trong video. Kinetics-400 bao gồm 201K video huấn luyện các hành động hàng ngày được phân loại thành 400 lớp duy nhất. Trong khi một số tập dữ liệu có thiên lệch trong phương thức âm thanh hoặc video, Zorro có thể học mức độ dựa vào từng phương thức.

**Benchmark đánh giá đơn phương thức**: Zorro có thể được huấn luyện trên dữ liệu đa phương thức nhưng được đánh giá trên dữ liệu đơn phương thức. Để tiếp tục chỉ ra điều này, chúng tôi đánh giá các mô hình Zorro được huấn luyện đa phương thức trên các tác vụ fine-tuning đơn phương thức: Kinetics-400 cho thị giác và ESC-50 cho âm thanh. Tập dữ liệu ESC-50 chứa 2k clip được phân loại thành 50 lớp duy nhất.

**Đầu vào Zorro**: Đầu vào cho mô hình của chúng tôi là video và âm thanh. Âm thanh và video được đồng bộ và bao phủ cùng một khoảng thời gian. Video bao gồm 8 khung hình có kích thước 224×224. Khi huấn luyện trong AudioSet, chúng tôi lấy mẫu video ở 3.12FPS dẫn đến 2.56s âm thanh và video. FPS cụ thể cho mỗi mô hình và độ dài âm thanh cho pre-training và fine-tuning được báo cáo trong Phần B trong Phụ lục. Trong quá trình huấn luyện, chúng tôi sử dụng cắt ngẫu nhiên cũng như tăng cường màu sắc trong các khung hình. Đối với ESC-50, chúng tôi khớp độ dài của mô hình pre-trained, lặp lại chuỗi âm thanh nếu cần thiết. Âm thanh được lấy mẫu ở 48kHz, chuyển đổi thành spectrogram làm đầu vào cho mô hình của chúng tôi sử dụng 128 bin. Để tăng cường âm thanh trong huấn luyện, chúng tôi sử dụng SpecAugment [38] và jittering tần số. Trong quá trình đánh giá, chúng tôi subsample video và âm thanh đầu vào thành nhiều clip cách đều nhau và trung bình hóa dự đoán của chúng.

**Chi tiết kiến trúc**: Zorro dựa trên các kiến trúc transformer đơn phương thức (ViT, Swin và HiP), được điều chỉnh cho xử lý đa phương thức (tương tự như [36]). Trong tất cả các thí nghiệm của chúng tôi, chúng tôi sử dụng ViT-B/16. Để biết chi tiết về kiến trúc ViT, Swin và HiP, xem Phần A trong Phụ lục.

**Chi tiết huấn luyện**: Chúng tôi sử dụng optimizer Adam với lịch trình learning rate cosine decay, weight decay và learning rate warmup. Khi fine-tuning, đối với Zorro-ViT và Zorro-Swin, chúng tôi thấy tốt hơn khi sử dụng optimizer SGD và momentum 0.9. Chúng tôi huấn luyện tất cả các mô hình trong 50 epoch ngoại trừ các tập dữ liệu ACAV-100M nơi chúng tôi huấn luyện trong 10 epoch và các baseline input-level và bottleneck nơi chúng tôi huấn luyện trong 25 để ngăn chặn overfitting nghiêm trọng. Chúng tôi thấy tốt nhất là sử dụng nfusion = 6 trong tất cả các mô hình. Đối với fine-tuning AudioSet, chúng tôi sử dụng mixup (α = 0.3) và label smoothing. Chúng tôi sử dụng cross-entropy loss cho các tập dữ liệu uni-label và binary sigmoid cross-entropy cho multi-label. Chúng tôi huấn luyện một classifier cho mỗi trong 4 đầu ra của mô hình và trung bình hóa dự đoán của nó. Đối với huấn luyện contrastive, chúng tôi làm theo quy trình được nêu trong Phần 3.3.

### 4.2. So sánh tối ưu

Tiếp theo, chúng tôi đánh giá Zorro so với các phương pháp tối ưu. Chúng tôi đánh giá Zorro được huấn luyện âm thanh-hình ảnh của chúng tôi trên các benchmark cho phân loại âm thanh-hình ảnh, phân loại video và phân loại âm thanh, giới thiệu tính phổ quát của phương pháp.

**Huấn luyện AudioSet-2M từ đầu**: Đầu tiên, chúng tôi đánh giá Zorro khi được huấn luyện từ đầu trên Audioset-2M sử dụng cả phương thức âm thanh và hình ảnh. Bảng 1 báo cáo rằng Zorro khớp hoặc vượt trội so với các phương pháp khác được huấn luyện trực tiếp trên AudioSet-2M từ đầu. Lưu ý rằng PlayItBack [48] không được liệt kê trong Bảng 1 vì nó được huấn luyện với AudioSet-500k. Cài đặt này cho thấy khả năng của mô hình thích ứng với đầu vào đa phương thức mà không cần dữ liệu pre-trained.

**So sánh đa phương thức**: Chúng tôi huấn luyện và đánh giá các mô hình pre-trained của chúng tôi trên AudioSet-500k (xem [36] để biết chi tiết), VGGSound và Kinetics-400 nơi chúng tôi sử dụng cả đầu vào âm thanh và hình ảnh. Tương tự như [36], đối với Zorro-ViT, chúng tôi phân bổ các trọng số khác nhau cho các tiềm ẩn âm thanh, video và fusion. Chúng tôi thấy điều này hữu ích để cải thiện độ chính xác fine-tuning. Bảng 2 báo cáo hiệu suất của các mô hình của chúng tôi. Chúng tôi chia bảng thành hai phần khác nhau. Đầu tiên, chúng tôi báo cáo hiệu suất Zorro khi tự giám sát contrastive được sử dụng để pre-training (không có nhãn). Zorro cải thiện so với tất cả các công việc trước đây trên AudioSet và VGGSound. Trong AudioSet, mô hình hiệu suất tốt nhất của chúng tôi trong cài đặt đó chỉ cách 1.2% so với Zorro với pre-training có giám sát, điều này chứng minh khả năng của kỹ thuật pre-training tự giám sát để học các đặc trưng tổng quát. Trong VGGSound, Zorro hoạt động tương tự với tối ưu có giám sát khi pre-training chỉ với tự giám sát. Cuối cùng, đối với Kinetics-400, hiệu suất kết quả không xa so với các mô hình với pre-training có giám sát. Trong phần dưới của bảng, chúng tôi báo cáo hiệu suất của Zorro khi sử dụng pre-training có giám sát. Chúng tôi bao gồm hiệu suất của mô hình khi được khởi tạo với ViT pre-trained trên ImageNet-21k. Ngay cả không có pre-training đa phương thức, Zorro có thể hoạt động hơn 1% tốt hơn so với các mô hình SOTA hiện có trong AudioSet. Khi pre-trained trên YouTube-8M, Zorro cải thiện hiệu suất của nó như một kết quả của bản chất đa phương thức của pre-training. Hiệu suất cuối cùng trong AudioSet đại diện cho một cải thiện 1.9% so với tối ưu, MBT [36]. Hơn nữa, không giống như Zorro, MBT không thể thực hiện suy luận đơn phương thức khi được huấn luyện với dữ liệu đa phương thức. Lưu ý, chúng tôi chưa chứng minh ở đây, nhưng Zorro cũng có thể được huấn luyện bằng các phương pháp tự giám sát đơn phương thức như MAE [25] và DINO [12] riêng biệt trên các luồng âm thanh và hình ảnh. Chúng tôi thảo luận về huấn luyện đơn phương thức có giám sát dưới đây.

**So sánh Video**: Để giới thiệu hiệu suất của Zorro trong chế độ đơn phương thức, chúng tôi fine-tune các mô hình của chúng tôi (pre-trained trên âm thanh và video) trên tác vụ phân loại video cho Kinetics-400 chỉ sử dụng video. Bảng 2 báo cáo kết quả. Mục tiêu của chúng tôi không phải là chỉ ra hiệu suất tối ưu trong cài đặt này, vì chúng tôi nhận thức được các cải tiến được thực hiện trên các kiến trúc Transformer để giải quyết tác vụ đó [34,54,55]. Mục tiêu của chúng tôi là cung cấp một cơ chế hiệu quả để pre-training các kiến trúc đó nhằm cải thiện hiệu suất cuối cùng trên suy luận đơn phương thức và đa phương thức. Khi Zorro được pre-trained bằng loss contrastive và fine-tuned trên Kinetics-400 (chỉ video), Zorro-ViT hoạt động chỉ tệ hơn 2.4% so với khi sử dụng đầu vào âm thanh-hình ảnh. Điều này cho thấy tính mạnh mẽ của mô hình của chúng tôi khi giảm xuống sử dụng một phương thức duy nhất. Hơn nữa, khi sử dụng mô hình Zorro pre-trained trên YT8M, mô hình của chúng tôi có thể hoạt động tương tự như các kiến trúc tương đương. Thay thế cho fine-tuning, chúng tôi cũng có thể sử dụng mô hình được huấn luyện âm thanh-hình ảnh (cột Audio+Video) và chỉ cung cấp video. Trong cài đặt đó, mô hình của chúng tôi được huấn luyện trên YouTube-8M hoạt động ở top-1 76.3, ngang bằng với kết quả fine-tuned chỉ video. Suy luận đơn phương thức này trên mô hình được huấn luyện đa phương thức không thể thực hiện với MBT, nơi cần huấn luyện lại.

**So sánh Âm thanh**: Để đánh giá khả năng âm thanh của Zorro, chúng tôi fine-tune các mô hình của chúng tôi trên ESC-50 (tập dữ liệu chỉ âm thanh) và báo cáo kết quả trong Bảng 2. Khi pre-trained trên YouTube-8M, Zorro hoạt động gần với AST, một transformer âm thanh chuyên biệt có kích thước tương đương. Khi sử dụng pre-training tự giám sát, Zorro cải thiện hiệu suất so với các phương pháp trước đây; Zorro-ViT có độ chính xác 93.6%, gần với các phương pháp có giám sát tối ưu.

### 4.3. So sánh kiến trúc

Trong phần này, chúng tôi thảo luận về các kiến trúc khác nhau được giới thiệu trong bài báo này. Trong Bảng 3, chúng tôi báo cáo so sánh cho các kiến trúc đó trong hai cài đặt: khi được huấn luyện từ đầu và khi pre-trained với loss contrastive âm thanh-hình ảnh tiếp theo bởi một lớp tuyến tính trên đầu, sử dụng Audioset-2M. Khi huấn luyện từ đầu, chúng tôi quan sát Zorro-Swin hoạt động tốt nhất qua các mô hình khác nhau, cả trong chế độ có giám sát và contrastive. Mặc dù số lượng tham số lớn hơn ViT, Swin huấn luyện nhanh hơn 25% so với ViT. HiP là nhanh nhất trong ba, trong khi không mất nhiều về độ chính xác. Xem Phần A trong Phụ lục để so sánh tốc độ mô hình. Hơn nữa, trong Bảng 2, chúng tôi cũng trình bày kết quả fine-tuning các kiến trúc này sau pre-training contrastive. Quan trọng là lưu ý rằng đối với ViT, trong bảng này chúng tôi sử dụng một tập tham số cho mỗi phương thức, làm tăng đáng kể số lượng tham số (98M đến 267M). Trong chế độ này, chúng tôi quan sát ViT là tốt nhất. Tuy nhiên, Swin và HiP nhanh hơn và giữ lại hầu hết hiệu suất.

### 4.4. Tính linh hoạt của mô hình Zorro

**Suy luận đơn phương thức với backbone đa phương thức**: Ở đây chúng tôi nghiên cứu khả năng của Zorro được huấn luyện âm thanh-hình ảnh tạo ra các đầu ra đơn phương thức có ý nghĩa khi được cung cấp dữ liệu đơn phương thức. Để đạt được điều này, chúng tôi zero out phương thức bị thiếu và chỉ cung cấp đầu vào hữu ích cho một phương thức, âm thanh hoặc video. Kết quả được báo cáo trong Bảng 3. Các mô hình không có đầu ra đơn phương thức chịu thiệt hại đáng kể từ một phương thức bị thiếu. Ngược lại, cả Zorro và sử dụng hai luồng phương thức riêng biệt đều đạt được hiệu suất cao khi chỉ có một phương thức được cung cấp. Điều này là do trong những mô hình đó, một số khả năng được phân bổ cho từng phương thức cụ thể và mô hình có thể tạo ra các đầu ra đơn phương thức.

**Pre-training đơn phương thức cho fine-tuning đa phương thức**: Trong bài báo, chúng tôi giả định có sẵn tập dữ liệu đa phương thức lớn để huấn luyện. Tuy nhiên, trong một số tình huống chúng tôi chỉ có sẵn lượng lớn các mẫu đơn phương thức (ví dụ: video hoặc âm thanh) và một tập nhỏ dữ liệu đa phương thức. Để giới thiệu tính linh hoạt của đề xuất của chúng tôi, chúng tôi chạy một thí nghiệm duy nhất nơi chúng tôi huấn luyện với hai tập dữ liệu đơn phương thức và fine-tune trên một tập dữ liệu đa phương thức nhỏ hơn. Chúng tôi chỉ sử dụng tín hiệu âm thanh từ tập dữ liệu AudioSet và các video từ tập dữ liệu Kinetics-400. Khi huấn luyện, chúng tôi trộn các batch với xác suất 0.5 cho mỗi tập dữ liệu, và không tính loss cho các phương thức bị thiếu. Để đánh giá, chúng tôi fine-tune mô hình thu được trên VGGSound và so sánh kết quả của nó với mô hình được huấn luyện từ đầu. Mô hình fine-tuned hoạt động ở độ chính xác top-1 59.2 trong khi mô hình được huấn luyện từ đầu hoạt động ở 54.4. Thí nghiệm này cho thấy tính linh hoạt của mô hình Zorro để thích ứng với huấn luyện đơn phương thức trong khi cung cấp khởi tạo hữu ích cho fine-tuning đa phương thức.

### 4.5. Cấu hình masking

Trong ablation này, chúng tôi nghiên cứu bốn loại attention masking khác nhau. Đầu tiên, chúng tôi đánh giá có luồng độc lập dữ liệu (two streams), nơi cả hai mô hình chia sẻ trọng số nhưng các phương thức không được kết nối. Thứ hai, chúng tôi đánh giá fusion input level, bao gồm không masking trong mô hình. Điều này giảm mô hình thành một ViT vanilla được áp dụng cho hai phương thức được nối. Lấy cảm hứng từ [36], chúng tôi cũng đánh giá bottleneck masking nơi các token fusion có thể attend đến các token của mỗi phương thức nhưng mỗi phương thức cũng có thể attend đến các token fusion. Chúng tôi muốn làm rõ rằng mặc dù phương pháp này sử dụng đề xuất chính từ MBT, nó không phải là một tái tạo công việc của họ. Cấu hình này buộc mỗi luồng chủ yếu tập trung vào một phương thức, nhưng thông tin có thể chảy qua các phương thức thông qua các vector fusion. Cuối cùng, chúng tôi so sánh tất cả các chiến lược masking đó với masking Zorro của chúng tôi. Đối với mỗi cấu hình masking, chúng tôi huấn luyện một mô hình theo cách có giám sát (giữ cùng số đầu ra để công bằng, ngoại trừ Two Streams có hai đầu ra). Chúng tôi cũng huấn luyện mô hình theo cách tự giám sát, nơi các đầu ra âm thanh và video được sử dụng để tính loss contrastive. Để báo cáo hiệu suất, chúng tôi huấn luyện một classifier tuyến tính trên đầu các biểu diễn contrastive.

Bảng 3 báo cáo kết quả. Chúng tôi rút ra hai kết luận chính. Đầu tiên, có các luồng độc lập phương thức là quan trọng cho huấn luyện tự giám sát. Cả cấu hình input-level và bottleneck đều ngay lập tức sụp đổ vì thông tin có thể chảy từ phương thức này sang phương thức khác. Hiệu suất cho Zorro và two streams rất tương tự vì Zorro khi được huấn luyện theo cách tự giám sát giảm xuống kiến trúc two stream. Thứ hai, chúng tôi thấy rằng có các luồng phương thức riêng biệt cũng hữu ích cho học có giám sát. Đặc biệt thú vị là nhìn vào hiệu suất của input-level, bottleneck và Zorro, nơi Zorro hoạt động tốt hơn vì các luồng phương thức được xử lý độc lập hơn. Chúng tôi tin rằng điều này là do khả năng của mô hình giữ thông tin đặc biệt theo phương thức thông qua mạng, có thể hữu ích ở các giai đoạn xử lý sau. Cuối cùng, đối với huấn luyện tự giám sát của Zorro, chúng tôi sử dụng phương trình 3, cũng huấn luyện đầu ra fusion. Mặc dù điều này tạo ra một giảm nhẹ về hiệu suất so với two streams, nó có lợi cho các tác vụ downstream. Thay thế, khi Zorro được huấn luyện chỉ sử dụng các đầu ra âm thanh và video, nó hoạt động giống như two streams (35.0 vs 34.8) vì hai mô hình tương đương.

## 5. Kết luận

Trong bài báo này, chúng tôi đã giới thiệu Zorro, một cấu hình masking Transformer mới cho phép huấn luyện và suy luận đơn phương thức và đa phương thức đồng thời, cũng như pre-training contrastive. Khác với các phương pháp tiếp cận trước đây đối với nhận thức đa phương thức, phương pháp đề xuất của chúng tôi có thể tạo ra cả đầu ra đơn phương thức và đa phương thức. Bằng cách chia luồng thông tin thành các luồng đơn phương thức và đa phương thức, chúng tôi có thể cải thiện hiệu suất khi kiến trúc được huấn luyện với loss có giám sát và cho thấy khả năng của mô hình được tự giám sát với loss contrastive. Chúng tôi đánh giá mô hình của chúng tôi trên các tác vụ đa phương thức, cho thấy tính linh hoạt tuyệt vời và hiệu suất tối ưu.
