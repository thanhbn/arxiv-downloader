# 2311.18765.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2311.18765.pdf
# File size: 4620724 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MLLMs-Augmented Visual-Language
Representation Learning
Yanqing Liu1,2*, Kai Wang1*†, Wenqi Shao2, Ping Luo2,3, Yu Qiao2,
Mike Zheng Shou1, Kaipeng Zhang2‡, and Yang You1‡
1National University of Singapore
2OpenGVLab, Shanghai AI Laboratory
3The University of Hong Kong
Abstract. Visual-language pre-training has achieved remarkable suc-
cess in many multi-modal tasks, largely attributed to the availability of
large-scale image-text datasets. In this work, we demonstrate that Multi-
modal Large Language Models (MLLMs) can enhance visual-language
representation learning by establishing richer image-text associations for
image-text datasets. Our approach is simple, utilizing MLLMs to extend
multiple diverse captions for each image. To prevent the bias introduced
by MLLMs’ hallucinations and monotonous language styles, we propose
“text shearing" to maintain the quality and availability of extended cap-
tions. In image-text retrieval, without introducing additional training
cost, our method consistently obtains 5.6 ∼35.0% and 16.8 ∼46.1%
improvement on Recall@1 under the fine-tuning and zero-shot settings,
respectively. Notably, we obtain zero-shot results that are comparable
to fine-tuning on target datasets, which encourages more exploration of
the versatile use of MLLMs. The datasets and codes are available at:
https://github.com/lyq312318224/MLLMs-Augmented.
Keywords: Visual-language pre-training ·Representation learning
1 Introduction
Visual-language pre-training has achieved remarkable success in image-text re-
trieval [25,26], image classification [49,66], vision question answering [3,32],
and image caption generation [31,66]. This success can be attributed to the
large-scale datasets collected from the Internet, such as CC3M [54], CC12M [8],
YFCC100M [58], LAION400M [52], etc. However, most of these datasets include
a non-negligible portion of noisy and mismatched image-text pairs [25,29,60],
whichlargelyaffectsthevisual-languagerepresentationlearning.Oneofthemost
straightforward approaches is: utilizing pre-trained models to recognize and re-
move mismatched pairs based on heuristic rules [1,7,17,38].
*equal contribution
†project lead
‡corresponding authorarXiv:2311.18765v3  [cs.CV]  13 Mar 2024

--- PAGE 2 ---
2 Y. Liu et al.
0 20 40 60 80
/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000052/uni00000049/uni00000003/uni00000055/uni00000048/uni00000050/uni00000052/uni00000059/uni0000004c/uni00000051/uni0000004a/uni0000000b/uni00000008/uni0000000c20.022.525.027.530.032.535.037.5/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni0000000b/uni00000008/uni0000000c
 /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000023/uni00000014
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000023/uni00000014
(a)Data Removing Ra-
tios vs. Performance
0 20 40 60 80 100
/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000003/uni00000052/uni00000049/uni00000003/uni00000055/uni00000048/uni0000005a/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000051/uni0000004a/uni0000000b/uni00000008/uni0000000c27.530.032.535.037.540.042.545.0/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni0000000b/uni00000008/uni0000000c
/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000023/uni00000014
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000023/uni00000014
(b)Data Rewriting Ra-
tios vs. Performance
Similar Text Structure
1.This image shows a young man 
standing in the street with two 
small dogs in his arms
2.The image shows a group of hippos standing on the edge of a 
river in the wild
3.The image depicts two men 
sitting on a couch in a tent
4.The image shows a woman 
standing by a window looking out 
into the distance
5.This image shows a basketball 
player jumping up to shoot the 
ball during a game
6.The image shows a field of sunflowers growing in a fieldSingle Focus
LLaVA: A man holding his hand 
up to the sky
LLaVA: A group of chickens in a 
cage
LLaVA: A soccer player in a 
yellow vest kicks the ball
MiniGPT -4(c)The analysis of captioning the image using MiniGPT-4
and LLaVA.
Fig. 1:The performance in (a) and (b) is zero-shot image-text retrieval results on
MSCOCO using pretrained BLIP. (c) shows that using a single model to generate
captionscaneasilyresultinsimilartextstructureandinsufficientglobalconsiderations.
These methods indeed reduce the influence of mismatched pairs. However,
simply removing mismatched pairs leads to a serious problem: the number of
training pairs is also reduced. As illustrated in Figure 1a, the performance of
image-textretrievalconsistentlydropsalotwhentheremovalratioislarge.Most
recently, several works [17,42,70] demonstrate that LLM and MLLM can be used
as re-writers to improve caption quality without reducing the number of training
pairs. Unfortunately, these re-writers inevitably introduce their caption styles,
i.e.text structure, which might disrupt the distribution of the original captions
and lead to difficulties in learning better visual-language representations.
To investigate the characteristics of previous rewriting works [17,42,70], we
conduct experiments to evaluate their influences in Figure 1b. Experimental
results empirically confirm that applying MLLMs to enhance the quality of
visual-language datasets is a promising approach, i.e.improving the original
performance largely. Nevertheless, excessive re-writing leads to non-trivial per-
formance drops. Therefore, it is valuable to figure out what led to these drops
and verify whether there exists bias from synthetic captions.
We explore the underlying reasons for the bias by analyzing the gener-
ated captions’ text structures, attention focus, and word frequency. We utilize
MiniGPT-4andLLaVAtocaptioneachimagefromCC3M,respectively.Herewe
use the same question {Describe the ⟨image ⟩in English:} to prompt the model
and set the same maximum number of generated tokens. As shown in Figure 1c
and Figure 3, we have the following observations. 1). MLLMs indeed have their
inherent text structures and focus. 2). Applying a single MLLM makes it hard
to provide comprehensive captions. 3). There are great diversities between the
word frequency statistics of different MLLMs and raw captions.

--- PAGE 3 ---
MLLMs-Augmented Visual-Language Representation Learning 3
Considering the limitations of a single model in capturing diverse image cap-
tions, we employ multiple MLLMs to enrich visual-language associations from
differentperspectives,therebyimprovingvisual-languagerepresentationlearning
duringpre-training.Toimprovethequalityandavailabilityofsyntheticcaptions,
the following specific designs are proposed. First, we crop the captions generated
by MLLMs to match the length of the original captions, which is called “text
shearing”. This not only reduces the repetitive occurrence of common words in
synthetic captions, alleviating the caption collapse problem identified in [60], but
also preserves the semantic concepts closest to the image, thereby mitigating the
impact of hallucinations typically appearing in the later part of the generated
text, as verified in [69]. Second, to obtain a set of comprehensive captions for
each image, we keep the raw and extended captions from MLLMs simultaneously
for standard visual-language pre-training.
Our approach exhibits the following characteristics: 1). It is compatible with
multiple visual-language pre-training frameworks like CLIP [49] and BLIP [32],
demonstrating significant performance improvements across various downstream
taskswithoutintroducingadditionaltrainingoverhead.Forexample,inMSCOCO
and Flickr30K’s zero-shot image-text retrieval, our method obtains 16.8 ∼46.1%
Recall@1 improvement. In zero-shot image classification, our method achieves an
average performance improvement of 13.4 on 15 common classification datasets
and 13.1 on ImageNet [14]. 2). For image-text retrieval, our zero-shot CLIP
outperforms the vanilla CLIP fine-tuned on the MSCOCO and Flickr30K re-
spectively by 9.9% and 23.5%. (The above results are all based on pre-training
on CC3M.) 3). When scaling to large datasets like CC12M and YFCC15M, our
method continues to deliver substantial performance improvements.
2 Related Work
2.1 Improving image-text datasets
Numerous studies [18,20,43] have emphasized the significance of high-quality
image-text datasets in influencing the transfer performance of visual-language
pre-training in downstream tasks. SemDeDup [1] enhances data efficiency and
out-of-distribution performance by identifying and removing semantically dupli-
cated data pairs at the embedding level. Cao et al. [7] introduced an approach
to improve dataset quality by removing samples that contain text in images. T-
MARS [38] masks the text region in the image and then filters out samples with
lowCLIP[49]similarity.Thesemethodsinevitablylosealotofvisualinformation
when filtering out samples, so some methods try to obtain higher-quality data by
rewriting captions. Santurkar et al. [51] underscored the importance of synthetic
captions and employed pre-trained language models to augment textual content.
Gadre et al. [20] introduced the DataComp benchmark for multi-modal datasets.
Fan et al. [17] leveraged the in-context learning capacity of large language mod-
els to rewrite captions, enriching the language structure while preserving core
semantics. Zhu et al. [70] employed ChatGPT [45] and BLIP-2 [31] interactively

--- PAGE 4 ---
4 Y. Liu et al.
Mini- GPT4 Otter Qwen- VL LLaVA -1.5Raw caption:   guitar in a living room
What does the guitar symbolize? Is it used for 
music or decoration?
A guitar sits infront of a windowThe image shows a living room with a couch and 
an acoustic guitar on the floor in front of it
A wooden guitar case sitting next to a wooden 
chair
Raw caption:   families relaxing on the shore
The beach and boats in the water at the shore
A beach scene with a green fence and a gateThe image shows a beach with boats parked on 
the shore
A beach with several umbrellas and boats in the 
water
Raw caption:  football player gets above 
the pack to punch the ball away
The soccer players are trying to block the ball .
A soccer player in a yellow jersey is being tackled 
by two other playersThe image shows several soccer players jumping 
up to head the ball in a game
A group of men fighting for a soccer ball
Raw caption: an electric hybrid bus in city centre
A double decker bus is parked in front of a gate .
A green and white double decker bus is parked in 
front of a fenceThe image shows a city bus driving down a street
A picture of a white double decker bus with 
graffiti on the front moving down the street
Fig. 2:The illustration of our method. Different MLLMs jointly construct one-to-many
image-text pairs with richer semantic associations.
to generate captions with rich visual information. Nguyen et al. [42] used BLIP-
2 [31] to rewrite captions for low-matching image-text pairs and combined origi-
nal and generated captions for training. Lai et al. [29] proposed enhancing visual
information in captions by fusing the original caption with captions generated
by LLaVA [36], leveraging large language models.
While approaches based on caption rewriting have yielded promising results,
relying on a single model for rewriting introduces the model’s inherent bias and
poses challenges in establishing a unified representation between vision and lan-
guage.Incontrast,ourmethodintroducesmoreaccurateanddiversedescriptions
for a single image while retaining rich visual information.
2.2 Multimodal large language models
Existing Multimodal Large Language Models (MLLMs) primarily rely on three
keytechnologies:MultimodalInstructionTuning(M-IT),MultimodalIn-Context
Learning (M-ICL), and Multimodal Chain-of-Thought (M-CoT) [65]. M-IT fa-
cilitates strong transfer performance by fine-tuning the model on datasets with
specific instruction formats. Notable models employing this technology include
LLaMA-Adapter [21,67], LLaVA [35,36], MiniGPT-4 [71], InstructBLIP [13],
Qwen-VL [5], and NExT-GPT [62]. M-ICL is a type of analogy learning from
a limited number of samples. Models like Flamingo [3], Otter [30], and Hug-
gingGPT [55] are developed using this approach. M-CoT necessitates models to
not only provide answers but also reasoning processes. Representative models
include Multimodal-CoT [68] and Visual ChatGPT [61]. These three technolo-
gies are not mutually exclusive, and many models effectively combine multiple
technologies. Since these MLLMs are often trained on the billion-level dataset,
they have extremely rich knowledge and excellent visual understanding and ex-
pression capabilities, which can be used to caption the images.

--- PAGE 5 ---
MLLMs-Augmented Visual-Language Representation Learning 5
3 Method
3.1 Preliminaries
In this section, we briefly introduce the preliminaries of visual-language pre-
training. CLIP [49] is a classic work that uses image-text pairs for contrastive
learning.Itemploysanencoder-onlyarchitecture,optimizingtheencoderthrough
acontrastivelossonimage-textpairs.Duringtraining,whenabatchof Nimage-
text pairs { xI,xT} is sampled, the contrastive loss for an image can be defined
as follows:
LI= clip_contrast( xI, xT) (1)
Where clip_contrast is image-text contrastive loss introduced in [49]. The loss
for text is computed in the same manner and can be denoted as LT. The total
loss for training is L= (LI+LT)/2.
BLIP [32] is an encoder-decoder based visual-language pre-training architec-
ture. Its three main pre-training objectives (image-text contrastive loss (ITC),
image-text matching loss (ITM), and language modeling loss (LM)) are jointly
optimizedduringthetrainingprocess.Specifically,ITCissimilartothatinCLIP.
ITM aims to capture the fine-grained alignment between vision and language.
LM is used to optimize the decoder.
As highlighted in many works [33,41,66], visual-language pre-training heavily
depends on extensive image-text datasets. The presence of low-quality image-
text data can significantly compromise the model’s performance. Therefore, im-
proving the multi-modal dataset has become an expected direction.
3.2 Overview
In this work, we employ multiple advanced MLLMs to augment the visual-
language representation learning from a data-centric perspective. We demon-
strate that different MLLMs can generate accurate and diverse captions in Fig-
ure 2. The goal of this approach is to utilize the MLLMs to establish richer
image-text associations in the current datasets. Our approach consists of two
processes: multi-view caption extractor and text shearing. Specifically, for each
image from an image-text dataset, we first introduce multiple advanced MLLMs
to synthesize extended captions. Then, by comprehensively analyzing the char-
acteristics of these extended captions, we have a key observation: MLLMs have
their caption styles and might generate hallucinated content. Based on this ob-
servation, we propose text shearing to maintain the quality and availability of
extended captions. After these processes, the raw and extended captions with
corresponding images are jointly used for standard visual-language pre-training.
3.3 Multi-View caption extractor
Given a dataset T={(xi
I, xi
T)}N
i=1, containing Npaired images x·
Iand texts
x·
T. We define a model pool G={g1···gk···gK}that includes Kadvanced

--- PAGE 6 ---
6 Y. Liu et al.
MLLMs. For each image xi
I,Gis used to obtain rich captions. The operation
can be formulated as follows,
Ci={ci
1···ci
n···ci
N}=G(xi
I, Q) (2)
Where Cirepresents the extended captions for image xi
I, and Qdenotes the
question input to the MLLM. For different models in G, we use the same simple
question template: {Describe the ⟨image ⟩in English:} to query the captions.
The simple question has little impact on the diversity of answers, so we can
obtain comprehensive captions of each image.
How to use these extended captions? After obtaining these extended
captions, one of the most straightforward ideas is to replace a set of raw captions
with new captions for training. Its effectiveness has been evaluated in many
previous works [42,70]. However, we have a concern about the operation in terms
of the difference between raw and new captions. To investigate this, we count the
frequency of the most occurring nouns of the extended captions (extracted from
images in CC3M by MiniGPT-4, Otter, Qwen-VL, and LLaVA-1.5, examples
are shown in Figure 2) and present the results in Figure 3. One can easily find
that different MLLMs output different common words in the generated captions.
These diverse words can greatly expand the linguistic concepts of the dataset.
Raw MiniGPT -4 Otter Qwen- VL LLaVA -1.5
Fig. 3:Statistics of common nouns in captions generated by MLLMs.
/uni00000015/uni00000013/uni00000017/uni00000013/uni00000019/uni00000013/uni0000001b/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000016/uni00000013/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni00000051/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000035/uni00000044/uni0000005a
/uni00000030/uni0000004c/uni00000051/uni0000004c/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017/uni00000032/uni00000057/uni00000057/uni00000048/uni00000055 /uni00000034/uni0000005a/uni00000048/uni00000051/uni00000010/uni00000039/uni0000002f /uni0000002f/uni0000002f/uni00000044/uni00000039 /uni00000024/uni00000010/uni00000014/uni00000011/uni00000018
(a)Average length of captions generated using
different MLLMs.
The image shows two people sitting on a bench 
under a tree at sunset. The tree has a large trunk and branches that stretch out into the sky, creating a silhouette of the people. The sky is orange and pink, 
and there is a hint of a setting sun in the background. The people are sitting with their backs to the camera, facing each other and looking at each other's reflection in the water. They are both wearing dark clothing and have their arms around each other. The bench is made of metal and has two seats, one for each person. It is situated under the tree, which provides some shade from the sun. (b)Raw caption: ⟨lovers on a park bench. ⟩Here
we use MiniGPT-4 to caption the image and
mark the hallucinations of the generated cap-
tion in gray.
Fig. 4:Analysis of extended captions.
Based on the above analysis, we plan to use all the extended captions for
training to improve the diversity of captions. However, there is another concern
about the difference in the length of the raw and extended captions. We present
the average length comparisons of these captions in Figure 4a. These captions
are generated by MLLMs under default settings.It can be seen that the gener-
ated captions by MiniGPT-4 and LLaVA-1.5 are notably longer, i.e.90 tokens.
While raw caption is shorter, i.e.15 tokens. To investigate the influence of vary-
ing caption lengths on pre-training, we visualize the longer caption generated

--- PAGE 7 ---
MLLMs-Augmented Visual-Language Representation Learning 7
by MiniGPT-4 in Figure 4b. We find that longer captions indeed produce hal-
lucinations that do not match the image, and these hallucinations often appear
in the later part of the generated text, which has an impact on representation
learning. The same conclusion is also verified in [69]. It states that as generation
progresses, the accumulation of past hallucinatory information and uncertainty
can derail the model and lead to new hallucinations.
3.4 Text Shearing
To eliminate the impact of excessive caption length, we propose text shearing to
make the length of the extended captions as same as the raw ones. Specifically,
we add a token number Tas a limit when using MLLMs to generate captions.
To prevent the generated captions from having incomplete expressions, we in-
tercept the first complete clause in the generated caption. Therefore, Eq. 2 can
be rewritten as:
C′
i={ci′
1···ci′
n···ci′
N}=G(xi
I, Q, T ) (3)
For the setting of T, we use a simple but effective strategy. We set it to the
average length of the original captions to mimic human annotation. By control-
ling the length of generated captions, we hope to effectively reduce the impact
of MLLMs’ hallucinations and their monotonous language styles.
Then, we jointly use original captions and newly generated captions to con-
struct a new dataset. The new image-text pairs can be expressed as {xI, x′
T}=
{xI, ci′
1},{xI, ci′
2}···,{xI, ci′
k}, where krepresents the number of MLLMs. We
use the enhanced dataset for subsequent standard visual-language pre-training.
Taking CLIP as an example, the loss on the image can be written as:
LI= clip_contrast( xI, x′
T) (4)
The total loss L= (LI+LT)/2. The parameters βfor CLIP is updated by
minimizing L:
β←arg min
βL. (5)
4 Experiments
4.1 Pre-training Details
Our method is implemented in Pytorch [47] and trained on a node equipped
with 8 NVIDIA A100 GPUs. For visual language pre-training, we follow the
implementation of CLIP [49] and BLIP [32] respectively. For CLIP, we train the
model based on the open source code OpenCLIP [24]. During training, our batch
size is set to 320. The number of epochs is set to 6 to better align the training
overhead. The model architecture is set as VIT-B-16. Other parameters are the
sameasthedefaultvaluesinOpenCLIP[24].ForBLIP,weuseaViT-B-16[16,59]
pre-trained on ImageNet [14] as the image encoder and Bert base[15] as the text
encoder. Regarding image preprocessing, we employ random cropping to achieve

--- PAGE 8 ---
8 Y. Liu et al.
Table 1: The results of zero-shot image-text retrieval and fine-tuning image-text
retrieval on MSCOCO and Flickr30K.
Method DatasetMSCOCO (5K test set) Flickr30K (1K test set)
Image→Text Text →Image Image→Text Text →Image
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
CLIP [49]CC3M [54] 26.0 52.3 64.2 18.3 42.8 55.4 40.0 67.0 76.9 28.4 56.0 66.6
Ours47.3 74.1 83.2 33.2 62.0 73.5 75.0 92.3 95.9 58.0 83.0 89.2
+21.3 +21.8 +19.0 +14.9 +19.2 +18.1 +35.0 +25.3 +19.0 +29.6 +27.0 +22.6
CC3M [54] 8.723.933.77.119.728.617.437.950.113.930.840.5
Ours35.9 62.4 73.9 26.5 51.1 62.7 63.5 86.6 91.7 49.3 74.8 83.1
+27.2+38.5+40.2+19.4+31.4+34.1+46.1+48.7+41.6+35.4+44.0+42.6
BLIP [32]CC3M [54] 67.5 88.9 93.7 50.9 76.9 85.2 88.5 98.5 99.5 74.0 92.4 95.7
Ours76.2 93.0 96.4 57.6 82.3 89.3 94.1 99.4 99.8 82.2 95.7 97.9
+8.7 +4.1 +2.7 +6.7 +5.4 +4.1 +5.6 +0.9 +0.3 +8.2 +3.3 +2.2
CC3M [54] 36.362.673.628.652.964.262.187.491.951.275.582.7
Ours59.3 81.5 88.9 45.2 71.0 80.2 85.5 97.1 98.8 72.0 90.3 94.3
+23.0+18.9+15.3+16.8+18.1+16.0+23.4+9.7+6.9+20.8+14.8+11.6
224×224during pre-training and increase the image resolution to 384×384
during fine-tuning. We apply RandomAugment [12] for image augmentation. For
optimization, we utilize an AdamW [37] optimizer with a learning rate warm-up
to 3e-4, combined with linear decay at a rate of 0.9. Our training batch size is
set to 1280, and the number of training epochs is 4.
Regarding the usage of multi-modal large language models, we employed
Mini-GPT4-Vicuna13B [71], Otter-Image-MPT7B [30], Qwen-VL-Chat [5], and
LLaVA-v1.5-13B [35]. During caption generation, we impose a maximum token
limit of 30 to control the caption length. For the synthetic caption generation,
we use beam search with the number of beams equal to 1.
We use CC3M [54], CC12M [8] and YFCC15M as pre-training datasets.
Amongthem,YFCC15MisasubsetofYFCC100M[58]thatcontainsEnglishde-
scriptions of images. Due to the inaccessibility of some images, the version of the
dataset we used contains fewer images than the original dataset. Among them,
on CC3M, compared with the 3.3M version of the original dataset, the version
we used contains 2.6M image-text pairs; on CC12M, compared with the 12.4M
version of the original dataset, the version we used contains 11.1M image-text
pairs; On YFCC15M, compared to the 15M of the original dataset, our version
contains 14.8M image-text pairs.
4.2 Evaluation
We evaluate the performance of the multi-modal task in Table 1 and Table 4.
We also provide results on the image classification task in Table 2 and Table 3.
Image-Text Retrieval. Image-text retrieval serves as a crucial metric for
evaluating the bridging capability between different modalities in a model. We
pre-train CLIP and BLIP on CC3M, conducting image-to-text retrieval (TR)
and text-to-image retrieval (IR) testing on the MSCOCO [34] and Flickr30K [48]
datasets. Results in Table 1 demonstrate significant performance improvements

--- PAGE 9 ---
MLLMs-Augmented Visual-Language Representation Learning 9
Table 2: The zero-shot evaluation results of our method on 15 common classification
datasets and ImageNet with CLIP pre-trained on CC3M and CC12M. The best results
are highlighted in bold. On CC3M, our method demonstrates an average improvement
of 13.4 on 15 datasets and 13.1 on ImageNet. The different results on vanilla CLIP are
due to the different training data downloaded from the Internet.
Data Model
Food-101
CIFAR-10
CIFAR-100
SUN397
Cars
Aircraft
DTD
Pets
Caltech-101
Flowers
STL-10
EuroSAT
RESISC45
GTSRB
Country211
Average
∆(%)
ImageNet
∆(%)
Model Architecture: ViT-B/16
CC3MCLIP 10.3 54.9 21.8 25.0 0.8 1.4 10.5 12.8 43.3 10.2 77.6 14.1 19.1 6.9 0.6 20.6 - 15.8 -
LaCLIP [17] 14.2 57.1 27.5 35.1 1.6 1.616.6 15.6 52.7 14.7 86.2 15.0 24.36.4 1.0 24.6 ↑4.021.5 ↑5.7
CLIP 9.2 28.4 9.8 21.9 1.1 1.1 8.3 7.6 40.7 8.9 70.4 13.2 14.8 5.6 0.4 16.1 - 11.9 -
Ours 18.7 58.4 32.4 43.8 3.9 1.520.2 32.1 63.5 17.5 87.3 25.1 23.113.0 2.0 29.5 ↑13.4 25.0 ↑13.1
CC12MCLIP 50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3 5.1 38.8 - 40.2 -
LaCLIP [17] 60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.946.2 ↑7.4 48.4 ↑8.2
CLIP 45.9 65.7 33.4 44.7 18.4 2.9 18.5 54.8 72.6 30.4 89.5 23.2 28.5 9.3 4.8 36.2 - 37.3 -
Ours 60.9 83.0 55.4 59.4 24.1 3.2 30.7 64.8 79.3 36.0 95.3 40.5 45.6 25.4 5.847.3 ↑11.147.5 ↑10.2
in both zero-shot and fine-tuning retrievals for both CLIP and BLIP. Notably,
ourapproachenablesCLIP’szero-shotretrievalperformancetooutperformmod-
els finetuned on the target dataset. Specifically, when using CLIP for zero-shot
retrieval on MSCOCO, the R@1 of TR and IR increase by 27.2 and 19.4, re-
spectively. For zero-shot retrieval on Flickr30K, the R@1 of TR and IR also
improves by 46.1 and 35.4, respectively. Similarly, our method also achieves an
improvement of 16.8 ∼23.4 when using BLIP for zero-shot retrieval. These sig-
nificant performance improvements indicate that the pre-trained model learns
better image-text representation.
Image Classification. We evaluate the performance of our method on the
task of image classification. In Table 2, we pre-train a CLIP with a VIT-B-16
architecture on CC3M and CC12M respectively, followed by testing on sixteen
common image classification datasets. We compare our results with [17]. Our
method exhibits significant performance improvements across most datasets, as
demonstrated. For the CLIP pre-trained on CC3M, there’s an average improve-
ment of 13.4 on 15 datasets and a 13.1 improvement on ImageNet [14]. For the
CLIP pre-trained on CC12M, there’s an average improvement of 11.1 and 10.2
respectively. These results highlight the effective enhancement of representation
learning achieved by our method.
Linear-Probing. We also explore the linear-probing performance in image
classification. We use CLIP pre-trained on CC3M and CC12M for evaluation re-
spectively. Similar to the zero-shot image classification, we compare our method
with [17] across 15 common datasets. The results in Table 3 show that our
method also has certain improvements for linear probing. This shows that rich
text concepts contribute to the effective training of visual encoders. As the vi-
sual encoder learns each image, it benefits from supervision through multiple
accurate and diverse captions. This allows it to efficiently acquire a universal

--- PAGE 10 ---
10 Y. Liu et al.
Table 3: Thelinearprobingresultsofourmethodon15commonclassificationdatasets
with CLIP pre-trained on CC3M and CC12M. On CC3M, our method demonstrates
an average improvement of 7.9 on 15 datasets.
Data Model
Food-101
CIFAR-10
CIFAR-100
SUN397
Cars
Aircraft
DTD
Pets
Caltech-101
Flowers
STL-10
EuroSAT
RESISC45
GTSRB
Country211
Average
∆(%)
Model Architecture: ViT-B/16
CC3MCLIP 62.6 86.8 68.1 58.5 32.8 40.9 63.4 69.6 82.0 89.4 91.7 95.9 89.0 71.9 13.3 67.7 -
LaCLIP [17] 63.8 87.7 69.5 60.2 32.4 42.7 64.0 71.1 83.3 90.2 93.4 95.8 89.7 74.6 13.2 68.8 ↑1.1
CLIP 52.8 79.7 58.8 52.4 21.9 26.4 53.1 56.6 77.5 76.6 85.7 93.9 84.5 66.7 8.7 59.7 -
Ours 64.0 87.7 68.5 59.1 34.5 32.1 60.4 73.3 85.5 83.6 92.6 95.3 87.9 78.4 10.6 67.6 ↑7.9
CC12MCLIP 81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4 -
LaCLIP [17] 82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5 ↑1.1
CLIP 73.9 89.9 71.2 68.9 62.8 35.6 67.1 81.1 90.3 87.7 95.0 95.6 89.1 77.4 13.8 73.3 -
Ours 80.6 93.9 78.1 72.1 62.1 35.6 72.1 82.7 90.6 87.9 97.3 95.7 91.1 84.8 13.8 75.9 ↑2.6
Table 4: The results of vision question answering, visual reasoning and image cap-
tioning of BLIP pre-trained on CC3M, CC12M, and YFCC15M, respectively.
Dataset VQAv2 A-OKVQA OK-VQA NLVR2COCO Caption NoCaps
test-dev test-std val test dev test-P B@4 CIDEr CIDEr SPICE
CC3M [54] 71.5 71.8 28.9 24.0 76.0 76.2 36.6 119.9 84.4 12.5
Ours 75.6 +4 .175.6 +3 .832.0 +3 .131.4 +7 .480.1 +4 .179.3 +3 .137.6 +1 .0125.6 +5 .797.0 +12 .613.8 +1 .3
CC12M [8] 73.5 73.5 31.5 31.0 78.7 79.0 37.2 123.1 99.4 13.5
Ours 77.0 +3 .577.1 +3 .632.0 +0 .531.6 +0 .681.2 +2 .581.9 +2 .939.1 +1 .9130.1 +7 .0103.6 +4 .214.3 +0 .8
YFCC15M [58] 70.5 70.6 29.8 29.2 74.0 74.2 36.8 122.0 97.5 13.3
Ours 72.8 +2 .372.8 +2 .231.1 +1 .330.0 +0 .878.5 +4 .577.8 +3 .637.6 +0 .8125.6 +3 .6101.1 +3 .613.9 +0 .6
representation of the image, leading to a more effective characterization of the
image and text embedding space.
Vision Question Answering. Visual Question Answering (VQA) [4] is the
task of providing answers based on an image and a question. We conduct tests
on the VQAv2 [22], A-OKVQA [53] and OK-VQA [40] using BLIP pre-trained
on CC3M, CC12M and YFCC15M, respectively. The results are presented in
Table 4. Similar to BLIP [32], we also consider VQA as an answer-generation
task. The consistent performance improvements obtained through our method
indicate that the model has acquired a more robust visual-language representa-
tion from datasets enriched with MLLMs’ knowledge. Besides, the improvement
on A-OKVQA and OK-VQA indicates that the model has more common sense
and world knowledge. These results suggest an enhanced capability in visual
understanding and language capabilities together.
Visual Reasoning. In the Natural Language Visual Reasoning (NLVR2)
task [57], the model is required to perform multi-modal reasoning by analyzing
two images and a natural language question. We conduct an evaluation using the

--- PAGE 11 ---
MLLMs-Augmented Visual-Language Representation Learning 11
pre-trained BLIP on CC3M, CC12M, and YFCC15M, respectively. As depicted
in Table 4, our method consistently delivers improved performance. This shows
that the model makes certain progress in natural language understanding, visual
recognition, and logical reasoning.
Image Captioning. Image Captioning is the task of generating a text de-
scription of the image content given an image. We conduct tests on COCO and
Nocaps [2] using the BLIP pre-trained on CC3M, CC12M, and YFCC15M, re-
spectively. Following the approach in [32], we initially fine-tune the pre-trained
model with the LM loss on COCO. To achieve better results, we also add "a
picture of" at the beginning of the prompt. The results are presented in Table 4,
indicating improvements in BLEU@4 and CIDEr metrics. These findings suggest
thatourmethodenhancesthemodel’sunderstandingoftherelationshipbetween
texts and images, resulting in higher similarity with human annotations during
the captioning process.
Dataset R@1↑R@5↑R@10↑MdR↓
CC3M [54] 26.0 46.3 58.0 7.0
Ours 28.3 +2 .350.6 +4 .360.7 +2 .75.0−2.0
CC12M [8] 34.0 56.7 68.5 4.0
Ours 36.2 +2 .260.6 +3 .970.6 +2 .13.0−1.0
YFCC15M [58] 28.2 48.0 63.2 6.0
Ours 32.3 +4 .153.6 +5 .665.4 +2 .24.0−2.0
Table 5: The retrieval performance on the
video-language retrieval dataset MSRVTT.
R@KstandsforrecallatKandMdRstands
for median rank.Video-Language Task. Text-to-
video retrieval is a metric for evalu-
ating a model’s generalization ability
in video-language tasks. We evaluate
our method on the MSRVTT dataset.
Following the [32], we fine-tune the
model on COCO. For video input,
we uniformly sample 8 frames from
it to get a sequence. The results in
Table 4 demonstrate a stable perfor-
mance improvement achieved by our
method. This suggests that robust
visual-language representation learning may be key to video-text retrieval.
4.3 Analysis
Scaling Ability. Notably, as shown in Table 4, our method shows certain im-
provements when pre-training BLIP on CC12M [8] and YFCC15M [58]. This
indicates that our method is scalable to larger datasets to some extent. It also
emphasizes the significance of constructing well-represented image-text pairs for
enhancing visual-language pre-training.
Training Cost. To ensure a fair comparison with baseline methods, we
carefully craft the training schedule. Considering that the number of image-text
pairs in the augmented dataset is ktimes the number of image-text pairs in
the original dataset, we adjust the pre-training epochs to be 1/kof the original
epochs. By doing this, we avoid introducing additional training overhead.
4.4 Ablation Study
Caption Length. The length of the generated captions is one of the factors af-
fecting visual-language pre-training. We utilize MiniGPT-4 to generate captions
for CC3M with varying max token number limits. By training BLIP on captions

--- PAGE 12 ---
12 Y. Liu et al.
2030405060708090100
/uni00000037/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni0000004b/uni00000057/uni0000004b5254565860626466/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni0000000b/uni00000008/uni0000000c
/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000011
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f
(a)Token Length vs.
Performance
200 160 120 80 40
/uni00000025/uni00000044/uni00000057/uni00000046/uni0000004b/uni00000003/uni00000036/uni0000004c/uni0000005d/uni000000480102030405060/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni0000000b/uni00000008/uni0000000c
/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000023/uni00000014/uni00000011
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000023/uni00000014(b)Batch size vs. Per-
formance
0 1 2 3 4
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000050/uni00000052/uni00000047/uni00000048/uni0000004f30354045505560/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni00000003/uni0000000b/uni00000008/uni0000000c
 /uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000023/uni00000014
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000023/uni00000014(c)Model Numbers vs.
Performance
2345678910
/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni0000005642.545.047.550.052.555.057.560.0/uni00000035/uni00000048/uni00000057/uni00000055/uni0000004c/uni00000048/uni00000059/uni00000044/uni0000004f/uni0000000b/uni00000008/uni0000000c
/uni00000037/uni00000048/uni0000005b/uni00000057/uni00000042/uni00000035/uni00000023/uni00000014/uni00000011
/uni0000002c/uni00000050/uni00000044/uni0000004a/uni00000048/uni00000042/uni00000035/uni00000023/uni00000014(d)Epochs vs. Perfor-
mance
Fig. 5:The influence of the token length, batch size, model numbers, and epochs on
the performance of visual-language pre-training.
BLIP(CC3M)：my living room
BLIP(Ours)：a living room with
a couch , a chair , a coffee 
table , and a television
BLIP(CC3M)：a tram
BLIP(Ours)：a trolley car 
on the streetBLIP(CC3M)：flowers on a table
BLIP(Ours)：a dining room 
table with a vase of flowers 
on it
BLIP(CC3M)：a train
BLIP(Ours)：a train going 
over a bridge
Fig. 7:Examples of image captioning using pre-trained BLIP without any fine-tuning.
Newly introduced visual concepts are represented in red. It can be seen that BLIP
pre-trained on our dataset has stronger visual concept capture capabilities.
of different lengths and conducting retrieval on MSCOCO, we present the re-
sults in Figure5a and observe that as the length of the caption increases, the
model’s performance tends to decrease. This may be because too-long captions
The image shows a view of a body of 
water with several boats in the 
foreground. In the background, there 
is a hill with trees and a small town 
visible. The sky is clear and blue with 
a few clouds in the distance. The 
horizon is visible with a large body of 
water visible in the distance.
This image shows a small town with houses and boats on the water. The 
sky is clear and blue, with some 
clouds in the distance. There are trees
and other vegetation on the banks of the river. The town appears to be 
small , with only a few houses visible in 
the distance. 
Fig. 6:Long captions for different images
tend to be similar due to the intrinsic infer-
ence manner of the model. Similar content
in captions is shown in red.cause more similar text features be-
tween captions of different images.
We illustrate this phenomenon in Fig-
ure 6. When the caption length in-
creases, the generated captions often
become identical in many instances.
This leads to different images easily
mapping to the same text features,
creating challenges in learning an ef-
fective representation.
Batch Size. We explore the im-
pact of batch size on training in Fig-
ure 5b. Training models on synthetic
captions requires a relatively high
batch size compared to using captions
annotated by humans. We attribute this to the fact that, with a small batch size,
the main gradient information obtained is noisy, differing significantly from the

--- PAGE 13 ---
MLLMs-Augmented Visual-Language Representation Learning 13
true gradient. Conversely, with an increased batch size, the gradient of the sam-
pled distribution becomes more similar to the gradient of the real distribution,
enabling the model to generalize better.
Number of MLLMs. We explore the impact of the number of MLLMs on
the transfer performance in Figure 5c. We add four MLLMs{MiniGPT-4, Otter,
Qwen-VL, and LLAVA-1.5}in order. Using pre-trained models for zero-shot re-
trieval on MSCOCO, we observe that, as the number of MLLMs increases, the
performance gradually improves. However, the magnitude of this improvement
is gradually decreasing, indicating that the one image’s caption information ob-
tained from MLLM is reaching saturation.
Training Epochs. We visualize the curve depicting the number of training
epochs versus the model’s performance on the retrieval task in Figure 5d. It
is evident that our method achieves promising performance with only a small
numberofepochs.Inthemainexperiment,wechooseepochnumber4tocompare
with the baseline without increasing the training cost. When further increasing
the number of epochs, our method continues to show certain improvements.
MLLMCOCO (R@1)Flickr30k (R@1)Avg.I2T T2I I2T T2I
Raw 36.3 28.6 62.1 51.2 44.6
MiniGPT-4 51.6 38.1 77.0 60.5 56.8
Otter 50.6 37.5 78.3 61.9 57.1
Qwen-VL 43.4 35.4 72.1 61.7 53.2
LLaVA-1.5 48.4 37.1 76.7 61.2 55.9
Table 6: The retrieval performance of
models trained on captions rewritten using
one MLLM.MLLM’s Independent Effect.
We also evaluate the performance
of rewriting captions using only one
MLLM and utilize the generated cap-
tionsfortraining.Captionsarerewrit-
ten using MiniGPT-4, Otter, Qwen-
VL, and LLaVA-1.5 respectively, and
the results are presented in Table 6.
The results indicate that there is an
upper limit to the performance im-
provement achievable by rewriting with only one MLLM. Besides, different
MLLMs also have different strengths in knowledge. Combining multiple MLLMs’
knowledge proves to be more effective in enhancing visual-language pre-training.
4.5 Visualization
Image Captioning Visualization. In Figure 7 we illustrate the difference in
image captioning between models pre-trained on the original CC3M dataset and
our improved dataset. Without any fine-tuning, our model exhibits a significant
improvement in the ability to recognize visual concepts in images.
Image-text Distribution Visualization. We visualize the feature distri-
bution of image-text pairs from the original CC3M and our dataset in Figure 8a.
It can be observed that the original CC3M dataset contains numerous discrete
and unmatched image-text pairs. In contrast, our dataset exhibits a distribution
where almost all images have corresponding texts. This distribution perspective
explains how our method aligns more continuous images with discrete language,
thereby enhancing visual-language representation learning.
Cosine Similarity Visualization. We visualize the cosine similarity com-
parison between the original CC3M dataset and our enhanced CC3M dataset in
Figure 8b. We use a pre-trained CLIP to calculate the cosine distance between

--- PAGE 14 ---
14 Y. Liu et al.
CC3M
 Ours
Image Text
(a)Image-text distribution visualization.
0.0 0.1 0.2 0.3 0.4 0.5 0.6
Image-text cosine similarity012345678DensityOriginal CC3M
Ours (b)Cosine similarity visualization.
Fig. 8:(a) Distribution of images and texts in the original CC3M dataset and our
enhanced CC3M dataset. Our method aligns images and texts better in the latent
space. (b) Cosine similarity distribution of images and texts. Our method improves the
average matching degree of image-text pairs.
image embeddings and text embeddings. Our method exhibits a higher average
similarity, which proves the effectiveness in improving the matching degree of the
image-text pairs and establishing robust image-text associations for datasets.
4.6 Comparison with More Methods
Table 7: The image-text retrieval results on COCO and Flickr30K and classification
results on ImageNet and ImageNetV2 [50] using the pre-trained CLIP on CC3M.
MethodCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2 Avg.I2T T2I I2T T2I
CLIP 13.9 9.6 26.3 18.0 14.6 12.5 15.8
VeCLIP [29] 32.0 22.1 57.2 36.5 20.7 17.9 31.1
CLIP 8.7 7.1 17.4 13.9 11.9 10.3 11.6
Ours 35.9 26.5 63.5 49.3 25.0 21.4 36.9
We also compare our work with the contemporaneous work VeCLIP [29] in
the same setting in Table 7. We use pre-trained CLIP to compare results on
image-text retrieval and image classification. It can be seen that our method
achieves better performance improvement than the caption fusion used in Ve-
CLIP. This also implies that establishing more continuous image-text corre-
spondences is the key to improving visual-language pre-training on small-scale
datasets. We also compare our method with [42] in the appendix.
5 Conclusion and Discussion
In this paper, we propose to augment visual-language representation learning by
leveraging multiple MLLMs. While retaining the rich visual information of the
original dataset, we utilize multiple MLLMs to extend diverse captions for each
image. Additionally, we introduce “text shearing" to address issues of halluci-
nations and monotonous language style in synthetic captions. Validated across

--- PAGE 15 ---
MLLMs-Augmented Visual-Language Representation Learning 15
variousvisual-languagepre-trainingframeworksanddatasets,ourmethodsignif-
icantly improves performance on numerous downstream tasks. This encourages
further exploration in the utilization of MLLMs in the future.
Limitations and future works Although our method has achieved re-
markable results in enhancing visual-language representation learning, a certain
proportion of noise persists due to unreliable MLLMs’ outputs. These noises
limit the further improvement of the model’s performance. Future research could
explore leveraging more powerful MLLMs to generate accurate captions and ex-
panding to larger datasets.
References
1. Abbas, A., Tirumala, K., Simig, D., Ganguli, S., Morcos, A.S.: Semdedup: Data-
efficient learning at web-scale through semantic deduplication. arXiv preprint
arXiv:2303.09540 (2023)
2. Agrawal, H., Desai, K., Wang, Y., Chen, X., Jain, R., Johnson, M., Batra, D.,
Parikh, D., Lee, S., Anderson, P.: Nocaps: Novel object captioning at scale. In:
ICCV. pp. 8948–8957 (2019)
3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model
for few-shot learning. Neurips 35, 23716–23736 (2022)
4. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:
Vqa: Visual question answering. In: ICCV. pp. 2425–2433 (2015)
5. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.: Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv
preprint arXiv:2308.12966 (2023)
6. Bossard, L., Guillaumin, M., Van Gool, L.: Food-101–mining discriminative com-
ponents with random forests. In: ECCV. pp. 446–461. Springer (2014)
7. Cao, L., Zhang, B., Chen, C., Yang, Y., Du, X., Zhang, W., Lu, Z., Zheng, Y.: Less
is more: Removing text-regions improves clip training efficiency and robustness.
arXiv preprint arXiv:2305.05095 (2023)
8. Changpinyo, S., Sharma, P., Ding, N., Soricut, R.: Conceptual 12m: Pushing web-
scale image-text pre-training to recognize long-tail visual concepts. In: CVPR. pp.
3558–3568 (2021)
9. Cheng, G., Han, J., Lu, X.: Remote sensing image scene classification: Benchmark
and state of the art. Proceedings of the IEEE 105(10), 1865–1883 (2017)
10. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., Vedaldi, A.: Describing textures
in the wild. In: CVPR. pp. 3606–3613 (2014)
11. Coates, A., Ng, A., Lee, H.: An analysis of single-layer networks in unsupervised
feature learning. In: Proceedings of the fourteenth international conference on ar-
tificial intelligence and statistics. pp. 215–223. JMLR Workshop and Conference
Proceedings (2011)
12. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated
data augmentation with a reduced search space. In: CVPR workshops. pp. 702–703
(2020)
13. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,
S.: Instructblip: Towards general-purpose vision-language models with instruction
tuning (2023)

--- PAGE 16 ---
16 Y. Liu et al.
14. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale
hierarchical image database. In: CVPR. pp. 248–255. Ieee (2009)
15. Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:Bert:Pre-trainingofdeepbidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805
(2018)
16. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 (2020)
17. Fan, L., Krishnan, D., Isola, P., Katabi, D., Tian, Y.: Improving clip training with
language rewrites. arXiv preprint arXiv:2305.20088 (2023)
18. Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., Schmidt,
L.: Data determines distributional robustness in contrastive language image pre-
training (clip). In: ICML. pp. 6216–6234. PMLR (2022)
19. Fei-Fei, L., Fergus, R., Perona, P.: One-shot learning of object categories. PAMI
28(4), 594–611 (2006)
20. Gadre, S.Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten,
R., Wortsman, M., Ghosh, D., Zhang, J., et al.: Datacomp: In search of the next
generation of multimodal datasets. arXiv preprint arXiv:2304.14108 (2023)
21. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C.,
Yue, X., Li, H., Qiao, Y.: Llama-adapter v2: Parameter-efficient visual instruction
model. arXiv preprint arXiv:2304.15010 (2023)
22. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In:
CVPR. pp. 6904–6913 (2017)
23. Helber, P., Bischke, B., Dengel, A., Borth, D.: Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing 12(7), 2217–
2226 (2019)
24. Ilharco,G.,Wortsman,M.,Wightman,R.,Gordon,C.,Carlini,N.,Taori,R.,Dave,
A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., Schmidt, L.:
Openclip (Jul 2021). https://doi.org/10.5281/zenodo.5143773 ,https://doi.
org/10.5281/zenodo.5143773 , if you use this software, please cite it as below.
25. Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q., Sung, Y.H.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision. In: ICML. pp. 4904–4916. PMLR (2021)
26. Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without convo-
lution or region supervision. In: ICML. pp. 5583–5594. PMLR (2021)
27. Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3d object representations for fine-
grained categorization. In: ICCV workshops. pp. 554–561 (2013)
28. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009)
29. Lai, Z., Zhang, H., Wu, W., Bai, H., Timofeev, A., Du, X., Gan, Z., Shan, J.,
Chuah, C.N., Yang, Y., et al.: From scarcity to efficiency: Improving clip training
via visual-enriched captions. arXiv preprint arXiv:2310.07699 (2023)
30. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
31. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023)

--- PAGE 17 ---
MLLMs-Augmented Visual-Language Representation Learning 17
32. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML. pp. 12888–
12900. PMLR (2022)
33. Li, Y., Fan, H., Hu, R., Feichtenhofer, C., He, K.: Scaling language-image pre-
training via masking. In: CVPR. pp. 23390–23400 (2023)
34. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV. pp. 740–755.
Springer (2014)
35. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.
arXiv preprint arXiv:2310.03744 (2023)
36. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning (2023)
37. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017)
38. Maini, P., Goyal, S., Lipton, Z.C., Kolter, J.Z., Raghunathan, A.: T-mars: Improv-
ing visual representations by circumventing text feature learning. arXiv preprint
arXiv:2307.03132 (2023)
39. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-grained visual
classification of aircraft. arXiv preprint arXiv:1306.5151 (2013)
40. Marino,K.,Rastegari,M.,Farhadi,A.,Mottaghi,R.:Ok-vqa:Avisualquestionan-
swering benchmark requiring external knowledge. In: CVPR. pp. 3195–3204 (2019)
41. Mu, N., Kirillov, A., Wagner, D., Xie, S.: Slip: Self-supervision meets language-
image pre-training. In: ECCV. pp. 529–544. Springer (2022)
42. Nguyen, T., Gadre, S.Y., Ilharco, G., Oh, S., Schmidt, L.: Improving multimodal
datasets with image captioning. arXiv preprint arXiv:2307.10350 (2023)
43. Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., Schmidt, L.: Quality not quantity:
On the interaction between dataset design and robustness of clip. Neurips 35,
21455–21469 (2022)
44. Nilsback, M.E., Zisserman, A.: Automated flower classification over a large number
of classes. In: 2008 Sixth Indian conference on computer vision, graphics & image
processing. pp. 722–729. IEEE (2008)
45. OpenAI: Introducing chatgpt. https://openai.com/blog/chatgpt (2022)
46. Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.: Cats and dogs. In: CVPR.
pp. 3498–3505. IEEE (2012)
47. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. Neurips 32(2019)
48. Plummer,B.A.,Wang,L.,Cervantes,C.M.,Caicedo,J.C.,Hockenmaier,J.,Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. In: ICCV. pp. 2641–2649 (2015)
49. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML. pp. 8748–8763. PMLR (2021)
50. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do imagenet classifiers generalize
to imagenet? In: ICML. pp. 5389–5400. PMLR (2019)
51. Santurkar, S., Dubois, Y., Taori, R., Liang, P., Hashimoto, T.: Is a caption worth
a thousand images? a controlled study for representation learning. arXiv preprint
arXiv:2207.07635 (2022)
52. Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,
Coombes,T.,Jitsev,J.,Komatsuzaki,A.:Laion-400m:Opendatasetofclip-filtered
400 million image-text pairs. arXiv preprint arXiv:2111.02114 (2021)

--- PAGE 18 ---
18 Y. Liu et al.
53. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A
benchmark for visual question answering using world knowledge. In: ECCV. pp.
146–162. Springer (2022)
54. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL. pp.
2556–2565 (2018)
55. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai
tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580
(2023)
56. Stallkamp, J., Schlipsing, M., Salmen, J., Igel, C.: The german traffic sign recogni-
tionbenchmark:amulti-classclassificationcompetition.In:IJCNN.pp.1453–1460.
IEEE (2011)
57. Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., Artzi, Y.: A corpus for reasoning
about natural language grounded in photographs. arXiv preprint arXiv:1811.00491
(2018)
58. Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth,
D., Li, L.J.: Yfcc100m: The new data in multimedia research. Communications of
the ACM 59(2), 64–73 (2016)
59. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training
data-efficient image transformers & distillation through attention. In: ICML. pp.
10347–10357. PMLR (2021)
60. Wang, A.J., Lin, K.Q., Zhang, D.J., Lei, S.W., Shou, M.Z.: Too large; data reduc-
tion for vision-language pre-training. arXiv preprint arXiv:2305.20087 (2023)
61. Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt:
Talking, drawing and editing with visual foundation models. arXiv preprint
arXiv:2303.04671 (2023)
62. Wu, S., Fei, H., Qu, L., Ji, W., Chua, T.S.: Next-gpt: Any-to-any multimodal llm
(2023)
63. Xiao,J.,Hays,J.,Ehinger,K.A.,Oliva,A.,Torralba,A.:Sundatabase:Large-scale
scene recognition from abbey to zoo. In: CVPR. pp. 3485–3492. IEEE (2010)
64. Xu, J., Mei, T., Yao, T., Rui, Y.: Msr-vtt: A large video description dataset for
bridging video and language. In: CVPR. pp. 5288–5296 (2016)
65. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., Chen, E.: A survey on multimodal
large language models. arXiv preprint arXiv:2306.13549 (2023)
66. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.:
Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 (2022)
67. Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H.,
Qiao, Y.: Llama-adapter: Efficient fine-tuning of language models with zero-init
attention. arXiv preprint arXiv:2303.16199 (2023)
68. Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., Smola, A.: Multimodal chain-
of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 (2023)
69. Zhou, Y., Cui, C., Yoon, J., Zhang, L., Deng, Z., Finn, C., Bansal, M., Yao, H.:
Analyzing and mitigating object hallucination in large vision-language models.
arXiv preprint arXiv:2310.00754 (2023)
70. Zhu, D., Chen, J., Haydarov, K., Shen, X., Zhang, W., Elhoseiny, M.: Chatgpt
asks, blip-2 answers: Automatic questioning towards enriched visual descriptions.
arXiv preprint arXiv:2303.06594 (2023)
71. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023)

--- PAGE 19 ---
MLLMs-Augmented Visual-Language Representation Learning 19
6 Dataset Details
Evaluation datasets. We provide specific information on the datasets used in
the image classification task in Table 8 and the datasets used in the multi-modal
task in Table 9.
Table 8: The details of the image classification datasets.
Dataset Categories Train Size Test Size
Food-101 [6] 101 75,750 25,250
CIFAR-10 [28] 10 50,000 10,000
CIFAR-100 [28] 100 50,000 10,000
SUN397 [63] 397 19,850 19,850
Cars [27] 196 8,144 8,041
Aircraft [39] 100 6,667 3,333
DTD [10] 47 3,760 1,880
Pets [46] 37 3,680 3,669
Caltech-101 [19] 102 3,060 6,085
Flowers [44] 102 2,040 6,149
STL-10 [11] 10 1,000 8,000
EuroSAT [23] 10 10,000 5,000
RESISC45 [9] 45 25,200 6,300
GTSRB [56] 43 26,640 12,630
Country211 [49] 211 42,200 21,100
ImageNet [14] 1000 1.28M 50,000
ImageNet-v2 [50] 1000 1.28M 10,000
Table 9: The details of the datasets used in multi-modal tasks.
Dataset Task Evaluation model
MSCOCO [34] image-text retrieval BLIP&CLIP
Flickr30K [48] image-text retrieval BLIP&CLIP
VQAv2 [22] vision question answering BLIP
A-OKVQA [53] vision question answering BLIP
OK-VQA [40] vision question answering BLIP
NLVR [57] vision reasoning BLIP
MSCOCO [34] image captioning BLIP
NoCaps [2] image captioning BLIP
MSRVTT [64] video retrieval BLIP
7 Implementation Details
7.1 Caption Generation
We provide the hyper-parameter settings of the used MLLMs: {MiniGPT-4 [71],
Otter [30], Qwen-VL [5], LLaVA-1.5 [35]} in the Table 10. After the new tokens
are generated and decoded using the decoder, we extract the first meaningful
substring in the string whose length is greater than 5 and ends with a period.
Then this substring is extended into the caption list for each image.

--- PAGE 20 ---
20 Y. Liu et al.
Table 10: Hyperparameter configuration of MLLMs.
Model Hyper-parameters Value
Mini-GPT4-Vicuna13Bmax_new_tokens 30
num_beams 1
do_sample True
min_length 1
top_p 0.3
repetition_penalty 1.0
length_penalty 1
temperature 1.0
Otter-Image-MPT7Bmax_new_tokens 30
num_beams 1
no_repeat_ngram_size 3
Qwen-VL-Chatmax_new_tokens 30
num_beams 1
do_sample False
min_new_tokens 8
length_penalty 0
num_return_sequences 1
LLaVA-v1.5-13Bmax_new_tokens 30
num_beams 1
do_sample True
top_p None
temperature 0.2
7.2 Evaluation
When evaluating image-text retrieval tasks, our settings are as follows: In the
zero-shot retrieval of MSCOCO and Flickr30K, we directly use pre-trained mod-
els. Unlike the zero-shot retrieval of Flickr30K in BLIP [32], we do not use a
model that is pre-trained and fine-tuned on MSCOCO. Correspondingly, the re-
sults of fine-tuning retrieval are the results of retrieval after fine-tuning on the
two target datasets respectively.
8 Comparison with More Methods
We also compare our work with the method mentioned in the [42]. Regarding
the methodology, [42] employs a similar approach of combining raw captions
and generated captions for training. In contrast to our use of multiple advanced
MLLMs, [42] relies solely on BLIP2 [31] for rewriting captions. While the aim of
[42] is to eliminate noise in the original large-scale dataset, our method aims to
enhance visual-language representation learning. Concerning experimental set-
tings, we primarily conduct pre-training based on the two datasets CC3M and
CC12M, whereas [42] focuses on pre-training using DataComp [20] datasets of
12.8M, 128M, and 1.28B. Regarding experimental results, our pre-trained CLIP

--- PAGE 21 ---
MLLMs-Augmented Visual-Language Representation Learning 21
on CC12M achieved a zero-shot classification result of 47.5 on ImageNet, sur-
passing even the best result of 31.7 on the medium scale (128M). This further
affirms the effectiveness of our method.
9 Statistics of Words in MLLMs
We count the most commonly used words in captions generated using different
MLLMs in Figure 9. The different distributions of these MLLMs on common
words also imply the diversity of generated captions.
10 Details about Code
We provide our code running logic in Figure 10. The code mainly includes the
generation of multi-view captions. For the image-text dataset, we use annota-
tion JSON files and corresponding image data for enhancement. Annotation files
include several entries consisting of image paths and captions. We split the an-
notation file into multiple parts to improve GPU utilization. For each part, we
use MLLMs to generate captions. The captions generated by multiple MLLMs
and the original captions are merged again as a new enhanced dataset. Each
image in the new dataset contains an original caption and 4 generated captions.
11 Word Cloud Visualization.
We present the word cloud visualization depicting captions generated by various
MLLMs in Figure 11. Notably, there are significant differences in common words.
12 Image-text Pairs and Image Captioning
We provide more image-text pairs on CC3M enhanced by our method in the
Figure 12. We also use BLIP pre-trained on CC3M without any fine-tuning for
image captioning and provide results in Figure 13. The images used are from
MSCOCO.

--- PAGE 22 ---
22 Y. Liu et al.
Raw
MiniGPT -4
Otter
Qwen -VL
LLaVA -1.5
Fig. 9:Statistics of commonly used words in MLLMs.

--- PAGE 23 ---
MLLMs-Augmented Visual-Language Representation Learning 23
Annotation file
Part-0 Part-n Part-N … …
MiniGPT -4 Qwen -VL Otter LLaVA -1.5
MiniGPT -4
AugmentedOtter
AugmentedQwen -VL
AugmentedLLaVA -1.5
Augmented
Augmented
Annotation fileSplit
Merge
Fig. 10: Code running logic.
Raw MiniGPT -4
Llava- 1.5 Qwen -VL
Fig. 11: Visualization of word clouds in captions generated by different MLLMs.

--- PAGE 24 ---
24 Y. Liu et al.
Raw:  detail of the fur collar
MiniGPT -4: The image shows a close-up of a brown surface with a white 
substance on it
Otter : The image shows a brown and white abstract painting
Qwen -VL: The image is brown and white with a black frame
LLaVA -1.5: A brown surface with white streaks
Raw:  a collection of vintage plates from the thrift store makes a charming 
vignette .MiniGPT -4: The image shows a wall with plates hanging on it
Otter : A series of plates is arranged on the wall, forming a letter A
Qwen -VL: Describe the image: A wall with plates and a letter A on it
LLaVA -1.5: A wall with a collection of plates and a letter A
Raw:  row of colored t -shirts in a store
MiniGPT -4: The image shows a rack of colorful t -shirts hanging on hangers in 
a clothing store
Otter : The clothing store offers a wide range of sizes from XS to XL
Qwen -VL: Colorful shirts hanging on a rack in a store.
LLaVA -1.5: A display of shirts in a store
Raw:  dangle earrings with a japanese leaves theme
MiniGPT -4: This is a pair of earrings made of glass and featuring a design of 
flowers and leaves on a silver chainOtter : The earrings are made of light and dark shades of blue and white
Qwen -VL: Dangle earrings with a floral theme in orange and navy blue
LLaVA -1.5: The earrings are made of metal and have a black and silver color
Raw:  fish in a coral reef
MiniGPT -4: The image shows a colorful coral reef with small fish swimming 
around the rocks
Otter : The underwater scene features a colorful coral reef
Qwen -VL: An underwater view of the coral reefs and rocks in the ocean.
LLaVA -1.5: A coral reef with a fish in the center
Raw:  the moon by night on the background of a street lamp
MiniGPT -4: This is an image of a moonlit sky with two moons visible in the 
backgroundOtter : The sky at night, with a full moon shining brightly in the dark
Qwen -VL: The moon and a street light in the night sky.
LLaVA -1.5: The moon is in the sky
Fig. 12: Enhanced Image-text pairs. Interestingly, these MLLMs have different image
recognition capabilities. The visual concepts extracted by different MLLMs enrich the
diversity of captions.

--- PAGE 25 ---
MLLMs-Augmented Visual-Language Representation Learning 25
BLIP(CC3M)：a mural on the 
side of a building
BLIP(Ours)：a street with a car 
driving down it
BLIP(CC3M)：the day
BLIP(Ours)：a city street with tall 
buildings on either side and 
people walking on the sidewalk
BLIP(CC3M)：elephants at a 
waterhole
BLIP(Ours)：an elephant and a 
baby elephant standing in the 
water
BLIP(CC3M)：a person opening 
a bottle of champagne
BLIP(Ours)：a person cutting a 
cake
BLIP(CC3M)：us with our food
BLIP(Ours)：a man and a 
woman sitting at a table with 
plates of food in front of them
BLIP(CC3M)：a bird in a bird 
feederBLIP
(Ours)：a bird in a cage
BLIP(CC3M)：a teddy bear
BLIP(Ours)：a teddy bear 
sitting on top of a table
BLIP(CC3M)：martial artist 
doing a handstand
BLIP(Ours)：a person doing 
something
BLIP(CC3M)：pizza at 
restaurant
BLIP(Ours)：a pizza on the 
table
BLIP(CC3M)：a green and white 
bathroom
BLIP(Ours)：a bathroom with a 
toilet and a shower
BLIP(CC3M)：an airport
BLIP(Ours)：a group of people 
at an airport
BLIP(CC3M)：a baseball game
BLIP(Ours)：a baseball player 
sliding into home plate
BLIP(CC3M)：person skiing
BLIP(Ours)：a person riding 
skis down a snowy slopeBLIP
(CC3M)：the train
BLIP(Ours)：a train with people 
on it
Fig. 13: Using pre-trained BLIP to caption the image. Images are from MSCOCO.
