# 2310.13355.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2310.13355.pdf
# File size: 4749493 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SILC: Improving Vision Language Pretraining with Self-Distillation
Muhammad Ferjad Naeem1⋆Yongqin Xian2*Xiaohua Zhai3*
Lukas Hoyer1,2◦Luc Van Gool1Federico Tombari2,4
1ETH Zurich2Google3Google Deepmind4TU Munich
{mnaeem,lhoyer,vangool }@vision.ee.ethz.ch, {yxian, xzhai,tombari }@google.com
Abstract
Image-Text pretraining on web-scale image caption
datasets has become the default recipe for open vocabulary
classification and retrieval models thanks to the success of
CLIP and its variants. Several works have also used CLIP
features for dense prediction tasks and have shown the
emergence of open-set abilities. However, the contrastive
objective used by these models only focuses on image-text
alignment and does not incentivise image feature learning
for dense prediction tasks. In this work, we introduce SILC,
a novel framework for vision language pretraining. SILC
improves image-text contrastive learning with the simple
addition of local-to-global correspondence learning by self-
distillation. We show that distilling local image features
from an exponential moving average (EMA) teacher model
significantly improves model performance on dense predic-
tions tasks like detection and segmentation, while also pro-
viding improvements on image-level tasks such as classifi-
cation and retrieval. SILC models sets a new state of the
art for zero-shot classification, few shot classification, im-
age and text retrieval, zero-shot segmentation, and open vo-
cabulary segmentation. We further show that SILC features
greatly benefit open vocabulary detection, captioning and
visual question answering.
1. Introduction.
Recent advancements in self-supervised learning [8, 10, 20,
40] and weakly supervised learning on web data [23, 43,
60] has spearheaded the development of foundational lan-
guage [13, 42] and vision-language models [23, 43, 60].
These methods get around the long term challenge of ob-
taining large labelled dataset by developing self-supervision
objectives. Developing open vocabulary computer vision
models that can reason beyond a pre-determined set of
⋆Research Consultant with Google, * Equal advising, ◦Intern at
Google during the project.
Open Vocabulary
Segmentation
(ADE847)Open Vocabulary
Segmentation
(PC459)
Open Vocabulary
Segmentation
(ADE150)
ZeroShot
Segmentation
(ADE150)
ZeroShot
Segmentation
(PC59)
ZeroShot
Segmentation
(VOC20)
Captioning
(COCO)
GQAVQAv2Open Vocabulary
Detection
(COCO)Open Vocabulary
Detection
(LVIS)Retrieval I2T
(COCO)Retrieval T2I
(COCO)ZeroShot
Classification
(ImageNet)
11.612.212.813.4
19.620.220.821.4
33.034.035.036.0
14.415.817.218.6
24.026.028.030.066.8
69.6
72.4
75.2118.0
119.0
120.0
121.051.8
52.6
53.4
54.263.6
64.2
64.8
65.440.6
41.2
41.8
42.431.832.633.434.262.263.464.665.8
44.445.847.248.6
74.675.275.876.4
CLIP SigLIP SILC-C (Ours) SILC-S (Ours)Figure 1. SILC improves image-text contrastive learning with
the addition of local-to-global correspondence learning by self-
distillation. As a result, SILC models learn more locally aware
visual features that are also grounded in language. SILC mod-
els offer significant improvements over CLIP (WebLI) and SigLIP
over a wide variety of computer vision tasks including classifica-
tion, segmentation, detection, captioning, VQA and retrieval.
classes has been a long-term challenge. The introduction
of web image-text datasets and the progress in compute
have enabled significant advances in this field. Popular-
ized by CLIP [43], contrastive pretraining utilizes large
datasets with paired image and text from the web and trains
a vision-language model (VLM) to embed them to a shared
latent space. Since these models are trained on a wide
set of concepts, the learned VLM allows for open vocab-
ulary inference [43]. However, developing open vocabu-
lary dense prediction models for segmentation and detec-
tion is still an open challenge, since internet-scale datasets
do not have dense pixel-level labels. Several works have
found that incorporating VLMs in segmentation and de-
tection models can unlock some open vocabulary abili-
ties [12, 14, 24, 52, 57]. Since CLIP is not trained for these
tasks, these methods get around its limitations by tuning the
1arXiv:2310.13355v2  [cs.CV]  7 Dec 2023

--- PAGE 2 ---
learned model with some dense prediction labelled dataset.
One set of methods utilizes a normal segmentation / detec-
tion model for class agnostic inference and then predict the
class logits with CLIP [12, 31]. Another family of methods
aims to distill VLMs directly into a dense prediction model
and utilize the text transformer to generate the class weights
to predict logits [18, 26]. These works have been highly
impactful towards expanding open vocabulary abilities of
dense prediction models. However, since the contrastive
pretraining objective does not explicitly encourage learning
good local features for dense prediction tasks, these meth-
ods are limited by the VLM’s intrinsic performance [40] as
we also show later in our experiments.
In the self-supervised literature, enforcing local-to-
global consistency by self-distillation has emerged as a
powerful pretraining objective [8, 40, 65] to learn vision
backbones that are competitive on classification as well as
dense prediction tasks, e.g. segmentation and detection.
However, these backbones can not directly be used for zero-
shot or open vocabulary inference as they do not contain
any notion of class or language in the model. In this work,
we propose SILC, which combines the advantages of these
two branches and unifies image-text contrastive pretraining
and local-to-global consistency learning. SILC utilises a
web image-text dataset to learn one model that improves
VLM performance on existing classification and retrieval
tasks while especially improving performance on zero-shot
and open vocabulary segmentation, open vocabulary detec-
tion, captioning and Visual Question Answering (VQA).
Our contributions are as follows: 1. We propose a novel
training framework for VLMs that pairs contrastive pre-
training on image-text data with self-distillation on web im-
ages. 2. While conceptually very simple, we show that
by learning stronger visual features with better local under-
standing, SILC models offer consistent improvements on
multitude of computer vision tasks. These improvements
are especially apparent on tasks that require better local un-
derstanding including zero-shot segmentation, open vocab-
ulary segmentation, open vocabulary detection, captioning
and Visual Question Answering (VQA). 3. We contribute a
new foundation model that sets a new state of the art on
zero-shot classification, few-shot classification, image-to-
text and text-to-image retrieval, zero-shot semantic segmen-
tation and open vocabulary semantic segmentation.
2. Related Works.
Image-Text Pretraining. Vision-language model (VLM)
pretraining [11, 23, 28, 43] aims to learn generic multi-
modal representations that generalize to a wide range of
downstream tasks. Substantial progress has been made
in this field towards better pretraining objectives [23, 50]
and better large-scale image-text dataset [11, 43]. One of
the most popular objective functions is contrastive learn-ing [23, 43] that pulls positive image and text pairs close and
pushes negative ones apart in the joint embedding space.
It is capable of scaling to a large-scale pretraining dataset
and learning highly discriminative image and text features.
Many works [17, 30, 37, 38, 47, 53, 59, 60] in this direc-
tion have demonstrated improvements across zero-shot im-
age classification and retrieval benchmarks.
Another line of research focuses on generative learn-
ing via autoregressive text generation [48–50]. Compared
to the contrastive learning, generative learning usually per-
forms better on text generation tasks e.g., image captioning
and VQA. Finally, there are hybrid methods [1, 27, 28, 32,
46, 56] that combine multiple objective functions including
generative, contrastive and multi-task losses. While many
VLMs [43, 50] mainly focus on learning global image-
text alignment that benefit image-level downstream tasks,
our work aims to develop a new VLM that benefits both
image-level and pixel-level tasks. There have been a few
attempts [15, 16, 33, 63] to improve VLMs for dense pre-
diction tasks including object detection and semantic seg-
mentation. However, they are either modeling the fine-
grained patch-text interactions that are not scalable [16, 33]
or rely on additional bounding box annotations [29, 63]. In
this work we propose to pair image-text contrastive learning
with self-distillation to learn a VLM.
Self-supervised Learning. Self-supervised learning is
another popular pretraining paradigm where features are
learned from image data itself. One branch of methods op-
timize the network to solve pretext tasks e.g., image col-
oring [62], inpainting [41], transformation prediction [19],
and patch ordering [35]. Another family of approaches
adopt instance-level discriminative learning via contrastive
learning [10, 21] and clustering [6, 7]. Recently, [22] shows
that masked autoencoder is also a scalable self-supervised
learner. Our work is inspired by DINO [8] which shows
that segmentation emerges from learning local and global-
views consistency. However, DINO cannot be directly used
for zero-shot and open-vocabulary inference because it only
learns image features. In contrast, our method is trained
on image and text data jointly. We show that together
with text data, the DINO objective allows the model to de-
velop an understanding of local features and their semantic
classes. Therefore our model can potentially directly benefit
far more computer-vision applications.
Zero-shot Semantic Segmentation. Zero-shot semantic
segmentation aims to segment arbitrary visual concepts in
the wild without dense annotations [51]. Methods in this
area rely on image-text pairs from a combination of image
captioning and web image-text dataset. Since these datasets
do not have dense labels, they utilize a self-supervised im-
age region to text attention criterion. Group-VIT [51] pro-
poses to introduce grouping tokens that cluster similar im-
age patches under each group token. MaskCLIP [64] and
2

--- PAGE 3 ---
CLIPpy [44] found that normal CLIP training results in
zero-shot segmentation emerging. ReCo [45] proposes a
refinement process on top of MaskCLIP by retrieval and co-
segmentation. Finally, the current state-of-the-art TCL [9]
learns a decoder to upsample the grounded patch embed-
dings and learns a region to text attention.
Open Vocabulary Segmentation and Detection. Open-
vocabulary semantic segmentation methods aim to segment
images according to a vocabulary of class categories pro-
vided at test-time containing additional unseen classes. In
contrast to zero-shot segmentation, open-vocabulary se-
mantic segmentation has access to a semantic segmenta-
tion dataset with a limited vocabulary for training. Re-
cent methods transfer the open-vocabulary capabilities of
CLIP from image- to pixel-level predictions. LSeg [25]
learns pixel-wise visual embeddings that align with CLIP
text embeddings while OpenSeg [18] learns class-agnostic
segmentation proposals to pool visual features for region-
text grounding. ZegFormer [14] and ZSseg [52] introduce a
two-stage framework, which first learns class-agnostic seg-
mentation mask predictions and classifies the correspond-
ing region using a frozen CLIP. OVSeg [31] further fine-
tunes CLIP on region-text pairs to compensate for the ap-
pearance shift of masked crops. To avoid the overhead of
two stages, CAT-Seg [12] learns the aggregation of cost vol-
umes between text embeddings and dense image embed-
dings from CLIP. Towards open vocabulary detection one
family of methods, e.g. OWVLv2 [34], RegionCLIP [63],
Detic [66], 3Ways [2], pseudolabel boxes for image caption
data to use for localization pretraining. An orthogonal fam-
ily of methods including [29, 54, 55, 61] pretrain models to
align class agnostic pseudoboxes to text as pretraining.
3. Method.
SILC builds on the contrastive pretraining framework of
CLIP [43] and SigLIP [60]. SILC consists of a two-tower
transformer model with a shared embedding space. We
utilize a web-scale paired image-text dataset and rely on
large-scale pretraining to learn the weights of the model.
The first component of our pretraining objective focuses on
aligning matching image-text pairs close together and away
from other images and texts in the batch. This objective
has been incredibly successful in recent literature [43, 60].
However, the contrastive objective in its current form does
not focus on capturing rich local image semantics necessary
for dense prediction tasks like segmentation and detection.
Therefore, we propose to pair the contrastive pretraining
objective with a local-to-global consistency objective that
uses self-distillation as shown in Figure 2. SILC gets its
name from the two training objectives consisting of Self-
Distillation from Images and Image- Language Contrastive
Alignment from Image-Text pairs.
Image Encoder 
(Teacher) A cute cat 
EMA 
Image Encoder 
(Student) Text Encoder 
Projection 
Projection 
& Centering Stop gradient 
Global-view 
Local-view Text
Local crop 
Global crop 
Figure 2. SILC is a two-tower transformer based VLM. The first
component of our training objective uses a global view of an image
covering a large area and its paired caption to optimise a batch-
wise contrastive loss for images and texts. The second compo-
nent of our training objective enforces local-to-global consistency
by self-distillation between the main model (the student) and an
Exponential Moving Average (EMA)-based teacher. This local-
to-global correspondence additionally allows the model to learn
good visual features. Together the two objectives allow the model
to excel at both traditional VLM tasks as well as tasks that require
local understanding like segmentation and detection.
3.1. Aligning Image and Text.
The contrastive pretraining objective relies on the Info-
NCE framework [39]. It utilizes large amount of web-
scale image-text dataset to learn an alignment between
paired image and text. Given a minibatch B=
{(I1, T1),(I2, T2), . . .}, where (Ii, Ti)denotes a matching
pair of image and text, the contrastive objective encour-
ages matching image and text pairs to lie close together
in a shared embedding space. The image Iiis processed
by a learnable Vision Transformer Fto get its feature em-
bedding. Similarly, the tokenized text Tiis processed by
a learnable Text Transformer Gto get its feature embed-
ding. These feature embeddings are normalized by their
l2norm to get fi=F(Ii)
∥F(Ii)∥2∈RJfor the image Iiand
gi=G(Ti)
∥G(Ti)∥2∈RJfor the paired text Tiwhere Jis the
feature dimension of the shared embedding space. The dot
product of fiandgicomputes their cosine similarity and is
optimized with a pair of cross-entropy losses as proposed
by CLIP [43] or a sigmoid loss as proposed by SigLIP [60].
The batch-wise contrastive losses of CLIP/ SigLIP, repre-
sented as Limage−text, rely on a large batch size to align
image-text pairs. This objective tuned over a large amount
of data learns a shared embedding space between image and
text and thus can be used for zero-shot transfer to multitude
of computer vision tasks.
3.2. Distilling Local Image Features.
The image-text contrastive loss has shown to be very suc-
cessful in learning zero-shot transfer models [23, 43]. Mod-
els learned with this objective have also been used to im-
prove dense prediction tasks like open vocabulary segmen-
3

--- PAGE 4 ---
tation and detection. However, the contrastive objective
alone does not explicitly focus on learning good visual fea-
tures for dense prediction tasks. These tasks require lo-
cal image semantics to be sufficiently encoded in the out-
put image and patch embeddings. Enforcing local-to-global
consistency has emerged as a powerful technique to accom-
plish this on large unlabelled image data [8, 40, 65] in self-
supervision literature. However, these methods can not be
directly used for open vocabulary models as they are trained
without any language information. In the second compo-
nent of our training framework, we take inspiration from
this subset of literature and additionally add local-to-global
consistency as a training objective for images in our image-
text dataset.
The basic idea of this objective is as follows. A teacher
network gets a global view of the image representing the
scene as a whole and produces a feature embedding. A
student model gets a partial view of the same image and
produces a feature embedding. A self-distillation objective
is introduced where the student needs to match the predic-
tion of the teacher while only having partial information.
This enforces the model to learn local semantics and their
relation to global semantics of the scene. We add this cri-
terion for the image encoder F. We add a projection as a
learnable MLP on top of the image encoder to map from
the original shared embedding space of dimension JtoK
where K > J . The student FSis the main image encoder
with a learnable projection head. Since we rely on noisy
web scale image-text data, we do not have an oracle teacher
for the student to match. We therefore construct our teacher
FTas a exponential moving average of the student FSfrom
the previous training iterations to realize our self-distillation
framework:
FT←λFT+ (1−λ)FS, (1)
where λcontrols the update step of the teacher. For a given
image Ii, the teacher processes its global crop to produce
pt
i∈RKand the student processes its local crop to pro-
duce ps
i∈RK. To prevent the teacher from collapsing
to a trivial solution, we apply sharpening on the outputs of
teacher with τtand student with τs. To encourage each fea-
ture dimension to contribute to the output feature, we addi-
tionally introduce a centering operation on the prediction of
the teacher. The centering term c∈RKis initialized with
0and is updated by a momentum update with a factor of m
with the first order batch statistics of the teacher’s prediction
at each step as follows: c←mc+ (1−m)1
|B|P|B|
i=1pt
i.
To learn local-to-global correspondences, the student is
faced with an information asymmetry. The student is given
a local view of an image which is realized as a random
crop over a small region of the image. The teacher, how-
ever, has access to a global view of the image containing
more information about the scene. The student is taskedwith matching the semantics of the teacher while only hav-
ing partial information. Therefore, for a given image, the
model needs to learn local semantics of the image and how
it would fit in the global context of this image. This is real-
ized as a knowledge-distillation loss where the student and
the teacher’s feature vectors are first converted to a prob-
ability distribution by applying a softmax on the teacher
prediction Pt(Igl
i) =softmax ((pt
i−c)/τt)and student
prediction Ps(Ilc
i) =softmax (ps
i/τs). The student is op-
timized to match the teacher with a cross-entropy loss,
Lself−dist=−Pt(Igl
i)⊺log(Ps(Ilc
i)). (2)
This self-distillation objective incentivises the image en-
coder to learn local semantics of images over the large web
scale dataset. Since the teacher is constructed with the stu-
dent’s weights, and the image level features are pooled from
patch embeddings in a Vision Transformer, this allows for
richer local semantics to be captured in the image level as
well as the patch level features.
While this objective has been explored in self-supervised
learning [8, 40], to the best of our knowledge, we are the
first work to show its complimentary nature to image-text
contrastive learning on web-scale dataset. We show that
when combined with text, this objective allows the model to
develop a local understanding of the semantics of an image
grounded in language. We find two important modifications
compared to previous works that allows it to be complimen-
tary to image-text contrastive learning. 1. Each global view
used in Lself−distneeds to be aligned with text, otherwise
the two objectives diverge. This is realized by computing
the image-text contrastive loss for each global view while
maintaining the same batch size. 2. The momentum sched-
uler of the EMA should not converge to 1.0. Otherwise the
teacher stops learning from image-text loss as the update
step becomes too small in the later stage of the training. We
therefore use a fixed momentum.
4. Experiments.
We compare our SILC pretraining framework with both
CLIP [43] and SigLIP [60] on the same test bench and per-
form extensive experimentation. SILC models based on the
CLIP objective are represented by SILC-C and the SigLIP
versions are represented by SILC-S . We show that SILC
sets a new state of the art on a variety of tasks: zero-
shot classification, few-shot classification, retrieval, zero-
shot segmentation and open vocabulary segmentation. We
further show that SILC models also improve other local se-
mantic understanding tasks including open vocabulary de-
tection, captioning and VQA.
4.1. Implementation Details.
We implement our model in jax in the bigvision code-
base [3, 4], following the contrastive pretraining setups
4

--- PAGE 5 ---
Zero-Shot Classification Few-shot classification Retrieval
Model ImageNet CIFAR100 ImageNet CIFAR100 COCO
T1 T1 1shot 5shot 10shot 1shot 5shot 10shot I2T@1 T2I@1
CLIP (WebLI) [60] 74.1 68.4 42.8 63.2 67.3 39.4 59.6 64.6 61.7 43.9
SILC-C* (Ours) 75.3 71.0 44.6 64.3 67.8 42.8 64.6 69.6 62.5 44.9
SILC-C (Ours) 76.2 72.3 45.3 65.0 68.5 45.2 66.9 71.3 66.1 49.1
SigLIP [60] 75.1 69.8 44.0 64.2 68.4 39.0 61.7 66.3 62.6 44.9
SILC-S*(Ours) 75.8 69.2 45.2 64.6 68.4 40.3 63.3 67.4 63.0 44.6
SILC-S(Ours) 76.6 70.6 45.9 65.2 68.9 41.8 64.9 68.9 66.2 48.7
Table 1. Comparing SILC* with baselines , we observe that our pretraining framework results in a significant improvement over both
CLIP and SigLIP objectives. We reproduce both CLIP and SigLIP on the same WebLI dataset [11] to quantify the improvements from our
proposed training objective. We further finetune SILC* on a cleaner subset to get our final model SILC and see that it unlocks additional
performance without significant extra retraining. The best performance for each variant is bolded , the second best is underlined .
from [60], and use the WebLI dataset[11] for our exper-
iments. We utilize two global views cropped between
(0.4−1.0)of the original image area and eight local views
cropped between (0.05−0.4)of the original image area
for the self-distillation loss. The global views are resized to
(256×256) and the local views are resized to (96×96). The
teacher momentum λis kept fixed at 0.966and the center
update momentum mis kept fixed at 0.9through the train-
ing. The teacher temperature τtis fixed at 0.04 and the stu-
dent temperature τsis fixed at 0.1.Kis65536 . We resize
the original image to (256×256) for the contrastive loss
between image-text pairs. We trained with a batch size of
16k on Google TPUs. We use example-seen to represent
how many image and text pairs are drawn from the dataset
throughout the training. We train all baselines in our main
comparisons in Table 1 for 20 Billion example-seen on the
WebLI dataset [11] following [60]. Our models trained on
WebLI are marked as SILC* . We use a rsqrt learning sched-
uler [58] with base learning rate of 0.001with50000 warm
up and 50000 cooldown steps. Additional training details
including compute cost are discussed in the supplementary.
We additionally finetune our model using a smaller but
cleaner WebLI subset [11] for 1 Billion additional example-
seen and represent this model as SILC . The smaller We-
bLI subset contains 100 million image-text pairs with finer-
grained text filters etc.
4.2. Classification and Retrieval.
We compare our pretraining framework with CLIP and
SigLIP under the same training and evaluation protocol in
Table 1. We compare at ViT/B16 and see that the introduc-
tion of self-distillation to both consistently improve their
performance on zero-shot classification, few shot classifica-
tion and retrieval. On zero-shot classification on ImageNet,
SILC-C* improves on CLIP (WebLI) by 1.2 points, simi-
larly we notice an improvement of 2.6 points on CIFAR-100
showing the benefit of local feature self-distillation. Similarimprovements are noted for few-shot classification where
SILC-C* improves over CLIP (WebLI) by 1.8, 1.1 and 0.5
points on ImageNet 1 shot, 5 shot and 10 shot classifica-
tion respectively. We make similar observation on retrieval
where SILC-C* shows improvements on image to text as
well as text to image retrieval. Moving to SigLIP versions
of the model, we see a similar trend where the introduction
of self-distillation objective allows SILC-S* to consistently
improve almost all metrics over the evaluated tasks. We
therefore conclude that capturing better local semantics re-
sults in learning stronger visual features which also helps
tasks that require global understanding of the image.
Comparing SILC* models with SILC, we notice that the
finetuning on the cleaner subset unlocks additional perfor-
mance for the model without significant extra training. For
the CLIP based SILC-C, We notice another 0.9 point im-
provement over SILC-C* on zero-shot ImageNet classifica-
tion. We observe improvements of the same magnitude on
few-shot classification. Comparing retrieval performance,
we see a significant increase in retrieval performance on
COCO where SILC-C achieves a 3.6 and 4.2 points im-
provement on Image to Text and Text to Image Recall@1.
The SigLIP based SILC-S follows a similar trend and con-
sistently improves on SILC-S* on all metrics. SILC models
set a new state-of-the-art for these tasks at ViT/B16 model
size. We also compare with open-source CLIP variants in
the supplementary and show SILC’s superior performance.
4.3. Zero-Shot Semantic Segmentation.
Zero-shot semantic segmentation aims to measure the
grounding performance of a VLM usually from its patch
embeddings. MaskCLIP [64] and CLIPpy [44] found
that this grounding naturally emerges as a consequence of
image-text contrastive training. We use a Vision Trans-
former with a MAP pooling head [58]. We observe that
grounding for our model emerges in the values of the MAP
head instead of the last encoder block. For a given set of
5

--- PAGE 6 ---
Image CLIP SILC-C G. Truth Image CLIP SILC-C G. TruthA-150
 PC-59
Figure 3. Qualitative results on zero-shot segmentation show that SILC-C achieves significant improvements over CLIP (WebLI).
SILC-C produces less noisy segmentation and better distinguishes semantic classes. This semantic segmentation emerges without any
segmentation supervision.
Model A-150 PC-59 Cityscapes VOC-20 COCO-Stuff
GroupVIT [51] 9.2 23.4 11.1 79.7 11.1
MaskCLIP [64] 9.8 26.4 12.6 74.9 16.4
ReCo [45] 11.2 22.3 21.1 57.7 14.8
TCL [9] 14.9 30.3 23.1 77.5 19.6
CLIP (WebLI) [60] 15.0 24.0 22.6 69.5 15.0
SILC-C* (Ours) 17.2 29.3 25.1 73.5 18.2
SILC-C (Ours) 19.3 31.6 26.9 77.5 20.8
SigLIP [60] 13.6 22.9 20.8 64.7 13.4
SILC-S* (Ours) 16.7 28.6 23.4 72.1 17.3
SILC-S (Ours) 18.6 30.9 25.2 76.3 19.7
Table 2. Comparing Zero-Shot Segmentation performance
we see that SILC* models trained on noisy web image-text data
already outperform several ZS segmentation baselines that use
cleaner image-text data. When we tune our model on a cleaner
subset of image-text data to get SILC-C, we see that it sets the ab-
solute state-of-the-art on 4 out of 5 datasets.
possible classes in a segmentation dataset, we obtain the
corresponding text embeddings by querying our text en-
coder with a standard prompt. We compute the cosine simi-
larity between the image patch embeddings and the text fea-
tures of each class name to generate a segmentation map in
zero-shot. We report the mean-IOU (mIOU) performance
of our model in Table 2 and compare with baselines at
ViT/B16 similar to previous works. We follow the eval-
uation protocol of TCL [9] without the background class.
However, we do not use any post-refinement e.g. PAMR as
we argue that the raw segmentation of a VLM is the true
depiction of its zero-shot performance.
Comparing against CLIP and SigLIP , we see that both
SILC-C* and SILC-S* show significantly superior zero-
shot semantic segmentation performance. In fact, both vari-
ants achieve multiple mIOU points improvements over all
5 datasets. This validates our hypothesis that the combina-
tion of image-text contrastive learning and local-to-global
correspondence learning allows the model to develop bet-
ter understanding of local semantics of the image grounded
in language. From Table 2, we observe that the CLIP ob-jective in general results in superior zero-shot segmentation
than the SigLIP objective. This is also apparent as we com-
pare SILC-C* with SILC-S*. Moreover, we observe that
finetuning on a cleaner subset further improves the zero-
shot segmentation performance of both SILC-C and SILC-
S. We observe that the CLIP variant SILC-C also outper-
forms SILC-S here. We show the improvements of SILC
on CLIP (WebLI) qualitatively in Figure 3. We can ob-
serve that SILC is better at segmenting and labeling seman-
tic classes in images. We would like to emphasize that SILC
achieves this without any segmentation ground truth.
Comparing with current SOTA , we observe that SILC-
C consistently beats all specialized zero-shot segmenta-
tion baselines on 4/5 datasets to set a new state of the
art (SOTA). Compared to the previous state of the art TCL,
SILC achieves a remarkable 4.3 mIOU points improvement
on A-150, 2.9 points improvement on PC-59, and 4.9 points
improvement on CityScapes. Similar improvements are
noted on VOC-20 and COCO-Stuff, however Group-VIT
maintains the best result on VOC-20. These methods use
relatively cleaner image captioning datasets for their seg-
mentation specific training objectives. We noticed that the
improvements in zero-shot segmentation are achievable by
just finetuning on a cleaner subset of data. We did not ob-
serve superior performance by learning an expensive patch-
wise attention as proposed by PACL [36]. We show in
the supplementary that SILC models also outperform PACL
trained on WebLI. Methods like TCL and ReCo, which
are designed to improve the zero-shot segmentation perfor-
mance of a frozen VLM, can in theory further improve the
performance of our model. However, since we aim to im-
prove vision-language pretraining over all tasks, this is out
of the scope of this work.
4.4. Open-Vocabulary Semantic Segmentation.
Open V ocabulary Semantic Segmentation aims to develop
segmentation models that can segment novel classes beyond
the training vocabulary. Most recent methods in this area
6

--- PAGE 7 ---
VLM Method A-847 PC-459 A-150 PC-59 VOC-20 VOC-21
CLIP-B/16 ZegFormer [14] 5.6 10.4 18.0 45.5 89.5 65.5
CLIP-B/16 ZSseg [52] 7.0 - 20.5 47.7 88.4 -
CLIP-B/16 OVSeg [31] 7.1 11.0 24.8 53.3 92.6 -
CLIP-B/16 CAT-Seg [12] 8.4 16.6 27.2 57.5 93.7 78.3
SILC-C-B/16 CAT-Seg [12] 13.4 (+5.0) 22.0 (+5.4) 36.6 (+9.4) 61.2 (+3.7) 95.9 (+2.2) 80.4 (+2.1)
SILC-S-B/16 CAT-Seg [12] 13.5 (+5.1) 21.9 (+5.3) 37.0 (+9.8) 61.2 (+3.7) 96.1 (+2.4) 80.9 (+2.6)
CLIP-L/14 ZSseg [52] 7.1 10.2 21.7 52.2 92.3 -
CLIP-L/14 OVSeg [31] 9.0 12.4 29.6 55.7 94.5 -
CLIP-L/14 CAT-Seg [12] 10.8 20.4 31.5 62.0 96.6 81.8
SILC-C-L/16 CAT-Seg [12] 15.0 (+4.2) 25.8 (+5.4) 37.7 (+6.2) 63.5 (+1.5) 97.6 (+1.0) 82.5 (+0.7)
CLIP-G/14 CAT-Seg [12] 13.3 21.4 36.2 61.5 97.1 81.4Table 3. Comparing Open
Vocabulary Semantic Seg-
mentation performance , we
observe that SILC models im-
prove over CLIP by signif-
icant margins on all unseen
test sets. SILC particularly
improves the performance for
challenging test sets with large
vocabularies. SILC-L/16 even
outperforms the much larger
CLIP-G/14. All models are
trained on COCO-Stuff.
Image CLIP SILC-C G.Truth Image CLIP SILC-C G.TruthA-150
 PC-459
Figure 4. Comparing qualitative examples for open vocabulary segmentation , we observe that SILCw/ CAT-Seg better distinguishes
semantically similar classes such as field/grass, runway/road, grandstand/chair and sand/water than CLIP.
Initialization Training data COCO LVIS
AP AP allAP rare
CLIP (WebLI) WebLI N-grams 40.4 31.9 29.2
SILC-C*(Ours) WebLI N-grams 41.8 33.3 30.4
SigLIP WebLI N-grams 40.9 32.8 30.4
SILC-S*(Ours) WebLI N-grams 42.7 34.2 32.4
Table 4. Training OWLv2 for Object Detection with SILC
models offers consistent improvement over CLIP and SigLIP for
open vocabulary object detection. These models are trained with
pseudo labels from WebLI N-grams [34] and evaluated zero-shot
on COCO and LVIS.
Model Classification Captioning Question Ans.
ImageNet SUN397 COCO GQA VQAv2
CLIP (WebLI) 82.3 82.4 118.1 52.5 63.5
SILC-C*(Ours) 83.8 83.4 120.8 53.1 64.6
SigLIP 82.5 82.2 117.5 51.9 63.0
SILC-S*(Ours) 83.7 82.9 121.2 53.2 64.5
Table 5. Evaluating SILC visual representation with LiT-
Decoder in a multi-task setup, we observe consistent improve-
ments on all tasks compared to CLIP and SigLIP. These improve-
ments are especially apparent for tasks that require local under-
standing of the image i.e. Captioning and Question Answering.rely on a pretrained CLIP due to its open-vocabulary capa-
bilities and adapt it for segmentation task. To evaluate the
open vocabulary segmentation potential of SILC, we take
the current state-of-the-art model CAT-Seg [12] and replace
the CLIP model used by the authors with SILC. The mod-
els are trained on COCO-Stuff-164k with 172 classes and
tested on unseen datasets with different vocabularies: ADE-
20k with 847 or 150 classes (A-847/A-150), Pascal Context
(PC-459/PC-59), and Pascal VOC (VOC-20/VOC-21).
From Table 3, we observe that SILC significantly im-
proves over CLIP [43]. In fact, SILC-C-B/16 performs on
par with the much bigger CLIP-G/14 on the three most chal-
lenging test datasets A-847, PC-459 and A-150. Moreover,
we observe that while SILC-S performed slightly worse
than SILC-C in zero-shot segmentation, it achieves slightly
better performance when trained for open vocabulary seg-
mentation. SILC-S-B/16 further improve on the perfor-
mance of SILC-C-B/16. The observed improvements of
SILC-C also transfer to the larger ViT-L variant, where
CAT-Seg with SILC-C-L/16 outperforms CAT-Seg with
CLIP-L/14 on all datasets by a significant margin. In par-
ticular, it achieves more than +4 mIOU improvement on the
challenging A-847, PC-459, and A-150. SILC-L/16 even
significantly outperforms the much bigger CLIP-G/14 on
all tested datasets. The improvements of SILC-C over CLIP
are also reflected in the qualitative examples in Fig. 4. We
7

--- PAGE 8 ---
Model ImageNet 0 shot ImageNet Few shot COCO Retrieval ZS Segmentation Open Vocab Seg
T1 1shot 5shot 10shot I2T@1 T2I@1 A-150 Stuff PC-59 PC-459 A-150 PC-59
CLIP (WebLI) 71.7 36.4 57.7 62.5 59.1 42.9 11.8 12.9 20.1 18.6 30.5 57.7
+ additional views 73.6 38.7 60.8 65.7 60.6 43.2 11.7 13.0 20.0 19.2 32.1 57.8
+ EMA 73.7 38.4 60.7 65.5 61.3 43.1 11.9 13.3 20.5 19.0 32.2 57.5
+ Self Dist (SILC-C* ) 74.3 39.9 61.2 65.7 62.7 43.9 12.2 15.3 21.1 21.0 33.3 60.7
Table 6. We ablate over each component of our model to verify our design choices. The addition of image augmentation and EMA to
CLIP (WebLI) improves classification and retrieval metrics while only slightly impact the segmentation. Adding local-to-global consistency
by self-distillation, we observe an improvement across the board especially on segmentation metrics.
observe that SILC-C better distinguishes semantically sim-
ilar classes such as grandstand/building, field/grass, run-
way/road and grandstand/chair. Further, it improves seg-
mentation in difficult cases and better handles transparent
segments as shown in supplementary. Results on addi-
tional WebLI models are also provided in supplementary
with similar conclusions.
4.5. SILC for Open-Vocabulary Detection.
We utilize OWLv2 [34] as a framework to test the open
vocabulary detection potential of SILC models. OWLv2
initializes a detection model with a contrastive image-text
model’s weights and utilizes pseudo labelled boxes from
WebLI (WebLI N-grams [34]) to learn an open vocabulary
detection model. We utilize the test bench of the authors and
retrain OWLv2 initialized from our VLM baselines to re-
port results in Table 4. We evaluate these models zero-shot
on COCO and LVIS without doing any finetuning on re-
spective dataset to test their open vocabulary performance.
From Table 4 we observe that SILC models also benefit
open vocabulary detection thanks to learning better local
semantics. SILC-S* achieves an improvement of +1.8AP
on COCO. The improvements are also consistent on the
challenging LVIS benchmark where SILC-S* achieves an
improvement of +1.4AP on all classes and a remarkable
+2.0AP on rare classes. We make similar observations as
we compare SILC-C* with CLIP (WebLI) where SILC-C*
offers consistent improvements. This further validates that
SILC models offer better performance for dense tasks.
4.6. Evaluating SILC features with LiT-Decoder.
LiT-Decoder [5] proposes to utilise a frozen image encoder
and train a single autoregressive decoder to learn a multi-
task model for Classification, Captioning and Visual Ques-
tion Answering. We use LiT-Decoder as a framework to
evaluate the quality of visual representations learned by
SILC models against baseline CLIP (WebLI) and SigLIP.
We use the authors’ implementation and only replace the
ViT with the respective baselines. We report results in Ta-
ble 5. We observe that SILC models offer consistent im-
provements in this multi-task setup as well. Compared to
SigLIP, SILC-S* improves classification by +1.2 points onImageNet and +1.0 point on SUN397. The improvements
are even more profound on captioning (+3.6 CIDEr score on
COCO) and Visual Question Answering (+1.3 on GQA and
+1.5 on VQAv2). These tasks greatly benefit from SILC
features’ ability to better encode local semantics. Similar
improvements are noted for CLIP based baselines. This fur-
ther validates that SILC features can simultaneously benefit
multiple computer vision problems.
4.7. Ablation on Model Components.
We ablate on the various design choices of our model and
their impact on various tasks. We train all models for 5
Billion example-seen and report the performance in Ta-
ble 6. Since our method processes additional image aug-
mentations in the contrastive loss, we first test if our im-
provements are a consequence of processing more augmen-
tations. We observe that the introduction of additional im-
age augmentations (second row) improve the classification
and retrieval metrics but their impact on zero-shot segmen-
tation and open vocabulary segmentation is not as signifi-
cant. When we add an EMA over this model’s weights simi-
lar to our model (third row), we notice a slight improvement
as seen in previous SSL literature. Finally when we add the
self-distillation from local crops, we see an improvement
across the board on all tasks. In particular, we observe the
strongest improvement on segmentation tasks highlighting
our proposal’s impact on them.
5. Conclusion.
We propose to integrate local-to-global correspondence
learning by self-distillation as a complementary objective
to the popular VLM contrastive objective of CLIP [43] and
SigLIP [60]. We show that the introduction of this re-
sults in remarkable performance improvements on several
computer vision tasks. We see a consistent performance
improvement on zero-shot classification, few-shot classifi-
cation, and retrieval. We further test our VLM on zero-
shot segmentation and show that our training framework re-
sults in significant improvements without using any dense
ground truth. Finally we show that SILC models as pre-
trained backbones significantly improve a model’s perfor-
8

--- PAGE 9 ---
mance on open vocabulary segmentation, open vocabulary
detection, captioning and VQA. SILC models set a new
state of the art in Vision-Language Foundational Models.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. In NeurIPS ,
2022. 2
[2] Relja Arandjelovi ´c, Alex Andonian, Arthur Mensch,
Olivier J H ´enaff, Jean-Baptiste Alayrac, and Andrew Zisser-
man. Three ways to improve feature alignment for open vo-
cabulary detection. arXiv preprint arXiv:2303.13518 , 2023.
3
[3] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big
vision. https://github.com/google-research/
big_vision , 2022. 4
[4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter plain vit baselines for imagenet-1k, 2022. 4
[5] Lucas Beyer, Bo Wan, Gagan Madan, Filip Pavetic, An-
dreas Steiner, Alexander Kolesnikov, Andr ´e Susano Pinto,
Emanuele Bugliarello, Xiao Wang, Qihang Yu, et al. A study
of autoregressive decoders for multi-tasking in computer vi-
sion. arXiv preprint arXiv:2303.17376 , 2023. 8
[6] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In ECCV , 2018. 2
[7] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learn-
ing of visual features by contrasting cluster assignments. In
NeurIPS , 2020. 2
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
ICCV , 2021. 1, 2, 4
[9] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learn-
ing to generate text-grounded mask for open-world semantic
segmentation from only image-text pairs. In CVPR , 2023. 3,
6
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 1, 2
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,
Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James
Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,
Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and
Radu Soricut. PaLI: A jointly-scaled multilingual language-
image model. In ICLR , 2023. 2, 5
[12] Seokju Cho, Heeseong Shin, Sunghwan Hong, Seungjun
An, Seungjun Lee, Anurag Arnab, Paul Hongsuck Seo,
and Seungryong Kim. Cat-seg: Cost aggregation foropen-vocabulary semantic segmentation. arXiv preprint
arXiv:2303.11797 , 2023. 1, 2, 3, 7
[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311 , 2022. 1
[14] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. De-
coupling zero-shot semantic segmentation. In CVPR , 2022.
1, 3, 7
[15] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang,
Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang,
Lu Yuan, Dong Chen, et al. Maskclip: Masked self-
distillation advances contrastive language-image pretraining.
InCVPR , 2023. 2
[16] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang,
Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu, Yann Le-
Cun, Nanyun Peng, et al. Coarse-to-fine vision-language
pre-training with fusion in the backbone. In NeurIPS , 2022.
2
[17] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig
Schmidt, Alexander Toshev, and Vaishaal Shankar. Data fil-
tering networks. arXiv preprint arXiv:2309.17425 , 2023. 2
[18] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scal-
ing open-vocabulary image segmentation with image-level
labels. In ECCV . Springer, 2022. 2, 3
[19] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. In ICLR , 2018. 2
[20] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. NeurIPS , 2020. 1
[21] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In CVPR , 2020. 2
[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In CVPR , 2022. 2
[23] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 1, 2, 3
[24] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and
Anelia Angelova. F-vlm: Open-vocabulary object detection
upon frozen vision and language models. ICLR , 2023. 1
[25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In International Conference on Learning Rep-
resentations , 2022. 3
[26] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Rene Ranftl. Language-driven semantic seg-
mentation. In International Conference on Learning Rep-
resentations , 2022. 2
[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
9

--- PAGE 10 ---
Align before fuse: Vision and language representation learn-
ing with momentum distillation. In NeurIPS , 2021. 2
[28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML ,
2022. 2
[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In CVPR , 2022. 2, 3
[30] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli
Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-
pervision exists everywhere: A data efficient contrastive
language-image pre-training paradigm. In ICML , 2021. 2
[31] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan
Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana
Marculescu. Open-vocabulary semantic segmentation with
mask-adapted clip. In CVPR , 2023. 2, 3, 7
[32] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:
Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks. In NeurIPS , 2019. 2
[33] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He,
and Tianrui Li. Segclip: Patch aggregation with learn-
able centers for open-vocabulary semantic segmentation. In
ICLR , 2023. 2
[34] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby.
Scaling open-vocabulary object detection. arXiv preprint
arXiv:2306.09683 , 2023. 3, 7, 8
[35] Ishan Misra and Laurens van der Maaten. Self-supervised
learning of pretext-invariant representations. In CVPR , 2020.
2
[36] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang,
Ashish Shah, Philip HS Torr, and Ser-Nam Lim. Open
vocabulary semantic segmentation with patch aligned con-
trastive learning. In CVPR , 2023. 6
[37] Muhammad Ferjad Naeem, Yongqin Xian, Luc V Gool, and
Federico Tombari. I2dformer: Learning image to document
attention for zero-shot image classification. NeurIPS , 2022.
2
[38] Muhammad Ferjad Naeem, Muhammad Gul Zain Ali Khan,
Yongqin Xian, Muhammad Zeshan Afzal, Didier Stricker,
Luc Van Gool, and Federico Tombari. I2mvformer: Large
language model generated multi-view document supervision
for zero-shot image classification. In CVPR , 2023. 2
[39] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 3
[40] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193 , 2023. 1, 2, 4
[41] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In CVPR , 2016. 2
[42] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gen-
erative pre-training. In OpenAI , 2018. 1[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICLR , 2021. 1, 2, 3, 4, 7, 8
[44] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi,
Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Per-
ceptual grouping in contrastive vision-language models. In
ICCV , 2023. 3, 5
[45] Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Re-
trieve and co-segment for zero-shot transfer. In NeurIPS ,
2022. 3, 6
[46] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. Flava: A foundational language and vision
alignment model. In CVPR , 2022. 2
[47] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue
Cao. Eva-clip: Improved training techniques for clip at scale.
arXiv preprint arXiv:2303.15389 , 2023. 2
[48] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xi-
aohua Zhai, Neil Houlsby, and Lucas Beyer. Image cap-
tioners are scalable vision learners too. arXiv preprint
arXiv:2306.07915 , 2023. 2
[49] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
Git: A generative image-to-text transformer for vision and
language. TMLR , 2022.
[50] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision. In ICLR , 2022. 2
[51] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision. In
CVPR , 2022. 2, 6
[52] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue
Cao, Han Hu, and Xiang Bai. A simple baseline for open-
vocabulary semantic segmentation with pre-trained vision-
language model. In ECCV . Springer, 2022. 1, 3, 7
[53] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. In ICLR , 2021. 2
[54] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan
Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.
Detclip: Dictionary-enriched visual-concept paralleled pre-
training for open-world detection. NeurIPS , 2022. 3
[55] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei
Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scal-
able open-vocabulary object detection pre-training via word-
region alignment. In CVPR , 2023. 3
[56] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. In TMLR ,
2022. 2
[57] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-
Chieh Chen. Convolutions die hard: Open-vocabulary seg-
mentation with single frozen convolutional clip. NeurIPS ,
2023. 1
10

--- PAGE 11 ---
[58] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. In CVPR , 2022. 5
[59] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
CVPR , 2022. 2
[60] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
InICCV , 2023. 1, 2, 3, 4, 5, 6, 8
[61] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun
Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-
Neng Hwang, and Jianfeng Gao. Glipv2: Unifying local-
ization and vision-language understanding. NeurIPS , 2022.
3
[62] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In ECCV , 2016. 2
[63] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan
Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang
Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based
language-image pretraining. In CVPR , 2022. 2, 3
[64] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In ECCV , 2022. 2, 5, 6
[65] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan Yuille, and Tao Kong. Image BERT pre-training
with online tokenizer. In ICLR , 2022. 2, 4
[66] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV . Springer,
2022. 3
11
