# 2310.16764.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/multimodal/2310.16764.pdf
# Kích thước tệp: 366746 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
2023-10-26
ConvNets Sánh ngang Vision Transformers ở Quy mô Lớn
Samuel L Smith1, Andrew Brock1, Leonard Berrada1và Soham De1
1Google DeepMind
Nhiều nhà nghiên cứu tin rằng ConvNets hoạt động tốt trên các tập dữ liệu nhỏ hoặc vừa phải, nhưng không 
cạnh tranh được với Vision Transformers khi có quyền truy cập vào các tập dữ liệu ở quy mô web. Chúng tôi thách thức 
niềm tin này bằng cách đánh giá một kiến trúc ConvNet hiệu suất cao được tiền huấn luyện trên JFT-4B, một tập dữ liệu 
lớn có nhãn của các hình ảnh thường được sử dụng để huấn luyện các mô hình nền tảng. Chúng tôi xem xét ngân sách 
tính toán tiền huấn luyện từ 0,4k đến 110k giờ lõi TPU-v4, và huấn luyện một loạt các mạng có độ sâu và chiều rộng 
tăng dần từ họ mô hình NFNet. Chúng tôi quan sát một quy luật tỷ lệ log-log giữa mất mát trên tập giữ lại và ngân sách 
tính toán. Sau khi tinh chỉnh trên ImageNet, NFNets sánh ngang hiệu suất báo cáo của Vision Transformers với ngân sách 
tính toán tương đương. Mô hình tinh chỉnh mạnh nhất của chúng tôi đạt độ chính xác Top-1 là 90,4%.
Từ khóa: ConvNets, CNN, Convolution, Transformer, Vision, ViTs, NFNets, JFT, Scaling, Image

Giới thiệu
Mạng Nơ-ron Tích chập (ConvNets) đã chịu trách nhiệm cho nhiều thành công ban đầu của học sâu. ConvNets sâu đã được triển khai thương mại lần đầu tiên hơn 20 năm trước (LeCun et al., 1998), trong khi thành công của AlexNet trong thử thách ImageNet năm 2012 đã tái khơi dậy sự quan tâm rộng rãi trong lĩnh vực này (Krizhevsky et al., 2017). Trong gần một thập kỷ ConvNets (thường là ResNets (He et al., 2016a,b)) đã thống trị các điểm chuẩn thị giác máy tính. Tuy nhiên trong những năm gần đây chúng ngày càng được thay thế bởi Vision Transformers (ViTs) (Dosovitskiy et al., 2020).

Đồng thời, cộng đồng thị giác máy tính đã chuyển từ chủ yếu đánh giá hiệu suất của các mạng được khởi tạo ngẫu nhiên trên các tập dữ liệu cụ thể như ImageNet, sang đánh giá hiệu suất của các mạng được tiền huấn luyện trên các tập dữ liệu lớn đa mục đích được thu thập từ web. Điều này đặt ra một câu hỏi quan trọng; liệu Vision Transformers có vượt trội hơn các kiến trúc ConvNet được tiền huấn luyện với ngân sách tính toán tương tự?

Mặc dù hầu hết các nhà nghiên cứu trong cộng đồng tin rằng Vision Transformers cho thấy thuộc tính tỷ lệ tốt hơn ConvNets, nhưng có ít bằng chứng đáng ngạc nhiên để hỗ trợ cho tuyên bố này. Nhiều bài báo nghiên cứu ViTs so sánh với các đường cơ sở ConvNet yếu (thường là kiến trúc ResNet gốc (He et al., 2016a)). Ngoài ra, các mô hình ViT mạnh nhất đã được tiền huấn luyện sử dụng ngân sách tính toán lớn vượt quá 500k giờ lõi TPU-v3 (Zhai et al., 2022), điều này vượt quá đáng kể tính toán được sử dụng để tiền huấn luyện ConvNets.

Chúng tôi đánh giá thuộc tính tỷ lệ của họ mô hình NFNet (Brock et al., 2021), một kiến trúc tích chập thuần túy được xuất bản đồng thời với các bài báo ViT đầu tiên, và ConvNet cuối cùng thiết lập SOTA mới trên ImageNet. Chúng tôi không thực hiện bất kỳ thay đổi nào đối với kiến trúc mô hình hoặc quy trình huấn luyện (ngoài việc điều chỉnh các siêu tham số đơn giản như tỷ lệ học tập hoặc ngân sách epoch). Chúng tôi xem xét ngân sách tính toán lên đến tối đa 110k giờ lõi TPU-v4,1 và tiền huấn luyện trên tập dữ liệu JFT-4B chứa khoảng 4 tỷ hình ảnh có nhãn từ 30k lớp (Sun et al., 2017). Chúng tôi quan sát một quy luật tỷ lệ log-log giữa mất mát xác thực và ngân sách tính toán được sử dụng để tiền huấn luyện mô hình. Sau khi tinh chỉnh trên ImageNet, các mạng của chúng tôi sánh ngang hiệu suất của các ViT được tiền huấn luyện với ngân sách tính toán tương đương (Alabdulmohsin et al., 2023; Zhai et al., 2022), như được hiển thị trong Hình 1.

NFNets được tiền huấn luyện tuân theo quy luật tỷ lệ
Chúng tôi huấn luyện một loạt các mô hình NFNet với độ sâu và chiều rộng khác nhau trên JFT-4B. Mỗi mô hình được huấn luyện cho một loạt ngân sách epoch từ 0,25 đến 8, sử dụng lịch trình tỷ lệ học tập suy giảm cosin.
1Lõi TPU-v4 có khoảng gấp đôi lý thuyết flops của lõi TPU-v3, tuy nhiên cả hai lõi đều có bộ nhớ tương tự.
Tác giả liên hệ: slsmith@google.com
©2023 Google DeepMind. Bảo lưu tất cả quyền arXiv:2310.16764v1 [cs.CV] 25 Oct 2023

--- TRANG 2 ---
ConvNets Sánh ngang Vision Transformers ở Quy mô Lớn
103104105
Giờ Lõi TPU-v41011121314Lỗi Top-1 ImageNet (%)
F1
F3
F3+
F7
F7+
F7+ với RA
ViT-g/14
ViT-G/14
SoViT-400m/14
Hình 1|Lỗi Top-1 ImageNet, sau khi tinh chỉnh các mô hình NFNet được tiền huấn luyện trong 50 epochs. Cả hai trục đều được chia tỷ lệ log. Hiệu suất cải thiện đều đặn khi tính toán được sử dụng trong quá trình tiền huấn luyện tăng lên. Mô hình lớn nhất của chúng tôi (F7+) đạt hiệu suất tương đương với những gì được báo cáo cho các ViT được tiền huấn luyện với ngân sách tính toán tương tự (Alabdulmohsin et al., 2023; Zhai et al., 2022). Hiệu suất của mô hình này cải thiện hơn nữa khi được tinh chỉnh với tăng cường lặp lại (RA) (Hoffer et al., 2019).

Tỷ lệ học tập cơ sở được điều chỉnh riêng biệt cho mỗi ngân sách epoch trên một lưới logarit nhỏ. Trong Hình 2, chúng tôi cung cấp mất mát xác thực ở cuối huấn luyện trên một tập giữ lại 130k hình ảnh, được vẽ so với ngân sách tính toán cần thiết để huấn luyện mỗi mô hình2. Chúng tôi lưu ý rằng F7 có cùng chiều rộng với F3, nhưng gấp đôi độ sâu. Tương tự F3 gấp đôi độ sâu của F1, và F1 gấp đôi độ sâu của F0. F3+ và F7+ có cùng độ sâu với F3 và F7 nhưng chiều rộng lớn hơn. Chúng tôi huấn luyện sử dụng SGD với Momentum và Adaptive Gradient Clipping (AGC) ở kích thước batch 4096, và chúng tôi sử dụng độ phân giải hình ảnh 224×224 trong quá trình huấn luyện và 256×256 khi đánh giá. Để biết thêm chi tiết mô tả kiến trúc NFNet và pipeline huấn luyện, chúng tôi giới thiệu người đọc đến bài báo gốc (Brock et al., 2021), bao gồm khung tiền huấn luyện cho JFT được mô tả trong Phần 6.2. Lưu ý rằng chúng tôi đã loại bỏ các hình ảnh gần trùng lặp trong tập huấn luyện và xác thực của ImageNet khỏi JFT-4B trước khi huấn luyện (Kolesnikov et al., 2020).

Hình 2 cho thấy một xu hướng tuyến tính rõ ràng, phù hợp với quy luật tỷ lệ log-log giữa mất mát xác thực và tính toán tiền huấn luyện. Điều này phù hợp với các quy luật tỷ lệ log-log được quan sát trước đó khi thực hiện mô hình hóa ngôn ngữ với transformers (Brown et al., 2020; Hoffmann et al., 2022).

Kích thước mô hình tối ưu và ngân sách epoch tối ưu (đạt mất mát xác thực thấp nhất) đều tăng kích thước khi ngân sách tính toán tăng. Chúng tôi phát hiện rằng một quy tắc thực tế đáng tin cậy là tỷ lệ kích thước mô hình và số epoch huấn luyện với cùng một tỷ lệ, như đã quan sát trước đó cho mô hình hóa ngôn ngữ bởi Hoffmann et al. (2022). Chúng tôi lưu ý rằng ngân sách epoch tối ưu lớn hơn 1 cho ngân sách tính toán tổng thể lớn hơn khoảng 5k giờ lõi TPU-v4.

Trong Hình 3, chúng tôi vẽ tỷ lệ học tập tối ưu quan sát được (làm tối thiểu mất mát xác thực), cho 3 mô hình của chúng tôi, qua một loạt ngân sách epoch.3 Lưu ý rằng chúng tôi đã điều chỉnh tỷ lệ học tập trên một lưới logarit được phân cách bởi các hệ số 2. Chúng tôi thấy rằng tất cả các mô hình trong họ NFNet đều cho thấy tỷ lệ học tập tối ưu tương tự α≈1.6 cho ngân sách epoch nhỏ. Tuy nhiên, tỷ lệ học tập tối ưu giảm khi ngân sách epoch tăng, và đối với các mô hình lớn, tỷ lệ học tập tối ưu giảm nhanh hơn. Trong thực tế, người ta có thể điều chỉnh tỷ lệ học tập một cách hiệu quả trong vòng 2 lần thử bằng cách giả định rằng tỷ lệ học tập tối ưu giảm chậm nhưng đơn điệu khi cả kích thước mô hình và ngân sách epoch tăng.

2Chúng tôi ước tính tính toán cần thiết để huấn luyện mỗi mô hình bằng mắt từ các bước điển hình mỗi giây đạt được bởi mỗi mô hình trong quá trình huấn luyện (khi không bị preempt).
3Tỷ lệ học tập tối ưu cho thấy các xu hướng rất tương tự cho tất cả các mô hình. Chúng tôi chọn 3 mô hình ở đây để rõ ràng về mặt hình ảnh.

--- TRANG 3 ---
ConvNets Sánh ngang Vision Transformers ở Quy mô Lớn
103104105
Giờ Lõi TPU-v42.42.52.62.72.82.93.0Mất mát Xác thực JFT-4B
F0
F1
F3
F3+
F7
F7+
Hình 2|Mất mát giữ lại của NFNets trên JFT-4B, được vẽ so với tính toán được sử dụng trong quá trình huấn luyện. Cả hai trục đều được chia tỷ lệ log, và mỗi đường cong biểu thị một mô hình khác nhau được huấn luyện cho một loạt ngân sách epoch. Chúng tôi quan sát một xu hướng tuyến tính, phù hợp với các quy luật tỷ lệ được quan sát cho mô hình hóa ngôn ngữ.

Cuối cùng, chúng tôi lưu ý rằng một số mô hình được tiền huấn luyện trong Hình 2 hoạt động kém hơn so với mong đợi. Ví dụ, đường cong cho các mô hình NFNet-F7+ ở các ngân sách tiền huấn luyện khác nhau không mượt mà. Chúng tôi tin rằng điều này phát sinh vì pipeline tải dữ liệu của chúng tôi không đảm bảo rằng mỗi ví dụ huấn luyện sẽ được lấy mẫu một lần mỗi epoch nếu quá trình huấn luyện bị preempt/khởi động lại, có thể gây ra một số ví dụ huấn luyện bị lấy mẫu thiếu nếu một quá trình huấn luyện được khởi động lại nhiều lần.

NFNets được tinh chỉnh có tính cạnh tranh với Vision Transformers trên ImageNet

Trong Hình 1, chúng tôi tinh chỉnh NFNets được tiền huấn luyện của chúng tôi trên ImageNet, và vẽ lỗi Top-1 so với tính toán được sử dụng trong quá trình tiền huấn luyện. Chúng tôi tinh chỉnh mỗi mô hình trong 50 epochs sử dụng tối thiểu hóa nhận thức độ sắc nét (SAM) (Foret et al., 2020) với độ sâu ngẫu nhiên và dropout. Chúng tôi huấn luyện ở độ phân giải 384×384 và đánh giá ở 480×480.

Độ chính xác Top-1 ImageNet cải thiện đều đặn khi ngân sách tính toán tăng. Mô hình được tiền huấn luyện đắt nhất của chúng tôi, một NFNet-F7+ được tiền huấn luyện trong 8 epochs, đạt độ chính xác Top-1 ImageNet là 90,3% trong khi cần khoảng 110k giờ lõi TPU-v4 để tiền huấn luyện và 1,6k giờ lõi TPU-v4 để tinh chỉnh. Hơn nữa, chúng tôi đạt 90,4% độ chính xác Top-1 nếu chúng tôi bổ sung giới thiệu tăng cường lặp lại trong quá trình tinh chỉnh (Fort et al., 2021; Hoffer et al., 2019) với bội số tăng cường 4.4

Để so sánh, độ chính xác Top-1 tốt nhất được báo cáo của một NFNet trên ImageNet mà không có dữ liệu bổ sung là 86,8% (Fort et al., 2021), đạt được bởi một NFNet-F5 với tăng cường lặp lại. Điều này chứng minh rằng NFNets hưởng lợi đáng kể từ tiền huấn luyện quy mô lớn.

Bất chấp những khác biệt đáng kể giữa hai kiến trúc mô hình, hiệu suất của NFNets được tiền huấn luyện ở quy mô lớn rất tương tự với hiệu suất của Vision Transformers được tiền huấn luyện. Ví dụ, Zhai et al. (2022) đạt 90,2% Top-1 trên ImageNet với một ViT-g/14, sau khi tiền huấn luyện trên JFT-3B trong 210k giờ lõi TPU-v3, và 90,45% với một ViT-G/14 sau khi tiền huấn luyện trên JFT-3B trong hơn 500k giờ lõi TPU-v3. Trong một nghiên cứu gần đây, Alabdulmohsin et al. (2023) tối ưu hóa kiến trúc ViT và đạt 90,3% Top-1 với một SoViT-400m/14 sau khi tiền huấn luyện trên JFT-3B trong 230k giờ TPU-v3.

Chúng tôi đánh giá tốc độ tiền huấn luyện cho các mô hình này trên TPU-v4 (sử dụng codebase của các tác giả gốc), và ước tính rằng ViT-g/14 sẽ mất 120k giờ lõi TPU-v4 để tiền huấn luyện, trong khi ViT-G/14 sẽ mất 280k giờ lõi TPU-v4 và SoViT-400m/14 sẽ mất 130k giờ lõi TPU-v4. Chúng tôi sử dụng các ước tính này để so sánh hiệu quả tiền huấn luyện của ViTs và NFNets trong Hình 1.

Tuy nhiên, chúng tôi lưu ý rằng NFNets được tối ưu hóa cho TPU-v4, và hoạt động kém hơn khi được đánh giá trên các thiết bị khác. Ví dụ, chúng tôi ước tính rằng NFNet-F7+ sẽ cần 250 giờ lõi TPU-v3 để tiền huấn luyện trong 8 epochs trong codebase của chúng tôi.

0.25 0.5 1 2 4 8
Epochs Huấn luyện0.40.81.6Tỷ lệ Học tập Tối ưu
F0
F3
F7+
Hình 3|Tỷ lệ học tập tối ưu hoạt động có thể dự đoán và dễ điều chỉnh. Tất cả các mô hình cho thấy tỷ lệ học tập tối ưu tương tự α∼1.6 khi ngân sách epoch nhỏ. Tỷ lệ học tập giảm chậm khi kích thước mô hình và ngân sách epoch tăng.

4Khi sử dụng tăng cường lặp lại, chúng tôi giảm số lần truyền qua dữ liệu sao cho tổng chi phí tính toán của tinh chỉnh là không đổi.

--- TRANG 4 ---
ConvNets Sánh ngang Vision Transformers ở Quy mô Lớn

Cuối cùng, chúng tôi lưu ý rằng các checkpoint được tiền huấn luyện đạt mất mát xác thực thấp nhất trên JFT-4B không phải lúc nào cũng đạt độ chính xác Top-1 cao nhất trên ImageNet sau khi tinh chỉnh. Đặc biệt, chúng tôi thấy rằng, dưới ngân sách tính toán tiền huấn luyện cố định, chế độ tinh chỉnh đều đặn ưu tiên các mô hình lớn hơn một chút và ngân sách epoch nhỏ hơn một chút. Một cách trực quan, các mô hình lớn hơn có nhiều khả năng hơn và do đó có thể thích ứng tốt hơn với nhiệm vụ mới. Trong một số trường hợp, tỷ lệ học tập lớn hơn một chút (trong quá trình tiền huấn luyện) cũng đạt hiệu suất tốt hơn sau khi tinh chỉnh.

Thảo luận

Công trình của chúng tôi củng cố bài học cay đắng. Các yếu tố quan trọng nhất quyết định hiệu suất của một mô hình được thiết kế hợp lý là tính toán và dữ liệu có sẵn để huấn luyện5 (Tolstikhin et al., 2021). Mặc dù thành công của ViTs trong thị giác máy tính là cực kỳ ấn tượng, theo quan điểm của chúng tôi không có bằng chứng mạnh mẽ để cho rằng các ViT được tiền huấn luyện vượt trội hơn các ConvNet được tiền huấn luyện khi được đánh giá một cách công bằng. Tuy nhiên, chúng tôi lưu ý rằng ViTs có thể có những lợi thế thực tế trong các bối cảnh cụ thể, chẳng hạn như khả năng sử dụng các thành phần mô hình tương tự trên nhiều phương thức (Bavishi et al., 2023).

Lời cảm ơn

Chúng tôi cảm ơn Lucas Beyer và Olivier Henaff vì phản hồi về bản thảo trước đó của ghi chú này. Chúng tôi cũng cảm ơn Lucas Beyer vì đã cung cấp ước tính tốc độ huấn luyện cho các mô hình ViT trên thiết bị TPU-v4.

Tài liệu tham khảo

I. Alabdulmohsin, X. Zhai, A. Kolesnikov, and L. Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. arXiv preprint arXiv:2305.13035, 2023.

5Bằng thiết kế hợp lý, chúng tôi có nghĩa là các mô hình đủ biểu đạt và có sự lan truyền gradient ổn định.

R. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Taşırlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b.

A. Brock, S. De, S. L. Smith, and K. Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059–1071. PMLR, 2021.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020.

S. Fort, A. Brock, R. Pascanu, S. De, and S. L. Smith. Drawing multiple augmentation samples per image during training efficiently decreases test error. arXiv preprint arXiv:2105.13343, 2021.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016a.

K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European conference on computer vision, pages 630–645. Springer, 2016b.

E. Hoffer, T. Ben-Nun, I. Hubara, N. Giladi, T. Hoefler, and D. Soudry. Augment your batch: better training with larger batches. arXiv preprint arXiv:1901.09335, 2019.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L.

--- TRANG 5 ---
ConvNets Sánh ngang Vision Transformers ở Quy mô Lớn

Casas, L. A. Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pages 491–507. Springer, 2020.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11): 2278–2324, 1998.

C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843–852, 2017.

I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261–24272, 2021.

X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–12113, 2022.
