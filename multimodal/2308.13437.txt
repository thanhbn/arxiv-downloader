# 2308.13437.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/multimodal/2308.13437.pdf
# File size: 2475861 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Position-Enhanced Visual Instruction Tuning
for Multimodal Large Language Models
Chi Chen1, Ruoyu Qin1, Fuwen Luo1, Xiaoyue Mi3, Peng Li2, Maosong Sun1, Yang Liu1,2
1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University
2Institute for AI Industry Research (AIR), Tsinghua University
3Institute of Computing Technology, Chinese Academy of Sciences
Abstract
Recently, Multimodal Large Language Models (MLLMs) that enable Large Lan-
guage Models (LLMs) to interpret images through visual instruction tuning have
achieved significant success. However, existing visual instruction tuning methods
only utilize image-language instruction data to align the language and image modal-
ities, lacking a more fine-grained cross-modal alignment. In this paper, we propose
Position-enhanced Visual Instruction Tuning (PVIT), which extends the functional-
ity of MLLMs by integrating an additional region-level vision encoder. This integra-
tion promotes a more detailed comprehension of images for the MLLM. In addition,
to efficiently achieve a fine-grained alignment between the vision modules and the
LLM, we design multiple data generation strategies to construct an image-region-
language instruction dataset. Finally, we present both quantitative experiments and
qualitative analysis that demonstrate the superiority of the proposed model. Code
and data will be released at https://github.com/PVIT-official/PVIT .
1 Introduction
Recently, Multimodal Large Language Models (MLLMs) have made remarkable progress in enabling
existing Large Language Models (LLMs) [ 2,4,26] to comprehend images [ 1,6,29,16]. The
underlying principle of these methods is to integrate the capabilities of existing visual or multimodal
models into the LLM. Current MLLMs can be categorized into two classes based on how they achieve
this. The first class [ 29,25,31] directly leverages the zero-shot and few-shot capabilities of language
models, enabling the LLM to invoke external multimodal models by designing specific prompts. The
second type [ 1,6,16] aligns visual features with the representation space of language models through
visual instruction tuning, achieving end-to-end model integration. These end-to-end MLLMs have
more multimodal capabilities than the first type and are therefore receiving more and more attention.
Despite their success, these end-to-end MLLMs align the pre-trained image-level vision encoder with
the LLM using only image-language instruction-following data. Without fine-grained multimodal
instruction data, the ability of the models for detailed image understanding remains limited. For
example, in the illustrated case in Figure 1, it is challenging for the current MLLMs to discriminate
specific objects in complex scenes. In addition, the format of current visual instruction data also
restricts the ability of the model to follow more elaborate instructions, such as those containing spatial
information (e.g., “ What is this object in [REGION]? ” in Figure 1). These types of instructions have
the potential to reduce the complexity of interactions with the model and enhance the precision of
the instructions provided. Therefore, to further augment the existing MLLMs, it is crucial to explore
fine-grained multimodal instruction data, especially instructions involving spatial information, to
enable the model to achieve a more detailed understanding of images and facilitate more flexible
interactions.
Preprint.arXiv:2308.13437v2  [cs.CV]  14 Sep 2023

--- PAGE 2 ---
Q: What is the yellow and black object on the left side of the bottle above the refrigerator?
Q: What is this object in [REGION]?A: It is a light ﬁxture.
A: This object is a small radio.Region-levelVision Encoder       MLLM
PVIT+
Vision Encoder         LLMMLLM+
Figure 1: Comparison of the current MLLM and PVIT. The MLLM has two evident limitations: (1)
inefficient information delivery using plain language, and (2) restricted ability for detailed image
understanding. PVIT addresses these by incorporating an additional region-level vision encoder to
the MLLM through position-enhanced instruction tuning.
This poses two challenges. On one hand, there is not much of fine-grained aligned multimodal data
compared to image-text pairs, let alone the corresponding instruction data for fine-tuning the MLLM.
On the other hand, how to utilize these data to effectively extend and enhance the capabilities of
the MLLM is an open question. Some of our concurrent works [ 3,35,34] have made preliminary
attempts by fine-tuning existing MLLMs to support multimodal instructions with spatial coordinates.
Specifically, Chen et al. [3]and Zhao et al. [35] directly incorporate spatial coordinates in natural
language numerical form into the instruction data and fine-tune the MLLM to understand them.
Zhang et al. [34] first extracts spatial features from the vision encoder based on the input regions
and integrates them into the natural language instructions as inputs to the LLM. However, it should
be noted that the vision encoder utilized in existing MLLMs, such as CLIP [ 22], is pre-trained
using image-level supervision and inherently possesses limited fine-grained image understanding
capabilities [ 36,13]. Consequently, fine-tuning directly on this basis could be sub-optimal and may
conflict with the already existing capabilities of the MLLM. Given the availability of region-level
aligned Vision-and-Language Pretraining (VLP) models [ 36,13], an intriguing possibility arises:
Can we further enhance MLLMs by integrating the capabilities of region-level vision encoders?
In this paper, we propose Position-enhanced Visual Instruction Tuning (PVIT) that extends the
MLLM by incorporating an additional region-level vision encoder to facilitate support for region-
based inputs, as shown in Figure 1. Specifically, we adopt the vision encoder from RegionCLIP [ 36]
and utilize it to extract region-level features by taking images and regions as inputs. As an additional
source of information, the incorporation of region-level features in this way has a minimal impact
on the original MLLM. Furthermore, since the features provided by RegionCLIP are themselves
already aligned to the language at a fine-grained level, the overhead of aligning it to the MLLM will
be relatively small. Inspired by Liu et al. [16], we design a two-stage training strategy for PVIT
that first pre-training a linear projection to align the region features to the LLM word embedding,
followed by end-to-end fine-tuning to follow complex fine-grained instructions.
As mentioned before, fine-grained multimodal instruction data is very scarce, which affects related
research from both training and evaluation aspects. To this end, we propose a region-level instruction
data generation scheme, designing different methods based on different data sources to fit the needs
of region-level instruction data generation. In addition, we present a new evaluation dataset, FineEval ,
2

--- PAGE 3 ---
designed specifically to assess the ability of MLLMs to adhere to instructions that demand fine-grained
spatial details. We hope our presented data will help future research in this area.
To summarize, our contributions are three-fold:
•We introduce position-enhanced visual instruction tuning (PVIT), a method that extends the
fine-grained understanding and interaction capabilities for MLLM.
•We propose a region-level instruction data construction scheme, as well as an evaluation
dataset to facilitate the training and evaluation of PVIT.
•We perform extensive experiments and demonstrate the effectiveness of our proposed
method.
2 Related Work
2.1 Multimodal Large Language Models
In order to take advantage of the powerful zero-shot and reasoning capabilities of LLMs, an increasing
amount of work has turned to building vision-and-language models based on LLMs, termed as
Multimodal Large Language Models (MLLMs). Specifically, these MLLMs can be categorized into
two classes. Some works directly take the zero-shot and few-shot learning abilities of the off-the-shelf
LLM to interpret the user intentions and invokes external multimodal models accordingly [ 29,25,31].
Although these models enable flexible multimodal capabilities, their performance is dependent on the
capabilities of the LLM and the external model itself, and is therefore limited. Another series of work
achieves end-to-end model integration by aligning the output features of the visual encoder with the
feature space of the language model and using them directly as input to the language model [ 1,11,16].
Despite their success, these end-to-end MLLMs only align the pre-trained image-level vision encoder
with the LLM. In contrast, we focus on integrate the abilities of the region-level vision encoder
through position-enhanced visual instruction tuning.
2.2 Region-Level Understanding for MLLMs
In Vision-and-Language Pre-training (VLP), to enhance the model’s fine-grained understanding
of images, it is common practice to incorporate extensive region-level supervision during pre-
training [ 13,36,32]. In terms of MLLMs, some recent works have made preliminary attemps by
fine-tuning the MLLM to support instructions with regions involved. Specifically, GPT4RoI [ 34]
aggregates the region-level features from the image-level vision encoder of the MLLM, and forms
a hybrid input of image-level features, region-level features and language instructions to the LLM.
Shikra [ 3] and ChatSpot [ 35] directly incorporate spatial coordinate in the instruction data and fine-
tune the MLLM to understand them. However, since these models are built on top of an image-level
vision encoder, direct fine-tuning to obtain region-level understanding can be sub-optimal and may
conflict with existing capabilities. In this paper, we extends the MLLM by integrating it with an
additional region-level vision encoder to exploit its fine-grained image understanding capabilities.
2.3 Multimodal Instruction Data
Existing work collects multimodal instruction data in two ways. Most of the works utilize the already
available annotated datasets to construct datasets in instruction format [ 30,5,12]. Typically, task
descriptions are generated through manual design or automatic generation by LLMs for each dataset
to serve as instructions. These are then combined with the original task input and output to create an
instruction dataset. While existing benchmark datasets offer a substantial sources of data, they often
fall short of addressing human requirements in real-world applications. Therefore, some works utilize
the self-instruct [ 28] pipeline to collect diverse instruction data by prompting the LLM with seed
examples to generate more instruction examples. For example, LLaV A [ 16] uses textual descriptions
of the image including captions and bounding boxes to prompt GPT-4 [ 19] to generate high quality
diverse multimodal instruction examples. We draw inspiration from these works and introduce a data
generation approach specifically designed for region-level instruction data construction.
3

--- PAGE 4 ---
3 Methods
3.1 Model Design
Our model architecture, depicted in Figure 2, consists of three primary components: a vision encoder,
a region encoder, and a large language model (LLM). The model processes an input image together
with instructions containing embedded regions and generate corresponding responses.
 Describe the relationship between                and                 .Region-1RegionEncoderRegion-2RegionEncoderImageVision EncoderBackboneRoI HeadRegion[x1, y1, x2, y2]Linear ProjectionRegion EncoderPVITLarge Language Model
Image
Figure 2: Model architecture of PVIT.
Taking Figure 2 as an example, the instruction can be expressed as “ <Image> Describe the relation-
ship between <Region> and <Region> ”. In this instruction, “ <Image> ” and “ <Region> ” are special
tokens that serve as placeholders indicating the insertion positions for the respective features. For the
textual part of the instructions, we directly obtain their word embeddings XT.
For the region part of the instructions, each region rkis represented as [x1, y1, x2, y2], with (x1, y1)
and(x2, y2)representing the relative coordinates of the top-left and bottom-right corners. We use
RegionCLIP [ 36] as the region encoder to extract the region features using RoI pooling with the
image Iandrkas input. Then we apply a linear projection layer to map the region features into the
representation space of the LLM. We denote the collection of all final region features as XR.
We use CLIP ViT-L/14 [ 22] as the image encoder to process the image Iand produce image features
XI. The LLM then combines the features XI,XT, andXRas input from the image, instructions,
and regions, respectively, and generates a response Y.
3.2 Training
Inspired by Liu et al. [16], our model is trained in a two-stage fashion. In the first stage, we initialize
the model with the pre-trained LLaV A [ 16], and freeze the parameters of the image encoder, the region
encoder, and the LLM. We only train the linear projection layer that is responsible for transforming
the region features. The purpose of this training stage is to align the region features to the embedding
space of the MLLM, without affecting the MLLM itself. To this end, we collect a large-scale region-
level aligned dataset, with each example consisting of an image, a bounding box, and a brief text
description of the object within the bounding box. During the training process, the model receives
the image and bounding box as input and then predicts the corresponding text.
After the first training stage, the model is already capable of understanding region features and
leveraging the region-level understanding abilities of the region encoder. To further achieve strong
capabilities in following instructions that contain regions, we adopt a second stage of training with
region-level instruction data. During this training stage, we only keep the parameters of the image
encoder and the region encoder frozen, and fine-tune the rest of the model to adapt to the region-level
instructions. The details on constructing the region-level instruction data will be provided in the
following section.
3.3 Region-level Instruction Data Construction
As shown in Figure 3, our data construction scheme consists of three strategies: (1) Dataset Con-
version , which converts existing Visual Question Answering (VQA) datasets that delivered with
bounding boxes into region-level instruction form; (2) Task-Specific Instruction Data Generation ,
4

--- PAGE 5 ---
(b) Task -Specific Instruction Data Generation
(a) Dataset Conversion
146k QuantityRandom 
Selection(c) General Instruction Data Generation
Template(a) Dataset Conversion
Task -specific Single and Multi -turn 
Instruction Data
Single -turn Instruction DataQuestions
Attributes
…… RelationshipsCaptions
Task -specific Prompt
Detailed 
DescriptionsCaptionsGrounding 
Annotations
Grounding Annotation Extraction
In-context Examples
……
Genera lMulti -turn Instruction Data
Grounding 
Annotations
Question: What is the object in <Region>[0.316, 0.706, 0.5, 0.879]</Region> ?
Answer: The object is a chair.
Question: Is the person indicated in <Region>[0.391, 0.16, 0.729, 0.352]</Region> wearing a black and white striped shirt or a green shirt?
Answer: The person indicated in that region is wearing a black and white striped shirt.
Question: What is the pose of the black -haired individual in <Region>[0.391, 0.16, 0.729, 0.352]</Region> ?
Answer: The black -haired individual is seated in a relaxed manner and is reaching out to grab something.
Question: What distinct characteristics can be observed about the design of the shirt that the man is wearing?
Answer: The man is wearing a stylish white and black striped shirt, which creates a bold visual contrast with its alternating white a nd black stripes.Question Diversity Answer Quality
86k Quantity Question Diversity Answer Quality
22k Quantity Question Diversity Answer Quality(b) Task -Specific Instruction Data Generation
(c) General Instruction Data GenerationFigure 3: Illustration of our proposed region-level instruction data construction scheme. In (a), we
utilize existing datasets (e.g., GQA) to construct simple instruction data by applying templates. In
(b), we leverage ChatGPT to generate data with higher diversity based on prompts and annotations
designed for specific task types. And in (c), we derive instruction data through an improved prompting
method, where we first generate detailed descriptions of images from original annotations, then extract
automatic grounding annotations from the detailed descriptions. Finally, employing captions, detailed
descriptions, automatic grounding annotations, and randomly selected in-context examples as prompts,
ChatGPT generates high-quality multi-turn instruction data with rich question types and complex
reasoning answers.
which leverages ChatGPT [ 18] to generate region-level instruction data for a predefined set of mul-
timodal tasks; and (3) General Instruction Data Generation , which enriches images with detailed
descriptions and grounding annotations generated automatically, complemented by diverse in-context
examples, to produce more general region-level instruction data. As indicated at the bottom of
Figure 3, diversity of the generated region-level instruction data increases from the first to the third
strategies, while quantity decreases due to computational and economic constraints. In general, the
three strategies work collaboratively, making us capable of obtaining a large volume of high-quality
and diverse region-level instruction data.
3.3.1 Dataset Conversion
In this strategy, we convert existing VQA datasets into a region-level instruction format using
dataset-specific templates. We utilize two VQA datasets for this conversion, including GQA [ 7] and
VCR [ 33]. It yields 146k single-turn region-level instruction data. The templates used for conversion
can be found in the supplementary material.
3.3.2 Task-Specific Instruction Data Generation
Even though we can acquire a large amount of data through dataset conversion at a low cost, the
diversity of this data remains limited. To tackle this problem, we suggest generating region-level
instruction data using ChatGPT for a predetermined set of multimodal tasks. In particular, we select
five representative tasks, including small object recognition, same-category object discrimination,
object relationship based reasoning, object attribute based reasoning, and optical character recognition
(OCR). We design a task-specific prompt for each task, which consists of three components: (1) a
system message that outlines the task and data format requirements, (2) in-context examples of the
5

--- PAGE 6 ---
specific task, and (3) the textual descriptions of the image for which new region-level instructions
are being generated. The comprehensive prompts are shown in the supplementary material. By
adjusting the system message and in-context examples accordingly, we can obtain both single-turn
and multi-turn data. In total, we achieve 20k single-turn data and 66k multi-turn data.
To obtain the textual descriptions of the images, we resort to the detailed annotations available in
existing datasets. Specifically, we utilize the datasets of MS COCO [ 15], Visual Genome [ 10], and
COCO-Text [27]. And the annotations including captions, object attributes, bounding boxes, etc.
3.3.3 General Instruction Data Generation
To further improve both the diversity and quality of the generated data, we extend our procedure
to generate more general instruction data. The outline of this enhanced data generation process is
depicted in Figure 3.
First, we notice that ChatGPT produces better results when given more informative textual descrip-
tions of images. Hence, we adopt the approach of LLaV A [ 16] to harness ChatGPT for generating
detailed image descriptions. These descriptions are generally longer than the captions, richer in
information, and easier for ChatGPT to understand compared to simpler annotations, such as object
attributes.1
Second, we employ an off-the-shelf visual grounding model [ 17] to ground the objects in the detailed
descriptions to their corresponding locations within the image, i.e., to identify the bounding boxes of
the objects. The bounding boxes that are too small are discarded.
Third, as in-context examples derived from existing datasets tend to cover only a narrow spectrum
of topics, we composed several in-context examples through brainstorming sessions. Consequently,
these freshly crafted in-context examples are markedly distinct from those derived from pre-existing
datasets.
Finally, for each image, we combine the captions, detailed description, and grounding annotations to
serve as its textual description, and randomly select three instances from our newly authored set of
in-context examples. We then input both the textual description and the selected in-context examples
into ChatGPT to generate region-level instruction data. The structure of the prompt is akin to the one
employed in the task-specific instruction data generation strategy, but only one prompt is utilized.
Through this enhanced strategy, we successfully obtained a total of 22k high-quality data entries,
replete with diverse question types and intricate reasoning responses.
4 Experiments
4.1 Baselines
We compare our model with three strong baselines:
•LLaV A [16], an MLLM trained on image-level multimodal instruction data.
•Shikra [3], an MLLM trained on referential dialogue data which are synthesized by convert-
ing existing datasets into dialogue form with GPT-4.
•GPT4RoI [34], an MLLM trained on multimodal instruction data derived from existing
datasets using templates and dataset from LLaV A enhanced with automatically detected
bounding boxes.
4.2 Implementation Details
We construct our model based on the LLaV A-7B framework [ 16]. For the region encoder, we employ
a variant of the RegionCLIP model, using a ResNet50x4 as the visual backbone and pre-trained on
the Conceptual Captions dataset [ 24]. In the first training stage, we use a batch size of 128and a
learning rate of 2×10−3for one epoch. A cosine annealing learning rate schedule is applied, with a
warmup ratio of 0.03. In the second training stage, we reduce the learning rate to 2×10−5and train
1In practice, we reuse the dataset LLaV A-Instruct-150k [16] for this step.
6

--- PAGE 7 ---
Method COCO GQA
LLaV A [16] 40.04 46.82
Shikra [3] 53.91 54.81
GPT4RoI [34] 64.01 52.64
PVIT (Ours) 64.53 55.77
Table 1: Results on the recognition task (COCO) and multimodal reasoning task (GQA).
for three epochs. The maximum sequence length of the LLM is set to 2048 . All training is carried
out on eight A100 GPUs, each with 40GB of memory.
4.3 Objective Evaluation
In this section, we quantitatively examined the object recognition and multimodal reasoning capabili-
ties of the models.
4.3.1 Object Recognition
For evaluation, we utilize the validation set of MS COCO [ 15]. When presented with an image and a
bounding box, a model is required to identify the category of the object within the bounding box. And
accuracy is leveraged as the evaluation metric. For LLaV A, which does not process region-specific
input, we adapt by using cropped images that align with the specified bounding boxes.
As shown in Table 1, both our proposed PVIT and the baseline GPT4RoI outperform LLaV A and
Shikra significantly. We believe this superior performance is due to the fact that both PVIT and
GPT4RoI incorporate region-level features. Among the compared models, LLaV A lags considerably.
This result aligns with our expectations. The use of cropped images introduces a shift in data
distribution, making it sub-optimal. Therefore, fine-grained multimodal instruction tuning is a
promising direction to pursue.
4.3.2 Multimodal Reasoning
We utilize the validation set of GQA [ 7] for evaluation. GQA is a visual question answering (QA)
dataset specifically crafted to assess visual reasoning and compositional QA capabilities. For LLaV A,
the inputs are the question and the entire image. For the other three MLLMs, information about
the bounding box accompanying the question provided in the GQA dataset is also included. And
accuracy is used as the evaluation metric.
The results are presented in Table 1. Our proposed PVIT achieves the highest performance, show-
casing its efficacy. However, in contrast to its performance on the COCO dataset, GPT4RoI does
not surpass Shikra. We posit that this outcome arises because questions in the GQA dataset can be
addressed without referencing the bounding boxes. As a result, GPT4RoI does not derive advantage
from the region-level features, emphasizing the superiority of our approach. LLaV A also lags behind.
We surmise that this can be attributed to the limited size and diversity of the instruction dataset upon
which LLaV A was trained.
4.4 Human Evaluation
Similar to LLMs, automatically assessing the instruction-following capability of MLLMs presents
a considerable challenge. As a result, we turn to human evaluations. We present a new evaluation
dataset, FineEval , designed specifically to assess the ability of MLLMs to adhere to instructions
that demand fine-grained spatial details. FineEval comprises 130 manually crafted questions based
on 50 images. These questions probe the capabilities of the models through four unique lenses:
object recognition, attribute description, reasoning, and others. Notably, the questions in FineEval
emphasize detailed spatial information, pertain to various relatively small objects, and address
complex relationships between objects. Two examples and the statistics of FineEval are shown in
Figure 4.
7

--- PAGE 8 ---
•Describe theobjects around theobject [REGION -1].
•What istheshape oftheobject [REGION -2].
•What isthe relationship between the object
[REGION -1]andtheobject [REGION -3]?
•What isitin[REGION -1]?
•Please describe therules ofthegame inthepicture .
36%
32%27%5%
Object Recognition
Attribute Description
Reasoning
OthersFigure 4: Two examples from our proposed human evaluation data FineEval (left) and the statistics
of FineEval (right).
0 20 40 60 80 100
% Win RateOthersReasoningAttribute
DescriptionObject
RecognitionT otal
58.337.142.944.742.7
33.327.928.631.929.8
8.335.028.623.427.5
(a)0 20 40 60 80 100
% Win RateOthersReasoningAttribute
DescriptionObject
RecognitionT otal
66.752.146.431.943.7
25.022.135.733.530.8
8.325.717.934.625.6
(b)0 20 40 60 80 100
% Win RateOthersReasoningAttribute
DescriptionObject
RecognitionT otal
79.256.550.650.553.5
12.537.738.035.135.7
8.35.811.414.410.9
(c)Win
Tie
Loss
Figure 5: Win rate of PVIT in human ranking, against LLaV A (a), Shikra (b) and GPT4RoI (c).
4.4.1 Quantitative Results
Inspired by Ouyang et al. [21], we employ pairwise comparisons to evaluate model performance on
FineEval. For any two models, human evaluators rank their responses, and the win rates across the
entire dataset are then calculated as the evaluation metric. To mitigate bias, we randomize the order
of answer presentation and enlist five evaluators for the assessment, which means every response will
receive five individual ranking result.
The results for our proposed PVIT against LLaV A, Shikra, and GPT4RoI are depicted in Figure 5.
From these results, it is evident that PVIT consistently outperforms the three baselines, and often by a
significant margin. The sole exception is in the realm of object recognition, where PVIT lags slightly
behind Shikra. Delving deeper into the results, we find that this is due to that the object counting
ability of PVIT is weaker than that of Shikra. We theorize it could be rectified by integrating more
count-specific instruction data into our training set and leave it as future work.
4.4.2 Qualitative Results
To provide a comprehensive understanding of the capabilities of our model, we present several cases
in Figure 6. Firstly, it is crucial to emphasize that many of the questions in these cases would be
challenging to frame clearly without the aid of bounding boxes. For instance, in cases 1, 2, 3, and 6,
multiple objects belong to the same category, making it difficult to specify the target object using just
language without inadvertently revealing the answer. This observation aligns with our rationale for
exploring the fine-grained multimodal instruction-following abilities of MLLMs. Delving into these
cases, we would like to emphasize the following four capabilities:
(1)Object Recognition : Our model excels at identifying objects demarcated by bounding boxes.
First, as anticipated, it effectively recognizes larger objects, which aligns with the results presented in
Table 1. Second, the model demonstrates proficiency in recognizing smaller objects, as showcased
by the correct identification of “[REGION-1]” in case 1 as a screen and “[REGION-2]” in case 5 as
a fish. Furthermore, it distinguishes between various bounding boxes within the same image. For
instance, in case 1, it accurately discerns that the bounding boxes correspond to different objects.
(2)Attribute Description : Beyond mere object recognition, our model effectively describes their
attributes. While it can detail attributes visually present — such as color and location, even for
smaller objects — it can also elaborate on characteristics inherent to the object but not visible in the
image. For instance, much of the description generated for the jellyfish in case 5 is extrapolated from
external knowledge, not the image itself. This suggests that using MLLMs for conventional pure
8

--- PAGE 9 ---
What isontheside ofthestage in[REGION -1]?
IfIliveinthebuilding intheregion, Iwould feelasense of
stability and security .Being inatallbuilding provides a
vantage point, allowing metoobserve thesurroundings and
have abroader perspective .Thebuilding’s architecture and
design might also contribute toafeeling ofcomfort andpro-
tection .Additionally, being inatallbuilding can offer a
sense ofprivacy andseclusion, shielding mefrom thehustle
and bustle oftheoutside world, Overall, living insuch a
building canprovide asense ofbeing elevated andincontrol
ofone’s environment .
Case 1
 Case 2
 Case 3
Case 4
 Case 5
The man and thewoman inthegiven region aretalking to
each other .They areengaged inaconversation orpossibly
discussing something .
Case 6
Ontheside ofthestage, there isascreen .
What color istheperson in[REGION -2]wearing?
Theperson intheregion iswearing agreen shirt.
What would youfeelifyoulivein[REGION -3]?What object isin[REGION -1]?
Isitanoutlier inthepicture?
Can youtellmethereason from theview ofthecolor ?
From theviewpoint ofcolor, thereason why theredfish
stands outisbecause itisadistinct and vibrant shade that
deviates from thecooler tones oftheother fishintheimage .
Theredfish’s color makes itvisually prominent and draws
attention, creating avisual contrast andmaking itstand out
from thesurrounding fish.
Please describe theobject in[REGION -1]indetail .
The object inthespecified region isawhite jellyfish .Itisa
delicate and graceful creature that floats inthewater .Its
translucent body iscovered insoft white jelly, giving ita
gentle andethereal appearance .The jellyfish hasseveral long,
slender tentacles that extend from itsmain body, allowing it
togracefully move through thewater and capture prey.Its
presence adds atouch ofbeauty and mystery tothe
underwater environment .Once upon atime, inafaraway land, there was abeautiful
planet with agolden hue.This planet, adorned with splendor,
captivated thehearts ofallwho gazed upon it.Itshimmered
gracefully inthesky, surrounded byamagical atmosphere .
The sunlight reflected offitssurface, casting awarm glow
thatilluminated thesurrounding space .
Theplanet wasasight tobehold, with itsunique features and
captivating beauty .Itstood tall and proud, amajestic
presence inthecosmos .Itsinhabitants, thebeings thatcalled
ithome, were fortunate enough towitness thewonders of
nature every day.They lived inharmony with theplanet,
cherishing theabundance oflifeitprovided .The color ofthemask inthegiven region isgreen, which
distinguishes itfrom theother masks worn bytheindividuals
intheimage .
What aretheman andthewoman in[REGION -2]doing?Yes, itisconsidered anoutlier inthepicture because itisnot
aligned with theother fish.Please tellmetherelationship between [REGION -1]and
[REGION -2].
The relationship between thetwo regions isthatthehand in
Region 1belongs totheswimmer inRegion 2.
Write afairytale story about theplanet in[REGION -1]. What's sodifferent about themask in[REGION -1]this
person wears from theothers?Theobject inthespecified region isaredfish.
Please describe theobject in[REGION -2]indetail .
The object inthe given region isayellow fish.Itis
swimming gracefully intheocean, with itsvibrant yellow
color contrasting against theblue water .The fishhasasleek
andstreamlined body, with along tailthattrails behind itas
itmoves .
Figure 6: Six representative cases showcasing the diverse capabilities of the proposed PVIT method.
Region Representation COCO
Region-level Features 64.54
Textual Coordinates 52.55
Table 2: Comparison of different types of region representation on the recognition task. “Textual
Coordinates” refer to the approach where region coordinates are directly inputted as textual data.
Type 1 Type 2 Type 3 Type 4
Format (Error Rate) 3% 3% 0% 20%
Question (Good / Bad) 20 / 9 13 / 16 22/8 12 / 12
Answer (Good / Bad) 18 / 11 16 / 13 18 / 12 20/4
Table 3: Human evaluation on textual descriptions of images. See main content for details.
vision tasks might offer significant potential, given that MLLMs can offer extensive knowledge not
encapsulated in images.
(3)Reasoning : Our model demonstrates reasoning skills based on the image and provided instructions.
For example, in case 2, it identifies both the swimmer and her hands, deducing logically that the
hands belong to the swimmer. In case 3, it discerns the color variations among fish and leverages its
knowledge of visual contrast to explain why the red fish stands out.
(4)Text Generation : Despite being fine-tuned on a multimodal dataset, our model maintains robust
text-generation capabilities. As shown in the cases, the majority of its responses are coherent
and grammatically correct, with cases 1 and 4 serving as representative examples. However, it is
important to highlight that these cases cannot fully evaluate the text generation ability of our method.
Comprehensive evaluation is reserved for future work.
9

--- PAGE 10 ---
4.5 Ablation Study
4.5.1 Effect of Region Representations
To assess the efficacy of the region-level features obtained using the region encoder, we try to
substitute them with textual coordinates. Specifically, each region rkis represented as [x1, y1, x2, y2],
with(x1, y1)and(x2, y2)representing the relative coordinates of the top-left and bottom-right
corners. All coordinates are normalized to the range [0,1] and are rounded to three decimal places.
These coordinates are directly incorporated into the instruction as text, for example, “ Describe the
object in [0.121, 0.212, 0.301, 0.413]”. We train this model in a similar two-stage way. In the first
stage, only the word embeddings of the LLM are trainable. In the second stage, all model parameters
are trainable except for those in the image encoder. Note that there is no region encoder in this model.
As evident from the results in Table 2, the utilization of region-level features considerably enhances
performance. This underscores the value of incorporating the region encoder.
4.5.2 Impact of Textual Descriptions for Images
Given the crucial importance of textual descriptions for images in the data generation process, we
explore four unique types of textual descriptions during the general instruction data generation
process: (1) detailed descriptions and automatic grounding annotations; (2) captions and manual
object annotations; (3) captions, detailed descriptions, and automatic grounding annotations; (4)
captions, manual object annotations, and detailed descriptions. We manually evaluate the data
generated from 30 identical images using these textual descriptions from three perspectives including
format correctness, question diversity and creativity, and answer correctness. The results in Table 3
highlight that the third type produces the most superior data and is leveraged in our work.
5 Conclusion
To extend the fine-grained visual instruction following capabilities of Multimodal Large Language
Models (MLLMs), we introduce a new approach: position-enhanced visual instruction tuning (PVIT).
This technique augments MLLMs using an existing region encoder. We also present a novel region-
level instruction data construction scheme and a challenging human-written evaluation dataset,
FineEval. Our extensive experiments underscore the efficacy of our approach.
Assessing the fine-grained instruction following capability of MLLMs continues to be a challenge. In
the future, we plan to expand the scope of FineEval and incorporate a broader range of questions,
with an emphasis on multi-turn dialogues. While the generated region-level instruction data has
proven valuable, there are avenues for further enhancement. For instance, the pursuit of generating
more organic dialog-style data remains a worthy endeavor.
Acknowledgments
We thank Siyu Wang, Qiusi Zou and Zhaolu Kang for their participation in this work.
References
[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,
Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual
language model for few-shot learning. Advances in Neural Information Processing Systems , 35:
23716–23736, 2022. 1, 3
[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020. 1
[3]Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:
Unleashing multimodal llm’s referential dialogue magic, 2023. 2, 3, 6, 7
[4]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker
10

--- PAGE 11 ---
Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,
Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,
Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,
Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei
Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling
language modeling with pathways, 2022. 1
[5]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose
vision-language models with instruction tuning, 2023. 3
[6]Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao
Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi,
Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all
you need: Aligning perception with language models, 2023. 1
[7]Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual
reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 6700–6709, 2019. 5, 7, 14
[8]Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas
Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings
of the IEEE/CVF International Conference on Computer Vision , pages 1780–1790, 2021. 14
[9]Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei.
Visual genome: Connecting language and vision using crowdsourced dense image annotations.
International Journal of Computer Vision , 123:32–73, 2017. doi: 10.1007/s11263-016-0981-7.
URL https://doi.org/10.1007/s11263-016-0981-7 . 14
[10] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual Genome: Connecting
language and vision using crowdsourced dense image annotations. International journal of
computer vision , 123:32–73, 2017. 6
[11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image
pre-training with frozen image encoders and large language models, 2023. 3
[12] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng
Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. M3it: A large-scale dataset towards
multi-modal multilingual instruction tuning, 2023. 3
[13] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu
Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image
pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10965–10975, 2022. 2, 3, 14
[14] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating
object hallucination in large vision-language models, 2023. 15, 16
[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In Computer
Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13 , pages 740–755. Springer, 2014. 6, 7
[16] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
1, 2, 3, 4, 6, 7
11

--- PAGE 12 ---
[17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei
Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: Marrying DINO with grounded
pre-training for open-set object detection, 2023. 6
[18] OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt . (Accessed
on Jun 18, 2023). 5
[19] OpenAI. GPT-4 technical report, 2023. 3
[20] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1
million captioned photographs. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and
K.Q. Weinberger, editors, Advances in Neural Information Processing Systems , volume 24.
Curran Associates, Inc., 2011. URL https://proceedings.neurips.cc/paper/2011/file/
5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf . 14
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,
Advances in Neural Information Processing Systems , volume 35, pages 27730–27744. Cur-
ran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/
2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf . 8
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In Marina
Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine
Learning , volume 139 of Proceedings of Machine Learning Research , pages 8748–8763. PMLR,
18–24 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a.html . 2, 4
[23] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
networks. arXiv preprint arXiv:1908.10084 , 2019. 14
[24] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A
cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pages 2556–2565, 2018. 6
[25] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
HuggingGPT: Solving AI tasks with ChatGPT and its friends in huggingface, 2023. 1, 3
[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation
language models, 2023. 1
[27] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. COCO-Text:
Dataset and benchmark for text detection and recognition in natural images, 2016. 6
[28] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi,
and Hannaneh Hajishirzi. Self-Instruct: Aligning language models with self-generated in-
structions. In Proceedings of the 61st Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pages 13484–13508, Toronto, Canada, July
2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL
https://aclanthology.org/2023.acl-long.754 . 3
[29] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.
Visual ChatGPT: Talking, drawing and editing with visual foundation models, 2023. 1, 3
[30] Zhiyang Xu, Ying Shen, and Lifu Huang. MultiInstruct: Improving multi-modal zero-shot
learning via instruction tuning. In Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers) , pages 11445–11465, Toronto, Canada,
12

--- PAGE 13 ---
July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.641.
URL https://aclanthology.org/2023.acl-long.641 . 3
[31] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,
Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. MM-REACT: Prompting chatgpt for
multimodal reasoning and action, 2023. 1, 3
[32] Yuan Yao, Qianyu Chen, Ao Zhang, Wei Ji, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun.
PEVL: Position-enhanced pre-training and prompt tuning for vision-language models, 2022. 3
[33] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:
Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 6720–6731, 2019. 5, 14
[34] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen,
and Ping Luo. GPT4RoI: Instruction tuning large language model on region-of-interest, 2023.
2, 3, 6, 7
[35] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang
Peng, Runpei Dong, Chunrui Han, and Xiangyu Zhang. ChatSpot: Bootstrapping multimodal
LLMs via precise referring instruction tuning, 2023. 2, 3
[36] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,
Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. RegionCLIP: Region-based language-image
pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16793–16803, 2022. 2, 3, 4
13

--- PAGE 14 ---
A Experimental Details
A.1 Training Data for the First Stage Training
In the initial training stage, we adapt existing multi-modal datasets to a suitable format for training
our model. Specifically, each training instance should comprise an image, a bounding box, and
a textual description of the object within the bounding box. For this purpose, we incorporate the
following data sources: (1) region descriptions from Visual Genome [ 9]2, (2) the grounded dataset
for COCO and Visual Genome with bounding boxes extracted by MDETR [ 8]3, and (3) the grounded
SBU [ 20] dataset with bounding boxes obtained by GLIP [ 13]4. For each data sample, we formulate
the instruction as “<Image>\n<Region>” where “<Image>” and “<Region>” are placeholders that
will be replaced with image- and region-level features. Given the image. the bounding box, and the
instruction, the model is asked to predict the corresponding annotations such as the human-annotated
region description or the phrase description grounded by off-the-shelf grounding models.
A.2 Evaluation on Objection Recognition
Given that the models have been trained on instruction-style data, their outputs typically manifest
as sentences, rather than solely the object category names. To address this, we introduce a post-
processing step that refines the outputs of the models to yield just the category names. In this
process, upon receiving an output of the model, we compute the similarity between this output and
the reference phrase “ an image of a [CLASS] ”. Here, “ [CLASS] ” stands in for each category name
within the dataset. This similarity calculation is based on Sentence-BERT [ 23]. The category that
demonstrates the highest similarity with the output of the model is then chosen as the final result.
A.3 Criteria for Human Evaluation in the Ablation Study on Textual Descriptions of Images
Each generated data entry is evaluated manually from three perspectives including format correctness,
question diversity and creativity, and answer correctness. The format is deemed correct if regions
are incorporated into the data entry as anticipated and presented in the right format. Those data
entries with a correct format undergo further assessment. A data entry earns a “Good” rating from
the question perspective if its question segment showcases diversity or creativity. Conversely, it is
rated as “Good” from the answer perspective if the response is semantically correct.
A.4 Data Filtering
During the task-specific and general instruction data generation processes, since ChatGPT does not
always adhere to the prompt perfectly, malformed data will be generated. Therefore we introduce
a filtering strategy to filter out ill-formed data. For the single-turn data, entries are discarded if the
answer contains a region or if the region format is not correct. In the case of multi-turn data, entries
are also excluded if the questions do not contain any region.
B Templates and Prompts
B.1 Templates for Dataset Conversion
We transform the existing VQA datasets into a region-level instruction format using dataset-specific
templates. We utilize two VQA datasets for this conversion, including GQA [ 7] and Visual Common-
sense Reasoning (VCR) [33].
For GQA, e begin by enhancing the questions with object annotations. Given an original question
such as “What is this bird called?” and the region annotation corresponding to the object "bird" in
the question, we incorporate “in <Region >” into the relevant mention, resulting in the augmented
question “What is this bird in <Region> called?”. Our final instruction template for the GQA dataset
2https://huggingface.co/datasets/visual_genome
3https://github.com/ashkamath/mdetr
4https://huggingface.co/datasets/gligen/sbu_tsv
14

--- PAGE 15 ---
Datasets Metric PVIT (Ours) LLaV A Shikra InstructBLIP MiniGPT-4 MM-GPT mPLUG-Owl
RandomAccuracy ( ↑) 83.95 50.37 86.90 88.57 79.67 50.10 53.97
Precision ( ↑) 79.77 50.19 94.40 84.09 78.24 50.05 52.07
Recall ( ↑) 92.27 99.13 79.27 95.13 82.20 100.00 99.60
F1-Score ( ↑) 85.56 66.64 86.19 89.27 80.17 66.71 68.39
Yes 59.62 98.77 43.26 56.57 52.53 99.90 95.63
PopularAccuracy ( ↑) 81.93 49.87 83.97 82.77 69.73 50.00 50.90
Precision ( ↑) 76.46 49.93 87.55 76.27 65.86 50.00 50.46
Recall ( ↑) 92.27 99.27 79.20 95.13 81.93 100.00 99.40
F1-Score ( ↑) 77.38 66.44 83.16 84.66 73.02 66.67 66.94
Yes 69.23 99.40 45.23 62.37 62.20 100.00 98.57
AdversarialAccuracy ( ↑) 73.03 49.70 83.10 72.10 65.17 50.00 50.67
Precision ( ↑) 66.63 49.85 85.60 65.13 61.19 50.00 50.34
Recall ( ↑) 92.27 99.07 79.60 95.13 82.93 100.00 99.33
F1-Score ( ↑) 77.38 66.32 82.49 77.32 70.42 66.67 66.82
Yes 69.23 99.37 46.50 73.03 67.77 100.00 98.67
Table 4: Object hallucination evaluation results based on the POPE pipeline [ 14]. We observe that
PVIT experiences the object hallucination problem no more frequently than the baseline models,
especially compared to our base model LLaV A.
takes the form “<Image>\n<Augmented Question>”, and the model is asked to predict the original
answer with this instruction.
For VCR, we use questions, answers, and rationales from the dataset to create instruction data.
We use the question directly as the instruction, while the ground truth response is formatted as
“<Answer><Rationale>”. We also make some modifications to the raw VCR data to make it more
like natural language. Specifically, all objects in the original data are represented by “<class><id>”,
where “<class>” represents the category name and “<id>” indicates the ordinal position of the entity
within that category in the image. Each of these objects corresponds to a specific region. In our
modified version, we replace these markers with "the <ord><class> in <region>", where “<ord>” is
an ordinal number such as “first”, “second”, etc., corresponding to the “<id>” in the original data.
B.2 Prompts for Instruction Data Generation
The overall prompts for generating instruction data with ChatGPT are constructed using the process
illustrated in Table 5. Depending on the task and generation method, unique system messages and
in-context examples are used. The system messages and in-context examples for the five tasks in
task-specific instruction data generation are presented in Tables 6 to 10:
• Small object recognition in Table 6;
• Same-category object discrimination in Table 7;
• Object relationship based reasoning in Table 8;
• Object attribute based reasoning in Table 9;
• Optical character recognition (OCR) in Table 10.
The system message and in-context examples for general instruction data generation is shown in
Table 11.
C Deeper Analysis on Capacity of Attribute Description
To go deeper into the capability of attribute description, we segmented the attribute description subset
of FineEval into categories of color, count, location, and other attributes. The win rates for these
more nuanced subsets are illustrated in Figure 7. Generally speaking, our model performs equal to
or better than other models except on the “count” subset, suggesting that PVIT may be not good at
counting objects. We theorize this shortcoming could be rectified by integrating more count-specific
instruction data into our training set. We leave this as future work.
15

--- PAGE 16 ---
0 20 40 60 80 100
% Win RateOther
AttributesLocationCountColor
47.253.616.753.1
25.025.038.928.1
27.821.444.418.8
(a)0 20 40 60 80 100
% Win RateOther
AttributesLocationCountColor
61.150.013.946.9
22.242.955.637.5
16.77.130.615.6
(b)0 20 40 60 80 100
% Win RateOther
AttributesLocationCountColor
80.646.422.216.7
8.346.466.766.7
11.17.111.116.7
(c)Win
Tie
LossFigure 7: Win rate of PVIT in human ranking on segmented attribute description subset, against
LLaV A (a), Shikra (b) and GPT4RoI (c).
D Object Hallucination
Object hallucination refers to the problem that a model generates objects that are not inconsistent with
the image [ 14]. We assess our propose PVIT method following the POPE evaluation pipeline [ 14] and
the results are shown in Table 4. We observe that PVIT experiences the object hallucination problem
no more frequently than the baseline models, especially compared to our base model LLaV A.
messages = [ { "role":"system", "content":task[‘system_message’] } ]
for example intask[‘in_context_examples’] :
messages.append({"role":"user", "content":example[‘context’]})
messages.append({"role":"assistant", "content":example[‘response’]} )
messages.append({"role":"user", "content":query_annotations})
Table 5: Illustration of the prompt construction process for ChatGPT to generate data according to
specific task requirements. For each task, including both task-specific and general instruction data gen-
eration, we devise corresponding task[‘system_message’] andtask[‘in_context_examples’] .
These serve the purposes of outlining the task requirements and providing in-context examples,
respectively. Each in-context example includes an input example[‘context’] and an output
example[‘response’] . Detailed prompts are presented in Tables 6 to 11. Lastly, we add
query_annotations , which provides annotation information regarding the target image.
16

--- PAGE 17 ---
System Message
You are an AI visual assistant that can analyze a single image. You receive five sentences, each
describing the same image you are observing. In addition, specific object locations within the image
are given, along with detailed coordinates. These coordinates are in the form of bounding boxes,
represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond
to the top left x, top left y, bottom right x, and bottom right y.
The task is to create a question related to the image based on the information provided by the image,
and provide the answer in detail. The question must involve mentioning the position of an object
in the image and asking questions related to that object. The position can be represented in the
following format: <Region>[x1, y1, x2, y2]</Region>, where (x1, y1, x2, y2) with floating numbers
ranging from 0 to 1 correspond to the top left x, top left y, bottom right x, and bottom right y of a
bounding box.
The questions should be as complex and diverse as possible, and the user must understand the object
relationships in the image in order to complete the question. Please try to select objects from smaller
bounding boxes for questioning.
Only include questions that have definite answers: (1) one can see the content in the image that the
question asks about and can answer confidently; (2) one can determine confidently from the image
that it is not in the image. Do not ask any question that cannot be answered confidently. Do not
include bounding box information in the output.
In-context Example
Context :
a living room with a bed a couch and a tv
The bedroom is decorated in modern style with hardwood floor, and painted walls with one having a
contrasting color.
The green velvet bed is low to the ground.
a black futon bed next to a window with a big green plant
A green couch is sitting in the corner of a living room.
vase: <Region>[0.39, 0.335, 0.445, 0.395]</Region>
couch: <Region>[0.001, 0.669, 0.124, 0.987]</Region>
remote: <Region>[0.795, 0.86, 0.867, 0.905]</Region>
remote: <Region>[0.772, 0.837, 0.859, 0.881]</Region>
potted plant: <Region>[0.292, 0.081, 0.56, 0.416]</Region>
bed: <Region>[0.061, 0.489, 0.721, 0.808]</Region>
remote: <Region>[0.832, 0.878, 0.929, 0.913]</Region>
tv: <Region>[0.722, 0.55, 0.898, 0.809]</Region>
vase: <Region>[0.856, 0.398, 0.991, 0.904]</Region>
Response :
Question: If the object in <Region>[0.39, 0.335, 0.445, 0.395]</Region> falls from the windowsill,
where is it most likely to land, and will it break?
======
Answer: The object is most likely to fall on the bed, so it won’t break.
Table 6: System message and in-context examples (limited to only one due to the length restriction)
for the small object recognition task.
17

--- PAGE 18 ---
System Message
You are an AI visual assistant that can analyze a single image. You receive five sentences, each
describing the same image you are observing. In addition, specific object locations within the image
are given, along with detailed coordinates. These coordinates are in the form of bounding boxes,
represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond
to the top left x, top left y, bottom right x, and bottom right y.
The task is to create a question related to the image based on the information provided by the image,
and provide the answer in detail. The question must involve mentioning the position of an object
in the image and asking questions related to that object. The position can be represented in the
following format: <Region>[x1, y1, x2, y2]</Region>, where (x1, y1, x2, y2) with floating numbers
ranging from 0 to 1 correspond to the top left x, top left y, bottom right x, and bottom right y of a
bounding box.
The questions should be as complex and diverse as possible, and the user must understand the object
relationships in the image in order to complete the question. Please select the object with the highest
quantity in the image for questioning.
Only include questions that have definite answers: (1) one can see the content in the image that the
question asks about and can answer confidently; (2) one can determine confidently from the image
that it is not in the image. Do not ask any question that cannot be answered confidently. Do not
include bounding box information in the output.
In-context Example
Context :
A left handed baseball player swinging a bat in front of a catcher and umpire.
a baseball player swinging a bat at a ball
A man hitting a baseball in a professional baseball game.
Baseball player hitting a ball with a baseball bat.
A baseball batter trying to hit a baseball.
sports ball: <Region>[0.274, 0.49, 0.317, 0.529]</Region>
person: <Region>[0.226, 0.001, 0.294, 0.137]</Region>
baseball glove: <Region>[0.511, 0.516, 0.568, 0.641]</Region>
person: <Region>[0.644, 0.273, 0.986, 0.892]</Region>
person: <Region>[0.442, 0.141, 0.67, 0.791]</Region>
baseball bat: <Region>[0.325, 0.397, 0.461, 0.577]</Region>
person: <Region>[0.767, 0.014, 0.893, 0.382]</Region>
person: <Region>[0.508, 0.401, 0.725, 0.873]</Region>
Response :
Question: What is the most likely position of this person in <Region>[0.508, 0.401, 0.725,
0.873]</Region> in this baseball game?
======
Answer: According to the position and posture, this person is most likely to be a catcher.
Table 7: System message and in-context examples (limited to only one due to the length restriction)
for the same-category object discrimination task.
18

--- PAGE 19 ---
System Message
You are an AI visual assistant that can analyze a single image. You receive detailed descriptions
of specific regions in the image and their corresponding locations, along with detailed coordinates.
These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating
numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x,
and bottom right y. In addition, the relationships between objects in the image are also provided in
the form of <subject: location><predicate><object: location>.
The task is to create a question related to the image based on the information provided by the image,
and provide the answer in detail. The question must involve mentioning the position of objects in
the image and asking relationships between these objects. The position can be represented in the
following format: <Region>[x1, y1, x2, y2]</Region>, where (x1, y1, x2, y2) with floating numbers
ranging from 0 to 1 correspond to the top left x, top left y, bottom right x, and bottom right y of a
bounding box.
The questions should be as complex and diverse as possible, and the user must understand the object
relationships in the image in order to complete the question.
Only include questions that have definite answers: (1) one can see the content in the image that the
question asks about and can answer confidently; (2) one can determine confidently from the image
that it is not in the image. Do not ask any question that cannot be answered confidently. Do not
include bounding box information in the output.
In-context Example
Context :
windows in the building: [0.008, 0.446, 0.058, 0.551]
trees behind the building: [0.531, 0.216, 0.702, 0.426]
a window on a building : [0.371, 0.441, 0.383, 0.496]
a window on a building : [0.308, 0.434, 0.329, 0.494]
a window on a building : [0.426, 0.424, 0.446, 0.489]
A window on the side of a building: [0.293, 0.454, 0.309, 0.494]
a window on a building : [0.324, 0.441, 0.361, 0.499]
a window on a building : [0.324, 0.441, 0.361, 0.499]
A window on the side of a building: [0.426, 0.456, 0.446, 0.486]
a window on a building : [0.426, 0.424, 0.446, 0.489]
<engine: [0.531, 0.584, 0.646, 0.664]><under><wing: [0.369, 0.506, 0.566, 0.662]>
<trees: [0.0, 0.14, 0.993, 0.541]><behind><building: [0.0, 0.368, 0.933, 0.679]>
<window: [0.451, 0.439, 0.493, 0.499]><on a><building: [0.013, 0.396, 0.963, 0.679]>
Response :
Question: What object is located at the <Region>[0.369, 0.506, 0.566, 0.662]</Region> on the
plane? What is beneath it?
======
Answer: This object is a wing, and below it is the engine of the airplane.
Table 8: System message and in-context examples (limited to only one due to the length restriction)
for the object relationship based reasoning task.
19

--- PAGE 20 ---
System Message
You are an AI visual assistant that can analyze a single image. You receive detailed descriptions
of specific regions in the image and their corresponding locations, along with detailed coordinates.
These coordinates are in the form of bounding boxes, represented as (x1, y1, x2, y2) with floating
numbers ranging from 0 to 1. These values correspond to the top left x, top left y, bottom right x,
and bottom right y. In addition, the attributes of objects in the image are also provided in the form of
<object: location><attributes>.
The task is to create a question related to the image based on the information provided by the image,
and provide the answer in detail. The question must involve mentioning the position of objects in
the image and be related to their attributes. The position can be represented in the following format:
<Region>[x1, y1, x2, y2]</Region>, where (x1, y1, x2, y2) with floating numbers ranging from 0 to
1 correspond to the top left x, top left y, bottom right x, and bottom right y of a bounding box.
The questions should be as complex and diverse as possible, and the user must understand the objects’
locations in the image in order to complete the question.
Only include questions that have definite answers: (1) one can see the content in the image that the
question asks about and can answer confidently; (2) one can determine confidently from the image
that it is not in the image. Do not ask any question that cannot be answered confidently. Do not
include bounding box information in the output.
In-context Example
Context :
the lamp shade is off white: [0.458, 0.11, 0.595, 0.233]
a picture hanging on a wall: [0.007, 0.015, 0.164, 0.223]
picture on the wall: [0.0, 0.0, 0.134, 0.203]
wall lamp with shade: [0.46, 0.112, 0.598, 0.362]
a white telephone on a table: [0.535, 0.608, 0.635, 0.71]
round metal decoration on the headboard: [0.38, 0.438, 0.401, 0.478]
the lamp base is made of metal: [0.482, 0.227, 0.58, 0.36]
a small bedside table : [0.331, 0.602, 0.669, 0.997]
wall lamp with shade: [0.46, 0.112, 0.598, 0.362]
a light fixture hanging on a wall: [0.46, 0.087, 0.601, 0.358]
<telephone: [0.531, 0.613, 0.63, 0.743]><corded, white, plastic, off-white>
<bedside table: [0.369, 0.62, 0.639, 1.0]><small, square>
<headboard: [0.005, 0.425, 0.409, 0.677]><embelllished, wooden>
<wall: [0.001, 0.003, 0.999, 0.963]><white>
<lamp: [0.463, 0.115, 0.596, 0.36]><off>
<frame: [0.001, 0.007, 0.135, 0.208]><golden>
<spread: [0.001, 0.625, 0.431, 0.997]><gold, yellow>
<floor: [0.426, 0.942, 0.821, 0.998]><carpeted>
<shade: [0.459, 0.118, 0.595, 0.235]><off-white>
<knob: [0.485, 0.805, 0.5, 0.833]><round>
<decoration: [0.383, 0.44, 0.396, 0.477]><metal, round>
<base: [0.49, 0.222, 0.585, 0.358]><metal>
Response :
Question: Please point out the metal parts in the <Region>[0.463, 0.115, 0.596, 0.36]</Region>. Is
the object in this region currently emitting light?
======
Answer: There is a lamp in that region, and its base is made of metal. The lamp is currently turned
off, so it is not emitting light.
Table 9: System message and in-context examples (limited to only one due to the length restriction)
for the object attribute based reasoning task.
20

--- PAGE 21 ---
System Message
You are an AI visual assistant that can analyze a single image. You receive five sentences, each
describing the same image you are observing. In addition, specific text locations within the image
are given, along with detailed coordinates. These coordinates are in the form of bounding boxes,
represented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond
to the top left x, top left y, bottom right x, and bottom right y.
The task is to create a question related to the image based on the information provided by the image,
and provide the answer in detail. The question must involve mentioning the position of a text in the
image and asking questions related to it. The position can be represented in the following format:
<Region>[x1, y1, x2, y2]</Region>, where (x1, y1, x2, y2) with floating numbers ranging from 0 to
1 correspond to the top left x, top left y, bottom right x, and bottom right y of a bounding box.
The questions should be as complex and diverse as possible, and the user must understand the object
relationships in the image in order to complete the question.
Only include questions that have definite answers: (1) one can see the content in the image that the
question asks about and can answer confidently; (2) one can determine confidently from the image
that it is not in the image. Do not ask any question that cannot be answered confidently. Do not
include bounding box information in the output.
In-context Example
Context :
US Navy Blue Angels flying through the sky with a smoke trail.
A navy air plane leaves a trail of smoke in the sky
a jet plane is flying by leaving a trail of smoke
a jet flying in the air leaving a trail behind
A plane with US NA VY painted on it flying in the air.
NA VY: [0.9, 0.35, 0.955, 0.463]
U: [0.902, 0.283, 0.932, 0.307]
NA VY: [0.867, 0.651, 0.923, 0.75]
S: [0.883, 0.614, 0.934, 0.645]
C: [0.89, 0.581, 0.943, 0.609]
S: [0.904, 0.316, 0.938, 0.34]
Response :
Question: Please tell me how many acute angles all the letters in <Region>[0.9, 0.35, 0.955,
0.463]</Region> contain. For example, ’A’ contains 3 acute angles, ’B’ contains no acute angles,
and ’Y’ contains 1 acute angle.
======
Answer: It is ’NA VY’ there, so it contains 7 acute angles.
Table 10: System message and in-context examples (limited to only one due to the length restriction)
for the optical character recognition (OCR) task.
21

--- PAGE 22 ---
System Message
You are an AI assistant for creating dialogues. Currently, there is an image that you cannot directly see,
but you have received five sentences, each describing the same image you are observing. Additionally,
you have received a detail description about the image and positions of certain objects mentioned in
the description within the image, formatted as label: <Region>[x1, y1, x2, y2]</Region>.
Your task is to ask questions about a specific Region in the image, inquiring about its relevant
position, attributes, and so on, and provide answers based on the information you have received.
Please strictly follow these 3 rules for the task:
1. The generated questions should be as diverse as possible.
2. The generated questions MUST involve <Region>[x1, y1, x2, y2]</Region>.
3. <Region> MUST NOT appear in the generated answers.
In-context Example
Context :
A black and white photograph of two men on side of the road.
Two men by a church and a parking lot.
an old black and white photo of two people
Two men standing near each other by some parked cars
Two men outside near a car and moving a cart of some sort.
This is an old black and white photograph featuring two men standing near each other on the side of
the road. They are situated close to a parking lot where several parked cars can be seen. The men
appear to be outside, possibly near a church or other local establishment.
One of the men seems to be moving a cart, while the other man is observing the process. The parking
lot contains at least four cars, with one parked further down the road. Additionally, there are two
clocks and a parking meter in the scene, which gives an indication of a public or commercial setting.
men the men the other man: <Region>[0.605, 0.271, 0.951, 0.865]</Region>
the parking lot: <Region>[0.006, 0.469, 0.995, 0.837]</Region>
one of the men the other man: <Region>[0.322, 0.289, 0.577, 0.827]</Region>
clocks: <Region>[0.822, 0.246, 0.865, 0.285]</Region>
a parking meter: <Region>[0.232, 0.446, 0.292, 0.548]</Region>
a cart: <Region>[0.219, 0.52, 0.671, 0.906]</Region>
clocks: <Region>[0.891, 0.246, 0.927, 0.286]</Region>
cars cars: <Region>[0.002, 0.526, 0.186, 0.83]</Region>
a parking meter: <Region>[0.225, 0.446, 0.295, 0.829]</Region>
Response :
Question:
What activity is one of the men performing in <Region>[0.605, 0.271, 0.951, 0.865]</Region>, and
what is the other man doing?
===
Answer:
One of the men is moving a cart in this region, while the other man is observing the process.
===
Question:
Describe the objects present in the parking lot within <Region>[0.006, 0.469, 0.995, 0.837]</Re-
gion>.
===
Answer:
Within the specified region, the parking lot contains several parked cars, with one of them parked
further down the road. Additionally, there is also a parking meter present in the scene.
===
Question:
What kind of setting does the object in <Region>[0.225, 0.446, 0.295, 0.829]</Region> imply?
===
Answer:
This parking meter illustrates the need to pay for metered parking, implying that this is a public or
commercial setting.
Table 11: System message and in-context examples (limited to only one due to the length restriction)
for General Instruction Data Generation.
22
